{
    "hands_on_practices": [
        {
            "introduction": "Before a surrogate model can be constructed from simulation data, one must first decide which specific simulations to perform. This practice  delves into this crucial \"Design of Experiments\" step by implementing Latin Hypercube Sampling (LHS), a powerful statistical method for efficiently exploring a high-dimensional input space. You will not only generate sample points but also learn to quantitatively assess their quality using standard space-filling and correlation metrics, a fundamental skill for any robust modeling workflow.",
            "id": "4127533",
            "problem": "You are tasked with designing and evaluating sampling plans for surrogate model training in energy systems modeling. Consider an input space that is a hypercube. Use the following precise definitions and requirements to formulate your program and produce the required outputs.\n\nLet the input domain be the unit hypercube $[0,1]^d$ in $d$ dimensions. A Latin hypercube sampling (LHS) design of $n$ points in $d$ dimensions is a set of $n$ vectors in $[0,1]^d$ such that, for each dimension $j \\in \\{1,\\dots,d\\}$, the $n$ scalar coordinates across the $n$ points are stratified into $n$ disjoint intervals of equal length and exactly one coordinate falls in each interval. You must generate such a design using a pseudorandom construction.\n\nAfter constructing the design, compute the following quantities:\n- The pairwise Pearson correlation coefficients between all pairs of dimensions, obtained from the sample covariance and sample standard deviations computed over the $n$ points. As a summary statistic, report the maximum absolute correlation across all distinct dimension pairs.\n- Space-filling metrics to characterize the uniformity of the sample distribution:\n  1. The minimum pairwise Euclidean distance between any two points in $[0,1]^d$.\n  2. The sum of inverse squared pairwise distances across all unordered pairs of distinct points, which penalizes clustering.\n  3. The maximum one-dimensional projected gap across all dimensions, defined as follows: for a given dimension $j$, sort the $n$ coordinates $x_{1j},\\dots,x_{nj}$, append the boundary points $0$ and $1$, and take the largest adjacent difference; then take the maximum over all $j \\in \\{1,\\dots,d\\}$.\n\nYou must implement the LHS using a stratified random construction that produces coordinates uniformly within each stratum. Your implementation must be deterministic under a specified integer seed for the pseudorandom number generator. All computations are to be performed in pure mathematical terms over the unit hypercube with no physical units.\n\nTest Suite:\n- Case $1$: $d=10$, $n=50$, seed $=12345$.\n- Case $2$: $d=5$, $n=10$, seed $=2025$.\n- Case $3$: $d=2$, $n=2$, seed $=7$.\n\nFor each case, generate a Latin hypercube sampling design and compute:\n- The maximum absolute pairwise Pearson correlation across dimensions (float).\n- The minimum pairwise Euclidean distance (float).\n- The sum of inverse squared pairwise distances across all unordered pairs (float).\n- The maximum one-dimensional projected gap across all dimensions (float).\n\nYour program should produce a single line of output containing the results for all three cases as a comma-separated list enclosed in square brackets, where each case’s results are a list of four floats rounded to six decimal places. For example, the format must be like $[[r_{11},r_{12},r_{13},r_{14}],[r_{21},r_{22},r_{23},r_{24}],[r_{31},r_{32},r_{33},r_{34}]]$, where $r_{ij}$ denotes the $j$-th result for the $i$-th test case.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the fields of numerical methods and statistics, well-posed with clear definitions and specific test cases, and objective in its formulation. All necessary information is provided, and the problem is internally consistent and computationally feasible. We may therefore proceed with a solution.\n\nThe task is to generate Latin Hypercube Sampling (LHS) designs and evaluate their quality using four specified metrics. The process involves two main stages: the construction of the design and the computation of the metrics.\n\n**1. Latin Hypercube Sampling (LHS) Construction**\n\nLet the number of dimensions be $d$ and the number of sample points be $n$. The input domain is the unit hypercube $[0, 1]^d$. A stratified random LHS design is constructed to ensure that for each dimension, each of the $n$ strata of the form $[(k-1)/n, k/n)$ for $k \\in \\{1, \\dots, n\\}$ contains exactly one point's projection. The algorithm is as follows:\n\n1.  Initialize an empty $n \\times d$ matrix, $X$, which will hold the design points.\n2.  For each dimension $j \\in \\{1, 2, \\dots, d\\}$:\n    a. Generate a random permutation $\\pi_j$ of the set of integers $\\{0, 1, \\dots, n-1\\}$. This permutation assigns a unique stratum index to each of the $n$ points for the current dimension, ensuring the Latin hypercube property.\n    b. Generate a vector of $n$ independent random numbers, $U_j = (u_{1j}, u_{2j}, \\dots, u_{nj})$, where each $u_{ij}$ is drawn from a uniform distribution $U(0, 1)$.\n    c. The coordinate of the $i$-th point in the $j$-th dimension, denoted by $x_{ij}$, is computed by placing it randomly within its assigned stratum:\n        $$x_{ij} = \\frac{\\pi_j(i) + u_{ij}}{n}$$\n        Here, $\\pi_j(i)$ is the $i$-th element of the permutation for dimension $j$. This formula maps the point into the interval $[\\pi_j(i)/n, (\\pi_j(i)+1)/n)$.\n3.  The complete LHS design is the resulting $n \\times d$ matrix $X = [x_{ij}]$, where each row is a point in $[0, 1]^d$. The entire process is made deterministic by initializing the pseudorandom number generator with a specified integer seed.\n\n**2. Evaluation Metrics**\n\nOnce the design matrix $X$ is generated, its quality is evaluated using four metrics.\n\n**Metric 1: Maximum Absolute Pairwise Pearson Correlation**\nThe Pearson correlation coefficient, $r_{jk}$, between two dimensions $j$ and $k$ quantifies the linear relationship between the corresponding columns of the design matrix. It is defined by the sample covariance and sample standard deviations:\n$$r_{jk} = \\frac{\\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{\\sqrt{\\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)^2} \\sqrt{\\sum_{i=1}^{n} (x_{ik} - \\bar{x}_k)^2}}$$\nwhere $\\bar{x}_j$ is the sample mean of the coordinates in dimension $j$. The summary statistic is the maximum absolute value of this coefficient over all unique pairs of dimensions, which provides a measure of the worst-case linear dependence in the design.\n$$C_{\\text{max}} = \\max_{1 \\le j < k \\le d} |r_{jk}|$$\nA low value for $C_{\\text{max}}$ is desirable. If $d < 2$, no distinct pairs exist, and this metric is taken to be $0$.\n\n**Metric 2: Minimum Pairwise Euclidean Distance**\nThis metric assesses the separation of points. The Euclidean distance between two points $x_i = (x_{i1}, \\dots, x_{id})$ and $x_k = (x_{k1}, \\dots, x_{kd})$ in the design is given by:\n$$d(x_i, x_k) = \\sqrt{\\sum_{j=1}^{d} (x_{ij} - x_{kj})^2}$$\nThis metric is the minimum distance found among all $\\binom{n}{2}$ distinct pairs of points in the design:\n$$D_{\\text{min}} = \\min_{1 \\le i < k \\le n} d(x_i, x_k)$$\nA larger value of $D_{\\text{min}}$ indicates better space-filling properties, as it implies points are not excessively clustered.\n\n**Metric 3: Sum of Inverse Squared Pairwise Distances**\nThis metric strongly penalizes point clustering. It is calculated by summing the inverse squared Euclidean distances over all unordered pairs of distinct points:\n$$S_{\\text{inv}} = \\sum_{1 \\le i < k \\le n} \\frac{1}{d(x_i, x_k)^2}$$\nwhere $d(x_i, x_k)$ is the distance defined above. A smaller value for $S_{\\text{inv}}$ suggests a more uniform distribution of points. If any two points are identical, $d(x_i, x_k)=0$ and this sum would diverge to infinity; however, the chosen LHS construction makes this event practically impossible.\n\n**Metric 4: Maximum One-Dimensional Projected Gap**\nThis metric evaluates the uniformity of the design's projection onto each axis. For each dimension $j \\in \\{1, \\dots, d\\}$:\n1.  Take the set of coordinates in dimension $j$, $\\{x_{1j}, x_{2j}, \\dots, x_{nj}\\}$.\n2.  Augment this set with the boundary points $0$ and $1$.\n3.  Sort these $n+2$ values to form an ordered sequence $p_0, p_1, \\dots, p_{n+1}$, where $p_0=0$ and $p_{n+1}=1$.\n4.  The gaps are the differences between adjacent values: $g_i = p_i - p_{i-1}$ for $i \\in \\{1, \\dots, n+1\\}$.\n5.  The maximum gap for dimension $j$ is $G_j = \\max_{i=1, \\dots, n+1} g_i$.\nThe final metric is the maximum of these gaps over all dimensions, indicating the largest \"empty\" interval along any single axis.\n$$G_{\\text{max}} = \\max_{j=1, \\dots, d} G_j$$\nA smaller $G_{\\text{max}}$ is indicative of better one-dimensional uniformity.\n\nThe implementation will apply this complete procedure to each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist\n\ndef solve():\n    \"\"\"\n    Main function to generate LHS designs and compute metrics for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 50, 12345),\n        (5, 10, 2025),\n        (2, 2, 7),\n    ]\n\n    all_results = []\n    for d, n, seed in test_cases:\n        # Step 1: Generate Latin Hypercube Sampling design\n        rng = np.random.default_rng(seed)\n        lhs_matrix = np.zeros((n, d))\n        for j in range(d):\n            permutation = rng.permutation(n)\n            uniform_variates = rng.uniform(size=n)\n            lhs_matrix[:, j] = (permutation + uniform_variates) / n\n\n        # Step 2: Compute evaluation metrics\n\n        # Metric 1: Maximum absolute pairwise Pearson correlation\n        if d < 2:\n            max_abs_corr = 0.0\n        else:\n            correlation_matrix = np.corrcoef(lhs_matrix, rowvar=False)\n            # Set diagonal to zero to ignore self-correlation\n            np.fill_diagonal(correlation_matrix, 0)\n            max_abs_corr = np.max(np.abs(correlation_matrix))\n\n        # Metric 2 & 3: Minimum distance and sum of inverse squared distances\n        if n < 2:\n            min_dist = 0.0\n            sum_inv_sq_dist = 0.0\n        else:\n            # pdist computes distances for all unordered pairs of distinct points\n            pairwise_distances = pdist(lhs_matrix, 'euclidean')\n            \n            # Metric 2: Minimum pairwise Euclidean distance\n            min_dist = np.min(pairwise_distances)\n            \n            # Metric 3: Sum of inverse squared pairwise distances\n            sum_inv_sq_dist = np.sum(1.0 / (pairwise_distances**2))\n\n        # Metric 4: Maximum one-dimensional projected gap\n        max_overall_gap = 0.0\n        for j in range(d):\n            projection = lhs_matrix[:, j]\n            # Append boundary points 0 and 1, then sort\n            sorted_points = np.sort(np.concatenate(([0.0], projection, [1.0])))\n            # Calculate gaps between adjacent points\n            gaps = np.diff(sorted_points)\n            # Update the maximum gap found so far\n            max_overall_gap = max(max_overall_gap, np.max(gaps))\n\n        # Store the four computed metrics for the current case\n        case_results = [max_abs_corr, min_dist, sum_inv_sq_dist, max_overall_gap]\n        all_results.append(case_results)\n\n    # Format the final output string as specified\n    formatted_cases = []\n    for case_res in all_results:\n        # Format each float to six decimal places\n        formatted_values = [f\"{v:.6f}\" for v in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_values)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_cases)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Once system data is collected, a primary goal of model reduction is to distill its essential dynamics into a far more compact representation. This exercise explores Proper Orthogonal Decomposition (POD), a cornerstone data-driven technique that uses Singular Value Decomposition (SVD) to extract the most energetic \"modes\" or patterns from a collection of system snapshots. By working through this problem , you will gain hands-on experience in quantifying the trade-off between model size and accuracy, a central theme in reduced-order modeling.",
            "id": "4127506",
            "problem": "You are given a snapshot matrix $X \\in \\mathbb{R}^{m \\times n}$ of an energy system, where rows correspond to state components (such as nodal voltages or thermal states) and columns correspond to time or operating-point samples. A reduced-order (surrogate) representation is obtained by selecting the leading singular value decomposition modes. Your task is to quantify the fraction of the total energy retained by a rank-$r$ truncation, as measured by the squared Frobenius norm. The retained energy fraction for rank $r$ is defined as the ratio of the energy captured by the first $r$ singular values to the total energy of $X$.\n\nStart from the following fundamental base:\n- The squared Frobenius norm of a matrix $X$ is defined by $\\lVert X \\rVert_F^2 = \\sum_{i=1}^m \\sum_{j=1}^n X_{ij}^2$.\n- The singular value decomposition (SVD) of $X$ is defined as $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices (i.e., $U^\\top U = I_m$ and $V^\\top V = I_n$), and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal rectangular matrix containing the singular values $\\sigma_1, \\sigma_2, \\dots, \\sigma_p$ on its diagonal, with $p = \\min(m,n)$ and $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$.\n- Orthogonal invariance of the Frobenius norm implies $\\lVert X \\rVert_F^2 = \\lVert \\Sigma \\rVert_F^2 = \\sum_{i=1}^p \\sigma_i^2$.\n\nDefine the retained energy fraction for a chosen rank $r$ as\n$$\nE_r = \\frac{\\sum_{i=1}^{r} \\sigma_i^2}{\\sum_{i=1}^{p} \\sigma_i^2},\n$$\nwhere $p = \\min(m,n)$ and $r$ is an integer with $0 \\le r \\le p$. If $r > p$, treat $r$ as $p$. If $\\sum_{i=1}^{p} \\sigma_i^2 = 0$ (i.e., $X$ is the zero matrix), define $E_r = 0$ for any $r$.\n\nImplement a program that, for each specified test matrix and rank $r$, computes $E_r$ by:\n- Computing the singular value decomposition of $X$.\n- Using the singular values to evaluate $E_r$ exactly as defined above.\n\nExpress each retained energy fraction $E_r$ as a decimal rounded to six places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$0.500000,0.750000$]\"), with no spaces.\n\nUse the following test suite (each case provides $X$ and $r$):\n- Case $1$: $X = \\begin{bmatrix} 5 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$, $r = 2$.\n- Case $2$: $X = \\begin{bmatrix} 4 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}$, $r = 3$.\n- Case $3$: $X = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{bmatrix}$, $r = 1$.\n- Case $4$: $X = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $r = 0$.\n- Case $5$: $X = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$, $r = 2$.\n- Case $6$: $X = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $r = 5$.\n\nYour program should output the retained energy fractions for the six cases as a single line in the exact format \"[$E_1,E_2,E_3,E_4,E_5,E_6$]\" with each $E_i$ rounded to six decimal places as specified.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in linear algebra, specifically the theory of singular value decomposition (SVD), and its application to model order reduction is a standard and well-posed concept. The problem is self-contained, with all necessary definitions, data, and boundary conditions clearly specified. There are no contradictions, ambiguities, or factual errors.\n\nThe task is to compute the retained energy fraction, $E_r$, for a rank-$r$ truncation of a given snapshot matrix $X$. This fraction quantifies the portion of the total system \"energy,\" as measured by the squared Frobenius norm, that is captured by the first $r$ dominant singular modes. The formula is given as:\n$$\nE_r = \\frac{\\sum_{i=1}^{r} \\sigma_i^2}{\\sum_{i=1}^{p} \\sigma_i^2}\n$$\nwhere $\\sigma_i$ are the singular values of $X$ sorted in descending order, $p = \\min(m, n)$ is the number of singular values for an $m \\times n$ matrix, and $r$ is the chosen rank of the approximation.\n\nThe fundamental principle is the orthogonal invariance of the Frobenius norm, which states that $\\lVert X \\rVert_F^2 = \\lVert U \\Sigma V^\\top \\rVert_F^2 = \\lVert \\Sigma \\rVert_F^2$. Because $\\Sigma$ is a diagonal rectangular matrix with the singular values $\\sigma_i$ on its diagonal, its squared Frobenius norm is simply the sum of the squares of the singular values:\n$$\n\\lVert X \\rVert_F^2 = \\sum_{i=1}^{p} \\sigma_i^2\n$$\nThis sum represents the total energy in the system snapshot matrix $X$. A rank-$r$ approximation of $X$, denoted $X_r$, is constructed using only the first $r$ singular values and their corresponding singular vectors. The energy of this approximation is $\\lVert X_r \\rVert_F^2 = \\sum_{i=1}^{r} \\sigma_i^2$. The retained energy fraction $E_r$ is the ratio of these two quantities.\n\nThe procedure is as follows:\n1. For each matrix $X$ and rank $r$, determine the matrix dimensions $m$ and $n$, and the maximum possible rank $p = \\min(m, n)$.\n2. Adjust the requested rank $r$ according to the specified rule: if $r > p$, it is treated as $r=p$. The effective rank for the numerator summation is $r_{eff} = \\min(r, p)$.\n3. Compute the singular values $\\sigma_1, \\sigma_2, \\dots, \\sigma_p$ of $X$.\n4. Calculate the total energy (denominator): $E_{total} = \\sum_{i=1}^{p} \\sigma_i^2$.\n5. Handle the special case where $E_{total} = 0$ (i.e., $X$ is the zero matrix). In this instance, $E_r$ is defined as $0$.\n6. Otherwise, calculate the retained energy (numerator): $E_{retained} = \\sum_{i=1}^{r_{eff}} \\sigma_i^2$.\n7. Compute the fraction $E_r = E_{retained} / E_{total}$.\n\nWe now apply this procedure to each test case.\n\n**Case 1:**\n- Given: $X = \\begin{bmatrix} 5 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$, $r = 2$.\n- Dimensions: $m=3, n=3$, so $p = \\min(3, 3) = 3$.\n- The matrix $X$ is diagonal, so its singular values are the absolute values of the diagonal elements, sorted in descending order: $\\sigma_1=5, \\sigma_2=3, \\sigma_3=1$.\n- Total energy: $\\sum_{i=1}^{3} \\sigma_i^2 = 5^2 + 3^2 + 1^2 = 25 + 9 + 1 = 35$.\n- Retained energy for $r=2$: $\\sum_{i=1}^{2} \\sigma_i^2 = 5^2 + 3^2 = 25 + 9 = 34$.\n- $E_2 = \\frac{34}{35} \\approx 0.97142857$. Rounded to six decimal places, this is $0.971429$.\n\n**Case 2:**\n- Given: $X = \\begin{bmatrix} 4 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}$, $r = 3$.\n- Dimensions: $m=4, n=3$, so $p = \\min(4, 3) = 3$.\n- The singular values are the non-zero entries on the main diagonal, sorted: $\\sigma_1=4, \\sigma_2=2, \\sigma_3=1$.\n- Total energy: $\\sum_{i=1}^{3} \\sigma_i^2 = 4^2 + 2^2 + 1^2 = 16 + 4 + 1 = 21$.\n- The rank is $r=3$, which equals $p$. The retained energy is the total energy.\n- $E_3 = \\frac{21}{21} = 1$. The value is $1.000000$.\n\n**Case 3:**\n- Given: $X = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{bmatrix}$, $r = 1$.\n- Dimensions: $m=3, n=2$, so $p = \\min(3, 2) = 2$.\n- The second column is a multiple of the first, so the matrix has rank $1$. This implies that only one singular value is non-zero. The eigenvalues of $X^\\top X = \\begin{bmatrix} 14 & 28 \\\\ 28 & 56 \\end{bmatrix}$ are $\\lambda_1=70, \\lambda_2=0$.\n- The singular values are $\\sigma_1 = \\sqrt{70}, \\sigma_2 = 0$.\n- Total energy: $\\sum_{i=1}^{2} \\sigma_i^2 = (\\sqrt{70})^2 + 0^2 = 70$.\n- Retained energy for $r=1$: $\\sum_{i=1}^{1} \\sigma_i^2 = (\\sqrt{70})^2 = 70$.\n- $E_1 = \\frac{70}{70} = 1$. The value is $1.000000$.\n\n**Case 4:**\n- Given: $X = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $r = 0$.\n- Dimensions: $m=2, n=2$, so $p = \\min(2, 2) = 2$.\n- The matrix is the identity matrix $I_2$. Its singular values are $\\sigma_1=1, \\sigma_2=1$.\n- Total energy: $\\sum_{i=1}^{2} \\sigma_i^2 = 1^2 + 1^2 = 2$.\n- The rank is $r=0$. The numerator is an empty sum, which is $0$.\n- Retained energy for $r=0$: $\\sum_{i=1}^{0} \\sigma_i^2 = 0$.\n- $E_0 = \\frac{0}{2} = 0$. The value is $0.000000$.\n\n**Case 5:**\n- Given: $X = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$, $r = 2$.\n- Dimensions: $m=3, n=3$, so $p = \\min(3, 3) = 3$.\n- This is the zero matrix. All its singular values are $0$.\n- Total energy: $\\sum_{i=1}^{3} \\sigma_i^2 = 0^2 + 0^2 + 0^2 = 0$.\n- According to the problem definition, if the total energy is $0$, then $E_r=0$ for any $r$.\n- $E_2 = 0$. The value is $0.000000$.\n\n**Case 6:**\n- Given: $X = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $r = 5$.\n- Dimensions: $m=2, n=2$, so $p = \\min(2, 2) = 2$.\n- The singular values are $\\sigma_1=1, \\sigma_2=1$.\n- The requested rank $r=5$ is greater than $p=2$. The problem states to treat $r$ as $p$ in this case. So, the effective rank is $r_{eff}=2$.\n- Total energy: $\\sum_{i=1}^{2} \\sigma_i^2 = 1^2 + 1^2 = 2$.\n- Retained energy for $r_{eff}=2$: $\\sum_{i=1}^{2} \\sigma_i^2 = 1^2 + 1^2 = 2$.\n- $E_5 = \\frac{2}{2} = 1$. The value is $1.000000$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the retained energy fraction for a series of test cases\n    involving matrix truncation via SVD.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[5, 0, 0], [0, 3, 0], [0, 0, 1]], dtype=float), 2),\n        (np.array([[4, 0, 0], [0, 2, 0], [0, 0, 1], [0, 0, 0]], dtype=float), 3),\n        (np.array([[1, 2], [2, 4], [3, 6]], dtype=float), 1),\n        (np.array([[1, 0], [0, 1]], dtype=float), 0),\n        (np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=float), 2),\n        (np.array([[1, 0], [0, 1]], dtype=float), 5),\n    ]\n\n    results = []\n    for X, r in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        \n        # Get matrix dimensions and maximum possible rank p\n        m, n = X.shape\n        p = min(m, n)\n\n        # Compute singular values. We only need the values, not the full SVD.\n        s = np.linalg.svd(X, compute_uv=False)\n\n        # Square the singular values to represent their energy.\n        s_sq = np.square(s)\n\n        # The total energy is the sum of all squared singular values.\n        total_energy = np.sum(s_sq)\n\n        # Per the problem: if total energy is 0, the fraction is 0.\n        if total_energy == 0:\n            e_r = 0.0\n        else:\n            # The effective rank for summation cannot exceed p.\n            # Also handles r=0 correctly with slicing.\n            # Per problem statement: if r > p, treat r as p.\n            rank_k = min(r, p)\n            \n            # The retained energy is the sum of the first rank_k squared singular values.\n            # s is already sorted in descending order by np.linalg.svd.\n            retained_energy = np.sum(s_sq[:rank_k])\n            \n            # The fraction is the ratio of retained to total energy.\n            e_r = retained_energy / total_energy\n        \n        results.append(e_r)\n\n    # Final print statement in the exact required format.\n    # Each result is formatted to six decimal places.\n    print(f\"[{','.join(f'{res:.6f}' for res in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In contrast to data-driven methods, physics-based model reduction simplifies a system by leveraging its intrinsic structure and governing equations. This practice focuses on Kron reduction, a classic technique in power systems analysis used to create an exact, smaller equivalent model for a subset of network nodes by systematically eliminating others. This exercise  provides a practical challenge in applying matrix partitioning and linear algebra to derive the reduced model from first principles, solidifying your understanding of how network topology can be analytically simplified.",
            "id": "4127526",
            "problem": "You are given a resistive five-bus network intended to model a direct-current energy system as a surrogate for small-signal behavior of an alternating-current network. The network can be represented by an admittance matrix, constructed from line admittances between connected buses and shunt admittances from each bus to ground. Let the bus voltage vector be $V \\in \\mathbb{R}^{5}$ in volts and the bus current injection vector be $I \\in \\mathbb{R}^{5}$ in amperes. Kirchhoff's Current Law (KCL) and Ohm's law imply the nodal relationship $I = Y V$, where $Y \\in \\mathbb{R}^{5 \\times 5}$ is the symmetric admittance matrix. Partition the buses into boundary buses $\\{1,5\\}$ and internal buses $\\{2,3,4\\}$ and apply model reduction by eliminating internal states to obtain the reduced response seen by the boundary buses.\n\nConstruct $Y$ as follows: for each undirected line $(p,q)$ with line admittance $g$ in siemens (S), add $g$ to $Y_{pp}$ and $Y_{qq}$, and add $-g$ to $Y_{pq}$ and $Y_{qp}$. For each bus $k$ with shunt admittance $y_k$ in siemens, add $y_k$ to $Y_{kk}$. All line admittances and shunts are strictly nonnegative real numbers, and the network is connected.\n\nYour program must:\n- Build the full admittance matrix $Y$ from provided line and shunt data.\n- Partition $Y$ consistent with the boundary buses $\\{1,5\\}$ and internal buses $\\{2,3,4\\}$.\n- Eliminate internal bus voltages by enforcing the internal KCL equations to derive a reduced relation between boundary currents and boundary voltages.\n- Compute and verify voltage-current relationships for each test case.\n\nFundamental base: Use Ohm’s law and Kirchhoff’s Current Law, the definition of nodal admittance for resistive networks, and standard linear algebra for eliminating internal variables. Do not rely on any pre-given reduced-order formulas; derive the boundary response from the full network equations by eliminating internal voltages using KCL.\n\nNetwork line set (common to all test cases), where each tuple is $(p,q,g)$ with buses indexed by $1,2,3,4,5$:\n- $(1,2,4.0)$\n- $(2,3,3.0)$\n- $(3,4,2.5)$\n- $(4,5,5.0)$\n- $(2,4,1.0)$\n- $(3,5,0.5)$\n\nTest suite:\n1. Happy path case with zero internal injections. Use shunts $[y_1,y_2,y_3,y_4,y_5] = [0.5,0.4,0.3,0.2,0.5]$ siemens, boundary bus voltages $V_b = [100.0,0.0]$ volts, and internal injections $I_i = [0.0,0.0,0.0]$ amperes. Compute the boundary currents predicted by the reduced model and by the full network with internal voltages eliminated under $I_i = 0$. Output the absolute infinity-norm of the mismatch in amperes as a floating-point number.\n2. General case with nonzero internal injections. Use shunts $[y_1,y_2,y_3,y_4,y_5] = [0.2,0.2,0.2,0.2,0.2]$ siemens, boundary bus voltages $V_b = [230.0,115.0]$ volts, and internal injections $I_i = [1.0,-0.5,0.2]$ amperes. Compute the boundary currents by eliminating internal voltages from the full network equations and by the derived reduced mapping that accounts for internal injections. Output the absolute infinity-norm of the mismatch in amperes as a floating-point number.\n3. Edge case testing numerical stability and reciprocity. Use shunts $[y_1,y_2,y_3,y_4,y_5] = [0.2,10^{-6},10^{-6},0.2,0.2]$ siemens, boundary bus voltages $V_b = [10.0,-10.0]$ volts, and internal injections $I_i = [0.0,0.0,0.0]$ amperes. Construct the reduced boundary admittance and check if it is symmetric within a tolerance of $10^{-9}$, i.e., whether $\\max_{i,j} |(Y_{\\mathrm{red}})_{ij} - (Y_{\\mathrm{red}})_{ji}| \\le 10^{-9}$. Output a boolean indicating whether symmetry holds.\n\nAngle units do not apply because the network is resistive and scalar-valued. All voltages must be in volts and all currents must be in amperes.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_1,result\\_2,result\\_3]$). For test cases $1$ and $2$, the results are floating-point numbers in amperes; for test case $3$, the result is a boolean.",
            "solution": "We start from the fundamental laws governing linear resistive networks. Ohm's law states that for a linear conductance connecting two nodes, the current is proportional to the voltage difference across the conductance. Kirchhoff's Current Law (KCL) states that the algebraic sum of currents at each node is zero when accounting for injections and flows.\n\nFor an $N$-bus resistive network, we define the nodal admittance matrix $Y \\in \\mathbb{R}^{N \\times N}$ such that the bus current injection vector $I \\in \\mathbb{R}^{N}$ and the bus voltage vector $V \\in \\mathbb{R}^{N}$ satisfy the linear relation\n$$\nI = Y V.\n$$\nFor each undirected line $(p,q)$ with admittance $g \\ge 0$, the contribution to $Y$ is\n$$\nY_{pp} \\leftarrow Y_{pp} + g, \\quad Y_{qq} \\leftarrow Y_{qq} + g, \\quad Y_{pq} \\leftarrow Y_{pq} - g, \\quad Y_{qp} \\leftarrow Y_{qp} - g,\n$$\nwhich enforces that the line current equals $g (V_p - V_q)$ flowing from $p$ to $q$, consistent with Ohm's law. For each bus $k$ with shunt admittance $y_k \\ge 0$, add\n$$\nY_{kk} \\leftarrow Y_{kk} + y_k,\n$$\nrepresenting a conductance from node $k$ to ground with current $y_k V_k$. The resulting $Y$ is symmetric and positive semidefinite for a connected network with nonnegative admittances; with positive shunts or sufficient connectivity, $Y$ is typically nonsingular.\n\nWe partition the bus set into boundary buses $b$ and internal buses $i$. For the five-bus system, take $b = \\{1,5\\}$ and $i = \\{2,3,4\\}$. Reorder $V$ and $I$ so $V = \\begin{bmatrix} V_b \\\\ V_i \\end{bmatrix}$ and $I = \\begin{bmatrix} I_b \\\\ I_i \\end{bmatrix}$, and partition $Y$ compatibly as\n$$\nY = \\begin{bmatrix} Y_{bb} & Y_{bi} \\\\ Y_{ib} & Y_{ii} \\end{bmatrix}.\n$$\nThe nodal equations then read\n$$\n\\begin{aligned}\nI_b &= Y_{bb} V_b + Y_{bi} V_i, \\\\\nI_i &= Y_{ib} V_b + Y_{ii} V_i.\n\\end{aligned}\n$$\nKCL at internal buses dictates the second equation. To eliminate internal voltages $V_i$, we solve the internal equation for $V_i$:\n$$\nV_i = Y_{ii}^{-1} (I_i - Y_{ib} V_b),\n$$\nassuming $Y_{ii}$ is nonsingular, which holds for strictly passive connected networks with sufficient shunt or line conductance at internal buses. Substitute this into the boundary equation to obtain a reduced relation:\n$$\nI_b = \\left( Y_{bb} - Y_{bi} Y_{ii}^{-1} Y_{ib} \\right) V_b + Y_{bi} Y_{ii}^{-1} I_i.\n$$\nDefine the reduced boundary admittance\n$$\nY_{\\mathrm{red}} := Y_{bb} - Y_{bi} Y_{ii}^{-1} Y_{ib}.\n$$\nWhen the internal injections satisfy $I_i = 0$, the reduced model simplifies to\n$$\nI_b = Y_{\\mathrm{red}} V_b,\n$$\nwhich is the Kron-reduced map. For nonzero $I_i$, the reduced map includes the additive term $Y_{bi} Y_{ii}^{-1} I_i$, which represents the effect of internal sources seen at the boundary. This derivation relies solely on KCL, Ohm's law, and elimination of internal variables via linear algebra.\n\nAlgorithmic steps:\n- Construct $Y$ from the given line set and shunts using the add-and-subtract rules for admittances.\n- Partition $Y$ into $Y_{bb}$, $Y_{bi}$, $Y_{ib}$, $Y_{ii}$ according to the boundary and internal bus indices.\n- For a given $V_b$ and $I_i$, compute $V_i$ by solving $Y_{ii} V_i = I_i - Y_{ib} V_b$ using a linear solver, not an explicit inverse, for numerical stability.\n- Compute $I_b$ from the full network equations as $I_b^{\\mathrm{full}} = Y_{bb} V_b + Y_{bi} V_i$.\n- Compute $I_b$ from the reduced model as $I_b^{\\mathrm{red}} = Y_{\\mathrm{red}} V_b + Y_{bi} Y_{ii}^{-1} I_i$, implemented by solving $Y_{ii} X = I_i$ for $X$ and forming $Y_{bi} X$.\n- Report the absolute infinity-norm mismatch $\\| I_b^{\\mathrm{full}} - I_b^{\\mathrm{red}} \\|_{\\infty}$ for the first two test cases, in amperes. For the symmetry check, compute $Y_{\\mathrm{red}}$ and verify whether $\\max_{i,j} |(Y_{\\mathrm{red}})_{ij} - (Y_{\\mathrm{red}})_{ji}| \\le 10^{-9}$; output a boolean accordingly.\n\nScientific realism and numerical considerations:\n- The networks are connected, and admittances are nonnegative, ensuring physical plausibility.\n- Shunt values in the third case are very small on internal buses to stress numerical conditioning but remain positive to keep $Y_{ii}$ invertible; using a direct solver maintains stability.\n- Symmetry of $Y$ follows from reciprocal passive networks; the reduced $Y_{\\mathrm{red}}$ inherits symmetry because it is a Schur complement of a symmetric matrix. The tolerance of $10^{-9}$ accounts for finite precision arithmetic.\n\nThe program implements these steps for the three specified test cases, outputs the absolute mismatch values (in amperes) for the first two cases, and a boolean symmetry check for the third case, aggregated as a single line in the required format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_admittance_matrix(num_buses, lines, shunts):\n    \"\"\"\n    Build the nodal admittance matrix Y for a resistive network.\n    - num_buses: int, number of buses\n    - lines: list of tuples (p, q, g) with 1-based bus indices and admittance g in siemens\n    - shunts: array-like of length num_buses, shunt admittances in siemens\n    \"\"\"\n    Y = np.zeros((num_buses, num_buses), dtype=float)\n    for p, q, g in lines:\n        # Convert to 0-based indices\n        i = p - 1\n        j = q - 1\n        # Add line admittance contributions\n        Y[i, i] += g\n        Y[j, j] += g\n        Y[i, j] -= g\n        Y[j, i] -= g\n    # Add shunts\n    for k in range(num_buses):\n        Y[k, k] += shunts[k]\n    return Y\n\ndef partition_matrix(Y, b_indices, i_indices):\n    \"\"\"\n    Partition the admittance matrix Y into submatrices Y_bb, Y_bi, Y_ib, Y_ii\n    according to boundary indices b_indices and internal indices i_indices.\n    \"\"\"\n    Y_bb = Y[np.ix_(b_indices, b_indices)]\n    Y_bi = Y[np.ix_(b_indices, i_indices)]\n    Y_ib = Y[np.ix_(i_indices, b_indices)]\n    Y_ii = Y[np.ix_(i_indices, i_indices)]\n    return Y_bb, Y_bi, Y_ib, Y_ii\n\ndef kron_reduce(Y_bb, Y_bi, Y_ib, Y_ii):\n    \"\"\"\n    Compute the Kron-reduced boundary admittance matrix:\n    Y_red = Y_bb - Y_bi * inv(Y_ii) * Y_ib\n    Implemented with linear solves for numerical stability.\n    \"\"\"\n    # Solve Y_ii * X = Y_ib for X\n    X = np.linalg.solve(Y_ii, Y_ib)\n    return Y_bb - Y_bi @ X\n\ndef boundary_currents_full(Y_bb, Y_bi, Y_ib, Y_ii, V_b, I_i):\n    \"\"\"\n    Compute boundary currents using the full network by eliminating internal voltages:\n    V_i = solve(Y_ii, I_i - Y_ib * V_b)\n    I_b_full = Y_bb * V_b + Y_bi * V_i\n    \"\"\"\n    rhs = I_i - (Y_ib @ V_b)\n    V_i = np.linalg.solve(Y_ii, rhs)\n    I_b_full = Y_bb @ V_b + Y_bi @ V_i\n    return I_b_full\n\ndef boundary_currents_reduced(Y_red, Y_bi, Y_ii, V_b, I_i):\n    \"\"\"\n    Compute boundary currents using the reduced mapping with internal injections:\n    I_b_red = Y_red * V_b + Y_bi * inv(Y_ii) * I_i\n    Implemented with a linear solve.\n    \"\"\"\n    X = np.linalg.solve(Y_ii, I_i)\n    return Y_red @ V_b + Y_bi @ X\n\ndef solve():\n    # Common line set: (p, q, g) with buses indexed 1..5\n    lines = [\n        (1, 2, 4.0),\n        (2, 3, 3.0),\n        (3, 4, 2.5),\n        (4, 5, 5.0),\n        (2, 4, 1.0),\n        (3, 5, 0.5),\n    ]\n    num_buses = 5\n    # Boundary and internal indices in 0-based indexing\n    b_idx = [0, 4]       # buses {1, 5}\n    i_idx = [1, 2, 3]    # buses {2, 3, 4}\n\n    results = []\n\n    # Test case 1: Happy path, zero internal injections\n    shunts1 = np.array([0.5, 0.4, 0.3, 0.2, 0.5], dtype=float)\n    Y1 = build_admittance_matrix(num_buses, lines, shunts1)\n    Y_bb1, Y_bi1, Y_ib1, Y_ii1 = partition_matrix(Y1, b_idx, i_idx)\n    Y_red1 = kron_reduce(Y_bb1, Y_bi1, Y_ib1, Y_ii1)\n    V_b1 = np.array([100.0, 0.0], dtype=float)   # volts\n    I_i1 = np.array([0.0, 0.0, 0.0], dtype=float)  # amperes\n    I_b_full_1 = boundary_currents_full(Y_bb1, Y_bi1, Y_ib1, Y_ii1, V_b1, I_i1)\n    I_b_red_1 = Y_red1 @ V_b1  # since I_i = 0, reduced mapping simplifies\n    mismatch1 = np.linalg.norm(I_b_full_1 - I_b_red_1, ord=np.inf)\n    results.append(mismatch1)\n\n    # Test case 2: General case, nonzero internal injections\n    shunts2 = np.array([0.2, 0.2, 0.2, 0.2, 0.2], dtype=float)\n    Y2 = build_admittance_matrix(num_buses, lines, shunts2)\n    Y_bb2, Y_bi2, Y_ib2, Y_ii2 = partition_matrix(Y2, b_idx, i_idx)\n    Y_red2 = kron_reduce(Y_bb2, Y_bi2, Y_ib2, Y_ii2)\n    V_b2 = np.array([230.0, 115.0], dtype=float)  # volts\n    I_i2 = np.array([1.0, -0.5, 0.2], dtype=float)  # amperes\n    I_b_full_2 = boundary_currents_full(Y_bb2, Y_bi2, Y_ib2, Y_ii2, V_b2, I_i2)\n    I_b_red_2 = boundary_currents_reduced(Y_red2, Y_bi2, Y_ii2, V_b2, I_i2)\n    mismatch2 = np.linalg.norm(I_b_full_2 - I_b_red_2, ord=np.inf)\n    results.append(mismatch2)\n\n    # Test case 3: Edge case, near-singular internal shunts; symmetry check of Y_red\n    shunts3 = np.array([0.2, 1e-6, 1e-6, 0.2, 0.2], dtype=float)\n    Y3 = build_admittance_matrix(num_buses, lines, shunts3)\n    Y_bb3, Y_bi3, Y_ib3, Y_ii3 = partition_matrix(Y3, b_idx, i_idx)\n    Y_red3 = kron_reduce(Y_bb3, Y_bi3, Y_ib3, Y_ii3)\n    # Symmetry check within tolerance\n    tol = 1e-9\n    symmetric = np.max(np.abs(Y_red3 - Y_red3.T)) <= tol\n    results.append(symmetric)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}