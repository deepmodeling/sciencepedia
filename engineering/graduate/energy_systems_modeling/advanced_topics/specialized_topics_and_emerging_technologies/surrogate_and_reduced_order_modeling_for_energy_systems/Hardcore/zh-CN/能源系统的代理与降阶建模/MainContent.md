## 引言
在现代能源系统的设计、优化与控制中，高保真度的数学模型扮演着不可或缺的角色。这些模型，通常源于基本物理定律，能够精确地预测系统行为，但其求解过程往往伴随着巨大的计算开销。当面临参数研究、不确定性量化或实时控制等需要对模型进行成千上万次评估的“多查询”任务时，这种计算瓶颈变得尤为突出，严重制约了工程创新与决策效率。本文旨在系统性地介绍应对这一挑战的强大工具：代理模型（Surrogate Models）与降阶模型（Reduced-Order Models, ROMs）。通过学习本文，读者将能够理解这些高效近似方法的核心思想，并掌握其在能源领域的应用之道。

为了全面构建这一知识体系，本文分为三个核心部分。第一章，“原理与机制”，将从根本上剖析高保真模型的计算瓶颈，并系统介绍两大技术分支——[基于投影的降阶模型](@entry_id:1130226)（如POD-Galerkin）与数据驱动及[物理信息](@entry_id:152556)融合的代理模型（如高斯过程回归和PINN）的数学原理与构建方法。第二章，“应用与跨学科连接”，将通过一系列真实案例，展示这些技术如何用于加速物理仿真、赋能系统设计、实现[数字孪生](@entry_id:171650)中的[实时控制](@entry_id:754131)，并连接能源、气候、材料等不同学科。最后，在“动手实践”部分，我们将通过具体的编程练习，将理论知识转化为解决实际问题的能力。现在，让我们首先深入探讨这些强大建模技术背后的基本原理与核心机制。

## 原理与机制

本章旨在深入探讨代理模型与[降阶模型](@entry_id:754172)构建背后的核心科学原理与关键技术机制。我们将从高保真模型面临的计算瓶颈出发，阐明为何需要这些高效的近似方法。随后，我们将系统地剖析两大主流技术分支：[基于投影的降阶模型](@entry_id:1130226)（ROMs）与数据驱动及物理信息融合的代理模型。对于每一类方法，我们将从其数学基础出发，详细解释其构建过程、内在机理、适用场景及其优缺点。

### 计算瓶颈：高保真建模的需求与挑战

能源系统的精确建模对于设计、优化和控制至关重要。这些模型通常源于质量、动量和能量守恒等基本物理定律，表现为一组复杂的[偏微分](@entry_id:194612)方程（PDEs）或[微分代数方程](@entry_id:748394)（DAEs）。为了进行数值求解，这些连续方程必须在空间和时间上进行离散化，例如通过有限元法、[有限体积法](@entry_id:141374)或有限差分法。这个过程将无限维的连续问题转化为一个高维的离散系统，我们称之为**[全阶模型](@entry_id:171001)（Full-Order Model, FOM）**。

一个典型的FOM可以表示为如下形式的[常微分方程组](@entry_id:907499)：
$$
\dot{\mathbf{x}} = \mathbf{f}(\mathbf{x}, \mathbf{p}, t)
$$
其中，$\mathbf{x} \in \mathbb{R}^n$ 是系统的状态向量，其维度 $n$（即自由度）可能非常巨大，轻松达到数百万甚至更高，代表了空间离散网格上所有点的温度、压力等物理量。$\mathbf{p} \in \mathbb{R}^k$ 则是模型的输入和参数，如边界条件、材料属性或控制信号。

虽然FOM具有最高的物理保真度，但其巨大的计算成本是其应用于“多查询”任务时的主要障碍。这些任务包括不确定性量化（UQ）、[实时优化](@entry_id:169327)、参数研究和[控制系统设计](@entry_id:273663)，它们无一例外地需要对模型进行成千上万次甚至更多的重复评估。

为了具体理解这一计算负担，我们来分析一个典型的[隐式时间积分](@entry_id:171761)求解过程的计算复杂度 。假设我们使用一个[隐式求解器](@entry_id:140315)（如[后向差分](@entry_id:1121313)格式, BDF）来求解一个包含 $n$ 个自由度的能源系统模型，总共需要模拟 $T$ 个时间步。在每个时间步，我们都需要求解一个非线性方程组，这通常通过牛顿法完成，平均需要 $m$ 次迭代。在单次牛顿迭代中，主要的计算开销包括：
1.  **残差组装**：计算[非线性](@entry_id:637147)函数 $\mathbf{f}$ 的当前值，对于局部耦合的物理问题（如[热传导](@entry_id:143509)），其计算成本与自由度数量成线性关系，即 $\mathcal{O}(n)$。
2.  **[雅可比矩阵](@entry_id:178326)组装**：计算系统状态的导数矩阵，其成本通常也为 $\mathcal{O}(n)$。
3.  **线性系统求解**：求解[牛顿法](@entry_id:140116)产生的线性方程组，这是最昂贵的一步。其成本表示为 $\mathcal{O}(n^{\gamma})$，其中 $\gamma$ 的取值依赖于[雅可比矩阵](@entry_id:178326)的[稀疏性](@entry_id:136793)和所用求解器的类型。对于[稀疏直接求解器](@entry_id:755097)，$\gamma$ 可能接近 $1.5$，而对于[稠密矩阵](@entry_id:174457)，$\gamma$ 可能高达 $3$。

因此，整个模拟的总运行时间 $R_{\text{total}}$ 可以估算为：
$$
R_{\text{total}} = T \times m \times (c_{R} n + c_{J} n + c_{S} n^{\gamma})
$$
其中 $c_{R}, c_{J}, c_{S}$ 是相应计算环节的比例常数。由于 $\gamma > 1$，$n^{\gamma}$ 项在 $n$ 很大时占主导地位，总运行时间的[标度律](@entry_id:266186)为 $\mathcal{O}(T n^{\gamma})$。

举一个具体的例子，考虑一个中等规模的能源系统模型，其自由度 $n = 10^6$，需要模拟 $T = 5000$ 个时间步。假设每次牛顿迭代 $m = 4$ 次，[线性求解器](@entry_id:751329)的标度因子 $\gamma = 1.5$。即使比例常数非常小（例如，$c_J = 8 \times 10^{-9}$, $c_R = 4 \times 10^{-9}$, $c_S = 5 \times 10^{-9}$ 秒/操作），总运行时间也会超过 $100,000$ 秒，即超过一天。对于需要数千次模型评估的优化任务，这将导致数年的计算时间，这在工程实践中是完全不可接受的。

这一巨大的计算鸿沟正是驱动代理模型与[降阶模型](@entry_id:754172)发展的根本动力。这些方法的共同目标是在保证可接[受精](@entry_id:274949)度的前提下，将单次评估的计算成本降低数个数量级，从而使多查询任务成为可能。

### 模型近似方法分类

为了应对上述挑战，研究人员发展了多种[模型简化](@entry_id:171175)技术，它们可以大致分为两类：**[基于投影的降阶模型](@entry_id:1130226)（Projection-based Reduced-Order Models, ROMs）**和**数据驱动的代理模型（Data-driven Surrogate Models）**。这两类方法与高保真的**[全阶模型](@entry_id:171001)（FOM）**在保真度、可解释性和计算成本上形成了鲜明的对比 。

- **[全阶模型](@entry_id:171001) (FOM)**：直接由物理守恒定律离散化得到，具有最高的物理结构保真度。然而，其计算成本与系统自由度 $n$ 高度相关（通常为超线性），使其在多查询场景下不切实际。

- **[降阶模型 (ROM)](@entry_id:1130750)**：这类方法的核心思想是将高维的[状态空间](@entry_id:160914)投影到一个精心选择的低维子空间上，从而将大规模的微分方程组转化为一个小得多的系统。其状态维度 $r \ll n$。由于ROMs通常保留了[原始方程](@entry_id:1130162)的结构（通过投影），它们可以继承FOMs的某些物理属性（如能量守恒），并且其[状态变量](@entry_id:138790)通常与主要的物理模态相关联，因此具有较好的**[可解释性](@entry_id:637759)**。当 $r$ 足够小时，ROMs的在线计算速度可以比FOMs快上几个数量级。

- **代理模型 (Surrogate Model)**：这类模型通常将复杂的FOM视为一个“黑箱”，致力于学习其输入与输出（或感兴趣量，QoI）之间的映射关系。它们完全基于从FOM运行中收集的数据进行训练，而不直接使用控制方程。典型的代理模型包括[高斯过程回归](@entry_id:276025)、多项式混沌展开和神经网络。代理模型的在线评估速度极快，通常只是一个[函数调用](@entry_id:753765)。然而，它们的保真度完全依赖于训练数据的质量和覆盖范围，对于训练域之外的输入（外插）可能会产生严重错误。此外，纯粹的数据驱动模型通常是“黑箱”，缺乏物理解释性，并且不自动保证物理守恒定律。

下文将对这两大类方法的核心原理和机制进行详细的阐述。

### [基于投影的降阶模型](@entry_id:1130226) (ROMs)

[基于投影的降阶模型](@entry_id:1130226)旨在通过保留系统最重要的动态特性来降低模型的维度。其基本流程包括两个关键步骤：首先，识别一个能够有效表示系统状态演化的低维子空间；其次，将原始的控制方程投影到这个子空间上，得到一个低维的动力系统。

#### 核心思想：[加权残差法](@entry_id:140285)与投影

让我们以一个线性的时不变（LTI）系统为例来阐明其核心思想 ：
$$
\dot{x}(t) = A x(t) + B u(t)
$$
其中 $x(t) \in \mathbb{R}^{n}$ 是高维状态。我们假设系统的状态可以由一组基向量的线性组合来近似，这组基[向量张成](@entry_id:152883)了一个低维的**试探子空间 (trial subspace)**。我们将这组[基向量](@entry_id:199546)的列排成一个矩阵 $\mathbf{V} \in \mathbb{R}^{n \times r}$，其中 $r \ll n$。因此，状态的近似表示为：
$$
x(t) \approx \mathbf{V} z(t)
$$
其中 $z(t) \in \mathbb{R}^{r}$ 是新的低维[坐标向量](@entry_id:153319)。

将这个近似代入原方程，会产生一个**残差 (residual)** $\mathbf{R}(t)$，因为它通常不满足原方程：
$$
\mathbf{R}(t) = \mathbf{V} \dot{z}(t) - A (\mathbf{V} z(t)) - B u(t) \neq \mathbf{0}
$$
**[加权残差法](@entry_id:140285) (Method of Weighted Residuals)** 的思想是，虽然我们不能让残差在所有方向上都为零，但我们可以要求它在某个**测试子空间 (test subspace)** 上是正交的。这个测试子空间由另一个基矩阵 $\mathbf{W} \in \mathbb{R}^{n \times r}$ 的列[向量张成](@entry_id:152883)。正交条件可以写为：
$$
\mathbf{W}^{\top} \mathbf{R}(t) = \mathbf{0}
$$
将残差表达式代入，我们得到：
$$
\mathbf{W}^{\top} (\mathbf{V} \dot{z}(t) - A \mathbf{V} z(t) - B u(t)) = \mathbf{0}
$$
整理后得到关于低维坐标 $z(t)$ 的动力学方程：
$$
(\mathbf{W}^{\top} \mathbf{V}) \dot{z}(t) = (\mathbf{W}^{\top} A \mathbf{V}) z(t) + (\mathbf{W}^{\top} B) u(t)
$$
如果矩阵 $(\mathbf{W}^{\top} \mathbf{V})$ 是可逆的，我们就可以得到一个标准的[降阶模型](@entry_id:754172)：
$$
\dot{z}(t) = A_r z(t) + B_r u(t)
$$
其中降阶矩阵为 $A_r = (\mathbf{W}^{\top} \mathbf{V})^{-1} (\mathbf{W}^{\top} A \mathbf{V})$ 和 $B_r = (\mathbf{W}^{\top} \mathbf{V})^{-1} (\mathbf{W}^{\top} B)$。

#### Galerkin与Petrov-Galerkin投影

根据测试基 $\mathbf{W}$ 和试探基 $\mathbf{V}$ 的选择，投影方法分为两种主要类型 ：

- **Galerkin投影**：这是最常见的情况，其中测试子空间与试探子空间相同，即 $\mathbf{W} = \mathbf{V}$。这种方法要求残差与近似状态所在的子空间正交。如果 $\mathbf{V}$ 的列是[线性无关](@entry_id:148207)的（作为基的要求），那么[格拉姆矩阵](@entry_id:203297) $\mathbf{V}^{\top}\mathbf{V}$ 就是可逆的。在这种情况下，降阶系统变为：
  $$
  \dot{z} = (\mathbf{V}^{\top} \mathbf{V})^{-1} (\mathbf{V}^{\top} A \mathbf{V}) z + (\mathbf{V}^{\top} \mathbf{V})^{-1} (\mathbf{V}^{\top} B) u
  $$
  特别地，如果[基向量](@entry_id:199546) $\mathbf{V}$ 是标准正交的（即 $\mathbf{V}^{\top}\mathbf{V} = \mathbf{I}$），则方程简化为 $\dot{z} = (\mathbf{V}^{\top} A \mathbf{V}) z + (\mathbf{V}^{\top} B) u$。

- **Petrov-Galerkin投影**：这是一个更广泛的框架，允许测试子空间与试探子空间不同，即 $\mathbf{W} \neq \mathbf{V}$。这提供更大的灵活性，可以选择特定的 $\mathbf{W}$ 来保证降阶模型满足某些期望的性质，例如稳定性或最优性。只要 $\mathbf{W}^{\top}\mathbf{V}$ 可逆，就可以得到一个有效的降阶模型。这种[投影几何](@entry_id:156239)上对应于一个[斜投影](@entry_id:752867)（oblique projection），而Galerkin投影对应于[正交投影](@entry_id:144168)。

#### 子空间构建：本征正交分解 (POD)

投影方法的成功与否在很大程度上取决于试探子空间（即基 $\mathbf{V}$）的选择。一个好的子空间应该能够以尽可能低的维度“捕获”系统绝大部分的动态行为或“能量”。**[本征正交分解](@entry_id:165074) (Proper Orthogonal Decomposition, POD)**，在许多领域也被称为**主成分分析 (Principal Component Analysis, PCA)**，是寻找这种最优子空间的最常用方法之一。

POD的核心思想是从系统的“快照”数据中提取主导模式。具体步骤如下 ：
1.  **[数据采集](@entry_id:273490)**：通过运行高保真的[全阶模型](@entry_id:171001)，在多个时间点 $t_1, t_2, \dots, t_m$ 采集系统的[状态向量](@entry_id:154607)，得到一系列快照 $\{x(t_1), x(t_2), \dots, x(t_m)\}$。
2.  **数据中心化**：计算快照的平均值 $\bar{x} = \frac{1}{m} \sum_{k=1}^{m} x(t_k)$，然后将每个快照减去平均值，得到中心化的快照。
3.  **构造[快照矩阵](@entry_id:1131792)**：将这些中心化的快照向量作为列，构成一个[快照矩阵](@entry_id:1131792) $X \in \mathbb{R}^{n \times m}$。
4.  **寻找[最优基](@entry_id:752971)**：POD的目标是寻找一组[标准正交基](@entry_id:147779)向量 $\{\phi_i\}_{i=1}^r$，使得所有快照投影到这个基上后的方差之和最大。这等价于最小化投影误差。可以证明，这个最优的基 $\{\phi_i\}$ 就是[快照矩阵](@entry_id:1131792)的**[左奇异向量](@entry_id:751233)**，可以通过**奇异值分解 (Singular Value Decomposition, SVD)** 得到。

SVD将[快照矩阵](@entry_id:1131792)分解为 $X = U \Sigma V^{\top}$，其中：
- $U \in \mathbb{R}^{n \times n}$ 是一个[正交矩阵](@entry_id:169220)，其列向量 $u_i$ 称为[左奇异向量](@entry_id:751233)。这些向量构成了POD基，代表了数据中的主要**空间模式**。
- $\Sigma \in \mathbb{R}^{n \times m}$ 是一个[对角矩阵](@entry_id:637782)，其对角线上的元素 $\sigma_i \ge 0$ 称为奇异值。奇异值的大小反映了[对应模](@entry_id:200367)式的重要性。$\sigma_i^2$ 与第 $i$ 个主成分的方差成正比。
- $V \in \mathbb{R}^{m \times m}$ 是一个[正交矩阵](@entry_id:169220)，其列向量 $v_i$ 称为[右奇异向量](@entry_id:754365)，代表了相应空间模式随时间变化的**时间模式**。

PCA与POD在[欧几里得范数](@entry_id:172687)下是等价的：PCA寻找的是样本协方差矩阵 $C = \frac{1}{m-1} X X^{\top}$ 的[特征向量](@entry_id:151813)，而这些[特征向量](@entry_id:151813)恰好就是SVD分解中的[左奇异向量](@entry_id:751233) $U$ 的列。

通过SVD，我们可以根据[奇异值](@entry_id:152907)的大小来选择最重要的 $r$ 个模式（即 $U$ 的前 $r$ 列）作为我们的降阶基 $\mathbf{V}$。截断的[奇异值](@entry_id:152907)之和与总奇异值之和的比率 $\left(\sum_{i=1}^{r} \sigma_{i}^{2}\right) / \left(\sum_{i=1}^{\min(n,m)} \sigma_{i}^{2}\right)$，量化了前 $r$ 个[基向量](@entry_id:199546)捕获的“能量”或“方差”的比例，为选择[降阶模型](@entry_id:754172)的维度 $r$ 提供了依据。

在实践中，当状态维度 $n$ 远大于快照数量 $m$ 时（$n \gg m$），直接计算 $n \times n$ 的协方差矩阵 $XX^\top$ 是不现实的。**[快照法](@entry_id:168045) (method of snapshots)** 提供了一种高效的替代方案：通过求解 $m \times m$ 的小规模特征值问题 $X^{\top}X v_i = \lambda_i v_i$，可以得到[右奇异向量](@entry_id:754365) $v_i$，然后通过关系式 $u_i = X v_i / \sigma_i$ 来恢复出POD基（[左奇异向量](@entry_id:751233)）。

#### [非线性](@entry_id:637147)挑战：超降阶

对于[非线性系统](@entry_id:168347)，即使我们已经构建了[降阶模型](@entry_id:754172) $\dot{z} = A_r z + \mathbf{V}^{\top} \mathbf{f}(\mathbf{V}z, \dots)$，在线评估时仍可能存在一个计算瓶颈。问题出在[非线性](@entry_id:637147)项 $\mathbf{f}(\mathbf{V}z)$ 的计算上 。为了计算它，我们必须首先通过 $\hat{x} = \mathbf{V}z$ 将低维坐标 $z$ “解压”回高维空间（成本 $\mathcal{O}(nr)$），然后在高维空间中评估[非线性](@entry_id:637147)函数 $\mathbf{f}(\hat{x})$（成本 $\mathcal{O}(n)$），最后再投影回低维空间（成本 $\mathcal{O}(nr)$）。当 $n$ 极大时，$\mathcal{O}(n)$ 的成本使得整个降阶模型的优势大打折扣。

**超降阶 (Hyper-reduction)** 是一类旨在解决这个问题的技术，其核心思想是避免计算完整的 $n$ 维[非线性](@entry_id:637147)向量 $\mathbf{f}$，而是通过只计算其中一小部分分量来近似整个向量。

**[离散经验插值法](@entry_id:748503) (Discrete Empirical Interpolation Method, DEIM)** 是一种流行的超降阶方法。其工作原理如下：
1.  **离线阶段**：首先，我们像为状态 $x$ 做POD一样，为**[非线性](@entry_id:637147)项** $\mathbf{f}$ 本身收集快照，并用POD找到一组基 $U \in \mathbb{R}^{N \times m}$，使得 $\mathbf{f}(x) \approx U c(x)$。
2.  **插值点选择**：DEIM通过一个[贪心算法](@entry_id:260925)，从 $N$ 个分量中选出 $m$ 个“经验插值点”。这些点的选择旨在保证后续插值问题的良定性。
3.  **在线阶段**：在求解时，我们只计算[非线性](@entry_id:637147)项 $\mathbf{f}(\mathbf{V}z)$ 在这 $m$ 个插值点上的值。然后，通过求解一个 $m \times m$ 的小[线性系统](@entry_id:147850)，确定系数向量 $c(z)$，并用 $U c(z)$ 来重构整个[非线性](@entry_id:637147)向量的近似。

通过DEIM，[非线性](@entry_id:637147)项的计算成本从 $\mathcal{O}(N)$ 降低到只与插值点数量 $m$（通常与 $r$ 相当）相关的 $\mathcal{O}(m)$，从而极大地加速了[非线性](@entry_id:637147)ROM的在线求解。

**Gappy POD** 是另一种相关技术，它不是通过插值而是通过[最小二乘拟合](@entry_id:751226)来从稀疏采样点重构[非线性](@entry_id:637147)向量。它同样需要一个POD基 $U$ 和一组采样点，但在给定采样值后，它求解一个小型[最小二乘问题](@entry_id:164198)来找到最优的系数 $c(x)$。当采样数据可能包含噪声时，Gappy POD通常比DEIM更鲁棒。

#### 实现层面：侵入式与非侵入式ROMs

根据实现方式的不同，ROMs可以被分为**侵入式 (intrusive)** 和**非侵入式 (non-intrusive)** 。

- **侵入式ROM**：我们上面讨论的基于Galerkin或Petrov-Galerkin投影的方法是典型的侵入式方法。它们的实现需要深入到现有的高保真求解器代码中，以获取并操作系统的内部算子，如质量矩阵 $\mathbf{M}$、刚度矩阵 $\mathbf{A}$ 以及计算[非线性](@entry_id:637147)项 $\mathbf{f}$ 的子程序。例如，为了构建降阶矩阵 $\mathbf{V}^{\top} A \mathbf{V}$，我们必须能够[访问矩阵](@entry_id:746217) $A$。同样，实现DEIM等超降阶技术也需要修改求解器内部的[非线性](@entry_id:637147)函数评估部分。这种侵入性虽然可以获得高效且结构保持的ROM，但实现起来比较复杂，且通常与特定的软件代码绑定。

- **非侵入式ROM**：这类方法将高保真求解器视为一个“黑箱”，只通过其标准的输入输出接口来与之交互。例如，可以运行多次求解器得到状态快照，然后使用POD构建基 $\mathbf{V}$。之后，不去投影控制方程，而是通过机器学习方法（如神经网络或动态模式分解）直接学习低维坐标 $z$ 随时间演化的规律。这种方法避免了修改代码，但可能难以保证ROM的稳定性和物理一致性。

#### 物理结构保持：稳定性与[无源性](@entry_id:171773)

对于模拟物理系统（尤其是能量系统）的ROM而言，仅仅在数值上逼近FOM的输出是不够的。一个可靠的ROM还必须能反映底层物理学的基本性质，如能量守恒、耗散和稳定性。如果降阶过程破坏了这些结构，可能会导致模型产生非物理的行为，例如凭空产生能量，或者在FOM本应稳定的地方变得不稳定。

**[无源性](@entry_id:171773) (Passivity)** 是描述能量系统行为的一个核心概念 。一个系统是无源的，意味着它不能凭空产生能量。更形式化地，对于一个输入为 $\mathbf{u}$、输出为 $\mathbf{y}$ 的系统，如果存在一个非负的**[储能函数](@entry_id:197811)** $S(\mathbf{x}) \ge 0$（代表系统内部存储的能量），使得对于所有时间 $t \ge 0$，以下不等式成立：
$$
S(\mathbf{x}(t)) - S(\mathbf{x}(0)) \le \int_0^t \mathbf{u}(\tau)^{\top} \mathbf{y}(\tau) \mathrm{d}\tau
$$
这个不等式表明，系统内部能量的增加量不会超过从外界通过输入-输出端口注入的总能量。右侧的积分项 $\mathbf{u}^{\top}\mathbf{y}$ 代表瞬时输入功率。

许多能量系统可以用**端口-哈密顿 (port-Hamiltonian)** 形式来描述，这种形式明确地揭示了系统的能量结构：
$$
\dot{\mathbf{x}} = (\mathbf{J} - \mathbf{R}) \nabla H(\mathbf{x}) + \mathbf{G}\mathbf{u}, \quad \mathbf{y} = \mathbf{G}^{\top} \nabla H(\mathbf{x})
$$
其中 $H(\mathbf{x})$ 是总能量（[哈密顿量](@entry_id:144286)），$\mathbf{J} = -\mathbf{J}^{\top}$ 是一个[反对称矩阵](@entry_id:155998)，描述了系统内部各能量存储单元之间的能量交换（不产生或消耗能量），$\mathbf{R} = \mathbf{R}^{\top} \succeq \mathbf{0}$ 是一个半正定对称矩阵，描述了能量的耗散（如摩擦、电阻热），$\mathbf{G}$ 是端口矩阵，描述了系统与外界的能量交互。

在这种结构下，当没有外部输入（$\mathbf{u}=\mathbf{0}$）时，能量的时间变化率为 $\frac{\mathrm{d}H}{\mathrm{d}t} = -\nabla H(\mathbf{x})^{\top} \mathbf{R} \nabla H(\mathbf{x}) \le 0$。这表明系统能量永不增加，保证了李雅普诺夫意义下的**稳定性**。如果[耗散矩阵](@entry_id:1123862) $\mathbf{R}$ 是严格正定的，那么能量会持续减少直至达到平衡点，系统是[渐近稳定](@entry_id:168077)的。

**结构保持降阶 (Structure-preserving reduction)** 的目标是通过精巧地选择投影（例如，特定的[Petrov-Galerkin](@entry_id:174072)投影），使得降阶后的模型仍然保持端口-哈密顿结构。这意味着降阶后的矩阵 $\mathbf{J}_r, \mathbf{R}_r$ 仍然分别是反对称和半正定的。这样的ROM能够自动继承原系统的[无源性](@entry_id:171773)和稳定性，从而生成物理上可靠且鲁棒的预测。相反，一个“天真”的Galerkin投影可能会破坏这些精细的矩阵结构，导致降阶模型不稳定，即使FOM本身是稳定的。因此，对于长期、可靠的仿真，保持物理结构至关重要。

### 数据驱动与物理信息融合的代理模型

与ROMs试图简化控制方程不同，代理模型旨在直接学习FOM的输入-输出行为。这类方法种类繁多，从纯粹的“黑箱”[数据拟合](@entry_id:149007)到深度融合物理知识的“灰箱”模型。

#### 黑箱与灰箱范式

- **黑箱模型 (Black-box)**：这是最纯粹的数据驱动方法。它将FOM视为一个完全不透明的函数，通过大量输入-输出样本对 $\{(z_i, y_i)\}$ 来训练一个[通用函数逼近器](@entry_id:637737)（如神经网络），以学习映射 $z \mapsto y$ 。这种方法的优点是通用性强，实现相对简单，不需了解FOM的内部机理。缺点是需要大量的训练数据，且模型的行为完全由数据决定，缺乏物理可解释性，也无法保证物理约束。

- **[灰箱模型](@entry_id:1125766) (Gray-box)**：[灰箱模型](@entry_id:1125766)介于黑箱与白箱（即FOM）之间 。它利用已知的物理结构，但允许模型中的某些参数或部分组件从数据中学习。例如，我们可能知道一个[热力学](@entry_id:172368)模型的基本方程形式，但其中的某些[热导](@entry_id:189019)率或传热系数 $\boldsymbol{\theta}$ 未知。灰箱方法就是利用实验数据来标定这些参数 $\boldsymbol{\theta}$。这种方法比黑箱模型需要的数据更少，且具有更好的物理基础和外插能力。

#### 高斯过程回归 (Kriging)

**[高斯过程回归](@entry_id:276025) (Gaussian Process Regression, GPR)**，在[地质统计学](@entry_id:749879)中也称为**克里金法 (Kriging)**，是一种强大而灵活的非参数贝叶斯方法，非常适合构建代理模型 。

GPR的核心思想是假设任何一组输入点对应的函数输出值都服从一个[联合高斯](@entry_id:636452)分布。一个高斯过程（GP）由一个**[均值函数](@entry_id:264860)** $\mu(x)$ 和一个**[协方差函数](@entry_id:265031)（或核函数）** $k(x,x')$ 完全定义。[均值函数](@entry_id:264860) $\mu(x)$ 代表了我们对函数在点 $x$ 处输出的先验猜测，而核函数 $k(x,x')$ 则描述了函数在不同点 $x$ 和 $x'$ 处输出值的相关性。核函数的选择反映了我们对[函数平滑](@entry_id:201048)性等性质的假设。

假设我们有 $n$ 组带有噪声的观测数据 $\{(x_i, y_i)\}_{i=1}^n$，其中 $y_i = f(x_i) + \varepsilon_i$，观测噪声 $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$。GPR利用[贝叶斯定理](@entry_id:897366)，将这些观测数据与GP先验结合，来计算在任何新测试点 $x_*$ 处的函数值 $f(x_*)$ 的**后验分布**。

由于所有分布都是高斯分布，这个[后验分布](@entry_id:145605)也是一个高斯分布，其均值和方差有解析解。后验均值 $\mu_{post}(x_*)$ 给出了对 $f(x_*)$ 的最优预测，而后验方差 $\sigma^2_{post}(x_*)$ 则量化了该预测的不确定性。其表达式为：
- **[后验均值](@entry_id:173826)**：$\mu_{post}(x_*) = \mu(x_*) + k_*^{\top} (K + \sigma^2 I)^{-1} (y - \mu(X))$
- **后验方差**：$\sigma^2_{post}(x_*) = k_{**} - k_*^{\top} (K + \sigma^2 I)^{-1} k_*$

其中，$K$ 是训练点之间的[协方差矩阵](@entry_id:139155)（$K_{ij} = k(x_i, x_j)$），$k_*$ 是测试点与各训练点之间的协方差向量，而 $k_{**}$ 是测试点自身的协方差。

GPR的一个显著优点是它不仅提供预测值，还自然地提供了预测的置信区间。这种不确定性量化能力在工程设计和[主动学习](@entry_id:157812)中非常宝贵。从另一个角度看，GPR的预测均值等价于**最佳线性无偏预测器 (Best Linear Unbiased Predictor, BLUP)**，它在所有线性无偏预测器中具有最小的均方误差。

#### 多项式混沌展开 (PCE)

当代理模型的主要目标是进行**[不确定性量化 (UQ)](@entry_id:756296)** 时，**[多项式混沌展开](@entry_id:162793) (Polynomial Chaos Expansion, PCE)** 是一种特别有效的方法 。PCE的核心思想是将模型的随机输出 $Y=f(X_1, \dots, X_d)$ 表示为关于随机输入 $X_i$ 的正交多项式级数。

任何平方可积的[随机变量](@entry_id:195330)都属于一个[希尔伯特空间](@entry_id:261193)，其[内积](@entry_id:750660)由关于输入联合概率测度的期望定义，即 $\langle g_1, g_2 \rangle = \mathbb{E}[g_1 g_2]$。PCE利用了这个空间结构，将输出 $Y$ 展开为一组多变量多项式基 $\{\Psi_j(X)\}$ 的[线性组合](@entry_id:154743)：
$$
Y = f(X) = \sum_{j=0}^{\infty} c_j \Psi_j(X)
$$
关键在于，这组多项式基是**正交的**，即对于 $j \neq k$，$\langle \Psi_j, \Psi_k \rangle = 0$。

这种正交性使得展开系数 $c_j$ 的计算变得非常简单，可以通过[正交投影](@entry_id:144168)得到：
$$
c_j = \frac{\langle Y, \Psi_j \rangle}{\langle \Psi_j, \Psi_j \rangle} = \frac{\mathbb{E}[Y \Psi_j(X)]}{\mathbb{E}[\Psi_j^2(X)]}
$$
当输入变量 $X_i$ 是[相互独立](@entry_id:273670)时，多变量正交多项式基可以通过[张量积](@entry_id:140694)的方式由单变量正交多项式构成。根据**Wiener-[Askey方案](@entry_id:187960)**，最优的多项式族由输入变量的概率分布决定：
- 对于**[标准正态分布](@entry_id:184509)**的输入，应使用**[Hermite多项式](@entry_id:153594)**。
- 对于**均匀分布**的输入，应使用**[Legendre多项式](@entry_id:141510)**。
- 对于**Gamma分布**的输入，应使用**[Laguerre多项式](@entry_id:200702)**。

一旦获得了PCE展开，计算输出 $Y$ 的[统计矩](@entry_id:268545)（如均值和方差）就变得轻而易举，它们可以直接由展开系数 $c_j$ 计算得出，无需进行昂贵的[蒙特卡洛](@entry_id:144354)抽样。例如，均值为 $\mathbb{E}[Y] = c_0$，方差为 $\text{Var}(Y) = \sum_{j=1}^{\infty} c_j^2 \mathbb{E}[\Psi_j^2]$。

#### 深度学习代理模型：FFNN与PINN

近年来，深度神经网络（DNNs）已成为构建代理模型的强大工具。

**[前馈神经网络](@entry_id:635871) (Feedforward Neural Network, FFNN)** 是最直接的应用方式 。它作为一个通用的[非线性](@entry_id:637147)[函数逼近](@entry_id:141329)器，被训练来学习从模型输入 $z=(u, \theta)$ 到输出 $y=J[T]$ 的黑箱映射。训练过程需要一个由多次高保真FOM求解生成的标记数据集 $\{(z_i, y_i)\}$。[损失函数](@entry_id:634569)通常是预测值与真实值之间的[均方误差](@entry_id:175403)，通过反向传播和[梯度下降](@entry_id:145942)算法进行优化。FFNN的优势在于其强大的[表达能力](@entry_id:149863)，但它需要大量数据，且不包含任何先验的物理知识。

**物理信息神经网络 (Physics-Informed Neural Network, PINN)** 则代表了一种将物理定律硬编码到神经网络训练过程中的“灰箱”方法 。对于一个由[PDE控制](@entry_id:165441)的系统，如[热传导](@entry_id:143509)问题：
$$
\rho c \partial_t T - \nabla \cdot (k \nabla T) = q
$$
PINN不学习输入到输出的映射，而是让神经网络 $\hat{T}_\phi(\mathbf{x}, t)$ 直接去逼近解场本身 $T(\mathbf{x}, t)$。其巧妙之处在于损失函数的设计。PINN的[损失函数](@entry_id:634569)是一个复合项，包括：
1.  **物理残差损失**：在时空域内选取一组[配置点](@entry_id:169000)（collocation points），计算神经网络在这些点上对PDE的满足程度，即PDE残差的范数。例如，$\| \rho c \partial_t \hat{T}_\phi - \nabla \cdot (k \nabla \hat{T}_\phi) - q \|^2$。
2.  **边界条件损失**：惩罚神经网络在边界上与规定边界条件的偏差。
3.  **初始条件损失**：惩罚神经网络在初始时刻与规定初始条件的偏差。
4.  **数据损失（可选）**：如果有一些稀疏的测量数据，也可以加入[损失函数](@entry_id:634569)中。

PINN的一个关键技术是**自动微分 (Automatic Differentiation, AD)**。AD能够精确计算神经网络输出对其输入的任何阶导数（如 $\partial_t \hat{T}_\phi, \nabla \hat{T}_\phi$），从而可以精确地构造物理残差项，而无需使用有限差分等会引入离散化误差的近似方法。

由于物理定律本身提供了监督信号，PINN原则上可以在没有任何来自FOM的解数据的情况下进行训练，这在获取数据极其昂贵的场景下是一个巨大的优势。为了成功训练PINN，通常需要采取一些技巧，如对变量进行无量纲化和归一化以改善优化问题的条件数，以及在训练过程中自适应地调整各部分损失的权重，以平衡不同约束的学习进度。