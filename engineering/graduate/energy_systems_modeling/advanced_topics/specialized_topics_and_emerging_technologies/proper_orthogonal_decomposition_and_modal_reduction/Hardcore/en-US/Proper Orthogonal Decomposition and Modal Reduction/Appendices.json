{
    "hands_on_practices": [
        {
            "introduction": "Before applying Proper Orthogonal Decomposition, a crucial decision involves data preparation. Should the analysis be performed on the raw simulation snapshots or on the fluctuations around a steady-state or mean value? This choice fundamentally alters the problem from finding modes that capture the total energy to finding modes that capture the variance, a concept central to the connection between POD and Principal Component Analysis (PCA). This practice  challenges you to explore the precise mathematical relationship between these two approaches and to understand the conditions under which the dominant modes change, providing deep insight into the foundation of data-centered modeling.",
            "id": "4114824",
            "problem": "An energy system model produces a high-dimensional state vector $x(t) \\in \\mathbb{R}^{n}$, representing, for example, nodal voltages and branch currents in a transmission network. A collection of $m$ snapshots $\\{x_{k}\\}_{k=1}^{m}$ is obtained at sampling instants $\\{t_{k}\\}_{k=1}^{m}$. Proper Orthogonal Decomposition (POD) seeks a set of spatial modes that optimally capture, in a least-squares sense, the energy of the data under the Euclidean inner product. Let the sample mean be $\\bar{x} = \\frac{1}{m} \\sum_{k=1}^{m} x_{k}$. Consider two ways to construct the correlation operator used for POD:\n\n1. Using raw snapshots (second moment): $C_{\\mathrm{raw}} = \\frac{1}{m} \\sum_{k=1}^{m} x_{k} x_{k}^{\\top}$.\n2. Using fluctuations about a constant reference state $x_{\\mathrm{ref}} \\in \\mathbb{R}^{n}$: define $y_{k} = x_{k} - x_{\\mathrm{ref}}$, and $C_{\\mathrm{fluc}} = \\frac{1}{m} \\sum_{k=1}^{m} y_{k} y_{k}^{\\top}$.\n\nPOD modes are given by the eigenvectors of the chosen correlation operator. From first principles, derive the precise relationship between $C_{\\mathrm{raw}}$ and $C_{\\mathrm{fluc}}$ when $x_{\\mathrm{ref}}$ is constant, and explain the distinction between computing correlation using raw snapshots versus fluctuations. Then, reason about the effect of replacing $C_{\\mathrm{raw}}$ by $C_{\\mathrm{fluc}}$ on the dominant POD modes (eigenvectors associated with the largest eigenvalues). Identify the condition(s) under which the dominant POD modes remain invariant and the condition(s) under which they change, without invoking any shortcut formulas beyond the definitions provided.\n\nSelect the single option that correctly captures both the distinction and the invariance/change conditions.\n\nA. Subtracting any constant reference state $x_{\\mathrm{ref}}$ leaves both the correlation operator and the dominant POD modes unchanged, because centering does not affect eigenvectors when $m$ is large.\n\nB. The difference between $C_{\\mathrm{raw}}$ and $C_{\\mathrm{fluc}}$ is a scalar multiple of the identity matrix, so the eigenvectors of the two operators are identical and the dominant POD modes are always invariant, independent of $x_{\\mathrm{ref}}$.\n\nC. When $x_{\\mathrm{ref}} = \\bar{x}$, $C_{\\mathrm{fluc}}$ equals $C_{\\mathrm{raw}}$ minus a rank-one outer product. The dominant POD modes remain invariant if and only if the removed direction $\\bar{x}$ aligns exactly with an eigenvector of $C_{\\mathrm{raw}}$ and the eigenvalue ordering is preserved; otherwise, the dominant modes rotate and may reorder.\n\nD. Dominant POD modes necessarily change whenever one subtracts any constant reference state $x_{\\mathrm{ref}}$, because covariance and second moment never share eigenvectors.",
            "solution": "The problem statement is a valid exercise in linear algebra and its application to Proper Orthogonal Decomposition (POD). It is scientifically grounded, well-posed, and objective. All necessary definitions are provided, and the question is answerable from first principles.\n\nLet us begin by deriving the relationship between the two correlation operators, $C_{\\mathrm{raw}}$ and $C_{\\mathrm{fluc}}$.\n\nThe definitions provided are:\nState vector: $x(t) \\in \\mathbb{R}^{n}$\nSnapshots: $\\{x_{k}\\}_{k=1}^{m}$\nSample mean: $\\bar{x} = \\frac{1}{m} \\sum_{k=1}^{m} x_{k}$\nRaw correlation operator (second moment matrix):\n$$C_{\\mathrm{raw}} = \\frac{1}{m} \\sum_{k=1}^{m} x_{k} x_{k}^{\\top}$$\nFluctuation vectors: $y_{k} = x_{k} - x_{\\mathrm{ref}}$ for a constant reference state $x_{\\mathrm{ref}} \\in \\mathbb{R}^{n}$.\nFluctuation correlation operator:\n$$C_{\\mathrm{fluc}} = \\frac{1}{m} \\sum_{k=1}^{m} y_{k} y_{k}^{\\top}$$\n\nWe substitute the definition of $y_{k}$ into the expression for $C_{\\mathrm{fluc}}$:\n$$C_{\\mathrm{fluc}} = \\frac{1}{m} \\sum_{k=1}^{m} (x_{k} - x_{\\mathrm{ref}})(x_{k} - x_{\\mathrm{ref}})^{\\top}$$\nExpanding the outer product inside the summation:\n$$C_{\\mathrm{fluc}} = \\frac{1}{m} \\sum_{k=1}^{m} (x_{k}x_{k}^{\\top} - x_{k}x_{\\mathrm{ref}}^{\\top} - x_{\\mathrm{ref}}x_{k}^{\\top} + x_{\\mathrm{ref}}x_{\\mathrm{ref}}^{\\top})$$\nWe can distribute the summation and the factor $\\frac{1}{m}$:\n$$C_{\\mathrm{fluc}} = \\left(\\frac{1}{m} \\sum_{k=1}^{m} x_{k}x_{k}^{\\top}\\right) - \\left(\\frac{1}{m} \\sum_{k=1}^{m} x_{k}x_{\\mathrm{ref}}^{\\top}\\right) - \\left(\\frac{1}{m} \\sum_{k=1}^{m} x_{\\mathrm{ref}}x_{k}^{\\top}\\right) + \\left(\\frac{1}{m} \\sum_{k=1}^{m} x_{\\mathrm{ref}}x_{\\mathrm{ref}}^{\\top}\\right)$$\nLet's evaluate each term.\nThe first term is the definition of $C_{\\mathrm{raw}}$.\nFor the second term, since $x_{\\mathrm{ref}}$ is a constant vector, we can factor it out of the summation:\n$$\\frac{1}{m} \\sum_{k=1}^{m} x_{k}x_{\\mathrm{ref}}^{\\top} = \\left(\\frac{1}{m} \\sum_{k=1}^{m} x_{k}\\right) x_{\\mathrm{ref}}^{\\top} = \\bar{x} x_{\\mathrm{ref}}^{\\top}$$\nFor the third term, similarly:\n$$\\frac{1}{m} \\sum_{k=1}^{m} x_{\\mathrm{ref}}x_{k}^{\\top} = x_{\\mathrm{ref}} \\left(\\frac{1}{m} \\sum_{k=1}^{m} x_{k}^{\\top}\\right) = x_{\\mathrm{ref}} \\left(\\frac{1}{m} \\sum_{k=1}^{m} x_{k}\\right)^{\\top} = x_{\\mathrm{ref}} \\bar{x}^{\\top}$$\nFor the fourth term, the summand is a constant matrix:\n$$\\frac{1}{m} \\sum_{k=1}^{m} x_{\\mathrm{ref}}x_{\\mathrm{ref}}^{\\top} = \\frac{1}{m} (m \\cdot x_{\\mathrm{ref}}x_{\\mathrm{ref}}^{\\top}) = x_{\\mathrm{ref}}x_{\\mathrm{ref}}^{\\top}$$\nCombining these results, we get the general relationship:\n$$C_{\\mathrm{fluc}} = C_{\\mathrm{raw}} - \\bar{x} x_{\\mathrm{ref}}^{\\top} - x_{\\mathrm{ref}} \\bar{x}^{\\top} + x_{\\mathrm{ref}}x_{\\mathrm{ref}}^{\\top}$$\nA common and important special case in data analysis (where POD is equivalent to Principal Component Analysis or PCA) is to choose the reference state to be the sample mean, i.e., $x_{\\mathrm{ref}} = \\bar{x}$. Let's investigate this case, as it is central to option C.\nIf $x_{\\mathrm{ref}} = \\bar{x}$:\n$$C_{\\mathrm{fluc}} = C_{\\mathrm{raw}} - \\bar{x} \\bar{x}^{\\top} - \\bar{x} \\bar{x}^{\\top} + \\bar{x} \\bar{x}^{\\top}$$\n$$C_{\\mathrm{fluc}} = C_{\\mathrm{raw}} - \\bar{x} \\bar{x}^{\\top}$$\nIn this case, $C_{\\mathrm{fluc}}$ is the sample covariance matrix. The relationship shows that the covariance matrix is the second moment matrix ($C_{\\mathrm{raw}}$) minus the rank-one matrix formed by the outer product of the mean vector with itself.\n\nThe conceptual distinction is that the POD modes from $C_{\\mathrm{raw}}$ find directions that maximize the \"energy\" or second moment of the raw data, which is variance with respect to the origin. The modes from $C_{\\mathrm{fluc}}$ (with $x_{\\mathrm{ref}} = \\bar{x}$) find directions of maximum variance with respect to the data's own center of mass, $\\bar{x}$. If $\\|\\bar{x}\\|$ is large compared to the fluctuations around it, the first mode of $C_{\\mathrm{raw}}$ will be strongly aligned with the direction of $\\bar{x}$, whereas the modes of $C_{\\mathrm{fluc}}$ will be insensitive to the mean and will capture the directions of greatest variability.\n\nNow, we analyze the effect on the POD modes (eigenvectors). Let $v$ be an eigenvector of $C_{\\mathrm{raw}}$ with corresponding eigenvalue $\\lambda$.\n$$C_{\\mathrm{raw}}v = \\lambda v$$\nWe examine the action of $C_{\\mathrm{fluc}}$ (with $x_{\\mathrm{ref}}=\\bar{x}$) on this eigenvector $v$:\n$$C_{\\mathrm{fluc}}v = (C_{\\mathrm{raw}} - \\bar{x}\\bar{x}^{\\top})v = C_{\\mathrm{raw}}v - \\bar{x}(\\bar{x}^{\\top}v)$$\n$$C_{\\mathrm{fluc}}v = \\lambda v - (\\bar{x}^{\\top}v)\\bar{x}$$\nFor $v$ to also be an eigenvector of $C_{\\mathrm{fluc}}$, the right-hand side must be a scalar multiple of $v$. This leads to two scenarios:\n\n1.  The vector $\\bar{x}$ is orthogonal to the eigenvector $v$: $\\bar{x}^{\\top}v = 0$.\n    In this case, the equation simplifies to $C_{\\mathrm{fluc}}v = \\lambda v$. Thus, $v$ is also an eigenvector of $C_{\\mathrm{fluc}}$ with the *same* eigenvalue $\\lambda$.\n\n2.  The vector $\\bar{x}$ is parallel to the eigenvector $v$: $\\bar{x} = c v$ for some scalar $c$. This means $\\bar{x}$ itself is an eigenvector of $C_{\\mathrm{raw}}$.\n    In this case, $v$ is aligned with $\\bar{x}$.\n    $C_{\\mathrm{fluc}}v = \\lambda v - (\\bar{x}^{\\top}v)\\bar{x}$. Since $v$ and $\\bar{x}$ are parallel, $(\\bar{x}^{\\top}v)\\bar{x}$ is also parallel to $v$. Let's assume $v$ is normalized, $v = \\bar{x}/\\|\\bar{x}\\|$. Then $\\bar{x}^\\top v = \\|\\bar{x}\\|$.\n    $C_{\\mathrm{fluc}}v = \\lambda v - (\\|\\bar{x}\\|) (\\|\\bar{x}\\|v) = (\\lambda - \\|\\bar{x}\\|^2) v$.\n    So, $v$ is also an eigenvector of $C_{\\mathrm{fluc}}$, but its eigenvalue is now $\\lambda - \\|\\bar{x}\\|^2$.\n\nFrom this, we deduce that the set of eigenvectors of $C_{\\mathrm{raw}}$ and $C_{\\mathrm{fluc}}$ are identical if and only if every eigenvector of $C_{\\mathrm{raw}}$ is either parallel or orthogonal to $\\bar{x}$. Since $C_{\\mathrm{raw}}$ is a real symmetric matrix, it has an orthonormal basis of eigenvectors. This condition is met if and only if $\\bar{x}$ is aligned with one of these eigenvectors. If $\\bar{x}$ is not an eigenvector of $C_{\\mathrm{raw}}$, then for a general eigenvector $v$, the term $(\\bar{x}^{\\top}v)\\bar{x}$ will point in a different direction from $v$, and $C_{\\mathrm{fluc}}v$ will be a linear combination of $v$ and $\\bar{x}$, meaning the original eigenvectors are \"rotated\" into new ones.\n\nTherefore, for the dominant POD modes (which are eigenvectors) to remain invariant, a necessary condition is that $\\bar{x}$ must be an eigenvector of $C_{\\mathrm{raw}}$.\nHowever, this is not sufficient. The dominant modes are those associated with the largest eigenvalues. If $\\bar{x}$ aligns with the dominant eigenvector $v_1$ of $C_{\\mathrm{raw}}$ (with eigenvalue $\\lambda_1$), the new eigenvalue is $\\mu_1 = \\lambda_1 - \\|\\bar{x}\\|^2$. If the second largest eigenvalue $\\lambda_2$ of $C_{\\mathrm{raw}}$ is such that $\\lambda_2 > \\mu_1$, the dominant mode of $C_{\\mathrm{fluc}}$ will become $v_2$, not $v_1$. The original dominant mode has changed. Thus, for the dominant modes to remain invariant, the eigenvalue ordering must also be preserved after the subtraction.\n\nIn summary, for $x_{\\mathrm{ref}}=\\bar{x}$, the dominant POD modes remain invariant if and only if:\n(i) The mean vector $\\bar{x}$ is an eigenvector of $C_{\\mathrm{raw}}$. (This ensures the eigenvectors don't rotate).\n(ii) The eigenvalue shift caused by subtracting $\\|\\bar{x}\\|^2$ from the eigenvalue corresponding to the direction of $\\bar{x}$ does not alter the ordering of the largest eigenvalues. (This ensures the dominant eigenvectors remain dominant).\n\nIf condition (i) fails, the modes rotate. If condition (i) holds but (ii) fails, the modes reorder.\n\nNow, we evaluate the given options:\n\n**A. Subtracting any constant reference state $x_{\\mathrm{ref}}$ leaves both the correlation operator and the dominant POD modes unchanged, because centering does not affect eigenvectors when $m$ is large.**\nThis is incorrect. As derived, $C_{\\mathrm{fluc}} \\neq C_{\\mathrm{raw}}$ in general. The eigenvectors are also not guaranteed to be unchanged. The large $m$ argument is irrelevant to the algebraic structure.\n\n**B. The difference between $C_{\\mathrm{raw}}$ and $C_{\\mathrm{fluc}}$ is a scalar multiple of the identity matrix, so the eigenvectors of the two operators are identical and the dominant POD modes are always invariant, independent of $x_{\\mathrm{ref}}$.**\nThis is incorrect. The difference, $C_{\\mathrm{raw}} - C_{\\mathrm{fluc}}$, is generally not a multiple of the identity matrix. For $x_{\\mathrm{ref}}=\\bar{x}$, it is $\\bar{x}\\bar{x}^{\\top}$, a rank-one matrix, which is not proportional to the identity matrix unless $n=1$. Therefore, the premise is false, and the conclusion that eigenvectors are always identical is also false.\n\n**C. When $x_{\\mathrm{ref}} = \\bar{x}$, $C_{\\mathrm{fluc}}$ equals $C_{\\mathrm{raw}}$ minus a rank-one outer product. The dominant POD modes remain invariant if and only if the removed direction $\\bar{x}$ aligns exactly with an eigenvector of $C_{\\mathrm{raw}}$ and the eigenvalue ordering is preserved; otherwise, the dominant modes rotate and may reorder.**\nThis statement is fully consistent with our derivation.\n- It correctly identifies the relationship for the standard case $x_{\\mathrm{ref}} = \\bar{x}$: $C_{\\mathrm{fluc}} = C_{\\mathrm{raw}} - \\bar{x}\\bar{x}^{\\top}$, where $\\bar{x}\\bar{x}^{\\top}$ is a rank-one outer product.\n- It correctly states the two necessary and sufficient conditions for the invariance of the dominant modes: alignment of $\\bar{x}$ with an eigenvector of $C_{\\mathrm{raw}}$ and preservation of the eigenvalue ordering.\n- It correctly notes that if these conditions are not met, the modes will either rotate (if $\\bar{x}$ is not an eigenvector) or reorder (if $\\bar{x}$ is an eigenvector but the eigenvalue ordering changes).\n\n**D. Dominant POD modes necessarily change whenever one subtracts any constant reference state $x_{\\mathrm{ref}}$, because covariance and second moment never share eigenvectors.**\nThis is incorrect. It makes an overly strong, false claim. We have shown explicit conditions under which the dominant modes *do* remain invariant. A simple counterexample is when the data is already mean-centered, i.e., $\\bar{x} = 0$. In this case, $C_{\\mathrm{fluc}} = C_{\\mathrm{raw}}$ and nothing changes. The reason given, that covariance ($C_{\\mathrm{fluc}}$) and the second moment ($C_{\\mathrm{raw}}$) \"never share eigenvectors,\" is also false, as we derived the exact conditions under which they do.\n\nTherefore, option C is the only one that correctly and precisely describes the situation.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "The power of POD lies in its ability to provide a highly efficient basis, but its practical effectiveness hinges on selecting the right number of modes, $r$. A model with too few modes will be inaccurate, while one with too many will fail to provide significant computational savings. This exercise  guides you through deriving the standard \"energy criterion\" for choosing $r$, which links the singular values directly to the amount of data variance captured. You will also analyze the critical trade-offs that arise when a system's energy is distributed across many modes, a common challenge in modeling complex energy systems.",
            "id": "4114865",
            "problem": "An energy systems model for a transmission network is simulated over a time window to collect $p$ state snapshots. Let the state dimension be $n$, and assemble the snapshot matrix $X \\in \\mathbb{R}^{n \\times p}$ whose columns are the centered snapshots. Consider Proper Orthogonal Decomposition (POD) under the Euclidean inner product, and let the singular value decomposition of $X$ be $X = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{q} \\ge 0$, where $q = \\min(n,p)$. Let $U_{r} \\in \\mathbb{R}^{n \\times r}$ denote the first $r$ left singular vectors. The rank-$r$ POD reconstruction of the snapshots is $X_{r} = U_{r} U_{r}^{\\top} X$. Define the total snapshot energy to be the squared Frobenius norm $\\lVert X \\rVert_{F}^{2}$, and the captured energy to be $\\lVert X_{r} \\rVert_{F}^{2}$.\n\nA model reduction requirement is to meet a target captured energy fraction $\\eta^{\\ast} \\in (0,1)$ with the smallest number of modes. Starting from the definitions of the singular value decomposition, the Frobenius norm, and orthogonal projection, derive an explicit criterion in terms of the singular values that selects $r$ as the smallest index meeting the target energy fraction. Then, reason about the implications when the singular values decay slowly, in terms of projection error and computational trade-offs for reduced-order modeling in energy systems.\n\nWhich option both states a correct explicit selection criterion for $r$ and correctly characterizes the trade-offs under slow singular value decay?\n\nA. Choose $r$ as $r = \\min \\{ k \\in \\{1,\\dots,q\\} : \\sum_{i=1}^{k} \\sigma_{i}^{2} / \\sum_{i=1}^{q} \\sigma_{i}^{2} \\ge \\eta^{\\ast} \\}$, since $\\lVert X_{r} \\rVert_{F}^{2} = \\sum_{i=1}^{r} \\sigma_{i}^{2}$ and $\\lVert X \\rVert_{F}^{2} = \\sum_{i=1}^{q} \\sigma_{i}^{2}$. If $\\{\\sigma_{i}\\}$ decays slowly, then many modes contribute materially to the energy, so $r$ must be large to meet $\\eta^{\\ast}$. This increases offline costs (e.g., singular value decomposition on a larger retained subspace), online costs (e.g., reduced-order simulation scales with $r$), memory, and possibly stiffness/conditioning of the reduced dynamics, while providing diminishing accuracy gains per additional mode; alternative responses include relaxing $\\eta^{\\ast}$, goal-oriented POD, hyper-reduction, or reweighting snapshots.\n\nB. Choose $r$ as $r = \\min \\{ k : \\sum_{i=1}^{k} \\sigma_{i} / \\sum_{i=1}^{q} \\sigma_{i} \\ge \\eta^{\\ast} \\}$, since energy is the sum of singular values, not their squares. If $\\{\\sigma_{i}\\}$ decays slowly, the threshold is reached with a small $r$, ensuring both high accuracy and aggressive reduction without trade-offs.\n\nC. Choose $r$ by requiring $\\sigma_{r} / \\sigma_{1} \\ge \\eta^{\\ast}$, because the relative magnitude of the $r$-th singular value controls the projection error. If $\\{\\sigma_{i}\\}$ decays slowly, $\\sigma_{r}$ remains close to $\\sigma_{1}$ for moderate $r$, so one can take small $r$ without sacrificing accuracy, avoiding computational overhead.\n\nD. Choose $r$ at the first index where a large relative gap appears, i.e., $\\sigma_{r} / \\sigma_{r+1} \\ge \\tau$ for a fixed $\\tau > 1$, because the spectral gap identifies the intrinsic dimensionality; the energy fraction is not required. Under slow decay, there is no clear gap, so $r$ is ambiguous, but using any small $r$ is acceptable because the gap heuristic supersedes the energy target.\n\nE. Replace the Euclidean inner product by an energy-weighted inner product with a mass matrix $M \\succ 0$, and choose $r = \\min \\{ k : \\sum_{i=1}^{k} \\lambda_{i} / \\sum_{i=1}^{q} \\lambda_{i} \\ge \\eta^{\\ast} \\}$, where $\\{ \\lambda_{i} \\}$ are eigenvalues of $X^{\\top} M X$. Slow decay of $\\{ \\sigma_{i} \\}$ is irrelevant because weighting reorders modes to meet any $\\eta^{\\ast}$ with small $r$, eliminating trade-offs.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, and objective, providing a complete and consistent setup for a standard problem in model order reduction using Proper Orthogonal Decomposition (POD).\n\nThe task is to derive an explicit criterion for selecting the reduced-order model dimension $r$ and to analyze the implications of a slow decay in singular values.\n\n**1. Derivation of the Selection Criterion for $r$**\n\nThe problem defines the total snapshot energy as $\\mathcal{E} = \\lVert X \\rVert_{F}^{2}$ and the captured energy in a rank-$r$ model as $\\mathcal{E}_{r} = \\lVert X_{r} \\rVert_{F}^{2}$. The requirement is to find the smallest integer $r$ such that the captured energy fraction meets a target $\\eta^{\\ast} \\in (0,1)$. This can be stated as:\n$$\n\\frac{\\mathcal{E}_{r}}{\\mathcal{E}} = \\frac{\\lVert X_{r} \\rVert_{F}^{2}}{\\lVert X \\rVert_{F}^{2}} \\ge \\eta^{\\ast}\n$$\n\nWe must express both the numerator and the denominator in terms of the singular values $\\{\\sigma_i\\}$ of the snapshot matrix $X$.\n\nThe Frobenius norm of a matrix $A$ is related to its singular values. The squared Frobenius norm is the sum of the squares of its singular values. Let's derive this from first principles. The squared Frobenius norm is the sum of squared entries, which is also the trace of $A^{\\top}A$: $\\lVert A \\rVert_{F}^{2} = \\mathrm{Tr}(A^{\\top}A)$.\n\nFor the total energy $\\mathcal{E}$, we use the snapshot matrix $X$ with its singular value decomposition (SVD) $X = U \\Sigma V^{\\top}$.\n$$\n\\mathcal{E} = \\lVert X \\rVert_{F}^{2} = \\mathrm{Tr}(X^{\\top}X) = \\mathrm{Tr}\\left( (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) \\right)\n$$\nUsing the property $(ABC)^{\\top} = C^{\\top}B^{\\top}A^{\\top}$:\n$$\n\\mathcal{E} = \\mathrm{Tr}(V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top})\n$$\nThe matrix $U$ consists of orthonormal columns, so $U^{\\top}U = I$, the identity matrix.\n$$\n\\mathcal{E} = \\mathrm{Tr}(V \\Sigma^{\\top} \\Sigma V^{\\top})\n$$\nUsing the cyclic property of the trace, $\\mathrm{Tr}(ABC) = \\mathrm{Tr}(CAB)$:\n$$\n\\mathcal{E} = \\mathrm{Tr}(\\Sigma^{\\top} \\Sigma V^{\\top}V)\n$$\nThe matrix $V$ also consists of orthonormal columns, so $V^{\\top}V = I$.\n$$\n\\mathcal{E} = \\mathrm{Tr}(\\Sigma^{\\top} \\Sigma)\n$$\nThe matrix $\\Sigma \\in \\mathbb{R}^{n \\times p}$ has the singular values $\\sigma_1, \\dots, \\sigma_q$ on its main diagonal, where $q = \\min(n,p)$. The matrix $\\Sigma^{\\top}\\Sigma$ is a $p \\times p$ diagonal matrix whose first $q$ diagonal entries are $\\sigma_1^2, \\dots, \\sigma_q^2$. The trace is the sum of the diagonal elements.\n$$\n\\mathcal{E} = \\lVert X \\rVert_{F}^{2} = \\sum_{i=1}^{q} \\sigma_{i}^{2}\n$$\n\nFor the captured energy $\\mathcal{E}_{r}$, we use the rank-$r$ POD reconstruction $X_{r} = U_{r} U_{r}^{\\top} X$. The matrix $P_r = U_r U_r^{\\top}$ is the orthogonal projector onto the subspace spanned by the first $r$ POD modes (the columns of $U_r$). By the Eckart-Young-Mirsky theorem, this projection $X_r$ is the best rank-$r$ approximation of $X$ in the Frobenius norm and is given by truncating the SVD of $X$:\n$$\nX_{r} = \\sum_{i=1}^{r} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\nwhere $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively. This matrix $X_r$ has an SVD given by $X_r = U_r \\Sigma_r V_r^{\\top}$, where $U_r \\in \\mathbb{R}^{n \\times r}$, $V_r \\in \\mathbb{R}^{p \\times r}$, and $\\Sigma_r = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_r)$. Applying the same derivation for the Frobenius norm:\n$$\n\\mathcal{E}_{r} = \\lVert X_{r} \\rVert_{F}^{2} = \\sum_{i=1}^{r} \\sigma_{i}^{2}\n$$\n\nCombining these results, the condition for selecting $r$ becomes:\n$$\n\\frac{\\sum_{i=1}^{r} \\sigma_{i}^{2}}{\\sum_{i=1}^{q} \\sigma_{i}^{2}} \\ge \\eta^{\\ast}\n$$\nSince the problem asks for the smallest number of modes, we select $r$ as the minimum integer $k$ that satisfies this inequality.\n$$\nr = \\min \\left\\{ k \\in \\{1, \\dots, q\\} : \\frac{\\sum_{i=1}^{k} \\sigma_{i}^{2}}{\\sum_{i=1}^{q} \\sigma_{i}^{2}} \\ge \\eta^{\\ast} \\right\\}\n$$\n\n**2. Implications of Slow Singular Value Decay**\n\nA slow decay of singular values means that $\\sigma_i$ decreases slowly as the index $i$ increases. This implies that there is no small set of dominant modes; instead, many modes make a non-negligible contribution to the total energy.\n\n-   **Impact on $r$**: To satisfy the energy criterion for a high target fraction $\\eta^{\\ast}$ (e.g., $\\eta^{\\ast} = 0.999$), the cumulative sum $\\sum_{i=1}^{k} \\sigma_{i}^{2}$ must capture a large portion of the total sum. If the terms $\\sigma_{i}^{2}$ decrease slowly, this cumulative sum will also grow slowly. Consequently, a large number of terms $k$ will be required, meaning the necessary POD rank $r$ will be large.\n\n-   **Computational Trade-offs**: The primary goal of model reduction is to reduce computational cost. A large $r$ works against this objective.\n    -   **Online Cost**: The cost of simulating the reduced-order model (ROM) is a function of $r$. For a system of ODEs, $\\dot{\\mathbf{y}} = \\mathbf{f}(\\mathbf{y})$, where $\\mathbf{y} \\in \\mathbb{R}^{r}$, the cost per time step is typically polynomial in $r$, e.g., $O(r^2)$ or $O(r^3)$. A large $r$ leads to expensive online simulations, diminishing the benefit of model reduction.\n    -   **Offline Cost**: The cost of constructing the ROM includes computing the SVD of the snapshot matrix $X \\in \\mathbb{R}^{n \\times p}$ and storing the basis $U_r \\in \\mathbb{R}^{n \\times r}$. While the SVD cost itself is often dominated by the dimensions $n$ and $p$, a large $r$ increases memory requirements for storing the basis $U_r$ and potentially for storing reduced-order operators, which can be dense tensors of size scaling with $r$.\n    -   **Accuracy vs. Complexity**: Slow decay implies a poor trade-off. Each additional mode (increasing $r$ by one) adds an amount of energy $\\sigma_{r+1}^2$ to the reconstruction. If this value is small, the accuracy gain is marginal, yet the computational cost increases. This represents a case of diminishing returns.\n    -   **System Properties**: A system with a slow-decaying singular value spectrum is often called \"less reducible.\" This can sometimes be associated with systems that have complex, multi-scale dynamics or chaotic behavior. Including modes with smaller singular values can also introduce stiffness into the reduced dynamical system, complicating its numerical integration.\n\n-   **Alternative Strategies**: When faced with slow decay, a simple energy-based POD may be insufficient. Standard practice involves considering other approaches, such as: relaxing the energy target $\\eta^{\\ast}$ (sacrificing accuracy for speed), using goal-oriented or weighted POD to focus accuracy on a specific quantity of interest, employing hyper-reduction techniques (like DEIM) to reduce the cost of evaluating nonlinear terms, or pre-processing the snapshots to emphasize certain dynamics.\n\n**3. Evaluation of Provided Options**\n\n**A. Choose $r$ as $r = \\min \\{ k \\in \\{1,\\dots,q\\} : \\sum_{i=1}^{k} \\sigma_{i}^{2} / \\sum_{i=1}^{q} \\sigma_{i}^{2} \\ge \\eta^{\\ast} \\}$, since $\\lVert X_{r} \\rVert_{F}^{2} = \\sum_{i=1}^{r} \\sigma_{i}^{2}$ and $\\lVert X \\rVert_{F}^{2} = \\sum_{i=1}^{q} \\sigma_{i}^{2}$. If $\\{\\sigma_{i}\\}$ decays slowly, then many modes contribute materially to the energy, so $r$ must be large to meet $\\eta^{\\ast}$. This increases offline costs (e.g., singular value decomposition on a larger retained subspace), online costs (e.g., reduced-order simulation scales with $r$), memory, and possibly stiffness/conditioning of the reduced dynamics, while providing diminishing accuracy gains per additional mode; alternative responses include relaxing $\\eta^{\\ast}$, goal-oriented POD, hyper-reduction, or reweighting snapshots.**\n-   This option presents the correct selection criterion for $r$, perfectly matching our derivation. The reasoning for the expressions for captured and total energy is also correct. The analysis of slow singular value decay is comprehensive and accurate, correctly identifying that a large $r$ is needed and detailing the resulting trade-offs in computational costs, memory, and accuracy. It also correctly lists relevant advanced strategies used in practice.\n-   **Verdict: Correct.**\n\n**B. Choose $r$ as $r = \\min \\{ k : \\sum_{i=1}^{k} \\sigma_{i} / \\sum_{i=1}^{q} \\sigma_{i} \\ge \\eta^{\\ast} \\}$, since energy is the sum of singular values, not their squares. If $\\{\\sigma_{i}\\}$ decays slowly, the threshold is reached with a small $r$, ensuring both high accuracy and aggressive reduction without trade-offs.**\n-   The selection criterion is incorrect. The problem defines energy via the squared Frobenius norm, which corresponds to the sum of *squared* singular values ($\\sum \\sigma_i^2$), not the sum of singular values ($\\sum \\sigma_i$). The subsequent analysis is also flawed: slow decay necessitates a *large* $r$, not a small one, and claiming there are \"no trade-offs\" is fundamentally incorrect in model reduction.\n-   **Verdict: Incorrect.**\n\n**C. Choose $r$ by requiring $\\sigma_{r} / \\sigma_{1} \\ge \\eta^{\\ast}$, because the relative magnitude of the $r$-th singular value controls the projection error. If $\\{\\sigma_{i}\\}$ decays slowly, $\\sigma_{r}$ remains close to $\\sigma_{1}$ for moderate $r$, so one can take small $r$ without sacrificing accuracy, avoiding computational overhead.**\n-   This proposes a different, heuristic criterion for choosing $r$, which does not satisfy the problem's explicit requirement of meeting a *cumulative energy fraction*. The analysis is also flawed; slow decay means the total error from omitted modes ($\\sum_{i=r+1}^q \\sigma_i^2$) is large for small $r$, so accuracy is indeed sacrificed.\n-   **Verdict: Incorrect.**\n\n**D. Choose $r$ at the first index where a large relative gap appears, i.e., $\\sigma_{r} / \\sigma_{r+1} \\ge \\tau$ for a fixed $\\tau > 1$, because the spectral gap identifies the intrinsic dimensionality; the energy fraction is not required. Under slow decay, there is no clear gap, so $r$ is ambiguous, but using any small $r$ is acceptable because the gap heuristic supersedes the energy target.**\n-   This is another heuristic that ignores the problem's explicit objective based on the energy fraction $\\eta^{\\ast}$. It incorrectly claims the energy fraction is not required. The conclusion that \"any small $r$ is acceptable\" is unsubstantiated and disregards the accuracy requirements of the modeling task.\n-   **Verdict: Incorrect.**\n\n**E. Replace the Euclidean inner product by an energy-weighted inner product with a mass matrix $M \\succ 0$, and choose $r = \\min \\{ k : \\sum_{i=1}^{k} \\lambda_{i} / \\sum_{i=1}^{q} \\lambda_{i} \\ge \\eta^{\\ast} \\}$, where $\\{ \\lambda_{i} \\}$ are eigenvalues of $X^{\\top} M X$. Slow decay of $\\{ \\sigma_{i} \\}$ is irrelevant because weighting reorders modes to meet any $\\eta^{\\ast}$ with small $r$, eliminating trade-offs.**\n-   This option changes the problem statement, which explicitly specifies the \"Euclidean inner product\". While weighted POD is a valid technique, it is not the subject of the question. Furthermore, the claim that weighting can \"meet any $\\eta^{\\ast}$ with small $r$, eliminating trade-offs\" is a gross oversimplification and generally false. A different weighting changes the definition of error and may or may not lead to a faster spectral decay; it never eliminates the fundamental trade-off between accuracy and complexity.\n-   **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "This practice  consolidates your understanding by guiding you through the complete end-to-end workflow of building and validating a reduced-order model. Starting from the governing partial differential equations of a gas pipeline, you will perform spatial discretization to create a full-order model (FOM), generate snapshot data from a simulation, and then apply POD with Galerkin projection to construct a compact ROM. By implementing the process from theory to code, you will gain hands-on experience with the practical steps and challenges involved in applying modal reduction to a tangible energy system model.",
            "id": "4114823",
            "problem": "Consider a one-dimensional, isothermal gas pipeline modeled on a periodic domain of length $L$ by the linearized conservation of mass and momentum equations, leading to the first-order system of partial differential equations (PDE): $\\frac{\\partial p}{\\partial t} = -a^2 \\frac{\\partial q}{\\partial x}$ and $\\frac{\\partial q}{\\partial t} = -\\frac{\\partial p}{\\partial x} - f q$, where $p$ denotes pressure deviation around a nominal operating point, $q$ denotes mass flow deviation, $a$ denotes wave speed, and $f$ denotes a small linearized friction coefficient. Discretize the spatial coordinate into $N$ uniformly spaced points with periodic boundary conditions, approximate the spatial derivative $\\frac{\\partial}{\\partial x}$ using a centered finite difference scheme, and form the semi-discrete linear state-space model $\\dot{x} = A x$ with $x = [p;q] \\in \\mathbb{R}^{2N}$ and a block matrix $A$ consistent with the discretization and the PDE structure, where the grid spacing is $\\Delta x = L / N$.\n\nUsing simulated transients of the semi-discrete model, compute Proper Orthogonal Decomposition (POD) modes that capture the dominant spatial-temporal structures of the state. Then, build a Reduced-Order Model (ROM) by a Galerkin projection of the full-order model onto the subspace spanned by the leading POD modes. Validate that the ROM reproduces pressure wave propagation with a specified accuracy by computing the space-time normalized root-mean-square error (NRMSE) of the pressure component between the full-order and reconstructed ROM trajectories. The NRMSE must be reported as a decimal fraction. No physical units are required for this problem; all quantities are dimensionless due to normalization around the nominal operating point.\n\nFundamental base to use: conservation of mass and momentum as stated above, spatial finite differences on a periodic grid, and the definition of POD as the set of orthonormal modes maximizing captured snapshot variance under an $\\ell_2$ inner product. The derivation must proceed from these principles, without introducing shortcut formulas that are not derived from the base.\n\nAlgorithmic requirements:\n- Construct the periodic centered finite difference operator $D \\in \\mathbb{R}^{N \\times N}$ such that for a vector $u \\in \\mathbb{R}^N$, $Du$ approximates $\\frac{\\partial u}{\\partial x}$ on the periodic grid.\n- Form the block matrix $A \\in \\mathbb{R}^{2N \\times 2N}$ consistent with the PDE structure.\n- Initialize a transient with a smooth Gaussian pressure bump and zero mass flow: $p_i(0) = \\exp\\left(-\\frac{(x_i - x_c)^2}{2 \\sigma^2}\\right)$ for grid points $x_i$, with $q_i(0) = 0$.\n- Simulate the full-order linear system using a time-stepping scheme derived from the matrix exponential of $A$ over a fixed time step $\\Delta t$, accumulating snapshots for all time steps from $t=0$ to $t=T$ inclusive.\n- Assemble the snapshot matrix $X \\in \\mathbb{R}^{2N \\times M}$ with columns equal to the concatenated state vectors at each time step, where $M$ is the number of snapshots.\n- Compute POD modes by performing a singular value decomposition of $X$ and extracting the first $r$ left singular vectors as the POD basis $\\mathbf{\\Phi}_r \\in \\mathbb{R}^{2N \\times r}$.\n- Project the full-order dynamics onto the POD subspace to obtain the ROM matrix $A_r = \\mathbf{\\Phi}_r^\\top A \\mathbf{\\Phi}_r \\in \\mathbb{R}^{r \\times r}$, and simulate the ROM using the same time-stepping scheme, with the ROM initial condition $x_r(0) = \\mathbf{\\Phi}_r^\\top x(0)$.\n- Reconstruct the ROM trajectory in the full space as $\\hat{x}(t) = \\mathbf{\\Phi}_r x_r(t)$, extract the pressure components $p(t)$ and $\\hat{p}(t)$, and compute the space-time NRMSE:\n$$\\mathrm{NRMSE} = \\sqrt{\\frac{\\sum_{k=0}^{M-1} \\| p(t_k) - \\hat{p}(t_k) \\|_2^2}{\\sum_{k=0}^{M-1} \\| p(t_k) \\|_2^2}}.$$\n- For each test case, return a boolean indicating whether $\\mathrm{NRMSE} \\leq \\varepsilon$, where $\\varepsilon$ is the specified tolerance.\n\nTest suite:\n- Case $1$: $N = 64$, $L = 1$, $a = 1$, $f = 0.02$, $T = 1.0$, $\\Delta t = 0.002$, $x_c = 0.5$, $\\sigma = 0.12$, $r = 6$, $\\varepsilon = 0.15$.\n- Case $2$: $N = 64$, $L = 1$, $a = 1$, $f = 0.05$, $T = 1.0$, $\\Delta t = 0.002$, $x_c = 0.5$, $\\sigma = 0.12$, $r = 2$, $\\varepsilon = 0.25$.\n- Case $3$: $N = 64$, $L = 1$, $a = 1$, $f = 0.0$, $T = 1.0$, $\\Delta t = 0.002$, $x_c = 0.5$, $\\sigma = 0.12$, $r = 1$, $\\varepsilon = 0.12$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3]$), where each entry is a boolean corresponding to each test case in the order specified above.",
            "solution": "The problem requires the development and validation of a reduced-order model (ROM) for a linearized one-dimensional gas pipeline model using Proper Orthogonal Decomposition (POD). The process involves spatial discretization of the governing partial differential equations (PDEs), simulation of the resulting full-order model (FOM), derivation of a POD basis, construction of the ROM via Galerkin projection, simulation of the ROM, and finally, validation by comparing the ROM's output to the FOM's output.\n\n### 1. Full-Order Model (FOM) Formulation\n\nThe physical system is described by a pair of linearized first-order PDEs on a periodic domain of length $L$:\n$$\n\\frac{\\partial p}{\\partial t} = -a^2 \\frac{\\partial q}{\\partial x} \\\\\n\\frac{\\partial q}{\\partial t} = -\\frac{\\partial p}{\\partial x} - f q\n$$\nHere, $p(x, t)$ is the pressure deviation, $q(x, t)$ is the mass flow deviation, $a$ is the wave speed, and $f$ is a friction coefficient.\n\nTo formulate a state-space model, we first discretize the spatial domain. We define $N$ uniformly spaced grid points $x_i = i \\Delta x$ for $i = 0, 1, \\dots, N-1$, where the grid spacing is $\\Delta x = L/N$. The state at any time $t$ is represented by vectors of pressure and mass flow at these grid points, denoted $\\mathbf{p}(t) \\in \\mathbb{R}^N$ and $\\mathbf{q}(t) \\in \\mathbb{R}^N$.\n\nThe spatial derivative $\\frac{\\partial}{\\partial x}$ is approximated using a second-order centered finite difference scheme on the periodic grid. For a function $u(x)$ evaluated at the grid points, its derivative at $x_i$ is approximated as:\n$$\n\\frac{\\partial u}{\\partial x} \\bigg|_{x_i} \\approx \\frac{u(x_{i+1}) - u(x_{i-1})}{2 \\Delta x}\n$$\nDue to periodicity, we enforce $u(x_N) = u(x_0)$ and $u(x_{-1}) = u(x_{N-1})$. This numerical differentiation can be expressed in matrix form as $D\\mathbf{u}$, where $D \\in \\mathbb{R}^{N \\times N}$ is the differentiation matrix. Its elements are given by:\n$$\nD_{i,j} =\n\\begin{cases}\n    1 / (2 \\Delta x) & \\text{if } j = (i+1) \\pmod{N} \\\\\n    -1 / (2 \\Delta x) & \\text{if } j = (i-1) \\pmod{N} \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\nThis results in a skew-symmetric circulant matrix, $D^T = -D$.\n\nApplying this discretization to the governing PDEs yields a system of ordinary differential equations (ODEs):\n$$\n\\frac{d\\mathbf{p}}{dt} = -a^2 D \\mathbf{q} \\\\\n\\frac{d\\mathbf{q}}{dt} = -D \\mathbf{p} - f I_N \\mathbf{q}\n$$\nwhere $I_N$ is the $N \\times N$ identity matrix.\n\nWe define the state vector $x \\in \\mathbb{R}^{2N}$ by concatenating the pressure and mass flow vectors: $x = [\\mathbf{p}^T, \\mathbf{q}^T]^T$. The system of ODEs can then be written in the standard linear state-space form $\\dot{x} = Ax$, where the state matrix $A \\in \\mathbb{R}^{2N \\times 2N}$ is a block matrix:\n$$\nA = \\begin{pmatrix}\n0_{N \\times N} & -a^2 D \\\\\n-D & -f I_N\n\\end{pmatrix}\n$$\nHere, $0_{N \\times N}$ is the $N \\times N$ zero matrix. This constitutes our full-order model (FOM).\n\n### 2. FOM Simulation and Snapshot Collection\n\nThe problem specifies an initial condition of a Gaussian pressure bump and zero mass flow:\n$$\np_i(0) = \\exp\\left(-\\frac{(x_i - x_c)^2}{2 \\sigma^2}\\right), \\quad q_i(0) = 0\n$$\nThe FOM is simulated over a time interval $[0, T]$ with a constant time step $\\Delta t$. The exact solution to the linear system $\\dot{x} = Ax$ is given by $x(t) = e^{At}x(0)$. We can use this to step forward in time:\n$$\nx(t_{k+1}) = e^{A \\Delta t} x(t_k)\n$$\nWe collect the state vectors at each time step $t_k = k \\Delta t$ for $k = 0, 1, \\dots, M-1$, where $M = T/\\Delta t + 1$ is the total number of snapshots. These snapshots are assembled as columns of a snapshot matrix $X$:\n$$\nX = [x(t_0), x(t_1), \\dots, x(t_{M-1})] \\in \\mathbb{R}^{2N \\times M}\n$$\n\n### 3. Model Reduction via Proper Orthogonal Decomposition (POD)\n\nThe goal of POD is to find a low-dimensional subspace that best captures the energy (variance) of the snapshot data. The optimal orthonormal basis for this subspace, in the $\\ell_2$ sense, is given by the leading left singular vectors of the snapshot matrix $X$. We compute the Singular Value Decomposition (SVD) of $X$:\n$$\nX = U \\Sigma V^T\n$$\nwhere $U \\in \\mathbb{R}^{2N \\times M}$ (in its economy form) has orthonormal columns (the POD modes), $\\Sigma \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix of singular values, and $V \\in \\mathbb{R}^{M \\times M}$ has orthonormal columns. The POD modes are the columns of $U$. The problem statement uses the notation $\\mathbf{\\Phi}_r$ for the reduced basis matrix. A reduced basis of rank $r$ is formed by taking the first $r$ columns of $U$:\n$$\n\\mathbf{\\Phi}_r = U[:, 0:r] \\in \\mathbb{R}^{2N \\times r}\n$$\n\n### 4. Reduced-Order Model (ROM) Formulation and Simulation\n\nA ROM is constructed by projecting the FOM onto the subspace spanned by the POD basis $\\mathbf{\\Phi}_r$. The state $x(t)$ is approximated by a linear combination of the basis vectors: $x(t) \\approx \\hat{x}(t) = \\mathbf{\\Phi}_r x_r(t)$, where $x_r(t) \\in \\mathbb{R}^r$ are the time-dependent modal coefficients. Substituting this approximation into the FOM equation and applying a Galerkin projection (projecting the residual onto the basis $\\mathbf{\\Phi}_r$) yields the ROM:\n$$\n\\mathbf{\\Phi}_r^T (\\mathbf{\\Phi}_r \\dot{x}_r) = \\mathbf{\\Phi}_r^T (A \\mathbf{\\Phi}_r x_r) \\implies \\dot{x}_r = (\\mathbf{\\Phi}_r^T A \\mathbf{\\Phi}_r) x_r\n$$\nThis gives the reduced-order linear system $\\dot{x}_r = A_r x_r$ with the ROM matrix $A_r = \\mathbf{\\Phi}_r^T A \\mathbf{\\Phi}_r \\in \\mathbb{R}^{r \\times r}$.\nThe initial condition for the ROM is obtained by projecting the full initial state: $x_r(0) = \\mathbf{\\Phi}_r^T x(0)$.\nThe ROM is then simulated using the same time-stepping scheme as the FOM:\n$$\nx_r(t_{k+1}) = e^{A_r \\Delta t} x_r(t_k)\n$$\n\n### 5. Validation and Error Quantification\n\nTo validate the ROM, we compare its solution to the FOM's solution. First, the ROM state trajectory $x_r(t_k)$ is reconstructed back into the full state space:\n$$\n\\hat{x}(t_k) = \\mathbf{\\Phi}_r x_r(t_k)\n$$\nThe comparison is performed using the space-time Normalized Root-Mean-Square Error (NRMSE) of the pressure component. The pressure vectors from the FOM and reconstructed ROM trajectories are $p(t_k) = x(t_k)[0:N]$ and $\\hat{p}(t_k) = \\hat{x}(t_k)[0:N]$, respectively. The NRMSE is defined as:\n$$\n\\mathrm{NRMSE} = \\sqrt{\\frac{\\sum_{k=0}^{M-1} \\| p(t_k) - \\hat{p}(t_k) \\|_2^2}{\\sum_{k=0}^{M-1} \\| p(t_k) \\|_2^2}}\n$$\nThis metric quantifies the relative error in the pressure solution over the entire simulation domain and time interval. The validation for each test case consists of checking if the computed $\\mathrm{NRMSE}$ is less than or equal to a given tolerance $\\varepsilon$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Solves the model reduction problem for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: N=64, L=1, a=1, f=0.02, T=1.0, dt=0.002, xc=0.5, sigma=0.12, r=6, eps=0.15\n        {'N': 64, 'L': 1.0, 'a': 1.0, 'f': 0.02, 'T': 1.0, 'dt': 0.002, 'xc': 0.5, 'sigma': 0.12, 'r': 6, 'epsilon': 0.15},\n        # Case 2: N=64, L=1, a=1, f=0.05, T=1.0, dt=0.002, xc=0.5, sigma=0.12, r=2, eps=0.25\n        {'N': 64, 'L': 1.0, 'a': 1.0, 'f': 0.05, 'T': 1.0, 'dt': 0.002, 'xc': 0.5, 'sigma': 0.12, 'r': 2, 'epsilon': 0.25},\n        # Case 3: N=64, L=1, a=1, f=0.0, T=1.0, dt=0.002, xc=0.5, sigma=0.12, r=1, eps=0.12\n        {'N': 64, 'L': 1.0, 'a': 1.0, 'f': 0.0, 'T': 1.0, 'dt': 0.002, 'xc': 0.5, 'sigma': 0.12, 'r': 1, 'epsilon': 0.12},\n    ]\n\n    results = []\n    for params in test_cases:\n        results.append(run_simulation_and_reduction(**params))\n\n    # Format the final output as a string list of booleans\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\ndef run_simulation_and_reduction(N, L, a, f, T, dt, xc, sigma, r, epsilon):\n    \"\"\"\n    Carries out the full process for a single test case: FOM simulation, POD, ROM simulation, and validation.\n    \"\"\"\n    # 1. Discretization and Model Setup\n    dx = L / N\n    x_grid = np.linspace(0, L, N, endpoint=False)\n    \n    # 2. Construct spatial differentiation matrix D for periodic BCs\n    D_super_diag = np.ones(N - 1)\n    D_sub_diag = -np.ones(N - 1)\n    D = (np.diag(D_super_diag, k=1) + np.diag(D_sub_diag, k=-1)) / (2 * dx)\n    D[0, N-1] = -1 / (2 * dx)\n    D[N-1, 0] = 1 / (2 * dx)\n\n    # 3. Construct the full-order state matrix A\n    A = np.block([\n        [np.zeros((N, N)), -a**2 * D],\n        [-D, -f * np.identity(N)]\n    ])\n\n    # 4. Initial Condition\n    p0 = np.exp(-(x_grid - xc)**2 / (2 * sigma**2))\n    q0 = np.zeros(N)\n    x0 = np.concatenate([p0, q0])\n\n    # 5. Full-Order Model (FOM) Simulation\n    num_steps = int(round(T / dt))\n    M = num_steps + 1\n    snapshot_matrix = np.zeros((2 * N, M))\n    snapshot_matrix[:, 0] = x0\n    \n    # Time-stepper matrix from matrix exponential\n    E_fom = expm(A * dt)\n    \n    x_current = x0\n    for k in range(1, M):\n        x_current = E_fom @ x_current\n        snapshot_matrix[:, k] = x_current\n\n    # 6. Proper Orthogonal Decomposition (POD)\n    # The economy SVD is sufficient and more efficient.\n    U, s, Vh = np.linalg.svd(snapshot_matrix, full_matrices=False)\n    # POD basis is the first r left singular vectors\n    Phi_r = U[:, :r]\n\n    # 7. Reduced-Order Model (ROM) Formulation and Simulation\n    Ar = Phi_r.T @ A @ Phi_r\n    xr0 = Phi_r.T @ x0\n    \n    # Time-stepper for ROM\n    E_rom = expm(Ar * dt)\n    \n    xr_snapshots = np.zeros((r, M))\n    xr_snapshots[:, 0] = xr0\n    \n    xr_current = xr0\n    for k in range(1, M):\n        xr_current = E_rom @ xr_current\n        xr_snapshots[:, k] = xr_current\n\n    # 8. Reconstruct ROM trajectory in full space\n    x_hat_snapshots = Phi_r @ xr_snapshots\n\n    # 9. Compute NRMSE for validation\n    p_fom_snapshots = snapshot_matrix[:N, :]\n    p_rom_reconstructed_snapshots = x_hat_snapshots[:N, :]\n    \n    error_p_matrix = p_fom_snapshots - p_rom_reconstructed_snapshots\n    \n    numerator = np.sum(error_p_matrix**2)\n    denominator = np.sum(p_fom_snapshots**2)\n    \n    if denominator == 0:\n        nrmse = 0.0\n    else:\n        nrmse = np.sqrt(numerator / denominator)\n    \n    # 10. Return boolean result of validation check\n    return nrmse <= epsilon\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}