{
    "hands_on_practices": [
        {
            "introduction": "Accurately tracking the state of a dynamic system, such as a battery's state of charge, is complicated by nonlinear behavior and inherent process noise. This first exercise challenges you to implement and compare two fundamental techniques for propagating uncertainty through a nonlinear model. By implementing the Unscented Transform and comparing its estimates to a high-fidelity Monte Carlo simulation, you will gain practical experience with a powerful tool used in advanced state estimation algorithms like the Unscented Kalman Filter .",
            "id": "4128494",
            "problem": "Consider a single-state discrete-time model of battery State of Charge (SOC) with additive process noise. Let the SOC at step $k$ be $x_k \\in \\mathbb{R}$, representing a dimensionless fraction in $[0,1]$ (no unit). The discrete-time update from $x_k$ to $x_{k+1}$ over a time step of duration $\\Delta t$ hours under a constant applied current $I$ (positive for discharge, negative for charge) is defined by\n$$\nx_{k+1} \\triangleq f(x_k; I, \\Delta t, C_{\\mathrm{eff}}, \\gamma) + w_k,\n$$\nwhere\n$$\nf(x; I, \\Delta t, C_{\\mathrm{eff}}, \\gamma) = x - \\frac{\\Delta t}{C_{\\mathrm{eff}}} \\, I \\left( 1 + \\gamma \\left(1 - 2x\\right)^3 \\right),\n$$\n$C_{\\mathrm{eff}}$ is the effective capacity in ampere-hours (Ah), and $w_k$ is the process noise modeled as a zero-mean Gaussian random variable $w_k \\sim \\mathcal{N}(0, q)$ with variance $q$. The nonlinearity level is governed by the dimensionless parameter $\\gamma \\ge 0$. The initial SOC $x_k$ is uncertain and modeled as a Gaussian random variable $x_k \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$.\n\nYour task is to implement the Unscented Transform (UT) to approximate the mean and covariance of $x_{k+1}$ given the Gaussian prior for $x_k$ and the nonlinear function $f(\\cdot)$, and to compare the UT-based estimates to empirical Monte Carlo estimates obtained by sampling $x_k$ and $w_k$ and propagating through the nonlinear update. Use the Unscented Transform scaling parameters $\\alpha_{\\mathrm{UT}}$, $\\beta_{\\mathrm{UT}}$, and $\\kappa_{\\mathrm{UT}}$ with the following specified values: $\\alpha_{\\mathrm{UT}} = 0.5$, $\\beta_{\\mathrm{UT}} = 2$, and $\\kappa_{\\mathrm{UT}} = 0$.\n\nFundamental base assumptions you must use:\n- Conservation of charge implies the deterministic SOC change under constant current over $\\Delta t$ is proportional to $\\Delta t/C_{\\mathrm{eff}}$, and the sign is determined by the direction of current.\n- Random variable transformation through a nonlinear function requires integrating against the input distribution; the Unscented Transform provides a deterministic sampling scheme to approximate these integrals for Gaussian priors.\n- Additive Gaussian noise with variance $q$ increases the output covariance by $q$.\n\nImplement a program that, for each test case, returns two scalar results:\n- The absolute difference between the UT mean estimate $\\mu_{\\mathrm{UT}}$ and the Monte Carlo mean $\\mu_{\\mathrm{MC}}$ expressed as a decimal fraction (dimensionless).\n- The absolute difference between the UT variance estimate $\\sigma^2_{\\mathrm{UT}}$ and the Monte Carlo variance $\\sigma^2_{\\mathrm{MC}}$ expressed as a decimal fraction (dimensionless).\n\nYou must treat SOC as a dimensionless fraction. Do not use a percentage sign anywhere; always express SOC-related quantities as decimal fractions. The angle unit does not apply in this problem.\n\nUse the following test suite of parameter values, which covers typical operating conditions, boundary conditions near the SOC extremes, and varying nonlinearity levels. Each test case is provided as a tuple $(\\mu_0, \\sigma_0, I, \\gamma, \\sigma_w, \\Delta t, C_{\\mathrm{eff}}, N_{\\mathrm{MC}})$, where $q = \\sigma_w^2$:\n- Test case $1$ (baseline linear regime): $(0.5, 0.01, 5.0, 0.0, 0.001, \\frac{1}{60}, 5.0, 50000)$\n- Test case $2$ (moderate nonlinearity, larger prior uncertainty): $(0.5, 0.05, 5.0, 2.0, 0.0015, \\frac{1}{60}, 5.0, 50000)$\n- Test case $3$ (high SOC near upper boundary, strong nonlinearity, higher current discharge): $(0.95, 0.02, 10.0, 5.0, 0.002, \\frac{1}{60}, 5.0, 50000)$\n- Test case $4$ (low SOC near lower boundary, moderate nonlinearity, charging current): $(0.05, 0.02, -3.0, 3.0, 0.001, \\frac{1}{60}, 5.0, 50000)$\n- Test case $5$ (extreme nonlinearity but very small prior variance): $(0.5, 0.001, 5.0, 10.0, 0.001, \\frac{1}{60}, 5.0, 50000)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must concatenate the pair of absolute errors for each test case in order, i.e., it should be of the form\n$$\n[\\varepsilon^{(1)}_{\\mu}, \\varepsilon^{(1)}_{\\sigma^2}, \\varepsilon^{(2)}_{\\mu}, \\varepsilon^{(2)}_{\\sigma^2}, \\ldots, \\varepsilon^{(5)}_{\\mu}, \\varepsilon^{(5)}_{\\sigma^2}],\n$$\nwhere $\\varepsilon^{(i)}_{\\mu} = \\left| \\mu_{\\mathrm{UT}}^{(i)} - \\mu_{\\mathrm{MC}}^{(i)} \\right|$ and $\\varepsilon^{(i)}_{\\sigma^2} = \\left| \\sigma_{\\mathrm{UT}}^{2\\,(i)} - \\sigma_{\\mathrm{MC}}^{2\\,(i)} \\right|$ are decimal fractions.\n\nThe program must be complete and runnable without any user input. It must implement the Unscented Transform for the one-dimensional case and Monte Carlo estimation according to the specifications above, and it must use the provided test cases exactly as given. The final line must be printed exactly in the described format.",
            "solution": "The problem requires the implementation and comparison of two methods for propagating uncertainty through a nonlinear a posteriori state-space model for a battery's State of Charge (SOC). The state at discrete time step $k$ is $x_k \\in \\mathbb{R}$, which is uncertain and modeled by a Gaussian distribution $x_k \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$. The state evolves according to the equation:\n$$\nx_{k+1} = f(x_k; I, \\Delta t, C_{\\mathrm{eff}}, \\gamma) + w_k\n$$\nwhere $f(\\cdot)$ is a nonlinear function and $w_k \\sim \\mathcal{N}(0, q)$ is additive zero-mean Gaussian process noise with variance $q = \\sigma_w^2$. The objective is to estimate the mean $\\mu_{k+1}$ and variance $\\sigma_{k+1}^2$ of the subsequent state $x_{k+1}$. We will compare the estimates from the Unscented Transform (UT) with those from a Monte Carlo (MC) simulation, which serves as a high-fidelity benchmark.\n\nThe nonlinear state transition function is given by:\n$$\nf(x; I, \\Delta t, C_{\\mathrm{eff}}, \\gamma) = x - \\frac{\\Delta t}{C_{\\mathrm{eff}}} \\, I \\left( 1 + \\gamma \\left(1 - 2x\\right)^3 \\right)\n$$\nwhere $I$ is the current, $C_{\\mathrm{eff}}$ is the effective capacity, $\\Delta t$ is the time step duration, and $\\gamma$ is a parameter controlling the nonlinearity.\n\n**Monte Carlo (MC) Simulation**\n\nThe Monte Carlo method provides a numerical approximation to the true posterior distribution of $x_{k+1}$. By the law of large numbers, the sample statistics converge to the true statistics of the transformed distribution as the number of samples approaches infinity. The procedure is as follows:\n1.  Generate a large number $N_{\\mathrm{MC}}$ of random samples, $\\{x_k^{(j)}\\}_{j=1}^{N_{\\mathrm{MC}}}$, from the initial Gaussian distribution $x_k \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$.\n2.  Generate $N_{\\mathrm{MC}}$ samples, $\\{w_k^{(j)}\\}_{j=1}^{N_{\\mathrm{MC}}}$, from the process noise distribution $w_k \\sim \\mathcal{N}(0, q)$.\n3.  For each sample $j$, propagate it through the full state update equation to obtain a sample of the next state:\n    $$\n    x_{k+1}^{(j)} = f(x_k^{(j)}; I, \\Delta t, C_{\\mathrm{eff}}, \\gamma) + w_k^{(j)}\n    $$\n4.  The empirical mean $\\mu_{\\mathrm{MC}}$ and variance $\\sigma_{\\mathrm{MC}}^2$ are then computed from the resulting set of samples $\\{x_{k+1}^{(j)}\\}$:\n    $$\n    \\mu_{\\mathrm{MC}} = \\frac{1}{N_{\\mathrm{MC}}} \\sum_{j=1}^{N_{\\mathrm{MC}}} x_{k+1}^{(j)}\n    $$\n    $$\n    \\sigma_{\\mathrm{MC}}^2 = \\frac{1}{N_{\\mathrm{MC}}-1} \\sum_{j=1}^{N_{\\mathrm{MC}}} (x_{k+1}^{(j)} - \\mu_{\\mathrm{MC}})^2\n    $$\n    Here, the unbiased sample variance (with denominator $N_{\\mathrm{MC}}-1$) is used.\n\n**Unscented Transform (UT)**\n\nThe Unscented Transform is a deterministic sampling technique that provides a more efficient way to approximate the statistics of a random variable undergoing a nonlinear transformation. It uses a minimal set of deterministically chosen sample points, called sigma points, which capture the first two moments (mean and covariance) of the prior Gaussian distribution.\n\nThe procedure for the one-dimensional state $x_k$ is as follows:\n\n1.  **Define Parameters**: The state dimension is $n=1$. The UT scaling parameters are given as $\\alpha_{\\mathrm{UT}} = 0.5$, $\\beta_{\\mathrm{UT}} = 2$, and $\\kappa_{\\mathrm{UT}} = 0$.\n\n2.  **Calculate Sigma Points and Weights**:\n    - A composite scaling parameter $\\lambda$ is computed:\n      $$\n      \\lambda = \\alpha_{\\mathrm{UT}}^2(n + \\kappa_{\\mathrm{UT}}) - n = (0.5)^2(1+0) - 1 = 0.25 - 1 = -0.75\n      $$\n    - A set of $2n+1=3$ sigma points $\\mathcal{X}_i$ are generated from the prior mean $\\mu_0$ and variance $\\sigma_0^2$:\n      \\begin{align*}\n      \\mathcal{X}_0 = \\mu_0 \\\\\n      \\mathcal{X}_1 = \\mu_0 + \\sqrt{(n+\\lambda)\\sigma_0^2} = \\mu_0 + \\sqrt{(1-0.75)}\\sigma_0 = \\mu_0 + 0.5\\sigma_0 \\\\\n      \\mathcal{X}_2 = \\mu_0 - \\sqrt{(n+\\lambda)\\sigma_0^2} = \\mu_0 - 0.5\\sigma_0\n      \\end{align*}\n    - Corresponding weights for the mean ($W_i^{(m)}$) and covariance ($W_i^{(c)}$) are calculated:\n      \\begin{align*}\n      W_0^{(m)} = \\frac{\\lambda}{n+\\lambda} = \\frac{-0.75}{1-0.75} = -3 \\\\\n      W_1^{(m)} = W_2^{(m)} = \\frac{1}{2(n+\\lambda)} = \\frac{1}{2(1-0.75)} = 2 \\\\\n      W_0^{(c)} = \\frac{\\lambda}{n+\\lambda} + (1 - \\alpha_{\\mathrm{UT}}^2 + \\beta_{\\mathrm{UT}}) = -3 + (1 - (0.5)^2 + 2) = -0.25 \\\\\n      W_1^{(c)} = W_2^{(c)} = \\frac{1}{2(n+\\lambda)} = 2\n      \\end{align*}\n\n3.  **Propagate Sigma Points**: Each sigma point $\\mathcal{X}_i$ is transformed through the nonlinear function $f(\\cdot)$, yielding a set of transformed points $\\mathcal{Y}_i$:\n    $$\n    \\mathcal{Y}_i = f(\\mathcal{X}_i; I, \\Delta t, C_{\\mathrm{eff}}, \\gamma) \\quad \\text{for } i \\in \\{0, 1, 2\\}\n    $$\n\n4.  **Calculate Propagated Mean and Covariance**: The predicted mean $\\mu_{\\mathrm{UT}}$ and covariance $P_y$ of the transformed variable (before adding process noise) are computed by taking the weighted sum of the propagated points:\n    $$\n    \\mu_{\\mathrm{UT}} = \\sum_{i=0}^{2} W_i^{(m)} \\mathcal{Y}_i\n    $$\n    $$\n    P_y = \\sum_{i=0}^{2} W_i^{(c)} (\\mathcal{Y}_i - \\mu_{\\mathrm{UT}})^2\n    $$\n\n5.  **Incorporate Process Noise**: The final estimate for the variance, $\\sigma_{\\mathrm{UT}}^2$, is obtained by adding the process noise variance $q$ to the propagated covariance $P_y$. The mean is unaffected as the process noise has zero mean.\n    $$\n    \\sigma_{\\mathrm{UT}}^2 = P_y + q\n    $$\n\n**Implementation and Comparison**\n\nThe program will implement both the UT and MC methods. For each of the five test cases provided, it will compute $(\\mu_{\\mathrm{UT}}, \\sigma_{\\mathrm{UT}}^2)$ and $(\\mu_{\\mathrm{MC}}, \\sigma_{\\mathrm{MC}}^2)$. The final output for each test case consists of two values: the absolute difference in means, $\\varepsilon_{\\mu} = |\\mu_{\\mathrm{UT}} - \\mu_{\\mathrm{MC}}|$, and the absolute difference in variances, $\\varepsilon_{\\sigma^2} = |\\sigma_{\\mathrm{UT}}^2 - \\sigma_{\\mathrm{MC}}^2|$. These values quantify the accuracy of the Unscented Transform approximation relative to the more computationally intensive Monte Carlo simulation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the battery SOC estimation problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # (mu_0, sigma_0, I, gamma, sigma_w, delta_t, C_eff, N_MC)\n        (0.5, 0.01, 5.0, 0.0, 0.001, 1.0/60.0, 5.0, 50000),\n        (0.5, 0.05, 5.0, 2.0, 0.0015, 1.0/60.0, 5.0, 50000),\n        (0.95, 0.02, 10.0, 5.0, 0.002, 1.0/60.0, 5.0, 50000),\n        (0.05, 0.02, -3.0, 3.0, 0.001, 1.0/60.0, 5.0, 50000),\n        (0.5, 0.001, 5.0, 10.0, 0.001, 1.0/60.0, 5.0, 50000),\n    ]\n\n    results = []\n    \n    # Use a fixed seed for reproducibility of Monte Carlo simulations\n    rng = np.random.default_rng(seed=42)\n\n    for case in test_cases:\n        mu_0, sigma_0, I, gamma, sigma_w, delta_t, C_eff, N_mc = case\n        \n        # Run Monte Carlo simulation\n        mu_mc, var_mc = monte_carlo(mu_0, sigma_0, I, gamma, sigma_w, delta_t, C_eff, N_mc, rng)\n        \n        # Run Unscented Transform\n        mu_ut, var_ut = unscented_transform(mu_0, sigma_0, I, gamma, sigma_w, delta_t, C_eff)\n        \n        # Calculate absolute differences\n        err_mu = abs(mu_ut - mu_mc)\n        err_var = abs(var_ut - var_mc)\n        \n        results.extend([err_mu, err_var])\n\n    # Format the output as a comma-separated string in brackets\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\ndef state_transition_func(x, I, delta_t, C_eff, gamma):\n    \"\"\"\n    The nonlinear state transition function f(x).\n    \"\"\"\n    term_I = (delta_t / C_eff) * I\n    non_linearity = 1.0 + gamma * (1.0 - 2.0 * x)**3\n    return x - term_I * non_linearity\n\ndef unscented_transform(mu_0, sigma_0, I, gamma, sigma_w, delta_t, C_eff):\n    \"\"\"\n    Approximates the mean and variance of the next state using the Unscented Transform.\n    \"\"\"\n    # State dimension\n    n = 1\n    \n    # UT scaling parameters\n    alpha_ut = 0.5\n    beta_ut = 2.0\n    kappa_ut = 0.0\n    \n    # Composite scaling parameter\n    lambda_ = alpha_ut**2 * (n + kappa_ut) - n\n    \n    # --- Step 1: Generate Sigma Points ---\n    var_0 = sigma_0**2\n    sigma_points = np.zeros(2 * n + 1)\n    sigma_points[0] = mu_0\n    \n    sqrt_term = np.sqrt((n + lambda_) * var_0)\n    sigma_points[1] = mu_0 + sqrt_term\n    sigma_points[2] = mu_0 - sqrt_term\n    \n    # --- Step 2: Calculate Weights ---\n    weights_m = np.zeros(2 * n + 1)\n    weights_c = np.zeros(2 * n + 1)\n    \n    weights_m[0] = lambda_ / (n + lambda_)\n    weights_c[0] = lambda_ / (n + lambda_) + (1 - alpha_ut**2 + beta_ut)\n    \n    for i in range(1, 2 * n + 1):\n        weights_m[i] = 1.0 / (2.0 * (n + lambda_))\n        weights_c[i] = 1.0 / (2.0 * (n + lambda_))\n        \n    # --- Step 3: Propagate Sigma Points ---\n    propagated_points = state_transition_func(sigma_points, I, delta_t, C_eff, gamma)\n    \n    # --- Step 4: Calculate Propagated Mean and Covariance ---\n    mu_ut = np.sum(weights_m * propagated_points)\n    \n    # Covariance before adding process noise\n    var_y = np.sum(weights_c * (propagated_points - mu_ut)**2)\n    \n    # --- Step 5: Add Process Noise ---\n    q = sigma_w**2\n    var_ut = var_y + q\n    \n    return mu_ut, var_ut\n\ndef monte_carlo(mu_0, sigma_0, I, gamma, sigma_w, delta_t, C_eff, N_mc, rng):\n    \"\"\"\n    Estimates the mean and variance of the next state using Monte Carlo simulation.\n    \"\"\"\n    # Generate initial state samples\n    x_k_samples = rng.normal(loc=mu_0, scale=sigma_0, size=N_mc)\n    \n    # Generate process noise samples\n    q = sigma_w**2\n    w_k_samples = rng.normal(loc=0.0, scale=np.sqrt(q), size=N_mc)\n    \n    # Propagate samples through the full state equation\n    x_k_plus_1_samples = state_transition_func(x_k_samples, I, delta_t, C_eff, gamma) + w_k_samples\n    \n    # Calculate empirical mean and variance\n    mu_mc = np.mean(x_k_plus_1_samples)\n    # Use ddof=1 for unbiased sample variance\n    var_mc = np.var(x_k_plus_1_samples, ddof=1)\n    \n    return mu_mc, var_mc\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Beyond simply tracking uncertainty, energy system operators must make concrete economic decisions in its presence. This practice moves from analysis to application, focusing on a generator dispatch problem where demand is unknown. You will solve the problem using two distinct philosophical approaches: stochastic optimization, which seeks the best average-case performance, and robust optimization, which guards against the worst-case outcome . This exercise illuminates the critical trade-offs between optimality and security, particularly when the assumed model of uncertainty may not perfectly match reality.",
            "id": "4128563",
            "problem": "Consider a small multi-period energy system with a single controllable generator and an external procurement option. Time is divided into $T$ discrete periods indexed by $t \\in \\{1,2,3\\}$. The generator dispatch in period $t$ is $g_t \\in \\{0,1,2\\}$ measured in energy units, subject to an initial condition $g_0 = 0$ and a ramping constraint $|g_t - g_{t-1}| \\le R$ with $R = 1$. Let the capacity be $2$ energy units per period. In each period, the random demand is $D_t \\ge 0$, and the realized demand vector is $\\mathbf{D} = (D_1, D_2, D_3)$. After the demand is realized, the operator can procure $s_t \\ge 0$ from an external source at a per-unit cost. The period energy balance is $g_t + s_t \\ge D_t$, and overproduction ($g_t  D_t$) is spilled without penalty. The per-period generation cost is $c \\cdot g_t$ and the per-unit external procurement cost is $p$. The total cost for a schedule $\\mathbf{g} = (g_1,g_2,g_3)$ and a realized demand $\\mathbf{D}$ is\n$$\nC(\\mathbf{g}, \\mathbf{D}) = \\sum_{t=1}^{3} \\left( c \\cdot g_t + p \\cdot s_t \\right),\n$$\nwhere $s_t$ is chosen to satisfy energy balance at minimal cost, that is,\n$$\ns_t = \\max\\{0, D_t - g_t\\},\n$$\nso that\n$$\nC(\\mathbf{g}, \\mathbf{D}) = \\sum_{t=1}^{3} \\left( c \\cdot g_t + p \\cdot \\max\\{0, D_t - g_t\\} \\right).\n$$\nDefine the feasible schedule set\n$$\n\\mathcal{G} = \\left\\{ \\mathbf{g} \\in \\{0,1,2\\}^3 \\,:\\, |g_t - g_{t-1}| \\le 1 \\text{ for } t=1,2,3 \\text{ and } g_0 = 0 \\right\\}.\n$$\nTwo uncertainty representations are considered for the training phase: an uncertainty set $\\mathcal{U}_0$ and a probability distribution $P_0$. The robust training problem is\n$$\n\\min_{\\mathbf{g} \\in \\mathcal{G}} \\; \\max_{\\mathbf{D} \\in \\mathcal{U}_0} \\; C(\\mathbf{g}, \\mathbf{D}),\n$$\nand the stochastic training problem is\n$$\n\\min_{\\mathbf{g} \\in \\mathcal{G}} \\; \\mathbb{E}_{P_0}\\left[ C(\\mathbf{g}, \\mathbf{D}) \\right].\n$$\nFor evaluation, the true distribution is $P^*$, which may differ from $P_0$. Let $\\mathbf{g}_R$ and $\\mathbf{g}_S$ denote the optimal schedules obtained from robust and stochastic training respectively. The Price of Robustness (PoR) is defined as\n$$\n\\text{PoR} = \\mathbb{E}_{P^*}\\left[ C(\\mathbf{g}_R, \\mathbf{D}) \\right] - \\mathbb{E}_{P^*}\\left[ C(\\mathbf{g}_S, \\mathbf{D}) \\right].\n$$\nThe out-of-sample regret under misspecification for a schedule $\\mathbf{g}$ is defined as\n$$\n\\text{Reg}(\\mathbf{g}) = \\mathbb{E}_{P^*}\\left[ C(\\mathbf{g}, \\mathbf{D}) \\right] - \\min_{\\mathbf{h} \\in \\mathcal{G}} \\mathbb{E}_{P^*}\\left[ C(\\mathbf{h}, \\mathbf{D}) \\right].\n$$\nYou must compute $\\text{PoR}$, $\\text{Reg}(\\mathbf{g}_R)$, and $\\text{Reg}(\\mathbf{g}_S)$ for each test case below. All costs must be expressed in Monetary Units (MU).\n\nAssume $\\mathcal{U}_0$ is a componentwise box with lower bound $\\mathbf{d}_{\\min} = (0,0,0)$ and upper bound $\\mathbf{d}_{\\max}$ provided per test case. Because $C(\\mathbf{g}, \\mathbf{D})$ is nondecreasing in each $D_t$ for fixed $\\mathbf{g}$, the inner maximization of the robust problem attains its maximum at $\\mathbf{D} = \\mathbf{d}_{\\max}$.\n\nTest suite (each case specifies $c$, $p$, $\\mathbf{d}_{\\max}$, $P_0$, and $P^*$):\n- Case $1$:\n  - $c = 20$, $p = 100$, $\\mathbf{d}_{\\max} = (1,2,2)$.\n  - $P_0$: scenarios $\\mathbf{D}^{(1)} = (0,1,1)$ with probability $0.5$, $\\mathbf{D}^{(2)} = (1,2,1)$ with probability $0.3$, $\\mathbf{D}^{(3)} = (0,0,2)$ with probability $0.2$.\n  - $P^*$: scenarios $\\mathbf{D}^{A} = (1,2,2)$ with probability $0.4$, $\\mathbf{D}^{B} = (0,1,2)$ with probability $0.4$, $\\mathbf{D}^{C} = (2,2,2)$ with probability $0.2$.\n- Case $2$:\n  - $c = 40$, $p = 60$, $\\mathbf{d}_{\\max} = (1,1,1)$.\n  - $P_0$: scenarios $\\mathbf{D}^{(1)} = (0,1,0)$ with probability $0.6$, $\\mathbf{D}^{(2)} = (1,1,1)$ with probability $0.4$.\n  - $P^*$: scenarios $\\mathbf{D}^{A} = (1,2,1)$ with probability $0.5$, $\\mathbf{D}^{B} = (0,0,1)$ with probability $0.5$.\n- Case $3$:\n  - $c = 20$, $p = 100$, $\\mathbf{d}_{\\max} = (0,0,0)$.\n  - $P_0$: scenario $\\mathbf{D} = (0,0,0)$ with probability $1.0$.\n  - $P^*$: scenario $\\mathbf{D} = (2,2,2)$ with probability $1.0$.\n\nYour program must:\n- Enumerate $\\mathcal{G}$ respecting $g_0 = 0$, $g_t \\in \\{0,1,2\\}$, and $|g_t - g_{t-1}| \\le 1$.\n- Solve the robust and stochastic training problems to obtain $\\mathbf{g}_R$ and $\\mathbf{g}_S$.\n- Compute $\\text{PoR}$, $\\text{Reg}(\\mathbf{g}_R)$, and $\\text{Reg}(\\mathbf{g}_S)$ under $P^*$.\n- Produce a single line of output containing the results as a comma-separated list of per-test-case triples, each triple being $[\\text{PoR}, \\text{Reg}_R, \\text{Reg}_S]$ rounded to two decimal places, all in Monetary Units (MU), enclosed in square brackets; that is,\n$$\n\\left[ [\\text{PoR}_1,\\text{Reg}_R^{(1)},\\text{Reg}_S^{(1)}], [\\text{PoR}_2,\\text{Reg}_R^{(2)},\\text{Reg}_S^{(2)}], [\\text{PoR}_3,\\text{Reg}_R^{(3)},\\text{Reg}_S^{(3)}] \\right].\n$$\nAngle units do not apply. Percentages are not used. All outputs must be numeric floats in Monetary Units (MU).",
            "solution": "The user has provided a well-defined optimization problem within the domain of energy systems modeling, focusing on the comparison of robust and stochastic optimization techniques under model misspecification. The problem is scientifically grounded, mathematically consistent, and all necessary data are provided. A computational solution is feasible due to the small, discrete nature of the decision space.\n\nThe core of the problem is to find optimal generator dispatch schedules $\\mathbf{g} = (g_1, g_2, g_3)$ and evaluate their performance. The solution proceeds methodologically as follows:\n\nFirst, we must explicitly define the set of all feasible schedules, $\\mathcal{G}$. A schedule $\\mathbf{g} = (g_1, g_2, g_3)$ is feasible if it adheres to the given constraints:\n1.  The per-period generation $g_t$ must be an integer from $\\{0, 1, 2\\}$.\n2.  The initial state is fixed at $g_0 = 0$.\n3.  The ramping constraint $|g_t - g_{t-1}| \\le 1$ must be satisfied for $t \\in \\{1, 2, 3\\}$.\n\nBy systematically enumerating all paths from $g_0=0$ that satisfy these constraints, we can construct the finite set $\\mathcal{G}$. The derivation is as follows:\n- For $t=1$, given $g_0=0$, the ramping constraint allows $g_1 \\in \\{0, 1\\}$.\n- For $t=2$, if $g_1=0$, then $g_2 \\in \\{0, 1\\}$. If $g_1=1$, then $g_2 \\in \\{0, 1, 2\\}$.\n- For $t=3$, the possible values of $g_3$ depend on $g_2$. If $g_2=0$, $g_3 \\in \\{0, 1\\}$. If $g_2=1$, $g_3 \\in \\{0, 1, 2\\}$. If $g_2=2$, $g_3 \\in \\{1, 2\\}$.\nThis constructive enumeration yields a total of $12$ unique feasible schedules in $\\mathcal{G}$. Since this set is small, we can solve the optimization problems by exhaustive search.\n\nThe total cost for a given schedule $\\mathbf{g}$ and a realized demand vector $\\mathbf{D}=(D_1, D_2, D_3)$ is given by the function:\n$$\nC(\\mathbf{g}, \\mathbf{D}) = \\sum_{t=1}^{3} \\left( c \\cdot g_t + p \\cdot \\max\\{0, D_t - g_t\\} \\right)\n$$\nwhere $c$ is the per-unit generation cost and $p$ is the per-unit external procurement cost.\n\nFor each test case, we proceed with the following steps:\n\n1.  **Solve the Robust Training Problem**: We find the robust-optimal schedule $\\mathbf{g}_R$. This schedule minimizes the worst-case cost over the uncertainty set $\\mathcal{U}_0$. As stated, the cost function $C(\\mathbf{g}, \\mathbf{D})$ is non-decreasing in each component of $\\mathbf{D}$, so the worst-case demand is $\\mathbf{d}_{\\max}$. The problem thus becomes:\n    $$\n    \\mathbf{g}_R \\in \\arg\\min_{\\mathbf{g} \\in \\mathcal{G}} C(\\mathbf{g}, \\mathbf{d}_{\\max})\n    $$\n    We compute $C(\\mathbf{g}, \\mathbf{d}_{\\max})$ for every $\\mathbf{g} \\in \\mathcal{G}$ and identify a schedule that achieves the minimum cost.\n\n2.  **Solve the Stochastic Training Problem**: We find the stochastically-optimal schedule $\\mathbf{g}_S$. This schedule minimizes the expected cost under the training probability distribution $P_0$. For a discrete distribution $P_0$ with scenarios $\\mathbf{D}^{(i)}$ and associated probabilities $\\pi_i$, the problem is:\n    $$\n    \\mathbf{g}_S \\in \\arg\\min_{\\mathbf{g} \\in \\mathcal{G}} \\mathbb{E}_{P_0}\\left[ C(\\mathbf{g}, \\mathbf{D}) \\right] = \\arg\\min_{\\mathbf{g} \\in \\mathcal{G}} \\sum_{i} \\pi_i C(\\mathbf{g}, \\mathbf{D}^{(i)})\n    $$\n    We compute this expected cost for every $\\mathbf{g} \\in \\mathcal{G}$ and find a minimizing schedule.\n\n3.  **Perform Out-of-Sample Evaluation**: The performance of all schedules, including $\\mathbf{g}_R$ and $\\mathbf{g}_S$, is evaluated under the true probability distribution $P^*$. We calculate the expected cost for every schedule $\\mathbf{g} \\in \\mathcal{G}$ under $P^*$:\n    $$\n    \\mathbb{E}_{P^*}\\left[ C(\\mathbf{g}, \\mathbf{D}) \\right] = \\sum_{j} \\pi_j^* C(\\mathbf{g}, \\mathbf{D}^{(j)})\n    $$\n    where $\\mathbf{D}^{(j)}$ are the scenarios and $\\pi_j^*$ are the probabilities from $P^*$. From these calculations, we identify the true minimum expected cost, which is the \"clairvoyant\" optimal solution:\n    $$\n    C^*_{P^*} = \\min_{\\mathbf{h} \\in \\mathcal{G}} \\mathbb{E}_{P^*}\\left[ C(\\mathbf{h}, \\mathbf{D}) \\right]\n    $$\n\n4.  **Compute Performance Metrics**: Using the results from the previous steps, we calculate the required metrics for each test case:\n    -   **Price of Robustness (PoR)**: The difference in out-of-sample performance between the robust and stochastic solutions.\n        $$\n        \\text{PoR} = \\mathbb{E}_{P^*}\\left[ C(\\mathbf{g}_R, \\mathbf{D}) \\right] - \\mathbb{E}_{P^*}\\left[ C(\\mathbf{g}_S, \\mathbf{D}) \\right]\n        $$\n    -   **Out-of-Sample Regret of $\\mathbf{g}_R$**: The performance loss of the robust solution compared to the true optimal solution.\n        $$\n        \\text{Reg}(\\mathbf{g}_R) = \\mathbb{E}_{P^*}\\left[ C(\\mathbf{g}_R, \\mathbf{D}) \\right] - C^*_{P^*}\n        $$\n    -   **Out-of-Sample Regret of $\\mathbf{g}_S$**: The performance loss of the stochastic solution compared to the true optimal solution.\n        $$\n        \\text{Reg}(\\mathbf{g}_S) = \\mathbb{E}_{P^*}\\left[ C(\\mathbf{g}_S, \\mathbf{D}) \\right] - C^*_{P^*}\n        $$\nIn cases where multiple schedules yield the same minimum cost for the robust or stochastic problems, a deterministic tie-breaking rule (e.g., selecting the first one encountered in the enumerated list) is used to ensure a unique result. The entire procedure is implemented in a single program that iterates through the provided test cases and formats the output as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the energy system optimization problem for all test cases.\n    \"\"\"\n\n    def generate_schedules():\n        \"\"\"\n        Enumerates the set of all feasible generation schedules G.\n        A schedule g = (g1, g2, g3) is feasible if:\n        1. g_t is in {0, 1, 2}.\n        2. g_0 = 0.\n        3. |g_t - g_{t-1}| = 1.\n        \"\"\"\n        schedules = []\n        g0 = 0\n        # Time t=1\n        for g1 in range(max(0, g0 - 1), min(2, g0 + 1) + 1):\n            # Time t=2\n            for g2 in range(max(0, g1 - 1), min(2, g1 + 1) + 1):\n                # Time t=3\n                for g3 in range(max(0, g2 - 1), min(2, g2 + 1) + 1):\n                    schedules.append(np.array([g1, g2, g3], dtype=int))\n        return schedules\n\n    def calculate_cost(g, D, c, p):\n        \"\"\"\n        Calculates the total cost C(g, D) for a given schedule and demand.\n        C(g, D) = sum(c*g_t + p*max(0, D_t - g_t)) for t=1,2,3.\n        \"\"\"\n        generation_cost = c * np.sum(g)\n        procurement_cost = p * np.sum(np.maximum(0, D - g))\n        return generation_cost + procurement_cost\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"c\": 20, \"p\": 100, \"d_max\": np.array([1, 2, 2]),\n            \"P0\": {\"scenarios\": [np.array([0, 1, 1]), np.array([1, 2, 1]), np.array([0, 0, 2])], \"probs\": [0.5, 0.3, 0.2]},\n            \"P_star\": {\"scenarios\": [np.array([1, 2, 2]), np.array([0, 1, 2]), np.array([2, 2, 2])], \"probs\": [0.4, 0.4, 0.2]}\n        },\n        {\n            \"c\": 40, \"p\": 60, \"d_max\": np.array([1, 1, 1]),\n            \"P0\": {\"scenarios\": [np.array([0, 1, 0]), np.array([1, 1, 1])], \"probs\": [0.6, 0.4]},\n            \"P_star\": {\"scenarios\": [np.array([1, 2, 1]), np.array([0, 0, 1])], \"probs\": [0.5, 0.5]}\n        },\n        {\n            \"c\": 20, \"p\": 100, \"d_max\": np.array([0, 0, 0]),\n            \"P0\": {\"scenarios\": [np.array([0, 0, 0])], \"probs\": [1.0]},\n            \"P_star\": {\"scenarios\": [np.array([2, 2, 2])], \"probs\": [1.0]}\n        }\n    ]\n\n    schedules = generate_schedules()\n    final_results = []\n\n    for case in test_cases:\n        c, p, d_max = case[\"c\"], case[\"p\"], case[\"d_max\"]\n        P0 = case[\"P0\"]\n        P_star = case[\"P_star\"]\n\n        # 1. Find g_R (Robust solution) by minimizing worst-case cost\n        robust_costs = [calculate_cost(g, d_max, c, p) for g in schedules]\n        min_robust_cost = min(robust_costs)\n        # Deterministic tie-breaking by taking the first occurrence\n        idx_gR = robust_costs.index(min_robust_cost)\n        \n        # 2. Find g_S (Stochastic solution) by minimizing expected cost under P0\n        stochastic_costs_p0 = []\n        for g in schedules:\n            expected_cost = sum(prob * calculate_cost(g, D, c, p) for D, prob in zip(P0[\"scenarios\"], P0[\"probs\"]))\n            stochastic_costs_p0.append(expected_cost)\n        min_stochastic_cost_p0 = min(stochastic_costs_p0)\n        idx_gS = stochastic_costs_p0.index(min_stochastic_cost_p0)\n        \n        # 3. Evaluate all schedules under P_star to find true performance and true optimum\n        true_expected_costs = []\n        for g in schedules:\n            expected_cost = sum(prob * calculate_cost(g, D, c, p) for D, prob in zip(P_star[\"scenarios\"], P_star[\"probs\"]))\n            true_expected_costs.append(expected_cost)\n            \n        min_true_cost = min(true_expected_costs)\n        \n        # Performance of g_R and g_S under P_star\n        exp_cost_gR_pstar = true_expected_costs[idx_gR]\n        exp_cost_gS_pstar = true_expected_costs[idx_gS]\n\n        # 4. Calculate metrics\n        PoR = exp_cost_gR_pstar - exp_cost_gS_pstar\n        Reg_R = exp_cost_gR_pstar - min_true_cost\n        Reg_S = exp_cost_gS_pstar - min_true_cost\n        \n        # Format results for this case\n        case_result_str = f\"[{PoR:.2f},{Reg_R:.2f},{Reg_S:.2f}]\"\n        final_results.append(case_result_str)\n\n    # Print the final output in the required format\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The final practice elevates our approach from passively managing uncertainty to actively reducing it through targeted actions. Often, the parameters of our system models are not perfectly known, which itself is a major source of uncertainty. This exercise puts you in the role of an engineer designing an experiment to best identify the parameters of a battery model . By finding an input current profile that minimizes the posterior parameter covariance, you will explore the powerful concept of A-optimal experiment design and its connection to the Fisher Information Matrix.",
            "id": "4128477",
            "problem": "You are tasked with designing an input current sequence for a single resistorâ€“capacitor battery equivalent circuit model to minimize the uncertainty of its parameters after observing the voltage response, subject to amplitude and energy constraints. The core modeling assumptions and derivation must start from the following fundamental bases: linear system dynamics for small signals, Gaussian measurement noise, and Bayesian estimation with linearized sensitivities.\n\nConsider the Thevenin model with one resistor and one capacitor. Let the input be the current sequence $\\{i_k\\}_{k=0}^{N-1}$, with sampling period $dt$ and zero-order hold between samples. The internal state $v_{1,k}$ evolves according to the discrete-time dynamics derived from the continuous-time first-order linear system $dv_1/dt = -(1/(R_1 C_1)) v_1 + (1/C_1) i(t)$:\n$$\nv_{1,k+1} = a(\\boldsymbol{\\theta})\\, v_{1,k} + b(\\boldsymbol{\\theta})\\, i_k,\n$$\nwhere $\\boldsymbol{\\theta} = [R_0, R_1, C_1]^\\top$, $a(\\boldsymbol{\\theta}) = \\exp\\!\\left(-\\dfrac{dt}{R_1 C_1}\\right)$, and $b(\\boldsymbol{\\theta}) = R_1 \\left(1 - a(\\boldsymbol{\\theta})\\right)$. The measured terminal voltage (relative to open-circuit voltage) is\n$$\ny_k = - R_0\\, i_k - v_{1,k} + \\varepsilon_k,\n$$\nwith independent and identically distributed Gaussian measurement noise $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nFor experiment design under uncertainty, work with normalized, dimensionless parameters defined by $\\boldsymbol{\\phi} = \\boldsymbol{\\theta} \\oslash \\boldsymbol{\\theta}_0$, where $\\boldsymbol{\\theta}_0$ is a nominal parameter vector and $\\oslash$ denotes element-wise division. Assume a Gaussian prior on $\\boldsymbol{\\phi}$, $\\boldsymbol{\\phi} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\mathbf{C}_{\\text{prior}})$ with diagonal covariance $\\mathbf{C}_{\\text{prior}} = \\operatorname{diag}(\\sigma_{\\phi,0}^2, \\sigma_{\\phi,1}^2, \\sigma_{\\phi,2}^2)$.\n\nUnder a first-order linearization of the measurement model around $\\boldsymbol{\\theta}_0$, the Fisher information matrix for the normalized parameters under input $\\{i_k\\}$ is\n$$\n\\mathbf{F} = \\frac{1}{\\sigma^2}\\, \\mathbf{J}_\\phi^\\top \\mathbf{J}_\\phi,\n$$\nwhere $\\mathbf{J}_\\phi \\in \\mathbb{R}^{N \\times 3}$ is the Jacobian of the noiseless output sequence $\\mathbf{y}(\\boldsymbol{\\theta}, \\{i_k\\})$ with respect to $\\boldsymbol{\\phi}$, evaluated at $\\boldsymbol{\\theta}_0$. The posterior covariance under the linear-Gaussian approximation is\n$$\n\\mathbf{C}_{\\text{post}} = \\left(\\mathbf{C}_{\\text{prior}}^{-1} + \\mathbf{F}\\right)^{-1}.\n$$\nYour design objective is to minimize the scalar functional\n$$\nJ(\\{i_k\\}) = \\operatorname{tr}\\!\\left(\\mathbf{C}_{\\text{post}}\\right),\n$$\nwhich is dimensionless because it is defined for the normalized parameters $\\boldsymbol{\\phi}$.\n\nThe input current sequence must satisfy the following constraints:\n- Amplitude bound: $|i_k| \\le u_{\\max}$ for all $k$.\n- Energy bound: $\\sum_{k=0}^{N-1} i_k^2\\, dt \\le E_{\\max}$.\nAll currents must be in amperes ($\\mathrm{A}$), time in seconds ($\\mathrm{s}$), resistances in ohms ($\\Omega$), capacitances in farads ($\\mathrm{F}$), and voltages in volts ($\\mathrm{V}$). The output objective $J(\\{i_k\\})$ is dimensionless.\n\nTo make the problem algorithmically testable, your program must evaluate a fixed set of candidate input sequence families (defined as unit-amplitude templates to be subsequently scaled to meet both constraints), choose the one that minimizes $J(\\{i_k\\})$, and report the result. The candidate templates, indexed by $c \\in \\{0,1,2,3,4,5,6,7,8\\}$, are:\n- $c=0$: zero sequence, $i_k = 0$.\n- $c=1$: step sequence with sign reversal halfway, $i_k = 1$ for $k  N/2$, and $i_k = -1$ for $k \\ge N/2$.\n- $c=2$: single sinusoid with $1$ cycle over the horizon, $i_k = \\sin\\left(2\\pi \\frac{1}{N} k\\right)$.\n- $c=3$: sinusoid with $3$ cycles, $i_k = \\sin\\left(2\\pi \\frac{3}{N} k\\right)$.\n- $c=4$: sinusoid with $10$ cycles, $i_k = \\sin\\left(2\\pi \\frac{10}{N} k\\right)$.\n- $c=5$: pseudo-random binary sequence (deterministic using a fixed seed), $i_k \\in \\{-1, +1\\}$.\n- $c=6$: white-noise sequence (deterministic using a fixed seed), $i_k$ i.i.d. from a standard normal template.\n- $c=7$: impulse pair, $i_{k^+} = +1$ at $k^+ = \\lfloor N/4 \\rfloor$, $i_{k^-} = -1$ at $k^- = \\lfloor 3N/4 \\rfloor$, and $i_k = 0$ otherwise.\n- $c=8$: two-sine combination, $i_k = \\sin\\left(2\\pi \\frac{1}{N} k\\right) + \\sin\\left(2\\pi \\frac{3}{N} k\\right)$.\n\nFor each candidate template, scale by a factor $s$ chosen to satisfy both constraints: $s \\le u_{\\max}$ and $s \\le \\sqrt{\\frac{E_{\\max}}{dt \\sum_{k=0}^{N-1} i_k^{(\\text{template})\\,2}}}$.\nUse $s = \\min\\left(u_{\\max}, \\sqrt{\\frac{E_{\\max}}{dt \\sum i_k^{(\\text{template})\\,2}}}\\right)$, and define the realized input sequence as $i_k^{(\\text{realized})} = s\\, i_k^{(\\text{template})}$.\n\nCompute $\\mathbf{J}_\\phi$ via finite differences of the noiseless output sequence with respect to each parameter component of $\\boldsymbol{\\theta}$ at $\\boldsymbol{\\theta}_0$, then convert to normalized parameters by multiplying each column by the corresponding nominal parameter component. Explicitly, for a small perturbation $\\delta\\theta_p$ to the $p$-th component,\n$$\n\\left[\\mathbf{J}_\\theta\\right]_{:,p} \\approx \\frac{\\mathbf{y}(\\boldsymbol{\\theta}_0 + \\delta\\theta_p\\, \\mathbf{e}_p, \\{i_k\\}) - \\mathbf{y}(\\boldsymbol{\\theta}_0, \\{i_k\\})}{\\delta\\theta_p}\n$$\n$$\n\\left[\\mathbf{J}_\\phi\\right]_{:,p} = \\theta_{0,p}\\, \\left[\\mathbf{J}_\\theta\\right]_{:,p}\n$$\n\nTest suite. Use the following three cases, each provided as a tuple $(N, dt, \\boldsymbol{\\theta}_0, \\boldsymbol{\\sigma}_\\phi, \\sigma, u_{\\max}, E_{\\max})$:\n- Case A (happy path): $N = 60$, $dt = 1$, $\\boldsymbol{\\theta}_0 = [0.02, 0.05, 2000]$, $\\boldsymbol{\\sigma}_\\phi = [0.5, 0.5, 0.5]$, $\\sigma = 0.01$, $u_{\\max} = 5$, $E_{\\max} = 600$.\n- Case B (low energy): $N = 60$, $dt = 1$, $\\boldsymbol{\\theta}_0 = [0.02, 0.05, 2000]$, $\\boldsymbol{\\sigma}_\\phi = [0.5, 0.5, 0.5]$, $\\sigma = 0.01$, $u_{\\max} = 5$, $E_{\\max} = 10$.\n- Case C (amplitude-limited): $N = 60$, $dt = 1$, $\\boldsymbol{\\theta}_0 = [0.02, 0.05, 2000]$, $\\boldsymbol{\\sigma}_\\phi = [0.5, 0.5, 0.5]$, $\\sigma = 0.01$, $u_{\\max} = 0.5$, $E_{\\max} = 600$.\n\nOutput specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the triple $[c^\\star, J^\\star, E_{\\text{used}}]$, where $c^\\star$ is the integer index of the best candidate template, $J^\\star$ is the minimized dimensionless objective value, and $E_{\\text{used}} = dt \\sum_k (i_k^{(\\text{realized})})^2$ is the realized input energy in $\\mathrm{A}^2\\cdot\\mathrm{s}$ expressed as a decimal number without the unit symbol. The overall output should be a list of these three triples in the order of the test suite cases, for example, $[[c_1,J_1,E_1],[c_2,J_2,E_2],[c_3,J_3,E_3]]$.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in established principles of electrical circuit theory and system identification, is mathematically well-posed, and is defined with objective, formal language. All necessary data and constraints are provided to arrive at a unique, computable solution.\n\nThe problem requires us to find the optimal input current sequence $\\{i_k\\}_{k=0}^{N-1}$ for a single resistor-capacitor (RC) battery model from a given set of candidate sequences. The optimality criterion is the minimization of the trace of the posterior covariance matrix of the model's normalized parameters, a measure of parameter uncertainty after an experiment. This is a classic A-optimal experiment design problem.\n\nThe model is a Thevenin equivalent circuit with parameters $\\boldsymbol{\\theta} = [R_0, R_1, C_1]^\\top$. The system dynamics are given in discrete time:\n$$\nv_{1,k+1} = a(\\boldsymbol{\\theta})\\, v_{1,k} + b(\\boldsymbol{\\theta})\\, i_k\n$$\n$$\ny_k = - R_0\\, i_k - v_{1,k}\n$$\nwhere $v_{1,k}$ is the voltage across the RC pair, $y_k$ is the noiseless terminal voltage measurement relative to the open-circuit voltage, and the coefficients are functions of the parameters:\n$$\na(\\boldsymbol{\\theta}) = \\exp\\left(-\\frac{dt}{R_1 C_1}\\right)\n$$\n$$\nb(\\boldsymbol{\\theta}) = R_1 \\left(1 - a(\\boldsymbol{\\theta})\\right)\n$$\nWe assume the initial condition $v_{1,0} = 0$, representing a system at rest.\n\nThe objective is to minimize $J(\\{i_k\\}) = \\operatorname{tr}(\\mathbf{C}_{\\text{post}})$, where the posterior covariance matrix for the normalized parameters $\\boldsymbol{\\phi} = \\boldsymbol{\\theta} \\oslash \\boldsymbol{\\theta}_0$ is given by the Bayesian update formula under a linear-Gaussian approximation:\n$$\n\\mathbf{C}_{\\text{post}} = \\left(\\mathbf{C}_{\\text{prior}}^{-1} + \\mathbf{F}\\right)^{-1}\n$$\nThe prior covariance $\\mathbf{C}_{\\text{prior}}$ is diagonal with entries $\\sigma_{\\phi,p}^2$. The Fisher Information Matrix (FIM) $\\mathbf{F}$ is:\n$$\n\\mathbf{F} = \\frac{1}{\\sigma^2}\\, \\mathbf{J}_\\phi^\\top \\mathbf{J}_\\phi\n$$\nHere, $\\mathbf{J}_\\phi \\in \\mathbb{R}^{N \\times 3}$ is the sensitivity Jacobian of the output sequence $\\mathbf{y} = [y_0, \\dots, y_{N-1}]^\\top$ with respect to the normalized parameters $\\boldsymbol{\\phi}$, evaluated at the nominal parameter values $\\boldsymbol{\\theta}_0$.\n\nThe algorithmic procedure to solve this problem for each test case is as follows:\n\n1.  **Iterate through Candidate Sequences**: For each candidate template $c \\in \\{0, 1, \\dots, 8\\}$, an input current sequence $\\{i_k\\}$ is generated and evaluated.\n\n2.  **Generate and Scale Input Sequence**:\n    a.  A raw template sequence $\\{i_k^{(\\text{raw})}\\}$ is generated according to the definition for candidate $c$. For pseudo-random sequences ($c=5, 6$), a fixed random seed is used to ensure deterministic and reproducible results.\n    b.  The problem refers to \"unit-amplitude templates\". To ensure this property and maintain consistency across all candidates, each raw template is normalized by its maximum absolute value. Let $M = \\max_k |i_k^{(\\text{raw})}|$. The unit-amplitude template is $\\{i_k^{(\\text{template})}\\} = \\{i_k^{(\\text{raw})} / M\\}$ if $M > 0$, and $\\{0\\}$ otherwise.\n    c.  A scaling factor $s$ is calculated to ensure the final sequence respects both the amplitude constraint ($|i_k| \\le u_{\\max}$) and the energy constraint ($\\sum_{k=0}^{N-1} i_k^2\\, dt \\le E_{\\max}$). With a unit-amplitude template, $|i_k^{(\\text{template})}| \\le 1$, so the amplitude constraint becomes $s \\le u_{\\max}$. The energy constraint leads to $s \\le \\sqrt{E_{\\max} / (dt \\sum_k (i_k^{(\\text{template})})^2)}$. The optimal scaling factor that maximally uses the allowed input range is:\n        $$\n        s = \\min\\left(u_{\\max}, \\sqrt{\\frac{E_{\\max}}{dt \\sum_{k=0}^{N-1} (i_k^{(\\text{template})})^2}}\\right)\n        $$\n        A special case for all-zero templates (e.g., $c=0$) is handled to avoid division by zero, where $s$ is effectively $0$.\n    d.  The realized input sequence is then $\\{i_k^{(\\text{realized})}\\} = s \\cdot \\{i_k^{(\\text{template})}\\}$.\n\n3.  **Calculate the Jacobian $\\mathbf{J}_\\phi$**:\n    a.  The Jacobian of the output $\\mathbf{y}$ with respect to the physical parameters $\\boldsymbol{\\theta}$ is computed via numerical forward differentiation around the nominal values $\\boldsymbol{\\theta}_0$. For each parameter $\\theta_p \\in \\{R_0, R_1, C_1\\}$, a small perturbation $\\delta\\theta_p$ is introduced. A robust choice for the perturbation size is $\\delta\\theta_p = \\theta_{0,p} \\sqrt{\\epsilon_{\\text{machine}}}$, where $\\epsilon_{\\text{machine}}$ is the machine epsilon for floating-point numbers.\n    b.  The $p$-th column of the Jacobian $\\mathbf{J}_\\theta$ is approximated as:\n        $$\n        [\\mathbf{J}_\\theta]_{:,p} \\approx \\frac{\\mathbf{y}(\\boldsymbol{\\theta}_0 + \\delta\\theta_p \\mathbf{e}_p, \\{i_k^{(\\text{realized})}\\}) - \\mathbf{y}(\\boldsymbol{\\theta}_0, \\{i_k^{(\\text{realized})}\\})}{\\delta\\theta_p}\n        $$\n        where $\\mathbf{y}(\\cdot)$ is the simulated noiseless output sequence.\n    c.  Using the chain rule, the Jacobian with respect to the normalized parameters $\\boldsymbol{\\phi}$ is obtained by scaling each column of $\\mathbf{J}_\\theta$:\n        $$\n        [\\mathbf{J}_\\phi]_{:,p} = \\theta_{0,p} [\\mathbf{J}_\\theta]_{:,p}\n        $$\n\n4.  **Evaluate the Objective Function $J$**:\n    a.  The inverse of the diagonal prior covariance matrix is calculated: $\\mathbf{C}_{\\text{prior}}^{-1} = \\operatorname{diag}(1/\\sigma_{\\phi,0}^2, 1/\\sigma_{\\phi,1}^2, 1/\\sigma_{\\phi,2}^2)$.\n    b.  The FIM is computed: $\\mathbf{F} = (1/\\sigma^2) \\mathbf{J}_\\phi^\\top \\mathbf{J}_\\phi$.\n    c.  The posterior covariance is found by matrix inversion: $\\mathbf{C}_{\\text{post}} = (\\mathbf{C}_{\\text{prior}}^{-1} + \\mathbf{F})^{-1}$.\n    d.  The objective function value is the trace of this matrix: $J = \\operatorname{tr}(\\mathbf{C}_{\\text{post}})$. For the zero-input case ($c=0$), $\\mathbf{J}_\\phi$ is a zero matrix, so $\\mathbf{F}=\\mathbf{0}$, $\\mathbf{C}_{\\text{post}} = \\mathbf{C}_{\\text{prior}}$, and $J = \\operatorname{tr}(\\mathbf{C}_{\\text{prior}})$.\n\n5.  **Select the Optimum**: The procedure is repeated for all candidate templates. The candidate index $c^\\star$ that yields the minimum objective value $J^\\star$ is selected as the optimal choice. The energy used by this optimal sequence, $E_{\\text{used}} = dt \\sum_k (i_{k,\\text{opt}}^{(\\text{realized})})^2$, is also recorded.\n\nThis process is applied to each of the three test cases provided. The final output is a list containing the triples $[c^\\star, J^\\star, E_{\\text{used}}]$ for each case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the optimal experiment design problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case A: Happy path\n        (60, 1.0, np.array([0.02, 0.05, 2000.0]), np.array([0.5, 0.5, 0.5]), 0.01, 5.0, 600.0),\n        # Case B: Low energy\n        (60, 1.0, np.array([0.02, 0.05, 2000.0]), np.array([0.5, 0.5, 0.5]), 0.01, 5.0, 10.0),\n        # Case C: Amplitude-limited\n        (60, 1.0, np.array([0.02, 0.05, 2000.0]), np.array([0.5, 0.5, 0.5]), 0.01, 0.5, 600.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, dt, theta0, sigma_phi, sigma, u_max, E_max = case\n        \n        # Reset the seed for each test case to ensure independence\n        np.random.seed(0)\n\n        best_J = float('inf')\n        best_c = -1\n        best_E_used = -1\n\n        for c in range(9):\n            # 1. Generate Raw Template\n            i_raw = generate_template(c, N)\n\n            # 2. Normalize to a unit-amplitude template\n            max_abs_raw = np.max(np.abs(i_raw))\n            if max_abs_raw > 0:\n                i_template = i_raw / max_abs_raw\n            else:\n                i_template = i_raw  # Already all zeros\n\n            # 3. Calculate scaling factor s and realized input\n            template_energy_sum_sq = np.sum(i_template**2)\n            if template_energy_sum_sq == 0:\n                s = 0.0\n            else:\n                s_energy = np.sqrt(E_max / (dt * template_energy_sum_sq))\n                s = min(u_max, s_energy)\n            \n            i_realized = s * i_template\n\n            # 4. Calculate objective function J\n            current_J = calculate_objective(i_realized, N, dt, theta0, sigma_phi, sigma)\n\n            # 5. Update best result\n            if current_J  best_J:\n                best_J = current_J\n                best_c = c\n                best_E_used = dt * np.sum(i_realized**2)\n        \n        results.append([best_c, best_J, best_E_used])\n\n    # Format the output as a string representation of a list of lists.\n    # The problem asks for a very specific representation.\n    # We build it manually to ensure correctness.\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        c_star, J_star, E_used = res\n        output_str += f\"[{c_star},{J_star:.7f},{E_used:.7f}]\"\n        if i  len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\n\ndef generate_template(c, N):\n    \"\"\"Generates the raw input current template for a given candidate index c.\"\"\"\n    k = np.arange(N)\n    if c == 0:\n        return np.zeros(N)\n    elif c == 1:\n        i = np.ones(N)\n        i[N//2:] = -1.0\n        return i\n    elif c == 2:\n        return np.sin(2 * np.pi * 1 * k / N)\n    elif c == 3:\n        return np.sin(2 * np.pi * 3 * k / N)\n    elif c == 4:\n        return np.sin(2 * np.pi * 10 * k / N)\n    elif c == 5:\n        # PRBS needs a fixed seed, handled in the main loop\n        return 2 * np.random.randint(0, 2, size=N) - 1\n    elif c == 6:\n        # White noise needs a fixed seed, handled in the main loop\n        return np.random.randn(N)\n    elif c == 7:\n        i = np.zeros(N)\n        k_plus = int(np.floor(N / 4))\n        k_minus = int(np.floor(3 * N / 4))\n        i[k_plus] = 1.0\n        i[k_minus] = -1.0\n        return i\n    elif c == 8:\n        return np.sin(2 * np.pi * 1 * k / N) + np.sin(2 * np.pi * 3 * k / N)\n    else:\n        raise ValueError(\"Invalid candidate index\")\n\n\ndef simulate_model(i_seq, theta, N, dt):\n    \"\"\"Simulates the noiseless voltage output of the battery model.\"\"\"\n    R0, R1, C1 = theta\n    \n    if R1 = 0 or C1 = 0:\n        return np.full(N, np.nan)\n        \n    a = np.exp(-dt / (R1 * C1))\n    b = R1 * (1 - a)\n    \n    v1 = np.zeros(N)\n    # The loop runs up to N-2, calculating v1[1] to v1[N-1]\n    for k in range(N - 1):\n        v1[k + 1] = a * v1[k] + b * i_seq[k]\n        \n    y = -R0 * i_seq - v1\n    return y\n\n\ndef calculate_objective(i_seq, N, dt, theta0, sigma_phi, sigma):\n    \"\"\"Calculates the objective function J = tr(C_post).\"\"\"\n    \n    num_params = len(theta0)\n    \n    # Handle the zero-input case, which gives no information\n    if np.all(i_seq == 0):\n        C_prior = np.diag(sigma_phi**2)\n        return np.trace(C_prior)\n\n    # Calculate Jacobian J_phi\n    J_phi = np.zeros((N, num_params))\n    y_nominal = simulate_model(i_seq, theta0, N, dt)\n    \n    # Machine epsilon for finite difference perturbation\n    eps = np.finfo(float).eps\n\n    for p in range(num_params):\n        theta_p = theta0.copy()\n        delta = theta_p[p] * np.sqrt(eps)\n        if delta == 0: # Handle cases where theta0 component is 0\n            delta = np.sqrt(eps)\n        theta_p[p] += delta\n        \n        y_perturbed = simulate_model(i_seq, theta_p, N, dt)\n        \n        J_theta_col_p = (y_perturbed - y_nominal) / delta\n        J_phi[:, p] = J_theta_col_p * theta0[p]\n\n    # Calculate Fisher Information Matrix (FIM)\n    F = (1 / sigma**2) * (J_phi.T @ J_phi)\n    \n    # Calculate Posterior Covariance\n    C_prior_inv = np.diag(1.0 / sigma_phi**2)\n    C_post_inv = C_prior_inv + F\n    \n    try:\n        C_post = np.linalg.inv(C_post_inv)\n    except np.linalg.LinAlgError:\n        # If matrix is singular, posterior uncertainty is infinite\n        return float('inf')\n        \n    # Return the trace\n    return np.trace(C_post)\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}