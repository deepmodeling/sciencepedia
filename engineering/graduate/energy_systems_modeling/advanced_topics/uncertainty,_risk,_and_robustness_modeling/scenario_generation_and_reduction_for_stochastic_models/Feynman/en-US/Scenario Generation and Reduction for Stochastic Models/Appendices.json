{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of scenario-based modeling is the ability to approximate a complex probability distribution with a small, tractable set of weighted points. This practice delves into the classical method of moment matching, a powerful technique for ensuring that your simplified model preserves key statistical characteristics of the original uncertainty. By working through the derivation of the underlying linear constraints and solving for the scenario weights, you will gain a foundational understanding of how a few representative scenarios can be rigorously calibrated to reflect the broader reality they are meant to capture .",
            "id": "4121537",
            "problem": "In long-term planning for electricity systems, uncertain net load $X$ is often represented by a finite set of scenarios to enable tractable optimization of operational and investment decisions. Consider the approximation of the distribution of $X$ by a discrete measure supported at representative points $\\xi_i$ with scenario weights $p_i$, where $i=1,\\dots,m$. The decision maker requires that the first $k$ moments of the approximation match the corresponding moments of $X$ to preserve key statistical characteristics used in system sizing and reliability assessment. Let $\\phi_j:\\mathbb{R}\\to\\mathbb{R}$ be fixed basis functions associated with the first $k$ moments, for $j=1,\\dots,k$.\n\n(a) Starting only from the definition of mathematical expectation and the construction of a discrete scenario approximation for a random variable, derive the linear constraints that the scenario weights $p_i$ must satisfy in order to preserve the first $k$ moments induced by the basis functions $\\phi_j$. Your derivation must explicitly justify why these constraints are linear in the weights and must include the normalization of the weights.\n\n(b) In the context of a simplified distributionally robust scenario reduction for net load in an energy system, suppose a planner chooses $m=3$ representative scenarios at $\\xi_1 = 50 \\text{ MW}$, $\\xi_2 = 120 \\text{ MW}$, and $\\xi_3 = 200 \\text{ MW}$. The planner wishes to preserve the first two moments of $X$, with known targets $\\mathbb{E}[X] = 120 \\text{ MW}$ and $\\mathbb{E}[X^2] = 15300 \\text{ (MW)}^2$, by taking the moment basis functions to be $\\phi_1(x)=x$ and $\\phi_2(x)=x^2$. Impose the normalization $\\sum_{i=1}^{3} p_i = 1$ and compute the scenario weights $p_1$, $p_2$, and $p_3$ that exactly preserve these two moments. Report the final weights as a row matrix $\\begin{pmatrix} p_1  p_2  p_3 \\end{pmatrix}$ in exact fractional form. The weights are dimensionless, so no units are required in the final report. No rounding is required.",
            "solution": "The problem posed is valid. It is scientifically grounded in probability theory and its application to stochastic modeling in energy systems, a well-established field. The problem is well-posed, providing all necessary data and relationships to form a solvable system of linear equations. The language is objective and the parameters are physically plausible. We may therefore proceed with a full solution.\n\n(a) Derivation of Linear Constraints\n\nLet $X$ be a continuous random variable representing the uncertain net load, with a probability density function denoted by $f_X(x)$. The expectation of a function $g(X)$ is defined by the integral:\n$$ \\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\,dx $$\nThe problem states that we are interested in the first $k$ moments of $X$, which are induced by a set of $k$ basis functions $\\phi_j:\\mathbb{R}\\to\\mathbb{R}$ for $j=1, \\dots, k$. The $j$-th moment, denoted $\\mu_j$, is the expected value of the corresponding basis function $\\phi_j(X)$:\n$$ \\mu_j = \\mathbb{E}[\\phi_j(X)] = \\int_{-\\infty}^{\\infty} \\phi_j(x) f_X(x) \\,dx $$\nThese values $\\mu_j$ are considered known target quantities.\n\nThe distribution of $X$ is approximated by a discrete measure. This means we replace the continuous random variable $X$ with a discrete random variable, which we can call $\\tilde{X}$. This discrete variable $\\tilde{X}$ can take one of $m$ specific values, $\\{\\xi_1, \\xi_2, \\dots, \\xi_m\\}$, which are the representative points. Each point $\\xi_i$ is assigned a scenario weight, or probability, $p_i$, for $i=1, \\dots, m$. For these weights to form a valid probability distribution, they must be non-negative ($p_i \\ge 0$) and sum to unity. The problem explicitly requires the inclusion of this normalization constraint:\n$$ \\sum_{i=1}^{m} p_i = 1 $$\nThe expectation of a function $g(\\tilde{X})$ for this discrete random variable is given by a weighted sum:\n$$ \\mathbb{E}[g(\\tilde{X})] = \\sum_{i=1}^{m} g(\\xi_i) p_i $$\nThe core requirement is that the moments of the discrete approximation $\\tilde{X}$ must match the moments of the original random variable $X$. This means that for each basis function $\\phi_j$, the following equality must hold:\n$$ \\mathbb{E}[\\phi_j(\\tilde{X})] = \\mathbb{E}[\\phi_j(X)] $$\nSubstituting the definitions for the discrete and continuous expectations, we obtain the moment-matching constraints:\n$$ \\sum_{i=1}^{m} \\phi_j(\\xi_i) p_i = \\mu_j \\quad \\text{for } j=1, \\dots, k $$\nTo justify why these constraints are linear, we must identify the variables and coefficients. The unknown quantities to be determined are the scenario weights $p_1, p_2, \\dots, p_m$. The representative points $\\xi_i$ and the basis functions $\\phi_j$ are given and fixed. Consequently, for any given $i$ and $j$, the value of $\\phi_j(\\xi_i)$ is a constant scalar. Let us define these constants as $A_{ji} = \\phi_j(\\xi_i)$. The moment-matching equations can then be rewritten as:\n$$ \\sum_{i=1}^{m} A_{ji} p_i = \\mu_j \\quad \\text{for } j=1, \\dots, k $$\nEach of these equations expresses the target moment $\\mu_j$ as a linear combination of the unknown weights $p_i$ with constant coefficients $A_{ji}$. By definition, this is a system of $k$ linear equations. The normalization condition, $\\sum_{i=1}^{m} p_i = 1$, is also a linear equation in the variables $p_i$.\n\nTherefore, the complete set of constraints that the scenario weights $p_i$ must satisfy consists of $k+1$ linear equations:\n1. Normalization: $\\sum_{i=1}^{m} p_i = 1$\n2. Moment-matching: $\\sum_{i=1}^{m} \\phi_j(\\xi_i) p_i = \\mu_j$ for $j=1, \\dots, k$\n\n(b) Computation of Scenario Weights\n\nWe are given $m=3$ scenarios with representative points $\\xi_1 = 50$, $\\xi_2 = 120$, and $\\xi_3 = 200$. We wish to preserve the first $k=2$ moments using basis functions $\\phi_1(x) = x$ and $\\phi_2(x) = x^2$. The target moments are $\\mathbb{E}[X] = \\mu_1 = 120$ and $\\mathbb{E}[X^2] = \\mu_2 = 15300$. We must also impose the normalization constraint $\\sum_{i=1}^{3} p_i = 1$.\n\nThis setup yields a system of $3$ linear equations in the $3$ unknown weights $p_1, p_2, p_3$:\n1. Normalization: $p_1 + p_2 + p_3 = 1$\n2. First moment ($\\mathbb{E}[\\tilde{X}] = \\mu_1$): $\\xi_1 p_1 + \\xi_2 p_2 + \\xi_3 p_3 = 120 \\implies 50 p_1 + 120 p_2 + 200 p_3 = 120$\n3. Second moment ($\\mathbb{E}[\\tilde{X}^2] = \\mu_2$): $\\xi_1^2 p_1 + \\xi_2^2 p_2 + \\xi_3^2 p_3 = 15300 \\implies 50^2 p_1 + 120^2 p_2 + 200^2 p_3 = 15300$\n\nLet's write the system explicitly:\n$$\n\\begin{cases}\np_1 + p_2 + p_3 = 1 \\\\\n50 p_1 + 120 p_2 + 200 p_3 = 120 \\\\\n2500 p_1 + 14400 p_2 + 40000 p_3 = 15300\n\\end{cases}\n$$\nWe can simplify the second and third equations by dividing by common factors. Dividing the second equation by $10$ and the third by $100$:\n$$\n\\begin{cases}\np_1 + p_2 + p_3 = 1 \\quad (1)\\\\\n5 p_1 + 12 p_2 + 20 p_3 = 12 \\quad (2)\\\\\n25 p_1 + 144 p_2 + 400 p_3 = 153 \\quad (3)\n\\end{cases}\n$$\nFrom equation $(1)$, we express $p_3$ in terms of $p_1$ and $p_2$: $p_3 = 1 - p_1 - p_2$. We substitute this into equations $(2)$ and $(3)$.\n\nSubstitution into $(2)$:\n$5 p_1 + 12 p_2 + 20(1 - p_1 - p_2) = 12$\n$5 p_1 + 12 p_2 + 20 - 20 p_1 - 20 p_2 = 12$\n$-15 p_1 - 8 p_2 = -8 \\implies 15 p_1 + 8 p_2 = 8 \\quad (A)$\n\nSubstitution into $(3)$:\n$25 p_1 + 144 p_2 + 400(1 - p_1 - p_2) = 153$\n$25 p_1 + 144 p_2 + 400 - 400 p_1 - 400 p_2 = 153$\n$-375 p_1 - 256 p_2 = -247 \\implies 375 p_1 + 256 p_2 = 247 \\quad (B)$\n\nNow we solve the $2 \\times 2$ system for $p_1$ and $p_2$. From equation $(A)$, we isolate $p_2$:\n$8 p_2 = 8 - 15 p_1 \\implies p_2 = 1 - \\frac{15}{8} p_1$\n\nSubstitute this expression for $p_2$ into equation $(B)$:\n$375 p_1 + 256 \\left(1 - \\frac{15}{8} p_1\\right) = 247$\n$375 p_1 + 256 - \\frac{256 \\times 15}{8} p_1 = 247$\nSince $\\frac{256}{8} = 32$, the equation becomes:\n$375 p_1 + 256 - (32 \\times 15) p_1 = 247$\n$375 p_1 + 256 - 480 p_1 = 247$\n$-105 p_1 = -9$\n$p_1 = \\frac{9}{105} = \\frac{3}{35}$\n\nNow, we find $p_2$:\n$p_2 = 1 - \\frac{15}{8} p_1 = 1 - \\frac{15}{8} \\left(\\frac{3}{35}\\right) = 1 - \\frac{45}{280} = 1 - \\frac{9}{56} = \\frac{56-9}{56} = \\frac{47}{56}$\n\nFinally, we find $p_3$ using $p_3 = 1 - p_1 - p_2$:\n$p_3 = 1 - \\frac{3}{35} - \\frac{47}{56}$\nTo sum the fractions, we find a common denominator for $35 = 5 \\times 7$ and $56 = 8 \\times 7$. The least common multiple is $5 \\times 8 \\times 7 = 280$.\n$p_3 = \\frac{280}{280} - \\frac{3 \\times 8}{35 \\times 8} - \\frac{47 \\times 5}{56 \\times 5} = \\frac{280}{280} - \\frac{24}{280} - \\frac{235}{280} = \\frac{280 - 24 - 235}{280} = \\frac{21}{280}$\nSimplifying the fraction for $p_3$ by dividing the numerator and denominator by their greatest common divisor, $7$:\n$p_3 = \\frac{21 \\div 7}{280 \\div 7} = \\frac{3}{40}$\n\nThe computed weights are $p_1 = \\frac{3}{35}$, $p_2 = \\frac{47}{56}$, and $p_3 = \\frac{3}{40}$. All weights are positive, so they constitute a valid probability distribution. The final answer is presented as a row matrix in exact fractional form.",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{3}{35}  \\frac{47}{56}  \\frac{3}{40} \\end{pmatrix}} $$"
        },
        {
            "introduction": "This practice guides you through building a complete, end-to-end scenario generation and reduction pipeline, a core task for any energy systems modeler. You will start by implementing Latin Hypercube Sampling (LHS), a sophisticated technique that guarantees superior coverage of high-dimensional uncertain spaces compared to simple random sampling. You will then pair this advanced generation method with a practical greedy forward selection algorithm to intelligently reduce a large set of scenarios to a manageable, representative subset, providing you with a powerful and practical computational tool .",
            "id": "4121597",
            "problem": "You are tasked with deriving, implementing, and validating a principled scenario generation and reduction method for stochastic models as encountered in energy systems modeling, based on Latin Hypercube Sampling (LHS). Begin from foundational probability constructs and derive why Latin Hypercube Sampling (LHS) yields stratified coverage for each marginal, and how empirical inverse cumulative distribution functions can map uniform stratified samples to arbitrary univariate marginal distributions. Then design an algorithm to reduce the generated scenarios to a smaller representative subset by minimizing a weighted distance between the original and reduced discrete distributions.\n\nFundamental base:\n- Let $U$ be a random variable with the uniform distribution on the unit interval, denoted $U \\sim \\mathrm{Unif}(0,1)$, with cumulative distribution function $F_U(u) = u$ for $u \\in [0,1]$.\n- For any univariate random variable $X$ with cumulative distribution function $F_X(x)$, the inverse transform sampling principle states that $X = F_X^{-1}(U)$ has cumulative distribution function $F_X(x)$ when $U \\sim \\mathrm{Unif}(0,1)$ and $F_X^{-1}$ is the generalized inverse cumulative distribution function.\n- Stratified sampling on $[0,1]$ divides the interval into $N$ disjoint strata $I_i = \\left[\\frac{i}{N}, \\frac{i+1}{N}\\right)$ for $i \\in \\{0,1,\\dots,N-1\\}$ and draws one point uniformly from each $I_i$.\n\nDefinitions and task requirements:\n- Latin Hypercube Sampling (LHS) is a stratified sampling method which, for a $d$-dimensional random vector with independent marginals, produces $N$ samples such that, for each marginal dimension $j \\in \\{1,\\dots,d\\}$, each stratum $I_i$ in $[0,1]$ is used exactly once across the $N$ samples. This is achieved by independently permuting the $N$ strata indices in each dimension and then drawing one uniform point within each stratum, paired across dimensions according to the independent permutations.\n- In energy systems modeling, empirical marginal distributions are often available as monotone sequences of values that approximate the inverse cumulative distribution function through piecewise-linear interpolation. For a given dimension $j$ with empirical support values $v^{(j)}_0 \\le v^{(j)}_1 \\le \\dots \\le v^{(j)}_{L_j-1}$ at equally spaced empirical quantiles $q_k = \\frac{k}{L_j-1}$ for $k \\in \\{0,\\dots,L_j-1\\}$, define the empirical inverse cumulative distribution function approximation $\\widehat{F}_j^{-1}(p)$ by linear interpolation of $(q_k, v^{(j)}_k)$, which maps $p \\in [0,1]$ to a value in the support of the empirical marginal.\n\nScenario reduction objective:\n- Let the generated $N$ samples be $\\{x_i\\}_{i=1}^N \\subset \\mathbb{R}^d$, and let the discrete empirical distribution assign equal weights $w_i = \\frac{1}{N}$ to each $x_i$. The reduction to $M$ representative scenarios selects a subset $S \\subset \\{1,\\dots,N\\}$ with $|S|=M$ to minimize the weighted distance objective\n$$\nJ(S) = \\sum_{i=1}^N w_i \\min_{s \\in S} \\left\\|x_i - x_s\\right\\|_2,\n$$\nwhere $\\left\\|\\cdot\\right\\|_2$ is the Euclidean norm. The minimization of $J(S)$ approximates the first-order Wasserstein (Kantorovich) distance between the original and reduced discrete measures.\n\nWhat you must implement:\n- Construct a reproducible algorithm that:\n  $1$) Generates $N$-by-$d$ uniform stratified samples using Latin Hypercube Sampling (LHS) with independent permutations per dimension.\n  $2$) Maps these uniform samples to $d$ empirical marginals via the empirical inverse cumulative distribution functions defined by piecewise-linear interpolation at equally spaced empirical quantiles.\n  $3$) Performs greedy forward selection scenario reduction to $M$ scenarios by iteratively adding the candidate index $c \\in \\{1,\\dots,N\\}$ that yields the largest decrease in the objective $J(S)$ until $|S|=M$. If $M \\ge N$, select all samples and the objective is $0$.\n  $4$) Validates the stratification property by checking, for each dimension $j$, that the set of stratum indices $\\left\\{\\left\\lfloor N u_{ij}\\right\\rfloor: i=1,\\dots,N\\right\\}$ equals $\\{0,1,\\dots,N-1\\}$, where $u_{ij}$ denotes the uniform sample in dimension $j$ for sample $i$.\n  $5$) Computes the scenario reduction objective $J(S)$ and the sample means for each dimension after mapping to empirical marginals.\n\nImplementation details:\n- Use a fixed random seed $12345$ for reproducibility.\n- The per-dimension empirical inverse cumulative distribution function must be implemented as piecewise-linear interpolation between points $(q_k, v^{(j)}_k)$ with $q_k = \\frac{k}{L_j-1}$ for $k \\in \\{0,\\dots,L_j-1\\}$.\n\nTest suite:\nProvide the following test cases, where each \"values list\" is the list of empirical support arrays for each dimension:\n- Case $1$ (general happy path): $N=10$, $d=3$, $M=4$, empirical supports\n  $v^{(1)} = (0.8, 1.0, 1.3, 1.35, 1.5, 1.7, 2.0, 2.3, 2.5, 2.8, 3.0)$,\n  $v^{(2)} = (50, 60, 75, 90, 120, 150, 200)$,\n  $v^{(3)} = (0.1, 0.2, 0.5, 0.9, 1.2, 1.5, 2.0)$.\n- Case $2$ (boundary with single sample and marginal): $N=1$, $d=1$, $M=1$, empirical supports\n  $v^{(1)} = (10, 20, 30)$.\n- Case $3$ (no reduction, identity check): $N=5$, $d=2$, $M=5$, empirical supports\n  $v^{(1)} = (0, 1, 2, 3, 4, 5)$,\n  $v^{(2)} = (-2, -1, 0, 1, 3, 6)$.\n- Case $4$ (extreme reduction): $N=8$, $d=4$, $M=1$, empirical supports\n  $v^{(1)} = (0.0, 0.4, 0.8, 1.2, 1.6, 2.0)$,\n  $v^{(2)} = (10, 15, 25, 40)$,\n  $v^{(3)} = (100, 120, 140, 160, 200)$,\n  $v^{(4)} = (-0.5, 0.0, 0.5, 1.0, 1.5)$.\n\nAnswer specification:\n- For each test case, your program must produce a result of the form $[b, j, m]$, where $b$ is a boolean indicating the stratification validity per dimension, $j$ is a float equal to the scenario reduction objective $J(S)$, and $m$ is the list of floats equal to the sample means in each dimension after mapping. Aggregate the results of all test cases into a single list of lists.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. For example, the structure must be like $[[\\text{True},0.123,[1.0,2.0]],\\dots]$ on a single line.",
            "solution": "The problem is valid. It is scientifically grounded in principles of statistics and numerical optimization, specifically Latin Hypercube Sampling (LHS), inverse transform sampling, and greedy algorithms for scenario reduction. The problem is well-posed, providing all necessary definitions, data, and a clear algorithmic specification. I will now provide a complete, reasoned solution.\n\nThe solution is structured into three principal stages: scenario generation, mapping to empirical distributions, and scenario reduction.\n\n### 1. Scenario Generation via Latin Hypercube Sampling (LHS)\n\nLHS is a stratified sampling technique designed to generate a set of $N$ samples from a $d$-dimensional probability distribution, ensuring that each marginal distribution is well-covered.\n\n**Principle of Stratification**: The core idea is to divide the support of each marginal distribution into $N$ equally probable strata and to draw exactly one sample from each stratum. For a random vector with independent marginals, we typically work with a $d$-dimensional unit hypercube $[0,1]^d$. Each dimension is partitioned into $N$ disjoint intervals $I_i = [\\frac{i}{N}, \\frac{i+1}{N})$ for $i \\in \\{0, 1, \\dots, N-1\\}$.\n\n**LHS Algorithm**: To generate $N$ samples $\\{u_k\\}_{k=1}^N$, where each $u_k \\in [0,1]^d$, the LHS algorithm proceeds as follows:\n1.  For each dimension $j \\in \\{1, \\dots, d\\}$, generate an independent random permutation $\\pi_j$ of the set of stratum indices $\\{0, 1, \\dots, N-1\\}$.\n2.  For each dimension $j$, generate $N$ independent random numbers $v_{kj} \\sim \\mathrm{Unif}(0,1)$ for $k \\in \\{1, \\dots, N\\}$.\n3.  The $j$-th component of the $k$-th sample vector $u_k$ is then constructed as:\n    $$\n    u_{kj} = \\frac{\\pi_j(k-1) + v_{kj}}{N}\n    $$\nThis construction guarantees that for any dimension $j$, the set of values $\\{\\lfloor N u_{kj} \\rfloor\\}_{k=1}^N$ is exactly the set $\\{0, 1, \\dots, N-1\\}$. This property, known as stratification, ensures that the samples are spread evenly across the full range of each marginal distribution, which is a key advantage over simple Monte Carlo sampling. For reproducibility, the random number generator used for permutations and uniform draws is seeded.\n\n### 2. Mapping Uniform Samples to Empirical Marginals\n\nThe uniform LHS samples on $[0,1]^d$ must be transformed into samples that follow the desired, often non-uniform, marginal distributions of the variables in the energy system model (e.g., wind speed, electricity price).\n\n**Inverse Transform Sampling**: This is achieved using the principle of inverse transform sampling. If $U$ is a uniform random variable on $[0,1]$, and $F_X$ is the cumulative distribution function (CDF) of a random variable $X$, then the random variable $X' = F_X^{-1}(U)$ has the same distribution as $X$. Here, $F_X^{-1}$ is the generalized inverse CDF, or quantile function.\n\n**Empirical Inverse CDF**: In practice, we often have empirical data rather than an analytical CDF. The problem provides empirical support values $v_0^{(j)}, v_1^{(j)}, \\dots, v_{L_j-1}^{(j)}$ for each dimension $j$, corresponding to equally spaced quantiles $q_k = \\frac{k}{L_j-1}$. These points $(q_k, v_k^{(j)})$ define an empirical approximation of the inverse CDF, $\\widehat{F}_j^{-1}$. The function $\\widehat{F}_j^{-1}(p)$ for any $p \\in [0,1]$ is evaluated using piecewise-linear interpolation between the given points. This mapping transforms each component $u_{kj}$ of the uniform LHS samples into a scenario value $x_{kj}$:\n$$\nx_{kj} = \\widehat{F}_j^{-1}(u_{kj})\n$$\nThe resulting set $\\{x_k\\}_{k=1}^N$ constitutes the initial, high-fidelity scenario set.\n\n### 3. Scenario Reduction via Greedy Forward Selection\n\nFor computational tractability in optimization models, the number of scenarios $N$ must often be reduced to a smaller, representative subset of size $M$. The goal is to select a subset of scenarios that \"best\" represents the original distribution.\n\n**Objective Function**: The quality of a reduced set $S \\subset \\{1, \\dots, N\\}$ of size $M$ is measured by the objective function:\n$$\nJ(S) = \\sum_{i=1}^N w_i \\min_{s \\in S} \\|x_i - x_s\\|_2 = \\frac{1}{N} \\sum_{i=1}^N \\min_{s \\in S} \\|x_i - x_s\\|_2\n$$\nThis function calculates the average distance from each original scenario to its nearest representative in the reduced set. Minimizing $J(S)$ is equivalent to minimizing the 1-Wasserstein distance between the original discrete distribution (with $N$ points) and the reduced one (with $M$ points, where each selected point $x_s$ now carries a new weight equal to the sum of weights of the original points it represents).\n\n**Greedy Algorithm**: Finding the optimal subset $S$ is a computationally hard problem. A common and effective heuristic is greedy forward selection.\n1.  **If $M \\ge N$**: The problem is trivial. All scenarios are selected, and the objective $J(S)$ is $0$.\n2.  **Initialization ($k=1$)**: The first scenario $x_{s_1}$ is chosen to minimize $J(\\{x_{s_1}\\}) = \\frac{1}{N} \\sum_{i=1}^N \\|x_i - x_{s_1}\\|_2$. The point $x_{s_1}$ that minimizes this sum is known as the geometric median of the set $\\{x_i\\}_{i=1}^N$.\n3.  **Iterative Step ($k=2, \\dots, M$)**: At each step $k$, given the already selected set $S_{k-1}$ of size $k-1$, the algorithm selects the next scenario $x_{s_k}$ from the remaining candidates. The chosen $x_{s_k}$ is the one that minimizes the objective function for the new set $S_k = S_{k-1} \\cup \\{x_{s_k}\\}$. This is equivalent to choosing the candidate $x_c$ which maximizes the reduction in the objective, i.e., maximizes $J(S_{k-1}) - J(S_{k-1} \\cup \\{x_c\\})$.\n    Let $d_i^{k-1} = \\min_{s \\in S_{k-1}} \\|x_i - x_s\\|_2$ be the current minimum distance for each point $x_i$. The next point to add, $x_c$, is the one that minimizes the new total distance:\n    $$\n    \\arg\\min_{c \\notin S_{k-1}} \\sum_{i=1}^N \\min(d_i^{k-1}, \\|x_i - x_c\\|_2)\n    $$\nThis process is repeated until $M$ scenarios are selected. The final computed values are the stratification validity boolean, the final objective value $J(S_M)$, and the means of the original $N$ mapped scenarios.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the scenario generation and reduction process\n    for all specified test cases.\n    \"\"\"\n    rng = np.random.default_rng(12345)\n\n    test_cases = [\n        (10, 3, 4, [\n            (0.8, 1.0, 1.3, 1.35, 1.5, 1.7, 2.0, 2.3, 2.5, 2.8, 3.0),\n            (50, 60, 75, 90, 120, 150, 200),\n            (0.1, 0.2, 0.5, 0.9, 1.2, 1.5, 2.0)\n        ]),\n        (1, 1, 1, [(10, 20, 30)]),\n        (5, 2, 5, [\n            (0, 1, 2, 3, 4, 5),\n            (-2, -1, 0, 1, 3, 6)\n        ]),\n        (8, 4, 1, [\n            (0.0, 0.4, 0.8, 1.2, 1.6, 2.0),\n            (10, 15, 25, 40),\n            (100, 120, 140, 160, 200),\n            (-0.5, 0.0, 0.5, 1.0, 1.5)\n        ])\n    ]\n\n    all_results = []\n    for N, d, M, v_lists in test_cases:\n        result = process_case(N, d, M, v_lists, rng)\n        all_results.append(result)\n\n    # Format the final output as a single-line string with no spaces.\n    formatted_results = []\n    for b, j, m_list in all_results:\n        b_str = str(b)\n        j_str = f\"{j:.12f}\".rstrip('0').rstrip('.') if j != 0 else \"0.0\"\n        m_str = f\"[{','.join(map(str, m_list))}]\"\n        formatted_results.append(f\"[{b_str},{j_str},{m_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef process_case(N, d, M, v_lists, rng):\n    \"\"\"\n    Processes a single test case for scenario generation and reduction.\n\n    Args:\n        N (int): Number of scenarios to generate.\n        d (int): Number of dimensions.\n        M (int): Number of scenarios to reduce to.\n        v_lists (list of tuples): Empirical support values for each dimension.\n        rng (np.random.Generator): Seeded random number generator.\n\n    Returns:\n        list: A list containing [stratification_validity, objective_J, means].\n    \"\"\"\n    \n    # 1. Generate N-by-d uniform stratified samples using LHS\n    if N  0:\n        unif_samples = np.zeros((N, d))\n        for j in range(d):\n            perm = rng.permutation(N)\n            rand_unif_in_stratum = rng.uniform(size=N)\n            unif_samples[:, j] = (perm + rand_unif_in_stratum) / N\n    else:\n        unif_samples = np.array([[]])\n\n    # 4. Validate the stratification property\n    is_stratified = True\n    if N  0:\n        for j in range(d):\n            strata_found = np.floor(unif_samples[:, j] * N).astype(int)\n            if set(strata_found) != set(range(N)):\n                is_stratified = False\n                break\n    \n    # 2. Map uniform samples to empirical marginals\n    if N  0:\n        mapped_scenarios = np.zeros((N, d))\n        for j in range(d):\n            v = np.array(v_lists[j])\n            L = len(v)\n            if L  1:\n                q = np.linspace(0, 1, num=L)\n                mapped_scenarios[:, j] = np.interp(unif_samples[:, j], q, v)\n            elif L == 1:\n                mapped_scenarios[:, j] = v[0]\n    else:\n        mapped_scenarios = np.zeros((0, d))\n\n    # 5. Compute sample means for the original N scenarios\n    if N  0:\n        means = np.mean(mapped_scenarios, axis=0).tolist()\n    else:\n        means = [float('nan')] * d  # Or handle as per conventions for N=0\n\n    # 3. Perform greedy forward selection scenario reduction\n    if M = N or N == 0:\n        objective_J = 0.0\n    else:\n        dist_matrix = cdist(mapped_scenarios, mapped_scenarios, 'euclidean')\n        \n        selected_indices = []\n        candidate_indices = list(range(N))\n\n        # First scenario selection (approximating geometric median)\n        min_sum_dist = np.inf\n        best_c_step1 = -1\n        for c in candidate_indices:\n            current_sum_dist = np.sum(dist_matrix[:, c])\n            if current_sum_dist  min_sum_dist:\n                min_sum_dist = current_sum_dist\n                best_c_step1 = c\n        \n        selected_indices.append(best_c_step1)\n        candidate_indices.remove(best_c_step1)\n        min_dists = dist_matrix[:, best_c_step1].copy()\n\n        # Greedy selection for the remaining M-1 scenarios\n        for _ in range(M - 1):\n            if not candidate_indices:\n                break\n            \n            min_objective_val = np.inf\n            best_c_step_k = -1\n\n            for c in candidate_indices:\n                potential_min_dists = np.minimum(min_dists, dist_matrix[:, c])\n                current_objective = np.sum(potential_min_dists)\n                if current_objective  min_objective_val:\n                    min_objective_val = current_objective\n                    best_c_step_k = c\n            \n            selected_indices.append(best_c_step_k)\n            candidate_indices.remove(best_c_step_k)\n            min_dists = np.minimum(min_dists, dist_matrix[:, best_c_step_k])\n\n        objective_J = np.sum(min_dists) / N\n\n    return [is_stratified, objective_J, means]\n\nsolve()\n```"
        },
        {
            "introduction": "The technical choices made in scenario modeling are not merely academic; they have direct and significant consequences for high-stakes investment and operational decisions. This practice illuminates this critical link by examining how we model rare but high-impact events, a central challenge in ensuring system reliability. By analyzing a risk-aware optimization problem, you will see how different sampling strategies, such as over-emphasizing extreme events, can distort risk perception and alter optimal decisions, providing crucial insight into the responsible application of these methods in practice .",
            "id": "4121477",
            "problem": "An energy system planner must choose whether to install a backup generator to mitigate rare, high-cost events associated with simultaneous high demand and low renewable output. The decision variable is $x \\in \\{0,1\\}$, where $x=1$ denotes installing the backup generator and $x=0$ denotes not installing it. The deterministic investment cost is $I(1)=15$ and $I(0)=0$. Operating loss is scenario-dependent: in a normal scenario $\\omega_N$ the operating loss is $L_N(0)=10$ and $L_N(1)=12$, and in a rare stress scenario $\\omega_R$ the operating loss is $L_R(0)=110$ and $L_R(1)=30$. The true probability distribution of scenarios is $p(\\omega_N)=0.99$, $p(\\omega_R)=0.01$.\n\nThe plannerâ€™s objective for a given decision $x$ is to minimize\n$$\nJ(x) \\;=\\; I(x) \\;+\\; \\mathbb{E}_p\\!\\big[L(x,\\omega)\\big] \\;+\\; \\lambda \\,\\mathrm{CVaR}_{\\alpha}\\!\\big(L(x,\\omega)\\big),\n$$\nwhere $\\lambda0$ is a risk-aversion parameter and $\\mathrm{CVaR}_{\\alpha}$ denotes Conditional Value-at-Risk at confidence level $\\alpha \\in (0,1)$. For this problem, take $\\lambda=0.1$ and $\\alpha=0.99$, and define Conditional Value-at-Risk (CVaR) via the coherent risk measure representation\n$$\n\\mathrm{CVaR}_{\\alpha}(L) \\;=\\; \\min_{\\eta \\in \\mathbb{R}} \\left\\{ \\eta \\;+\\; \\frac{1}{1-\\alpha}\\,\\mathbb{E}\\big[(L-\\eta)_+\\big] \\right\\},\n$$\nwhere $(u)_+ = \\max\\{u,0\\}$ and the expectation is with respect to the relevant probability measure.\n\nTwo scenario-generation strategies are considered for computing a Sample Average Approximation (SAA) of $J(x)$:\n\n1. Standard sampling from the true distribution $p$, using equal weights on each sampled scenario.\n2. Rare-event emphasis: oversampling the rare scenario with sampling distribution $q$ such that $q(\\omega_N)=0.5$ and $q(\\omega_R)=0.5$, using either (i) equal weights on sampled scenarios (no reweighting), or (ii) importance weights $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$ so that sample-weighted expectations target $\\mathbb{E}_p[\\cdot]$.\n\nUsing the definitions of expectation and Conditional Value-at-Risk, analyze how rare-event emphasis affects optimization decisions and tail risk estimates. Select all statements that are correct.\n\nA. Oversampling rare events without correcting weights can cause the optimal decision to switch toward installing the backup generator due to distortion of the sample-average expectation, even if the true risk-aware objective favors not installing the backup.\n\nB. With correct importance weights $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$, the weighted Sample Average Approximation is a consistent estimator of both $\\mathbb{E}_p[L(x,\\omega)]$ and $\\mathrm{CVaR}_{\\alpha}(L(x,\\omega))$, and the optimal decision under the sample converges to the true risk-aware optimum as the number of samples grows.\n\nC. Emphasizing rare events with correct importance weights necessarily lowers the variance of both the expectation and Conditional Value-at-Risk estimators compared to simple random sampling from $p$.\n\nD. Scenario reduction that retains only extreme scenarios, even with weights adjusted to the original probabilities, always preserves the optimal decision of the risk-aware problem.\n\nE. In the discrete two-scenario example with $p(\\omega_N)=0.99$, $p(\\omega_R)=0.01$, $\\alpha=0.99$, and the given losses, oversampling to $q(\\omega_N)=q(\\omega_R)=0.5$ without reweighting leaves $\\mathrm{CVaR}_{0.99}$ unchanged for both $x=0$ and $x=1$, while altering the expectation; hence any change in the optimal decision arises from expectation distortion rather than from a change in the tail risk estimate.",
            "solution": "The user wants me to validate and solve a problem concerning scenario generation and risk-aware optimization in energy systems.\n\n### Step 1: Extract Givens\n\n-   **Decision variable**: $x \\in \\{0, 1\\}$, where $x=1$ is installing a backup generator, and $x=0$ is not.\n-   **Investment Cost**: $I(0)=0$ and $I(1)=15$.\n-   **Scenarios**: Normal scenario $\\omega_N$ and rare stress scenario $\\omega_R$.\n-   **Operating Loss**: A scenario-dependent function $L(x,\\omega)$.\n    -   For $x=0$: $L_N(0)=L(0,\\omega_N)=10$ and $L_R(0)=L(0,\\omega_R)=110$.\n    -   For $x=1$: $L_N(1)=L(1,\\omega_N)=12$ and $L_R(1)=L(1,\\omega_R)=30$.\n-   **True Probability Distribution ($p$)**: $p(\\omega_N)=0.99$ and $p(\\omega_R)=0.01$.\n-   **Objective Function**: Minimize $J(x) = I(x) + \\mathbb{E}_p[L(x,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{\\alpha}[L(x,\\omega)]$.\n-   **Parameters**: Risk-aversion $\\lambda=0.1$ and confidence level $\\alpha=0.99$.\n-   **CVaR Definition**: $\\mathrm{CVaR}_{\\alpha}(L) = \\min_{\\eta \\in \\mathbb{R}} \\{ \\eta + \\frac{1}{1-\\alpha}\\,\\mathbb{E}[(L-\\eta)_+] \\}$, where $(u)_+ = \\max\\{u,0\\}$.\n-   **Sampling Strategies**:\n    1.  Standard sampling from $p$.\n    2.  Rare-event emphasis (oversampling) using distribution $q$ with $q(\\omega_N)=0.5, q(\\omega_R)=0.5$.\n        -   (i) Without reweighting (equal weights).\n        -   (ii) With importance weights $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem uses standard, well-established concepts from stochastic optimization and risk management, including expected value, Conditional Value-at-Risk (CVaR), Sample Average Approximation (SAA), and importance sampling. These are core techniques in energy systems modeling and operations research. The setup is mathematically rigorous.\n-   **Well-Posed**: All necessary data, functions, and parameters ($I(x)$, $L(x,\\omega)$, $p(\\omega)$, $q(\\omega)$, $\\lambda$, $\\alpha$) are explicitly defined. The objective function is clearly specified. The questions posed in the options are answerable through direct calculation and application of the relevant theory.\n-   **Objective**: The problem is stated in precise, quantitative, and unbiased language.\n\nThe problem does not violate any criteria for validity. It is scientifically sound, well-posed, objective, complete, and non-trivial.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with the solution.\n\n### Analysis of the Optimization Problem\n\nFirst, I will calculate the true optimal decision by evaluating the objective function $J(x)$ for $x=0$ and $x=1$ using the true probability distribution $p$.\n\n**Case 1: $x=0$ (Do not install generator)**\n-   Investment cost: $I(0) = 0$.\n-   Loss distribution: $\\{10, 110\\}$ with probabilities $\\{0.99, 0.01\\}$.\n-   Expected loss: $\\mathbb{E}_p[L(0,\\omega)] = L_N(0)p(\\omega_N) + L_R(0)p(\\omega_R) = (10)(0.99) + (110)(0.01) = 9.9 + 1.1 = 11$.\n-   Conditional Value-at-Risk ($\\mathrm{CVaR}_{0.99}$): The confidence level $\\alpha=0.99$ corresponds to a tail probability of $1-\\alpha=0.01$. The probability of the rare scenario $\\omega_R$ is exactly $0.01$. Therefore, the CVaR is the expectation of the loss in the worst $1\\%$ of cases, which is simply the loss in the rare scenario.\n    $$ \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = L_R(0) = 110 $$\n    This can be formally verified using the given definition with $\\eta = \\mathrm{VaR}_{0.99}(L(0,\\omega)) = 10$.\n    $$ \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = 10 + \\frac{1}{1-0.99} \\left[ (10-10)_+ \\cdot 0.99 + (110-10)_+ \\cdot 0.01 \\right] = 10 + \\frac{1}{0.01} [100 \\cdot 0.01] = 10 + 100 = 110 $$\n-   Total objective value for $x=0$:\n    $$ J(0) = I(0) + \\mathbb{E}_p[L(0,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = 0 + 11 + (0.1)(110) = 11 + 11 = 22 $$\n\n**Case 2: $x=1$ (Install generator)**\n-   Investment cost: $I(1) = 15$.\n-   Loss distribution: $\\{12, 30\\}$ with probabilities $\\{0.99, 0.01\\}$.\n-   Expected loss: $\\mathbb{E}_p[L(1,\\omega)] = L_N(1)p(\\omega_N) + L_R(1)p(\\omega_R) = (12)(0.99) + (30)(0.01) = 11.88 + 0.3 = 12.18$.\n-   Conditional Value-at-Risk ($\\mathrm{CVaR}_{0.99}$): Similar to the case for $x=0$, the CVaR is the loss in the rare scenario.\n    $$ \\mathrm{CVaR}_{0.99}(L(1,\\omega)) = L_R(1) = 30 $$\n-   Total objective value for $x=1$:\n    $$ J(1) = I(1) + \\mathbb{E}_p[L(1,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{0.99}(L(1,\\omega)) = 15 + 12.18 + (0.1)(30) = 15 + 12.18 + 3 = 30.18 $$\n\n**Conclusion on True Optimal Decision**\nSince $J(0) = 22  J(1) = 30.18$, the true optimal decision is $x^*=0$, which is to not install the generator.\n\n### Evaluation of Options\n\n**A. Oversampling rare events without correcting weights can cause the optimal decision to switch toward installing the backup generator due to distortion of the sample-average expectation, even if the true risk-aware objective favors not installing the backup.**\n\nThis statement suggests analyzing the problem under the oversampling distribution $q(\\omega_N)=0.5, q(\\omega_R)=0.5$ as if it were the true distribution. Let's compute the objective $J_q(x)$ under this distorted distribution.\n\n-   For $x=0$:\n    -   $\\mathbb{E}_q[L(0,\\omega)] = (10)(0.5) + (110)(0.5) = 5 + 55 = 60$.\n    -   The CVaR calculation requires considering the tail under distribution $q$. The worst outcome is loss $110$ with probability $0.5$. Since $0.5  1-\\alpha = 0.01$, the worst $1\\%$ of outcomes all correspond to the rare event, so $\\mathrm{CVaR}_{0.99,q}(L(0,\\omega)) = 110$.\n    -   $J_q(0) = 0 + 60 + (0.1)(110) = 71$.\n-   For $x=1$:\n    -   $\\mathbb{E}_q[L(1,\\omega)] = (12)(0.5) + (30)(0.5) = 6 + 15 = 21$.\n    -   Similarly, the worst outcome is loss $30$ with probability $0.5  0.01$, so $\\mathrm{CVaR}_{0.99,q}(L(1,\\omega)) = 30$.\n    -   $J_q(1) = 15 + 21 + (0.1)(30) = 15 + 21 + 3 = 39$.\n\nUnder the distorted belief $q$, the optimal decision is $x^*_q=1$ because $J_q(1) = 39  J_q(0) = 71$. The true optimal decision was $x=0$. The decision has switched. The reason is the massive distortion in the expectation term. For $x=0$, the expectation increased from $11$ to $60$. For $x=1$, it increased from $12.18$ to $21$. The perceived penalty for not installing the generator increased far more, leading to the decision switch.\n\nVerdict: **Correct**.\n\n**B. With correct importance weights $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$, the weighted Sample Average Approximation is a consistent estimator of both $\\mathbb{E}_p[L(x,\\omega)]$ and $\\mathrm{CVaR}_{\\alpha}(L(x,\\omega))$, and the optimal decision under the sample converges to the true risk-aware optimum as the number of samples grows.**\n\nThis statement describes the fundamental principle of importance sampling (IS). For any function $g(\\omega)$, the expectation of its weighted sample average under distribution $q$ equals the true expectation under $p$:\n$$ \\mathbb{E}_q[w(\\omega)g(\\omega)] = \\sum_{\\omega} q(\\omega) \\frac{p(\\omega)}{q(\\omega)} g(\\omega) = \\sum_{\\omega} p(\\omega)g(\\omega) = \\mathbb{E}_p[g(\\omega)] $$\nThis shows that the weighted sample mean is an unbiased estimator of the true mean. By the law of large numbers, the Sample Average Approximation (SAA) will converge to the true value. This applies directly to estimating $\\mathbb{E}_p[L(x,\\omega)]$. It also applies to estimating the expectation inside the CVaR formula, $\\mathbb{E}_p[(L-\\eta)_+]$. Since the SAA estimators for all components of the objective function $J(x)$ are consistent (i.e., converge to their true values as sample size increases), the SAA objective converges to the true objective. Under general conditions, this ensures that the optimizer of the SAA problem converges to the optimizer of the true problem.\n\nVerdict: **Correct**.\n\n**C. Emphasizing rare events with correct importance weights necessarily lowers the variance of both the expectation and Conditional Value-at-Risk estimators compared to simple random sampling from $p$.**\n\nThis claims that the given importance sampling scheme is *necessarily* a variance reduction technique for all estimators of interest. Importance sampling can reduce variance, but it can also increase it if the importance distribution $q$ is chosen poorly. Let's test this claim by calculating the variance of the estimator for $\\mathbb{E}_p[L(1,\\omega)]$.\nThe estimator for a single sample is $Y = w(\\omega)L(1,\\omega)$. Its variance under $q$ is $\\mathrm{Var}_q(Y) = \\mathbb{E}_q[Y^2] - (\\mathbb{E}_q[Y])^2$. We know $\\mathbb{E}_q[Y] = \\mathbb{E}_p[L(1,\\omega)] = 12.18$.\n$$ \\mathbb{E}_q[Y^2] = \\mathbb{E}_q\\left[\\left(\\frac{p(\\omega)}{q(\\omega)}L(1,\\omega)\\right)^2\\right] = \\sum_{\\omega}q(\\omega)\\frac{p(\\omega)^2}{q(\\omega)^2}L(1,\\omega)^2 = \\sum_{\\omega}\\frac{p(\\omega)^2}{q(\\omega)}L(1,\\omega)^2 $$\n$$ \\mathbb{E}_q[Y^2] = \\frac{p(\\omega_N)^2}{q(\\omega_N)}L_N(1)^2 + \\frac{p(\\omega_R)^2}{q(\\omega_R)}L_R(1)^2 = \\frac{0.99^2}{0.5}(12^2) + \\frac{0.01^2}{0.5}(30^2) $$\n$$ \\mathbb{E}_q[Y^2] = \\frac{0.9801}{0.5}(144) + \\frac{0.0001}{0.5}(900) = (1.9602)(144) + (0.0002)(900) = 282.2688 + 0.18 = 282.4488 $$\nSo, $\\mathrm{Var}_q(w(\\omega)L(1,\\omega)) = 282.4488 - 12.18^2 \\approx 282.4488 - 148.3524 = 134.0964$.\n\nNow, let's compute the variance for standard sampling from $p$. The estimator is $L(1,\\omega)$.\n$$ \\mathrm{Var}_p(L(1,\\omega)) = \\mathbb{E}_p[L(1,\\omega)^2] - (\\mathbb{E}_p[L(1,\\omega)])^2 $$\n$$ \\mathbb{E}_p[L(1,\\omega)^2] = (12^2)(0.99) + (30^2)(0.01) = (144)(0.99) + (900)(0.01) = 142.56 + 9 = 151.56 $$\nSo, $\\mathrm{Var}_p(L(1,\\omega)) = 151.56 - 12.18^2 \\approx 151.56 - 148.3524 = 3.2076$.\nSince $134.0964  3.2076$, the variance of the importance sampling estimator for $\\mathbb{E}_p[L(1,\\omega)]$ is significantly *higher* than for standard sampling. The word \"necessarily\" makes the statement false.\n\nVerdict: **Incorrect**.\n\n**D. Scenario reduction that retains only extreme scenarios, even with weights adjusted to the original probabilities, always preserves the optimal decision of the risk-aware problem.**\n\nLet's test this by performing a scenario reduction that retains only the \"extreme\" scenario $\\omega_R$. To \"adjust weights\", the simplest interpretation is to assign the full probability mass to this single remaining scenario. Let's call the new measure $p'$. So, $p'(\\omega_R)=1$.\n-   For $x=0$: The loss is deterministically $L_R(0)=110$. The expectation is $110$ and the CVaR of a constant is the constant itself, so $\\mathrm{CVaR}_{0.99}=110$.\n    $J_{red}(0) = 0 + 110 + (0.1)(110) = 121$.\n-   For $x=1$: The loss is deterministically $L_R(1)=30$. The expectation is $30$ and $\\mathrm{CVaR}_{0.99}=30$.\n    $J_{red}(1) = 15 + 30 + (0.1)(30) = 15 + 30 + 3 = 48$.\n\nThe optimal decision in this reduced model is $x=1$ since $48  121$. This is different from the true optimal decision of $x=0$. The word \"always\" makes this strong claim easy to falsify with a counterexample, which we have just constructed.\n\nVerdict: **Incorrect**.\n\n**E. In the discrete two-scenario example with $p(\\omega_N)=0.99$, $p(\\omega_R)=0.01$, $\\alpha=0.99$, and the given losses, oversampling to $q(\\omega_N)=q(\\omega_R)=0.5$ without reweighting leaves $\\mathrm{CVaR}_{0.99}$ unchanged for both $x=0$ and $x=1$, while altering the expectation; hence any change in the optimal decision arises from expectation distortion rather than from a change in the tail risk estimate.**\n\nThis statement makes several testable claims about the specific example.\n1.  **CVaR is unchanged**:\n    -   Under $p$: As calculated previously, $\\mathrm{CVaR}_{0.99,p}(L(0,\\omega)) = 110$ and $\\mathrm{CVaR}_{0.99,p}(L(1,\\omega)) = 30$.\n    -   Under $q$: As calculated for option A, $\\mathrm{CVaR}_{0.99,q}(L(0,\\omega)) = 110$ and $\\mathrm{CVaR}_{0.99,q}(L(1,\\omega)) = 30$.\n    This is because in both distributions, the probability of the single worst event ($\\omega_R$) is greater than or equal to the tail probability $1-\\alpha=0.01$. Thus, the CVaR is simply the loss in that single worst event, $L_R(x)$. The claim that CVaR is unchanged is true.\n2.  **Expectation is altered**:\n    -   For $x=0$, $\\mathbb{E}_p=11$ and $\\mathbb{E}_q=60$. Altered.\n    -   For $x=1$, $\\mathbb{E}_p=12.18$ and $\\mathbb{E}_q=21$. Altered.\n    This claim is true.\n3.  **Logical conclusion**: The objective is $J(x) = I(x) + \\mathbb{E}[L(x,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{\\alpha}[L(x,\\omega)]$. When moving from distribution $p$ to $q$, the investment cost $I(x)$ is unchanged by definition. As just shown, for the specific parameters of this problem, the $\\mathrm{CVaR}_{0.99}$ term is also unchanged for both decisions. Therefore, the entire change in the objective function $J(x)$ for each decision, and thus the entire change in the relative difference $J(1)-J(0)$, is due to the change in the expectation term. The decision did switch (from $x=0$ to $x=1$), so this logic correctly identifies the source of the change.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}