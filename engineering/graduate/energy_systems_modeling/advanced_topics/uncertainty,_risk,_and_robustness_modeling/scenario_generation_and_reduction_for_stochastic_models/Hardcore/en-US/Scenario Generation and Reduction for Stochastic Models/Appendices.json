{
    "hands_on_practices": [
        {
            "introduction": "In energy systems, many uncertain variables like wind speed or electricity demand are not independent over time; today's value influences tomorrow's. This exercise explores the generation of multi-period scenarios using an autoregressive ($AR(1)$) model, a fundamental tool for capturing such temporal correlations. Mastering this practice is key to creating realistic time-series data for stochastic simulations and optimization models .",
            "id": "4121571",
            "problem": "A system operator models hourly net demand uncertainty in a short-term unit commitment for an energy system using an autoregressive of order $1$ ($AR(1)$) process. Let $\\{X_t\\}_{t \\ge 0}$ denote the net demand deviation from the latest point forecast, in per-unit, at hour $t$, with dynamics defined by\n$$\nX_t \\;=\\; \\phi\\, X_{t-1} \\;+\\; \\epsilon_t,\\quad t \\in \\{1,\\dots,H\\},\n$$\nwhere $\\epsilon_t$ are independent and identically distributed (i.i.d.) Gaussian innovations with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$, independent of $X_0$. The operator wants to generate $N$ multi-period scenarios of length $H$ that are correctly distributed conditional on the realized value $X_0 = x_0$, so that the inter-temporal correlation induced by $\\phi$ is preserved across the $H$ steps.\n\nWhich option best characterizes the correct conditional distribution of the future vector $\\mathbf{X}_{1:H} := (X_1,\\dots,X_H)^\\top$ given $X_0=x_0$, together with a valid algorithm to generate $N$ scenario paths of length $H$ that respect this distribution?\n\nA. The conditional law is multivariate Gaussian with a nonzero, $x_0$-dependent mean and a non-diagonal covariance reflecting serial correlation. Specifically, there exists a mean vector $\\boldsymbol{\\mu}_H(x_0)$ and a positive-definite covariance matrix $\\boldsymbol{\\Sigma}_H(\\phi,\\sigma_\\epsilon^2)$ such that\n$$\n\\mathbf{X}_{1:H}\\,\\big|\\,X_0=x_0 \\;\\sim\\; \\mathcal{N}\\!\\big(\\boldsymbol{\\mu}_H(x_0),\\,\\boldsymbol{\\Sigma}_H(\\phi,\\sigma_\\epsilon^2)\\big),\n$$\nand two equivalent sampling procedures are valid: \n(i) recursively simulate $X_t=\\phi X_{t-1}+\\epsilon_t$ for $t\\in\\{1,\\dots,H\\}$ using i.i.d. draws $\\epsilon_t\\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$ starting from $X_0=x_0$;\n(ii) sample $\\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}_H)$ and set $\\mathbf{x}=\\boldsymbol{\\mu}_H(x_0)+\\boldsymbol{L}\\,\\boldsymbol{z}$ where $\\boldsymbol{\\Sigma}_H=\\boldsymbol{L}\\boldsymbol{L}^\\top$ is a Cholesky factorization. Repeat independently $N$ times to get $N$ scenarios.\n\nB. The conditional law at each future hour is the stationary marginal of the $AR(1)$, independent across time, so $X_t\\,|\\,X_0=x_0 \\sim \\mathcal{N}\\!\\big(0,\\,\\sigma_\\epsilon^2/(1-\\phi^2)\\big)$ for all $t\\in\\{1,\\dots,H\\}$ and $\\{X_t\\}_{t=1}^H$ are independent. To generate $N$ scenarios, draw $H$ independent samples from $\\mathcal{N}\\!\\big(0,\\,\\sigma_\\epsilon^2/(1-\\phi^2)\\big)$ per scenario.\n\nC. The conditional law is multivariate Gaussian with zero mean and a strictly Toeplitz covariance $\\boldsymbol{\\Sigma}$ whose $(i,j)$ element is $\\sigma_\\epsilon^2\\,\\phi^{i+j}$ for $i,j\\in\\{1,\\dots,H\\}$. To sample, draw $\\mathbf{X}_{1:H}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})$ independently $N$ times, ignoring $X_0$.\n\nD. The conditional law has the same mean at all future times, equal to $\\phi\\,x_0$, and the covariance is diagonal with entries $\\sigma_\\epsilon^2/(1-\\phi^2)$. To sample, set $X_t=\\phi\\,x_0+\\eta_t$ with independent $\\eta_t\\sim \\mathcal{N}\\!\\big(0,\\,\\sigma_\\epsilon^2/(1-\\phi^2)\\big)$ for $t\\in\\{1,\\dots,H\\}$, repeating $N$ times.\n\nSelect the most accurate option.",
            "solution": "The user wants me to validate the problem statement and, if valid, provide a detailed solution.\n\n### Step 1: Extract Givens\n-   The process $\\{X_t\\}_{t \\ge 0}$ represents the hourly net demand deviation from a point forecast.\n-   The process follows an autoregressive of order $1$ ($AR(1)$) model: $X_t = \\phi\\, X_{t-1} + \\epsilon_t$.\n-   The time horizon is $t \\in \\{1,\\dots,H\\}$.\n-   The innovations $\\epsilon_t$ are independent and identically distributed (i.i.d.) as a Gaussian distribution: $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$.\n-   The innovations $\\epsilon_t$ are independent of the initial state $X_0$.\n-   The system is conditioned on a realized initial value $X_0 = x_0$.\n-   The goal is to generate $N$ scenarios of length $H$ for the vector $\\mathbf{X}_{1:H} := (X_1,\\dots,X_H)^\\top$.\n-   The question asks for the correct characterization of the conditional distribution $\\mathbf{X}_{1:H} | X_0=x_0$ and a valid algorithm for sampling from it.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the following criteria:\n-   **Scientifically Grounded**: The $AR(1)$ process is a fundamental and widely used model in time series analysis, particularly for modeling forecast errors in energy systems. The formulation is standard and mathematically rigorous. The problem is scientifically sound.\n-   **Well-Posed**: The problem is clearly defined. It specifies the stochastic process, its parameters, the conditioning information, and the objective. The question asks for the characterization of a conditional probability distribution and corresponding sampling methods, which have a unique and correct answer based on the theory of stochastic processes.\n-   **Objective**: The language is precise and mathematical. Terms like \"$AR(1)$\", \"i.i.d. Gaussian innovations\", and \"conditional distribution\" are unambiguous.\n-   **Incomplete or Contradictory Setup**: The problem provides all necessary information to determine the conditional distribution of the future states. There are no contradictions in the setup. Although not explicitly stated, the context of scenario generation and the mention of stationary distributions in the options imply that the process is stable, i.e., $|\\phi| < 1$, which is a standard assumption but not strictly necessary for deriving the conditional law over a finite horizon $H$. The problem is self-contained.\n-   **Unrealistic or Infeasible**: The setup is highly realistic. System operators routinely perform such analyses for short-term scheduling and risk management.\n-   **Ill-Posed or Poorly Structured**: The problem is well-structured and leads to a unique, stable, and meaningful solution.\n-   **Pseudo-Profound, Trivial, or Tautological**: The problem is a standard but non-trivial exercise in applied probability and time series analysis, requiring a solid understanding of conditional multivariate distributions. It is neither trivial nor contrived.\n-   **Outside Scientific Verifiability**: The claims and derivations are purely mathematical and can be rigorously verified.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a well-posed, scientifically grounded question from the field of stochastic modeling. I will now proceed with the derivation and evaluation.\n\n### Principle-Based Derivation\nThe problem asks for the distribution of the vector $\\mathbf{X}_{1:H} = (X_1, \\dots, X_H)^\\top$ conditional on the initial state $X_0 = x_0$. The process is defined by the recurrence relation $X_t = \\phi X_{t-1} + \\epsilon_t$, with $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ being i.i.d. and independent of $X_0$.\n\n1.  **Nature of the Distribution**:\n    Let's express the first few terms as a function of $x_0$ and the innovations:\n    $X_1 = \\phi X_0 + \\epsilon_1$\n    $X_2 = \\phi X_1 + \\epsilon_2 = \\phi(\\phi X_0 + \\epsilon_1) + \\epsilon_2 = \\phi^2 X_0 + \\phi \\epsilon_1 + \\epsilon_2$\n    By induction, we can express any $X_t$ as:\n    $$\n    X_t = \\phi^t X_0 + \\sum_{k=1}^t \\phi^{t-k} \\epsilon_k\n    $$\n    Given $X_0=x_0$, each $X_t$ is a linear transformation of the Gaussian random variables $\\epsilon_1, \\dots, \\epsilon_t$ plus a constant term $\\phi^t x_0$. A vector whose components are linear combinations of Gaussian variables is itself multivariate Gaussian. Thus, the conditional distribution of $\\mathbf{X}_{1:H}$ given $X_0=x_0$ is multivariate normal.\n\n2.  **Conditional Mean Vector**:\n    The mean of the vector is the vector of the means of its components. We compute the conditional expectation of $X_t$:\n    $$\n    E[X_t | X_0=x_0] = E\\left[\\phi^t x_0 + \\sum_{k=1}^t \\phi^{t-k} \\epsilon_k\\right] = \\phi^t x_0 + \\sum_{k=1}^t \\phi^{t-k} E[\\epsilon_k]\n    $$\n    Since $E[\\epsilon_k] = 0$ for all $k$, the sum is zero.\n    $$\n    E[X_t | X_0=x_0] = \\phi^t x_0\n    $$\n    The conditional mean vector $\\boldsymbol{\\mu}_H(x_0)$ is therefore:\n    $$\n    \\boldsymbol{\\mu}_H(x_0) = E[\\mathbf{X}_{1:H} | X_0=x_0] = \\begin{pmatrix} \\phi x_0 \\\\ \\phi^2 x_0 \\\\ \\vdots \\\\ \\phi^H x_0 \\end{pmatrix} = x_0 \\begin{pmatrix} \\phi \\\\ \\phi^2 \\\\ \\vdots \\\\ \\phi^H \\end{pmatrix}\n    $$\n    This mean is evidently dependent on $x_0$ (unless $x_0=0$) and is not constant over time.\n\n3.  **Conditional Covariance Matrix**:\n    The $(i, j)$-th element of the conditional covariance matrix $\\boldsymbol{\\Sigma}_H$ is $\\text{Cov}(X_i, X_j | X_0=x_0)$. Let's assume $i \\le j$ without loss of generality. The covariance is independent of the mean, so:\n    $$\n    \\text{Cov}(X_i, X_j | X_0=x_0) = E[(X_i - E[X_i|X_0=x_0])(X_j - E[X_j|X_0=x_0])]\n    $$\n    Using the expression for $X_t$, we have $X_t - E[X_t|X_0=x_0] = \\sum_{k=1}^t \\phi^{t-k} \\epsilon_k$.\n    $$\n    \\text{Cov}(X_i, X_j | X_0=x_0) = E\\left[ \\left( \\sum_{k=1}^i \\phi^{i-k} \\epsilon_k \\right) \\left( \\sum_{l=1}^j \\phi^{j-l} \\epsilon_l \\right) \\right]\n    $$\n    Expanding the product and using the linearity of expectation, we get terms involving $E[\\epsilon_k \\epsilon_l]$. Since the innovations are i.i.d., $E[\\epsilon_k \\epsilon_l] = \\sigma_\\epsilon^2$ if $k=l$ and $0$ otherwise. We only need to sum over the terms where $k=l$. Since $i \\le j$, this sum runs from $k=1$ to $i$.\n    $$\n    \\text{Cov}(X_i, X_j | X_0=x_0) = \\sum_{k=1}^i \\phi^{i-k} \\phi^{j-k} E[\\epsilon_k^2] = \\sigma_\\epsilon^2 \\sum_{k=1}^i \\phi^{i+j-2k}\n    $$\n    This formula shows that the covariance matrix is independent of $x_0$. It is clearly **not diagonal**, as for $i \\ne j$, the covariance is generally non-zero. For example, for $i=1, j=2$:\n    $$\n    \\text{Cov}(X_1, X_2 | X_0=x_0) = \\sigma_\\epsilon^2 \\sum_{k=1}^1 \\phi^{1+2-2k} = \\sigma_\\epsilon^2 \\phi^1 = \\phi \\sigma_\\epsilon^2\n    $$\n    The covariance matrix is also **not Toeplitz** (where elements depend only on $|i-j|$). For example, $\\text{Cov}(X_2, X_3 | X_0=x_0) = \\sigma_\\epsilon^2 \\sum_{k=1}^2 \\phi^{2+3-2k} = \\sigma_\\epsilon^2(\\phi^3 + \\phi^1) = \\phi\\sigma_\\epsilon^2(1+\\phi^2)$, which is different from $\\text{Cov}(X_1, X_2 | X_0=x_0)$.\n    The covariance matrix, being a covariance matrix of a real process (with $\\sigma_\\epsilon^2 > 0$), is positive-definite.\n\n4.  **Sampling Algorithms**:\n    (i) **Recursive Simulation**: One can generate a sample path $(x_1, \\dots, x_H)$ by starting with $x_0$ and iteratively computing $x_t = \\phi x_{t-1} + e_t$ for $t=1, \\dots, H$, where each $e_t$ is an independent draw from $\\mathcal{N}(0, \\sigma_\\epsilon^2)$. By its construction, this procedure exactly follows the definition of the process and correctly generates a sample from the conditional distribution.\n    (ii) **Multivariate Normal Sampling**: A standard method to draw a sample $\\mathbf{x}$ from a multivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ is to:\n    a. Compute the Cholesky decomposition of the covariance matrix, $\\boldsymbol{\\Sigma} = \\boldsymbol{L}\\boldsymbol{L}^\\top$.\n    b. Generate a vector $\\boldsymbol{z}$ of $H$ independent standard normal variables, i.e., $\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I}_H)$.\n    c. Construct the sample as $\\mathbf{x} = \\boldsymbol{\\mu} + \\boldsymbol{L}\\boldsymbol{z}$.\n    This method is mathematically equivalent to drawing directly from $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n\nBoth methods are valid for generating scenarios from the derived distribution.\n\n### Option-by-Option Analysis\n\n**A. The conditional law is multivariate Gaussian with a nonzero, $x_0$-dependent mean and a non-diagonal covariance reflecting serial correlation. Specifically, there exists a mean vector $\\boldsymbol{\\mu}_H(x_0)$ and a positive-definite covariance matrix $\\boldsymbol{\\Sigma}_H(\\phi,\\sigma_\\epsilon^2)$ such that $\\mathbf{X}_{1:H}\\,\\big|\\,X_0=x_0 \\;\\sim\\; \\mathcal{N}\\!\\big(\\boldsymbol{\\mu}_H(x_0),\\,\\boldsymbol{\\Sigma}_H(\\phi,\\sigma_\\epsilon^2)\\big)$, and two equivalent sampling procedures are valid: (i) recursively simulate $X_t=\\phi X_{t-1}+\\epsilon_t$ for $t\\in\\{1,\\dots,H\\}$ using i.i.d. draws $\\epsilon_t\\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$ starting from $X_0=x_0$; (ii) sample $\\boldsymbol{z}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{I}_H)$ and set $\\mathbf{x}=\\boldsymbol{\\mu}_H(x_0)+\\boldsymbol{L}\\,\\boldsymbol{z}$ where $\\boldsymbol{\\Sigma}_H=\\boldsymbol{L}\\boldsymbol{L}^\\top$ is a Cholesky factorization. Repeat independently $N$ times to get $N$ scenarios.**\n-   **Verdict:** **Correct**.\n-   **Justification:** This statement accurately aligns with our derivation. It correctly identifies the distribution as multivariate Gaussian, the mean as $x_0$-dependent and generally non-zero, and the covariance as non-diagonal and positive-definite. It also correctly describes two valid and equivalent sampling procedures: direct recursive simulation and standard multivariate normal sampling via Cholesky decomposition.\n\n**B. The conditional law at each future hour is the stationary marginal of the $AR(1)$, independent across time, so $X_t\\,|\\,X_0=x_0 \\sim \\mathcal{N}\\!\\big(0,\\,\\sigma_\\epsilon^2/(1-\\phi^2)\\big)$ for all $t\\in\\{1,\\dots,H\\}$ and $\\{X_t\\}_{t=1}^H$ are independent. To generate $N$ scenarios, draw $H$ independent samples from $\\mathcal{N}\\!\\big(0,\\,\\sigma_\\epsilon^2/(1-\\phi^2)\\big)$ per scenario.**\n-   **Verdict:** **Incorrect**.\n-   **Justification:** This option makes several critical errors. First, it claims the conditional mean is $0$, but it is $\\phi^t x_0$. Second, it claims the conditional variance is the stationary variance $\\sigma_\\epsilon^2/(1-\\phi^2)$, but the true conditional variance is time-dependent and smaller than the stationary variance for any finite $t$. Third, it claims the random variables $\\{X_t\\}_{t=1}^H$ are conditionally independent, which contradicts the fact that the process has serial correlation, resulting in a non-diagonal covariance matrix.\n\n**C. The conditional law is multivariate Gaussian with zero mean and a strictly Toeplitz covariance $\\boldsymbol{\\Sigma}$ whose $(i,j)$ element is $\\sigma_\\epsilon^2\\,\\phi^{i+j}$ for $i,j\\in\\{1,\\dots,H\\}$. To sample, draw $\\mathbf{X}_{1:H}\\sim \\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})$ independently $N$ times, ignoring $X_0$.**\n-   **Verdict:** **Incorrect**.\n-   **Justification:** This option is flawed in every aspect. The conditional mean is not zero; it depends on $x_0$. The conditional covariance matrix is not Toeplitz, as shown in the derivation. Furthermore, the given formula for the covariance elements, $\\sigma_\\epsilon^2\\,\\phi^{i+j}$, is incorrect for both the conditional and unconditional covariance. Ignoring $X_0$ is fundamentally wrong when deriving a conditional distribution.\n\n**D. The conditional law has the same mean at all future times, equal to $\\phi\\,x_0$, and the covariance is diagonal with entries $\\sigma_\\epsilon^2/(1-\\phi^2)$. To sample, set $X_t=\\phi\\,x_0+\\eta_t$ with independent $\\eta_t\\sim \\mathcal{N}\\!\\big(0,\\,\\sigma_\\epsilon^2/(1-\\phi^2)\\big)$ for $t\\in\\{1,\\dots,H\\}$, repeating $N$ times.**\n-   **Verdict:** **Incorrect**.\n-   **Justification:** This option is also incorrect on multiple grounds. The conditional mean is $\\phi^t x_0$, which is not constant over time $t$. The covariance matrix is not diagonal because of the autoregressive nature of the process. The diagonal entries (variances) are not the stationary variance $\\sigma_\\epsilon^2/(1-\\phi^2)$. The proposed sampling method generates independent random variables, failing to capture the essential temporal correlation.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "After identifying a large set of possible future scenarios, we often need to reduce it to a smaller, computationally tractable set for optimization. This practice demonstrates a foundational technique for scenario reduction: assigning weights to a few representative points to ensure that key statistical properties, or moments, of the original uncertainty are preserved. This exercise provides a concrete, hands-on calculation to find scenario weights that match a distribution's mean and variance, a critical skill in building robust models .",
            "id": "4121537",
            "problem": "In long-term planning for electricity systems, uncertain net load $X$ is often represented by a finite set of scenarios to enable tractable optimization of operational and investment decisions. Consider the approximation of the distribution of $X$ by a discrete measure supported at representative points $\\xi_i$ with scenario weights $p_i$, where $i=1,\\dots,m$. The decision maker requires that the first $k$ moments of the approximation match the corresponding moments of $X$ to preserve key statistical characteristics used in system sizing and reliability assessment. Let $\\phi_j:\\mathbb{R}\\to\\mathbb{R}$ be fixed basis functions associated with the first $k$ moments, for $j=1,\\dots,k$.\n\n(a) Starting only from the definition of mathematical expectation and the construction of a discrete scenario approximation for a random variable, derive the linear constraints that the scenario weights $p_i$ must satisfy in order to preserve the first $k$ moments induced by the basis functions $\\phi_j$. Your derivation must explicitly justify why these constraints are linear in the weights and must include the normalization of the weights.\n\n(b) In the context of a simplified distributionally robust scenario reduction for net load in an energy system, suppose a planner chooses $m=3$ representative scenarios at $\\xi_1 = 50$ MW, $\\xi_2 = 120$ MW, and $\\xi_3 = 200$ MW. The planner wishes to preserve the first two moments of $X$, with known targets $\\mathbb{E}[X] = 120$ MW and $\\mathbb{E}[X^2] = 15300$ (MW)$^2$, by taking the moment basis functions to be $\\phi_1(x)=x$ and $\\phi_2(x)=x^2$. Impose the normalization $\\sum_{i=1}^{3} p_i = 1$ and compute the scenario weights $p_1$, $p_2$, and $p_3$ that exactly preserve these two moments. Report the final weights as a row matrix $\\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix}$ in exact fractional form. The weights are dimensionless, so no units are required in the final report. No rounding is required.",
            "solution": "The problem posed is valid. It is scientifically grounded in probability theory and its application to stochastic modeling in energy systems, a well-established field. The problem is well-posed, providing all necessary data and relationships to form a solvable system of linear equations. The language is objective and the parameters are physically plausible. We may therefore proceed with a full solution.\n\n(a) Derivation of Linear Constraints\n\nLet $X$ be a continuous random variable representing the uncertain net load, with a probability density function denoted by $f_X(x)$. The expectation of a function $g(X)$ is defined by the integral:\n$$ \\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\,dx $$\nThe problem states that we are interested in the first $k$ moments of $X$, which are induced by a set of $k$ basis functions $\\phi_j:\\mathbb{R}\\to\\mathbb{R}$ for $j=1, \\dots, k$. The $j$-th moment, denoted $\\mu_j$, is the expected value of the corresponding basis function $\\phi_j(X)$:\n$$ \\mu_j = \\mathbb{E}[\\phi_j(X)] = \\int_{-\\infty}^{\\infty} \\phi_j(x) f_X(x) \\,dx $$\nThese values $\\mu_j$ are considered known target quantities.\n\nThe distribution of $X$ is approximated by a discrete measure. This means we replace the continuous random variable $X$ with a discrete random variable, which we can call $\\tilde{X}$. This discrete variable $\\tilde{X}$ can take one of $m$ specific values, $\\{\\xi_1, \\xi_2, \\dots, \\xi_m\\}$, which are the representative points. Each point $\\xi_i$ is assigned a scenario weight, or probability, $p_i$, for $i=1, \\dots, m$. For these weights to form a valid probability distribution, they must be non-negative ($p_i \\ge 0$) and sum to unity. The problem explicitly requires the inclusion of this normalization constraint:\n$$ \\sum_{i=1}^{m} p_i = 1 $$\nThe expectation of a function $g(\\tilde{X})$ for this discrete random variable is given by a weighted sum:\n$$ \\mathbb{E}[g(\\tilde{X})] = \\sum_{i=1}^{m} g(\\xi_i) p_i $$\nThe core requirement is that the moments of the discrete approximation $\\tilde{X}$ must match the moments of the original random variable $X$. This means that for each basis function $\\phi_j$, the following equality must hold:\n$$ \\mathbb{E}[\\phi_j(\\tilde{X})] = \\mathbb{E}[\\phi_j(X)] $$\nSubstituting the definitions for the discrete and continuous expectations, we obtain the moment-matching constraints:\n$$ \\sum_{i=1}^{m} \\phi_j(\\xi_i) p_i = \\mu_j \\quad \\text{for } j=1, \\dots, k $$\nTo justify why these constraints are linear, we must identify the variables and coefficients. The unknown quantities to be determined are the scenario weights $p_1, p_2, \\dots, p_m$. The representative points $\\xi_i$ and the basis functions $\\phi_j$ are given and fixed. Consequently, for any given $i$ and $j$, the value of $\\phi_j(\\xi_i)$ is a constant scalar. Let us define these constants as $A_{ji} = \\phi_j(\\xi_i)$. The moment-matching equations can then be rewritten as:\n$$ \\sum_{i=1}^{m} A_{ji} p_i = \\mu_j \\quad \\text{for } j=1, \\dots, k $$\nEach of these equations expresses the target moment $\\mu_j$ as a linear combination of the unknown weights $p_i$ with constant coefficients $A_{ji}$. By definition, this is a system of $k$ linear equations. The normalization condition, $\\sum_{i=1}^{m} p_i = 1$, is also a linear equation in the variables $p_i$.\n\nTherefore, the complete set of constraints that the scenario weights $p_i$ must satisfy consists of $k+1$ linear equations:\n1. Normalization: $\\sum_{i=1}^{m} p_i = 1$\n2. Moment-matching: $\\sum_{i=1}^{m} \\phi_j(\\xi_i) p_i = \\mu_j$ for $j=1, \\dots, k$\n\n(b) Computation of Scenario Weights\n\nWe are given $m=3$ scenarios with representative points $\\xi_1 = 50$, $\\xi_2 = 120$, and $\\xi_3 = 200$. We wish to preserve the first $k=2$ moments using basis functions $\\phi_1(x) = x$ and $\\phi_2(x) = x^2$. The target moments are $\\mathbb{E}[X] = \\mu_1 = 120$ and $\\mathbb{E}[X^2] = \\mu_2 = 15300$. We must also impose the normalization constraint $\\sum_{i=1}^{3} p_i = 1$.\n\nThis setup yields a system of $3$ linear equations in the $3$ unknown weights $p_1, p_2, p_3$:\n1. Normalization: $p_1 + p_2 + p_3 = 1$\n2. First moment ($\\mathbb{E}[\\tilde{X}] = \\mu_1$): $\\xi_1 p_1 + \\xi_2 p_2 + \\xi_3 p_3 = 120 \\implies 50 p_1 + 120 p_2 + 200 p_3 = 120$\n3. Second moment ($\\mathbb{E}[\\tilde{X}^2] = \\mu_2$): $\\xi_1^2 p_1 + \\xi_2^2 p_2 + \\xi_3^2 p_3 = 15300 \\implies 2500 p_1 + 14400 p_2 + 40000 p_3 = 15300$\n\nLet's write the system explicitly:\n$$\n\\begin{cases}\np_1 + p_2 + p_3 = 1 \\\\\n50 p_1 + 120 p_2 + 200 p_3 = 120 \\\\\n2500 p_1 + 14400 p_2 + 40000 p_3 = 15300\n\\end{cases}\n$$\nWe can simplify the second and third equations by dividing by common factors. Dividing the second equation by $10$ and the third by $100$:\n$$\n\\begin{cases}\np_1 + p_2 + p_3 = 1 \\quad &(1)\\\\\n5 p_1 + 12 p_2 + 20 p_3 = 12 \\quad &(2)\\\\\n25 p_1 + 144 p_2 + 400 p_3 = 153 \\quad &(3)\n\\end{cases}\n$$\nFrom equation $(1)$, we express $p_3$ in terms of $p_1$ and $p_2$: $p_3 = 1 - p_1 - p_2$. We substitute this into equations $(2)$ and $(3)$.\n\nSubstitution into $(2)$:\n$5 p_1 + 12 p_2 + 20(1 - p_1 - p_2) = 12$\n$5 p_1 + 12 p_2 + 20 - 20 p_1 - 20 p_2 = 12$\n$-15 p_1 - 8 p_2 = -8 \\implies 15 p_1 + 8 p_2 = 8 \\quad (A)$\n\nSubstitution into $(3)$:\n$25 p_1 + 144 p_2 + 400(1 - p_1 - p_2) = 153$\n$25 p_1 + 144 p_2 + 400 - 400 p_1 - 400 p_2 = 153$\n$-375 p_1 - 256 p_2 = -247 \\implies 375 p_1 + 256 p_2 = 247 \\quad (B)$\n\nNow we solve the $2 \\times 2$ system for $p_1$ and $p_2$. From equation $(A)$, we isolate $p_2$:\n$8 p_2 = 8 - 15 p_1 \\implies p_2 = 1 - \\frac{15}{8} p_1$\n\nSubstitute this expression for $p_2$ into equation $(B)$:\n$375 p_1 + 256 \\left(1 - \\frac{15}{8} p_1\\right) = 247$\n$375 p_1 + 256 - \\frac{256 \\times 15}{8} p_1 = 247$\nSince $\\frac{256}{8} = 32$, the equation becomes:\n$375 p_1 + 256 - (32 \\times 15) p_1 = 247$\n$375 p_1 + 256 - 480 p_1 = 247$\n$-105 p_1 = -9$\n$p_1 = \\frac{9}{105} = \\frac{3}{35}$\n\nNow, we find $p_2$:\n$p_2 = 1 - \\frac{15}{8} p_1 = 1 - \\frac{15}{8} \\left(\\frac{3}{35}\\right) = 1 - \\frac{45}{280} = 1 - \\frac{9}{56} = \\frac{56-9}{56} = \\frac{47}{56}$\n\nFinally, we find $p_3$ using $p_3 = 1 - p_1 - p_2$:\n$p_3 = 1 - \\frac{3}{35} - \\frac{47}{56}$\nTo sum the fractions, we find a common denominator for $35 = 5 \\times 7$ and $56 = 8 \\times 7$. The least common multiple is $5 \\times 8 \\times 7 = 280$.\n$p_3 = \\frac{280}{280} - \\frac{3 \\times 8}{35 \\times 8} - \\frac{47 \\times 5}{56 \\times 5} = \\frac{280}{280} - \\frac{24}{280} - \\frac{235}{280} = \\frac{280 - 24 - 235}{280} = \\frac{21}{280}$\nSimplifying the fraction for $p_3$ by dividing the numerator and denominator by their greatest common divisor, $7$:\n$p_3 = \\frac{21 \\div 7}{280 \\div 7} = \\frac{3}{40}$\n\nThe computed weights are $p_1 = \\frac{3}{35}$, $p_2 = \\frac{47}{56}$, and $p_3 = \\frac{3}{40}$. All weights are positive, so they constitute a valid probability distribution. The final answer is presented as a row matrix in exact fractional form.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{3}{35} & \\frac{47}{56} & \\frac{3}{40} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The ultimate goal of scenario modeling is to inform better decision-making, especially when facing high-impact, low-probability events. This exercise moves beyond simple statistical representation to explore how scenario sampling and weighting strategies directly affect risk-aware decisions, using Conditional Value-at-Risk ($\\mathrm{CVaR}$) as a risk measure. You will analyze how emphasizing rare events can profoundly alter the perceived risk and lead to different optimal choices in energy system planning .",
            "id": "4121477",
            "problem": "An energy system planner must choose whether to install a backup generator to mitigate rare, high-cost events associated with simultaneous high demand and low renewable output. The decision variable is $x \\in \\{0,1\\}$, where $x=1$ denotes installing the backup generator and $x=0$ denotes not installing it. The deterministic investment cost is $I(1)=15$ and $I(0)=0$. Operating loss is scenario-dependent: in a normal scenario $\\omega_N$ the operating loss is $L_N(0)=10$ and $L_N(1)=12$, and in a rare stress scenario $\\omega_R$ the operating loss is $L_R(0)=110$ and $L_R(1)=30$. The true probability distribution of scenarios is $p(\\omega_N)=0.99$, $p(\\omega_R)=0.01$.\n\nThe plannerâ€™s objective for a given decision $x$ is to minimize\n$$\nJ(x) \\;=\\; I(x) \\;+\\; \\mathbb{E}_p\\!\\big[L(x,\\omega)\\big] \\;+\\; \\lambda \\,\\mathrm{CVaR}_{\\alpha}\\!\\big(L(x,\\omega)\\big),\n$$\nwhere $\\lambda>0$ is a risk-aversion parameter and $\\mathrm{CVaR}_{\\alpha}$ denotes Conditional Value-at-Risk at confidence level $\\alpha \\in (0,1)$. For this problem, take $\\lambda=0.1$ and $\\alpha=0.99$, and define Conditional Value-at-Risk (CVaR) via the coherent risk measure representation\n$$\n\\mathrm{CVaR}_{\\alpha}(L) \\;=\\; \\min_{\\eta \\in \\mathbb{R}} \\left\\{ \\eta \\;+\\; \\frac{1}{1-\\alpha}\\,\\mathbb{E}\\big[(L-\\eta)_+\\big] \\right\\},\n$$\nwhere $(u)_+ = \\max\\{u,0\\}$ and the expectation is with respect to the relevant probability measure.\n\nTwo scenario-generation strategies are considered for computing a Sample Average Approximation (SAA) of $J(x)$:\n\n1. Standard sampling from the true distribution $p$, using equal weights on each sampled scenario.\n2. Rare-event emphasis: oversampling the rare scenario with sampling distribution $q$ such that $q(\\omega_N)=0.5$ and $q(\\omega_R)=0.5$, using either (i) equal weights on sampled scenarios (no reweighting), or (ii) importance weights $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$ so that sample-weighted expectations target $\\mathbb{E}_p[\\cdot]$.\n\nUsing the definitions of expectation and Conditional Value-at-Risk, analyze how rare-event emphasis affects optimization decisions and tail risk estimates. Select all statements that are correct.\n\nA. Oversampling rare events without correcting weights can cause the optimal decision to switch toward installing the backup generator due to distortion of the sample-average expectation, even if the true risk-aware objective favors not installing the backup.\n\nB. With correct importance weights $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$, the weighted Sample Average Approximation is a consistent estimator of both $\\mathbb{E}_p[L(x,\\omega)]$ and $\\mathrm{CVaR}_{\\alpha}(L(x,\\omega))$, and the optimal decision under the sample converges to the true risk-aware optimum as the number of samples grows.\n\nC. Emphasizing rare events with correct importance weights necessarily lowers the variance of both the expectation and Conditional Value-at-Risk estimators compared to simple random sampling from $p$.\n\nD. Scenario reduction that retains only extreme scenarios, even with weights adjusted to the original probabilities, always preserves the optimal decision of the risk-aware problem.\n\nE. In the discrete two-scenario example with $p(\\omega_N)=0.99$, $p(\\omega_R)=0.01$, $\\alpha=0.99$, and the given losses, oversampling to $q(\\omega_N)=q(\\omega_R)=0.5$ without reweighting leaves $\\mathrm{CVaR}_{0.99}$ unchanged for both $x=0$ and $x=1$, while altering the expectation; hence any change in the optimal decision arises from expectation distortion rather than from a change in the tail risk estimate.",
            "solution": "The user wants me to validate and solve a problem concerning scenario generation and risk-aware optimization in energy systems.\n\n### Step 1: Extract Givens\n\n-   **Decision variable**: $x \\in \\{0, 1\\}$, where $x=1$ is installing a backup generator, and $x=0$ is not.\n-   **Investment Cost**: $I(0)=0$ and $I(1)=15$.\n-   **Scenarios**: Normal scenario $\\omega_N$ and rare stress scenario $\\omega_R$.\n-   **Operating Loss**: A scenario-dependent function $L(x,\\omega)$.\n    -   For $x=0$: $L_N(0)=L(0,\\omega_N)=10$ and $L_R(0)=L(0,\\omega_R)=110$.\n    -   For $x=1$: $L_N(1)=L(1,\\omega_N)=12$ and $L_R(1)=L(1,\\omega_R)=30$.\n-   **True Probability Distribution ($p$)**: $p(\\omega_N)=0.99$ and $p(\\omega_R)=0.01$.\n-   **Objective Function**: Minimize $J(x) = I(x) + \\mathbb{E}_p[L(x,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{\\alpha}[L(x,\\omega)]$.\n-   **Parameters**: Risk-aversion $\\lambda=0.1$ and confidence level $\\alpha=0.99$.\n-   **CVaR Definition**: $\\mathrm{CVaR}_{\\alpha}(L) = \\min_{\\eta \\in \\mathbb{R}} \\{ \\eta + \\frac{1}{1-\\alpha}\\,\\mathbb{E}[(L-\\eta)_+] \\}$, where $(u)_+ = \\max\\{u,0\\}$.\n-   **Sampling Strategies**:\n    1.  Standard sampling from $p$.\n    2.  Rare-event emphasis (oversampling) using distribution $q$ with $q(\\omega_N)=0.5, q(\\omega_R)=0.5$.\n        -   (i) Without reweighting (equal weights).\n        -   (ii) With importance weights $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem uses standard, well-established concepts from stochastic optimization and risk management, including expected value, Conditional Value-at-Risk (CVaR), Sample Average Approximation (SAA), and importance sampling. These are core techniques in energy systems modeling and operations research. The setup is mathematically rigorous.\n-   **Well-Posed**: All necessary data, functions, and parameters ($I(x)$, $L(x,\\omega)$, $p(\\omega)$, $q(\\omega)$, $\\lambda$, $\\alpha$) are explicitly defined. The objective function is clearly specified. The questions posed in the options are answerable through direct calculation and application of the relevant theory.\n-   **Objective**: The problem is stated in precise, quantitative, and unbiased language.\n\nThe problem does not violate any criteria for validity. It is scientifically sound, well-posed, objective, complete, and non-trivial.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with the solution.\n\n### Analysis of the Optimization Problem\n\nFirst, I will calculate the true optimal decision by evaluating the objective function $J(x)$ for $x=0$ and $x=1$ using the true probability distribution $p$.\n\n**Case 1: $x=0$ (Do not install generator)**\n-   Investment cost: $I(0) = 0$.\n-   Loss distribution: $\\{10, 110\\}$ with probabilities $\\{0.99, 0.01\\}$.\n-   Expected loss: $\\mathbb{E}_p[L(0,\\omega)] = L_N(0)p(\\omega_N) + L_R(0)p(\\omega_R) = (10)(0.99) + (110)(0.01) = 9.9 + 1.1 = 11$.\n-   Conditional Value-at-Risk ($\\mathrm{CVaR}_{0.99}$): The confidence level $\\alpha=0.99$ corresponds to a tail probability of $1-\\alpha=0.01$. The probability of the rare scenario $\\omega_R$ is exactly $0.01$. Therefore, the CVaR is the expectation of the loss in the worst $1\\%$ of cases, which is simply the loss in the rare scenario.\n    $$ \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = L_R(0) = 110 $$\n    This can be formally verified using the given definition with $\\eta = \\mathrm{VaR}_{0.99}(L(0,\\omega)) = 10$.\n    $$ \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = 10 + \\frac{1}{1-0.99} \\left[ (10-10)_+ \\cdot 0.99 + (110-10)_+ \\cdot 0.01 \\right] = 10 + \\frac{1}{0.01} [100 \\cdot 0.01] = 10 + 100 = 110 $$\n-   Total objective value for $x=0$:\n    $$ J(0) = I(0) + \\mathbb{E}_p[L(0,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = 0 + 11 + (0.1)(110) = 11 + 11 = 22 $$\n\n**Case 2: $x=1$ (Install generator)**\n-   Investment cost: $I(1) = 15$.\n-   Loss distribution: $\\{12, 30\\}$ with probabilities $\\{0.99, 0.01\\}$.\n-   Expected loss: $\\mathbb{E}_p[L(1,\\omega)] = L_N(1)p(\\omega_N) + L_R(1)p(\\omega_R) = (12)(0.99) + (30)(0.01) = 11.88 + 0.3 = 12.18$.\n-   Conditional Value-at-Risk ($\\mathrm{CVaR}_{0.99}$): Similar to the case for $x=0$, the CVaR is the loss in the rare scenario.\n    $$ \\mathrm{CVaR}_{0.99}(L(1,\\omega)) = L_R(1) = 30 $$\n-   Total objective value for $x=1$:\n    $$ J(1) = I(1) + \\mathbb{E}_p[L(1,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{0.99}(L(1,\\omega)) = 15 + 12.18 + (0.1)(30) = 15 + 12.18 + 3 = 30.18 $$\n\n**Conclusion on True Optimal Decision**\nSince $J(0) = 22 < J(1) = 30.18$, the true optimal decision is $x^*=0$, which is to not install the generator.\n\n### Evaluation of Options\n\n**A. Oversampling rare events without correcting weights can cause the optimal decision to switch toward installing the backup generator due to distortion of the sample-average expectation, even if the true risk-aware objective favors not installing the backup.**\n\nThis statement suggests analyzing the problem under the oversampling distribution $q(\\omega_N)=0.5, q(\\omega_R)=0.5$ as if it were the true distribution. Let's compute the objective $J_q(x)$ under this distorted distribution.\n\n-   For $x=0$:\n    -   $\\mathbb{E}_q[L(0,\\omega)] = (10)(0.5) + (110)(0.5) = 5 + 55 = 60$.\n    -   The CVaR calculation requires considering the tail under distribution $q$. The worst outcome is loss $110$ with probability $0.5$. Since $0.5 > 1-\\alpha = 0.01$, the worst $1\\%$ of outcomes all correspond to the rare event, so $\\mathrm{CVaR}_{0.99,q}(L(0,\\omega)) = 110$.\n    -   $J_q(0) = 0 + 60 + (0.1)(110) = 71$.\n-   For $x=1$:\n    -   $\\mathbb{E}_q[L(1,\\omega)] = (12)(0.5) + (30)(0.5) = 6 + 15 = 21$.\n    -   Similarly, the worst outcome is loss $30$ with probability $0.5 > 0.01$, so $\\mathrm{CVaR}_{0.99,q}(L(1,\\omega)) = 30$.\n    -   $J_q(1) = 15 + 21 + (0.1)(30) = 15 + 21 + 3 = 39$.\n\nUnder the distorted belief $q$, the optimal decision is $x^*_q=1$ because $J_q(1) = 39 < J_q(0) = 71$. The true optimal decision was $x=0$. The decision has switched. The reason is the massive distortion in the expectation term. For $x=0$, the expectation increased from $11$ to $60$. For $x=1$, it increased from $12.18$ to $21$. The perceived penalty for not installing the generator increased far more, leading to the decision switch.\n\nVerdict: **Correct**.\n\n**B. With correct importance weights $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$, the weighted Sample Average Approximation is a consistent estimator of both $\\mathbb{E}_p[L(x,\\omega)]$ and $\\mathrm{CVaR}_{\\alpha}(L(x,\\omega))$, and the optimal decision under the sample converges to the true risk-aware optimum as the number of samples grows.**\n\nThis statement describes the fundamental principle of importance sampling (IS). For any function $g(\\omega)$, the expectation of its weighted sample average under distribution $q$ equals the true expectation under $p$:\n$$ \\mathbb{E}_q[w(\\omega)g(\\omega)] = \\sum_{\\omega} q(\\omega) \\frac{p(\\omega)}{q(\\omega)} g(\\omega) = \\sum_{\\omega} p(\\omega)g(\\omega) = \\mathbb{E}_p[g(\\omega)] $$\nThis shows that the weighted sample mean is an unbiased estimator of the true mean. By the law of large numbers, the Sample Average Approximation (SAA) will converge to the true value. This applies directly to estimating $\\mathbb{E}_p[L(x,\\omega)]$. It also applies to estimating the expectation inside the CVaR formula, $\\mathbb{E}_p[(L-\\eta)_+]$. Since the SAA estimators for all components of the objective function $J(x)$ are consistent (i.e., converge to their true values as sample size increases), the SAA objective converges to the true objective. Under general conditions, this ensures that the optimizer of the SAA problem converges to the optimizer of the true problem.\n\nVerdict: **Correct**.\n\n**C. Emphasizing rare events with correct importance weights necessarily lowers the variance of both the expectation and Conditional Value-at-Risk estimators compared to simple random sampling from $p$.**\n\nThis claims that the given importance sampling scheme is *necessarily* a variance reduction technique for all estimators of interest. Importance sampling can reduce variance, but it can also increase it if the importance distribution $q$ is chosen poorly. Let's test this claim by calculating the variance of the estimator for $\\mathbb{E}_p[L(1,\\omega)]$.\nThe estimator for a single sample is $Y = w(\\omega)L(1,\\omega)$. Its variance under $q$ is $\\mathrm{Var}_q(Y) = \\mathbb{E}_q[Y^2] - (\\mathbb{E}_q[Y])^2$. We know $\\mathbb{E}_q[Y] = \\mathbb{E}_p[L(1,\\omega)] = 12.18$.\n$$ \\mathbb{E}_q[Y^2] = \\mathbb{E}_q\\left[\\left(\\frac{p(\\omega)}{q(\\omega)}L(1,\\omega)\\right)^2\\right] = \\sum_{\\omega}q(\\omega)\\frac{p(\\omega)^2}{q(\\omega)^2}L(1,\\omega)^2 = \\sum_{\\omega}\\frac{p(\\omega)^2}{q(\\omega)}L(1,\\omega)^2 $$\n$$ \\mathbb{E}_q[Y^2] = \\frac{p(\\omega_N)^2}{q(\\omega_N)}L_N(1)^2 + \\frac{p(\\omega_R)^2}{q(\\omega_R)}L_R(1)^2 = \\frac{0.99^2}{0.5}(12^2) + \\frac{0.01^2}{0.5}(30^2) $$\n$$ \\mathbb{E}_q[Y^2] = \\frac{0.9801}{0.5}(144) + \\frac{0.0001}{0.5}(900) = (1.9602)(144) + (0.0002)(900) = 282.2688 + 0.18 = 282.4488 $$\nSo, $\\mathrm{Var}_q(w(\\omega)L(1,\\omega)) = 282.4488 - 12.18^2 \\approx 282.4488 - 148.3524 = 134.0964$.\n\nNow, let's compute the variance for standard sampling from $p$. The estimator is $L(1,\\omega)$.\n$$ \\mathrm{Var}_p(L(1,\\omega)) = \\mathbb{E}_p[L(1,\\omega)^2] - (\\mathbb{E}_p[L(1,\\omega)])^2 $$\n$$ \\mathbb{E}_p[L(1,\\omega)^2] = (12^2)(0.99) + (30^2)(0.01) = (144)(0.99) + (900)(0.01) = 142.56 + 9 = 151.56 $$\nSo, $\\mathrm{Var}_p(L(1,\\omega)) = 151.56 - 12.18^2 \\approx 151.56 - 148.3524 = 3.2076$.\nSince $134.0964 > 3.2076$, the variance of the importance sampling estimator for $\\mathbb{E}_p[L(1,\\omega)]$ is significantly *higher* than for standard sampling. The word \"necessarily\" makes the statement false.\n\nVerdict: **Incorrect**.\n\n**D. Scenario reduction that retains only extreme scenarios, even with weights adjusted to the original probabilities, always preserves the optimal decision of the risk-aware problem.**\n\nLet's test this by performing a scenario reduction that retains only the \"extreme\" scenario $\\omega_R$. To \"adjust weights\", the simplest interpretation is to assign the full probability mass to this single remaining scenario. Let's call the new measure $p'$. So, $p'(\\omega_R)=1$.\n-   For $x=0$: The loss is deterministically $L_R(0)=110$. The expectation is $110$ and the CVaR of a constant is the constant itself, so $\\mathrm{CVaR}_{0.99}=110$.\n    $J_{red}(0) = 0 + 110 + (0.1)(110) = 121$.\n-   For $x=1$: The loss is deterministically $L_R(1)=30$. The expectation is $30$ and $\\mathrm{CVaR}_{0.99}=30$.\n    $J_{red}(1) = 15 + 30 + (0.1)(30) = 15 + 30 + 3 = 48$.\n\nThe optimal decision in this reduced model is $x=1$ since $48 < 121$. This is different from the true optimal decision of $x=0$. The word \"always\" makes this strong claim easy to falsify with a counterexample, which we have just constructed.\n\nVerdict: **Incorrect**.\n\n**E. In the discrete two-scenario example with $p(\\omega_N)=0.99$, $p(\\omega_R)=0.01$, $\\alpha=0.99$, and the given losses, oversampling to $q(\\omega_N)=q(\\omega_R)=0.5$ without reweighting leaves $\\mathrm{CVaR}_{0.99}$ unchanged for both $x=0$ and $x=1$, while altering the expectation; hence any change in the optimal decision arises from expectation distortion rather than from a change in the tail risk estimate.**\n\nThis statement makes several testable claims about the specific example.\n1.  **CVaR is unchanged**:\n    -   Under $p$: As calculated previously, $\\mathrm{CVaR}_{0.99,p}(L(0,\\omega)) = 110$ and $\\mathrm{CVaR}_{0.99,p}(L(1,\\omega)) = 30$.\n    -   Under $q$: As calculated for option A, $\\mathrm{CVaR}_{0.99,q}(L(0,\\omega)) = 110$ and $\\mathrm{CVaR}_{0.99,q}(L(1,\\omega)) = 30$.\n    This is because in both distributions, the probability of the single worst event ($\\omega_R$) is greater than or equal to the tail probability $1-\\alpha=0.01$. Thus, the CVaR is simply the loss in that single worst event, $L_R(x)$. The claim that CVaR is unchanged is true.\n2.  **Expectation is altered**:\n    -   For $x=0$, $\\mathbb{E}_p=11$ and $\\mathbb{E}_q=60$. Altered.\n    -   For $x=1$, $\\mathbb{E}_p=12.18$ and $\\mathbb{E}_q=21$. Altered.\n    This claim is true.\n3.  **Logical conclusion**: The objective is $J(x) = I(x) + \\mathbb{E}[L(x,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{\\alpha}[L(x,\\omega)]$. When moving from distribution $p$ to $q$, the investment cost $I(x)$ is unchanged by definition. As just shown, for the specific parameters of this problem, the $\\mathrm{CVaR}_{0.99}$ term is also unchanged for both decisions. Therefore, the entire change in the objective function $J(x)$ for each decision, and thus the entire change in the relative difference $J(1)-J(0)$, is due to the change in the expectation term. The decision did switch (from $x=0$ to $x=1$), so this logic correctly identifies the source of the change.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}