{
    "hands_on_practices": [
        {
            "introduction": "这项综合性实践将指导您实现一个完整的场景分析流程，涵盖从生成到简化的全过程。您将首先使用拉丁超立方采样（LHS）这一强大技术，以确保对不确定性空间的广泛覆盖，然后将这些样本映射到符合实际的经验分布。最后，您将实现一个贪心算法，将大规模场景集简化为一个计算上易于处理的小型代表集，从而获得随机建模的全流程实践经验。",
            "id": "4121597",
            "problem": "您的任务是推导、实现并验证一种基于拉丁超立方抽样（LHS）的、原则性的场景生成与削减方法，该方法适用于能源系统建模中遇到的随机模型。请从基础概率构造出发，推导为何拉丁超立方抽样（LHS）能为每个边际产生分层覆盖，以及经验逆累积分布函数如何将均匀分层样本映射到任意单变量边际分布。然后，设计一种算法，通过最小化原始离散分布与削减后离散分布之间的加权距离，将生成的场景削减为一个更小的代表性子集。\n\n基本原理：\n- 令 $U$ 为单位区间上的均匀分布随机变量，记为 $U \\sim \\mathrm{Unif}(0,1)$，其累积分布函数为 $F_U(u) = u$，$u \\in [0,1]$。\n- 对于任何累积分布函数为 $F_X(x)$ 的单变量随机变量 $X$，逆变换采样原理指出，当 $U \\sim \\mathrm{Unif}(0,1)$ 且 $F_X^{-1}$ 是广义逆累积分布函数时，$X = F_X^{-1}(U)$ 的累积分布函数为 $F_X(x)$。\n- 在 $[0,1]$ 上的分层抽样将该区间划分为 $N$ 个不相交的层 $I_i = \\left[\\frac{i}{N},\\frac{i+1}{N}\\right)$（其中 $i \\in \\{0,1,\\dots,N-1\\}$），并从每个 $I_i$ 中均匀抽取一个点。\n\n定义与任务要求：\n- 拉丁超立方抽样（LHS）是一种分层抽样方法，对于一个具有独立边际的 $d$ 维随机向量，它能产生 $N$ 个样本，使得对于每个边际维度 $j \\in \\{1,\\dots,d\\}$，$[0,1]$ 中的每个层 $I_i$ 在 $N$ 个样本中都恰好被使用一次。这是通过在每个维度上独立地排列 $N$ 个层的索引，然后根据这些独立排列在每个层内抽取一个均匀点并跨维度配对实现的。\n- 在能源系统建模中，经验边际分布通常以单调数值序列的形式给出，这些序列通过分段线性插值来近似逆累积分布函数。对于给定的维度 $j$，其在等距经验分位数 $q_k = \\frac{k}{L_j-1}$（其中 $k \\in \\{0,\\dots,L_j-1\\}$）处有经验支撑值 $v^{(j)}_0 \\le v^{(j)}_1 \\le \\dots \\le v^{(j)}_{L_j-1}$，通过对 $(q_k, v^{(j)}_k)$ 进行线性插值来定义经验逆累积分布函数近似 $\\widehat{F}_j^{-1}(p)$，该函数将 $p \\in [0,1]$ 映射到经验边际的支撑集中的一个值。\n\n场景削减目标：\n- 令生成的 $N$ 个样本为 $\\{x_i\\}_{i=1}^N \\subset \\mathbb{R}^d$，并让离散经验分布为每个 $x_i$ 分配相等的权重 $w_i = \\frac{1}{N}$。削减至 $M$ 个代表性场景的过程是选择一个子集 $S \\subset \\{1,\\dots,N\\}$ 且 $|S|=M$，以最小化加权距离目标\n$$\nJ(S) = \\sum_{i=1}^N w_i \\min_{s \\in S} \\left\\|x_i - x_s\\right\\|_2,\n$$\n其中 $\\left\\|\\cdot\\right\\|_2$ 是欧几里得范数。$J(S)$ 的最小化近似了原始离散测度与削减后离散测度之间的一阶 Wasserstein (Kantorovich) 距离。\n\n您必须实现的内容：\n- 构建一个可复现的算法，该算法：\n  $1$) 使用拉丁超立方抽样（LHS）及各维度独立的排列，生成 $N \\times d$ 的均匀分层样本。\n  $2$) 通过在等距经验分位数处进行分段线性插值定义的经验逆累积分布函数，将这些均匀样本映射到 $d$ 个经验边际。\n  $3$) 执行贪心前向选择场景削减至 $M$ 个场景，通过迭代添加能使目标函数 $J(S)$ 下降最大的候选索引 $c \\in \\{1,\\dots,N\\}$，直到 $|S|=M$。如果 $M \\ge N$，则选择所有样本，目标为 $0$。\n  $4$) 通过检查对每个维度 $j$，层索引集合 $\\left\\{\\left\\lfloor N u_{ij}\\right\\rfloor: i=1,\\dots,N\\right\\}$ 是否等于 $\\{0,1,\\dots,N-1\\}$ 来验证分层属性，其中 $u_{ij}$ 表示样本 $i$ 在维度 $j$ 上的均匀样本值。\n  $5$) 计算场景削减目标 $J(S)$ 以及映射到经验边际后各维度的样本均值。\n\n实现细节：\n- 使用固定的随机种子 $12345$ 以保证可复现性。\n- 每个维度的经验逆累积分布函数必须实现为点 $(q_k, v^{(j)}_k)$ 之间的分段线性插值，其中 $q_k = \\frac{k}{L_j-1}$，$k \\in \\{0,\\dots,L_j-1\\}$。\n\n测试套件：\n提供以下测试用例，其中每个“值列表”是各维度的经验支撑值数组列表：\n- 用例 $1$ (常规成功路径)：$N=10$, $d=3$, $M=4$，经验支撑值\n  $v^{(1)} = (0.8, 1.0, 1.3, 1.35, 1.5, 1.7, 2.0, 2.3, 2.5, 2.8, 3.0)$,\n  $v^{(2)} = (50, 60, 75, 90, 120, 150, 200)$,\n  $v^{(3)} = (0.1, 0.2, 0.5, 0.9, 1.2, 1.5, 2.0)$。\n- 用例 $2$ (单个样本和边际的边界情况)：$N=1$, $d=1$, $M=1$，经验支撑值\n  $v^{(1)} = (10, 20, 30)$。\n- 用例 $3$ (无削减，同一性检查)：$N=5$, $d=2$, $M=5$，经验支撑值\n  $v^{(1)} = (0, 1, 2, 3, 4, 5)$,\n  $v^{(2)} = (-2, -1, 0, 1, 3, 6)$。\n- 用例 $4$ (极端削减)：$N=8$, $d=4$, $M=1$，经验支撑值\n  $v^{(1)} = (0.0, 0.4, 0.8, 1.2, 1.6, 2.0)$,\n  $v^{(2)} = (10, 15, 25, 40)$,\n  $v^{(3)} = (100, 120, 140, 160, 200)$,\n  $v^{(4)} = (-0.5, 0.0, 0.5, 1.0, 1.5)$。\n\n答案规范：\n- 对于每个测试用例，您的程序必须生成形如 $[b, j, m]$ 的结果，其中 $b$ 是一个布尔值，表示各维度的分层有效性；$j$ 是一个浮点数，等于场景削减目标 $J(S)$；$m$ 是一个浮点数列表，等于映射后各维度的样本均值。将所有测试用例的结果汇总到一个列表的列表中。\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果，且不含空格。例如，结构必须类似于单行上的 $[[\\text{True},0.123,[1.0,2.0]],\\dots]$。",
            "solution": "该问题是有效的。它在科学上基于统计学和数值优化的原理，特别是拉丁超立方抽样（LHS）、逆变换采样和用于场景削减的贪心算法。该问题是适定的，提供了所有必要的定义、数据和清晰的算法规范。我现在将提供一个完整且合理的解决方案。\n\n该解决方案分为三个主要阶段：场景生成、映射到经验分布和场景削减。\n\n### 1. 通过拉丁超立方抽样（LHS）生成场景\n\nLHS 是一种分层抽样技术，旨在从一个 $d$ 维概率分布中生成一组 $N$ 个样本，确保每个边际分布都得到良好覆盖。\n\n**分层原理**：核心思想是将每个边际分布的支撑集划分为 $N$ 个等概率的层，并从每个层中精确抽取一个样本。对于具有独立边际的随机向量，我们通常处理一个 $d$ 维单位超立方体 $[0,1]^d$。每个维度被划分为 $N$ 个不相交的区间 $I_i = [\\frac{i}{N}, \\frac{i+1}{N})$，其中 $i \\in \\{0, 1, \\dots, N-1\\}$。\n\n**LHS 算法**：为生成 $N$ 个样本 $\\{u_k\\}_{k=1}^N$（其中每个 $u_k \\in [0,1]^d$），LHS 算法按以下步骤进行：\n1.  对于每个维度 $j \\in \\{1, \\dots, d\\}$，生成层索引集合 $\\{0, 1, \\dots, N-1\\}$ 的一个独立随机排列 $\\pi_j$。\n2.  对于每个维度 $j$，为 $k \\in \\{1, \\dots, N\\}$ 生成 $N$ 个独立的随机数 $v_{kj} \\sim \\mathrm{Unif}(0,1)$。\n3.  第 $k$ 个样本向量 $u_k$ 的第 $j$ 个分量构造如下：\n    $$\n    u_{kj} = \\frac{\\pi_j(k-1) + v_{kj}}{N}\n    $$\n这种构造保证了对于任何维度 $j$，值集合 $\\{\\lfloor N u_{kj} \\rfloor\\}_{k=1}^N$ 恰好是集合 $\\{0, 1, \\dots, N-1\\}$。这一特性被称为分层，它确保样本在每个边际分布的整个范围内均匀分布，这是相比于简单蒙特卡洛采样的关键优势。为保证可复现性，用于排列和均匀抽样的随机数生成器需要设定种子。\n\n### 2. 将均匀样本映射到经验边际\n\n$[0,1]^d$ 上的均匀 LHS 样本必须被转换为遵循能源系统模型中变量（如风速、电价）所需的目标分布（通常是非均匀的）的样本。\n\n**逆变换采样**：这是通过逆变换采样原理实现的。如果 $U$ 是 $[0,1]$ 上的均匀随机变量，而 $F_X$ 是随机变量 $X$ 的累积分布函数（CDF），那么随机变量 $X' = F_X^{-1}(U)$ 与 $X$ 具有相同的分布。这里，$F_X^{-1}$ 是广义逆 CDF，或称分位数函数。\n\n**经验逆累积分布函数**：在实践中，我们通常拥有经验数据而非解析的 CDF。问题为每个维度 $j$ 提供了经验支撑值 $v_0^{(j)}, v_1^{(j)}, \\dots, v_{L_j-1}^{(j)}$，它们对应于等距的分位数 $q_k = \\frac{k}{L_j-1}$。这些点 $(q_k, v_k^{(j)})$ 定义了逆 CDF 的一个经验近似 $\\widehat{F}_j^{-1}$。对于任何 $p \\in [0,1]$，函数 $\\widehat{F}_j^{-1}(p)$ 的值通过在给定点之间进行分段线性插值来评估。此映射将均匀 LHS 样本的每个分量 $u_{kj}$ 转换为一个场景值 $x_{kj}$：\n$$\nx_{kj} = \\widehat{F}_j^{-1}(u_{kj})\n$$\n得到的集合 $\\{x_k\\}_{k=1}^N$ 构成了初始的高保真度场景集。\n\n### 3. 通过贪心前向选择进行场景削减\n\n为了在优化模型中保持计算的可行性，场景数量 $N$ 通常必须削减到一个更小的、具有代表性的规模为 $M$ 的子集。目标是选择一个能够“最佳”代表原始分布的场景子集。\n\n**目标函数**：一个大小为 $M$ 的削减集 $S \\subset \\{1, \\dots, N\\}$ 的质量由以下目标函数衡量：\n$$\nJ(S) = \\sum_{i=1}^N w_i \\min_{s \\in S} \\|x_i - x_s\\|_2 = \\frac{1}{N} \\sum_{i=1}^N \\min_{s \\in S} \\|x_i - x_s\\|_2\n$$\n该函数计算每个原始场景到其在削减集中最近代表点的平均距离。最小化 $J(S)$ 等价于最小化原始离散分布（有 $N$ 个点）与削减后分布（有 $M$ 个点，每个选定点 $x_s$ 的新权重等于它所代表的原始点的权重之和）之间的 1-Wasserstein 距离。\n\n**贪心算法**：找到最优子集 $S$ 是一个计算上困难的问题。一种常见且有效的启发式方法是贪心前向选择。\n1.  **如果 $M \\ge N$**：问题是平凡的。所有场景都被选中，目标函数 $J(S)$ 为 $0$。\n2.  **初始化 ($k=1$)**：选择第一个场景 $x_{s_1}$ 以最小化 $J(\\{x_{s_1}\\}) = \\frac{1}{N} \\sum_{i=1}^N \\|x_i - x_{s_1}\\|_2$。最小化该和的点 $x_{s_1}$ 被称为集合 $\\{x_i\\}_{i=1}^N$ 的几何中位数。\n3.  **迭代步骤 ($k=2, \\dots, M$)**：在每一步 $k$，给定已选的大小为 $k-1$ 的集合 $S_{k-1}$，算法从剩余的候选中选择下一个场景 $x_{s_k}$。被选中的 $x_{s_k}$ 是使得新集合 $S_k = S_{k-1} \\cup \\{x_{s_k}\\}$ 的目标函数最小化的那一个。这等价于选择能使目标函数下降最大的候选点 $x_c$，即最大化 $J(S_{k-1}) - J(S_{k-1} \\cup \\{x_c\\})$。\n    令 $d_i^{k-1} = \\min_{s \\in S_{k-1}} \\|x_i - x_s\\|_2$ 为每个点 $x_i$ 当前的最小距离。要添加的下一个点 $x_c$ 是最小化新的总距离的点：\n    $$\n    \\arg\\min_{c \\notin S_{k-1}} \\sum_{i=1}^N \\min(d_i^{k-1}, \\|x_i - x_c\\|_2)\n    $$\n    重复此过程，直到选出 $M$ 个场景。最终计算的值是分层有效性布尔值、最终目标值 $J(S_M)$ 以及原始 $N$ 个映射后场景的均值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the scenario generation and reduction process\n    for all specified test cases.\n    \"\"\"\n    rng = np.random.default_rng(12345)\n\n    test_cases = [\n        (10, 3, 4, [\n            (0.8, 1.0, 1.3, 1.35, 1.5, 1.7, 2.0, 2.3, 2.5, 2.8, 3.0),\n            (50, 60, 75, 90, 120, 150, 200),\n            (0.1, 0.2, 0.5, 0.9, 1.2, 1.5, 2.0)\n        ]),\n        (1, 1, 1, [(10, 20, 30)]),\n        (5, 2, 5, [\n            (0, 1, 2, 3, 4, 5),\n            (-2, -1, 0, 1, 3, 6)\n        ]),\n        (8, 4, 1, [\n            (0.0, 0.4, 0.8, 1.2, 1.6, 2.0),\n            (10, 15, 25, 40),\n            (100, 120, 140, 160, 200),\n            (-0.5, 0.0, 0.5, 1.0, 1.5)\n        ])\n    ]\n\n    all_results = []\n    for N, d, M, v_lists in test_cases:\n        result = process_case(N, d, M, v_lists, rng)\n        all_results.append(result)\n\n    # Format the final output as a single-line string with no spaces.\n    formatted_results = []\n    for b, j, m_list in all_results:\n        b_str = str(b)\n        j_str = f\"{j:.12f}\".rstrip('0').rstrip('.') if j != 0 else \"0.0\"\n        m_str_list = [f\"{val:.12f}\".rstrip('0').rstrip('.') for val in m_list]\n        m_str = f\"[{','.join(m_str_list)}]\"\n        formatted_results.append(f\"[{b_str},{j_str},{m_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef process_case(N, d, M, v_lists, rng):\n    \"\"\"\n    Processes a single test case for scenario generation and reduction.\n\n    Args:\n        N (int): Number of scenarios to generate.\n        d (int): Number of dimensions.\n        M (int): Number of scenarios to reduce to.\n        v_lists (list of tuples): Empirical support values for each dimension.\n        rng (np.random.Generator): Seeded random number generator.\n\n    Returns:\n        list: A list containing [stratification_validity, objective_J, means].\n    \"\"\"\n    \n    # 1. Generate N-by-d uniform stratified samples using LHS\n    if N > 0:\n        unif_samples = np.zeros((N, d))\n        for j in range(d):\n            perm = rng.permutation(N)\n            rand_unif_in_stratum = rng.uniform(size=N)\n            unif_samples[:, j] = (perm + rand_unif_in_stratum) / N\n    else:\n        unif_samples = np.array([[]])\n\n    # 4. Validate the stratification property\n    is_stratified = True\n    if N > 0:\n        for j in range(d):\n            strata_found = np.floor(unif_samples[:, j] * N).astype(int)\n            if set(strata_found) != set(range(N)):\n                is_stratified = False\n                break\n    \n    # 2. Map uniform samples to empirical marginals\n    if N > 0:\n        mapped_scenarios = np.zeros((N, d))\n        for j in range(d):\n            v = np.array(v_lists[j])\n            L = len(v)\n            if L > 1:\n                q = np.linspace(0, 1, num=L)\n                mapped_scenarios[:, j] = np.interp(unif_samples[:, j], q, v)\n            elif L == 1:\n                mapped_scenarios[:, j] = v[0]\n    else:\n        mapped_scenarios = np.zeros((0, d))\n\n    # 5. Compute sample means for the original N scenarios\n    if N > 0:\n        means = np.mean(mapped_scenarios, axis=0).tolist()\n    else:\n        means = [float('nan')] * d\n\n    # 3. Perform greedy forward selection scenario reduction\n    if M >= N or N == 0:\n        objective_J = 0.0\n    else:\n        dist_matrix = cdist(mapped_scenarios, mapped_scenarios, 'euclidean')\n        \n        selected_indices = []\n        candidate_indices = list(range(N))\n\n        # First scenario selection (approximating geometric median)\n        min_sum_dist = np.inf\n        best_c_step1 = -1\n        for c in candidate_indices:\n            current_sum_dist = np.sum(dist_matrix[:, c])\n            if current_sum_dist  min_sum_dist:\n                min_sum_dist = current_sum_dist\n                best_c_step1 = c\n        \n        selected_indices.append(best_c_step1)\n        candidate_indices.remove(best_c_step1)\n        min_dists = dist_matrix[:, best_c_step1].copy()\n\n        # Greedy selection for the remaining M-1 scenarios\n        for _ in range(M - 1):\n            if not candidate_indices:\n                break\n            \n            min_objective_val = np.inf\n            best_c_step_k = -1\n\n            for c in candidate_indices:\n                potential_min_dists = np.minimum(min_dists, dist_matrix[:, c])\n                current_objective = np.sum(potential_min_dists)\n                if current_objective  min_objective_val:\n                    min_objective_val = current_objective\n                    best_c_step_k = c\n            \n            selected_indices.append(best_c_step_k)\n            candidate_indices.remove(best_c_step_k)\n            min_dists = np.minimum(min_dists, dist_matrix[:, best_c_step_k])\n\n        objective_J = np.sum(min_dists) / N\n\n    return [is_stratified, objective_J, means]\n\nsolve()\n```"
        },
        {
            "introduction": "确保简化后的场景集能准确反映原始不确定性的统计特性至关重要。本练习聚焦于矩匹配这一基本技术，您将推导并应用所需的线性约束，以保留均值和方差等关键矩。掌握此方法对于创建可靠的场景集至关重要，这些场景集将引导优化模型做出稳健的决策。",
            "id": "4121537",
            "problem": "在电力系统的长期规划中，不确定的净负荷 $X$ 通常用一个有限的场景集合来表示，以便对运行和投资决策进行易于处理的优化。考虑用一个离散测度来近似 $X$ 的分布，该测度支撑于具有场景权重 $p_i$ 的代表性点 $\\xi_i$，其中 $i=1,\\dots,m$。决策者要求该近似的前 $k$ 阶矩与 $X$ 的相应矩相匹配，以保留用于系统规模确定和可靠性评估的关键统计特性。设 $\\phi_j:\\mathbb{R}\\to\\mathbb{R}$ 为与前 $k$ 阶矩相关的固定基函数，其中 $j=1,\\dots,k$。\n\n(a) 仅从数学期望的定义和随机变量的离散场景近似的构造出发，推导场景权重 $p_i$ 为保留由基函数 $\\phi_j$ 诱导的前 $k$ 阶矩而必须满足的线性约束。你的推导必须明确说明为什么这些约束在权重上是线性的，并且必须包括权重的归一化。\n\n(b) 在能源系统中净负荷的简化分布鲁棒场景削减背景下，假设一个规划者在 $\\xi_1=50$ MW，$\\xi_2=120$ MW 和 $\\xi_3=200$ MW 处选择了 $m=3$ 个代表性场景。规划者希望通过取矩基函数为 $\\phi_1(x)=x$ 和 $\\phi_2(x)=x^2$ 来保留 $X$ 的前两阶矩，已知目标为 $\\mathbb{E}[X]=120$ MW 和 $\\mathbb{E}[X^2]=15300$ $(\\mathrm{MW})^2$。施加归一化条件 $\\sum_{i=1}^{3} p_i=1$，并计算能精确保留这两阶矩的场景权重 $p_1$、$p_2$ 和 $p_3$。以精确分数形式的行矩阵 $\\begin{pmatrix} p_1  p_2  p_3 \\end{pmatrix}$ 报告最终权重。权重是无量纲的，因此最终报告中不需要单位。不需要四舍五入。",
            "solution": "所提出的问题是有效的。它在科学上植根于概率论及其在能源系统随机建模中的应用，这是一个成熟的领域。问题是适定的，提供了形成可解线性方程组所需的所有数据和关系。语言是客观的，参数在物理上是合理的。因此，我们可以进行完整解答。\n\n(a) 线性约束的推导\n\n设 $X$ 是一个连续随机变量，代表不确定的净负荷，其概率密度函数表示为 $f_X(x)$。函数 $g(X)$ 的期望由以下积分定义：\n$$ \\mathbb{E}[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\,dx $$\n问题陈述我们关心的是 $X$ 的前 $k$ 阶矩，这些矩是由一组 $k$ 个基函数 $\\phi_j:\\mathbb{R}\\to\\mathbb{R}$ ($j=1, \\dots, k$) 诱导的。第 $j$ 阶矩，记为 $\\mu_j$，是相应基函数 $\\phi_j(X)$ 的期望值：\n$$ \\mu_j = \\mathbb{E}[\\phi_j(X)] = \\int_{-\\infty}^{\\infty} \\phi_j(x) f_X(x) \\,dx $$\n这些值 $\\mu_j$被视为已知的目标量。\n\n$X$ 的分布由一个离散测度来近似。这意味着我们将连续随机变量 $X$ 替换为一个离散随机变量，我们称之为 $\\tilde{X}$。这个离散变量 $\\tilde{X}$ 可以取 $m$ 个特定值中的一个，即 $\\{\\xi_1, \\xi_2, \\dots, \\xi_m\\}$，这些是代表性点。每个点 $\\xi_i$ 都被赋予一个场景权重或概率 $p_i$，其中 $i=1, \\dots, m$。为使这些权重构成一个有效的概率分布，它们必须是非负的 ($p_i \\ge 0$) 并且总和为一。问题明确要求包含此归一化约束：\n$$ \\sum_{i=1}^{m} p_i = 1 $$\n对于这个离散随机变量，函数 $g(\\tilde{X})$ 的期望由一个加权和给出：\n$$ \\mathbb{E}[g(\\tilde{X})] = \\sum_{i=1}^{m} g(\\xi_i) p_i $$\n核心要求是离散近似 $\\tilde{X}$ 的矩必须与原始随机变量 $X$ 的矩相匹配。这意味着对于每个基函数 $\\phi_j$，以下等式必须成立：\n$$ \\mathbb{E}[\\phi_j(\\tilde{X})] = \\mathbb{E}[\\phi_j(X)] $$\n代入离散和连续期望的定义，我们得到矩匹配约束：\n$$ \\sum_{i=1}^{m} \\phi_j(\\xi_i) p_i = \\mu_j \\quad \\text{for } j=1, \\dots, k $$\n为了说明这些约束为什么是线性的，我们必须确定变量和系数。待确定的未知量是场景权重 $p_1, p_2, \\dots, p_m$。代表性点 $\\xi_i$ 和基函数 $\\phi_j$ 是给定且固定的。因此，对于任何给定的 $i$ 和 $j$，$\\phi_j(\\xi_i)$ 的值是一个常数标量。让我们将这些常数定义为 $A_{ji} = \\phi_j(\\xi_i)$。矩匹配方程可以重写为：\n$$ \\sum_{i=1}^{m} A_{ji} p_i = \\mu_j \\quad \\text{for } j=1, \\dots, k $$\n这些方程中的每一个都将目标矩 $\\mu_j$ 表示为未知权重 $p_i$ 与常数系数 $A_{ji}$ 的线性组合。根据定义，这是一个包含 $k$ 个线性方程的方程组。归一化条件 $\\sum_{i=1}^{m} p_i = 1$ 同样是关于变量 $p_i$ 的一个线性方程。\n\n因此，场景权重 $p_i$ 必须满足的完整约束集由 $k+1$ 个线性方程组成：\n1. 归一化：$\\sum_{i=1}^{m} p_i = 1$\n2. 矩匹配：$\\sum_{i=1}^{m} \\phi_j(\\xi_i) p_i = \\mu_j$ for $j=1, \\dots, k$\n\n(b) 场景权重的计算\n\n给定 $m=3$ 个场景，其代表性点为 $\\xi_1 = 50$，$\\xi_2 = 120$ 和 $\\xi_3 = 200$。我们希望使用基函数 $\\phi_1(x) = x$ 和 $\\phi_2(x) = x^2$ 来保留前 $k=2$ 阶矩。目标矩为 $\\mathbb{E}[X] = \\mu_1 = 120$ 和 $\\mathbb{E}[X^2] = \\mu_2 = 15300$。我们还必须施加归一化约束 $\\sum_{i=1}^{3} p_i = 1$。\n\n这个设置产生了一个关于3个未知权重 $p_1, p_2, p_3$ 的3个线性方程组：\n1. 归一化：$p_1 + p_2 + p_3 = 1$\n2. 一阶矩 ($\\mathbb{E}[\\tilde{X}] = \\mu_1$)：$\\xi_1 p_1 + \\xi_2 p_2 + \\xi_3 p_3 = 120 \\implies 50 p_1 + 120 p_2 + 200 p_3 = 120$\n3. 二阶矩 ($\\mathbb{E}[\\tilde{X}^2] = \\mu_2$)：$\\xi_1^2 p_1 + \\xi_2^2 p_2 + \\xi_3^2 p_3 = 15300 \\implies 2500 p_1 + 14400 p_2 + 40000 p_3 = 15300$\n\n让我们明确地写出这个方程组：\n$$\n\\begin{cases}\np_1 + p_2 + p_3 = 1 \\\\\n50 p_1 + 120 p_2 + 200 p_3 = 120 \\\\\n2500 p_1 + 14400 p_2 + 40000 p_3 = 15300\n\\end{cases}\n$$\n我们可以通过除以公因数来简化第二个和第三个方程。将第二个方程除以 $10$，第三个方程除以 $100$：\n$$\n\\begin{cases}\np_1 + p_2 + p_3 = 1 \\quad (1)\\\\\n5 p_1 + 12 p_2 + 20 p_3 = 12 \\quad (2)\\\\\n25 p_1 + 144 p_2 + 400 p_3 = 153 \\quad (3)\n\\end{cases}\n$$\n从方程 $(1)$，我们将 $p_3$ 表示为 $p_1$ 和 $p_2$ 的函数：$p_3 = 1 - p_1 - p_2$。我们将其代入方程 $(2)$ 和 $(3)$。\n\n代入 $(2)$：\n$5 p_1 + 12 p_2 + 20(1 - p_1 - p_2) = 12$\n$5 p_1 + 12 p_2 + 20 - 20 p_1 - 20 p_2 = 12$\n$-15 p_1 - 8 p_2 = -8 \\implies 15 p_1 + 8 p_2 = 8 \\quad (A)$\n\n代入 $(3)$：\n$25 p_1 + 144 p_2 + 400(1 - p_1 - p_2) = 153$\n$25 p_1 + 144 p_2 + 400 - 400 p_1 - 400 p_2 = 153$\n$-375 p_1 - 256 p_2 = -247 \\implies 375 p_1 + 256 p_2 = 247 \\quad (B)$\n\n现在我们求解关于 $p_1$ 和 $p_2$ 的 $2 \\times 2$ 方程组。从方程 $(A)$，我们分离出 $p_2$：\n$8 p_2 = 8 - 15 p_1 \\implies p_2 = 1 - \\frac{15}{8} p_1$\n\n将 $p_2$ 的这个表达式代入方程 $(B)$：\n$375 p_1 + 256 \\left(1 - \\frac{15}{8} p_1\\right) = 247$\n$375 p_1 + 256 - \\frac{256 \\times 15}{8} p_1 = 247$\n由于 $\\frac{256}{8} = 32$，方程变为：\n$375 p_1 + 256 - (32 \\times 15) p_1 = 247$\n$375 p_1 + 256 - 480 p_1 = 247$\n$-105 p_1 = -9$\n$p_1 = \\frac{9}{105} = \\frac{3}{35}$\n\n现在，我们求 $p_2$：\n$p_2 = 1 - \\frac{15}{8} p_1 = 1 - \\frac{15}{8} \\left(\\frac{3}{35}\\right) = 1 - \\frac{45}{280} = 1 - \\frac{9}{56} = \\frac{56-9}{56} = \\frac{47}{56}$\n\n最后，我们使用 $p_3 = 1 - p_1 - p_2$ 来求 $p_3$：\n$p_3 = 1 - \\frac{3}{35} - \\frac{47}{56}$\n为了对分数求和，我们找到 $35 = 5 \\times 7$ 和 $56 = 8 \\times 7$ 的公分母。最小公倍数是 $5 \\times 8 \\times 7 = 280$。\n$p_3 = \\frac{280}{280} - \\frac{3 \\times 8}{35 \\times 8} - \\frac{47 \\times 5}{56 \\times 5} = \\frac{280}{280} - \\frac{24}{280} - \\frac{235}{280} = \\frac{280 - 24 - 235}{280} = \\frac{21}{280}$\n通过将分子和分母除以它们的最大公约数 $7$ 来简化 $p_3$ 的分数：\n$p_3 = \\frac{21 \\div 7}{280 \\div 7} = \\frac{3}{40}$\n\n计算出的权重为 $p_1 = \\frac{3}{35}$，$p_2 = \\frac{47}{56}$ 和 $p_3 = \\frac{3}{40}$。所有权重均为正数，因此它们构成了一个有效的概率分布。最终答案以精确分数形式的行矩阵呈现。",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{3}{35}  \\frac{47}{56}  \\frac{3}{40} \\end{pmatrix}} $$"
        },
        {
            "introduction": "一个场景集的最终考验在于其所引导的决策质量，尤其是在风险情境下。本实践探讨了场景采样策略与风险规避优化之间的关键相互作用，重点关注如何处理罕见但影响重大的事件。通过分析过采样和重要性加权如何影响一个涉及条件风险价值（CVaR）的决策问题，您将更深刻地理解如何构建对尾部风险敏感的模型。",
            "id": "4121477",
            "problem": "一个能源系统规划者必须选择是否安装备用发电机，以减轻与高需求和低可再生能源输出同时发生的罕见、高成本事件。决策变量为 $x \\in \\{0,1\\}$，其中 $x=1$ 表示安装备用发电机，$x=0$ 表示不安装。确定性投资成本为 $I(1)=15$ 和 $I(0)=0$。运营损失取决于情景：在正常情景 $\\omega_N$ 下，运营损失为 $L_N(0)=10$ 和 $L_N(1)=12$；在罕见的压力情景 $\\omega_R$ 下，运营损失为 $L_R(0)=110$ 和 $L_R(1)=30$。情景的真实概率分布为 $p(\\omega_N)=0.99$，$p(\\omega_R)=0.01$。\n\n对于给定的决策 $x$，规划者的目标是最小化\n$$\nJ(x) \\;=\\; I(x) \\;+\\; \\mathbb{E}_p\\!\\big[L(x,\\omega)\\big] \\;+\\; \\lambda \\,\\mathrm{CVaR}_{\\alpha}\\!\\big(L(x,\\omega)\\big),\n$$\n其中 $\\lambda > 0$ 是一个风险规避参数，$\\mathrm{CVaR}_{\\alpha}$ 表示置信水平为 $\\alpha \\in (0,1)$ 的条件风险价值。对于此问题，取 $\\lambda=0.1$ 和 $\\alpha=0.99$，并通过相干风险度量表示法定义条件风险价值 (CVaR)\n$$\n\\mathrm{CVaR}_{\\alpha}(L) \\;=\\; \\min_{\\eta \\in \\mathbb{R}} \\left\\{ \\eta \\;+\\; \\frac{1}{1-\\alpha}\\,\\mathbb{E}\\big[(L-\\eta)_+\\big] \\right\\},\n$$\n其中 $(u)_+ = \\max\\{u,0\\}$，期望是相对于相关概率度量计算的。\n\n为了计算 $J(x)$ 的样本平均近似 (SAA)，考虑了两种情景生成策略：\n\n1. 从真实分布 $p$ 进行标准抽样，对每个抽样情景使用相等的权重。\n2. 罕见事件强调：使用抽样分布 $q$（其中 $q(\\omega_N)=0.5$ 和 $q(\\omega_R)=0.5$）对罕见情景进行过采样，并采用以下两种方式之一：(i) 对抽样情景使用相等权重（不重新加权），或 (ii) 使用重要性权重 $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$，以使样本加权期望以 $\\mathbb{E}_p[\\cdot]$ 为目标。\n\n使用期望和条件风险价值的定义，分析罕见事件强调如何影响优化决策和尾部风险估计。选择所有正确的陈述。\n\nA. 对罕见事件进行过采样而不校正权重，可能会因样本平均期望的失真而导致最优决策转向安装备用发电机，即使真实的风险感知目标倾向于不安装备用发电机。\n\nB. 使用正确的重要性权重 $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$，加权样本平均近似是 $\\mathbb{E}_p[L(x,\\omega)]$ 和 $\\mathrm{CVaR}_{\\alpha}(L(x,\\omega))$ 的一致估计量，并且随着样本数量的增加，样本下的最优决策会收敛到真实的风险感知最优解。\n\nC. 与从 $p$ 进行简单随机抽样相比，使用正确的重要性权重强调罕见事件必然会降低期望和条件风险价值估计量的方差。\n\nD. 即使权重已根据原始概率进行调整，仅保留极端情景的情景削减方法也总能保留风险感知问题的最优决策。\n\nE. 在具有 $p(\\omega_N)=0.99$，$p(\\omega_R)=0.01$，$\\alpha=0.99$ 和给定损失的离散双情景示例中，过采样至 $q(\\omega_N)=q(\\omega_R)=0.5$ 且不重新加权，对于 $x=0$ 和 $x=1$ 的 $\\mathrm{CVaR}_{0.99}$ 保持不变，但改变了期望；因此，最优决策的任何变化都源于期望的失真，而非尾部风险估计的变化。",
            "solution": "用户希望我验证并解决一个关于能源系统中情景生成和风险感知优化的问题。\n\n### 步骤 1：提取已知条件\n\n-   **决策变量**：$x \\in \\{0, 1\\}$，其中 $x=1$ 表示安装备用发电机，$x=0$ 表示不安装。\n-   **投资成本**：$I(0)=0$ 和 $I(1)=15$。\n-   **情景**：正常情景 $\\omega_N$ 和罕见压力情景 $\\omega_R$。\n-   **运营损失**：一个依赖于情景的函数 $L(x,\\omega)$。\n    -   对于 $x=0$：$L_N(0)=L(0,\\omega_N)=10$ 和 $L_R(0)=L(0,\\omega_R)=110$。\n    -   对于 $x=1$：$L_N(1)=L(1,\\omega_N)=12$ 和 $L_R(1)=L(1,\\omega_R)=30$。\n-   **真实概率分布 ($p$)**：$p(\\omega_N)=0.99$ 和 $p(\\omega_R)=0.01$。\n-   **目标函数**：最小化 $J(x) = I(x) + \\mathbb{E}_p[L(x,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{\\alpha}[L(x,\\omega)]$。\n-   **参数**：风险规避 $\\lambda=0.1$ 和置信水平 $\\alpha=0.99$。\n-   **CVaR 定义**：$\\mathrm{CVaR}_{\\alpha}(L) = \\min_{\\eta \\in \\mathbb{R}} \\{ \\eta + \\frac{1}{1-\\alpha}\\,\\mathbb{E}[(L-\\eta)_+] \\}$，其中 $(u)_+ = \\max\\{u,0\\}$。\n-   **抽样策略**：\n    1.  从 $p$ 进行标准抽样。\n    2.  罕见事件强调（过采样），使用分布 $q$，其中 $q(\\omega_N)=0.5, q(\\omega_R)=0.5$。\n        -   (i) 不重新加权（相等权重）。\n        -   (ii) 使用重要性权重 $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n-   **科学依据**：该问题使用了随机优化和风险管理中的标准、成熟的概念，包括期望值、条件风险价值 (CVaR)、样本平均近似 (SAA) 和重要性抽样。这些是能源系统建模和运筹学中的核心技术。其设置在数学上是严谨的。\n-   **适定性**：所有必要的数据、函数和参数（$I(x)$, $L(x,\\omega)$, $p(\\omega)$, $q(\\omega)$, $\\lambda$, $\\alpha$）都已明确定义。目标函数已清楚指定。选项中提出的问题可以通过直接计算和应用相关理论来回答。\n-   **客观性**：问题以精确、定量和无偏见的语言陈述。\n\n该问题不违反任何有效性标准。它在科学上是合理的、适定的、客观的、完整的且非平凡的。\n\n### 步骤 3：结论和行动\n\n问题陈述是**有效的**。我将继续进行求解。\n\n### 优化问题分析\n\n首先，我将使用真实概率分布 $p$ 计算 $x=0$ 和 $x=1$ 的目标函数 $J(x)$，从而确定真实的最优决策。\n\n**情况 1：$x=0$（不安装发电机）**\n-   投资成本：$I(0) = 0$。\n-   损失分布：$\\{10, 110\\}$，概率为 $\\{0.99, 0.01\\}$。\n-   期望损失：$\\mathbb{E}_p[L(0,\\omega)] = L_N(0)p(\\omega_N) + L_R(0)p(\\omega_R) = (10)(0.99) + (110)(0.01) = 9.9 + 1.1 = 11$。\n-   条件风险价值（$\\mathrm{CVaR}_{0.99}$）：置信水平 $\\alpha=0.99$ 对应于尾部概率 $1-\\alpha=0.01$。罕见情景 $\\omega_R$ 的概率恰好是 $0.01$。因此，CVaR 是最坏 1% 情况下的损失期望，即罕见情景下的损失。\n    $$ \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = L_R(0) = 110 $$\n    这可以使用给定定义，并取 $\\eta = \\mathrm{VaR}_{0.99}(L(0,\\omega)) = 10$ 进行正式验证。\n    $$ \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = 10 + \\frac{1}{1-0.99} \\left[ (10-10)_+ \\cdot 0.99 + (110-10)_+ \\cdot 0.01 \\right] = 10 + \\frac{1}{0.01} [100 \\cdot 0.01] = 10 + 100 = 110 $$\n-   $x=0$ 的总目标值：\n    $$ J(0) = I(0) + \\mathbb{E}_p[L(0,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{0.99}(L(0,\\omega)) = 0 + 11 + (0.1)(110) = 11 + 11 = 22 $$\n\n**情况 2：$x=1$（安装发电机）**\n-   投资成本：$I(1) = 15$。\n-   损失分布：$\\{12, 30\\}$，概率为 $\\{0.99, 0.01\\}$。\n-   期望损失：$\\mathbb{E}_p[L(1,\\omega)] = L_N(1)p(\\omega_N) + L_R(1)p(\\omega_R) = (12)(0.99) + (30)(0.01) = 11.88 + 0.3 = 12.18$。\n-   条件风险价值（$\\mathrm{CVaR}_{0.99}$）：与 $x=0$ 的情况类似，CVaR 是罕见情景下的损失。\n    $$ \\mathrm{CVaR}_{0.99}(L(1,\\omega)) = L_R(1) = 30 $$\n-   $x=1$ 的总目标值：\n    $$ J(1) = I(1) + \\mathbb{E}_p[L(1,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{0.99}(L(1,\\omega)) = 15 + 12.18 + (0.1)(30) = 15 + 12.18 + 3 = 30.18 $$\n\n**关于真实最优决策的结论**\n由于 $J(0) = 22  J(1) = 30.18$，真实的最优决策是 $x^*=0$，即不安装发电机。\n\n### 选项评估\n\n**A. 对罕见事件进行过采样而不校正权重，可能会因样本平均期望的失真而导致最优决策转向安装备用发电机，即使真实的风险感知目标倾向于不安装备用发电机。**\n\n该陈述建议在过采样分布 $q(\\omega_N)=0.5, q(\\omega_R)=0.5$ 下分析问题，就好像它是真实分布一样。让我们在这个失真的分布下计算目标 $J_q(x)$。\n\n-   对于 $x=0$：\n    -   $\\mathbb{E}_q[L(0,\\omega)] = (10)(0.5) + (110)(0.5) = 5 + 55 = 60$。\n    -   CVaR 的计算需要考虑分布 $q$ 下的尾部。最坏的结果是损失 110，概率为 0.5。由于 $0.5 > 1-\\alpha = 0.01$，最坏的 1% 结果都对应于罕见事件，所以 $\\mathrm{CVaR}_{0.99,q}(L(0,\\omega)) = 110$。\n    -   $J_q(0) = 0 + 60 + (0.1)(110) = 71$。\n-   对于 $x=1$：\n    -   $\\mathbb{E}_q[L(1,\\omega)] = (12)(0.5) + (30)(0.5) = 6 + 15 = 21$。\n    -   类似地，最坏的结果是损失 30，概率为 $0.5 > 0.01$，所以 $\\mathrm{CVaR}_{0.99,q}(L(1,\\omega)) = 30$。\n    -   $J_q(1) = 15 + 21 + (0.1)(30) = 15 + 21 + 3 = 39$。\n\n在失真的信念 $q$ 下，最优决策是 $x^*_q=1$，因为 $J_q(1) = 39  J_q(0) = 71$。真实的最优决策是 $x=0$。决策发生了转变。原因是期望项的巨大失真。对于 $x=0$，期望从 11 增加到 60。对于 $x=1$，期望从 12.18 增加到 21。不安装发电机的感知惩罚增加得更多，导致了决策的转变。\n\n结论：**正确**。\n\n**B. 使用正确的重要性权重 $w(\\omega)=\\frac{p(\\omega)}{q(\\omega)}$，加权样本平均近似是 $\\mathbb{E}_p[L(x,\\omega)]$ 和 $\\mathrm{CVaR}_{\\alpha}(L(x,\\omega))$ 的一致估计量，并且随着样本数量的增加，样本下的最优决策会收敛到真实的风险感知最优解。**\n\n该陈述描述了重要性抽样 (IS) 的基本原理。对于任何函数 $g(\\omega)$，其在分布 $q$ 下的加权样本平均的期望等于在分布 $p$ 下的真实期望：\n$$ \\mathbb{E}_q[w(\\omega)g(\\omega)] = \\sum_{\\omega} q(\\omega) \\frac{p(\\omega)}{q(\\omega)} g(\\omega) = \\sum_{\\omega} p(\\omega)g(\\omega) = \\mathbb{E}_p[g(\\omega)] $$\n这表明加权样本均值是真实均值的无偏估计量。根据大数定律，样本平均近似 (SAA) 将收敛到真实值。这直接适用于估计 $\\mathbb{E}_p[L(x,\\omega)]$。它也适用于估计 CVaR 公式内部的期望 $\\mathbb{E}_p[(L-\\eta)_+]$。由于目标函数 $J(x)$ 的所有分量的 SAA 估计量都是一致的（即，随着样本量的增加收敛到它们的真实值），SAA 目标函数收敛到真实目标函数。在一般条件下，这确保了 SAA 问题的优化器收敛到真实问题的优化器。\n\n结论：**正确**。\n\n**C. 与从 $p$ 进行简单随机抽样相比，使用正确的重要性权重强调罕见事件必然会降低期望和条件风险价值估计量的方差。**\n\n这声称给定的重要性抽样方案对于所有感兴趣的估计量都*必然*是一种方差缩减技术。重要性抽样可以减少方差，但如果重要性分布 $q$ 选择不当，也可能增加方差。让我们通过计算 $\\mathbb{E}_p[L(1,\\omega)]$ 估计量的方差来检验这一说法。\n单个样本的估计量是 $Y = w(\\omega)L(1,\\omega)$。其在 $q$ 下的方差为 $\\mathrm{Var}_q(Y) = \\mathbb{E}_q[Y^2] - (\\mathbb{E}_q[Y])^2$。我们知道 $\\mathbb{E}_q[Y] = \\mathbb{E}_p[L(1,\\omega)] = 12.18$。\n$$ \\mathbb{E}_q[Y^2] = \\mathbb{E}_q\\left[\\left(\\frac{p(\\omega)}{q(\\omega)}L(1,\\omega)\\right)^2\\right] = \\sum_{\\omega}q(\\omega)\\frac{p(\\omega)^2}{q(\\omega)^2}L(1,\\omega)^2 = \\sum_{\\omega}\\frac{p(\\omega)^2}{q(\\omega)}L(1,\\omega)^2 $$\n$$ \\mathbb{E}_q[Y^2] = \\frac{p(\\omega_N)^2}{q(\\omega_N)}L_N(1)^2 + \\frac{p(\\omega_R)^2}{q(\\omega_R)}L_R(1)^2 = \\frac{0.99^2}{0.5}(12^2) + \\frac{0.01^2}{0.5}(30^2) $$\n$$ \\mathbb{E}_q[Y^2] = \\frac{0.9801}{0.5}(144) + \\frac{0.0001}{0.5}(900) = (1.9602)(144) + (0.0002)(900) = 282.2688 + 0.18 = 282.4488 $$\n因此，$\\mathrm{Var}_q(w(\\omega)L(1,\\omega)) = 282.4488 - 12.18^2 \\approx 282.4488 - 148.3524 = 134.0964$。\n\n现在，让我们计算从 $p$ 进行标准抽样的方variance。估计量是 $L(1,\\omega)$。\n$$ \\mathrm{Var}_p(L(1,\\omega)) = \\mathbb{E}_p[L(1,\\omega)^2] - (\\mathbb{E}_p[L(1,\\omega)])^2 $$\n$$ \\mathbb{E}_p[L(1,\\omega)^2] = (12^2)(0.99) + (30^2)(0.01) = (144)(0.99) + (900)(0.01) = 142.56 + 9 = 151.56 $$\n因此，$\\mathrm{Var}_p(L(1,\\omega)) = 151.56 - 12.18^2 \\approx 151.56 - 148.3524 = 3.2076$。\n由于 $134.0964 > 3.2076$，$\\mathbb{E}_p[L(1,\\omega)]$ 的重要性抽样估计量的方差显著*高于*标准抽样。“必然”这个词使得该陈述为假。\n\n结论：**不正确**。\n\n**D. 即使权重已根据原始概率进行调整，仅保留极端情景的情景削减方法也总能保留风险感知问题的最优决策。**\n\n让我们通过执行一个只保留“极端”情景 $\\omega_R$ 的情景削减来检验这一点。对于“调整权重”，最简单的解释是将全部概率质量分配给这个唯一剩下的情景。我们称这个新度量为 $p'$。所以，$p'(\\omega_R)=1$。\n-   对于 $x=0$：损失是确定性的 $L_R(0)=110$。期望是 110，一个常数的 CVaR 就是它本身，所以 $\\mathrm{CVaR}_{0.99}=110$。\n    $J_{red}(0) = 0 + 110 + (0.1)(110) = 121$。\n-   对于 $x=1$：损失是确定性的 $L_R(1)=30$。期望是 30，$\\mathrm{CVaR}_{0.99}=30$。\n    $J_{red}(1) = 15 + 30 + (0.1)(30) = 15 + 30 + 3 = 48$。\n\n在这个削减后的模型中，最优决策是 $x=1$，因为 $48  121$。这与真实的最优决策 $x=0$ 不同。“总是”这个词使得这个强有力的主张很容易被我们刚刚构建的反例所证伪。\n\n结论：**不正确**。\n\n**E. 在具有 $p(\\omega_N)=0.99$，$p(\\omega_R)=0.01$，$\\alpha=0.99$ 和给定损失的离散双情景示例中，过采样至 $q(\\omega_N)=q(\\omega_R)=0.5$ 且不重新加权，对于 $x=0$ 和 $x=1$ 的 $\\mathrm{CVaR}_{0.99}$ 保持不变，但改变了期望；因此，最优决策的任何变化都源于期望的失真，而非尾部风险估计的变化。**\n\n这个陈述对这个具体例子提出了几个可检验的主张。\n1.  **CVaR 不变**：\n    -   在 $p$ 分布下：如前所述，$\\mathrm{CVaR}_{0.99,p}(L(0,\\omega)) = 110$ 和 $\\mathrm{CVaR}_{0.99,p}(L(1,\\omega)) = 30$。\n    -   在 $q$ 分布下：如选项 A 中所计算，$\\mathrm{CVaR}_{0.99,q}(L(0,\\omega)) = 110$ 和 $\\mathrm{CVaR}_{0.99,q}(L(1,\\omega)) = 30$。\n    这是因为在两种分布中，单一最坏事件（$\\omega_R$）的概率都大于或等于尾部概率 $1-\\alpha=0.01$。因此，CVaR 就是该单一最坏事件中的损失 $L_R(x)$。CVaR 不变的说法是正确的。\n2.  **期望被改变**：\n    -   对于 $x=0$，$\\mathbb{E}_p=11$ 和 $\\mathbb{E}_q=60$。已改变。\n    -   对于 $x=1$，$\\mathbb{E}_p=12.18$ 和 $\\mathbb{E}_q=21$。已改变。这个说法是正确的。\n3.  **逻辑结论**：目标是 $J(x) = I(x) + \\mathbb{E}[L(x,\\omega)] + \\lambda \\cdot \\mathrm{CVaR}_{\\alpha}[L(x,\\omega)]$。当从分布 $p$ 转换到 $q$ 时，根据定义，投资成本 $I(x)$ 不变。如刚才所示，对于此问题的特定参数，$\\mathrm{CVaR}_{0.99}$ 项对于两个决策也保持不变。因此，每个决策的目标函数 $J(x)$ 的全部变化，以及相对差异 $J(1)-J(0)$ 的全部变化，都归因于期望项的变化。决策确实发生了转变（从 $x=0$ 到 $x=1$），因此这个逻辑正确地指出了变化的来源。\n\n结论：**正确**。",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}