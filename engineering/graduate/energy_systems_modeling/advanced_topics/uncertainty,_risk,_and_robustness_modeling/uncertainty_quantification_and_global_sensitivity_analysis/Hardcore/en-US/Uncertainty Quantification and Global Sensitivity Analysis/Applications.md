## Applications and Interdisciplinary Connections

Having established the fundamental principles and computational mechanisms of Uncertainty Quantification (UQ) and Global Sensitivity Analysis (GSA) in the preceding chapters, we now turn to their application. The true value of these methods is realized when they are applied to complex, real-world systems to generate actionable insights, support [robust decision-making](@entry_id:1131081), and advance scientific understanding. This chapter explores a curated set of applications across diverse fields, demonstrating how the core concepts of UQ and GSA are adapted and extended to address practical challenges. Our focus will be not on re-teaching the principles, but on illustrating their utility, power, and versatility in interdisciplinary contexts.

### The Foundation: Characterizing Uncertainty in Model Inputs

The fidelity of any UQ or GSA study is fundamentally constrained by the quality of its input uncertainty characterization. This initial step involves translating empirical knowledge, physical constraints, and expert judgment into formal probabilistic representations. The process is far from trivial and requires careful consideration of the phenomena being modeled.

In many energy and economic systems, for example, key uncertain variables like annual average fuel prices or electricity demand growth rates are not simple, symmetric random variables. Fuel prices are strictly positive, often exhibit right-skewness due to [market volatility](@entry_id:1127633) and price spikes, and may possess heavy tails, meaning extreme events are more probable than a Gaussian distribution would suggest. Similarly, load growth rates are physically constrained to be greater than $-1$ (to avoid non-physical negative demand) and often show a slight right-skew. A rigorous UQ workflow must select parametric distributions that honor these properties. For a positive, right-skewed variable like fuel price, lognormal or Gamma distributions are common starting points. To capture heavier tails, one might model the logarithm of the price with a Student-t distribution. For a variable like load growth, a transformation such as modeling $\ln(1+g)$ as Gaussian can elegantly enforce the support on $(-1, \infty)$ while introducing appropriate skew. These choices are critical, as they directly influence the variance of model outputs and the stability of sensitivity indices .

The challenge of input characterization is amplified when dealing with high-dimensional, correlated fields, such as those describing renewable energy generation. The power output from a fleet of wind and solar farms is governed by meteorological processes that are correlated in both space and time. A geographically widespread storm, for instance, affects wind speeds and cloud cover across numerous sites simultaneously. A robust uncertainty model must capture these dependencies. Spatio-temporal covariance functions, parameterized by correlation lengths and times, provide a framework for this. For instance, the variance of the total aggregated power from multiple wind farms is not simply the sum of individual variances; it is significantly increased by positive spatial correlation. Greater correlation reduces the "geographic smoothing" effect, where local, independent fluctuations would otherwise cancel out. Furthermore, since the marginal distributions of wind speed or solar [irradiance](@entry_id:176465) are often non-Gaussian (e.g., Weibull for wind speed), copula methods are an indispensable tool. A Gaussian [copula](@entry_id:269548), for example, can join arbitrary user-specified marginal distributions with a linear correlation structure, although care must be taken as it may fail to capture complex tail dependencies where extreme events are highly correlated. When inputs are correlated, classical Sobol' indices can be misleading, and advanced GSA methods like Shapley effects may be required to fairly attribute output variance .

Uncertainty characterization is also central to forecasting. Rather than producing a single point forecast for a quantity like short-term electricity demand, modern methods generate a full probabilistic forecast. Quantile regression is a powerful technique for this purpose, as it allows for the modeling of conditional [quantiles](@entry_id:178417) of the demand distribution as a function of predictors like temperature and time of day. By modeling multiple [quantiles](@entry_id:178417) (e.g., for $\tau = 0.05, 0.1, \dots, 0.95$), one can construct a complete predictive distribution. A key advantage of this approach is its ability to capture heteroscedasticity—the tendency for forecast uncertainty to vary with conditions, such as higher demand variability on very hot days. While Ordinary Least Squares (OLS) regression only models the conditional mean, a set of linear quantile regressions can represent a [conditional distribution](@entry_id:138367) whose spread changes with the predictors. A practical challenge in this method is "quantile crossing," where estimated quantile functions may cross due to [sampling error](@entry_id:182646), violating the [monotonicity](@entry_id:143760) of a valid [quantile function](@entry_id:271351). This requires post-processing or constrained estimation techniques to ensure the generation of valid probabilistic intervals .

### Propagating Uncertainty Through Diverse System Models

Once inputs are characterized, their uncertainty must be propagated through the system model. The nature of this propagation depends critically on the mathematical structure of the model itself.

In the idealized case of a linear model, [uncertainty propagation](@entry_id:146574) and sensitivity analysis become analytically tractable. Consider the DC power flow approximation used in transmission network analysis, where bus voltage angles and line power flows are linear functions of the power injections. If the uncertain injections are modeled as independent Gaussian random variables, the output line flow, being a linear combination of these inputs, will also be Gaussian. Its mean and variance can be calculated directly from the means and variances of the inputs and the model's linear coefficients. In this context, the variance-based Sobol' indices have a particularly clear interpretation: the first-order index for an input is simply the fraction of the total output variance contributed by that input's term in the linear model. For an additive model with independent inputs, the first-order indices sum to one, indicating a lack of interaction effects .

However, most real-world models are not linear. The full AC power flow equations, for instance, constitute a large system of nonlinear algebraic equations. Propagating uncertainty through such a model presents profound challenges. The input-to-output map can be highly non-smooth, and for a given set of uncertain inputs, the equations may admit multiple solutions or no solution at all. This latter issue defines a complex, often non-convex feasibility boundary in the input space. Furthermore, the model can exhibit discontinuous "regime switching," such as when a generator hits a reactive power limit and its governing equations change from a fixed-voltage (PV) to a fixed-power (PQ) type. These nonlinearities and discontinuities can transform simple, symmetric input distributions (like a Gaussian) into complex, skewed, or even multimodal output distributions. Such behavior poses a significant challenge for UQ methods like low-order Polynomial Chaos Expansions, which converge slowly for non-[smooth functions](@entry_id:138942). Consequently, GSA for such systems must be carefully conditioned on the set of feasible inputs to avoid conflating the intrinsic sensitivity of the model with the sensitivity of the feasibility boundary itself .

UQ and GSA are also applied to models that are not systems of continuous equations but are based on optimization and integer logic, such as the unit commitment (UC) problem in power systems. In a UC model, decisions about starting up or shutting down power plants are binary, and the total system cost is a function of these discrete decisions. The sensitivity of the total cost to a parameter, such as a generator's minimum up time $\tau$, cannot be assessed with derivatives in the usual sense. Instead, sensitivity manifests through changes in the optimal schedule. For example, increasing the minimum up time from $\tau$ to $\tau+1$ may force a generator that was previously shut down to remain online for an extra period, incurring an additional no-load cost. The sensitivity of the cost is therefore related to the set of scenarios where the minimum up time constraint is binding. GSA in this context involves tracking how the set of [active constraints](@entry_id:636830) changes across the space of uncertain inputs, providing a discrete, combinatorial view of sensitivity .

### Advanced Applications and Interdisciplinary Connections

The frameworks of UQ and GSA are highly adaptable and find application in specialized analyses and a wide range of scientific disciplines beyond energy systems.

#### Risk Analysis and Decision-Making

A primary goal of UQ is to inform risk-aware decision-making. Instead of focusing on the entire output distribution of a quantity like system cost, decision-makers are often most concerned with the tail of the distribution, which represents high-cost, high-risk outcomes. Financial risk measures are commonly adapted for this purpose. **Value-at-Risk at level $\alpha$** ($\mathrm{VaR}_\alpha$) is the $\alpha$-quantile of the cost distribution, answering the question: "What is the cost level that will only be exceeded with a probability of $1-\alpha$?" While simple, VaR does not describe the severity of losses beyond this point. A more informative measure is **Conditional Value-at-Risk at level $\alpha$** ($\mathrm{CVaR}_\alpha$), defined as the expected cost given that the cost has exceeded the $\mathrm{VaR}_\alpha$ threshold. This provides a measure of the expected loss in the worst-case scenarios. GSA can be extended to these risk measures, allowing analysts to determine which uncertain inputs are the primary drivers of [tail risk](@entry_id:141564), a question often more relevant than what drives the average cost .

#### Reliability and Rare Event Analysis

In many engineering systems, failure is a rare event. In [power system reliability](@entry_id:1130080), for example, a loss-of-load event may have a very small probability. Estimating such low probabilities with standard Monte Carlo simulation is computationally prohibitive, as an enormous number of samples would be required to observe a sufficient number of failure events. **Importance Sampling** is a variance reduction technique that addresses this by modifying the sampling process. Instead of sampling from the true probability distribution of the inputs, samples are drawn from a biased "proposal" distribution that deliberately generates more samples in the failure region. To maintain an unbiased estimate of the event probability, each sample is weighted by the [likelihood ratio](@entry_id:170863) of the true density to the proposal density. A well-chosen [proposal distribution](@entry_id:144814), for instance one created by "tilting" the original distribution's mean toward the failure boundary, can dramatically reduce the number of samples required to achieve a given level of accuracy in the probability estimate .

#### Surrogate Modeling for Efficient Analysis

The computational cost of running a high-fidelity simulator can make large-scale UQ and GSA infeasible. A common strategy is to first build a computationally cheap **surrogate model** (or metamodel) that accurately approximates the input-output relationship of the full simulator. GSA is then performed on this fast-running surrogate. **Polynomial Chaos Expansions (PCE)** are a powerful class of [surrogate models](@entry_id:145436) that represent the output as a spectral expansion in terms of orthogonal polynomials of the random inputs. For independent inputs, the total output variance decomposes into a simple [sum of squares](@entry_id:161049) of the expansion coefficients, making the calculation of Sobol' indices trivial once the PCE is constructed. This technique is widely used in fields like [environmental modeling](@entry_id:1124562) to analyze the sensitivity of complex models, such as those for evapotranspiration . Even simpler analytical surrogates can be effective. In computational combustion, a complex [radiative heat transfer](@entry_id:149271) calculation might be replaced by a surrogate based on physical approximations, such as the Weighted Sum of Gray Gases (WSGG) model. If this surrogate has a simple mathematical form (e.g., a product of functions of the inputs), Sobol' indices can often be derived analytically, providing rapid sensitivity insights without any sampling .

#### Robust Design in Engineering and Biology

A key application of GSA is in achieving **robust design**. A robust system is one whose performance is insensitive to variations in its underlying parameters. GSA provides a formal language for quantifying this property. Parameters with low total-order Sobol' indices ($S_{T_i}$) are those to which the output is insensitive, considering both their direct effects and their effects through interactions. A design can be considered robust with respect to a set of parameters if their corresponding $S_{T_i}$ values are all small. This concept is universal. In synthetic biology, for example, a [gene circuit](@entry_id:263036) is considered reliable if its protein output remains within a desired functional range despite biochemical fluctuations in parameters like promoter strength or degradation rates. Designing a circuit where the key parameters have low total sensitivity indices is a direct strategy for achieving high biological reliability, as it ensures that the output variance will be small, increasing the probability that the output remains within its functional bounds .

#### Hierarchical Variance Decomposition

The core idea of GSA—decomposing variance into contributions from different sources—is a powerful concept that appears in many statistical contexts. One such parallel is in modeling complex hierarchical systems, such as those found in advanced manufacturing. In semiconductor manufacturing, the overlay error in lithography arises from multiple nested sources of variability: lot-to-lot, wafer-to-wafer, die-to-die, and feature-to-feature. A hierarchical statistical model can be constructed to explicitly represent each of these levels, including both random offsets and spatially varying functional components (e.g., a wafer-level distortion field modeled with a Gaussian Process). By fitting such a model, one can apply the law of total variance to decompose the total observed variability into the contributions from each level of the hierarchy. This "[variance components analysis](@entry_id:911841)" is conceptually analogous to GSA and allows engineers to identify and target the dominant sources of error in the manufacturing process .

### Establishing Credibility: A Broader Perspective

The utility of any UQ and GSA study depends on its credibility. This is not merely a statistical property but a broader requirement built upon rigorous practice in model development, verification, validation, and communication.

#### Verification, Validation, and Numerical Uncertainty

For any computational model, it is essential to distinguish between **verification** and **validation (V)**. Verification is the process of determining that the computational model correctly solves the mathematical equations it is intended to solve. It answers the question, "Are we solving the equations right?" This involves activities like code debugging and, critically, estimating the numerical error arising from discretization and iterative solvers. Validation, in contrast, is the process of determining the degree to which the mathematical model is an accurate representation of reality for its intended purpose. It answers the question, "Are we solving the right equations?" This involves comparing model predictions to real-world observations to quantify model discrepancy.

A credible UQ framework must account for all major sources of uncertainty: (1) uncertainty in model inputs, (2) uncertainty in model parameters, (3) numerical error from the simulation code (quantified via verification), and (4) [model-form error](@entry_id:274198) or discrepancy (quantified via validation). For instance, using [grid refinement](@entry_id:750066) studies and Richardson [extrapolation](@entry_id:175955), one can estimate the magnitude of numerical error in a simulation output. This numerical uncertainty can then be treated as an additional random variable in the total [uncertainty budget](@entry_id:151314), allowing its contribution to the total output variance to be quantified alongside the contributions from [parametric uncertainty](@entry_id:264387). Ignoring either verification or validation leads to an incomplete uncertainty budget and produces predictive intervals that are biased or overconfident  . The ASME V 40 standard provides a risk-informed framework for determining the level of rigor required for V activities based on the consequence of the decision the model will inform and the model's influence on that decision. For high-risk decisions, such as using a model in lieu of a clinical trial, the highest level of credibility is required, demanding comprehensive evidence from V, UQ, and process governance .

#### Communicating Results to Decision-Makers

Finally, the impact of a UQ/GSA study hinges on its effective communication to stakeholders, who are often non-experts in statistics. The goal is to convey complex results in a manner that is understandable, transparent, and actionable. Best practices for communication move far beyond reporting single [point estimates](@entry_id:753543). Presenting summaries of the predictive distribution—such as medians, interquartile ranges, and 5-95% [prediction intervals](@entry_id:635786)—gives decision-makers a tangible sense of the uncertainty and risk. Tail-risk metrics like CVaR are essential for discussions involving budgets or safety margins.

Visualizations are particularly powerful. Overlaying the empirical cumulative distribution functions (CDFs) of a key metric (like cost) for different proposed portfolios allows for a direct comparison of risk profiles across the full range of possible outcomes. For communicating sensitivity analysis, it is crucial to use global measures like Sobol' indices that capture nonlinearities and interactions, rather than misleading local measures like one-at-a-time tornado charts. Most importantly, the GSA results should be translated into a narrative that links high-importance inputs to potential actions, such as targeted R or data collection efforts that could reduce the most critical uncertainties and lead to a more confident decision .