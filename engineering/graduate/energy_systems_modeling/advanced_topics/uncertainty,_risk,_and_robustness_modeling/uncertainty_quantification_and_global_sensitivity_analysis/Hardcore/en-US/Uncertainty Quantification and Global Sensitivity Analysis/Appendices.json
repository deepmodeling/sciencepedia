{
    "hands_on_practices": [
        {
            "introduction": "A core skill in global sensitivity analysis is the ability to compute sensitivity indices directly from their variance-based definitions. This exercise provides a foundational, hands-on calculation of Sobol' indices for a simple but illustrative model, where the solution is analytically tractable. By working through this derivation, you will gain a concrete understanding of how total output variance is partitioned among uncertain inputs and see how a model's structure is reflected in the sensitivity indices .",
            "id": "4133329",
            "problem": "In a stylized uncertainty quantification study of an energy systems surrogate, consider the scalar output model $Y = X_1^2 + X_2$, where $X_1$ and $X_2$ are independent uncertain inputs. Suppose $X_1 \\sim \\mathcal{U}(-1, 2)$ and $X_2 \\sim \\mathcal{U}(0, 3)$, modeling epistemic ranges for two independent drivers in an energy systems model. Using the variance-based definitions of Sobol' sensitivity indices grounded in the law of total variance and conditional expectation, derive analytically the first-order Sobol' indices $S_1$ and $S_2$ and the total-effect Sobol' index $S_{T1}$. Then interpret whether any second-order interaction between $X_1$ and $X_2$ is present and explain how this is reflected in the indices.\n\nExpress $S_1$, $S_2$, and $S_{T1}$ as exact fractions with no rounding, and report your final answer in the order $S_1$, $S_2$, $S_{T1}$.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of probability theory and global sensitivity analysis, is well-posed with all necessary information provided, and is stated objectively.\n\nThe model for the scalar output is given by $Y = X_1^2 + X_2$. The inputs $X_1$ and $X_2$ are independent random variables with specified uniform distributions: $X_1 \\sim \\mathcal{U}(-1, 2)$ and $X_2 \\sim \\mathcal{U}(0, 3)$.\n\nThe first-order Sobol' index, $S_i$, is defined as the fraction of variance in the output $Y$ that is attributable to the input $X_i$ alone:\n$$S_i = \\frac{V_i}{V(Y)} = \\frac{V(E(Y|X_i))}{V(Y)}$$\nThe total-effect Sobol' index, $S_{Ti}$, is the fraction of variance in $Y$ attributable to $X_i$, including its main effect and all of its interactions with other inputs:\n$$S_{Ti} = \\frac{E(V(Y|X_{\\sim i}))}{V(Y)}$$\nwhere $X_{\\sim i}$ denotes the set of all input variables except $X_i$. For a two-variable model, the variance decomposition is $V(Y) = V_1 + V_2 + V_{12}$, which implies $1 = S_1 + S_2 + S_{12}$. The total-effect indices are $S_{T1} = S_1 + S_{12}$ and $S_{T2} = S_2 + S_{12}$.\n\nFirst, we compute the necessary moments and variances of the input variables. For a general uniform distribution $\\mathcal{U}(a, b)$, the mean is $E(X) = \\frac{a+b}{2}$ and the variance is $V(X) = \\frac{(b-a)^2}{12}$.\n\nFor $X_1 \\sim \\mathcal{U}(-1, 2)$:\nThe probability density function is $p_1(x_1) = \\frac{1}{2 - (-1)} = \\frac{1}{3}$ for $x_1 \\in [-1, 2]$.\nThe mean is $E(X_1) = \\frac{-1+2}{2} = \\frac{1}{2}$.\nThe variance is $V(X_1) = \\frac{(2-(-1))^2}{12} = \\frac{3^2}{12} = \\frac{9}{12} = \\frac{3}{4}$.\nWe require moments of $X_1^2$.\n$E(X_1^2) = \\int_{-1}^{2} x_1^2 p_1(x_1) dx_1 = \\int_{-1}^{2} x_1^2 \\frac{1}{3} dx_1 = \\frac{1}{3} \\left[\\frac{x_1^3}{3}\\right]_{-1}^{2} = \\frac{1}{9} (2^3 - (-1)^3) = \\frac{1}{9}(8+1) = 1$.\n$E(X_1^4) = \\int_{-1}^{2} x_1^4 \\frac{1}{3} dx_1 = \\frac{1}{3} \\left[\\frac{x_1^5}{5}\\right]_{-1}^{2} = \\frac{1}{15} (2^5 - (-1)^5) = \\frac{1}{15}(32+1) = \\frac{33}{15} = \\frac{11}{5}$.\nThe variance of $X_1^2$ is $V(X_1^2) = E(X_1^4) - (E(X_1^2))^2 = \\frac{11}{5} - 1^2 = \\frac{6}{5}$.\n\nFor $X_2 \\sim \\mathcal{U}(0, 3)$:\nThe probability density function is $p_2(x_2) = \\frac{1}{3-0} = \\frac{1}{3}$ for $x_2 \\in [0, 3]$.\nThe mean is $E(X_2) = \\frac{0+3}{2} = \\frac{3}{2}$.\nThe variance is $V(X_2) = \\frac{(3-0)^2}{12} = \\frac{9}{12} = \\frac{3}{4}$.\n\nNext, we calculate the total variance of the output, $V(Y)$. Since $X_1$ and $X_2$ are independent, $X_1^2$ and $X_2$ are also independent. Thus, the variance of their sum is the sum of their variances:\n$V(Y) = V(X_1^2 + X_2) = V(X_1^2) + V(X_2) = \\frac{6}{5} + \\frac{3}{4} = \\frac{24 + 15}{20} = \\frac{39}{20}$.\n\nNow we compute the terms needed for the first-order indices.\nFor $S_1$, we need $V_1 = V(E(Y|X_1))$.\n$E(Y|X_1) = E(X_1^2 + X_2 | X_1) = E(X_1^2|X_1) + E(X_2|X_1)$. Due to independence, $E(X_2|X_1) = E(X_2)$.\n$E(Y|X_1) = X_1^2 + E(X_2) = X_1^2 + \\frac{3}{2}$.\n$V_1 = V(X_1^2 + \\frac{3}{2}) = V(X_1^2) = \\frac{6}{5}$.\nSo, $S_1 = \\frac{V_1}{V(Y)} = \\frac{6/5}{39/20} = \\frac{6}{5} \\times \\frac{20}{39} = \\frac{120}{195} = \\frac{24}{39} = \\frac{8}{13}$.\n\nFor $S_2$, we need $V_2 = V(E(Y|X_2))$.\n$E(Y|X_2) = E(X_1^2 + X_2 | X_2) = E(X_1^2|X_2) + E(X_2|X_2)$. Due to independence, $E(X_1^2|X_2) = E(X_1^2)$.\n$E(Y|X_2) = E(X_1^2) + X_2 = 1 + X_2$.\n$V_2 = V(1 + X_2) = V(X_2) = \\frac{3}{4}$.\nSo, $S_2 = \\frac{V_2}{V(Y)} = \\frac{3/4}{39/20} = \\frac{3}{4} \\times \\frac{20}{39} = \\frac{60}{156} = \\frac{15}{39} = \\frac{5}{13}$.\n\nTo interpret the interaction, we sum the first-order indices: $S_1 + S_2 = \\frac{8}{13} + \\frac{5}{13} = \\frac{13}{13} = 1$.\nThe relation $S_1 + S_2 + S_{12} = 1$ implies that the second-order interaction index $S_{12} = 0$. This result is expected because the model $Y = X_1^2 + X_2$ is additive, meaning it is a sum of functions of individual inputs, $Y = f_1(X_1) + f_2(X_2)$. Additive models have no interactions by definition, so the variance of the output is fully explained by the first-order effects.\n\nFinally, we compute the total-effect index $S_{T1}$. By definition, $S_{T1} = S_1 + S_{12}$.\nSince $S_{12} = 0$, it follows that $S_{T1} = S_1$.\nTherefore, $S_{T1} = \\frac{8}{13}$.\nThis can be confirmed using the alternative definition. $S_{T1} = E(V(Y|X_2)) / V(Y)$.\n$V(Y|X_2) = V(X_1^2+X_2 | X_2) = V(X_1^2) = \\frac{6}{5}$. Since this is a constant, its expectation is $E(V(Y|X_2)) = \\frac{6}{5}$.\nThen $S_{T1} = \\frac{6/5}{39/20} = \\frac{8}{13}$, confirming the result.\n\nThe requested indices are $S_1 = \\frac{8}{13}$, $S_2 = \\frac{5}{13}$, and $S_{T1} = \\frac{8}{13}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{8}{13} & \\frac{5}{13} & \\frac{8}{13}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "For complex \"black-box\" models where analytical derivations are impossible, surrogate modeling techniques like Polynomial Chaos Expansions (PCE) are indispensable. This practice delves into the theoretical heart of PCE, revealing how its use of orthogonal polynomials allows for an elegant decomposition of the model's variance into a simple sum of coefficients. This exercise demonstrates the mathematical principle that makes PCE a computationally efficient cornerstone of modern Uncertainty Quantification .",
            "id": "4133297",
            "problem": "Consider a simplified energy systems modeling setting where a scalar system output $Y = g(X)$ represents the normalized deviation of net supply from its day-ahead forecast due to an uncertain factor $X$, with $X \\sim \\mathcal{N}(0,1)$. In uncertainty quantification, a Polynomial Chaos Expansion (PCE) is used to represent $Y$ as a series expansion in orthogonal polynomials with respect to the probability measure of the inputs. For Gaussian inputs, the appropriate orthogonal polynomials are the probabilists' Hermite polynomials. Starting from first principles of orthogonality under the Gaussian measure, construct the Hermite PCE of $Y$ truncated to order $3$ using an orthonormal basis of Hermite polynomials with respect to the measure of $X$. Explicitly write the truncated expansion in terms of the orthonormal basis functions and the projection coefficients. Then, using orthogonality and the definition of variance as the second central moment, express the variance of the truncated PCE approximation of $Y$ entirely as a sum of squared coefficients of the orthonormal basis functions. Your final answer must be a single closed-form analytic expression involving only the truncated projection coefficients and must not include any integrals or expectations. No numerical rounding is required and no physical units are involved.",
            "solution": "The appropriate orthogonal polynomial family for inputs with a standard Gaussian distribution $X \\sim \\mathcal{N}(0,1)$ is the family of probabilists' Hermite polynomials. Define the probabilists' Hermite polynomials $\\{H_{n}(x)\\}_{n \\geq 0}$ by the generating function or equivalently by the recurrence with initial polynomials\n$$\nH_{0}(x) = 1,\\quad H_{1}(x) = x,\\quad H_{2}(x) = x^{2} - 1,\\quad H_{3}(x) = x^{3} - 3x.\n$$\nThese polynomials satisfy the orthogonality relation with respect to the Gaussian weight function $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$:\n$$\n\\int_{-\\infty}^{\\infty} H_{m}(x)\\, H_{n}(x)\\, \\phi(x)\\, dx = \\mathbb{E}\\!\\left[H_{m}(X) H_{n}(X)\\right] = n!\\, \\delta_{mn},\n$$\nwhere $\\delta_{mn}$ is the Kronecker delta.\n\nTo form an orthonormal basis with respect to the measure of $X$, define\n$$\n\\psi_{n}(x) = \\frac{H_{n}(x)}{\\sqrt{n!}},\\quad n \\geq 0.\n$$\nBy construction,\n$$\n\\mathbb{E}\\!\\left[\\psi_{m}(X)\\, \\psi_{n}(X)\\right] = \\delta_{mn}.\n$$\n\nA truncated Hermite Polynomial Chaos Expansion (PCE) of $Y = g(X)$ up to order $3$ in the orthonormal basis $\\{\\psi_{n}\\}_{n=0}^{3}$ is\n$$\nY \\approx \\sum_{n=0}^{3} c_{n}\\, \\psi_{n}(X) = c_{0}\\, \\psi_{0}(X) + c_{1}\\, \\psi_{1}(X) + c_{2}\\, \\psi_{2}(X) + c_{3}\\, \\psi_{3}(X),\n$$\nwhere the coefficients $c_{n}$ are the $L^{2}$ projections of $Y$ onto the orthonormal basis:\n$$\nc_{n} = \\mathbb{E}\\!\\left[Y\\, \\psi_{n}(X)\\right] = \\int_{-\\infty}^{\\infty} g(x)\\, \\psi_{n}(x)\\, \\phi(x)\\, dx,\\quad n=0,1,2,3.\n$$\nIn terms of explicit functions,\n$$\n\\psi_{0}(x) = 1,\\quad \\psi_{1}(x) = x,\\quad \\psi_{2}(x) = \\frac{x^{2}-1}{\\sqrt{2}},\\quad \\psi_{3}(x) = \\frac{x^{3}-3x}{\\sqrt{6}}.\n$$\n\nWe now derive the variance of the truncated expansion using orthonormality. By definition, the mean of the truncated expansion is\n$$\n\\mathbb{E}\\!\\left[\\sum_{n=0}^{3} c_{n}\\, \\psi_{n}(X)\\right] = \\sum_{n=0}^{3} c_{n}\\, \\mathbb{E}\\!\\left[\\psi_{n}(X)\\right] = c_{0}\\, \\mathbb{E}\\!\\left[\\psi_{0}(X)\\right] + \\sum_{n=1}^{3} c_{n}\\, \\mathbb{E}\\!\\left[\\psi_{n}(X)\\right].\n$$\nBecause $\\psi_{0}(x) = 1$, we have $\\mathbb{E}\\!\\left[\\psi_{0}(X)\\right] = 1$, and because the higher-order orthonormal basis functions have zero mean under the Gaussian measure,\n$$\n\\mathbb{E}\\!\\left[\\psi_{n}(X)\\right] = 0\\quad \\text{for}\\quad n \\geq 1.\n$$\nThus the mean of the truncated expansion is\n$$\n\\mu \\equiv \\mathbb{E}\\!\\left[Y\\right] \\approx c_{0}.\n$$\n\nThe variance of the truncated expansion is defined by\n$$\n\\operatorname{Var}\\!\\left[Y\\right] \\approx \\mathbb{E}\\!\\left[\\left(\\sum_{n=0}^{3} c_{n}\\, \\psi_{n}(X) - c_{0}\\right)^{2}\\right].\n$$\nSince $c_{0} = \\mathbb{E}\\!\\left[Y\\right]$ and $\\psi_{0}(X) = 1$, subtracting the mean removes the $n=0$ term, giving\n$$\n\\operatorname{Var}\\!\\left[Y\\right] \\approx \\mathbb{E}\\!\\left[\\left(\\sum_{n=1}^{3} c_{n}\\, \\psi_{n}(X)\\right)^{2}\\right] = \\mathbb{E}\\!\\left[\\sum_{n=1}^{3} \\sum_{m=1}^{3} c_{n}\\, c_{m}\\, \\psi_{n}(X)\\, \\psi_{m}(X)\\right].\n$$\nUsing linearity of expectation and orthonormality,\n$$\n\\operatorname{Var}\\!\\left[Y\\right] \\approx \\sum_{n=1}^{3} \\sum_{m=1}^{3} c_{n}\\, c_{m}\\, \\mathbb{E}\\!\\left[\\psi_{n}(X)\\, \\psi_{m}(X)\\right] = \\sum_{n=1}^{3} \\sum_{m=1}^{3} c_{n}\\, c_{m}\\, \\delta_{nm} = \\sum_{n=1}^{3} c_{n}^{2}.\n$$\n\nTherefore, when using the orthonormal Hermite basis up to order $3$, the variance of the truncated Hermite PCE approximation of $Y$ is exactly the sum of the squared coefficients of the non-constant basis functions:\n$$\n\\operatorname{Var}\\!\\left[Y\\right] \\approx c_{1}^{2} + c_{2}^{2} + c_{3}^{2}.\n$$\nThis expression requires no further assumptions beyond orthonormality of the basis with respect to the distribution of $X$ and the truncation at order $3$.",
            "answer": "$$\\boxed{c_{1}^{2} + c_{2}^{2} + c_{3}^{2}}$$"
        },
        {
            "introduction": "This final practice integrates theory and computation into a complete Global Sensitivity Analysis (GSA) workflow, mirroring the process used in research. You will implement the widely-used Saltelli sampling method to estimate Sobol' indices for a benchmark nonlinear function and validate your numerical results against the true analytical values. This capstone exercise builds practical skills in constructing and verifying the computational tools essential for analyzing complex systems under uncertainty .",
            "id": "4133362",
            "problem": "Consider the benchmark Ishigami function, which is widely used for uncertainty quantification and global sensitivity analysis to stress-test nonlinear response surfaces. Let the uncertain inputs be $x_1$, $x_2$, and $x_3$, each independently and identically distributed as uniform random variables on $[-\\pi,\\pi]$. Define the model output $f$ by\n$$\nf(x_1,x_2,x_3;a,b) \\equiv \\sin(x_1) + a \\sin^2(x_2) + b\\, x_3^4 \\sin(x_1),\n$$\nwhere $a$ and $b$ are real-valued parameters to be calibrated. In an energy systems modeling context, interpret $x_1$ as a dispatchable flexibility proxy, $x_2$ as variable renewable resource variability, and $x_3$ as policy-driven structural change; the model output $f$ is a dimensionless reliability surrogate. Your tasks are:\n\n1. Starting from the fundamental definitions of expectation and variance, and the variance-based sensitivity decomposition underlying Sobol indices, derive a calibration rule to set parameters $a$ and $b$ so that:\n   - The unconditional mean satisfies $\\mathbb{E}[f] = m_{\\text{target}}$ for a given target $m_{\\text{target}}$ (dimensionless).\n   - The ratio of first-order sensitivities satisfies $R_{\\text{target}} = S_2 / S_1$, where $S_i$ denotes the first-order Sobol index for input $x_i$.\n\n   The calibration must be expressed in closed form using only $m_{\\text{target}}$ and $R_{\\text{target}}$ and constants derived from the input distributions on $[-\\pi,\\pi]$.\n\n2. Implement a Monte Carlo Global Sensitivity Analysis (GSA) pipeline using the Saltelli sampling scheme (first-order estimator) and the Jansen estimator (total-order estimator) to estimate the first-order indices $S_1$, $S_2$, $S_3$ and total-effect indices $T_1$, $T_2$, $T_3$. The pipeline must:\n   - Use independent base samples of size $N$ for matrices $A$ and $B$ in dimension $d=3$, with all coordinates sampled uniformly from $[-\\pi,\\pi]$.\n   - Construct mixed matrices $A_{B}^{(i)}$ by replacing the $i$-th column of $A$ with the $i$-th column of $B$ for $i \\in \\{1,2,3\\}$.\n   - Estimate first-order indices using the Saltelli estimator and total-effect indices using the Jansen estimator.\n   - Use an absolute error tolerance $\\epsilon = 3 \\times 10^{-2}$ to validate the pipeline against analytically derived indices: the pipeline is deemed valid for a test case if the absolute error between estimated and analytical indices is at most $\\epsilon$ for all three first-order indices and all three total-effect indices.\n\n3. For numerical reproducibility, fix the random seeds specified in the test suite below.\n\nAll outputs are dimensionless; no physical units are required. All angles are in radians. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each test case must be a list containing, in this exact order, the calibrated parameters and the estimated indices and validation flags: $[a,b,S_1,S_2,S_3,T_1,T_2,T_3,\\text{pass\\_first\\_order},\\text{pass\\_total}]$. Each entry must be a boolean, an integer, a float, or a list of these types.\n\nUse the following test suite, which exercises a standard case, a moderate case, a high $R_{\\text{target}}$ case, and a small-sample stress case:\n\n- Test case $1$: $m_{\\text{target}} = 3.5$, $R_{\\text{target}} = 1.409$, $N = 30000$, $\\text{seed} = 42$.\n- Test case $2$: $m_{\\text{target}} = 2.0$, $R_{\\text{target}} = 0.5$, $N = 15000$, $\\text{seed} = 123$.\n- Test case $3$: $m_{\\text{target}} = 1.0$, $R_{\\text{target}} = 4.0$, $N = 15000$, $\\text{seed} = 456$.\n- Test case $4$: $m_{\\text{target}} = 2.0$, $R_{\\text{target}} = 0.01$, $N = 4000$, $\\text{seed} = 789$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$.",
            "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded, well-posed, and contains all necessary information to derive a unique, verifiable solution.\n\nThe problem requires a three-part solution: first, the derivation of a calibration rule for the parameters of the Ishigami function; second, the derivation of the analytical Sobol indices for validation; and third, the implementation of a Monte Carlo Global Sensitivity Analysis (GSA) pipeline to estimate these indices numerically.\n\n### 1. Analytical Derivation of the Calibration Rule\n\nThe Ishigami function is given by:\n$$\nf(x_1,x_2,x_3;a,b) = \\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1)\n$$\nThe inputs $x_1, x_2, x_3$ are independent and identically distributed random variables from a uniform distribution on $[-\\pi, \\pi]$, i.e., $x_i \\sim U[-\\pi, \\pi]$. The probability density function is $p(x) = \\frac{1}{2\\pi}$ for $x \\in [-\\pi, \\pi]$.\n\nWe first compute the expectations of the elementary functions involved, which will be used throughout the derivation:\n$$\n\\mathbb{E}[\\sin(x_i)] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\sin(x) dx = 0\n$$\n$$\n\\mathbb{E}[\\sin^2(x_i)] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\sin^2(x) dx = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\frac{1 - \\cos(2x)}{2} dx = \\frac{1}{2}\n$$\n$$\n\\mathbb{E}[x_3^4] = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} x^4 dx = \\frac{1}{2\\pi} \\left[ \\frac{x^5}{5} \\right]_{-\\pi}^{\\pi} = \\frac{1}{10\\pi}(\\pi^5 - (-\\pi)^5) = \\frac{\\pi^4}{5}\n$$\n\n**1.1. Unconditional Mean Condition**\n\nThe first calibration condition is $\\mathbb{E}[f] = m_{\\text{target}}$. We compute the unconditional mean of $f$ by applying the linearity of expectation:\n$$\n\\mathbb{E}[f] = \\mathbb{E}[\\sin(x_1)] + a\\mathbb{E}[\\sin^2(x_2)] + b\\mathbb{E}[x_3^4 \\sin(x_1)]\n$$\nDue to the independence of the inputs, the expectation of the product term is the product of expectations:\n$$\n\\mathbb{E}[x_3^4 \\sin(x_1)] = \\mathbb{E}[x_3^4] \\mathbb{E}[\\sin(x_1)] = \\left(\\frac{\\pi^4}{5}\\right)(0) = 0\n$$\nSubstituting the known expectations, we get:\n$$\n\\mathbb{E}[f] = 0 + a\\left(\\frac{1}{2}\\right) + 0 = \\frac{a}{2}\n$$\nEquating this to the target mean gives the first part of our rule:\n$$\n\\frac{a}{2} = m_{\\text{target}} \\implies a = 2 m_{\\text{target}}\n$$\n\n**1.2. Sensitivity Ratio Condition**\n\nThe second condition is $R_{\\text{target}} = S_2 / S_1$, where $S_i$ is the first-order Sobol index for input $x_i$. The index $S_i$ is defined as $S_i = D_i/D$, where $D = \\text{Var}(f)$ is the total variance and $D_i = \\text{Var}(\\mathbb{E}[f|x_i])$ is the first-order partial variance of $x_i$. The ratio simplifies to $R_{\\text{target}} = D_2/D_1$.\n\nWe must compute the conditional expectations $\\mathbb{E}[f|x_i]$:\nFor $x_1$:\n$$\n\\mathbb{E}[f|x_1] = \\mathbb{E}[\\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1) | x_1] = \\sin(x_1) + a\\mathbb{E}[\\sin^2(x_2)] + b\\mathbb{E}[x_3^4]\\sin(x_1)\n$$\n$$\n\\mathbb{E}[f|x_1] = \\sin(x_1) + \\frac{a}{2} + b\\frac{\\pi^4}{5}\\sin(x_1) = \\left(1 + b\\frac{\\pi^4}{5}\\right)\\sin(x_1) + \\frac{a}{2}\n$$\nThe partial variance $D_1$ is the variance of this expression:\n$$\nD_1 = \\text{Var}(\\mathbb{E}[f|x_1]) = \\text{Var}\\left(\\left(1 + b\\frac{\\pi^4}{5}\\right)\\sin(x_1) + \\frac{a}{2}\\right) = \\left(1 + b\\frac{\\pi^4}{5}\\right)^2 \\text{Var}(\\sin(x_1))\n$$\nSince $\\mathbb{E}[\\sin(x_1)] = 0$, $\\text{Var}(\\sin(x_1)) = \\mathbb{E}[\\sin^2(x_1)] - (\\mathbb{E}[\\sin(x_1)])^2 = 1/2$.\n$$\nD_1 = \\frac{1}{2}\\left(1 + b\\frac{\\pi^4}{5}\\right)^2\n$$\n\nFor $x_2$:\n$$\n\\mathbb{E}[f|x_2] = \\mathbb{E}[\\sin(x_1) + a \\sin^2(x_2) + b x_3^4 \\sin(x_1) | x_2] = \\mathbb{E}[\\sin(x_1)] + a\\sin^2(x_2) + b\\mathbb{E}[x_3^4]\\mathbb{E}[\\sin(x_1)]\n$$\n$$\n\\mathbb{E}[f|x_2] = 0 + a\\sin^2(x_2) + 0 = a\\sin^2(x_2)\n$$\nThe partial variance $D_2$ is:\n$$\nD_2 = \\text{Var}(a\\sin^2(x_2)) = a^2 \\text{Var}(\\sin^2(x_2)) = a^2 (\\mathbb{E}[\\sin^4(x_2)] - (\\mathbb{E}[\\sin^2(x_2)])^2)\n$$\nWe need $\\mathbb{E}[\\sin^4(x_2)] = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi}\\sin^4(x)dx = \\frac{3}{8}$.\n$$\nD_2 = a^2 \\left(\\frac{3}{8} - \\left(\\frac{1}{2}\\right)^2\\right) = a^2 \\left(\\frac{3}{8} - \\frac{1}{4}\\right) = \\frac{a^2}{8}\n$$\n\nNow we can form the ratio $D_2/D_1$:\n$$\nR_{\\text{target}} = \\frac{D_2}{D_1} = \\frac{a^2/8}{\\frac{1}{2}(1 + b\\frac{\\pi^4}{5})^2} = \\frac{a^2}{4(1 + b\\frac{\\pi^4}{5})^2}\n$$\nSubstituting $a = 2 m_{\\text{target}}$:\n$$\nR_{\\text{target}} = \\frac{(2 m_{\\text{target}})^2}{4(1 + b\\frac{\\pi^4}{5})^2} = \\frac{m_{\\text{target}}^2}{(1 + b\\frac{\\pi^4}{5})^2}\n$$\nSolving for $b$, and assuming the positive root for a conventional solution (as the test cases with positive $m_{\\text{target}}$ imply):\n$$\n1 + b\\frac{\\pi^4}{5} = \\frac{m_{\\text{target}}}{\\sqrt{R_{\\text{target}}}} \\implies b = \\frac{5}{\\pi^4}\\left(\\frac{m_{\\text{target}}}{\\sqrt{R_{\\text{target}}}} - 1\\right)\n$$\nThis completes the calibration rule.\n\n### 2. Analytical Sobol Indices for Validation\n\nTo validate the numerical pipeline, we must derive the analytical expressions for all first-order and total-effect indices. This requires computing the total variance $D$. We use the ANOVA decomposition $D = \\sum_i D_i + \\sum_{i<j} D_{ij} + \\dots$.\n\nPartial variances already computed:\n$D_1 = \\frac{1}{2}\\left(1 + b\\frac{\\pi^4}{5}\\right)^2$\n$D_2 = \\frac{a^2}{8}$\n$D_3 = \\text{Var}(\\mathbb{E}[f|x_3]) = \\text{Var}(\\frac{a}{2}) = 0$.\n\nWe check for interaction terms. The only one is between $x_1$ and $x_3$.\n$D_{13} = \\text{Var}(\\mathbb{E}[f|x_1, x_3] - \\mathbb{E}[f|x_1] - \\mathbb{E}[f|x_3] + \\mathbb{E}[f])$.\nThe term is $f_{13} = b(x_3^4 - \\mathbb{E}[x_3^4])\\sin(x_1) = b(x_3^4 - \\frac{\\pi^4}{5})\\sin(x_1)$.\n$D_{13} = \\text{Var}(f_{13}) = b^2 \\text{Var}((x_3^4 - \\frac{\\pi^4}{5})\\sin(x_1))$. Since $x_1$ and $x_3$ are independent, and $\\mathbb{E}[\\sin(x_1)]=0$, this variance separates:\n$D_{13} = b^2 \\mathbb{E}[(x_3^4 - \\frac{\\pi^4}{5})^2] \\mathbb{E}[\\sin^2(x_1)] = b^2 \\text{Var}(x_3^4) \\mathbb{E}[\\sin^2(x_1)]$.\n$\\text{Var}(x_3^4) = \\mathbb{E}[x_3^8] - (\\mathbb{E}[x_3^4])^2 = \\frac{\\pi^8}{9} - (\\frac{\\pi^4}{5})^2 = \\pi^8(\\frac{1}{9} - \\frac{1}{25}) = \\frac{16\\pi^8}{225}$.\n$D_{13} = b^2 \\left(\\frac{16\\pi^8}{225}\\right) \\left(\\frac{1}{2}\\right) = \\frac{8b^2\\pi^8}{225}$.\nAll other interaction terms ($D_{12}, D_{23}, D_{123}$) are zero.\n\nThe total variance is $D = D_1 + D_2 + D_3 + D_{13} = \\frac{1}{2}\\left(1 + b\\frac{\\pi^4}{5}\\right)^2 + \\frac{a^2}{8} + \\frac{8b^2\\pi^8}{225}$.\n\nThe first-order indices are:\n$S_1 = D_1/D$, $S_2 = D_2/D$, $S_3 = 0$.\n\nFor total-effect indices $T_i = D_i^T/D$, the numerators $D_i^T$ sum all partial variances involving input $x_i$:\n$D_1^T = D_1 + D_{13} = \\frac{1}{2}\\left(1 + b\\frac{\\pi^4}{5}\\right)^2 + \\frac{8b^2\\pi^8}{225}$.\n$D_2^T = D_2 = \\frac{a^2}{8}$.\n$D_3^T = D_3 + D_{13} = D_{13} = \\frac{8b^2\\pi^8}{225}$.\n\nThe total-effect indices are:\n$T_1 = (D_1+D_{13})/D$, $T_2 = D_2/D = S_2$, $T_3 = D_{13}/D$.\n\nThese formulas allow for the computation of the true indices for any given $a$ and $b$.\n\n### 3. Numerical Estimation via Monte Carlo GSA\n\nThe GSA pipeline is implemented using the Saltelli sampling scheme with $N(d+2)$ total model evaluations, where $d=3$ is the number of inputs.\n\n**3.1. Sampling**\n1.  Generate two independent sample matrices, $A$ and $B$, of size $N \\times 3$, drawing all values from $U[-\\pi, \\pi]$.\n2.  For each input $i \\in \\{1, 2, 3\\}$, construct a \"mixed\" matrix $A_B^{(i)}$ by taking matrix $A$ and replacing its $i$-th column with the $i$-th column of matrix $B$.\n\n**3.2. Model Evaluation**\nThe Ishigami function $f$ is evaluated for each row of the matrices $A$, $B$, and $A_B^{(i)}$ for $i=1,2,3$, yielding five output vectors of length $N$: $y_A, y_B, y_{A_B^{(1)}}, y_{A_B^{(2)}}, y_{A_B^{(3)}}$.\n\n**3.3. Index Estimation**\nThe estimators for the partial and total variances are:\n- The total variance $D$ is estimated from the sample variance of one of the base samples:\n  $$ \\hat{D} = \\text{Var}(y_A) $$\n- The first-order variance $D_i$ is estimated using the Saltelli estimator:\n  $$ \\hat{D}_i = \\frac{1}{N} \\sum_{j=1}^{N} y_B^{(j)} (y_{A_B^{(i)}}^{(j)} - y_A^{(j)}) $$\n- The total-effect variance $D_i^T$ is estimated using the Jansen estimator:\n  $$ \\hat{D}_i^T = \\frac{1}{2N} \\sum_{j=1}^{N} (y_A^{(j)} - y_{A_B^{(i)}}^{(j)})^2 $$\n\nThe estimated Sobol indices are then calculated as $\\hat{S}_i = \\hat{D}_i / \\hat{D}$ and $\\hat{T}_i = \\hat{D}_i^T / \\hat{D}$.\n\n**3.4. Validation**\nFor each test case, the estimated indices $(\\hat{S}_i, \\hat{T}_i)$ are compared to the analytical indices $(S_i, T_i)$ derived in section 2. The pipeline is validated separately for first-order and total-effect indices. The validation passes if the absolute error is less than or equal to the tolerance $\\epsilon = 3 \\times 10^{-2}$ for all three indices in the respective group.\n\nThe final implementation will encapsulate these steps, process the provided test suite, and format the output as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the full GSA workflow for the given test cases.\n    \"\"\"\n    test_cases = [\n        {'m_target': 3.5, 'R_target': 1.409, 'N': 30000, 'seed': 42},\n        {'m_target': 2.0, 'R_target': 0.5, 'N': 15000, 'seed': 123},\n        {'m_target': 1.0, 'R_target': 4.0, 'N': 15000, 'seed': 456},\n        {'m_target': 2.0, 'R_target': 0.01, 'N': 4000, 'seed': 789},\n    ]\n    \n    epsilon = 3e-2\n    all_results = []\n\n    for case in test_cases:\n        m_target = case['m_target']\n        R_target = case['R_target']\n        N = case['N']\n        seed = case['seed']\n\n        # Task 1: Calibrate parameters a and b\n        a = 2 * m_target\n        # Assuming m_target > 0 as per test cases for the sqrt\n        b = (5 / np.pi**4) * (m_target / np.sqrt(R_target) - 1)\n\n        # Pre-compute analytical indices for validation\n        analytical_S, analytical_T = calculate_analytical_indices(a, b)\n\n        # Task 2: Run Monte Carlo GSA pipeline\n        estimated_S, estimated_T = monte_carlo_gsa(a, b, N, seed)\n\n        # Validate the pipeline\n        pass_first_order = np.all(np.abs(estimated_S - analytical_S) = epsilon)\n        pass_total = np.all(np.abs(estimated_T - analytical_T) = epsilon)\n\n        # Format results for the current test case\n        case_results = [\n            a, b,\n            estimated_S[0], estimated_S[1], estimated_S[2],\n            estimated_T[0], estimated_T[1], estimated_T[2],\n            pass_first_order, pass_total\n        ]\n        all_results.append(case_results)\n\n    # Print the final result in the exact required format\n    print(all_results)\n    \ndef ishigami_function(X, a, b):\n    \"\"\"\n    Computes the Ishigami function for a matrix of inputs.\n    X: a numpy array of shape (N, 3)\n    a, b: model parameters\n    \"\"\"\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n    x3 = X[:, 2]\n    \n    term1 = np.sin(x1)\n    term2 = a * np.sin(x2)**2\n    term3 = b * x3**4 * np.sin(x1)\n    \n    return term1 + term2 + term3\n\ndef calculate_analytical_indices(a, b):\n    \"\"\"\n    Calculates the analytical first-order and total-effect Sobol indices.\n    \"\"\"\n    pi_4 = np.pi**4\n    pi_8 = np.pi**8\n\n    # Partial variances\n    D1 = 0.5 * (1 + b * pi_4 / 5)**2\n    D2 = a**2 / 8\n    D13 = 8 * b**2 * pi_8 / 225\n\n    # Total variance\n    D_total = D1 + D2 + D13\n\n    if D_total == 0:\n        return np.zeros(3), np.zeros(3)\n\n    # First-order indices\n    S1 = D1 / D_total\n    S2 = D2 / D_total\n    S3 = 0.0\n    \n    # Total-effect indices\n    T1 = (D1 + D13) / D_total\n    T2 = D2 / D_total  # T2 is equal to S2\n    T3 = D13 / D_total\n\n    S_ana = np.array([S1, S2, S3])\n    T_ana = np.array([T1, T2, T3])\n    \n    return S_ana, T_ana\n\ndef monte_carlo_gsa(a, b, N, seed):\n    \"\"\"\n    Performs Monte Carlo GSA using Saltelli sampling.\n    \"\"\"\n    d = 3  # Number of input dimensions\n    rng = np.random.default_rng(seed)\n\n    # Generate Saltelli samples\n    A = rng.uniform(-np.pi, np.pi, size=(N, d))\n    B = rng.uniform(-np.pi, np.pi, size=(N, d))\n\n    # Evaluate the model on base samples\n    y_A = ishigami_function(A, a, b)\n    y_B = ishigami_function(B, a, b)\n\n    S_est = np.zeros(d)\n    T_est = np.zeros(d)\n    \n    # Use sample variance of y_A as the estimator for total variance.\n    # Using ddof=1 for an unbiased estimator.\n    total_variance_est = np.var(y_A, ddof=1)\n    \n    if total_variance_est == 0:\n        return np.zeros(d), np.zeros(d)\n\n    for i in range(d):\n        # Create mixed matrix A_B^(i)\n        A_B_i = A.copy()\n        A_B_i[:, i] = B[:, i]\n\n        # Evaluate the model on the mixed matrix\n        y_A_B_i = ishigami_function(A_B_i, a, b)\n\n        # Estimate first-order variance (Saltelli estimator)\n        first_order_var_est = np.mean(y_B * (y_A_B_i - y_A))\n        \n        # Estimate total-effect variance (Jansen estimator)\n        total_effect_var_est = 0.5 * np.mean((y_A - y_A_B_i)**2)\n\n        S_est[i] = first_order_var_est / total_variance_est\n        T_est[i] = total_effect_var_est / total_variance_est\n        \n    return S_est, T_est\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}