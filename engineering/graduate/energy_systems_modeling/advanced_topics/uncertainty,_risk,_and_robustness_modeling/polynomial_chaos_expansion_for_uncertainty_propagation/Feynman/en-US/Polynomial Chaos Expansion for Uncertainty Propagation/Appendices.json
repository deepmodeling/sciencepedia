{
    "hands_on_practices": [
        {
            "introduction": "Polynomial Chaos Expansions rely on basis functions tailored to the probability distribution of the uncertain inputs. For the ubiquitous Gaussian distribution, the appropriate basis is the set of Hermite polynomials. This first practice provides a hands-on derivation of the first few probabilists' Hermite polynomials directly from their defining properties, and asks you to verify their crucial orthogonality, which is the property that makes PCE computationally efficient .",
            "id": "4112413",
            "problem": "In a single-bus electricity system planning study, a normalized net-demand forecast error $Z$ is modeled as a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$, justified by aggregation of many small independent uncertainties and central limit behavior. To construct a Polynomial Chaos Expansion (PCE) for a linearized net power imbalance functional of $Z$, an orthogonal polynomial basis with respect to the standard normal weight is required. Let the weight function be $w(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$ and let $\\{He_{n}(x)\\}_{n=0}^{\\infty}$ denote the probabilists’ Hermite polynomials, defined by the property that they form a complete orthogonal system on $\\mathbb{R}$ with respect to $w(x)$ and are generated from a core definition grounded in standard normal calculus. \n\nStarting from fundamental definitions of orthogonal polynomials associated with $w(x)$ and known calculus identities for the standard normal density, derive the explicit forms of $He_{0}(x)$, $He_{1}(x)$, $He_{2}(x)$, and $He_{3}(x)$. Then, using only the standard normal weight $w(x)$ and first principles (e.g., integration by parts and properties of $\\exp(-x^{2}/2)$), verify the orthogonality relations for these first four polynomials, namely that $\\int_{-\\infty}^{\\infty} He_{m}(x)\\,He_{n}(x)\\,w(x)\\,\\mathrm{d}x = 0$ for $m \\neq n$ and determine the normalization integrals $\\int_{-\\infty}^{\\infty} He_{n}(x)^{2}\\,w(x)\\,\\mathrm{d}x$ for $n \\in \\{0,1,2,3\\}$.\n\nReport your final answer as the row vector of the four explicit polynomial expressions $\\big[He_{0}(x),\\,He_{1}(x),\\,He_{2}(x),\\,He_{3}(x)\\big]$. No rounding is required. No units are required because the basis functions are dimensionless by construction.",
            "solution": "The problem is validated as scientifically sound, well-posed, and objective. It is a standard problem in the theory of special functions, specifically concerning Hermite polynomials, which are fundamental to Polynomial Chaos Expansions for systems with Gaussian uncertainties. We will proceed with a full derivation and verification as requested.\n\nThe task is to derive the first four probabilists' Hermite polynomials $\\{He_{n}(x)\\}_{n=0}^{3}$ and verify their orthogonality properties with respect to the standard normal weight function $w(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$.\n\nA fundamental definition for the probabilists' Hermite polynomials is the Rodrigues-like formula, which is grounded in the calculus of the Gaussian function:\n$$\nHe_{n}(x) = (-1)^{n} \\exp\\left(\\frac{x^{2}}{2}\\right) \\frac{d^{n}}{dx^{n}} \\exp\\left(-\\frac{x^{2}}{2}\\right)\n$$\nWe will use this definition to derive the explicit forms of $He_{0}(x)$, $He_{1}(x)$, $He_{2}(x)$, and $He_{3}(x)$.\n\nFor $n=0$:\n$$\nHe_{0}(x) = (-1)^{0} \\exp\\left(\\frac{x^{2}}{2}\\right) \\frac{d^{0}}{dx^{0}} \\exp\\left(-\\frac{x^{2}}{2}\\right) = 1 \\cdot \\exp\\left(\\frac{x^{2}}{2}\\right) \\exp\\left(-\\frac{x^{2}}{2}\\right) = 1\n$$\n\nFor $n=1$:\nWe first compute the derivative: $\\frac{d}{dx} \\exp\\left(-\\frac{x^{2}}{2}\\right) = -x \\exp\\left(-\\frac{x^{2}}{2}\\right)$.\n$$\nHe_{1}(x) = (-1)^{1} \\exp\\left(\\frac{x^{2}}{2}\\right) \\left(-x \\exp\\left(-\\frac{x^{2}}{2}\\right)\\right) = x\n$$\n\nFor $n=2$:\nWe need the second derivative:\n$$\n\\frac{d^{2}}{dx^{2}} \\exp\\left(-\\frac{x^{2}}{2}\\right) = \\frac{d}{dx} \\left(-x \\exp\\left(-\\frac{x^{2}}{2}\\right)\\right) = -1 \\cdot \\exp\\left(-\\frac{x^{2}}{2}\\right) + (-x)\\left(-x \\exp\\left(-\\frac{x^{2}}{2}\\right)\\right) = (x^{2}-1) \\exp\\left(-\\frac{x^{2}}{2}\\right)\n$$\nThus,\n$$\nHe_{2}(x) = (-1)^{2} \\exp\\left(\\frac{x^{2}}{2}\\right) \\left((x^{2}-1) \\exp\\left(-\\frac{x^{2}}{2}\\right)\\right) = x^{2}-1\n$$\n\nFor $n=3$:\nWe need the third derivative:\n\\begin{align*}\n\\frac{d^{3}}{dx^{3}} \\exp\\left(-\\frac{x^{2}}{2}\\right) &= \\frac{d}{dx} \\left((x^{2}-1) \\exp\\left(-\\frac{x^{2}}{2}\\right)\\right) \\\\\n&= 2x \\exp\\left(-\\frac{x^{2}}{2}\\right) + (x^{2}-1)\\left(-x \\exp\\left(-\\frac{x^{2}}{2}\\right)\\right) \\\\\n&= (2x - x^{3} + x) \\exp\\left(-\\frac{x^{2}}{2}\\right) \\\\\n&= (-x^{3} + 3x) \\exp\\left(-\\frac{x^{2}}{2}\\right)\n\\end{align*}\nThus,\n$$\nHe_{3}(x) = (-1)^{3} \\exp\\left(\\frac{x^{2}}{2}\\right) \\left((-x^{3} + 3x) \\exp\\left(-\\frac{x^{2}}{2}\\right)\\right) = -(-x^{3} + 3x) = x^{3}-3x\n$$\nThe first four Hermite polynomials are:\n$He_{0}(x) = 1$\n$He_{1}(x) = x$\n$He_{2}(x) = x^{2}-1$\n$He_{3}(x) = x^{3}-3x$\n\nNext, we verify the orthogonality relations. The inner product of two functions $f(x)$ and $g(x)$ with respect to the weight $w(x)$ is defined as $\\langle f, g \\rangle = \\int_{-\\infty}^{\\infty} f(x)g(x)w(x)\\,\\mathrm{d}x$. We need to show that $\\langle He_{m}, He_{n} \\rangle = 0$ for $m \\neq n$ and determine the value for $m=n$.\n\nWe will demonstrate this using a general proof based on first principles, namely integration by parts. Let us consider the integral for $\\langle He_{m}, He_{n} \\rangle$ with $m \\le n$:\n$$\n\\langle He_{m}, He_{n} \\rangle = \\int_{-\\infty}^{\\infty} He_{m}(x) He_{n}(x) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)\\,\\mathrm{d}x\n$$\nSubstituting the Rodrigues-like formula for $He_{n}(x)$:\n$$\n\\langle He_{m}, He_{n} \\rangle = \\int_{-\\infty}^{\\infty} He_{m}(x) \\left[ (-1)^{n} \\exp\\left(\\frac{x^{2}}{2}\\right) \\frac{d^{n}}{dx^{n}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\right] \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^{2}}{2}\\right)\\,\\mathrm{d}x\n$$\nThe terms $\\exp\\left(\\frac{x^{2}}{2}\\right)$ and $\\exp\\left(-\\frac{x^{2}}{2}\\right)$ cancel out:\n$$\n\\langle He_{m}, He_{n} \\rangle = \\frac{(-1)^{n}}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} He_{m}(x) \\left( \\frac{d^{n}}{dx^{n}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\right) \\,\\mathrm{d}x\n$$\nWe apply integration by parts, with $u = He_{m}(x)$ and $dv = \\frac{d^{n}}{dx^{n}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\, \\mathrm{d}x$. Then $du = \\frac{d}{dx}He_{m}(x) \\, \\mathrm{d}x$ and $v = \\frac{d^{n-1}}{dx^{n-1}} \\exp\\left(-\\frac{x^{2}}{2}\\right)$.\n$$\n\\int u\\,dv = [uv]_{-\\infty}^{\\infty} - \\int v\\,du\n$$\nThe boundary term $[He_{m}(x) \\frac{d^{n-1}}{dx^{n-1}} \\exp\\left(-\\frac{x^{2}}{2}\\right)]_{-\\infty}^{\\infty}$ is $0$, because $\\exp\\left(-\\frac{x^{2}}{2}\\right)$ and all its derivatives decay to $0$ faster than any polynomial grows as $x \\to \\pm\\infty$.\nRepeating integration by parts $m$ times, we obtain:\n$$\n\\int_{-\\infty}^{\\infty} He_{m}(x) \\left( \\frac{d^{n}}{dx^{n}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\right) \\,\\mathrm{d}x = (-1)^{m} \\int_{-\\infty}^{\\infty} \\left( \\frac{d^{m}}{dx^{m}} He_{m}(x) \\right) \\left( \\frac{d^{n-m}}{dx^{n-m}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\right) \\,\\mathrm{d}x\n$$\nSince $He_{m}(x)$ is a monic polynomial of degree $m$ (i.e., the coefficient of $x^{m}$ is $1$), its $m$-th derivative is a constant: $\\frac{d^{m}}{dx^{m}} He_{m}(x) = m!$.\n$$\n\\langle He_{m}, He_{n} \\rangle = \\frac{(-1)^{n}}{\\sqrt{2\\pi}} (-1)^{m} m! \\int_{-\\infty}^{\\infty} \\frac{d^{n-m}}{dx^{n-m}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\,\\mathrm{d}x\n$$\nCase 1: $m < n$. In this case, $n-m \\ge 1$. The integral is of a derivative over $(-\\infty, \\infty)$:\n$$\n\\int_{-\\infty}^{\\infty} \\frac{d^{n-m}}{dx^{n-m}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\,\\mathrm{d}x = \\left[ \\frac{d^{n-m-1}}{dx^{n-m-1}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\right]_{-\\infty}^{\\infty} = 0\n$$\nTherefore, for $m \\neq n$, $\\langle He_{m}, He_{n} \\rangle = 0$. This confirms the orthogonality for all pairs with $m,n \\in \\{0,1,2,3\\}$ and $m \\neq n$.\n\nCase 2: $m = n$. This is the normalization integral.\n$$\n\\langle He_{n}, He_{n} \\rangle = \\frac{(-1)^{n}(-1)^{n}}{\\sqrt{2\\pi}} n! \\int_{-\\infty}^{\\infty} \\frac{d^{0}}{dx^{0}} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\,\\mathrm{d}x = \\frac{n!}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\,\\mathrm{d}x\n$$\nThe integral is the basis of the Gaussian distribution: $\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{x^{2}}{2}\\right) \\,\\mathrm{d}x = \\sqrt{2\\pi}$.\n$$\n\\langle He_{n}, He_{n} \\rangle = \\frac{n!}{\\sqrt{2\\pi}} (\\sqrt{2\\pi}) = n!\n$$\nThis general result determines the normalization integrals for $n \\in \\{0,1,2,3\\}$:\n$\\langle He_{0}, He_{0} \\rangle = 0! = 1$\n$\\langle He_{1}, He_{1} \\rangle = 1! = 1$\n$\\langle He_{2}, He_{2} \\rangle = 2! = 2$\n$\\langle He_{3}, He_{3} \\rangle = 3! = 6$\nThe derivations and verifications are complete. The final answer is the set of explicit polynomial expressions.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & x & x^{2}-1 & x^{3}-3x\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With an understanding of how to generate basis functions, we can now construct a full PCE surrogate model for a quantity of interest. This exercise walks you through this process for a non-polynomial response function—a common case in engineering models—subject to a uniformly distributed input variable. You will use the spectral projection method to compute the expansion coefficients and then leverage the properties of the resulting surrogate to efficiently calculate its statistical mean and variance .",
            "id": "4112418",
            "problem": "In an alternating-current transmission corridor model within energy systems modeling, the normalized active power transfer between two buses can be idealized as a scalar response $P(\\xi) = \\sin(\\xi)$, where $\\xi$ is a dimensionless uncertain input representing a standardized angle-like quantity. Assume $\\xi$ is distributed uniformly on the interval $[-1,1]$ and is measured in radians. Use generalized Polynomial Chaos (gPC), with Legendre polynomials as the basis appropriate for a uniform distribution, to construct the truncated expansion of $P(\\xi)$ up to total polynomial degree $3$.\n\nStarting from the definition of the $L^{2}$ projection with respect to the probability measure and the orthogonality of Legendre polynomials on $[-1,1]$ with the uniform weight, derive the expansion coefficients explicitly. Then, use the orthogonality of the truncated basis to compute the approximate mean and variance of the truncated gPC representation. Express the final mean and variance as closed-form analytical expressions involving $\\sin(1)$ and $\\cos(1)$ only. Do not round; provide exact expressions. Report your two final quantities (mean, variance) as a two-entry row vector. Angles must be expressed in radians.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of uncertainty quantification and power systems modeling, is well-posed with all necessary information provided, and is stated objectively.\n\nThe problem asks for the construction of a generalized Polynomial Chaos (gPC) expansion for the function $P(\\xi) = \\sin(\\xi)$, where the input variable $\\xi$ follows a uniform distribution on the interval $[-1, 1]$, denoted as $\\xi \\sim U[-1, 1]$. The expansion is to be truncated at a total polynomial degree of $M=3$. Subsequently, the mean and variance of this truncated representation are to be computed.\n\nThe probability density function (PDF) for $\\xi \\sim U[-1, 1]$ is $f_\\xi(x) = \\frac{1}{2}$ for $x \\in [-1, 1]$ and $0$ otherwise. For a uniformly distributed random variable, the appropriate orthogonal polynomial basis is the set of Legendre polynomials, denoted $\\Phi_j(\\xi)$. The gPC expansion of $P(\\xi)$ up to degree $M=3$ is given by:\n$$\n\\hat{P}_3(\\xi) = \\sum_{j=0}^{3} c_j \\Phi_j(\\xi) = c_0 \\Phi_0(\\xi) + c_1 \\Phi_1(\\xi) + c_2 \\Phi_2(\\xi) + c_3 \\Phi_3(\\xi)\n$$\nThe first four Legendre polynomials are:\n$$\n\\Phi_0(\\xi) = 1 \\\\\n\\Phi_1(\\xi) = \\xi \\\\\n\\Phi_2(\\xi) = \\frac{1}{2}(3\\xi^2 - 1) \\\\\n\\Phi_3(\\xi) = \\frac{1}{2}(5\\xi^3 - 3\\xi)\n$$\nThe coefficients $c_j$ are determined by projecting the function $P(\\xi)$ onto each basis polynomial. The inner product of two functions $f(\\xi)$ and $g(\\xi)$ with respect to the probability measure of $\\xi$ is defined as:\n$$\n\\langle f, g \\rangle = E[f(\\xi)g(\\xi)] = \\int_{-1}^{1} f(x)g(x)f_\\xi(x)dx = \\frac{1}{2} \\int_{-1}^{1} f(x)g(x)dx\n$$\nThe Legendre polynomials are orthogonal with respect to this inner product. Their squared norms are given by:\n$$\n\\langle \\Phi_j^2 \\rangle = \\frac{1}{2} \\int_{-1}^{1} [\\Phi_j(x)]^2 dx = \\frac{1}{2} \\frac{2}{2j+1} = \\frac{1}{2j+1}\n$$\nThe coefficient $c_j$ is calculated as:\n$$\nc_j = \\frac{\\langle P, \\Phi_j \\rangle}{\\langle \\Phi_j^2 \\rangle} = (2j+1) \\langle P, \\Phi_j \\rangle = \\frac{2j+1}{2} \\int_{-1}^{1} \\sin(\\xi) \\Phi_j(\\xi) d\\xi\n$$\nWe now compute the coefficients $c_0, c_1, c_2, c_3$.\n\nFor $c_0$:\n$$\nc_0 = \\frac{1}{2} \\int_{-1}^{1} \\sin(\\xi) \\Phi_0(\\xi) d\\xi = \\frac{1}{2} \\int_{-1}^{1} \\sin(\\xi) d\\xi\n$$\nSince $\\sin(\\xi)$ is an odd function, its integral over the symmetric interval $[-1, 1]$ is zero.\n$$\nc_0 = 0\n$$\n\nFor $c_1$:\n$$\nc_1 = \\frac{3}{2} \\int_{-1}^{1} \\sin(\\xi) \\Phi_1(\\xi) d\\xi = \\frac{3}{2} \\int_{-1}^{1} \\xi \\sin(\\xi) d\\xi\n$$\nThe integrand $\\xi \\sin(\\xi)$ is an even function. Using integration by parts ($\\int u dv = uv - \\int v du$ with $u=\\xi, dv=\\sin(\\xi)d\\xi$):\n$$\n\\int \\xi \\sin(\\xi) d\\xi = -\\xi \\cos(\\xi) + \\sin(\\xi)\n$$\n$$\nc_1 = \\frac{3}{2} [-\\xi \\cos(\\xi) + \\sin(\\xi)]_{-1}^{1} = \\frac{3}{2} \\left( (-\\cos(1) + \\sin(1)) - (-( -1)\\cos(-1) + \\sin(-1)) \\right)\n$$\nUsing $\\cos(-1)=\\cos(1)$ and $\\sin(-1)=-\\sin(1)$:\n$$\nc_1 = \\frac{3}{2} (\\sin(1) - \\cos(1) - (\\cos(1) - \\sin(1))) = \\frac{3}{2} (2\\sin(1) - 2\\cos(1)) = 3(\\sin(1) - \\cos(1))\n$$\n\nFor $c_2$:\n$$\nc_2 = \\frac{5}{2} \\int_{-1}^{1} \\sin(\\xi) \\Phi_2(\\xi) d\\xi = \\frac{5}{4} \\int_{-1}^{1} \\sin(\\xi) (3\\xi^2 - 1) d\\xi\n$$\nThe integrand is a product of an odd function, $\\sin(\\xi)$, and an even function, $(3\\xi^2 - 1)$, resulting in an odd function. The integral over $[-1, 1]$ is zero.\n$$\nc_2 = 0\n$$\n\nFor $c_3$:\n$$\nc_3 = \\frac{7}{2} \\int_{-1}^{1} \\sin(\\xi) \\Phi_3(\\xi) d\\xi = \\frac{7}{4} \\int_{-1}^{1} \\sin(\\xi) (5\\xi^3 - 3\\xi) d\\xi\n$$\nThe integrand is a product of two odd functions, which is an even function. The integral is twice the integral from $0$ to $1$:\n$$\nc_3 = \\frac{7}{2} \\int_{0}^{1} \\sin(\\xi) (5\\xi^3 - 3\\xi) d\\xi = \\frac{35}{2} \\int_{0}^{1} \\xi^3 \\sin(\\xi) d\\xi - \\frac{21}{2} \\int_{0}^{1} \\xi \\sin(\\xi) d\\xi\n$$\nWe already found $\\int_{0}^{1} \\xi \\sin(\\xi) d\\xi = [-\\xi \\cos(\\xi) + \\sin(\\xi)]_0^1 = \\sin(1) - \\cos(1)$. For the other integral, we use repeated integration by parts:\n$$\n\\int \\xi^3 \\sin(\\xi) d\\xi = (6\\xi - \\xi^3)\\cos(\\xi) + (3\\xi^2 - 6)\\sin(\\xi)\n$$\n$$\n\\int_{0}^{1} \\xi^3 \\sin(\\xi) d\\xi = [(6\\xi - \\xi^3)\\cos(\\xi) + (3\\xi^2 - 6)\\sin(\\xi)]_0^1 = (5\\cos(1) - 3\\sin(1)) - 0 = 5\\cos(1) - 3\\sin(1)\n$$\nSubstituting these results into the expression for $c_3$:\n$$\nc_3 = \\frac{35}{2} (5\\cos(1) - 3\\sin(1)) - \\frac{21}{2} (\\sin(1) - \\cos(1))\n$$\n$$\nc_3 = \\frac{1}{2} (175\\cos(1) - 105\\sin(1) - 21\\sin(1) + 21\\cos(1))\n$$\n$$\nc_3 = \\frac{1}{2} (196\\cos(1) - 126\\sin(1)) = 98\\cos(1) - 63\\sin(1)\n$$\nAlternatively, $c_3 = 7(14\\cos(1) - 9\\sin(1))$.\n\nThe mean of the truncated gPC expansion is given by the first coefficient:\n$$\n\\mu_{\\hat{P}} = E[\\hat{P}_3(\\xi)] = c_0\n$$\n$$\n\\mu_{\\hat{P}} = 0\n$$\nThe variance is calculated from the remaining coefficients due to the orthogonality of the basis polynomials:\n$$\n\\sigma^2_{\\hat{P}} = \\text{Var}[\\hat{P}_3(\\xi)] = \\sum_{j=1}^{3} c_j^2 \\langle \\Phi_j^2 \\rangle = \\sum_{j=1}^{3} \\frac{c_j^2}{2j+1}\n$$\n$$\n\\sigma^2_{\\hat{P}} = \\frac{c_1^2}{3} + \\frac{c_2^2}{5} + \\frac{c_3^2}{7}\n$$\nSubstituting the calculated coefficients ($c_2 = 0$):\n$$\n\\sigma^2_{\\hat{P}} = \\frac{(3(\\sin(1) - \\cos(1)))^2}{3} + \\frac{(98\\cos(1) - 63\\sin(1))^2}{7}\n$$\n$$\n\\sigma^2_{\\hat{P}} = \\frac{9(\\sin(1) - \\cos(1))^2}{3} + \\frac{(7(14\\cos(1) - 9\\sin(1)))^2}{7}\n$$\n$$\n\\sigma^2_{\\hat{P}} = 3(\\sin(1) - \\cos(1))^2 + 7(14\\cos(1) - 9\\sin(1))^2\n$$\nExpanding the terms:\n$$\n(\\sin(1)-\\cos(1))^2 = \\sin^2(1) - 2\\sin(1)\\cos(1) + \\cos^2(1) = 1 - 2\\sin(1)\\cos(1)\n$$\n$$\n(14\\cos(1) - 9\\sin(1))^2 = 196\\cos^2(1) - 252\\sin(1)\\cos(1) + 81\\sin^2(1)\n$$\nCombining these into the variance expression:\n$$\n\\sigma^2_{\\hat{P}} = 3(1 - 2\\sin(1)\\cos(1)) + 7(196\\cos^2(1) - 252\\sin(1)\\cos(1) + 81\\sin^2(1))\n$$\n$$\n\\sigma^2_{\\hat{P}} = 3 - 6\\sin(1)\\cos(1) + 1372\\cos^2(1) - 1764\\sin(1)\\cos(1) + 567\\sin^2(1)\n$$\n$$\n\\sigma^2_{\\hat{P}} = 3 + 1372\\cos^2(1) + 567\\sin^2(1) - 1770\\sin(1)\\cos(1)\n$$\nThe mean and variance are reported as a two-entry row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & 3 + 1372\\cos^2(1) + 567\\sin^2(1) - 1770\\sin(1)\\cos(1) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A crucial assumption in the standard Polynomial Chaos Expansion framework is the statistical independence of the input random variables. In many real-world energy systems, however, inputs such as electricity load $L$ and renewable generation $R$ are correlated. This exercise demonstrates the importance of correctly modeling the input joint distribution by asking you to analytically derive the bias introduced in the model's expected outcome when a known correlation $\\rho$ is ignored, providing a clear lesson on a critical pitfall in uncertainty analysis .",
            "id": "4112417",
            "problem": "An energy system consists of a stochastic demand (load) and a stochastic renewable generation output that are known to be dependent due to shared meteorological drivers and socioeconomic patterns. Let the load be represented by the random variable $L$ and the renewable output by the random variable $R$. Assume $(L,R)$ is jointly Gaussian with means $\\mu_L$ and $\\mu_R$, variances $\\sigma_L^2$ and $\\sigma_R^2$, and correlation coefficient $\\rho \\in (-1,1)$. A stylized net balancing cost used in planning models is the expected squared mismatch, defined by the quantity of interest $Q(L,R) = (L - R)^2$.\n\nSuppose a Polynomial Chaos Expansion (PCE) is constructed to propagate the uncertainty of $Q(L,R)$, but the basis is selected under the incorrect assumption that $L$ and $R$ are independent while preserving their marginal distributions. In this mis-specified PCE, the constant coefficient corresponds to the expectation computed with respect to the product of the marginals, i.e., the expectation under independence. Define the bias $b$ as the difference between this independence-based expectation and the true expectation under the correlated joint distribution, that is $b = \\mathbb{E}_{\\text{ind}}[Q] - \\mathbb{E}[Q]$.\n\nUsing only fundamental definitions of expectation, variance, covariance, and correlation for jointly Gaussian variables, determine the expression for $b$ in terms of $\\sigma_L$, $\\sigma_R$, and $\\rho$. Consider the general case with arbitrary $\\mu_L$ and $\\mu_R$.\n\nChoose the correct option:\n\nA. $b = 2 \\,\\rho \\,\\sigma_L \\,\\sigma_R$\n\nB. $b = - 2 \\,\\rho \\,\\sigma_L \\,\\sigma_R$\n\nC. $b = \\rho^2 \\,\\sigma_L^2 \\,\\sigma_R^2$\n\nD. $b = 0$",
            "solution": "The problem statement is first validated for scientific soundness, consistency, and completeness before a solution is attempted.\n\nThe givens are as follows:\n- Two random variables, $L$ (load) and $R$ (renewable output).\n- The pair $(L,R)$ is jointly Gaussian.\n- The parameters of the joint distribution are the means $\\mu_L$ and $\\mu_R$, the variances $\\sigma_L^2$ and $\\sigma_R^2$, and the correlation coefficient $\\rho \\in (-1,1)$.\n- The quantity of interest is $Q(L,R) = (L - R)^2$.\n- The bias $b$ is defined as $b = \\mathbb{E}_{\\text{ind}}[Q] - \\mathbb{E}[Q]$.\n- $\\mathbb{E}[Q]$ is the true expectation of $Q$ under the correlated joint distribution of $(L,R)$.\n- $\\mathbb{E}_{\\text{ind}}[Q]$ is the expectation of $Q$ computed under the incorrect assumption that $L$ and $R$ are independent, while preserving their marginal distributions (i.e., with respect to the product of the marginal probability density functions).\n- The task is to find the expression for $b$ in terms of $\\sigma_L$, $\\sigma_R$, and $\\rho$.\n\nThe problem statement is valid. It is scientifically grounded in standard probability theory and its application to energy systems modeling, a well-established field. The concepts of joint Gaussian distributions, correlation, expectation, and the effect of mis-specifying model dependencies are fundamental in uncertainty quantification. The problem is well-posed, providing all necessary information to derive a unique mathematical solution. The language is objective and precise. No flaws, such as contradictions, ambiguities, or factual unsoundness, are present. Therefore, we can proceed with the derivation of the solution.\n\nThe bias $b$ is defined as the difference between the expectation under the independence assumption and the true expectation.\n$$b = \\mathbb{E}_{\\text{ind}}[Q] - \\mathbb{E}[Q]$$\nThe quantity of interest is $Q = (L - R)^2$. We can expand this expression:\n$$Q = L^2 - 2LR + R^2$$\nBy the linearity of the expectation operator, the true expectation $\\mathbb{E}[Q]$ is:\n$$\\mathbb{E}[Q] = \\mathbb{E}[L^2 - 2LR + R^2] = \\mathbb{E}[L^2] - 2\\mathbb{E}[LR] + \\mathbb{E}[R^2]$$\nSimilarly, the expectation under the independence assumption, $\\mathbb{E}_{\\text{ind}}[Q]$, is:\n$$\\mathbb{E}_{\\text{ind}}[Q] = \\mathbb{E}_{\\text{ind}}[L^2 - 2LR + R^2] = \\mathbb{E}_{\\text{ind}}[L^2] - 2\\mathbb{E}_{\\text{ind}}[LR] + \\mathbb{E}_{\\text{ind}}[R^2]$$\nThe problem states that the marginal distributions are preserved. This means that any expectation involving only $L$ or only $R$ is the same under both the true and the independence-based probability measures. Therefore:\n$$\\mathbb{E}_{\\text{ind}}[L^2] = \\mathbb{E}[L^2]$$\n$$\\mathbb{E}_{\\text{ind}}[R^2] = \\mathbb{E}[R^2]$$\nThe difference between the two expectations arises solely from the cross-term involving both $L$ and $R$. Under the independence assumption, the expectation of the product is the product of the expectations:\n$$\\mathbb{E}_{\\text{ind}}[LR] = \\mathbb{E}[L]\\mathbb{E}[R] = \\mu_L \\mu_R$$\nThe true expectation of the product $\\mathbb{E}[LR]$ is related to the covariance:\n$$\\text{Cov}(L, R) = \\mathbb{E}[LR] - \\mathbb{E}[L]\\mathbb{E}[R]$$\n$$\\implies \\mathbb{E}[LR] = \\text{Cov}(L, R) + \\mathbb{E}[L]\\mathbb{E}[R] = \\text{Cov}(L, R) + \\mu_L \\mu_R$$\nNow, we can compute the bias $b$ by subtracting $\\mathbb{E}[Q]$ from $\\mathbb{E}_{\\text{ind}}[Q]$:\n$$b = \\left( \\mathbb{E}[L^2] - 2\\mathbb{E}_{\\text{ind}}[LR] + \\mathbb{E}[R^2] \\right) - \\left( \\mathbb{E}[L^2] - 2\\mathbb{E}[LR] + \\mathbb{E}[R^2] \\right)$$\nThe terms $\\mathbb{E}[L^2]$ and $\\mathbb{E}[R^2]$ cancel out:\n$$b = -2\\mathbb{E}_{\\text{ind}}[LR] + 2\\mathbb{E}[LR]$$\n$$b = 2 \\left( \\mathbb{E}[LR] - \\mathbb{E}_{\\text{ind}}[LR] \\right)$$\nSubstituting the expressions for the expectations of the product:\n$$b = 2 \\left( (\\text{Cov}(L, R) + \\mu_L \\mu_R) - (\\mu_L \\mu_R) \\right)$$\n$$b = 2 \\text{Cov}(L, R)$$\nThe correlation coefficient $\\rho$ is defined as $\\rho = \\frac{\\text{Cov}(L, R)}{\\sigma_L \\sigma_R}$. This gives $\\text{Cov}(L, R) = \\rho \\,\\sigma_L \\,\\sigma_R$.\nSubstituting this into the expression for the bias $b$:\n$$b = 2 \\,\\rho \\,\\sigma_L \\,\\sigma_R$$\nThis result is independent of the means $\\mu_L$ and $\\mu_R$, as required.\n\nNow, we evaluate each of the given options.\n\nA. $b = 2 \\,\\rho \\,\\sigma_L \\,\\sigma_R$\nThis expression matches our derived result. The bias in the constant PCE coefficient (the expectation) due to incorrectly assuming independence is exactly twice the covariance of the input variables.\nVerdict: **Correct**.\n\nB. $b = - 2 \\,\\rho \\,\\sigma_L \\,\\sigma_R$\nThis expression has the opposite sign. It would result from an incorrect definition of the bias as $b = \\mathbb{E}[Q] - \\mathbb{E}_{\\text{ind}}[Q]$. Given the problem's definition $b = \\mathbb{E}_{\\text{ind}}[Q] - \\mathbb{E}[Q]$, this option is incorrect.\nVerdict: **Incorrect**.\n\nC. $b = \\rho^2 \\,\\sigma_L^2 \\,\\sigma_R^2$\nThis expression is equivalent to $(\\text{Cov}(L,R))^2$. Dimensionally, if $L$ and $R$ have units of, for instance, power (e.g., megawatts, $MW$), then $Q = (L-R)^2$ has units of $MW^2$. The bias $b$, being a difference of expectations of $Q$, must also have units of $MW^2$. The standard deviations $\\sigma_L$ and $\\sigma_R$ have units of $MW$, while $\\rho$ is dimensionless. The expression $\\rho^2 \\,\\sigma_L^2 \\,\\sigma_R^2$ has units of $(MW^2)(MW^2) = MW^4$. This is dimensionally inconsistent. Furthermore, our derivation yields a different algebraic form.\nVerdict: **Incorrect**.\n\nD. $b = 0$\nThis would imply $\\mathbb{E}_{\\text{ind}}[Q] = \\mathbb{E}[Q]$. Our derivation shows that $b = 2\\,\\rho\\,\\sigma_L\\,\\sigma_R$. This is zero only if $\\rho = 0$, or if one of the variables is deterministic ($\\sigma_L = 0$ or $\\sigma_R = 0$). Since the problem is concerned with dependent variables, we must consider the general case where $\\rho \\neq 0$. Thus, the bias is generally non-zero.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}