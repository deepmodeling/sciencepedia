{
    "hands_on_practices": [
        {
            "introduction": "This first exercise drills down into the heart of the Kalman filter: the measurement update step. We begin with a prediction of a system's state and its associated uncertainty, then incorporate a new, noisy measurement. The core principle at play is the optimal blending of this prior belief with new information, where the Kalman gain acts as the weighting factor that minimizes the final estimation error. This practice  offers a concrete application to a microgrid, allowing you to calculate the key components of the update and observe how a measurement refines the state estimate and reduces its uncertainty.",
            "id": "4124641",
            "problem": "An islanded microgrid features a single dominant bus whose voltage phase angle evolves approximately as a random walk due to stochastic power balance fluctuations and inverter droop control. Let the scalar state $x_{k}$ denote the bus voltage phase angle in radians at discrete time index $k$. The process model and measurement model are specified by the linear time-invariant state-space system\n$$\nx_{k+1} = A x_{k} + w_{k}, \\quad z_{k} = C x_{k} + v_{k},\n$$\nwhere $w_{k}$ and $v_{k}$ are mutually independent, zero-mean Gaussian sequences with covariances $Q$ and $R$, respectively. A Phasor Measurement Unit (PMU) provides a direct measurement of the angle.\n\nFor a single measurement update at time $k$, you are given the following numerically specified parameters and prior quantities:\n- State transition coefficient $A = 1$,\n- Measurement coefficient $C = 1$,\n- Process noise covariance $Q = 2.5 \\times 10^{-4} \\text{ rad}^{2}$,\n- Measurement noise covariance $R = 7.5 \\times 10^{-3} \\text{ rad}^{2}$,\n- Prior (predicted) mean $\\hat{x}_{k|k-1} = 0.12 \\text{ rad}$,\n- Prior (predicted) covariance $P_{k|k-1} = 0.0102 \\text{ rad}^{2}$,\n- PMU measurement $z_{k} = 0.155 \\text{ rad}$.\n\nStarting from first principles for linear Gaussian state estimation, derive the measurement update for this scalar case and compute the Kalman gain $K_{k}$, the posterior (updated) state estimate $\\hat{x}_{k|k}$ in radians, and the posterior (updated) covariance $P_{k|k}$ in squared radians for this single measurement update step. Round your final numerical values to four significant figures. Express the state estimate in radians and the covariance in squared radians. Provide your final three values in the order $K_{k}$, $\\hat{x}_{k|k}$, $P_{k|k}$.",
            "solution": "The user has provided a problem statement for validation and solution.\n\n### Step 1: Extract Givens\nThe verbatim givens extracted from the problem statement are:\n- State-space model: $x_{k+1} = A x_{k} + w_{k}$, $z_{k} = C x_{k} + v_{k}$\n- Noise characteristics: $w_{k}$ and $v_{k}$ are mutually independent, zero-mean Gaussian sequences with covariances $Q$ and $R$, respectively.\n- State transition coefficient: $A = 1$\n- Measurement coefficient: $C = 1$\n- Process noise covariance: $Q = 2.5 \\times 10^{-4} \\text{ rad}^{2}$\n- Measurement noise covariance: $R = 7.5 \\times 10^{-3} \\text{ rad}^{2}$\n- Prior (predicted) mean: $\\hat{x}_{k|k-1} = 0.12 \\text{ rad}$\n- Prior (predicted) covariance: $P_{k|k-1} = 0.0102 \\text{ rad}^{2}$\n- PMU measurement: $z_{k} = 0.155 \\text{ rad}$\n- Required outputs: Kalman gain $K_{k}$, posterior state estimate $\\hat{x}_{k|k}$, posterior covariance $P_{k|k}$.\n- Rounding requirement: Final numerical values rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the established criteria.\n\n1.  **Scientifically Grounded**: The problem describes a standard application of a Kalman filter for state estimation of a bus voltage phase angle in a microgrid. The random walk process model ($A=1$) is a valid and common simplification for systems subject to stochastic drift. The use of a Phasor Measurement Unit (PMU) is standard technology for this purpose. The problem is firmly grounded in control theory and power systems engineering.\n2.  **Well-Posed**: The problem provides all necessary numerical values and a complete description of the state-space model required to perform a single measurement update step of the Kalman filter. A unique solution for the requested quantities ($K_{k}$, $\\hat{x}_{k|k}$, $P_{k|k}$) exists and is stable.\n3.  **Objective**: The problem is stated in precise, technical language with no subjective or ambiguous terms. All parameters are defined numerically.\n4.  **Completeness and Consistency**: The problem is self-contained. The given data are dimensionally consistent (angles in radians, covariances in radians squared). There are no contradictions.\n5.  **Plausibility**: The numerical values for the covariances, state, and measurement are of a realistic magnitude for the described physical system.\n\nThe problem statement has no identifiable flaws. It is a valid, well-posed, and scientifically sound problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation and Calculation\nWe seek to compute the measurement update for a scalar linear Gaussian system. The state $x_k$ is a random variable. The prior knowledge about $x_k$ at time $k$, before incorporating the measurement $z_k$, is captured by the prior probability distribution $p(x_k | z_{1:k-1})$. For a Kalman filter, this is a Gaussian distribution with mean $\\hat{x}_{k|k-1}$ and variance $P_{k|k-1}$:\n$$x_k \\sim \\mathcal{N}(\\hat{x}_{k|k-1}, P_{k|k-1})$$\nThe measurement $z_k$ provides new information. The relationship $z_k = C x_k + v_k$ where $v_k \\sim \\mathcal{N}(0, R)$ defines the likelihood of observing $z_k$ given the state $x_k$. This likelihood function $p(z_k | x_k)$ is a Gaussian distribution centered at $C x_k$ with variance $R$:\n$$p(z_k|x_k) \\sim \\mathcal{N}(C x_k, R)$$\nAccording to Bayes' theorem, the posterior distribution $p(x_k | z_k, z_{1:k-1}) \\equiv p(x_k|z_{1:k})$ is proportional to the product of the likelihood and the prior:\n$$p(x_k|z_{1:k}) \\propto p(z_k|x_k) p(x_k|z_{1:k-1})$$\nSince the product of two Gaussian distributions is another Gaussian distribution, the posterior is also Gaussian, characterized by its mean $\\hat{x}_{k|k}$ and variance $P_{k|k}$. The expressions for the posterior mean and variance are the standard Kalman filter measurement update equations. For the scalar case, these are:\n\n1.  **Kalman Gain ($K_k$):** The gain determines how much weight is given to the new measurement.\n    $$K_k = \\frac{P_{k|k-1} C}{C^2 P_{k|k-1} + R}$$\n2.  **Posterior State Estimate ($\\hat{x}_{k|k}$):** The updated state estimate is a linear combination of the prior estimate and the measurement innovation.\n    $$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (z_k - C \\hat{x}_{k|k-1})$$\n3.  **Posterior Covariance ($P_{k|k}$):** The updated covariance reflects the reduction in uncertainty after the measurement.\n    $$P_{k|k} = (1 - K_k C) P_{k|k-1}$$\n\nWe now substitute the given numerical values. The problem specifies $C=1$, which simplifies the equations.\n\n**Given values:**\n- Prior mean: $\\hat{x}_{k|k-1} = 0.12$\n- Prior covariance: $P_{k|k-1} = 0.0102$\n- Measurement: $z_k = 0.155$\n- Measurement noise covariance: $R = 7.5 \\times 10^{-3} = 0.0075$\n- Measurement coefficient: $C = 1$\n\n**Calculation of Kalman Gain ($K_k$):**\nUsing the simplified formula for $C=1$:\n$$K_k = \\frac{P_{k|k-1}}{P_{k|k-1} + R}$$\n$$K_k = \\frac{0.0102}{0.0102 + 0.0075} = \\frac{0.0102}{0.0177} \\approx 0.576271186$$\nRounding to four significant figures, the Kalman gain is $K_k = 0.5763$. The gain is a dimensionless quantity.\n\n**Calculation of Posterior State Estimate ($\\hat{x}_{k|k}$):**\nFirst, we compute the measurement innovation, which is the difference between the actual measurement and the predicted measurement ($C\\hat{x}_{k|k-1}$):\n$$y_k = z_k - C \\hat{x}_{k|k-1} = 0.155 - (1)(0.12) = 0.035 \\text{ rad}$$\nNow, we update the state estimate:\n$$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k y_k$$\n$$\\hat{x}_{k|k} = 0.12 + (0.576271186)(0.035) = 0.12 + 0.0201694915 = 0.1401694915 \\text{ rad}$$\nRounding to four significant figures, the posterior state estimate is $\\hat{x}_{k|k} = 0.1402$ rad.\n\n**Calculation of Posterior Covariance ($P_{k|k}$):**\nUsing the simplified formula for $C=1$:\n$$P_{k|k} = (1 - K_k) P_{k|k-1}$$\n$$P_{k|k} = (1 - 0.576271186)(0.0102) = (0.423728814)(0.0102) = 0.0043220339 \\text{ rad}^2$$\nRounding to four significant figures, the posterior covariance is $P_{k|k} = 0.004322$ rad$^2$. This value represents a significant reduction from the prior covariance of $0.0102$ rad$^2$, indicating that the measurement was informative.\n\nThe final numerical results for the three requested quantities, rounded to four significant figures, are:\n- $K_k = 0.5763$\n- $\\hat{x}_{k|k} = 0.1402$ rad\n- $P_{k|k} = 0.004322$ rad$^2$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5763 & 0.1402 & 0.004322\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world energy systems are rarely linear. This exercise confronts the challenge of state estimation for systems with nonlinear measurement models, a common scenario in power systems where quantities like power are often quadratic functions of voltage. We will compare the widely used Extended Kalman Filter (EKF), which relies on linearization, with the more robust Unscented Kalman Filter (UKF). Through a carefully designed thought experiment , you will quantify the estimation bias introduced by the EKF's approximation and see why the UKF can provide more accurate results in the presence of significant nonlinearity.",
            "id": "4124654",
            "problem": "A distribution feeder bus voltage magnitude $x$ (in per unit) is to be estimated from a nonlinear power injection measurement with significant curvature. In a simplified but scientifically consistent constant-conductance approximation, the real power injection $y$ obeys the nonlinear measurement model $y = h(x) + v$, where $h(x) = Gx^2$, the conductance $G$ is known and positive, and the additive measurement noise $v$ is zero-mean Gaussian with variance $R$. The prior state is modeled as Gaussian $x \\sim \\mathcal{N}(m, P)$, representing the uncertainty in the bus voltage before assimilating the measurement. Consider the case $G=1$, $m=1.0$, $P=0.04$, and $R=0.01$.\n\nYou will compare the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF) by analyzing the bias in the posterior mean induced by the curvature of the measurement function. Specifically, define the measurement to be the prior expected value under the true nonlinear model, $y := \\mathbb{E}[h(x)]$, so that, in an information-theoretic sense, the measurement is centered at the prior expectation. Starting from the core definitions of Gaussian moments and the standard update structures for the EKF and UKF (with the Unscented Transform applied in the measurement update), derive, from first principles, how the measurement curvature affects the innovation and the posterior mean in each filter.\n\nThen, compute the posterior mean updates produced by the EKF and the UKF for the given numerical values, and define the bias as the magnitude of the shift of the posterior mean away from the prior mean under this centered measurement, i.e., $|m^{+} - m|$. Report the EKF bias magnitude. Round your answer to four significant figures and express it in per unit (pu).",
            "solution": "The problem asks for an analysis of the bias induced by the Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF) for a specific nonlinear state estimation problem, and to compute the EKF bias magnitude.\n\nFirst, we establish the components of the state estimation problem as given.\nThe state is the bus voltage magnitude, a scalar $x$.\nThe prior knowledge of the state is modeled by a Gaussian distribution: $x \\sim \\mathcal{N}(m, P)$, with prior mean $m=1.0$ and prior variance $P=0.04$.\nThe measurement model is $y = h(x) + v$, where the measurement function is $h(x) = Gx^2$. We are given $G=1$, so $h(x) = x^2$.\nThe measurement noise $v$ is a zero-mean Gaussian random variable with variance $R$, so $v \\sim \\mathcal{N}(0, R)$, with $R=0.01$.\n\nA critical part of the problem setup is the definition of the specific measurement value to be used. The measurement $y$ is set to the prior expected value of the true nonlinear function, $h(x)$. We calculate this value first.\nThe expectation of $h(x) = x^2$ for a random variable $x$ with mean $m$ and variance $P$ is given by the property $\\mathbb{E}[x^2] = (\\mathbb{E}[x])^2 + \\text{Var}(x)$.\nTherefore, the measurement value is:\n$$y := \\mathbb{E}[h(x)] = \\mathbb{E}[x^2] = m^2 + P$$\nSubstituting the given numerical values, $m=1.0$ and $P=0.04$:\n$$y = (1.0)^2 + 0.04 = 1.0 + 0.04 = 1.04$$\n\nNext, we derive the posterior mean update for the EKF and analyze its bias. The EKF linearizes the nonlinear measurement function $h(x)$ around the prior mean $m$. The linearized function is:\n$$h(x) \\approx h(m) + H(x-m)$$\nwhere $H$ is the Jacobian of $h(x)$ evaluated at $x=m$. For our scalar case, this is the derivative:\n$$H = \\frac{dh}{dx}\\bigg|_{x=m} = \\frac{d(x^2)}{dx}\\bigg|_{x=m} = 2x\\bigg|_{x=m} = 2m$$\nThe EKF uses this linearized model to predict the measurement. The predicted measurement, denoted $\\hat{y}_{EKF}$, is the expectation of the linearized function:\n$$\\hat{y}_{EKF} = \\mathbb{E}[h(m) + H(x-m)] = h(m) + H(\\mathbb{E}[x]-m) = h(m) + H(m-m) = h(m) = m^2$$\nThe EKF innovation is the difference between the actual measurement $y$ and the filter's predicted measurement $\\hat{y}_{EKF}$:\n$$\\nu_{EKF} = y - \\hat{y}_{EKF} = (m^2 + P) - m^2 = P$$\nThis non-zero innovation arises entirely from the EKF's approximation error. The filter's first-order approximation of the expected measurement, $m^2$, differs from the true expected measurement, $m^2+P$, by a term $P$, which is related to the curvature of $h(x)$.\n\nThe standard Kalman filter update equations proceed with this innovation. The innovation covariance is:\n$$S_{EKF} = H P H^T + R$$\nSubstituting $H=2m$:\n$$S_{EKF} = (2m) P (2m) + R = 4m^2P + R$$\nThe Kalman gain is:\n$$K_{EKF} = P H^T S_{EKF}^{-1} = P(2m)(4m^2P + R)^{-1} = \\frac{2mP}{4m^2P+R}$$\nThe posterior mean, $m_{EKF}^{+}$, is updated from the prior mean $m$ as follows:\n$$m_{EKF}^{+} = m + K_{EKF} \\nu_{EKF} = m + \\left(\\frac{2mP}{4m^2P+R}\\right) P = m + \\frac{2mP^2}{4m^2P+R}$$\nThe bias, defined as the shift of the posterior mean from the prior mean, is the magnitude of the correction term:\n$$|m_{EKF}^{+} - m| = \\left|\\frac{2mP^2}{4m^2P+R}\\right|$$\nSince $m>0$ and $P>0$ and $R>0$, the absolute value is redundant.\n\nFor comparison, we briefly analyze the UKF. The Unscented Transform (UT) used by the UKF propagates a set of deterministically chosen sigma points through the true nonlinear function $h(x)=x^2$ to estimate the moments of the output distribution. For a quadratic function and a Gaussian input, the UT computes the exact mean of the transformed variable. Therefore, the UKF's predicted measurement is:\n$$\\hat{y}_{UKF} = \\mathbb{E}[h(x)] = m^2+P$$\nThe UKF innovation is then:\n$$\\nu_{UKF} = y - \\hat{y}_{UKF} = (m^2+P) - (m^2+P) = 0$$\nSince the innovation is zero, the correction term is zero, and the posterior mean is not updated:\n$$m_{UKF}^{+} = m + K_{UKF} \\nu_{UKF} = m + 0 = m$$\nThus, the UKF correctly identifies that the measurement conveys no new information relative to the prior and exhibits zero bias in this specific scenario, i.e., $|m_{UKF}^{+} - m| = 0$.\n\nThe problem requires the computation of the EKF bias magnitude. We substitute the given numerical values into the derived expression: $m=1.0$, $P=0.04$, and $R=0.01$.\n$$|m_{EKF}^{+} - m| = \\frac{2(1.0)(0.04)^2}{4(1.0)^2(0.04) + 0.01}$$\n$$|m_{EKF}^{+} - m| = \\frac{2(0.0016)}{4(0.04) + 0.01} = \\frac{0.0032}{0.16 + 0.01} = \\frac{0.0032}{0.17}$$\n$$|m_{EKF}^{+} - m| \\approx 0.018823529...$$\nRounding the result to four significant figures gives $0.01882$. The units are per unit (pu).",
            "answer": "$$\n\\boxed{0.01882}\n$$"
        },
        {
            "introduction": "State variables in physical systems, such as the charge in a battery or the water level in a reservoir, are often bound by strict physical limits. The standard Kalman filter ignores these constraints, potentially yielding physically impossible estimates. This practice introduces Moving Horizon Estimation (MHE), an optimization-based approach that explicitly incorporates such constraints. By implementing and comparing a constrained MHE with an unconstrained Kalman Filter , you will gain hands-on experience with a powerful technique for ensuring that your estimates remain within the realm of physical possibility, a crucial requirement for reliable monitoring and control of energy systems.",
            "id": "4124685",
            "problem": "Consider a discrete-time model of a single energy storage device governed by conservation of energy over a uniform time step. Let the state be the stored energy $x_k$ in megawatt-hour (MWh) at discrete time index $k$, and let the commanded power input $u_k$ in megawatt (MW) be positive for charging and negative for discharging. Let the self-discharge rate per time step be $\\alpha$ (unitless), the charging efficiency be $\\eta_c$ (unitless), the discharging efficiency be $\\eta_d$ (unitless), and the sampling period be $\\Delta t$ in hours. The storage is physically constrained by $x_{\\min} \\le x_k \\le x_{\\max}$. The dynamics are modeled as a linear time-varying system with process noise and measurement noise:\n$$\nx_{k+1} = a\\, x_k + b_k\\, u_k + w_k,\\quad a = 1 - \\alpha,\\quad b_k = \n\\begin{cases}\n\\eta_c \\Delta t, & u_k \\ge 0 \\\\\n\\frac{\\Delta t}{\\eta_d}, & u_k &lt; 0\n\\end{cases}\n$$\nwith Gaussian process noise $w_k \\sim \\mathcal{N}(0,q)$, and measurements\n$$\ny_k = c\\, x_k + v_k,\\quad c = 1,\\quad v_k \\sim \\mathcal{N}(0,r).\n$$\nAssume the initial state $x_0$ is known and the initial covariance for estimation is $P_0$. In the physical system, the stored energy $x_k$ must satisfy the saturation constraints $x_{\\min} \\le x_k \\le x_{\\max}$ at all times, while the unconstrained estimator may produce estimates that violate these bounds.\n\nMoving Horizon Estimation (MHE) over a horizon of length $N$ constructs an estimate of the complete state trajectory $\\{x_0,\\dots,x_N\\}$ by minimizing a weighted sum of squared residuals derived from the dynamic model and the measurements, subject to the physical constraints $x_{\\min} \\le x_k \\le x_{\\max}$. Under Gaussian noise assumptions, the maximum likelihood estimate is equivalent to weighted least squares where each residual is scaled by the square root of its variance. The unconstrained Kalman Filter (KF) produces a recursive minimum variance estimate for linear systems with Gaussian noise but does not enforce the hard bounds on $x_k$.\n\nStarting from the core definitions of discrete-time energy conservation, Gaussian noise modeling, and least-squares optimality, implement both the constrained MHE (as a weighted linear least-squares with box constraints on the states) and the unconstrained Kalman Filter. Simulate ground-truth trajectories by applying the same linear dynamics with additive Gaussian process noise and saturating the energy to $[x_{\\min},x_{\\max}]$ after each step to reflect physical realism, and generate noisy measurements accordingly. For each test case, compute the absolute estimation errors for the final state $x_N$ for both methods, and return whether the MHE yields a strictly smaller absolute error than the KF.\n\nAll energies must be treated in megawatt-hour ($\\mathrm{MWh}$), powers in megawatt ($\\mathrm{MW}$), and rates $\\alpha$ are unitless per time step, with $\\Delta t$ measured in hours. Angles are not involved. The required final outputs are booleans and thus have no units.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[True,False,True]\". The list must contain one boolean per test case indicating whether the MHE absolute error for $x_N$ is strictly smaller than the KF absolute error for $x_N$.\n\nUse the following test suite, which covers typical operation, tight capacity constraints, and high process noise:\n\n- Test Case $1$ (general operation):\n    - $x_0 = 20$ $\\mathrm{MWh}$, $x_{\\min} = 0$ $\\mathrm{MWh}$, $x_{\\max} = 100$ $\\mathrm{MWh}$, $\\Delta t = 1$ $\\mathrm{h}$, $\\alpha = 0.01$, $\\eta_c = 0.95$, $\\eta_d = 0.9$, $q = 0.5$ $\\mathrm{MWh}^2$, $r = 1.0$ $\\mathrm{MWh}^2$, $P_0 = 0.5$ $\\mathrm{MWh}^2$, $N = 8$, $u_0,\\dots,u_7 = [10,10,0,-8,-8,5,0,-6]$ $\\mathrm{MW}$, random seed $42$.\n- Test Case $2$ (tight capacity bounds):\n    - $x_0 = 30$ $\\mathrm{MWh}$, $x_{\\min} = 10$ $\\mathrm{MWh}$, $x_{\\max} = 40$ $\\mathrm{MWh}$, $\\Delta t = 1$ $\\mathrm{h}$, $\\alpha = 0.01$, $\\eta_c = 0.95$, $\\eta_d = 0.9$, $q = 0.5$ $\\mathrm{MWh}^2$, $r = 4.0$ $\\mathrm{MWh}^2$, $P_0 = 0.5$ $\\mathrm{MWh}^2$, $N = 8$, $u_0,\\dots,u_7 = [12,12,-12,-12,8,8,-10,-10]$ $\\mathrm{MW}$, random seed $43$.\n- Test Case $3$ (high process noise):\n    - $x_0 = 50$ $\\mathrm{MWh}$, $x_{\\min} = 0$ $\\mathrm{MWh}$, $x_{\\max} = 100$ $\\mathrm{MWh}$, $\\Delta t = 1$ $\\mathrm{h}$, $\\alpha = 0.02$, $\\eta_c = 0.95$, $\\eta_d = 0.9$, $q = 4.0$ $\\mathrm{MWh}^2$, $r = 1.0$ $\\mathrm{MWh}^2$, $P_0 = 0.5$ $\\mathrm{MWh}^2$, $N = 8$, $u_0,\\dots,u_7 = [5,-5,5,-5,5,-5,5,-5]$ $\\mathrm{MW}$, random seed $44$.\n\nYour implementation must:\n- Construct the time-varying input gain $b_k$ based on the sign of $u_k$.\n- Simulate the ground-truth energy trajectory with additive Gaussian process noise and saturate it to $[x_{\\min},x_{\\max}]$ each step.\n- Generate noisy measurements $y_k$ of the energy.\n- Implement the unconstrained Kalman Filter to estimate $x_N$.\n- Implement constrained Moving Horizon Estimation as a single weighted linear least-squares problem over $\\{x_0,\\dots,x_N\\}$ with box constraints $x_{\\min} \\le x_k \\le x_{\\max}$, by stacking dynamic residuals and measurement residuals (each scaled by the inverse of their standard deviations) and including a prior term for $x_0$.\n- Compare the absolute final-state estimation errors $|x_N^{\\text{est}} - x_N^{\\text{true}}|$ for both methods and output one boolean per test case indicating whether the MHE error is strictly smaller than the KF error.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\").",
            "solution": "We begin from the conservation of energy in discrete time and the definitions of the system. The stored energy $x_k$ evolves according to\n$x_{k+1} = x_k + \\Delta x_k$,\nwhere for charging, $\\Delta x_k = \\eta_c \\Delta t \\, u_k$ when $u_k \\ge 0$, and for discharging, $\\Delta x_k = \\frac{\\Delta t}{\\eta_d} \\, u_k$ when $u_k < 0$ (with $u_k$ negative). Self-discharge is modeled as a linear decay $x_{k+1} = (1 - \\alpha) x_k + \\text{net input}$, so defining $a = 1 - \\alpha$ and a time-varying input gain\n$$\nb_k =\n\\begin{cases}\n\\eta_c \\Delta t, & u_k \\ge 0 \\\\\n\\frac{\\Delta t}{\\eta_d}, & u_k &lt; 0\n\\end{cases},\n$$\nwe obtain the linear time-varying model\n$$\nx_{k+1} = a\\, x_k + b_k\\, u_k + w_k,\n$$\nwhere $w_k \\sim \\mathcal{N}(0,q)$ models unmodeled disturbances. The measurement is the stored energy corrupted by sensor noise,\n$$\ny_k = c\\, x_k + v_k,\\quad c = 1,\\quad v_k \\sim \\mathcal{N}(0,r).\n$$\nPhysical bounds impose $x_{\\min} \\le x_k \\le x_{\\max}$.\n\nFor ground-truth simulation, we generate process noise $w_k \\sim \\mathcal{N}(0,q)$ and propagate the dynamics\n$$\nx_{k+1}^{\\text{true}} = a\\, x_k^{\\text{true}} + b_k u_k + w_k,\n$$\nand then saturate $x_{k+1}^{\\text{true}}$ to $[x_{\\min},x_{\\max}]$. Measurements are generated as\n$$\ny_k = x_k^{\\text{true}} + v_k,\\quad v_k \\sim \\mathcal{N}(0,r).\n$$\n\nThe unconstrained Kalman Filter is derived from the optimal linear minimum-variance estimator for Gaussian noise. For a scalar linear system, the recursion consists of predict and update steps. With initial mean $x_0$ and covariance $P_0$, for $k = 0,\\dots,N-1$:\n- Prediction:\n$$\n\\hat{x}_{k+1|k} = a\\, \\hat{x}_{k|k} + b_k u_k,\\quad P_{k+1|k} = a^2 P_{k|k} + q.\n$$\n- Update with measurement $y_{k+1}$:\n$$\nK_{k+1} = \\frac{P_{k+1|k} c}{c^2 P_{k+1|k} + r},\\quad\n\\hat{x}_{k+1|k+1} = \\hat{x}_{k+1|k} + K_{k+1} (y_{k+1} - c \\hat{x}_{k+1|k}),\\quad\nP_{k+1|k+1} = (1 - K_{k+1} c) P_{k+1|k}.\n$$\nThe final estimate is $\\hat{x}_N = \\hat{x}_{N|N}$.\n\nMoving Horizon Estimation over a finite window constructs the estimate of the entire trajectory $\\mathbf{z} = [x_0,\\dots,x_N]^\\top$ by minimizing the sum of squared residuals corresponding to the dynamic equations and measurements, scaled by the inverse standard deviations to reflect Gaussian likelihood. Specifically, define the dynamic residuals\n$r_k^{\\text{dyn}} = x_{k+1} - a x_k - b_k u_k$ for $k=0,\\dots,N-1$,\nand the measurement residuals\n$r_k^{\\text{meas}} = y_k - c x_k$ for $k=0,\\dots,N$.\nInclude a prior residual on the initial state,\n$r^{\\text{prior}} = x_0 - x_0^{\\text{prior}}$,\nwith prior variance $P_0$. Under independent Gaussian noise assumptions, the negative log-likelihood (up to a constant) is\n$$\nJ(\\mathbf{z}) = \\sum_{k=0}^{N-1} \\frac{(r_k^{\\text{dyn}})^2}{q} + \\sum_{k=0}^{N} \\frac{(r_k^{\\text{meas}})^2}{r} + \\frac{(r^{\\text{prior}})^2}{P_0}.\n$$\nThis is a weighted linear least-squares problem in the variables $\\mathbf{z}$ when $y_k$ and $u_k$ are given. We can stack all residuals into a linear system $A \\mathbf{z} \\approx \\mathbf{b}$ by writing each residual as a linear form in $\\mathbf{z}$ equated to a known scalar:\n- For each $k=0,\\dots,N-1$, define a row with entries $[-a \\text{ at } k, 1 \\text{ at } k+1]$ and right-hand side $b_k u_k$, then scale both by $1/\\sqrt{q}$.\n- For each $k=0,\\dots,N$, define a row with entry $[c \\text{ at } k]$ and right-hand side $y_k$, scaled by $1/\\sqrt{r}$.\n- For the prior, define a row with entry $[1 \\text{ at } 0]$ and right-hand side $x_0^{\\text{prior}}$, scaled by $1/\\sqrt{P_0}$.\nThe MHE solution is obtained by minimizing $\\lVert A \\mathbf{z} - \\mathbf{b} \\rVert_2$ subject to box constraints $x_{\\min} \\le x_k \\le x_{\\max}$ for all $k$, which we solve using bound-constrained linear least-squares.\n\nAlgorithmic steps:\n1. Construct $b_k$ from $u_k$ and the efficiencies.\n2. Simulate the true trajectory with additive process noise and saturation to $[x_{\\min},x_{\\max}]$ at each step; generate measurements with additive sensor noise.\n3. Run the unconstrained Kalman Filter recursion to obtain $\\hat{x}_N^{\\text{KF}}$.\n4. Form the stacked and weighted least-squares system for MHE and solve for $\\mathbf{z}$ with box constraints; extract $x_N^{\\text{MHE}}$.\n5. Compute the absolute estimation errors $|x_N^{\\text{KF}} - x_N^{\\text{true}}|$ and $|x_N^{\\text{MHE}} - x_N^{\\text{true}}|$.\n6. For each test case, output a boolean indicating whether the MHE error is strictly smaller than the KF error.\n\nWe apply this to the three provided test cases. The first case exercises nominal behavior with moderate noise and wide bounds. The second case exercises tight capacity constraints, where enforcing bounds in MHE is expected to help relative to an unconstrained KF. The third case exercises high process noise, where the model mismatch reduces the advantage of constraints. The program aggregates the three booleans in the required single-line format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import lsq_linear\n\ndef compute_b_seq(u_seq, eta_c, eta_d, dt):\n    # Time-varying input gain b_k depending on sign of u_k\n    b = np.empty_like(u_seq, dtype=float)\n    for i, u in enumerate(u_seq):\n        if u >= 0:\n            b[i] = eta_c * dt\n        else:\n            b[i] = (dt / eta_d)\n    return b\n\ndef simulate_true(a, b_seq, x0, u_seq, q, r, emin, emax, seed):\n    N = len(u_seq)\n    rng = np.random.default_rng(seed)\n    w = rng.normal(0.0, np.sqrt(q), size=N)\n    v = rng.normal(0.0, np.sqrt(r), size=N+1)\n    x_true = np.empty(N+1, dtype=float)\n    x_true[0] = x0\n    for k in range(N):\n        x_next = a * x_true[k] + b_seq[k] * u_seq[k] + w[k]\n        # physical saturation\n        x_true[k+1] = np.clip(x_next, emin, emax)\n    y = x_true + v\n    return x_true, y\n\ndef kalman_filter_final(a, b_seq, c, x0, P0, u_seq, q, r, y):\n    # Unconstrained scalar Kalman filter to final state\n    N = len(u_seq)\n    x_est = float(x0)\n    P = float(P0)\n    for k in range(N):\n        # Predict to k+1\n        x_pred = a * x_est + b_seq[k] * u_seq[k]\n        P_pred = a * a * P + q\n        # Update with y_{k+1}\n        denom = c * c * P_pred + r\n        K = (P_pred * c) / denom\n        innovation = y[k+1] - c * x_pred\n        x_est = x_pred + K * innovation\n        P = (1.0 - K * c) * P_pred\n    return x_est\n\ndef mhe_final_state(a, b_seq, c, x0_prior, P0, u_seq, q, r, y, emin, emax):\n    # Solve weighted linear least squares with box constraints over z = [x0,...,xN]\n    N = len(u_seq)\n    n_vars = N + 1\n    rows = []\n    rhs = []\n    # Dynamic residuals: (x_{k+1} - a x_k) ≈ b_k u_k, weighted by 1/sqrt(q)\n    sigma_dyn = np.sqrt(q)\n    for k in range(N):\n        row = np.zeros(n_vars, dtype=float)\n        row[k] = -a\n        row[k+1] = 1.0\n        b_scalar = b_seq[k] * u_seq[k]\n        rows.append(row / sigma_dyn)\n        rhs.append(b_scalar / sigma_dyn)\n    # Measurement residuals: c x_k ≈ y_k, weighted by 1/sqrt(r)\n    sigma_meas = np.sqrt(r)\n    for k in range(n_vars):\n        row = np.zeros(n_vars, dtype=float)\n        row[k] = c\n        rows.append(row / sigma_meas)\n        rhs.append(y[k] / sigma_meas)\n    # Prior on x0: x0 ≈ x0_prior, weighted by 1/sqrt(P0)\n    sigma_prior = np.sqrt(P0)\n    row_prior = np.zeros(n_vars, dtype=float)\n    row_prior[0] = 1.0\n    rows.append(row_prior / sigma_prior)\n    rhs.append(x0_prior / sigma_prior)\n    A = np.vstack(rows)\n    bvec = np.array(rhs, dtype=float)\n    # Box bounds\n    lb = np.full(n_vars, emin, dtype=float)\n    ub = np.full(n_vars, emax, dtype=float)\n    result = lsq_linear(A, bvec, bounds=(lb, ub), method='trf', lsq_solver='exact')\n    z = result.x\n    return float(z[-1])\n\ndef run_case(case):\n    # Unpack case parameters\n    x0 = case['x0']\n    emin = case['emin']\n    emax = case['emax']\n    dt = case['dt']\n    alpha = case['alpha']\n    eta_c = case['eta_c']\n    eta_d = case['eta_d']\n    q = case['q']\n    r = case['r']\n    P0 = case['P0']\n    N = case['N']\n    u_seq = np.array(case['u'], dtype=float)\n    seed = case['seed']\n    a = 1.0 - alpha\n    c = 1.0\n    b_seq = compute_b_seq(u_seq, eta_c, eta_d, dt)\n    # Simulate true trajectory and measurements\n    x_true, y = simulate_true(a, b_seq, x0, u_seq, q, r, emin, emax, seed)\n    # Compute estimates\n    x_kf = kalman_filter_final(a, b_seq, c, x0, P0, u_seq, q, r, y)\n    x_mhe = mhe_final_state(a, b_seq, c, x0, P0, u_seq, q, r, y, emin, emax)\n    # Errors at final time N\n    err_kf = abs(x_kf - x_true[-1])\n    err_mhe = abs(x_mhe - x_true[-1])\n    # Return boolean: True if MHE strictly better\n    return err_mhe < err_kf\n\ndef solve():\n    test_cases = [\n        {\n            'x0': 20.0, 'emin': 0.0, 'emax': 100.0, 'dt': 1.0,\n            'alpha': 0.01, 'eta_c': 0.95, 'eta_d': 0.9,\n            'q': 0.5, 'r': 1.0, 'P0': 0.5, 'N': 8,\n            'u': [10, 10, 0, -8, -8, 5, 0, -6], 'seed': 42\n        },\n        {\n            'x0': 30.0, 'emin': 10.0, 'emax': 40.0, 'dt': 1.0,\n            'alpha': 0.01, 'eta_c': 0.95, 'eta_d': 0.9,\n            'q': 0.5, 'r': 4.0, 'P0': 0.5, 'N': 8,\n            'u': [12, 12, -12, -12, 8, 8, -10, -10], 'seed': 43\n        },\n        {\n            'x0': 50.0, 'emin': 0.0, 'emax': 100.0, 'dt': 1.0,\n            'alpha': 0.02, 'eta_c': 0.95, 'eta_d': 0.9,\n            'q': 4.0, 'r': 1.0, 'P0': 0.5, 'N': 8,\n            'u': [5, -5, 5, -5, 5, -5, 5, -5], 'seed': 44\n        }\n    ]\n    results = []\n    for case in test_cases:\n        res = run_case(case)\n        results.append(res)\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}