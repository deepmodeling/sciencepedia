{
    "hands_on_practices": [
        {
            "introduction": "The Kalman filter's recursive loop begins with the \"predict\" step, where we forecast the system's state and uncertainty based on its dynamic model. In this exercise , you will propagate a state estimate for a building's thermal model to understand how uncertainty evolves through the system dynamics. This practice builds a foundation for the filter's ability to anticipate a system's behavior before correcting it with a measurement.",
            "id": "4124639",
            "problem": "A three-state Resistive-Capacitive (RC) thermal network models a single-zone building with states representing indoor air temperature, internal thermal mass temperature, and envelope temperature. The discrete-time linear state evolution law over one sampling interval is\n$$\nx_{k+1} = A x_{k} + B u_{k} + w_{k},\n$$\nwhere $x_{k} \\in \\mathbb{R}^{3}$ is the state vector of absolute temperatures in Kelvin, $u_{k} \\in \\mathbb{R}$ is the heating power input in kilowatts, and $w_{k} \\in \\mathbb{R}^{3}$ is zero-mean process noise. The random vectors $x_{k}$ and $w_{k}$ are jointly Gaussian and independent, and the prior filtering distribution at time $k$ is $x_{k} \\sim \\mathcal{N}(\\hat{x}_{k|k}, P_{k|k})$.\n\nYou are given the following numerically specified model and prior:\n- State transition matrix\n$$\nA = \\begin{pmatrix}\n0.97 & 0.02 & 0.00 \\\\\n0.03 & 0.95 & 0.02 \\\\\n0.00 & 0.05 & 0.93\n\\end{pmatrix}.\n$$\n- Input matrix\n$$\nB = \\begin{pmatrix}\n0.15 \\\\\n0.10 \\\\\n0.02\n\\end{pmatrix}.\n$$\n- Process noise covariance\n$$\nQ = \\begin{pmatrix}\n0.04 & 0 & 0 \\\\\n0 & 0.05 & 0 \\\\\n0 & 0 & 0.03\n\\end{pmatrix}.\n$$\n- Prior mean and covariance\n$$\n\\hat{x}_{k|k} = \\begin{pmatrix} 296.0 \\\\ 294.5 \\\\ 293.0 \\end{pmatrix}, \\quad\nP_{k|k} = \\begin{pmatrix}\n0.25 & 0 & 0 \\\\\n0 & 0.36 & 0 \\\\\n0 & 0 & 0.16\n\\end{pmatrix}.\n$$\n- Heating input $u_{k} = 6.0$.\n\nStarting from the discrete-time linear state evolution law and the definitions of mean and covariance for random vectors, use the property that an affine transformation of a Gaussian random vector, added to an independent zero-mean Gaussian noise, is Gaussian. Derive the one-step prediction distribution for $x_{k+1}$ and then evaluate the predicted covariance element corresponding to the middle state (the $(2,2)$ element of the predicted covariance). Compute the predicted variance of the middle-state temperature and express the result in $\\mathrm{K}^{2}$. Round your final numerical answer to four significant figures.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions for a discrete-time linear state evolution model of a thermal network:\n- State evolution law: $x_{k+1} = A x_{k} + B u_{k} + w_{k}$\n- State vector: $x_{k} \\in \\mathbb{R}^{3}$ (absolute temperatures in Kelvin)\n- Input: $u_{k} \\in \\mathbb{R}$ (heating power in kilowatts)\n- Process noise: $w_{k} \\in \\mathbb{R}^{3}$, zero-mean, i.e., $\\mathbb{E}[w_{k}] = 0$\n- Statistical properties: $x_{k}$ and $w_{k}$ are jointly Gaussian and independent.\n- Prior filtering distribution at time $k$: $x_{k} \\sim \\mathcal{N}(\\hat{x}_{k|k}, P_{k|k})$\n- State transition matrix:\n$$\nA = \\begin{pmatrix}\n0.97 & 0.02 & 0.00 \\\\\n0.03 & 0.95 & 0.02 \\\\\n0.00 & 0.05 & 0.93\n\\end{pmatrix}\n$$\n- Input matrix:\n$$\nB = \\begin{pmatrix}\n0.15 \\\\\n0.10 \\\\\n0.02\n\\end{pmatrix}\n$$\n- Process noise covariance matrix:\n$$\nQ = \\begin{pmatrix}\n0.04 & 0 & 0 \\\\\n0 & 0.05 & 0 \\\\\n0 & 0 & 0.03\n\\end{pmatrix}\n$$\n- Prior mean vector:\n$$\n\\hat{x}_{k|k} = \\begin{pmatrix} 296.0 \\\\ 294.5 \\\\ 293.0 \\end{pmatrix}\n$$\n- Prior covariance matrix:\n$$\nP_{k|k} = \\begin{pmatrix}\n0.25 & 0 & 0 \\\\\n0 & 0.36 & 0 \\\\\n0 & 0 & 0.16\n\\end{pmatrix}\n$$\n- Control input at time $k$: $u_{k} = 6.0$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a state-space model for a thermal system, which is a standard and scientifically sound approach in energy systems modeling and control theory. The use of a Kalman filter's prediction step is a fundamental technique in state estimation.\n- **Well-Posed**: The problem is well-posed. It provides all necessary numerical values and a clear objective: to derive the one-step prediction distribution and compute a specific element of the predicted covariance matrix. A unique solution exists.\n- **Objective**: The problem is stated using precise, objective mathematical language.\n- The problem is self-contained and internally consistent. The matrices are of compatible dimensions for the operations described. The setup is a standard application of state estimation theory.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n## SOLUTION\nThe problem requires the derivation of the one-step prediction distribution for the state vector $x_{k+1}$ and the calculation of the $(2,2)$ element of the predicted state covariance matrix, $P_{k+1|k}$.\n\nThe state evolution is governed by the linear equation:\n$$x_{k+1} = A x_{k} + B u_{k} + w_{k}$$\nWe are given that the state $x_k$ at time $k$ is a Gaussian random vector with mean $\\hat{x}_{k|k}$ and covariance $P_{k|k}$, i.e., $x_{k} \\sim \\mathcal{N}(\\hat{x}_{k|k}, P_{k|k})$. The process noise $w_k$ is an independent, zero-mean Gaussian random vector with covariance $Q$, i.e., $w_{k} \\sim \\mathcal{N}(0, Q)$. The input $u_k$ is a deterministic scalar.\n\nThe one-step predicted state, denoted $x_{k+1|k}$, is the random variable $x_{k+1}$ conditioned on the information available up to time $k$. The problem asks for its distribution. Since $x_{k+1}$ is an affine transformation of the sum of two independent Gaussian vectors ($x_k$ and $w_k$), it is also a Gaussian random vector. Its distribution is fully characterized by its mean and covariance.\n\nFirst, we derive the predicted mean, $\\hat{x}_{k+1|k}$, which is the expected value of $x_{k+1}$:\n$$\\hat{x}_{k+1|k} = \\mathbb{E}[x_{k+1}] = \\mathbb{E}[A x_{k} + B u_{k} + w_{k}]$$\nBy the linearity of the expectation operator:\n$$\\hat{x}_{k+1|k} = \\mathbb{E}[A x_{k}] + \\mathbb{E}[B u_{k}] + \\mathbb{E}[w_{k}]$$\nGiven that $\\mathbb{E}[x_{k}] = \\hat{x}_{k|k}$, $u_k$ is a deterministic constant, and $\\mathbb{E}[w_{k}] = 0$:\n$$\\hat{x}_{k+1|k} = A \\mathbb{E}[x_{k}] + B u_{k} + 0 = A \\hat{x}_{k|k} + B u_{k}$$\n\nNext, we derive the predicted covariance, $P_{k+1|k}$. By definition, the covariance is:\n$$P_{k+1|k} = \\text{Cov}(x_{k+1}) = \\mathbb{E}[ (x_{k+1} - \\hat{x}_{k+1|k})(x_{k+1} - \\hat{x}_{k+1|k})^T ]$$\nSubstitute the expressions for $x_{k+1}$ and $\\hat{x}_{k+1|k}$:\n$$x_{k+1} - \\hat{x}_{k+1|k} = (A x_{k} + B u_{k} + w_{k}) - (A \\hat{x}_{k|k} + B u_{k}) = A(x_{k} - \\hat{x}_{k|k}) + w_{k}$$\nThe covariance becomes:\n$$P_{k+1|k} = \\mathbb{E}[ (A(x_{k} - \\hat{x}_{k|k}) + w_{k})(A(x_{k} - \\hatx_{k|k}) + w_{k})^T ]$$\nExpanding the product yields four terms. Taking the expectation of each:\n$$P_{k+1|k} = \\mathbb{E}[A(x_{k} - \\hat{x}_{k|k})(x_{k} - \\hat{x}_{k|k})^T A^T] + \\mathbb{E}[A(x_{k} - \\hat{x}_{k|k})w_{k}^T] + \\mathbb{E}[w_{k}(x_{k} - \\hat{x}_{k|k})^T A^T] + \\mathbb{E}[w_{k}w_{k}^T]$$\n- The first term is $A \\mathbb{E}[(x_{k} - \\hat{x}_{k|k})(x_{k} - \\hat{x}_{k|k})^T] A^T = A P_{k|k} A^T$.\n- The second and third terms are zero because $x_k$ and $w_k$ are independent, which implies that the error term $(x_k - \\hat{x}_{k|k})$ is independent of $w_k$. Since $\\mathbb{E}[w_k] = 0$, the expectation of the cross-product terms is zero.\n- The fourth term is $\\mathbb{E}[w_{k}w_{k}^T] = \\text{Cov}(w_k) = Q$, as $w_k$ is zero-mean.\nThus, the predicted covariance is:\n$$P_{k+1|k} = A P_{k|k} A^T + Q$$\n\nThe one-step prediction distribution for $x_{k+1}$ is therefore Gaussian:\n$$x_{k+1} \\sim \\mathcal{N}(A \\hat{x}_{k|k} + B u_{k}, A P_{k|k} A^T + Q)$$\n\nThe problem asks for the $(2,2)$ element of the predicted covariance matrix $P_{k+1|k}$. This is the predicted variance of the second state, which represents the internal thermal mass temperature. We denote this element as $(P_{k+1|k})_{22}$.\n$$(P_{k+1|k})_{22} = (A P_{k|k} A^T)_{22} + Q_{22}$$\nThe matrix $P_{k|k}$ is diagonal, which simplifies the calculation. Let $A_{2,*}$ denote the second row of matrix $A$. The term $(A P_{k|k} A^T)_{22}$ can be computed as $A_{2,*} P_{k|k} (A_{2,*})^T$.\n$$A_{2,*} = \\begin{pmatrix} 0.03 & 0.95 & 0.02 \\end{pmatrix}$$\n$$P_{k|k} = \\begin{pmatrix} 0.25 & 0 & 0 \\\\ 0 & 0.36 & 0 \\\\ 0 & 0 & 0.16 \\end{pmatrix}$$\nFirst, we compute the product $A_{2,*} P_{k|k}$:\n$$A_{2,*} P_{k|k} = \\begin{pmatrix} 0.03 & 0.95 & 0.02 \\end{pmatrix} \\begin{pmatrix} 0.25 & 0 & 0 \\\\ 0 & 0.36 & 0 \\\\ 0 & 0 & 0.16 \\end{pmatrix} = \\begin{pmatrix} 0.03 \\times 0.25 & 0.95 \\times 0.36 & 0.02 \\times 0.16 \\end{pmatrix}$$\n$$A_{2,*} P_{k|k} = \\begin{pmatrix} 0.0075 & 0.342 & 0.0032 \\end{pmatrix}$$\nNext, we compute the product with $(A_{2,*})^T$:\n$$(A P_{k|k} A^T)_{22} = \\begin{pmatrix} 0.0075 & 0.342 & 0.0032 \\end{pmatrix} \\begin{pmatrix} 0.03 \\\\ 0.95 \\\\ 0.02 \\end{pmatrix}$$\n$$(A P_{k|k} A^T)_{22} = (0.0075)(0.03) + (0.342)(0.95) + (0.0032)(0.02)$$\n$$(A P_{k|k} A^T)_{22} = 0.000225 + 0.3249 + 0.000064 = 0.325189$$\nNow, we add the $(2,2)$ element of the process noise covariance matrix $Q$:\n$$Q = \\begin{pmatrix} 0.04 & 0 & 0 \\\\ 0 & 0.05 & 0 \\\\ 0 & 0 & 0.03 \\end{pmatrix} \\implies Q_{22} = 0.05$$\nSo, the predicted variance of the middle state is:\n$$(P_{k+1|k})_{22} = 0.325189 + 0.05 = 0.375189$$\nThe problem requires the result to be rounded to four significant figures.\n$$(P_{k+1|k})_{22} \\approx 0.3752 \\, \\mathrm{K}^2$$",
            "answer": "$$\n\\boxed{0.3752}\n$$"
        },
        {
            "introduction": "After predicting the state, the filter performs an \"update\" to refine its estimate using new data. This practice demonstrates the optimal fusion of a prior estimate with a new measurement, a process governed by the Kalman gain. By working through a clear, scalar example from a microgrid , you will gain an intuitive understanding of how the filter produces a more accurate posterior estimate with reduced uncertainty.",
            "id": "4124641",
            "problem": "An islanded microgrid features a single dominant bus whose voltage phase angle evolves approximately as a random walk due to stochastic power balance fluctuations and inverter droop control. Let the scalar state $x_{k}$ denote the bus voltage phase angle in radians at discrete time index $k$. The process model and measurement model are specified by the linear time-invariant state-space system\n$$\nx_{k+1} = A x_{k} + w_{k}, \\quad z_{k} = C x_{k} + v_{k},\n$$\nwhere $w_{k}$ and $v_{k}$ are mutually independent, zero-mean Gaussian sequences with covariances $Q$ and $R$, respectively. A Phasor Measurement Unit (PMU) provides a direct measurement of the angle.\n\nFor a single measurement update at time $k$, you are given the following numerically specified parameters and prior quantities:\n- State transition coefficient $A = 1$,\n- Measurement coefficient $C = 1$,\n- Process noise covariance $Q = 2.5 \\times 10^{-4} \\text{ rad}^{2}$,\n- Measurement noise covariance $R = 7.5 \\times 10^{-3} \\text{ rad}^{2}$,\n- Prior (predicted) mean $\\hat{x}_{k|k-1} = 0.12 \\text{ rad}$,\n- Prior (predicted) covariance $P_{k|k-1} = 0.0102 \\text{ rad}^{2}$,\n- PMU measurement $z_{k} = 0.155 \\text{ rad}$.\n\nStarting from first principles for linear Gaussian state estimation, derive the measurement update for this scalar case and compute the Kalman gain $K_{k}$, the posterior (updated) state estimate $\\hat{x}_{k|k}$ in radians, and the posterior (updated) covariance $P_{k|k}$ in squared radians for this single measurement update step. Round your final numerical values to four significant figures. Express the state estimate in radians and the covariance in squared radians. Provide your final three values in the order $K_{k}$, $\\hat{x}_{k|k}$, $P_{k|k}$.",
            "solution": "The user has provided a problem statement for validation and solution.\n\n### Step 1: Extract Givens\nThe verbatim givens extracted from the problem statement are:\n- State-space model: $x_{k+1} = A x_{k} + w_{k}$, $z_{k} = C x_{k} + v_{k}$\n- Noise characteristics: $w_{k}$ and $v_{k}$ are mutually independent, zero-mean Gaussian sequences with covariances $Q$ and $R$, respectively.\n- State transition coefficient: $A = 1$\n- Measurement coefficient: $C = 1$\n- Process noise covariance: $Q = 2.5 \\times 10^{-4} \\text{ rad}^{2}$\n- Measurement noise covariance: $R = 7.5 \\times 10^{-3} \\text{ rad}^{2}$\n- Prior (predicted) mean: $\\hat{x}_{k|k-1} = 0.12 \\text{ rad}$\n- Prior (predicted) covariance: $P_{k|k-1} = 0.0102 \\text{ rad}^{2}$\n- PMU measurement: $z_{k} = 0.155 \\text{ rad}$\n- Required outputs: Kalman gain $K_{k}$, posterior state estimate $\\hat{x}_{k|k}$, posterior covariance $P_{k|k}$.\n- Rounding requirement: Final numerical values rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the established criteria.\n\n1.  **Scientifically Grounded**: The problem describes a standard application of a Kalman filter for state estimation of a bus voltage phase angle in a microgrid. The random walk process model ($A=1$) is a valid and common simplification for systems subject to stochastic drift. The use of a Phasor Measurement Unit (PMU) is standard technology for this purpose. The problem is firmly grounded in control theory and power systems engineering.\n2.  **Well-Posed**: The problem provides all necessary numerical values and a complete description of the state-space model required to perform a single measurement update step of the Kalman filter. A unique solution for the requested quantities ($K_{k}$, $\\hat{x}_{k|k}$, $P_{k|k}$) exists and is stable.\n3.  **Objective**: The problem is stated in precise, technical language with no subjective or ambiguous terms. All parameters are defined numerically.\n4.  **Completeness and Consistency**: The problem is self-contained. The given data are dimensionally consistent (angles in radians, covariances in radians squared). There are no contradictions.\n5.  **Plausibility**: The numerical values for the covariances, state, and measurement are of a realistic magnitude for the described physical system.\n\nThe problem statement has no identifiable flaws. It is a valid, well-posed, and scientifically sound problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation and Calculation\nWe seek to compute the measurement update for a scalar linear Gaussian system. The state $x_k$ is a random variable. The prior knowledge about $x_k$ at time $k$, before incorporating the measurement $z_k$, is captured by the prior probability distribution $p(x_k | z_{1:k-1})$. For a Kalman filter, this is a Gaussian distribution with mean $\\hat{x}_{k|k-1}$ and variance $P_{k|k-1}$:\n$$x_k \\sim \\mathcal{N}(\\hat{x}_{k|k-1}, P_{k|k-1})$$\nThe measurement $z_k$ provides new information. The relationship $z_k = C x_k + v_k$ where $v_k \\sim \\mathcal{N}(0, R)$ defines the likelihood of observing $z_k$ given the state $x_k$. This likelihood function $p(z_k | x_k)$ is a Gaussian distribution centered at $C x_k$ with variance $R$:\n$$p(z_k|x_k) \\sim \\mathcal{N}(C x_k, R)$$\nAccording to Bayes' theorem, the posterior distribution $p(x_k | z_k, z_{1:k-1}) \\equiv p(x_k|z_{1:k})$ is proportional to the product of the likelihood and the prior:\n$$p(x_k|z_{1:k}) \\propto p(z_k|x_k) p(x_k|z_{1:k-1})$$\nSince the product of two Gaussian distributions is another Gaussian distribution, the posterior is also Gaussian, characterized by its mean $\\hat{x}_{k|k}$ and variance $P_{k|k}$. The expressions for the posterior mean and variance are the standard Kalman filter measurement update equations. For the scalar case, these are:\n\n1.  **Kalman Gain ($K_k$):** The gain determines how much weight is given to the new measurement.\n    $$K_k = \\frac{P_{k|k-1} C}{C^2 P_{k|k-1} + R}$$\n2.  **Posterior State Estimate ($\\hat{x}_{k|k}$):** The updated state estimate is a linear combination of the prior estimate and the measurement innovation.\n    $$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (z_k - C \\hat{x}_{k|k-1})$$\n3.  **Posterior Covariance ($P_{k|k}$):** The updated covariance reflects the reduction in uncertainty after the measurement.\n    $$P_{k|k} = (1 - K_k C) P_{k|k-1}$$\n\nWe now substitute the given numerical values. The problem specifies $C=1$, which simplifies the equations.\n\n**Given values:**\n- Prior mean: $\\hat{x}_{k|k-1} = 0.12$\n- Prior covariance: $P_{k|k-1} = 0.0102$\n- Measurement: $z_k = 0.155$\n- Measurement noise covariance: $R = 7.5 \\times 10^{-3} = 0.0075$\n- Measurement coefficient: $C = 1$\n\n**Calculation of Kalman Gain ($K_k$):**\nUsing the simplified formula for $C=1$:\n$$K_k = \\frac{P_{k|k-1}}{P_{k|k-1} + R}$$\n$$K_k = \\frac{0.0102}{0.0102 + 0.0075} = \\frac{0.0102}{0.0177} \\approx 0.576271186$$\nRounding to four significant figures, the Kalman gain is $K_k = 0.5763$. The gain is a dimensionless quantity.\n\n**Calculation of Posterior State Estimate ($\\hat{x}_{k|k}$):**\nFirst, we compute the measurement innovation, which is the difference between the actual measurement and the predicted measurement ($C\\hat{x}_{k|k-1}$):\n$$y_k = z_k - C \\hat{x}_{k|k-1} = 0.155 - (1)(0.12) = 0.035 \\text{ rad}$$\nNow, we update the state estimate:\n$$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k y_k$$\n$$\\hat{x}_{k|k} = 0.12 + (0.576271186)(0.035) = 0.12 + 0.0201694915 = 0.1401694915 \\text{ rad}$$\nRounding to four significant figures, the posterior state estimate is $\\hat{x}_{k|k} = 0.1402$ rad.\n\n**Calculation of Posterior Covariance ($P_{k|k}$):**\nUsing the simplified formula for $C=1$:\n$$P_{k|k} = (1 - K_k) P_{k|k-1}$$\n$$P_{k|k} = (1 - 0.576271186)(0.0102) = (0.423728814)(0.0102) = 0.0043220339 \\text{ rad}^2$$\nRounding to four significant figures, the posterior covariance is $P_{k|k} = 0.004322$ rad$^2$. This value represents a significant reduction from the prior covariance of $0.0102$ rad$^2$, indicating that the measurement was informative.\n\nThe final numerical results for the three requested quantities, rounded to four significant figures, are:\n- $K_k = 0.5763$\n- $\\hat{x}_{k|k} = 0.1402$ rad\n- $P_{k|k} = 0.004322$ rad$^2$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.5763 & 0.1402 & 0.004322\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Most real-world energy systems are nonlinear, requiring advanced estimation techniques beyond the standard Kalman filter. This exercise compares the popular Extended Kalman Filter (EKF) with the Unscented Kalman Filter (UKF) to highlight the impact of linearization on estimation accuracy. By analyzing a system with significant measurement curvature , you will quantify the bias introduced by the EKF's approximation and understand the advantages of the UKF's statistical approach.",
            "id": "4124654",
            "problem": "A distribution feeder bus voltage magnitude $x$ (in per unit) is to be estimated from a nonlinear power injection measurement with significant curvature. In a simplified but scientifically consistent constant-conductance approximation, the real power injection $y$ obeys the nonlinear measurement model $y = h(x) + v$, where $h(x) = G x^{2}$, the conductance $G$ is known and positive, and the additive measurement noise $v$ is zero-mean Gaussian with variance $R$. The prior state is modeled as Gaussian $x \\sim \\mathcal{N}(m, P)$, representing the uncertainty in the bus voltage before assimilating the measurement. Consider the case $G = 1$, $m = 1.0$, $P = 0.04$, and $R = 0.01$.\n\nYou will compare the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF) by analyzing the bias in the posterior mean induced by the curvature of the measurement function. Specifically, define the measurement to be the prior expected value under the true nonlinear model, $y := \\mathbb{E}[h(x)]$, so that, in an information-theoretic sense, the measurement is centered at the prior expectation. Starting from the core definitions of Gaussian moments and the standard update structures for the EKF and UKF (with the Unscented Transform applied in the measurement update), derive, from first principles, how the measurement curvature affects the innovation and the posterior mean in each filter.\n\nThen, compute the posterior mean updates produced by the EKF and the UKF for the given numerical values, and define the bias as the magnitude of the shift of the posterior mean away from the prior mean under this centered measurement, i.e., $|m^{+} - m|$. Report the EKF bias magnitude. Round your answer to four significant figures and express it in per unit (pu).",
            "solution": "The problem asks for an analysis of the bias induced by the Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF) for a specific nonlinear state estimation problem, and to compute the EKF bias magnitude.\n\nFirst, we establish the components of the state estimation problem as given.\nThe state is the bus voltage magnitude, a scalar $x$.\nThe prior knowledge of the state is modeled by a Gaussian distribution: $x \\sim \\mathcal{N}(m, P)$, with prior mean $m=1.0$ and prior variance $P=0.04$.\nThe measurement model is $y = h(x) + v$, where the measurement function is $h(x) = Gx^2$. We are given $G=1$, so $h(x) = x^2$.\nThe measurement noise $v$ is a zero-mean Gaussian random variable with variance $R$, so $v \\sim \\mathcal{N}(0, R)$, with $R=0.01$.\n\nA critical part of the problem setup is the definition of the specific measurement value to be used. The measurement $y$ is set to the prior expected value of the true nonlinear function, $h(x)$. We calculate this value first.\nThe expectation of $h(x) = x^2$ for a random variable $x$ with mean $m$ and variance $P$ is given by the property $\\mathbb{E}[x^2] = (\\mathbb{E}[x])^2 + \\text{Var}(x)$.\nTherefore, the measurement value is:\n$$y := \\mathbb{E}[h(x)] = \\mathbb{E}[x^2] = m^2 + P$$\nSubstituting the given numerical values, $m=1.0$ and $P=0.04$:\n$$y = (1.0)^2 + 0.04 = 1.0 + 0.04 = 1.04$$\n\nNext, we derive the posterior mean update for the EKF and analyze its bias. The EKF linearizes the nonlinear measurement function $h(x)$ around the prior mean $m$. The linearized function is:\n$$h(x) \\approx h(m) + H(x-m)$$\nwhere $H$ is the Jacobian of $h(x)$ evaluated at $x=m$. For our scalar case, this is the derivative:\n$$H = \\frac{dh}{dx}\\bigg|_{x=m} = \\frac{d(x^2)}{dx}\\bigg|_{x=m} = 2x\\bigg|_{x=m} = 2m$$\nThe EKF uses this linearized model to predict the measurement. The predicted measurement, denoted $\\hat{y}_{EKF}$, is the expectation of the linearized function:\n$$\\hat{y}_{EKF} = \\mathbb{E}[h(m) + H(x-m)] = h(m) + H(\\mathbb{E}[x]-m) = h(m) + H(m-m) = h(m) = m^2$$\nThe EKF innovation is the difference between the actual measurement $y$ and the filter's predicted measurement $\\hat{y}_{EKF}$:\n$$\\nu_{EKF} = y - \\hat{y}_{EKF} = (m^2 + P) - m^2 = P$$\nThis non-zero innovation arises entirely from the EKF's approximation error. The filter's first-order approximation of the expected measurement, $m^2$, differs from the true expected measurement, $m^2+P$, by a term $P$, which is related to the curvature of $h(x)$.\n\nThe standard Kalman filter update equations proceed with this innovation. The innovation covariance is:\n$$S_{EKF} = H P H^T + R$$\nSubstituting $H=2m$:\n$$S_{EKF} = (2m) P (2m) + R = 4m^2P + R$$\nThe Kalman gain is:\n$$K_{EKF} = P H^T S_{EKF}^{-1} = P(2m)(4m^2P + R)^{-1} = \\frac{2mP}{4m^2P+R}$$\nThe posterior mean, $m_{EKF}^{+}$, is updated from the prior mean $m$ as follows:\n$$m_{EKF}^{+} = m + K_{EKF} \\nu_{EKF} = m + \\left(\\frac{2mP}{4m^2P+R}\\right) P = m + \\frac{2mP^2}{4m^2P+R}$$\nThe bias, defined as the shift of the posterior mean from the prior mean, is the magnitude of the correction term:\n$$|m_{EKF}^{+} - m| = \\left|\\frac{2mP^2}{4m^2P+R}\\right|$$\nSince $m>0$, $P>0$, and $R>0$, the absolute value is redundant.\n\nFor comparison, we briefly analyze the UKF. The Unscented Transform (UT) used by the UKF propagates a set of deterministically chosen sigma points through the true nonlinear function $h(x)=x^2$ to estimate the moments of the output distribution. For a quadratic function and a Gaussian input, the UT computes the exact mean of the transformed variable. Therefore, the UKF's predicted measurement is:\n$$\\hat{y}_{UKF} = \\mathbb{E}[h(x)] = m^2+P$$\nThe UKF innovation is then:\n$$\\nu_{UKF} = y - \\hat{y}_{UKF} = (m^2+P) - (m^2+P) = 0$$\nSince the innovation is zero, the correction term is zero, and the posterior mean is not updated:\n$$m_{UKF}^{+} = m + K_{UKF} \\nu_{UKF} = m + 0 = m$$\nThus, the UKF correctly identifies that the measurement conveys no new information relative to the prior and exhibits zero bias in this specific scenario, i.e., $|m_{UKF}^{+} - m| = 0$.\n\nThe problem requires the computation of the EKF bias magnitude. We substitute the given numerical values into the derived expression: $m=1.0$, $P=0.04$, and $R=0.01$.\n$$|m_{EKF}^{+} - m| = \\frac{2(1.0)(0.04)^2}{4(1.0)^2(0.04) + 0.01}$$\n$$|m_{EKF}^{+} - m| = \\frac{2(0.0016)}{4(0.04) + 0.01} = \\frac{0.0032}{0.16 + 0.01} = \\frac{0.0032}{0.17}$$\n$$|m_{EKF}^{+} - m| \\approx 0.018823529...$$\nRounding the result to four significant figures gives $0.01882$. The units are per unit (pu).",
            "answer": "$$\n\\boxed{0.01882}\n$$"
        }
    ]
}