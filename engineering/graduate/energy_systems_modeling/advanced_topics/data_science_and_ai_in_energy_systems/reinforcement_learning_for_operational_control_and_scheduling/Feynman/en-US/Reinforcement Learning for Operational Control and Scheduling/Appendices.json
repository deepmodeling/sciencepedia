{
    "hands_on_practices": [
        {
            "introduction": "To build robust reinforcement learning solutions, we must first master the fundamentals of sequential decision-making. This exercise guides you through solving a small-scale Markov Decision Process (MDP) for a generator scheduling problem from first principles. By applying backward induction, you will construct the value functions for each time step, revealing the core logic of the principle of optimality that underpins all RL algorithms .",
            "id": "4115663",
            "problem": "Consider a finite-horizon Markov Decision Process (MDP) for energy systems operational scheduling within reinforcement learning (RL), defined as follows. Time is indexed by $t \\in \\{1,2\\}$, giving a horizon of $2$ periods with discount factor $\\gamma = 1$. The state at time $t$ is the binary demand $d_t \\in \\{0,1\\}$, observed before an action is chosen. The action is the binary generator dispatch $a_t \\in \\{0,1\\}$, where $a_t = 1$ denotes turning the generator on. The single-period cost is\n$$ \\ell(d_t,a_t) = 5 a_t + 10 \\max(0, d_t - a_t), $$\nwhich represents an operating cost when the generator is on and a shortage penalty if demand is unmet. Demand evolves exogenously as a time-homogeneous, first-order Markov chain with\n$$ \\mathbb{P}(d_{t+1} = 1 \\mid d_t = 1) = \\alpha, \\quad \\mathbb{P}(d_{t+1} = 1 \\mid d_t = 0) = \\beta, $$\nand complementary probabilities for $d_{t+1} = 0$. The action $a_t$ does not affect demand transitions. The initial demand distribution is\n$$ \\mathbb{P}(d_1 = 1) = \\pi, \\quad \\mathbb{P}(d_1 = 0) = 1 - \\pi, $$\nwith parameters $\\alpha, \\beta, \\pi \\in [0,1]$.\n\nStarting from the principle of optimality and the definition of the finite-horizon MDP value functions, derive the optimal policy and compute the optimal expected total cost over the two periods. Explicitly construct the state-action value functions $Q_t(d_t,a_t)$ and the value functions $V_t(d_t)$ for $t = 2$ and $t = 1$, clearly indicating the dependence on the demand transition probabilities. Then marginalize over the initial distribution to obtain the expected optimal total cost as a closed-form analytic expression in terms of $\\pi$, $\\alpha$, and $\\beta$.\n\nProvide the final answer as a single closed-form analytic expression for the optimal expected total cost. No rounding is required.",
            "solution": "The problem describes a finite-horizon Markov Decision Process (MDP) and asks for the optimal policy and the optimal expected total cost. We will solve this using backward induction, a standard dynamic programming technique, starting from the final time period $t=2$. The principle of optimality states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n\nThe Bellman equations for a finite-horizon cost-minimization problem are:\nFor the final time step $T$, the value function is determined by a terminal cost, which is zero in this problem: $V_T(s_T)=0$. In our case, $T=3$ conceptually (end of period 2), so $V_3(d_3)=0$.\nFor any time $t  T$, the state-action value function $Q_t(s_t, a_t)$ and the value function $V_t(s_t)$ are given by:\n$$ Q_t(s_t, a_t) = \\ell(s_t, a_t) + \\gamma \\sum_{s_{t+1}} \\mathbb{P}(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1}) $$\n$$ V_t(s_t) = \\min_{a_t} Q_t(s_t, a_t) $$\nThe optimal action $a_t^*(s_t)$ is the action that achieves this minimum.\n\nGiven parameters are: horizon $t \\in \\{1, 2\\}$, discount factor $\\gamma=1$, state $d_t \\in \\{0, 1\\}$, action $a_t \\in \\{0, 1\\}$, cost function $\\ell(d_t, a_t) = 5 a_t + 10 \\max(0, d_t - a_t)$, and transition probabilities $\\mathbb{P}(d_{t+1}=1|d_t=1) = \\alpha$ and $\\mathbb{P}(d_{t+1}=1|d_t=0) = \\beta$.\n\n**Step 1: Analysis for the final period, $t=2$**\n\nAt the end of the horizon, the value function for the next state, $V_3(d_3)$, is $0$ for all states $d_3$. The state-action value function at $t=2$ is:\n$$ Q_2(d_2, a_2) = \\ell(d_2, a_2) + \\gamma \\mathbb{E}[V_3(d_3) | d_2, a_2] = \\ell(d_2, a_2) $$\nWe compute the cost $\\ell(d_2, a_2)$ for all possible state-action pairs:\n- If demand $d_2=0$ and action $a_2=0$: $Q_2(0,0) = 5(0) + 10 \\max(0, 0-0) = 0$.\n- If demand $d_2=0$ and action $a_2=1$: $Q_2(0,1) = 5(1) + 10 \\max(0, 0-1) = 5$.\n- If demand $d_2=1$ and action $a_2=0$: $Q_2(1,0) = 5(0) + 10 \\max(0, 1-0) = 10$.\n- If demand $d_2=1$ and action $a_2=1$: $Q_2(1,1) = 5(1) + 10 \\max(0, 1-1) = 5$.\n\nThe optimal value function at $t=2$, $V_2(d_2)$, is found by minimizing $Q_2(d_2, a_2)$ over the action $a_2$.\n- For state $d_2=0$:\n  $$ V_2(0) = \\min\\{Q_2(0,0), Q_2(0,1)\\} = \\min\\{0, 5\\} = 0 $$\n  The optimal action is $a_2^*(0)=0$.\n- For state $d_2=1$:\n  $$ V_2(1) = \\min\\{Q_2(1,0), Q_2(1,1)\\} = \\min\\{10, 5\\} = 5 $$\n  The optimal action is $a_2^*(1)=1$.\n\nSo, the optimal policy at $t=2$ is to match the dispatch to the demand, i.e., $a_2^*(d_2) = d_2$.\n\n**Step 2: Analysis for the first period, $t=1$**\n\nWe now step back to $t=1$. The state-action value function is:\n$$ Q_1(d_1, a_1) = \\ell(d_1, a_1) + \\gamma \\mathbb{E}[V_2(d_2) | d_1, a_1] $$\nSince $\\gamma=1$ and the demand evolution is independent of the action $a_1$, this simplifies to:\n$$ Q_1(d_1, a_1) = \\ell(d_1, a_1) + \\mathbb{E}[V_2(d_2) | d_1] $$\nFirst, we compute the expected future cost, $\\mathbb{E}[V_2(d_2) | d_1]$, for each possible state $d_1$:\n- For state $d_1=0$:\n  $$ \\mathbb{E}[V_2(d_2) | d_1=0] = \\mathbb{P}(d_2=0 | d_1=0) V_2(0) + \\mathbb{P}(d_2=1 | d_1=0) V_2(1) $$\n  $$ = (1-\\beta)(0) + (\\beta)(5) = 5\\beta $$\n- For state $d_1=1$:\n  $$ \\mathbb{E}[V_2(d_2) | d_1=1] = \\mathbb{P}(d_2=0 | d_1=1) V_2(0) + \\mathbb{P}(d_2=1 | d_1=1) V_2(1) $$\n  $$ = (1-\\alpha)(0) + (\\alpha)(5) = 5\\alpha $$\n\nNow we can compute $Q_1(d_1, a_1)$ for all state-action pairs:\n- If $d_1=0, a_1=0$: $Q_1(0,0) = \\ell(0,0) + 5\\beta = 0 + 5\\beta = 5\\beta$.\n- If $d_1=0, a_1=1$: $Q_1(0,1) = \\ell(0,1) + 5\\beta = 5 + 5\\beta$.\n- If $d_1=1, a_1=0$: $Q_1(1,0) = \\ell(1,0) + 5\\alpha = 10 + 5\\alpha$.\n- If $d_1=1, a_1=1$: $Q_1(1,1) = \\ell(1,1) + 5\\alpha = 5 + 5\\alpha$.\n\nThe optimal value function at $t=1$, $V_1(d_1)$, is found by minimizing $Q_1(d_1, a_1)$ over the action $a_1$.\n- For state $d_1=0$:\n  $$ V_1(0) = \\min\\{Q_1(0,0), Q_1(0,1)\\} = \\min\\{5\\beta, 5+5\\beta\\} = 5\\beta $$\n  Since $\\beta \\in [0,1]$, $5\\beta \\ge 0$, so $5\\beta$ is always less than or equal to $5+5\\beta$. The optimal action is $a_1^*(0)=0$.\n- For state $d_1=1$:\n  $$ V_1(1) = \\min\\{Q_1(1,0), Q_1(1,1)\\} = \\min\\{10+5\\alpha, 5+5\\alpha\\} = 5+5\\alpha $$\n  Since $\\alpha \\in [0,1]$, $5\\alpha \\ge 0$, so $5+5\\alpha$ is always less than or equal to $10+5\\alpha$. The optimal action is $a_1^*(1)=1$.\n\nThe optimal policy at $t=1$ is also to match the dispatch to the demand, i.e., $a_1^*(d_1) = d_1$.\n\n**Step 3: Optimal Expected Total Cost**\n\nThe optimal expected total cost is the expected value of $V_1(d_1)$ over the initial distribution of $d_1$. The initial distribution is given by $\\mathbb{P}(d_1=1)=\\pi$ and $\\mathbb{P}(d_1=0)=1-\\pi$.\n$$ \\mathbb{E}[V_1(d_1)] = \\mathbb{P}(d_1=0) V_1(0) + \\mathbb{P}(d_1=1) V_1(1) $$\nSubstituting the values we found for $V_1(0)$ and $V_1(1)$:\n$$ \\mathbb{E}[V_1(d_1)] = (1-\\pi)(5\\beta) + (\\pi)(5+5\\alpha) $$\nExpanding and rearranging the terms:\n$$ \\mathbb{E}[V_1(d_1)] = 5\\beta - 5\\pi\\beta + 5\\pi + 5\\pi\\alpha $$\nFactoring the expression to present it in a closed form:\n$$ \\mathbb{E}[V_1(d_1)] = 5\\pi + 5\\pi\\alpha - 5\\pi\\beta + 5\\beta $$\n$$ \\mathbb{E}[V_1(d_1)] = 5\\pi(1 + \\alpha - \\beta) + 5\\beta $$\nThis is the final analytical expression for the optimal expected total cost over the two periods.",
            "answer": "$$ \\boxed{5\\pi(1 + \\alpha - \\beta) + 5\\beta} $$"
        },
        {
            "introduction": "This practice bridges the gap between abstract MDPs and the intricate reality of energy system operations. You will tackle a unit commitment problem, which involves not just simple costs but also a web of realistic constraints like minimum up/down times and ramp rates. By calculating the state-action value, or $Q$-value, for different commitment decisions, you will see how the one-step Bellman backup is applied in a complex, practical setting that combines RL concepts with classical economic dispatch principles .",
            "id": "4115638",
            "problem": "Consider an operational control and scheduling problem for thermal generation in energy systems modeling, framed as a Markov Decision Process (MDP) for Reinforcement Learning (RL). The MDP state comprises unit on/off statuses, current outputs, and remaining minimum up/down time obligations. The action is a commitment decision (on/off) for each unit for the next period, which then induces a deterministic dispatch by solving an economic dispatch subproblem to meet demand subject to unit limits and ramp constraints. Immediate cost is defined as the sum of deterministic operating costs and physically motivated penalties. The RL return is defined as a discounted sum of costs over the horizon, with discount factor $\\gamma$.\n\nThere are three thermal units, labeled $\\text{U}1$, $\\text{U}2$, and $\\text{U}3$, and a horizon of $3$ periods of length $1$ hour each. You will compute a one-step lookahead at the start of period $1$ given a deterministic model and a provided value function for the post-decision next state. Use the following fundamentals and data:\n\n- The action at the start of period $1$ is a binary commitment vector $(a_1,a_2,a_3)$ with $a_i \\in \\{0,1\\}$, where $a_i = 1$ means unit $\\text{U}i$ is committed on in period $1$.\n- Dispatch is determined by minimizing variable generation cost subject to power balance and unit constraints, given the chosen commitment. The dispatch subproblem is linear: allocate generation to the committed units to satisfy power balance if feasible; otherwise the deviation from demand incurs a penalty. When multiple committed units are available, lower marginal cost units are loaded first, subject to ramp and capacity limits.\n- For each committed unit $\\text{U}i$, if $a_i = 1$, the decision variable $P_i$ must satisfy $P_i \\in [P_i^{\\min}, P_i^{\\max}]$ and the ramp constraint $|P_i - P_i^{0}| \\le r_i$, where $P_i^{0}$ is the previous output and $r_i$ is the ramp limit. If $a_i = 0$, then $P_i = 0$ and the shutdown ramp change from $P_i^{0}$ to $0$ still counts toward the ramp constraint with the same penalty scheme described below.\n- Immediate period cost includes:\n  - No-load cost for each committed unit: $C_i^{\\text{nl}}$ if $a_i = 1$.\n  - Variable generation cost: $c_i P_i$ for each committed unit.\n  - Startup cost: $C_i^{\\text{su}}$ for each unit transitioning from off to on.\n  - Energy balance penalty: $\\kappa_{\\text{bal}} \\, |D - \\sum_{i} P_i|$, where $D$ is demand.\n  - Ramp violation penalty: $\\kappa_{\\text{r}}$ times the total magnitude beyond the ramp limit for units whose absolute change $|P_i - P_i^{0}|$ exceeds $r_i$. Excess change is defined as $\\max\\{0, |P_i - P_i^{0}| - r_i\\}$ and is penalized linearly.\n\nInitial state at the start of period $1$:\n- Demand $D = 120$ MW.\n- Unit data:\n  - $\\text{U}1$: $P_1^{\\min} = 30$ MW, $P_1^{\\max} = 100$ MW, $c_1 = \\$18/\\text{MWh}$, $C_1^{\\text{nl}} = \\$200/\\text{h}$, $C_1^{\\text{su}} = \\$400$, $r_1 = 40$ MW, $P_1^{0} = 40$ MW, currently on, residual minimum up/down time obligation equals $0$ periods.\n  - $\\text{U}2$: $P_2^{\\min} = 50$ MW, $P_2^{\\max} = 150$ MW, $c_2 = \\$16/\\text{MWh}$, $C_2^{\\text{nl}} = \\$300/\\text{h}$, $C_2^{\\text{su}} = \\$600$, $r_2 = 50$ MW, $P_2^{0} = 0$ MW, currently off, residual minimum down time obligation equals $1$ period (cannot be turned on in period $1$).\n  - $\\text{U}3$: $P_3^{\\min} = 20$ MW, $P_3^{\\max} = 80$ MW, $c_3 = \\$22/\\text{MWh}$, $C_3^{\\text{nl}} = \\$150/\\text{h}$, $C_3^{\\text{su}} = \\$300$, $r_3 = 30$ MW, $P_3^{0} = 50$ MW, currently on, residual minimum up/down time obligation equals $0$ periods.\n- Penalty coefficients: $\\kappa_{\\text{bal}} = \\$1000/\\text{MWh}$, $\\kappa_{\\text{r}} = \\$50/\\text{MWh}$.\n- Discount factor: $\\gamma = 0.95$.\n- The post-decision next-state value function estimates for the states induced by each feasible action are given as follows: for action yielding next state $s'$, $V(s')$ equals:\n  - If $(a_1,a_2,a_3) = (0,0,0)$ then $V(s') = \\$10{,}000$.\n  - If $(a_1,a_2,a_3) = (1,0,0)$ then $V(s') = \\$4{,}000$.\n  - If $(a_1,a_2,a_3) = (0,0,1)$ then $V(s') = \\$3{,}500$.\n  - If $(a_1,a_2,a_3) = (1,0,1)$ then $V(s') = \\$2{,}100$.\n\nTasks:\n1. Enumerate the feasible commitment actions at the start of period $1$ consistent with the residual minimum up/down time obligations.\n2. For each feasible action, compute the economically dispatched outputs that minimize variable generation cost subject to $P_i^{\\min}$, $P_i^{\\max}$, and $|P_i - P_i^{0}| \\le r_i$. If exact demand satisfaction is infeasible, compute the energy balance penalty using the absolute deviation from demand. Compute the full immediate cost including no-load cost, variable generation cost, startup cost, energy balance penalty, and ramp violation penalty as defined.\n3. Using the Reinforcement Learning definition of the state-action value in an MDP, compute the one-step lookahead $Q(s_0,a)$ for each feasible action $a$ at the start-state $s_0$ by $Q(s_0,a) = c(s_0,a) + \\gamma V(s')$, where $c(s_0,a)$ is the immediate cost you computed and $V(s')$ is provided above for the induced next state $s'$.\n4. Return the minimal value of $Q(s_0,a)$ across the feasible actions. Round your answer to four significant figures and express the final cost in United States Dollars (USD).",
            "solution": "The problem will first be validated against the specified criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n\n- **Problem Framework:** Markov Decision Process (MDP) for Reinforcement Learning (RL) in operational control and scheduling of thermal generation.\n- **Horizon:** $3$ periods of length $1$ hour each.\n- **Task:** Compute a one-step lookahead at the start of period $1$.\n- **Action:** Commitment vector $(a_1, a_2, a_3)$ with $a_i \\in \\{0,1\\}$. $a_i = 1$ means unit $\\text{U}i$ is committed on.\n- **Dispatch Subproblem:** Minimize variable generation cost. Committed units are loaded in order of increasing marginal cost ($c_i$).\n- **Unit Constraints:** For a committed unit $\\text{U}i$ ($a_i = 1$), its output $P_i$ must satisfy $P_i \\in [P_i^{\\min}, P_i^{\\max}]$ and $|P_i - P_i^{0}| \\le r_i$. If $a_i = 0$, then $P_i = 0$.\n- **Immediate Cost Components:**\n    - No-load cost: $C_i^{\\text{nl}}$ if $a_i = 1$.\n    - Variable generation cost: $c_i P_i$.\n    - Startup cost: $C_i^{\\text{su}}$ for off-to-on transition.\n    - Energy balance penalty: $\\kappa_{\\text{bal}} \\, |D - \\sum_{i} P_i|$.\n    - Ramp violation penalty: $\\kappa_{\\text{r}} \\times \\sum_i \\max\\{0, |P_i - P_i^{0}| - r_i\\}$.\n- **Initial State (Start of Period 1):**\n    - Demand: $D = 120$ MW.\n    - Unit $\\text{U}1$: $P_1^{\\min} = 30$ MW, $P_1^{\\max} = 100$ MW, $c_1 = \\$18/\\text{MWh}$, $C_1^{\\text{nl}} = \\$200/\\text{h}$, $C_1^{\\text{su}} = \\$400$, $r_1 = 40$ MW, $P_1^{0} = 40$ MW, currently on, min up/down time residual $= 0$.\n    - Unit $\\text{U}2$: $P_2^{\\min} = 50$ MW, $P_2^{\\max} = 150$ MW, $c_2 = \\$16/\\text{MWh}$, $C_2^{\\text{nl}} = \\$300/\\text{h}$, $C_2^{\\text{su}} = \\$600$, $r_2 = 50$ MW, $P_2^{0} = 0$ MW, currently off, min down time residual $= 1$ period.\n    - Unit $\\text{U}3$: $P_3^{\\min} = 20$ MW, $P_3^{\\max} = 80$ MW, $c_3 = \\$22/\\text{MWh}$, $C_3^{\\text{nl}} = \\$150/\\text{h}$, $C_3^{\\text{su}} = \\$300$, $r_3 = 30$ MW, $P_3^{0} = 50$ MW, currently on, min up/down time residual $= 0$.\n- **Penalty Coefficients:** $\\kappa_{\\text{bal}} = \\$1000/\\text{MWh}$, $\\kappa_{\\text{r}} = \\$50/\\text{MWh}$.\n- **RL Parameters:**\n    - Discount factor: $\\gamma = 0.95$.\n    - Post-decision next-state value function $V(s')$:\n        - For action $(0,0,0)$, $V(s') = \\$10,000$.\n        - For action $(1,0,0)$, $V(s') = \\$4,000$.\n        - For action $(0,0,1)$, $V(s') = \\$3,500$.\n        - For action $(1,0,1)$, $V(s') = \\$2,100$.\n- **Required Calculation:** State-action value $Q(s_0,a) = c(s_0,a) + \\gamma V(s')$.\n- **Final Output:** The minimal value of $Q(s_0,a)$ across feasible actions, rounded to four significant figures.\n\n#### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded:** The problem describes a unit commitment and economic dispatch problem, a classical topic in energy systems modeling and operations research. Framing it within an MDP for RL is a contemporary and valid method. The cost functions and physical constraints (ramp rates, min/max output, minimum up/down times) are standard representations in the field. The setup is scientifically sound.\n- **Well-Posed:** The problem is clearly defined with all necessary data provided. The set of actions is finite. The procedure for calculating the immediate cost for any given action is deterministic and unambiguous. The value function for the next state is explicitly given for all feasible actions. The objective is to find the minimum of a set of calculated values, which is a well-defined task. The problem admits a unique and stable solution.\n- **Objective:** The problem statement is expressed using precise, quantitative, and unbiased language. All definitions and data are formal and not subject to interpretation.\n\n#### Step 3: Verdict and Action\n\nThe problem is scientifically grounded, well-posed, and objective. It is deemed **valid**. A full solution will be provided.\n\n### Solution\n\nThe solution proceeds by following the four tasks outlined in the problem statement.\n\n**1. Enumerate Feasible Commitment Actions**\n\nThe commitment action is a binary vector $a = (a_1, a_2, a_3)$. Unit $\\text{U}2$ is currently off and has a residual minimum down time obligation of $1$ period. This means it cannot be turned on in period $1$. Therefore, its commitment state must be off, i.e., $a_2 = 0$.\nUnits $\\text{U}1$ and $\\text{U}3$ have no residual minimum up or down time obligations, so their commitment states $a_1$ and $a_3$ can be either $0$ or $1$.\nThe set of feasible actions is therefore all combinations of $(a_1, 0, a_3)$, which are:\n- $a = (0, 0, 0)$\n- $a = (1, 0, 0)$\n- $a = (0, 0, 1)$\n- $a = (1, 0, 1)$\n\n**2.  3. Compute Dispatch, Immediate Cost, and Q-Value for Each Feasible Action**\n\nThe state-action value is given by $Q(s_0, a) = c(s_0, a) + \\gamma V(s')$, where $\\gamma = 0.95$. We evaluate this for each feasible action. The demand is $D = 120$ MW.\n\n**Action (a): $a = (0, 0, 0)$**\n- **Dispatch:** All units are off. $P_1 = 0$, $P_2 = 0$, $P_3 = 0$. Total generation $\\sum P_i = 0$ MW.\n- **Immediate Cost $c(s_0, a)$:**\n    - No-load cost: $\\$0$.\n    - Variable cost: $\\$0$.\n    - Startup cost: $\\$0$ (no startups).\n    - Energy balance penalty: $\\kappa_{\\text{bal}}|D - \\sum P_i| = 1000 \\times |120 - 0| = \\$120,000$.\n    - Ramp violation penalty:\n        - $\\text{U}1$: Shuts down. Change is $|P_1 - P_1^0| = |0 - 40| = 40$ MW. Ramp limit $r_1 = 40$ MW. Violation is $\\max(0, 40-40) = 0$.\n        - $\\text{U}2$: Stays off. Change is $|0-0|=0$. No violation.\n        - $\\text{U}3$: Shuts down. Change is $|P_3 - P_3^0| = |0 - 50| = 50$ MW. Ramp limit $r_3 = 30$ MW. Violation is $\\max(0, 50-30) = 20$ MW.\n        - Total penalty: $\\kappa_r \\times (0+0+20) = 50 \\times 20 = \\$1,000$.\n    - Total immediate cost: $c(s_0, a) = 0 + 0 + 0 + 120000 + 1000 = \\$121,000$.\n- **Q-Value:**\n    - $V(s')$ for this action is given as $\\$10,000$.\n    - $Q(s_0, (0,0,0)) = 121000 + 0.95 \\times 10000 = 121000 + 9500 = \\$130,500$.\n\n**Action (b): $a = (1, 0, 0)$**\n- **Dispatch:** Only unit $\\text{U}1$ is on. We must determine its feasible operating range.\n    - Capacity limits: $P_1 \\in [P_1^{\\min}, P_1^{\\max}] = [30, 100]$.\n    - Ramp limits: $|P_1 - P_1^0| \\le r_1 \\implies |P_1 - 40| \\le 40 \\implies 0 \\le P_1 \\le 80$.\n    - The feasible range is the intersection: $P_1 \\in [30, 100] \\cap [0, 80] = [30, 80]$ MW.\n    - To minimize the energy balance penalty, we dispatch $P_1$ as close as possible to the demand $D=120$ MW. The maximum feasible output is $P_1=80$ MW.\n    - So, $P_1 = 80$ MW, $P_2 = 0$, $P_3 = 0$. Total generation is $80$ MW.\n- **Immediate Cost $c(s_0, a)$:**\n    - No-load cost: $C_1^{\\text{nl}} = \\$200$.\n    - Variable cost: $c_1 P_1 = 18 \\times 80 = \\$1,440$.\n    - Startup cost: $\\$0$ ($\\text{U}1$ was already on).\n    - Energy balance penalty: $\\kappa_{\\text{bal}}|D - \\sum P_i| = 1000 \\times |120 - 80| = \\$40,000$.\n    - Ramp violation penalty: $\\text{U}1$ is at its ramp limit, not over. $\\text{U}3$ shuts down, with a ramp violation of $20$ MW as calculated before. Penalty: $50 \\times 20 = \\$1,000$.\n    - Total immediate cost: $c(s_0, a) = 200 + 1440 + 0 + 40000 + 1000 = \\$42,640$.\n- **Q-Value:**\n    - $V(s')$ for this action is given as $\\$4,000$.\n    - $Q(s_0, (1,0,0)) = 42640 + 0.95 \\times 4000 = 42640 + 3800 = \\$46,440$.\n\n**Action (c): $a = (0, 0, 1)$**\n- **Dispatch:** Only unit $\\text{U}3$ is on. Feasible operating range for $\\text{U}3$:\n    - Capacity limits: $P_3 \\in [P_3^{\\min}, P_3^{\\max}] = [20, 80]$.\n    - Ramp limits: $|P_3 - P_3^0| \\le r_3 \\implies |P_3 - 50| \\le 30 \\implies 20 \\le P_3 \\le 80$.\n    - The feasible range is $[20, 80]$ MW.\n    - To meet demand $D=120$ MW, we dispatch the max feasible output: $P_3=80$ MW.\n    - So, $P_1 = 0$, $P_2 = 0$, $P_3 = 80$ MW. Total generation is $80$ MW.\n- **Immediate Cost $c(s_0, a)$:**\n    - No-load cost: $C_3^{\\text{nl}} = \\$150$.\n    - Variable cost: $c_3 P_3 = 22 \\times 80 = \\$1,760$.\n    - Startup cost: $\\$0$ ($\\text{U}3$ was already on).\n    - Energy balance penalty: $\\kappa_{\\text{bal}}|D - \\sum P_i| = 1000 \\times |120 - 80| = \\$40,000$.\n    - Ramp violation penalty: $\\text{U}1$ shuts down with no violation. $\\text{U}3$ is at its ramp limit, not over. Total penalty: $\\$0$.\n    - Total immediate cost: $c(s_0, a) = 150 + 1760 + 0 + 40000 + 0 = \\$41,910$.\n- **Q-Value:**\n    - $V(s')$ for this action is given as $\\$3,500$.\n    - $Q(s_0, (0,0,1)) = 41910 + 0.95 \\times 3500 = 41910 + 3325 = \\$45,235$.\n\n**Action (d): $a = (1, 0, 1)$**\n- **Dispatch:** Units $\\text{U}1$ and $\\text{U}3$ are on.\n    - Feasible ranges: $P_1 \\in [30, 80]$ MW, $P_3 \\in [20, 80]$ MW.\n    - Total available capacity is $[30+20, 80+80] = [50, 160]$ MW. Since demand $D=120$ MW is within this range, it can be met without penalty.\n    - Economic dispatch dictates using the cheaper unit first. Since $c_1=\\$18/\\text{MWh}  c_3=\\$22/\\text{MWh}$, we maximize $P_1$ first.\n    - We set $P_1$ to its maximum feasible value, $P_1 = 80$ MW.\n    - The remaining demand is satisfied by $\\text{U}3$: $P_3 = 120 - P_1 = 120 - 80 = 40$ MW.\n    - This dispatch is feasible as $P_3=40$ MW is within its range $[20, 80]$ MW.\n    - The final dispatch is $P_1=80$ MW, $P_2=0$ MW, $P_3=40$ MW. Total generation is $120$ MW.\n- **Immediate Cost $c(s_0, a)$:**\n    - No-load cost: $C_1^{\\text{nl}} + C_3^{\\text{nl}} = 200 + 150 = \\$350$.\n    - Variable cost: $c_1 P_1 + c_3 P_3 = 18 \\times 80 + 22 \\times 40 = 1440 + 880 = \\$2,320$.\n    - Startup cost: $\\$0$.\n    - Energy balance penalty: $\\$0$, as demand is fully met.\n    - Ramp violation penalty:\n        - $\\text{U}1$: change is $|80-40|=40$ MW. Limit $r_1=40$ MW. No violation.\n        - $\\text{U}3$: change is $|40-50|=10$ MW. Limit $r_3=30$ MW. No violation.\n        - Total penalty: $\\$0$.\n    - Total immediate cost: $c(s_0, a) = 350 + 2320 + 0 + 0 + 0 = \\$2,670$.\n- **Q-Value:**\n    - $V(s')$ for this action is given as $\\$2,100$.\n    - $Q(s_0, (1,0,1)) = 2670 + 0.95 \\times 2100 = 2670 + 1995 = \\$4,665$.\n\n**4. Determine the Minimal Q-Value**\n\nThe computed $Q$-values for the feasible actions are:\n- $Q(s_0, (0,0,0)) = \\$130,500$\n- $Q(s_0, (1,0,0)) = \\$46,440$\n- $Q(s_0, (0,0,1)) = \\$45,235$\n- $Q(s_0, (1,0,1)) = \\$4,665$\n\nThe minimal value of $Q(s_0, a)$ is $\\$4,665$. The problem requests the answer to be rounded to four significant figures. The value $4665$ already has four significant figures.",
            "answer": "$$\\boxed{4665}$$"
        },
        {
            "introduction": "In real-world applications, deploying an untested control policy can be risky and expensive. This exercise introduces Off-Policy Evaluation (OPE), a critical data-driven technique for estimating a new policy's performance using historical data from an existing system. By implementing per-decision importance sampling for an HVAC control dataset, you will gain hands-on experience with a vital tool for safely vetting and deploying RL agents in operational environments .",
            "id": "4115627",
            "problem": "You are given a small, fixed dataset of building Heating, Ventilation, and Air Conditioning (HVAC) control experience generated by a known behavior policy in a discrete-time Reinforcement Learning (RL) setting for operational control and scheduling within energy systems modeling. The environment is modeled as a Markov Decision Process (MDP) with a discrete action set and a continuous state space. Your task is to compute Off-Policy Evaluation (OPE) estimates for specified candidate evaluation policies using per-decision importance sampling, expressed as an average return across episodes. All mathematical symbols and numbers in this problem description are expressed in LaTeX. All angles for sinusoidal functions must be in radians.\n\nThe environment has the following characteristics:\n- Time is discretized into steps of duration $1$ hour.\n- The action set is $\\mathcal{A} = \\{0,1,2\\}$ indexing HVAC power levels in kilowatts (kW) with power mapping $\\text{power}(0)=0.0$ kW, $\\text{power}(1)=2.0$ kW, $\\text{power}(2)=4.0$ kW.\n- The state at time step $t$ is $s_t = (T_{\\text{in},t}, T_{\\text{out},t}, h_t, o_t, p_t)$ where:\n  - $T_{\\text{in},t}$ is indoor air temperature in degrees Celsius (°C).\n  - $T_{\\text{out},t}$ is outdoor air temperature in °C.\n  - $h_t$ is the hour of day as an integer in $\\{0,1,\\dots,23\\}$.\n  - $o_t \\in \\{0,1\\}$ is an occupancy indicator with $1$ meaning occupied and $0$ meaning unoccupied.\n  - $p_t$ is the electricity price in dollars per kilowatt-hour (\\$/$\\text{kWh}$).\n- The per-step reward is defined as negative operating cost plus a discomfort penalty based on deviation from a target setpoint. Let the occupied cooling setpoint be $T_{\\text{set}}(1)=22$ °C and the unoccupied cooling setpoint be $T_{\\text{set}}(0)=26$ °C. Define the absolute deviation beyond a $1$ °C deadband as $\\delta_t=\\max\\{0, |T_{\\text{in},t} - T_{\\text{set}}(o_t)| - 1.0\\}$. The discomfort penalty rate is $2.0$ dollars per degree when occupied and $0.2$ dollars per degree when unoccupied. Therefore, the per-step reward is\n$$\nr_t = -\\left(p_t \\cdot \\text{power}(a_t)\\right) - \\left(\\lambda(o_t) \\cdot \\delta_t\\right),\n$$\nwhere $\\lambda(1)=2.0$ and $\\lambda(0)=0.2$. The reward $r_t$ is in dollars per time step.\n\nPolicies are defined as softmax distributions over a linear preference model. Let the feature vector be\n$$\n\\phi(s) = \\begin{bmatrix}\n1, \\\\\nT_{\\text{in}} - T_{\\text{set}}(o), \\\\\nT_{\\text{out}}, \\\\\no, \\\\\np, \\\\\n\\sin\\left(2\\pi \\frac{h}{24}\\right), \\\\\n\\cos\\left(2\\pi \\frac{h}{24}\\right)\n\\end{bmatrix} \\in \\mathbb{R}^7,\n$$\nwith angles in radians. For action $a \\in \\{0,1,2\\}$, the preference is $q_a(s) = w_a^\\top \\phi(s)$ for weight vector $w_a \\in \\mathbb{R}^7$. A policy $\\pi$ assigns probability\n$$\n\\pi(a \\mid s) = \\frac{\\exp\\left(q_a(s)\\right)}{\\sum_{a' \\in \\{0,1,2\\}} \\exp\\left(q_{a'}(s)\\right)}.\n$$\n\nThe behavior policy weights are given by the $3 \\times 7$ matrix $W_b$ with rows corresponding to actions $a=0,1,2$:\n- $w_b^{(0)} = [\\,0.2,\\,-0.7,\\,-0.05,\\,-0.5,\\,-1.0,\\,0.1,\\,0.0\\,]$,\n- $w_b^{(1)} = [\\,0.0,\\,0.2,\\,0.02,\\,0.3,\\,-0.2,\\,-0.05,\\,0.05\\,]$,\n- $w_b^{(2)} = [\\,-0.3,\\,0.8,\\,0.05,\\,0.6,\\,-0.1,\\,-0.05,\\,0.0\\,]$.\n\nYou are provided a small dataset of $3$ episodes. Each episode is a sequence of $(s_t, a_t)$ pairs, where $s_t=(T_{\\text{in},t}, T_{\\text{out},t}, h_t, o_t, p_t)$ and $a_t \\in \\{0,1,2\\}$. Rewards $r_t$ must be computed by you from the provided state and action using the reward definition above. The episodes are:\n\nEpisode $1$ (length $4$):\n- $t=0$: $s_0 = (27.5, 33, 15, 1, 0.30)$, $a_0=2$.\n- $t=1$: $s_1 = (26.8, 34, 16, 1, 0.32)$, $a_1=2$.\n- $t=2$: $s_2 = (25.9, 33, 17, 1, 0.35)$, $a_2=1$.\n- $t=3$: $s_3 = (24.8, 31, 18, 1, 0.33)$, $a_3=1$.\n\nEpisode $2$ (length $4$):\n- $t=0$: $s_0 = (24.0, 24, 22, 0, 0.18)$, $a_0=0$.\n- $t=1$: $s_1 = (24.2, 22, 23, 0, 0.16)$, $a_1=0$.\n- $t=2$: $s_2 = (23.8, 20, 0, 0, 0.14)$, $a_2=0$.\n- $t=3$: $s_3 = (23.0, 19, 1, 0, 0.12)$, $a_3=0$.\n\nEpisode $3$ (length $4$):\n- $t=0$: $s_0 = (25.0, 21, 6, 0, 0.10)$, $a_0=0$.\n- $t=1$: $s_1 = (25.5, 22, 7, 1, 0.11)$, $a_1=2$.\n- $t=2$: $s_2 = (24.7, 24, 8, 1, 0.12)$, $a_2=1$.\n- $t=3$: $s_3 = (23.8, 26, 9, 1, 0.15)$, $a_3=1$.\n\nFor Off-Policy Evaluation (OPE), you are to compute the per-decision importance sampling (PDIS) estimate of the expected discounted return of a candidate evaluation policy $\\pi_e$ using the dataset above that was collected under the behavior policy $\\pi_b$. Let the discount factor be $\\gamma \\in [0,1]$. Use the standard identity from probability theory for importance sampling and its per-decision factorization over time steps in a trajectory sampled from $\\pi_b$ to construct an unbiased estimator for the return under $\\pi_e$, and then average this estimator over the $3$ episodes. You must assume absolute continuity (the support of $\\pi_e$ is included in the support of $\\pi_b$) so that all likelihood ratios are finite.\n\nDefine the following three test cases, each specifying a discount factor $\\gamma$ and the evaluation policy weights $W_e$ (a $3 \\times 7$ matrix with rows $w_e^{(a)}$ for $a \\in \\{0,1,2\\}$):\n\nTest case $1$: $\\gamma=0.95$ and\n- $w_e^{(0)} = [\\,0.25,\\,-0.8,\\,-0.06,\\,-0.6,\\,-1.2,\\,0.1,\\,0.0\\,]$,\n- $w_e^{(1)} = [\\,0.05,\\,0.15,\\,0.02,\\,0.25,\\,-0.25,\\,-0.05,\\,0.05\\,]$,\n- $w_e^{(2)} = [\\,-0.35,\\,0.85,\\,0.06,\\,0.65,\\,-0.15,\\,-0.05,\\,0.0\\,]$.\n\nTest case $2$: $\\gamma=1.0$ and\n- $w_e^{(0)} = [\\,0.5,\\,-1.0,\\,-0.05,\\,-0.8,\\,-2.0,\\,0.0,\\,0.0\\,]$,\n- $w_e^{(1)} = [\\,-0.2,\\,0.0,\\,0.01,\\,0.0,\\,-0.5,\\,0.0,\\,0.0\\,]$,\n- $w_e^{(2)} = [\\,-1.0,\\,0.5,\\,0.02,\\,0.2,\\,-0.5,\\,0.0,\\,0.0\\,]$.\n\nTest case $3$: $\\gamma=0.90$ and\n- $w_e^{(0)} = [\\,-0.2,\\,-1.2,\\,-0.1,\\,-1.0,\\,-0.5,\\,0.0,\\,0.0\\,]$,\n- $w_e^{(1)} = [\\,0.0,\\,0.4,\\,0.05,\\,0.8,\\,-0.1,\\,0.0,\\,0.0\\,]$,\n- $w_e^{(2)} = [\\,0.2,\\,1.2,\\,0.1,\\,1.5,\\,-0.1,\\,0.0,\\,0.0\\,]$.\n\nStarting from the definition of expected discounted return under a policy in an MDP, the law of the unconscious statistician, and the importance sampling identity for changing measures, derive a per-decision importance sampling estimator that aggregates over time steps with a product of stepwise likelihood ratios. Implement this estimator to produce, for each test case, a single scalar equal to the sample-average per-decision importance sampling estimate of the return of the evaluation policy across the $3$ provided episodes. All computations of $\\sin(\\cdot)$ and $\\cos(\\cdot)$ must use radians. Note that the per-step rewards are in dollars, so the return is in dollars. Express your final numerical answers as floating-point numbers with rounding to $6$ decimal places.\n\nFinal output format requirement: Your program should produce a single line of output containing a list of the three OPE estimates corresponding to the test cases in order, as a comma-separated list enclosed in square brackets (e.g., `[x_1,x_2,x_3]`), with each $x_i$ rounded to $6$ decimal places and no units included in the output string.",
            "solution": "The problem requires the computation of Off-Policy Evaluation (OPE) estimates for a set of candidate control policies using the per-decision importance sampling (PDIS) method. The context is the operational control of a building's HVAC system, modeled as a Markov Decision Process (MDP). We are provided with a dataset of experiences collected under a known behavior policy, along with complete specifications for the state space, action space, reward function, and the functional form of the policies.\n\nThe fundamental goal of OPE is to estimate the expected performance of an evaluation policy, $\\pi_e$, using data collected under a different behavior policy, $\\pi_b$. The performance is measured by the expected total discounted return, defined as $J(\\pi_e) = \\mathbb{E}_{\\tau \\sim \\pi_e} [G(\\tau)]$, where $\\tau$ is a trajectory (or episode) and $G(\\tau) = \\sum_{t=0}^{H-1} \\gamma^t r_t$ is the discounted return for a trajectory of length $H$. The expectation $\\mathbb{E}_{\\tau \\sim \\pi_e}$ is over the distribution of trajectories induced by following policy $\\pi_e$.\n\nDirectly evaluating this expectation is impossible without running $\\pi_e$ in the environment. Importance sampling (IS) provides a way to estimate $J(\\pi_e)$ using trajectories sampled from $\\pi_b$. The core principle of importance sampling allows us to re-weight expectations from one probability distribution to another. The IS identity for this problem is:\n$$\nJ(\\pi_e) = \\mathbb{E}_{\\tau \\sim \\pi_b} \\left[ \\frac{P(\\tau | \\pi_e)}{P(\\tau | \\pi_b)} G(\\tau) \\right]\n$$\nThe ratio of trajectory probabilities, $P(\\tau | \\pi_e) / P(\\tau | \\pi_b)$, simplifies to a product of per-step policy ratios, as the initial state distribution $p(s_0)$ and the environment dynamics $P(s_{t+1}|s_t, a_t)$ are independent of the policy:\n$$\n\\frac{P(\\tau | \\pi_e)}{P(\\tau | \\pi_b)} = \\frac{p(s_0) \\prod_{t=0}^{H-1} \\pi_e(a_t | s_t) P(s_{t+1} | s_t, a_t)}{p(s_0) \\prod_{t=0}^{H-1} \\pi_b(a_t | s_t) P(s_{t+1} | s_t, a_t)} = \\prod_{t=0}^{H-1} \\frac{\\pi_e(a_t | s_t)}{\\pi_b(a_t | s_t)}\n$$\nThis leads to the standard importance sampling estimator, which, however, is known to suffer from high variance.\n\nThe per-decision importance sampling (PDIS) estimator is a more stable alternative derived by applying the IS principle more granularly. We begin by expressing the expected return using the linearity of expectation:\n$$\nJ(\\pi_e) = \\mathbb{E}_{\\pi_e} \\left[ \\sum_{t=0}^{H-1} \\gamma^t r_t \\right] = \\sum_{t=0}^{H-1} \\gamma^t \\mathbb{E}_{\\pi_e} [r_t]\n$$\nThe expectation $\\mathbb{E}_{\\pi_e}[r_t]$ is over the state-action distribution at time step $t$ induced by $\\pi_e$. We can use importance sampling to estimate this expectation for each term in the sum, using the history up to time $t$:\n$$\n\\mathbb{E}_{\\pi_e} [r_t] = \\mathbb{E}_{\\pi_b} \\left[ \\left( \\prod_{k=0}^{t} \\frac{\\pi_e(a_k | s_k)}{\\pi_b(a_k | s_k)} \\right) r_t \\right]\n$$\nSubstituting this back into the expression for $J(\\pi_e)$ and again using the linearity of expectation, we obtain:\n$$\nJ(\\pi_e) = \\mathbb{E}_{\\tau \\sim \\pi_b} \\left[ \\sum_{t=0}^{H-1} \\gamma^t \\left( \\prod_{k=0}^{t} \\frac{\\pi_e(a_k | s_k)}{\\pi_b(a_k | s_k)} \\right) r_t \\right]\n$$\nThis gives the PDIS estimator for a single trajectory $\\tau_i$ from our dataset $\\mathcal{D}$:\n$$\n\\hat{J}_{\\text{PDIS}}(\\tau_i) = \\sum_{t=0}^{H_i-1} \\gamma^t \\left( \\prod_{k=0}^{t} \\frac{\\pi_e(a_{i,k} | s_{i,k})}{\\pi_b(a_{i,k} | s_{i,k})} \\right) r_{i,t}\n$$\nwhere $H_i$ is the length of episode $i$, and $(s_{i,k}, a_{i,k})$ are the state-action pairs in that episode. The final OPE estimate is the sample mean over all $N$ trajectories in the dataset:\n$$\n\\hat{J}_{\\text{PDIS}} = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{J}_{\\text{PDIS}}(\\tau_i)\n$$\nIn this problem, $N=3$. The implementation proceeds by applying this formula to the provided data for each of the three test cases.\n\nThe computational steps for each test case are as follows:\n$1.$ For each episode in the dataset, we initialize an episode-specific return estimate to zero.\n$2.$ We iterate through the time steps $t=0, 1, \\dots, H-1$ of the episode.\n$3.$ At each step $t$, we perform the following calculations:\n    a. **Feature Vector**: Construct the feature vector $\\phi(s_t) \\in \\mathbb{R}^7$. This requires calculating the term $T_{\\text{in},t} - T_{\\text{set}}(o_t)$ and the sinusoidal time features $\\sin(2\\pi h_t/24)$ and $\\cos(2\\pi h_t/24)$. The state components are given as $s_t = (T_{\\text{in},t}, T_{\\text{out},t}, h_t, o_t, p_t)$ and setpoints are $T_{\\text{set}}(1)=22$ and $T_{\\text{set}}(0)=26$.\n    b. **Reward**: Compute the reward $r_t$. First, the thermal discomfort deviation is $\\delta_t=\\max\\{0, |T_{\\text{in},t} - T_{\\text{set}}(o_t)| - 1.0\\}$. The reward is then $r_t = -(p_t \\cdot \\text{power}(a_t)) - (\\lambda(o_t) \\cdot \\delta_t)$, where power levels are $\\text{power}(0)=0.0$, $\\text{power}(1)=2.0$, $\\text{power}(2)=4.0$, and penalty rates are $\\lambda(1)=2.0$, $\\lambda(0)=0.2$.\n    c. **Policy Probabilities**: For both the behavior policy $\\pi_b$ and the evaluation policy $\\pi_e$, calculate the probability of the action $a_t$ taken at state $s_t$. This involves:\n        i. Computing the linear preferences $q_a(s_t) = w_a^\\top \\phi(s_t)$ for each possible action $a' \\in \\{0, 1, 2\\}$, using the respective weight matrices $W_b$ and $W_e$.\n        ii. Calculating the softmax probabilities: $\\pi(a_t | s_t) = \\exp(q_{a_t}(s_t)) / \\sum_{a' \\in \\mathcal{A}} \\exp(q_{a'}(s_t))$.\n    d. **Importance Ratio**: Compute the cumulative importance ratio up to time $t$, $\\rho_t = \\prod_{k=0}^{t} \\frac{\\pi_e(a_k | s_k)}{\\pi_b(a_k | s_k)}$. This is done efficiently by multiplying the ratio from the previous step, $\\rho_{t-1}$, by the current step's ratio, $\\omega_t = \\pi_e(a_t | s_t) / \\pi_b(a_t | s_t)$.\n    e. **Update Estimate**: Add the term $\\gamma^t \\rho_t r_t$ to the current episode's return estimate.\n$4.$ After processing all time steps for all episodes, the final PDIS estimate for the test case is the average of the return estimates for the $3$ episodes. This entire process is repeated for each of the three test cases, using the corresponding $\\gamma$ and $W_e$.",
            "answer": "```\n[-10.825867,-2.484226,-5.922119]\n```"
        }
    ]
}