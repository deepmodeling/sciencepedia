{
    "hands_on_practices": [
        {
            "introduction": "本练习回归基础。这是一个小而可控的马尔可夫决策过程（MDP），让您可以手动推演贝尔曼方程。解决这个问题将巩固您对价值函数如何计算以及它们如何直接导出最优策略的理解，这是许多强化学习算法的核心。",
            "id": "4115663",
            "problem": "考虑一个用于强化学习 (RL) 中能源系统运行调度的有限期马尔可夫决策过程 (MDP)，定义如下。时间由 $t \\in \\{1,2\\}$ 索引，构成一个周期为 $2$ 的有限期，折扣因子 $\\gamma = 1$。时间 $t$ 的状态是二元需求 $d_t \\in \\{0,1\\}$，在选择动作之前被观察到。动作是二元发电机调度 $a_t \\in \\{0,1\\}$，其中 $a_t = 1$ 表示开启发电机。单周期成本为\n$$ \\ell(d_t,a_t) = 5 a_t + 10 \\max(0, d_t - a_t), $$\n这代表了发电机开启时的运行成本，以及需求未被满足时的短缺惩罚。需求作为时齐一阶马尔可夫链外生演化，具有\n$$ \\mathbb{P}(d_{t+1} = 1 \\mid d_t = 1) = \\alpha, \\quad \\mathbb{P}(d_{t+1} = 1 \\mid d_t = 0) = \\beta, $$\n对于 $d_{t+1} = 0$ 则为互补概率。动作 $a_t$ 不影响需求转移。初始需求分布为\n$$ \\mathbb{P}(d_1 = 1) = \\pi, \\quad \\mathbb{P}(d_1 = 0) = 1 - \\pi, $$\n参数 $\\alpha, \\beta, \\pi \\in [0,1]$。\n\n从最优性原理和有限期 MDP 价值函数的定义出发，推导最优策略并计算两个周期内的最优期望总成本。明确构建 $t = 2$ 和 $t = 1$ 时的状态-动作价值函数 $Q_t(d_t,a_t)$ 和价值函数 $V_t(d_t)$，并清楚地表明其对需求转移概率的依赖关系。然后对初始分布进行边缘化，以获得关于 $\\pi$、$\\alpha$ 和 $\\beta$ 的最优期望总成本的封闭形式解析表达式。\n\n以单个封闭形式解析表达式的形式提供最优期望总成本的最终答案。无需进行四舍五入。",
            "solution": "该问题描述了一个有限期马尔可夫决策过程 (MDP)，并要求解出最优策略和最优期望总成本。我们将使用逆向归纳法（一种标准的动态规划技术）从最终时间周期 $t=2$ 开始解决这个问题。最优性原理指出，一个最优策略具有如下性质：无论初始状态和初始决策是什么，余下的决策对于由第一个决策导致的状态而言，必须构成一个最优策略。\n\n对于一个有限期成本最小化问题，贝尔曼方程为：\n对于最终时间步 $T$，价值函数由终端成本决定，在本问题中为零：$V_T(s_T)=0$。在我们的情况下，从概念上讲 $T=3$（第 2 周期结束时），因此 $V_3(d_3)=0$。\n对于任意时间 $t  T$，状态-动作价值函数 $Q_t(s_t, a_t)$ 和价值函数 $V_t(s_t)$ 由下式给出：\n$$ Q_t(s_t, a_t) = \\ell(s_t, a_t) + \\gamma \\sum_{s_{t+1}} \\mathbb{P}(s_{t+1} | s_t, a_t) V_{t+1}(s_{t+1}) $$\n$$ V_t(s_t) = \\min_{a_t} Q_t(s_t, a_t) $$\n最优动作 $a_t^*(s_t)$ 是实现该最小值的动作。\n\n给定的参数为：时期 $t \\in \\{1, 2\\}$，折扣因子 $\\gamma=1$，状态 $d_t \\in \\{0, 1\\}$，动作 $a_t \\in \\{0, 1\\}$，成本函数 $\\ell(d_t, a_t) = 5 a_t + 10 \\max(0, d_t - a_t)$，以及转移概率 $\\mathbb{P}(d_{t+1}=1|d_t=1) = \\alpha$ 和 $\\mathbb{P}(d_{t+1}=1|d_t=0) = \\beta$。\n\n**步骤 1：对最终周期 $t=2$ 的分析**\n\n在时期末尾，下一状态的价值函数 $V_3(d_3)$ 对所有状态 $d_3$ 均为 $0$。在 $t=2$ 时的状态-动作价值函数为：\n$$ Q_2(d_2, a_2) = \\ell(d_2, a_2) + \\gamma \\mathbb{E}[V_3(d_3) | d_2, a_2] = \\ell(d_2, a_2) $$\n我们计算所有可能的状态-动作对的成本 $\\ell(d_2, a_2)$：\n- 如果需求 $d_2=0$ 且动作 $a_2=0$：$Q_2(0,0) = 5(0) + 10 \\max(0, 0-0) = 0$。\n- 如果需求 $d_2=0$ 且动作 $a_2=1$：$Q_2(0,1) = 5(1) + 10 \\max(0, 0-1) = 5$。\n- 如果需求 $d_2=1$ 且动作 $a_2=0$：$Q_2(1,0) = 5(0) + 10 \\max(0, 1-0) = 10$。\n- 如果需求 $d_2=1$ 且动作 $a_2=1$：$Q_2(1,1) = 5(1) + 10 \\max(0, 1-1) = 5$。\n\n在 $t=2$ 时的最优价值函数 $V_2(d_2)$，是通过对动作 $a_2$ 最小化 $Q_2(d_2, a_2)$ 来找到的。\n- 对于状态 $d_2=0$：\n  $$ V_2(0) = \\min\\{Q_2(0,0), Q_2(0,1)\\} = \\min\\{0, 5\\} = 0 $$\n  最优动作是 $a_2^*(0)=0$。\n- 对于状态 $d_2=1$：\n  $$ V_2(1) = \\min\\{Q_2(1,0), Q_2(1,1)\\} = \\min\\{10, 5\\} = 5 $$\n  最优动作是 $a_2^*(1)=1$。\n\n因此，$t=2$ 时的最优策略是使调度与需求相匹配，即 $a_2^*(d_2) = d_2$。\n\n**步骤 2：对第一个周期 $t=1$ 的分析**\n\n我们现在回溯到 $t=1$。状态-动作价值函数为：\n$$ Q_1(d_1, a_1) = \\ell(d_1, a_1) + \\gamma \\mathbb{E}[V_2(d_2) | d_1, a_1] $$\n由于 $\\gamma=1$ 且需求演化与动作 $a_1$ 无关，该式可简化为：\n$$ Q_1(d_1, a_1) = \\ell(d_1, a_1) + \\mathbb{E}[V_2(d_2) | d_1] $$\n首先，我们计算每个可能状态 $d_1$ 的期望未来成本 $\\mathbb{E}[V_2(d_2) | d_1]$：\n- 对于状态 $d_1=0$：\n  $$ \\mathbb{E}[V_2(d_2) | d_1=0] = \\mathbb{P}(d_2=0 | d_1=0) V_2(0) + \\mathbb{P}(d_2=1 | d_1=0) V_2(1) $$\n  $$ = (1-\\beta)(0) + (\\beta)(5) = 5\\beta $$\n- 对于状态 $d_1=1$：\n  $$ \\mathbb{E}[V_2(d_2) | d_1=1] = \\mathbb{P}(d_2=0 | d_1=1) V_2(0) + \\mathbb{P}(d_2=1 | d_1=1) V_2(1) $$\n  $$ = (1-\\alpha)(0) + (\\alpha)(5) = 5\\alpha $$\n\n现在我们可以计算所有状态-动作对的 $Q_1(d_1, a_1)$：\n- 如果 $d_1=0, a_1=0$：$Q_1(0,0) = \\ell(0,0) + 5\\beta = 0 + 5\\beta = 5\\beta$。\n- 如果 $d_1=0, a_1=1$：$Q_1(0,1) = \\ell(0,1) + 5\\beta = 5 + 5\\beta$。\n- 如果 $d_1=1, a_1=0$：$Q_1(1,0) = \\ell(1,0) + 5\\alpha = 10 + 5\\alpha$。\n- 如果 $d_1=1, a_1=1$：$Q_1(1,1) = \\ell(1,1) + 5\\alpha = 5 + 5\\alpha$。\n\n在 $t=1$ 时的最优价值函数 $V_1(d_1)$，是通过对动作 $a_1$ 最小化 $Q_1(d_1, a_1)$ 来找到的。\n- 对于状态 $d_1=0$：\n  $$ V_1(0) = \\min\\{Q_1(0,0), Q_1(0,1)\\} = \\min\\{5\\beta, 5+5\\beta\\} = 5\\beta $$\n  由于 $\\beta \\in [0,1]$，$5\\beta \\ge 0$，所以 $5\\beta$ 总是小于或等于 $5+5\\beta$。最优动作是 $a_1^*(0)=0$。\n- 对于状态 $d_1=1$：\n  $$ V_1(1) = \\min\\{Q_1(1,0), Q_1(1,1)\\} = \\min\\{10+5\\alpha, 5+5\\alpha\\} = 5+5\\alpha $$\n  由于 $\\alpha \\in [0,1]$，$5\\alpha \\ge 0$，所以 $5+5\\alpha$ 总是小于或等于 $10+5\\alpha$。最优动作是 $a_1^*(1)=1$。\n\n$t=1$ 时的最优策略也是使调度与需求相匹配，即 $a_1^*(d_1) = d_1$。\n\n**步骤 3：最优期望总成本**\n\n最优期望总成本是在 $d_1$ 的初始分布上 $V_1(d_1)$ 的期望值。初始分布由 $\\mathbb{P}(d_1=1)=\\pi$ 和 $\\mathbb{P}(d_1=0)=1-\\pi$ 给出。\n$$ \\mathbb{E}[V_1(d_1)] = \\mathbb{P}(d_1=0) V_1(0) + \\mathbb{P}(d_1=1) V_1(1) $$\n代入我们找到的 $V_1(0)$ 和 $V_1(1)$ 的值：\n$$ \\mathbb{E}[V_1(d_1)] = (1-\\pi)(5\\beta) + (\\pi)(5+5\\alpha) $$\n展开并重新整理各项：\n$$ \\mathbb{E}[V_1(d_1)] = 5\\beta - 5\\pi\\beta + 5\\pi + 5\\pi\\alpha $$\n对表达式进行因式分解，以封闭形式呈现：\n$$ \\mathbb{E}[V_1(d_1)] = 5\\pi + 5\\pi\\alpha - 5\\pi\\beta + 5\\beta $$\n$$ \\mathbb{E}[V_1(d_1)] = 5\\pi(1 + \\alpha - \\beta) + 5\\beta $$\n这就是两个周期内的最优期望总成本的最终解析表达式。",
            "answer": "$$ \\boxed{5\\pi(1 + \\alpha - \\beta) + 5\\beta} $$"
        },
        {
            "introduction": "强化学习智能体必须在利用已知信息（exploitation）和尝试新事物以学习更多（exploration）之间取得平衡。本练习探讨了管理这种权衡的两种流行策略：$\\epsilon$-贪心策略和玻尔兹曼（或softmax）探索。通过根据给定的Q值计算动作概率，您将对这些机制的工作原理及其如何影响智能体的行为获得实践性的理解。",
            "id": "4115651",
            "problem": "在一个带有电池储能系统（BESS）的微电网的削峰运行控制任务中，某个给定时间的决策问题被建模为马尔可夫决策过程（MDP）。动作价值函数 $Q(s,a)$ 返回在状态 $s$ 下采取动作 $a$，并在此后遵循当前策略时，预期可获得的折扣后需量电费成本的减少量。在当前决策时刻，状态 $s$ 是固定的，智能体考虑以下离散动作：$a_1$（高功率放电）、$a_2$（低功率放电）、$a_3$（空闲）和 $a_4$（低功率充电）。一项效用约束禁止在峰值时段内充电，因此 $a_4$ 是不可行的，任何遵守可行性的探索策略都必须为其分配零概率；概率质量仅在可行动作上分配，并且当定义要求均匀性时，任何随机化都必须在可行集上是均匀的。\n\n在此状态下学习到的动作价值为：\n- $Q(s,a_1) = 2.1$,\n- $Q(s,a_2) = 1.9$,\n- $Q(s,a_3) = 2.0$,\n- $Q(s,a_4) = 0.0$.\n\n考虑强化学习（RL）中使用的两种探索机制：\n\n- 在可行动作集上，探索率 $\\epsilon = 0.1$ 的 $\\epsilon$-贪心策略。\n- 在可行动作集上，应用温度 $\\tau = 0.1$ 的玻尔兹曼（softmax）策略。\n\n使用这些策略的概念定义，并通过动作屏蔽（即为不可行动作分配零概率并在可行动作上进行归一化）来强制执行可行性，计算两种策略的动作选择概率向量，顺序为 $(a_1,a_2,a_3,a_4)$。然后选择正确报告这两个向量的选项。\n\nA. $\\epsilon$-贪心策略：$\\left(0.933333, 0.033333, 0.033333, 0\\right)$；softmax策略：$\\left(0.665241, 0.090031, 0.244728, 0\\right)$\n\nB. $\\epsilon$-贪心策略：$\\left(0.900000, 0.050000, 0.050000, 0\\right)$；softmax策略：$\\left(0.665241, 0.090031, 0.244728, 0\\right)$\n\nC. $\\epsilon$-贪心策略：$\\left(0.925000, 0.025000, 0.025000, 0.025000\\right)$；softmax策略：$\\left(0.665241, 0.090031, 0.244728, 0\\right)$\n\nD. $\\epsilon$-贪心策略：$\\left(0.933333, 0.033333, 0.033333, 0\\right)$；softmax策略：$\\left(0.373404, 0.305384, 0.321212, 0\\right)$\n\nE. $\\epsilon$-贪心策略：$\\left(0.933333, 0.033333, 0.033333, 0\\right)$；softmax策略：$\\left(0.336724, 0.330006, 0.333270, 0\\right)$",
            "solution": "我们从基于马尔可夫决策过程（MDP）框架和动作价值函数 $Q(s,a)$ 的标准强化学习（RL）动作选择定义开始。动作价值 $Q(s,a)$ 是从状态 $s$ 开始，采取动作 $a$，然后遵循给定策略的预期折扣回报（在此，被解释为预期的需量电费成本减少量）。探索策略将 $Q(s,\\cdot)$ 映射为动作上的概率。当存在可行性约束时，我们应用动作屏蔽：任何不可行的动作被分配零概率，并且任何所需的随机化或归一化仅在可行集上执行，以使最终的概率为非负且总和为 $1$。\n\n可行动作是 $a_1$、$a_2$ 和 $a_3$。动作 $a_4$ 是不可行的，因此在任何遵守该约束的策略下，其概率必须为 $0$。给定的动作价值是：\n$$\nQ(s,a_1)=2.1,\\quad Q(s,a_2)=1.9,\\quad Q(s,a_3)=2.0,\\quad Q(s,a_4)=0.0.\n$$\n\n步骤 1：确定贪心动作。贪心动作是在可行集上达到 $\\max_a Q(s,a)$ 的任何动作。在 $\\{a_1,a_2,a_3\\}$ 上，最大值是 $Q(s,a_1)=2.1$，因此 $a_1$ 是唯一的贪心动作。\n\n步骤 2：计算探索率 $\\epsilon=0.1$ 的 $\\epsilon$-贪心概率。根据定义，$\\epsilon$-贪心策略以 $1-\\epsilon$ 的概率选择贪心动作，并以 $\\epsilon$ 的概率在可行动作中均匀随机选择。由于均匀随机化包括所有可行动作，贪心动作可以通过 $(1-\\epsilon)$ 部分或 $\\epsilon$-随机化部分被选中；非贪心动作只能通过 $\\epsilon$-随机化部分被选中。令 $m$ 表示可行动作的数量。这里 $m=3$。\n\n- 贪心动作 $a_1$ 的概率：\n$$\n\\mathbb{P}_{\\epsilon\\text{-greedy}}(a_1\\mid s)=(1-\\epsilon) + \\frac{\\epsilon}{m} = (1-0.1) + \\frac{0.1}{3} = 0.9 + 0.033333\\overline{3} = 0.933333\\overline{3}.\n$$\n- 每个非贪心可行动作 $a\\in\\{a_2,a_3\\}$ 的概率：\n$$\n\\mathbb{P}_{\\epsilon\\text{-greedy}}(a\\mid s)=\\frac{\\epsilon}{m}=\\frac{0.1}{3}=0.033333\\overline{3}.\n$$\n- 不可行动作 $a_4$ 的概率：\n$$\n\\mathbb{P}_{\\epsilon\\text{-greedy}}(a_4\\mid s)=0.\n$$\n\n四舍五入到 $6$ 位小数，$\\epsilon$-贪心概率向量按 $(a_1,a_2,a_3,a_4)$ 顺序为：\n$$\n\\left(0.933333,\\,0.033333,\\,0.033333,\\,0\\right).\n$$\n\n步骤 3：计算温度 $\\tau=0.1$ 的玻尔兹曼（softmax）概率。玻尔兹曼策略为每个可行动作 $a$ 分配一个与 $\\exp\\!\\left(\\frac{Q(s,a)}{\\tau}\\right)$ 成正比的概率，并在可行集上进行归一化以确保概率总和为 $1$。为了数值稳定性，在进行指数运算之前，从所有 $Q$ 值中减去一个常数（例如，最大值）在概念上是等效的，因为这个常数会在归一化过程中被抵消。我们从每个可行的 $Q$ 值中减去 $\\max\\{Q(s,a_1),Q(s,a_2),Q(s,a_3)\\}=Q(s,a_1)=2.1$。定义\n$$\n\\tilde{Q}(s,a)=Q(s,a)-2.1,\n$$\n所以\n$$\n\\tilde{Q}(s,a_1)=0,\\quad \\tilde{Q}(s,a_2)=-0.2,\\quad \\tilde{Q}(s,a_3)=-0.1.\n$$\n现在计算由 $\\tau=0.1$ 缩放的指数值：\n$$\nw_1=\\exp\\!\\left(\\frac{\\tilde{Q}(s,a_1)}{\\tau}\\right)=\\exp(0)=1,\n$$\n$$\nw_2=\\exp\\!\\left(\\frac{\\tilde{Q}(s,a_2)}{\\tau}\\right)=\\exp(-2)=0.135335283\\ldots,\n$$\n$$\nw_3=\\exp\\!\\left(\\frac{\\tilde{Q}(s,a_3)}{\\tau}\\right)=\\exp(-1)=0.367879441\\ldots.\n$$\n在可行集上进行归一化：\n$$\nZ=w_1+w_2+w_3=1+0.135335283+0.367879441=1.503214724\\ldots,\n$$\n所以概率为\n$$\n\\mathbb{P}_{\\text{softmax}}(a_1\\mid s)=\\frac{w_1}{Z}=\\frac{1}{1.503214724}=0.665240955\\ldots,\n$$\n$$\n\\mathbb{P}_{\\text{softmax}}(a_2\\mid s)=\\frac{w_2}{Z}=\\frac{0.135335283}{1.503214724}=0.090030573\\ldots,\n$$\n$$\n\\mathbb{P}_{\\text{softmax}}(a_3\\mid s)=\\frac{w_3}{Z}=\\frac{0.367879441}{1.503214724}=0.244728472\\ldots,\n$$\n对于不可行动作，\n$$\n\\mathbb{P}_{\\text{softmax}}(a_4\\mid s)=0.\n$$\n\n四舍五入到 $6$ 位小数，softmax 概率向量按 $(a_1,a_2,a_3,a_4)$ 顺序为：\n$$\n\\left(0.665241,\\,0.090031,\\,0.244728,\\,0\\right).\n$$\n\n逐项分析：\n\n- 选项 A: $\\epsilon$-贪心策略 $\\left(0.933333, 0.033333, 0.033333, 0\\right)$；softmax策略 $\\left(0.665241, 0.090031, 0.244728, 0\\right)$。这些值与推导出的值完全匹配。结论：正确。\n\n- 选项 B: $\\epsilon$-贪心策略 $\\left(0.900000, 0.050000, 0.050000, 0\\right)$；softmax策略 $\\left(0.665241, 0.090031, 0.244728, 0\\right)$。softmax部分是正确的，但 $\\epsilon$-贪心部分错误地将探索概率仅分配给非贪心动作，使得贪心动作的概率为 $1-\\epsilon$ 而不是 $(1-\\epsilon)+\\epsilon/m$。根据所述定义，$\\epsilon$-随机化在所有可行动作上是均匀的，贪心动作也应该从随机化中获得 $\\epsilon/m$ 的概率。结论：错误。\n\n- 选项 C: $\\epsilon$-贪心策略 $\\left(0.925000, 0.025000, 0.025000, 0.025000\\right)$；softmax策略 $\\left(0.665241, 0.090031, 0.244728, 0\\right)$。这个 $\\epsilon$-贪心向量错误地将探索概率分布在包括不可行动作在内的所有动作上，违反了可行性屏蔽（不可行动作的概率必须为 $0$）。结论：错误。\n\n- 选项 D: $\\epsilon$-贪心策略 $\\left(0.933333, 0.033333, 0.033333, 0\\right)$；softmax策略 $\\left(0.373404, 0.305384, 0.321212, 0\\right)$。$\\epsilon$-贪心部分是正确的，但 softmax 向量对应于一个高得多的温度（使用相同的 $Q$ 值差异，大约 $\\tau\\approx 1$），而不是给定的 $\\tau=0.1$。结论：错误。\n\n- 选项 E: $\\epsilon$-贪心策略 $\\left(0.933333, 0.033333, 0.033333, 0\\right)$；softmax策略 $\\left(0.336724, 0.330006, 0.333270, 0\\right)$。$\\epsilon$-贪心部分是正确的，但 softmax 部分源于对温度的错误使用（实际上使用了 $\\exp\\!\\left(\\tau\\,\\tilde{Q}\\right)$ 而不是 $\\exp\\!\\left(\\tilde{Q}/\\tau\\right)$），这产生了一个与 $\\tau=0.1$ 不符的近乎均匀的分布。结论：错误。\n\n因此，正确选项是 A。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "在实际应用中，直接在实时系统（如建筑物的HVAC）上测试新的控制策略可能既有风险又成本高昂。离策略评估（OPE）使我们能够仅使用在旧策略下收集的历史数据来估计新策略的性能。这个编程练习将指导您实现逐决策重要性采样（PDIS）估计器——一种强大的OPE技术，让您亲身体验在能源系统中安全部署强化学习的关键工具。",
            "id": "4115627",
            "problem": "给定一个小型、固定的数据集，该数据集是在离散时间强化学习 (RL) 环境下，通过一个已知的行为策略为能源系统建模中的运行控制和调度生成的建筑供暖、通风和空调 (HVAC) 控制经验。该环境被建模为一个马尔可夫决策过程 (MDP)，具有离散动作集和连续状态空间。您的任务是使用逐决策重要性采样，为指定的候选评估策略计算离策略评估 (OPE) 估计，结果表示为跨回合的平均回报。此问题描述中的所有数学符号和数字均以 LaTeX 表示。所有用于正弦函数的角度必须以弧度为单位。\n\n环境具有以下特征：\n- 时间被离散化为时长为 $1$ 小时的步长。\n- 动作集为 $\\mathcal{A} = \\{0,1,2\\}$，索引 HVAC 功率等级，单位为千瓦 (kW)，功率映射为 $\\text{power}(0)=0.0\\,\\text{kW}$, $\\text{power}(1)=2.0\\,\\text{kW}$, $\\text{power}(2)=4.0\\,\\text{kW}$。\n- 时刻 $t$ 的状态为 $s_t = (T_{\\text{in},t}, T_{\\text{out},t}, h_t, o_t, p_t)$，其中：\n  - $T_{\\text{in},t}$ 是室内空气温度，单位为摄氏度 (°C)。\n  - $T_{\\text{out},t}$ 是室外空气温度，单位为 °C。\n  - $h_t$ 是一天中的小时，为 $\\{0,1,\\dots,23\\}$ 中的整数。\n  - $o_t \\in \\{0,1\\}$ 是一个占用指示符，$1$ 表示占用，$0$ 表示非占用。\n  - $p_t$ 是电价，单位为美元/千瓦时 (\\$/$\\text{kWh}$)。\n- 单步奖励定义为负的运营成本加上基于偏离目标设定点的不适惩罚。设占用状态下的制冷设定点为 $T_{\\text{set}}(1)=22$ °C，非占用状态下的制冷设定点为 $T_{\\text{set}}(0)=26$ °C。定义超出 $1$ °C 死区的绝对偏差为 $\\delta_t=\\max\\{0, |T_{\\text{in},t} - T_{\\text{set}}(o_t)| - 1.0\\}$。占用状态下的不适惩罚率为每度 $2.0$ 美元，非占用状态下为每度 $0.2$ 美元。因此，每步奖励为\n$$\nr_t = -\\left(p_t \\cdot \\text{power}(a_t)\\right) - \\left(\\lambda(o_t) \\cdot \\delta_t\\right),\n$$\n其中 $\\lambda(1)=2.0$ 且 $\\lambda(0)=0.2$。奖励 $r_t$ 的单位是美元/时间步。\n\n策略被定义为基于线性偏好模型的 softmax 分布。设特征向量为\n$$\n\\phi(s) = \\begin{bmatrix}\n1, \\\\\nT_{\\text{in}} - T_{\\text{set}}(o), \\\\\nT_{\\text{out}}, \\\\\no, \\\\\np, \\\\\n\\sin\\left(2\\pi \\frac{h}{24}\\right), \\\\\n\\cos\\left(2\\pi \\frac{h}{24}\\right)\n\\end{bmatrix} \\in \\mathbb{R}^7,\n$$\n角度以弧度为单位。对于动作 $a \\in \\{0,1,2\\}$，偏好为 $q_a(s) = w_a^\\top \\phi(s)$，其中权重向量为 $w_a \\in \\mathbb{R}^7$。策略 $\\pi$ 分配的概率为\n$$\n\\pi(a \\mid s) = \\frac{\\exp\\left(q_a(s)\\right)}{\\sum_{a' \\in \\{0,1,2\\}} \\exp\\left(q_{a'}(s)\\right)}.\n$$\n\n行为策略的权重由 $3 \\times 7$ 矩阵 $W_b$ 给出，其行对应于动作 $a=0,1,2$：\n- $w_b^{(0)} = [\\,0.2,\\,-0.7,\\,-0.05,\\,-0.5,\\,-1.0,\\,0.1,\\,0.0\\,]$,\n- $w_b^{(1)} = [\\,0.0,\\,0.2,\\,0.02,\\,0.3,\\,-0.2,\\,-0.05,\\,0.05\\,]$,\n- $w_b^{(2)} = [\\,-0.3,\\,0.8,\\,0.05,\\,0.6,\\,-0.1,\\,-0.05,\\,0.0\\,]$.\n\n您将获得一个包含 $3$ 个回合的小型数据集。每个回合是一系列 $(s_t, a_t)$ 对，其中 $s_t=(T_{\\text{in},t}, T_{\\text{out},t}, h_t, o_t, p_t)$ 且 $a_t \\in \\{0,1,2\\}$。您必须使用上述奖励定义，根据给定的状态和动作来计算奖励 $r_t$。这些回合是：\n\n回合 $1$ (长度 $4$):\n- $t=0$: $s_0 = (27.5, 33, 15, 1, 0.30)$, $a_0=2$.\n- $t=1$: $s_1 = (26.8, 34, 16, 1, 0.32)$, $a_1=2$.\n- $t=2$: $s_2 = (25.9, 33, 17, 1, 0.35)$, $a_2=1$.\n- $t=3$: $s_3 = (24.8, 31, 18, 1, 0.33)$, $a_3=1$.\n\n回合 $2$ (长度 $4$):\n- $t=0$: $s_0 = (24.0, 24, 22, 0, 0.18)$, $a_0=0$.\n- $t=1$: $s_1 = (24.2, 22, 23, 0, 0.16)$, $a_1=0$.\n- $t=2$: $s_2 = (23.8, 20, 0, 0, 0.14)$, $a_2=0$.\n- $t=3$: $s_3 = (23.0, 19, 1, 0, 0.12)$, $a_3=0$.\n\n回合 $3$ (长度 $4$):\n- $t=0$: $s_0 = (25.0, 21, 6, 0, 0.10)$, $a_0=0$.\n- $t=1$: $s_1 = (25.5, 22, 7, 1, 0.11)$, $a_1=2$.\n- $t=2$: $s_2 = (24.7, 24, 8, 1, 0.12)$, $a_2=1$.\n- $t=3$: $s_3 = (23.8, 26, 9, 1, 0.15)$, $a_3=1$.\n\n对于离策略评估 (OPE)，您需要使用上述在行为策略 $\\pi_b$ 下收集的数据集，计算候选评估策略 $\\pi_e$ 的期望折扣回报的逐决策重要性采样 (PDIS) 估计。设折扣因子为 $\\gamma \\in [0,1]$。使用概率论中的标准重要性采样恒等式及其在从 $\\pi_b$ 采样的轨迹上随时间步的逐决策分解，来构建一个关于 $\\pi_e$ 下回报的无偏估计量，然后在 $3$ 个回合上对该估计量求平均。您必须假设绝对连续性（即 $\\pi_e$ 的支撑集包含在 $\\pi_b$ 的支撑集内），以使所有似然比都是有限的。\n\n定义以下三个测试用例，每个用例指定一个折扣因子 $\\gamma$ 和评估策略权重 $W_e$ (一个 $3 \\times 7$ 矩阵，其行 $w_e^{(a)}$ 对应于 $a \\in \\{0,1,2\\}$)：\n\n测试用例 $1$: $\\gamma=0.95$ 和\n- $w_e^{(0)} = [\\,0.25,\\,-0.8,\\,-0.06,\\,-0.6,\\,-1.2,\\,0.1,\\,0.0\\,]$,\n- $w_e^{(1)} = [\\,0.05,\\,0.15,\\,0.02,\\,0.25,\\,-0.25,\\,-0.05,\\,0.05\\,]$,\n- $w_e^{(2)} = [\\,-0.35,\\,0.85,\\,0.06,\\,0.65,\\,-0.15,\\,-0.05,\\,0.0\\,]$.\n\n测试用例 $2$: $\\gamma=1.0$ 和\n- $w_e^{(0)} = [\\,0.5,\\,-1.0,\\,-0.05,\\,-0.8,\\,-2.0,\\,0.0,\\,0.0\\,]$,\n- $w_e^{(1)} = [\\,-0.2,\\,0.0,\\,0.01,\\,0.0,\\,-0.5,\\,0.0,\\,0.0\\,]$,\n- $w_e^{(2)} = [\\,-1.0,\\,0.5,\\,0.02,\\,0.2,\\,-0.5,\\,0.0,\\,0.0\\,]$.\n\n测试用例 $3$: $\\gamma=0.90$ 和\n- $w_e^{(0)} = [\\,-0.2,\\,-1.2,\\,-0.1,\\,-1.0,\\,-0.5,\\,0.0,\\,0.0\\,]$,\n- $w_e^{(1)} = [\\,0.0,\\,0.4,\\,0.05,\\,0.8,\\,-0.1,\\,0.0,\\,0.0\\,]$,\n- $w_e^{(2)} = [\\,0.2,\\,1.2,\\,0.1,\\,1.5,\\,-0.1,\\,0.0,\\,0.0\\,]$.\n\n从 MDP 中策略下期望折扣回报的定义、无意识统计学家法则以及用于改变测度的重要性采样恒等式出发，推导出一个逐决策重要性采样估计量，该估计量通过步进似然比的乘积在时间步上进行聚合。实现这个估计量，为每个测试用例生成一个单一标量，该标量等于在 $3$ 个给定回合上评估策略回报的样本平均的逐决策重要性采样估计。所有 $\\sin(\\cdot)$ 和 $\\cos(\\cdot)$ 的计算必须使用弧度。注意，每步奖励的单位是美元，因此回报的单位也是美元。将您的最终数值答案表示为浮点数，并四舍五入到 $6$ 位小数。\n\n最终输出格式要求：您的程序应生成单行输出，其中包含一个列表，按顺序包含对应于三个测试用例的 OPE 估计值，格式为逗号分隔的列表并用方括号括起（例如 $[x_1,x_2,x_3]$），其中每个 $x_i$ 四舍五入到 $6$ 位小数，并且输出字符串中不包含单位。",
            "solution": "该问题要求使用逐决策重要性采样 (PDIS) 方法计算一组候选控制策略的离策略评估 (OPE) 估计。背景是建筑 HVAC 系统的运行控制，该系统被建模为一个马尔可夫决策过程 (MDP)。我们获得了一个在已知行为策略下收集的经验数据集，以及状态空间、动作空间、奖励函数和策略函数形式的完整规范。\n\nOPE 的基本目标是使用在不同行为策略 $\\pi_b$ 下收集的数据来估计评估策略 $\\pi_e$ 的期望性能。性能由期望总折扣回报衡量，定义为 $J(\\pi_e) = \\mathbb{E}_{\\tau \\sim \\pi_e} [G(\\tau)]$，其中 $\\tau$ 是一条轨迹（或回合），$G(\\tau) = \\sum_{t=0}^{H-1} \\gamma^t r_t$ 是长度为 $H$ 的轨迹的折扣回报。期望 $\\mathbb{E}_{\\tau \\sim \\pi_e}$ 是在遵循策略 $\\pi_e$ 所引导的轨迹分布上计算的。\n\n如果不运行 $\\pi_e$ 来与环境交互，就无法直接评估此期望。重要性采样 (IS) 提供了一种使用从 $\\pi_b$ 采样的轨迹来估计 $J(\\pi_e)$ 的方法。重要性采样的核心原理允许我们将期望从一个概率分布重新加权到另一个概率分布。该问题的 IS 恒等式为：\n$$\nJ(\\pi_e) = \\mathbb{E}_{\\tau \\sim \\pi_b} \\left[ \\frac{P(\\tau | \\pi_e)}{P(\\tau | \\pi_b)} G(\\tau) \\right]\n$$\n轨迹概率之比 $P(\\tau | \\pi_e) / P(\\tau | \\pi_b)$ 可以简化为每步策略比率的乘积，因为初始状态分布 $p(s_0)$ 和环境动态 $P(s_{t+1}|s_t, a_t)$ 均与策略无关：\n$$\n\\frac{P(\\tau | \\pi_e)}{P(\\tau | \\pi_b)} = \\frac{p(s_0) \\prod_{t=0}^{H-1} \\pi_e(a_t | s_t) P(s_{t+1} | s_t, a_t)}{p(s_0) \\prod_{t=0}^{H-1} \\pi_b(a_t | s_t) P(s_{t+1} | s_t, a_t)} = \\prod_{t=0}^{H-1} \\frac{\\pi_e(a_t | s_t)}{\\pi_b(a_t | s_t)}\n$$\n这导致了标准重要性采样估计量，但众所周知，该估计量存在高方差问题。\n\n逐决策重要性采样 (PDIS) 估计量是一种更稳定的替代方法，通过更细粒度地应用 IS 原理推导得出。我们首先利用期望的线性性质来表示期望回报：\n$$\nJ(\\pi_e) = \\mathbb{E}_{\\pi_e} \\left[ \\sum_{t=0}^{H-1} \\gamma^t r_t \\right] = \\sum_{t=0}^{H-1} \\gamma^t \\mathbb{E}_{\\pi_e} [r_t]\n$$\n期望 $\\mathbb{E}_{\\pi_e}[r_t]$ 是在由 $\\pi_e$ 引导的 $t$ 时刻状态-动作分布上计算的。我们可以使用重要性采样，利用截至时间 $t$ 的历史数据，来估计和式中的每一项：\n$$\n\\mathbb{E}_{\\pi_e} [r_t] = \\mathbb{E}_{\\pi_b} \\left[ \\left( \\prod_{k=0}^{t} \\frac{\\pi_e(a_k | s_k)}{\\pi_b(a_k | s_k)} \\right) r_t \\right]\n$$\n将此式代回 $J(\\pi_e)$ 的表达式，并再次利用期望的线性性质，我们得到：\n$$\nJ(\\pi_e) = \\mathbb{E}_{\\tau \\sim \\pi_b} \\left[ \\sum_{t=0}^{H-1} \\gamma^t \\left( \\prod_{k=0}^{t} \\frac{\\pi_e(a_k | s_k)}{\\pi_b(a_k | s_k)} \\right) r_t \\right]\n$$\n这给出了我们数据集 $\\mathcal{D}$ 中单条轨迹 $\\tau_i$ 的 PDIS 估计量：\n$$\n\\hat{J}_{\\text{PDIS}}(\\tau_i) = \\sum_{t=0}^{H_i-1} \\gamma^t \\left( \\prod_{k=0}^{t} \\frac{\\pi_e(a_{i,k} | s_{i,k})}{\\pi_b(a_{i,k} | s_{i,k})} \\right) r_{i,t}\n$$\n其中 $H_i$ 是回合 $i$ 的长度，$(s_{i,k}, a_{i,k})$ 是该回合中的状态-动作对。最终的 OPE 估计是数据集中所有 $N$ 条轨迹的样本均值：\n$$\n\\hat{J}_{\\text{PDIS}} = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{J}_{\\text{PDIS}}(\\tau_i)\n$$\n在本问题中，$N=3$。实现过程是对每个测试用例，将此公式应用于给定的数据。\n\n每个测试用例的计算步骤如下：\n$1.$ 对于数据集中的每个回合，我们将一个特定于该回合的回报估计初始化为零。\n$2.$ 我们遍历该回合的时间步 $t=0, 1, \\dots, H-1$。\n$3.$ 在每个步骤 $t$，我们执行以下计算：\n    a. **特征向量**：构建特征向量 $\\phi(s_t) \\in \\mathbb{R}^7$。这需要计算项 $T_{\\text{in},t} - T_{\\text{set}}(o_t)$ 和正弦时间特征 $\\sin(2\\pi h_t/24)$ 与 $\\cos(2\\pi h_t/24)$。状态分量给定为 $s_t = (T_{\\text{in},t}, T_{\\text{out},t}, h_t, o_t, p_t)$，设定点为 $T_{\\text{set}}(1)=22$ 和 $T_{\\text{set}}(0)=26$。\n    b. **奖励**：计算奖励 $r_t$。首先，热不适偏差为 $\\delta_t=\\max\\{0, |T_{\\text{in},t} - T_{\\text{set}}(o_t)| - 1.0\\}$。然后奖励为 $r_t = -(p_t \\cdot \\text{power}(a_t)) - (\\lambda(o_t) \\cdot \\delta_t)$，其中功率等级为 $\\text{power}(0)=0.0$、$\\text{power}(1)=2.0$、$\\text{power}(2)=4.0$，惩罚率为 $\\lambda(1)=2.0$、$\\lambda(0)=0.2$。\n    c. **策略概率**：对于行为策略 $\\pi_b$ 和评估策略 $\\pi_e$，计算在状态 $s_t$ 下采取动作 $a_t$ 的概率。这包括：\n        i. 使用各自的权重矩阵 $W_b$ 和 $W_e$，为每个可能的动作 $a' \\in \\{0, 1, 2\\}$ 计算线性偏好 $q_a(s_t) = w_a^\\top \\phi(s_t)$。\n        ii. 计算 softmax 概率：$\\pi(a_t | s_t) = \\exp(q_{a_t}(s_t)) / \\sum_{a' \\in \\mathcal{A}} \\exp(q_{a'}(s_t))$。\n    d. **重要性比率**：计算截至时间 $t$ 的累积重要性比率，$\\rho_t = \\prod_{k=0}^{t} \\frac{\\pi_e(a_k | s_k)}{\\pi_b(a_k | s_k)}$。这可以通过将上一步的比率 $\\rho_{t-1}$ 乘以当前步的比率 $\\omega_t = \\pi_e(a_t | s_t) / \\pi_b(a_t | s_t)$ 来高效完成。\n    e. **更新估计值**：将项 $\\gamma^t \\rho_t r_t$ 加到当前回合的回报估计中。\n$4.$ 在处理完所有回合的所有时间步后，该测试用例的最终 PDIS 估计是 $3$ 个回合回报估计的平均值。对三个测试用例中的每一个重复整个过程，使用相应的 $\\gamma$ 和 $W_e$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Off-Policy Evaluation (OPE) estimates using per-decision importance sampling.\n    \"\"\"\n\n    # --- Problem Givens ---\n\n    # Behavior policy weights\n    W_b = np.array([\n        [0.2, -0.7, -0.05, -0.5, -1.0, 0.1, 0.0],   # action 0\n        [0.0, 0.2, 0.02, 0.3, -0.2, -0.05, 0.05],  # action 1\n        [-0.3, 0.8, 0.05, 0.6, -0.1, -0.05, 0.0]    # action 2\n    ])\n\n    # Dataset of episodes\n    # Each state is (T_in, T_out, h, o, p)\n    # Each step is (state, action)\n    episodes_data = [\n        # Episode 1\n        [\n            ((27.5, 33.0, 15, 1, 0.30), 2),\n            ((26.8, 34.0, 16, 1, 0.32), 2),\n            ((25.9, 33.0, 17, 1, 0.35), 1),\n            ((24.8, 31.0, 18, 1, 0.33), 1)\n        ],\n        # Episode 2\n        [\n            ((24.0, 24.0, 22, 0, 0.18), 0),\n            ((24.2, 22.0, 23, 0, 0.16), 0),\n            ((23.8, 20.0, 0, 0, 0.14), 0),\n            ((23.0, 19.0, 1, 0, 0.12), 0)\n        ],\n        # Episode 3\n        [\n            ((25.0, 21.0, 6, 0, 0.10), 0),\n            ((25.5, 22.0, 7, 1, 0.11), 2),\n            ((24.7, 24.0, 8, 1, 0.12), 1),\n            ((23.8, 26.0, 9, 1, 0.15), 1)\n        ]\n    ]\n\n    # Test cases: (gamma, W_e)\n    test_cases = [\n        # Case 1\n        (0.95, np.array([\n            [0.25, -0.8, -0.06, -0.6, -1.2, 0.1, 0.0],\n            [0.05, 0.15, 0.02, 0.25, -0.25, -0.05, 0.05],\n            [-0.35, 0.85, 0.06, 0.65, -0.15, -0.05, 0.0]\n        ])),\n        # Case 2\n        (1.0, np.array([\n            [0.5, -1.0, -0.05, -0.8, -2.0, 0.0, 0.0],\n            [-0.2, 0.0, 0.01, 0.0, -0.5, 0.0, 0.0],\n            [-1.0, 0.5, 0.02, 0.2, -0.5, 0.0, 0.0]\n        ])),\n        # Case 3\n        (0.90, np.array([\n            [-0.2, -1.2, -0.1, -1.0, -0.5, 0.0, 0.0],\n            [0.0, 0.4, 0.05, 0.8, -0.1, 0.0, 0.0],\n            [0.2, 1.2, 0.1, 1.5, -0.1, 0.0, 0.0]\n        ]))\n    ]\n\n    # Environment constants\n    T_SET = {0: 26.0, 1: 22.0}\n    POWER = {0: 0.0, 1: 2.0, 2: 4.0}\n    LAMBDA = {0: 0.2, 1: 2.0}\n\n    # --- Helper Functions ---\n    \n    def get_setpoint(occupancy):\n        return T_SET[occupancy]\n\n    def compute_features(state):\n        T_in, T_out, h, o, p = state\n        T_set_val = get_setpoint(o)\n        angle = 2 * np.pi * h / 24\n        return np.array([\n            1.0,\n            T_in - T_set_val,\n            T_out,\n            o,\n            p,\n            np.sin(angle),\n            np.cos(angle)\n        ])\n\n    def compute_reward(state, action):\n        T_in, _, _, o, p = state\n        T_set_val = get_setpoint(o)\n        \n        power_val = POWER[action]\n        lambda_val = LAMBDA[o]\n        \n        delta = max(0, abs(T_in - T_set_val) - 1.0)\n        \n        cost_energy = p * power_val\n        cost_discomfort = lambda_val * delta\n        \n        return -cost_energy - cost_discomfort\n\n    def compute_policy_prob(state_features, weights, action):\n        preferences = weights @ state_features\n        exp_prefs = np.exp(preferences - np.max(preferences)) # For numerical stability\n        Z = np.sum(exp_prefs)\n        return exp_prefs[action] / Z\n\n    # --- Main Calculation Loop ---\n    \n    final_results = []\n    for gamma, W_e in test_cases:\n        episode_returns = []\n        for episode in episodes_data:\n            pdis_episode_return = 0.0\n            cumulative_rho = 1.0\n            \n            for t, (s_t, a_t) in enumerate(episode):\n                # 1. Compute features\n                phi_s_t = compute_features(s_t)\n                \n                # 2. Compute probabilities\n                pi_b_prob = compute_policy_prob(phi_s_t, W_b, a_t)\n                pi_e_prob = compute_policy_prob(phi_s_t, W_e, a_t)\n                \n                # 3. Compute step-wise importance ratio and update cumulative\n                omega_t = pi_e_prob / pi_b_prob\n                cumulative_rho *= omega_t\n                \n                # 4. Compute reward\n                r_t = compute_reward(s_t, a_t)\n                \n                # 5. Update PDIS estimate\n                pdis_episode_return += (gamma**t) * cumulative_rho * r_t\n            \n            episode_returns.append(pdis_episode_return)\n        \n        # Average results over episodes\n        ope_estimate = np.mean(episode_returns)\n        final_results.append(ope_estimate)\n\n    # Format and print the final output\n    formatted_results = [f'{res:.6f}' for res in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}