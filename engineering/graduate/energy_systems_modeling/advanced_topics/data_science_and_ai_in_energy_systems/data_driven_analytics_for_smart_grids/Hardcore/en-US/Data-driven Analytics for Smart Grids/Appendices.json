{
    "hands_on_practices": [
        {
            "introduction": "Raw data from sensors in a power grid is often corrupted by noise. Rather than treating each sensor's data in isolation, we can leverage the grid's physical topology to obtain more robust estimates. This practice introduces a powerful technique from graph signal processing, where bus voltage measurements are modeled as a signal on the network graph, allowing us to denoise the data by enforcing smoothness across connected buses through Laplacian regularization .",
            "id": "4082630",
            "problem": "Consider a power distribution network with three buses connected in a chain topology, where the electrical coupling inferred from historical data is modeled as an undirected weighted graph. Let the symmetric weight matrix $W$ have entries $w_{12} = 10$, $w_{23} = 5$, and $w_{13} = 0$, with $w_{ij} = w_{ji}$ and $w_{ii} = 0$. The combinatorial graph Laplacian $L$ is defined as $L = D - W$, where $D$ is the diagonal degree matrix with $D_{ii} = \\sum_{j} w_{ij}$. We model bus voltage magnitudes as a graph signal $x \\in \\mathbb{R}^{3}$ aligned with the bus indices. A noisy measurement vector $y \\in \\mathbb{R}^{3}$ of bus voltage magnitudes in per unit (p.u.) is observed as $y = [0.98, 1.03, 0.96]^\\top$. In data-driven denoising for smart grids, an estimator can be obtained by solving the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\|y - x\\|^{2} + \\lambda \\, x^{\\top} L x,\n$$\nwhere $\\lambda  0$ is a regularization parameter chosen via cross-validation; take $\\lambda = 0.2$.\n\nStarting from fundamental definitions of the graph Laplacian and first-order optimality conditions for convex quadratic optimization, derive the closed-form solution for the minimizer $x^{\\star}$ of the objective above. Then, using the given $W$, compute $L$, form the matrix equation implied by the optimality conditions, and evaluate the denoised bus voltage magnitudes $x^{\\star}$ for the provided $y$ and $\\lambda$. Express the final denoised bus voltage magnitudes in per unit (p.u.). Round your answer to four significant figures.",
            "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   **Network Topology**: Three buses in a chain.\n-   **Weight Matrix ($W$)**: A symmetric matrix with entries $w_{12} = 10$, $w_{23} = 5$, $w_{13} = 0$, and $w_{ij} = w_{ji}$, $w_{ii} = 0$.\n-   **Graph Laplacian ($L$)**: Defined as $L = D - W$.\n-   **Degree Matrix ($D$)**: A diagonal matrix with entries $D_{ii} = \\sum_{j} w_{ij}$.\n-   **Graph Signal ($x$)**: $x \\in \\mathbb{R}^{3}$, representing bus voltage magnitudes.\n-   **Noisy Measurement ($y$)**: $y = [0.98, 1.03, 0.96]^\\top$ in per unit (p.u.).\n-   **Optimization Problem**: $\\min_{x \\in \\mathbb{R}^{3}} \\ \\|y - x\\|^{2} + \\lambda \\, x^{\\top} L x$.\n-   **Regularization Parameter ($\\lambda$)**: $\\lambda = 0.2$.\n-   **Required Output**: The denoised bus voltage magnitudes $x^{\\star}$, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n\n-   **Scientifically Grounded**: The problem is well-grounded in the fields of graph signal processing and convex optimization. The formulation is a standard Tikhonov regularization problem on a graph, a widely used technique in machine learning and data science, including applications in smart grid analytics. The use of a graph Laplacian to enforce smoothness on a signal defined over the graph is a fundamental concept.\n-   **Well-Posed**: The objective function, $J(x) = \\|y - x\\|^{2} + \\lambda x^{\\top} L x$, is the sum of two convex functions. The term $\\|y - x\\|^{2}$ is strictly convex. The term $x^{\\top} L x$ (the graph Laplacian quadratic form) is convex because the combinatorial graph Laplacian $L$ is always positive semi-definite for graphs with non-negative weights. Since $\\lambda  0$, the sum of a strictly convex function and a convex function is strictly convex. A strictly convex function has a unique minimizer. Therefore, a unique solution $x^{\\star}$ exists, and the problem is well-posed.\n-   **Objective**: The problem is stated in precise, formal mathematical language, devoid of any subjective or ambiguous terminology.\n-   **Completeness and Consistency**: All necessary data ($W$, $y$, $\\lambda$) and definitions ($L$, $D$) are provided. There are no contradictions in the setup.\n-   **Realism**: The per unit voltage values in $y$ are realistic for a power distribution system. The weights in $W$ are abstract coupling values, but their scale does not introduce any physical implausibility.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, mathematically well-posed, objective, and self-contained. It is therefore deemed **valid**. A full solution will be provided.\n\nThe objective is to find the vector $x^{\\star}$ that minimizes the cost function:\n$$\nJ(x) = \\|y - x\\|^{2} + \\lambda \\, x^{\\top} L x\n$$\nThis is a convex quadratic optimization problem. The minimizer $x^{\\star}$ can be found by setting the gradient of $J(x)$ with respect to $x$ to zero.\n\nFirst, we expand the objective function. The squared Euclidean norm is $\\|y - x\\|^{2} = (y - x)^{\\top}(y-x)$.\n$$\nJ(x) = (y - x)^{\\top}(y - x) + \\lambda \\, x^{\\top} L x\n$$\n$$\nJ(x) = (y^{\\top} - x^{\\top})(y - x) + \\lambda \\, x^{\\top} L x\n$$\n$$\nJ(x) = y^{\\top}y - y^{\\top}x - x^{\\top}y + x^{\\top}x + \\lambda \\, x^{\\top} L x\n$$\nSince $x^{\\top}y$ is a scalar, it is equal to its transpose $(x^{\\top}y)^{\\top} = y^{\\top}x$. Also, $x^{\\top}x = x^{\\top}Ix$, where $I$ is the identity matrix.\n$$\nJ(x) = x^{\\top}(I + \\lambda L)x - 2y^{\\top}x + y^{\\top}y\n$$\nThis is a quadratic form in $x$. To find the minimum, we compute the gradient $\\nabla_x J(x)$ and set it to zero. Using standard matrix calculus identities, $\\nabla_x(x^{\\top}Ax) = (A+A^{\\top})x$ and $\\nabla_x(b^{\\top}x) = b$.\nThe matrix $(I + \\lambda L)$ is symmetric because both $I$ and $L$ are symmetric. Thus, the gradient of the quadratic term is $2(I+\\lambda L)x$.\n$$\n\\nabla_x J(x) = 2(I + \\lambda L)x - 2y\n$$\nSetting the gradient to zero to find the optimal $x^{\\star}$:\n$$\n2(I + \\lambda L)x^{\\star} - 2y = 0\n$$\n$$\n(I + \\lambda L)x^{\\star} = y\n$$\nThe matrix $(I + \\lambda L)$ is positive definite and therefore invertible, because $I$ is positive definite and $\\lambda L$ is positive semi-definite for $\\lambda  0$. The closed-form solution for the denoised signal $x^{\\star}$ is:\n$$\nx^{\\star} = (I + \\lambda L)^{-1}y\n$$\nNext, we compute the numerical values. First, we construct the matrices $W$, $D$, and $L$.\nGiven $w_{12} = 10$, $w_{23} = 5$, $w_{13} = 0$, $w_{ij}=w_{ji}$, and $w_{ii}=0$:\n$$\nW = \\begin{pmatrix} 0  10  0 \\\\ 10  0  5 \\\\ 0  5  0 \\end{pmatrix}\n$$\nThe degree matrix $D$ has diagonal entries $D_{ii} = \\sum_j w_{ij}$:\n$D_{11} = w_{12} + w_{13} = 10 + 0 = 10$\n$D_{22} = w_{21} + w_{23} = 10 + 5 = 15$\n$D_{33} = w_{31} + w_{32} = 0 + 5 = 5$\nSo, the degree matrix is:\n$$\nD = \\begin{pmatrix} 10  0  0 \\\\ 0  15  0 \\\\ 0  0  5 \\end{pmatrix}\n$$\nThe combinatorial graph Laplacian $L$ is $L = D - W$:\n$$\nL = \\begin{pmatrix} 10  0  0 \\\\ 0  15  0 \\\\ 0  0  5 \\end{pmatrix} - \\begin{pmatrix} 0  10  0 \\\\ 10  0  5 \\\\ 0  5  0 \\end{pmatrix} = \\begin{pmatrix} 10  -10  0 \\\\ -10  15  -5 \\\\ 0  -5  5 \\end{pmatrix}\n$$\nNow, we construct the matrix $A = I + \\lambda L$ with $\\lambda = 0.2$:\n$$\n\\lambda L = 0.2 \\begin{pmatrix} 10  -10  0 \\\\ -10  15  -5 \\\\ 0  -5  5 \\end{pmatrix} = \\begin{pmatrix} 2  -2  0 \\\\ -2  3  -1 \\\\ 0  -1  1 \\end{pmatrix}\n$$\n$$\nA = I + \\lambda L = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} + \\begin{pmatrix} 2  -2  0 \\\\ -2  3  -1 \\\\ 0  -1  1 \\end{pmatrix} = \\begin{pmatrix} 3  -2  0 \\\\ -2  4  -1 \\\\ 0  -1  2 \\end{pmatrix}\n$$\nWe must solve the linear system $Ax^{\\star} = y$:\n$$\n\\begin{pmatrix} 3  -2  0 \\\\ -2  4  -1 \\\\ 0  -1  2 \\end{pmatrix} x^{\\star} = \\begin{pmatrix} 0.98 \\\\ 1.03 \\\\ 0.96 \\end{pmatrix}\n$$\nTo solve for $x^{\\star}$, we can find the inverse of $A$. The determinant of $A$ is:\n$$\n\\det(A) = 3((4)(2) - (-1)(-1)) - (-2)((-2)(2) - (-1)(0)) = 3(7) + 2(-4) = 21 - 8 = 13\n$$\nThe adjugate of $A$ is the transpose of its cofactor matrix. Since $A$ is symmetric, its adjugate is also symmetric.\n$$\n\\text{adj}(A) = \\begin{pmatrix} (4)(2)-(-1)(-1)  -((-2)(2)-0)  (-2)(-1)-0 \\\\ -((-2)(2)-(-1)(0))  (3)(2)-0  -((3)(-1)-0) \\\\ (-2)(-1)-(4)(0)  -((3)(-1)-(-2)(0))  (3)(4)-(-2)(-2) \\end{pmatrix}^{\\top} = \\begin{pmatrix} 7  4  2 \\\\ 4  6  3 \\\\ 2  3  8 \\end{pmatrix}\n$$\nThe inverse is $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$:\n$$\nA^{-1} = \\frac{1}{13} \\begin{pmatrix} 7  4  2 \\\\ 4  6  3 \\\\ 2  3  8 \\end{pmatrix}\n$$\nNow we compute $x^{\\star} = A^{-1}y$:\n$$\nx^{\\star} = \\frac{1}{13} \\begin{pmatrix} 7  4  2 \\\\ 4  6  3 \\\\ 2  3  8 \\end{pmatrix} \\begin{pmatrix} 0.98 \\\\ 1.03 \\\\ 0.96 \\end{pmatrix} = \\frac{1}{13} \\begin{pmatrix} 7(0.98) + 4(1.03) + 2(0.96) \\\\ 4(0.98) + 6(1.03) + 3(0.96) \\\\ 2(0.98) + 3(1.03) + 8(0.96) \\end{pmatrix}\n$$\n$$\nx^{\\star} = \\frac{1}{13} \\begin{pmatrix} 6.86 + 4.12 + 1.92 \\\\ 3.92 + 6.18 + 2.88 \\\\ 1.96 + 3.09 + 7.68 \\end{pmatrix} = \\frac{1}{13} \\begin{pmatrix} 12.90 \\\\ 12.98 \\\\ 12.73 \\end{pmatrix}\n$$\n$$\nx^{\\star} = \\begin{pmatrix} 12.90 / 13 \\\\ 12.98 / 13 \\\\ 12.73 / 13 \\end{pmatrix} = \\begin{pmatrix} 0.9923076... \\\\ 0.9984615... \\\\ 0.9792307... \\end{pmatrix}\n$$\nThe problem requires rounding the final answer to four significant figures.\n$x^{\\star}_1 = 0.9923076... \\approx 0.9923$\n$x^{\\star}_2 = 0.9984615... \\approx 0.9985$\n$x^{\\star}_3 = 0.9792307... \\approx 0.9792$\nThe denoised bus voltage magnitudes are $x^{\\star}_1 \\approx 0.9923$ p.u., $x^{\\star}_2 \\approx 0.9985$ p.u., and $x^{\\star}_3 \\approx 0.9792$ p.u.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.9923  0.9985  0.9792\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After processing raw data, a primary goal in grid analytics is forecasting future behavior, such as the net-load on a feeder. While simple methods like the Exponentially Weighted Moving Average (EWMA) are common, this exercise reveals their deep connection to the Kalman filter, which provides the optimal linear estimate for a dynamic system. By deriving the optimal EWMA parameter from the principles of a linear Gaussian state-space model, you will gain a fundamental understanding of how to construct and justify robust time-series forecasters .",
            "id": "4082672",
            "problem": "Consider a univariate feeder net-load process in a smart grid that is modeled as a linear Gaussian state-space with a simple deterministic drift. The latent level is denoted by $x_t$, the observed net-load is $y_t$, and the drift per time step is a known constant $d$. The system evolves as\n$$\nx_t = x_{t-1} + d + w_t,\\quad w_t \\sim \\mathcal{N}(0,q),\n$$\nand\n$$\ny_t = x_t + v_t,\\quad v_t \\sim \\mathcal{N}(0,r),\n$$\nwhere $q \\ge 0$ and $r  0$ are process and measurement noise variances, respectively, and $\\mathcal{N}(0,\\sigma^2)$ denotes a zero-mean Gaussian distribution with variance $\\sigma^2$. All noises are mutually independent over time and across processes.\n\nAn Exponentially Weighted Moving Average (EWMA) forecaster produces a filtered estimate $\\hat{x}_t$ and a one-step-ahead forecast $\\hat{y}_{t+1|t}$ using a forgetting factor $\\alpha \\in [0,1]$:\n$$\n\\hat{x}_t = (1-\\alpha)\\left(\\hat{x}_{t-1} + d\\right) + \\alpha y_t,\\qquad \\hat{y}_{t+1|t} = \\hat{x}_t + d,\n$$\ninitialized with a finite $\\hat{x}_0$.\n\nYour tasks:\n- Derive from first principles the value of the forgetting factor $\\alpha^\\star$ that minimizes the one-step-ahead mean squared prediction error $E\\left[(y_{t+1} - \\hat{y}_{t+1|t})^2\\right]$ in steady state under the given model. Begin your derivation from the minimum mean square error principle for linear Gaussian systems and the scalar discrete-time algebraic Riccati equation. Do not assume any pre-stated final formula for $\\alpha^\\star$; instead, derive it as an implication of the model assumptions.\n- Implement a program that:\n  - Computes $\\alpha^\\star$ as a function of $(q,r)$, independent of $d$.\n  - Implements the EWMA recursion as specified above.\n  - For each test case below, reports only the theoretically optimal $\\alpha^\\star$.\n\nScientific realism and modeling notes:\n- The derivation must start from the core definitions of minimum mean square error estimation under linear Gaussian assumptions and must explicitly connect the EWMA recursion to the steady-state form of the optimal linear estimator for the given model.\n- The constant drift $d$ is known and should be incorporated additively in the predictor; its presence does not alter the steady-state optimal gain that weights $y_t$ relative to the prior.\n\nTest suite:\nUse the following parameter sets. Each test case is a tuple $(q,r,d,T,\\text{seed})$, where $T$ is the simulation length and $\\text{seed}$ is a pseudo-random seed for any internal simulation you may perform. The required output does not depend on simulations, but these parameters are provided for completeness and coverage.\n- Case A (boundary, nearly static level): $(q=\\;0,\\; r=\\;1,\\; d=\\;0.05,\\; T=\\;200,\\; \\text{seed}=\\;42)$.\n- Case B (measurement dominated): $(q=\\;0.01,\\; r=\\;1.0,\\; d=\\;0.05,\\; T=\\;200,\\; \\text{seed}=\\;1)$.\n- Case C (process dominated): $(q=\\;1.0,\\; r=\\;0.01,\\; d=\\;0.0,\\; T=\\;200,\\; \\text{seed}=\\;7)$.\n- Case D (balanced): $(q=\\;0.2,\\; r=\\;0.2,\\; d=\\;0.1,\\; T=\\;500,\\; \\text{seed}=\\;2024)$.\n- Case E (noisy measurements): $(q=\\;0.5,\\; r=\\;5.0,\\; d=\\;-0.02,\\; T=\\;200,\\; \\text{seed}=\\;99)$.\n\nAnswer specification:\n- For each test case, compute the single float $\\alpha^\\star$ implied by your derivation for the given $(q,r)$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The ordering must match the test suite ordering above. For example, the output format should be exactly like $[\\alpha^\\star_A,\\alpha^\\star_B,\\alpha^\\star_C,\\alpha^\\star_D,\\alpha^\\star_E]$ with no extra whitespace or text.\n\nThere are no physical units or angles in this problem; report all numbers as dimensionless real values. All final answers must be floats.",
            "solution": "The problem requires the derivation of the optimal forgetting factor, $\\alpha^\\star$, for an Exponentially Weighted Moving Average (EWMA) forecaster. The criterion for optimality is the minimization of the one-step-ahead mean squared prediction error in steady state. The derivation must originate from first principles of optimal estimation for linear Gaussian systems.\n\nThe system is described by a linear Gaussian state-space model:\nState equation: $x_t = x_{t-1} + d + w_t$, with process noise $w_t \\sim \\mathcal{N}(0,q)$.\nMeasurement equation: $y_t = x_t + v_t$, with measurement noise $v_t \\sim \\mathcal{N}(0,r)$.\n\nHere, $x_t$ is the latent state, $y_t$ is the observation, $d$ is a known deterministic drift, and $q \\ge 0$ and $r  0$ are the variances of the process and measurement noise, respectively.\n\nThe EWMA forecaster provides a filtered estimate $\\hat{x}_t$ and a forecast $\\hat{y}_{t+1|t}$:\n$\\hat{x}_t = (1-\\alpha)(\\hat{x}_{t-1} + d) + \\alpha y_t$\n$\\hat{y}_{t+1|t} = \\hat{x}_t + d$\n\nThe fundamental principle for this problem is that for a linear Gaussian system, the Kalman filter provides the Minimum Mean Square Error (MMSE) estimate of the latent state. Therefore, the optimal forecaster must be equivalent to the Kalman forecaster in its steady-state form. We will derive the steady-state Kalman filter for the given system and equate its structure to the EWMA filter to find the optimal parameter $\\alpha^\\star$.\n\n**Step 1: Formulate the Kalman Filter**\n\nThe state-space model can be written in the standard form $x_t = Fx_{t-1} + Bu_{t-1} + w_t$ and $y_t = Hx_t + v_t$. For this specific scalar system, the matrices and inputs are:\n- State transition matrix: $F = 1$\n- Observation matrix: $H = 1$\n- Control input matrix: $B = 1$\n- Control input: $u_{t-1} = d$\n- Process noise variance: $Q = q$\n- Measurement noise variance: $R = r$\n\nThe Kalman filter operates in two steps: prediction and update. Let $\\hat{x}_{t|t-1}$ be the prior estimate of the state at time $t$ given observations up to $t-1$, and $\\hat{x}_{t|t}$ be the posterior estimate after incorporating the measurement at time $t$. Let $P_{t|t-1}$ and $P_{t|t}$ be the corresponding error variances, $P_{t|t-1} = E[(x_t - \\hat{x}_{t|t-1})^2]$ and $P_{t|t} = E[(x_t - \\hat{x}_{t|t})^2]$.\n\nThe Kalman filter equations are:\n\nPrediction Step:\n1.  State prediction: $\\hat{x}_{t|t-1} = F\\hat{x}_{t-1|t-1} + Bu_{t-1} = \\hat{x}_{t-1|t-1} + d$\n2.  Error covariance prediction: $P_{t|t-1} = FP_{t-1|t-1}F^T + Q = P_{t-1|t-1} + q$\n\nUpdate Step:\n3.  Kalman gain: $K_t = P_{t|t-1}H^T(HP_{t|t-1}H^T + R)^{-1} = P_{t|t-1}(P_{t|t-1} + r)^{-1}$\n4.  State update: $\\hat{x}_{t|t} = \\hat{x}_{t|t-1} + K_t(y_t - H\\hat{x}_{t|t-1}) = \\hat{x}_{t|t-1} + K_t(y_t - \\hat{x}_{t|t-1})$\n5.  Error covariance update: $P_{t|t} = (I - K_tH)P_{t|t-1} = (1 - K_t)P_{t|t-1}$\n\nNote that the deterministic drift $d$ affects the state estimates but not the error covariances or the Kalman gain, as is standard for linear systems.\n\n**Step 2: Connect the Kalman Filter to the EWMA filter**\n\nTo compare the Kalman filter with the given EWMA recursion, we combine the state prediction and update equations into a single expression for the posterior estimate $\\hat{x}_{t|t}$.\nSubstituting $\\hat{x}_{t|t-1} = \\hat{x}_{t-1|t-1} + d$ into the state update equation gives:\n$\\hat{x}_{t|t} = (\\hat{x}_{t-1|t-1} + d) + K_t(y_t - (\\hat{x}_{t-1|t-1} + d))$\nRearranging the terms, we get:\n$\\hat{x}_{t|t} = (1 - K_t)(\\hat{x}_{t-1|t-1} + d) + K_t y_t$\n\nNow, compare this with the EWMA filter's update rule:\n$\\hat{x}_t = (1-\\alpha)(\\hat{x}_{t-1} + d) + \\alpha y_t$\n\nBy direct inspection, the Kalman filter's posterior update has the exact same functional form as the EWMA update. The correspondence is:\n- EWMA filtered state $\\hat{x}_t \\Leftrightarrow$ Kalman posterior state estimate $\\hat{x}_{t|t}$\n- EWMA forgetting factor $\\alpha \\Leftrightarrow$ Kalman gain $K_t$\n\nThe one-step-ahead forecast in the Kalman framework is $\\hat{y}_{t+1|t} = H\\hat{x}_{t+1|t} = \\hat{x}_{t|t} + d$. This also matches the EWMA forecaster's definition $\\hat{y}_{t+1|t} = \\hat{x}_t + d$. The objective is to minimize the steady-state mean squared prediction error $E[(y_{t+1}-\\hat{y}_{t+1|t})^2]$. This error is $E[((x_{t+1}-\\hat{x}_{t+1|t})+v_{t+1})^2] = E[(x_{t+1}-\\hat{x}_{t+1|t})^2] + E[v_{t+1}^2] = P_{t+1|t} + r$. Minimizing this quantity is equivalent to minimizing the prediction error variance $P_{t+1|t}$.\n\n**Step 3: Derive the Optimal Steady-State Forgetting Factor $\\alpha^\\star$**\n\nThe optimal forgetting factor $\\alpha^\\star$ is the one that minimizes the prediction error in steady state. This corresponds to the steady-state Kalman gain, $K = \\lim_{t\\to\\infty} K_t$. In steady state, the error covariances converge to constant values, $P_{t|t-1} \\to P$ and $P_{t|t} \\to P_{\\text{post}}$, and the gain becomes constant, $K_t \\to K$.\n\nThe recursive equations for the error covariances in steady state become:\n(i) $P = P_{\\text{post}} + q$\n(ii) $P_{\\text{post}} = (1 - K)P$\n\nSubstituting (ii) into (i):\n$P = (1 - K)P + q$\n$P - (1-K)P = q$\n$KP = q$\n\nThe steady-state gain $K$ is related to the steady-state prediction variance $P$ by:\n$K = P / (P+r)$\nThis can be rearranged to express $P$ in terms of $K$:\n$K(P+r) = P \\implies KP + Kr = P \\implies Kr = P(1-K) \\implies P = \\frac{Kr}{1-K}$ (for $K \\neq 1$)\n\nNow, we substitute this expression for $P$ into the equation $KP = q$:\n$K \\left(\\frac{Kr}{1-K}\\right) = q$\n$K^2 r = q(1-K)$\n$rK^2 + qK - q = 0$\n\nThis is a quadratic equation for the steady-state Kalman gain $K$, which is our desired optimal forgetting factor $\\alpha^\\star$. We solve for $K$ using the quadratic formula:\n$K = \\frac{-q \\pm \\sqrt{q^2 - 4(r)(-q)}}{2r} = \\frac{-q \\pm \\sqrt{q^2 + 4qr}}{2r}$\n\nSince the forgetting factor $\\alpha$ (and thus the Kalman gain $K$) must be non-negative (as $r0$, $P \\ge 0$), we must choose the positive root:\n$\\alpha^\\star = K = \\frac{-q + \\sqrt{q^2 + 4qr}}{2r}$\n\nThis is the final expression for the optimal forgetting factor. It is a function of only the process noise variance $q$ and measurement noise variance $r$, as expected. The derivation started from the MMSE principle embedded in the Kalman filter and explicitly connected the optimal filter structure to the EWMA form, as required.\n\nThis can be verified for the boundary case $q=0$. If there is no process noise, the state evolves deterministically. The formula gives $\\alpha^\\star = (-0 + \\sqrt{0})/(2r) = 0$. This is correct: the optimal filter should ignore new measurements ($\\alpha=0$) and simply follow the deterministic model $\\hat{x}_t = \\hat{x}_{t-1} + d$.\nFor the case where $r \\to 0$ (not allowed by $r0$ but for intuition), $\\alpha^\\star \\to 1$, meaning we should trust the noise-free measurement completely.",
            "answer": "[0.0,0.09512489497563178,0.9901960784313726,0.6180339887498948,0.2701562118716424]"
        },
        {
            "introduction": "A truly useful forecast often quantifies its own uncertainty, moving beyond a single point value to a full probability distribution. This practice addresses the critical task of evaluating such probabilistic forecasts, which are essential for risk management in grid operations. You will learn to decompose the Brier score, a standard metric for binary events, into its constituent parts—reliability, resolution, and uncertainty—thereby gaining a much richer and more actionable assessment of a forecast model's performance .",
            "id": "4082660",
            "problem": "A distribution system operator produces day-ahead probabilistic forecasts for a binary event on a medium-voltage feeder: the evening peak net-load exceeding a fixed threshold. For each day $t \\in \\{1,\\dots,N\\}$, the forecasting system outputs a probability $p_t \\in [0,1]$ for the event, and the realized binary outcome is $y_t \\in \\{0,1\\}$. You are provided an evaluation sample of $N = 200$ days. To analyze calibration and decompose the Brier score, the forecasts have been grouped into $K=5$ non-overlapping probability bins with the following summary statistics, where $p_i$ is the representative forecast probability for bin $i$, $N_i$ is the number of forecasts in bin $i$, and $e_i$ is the number of observed exceedance events in bin $i$:\n\n- Bin $1$: $p_1 = 0.05$, $N_1 = 40$, $e_1 = 4$.\n- Bin $2$: $p_2 = 0.20$, $N_2 = 60$, $e_2 = 15$.\n- Bin $3$: $p_3 = 0.40$, $N_3 = 50$, $e_3 = 17$.\n- Bin $4$: $p_4 = 0.70$, $N_4 = 30$, $e_4 = 18$.\n- Bin $5$: $p_5 = 0.90$, $N_5 = 20$, $e_5 = 18$.\n\nStarting from the definition of the Brier score as the mean squared error between the probabilistic forecast and the binary outcome, and using only fundamental probability laws (for example, the law of total expectation and the law of total variance), do the following:\n\n1. Derive the calibration curve (reliability diagram) for this binned dataset by expressing, for each bin $i$, the empirical conditional event frequency $\\hat{o}_i$ as a function of the forecast probability $p_i$ and the bin counts. Explain how this curve relates to perfect calibration.\n\n2. Derive the finite-sample decomposition of the Brier score into reliability, resolution, and uncertainty components for this binned dataset in terms of $\\{p_i, N_i, e_i\\}_{i=1}^{K}$, beginning from first principles, without invoking any result that is not justified by the aforementioned probability laws and the definition of the Brier score. Identify each term conceptually.\n\n3. Compute numerically each component and the resulting Brier score for the provided data.\n\nReport only the Brier score as a unitless decimal rounded to four significant figures in your final answer.",
            "solution": "The problem as stated is subjected to validation.\n\n### Step 1: Extract Givens\n- A probabilistic forecast system for a binary event $y_t \\in \\{0, 1\\}$ produces a probability $p_t \\in [0, 1]$ for each day $t$.\n- Total number of days in the evaluation sample: $N=200$.\n- The data is grouped into $K=5$ non-overlapping probability bins.\n- For each bin $i \\in \\{1, \\dots, 5\\}$, the following statistics are provided:\n    - $p_i$: representative forecast probability for bin $i$.\n    - $N_i$: number of forecasts in bin $i$.\n    - $e_i$: number of observed events (where $y_t=1$) in bin $i$.\n- Bin data:\n    - Bin $1$: $p_1 = 0.05$, $N_1 = 40$, $e_1 = 4$.\n    - Bin $2$: $p_2 = 0.20$, $N_2 = 60$, $e_2 = 15$.\n    - Bin $3$: $p_3 = 0.40$, $N_3 = 50$, $e_3 = 17$.\n    - Bin $4$: $p_4 = 0.70$, $N_4 = 30$, $e_4 = 18$.\n    - Bin $5$: $p_5 = 0.90$, $N_5 = 20$, $e_5 = 18$.\n- The Brier score is defined as the mean squared error between the forecast and the outcome.\n- The derivation must begin from this definition and use only fundamental probability laws.\n- The tasks are: (1) Derive the calibration curve, (2) Derive the Brier score decomposition, (3) Compute the numerical components and the total Brier score.\n- The final reported answer is the numerical Brier score rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem is grounded in the established statistical theory of probabilistic forecast verification. The Brier score and its decomposition into reliability, resolution, and uncertainty are standard tools for evaluating binary event forecasts. The context of smart grid analytics is appropriate and scientifically sound.\n- **Well-Posed:** All necessary data ($N_i$, $p_i$, $e_i$) are provided to perform the required derivations and computations. The tasks are clearly stated, leading to a unique numerical answer for the Brier score.\n- **Objective:** The problem is stated using precise, quantitative language, free of ambiguity or subjective claims.\n- **Incomplete or Contradictory Setup:** The problem is self-contained. A consistency check confirms that the sum of forecasts in all bins equals the total number of forecasts: $\\sum_{i=1}^{5} N_i = 40 + 60 + 50 + 30 + 20 = 200 = N$. The number of events in each bin, $e_i$, is less than or equal to the number of forecasts, $N_i$, as required.\n- **Unrealistic or Infeasible:** The provided data values are plausible for a real-world forecast system evaluation.\n- **Other Flaws:** The problem is not ill-posed, pseudo-profound, trivial, or unverifiable. It requires a rigorous derivation from first principles, demonstrating a fundamental understanding of the topic.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n#### 1. Calibration Curve (Reliability Diagram)\n\nFor a particular bin $i$, all $N_i$ forecasts are represented by the single probability value $p_i$. The empirical or observed frequency of the event for this bin is the ratio of the number of observed events, $e_i$, to the total number of forecasts in that bin, $N_i$. We denote this empirical conditional event frequency as $\\hat{o}_i$.\n\n$$\n\\hat{o}_i = \\frac{e_i}{N_i}\n$$\n\nA calibration curve, also known as a reliability diagram, is a plot of the observed frequency $\\hat{o}_i$ against the forecast probability $p_i$ for each bin. A perfectly calibrated or reliable forecasting system would exhibit $\\hat{o}_i = p_i$ for all $i$. Such points would lie on the diagonal line where the ordinate equals the abscissa. Deviations from this diagonal indicate miscalibration.\n\nFor the given data, we compute the observed frequencies for each bin:\n- Bin $1$: $\\hat{o}_1 = \\frac{e_1}{N_1} = \\frac{4}{40} = 0.10$\n- Bin $2$: $\\hat{o}_2 = \\frac{e_2}{N_2} = \\frac{15}{60} = 0.25$\n- Bin $3$: $\\hat{o}_3 = \\frac{e_3}{N_3} = \\frac{17}{50} = 0.34$\n- Bin $4$: $\\hat{o}_4 = \\frac{e_4}{N_4} = \\frac{18}{30} = 0.60$\n- Bin $5$: $\\hat{o}_5 = \\frac{e_5}{N_5} = \\frac{18}{20} = 0.90$\n\nThe calibration curve consists of the points $(p_i, \\hat{o}_i)$: $(0.05, 0.10)$, $(0.20, 0.25)$, $(0.40, 0.34)$, $(0.70, 0.60)$, and $(0.90, 0.90)$. The forecaster appears to underpredict for low probabilities (e.g., bin 1 where $0.10  0.05$) and overpredict for mid-to-high probabilities (e.g., bin 4 where $0.60  0.70$).\n\n#### 2. Brier Score Decomposition Derivation\n\nThe Brier score ($BS$) is defined as the mean squared error between the probabilistic forecasts $p_t$ and the binary outcomes $y_t$:\n\n$$\nBS = \\frac{1}{N} \\sum_{t=1}^{N} (p_t - y_t)^2\n$$\n\nSince the forecasts are grouped into $K$ bins, we can partition the sum over the bins. Within each bin $i$, all forecasts are represented by $p_i$. The sum becomes:\n\n$$\nBS = \\frac{1}{N} \\sum_{i=1}^{K} \\sum_{t \\in \\text{bin } i} (p_i - y_t)^2\n$$\n\nFor a given bin $i$, there are $e_i$ instances where $y_t=1$ and $N_i - e_i$ instances where $y_t=0$. The inner sum can thus be explicitly written as:\n\n$$\n\\sum_{t \\in \\text{bin } i} (p_i - y_t)^2 = e_i (p_i - 1)^2 + (N_i - e_i) (p_i - 0)^2\n$$\n\nSubstituting this back into the $BS$ expression:\n\n$$\nBS = \\frac{1}{N} \\sum_{i=1}^{K} \\left[ e_i (p_i - 1)^2 + (N_i - e_i) p_i^2 \\right]\n$$\n\nTo decompose this, we rearrange the term inside the summation by dividing and multiplying by $N_i$:\n\n$$\nBS = \\frac{1}{N} \\sum_{i=1}^{K} N_i \\left[ \\frac{e_i}{N_i} (p_i - 1)^2 + \\left(1 - \\frac{e_i}{N_i}\\right) p_i^2 \\right]\n$$\n\nUsing the definition $\\hat{o}_i = e_i/N_i$:\n\n$$\nBS = \\frac{1}{N} \\sum_{i=1}^{K} N_i \\left[ \\hat{o}_i (p_i^2 - 2p_i + 1) + p_i^2 - \\hat{o}_i p_i^2 \\right]\n= \\frac{1}{N} \\sum_{i=1}^{K} N_i \\left[ p_i^2 - 2p_i\\hat{o}_i + \\hat{o}_i \\right]\n$$\n\nBy adding and subtracting $\\hat{o}_i^2$ inside the brackets, we can complete the square:\n\n$$\nBS = \\frac{1}{N} \\sum_{i=1}^{K} N_i \\left[ (p_i^2 - 2p_i\\hat{o}_i + \\hat{o}_i^2) + \\hat{o}_i - \\hat{o}_i^2 \\right]\n= \\frac{1}{N} \\sum_{i=1}^{K} N_i \\left[ (p_i - \\hat{o}_i)^2 + \\hat{o}_i(1 - \\hat{o}_i) \\right]\n$$\n\nThis separates the expression into two terms:\n\n$$\nBS = \\frac{1}{N} \\sum_{i=1}^{K} N_i (p_i - \\hat{o}_i)^2 + \\frac{1}{N} \\sum_{i=1}^{K} N_i \\hat{o}_i(1 - \\hat{o}_i)\n$$\n\nThe first term is the **Reliability** component ($REL$). It is the weighted average of the squared differences between the forecast probabilities and the observed frequencies in each bin. It measures the calibration of the forecast; for a perfectly reliable forecast, $p_i = \\hat{o}_i$ for all $i$, and $REL = 0$. A lower value is better.\n\n$$\nREL = \\frac{1}{N} \\sum_{i=1}^{K} N_i (p_i - \\hat{o}_i)^2\n$$\n\nThe second term can be further decomposed by introducing the overall mean outcome, or climatology, $\\bar{o} = \\frac{1}{N} \\sum_{t=1}^N y_t = \\frac{1}{N} \\sum_{i=1}^K e_i = \\frac{1}{N} \\sum_{i=1}^K N_i \\hat{o}_i$.\nWe use the law of total variance for the binary outcome variable $Y$. The variance of $Y$ is $Var(Y) = E[(Y - E[Y])^2]$. For a binary variable with mean $\\bar{o}$, the variance is $\\bar{o}(1-\\bar{o})$. The law of total variance states: $Var(Y) = E[Var(Y|X)] + Var(E[Y|X])$, where $X$ is a conditioning variable (here, the binned forecast probability).\n\n- The total variance of the outcome is the **Uncertainty** ($UNC$), which is independent of the forecast and depends only on the inherent variability of the event.\n  $$UNC = \\bar{o}(1 - \\bar{o})$$\n\n- The term $Var(E[Y|X])$ represents the variance of the conditional expectations. For our binned data, this is the weighted variance of the conditional observed frequencies $\\hat{o}_i$ around the overall mean $\\bar{o}$. This is the **Resolution** component ($RES$). It measures the forecast's ability to resolve the sample into subsamples with different outcome frequencies. High resolution (large differences between $\\hat{o}_i$ and $\\bar{o}$) is desirable. A higher value is better.\n  $$RES = \\frac{1}{N} \\sum_{i=1}^{K} N_i (\\hat{o}_i - \\bar{o})^2$$\n\n- The term $E[Var(Y|X)]$ is the expected value of the conditional variances. For our binned data, the variance within bin $i$ is $\\hat{o}_i(1-\\hat{o}_i)$. The expectation is the weighted average over all bins.\n  $$E[Var(Y|X)] = \\frac{1}{N} \\sum_{i=1}^{K} N_i \\hat{o}_i(1 - \\hat{o}_i)$$\n\nFrom the law of total variance, $E[Var(Y|X)] = UNC - RES$. Comparing this with our derived expression for $BS$, we see that the second term is indeed $UNC - RES$.\nTherefore, the full decomposition is:\n\n$$\nBS = REL - RES + UNC\n$$\n\n#### 3. Numerical Computation\n\nFirst, we compute the necessary summary statistics.\n- Total events: $E = \\sum e_i = 4 + 15 + 17 + 18 + 18 = 72$.\n- Overall mean outcome (climatology): $\\bar{o} = \\frac{E}{N} = \\frac{72}{200} = 0.36$.\n- Observed frequencies $\\hat{o}_i$ were calculated in Part 1: $\\{0.10, 0.25, 0.34, 0.60, 0.90\\}$.\n\nNow, we compute each component:\n\n**Reliability ($REL$):**\n$$\nREL = \\frac{1}{200} \\left[ 40(0.05-0.10)^2 + 60(0.20-0.25)^2 + 50(0.40-0.34)^2 + 30(0.70-0.60)^2 + 20(0.90-0.90)^2 \\right]\n$$\n$$\nREL = \\frac{1}{200} \\left[ 40(-0.05)^2 + 60(-0.05)^2 + 50(0.06)^2 + 30(0.10)^2 + 20(0)^2 \\right]\n$$\n$$\nREL = \\frac{1}{200} \\left[ 40(0.0025) + 60(0.0025) + 50(0.0036) + 30(0.01) + 0 \\right]\n$$\n$$\nREL = \\frac{1}{200} \\left[ 0.1 + 0.15 + 0.18 + 0.3 \\right] = \\frac{0.73}{200} = 0.00365\n$$\n\n**Resolution ($RES$):**\n$$\nRES = \\frac{1}{200} \\sum_{i=1}^{5} N_i (\\hat{o}_i - 0.36)^2\n$$\n$$\nRES = \\frac{1}{200} \\left[ 40(0.10-0.36)^2 + 60(0.25-0.36)^2 + 50(0.34-0.36)^2 + 30(0.60-0.36)^2 + 20(0.90-0.36)^2 \\right]\n$$\n$$\nRES = \\frac{1}{200} \\left[ 40(-0.26)^2 + 60(-0.11)^2 + 50(-0.02)^2 + 30(0.24)^2 + 20(0.54)^2 \\right]\n$$\n$$\nRES = \\frac{1}{200} \\left[ 40(0.0676) + 60(0.0121) + 50(0.0004) + 30(0.0576) + 20(0.2916) \\right]\n$$\n$$\nRES = \\frac{1}{200} \\left[ 2.704 + 0.726 + 0.02 + 1.728 + 5.832 \\right] = \\frac{11.01}{200} = 0.05505\n$$\n\n**Uncertainty ($UNC$):**\n$$\nUNC = \\bar{o}(1 - \\bar{o}) = 0.36(1 - 0.36) = 0.36(0.64) = 0.2304\n$$\n\n**Brier Score ($BS$):**\nFinally, we compute the Brier score using the decomposition:\n$$\nBS = REL - RES + UNC = 0.00365 - 0.05505 + 0.2304 = 0.179\n$$\nThe question requires the answer rounded to four significant figures.\n$$\nBS \\approx 0.1790\n$$\nAs a verification, the Brier Score can be computed directly:\n$BS = \\frac{1}{N} \\sum_{i=1}^{K} [e_i(p_i-1)^2 + (N_i-e_i)p_i^2]$\n$BS = \\frac{1}{200} [4(-0.95)^2+36(0.05)^2 + 15(-0.8)^2+45(0.2)^2 + 17(-0.6)^2+33(0.4)^2 + 18(-0.3)^2+12(0.7)^2 + 18(-0.1)^2+2(0.9)^2]$\n$BS = \\frac{1}{200} [3.61+0.09 + 9.6+1.8 + 6.12+5.28 + 1.62+5.88 + 0.18+1.62] = \\frac{35.8}{200} = 0.179$.\nThe direct calculation confirms the result from the decomposition. The Brier score for this forecast system over the evaluation period is $0.1790$.",
            "answer": "$$\n\\boxed{0.1790}\n$$"
        }
    ]
}