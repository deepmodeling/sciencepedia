## 引言
随着可再生能源的整合和电网结构的日益复杂，对[电力](@entry_id:264587)系统进行快速、准确的监测与故障诊断，已成为保障能源安全与可靠性的核心挑战。传统的监控方法难以充分利用由[相量测量单元](@entry_id:1129603)（PMU）等现代传感器产生的海量、高速数据流，而深度学习为此提供了强大的数据驱动解决方案。

本文旨在系统性地介绍如何应用深度学习技术解决这一挑战。我们将分三步展开：
1.  在 **“原理与机制”** 一章中，我们将深入剖析捕捉电网时空动态的核心模型（如TCN、GNN）及其背后的科学原理。
2.  接着，在 **“应用与跨学科连接”** 一章中，我们将展示这些技术如何与[电力](@entry_id:264587)[系统工程](@entry_id:180583)实践相结合，以解决状态估计、自适应保护等具体问题。
3.  最后，通过 **“动手实践”** 环节，您将有机会将理论知识应用于解决实际问题。

本文将为您构建一个从理论到实践的完整知识框架，助您掌握利用深度学习提升电网智能化的前沿方法。让我们首先从[深度学习](@entry_id:142022)应用于电网监测的基础——其核心原理与机制开始。

## 原理与机制

本章在前一章介绍的基础上，深入探讨了将深度学习应用于电网监测与故障诊断的核心科学原理和技术机制。我们将从电网监测所需的数据基础开始，系统地剖析用于建模电网时空动态的关键[深度学习架构](@entry_id:634549)，并进一步讨论旨在提升[模型鲁棒性](@entry_id:636975)、物理一致性和可靠性的高级方法论。本章旨在为读者构建一个坚实而全面的理论框架，以理解和开发先进的、数据驱动的电网智能分析系统。

### 数据驱动的电网监测基础

深度学习模型的性能在很大程度上取决于其所使用数据的质量和特性。在电网监测领域，传感技术的选择直接决定了我们捕捉电网动态行为的能力。

#### 来自[相量测量单元](@entry_id:1129603)（PMU）的高保真数据

现代电网监测的基石是**[相量测量单元](@entry_id:1129603)（Phasor Measurement Unit, PMU）**。与传统的**监控和数据采集（Supervisory Control And Data Acquisition, SCADA）** 系统相比，PMU在数据的时间分辨率和同步性方面提供了革命性的提升。S[CAD](@entry_id:157566)A系统通常以较低的频率（例如，每2到10秒一次）采集电压幅值、有功功率等[稳态](@entry_id:139253)量的快照，且各测量点之间缺乏精确的时间同步。

与之相对，PMU通过将电网的交流电压和电流[波形建模](@entry_id:756631)为准[正弦信号](@entry_id:196767)，来提取**[同步相量](@entry_id:1132786)（synchrophasor）**。例如，一个电压波形 $v_i(t)$ 可以表示为 $v_i(t) \approx \Re\{V_i(t) e^{\mathrm{j} 2\pi f_0 t}\}$，其中 $V_i(t) \in \mathbb{C}$ 是一个缓慢变化的复数包络，即相量。PMU能够以极高的[采样率](@entry_id:264884)（典型值为每秒30至120次）报告这些相量，以及系统频率 $f_t$ 和[频率变化率](@entry_id:1131088)（Rate Of Change Of Frequency, [ROCOF](@entry_id:1131088)）$\dot{f}_t$。至关重要的是，所有PMU都通过**全球定位系统（GPS）** 接收时间信号，将其测量值标记到**协调世界时（UTC）**，实现了微秒级别的同步精度。

这种高[采样率](@entry_id:264884)和高精度同步性对于观测快速暂态事件（如持续时间约为 $100\,\mathrm{ms}$ 的电网故障）至关重要。根据奈奎斯特-香农采样定理，要准确捕捉一个事件，[采样频率](@entry_id:264884)必须远高于事件动态变化的频率。PMU的采样间隔（例如，在60Hz下约为 $16.7\,\mathrm{ms}$）远小于典型故障的持续时间，因此可以在一次故障期间捕获多个数据点，从而形成对事件演化过程的“高清录像”。而SCADA系统的采样间隔长达数秒，[几乎必然](@entry_id:262518)会因为**混叠（aliasing）**而错过故障的暂态过程。此外，PMU的精确时间同步使得分析人员能够比较来自电网不同地理位置的数据，以重构故障的起始时间和空间传播路径，而S[CAD](@entry_id:157566)A数据因存在不可预测的时间戳偏差而无法实现这一点 。

#### 为[深度学习](@entry_id:142022)构建[特征向量](@entry_id:151813)

为了将PMU提供的数据输入深度学习模型，我们需要将其构建成实值[特征向量](@entry_id:151813)。一个典型的PMU时间序列向量 $x_t \in \mathbb{R}^d$ 的构建方式如下：

1.  **[相量表示](@entry_id:196506)**：对于每个复数[相量](@entry_id:270266)（如电压 $V_i(t)$ 或电流 $I_{ij}(t)$），我们通常使用其**笛卡尔坐标**，即实部和虚部 $(\Re\{V_i(t)\}, \Im\{V_i(t)\})$，而非极坐标（幅值和相角）。这是因为相角存在从 $\pi$ 到 $-\pi$ 的环绕（wrap-around）[不连续性](@entry_id:144108)，这会给[基于梯度的优化](@entry_id:169228)算法带来困难，而实部和虚部则是平滑变化的函数，是更稳定的表示。

2.  **标量测量**：系统频率 $f_t$ 及其变化率 $\dot{f}_t$ 是反映电网[动态平衡](@entry_id:136767)的关键指标，特别是对于由[发电机](@entry_id:268282)-负荷不平衡引起的故障。它们是实数值，应直接包含在[特征向量](@entry_id:151813)中。

3.  **向量拼接**：最后，我们将来自电网中所有被监测母线（集合 $\mathcal{B}$）和支路（集合 $\mathcal{E}$）的测量值拼接成一个大的[特征向量](@entry_id:151813)。因此，在时刻 $t$ 的完整[特征向量](@entry_id:151813) $x_t$ 的维度为 $d = 2|\mathcal{B}| + 2|\mathcal{E}| + 2$。这个高维、高保真的[时间序列数据](@entry_id:262935)流构成了后续所有[深度学习模型](@entry_id:635298)的基础 。

### 建模时空动态

电网故障是一个在时间和空间上均表现出复杂动态的过程。因此，有效的深度学习模型必须能够同时捕捉电网状态的时间演化和其在物理拓扑结构上的空间传播。

#### 捕捉时间依赖性：从循环到卷积与注意力

传统上，[时间序列建模](@entry_id:1133184)依赖于**[循环神经网络](@entry_id:634803)（Recurrent Neural Networks, RNNs）** 及其变体，如**[长短期记忆网络](@entry_id:635790)（Long Short-Term Memory, [LSTM](@entry_id:635790)）**。[LSTM](@entry_id:635790)通过其[门控机制](@entry_id:152433)和循环状态传递，能够学习时间序列中的依赖关系。然而，其固有的顺序处理方式和近似指数衰减的记忆核，使其在捕捉极长程或特定周期性依赖时存在一定的局限性，其**[归纳偏置](@entry_id:137419)（inductive bias）** 更倾向于近期信息。近年来，基于[卷积和](@entry_id:263238)注意力的非循环架构在时间序列分析中展现出强大的能力。

##### [时间卷积网络](@entry_id:1132914) (TCN)

**[时间卷积网络](@entry_id:1132914)（Temporal Convolutional Network, TCN）** 是一种专为[序列数据](@entry_id:636380)设计的卷积架构，它通过堆叠**因果[扩张卷积](@entry_id:636365)（causal dilated convolutions）** 来有效捕捉长程时间依赖性。

一个TCN层的核心操作是[扩张卷积](@entry_id:636365)，其定义为：
$$ y_k[t] = \sum_{i=0}^{K-1} w^{(k)}_i x[t - i \cdot d_k] $$
其中，$K$ 是卷积核大小，$d_k$ 是第 $k$ 层的扩张因子，$w^{(k)}$ 是可学习的[卷积核](@entry_id:1123051)权重。扩张操作相当于在卷积核的元素之间插入 $d_k-1$ 个零，使得卷积核可以跨越更广的输入范围。为了保证**因果性**（即在时间 $t$ 的输出仅依赖于 $t$ 及之前时间的输入），TCN在计算前对输入序列进行**左填充（left padding）**，填充的长度为 $(K-1)d_k$。

TCN的强大之处在于其**感受野（receptive field）** 的增长方式。对于一个由 $L$ 层卷积堆叠而成的TCN，其总[感受野大小](@entry_id:634995) $R$ 为：
$$ R = 1 + \sum_{k=1}^{L} (K-1)d_k $$
通过采用[指数增长](@entry_id:141869)的扩张因[子序列](@entry_id:147702)（例如，$d_k = 2^{k-1}$），感受野可以随层数 $L$ 呈指数级增长。例如，对于一个核大小 $K=3$ 的TCN，其[感受野大小](@entry_id:634995)为 $R = 2^{L+1} - 1$。这意味着仅用少量层数就能覆盖非常长的时间窗口。例如，对于一个采样率为 $100\,\mathrm{Hz}$ 的PMU信号，为了分析故障后 $0.5\,\mathrm{s}$ （即50个采样点）的动态，只需一个5层的TCN（$L=5$），其[感受野](@entry_id:636171)便可达到 $2^{5+1}-1 = 63$ 个时间步，足以完整覆盖该时间窗口 。这种非循[环的结构](@entry_id:150907)不仅高效，还避免了RNN中的梯度消失/爆炸问题。

##### Transformer网络

**Transformer** 网络是另一种强大的序列模型，其核心是**[自注意力机制](@entry_id:638063)（self-attention mechanism）**。标准的[自注意力机制](@entry_id:638063)是**置换不变的（permutation-invariant）**，即它将输入视为一个集合而非序列，这使其无法直接处理时间序列。为了解决这个问题，我们需要引入位置信息。

对于电网振荡检测等任务，**[正弦位置编码](@entry_id:637792)（sinusoidal positional encoding）** 是一种特别有效的选择。它为每个时间步 $t$ 分配一个固定的编码向量 $p_t \in \mathbb{R}^{d_{\text{model}}}$，其分量定义为：
$$
p_{t,2k} = \sin(t/\tau_k), \quad p_{t,2k+1} = \cos(t/\tau_k)
$$
其中 $\{\tau_k\}$ 是一组几何间隔的波长。这种编码方式的一个关键特性是，任意两个位置 $t$ 和 $s$ 的[位置编码](@entry_id:634769)的点积 $p_t \cdot p_s$ 仅是其相对距离 $t-s$ 的函数：
$$
p_t \cdot p_s = \sum_{k} \cos\left(\frac{t-s}{\tau_k}\right)
$$
由于注意力分数是通过查询（Query）和键（Key）向量的点积计算的，这种特性使得[注意力机制](@entry_id:917648)天然地具有了检测特定时间滞后（lag）下周期性相似性的[归纳偏置](@entry_id:137419)，这对于识别具有特定频率的电网振荡极为有利。

对于在线故障诊断等实时应用，模型不能使用未来的信息。这可以通过在注意力分数矩阵上应用一个**因果掩码（causal mask）** 来实现，该掩码将所有未来位置的注意力权重设为零（或在计算softmax前设为 $-\infty$）。结合了[位置编码](@entry_id:634769)和因果掩码的Transformer层，其计算过程为：
$$
\mathrm{Attn}(H) = \mathrm{softmax}\! \left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V
$$
其中 $Q, K, V$ 是输入 $H$ 的线性投影，$M$ 是因果掩码矩阵。与[LSTM](@entry_id:635790)倾向于近期记忆的偏置不同，Transformer通过其点积相似性机制，可以直接比较和关联序列中任意两个位置的模式，为捕捉周期性和[长程依赖](@entry_id:181727)提供了更直接和强大的机制 。

#### 融入物理结构：[图神经网络 (GNN)](@entry_id:750014)

[电力](@entry_id:264587)网络本质上是一个图结构，其中母线是节点，输电线路是边。**[图神经网络](@entry_id:136853)（Graph Neural Network, GNN）** 为利用这种固有的拓扑信息提供了一个强大的框架。通过在图上传递和聚合信息，GNN可以学习到节点（母线）状态如何受其邻居影响的模式。

##### 物理一致的消息传递

为了使GNN的学习过程与电网的物理现实保持一致，其[消息传递](@entry_id:751915)机制应该反映电网的基本物理定律。根据欧姆定律和基尔霍夫电流定律，可以推导出描述电网[稳态](@entry_id:139253)行为的**节点分析方程**：
$$ I_i = \left(\sum_{j \in \mathcal{N}(i)} Y_{ij} + Y_{i,\text{sh}}\right) V_i - \sum_{j \in \mathcal{N}(i)} Y_{ij} V_j $$
其中，$I_i$ 和 $V_i$ 分别是母线 $i$ 的注入电流和电压[相量](@entry_id:270266)，$\mathcal{N}(i)$ 是母线 $i$ 的邻居集合，$Y_{ij} \in \mathbb{C}$ 是连接母线 $i$ 和 $j$ 的线路的复数**导纳（admittance）**。这个方程明确指出，母线 $i$ 的状态（由 $I_i$ 和 $V_i$ 描述）与其邻居母线的状态（$V_j$）之间的[耦合强度](@entry_id:275517)和相位关系，直接由线路导纳 $Y_{ij}$ 决定。导纳越大（即阻抗越小），电气连接越强，扰动（如故障）的传播影响也越显著。

因此，一个物理一致的GNN层应该将线路导纳 $Y_{ij}$ 作为其消息聚合的权重。一个典型的**物理信息引导的GNN (physics-informed GNN)** 更新规则可以定义为：
$$
h_i^{(\ell+1)} = \sigma\left( W_1 h_i^{(\ell)} + \sum_{j \in \mathcal{N}(i)} Y_{ij} W_2 h_j^{(\ell)} \right)
$$
其中 $h_i^{(\ell)}$ 是母线 $i$ 在第 $\ell$ 层的隐藏状态，$W_1$ 和 $W_2$ 是可学习的权重矩阵，$\sigma$ 是[非线性激活函数](@entry_id:635291)。由于 $Y_{ij}$ 是复数，而神经网络通常在[实数域](@entry_id:151347)上操作，这个[复数乘法](@entry_id:167843)需要通过作用于实部-虚部拼接向量的实线性算子来实现。这种架构将物理耦合直接嵌入到网络结构中，相比于使用不正确的权重（如阻抗大小 $|Z_{ij}|$ 或仅电阻 $R_{ij}$）或让模型从头学习任意边权重，具有更强的[归纳偏置](@entry_id:137419)和更高的数据效率 。

##### 用于[电网状态估计](@entry_id:1125806)的[图卷积网络](@entry_id:194500) (GCN)

**[图卷积网络](@entry_id:194500)（Graph Convolutional Network, GCN）** 是GNN的一种流行变体，其层更新规则可以被解释为图上的一个**低通滤波器（low-pass filter）**，执行**拓扑平滑（topological smoothing）**。一个标准的GCN层定义如下：
$$ H^{(l+1)} = \sigma\left(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)}\right) $$
在这个公式中，$H^{(l)}$ 是第 $l$ 层的节[点特征](@entry_id:155984)矩阵。关键算子 $\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$ 被称为**对称归一化的邻接矩阵**。在电网应用中，[邻接矩阵](@entry_id:151010) $A$ 通常基于导纳的幅值来定义，即 $A_{ij} = |Y_{ij}|$，表示电气连接的强度。$\tilde{A} = A + I$ 是增加了自环的邻接矩阵，以确保节点在更新时也考虑自身的信息。$\tilde{D}$ 是 $\tilde{A}$ 的对角度矩阵。

这个操作的本质是对每个节点的特征与其邻居的特征进行加权平均。权重不仅与连接强度 $A_{ij}$ 成正比，还通过度矩阵 $\tilde{D}$ 进行了归一化，这可以防止高度数节点（[连接线](@entry_id:196944)路多的母线）的特征在数值上占据主导地位，从而稳定了学习过程。从物理角度看，这种平滑操作是合理的：通过强电气连接（高导纳）相连的母线，其状态（如电压、频率）在物理上也更趋于一致。因此，GCN的平滑操作与电网中扰动传播的物理现实相符，使得模型能够以一种物理上有意义的方式聚合关于局部事件（如故障）的信息 。

### 鲁棒与可靠学习的高级方法论

为了在实际应用中部署可靠的深度学习系统，除了设计有效的[网络架构](@entry_id:268981)外，还必须解决一系列高级挑战，包括如何保证模型的物理一致性、如何处理数据中的固有偏差，以及如何量化和理解模型的预测不确定性。

#### 强制物理一致性：[物理信息](@entry_id:152556)引导的[损失函数](@entry_id:634569)

除了在模型架构中嵌入物理知识，我们还可以通过设计**[物理信息](@entry_id:152556)引导的损失函数（physics-informed loss function）**，在训练过程中直接对违反物理定律的预测进行惩罚。这种方法属于**[物理信息神经网络](@entry_id:145229)（Physics-Informed Neural Networks, PINNs）** 的范畴。

对于交流电网，一个基本的物理约束是**功率平衡方程**。在任意一个非松弛母线（non-slack bus）$i$ 处，其计算出的有功功率注入 $\hat{P}_i$ 必须等于其计划的有功功率注入 $P_i^{\text{sched}}$。有功功率的计算公式为：
$$ P_i = \sum_{j \in \mathcal{N}} V_i V_j (G_{ij}\cos(\theta_i - \theta_j) + B_{ij}\sin(\theta_i - \theta_j)) $$
其中 $Y_{ij} = G_{ij} + jB_{ij}$。我们可以定义一个**功率残差（power residual）** $r_i = \hat{P}_i - P_i^{\text{sched}}$，其中 $\hat{P}_i$ 是使用神经网络预测的电压幅值 $\hat{V}$ 和相角 $\hat{\theta}$ 计算得出的。训练的目标之一就是驱动这个残差趋近于零。需要注意的是，这个约束不适用于作为[系统平衡](@entry_id:1132826)点的松弛母线（slack bus），并且为了得到唯一的相角解，必须固定一个参考相角（如 $\theta_{\text{slack}} = 0$）。

**[增广拉格朗日方法](@entry_id:165608)（Augmented Lagrangian Method, ALM）** 为处理这类[等式约束](@entry_id:175290)提供了一个有原则的优化框架。结合标准的[监督学习](@entry_id:161081)损失 $\ell_{\text{sup}}$，ALM的总损失函数可以写成：
$$ \mathcal{L}(\phi, \lambda) = \mathbb{E}_{(x,y)}\left[\ell_{\text{sup}}(f_\phi(x), y)\right] + \sum_{i} \lambda_i r_i(\hat{V}, \hat{\theta}) + \frac{\rho}{2} \sum_{i} \frac{r_i(\hat{V}, \hat{\theta})^2}{\sigma_i^2} $$
这个损失函数包含三部分：监督损失项、与[拉格朗日乘子](@entry_id:142696)（[对偶变量](@entry_id:143282)）$\lambda_i$ 相关的线性惩罚项，以及二次惩罚项。其中，$\rho$ 是惩罚系数，$\sigma_i^2$ 是计划功率不确定性的方差，用于对残差进行归一化加权。[对偶变量](@entry_id:143282) $\lambda_i$ 在训练过程中通过梯度上升进行更新：$\lambda_i \leftarrow \lambda_i + \rho \mathbb{E}[r_i]$。这个框架将物理定律作为软约束整合到学习过程中，引导模型产生物理上可行的解 。

#### 应对[数据稀疏性](@entry_id:136465)：[故障检测](@entry_id:270968)中的[类别不平衡](@entry_id:636658)

在电网故障诊断中，一个普遍存在的挑战是**[类别不平衡](@entry_id:636658)（class imbalance）**：正常运行的数据样本（负类）远远多于故障样本（正类），但我们更关心对稀有故障事件的准确检测。标准的[经验风险最小化](@entry_id:633880)在这种情况下会偏向于多数类，导致对少数类的检测性能不佳。有几种策略可以应对这一挑战：

1.  **[重采样](@entry_id:142583)（Resampling）**：这是一种数据层面的方法，通过**过采样（oversampling）** 少数类样本或**欠采样（undersampling）** 多数类样本来改变训练数据的类别分布，使其[趋于平衡](@entry_id:150414)。例如，将类别比例调整为1:1。这样做可以改变[经验风险](@entry_id:633993)目标中各类别的权重，迫使模型同等重视所有类别。但需要注意，这种方法会改变模型的有效训练先验，可能导致模型输出的概率值与真实世界的数据分布不匹配，即概率**校准（calibration）** 失真 。

2.  **代价敏感学习（Cost-sensitive Learning）**：这是一种算法层面的方法，它在不改变数据分布的情况下，通过为不同类别的损失分配不同权重来解决不[平衡问题](@entry_id:636409)。例如，对于[交叉熵损失](@entry_id:141524)，可以将其修改为 $w_y L(f(x), y)$，其中 $w_y$ 是类别权重。通常，少数类的权重会被设得更高（例如，与类别频率成反比），这样来自少数类的样本在梯度更新中会产生更大的影响。这种方法直接将非对称的误分类代价（例如，漏报故障的代价远高于误报）整合到训练目标中 。

3.  **[焦点损失](@entry_id:634901)（Focal Loss）**：这是一种先进的[损失函数](@entry_id:634569)，旨在让模型在训练过程中专注于“困难”的样本。其定义为：
    $$ L_{\text{FL}}(p_t) = -\alpha_t (1 - p_t)^{\gamma} \log(p_t) $$
    其中 $p_t$ 是模型对正确类别的预测概率，$\alpha_t$ 是类别平衡权重，$\gamma \ge 0$ 是**聚焦参数**。关键在于调制因子 $(1 - p_t)^{\gamma}$。对于一个“简单”的、被模型高置信度正确分类的样本（$p_t \to 1$），该因子趋近于0，从而大大降低了该样本的损失贡献。而对于“困难”的、模型预测不准的样本（$p_t$ 较小），该因子接近于1，其损失贡献不受影响。这使得训练过程自动地将注意力集中在少数类和边界样本上。当 $\gamma=0$ 时，[焦点损失](@entry_id:634901)退化为标准的加权[交叉熵损失](@entry_id:141524) 。

#### 量化预测不确定性

在电网故障诊断等高风险决策场景中，模型不仅要做出准确的预测，还必须能够量化其预测的**不确定性（uncertainty）**。这对于评估预测的可靠性、避免过度自信的错误至关重要。预测不确定性主要分为两类：

1.  **[偶然不确定性](@entry_id:634772)（Aleatoric Uncertainty）**：源于数据生成过程中固有的、不可约减的随机性，例如传感器[测量噪声](@entry_id:275238)或故障过程本身的随机波动。它反映在给定输入 $x$ 和模型参数 $\mathbf{w}$ 下的[条件概率分布](@entry_id:163069) $p(y \mid x, \mathbf{w})$ 的方差中。这种不确定性即使在拥有无限多数据的情况下也无法消除。为了捕捉它，模型需要输出一个概率分布而不是一个[点估计](@entry_id:174544) 。

2.  **认知不确定性（Epistemic Uncertainty）**：源于模型自身的不确定性，即由于训练数据有限，我们无法唯一确定最优的模型参数 $\mathbf{w}$。它由参数的后验分布 $p(\mathbf{w} \mid \mathcal{D})$ 来描述。认知不确定性可以通过增加训练数据来降低，并且对于远离训练数据分布的**分布外（Out-of-Distribution, OOD）** 输入，其值通常会很高。

实践中，估计认知不确定性的常用方法包括：
- **[贝叶斯神经网络](@entry_id:746725)（Bayesian Neural Networks, BNNs）**：通过对模型权重施加先验分布，并使用贝叶斯推断（通常是[变分推断](@entry_id:634275)等近似方法）来计算权重的[后验分布](@entry_id:145605)。预测时，通过在[后验分布](@entry_id:145605)[上采样](@entry_id:275608)多组权重并取平均来得到最终结果，预测结果的方差即为认知不确定性的一种度量。
- **[深度集成](@entry_id:636362)（Deep Ensembles）**：训练多个（例如5-10个）结构相同但从不同随机初始化开始的独立模型。对于一个给定的输入，这组模型的预测结果之间的差异（方差）可以作为认知不确定性的一个实用且有效的估计。为了同时捕捉两种不确定性，集成中的每个模型都应输出一个概率分布 。

#### 跨电网泛化：[迁移学习](@entry_id:178540)与[领域自适应](@entry_id:637871)

在实际部署中，我们经常面临一个挑战：在一个电网（源域 $\mathcal{G}_S$）上训练的模型，能否在另一个拓扑结构、线路参数或仪器校准都不同的电网（目标域 $\mathcal{G}_T$）上表现良好？这就是**迁移学习（transfer learning）** 或**[领域自适应](@entry_id:637871)（domain adaptation）** 的问题。

这个问题的核心在于学习一种**表示（representation）** $\phi_\theta(\mathbf{x})$，该表示能够捕捉跨领域不变的本质特征，同时丢弃领域特有的无关信息。从[统计学习理论](@entry_id:274291)的角度看，一个模型在新领域上的性能（目标风险 $R_T$）受到其在原[领域性](@entry_id:180362)能（源风险 $R_S$）、源域和目标域在表示空间中的分布差异，以及一个反映任务本身在两个领域上内在差异的项的共同制约。

因此，理想的表示 $z = \phi_\theta(\mathbf{x})$ 可以被分解为**领域不变（domain-invariant）** 部分 $z^I$ 和**领域特定（domain-specific）** 部分 $z^S$。
- **领域不变特征 $z^I$** 应满足两个条件：(1) **标签充分性**，即它保留了预测标签 $y$ 所需的全部信息，使得[条件概率分布](@entry_id:163069)在两个领域中近似相等 $P_{\mathcal{D}_S}(y\mid z^I) \approx P_{\mathcal{D}_T}(y\mid z^I)$；(2) **领域无关性**，即它不包含关于样本来自哪个领域的信息，信息论上表现为与领域标签 $d$ 的互信息 $I(z^I; d)$ 趋近于零。
- **领域特定特征 $z^S$** 则相反，它携带了大量关于领域身份的信息（$I(z^S; d)$ 很大），但在给定 $z^I$ 的情况下，对预测标签 $y$ 不再提供额外信息（$I(z^S; y \mid z^I) \approx 0$）。

在电网的物理背景下，领域不变特征 $z^I$ 应该对应于那些由普适物理定律（如基尔霍夫定律、欧姆定律）决定的、故障类型的通用暂态模式。而领域特定特征 $z^S$ 则捕捉了随电网而变的拓扑结构、线路参数和[传感器校准](@entry_id:1131484)等具体细节。通过设计能够分离这两种特征的[表示学习](@entry_id:634436)算法，我们可以显著提高模型在不同电网间的泛化能力 。