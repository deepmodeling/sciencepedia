{
    "hands_on_practices": [
        {
            "introduction": "Before a deep learning model can diagnose a fault, raw sensor data must be transformed into meaningful features. A critical indicator of grid stress is the Rate of Change of Frequency (RoCoF), but estimating this derivative from discrete, and potentially noisy, PMU samples requires a robust numerical method. This exercise  provides hands-on practice with a standard technique—calculating the slope from a moving window of data using least-squares regression—which is a foundational skill in time-series signal processing for any monitoring application.",
            "id": "4083510",
            "problem": "A Phasor Measurement Unit (PMU) monitors grid frequency for a nominally $50\\,\\text{Hz}$ power system. The PMU sampling rate is $50$ samples per second, so each sample occurs at time $t_k = k \\times 0.02\\,\\text{s}$ for integer $k \\geq 0$. A deep learning fault diagnosis pipeline combines a Long Short-Term Memory (LSTM) network with a threshold-based early warning module. One engineered feature fed to the LSTM is the Rate of Change of Frequency (RoCoF), which, for the PMU’s discrete-time measurements, is computed at each sample by the slope of the best-fit line (ordinary least squares) of frequency versus time over a trailing window of $M = 5$ samples.\n\nAn event occurs that induces a linear ramp in frequency due to a net generation trip. The PMU reports the following frequency samples $f_k$ (in $\\text{Hz}$) at times $t_k$ (in $\\text{s}$), with the ramp starting at $t = 0.10\\,\\text{s}$:\n$$\n\\begin{aligned}\n&k: &&0 &&1 &&2 &&3 &&4 &&5 &&6 &&7 &&8 &&9 &&10 &&11 &&12\\\\\n&t_k: &&0 &&0.02 &&0.04 &&0.06 &&0.08 &&0.10 &&0.12 &&0.14 &&0.16 &&0.18 &&0.20 &&0.22 &&0.24\\\\\n&f_k: &&50.000 &&50.000 &&50.000 &&50.000 &&50.000 &&50.000 &&49.988 &&49.976 &&49.964 &&49.952 &&49.940 &&49.928 &&49.916\n\\end{aligned}\n$$\nThe threshold-based early warning module triggers when the magnitude of the estimated RoCoF (computed as the slope over the most recent $M = 5$ samples) first exceeds the threshold $\\theta = 0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nStarting from first principles, use the definition of RoCoF as the time derivative of frequency, and its discrete-time estimation via least-squares slope over the specified window, to determine:\n- The RoCoF estimate (in $\\text{Hz}\\cdot \\text{s}^{-1}$) at the earliest sample where the threshold condition is met.\n- The earliest detection time (in $\\text{s}$), defined as the time stamp $t_k$ of that sample.\n\nRound both requested quantities to four significant figures. Express the detection time in seconds.",
            "solution": "The problem requires the calculation of the Rate of Change of Frequency (RoCoF) from a discrete series of frequency measurements. The RoCoF is estimated as the slope of the best-fit line using an ordinary least squares (OLS) regression over a trailing window of $M=5$ samples. We must find the first time instant $t_k$ at which the magnitude of this estimated RoCoF exceeds a threshold $\\theta = 0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nFirst, let's establish the formula for the OLS slope. For a set of $M$ data points $(x_i, y_i)$, the slope $b$ of the best-fit line $y = a + bx$ is given by:\n$$\nb = \\frac{M \\sum_{i=1}^{M} x_i y_i - (\\sum_{i=1}^{M} x_i) (\\sum_{i=1}^{M} y_i)}{M \\sum_{i=1}^{M} x_i^2 - (\\sum_{i=1}^{M} x_i)^2}\n$$\nIn our case, the data points are time and frequency, $(t_j, f_j)$. The time samples are uniformly spaced with a sampling period of $\\Delta t = 0.02\\,\\text{s}$. This uniformity allows for a significant simplification of the slope formula.\n\nLet us consider a window of $M=5$ samples ending at index $k$. The samples in this window are indexed from $k-4$ to $k$. To simplify the OLS formula, we can define a local, centered time variable for the window. Let the indices within the window be $j \\in \\{-2, -1, 0, 1, 2\\}$, corresponding to sample indices $\\{k-4, k-3, k-2, k-1, k\\}$. The time coordinates relative to the center of the window ($t_{k-2}$) are $\\tau_j = j \\Delta t$. With this centered time variable, $\\sum \\tau_j = 0$, and the slope formula simplifies to:\n$$\n\\text{RoCoF}_k = b = \\frac{\\sum_{j=-2}^{2} \\tau_j f_{k-2+j}}{\\sum_{j=-2}^{2} \\tau_j^2}\n$$\nThe denominator is a constant for a fixed window size and sampling rate:\n$$\n\\sum_{j=-2}^{2} \\tau_j^2 = \\sum_{j=-2}^{2} (j \\Delta t)^2 = (\\Delta t)^2 \\sum_{j=-2}^{2} j^2 = (\\Delta t)^2 ((-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2) = 10(\\Delta t)^2\n$$\nSubstituting $\\Delta t = 0.02\\,\\text{s}$, the denominator is $10 \\times (0.02)^2 = 10 \\times 0.0004 = 0.004\\,\\text{s}^2$.\n\nThe numerator is:\n$$\n\\sum_{j=-2}^{2} \\tau_j f_{k-2+j} = \\Delta t \\sum_{j=-2}^{2} j f_{k-2+j} = \\Delta t (-2f_{k-4} - 1f_{k-3} + 0f_{k-2} + 1f_{k-1} + 2f_k)\n$$\nCombining the numerator and denominator, the RoCoF at sample $k$ is:\n$$\n\\text{RoCoF}_k = \\frac{\\Delta t (-2f_{k-4} - f_{k-3} + f_{k-1} + 2f_k)}{10(\\Delta t)^2} = \\frac{-2f_{k-4} - f_{k-3} + f_{k-1} + 2f_k}{10 \\Delta t}\n$$\nSince $10 \\Delta t = 10 \\times 0.02 = 0.2\\,\\text{s}$, the formula becomes:\n$$\n\\text{RoCoF}_k = \\frac{1}{0.2} (-2f_{k-4} - f_{k-3} + f_{k-1} + 2f_k) = 5(-2f_{k-4} - f_{k-3} + f_{k-1} + 2f_k)\n$$\nWe can now apply this formula iteratively, starting from the first possible window (ending at $k=4$), until the condition $|\\text{RoCoF}_k| > 0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$ is met. The frequency data $f_k$ is provided.\n\nFor $k < 6$, all frequencies in any $5$-sample window are $50.000\\,\\text{Hz}$. The slope of a constant function is zero.\nFor $k=4$, the window is $\\{f_0, f_1, f_2, f_3, f_4\\}$, all $50.000\\,\\text{Hz}$. Thus, $\\text{RoCoF}_4 = 0$.\nFor $k=5$, the window is $\\{f_1, f_2, f_3, f_4, f_5\\}$, all $50.000\\,\\text{Hz}$. Thus, $\\text{RoCoF}_5 = 0$.\nThe magnitude $|\\text{RoCoF}_k|$ is $0$, which is not greater than $0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nFor $k=6$: The window samples are $\\{f_2, f_3, f_4, f_5, f_6\\}$.\nFrequencies: $\\{50.000, 50.000, 50.000, 50.000, 49.988\\}$.\n$$\n\\text{RoCoF}_6 = 5 \\times (-2f_2 - f_3 + f_5 + 2f_6) = 5 \\times [-2(50.000) - 1(50.000) + 1(50.000) + 2(49.988)]\n$$\n$$\n\\text{RoCoF}_6 = 5 \\times [-100.000 + 2(49.988)] = 5 \\times [-100.000 + 99.976] = 5 \\times (-0.024) = -0.12\\,\\text{Hz}\\cdot \\text{s}^{-1}\n$$\n$|\\text{RoCoF}_6| = 0.12\\,\\text{Hz}\\cdot \\text{s}^{-1}$, which is not greater than $0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nFor $k=7$: The window samples are $\\{f_3, f_4, f_5, f_6, f_7\\}$.\nFrequencies: $\\{50.000, 50.000, 50.000, 49.988, 49.976\\}$.\n$$\n\\text{RoCoF}_7 = 5 \\times (-2f_3 - f_4 + f_6 + 2f_7) = 5 \\times [-2(50.000) - 1(50.000) + 1(49.988) + 2(49.976)]\n$$\n$$\n\\text{RoCoF}_7 = 5 \\times [-150.000 + 49.988 + 99.952] = 5 \\times [-150.000 + 149.940] = 5 \\times (-0.060) = -0.30\\,\\text{Hz}\\cdot \\text{s}^{-1}\n$$\n$|\\text{RoCoF}_7| = 0.30\\,\\text{Hz}\\cdot \\text{s}^{-1}$, which is not greater than $0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n\nFor $k=8$: The window samples are $\\{f_4, f_5, f_6, f_7, f_8\\}$.\nFrequencies: $\\{50.000, 50.000, 49.988, 49.976, 49.964\\}$.\n$$\n\\text{RoCoF}_8 = 5 \\times (-2f_4 - f_5 + f_7 + 2f_8) = 5 \\times [-2(50.000) - 1(50.000) + 1(49.976) + 2(49.964)]\n$$\n$$\n\\text{RoCoF}_8 = 5 \\times [-150.000 + 49.976 + 99.928] = 5 \\times [-150.000 + 149.904] = 5 \\times (-0.096) = -0.48\\,\\text{Hz}\\cdot \\text{s}^{-1}\n$$\n$|\\text{RoCoF}_8| = 0.48\\,\\text{Hz}\\cdot \\text{s}^{-1}$. This value is greater than the threshold of $0.4\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\nThis is the first sample index at which the threshold condition is met.\n\nThe requested quantities are:\n1. The RoCoF estimate at this sample: $\\text{RoCoF}_8 = -0.48\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n2. The earliest detection time, which is the timestamp of this sample: $t_8$.\nThe times are given by $t_k = k \\times 0.02\\,\\text{s}$.\n$t_8 = 8 \\times 0.02\\,\\text{s} = 0.16\\,\\text{s}$.\n\nThe problem requires rounding both quantities to four significant figures.\n- RoCoF estimate: $-0.48\\,\\text{Hz}\\cdot \\text{s}^{-1}$ rounded to four significant figures is $-0.4800\\,\\text{Hz}\\cdot \\text{s}^{-1}$.\n- Detection time: $0.16\\,\\text{s}$ rounded to four significant figures is $0.1600\\,\\text{s}$.\nThe final answers are therefore $-0.4800$ and $0.1600$.",
            "answer": "$$\\boxed{\\begin{pmatrix} -0.4800 & 0.1600 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Building a fault detection model is only half the battle; we must also rigorously evaluate its performance, especially in scenarios with severe class imbalance where faults are rare events. Standard metrics like accuracy can be highly misleading, so more sophisticated tools are needed. This problem  challenges you to derive and compute two important metrics, the F1-score and the Matthews Correlation Coefficient (MCC), demonstrating why the MCC is often a more reliable performance indicator for imbalanced classification tasks.",
            "id": "4083465",
            "problem": "A utility-scale distribution grid is monitored by time-synchronized Phasor Measurement Units (PMU) producing high-rate measurements. A Convolutional Neural Network (CNN) is trained to detect transient line faults from labeled PMU time windows. Over a deployment week, the CNN produces a binary decision $X \\in \\{0,1\\}$ for each time window, and the ground-truth label is $Y \\in \\{0,1\\}$, where $X=1$ and $Y=1$ denote a fault window.\n\nThe evaluation set contains $N$ independent windows with severe class imbalance typical of fault monitoring: the confusion matrix counts are\n- true positives $TP = 600$,\n- false positives $FP = 400$,\n- false negatives $FN = 1{,}400$,\n- true negatives $TN = 199{,}600$,\nso the total number of windows is $N = TP + FP + FN + TN = 202{,}000$, with a positive class base rate $2{,}000$ out of $202{,}000$.\n\nStarting only from first principles and core definitions appropriate to classification and correlation (namely, the Pearson product-moment correlation coefficient definition for binary random variables, and the definitions of precision, recall, and harmonic mean), derive the Matthews correlation coefficient (MCC) for this binary confusion matrix and the F1-score. Then compute the ratio $\\text{MCC}/\\text{F1}$ for the given counts.\n\nExpress the final ratio as a pure number with no units, and round your answer to four significant figures.",
            "solution": "### Derivation and Calculation\n\n**1. Derivation of the F1-Score**\n\nThe F1-score is defined as the harmonic mean of precision ($P$) and recall ($R$).\nFirst, we define precision and recall in terms of the confusion matrix elements:\n-   Precision ($P$): The fraction of positive predictions that are correct.\n    $$P = \\frac{TP}{TP + FP}$$\n-   Recall ($R$): The fraction of actual positives that are correctly identified. Also known as sensitivity.\n    $$R = \\frac{TP}{TP + FN}$$\nThe harmonic mean of two numbers $a$ and $b$ is given by $H = \\frac{2}{\\frac{1}{a} + \\frac{1}{b}} = \\frac{2ab}{a+b}$.\nSubstituting $P$ and $R$ for $a$ and $b$ gives the F1-score:\n$$F1 = \\frac{2PR}{P+R} = \\frac{2 \\left(\\frac{TP}{TP+FP}\\right) \\left(\\frac{TP}{TP+FN}\\right)}{\\frac{TP}{TP+FP} + \\frac{TP}{TP+FN}}$$\nTo simplify, we multiply the numerator and denominator by $(TP+FP)(TP+FN)$:\n$$F1 = \\frac{2(TP)^2}{TP(TP+FN) + TP(TP+FP)} = \\frac{2(TP)^2}{TP^2 + TP \\cdot FN + TP^2 + TP \\cdot FP}$$\nFactoring out $TP$ from the denominator (assuming $TP \\neq 0$):\n$$F1 = \\frac{2TP}{TP+FN + TP+FP} = \\frac{2TP}{2TP + FP + FN}$$\nThis is the standard formula for the F1-score.\n\n**2. Derivation of the Matthews Correlation Coefficient (MCC)**\n\nThe MCC for binary classification is equivalent to the Pearson product-moment correlation coefficient $\\rho_{X,Y}$ for the binary variables $X$ (prediction) and $Y$ (truth), where $X, Y \\in \\{0, 1\\}$.\nThe formula for $\\rho_{X,Y}$ is:\n$$\\rho_{X,Y} = \\frac{\\text{cov}(X, Y)}{\\sigma_X \\sigma_Y} = \\frac{E[XY] - E[X]E[Y]}{\\sqrt{\\text{Var}(X)} \\sqrt{\\text{Var}(Y)}}$$\nThe expectation values are estimated from the confusion matrix counts, with $N = TP+FP+FN+TN$:\n-   $E[X] = P(X=1) = \\frac{TP+FP}{N}$\n-   $E[Y] = P(Y=1) = \\frac{TP+FN}{N}$\n-   $XY=1$ only if $X=1$ and $Y=1$, so $E[XY] = P(X=1, Y=1) = \\frac{TP}{N}$\n\nThe numerator, the covariance, is:\n$$\\text{cov}(X, Y) = \\frac{TP}{N} - \\left(\\frac{TP+FP}{N}\\right)\\left(\\frac{TP+FN}{N}\\right) = \\frac{N \\cdot TP - (TP+FP)(TP+FN)}{N^2}$$\nExpanding the terms:\n$$N \\cdot TP - (TP^2 + TP \\cdot FN + FP \\cdot TP + FP \\cdot FN)$$\nSubstituting $N = TP+FP+FN+TN$:\n$$(TP+FP+FN+TN)TP - (TP^2 + TP \\cdot FN + FP \\cdot TP + FP \\cdot FN)$$\n$$= (TP^2+FP \\cdot TP+FN \\cdot TP+TN \\cdot TP) - (TP^2 + TP \\cdot FN + FP \\cdot TP + FP \\cdot FN)$$\n$$= TN \\cdot TP - FP \\cdot FN$$\nThus, the numerator is $\\frac{TP \\cdot TN - FP \\cdot FN}{N^2}$.\n\nThe denominator involves the variances. For a binary variable $A$, $\\text{Var}(A) = E[A^2] - (E[A])^2$. Since $A^2=A$ for $A \\in \\{0, 1\\}$, $\\text{Var}(A) = E[A] - (E[A])^2 = E[A](1-E[A]) = P(A=1)P(A=0)$.\n-   $\\text{Var}(X) = P(X=1)P(X=0) = \\left(\\frac{TP+FP}{N}\\right)\\left(\\frac{TN+FN}{N}\\right)$\n-   $\\text{Var}(Y) = P(Y=1)P(Y=0) = \\left(\\frac{TP+FN}{N}\\right)\\left(\\frac{TN+FP}{N}\\right)$\nThe product of the standard deviations is:\n$$\\sigma_X \\sigma_Y = \\sqrt{\\text{Var}(X)\\text{Var}(Y)} = \\sqrt{\\frac{(TP+FP)(TN+FN)}{N^2} \\frac{(TP+FN)(TN+FP)}{N^2}}$$\n$$= \\frac{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}{N^2}$$\n\nCombining the numerator and denominator gives the MCC:\n$$\\text{MCC} = \\rho_{X,Y} = \\frac{\\frac{TP \\cdot TN - FP \\cdot FN}{N^2}}{\\frac{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}{N^2}}$$\n$$\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n\n**3. Numerical Calculation**\n\nGiven values are $TP = 600$, $FP = 400$, $FN = 1{,}400$, and $TN = 199{,}600$.\n\n-   **F1-Score Calculation**:\n    $$F1 = \\frac{2 \\times 600}{2 \\times 600 + 400 + 1{,}400} = \\frac{1{,}200}{1{,}200 + 400 + 1{,}400} = \\frac{1{,}200}{3{,}000} = 0.4$$\n\n-   **MCC Calculation**:\n    -   Numerator: $TP \\cdot TN - FP \\cdot FN = (600)(199{,}600) - (400)(1{,}400) = 119{,}760{,}000 - 560{,}000 = 119{,}200{,}000$.\n    -   Denominator terms:\n        -   $TP+FP = 600 + 400 = 1{,}000$\n        -   $TP+FN = 600 + 1{,}400 = 2{,}000$\n        -   $TN+FP = 199{,}600 + 400 = 200{,}000$\n        -   $TN+FN = 199{,}600 + 1{,}400 = 201{,}000$\n    -   Denominator: $\\sqrt{(1{,}000)(2{,}000)(200{,}000)(201{,}000)} = \\sqrt{8.04 \\times 10^{16}} = \\sqrt{8.04} \\times 10^8$.\n    -   $MCC = \\frac{119{,}200{,}000}{\\sqrt{8.04} \\times 10^8} = \\frac{1.192 \\times 10^8}{\\sqrt{8.04} \\times 10^8} = \\frac{1.192}{\\sqrt{8.04}}$.\n    -   $MCC \\approx \\frac{1.192}{2.83548937...} \\approx 0.420380...$\n\n**4. Ratio Calculation**\n\nFinally, we compute the ratio $\\text{MCC}/\\text{F1}$:\n$$\\frac{\\text{MCC}}{\\text{F1}} = \\frac{0.420380...}{0.4} = 1.05095...$$\nRounding to four significant figures, the result is $1.051$.",
            "answer": "$$\\boxed{1.051}$$"
        },
        {
            "introduction": "A powerful approach to fault diagnosis is to move beyond simple classification and embrace a probabilistic framework. By modeling the statistical distribution of features for different fault types, we can use Bayes' theorem to calculate the posterior probability of each fault class given a new measurement. This exercise  guides you through the core mechanics of a Gaussian class-conditional model, showing how to combine the likelihood of an observation with prior probabilities to determine the most likely diagnosis in a principled, quantitative way.",
            "id": "4083436",
            "problem": "A transmission-level Phasor Measurement Unit (PMU) streams synchronized current phasors sampled at high rate during an event. A trained Deep Neural Network (DNN) encoder maps a short window of the three-phase current waveform to a two-dimensional latent feature vector $x = [\\Delta |I|, \\Delta \\theta]^{\\top}$, where $\\Delta |I|$ is the step change in the current magnitude in per-unit and $\\Delta \\theta$ is the abrupt phase jump in radians at the event onset. A Gaussian class-conditional generative model is used in the latent space for fault diagnosis. The classes are normal (no fault), single-line-to-ground, line-to-line, and three-phase, denoted $\\mathcal{C} \\in \\{\\mathrm{N}, \\mathrm{SLG}, \\mathrm{LL}, 3\\phi\\}$, with prior probabilities $\\pi_{\\mathrm{N}} = 0.985$, $\\pi_{\\mathrm{SLG}} = 0.010$, $\\pi_{\\mathrm{LL}} = 0.004$, and $\\pi_{3\\phi} = 0.001$. Conditioned on a class $c$, the latent vector $x$ is modeled as a bivariate Gaussian with mean $\\mu_{c}$ and diagonal covariance $\\Sigma_{c} = \\mathrm{diag}(\\sigma_{I,c}^{2}, \\sigma_{\\theta,c}^{2})$:\n- For $\\mathrm{N}$: $\\mu_{\\mathrm{N}} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\sigma_{I,\\mathrm{N}} = 0.05$, $\\sigma_{\\theta,\\mathrm{N}} = 0.02$.\n- For $\\mathrm{SLG}$: $\\mu_{\\mathrm{SLG}} = \\begin{bmatrix} 0.8 \\\\ -0.3 \\end{bmatrix}$, $\\sigma_{I,\\mathrm{SLG}} = 0.3$, $\\sigma_{\\theta,\\mathrm{SLG}} = 0.1$.\n- For $\\mathrm{LL}$: $\\mu_{\\mathrm{LL}} = \\begin{bmatrix} 1.2 \\\\ -0.5 \\end{bmatrix}$, $\\sigma_{I,\\mathrm{LL}} = 0.25$, $\\sigma_{\\theta,\\mathrm{LL}} = 0.15$.\n- For $3\\phi$: $\\mu_{3\\phi} = \\begin{bmatrix} 1.6 \\\\ -0.05 \\end{bmatrix}$, $\\sigma_{I,3\\phi} = 0.2$, $\\sigma_{\\theta,3\\phi} = 0.05$.\n\nA simulated three-phase fault at a monitored bus produces a latent feature measurement $x^{\\ast} = \\begin{bmatrix} \\Delta |I| \\\\ \\Delta \\theta \\end{bmatrix} = \\begin{bmatrix} 1.55 \\\\ -0.06 \\end{bmatrix}$, where $\\Delta |I|$ is in per-unit and $\\Delta \\theta$ is in radians. Under the above Gaussian class-conditional model and priors, compute the posterior probability $p(\\mathcal{C} = 3\\phi \\mid x^{\\ast})$. Express your final answer as a decimal number (unitless) and round your answer to four significant figures.",
            "solution": "The problem requires the computation of the posterior probability $p(\\mathcal{C} = 3\\phi \\mid x^{\\ast})$ for a given latent feature measurement $x^{\\ast}$. This is a classic Bayesian inference problem. We will use Bayes' theorem, which states that the posterior probability of a class $\\mathcal{C}=c$ given an observation $x$ is proportional to the product of the likelihood of the observation given the class and the prior probability of the class.\n\nThe formula for the posterior probability is:\n$$ p(\\mathcal{C}=c \\mid x) = \\frac{p(x \\mid \\mathcal{C}=c) p(\\mathcal{C}=c)}{p(x)} $$\nwhere $p(x \\mid \\mathcal{C}=c)$ is the class-conditional likelihood, $p(\\mathcal{C}=c)$ is the class prior probability, and $p(x)$ is the marginal probability of the evidence. The evidence term is a normalizing constant, calculated by summing over all possible classes using the law of total probability:\n$$ p(x) = \\sum_{c' \\in \\{\\mathrm{N}, \\mathrm{SLG}, \\mathrm{LL}, 3\\phi\\}} p(x \\mid \\mathcal{C}=c') p(\\mathcal{C}=c') $$\nFor the specific class $\\mathcal{C} = 3\\phi$ and the measurement $x^{\\ast}$, the posterior probability is:\n$$ p(\\mathcal{C}=3\\phi \\mid x^{\\ast}) = \\frac{p(x^{\\ast} \\mid \\mathcal{C}=3\\phi) p(\\mathcal{C}=3\\phi)}{\\sum_{c' \\in \\{\\mathrm{N}, \\mathrm{SLG}, \\mathrm{LL}, 3\\phi\\}} p(x^{\\ast} \\mid \\mathcal{C}=c') p(\\mathcal{C}=c')} $$\nThe class-conditional likelihood $p(x \\mid \\mathcal{C}=c)$ is given as a bivariate Gaussian distribution $\\mathcal{N}(x \\mid \\mu_c, \\Sigma_c)$ with a diagonal covariance matrix $\\Sigma_c = \\mathrm{diag}(\\sigma_{I,c}^{2}, \\sigma_{\\theta,c}^{2})$. The probability density function (PDF) for a latent vector $x = [\\Delta |I|, \\Delta \\theta]^{\\top} \\equiv [x_I, x_\\theta]^{\\top}$ is:\n$$ p(x \\mid \\mathcal{C}=c) = \\frac{1}{2\\pi \\sigma_{I,c} \\sigma_{\\theta,c}} \\exp\\left(-\\frac{1}{2}\\left[\\left(\\frac{x_I - \\mu_{I,c}}{\\sigma_{I,c}}\\right)^2 + \\left(\\frac{x_\\theta - \\mu_{\\theta,c}}{\\sigma_{\\theta,c}}\\right)^2\\right]\\right) $$\nLet $U_c = p(x^{\\ast} \\mid \\mathcal{C}=c) p(\\mathcal{C}=c)$ be the unnormalized posterior probability for class $c$. The posterior is then $p(\\mathcal{C}=c \\mid x^{\\ast}) = U_c / \\sum_{c'} U_{c'}$. The term $1/(2\\pi)$ is a common factor in all likelihoods and will cancel out. Therefore, we can work with a proportional quantity $u_c$:\n$$ u_c = \\frac{\\pi_c}{\\sigma_{I,c} \\sigma_{\\theta,c}} \\exp\\left(-\\frac{1}{2} E_c\\right) $$\nwhere $\\pi_c = p(\\mathcal{C}=c)$ is the prior probability and $E_c$ is the squared Mahalanobis distance term in the exponent:\n$$ E_c = \\left(\\frac{x_I^{\\ast} - \\mu_{I,c}}{\\sigma_{I,c}}\\right)^2 + \\left(\\frac{x_\\theta^{\\ast} - \\mu_{\\theta,c}}{\\sigma_{\\theta,c}}\\right)^2 $$\nWe are given the measurement $x^{\\ast} = \\begin{bmatrix} 1.55 \\\\ -0.06 \\end{bmatrix}$. We now calculate $E_c$ and $u_c$ for each of the four classes.\n\n1.  **Normal Class ($\\mathrm{N}$):**\n    $\\mu_{\\mathrm{N}} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\sigma_{I,\\mathrm{N}} = 0.05$, $\\sigma_{\\theta,\\mathrm{N}} = 0.02$, $\\pi_{\\mathrm{N}} = 0.985$.\n    $$ E_{\\mathrm{N}} = \\left(\\frac{1.55 - 0}{0.05}\\right)^2 + \\left(\\frac{-0.06 - 0}{0.02}\\right)^2 = 31^2 + (-3)^2 = 961 + 9 = 970 $$\n    $$ u_{\\mathrm{N}} = \\frac{0.985}{0.05 \\times 0.02} \\exp\\left(-\\frac{970}{2}\\right) = 985 \\exp(-485) $$\n    This value is extremely small and can be approximated as $0$.\n\n2.  **Single-Line-to-Ground Class ($\\mathrm{SLG}$):**\n    $\\mu_{\\mathrm{SLG}} = \\begin{bmatrix} 0.8 \\\\ -0.3 \\end{bmatrix}$, $\\sigma_{I,\\mathrm{SLG}} = 0.3$, $\\sigma_{\\theta,\\mathrm{SLG}} = 0.1$, $\\pi_{\\mathrm{SLG}} = 0.010$.\n    $$ E_{\\mathrm{SLG}} = \\left(\\frac{1.55 - 0.8}{0.3}\\right)^2 + \\left(\\frac{-0.06 - (-0.3)}{0.1}\\right)^2 = (2.5)^2 + (2.4)^2 = 6.25 + 5.76 = 12.01 $$\n    $$ u_{\\mathrm{SLG}} = \\frac{0.010}{0.3 \\times 0.1} \\exp\\left(-\\frac{12.01}{2}\\right) = \\frac{1}{3} \\exp(-6.005) \\approx 8.2221 \\times 10^{-4} $$\n\n3.  **Line-to-Line Class ($\\mathrm{LL}$):**\n    $\\mu_{\\mathrm{LL}} = \\begin{bmatrix} 1.2 \\\\ -0.5 \\end{bmatrix}$, $\\sigma_{I,\\mathrm{LL}} = 0.25$, $\\sigma_{\\theta,\\mathrm{LL}} = 0.15$, $\\pi_{\\mathrm{LL}} = 0.004$.\n    $$ E_{\\mathrm{LL}} = \\left(\\frac{1.55 - 1.2}{0.25}\\right)^2 + \\left(\\frac{-0.06 - (-0.5)}{0.15}\\right)^2 = (1.4)^2 + \\left(\\frac{0.44}{0.15}\\right)^2 = 1.96 + \\left(\\frac{44}{15}\\right)^2 = 1.96 + \\frac{1936}{225} \\approx 10.5644 $$\n    $$ u_{\\mathrm{LL}} = \\frac{0.004}{0.25 \\times 0.15} \\exp\\left(-\\frac{10.5644}{2}\\right) = \\frac{0.004}{0.0375} \\exp\\left(-5.2822\\right) \\approx 5.4198 \\times 10^{-4} $$\n\n4.  **Three-Phase Class ($3\\phi$):**\n    $\\mu_{3\\phi} = \\begin{bmatrix} 1.6 \\\\ -0.05 \\end{bmatrix}$, $\\sigma_{I,3\\phi} = 0.2$, $\\sigma_{\\theta,3\\phi} = 0.05$, $\\pi_{3\\phi} = 0.001$.\n    $$ E_{3\\phi} = \\left(\\frac{1.55 - 1.6}{0.2}\\right)^2 + \\left(\\frac{-0.06 - (-0.05)}{0.05}\\right)^2 = (-0.25)^2 + (-0.2)^2 = 0.0625 + 0.04 = 0.1025 $$\n    $$ u_{3\\phi} = \\frac{0.001}{0.2 \\times 0.05} \\exp\\left(-\\frac{0.1025}{2}\\right) = \\frac{0.001}{0.01} \\exp(-0.05125) = 0.1 \\exp(-0.05125) \\approx 0.095004 $$\n\nNow, we compute the sum of these unnormalized posterior values, which is proportional to the evidence $p(x^{\\ast})$:\n$$ \\sum_{c'} u_{c'} = u_{\\mathrm{N}} + u_{\\mathrm{SLG}} + u_{\\mathrm{LL}} + u_{3\\phi} \\approx 0 + (8.2221 \\times 10^{-4}) + (5.4198 \\times 10^{-4}) + 0.095004 \\approx 0.096368 $$\nFinally, the posterior probability for the three-phase fault class is the ratio of its unnormalized posterior to the sum:\n$$ p(\\mathcal{C}=3\\phi \\mid x^{\\ast}) = \\frac{u_{3\\phi}}{\\sum_{c'} u_{c'}} \\approx \\frac{0.095004}{0.096368} \\approx 0.985846 $$\nRounding to four significant figures, we get $0.9858$.",
            "answer": "$$\\boxed{0.9858}$$"
        }
    ]
}