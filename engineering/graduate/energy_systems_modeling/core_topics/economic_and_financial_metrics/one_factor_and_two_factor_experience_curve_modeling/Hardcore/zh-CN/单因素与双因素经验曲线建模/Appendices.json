{
    "hands_on_practices": [
        {
            "introduction": "在构建经验曲线模型时，一个核心问题是选择合适的模型复杂度。我们应该使用简单的单因素模型，还是包含额外驱动因素的双因素模型？本练习将指导您使用交叉验证（一种评估模型在未见数据上预测性能的稳健方法）来量化比较这两种模型的优劣，从而为模型选择提供实证依据。",
            "id": "4109570",
            "problem": "您的任务是设计并实现一个编程式的 $K$-折交叉验证 (CV) 程序，以比较能源系统建模中单因子与双因子经验曲线模型的预测性能。因变量是单位成本的自然对数，损失函数必须在对数成本空间中运行。交叉验证必须通过将连续的数据块分配到各个折（fold）中来尊重时间结构。该比较必须在三个由科学上合理、自洽的生成过程构建的合成测试用例上进行。\n\n起点和基本依据：假设一种能源技术的单位成本由相对于规模和另一个驱动因素的恒定弹性决定。具有乘法可分性的恒定弹性意味着，在取自然对数后，模型在参数上变为线性。因此，模型训练必须使用普通最小二乘法 (OLS)，模型比较必须在对数成本空间中明确指定的损失函数下进行。\n\n要求：\n- 使用分块 $K$-折交叉验证 (CV)，其定义为将按时间排序的数据划分为 $K$ 个连续的数据块，每次使用一个块作为验证集，其余的块共同作为训练集。\n- 将对数成本的损失定义为所有 $K$ 折中全部验证点的均方误差 (Mean Squared Error (MSE))。如果 $\\hat{y}_i$ 表示预测的对数成本，$y_i$ 表示观测到的对数成本，并且所有折中总共有 $m$ 个验证点，则损失必须是：\n$$\n\\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^{m} \\left(y_i - \\hat{y}_i\\right)^2.\n$$\n- 成本以美元/千瓦 ($USD/kW$) 为单位。在取对数之前，通过除以 $1$ $USD/kW$ 进行归一化，以使自然对数应用于一个无量纲的比率。\n- 单因子模型：将对数成本对一个源自规模（经对数转换）的单一解释变量进行回归。\n- 双因子模型：将对数成本对两个源自规模和另一个驱动因素（均经对数转换）的解释变量进行回归。\n- 使用 OLS 在训练折上估计参数线性模型，并在留出的折上生成验证预测。\n\n测试套件和数据生成：\n构建三个具有确定性生成过程的测试用例。在所有情况下，令 $t \\in \\{1,2,\\dots,N\\}$ 表示时间索引，$S_t$ 表示累积规模，$D_t$ 表示另一个驱动因素，$C_t$ 表示单位成本。使用指定的序列定义 $S_t$ 和 $D_t$，然后将单位成本定义为\n$$\nC_t = A \\cdot S_t^{-b} \\cdot D_t^{-g} \\cdot e^{\\varepsilon_t},\n$$\n其中 $A$ 是一个以 $USD/kW$ 为单位的正尺度参数，$\\varepsilon_t$ 是一个确定性扰动项。观测到的因变量为 $y_t = \\ln\\left(C_t / (1\\ \\text{USD}/\\text{kW})\\right)$。\n\n- 案例 A（通用“理想路径”双因子）：\n  - $N = 30$, $K = 5$, $A = 1000$, $b = 0.3$, $g = 0.2$。\n  - 生产增量：$s_t = 0.5 + 0.02 t + 0.1 \\sin(0.1 t)$，累积规模：$S_t = \\sum_{i=1}^{t} s_i$。\n  - 知识增量：$d_t = 0.2 + 0.01 t$，知识存量：$D_t = \\sum_{i=1}^{t} d_i$。\n  - 扰动项：$\\varepsilon_t = 0.05 \\sin(0.3 t)$。\n\n- 案例 B（由单因子主导的边缘案例）：\n  - $N = 8$, $K = 4$, $A = 800$, $b = 0.35$, $g = 0$。\n  - 生产增量：$s_t = 0.6 + 0.03 t$，累积规模：$S_t = \\sum_{i=1}^{t} s_i$。\n  - 额外驱动因素为常数：对于所有 $t$，$D_t = 1$。\n  - 扰动项：$\\varepsilon_t = 0.02 \\cos(0.5 t)$。\n\n- 案例 C（具有强共线性和留一法交叉验证的边界案例）：\n  - $N = 12$, $K = 12$, $A = 900$, $b = 0.25$, $g = 0.1$。\n  - 生产增量：$s_t = 0.7 + 0.01 t$，累积规模：$S_t = \\sum_{i=1}^{t} s_i$。\n  - 额外驱动因素：$D_t = 1.05 S_t + 0.5$。\n  - 扰动项：$\\varepsilon_t = 0.04 \\sin(0.2 t)$。\n\n实现细节：\n- 对每个案例，使用上述定义构建 $S_t$、$D_t$ 和 $C_t$，并将 $y_t$ 计算为归一化成本的自然对数。\n- 对于单因子模型，使用截距项和 $\\ln(S_t)$ 对 $y_t$ 进行拟合。\n- 对于双因子模型，使用截距项、$\\ln(S_t)$ 和 $\\ln(D_t)$ 对 $y_t$ 进行拟合。\n- 使用分块 $K$-折 CV：将索引 $\\{1,\\dots,N\\}$ 划分为 $K$ 个连续的折，其大小相差最多为 1。在每个折中，验证集是当前块，训练集是其补集。\n- 将 $\\mathcal{L}_{1}$ 计算为单因子模型在所有折上的平均验证 MSE，并以类似方式计算双因子模型的 $\\mathcal{L}_{2}$。\n\n最终输出要求：\n- 对于每个测试用例，计算标量差 $\\Delta = \\mathcal{L}_{1} - \\mathcal{L}_{2}$，单位为对数成本单位（无量纲）。将这三个值报告为纯十进制数。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[$\\text{result1}$,$\\text{result2}$,$\\text{result3}$]”），不含任何附加文本。\n\n所有三角函数的计算都必须以弧度表示，所有数值输出都必须是十进制数，不带百分号。",
            "solution": "该任务是编程式地实现一个分块 $K$-折交叉验证 (CV) 程序，以针对三个不同的、合成生成的测试用例，比较单因子经验曲线模型与双因子模型。比较指标是在留出验证集上计算并汇总所有折结果的均方误差 (MSE) 差异。\n\n在时间 $t$ 的单位成本 $C_t$ 的生成过程由一个具有恒定弹性的乘法模型给出：\n$$\nC_t = A \\cdot S_t^{-b} \\cdot D_t^{-g} \\cdot e^{\\varepsilon_t}\n$$\n此处，$S_t$ 是生产的累积规模，$D_t$ 是一个额外的解释性驱动因素（例如，知识存量），$A$ 是一个基线成本参数，$b$ 和 $g$ 分别是关于规模和额外驱动因素的学习弹性，$\\varepsilon_t$ 是一个确定性扰动项。\n\n通过取自然对数并将成本除以 $1 \\text{ USD/kW}$ 进行归一化以创建一个无量纲的量，我们得到一个在其参数上是线性的模型：\n$$\ny_t = \\ln\\left(\\frac{C_t}{1 \\text{ USD/kW}}\\right) = \\ln(A) - b\\ln(S_t) - g\\ln(D_t) + \\varepsilon_t\n$$\n这种对数线性形式是我们估计程序的基础。我们将比较两个使用普通最小二乘法 (OLS) 对这些对数转换后的数据进行拟合的模型。\n\n模型 1（单因子）：此模型仅将对数成本 $y_t$ 与累积规模的对数 $\\ln(S_t)$ 相关联。它包含一个截距项。模型方程为：\n$$\ny_t = \\beta_0 + \\beta_1 \\ln(S_t) + u_t\n$$\n其中 $\\beta_0$ 和 $\\beta_1$ 是待估计的系数，$u_t$ 是误差项。\n\n模型 2（双因子）：此模型通过将额外驱动因素的对数 $\\ln(D_t)$ 作为第二个解释变量来扩展单因子模型。模型方程为：\n$$\ny_t = \\gamma_0 + \\gamma_1 \\ln(S_t) + \\gamma_2 \\ln(D_t) + v_t\n$$\n其中 $\\gamma_0$、$\\gamma_1$ 和 $\\gamma_2$ 是待估计的系数，$v_t$ 是误差项。\n\n分析的核心是分块 $K$-折交叉验证程序。选择此方法是为了尊重数据的时间顺序，这在时间序列背景下对于防止前视偏差至关重要。包含 $N$ 个观测值的数据集被划分为 $K$ 个连续、不重叠的块（折）。该程序迭代 $K$ 次。在每次迭代 $k \\in \\{1, \\dots, K\\}$ 中，第 $k$ 个块用作验证集，其余的 $K-1$ 个块组合成训练集。\n\n对于 $K$ 次迭代中的每一次，我们执行以下步骤：\n1.  两个模型的模型参数（$\\beta$ 和 $\\gamma$）都使用 OLS 在训练数据上进行估计。对于一般线性模型 $y = X\\beta + \\epsilon$，OLS 估计值为 $\\hat{\\beta} = (X^T X)^{-1} X^T y$。此计算使用数值稳定的算法，例如由 `numpy.linalg.lstsq` 提供的算法。\n2.  然后使用拟合的模型来预测验证集中观测值 $i$ 的对数成本 $\\hat{y}_i$。\n3.  存储验证集的真实观测对数成本 $y_i$ 和预测对数成本 $\\hat{y}_i$。\n\n完成所有 $K$ 折后，汇总所有存储的验证预测。每个模型的总体性能通过所有 $m$ 个验证点（其中 $m=N$）的均方误差 (MSE) 进行量化。损失函数定义为：\n$$\n\\mathcal{L} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n$$\n设 $\\mathcal{L}_1$ 为单因子模型的损失，$\\mathcal{L}_2$ 为双因子模型的损失。最终比较通过计算差异 $\\Delta = \\mathcal{L}_1 - \\mathcal{L}_2$ 来执行。$\\Delta$ 的正值表示双因子模型具有更低的 MSE，因此在此评估框架下具有更优的预测性能。\n\n整个程序将应用于三个不同的测试用例，每个用例都由一组特定的参数（$A, b, g, N, K$）以及 $S_t$、$D_t$ 和 $\\varepsilon_t$ 的函数形式定义。该逻辑被封装在一个 Python 程序中，该程序首先为每个案例生成数据，然后执行所述的交叉验证工作流程以计算 $\\mathcal{L}_1$ 和 $\\mathcal{L}_2$，最后计算它们的差值 $\\Delta$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the cross-validation comparison for all test cases.\n    \"\"\"\n\n    def perform_kfold_cv(X, y, K):\n        \"\"\"\n        Performs blocked K-fold cross-validation for a given model.\n\n        Args:\n            X (np.ndarray): The design matrix, including an intercept column.\n            y (np.ndarray): The dependent variable vector.\n            K (int): The number of folds.\n\n        Returns:\n            float: The mean squared error over all validation predictions.\n        \"\"\"\n        n_samples = len(y)\n        indices = np.arange(n_samples)\n\n        # Determine the size of each contiguous block for the folds.\n        # This ensures fold sizes differ by at most 1.\n        base_fold_size = n_samples // K\n        num_larger_folds = n_samples % K\n        \n        fold_start_indices = [0]\n        current_idx = 0\n        for i in range(K):\n            fold_size = base_fold_size + 1 if i  num_larger_folds else base_fold_size\n            current_idx += fold_size\n            if i  K - 1:\n                fold_start_indices.append(current_idx)\n\n        all_y_true = []\n        all_y_pred = []\n\n        for i in range(K):\n            # Define validation and training indices for the current fold\n            val_start = fold_start_indices[i]\n            val_end = fold_start_indices[i+1] if i + 1  K else n_samples\n            val_idx = indices[val_start:val_end]\n            \n            train_mask = np.ones(n_samples, dtype=bool)\n            train_mask[val_idx] = False\n            train_idx = indices[train_mask]\n\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_val, y_val = X[val_idx], y[val_idx]\n\n            # Fit model using OLS on the training set\n            # np.linalg.lstsq is numerically robust and handles (multi)collinearity\n            try:\n                coeffs = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n            except np.linalg.LinAlgError:\n                # This should not occur with the given test cases, but is a safeguard.\n                # If it occurs, prediction quality is zero.\n                coeffs = np.zeros(X_train.shape[1])\n\n            # Predict on the validation set\n            y_pred = X_val @ coeffs\n            \n            all_y_true.append(y_val)\n            all_y_pred.append(y_pred)\n\n        # Concatenate results from all folds\n        y_true_flat = np.concatenate(all_y_true)\n        y_pred_flat = np.concatenate(all_y_pred)\n        \n        # Calculate overall Mean Squared Error\n        mse = np.mean((y_true_flat - y_pred_flat)**2)\n        return mse\n\n    def run_case(params):\n        \"\"\"\n        Generates data and runs the CV comparison for a single test case.\n        \"\"\"\n        case_id, N, K, A, b, g, s_func, d_spec, eps_func = params\n\n        # Generate data based on the case specification\n        t = np.arange(1, N + 1)\n        \n        s_t = s_func(t)\n        S_t = np.cumsum(s_t)\n\n        if case_id == 'A':\n            d_t = d_spec(t)\n            D_t = np.cumsum(d_t)\n        elif case_id == 'B':\n            D_t = np.full(N, d_spec)\n        elif case_id == 'C':\n            D_t = d_spec(S_t)\n\n        epsilon_t = eps_func(t)\n        \n        # Compute cost and the log-transformed dependent variable y_t\n        C_t = A * (S_t ** -b) * (D_t ** -g) * np.exp(epsilon_t)\n        y = np.log(C_t)\n\n        # Prepare regressor data\n        log_S = np.log(S_t)\n        log_D = np.log(D_t)\n        intercept = np.ones_like(y)\n\n        # Create design matrices for the two models\n        X1 = np.stack([intercept, log_S], axis=1) # One-factor model\n        X2 = np.stack([intercept, log_S, log_D], axis=1) # Two-factor model\n\n        # Perform CV and get MSE for each model\n        loss1 = perform_kfold_cv(X1, y, K)\n        loss2 = perform_kfold_cv(X2, y, K)\n\n        # Return the difference in loss\n        return loss1 - loss2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('A', 30, 5, 1000, 0.3, 0.2, \n         lambda t: 0.5 + 0.02 * t + 0.1 * np.sin(0.1 * t), \n         lambda t: 0.2 + 0.01 * t,\n         lambda t: 0.05 * np.sin(0.3 * t)),\n        ('B', 8, 4, 800, 0.35, 0,\n         lambda t: 0.6 + 0.03 * t,\n         1.0,\n         lambda t: 0.02 * np.cos(0.5 * t)),\n        ('C', 12, 12, 900, 0.25, 0.1,\n         lambda t: 0.7 + 0.01 * t,\n         lambda S: 1.05 * S + 0.5,\n         lambda t: 0.04 * np.sin(0.2 * t))\n    ]\n    \n    results = [run_case(case) for case in test_cases]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "即使双因素模型在预测上表现更优，我们仍需警惕一个常见的陷阱：多重共线性。当两个解释变量（如累计产量和知识存量）高度相关时，模型参数的估计可能变得非常不稳定且难以解释。本练习将带您从第一性原理出发，通过编程计算方差膨胀因子（VIF）——一个诊断多重共线性严重程度的标准指标——来掌握这项关键的模型诊断技能。",
            "id": "4109606",
            "problem": "考虑一个用于能源技术单位成本的双因素经验曲线模型，其对数形式为 $ \\ln C = \\alpha - b \\ln Q - c \\ln S $，其中 $ C $ 表示以固定货币单位/能量单位计量的单位成本，$ Q $ 表示以能量单位计量的累积产量，$ S $ 表示一个无量纲指数形式的累积溢出或知识存量，而 $ \\alpha $、$ b $、$ c $ 是待估计的未知参数。在实际使用普通最小二乘法（OLS）进行估计时，预测变量之间的强线性相关性会增加参数估计的不确定性，这种现象被称为多重共线性。一种基于 OLS 和决定系数的标准诊断方法是方差膨胀因子（VIF），它应用于回归中的每个预测变量。您的任务是，仅依赖 OLS 和决定系数的定义，从基本原理出发，实现一个程序，通过辅助回归（将每个预测变量对另一个预测变量和一个截距项进行回归）来计算每个预测变量的 VIF，从而诊断预测变量对 $ (\\ln Q, \\ln S) $ 中的多重共线性。三角函数中的角度必须以弧度处理。\n\n使用以下三个对于能源系统建模具有科学合理性的合成数据集测试套件。对于每个数据集，根据指定的索引范围构建序列 $ \\{Q_i\\} $ 和 $ \\{S_i\\} $，然后在执行诊断前转换为 $ x_{1,i} = \\ln Q_i $ 和 $ x_{2,i} = \\ln S_i $。\n\n- 数据集 A（一般情况，中度相关）：对于 $ i = 1,2,\\dots,20 $，\n  $$ Q_i = 3.0 \\times 10^{5} + 1.8 \\times 10^{4} \\cdot i^{1.1}, $$\n  $$ S_i = 2.5 \\times 10^{4} + 7.0 \\times 10^{3} \\cdot \\sin(0.2 \\, i) + 1.4 \\times 10^{3} \\cdot i. $$\n  对 $ \\sin(\\cdot) $ 使用弧度。\n\n- 数据集 B（近似共线性边界情况）：对于 $ i = 1,2,\\dots,40 $，\n  $$ Q_i = 1.0 \\times 10^{5} + 1.0 \\times 10^{4} \\cdot i, $$\n  $$ S_i = 1.0 \\times 10^{2} \\cdot Q_i^{0.97} \\cdot \\left(1 + 10^{-4} \\cdot \\cos(i)\\right). $$\n  对 $ \\cos(\\cdot) $ 使用弧度。\n\n- 数据集 C（小样本边界情况）：对于 $ i = 1,2,\\dots,6 $，\n  $$ u = [0, 2, 1, 3, 0.5, 2.5], \\quad v = [2, 0, -1, 1.5, 2.2, 0.5], $$\n  $$ Q_i = 1.2 \\times 10^{5} + 2.0 \\times 10^{4} \\cdot u_i, $$\n  $$ S_i = 4.5 \\times 10^{4} + 2.0 \\times 10^{3} \\cdot v_i. $$\n\n为每个数据集实现以下步骤：\n1. 计算 $ x_{1,i} = \\ln Q_i $ 和 $ x_{2,i} = \\ln S_i $。\n2. 对于 $ \\{x_1, x_2\\} $ 中的每个预测变量 $ x_j $，估计一个辅助 OLS 回归，该回归将 $ x_j $ 对一个截距项和另一个预测变量 $ x_k $（其中 $ j \\neq k $）进行回归。根据这个辅助回归，使用其基于平方和的定义来计算决定系数 $ R_j^2 $。\n3. 使用基于从辅助回归中获得的 $ R_j^2 $ 的方差膨胀框架，量化每个预测变量的多重共线性，并报告所得的 VIF 值。\n4. 使用两条决策规则解释能源成本数据集中的警戒阈值。为每个数据集定义两个布尔值：一个表示是否有任何预测变量的 VIF 严格超过 $ 5 $（中度警戒），另一个表示是否有任何预测变量的 VIF 严格超过 $ 10 $（高度警戒）。所有比较都使用小数表示，而非百分号。\n\n输出规格：\n- 对于每个数据集，生成一个包含四个条目的列表：$ \\ln Q $ 的 VIF（四舍五入到 $ 3 $ 位小数），$ \\ln S $ 的 VIF（四舍五入到 $ 3 $ 位小数），一个指示是否有任何 VIF $  5 $ 的布尔值，以及一个指示是否有任何 VIF $  10 $ 的布尔值。\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个数据集的结果都包含在自己的方括号内，并按 A、B、C 的顺序排列。例如，输出结构必须是 $ [[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]] $。\n- 不允许外部输入；所有计算必须是自包含的。",
            "solution": "该问题要求从基本原理出发，为一个双因素经验曲线模型实现多重共线性诊断。单位成本 $C$ 的模型由以下对数形式给出：\n$$ \\ln C = \\alpha - b \\ln Q - c \\ln S $$\n其中 $Q$ 是累积产量，$S$ 是累积知识存量，$\\alpha$、$b$ 和 $c$ 是参数。该线性模型中的预测变量是 $x_1 = \\ln Q$ 和 $x_2 = \\ln S$。当这些预测变量高度线性相关时，就会出现多重共线性，这会夸大参数 $b$ 和 $c$ 的普通最小二乘法 (OLS) 估计的方差，使其不可靠。\n\n所选的诊断工具是方差膨胀因子 (VIF)。对于多元回归模型中的给定预测变量 $x_j$，其 VIF 量化了由于其与其他预测变量的线性依赖性，其估计系数的方差增加了多少。预测变量 $x_j$ 的 VIF 定义为：\n$$ \\text{VIF}_j = \\frac{1}{1 - R_j^2} $$\n在这里，$R_j^2$ 是一个辅助 OLS 回归的决定系数，其中 $x_j$ 被视为因变量，所有其他预测变量（在本例中，仅为 $x_k$，其中 $k \\neq j$）和一个截距项是自变量。\n\n任务是为三个给定的数据集计算预测变量 $x_1 = \\ln Q$ 和 $x_2 = \\ln S$ 的 $\\text{VIF}_1$ 和 $\\text{VIF}_2$。这需要从基本原理出发执行两个辅助简单线性回归：\n1.  将 $x_1$ 对 $x_2$ 进行回归：$x_{1,i} = \\beta_{10} + \\beta_{11} x_{2,i} + \\epsilon_{1,i}$\n2.  将 $x_2$ 对 $x_1$ 进行回归：$x_{2,i} = \\beta_{20} + \\beta_{21} x_{1,i} + \\epsilon_{2,i}$\n\n对于一组 $n$ 个观测值 $\\{(x_i, y_i)\\}_{i=1}^n$，一个因变量 $y$ 对一个自变量 $x$ 的通用简单线性回归模型是 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$。最小化残差平方和的 OLS 估计量 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$ 由以下公式给出：\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n其中 $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ 和 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ 是样本均值。\n\n一旦估计出系数，就可以计算决定系数 $R^2$。$R^2$ 衡量因变量方差中可由自变量（们）预测的部分。它定义为：\n$$ R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}} $$\n其中：\n-   总平方和 (SST) = $\\sum_{i=1}^n (y_i - \\bar{y})^2$，衡量因变量的总样本方差。\n-   残差平方和 (SSR) = $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$，衡量模型未能解释的方差，其中 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ 是预测值。\n\n在两个预测变量 $x_1$ 和 $x_2$ 的特定情况下，将 $x_1$ 对 $x_2$ 回归所得的决定系数（$R_1^2$）与将 $x_2$ 对 $x_1$ 回归所得的决定系数（$R_2^2$）是相同的。两者都等于 $x_1$ 和 $x_2$ 之间皮尔逊相关系数的平方，即 $R_1^2 = R_2^2 = r_{x_1, x_2}^2$。因此，它们的 VIF 值必须相同：$\\text{VIF}_1 = \\text{VIF}_2$。这提供了一个有用的一致性检查。\n\n每个数据集的计算过程如下：\n1.  根据提供的公式生成累积产量 $\\{Q_i\\}$ 和知识存量 $\\{S_i\\}$ 的时间序列数据。\n    对于数据集 A，其中 $i = 1, 2, \\dots, 20$：\n    $Q_i = 3.0 \\times 10^{5} + 1.8 \\times 10^{4} \\cdot i^{1.1}$\n    $S_i = 2.5 \\times 10^{4} + 7.0 \\times 10^{3} \\cdot \\sin(0.2 \\, i) + 1.4 \\times 10^{3} \\cdot i$\n    对于数据集 B，其中 $i = 1, 2, \\dots, 40$：\n    $Q_i = 1.0 \\times 10^{5} + 1.0 \\times 10^{4} \\cdot i$\n    $S_i = 1.0 \\times 10^{2} \\cdot Q_i^{0.97} \\cdot (1 + 10^{-4} \\cdot \\cos(i))$\n    对于数据集 C，其中 $i = 1, 2, \\dots, 6$：\n    $u = [0, 2, 1, 3, 0.5, 2.5]$，$v = [2, 0, -1, 1.5, 2.2, 0.5]$\n    $Q_i = 1.2 \\times 10^{5} + 2.0 \\times 10^{4} \\cdot u_i$\n    $S_i = 4.5 \\times 10^{4} + 2.0 \\times 10^{3} \\cdot v_i$\n2.  使用自然对数转换数据：$x_{1,i} = \\ln Q_i$ 和 $x_{2,i} = \\ln S_i$。\n3.  要计算 $x_1 = \\ln Q$ 的 $\\text{VIF}_1$：\n    a.  执行 OLS 回归，其中 $y=x_1$，$x=x_2$，以求得 $\\hat{\\beta}_{10}, \\hat{\\beta}_{11}$。\n    b.  使用此回归的 SST 和 SSR 计算 $R_1^2$。\n    c.  计算 $\\text{VIF}_1 = 1 / (1 - R_1^2)$。\n4.  要计算 $x_2 = \\ln S$ 的 $\\text{VIF}_2$：\n    a.  执行 OLS 回归，其中 $y=x_2$，$x=x_1$，以求得 $\\hat{\\beta}_{20}, \\hat{\\beta}_{21}$。\n    b.  计算 $R_2^2$。\n    c.  计算 $\\text{VIF}_2 = 1 / (1 - R_2^2)$。\n5.  应用决策规则：确定是否有任何 VIF 值超过 $5$（中度警戒）和 $10$（高度警戒）的阈值。\n6.  每个数据集的最终结果是一个列表，包含 $ \\ln Q $ 的 VIF（四舍五入到 3 位小数）、$ \\ln S $ 的 VIF（四舍五入到 3 位小数）以及对应于决策规则的两个布尔值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multicollinearity diagnosis problem for three datasets.\n    \"\"\"\n\n    def calculate_vif(y_vec, x_vec):\n        \"\"\"\n        Computes the Variance Inflation Factor (VIF) for a predictor by regressing it\n        on another predictor from first principles (OLS and R-squared).\n\n        Args:\n            y_vec (np.ndarray): The dependent variable vector (the predictor for which VIF is calculated).\n            x_vec (np.ndarray): The independent variable vector (the other predictor).\n\n        Returns:\n            float: The calculated VIF. Returns np.inf for perfect multicollinearity.\n        \"\"\"\n        n = len(y_vec)\n        if n  2:\n            return np.nan \n\n        # Calculate means\n        y_mean = np.mean(y_vec)\n        x_mean = np.mean(x_vec)\n\n        # Calculate OLS coefficient beta_1 for y = beta_0 + beta_1*x\n        numerator = np.sum((x_vec - x_mean) * (y_vec - y_mean))\n        denominator = np.sum((x_vec - x_mean)**2)\n\n        if denominator == 0:\n            # The independent variable has zero variance, regression is ill-defined.\n            return np.nan\n\n        beta_1 = numerator / denominator\n        beta_0 = y_mean - beta_1 * x_mean\n\n        # Calculate predicted values and sums of squares\n        y_pred = beta_0 + beta_1 * x_vec\n        sst = np.sum((y_vec - y_mean)**2)\n        ssr = np.sum((y_vec - y_pred)**2)\n\n        if sst == 0:\n            # The dependent variable has zero variance.\n            return np.nan\n\n        # Calculate R-squared\n        r_squared = 1.0 - (ssr / sst)\n\n        # Handle perfect multicollinearity case (R^2 = 1)\n        if r_squared >= 1.0 - 1e-12: # Use a tolerance for floating point comparison\n            return np.inf\n\n        # Calculate VIF\n        vif = 1.0 / (1.0 - r_squared)\n        return vif\n\n    # Define the datasets\n    # Dataset A\n    i_A = np.arange(1, 21)\n    Q_A = 3.0e5 + 1.8e4 * np.power(i_A, 1.1)\n    S_A = 2.5e4 + 7.0e3 * np.sin(0.2 * i_A) + 1.4e3 * i_A\n\n    # Dataset B\n    i_B = np.arange(1, 41)\n    Q_B = 1.0e5 + 1.0e4 * i_B\n    S_B = 1.0e2 * np.power(Q_B, 0.97) * (1.0 + 1e-4 * np.cos(i_B))\n\n    # Dataset C\n    u_C = np.array([0.0, 2.0, 1.0, 3.0, 0.5, 2.5])\n    v_C = np.array([2.0, 0.0, -1.0, 1.5, 2.2, 0.5])\n    Q_C = 1.2e5 + 2.0e4 * u_C\n    S_C = 4.5e4 + 2.0e3 * v_C\n\n    test_cases = [\n        {\"name\": \"A\", \"Q\": Q_A, \"S\": S_A},\n        {\"name\": \"B\", \"Q\": Q_B, \"S\": S_B},\n        {\"name\": \"C\", \"Q\": Q_C, \"S\": S_C},\n    ]\n\n    all_results_str = []\n    \n    for case in test_cases:\n        # Step 1: Compute transformed predictors\n        x1 = np.log(case[\"Q\"])  # x1 corresponds to ln(Q)\n        x2 = np.log(case[\"S\"])  # x2 corresponds to ln(S)\n\n        # Step 2  3: Compute VIF for each predictor\n        # VIF for ln(Q) is based on regression of ln(Q) on ln(S)\n        vif_lnQ = calculate_vif(y_vec=x1, x_vec=x2)\n        \n        # VIF for ln(S) is based on regression of ln(S) on ln(Q)\n        vif_lnS = calculate_vif(y_vec=x2, x_vec=x1)\n        \n        # Step 4: Apply decision rules\n        # Since VIFs must be equal for 2 predictors, we can check one\n        moderate_concern = vif_lnQ > 5.0\n        strong_concern = vif_lnQ > 10.0\n        \n        # Round VIFs for output\n        vif_lnQ_rounded = round(vif_lnQ, 3)\n        vif_lnS_rounded = round(vif_lnS, 3)\n        \n        # Format the result string for this case to avoid spaces\n        # Python's str(bool) gives 'True'/'False' literals\n        result_str = (\n            f\"[{vif_lnQ_rounded},\"\n            f\"{vif_lnS_rounded},\"\n            f\"{str(moderate_concern)},\"\n            f\"{str(strong_concern)}]\"\n        )\n        all_results_str.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "模型估计完成后，我们的工作并未结束；解释结果同样至关重要。在经验曲线分析中，我们通常更关心直观的“学习率”（$LR$），而不是模型中的经验指数（$b$）。本练习将演示如何将从模型中得到的 $b$ 的置信区间，正确地转换为学习率的置信区间，并解释为何该区间通常是不对称的，从而加深您对统计不确定性传播的理解。",
            "id": "4109590",
            "problem": "考虑一种技术，其单位成本遵循嵌入在能源系统建模中常用的双因素设定中的单因素经验关系：单位成本$C_t$的对数被建模为 $\\ln(C_t) = \\beta_0 - b \\ln(Q_t) + \\gamma Z_t + \\varepsilon_t$，其中$Q_t$是累积产量，$Z_t$是一个辅助驱动因素（例如，日历时间或研发强度），$b$是与累积产量相关的经验指数，$\\varepsilon_t$是一个均值为零的扰动项。在典型的Wright型单因素经验曲线下，累积产量翻倍（$Q_t \\mapsto 2 Q_t$）意味着进步率为 $PR = 2^{-b}$，学习率为 $LR = 1 - PR = 1 - 2^{-b}$。假设一个大样本、异方差稳健的$b$估计量得出$\\hat{b} = 0.265$，标准误为$s_b = 0.022959$，并且当$\\alpha = 0.05$时，$b$的$(1 - \\alpha)$置信区间为$[0.22, 0.31]$。\n\n从上述基本定义出发，并仅使用关于置信区间和单调变换的经过充分检验的统计学事实，通过映射$LR(b) = 1 - 2^{-b}$变换$b$的置信区间端点，来构建学习率$LR$的$(1 - \\alpha)$置信区间。然后，解释为什么得到的$LR$区间通常围绕点估计$LR(\\hat{b})$不对称，并将其与通过在$b = \\hat{b}$处对$LR(b)$进行线性化得到的一阶（delta方法）对称近似进行对比。\n\n将您最终的$LR$数值区间表示为小数，并将每个端点四舍五入到四位有效数字。",
            "solution": "### 解答\n\n问题要求在给定经验指数$b$的置信区间的情况下，构建学习率$LR$的$(1-\\alpha)$置信区间。这两个量之间的关系由以下函数给出：\n$$LR(b) = 1 - 2^{-b}$$\n该函数也可以使用自然指数函数写成$LR(b) = 1 - \\exp(-b \\ln(2))$。\n\n只要变换函数在原始参数的置信区间上是严格单调的，就可以通过将该函数应用于区间端点来找到变换后参数的置信区间。为验证这一点，我们计算$LR(b)$关于$b$的一阶导数：\n$$\\frac{d(LR)}{db} = \\frac{d}{db}(1 - \\exp(-b \\ln(2))) = - \\exp(-b \\ln(2)) \\cdot (-\\ln(2)) = (\\ln(2)) 2^{-b}$$\n由于对于所有有限的$b$，$\\ln(2)  0$且$2^{-b}  0$，因此导数$\\frac{d(LR)}{db}$严格为正。这证实了$LR(b)$是$b$的一个严格递增函数。\n\n鉴于$b$的$(1 - \\alpha)$置信区间为$[b_{lower}, b_{upper}]$，并且变换$LR(b)$是严格递增的，因此$LR$对应的$(1 - \\alpha)$置信区间为$[LR(b_{lower}), LR(b_{upper})]$。\n\n问题给出了$b$的$95\\%$置信区间为$[0.22, 0.31]$。因此，我们有：\n- $b_{lower} = 0.22$\n- $b_{upper} = 0.31$\n\n$LR$置信区间的下界是：\n$$LR_{lower} = LR(b_{lower}) = 1 - 2^{-0.22} \\approx 1 - 0.858564 = 0.141436$$\n$LR$置信区间的上界是：\n$$LR_{upper} = LR(b_{upper}) = 1 - 2^{-0.31} \\approx 1 - 0.806639 = 0.193361$$\n\n按照要求将每个端点四舍五入到四位有效数字，我们得到：\n- $LR_{lower} \\approx 0.1414$\n- $LR_{upper} \\approx 0.1934$\n最终得到的$LR$置信区间为$[0.1414, 0.1934]$。\n\n任务的下一部分是解释该区间的不对称性。$b$的置信区间$[0.22, 0.31]$是关于点估计$\\hat{b} = 0.265$对称的，因为$0.265 - 0.22 = 0.045$且$0.31 - 0.265 = 0.045$。学习率的点估计为：\n$$LR(\\hat{b}) = LR(0.265) = 1 - 2^{-0.265} \\approx 1 - 0.832203 = 0.167797$$\n四舍五入后，$LR(\\hat{b}) \\approx 0.1678$。\n\n区间$[LR_{lower}, LR_{upper}]$关于点估计$LR(\\hat{b})$的不对称性是通过比较点估计到两端点的距离来确定的：\n- 上半区间宽度：$LR_{upper} - LR(\\hat{b}) \\approx 0.193361 - 0.167797 = 0.025564$\n- 下半区间宽度：$LR(\\hat{b}) - LR_{lower} \\approx 0.167797 - 0.141436 = 0.026361$\n由于$0.025564 \\neq 0.026361$，所以$LR$的置信区间是不对称的。具体来说，点估计$LR(\\hat{b})$比下界更接近上界。\n\n这种不对称性源于变换$LR(b) = 1 - 2^{-b}$是非线性的。该非线性的性质由函数的二阶导数揭示：\n$$\\frac{d^2(LR)}{db^2} = \\frac{d}{db}\\left((\\ln(2)) 2^{-b}\\right) = (\\ln(2)) \\cdot (-\\ln(2) 2^{-b}) = -(\\ln(2))^2 2^{-b}$$\n由于$-(\\ln(2))^2  0$且$2^{-b}  0$，二阶导数$\\frac{d^2(LR)}{db^2}$严格为负。这表明函数$LR(b)$是严格凹函数。对于一个严格凹函数，其斜率随着输入变量的增加而减小。因此，对于一个固定的步长$\\delta  0$，输入值越大，函数值的变化就越小，即$LR(\\hat{b} + \\delta) - LR(\\hat{b})  LR(\\hat{b}) - LR(\\hat{b} - \\delta)$。这正是观察到的不对称性的来源，即区间的上半部分比下半部分更窄。\n\n相比之下，一阶（delta方法）近似构造的是一个对称的置信区间。Delta方法通过在点估计$\\hat{b}$附近对函数$LR(b)$进行线性化来近似变换后参数$LR$的标准误：\n$$s_{LR} \\approx \\left| \\frac{d(LR)}{db} \\right|_{b=\\hat{b}} | \\cdot s_b = \\left| (\\ln 2) 2^{-\\hat{b}} \\right| \\cdot s_b$$\n由此构造出的置信区间是天生对称的：\n$$[LR(\\hat{b}) - z_{\\alpha/2} s_{LR}, \\quad LR(\\hat{b}) + z_{\\alpha/2} s_{LR}]$$\n该方法实际上是用曲线函数$LR(b)$在$\\hat{b}$处的切线来近似它。通过忽略曲率（即二阶导数），delta方法未能捕捉到由变换的非线性性质引起的不对称性。此处所执行的对区间端点的精确变换更为优越，因为它完全考虑了这种非线性。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1414  0.1934 \\end{pmatrix}}\n$$"
        }
    ]
}