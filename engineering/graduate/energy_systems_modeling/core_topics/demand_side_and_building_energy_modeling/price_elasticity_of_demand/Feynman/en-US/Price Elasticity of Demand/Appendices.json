{
    "hands_on_practices": [
        {
            "introduction": "Before we can estimate elasticity from data, we must understand its microeconomic foundation. This practice grounds the concept of demand response in the rigorous theory of consumer choice. By deriving the elasticity of substitution for the widely-used Constant Elasticity of Substitution (CES) utility function, you will connect the abstract principle of utility maximization to a concrete, measurable parameter that governs how consumers substitute goods like peak and off-peak electricity .",
            "id": "4113588",
            "problem": "Consider a representative electricity consumer in an energy system with time-of-use pricing who chooses peak-period consumption $Q_{p}$ and off-peak consumption $Q_{o}$ to maximize a two-period utility function subject to a linear budget. Let prices be $(P_{p}, P_{o})$ and income be $Y$. The consumer’s utility function is Constant Elasticity of Substitution (CES): \n$$U(Q_{p}, Q_{o}) = \\left(\\alpha Q_{p}^{\\rho} + (1 - \\alpha) Q_{o}^{\\rho}\\right)^{\\frac{1}{\\rho}},$$\nwhere $0  \\alpha  1$ and $\\rho \\in (-\\infty, 1)$ with $\\rho \\neq 0$. The budget constraint is \n$$P_{p} Q_{p} + P_{o} Q_{o} = Y,$$\nand both consumption quantities satisfy $Q_{p}  0$ and $Q_{o}  0$. \n\nDefine the elasticity of substitution between peak and off-peak consumption as the absolute Hicksian elasticity along a fixed utility level:\n$$\\sigma \\equiv \\left|\\frac{d \\ln\\!\\left(\\frac{Q_{p}}{Q_{o}}\\right)}{d \\ln\\!\\left(\\text{MRS}_{po}\\right)}\\right|,$$\nwhere $\\text{MRS}_{po}$ is the Marginal Rate of Substitution (MRS) of peak for off-peak, given by \n$$\\text{MRS}_{po} \\equiv \\frac{\\partial U / \\partial Q_{p}}{\\partial U / \\partial Q_{o}}.$$\n\nStarting from first principles—utility maximization under the budget constraint, the definition of the Marginal Rate of Substitution (MRS), and the optimality condition equating MRS to the price ratio for an interior solution—derive a closed-form expression for the elasticity of substitution $\\sigma$ for the CES utility above as a function of the parameter $\\rho$ only. Express your final answer as a single symbolic expression. No rounding is required. The final elasticity is dimensionless; do not include any units in your answer.",
            "solution": "The problem as stated is well-posed, scientifically grounded, and internally consistent, containing all necessary information for a rigorous derivation. We can therefore proceed to find the elasticity of substitution, $\\sigma$.\n\nThe objective is to derive a closed-form expression for the elasticity of substitution, $\\sigma$, for the given Constant Elasticity of Substitution (CES) utility function. The definition provided is:\n$$ \\sigma \\equiv \\left| \\frac{d \\ln\\left(\\frac{Q_{p}}{Q_{o}}\\right)}{d \\ln\\left(\\text{MRS}_{po}\\right)} \\right| $$\nOur derivation will proceed in three main steps: first, we calculate the Marginal Rate of Substitution ($\\text{MRS}_{po}$); second, we establish a relationship between the consumption ratio $\\frac{Q_{p}}{Q_{o}}$ and the $\\text{MRS}_{po}$; and third, we compute the required logarithmic derivative.\n\nThe consumer's utility function is:\n$$ U(Q_{p}, Q_{o}) = \\left(\\alpha Q_{p}^{\\rho} + (1 - \\alpha) Q_{o}^{\\rho}\\right)^{\\frac{1}{\\rho}} $$\nwhere $0  \\alpha  1$ and $\\rho \\in (-\\infty, 1)$ with $\\rho \\neq 0$.\n\nFirst, we compute the marginal utilities, which are the partial derivatives of the utility function with respect to $Q_{p}$ and $Q_{o}$. Using the chain rule, for $Q_{p}$:\n$$ \\frac{\\partial U}{\\partial Q_{p}} = \\frac{1}{\\rho} \\left(\\alpha Q_{p}^{\\rho} + (1 - \\alpha) Q_{o}^{\\rho}\\right)^{\\frac{1}{\\rho} - 1} \\cdot (\\alpha \\rho Q_{p}^{\\rho - 1}) $$\n$$ \\frac{\\partial U}{\\partial Q_{p}} = \\alpha Q_{p}^{\\rho - 1} \\left(\\alpha Q_{p}^{\\rho} + (1 - \\alpha) Q_{o}^{\\rho}\\right)^{\\frac{1 - \\rho}{\\rho}} $$\nSimilarly, for $Q_{o}$:\n$$ \\frac{\\partial U}{\\partial Q_{o}} = \\frac{1}{\\rho} \\left(\\alpha Q_{p}^{\\rho} + (1 - \\alpha) Q_{o}^{\\rho}\\right)^{\\frac{1}{\\rho} - 1} \\cdot ((1 - \\alpha) \\rho Q_{o}^{\\rho - 1}) $$\n$$ \\frac{\\partial U}{\\partial Q_{o}} = (1 - \\alpha) Q_{o}^{\\rho - 1} \\left(\\alpha Q_{p}^{\\rho} + (1 - \\alpha) Q_{o}^{\\rho}\\right)^{\\frac{1 - \\rho}{\\rho}} $$\n\nNext, we find the Marginal Rate of Substitution, $\\text{MRS}_{po}$, which is defined as the ratio of the marginal utilities:\n$$ \\text{MRS}_{po} \\equiv \\frac{\\partial U / \\partial Q_{p}}{\\partial U / \\partial Q_{o}} = \\frac{\\alpha Q_{p}^{\\rho - 1} \\left(\\alpha Q_{p}^{\\rho} + (1 - \\alpha) Q_{o}^{\\rho}\\right)^{\\frac{1 - \\rho}{\\rho}}}{(1 - \\alpha) Q_{o}^{\\rho - 1} \\left(\\alpha Q_{p}^{\\rho} + (1 - \\alpha) Q_{o}^{\\rho}\\right)^{\\frac{1 - \\rho}{\\rho}}} $$\nThe term common to both the numerator and the denominator cancels, yielding:\n$$ \\text{MRS}_{po} = \\frac{\\alpha}{1 - \\alpha} \\frac{Q_{p}^{\\rho - 1}}{Q_{o}^{\\rho - 1}} = \\frac{\\alpha}{1 - \\alpha} \\left(\\frac{Q_{p}}{Q_{o}}\\right)^{\\rho - 1} $$\nFor an interior solution, the optimality condition equates the $\\text{MRS}_{po}$ to the price ratio, $\\frac{P_p}{P_o}$. While this condition is part of the problem setup, the derivation of $\\sigma$ depends only on the functional relationship between the consumption ratio and the MRS, not on the prices themselves.\n\nOur goal is to find the elasticity of substitution, $\\sigma$, which involves the logarithm of the consumption ratio $\\frac{Q_p}{Q_o}$ and the logarithm of the $\\text{MRS}_{po}$. We therefore rearrange the expression for $\\text{MRS}_{po}$ to isolate the consumption ratio:\n$$ \\left(\\frac{Q_{p}}{Q_{o}}\\right)^{\\rho - 1} = \\frac{1 - \\alpha}{\\alpha} \\text{MRS}_{po} $$\nTo solve for $\\frac{Q_{p}}{Q_{o}}$, we raise both sides to the power of $\\frac{1}{\\rho - 1}$:\n$$ \\frac{Q_{p}}{Q_{o}} = \\left(\\frac{1 - \\alpha}{\\alpha} \\text{MRS}_{po} \\right)^{\\frac{1}{\\rho - 1}} = \\left(\\frac{1 - \\alpha}{\\alpha}\\right)^{\\frac{1}{\\rho - 1}} \\left(\\text{MRS}_{po}\\right)^{\\frac{1}{\\rho - 1}} $$\nNow, we take the natural logarithm of both sides of this equation:\n$$ \\ln\\left(\\frac{Q_{p}}{Q_{o}}\\right) = \\ln\\left[ \\left(\\frac{1 - \\alpha}{\\alpha}\\right)^{\\frac{1}{\\rho - 1}} \\left(\\text{MRS}_{po}\\right)^{\\frac{1}{\\rho - 1}} \\right] $$\nUsing the properties of logarithms, $\\ln(xy) = \\ln(x) + \\ln(y)$ and $\\ln(x^a) = a \\ln(x)$, we get:\n$$ \\ln\\left(\\frac{Q_{p}}{Q_{o}}\\right) = \\frac{1}{\\rho - 1} \\ln\\left(\\frac{1 - \\alpha}{\\alpha}\\right) + \\frac{1}{\\rho - 1} \\ln\\left(\\text{MRS}_{po}\\right) $$\nWe are now prepared to calculate the elasticity of substitution, $\\sigma$. We differentiate $\\ln\\left(\\frac{Q_{p}}{Q_{o}}\\right)$ with respect to $\\ln\\left(\\text{MRS}_{po}\\right)$. Note that the term $\\frac{1}{\\rho - 1} \\ln\\left(\\frac{1 - \\alpha}{\\alpha}\\right)$ is a constant with respect to $\\text{MRS}_{po}$.\n$$ \\frac{d \\ln\\left(\\frac{Q_{p}}{Q_{o}}\\right)}{d \\ln\\left(\\text{MRS}_{po}\\right)} = \\frac{d}{d\\ln\\left(\\text{MRS}_{po}\\right)} \\left[ \\frac{1}{\\rho - 1} \\ln\\left(\\frac{1 - \\alpha}{\\alpha}\\right) + \\frac{1}{\\rho - 1} \\ln\\left(\\text{MRS}_{po}\\right) \\right] $$\n$$ \\frac{d \\ln\\left(\\frac{Q_{p}}{Q_{o}}\\right)}{d \\ln\\left(\\text{MRS}_{po}\\right)} = 0 + \\frac{1}{\\rho - 1} \\cdot 1 = \\frac{1}{\\rho - 1} $$\nFinally, we apply the absolute value as per the definition of $\\sigma$:\n$$ \\sigma = \\left| \\frac{1}{\\rho - 1} \\right| $$\nThe problem states that the parameter $\\rho$ is in the range $\\rho \\in (-\\infty, 1)$, which implies $\\rho  1$. Consequently, the denominator $\\rho - 1$ is always negative. Therefore, the absolute value is:\n$$ |\\rho - 1| = -(\\rho - 1) = 1 - \\rho $$\nSubstituting this into the expression for $\\sigma$:\n$$ \\sigma = \\frac{1}{|\\rho - 1|} = \\frac{1}{1 - \\rho} $$\nThis expression for the elasticity of substitution depends only on the parameter $\\rho$, as required by the problem statement. This is a classic result for the CES utility function.",
            "answer": "$$\n\\boxed{\\frac{1}{1-\\rho}}\n$$"
        },
        {
            "introduction": "In the real world, price is rarely the only factor influencing energy consumption. This computational exercise moves from pure theory to the practical challenge of estimation with observational data. You will explore the critical issue of omitted variable bias by simulating a scenario where price changes are correlated with weather, a common confounder. By implementing an Ordinary Least Squares (OLS) regression, you will see firsthand how failing to control for relevant variables can lead to incorrect elasticity estimates .",
            "id": "4113552",
            "problem": "Consider a dataset generated from Advanced Metering Infrastructure (AMI) smart meters that record electricity consumption across two time windows within a day: an off-peak window and a peak window. Let $i \\in \\{1,\\dots,N\\}$ index households. Define the natural logarithm of consumption change as $y_i = \\ln\\left(\\frac{C^{\\text{peak}}_i}{C^{\\text{off}}_i}\\right)$ and the natural logarithm of the price ratio as $x_i = \\ln\\left(\\frac{P^{\\text{peak}}_i}{P^{\\text{off}}_i}\\right)$, where $C^{\\text{peak}}_i$ and $C^{\\text{off}}_i$ are consumption in kilowatt-hours (kWh), and $P^{\\text{peak}}_i$ and $P^{\\text{off}}_i$ are prices in dollars per kilowatt-hour ($\\$/\\text{kWh}$). Let $w_i$ denote a scalar weather change measure between the peak and off-peak windows, for example, Cooling Degree Hours (CDH) in degree-hour units, constructed so that larger $w_i$ implies higher thermal load during the peak window.\n\nAssume the following structural relationship arises from consumer optimization under time-varying prices, consistent with the definition of price elasticity of demand:\n$$\ny_i = \\alpha + \\gamma x_i + \\beta_w w_i + \\varepsilon_i,\n$$\nwhere $\\alpha$ is a constant, $\\gamma$ is the cross-time substitution elasticity (dimensionless), $\\beta_w$ captures weather sensitivity (change in $\\ln$ consumption per unit of weather), and $\\varepsilon_i$ is a mean-zero disturbance satisfying $E[\\varepsilon_i \\mid x_i, w_i] = 0$. The objective is to estimate $\\gamma$ from a cross-section of households using Ordinary Least Squares (OLS) regression both without weather controls and with weather controls, and to reason about identification when $x_i$ and $w_i$ co-vary.\n\nYour task is to write a complete program that:\n1. Generates synthetic data for $(x_i,w_i,y_i)$ according to the model above using the given parameter sets.\n2. Estimates $\\gamma$ by OLS in two specifications: (i) regressing $y_i$ on $x_i$ with an intercept only, and (ii) regressing $y_i$ on both $x_i$ and $w_i$ with an intercept.\n3. Returns, for each test case, two floating-point numbers: the estimated $\\gamma$ without weather controls and the estimated $\\gamma$ with weather controls. These outputs are dimensionless quantities, and no physical unit applies to the elasticity estimates. Do not use a percentage sign; outputs must be decimals.\n\nData generation details to ensure scientific realism:\n- Draw $x_i$ independently from a normal distribution with mean $\\mu_X$ and standard deviation $\\sigma_X$, representing exogenous variation in the peak-to-off-peak price ratio.\n- Construct $w_i$ to have a specified correlation with $x_i$. To achieve correlation coefficient $\\rho_{XW}$ in finite samples, standardize $x_i$ to zero mean and unit variance within the sample to form $x_i^{\\text{std}}$, draw $z_i \\sim \\mathcal{N}(0,1)$ independently of $x_i$, set $w_i^{\\text{std}} = \\rho_{XW} x_i^{\\text{std}} + \\sqrt{1-\\rho_{XW}^2}\\, z_i$, and scale $w_i = \\sigma_W w_i^{\\text{std}}$. This yields $\\operatorname{Corr}(x_i^{\\text{std}}, w_i^{\\text{std}}) \\approx \\rho_{XW}$.\n- Draw $\\varepsilon_i$ independently from a normal distribution with mean $0$ and standard deviation $\\sigma_\\varepsilon$.\n- Use $\\alpha = 0$ in all test cases.\n\nUse the natural logarithm $\\ln(\\cdot)$ for all log quantities. Prices must be interpreted in $\\$/\\text{kWh}$ and consumption in $\\text{kWh}$, but the program will only manipulate the log ratios, which are dimensionless.\n\nImplement OLS from first principles by solving the least-squares normal equations for the intercept and coefficients in each specification, using numerically stable methods appropriate for advanced programming tasks.\n\nTest Suite:\nFor each test case, use the tuple $(N, \\gamma, \\beta_w, \\mu_X, \\sigma_X, \\rho_{XW}, \\sigma_W, \\sigma_\\varepsilon, \\text{seed})$ with the following values:\n- Test case $1$ (happy path, exogenous price variation): $(1000, -0.3, 0.5, 0.2, 0.1, 0.0, 1.0, 0.05, 1)$.\n- Test case $2$ (concurrent weather positively correlated with price ratio): $(1000, -0.3, 0.5, 0.2, 0.1, 0.7, 1.0, 0.05, 2)$.\n- Test case $3$ (concurrent weather negatively correlated with price ratio): $(1000, -0.3, 0.5, 0.2, 0.1, -0.7, 1.0, 0.05, 3)$.\n- Test case $4$ (boundary case: near-zero price variation): $(1000, -0.3, 0.5, 0.2, 0.01, 0.0, 1.0, 0.05, 4)$.\n- Test case $5$ (edge case: high multicollinearity with weather control): $(1000, -0.3, 0.5, 0.2, 0.1, 0.95, 1.0, 0.05, 5)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The output should list, in order, for each test case, first the estimated $\\gamma$ without weather controls and then the estimated $\\gamma$ with weather controls. For example, the output format must be exactly\n$[\\text{tc1\\_no\\_w},\\text{tc1\\_with\\_w},\\text{tc2\\_no\\_w},\\dots,\\text{tc5\\_with\\_w}]$,\nwhere each entry is a floating-point decimal number.",
            "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed, and scientifically grounded exercise in econometrics. It is free from any of the invalidating flaws listed in the instructions. The problem asks for the estimation of a price elasticity parameter, $\\gamma$, from synthetically generated data, using OLS regression under two different model specifications. This is a standard procedure to demonstrate the concept of omitted variable bias.\n\nThe structural relationship from which the data is generated is:\n$$\ny_i = \\alpha + \\gamma x_i + \\beta_w w_i + \\varepsilon_i\n$$\nwhere $y_i$ is the log consumption ratio, $x_i$ is the log price ratio, $w_i$ is a weather-related variable, and $\\varepsilon_i$ is a random disturbance term. The assumption $E[\\varepsilon_i \\mid x_i, w_i] = 0$ implies that if we include both $x_i$ and $w_i$ in a regression, the OLS estimators for the coefficients $\\gamma$ and $\\beta_w$ will be unbiased.\n\nWe are tasked with estimating $\\gamma$ using two models:\n1.  A \"short\" regression that omits the weather variable: $y_i$ is regressed on $x_i$ and a constant.\n2.  A \"long\" regression that includes the weather variable: $y_i$ is regressed on $x_i$, $w_i$, and a constant.\n\nThe core of the problem lies in implementing the OLS estimator and observing its behavior. For a general linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, the OLS estimator $\\hat{\\boldsymbol{\\beta}}$ for the coefficient vector $\\boldsymbol{\\beta}$ is derived by minimizing the sum of squared residuals, $\\boldsymbol{\\varepsilon}^T\\boldsymbol{\\varepsilon} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$. This minimization problem yields the normal equations:\n$$\n(\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}\n$$\nThe solution for the estimated coefficients is thus:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nThis equation will be solved for $\\hat{\\boldsymbol{\\beta}}$ for each of the two models. Numerically, it is more stable to solve the linear system presented by the normal equations directly rather than computing the matrix inverse explicitly.\n\nThe data generation process is as follows:\nFirst, a vector of log price ratios $\\mathbf{x}$ of size $N$ is drawn from a normal distribution a $\\mathcal{N}(\\mu_X, \\sigma_X^2)$.\nSecond, a correlated weather vector $\\mathbf{w}$ is constructed. To achieve a target correlation $\\rho_{XW}$ with $\\mathbf{x}$, the vector $\\mathbf{x}$ is first standardized within the sample to have zero mean and unit variance: $\\mathbf{x}^{\\text{std}} = (\\mathbf{x} - \\bar{x}) / s_x$, where $s_x$ is the sample standard deviation. A vector $\\mathbf{z}$ of standard normal random variables, independent of $\\mathbf{x}$, is drawn. A standardized weather vector is then constructed as $\\mathbf{w}^{\\text{std}} = \\rho_{XW} \\mathbf{x}^{\\text{std}} + \\sqrt{1-\\rho_{XW}^2}\\, \\mathbf{z}$. Finally, this vector is scaled to have a desired standard deviation $\\sigma_W$, yielding $\\mathbf{w} = \\sigma_W \\mathbf{w}^{\\text{std}}$.\nThird, a vector of disturbances $\\boldsymbol{\\varepsilon}$ is drawn from $\\mathcal{N}(0, \\sigma_\\varepsilon^2)$.\nFinally, the dependent variable vector is computed as $\\mathbf{y} = \\alpha + \\gamma \\mathbf{x} + \\beta_w \\mathbf{w} + \\boldsymbol{\\varepsilon}$. Per the problem, $\\alpha = 0$.\n\nFor model (i), the \"short\" regression ($y_i$ on $x_i$), the design matrix $\\mathbf{X}_1$ has two columns: a column of ones for the intercept and the vector $\\mathbf{x}$. The resulting coefficient vector is $\\hat{\\boldsymbol{\\beta}}_1 = [\\hat{\\alpha}, \\hat{\\gamma}_{\\text{short}}]^T$. The second element, $\\hat{\\gamma}_{\\text{short}}$, is the estimate of interest. If the omitted variable $w_i$ is correlated with the included variable $x_i$ (i.e., $\\rho_{XW} \\neq 0$) and its true coefficient $\\beta_w$ is non-zero, then the estimate $\\hat{\\gamma}_{\\text{short}}$ will be biased. The expected value of this estimator will be $E[\\hat{\\gamma}_{\\text{short}}] = \\gamma + \\beta_w \\cdot \\delta$, where $\\delta$ is the coefficient from regressing the omitted variable $w_i$ on the included variable $x_i$.\n\nFor model (ii), the \"long\" regression ($y_i$ on $x_i$ and $w_i$), the design matrix $\\mathbf{X}_2$ has three columns: a column of ones, the vector $\\mathbf{x}$, and the vector $\\mathbf{w}$. The resulting coefficient vector is $\\hat{\\boldsymbol{\\beta}}_2 = [\\hat{\\alpha}, \\hat{\\gamma}_{\\text{long}}, \\hat{\\beta}_w]^T$. Because this model matches the true data generating process and satisfies the OLS assumption $E[\\varepsilon_i \\mid x_i, w_i] = 0$, the estimate $\\hat{\\gamma}_{\\text{long}}$ (the second element of $\\hat{\\boldsymbol{\\beta}}_2$) will be an unbiased and consistent estimator of the true parameter $\\gamma$.\n\nThe program will execute this procedure for each test case, generating data and then computing both $\\hat{\\gamma}_{\\text{short}}$ and $\\hat{\\gamma}_{\\text{long}}$ by solving the respective normal equations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Generates synthetic data and performs OLS estimations for five test cases\n    to analyze omitted variable bias in elasticity estimation.\n    \"\"\"\n    \n    # Test Suite:\n    # (N, gamma_true, beta_w, mu_X, sigma_X, rho_XW, sigma_W, sigma_eps, seed)\n    test_cases = [\n        (1000, -0.3, 0.5, 0.2, 0.1, 0.0, 1.0, 0.05, 1),\n        (1000, -0.3, 0.5, 0.2, 0.1, 0.7, 1.0, 0.05, 2),\n        (1000, -0.3, 0.5, 0.2, 0.1, -0.7, 1.0, 0.05, 3),\n        (1000, -0.3, 0.5, 0.2, 0.01, 0.0, 1.0, 0.05, 4),\n        (1000, -0.3, 0.5, 0.2, 0.1, 0.95, 1.0, 0.05, 5),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N, gamma_true, beta_w, mu_X, sigma_X, rho_XW, sigma_W, sigma_eps, seed = case\n        \n        # Initialize a random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate synthetic data\n        \n        # Draw x_i from a normal distribution\n        x = rng.normal(loc=mu_X, scale=sigma_X, size=N)\n        \n        # Construct w_i to have a specified correlation rho_XW with x_i\n        # Standardize x in the sample\n        # Using ddof=1 for sample standard deviation is standard practice\n        x_std = (x - np.mean(x)) / np.std(x, ddof=1)\n        \n        # Draw z_i from a standard normal distribution, independent of x_i\n        z = rng.normal(loc=0, scale=1, size=N)\n        \n        # Construct standardized weather variable w_std\n        # This construction ensures Corr(x_std, w_std) is approximately rho_XW\n        w_std = rho_XW * x_std + np.sqrt(1 - rho_XW**2) * z\n        \n        # Scale w to have the specified standard deviation sigma_W\n        w = sigma_W * w_std\n        \n        # Draw the error term epsilon_i\n        epsilon = rng.normal(loc=0, scale=sigma_eps, size=N)\n        \n        # Generate the dependent variable y_i using the structural equation\n        # alpha is given as 0\n        y = gamma_true * x + beta_w * w + epsilon\n\n        # 2. Estimate gamma using OLS from first principles\n\n        # Function to perform OLS estimation\n        def ols_estimate(X, y_vec):\n            \"\"\"\n            Solves the OLS normal equations: (X'X)b = X'y for b.\n            Returns the vector of estimated coefficients.\n            \"\"\"\n            # Using np.linalg.solve is more numerically stable than inverting X'X\n            try:\n                coeffs = np.linalg.solve(X.T @ X, X.T @ y_vec)\n                return coeffs\n            except np.linalg.LinAlgError:\n                # Handle cases of perfect multicollinearity, though not expected here\n                # Return NaNs if the matrix is singular\n                return np.full(X.shape[1], np.nan)\n\n        # Specification (i): OLS without weather control (y on x and intercept)\n        # Design matrix X1 = [1, x]\n        X1 = np.vstack([np.ones(N), x]).T\n        coeffs1 = ols_estimate(X1, y)\n        gamma_hat_no_w = coeffs1[1]\n        \n        # Specification (ii): OLS with weather control (y on x, w, and intercept)\n        # Design matrix X2 = [1, x, w]\n        X2 = np.vstack([np.ones(N), x, w]).T\n        coeffs2 = ols_estimate(X2, y)\n        gamma_hat_with_w = coeffs2[1]\n        \n        results.extend([gamma_hat_no_w, gamma_hat_with_w])\n\n    # Format the output as a comma-separated list of floats in brackets\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A single, population-wide elasticity often masks important differences across consumer segments. This advanced practice introduces Bayesian hierarchical modeling as a powerful framework to estimate segment-specific elasticities while \"borrowing strength\" across groups. You will implement an analytical Bayesian model to derive posterior distributions, learning to quantify parameter uncertainty and appreciate the concept of \"shrinkage\" toward a common mean, which is crucial for robust estimation with limited data per segment .",
            "id": "4113551",
            "problem": "Consider the estimation of the price elasticity of demand in an energy systems modeling context across multiple consumer segments using a Bayesian hierarchical model. Let the price elasticity of demand be defined fundamentally as $E_p = \\frac{d \\ln Q}{d \\ln P}$, where $Q$ denotes quantity in kilowatt-hour (kWh) and $P$ denotes price in United States Dollar per kilowatt-hour (USD/kWh). Assume a log-linear demand model for each segment $s$ with time index $t$ given by\n$$\ny_{s,t} = \\alpha_s + \\beta_s x_{s,t} + \\varepsilon_{s,t},\n$$\nwhere $y_{s,t} = \\ln Q_{s,t}$, $x_{s,t} = \\ln P_{s,t}$, $\\alpha_s$ is a segment-specific intercept, $\\beta_s$ is the segment-specific elasticity (note $E_{p,s} = \\beta_s$), and $\\varepsilon_{s,t}$ are independent Gaussian errors with $\\varepsilon_{s,t} \\sim \\mathcal{N}(0,\\sigma_s^2)$ and known $\\sigma_s  0$. Assume a Bayesian hierarchical prior across segments:\n$$\n\\beta_s \\mid \\mu, \\tau^2 \\sim \\mathcal{N}(\\mu, \\tau^2), \\quad \\mu \\sim \\mathcal{N}(m_0, V_0),\n$$\nwith known hyperparameters $m_0 \\in \\mathbb{R}$, $V_0  0$, and $\\tau^2  0$.\n\nFrom the fundamental definitions above and the model structure, derive the posterior distributions needed to quantify uncertainty in $E_{p,s} = \\beta_s$ and in the population mean elasticity $\\mu$. Your derivation must start from the classical properties of the Ordinary Least Squares (OLS) estimator under Gaussian errors and Bayes’ theorem. In particular, use the distributional result for the OLS slope estimator under known Gaussian noise to connect empirical segment-wise estimates to the hierarchical structure, and then derive posterior quantities that can be computed exactly without numerical sampling.\n\nImplementation requirements:\n- Use the natural logarithm for all transformations involving $\\ln$.\n- Prices $P$ must be treated in $\\text{USD}/\\text{kWh}$, and quantities $Q$ must be treated in $\\text{kWh}$. Elasticities are dimensionless and must be expressed as decimals (not as percentages).\n- For each test case, compute for every segment $s$ the posterior mean of $E_{p,s}$ and the endpoints of the symmetric $95$-level Bayesian credible interval. The credible interval endpoints must be reported as $(\\text{mean} \\pm 1.96 \\times \\text{posterior standard deviation})$.\n- Also compute the posterior mean and posterior variance of the population mean elasticity $\\mu$.\n- The final program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is a nested list of the form\n$$\n[\\text{posterior mean of } \\mu,\\ \\text{posterior variance of } \\mu,\\ [[\\text{segment } 1\\ \\text{posterior mean},\\ \\text{lower},\\ \\text{upper}],\\ldots,[\\text{segment } S\\ \\text{posterior mean},\\ \\text{lower},\\ \\text{upper}]]]\n$$\nwith all values expressed as floating-point numbers.\n\nTest suite:\n- Test Case $1$ (balanced variation across segments):\n    - Hyperparameters: $m_0 = -0.3$, $V_0 = 0.04$, $\\tau = 0.25$ (so $\\tau^2 = 0.0625$).\n    - Segment $1$: $\\sigma_1 = 0.06$, $P = [0.12,0.14,0.16,0.18,0.20,0.22]$, $Q = [430,410,395,382,370,360]$.\n    - Segment $2$: $\\sigma_2 = 0.05$, $P = [0.10,0.105,0.11,0.115,0.12,0.125]$, $Q = [520,515,510,505,500,495]$.\n    - Segment $3$: $\\sigma_3 = 0.04$, $P = [0.09,0.11,0.13,0.15,0.17]$, $Q = [2300,2200,2100,2000,1900]$.\n- Test Case $2$ (near-constant price edge case in one segment, strong pooling):\n    - Hyperparameters: $m_0 = -0.25$, $V_0 = 0.01$, $\\tau = 0.08$ (so $\\tau^2 = 0.0064$).\n    - Segment $1$: $\\sigma_1 = 0.06$, $P = [0.11,0.13,0.15,0.17,0.19]$, $Q = [450,430,410,395,380]$.\n    - Segment $2$: $\\sigma_2 = 0.05$, $P = [0.1500,0.1505,0.1495,0.1500,0.1502]$, $Q = [400,400,399,401,400]$.\n    - Segment $3$: $\\sigma_3 = 0.04$, $P = [0.08,0.10,0.12,0.14]$, $Q = [2400,2300,2200,2100]$.\n- Test Case $3$ (weak pooling, high variation):\n    - Hyperparameters: $m_0 = -0.5$, $V_0 = 0.25$, $\\tau = 0.5$ (so $\\tau^2 = 0.25$).\n    - Segment $1$: $\\sigma_1 = 0.06$, $P = [0.09,0.13,0.17,0.21]$, $Q = [480,420,380,350]$.\n    - Segment $2$: $\\sigma_2 = 0.05$, $P = [0.10,0.12,0.15,0.18]$, $Q = [510,490,470,455]$.\n    - Segment $3$: $\\sigma_3 = 0.04$, $P = [0.07,0.11,0.15,0.19,0.23]$, $Q = [2600,2400,2200,2050,1950]$.\n\nAlgorithmic task:\n- For each segment $s$ in each test case, compute the OLS slope estimator $b_s$ from the regression of $y_{s,t}$ on $x_{s,t}$ with an intercept, and its sampling variance $V_s$ under known $\\sigma_s$.\n- Using the hierarchical structure and Bayes’ theorem, derive and compute the posterior mean and variance of the population mean elasticity $\\mu$, then derive and compute the posterior mean and variance of each segment elasticity $\\beta_s$, and finally compute the symmetric $95$-level credible interval for each $\\beta_s$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and equals\n$$\n[\\mu_{\\text{mean}},\\ \\mu_{\\text{var}},\\ [[\\beta_{1,\\text{mean}},\\ \\beta_{1,\\text{lower}},\\ \\beta_{1,\\text{upper}}],\\ldots,[\\beta_{S,\\text{mean}},\\ \\beta_{S,\\text{lower}},\\ \\beta_{S,\\text{upper}}]]].\n$$\nAll elasticity-related quantities must be reported as dimensionless decimals.",
            "solution": "The objective is to derive the posterior distributions for the segment-specific price elasticities of demand, $\\beta_s$, and the population mean elasticity, $\\mu$, within a Bayesian hierarchical model. The solution is entirely analytic, exploiting the conjugacy of Normal distributions.\n\nThe model is specified as follows:\n1.  **Demand Model (Likelihood Level 1)**: For each segment $s$ and time point $t$, the log-linear demand model is $y_{s,t} = \\alpha_s + \\beta_s x_{s,t} + \\varepsilon_{s,t}$, where $y_{s,t} = \\ln Q_{s,t}$ and $x_{s,t} = \\ln P_{s,t}$. The errors are independent and identically distributed as $\\varepsilon_{s,t} \\sim \\mathcal{N}(0, \\sigma_s^2)$, with known variance $\\sigma_s^2$. The parameter of interest is the elasticity, $\\beta_s$.\n\n2.  **Hierarchical Priors (Likelihood Level 2  Prior)**: The segment-specific elasticities $\\beta_s$ are drawn from a common population distribution, $\\beta_s | \\mu, \\tau^2 \\sim \\mathcal{N}(\\mu, \\tau^2)$. The hyperparameter $\\mu$ itself has a prior distribution, $\\mu \\sim \\mathcal{N}(m_0, V_0)$. The hyperparameters $m_0$, $V_0$, and $\\tau^2$ are known.\n\nThe derivation proceeds in three main steps:\n1.  Summarize the data from each segment using the Ordinary Least Squares (OLS) estimator.\n2.  Derive the posterior distribution of the population mean elasticity $\\mu$.\n3.  Derive the posterior distribution for each segment-specific elasticity $\\beta_s$.\n\n**Step 1: Data Summary via OLS**\n\nFor each segment $s$ with $T_s$ observations, the OLS estimator for the slope $\\beta_s$ is denoted as $b_s$. In a linear regression model with known error variance $\\sigma_s^2$, the sampling distribution of $b_s$ is Normal. The OLS estimator is calculated as:\n$$\nb_s = \\frac{\\sum_{t=1}^{T_s} (x_{s,t} - \\bar{x}_s)(y_{s,t} - \\bar{y}_s)}{\\sum_{t=1}^{T_s} (x_{s,t} - \\bar{x}_s)^2}\n$$\nwhere $\\bar{x}_s$ and $\\bar{y}_s$ are the sample means of $x_{s,t}$ and $y_{s,t}$, respectively.\n\nThe sampling variance of this estimator, which we denote as $V_s$, is:\n$$\nV_s = \\text{Var}(b_s) = \\frac{\\sigma_s^2}{\\sum_{t=1}^{T_s} (x_{s,t} - \\bar{x}_s)^2}\n$$\nThus, the OLS estimate $b_s$ provides a data-driven likelihood for the true parameter $\\beta_s$:\n$$\np(b_s | \\beta_s) = \\mathcal{N}(b_s; \\beta_s, V_s)\n$$\nThis means that, conditional on the true elasticity $\\beta_s$, the OLS estimate $b_s$ is normally distributed with mean $\\beta_s$ and variance $V_s$.\n\n**Step 2: Posterior Distribution of Population Mean $\\mu$**\n\nTo find the posterior distribution of $\\mu$, $p(\\mu | b_1, \\dots, b_S)$, we use Bayes' theorem:\n$$\np(\\mu | b_1, \\dots, b_S) \\propto p(b_1, \\dots, b_S | \\mu) \\, p(\\mu)\n$$\nThe prior is given as $p(\\mu) = \\mathcal{N}(\\mu; m_0, V_0)$. The likelihood term $p(b_1, \\dots, b_S | \\mu)$ is found by first noting that the $b_s$ are conditionally independent given $\\mu$, so $p(b_1, \\dots, b_S | \\mu) = \\prod_{s=1}^S p(b_s | \\mu)$. The marginal likelihood for a single segment's estimate $b_s$ given $\\mu$ is found by integrating out $\\beta_s$:\n$$\np(b_s | \\mu) = \\int p(b_s | \\beta_s) p(\\beta_s | \\mu) d\\beta_s\n$$\nWe are integrating over the product of two Normal distributions: $p(b_s | \\beta_s) = \\mathcal{N}(b_s; \\beta_s, V_s)$ and $p(\\beta_s | \\mu) = \\mathcal{N}(\\beta_s; \\mu, \\tau^2)$. This is a convolution of two Normal distributions, which results in another Normal distribution whose mean is the sum of the means and whose variance is the sum of the variances:\n$$\np(b_s | \\mu) = \\mathcal{N}(b_s; \\mu, V_s + \\tau^2)\n$$\nNow, the posterior for $\\mu$ is proportional to the product of Normal densities:\n$$\np(\\mu | b_1, \\dots, b_S) \\propto \\mathcal{N}(\\mu; m_0, V_0) \\prod_{s=1}^S \\mathcal{N}(b_s; \\mu, V_s + \\tau^2)\n$$\nThe product of Normal densities is also Normal (in $\\mu$). The posterior distribution $p(\\mu | b_1, \\dots, b_S)$ is therefore $\\mathcal{N}(\\mu_{\\text{post}}, V_{\\text{post}})$, where the posterior precision is the sum of the prior precision and the data precisions, and the posterior mean is a precision-weighted average.\n\nThe posterior variance of $\\mu$, denoted $\\mu_{\\text{var}}$, is:\n$$\n\\mu_{\\text{var}} = V_{\\text{post}} = \\left( \\frac{1}{V_0} + \\sum_{s=1}^S \\frac{1}{V_s + \\tau^2} \\right)^{-1}\n$$\nThe posterior mean of $\\mu$, denoted $\\mu_{\\text{mean}}$, is:\n$$\n\\mu_{\\text{mean}} = \\mu_{\\text{post}} = \\mu_{\\text{var}} \\left( \\frac{m_0}{V_0} + \\sum_{s=1}^S \\frac{b_s}{V_s + \\tau^2} \\right)\n$$\n\n**Step 3: Posterior Distribution of Segment Elasticity $\\beta_s$**\n\nThe posterior for a specific elasticity $\\beta_s$ is conditioned on all available data, $p(\\beta_s | b_1, \\dots, b_S)$. We can find this by first finding the posterior conditional on $\\mu$, $p(\\beta_s | \\mu, b_1, \\dots, b_S)$, and then integrating $\\mu$ out using its posterior distribution $p(\\mu | b_1, \\dots, b_S)$. Due to conditional independence in the model structure, $p(\\beta_s | \\mu, b_1, \\dots, b_S) = p(\\beta_s | \\mu, b_s)$.\n\nThe posterior for $\\beta_s$ given $\\mu$ and $b_s$ comes from a simple Bayesian update:\n$$\np(\\beta_s | \\mu, b_s) \\propto p(b_s | \\beta_s) p(\\beta_s | \\mu) = \\mathcal{N}(b_s; \\beta_s, V_s) \\mathcal{N}(\\beta_s; \\mu, \\tau^2)\n$$\nThis is a standard Normal-Normal conjugate model. The posterior $p(\\beta_s | \\mu, b_s)$ is Normal with mean and variance:\n$$\n\\text{E}[\\beta_s | \\mu, b_s] = \\frac{\\frac{1}{V_s}b_s + \\frac{1}{\\tau^2}\\mu}{\\frac{1}{V_s}+\\frac{1}{\\tau^2}} = \\frac{\\tau^2 b_s + V_s \\mu}{V_s + \\tau^2}\n$$\n$$\n\\text{Var}(\\beta_s | \\mu, b_s) = \\left( \\frac{1}{V_s} + \\frac{1}{\\tau^2} \\right)^{-1} = \\frac{V_s \\tau^2}{V_s + \\tau^2}\n$$\nThe marginal posterior for $\\beta_s$ is also Normal, as it is a mixture of Normals. Its mean and variance can be found using the laws of total expectation and total variance:\n$$\n\\beta_{s,\\text{mean}} = \\text{E}[\\beta_s | b_1, \\dots, b_S] = \\text{E}_{\\mu | \\mathbf{b}} \\left[ \\text{E}[\\beta_s | \\mu, b_s] \\right]\n$$\n$$\n\\beta_{s,\\text{mean}} = \\text{E}_{\\mu | \\mathbf{b}} \\left[ \\frac{\\tau^2 b_s + V_s \\mu}{V_s + \\tau^2} \\right] = \\frac{\\tau^2 b_s + V_s \\text{E}[\\mu | \\mathbf{b}]}{V_s + \\tau^2} = \\frac{\\tau^2 b_s + V_s \\mu_{\\text{mean}}}{V_s + \\tau^2}\n$$\nThis is the \"shrinkage\" estimator, where the segment-specific estimate $b_s$ is pulled towards the posterior population mean $\\mu_{\\text{mean}}$.\n\n$$\n\\beta_{s,\\text{var}} = \\text{Var}(\\beta_s | b_1, \\dots, b_S) = \\text{E}_{\\mu | \\mathbf{b}}\\left[ \\text{Var}(\\beta_s | \\mu, b_s) \\right] + \\text{Var}_{\\mu | \\mathbf{b}}\\left[ \\text{E}[\\beta_s | \\mu, b_s] \\right]\n$$\nThe first term is $\\text{E}_{\\mu | \\mathbf{b}} \\left[ \\frac{V_s \\tau^2}{V_s + \\tau^2} \\right] = \\frac{V_s \\tau^2}{V_s + \\tau^2}$.\nThe second term is $\\text{Var}_{\\mu | \\mathbf{b}} \\left[ \\frac{\\tau^2 b_s + V_s \\mu}{V_s + \\tau^2} \\right] = \\left(\\frac{V_s}{V_s + \\tau^2}\\right)^2 \\text{Var}(\\mu | \\mathbf{b}) = \\left(\\frac{V_s}{V_s + \\tau^2}\\right)^2 \\mu_{\\text{var}}$.\nSo, the posterior variance for $\\beta_s$ is:\n$$\n\\beta_{s,\\text{var}} = \\frac{V_s \\tau^2}{V_s + \\tau^2} + \\left( \\frac{V_s}{V_s + \\tau^2} \\right)^2 \\mu_{\\text{var}}\n$$\n\nFinally, the symmetric $95\\%$ Bayesian credible interval for $\\beta_s$ is constructed using the posterior mean and standard deviation ($\\beta_{s,\\text{sd}} = \\sqrt{\\beta_{s,\\text{var}}}$), leveraging the Normal posterior:\n$$\n[\\beta_{s,\\text{mean}} - 1.96 \\cdot \\beta_{s,\\text{sd}}, \\quad \\beta_{s,\\text{mean}} + 1.96 \\cdot \\beta_{s,\\text{sd}}]\n$$\nThese formulas provide a complete analytical solution to the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian hierarchical modeling problem for price elasticity of demand\n    across the specified test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"hyperparameters\": {\"m0\": -0.3, \"V0\": 0.04, \"tau\": 0.25},\n            \"segments\": [\n                {\"sigma\": 0.06, \"P\": [0.12, 0.14, 0.16, 0.18, 0.20, 0.22], \"Q\": [430, 410, 395, 382, 370, 360]},\n                {\"sigma\": 0.05, \"P\": [0.10, 0.105, 0.11, 0.115, 0.12, 0.125], \"Q\": [520, 515, 510, 505, 500, 495]},\n                {\"sigma\": 0.04, \"P\": [0.09, 0.11, 0.13, 0.15, 0.17], \"Q\": [2300, 2200, 2100, 2000, 1900]}\n            ]\n        },\n        # Test Case 2\n        {\n            \"hyperparameters\": {\"m0\": -0.25, \"V0\": 0.01, \"tau\": 0.08},\n            \"segments\": [\n                {\"sigma\": 0.06, \"P\": [0.11, 0.13, 0.15, 0.17, 0.19], \"Q\": [450, 430, 410, 395, 380]},\n                {\"sigma\": 0.05, \"P\": [0.1500, 0.1505, 0.1495, 0.1500, 0.1502], \"Q\": [400, 400, 399, 401, 400]},\n                {\"sigma\": 0.04, \"P\": [0.08, 0.10, 0.12, 0.14], \"Q\": [2400, 2300, 2200, 2100]}\n            ]\n        },\n        # Test Case 3\n        {\n            \"hyperparameters\": {\"m0\": -0.5, \"V0\": 0.25, \"tau\": 0.5},\n            \"segments\": [\n                {\"sigma\": 0.06, \"P\": [0.09, 0.13, 0.17, 0.21], \"Q\": [480, 420, 380, 350]},\n                {\"sigma\": 0.05, \"P\": [0.10, 0.12, 0.15, 0.18], \"Q\": [510, 490, 470, 455]},\n                {\"sigma\": 0.04, \"P\": [0.07, 0.11, 0.15, 0.19, 0.23], \"Q\": [2600, 2400, 2200, 2050, 1950]}\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack hyperparameters\n        m0 = case[\"hyperparameters\"][\"m0\"]\n        V0 = case[\"hyperparameters\"][\"V0\"]\n        tau_sq = case[\"hyperparameters\"][\"tau\"] ** 2\n\n        b_s_list = []\n        V_s_list = []\n\n        # Step 1: Compute OLS estimates and variances for each segment\n        for segment in case[\"segments\"]:\n            P = np.array(segment[\"P\"])\n            Q = np.array(segment[\"Q\"])\n            sigma_s = segment[\"sigma\"]\n\n            # Log-transform data\n            x = np.log(P)\n            y = np.log(Q)\n            \n            # OLS calculations\n            x_bar = np.mean(x)\n            y_bar = np.mean(y)\n            \n            ssx = np.sum((x - x_bar)**2)\n            spxy = np.sum((x - x_bar) * (y - y_bar))\n\n            b_s = spxy / ssx\n            V_s = sigma_s**2 / ssx\n\n            b_s_list.append(b_s)\n            V_s_list.append(V_s)\n            \n        # Step 2: Compute posterior for population mean mu\n        mu_var_precision = 1 / V0\n        mu_mean_numerator = m0 / V0\n        \n        for i in range(len(b_s_list)):\n            b_s = b_s_list[i]\n            V_s = V_s_list[i]\n            \n            common_denom = V_s + tau_sq\n            mu_var_precision += 1 / common_denom\n            mu_mean_numerator += b_s / common_denom\n\n        mu_var = 1 / mu_var_precision\n        mu_mean = mu_var * mu_mean_numerator\n\n        # Step 3: Compute posterior for each segment elasticity beta_s\n        segment_results = []\n        for i in range(len(b_s_list)):\n            b_s = b_s_list[i]\n            V_s = V_s_list[i]\n\n            # Posterior mean of beta_s\n            common_denom = V_s + tau_sq\n            beta_s_mean = (tau_sq * b_s + V_s * mu_mean) / common_denom\n\n            # Posterior variance of beta_s\n            term1 = (V_s * tau_sq) / common_denom\n            term2 = (V_s / common_denom)**2 * mu_var\n            beta_s_var = term1 + term2\n            beta_s_std = np.sqrt(beta_s_var)\n            \n            # 95% credible interval\n            z_score = 1.96\n            lower_bound = beta_s_mean - z_score * beta_s_std\n            upper_bound = beta_s_mean + z_score * beta_s_std\n            \n            segment_results.append([beta_s_mean, lower_bound, upper_bound])\n\n        case_result = [mu_mean, mu_var, segment_results]\n        results.append(case_result)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}