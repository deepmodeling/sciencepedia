{
    "hands_on_practices": [
        {
            "introduction": "To effectively integrate batteries into long-term energy planning, models require simplified yet powerful representations of degradation. This first practice introduces a foundational 'two-factor' model that accounts for the primary drivers of capacity fade: the passage of time (calendar aging) and active use (cycling aging). By applying this linear additive model to a hypothetical scenario, you will gain hands-on experience in calculating the remaining capacity of a battery, a key metric for assessing its long-term viability .",
            "id": "4071265",
            "problem": "A utility-scale Lithium-ion battery energy storage system of nominal capacity $Q_{0}$ (in $\\mathrm{kWh}$) is represented in a long-run planning model using reduced-form degradation components calibrated from electrochemical and empirical evidence. Two well-tested facts underpin the representation:\n- For diffusion-limited growth of the Solid Electrolyte Interphase (SEI), the loss of cyclable lithium, and hence capacity fade, scales as the square root of calendar time $t$; this is a consequence of Fickian transport controlling the rate of SEI growth, yielding a thickness $\\delta(t) \\propto \\sqrt{t}$ and capacity loss $\\Delta Q_{\\mathrm{cal}}(t) \\propto \\delta(t)$.\n- For cycling at moderate depth-of-discharge and fixed operating conditions, the incremental loss of cyclable lithium per Equivalent Full Cycle (EFC) is approximately constant, producing an accumulated capacity loss that is linear in the number of EFCs, $N$, i.e., $\\Delta Q_{\\mathrm{cyc}}(N) \\propto N$, where an EFC aggregates partial cycles such that the total energy throughput equals one nominal full charge/discharge.\n\nAssume that, over the time horizon of interest, the calendar and cycling mechanisms are independent and their losses add linearly, with proportionality constants $\\alpha$ (for calendar time) and $\\beta$ (for cycling), both calibrated at the pack level. Starting from these base facts, derive an analytic expression for the residual capacity $Q(t,N)$ in terms of $Q_{0}$, $\\alpha$, $\\beta$, $t$, and $N$.\n\nThen, using $Q_{0}=100\\ \\mathrm{kWh}$, $\\alpha=0.2\\ \\mathrm{kWh}/\\sqrt{\\mathrm{day}}$, $\\beta=0.05\\ \\mathrm{kWh}$ per EFC, $t=90\\ \\mathrm{day}$, and $N=300$ EFC, compute the numerical value of $Q(t,N)$. Express the final capacity in $\\mathrm{kWh}$ and round your answer to four significant figures.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   Nominal capacity: $Q_{0}$ (in $\\mathrm{kWh}$)\n-   Calendar time: $t$\n-   Number of Equivalent Full Cycles (EFCs): $N$\n-   Calendar capacity loss model: $\\Delta Q_{\\mathrm{cal}}(t) \\propto \\sqrt{t}$\n-   Cycling capacity loss model: $\\Delta Q_{\\mathrm{cyc}}(N) \\propto N$\n-   Assumption: Calendar and cycling degradation mechanisms are independent and their losses add linearly.\n-   Proportionality constant for calendar loss: $\\alpha$\n-   Proportionality constant for cycling loss: $\\beta$\n-   Required derivation: An analytic expression for the residual capacity $Q(t,N)$ in terms of $Q_{0}$, $\\alpha$, $\\beta$, $t$, and $N$.\n-   Numerical values for calculation:\n    -   $Q_{0} = 100\\ \\mathrm{kWh}$\n    -   $\\alpha = 0.2\\ \\mathrm{kWh}/\\sqrt{\\mathrm{day}}$\n    -   $\\beta = 0.05\\ \\mathrm{kWh}$ per EFC\n    -   $t = 90\\ \\mathrm{day}$\n    -   $N = 300$ EFC\n-   Final instruction: Compute the numerical value of $Q(t,N)$ and round to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n-   **Scientifically Grounded**: The problem is grounded in established, albeit simplified, models of battery degradation. The square-root-of-time dependence for calendar aging (linked to diffusion-limited SEI layer growth) and the linear dependence on the number of equivalent full cycles are standard and widely used approximations in the field of battery modeling and energy systems analysis. The premises are scientifically sound for a reduced-form model.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary information, including initial conditions ($Q_0$), model forms ($\\propto \\sqrt{t}$, $\\propto N$), constants ($\\alpha, \\beta$), and a clear set of assumptions (independence and linear additivity of loss mechanisms), to derive a unique analytical expression and compute a corresponding numerical result.\n-   **Objective**: The problem is stated in objective, technical language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ill-posed structure.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\nThe residual capacity of the battery, $Q(t,N)$, is defined as the initial nominal capacity, $Q_{0}$, minus the total capacity loss, $\\Delta Q_{\\mathrm{total}}(t,N)$.\n$$\nQ(t,N) = Q_{0} - \\Delta Q_{\\mathrm{total}}(t,N)\n$$\nThe problem states that the total loss is the linear sum of the independent calendar and cycling loss components.\n$$\n\\Delta Q_{\\mathrm{total}}(t,N) = \\Delta Q_{\\mathrm{cal}}(t) + \\Delta Q_{\\mathrm{cyc}}(N)\n$$\nThe calendar-induced capacity loss, $\\Delta Q_{\\mathrm{cal}}(t)$, is given as proportional to the square root of time, with a proportionality constant $\\alpha$.\n$$\n\\Delta Q_{\\mathrm{cal}}(t) = \\alpha \\sqrt{t}\n$$\nThe cycling-induced capacity loss, $\\Delta Q_{\\mathrm{cyc}}(N)$, is given as proportional to the number of equivalent full cycles, with a proportionality constant $\\beta$.\n$$\n\\Delta Q_{\\mathrm{cyc}}(N) = \\beta N\n$$\nSubstituting these expressions into the equation for total loss gives:\n$$\n\\Delta Q_{\\mathrm{total}}(t,N) = \\alpha \\sqrt{t} + \\beta N\n$$\nTherefore, the analytic expression for the residual capacity $Q(t,N)$ is:\n$$\nQ(t,N) = Q_{0} - (\\alpha \\sqrt{t} + \\beta N)\n$$\nThis is the first part of the required solution.\n\nFor the second part, we substitute the provided numerical values into this expression. The given values are:\n$Q_{0} = 100\\ \\mathrm{kWh}$\n$\\alpha = 0.2\\ \\mathrm{kWh}/\\sqrt{\\mathrm{day}}$\n$\\beta = 0.05\\ \\mathrm{kWh}$ per EFC\n$t = 90\\ \\mathrm{day}$\n$N = 300$ EFC\n\nFirst, we calculate the calendar loss component:\n$$\n\\Delta Q_{\\mathrm{cal}}(90) = \\alpha \\sqrt{t} = 0.2 \\cdot \\sqrt{90}\n$$\nSince $\\sqrt{90} = \\sqrt{9 \\cdot 10} = 3\\sqrt{10}$, this becomes:\n$$\n\\Delta Q_{\\mathrm{cal}}(90) = 0.2 \\cdot 3\\sqrt{10} = 0.6\\sqrt{10}\\ \\mathrm{kWh}\n$$\nNext, we calculate the cycling loss component:\n$$\n\\Delta Q_{\\mathrm{cyc}}(300) = \\beta N = 0.05 \\cdot 300 = 15\\ \\mathrm{kWh}\n$$\nThe total capacity loss is the sum of these two components:\n$$\n\\Delta Q_{\\mathrm{total}}(90, 300) = (0.6\\sqrt{10} + 15)\\ \\mathrm{kWh}\n$$\nThe residual capacity is then:\n$$\nQ(90, 300) = Q_{0} - \\Delta Q_{\\mathrm{total}}(90, 300) = 100 - (0.6\\sqrt{10} + 15)\n$$\n$$\nQ(90, 300) = 85 - 0.6\\sqrt{10}\\ \\mathrm{kWh}\n$$\nTo find the numerical value, we approximate $\\sqrt{10} \\approx 3.16227766$:\n$$\nQ(90, 300) \\approx 85 - 0.6 \\cdot 3.16227766\n$$\n$$\nQ(90, 300) \\approx 85 - 1.897366596\n$$\n$$\nQ(90, 300) \\approx 83.102633404\\ \\mathrm{kWh}\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $8$, $3$, $1$, and $0$. The fifth figure is $2$, so we round down.\n$$\nQ(90, 300) \\approx 83.10\\ \\mathrm{kWh}\n$$",
            "answer": "$$\\boxed{83.10}$$"
        },
        {
            "introduction": "While simple models distinguish between calendar and cycling age, a deeper question arises: is all cycling equally damaging? This exercise explores that nuance by comparing two operating schedules with identical energy throughput but different power profiles . By calculating the wear using a convex penalty function, you will directly observe why high-power, volatile operation is more detrimental than smoother cycling, providing a clear justification for the convex formulations used in modern optimization models to encourage battery-friendly dispatch.",
            "id": "4071253",
            "problem": "A grid-connected lithium-ion battery is modeled in a time-discrete planning framework with time step $\\Delta t$, energy capacity $E$, and perfect round-trip efficiency (no losses). The state of charge $s_t$ evolves according to the core definition\n$$\ns_{t+1} \\;=\\; s_t \\;+\\; \\frac{\\Delta t}{E}\\,P_t,\n$$\nwhere $P_t$ is the real power at time $t$, taken positive for charging (energy into the battery) and negative for discharging (energy out of the battery). Define the incremental normalized cycle-depth at time $t$ as\n$$\nC_r(t) \\;=\\; |\\Delta s_t| \\;=\\; \\left|\\frac{\\Delta t}{E}\\,P_t\\right|.\n$$\nA widely used convex surrogate for cycle-induced wear in planning models aggregates over time via\n$$\nW \\;=\\; \\sum_{t} g\\!\\big(C_r(t)\\big),\n$$\nwith a quadratic cycle-stress function $g(C_r) = C_r^{2}$.\n\nConsider two $T=8$-period schedules over a horizon with $\\Delta t = 0.5\\,\\text{h}$ and battery energy capacity $E = 100\\,\\text{kWh}$. The initial state of charge is $s_0 = 0.5$, and the power limits are such that both schedules are feasible. The two candidate power schedules are:\n\n- Schedule A (low peak magnitude, more frequent cycling):\n  $P = \\big[+50,\\,-50,\\,+50,\\,-50,\\,+50,\\,-50,\\,+50,\\,-50\\big]\\,\\text{kW}$.\n\n- Schedule B (higher peak magnitude, sparser cycling):\n  $P = \\big[+100,\\,-100,\\,0,\\,0,\\,+100,\\,-100,\\,0,\\,0\\big]\\,\\text{kW}$.\n\nBoth schedules have the same total absolute energy throughput $\\sum_{t=1}^{8} |P_t|\\,\\Delta t$ but different peak $|P_t|$.\n\nUsing only the definitions above, compute the difference in total wear $\\Delta W \\equiv W_B - W_A$, where $W_A$ and $W_B$ are the total wears for Schedules A and B, respectively, under $g(C_r)=C_r^{2}$. Express the final wear difference as a decimal (dimensionless). Provide an exact value (no rounding necessary).",
            "solution": "The problem requires the computation of the difference in total wear, $\\Delta W \\equiv W_B - W_A$, between two power schedules, A and B. The total wear, $W$, is defined as the sum over time of a cycle-stress function $g(C_r)$, which depends on the incremental normalized cycle-depth, $C_r(t)$.\n\nThe primary definitions given are:\n1.  Incremental normalized cycle-depth: $C_r(t) = |\\Delta s_t| = \\left|\\frac{\\Delta t}{E}\\,P_t\\right|$\n2.  Total wear: $W = \\sum_{t} g(C_r(t))$\n3.  Cycle-stress function: $g(C_r) = C_r^{2}$\n\nThe constants provided are the time step $\\Delta t = 0.5\\,\\text{h}$ and the battery energy capacity $E = 100\\,\\text{kWh}$. We can pre-calculate the constant factor used to determine the cycle-depth:\n$$\n\\frac{\\Delta t}{E} = \\frac{0.5\\,\\text{h}}{100\\,\\text{kWh}} = 0.005\\,\\text{kW}^{-1}\n$$\nThis factor, when multiplied by the power $P_t$ in $\\text{kW}$, yields the dimensionless quantity $C_r(t)$.\n\nFirst, we calculate the total wear for Schedule A, denoted $W_A$.\nThe power schedule for A is given as a vector over $T=8$ periods:\n$$\nP_A = \\big[+50,\\,-50,\\,+50,\\,-50,\\,+50,\\,-50,\\,+50,\\,-50\\big]\\,\\text{kW}\n$$\nFor every time step $t$ from $1$ to $8$, the magnitude of the power is constant: $|P_t| = 50\\,\\text{kW}$.\nWe can calculate the incremental normalized cycle-depth for any time step $t$ for Schedule A:\n$$\nC_{r,A}(t) = \\left|\\frac{\\Delta t}{E} P_t\\right| = \\left|0.005\\,\\text{kW}^{-1} \\times (\\pm 50\\,\\text{kW})\\right| = |\\pm 0.25| = 0.25\n$$\nSince $C_{r,A}(t)$ is constant for all $t \\in \\{1, \\dots, 8\\}$, the cycle-stress $g(C_{r,A}(t))$ is also constant:\n$$\ng(C_{r,A}(t)) = \\left(C_{r,A}(t)\\right)^2 = (0.25)^2 = 0.0625\n$$\nThe total wear for Schedule A is the sum of these values over the $8$ time steps:\n$$\nW_A = \\sum_{t=1}^{8} g(C_{r,A}(t)) = 8 \\times 0.0625 = 0.5\n$$\n\nNext, we calculate the total wear for Schedule B, denoted $W_B$.\nThe power schedule for B is:\n$$\nP_B = \\big[+100,\\,-100,\\,0,\\,0,\\,+100,\\,-100,\\,0,\\,0\\big]\\,\\text{kW}\n$$\nWe must calculate the cycle-depth for each time step.\nFor $t \\in \\{1, 2, 5, 6\\}$, the power magnitude is $|P_t| = 100\\,\\text{kW}$. The cycle-depth is:\n$$\nC_{r,B}(t) = \\left|\\frac{\\Delta t}{E} P_t\\right| = \\left|0.005\\,\\text{kW}^{-1} \\times (\\pm 100\\,\\text{kW})\\right| = |\\pm 0.5| = 0.5\n$$\nFor these time steps, the cycle-stress is:\n$$\ng(C_{r,B}(t)) = (0.5)^2 = 0.25\n$$\nFor $t \\in \\{3, 4, 7, 8\\}$, the power is $P_t = 0\\,\\text{kW}$. The cycle-depth is:\n$$\nC_{r,B}(t) = \\left|\\frac{\\Delta t}{E} \\times 0\\right| = 0\n$$\nFor these time steps, the cycle-stress is:\n$$\ng(C_{r,B}(t)) = (0)^2 = 0\n$$\nThe total wear for Schedule B is the sum of the cycle-stresses over the $8$ periods. There are $4$ non-zero terms and $4$ zero terms:\n$$\nW_B = \\sum_{t=1}^{8} g(C_{r,B}(t)) = (4 \\times 0.25) + (4 \\times 0) = 1.0\n$$\n\nFinally, we compute the difference in total wear, $\\Delta W = W_B - W_A$.\n$$\n\\Delta W = 1.0 - 0.5 = 0.5\n$$\nThe result demonstrates that even with the same total energy throughput, the schedule with higher peak power (and thus deeper individual cycles) induces greater wear according to the convex quadratic wear model. This justifies the use of such functions in planning models to incentivize smoother battery operation.",
            "answer": "$$\\boxed{0.5}$$"
        },
        {
            "introduction": "The most accurate degradation models, such as those based on rainflow counting, are often too complex and non-linear to be embedded directly into large-scale planning optimizations. This advanced challenge tackles this problem head-on by asking you to build a bridge between detailed physics and tractable mathematics . You will implement a cutting-plane algorithm to construct a piecewise-linear convex surrogate for a sophisticated rainflow-based cost function, a powerful technique at the frontier of energy systems modeling.",
            "id": "4071249",
            "problem": "You are asked to design and implement a cutting-plane algorithm that constructs a piecewise-linear convex surrogate for rainflow-based battery degradation cost over a discrete State of Charge (SoC) trajectory domain. The surrogate must be iteratively refined using offline rainflow evaluations to reduce approximation gaps on a finite sample pool, thereby emulating planning-model compatible representations of battery degradation.\n\nStart from the following context-appropriate fundamental base:\n\n- A battery State of Charge trajectory over a discretized horizon is a vector $s \\in \\mathcal{S} = [0,1]^T$, where $T$ is the number of time steps and each component $s_t \\in [0,1]$ denotes SoC at time step $t$.\n- The classical rainflow counting algorithm maps any sequence $s \\in \\mathcal{S}$ to a multiset of cycles $\\{(d_i, w_i)\\}_i$, where each $d_i \\in [0,1]$ is a cycle depth (amplitude between adjacent extrema) and $w_i \\in \\{1, \\tfrac{1}{2}\\}$ is the cycle weight (full or half cycle). The algorithm is memoryless with respect to cycle concatenation and is widely used for cycle extraction.\n- The total degradation cost under a cycle-depth penalty is\n$$\nf(s) \\;=\\; \\sum_{i} \\kappa\\, \\phi(d_i)\\, w_i,\n$$\nwhere $\\kappa > 0$ is a scaling constant and $\\phi(d)$ is convex and non-decreasing in $d$. For this problem, take $\\phi(d) = d^\\alpha$ with $\\alpha \\ge 2$, which is convex on $[0,1]$.\n- A well-tested observation from the literature is that if $\\phi(d)$ is convex and non-decreasing, then the rainflow-aggregated degradation $f(s)$ is a convex functional of the SoC trajectory $s$.\n\nYour tasks:\n\n1. Implement a function that computes $f(s)$ using the classical rainflow counting method on $s \\in [0,1]^T$ with the penalty $\\phi(d) = d^\\alpha$. The cycle extraction must be scientifically sound: identify reversals (local extrema), apply the rainflow criteria to count closed cycles with weight $1$ and residual half cycles with weight $\\tfrac{1}{2}$, and compute the sum of $\\kappa d^\\alpha$ times weights.\n2. Implement an iterative cutting-plane algorithm that builds a piecewise-linear convex surrogate\n$$\ng(s) \\;=\\; \\max_{k=1,\\dots,K} \\left\\{ a_k + b_k^\\top s \\right\\}\n$$\nas an underestimator of $f(s)$ over a finite sample pool $\\mathcal{X} \\subset \\mathcal{S}$. Each cut is constructed at an anchor $x_k \\in \\mathcal{X}$ using an approximate subgradient $b_k$ obtained via finite differences:\n   - For each coordinate $j \\in \\{1,\\dots,T\\}$, approximate $\\frac{\\partial f}{\\partial s_j}(x_k)$ by forward or central differences with a small perturbation $\\varepsilon > 0$, respecting the bounds $[0,1]$ by clipping. Use central differences $\\frac{f(x_k + \\varepsilon e_j) - f(x_k - \\varepsilon e_j)}{2\\varepsilon}$ when both perturbed points lie in $[0,1]^T$, and one-sided differences otherwise.\n   - To ensure the cut underestimates on $\\mathcal{X}$, introduce a scalar $\\gamma_k \\in (0,1]$ that scales the approximate subgradient based on offline evaluations of $f$ at all $s \\in \\mathcal{X}$. Define the cut as\n     $$\n     h_k(s) \\;=\\; f(x_k) + \\gamma_k\\, b_k^\\top (s - x_k),\n     $$\n     and choose $\\gamma_k$ to satisfy $h_k(s) \\le f(s)$ for all $s \\in \\mathcal{X}$ whenever possible. One way to achieve this is\n     $$\n     \\gamma_k \\;=\\; \\min\\left\\{\\, 1, \\; \\min_{s \\in \\mathcal{X}:\\; b_k^\\top(s-x_k) > 0} \\frac{f(s)-f(x_k)}{b_k^\\top(s-x_k)} \\,\\right\\},\n     $$\n     with the convention that the inner minimum over an empty set equals $1$. If the outer minimum is negative due to numerical artifacts, replace it by $0$, which yields the constant cut $h_k(s) = f(x_k)$.\n   - Set $a_k = f(x_k) - (\\gamma_k b_k)^\\top x_k$ and $b_k \\leftarrow \\gamma_k b_k$, so the surrogate is $g(s) = \\max_k \\{a_k + b_k^\\top s\\}$.\n3. Construct an initial set of anchors (for example, constant and structured trajectories) and iteratively refine:\n   - Evaluate $f(s)$ for all $s \\in \\mathcal{X}$.\n   - Build initial cuts at chosen anchors.\n   - At each iteration, compute the approximation gaps $\\Delta(s) = f(s) - g(s)$ for all $s \\in \\mathcal{X}$, select the $s^\\star$ that maximizes $\\Delta(s)$, add a cut at $x = s^\\star$ using the procedure above, and repeat until either $\\max_{s \\in \\mathcal{X}} \\Delta(s) \\le \\tau$ or a maximum number $K_{\\max}$ of cuts is reached. Here $\\tau > 0$ is a tolerance.\n4. Return the final maximum approximation gap $\\max_{s \\in \\mathcal{X}} \\Delta(s)$ for each test case as a float without units. All outputs are dimensionless because the objective uses $\\kappa$ and exponents on normalized depths; do not include any unit labels.\n\nTest suite and required program output:\n\nImplement the above for the following parameter sets, using a fixed random seed for reproducibility in generating $\\mathcal{X}$ whenever randomness is used. For each case, construct $\\mathcal{X}$ by including some fixed structured trajectories (for example, the zero sequence, a ramp, a triangular wave, and a constant sequence) and then adding random trajectories normalized to $[0,1]^T$.\n\n- Case A (happy path):\n  - $T = 24$, $N = 30$, $\\kappa = 1.0$, $\\alpha = 2.5$, $\\varepsilon = 10^{-3}$, $\\tau = 10^{-4}$, $K_{\\max} = 30$, random seed fixed.\n- Case B (boundary emphasis with many nearly constant segments):\n  - $T = 12$, $N = 12$, $\\kappa = 1.0$, $\\alpha = 3.0$, $\\varepsilon = 10^{-3}$, $\\tau = 10^{-3}$, $K_{\\max} = 20$, random seed fixed.\n- Case C (strong convexity of cycle penalty):\n  - $T = 16$, $N = 25$, $\\kappa = 1.0$, $\\alpha = 4.0$, $\\varepsilon = 10^{-3}$, $\\tau = 10^{-4}$, $K_{\\max} = 30$, random seed fixed.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the entries ordered as $[\\text{gap}_A,\\text{gap}_B,\\text{gap}_C]$, where each $\\text{gap}$ is the final maximum approximation gap for the corresponding case expressed as a float (dimensionless). No other text should be printed.",
            "solution": "The objective is to design and implement a cutting-plane algorithm to construct a piecewise-linear, convex surrogate function $g(s)$ that approximates a complex, non-linear battery degradation cost function, $f(s)$. The cost function $f(s)$ is based on the rainflow-counting analysis of a battery's State of Charge (SoC) trajectory, $s \\in [0,1]^T$. The surrogate $g(s)$ must be an underestimator of $f(s)$ and is intended for use within computationally tractable planning and optimization models. The algorithm iteratively refines the surrogate by adding cutting planes at points of maximum approximation error, evaluated over a finite pool of sample trajectories $\\mathcal{X}$.\n\nThe methodological approach is structured into three main components: the definition and computation of the degradation cost function $f(s)$; the formulation of the cutting-plane algorithm; and the specific procedure for generating each cut.\n\n### 1. The Degradation Cost Functional $f(s)$\n\nThe degradation cost $f(s)$ for an SoC trajectory $s = (s_1, \\dots, s_T)$ is defined as the aggregated cost over all stress cycles identified by the rainflow-counting algorithm. The formula is given by:\n$$\nf(s) = \\sum_{i} \\kappa\\, \\phi(d_i)\\, w_i\n$$\nwhere $\\kappa > 0$ is a scaling constant, and the summation is over all cycles identified in the trajectory $s$. Each cycle $i$ is characterized by a depth (amplitude) $d_i \\in [0,1]$ and a weight $w_i \\in \\{1, \\frac{1}{2}\\}$, corresponding to a full or half cycle, respectively. The function $\\phi(d)$ quantifies the stress induced by a cycle of depth $d$. For this problem, we use the convex, non-decreasing function $\\phi(d) = d^{\\alpha}$ with $\\alpha \\ge 2$.\n\nThe computation of $f(s)$ is performed via the following steps:\n1.  **Reversal Identification**: The input trajectory $s$ is first processed to identify its sequence of local extrema, known as reversals. This involves filtering out consecutive identical values to form a compact sequence, and then extracting all points that are local maxima or minima, including the start and end points of the trajectory.\n2.  **Rainflow Cycle Counting**: A stack-based algorithm, compliant with standards such as ASTM E1049, is applied to the sequence of reversals. The algorithm processes reversals one by one. For a trio of consecutive reversals $(Y, X, P)$, if the range of the current segment $|P-X|$ is greater than or equal to the range of the previous segment $|X-Y|$, a full cycle of depth $|X-Y|$ is counted, and both $X$ and $Y$ are removed from consideration. This process is repeated until no more full cycles can be closed.\n3.  **Residual Half-Cycles**: Any reversals remaining on the stack after processing the entire trajectory form residual half-cycles. For each adjacent pair of remaining reversals $(Y, X)$, a half-cycle of depth $|X-Y|$ is counted.\n4.  **Cost Aggregation**: The total degradation cost $f(s)$ is computed by summing the penalty $\\kappa d_i^\\alpha$ for each counted cycle $i$, weighted by its corresponding weight $w_i$.\n\n### 2. The Cutting-Plane Algorithm\n\nThe algorithm constructs the surrogate $g(s)$ as the pointwise maximum of a set of affine functions (cuts):\n$$\ng(s) = \\max_{k=1,\\dots,K} \\{ a_k + b_k^\\top s \\}\n$$\nThis form ensures that $g(s)$ is a convex and piecewise-linear function. The algorithm iteratively adds cuts to improve the approximation quality over the sample pool $\\mathcal{X}$.\n\nThe procedure is as follows:\n1.  **Initialization**:\n    - A sample pool $\\mathcal{X}$ of $N$ SoC trajectories is generated. This pool includes a set of deterministic, structured trajectories (e.g., constant, ramp, triangular wave) to cover fundamental behaviors, and a set of random trajectories to ensure diverse coverage of the space $\\mathcal{S}=[0,1]^T$.\n    - The exact degradation cost $f(s)$ is pre-computed and cached for every sample $s \\in \\mathcal{X}$.\n    - An initial set of cuts is generated using the structured trajectories as anchor points $x_k$. This provides a foundational approximation.\n2.  **Iterative Refinement**: The algorithm enters a loop that continues until either the maximum number of cuts $K_{\\max}$ is reached or the approximation error is acceptably small.\n    - In each iteration, the current surrogate $g(s)$ is evaluated for all $s \\in \\mathcal{X}$.\n    - The approximation gap is calculated for each sample: $\\Delta(s) = f(s) - g(s)$.\n    - The sample $s^\\star$ that exhibits the maximum gap is identified: $s^\\star = \\arg\\max_{s \\in \\mathcal{X}} \\Delta(s)$.\n    - If $\\Delta(s^\\star)$ is below a specified tolerance $\\tau$, the algorithm terminates.\n    - Otherwise, a new cut is generated using $s^\\star$ as the anchor point $x_k$, and this cut is added to the set defining $g(s)$.\n\n### 3. Cut Generation\n\nA new cut at an anchor point $x_k \\in \\mathcal{X}$ is an affine function $h_k(s) = a_k + b_k^\\top s$ designed to be a valid underestimator of $f(s)$. Its construction involves two main steps:\n\n1.  **Subgradient Approximation**: Since $f(s)$ is complex and may not be continuously differentiable, we use an approximate subgradient, which provides the linear term $b_k$. This vector is computed using a finite difference scheme. For each component $j \\in \\{1, \\dots, T\\}$, the partial derivative $\\frac{\\partial f}{\\partial s_j}(x_k)$ is approximated. To handle the domain boundaries at $s_j=0$ and $s_j=1$, a generalized central difference formula is used:\n    $$\n    b_{k,j} = \\frac{f(\\text{clip}(x_k + \\varepsilon e_j)) - f(\\text{clip}(x_k - \\varepsilon e_j))}{\\text{clip}(x_{k,j} + \\varepsilon) - \\text{clip}(x_{k,j} - \\varepsilon)}\n    $$\n    where $e_j$ is the $j$-th standard basis vector, $\\varepsilon$ is a small perturbation, and $\\text{clip}(\\cdot)$ ensures the argument remains in $[0,1]$. This formulation gracefully transitions from a central difference in the interior of the domain to a one-sided (forward or backward) difference at the boundaries.\n\n2.  **Underestimation Guarantee**: The approximate subgradient does not guarantee that the resulting tangent plane $f(x_k) + b_k^\\top (s - x_k)$ is a global underestimator. To enforce this property over the known sample pool $\\mathcal{X}$, the gradient $b_k$ is scaled by a factor $\\gamma_k \\in [0,1]$. This factor is chosen to be the largest value that prevents the cut from \"overshooting\" $f(s)$ for any sample in $\\mathcal{X}$. It is defined as:\n    $$\n    \\gamma_k = \\min\\left\\{ 1, \\min_{s \\in \\mathcal{X} : b_k^\\top(s-x_k) > 0} \\frac{f(s)-f(x_k)}{b_k^\\top(s-x_k)} \\right\\}\n    $$\n    The inner minimum is taken over all samples where the cut lies above the anchor value. If this set is empty, $\\gamma_k=1$. If numerical artifacts lead to a negative minimum, $\\gamma_k$ is set to $0$. The final cut is defined by the scaled gradient $b_k' = \\gamma_k b_k$ and the intercept $a_k = f(x_k) - (b_k')^\\top x_k$.\n\nThis iterative process constructs a high-quality, convex, piecewise-linear underestimator $g(s)$ for the complex degradation function $f(s)$, making it suitable for integration into large-scale energy system optimization problems. The final output is the maximum approximation gap over the sample pool upon termination.",
            "answer": "```python\nimport numpy as np\n\ndef _get_reversals(series):\n    \"\"\"Extracts the reversal points from a time series.\"\"\"\n    if not isinstance(series, np.ndarray):\n        series = np.array(series)\n    \n    # Filter out consecutive duplicates\n    if series.size == 0:\n        return []\n    \n    unique_series = [series[0]]\n    for i in range(1, len(series)):\n        if series[i] != series[i-1]:\n            unique_series.append(series[i])\n    \n    if len(unique_series)  3:\n        return unique_series\n\n    # Identify extrema\n    reversals = [unique_series[0]]\n    for i in range(1, len(unique_series) - 1):\n        p_prev, p_curr, p_next = unique_series[i-1], unique_series[i], unique_series[i+1]\n        is_peak = p_curr > p_prev and p_curr > p_next\n        is_valley = p_curr  p_prev and p_curr  p_next\n        if is_peak or is_valley:\n            reversals.append(p_curr)\n    reversals.append(unique_series[-1])\n    \n    return reversals\n\ndef rainflow_cost(s, kappa, alpha):\n    \"\"\"Computes the rainflow-based degradation cost for an SoC trajectory.\"\"\"\n    reversals = _get_reversals(s)\n    if len(reversals)  2:\n        return 0.0\n\n    cycles = []\n    stack = []\n\n    for p in reversals:\n        while len(stack) >= 2:\n            x = stack[-1]\n            y = stack[-2]\n            range_xy = abs(x - y)\n            range_px = abs(p - x)\n            if range_px >= range_xy:\n                cycles.append({'depth': range_xy, 'weight': 1.0})\n                stack.pop()\n                stack.pop()\n            else:\n                break\n        stack.append(p)\n\n    while len(stack) >= 2:\n        x = stack.pop()\n        y = stack[-1]\n        range_yx = abs(y - x)\n        cycles.append({'depth': range_yx, 'weight': 0.5})\n    \n    total_cost = sum(kappa * (c['depth']**alpha) * c['weight'] for c in cycles)\n    return total_cost\n\ndef calculate_subgradient(s_anchor_tuple, f_eval_func, T, epsilon):\n    \"\"\"Calculates the approximate subgradient using a generalized finite difference.\"\"\"\n    s_anchor = np.array(s_anchor_tuple)\n    b = np.zeros(T)\n    \n    for j in range(T):\n        s_plus_val = min(s_anchor[j] + epsilon, 1.0)\n        s_minus_val = max(s_anchor[j] - epsilon, 0.0)\n\n        s_plus = s_anchor.copy()\n        s_plus[j] = s_plus_val\n        f_plus = f_eval_func(tuple(s_plus))\n\n        s_minus = s_anchor.copy()\n        s_minus[j] = s_minus_val\n        f_minus = f_eval_func(tuple(s_minus))\n\n        delta_s_j = s_plus_val - s_minus_val\n        if delta_s_j > 1e-12:\n            b[j] = (f_plus - f_minus) / delta_s_j\n        else:\n            b[j] = 0.0\n    return b\n\ndef solve():\n    \"\"\"Main solver function to run the cutting-plane algorithm for all test cases.\"\"\"\n    test_cases = [\n        # Case A: T, N, kappa, alpha, epsilon, tau, K_max, seed\n        (24, 30, 1.0, 2.5, 1e-3, 1e-4, 30, 42),\n        # Case B: T, N, kappa, alpha, epsilon, tau, K_max, seed\n        (12, 12, 1.0, 3.0, 1e-3, 1e-3, 20, 42),\n        # Case C: T, N, kappa, alpha, epsilon, tau, K_max, seed\n        (16, 25, 1.0, 4.0, 1e-3, 1e-4, 30, 42),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        T, N, kappa, alpha, epsilon, tau, K_max, seed = case\n        \n        rng = np.random.default_rng(seed)\n\n        # 1. Generate sample pool X\n        structured_trajs = []\n        structured_trajs.append(np.zeros(T))\n        structured_trajs.append(np.full(T, 0.5))\n        structured_trajs.append(np.linspace(0, 1, T))\n        \n        half = T // 2\n        s_tri = np.zeros(T)\n        if half > 0:\n            s_tri[:half] = np.linspace(0, 1, half, endpoint=False)\n        if T - half > 0:\n            s_tri[half:] = np.linspace(1, 0, T - half)\n        structured_trajs.append(s_tri)\n\n        num_random = N - len(structured_trajs)\n        random_trajs = [rng.random(T) for _ in range(num_random)]\n        \n        X_list = structured_trajs + random_trajs\n        X_tuples = [tuple(s) for s in X_list]\n\n        # 2. Pre-compute f(s) for all s in X using a cache\n        f_cache = {}\n        def f(s_tuple):\n            if s_tuple in f_cache:\n                return f_cache[s_tuple]\n            cost = rainflow_cost(s_tuple, kappa, alpha)\n            f_cache[s_tuple] = cost\n            return cost\n\n        f_X = {s_tuple: f(s_tuple) for s_tuple in X_tuples}\n\n        # 3. Initialize cuts from structured trajectories\n        cuts = []\n        initial_anchors = [tuple(s) for s in structured_trajs]\n\n        for xk_tuple in initial_anchors:\n            b_k = calculate_subgradient(xk_tuple, f, T, epsilon)\n            f_xk = f_X[xk_tuple]\n            \n            ratios = []\n            xk_arr = np.array(xk_tuple)\n            for s_tuple in X_tuples:\n                s_arr = np.array(s_tuple)\n                dot_product = b_k.dot(s_arr - xk_arr)\n                if dot_product > 1e-9:\n                    ratio = (f_X[s_tuple] - f_xk) / dot_product\n                    ratios.append(ratio)\n            \n            gamma_k = min(1.0, min(ratios)) if ratios else 1.0\n            gamma_k = max(0.0, gamma_k)\n\n            b_k_final = gamma_k * b_k\n            a_k = f_xk - b_k_final.dot(xk_arr)\n            cuts.append((a_k, b_k_final))\n\n        # 4. Iterative refinement loop\n        max_gap = float('inf')\n        num_cuts = len(cuts)\n        while num_cuts  K_max:\n            # Find current max gap and corresponding point s_star\n            current_max_gap = -1.0\n            s_star_tuple = None\n\n            for s_tuple in X_tuples:\n                s_arr = np.array(s_tuple)\n                g_s = max(a + b.dot(s_arr) for a, b in cuts) if cuts else 0.0\n                gap = f_X[s_tuple] - g_s\n                if gap > current_max_gap:\n                    current_max_gap = gap\n                    s_star_tuple = s_tuple\n            \n            max_gap = current_max_gap\n            \n            if max_gap = tau:\n                break\n            \n            # Add new cut at s_star\n            x_new_tuple = s_star_tuple\n            b_new = calculate_subgradient(x_new_tuple, f, T, epsilon)\n            f_xnew = f_X[x_new_tuple]\n\n            ratios = []\n            x_new_arr = np.array(x_new_tuple)\n            for s_tuple in X_tuples:\n                s_arr = np.array(s_tuple)\n                dot_product = b_new.dot(s_arr - x_new_arr)\n                if dot_product > 1e-9:\n                    ratio = (f_X[s_tuple] - f_xnew) / dot_product\n                    ratios.append(ratio)\n            \n            gamma_new = min(1.0, min(ratios)) if ratios else 1.0\n            gamma_new = max(0.0, gamma_new)\n            \n            b_new_final = gamma_new * b_new\n            a_new = f_xnew - b_new_final.dot(x_new_arr)\n            cuts.append((a_new, b_new_final))\n            num_cuts += 1\n        \n        # 5. Calculate final max gap after loop terminates\n        final_max_gap = -1.0\n        if not X_tuples:\n             final_max_gap = 0.0\n        else:\n            for s_tuple in X_tuples:\n                s_arr = np.array(s_tuple)\n                g_s = max(a + b.dot(s_arr) for a, b in cuts) if cuts else 0.0\n                gap = f_X[s_tuple] - g_s\n                if gap > final_max_gap:\n                    final_max_gap = gap\n        \n        results.append(final_max_gap)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}