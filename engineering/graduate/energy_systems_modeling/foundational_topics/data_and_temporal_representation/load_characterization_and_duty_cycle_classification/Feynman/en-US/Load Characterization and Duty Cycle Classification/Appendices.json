{
    "hands_on_practices": [
        {
            "introduction": "Before we can classify load duty cycles, we must first establish a robust definition. While a simple on/off percentage works for idealized devices, real-world equipment often has standby power, variable operating levels, and is measured with noisy sensors. This practice challenges you to design a metric that is resilient to these practical issues by systematically evaluating potential formulas against a set of logical axioms . Developing this skill is crucial for ensuring that your analysis is based on meaningful characterizations rather than artifacts of a poorly chosen definition.",
            "id": "4101869",
            "problem": "An electric end-use load exhibits time-varying power consumption $P(t)$ over a finite observation horizon $[0,T]$, where $T>0$ is known and $P(t) \\ge 0$ is integrable. The load has a known standby (baseline) consumption $P_{\\text{standby}} \\ge 0$ and a known nominal on-state consumption $P_{\\text{on}} > P_{\\text{standby}}$. Measurements may contain small excursions below $P_{\\text{standby}}$ due to sensor noise or transient artifacts. Define the positive-part function $(x)_+ \\equiv \\max\\{x,0\\}$, and the indicator function $\\mathbb{1}\\{A\\}$ which equals $1$ if event $A$ holds and $0$ otherwise.\n\nYour goal is to select a single scalar mapping $M[P(\\cdot)]$ from the following options that can serve as a scientifically robust duty cycle metric satisfying all of the following axioms simultaneously:\n\n- Baseline invariance: When $P(t)=P_{\\text{standby}}$ for almost every $t\\in[0,T]$, the metric returns $0$.\n- On-state normalization: When $P(t)=P_{\\text{on}}$ for almost every $t\\in[0,T]$, the metric returns $1$.\n- Exactness for two-level behavior: When $P(t)\\in\\{P_{\\text{standby}},P_{\\text{on}}\\}$ for almost every $t$ (with any arrangement of on/off intervals), the metric equals the true time fraction spent in the on-state, i.e., the duty cycle $d \\equiv \\frac{t_{\\text{on}}}{T}$.\n- Nonnegativity under measurement noise: The metric never becomes negative because $P(t) < P_{\\text{standby}}$ on a set of nonzero measure.\n- Graded proportionality in the excess-power regime: At times $t$ where $P_{\\text{standby}} < P(t) \\le P_{\\text{on}}$, the instantaneous contribution to the metric scales linearly with the excess power $P(t)-P_{\\text{standby}}$, rather than collapsing those intervals to purely off or purely on.\n- Upper bound under nominal on-cap: If $P(t) \\le P_{\\text{on}}$ for almost every $t\\in[0,T]$, then the metric is guaranteed to lie in the closed interval $[0,1]$.\n\nWhich one of the following candidate definitions satisfies all of the axioms above?\n\nA. $M_A[P(\\cdot)] \\;=\\; \\dfrac{1}{T\\!\\left(P_{\\text{on}}-P_{\\text{standby}}\\right)} \\displaystyle\\int_{0}^{T} \\Big(P(t)-P_{\\text{standby}}\\Big)\\,dt$\n\nB. $M_B[P(\\cdot)] \\;=\\; \\dfrac{1}{T} \\displaystyle\\int_{0}^{T} \\mathbb{1}\\!\\left\\{P(t)\\ge P_{\\text{on}}\\right\\} \\, dt$\n\nC. $M_C[P(\\cdot)] \\;=\\; \\dfrac{1}{T\\!\\left(P_{\\text{on}}-P_{\\text{standby}}\\right)} \\displaystyle\\int_{0}^{T} \\Big(P(t)-P_{\\text{standby}}\\Big)_{+}\\,dt$\n\nD. $M_D[P(\\cdot)] \\;=\\; \\dfrac{1}{T\\,P_{\\text{on}}} \\displaystyle\\int_{0}^{T} P(t)\\,dt$",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective before proceeding to a solution.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A time-varying power consumption signal $P(t)$ for an electric load.\n- The observation horizon is a finite interval $[0,T]$, with $T>0$.\n- The power consumption is non-negative, $P(t) \\ge 0$, and integrable on $[0,T]$.\n- A known standby (baseline) consumption level $P_{\\text{standby}} \\ge 0$.\n- A known nominal on-state consumption level $P_{\\text{on}} > P_{\\text{standby}}$.\n- The possibility of measurement noise causing small excursions of $P(t)$ below $P_{\\text{standby}}$.\n- The definition of the positive-part function: $(x)_+ \\equiv \\max\\{x,0\\}$.\n- The definition of the indicator function: $\\mathbb{1}\\{A\\}$ is $1$ if event $A$ holds, and $0$ otherwise.\n\nThe problem requires selecting a scalar mapping $M[P(\\cdot)]$ that must satisfy six axioms simultaneously:\n1.  **Baseline invariance**: If $P(t)=P_{\\text{standby}}$ for almost every $t\\in[0,T]$, then $M[P(\\cdot)]=0$.\n2.  **On-state normalization**: If $P(t)=P_{\\text{on}}$ for almost every $t\\in[0,T]$, then $M[P(\\cdot)]=1$.\n3.  **Exactness for two-level behavior**: If $P(t)\\in\\{P_{\\text{standby}}, P_{\\text{on}}\\}$ for almost every $t$, the metric must equal the true time fraction spent in the on-state, $d \\equiv \\frac{t_{\\text{on}}}{T}$.\n4.  **Nonnegativity under measurement noise**: The metric must be non-negative, $M[P(\\cdot)] \\ge 0$, even if $P(t) < P_{\\text{standby}}$ on a set of nonzero measure.\n5.  **Graded proportionality in the excess-power regime**: For times $t$ where $P_{\\text{standby}} < P(t) \\le P_{\\text{on}}$, the instantaneous contribution to the metric must scale linearly with the excess power, $P(t)-P_{\\text{standby}}$.\n6.  **Upper bound under nominal on-cap**: If $P(t) \\le P_{\\text{on}}$ for almost every $t\\in[0,T]$, the metric must be in the closed interval $[0,1]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the domain of energy systems analysis and electrical load modeling. The concepts of standby power, nominal power, and duty cycle are standard in engineering. The axioms provided are a set of clear, mathematically formal requirements for a robust metric. The problem is well-posed, asking for a verification of which of the given options satisfies all axioms. It is objective and self-contained, with all necessary terms defined. The consideration of measurement noise makes the problem statement more realistic and practically relevant. There are no scientific or logical contradictions, no ambiguities, and the problem is not trivial.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived by systematically evaluating each candidate option against the six specified axioms.\n\n### Derivation and Option Evaluation\n\nLet us denote the normalization factor $\\Delta P = P_{\\text{on}} - P_{\\text{standby}}$. From the givens, we know $\\Delta P > 0$.\n\n#### Evaluation of Option A\n$M_A[P(\\cdot)] \\;=\\; \\dfrac{1}{T\\left(P_{\\text{on}}-P_{\\text{standby}}\\right)} \\displaystyle\\int_{0}^{T} \\Big(P(t)-P_{\\text{standby}}\\Big)\\,dt$\n\n1.  **Baseline invariance**: If $P(t) = P_{\\text{standby}}$, the integrand is $P_{\\text{standby}} - P_{\\text{standby}} = 0$. The integral is $0$, thus $M_A = 0$. This axiom is satisfied.\n2.  **On-state normalization**: If $P(t) = P_{\\text{on}}$, the integrand is $P_{\\text{on}} - P_{\\text{standby}}$. The integral becomes $\\int_{0}^{T} (P_{\\text{on}} - P_{\\text{standby}}) \\,dt = T(P_{\\text{on}} - P_{\\text{standby}})$. Thus, $M_A = \\frac{T(P_{\\text{on}} - P_{\\text{standby}})}{T(P_{\\text{on}} - P_{\\text{standby}})} = 1$. This axiom is satisfied.\n3.  **Exactness for two-level behavior**: Let the total time at $P_{\\text{on}}$ be $t_{\\text{on}}$. The integral splits into two parts: $\\int_{t \\text{ where } P=P_{\\text{on}}} (P_{\\text{on}}-P_{\\text{standby}}) \\,dt + \\int_{t \\text{ where } P=P_{\\text{standby}}} (P_{\\text{standby}}-P_{\\text{standby}}) \\,dt = t_{\\text{on}}(P_{\\text{on}}-P_{\\text{standby}}) + 0$. Thus, $M_A = \\frac{t_{\\text{on}}(P_{\\text{on}}-P_{\\text{standby}})}{T(P_{\\text{on}}-P_{\\text{standby}})} = \\frac{t_{\\text{on}}}{T}$. This axiom is satisfied.\n4.  **Nonnegativity under measurement noise**: If there is an interval where $P(t) < P_{\\text{standby}}$, the integrand $P(t)-P_{\\text{standby}}$ will be negative on that interval. If the integral of these negative parts is large enough to dominate any positive parts, the total integral can be negative. Since $T > 0$ and $P_{\\text{on}}-P_{\\text{standby}} > 0$, a negative integral results in a negative metric $M_A$. This axiom is **not satisfied**.\n\nBecause Option A fails Axiom 4, it is **Incorrect**.\n\n#### Evaluation of Option B\n$M_B[P(\\cdot)] \\;=\\; \\dfrac{1}{T} \\displaystyle\\int_{0}^{T} \\mathbb{1}\\!\\left\\{P(t)\\ge P_{\\text{on}}\\right\\} \\, dt$\n\n1.  **Baseline invariance**: If $P(t) = P_{\\text{standby}}$, then since $P_{\\text{standby}} < P_{\\text{on}}$, the condition $P(t)\\ge P_{\\text{on}}$ is never met. The integrand $\\mathbb{1}\\{P(t)\\ge P_{\\text{on}}\\}$ is $0$ for all $t$. The integral is $0$, so $M_B = 0$. This axiom is satisfied.\n2.  **On-state normalization**: If $P(t) = P_{\\text{on}}$, the condition $P(t)\\ge P_{\\text{on}}$ is always met. The integrand is $1$ for all $t$. The integral is $\\int_{0}^{T} 1 \\,dt = T$. Thus, $M_B = \\frac{T}{T} = 1$. This axiom is satisfied.\n3.  **Exactness for two-level behavior**: If $P(t) \\in \\{P_{\\text{standby}},P_{\\text{on}}\\}$, the integrand $\\mathbb{1}\\{P(t)\\ge P_{\\text{on}}\\}$ is $1$ only when $P(t)=P_{\\text{on}}$ and $0$ when $P(t)=P_{\\text{standby}}$. The integral measures the total time for which $P(t)=P_{\\text{on}}$, which is $t_{\\text{on}}$. Thus, $M_B = \\frac{t_{\\text{on}}}{T}$. This axiom is satisfied.\n4.  **Nonnegativity under measurement noise**: The integrand $\\mathbb{1}\\{\\cdot\\}$ is by definition either $0$ or $1$, so it is non-negative. The integral of a non-negative function is non-negative. Thus, $M_B \\ge 0$. This axiom is satisfied.\n5.  **Graded proportionality**: Consider the regime $P_{\\text{standby}} < P(t) \\le P_{\\text{on}}$. For any time $t$ where $P(t)$ is strictly less than $P_{\\text{on}}$, the condition $P(t) \\ge P_{\\text{on}}$ is false, and the integrand is $0$. This means that all power levels between $P_{\\text{standby}}$ and $P_{\\text{on}}$ are treated as 'off', contributing nothing to the metric. The instantaneous contribution does not scale with $P(t)-P_{\\text{standby}}$; it is always $0$. This axiom is **not satisfied**.\n\nBecause Option B fails Axiom 5, it is **Incorrect**.\n\n#### Evaluation of Option C\n$M_C[P(\\cdot)] \\;=\\; \\dfrac{1}{T\\left(P_{\\text{on}}-P_{\\text{standby}}\\right)} \\displaystyle\\int_{0}^{T} \\Big(P(t)-P_{\\text{standby}}\\Big)_{+}\\,dt$\n\n1.  **Baseline invariance**: If $P(t) = P_{\\text{standby}}$, the integrand is $(P_{\\text{standby}} - P_{\\text{standby}})_+ = (0)_+ = 0$. The integral is $0$, so $M_C = 0$. This axiom is satisfied.\n2.  **On-state normalization**: If $P(t) = P_{\\text{on}}$, the integrand is $(P_{\\text{on}} - P_{\\text{standby}})_+$. Since $P_{\\text{on}} > P_{\\text{standby}}$, this equals $P_{\\text{on}} - P_{\\text{standby}}$. The integral is $T(P_{\\text{on}} - P_{\\text{standby}})$. Thus, $M_C = \\frac{T(P_{\\text{on}}-P_{\\text{standby}})}{T(P_{\\text{on}}-P_{\\text{standby}})} = 1$. This axiom is satisfied.\n3.  **Exactness for two-level behavior**: If $P(t) \\in \\{P_{\\text{standby}},P_{\\text{on}}\\}$, the integrand $(P(t)-P_{\\text{standby}})_+$ is $0$ when $P(t)=P_{\\text{standby}}$ and $P_{\\text{on}}-P_{\\text{standby}}$ when $P(t)=P_{\\text{on}}$. The integral is therefore $\\int_{t \\text{ where } P=P_{\\text{on}}} (P_{\\text{on}}-P_{\\text{standby}}) \\,dt = t_{\\text{on}}(P_{\\text{on}}-P_{\\text{standby}})$. So $M_C = \\frac{t_{\\text{on}}(P_{\\text{on}}-P_{\\text{standby}})}{T(P_{\\text{on}}-P_{\\text{standby}})} = \\frac{t_{\\text{on}}}{T}$. This axiom is satisfied.\n4.  **Nonnegativity under measurement noise**: The integrand is $(P(t)-P_{\\text{standby}})_+ = \\max\\{P(t)-P_{\\text{standby}}, 0\\}$. By definition, this term is always non-negative. The integral of a non-negative function is non-negative, and the pre-factor is positive. Thus, $M_C \\ge 0$ is always true. This axiom is satisfied.\n5.  **Graded proportionality**: In the regime $P_{\\text{standby}} < P(t) \\le P_{\\text{on}}$, the quantity $P(t)-P_{\\text{standby}}$ is positive. Then, by definition of the positive-part function, $(P(t)-P_{\\text{standby}})_+ = P(t)-P_{\\text{standby}}$. The instantaneous contribution to the integral is therefore linear in the excess power $P(t)-P_{\\text{standby}}$. This axiom is satisfied.\n6.  **Upper bound under nominal on-cap**: If $P(t) \\le P_{\\text{on}}$ for almost every $t$, then $P(t)-P_{\\text{standby}} \\le P_{\\text{on}}-P_{\\text{standby}}$. The positive-part function is monotonically non-decreasing, so $(P(t)-P_{\\text{standby}})_+ \\le (P_{\\text{on}}-P_{\\text{standby}})_+ = P_{\\text{on}}-P_{\\text{standby}}$. Integrating this inequality over $[0,T]$ gives $\\int_{0}^{T} (P(t)-P_{\\text{standby}})_+ \\,dt \\le \\int_{0}^{T} (P_{\\text{on}}-P_{\\text{standby}}) \\,dt = T(P_{\\text{on}}-P_{\\text{standby}})$. Dividing by $T(P_{\\text{on}}-P_{\\text{standby}})$ yields $M_C \\le 1$. Combined with Axiom 4 ($M_C \\ge 0$), we have $M_C \\in [0,1]$. This axiom is satisfied.\n\nOption C satisfies all six axioms. It is therefore **Correct**.\n\n#### Evaluation of Option D\n$M_D[P(\\cdot)] \\;=\\; \\dfrac{1}{T\\,P_{\\text{on}}} \\displaystyle\\int_{0}^{T} P(t)\\,dt$\n\n1.  **Baseline invariance**: If $P(t) = P_{\\text{standby}}$, the integral is $\\int_0^T P_{\\text{standby}} dt = T P_{\\text{standby}}$. The metric becomes $M_D = \\frac{T P_{\\text{standby}}}{T P_{\\text{on}}} = \\frac{P_{\\text{standby}}}{P_{\\text{on}}}$. The problem states $P_{\\text{on}} > P_{\\text{standby}} \\ge 0$. Unless $P_{\\text{standby}}=0$, $M_D$ will be greater than $0$. The axiom requires the metric to be $0$. This axiom is **not satisfied** in the general case where $P_{\\text{standby}} > 0$.\n2.  **On-state normalization**: If $P(t)=P_{\\text{on}}$, the integral is $T P_{\\text{on}}$, so $M_D = \\frac{T P_{\\text{on}}}{T P_{\\text{on}}} = 1$. This axiom is satisfied.\n3.  **Exactness for two-level behavior**: With on-time $t_{\\text{on}}$ and off-time $t_{\\text{off}}=T-t_{\\text{on}}$, the integral is $\\int P(t)dt = P_{\\text{on}}t_{\\text{on}} + P_{\\text{standby}}t_{\\text{off}} = P_{\\text{on}}t_{\\text{on}} + P_{\\text{standby}}(T-t_{\\text{on}})$. The metric is $M_D = \\frac{P_{\\text{on}}t_{\\text{on}} + P_{\\text{standby}}(T-t_{\\text{on}})}{T P_{\\text{on}}} = \\frac{t_{\\text{on}}}{T} + \\frac{P_{\\text{standby}}}{P_{\\text{on}}}(1-\\frac{t_{\\text{on}}}{T})$. This is not equal to $\\frac{t_{\\text{on}}}{T}$ unless $P_{\\text{standby}}=0$ or $t_{\\text{on}}=T$. This axiom is **not satisfied** in general.\n4.  **Nonnegativity under measurement noise**: The problem states $P(t) \\ge 0$. The integral of a non-negative function is non-negative. As $T > 0$ and $P_{\\text{on}} > 0$, $M_D \\ge 0$. This axiom is satisfied.\n5.  **Graded proportionality**: The integrand is $P(t)$. This is not a function of the excess power $P(t)-P_{\\text{standby}}$. It does not isolate the power consumption above the baseline. This axiom is **not satisfied**.\n\nBecause Option D fails Axioms 1, 3, and 5, it is **Incorrect**.\n\nConclusion: Only option C satisfies all the required axioms for a robust duty cycle metric. The use of the positive-part function, $(x)_+$, is crucial for handling potential measurement noise (Axiom 4), while the subtraction of the baseline $P_{\\text{standby}}$ and normalization by the dynamic range $P_{\\text{on}}-P_{\\text{standby}}$ ensure proper scaling and boundary conditions (Axioms 1, 2, 5, 6).",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Once a load profile is characterized, it is vital to understand its physical impact on the energy system. A common and dangerous oversimplification is to assume that two profiles with the same average power exert the same stress on equipment. This exercise demonstrates why the *shape* of the load profile is critically important, particularly for thermally limited components like motors and drives, where losses often scale with the square of the power or current . Understanding this principle is key to correctly assessing equipment health, preventing premature failures, and designing systems that can withstand the demands of intermittent operation.",
            "id": "4101853",
            "problem": "An industrial Variable Speed Drive (VSD) supplies a three-phase motor from a stiff grid with approximately constant line-to-line voltage and near-unity power factor over its operating range. Consider two one-hour duty cycles that deliver the same net energy to the driven process. The VSD and upstream conductors are thermally limited primarily by conduction-related losses that scale with the square of electrical current. Assume that over the operating range, electrical input current is approximately proportional to electrical input power, so that internal Joule heating and conduction losses scale proportionally to the time integral of the square of input power. Also assume that the thermal time constant of the limiting component is much longer than the cycle duration, so that cumulative energy of losses is the relevant severity measure.\n\nLet the rated input power of the VSD be $P_{\\text{rated}} = 10\\,\\text{kW}$. Define two duty cycles over a duration of $T = 1\\,\\text{h}$:\n\n- Profile A: Constant input power $P(t) = 5\\,\\text{kW}$ for all $t \\in [0,T]$.\n- Profile B: Square-wave input power taking values $P(t) = 10\\,\\text{kW}$ for $t \\in [0,0.5T)$ and $P(t) = 0\\,\\text{kW}$ for $t \\in [0.5T,T]$.\n\nBoth profiles deliver the same net energy $\\int_{0}^{T} P(t)\\,dt$.\n\nA commonly used utilization factor is defined as the ratio of average input power to rated input power. In practice, this factor is often used to classify duty severity. You are asked to assess whether this factor appropriately captures thermal severity under the above assumptions for variable-speed operation and, if not, to evaluate a corrected severity metric that penalizes power peaks consistent with the assumed loss mechanism.\n\nWhich of the following statements are correct?\n\nA. Because both profiles have the same average input power, the utilization factor correctly infers that the quadratically scaling thermal severity is identical for both profiles.\n\nB. A dimensionless corrected severity metric that is consistent with the assumed loss mechanism and that normalizes by energy throughput and rated power yields, for these two profiles, values $M_{\\text{A}} = 0.5$ and $M_{\\text{B}} = 1.0$, thereby ranking Profile B as more severe.\n\nC. Under the stated assumptions, cumulative conduction-related thermal losses over the cycle scale with $\\int_{0}^{T} P(t)\\,dt$, hence both profiles are thermally equivalent.\n\nD. If the input power waveform $P(t)$ is multiplied by a constant amplitude factor $c>0$ while $P_{\\text{rated}}$ is held fixed, the corrected severity metric remains invariant.\n\nE. The corrected severity metric in statement B can be equivalently written as the ratio $\\dfrac{P_{\\text{RMS}}^{2}}{P_{\\text{rated}}\\,P_{\\text{avg}}}$, where $P_{\\text{RMS}}^{2} \\equiv \\dfrac{1}{T}\\int_{0}^{T} P^{2}(t)\\,dt$ and $P_{\\text{avg}} \\equiv \\dfrac{1}{T}\\int_{0}^{T} P(t)\\,dt$.",
            "solution": "The problem statement is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **System**: An industrial Variable Speed Drive (VSD) supplies a three-phase motor from a stiff grid.\n*   **Grid Conditions**: Approximately constant line-to-line voltage.\n*   **Power Factor**: Near-unity over the operating range.\n*   **Thermal Limitation**: Primarily due to conduction-related losses.\n*   **Loss Scaling Assumption**: Losses scale with the square of electrical current ($I^2$).\n*   **Current-Power Assumption**: Input current is approximately proportional to electrical input power, $I \\propto P(t)$.\n*   **Resulting Loss Scaling**: Internal Joule heating and conduction losses scale proportionally to the time integral of the square of input power. This means the cumulative energy of losses, $E_{\\text{loss}}$, is proportional to $\\int_{0}^{T} P^{2}(t)\\,dt$.\n*   **Thermal Time Constant Assumption**: The thermal time constant of the limiting component is much longer than the cycle duration.\n*   **Severity Measure**: Cumulative energy of losses is the relevant severity measure.\n*   **Rated Power**: $P_{\\text{rated}} = 10\\,\\text{kW}$.\n*   **Cycle Duration**: $T = 1\\,\\text{h}$.\n*   **Profile A**: Constant input power $P(t) = 5\\,\\text{kW}$ for all $t \\in [0,T]$.\n*   **Profile B**: Square-wave input power with $P(t) = 10\\,\\text{kW}$ for $t \\in [0,0.5T)$ and $P(t) = 0\\,\\text{kW}$ for $t \\in [0.5T, T]$.\n*   **Energy Equivalence**: Both profiles deliver the same net energy, $\\int_{0}^{T} P(t)\\,dt$.\n*   **Definition**: A utilization factor is defined as the ratio of average input power to rated input power.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientific Grounding**: The problem is well-grounded in the principles of electrical engineering and thermal analysis of electrical equipment. The assumption that conduction losses ($I^2R$ losses) are dominant is reasonable. The assumption $I \\propto P(t)$ follows from the power equation $P = \\sqrt{3} V_{LL} I \\cos(\\phi)$ when voltage $V_{LL}$ and power factor $\\cos(\\phi)$ are assumed constant, which is explicitly stated in the problem (stiff grid, near-unity power factor). The consequence that loss energy scales with $\\int P^2(t)\\,dt$ is a direct and logical deduction from these premises. The assumption of a long thermal time constant is standard for simplifying duty cycle analysis to an energy-based approach.\n\n2.  **Well-Posedness**: The problem is well-posed. All necessary data and definitions are provided to perform the required calculations and evaluate the statements. The question is specific and answerable.\n\n3.  **Objectivity**: The problem is stated in objective, technical language, free from subjective or biased phrasing.\n\n4.  **Completeness and Consistency**: The setup is both complete and internally consistent. The claim that both profiles deliver the same net energy can be verified:\n    *   Energy for Profile A: $E_A = \\int_{0}^{T} (5\\,\\text{kW})\\,dt = (5\\,\\text{kW}) \\cdot T$.\n    *   Energy for Profile B: $E_B = \\int_{0}^{0.5T} (10\\,\\text{kW})\\,dt + \\int_{0.5T}^{T} (0\\,\\text{kW})\\,dt = (10\\,\\text{kW}) \\cdot (0.5T) = (5\\,\\text{kW}) \\cdot T$.\n    *   The energies are indeed equal, confirming the statement's consistency.\n\n5.  **Realism**: The power levels, duty cycle shapes, and the overall scenario are realistic for industrial applications of VSDs and motors.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, self-consistent, well-posed, and based on reasonable and clearly stated physical assumptions. The solution process may proceed.\n\n### Solution Derivation\n\nThe core of the problem is to compare the thermal severity of two duty cycles. According to the problem statement, thermal severity is determined by the cumulative energy of losses, which is proportional to the time integral of the square of the input power. Let the constant of proportionality be $k$.\n$$E_{\\text{loss}} = k \\int_{0}^{T} P(t)^2 \\,dt$$\nTo compare the severity of different profiles, we need to calculate this integral for each. It is convenient to work with the root-mean-square (RMS) power, as its square is directly proportional to the average rate of loss energy generation.\nThe average power is $P_{\\text{avg}} = \\frac{1}{T}\\int_{0}^{T} P(t)\\,dt$.\nThe mean square power is $P_{\\text{RMS}}^2 = \\frac{1}{T}\\int_{0}^{T} P^2(t)\\,dt$.\nThus, the total loss energy is $E_{\\text{loss}} = k \\cdot T \\cdot P_{\\text{RMS}}^2$. For a fixed duration $T$, the thermal severity is proportional to $P_{\\text{RMS}}^2$.\n\n**Calculations for Profile A:**\nThe power is constant, $P_A(t) = 5\\,\\text{kW}$.\nThe average power is:\n$$P_{A, \\text{avg}} = \\frac{1}{T} \\int_{0}^{T} 5 \\,dt = 5\\,\\text{kW}$$\nThe mean square power is:\n$$P_{A, \\text{RMS}}^2 = \\frac{1}{T} \\int_{0}^{T} (5)^2 \\,dt = \\frac{1}{T} \\int_{0}^{T} 25 \\,dt = 25\\,(\\text{kW})^2$$\n\n**Calculations for Profile B:**\nThe power is a square wave, $P_B(t) = 10\\,\\text{kW}$ for $t \\in [0, 0.5T)$ and $P_B(t) = 0\\,\\text{kW}$ for $t \\in [0.5T, T]$.\nThe average power is:\n$$P_{B, \\text{avg}} = \\frac{1}{T} \\left( \\int_{0}^{0.5T} 10 \\,dt + \\int_{0.5T}^{T} 0 \\,dt \\right) = \\frac{1}{T} (10 \\cdot 0.5T) = 5\\,\\text{kW}$$\nThe mean square power is:\n$$P_{B, \\text{RMS}}^2 = \\frac{1}{T} \\left( \\int_{0}^{0.5T} (10)^2 \\,dt + \\int_{0.5T}^{T} (0)^2 \\,dt \\right) = \\frac{1}{T} (100 \\cdot 0.5T) = 50\\,(\\text{kW})^2$$\n\n**Comparison of Thermal Severity:**\nThe thermal severity is proportional to $P_{\\text{RMS}}^2$.\nFor Profile A, severity is proportional to $25\\,(\\text{kW})^2$.\nFor Profile B, severity is proportional to $50\\,(\\text{kW})^2$.\nProfile B is twice as thermally severe as Profile A, despite having the same average power.\n\n### Option-by-Option Analysis\n\n**A. Because both profiles have the same average input power, the utilization factor correctly infers that the quadratically scaling thermal severity is identical for both profiles.**\nBoth profiles have $P_{\\text{avg}} = 5\\,\\text{kW}$. The rated power is $P_{\\text{rated}} = 10\\,\\text{kW}$. The utilization factor for both is $\\frac{5\\,\\text{kW}}{10\\,\\text{kW}} = 0.5$. If this factor were a correct measure of thermal severity, it would imply the severities are identical. However, our analysis showed that the thermal severity (proportional to $P_{\\text{RMS}}^2$) is $25\\,(\\text{kW})^2$ for Profile A and $50\\,(\\text{kW})^2$ for Profile B. The severities are not identical. Therefore, the utilization factor based on average power is an incorrect measure for this loss mechanism.\nVerdict: Incorrect.\n\n**B. A dimensionless corrected severity metric that is consistent with the assumed loss mechanism and that normalizes by energy throughput and rated power yields, for these two profiles, values $M_{\\text{A}} = 0.5$ and $M_{\\text{B}} = 1.0$, thereby ranking Profile B as more severe.**\nA metric consistent with the loss mechanism must be proportional to the total loss energy, $\\int_{0}^{T} P(t)^2\\,dt$. The energy throughput is $E_{\\text{net}} = \\int_{0}^{T} P(t)\\,dt$. Normalizing by energy throughput and rated power suggests a metric of the form:\n$$M = \\frac{\\int_{0}^{T} P(t)^2\\,dt}{(\\int_{0}^{T} P(t)\\,dt) \\cdot P_{\\text{rated}}}$$\nLet's compute this for both profiles. We know $P_{\\text{rated}} = 10\\,\\text{kW}$ and $\\int_{0}^{T} P(t)\\,dt = (5\\,\\text{kW}) \\cdot T$ for both.\nFor Profile A:\n$$M_A = \\frac{\\int_{0}^{T} (5)^2\\,dt}{(5T) \\cdot 10} = \\frac{25T}{50T} = 0.5$$\nFor Profile B:\n$$M_B = \\frac{\\int_{0}^{0.5T} (10)^2\\,dt}{(5T) \\cdot 10} = \\frac{100 \\cdot (0.5T)}{50T} = \\frac{50T}{50T} = 1.0$$\nThe calculated values $M_A = 0.5$ and $M_B = 1.0$ match the statement. This metric correctly ranks Profile B ($M_B=1.0$) as more severe than Profile A ($M_A=0.5$).\nVerdict: Correct.\n\n**C. Under the stated assumptions, cumulative conduction-related thermal losses over the cycle scale with $\\int_{0}^{T} P(t)\\,dt$, hence both profiles are thermally equivalent.**\nThis statement is a direct contradiction of the problem's premises. The problem explicitly states that losses scale with the square of the current, and current is proportional to power. Therefore, cumulative losses scale with $\\int_{0}^{T} P(t)^2\\,dt$, not with $\\int_{0}^{T} P(t)\\,dt$. Since the integral of $P(t)$ is the same for both profiles but the integral of $P(t)^2$ is not, the thermal equivalence conclusion is false.\nVerdict: Incorrect.\n\n**D. If the input power waveform $P(t)$ is multiplied by a constant amplitude factor $c>0$ while $P_{\\text{rated}}$ is held fixed, the corrected severity metric remains invariant.**\nLet the new waveform be $P'(t) = c \\cdot P(t)$. The corrected metric from option B is $M = \\frac{\\int_{0}^{T} P(t)^2\\,dt}{(\\int_{0}^{T} P(t)\\,dt) \\cdot P_{\\text{rated}}}$. Let's compute the new metric $M'$ for the waveform $P'(t)$:\n$$M' = \\frac{\\int_{0}^{T} (P'(t))^2\\,dt}{(\\int_{0}^{T} P'(t)\\,dt) \\cdot P_{\\text{rated}}} = \\frac{\\int_{0}^{T} (c \\cdot P(t))^2\\,dt}{(\\int_{0}^{T} c \\cdot P(t)\\,dt) \\cdot P_{\\text{rated}}}$$\n$$M' = \\frac{c^2 \\int_{0}^{T} P(t)^2\\,dt}{c \\int_{0}^{T} P(t)\\,dt \\cdot P_{\\text{rated}}} = c \\left( \\frac{\\int_{0}^{T} P(t)^2\\,dt}{(\\int_{0}^{T} P(t)\\,dt) \\cdot P_{\\text{rated}}} \\right) = c \\cdot M$$\nThe metric $M'$ is equal to $c \\cdot M$, not $M$. Thus, the metric is not invariant; it scales linearly with the factor $c$.\nVerdict: Incorrect.\n\n**E. The corrected severity metric in statement B can be equivalently written as the ratio $\\dfrac{P_{\\text{RMS}}^{2}}{P_{\\text{rated}}\\,P_{\\text{avg}}}$, where $P_{\\text{RMS}}^{2} \\equiv \\dfrac{1}{T}\\int_{0}^{T} P^{2}(t)\\,dt$ and $P_{\\text{avg}} \\equiv \\dfrac{1}{T}\\int_{0}^{T} P(t)\\,dt$.**\nLet's substitute the definitions of $P_{\\text{RMS}}^2$ and $P_{\\text{avg}}$ into the given ratio:\n$$\\frac{P_{\\text{RMS}}^{2}}{P_{\\text{rated}}\\,P_{\\text{avg}}} = \\frac{\\frac{1}{T}\\int_{0}^{T} P(t)^2\\,dt}{P_{\\text{rated}} \\cdot \\left(\\frac{1}{T}\\int_{0}^{T} P(t)\\,dt\\right)}$$\nThe factor of $\\frac{1}{T}$ in the numerator and denominator cancels out, leaving:\n$$\\frac{\\int_{0}^{T} P(t)^2\\,dt}{P_{\\text{rated}} \\cdot \\int_{0}^{T} P(t)\\,dt}$$\nThis expression is identical to the one we formulated for the corrected severity metric $M$ in the analysis of option B. Therefore, the statement is a correct algebraic reformulation of the metric.\nVerdict: Correct.",
            "answer": "$$\\boxed{BE}$$"
        },
        {
            "introduction": "Modern energy datasets from smart meters can contain thousands of daily load profiles, making manual classification impossible. This hands-on coding practice introduces a powerful approach for automatically discovering distinct usage patterns using unsupervised machine learning. You will implement the k-means algorithm from first principles to group profiles based on their shape and use a validation metric—the silhouette score—to determine the natural number of load classes present in the data . This exercise provides a foundational skill in energy data analytics, enabling you to segment large populations of loads for applications like targeted demand response and improved forecasting.",
            "id": "4101893",
            "problem": "Consider a collection of daily electric load profiles represented as hourly average power measurements over a single day. Let each profile be a vector $x \\in \\mathbb{R}^{24}$ whose entries are nonnegative and measured in kilowatts. To focus on duty cycle shape rather than absolute magnitude, each profile is transformed by unit daily energy normalization: for a given $x$, define $y = x / \\left(\\sum_{t=1}^{24} x_t\\right)$; the normalized profile $y \\in \\mathbb{R}^{24}$ has $\\sum_{t=1}^{24} y_t = 1$ and encodes the relative distribution of daily energy across hours.\n\nThe task is to derive and implement clustering of these normalized profiles using the $k$-means method from first principles, and then to determine the number of clusters $k$ using the silhouette score so as to capture distinct duty cycle classes. Use the following fundamental bases:\n\n- The definition of Euclidean distance in $\\mathbb{R}^{T}$ for $T=24$, where for two vectors $u,v \\in \\mathbb{R}^{24}$, the squared distance is $\\|u - v\\|_2^2 = \\sum_{t=1}^{24} (u_t - v_t)^2$.\n- The $k$-means clustering objective, defined as minimizing the within-cluster sum of squared Euclidean distances between data points and their assigned cluster centroids.\n- The silhouette score for a given clustering, defined for each data point $i$ as $s(i) = \\dfrac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}$, where $a(i)$ is the average distance from $i$ to other points in its assigned cluster, and $b(i)$ is the minimum over other clusters of the average distance from $i$ to points in those clusters. The overall silhouette score is the average of $s(i)$ over all points.\n\nYou must:\n- Derive the centroid update rule and the assignment rule for $k$-means based on minimizing the within-cluster squared Euclidean distances, starting from the above fundamental bases and definitions.\n- Derive the silhouette score formula from first principles based on intra-cluster and nearest other-cluster average distances.\n- Implement $k$-means from scratch (no external clustering libraries), operating on the normalized profiles $y$.\n- For selecting $k$, evaluate the average silhouette score for candidate values of $k$ and choose the $k$ that maximizes this score. In case of a tie in the average silhouette score across different $k$ values, choose the smallest such $k$.\n\nDesign the program to process the following test suite, each test case specifying a synthetic dataset and a candidate set of $k$ values. All random generation must be seeded as specified to ensure reproducibility.\n\nTest case $1$ (happy path, three distinct duty cycle classes):\n- Number of hours $T = 24$.\n- Number of profiles $N = 90$.\n- Construct three prototype normalized shapes in $\\mathbb{R}^{24}$ using hour index $t \\in \\{1,\\dots,24\\}$:\n  - Continuous base load: $p^{(1)}_t = 1$ for all $t$.\n  - Daytime peak: $p^{(2)}_t = 0.2 + \\exp\\left(-\\dfrac{(t-13)^2}{2\\cdot 3^2}\\right)$.\n  - Evening peak: $p^{(3)}_t = 0.2 + \\exp\\left(-\\dfrac{(t-20)^2}{2\\cdot 2^2}\\right)$.\n- Generate $30$ profiles from each prototype by adding independent Gaussian noise $n_t \\sim \\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.05$, then clip to be nonnegative using a small floor $\\epsilon = 10^{-6}$, and then normalize to unit sum as $y = x / \\left(\\sum_{t=1}^{24} x_t\\right)$.\n- Random amplitude scaling before normalization: for each sample, multiply $p^{(j)}$ by a factor $\\alpha$ drawn from a uniform distribution on $[0.8, 1.2]$.\n- Random seed for this test case: $\\text{seed} = 42$.\n- Candidate $k$ values: $\\{2,3,4,5\\}$.\n\nTest case $2$ (two classes with partial overlap, testing silhouette’s ability to avoid over-splitting):\n- Number of hours $T = 24$.\n- Number of profiles $N = 80$.\n- Construct two prototype normalized shapes:\n  - Morning peak: $p^{(4)}_t = 0.25 + \\exp\\left(-\\dfrac{(t-8)^2}{2\\cdot 3^2}\\right)$.\n  - Midday peak: $p^{(5)}_t = 0.25 + \\exp\\left(-\\dfrac{(t-13)^2}{2\\cdot 4^2}\\right)$.\n- Generate $40$ profiles from each prototype using Gaussian noise with $\\sigma = 0.08$, clip to nonnegative using $\\epsilon = 10^{-6}$, and normalize to unit sum.\n- Random amplitude scaling before normalization: $\\alpha \\sim \\text{Uniform}[0.85, 1.15]$.\n- Random seed for this test case: $\\text{seed} = 123$.\n- Candidate $k$ values: $\\{2,3,4,5\\}$.\n\nTest case $3$ (boundary condition with very small dataset):\n- Number of hours $T = 24$.\n- Number of profiles $N = 4$.\n- Construct two prototype normalized shapes:\n  - Evening-heavy: $p^{(6)}_t = 0.2 + \\exp\\left(-\\dfrac{(t-19)^2}{2\\cdot 2^2}\\right)$.\n  - Flat baseline: $p^{(7)}_t = 1$ for all $t$.\n- Generate $2$ profiles from each prototype using Gaussian noise with $\\sigma = 0.02$, clip to nonnegative using $\\epsilon = 10^{-6}$, and normalize to unit sum.\n- Random amplitude scaling before normalization: $\\alpha \\sim \\text{Uniform}[0.9, 1.1]$.\n- Random seed for this test case: $\\text{seed} = 999$.\n- Candidate $k$ values: $\\{2,3,4\\}$.\n\nAlgorithmic requirements:\n- Implement $k$-means with random initialization by selecting $k$ distinct data points as initial centroids. Iterate assignment and centroid update until convergence, where convergence is defined as no change in assignments or the maximum centroid shift being less than a tolerance $\\tau = 10^{-6}$, or a maximum of $I_{\\max} = 300$ iterations is reached.\n- To mitigate sensitivity to initialization, perform $R = 10$ random restarts and choose the run that minimizes the within-cluster sum of squared distances. Handle empty clusters by reinitializing their centroids to a randomly chosen data point.\n- Compute silhouette scores using Euclidean distances on the normalized profiles. For a singleton cluster, define $a(i) = 0$ for its member when computing the silhouette formula.\n\nOutput specification:\n- For each test case, choose the $k$ in its candidate set that maximizes the average silhouette score as described. If multiple $k$ values achieve the same maximum average silhouette score within a numerical tolerance of $\\delta = 10^{-9}$, select the smallest such $k$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3]$, where $k_j$ is the selected number of clusters for test case $j$.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of cluster analysis, mathematically well-posed, and provides a complete and consistent set of definitions, algorithms, and test cases. It is an objective and verifiable task in a standard area of energy systems modeling.\n\n### Derivation of the $k$-Means Algorithm\n\nThe $k$-means algorithm aims to partition a set of $N$ data points $\\{y_1, y_2, \\dots, y_N\\}$, where each $y_i \\in \\mathbb{R}^{T}$, into $k$ disjoint clusters $\\{C_1, C_2, \\dots, C_k\\}$. The objective is to minimize the within-cluster sum of squares (WCSS), also known as inertia. The objective function, $J$, is defined as the sum of squared Euclidean distances between each data point and the centroid of its assigned cluster.\n\nLet $\\mu_j$ be the centroid of cluster $C_j$. The objective function is:\n$$\nJ = \\sum_{j=1}^{k} \\sum_{y_i \\in C_j} \\|y_i - \\mu_j\\|_2^2\n$$\nwhere $\\|y_i - \\mu_j\\|_2^2 = \\sum_{t=1}^{T} (y_{it} - \\mu_{jt})^2$ is the squared Euclidean distance.\n\nMinimizing $J$ with respect to both the cluster assignments (the sets $C_j$) and the centroids (the vectors $\\mu_j$) is a computationally hard problem. The $k$-means algorithm employs an iterative coordinate descent approach, alternately optimizing for one set of variables while holding the other fixed. This process consists of two steps: the assignment step and the update step.\n\n**1. Assignment Step (E-step):**\nIn this step, we assume the cluster centroids $\\{\\mu_1, \\dots, \\mu_k\\}$ are fixed. We aim to minimize $J$ by assigning each data point $y_i$ to a cluster. Since $J$ is a sum over all clusters, and each point belongs to exactly one cluster, we can minimize $J$ by minimizing the contribution of each point individually. For a single point $y_i$, its contribution to $J$ is $\\|y_i - \\mu_j\\|_2^2$, where $j$ is the index of the cluster to which $y_i$ is assigned. To minimize this term, we must assign $y_i$ to the cluster with the nearest centroid. This gives the assignment rule:\n$$\ny_i \\in C_j \\iff \\|y_i - \\mu_j\\|_2 \\le \\|y_i - \\mu_l\\|_2 \\quad \\forall l \\in \\{1, \\dots, k\\}\n$$\nIn practice, this is equivalent to comparing squared distances, which is computationally more efficient:\n$$\ny_i \\text{ is assigned to cluster } j^* = \\arg\\min_{j \\in \\{1,\\dots,k\\}} \\|y_i - \\mu_j\\|_2^2\n$$\n\n**2. Update Step (M-step):**\nIn this step, we assume the cluster assignments $\\{C_1, \\dots, C_k\\}$ are fixed. We aim to minimize $J$ by updating the centroids $\\{\\mu_1, \\dots, \\mu_k\\}$. The objective function $J$ is a sum of terms, each depending on only one centroid $\\mu_j$. We can therefore minimize $J$ by minimizing each term independently:\n$$\n\\min_{\\mu_j} \\left( J_j = \\sum_{y_i \\in C_j} \\|y_i - \\mu_j\\|_2^2 \\right) \\quad \\text{for each } j \\in \\{1, \\dots, k\\}\n$$\nTo find the vector $\\mu_j$ that minimizes the sum of squared distances $J_j$, we take the gradient of $J_j$ with respect to $\\mu_j$ and set it to zero.\nThe objective for cluster $j$ is $J_j = \\sum_{y_i \\in C_j} (y_i - \\mu_j)^T (y_i - \\mu_j)$.\nThe gradient is:\n$$\n\\nabla_{\\mu_j} J_j = \\nabla_{\\mu_j} \\sum_{y_i \\in C_j} (y_i^T y_i - 2y_i^T \\mu_j + \\mu_j^T \\mu_j) = \\sum_{y_i \\in C_j} (-2y_i + 2\\mu_j)\n$$\nSetting the gradient to zero to find the minimum:\n$$\n\\sum_{y_i \\in C_j} (-2y_i + 2\\mu_j) = 0 \\implies \\sum_{y_i \\in C_j} 2\\mu_j = \\sum_{y_i \\in C_j} 2y_i\n$$\nLet $|C_j|$ be the number of points in cluster $C_j$. Then $\\sum_{y_i \\in C_j} \\mu_j = |C_j| \\mu_j$.\n$$\n|C_j| \\mu_j = \\sum_{y_i \\in C_j} y_i \\implies \\mu_j = \\frac{1}{|C_j|} \\sum_{y_i \\in C_j} y_i\n$$\nThis derivation shows that the optimal centroid for a cluster is the arithmetic mean (the center of mass) of all data points assigned to it.\n\nThe algorithm iterates between these two steps until convergence, which is typically defined by the cluster assignments no longer changing, or the centroids moving by a negligible amount.\n\n### Derivation of the Silhouette Score\n\nThe silhouette score is a measure used to evaluate the quality of a clustering. It quantifies how well each data point fits within its assigned cluster compared to other, neighboring clusters. The score is calculated for each point and then averaged over all points.\n\nFor a single data point $y_i$ in a cluster $C_j$:\n\n**1. Intra-Cluster Distance, $a(i)$:**\nThis value measures the cohesion of the point with respect to its own cluster. It is defined as the average Euclidean distance from $y_i$ to all other points in the same cluster, $C_j$.\n$$\na(i) = \\frac{1}{|C_j| - 1} \\sum_{y_l \\in C_j, l \\ne i} \\|y_i - y_l\\|_2\n$$\nA small value of $a(i)$ indicates that the point is well-matched to its cluster. If the cluster $C_j$ contains only a single point (a singleton cluster, i.e., $|C_j|=1$), the sum is empty, and $a(i)$ is defined to be $0$.\n\n**2. Inter-Cluster Distance, $b(i)$:**\nThis value measures the separation of the point from other clusters. It is defined as the minimum of the average distances from $y_i$ to all points in any other cluster $C_m$, where $m \\ne j$.\nFirst, for each other cluster $C_m$, we calculate the average distance from $y_i$ to its points:\n$$\nd(i, C_m) = \\frac{1}{|C_m|} \\sum_{y_l \\in C_m} \\|y_i - y_l\\|_2\n$$\nThen, $b(i)$ is the minimum of these values over all clusters to which $y_i$ does not belong. This represents the distance to the \"nearest neighboring cluster\".\n$$\nb(i) = \\min_{m \\ne j} \\{ d(i, C_m) \\}\n$$\nA large value of $b(i)$ indicates that the point is far from other clusters.\n\n**3. Silhouette Coefficient, $s(i)$:**\nThe silhouette coefficient for the point $y_i$ combines $a(i)$ and $b(i)$ into a single normalized score:\n$$\ns(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n$$\nThe value of $s(i)$ ranges from $-1$ to $1$:\n- $s(i) \\approx 1$: Indicates that the point is well-clustered. Its intra-cluster distance $a(i)$ is much smaller than its inter-cluster distance $b(i)$.\n- $s(i) \\approx 0$: Indicates that the point lies on or near the decision boundary between two clusters. Its distances $a(i)$ and $b(i)$ are comparable.\n- $s(i) \\approx -1$: Indicates that the point may have been misclassified. Its intra-cluster distance $a(i)$ is much larger than its inter-cluster distance $b(i)$, suggesting it is closer to the neighboring cluster than to its own.\n\nThe overall quality of a clustering for a given $k$ is the average silhouette score over all $N$ data points:\n$$\nS_k = \\frac{1}{N} \\sum_{i=1}^{N} s(i)\n$$\nThe optimal number of clusters, $k$, is chosen as the one that maximizes this average score.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(T, prototypes, n_per_proto, noise_sigma, amp_range, seed):\n    \"\"\"Generates synthetic load profile data for a test case.\"\"\"\n    rng = np.random.default_rng(seed)\n    data = []\n    t_coords = np.arange(1, T + 1)\n    \n    # Pre-defined prototype shapes\n    proto_defs = {\n        1: lambda t: np.ones_like(t, dtype=float),\n        2: lambda t: 0.2 + np.exp(-(t - 13)**2 / (2 * 3**2)),\n        3: lambda t: 0.2 + np.exp(-(t - 20)**2 / (2 * 2**2)),\n        4: lambda t: 0.25 + np.exp(-(t - 8)**2 / (2 * 3**2)),\n        5: lambda t: 0.25 + np.exp(-(t - 13)**2 / (2 * 4**2)),\n6: lambda t: 0.2 + np.exp(-(t - 19)**2 / (2 * 2**2)),\n        7: lambda t: np.ones_like(t, dtype=float),\n    }\n\n    for proto_idx, n in zip(prototypes, n_per_proto):\n        p_base = proto_defs[proto_idx](t_coords)\n        for _ in range(n):\n            alpha = rng.uniform(amp_range[0], amp_range[1])\n            noise = rng.normal(0, noise_sigma, size=T)\n            \n            x = alpha * p_base + noise\n            x = np.maximum(x, 1e-6)  # Clip to be non-negative\n            \n            y = x / np.sum(x)  # Normalize to unit daily energy\n            data.append(y)\n    \n    return np.array(data)\n\ndef kmeans(data, k, n_restarts, max_iter, tol, seed):\n    \"\"\"\n    Implements k-means clustering from first principles.\n    \n    Returns the best clustering (labels, centroids, wcss) found over all restarts.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n_samples, n_features = data.shape\n    \n    best_wcss = np.inf\n    best_labels = None\n    best_centroids = None\n    \n    restart_seeds = rng.integers(0, 2**32 - 1, size=n_restarts)\n\n    for i in range(n_restarts):\n        restart_rng = np.random.default_rng(restart_seeds[i])\n        \n        initial_indices = restart_rng.choice(n_samples, k, replace=False)\n        centroids = data[initial_indices]\n        \n        current_labels = np.full(n_samples, -1, dtype=int)\n\n        for iteration in range(max_iter):\n            # Assignment step\n            dist_sq = np.sum((data[:, np.newaxis, :] - centroids[np.newaxis, :, :])**2, axis=2)\n            new_labels = np.argmin(dist_sq, axis=1)\n\n            # Convergence check: no change in assignments\n            if np.array_equal(new_labels, current_labels):\n                break\n            \n            current_labels = new_labels\n            \n            # Update step\n            old_centroids = np.copy(centroids)\n            for c_idx in range(k):\n                cluster_points = data[current_labels == c_idx]\n                if len(cluster_points) == 0:\n                    # Handle empty cluster by re-initializing to a random point\n                    reinit_idx = restart_rng.choice(n_samples)\n                    centroids[c_idx] = data[reinit_idx]\n                else:\n                    centroids[c_idx] = cluster_points.mean(axis=0)\n\n            # Convergence check: centroid shift below tolerance\n            centroid_shifts = np.linalg.norm(centroids - old_centroids, axis=1)\n            if np.max(centroid_shifts) < tol:\n                break\n        \n        # Calculate WCSS for this run\n        wcss = 0\n        for c_idx in range(k):\n            cluster_points = data[current_labels == c_idx]\n            if len(cluster_points) > 0:\n                wcss += np.sum((cluster_points - centroids[c_idx])**2)\n        \n        if wcss < best_wcss:\n            best_wcss = wcss\n            best_labels = current_labels\n            best_centroids = centroids\n            \n    return best_labels, best_centroids, best_wcss\n\ndef calculate_silhouette_score(data, labels):\n    \"\"\"Calculates the average silhouette score for a given clustering.\"\"\"\n    n_samples = data.shape[0]\n    unique_labels = np.unique(labels)\n    n_clusters = len(unique_labels)\n    \n    if n_clusters <= 1:\n        return 0.0\n    if n_clusters == n_samples:\n        return 1.0\n\n    dist_matrix = np.linalg.norm(data[:, np.newaxis, :] - data[np.newaxis, :, :], axis=2)\n    \n    s_scores = np.zeros(n_samples)\n    for i in range(n_samples):\n        my_label = labels[i]\n        \n        # Intra-cluster distance a(i)\n        in_cluster_mask = (labels == my_label)\n        in_cluster_mask[i] = False\n        n_in_cluster = np.sum(in_cluster_mask)\n        \n        if n_in_cluster == 0:\n            a_i = 0.0  # Singleton cluster\n        else:\n            in_cluster_dists = dist_matrix[i, in_cluster_mask]\n            a_i = np.mean(in_cluster_dists)\n\n        # Inter-cluster distance b(i)\n        b_i = np.inf\n        other_labels = unique_labels[unique_labels != my_label]\n        for other_label in other_labels:\n            other_cluster_mask = (labels == other_label)\n            other_cluster_dists = dist_matrix[i, other_cluster_mask]\n            mean_dist = np.mean(other_cluster_dists)\n            if mean_dist < b_i:\n                b_i = mean_dist\n        \n        if max(a_i, b_i) == 0:\n            s_scores[i] = 0.0\n        else:\n            s_scores[i] = (b_i - a_i) / max(a_i, b_i)\n            \n    return np.mean(s_scores)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"T\": 24, \"N\": 90,\n            \"prototypes\": [1, 2, 3], \"n_per_proto\": [30, 30, 30],\n            \"noise_sigma\": 0.05, \"amp_range\": [0.8, 1.2],\n            \"seed\": 42, \"candidate_k\": [2, 3, 4, 5]\n        },\n        {\n            \"T\": 24, \"N\": 80,\n            \"prototypes\": [4, 5], \"n_per_proto\": [40, 40],\n            \"noise_sigma\": 0.08, \"amp_range\": [0.85, 1.15],\n            \"seed\": 123, \"candidate_k\": [2, 3, 4, 5]\n        },\n        {\n            \"T\": 24, \"N\": 4,\n            \"prototypes\": [6, 7], \"n_per_proto\": [2, 2],\n            \"noise_sigma\": 0.02, \"amp_range\": [0.9, 1.1],\n            \"seed\": 999, \"candidate_k\": [2, 3, 4]\n        }\n    ]\n\n    # Algorithmic parameters\n    I_MAX = 300\n    TOL = 1e-6\n    R = 10\n    DELTA = 1e-9\n\n    results = []\n    case_idx = 0\n    for case in test_cases:\n        case_idx += 1\n        data = generate_data(\n            T=case[\"T\"],\n            prototypes=case[\"prototypes\"],\n            n_per_proto=case[\"n_per_proto\"],\n            noise_sigma=case[\"noise_sigma\"],\n            amp_range=case[\"amp_range\"],\n            seed=case[\"seed\"]\n        )\n        \n        best_k = -1\n        max_silhouette = -np.inf\n        \n        # Use a consistent seed for k-means runs for reproducibility\n        kmeans_seed_base = case[\"seed\"]\n\n        for k in sorted(case[\"candidate_k\"]):\n            labels, _, _ = kmeans(\n                data=data, k=k, n_restarts=R,\n                max_iter=I_MAX, tol=TOL, seed=kmeans_seed_base\n            )\n            score = calculate_silhouette_score(data, labels)\n            \n            # Select k that maximizes silhouette score.\n            # Only update if score is significantly better.\n            # This preserves the smallest k in case of a tie.\n            if score > max_silhouette + DELTA:\n                max_silhouette = score\n                best_k = k\n        \n        results.append(best_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}