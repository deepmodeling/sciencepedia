{
    "hands_on_practices": [
        {
            "introduction": "The economic dispatch of generation is a cornerstone of power system operations, aiming to meet demand at the minimum possible cost. This exercise explores the mathematical properties of the generator cost function, specifically convexity, which is a critical feature that enables efficient and reliable optimization. By using the Hessian matrix to formally test for convexity, you will build a foundational understanding of why convex models are so desirable in energy systems analysis, as they guarantee that a locally optimal solution is also the global optimum .",
            "id": "4101437",
            "problem": "Consider a single thermal generator with real power output $p$ (in megawatts, MW) and a quadratic fuel cost function $c(p)=a p^{2}+b p$, where $a>0$ and $b\\in\\mathbb{R}$ are parameters. The generator participates in a market with a constant price $\\lambda$ (in dollars per megawatt-hour, $\\$\\text{/}\\text{MWh}$), and it chooses $p$ to maximize hourly profit $\\pi(p)=\\lambda p - c(p)$ subject to the engineering capacity constraint $0\\leq p\\leq P_{\\max}$. Assume the feasible set is closed and convex and the cost function is twice continuously differentiable.\n\n(a) Using the second-derivative characterization of convexity for twice-differentiable functions on $\\mathbb{R}$, determine from first principles whether $c(p)=a p^{2}+b p$ is convex by analyzing its Hessian. Then, explain how the convexity (or strict convexity) of $c(p)$ affects the uniqueness of the optimizer when maximizing $\\pi(p)$ over a convex feasible set.\n\n(b) For the specific parameter values $a=0.33$, $b=3.10$, $\\lambda=14.70$, and $P_{\\max}=50$, compute the unique profit-maximizing dispatch $p^{\\ast}$ that satisfies the first-order optimality conditions and the capacity constraint. Express your final numerical answer in megawatts (MW) and round to four significant figures.",
            "solution": "The problem is valid as it represents a standard, well-posed economic dispatch problem in energy systems, grounded in established principles of optimization and economics. All necessary data are provided, and the premises are scientifically sound and internally consistent.\n\nPart (a): Convexity Analysis and Uniqueness\n\nThe fuel cost function is given as $c(p) = a p^{2} + b p$ for $p \\in \\mathbb{R}$, with the parameter $a > 0$. The function is stated to be twice continuously differentiable. To determine its convexity, we analyze its Hessian matrix. For a scalar function of a single variable, the Hessian is a $1 \\times 1$ matrix containing the second derivative. A function $f(x)$ is convex if its Hessian is positive semi-definite and strictly convex if its Hessian is positive definite. For a single-variable function, this condition simplifies to its second derivative being non-negative ($f''(x) \\geq 0$) for convexity, and strictly positive ($f''(x) > 0$) for strict convexity.\n\nFirst, we compute the first and second derivatives of the cost function $c(p)$ with respect to $p$:\nThe first derivative, which represents the marginal cost, is:\n$$ c'(p) = \\frac{d}{dp}(a p^{2} + b p) = 2ap + b $$\nThe second derivative is:\n$$ c''(p) = \\frac{d}{dp}(2ap + b) = 2a $$\nThe Hessian of $c(p)$ is the $1 \\times 1$ matrix $[c''(p)] = [2a]$. A matrix is positive definite if all its leading principal minors are positive. For a $1 \\times 1$ matrix $[m]$, this condition is simply $m > 0$.\nGiven the problem statement specifies $a > 0$, the second derivative $c''(p) = 2a$ is a positive constant.\n$$ c''(p) = 2a > 0 $$\nSince the second derivative is strictly positive for all values of $p$, the cost function $c(p)$ is strictly convex.\n\nNow, we consider the effect of this on the uniqueness of the optimizer for the profit function $\\pi(p) = \\lambda p - c(p)$. The optimization problem is to maximize $\\pi(p)$ over the convex feasible set $[0, P_{\\max}]$. Maximizing $\\pi(p)$ is equivalent to minimizing its negative, let's call this function $f(p)$:\n$$ f(p) = -\\pi(p) = -(\\lambda p - c(p)) = c(p) - \\lambda p $$\nThe function $f(p)$ is the sum of a strictly convex function $c(p)$ and a linear function $-\\lambda p$. The sum of a strictly convex function and a linear function is also strictly convex. To verify, we can check the second derivative of $f(p)$:\n$$ f''(p) = \\frac{d^2}{dp^2}(c(p) - \\lambda p) = c''(p) - 0 = 2a $$\nSince $a > 0$, we have $f''(p) > 0$, which confirms that $f(p)$ is a strictly convex function.\n\nA fundamental theorem in optimization states that if a function is strictly convex, it can have at most one global minimizer. The optimization problem is to find a minimizer of the strictly convex function $f(p)$ over the feasible set $S = [0, P_{\\max}]$. The set $S$ is non-empty, closed, and convex. The existence of a minimizer is guaranteed by the Extreme Value Theorem, as $f(p)$ is continuous and the set $S$ is compact (closed and bounded). Because the function is strictly convex and a minimizer is known to exist, that minimizer must be unique. Therefore, there is a unique power output $p^{\\ast}$ that minimizes $f(p) = -\\pi(p)$, and thus uniquely maximizes the profit $\\pi(p)$.\n\nPart (b): Calculation of the Optimal Dispatch\n\nThe problem is to find $p^{\\ast}$ that solves:\n$$ \\max_{p} \\pi(p) = \\lambda p - (a p^{2} + b p) $$\nsubject to the constraint $0 \\leq p \\leq P_{\\max}$.\n\nThe profit function is $\\pi(p) = -a p^{2} + (\\lambda - b) p$. This is a quadratic function of $p$. Since $a > 0$, the coefficient of the $p^2$ term is negative, meaning the graph of $\\pi(p)$ is a downward-opening parabola. The global maximum of such a function, without constraints, occurs where its first derivative is zero.\n\nThe first-order condition for an unconstrained maximum is $\\pi'(p) = 0$:\n$$ \\frac{d\\pi}{dp} = \\frac{d}{dp}(-a p^{2} + (\\lambda - b) p) = -2ap + (\\lambda - b) = 0 $$\nSolving for the unconstrained optimal dispatch, which we denote as $p_{uc}$:\n$$ 2ap_{uc} = \\lambda - b $$\n$$ p_{uc} = \\frac{\\lambda - b}{2a} $$\nThe value $p_{uc}$ represents the vertex of the parabola. The optimal solution $p^{\\ast}$ for the constrained problem is found by considering the position of $p_{uc}$ relative to the feasible interval $[0, P_{\\max}]$.\n1. If $p_{uc}$ falls within the interval, i.e., $0 \\leq p_{uc} \\leq P_{\\max}$, then the constrained optimum is $p^{\\ast} = p_{uc}$.\n2. If $p_{uc} < 0$, the profit function is decreasing over the entire feasible interval $[0, P_{\\max}]$. The maximum profit is achieved at the lower bound, so $p^{\\ast} = 0$.\n3. If $p_{uc} > P_{\\max}$, the profit function is increasing over the entire feasible interval $[0, P_{\\max}]$. The maximum profit is achieved at the upper bound, so $p^{\\ast} = P_{\\max}$.\n\nThis can be concisely expressed as projecting $p_{uc}$ onto the feasible set: $p^{\\ast} = \\max(0, \\min(p_{uc}, P_{\\max}))$.\n\nNow we substitute the given numerical values: $a = 0.33$, $b = 3.10$, $\\lambda = 14.70$, and $P_{\\max} = 50$.\nFirst, calculate the unconstrained optimum $p_{uc}$:\n$$ p_{uc} = \\frac{14.70 - 3.10}{2 \\times 0.33} = \\frac{11.60}{0.66} $$\n$$ p_{uc} \\approx 17.5757... $$\nNext, we check this value against the constraints. The feasible interval is $[0, 50]$. Since $0 \\leq 17.5757... \\leq 50$, the unconstrained maximum lies within the feasible region.\nTherefore, the unique profit-maximizing dispatch is $p^{\\ast} = p_{uc}$.\n\nThe problem requires the final answer to be rounded to four significant figures.\n$$ p^{\\ast} = 17.5757... \\, \\text{MW} $$\nRounding to four significant figures, we get:\n$$ p^{\\ast} = 17.58 \\, \\text{MW} $$",
            "answer": "$$ \\boxed{17.58} $$"
        },
        {
            "introduction": "While the full Alternating Current (AC) power flow equations provide the most accurate representation of electricity transmission, their nonlinearity makes large-scale analysis computationally intensive. Consequently, linearized models like the Direct Current (DC) power flow are workhorses of energy system planning. This practice requires you to derive both the exact AC power flow and the approximate DC power flow models from first principles, illuminating the assumptions that enable this powerful simplification . By numerically comparing the models, you will gain crucial intuition about the trade-offs between model accuracy and computational tractability.",
            "id": "4101439",
            "problem": "Consider a single transmission line modeled as a lumped series element connecting bus $i$ to bus $j$ in an alternating current power system. Use the phasor form of Ohm’s law and the complex power definition as the fundamental base. Assume a steady-state sinusoidal regime, and model the line as purely reactive with series reactance $x > 0$ (that is, negligible resistance and shunt elements). Let the complex bus voltages be $V_i = |V_i| e^{\\mathbf{j} \\theta_i}$ and $V_j = |V_j| e^{\\mathbf{j} \\theta_j}$, where $\\mathbf{j} = \\sqrt{-1}$, $|V|$ is in per-unit (p.u.), and angles are in radians. You must:\n\n- Starting only from the phasor Ohm’s law $I_{ij} = y \\left(V_i - V_j\\right)$ and the complex power definition $S_{ij} = V_i I_{ij}^*$, derive the exact real power flow $P_{ij}$ from bus $i$ to bus $j$ for a purely reactive line with series reactance $x$.\n- From that exact expression, impose the standard Direct Current (DC) power flow linearization assumptions appropriate for transmission system studies, namely: (i) voltage magnitudes are close to unity, (ii) voltage angle differences are small, and (iii) line resistance is negligible compared to reactance. Derive the resulting linear estimate of real power flow.\n- Using both the exact (nonlinear) and the linearized (DC) expressions, compute the absolute discrepancy between the two real power flow predictions for each of the following test cases.\n\nAll quantities are to be expressed in per-unit (p.u.) on a $100$ megavolt-amperes base, and all angles are in radians. For each test case, the parameters are $(x,\\;|V_i|,\\;\\theta_i,\\;|V_j|,\\;\\theta_j)$:\n- Test case $1$: $(x = 0.2,\\; |V_i| = 0.95,\\; \\theta_i = 0.10,\\; |V_j| = 1.05,\\; \\theta_j = 0.02)$.\n- Test case $2$: $(x = 0.3,\\; |V_i| = 0.90,\\; \\theta_i = 0.00,\\; |V_j| = 1.10,\\; \\theta_j = 0.00)$.\n- Test case $3$: $(x = 0.1,\\; |V_i| = 0.88,\\; \\theta_i = 0.04,\\; |V_j| = 1.08,\\; \\theta_j = 0.00)$.\n- Test case $4$: $(x = 0.4,\\; |V_i| = 0.92,\\; \\theta_i = 0.25,\\; |V_j| = 1.08,\\; \\theta_j = -0.05)$.\n- Test case $5$: $(x = 0.25,\\; |V_i| = 1.10,\\; \\theta_i = -0.06,\\; |V_j| = 0.90,\\; \\theta_j = 0.06)$.\n\nFor each test case:\n- Compute the exact real power flow $P_{ij}^{\\mathrm{AC}}$ using the derived nonlinear expression.\n- Compute the DC-approximated real power flow $P_{ij}^{\\mathrm{DC}}$ using the derived linear expression.\n- Compute the absolute discrepancy $\\Delta = \\left| P_{ij}^{\\mathrm{AC}} - P_{ij}^{\\mathrm{DC}} \\right|$ in p.u.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results for all five test cases as a comma-separated list enclosed in square brackets, with each $\\Delta$ rounded to six decimal places (for example, $[0.123456,0.000001,0.987650,0.100000,0.000000]$).",
            "solution": "The problem statement has been validated and is deemed sound. It is a well-posed, scientifically grounded problem in electrical power system analysis that requires the derivation and comparison of the exact AC power flow equation and its linearized DC approximation for a lossless transmission line. All necessary data and fundamental equations are provided, and the objective is clear and formalizable.\n\nThe solution proceeds in three steps: first, the derivation of the exact, nonlinear expression for real power flow ($P_{ij}^{\\mathrm{AC}}$); second, the derivation of the linearized, approximate expression for real power flow ($P_{ij}^{\\mathrm{DC}}$) by applying standard simplifying assumptions; and third, the computation of the absolute discrepancy between these two models for the provided test cases.\n\n**1. Derivation of the Exact Real Power Flow ($P_{ij}^{\\mathrm{AC}}$)**\n\nThe analysis begins with the fundamental definitions for complex power and current in an AC circuit. The complex power flowing from bus $i$ towards bus $j$, denoted $S_{ij}$, is defined as:\n$$S_{ij} = P_{ij} + \\mathbf{j}Q_{ij} = V_i I_{ij}^*$$\nwhere $P_{ij}$ is the real power, $Q_{ij}$ is the reactive power, $V_i$ is the complex voltage at bus $i$, and $I_{ij}^*$ is the complex conjugate of the current $I_{ij}$ flowing from bus $i$ to bus $j$. The imaginary unit is $\\mathbf{j} = \\sqrt{-1}$.\n\nThe current $I_{ij}$ is given by the phasor form of Ohm's law:\n$$I_{ij} = Y_{ij}(V_i - V_j)$$\nwhere $Y_{ij}$ is the series admittance of the line connecting the buses. The problem specifies a purely reactive line with series reactance $x$. The series impedance is therefore $Z_{ij} = \\mathbf{j}x$. The admittance is the reciprocal of the impedance:\n$$Y_{ij} = \\frac{1}{Z_{ij}} = \\frac{1}{\\mathbf{j}x} = -\\frac{\\mathbf{j}}{x}$$\n\nSubstituting this admittance into the current equation gives:\n$$I_{ij} = -\\frac{\\mathbf{j}}{x}(V_i - V_j)$$\n\nNext, we find the complex conjugate of the current, $I_{ij}^*$:\n$$I_{ij}^* = \\left(-\\frac{\\mathbf{j}}{x}(V_i - V_j)\\right)^* = \\left(-\\frac{\\mathbf{j}}{x}\\right)^* (V_i - V_j)^* = \\frac{\\mathbf{j}}{x}(V_i^* - V_j^*)$$\n\nNow, we substitute $I_{ij}^*$ into the complex power equation:\n$$S_{ij} = V_i \\left(\\frac{\\mathbf{j}}{x}(V_i^* - V_j^*)\\right) = \\frac{\\mathbf{j}}{x}(V_i V_i^* - V_i V_j^*)$$\n\nThe product of a complex number and its conjugate is the square of its magnitude, so $V_i V_i^* = |V_i|^2$. The complex voltages are given in polar form as $V_i = |V_i| e^{\\mathbf{j}\\theta_i}$ and $V_j = |V_j| e^{\\mathbf{j}\\theta_j}$. The term $V_i V_j^*$ can be expressed as:\n$$V_i V_j^* = \\left(|V_i| e^{\\mathbf{j}\\theta_i}\\right) \\left(|V_j| e^{-\\mathbf{j}\\theta_j}\\right) = |V_i||V_j| e^{\\mathbf{j}(\\theta_i - \\theta_j)}$$\n\nUsing Euler's formula, $e^{\\mathbf{j}\\phi} = \\cos(\\phi) + \\mathbf{j}\\sin(\\phi)$, we expand this term:\n$$V_i V_j^* = |V_i||V_j| \\left(\\cos(\\theta_i - \\theta_j) + \\mathbf{j}\\sin(\\theta_i - \\theta_j)\\right)$$\n\nSubstituting these expressions back into the equation for $S_{ij}$:\n$$S_{ij} = \\frac{\\mathbf{j}}{x} \\left( |V_i|^2 - |V_i||V_j| \\left(\\cos(\\theta_i - \\theta_j) + \\mathbf{j}\\sin(\\theta_i - \\theta_j)\\right) \\right)$$\n$$S_{ij} = \\frac{\\mathbf{j}}{x}|V_i|^2 - \\frac{\\mathbf{j}}{x}|V_i||V_j|\\cos(\\theta_i - \\theta_j) - \\frac{\\mathbf{j}^2}{x}|V_i||V_j|\\sin(\\theta_i - \\theta_j)$$\n\nSince $\\mathbf{j}^2 = -1$, the equation simplifies to:\n$$S_{ij} = \\frac{|V_i||V_j|}{x}\\sin(\\theta_i - \\theta_j) + \\mathbf{j}\\left(\\frac{|V_i|^2}{x} - \\frac{|V_i||V_j|}{x}\\cos(\\theta_i - \\theta_j)\\right)$$\n\nThe real power flow, $P_{ij}^{\\mathrm{AC}}$, is the real part of $S_{ij}$.\n$$P_{ij}^{\\mathrm{AC}} = \\mathrm{Re}\\{S_{ij}\\} = \\frac{|V_i||V_j|}{x}\\sin(\\theta_i - \\theta_j)$$\nThis is the exact, nonlinear expression for real power flow over a lossless transmission line.\n\n**2. Derivation of the Linearized DC Power Flow ($P_{ij}^{\\mathrm{DC}}$)**\n\nThe DC power flow model is a linearization of the AC power flow equations based on a set of standard assumptions for high-voltage transmission systems operating under normal conditions. The assumptions are:\n(i) Voltage magnitudes are approximately unity in per-unit: $|V_i| \\approx 1$ and $|V_j| \\approx 1$.\n(ii) Voltage angle differences between connected buses are small: $(\\theta_i - \\theta_j)$ is small, which allows the approximation $\\sin(\\theta_i - \\theta_j) \\approx (\\theta_i - \\theta_j)$ and $\\cos(\\theta_i - \\theta_j) \\approx 1$. The angles must be in radians for this approximation to be valid.\n(iii) Line resistance is negligible compared to reactance ($r \\ll x$), which was already incorporated into our model by setting the line impedance to $Z_{ij} = \\mathbf{j}x$.\n\nApplying assumptions (i) and (ii) to the exact real power flow equation:\n$$P_{ij}^{\\mathrm{AC}} = \\frac{|V_i||V_j|}{x}\\sin(\\theta_i - \\theta_j)$$\n$$P_{ij}^{\\mathrm{DC}} \\approx \\frac{(1)(1)}{x}(\\theta_i - \\theta_j)$$\nThis yields the linearized DC power flow equation:\n$$P_{ij}^{\\mathrm{DC}} = \\frac{\\theta_i - \\theta_j}{x}$$\nThis expression is linear in the state variables $\\theta_i$ and $\\theta_j$.\n\n**3. Computation of Absolute Discrepancy**\n\nFor each test case, we will compute the absolute discrepancy $\\Delta$ between the exact AC power flow and the approximate DC power flow:\n$$\\Delta = \\left| P_{ij}^{\\mathrm{AC}} - P_{ij}^{\\mathrm{DC}} \\right| = \\left| \\frac{|V_i||V_j|}{x}\\sin(\\theta_i - \\theta_j) - \\frac{\\theta_i - \\theta_j}{x} \\right|$$\nThe calculations will be performed for each set of parameters provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the absolute discrepancy between the exact AC and the linearized DC\n    real power flow models for a lossless transmission line.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple is in the format: (x, |V_i|, theta_i, |V_j|, theta_j)\n    # where x is reactance in p.u., |V| are voltage magnitudes in p.u.,\n    # and theta are voltage angles in radians.\n    test_cases = [\n        (0.2, 0.95, 0.10, 1.05, 0.02),\n        (0.3, 0.90, 0.00, 1.10, 0.00),\n        (0.1, 0.88, 0.04, 1.08, 0.00),\n        (0.4, 0.92, 0.25, 1.08, -0.05),\n        (0.25, 1.10, -0.06, 0.90, 0.06),\n    ]\n\n    results = []\n    for case in test_cases:\n        x, vi_mag, theta_i, vj_mag, theta_j = case\n        \n        # Calculate the angle difference\n        theta_diff = theta_i - theta_j\n        \n        # 1. Compute the exact real power flow (P_ac)\n        # P_ac = (|V_i| * |V_j| / x) * sin(theta_i - theta_j)\n        p_ac = (vi_mag * vj_mag / x) * np.sin(theta_diff)\n        \n        # 2. Compute the DC-approximated real power flow (P_dc)\n        # P_dc = (theta_i - theta_j) / x\n        p_dc = theta_diff / x\n        \n        # 3. Compute the absolute discrepancy\n        discrepancy = abs(p_ac - p_dc)\n        \n        results.append(discrepancy)\n\n    # Format the results to six decimal places as strings\n    formatted_results = [f\"{res:.6f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Simple convex cost functions are useful approximations, but real-world thermal generators exhibit nonconvex behaviors due to physical characteristics like valve-point loading. This practice moves into the challenging domain of nonconvex optimization, where multiple local minima can exist, and standard optimization algorithms may fail to find the true, lowest-cost solution. By implementing a gradient-based solver on a dispatch problem with a nonconvex cost curve, you will directly experience how such methods can converge to suboptimal solutions depending on their starting point . This exercise underscores the critical importance of understanding model structure and the potential need for advanced global optimization techniques in realistic energy system models.",
            "id": "4101426",
            "problem": "Consider a two-generator economic dispatch problem from energy systems modeling, where generation must balance demand and operating costs are nonconvex due to valve-point effects. Let generator $i \\in \\{1,2\\}$ produce power $p_i$ with bounds $p_i \\in [\\underline{p}_i,\\overline{p}_i]$, and total demand $D$ must be met by supply such that $p_1 + p_2 = D$. Each generator has a nonconvex smooth cost due to sinusoidal ripples superimposed on a convex quadratic heat-rate curve. The objective is to minimize total operating cost over feasible power outputs.\n\nFundamental base:\n- Economic dispatch enforces the power balance $p_1 + p_2 = D$ and box constraints $p_i \\in [\\underline{p}_i,\\overline{p}_i]$.\n- Costs are additive across generators, and physical valves in thermal generators can introduce ripple-like nonconvexities in the cost curve, modeled by sinusoidal terms. Sinusoidal functions are evaluated in radians.\n\nDefine the following specific instance:\n- Demand: $D = 180$.\n- Bounds: $\\underline{p}_1 = 10$, $\\overline{p}_1 = 120$, $\\underline{p}_2 = 20$, $\\overline{p}_2 = 200$.\n- Costs:\n$$C_1(p_1) = a_1 p_1^2 + b_1 p_1 + c_1 + d_1 \\sin(\\omega_1 p_1),$$\n$$C_2(p_2) = a_2 p_2^2 + b_2 p_2 + c_2 + d_2 \\sin(\\omega_2 p_2 + \\phi),$$\nwith parameters $a_1 = 2 \\times 10^{-3}$, $b_1 = 4$, $c_1 = 100$, $d_1 = 15$, $\\omega_1 = 0.25$, and $a_2 = 1.5 \\times 10^{-3}$, $b_2 = 3.5$, $c_2 = 80$, $d_2 = 12$, $\\omega_2 = 0.33$, $\\phi = 0.5$. All angles in the sine functions must be treated in radians. The global optimization target is to minimize $C_1(p_1) + C_2(p_2)$ subject to $p_1 + p_2 = D$ and the box constraints.\n\nTo implement a gradient-based approach that can fail on nonconvex problems, relax the equality constraint by adding a quadratic penalty with coefficient $\\mu$, forming the unconstrained penalized objective\n$$F(p_1,p_2) = C_1(p_1) + C_2(p_2) + \\mu \\, (p_1 + p_2 - D)^2,$$\nwith $\\mu = 200$. A projected gradient descent with backtracking line search applied to $F$ and projection onto the box constraints $[\\underline{p}_1,\\overline{p}_1] \\times [\\underline{p}_2,\\overline{p}_2]$ may converge to different local minima depending on initialization.\n\nTasks:\n1. Compute a numerically reliable approximation of the global minimum of the constrained problem by reducing to one dimension via $p_2 = D - p_1$ and performing a sufficiently fine search over the feasible interval for $p_1$ (only consider $p_1$ values for which $p_2$ is within its bounds). Return the minimum value of $C_1(p_1) + C_2(D - p_1)$, and the corresponding $(p_1^\\star,p_2^\\star)$.\n2. Implement a projected gradient descent with backtracking line search on $F(p_1,p_2)$, using only local gradient information and projection onto the box $[\\underline{p}_1,\\overline{p}_1] \\times [\\underline{p}_2,\\overline{p}_2]$ after each step. After the algorithm terminates, map the final iterate to the feasible manifold by setting $p_1$ to the nearest value in the feasible interval for $p_1$ implied by $p_2 = D - p_1$, and set $p_2 = D - p_1$ to exactly enforce power balance. Evaluate the true constrained cost $C_1(p_1) + C_2(p_2)$ at this mapped point.\n3. For each specified initialization, determine whether the gradient method reached the global minimum by comparing the true constrained cost at the mapped point to the global best cost from step $1$ within tolerance $\\epsilon = 10^{-4}$ (declare success if the absolute difference is less than or equal to $\\epsilon$). All sine arguments must use radians.\n\nAngle unit specification: All angles in the sine functions must be in radians.\n\nNo physical unit is required in the final numeric outputs.\n\nTest suite initializations for projected gradient descent:\n- Initialization $1$: $(p_1^{(0)},p_2^{(0)}) = (10, 170)$.\n- Initialization $2$: $(p_1^{(0)},p_2^{(0)}) = (120, 60)$.\n- Initialization $3$: $(p_1^{(0)},p_2^{(0)}) = (80, 100)$.\n- Initialization $4$: $(p_1^{(0)},p_2^{(0)}) = (30, 150)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite initializations, where each entry is a boolean indicating whether the gradient method reached the global minimum within tolerance $\\epsilon$. For example, the output must look like $[\\text{True},\\text{False},\\text{False},\\text{True}]$ when appropriate.",
            "solution": "The problem is a nonconvex optimization problem that aims to find the minimum cost dispatch for two power generators. We will first find a high-fidelity approximation of the global minimum by reducing the problem to a single dimension and performing a fine-grained search. Then, for several initial points, we will run a projected gradient descent algorithm on a penalized version of the objective function to find local minima. Finally, we compare the costs found by the gradient-based method to the global minimum to determine its success in each case.\n\n**Part 1: Global Minimum Approximation**\n\nThe objective is to minimize the total cost $C_{total}(p_1, p_2) = C_1(p_1) + C_2(p_2)$ subject to $p_1 + p_2 = D$ and individual generator limits. We can substitute the power balance constraint into the objective function to reduce the problem to a single variable, $p_1$.\nLet $p_2 = D - p_1 = 180 - p_1$. The total cost becomes a function of $p_1$ alone:\n$$C(p_1) = C_1(p_1) + C_2(180 - p_1)$$\nThe feasible range for $p_1$ must satisfy both its own bounds and the implied bounds on $p_2$:\n1.  $p_1 \\in [\\underline{p}_1, \\overline{p}_1] \\implies p_1 \\in [10, 120]$.\n2.  $p_2 \\in [\\underline{p}_2, \\overline{p}_2] \\implies 20 \\le 180 - p_1 \\le 200$.\nThe second condition gives $180 - 200 \\le p_1 \\le 180 - 20$, which simplifies to $-20 \\le p_1 \\le 160$.\nIntersecting these constraints, we find the feasible domain for $p_1$ is $[\\max(10, -20), \\min(120, 160)] = [10, 120]$.\n\nTo find the global minimum, we perform a fine-grained search over the interval $p_1 \\in [10, 120]$. By discretizing this interval into a large number of points and evaluating $C(p_1)$ at each point, we can find a reliable numerical approximation of the global minimum cost, which we denote $C^\\star$.\n\n**Part 2: Projected Gradient Descent**\n\nThis task uses a penalty method to handle the equality constraint $p_1 + p_2 = D$. The unconstrained (but still box-constrained) objective function is:\n$$F(p_1, p_2) = C_1(p_1) + C_2(p_2) + \\mu (p_1 + p_2 - D)^2$$\nwhere $\\mu=200$. We apply a projected gradient descent method. At each iteration $k$, the update is:\n$\\mathbf{p}^{(k+1)} = \\Pi_{[\\underline{\\mathbf{p}}, \\overline{\\mathbf{p}}]}(\\mathbf{p}^{(k)} - \\alpha_k \\nabla F(\\mathbf{p}^{(k)}))$, where $\\mathbf{p} = (p_1, p_2)$ and $\\Pi$ is the projection operator onto the box $[\\underline{p}_1, \\overline{p}_1] \\times [\\underline{p}_2, \\overline{p}_2]$.\n\nThe gradient of $F$ is $\\nabla F(\\mathbf{p}) = [\\frac{\\partial F}{\\partial p_1}, \\frac{\\partial F}{\\partial p_2}]^T$, with components:\n$$\\frac{\\partial F}{\\partial p_1} = \\frac{dC_1}{dp_1} + 2\\mu(p_1 + p_2 - D) = (2a_1 p_1 + b_1 + d_1 \\omega_1 \\cos(\\omega_1 p_1)) + 2\\mu(p_1 + p_2 - D)$$\n$$\\frac{\\partial F}{\\partial p_2} = \\frac{dC_2}{dp_2} + 2\\mu(p_1 + p_2 - D) = (2a_2 p_2 + b_2 + d_2 \\omega_2 \\cos(\\omega_2 p_2 + \\phi)) + 2\\mu(p_1 + p_2 - D)$$\nThe step size $\\alpha_k$ is determined by a backtracking line search, which ensures sufficient decrease in the objective function $F$ at each step.\n\nAfter the algorithm converges to a point $\\mathbf{p}_{final}$, this point is mapped back to the feasible set of the original problem. This is done by first projecting $p_{1, final}$ onto its feasible interval $[10, 120]$ to get $p_{1, mapped}$, and then setting $p_{2, mapped} = D - p_{1, mapped}$ to enforce the power balance constraint. The true cost $C_{local} = C_1(p_{1, mapped}) + C_2(p_{2, mapped})$ is then calculated.\n\n**Part 3: Comparison**\n\nFor each of the four initializations, we run the projected gradient descent algorithm to find a solution and its corresponding cost $C_{local}$. We then compare this cost to the global minimum $C^\\star$ found in Part 1. The gradient method is declared successful if $|C_{local} - C^\\star| \\le \\epsilon$, where $\\epsilon = 10^{-4}$. The final output will be a list of booleans indicating success for each initialization.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the two-generator economic dispatch problem.\n\n    The solution involves three main steps:\n    1. Find a reliable approximation of the global minimum cost by reformulating the\n       problem in one dimension and performing a fine-grained grid search.\n    2. For each specified initial point, run a projected gradient descent algorithm\n       on a penalized version of the objective function to find a local minimum.\n    3. Compare the cost found by the gradient method with the global minimum\n       to determine if the global optimum was reached within a given tolerance.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    D = 180.0\n    p1_min, p1_max = 10.0, 120.0\n    p2_min, p2_max = 20.0, 200.0\n\n    a1, b1, c1, d1, omega1 = 2e-3, 4.0, 100.0, 15.0, 0.25\n    a2, b2, c2, d2, omega2, phi = 1.5e-3, 3.5, 80.0, 12.0, 0.33, 0.5\n\n    mu = 200.0\n    epsilon = 1e-4\n\n    # --- Helper Functions ---\n\n    def c1_func(p1):\n        \"\"\"Cost of generator 1.\"\"\"\n        return a1 * p1**2 + b1 * p1 + c1 + d1 * np.sin(omega1 * p1)\n\n    def c2_func(p2):\n        \"\"\"Cost of generator 2.\"\"\"\n        return a2 * p2**2 + b2 * p2 + c2 + d2 * np.sin(omega2 * p2 + phi)\n\n    def grad_c1(p1):\n        \"\"\"Gradient of C1.\"\"\"\n        return 2 * a1 * p1 + b1 + d1 * omega1 * np.cos(omega1 * p1)\n\n    def grad_c2(p2):\n        \"\"\"Gradient of C2.\"\"\"\n        return 2 * a2 * p2 + b2 + d2 * omega2 * np.cos(omega2 * p2 + phi)\n\n    def F_penalized(p):\n        \"\"\"Penalized objective function F(p1, p2).\"\"\"\n        p1, p2 = p\n        cost = c1_func(p1) + c2_func(p2)\n        penalty = mu * (p1 + p2 - D)**2\n        return cost + penalty\n\n    def grad_F(p):\n        \"\"\"Gradient of the penalized objective F.\"\"\"\n        p1, p2 = p\n        penalty_grad_term = 2 * mu * (p1 + p2 - D)\n        grad_p1 = grad_c1(p1) + penalty_grad_term\n        grad_p2 = grad_c2(p2) + penalty_grad_term\n        return np.array([grad_p1, grad_p2])\n\n    def project(p):\n        \"\"\"Project p onto the box constraints.\"\"\"\n        p1, p2 = p\n        p1_proj = np.clip(p1, p1_min, p1_max)\n        p2_proj = np.clip(p2, p2_min, p2_max)\n        return np.array([p1_proj, p2_proj])\n\n    # --- Task 1: Find Global Minimum ---\n    p1_feasible_min = max(p1_min, D - p2_max)\n    p1_feasible_max = min(p1_max, D - p2_min)\n    \n    # Fine-grained search over the feasible 1D interval for p1\n    num_points = 200000\n    p1_grid = np.linspace(p1_feasible_min, p1_feasible_max, num_points)\n    p2_grid = D - p1_grid\n    \n    total_cost_grid = c1_func(p1_grid) + c2_func(p2_grid)\n    global_min_cost = np.min(total_cost_grid)\n\n    # --- Task 2 & 3: Projected Gradient Descent and Comparison ---\n    initializations = [\n        np.array([10.0, 170.0]),\n        np.array([120.0, 60.0]),\n        np.array([80.0, 100.0]),\n        np.array([30.0, 150.0]),\n    ]\n    \n    results = []\n\n    for p_init in initializations:\n        p_curr = np.copy(p_init)\n\n        # PGD algorithm parameters\n        max_iter = 2000\n        tolerance = 1e-8\n        c_bt = 0.5  # Armijo condition constant\n        tau_bt = 0.8 # Backtracking step size reduction factor\n\n        for _ in range(max_iter):\n            grad = grad_F(p_curr)\n            dk = -grad\n            \n            # Backtracking line search\n            alpha = 1.0\n            f_curr = F_penalized(p_curr)\n            grad_dot_d = np.dot(grad, dk)\n            \n            while F_penalized(p_curr + alpha * dk) > f_curr + c_bt * alpha * grad_dot_d:\n                alpha *= tau_bt\n            \n            p_next_unproj = p_curr + alpha * dk\n            p_next = project(p_next_unproj)\n            \n            if np.linalg.norm(p_next - p_curr) < tolerance:\n                p_curr = p_next\n                break\n            \n            p_curr = p_next\n\n        # Map the final PGD iterate to the feasible manifold of the original problem\n        p_final_pgd = p_curr\n        p1_mapped = np.clip(p_final_pgd[0], p1_feasible_min, p1_feasible_max)\n        p2_mapped = D - p1_mapped\n        \n        # Calculate the true constrained cost at the mapped point\n        cost_pgd = c1_func(p1_mapped) + c2_func(p2_mapped)\n        \n        # Compare with the global minimum\n        is_global = abs(cost_pgd - global_min_cost) <= epsilon\n        results.append(is_global)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}