{
    "hands_on_practices": [
        {
            "introduction": "Before attempting to calibrate a complex model, it is essential to understand which parameters have a significant impact on its output. This practice explores local sensitivity, which measures how the model output changes in response to small perturbations in each parameter. By quantifying these sensitivities for a heat pump performance model, you can predict which parameters will be identifiable from experimental data, thereby guiding experimental design and model simplification. ",
            "id": "4073866",
            "problem": "A single-stage vapor-compression heat pump is characterized at steady state by a parametric performance map that predicts its heating capacity as a function of operating conditions. Let the operating-point vector be $x_{t} = (T_{h}, T_{c}, f, m)$, where $T_{h}$ is the condenser refrigerant temperature in Kelvin, $T_{c}$ is the evaporator refrigerant temperature in Kelvin, $f$ is the compressor electrical frequency in hertz, and $m$ is the refrigerant mass flow rate in kilograms per second. The model for the heating capacity is\n$$\nf_{\\theta}(x) \\;=\\; \\theta_{1} \\;+\\; \\theta_{2}\\,(T_{h} - T_{c}) \\;+\\; \\theta_{3}\\,\\ln\\!\\left(\\frac{T_{h}}{T_{c}}\\right) \\;+\\; \\theta_{4}\\,\\frac{f}{f + \\theta_{5}} \\;+\\; \\theta_{6}\\,\\exp\\!\\left(-\\frac{\\theta_{7}}{m}\\right),\n$$\nwhere $\\theta = (\\theta_{1}, \\theta_{2}, \\theta_{3}, \\theta_{4}, \\theta_{5}, \\theta_{6}, \\theta_{7})$ is the vector of unknown parameters, and $f_{\\theta}(x)$ is expressed in kilowatts. Assume the local operating point is $T_{h} = 315 \\ \\mathrm{K}$, $T_{c} = 275 \\ \\mathrm{K}$, $f = 50 \\ \\mathrm{Hz}$, and $m = 0.10 \\ \\mathrm{kg \\ s^{-1}}$. The current parameter estimate is $\\theta_{1} = 3.0 \\ \\mathrm{kW}$, $\\theta_{2} = 0.12 \\ \\mathrm{kW \\ K^{-1}}$, $\\theta_{3} = 1.5 \\ \\mathrm{kW}$, $\\theta_{4} = 2.0 \\ \\mathrm{kW}$, $\\theta_{5} = 60 \\ \\mathrm{Hz}$, $\\theta_{6} = 1.2 \\ \\mathrm{kW}$, and $\\theta_{7} = 0.05 \\ \\mathrm{kg \\ s^{-1}}$.\n\nStarting from first principles—namely, the definition of local sensitivity as the partial derivative of the output with respect to a parameter, and the role of sensitivity in parameter estimation under an additive measurement-noise model—do the following:\n\n- Compute the local sensitivities $\\frac{\\partial f_{\\theta}(x_{t})}{\\partial \\theta_{i}}$ for $i \\in \\{1,2,3,4,5,6,7\\}$ at the operating point $x_{t}$.\n- Form the dimensionless, parameter-scaled sensitivity magnitudes\n$$\n\\tilde{s}_{i} \\;=\\; \\left| \\frac{\\partial f_{\\theta}(x_{t})}{\\partial \\theta_{i}} \\cdot \\frac{\\theta_{i}}{f_{\\theta}(x_{t})} \\right|\n$$\nfor $i \\in \\{1,2,3,4,5,6,7\\}$, and then compute the infinity-norm of the scaled sensitivity vector, $\\|\\tilde{s}\\|_{\\infty} = \\max_{i} \\tilde{s}_{i}$. Round your final reported infinity-norm to four significant figures and express it as a pure number (dimensionless).\n- Explain qualitatively, using the framework of the Fisher Information Matrix (FIM) for Gaussian measurement noise, how the magnitudes of these scaled sensitivities guide which parameters can be reliably calibrated at $x_{t}$ and which require additional excitation or experimental design.\n\nYour final answer must be the single numerical value of $\\|\\tilde{s}\\|_{\\infty}$, rounded to four significant figures, expressed as a pure number with no units.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a complete solution. There are no contradictions, ambiguities, or unsound premises. We may proceed with the solution.\n\nThe problem requires a three-part analysis:\n1.  Derivation and computation of local sensitivities of a heat pump model output with respect to its parameters.\n2.  Calculation of the dimensionless, parameter-scaled sensitivity magnitudes and their infinity-norm.\n3.  A qualitative explanation of the role of these sensitivities in parameter identifiability, framed by the Fisher Information Matrix (FIM).\n\nThe model for the heating capacity, $f_{\\theta}(x)$, is given by:\n$$f_{\\theta}(x) \\;=\\; \\theta_{1} \\;+\\; \\theta_{2}\\,(T_{h} - T_{c}) \\;+\\; \\theta_{3}\\,\\ln\\!\\left(\\frac{T_{h}}{T_{c}}\\right) \\;+\\; \\theta_{4}\\,\\frac{f}{f + \\theta_{5}} \\;+\\; \\theta_{6}\\,\\exp\\!\\left(-\\frac{\\theta_{7}}{m}\\right)$$\nThe operating point is $x_{t} = (T_{h}, T_{c}, f, m)$ with $T_{h} = 315 \\ \\mathrm{K}$, $T_{c} = 275 \\ \\mathrm{K}$, $f = 50 \\ \\mathrm{Hz}$, and $m = 0.10 \\ \\mathrm{kg \\ s^{-1}}$.\nThe parameter vector is $\\theta = (\\theta_{1}, \\theta_{2}, \\theta_{3}, \\theta_{4}, \\theta_{5}, \\theta_{6}, \\theta_{7})$ with values $\\theta_1 = 3.0 \\ \\mathrm{kW}$, $\\theta_2 = 0.12 \\ \\mathrm{kW \\ K^{-1}}$, $\\theta_3 = 1.5 \\ \\mathrm{kW}$, $\\theta_4 = 2.0 \\ \\mathrm{kW}$, $\\theta_5 = 60 \\ \\mathrm{Hz}$, $\\theta_6 = 1.2 \\ \\mathrm{kW}$, and $\\theta_7 = 0.05 \\ \\mathrm{kg \\ s^{-1}}$.\n\nFirst, we compute the local sensitivities, which are the partial derivatives of the model output $f_{\\theta}(x)$ with respect to each parameter $\\theta_{i}$, evaluated at the given operating point $x_t$ and parameter estimate $\\theta$.\n\nThe partial derivatives are:\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{1}} = 1 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{2}} = T_{h} - T_{c} $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{3}} = \\ln\\left(\\frac{T_{h}}{T_{c}}\\right) $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{4}} = \\frac{f}{f + \\theta_{5}} $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{5}} = \\frac{\\partial}{\\partial \\theta_{5}} \\left( \\theta_{4} f (f + \\theta_{5})^{-1} \\right) = - \\frac{\\theta_{4} f}{(f + \\theta_{5})^{2}} $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{6}} = \\exp\\left(-\\frac{\\theta_{7}}{m}\\right) $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{7}} = \\frac{\\partial}{\\partial \\theta_{7}} \\left( \\theta_{6} \\exp\\left(-\\frac{\\theta_{7}}{m}\\right) \\right) = \\theta_{6} \\exp\\left(-\\frac{\\theta_{7}}{m}\\right) \\left(-\\frac{1}{m}\\right) = -\\frac{\\theta_{6}}{m} \\exp\\left(-\\frac{\\theta_{7}}{m}\\right) $$\n\nNext, we evaluate these derivatives at the specified point:\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{1}} = 1 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{2}} = 315 - 275 = 40 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{3}} = \\ln\\left(\\frac{315}{275}\\right) \\approx 0.135760 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{4}} = \\frac{50}{50 + 60} = \\frac{50}{110} = \\frac{5}{11} \\approx 0.454545 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{5}} = - \\frac{2.0 \\cdot 50}{(50 + 60)^{2}} = - \\frac{100}{110^{2}} = - \\frac{100}{12100} = -\\frac{1}{121} \\approx -0.00826446 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{6}} = \\exp\\left(-\\frac{0.05}{0.10}\\right) = \\exp(-0.5) \\approx 0.606531 $$\n$$ \\frac{\\partial f_{\\theta}}{\\partial \\theta_{7}} = -\\frac{1.2}{0.10} \\exp(-0.5) = -12 \\exp(-0.5) \\approx -7.278368 $$\n\nTo compute the dimensionless, scaled sensitivities $\\tilde{s}_{i}$, we first need the value of the model output $f_{\\theta}(x_t)$:\n$$ f_{\\theta}(x_t) = 3.0 + 0.12(315 - 275) + 1.5\\ln\\left(\\frac{315}{275}\\right) + 2.0\\left(\\frac{50}{50+60}\\right) + 1.2\\exp\\left(-\\frac{0.05}{0.1}\\right) $$\n$$ f_{\\theta}(x_t) \\approx 3.0 + 0.12(40) + 1.5(0.135760) + 2.0(0.454545) + 1.2(0.606531) $$\n$$ f_{\\theta}(x_t) \\approx 3.0 + 4.8 + 0.203640 + 0.909091 + 0.727837 $$\n$$ f_{\\theta}(x_t) \\approx 9.640568 \\ \\mathrm{kW} $$\n\nNow, we compute each scaled sensitivity $\\tilde{s}_{i} = \\left| \\frac{\\partial f_{\\theta}(x_{t})}{\\partial \\theta_{i}} \\cdot \\frac{\\theta_{i}}{f_{\\theta}(x_{t})} \\right|$:\n$$ \\tilde{s}_{1} = \\left| 1 \\cdot \\frac{3.0}{9.640568} \\right| \\approx 0.311183 $$\n$$ \\tilde{s}_{2} = \\left| 40 \\cdot \\frac{0.12}{9.640568} \\right| = \\left| \\frac{4.8}{9.640568} \\right| \\approx 0.497893 $$\n$$ \\tilde{s}_{3} = \\left| 0.135760 \\cdot \\frac{1.5}{9.640568} \\right| \\approx 0.021123 $$\n$$ \\tilde{s}_{4} = \\left| \\frac{5}{11} \\cdot \\frac{2.0}{9.640568} \\right| \\approx 0.094298 $$\n$$ \\tilde{s}_{5} = \\left| -\\frac{1}{121} \\cdot \\frac{60}{9.640568} \\right| \\approx |-0.00826446 \\cdot 6.22361| \\approx 0.051435 $$\n$$ \\tilde{s}_{6} = \\left| \\exp(-0.5) \\cdot \\frac{1.2}{9.640568} \\right| \\approx |0.606531 \\cdot 0.124474| \\approx 0.075498 $$\n$$ \\tilde{s}_{7} = \\left| -12 \\exp(-0.5) \\cdot \\frac{0.05}{9.640568} \\right| \\approx |-7.278368 \\cdot 0.0051864| \\approx 0.037749 $$\n\nThe vector of scaled sensitivity magnitudes is $\\tilde{s} \\approx (0.3112, 0.4979, 0.0211, 0.0943, 0.0514, 0.0755, 0.0377)$.\nThe infinity-norm of this vector is the maximum value among its components:\n$$ \\|\\tilde{s}\\|_{\\infty} = \\max_{i} \\tilde{s}_{i} = \\max\\{0.311183, 0.497893, 0.021123, 0.094298, 0.051435, 0.075498, 0.037749\\} $$\n$$ \\|\\tilde{s}\\|_{\\infty} = \\tilde{s}_{2} \\approx 0.497893 $$\nRounding to four significant figures, we get $\\|\\tilde{s}\\|_{\\infty} = 0.4979$.\n\nFinally, we address the qualitative role of these sensitivities. The framework begins with an additive noise model for measurements of the heating capacity, $y_t = f_{\\theta}(x_t) + \\epsilon_t$, where $\\epsilon_t$ is typically assumed to be i.i.d. Gaussian noise with zero mean and variance $\\sigma^2$, i.e., $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$.\nThe Fisher Information Matrix (FIM) is a fundamental concept in parameter estimation theory. For a single data point $y_t$, its elements are given by:\n$$ \\mathcal{I}_{ij}(\\theta) = \\frac{1}{\\sigma^2} \\frac{\\partial f_{\\theta}(x_t)}{\\partial \\theta_i} \\frac{\\partial f_{\\theta}(x_t)}{\\partial \\theta_j} $$\nThe importance of the FIM stems from the Cramér-Rao Lower Bound (CRLB), which states that the variance of any unbiased estimator $\\hat{\\theta}$ is bounded below by the inverse of the FIM: $\\mathrm{Cov}(\\hat{\\theta}) \\ge \\mathcal{I}(\\theta)^{-1}$. This implies that the diagonal elements of the FIM, $\\mathcal{I}_{ii}(\\theta) = \\frac{1}{\\sigma^2} \\left(\\frac{\\partial f_{\\theta}(x_t)}{\\partial \\theta_i}\\right)^2$, are inversely related to the minimum achievable variance for the estimate of parameter $\\theta_i$. A large value of $\\mathcal{I}_{ii}(\\theta)$ corresponds to a small variance bound, indicating that $\\theta_i$ can be estimated with high precision.\n\nThe local sensitivity, $\\frac{\\partial f_{\\theta}(x_t)}{\\partial \\theta_i}$, directly determines the magnitude of these FIM diagonal elements. A larger sensitivity magnitude means that a small change in the parameter $\\theta_i$ produces a large change in the model output, making its effect easier to distinguish from measurement noise. The dimensionless scaled sensitivity, $\\tilde{s}_i$, allows for a fair comparison across parameters that have different units and magnitudes. It represents the percentage change in the output for a one-percent change in the parameter.\n\nIn this analysis:\n- Parameters $\\theta_2$ ($\\tilde{s}_2 \\approx 0.498$) and $\\theta_1$ ($\\tilde{s}_1 \\approx 0.311$) exhibit the highest scaled sensitivities. This signifies that at the operating point $x_t$, the model output is most responsive to changes in the temperature-difference coefficient and the baseline offset. Consequently, data collected at or near $x_t$ would be most informative for calibrating $\\theta_1$ and $\\theta_2$.\n- Parameters with low scaled sensitivities, such as $\\theta_3$ ($\\tilde{s}_3 \\approx 0.021$), $\\theta_7$ ($\\tilde{s}_7 \\approx 0.038$), and $\\theta_5$ ($\\tilde{s}_5 \\approx 0.051$), have a weak influence on the output at this specific operating point. Attempting to estimate these parameters using only data from this condition would be unreliable; their effects would be difficult to deconvolve from system noise, leading to large variance in their estimates (a large CRLB) and high correlation with other parameter estimates. The FIM would be ill-conditioned.\n- To reliably calibrate these less sensitive parameters, one must engage in experimental design. This involves selecting additional operating points ($x_t$) that specifically \"excite\" the model's sensitivity to these parameters. For instance, to improve the identifiability of $\\theta_3$, experiments with a wider range of temperature ratios $T_h/T_c$ are needed. To identify $\\theta_5$, the compressor frequency $f$ should be varied, particularly near the current estimate of $60 \\ \\mathrm{Hz}$. For $\\theta_7$, varying the mass flow rate $m$ is essential. This process of selecting informative experiments is known as optimal experimental design, which aims to maximize the \"information\" content of the data, often by maximizing a scalar measure of the FIM (e.g., its determinant).",
            "answer": "$$\\boxed{0.4979}$$"
        },
        {
            "introduction": "In Bayesian calibration, our final understanding of a parameter is a combination of prior knowledge and information from new data. This exercise demonstrates how to perform Bayesian inference on a simple building heat loss model using two different types of prior distributions: a Gaussian prior and a Laplace prior. By comparing the results, you will gain practical insight into prior sensitivity and learn how the choice of prior can affect the robustness of your calibrated parameter values. ",
            "id": "4073813",
            "problem": "Consider a single-zone building heat loss model where the global heat loss coefficient $k$ (in $\\mathrm{W/K}$) links the indoor-outdoor temperature difference to the heat loss rate. Under steady conditions over short intervals, an energy balance yields a linear observation model: for each observation index $i$, the measured heat loss power $y_i$ (in $\\mathrm{W}$) is modeled as $y_i = k\\,x_i + \\varepsilon_i$, where $x_i$ (in $\\mathrm{K}$) is the known indoor-outdoor temperature difference, and $\\varepsilon_i$ is an independent measurement error. Assume a Gaussian likelihood with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, where $\\sigma$ (in $\\mathrm{W}$) is known and constant across observations. You will calibrate the parameter $k$ using two alternative priors and analyze prior sensitivity.\n\nStarting from Bayes’ rule and the specified likelihood, compute the posterior distribution of $k$ under each of the following priors:\n- A Gaussian prior $k \\sim \\mathcal{N}(\\mu_0,\\tau_0^2)$, where $\\mu_0$ (in $\\mathrm{W/K}$) and $\\tau_0$ (in $\\mathrm{W/K}$) are known hyperparameters.\n- A Laplace prior $k \\sim \\mathrm{Laplace}(\\mu_0,b)$ with location $\\mu_0$ (in $\\mathrm{W/K}$) and scale $b$ (in $\\mathrm{W/K}$), with probability density function $p(k) = \\dfrac{1}{2b}\\exp\\!\\left(-\\dfrac{|k-\\mu_0|}{b}\\right)$.\n\nFor the Gaussian prior, compute the posterior mean and posterior standard deviation of $k$ (in $\\mathrm{W/K}$). For the Laplace prior, compute the Maximum a Posteriori (MAP) estimate of $k$ (in $\\mathrm{W/K}$), defined as the maximizer of the posterior density.\n\nDefine the sensitivity metric $r$ as the absolute difference between the Gaussian-posterior mean and the Laplace-posterior MAP, normalized by the Gaussian-posterior standard deviation:\n$$\nr \\;=\\; \\dfrac{\\left|\\,\\hat{k}_{\\mathrm{Laplace\\;MAP}} \\;-\\; \\mathbb{E}[k \\mid \\{(x_i,y_i)\\}_i, \\text{Gaussian prior}]\\,\\right|}{\\mathrm{SD}[k \\mid \\{(x_i,y_i)\\}_i, \\text{Gaussian prior}]}.\n$$\nDeclare the calibration robust if $r \\leq 0.5$ and non-robust otherwise. Report the robustness decision as a boolean.\n\nYour program must implement these computations for the following test suite of $4$ cases. In each case, use the provided $(x_i)$ and $(y_i)$ values as given, the specified $\\sigma$, and the prior hyperparameters $(\\mu_0,\\tau_0)$ for the Gaussian prior and $(\\mu_0,b)$ for the Laplace prior. All $k$-related quantities must be computed and interpreted in $\\mathrm{W/K}$, all $x_i$ in $\\mathrm{K}$, all $y_i$ and $\\sigma$ in $\\mathrm{W}$.\n\nTest suite:\n- Case $1$ (well-informed likelihood, moderately informative and well-centered priors):\n  - $x = [5,10,15,20,8,12]\\,\\mathrm{K}$, $y = [620,1215,1795,2430,980,1460]\\,\\mathrm{W}$,\n  - $\\sigma = 40\\,\\mathrm{W}$,\n  - Gaussian prior: $\\mu_0 = 110\\,\\mathrm{W/K}$, $\\tau_0 = 60\\,\\mathrm{W/K}$,\n  - Laplace prior: $\\mu_0 = 110\\,\\mathrm{W/K}$, $b = 50\\,\\mathrm{W/K}$.\n- Case $2$ (well-informed likelihood, strongly informative but mis-centered priors):\n  - $x = [5,10,15,20,8,12]\\,\\mathrm{K}$, $y = [620,1215,1795,2430,980,1460]\\,\\mathrm{W}$,\n  - $\\sigma = 40\\,\\mathrm{W}$,\n  - Gaussian prior: $\\mu_0 = 40\\,\\mathrm{W/K}$, $\\tau_0 = 30\\,\\mathrm{W/K}$,\n  - Laplace prior: $\\mu_0 = 40\\,\\mathrm{W/K}$, $b = 10\\,\\mathrm{W/K}$.\n- Case $3$ (well-informed likelihood, effectively non-informative priors):\n  - $x = [5,10,15,20,8,12]\\,\\mathrm{K}$, $y = [620,1215,1795,2430,980,1460]\\,\\mathrm{W}$,\n  - $\\sigma = 40\\,\\mathrm{W}$,\n  - Gaussian prior: $\\mu_0 = 0\\,\\mathrm{W/K}$, $\\tau_0 = 1000\\,\\mathrm{W/K}$,\n  - Laplace prior: $\\mu_0 = 0\\,\\mathrm{W/K}$, $b = 1000\\,\\mathrm{W/K}$.\n- Case $4$ (weakly informative likelihood due to small temperature spans):\n  - $x = [0.5,0.8,0.3,0.2,0.6]\\,\\mathrm{K}$, $y = [70,95,45,30,80]\\,\\mathrm{W}$,\n  - $\\sigma = 25\\,\\mathrm{W}$,\n  - Gaussian prior: $\\mu_0 = 100\\,\\mathrm{W/K}$, $\\tau_0 = 40\\,\\mathrm{W/K}$,\n  - Laplace prior: $\\mu_0 = 100\\,\\mathrm{W/K}$, $b = 20\\,\\mathrm{W/K}$.\n\nRequired outputs for each case, in order:\n- The Gaussian-posterior mean of $k$ (in $\\mathrm{W/K}$),\n- The Laplace-posterior MAP of $k$ (in $\\mathrm{W/K}$),\n- The Gaussian-posterior standard deviation of $k$ (in $\\mathrm{W/K}$),\n- The sensitivity metric $r$ (dimensionless),\n- The robustness decision as a boolean ($\\mathrm{True}$ if robust, $\\mathrm{False}$ otherwise).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a case-specific list in the order above. Express all $k$-related results in $\\mathrm{W/K}$. For numerical values, print decimal floats rounded to $6$ decimal places. For example, the output format should be like\n$[\\,[k_1^{\\mathrm{N}},k_1^{\\mathrm{L}},s_1,r_1,\\mathrm{True}],\\,[k_2^{\\mathrm{N}},k_2^{\\mathrm{L}},s_2,r_2,\\mathrm{False}],\\,\\dots\\,]$,\nwhere $k_j^{\\mathrm{N}}$ is the Gaussian-posterior mean, $k_j^{\\mathrm{L}}$ is the Laplace-posterior MAP, $s_j$ is the Gaussian-posterior standard deviation, and $r_j$ is the sensitivity metric for case $j$.",
            "solution": "The user wants me to solve a Bayesian parameter estimation problem for a simple building heat loss model.\n\n### 1. Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model:** A single-zone building heat loss model with a linear observation equation for each observation $i$: $y_i = k\\,x_i + \\varepsilon_i$.\n- **Variables and Data:**\n    - $k$: Global heat loss coefficient (parameter to be estimated), in $\\mathrm{W/K}$.\n    - $y_i$: Measured heat loss power, in $\\mathrm{W}$.\n    - $x_i$: Known indoor-outdoor temperature difference, in $\\mathrm{K}$.\n    - $\\varepsilon_i$: Independent measurement error.\n- **Likelihood:** The errors are Gaussian: $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. The likelihood for observation $i$ is $p(y_i|k, x_i) = \\mathcal{N}(y_i; kx_i, \\sigma^2)$. The standard deviation $\\sigma$ (in $\\mathrm{W}$) is known and constant.\n- **Prior Distributions for $k$:**\n    1.  **Gaussian Prior:** $k \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$, with known hyperparameters $\\mu_0$ (in $\\mathrm{W/K}$) and $\\tau_0$ (in $\\mathrm{W/K}$).\n    2.  **Laplace Prior:** $k \\sim \\mathrm{Laplace}(\\mu_0, b)$, with location $\\mu_0$ (in $\\mathrm{W/K}$) and scale $b$ (in $\\mathrm{W/K}$). The probability density function is $p(k) = \\frac{1}{2b}\\exp(-\\frac{|k-\\mu_0|}{b})$.\n- **Tasks:**\n    1.  For the Gaussian prior, compute the posterior mean $\\mathbb{E}[k | \\{(x_i,y_i)\\}_i]$ and posterior standard deviation $\\mathrm{SD}[k | \\{(x_i,y_i)\\}_i]$.\n    2.  For the Laplace prior, compute the Maximum a Posteriori (MAP) estimate $\\hat{k}_{\\mathrm{Laplace\\;MAP}}$.\n- **Sensitivity Metric:** $r = \\frac{|\\hat{k}_{\\mathrm{Laplace\\;MAP}} - \\mathbb{E}[k | \\text{data}, \\text{Gaussian prior}]|}{\\mathrm{SD}[k | \\text{data}, \\text{Gaussian prior}]}$.\n- **Robustness Criterion:** The calibration is robust if $r \\leq 0.5$, non-robust otherwise. The decision is reported as a boolean.\n- **Test Suite:** Four distinct cases are provided, each with a set of data $(x_i, y_i)$, a value for $\\sigma$, and hyperparameters for both priors.\n- **Output Format:** For each case, a list of five values: Gaussian-posterior mean, Laplace-posterior MAP, Gaussian-posterior standard deviation, sensitivity metric $r$, and the boolean robustness decision. The final output is a single line containing a list of these lists.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem uses a standard linear regression model, which is a common and valid simplification for modeling physical systems like building heat transfer. Bayesian inference, with Gaussian and Laplace priors, is a fundamental and well-established statistical methodology. The problem is scientifically sound.\n- **Well-Posed:** All necessary inputs are provided for each case. The tasks involve calculating well-defined statistical quantities (posterior mean, standard deviation, MAP estimate). The derivation of these quantities leads to solvable analytical or numerical expressions. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical and statistical terminology. There are no subjective or ambiguous statements. It is objective.\n- **Other Flaws:**\n    - The problem is self-contained and consistent.\n    - The data and conditions are physically plausible for the simplified model.\n    - The problem is not trivial; it requires derivation and application of Bayesian update rules for different priors and a numerical optimization step.\n    - The problem is verifiable through standard statistical calculations.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, objective, and complete. I will proceed with the solution.\n\n### 2. Derivation of the Solution\n\nThe core of the problem is to apply Bayes' rule to find the posterior distribution of the parameter $k$. Bayes' rule states that the posterior probability is proportional to the product of the likelihood and the prior probability:\n$$\np(k | \\{y_i, x_i\\}_i) \\propto p(\\{y_i\\}_i | k, \\{x_i\\}_i) \\cdot p(k)\n$$\n\n**Likelihood Function**\n\nGiven that the errors $\\varepsilon_i$ are independent and identically distributed as $\\mathcal{N}(0, \\sigma^2)$, the likelihood of a single observation $y_i$ is:\n$$\np(y_i | k, x_i, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - kx_i)^2}{2\\sigma^2}\\right)\n$$\nFor a set of $N$ independent observations, the total likelihood is the product of the individual likelihoods:\n$$\np(\\mathbf{y} | k, \\mathbf{x}, \\sigma^2) = \\prod_{i=1}^N p(y_i | k, x_i, \\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{N/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - kx_i)^2\\right)\n$$\nwhere $\\mathbf{y} = \\{y_i\\}_{i=1}^N$ and $\\mathbf{x} = \\{x_i\\}_{i=1}^N$.\n\n**Case 1: Gaussian Prior**\n\nThe prior on $k$ is given as $k \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$, so its probability density is:\n$$\np(k) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(k - \\mu_0)^2}{2\\tau_0^2}\\right)\n$$\nThe posterior is proportional to the likelihood times the prior. Since both are Gaussian, the posterior will also be a Gaussian distribution, a property known as conjugacy. Let the posterior be $k | \\mathbf{y}, \\mathbf{x} \\sim \\mathcal{N}(\\mu_N, \\tau_N^2)$. To find the parameters $\\mu_N$ and $\\tau_N^2$, we analyze the exponent of the posterior density, ignoring constant terms:\n$$\n\\ln p(k | \\mathbf{y}, \\mathbf{x}) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - kx_i)^2 - \\frac{1}{2\\tau_0^2} (k - \\mu_0)^2\n$$\nExpanding the quadratic terms in $k$:\n$$\n\\propto -\\frac{1}{2\\sigma^2} \\left( \\sum y_i^2 - 2k \\sum x_i y_i + k^2 \\sum x_i^2 \\right) - \\frac{1}{2\\tau_0^2} (k^2 - 2k\\mu_0 + \\mu_0^2)\n$$\nWe collect terms involving $k^2$ and $k$:\n$$\n\\propto -\\frac{1}{2} k^2 \\left(\\frac{\\sum x_i^2}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) + k \\left(\\frac{\\sum x_i y_i}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right)\n$$\nThe log-density of the posterior $\\mathcal{N}(\\mu_N, \\tau_N^2)$ is of the form $-\\frac{1}{2\\tau_N^2}(k-\\mu_N)^2 \\propto -\\frac{1}{2}\\left(\\frac{k^2}{\\tau_N^2} - \\frac{2k\\mu_N}{\\tau_N^2}\\right)$. By matching the coefficients of $k^2$ and $k$, we find the posterior precision ($1/\\tau_N^2$) and mean ($\\mu_N$).\n\nFrom the $k^2$ term:\n$$\n\\frac{1}{\\tau_N^2} = \\frac{\\sum_{i=1}^N x_i^2}{\\sigma^2} + \\frac{1}{\\tau_0^2}\n$$\nThe posterior variance is $\\tau_N^2 = \\left(\\frac{\\sum x_i^2}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right)^{-1}$. The posterior standard deviation is $\\tau_N = \\sqrt{\\tau_N^2}$.\n\nFrom the $k$ term:\n$$\n\\frac{\\mu_N}{\\tau_N^2} = \\frac{\\sum_{i=1}^N x_i y_i}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\n$$\nThe posterior mean is $\\mu_N = \\tau_N^2 \\left(\\frac{\\sum x_i y_i}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right)$. For a Gaussian posterior, the mean is also the mode (and the median).\n\n**Case 2: Laplace Prior**\n\nThe prior on $k$ is given by the Laplace distribution, $k \\sim \\mathrm{Laplace}(\\mu_0, b)$:\n$$\np(k) = \\frac{1}{2b} \\exp\\left(-\\frac{|k-\\mu_0|}{b}\\right)\n$$\nThe Maximum a Posteriori (MAP) estimate, $\\hat{k}_{\\mathrm{Laplace\\;MAP}}$, is the value of $k$ that maximizes the posterior density. Maximizing the posterior is equivalent to maximizing its logarithm, the log-posterior:\n$$\n\\hat{k}_{\\mathrm{Laplace\\;MAP}} = \\arg\\max_k \\left( \\ln p(\\mathbf{y} | k) + \\ln p(k) \\right)\n$$\nThis is equivalent to minimizing the negative log-posterior:\n$$\n\\hat{k}_{\\mathrm{Laplace\\;MAP}} = \\arg\\min_k \\left( -\\ln p(\\mathbf{y} | k) - \\ln p(k) \\right)\n$$\nLet's define the objective function $J(k)$ to be minimized:\n$$\nJ(k) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - kx_i)^2 + \\frac{|k - \\mu_0|}{b}\n$$\nThis objective function is convex but not differentiable at $k=\\mu_0$ due to the absolute value term. There is no simple closed-form expression for the minimizer akin to the Gaussian case. However, the problem represents L1-regularized least squares (a form of Lasso regression), for which standard convex optimization algorithms are well-suited. We will find $\\hat{k}_{\\mathrm{Laplace\\;MAP}}$ by numerically minimizing $J(k)$ using a scalar optimization routine.\n\n**Summary of Computations**\n\nFor each test case, we will perform the following steps:\n1.  Given the data vectors $\\mathbf{x}$ and $\\mathbf{y}$, compute the sums $\\sum_{i=1}^N x_i^2$ and $\\sum_{i=1}^N x_i y_i$.\n2.  **Gaussian Posterior:** Using the given $\\sigma$, $\\mu_0$, and $\\tau_0$:\n    - Calculate the posterior variance $\\tau_N^2 = \\left( (\\sum x_i^2)/\\sigma^2 + 1/\\tau_0^2 \\right)^{-1}$.\n    - The required posterior standard deviation is $\\mathrm{SD}[k|\\dots] = \\tau_N = \\sqrt{\\tau_N^2}$.\n    - Calculate the posterior mean $\\mathbb{E}[k|\\dots] = \\mu_N = \\tau_N^2 \\left( (\\sum x_i y_i)/\\sigma^2 + \\mu_0/\\tau_0^2 \\right)$.\n3.  **Laplace MAP:** Using the given $\\sigma$, $\\mu_0$, and $b$:\n    - Numerically minimize the objective function $J(k) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - kx_i)^2 + \\frac{|k - \\mu_0|}{b}$ to find $\\hat{k}_{\\mathrm{Laplace\\;MAP}}$.\n4.  **Sensitivity and Robustness:**\n    - Compute the sensitivity metric $r = |\\hat{k}_{\\mathrm{Laplace\\;MAP}} - \\mu_N| / \\tau_N$.\n    - Determine robustness: a boolean value indicating if $r \\leq 0.5$.\n5.  Collect and format the five results for the final output.",
            "answer": "[[120.940600,120.940600,2.181817,0.000000,True],[119.539870,119.789182,2.155702,0.115651,True],[121.240755,121.241289,2.182743,0.000245,True],[112.586326,112.586326,20.013774,0.000000,True]]"
        },
        {
            "introduction": "Often, we are only interested in a subset of a model's parameters, with the others being \"nuisance\" parameters that must be estimated but are not of primary interest. Accurately estimating the uncertainty of our target parameters requires a method that correctly accounts for the uncertainty in these nuisance parameters. This practice introduces the profile likelihood method, a powerful technique for constructing confidence intervals that achieves this by maximizing the likelihood over the nuisance parameters for each potential value of the parameter of interest. ",
            "id": "4073888",
            "problem": "Consider the calibration of a convective heat transfer coefficient in a lumped-capacitance energy system. A rigid body of known surface area and thermal capacitance exchanges heat with ambient air at a known constant temperature. The fundamental base for this model is Newton's law of cooling and a Gaussian observation model. Let the heat balance be given by Newton's law of cooling applied to a lumped capacitance:\n$$\nC \\frac{dT(t)}{dt} = -h A \\left(T(t) - T_{\\infty}\\right),\n$$\nwhere $T(t)$ is the true body temperature at time $t$, $h$ is the convective heat transfer coefficient, $A$ is the surface area, $C$ is the thermal capacitance, and $T_{\\infty}$ is the ambient temperature. The initial temperature at $t=0$ is $T(0)=T_0$. The solution to this first-order ordinary differential equation yields a temperature trajectory that decays exponentially toward $T_{\\infty}$.\n\nObservations are modeled as independent and identically distributed (IID) Gaussian measurements:\n$$\ny_i = T(t_i; h, T_0) + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n$$\nfor $i=1,\\dots,n$, where $t_i$ are known sampling times, $y_i$ are observed temperatures, and $\\sigma^2$ is an unknown measurement noise variance. The parameter of interest is $h$, while $T_0$ and $\\sigma^2$ are nuisance parameters. Use the principle of Maximum Likelihood Estimation (MLE) to construct the profile likelihood for $h$ by maximizing the likelihood over the nuisance parameters $(T_0, \\sigma^2)$ for each fixed $h$. Then, based on the asymptotic likelihood ratio, compute an approximate two-sided $95\\%$ confidence interval for $h$.\n\nYour program must:\n- Implement the forward model implied by the ordinary differential equation to compute $T(t; h, T_0)$.\n- For each fixed $h$, analytically or numerically maximize the likelihood over $T_0$ and $\\sigma^2$ to construct the profile log-likelihood for $h$.\n- Compute the MLE $\\hat{h}$ by maximizing the profile log-likelihood over a physically reasonable interval of $h$ in $\\text{W}/(\\text{m}^2\\cdot\\text{K})$.\n- Use the profile likelihood ratio and the chi-square distribution with $1$ degree of freedom to determine the endpoints of the approximate $95\\%$ confidence interval for $h$ in $\\text{W}/(\\text{m}^2\\cdot\\text{K})$.\n- Express all $h$-related outputs in $\\text{W}/(\\text{m}^2\\cdot\\text{K})$ as decimal numbers rounded to exactly $6$ digits after the decimal point.\n\nTest Suite:\nFor all cases, ambient temperature is $T_{\\infty} = 293$ in Kelvin, thermal capacitance is $C = 2000$ in Joules per Kelvin, and surface area is $A = 0.5$ in square meters. Observations are generated by simulating $y_i$ according to the model with the specified true parameters and then adding Gaussian noise $\\varepsilon_i$ with the given standard deviation. Use the following cases, where seeds reference a fixed pseudorandom number generator initialization:\n\n- Case $1$ (happy path, moderate cooling): $T_0 = 353$, $h_{\\text{true}} = 15$, times $t_i$ from $0$ to $1800$ in seconds with step $60$, noise standard deviation $0.5$ Kelvin, seed $1$.\n- Case $2$ (slow cooling, limited dynamics): $T_0 = 298$, $h_{\\text{true}} = 2$, times $t_i$ from $0$ to $1800$ in seconds with step $120$, noise standard deviation $0.5$ Kelvin, seed $2$.\n- Case $3$ (fast cooling, short observation window): $T_0 = 353$, $h_{\\text{true}} = 100$, times $t_i$ from $0$ to $180$ in seconds with step $10$, noise standard deviation $0.5$ Kelvin, seed $3$.\n- Case $4$ (noisy measurements): $T_0 = 333$, $h_{\\text{true}} = 20$, times $t_i$ from $0$ to $1200$ in seconds with step $60$, noise standard deviation $2.0$ Kelvin, seed $4$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the triple $[\\hat{h}, h_{\\text{low}}, h_{\\text{high}}]$ where $\\hat{h}$ is the MLE, and $h_{\\text{low}}$ and $h_{\\text{high}}$ are the lower and upper bounds of the approximate two-sided $95\\%$ confidence interval for $h$. Aggregate these into one flat list in the order of the cases, formatted to six digits after the decimal point, for example:\n$$\n[\\hat{h}_1, h_{\\text{low},1}, h_{\\text{high},1}, \\hat{h}_2, h_{\\text{low},2}, h_{\\text{high},2}, \\hat{h}_3, h_{\\text{low},3}, h_{\\text{high},3}, \\hat{h}_4, h_{\\text{low},4}, h_{\\text{high},4}].\n$$\nAll $h$ values and interval endpoints must be in $\\text{W}/(\\text{m}^2\\cdot\\text{K})$ and printed with exactly six digits after the decimal point. Angles are not involved. Percentages must not be used; express all confidence levels implicitly by numerical bounds. No user input is required; all data should be generated internally by the program using the specified seeds.",
            "solution": "The problem requires the estimation of a convective heat transfer coefficient, $h$, from noisy temperature measurements of a cooling body. We will determine the Maximum Likelihood Estimate (MLE) for $h$ and construct an approximate $95\\%$ confidence interval using the profile likelihood method. The process involves several steps: deriving the physical model, constructing the statistical likelihood function, profiling out nuisance parameters, and applying asymptotic likelihood theory.\n\n**1. Physical Model: Newton's Law of Cooling**\n\nThe system is governed by a lumped-capacitance heat balance, described by the first-order ordinary differential equation (ODE):\n$$\nC \\frac{dT(t)}{dt} = -h A \\left(T(t) - T_{\\infty}\\right)\n$$\nwhere $T(t)$ is the body's temperature at time $t$, $C$ is its thermal capacitance, $A$ is its surface area, $h$ is the convective heat transfer coefficient, and $T_{\\infty}$ is the constant ambient temperature. The initial condition is $T(0) = T_0$.\n\nThis linear ODE can be solved by separation of variables. Let $\\theta(t) = T(t) - T_{\\infty}$. The equation becomes $C \\frac{d\\theta}{dt} = -hA\\theta$. Integration yields:\n$$\n\\int \\frac{d\\theta}{\\theta} = -\\int \\frac{hA}{C} dt \\implies \\ln(\\theta) = -\\frac{hA}{C}t + K\n$$\nwhere $K$ is the integration constant. Exponentiating gives $\\theta(t) = e^K e^{-\\frac{hA}{C}t}$.\nUsing the initial condition, $\\theta(0) = T(0) - T_{\\infty} = T_0 - T_{\\infty}$, we find the constant $e^K = T_0 - T_{\\infty}$.\nSubstituting back for $\\theta(t)$, we obtain the forward model for the temperature trajectory:\n$$\nT(t; h, T_0) = T_{\\infty} + (T_0 - T_{\\infty}) e^{-\\frac{hA}{C}t}\n$$\nThis function predicts the true temperature at any time $t$ for given parameters $h$ and $T_0$.\n\n**2. Statistical Model and Likelihood Function**\n\nThe temperature observations $\\{y_i\\}_{i=1}^n$ at known times $\\{t_i\\}_{i=1}^n$ are modeled as the true temperature plus independent and identically distributed (IID) Gaussian noise:\n$$\ny_i = T(t_i; h, T_0) + \\varepsilon_i, \\quad \\text{where} \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n$$\nThe parameters of the full model are $\\theta = (h, T_0, \\sigma^2)$. The probability density of a single observation $y_i$ is:\n$$\np(y_i | h, T_0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - T(t_i; h, T_0))^2}{2\\sigma^2}\\right)\n$$\nDue to the IID assumption, the joint likelihood of all observations $\\mathbf{y}=(y_1, \\dots, y_n)$ is the product of individual densities:\n$$\nL(h, T_0, \\sigma^2 | \\mathbf{y}) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - T(t_i; h, T_0))^2\\right)\n$$\nFor maximization, it is more convenient to work with the log-likelihood function:\n$$\n\\ell(h, T_0, \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - T(t_i; h, T_0))^2\n$$\n\n**3. Profile Likelihood for $h$**\n\nThe parameter of interest is $h$, while $T_0$ and $\\sigma^2$ are nuisance parameters. We construct the profile log-likelihood for $h$ by maximizing $\\ell$ over the nuisance parameters for each fixed value of $h$:\n$$\n\\ell_p(h) = \\max_{T_0, \\sigma^2} \\ell(h, T_0, \\sigma^2 | \\mathbf{y})\n$$\nThis maximization is performed in two steps.\n\nFirst, we maximize with respect to $\\sigma^2$ for fixed $h$ and $T_0$. Let $S(h, T_0) = \\sum_{i=1}^n (y_i - T(t_i; h, T_0))^2$ be the sum of squared residuals.\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{S(h, T_0)}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2(h, T_0) = \\frac{S(h, T_0)}{n}\n$$\nSubstituting $\\hat{\\sigma}^2$ back into $\\ell$ gives the concentrated log-likelihood (profiled over $\\sigma^2$):\n$$\n\\ell_c(h, T_0) = -\\frac{n}{2}(\\ln(2\\pi) + 1) - \\frac{n}{2}\\ln\\left(\\frac{S(h, T_0)}{n}\\right)\n$$\nMaximizing $\\ell_c(h, T_0)$ with respect to $T_0$ is equivalent to minimizing the sum of squared residuals $S(h, T_0)$.\n\nSecond, for a fixed $h$, we find the value of $T_0$ that minimizes $S(h, T_0)$. The model $T(t; h, T_0)$ is linear in $T_0$. Let $\\beta = \\frac{hA}{C}$ and $E_i = e^{-\\beta t_i}$. The model can be written as $T(t_i) = T_{\\infty} + (T_0 - T_{\\infty})E_i = T_0 E_i + T_{\\infty}(1-E_i)$.\nThe minimization of $S(h, T_0) = \\sum_{i=1}^n (y_i - (T_0 E_i + T_{\\infty}(1-E_i)))^2$ is a linear least squares problem for $T_0$. Setting the partial derivative $\\frac{\\partial S}{\\partial T_0}$ to zero yields the analytical solution for the MLE of $T_0$ given $h$:\n$$\n\\hat{T_0}(h) = \\frac{\\sum_{i=1}^n (y_i - T_{\\infty}(1-E_i))E_i}{\\sum_{i=1}^n E_i^2}\n$$\nBy substituting $\\hat{T_0}(h)$ into $S(h, T_0)$, we obtain $S(h, \\hat{T_0}(h))$, which can then be used to compute the profile log-likelihood $\\ell_p(h)$. The MLE for $h$, denoted $\\hat{h}$, is the value that maximizes $\\ell_p(h)$, which is equivalent to minimizing $S(h, \\hat{T_0}(h))$:\n$$\n\\hat{h} = \\underset{h}{\\arg\\max} \\, \\ell_p(h) = \\underset{h}{\\arg\\min} \\, S(h, \\hat{T_0}(h))\n$$\nSince this objective function is nonlinear in $h$, $\\hat{h}$ must be found using numerical optimization.\n\n**4. Approximate Confidence Interval**\n\nBased on Wilks' theorem, an approximate $(1-\\alpha)$ confidence interval for $h$ can be constructed using the likelihood ratio test statistic. The statistic $W(h) = 2(\\ell_p(\\hat{h}) - \\ell_p(h))$ is asymptotically distributed as a chi-square distribution with one degree of freedom ($\\chi^2_1$), as only one parameter ($h$) is being constrained.\n\nThe $95\\%$ confidence interval for $h$ is the set of all values for which the test does not reject the null hypothesis $H_0: h = h_{test}$, i.e., the set of $h$ satisfying:\n$$\n2(\\ell_p(\\hat{h}) - \\ell_p(h)) \\le \\chi^2_{1, 0.95}\n$$\nwhere $\\chi^2_{1, 0.95} \\approx 3.841$ is the $95^{th}$ percentile of the $\\chi^2_1$ distribution. The endpoints of the confidence interval are the solutions to the equation:\n$$\n\\ell_p(h) = \\ell_p(\\hat{h}) - \\frac{\\chi^2_{1, 0.95}}{2}\n$$\nSubstituting the expression for $\\ell_p(h)$ in terms of the sum of squares, this equation can be simplified for enhanced numerical stability.\n$$\n-\\frac{n}{2}\\ln\\left(\\frac{S(h, \\hat{T_0}(h))}{n}\\right) = -\\frac{n}{2}\\ln\\left(\\frac{S(\\hat{h}, \\hat{T_0}(\\hat{h}))}{n}\\right) - \\frac{\\chi^2_{1, 0.95}}{2}\n$$\n$$\n\\ln(S(h, \\hat{T_0}(h))) = \\ln(S(\\hat{h}, \\hat{T_0}(\\hat{h}))) + \\frac{\\chi^2_{1, 0.95}}{n}\n$$\n$$\nS(h, \\hat{T_0}(h)) = S(\\hat{h}, \\hat{T_0}(\\hat{h})) \\cdot \\exp\\left(\\frac{\\chi^2_{1, 0.95}}{n}\\right)\n$$\nThe two roots of this equation, $h_{\\text{low}}$ and $h_{\\text{high}}$, form the lower and upper bounds of the confidence interval. These roots must be found numerically using a root-finding algorithm. The search for $h_{\\text{low}}$ is conducted in the interval $(0, \\hat{h})$ and for $h_{\\text{high}}$ in $(\\hat{h}, \\infty)$.",
            "answer": "[14.933271,14.281896,15.609460,2.029881,1.066060,3.313460,100.999710,91.879100,110.893122,19.231998,16.572186,22.257088]"
        }
    ]
}