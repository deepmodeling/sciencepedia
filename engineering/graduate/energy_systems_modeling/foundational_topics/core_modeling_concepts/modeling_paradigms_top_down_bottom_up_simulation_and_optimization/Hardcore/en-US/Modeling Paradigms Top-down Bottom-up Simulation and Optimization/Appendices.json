{
    "hands_on_practices": [
        {
            "introduction": "Top-down models, such as Computable General Equilibrium (CGE) models, rely on a comprehensive and internally consistent snapshot of an economy's structure. This practice introduces the Social Accounting Matrix (SAM), the foundational double-entry bookkeeping framework that records all monetary flows between industries, institutions, and the rest of the world. By working through the logic of balancing the government account from input-output data, you will gain a fundamental understanding of the data architecture that underpins all macroeconomic energy-economy modeling .",
            "id": "4106006",
            "problem": "An energy-oriented Social Accounting Matrix (SAM) is a square matrix that records all transactions between accounts such that, for every account, total inflows equal total outflows. In top-down Computable General Equilibrium (CGE) modeling, a SAM provides the equilibrium-consistent macroeconomic datascape required for simulation and optimization of policy scenarios that include energy sectors. The key accounting identity is that for each account, the sum of receipts equals the sum of payments, and any residual is recorded as a saving or deficit that flows to or from the capital account.\n\nConsider an economy with five production activities: Electricity ($E$), Manufacturing ($M$), Services ($S$), Transport ($T$), and Agriculture ($A$); and four institutional accounts: Households ($H$), Government ($G$), Rest of the World ($R$), and Capital ($K$). All flows are reported in billions of domestic currency units. From the input–output (IO) table and national accounts, you are given the following data:\n\n- Interindustry transactions $T_{ij}$ where $i$ is the selling activity and $j$ is the buying activity:\n  - From $E$: $T_{EE}=5$, $T_{EM}=10$, $T_{ES}=2$, $T_{ET}=4$, $T_{EA}=1$.\n  - From $M$: $T_{ME}=3$, $T_{MM}=8$, $T_{MS}=5$, $T_{MT}=6$, $T_{MA}=4$.\n  - From $S$: $T_{SE}=2$, $T_{SM}=7$, $T_{SS}=9$, $T_{ST}=5$, $T_{SA}=3$.\n  - From $T$: $T_{TE}=1$, $T_{TM}=4$, $T_{TS}=3$, $T_{TT}=6$, $T_{TA}=2$.\n  - From $A$: $T_{AE}=0.5$, $T_{AM}=3$, $T_{AS}=2$, $T_{AT}=1$, $T_{AA}=7$.\n- Household final consumption by activity $c_{i}$: $c_{E}=12$, $c_{M}=25$, $c_{S}=40$, $c_{T}=18$, $c_{A}=15$.\n- Government final consumption by activity $g_{i}$ (measured at basic prices, that is, excluding product taxes): $g_{E}=5$, $g_{M}=7$, $g_{S}=12$, $g_{T}=10$, $g_{A}=6$.\n- Exports by activity $x_{i}$: $x_{E}=8$, $x_{M}=20$, $x_{S}=10$, $x_{T}=6$, $x_{A}=12$.\n- Imports by commodity $m_{i}$: $m_{E}=5$, $m_{M}=18$, $m_{S}=12$, $m_{T}=9$, $m_{A}=7$.\n- Ad valorem product tax rates on gross output $\\tau^{p}_{i}$: $\\tau^{p}_{E}=0.05$, $\\tau^{p}_{M}=0.10$, $\\tau^{p}_{S}=0.08$, $\\tau^{p}_{T}=0.06$, $\\tau^{p}_{A}=0.04$.\n- Ad valorem consumption tax rates on household consumption $\\tau^{c}_{i}$: $\\tau^{c}_{E}=0.02$, $\\tau^{c}_{M}=0.04$, $\\tau^{c}_{S}=0.03$, $\\tau^{c}_{T}=0.05$, $\\tau^{c}_{A}=0.01$.\n- Ad valorem tariff rates on imports $\\tau^{m}_{i}$: $\\tau^{m}_{E}=0.03$, $\\tau^{m}_{M}=0.05$, $\\tau^{m}_{S}=0.02$, $\\tau^{m}_{T}=0.04$, $\\tau^{m}_{A}=0.03$.\n- Wage payments by activities to households $w_{i}$: $w_{E}=18$, $w_{M}=35$, $w_{S}=38$, $w_{T}=21$, $w_{A}=18$.\n- Household income tax rate on wage income $\\tau^{y}=0.15$.\n- Government transfers to households $tr^{G\\to H}=8$.\n- Government interest payments $r^{G}=2$.\n\nConstruct the SAM-consistent government account from first principles by mapping the IO data to SAM components as follows:\n\n1. Infer gross domestic output for each activity $Y_{i}$ from IO flows and final demands using the identity that total sales of activity $i$ equal interindustry sales plus household consumption plus government consumption plus exports.\n2. Use the SAM identity that tax flows are recorded as payments from the respective bases to the government account and compute the government’s tax revenues from product taxes on gross outputs, consumption taxes on household consumption, import tariffs on imports, and household income taxes on wages.\n3. Use the SAM identity that government outlays are recorded as payments from the government account to other accounts and compute total government expenditures as the sum of government final consumption at basic prices, transfers to households, and interest payments.\n4. Enforce the double-entry accounting identity for the government account, whereby the residual flow, government savings $S_{G}$ (a payment from $G$ to $K$ if positive, or an inflow to $G$ from $K$ if negative), balances the account so that total inflows equal total outflows.\n\nCompute the unique scalar value of $S_{G}$ implied by the above data and identities. Express your final answer in billions of domestic currency units. No rounding is required; report the exact value.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of Social Accounting Matrices (SAM), well-posed with a clear and complete set of data and identities, and expressed in objective, formal language. The task is to compute the government savings, $S_G$, by constructing the government account from the provided input-output and national accounts data. This involves a four-step process as outlined in the problem statement.\n\nThe government account, like all accounts in a SAM, must balance. This means that total inflows (receipts) must equal total outflows (payments). The balancing item for the government account is government savings, $S_G$. The fundamental identity is:\n$$\n\\text{Total Government Revenue} = \\text{Total Government Expenditures} + S_G\n$$\nTherefore, government savings is the residual:\n$$\nS_G = \\text{Total Government Revenue} - \\text{Total Government Expenditures}\n$$\nA positive value for $S_G$ represents a government surplus (a payment from the Government account, $G$, to the Capital account, $K$), while a negative value represents a deficit (an inflow to $G$ from $K$).\n\nWe will proceed by calculating total government revenue and total government expenditures based on the provided data.\n\n**Step 1: Compute Gross Domestic Output for each activity ($Y_i$)**\n\nGross domestic output for an activity $i$, denoted $Y_i$, is the total value of its sales. This is the sum of its sales to other industries (intermediate consumption), to households (final consumption), to the government (final consumption), and to the rest of the world (exports). The formula is:\n$$\nY_i = \\sum_{j \\in \\{E,M,S,T,A\\}} T_{ij} + c_i + g_i + x_i\n$$\nwhere $T_{ij}$ represents the sale from activity $i$ to activity $j$, $c_i$ is household consumption of product $i$, $g_i$ is government consumption of product $i$, and $x_i$ is exports of product $i$.\n\nFor Electricity ($E$):\n$Y_E = T_{EE} + T_{EM} + T_{ES} + T_{ET} + T_{EA} + c_E + g_E + x_E = 5 + 10 + 2 + 4 + 1 + 12 + 5 + 8 = 47$\n\nFor Manufacturing ($M$):\n$Y_M = T_{ME} + T_{MM} + T_{MS} + T_{MT} + T_{MA} + c_M + g_M + x_M = 3 + 8 + 5 + 6 + 4 + 25 + 7 + 20 = 78$\n\nFor Services ($S$):\n$Y_S = T_{SE} + T_{SM} + T_{SS} + T_{ST} + T_{SA} + c_S + g_S + x_S = 2 + 7 + 9 + 5 + 3 + 40 + 12 + 10 = 88$\n\nFor Transport ($T$):\n$Y_T = T_{TE} + T_{TM} + T_{TS} + T_{TT} + T_{TA} + c_T + g_T + x_T = 1 + 4 + 3 + 6 + 2 + 18 + 10 + 6 = 50$\n\nFor Agriculture ($A$):\n$Y_A = T_{AE} + T_{AM} + T_{AS} + T_{AT} + T_{AA} + c_A + g_A + x_A = 0.5 + 3 + 2 + 1 + 7 + 15 + 6 + 12 = 46.5$\n\nThe gross outputs are: $Y_E=47$, $Y_M=78$, $Y_S=88$, $Y_T=50$, and $Y_A=46.5$.\n\n**Step 2: Compute Total Government Revenue ($R_G$)**\n\nGovernment revenue is composed of four types of taxes:\n1.  **Product Tax Revenue ($R_p$)**: This is the sum of taxes levied on the gross output of each activity.\n    $$R_p = \\sum_{i} \\tau^{p}_{i} Y_i$$\n    $R_p = (0.05)(47) + (0.10)(78) + (0.08)(88) + (0.06)(50) + (0.04)(46.5)$\n    $R_p = 2.35 + 7.8 + 7.04 + 3.0 + 1.86 = 22.05$\n\n2.  **Consumption Tax Revenue ($R_c$)**: This is the sum of taxes on household consumption.\n    $$R_c = \\sum_{i} \\tau^{c}_{i} c_i$$\n    $R_c = (0.02)(12) + (0.04)(25) + (0.03)(40) + (0.05)(18) + (0.01)(15)$\n    $R_c = 0.24 + 1.0 + 1.2 + 0.9 + 0.15 = 3.49$\n\n3.  **Tariff Revenue ($R_m$)**: This is the sum of taxes on imports.\n    $$R_m = \\sum_{i} \\tau^{m}_{i} m_i$$\n    $R_m = (0.03)(5) + (0.05)(18) + (0.02)(12) + (0.04)(9) + (0.03)(7)$\n    $R_m = 0.15 + 0.9 + 0.24 + 0.36 + 0.21 = 1.86$\n\n4.  **Household Income Tax Revenue ($R_y$)**: This is the tax on total wage income paid by activities to households.\n    First, we calculate total wage income, $W$:\n    $W = \\sum_{i} w_i = 18 + 35 + 38 + 21 + 18 = 130$\n    Then, we apply the income tax rate $\\tau^y$:\n    $R_y = \\tau^y \\times W = 0.15 \\times 130 = 19.5$\n\nTotal Government Revenue, $R_G$, is the sum of these components:\n$$R_G = R_p + R_c + R_m + R_y$$\n$$R_G = 22.05 + 3.49 + 1.86 + 19.5 = 46.9$$\n\n**Step 3: Compute Total Government Expenditures ($E_G$)**\n\nGovernment expenditures consist of government final consumption, transfers to households, and interest payments.\n1.  **Government Final Consumption ($G_c$)**: This is the sum of government purchases from all activities.\n    $$G_c = \\sum_i g_i$$\n    $G_c = 5 + 7 + 12 + 10 + 6 = 40$\n\n2.  **Transfers to Households ($Tr_{GH}$)**:\n    $Tr_{GH} = tr^{G\\to H} = 8$\n\n3.  **Interest Payments ($Int_G$)**:\n    $Int_G = r^G = 2$\n\nTotal Government Expenditures, $E_G$, is the sum of these outlays:\n$$E_G = G_c + Tr_{GH} + Int_G$$\n$$E_G = 40 + 8 + 2 = 50$$\n\n**Step 4: Compute Government Savings ($S_G$)**\n\nFinally, we compute government savings by subtracting total expenditures from total revenues:\n$$S_G = R_G - E_G$$\n$$S_G = 46.9 - 50 = -3.1$$\n\nThe resulting value of government savings, $S_G$, is $-3.1$ billion domestic currency units. The negative sign indicates a government deficit, which is recorded in the SAM as a payment from the Capital account to the Government account.",
            "answer": "$$\\boxed{-3.1}$$"
        },
        {
            "introduction": "In contrast to the macroeconomic scope of top-down models, bottom-up optimization models capture the detailed engineering and economic constraints of specific energy technologies. This exercise delves into the practical craft of formulating a Mixed-Integer Linear Program (MILP) for a classic energy problem—Unit Commitment. By analyzing the role of \"big-$M$\" constants in linking continuous generation decisions to discrete on/off states, you will learn how careful formulation is critical for creating a \"tight\" model that solves efficiently and avoids non-physical solutions in its linear programming relaxation .",
            "id": "4105995",
            "problem": "A single thermal generating unit is modeled over discrete hourly time steps using a Mixed-Integer Linear Programming (MILP) Unit Commitment (UC) formulation. Let $t \\in \\{1,2,3\\}$ denote time periods, with the unit’s on/off status $y_t \\in \\{0,1\\}$, generation $P_t \\ge 0$, and the Linear Programming (LP) relaxation permitting $y_t \\in [0,1]$. The generator has minimum and maximum output bounds and symmetric generation ramp limits. The following constraints enforce output bounds, ramping, and on/off linkage in a simplified big-$M$ relaxation framework:\n- Output bounds: $P_t \\le P^{\\max} y_t$ and $P_t \\ge P^{\\min} y_t$ for all $t$.\n- Upward ramp: $P_t - P_{t-1} \\le R^{\\mathrm{U}} + M^{\\mathrm{up}} (1 - y_{t-1})$ for all $t \\ge 2$.\n- Downward ramp: $P_{t-1} - P_t \\le R^{\\mathrm{D}} + M^{\\mathrm{down}} (1 - y_t)$ for all $t \\ge 2$.\n\nHere $P^{\\min}$ denotes the minimum stable output when online, $P^{\\max}$ the maximum output, $R^{\\mathrm{U}}$ the upward ramp limit per hour when staying online, and $R^{\\mathrm{D}}$ the downward ramp limit per hour when staying online. The big-$M$ constants $M^{\\mathrm{up}}$ and $M^{\\mathrm{down}}$ are intended to render the ramp constraints redundant when the unit transitions from off to on or on to off, respectively. Consider the specific instance with $P^{\\min} = 120$ megawatts, $P^{\\max} = 275$ megawatts, $R^{\\mathrm{U}} = 73$ megawatts per hour, $R^{\\mathrm{D}} = 91$ megawatts per hour, hourly discretization ($1$ hour), and initial conditions $y_1$ free and $P_1$ free subject to the above bounds.\n\nStarting from first principles of MILP and LP relaxations, explain why overly large big-$M$ constants in the above ramping constraints and any big-$M$ approach to minimum generation enforcement can create weak LP relaxations that admit nonphysical fractional $y_t$ solutions with artificially loosened ramps and output levels. Then, derive the tightest values of $M^{\\mathrm{up}}$ and $M^{\\mathrm{down}}$ that ensure each big-$M$ term renders its respective ramp constraint redundant exactly when the corresponding $y$ equals $0$, and does so without restricting physically feasible transitions implied by the output bounds. Compute these minimal values for the given instance.\n\nProvide your final answer as a single row matrix containing $M^{\\mathrm{up}}$ and $M^{\\mathrm{down}}$, rounded to three significant figures. Express the values in megawatts (MW).",
            "solution": "The problem requires an explanation of the detrimental effect of overly large big-$M$ constants on the Linear Programming (LP) relaxation of a Mixed-Integer Linear Programming (MILP) model, and the derivation of the tightest possible values for such constants in a given Unit Commitment (UC) formulation.\n\nFirst, we address the conceptual part of the problem. A MILP solver typically uses a branch-and-bound algorithm, which relies on solving LP relaxations of the problem at various nodes of a search tree. In an LP relaxation, integer constraints like $y_t \\in \\{0, 1\\}$ are relaxed to continuous constraints, here $y_t \\in [0, 1]$. The solution to this LP relaxation provides a bound on the optimal integer solution. The 'tightness' of this relaxation is critical: a tighter relaxation, whose feasible region more closely approximates the convex hull of the true integer feasible solutions, provides a better bound. This allows the solver to prune branches of the search tree more effectively, leading to significantly faster solution times.\n\nOverly large big-$M$ constants weaken this LP relaxation. Let's analyze the upward ramp constraint:\n$$P_t - P_{t-1} \\le R^{\\mathrm{U}} + M^{\\mathrm{up}} (1 - y_{t-1})$$\nIn the integer model, if the unit is on ($y_{t-1} = 1$), the constraint correctly becomes $P_t - P_{t-1} \\le R^{\\mathrm{U}}$. If the unit is off ($y_{t-1} = 0$), the constraint is $P_t - P_{t-1} \\le R^{\\mathrm{U}} + M^{\\mathrm{up}}$. The term $M^{\\mathrm{up}}$ is chosen to be large enough to make this inequality redundant for any feasible startup.\nHowever, in the LP relaxation, $y_{t-1}$ can take fractional values between $0$ and $1$. If $y_{t-1}$ is, for example, $0.1$, the constraint becomes $P_t - P_{t-1} \\le R^{\\mathrm{U}} + 0.9 M^{\\mathrm{up}}$. If $M^{\\mathrm{up}}$ is chosen arbitrarily large (e.g., $10^6$), the right-hand side becomes enormous. This allows the LP solution to have a large, non-physical difference between $P_t$ and $P_{t-1}$ even for a very small commitment status $y_{t-1}$. The LP solver might exploit this weakness to find a fractional solution with a better objective value, but this solution is far from any physically realizable integer solution, providing a poor bound for the MILP search.\n\nSimilarly, the problem mentions \"any big-$M$ approach to minimum generation enforcement\". The formulation provided, $P_t \\ge P^{\\min} y_t$, is actually a tight formulation that does not use a big-$M$ constant. However, a hypothetical weaker formulation might be $P_t \\ge P^{\\min} - M(1 - y_t)$. If $y_t=1$, this correctly enforces $P_t \\ge P^{\\min}$. If $y_t=0$, it becomes $P_t \\ge P^{\\min} - M$; for this to be non-restrictive (as $P_t$ must be $0$), $M$ must be at least $P^{\\min}$. In the LP relaxation, if $y_t = 0.1$ and we choose an overly large $M$, say $M=P^{\\max}$, the constraint becomes $P_t \\ge P^{\\min} - 0.9 P^{\\max}$. Since $P^{\\max} > P^{\\min}$, this lower bound is negative, and the constraint reduces to $P_t \\ge 0$. In contrast, the tight formulation $P_t \\ge P^{\\min} y_t$ would enforce $P_t \\ge 0.1 P^{\\min}$, a much stronger and more informative bound. This illustrates how poorly chosen big-$M$ values create weak LP relaxations.\n\nNext, we derive the tightest (i.e., smallest possible) valid values for $M^{\\mathrm{up}}$ and $M^{\\mathrm{down}}$. A big-$M$ value is considered tight if it is just large enough to render the constraint inactive for all valid integer scenarios it is intended to cover, without being excessively large.\n\nDerivation of $M^{\\mathrm{up}}$:\nThe upward ramp constraint is $P_t - P_{t-1} \\le R^{\\mathrm{U}} + M^{\\mathrm{up}} (1 - y_{t-1})$.\nThe big-$M$ term is active when $y_{t-1}=0$. This corresponds to a unit that is offline at time $t-1$. In this state, the generation output must be zero, so $P_{t-1}=0$. The constraint simplifies to:\n$$P_t \\le R^{\\mathrm{U}} + M^{\\mathrm{up}}$$\nThis inequality must hold for any valid power output $P_t$ at time $t$, given that the unit was off at $t-1$. The unit might remain off ($y_t=0, P_t=0$), or it might start up ($y_t=1$). In the case of a startup, the power output $P_t$ is bounded by $P^{\\min} \\le P_t \\le P^{\\max}$. To ensure the constraint is never violated, it must hold for the maximum possible value of $P_t$. In this simplified model without explicit startup limits, the unit can theoretically start up to its maximum capacity. Thus, the maximum value for $P_t$ is $P^{\\max}$.\nTo make the constraint redundant, we must have:\n$$P^{\\max} \\le R^{\\mathrm{U}} + M^{\\mathrm{up}}$$\nTo find the tightest (smallest) value of $M^{\\mathrm{up}}$, we set it to the minimum value that satisfies this condition:\n$$M^{\\mathrm{up}} = P^{\\max} - R^{\\mathrm{U}}$$\n\nDerivation of $M^{\\mathrm{down}}$:\nThe downward ramp constraint is $P_{t-1} - P_t \\le R^{\\mathrm{D}} + M^{\\mathrm{down}} (1 - y_t)$.\nThe big-$M$ term is active when $y_t=0$. This corresponds to a unit that is offline at time $t$. In this state, its generation output must be zero, so $P_t=0$. The constraint simplifies to:\n$$P_{t-1} \\le R^{\\mathrm{D}} + M^{\\mathrm{down}}$$\nThis inequality must hold for any valid power output $P_{t-1}$ at time $t-1$, given that the unit shuts down at time $t$. The unit might have been off ($y_{t-1}=0, P_{t-1}=0$), or it might have been on ($y_{t-1}=1$). In the case where it was on, its power output $P_{t-1}$ was bounded by $P^{\\min} \\le P_{t-1} \\le P^{\\max}$. To ensure the constraint is never violated, it must hold for the maximum possible value of $P_{t-1}$. This is $P^{\\max}$.\nTo make the constraint redundant, we must have:\n$$P^{\\max} \\le R^{\\mathrm{D}} + M^{\\mathrm{down}}$$\nThe tightest (smallest) value of $M^{\\mathrm{down}}$ is therefore:\n$$M^{\\mathrm{down}} = P^{\\max} - R^{\\mathrm{D}}$$\n\nFinally, we compute the numerical values for the given instance:\n$P^{\\min} = 120$ MW\n$P^{\\max} = 275$ MW\n$R^{\\mathrm{U}} = 73$ MW/hr\n$R^{\\mathrm{D}} = 91$ MW/hr\n\nUsing the derived formulas:\n$$M^{\\mathrm{up}} = P^{\\max} - R^{\\mathrm{U}} = 275 - 73 = 202 \\, \\text{MW}$$\n$$M^{\\mathrm{down}} = P^{\\max} - R^{\\mathrm{D}} = 275 - 91 = 184 \\, \\text{MW}$$\n\nThese values are already expressed to three significant figures.\nThe final answer is the row matrix containing these two values.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 202 & 184 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Effective energy system modeling often requires bridging paradigms by using detailed bottom-up analyses to inform broader simulation or optimization models. This practice focuses on a key bottom-up projection technique: the technological experience curve, which posits that unit costs decline with cumulative deployment. You will learn to calibrate this model from historical data using log-linear regression and, critically, to quantify the uncertainty in your cost projections by generating statistically robust prediction intervals—a vital step for any credible analysis of future energy pathways .",
            "id": "4105975",
            "problem": "Consider an energy technology whose unit cost depends on cumulative deployment via a power-law experience curve grounded in empirical regularity. Assume the following foundational model: for cumulative deployment $Q$ measured in gigawatts (GW) and unit cost $C(Q)$ measured in dollars per kilowatt ($/kW$), the cost obeys $C(Q) = C_0 Q^{-\\beta}$ for some constant $C_0 > 0$ and learning exponent $\\beta > 0$. Under the assumption that logarithms are taken in the natural base and that the logarithmic residuals are independent and identically distributed Gaussian random variables, the model can be expressed as $\\ln C = \\alpha - \\beta \\ln Q + \\varepsilon$, where $\\alpha = \\ln C_0$ and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nYou are provided with historically observed bottom-up data as ordered pairs $(Q_t, C_t)$ with $Q_t$ in gigawatts (GW) and $C_t$ in dollars per kilowatt ($/kW$):\n$(5,2200)$, $(10,2000)$, $(20,1800)$, $(35,1600)$, $(55,1400)$, $(80,1250)$, $(105,1150)$, $(130,1075)$.\n\nYour task is to:\n- Calibrate the learning exponent $\\beta$ and intercept $\\alpha$ via log-linear regression under the stated Gaussian error assumption.\n- Using the calibrated model and a statistically principled uncertainty quantification, produce predictive distributions for future costs corresponding to three cumulative deployment scenarios derived from distinct modeling paradigms:\n    1. A top-down macroeconomic projection with $Q_{\\mathrm{TD}} = 200$ GW.\n    2. A bottom-up optimization plan output with $Q_{\\mathrm{OPT}} = 130$ GW.\n    3. A simulation-based adoption trajectory with $Q_{\\mathrm{SIM}} = 600$ GW.\n- For each scenario $Q_\\star \\in \\{Q_{\\mathrm{TD}}, Q_{\\mathrm{OPT}}, Q_{\\mathrm{SIM}}\\}$, output the predicted median unit cost and a two-sided $(1-\\alpha)$ prediction interval with $\\alpha = 0.05$ on the original cost scale in $$/kW$. Assume prediction intervals are computed for a new observation (not just the mean response) in the logarithmic space and then mapped back to the original cost space.\n\nExpress the three scenario results as lists of three floats $[m_\\star, \\ell_\\star, u_\\star]$ where $m_\\star$ is the predicted median cost, $\\ell_\\star$ is the lower bound, and $u_\\star$ is the upper bound of the two-sided $(1-\\alpha)$ prediction interval, all in $$/kW$. The final program output should be a single line containing a comma-separated list of these three lists enclosed in square brackets with no spaces, in the exact format:\n`[[m_{\\mathrm{TD}},\\ell_{\\mathrm{TD}},u_{\\mathrm{TD}}],[m_{\\mathrm{OPT}},\\ell_{\\mathrm{OPT}},u_{\\mathrm{OPT}}],[m_{\\mathrm{SIM}},\\ell_{\\mathrm{SIM}},u_{\\mathrm{SIM}}]]`.\n\nTest suite and coverage requirements:\n- Use the specified historical dataset and the three scenario cumulative deployments $Q_{\\mathrm{TD}} = 200$ GW, $Q_{\\mathrm{OPT}} = 130$ GW, and $Q_{\\mathrm{SIM}} = 600$ GW.\n- These scenarios collectively test a general case ($Q_{\\mathrm{TD}}$), a boundary case at the latest observed deployment ($Q_{\\mathrm{OPT}}$), and a far-extrapolation edge case ($Q_{\\mathrm{SIM}}$).\n- Your program must produce the single-line output containing the three lists of floats (each list corresponding to a scenario) in $$/kW$ as specified.",
            "solution": "The problem is valid as it is scientifically grounded in the established theory of technological experience curves, is mathematically well-posed with sufficient data, and is formulated objectively. It requires a standard but rigorous application of log-linear regression and statistical prediction. I will now proceed with the solution.\n\nThe problem requires us to model the relationship between cumulative deployment, $Q$, and unit cost, $C$, of an energy technology. The proposed model is a power law:\n$$C(Q) = C_0 Q^{-\\beta}$$\nwhere $C_0$ is the initial cost and $\\beta$ is the learning exponent.\n\nTo estimate the parameters $C_0$ and $\\beta$ from the provided data, we linearize the model by taking the natural logarithm of both sides:\n$$\\ln(C) = \\ln(C_0 Q^{-\\beta}) = \\ln(C_0) + \\ln(Q^{-\\beta}) = \\ln(C_0) - \\beta \\ln(Q)$$\nThe problem statement gives this in the form $\\ln C = \\alpha - \\beta \\ln Q + \\varepsilon$, where $\\alpha = \\ln C_0$ and $\\varepsilon$ is a normally distributed error term with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThis equation is in the form of a simple linear regression model, $y = \\beta_0 + \\beta_1 x + \\varepsilon$, by setting:\n$y = \\ln C$\n$x = \\ln Q$\n$\\beta_0 = \\alpha$\n$\\beta_1 = -\\beta$\n\nOur first step is to transform the given historical data $(Q_t, C_t)$ into logarithmic coordinates $(x_t, y_t) = (\\ln Q_t, \\ln C_t)$ and then perform an ordinary least squares (OLS) regression to find the best-fit estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$. From these, we can find the estimates for our original model parameters: $\\hat{\\alpha} = \\hat{\\beta}_0$ and $\\hat{\\beta} = -\\hat{\\beta}_1$.\n\nThe historical data provided consists of $n=8$ pairs:\n$Q = [5, 10, 20, 35, 55, 80, 105, 130]$ GW\n$C = [2200, 2000, 1800, 1600, 1400, 1250, 1150, 1075]$ $/kW\n\nThe second step is to use the calibrated model to make predictions for three future deployment scenarios: $Q_\\star \\in \\{200, 130, 600\\}$ GW. For each $Q_\\star$, we must calculate:\n1.  The predicted median cost, $m_\\star$.\n2.  A two-sided $95\\%$ prediction interval, $[\\ell_\\star, u_\\star]$.\n\nThe prediction is performed in the logarithmic space first. For a new deployment $Q_\\star$, we calculate its logarithm $x_\\star = \\ln Q_\\star$. The predicted value for the log-cost is:\n$$\\hat{y}_\\star = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_\\star$$\nSince the errors $\\varepsilon$ are normally distributed, the log-cost $y = \\ln C$ is also normally distributed. This means the cost $C = \\exp(y)$ follows a log-normal distribution. The median of this log-normal distribution is $\\exp(\\mathbb{E}[y])$. Our best estimate for $\\mathbb{E}[y]$ at $x_\\star$ is $\\hat{y}_\\star$. Therefore, the predicted median cost is:\n$$m_\\star = \\exp(\\hat{y}_\\star)$$\n\nTo compute the prediction interval, we must account for the uncertainty in the model parameters and the inherent variability of a new observation. A $(1-\\alpha_{CI})$ prediction interval for a new observation $y_{new}$ at a point $x_\\star$ is given by:\n$$\\hat{y}_\\star \\pm t_{n-2, \\alpha_{CI}/2} \\cdot s_e \\sqrt{1 + \\frac{1}{n} + \\frac{(x_\\star - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}}$$\nwhere:\n-   $n=8$ is the number of data points.\n-   $t_{n-2, \\alpha_{CI}/2}$ is the critical value from the t-distribution with $n-2=6$ degrees of freedom. For a $95\\%$ interval, $\\alpha_{CI}=0.05$, so we need the value at the $1 - 0.05/2 = 0.975$ percentile.\n-   $s_e$ is the standard error of the regression (also called the residual standard error), calculated as $s_e = \\sqrt{\\frac{\\sum(y_i - \\hat{y}_i)^2}{n-2}}$.\n-   $\\bar{x}$ is the mean of the observed log-deployments $x_i$.\n-   $\\sum(x_i - \\bar{x})^2$ is the sum of squared deviations for the observed $x_i$.\n\nLet the margin of error (the second term in the formula above) be denoted by $ME$. The prediction interval in log-space is $[\\hat{y}_\\star - ME, \\hat{y}_\\star + ME]$.\nTo obtain the interval on the original cost scale, we exponentiate the bounds:\n$$\\ell_\\star = \\exp(\\hat{y}_\\star - ME)$$\n$$u_\\star = \\exp(\\hat{y}_\\star + ME)$$\n\nThese calculations will be performed for each of the three scenarios, $Q_{\\mathrm{TD}} = 200$, $Q_{\\mathrm{OPT}} = 130$, and $Q_{\\mathrm{SIM}} = 600$, and the results will be formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress, t\n\ndef solve():\n    \"\"\"\n    Calibrates a power-law experience curve model using log-linear regression\n    and computes prediction intervals for future costs based on three deployment scenarios.\n    \"\"\"\n    # Step 1: Define historical data and scenario parameters\n    # Historical data: (Cumulative Deployment in GW, Unit Cost in $/kW)\n    data = np.array([\n        (5, 2200),\n        (10, 2000),\n        (20, 1800),\n        (35, 1600),\n        (55, 1400),\n        (80, 1250),\n        (105, 1150),\n        (130, 1075)\n    ])\n    \n    # Scenarios for prediction (Cumulative Deployment in GW)\n    q_scenarios = [200.0, 130.0, 600.0]  # TD, OPT, SIM\n\n    # Model: C = C_0 * Q^-beta\n    # Log-linear form: ln(C) = ln(C_0) - beta * ln(Q) => y = beta_0 + beta_1 * x\n    # where y = ln(C), x = ln(Q), beta_0 = ln(C_0), beta_1 = -beta\n\n    q_obs = data[:, 0]\n    c_obs = data[:, 1]\n    \n    # Transform data to log-space\n    x_obs = np.log(q_obs) # ln(Q)\n    y_obs = np.log(c_obs) # ln(C)\n    \n    # Step 2: Perform log-linear regression\n    # The `linregress` function from scipy provides all necessary OLS results.\n    # slope = beta_1, intercept = beta_0\n    slope, intercept, _, _, std_err_slope = linregress(x_obs, y_obs)\n    \n    # Step 3: Calculate parameters for prediction interval\n    n = len(x_obs)\n    df = n - 2  # Degrees of freedom for t-distribution\n    alpha_ci = 0.05 # Significance level for a 95% prediction interval\n    \n    # Critical t-value for a two-sided interval\n    t_crit = t.ppf(1 - alpha_ci / 2, df)\n    \n    # Mean and sum of squared deviations of the predictor variable\n    x_mean = np.mean(x_obs)\n    s_xx = np.sum((x_obs - x_mean)**2)\n    \n    # Standard error of the regression estimate (s_e)\n    # This can be derived from the standard error of the slope:\n    # std_err_slope = s_e / sqrt(S_xx) => s_e = std_err_slope * sqrt(S_xx)\n    s_e = std_err_slope * np.sqrt(s_xx)\n    \n    # Step 4: Calculate predictions and intervals for each scenario\n    results = []\n    for q_star in q_scenarios:\n        # Transform scenario deployment to log-space\n        x_star = np.log(q_star)\n        \n        # Predict the log-cost\n        y_hat_star = intercept + slope * x_star\n        \n        # Calculate the median cost by transforming back to original scale\n        # For a log-normal distribution, median = exp(mean of log-distribution)\n        median_cost = np.exp(y_hat_star)\n        \n        # Calculate the margin of error for the prediction interval in log-space\n        # The term under the square root accounts for uncertainty in the mean response\n        # plus the variability of a single future observation.\n        pred_err_term = np.sqrt(1 + (1 / n) + ((x_star - x_mean)**2 / s_xx))\n        margin_of_error_log = t_crit * s_e * pred_err_term\n        \n        # Determine the prediction interval bounds in log-space\n        log_lower_bound = y_hat_star - margin_of_error_log\n        log_upper_bound = y_hat_star + margin_of_error_log\n        \n        # Transform bounds back to the original cost scale\n        lower_bound = np.exp(log_lower_bound)\n        upper_bound = np.exp(log_upper_bound)\n        \n        results.append([median_cost, lower_bound, upper_bound])\n\n    # Step 5: Format the final output string\n    # E.g., [[m1,l1,u1],[m2,l2,u2],[m3,l3,u3]]\n    result_str_parts = []\n    for res in results:\n        # Format each scenario's result list\n        part = f\"[{res[0]},{res[1]},{res[2]}]\"\n        result_str_parts.append(part)\n        \n    final_output = f\"[{','.join(result_str_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}