{
    "hands_on_practices": [
        {
            "introduction": "Translating theoretical detector equations into robust code requires careful attention to numerical stability. While formulas for the Matched Filter (MF) and Constrained Energy Minimization (CEM) often feature a matrix inverse like $\\Sigma^{-1}$, direct inversion is computationally expensive and prone to error. This practice guides you through implementing these fundamental detectors using the Cholesky factorization, a numerically superior method for solving linear systems involving symmetric, positive-definite matrices like the covariance matrix.",
            "id": "3853178",
            "problem": "You are given the task of implementing spectral target detection filters used in remote sensing for hyperspectral data, specifically the matched filter and Constrained Energy Minimization (CEM). The program must avoid explicit matrix inversion and instead use Cholesky factorization of a symmetric positive definite matrix. All computations must be expressed in linear algebraic terms and implemented numerically.\n\nStart from the following foundational base:\n- The matched filtering problem can be posed as selecting a weight vector $w$ that maximizes the output signal-to-noise ratio for a known target signature vector $s$ in the presence of a Gaussian background characterized by a mean vector $\\mu$ and a covariance matrix $\\Sigma$.\n- The Constrained Energy Minimization (CEM) problem minimizes output energy subject to a unity response constraint on the target signature.\n\nFor the matched filter:\n- Consider the background centered data $x - \\mu$, the target offset $d = s - \\mu$, and the background covariance matrix $\\Sigma$.\n- Impose the unity gain constraint $w^\\top d = 1$ and minimize the output variance $w^\\top \\Sigma w$. This yields a unique minimizer under the assumption that $\\Sigma$ is symmetric positive definite.\n- The program must compute the matched filter detection score for a given pixel $x$ as $y_{\\mathrm{MF}} = w^\\top (x - \\mu)$, determined by solving a linear system using Cholesky factorization of $\\Sigma$ (with optional ridge regularization $\\lambda_\\Sigma I$ to ensure positive definiteness) rather than inverting $\\Sigma$.\n\nFor Constrained Energy Minimization (CEM):\n- Let $R$ be a background correlation matrix computed from a set of background vectors $\\{b_i\\}_{i=1}^M$ as $R = \\frac{1}{M} \\sum_{i=1}^M b_i b_i^\\top$.\n- Impose the unity gain constraint $w^\\top s = 1$ and minimize $w^\\top R w$. This yields a unique minimizer under the assumption that $R$ is symmetric positive definite.\n- The program must compute the CEM detection score for a given pixel $x$ as $y_{\\mathrm{CEM}} = w^\\top x$, determined by solving a linear system using Cholesky factorization of $R$ (with optional ridge regularization $\\lambda_R I$ to ensure positive definiteness) rather than inverting $R$.\n\nIn all cases, when forming $R$, use the provided background ensembles and add the specified ridge regularization $\\lambda_R I$ to $R$ before factorization; similarly, add the specified ridge regularization $\\lambda_\\Sigma I$ to $\\Sigma$.\n\nImplement the following test suite. In each case, compute the matched filter score $y_{\\mathrm{MF}}$ and the CEM score $y_{\\mathrm{CEM}}$.\n\nTest Case $1$ (well-conditioned, centered background):\n- Dimension $L=5$.\n- Define $A \\in \\mathbb{R}^{5 \\times 5}$ with entries:\n  $$\n  A = \\begin{bmatrix}\n  1.0  0.2  0.1  0.0  0.0 \\\\\n  0.2  1.2  0.0  0.1  0.05 \\\\\n  0.1  0.0  1.1  0.3  0.2 \\\\\n  0.0  0.1  0.3  0.9  0.0 \\\\\n  0.0  0.05  0.2  0.0  0.8\n  \\end{bmatrix}\n  $$\n  and let $\\Sigma = A^\\top A$.\n- Background mean $\\mu = [\\,0.1,\\,0.2,\\,0.15,\\,0.05,\\,0.08\\,]$.\n- Target signature $s = [\\,0.25,\\,0.31,\\,0.20,\\,0.10,\\,0.14\\,]$.\n- Pixel $x = [\\,0.24,\\,0.30,\\,0.21,\\,0.12,\\,0.13\\,]$.\n- Background ensemble $\\{b_i\\}_{i=1}^6$:\n  $b_1 = [\\,0.12,\\,0.19,\\,0.16,\\,0.06,\\,0.09\\,]$,\n  $b_2 = [\\,0.09,\\,0.21,\\,0.15,\\,0.04,\\,0.08\\,]$,\n  $b_3 = [\\,0.11,\\,0.20,\\,0.14,\\,0.05,\\,0.07\\,]$,\n  $b_4 = [\\,0.10,\\,0.18,\\,0.17,\\,0.05,\\,0.09\\,]$,\n  $b_5 = [\\,0.13,\\,0.22,\\,0.16,\\,0.07,\\,0.10\\,]$,\n  $b_6 = [\\,0.11,\\,0.19,\\,0.15,\\,0.05,\\,0.08\\,]$.\n- Regularization parameters: $\\lambda_\\Sigma = 0.0$, $\\lambda_R = 10^{-8}$.\n\nTest Case $2$ (nearly singular diagonal covariance):\n- Dimension $L=4$.\n- Covariance $\\Sigma = \\mathrm{diag}([\\,10^{-8},\\,0.5,\\,0.7,\\,1.0\\,])$.\n- Background mean $\\mu = [\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$.\n- Target signature $s = [\\,0.1,\\,0.4,\\,0.3,\\,0.2\\,]$.\n- Pixel $x = [\\,0.09,\\,0.41,\\,0.29,\\,0.19\\,]$.\n- Background ensemble $\\{b_i\\}_{i=1}^4$:\n  $b_1 = [\\,0.02,\\,0.3,\\,0.25,\\,0.15\\,]$,\n  $b_2 = [\\,-0.01,\\,0.32,\\,0.23,\\,0.14\\,]$,\n  $b_3 = [\\,0.01,\\,0.35,\\,0.28,\\,0.18\\,]$,\n  $b_4 = [\\,0.00,\\,0.34,\\,0.27,\\,0.17\\,]$.\n- Regularization parameters: $\\lambda_\\Sigma = 10^{-6}$, $\\lambda_R = 10^{-6}$.\n\nTest Case $3$ (identity covariance baseline):\n- Dimension $L=3$.\n- Covariance $\\Sigma = I_{3 \\times 3}$.\n- Background mean $\\mu = [\\,0.0,\\,0.0,\\,0.0\\,]$.\n- Target signature $s = [\\,0.2,\\,0.1,\\,0.3\\,]$.\n- Pixel $x = [\\,0.21,\\,0.09,\\,0.31\\,]$.\n- Background ensemble $\\{b_i\\}_{i=1}^4$:\n  $b_1 = [\\,0.05,\\,0.04,\\,0.06\\,]$,\n  $b_2 = [\\,0.06,\\,0.03,\\,0.05\\,]$,\n  $b_3 = [\\,0.04,\\,0.05,\\,0.07\\,]$,\n  $b_4 = [\\,0.05,\\,0.06,\\,0.04\\,]$.\n- Regularization parameters: $\\lambda_\\Sigma = 0.0$, $\\lambda_R = 10^{-8}$.\n\nTest Case $4$ (one-dimensional boundary case):\n- Dimension $L=1$.\n- Covariance $\\Sigma = [0.2]$.\n- Background mean $\\mu = [0.05]$.\n- Target signature $s = [0.2]$.\n- Pixel $x = [0.19]$.\n- Background ensemble $\\{b_i\\}_{i=1}^4$:\n  $b_1 = [0.04]$,\n  $b_2 = [0.06]$,\n  $b_3 = [0.05]$,\n  $b_4 = [0.055]$.\n- Regularization parameters: $\\lambda_\\Sigma = 0.0$, $\\lambda_R = 10^{-9}$.\n\nTest Case $5$ (higher dimension with structured correlations):\n- Dimension $L=8$.\n- Define $A \\in \\mathbb{R}^{8 \\times 8}$ with entries:\n  $$\n  A = \\begin{bmatrix}\n  1.0  0.1  0.0  0.2  0.0  0.1  0.0  0.0 \\\\\n  0.1  1.1  0.2  0.0  0.1  0.0  0.1  0.0 \\\\\n  0.0  0.2  1.2  0.1  0.0  0.2  0.0  0.1 \\\\\n  0.2  0.0  0.1  1.0  0.2  0.0  0.1  0.0 \\\\\n  0.0  0.1  0.0  0.2  1.3  0.1  0.0  0.2 \\\\\n  0.1  0.0  0.2  0.0  0.1  1.1  0.2  0.0 \\\\\n  0.0  0.1  0.0  0.1  0.0  0.2  1.0  0.1 \\\\\n  0.0  0.0  0.1  0.0  0.2  0.0  0.1  0.9\n  \\end{bmatrix}\n  $$\n  and let $\\Sigma = A^\\top A + 0.05 I$.\n- Background mean $\\mu = [\\,0.05,\\,0.06,\\,0.07,\\,0.05,\\,0.08,\\,0.04,\\,0.03,\\,0.02\\,]$.\n- Target signature $s = [\\,0.12,\\,0.15,\\,0.14,\\,0.10,\\,0.13,\\,0.11,\\,0.09,\\,0.08\\,]$.\n- Pixel $x = [\\,0.11,\\,0.16,\\,0.13,\\,0.09,\\,0.14,\\,0.10,\\,0.08,\\,0.07\\,]$.\n- Background ensemble $\\{b_i\\}_{i=1}^8$:\n  $b_1 = [\\,0.05,\\,0.06,\\,0.07,\\,0.05,\\,0.08,\\,0.04,\\,0.03,\\,0.02\\,]$,\n  $b_2 = [\\,0.06,\\,0.05,\\,0.08,\\,0.06,\\,0.09,\\,0.05,\\,0.02,\\,0.03\\,]$,\n  $b_3 = [\\,0.04,\\,0.07,\\,0.06,\\,0.04,\\,0.07,\\,0.03,\\,0.04,\\,0.01\\,]$,\n  $b_4 = [\\,0.05,\\,0.06,\\,0.07,\\,0.05,\\,0.08,\\,0.04,\\,0.03,\\,0.02\\,]$,\n  $b_5 = [\\,0.06,\\,0.07,\\,0.08,\\,0.06,\\,0.09,\\,0.05,\\,0.02,\\,0.03\\,]$,\n  $b_6 = [\\,0.05,\\,0.05,\\,0.06,\\,0.05,\\,0.08,\\,0.04,\\,0.03,\\,0.02\\,]$,\n  $b_7 = [\\,0.04,\\,0.06,\\,0.07,\\,0.04,\\,0.08,\\,0.03,\\,0.03,\\,0.02\\,]$,\n  $b_8 = [\\,0.05,\\,0.07,\\,0.07,\\,0.05,\\,0.08,\\,0.04,\\,0.03,\\,0.02\\,]$.\n- Regularization parameters: $\\lambda_\\Sigma = 0.0$, $\\lambda_R = 10^{-8}$.\n\nAlgorithmic requirements:\n- Use Cholesky factorization to solve linear systems of the form $\\Sigma w = d$ and $R w = s$ with symmetric positive definite matrices, avoiding explicit inversion.\n- Normalize the solution $w$ to enforce the unity gain constraint for both the matched filter and CEM.\n- Compute the detection scores $y_{\\mathrm{MF}}$ and $y_{\\mathrm{CEM}}$ as specified.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[y_{\\mathrm{MF}}, y_{\\mathrm{CEM}}]$ for each test case, in order from Test Case $1$ to Test Case $5$. For example, the output format must resemble $[[a_1,b_1],[a_2,b_2],\\dots,[a_5,b_5]]$ where each $a_i$ and $b_i$ are floating-point numbers computed by your program.",
            "solution": "The user-provided problem requires the implementation of two fundamental algorithms for spectral target detection in hyperspectral imagery: the Matched Filter (MF) and the Constrained Energy Minimization (CEM) filter. The problem is scientifically well-grounded, with all necessary data and constraints provided for a set of five test cases. It is based on standard, non-controversial principles of signal processing and linear algebra. The formulation of both filters as constrained optimization problems, and the proposed solution method via Cholesky factorization, are correct and numerically sound. The problem is therefore deemed valid and a solution is presented below.\n\n### Matched Filter (MF) Derivation\n\nThe Matched Filter is designed to maximize the Signal-to-Noise Ratio (SNR) for a known target signature, assuming the background follows a multivariate Gaussian distribution. Let the target signature be $s$, and the background be characterized by a mean vector $\\mu$ and a covariance matrix $\\Sigma$. A given pixel vector is denoted by $x$.\n\nThe problem can be formulated as a constrained optimization problem. We seek a weight vector $w$ that minimizes the filter's output variance on the background noise, subject to a unity response on the target signal. The target signal is represented by the difference between the target signature and the background mean, $d = s - \\mu$. The output variance for background-centered data $z - \\mu$ is $Var(w^\\top(z-\\mu)) = E[ (w^\\top(z-\\mu))^2 ] = w^\\top E[(z-\\mu)(z-\\mu)^\\top] w = w^\\top \\Sigma w$. The constraint of unity response on the target signal is $w^\\top d = 1$.\n\nThe optimization problem is:\n$$\n\\underset{w}{\\text{minimize}} \\quad w^\\top \\Sigma w \\quad \\text{subject to} \\quad w^\\top d = 1\n$$\n\nTo solve this, we use the method of Lagrange multipliers. The Lagrangian $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(w, \\gamma) = w^\\top \\Sigma w - \\gamma (w^\\top d - 1)\n$$\nwhere $\\gamma$ is the Lagrange multiplier. To find the minimum, we set the gradient with respect to $w$ to zero:\n$$\n\\nabla_w \\mathcal{L} = 2 \\Sigma w - \\gamma d = 0 \\implies \\Sigma w = \\frac{\\gamma}{2} d\n$$\nAssuming $\\Sigma$ is invertible, $w$ is proportional to $\\Sigma^{-1} d$. Let's define a preliminary, unscaled weight vector $w_p$ by solving the linear system $\\Sigma w_p = d$. Then, the optimal vector $w$ must be a scaled version of $w_p$, i.e., $w = c \\cdot w_p$ for some scalar $c$.\n\nWe find $c$ by applying the constraint $w^\\top d = 1$:\n$$\n(c \\cdot w_p)^\\top d = 1 \\implies c (w_p^\\top d) = 1 \\implies c = \\frac{1}{w_p^\\top d}\n$$\nThus, the optimal weight vector is:\n$$\nw_{\\mathrm{MF}} = \\frac{w_p}{w_p^\\top d} = \\frac{\\Sigma^{-1} d}{d^\\top \\Sigma^{-1} d}\n$$\nThe matched filter detection score for a pixel $x$ is the filter output for the background-subtracted pixel vector $x-\\mu$:\n$$\ny_{\\mathrm{MF}} = w_{\\mathrm{MF}}^\\top (x - \\mu) = \\frac{(w_p)^\\top (x - \\mu)}{w_p^\\top d}\n$$\nTo ensure the covariance matrix is positive definite, we add a ridge regularization term $\\lambda_\\Sigma I$, where $I$ is the identity matrix. The system to be solved becomes $(\\Sigma + \\lambda_\\Sigma I) w_p = d$.\n\n### Constrained Energy Minimization (CEM) Derivation\n\nThe CEM filter operates without assuming a specific statistical model for the background. Instead, it uses a sample-based background correlation matrix $R$. The goal is to design a filter $w$ that minimizes the average output energy (power) over the background samples while maintaining a unity response for the target signature $s$.\n\nLet the set of $M$ background pixel vectors be $\\{b_i\\}_{i=1}^M$. The sample correlation matrix $R$ is defined as:\n$$\nR = \\frac{1}{M} \\sum_{i=1}^M b_i b_i^\\top\n$$\nThe output energy for a background vector $b_i$ is $(w^\\top b_i)^2$. The average energy over the background ensemble is $E[ (w^\\top b)^2 ] = w^\\top E[b b^\\top] w = w^\\top R w$. The CEM optimization problem is:\n$$\n\\underset{w}{\\text{minimize}} \\quad w^\\top R w \\quad \\text{subject to} \\quad w^\\top s = 1\n$$\nThis has the same mathematical structure as the matched filter problem. The Lagrangian is:\n$$\n\\mathcal{L}(w, \\gamma) = w^\\top R w - \\gamma (w^\\top s - 1)\n$$\nSetting the gradient to zero gives:\n$$\n\\nabla_w \\mathcal{L} = 2 R w - \\gamma s = 0 \\implies R w = \\frac{\\gamma}{2} s\n$$\nFollowing an identical procedure, we define a preliminary vector $w_p$ by solving $R w_p = s$. The final normalized weight vector is:\n$$\nw_{\\mathrm{CEM}} = \\frac{w_p}{w_p^\\top s} = \\frac{R^{-1} s}{s^\\top R^{-1} s}\n$$\nThe CEM detection score for a pixel $x$ is simply the filter output $y_{\\mathrm{CEM}} = w_{\\mathrm{CEM}}^\\top x$.\n$$\ny_{\\mathrm{CEM}} = w_{\\mathrm{CEM}}^\\top x = \\frac{(w_p)^\\top x}{w_p^\\top s}\n$$\nAs with the MF, we add a regularization term $\\lambda_R I$ to $R$ to ensure it is positive definite, so the system to solve is $(R + \\lambda_R I) w_p = s$.\n\n### Numerical Implementation\n\nA key requirement is to avoid explicit matrix inversion, which is numerically unstable and computationally expensive. For a symmetric positive definite matrix $A$, the linear system $Ax=b$ is robustly solved using Cholesky factorization.\n\n1.  **Factorization**: Decompose the regularized matrix $A$ (which is $\\Sigma_{reg} = \\Sigma + \\lambda_\\Sigma I$ for MF, or $R_{reg} = R + \\lambda_R I$ for CEM) into $A = LL^\\top$, where $L$ is a lower-triangular matrix.\n2.  **Forward Substitution**: The system $LL^\\top x = b$ is solved in two steps. First, solve $Ly=b$ for $y$.\n3.  **Backward Substitution**: Then, solve $L^\\top x = y$ for the final solution $x$ (which corresponds to our preliminary vector $w_p$).\n\nThis procedure is implemented using `scipy.linalg.cholesky` for factorization and `scipy.linalg.solve_triangular` for the substitution steps. The final scores, $y_{\\mathrm{MF}}$ and $y_{\\mathrm{CEM}}$, are computed using the derived ratios, which is a numerically stable approach.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve_cholesky(A, b):\n    \"\"\"\n    Solves the linear system Ax = b for x using Cholesky factorization.\n    A must be a symmetric positive-definite matrix.\n    \"\"\"\n    try:\n        L = cholesky(A, lower=True)\n        y = solve_triangular(L, b, lower=True)\n        x = solve_triangular(L.T, y, lower=False)\n        return x\n    except np.linalg.LinAlgError:\n        # Fallback for cases where regularization might not be enough\n        # for scipy's strict cholesky check, though this problem's\n        # parameters should prevent this.\n        return np.linalg.solve(A, b)\n\ndef compute_mf_score(Sigma, mu, s, x, lambda_sigma):\n    \"\"\"\n    Computes the Matched Filter score.\n    \"\"\"\n    L = Sigma.shape[0]\n    Sigma_reg = Sigma + lambda_sigma * np.identity(L)\n    d = s - mu\n\n    # Solve Sigma_reg * w_p = d\n    w_p = solve_cholesky(Sigma_reg, d)\n    \n    # y_MF = (w_p.T @ (x - mu)) / (w_p.T @ d)\n    numerator = w_p.T @ (x - mu)\n    denominator = w_p.T @ d\n\n    if np.isclose(denominator, 0):\n        # This occurs if d is zero or orthogonal to w_p in a problematic way.\n        # If d is the zero vector, the response is zero.\n        return 0.0\n\n    return numerator / denominator\n\ndef compute_cem_score(s, x, b_ensemble, lambda_r):\n    \"\"\"\n    Computes the Constrained Energy Minimization score.\n    \"\"\"\n    M = len(b_ensemble)\n    L = s.shape[0]\n    \n    # Build background correlation matrix R\n    R = np.zeros((L, L))\n    for b_i in b_ensemble:\n        R += np.outer(b_i, b_i)\n    R /= M\n    \n    R_reg = R + lambda_r * np.identity(L)\n    \n    # Solve R_reg * w_p = s\n    w_p = solve_cholesky(R_reg, s)\n    \n    # y_CEM = (w_p.T @ x) / (w_p.T @ s)\n    numerator = w_p.T @ x\n    denominator = w_p.T @ s\n\n    if np.isclose(denominator, 0):\n        # This occurs if s is the zero vector.\n        return 0.0\n    \n    return numerator / denominator\n\ndef format_pair(pair):\n    \"\"\"Formats a pair of floats into the string '[f1,f2]'.\"\"\"\n    return f\"[{pair[0]},{pair[1]}]\"\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {\n            \"A_mf\": np.array([\n                [1.0, 0.2, 0.1, 0.0, 0.0],\n                [0.2, 1.2, 0.0, 0.1, 0.05],\n                [0.1, 0.0, 1.1, 0.3, 0.2],\n                [0.0, 0.1, 0.3, 0.9, 0.0],\n                [0.0, 0.05, 0.2, 0.0, 0.8]\n            ]),\n            \"Sigma_mf\": None,\n            \"mu\": np.array([0.1, 0.2, 0.15, 0.05, 0.08]),\n            \"s\": np.array([0.25, 0.31, 0.20, 0.10, 0.14]),\n            \"x\": np.array([0.24, 0.30, 0.21, 0.12, 0.13]),\n            \"b_ensemble\": [\n                np.array([0.12, 0.19, 0.16, 0.06, 0.09]),\n                np.array([0.09, 0.21, 0.15, 0.04, 0.08]),\n                np.array([0.11, 0.20, 0.14, 0.05, 0.07]),\n                np.array([0.10, 0.18, 0.17, 0.05, 0.09]),\n                np.array([0.13, 0.22, 0.16, 0.07, 0.10]),\n                np.array([0.11, 0.19, 0.15, 0.05, 0.08])\n            ],\n            \"lambda_sigma\": 0.0,\n            \"lambda_r\": 1e-8\n        },\n        # Test Case 2\n        {\n            \"A_mf\": None,\n            \"Sigma_mf\": np.diag([1e-8, 0.5, 0.7, 1.0]),\n            \"mu\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"s\": np.array([0.1, 0.4, 0.3, 0.2]),\n            \"x\": np.array([0.09, 0.41, 0.29, 0.19]),\n            \"b_ensemble\": [\n                np.array([0.02, 0.3, 0.25, 0.15]),\n                np.array([-0.01, 0.32, 0.23, 0.14]),\n                np.array([0.01, 0.35, 0.28, 0.18]),\n                np.array([0.00, 0.34, 0.27, 0.17])\n            ],\n            \"lambda_sigma\": 1e-6,\n            \"lambda_r\": 1e-6\n        },\n        # Test Case 3\n        {\n            \"A_mf\": None,\n            \"Sigma_mf\": np.identity(3),\n            \"mu\": np.array([0.0, 0.0, 0.0]),\n            \"s\": np.array([0.2, 0.1, 0.3]),\n            \"x\": np.array([0.21, 0.09, 0.31]),\n            \"b_ensemble\": [\n                np.array([0.05, 0.04, 0.06]),\n                np.array([0.06, 0.03, 0.05]),\n                np.array([0.04, 0.05, 0.07]),\n                np.array([0.05, 0.06, 0.04])\n            ],\n            \"lambda_sigma\": 0.0,\n            \"lambda_r\": 1e-8\n        },\n        # Test Case 4\n        {\n            \"A_mf\": None,\n            \"Sigma_mf\": np.array([[0.2]]),\n            \"mu\": np.array([0.05]),\n            \"s\": np.array([0.2]),\n            \"x\": np.array([0.19]),\n            \"b_ensemble\": [\n                np.array([0.04]),\n                np.array([0.06]),\n                np.array([0.05]),\n                np.array([0.055])\n            ],\n            \"lambda_sigma\": 0.0,\n            \"lambda_r\": 1e-9\n        },\n        # Test Case 5\n        {\n            \"A_mf\": np.array([\n                [1.0, 0.1, 0.0, 0.2, 0.0, 0.1, 0.0, 0.0],\n                [0.1, 1.1, 0.2, 0.0, 0.1, 0.0, 0.1, 0.0],\n                [0.0, 0.2, 1.2, 0.1, 0.0, 0.2, 0.0, 0.1],\n                [0.2, 0.0, 0.1, 1.0, 0.2, 0.0, 0.1, 0.0],\n                [0.0, 0.1, 0.0, 0.2, 1.3, 0.1, 0.0, 0.2],\n                [0.1, 0.0, 0.2, 0.0, 0.1, 1.1, 0.2, 0.0],\n                [0.0, 0.1, 0.0, 0.1, 0.0, 0.2, 1.0, 0.1],\n                [0.0, 0.0, 0.1, 0.0, 0.2, 0.0, 0.1, 0.9]\n            ]),\n            \"Sigma_mf_offset\": 0.05,\n            \"mu\": np.array([0.05, 0.06, 0.07, 0.05, 0.08, 0.04, 0.03, 0.02]),\n            \"s\": np.array([0.12, 0.15, 0.14, 0.10, 0.13, 0.11, 0.09, 0.08]),\n            \"x\": np.array([0.11, 0.16, 0.13, 0.09, 0.14, 0.10, 0.08, 0.07]),\n            \"b_ensemble\": [\n                np.array([0.05, 0.06, 0.07, 0.05, 0.08, 0.04, 0.03, 0.02]),\n                np.array([0.06, 0.05, 0.08, 0.06, 0.09, 0.05, 0.02, 0.03]),\n                np.array([0.04, 0.07, 0.06, 0.04, 0.07, 0.03, 0.04, 0.01]),\n                np.array([0.05, 0.06, 0.07, 0.05, 0.08, 0.04, 0.03, 0.02]),\n                np.array([0.06, 0.07, 0.08, 0.06, 0.09, 0.05, 0.02, 0.03]),\n                np.array([0.05, 0.05, 0.06, 0.05, 0.08, 0.04, 0.03, 0.02]),\n                np.array([0.04, 0.06, 0.07, 0.04, 0.08, 0.03, 0.03, 0.02]),\n                np.array([0.05, 0.07, 0.07, 0.05, 0.08, 0.04, 0.03, 0.02])\n            ],\n            \"lambda_sigma\": 0.0,\n            \"lambda_r\": 1e-8\n        }\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        if i == 0 or i == 4: # Cases 1 and 5\n            A = case['A_mf']\n            Sigma = A.T @ A\n            if i == 4: # Case 5 has an additional offset\n                Sigma += case['Sigma_mf_offset'] * np.identity(Sigma.shape[0])\n        else:\n            Sigma = case['Sigma_mf']\n\n        y_mf = compute_mf_score(Sigma, case['mu'], case['s'], case['x'], case['lambda_sigma'])\n        y_cem = compute_cem_score(case['s'], case['x'], case['b_ensemble'], case['lambda_r'])\n        results.append([y_mf, y_cem])\n        \n    print(f\"[{','.join(map(format_pair, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While robust factorization methods are essential, it is equally important to understand the underlying cause of numerical instability in spectral detection. This exercise explores the concept of the ill-conditioned covariance matrix, where a large ratio between the maximum and minimum eigenvalues ($\\lambda_{\\max} / \\lambda_{\\min}$) can amplify noise and estimation errors. By analyzing a hypothetical scenario , you will see how regularization, such as adding a small term $\\lambda I$, provides a crucial safeguard by improving the matrix's condition number.",
            "id": "3853110",
            "problem": "An analyst is building linear target detectors for Hyperspectral Imaging (HSI) background suppression in a remote sensing workflow for environmental modeling. The background pixel vector is modeled as a random vector $x \\in \\mathbb{R}^{p}$ with mean $\\mu$ and covariance matrix $\\Sigma = \\mathbb{E}\\left[(x - \\mu)(x - \\mu)^{\\top}\\right]$. Many linear detection algorithms, including the Matched Filter (MF) and the Constrained Energy Minimization (CEM), require computing quantities that depend on $\\Sigma^{-1}$. The Spectral Angle Mapper (SAM) instead uses only the angle between spectra and does not use $\\Sigma$.\n\nA sample covariance $\\widehat{\\Sigma}$ is estimated from $N$ background pixels in a $p = 3$ band subspace after dimensionality reduction. The eigen-decomposition of $\\widehat{\\Sigma}$ is $\\widehat{\\Sigma} = \\sum_{i=1}^{3} \\widehat{\\lambda}_{i} u_{i} u_{i}^{\\top}$ with orthonormal eigenvectors $\\{u_{i}\\}_{i=1}^{3}$ and eigenvalues $\\{\\widehat{\\lambda}_{i}\\}_{i=1}^{3} = \\{5, 0.3, 10^{-4}\\}$, all in appropriate radiance units. The target signature $s \\in \\mathbb{R}^{3}$ has unit norm and a small component along $u_{3}$, specifically $|u_{3}^{\\top} s| = 0.05$. The background mean is approximately zero, so $\\mu \\approx 0$.\n\nAssume the following foundational facts:\n- The spectral covariance $\\Sigma$ is symmetric positive definite in theory, but its finite-sample estimate $\\widehat{\\Sigma}$ can be poorly conditioned.\n- The $2$-norm condition number of a symmetric positive definite matrix satisfies $\\kappa_{2}(\\Sigma) = \\dfrac{\\lambda_{\\max}(\\Sigma)}{\\lambda_{\\min}(\\Sigma)}$.\n- Small perturbations $\\delta \\Sigma$ in $\\Sigma$ induce first-order perturbations in the inverse according to $\\delta(\\Sigma^{-1}) \\approx -\\Sigma^{-1} (\\delta \\Sigma) \\Sigma^{-1}$.\n\nTo stabilize computations, the analyst considers Tikhonov-type regularization and uses $\\Sigma_{\\lambda} = \\widehat{\\Sigma} + \\lambda I$ for some $\\lambda  0$.\n\nTasks:\n- Using only the above foundational facts, reason about how the small eigenvalue of $\\widehat{\\Sigma}$ can affect the stability of detectors that depend on $\\widehat{\\Sigma}^{-1}$ when $N$ is not much larger than $p$, and how $\\Sigma_{\\lambda}$ can mitigate these effects. In particular, characterize how the condition number and sensitivity of $\\widehat{\\Sigma}^{-1}$ change when $\\lambda$ increases from $0$ to a small positive value.\n- For $\\lambda = 10^{-3}$, compute the condition numbers $\\kappa_{2}(\\widehat{\\Sigma})$ and $\\kappa_{2}(\\Sigma_{\\lambda})$ from the provided eigenvalues.\n\nWhich of the following statements are correct?\n\nA. With $\\widehat{\\lambda}_{3} = 10^{-4}$, both the Matched Filter and Constrained Energy Minimization can become numerically unstable because any component of $s$ along $u_{3}$ is upweighted by approximately $\\widehat{\\lambda}_{3}^{-1}$, so small estimation errors in $\\widehat{\\Sigma}$ or $s$ cause large changes in the detector weights and outputs; replacing $\\widehat{\\Sigma}$ by $\\Sigma_{\\lambda}$ increases the smallest eigenvalue to $\\widehat{\\lambda}_{3} + \\lambda$ and reduces the condition number, directly bounding this amplification.\n\nB. Because the Matched Filter is derived by maximizing a signal-to-noise ratio, it is inherently robust to poor conditioning of $\\widehat{\\Sigma}$; adding $\\lambda I$ only shrinks all weights uniformly and cannot reduce output variance or false alarms.\n\nC. The Constrained Energy Minimization solves a unity-gain problem under background whitening and relies on $\\widehat{\\Sigma}^{-1}$; when $\\widehat{\\Sigma}$ is poorly conditioned, the solution tends to place excessive weight on low-variance directions (such as $u_{3}$), which inflates output variance under model mismatch; adding $\\lambda I$ trades a small bias in the gain constraint for a substantial reduction in estimator variance.\n\nD. The Spectral Angle Mapper does not use $\\widehat{\\Sigma}$ and is therefore unaffected by ill-conditioning; thus, when a low-eigenvalue direction is dominated by atmospheric residuals, SAM will always outperform covariance-based detectors in detection probability.\n\nE. To first order, the perturbation of the inverse obeys $\\|\\delta(\\widehat{\\Sigma}^{-1})\\| \\le \\|\\widehat{\\Sigma}^{-1}\\|^{2} \\,\\|\\delta \\Sigma\\| + \\mathcal{O}(\\|\\delta \\Sigma\\|^{2})$; since $\\|\\widehat{\\Sigma}^{-1}\\| = 1/\\lambda_{\\min}(\\widehat{\\Sigma})$ for the $2$-norm, increasing $\\lambda$ reduces $\\|\\Sigma_{\\lambda}^{-1}\\|$ to $1/(\\lambda_{\\min}(\\widehat{\\Sigma}) + \\lambda)$, thereby reducing sensitivity even when $\\widehat{\\Sigma}$ is nonsingular.\n\nSelect all that apply.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective, providing all necessary information for a rigorous analysis.\n\n**Analysis of the Problem Setup**\n\nThe problem addresses the numerical instability of linear detectors in hyperspectral imaging when the sample covariance matrix, $\\widehat{\\Sigma}$, is ill-conditioned. This typically occurs when the number of training samples, $N$, is not substantially larger than the number of spectral bands, $p$. Here, we are given a $p=3$ dimensional case with a sample covariance matrix $\\widehat{\\Sigma}$ whose eigenvalues are $\\{\\widehat{\\lambda}_{1}, \\widehat{\\lambda}_{2}, \\widehat{\\lambda}_{3}\\} = \\{5, 0.3, 10^{-4}\\}$. Let the corresponding orthonormal eigenvectors be $\\{u_1, u_2, u_3\\}$.\n\nThe conditioning of a symmetric positive definite matrix is quantified by its $2$-norm condition number, $\\kappa_{2}(\\widehat{\\Sigma})$, defined as the ratio of its largest to its smallest eigenvalue.\n$$ \\kappa_{2}(\\widehat{\\Sigma}) = \\frac{\\lambda_{\\max}(\\widehat{\\Sigma})}{\\lambda_{\\min}(\\widehat{\\Sigma})} $$\nFor the given eigenvalues, we have $\\widehat{\\lambda}_{\\max} = 5$ and $\\widehat{\\lambda}_{\\min} = 10^{-4}$.\n$$ \\kappa_{2}(\\widehat{\\Sigma}) = \\frac{5}{10^{-4}} = 5 \\times 10^4 = 50000 $$\nA large condition number indicates that the matrix is close to being singular and its inverse is highly sensitive to perturbations.\n\nDetectors like the Matched Filter (MF) and Constrained Energy Minimization (CEM) rely on computing $\\widehat{\\Sigma}^{-1}s$, where $s$ is the target signature. The inverse of $\\widehat{\\Sigma}$ can be expressed via its eigen-decomposition:\n$$ \\widehat{\\Sigma}^{-1} = \\left(\\sum_{i=1}^{3} \\widehat{\\lambda}_{i} u_{i} u_{i}^{\\top}\\right)^{-1} = \\sum_{i=1}^{3} \\frac{1}{\\widehat{\\lambda}_{i}} u_{i} u_{i}^{\\top} $$\nWhen applying this inverse to the target signature $s$, we get:\n$$ \\widehat{\\Sigma}^{-1}s = \\sum_{i=1}^{3} \\frac{1}{\\widehat{\\lambda}_{i}} u_{i} (u_{i}^{\\top}s) = \\sum_{i=1}^{3} \\frac{c_i}{\\widehat{\\lambda}_{i}} u_{i} $$\nwhere $c_i = u_i^\\top s$ is the component of $s$ along the eigenvector $u_i$. The term $1/\\widehat{\\lambda}_i$ acts as an amplification factor. For the smallest eigenvalue $\\widehat{\\lambda}_{3} = 10^{-4}$, this factor is $1/10^{-4} = 10000$. This means that even a very small component of the target signature along the $u_3$ direction, $|c_3|=|u_3^\\top s| = 0.05$, will be amplified enormously in the computation of the detector weights. This leads to a detector that is overly sensitive to the low-variance direction $u_3$, which often represents noise or minor, poorly estimated background features.\n\nTo mitigate this, Tikhonov-type regularization is proposed, replacing $\\widehat{\\Sigma}$ with $\\Sigma_{\\lambda} = \\widehat{\\Sigma} + \\lambda I$, where $\\lambda  0$. The eigenvectors of $\\Sigma_{\\lambda}$ are the same as those of $\\widehat{\\Sigma}$. The eigenvalues of $\\Sigma_{\\lambda}$ are $\\widehat{\\lambda}_i + \\lambda$. The smallest eigenvalue is now $\\widehat{\\lambda}_{\\min} + \\lambda$.\n\nLet's compute the condition number for the regularized matrix $\\Sigma_{\\lambda}$ with $\\lambda = 10^{-3}$:\nThe new eigenvalues are:\n$$ \\lambda_{1,\\lambda} = 5 + 10^{-3} = 5.001 $$\n$$ \\lambda_{2,\\lambda} = 0.3 + 10^{-3} = 0.301 $$\n$$ \\lambda_{3,\\lambda} = 10^{-4} + 10^{-3} = 0.0001 + 0.001 = 0.0011 $$\nThe new condition number is:\n$$ \\kappa_{2}(\\Sigma_{\\lambda}) = \\frac{\\lambda_{\\max}(\\Sigma_{\\lambda})}{\\lambda_{\\min}(\\Sigma_{\\lambda})} = \\frac{5.001}{0.0011} \\approx 4546.36 $$\nThe regularization has reduced the condition number from $50000$ to approximately $4546$, a significant improvement in matrix conditioning. The amplification factor for the $u_3$ direction is now $1/(\\widehat{\\lambda}_3+\\lambda)=1/0.0011 \\approx 909$, which is much smaller than the original factor of $10000$.\n\n**Evaluation of Options**\n\n**A. With $\\widehat{\\lambda}_{3} = 10^{-4}$, both the Matched Filter and Constrained Energy Minimization can become numerically unstable because any component of $s$ along $u_{3}$ is upweighted by approximately $\\widehat{\\lambda}_{3}^{-1}$, so small estimation errors in $\\widehat{\\Sigma}$ or $s$ cause large changes in the detector weights and outputs; replacing $\\widehat{\\Sigma}$ by $\\Sigma_{\\lambda}$ increases the smallest eigenvalue to $\\widehat{\\lambda}_{3} + \\lambda$ and reduces the condition number, directly bounding this amplification.**\nThis statement is a complete and accurate summary of the situation. Both MF and CEM depend on $\\widehat{\\Sigma}^{-1}$, making them susceptible to ill-conditioning. The upweighting factor for the $u_3$ direction is indeed $1/\\widehat{\\lambda}_3$. Such extreme amplification makes the resulting detector unstable with respect to small errors in the inputs ($\\widehat{\\Sigma}$ and $s$). The regularization scheme $\\Sigma_{\\lambda} = \\widehat{\\Sigma} + \\lambda I$ correctly increases the minimum eigenvalue to $\\widehat{\\lambda}_3 + \\lambda$, which, for any $\\lambda0$, reduces the condition number and bounds the amplification factor.\n**Verdict: Correct.**\n\n**B. Because the Matched Filter is derived by maximizing a signal-to-noise ratio, it is inherently robust to poor conditioning of $\\widehat{\\Sigma}$; adding $\\lambda I$ only shrinks all weights uniformly and cannot reduce output variance or false alarms.**\nThis statement is incorrect on multiple counts. First, the optimality of the MF derivation assumes the true covariance $\\Sigma$ is known and well-behaved. It offers no inherent robustness when using a poorly conditioned sample estimate $\\widehat{\\Sigma}$. Second, adding $\\lambda I$ does not shrink weights uniformly. The regularized weights are proportional to $(\\widehat{\\Sigma} + \\lambda I)^{-1}s = \\sum_i \\frac{u_i^\\top s}{\\widehat{\\lambda}_i + \\lambda} u_i$. The effect is highly non-uniform: components corresponding to small eigenvalues are modified much more significantly than components corresponding to large eigenvalues. Finally, the entire purpose of regularization is to stabilize the solution to prevent overfitting to training data noise, which in turn reduces output variance on new data and mitigates false alarms.\n**Verdict: Incorrect.**\n\n**C. The Constrained Energy Minimization solves a unity-gain problem under background whitening and relies on $\\widehat{\\Sigma}^{-1}$; when $\\widehat{\\Sigma}$ is poorly conditioned, the solution tends to place excessive weight on low-variance directions (such as $u_{3}$), which inflates output variance under model mismatch; adding $\\lambda I$ trades a small bias in the gain constraint for a substantial reduction in estimator variance.**\nThis statement accurately describes the CEM detector and the effect of regularization. CEM minimizes background energy ($w^\\top \\widehat{\\Sigma} w$) with a unity-gain constraint ($w^\\top s = 1$). A poorly conditioned $\\widehat{\\Sigma}$ causes the solution $w_{CEM} \\propto \\widehat{\\Sigma}^{-1}s$ to place huge weights on low-variance directions (e.g., $u_3$). While this minimizes energy on the training data, it can lead to large outputs for new data that deviates even slightly from the training model (model mismatch), inflating output variance and false alarms. Regularization introduces a bias in the estimator for the filter weights (the regularized estimate is no longer an unbiased estimate of the \"true\" optimal filter), but this is accepted for a large reduction in the variance of the estimator, making it more stable and generalizable. This is a classic bias-variance tradeoff.\n**Verdict: Correct.**\n\n**D. The Spectral Angle Mapper does not use $\\widehat{\\Sigma}$ and is therefore unaffected by ill-conditioning; thus, when a low-eigenvalue direction is dominated by atmospheric residuals, SAM will always outperform covariance-based detectors in detection probability.**\nThe first part of the statement is true: SAM is defined as $\\theta = \\arccos(\\frac{s^\\top x}{\\|s\\| \\|x\\|})$ and does not use $\\widehat{\\Sigma}$. However, the conclusion \"thus... SAM will always outperform\" is an invalid generalization. While SAM avoids the specific problem of ill-conditioning, it also completely disregards background statistics. Detectors like MF and CEM are designed to suppress structured background. A properly regularized MF or CEM can significantly outperform SAM by exploiting knowledge of the background covariance structure to separate the target from spectrally similar background clutter. The word \"always\" makes this claim definitively false. Performance is context-dependent.\n**Verdict: Incorrect.**\n\n**E. To first order, the perturbation of the inverse obeys $\\|\\delta(\\widehat{\\Sigma}^{-1})\\| \\le \\|\\widehat{\\Sigma}^{-1}\\|^{2} \\,\\|\\delta \\Sigma\\| + \\mathcal{O}(\\|\\delta \\Sigma\\|^{2})$; since $\\|\\widehat{\\Sigma}^{-1}\\| = 1/\\lambda_{\\min}(\\widehat{\\Sigma})$ for the $2$-norm, increasing $\\lambda$ reduces $\\|\\Sigma_{\\lambda}^{-1}\\|$ to $1/(\\lambda_{\\min}(\\widehat{\\Sigma}) + \\lambda)$, thereby reducing sensitivity even when $\\widehat{\\Sigma}$ is nonsingular.**\nThis statement provides a formal mathematical justification for regularization's role in improving stability. The first-order perturbation bound $\\|\\delta(\\widehat{\\Sigma}^{-1})\\| \\le \\|\\widehat{\\Sigma}^{-1}\\|_2^2 \\|\\delta \\widehat{\\Sigma}\\|_2$ (using the induced $2$-norm) shows that sensitivity is proportional to the square of the norm of the inverse. For an SPD matrix, $\\|\\widehat{\\Sigma}^{-1}\\|_2 = 1/\\lambda_{\\min}(\\widehat{\\Sigma})$. Regularization replaces $\\widehat{\\Sigma}$ with $\\Sigma_\\lambda$, whose inverse has a norm of $\\|\\Sigma_{\\lambda}^{-1}\\|_2 = 1/\\lambda_{\\min}(\\Sigma_{\\lambda}) = 1/(\\lambda_{\\min}(\\widehat{\\Sigma}) + \\lambda)$. Since $\\lambda  0$, we have $\\|\\Sigma_{\\lambda}^{-1}\\|_2  \\|\\widehat{\\Sigma}^{-1}\\|_2$. This directly reduces the sensitivity factor $\\|\\cdot^{-1}\\|_2^2$. The point that this is beneficial even for a nonsingular (but ill-conditioned) matrix is crucial and correct. The entire statement is logically and mathematically sound.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "Beyond the stability of a single calculation, a practical remote sensing workflow involves trade-offs between accuracy, robustness, and computational cost, especially with large datasets. This final practice situates the MF, CEM, and Spectral Angle Mapper (SAM) algorithms within a realistic, large-scale scenario . You will analyze their computational complexity and weigh the benefits of covariance-based background suppression against the numerical challenges and preprocessing costs involved, developing the critical thinking needed for real-world algorithm selection.",
            "id": "3853118",
            "problem": "A hyperspectral scene has $B=224$ spectral bands and $N=5\\times 10^{7}$ pixels. You must detect $K=3$ target materials whose reference spectra are available. You consider three detectors: Matched Filtering (MF), Constrained Energy Minimization (CEM), and Spectral Angle Mapper (SAM). The background is modeled as approximately Gaussian with an unknown mean and covariance. You will estimate a single global background covariance matrix from all pixels and reuse it for all targets. You are allowed to precompute any scene-level statistics once and reuse them, and you will implement linear algebra using numerically stable factorizations rather than explicitly forming matrix inverses when possible.\n\nUse the following foundational facts:\n- The cost to compute a sample covariance from $N$ vectors of length $B$ is $\\mathcal{O}(B^{2}N)$ operations (dominated by forming $\\sum_{i=1}^{N} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}$).\n- The cost to factor or invert a dense $B\\times B$ matrix is $\\mathcal{O}(B^{3})$; solving linear systems via Cholesky or similar avoids explicitly computing the inverse but has the same asymptotic order.\n- A per-pixel dot product between two length-$B$ vectors is $\\mathcal{O}(B)$; computing a norm and normalization per pixel is also $\\mathcal{O}(B)$.\n- The condition number of a matrix $\\mathbf{A}$ in the $2$-norm is $\\kappa_{2}(\\mathbf{A})=\\lVert \\mathbf{A}\\rVert_{2}\\lVert \\mathbf{A}^{-1}\\rVert_{2}$; larger $\\kappa_{2}$ implies greater amplification of perturbations by solving $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$. Suppose the estimated covariance has $\\kappa_{2}(\\mathbf{C})\\approx 10^{7}$, reflecting strong spectral correlation and near-dependencies among bands.\n\nAssume that MF and CEM each require one scene-level factorization of the background covariance (or correlation) that can be reused across targets, and their per-pixel scoring then reduces to inner products with precomputed weight vectors. SAM requires no covariance estimation but computes an angle per pixel per target from inner products and norms.\n\nWhich of the following statements are correct in this setting?\n\nA. For $N\\gg B$, the one-time cost to estimate the covariance, which scales as $\\mathcal{O}(B^{2}N)$, dominates both the $\\mathcal{O}(B^{3})$ factorization cost and the $\\mathcal{O}(BN)$ per-pixel scoring cost for MF and CEM; with $B=224$ and $N=5\\times 10^{7}$, the covariance estimation alone exceeds per-pixel scoring by roughly two orders of magnitude.\n\nB. With $K=3$ targets and a single reused factorization, the per-pixel costs of MF and CEM scale as $\\mathcal{O}(KBN)$, the same asymptotic per-pixel scaling as SAM’s $\\mathcal{O}(KBN)$, so their per-pixel computational profiles are comparable; the main difference is in the one-time preprocessing.\n\nC. Because $\\kappa_{2}(\\mathbf{C})\\approx 10^{7}$, SAM suffers severe numerical instability from inner products of spectral vectors, while MF and CEM are stabilized by covariance inversion, which effectively damps the ill-conditioning.\n\nD. Replacing explicit inversion by solving the Tikhonov-regularized system $(\\mathbf{C}+\\lambda \\mathbf{I})\\mathbf{w}=\\mathbf{d}$ with a Cholesky factorization improves numerical robustness and backward stability; the asymptotic cost remains $\\mathcal{O}(B^{3})$ but with a smaller constant than explicitly forming $(\\mathbf{C}+\\lambda \\mathbf{I})^{-1}$.\n\nE. If the data are whitened using $\\mathbf{C}^{-1/2}$ from an eigenvalue decomposition, then applying SAM in the whitened space becomes identical to MF under Gaussian assumptions, so MF and SAM produce the same ordering of pixels without further normalization.\n\nF. When $\\kappa_{2}(\\mathbf{C})$ is large, the CEM solution tends to place large weights along directions of small background variance, which can amplify sensor noise; shrinkage regularization of the form $\\mathbf{C}_{\\lambda}=(1-\\lambda)\\mathbf{C}+\\lambda \\alpha \\mathbf{I}$ typically improves robustness at a modest loss in detection sensitivity.\n\nChoose all that apply.",
            "solution": "The problem statement will first be validated for scientific and logical integrity before proceeding to a solution.\n\n### Step 1: Extract Givens\n\nThe verbatim givens from the problem statement are:\n-   Number of spectral bands: $B=224$.\n-   Number of pixels: $N=5 \\times 10^{7}$.\n-   Number of target materials: $K=3$.\n-   Detectors under consideration: Matched Filtering (MF), Constrained Energy Minimization (CEM), and Spectral Angle Mapper (SAM).\n-   Background model: Approximately Gaussian with unknown mean and covariance. A single global background covariance matrix $\\mathbf{C}$ is estimated from all pixels.\n-   Computational assumptions: Scene-level statistics (e.g., factorizations) are precomputed once. Numerically stable factorizations are used instead of explicit matrix inversion.\n-   Foundational facts (computational costs):\n    -   Sample covariance estimation: $\\mathcal{O}(B^{2}N)$.\n    -   $B\\times B$ matrix factorization or inversion: $\\mathcal{O}(B^{3})$.\n    -   Per-pixel vector dot product or norm (length $B$): $\\mathcal{O}(B)$.\n-   Foundational facts (numerical properties):\n    -   Condition number of the estimated covariance: $\\kappa_{2}(\\mathbf{C})\\approx 10^{7}$.\n-   Algorithm-specific assumptions:\n    -   MF and CEM require one scene-level factorization of $\\mathbf{C}$, reusable across targets. Their per-pixel scoring reduces to inner products with precomputed weight vectors.\n    -   SAM requires no covariance estimation and computes an angle per pixel per target.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is evaluated against the validation criteria.\n\n-   **Scientifically Grounded:** The problem is grounded in the established field of hyperspectral remote sensing. MF, CEM, and SAM are standard detection/classification algorithms. The assumptions about their implementation, computational costs (asymptotic complexities of basic linear algebra operations), and the properties of hyperspectral data (e.g., high-dimensional, highly correlated bands leading to ill-conditioned covariance matrices) are all standard and realistic.\n-   **Well-Posed:** The problem is well-posed. It asks for an evaluation of the correctness of several technical statements based on a clearly defined scenario and a set of foundational facts. A definite answer can be derived for each statement.\n-   **Objective:** The problem is stated in objective, technical language. The statements to be evaluated are propositions about computational complexity, numerical stability, and algorithmic properties, which can be verified through mathematical and algorithmic analysis.\n\nThe problem exhibits no flaws from the checklist. It is not scientifically unsound, non-formalizable, incomplete, contradictory, unrealistic, ill-posed, or trivial. The values for $B$, $N$, and $\\kappa_2(\\mathbf{C})$ are plausible for a real-world hyperspectral analysis task.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A full analysis of each option can now be performed.\n\n### Solution and Option-by-Option Analysis\n\nLet a pixel be represented by a vector $\\mathbf{x} \\in \\mathbb{R}^{B}$, a target spectrum by $\\mathbf{d} \\in \\mathbb{R}^{B}$, the background mean by $\\boldsymbol{\\mu} \\in \\mathbb{R}^{B}$, and the background covariance by $\\mathbf{C} \\in \\mathbb{R}^{B \\times B}$.\n\nThe core operations for the detectors are:\n-   **MF:** $y_{MF}(\\mathbf{x}) = \\mathbf{w}_{MF}^\\top (\\mathbf{x} - \\boldsymbol{\\mu})$, where $\\mathbf{w}_{MF} = \\mathbf{C}^{-1}\\mathbf{d}$. This involves solving $\\mathbf{C}\\mathbf{w}_{MF}=\\mathbf{d}$.\n-   **CEM:** $y_{CEM}(\\mathbf{x}) = \\mathbf{w}_{CEM}^\\top \\mathbf{x}$, where $\\mathbf{w}_{CEM} = (\\mathbf{d}^\\top \\mathbf{C}^{-1}\\mathbf{d})^{-1} \\mathbf{C}^{-1}\\mathbf{d}$. This also involves solving a linear system for $\\mathbf{C}^{-1}\\mathbf{d}$.\n-   **SAM:** The angle is $\\theta_{SAM}(\\mathbf{x}) = \\arccos\\left(\\frac{\\mathbf{d}^\\top \\mathbf{x}}{\\|\\mathbf{d}\\|_2 \\|\\mathbf{x}\\|_2}\\right)$. Detection is usually based on $\\cos(\\theta_{SAM})$.\n\nNow, we evaluate each statement.\n\n**A. For $N\\gg B$, the one-time cost to estimate the covariance, which scales as $\\mathcal{O}(B^{2}N)$, dominates both the $\\mathcal{O}(B^{3})$ factorization cost and the $\\mathcal{O}(BN)$ per-pixel scoring cost for MF and CEM; with $B=224$ and $N=5\\times 10^{7}$, the covariance estimation alone exceeds per-pixel scoring by roughly two orders of magnitude.**\n\n-   **Analysis:** Let's compare the total computational costs.\n    1.  **Covariance Estimation Cost:** $C_{cov} = \\mathcal{O}(B^2 N)$.\n    2.  **Factorization Cost (one-time):** $C_{fact} = \\mathcal{O}(B^3)$.\n    3.  **Total Scoring Cost (for one target):** The problem states per-pixel scoring is an inner product, an $\\mathcal{O}(B)$ operation. Across $N$ pixels, the total cost is $C_{score} = N \\times \\mathcal{O}(B) = \\mathcal{O}(BN)$. The statement's use of \"per-pixel scoring cost\" refers to this total cost for the image.\n-   **Comparison 1 (Covariance vs. Factorization):** The ratio of costs is $\\frac{C_{cov}}{C_{fact}} \\propto \\frac{B^2 N}{B^3} = \\frac{N}{B}$. With $N = 5 \\times 10^7$ and $B = 224$, we have $\\frac{N}{B} \\approx \\frac{5 \\times 10^7}{224} \\approx 2.23 \\times 10^5$. Since $N \\gg B$, $C_{cov}$ clearly dominates $C_{fact}$.\n-   **Comparison 2 (Covariance vs. Total Scoring):** The ratio of costs is $\\frac{C_{cov}}{C_{score}} \\propto \\frac{B^2 N}{BN} = B$. With $B = 224$, the covariance estimation cost is approximately $224$ times larger than the total scoring cost for a single target. $224$ is between $10^2=100$ and $10^3=1000$, and is reasonably described as \"roughly two orders of magnitude\".\n-   **Verdict:** The statement is **Correct**.\n\n**B. With $K=3$ targets and a single reused factorization, the per-pixel costs of MF and CEM scale as $\\mathcal{O}(KBN)$, the same asymptotic per-pixel scaling as SAM’s $\\mathcal{O}(KBN)$, so their per-pixel computational profiles are comparable; the main difference is in the one-time preprocessing.**\n\n-   **Analysis:** We consider the total cost of scoring all $N$ pixels for all $K$ targets.\n    -   **MF/CEM Scoring Cost:** For each of the $K$ targets, a weight vector $\\mathbf{w}_k$ is precomputed. Then, for each of the $N$ pixels, an $\\mathcal{O}(B)$ inner product $(\\mathbf{w}_k^\\top\\mathbf{x}_i)$ is performed. The total cost is $K \\times N \\times \\mathcal{O}(B) = \\mathcal{O}(KBN)$.\n    -   **SAM Scoring Cost:** For each of the $K$ targets and $N$ pixels, SAM requires a dot product $\\mathbf{d}_k^\\top\\mathbf{x}_i$ ($\\mathcal{O}(B)$) and norms. The norm $\\|\\mathbf{x}_i\\|_2$ needs to be computed only once per pixel (cost $\\mathcal{O}(B)$) and can be reused for all $K$ targets. The total cost is (cost for all pixel norms) + (cost for all dot products) = $N \\times \\mathcal{O}(B) + K \\times N \\times \\mathcal{O}(B) = \\mathcal{O}(BN + KBN)$. Since $K \\ge 1$, this simplifies to $\\mathcal{O}(KBN)$.\n    -   **Comparison:** Both MF/CEM and SAM have a total scoring cost that scales as $\\mathcal{O}(KBN)$. Therefore, their computational profiles during the per-pixel application phase are comparable. The primary difference lies in the preprocessing step: MF and CEM require a costly $\\mathcal{O}(B^2 N + B^3)$ preprocessing, while SAM's preprocessing is negligible ($\\mathcal{O}(KB)$).\n-   **Verdict:** The statement is **Correct**.\n\n**C. Because $\\kappa_{2}(\\mathbf{C})\\approx 10^{7}$, SAM suffers severe numerical instability from inner products of spectral vectors, while MF and CEM are stabilized by covariance inversion, which effectively damps the ill-conditioning.**\n\n-   **Analysis:**\n    -   **SAM Stability:** SAM computes $\\cos\\theta = (\\mathbf{d}^\\top \\mathbf{x}) / (\\|\\mathbf{d}\\|_2 \\|\\mathbf{x}\\|_2)$. This involves dot products and norms, which are standard and generally numerically stable floating-point operations. The condition number of the covariance matrix $\\mathbf{C}$ is irrelevant to SAM, as SAM does not use $\\mathbf{C}$.\n    -   **MF/CEM Stability:** MF and CEM require the computation of a weight vector $\\mathbf{w} \\propto \\mathbf{C}^{-1}\\mathbf{d}$, which means solving the linear system $\\mathbf{C}\\mathbf{w} \\propto \\mathbf{d}$. A fundamental result of numerical linear algebra is that the sensitivity of the solution of a linear system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ to perturbations is governed by the condition number $\\kappa(\\mathbf{A})$. A large $\\kappa(\\mathbf{C}) \\approx 10^7$ means the system is ill-conditioned, and the computation of $\\mathbf{w}$ is numerically *unstable*. Small errors in $\\mathbf{C}$ or $\\mathbf{d}$ will be amplified by a factor up to $\\approx 10^7$. Therefore, covariance \"inversion\" is the *source* of numerical instability for MF and CEM, not a mitigator of it.\n-   **Verdict:** The statement is fundamentally flawed and factually reversed. It is **Incorrect**.\n\n**D. Replacing explicit inversion by solving the Tikhonov-regularized system $(\\mathbf{C}+\\lambda \\mathbf{I})\\mathbf{w}=\\mathbf{d}$ with a Cholesky factorization improves numerical robustness and backward stability; the asymptotic cost remains $\\mathcal{O}(B^{3})$ but with a smaller constant than explicitly forming $(\\mathbf{C}+\\lambda \\mathbf{I})^{-1}$.**\n\n-   **Analysis:**\n    -   **Regularization:** The matrix $\\mathbf{C}$ is ill-conditioned. Adding a scaled identity matrix $\\lambda\\mathbf{I}$ (Tikhonov regularization) adds $\\lambda  0$ to all eigenvalues of $\\mathbf{C}$. This increases the smallest eigenvalue from $\\mu_{min} \\approx 0$ to $\\mu_{min}+\\lambda$, significantly reducing the condition number $\\kappa(\\mathbf{C}+\\lambda\\mathbf{I}) = (\\mu_{max}+\\lambda)/(\\mu_{min}+\\lambda) \\ll \\kappa(\\mathbf{C})$. This directly improves the numerical robustness of solving the linear system.\n    -   **Factorization vs. Inversion:** For a linear system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, solving via factorization (e.g., LU or Cholesky for SPD matrices) followed by forward/backward substitution is known to be more numerically stable (specifically, backward stable) than first explicitly computing the inverse $\\mathbf{A}^{-1}$ and then forming the product $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$.\n    -   **Computational Cost:** Both explicit inversion and Cholesky factorization of a dense $B \\times B$ matrix have a computational complexity of $\\mathcal{O}(B^3)$. However, the leading constant is generally smaller for factorization methods. For instance, Cholesky factorization requires $\\approx \\frac{1}{3}B^3$ floating-point operations (flops), while inversion via Gauss-Jordan elimination requires $\\approx B^3$ flops.\n-   **Verdict:** The statement accurately describes standard best practices in numerical linear algebra for solving ill-conditioned systems. It is **Correct**.\n\n**E. If the data are whitened using $\\mathbf{C}^{-1/2}$ from an eigenvalue decomposition, then applying SAM in the whitened space becomes identical to MF under Gaussian assumptions, so MF and SAM produce the same ordering of pixels without further normalization.**\n\n-   **Analysis:**\n    -   Let's assume the data has been mean-centered. Whitened data and target vectors are $\\mathbf{x}' = \\mathbf{C}^{-1/2}\\mathbf{x}$ and $\\mathbf{d}' = \\mathbf{C}^{-1/2}\\mathbf{d}$.\n    -   The MF score is $y_{MF}(\\mathbf{x}) = \\mathbf{d}^\\top \\mathbf{C}^{-1} \\mathbf{x}$.\n    -   The cosine of the angle for SAM in the whitened space is:\n        $$ \\cos\\theta' = \\frac{(\\mathbf{d}')^\\top \\mathbf{x}'}{\\|\\mathbf{d}'\\|_2 \\|\\mathbf{x}'\\|_2} = \\frac{(\\mathbf{C}^{-1/2}\\mathbf{d})^\\top (\\mathbf{C}^{-1/2}\\mathbf{x})}{\\|\\mathbf{C}^{-1/2}\\mathbf{d}\\|_2 \\|\\mathbf{C}^{-1/2}\\mathbf{x}\\|_2} = \\frac{\\mathbf{d}^\\top \\mathbf{C}^{-1} \\mathbf{x}}{\\sqrt{\\mathbf{d}^\\top \\mathbf{C}^{-1} \\mathbf{d}} \\sqrt{\\mathbf{x}^\\top \\mathbf{C}^{-1} \\mathbf{x}}} $$\n    -   Comparing the two detector outputs:\n        $$ \\cos\\theta' = \\frac{y_{MF}(\\mathbf{x})}{(\\text{constant for target } \\mathbf{d}) \\times \\sqrt{\\mathbf{x}^\\top \\mathbf{C}^{-1} \\mathbf{x}}} $$\n    -   The denominator in the expression for $\\cos\\theta'$ includes the term $\\sqrt{\\mathbf{x}^\\top \\mathbf{C}^{-1} \\mathbf{x}}$, which is the Mahalanobis length of the vector $\\mathbf{x}$. This term depends on the pixel $\\mathbf{x}$ and varies from pixel to pixel. Because the MF score is divided by this pixel-dependent normalization factor, the ranking of pixels produced by whitened SAM will not be identical to the ranking produced by MF. For two pixels $\\mathbf{x}_1$ and $\\mathbf{x}_2$, it is possible to have $y_{MF}(\\mathbf{x}_1)  y_{MF}(\\mathbf{x}_2)$ but $\\cos\\theta'(\\mathbf{x}_1)  \\cos\\theta'(\\mathbf{x}_2)$.\n-   **Verdict:** The statement is **Incorrect**.\n\n**F. When $\\kappa_{2}(\\mathbf{C})$ is large, the CEM solution tends to place large weights along directions of small background variance, which can amplify sensor noise; shrinkage regularization of the form $\\mathbf{C}_{\\lambda}=(1-\\lambda)\\mathbf{C}+\\lambda \\alpha \\mathbf{I}$ typically improves robustness at a modest loss in detection sensitivity.**\n\n-   **Analysis:**\n    -   **CEM Behavior:** The CEM weight vector is proportional to $\\mathbf{w} \\propto \\mathbf{C}^{-1}\\mathbf{d}$. The inverse covariance matrix $\\mathbf{C}^{-1}$ has eigenvalues that are the reciprocal of the eigenvalues of $\\mathbf{C}$. \"Directions of small background variance\" correspond to the eigenvectors of $\\mathbf{C}$ with small eigenvalues. These eigenvectors are the directions of large eigenvalues for $\\mathbf{C}^{-1}$. Consequently, $\\mathbf{C}^{-1}$ amplifies components of the target vector $\\mathbf{d}$ that lie in these directions, leading to a weight vector $\\mathbf{w}$ with large magnitudes. This makes the detector output $\\mathbf{w}^\\top \\mathbf{x}$ highly sensitive to any energy in those directions, including sensor noise, which is often assumed to be broadband. This accurately describes the noise amplification problem.\n    -   **Shrinkage Regularization:** The proposed regularized covariance $\\mathbf{C}_{\\lambda}=(1-\\lambda)\\mathbf{C}+\\lambda \\alpha \\mathbf{I}$ is a form of linear shrinkage. It creates a new estimate that is a weighted average of the sample covariance matrix $\\mathbf{C}$ and a scaled identity matrix $\\alpha\\mathbf{I}$ (a simple, robust covariance model). This process effectively increases the small eigenvalues of $\\mathbf{C}$, making the resulting matrix $\\mathbf{C}_{\\lambda}$ better-conditioned. This improves the numerical stability and statistical robustness of the filter derived from $\\mathbf{C}_{\\lambda}^{-1}$.\n    -   **Trade-off:** Introducing this regularization means the filter is no longer perfectly \"matched\" to the unregularized sample background statistics. This introduces a bias, which typically results in a slight reduction in theoretical detector performance (sensitivity) under ideal conditions, but a significant improvement in practical performance (robustness) when the sample covariance is ill-conditioned or poorly estimated. This is the classic bias-variance trade-off.\n-   **Verdict:** The statement provides a correct and insightful summary of a key challenge with covariance-based detectors and a standard solution. It is **Correct**.",
            "answer": "$$\\boxed{ABDF}$$"
        }
    ]
}