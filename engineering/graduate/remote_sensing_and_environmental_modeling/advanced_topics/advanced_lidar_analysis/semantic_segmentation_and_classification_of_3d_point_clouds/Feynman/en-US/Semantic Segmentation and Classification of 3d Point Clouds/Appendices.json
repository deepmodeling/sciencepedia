{
    "hands_on_practices": [
        {
            "introduction": "This practice addresses the fundamental task of point cloud registration, which is essential for comparing or fusing datasets acquired at different times or from different sensors. Before any meaningful analysis like change detection can occur, point clouds must be aligned within a common coordinate system. This exercise  guides you through the derivation of the rigid transformation from first principles and its estimation using the elegant and powerful Singular Value Decomposition (SVD) method, providing a cornerstone skill for any 3D data scientist.",
            "id": "3844964",
            "problem": "In a multi-temporal airborne Light Detection and Ranging (LiDAR) remote sensing campaign over a mixed urban–forest environment, two three-dimensional point clouds are acquired in different coordinate frames: a sensor-local frame and a georeferenced map frame. To propagate semantic segmentation labels from one acquisition to the other and to perform consistent environmental modeling across epochs, one needs to estimate a rigid transformation between the two frames that preserves Euclidean distances and orientation. Let the rigid transformation be defined by the mapping $p \\mapsto p'$ with $p, p' \\in \\mathbb{R}^{3}$ such that $p' = R p + t$, where $R \\in \\mathbb{R}^{3 \\times 3}$ is a rotation matrix and $t \\in \\mathbb{R}^{3}$ is a translation vector.\n\nStarting only from foundational principles of Euclidean geometry and least squares estimation, do the following:\n1. Derive the form $p' = R p + t$ under the constraint of distance preservation and proper orientation, showing that $R$ must satisfy $R^{\\top} R = I$ and $\\det(R) = 1$.\n2. Formulate the problem of estimating $(R, t)$ from $N$ corresponding points $\\{(p_i, q_i)\\}_{i=1}^{N}$ as the minimization of the sum of squared residuals $J(R, t) = \\sum_{i=1}^{N} \\| q_i - (R p_i + t) \\|^{2}$, and derive, by centering with the sample means, the reduced objective in terms of a rotation-only optimization. Show that this rotation-only problem is the orthogonal Procrustes problem, and that its solution can be obtained via the Singular Value Decomposition (SVD), ensuring $\\det(R) = 1$.\n3. Consider the following six correspondences extracted from stable, manually verified keypoints on planar roofs and vertical facades (chosen to be symmetric about the sensor’s local origin to improve numerical conditioning):\n   - Sensor-local coordinates (source points): $p_1 = (1, 0, 0)$, $p_2 = (-1, 0, 0)$, $p_3 = (0, 1, 0)$, $p_4 = (0, -1, 0)$, $p_5 = (0, 0, 1)$, $p_6 = (0, 0, -1)$.\n   - Georeferenced map frame (target points): $q_i = R p_i + t$ with an unknown $(R, t)$ to be estimated.\n   Assume the true transformation consists of a rotation about the $z$-axis by $\\pi/2$ radians and a translation $t = (2, -1, 3)$, but do not use these values directly in your derivation; instead, apply the SVD-based orthogonal Procrustes solution to the given correspondences. Explicitly compute the sample means, the cross-covariance, and the SVD to recover $R$ and $t$.\n4. Express the final answer as a single row matrix using the LaTeX $\\texttt{pmatrix}$ environment that lists the entries of $R$ in row-major order followed by the three entries of $t$. No rounding is required. No units are required.\n\nYour final answer must be a calculation in the specified row matrix form.",
            "solution": "The problem is valid as it is scientifically grounded in Euclidean geometry and estimation theory, well-posed with sufficient data for a unique solution, and stated objectively. It represents a standard problem in 3D data processing, specifically the registration of point clouds. We will proceed with a full derivation and solution.\n\nThe solution is presented in three parts as requested by the problem statement, followed by the final formatted answer.\n\n1. Derivation of the Rigid Transformation Properties\n\nA rigid transformation is a mapping $T: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{3}$ that preserves the Euclidean distance between any pair of points. Let the transformation be an affine map of the form $p' = T(p) = R p + t$, where $p, p' \\in \\mathbb{R}^{3}$, $R$ is a $3 \\times 3$ matrix, and $t$ is a $3 \\times 1$ translation vector.\n\nLet $p_1$ and $p_2$ be two points in $\\mathbb{R}^{3}$. Their transformed counterparts are $p'_1 = R p_1 + t$ and $p'_2 = R p_2 + t$. The vector connecting the transformed points is $p'_1 - p'_2 = (R p_1 + t) - (R p_2 + t) = R(p_1 - p_2)$.\n\nThe condition of distance preservation states that the squared Euclidean distance is invariant under the transformation:\n$$ \\| p'_1 - p'_2 \\|^{2} = \\| p_1 - p_2 \\|^{2} $$\nSubstituting the expression for the transformed vector:\n$$ \\| R(p_1 - p_2) \\|^{2} = \\| p_1 - p_2 \\|^{2} $$\nLet $v = p_1 - p_2$. This relation must hold for any vector $v \\in \\mathbb{R}^{3}$. Using the definition of the squared Euclidean norm, $\\|x\\|^2 = x^{\\top}x$:\n$$ (Rv)^{\\top}(Rv) = v^{\\top}v $$\n$$ v^{\\top}R^{\\top}Rv = v^{\\top}Iv $$\nwhere $I$ is the $3 \\times 3$ identity matrix. Since this equality must hold for all $v \\in \\mathbb{R}^{3}$, the quadratic forms must be identical, which implies their defining matrices are equal:\n$$ R^{\\top}R = I $$\nA matrix $R$ that satisfies this condition is called an orthogonal matrix. This demonstrates that the linear part of a rigid transformation must be orthogonal.\n\nThe problem further requires that the transformation preserves orientation. The orientation of a coordinate system is determined by the sign of the determinant of its basis vectors. A transformation matrix $R$ preserves orientation if its determinant is positive. From the orthogonality condition $R^{\\top}R = I$, we can take the determinant of both sides:\n$$ \\det(R^{\\top}R) = \\det(I) $$\n$$ \\det(R^{\\top})\\det(R) = 1 $$\nSince $\\det(R^{\\top}) = \\det(R)$, this leads to:\n$$ (\\det(R))^{2} = 1 $$\nThis implies that $\\det(R)$ can be either $1$ or $-1$.\n- If $\\det(R) = 1$, the transformation is a proper rotation. It preserves the \"handedness\" of the coordinate system.\n- If $\\det(R) = -1$, the transformation is an improper rotation (a roto-reflection), which includes a reflection and inverts the handedness of the coordinate system.\n\nTo preserve orientation, we must select the case where $\\det(R) = 1$. Such a matrix is a member of the special orthogonal group $SO(3)$. Thus, a rigid transformation that preserves distances and orientation has the form $p' = Rp + t$ where $R$ is a special orthogonal matrix ($R^{\\top}R = I$ and $\\det(R)=1$).\n\n2. Least Squares Formulation and Derivation of the Orthogonal Procrustes Solution\n\nGiven $N$ pairs of corresponding points $\\{(p_i, q_i)\\}_{i=1}^{N}$, we seek to find the rotation $R \\in SO(3)$ and translation $t \\in \\mathbb{R}^{3}$ that minimize the sum of squared residuals (or errors):\n$$ J(R, t) = \\sum_{i=1}^{N} \\| q_i - (R p_i + t) \\|^{2} $$\nTo find the optimal translation $t$, we can minimize $J$ with respect to $t$ by setting its gradient to zero.\n$$ \\frac{\\partial J}{\\partial t} = \\sum_{i=1}^{N} \\frac{\\partial}{\\partial t} \\| q_i - R p_i - t \\|^{2} = \\sum_{i=1}^{N} 2(q_i - R p_i - t)(-1) = 0 $$\n$$ \\sum_{i=1}^{N} (q_i - R p_i - t) = 0 $$\n$$ \\sum_{i=1}^{N} q_i - R \\sum_{i=1}^{N} p_i - \\sum_{i=1}^{N} t = 0 $$\nLet $\\bar{p} = \\frac{1}{N}\\sum_{i=1}^{N} p_i$ and $\\bar{q} = \\frac{1}{N}\\sum_{i=1}^{N} q_i$ be the centroids (sample means) of the two point sets.\n$$ N\\bar{q} - R(N\\bar{p}) - Nt = 0 $$\n$$ t = \\bar{q} - R\\bar{p} $$\nThis shows that the optimal translation $t$ depends on the optimal rotation $R$ and the centroids of the point sets.\n\nWe can substitute this expression for $t$ back into the objective function to obtain a reduced objective that depends only on $R$:\n$$ q_i - R p_i - t = q_i - R p_i - (\\bar{q} - R\\bar{p}) = (q_i - \\bar{q}) - R(p_i - \\bar{p}) $$\nLet the centered point coordinates be $p'_i = p_i - \\bar{p}$ and $q'_i = q_i - \\bar{q}$. The objective function becomes:\n$$ J(R) = \\sum_{i=1}^{N} \\| q'_i - R p'_i \\|^{2} $$\nExpanding the squared norm:\n$$ J(R) = \\sum_{i=1}^{N} (q'_i - R p'_i)^{\\top}(q'_i - R p'_i) = \\sum_{i=1}^{N} ( (q'_i)^{\\top}q'_i - (q'_i)^{\\top}R p'_i - (p'_i)^{\\top}R^{\\top}q'_i + (p'_i)^{\\top}R^{\\top}R p'_i ) $$\nSince $R$ is orthogonal, $R^{\\top}R = I$. Also, $(p'_i)^{\\top}R^{\\top}q'_i$ is a scalar, so it is equal to its transpose, $((p'_i)^{\\top}R^{\\top}q'_i)^{\\top} = (q'_i)^{\\top}R p'_i$.\n$$ J(R) = \\sum_{i=1}^{N} (\\|q'_i\\|^{2} + \\|p'_i\\|^{2}) - 2 \\sum_{i=1}^{N} (q'_i)^{\\top}R p'_i $$\nThe terms $\\sum \\|q'_i\\|^{2}$ and $\\sum \\|p'_i\\|^{2}$ are constant with respect to $R$. Therefore, minimizing $J(R)$ is equivalent to maximizing the term $\\sum_{i=1}^{N} (q'_i)^{\\top}R p'_i$.\n\nWe use the trace operator to convert this sum into a matrix expression. For any scalar $s$, $s = \\text{tr}(s)$.\n$$ \\sum_{i=1}^{N} (q'_i)^{\\top}R p'_i = \\sum_{i=1}^{N} \\text{tr}((q'_i)^{\\top}R p'_i) = \\sum_{i=1}^{N} \\text{tr}(p'_i (q'_i)^{\\top}R) $$\nUsing the linearity of the trace, this is equivalent to maximizing:\n$$ \\text{tr}\\left(\\left(\\sum_{i=1}^{N} p'_i (q'_i)^{\\top}\\right)R\\right) = \\text{tr}(HR) $$\nwhere $H = \\sum_{i=1}^{N} p'_i (q'_i)^{\\top}$ is the $3 \\times 3$ cross-covariance matrix of the centered point sets. Using the cyclic property of the trace, $\\text{tr}(HR) = \\text{tr}(RH)$. We wish to maximize $\\text{tr}(RH)$. This is the orthogonal Procrustes problem.\n\nLet the Singular Value Decomposition (SVD) of $H$ be $H = U S V^{\\top}$, where $U, V \\in O(3)$ are orthogonal matrices and $S$ is a diagonal matrix of non-negative singular values, $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$.\n$$ \\text{tr}(RH) = \\text{tr}(R U S V^{\\top}) $$\nUsing the cyclic property, $\\text{tr}(ABCD) = \\text{tr}(DABC)$:\n$$ \\text{tr}(R U S V^{\\top}) = \\text{tr}(V^{\\top} R U S) $$\nLet $M = V^{\\top} R U$. Since $V$, $R$, and $U$ are orthogonal, $M$ is also an orthogonal matrix. The objective becomes maximizing $\\text{tr}(MS)$.\n$$ \\text{tr}(MS) = \\sum_{j=1}^{3} (MS)_{jj} = \\sum_{j=1}^{3} M_{jj} \\sigma_j $$\nSince $M$ is orthogonal, its column vectors are unit vectors, which implies $|M_{jj}| \\le 1$. To maximize this sum, we should choose $M_{jj}$ to be as large as possible. The optimal choice is $M=I$, which gives a maximum value of $\\sum \\sigma_j$.\nSo, $V^{\\top} R U = I \\implies R = V I U^{\\top} = V U^{\\top}$.\n\nWe must ensure $\\det(R)=1$. We have $\\det(R) = \\det(V U^{\\top}) = \\det(V)\\det(U^{\\top}) = \\det(V)\\det(U)$. This product can be $-1$ if one of $\\det(U)$ or $\\det(V)$ is $-1$ and the other is $1$. This corresponds to a reflection, which is the mathematically closest orthogonal matrix but not a proper rotation. If $\\det(VU^\\top) = -1$, the optimal rotation is found by forcing the determinant to be $1$. This is achieved by setting $M=\\text{diag}(1, 1, -1)$ instead of $I$, which flips the sign of the smallest singular value term, giving a trace of $\\sigma_1 + \\sigma_2 - \\sigma_3$. This is the best possible result under the $\\det(M) = -1$ constraint (assuming $\\det(U)\\det(V)=1$; if $\\det(U)\\det(V)=-1$, we need $\\det(M)=-1$ to get $\\det(R)=1$).\n\nA robust way to write the solution for $R \\in SO(3)$ is:\n$$ R = V \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & \\det(VU^{\\top}) \\end{pmatrix} U^{\\top} $$\nThis ensures that $\\det(R) = \\det(V)\\det(U^{\\top})\\det(\\text{diag}(...)) = \\det(VU^{\\top})^2 = 1$.\n\n3. Application to the Given Keypoints\n\nThe source points are:\n$p_1 = (1, 0, 0)^{\\top}$, $p_2 = (-1, 0, 0)^{\\top}$, $p_3 = (0, 1, 0)^{\\top}$, $p_4 = (0, -1, 0)^{\\top}$, $p_5 = (0, 0, 1)^{\\top}$, $p_6 = (0, 0, -1)^{\\top}$.\nThe number of points is $N=6$.\n\nFirst, compute the centroid of the source points:\n$$ \\bar{p} = \\frac{1}{6}\\sum_{i=1}^{6} p_i = \\frac{1}{6} \\begin{pmatrix} 1-1+0+0+0+0 \\\\ 0+0+1-1+0+0 \\\\ 0+0+0+0+1-1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe target points $q_i$ are generated by $q_i = R_{true} p_i + t_{true}$ where $R_{true}$ is a rotation about the $z$-axis by $\\pi/2$ radians and $t_{true}=(2, -1, 3)^{\\top}$.\n$$ R_{true} = \\begin{pmatrix} \\cos(\\pi/2) & -\\sin(\\pi/2) & 0 \\\\ \\sin(\\pi/2) & \\cos(\\pi/2) & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} $$\nThe target points are:\n$q_1 = (0, 1, 0)^{\\top} + (2, -1, 3)^{\\top} = (2, 0, 3)^{\\top}$\n$q_2 = (0, -1, 0)^{\\top} + (2, -1, 3)^{\\top} = (2, -2, 3)^{\\top}$\n$q_3 = (-1, 0, 0)^{\\top} + (2, -1, 3)^{\\top} = (1, -1, 3)^{\\top}$\n$q_4 = (1, 0, 0)^{\\top} + (2, -1, 3)^{\\top} = (3, -1, 3)^{\\top}$\n$q_5 = (0, 0, 1)^{\\top} + (2, -1, 3)^{\\top} = (2, -1, 4)^{\\top}$\n$q_6 = (0, 0, -1)^{\\top} + (2, -1, 3)^{\\top} = (2, -1, 2)^{\\top}$\n\nNext, compute the centroid of the target points:\n$$ \\bar{q} = \\frac{1}{6}\\left( \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -2 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 3 \\\\ -1 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\\\ 2 \\end{pmatrix} \\right) = \\frac{1}{6}\\begin{pmatrix} 12 \\\\ -6 \\\\ 18 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} $$\nThe centered points are $p'_i = p_i - \\bar{p} = p_i$ and $q'_i = q_i - \\bar{q}$.\n$q'_1 = (0, 1, 0)^{\\top}$, $q'_2= (0, -1, 0)^{\\top}$, $q'_3 = (-1, 0, 0)^{\\top}$, $q'_4 = (1, 0, 0)^{\\top}$, $q'_5 = (0, 0, 1)^{\\top}$, $q'_6 = (0, 0, -1)^{\\top}$.\n\nNow, compute the cross-covariance matrix $H = \\sum_{i=1}^{6} p'_i (q'_i)^{\\top}$:\n$H = p_1 (q'_1)^{\\top} + p_2 (q'_2)^{\\top} + p_3 (q'_3)^{\\top} + p_4 (q'_4)^{\\top} + p_5 (q'_5)^{\\top} + p_6 (q'_6)^{\\top}$\n$H = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}(0,1,0) + \\begin{pmatrix}-1\\\\0\\\\0\\end{pmatrix}(0,-1,0) + \\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}(-1,0,0) + \\begin{pmatrix}0\\\\-1\\\\0\\end{pmatrix}(1,0,0) + \\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}(0,0,1) + \\begin{pmatrix}0\\\\0\\\\-1\\end{pmatrix}(0,0,-1)$\n$H = \\begin{pmatrix}0&1&0\\\\0&0&0\\\\0&0&0\\end{pmatrix} + \\begin{pmatrix}0&1&0\\\\0&0&0\\\\0&0&0\\end{pmatrix} + \\begin{pmatrix}0&0&0\\\\-1&0&0\\\\0&0&0\\end{pmatrix} + \\begin{pmatrix}0&0&0\\\\-1&0&0\\\\0&0&0\\end{pmatrix} + \\begin{pmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{pmatrix} + \\begin{pmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{pmatrix}$\n$$ H = \\begin{pmatrix} 0 & 2 & 0 \\\\ -2 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} $$\nNext, find the SVD of $H = U S V^{\\top}$.\nWe compute $HH^{\\top}$ and $H^{\\top}H$:\n$$ HH^{\\top} = \\begin{pmatrix} 0 & 2 & 0 \\\\ -2 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 0 & -2 & 0 \\\\ 2 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} = 4I $$\n$$ H^{\\top}H = \\begin{pmatrix} 0 & -2 & 0 \\\\ 2 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 0 & 2 & 0 \\\\ -2 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} = 4I $$\nThe eigenvalues of $HH^{\\top}$ are $\\lambda_1=\\lambda_2=\\lambda_3=4$. The singular values are $\\sigma_i=\\sqrt{\\lambda_i}$, so $\\sigma_1=\\sigma_2=\\sigma_3=2$. The matrix of singular values is $S=2I$.\nThe columns of $V$ are eigenvectors of $H^{\\top}H$, and columns of $U$ are eigenvectors of $HH^{\\top}$. Since these matrices are $4I$, any orthonormal basis is a valid choice of eigenvectors. We must choose them consistently via the relation $U S = H V$.\nLet's choose the simplest basis for $V$, the identity matrix: $V = I = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\nNow we find $U$ from $U = HVS^{-1} = H(I)(2I)^{-1} = \\frac{1}{2}H$.\n$$ U = \\frac{1}{2}\\begin{pmatrix} 0 & 2 & 0 \\\\ -2 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} $$\nWe check that $U$ is orthogonal: $U^{\\top}U = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = I$. It is.\nNow we compute the rotation $R$. First, check the determinant condition:\n$\\det(VU^{\\top}) = \\det(IU^{\\top}) = \\det(U^{\\top}) = \\det(U) = -1(-1) = 1$.\nSince the determinant is $1$, no correction is needed.\n$$ R = V U^{\\top} = I U^{\\top} = U^{\\top} = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} $$\nThis is the estimated rotation matrix, which correctly matches $R_{true}$.\n\nFinally, we compute the translation vector $t$:\n$$ t = \\bar{q} - R\\bar{p} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} $$\nThis is the estimated translation vector, which correctly matches $t_{true}$.\n\n4. Final Answer Formulation\nThe final answer is composed of the entries of the rotation matrix $R$ in row-major order, followed by the entries of the translation vector $t$.\n$R = \\begin{pmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$\n$t = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$\nThe elements are $(R_{11}, R_{12}, R_{13}, R_{21}, R_{22}, R_{23}, R_{31}, R_{32}, R_{33}, t_1, t_2, t_3)$.\nThis corresponds to $(0, -1, 0, 1, 0, 0, 0, 0, 1, 2, -1, 3)$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 2 & -1 & 3 \\end{pmatrix} } $$"
        },
        {
            "introduction": "After globally aligning point clouds, the next step is often to compute local geometric features that describe the surface at each point. The surface normal is one of the most critical features, but its estimation is sensitive to data imperfections like occlusions, which are common in LiDAR scans. This problem  presents a scenario to quantify how a systematic gap in neighborhood data can introduce a predictable bias in the estimated normal, offering deep insight into the interplay between sampling patterns and the accuracy of derived geometric attributes.",
            "id": "3844932",
            "problem": "A Light Detection and Ranging (LiDAR) point cloud samples a smooth terrain surface near a reference point with true normal along the $z$-axis. In a local orthonormal coordinate frame centered at the reference point, the surface height is well-approximated by the second-order Taylor expansion $z(x,y) = \\frac{1}{2} k x^{2}$, where $k$ is the principal curvature along the $x$-direction and curvature along the $y$-direction is negligible over the neighborhood of interest. Principal Component Analysis (PCA) is used to estimate the local normal from points in a neighborhood of radius $R$ in the tangent plane $(x,y)$, but the neighborhood is truncated by occlusion: points are uniformly sampled over a circular sector domain $\\mathcal{D} = \\{(r,\\theta): 0 \\le r \\le R, -\\pi + \\phi \\le \\theta \\le \\pi - \\phi\\}$ in polar coordinates, which excludes a wedge of angular width $2\\phi$ centered on the negative $x$-axis (that is, directions near $\\theta = \\pi$ are occluded). The PCA is computed with distance weights $w_{j} = w(\\rho_{j})$ that depend only on the planar radial distance $\\rho_{j}$ of each point from the reference origin; specifically, let $w(r) = r^{-\\alpha}$ with $0 \\le \\alpha < 2$.\n\nStarting from the fundamental definitions of the covariance used in Weighted Principal Component Analysis (WPCA) and the smooth-surface approximation, derive the leading-order small-slope bias angle $\\theta_{b}$ (in radians) between the estimated normal and the true normal at the reference point due to the occlusion-induced truncation. Assume the WPCA normal aligns with the surface normal at the weighted centroid to leading order in the curvature-induced slope. Express your final answer as a single closed-form analytic expression for $\\theta_{b}$ in terms of $k$, $R$, $\\phi$, and $\\alpha$, and state the angle in radians. No numerical evaluation is required.",
            "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, objective, and internally consistent.\n\nThe core of the problem is to determine the bias in the normal vector estimation caused by an occluded sampling domain. The problem provides a crucial simplification: the estimated normal, derived from Weighted Principal Component Analysis (WPCA), aligns with the true surface normal evaluated at the weighted centroid of the sampled points. This allows us to circumvent the full WPCA calculation and instead focus on finding this centroid.\n\nThe surface is locally approximated by $z(x,y) = \\frac{1}{2} k x^{2}$. The true normal at the origin $(0,0,0)$ is along the $z$-axis, $\\vec{n}_{true} = (0, 0, 1)$.\nThe weighted centroid $(\\bar{x}, \\bar{y}, \\bar{z})$ is defined by the integrals over the sampling domain $\\mathcal{D}$. We are interested in the planar coordinates $(\\bar{x}, \\bar{y})$:\n$$ \\bar{x} = \\frac{\\iint_{\\mathcal{D}} x \\, w(x,y) \\, dA}{\\iint_{\\mathcal{D}} w(x,y) \\, dA} $$\n$$ \\bar{y} = \\frac{\\iint_{\\mathcal{D}} y \\, w(x,y) \\, dA}{\\iint_{\\mathcal{D}} w(x,y) \\, dA} $$\nThe sampling domain is a circular sector given in polar coordinates as $\\mathcal{D} = \\{(r,\\theta): 0 \\le r \\le R, -\\pi + \\phi \\le \\theta \\le \\pi - \\phi\\}$. The weight function depends only on the radial distance $r$, $w(x,y) = w(r) = r^{-\\alpha}$. The area element is $dA = r \\,dr\\,d\\theta$.\n\nFirst, we compute the denominator, which is the total weight $W_{tot}$ over the domain $\\mathcal{D}$.\n$$ W_{tot} = \\iint_{\\mathcal{D}} w(r) \\, dA = \\int_{-\\pi+\\phi}^{\\pi-\\phi} \\int_{0}^{R} r^{-\\alpha} \\cdot r \\, dr \\, d\\theta $$\nThe integral over $r$ is:\n$$ \\int_{0}^{R} r^{1-\\alpha} \\, dr = \\left[ \\frac{r^{2-\\alpha}}{2-\\alpha} \\right]_{0}^{R} = \\frac{R^{2-\\alpha}}{2-\\alpha} $$\nThis integral converges because the problem specifies $0 \\le \\alpha < 2$.\nThe integral over $\\theta$ is:\n$$ \\int_{-\\pi+\\phi}^{\\pi-\\phi} d\\theta = (\\pi-\\phi) - (-\\pi+\\phi) = 2\\pi - 2\\phi = 2(\\pi-\\phi) $$\nThus, the total weight is:\n$$ W_{tot} = 2(\\pi-\\phi) \\frac{R^{2-\\alpha}}{2-\\alpha} $$\n\nNext, we compute the numerators for $\\bar{x}$ and $\\bar{y}$. For $\\bar{y}$, we substitute $y = r\\sin\\theta$:\n$$ N_y = \\iint_{\\mathcal{D}} y \\, w(r) \\, dA = \\int_{-\\pi+\\phi}^{\\pi-\\phi} \\int_{0}^{R} (r\\sin\\theta) \\, r^{-\\alpha} \\cdot r \\, dr \\, d\\theta $$\n$$ N_y = \\left( \\int_{0}^{R} r^{2-\\alpha} \\, dr \\right) \\left( \\int_{-\\pi+\\phi}^{\\pi-\\phi} \\sin\\theta \\, d\\theta \\right) $$\nThe integral of $\\sin\\theta$ is $[-\\cos\\theta]_{-\\pi+\\phi}^{\\pi-\\phi} = -\\cos(\\pi-\\phi) - (-\\cos(-\\pi+\\phi))$. Since $\\cos$ is an even function, $\\cos(-\\pi+\\phi) = \\cos(\\pi-\\phi)$, making the result $0$. Therefore, $N_y=0$ and $\\bar{y}=0$. This is expected due to the symmetry of the domain $\\mathcal{D}$ with respect to the $x$-axis.\n\nFor $\\bar{x}$, we substitute $x = r\\cos\\theta$:\n$$ N_x = \\iint_{\\mathcal{D}} x \\, w(r) \\, dA = \\int_{-\\pi+\\phi}^{\\pi-\\phi} \\int_{0}^{R} (r\\cos\\theta) \\, r^{-\\alpha} \\cdot r \\, dr \\, d\\theta $$\n$$ N_x = \\left( \\int_{0}^{R} r^{2-\\alpha} \\, dr \\right) \\left( \\int_{-\\pi+\\phi}^{\\pi-\\phi} \\cos\\theta \\, d\\theta \\right) $$\nThe integral over $r$ is $\\frac{R^{3-\\alpha}}{3-\\alpha}$, which is valid for $\\alpha < 3$, a condition satisfied by $0 \\le \\alpha < 2$. The integral over $\\theta$ is:\n$$ \\int_{-\\pi+\\phi}^{\\pi-\\phi} \\cos\\theta \\, d\\theta = [\\sin\\theta]_{-\\pi+\\phi}^{\\pi-\\phi} = \\sin(\\pi-\\phi) - \\sin(-\\pi+\\phi) $$\nUsing trigonometric identities, $\\sin(\\pi-\\phi) = \\sin\\phi$ and $\\sin(-\\pi+\\phi) = -\\sin(\\pi-\\phi) = -\\sin\\phi$. So the integral evaluates to $\\sin\\phi - (-\\sin\\phi) = 2\\sin\\phi$.\nThus, the numerator for $\\bar{x}$ is:\n$$ N_x = \\frac{R^{3-\\alpha}}{3-\\alpha} \\cdot 2\\sin\\phi $$\n\nNow we can compute the coordinate $\\bar{x}$ of the weighted centroid:\n$$ \\bar{x} = \\frac{N_x}{W_{tot}} = \\frac{2\\sin\\phi \\frac{R^{3-\\alpha}}{3-\\alpha}}{2(\\pi-\\phi) \\frac{R^{2-\\alpha}}{2-\\alpha}} = R \\left( \\frac{2-\\alpha}{3-\\alpha} \\right) \\left( \\frac{\\sin\\phi}{\\pi-\\phi} \\right) $$\nThe weighted centroid is located at $(\\bar{x}, 0) = \\left(R \\frac{2-\\alpha}{3-\\alpha} \\frac{\\sin\\phi}{\\pi-\\phi}, 0\\right)$ in the $xy$-plane.\n\nThe estimated normal $\\vec{n}_{est}$ is the surface normal at this point $(\\bar{x}, \\bar{y})$. The surface can be defined by the level set function $S(x,y,z) = z - \\frac{1}{2} k x^2 = 0$. The normal vector is given by the gradient $\\nabla S$:\n$$ \\vec{N}(x,y,z) = \\nabla S = \\left(\\frac{\\partial S}{\\partial x}, \\frac{\\partial S}{\\partial y}, \\frac{\\partial S}{\\partial z}\\right) = (-kx, 0, 1) $$\nEvaluating this at $(x,y) = (\\bar{x}, 0)$, we get the estimated normal vector:\n$$ \\vec{n}_{est} = (-k\\bar{x}, 0, 1) $$\n\nThe bias angle $\\theta_b$ is the angle between the estimated normal $\\vec{n}_{est}$ and the true normal $\\vec{n}_{true} = (0,0,1)$.\nThe angle a vector makes with the $z$-axis can be found using the tangent function. The vector $\\vec{n}_{est}$ lies in the $xz$-plane.\n$$ \\tan(\\theta_b) = \\frac{|\\text{component perpendicular to } \\vec{n}_{true}|}{|\\text{component parallel to } \\vec{n}_{true}|} = \\frac{\\sqrt{(-k\\bar{x})^2 + 0^2}}{1} = |k\\bar{x}| $$\nThe problem asks for the leading-order small-slope bias angle. A small slope corresponds to a small curvature $k$. For small $|k\\bar{x}|$, we can use the approximation $\\tan(\\theta_b) \\approx \\theta_b$.\nTherefore, the bias angle in radians is given by:\n$$ \\theta_b \\approx |k\\bar{x}| = \\left| k \\cdot R \\frac{2-\\alpha}{3-\\alpha} \\frac{\\sin\\phi}{\\pi-\\phi} \\right| $$\nSince $R>0$, $0 \\le \\phi < \\pi$, and $0 \\le \\alpha < 2$, all terms other than $k$ are non-negative. The bias angle, being a geometric angle, is a non-negative quantity. We thus write the expression in terms of $|k|$.\n\n$$ \\theta_b = |k| R \\frac{2-\\alpha}{3-\\alpha} \\frac{\\sin\\phi}{\\pi-\\phi} $$\nThis is the final expression for the leading-order bias angle in radians.",
            "answer": "$$ \\boxed{|k| R \\frac{2-\\alpha}{3-\\alpha} \\frac{\\sin(\\phi)}{\\pi - \\phi}} $$"
        },
        {
            "introduction": "Once a semantic segmentation model has been trained and applied, its performance must be rigorously quantified. This final practice focuses on the crucial step of evaluation, moving beyond textbook examples to a realistic scenario. In this exercise , you will not only construct a confusion matrix and compute standard metrics like precision and recall, but also model and analyze how performance degrades at the boundaries between different object classes—a common challenge in real-world environmental applications.",
            "id": "3844924",
            "problem": "A Light Detection and Ranging (LiDAR) semantic segmentation pipeline is deployed to classify a three-dimensional point cloud acquired over a mixed urban–estuarine landscape into four semantic classes: ground, vegetation, building, and water. Consider a single, temporally consistent acquisition such that sampling density and occlusion patterns are stable across the scene. You are given the following scientifically plausible setup grounded in standard definitions of classification events:\n\n- There are $4$ classes: $C_{1}$ (ground), $C_{2}$ (vegetation), $C_{3}$ (building), and $C_{4}$ (water).\n- The true class counts are: $N_{1} = 5000$, $N_{2} = 8000$, $N_{3} = 3000$, $N_{4} = 4000$.\n- A baseline classifier produces the following counts of predicted labels for each true class (rows index true classes and columns index predicted classes in the order $C_{1},C_{2},C_{3},C_{4}$):\n  - For $C_{1}$: $[4600, 200, 150, 50]$.\n  - For $C_{2}$: $[300, 7200, 200, 300]$.\n  - For $C_{3}$: $[100, 120, 2700, 80]$.\n  - For $C_{4}$: $[50, 250, 100, 3600]$.\n\nEnvironmental boundaries (e.g., vegetation–water ecotones, ground–building edges) induce additional label ambiguity for points whose local neighborhoods straddle heterogeneous materials. Empirical field mapping indicates that boundary effects manifest as a redistribution of some diagonal (correct) predictions into the most confusable neighboring classes. Model this boundary-induced redistribution as the following per-row reassignments from the diagonal of the baseline matrix to its off-diagonals:\n\n- From $C_{1}$ correctly predicted points, reassign $60$ to $C_{2}$, $40$ to $C_{3}$, and $20$ to $C_{4}$.\n- From $C_{2}$ correctly predicted points, reassign $80$ to $C_{1}$, $60$ to $C_{3}$, and $60$ to $C_{4}$.\n- From $C_{3}$ correctly predicted points, reassign $30$ to $C_{1}$, $40$ to $C_{2}$, and $20$ to $C_{4}$.\n- From $C_{4}$ correctly predicted points, reassign $40$ to $C_{1}$, $80$ to $C_{2}$, and $40$ to $C_{3}$.\n\nTasks:\n1. Construct the baseline confusion matrix and the boundary-affected confusion matrix, ensuring that each row sum equals the corresponding true class count $N_{k}$ and that the grand total equals $N_{1}+N_{2}+N_{3}+N_{4}$.\n2. Using only core definitions of classification events and metrics, derive for the boundary-affected confusion matrix the per-class precision, recall, and $F1$-score for each $C_{k}$, where precision, recall, and $F1$-score are defined from true positives, false positives, and false negatives.\n3. Explain, using the confusion matrix structure, why boundary points change these metrics for each class in terms of increases in off-diagonal entries and corresponding changes in false positives and false negatives.\n4. Finally, compute the macro-averaged $F1$-score (the arithmetic mean of the per-class $F1$-scores) for the boundary-affected confusion matrix. Express your final answer as a single exact fraction or a closed-form analytic expression. Do not round. The final answer must be this macro-averaged $F1$-score.",
            "solution": "The problem statement is subjected to validation.\n\nGivens are extracted as follows:\n- There are $4$ classes, denoted $C_{k}$ for $k \\in \\{1, 2, 3, 4\\}$.\n- The true class counts (number of points per class) are $N_{1} = 5000$, $N_{2} = 8000$, $N_{3} = 3000$, and $N_{4} = 4000$. The total number of points is $N = N_{1}+N_{2}+N_{3}+N_{4} = 20000$.\n- A baseline classifier's performance is given by the rows of a confusion matrix, where rows are true classes and columns are predicted classes:\n  - For true class $C_{1}$: $[4600, 200, 150, 50]$. Row sum: $4600+200+150+50 = 5000 = N_{1}$.\n  - For true class $C_{2}$: $[300, 7200, 200, 300]$. Row sum: $300+7200+200+300 = 8000 = N_{2}$.\n  - For true class $C_{3}$: $[100, 120, 2700, 80]$. Row sum: $100+120+2700+80 = 3000 = N_{3}$.\n  - For true class $C_{4}$: $[50, 250, 100, 3600]$. Row sum: $50+250+100+3600 = 4000 = N_{4}$.\n- Boundary effects cause a redistribution of correctly classified points (diagonal entries) to off-diagonal entries within the same row:\n  - From $C_{1}$: $60$ points to $C_{2}$, $40$ to $C_{3}$, $20$ to $C_{4}$. Total moved: $120$.\n  - From $C_{2}$: $80$ points to $C_{1}$, $60$ to $C_{3}$, $60$ to $C_{4}$. Total moved: $200$.\n  - From $C_{3}$: $30$ points to $C_{1}$, $40$ to $C_{2}$, $20$ to $C_{4}$. Total moved: $90$.\n  - From $C_{4}$: $40$ points to $C_{1}$, $80$ to $C_{2}$, $40$ to $C_{3}$. Total moved: $160$.\n\nThe validation check confirms that the problem is scientifically grounded in the domain of remote sensing and machine learning classification. The provided data is self-consistent, as the row sums of the baseline predictions match the true class counts. The boundary effect is described as a set of operations that conserve the row sums, which is logically consistent. The problem is well-posed, objective, and contains sufficient information for a unique solution. No flaws are detected. The problem is deemed valid.\n\nThe solution proceeds as follows.\n\n**1. Construction of Confusion Matrices**\n\nA confusion matrix $M$ is a square matrix of size $K \\times K$ for a $K$-class problem, where the entry $M_{ij}$ is the number of observations of true class $C_i$ that are predicted as class $C_j$.\n\nThe baseline confusion matrix, $M_{base}$, is constructed directly from the provided counts:\n$$\nM_{base} = \\begin{pmatrix}\n4600 & 200 & 150 & 50 \\\\\n300 & 7200 & 200 & 300 \\\\\n100 & 120 & 2700 & 80 \\\\\n50 & 250 & 100 & 3600\n\\end{pmatrix}\n$$\nAs verified during validation, the sum of each row $k$ equals the corresponding true class count $N_k$. The grand total of all entries is $N = 20000$.\n\nThe boundary-affected confusion matrix, $M_{bound}$, is obtained by applying the specified redistributions to $M_{base}$. For each row $k$, we subtract the total number of reassigned points from the diagonal element $M_{kk}$ and add them to the corresponding off-diagonal elements $M_{kj}$ ($j \\neq k$).\n\nFor Row 1 (True $C_1$):\n- Diagonal: $4600 - (60+40+20) = 4600 - 120 = 4480$.\n- Off-diagonals: $M_{12} \\to 200+60=260$; $M_{13} \\to 150+40=190$; $M_{14} \\to 50+20=70$.\n- New Row 1: $[4480, 260, 190, 70]$. Sum: $5000 = N_1$.\n\nFor Row 2 (True $C_2$):\n- Diagonal: $7200 - (80+60+60) = 7200 - 200 = 7000$.\n- Off-diagonals: $M_{21} \\to 300+80=380$; $M_{23} \\to 200+60=260$; $M_{24} \\to 300+60=360$.\n- New Row 2: $[380, 7000, 260, 360]$. Sum: $8000 = N_2$.\n\nFor Row 3 (True $C_3$):\n- Diagonal: $2700 - (30+40+20) = 2700 - 90 = 2610$.\n- Off-diagonals: $M_{31} \\to 100+30=130$; $M_{32} \\to 120+40=160$; $M_{34} \\to 80+20=100$.\n- New Row 3: $[130, 160, 2610, 100]$. Sum: $3000 = N_3$.\n\nFor Row 4 (True $C_4$):\n- Diagonal: $3600 - (40+80+40) = 3600 - 160 = 3440$.\n- Off-diagonals: $M_{41} \\to 50+40=90$; $M_{42} \\to 250+80=330$; $M_{43} \\to 100+40=140$.\n- New Row 4: $[90, 330, 140, 3440]$. Sum: $4000 = N_4$.\n\nThe resulting boundary-affected confusion matrix is:\n$$\nM_{bound} = \\begin{pmatrix}\n4480 & 260 & 190 & 70 \\\\\n380 & 7000 & 260 & 360 \\\\\n130 & 160 & 2610 & 100 \\\\\n90 & 330 & 140 & 3440\n\\end{pmatrix}\n$$\nThe row sums are preserved, and the grand total remains $20000$.\n\n**2. Per-Class Classification Metrics for $M_{bound}$**\n\nFor each class $C_k$, we compute the true positives ($TP_k$), false positives ($FP_k$), and false negatives ($FN_k$).\n- $TP_k = M_{bound, kk}$ (diagonal element).\n- $FP_k = (\\sum_{i=1}^{4} M_{bound, ik}) - TP_k$ (column sum minus diagonal).\n- $FN_k = (\\sum_{j=1}^{4} M_{bound, kj}) - TP_k = N_k - TP_k$ (row sum minus diagonal).\n\nThe metrics are defined as:\n- Precision: $P_k = \\frac{TP_k}{TP_k + FP_k}$\n- Recall: $R_k = \\frac{TP_k}{TP_k + FN_k}$\n- $F1$-score: $F1_k = 2 \\frac{P_k R_k}{P_k + R_k} = \\frac{2TP_k}{2TP_k + FP_k + FN_k}$\n\nFirst, we compute the column sums of $M_{bound}$:\n- Col 1 sum: $4480+380+130+90 = 5080$.\n- Col 2 sum: $260+7000+160+330 = 7750$.\n- Col 3 sum: $190+260+2610+140 = 3200$.\n- Col 4 sum: $70+360+100+3440 = 3970$.\n\nNow, we calculate the metrics for each class:\n\n- **Class $C_1$ (ground):**\n  $TP_1 = 4480$.\n  $FP_1 = 5080 - 4480 = 600$.\n  $FN_1 = 5000 - 4480 = 520$.\n  $P_1 = \\frac{4480}{5080} = \\frac{112}{127}$.\n  $R_1 = \\frac{4480}{5000} = \\frac{112}{125}$.\n  $F1_1 = \\frac{2 \\times 4480}{2 \\times 4480 + 600 + 520} = \\frac{8960}{8960 + 1120} = \\frac{8960}{10080} = \\frac{896}{1008} = \\frac{8}{9}$.\n\n- **Class $C_2$ (vegetation):**\n  $TP_2 = 7000$.\n  $FP_2 = 7750 - 7000 = 750$.\n  $FN_2 = 8000 - 7000 = 1000$.\n  $P_2 = \\frac{7000}{7750} = \\frac{28}{31}$.\n  $R_2 = \\frac{7000}{8000} = \\frac{7}{8}$.\n  $F1_2 = \\frac{2 \\times 7000}{2 \\times 7000 + 750 + 1000} = \\frac{14000}{14000 + 1750} = \\frac{14000}{15750} = \\frac{1400}{1575} = \\frac{56}{63} = \\frac{8}{9}$.\n\n- **Class $C_3$ (building):**\n  $TP_3 = 2610$.\n  $FP_3 = 3200 - 2610 = 590$.\n  $FN_3 = 3000 - 2610 = 390$.\n  $P_3 = \\frac{2610}{3200} = \\frac{261}{320}$.\n  $R_3 = \\frac{2610}{3000} = \\frac{261}{300} = \\frac{87}{100}$.\n  $F1_3 = \\frac{2 \\times 2610}{2 \\times 2610 + 590 + 390} = \\frac{5220}{5220 + 980} = \\frac{5220}{6200} = \\frac{522}{620} = \\frac{261}{310}$.\n\n- **Class $C_4$ (water):**\n  $TP_4 = 3440$.\n  $FP_4 = 3970 - 3440 = 530$.\n  $FN_4 = 4000 - 3440 = 560$.\n  $P_4 = \\frac{3440}{3970} = \\frac{344}{397}$.\n  $R_4 = \\frac{3440}{4000} = \\frac{344}{400} = \\frac{43}{50}$.\n  $F1_4 = \\frac{2 \\times 3440}{2 \\times 3440 + 530 + 560} = \\frac{6880}{6880 + 1090} = \\frac{6880}{7970} = \\frac{688}{797}$.\n\n**3. Explanation of Metric Changes Due to Boundary Effects**\n\nThe boundary-induced redistribution exclusively moves points from a correct classification to an incorrect one. Let us analyze this for an arbitrary class $C_k$.\n- The counts of **True Positives**, $TP_k=M_{kk}$, are explicitly reduced as points are reassigned from the diagonal.\n- The counts of **False Negatives**, $FN_k$, are the sum of off-diagonal elements in row $k$ ($FN_k = \\sum_{j \\neq k} M_{kj}$). Since points from $M_{kk}$ are moved to $M_{kj}$ ($j \\neq k$), $FN_k$ increases by the exact amount that $TP_k$ decreases.\n- The counts of **False Positives**, $FP_k$, are the sum of off-diagonal elements in column $k$ ($FP_k = \\sum_{i \\neq k} M_{ik}$). The problem states that for every class $C_i$, some of its correctly classified points are reassigned to other classes, including $C_k$. This means that the entries $M_{ik}$ for $i \\neq k$ increase. Consequently, $FP_k$ increases for all classes.\n\nThese changes directly impact the metrics:\n- **Recall**, $R_k = \\frac{TP_k}{TP_k + FN_k} = \\frac{TP_k}{N_k}$. Since $TP_k$ decreases and the denominator $N_k$ is constant, recall for each class decreases.\n- **Precision**, $P_k = \\frac{TP_k}{TP_k + FP_k}$. The numerator $TP_k$ decreases while the denominator term $FP_k$ increases. Both factors contribute to a decrease in the ratio, so precision for each class decreases.\n- **$F1$-score**, $F1_k$, is the harmonic mean of precision and recall. Since both $P_k$ and $R_k$ decrease, their harmonic mean must also decrease. The boundary effects, as modeled, unambiguously degrade the classifier's performance for every class.\n\n**4. Macro-Averaged F1-Score**\n\nThe macro-averaged $F1$-score, $F1_{macro}$, is the arithmetic mean of the per-class $F1$-scores.\n$$\nF1_{macro} = \\frac{1}{4} \\sum_{k=1}^{4} F1_k = \\frac{1}{4} (F1_1 + F1_2 + F1_3 + F1_4)\n$$\nSubstituting the calculated values:\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{8}{9} + \\frac{8}{9} + \\frac{261}{310} + \\frac{688}{797} \\right) = \\frac{1}{4} \\left( \\frac{16}{9} + \\frac{261}{310} + \\frac{688}{797} \\right)\n$$\nTo combine these into a single fraction, we find a common denominator. The denominators are $9=3^2$, $310=2 \\times 5 \\times 31$, and $797$ (which is a prime number). The least common multiple is $9 \\times 310 \\times 797 = 2223630$.\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{16 \\times (310 \\times 797)}{2223630} + \\frac{261 \\times (9 \\times 797)}{2223630} + \\frac{688 \\times (9 \\times 310)}{2223630} \\right)\n$$\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{16 \\times 247070}{2223630} + \\frac{261 \\times 7173}{2223630} + \\frac{688 \\times 2790}{2223630} \\right)\n$$\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{3953120 + 1872153 + 1919520}{2223630} \\right)\n$$\n$$\nF1_{macro} = \\frac{1}{4} \\left( \\frac{7744793}{2223630} \\right)\n$$\n$$\nF1_{macro} = \\frac{7744793}{4 \\times 2223630} = \\frac{7744793}{8894520}\n$$\nThe numerator and denominator have no common prime factors, so this fraction is in its simplest form.",
            "answer": "$$\n\\boxed{\\frac{7744793}{8894520}}\n$$"
        }
    ]
}