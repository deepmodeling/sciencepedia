## 引言
在地理空间科学领域，我们面临着一个核心挑战：如何从遥感卫星、地面传感器等来源获取的海量数据中，理解和模拟地球系统中复杂的相互作用。无论是预测[气候变化对生态系统的影响](@entry_id:190331)，还是模拟污染物在流域中的输运，其本质都是一个关系建模问题。传统的机器学习方法往往将空间实体视为孤立的数据点，难以捕捉它们之间内在的、具有物理意义的连接。图神经网络（GNN）作为一种专为处理关系数据而设计的[深度学习架构](@entry_id:634549)，为此提供了革命性的解决方案。它将地理空间系统自然地抽象为由节点（如传感器、像素点）和边（如邻近、水流）构成的图，从而能够学习和推理实体间的复杂依赖关系。

然而，将GNN应用于科学领域并不仅仅是套用一个算法。一个关键的知识鸿沟在于如何确保这些数据驱动的模型能够尊重基本的物理原理，并能处理地理[空间数据](@entry_id:924273)特有的时空、多尺度和多模态特性。本文旨在系统性地填补这一鸿沟，为读者提供一个从理论到实践的完整指南。

本文分为三个核心部分。第一章“原理与机制”将奠定理论基础，深入探讨如何构建地理空间图、信息传递的核心机制，以及先进的GNN架构。第二章“应用与跨学科连接”将展示这些原理在真实世界中的应用，涵盖[物理信息](@entry_id:152556)驱动建模、时空系统模拟及多源[数据融合](@entry_id:141454)，并探讨其在[科学推断](@entry_id:155119)中的潜力。最后，在“动手实践”部分，读者将通过具体的编程练习，将理论知识转化为解决实际问题的能力。通过本文的学习，您将掌握使用GNN进行地理空间关系建模的前沿知识，并学会如何构建既具备强大预测能力又符合物理解释性的模型。

## 原理与机制

本章旨在深入探讨将图神经网络（GNN）应用于地理空间关系建模的核心原理与机制。我们将从最基本的问题——如何将地理[空间数据](@entry_id:924273)表示为图——开始，逐步深入到信息如何在图上传播的数学机理，最后介绍几种先进的 GNN 架构及其在环境与遥感建模中的设计考量。

### 地理空间图的构建：从数据到关系

GNN 的第一步，也是最关键的一步，是构建图 $G=(V, E)$。这个图的结构本身就蕴含了我们对地理空间系统运作方式的先验知识和假设。一个精心设计的图结构是 GNN 模型成功的基石。

#### 图的构成要素：节点、边与特征

在地理空间背景下，图的组成部分具有明确的物理或地理意义。

**节点（Nodes）** $V$ 代表了我们研究的基本空间单元。这些单元可以是规则的，如遥感影像中的网格单元（grid cells）；也可以是不规则的，如根据地形划分的流域（catchments）或行政边界定义的普查区（census tracts）。每个节点 $i$ 都可以携带一个[特征向量](@entry_id:151813) $\mathbf{x}_i \in \mathbb{R}^p$，该向量可以包含从遥感数据中提取的各种物理量，例如归一化植被指数（NDVI）、地表温度（LST），以及高程等辅助数据。 

**边（Edges）** $E$ 则定义了这些空间单元之间的关系。边的定义直接决定了 GNN 中信息流动的路径，因此，如何定义边是地理空间 GNN 建模的核心。常见的边定义策略包括：

1.  **邻近关系（Proximity-based）**: 这是最直观的建图方式，它直接体现了“地理学第一定律”：万物皆有联系，但相近的事物关联更强。
    *   **邻接（Contiguity）**: 对于面状数据（如多边形），邻接是一种常见的边定义方式。如果两个多边形共享边界，则认为它们之间存在一条边。邻接规则可以进一步细分。**车邻接（Rook Contiguity）** 要求两个多边形共享一段长度大于零的边界，而 **后邻接（Queen Contiguity）** 则更为宽松，只要共享至少一个[边界点](@entry_id:176493)（包括顶点）即可。例如，在一个 $2 \times 3$ 的普查区网格布局中，使用后邻接会比车邻接产生更多的边，因为它包含了对角线方向的连接。这种差异会显著改变图的[拓扑性质](@entry_id:141605)：后邻接图通常具有更高的平均度、更小的直径和更高的[聚类系数](@entry_id:144483)。在 GNN 中，这意味着信息可以在更少的层数内传播到更远的地方，并且局部特征平滑效应更强。
    *   **距离阈值（Distance-based）**: 另一种方法是连接所有距离小于某个阈值 $\rho$ 的节点对。这种方法（称为半径图）的关键在于如何测量距离。在处理覆盖大范围地理区域的数据时，必须使用 **[测地距离](@entry_id:159682)（Geodesic Distance）**，例如通过 **Haversine 公式** 计算的[大圆](@entry_id:268970)距离。在大尺度上使用欧几里得距离（例如，在 Plate Carrée 等经纬度投影坐标系下计算）会引入严重的失真，尤其是在高纬度地区。这种失真会导致模型产生人为的各向异性，违背了基于真实地理邻近性的建模初衷。

2.  **流关系（Flow-based）**: 对于许多物理过程，如水文学，空间关系不仅仅是邻近，更是有向的流动。例如，在流域网络中，一个上游子流域的水会汇入下游子流域。这种关系可以用一个[有向图](@entry_id:920596)来表示，其中一条边 $(j \to i)$ 表示水从流域 $j$ 流向流域 $i$。这样的图结构通常是 **有向无环图（Directed Acyclic Graph, DAG）**，因为它反映了重力驱动下水[单向流](@entry_id:262401)动的物理现实。这种建图方式能够将[质量守恒](@entry_id:204015)等物理原理直接编码到模型结构中。

3.  **语义关系（Semantic-based）**: 有时，我们可能希望连接具有相似属性的节点，例如，连接所有[土地覆盖](@entry_id:1127047)类型同为“森林”的单元。然而，这种方法通常不能替代空间邻近性。两个“森林”单元可能相隔千里，而一个“森林”单元和一个“水体”单元可能紧紧相邻。因此，单纯依赖语义关系会丢失地理学第一定律所强调的连续空间依赖性。[@problem-id:3818305]

**特征（Features）** 除了节[点特征](@entry_id:155984)外，边也可以携带特征 $\mathbf{e}_{ij}$，例如节点间的[测地距离](@entry_id:159682)、相对方位角、高[程差](@entry_id:201533)等。这些边特征为 GNN 提供了更丰富的关系信息，使其能够学习更复杂的空间依赖模式。 

#### 将物理原理融入图结构

图的构建不仅是技术选择，更是对底层物理过程的抽象。

**各向同性、各向异性与地理学第一定律**: 地理学第一定律可以通过为边赋予权重来量化。一个常见的做法是使用一个随距离单调递减的[核函数](@entry_id:145324)，例如高斯核 $w_{ij} = \exp(-d_{ij}^2 / 2\sigma^2)$，其中 $d_{ij}$ 是节点间的[测地距离](@entry_id:159682)。这样，距离越近的邻居在[信息聚合](@entry_id:137588)时权重越大，影响力也越大。 在这个过程中，保持各向同性（即权重仅依赖于距离，不依赖于方向）至关重要。然而，如前所述，不恰当的坐标投影会破坏这种各向同性。例如，Plate Carrée（等距矩形）投影在东西方向上会随纬度 $\phi$ 的增加而拉伸距离，其距离测度 $d_{\mathrm{p}}$ 与真实[测地距离](@entry_id:159682) $d_{\mathrm{g}}$ 的关系在东西方向上近似为 $d_{\mathrm{p}} \approx d_{\mathrm{g}} / \cos\phi$。如果在高斯核中直接使用 $d_{\mathrm{p}}$，会导致模型在东西方向上的[权重衰减](@entry_id:635934)过快，从而产生人为的各向异性。一个补救措施是让核函数的带宽 $\sigma$ 随纬度变化，具体而言，使用一个修正后的带宽 $\sigma_{\mathrm{p}}(\phi) = \sigma / \cos\phi$，以抵消投影带来的距离失真。例如，在纬度 $60^{\circ}$ 处，$\cos(60^{\circ}) = 0.5$，修正后的带宽应为目标带宽的两倍。

**扩散与平流**: 图的拓扑结构应与所建模的物理过程相匹配。对于像大气强迫（如气温、降水）这样表现出空间平滑性的变量，使用基于邻接的无向图是合适的。GNN 在这种图上的消息传递类似于一个[扩散过程](@entry_id:268015)，能够有效地平滑和插值场变量。然而，对于像径流、污染物输送这样的通量（flux）变量，这个模型就是错误的。水和污染物不会穿过分水岭（即流域边界）进行“扩散”。对于这类过程，必须使用反映真实流动路径的[有向图](@entry_id:920596)（如水流图）。GNN 在这种有向图上的[消息传递](@entry_id:751915)，可以自然地模拟物质沿河网的汇集（平流）过程，这是一种与物理过程一致的归纳偏置。混用这两种图结构会导致模型违反基本的物理定律，例如，在邻接图上传播径流量会造成信息跨越分水岭的“泄露”。

### 核心机制：消息传递

一旦图被构建，GNN 就通过一个称为 **消息传递（Message Passing）** 的迭代过程来学习节点表示。在每一层（或每一次迭代）中，每个节点都会聚合其邻居的信息，并结合自身当前的信息来更新其状态。

一个通用的消息传递层可以抽象地表示为两个阶段：

1.  **聚合（AGGREGATE）**: 节点 $i$ 从其邻居集合 $\mathcal{N}(i)$ 中收集信息（消息），并将其聚合成一个单一的向量 $\mathbf{m}_i$。
    $$ \mathbf{m}_i = \bigoplus_{j \in \mathcal{N}(i)} \psi(\mathbf{h}_i, \mathbf{h}_j, \mathbf{e}_{ij}) $$
    其中 $\mathbf{h}_i$ 和 $\mathbf{h}_j$ 是节点 $i$ 和 $j$ 的当前特征表示，$\mathbf{e}_{ij}$ 是边特征，$\psi$ 是一个可学习的消息函数，$\bigoplus$ 是一个对邻居顺序不敏感的聚合函数（如求和、平均或最大值）。

2.  **更新（UPDATE）**: 节点 $i$ 使用聚合后的消息 $\mathbf{m}_i$ 和它自身的旧表示 $\mathbf{h}_i$ 来计算其新的表示 $\mathbf{h}_i'$。
    $$ \mathbf{h}_i' = \phi(\mathbf{h}_i, \mathbf{m}_i) $$
    其中 $\phi$ 是一个可学习的[更新函数](@entry_id:275392)。

GNN 的不同变体主要是在消息函数 $\psi$、[更新函数](@entry_id:275392) $\phi$ 和聚合函数 $\bigoplus$ 的具体形式上有所不同。

#### 聚合函数：从简单求和到归一化平均

聚合函数的选择对 GNN 的性能至关重要，尤其是在处理具有不同度的节点的[异构图](@entry_id:911820)时。

一个最简单的聚合器是 **求和聚合（Sum Aggregator）**。然而，这种方法存在一个严重缺陷：它会不成比例地放大来自高密度邻域的信号。考虑一个异构地理图，其中节点 $p$ 有 $25$ 个邻居，而节点 $q$ 只有 $3$ 个邻居。即使所有邻居的特征值都完全相同（例如，都为 $1$），求和聚合也会给节点 $p$ 产生一个值为 $25$ 的聚合消息，而给节点 $q$ 产生的消息仅为 $3$。这显然不符合我们对“平均混合效应”的直观理解，即聚合信号应反映邻居的平均状态，而不是邻居的数量。

一个有效的补救措施是使用 **均值聚合（Mean Aggregator）**。对于包含多种关系类型的[异构图](@entry_id:911820)，更精细化的做法是采用 **关系级均值聚合**。在这种方法中，我们对每种关系类型分别计算邻居特征的平均值，然后再将这些特定于关系的结果组合起来：
$$ \mathbf{m}_v^{\text{norm}} = \sum_{r \in \mathcal{R}} \frac{1}{|\mathcal{N}_r(v)|} \sum_{u \in \mathcal{N}_r(v)} \mathbf{h}_u $$
其中 $\mathcal{R}$ 是所有关系类型的集合，$\mathcal{N}_r(v)$ 是节点 $v$ 在关系 $r$ 下的邻居集合。在上述例子中，若节点 $p$ 有 $20$ 个水文邻居和 $5$ 个道路邻居，节点 $q$ 有 $2$ 个水文邻居和 $1$ 个道路邻居，且所有邻居特征为 $1$，则该聚合器为两个节点都计算出聚合消息 $1+1=2$，从而消除了度带来的偏差。

#### 图算子与传播规则

消息传递过程的数学本质可以从[图信号处理](@entry_id:183351)的角度来理解。不同的传播规则对应于在图上应用不同的线性算子。

**随机游走与标签传播**: 考虑一个在图上进行的[随机游走过程](@entry_id:171699)。从节点 $i$ 跳转到邻居 $j$ 的概率可以定义为与边权 $w_{ij}$ 成正比。对于一个加权无向图，这导致了行随机的 **[转移矩阵](@entry_id:145510)（Transition Matrix）** $\mathbf{P} = \mathbf{D}^{-1}\mathbf{A}$，其中 $\mathbf{A}$ 是加权邻接矩阵，$D_{ii} = \sum_j A_{ij}$ 是对角度矩阵。$\mathbf{P}^k$ 的 $(i,j)$ 元素表示从节点 $i$ 出发经过 $k$ 步到达节点 $j$ 的概率。

这个概念与一个经典的[半监督学习](@entry_id:636420)算法——**标签传播（Label Propagation）**——密切相关。该算法通过以下迭代过程将初始标签信息 $y$ 传播到整个图：
$$ \mathbf{f}^{(t+1)} = \alpha \mathbf{P} \mathbf{f}^{(t)} + (1-\alpha) \mathbf{y} $$
其中 $0  \alpha  1$ 是一个控制信息扩散强度的参数。当这个过程收敛时，其不动点解 $\mathbf{f}^*$ 有一个[封闭形式](@entry_id:272960)：
$$ \mathbf{f}^* = (1-\alpha)(\mathbf{I} - \alpha \mathbf{P})^{-1} \mathbf{y} $$
这个解的存在性和唯一性由以下事实保证：由于 $\mathbf{P}$ 的[谱半径](@entry_id:138984) $\rho(\mathbf{P})=1$ 且 $0  \alpha  1$，矩阵 $\alpha\mathbf{P}$ 的谱半径严格小于 $1$，因此 $(\mathbf{I} - \alpha \mathbf{P})$ 是可逆的。这个解可以展开为 Neumann 级数 $\mathbf{f}^* = (1-\alpha) \sum_{k=0}^{\infty} \alpha^k \mathbf{P}^k \mathbf{y}$。这揭示了标签传播的本质：它是一个应用于初始标签 $\mathbf{y}$ 的无限阶线性[图滤波](@entry_id:193076)器。这个框架与一些现代GNN架构（如 APPNP）惊人地相似，后者将一个基础预测（相当于 $\mathbf{y}$）与一个后续的[传播步骤](@entry_id:204825)[解耦](@entry_id:160890)，从而连接了经典[图算法](@entry_id:148535)与深度学习模型。

**图拉普拉斯算子与[谱理论](@entry_id:275351)**: 另一个理解图传播的核心工具是 **[图拉普拉斯算子](@entry_id:275190)（Graph Laplacian）**。它与图上信号的平滑度密切相关。一个图信号 $\mathbf{f}$ 的“平滑度能量”可以定义为所有边上信号差异的加权平方和。为了避免高度节点被过度惩罚，我们通常对信号进行度归一化，即衡量 $(\frac{f_i}{\sqrt{d_i}} - \frac{f_j}{\sqrt{d_j}})^2$。总能量可以写成一个二次型 $\mathbf{f}^\top \mathbf{L}_{\text{sym}} \mathbf{f}$，其中 $\mathbf{L}_{\text{sym}}$ 就是 **对称[归一化拉普拉斯算子](@entry_id:637401)**：
$$ \mathbf{L}_{\text{sym}} = \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{A} \mathbf{D}^{-1/2} $$
这个算子是半正定的，其[最小特征值](@entry_id:177333)为 $0$，对应的[特征向量](@entry_id:151813)为 $\mathbf{D}^{1/2}\mathbf{1}$。它的第二个[最小特征值](@entry_id:177333) $\lambda_2$ 被称为图的 **[代数连通度](@entry_id:152762)**，它衡量了图的连通紧密程度。

与 $\mathbf{L}_{\text{sym}}$ 密切相关的是 **随机游走[归一化拉普拉斯算子](@entry_id:637401)** $\mathbf{L}_{\text{rw}} = \mathbf{I} - \mathbf{D}^{-1}\mathbf{A} = \mathbf{I} - \mathbf{P}$。这个算子描述了随机游走一步后信号的期望变化。这两个[拉普拉斯算子](@entry_id:146319)通过[相似变换](@entry_id:152935)关联：$\mathbf{L}_{\text{sym}} = \mathbf{D}^{1/2} \mathbf{L}_{\text{rw}} \mathbf{D}^{-1/2}$，因此它们具有相同的[特征值谱](@entry_id:1124216)。[拉普拉斯算子](@entry_id:146319)是谱图理论和许多 GNN 模型（如[图卷积网络](@entry_id:194500) GCN）的理论基础。

### 先进机制与架构

基于通用的消息传递框架，研究人员开发了多种具有特定优势的先进 GNN 架构。

#### 学会关注：[图注意力网络](@entry_id:1125735) (GAT)

[标准化](@entry_id:637219)的聚合函数（如均值聚合）对所有邻居一视同仁。然而，在许多应用中，不同的邻居应该具有不同的重要性。**[图注意力网络](@entry_id:1125735)（Graph Attention Network, GAT）** 引入了[注意力机制](@entry_id:917648)，允许模型为每个邻居动态地学习一个权重。

GAT 的核心思想是，对于中心节点 $i$ 和它的一个邻居 $j$，计算一个 **注意力系数（attention coefficient）** $\alpha_{ij}$，该系数衡量了节点 $j$ 的信息对节点 $i$ 的重要性。这个过程通常遵循以下步骤：
1.  **特征变换**: 首先，使用一个共享的线性变换（权重矩阵 $\mathbf{W}$）将节[点特征](@entry_id:155984)映射到一个更高维的空间：$\mathbf{h}_i = \mathbf{W}\mathbf{x}_i$。
2.  **计算注意力分数**: 接着，一个[注意力机制](@entry_id:917648)（通常是一个小型[前馈网络](@entry_id:1124893)）计算一个未归一化的注意力分数 $u_{ij}$。这个分数应该依赖于源节点 $i$ 和目标节点 $j$ 的特征，甚至可以包含边特征 $\mathbf{e}_{ij}$。一个通用的形式是：
    $$ u_{ij} = \text{LeakyReLU}\left(\mathbf{a}^\top [\mathbf{W}\mathbf{x}_i \, \| \, \mathbf{W}\mathbf{x}_j \, \| \, \mathbf{U}\mathbf{e}_{ij}]\right) $$
    其中 $\|$ 表示向量拼接，$\mathbf{U}$ 是边特征的[变换矩阵](@entry_id:151616)，$\mathbf{a}$ 是一个可学习的权重向量。
3.  **归一化**: 最后，使用 **[Softmax](@entry_id:636766)** 函数对节点 $i$ 的所有邻居的注意力分数进行归一化，得到最终的注意力系数：
    $$ \alpha_{ij} = \frac{\exp(u_{ij})}{\sum_{k \in \mathcal{N}(i)} \exp(u_{ik})} $$
这些系数随后被用于对邻居的特征进行加权求和，形成聚合消息：$\mathbf{h}_i' = \sigma(\sum_{j \in \mathcal{N}(i)} \alpha_{ij} \mathbf{h}_j)$。

标准 GAT 的一个特性是，对于一个节点的所有邻居，只要它们的注意力分数是有限的，那么计算出的所有注意力系数 $\alpha_{ij}$ 都将是严格正的。这意味着模型仍然“关注”所有的邻居。在某些情况下，我们可能希望模型能够进行“硬”选择，即只关注少数几个最重要的邻居。这可以通过几种方式实现：
*   **低温 [Softmax](@entry_id:636766)**: 在 [Softmax](@entry_id:636766) 中引入一个温度参数 $\tau > 0$，$ \alpha_{ij}(\tau) = \exp(u_{ij}/\tau) / \sum_k \exp(u_{ik}/\tau)$。当 $\tau \to 0^+$ 时，注意力会集中到具有最高分数的邻居上。
*   **稀疏化激活**: 将 [Softmax](@entry_id:636766) 替换为能够输出稀疏概率分布的函数，如 **Sparsemax** 或 `top-k` 选择。这些方法可以产生精确为零的注意力系数，从而实现自适应的邻域选择。值得注意的是，一些看似能诱导稀疏性的正则化项，如熵惩罚 $H(\boldsymbol{\alpha}_i) = -\sum_j \alpha_{ij} \log \alpha_{ij}$，实际上会鼓励注意力分布变得更均匀，从而起到**反稀疏化**的作用。

#### 尊重对称性：等变 GNN

在处理物理系统或地理[空间数据](@entry_id:924273)时，一个理想的模型应该能遵守底层的物理或[几何对称性](@entry_id:189059)。一个核心的对称性是 **[等变性](@entry_id:636671)（Equivariance）**。如果一个操作（如 GNN 层）对输入的某种变换（如旋转）作出可预测的、相应的变换，我们就称该操作是等变的。

对于在二维[欧几里得空间](@entry_id:138052)中近似的地理数据，一个重要的对称群是 **SE(2) 群**，它包括平移和旋转。一个 SE(2) 等变的 GNN 层应满足：
*   **[平移等变性](@entry_id:636340)**: 如果所有节点的位置被平移，只要模型的输入是相对几何特征（如相对距离和方位），其输出就不应改变。
*   **旋转[等变性](@entry_id:636671)**: 如果所有节点的位置和所有矢量特征（如风矢量）被全局旋转一个角度 $\theta$，模型的输出矢量也应相应地旋转相同的角度 $\theta$。

构建一个等变的聚合器需要仔细设计。例如，考虑一个聚合器，其目标是输出一个二维矢量 $m_i$。邻居 $j$ 的信息包括相对距离 $r_{ij}$、相对方位角 $\phi_{ij}$（其单位矢量为 $u_{ij}$）和标量特征 $s_j$。一个满足 SE(2) [等变性](@entry_id:636671)的聚合器形式是：
$$ m_i = \sum_{j \in \mathcal{N}(i)} \psi(r_{ij}) \, u_{ij} \, \sigma(s_j) $$
这里，$\psi$ 和 $\sigma$ 是可学习的标量函数。当系统旋转 $\theta$ 时，$u_{ij}$ 变为 $R(\theta)u_{ij}$（其中 $R(\theta)$ 是[旋转矩阵](@entry_id:140302)），而标量项不变。因此，整个和式 $m_i$ 也恰好旋转了 $\theta$。相反，其他看似合理的构造，如 $m_i = \sum_j \psi(r_{ij}, \phi_{ij}) v_j$ 或 $m_i = m_i^A + b$（其中 $b$ 是一个可学习的偏置向量），通常会破坏旋转[等变性](@entry_id:636671)，因为 $\psi$ 对 $\phi_{ij}$ 的依赖或固定的偏置 $b$ 在旋转下不会正确地变换。构建等变 GNN 是确保模型物理一致性和泛化能力的关键一步。

#### 架构设计考量：深度、宽度与过平滑

在实践中，GNN 的一个核心设计决策是其 **深度**（层数 $L$）和 **宽度**（隐藏层维度 $W$）的选择。这个选择涉及到一个复杂的权衡。

1.  **[感受野](@entry_id:636171)（Receptive Field）**: GNN 的深度 $L$ 决定了其 **[感受野](@entry_id:636171)** 的大小。在一个 $L$ 层的 GNN 中，一个节点的新特征取决于其 $L$-hop 邻域内的所有节点。为了捕捉大范围的空间依赖关系（例如，城市路网中的中尺度交通模式），需要足够大的[感受野](@entry_id:636171)，这要求 GNN 有足够的深度。例如，若要求[感受野](@entry_id:636171)至少覆盖 $10$ 跳（hops）的邻居，则模型深度 $L$ 必须大于等于 $10$。

2.  **过平滑（Over-smoothing）**: GNN 的一个主要问题是 **过平滑**。当 GNN 层数过多时，重复的邻域平均操作会使得所有节点的表示趋于相同，从而丢失有用的局部信息。从[谱理论](@entry_id:275351)的角度看，重复应用图传播算子（如对称归一化邻接矩阵）会使其[特征向量](@entry_id:151813)对应的分量乘以 $\lambda_k^L$。由于除 $\lambda_1=1$ 外，其他特征值的绝对值都小于或等于 $1$，因此经过多层传播后，与非平凡[特征向量](@entry_id:151813)（$\lambda_k  1$）相关的信号分量会指数级衰减。为了避免过平滑，我们需要限制深度 $L$，使得最重要的非平凡分量（对应于 $\lambda_2$）的衰减在一个可接受的范围内，例如 $|\lambda_2|^L \ge \epsilon$。对于一个给定的图（其 $|\lambda_2|$ 已知），这个不等式给出了深度 $L$ 的一个上限。

3.  **参数预算与[模型容量](@entry_id:634375)**: GNN 的参数量通常与深度 $L$ 和宽度 $W$ 的平方成正比，即 $P \approx L W^2$。同时，为了避免信息在聚合过程中被过度压缩（**over-squashing**），模型的宽度 $W$ 需要足够大，以容纳来自不断扩大的感受野的信息。在邻居平均分支因子为 $b$ 的图上，[感受野大小](@entry_id:634995)约以 $b^L$ 增长，因此宽度 $W$ 可能需要与 $b^L$ 成比例。将这两个约束结合，我们得到 $L (\eta b^L)^2 \le P_{\text{max}}$，其中 $\eta$ 是一个比例常数，$P_{\text{max}}$ 是总参数预算。这个约束也给出了深度 $L$ 的一个上限。

综上所述，GNN 的最优深度 $L^*$ 是在一个由最小感受野需求（$L$ 的下限）以及过平滑和参数预算（$L$ 的上限）共同定义的 **可行集** 内进行选择的结果。在许多情况下，选择满足所有约束的最小深度是一个合理的策略，因为它在保证足够大的[感受野](@entry_id:636171)的同时，最大限度地缓解了过平滑问题和参数开销。