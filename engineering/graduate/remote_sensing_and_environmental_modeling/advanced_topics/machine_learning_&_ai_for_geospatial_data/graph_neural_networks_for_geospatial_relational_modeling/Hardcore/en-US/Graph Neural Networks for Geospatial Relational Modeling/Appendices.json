{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in applying Graph Neural Networks to geospatial problems is translating raw spatial data into a meaningful graph structure. This process is not arbitrary; it involves encoding domain-specific knowledge and physical relationships into the graph's adjacency. In this exercise, you will engage in this foundational task by using Light Detection and Ranging (LiDAR) data to define connections between land parcels based on the principle of forest canopy continuity (). By implementing a rule based on height and continuity thresholds, you will discover how modeling choices directly shape the graph's topology and, consequently, how a GNN would perceive and process spatial interactions.",
            "id": "3818275",
            "problem": "A geospatial relational model is constructed for a set of parcels using Light Detection and Ranging (LiDAR) point cloud data aggregated along shared parcel boundaries. The objective is to define edges in an undirected graph based on canopy continuity using a height threshold and to compute adjacency statistics for different thresholds. The intended application is to produce an adjacency structure suitable for a Graph Neural Network (GNN) that models geospatial interactions.\n\nFundamental base:\n- LiDAR measures canopy height above ground, which is a physical quantity expressed in meters. Let the canopy height at a sample point be denoted by $h \\in \\mathbb{R}_{\\ge 0}$ with units of meters.\n- A graph is defined as $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of undirected edges. The adjacency matrix $A \\in \\{0,1\\}^{|V| \\times |V|}$ indicates edge presence, with $A_{ij} = 1$ if and only if $(i,j) \\in E$ for $i \\ne j$, and $A_{ii} = 0$.\n- The average degree is defined as $\\bar{d} = \\frac{2|E|}{|V|}$ for an undirected graph.\n\nEdge definition based on canopy continuity:\n- Consider two parcels $i$ and $j$ that share a boundary. LiDAR-derived sample sequences along the shared boundary are given as lists of heights for each parcel, $H_{i \\to j} = [h_{i,0}, h_{i,1}, \\dots, h_{i,L-1}]$ and $H_{j \\to i} = [h_{j,0}, h_{j,1}, \\dots, h_{j,L-1}]$, where $L$ is the number of boundary-aligned samples and each $h_{i,k}$ and $h_{j,k}$ are in meters.\n- Given a canopy height threshold $T$ in meters and a continuity run-length requirement $R \\in \\mathbb{N}$, define binary sequences $b_{i,k} = \\mathbb{I}(h_{i,k} \\ge T)$ and $b_{j,k} = \\mathbb{I}(h_{j,k} \\ge T)$ for $k = 0, 1, \\dots, L-1$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n- Define the continuity sequence $c_k = b_{i,k} \\land b_{j,k}$ for $k = 0, 1, \\dots, L-1$. An undirected edge $(i,j)$ exists if and only if there is a contiguous run of at least $R$ indices in $\\{0,1,\\dots,L-1\\}$ where $c_k = 1$.\n\nData specification:\n- Number of parcels is $N = 5$ with indices $0, 1, 2, 3, 4$.\n- Neighbor pairs (shared boundaries) and LiDAR boundary sequences (in meters) are:\n    - Pair $(0,1)$: $H_{0 \\to 1} = [9,11,12,13,8]$, $H_{1 \\to 0} = [10,12,11,14,7]$.\n    - Pair $(1,2)$: $H_{1 \\to 2} = [5,6,7,8]$, $H_{2 \\to 1} = [5,6,7,8]$.\n    - Pair $(2,3)$: $H_{2 \\to 3} = [20,22,25,27,19,16]$, $H_{3 \\to 2} = [18,21,24,26,20,15]$.\n    - Pair $(3,4)$: $H_{3 \\to 4} = [0,0,0,2]$, $H_{4 \\to 3} = [0,1,0,3]$.\n    - Pair $(0,4)$: $H_{0 \\to 4} = [10,10,10,10]$, $H_{4 \\to 0} = [9,11,12,10]$.\n    - Pair $(1,4)$: $H_{1 \\to 4} = [14,16,18,19]$, $H_{4 \\to 1} = [13,15,17,18]$.\n    - Pair $(1,3)$: $H_{1 \\to 3} = [8,9,10,11,12]$, $H_{3 \\to 1} = [7,8,9,10,11]$.\n- All heights are in meters. All thresholds $T$ must be specified in meters.\n\nTasks:\n1. For each $(T, R)$ pair in the test suite, construct the undirected graph $G = (V, E)$ using the defined edge rule, where $V = \\{0,1,2,3,4\\}$.\n2. Compute:\n   - The number of edges $|E|$.\n   - The average degree $\\bar{d}$ as a float rounded to three decimal places.\n   - The number of connected components (an integer).\n   - The size of the largest connected component (an integer).\n\nTest suite:\n- The set of $(T, R)$ pairs to evaluate is $[(0,3), (10,3), (15,3), (25,3), (10,4)]$. All $T$ values are in meters, and $R$ is an integer number of samples.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result should itself be a list of the form $[|E|, \\bar{d}, \\text{n_components}, \\text{lcc_size}]$, where $|E|$ is an integer, $\\bar{d}$ is a float rounded to three decimal places, `n_components` is an integer, and `lcc_size` is an integer.\n- For example, the output format must resemble $[[e_1, \\$\\bar{d}_1\\$, c_1, l_1], [e_2, \\$\\bar{d}_2\\$, c_2, l_2], \\dots]$ with no additional text.\n\nScientific realism and constraints:\n- Treat the graph as undirected.\n- Use the inclusive threshold rule $h \\ge T$.\n- The continuity requirement $R$ is a strict minimum run length in sample indices.\n- No external data should be used beyond the provided sequences. All heights and thresholds are in meters, and all computed statistics are dimensionless counts or floats.",
            "solution": "The problem statement has been critically reviewed and is determined to be valid. It is scientifically grounded in the principles of geospatial data analysis and graph theory, well-posed with clear definitions and sufficient data, and objective in its formulation. The problem is free of contradictions, ambiguities, and factual errors. We may therefore proceed with a formal solution.\n\nThe core of the problem is to construct an undirected graph $G = (V, E)$ based on a specific rule of \"canopy continuity\" and then to compute a set of standard graph-theoretic statistics for different parameterizations of this rule. The set of nodes is fixed as $V = \\{0, 1, 2, 3, 4\\}$, representing $N=5$ land parcels.\n\nThe procedure is as follows:\n1.  For each test case, defined by a canopy height threshold $T$ (in meters) and a continuity run-length requirement $R$ (a dimensionless count), we determine the set of edges $E$.\n2.  An edge $(i, j)$ is added to $E$ if and only if parcels $i$ and $j$ are listed as a neighbor pair and satisfy the canopy continuity criterion.\n3.  Once the graph $G$ is fully constructed for a given $(T, R)$ pair, we compute the required statistics: the number of edges $|E|$, the average degree $\\bar{d}$, the number of connected components, and the size of the largest connected component.\n\n**Edge Definition Algorithm**\n\nFor a given pair of neighboring parcels $(i, j)$, with corresponding LiDAR height sample sequences $H_{i \\to j} = [h_{i,0}, \\dots, h_{i,L-1}]$ and $H_{j \\to i} = [h_{j,0}, \\dots, h_{j,L-1}]$, and given parameters $(T, R)$:\n\n1.  **Thresholding**: Create two binary sequences, $b_i = [b_{i,0}, \\dots, b_{i,L-1}]$ and $b_j = [b_{j,0}, \\dots, b_{j,L-1}]$, where the elements are determined by the indicator function $\\mathbb{I}(\\cdot)$:\n    $$b_{i,k} = \\mathbb{I}(h_{i,k} \\ge T)$$\n    $$b_{j,k} = \\mathbb{I}(h_{j,k} \\ge T)$$\n    for each sample index $k \\in \\{0, 1, \\dots, L-1\\}$.\n\n2.  **Continuity Check**: Compute a single continuity sequence $c = [c_0, \\dots, c_{L-1}]$ by applying a logical AND operation ($\\land$) element-wise to the binary sequences:\n    $$c_k = b_{i,k} \\land b_{j,k}$$\n    This means $c_k = 1$ only if the canopy height is above the threshold $T$ for *both* parcels at the sample location $k$.\n\n3.  **Run-Length Evaluation**: Search the continuity sequence $c$ for any contiguous subsequence of $1$s with a length of at least $R$. If such a run exists, an undirected edge $(i, j)$ is formed. Otherwise, no edge is formed between parcels $i$ and $j$.\n\n**Graph Statistics Calculation**\n\nFor a constructed graph $G=(V, E)$ with $|V|=N=5$ nodes:\n\n1.  **Number of Edges $|E|$**: This is the total count of unique edges created by the algorithm.\n2.  **Average Degree $\\bar{d}$**: For an undirected graph, this is given by the formula $\\bar{d} = \\frac{2|E|}{|V|}$.\n3.  **Connected Components**: The number of connected components and the size of the largest component are determined by traversing the graph. A standard algorithm such as Breadth-First Search (BFS) or Depth-First Search (DFS) is employed. We iterate through all nodes from $0$ to $N-1$. If a node has not yet been visited, we start a new traversal from it, increment the component count, and explore all reachable nodes, counting them to determine the size of this new component. The largest size found across all components is recorded.\n\n**Execution for each Test Case**\n\nWe apply this methodology to each $(T, R)$ pair in the provided test suite.\n\n**Test Case 1: $(T, R) = (0, 3)$**\n- With $T=0$ meters, any height $h \\ge 0$ satisfies the threshold. Since all provided heights are non-negative, $b_{i,k}=1$ and $b_{j,k}=1$ for all samples.\n- The continuity sequence $c$ for any pair will be a sequence of all $1$s.\n- An edge exists if the length of the boundary sequence, $L$, is at least $R=3$.\n- Pair $(0,1), L=5 \\ge 3 \\implies$ Edge.\n- Pair $(1,2), L=4 \\ge 3 \\implies$ Edge.\n- Pair $(2,3), L=6 \\ge 3 \\implies$ Edge.\n- Pair $(3,4), L=4 \\ge 3 \\implies$ Edge.\n- Pair $(0,4), L=4 \\ge 3 \\implies$ Edge.\n- Pair $(1,4), L=4 \\ge 3 \\implies$ Edge.\n- Pair $(1,3), L=5 \\ge 3 \\implies$ Edge.\n- All $7$ potential neighbor pairs form edges.\n- $|E| = 7$. $\\bar{d} = \\frac{2 \\times 7}{5} = 2.8$.\n- The resulting graph is fully connected. Number of components is $1$. Size of largest component is $5$.\n- Result: $[7, 2.800, 1, 5]$\n\n**Test Case 2: $(T, R) = (10, 3)$**\n- We check each pair with $T=10$ meters and $R=3$.\n- $(0,1)$: $c = [0,1,1,1,0]$. Max run length is $3$. $3 \\ge 3 \\implies$ Edge.\n- $(1,2)$: $c = [0,0,0,0]$. Max run is $0 < 3 \\implies$ No Edge.\n- $(2,3)$: $c = [1,1,1,1,1,1]$. Max run is $6 \\ge 3 \\implies$ Edge.\n- $(3,4)$: $c = [0,0,0,0]$. Max run is $0 < 3 \\implies$ No Edge.\n- $(0,4)$: $c = [0,1,1,1]$. Max run is $3 \\ge 3 \\implies$ Edge.\n- $(1,4)$: $c = [1,1,1,1]$. Max run is $4 \\ge 3 \\implies$ Edge.\n- $(1,3)$: $c = [0,0,0,1,1]$. Max run is $2 < 3 \\implies$ No Edge.\n- Edges: $(0,1), (2,3), (0,4), (1,4)$.\n- $|E| = 4$. $\\bar{d} = \\frac{2 \\times 4}{5} = 1.6$.\n- Components: Node $0$ is connected to $1$ and $4$. Node $1$ is connected to $4$. This forms component $\\{0,1,4\\}$. Node $2$ is connected to $3$, forming component $\\{2,3\\}$.\n- Number of components is $2$. Size of largest component is $3$.\n- Result: $[4, 1.600, 2, 3]$\n\n**Test Case 3: $(T, R) = (15, 3)$**\n- We check each pair with $T=15$ meters and $R=3$.\n- $(0,1)$: No heights $\\ge 15$. No edge.\n- $(1,2)$: No heights $\\ge 15$. No edge.\n- $(2,3)$: $c = [1,1,1,1,1,1]$. Max run is $6 \\ge 3 \\implies$ Edge.\n- $(3,4)$: No heights $\\ge 15$. No edge.\n- $(0,4)$: No heights $\\ge 15$. No edge.\n- $(1,4)$: $c = [0,1,1,1]$. Max run is $3 \\ge 3 \\implies$ Edge.\n- $(1,3)$: Heights are all $\\le 12$. No edge.\n- Edges: $(2,3), (1,4)$.\n- $|E| = 2$. $\\bar{d} = \\frac{2 \\times 2}{5} = 0.8$.\n- Components: $\\{0\\}$, $\\{1,4\\}$, $\\{2,3\\}$.\n- Number of components is $3$. Size of largest component is $2$.\n- Result: $[2, 0.800, 3, 2]$\n\n**Test Case 4: $(T, R) = (25, 3)$**\n- We check each pair with $T=25$ meters and $R=3$.\n- $(2,3)$: $H_{2 \\to 3} = [20,22,25,27,19,16]$, $H_{3 \\to 2} = [18,21,24,26,20,15]$.\n  $b_2 = [0,0,1,1,0,0]$, $b_3 = [0,0,0,1,0,0]$. $c = [0,0,0,1,0,0]$. Max run is $1 < 3 \\implies$ No Edge.\n- No other pair has heights reaching $25$ meters.\n- No edges are formed. $|E| = 0$. $\\bar{d} = 0.0$.\n- Components: $5$ isolated nodes $\\{0\\}, \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\}$.\n- Number of components is $5$. Size of largest component is $1$.\n- Result: $[0, 0.000, 5, 1]$\n\n**Test Case 5: $(T, R) = (10, 4)$**\n- This uses $T=10$ meters as in Case 2, but with a stricter requirement of $R=4$. We re-evaluate the edges from Case 2.\n- $(0,1)$: Max run was $3 < 4 \\implies$ No Edge.\n- $(2,3)$: Max run was $6 \\ge 4 \\implies$ Edge.\n- $(0,4)$: Max run was $3 < 4 \\implies$ No Edge.\n- $(1,4)$: Max run was $4 \\ge 4 \\implies$ Edge.\n- Edges: $(2,3), (1,4)$. This is the same edge set as Case 3.\n- $|E| = 2$. $\\bar{d} = \\frac{2 \\times 2}{5} = 0.8$.\n- Components: $\\{0\\}$, $\\{1,4\\}$, $\\{2,3\\}$.\n- Number of components is $3$. Size of largest component is $2$.\n- Result: $[2, 0.800, 3, 2]$",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the geospatial relational modeling problem by constructing graphs\n    and computing statistics for a set of test cases.\n    \"\"\"\n    \n    # Number of parcels (nodes in the graph)\n    N = 5\n\n    # LiDAR boundary sequences for all neighbor pairs.\n    # Format: (parcel_i, parcel_j, Heights_i->j, Heights_j->i)\n    neighbor_data = [\n        (0, 1, [9, 11, 12, 13, 8], [10, 12, 11, 14, 7]),\n        (1, 2, [5, 6, 7, 8], [5, 6, 7, 8]),\n        (2, 3, [20, 22, 25, 27, 19, 16], [18, 21, 24, 26, 20, 15]),\n        (3, 4, [0, 0, 0, 2], [0, 1, 0, 3]),\n        (0, 4, [10, 10, 10, 10], [9, 11, 12, 10]),\n        (1, 4, [14, 16, 18, 19], [13, 15, 17, 18]),\n        (1, 3, [8, 9, 10, 11, 12], [7, 8, 9, 10, 11]),\n    ]\n    \n    # Test suite of (Threshold, Run-length) pairs.\n    test_cases = [(0, 3), (10, 3), (15, 3), (25, 3), (10, 4)]\n    \n    final_results = []\n\n    def check_edge(h1, h2, T, R):\n        \"\"\"\n        Determines if an edge exists between two parcels based on canopy continuity.\n        \"\"\"\n        # Convert height sequences to binary based on threshold T.\n        b1 = np.greater_equal(h1, T)\n        b2 = np.greater_equal(h2, T)\n        \n        # Compute the continuity sequence via logical AND.\n        c = np.logical_and(b1, b2)\n        \n        # Find the longest contiguous run of 1s in the continuity sequence.\n        max_run = 0\n        current_run = 0\n        for val in c:\n            if val:\n                current_run += 1\n            else:\n                max_run = max(max_run, current_run)\n                current_run = 0\n        max_run = max(max_run, current_run) # Account for a run at the end.\n        \n        return max_run >= R\n\n    def get_graph_stats(adj, num_nodes):\n        \"\"\"\n        Calculates the number of connected components and the size of the largest one.\n        \"\"\"\n        if not any(adj.values()): # Handle graph with no edges\n            return num_nodes, 1\n\n        visited = [False] * num_nodes\n        num_components = 0\n        largest_component_size = 0\n        \n        for i in range(num_nodes):\n            if not visited[i]:\n                num_components += 1\n                current_component_size = 0\n                q = [i]\n                visited[i] = True\n                head = 0\n                while head < len(q):\n                    u = q[head]\n                    head += 1\n                    current_component_size += 1\n                    for v in adj[u]:\n                        if not visited[v]:\n                            visited[v] = True\n                            q.append(v)\n                largest_component_size = max(largest_component_size, current_component_size)\n        \n        return num_components, largest_component_size\n\n    for T, R in test_cases:\n        # Initialize an adjacency list for the graph.\n        adj = {i: [] for i in range(N)}\n        num_edges = 0\n        \n        # Iterate through all potential neighbor pairs to build the graph.\n        for u, v, h_u, h_v in neighbor_data:\n            if check_edge(h_u, h_v, T, R):\n                adj[u].append(v)\n                adj[v].append(u)\n                num_edges += 1\n        \n        # Compute the required statistics.\n        avg_degree = round(2 * num_edges / N, 3)\n        n_components, lcc_size = get_graph_stats(adj, N)\n        \n        # Store the results for this test case.\n        final_results.append([num_edges, avg_degree, n_components, lcc_size])\n\n    # Format the final output string exactly as specified.\n    result_str = \",\".join(map(str, final_results))\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While the previous exercise focused on building a graph from local data, many environmental models operate at a global scale. On a sphere, the choice of grid system has profound implications for the resulting graph's topology. This practice guides you through an analytical comparison of two fundamental global discretizations: the familiar latitude-longitude grid and the more isotropic icosahedral grid (). By deriving and computing key metrics like the average node degree $\\bar{k}$ and clustering coefficient $\\bar{C}$, you will gain a deep, quantitative understanding of how these structures differ and why these differences are critical for designing GNNs that model global processes without introducing artificial biases.",
            "id": "3818288",
            "problem": "You are modeling two alternative geospatial graph constructions for message passing in a Graph Neural Network (GNN) used in global environmental modeling: a triangulated spherical graph derived from subdividing a regular icosahedron and a latitude–longitude grid graph on a rectangular index domain. You will construct both graphs purely combinatorially (no distances required), compute their average node degree and average clustering coefficient, and then compare these metrics across a small test suite. All quantities are dimensionless.\n\nFundamental base to use: the Euler characteristic of the sphere with $V - E + F = 2$ for a connected, closed triangulated surface; the combinatorial relation $3F = 2E$ for triangulations without boundary; the definition of the average node degree as $\\bar{k} = \\frac{1}{V} \\sum_{i=1}^{V} k_i = \\frac{2E}{V}$; and the definition of the local clustering coefficient for a node with degree $k_i \\ge 2$ as $C_i = \\frac{m_i}{\\binom{k_i}{2}}$, where $m_i$ is the number of edges among the neighbors of node $i$, with $C_i = 0$ if $k_i < 2$. You must not assume or use any shortcut formula beyond these fundamentals.\n\nGraph constructions to implement:\n- Spherical triangulation from a subdivided icosahedron: Start with a regular icosahedron ($12$ vertices, $20$ triangular faces) embedded in $\\mathbb{R}^3$. Build a geodesic refinement by subdividing each triangular face into $4$ smaller triangles by introducing edge midpoints and projecting those midpoints back onto the unit sphere (normalize vectors). Repeat this subdivision $s$ times. Construct the undirected graph by connecting vertices that share an edge in the final triangulation.\n- Latitude–longitude grid graph: Consider a rectangular index grid with $L$ latitudinal rows and $M$ longitudinal columns. The node set is $\\{(i,j): i \\in \\{0,\\dots,L-1\\}, j \\in \\{0,\\dots,M-1\\}\\}$, each representing a distinct grid intersection excluding the geographic poles. Construct undirected edges using $4$-neighborhood connectivity: for each node $(i,j)$, connect to $(i,(j-1) \\bmod M)$, $(i,(j+1) \\bmod M)$, $(i-1,j)$ if $i>0$, and $(i+1,j)$ if $i<L-1$. This yields periodic wrap in longitude and open boundaries in latitude.\n\nComputations to perform for each graph:\n- Average node degree $\\bar{k}$ computed as $\\frac{2E}{V}$.\n- Average clustering coefficient computed as $\\bar{C} = \\frac{1}{V} \\sum_{i=1}^{V} C_i$ with $C_i$ defined as above.\n\nTest suite:\n- Case $1$: Subdivision level $s = 0$; latitude–longitude grid parameters $L = 5$, $M = 8$.\n- Case $2$: Subdivision level $s = 1$; latitude–longitude grid parameters $L = 10$, $M = 20$.\n- Case $3$: Subdivision level $s = 2$; latitude–longitude grid parameters $L = 30$, $M = 60$.\n- Case $4$: Subdivision level $s = 3$; latitude–longitude grid parameters $L = 45$, $M = 90$.\n\nOutput required:\n- For each test case, compute and return a list of five floating-point numbers in the following order: $[\\bar{k}_{\\text{ico}}, \\bar{C}_{\\text{ico}}, \\bar{k}_{\\text{latlon}}, \\bar{C}_{\\text{latlon}}, \\Delta \\bar{k}]$, where $\\Delta \\bar{k} = \\bar{k}_{\\text{ico}} - \\bar{k}_{\\text{latlon}}$.\n- Your program should produce a single line of output containing the results as a comma-separated list of these per-case lists with no whitespace, enclosed in square brackets, for example: \"[[a1,b1,c1,d1,e1],[a2,b2,c2,d2,e2],[a3,b3,c3,d3,e3],[a4,b4,c4,d4,e4]]\".",
            "solution": "The problem requires the calculation and comparison of topological properties—average node degree and average clustering coefficient—for two distinct types of graphs used in geospatial modeling. The validation confirms that the problem is scientifically sound, well-posed, and objective. We proceed with a principled derivation of the required quantities for each graph type.\n\n### 1. Analysis of the Spherical Icosahedral Graph\n\nThe first graph is generated by recursively subdividing the faces of a regular icosahedron. Let $V_s$, $E_s$, and $F_s$ be the number of vertices, edges, and faces at subdivision level $s$, respectively.\n\n**1.1. Combinatorial Properties ($V_s, E_s, \\bar{k}_{\\text{ico}}$)**\n\nFor $s=0$, the graph is a regular icosahedron, which is a triangulation of a sphere with $V_0=12$ vertices, $E_0=30$ edges, and $F_0=20$ triangular faces. These values satisfy the Euler characteristic for a sphere, $V - E + F = 12 - 30 + 20 = 2$, and the condition for a triangulation, $3F = 3(20) = 60 = 2E = 2(30)$.\n\nThe subdivision process transforms a graph at level $s$ to level $s+1$. Each of the $F_s$ triangular faces is divided into $4$ smaller triangles, so $F_{s+1} = 4F_s$. This implies $F_s = F_0 \\cdot 4^s = 20 \\cdot 4^s$. Since the resulting graph is also a triangulation of a sphere, the relation $3F_s = 2E_s$ holds for all $s$. Therefore, $E_s = \\frac{3}{2}F_s = \\frac{3}{2}(20 \\cdot 4^s) = 30 \\cdot 4^s$.\n\nTo find the number of vertices $V_s$, we use the Euler characteristic $V_s - E_s + F_s = 2$.\n$$V_s = 2 + E_s - F_s = 2 + 30 \\cdot 4^s - 20 \\cdot 4^s = 10 \\cdot 4^s + 2$$\nThe average node degree $\\bar{k}_{\\text{ico}}(s)$ is defined as $\\frac{2E}{V}$.\n$$\\bar{k}_{\\text{ico}}(s) = \\frac{2E_s}{V_s} = \\frac{2 \\cdot (30 \\cdot 4^s)}{10 \\cdot 4^s + 2} = \\frac{60 \\cdot 4^s}{10 \\cdot 4^s + 2}$$\nAs $s \\to \\infty$, the degree approaches $\\frac{60}{10} = 6$, characteristic of a large hexagonal grid (the dual of a triangular grid).\n\n**1.2. Average Clustering Coefficient ($\\bar{C}_{\\text{ico}}$)**\n\nThe average clustering coefficient $\\bar{C} = \\frac{1}{V}\\sum_{i=1}^{V} C_i$, where $C_i = m_i / \\binom{k_i}{2}$ for a node $i$ with degree $k_i \\ge 2$, and $m_i$ is the number of edges among its neighbors.\n\nThe subdivision process creates two distinct types of vertices.\n1.  The original $12$ vertices of the icosahedron. By the symmetry of the construction, these vertices always maintain a neighborhood structure identical to the original icosahedron. Each is connected to $5$ neighbors, which form a closed cycle of $5$ edges. Thus, for these $12$ vertices, the degree is $k_i = 5$ and the number of edges among neighbors is $m_i = 5$. The local clustering coefficient is constant for all $s$:\n    $$C_{\\text{penta}} = \\frac{5}{\\binom{5}{2}} = \\frac{5}{10} = 0.5$$\n2.  Vertices introduced during subdivision. These are created on the midpoints of previous edges. Any such vertex is surrounded by $6$ triangular faces and thus has $6$ neighbors. These neighbors form a closed cycle of $6$ edges. For these vertices, the degree is $k_i=6$ and $m_i=6$. Their local clustering coefficient is:\n    $$C_{\\text{hexa}} = \\frac{6}{\\binom{6}{2}} = \\frac{6}{15} = 0.4$$\nThe number of these \"hexagonal\" vertices at level $s$ is $V_s - 12 = (10 \\cdot 4^s + 2) - 12 = 10 \\cdot 4^s - 10$.\n\nThe average clustering coefficient $\\bar{C}_{\\text{ico}}(s)$ is the weighted average of these two values:\n$$\\bar{C}_{\\text{ico}}(s) = \\frac{1}{V_s} \\left( 12 \\cdot C_{\\text{penta}} + (V_s - 12) \\cdot C_{\\text{hexa}} \\right)$$\n$$\\bar{C}_{\\text{ico}}(s) = \\frac{1}{10 \\cdot 4^s + 2} \\left( 12 \\cdot 0.5 + (10 \\cdot 4^s - 10) \\cdot 0.4 \\right)$$\n$$\\bar{C}_{\\text{ico}}(s) = \\frac{6 + 4 \\cdot 4^s - 4}{10 \\cdot 4^s + 2} = \\frac{4 \\cdot 4^s + 2}{10 \\cdot 4^s + 2} = \\frac{4^{s+1} + 2}{10 \\cdot 4^s + 2}$$\n\n### 2. Analysis of the Latitude-Longitude Grid Graph\n\nThis graph is constructed on a rectangular grid of indices with $L$ rows and $M$ columns.\n\n**2.1. Combinatorial Properties ($V, E, \\bar{k}_{\\text{latlon}}$)**\n\nThe number of vertices is simply $V = L \\cdot M$.\nEdges are defined by a $4$-neighborhood rule with longitudinal periodicity. We can count the total number of edges $E$ by summing horizontal and vertical edges.\n-   **Horizontal edges:** Each of the $L$ rows is a cycle of $M$ vertices, contributing $M$ edges per row. Total horizontal edges: $L \\cdot M$.\n-   **Vertical edges:** Each of the $M$ columns is a path of $L$ vertices (no periodicity), contributing $L-1$ edges per column. Total vertical edges: $M \\cdot (L-1)$.\n\nTotal edges $E = L \\cdot M + M \\cdot (L-1) = LM + LM - M = M(2L - 1)$.\nThe average node degree $\\bar{k}_{\\text{latlon}}$ is:\n$$\\bar{k}_{\\text{latlon}}(L) = \\frac{2E}{V} = \\frac{2 \\cdot M(2L - 1)}{L \\cdot M} = \\frac{2(2L-1)}{L} = 4 - \\frac{2}{L}$$\nThis value only depends on $L$. As $L \\to \\infty$, $\\bar{k} \\to 4$, the degree of an infinite square lattice.\n\n**2.2. Average Clustering Coefficient ($\\bar{C}_{\\text{latlon}}$)**\n\nThe clustering coefficient measures the density of triangles in a graph. The lat-lon grid is a bipartite graph. We can assign a color, $(i+j) \\pmod 2$, to each node $(i,j)$. An edge always connects nodes of different colors (e.g., $(i,j)$ to $(i, j+1)$ results in a color change since $(i+(j+1)) \\pmod 2 \\neq (i+j) \\pmod 2$). A path of length two returns to a node of the same color as the starting node. Since edges only connect nodes of different colors, no two neighbors of a given node can be connected to each other. This means the graph contains no triangles ($3$-cycles), so the number of edges between neighbors of any node $i$, $m_i$, is always $0$.\nFor any node $i$ with degree $k_i \\ge 2$ (which is true for all nodes in the given test cases, as $L,M \\ge 3$), the local clustering coefficient is:\n$$C_i = \\frac{m_i}{\\binom{k_i}{2}} = \\frac{0}{\\binom{k_i}{2}} = 0$$\nTherefore, the average clustering coefficient is also zero.\n$$\\bar{C}_{\\text{latlon}} = 0$$\n\n### 3. Computations for Test Suite\n\nFor each case, we apply the derived formulas.\n- $\\bar{k}_{\\text{ico}}(s) = \\frac{60 \\cdot 4^s}{10 \\cdot 4^s + 2}$\n- $\\bar{C}_{\\text{ico}}(s) = \\frac{4^{s+1} + 2}{10 \\cdot 4^s + 2}$\n- $\\bar{k}_{\\text{latlon}}(L) = 4 - \\frac{2}{L}$\n- $\\bar{C}_{\\text{latlon}} = 0$\n- $\\Delta \\bar{k} = \\bar{k}_{\\text{ico}}(s) - \\bar{k}_{\\text{latlon}}(L)$\n\n**Case 1:** $s=0, L=5, M=8$\n- $\\bar{k}_{\\text{ico}} = \\frac{60}{12} = 5.0$\n- $\\bar{C}_{\\text{ico}} = \\frac{6}{12} = 0.5$\n- $\\bar{k}_{\\text{latlon}} = 4 - \\frac{2}{5} = 3.6$\n- $\\bar{C}_{\\text{latlon}} = 0.0$\n- $\\Delta \\bar{k} = 5.0 - 3.6 = 1.4$\n\n**Case 2:** $s=1, L=10, M=20$\n- $\\bar{k}_{\\text{ico}} = \\frac{60 \\cdot 4}{10 \\cdot 4 + 2} = \\frac{240}{42} \\approx 5.714$\n- $\\bar{C}_{\\text{ico}} = \\frac{4^2 + 2}{42} = \\frac{18}{42} \\approx 0.429$\n- $\\bar{k}_{\\text{latlon}} = 4 - \\frac{2}{10} = 3.8$\n- $\\bar{C}_{\\text{latlon}} = 0.0$\n- $\\Delta \\bar{k} = \\frac{240}{42} - 3.8 \\approx 1.914$\n\n**Case 3:** $s=2, L=30, M=60$\n- $\\bar{k}_{\\text{ico}} = \\frac{60 \\cdot 16}{10 \\cdot 16 + 2} = \\frac{960}{162} \\approx 5.926$\n- $\\bar{C}_{\\text{ico}} = \\frac{4^3 + 2}{162} = \\frac{66}{162} \\approx 0.407$\n- $\\bar{k}_{\\text{latlon}} = 4 - \\frac{2}{30} = 4 - \\frac{1}{15} \\approx 3.933$\n- $\\bar{C}_{\\text{latlon}} = 0.0$\n- $\\Delta \\bar{k} = \\frac{960}{162} - (4 - \\frac{1}{15}) \\approx 1.993$\n\n**Case 4:** $s=3, L=45, M=90$\n- $\\bar{k}_{\\text{ico}} = \\frac{60 \\cdot 64}{10 \\cdot 64 + 2} = \\frac{3840}{642} \\approx 5.981$\n- $\\bar{C}_{\\text{ico}} = \\frac{4^4 + 2}{642} = \\frac{258}{642} \\approx 0.402$\n- $\\bar{k}_{\\text{latlon}} = 4 - \\frac{2}{45} \\approx 3.956$\n- $\\bar{C}_{\\text{latlon}} = 0.0$\n- $\\Delta \\bar{k} = \\frac{3840}{642} - (4 - \\frac{2}{45}) \\approx 2.026",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes graph metrics for icosahedral and lat-lon graphs\n    based on the derivations.\n    \"\"\"\n    test_cases = [\n        # (subdivision_level_s, lat_rows_L, lon_cols_M)\n        (0, 5, 8),\n        (1, 10, 20),\n        (2, 30, 60),\n        (3, 45, 90),\n    ]\n\n    all_results = []\n    for s, L, M in test_cases:\n        # Icosahedral graph calculations\n        power_of_4_s = 4**s\n        k_ico = (60.0 * power_of_4_s) / (10.0 * power_of_4_s + 2.0)\n        c_ico = (4.0 * power_of_4_s + 2.0) / (10.0 * power_of_4_s + 2.0)\n\n        # Latitude-Longitude grid graph calculations\n        k_latlon = 4.0 - 2.0 / L\n        c_latlon = 0.0\n\n        # Difference in average degree\n        delta_k = k_ico - k_latlon\n\n        result = [k_ico, c_ico, k_latlon, c_latlon, delta_k]\n        all_results.append(result)\n\n    # Format the final output string according to the problem specification.\n    # The output is a list of lists, represented as a string, with no whitespace.\n    # e.g., \"[[a1,b1,c1,d1,e1],[a2,b2,c2,d2,e2],...]\"\n    \n    list_of_list_strings = []\n    for inner_list in all_results:\n        # Convert each float in the inner list to its string representation\n        stringified_inner_list = map(str, inner_list)\n        # Join them with commas and enclose in square brackets\n        list_of_list_strings.append(f\"[{','.join(stringified_inner_list)}]\")\n    \n    # Join the string representations of inner lists and enclose in outer brackets\n    final_output_string = f\"[{','.join(list_of_list_strings)}]\"\n\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful model is of little use without a reliable assessment of its performance. In geospatial modeling, the inherent spatial autocorrelation of data can render standard cross-validation techniques misleadingly optimistic. This final practice addresses this critical challenge by guiding you through the implementation of a spatially blocked cross-validation scheme, a robust method for preventing information leakage between training and validation sets (). You will learn to quantify the range of spatial dependency using geostatistical models and use this to construct buffered data folds, ensuring a fair and accurate evaluation of your GNN's ability to generalize to new, unseen locations.",
            "id": "3818318",
            "problem": "Consider a geospatial node set representing Remote Sensing observations to be modeled with a Graph Neural Network (GNN). The goal is to design Cross-Validation (CV) folds that prevent spatial leakage by using spatially blocked folds. Leakage occurs when validation nodes are within the spatial autocorrelation range or within the graph message-passing radius of training nodes. Assume a second-order stationary, isotropic Gaussian process for the target field, and let the spatial autocorrelation be given by a correlation function $\\rho(h)$ that depends only on the separation distance $h$. The effective independence range $d_{\\mathrm{eff}}$ is defined as the smallest $h$ such that $\\rho(h) \\le \\tau$, where $\\tau \\in (0,1)$ is a user-specified threshold for negligible correlation. The required block-buffer distance to avoid leakage is $d_{\\mathrm{block}} = \\max\\{d_{\\mathrm{eff}}, r_g\\}$, where $r_g$ is the GNN graph connectivity/message-passing radius.\n\nUse the following well-tested correlation models from geostatistics as the foundational base:\n- Exponential correlation: $\\rho(h) = \\exp\\{-h / \\ell\\}$, where $\\ell > 0$ is the length scale.\n- Matérn correlation: $\\rho(h) = \\dfrac{2^{1 - \\nu}}{\\Gamma(\\nu)} \\left(\\dfrac{h}{\\ell}\\right)^{\\nu} K_{\\nu}\\!\\left(\\dfrac{h}{\\ell}\\right)$, where $\\nu > 0$ is the smoothness, $\\ell > 0$ is the length scale, $\\Gamma(\\cdot)$ is the Gamma function, and $K_{\\nu}(\\cdot)$ is the modified Bessel function of the second kind.\n\nConstruct $k$ spatially blocked folds as follows:\n1. Compute $d_{\\mathrm{eff}}$ as the smallest $h$ such that $\\rho(h) \\le \\tau$ for the given correlation model and parameters.\n2. Set $d_{\\mathrm{block}} = \\max\\{d_{\\mathrm{eff}}, r_g\\}$.\n3. Discretize the spatial domain into square blocks of side length $b$ with $b \\ge d_{\\mathrm{block}}$. Assign each node to a block via integer division of its coordinates by $b$ in kilometers.\n4. Map each unique block to a fold index in $\\{0,1,\\dots,k-1\\}$ using a deterministic hash of the block indices. For a given fold $f$, the validation set consists of all nodes whose blocks map to $f$.\n5. To prevent leakage, exclude from the training set any node whose Euclidean distance to any validation node is strictly less than $d_{\\mathrm{block}}$.\n\nFor each fold, compute the minimum inter-set distance $d_{\\min}$ between the final training and validation sets. A fold is leakage-free if $d_{\\min} \\ge d_{\\mathrm{block}}$. A test case is leakage-free if all folds are leakage-free.\n\nAll distances must be computed and reported in kilometers (km). Round $d_{\\mathrm{eff}}$ to three decimal places. Angles are not used. Express any fractions as decimal numbers.\n\nImplement a complete, runnable program that, for the following test suite, computes $d_{\\mathrm{eff}}$ and evaluates leakage-free blocked CV as described above:\n\nTest Suite (each case specifies domain, grid, correlation model and parameters, threshold, graph radius, and number of folds):\n- Case 1 (happy path): Domain $[0,50] \\times [0,50]$ km, nodes on a $10 \\times 10$ grid with $5$ km spacing; model exponential with $\\ell = 5$ km; threshold $\\tau = 0.05$; graph radius $r_g = 8$ km; folds $k = 5$.\n- Case 2 (Matérn general): Domain $[0,60] \\times [0,60]$ km, nodes on an $11 \\times 11$ grid with $6$ km spacing; model Matérn with $\\nu = 1.5$ and $\\ell = 7$ km; threshold $\\tau = 0.10$; graph radius $r_g = 10$ km; folds $k = 4$.\n- Case 3 (Matérn with $\\nu = 0.5$ equivalence to exponential): Domain $[0,100] \\times [0,100]$ km, nodes on an $11 \\times 11$ grid with $10$ km spacing; model Matérn with $\\nu = 0.5$ and $\\ell = 12$ km; threshold $\\tau = 0.20$; graph radius $r_g = 5$ km; folds $k = 6$.\n- Case 4 (edge case: domain too small): Domain $[0,20] \\times [0,20]$ km, nodes on a $5 \\times 5$ grid with $5$ km spacing; model exponential with $\\ell = 8$ km; threshold $\\tau = 0.05$; graph radius $r_g = 3$ km; folds $k = 3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is a list of the form $[d_{\\mathrm{eff}}, \\text{leak\\_free}]$ with $d_{\\mathrm{eff}}$ rounded to three decimals and `leak_free` a boolean indicating whether all folds in the case are leakage-free, for example: $[[14.979, True], [..., False], ...]$.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of geostatistics and machine learning, well-posed with a clear algorithmic procedure, and contains all necessary data for a unique solution. I will therefore proceed with a complete solution.\n\nThe problem requires the design and evaluation of a spatially blocked cross-validation (CV) strategy for a Graph Neural Network (GNN) model applied to geospatial data. The core principle is to prevent information leakage between training and validation sets, which can occur due to spatial autocorrelation. When data points are near each other, their values are often correlated. If training and validation sets contain spatially proximate points, a model may learn to predict validation targets simply by proximity to training data, leading to an overly optimistic estimate of its generalization performance.\n\nOur approach is structured as follows:\n1.  Quantify the spatial autocorrelation range.\n2.  Define a buffer distance that accounts for both this autocorrelation and the GNN's message-passing radius.\n3.  Construct CV folds using spatial blocks that are separated by this buffer distance.\n4.  Verify that the resulting training and validation sets for each fold are indeed spatially separated.\n\n**1. Quantifying Spatial Autocorrelation: The Effective Independence Range ($d_{\\mathrm{eff}}$)**\n\nThe problem posits that the underlying geospatial field follows a second-order stationary and isotropic Gaussian process. This implies that the correlation between observations at two locations depends only on the Euclidean distance $h$ between them, described by a correlation function $\\rho(h)$. We define the effective independence range, $d_{\\mathrm{eff}}$, as the minimum distance at which the spatial correlation drops below a specified negligible threshold $\\tau$. Mathematically, $d_{\\mathrm{eff}}$ is the solution to the equation:\n$$\n\\rho(h) = \\tau\n$$\nSince correlation functions $\\rho(h)$ are monotonically decreasing for $h \\ge 0$, a unique solution for $h$ exists for any $\\tau \\in (0,1)$.\n\nThe problem specifies two correlation models:\n-   **Exponential Model**: The correlation is given by $\\rho(h) = \\exp\\{-h / \\ell\\}$, where $\\ell$ is the length scale. We can solve for $d_{\\mathrm{eff}}$ analytically:\n    $$\n    \\exp\\{-d_{\\mathrm{eff}} / \\ell\\} = \\tau \\implies -d_{\\mathrm{eff}} / \\ell = \\ln(\\tau) \\implies d_{\\mathrm{eff}} = -\\ell \\ln(\\tau)\n    $$\n-   **Matérn Model**: The correlation is $\\rho(h) = \\dfrac{2^{1 - \\nu}}{\\Gamma(\\nu)} \\left(\\dfrac{h}{\\ell}\\right)^{\\nu} K_{\\nu}\\!\\left(\\dfrac{h}{\\ell}\\right)$, where $\\nu$ is a smoothness parameter, $\\ell$ is the length scale, $\\Gamma(\\cdot)$ is the Gamma function, and $K_{\\nu}(\\cdot)$ is the modified Bessel function of the second kind. For this model, an analytical solution for $h$ is generally not available. We must find the root of the function $f(h) = \\rho(h) - \\tau$ using a numerical method. Given that $f(h)$ is monotonic, a robust root-finding algorithm like the Brent-Dekker method (`brentq`) is suitable.\n\nA special case of the Matérn model, when $\\nu=0.5$, simplifies to the exponential model, $\\rho(h) = \\exp(-h/\\ell)$, for which the analytical solution can be used.\n\n**2. Defining the Block-Buffer Distance ($d_{\\mathrm{block}}$)**\n\nLeakage can occur through two mechanisms: implicit correlation in the underlying field and explicit information flow in the GNN. The GNN propagates information between nodes up to a distance determined by its number of layers and graph connectivity, which we denote as the message-passing radius $r_g$. To prevent both types of leakage, we must enforce a separation distance that accounts for the larger of these two ranges. Thus, the required block-buffer distance, $d_{\\mathrm{block}}$, is defined as:\n$$\nd_{\\mathrm{block}} = \\max\\{d_{\\mathrm{eff}}, r_g\\}\n$$\n\n**3. Spatially Blocked Cross-Validation Algorithm**\n\nWith $d_{\\mathrm{block}}$ established, we construct $k$ CV folds using the following deterministic procedure:\n1.  **Node Generation**: First, we generate the set of node coordinates based on the specified grid size and spacing within the given domain.\n2.  **Domain Discretization**: The spatial domain is partitioned into a grid of square blocks. To ensure the blocks are large enough to facilitate separation, we set the side length of each block, $b$, to be equal to $d_{\\mathrm{block}}$. A node at coordinates $(x, y)$ is assigned to the block with integer indices $(\\lfloor x/b \\rfloor, \\lfloor y/b \\rfloor)$.\n3.  **Fold Assignment**: Each unique block, identified by its index tuple, is deterministically mapped to one of the $k$ folds. This is achieved by applying a hash function to the block index tuple and taking the result modulo $k$. All nodes within the same block are thus assigned to the same fold.\n4.  **Buffered Training Set Construction**: For each fold $f \\in \\{0, 1, \\dots, k-1\\}$:\n    a. The **validation set** consists of all nodes assigned to fold $f$.\n    b. The initial **training set** consists of all nodes not in the validation set.\n    c. To prevent leakage, we create a buffer zone around the validation set. Any node in the initial training set whose Euclidean distance to *any* node in the validation set is strictly less than $d_{\\mathrm{block}}$ is removed. The remaining nodes form the final, buffered training set.\n\n**4. Verification of Leakage Prevention**\n\nThe final step is to verify that the procedure was successful. For each fold, we calculate the minimum inter-set distance, $d_{\\min}$, which is the minimum Euclidean distance between any node in the final training set and any node in the validation set.\n$$\nd_{\\min} = \\min_{\\mathbf{x}_{\\text{train}} \\in \\text{TrainSet}, \\mathbf{x}_{\\text{val}} \\in \\text{ValSet}} ||\\mathbf{x}_{\\text{train}} - \\mathbf{x}_{\\text{val}}||_2\n$$\nA fold is deemed **leakage-free** if $d_{\\min} \\ge d_{\\mathrm{block}}$. By the construction of our buffered training set, this condition must hold, provided that both the final training set and the validation set are non-empty. If either set is empty, the distance is effectively infinite, and the fold is also considered leakage-free. A test case is leakage-free if and only if all $k$ of its folds are leakage-free. The implementation will compute this value for each fold to confirm the integrity of the process.\n\nThe following Python code implements this entire procedure for the provided test suite, using `numpy` for efficient array operations and `scipy` for the special functions and numerical root-finding required for the Matérn model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma, kv\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Exponential model\n        {'domain_max': 50, 'grid_n': 10, 'spacing': 5, 'model': 'exponential', \n         'params': {'ell': 5}, 'tau': 0.05, 'r_g': 8, 'k': 5},\n        # Case 2: Matérn model (general)\n        {'domain_max': 60, 'grid_n': 11, 'spacing': 6, 'model': 'matern', \n         'params': {'nu': 1.5, 'ell': 7}, 'tau': 0.10, 'r_g': 10, 'k': 4},\n        # Case 3: Matérn model (nu=0.5, equivalent to exponential)\n        {'domain_max': 100, 'grid_n': 11, 'spacing': 10, 'model': 'matern', \n         'params': {'nu': 0.5, 'ell': 12}, 'tau': 0.20, 'r_g': 5, 'k': 6},\n        # Case 4: Edge case where d_block is large relative to domain\n        {'domain_max': 20, 'grid_n': 5, 'spacing': 5, 'model': 'exponential', \n         'params': {'ell': 8}, 'tau': 0.05, 'r_g': 3, 'k': 3},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _process_case(case)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    output_parts = []\n    for res_item in results:\n        # Format d_eff to 3 decimal places, even with trailing zeros.\n        d_eff_str = f\"{res_item[0]:.3f}\"\n        leak_free_str = str(res_item[1])\n        output_parts.append(f\"[{d_eff_str},{leak_free_str}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\ndef _process_case(case_params):\n    \"\"\"\n    Processes a single test case from the suite.\n    \"\"\"\n    # 1. Generate node coordinates\n    coords = np.arange(case_params['grid_n']) * case_params['spacing']\n    xv, yv = np.meshgrid(coords, coords)\n    nodes = np.vstack([xv.ravel(), yv.ravel()]).T\n\n    # 2. Compute effective independence range d_eff\n    if case_params['model'] == 'exponential':\n        d_eff = _d_eff_exponential(case_params['params']['ell'], case_params['tau'])\n    elif case_params['model'] == 'matern':\n        d_eff = _d_eff_matern(case_params['params']['nu'], case_params['params']['ell'], case_params['tau'])\n    \n    d_eff_rounded = round(d_eff, 3)\n\n    # 3. Compute block-buffer distance d_block\n    d_block = max(d_eff_rounded, case_params['r_g'])\n    \n    # 4. Assign nodes to blocks and then to folds\n    # Use b = d_block as the block side length.\n    # Handle the case where d_block is zero or negative (though unlikely).\n    b = d_block if d_block > 0 else 1.0 \n    block_indices = np.floor(nodes / b).astype(int)\n    \n    k = case_params['k']\n    fold_assignments = np.array([hash(tuple(bi)) % k for bi in block_indices])\n\n    # 5. Iterate through folds, build buffered sets, and check for leakage\n    is_case_leak_free = True\n    for f in range(k):\n        val_mask = fold_assignments == f\n        train_mask_potential = ~val_mask\n\n        validation_nodes = nodes[val_mask]\n        train_nodes_potential = nodes[train_mask_potential]\n        \n        # If no nodes in validation set for this fold, it's trivially leakage-free.\n        if validation_nodes.shape[0] == 0:\n            continue\n            \n        # If no potential training nodes, also trivially leakage-free.\n        if train_nodes_potential.shape[0] == 0:\n            continue\n\n        # Exclude training nodes within d_block of any validation node\n        # Using squared distances for efficiency\n        dist_sq = np.sum((train_nodes_potential[:, np.newaxis, :] - validation_nodes[np.newaxis, :, :])**2, axis=-1)\n        min_dist_sq_to_val = np.min(dist_sq, axis=1)\n        \n        # The condition for keeping a node is that its minimum distance to any validation node\n        # is GREATER THAN OR EQUAL TO d_block.\n        keep_mask = min_dist_sq_to_val >= d_block**2\n        \n        final_train_nodes = train_nodes_potential[keep_mask]\n        \n        # Verify leakage-free condition\n        # This is a verification step. By construction, this should always be true\n        # unless one of the final sets is empty.\n        is_fold_leak_free = True\n        if final_train_nodes.shape[0] > 0 and validation_nodes.shape[0] > 0:\n            final_dist_sq = np.sum((final_train_nodes[:, np.newaxis, :] - validation_nodes[np.newaxis, :, :])**2, axis=-1)\n            d_min_sq = np.min(final_dist_sq)\n            d_min = np.sqrt(d_min_sq)\n            \n            # Use a small tolerance for floating point comparisons\n            if not np.isclose(d_min, d_block) and d_min < d_block:\n                is_fold_leak_free = False\n        \n        if not is_fold_leak_free:\n            is_case_leak_free = False\n            break\n\n    return [d_eff_rounded, is_case_leak_free]\n\ndef _d_eff_exponential(ell, tau):\n    \"\"\"Calculates d_eff for the Exponential correlation model.\"\"\"\n    return -ell * np.log(tau)\n\ndef _matern_corr(h, nu, ell):\n    \"\"\"Computes the Matérn correlation.\"\"\"\n    if h == 0:\n        return 1.0\n    # To prevent division by zero or negative inputs for h\n    h = np.maximum(h, 1e-9)\n        \n    const = 2**(1 - nu) / gamma(nu)\n    arg = h / ell\n    \n    return const * (arg**nu) * kv(nu, arg)\n\ndef _d_eff_matern(nu, ell, tau):\n    \"\"\"Calculates d_eff for the Matérn model using a numerical solver.\"\"\"\n    # Special case for nu=0.5, which is equivalent to exponential\n    if np.isclose(nu, 0.5):\n        return _d_eff_exponential(ell, tau)\n    \n    # Objective function for the root finder\n    objective = lambda h: _matern_corr(h, nu, ell) - tau\n    \n    # Find the root h where the correlation equals the threshold tau.\n    # The function is monotonic, so a root is guaranteed.\n    # Bracket [1e-9, large_number] is safe.\n    try:\n        d_eff = brentq(objective, 1e-9, ell * 50)\n    except ValueError:\n        # Fallback if root is not in bracket, though unlikely with this large range\n        d_eff = brentq(objective, 1e-9, ell * 500)\n    \n    return d_eff\n\nsolve()\n\n```"
        }
    ]
}