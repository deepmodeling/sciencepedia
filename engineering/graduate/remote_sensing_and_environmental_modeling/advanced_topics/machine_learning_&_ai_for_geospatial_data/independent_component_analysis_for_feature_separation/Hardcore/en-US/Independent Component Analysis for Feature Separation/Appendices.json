{
    "hands_on_practices": [
        {
            "introduction": "A critical first step in many signal processing pipelines, including Independent Component Analysis (ICA), is whitening the data. This process, often accomplished using Principal Component Analysis (PCA), transforms the data to have zero mean and an identity covariance matrix, ensuring the components are uncorrelated. However, this is only a second-order property. This exercise  provides a crucial hands-on demonstration that zero correlation does not guarantee statistical independence, motivating the need for methods like ICA that leverage higher-order statistics to uncover the true underlying structure in environmental data.",
            "id": "3822180",
            "problem": "A hyperspectral imaging retrieval in a remote sensing and environmental modeling workflow seeks to separate latent biophysical drivers via Independent Component Analysis (ICA). Whitening in ICA is used to enforce zero mean and identity covariance for the observed feature vector. However, zero correlation does not imply independence. To demonstrate this within a scientifically realistic setup, consider a single latent standardized driver $S$ representing a normalized leaf water content anomaly, modeled as a continuous random variable with a symmetric probability density function (PDF). Field engineers apply nonlinear preprocessing to derive two engineered features from $S$ to mitigate sensor nonlinearity:\n- A saturating index defined by $X_{1} = g_{1}(S) = S - S^{3}$,\n- A quadratic index centered to zero mean defined by $X_{2} = g_{2}(S) = S^{2} - \\mathbb{E}[S^{2}]$.\nAssume $S \\sim \\mathrm{Uniform}([-1,1])$, which models a bounded normalized anomaly typical of biophysical indices after min-max normalization, with PDF $f_{S}(s) = \\frac{1}{2}$ for $s \\in [-1,1]$ and zero otherwise.\n\nStarting from the fundamental definitions that (i) independence means $\\mathbb{E}[h(X_{1})\\,k(X_{2})] = \\mathbb{E}[h(X_{1})]\\mathbb{E}[k(X_{2})]$ for all bounded measurable functions $h$ and $k$, and (ii) zero correlation means $\\mathrm{Cov}(X_{1},X_{2}) = \\mathbb{E}[X_{1}X_{2}] - \\mathbb{E}[X_{1}]\\mathbb{E}[X_{2}] = 0$, analyze this counterexample. First, verify that the pair $(X_{1},X_{2})$ is decorrelated (zero covariance) under the stated model. Then, to explicitly demonstrate that decorrelation does not imply independence, compute the fourth-order mixed moment discrepancy\n$$\n\\Delta \\equiv \\mathbb{E}\\!\\left[X_{1}^{2}X_{2}^{2}\\right] - \\mathbb{E}\\!\\left[X_{1}^{2}\\right]\\mathbb{E}\\!\\left[X_{2}^{2}\\right].\n$$\nA nonzero value of $\\Delta$ certifies statistical dependence. Provide $\\Delta$ as a single exact rational number. No rounding is required. Express the final answer without units.",
            "solution": "The user wants to analyze a specific counterexample to the notion that decorrelation implies independence, within the context of Independent Component Analysis (ICA). The problem is to first verify that two engineered features, $X_{1}$ and $X_{2}$, derived from a latent variable $S$, are decorrelated. Second, it requires the calculation of a fourth-order mixed moment discrepancy, $\\Delta$, to demonstrate their statistical dependence.\n\nThe latent variable $S$ is uniformly distributed on the interval $[-1, 1]$, denoted $S \\sim \\mathrm{Uniform}([-1,1])$. Its probability density function (PDF) is $f_{S}(s) = \\frac{1}{2}$ for $s \\in [-1,1]$ and $f_S(s)=0$ otherwise.\n\nThe expected value of a function $g(S)$ is given by the integral:\n$$\n\\mathbb{E}[g(S)] = \\int_{-1}^{1} g(s) f_{S}(s) \\, ds = \\frac{1}{2} \\int_{-1}^{1} g(s) \\, ds\n$$\nWe first compute the general moments of $S$, $\\mathbb{E}[S^n]$, for integer $n \\ge 0$.\nIf $n$ is an odd integer, $s^n$ is an odd function. Since the integration interval $[-1, 1]$ is symmetric about $0$, the integral is zero.\n$$\n\\mathbb{E}[S^n] = \\frac{1}{2} \\int_{-1}^{1} s^n \\, ds = 0 \\quad (\\text{for odd } n)\n$$\nIf $n$ is an even integer, $s^n$ is an even function. The integral becomes:\n$$\n\\mathbb{E}[S^n] = \\frac{1}{2} \\int_{-1}^{1} s^n \\, ds = \\frac{1}{2} \\cdot 2 \\int_{0}^{1} s^n \\, ds = \\left[ \\frac{s^{n+1}}{n+1} \\right]_{0}^{1} = \\frac{1}{n+1} \\quad (\\text{for even } n)\n$$\nWe will need the following specific moments:\n$\\mathbb{E}[S] = 0$\n$\\mathbb{E}[S^2] = \\frac{1}{3}$\n$\\mathbb{E}[S^3] = 0$\n$\\mathbb{E}[S^4] = \\frac{1}{5}$\n$\\mathbb{E}[S^5] = 0$\n$\\mathbb{E}[S^6] = \\frac{1}{7}$\n$\\mathbb{E}[S^8] = \\frac{1}{9}$\n$\\mathbb{E}[S^{10}] = \\frac{1}{11}$\n\nThe two engineered features are defined as:\n$X_{1} = S - S^{3}$\n$X_{2} = S^{2} - \\mathbb{E}[S^{2}] = S^{2} - \\frac{1}{3}$\n\n**Part 1: Verify Decorrelation**\nTwo random variables $X_1$ and $X_2$ are decorrelated if their covariance is zero: $\\mathrm{Cov}(X_{1},X_{2}) = 0$. The covariance is defined as $\\mathrm{Cov}(X_{1},X_{2}) = \\mathbb{E}[X_{1}X_{2}] - \\mathbb{E}[X_{1}]\\mathbb{E}[X_{2}]$.\n\nFirst, we compute the expected values of $X_1$ and $X_2$:\n$$\n\\mathbb{E}[X_{1}] = \\mathbb{E}[S - S^{3}] = \\mathbb{E}[S] - \\mathbb{E}[S^{3}] = 0 - 0 = 0\n$$\n$$\n\\mathbb{E}[X_{2}] = \\mathbb{E}\\left[S^{2} - \\frac{1}{3}\\right] = \\mathbb{E}[S^{2}] - \\frac{1}{3} = \\frac{1}{3} - \\frac{1}{3} = 0\n$$\nSince $\\mathbb{E}[X_{1}] = 0$ and $\\mathbb{E}[X_{2}] = 0$, the covariance simplifies to $\\mathrm{Cov}(X_{1},X_{2}) = \\mathbb{E}[X_{1}X_{2}]$.\nWe compute the expectation of the product $X_{1}X_{2}$:\n$$\n\\mathbb{E}[X_{1}X_{2}] = \\mathbb{E}\\left[(S-S^3)\\left(S^2 - \\frac{1}{3}\\right)\\right] = \\mathbb{E}\\left[S^3 - \\frac{1}{3}S - S^5 + \\frac{1}{3}S^3\\right] = \\mathbb{E}\\left[- \\frac{1}{3}S + \\frac{4}{3}S^3 - S^5\\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[X_{1}X_{2}] = -\\frac{1}{3}\\mathbb{E}[S] + \\frac{4}{3}\\mathbb{E}[S^3] - \\mathbb{E}[S^5] = -\\frac{1}{3}(0) + \\frac{4}{3}(0) - 0 = 0\n$$\nSince $\\mathrm{Cov}(X_{1},X_{2}) = 0$, the variables $X_1$ and $X_2$ are decorrelated. This completes the verification.\n\n**Part 2: Compute the Discrepancy $\\Delta$**\nThe discrepancy is defined as $\\Delta = \\mathbb{E}[X_{1}^{2}X_{2}^{2}] - \\mathbb{E}[X_{1}^{2}]\\mathbb{E}[X_{2}^{2}]$. A non-zero $\\Delta$ indicates that $X_1^2$ and $X_2^2$ are correlated, which in turn proves that $X_1$ and $X_2$ are not independent. We calculate each term separately.\n\nFirst, calculate $\\mathbb{E}[X_{1}^{2}]$:\n$X_{1}^{2} = (S-S^{3})^2 = S^2 - 2S^4 + S^6$\n$$\n\\mathbb{E}[X_{1}^{2}] = \\mathbb{E}[S^2 - 2S^4 + S^6] = \\mathbb{E}[S^2] - 2\\mathbb{E}[S^4] + \\mathbb{E}[S^6]\n$$\n$$\n\\mathbb{E}[X_{1}^{2}] = \\frac{1}{3} - 2\\left(\\frac{1}{5}\\right) + \\frac{1}{7} = \\frac{1}{3} - \\frac{2}{5} + \\frac{1}{7} = \\frac{35 - 42 + 15}{105} = \\frac{8}{105}\n$$\nNext, calculate $\\mathbb{E}[X_{2}^{2}]$:\n$X_{2}^{2} = \\left(S^2 - \\frac{1}{3}\\right)^2 = S^4 - \\frac{2}{3}S^2 + \\frac{1}{9}$\n$$\n\\mathbb{E}[X_{2}^{2}] = \\mathbb{E}\\left[S^4 - \\frac{2}{3}S^2 + \\frac{1}{9}\\right] = \\mathbb{E}[S^4] - \\frac{2}{3}\\mathbb{E}[S^2] + \\frac{1}{9}\n$$\n$$\n\\mathbb{E}[X_{2}^{2}] = \\frac{1}{5} - \\frac{2}{3}\\left(\\frac{1}{3}\\right) + \\frac{1}{9} = \\frac{1}{5} - \\frac{2}{9} + \\frac{1}{9} = \\frac{1}{5} - \\frac{1}{9} = \\frac{9-5}{45} = \\frac{4}{45}\n$$\nNow, we calculate the product term of $\\Delta$:\n$$\n\\mathbb{E}[X_{1}^{2}]\\mathbb{E}[X_{2}^{2}] = \\left(\\frac{8}{105}\\right)\\left(\\frac{4}{45}\\right) = \\frac{32}{4725}\n$$\nFinally, we calculate $\\mathbb{E}[X_{1}^{2}X_{2}^{2}]$:\n$X_{1}^{2}X_{2}^{2} = (S^2 - 2S^4 + S^6)\\left(S^4 - \\frac{2}{3}S^2 + \\frac{1}{9}\\right)$\nExpanding this product gives:\n$S^6 - \\frac{2}{3}S^4 + \\frac{1}{9}S^2 - 2S^8 + \\frac{4}{3}S^6 - \\frac{2}{9}S^4 + S^{10} - \\frac{2}{3}S^8 + \\frac{1}{9}S^6$\nCollecting terms by power of $S$:\n$X_{1}^{2}X_{2}^{2} = S^{10} + \\left(-2-\\frac{2}{3}\\right)S^8 + \\left(1+\\frac{4}{3}+\\frac{1}{9}\\right)S^6 + \\left(-\\frac{2}{3}-\\frac{2}{9}\\right)S^4 + \\frac{1}{9}S^2$\n$X_{1}^{2}X_{2}^{2} = S^{10} - \\frac{8}{3}S^8 + \\frac{22}{9}S^6 - \\frac{8}{9}S^4 + \\frac{1}{9}S^2$\nTaking the expectation:\n$$\n\\mathbb{E}[X_{1}^{2}X_{2}^{2}] = \\mathbb{E}[S^{10}] - \\frac{8}{3}\\mathbb{E}[S^8] + \\frac{22}{9}\\mathbb{E}[S^6] - \\frac{8}{9}\\mathbb{E}[S^4] + \\frac{1}{9}\\mathbb{E}[S^2]\n$$\n$$\n\\mathbb{E}[X_{1}^{2}X_{2}^{2}] = \\frac{1}{11} - \\frac{8}{3}\\left(\\frac{1}{9}\\right) + \\frac{22}{9}\\left(\\frac{1}{7}\\right) - \\frac{8}{9}\\left(\\frac{1}{5}\\right) + \\frac{1}{9}\\left(\\frac{1}{3}\\right)\n$$\n$$\n= \\frac{1}{11} - \\frac{8}{27} + \\frac{22}{63} - \\frac{8}{45} + \\frac{1}{27} = \\frac{1}{11} - \\frac{7}{27} + \\frac{22}{63} - \\frac{8}{45}\n$$\nThe least common multiple of the denominators $(11, 27, 63, 45)$ is $10395$.\n$$\n\\mathbb{E}[X_{1}^{2}X_{2}^{2}] = \\frac{945}{10395} - \\frac{7 \\cdot 385}{10395} + \\frac{22 \\cdot 165}{10395} - \\frac{8 \\cdot 231}{10395}\n$$\n$$\n= \\frac{945 - 2695 + 3630 - 1848}{10395} = \\frac{4575 - 4543}{10395} = \\frac{32}{10395}\n$$\nNow we compute $\\Delta$:\n$$\n\\Delta = \\mathbb{E}[X_{1}^{2}X_{2}^{2}] - \\mathbb{E}[X_{1}^{2}]\\mathbb{E}[X_{2}^{2}] = \\frac{32}{10395} - \\frac{32}{4725}\n$$\nTo subtract these fractions, we find a common denominator.\n$10395 = 3^3 \\cdot 5 \\cdot 7 \\cdot 11$\n$4725 = 3^3 \\cdot 5^2 \\cdot 7$\nThe least common denominator is $3^3 \\cdot 5^2 \\cdot 7 \\cdot 11 = 51975$.\n$$\n\\Delta = 32\\left(\\frac{1}{10395} - \\frac{1}{4725}\\right) = 32\\left(\\frac{5}{51975} - \\frac{11}{51975}\\right) = 32\\left(\\frac{-6}{51975}\\right) = \\frac{-192}{51975}\n$$\nWe simplify this fraction. The numerator is $192 = 3 \\cdot 64$. The denominator $51975$ is divisible by $3$ (sum of digits is $27$).\n$51975 \\div 3 = 17325$.\n$$\n\\Delta = \\frac{-64}{17325}\n$$\nSince $17325$ is odd, the fraction is in simplest form. The non-zero value of $\\Delta$ confirms that $X_1$ and $X_2$ are statistically dependent, despite being decorrelated.",
            "answer": "$$\\boxed{-\\frac{64}{17325}}$$"
        },
        {
            "introduction": "With the necessity of seeking statistical independence established, we now explore the mechanics of how an ICA algorithm achieves this goal. Rather than treating it as a black box, this practice  guides you through a single optimization step derived from the powerful Maximum Likelihood (ML) framework. By calculating the gradient update, you will gain a concrete understanding of how prior assumptions about the source distributions and the geometry of the linear transformation combine to drive the demixing matrix towards a solution that maximizes the likelihood of the observed data.",
            "id": "3822243",
            "problem": "Consider the instantaneous linear mixing model for multispectral remote sensing pixels, where each observed vector $x_t \\in \\mathbb{R}^n$ at time (or sample index) $t$ is modeled as $x_t = A s_t$ for some unknown mixing matrix $A \\in \\mathbb{R}^{n \\times n}$ and latent independent source vector $s_t \\in \\mathbb{R}^n$. Independent Component Analysis (ICA) aims to recover a demixing matrix $W \\in \\mathbb{R}^{n \\times n}$ such that $y_t = W x_t$ has statistically independent components. Under the Maximum Likelihood (ML) principle, and assuming component-wise independence with chosen source priors $p_i(y)$, the log-likelihood of the observed data $\\{x_t\\}_{t=1}^T$ as a function of $W$ is\n$$\n\\mathcal{L}(W) = \\sum_{t=1}^T \\left( \\sum_{i=1}^n \\log p_i(y_{i t}) + \\log \\left| \\det W \\right| \\right),\n$$\nwhere $y_t = W x_t$ and $y_{i t}$ denotes the $i$-th component of $y_t$. The task is to implement one gradient-ascent step on $\\mathcal{L}(W)$ using the chain rule and change-of-variables principles, for two different prior choices:\n- Logistic-like prior: $\\log p_i(y) = - \\log \\cosh(y) + C_i$,\n- Laplace prior: $\\log p_i(y) = - |y| + C_i$,\nwhere $C_i$ is a constant independent of $y$ and can be ignored for optimization with respect to $W$.\n\nStarting from a given initial $W$, a dataset $\\{x_t\\}_{t=1}^T$, and a step size $\\eta > 0$, perform a single update $W_{\\text{new}} = W + \\eta \\nabla \\mathcal{L}(W)$ computed from first principles.\n\nImplement this update and compute the numerical result for each of the following test cases. For all cases, $y_t = W x_t$ must be computed exactly as stated, and for the Laplace prior at points where $y=0$, use the convention $\\frac{d}{dy} \\log p(y) \\big|_{y=0} = 0$.\n\nTest Suite:\n- Case $1$ (happy path, $n=3$ with logistic-like prior):\n  - $n = 3$, $T = 5$,\n  - $W = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$,\n  - $\\{x_t\\}_{t=1}^5$ with\n    - $x_1 = (0.34, 0.12, 0.48)$,\n    - $x_2 = (0.28, 0.20, 0.52)$,\n    - $x_3 = (0.41, 0.15, 0.38)$,\n    - $x_4 = (0.30, 0.25, 0.45)$,\n    - $x_5 = (0.36, 0.18, 0.50)$,\n  - $\\eta = 0.1$,\n  - Prior: logistic-like ($\\log p_i(y) = - \\log \\cosh(y) + C_i$).\n\n- Case $2$ (boundary condition, Laplace prior with zeros, $n=2$):\n  - $n = 2$, $T = 4$,\n  - $W = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n  - $\\{x_t\\}_{t=1}^4$ with\n    - $x_1 = (0.50, 0.00)$,\n    - $x_2 = (0.45, 0.00)$,\n    - $x_3 = (0.60, 0.00)$,\n    - $x_4 = (0.55, 0.00)$,\n  - $\\eta = 0.2$,\n  - Prior: Laplace ($\\log p_i(y) = - |y| + C_i$) with the stated convention at $y=0$.\n\n- Case $3$ (near-singular $W$ to stress the change-of-variables term, logistic-like prior, $n=2$):\n  - $n = 2$, $T = 5$,\n  - $W = \\begin{bmatrix} 1.0 & 0.97 \\\\ 0.97 & 0.941 \\end{bmatrix}$,\n  - $\\{x_t\\}_{t=1}^5$ with\n    - $x_1 = (0.40, 0.35)$,\n    - $x_2 = (0.42, 0.33)$,\n    - $x_3 = (0.38, 0.37)$,\n    - $x_4 = (0.41, 0.34)$,\n    - $x_5 = (0.39, 0.36)$,\n  - $\\eta = 0.05$,\n  - Prior: logistic-like ($\\log p_i(y) = - \\log \\cosh(y) + C_i$).\n\n- Case $4$ (large step size sensitivity, logistic-like prior, $n=3$):\n  - $n = 3$, $T = 3$,\n  - $W = \\begin{bmatrix} 0.9 & 0.1 & 0.0 \\\\ 0.05 & 1.0 & 0.05 \\\\ 0.0 & 0.1 & 0.95 \\end{bmatrix}$,\n  - $\\{x_t\\}_{t=1}^3$ with\n    - $x_1 = (0.30, 0.40, 0.50)$,\n    - $x_2 = (0.35, 0.45, 0.55)$,\n    - $x_3 = (0.25, 0.38, 0.48)$,\n  - $\\eta = 1.0$,\n  - Prior: logistic-like ($\\log p_i(y) = - \\log \\cosh(y) + C_i$).\n\nFinal output format:\n- Your program should produce a single line of output containing the updated matrices for all four cases as a comma-separated list of lists, each inner list containing the row-major flattened entries of $W_{\\text{new}}$ rounded to six decimal places. For example, an output for two cases would look like $[[w_{11}, w_{12}, \\dots], [\\dots]]$ with each $w_{ij}$ a decimal number. Do not print any additional text.",
            "solution": "The problem requires the implementation of a single gradient ascent step for optimizing the demixing matrix $W$ in Independent Component Analysis (ICA). The optimization is based on the Maximum Likelihood (ML) principle.\n\nThe instantaneous linear mixing model is given by $x_t = A s_t$, where $x_t \\in \\mathbb{R}^n$ is the observed vector, $s_t \\in \\mathbb{R}^n$ is the latent source vector with statistically independent components, and $A \\in \\mathbb{R}^{n \\times n}$ is an unknown mixing matrix. The goal of ICA is to find a demixing matrix $W \\in \\mathbb{R}^{n \\times n}$ such that the components of the estimated source vector, $y_t = W x_t$, are as statistically independent as possible.\n\nThe log-likelihood of the observed data $\\{x_t\\}_{t=1}^T$ given the model and the demixing matrix $W$ is:\n$$\n\\mathcal{L}(W) = \\sum_{t=1}^T \\left( \\sum_{i=1}^n \\log p_i(y_{i t}) + \\log \\left| \\det W \\right| \\right)\n$$\nwhere $y_t = W x_t$, $y_{it}$ is the $i$-th component of $y_t$, and $p_i$ is the prior probability distribution for the $i$-th source component. We can rewrite the log-likelihood as:\n$$\n\\mathcal{L}(W) = \\sum_{t=1}^T \\sum_{i=1}^n \\log p_i((W x_t)_i) + T \\log \\left| \\det W \\right|\n$$\nThe task is to perform one step of gradient ascent on this log-likelihood function. The update rule is given by:\n$$\nW_{\\text{new}} = W + \\eta \\nabla \\mathcal{L}(W)\n$$\nwhere $\\eta > 0$ is the step size and $\\nabla \\mathcal{L}(W)$ is the gradient of $\\mathcal{L}(W)$ with respect to the matrix $W$.\n\nTo find the gradient, we compute the partial derivative of $\\mathcal{L}(W)$ with respect to each element $W_{kl}$. The gradient is a matrix whose $(k,l)$-th entry is $\\frac{\\partial \\mathcal{L}(W)}{\\partial W_{kl}}$.\nThe log-likelihood consists of two terms. Let's find the gradient for each term separately.\n\n**1. Gradient of the prior term:**\nLet the first term be $\\mathcal{L}_1(W) = \\sum_{t=1}^T \\sum_{i=1}^n \\log p_i(y_{it})$.\nUsing the chain rule, the partial derivative with respect to $W_{kl}$ is:\n$$\n\\frac{\\partial \\mathcal{L}_1(W)}{\\partial W_{kl}} = \\sum_{t=1}^T \\sum_{i=1}^n \\frac{\\partial \\log p_i(y_{it})}{\\partial y_{it}} \\frac{\\partial y_{it}}{\\partial W_{kl}}\n$$\nLet's define the score function $\\phi_i(u) = \\frac{d}{du} \\log p_i(u)$. The relationship $y_t = W x_t$ means $y_{it} = \\sum_{j=1}^n W_{ij} x_{jt}$.\nThe partial derivative of $y_{it}$ with respect to $W_{kl}$ is:\n$$\n\\frac{\\partial y_{it}}{\\partial W_{kl}} = \\frac{\\partial}{\\partial W_{kl}} \\left( \\sum_{j=1}^n W_{ij} x_{jt} \\right) = \\delta_{ik} x_{lt}\n$$\nwhere $\\delta_{ik}$ is the Kronecker delta. This derivative is non-zero only when $i=k$.\nSubstituting this back, the sum over $i$ collapses to a single term where $i=k$:\n$$\n\\frac{\\partial \\mathcal{L}_1(W)}{\\partial W_{kl}} = \\sum_{t=1}^T \\phi_k(y_{kt}) x_{lt}\n$$\nThis expression is the $(k,l)$-th element of the matrix sum $\\sum_{t=1}^T \\phi(y_t) x_t^T$, where $\\phi(y_t)$ is a column vector with elements $\\phi_i(y_{it})$.\nSo, the gradient of the first term in matrix form is:\n$$\n\\nabla_W \\mathcal{L}_1(W) = \\sum_{t=1}^T \\phi(W x_t) x_t^T\n$$\n\n**2. Gradient of the determinant term:**\nLet the second term be $\\mathcal{L}_2(W) = T \\log \\left| \\det W \\right|$.\nA standard result from matrix calculus states that the gradient of the log-determinant is:\n$$\n\\nabla_W (\\log \\left| \\det W \\right|) = (W^{-1})^T = W^{-T}\n$$\nTherefore, the gradient of the second term is:\n$$\n\\nabla_W \\mathcal{L}_2(W) = T (W^{-1})^T\n$$\nThis term acts as a barrier, preventing the matrix $W$ from becoming singular (where $\\det W = 0$).\n\n**Total Gradient and Update Rule:**\nCombining the two parts, the total gradient of the log-likelihood is:\n$$\n\\nabla \\mathcal{L}(W) = \\sum_{t=1}^T \\phi(W x_t) x_t^T + T (W^{-1})^T\n$$\nThe gradient ascent update rule for one step is:\n$$\nW_{\\text{new}} = W + \\eta \\left( \\sum_{t=1}^T \\phi(W x_t) x_t^T + T (W^{-1})^T \\right)\n$$\n\n**Score Functions for Specified Priors:**\nWe need to determine the score function $\\phi(y)$ for the two given priors.\n\n- **Logistic-like prior**: $\\log p_i(y) = - \\log \\cosh(y) + C_i$.\n  $$\n  \\phi(y) = \\frac{d}{dy} (-\\log \\cosh(y)) = - \\frac{1}{\\cosh(y)} \\frac{d}{dy}(\\cosh(y)) = - \\frac{\\sinh(y)}{\\cosh(y)} = -\\tanh(y)\n  $$\n\n- **Laplace prior**: $\\log p_i(y) = -|y| + C_i$.\n  $$\n  \\phi(y) = \\frac{d}{dy} (-|y|) = -\\text{sgn}(y)\n  $$\n  For $y=0$, the derivative is undefined. The problem specifies the convention $\\frac{d}{dy} \\log p(y) \\big|_{y=0} = 0$. This means $\\phi(0) = 0$, which is consistent with the standard definition of the sign function where $\\text{sgn}(0)=0$.\n\n**Algorithm for Implementation:**\nFor each test case with parameters $W$, $\\{x_t\\}_{t=1}^T$, $\\eta$, $T$ and the chosen prior:\n1.  Represent the data $\\{x_t\\}$ as an $n \\times T$ matrix $X$, where the $t$-th column is the vector $x_t$.\n2.  Compute the estimated sources: $Y = WX$. This is an $n \\times T$ matrix where the $t$-th column is $y_t$.\n3.  Compute the score function matrix $\\Phi(Y)$ of the same size as $Y$.\n    - For the logistic-like prior, $\\Phi(Y) = -\\tanh(Y)$ (element-wise).\n    - For the Laplace prior, $\\Phi(Y) = -\\text{sgn}(Y)$ (element-wise).\n4.  Compute the first part of the gradient: $G_1 = \\Phi(Y) X^T$.\n5.  Compute the inverse of $W$, $W^{-1}$.\n6.  Compute the second part of the gradient: $G_2 = T (W^{-1})^T$.\n7.  Calculate the total gradient: $\\nabla_W \\mathcal{L} = G_1 + G_2$.\n8.  Perform the update: $W_{\\text{new}} = W + \\eta \\nabla_W \\mathcal{L}$.\n9.  The final result for the case is the row-major flattened $W_{\\text{new}}$ matrix, with each element rounded to six decimal places.\n\nThis procedure will be applied to each of the four test cases provided.",
            "answer": "```\n[[0.938883,0.481261,0.412498,0.487056,0.92383,0.414008,0.422631,0.418461,0.916942],[-0.56,0.8,-0.8,0.8],[1.341499,0.370591,0.40562,1.319525],[0.606708,0.428612,-3.54117,1.839818,-1.761279,-2.871891,-2.483984,-1.821013,1.488667]]\n```"
        },
        {
            "introduction": "After applying an ICA algorithm to separate a mixture of signals, the final and critical step is to evaluate its performance. In practical applications and especially in simulation studies where the ground truth is known, quantitative metrics are essential for assessing the quality of separation. This practice  introduces two industry-standard metrics: the Signal-to-Interference Ratio ($SIR$) and the Signal-to-Distortion Ratio ($SDR$). By calculating these values for a given recovered signal, you will learn how to precisely quantify the algorithm's success in isolating a target source from both interfering sources and measurement noise.",
            "id": "3822206",
            "problem": "A remote sensing instrument records two spectral channels that linearly mix two independent environmental features: aerosol optical depth anomaly and vegetation index anomaly. Assume an instantaneous linear mixture with additive measurement noise, modeled as $x = A s + n$, where $x \\in \\mathbb{R}^{2 \\times T}$ is the observed data across $T$ spatial samples, $A \\in \\mathbb{R}^{2 \\times 2}$ is the mixing matrix, $s \\in \\mathbb{R}^{2 \\times T}$ stacks the two independent source signals, and $n \\in \\mathbb{R}^{2 \\times T}$ is additive noise. The ground-truth source signals across $T = 4$ samples are\n$$\ns_1 = [\\,2,\\,-2,\\,2,\\,-2\\,], \\quad s_2 = [\\,1,\\,1,\\,-1,\\,-1\\,],\n$$\nand the mixing matrix and noise are\n$$\nA = \\begin{pmatrix} 1.0 & 0.4 \\\\ 0.2 & 1.2 \\end{pmatrix}, \\quad\nn = \\begin{pmatrix}\n0.1 & -0.1 & 0.0 & 0.05 \\\\\n-0.05 & 0.0 & 0.1 & -0.05\n\\end{pmatrix}.\n$$\nAn Independent Component Analysis (ICA) algorithm estimates a demixing matrix $W \\in \\mathbb{R}^{2 \\times 2}$, yielding recovered components $y = W x$, with\n$$\nW = \\begin{pmatrix} 1.02 & -0.35 \\\\ -0.15 & 0.88 \\end{pmatrix}.\n$$\nUsing energy-based blind source separation evaluation grounded in orthogonal projections onto the source subspace spanned by $\\{s_1, s_2\\}$, compute the Signal-to-Interference Ratio (SIR) and the Signal-to-Distortion Ratio (SDR) in decibels for the recovered component corresponding to $s_1$, that is, for the first recovered source $y_1$. Express both SIR and SDR in decibels and round your answers to three significant figures. Provide the final pair as a row matrix in the order $[\\,\\text{SIR},\\,\\text{SDR}\\,]$.",
            "solution": "The problem requires the computation of the Signal-to-Interference Ratio (SIR) and Signal-to-Distortion Ratio (SDR) for the first recovered component in an Independent Component Analysis (ICA) context.\n\nFirst, we establish the relationship between the recovered signals $y$, the source signals $s$, and the noise $n$. The observed signal is given by the linear mixing model:\n$$x = As + n$$\nThe recovered signal is obtained by applying the estimated demixing matrix $W$:\n$$y = Wx = W(As + n) = WAs + Wn$$\nLet's define the global transfer matrix $G = WA$ and the filtered noise $n' = Wn$. The equation becomes:\n$$y = Gs + n'$$\nThe problem specifies the matrices $A$ and $W$:\n$$A = \\begin{pmatrix} 1.0 & 0.4 \\\\ 0.2 & 1.2 \\end{pmatrix}, \\quad W = \\begin{pmatrix} 1.02 & -0.35 \\\\ -0.15 & 0.88 \\end{pmatrix}$$\nWe compute the global matrix $G$:\n$$G = WA = \\begin{pmatrix} 1.02 & -0.35 \\\\ -0.15 & 0.88 \\end{pmatrix} \\begin{pmatrix} 1.0 & 0.4 \\\\ 0.2 & 1.2 \\end{pmatrix}$$\n$$G = \\begin{pmatrix} (1.02)(1.0) + (-0.35)(0.2) & (1.02)(0.4) + (-0.35)(1.2) \\\\ (-0.15)(1.0) + (0.88)(0.2) & (-0.15)(0.4) + (0.88)(1.2) \\end{pmatrix}$$\n$$G = \\begin{pmatrix} 1.02 - 0.07 & 0.408 - 0.42 \\\\ -0.15 + 0.176 & -0.06 + 1.056 \\end{pmatrix} = \\begin{pmatrix} 0.95 & -0.012 \\\\ 0.026 & 0.996 \\end{pmatrix}$$\nThe recovered signals $y_1$ and $y_2$ (the rows of matrix $y$) can be expressed in terms of the source signals $s_1$ and $s_2$ (the rows of matrix $s$) and the filtered noise. We are interested in $y_1$:\n$$y_1 = G_{11}s_1 + G_{12}s_2 + n'_1$$\nwhere $n'_1$ is the first row of $n' = Wn$. Since $G_{11} \\approx 1$ and $G_{12} \\approx 0$, $y_1$ is the recovered component corresponding to the source $s_1$.\n\nThe evaluation metrics SIR and SDR are based on the decomposition of the recovered signal $y_1$ into three orthogonal components: a target signal component, an interference component, and an artifact (noise) component. This decomposition is achieved by orthogonal projection onto the source signal subspace.\nThe source signals are given as:\n$$s_1 = [\\,2,\\,-2,\\,2,\\,-2\\,], \\quad s_2 = [\\,1,\\,1,\\,-1,\\,-1\\,]$$\nWe check for orthogonality by computing their dot product:\n$$s_1 \\cdot s_2 = (2)(1) + (-2)(1) + (2)(-1) + (-2)(-1) = 2 - 2 - 2 + 2 = 0$$\nSince the sources are orthogonal, they form an orthogonal basis for the source subspace. The decomposition of $y_1$ is:\n- Target component: $s_{\\text{target}} = \\text{proj}_{s_1}(y_1) = \\frac{y_1 \\cdot s_1}{\\|s_1\\|^2} s_1$\n- Interference component: $e_{\\text{interf}} = \\text{proj}_{s_2}(y_1) = \\frac{y_1 \\cdot s_2}{\\|s_2\\|^2} s_2$\n- Artifact component: $e_{\\text{artif}} = y_1 - s_{\\text{target}} - e_{\\text{interf}}$\n\nThe SIR and SDR in decibels (dB) are defined by the energy (squared L2-norm) of these components:\n$$\\text{SIR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|e_{\\text{interf}}\\|^2} \\right)$$\n$$\\text{SDR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|e_{\\text{interf}} + e_{\\text{artif}}\\|^2} \\right)$$\nThe total distortion term is $e_{\\text{interf}} + e_{\\text{artif}} = y_1 - s_{\\text{target}}$.\n\nTo calculate these values, we first need to compute the recovered signal vector $y_1$.\nFirst, we find the observed signal matrix $x = As+n$.\n$$As = \\begin{pmatrix} 1.0 & 0.4 \\\\ 0.2 & 1.2 \\end{pmatrix} \\begin{pmatrix} 2 & -2 & 2 & -2 \\\\ 1 & 1 & -1 & -1 \\end{pmatrix} = \\begin{pmatrix} 2.4 & -1.6 & 1.6 & -2.4 \\\\ 1.6 & 0.8 & -0.8 & -1.6 \\end{pmatrix}$$\n$$n = \\begin{pmatrix} 0.1 & -0.1 & 0.0 & 0.05 \\\\ -0.05 & 0.0 & 0.1 & -0.05 \\end{pmatrix}$$\n$$x = As + n = \\begin{pmatrix} 2.5 & -1.7 & 1.6 & -2.35 \\\\ 1.55 & 0.8 & -0.7 & -1.65 \\end{pmatrix}$$\nLet $x_1$ and $x_2$ be the rows of $x$. Now we compute $y_1$, the first row of $y=Wx$.\n$$y_1 = W_{11}x_1 + W_{12}x_2 = 1.02 x_1 - 0.35 x_2$$\n$$y_1 = 1.02 [\\,2.5, -1.7, 1.6, -2.35\\,] - 0.35 [\\,1.55, 0.8, -0.7, -1.65\\,]$$\n$$y_1 = [\\,2.55, -1.734, 1.632, -2.397\\,] - [\\,0.5425, 0.28, -0.245, -0.5775\\,]$$\n$$y_1 = [\\,2.0075, -2.014, 1.877, -1.8195\\,]$$\nNext, we compute the necessary dot products and norms.\n$$\\|s_1\\|^2 = 2^2 + (-2)^2 + 2^2 + (-2)^2 = 16$$\n$$\\|s_2\\|^2 = 1^2 + 1^2 + (-1)^2 + (-1)^2 = 4$$\n$$y_1 \\cdot s_1 = (2.0075)(2) + (-2.014)(-2) + (1.877)(2) + (-1.8195)(-2) = 4.015 + 4.028 + 3.754 + 3.639 = 15.436$$\n$$y_1 \\cdot s_2 = (2.0075)(1) + (-2.014)(1) + (1.877)(-1) + (-1.8195)(-1) = 2.0075 - 2.014 - 1.877 + 1.8195 = -0.064$$\nNow we compute the energy of the target and interference components.\n$$\\|s_{\\text{target}}\\|^2 = \\left\\| \\frac{y_1 \\cdot s_1}{\\|s_1\\|^2} s_1 \\right\\|^2 = \\left(\\frac{15.436}{16}\\right)^2 \\|s_1\\|^2 = \\frac{15.436^2}{16^2} \\times 16 = \\frac{15.436^2}{16} = \\frac{238.270096}{16} = 14.891881$$\n$$\\|e_{\\text{interf}}\\|^2 = \\left\\| \\frac{y_1 \\cdot s_2}{\\|s_2\\|^2} s_2 \\right\\|^2 = \\left(\\frac{-0.064}{4}\\right)^2 \\|s_2\\|^2 = \\frac{(-0.064)^2}{4^2} \\times 4 = \\frac{0.004096}{4} = 0.001024$$\nWe can now calculate the SIR.\n$$\\text{SIR} = 10 \\log_{10} \\left( \\frac{14.891881}{0.001024} \\right) = 10 \\log_{10}(14542.8525...)= 41.6265... \\text{ dB}$$\nRounding to three significant figures, $\\text{SIR} = 41.6 \\text{ dB}$.\n\nFor the SDR, we need the energy of the total distortion, $\\|y_1 - s_{\\text{target}}\\|^2$. By the properties of orthogonal projection, $s_{\\text{target}}$ and $y_1 - s_{\\text{target}}$ are orthogonal. Therefore, by the Pythagorean theorem: $\\|y_1\\|^2 = \\|s_{\\text{target}}\\|^2 + \\|y_1 - s_{\\text{target}}\\|^2$.\nThe distortion energy is $\\|y_1 - s_{\\text{target}}\\|^2 = \\|y_1\\|^2 - \\|s_{\\text{target}}\\|^2$.\nFirst, calculate $\\|y_1\\|^2$:\n$$\\|y_1\\|^2 = 2.0075^2 + (-2.014)^2 + 1.877^2 + (-1.8195)^2$$\n$$\\|y_1\\|^2 = 4.03005625 + 4.056196 + 3.523129 + 3.31058025 = 14.9199615$$\nThe total distortion energy is:\n$$\\|y_1 - s_{\\text{target}}\\|^2 = 14.9199615 - 14.891881 = 0.0280805$$\nNow we calculate the SDR.\n$$\\text{SDR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|y_1 - s_{\\text{target}}\\|^2} \\right) = 10 \\log_{10} \\left( \\frac{14.891881}{0.0280805} \\right) = 10 \\log_{10}(530.324...)= 27.2454... \\text{ dB}$$\nRounding to three significant figures, $\\text{SDR} = 27.2 \\text{ dB}$.\n\nThe final answer is the pair $[\\,\\text{SIR},\\,\\text{SDR}\\,]$ with values rounded to three significant figures.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n41.6 & 27.2\n\\end{pmatrix}\n}\n$$"
        }
    ]
}