{
    "hands_on_practices": [
        {
            "introduction": "独立成分分析 (ICA) 的核心在于其优化算法，旨在找到最大化非高斯性的投影方向。FastICA 是一种高效且广泛应用的算法，它通过一种迭代的定点方法来实现这一目标。本练习将引导你亲手执行 FastICA 的单步更新过程，从而揭示该算法如何从一个初始方向开始，逐步逼近一个独立的成分。",
            "id": "3822211",
            "problem": "给定一组白化的遥感像素特征向量和一个初始方向向量。考虑使用单单元提取和不动点迭代的独立分量分析（ICA）进行特征分离。数据经过白化处理，因此每个特征的均值为零，协方差为单位矩阵。使用双曲正切非线性函数。您的任务是，对于每个测试用例，使用样本均值来近似期望，对初始方向向量应用一次不动点更新迭代，然后将更新后的向量重新归一化为单位范数。\n\n背景与定义：\n- 独立分量分析（ICA）旨在寻找一个方向 $w$，使得在适当约束下，投影 $u = w^\\top x$ 的非高斯性最大化，其中 $x$ 是白化数据向量。白化意味着零均值和单位协方差，即对于总体而言 $\\mathbb{E}[x] = 0$ 和 $\\mathbb{E}[x x^\\top] = I$，对于此处考虑的有限数据矩阵，则使用其样本模拟。\n- 快速独立分量分析（FastICA）使用一个带有选定非线性函数的不动点迭代来更新 $w$。使用双曲正切非线性函数 $g(u) = \\tanh(u)$ 及其在所提供数据矩阵上的基于样本的应用。\n- 数据矩阵以 $X \\in \\mathbb{R}^{p \\times n}$ 的形式提供，其中有 $p$ 个特征（行）和 $n$ 个样本（列）。初始方向向量为 $w_0 \\in \\mathbb{R}^p$。所有期望都将通过对 $n$ 个样本的样本均值来近似。计算更新后，强制执行 $\\|w\\|_2 = 1$。\n\n测试套件：\n- 数据集 $\\mathcal{D}_1$：一个 $2 \\times 4$ 的白化矩阵，其列向量为 $[\\sqrt{2}, 0]^\\top$、 $[-\\sqrt{2}, 0]^\\top$、 $[0, \\sqrt{2}]^\\top$、 $[0, -\\sqrt{2}]^\\top$。\n- 数据集 $\\mathcal{D}_2$：一个 $2 \\times 8$ 的白化矩阵，其列向量为 $[a, 0]^\\top$、 $[-a, 0]^\\top$、 $[b, 0]^\\top$、 $[-b, 0]^\\top$、 $[0, c]^\\top$、 $[0, -c]^\\top$、 $[0, d]^\\top$、 $[0, -d]^\\top$，其中 $a = \\sqrt{3}$，$b = 1$，$c = \\sqrt{3}$，$d = 1$。这在保持白化的同时，产生了具有更重尾部的投影。\n- 对所有期望使用各列的样本均值。单次迭代意味着从 $w_0$ 计算一次更新后的方向，然后归一化为单位范数。\n\n将此应用于以下 $5$ 个测试用例：\n1. $(X = \\mathcal{D}_1, w_0 = [0.6, 0.8]^\\top)$\n2. $(X = \\mathcal{D}_1, w_0 = [1.0, 0.0]^\\top)$\n3. $(X = \\mathcal{D}_2, w_0 = [-0.3, 0.95]^\\top)$\n4. $(X = \\mathcal{D}_2, w_0 = [10.0, -10.0]^\\top)$\n5. 边缘情况 $(X = \\mathcal{D}_1, w_0 = [10^{-9}, -10^{-9}]^\\top)$\n\n输出规格：\n- 对于每个测试用例，输出更新并归一化后的方向向量 $w$，形式为一个包含两个浮点数的列表，四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素是对应测试用例的双元素列表，并按顺序排列。例如，格式应为 $[[w_{11}, w_{12}], [w_{21}, w_{22}], \\dots]$，不含任何额外文本。\n\n不涉及物理单位或角度单位；所有输出均为无量纲。所有数学运算必须使用标准浮点算术实现。",
            "solution": "此问题已经过验证，被认为是科学上合理、定义明确且自洽的。所提供的数据和条件足以得出唯一解，并且是一致的。\n\n此问题的核心在于应用快速独立分量分析（FastICA）算法的单次迭代步骤来寻找一个单元（即找到一个独立分量）。ICA的目标是找到信号的一个线性变换，使其分量的统计独立性最大化。对于单个分量，这等同于寻找一个方向向量 $w$，使得数据 $x$ 的投影 $u = w^\\top x$ 具有最大的非高斯性。\n\nFastICA算法提供了一种有效的不动点迭代方案来寻找这样的方向 $w$。数据 $x$ 假定是白化的，意味着它具有零均值 $\\mathbb{E}[x] = 0$ 和单位协方差矩阵 $\\mathbb{E}[xx^\\top] = I$。问题指出，这些总体期望应由其在所提供数据矩阵上的相应样本均值替代。\n\n对于一个初始向量 $w$，通用的单单元FastICA更新规则如下：\n$$\nw^+ \\leftarrow \\mathbb{E}[x g(w^\\top x)] - \\mathbb{E}[g'(w^\\top x)] w\n$$\n其中 $g(u)$ 是一个合适的非二次函数，而 $g'(u)$ 是其导数。此次更新后，新向量 $w^+$ 被归一化为单位范数：\n$$\nw_{new} \\leftarrow \\frac{w^+}{\\|w^+\\|_2}\n$$\n\n问题指定使用双曲正切非线性函数，$g(u) = \\tanh(u)$。其导数为 $g'(u) = 1 - \\tanh^2(u)$。\n\n给定一个数据矩阵 $X \\in \\mathbb{R}^{p \\times n}$，包含 $n$ 个样本（列）$x_1, \\dots, x_n$，以及一个初始方向向量 $w_0 \\in \\mathbb{R}^p$，单次迭代的基于样本的更新方程为：\n1.  计算投影：对于每个样本 $i = 1, \\dots, n$，计算 $u_i = w_0^\\top x_i$。\n2.  计算未归一化的更新向量 $w^+$：\n    $$\n    w^+ = \\left( \\frac{1}{n} \\sum_{i=1}^n x_i \\tanh(w_0^\\top x_i) \\right) - \\left( \\frac{1}{n} \\sum_{i=1}^n (1 - \\tanh^2(w_0^\\top x_i)) \\right) w_0\n    $$\n3.  归一化向量：\n    $$\n    w = \\frac{w^+}{\\|w^+\\|_2}\n    $$\n我们现在将此过程应用于全部 $5$ 个测试用例。\n\n**案例 1: $(X = \\mathcal{D}_1, w_0 = [0.6, 0.8]^\\top)$**\n数据矩阵为 $X_1 = \\begin{pmatrix} \\sqrt{2}  -\\sqrt{2}  0  0 \\\\ 0  0  \\sqrt{2}  -\\sqrt{2} \\end{pmatrix}$，有 $n=4$ 个样本。初始向量为 $w_0 = \\begin{pmatrix} 0.6 \\\\ 0.8 \\end{pmatrix}$。\n\n1.  投影 $u_i = w_0^\\top x_i$：\n    $u_1 = 0.6\\sqrt{2}$，$u_2 = -0.6\\sqrt{2}$，$u_3 = 0.8\\sqrt{2}$，$u_4 = -0.8\\sqrt{2}$。\n2.  更新 $w^+$：\n    第一项是 $\\frac{1}{4} \\sum_{i=1}^4 x_i \\tanh(u_i)$。根据 tanh 的对称性，这可以简化为：\n    $$\n    \\frac{1}{4} \\left( 2 \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} \\tanh(0.6\\sqrt{2}) + 2 \\begin{pmatrix} 0 \\\\ \\sqrt{2} \\end{pmatrix} \\tanh(0.8\\sqrt{2}) \\right) = \\begin{pmatrix} \\frac{\\sqrt{2}}{2} \\tanh(0.6\\sqrt{2}) \\\\ \\frac{\\sqrt{2}}{2} \\tanh(0.8\\sqrt{2}) \\end{pmatrix} \\approx \\begin{pmatrix} 0.487732 \\\\ 0.573490 \\end{pmatrix}\n    $$\n    第二项的标量系数是 $\\frac{1}{4} \\sum_{i=1}^4 (1 - \\tanh^2(u_i))$。根据 $\\tanh^2$ 的对称性，这等于：\n    $$\n    \\frac{1}{4} \\left( 2(1 - \\tanh^2(0.6\\sqrt{2})) + 2(1 - \\tanh^2(0.8\\sqrt{2})) \\right) = 1 - \\frac{1}{2}(\\tanh^2(0.6\\sqrt{2}) + \\tanh^2(0.8\\sqrt{2})) \\approx 0.433244\n    $$\n    第二项向量为 $0.433244 \\times w_0 \\approx \\begin{pmatrix} 0.259946 \\\\ 0.346595 \\end{pmatrix}$。\n    $w^+ \\approx \\begin{pmatrix} 0.487732 \\\\ 0.573490 \\end{pmatrix} - \\begin{pmatrix} 0.259946 \\\\ 0.346595 \\end{pmatrix} = \\begin{pmatrix} 0.227786 \\\\ 0.226895 \\end{pmatrix}$。\n3.  归一化：\n    $\\|w^+\\|_2 \\approx \\sqrt{0.227786^2 + 0.226895^2} \\approx 0.321506$。\n    $w = \\frac{w^+}{\\|w^+\\|_2} \\approx \\begin{pmatrix} 0.708496 \\\\ 0.705725 \\end{pmatrix}$。\n\n**案例 2: $(X = \\mathcal{D}_1, w_0 = [1.0, 0.0]^\\top)$**\n数据矩阵是 $X_1$，且 $w_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。这个初始向量已经与数据分布的一个轴对齐。\n1.  投影：$u_1 = \\sqrt{2}$，$u_2 = -\\sqrt{2}$，$u_3 = 0$，$u_4 = 0$。\n2.  更新 $w^+$：\n    第一项：$\\frac{1}{4} \\left( 2 \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} \\tanh(\\sqrt{2}) \\right) = \\begin{pmatrix} \\frac{\\sqrt{2}}{2}\\tanh(\\sqrt{2}) \\\\ 0 \\end{pmatrix}$。\n    第二项系数：$\\frac{1}{4} (2(1-\\tanh^2(\\sqrt{2})) + 2(1-\\tanh^2(0))) = 1 - \\frac{1}{2}\\tanh^2(\\sqrt{2})$。\n    $w^+$ 的第二个分量为 $0$。第一个分量是 $\\frac{\\sqrt{2}}{2}\\tanh(\\sqrt{2}) - (1 - \\frac{1}{2}\\tanh^2(\\sqrt{2})) \\approx 0.62813 - 0.60547 = 0.02266  0$。\n3.  归一化：\n    由于 $w^+$ 是 $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 的一个正倍数，归一化后得到 $w = \\begin{pmatrix} 1.0 \\\\ 0.0 \\end{pmatrix}$。这个方向是算法的一个不动点。\n\n**案例 3: $(X = \\mathcal{D}_2, w_0 = [-0.3, 0.95]^\\top)$**\n数据矩阵 $X_2$ 有 $n=8$ 个样本，其中 $a=c=\\sqrt{3}, b=d=1$。$w_0 = \\begin{pmatrix} -0.3 \\\\ 0.95 \\end{pmatrix}$。\n1.  投影：$u_1 = -0.3\\sqrt{3}, u_2 = 0.3\\sqrt{3}, u_3 = -0.3, u_4 = 0.3, u_5 = 0.95\\sqrt{3}, u_6 = -0.95\\sqrt{3}, u_7 = 0.95, u_8 = -0.95$。\n2.  更新 $w^+$：\n    第一项：$\\frac{1}{8} \\sum x_i \\tanh(u_i) = \\frac{1}{4} \\begin{pmatrix} -\\sqrt{3}\\tanh(0.3\\sqrt{3}) - \\tanh(0.3) \\\\ \\sqrt{3}\\tanh(0.95\\sqrt{3}) + \\tanh(0.95) \\end{pmatrix} \\approx \\begin{pmatrix} -0.279670 \\\\ 0.586812 \\end{pmatrix}$。\n    第二项系数：$\\frac{1}{8} \\sum (1-\\tanh^2(u_i)) = 1 - \\frac{1}{4}(\\tanh^2(0.3\\sqrt{3}) + \\tanh^2(0.3) + \\tanh^2(0.95\\sqrt{3}) + \\tanh^2(0.95)) \\approx 0.569615$。\n    第二项向量：$0.569615 \\times w_0 \\approx \\begin{pmatrix} -0.170885 \\\\ 0.541134 \\end{pmatrix}$。\n    $w^+ \\approx \\begin{pmatrix} -0.279670 \\\\ 0.586812 \\end{pmatrix} - \\begin{pmatrix} -0.170885 \\\\ 0.541134 \\end{pmatrix} = \\begin{pmatrix} -0.108785 \\\\ 0.045678 \\end{pmatrix}$。\n3.  归一化：\n    $\\|w^+\\|_2 \\approx \\sqrt{(-0.108785)^2 + 0.045678^2} \\approx 0.117986$。\n    $w = \\frac{w^+}{\\|w^+\\|_2} \\approx \\begin{pmatrix} -0.922020 \\\\ 0.387147 \\end{pmatrix}$。\n\n**案例 4: $(X = \\mathcal{D}_2, w_0 = [10.0, -10.0]^\\top)$**\n数据矩阵是 $X_2$，且 $w_0 = 10 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。$w_0$ 的分量很大。\n1.  投影：$u_i = w_0^\\top x_i$ 的值在绝对值上都很大（例如，$10\\sqrt{3} \\approx 17.32$，$10$）。\n2.  更新 $w^+$：\n    对于大的 $|u|$，$\\tanh(u) \\approx \\text{sign}(u)$ 并且其导数 $1-\\tanh^2(u) \\approx 0$。\n    第二项 $\\mathbb{E}[g'(w^\\top x)]w$ 将接近于零。\n    第一项 $\\mathbb{E}[x g(w^\\top x)]$ 占主导地位。$g(u_i) = \\tanh(w_0^\\top x_i) \\approx \\text{sign}(w_0^\\top x_i)$。\n    符号是 $\\text{sign}([10,-10]^\\top x_i)$。\n    $w^+ \\approx \\frac{1}{8} \\sum x_i \\text{sign}(w_0^\\top x_i) = \\frac{1}{8} \\begin{pmatrix} 2(\\sqrt{3}+1) \\\\ -2(\\sqrt{3}+1) \\end{pmatrix} = \\frac{\\sqrt{3}+1}{4} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n3.  归一化：\n    $w^+$ 是 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 的一个正倍数。归一化这个向量得到 $\\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\approx \\begin{pmatrix} 0.707107 \\\\ -0.707107 \\end{pmatrix}$。\n    这是算法的一个不动点方向。\n\n**案例 5: $(X = \\mathcal{D}_1, w_0 = [10^{-9}, -10^{-9}]^\\top)$**\n数据矩阵是 $X_1$，且 $w_0 = \\epsilon \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$，其中 $\\epsilon = 10^{-9}$。初始向量非常接近原点。\n1.  投影：$u_i=w_0^\\top x_i$ 的值都非常小（数量级为 $\\epsilon$）。\n2.  更新 $w^+$：\n    对于小的 $u$，$\\tanh(u) \\approx u - u^3/3$ 并且 $1-\\tanh^2(u) \\approx 1 - u^2$。\n    更新规则简化为 $w^+ \\approx (\\|w_0\\|^2 w_0 - \\frac{1}{3} \\mathbb{E}[x(w_0^\\top x)^3])$。\n    我们计算 $\\frac{1}{4} \\sum x_i(w_0^\\top x_i)^3 = 2\\epsilon^3 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n    $\\|w_0\\|^2 = 2\\epsilon^2$。\n    $w^+ \\approx 2\\epsilon^2 (\\epsilon \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}) - \\frac{1}{3} (2\\epsilon^3 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}) = (2\\epsilon^3 - \\frac{2}{3}\\epsilon^3) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{4}{3}\\epsilon^3 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n3.  归一化：\n    同样，$w^+$ 是 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 的一个正倍数。归一化后得到 $w = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\approx \\begin{pmatrix} 0.707107 \\\\ -0.707107 \\end{pmatrix}$。这表明即使从原点附近开始，算法也会被高阶统计量（与峰度相关）驱动，朝向一个非高斯方向。\n\n结果四舍五入到6位小数，如下所示：\n1.  $[0.708496, 0.705725]$\n2.  $[1.000000, 0.000000]$\n3.  $[-0.922020, 0.387147]$\n4.  $[0.707107, -0.707107]$\n5.  $[0.707107, -0.707107]$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ICA single-step update problem for a suite of test cases.\n    \"\"\"\n\n    # Define the datasets\n    sqrt2 = np.sqrt(2)\n    sqrt3 = np.sqrt(3)\n\n    D1 = np.array([\n        [sqrt2, -sqrt2, 0, 0],\n        [0, 0, sqrt2, -sqrt2]\n    ])\n\n    a, b, c, d = sqrt3, 1.0, sqrt3, 1.0\n    D2 = np.array([\n        [a, -a, b, -b, 0, 0, 0, 0],\n        [0, 0, 0, 0, c, -c, d, -d]\n    ])\n\n    # Define the test cases\n    test_cases = [\n        {'X': D1, 'w0': np.array([0.6, 0.8])},\n        {'X': D1, 'w0': np.array([1.0, 0.0])},\n        {'X': D2, 'w0': np.array([-0.3, 0.95])},\n        {'X': D2, 'w0': np.array([10.0, -10.0])},\n        # Edge case w0 is very small\n        {'X': D1, 'w0': np.array([1e-9, -1e-9])},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case['X']\n        w0 = case['w0']\n        p, n = X.shape  # p features, n samples\n        \n        # Ensure w0 is a column vector for matrix operations\n        w0 = w0.reshape(p, 1)\n\n        # 1. Compute projections u = w0^T * X\n        # w0.T is (1, p), X is (p, n) - u is (1, n)\n        u = w0.T @ X\n\n        # Apply nonlinearity g(u) = tanh(u)\n        g_u = np.tanh(u)\n\n        # Apply derivative of nonlinearity g'(u) = 1 - tanh^2(u)\n        g_prime_u = 1 - g_u**2\n\n        # 2. Compute the unnormalized updated vector w+\n        # This corresponds to the FastICA update rule:\n        # w+ = E[x * g(w^T*x)] - E[g'(w^T*x)] * w\n        # where expectations E are sample averages.\n\n        # First term: E[x * g(w^T*x)] = (1/n) * sum(xi * g(ui))\n        # In matrix form: (1/n) * X @ g_u.T\n        # X is (p, n), g_u.T is (n, 1) - term1 is (p, 1)\n        term1 = (X @ g_u.T) / n\n\n        # Second term: E[g'(w^T*x)] * w = (1/n) * sum(g'(ui)) * w0\n        # In matrix form: np.mean(g_prime_u) * w0\n        term2 = np.mean(g_prime_u) * w0\n\n        w_plus = term1 - term2\n\n        # 3. Normalize the new vector to unit norm\n        norm_w_plus = np.linalg.norm(w_plus)\n        \n        # Handle the case where w_plus is the zero vector, although unlikely here.\n        if norm_w_plus == 0:\n            w_new = np.zeros_like(w_plus)\n        else:\n            w_new = w_plus / norm_w_plus\n            \n        # Format the result: flatten to a 1D array, round to 6 decimal places, and convert to list\n        result_vector = np.round(w_new.flatten(), 6).tolist()\n        results.append(result_vector)\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation and join with commas.\n    str_results = [str(r) for r in results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了基于非高斯性最大化的方法，我们还可以从统计建模的视角来构建 ICA 算法。最大似然 (ML) 框架提供了一种原理清晰且功能强大的途径，它通过为源信号选择合适的先验概率分布来指导分离过程。本练习将要求你推导并实现基于最大似然原理的梯度上升更新步骤，这有助于你理解如何将一个概率模型转化为一个具体的优化算法。",
            "id": "3822243",
            "problem": "考虑用于多光谱遥感像素的瞬时线性混合模型，其中，在时间（或样本索引）$t$ 的每个观测向量 $x_t \\in \\mathbb{R}^n$ 被建模为 $x_t = A s_t$，对于某个未知的混合矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和潜在的独立源向量 $s_t \\in \\mathbb{R}^n$。独立成分分析（ICA）旨在恢复一个解混矩阵 $W \\in \\mathbb{R}^{n \\times n}$，使得 $y_t = W x_t$ 的分量在统计上是独立的。根据最大似然（ML）原则，并假设分量独立且选择了源先验 $p_i(y)$，观测数据 $\\{x_t\\}_{t=1}^T$ 关于 $W$ 的对数似然函数为\n$$\n\\mathcal{L}(W) = \\sum_{t=1}^T \\left( \\sum_{i=1}^n \\log p_i(y_{i t}) + \\log \\left| \\det W \\right| \\right),\n$$\n其中 $y_t = W x_t$ 且 $y_{i t}$ 表示 $y_t$ 的第 $i$ 个分量。任务是使用链式法则和变量替换原则，对 $\\mathcal{L}(W)$ 实现一步梯度上升，针对两种不同的先验选择：\n- 类 Logistic 先验：$\\log p_i(y) = - \\log \\cosh(y) + C_i$，\n- Laplace 先验：$\\log p_i(y) = - |y| + C_i$，\n其中 $C_i$ 是一个与 $y$ 无关的常数，在对 $W$ 进行优化时可以忽略。\n\n从给定的初始 $W$、数据集 $\\{x_t\\}_{t=1}^T$ 和步长 $\\eta  0$ 开始，根据第一性原理计算并执行单次更新 $W_{\\text{new}} = W + \\eta \\nabla \\mathcal{L}(W)$。\n\n实现此更新并为以下每个测试用例计算数值结果。对于所有情况，$y_t = W x_t$ 必须严格按照规定计算，并且对于 Laplace 先验，在 $y=0$ 的点，使用约定 $\\frac{d}{dy} \\log p(y) \\big|_{y=0} = 0$。\n\n测试套件：\n- 用例 1 (正常路径，$n=3$，使用类 Logistic 先验)：\n  - $n = 3$, $T = 5$,\n  - $W = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix}$,\n  - $\\{x_t\\}_{t=1}^5$ 其中\n    - $x_1 = (0.34, 0.12, 0.48)$,\n    - $x_2 = (0.28, 0.20, 0.52)$,\n    - $x_3 = (0.41, 0.15, 0.38)$,\n    - $x_4 = (0.30, 0.25, 0.45)$,\n    - $x_5 = (0.36, 0.18, 0.50)$,\n  - $\\eta = 0.1$,\n  - 先验：类 Logistic ($\\log p_i(y) = - \\log \\cosh(y) + C_i$)。\n\n- 用例 2 (边界条件，Laplace 先验含零值，$n=2$)：\n  - $n = 2$, $T = 4$,\n  - $W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $\\{x_t\\}_{t=1}^4$ 其中\n    - $x_1 = (0.50, 0.00)$,\n    - $x_2 = (0.45, 0.00)$,\n    - $x_3 = (0.60, 0.00)$,\n    - $x_4 = (0.55, 0.00)$,\n  - $\\eta = 0.2$,\n  - 先验：Laplace ($\\log p_i(y) = - |y| + C_i$)，使用在 $y=0$ 处的规定约定。\n\n- 用例 3 (接近奇异的 $W$ 以强调变量替换项，类 Logistic 先验，$n=2$)：\n  - $n = 2$, $T = 5$,\n  - $W = \\begin{bmatrix} 1.0  0.97 \\\\ 0.97  0.941 \\end{bmatrix}$,\n  - $\\{x_t\\}_{t=1}^5$ 其中\n    - $x_1 = (0.40, 0.35)$,\n    - $x_2 = (0.42, 0.33)$,\n    - $x_3 = (0.38, 0.37)$,\n    - $x_4 = (0.41, 0.34)$,\n    - $x_5 = (0.39, 0.36)$,\n  - $\\eta = 0.05$,\n  - 先验：类 Logistic ($\\log p_i(y) = - \\log \\cosh(y) + C_i$)。\n\n- 用例 4 (大步长敏感性，类 Logistic 先验，$n=3$)：\n  - $n = 3$, $T = 3$,\n  - $W = \\begin{bmatrix} 0.9  0.1  0.0 \\\\ 0.05  1.0  0.05 \\\\ 0.0  0.1  0.95 \\end{bmatrix}$,\n  - $\\{x_t\\}_{t=1}^3$ 其中\n    - $x_1 = (0.30, 0.40, 0.50)$,\n    - $x_2 = (0.35, 0.45, 0.55)$,\n    - $x_3 = (0.25, 0.38, 0.48)$,\n  - $\\eta = 1.0$,\n  - 先验：类 Logistic ($\\log p_i(y) = - \\log \\cosh(y) + C_i$)。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有四个用例的更新后矩阵。格式为逗号分隔的列表的列表，每个内部列表包含 $W_{\\text{new}}$ 按行主序展开并四舍五入到六位小数的条目。例如，两个用例的输出将类似于 $[[w_{11}, w_{12}, \\dots], [\\dots]]$，其中每个 $w_{ij}$ 是一个十进制数。不要打印任何额外文本。",
            "solution": "该问题要求在独立成分分析（ICA）中，为优化解混矩阵 $W$ 实现单步梯度上升。该优化基于最大似然（ML）原则。\n\n瞬时线性混合模型由 $x_t = A s_t$ 给出，其中 $x_t \\in \\mathbb{R}^n$ 是观测向量，$s_t \\in \\mathbb{R}^n$ 是具有统计独立分量的潜在源向量，$A \\in \\mathbb{R}^{n \\times n}$ 是一个未知的混合矩阵。ICA 的目标是找到一个解混矩阵 $W \\in \\mathbb{R}^{n \\times n}$，使得估计的源向量 $y_t = W x_t$ 的分量尽可能地统计独立。\n\n在给定模型和解混矩阵 $W$ 的情况下，观测数据 $\\{x_t\\}_{t=1}^T$ 的对数似然函数为：\n$$\n\\mathcal{L}(W) = \\sum_{t=1}^T \\left( \\sum_{i=1}^n \\log p_i(y_{i t}) + \\log \\left| \\det W \\right| \\right)\n$$\n其中 $y_t = W x_t$，$y_{it}$ 是 $y_t$ 的第 $i$ 个分量，$p_i$ 是第 $i$ 个源分量的先验概率分布。我们可以将对数似然重写为：\n$$\n\\mathcal{L}(W) = \\sum_{t=1}^T \\sum_{i=1}^n \\log p_i((W x_t)_i) + T \\log \\left| \\det W \\right|\n$$\n任务是对此对数似然函数执行一步梯度上升。更新规则由下式给出：\n$$\nW_{\\text{new}} = W + \\eta \\nabla \\mathcal{L}(W)\n$$\n其中 $\\eta  0$ 是步长，$\\nabla \\mathcal{L}(W)$ 是 $\\mathcal{L}(W)$ 相对于矩阵 $W$ 的梯度。\n\n为了找到梯度，我们计算 $\\mathcal{L}(W)$ 相对于每个元素 $W_{kl}$ 的偏导数。梯度是一个矩阵，其第 $(k,l)$ 个元素是 $\\frac{\\partial \\mathcal{L}(W)}{\\partial W_{kl}}$。\n对数似然函数由两项组成。我们分别求每一项的梯度。\n\n**1. 先验项的梯度：**\n设第一项为 $\\mathcal{L}_1(W) = \\sum_{t=1}^T \\sum_{i=1}^n \\log p_i(y_{it})$。\n使用链式法则，关于 $W_{kl}$ 的偏导数是：\n$$\n\\frac{\\partial \\mathcal{L}_1(W)}{\\partial W_{kl}} = \\sum_{t=1}^T \\sum_{i=1}^n \\frac{\\partial \\log p_i(y_{it})}{\\partial y_{it}} \\frac{\\partial y_{it}}{\\partial W_{kl}}\n$$\n我们定义得分函数 $\\phi_i(u) = \\frac{d}{du} \\log p_i(u)$。关系 $y_t = W x_t$ 意味着 $y_{it} = \\sum_{j=1}^n W_{ij} x_{jt}$。\n$y_{it}$ 关于 $W_{kl}$ 的偏导数是：\n$$\n\\frac{\\partial y_{it}}{\\partial W_{kl}} = \\frac{\\partial}{\\partial W_{kl}} \\left( \\sum_{j=1}^n W_{ij} x_{jt} \\right) = \\delta_{ik} x_{lt}\n$$\n其中 $\\delta_{ik}$ 是克罗内克 δ。该导数仅在 $i=k$ 时非零。\n将其代回，对 $i$ 的求和将坍缩为 $i=k$ 的单项：\n$$\n\\frac{\\partial \\mathcal{L}_1(W)}{\\partial W_{kl}} = \\sum_{t=1}^T \\phi_k(y_{kt}) x_{lt}\n$$\n该表达式是矩阵和 $\\sum_{t=1}^T \\phi(y_t) x_t^T$ 的第 $(k,l)$ 个元素，其中 $\\phi(y_t)$ 是一个列向量，其元素为 $\\phi_i(y_{it})$。\n因此，第一项的梯度矩阵形式为：\n$$\n\\nabla_W \\mathcal{L}_1(W) = \\sum_{t=1}^T \\phi(W x_t) x_t^T\n$$\n\n**2. 行列式项的梯度：**\n设第二项为 $\\mathcal{L}_2(W) = T \\log \\left| \\det W \\right|$。\n矩阵微积分的一个标准结果指出，对数行列式的梯度是：\n$$\n\\nabla_W (\\log \\left| \\det W \\right|) = (W^{-1})^T = W^{-T}\n$$\n因此，第二项的梯度是：\n$$\n\\nabla_W \\mathcal{L}_2(W) = T (W^{-1})^T\n$$\n这一项起到屏障作用，防止矩阵 $W$ 变为奇异矩阵（即 $\\det W = 0$）。\n\n**总梯度和更新规则：**\n结合这两个部分，对数似然函数的总梯度是：\n$$\n\\nabla \\mathcal{L}(W) = \\sum_{t=1}^T \\phi(W x_t) x_t^T + T (W^{-1})^T\n$$\n单步梯度上升的更新规则是：\n$$\nW_{\\text{new}} = W + \\eta \\left( \\sum_{t=1}^T \\phi(W x_t) x_t^T + T (W^{-1})^T \\right)\n$$\n\n**指定先验的得分函数：**\n我们需要为给定的两种先验确定得分函数 $\\phi(y)$。\n\n- **类 Logistic 先验**：$\\log p_i(y) = - \\log \\cosh(y) + C_i$。\n  $$\n  \\phi(y) = \\frac{d}{dy} (-\\log \\cosh(y)) = - \\frac{1}{\\cosh(y)} \\frac{d}{dy}(\\cosh(y)) = - \\frac{\\sinh(y)}{\\cosh(y)} = -\\tanh(y)\n  $$\n\n- **Laplace 先验**：$\\log p_i(y) = -|y| + C_i$。\n  $$\n  \\phi(y) = \\frac{d}{dy} (-|y|) = -\\text{sgn}(y)\n  $$\n  对于 $y=0$，导数是未定义的。问题中指定了约定 $\\frac{d}{dy} \\log p(y) \\big|_{y=0} = 0$。这意味着 $\\phi(0) = 0$，这与符号函数的标准定义（其中 $\\text{sgn}(0)=0$）一致。\n\n**实现算法：**\n对于每个具有参数 $W$、$\\{x_t\\}_{t=1}^T$、$\\eta$、$T$ 和所选先验的测试用例：\n1. 将数据 $\\{x_t\\}$ 表示为一个 $n \\times T$ 的矩阵 $X$，其中第 $t$ 列是向量 $x_t$。\n2. 计算估计的源：$Y = WX$。这是一个 $n \\times T$ 的矩阵，其中第 $t$ 列是 $y_t$。\n3. 计算与 $Y$ 大小相同的得分函数矩阵 $\\Phi(Y)$。\n    - 对于类 Logistic 先验，$\\Phi(Y) = -\\tanh(Y)$ (逐元素)。\n    - 对于 Laplace 先验，$\\Phi(Y) = -\\text{sgn}(Y)$ (逐元素)。\n4. 计算梯度的第一部分：$G_1 = \\Phi(Y) X^T$。\n5. 计算 $W$ 的逆矩阵 $W^{-1}$。\n6. 计算梯度的第二部分：$G_2 = T (W^{-1})^T$。\n7. 计算总梯度：$\\nabla_W \\mathcal{L} = G_1 + G_2$。\n8. 执行更新：$W_{\\text{new}} = W + \\eta \\nabla_W \\mathcal{L}$。\n9. 该用例的最终结果是按行主序展开的 $W_{\\text{new}}$ 矩阵，每个元素四舍五入到六位小数。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the four test cases for a single gradient ascent step in ICA.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: happy path, n=3 with logistic-like prior\n        {\n            \"n\": 3, \"T\": 5, \"eta\": 0.1, \"prior\": \"logistic\",\n            \"W\": np.array([[1.0, 0.0, 0.0],\n                           [0.0, 1.0, 0.0],\n                           [0.0, 0.0, 1.0]]),\n            \"X\": np.array([[0.34, 0.28, 0.41, 0.30, 0.36],\n                           [0.12, 0.20, 0.15, 0.25, 0.18],\n                           [0.48, 0.52, 0.38, 0.45, 0.50]])\n        },\n        # Case 2: boundary condition, Laplace prior with zeros, n=2\n        {\n            \"n\": 2, \"T\": 4, \"eta\": 0.2, \"prior\": \"laplace\",\n            \"W\": np.array([[1.0, 0.0],\n                           [0.0, 1.0]]),\n            \"X\": np.array([[0.50, 0.45, 0.60, 0.55],\n                           [0.00, 0.00, 0.00, 0.00]])\n        },\n        # Case 3: near-singular W, logistic-like prior, n=2\n        {\n            \"n\": 2, \"T\": 5, \"eta\": 0.05, \"prior\": \"logistic\",\n            \"W\": np.array([[1.0, 0.97],\n                           [0.97, 0.941]]),\n            \"X\": np.array([[0.40, 0.42, 0.38, 0.41, 0.39],\n                           [0.35, 0.33, 0.37, 0.34, 0.36]])\n        },\n        # Case 4: large step size sensitivity, logistic-like prior, n=3\n        {\n            \"n\": 3, \"T\": 3, \"eta\": 1.0, \"prior\": \"logistic\",\n            \"W\": np.array([[0.9, 0.1, 0.0],\n                           [0.05, 1.0, 0.05],\n                           [0.0, 0.1, 0.95]]),\n            \"X\": np.array([[0.30, 0.35, 0.25],\n                           [0.40, 0.45, 0.38],\n                           [0.50, 0.55, 0.48]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        W = case[\"W\"]\n        X = case[\"X\"]\n        T = case[\"T\"]\n        eta = case[\"eta\"]\n        prior = case[\"prior\"]\n\n        # 1. Compute estimated sources Y = WX\n        Y = W @ X\n\n        # 2. Compute the score function matrix Phi(Y)\n        if prior == \"logistic\":\n            phi_Y = -np.tanh(Y)\n        elif prior == \"laplace\":\n            phi_Y = -np.sign(Y)\n        else:\n            raise ValueError(\"Unknown prior\")\n\n        # 3. Compute the first part of the gradient: G1 = Phi(Y) * X^T\n        grad_part1 = phi_Y @ X.T\n\n        # 4. Compute the inverse of W\n        W_inv = np.linalg.inv(W)\n\n        # 5. Compute the second part of the gradient: G2 = T * (W^-1)^T\n        grad_part2 = T * W_inv.T\n        \n        # 6. Calculate the total gradient\n        grad_L = grad_part1 + grad_part2\n\n        # 7. Perform the update: W_new = W + eta * grad_L\n        W_new = W + eta * grad_L\n\n        # 8. Flatten, round, and store the result\n        flat_rounded_W_new = np.round(W_new.flatten(), 6).tolist()\n        results.append(flat_rounded_W_new)\n\n    # Convert each list of floats to its string representation\n    results_as_strings = [str(r).replace(\" \", \"\") for r in results]\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在成功应用 ICA 算法分离出信号后，一个关键问题随之而来：分离的效果究竟如何？为了客观地评价分离质量，特别是在有“地面真实”数据可供参考的模拟或受控实验中，我们需要定量的评估指标。本练习将介绍信号与干扰比 ($SIR$) 和信号与失真比 ($SDR$) 这两个在盲源分离领域广泛使用的核心性能指标，通过计算这些指标，你将学会如何量化评价 ICA 算法的性能。",
            "id": "3822206",
            "problem": "一个遥感仪器记录了两个光谱通道，这两个通道线性混合了两个独立的环境特征：气溶胶光学厚度异常和植被指数异常。假设一个瞬时线性混合模型，带有加性测量噪声，模型为 $x = A s + n$，其中 $x \\in \\mathbb{R}^{2 \\times T}$ 是在 $T$ 个空间样本上观测到的数据，$A \\in \\mathbb{R}^{2 \\times 2}$ 是混合矩阵，$s \\in \\mathbb{R}^{2 \\times T}$ 堆叠了两个独立的源信号，$n \\in \\mathbb{R}^{2 \\times T}$ 是加性噪声。在 $T = 4$ 个样本上的真实源信号为\n$$\ns_1 = [\\,2,\\,-2,\\,2,\\,-2\\,], \\quad s_2 = [\\,1,\\,1,\\,-1,\\,-1\\,],\n$$\n混合矩阵和噪声为\n$$\nA = \\begin{pmatrix} 1.0  0.4 \\\\ 0.2  1.2 \\end{pmatrix}, \\quad\nn = \\begin{pmatrix}\n0.1  -0.1  0.0  0.05 \\\\\n-0.05  0.0  0.1  -0.05\n\\end{pmatrix}.\n$$\n一个独立分量分析（ICA）算法估计出一个解混矩阵 $W \\in \\mathbb{R}^{2 \\times 2}$，得到恢复的分量 $y = W x$，其中\n$$\nW = \\begin{pmatrix} 1.02  -0.35 \\\\ -0.15  0.88 \\end{pmatrix}.\n$$\n使用基于能量的盲源分离评估方法，该方法基于对由 $\\{s_1, s_2\\}$ 张成的源子空间的正交投影，计算与 $s_1$ 对应的恢复分量（即第一个恢复的源 $y_1$）的信干比（SIR）和信号失真比（SDR），单位为分贝。将 SIR 和 SDR 均以分贝表示，并将您的答案四舍五入到三位有效数字。以行矩阵的形式提供最终结果对，顺序为 $[\\,\\text{SIR},\\,\\text{SDR}\\,]$。",
            "solution": "该问题要求在独立分量分析（ICA）的背景下，计算第一个恢复分量的信干比（SIR）和信号失真比（SDR）。\n\n首先，我们建立恢复信号 $y$、源信号 $s$ 和噪声 $n$ 之间的关系。观测信号由线性混合模型给出：\n$$x = As + n$$\n通过应用估计的解混矩阵 $W$ 获得恢复信号：\n$$y = Wx = W(As + n) = WAs + Wn$$\n我们定义全局传递矩阵 $G = WA$ 和滤波后的噪声 $n' = Wn$。方程变为：\n$$y = Gs + n'$$\n问题指明了矩阵 $A$ 和 $W$：\n$$A = \\begin{pmatrix} 1.0  0.4 \\\\ 0.2  1.2 \\end{pmatrix}, \\quad W = \\begin{pmatrix} 1.02  -0.35 \\\\ -0.15  0.88 \\end{pmatrix}$$\n我们计算全局矩阵 $G$：\n$$G = WA = \\begin{pmatrix} 1.02  -0.35 \\\\ -0.15  0.88 \\end{pmatrix} \\begin{pmatrix} 1.0  0.4 \\\\ 0.2  1.2 \\end{pmatrix}$$\n$$G = \\begin{pmatrix} (1.02)(1.0) + (-0.35)(0.2)  (1.02)(0.4) + (-0.35)(1.2) \\\\ (-0.15)(1.0) + (0.88)(0.2)  (-0.15)(0.4) + (0.88)(1.2) \\end{pmatrix}$$\n$$G = \\begin{pmatrix} 1.02 - 0.07  0.408 - 0.42 \\\\ -0.15 + 0.176  -0.06 + 1.056 \\end{pmatrix} = \\begin{pmatrix} 0.95  -0.012 \\\\ 0.026  0.996 \\end{pmatrix}$$\n恢复信号 $y_1$ 和 $y_2$（矩阵 $y$ 的行）可以用源信号 $s_1$ 和 $s_2$（矩阵 $s$ 的行）以及滤波后的噪声来表示。我们关心的是 $y_1$：\n$$y_1 = G_{11}s_1 + G_{12}s_2 + n'_1$$\n其中 $n'_1$ 是 $n' = Wn$ 的第一行。由于 $G_{11} \\approx 1$ 且 $G_{12} \\approx 0$，因此 $y_1$ 是对应于源 $s_1$ 的恢复分量。\n\n评估指标 SIR 和 SDR 基于将恢复信号 $y_1$ 分解为三个正交分量：目标信号分量、干扰分量和伪影（噪声）分量。这种分解是通过对源信号子空间的正交投影实现的。\n源信号给出如下：\n$$s_1 = [\\,2,\\,-2,\\,2,\\,-2\\,], \\quad s_2 = [\\,1,\\,1,\\,-1,\\,-1\\,]$$\n我们通过计算它们的点积来检查正交性：\n$$s_1 \\cdot s_2 = (2)(1) + (-2)(1) + (2)(-1) + (-2)(-1) = 2 - 2 - 2 + 2 = 0$$\n由于源信号是正交的，它们构成了源子空间的一个正交基。$y_1$ 的分解如下：\n- 目标分量：$s_{\\text{target}} = \\text{proj}_{s_1}(y_1) = \\frac{y_1 \\cdot s_1}{\\|s_1\\|^2} s_1$\n- 干扰分量：$e_{\\text{interf}} = \\text{proj}_{s_2}(y_1) = \\frac{y_1 \\cdot s_2}{\\|s_2\\|^2} s_2$\n- 伪影分量：$e_{\\text{artif}} = y_1 - s_{\\text{target}} - e_{\\text{interf}}$\n\n以分贝（dB）为单位的 SIR 和 SDR 由这些分量的能量（L2 范数的平方）定义：\n$$\\text{SIR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|e_{\\text{interf}}\\|^2} \\right)$$\n$$\\text{SDR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|e_{\\text{interf}} + e_{\\text{artif}}\\|^2} \\right)$$\n总失真项为 $e_{\\text{interf}} + e_{\\text{artif}} = y_1 - s_{\\text{target}}$。\n\n为了计算这些值，我们首先需要计算恢复的信号向量 $y_1$。\n首先，我们求出观测信号矩阵 $x = As+n$。\n$$As = \\begin{pmatrix} 1.0  0.4 \\\\ 0.2  1.2 \\end{pmatrix} \\begin{pmatrix} 2  -2  2  -2 \\\\ 1  1  -1  -1 \\end{pmatrix} = \\begin{pmatrix} 2.4  -1.6  1.6  -2.4 \\\\ 1.6  0.8  -0.8  -1.6 \\end{pmatrix}$$\n$$n = \\begin{pmatrix} 0.1  -0.1  0.0  0.05 \\\\ -0.05  0.0  0.1  -0.05 \\end{pmatrix}$$\n$$x = As + n = \\begin{pmatrix} 2.5  -1.7  1.6  -2.35 \\\\ 1.55  0.8  -0.7  -1.65 \\end{pmatrix}$$\n设 $x_1$ 和 $x_2$ 为 $x$ 的行向量。现在我们计算 $y_1$，即 $y=Wx$ 的第一行。\n$$y_1 = W_{11}x_1 + W_{12}x_2 = 1.02 x_1 - 0.35 x_2$$\n$$y_1 = 1.02 [\\,2.5, -1.7, 1.6, -2.35\\,] - 0.35 [\\,1.55, 0.8, -0.7, -1.65\\,]$$\n$$y_1 = [\\,2.55, -1.734, 1.632, -2.397\\,] - [\\,0.5425, 0.28, -0.245, -0.5775\\,]$$\n$$y_1 = [\\,2.0075, -2.014, 1.877, -1.8195\\,]$$\n接下来，我们计算所需的点积和范数。\n$$\\|s_1\\|^2 = 2^2 + (-2)^2 + 2^2 + (-2)^2 = 16$$\n$$\\|s_2\\|^2 = 1^2 + 1^2 + (-1)^2 + (-1)^2 = 4$$\n$$y_1 \\cdot s_1 = (2.0075)(2) + (-2.014)(-2) + (1.877)(2) + (-1.8195)(-2) = 4.015 + 4.028 + 3.754 + 3.639 = 15.436$$\n$$y_1 \\cdot s_2 = (2.0075)(1) + (-2.014)(1) + (1.877)(-1) + (-1.8195)(-1) = 2.0075 - 2.014 - 1.877 + 1.8195 = -0.064$$\n现在我们计算目标分量和干扰分量的能量。\n$$\\|s_{\\text{target}}\\|^2 = \\left\\| \\frac{y_1 \\cdot s_1}{\\|s_1\\|^2} s_1 \\right\\|^2 = \\left(\\frac{15.436}{16}\\right)^2 \\|s_1\\|^2 = \\frac{15.436^2}{16^2} \\times 16 = \\frac{15.436^2}{16} = \\frac{238.270096}{16} = 14.891881$$\n$$\\|e_{\\text{interf}}\\|^2 = \\left\\| \\frac{y_1 \\cdot s_2}{\\|s_2\\|^2} s_2 \\right\\|^2 = \\left(\\frac{-0.064}{4}\\right)^2 \\|s_2\\|^2 = \\frac{(-0.064)^2}{4^2} \\times 4 = \\frac{0.004096}{4} = 0.001024$$\n我们现在可以计算 SIR。\n$$\\text{SIR} = 10 \\log_{10} \\left( \\frac{14.891881}{0.001024} \\right) = 10 \\log_{10}(14542.8525...)= 41.6265... \\text{ dB}$$\n四舍五入到三位有效数字，$\\text{SIR} = 41.6 \\text{ dB}$。\n\n对于 SDR，我们需要总失真的能量 $\\|y_1 - s_{\\text{target}}\\|^2$。根据正交投影的性质，$s_{\\text{target}}$ 和 $y_1 - s_{\\text{target}}$ 是正交的。因此，根据勾股定理：$\\|y_1\\|^2 = \\|s_{\\text{target}}\\|^2 + \\|y_1 - s_{\\text{target}}\\|^2$。\n失真能量为 $\\|y_1 - s_{\\text{target}}\\|^2 = \\|y_1\\|^2 - \\|s_{\\text{target}}\\|^2$。\n首先，计算 $\\|y_1\\|^2$：\n$$\\|y_1\\|^2 = 2.0075^2 + (-2.014)^2 + 1.877^2 + (-1.8195)^2$$\n$$\\|y_1\\|^2 = 4.03005625 + 4.056196 + 3.523129 + 3.31058025 = 14.9199615$$\n总失真能量为：\n$$\\|y_1 - s_{\\text{target}}\\|^2 = 14.9199615 - 14.891881 = 0.0280805$$\n现在我们计算 SDR。\n$$\\text{SDR} = 10 \\log_{10} \\left( \\frac{\\|s_{\\text{target}}\\|^2}{\\|y_1 - s_{\\text{target}}\\|^2} \\right) = 10 \\log_{10} \\left( \\frac{14.891881}{0.0280805} \\right) = 10 \\log_{10}(530.324...)= 27.2454... \\text{ dB}$$\n四舍五入到三位有效数字，$\\text{SDR} = 27.2 \\text{ dB}$。\n\n最终答案是 $[\\,\\text{SIR},\\,\\text{SDR}\\,]$ 对，其值四舍五入到三位有效数字。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n41.6  27.2\n\\end{pmatrix}\n}\n$$"
        }
    ]
}