{
    "hands_on_practices": [
        {
            "introduction": "Support Vector Machines (SVMs) are powerful classifiers defined by the principle of maximal margin separation. This exercise provides a hands-on opportunity to engage with the core geometric intuition behind SVMs. By working through the optimization of the decision boundary for a simple, linearly separable dataset from first principles , you will gain a concrete understanding of how the algorithm identifies the most robust separating hyperplane and the crucial role played by the support vectors.",
            "id": "3801071",
            "problem": "A multispectral satellite scene is being analyzed to separate two land cover categories in a two-feature space, where the features are normalized band ratios denoted by $x_{1}$ and $x_{2}$. Four labeled prototype observations, obtained from spectrally homogeneous calibration patches, are given in $\\mathbb{R}^{2}$: the positive class $\\mathcal{C}_{+}$ has points at $(0,1)$ and $(1,0)$, and the negative class $\\mathcal{C}_{-}$ has points at $(0,-1)$ and $(-1,0)$. A hard-margin Support Vector Machine (SVM) is trained with a linear decision function $f(x) = w^{\\top}x + b$, where $w \\in \\mathbb{R}^{2}$ and $b \\in \\mathbb{R}$, under the standard separability constraints for training samples $(x_{i}, y_{i})$ with labels $y_{i} \\in \\{+1,-1\\}$.\n\nStarting from the definition of the geometric margin for a labeled point $(x_{i}, y_{i})$ under $(w,b)$, defined as $\\gamma_{i} = \\frac{y_{i}(w^{\\top}x_{i} + b)}{\\|w\\|}$, and using the well-tested equivalence that maximizing the minimal geometric margin over $i$ is achieved by minimizing $\\|w\\|$ subject to $y_{i}(w^{\\top}x_{i} + b) \\ge 1$ for all $i$, determine the maximum geometric margin attained by the optimal hard-margin SVM for this dataset and identify which training points are the support vectors.\n\nExpress the maximum geometric margin as an exact value. No rounding is required. The support vectors should be justified in your reasoning, but the final answer must be the margin only.",
            "solution": "The user has provided a valid problem statement.\n\nThe problem asks for the maximum geometric margin of a hard-margin Support Vector Machine (SVM) for a given set of four labeled data points in $\\mathbb{R}^{2}$. The solution requires finding the optimal weight vector $w$ and bias $b$ that define the separating hyperplane.\n\nThe four labeled training points are:\nFor the positive class $\\mathcal{C}_{+}$ (label $y=+1$):\n$x_{A} = (0, 1)^{\\top}$\n$x_{B} = (1, 0)^{\\top}$\n\nFor the negative class $\\mathcal{C}_{-}$ (label $y=-1$):\n$x_{C} = (0, -1)^{\\top}$\n$x_{D} = (-1, 0)^{\\top}$\n\nThe SVM learning problem is to find the parameters $w = (w_1, w_2)^{\\top}$ and $b \\in \\mathbb{R}$ that minimize $\\frac{1}{2}\\|w\\|^2$ subject to the constraints $y_{i}(w^{\\top}x_{i} + b) \\ge 1$ for all $i \\in \\{A, B, C, D\\}$.\n\nLet's write out the four constraints explicitly:\n1. For point $x_A=(0,1)^{\\top}$ with $y_A=+1$:\n$y_A(w^{\\top}x_A + b) = (+1)(w_1 \\cdot 0 + w_2 \\cdot 1 + b) = w_2 + b \\ge 1$.\n\n2. For point $x_B=(1,0)^{\\top}$ with $y_B=+1$:\n$y_B(w^{\\top}x_B + b) = (+1)(w_1 \\cdot 1 + w_2 \\cdot 0 + b) = w_1 + b \\ge 1$.\n\n3. For point $x_C=(0,-1)^{\\top}$ with $y_C=-1$:\n$y_C(w^{\\top}x_C + b) = (-1)(w_1 \\cdot 0 + w_2 \\cdot (-1) + b) = -(-w_2 + b) = w_2 - b \\ge 1$.\n\n4. For point $x_D=(-1,0)^{\\top}$ with $y_D=-1$:\n$y_D(w^{\\top}x_D + b) = (-1)(w_1 \\cdot (-1) + w_2 \\cdot 0 + b) = -(-w_1 + b) = w_1 - b \\ge 1$.\n\nWe thus have a system of four linear inequalities for the parameters $w_1, w_2, b$:\n$$\n\\begin{cases}\nw_2 + b \\ge 1 \\\\\nw_1 + b \\ge 1 \\\\\nw_2 - b \\ge 1 \\\\\nw_1 - b \\ge 1\n\\end{cases}\n$$\nFrom the second and fourth inequalities, we have $w_1 \\ge 1 - b$ and $w_1 \\ge 1 + b$. This implies $w_1$ must be greater than or equal to the maximum of these two quantities: $w_1 \\ge \\max(1-b, 1+b)$.\nSimilarly, from the first and third inequalities, we have $w_2 \\ge 1 - b$ and $w_2 \\ge 1 + b$, which implies $w_2 \\ge \\max(1-b, 1+b)$.\n\nThe objective is to minimize $\\frac{1}{2}\\|w\\|^2 = \\frac{1}{2}(w_1^2 + w_2^2)$. This is equivalent to minimizing $\\|w\\|^2 = w_1^2 + w_2^2$. Since $w_1^2$ and $w_2^2$ are non-negative and increasing for positive arguments, we must choose the smallest possible values for $|w_1|$ and $|w_2|$ that satisfy the constraints. From the problem geometry, it is clear that $w_1$ and $w_2$ will be positive.\nThus, to minimize $w_1^2 + w_2^2$, we must set $w_1$ and $w_2$ to their minimum possible values.\n$$\nw_1 = \\max(1-b, 1+b)\n$$\n$$\nw_2 = \\max(1-b, 1+b)\n$$\nNote that $\\max(1-b, 1+b)$ can be expressed using the absolute value function as $1+|b|$. Thus, $w_1 = w_2 = 1+|b|$.\n\nThe objective function can now be expressed solely in terms of $b$:\n$$\n\\|w\\|^2 = w_1^2 + w_2^2 = (1+|b|)^2 + (1+|b|)^2 = 2(1+|b|)^2\n$$\nWe need to find the value of $b$ that minimizes this expression. The function $(1+|b|)^2$ is minimized when $|b|$ is minimized. The minimum value of $|b|$ is $0$, which occurs when $b=0$.\n\nTherefore, the optimal bias is $b_{opt} = 0$.\nSubstituting this back into the expressions for $w_1$ and $w_2$:\n$$\nw_1 = 1+|0| = 1\n$$\n$$\nw_2 = 1+|0| = 1\n$$\nThe optimal weight vector is $w_{opt} = (1, 1)^{\\top}$.\n\nThe norm of this optimal weight vector is $\\|w_{opt}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$.\n\nThe maximum geometric margin, $\\gamma_{max}$, is given by the formula $\\gamma_{max} = \\frac{1}{\\|w_{opt}\\|}$.\nSubstituting the value of $\\|w_{opt}\\|$, we get:\n$$\n\\gamma_{max} = \\frac{1}{\\sqrt{2}}\n$$\n\nTo identify the support vectors, we check which points satisfy the equality condition $y_i(w_{opt}^{\\top}x_i + b_{opt}) = 1$. With $w_{opt}=(1,1)^{\\top}$ and $b_{opt}=0$:\n- Point $x_A=(0,1)^{\\top}, y_A=+1$: $(+1)(1 \\cdot 0 + 1 \\cdot 1 + 0) = 1$. This is a support vector.\n- Point $x_B=(1,0)^{\\top}, y_B=+1$: $(+1)(1 \\cdot 1 + 1 \\cdot 0 + 0) = 1$. This is a support vector.\n- Point $x_C=(0,-1)^{\\top}, y_C=-1$: $(-1)(1 \\cdot 0 + 1 \\cdot (-1) + 0) = (-1)(-1) = 1$. This is a support vector.\n- Point $x_D=(-1,0)^{\\top}, y_D=-1$: $(-1)(1 \\cdot (-1) + 1 \\cdot 0 + 0) = (-1)(-1) = 1$. This is a support vector.\nAll four training points are support vectors. They lie on the margin-defining hyperplanes $w^{\\top}x+b = \\pm 1$, which are $x_1+x_2 = 1$ and $x_1+x_2 = -1$. The separating hyperplane is $x_1+x_2=0$.\n\nThe maximum geometric margin is the distance from any of these support vectors to the separating hyperplane. For example, for $x_A=(0,1)^{\\top}$ and the hyperplane $1 \\cdot x_1 + 1 \\cdot x_2 + 0 = 0$, the distance is $\\frac{|1 \\cdot 0 + 1 \\cdot 1 + 0|}{\\sqrt{1^2+1^2}} = \\frac{1}{\\sqrt{2}}$. This confirms our result.",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2}}}$$"
        },
        {
            "introduction": "The performance of a Random Forest classifier is critically dependent on navigating the bias-variance trade-off through careful hyperparameter tuning. This exercise simulates the practical task of model selection, a cornerstone of applied machine learning. By finding the optimal values for key hyperparameters that minimize a given error function , you will develop an intuition for controlling model complexity to achieve strong generalization performance, particularly in the context of imbalanced remote sensing datasets.",
            "id": "3801066",
            "problem": "A land cover classification task uses a Random Forest (RF) classifier trained on fused multispectral and textural features extracted from satellite imagery. The feature vector has $p$ bands, with $p = 36$. The dataset is multi-class and imbalanced: agriculture has class prior $0.68$, urban has class prior $0.22$, and wetlands has class prior $0.10$. To prevent majority-class dominance, model selection is performed using the Balanced Error Rate (BER), defined for multi-class classification as the arithmetic mean of per-class misclassification rates. The goal is to choose three RF hyperparameters that jointly balance bias and variance: the minimum node size $n_{\\min}$, the maximum tree depth $d_{\\max}$, and the number of features considered at each split $m_{\\mathrm{try}}$.\n\nFrom first principles, use the following foundational bases:\n- Empirical risk minimization and $k$-fold cross-validation: the expected generalization error is estimated by averaging validation errors across folds, here with $k = 10$.\n- Bias-variance decomposition: increasing model complexity typically reduces bias and increases variance, whereas decreasing model complexity does the opposite.\n- Balanced Error Rate (BER) and class weighting: when classes are imbalanced, minimizing BER better aligns with minimizing expected misclassification across classes rather than across samples.\n\nA $10$-fold cross-validation was conducted for each hyperparameter independently while holding the others at a neutral baseline. The resulting validation curves were smoothed and fitted to functions that encapsulate the bias-variance trade-off observed in practice for RFs on remote sensing data. These fitted curves are:\n- For minimum node size $n_{\\min}$ (integer domain $1 \\leq n_{\\min} \\leq 20$):\n$$\nE_{n}(n_{\\min}) \\;=\\; 0.15 \\;+\\; \\frac{0.064}{n_{\\min}} \\;+\\; 0.001\\,n_{\\min}.\n$$\n- For maximum depth $d_{\\max}$ (integer domain $2 \\leq d_{\\max} \\leq 40$):\n$$\nE_{d}(d_{\\max}) \\;=\\; 0.14 \\;+\\; \\frac{0.144}{d_{\\max}} \\;+\\; 0.001\\,d_{\\max}.\n$$\n- For number of features at each split $m_{\\mathrm{try}}$ (integer domain $1 \\leq m_{\\mathrm{try}} \\leq p$):\n$$\nE_{m}(m_{\\mathrm{try}}) \\;=\\; 0.13 \\;+\\; 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} \\;+\\; \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right),\n$$\nwith $p = 36$.\n\nAssume the hyperparameter-wise cross-validation fits are separable approximations of the contributions to validation BER, so that a balanced configuration can be selected by minimizing each fitted curve over its domain. Compute the values $n_{\\min}^{\\star}$, $d_{\\max}^{\\star}$, and $m_{\\mathrm{try}}^{\\star}$ that minimize their respective curves under the given integer constraints. Provide the final answer as a single row matrix $\\left(n_{\\min}^{\\star} \\;\\; d_{\\max}^{\\star} \\;\\; m_{\\mathrm{try}}^{\\star}\\right)$ inside a `pmatrix` environment. No rounding is required.",
            "solution": "The problem statement is first validated according to the specified criteria.\n\n### Step 1: Extract Givens\n- **Model and Task**: A Random Forest (RF) classifier for a multi-class land cover classification task.\n- **Feature Vector Dimension**: The number of features is $p = 36$.\n- **Class Priors**: Agriculture ($0.68$), Urban ($0.22$), Wetlands ($0.10$).\n- **Performance Metric**: Balanced Error Rate (BER).\n- **Hyperparameters for Optimization**: Minimum node size ($n_{\\min}$), maximum tree depth ($d_{\\max}$), and number of features per split ($m_{\\mathrm{try}}$).\n- **Optimization Methodology**: Based on $k$-fold cross-validation with $k = 10$. The optimization is simplified by assuming separability of hyperparameter contributions to the error.\n- **Fitted Error Functions**:\n  - For minimum node size $n_{\\min}$ on the integer domain $1 \\leq n_{\\min} \\leq 20$:\n    $$E_{n}(n_{\\min}) = 0.15 + \\frac{0.064}{n_{\\min}} + 0.001 n_{\\min}$$\n  - For maximum depth $d_{\\max}$ on the integer domain $2 \\leq d_{\\max} \\leq 40$:\n    $$E_{d}(d_{\\max}) = 0.14 + \\frac{0.144}{d_{\\max}} + 0.001 d_{\\max}$$\n  - For features per split $m_{\\mathrm{try}}$ on the integer domain $1 \\leq m_{\\mathrm{try}} \\leq p$, with $p=36$:\n    $$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right)$$\n- **Objective**: Compute the optimal integer values $n_{\\min}^{\\star}$, $d_{\\max}^{\\star}$, and $m_{\\mathrm{try}}^{\\star}$ that minimize their respective error functions over the given domains.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to a critical review.\n- **Scientific Grounding**: The problem is well-grounded in the principles of statistical machine learning. It addresses the practical task of hyperparameter tuning for a Random Forest classifier. The concepts of cross-validation, the bias-variance trade-off, and the use of a Balanced Error Rate for imbalanced datasets are all standard and sound. The functional forms for the validation error curves are plausible phenomenological models of the trade-off, where error is high for both overly simple and overly complex models.\n- **Well-Posedness**: The problem is well-posed. It requires the minimization of three distinct, well-defined functions over specified integer domains. The assumption of separability, while a simplification of the true joint optimization problem, makes the problem tractable and is a common heuristic. A unique integer-valued solution is expected for each minimization.\n- **Objectivity**: The problem is stated using precise, objective mathematical language.\n- **Conclusion**: The problem does not violate any of the specified invalidity criteria. It is scientifically sound, self-contained, well-posed, and objective.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived.\n\nThe task is to find the integer values of the hyperparameters $n_{\\min}$, $d_{\\max}$, and $m_{\\mathrm{try}}$ that minimize their respective validation error functions, $E_{n}$, $E_{d}$, and $E_{m}$. This can be achieved by finding the minimum of each function independently. For a function of the form $f(x) = c_0 + c_1/x + c_2 x$ with $c_1, c_2 > 0$, the function is convex for $x > 0$. The minimum for a continuous variable $x$ is found by setting the derivative to zero. If the solution is not an integer, the integer minimum must be at one of the two integers bracketing the continuous minimum.\n\n**1. Optimization of Minimum Node Size ($n_{\\min}$)**\n\nThe error function for $n_{\\min}$ is given by:\n$$E_{n}(n_{\\min}) = 0.15 + \\frac{0.064}{n_{\\min}} + 0.001 n_{\\min}$$\nThe domain for $n_{\\min}$ is the set of integers in $[1, 20]$.\nTo find the minimum, we first treat $n_{\\min}$ as a continuous variable and compute the first derivative with respect to $n_{\\min}$:\n$$\\frac{dE_n}{dn_{\\min}} = - \\frac{0.064}{n_{\\min}^2} + 0.001$$\nSetting the derivative to zero to find the critical point:\n$$0.001 = \\frac{0.064}{n_{\\min}^2} \\implies n_{\\min}^2 = \\frac{0.064}{0.001} = 64$$\nSolving for $n_{\\min}$ (and taking the positive root as node size must be positive):\n$$n_{\\min} = \\sqrt{64} = 8$$\nTo confirm this is a minimum, we check the second derivative:\n$$\\frac{d^2E_n}{dn_{\\min}^2} = \\frac{2 \\times 0.064}{n_{\\min}^3} = \\frac{0.128}{n_{\\min}^3}$$\nFor any $n_{\\min} > 0$, the second derivative is positive, confirming that $n_{\\min} = 8$ is a local minimum. Since this is the only critical point in the domain, it is the global minimum for the continuous function.\nThe continuous minimum occurs at an integer value, $8$, which lies within the specified domain $[1, 20]$. Therefore, the optimal integer value is $n_{\\min}^{\\star} = 8$.\n\n**2. Optimization of Maximum Tree Depth ($d_{\\max}$)**\n\nThe error function for $d_{\\max}$ is:\n$$E_{d}(d_{\\max}) = 0.14 + \\frac{0.144}{d_{\\max}} + 0.001 d_{\\max}$$\nThe domain for $d_{\\max}$ is the set of integers in $[2, 40]$.\nWe compute the first derivative with respect to $d_{\\max}$:\n$$\\frac{dE_d}{dd_{\\max}} = - \\frac{0.144}{d_{\\max}^2} + 0.001$$\nSetting the derivative to zero:\n$$0.001 = \\frac{0.144}{d_{\\max}^2} \\implies d_{\\max}^2 = \\frac{0.144}{0.001} = 144$$\nSolving for $d_{\\max}$:\n$$d_{\\max} = \\sqrt{144} = 12$$\nThe second derivative is $\\frac{d^2E_d}{dd_{\\max}^2} = \\frac{2 \\times 0.144}{d_{\\max}^3} = \\frac{0.288}{d_{\\max}^3}$, which is positive for $d_{\\max} > 0$. Thus, $d_{\\max} = 12$ is a minimum.\nThe continuous minimum occurs at an integer value, $12$, which is in the domain $[2, 40]$. The optimal integer value is $d_{\\max}^{\\star} = 12$.\n\n**3. Optimization of Features per Split ($m_{\\mathrm{try}}$)**\n\nThe error function for $m_{\\rm try}$ is given by:\n$$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right)$$\nWe are given $p = 36$, so $\\sqrt{p} = 6$. The function becomes:\n$$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{6}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{6}\\right)$$\nThe domain for $m_{\\mathrm{try}}$ is the set of integers in $[1, 36]$.\nWe compute the first derivative with respect to $m_{\\mathrm{try}}$:\n$$\\frac{dE_m}{dm_{\\mathrm{try}}} = 0.005\\left(-\\frac{6}{m_{\\mathrm{try}}^2} + \\frac{1}{6}\\right)$$\nSetting the derivative to zero:\n$$\\frac{1}{6} = \\frac{6}{m_{\\mathrm{try}}^2} \\implies m_{\\mathrm{try}}^2 = 36$$\nSolving for $m_{\\mathrm{try}}$:\n$$m_{\\mathrm{try}} = \\sqrt{36} = 6$$\nThe second derivative is $\\frac{d^2E_m}{dm_{\\mathrm{try}}^2} = 0.005\\left(\\frac{12}{m_{\\mathrm{try}}^3}\\right) = \\frac{0.06}{m_{\\mathrm{try}}^3}$, which is positive for $m_{\\mathrm{try}} > 0$. Thus, $m_{\\mathrm{try}} = 6$ is a minimum.\nThe continuous minimum occurs at an integer value, $6$, which is within the domain $[1, 36]$. The optimal integer value is $m_{\\mathrm{try}}^{\\star} = 6$. This result aligns with the common heuristic to set $m_{\\mathrm{try}} \\approx \\sqrt{p}$.\n\nThe optimal hyperparameter configuration is $(n_{\\min}^{\\star}, d_{\\max}^{\\star}, m_{\\mathrm{try}}^{\\star})$. Based on the independent minimizations, the optimal values are $(8, 12, 6)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 8 & 12 & 6 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Dropout is a fundamental regularization technique that is essential for training modern Artificial Neural Networks (ANNs) and preventing overfitting. This problem moves beyond simply applying dropout and challenges you to understand its underlying statistical mechanism. By deriving the expected behavior of a network layer under dropout and the corresponding adjustment needed at inference time , you will demystify how this technique works and appreciate the probabilistic reasoning that ensures model consistency from training to deployment.",
            "id": "3801078",
            "problem": "A team is training an Artificial Neural Network (ANN) for land-cover classification from multispectral satellite images in a remote sensing and environmental modeling application. Consider a single hidden layer with $n$ units producing a deterministic activation vector $h \\in \\mathbb{R}^{n}$ for a given pixel. During training, the team applies dropout with rate $p \\in (0,1)$ to this hidden layer, where each unit is independently retained with probability $1-p$ and dropped with probability $p$. The next layer computes a scalar pre-activation by a linear map with weights $w \\in \\mathbb{R}^{n}$, so the scalar pre-activation with dropout is $y = w^{\\top}(m \\odot h)$, where $m \\in \\{0,1\\}^{n}$ is the random dropout mask and $\\odot$ denotes the Hadamard (elementwise) product.\n\nStarting only from the following base facts: the Bernoulli model for independent retention indicators, the linearity of expectation, and the definition of the Hadamard product, derive the following two quantities:\n- The expected number of retained units in the hidden layer under dropout rate $p$.\n- A single scalar weight scaling factor $s$ applied uniformly to all components of $w$ at inference (no dropout) that preserves the training-time mean of the scalar pre-activation $y$ for any deterministic $h$ and any $w$.\n\nAssume the dropout mask components $m_{i}$ are independent and identically distributed Bernoulli random variables with parameter $1-p$, and that $m$ is independent of $h$. Express your final result as a closed-form analytical expression. No numerical approximation is required. The final answer must be a single row matrix containing the expected count and the scaling factor, in that order.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of probability theory and machine learning, well-posed with a clear objective and sufficient information, and uses precise, objective language. The problem is a standard, formalizable derivation related to the dropout technique in neural networks. Therefore, a solution will be provided.\n\nThe problem asks for two quantities derived from first principles: the expected number of retained units in a hidden layer during dropout, and a weight scaling factor for inference that preserves the mean output.\n\nLet the hidden layer have $n$ units. The dropout mask is a random vector $m \\in \\{0, 1\\}^n$, where each component $m_i$ for $i \\in \\{1, 2, \\dots, n\\}$ is an independent and identically distributed (i.i.d.) Bernoulli random variable. A unit $i$ is retained if $m_i = 1$ and dropped if $m_i = 0$. The problem states that the probability of retention for each unit is $1-p$, where $p \\in (0,1)$ is the dropout rate. Therefore, $m_i \\sim \\text{Bernoulli}(1-p)$.\n\nThe probability mass function for $m_i$ is:\n$P(m_i = 1) = 1-p$\n$P(m_i = 0) = p$\n\nThe expectation of a Bernoulli random variable with parameter $\\theta$ is $\\theta$. Thus, the expectation of each $m_i$ is:\n$$E[m_i] = 1 \\cdot P(m_i=1) + 0 \\cdot P(m_i=0) = 1 \\cdot (1-p) + 0 \\cdot p = 1-p$$\n\n**Part 1: Expected Number of Retained Units**\n\nLet $N_{retained}$ be the random variable representing the total number of retained units in the hidden layer. A unit is retained if its corresponding mask element is $1$. The total number of retained units is the sum of the indicators in the mask vector $m$:\n$$N_{retained} = \\sum_{i=1}^{n} m_i$$\n\nTo find the expected number of retained units, we take the expectation of $N_{retained}$. Based on the given principle of linearity of expectation, we can write:\n$$E[N_{retained}] = E\\left[\\sum_{i=1}^{n} m_i\\right] = \\sum_{i=1}^{n} E[m_i]$$\n\nAs established, $E[m_i] = 1-p$ for all $i \\in \\{1, 2, \\dots, n\\}$. Substituting this into the sum:\n$$E[N_{retained}] = \\sum_{i=1}^{n} (1-p)$$\n\nSince $(1-p)$ is a constant with respect to the summation index $i$, we are summing this constant $n$ times:\n$$E[N_{retained}] = n(1-p)$$\nThis is the expected number of retained units in the hidden layer.\n\n**Part 2: Inference Weight Scaling Factor**\n\nAt training time, the scalar pre-activation $y$ is computed using the dropout mask $m$:\n$$y = w^{\\top}(m \\odot h)$$\nwhere $w \\in \\mathbb{R}^{n}$ is the weight vector, $h \\in \\mathbb{R}^{n}$ is the deterministic activation vector, and $\\odot$ is the Hadamard (elementwise) product. Expanded, this is:\n$$y = \\sum_{i=1}^{n} w_i (m_i h_i)$$\n\nWe are asked to find a scaling factor $s$ such that the inference-time output (with no dropout) equals the *mean* (expected value) of the training-time output. First, let's compute the expected value of $y$ at training time, denoted $E[y]$.\n$$E[y] = E\\left[\\sum_{i=1}^{n} w_i m_i h_i\\right]$$\n\nUsing the linearity of expectation:\n$$E[y] = \\sum_{i=1}^{n} E[w_i m_i h_i]$$\n\nThe weight vector $w$ and the activation vector $h$ are given as deterministic for a specific input. The dropout mask $m$ is independent of $h$. Therefore, $w_i$ and $h_i$ can be treated as constants with respect to the expectation operator, which averages over the distribution of $m$.\n$$E[y] = \\sum_{i=1}^{n} w_i h_i E[m_i]$$\n\nSubstituting $E[m_i] = 1-p$:\n$$E[y] = \\sum_{i=1}^{n} w_i h_i (1-p) = (1-p) \\sum_{i=1}^{n} w_i h_i$$\n\nThe summation term $\\sum_{i=1}^{n} w_i h_i$ is the definition of the dot product $w^{\\top}h$. So, the mean pre-activation at training time is:\n$$E[y] = (1-p) w^{\\top}h$$\n\nNow, consider the inference time. At inference, dropout is turned off, meaning all hidden units are retained (effectively, $m$ is a vector of all ones). The problem specifies that the weights $w$ are to be scaled by a uniform scalar factor $s$. The resulting inference-time pre-activation, let's call it $y_{inf}$, is:\n$$y_{inf} = (s w)^{\\top} h = s (w^{\\top} h)$$\n\nThe condition is that the inference-time output must preserve the training-time mean for any $h$ and $w$. We equate the two expressions:\n$$y_{inf} = E[y]$$\n$$s (w^{\\top}h) = (1-p) w^{\\top}h$$\n\nThis equality must hold for any choice of $w \\in \\mathbb{R}^{n}$ and $h \\in \\mathbb{R}^{n}$. We can choose $w$ and $h$ such that their dot product $w^{\\top}h \\neq 0$. For any such choice, we can divide both sides by $w^{\\top}h$, which yields:\n$$s = 1-p$$\n\nThis scalar factor $s = 1-p$ is applied to the weights at inference time to ensure the expected output of each neuron remains consistent between training and testing, a technique often referred to as weight scaling at test time.\n\nThe two required quantities are the expected number of retained units, $n(1-p)$, and the scaling factor, $1-p$.",
            "answer": "$$\\boxed{\\begin{pmatrix} n(1-p) & 1-p \\end{pmatrix}}$$"
        }
    ]
}