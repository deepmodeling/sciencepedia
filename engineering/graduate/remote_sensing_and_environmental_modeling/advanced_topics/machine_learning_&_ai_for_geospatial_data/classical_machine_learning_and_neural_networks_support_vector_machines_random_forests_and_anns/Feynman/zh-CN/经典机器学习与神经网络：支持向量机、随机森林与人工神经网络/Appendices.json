{
    "hands_on_practices": [
        {
            "introduction": "支持向量机（SVM）的精髓在于其最大化间隔分类器的思想。这个练习旨在通过一个简单的二维数据集，让您亲手计算定义决策边界的最优参数。通过解决这个基本问题，您将深刻理解SVM如何通过求解一个约束优化问题来找到最大几何间隔，并确定哪些数据点（即支持向量）是定义该边界的关键 。",
            "id": "3801071",
            "problem": "我们正在分析一幅多光谱卫星场景，以在一个二维特征空间中分离两种地物类别，其中的特征是由归一化波段比表示的 $x_{1}$ 和 $x_{2}$。从光谱均匀的校准块中获得了四个带标签的原型观测值，它们在 $\\mathbb{R}^{2}$ 中给出：正类 $\\mathcal{C}_{+}$ 的点位于 $(0,1)$ 和 $(1,0)$，负类 $\\mathcal{C}_{-}$ 的点位于 $(0,-1)$ 和 $(-1,0)$。我们使用线性决策函数 $f(x) = w^{\\top}x + b$ 训练一个硬间隔支持向量机 (SVM)，其中 $w \\in \\mathbb{R}^{2}$ 且 $b \\in \\mathbb{R}$，并遵循针对带标签 $y_{i} \\in \\{+1,-1\\}$ 的训练样本 $(x_{i}, y_{i})$ 的标准可分性约束。\n\n从一个带标签点 $(x_{i}, y_{i})$ 在 $(w,b)$ 下的几何间隔的定义（定义为 $\\gamma_{i} = \\frac{y_{i}(w^{\\top}x_{i} + b)}{\\|w\\|}$）出发，并利用“最大化关于 $i$ 的最小几何间隔等价于在所有 $i$ 满足 $y_{i}(w^{\\top}x_{i} + b) \\ge 1$ 的约束下最小化 $\\|w\\|$”这一经过充分检验的等价关系，确定最优硬间隔SVM在此数据集上所能达到的最大几何间隔，并识别出哪些训练点是支持向量。\n\n请将最大几何间隔表示为一个精确值。无需四舍五入。您的推理中应证明支持向量的确定过程，但最终答案只需给出间隔值。",
            "solution": "用户提供了一个有效的问题陈述。\n\n该问题要求计算对于给定的一组位于 $\\mathbb{R}^{2}$ 的四个带标签数据点，硬间隔支持向量机 (SVM) 的最大几何间隔。解决方案需要找到定义分离超平面的最优权重向量 $w$ 和偏置 $b$。\n\n四个带标签的训练点如下：\n对于正类 $\\mathcal{C}_{+}$ (标签 $y=+1$):\n$x_{A} = (0, 1)^{\\top}$\n$x_{B} = (1, 0)^{\\top}$\n\n对于负类 $\\mathcal{C}_{-}$ (标签 $y=-1$):\n$x_{C} = (0, -1)^{\\top}$\n$x_{D} = (-1, 0)^{\\top}$\n\nSVM学习问题是找到参数 $w = (w_1, w_2)^{\\top}$ 和 $b \\in \\mathbb{R}$，使得在所有 $i \\in \\{A, B, C, D\\}$ 满足约束条件 $y_{i}(w^{\\top}x_{i} + b) \\ge 1$ 的情况下，最小化 $\\frac{1}{2}\\|w\\|^2$。\n\n让我们明确地写出这四个约束条件：\n1. 对于点 $x_A=(0,1)^{\\top}$，其标签为 $y_A=+1$：\n$y_A(w^{\\top}x_A + b) = (+1)(w_1 \\cdot 0 + w_2 \\cdot 1 + b) = w_2 + b \\ge 1$。\n\n2. 对于点 $x_B=(1,0)^{\\top}$，其标签为 $y_B=+1$：\n$y_B(w^{\\top}x_B + b) = (+1)(w_1 \\cdot 1 + w_2 \\cdot 0 + b) = w_1 + b \\ge 1$。\n\n3. 对于点 $x_C=(0,-1)^{\\top}$，其标签为 $y_C=-1$：\n$y_C(w^{\\top}x_C + b) = (-1)(w_1 \\cdot 0 + w_2 \\cdot (-1) + b) = -(-w_2 + b) = w_2 - b \\ge 1$。\n\n4. 对于点 $x_D=(-1,0)^{\\top}$，其标签为 $y_D=-1$：\n$y_D(w^{\\top}x_D + b) = (-1)(w_1 \\cdot (-1) + w_2 \\cdot 0 + b) = -(-w_1 + b) = w_1 - b \\ge 1$。\n\n因此，我们得到了关于参数 $w_1, w_2, b$ 的四个线性不等式组：\n$$\n\\begin{cases}\nw_2 + b \\ge 1 \\\\\nw_1 + b \\ge 1 \\\\\nw_2 - b \\ge 1 \\\\\nw_1 - b \\ge 1\n\\end{cases}\n$$\n从第二个和第四个不等式，我们有 $w_1 \\ge 1 - b$ 和 $w_1 \\ge 1 + b$。这意味着 $w_1$ 必须大于或等于这两个量的最大值：$w_1 \\ge \\max(1-b, 1+b)$。\n类似地，从第一个和第三个不等式，我们有 $w_2 \\ge 1 - b$ 和 $w_2 \\ge 1 + b$，这意味着 $w_2 \\ge \\max(1-b, 1+b)$。\n\n我们的目标是最小化 $\\frac{1}{2}\\|w\\|^2 = \\frac{1}{2}(w_1^2 + w_2^2)$。这等价于最小化 $\\|w\\|^2 = w_1^2 + w_2^2$。由于 $w_1^2$ 和 $w_2^2$ 是非负的，并且对于正参数是递增的，我们必须为 $|w_1|$ 和 $|w_2|$ 选择满足约束条件的最小可能值。从问题的几何结构来看，很明显 $w_1$ 和 $w_2$ 将是正数。\n因此，为了最小化 $w_1^2 + w_2^2$，我们必须将 $w_1$ 和 $w_2$ 设置为它们的最小可能值。\n$$\nw_1 = \\max(1-b, 1+b)\n$$\n$$\nw_2 = \\max(1-b, 1+b)\n$$\n注意 $\\max(1-b, 1+b)$ 可以用绝对值函数表示为 $1+|b|$。因此，$w_1 = w_2 = 1+|b|$。\n\n目标函数现在可以仅用 $b$ 来表示：\n$$\n\\|w\\|^2 = w_1^2 + w_2^2 = (1+|b|)^2 + (1+|b|)^2 = 2(1+|b|)^2\n$$\n我们需要找到使这个表达式最小化的 $b$ 值。函数 $(1+|b|)^2$ 在 $|b|$ 最小时取最小值。$|b|$ 的最小值为 $0$，此时 $b=0$。\n\n因此，最优偏置是 $b_{opt} = 0$。\n将此值代回到 $w_1$ 和 $w_2$ 的表达式中：\n$$w_1 = 1+|0| = 1$$\n$$w_2 = 1+|0| = 1$$\n最优权重向量是 $w_{opt} = (1, 1)^{\\top}$。\n\n这个最优权重向量的范数是 $\\|w_{opt}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$。\n\n最大几何间隔 $\\gamma_{max}$ 由公式 $\\gamma_{max} = \\frac{1}{\\|w_{opt}\\|}$ 给出。\n代入 $\\|w_{opt}\\|$ 的值，我们得到：\n$$\n\\gamma_{max} = \\frac{1}{\\sqrt{2}}\n$$\n\n为了识别支持向量，我们检查哪些点满足等式条件 $y_i(w_{opt}^{\\top}x_i + b_{opt}) = 1$。当 $w_{opt}=(1,1)^{\\top}$ 且 $b_{opt}=0$ 时：\n- 点 $x_A=(0,1)^{\\top}, y_A=+1$: $(+1)(1 \\cdot 0 + 1 \\cdot 1 + 0) = 1$。这是一个支持向量。\n- 点 $x_B=(1,0)^{\\top}, y_B=+1$: $(+1)(1 \\cdot 1 + 1 \\cdot 0 + 0) = 1$。这是一个支持向量。\n- 点 $x_C=(0,-1)^{\\top}, y_C=-1$: $(-1)(1 \\cdot 0 + 1 \\cdot (-1) + 0) = (-1)(-1) = 1$。这是一个支持向量。\n- 点 $x_D=(-1,0)^{\\top}, y_D=-1$: $(-1)(1 \\cdot (-1) + 1 \\cdot 0 + 0) = (-1)(-1) = 1$。这是一个支持向量。\n所有四个训练点都是支持向量。它们位于定义间隔的超平面 $w^{\\top}x+b = \\pm 1$ 上，即 $x_1+x_2 = 1$ 和 $x_1+x_2 = -1$。分离超平面是 $x_1+x_2=0$。\n\n最大几何间隔是任何一个支持向量到分离超平面的距离。例如，对于点 $x_A=(0,1)^{\\top}$ 和超平面 $1 \\cdot x_1 + 1 \\cdot x_2 + 0 = 0$，距离是 $\\frac{|1 \\cdot 0 + 1 \\cdot 1 + 0|}{\\sqrt{1^2+1^2}} = \\frac{1}{\\sqrt{2}}$。这证实了我们的结果。",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2}}}$$"
        },
        {
            "introduction": "随机森林（RF）等集成模型的强大性能高度依赖于超参数的精细调整，这直接关系到模型的偏见-方差权衡。本练习模拟了在遥感分类任务中，通过分析交叉验证误差曲线来优化模型复杂度的过程。您将通过对给定的误差函数进行最小化，为最小节点大小、最大树深和分裂特征数找到最佳值，从而在欠拟合与过拟合之间取得平衡 。",
            "id": "3801066",
            "problem": "一项土地覆盖分类任务使用一个随机森林 (RF) 分类器，该分类器在从卫星影像中提取的融合多光谱与纹理特征上进行训练。特征向量有 $p$ 个维度，其中 $p = 36$。数据集是多类别且不平衡的：农业的类别先验概率为 $0.68$，城市的类别先验概率为 $0.22$，湿地的类别先验概率为 $0.10$。为防止多数类主导，模型选择使用平衡错误率 (BER) 进行，该指标在多类别分类中定义为各类错分率的算术平均值。目标是选择三个能共同平衡偏差和方差的随机森林超参数：最小节点大小 $n_{\\min}$、最大树深度 $d_{\\max}$ 以及每次分裂时考虑的特征数量 $m_{\\mathrm{try}}$。\n\n基于以下基本原理：\n- 经验风险最小化和 $k$-折交叉验证：通过对各折的验证误差求平均来估计期望泛化误差，此处 $k = 10$。\n- 偏差-方差分解：增加模型复杂度通常会减少偏差并增加方差，而降低模型复杂度则相反。\n- 平衡错误率 (BER) 和类别加权：当类别不平衡时，最小化 BER 能更好地与最小化跨类别的期望错分率保持一致，而不是跨样本的。\n\n针对每个超参数独立地进行了 $10$-折交叉验证，同时将其他超参数固定在一个中性基准值上。得到的验证曲线经过平滑处理，并拟合成函数，这些函数概括了在遥感数据上随机森林实践中观察到的偏差-方差权衡。这些拟合曲线为：\n- 对于最小节点大小 $n_{\\min}$ (整数定义域 $1 \\leq n_{\\min} \\leq 20$):\n$$\nE_{n}(n_{\\min}) \\;=\\; 0.15 \\;+\\; \\frac{0.064}{n_{\\min}} \\;+\\; 0.001\\,n_{\\min}.\n$$\n- 对于最大树深度 $d_{\\max}$ (整数定义域 $2 \\leq d_{\\max} \\leq 40$):\n$$\nE_{d}(d_{\\max}) \\;=\\; 0.14 \\;+\\; \\frac{0.144}{d_{\\max}} \\;+\\; 0.001\\,d_{\\max}.\n$$\n- 对于每次分裂的特征数 $m_{\\mathrm{try}}$ (整数定义域 $1 \\leq m_{\\mathrm{try}} \\leq p$):\n$$\nE_{m}(m_{\\mathrm{try}}) \\;=\\; 0.13 \\;+\\; 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} \\;+\\; \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right),\n$$\n其中 $p = 36$。\n\n假设逐超参数的交叉验证拟合结果是对各超参数对验证 BER 贡献的可分离近似，因此可以通过在各自的定义域上最小化每个拟合曲线来选择一个平衡的配置。计算在给定的整数约束下，使各自曲线最小化的值 $n_{\\min}^{\\star}$、$d_{\\max}^{\\star}$ 和 $m_{\\mathrm{try}}^{\\star}$。将最终答案以单行矩阵 $\\left(n_{\\min}^{\\star} \\;\\; d_{\\max}^{\\star} \\;\\; m_{\\mathrm{try}}^{\\star}\\right)$ 的形式放在 $\\mathrm{pmatrix}$ 环境中。无需四舍五入。",
            "solution": "首先根据指定标准对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **模型与任务**：用于多类别土地覆盖分类任务的随机森林 (RF) 分类器。\n- **特征向量维度**：特征数量为 $p = 36$。\n- **类别先验概率**：农业 ($0.68$)、城市 ($0.22$)、湿地 ($0.10$)。\n- **性能指标**：平衡错误率 (BER)。\n- **待优化的超参数**：最小节点大小 ($n_{\\min}$)、最大树深度 ($d_{\\max}$) 和每次分裂的特征数 ($m_{\\mathrm{try}}$)。\n- **优化方法**：基于 $k$-折交叉验证，其中 $k=10$。通过假设各超参数对误差的贡献是可分离的来简化优化过程。\n- **拟合误差函数**：\n  - 对于最小节点大小 $n_{\\min}$，整数定义域为 $1 \\leq n_{\\min} \\leq 20$：\n    $$E_{n}(n_{\\min}) = 0.15 + \\frac{0.064}{n_{\\min}} + 0.001 n_{\\min}$$\n  - 对于最大树深度 $d_{\\max}$，整数定义域为 $2 \\leq d_{\\max} \\leq 40$：\n    $$E_{d}(d_{\\max}) = 0.14 + \\frac{0.144}{d_{\\max}} + 0.001 d_{\\max}$$\n  - 对于每次分裂的特征数 $m_{\\mathrm{try}}$，整数定义域为 $1 \\leq m_{\\mathrm{try}} \\leq p$，其中 $p=36$：\n    $$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right)$$\n- **目标**：计算在给定定义域上使各自误差函数最小化的最优整数值 $n_{\\min}^{\\star}$、$d_{\\max}^{\\star}$ 和 $m_{\\mathrm{try}}^{\\star}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n对问题进行严格审查。\n- **科学依据**：该问题在统计机器学习原理方面有坚实的基础。它解决了随机森林分类器超参数调优的实际任务。交叉验证、偏差-方差权衡以及对不平衡数据集使用平衡错误率等概念都是标准且合理的。验证误差曲线的函数形式是对此权衡的合理唯象模型，其中对于过于简单和过于复杂的模型，误差都很高。\n- **适定性**：该问题是适定的。它要求在指定的整数定义域上最小化三个不同的、明确定义的函数。可分离性的假设虽然是对真实联合优化问题的简化，但它使问题变得易于处理，并且是一种常见的启发式方法。预计每个最小化问题都有一个唯一的整数值解。\n- **客观性**：该问题使用精确、客观的数学语言陈述。\n- **结论**：该问题没有违反任何指定的无效标准。它在科学上是合理的、自洽的、适定的和客观的。\n\n### 步骤 3：裁定与行动\n该问题被判定为**有效**。将推导解答。\n\n任务是找到超参数 $n_{\\min}$、$d_{\\max}$ 和 $m_{\\mathrm{try}}$ 的整数值，以使其各自的验证误差函数 $E_{n}$、$E_{d}$ 和 $E_{m}$ 最小化。这可以通过独立地找到每个函数的最小值来实现。对于形式为 $f(x) = c_0 + c_1/x + c_2 x$ 且 $c_1, c_2 > 0$ 的函数，当 $x > 0$ 时该函数是凸函数。连续变量 $x$ 的最小值可以通过将其导数设为零来找到。如果解不是整数，则整数最小值必定位于连续最小值两侧的两个整数之一。\n\n**1. 最小节点大小 ($n_{\\min}$) 的优化**\n\n$n_{\\min}$ 的误差函数由下式给出：\n$$E_{n}(n_{\\min}) = 0.15 + \\frac{0.064}{n_{\\min}} + 0.001 n_{\\min}$$\n$n_{\\min}$ 的定义域是 $[1, 20]$ 内的整数集合。\n为求最小值，我们首先将 $n_{\\min}$ 视为连续变量，并计算其关于 $n_{\\min}$ 的一阶导数：\n$$\\frac{dE_n}{dn_{\\min}} = - \\frac{0.064}{n_{\\min}^2} + 0.001$$\n将导数设为零以求临界点：\n$$0.001 = \\frac{0.064}{n_{\\min}^2} \\implies n_{\\min}^2 = \\frac{0.064}{0.001} = 64$$\n求解 $n_{\\min}$ (由于节点大小必须为正，取正根)：\n$$n_{\\min} = \\sqrt{64} = 8$$\n为确认这是一个最小值，我们检查二阶导数：\n$$\\frac{d^2E_n}{dn_{\\min}^2} = \\frac{2 \\times 0.064}{n_{\\min}^3} = \\frac{0.128}{n_{\\min}^3}$$\n对于任何 $n_{\\min} > 0$，二阶导数均为正，确认 $n_{\\min} = 8$ 是一个局部最小值。由于这是定义域内唯一的临界点，因此它也是连续函数的全局最小值。\n连续函数的最小值出现在整数值 $8$ 处，该值位于指定定义域 $[1, 20]$ 内。因此，最优整数值为 $n_{\\min}^{\\star} = 8$。\n\n**2. 最大树深度 ($d_{\\max}$) 的优化**\n\n$d_{\\max}$ 的误差函数为：\n$$E_{d}(d_{\\max}) = 0.14 + \\frac{0.144}{d_{\\max}} + 0.001 d_{\\max}$$\n$d_{\\max}$ 的定义域是 $[2, 40]$ 内的整数集合。\n我们计算其关于 $d_{\\max}$ 的一阶导数：\n$$\\frac{dE_d}{dd_{\\max}} = - \\frac{0.144}{d_{\\max}^2} + 0.001$$\n将导数设为零：\n$$0.001 = \\frac{0.144}{d_{\\max}^2} \\implies d_{\\max}^2 = \\frac{0.144}{0.001} = 144$$\n求解 $d_{\\max}$：\n$$d_{\\max} = \\sqrt{144} = 12$$\n二阶导数为 $\\frac{d^2E_d}{dd_{\\max}^2} = \\frac{2 \\times 0.144}{d_{\\max}^3} = \\frac{0.288}{d_{\\max}^3}$，对于 $d_{\\max} > 0$ 其为正。因此，$d_{\\max} = 12$ 是一个最小值。\n连续函数的最小值出现在整数值 $12$ 处，该值位于定义域 $[2, 40]$ 内。最优整数值为 $d_{\\max}^{\\star} = 12$。\n\n**3. 每次分裂的特征数 ($m_{\\mathrm{try}}$) 的优化**\n\n$m_{\\rm try}$ 的误差函数由下式给出：\n$$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right)$$\n给定 $p = 36$，因此 $\\sqrt{p} = 6$。函数变为：\n$$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{6}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{6}\\right)$$\n$m_{\\mathrm{try}}$ 的定义域是 $[1, 36]$ 内的整数集合。\n我们计算其关于 $m_{\\mathrm{try}}$ 的一阶导数：\n$$\\frac{dE_m}{dm_{\\mathrm{try}}} = 0.005\\left(-\\frac{6}{m_{\\mathrm{try}}^2} + \\frac{1}{6}\\right)$$\n将导数设为零：\n$$\\frac{1}{6} = \\frac{6}{m_{\\mathrm{try}}^2} \\implies m_{\\mathrm{try}}^2 = 36$$\n求解 $m_{\\mathrm{try}}$：\n$$m_{\\mathrm{try}} = \\sqrt{36} = 6$$\n二阶导数为 $\\frac{d^2E_m}{dm_{\\mathrm{try}}^2} = 0.005\\left(\\frac{12}{m_{\\mathrm{try}}^3}\\right) = \\frac{0.06}{m_{\\mathrm{try}}^3}$，对于 $m_{\\mathrm{try}} > 0$ 其为正。因此，$m_{\\mathrm{try}} = 6$ 是一个最小值。\n连续函数的最小值出现在整数值 $6$ 处，该值位于定义域 $[1, 36]$ 内。最优整数值为 $m_{\\mathrm{try}}^{\\star} = 6$。这个结果与设置 $m_{\\mathrm{try}} \\approx \\sqrt{p}$ 的常用启发式方法一致。\n\n最优超参数配置为 $(n_{\\min}^{\\star}, d_{\\max}^{\\star}, m_{\\mathrm{try}}^{\\star})$。根据独立的最小化计算，最优值为 $(8, 12, 6)$。",
            "answer": "$$\\boxed{\\begin{pmatrix} 8  12  6 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在训练深度人工神经网络（ANN）时，Dropout是一种强大而常用的正则化技术，用于防止模型过拟合。为了正确使用它，理解其在训练和推理阶段的不同行为至关重要。本练习要求您从第一性原理出发，推导出在推理时保持网络输出期望一致性所需的权重缩放因子，从而揭示Dropout技术背后的数学原理 。",
            "id": "3801078",
            "problem": "一个团队正在一个遥感与环境建模应用中，训练一个用于从多光谱卫星图像进行土地覆盖分类的人工神经网络（ANN）。考虑一个具有 $n$ 个单元的单隐藏层，对于给定的像素，它产生一个确定性的激活向量 $h \\in \\mathbb{R}^{n}$。在训练期间，该团队对这个隐藏层应用 dropout（随机失活），其丢弃率为 $p \\in (0,1)$，其中每个单元以概率 $1-p$ 被独立保留，以概率 $p$ 被丢弃。下一层通过一个权重为 $w \\in \\mathbb{R}^{n}$ 的线性映射计算一个标量预激活值，因此带有 dropout 的标量预激活值为 $y = w^{\\top}(m \\odot h)$，其中 $m \\in \\{0,1\\}^{n}$ 是随机的 dropout 掩码，$\\odot$ 表示哈达玛（逐元素）积。\n\n仅从以下基本事实出发：独立保留指示器的伯努利模型、期望的线性性以及哈达玛积的定义，推导以下两个量：\n- 在丢弃率 $p$ 下，隐藏层中被保留单元的期望数量。\n- 在推理时（无 dropout）均匀应用于 $w$ 的所有分量的一个标量权重缩放因子 $s$，该因子对于任何确定性的 $h$ 和任何 $w$，都能保持标量预激活值 $y$ 的训练时均值。\n\n假设 dropout 掩码分量 $m_{i}$ 是参数为 $1-p$ 的独立同分布的伯努利随机变量，并且 $m$ 独立于 $h$。将您的最终结果表示为一个闭式解析表达式。不需要数值近似。最终答案必须是一个单行矩阵，按顺序包含期望计数和缩放因子。",
            "solution": "问题陈述已经过验证，被认为是合理的。它在科学上基于概率论和机器学习的原理，问题提出得很好，目标明确，信息充分，并使用了精确、客观的语言。该问题是与神经网络中 dropout 技术相关的一个标准的、可形式化的推导。因此，将提供一个解答。\n\n问题要求从第一性原理推导两个量：在 dropout 期间隐藏层中被保留单元的期望数量，以及一个在推理时保持均值输出的权重缩放因子。\n\n设隐藏层有 $n$ 个单元。dropout 掩码是一个随机向量 $m \\in \\{0, 1\\}^n$，其中每个分量 $m_i$（对于 $i \\in \\{1, 2, \\dots, n\\}$）是一个独立同分布（i.i.d.）的伯努利随机变量。如果 $m_i = 1$，则单元 $i$ 被保留；如果 $m_i = 0$，则被丢弃。问题陈述每个单元的保留概率为 $1-p$，其中 $p \\in (0,1)$ 是 dropout 率。因此，$m_i \\sim \\text{Bernoulli}(1-p)$。\n\n$m_i$ 的概率质量函数为：\n$$P(m_i = 1) = 1-p$$\n$$P(m_i = 0) = p$$\n\n参数为 $\\theta$ 的伯努利随机变量的期望是 $\\theta$。因此，每个 $m_i$ 的期望是：\n$$E[m_i] = 1 \\cdot P(m_i=1) + 0 \\cdot P(m_i=0) = 1 \\cdot (1-p) + 0 \\cdot p = 1-p$$\n\n**第1部分：被保留单元的期望数量**\n\n设 $N_{retained}$ 是表示隐藏层中被保留单元总数的随机变量。如果一个单元对应的掩码元素为 $1$，则该单元被保留。被保留单元的总数是掩码向量 $m$ 中所有指示器的和：\n$$N_{retained} = \\sum_{i=1}^{n} m_i$$\n\n为了求出被保留单元的期望数量，我们计算 $N_{retained}$ 的期望。根据给定的期望线性性原理，我们可以写出：\n$$E[N_{retained}] = E\\left[\\sum_{i=1}^{n} m_i\\right] = \\sum_{i=1}^{n} E[m_i]$$\n\n如前所述，$E[m_i] = 1-p$ 对于所有 $i \\in \\{1, 2, \\dots, n\\}$ 成立。将此代入求和式中：\n$$E[N_{retained}] = \\sum_{i=1}^{n} (1-p)$$\n\n由于 $(1-p)$ 相对于求和索引 $i$ 是一个常数，我们将这个常数相加 $n$ 次：\n$$E[N_{retained}] = n(1-p)$$\n这就是隐藏层中被保留单元的期望数量。\n\n**第2部分：推理时权重缩放因子**\n\n在训练时，标量预激活值 $y$ 是使用 dropout 掩码 $m$ 计算的：\n$$y = w^{\\top}(m \\odot h)$$\n其中 $w \\in \\mathbb{R}^{n}$ 是权重向量，$h \\in \\mathbb{R}^{n}$ 是确定性的激活向量，$\\odot$ 是哈达玛（逐元素）积。展开后为：\n$$y = \\sum_{i=1}^{n} w_i (m_i h_i)$$\n\n我们被要求找到一个缩放因子 $s$，使得推理时的输出（无 dropout）等于训练时输出的*均值*（期望值）。首先，我们计算训练时 $y$ 的期望值，记为 $E[y]$。\n$$E[y] = E\\left[\\sum_{i=1}^{n} w_i m_i h_i\\right]$$\n\n使用期望的线性性：\n$$E[y] = \\sum_{i=1}^{n} E[w_i m_i h_i]$$\n\n权重向量 $w$ 和激活向量 $h$ 对于特定输入是确定性的。dropout 掩码 $m$ 独立于 $h$。因此，$w_i$ 和 $h_i$ 可以被视为关于期望算子的常数，该期望算子对 $m$ 的分布进行平均。\n$$E[y] = \\sum_{i=1}^{n} w_i h_i E[m_i]$$\n\n代入 $E[m_i] = 1-p$：\n$$E[y] = \\sum_{i=1}^{n} w_i h_i (1-p) = (1-p) \\sum_{i=1}^{n} w_i h_i$$\n\n求和项 $\\sum_{i=1}^{n} w_i h_i$ 是点积 $w^{\\top}h$ 的定义。所以，训练时的平均预激活值为：\n$$E[y] = (1-p) w^{\\top}h$$\n\n现在，考虑推理时间。在推理时，dropout被关闭，意味着所有隐藏单元都被保留（实际上，$m$ 是一个全为1的向量）。问题规定权重 $w$ 要按一个统一的标量因子 $s$ 进行缩放。得到的推理时预激活值，我们称之为 $y_{inf}$，是：\n$$y_{inf} = (s w)^{\\top} h = s (w^{\\top} h)$$\n\n条件是对于任何 $h$ 和 $w$，推理时的输出必须保持训练时的均值。我们令两个表达式相等：\n$$y_{inf} = E[y]$$\n$$s (w^{\\top}h) = (1-p) w^{\\top}h$$\n\n这个等式必须对 $w \\in \\mathbb{R}^{n}$ 和 $h \\in \\mathbb{R}^{n}$ 的任何选择都成立。我们可以选择 $w$ 和 $h$ 使它们的点积 $w^{\\top}h \\neq 0$。对于任何这样的选择，我们可以在等式两边同时除以 $w^{\\top}h$，得到：\n$$s = 1-p$$\n\n这个标量因子 $s = 1-p$ 在推理时应用于权重，以确保每个神经元的期望输出在训练和测试之间保持一致，这种技术通常被称为测试时权重缩放。\n\n所需的两个量是被保留单元的期望数量 $n(1-p)$ 和缩放因子 $1-p$。",
            "answer": "$$\\boxed{\\begin{pmatrix} n(1-p)  1-p \\end{pmatrix}}$$"
        }
    ]
}