{
    "hands_on_practices": [
        {
            "introduction": "支持向量机（SVM）是一种强大的分类工具，其核心思想是在特征空间中找到一个能够以最大“间隔”将不同类别分开的决策边界。这个练习通过一个简化的遥感场景，让您亲手计算线性SVM的最佳超平面和最大几何间隔，从而深入理解SVM的优化目标和支持向量在定义决策边界中的关键作用。",
            "id": "3801071",
            "problem": "正在分析一个多光谱卫星场景，以在一个二维特征空间中分离两种土地覆盖类别，其中特征是由归一化波段比表示的 $x_{1}$ 和 $x_{2}$。从光谱均匀的校准区域获得了四个带标签的原型观测值，它们在 $\\mathbb{R}^{2}$ 中给出：正类 $\\mathcal{C}_{+}$ 的点位于 $(0,1)$ 和 $(1,0)$，负类 $\\mathcal{C}_{-}$ 的点位于 $(0,-1)$ 和 $(-1,0)$。一个硬间隔支持向量机（SVM）使用线性决策函数 $f(x) = w^{\\top}x + b$ 进行训练，其中 $w \\in \\mathbb{R}^{2}$ 且 $b \\in \\mathbb{R}$，遵循带有标签 $y_{i} \\in \\{+1,-1\\}$ 的训练样本 $(x_{i}, y_{i})$ 的标准可分性约束。\n\n从带标签点 $(x_{i}, y_{i})$ 在 $(w,b)$ 下的几何间隔的定义开始，该定义为 $\\gamma_{i} = \\frac{y_{i}(w^{\\top}x_{i} + b)}{\\|w\\|}$，并利用一个经过充分验证的等价关系，即最大化所有 $i$ 上的最小几何间隔等价于最小化 $\\|w\\|$（在对所有 $i$ 满足 $y_{i}(w^{\\top}x_{i} + b) \\ge 1$ 的约束下），确定最优硬间隔SVM对于此数据集所能达到的最大几何间隔，并指明哪些训练点是支持向量。\n\n将最大几何间隔表示为一个精确值。无需四舍五入。你的推理中应证明支持向量的确定过程，但最终答案只需给出间隔值。",
            "solution": "用户提供了一个有效的问题陈述。\n\n问题要求计算给定 $\\mathbb{R}^{2}$ 中四个带标签数据点的硬间隔支持向量机（SVM）的最大几何间隔。解决方法需要找到定义分离超平面的最优权重向量 $w$ 和偏置 $b$。\n\n四个带标签的训练点是：\n对于正类 $\\mathcal{C}_{+}$（标签 $y=+1$）：\n$x_{A} = (0, 1)^{\\top}$\n$x_{B} = (1, 0)^{\\top}$\n\n对于负类 $\\mathcal{C}_{-}$（标签 $y=-1$）：\n$x_{C} = (0, -1)^{\\top}$\n$x_{D} = (-1, 0)^{\\top}$\n\nSVM 学习问题是找到参数 $w = (w_1, w_2)^{\\top}$ 和 $b \\in \\mathbb{R}$，使得在约束条件 $y_{i}(w^{\\top}x_{i} + b) \\ge 1$ 对所有 $i \\in \\{A, B, C, D\\}$ 成立的情况下，最小化 $\\frac{1}{2}\\|w\\|^2$。\n\n让我们明确地写出这四个约束条件：\n1. 对于点 $x_A=(0,1)^{\\top}$，其标签为 $y_A=+1$：\n$y_A(w^{\\top}x_A + b) = (+1)(w_1 \\cdot 0 + w_2 \\cdot 1 + b) = w_2 + b \\ge 1$。\n\n2. 对于点 $x_B=(1,0)^{\\top}$，其标签为 $y_B=+1$：\n$y_B(w^{\\top}x_B + b) = (+1)(w_1 \\cdot 1 + w_2 \\cdot 0 + b) = w_1 + b \\ge 1$。\n\n3. 对于点 $x_C=(0,-1)^{\\top}$，其标签为 $y_C=-1$：\n$y_C(w^{\\top}x_C + b) = (-1)(w_1 \\cdot 0 + w_2 \\cdot (-1) + b) = -(-w_2 + b) = w_2 - b \\ge 1$。\n\n4. 对于点 $x_D=(-1,0)^{\\top}$，其标签为 $y_D=-1$：\n$y_D(w^{\\top}x_D + b) = (-1)(w_1 \\cdot (-1) + w_2 \\cdot 0 + b) = -(-w_1 + b) = w_1 - b \\ge 1$。\n\n因此，我们得到一个关于参数 $w_1, w_2, b$ 的四个线性不等式组成的系统：\n$$\n\\begin{cases}\nw_2 + b \\ge 1 \\\\\nw_1 + b \\ge 1 \\\\\nw_2 - b \\ge 1 \\\\\nw_1 - b \\ge 1\n\\end{cases}\n$$\n从第二个和第四个不等式，我们有 $w_1 \\ge 1 - b$ 和 $w_1 \\ge 1 + b$。这意味着 $w_1$ 必须大于或等于这两个量中的最大值：$w_1 \\ge \\max(1-b, 1+b)$。\n类似地，从第一个和第三个不等式，我们有 $w_2 \\ge 1 - b$ 和 $w_2 \\ge 1 + b$，这意味着 $w_2 \\ge \\max(1-b, 1+b)$。\n\n目标是最小化 $\\frac{1}{2}\\|w\\|^2 = \\frac{1}{2}(w_1^2 + w_2^2)$。这等价于最小化 $\\|w\\|^2 = w_1^2 + w_2^2$。由于 $w_1^2$ 和 $w_2^2$ 是非负的，并且对于正参数是递增的，我们必须选择满足约束条件的 $|w_1|$ 和 $|w_2|$ 的最小可能值。从问题的几何结构可以清楚地看出，$w_1$ 和 $w_2$ 将是正数。\n因此，为了最小化 $w_1^2 + w_2^2$，我们必须将 $w_1$ 和 $w_2$ 设置为其可能的最小值。\n$$\nw_1 = \\max(1-b, 1+b)\n$$\n$$\nw_2 = \\max(1-b, 1+b)\n$$\n注意，$\\max(1-b, 1+b)$ 可以用绝对值函数表示为 $1+|b|$。因此，$w_1 = w_2 = 1+|b|$。\n\n目标函数现在可以完全用 $b$ 来表示：\n$$\n\\|w\\|^2 = w_1^2 + w_2^2 = (1+|b|)^2 + (1+|b|)^2 = 2(1+|b|)^2\n$$\n我们需要找到使该表达式最小化的 $b$ 值。当 $|b|$ 最小时，函数 $(1+|b|)^2$ 最小。$|b|$ 的最小值为 $0$，这在 $b=0$ 时发生。\n\n因此，最优偏置是 $b_{opt} = 0$。\n将这个值代回到 $w_1$ 和 $w_2$ 的表达式中：\n$$\nw_1 = 1+|0| = 1\n$$\n$$\nw_2 = 1+|0| = 1\n$$\n最优权重向量是 $w_{opt} = (1, 1)^{\\top}$。\n\n这个最优权重向量的范数是 $\\|w_{opt}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$。\n\n最大几何间隔 $\\gamma_{max}$ 由公式 $\\gamma_{max} = \\frac{1}{\\|w_{opt}\\|}$ 给出。\n代入 $\\|w_{opt}\\|$ 的值，我们得到：\n$$\n\\gamma_{max} = \\frac{1}{\\sqrt{2}}\n$$\n\n为了确定支持向量，我们检查哪些点满足等式条件 $y_i(w_{opt}^{\\top}x_i + b_{opt}) = 1$。当 $w_{opt}=(1,1)^{\\top}$ 且 $b_{opt}=0$ 时：\n- 点 $x_A=(0,1)^{\\top}, y_A=+1$：$(+1)(1 \\cdot 0 + 1 \\cdot 1 + 0) = 1$。这是一个支持向量。\n- 点 $x_B=(1,0)^{\\top}, y_B=+1$：$(+1)(1 \\cdot 1 + 1 \\cdot 0 + 0) = 1$。这是一个支持向量。\n- 点 $x_C=(0,-1)^{\\top}, y_C=-1$：$(-1)(1 \\cdot 0 + 1 \\cdot (-1) + 0) = (-1)(-1) = 1$。这是一个支持向量。\n- 点 $x_D=(-1,0)^{\\top}, y_D=-1$：$(-1)(1 \\cdot (-1) + 1 \\cdot 0 + 0) = (-1)(-1) = 1$。这是一个支持向量。\n所有四个训练点都是支持向量。它们位于定义间隔的超平面 $w^{\\top}x+b = \\pm 1$上，即 $x_1+x_2 = 1$ 和 $x_1+x_2 = -1$。分离超平面是 $x_1+x_2=0$。\n\n最大几何间隔是任何一个支持向量到分离超平面的距离。例如，对于点 $x_A=(0,1)^{\\top}$ 和超平面 $1 \\cdot x_1 + 1 \\cdot x_2 + 0 = 0$，距离是 $\\frac{|1 \\cdot 0 + 1 \\cdot 1 + 0|}{\\sqrt{1^2+1^2}} = \\frac{1}{\\sqrt{2}}$。这证实了我们的结果。",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2}}}$$"
        },
        {
            "introduction": "随机森林（RF）的性能在很大程度上取决于其超参数的选择，这些参数直接控制着模型的复杂度和对偏差-方差的权衡。本练习模拟了一个在遥感分类中至关重要的超参数调优过程，要求您通过分析给定的验证误差曲线来确定最小节点大小、最大树深和每次分裂的特征数。通过解决这个问题，您将掌握如何通过调整模型复杂度来最小化泛化误差的核心技能。",
            "id": "3801066",
            "problem": "一项土地覆盖分类任务使用了一个随机森林 (RF) 分类器，该分类器基于从卫星影像中提取的融合多光谱和纹理特征进行训练。特征向量有 $p$ 个波段，其中 $p = 36$。该数据集是多类别且不平衡的：农业类的先验概率为 $0.68$，城市类的先验概率为 $0.22$，湿地类的先验概率为 $0.10$。为防止多数类主导，模型选择使用平衡错误率 (BER) 进行，对于多类别分类，BER 定义为各类别错分率的算术平均值。目标是选择三个能共同平衡偏差和方差的 RF 超参数：最小节点大小 $n_{\\min}$、最大树深 $d_{\\max}$ 以及每次分裂时考虑的特征数 $m_{\\mathrm{try}}$。\n\n从第一性原理出发，使用以下基本依据：\n- 经验风险最小化和 $k$ 折交叉验证：期望泛化误差通过对各折的验证误差求平均来估计，此处 $k = 10$。\n- 偏差-方差分解：增加模型复杂度通常会减少偏差并增加方差，而降低模型复杂度则相反。\n- 平衡错误率 (BER) 和类别加权：当类别不平衡时，最小化 BER 能更好地与最小化跨类别的期望错分（而非跨样本的期望错分）保持一致。\n\n对每个超参数独立进行了 $10$ 折交叉验证，同时将其他超参数保持在一个中性基准值。得到的验证曲线经过平滑处理，并拟合到能够概括在遥感数据上随机森林实践中观察到的偏差-方差权衡的函数。这些拟合曲线如下：\n- 对于最小节点大小 $n_{\\min}$（整数域 $1 \\leq n_{\\min} \\leq 20$）：\n$$\nE_{n}(n_{\\min}) \\;=\\; 0.15 \\;+\\; \\frac{0.064}{n_{\\min}} \\;+\\; 0.001\\,n_{\\min}.\n$$\n- 对于最大深度 $d_{\\max}$（整数域 $2 \\leq d_{\\max} \\leq 40$）：\n$$\nE_{d}(d_{\\max}) \\;=\\; 0.14 \\;+\\; \\frac{0.144}{d_{\\max}} \\;+\\; 0.001\\,d_{\\max}.\n$$\n- 对于每次分裂的特征数 $m_{\\mathrm{try}}$（整数域 $1 \\leq m_{\\mathrm{try}} \\leq p$）：\n$$\nE_{m}(m_{\\mathrm{try}}) \\;=\\; 0.13 \\;+\\; 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} \\;+\\; \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right),\n$$\n其中 $p = 36$。\n\n假设按超参数进行的交叉验证拟合是各超参数对验证 BER 贡献的可分离近似，因此可以通过在各自定义域上最小化每个拟合曲线来选择一个平衡的配置。在给定的整数约束下，计算使各自曲线最小化的值 $n_{\\min}^{\\star}$、$d_{\\max}^{\\star}$ 和 $m_{\\mathrm{try}}^{\\star}$。将最终答案以单行矩阵 $\\left(n_{\\min}^{\\star} \\;\\; d_{\\max}^{\\star} \\;\\; m_{\\mathrm{try}}^{\\star}\\right)$ 的形式放在 $\\mathrm{pmatrix}$ 环境中。无需四舍五入。",
            "solution": "首先根据指定标准对问题陈述进行验证。\n\n### 第1步：提取已知信息\n- **模型与任务**：用于多类别土地覆盖分类任务的随机森林 (RF) 分类器。\n- **特征向量维度**：特征数量为 $p = 36$。\n- **类别先验概率**：农业 ($0.68$)、城市 ($0.22$)、湿地 ($0.10$)。\n- **性能指标**：平衡错误率 (BER)。\n- **待优化的超参数**：最小节点大小 ($n_{\\min}$)、最大树深 ($d_{\\max}$) 和每次分裂的特征数 ($m_{\\mathrm{try}}$)。\n- **优化方法**：基于 $k$ 折交叉验证，其中 $k = 10$。通过假设超参数对误差的贡献是可分离的来简化优化。\n- **拟合的误差函数**：\n  - 对于最小节点大小 $n_{\\min}$，在整数域 $1 \\leq n_{\\min} \\leq 20$ 上：\n    $$E_{n}(n_{\\min}) = 0.15 + \\frac{0.064}{n_{\\min}} + 0.001 n_{\\min}$$\n  - 对于最大深度 $d_{\\max}$，在整数域 $2 \\leq d_{\\max} \\leq 40$ 上：\n    $$E_{d}(d_{\\max}) = 0.14 + \\frac{0.144}{d_{\\max}} + 0.001 d_{\\max}$$\n  - 对于每次分裂的特征数 $m_{\\mathrm{try}}$，在整数域 $1 \\leq m_{\\mathrm{try}} \\leq p$ 上，其中 $p=36$：\n    $$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right)$$\n- **目标**：计算在给定定义域上使各自误差函数最小化的最优整数值 $n_{\\min}^{\\star}$、$d_{\\max}^{\\star}$ 和 $m_{\\mathrm{try}}^{\\star}$。\n\n### 第2步：使用提取的已知信息进行验证\n对问题进行严格审查。\n- **科学依据**：该问题在统计机器学习的原理上有充分的依据。它解决了随机森林分类器超参数调优的实际任务。交叉验证、偏差-方差权衡以及对不平衡数据集使用平衡错误率等概念都是标准且合理的。验证误差曲线的函数形式是这种权衡的合理唯象模型，即对于过于简单和过于复杂的模型，误差都很高。\n- **适定性**：该问题是适定的。它要求在指定的整数域上最小化三个不同的、明确定义的函数。可分离性的假设虽然是对真实联合优化问题的简化，但使问题变得易于处理，并且是一种常见的启发式方法。预计每次最小化都会得到一个唯一的整数值解。\n- **客观性**：该问题使用精确、客观的数学语言进行陈述。\n- **结论**：该问题没有违反任何指定的无效标准。它是科学合理、自洽、适定且客观的。\n\n### 第3步：判定与行动\n该问题被判定为**有效**。将推导解答。\n\n任务是找到超参数 $n_{\\min}$、$d_{\\max}$ 和 $m_{\\mathrm{try}}$ 的整数值，以最小化它们各自的验证误差函数 $E_{n}$、$E_{d}$ 和 $E_{m}$。这可以通过独立地找到每个函数的最小值来实现。对于形式为 $f(x) = c_0 + c_1/x + c_2 x$ 且 $c_1, c_2  0$ 的函数，当 $x  0$ 时，该函数是凸函数。对于连续变量 $x$，其最小值可以通过将导数设为零来找到。如果解不是整数，则整数最小值必定位于包含该连续最小值的两个整数之一。\n\n**1. 最小节点大小 ($n_{\\min}$) 的优化**\n\n$n_{\\min}$ 的误差函数由下式给出：\n$$E_{n}(n_{\\min}) = 0.15 + \\frac{0.064}{n_{\\min}} + 0.001 n_{\\min}$$\n$n_{\\min}$ 的定义域是 $[1, 20]$ 中的整数集合。\n为了找到最小值，我们首先将 $n_{\\min}$ 视为一个连续变量，并计算其关于 $n_{\\min}$ 的一阶导数：\n$$\\frac{dE_n}{dn_{\\min}} = - \\frac{0.064}{n_{\\min}^2} + 0.001$$\n将导数设为零以找到临界点：\n$$0.001 = \\frac{0.064}{n_{\\min}^2} \\implies n_{\\min}^2 = \\frac{0.064}{0.001} = 64$$\n求解 $n_{\\min}$（并取正根，因为节点大小必须为正）：\n$$n_{\\min} = \\sqrt{64} = 8$$\n为了确认这是一个最小值，我们检查二阶导数：\n$$\\frac{d^2E_n}{dn_{\\min}^2} = \\frac{2 \\times 0.064}{n_{\\min}^3} = \\frac{0.128}{n_{\\min}^3}$$\n对于任何 $n_{\\min}  0$，二阶导数都是正的，这证实了 $n_{\\min} = 8$ 是一个局部最小值。由于这是定义域内唯一的临界点，因此它是该连续函数的全局最小值。\n连续最小值出现在整数值 $8$ 上，该值位于指定的定义域 $[1, 20]$ 内。因此，最优整数值为 $n_{\\min}^{\\star} = 8$。\n\n**2. 最大树深 ($d_{\\max}$) 的优化**\n\n$d_{\\max}$ 的误差函数为：\n$$E_{d}(d_{\\max}) = 0.14 + \\frac{0.144}{d_{\\max}} + 0.001 d_{\\max}$$\n$d_{\\max}$ 的定义域是 $[2, 40]$ 中的整数集合。\n我们计算其关于 $d_{\\max}$ 的一阶导数：\n$$\\frac{dE_d}{dd_{\\max}} = - \\frac{0.144}{d_{\\max}^2} + 0.001$$\n将导数设为零：\n$$0.001 = \\frac{0.144}{d_{\\max}^2} \\implies d_{\\max}^2 = \\frac{0.144}{0.001} = 144$$\n求解 $d_{\\max}$：\n$$d_{\\max} = \\sqrt{144} = 12$$\n二阶导数为 $\\frac{d^2E_d}{dd_{\\max}^2} = \\frac{2 \\times 0.144}{d_{\\max}^3} = \\frac{0.288}{d_{\\max}^3}$，对于 $d_{\\max}  0$ 为正值。因此，$d_{\\max} = 12$ 是一个最小值。\n连续最小值出现在整数值 $12$ 上，该值在定义域 $[2, 40]$ 内。最优整数值为 $d_{\\max}^{\\star} = 12$。\n\n**3. 每次分裂的特征数 ($m_{\\mathrm{try}}$) 的优化**\n\n$m_{\\rm try}$ 的误差函数由下式给出：\n$$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{\\sqrt{p}}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{\\sqrt{p}}\\right)$$\n给定 $p = 36$，所以 $\\sqrt{p} = 6$。函数变为：\n$$E_{m}(m_{\\mathrm{try}}) = 0.13 + 0.005\\left(\\frac{6}{m_{\\mathrm{try}}} + \\frac{m_{\\mathrm{try}}}{6}\\right)$$\n$m_{\\mathrm{try}}$ 的定义域是 $[1, 36]$ 中的整数集合。\n我们计算其关于 $m_{\\mathrm{try}}$ 的一阶导数：\n$$\\frac{dE_m}{dm_{\\mathrm{try}}} = 0.005\\left(-\\frac{6}{m_{\\mathrm{try}}^2} + \\frac{1}{6}\\right)$$\n将导数设为零：\n$$\\frac{1}{6} = \\frac{6}{m_{\\mathrm{try}}^2} \\implies m_{\\mathrm{try}}^2 = 36$$\n求解 $m_{\\mathrm{try}}$：\n$$m_{\\mathrm{try}} = \\sqrt{36} = 6$$\n二阶导数为 $\\frac{d^2E_m}{dm_{\\mathrm{try}}^2} = 0.005\\left(\\frac{12}{m_{\\mathrm{try}}^3}\\right) = \\frac{0.06}{m_{\\mathrm{try}}^3}$，对于 $m_{\\mathrm{try}}  0$ 为正值。因此，$m_{\\mathrm{try}} = 6$ 是一个最小值。\n连续最小值出现在整数值 $6$ 上，该值在定义域 $[1, 36]$ 内。最优整数值为 $m_{\\mathrm{try}}^{\\star} = 6$。这个结果与设置 $m_{\\mathrm{try}} \\approx \\sqrt{p}$ 的常用启发式方法一致。\n\n最优的超参数配置为 $(n_{\\min}^{\\star}, d_{\\max}^{\\star}, m_{\\mathrm{try}}^{\\star})$。根据独立的最小化计算，最优值为 $(8, 12, 6)$。",
            "answer": "$$\\boxed{\\begin{pmatrix} 8  12  6 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Dropout是防止人工神经网络（ANN）过拟合的一种关键正则化技术。本练习旨在揭示其背后的数学原理，要求您从基本概率论出发，推导在训练期间随机“丢弃”神经元对网络输出的期望影响，并确定在推理阶段如何调整权重以补偿这一过程。通过这个实践，您将能够深入理解Dropout为何有效，以及如何在实践中正确地使用它。",
            "id": "3801078",
            "problem": "一个团队正在训练一个人工神经网络（ANN），用于遥感和环境建模应用中，从多光谱卫星图像进行土地覆盖分类。考虑一个具有$n$个单元的单隐藏层，对于给定的像素，该隐藏层产生一个确定性的激活向量$h \\in \\mathbb{R}^{n}$。在训练期间，该团队对这个隐藏层应用丢弃率为$p \\in (0,1)$的dropout，其中每个单元以概率$1-p$被独立保留，以概率$p$被丢弃。下一层通过一个权重为$w \\in \\mathbb{R}^{n}$的线性映射计算一个标量预激活值，因此带有dropout的标量预激活值为$y = w^{\\top}(m \\odot h)$，其中$m \\in \\{0,1\\}^{n}$是随机的dropout掩码，$\\odot$表示哈达玛（Hadamard）积（逐元素积）。\n\n仅从以下基本事实出发：独立保留指示器的伯努利模型、期望的线性性以及哈达玛积的定义，推导以下两个量：\n- 在丢弃率$p$下，隐藏层中保留单元的期望数量。\n- 在推断时（无dropout）统一应用于$w$所有分量的一个标量权重缩放因子$s$，该因子对于任何确定性的$h$和任何$w$都保持标量预激活值$y$在训练时的均值。\n\n假设dropout掩码分量$m_{i}$是参数为$1-p$的独立同分布伯努利随机变量，并且$m$与$h$无关。将您的最终结果表示为封闭形式的解析表达式。不需要数值近似。最终答案必须是一个单行矩阵，按顺序包含期望计数和缩放因子。",
            "solution": "问题陈述已经过验证，被认为是合理的。它在科学上基于概率论和机器学习的原理，问题提出得很好，目标明确，信息充分，并使用了精确、客观的语言。该问题是与神经网络中dropout技术相关的一个标准的、可形式化的推导。因此，将提供一个解决方案。\n\n问题要求从第一性原理推导两个量：在dropout期间隐藏层中保留单元的期望数量，以及一个在推断时保持平均输出的权重缩放因子。\n\n设隐藏层有$n$个单元。Dropout掩码是一个随机向量$m \\in \\{0, 1\\}^n$，其中每个分量$m_i$（对于$i \\in \\{1, 2, \\dots, n\\}$）是一个独立同分布（i.i.d.）的伯努利随机变量。如果$m_i = 1$，则单元$i$被保留；如果$m_i = 0$，则被丢弃。问题陈述每个单元的保留概率为$1-p$，其中$p \\in (0,1)$是丢弃率。因此，$m_i \\sim \\text{Bernoulli}(1-p)$。\n\n$m_i$的概率质量函数为：\n$P(m_i = 1) = 1-p$\n$P(m_i = 0) = p$\n\n参数为$\\theta$的伯努利随机变量的期望是$\\theta$。因此，每个$m_i$的期望是：\n$$E[m_i] = 1 \\cdot P(m_i=1) + 0 \\cdot P(m_i=0) = 1 \\cdot (1-p) + 0 \\cdot p = 1-p$$\n\n**第1部分：保留单元的期望数量**\n\n设$N_{retained}$是表示隐藏层中保留单元总数的随机变量。如果一个单元对应的掩码元素为$1$，则该单元被保留。保留单元的总数是掩码向量$m$中指示器的总和：\n$$N_{retained} = \\sum_{i=1}^{n} m_i$$\n\n为了求出保留单元的期望数量，我们取$N_{retained}$的期望。根据给定的期望线性性原理，我们可以写出：\n$$E[N_{retained}] = E\\left[\\sum_{i=1}^{n} m_i\\right] = \\sum_{i=1}^{n} E[m_i]$$\n\n如前所述，$E[m_i] = 1-p$对于所有$i \\in \\{1, 2, \\dots, n\\}$成立。将此代入求和式中：\n$$E[N_{retained}] = \\sum_{i=1}^{n} (1-p)$$\n\n由于$(1-p)$是相对于求和索引$i$的常数，我们将这个常数相加$n$次：\n$$E[N_{retained}] = n(1-p)$$\n这就是隐藏层中保留单元的期望数量。\n\n**第2部分：推断权重缩放因子**\n\n在训练时，标量预激活值$y$是使用dropout掩码$m$计算的：\n$$y = w^{\\top}(m \\odot h)$$\n其中$w \\in \\mathbb{R}^{n}$是权重向量，$h \\in \\mathbb{R}^{n}$是确定性激活向量，$\\odot$是哈达玛（Hadamard）积（逐元素积）。展开后，表达式为：\n$$y = \\sum_{i=1}^{n} w_i (m_i h_i)$$\n\n我们需要找到一个缩放因子$s$，使得推断时的输出（无dropout）等于训练时输出的*均值*（期望值）。首先，我们计算训练时$y$的期望值，记为$E[y]$。\n$$E[y] = E\\left[\\sum_{i=1}^{n} w_i m_i h_i\\right]$$\n\n使用期望的线性性：\n$$E[y] = \\sum_{i=1}^{n} E[w_i m_i h_i]$$\n\n权重向量$w$和激活向量$h$对于特定输入是确定性的。Dropout掩码$m$与$h$无关。因此，$w_i$和$h_i$可以被视为关于期望算子的常数，该算子对$m$的分布进行平均。\n$$E[y] = \\sum_{i=1}^{n} w_i h_i E[m_i]$$\n\n代入$E[m_i] = 1-p$：\n$$E[y] = \\sum_{i=1}^{n} w_i h_i (1-p) = (1-p) \\sum_{i=1}^{n} w_i h_i$$\n\n求和项$\\sum_{i=1}^{n} w_i h_i$是点积$w^{\\top}h$的定义。所以，训练时的平均预激活值为：\n$$E[y] = (1-p) w^{\\top}h$$\n\n现在，考虑推断时间。在推断时，dropout被关闭，意味着所有隐藏单元都被保留（实际上，$m$是一个全为1的向量）。问题规定权重$w$要被一个统一的标量因子$s$缩放。得到的推断时预激活值，我们称之为$y_{inf}$，是：\n$$y_{inf} = (s w)^{\\top} h = s (w^{\\top} h)$$\n\n条件是推断时的输出必须对于任何$h$和$w$都保持训练时的均值。我们令两个表达式相等：\n$$y_{inf} = E[y]$$\n$$s (w^{\\top}h) = (1-p) w^{\\top}h$$\n\n这个等式必须对任何$w \\in \\mathbb{R}^{n}$和$h \\in \\mathbb{R}^{n}$的选择都成立。我们可以选择$w$和$h$使得它们的点积$w^{\\top}h \\neq 0$。对于任何这样的选择，我们可以在等式两边同时除以$w^{\\top}h$，得到：\n$$s = 1-p$$\n\n这个标量因子$s = 1-p$在推断时应用于权重，以确保每个神经元的期望输出在训练和测试之间保持一致，这种技术通常被称为测试时权重缩放。\n\n所需的两个量是保留单元的期望数量$n(1-p)$和缩放因子$1-p$。",
            "answer": "$$\\boxed{\\begin{pmatrix} n(1-p)  1-p \\end{pmatrix}}$$"
        }
    ]
}