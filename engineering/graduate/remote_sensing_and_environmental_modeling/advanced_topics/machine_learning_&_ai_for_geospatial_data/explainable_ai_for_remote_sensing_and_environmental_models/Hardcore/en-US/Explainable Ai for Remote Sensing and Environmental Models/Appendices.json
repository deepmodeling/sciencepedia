{
    "hands_on_practices": [
        {
            "introduction": "A fundamental step in building any predictive model is understanding which input features are most informative. This practice introduces a powerful, model-agnostic technique to answer this question by applying principles from information theory. You will calculate the mutual information between a spectral band and land cover classes, providing a formal way to quantify feature relevance before a model is even built .",
            "id": "3811303",
            "problem": "A satellite-based land cover classifier uses a single discretized Near-Infrared (NIR) spectral band as a feature, quantified into three bins $X \\in \\{L, M, H\\}$ corresponding to low, medium, and high reflectance. The target land cover class is $Y \\in \\{F, W, U\\}$ with $F$ for forest, $W$ for water, and $U$ for urban. The following toy dataset comprises $N = 300$ pixels with empirically observed joint counts $n_{x,y}$ between $X$ and $Y$:\n- $n_{L,W} = 80$, $n_{L,F} = 10$, $n_{L,U} = 20$,\n- $n_{M,W} = 18$, $n_{M,F} = 30$, $n_{M,U} = 45$,\n- $n_{H,W} = 2$, $n_{H,F} = 80$, $n_{H,U} = 15$.\nStarting from foundational probability and information-theoretic definitions (without invoking any pre-derived shortcuts), use the empirical joint distribution $p(x,y)$ and the associated marginals $p(x)$ and $p(y)$ induced by the counts above to derive and compute the mutual information $I(X;Y)$ using the natural logarithm $\\ln$. Express your final numerical result in nats, and round your answer to four significant figures. Briefly articulate the implications of the computed value for Explainable Artificial Intelligence (XAI) feature selection in remote sensing, focusing on how $I(X;Y)$ informs the relevance of this NIR band for land cover classification.",
            "solution": "The user wants to compute the mutual information between a feature and a target class and discuss its implications.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   Feature set: $X \\in \\{L, M, H\\}$ (Low, Medium, High NIR reflectance)\n-   Target class set: $Y \\in \\{F, W, U\\}$ (Forest, Water, Urban)\n-   Total number of samples: $N = 300$\n-   Joint counts $n_{x,y}$:\n    -   $n_{L,W} = 80$\n    -   $n_{L,F} = 10$\n    -   $n_{L,U} = 20$\n    -   $n_{M,W} = 18$\n    -   $n_{M,F} = 30$\n    -   $n_{M,U} = 45$\n    -   $n_{H,W} = 2$\n    -   $n_{H,F} = 80$\n    -   $n_{H,U} = 15$\n-   Task: Calculate mutual information $I(X;Y)$ using the natural logarithm $\\ln$.\n-   Task: Round the numerical result to four significant figures.\n-   Task: Articulate the implications of the result for Explainable Artificial Intelligence (XAI) in remote sensing.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientific Grounding**: The problem is scientifically grounded. It applies fundamental concepts from information theory (mutual information) to a standard problem in remote sensing and machine learning (feature relevance analysis for land cover classification). The physical premise that different land covers have different NIR reflectances is correct (e.g., water absorbs NIR, hence low reflectance; healthy vegetation reflects NIR strongly, hence high reflectance).\n-   **Well-Posedness**: The problem is well-posed. It provides a complete set of joint counts for the two variables, which is sufficient to determine the joint and marginal probability distributions required for the calculation. The total count is consistent: $\\sum n_{x,y} = 80+10+20+18+30+45+2+80+15 = 300 = N$. A unique numerical solution exists.\n-   **Objectivity**: The problem is stated using precise, objective, and quantitative language.\n\nThe problem is free of any scientific or factual unsoundness, incompleteness, contradictions, or ambiguity. It represents a standard, formalizable exercise in applied information theory.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe mutual information $I(X;Y)$ between two discrete random variables $X$ and $Y$ is defined as the reduction in uncertainty about one variable from knowing the other. It is formally expressed as:\n$$I(X;Y) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x,y) \\log \\left( \\frac{p(x,y)}{p(x)p(y)} \\right)$$\nThe problem specifies the use of the natural logarithm, $\\ln$, so the result will be in units of nats.\n\nFirst, we must compute the empirical joint and marginal probabilities from the given counts. The total number of pixels is $N=300$.\n\n**1. Joint Probabilities $p(x,y)$**\nThe joint probability $p(x,y)$ is the ratio of the joint count $n_{x,y}$ to the total count $N$.\n$p(x,y) = \\frac{n_{x,y}}{N} = \\frac{n_{x,y}}{300}$\nThe matrix of joint probabilities is:\n-   $p(L,W) = \\frac{80}{300}$\n-   $p(L,F) = \\frac{10}{300}$\n-   $p(L,U) = \\frac{20}{300}$\n-   $p(M,W) = \\frac{18}{300}$\n-   $p(M,F) = \\frac{30}{300}$\n-   $p(M,U) = \\frac{45}{300}$\n-   $p(H,W) = \\frac{2}{300}$\n-   $p(H,F) = \\frac{80}{300}$\n-   $p(H,U) = \\frac{15}{300}$\n\n**2. Marginal Probabilities $p(x)$ and $p(y)$**\nThe marginal probabilities are found by summing the joint probabilities over the other variable.\n$p(x) = \\sum_{y \\in Y} p(x,y)$\n-   $p(L) = p(L,W) + p(L,F) + p(L,U) = \\frac{80+10+20}{300} = \\frac{110}{300}$\n-   $p(M) = p(M,W) + p(M,F) + p(M,U) = \\frac{18+30+45}{300} = \\frac{93}{300}$\n-   $p(H) = p(H,W) + p(H,F) + p(H,U) = \\frac{2+80+15}{300} = \\frac{97}{300}$\n\n$p(y) = \\sum_{x \\in X} p(x,y)$\n-   $p(W) = p(L,W) + p(M,W) + p(H,W) = \\frac{80+18+2}{300} = \\frac{100}{300}$\n-   $p(F) = p(L,F) + p(M,F) + p(H,F) = \\frac{10+30+80}{300} = \\frac{120}{300}$\n-   $p(U) = p(L,U) + p(M,U) + p(H,U) = \\frac{20+45+15}{300} = \\frac{80}{300}$\n\n**3. Compute Mutual Information $I(X;Y)$**\nNow we substitute these probabilities into the mutual information formula. The term $\\frac{p(x,y)}{p(x)p(y)}$ simplifies to $\\frac{n_{x,y}/N}{(n_x/N)(n_y/N)} = \\frac{n_{x,y} N}{n_x n_y}$, where $n_x = \\sum_y n_{x,y}$ and $n_y = \\sum_x n_{x,y}$.\n$I(X;Y)$ is the sum of $9$ terms of the form $p(x,y) \\ln \\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$:\n\n$I(X;Y) = $\n$\\frac{80}{300} \\ln\\left(\\frac{80/300}{(110/300)(100/300)}\\right) + \\frac{10}{300} \\ln\\left(\\frac{10/300}{(110/300)(120/300)}\\right) + \\frac{20}{300} \\ln\\left(\\frac{20/300}{(110/300)(80/300)}\\right) +$\n$\\frac{18}{300} \\ln\\left(\\frac{18/300}{(93/300)(100/300)}\\right) + \\frac{30}{300} \\ln\\left(\\frac{30/300}{(93/300)(120/300)}\\right) + \\frac{45}{300} \\ln\\left(\\frac{45/300}{(93/300)(80/300)}\\right) +$\n$\\frac{2}{300} \\ln\\left(\\frac{2/300}{(97/300)(100/300)}\\right) + \\frac{80}{300} \\ln\\left(\\frac{80/300}{(97/300)(120/300)}\\right) + \\frac{15}{300} \\ln\\left(\\frac{15/300}{(97/300)(80/300)}\\right)$\n\nSimplifying the arguments of the logarithms:\n$I(X;Y) = $\n$\\frac{80}{300} \\ln\\left(\\frac{80 \\cdot 300}{110 \\cdot 100}\\right) + \\frac{10}{300} \\ln\\left(\\frac{10 \\cdot 300}{110 \\cdot 120}\\right) + \\frac{20}{300} \\ln\\left(\\frac{20 \\cdot 300}{110 \\cdot 80}\\right) +$\n$\\frac{18}{300} \\ln\\left(\\frac{18 \\cdot 300}{93 \\cdot 100}\\right) + \\frac{30}{300} \\ln\\left(\\frac{30 \\cdot 300}{93 \\cdot 120}\\right) + \\frac{45}{300} \\ln\\left(\\frac{45 \\cdot 300}{93 \\cdot 80}\\right) +$\n$\\frac{2}{300} \\ln\\left(\\frac{2 \\cdot 300}{97 \\cdot 100}\\right) + \\frac{80}{300} \\ln\\left(\\frac{80 \\cdot 300}{97 \\cdot 120}\\right) + \\frac{15}{300} \\ln\\left(\\frac{15 \\cdot 300}{97 \\cdot 80}\\right)$\n\nPerforming the numerical calculations:\n$I(X;Y) = \\frac{1}{300} [ 80 \\ln(2.1818...) + 10 \\ln(0.2272...) + 20 \\ln(0.6818...) +$\n$18 \\ln(0.5806...) + 30 \\ln(0.8064...) + 45 \\ln(1.8145...) +$\n$2 \\ln(0.0618...) + 80 \\ln(2.0618...) + 15 \\ln(0.5800...) ]$\n\n$I(X;Y) \\approx \\frac{1}{300} [ 80(0.78013) + 10(-1.48160) + 20(-0.38289) +$\n$18(-0.54363) + 30(-0.21506) + 45(0.59579) +$\n$2(-2.78290) + 80(0.72363) + 15(-0.54472) ]$\n\n$I(X;Y) \\approx \\frac{1}{300} [ 62.410 - 14.816 - 7.658 - 9.785 - 6.452 + 26.811 - 5.566 + 57.890 - 8.171 ]$\n$I(X;Y) \\approx \\frac{94.663}{300} \\approx 0.315543...$\n\nRounding to four significant figures, we get $0.3155$.\n\n**4. Implications for XAI in Remote Sensing**\n\nThe calculated mutual information, $I(X;Y) \\approx 0.3155$ nats, is a quantitative measure of the statistical dependency between the discretized NIR band ($X$) and the land cover class ($Y$). In the context of Explainable AI (XAI) for feature selection, this value has direct and clear implications:\n\n-   **Quantification of Feature Relevance**: A mutual information of zero would imply that the feature and target are statistically independent, meaning the NIR band offers no information for distinguishing between forest, water, and urban classes. Since $I(X;Y) > 0$, we can state with formal rigor that the NIR band is a relevant feature for this classification task.\n\n-   **Model-Agnostic Explainability**: This calculation is performed on the data distribution itself, independent of any specific machine learning model (e.g., a neural network, a random forest). This provides a fundamental, model-agnostic explanation for the feature's utility. For XAI purposes, this is a powerful statement: the NIR band is inherently informative according to information theory, not just because a complex \"black box\" model happens to use it.\n\n-   **Basis for Feature Selection and Ranking**: In a real-world remote sensing scenario with dozens or hundreds of spectral bands, one could compute $I(X_i; Y)$ for each band $X_i$. These values would allow for a principled ranking of all available features. Bands with higher mutual information would be considered more important. This ranking provides a simple, transparent, and justifiable method for feature selection, a core goal of XAI. For instance, if another spectral band had a mutual information of $0.1$ nats, we could objectively state that this NIR band is approximately three times more informative. The non-zero value of $0.3155$ nats confirms that this NIR band is a valuable candidate for inclusion in a predictive model.",
            "answer": "$$\\boxed{0.3155}$$"
        },
        {
            "introduction": "Beyond general feature relevance, explaining individual predictions is a core goal of XAI. This exercise delves into SHAP (SHapley Additive exPlanations), one of the most rigorous attribution methods, by focusing on a critical nuance: the handling of correlated features common in remote sensing data. By deriving SHAP values under different background assumptions, you will uncover how feature dependencies can profoundly alter explanations, a key insight for producing trustworthy results .",
            "id": "3811343",
            "problem": "Consider a linear pixel-level model in remote sensing that estimates a vegetation-related index from two Sentinel-2 spectral bands, where $X_1$ is the reflectance in band B4 (red) and $X_2$ is the reflectance in band B8 (near-infrared), with the prediction given by $f(x) = w^{\\top} x + b$. The explanatory framework is SHapley Additive exPlanations (SHAP), which assigns feature-wise contributions that sum to the difference $f(x) - \\mathbb{E}[f(X)]$ for a specified background distribution. Two background distributions are considered: the interventional background that treats inputs as independent, and the conditional background that preserves the statistical dependence structure among inputs.\n\nAssume the following scientifically plausible setup for a vegetated area in a temperate biome:\n- The random vector $(X_1, X_2)$ follows a bivariate normal distribution with mean $\\mu = (\\mu_1, \\mu_2) = (0.12, 0.35)$, standard deviations $\\sigma_1 = 0.05$ and $\\sigma_2 = 0.10$, and correlation $\\rho = -0.6$. The covariance matrix is therefore\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_1^2 & \\rho \\, \\sigma_1 \\sigma_2 \\\\\n\\rho \\, \\sigma_1 \\sigma_2 & \\sigma_2^2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.05^2 & -0.6 \\cdot 0.05 \\cdot 0.10 \\\\\n-0.6 \\cdot 0.05 \\cdot 0.10 & 0.10^2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.0025 & -0.003 \\\\\n-0.003 & 0.01\n\\end{pmatrix}.\n$$\n- The instance of interest (a single pixel) is $x = (x_1, x_2) = (0.08, 0.60)$.\n- The linear model parameters are $w = (w_1, w_2) = (-0.8, 1.0)$ and $b = 0.05$.\n\nStarting from first principles:\n1. Use the Shapley value definition from cooperative game theory and the law of total expectation to derive closed-form expressions for the SHAP values under the interventional background, $\\phi_i^{\\mathrm{int}}$, for $i \\in \\{1,2\\}$.\n2. Use the Shapley value definition and the conditional expectation formula for a multivariate normal distribution to derive closed-form expressions for the SHAP values under the conditional background, $\\phi_i^{\\mathrm{cond}}$, for $i \\in \\{1,2\\}$.\n\nThen, for the specified parameters and instance, compute the numerical values of $\\phi_1^{\\mathrm{int}}$ and $\\phi_1^{\\mathrm{cond}}$, and report the ratio\n$$\nr \\;=\\; \\frac{\\phi_1^{\\mathrm{cond}}}{\\phi_1^{\\mathrm{int}}}.\n$$\nExpress the final result for $r$ as a real number with no units and round your answer to four significant figures.",
            "solution": "The problem requires the derivation of closed-form expressions for SHAP values under two different background distribution assumptions (interventional and conditional) for a linear model, and then the calculation of the ratio of these values for a specific feature.\n\nThe model is linear: $f(x) = w^{\\top}x + b$, where $x = (x_1, x_2)$ and $w = (w_1, w_2)$.\nThe SHAP value for feature $i$ in a set of features $N$ is defined as:\n$$\n\\phi_i(f, x) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} [v(S \\cup \\{i\\}) - v(S)]\n$$\nwhere $v(S) = \\mathbb{E}[f(X) | X_S = x_S]$. This is the expected model output given that the features in subset $S$ are fixed to their values from the instance $x$. For this problem, the set of features is $N = \\{1, 2\\}$, so $|N|=2$. The formula simplifies to:\n$$\n\\phi_i(f, x) = \\frac{0! (2-0-1)!}{2!} [v(\\{i\\}) - v(\\emptyset)] + \\frac{1! (2-1-1)!}{2!} [v(N) - v(N \\setminus \\{i\\})]\n$$\n$$\n\\phi_i(f, x) = \\frac{1}{2} [ (v(\\{i\\}) - v(\\emptyset)) + (v(\\{1, 2\\}) - v(\\{j\\})) ] \\quad \\text{for } j \\neq i\n$$\nThe value functions are:\n$v(\\emptyset) = \\mathbb{E}[f(X)]$.\n$v(\\{1\\}) = \\mathbb{E}[f(X) | X_1=x_1]$.\n$v(\\{2\\}) = \\mathbb{E}[f(X) | X_2=x_2]$.\n$v(\\{1, 2\\}) = \\mathbb{E}[f(X) | X_1=x_1, X_2=x_2] = f(x_1, x_2)$.\n\nThe two background assumptions differ in how they define the expectation $\\mathbb{E}[\\cdot]$.\n\n**1. Derivation of Interventional SHAP values ($\\phi_i^{\\mathrm{int}}$)**\n\nUnder the interventional background, features are treated as independent. The expectation over features not in $S$ is taken with respect to their marginal distributions. The model is $f(X) = w_1 X_1 + w_2 X_2 + b$.\n\n$v(\\emptyset) = \\mathbb{E}[f(X)] = w_1 \\mathbb{E}[X_1] + w_2 \\mathbb{E}[X_2] + b = w_1 \\mu_1 + w_2 \\mu_2 + b$.\n$v(\\{1\\}) = \\mathbb{E}[w_1 x_1 + w_2 X_2 + b] = w_1 x_1 + w_2 \\mathbb{E}[X_2] + b = w_1 x_1 + w_2 \\mu_2 + b$.\n$v(\\{2\\}) = \\mathbb{E}[w_1 X_1 + w_2 x_2 + b] = w_1 \\mathbb{E}[X_1] + w_2 x_2 + b = w_1 \\mu_1 + w_2 x_2 + b$.\n$v(\\{1, 2\\}) = f(x) = w_1 x_1 + w_2 x_2 + b$.\n\nThe marginal contributions for feature $1$ are:\n$v(\\{1\\}) - v(\\emptyset) = (w_1 x_1 + w_2 \\mu_2 + b) - (w_1 \\mu_1 + w_2 \\mu_2 + b) = w_1(x_1 - \\mu_1)$.\n$v(\\{1, 2\\}) - v(\\{2\\}) = (w_1 x_1 + w_2 x_2 + b) - (w_1 \\mu_1 + w_2 x_2 + b) = w_1(x_1 - \\mu_1)$.\n\nThe interventional SHAP value for feature $1$ is the average of these contributions:\n$$\n\\phi_1^{\\mathrm{int}} = \\frac{1}{2} [w_1(x_1 - \\mu_1) + w_1(x_1 - \\mu_1)] = w_1(x_1 - \\mu_1)\n$$\nBy symmetry, the general closed-form expression for feature $i$ is:\n$$\n\\phi_i^{\\mathrm{int}} = w_i(x_i - \\mu_i)\n$$\n\n**2. Derivation of Conditional SHAP values ($\\phi_i^{\\mathrm{cond}}$)**\n\nUnder the conditional background, the expectation preserves the statistical dependence between features. The expectation over features not in $S$ is taken with respect to the conditional probability distribution $p(X_{N \\setminus S} | X_S = x_S)$.\nFor a bivariate normal distribution $(X_1, X_2)$, the conditional expectations are linear:\n$\\mathbb{E}[X_2 | X_1=x_1] = \\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1}(x_1 - \\mu_1)$.\n$\\mathbb{E}[X_1 | X_2=x_2] = \\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2)$.\n\nThe value functions are:\n$v(\\emptyset) = \\mathbb{E}[f(X)] = w_1 \\mu_1 + w_2 \\mu_2 + b$.\n$v(\\{1\\}) = \\mathbb{E}[f(X) | X_1=x_1] = w_1 x_1 + w_2 \\mathbb{E}[X_2 | X_1=x_1] + b = w_1 x_1 + w_2(\\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1}(x_1 - \\mu_1)) + b$.\n$v(\\{2\\}) = \\mathbb{E}[f(X) | X_2=x_2] = w_1 \\mathbb{E}[X_1 | X_2=x_2] + w_2 x_2 + b = w_1(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2)) + w_2 x_2 + b$.\n$v(\\{1, 2\\}) = f(x) = w_1 x_1 + w_2 x_2 + b$.\n\nThe marginal contributions for feature $1$ are:\nContribution when added first ($S=\\emptyset$):\n$v(\\{1\\}) - v(\\emptyset) = [w_1 x_1 + w_2(\\mu_2 + \\rho \\frac{\\sigma_2}{\\sigma_1}(x_1 - \\mu_1)) + b] - [w_1 \\mu_1 + w_2 \\mu_2 + b] = w_1(x_1 - \\mu_1) + w_2 \\rho \\frac{\\sigma_2}{\\sigma_1}(x_1 - \\mu_1)$.\nContribution when added second ($S=\\{2\\}$):\n$v(\\{1, 2\\}) - v(\\{2\\}) = [w_1 x_1 + w_2 x_2 + b] - [w_1(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2)) + w_2 x_2 + b] = w_1 x_1 - w_1(\\mu_1 + \\rho \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2)) = w_1(x_1 - \\mu_1) - w_1 \\rho \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2)$.\n\nThe conditional SHAP value for feature $1$ is the average of these contributions:\n$$\n\\phi_1^{\\mathrm{cond}} = \\frac{1}{2} \\left[ \\left(w_1(x_1 - \\mu_1) + w_2 \\rho \\frac{\\sigma_2}{\\sigma_1}(x_1 - \\mu_1)\\right) + \\left(w_1(x_1 - \\mu_1) - w_1 \\rho \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2)\\right) \\right]\n$$\n$$\n\\phi_1^{\\mathrm{cond}} = w_1(x_1 - \\mu_1) + \\frac{\\rho}{2} \\left[ w_2 \\frac{\\sigma_2}{\\sigma_1}(x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2) \\right]\n$$\nThis is the closed-form expression for $\\phi_1^{\\mathrm{cond}}$. A similar expression can be derived for $\\phi_2^{\\mathrm{cond}}$.\n\n**3. Numerical Computation**\n\nWe are given the following parameters:\n$w = (w_1, w_2) = (-0.8, 1.0)$\n$x = (x_1, x_2) = (0.08, 0.60)$\n$\\mu = (\\mu_1, \\mu_2) = (0.12, 0.35)$\n$\\sigma_1 = 0.05$, $\\sigma_2 = 0.10$\n$\\rho = -0.6$\n\nFirst, compute the differences from the mean:\n$x_1 - \\mu_1 = 0.08 - 0.12 = -0.04$\n$x_2 - \\mu_2 = 0.60 - 0.35 = 0.25$\n\nNext, compute $\\phi_1^{\\mathrm{int}}$:\n$$\n\\phi_1^{\\mathrm{int}} = w_1(x_1 - \\mu_1) = (-0.8)(-0.04) = 0.032\n$$\n\nNow, compute $\\phi_1^{\\mathrm{cond}}$. The first term is simply $\\phi_1^{\\mathrm{int}}$. The second term, which captures the effect of correlation, is:\n$$\n\\text{Correction Term} = \\frac{\\rho}{2} \\left[ w_2 \\frac{\\sigma_2}{\\sigma_1}(x_1 - \\mu_1) - w_1 \\frac{\\sigma_1}{\\sigma_2}(x_2 - \\mu_2) \\right]\n$$\nWe need the ratios of standard deviations:\n$\\frac{\\sigma_2}{\\sigma_1} = \\frac{0.10}{0.05} = 2$\n$\\frac{\\sigma_1}{\\sigma_2} = \\frac{0.05}{0.10} = 0.5$\n\nSubstituting the values into the correction term:\n$$\n\\text{Correction Term} = \\frac{-0.6}{2} \\left[ (1.0)(2)(-0.04) - (-0.8)(0.5)(0.25) \\right]\n$$\n$$\n= -0.3 \\left[ -0.08 - (-0.4)(0.25) \\right]\n$$\n$$\n= -0.3 \\left[ -0.08 + 0.1 \\right]\n$$\n$$\n= -0.3 [0.02] = -0.006\n$$\nNow, we find $\\phi_1^{\\mathrm{cond}}$:\n$$\n\\phi_1^{\\mathrm{cond}} = \\phi_1^{\\mathrm{int}} + \\text{Correction Term} = 0.032 - 0.006 = 0.026\n$$\n\nFinally, compute the ratio $r$:\n$$\nr = \\frac{\\phi_1^{\\mathrm{cond}}}{\\phi_1^{\\mathrm{int}}} = \\frac{0.026}{0.032} = \\frac{26}{32} = \\frac{13}{16} = 0.8125\n$$\nThe problem specifies rounding to four significant figures. The result $0.8125$ already has four significant figures.",
            "answer": "$$\\boxed{0.8125}$$"
        },
        {
            "introduction": "Modern deep learning models, especially those with attention mechanisms, often produce visually compelling maps that seem to explain their reasoning. This practice challenges you to critically evaluate such claims by constructing a counterexample where a model's 'attention' is misleading. Through a hands-on coding exercise, you will implement a formal faithfulness test to prove that a visually plausible explanation does not reflect the model's true reliance on a spurious artifact, a crucial skill for any practitioner aiming to move beyond superficial interpretations .",
            "id": "3811355",
            "problem": "You are asked to construct and analyze a synthetic counterexample, in the context of Explainable Artificial Intelligence (XAI) for remote sensing and environmental models, where visually plausible attention maps are not faithful to the true predictive mechanism of a classifier. The remote sensing motivation is a known edge artifact: a spuriously predictive border, such as a sensor frame or stitching seam, that correlates with a target label but does not reflect the underlying environmental phenomenon inside the scene. The goal is to derive, implement, and test a principled faithfulness criterion that demonstrates the failure of visually plausible attention.\n\nStart from the following fundamental bases and definitions. A binary classifier $f$ on an image $x \\in \\mathbb{R}^{H \\times W}$ with output $f(x) \\in [0,1]$ is defined by the logistic function\n$$\nf(x) \\triangleq \\sigma(z(x)) = \\frac{1}{1 + e^{-z(x)}},\n$$\nwhere the logit $z(x)$ is a linear function of two spatial feature sets: a border frame and an interior center disk. Let $\\mathcal{B}$ denote the set of border pixels of width $w_{\\mathrm{bord}}$ (all pixels whose minimum distance to the image boundary is strictly less than $w_{\\mathrm{bord}}$), and let $\\mathcal{C}$ denote the set of pixels inside a disk of radius $r$ centered at the image center. The logit is\n$$\nz(x) = \\alpha_b \\sum_{(i,j) \\in \\mathcal{B}} x_{ij} + \\alpha_c \\sum_{(i,j) \\in \\mathcal{C}} x_{ij} + \\beta,\n$$\nwith parameters $\\alpha_b \\in \\mathbb{R}$, $\\alpha_c \\in \\mathbb{R}$, and bias $\\beta \\in \\mathbb{R}$. The synthetic dataset is generated by composing three parts: a background with constant intensity, a center disk whose intensity reflects a plausible environmental signal (e.g., vegetation greenness), and a border frame that spuriously encodes the label. Precisely:\n- Background intensity is set to $\\mu_{\\mathrm{bg}}$ for all $(i,j) \\notin \\mathcal{C} \\cup \\mathcal{B}$.\n- Center disk intensity for $(i,j) \\in \\mathcal{C}$ is set to $\\mu_{y_{\\mathrm{true}}} + \\varepsilon_{ij}$, where $y_{\\mathrm{true}} \\in \\{0,1\\}$ is the underlying environmental label, $\\mu_{1} > \\mu_{0}$, and $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ is independent pixel noise.\n- Border intensity for $(i,j) \\in \\mathcal{B}$ is set to $s \\cdot y_{\\mathrm{spur}}$, where $y_{\\mathrm{spur}} \\in \\{0,1\\}$ is a spuriously predictive label; for this counterexample, take $y_{\\mathrm{spur}} = y_{\\mathrm{true}}$ so the border is perfectly predictive during training, even though it is non-causal.\n\nDefine a visually plausible attention map $A(x)$ that highlights the center disk by combining a radially symmetric Gaussian centered at the image midpoint with the local contrast relative to the background:\n$$\nG_{ij} = \\exp\\left(-\\frac{(i - i_0)^2 + (j - j_0)^2}{2 \\sigma_g^2}\\right), \\quad S_{ij} = \\left|x_{ij} - \\mu_{\\mathrm{bg}}\\right|, \\quad A_{ij}(x) = \\frac{G_{ij} \\cdot S_{ij}}{\\max_{u,v} (G_{uv} \\cdot S_{uv})},\n$$\nwhere $(i_0,j_0)$ is the image center and $\\sigma_g$ is a Gaussian width. This attention is visually plausible in environmental imaging, as it tends to highlight the central phenomenon, but it is not necessarily faithful to the classifier $f$ if $f$ relies on the border.\n\nUse the deletion-based faithfulness test. For a fraction $\\phi \\in (0,1)$ and a neutral reference intensity $\\rho$, define two perturbations:\n1. Attention deletion: set the top $\\phi \\cdot H \\cdot W$ pixels by $A_{ij}(x)$ to $\\rho$, yielding $x^{\\mathrm{att}}$; compute $\\Delta_{\\mathrm{att}} = f(x) - f(x^{\\mathrm{att}})$.\n2. Border deletion: set all $(i,j) \\in \\mathcal{B}$ to $\\rho$, yielding $x^{\\mathrm{bor}}$; compute $\\Delta_{\\mathrm{bor}} = f(x) - f(x^{\\mathrm{bor}})$.\n\nDefine the faithfulness decision $F$ as the boolean\n$$\nF = \\left[\\Delta_{\\mathrm{att}} \\ge \\Delta_{\\mathrm{bor}}\\right],\n$$\nwhere $[\\cdot]$ denotes the truth value. A faithful attention should produce a larger drop than deleting features that the model does not rely on; in this counterexample, we expect deletion of the spuriously predictive border to reduce the prediction more than deleting the attention-highlighted center, thus $F$ should be false in most cases.\n\nYour program must implement the synthetic generator, classifier, attention, and deletion tests exactly as defined, and evaluate the following test suite. For each case, use $y_{\\mathrm{true}} = 1$, $y_{\\mathrm{spur}} = 1$, $\\mu_{\\mathrm{bg}} = 0.5$, and $\\sigma_g = r/2$. In all cases, express intensities as unitless decimals.\n\nTest Suite (four cases):\n- Case $1$: $H = 32$, $W = 32$, $r = 8$, $\\mu_{1} = 0.8$, $\\mu_{0} = 0.2$, $\\sigma = 0.01$, $w_{\\mathrm{bord}} = 1$, $s = 1.0$, $\\alpha_b = 0.03$, $\\alpha_c = 0.0$, $\\beta = -1.0$, $\\phi = 0.15$, $\\rho = 0.5$.\n- Case $2$: $H = 32$, $W = 32$, $r = 8$, $\\mu_{1} = 0.8$, $\\mu_{0} = 0.2$, $\\sigma = 0.01$, $w_{\\mathrm{bord}} = 1$, $s = 0.5$, $\\alpha_b = 0.03$, $\\alpha_c = 0.0$, $\\beta = -1.0$, $\\phi = 0.15$, $\\rho = 0.5$.\n- Case $3$: $H = 32$, $W = 32$, $r = 8$, $\\mu_{1} = 0.8$, $\\mu_{0} = 0.2$, $\\sigma = 0.01$, $w_{\\mathrm{bord}} = 2$, $s = 1.0$, $\\alpha_b = 0.02$, $\\alpha_c = 0.0$, $\\beta = -1.0$, $\\phi = 0.15$, $\\rho = 0.5$.\n- Case $4$: $H = 32$, $W = 32$, $r = 8$, $\\mu_{1} = 0.8$, $\\mu_{0} = 0.2$, $\\sigma = 0.01$, $w_{\\mathrm{bord}} = 1$, $s = 0.1$, $\\alpha_b = 0.03$, $\\alpha_c = 0.0$, $\\beta = -1.0$, $\\phi = 0.15$, $\\rho = 0.5$.\n\nYour program should produce a single line of output containing the faithfulness decisions for the four cases as a comma-separated list enclosed in square brackets, e.g., $[F_1,F_2,F_3,F_4]$, where each $F_k$ is the boolean result for case $k$.",
            "solution": "The user wishes to evaluate a faithfulness criterion for an Explainable AI (XAI) method in a synthetic remote sensing context. The problem is designed as a counterexample where a visually plausible attention map is not faithful to the model's predictive mechanism. The model is a binary classifier trained on images with two distinct feature regions: a central disk and a border frame. It is explicitly constructed to rely solely on the border artifact, which is spuriously correlated with the label, while ignoring the physically meaningful signal in the center.\n\nThe solution proceeds by first implementing the required components as defined in the problem statement: the synthetic image generator, the classifier, the attention map generator, and the faithfulness test.\n\n**1. Preliminaries and Definitions**\n\nWe are given an image $x \\in \\mathbb{R}^{H \\times W}$.\nThe classifier is defined by the logistic function $f(x) = \\sigma(z(x)) = (1 + e^{-z(x)})^{-1}$, where $z(x)$ is the logit.\n\nThe image space is partitioned into three sets of pixel indices $(i,j)$:\n- The border frame, $\\mathcal{B}$, contains pixels whose minimum distance to the image boundary is less than $w_{\\mathrm{bord}}$. For a grid from $(0,0)$ to $(H-1, W-1)$, a pixel $(i,j)$ is in $\\mathcal{B}$ if $\\min(i, j, H-1-i, W-1-j) < w_{\\mathrm{bord}}$.\n- The center disk, $\\mathcal{C}$, contains pixels within a radius $r$ of the image center $(i_0, j_0) = (\\frac{H-1}{2}, \\frac{W-1}{2})$. A pixel $(i,j)$ is in $\\mathcal{C}$ if $\\sqrt{(i-i_0)^2 + (j-j_0)^2} \\le r$.\n- The background, which comprises all other pixels.\n\nThe logit $z(x)$ is a linear combination of pixel intensities within these regions:\n$$\nz(x) = \\alpha_b \\sum_{(i,j) \\in \\mathcal{B}} x_{ij} + \\alpha_c \\sum_{(i,j) \\in \\mathcal{C}} x_{ij} + \\beta\n$$\nCrucially, for all test cases, the weight for the center region is set to $\\alpha_c = 0.0$. This makes the classifier $f(x)$ entirely dependent on the border pixels and completely insensitive to the center disk's content.\n\n**2. Synthetic Image Generation**\n\nA synthetic image $x$ is generated based on a true environmental label $y_{\\mathrm{true}}$ and a spurious label $y_{\\mathrm{spur}}$. For this problem, we are given $y_{\\mathrm{spur}} = y_{\\mathrm{true}}$. The pixel intensities $x_{ij}$ are set as follows:\n- For $(i,j) \\in \\mathcal{B}$ (border): $x_{ij} = s \\cdot y_{\\mathrm{spur}}$.\n- For $(i,j) \\in \\mathcal{C}$ (center): $x_{ij} = \\mu_{y_{\\mathrm{true}}} + \\varepsilon_{ij}$, where $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2)$ is Gaussian noise.\n- For all other pixels (background): $x_{ij} = \\mu_{\\mathrm{bg}}$.\n\nAll test cases use $y_{\\mathrm{true}}=1$, so the border intensity is $s$ and the center intensity is approximately $\\mu_1$.\n\n**3. Visually Plausible Attention Map**\n\nAn attention map $A(x)$ is defined to be visually plausible, highlighting the central feature. It combines a radially symmetric Gaussian $G$ with a local contrast measure $S$:\n$$\nG_{ij} = \\exp\\left(-\\frac{(i - i0)^2 + (j - j0)^2}{2 \\sigma_g^2}\\right) \\quad \\text{with } \\sigma_g = r/2\n$$\n$$\nS_{ij} = \\left|x_{ij} - \\mu_{\\mathrm{bg}}\\right|\n$$\nThe normalized attention map is $A_{ij}(x) = (G_{ij} \\cdot S_{ij}) / \\max_{u,v} (G_{uv} \\cdot S_{uv})$.\nThis formulation ensures high attention values in the center disk, where both $G_{ij}$ and $S_{ij} = |\\mu_1 + \\varepsilon_{ij} - \\mu_{\\mathrm{bg}}|$ are large, and low values at the border, where $G_{ij}$ is very small due to the distance from the center.\n\n**4. Faithfulness Evaluation using Deletion Tests**\n\nThe faithfulness of the attention map $A(x)$ is evaluated by comparing the effect of two different image perturbations on the classifier's output. A neutral reference intensity $\\rho$ is used to replace pixels.\n\n- **Attention Deletion**: A new image $x^{\\mathrm{att}}$ is created by setting the top $\\phi \\cdot H \\cdot W$ pixels with the highest attention values in $A(x)$ to $\\rho$. The change in the classifier's output is $\\Delta_{\\mathrm{att}} = f(x) - f(x^{\\mathrm{att}})$. Since the attention map $A(x)$ highlights the central disk and $\\alpha_c=0$, the classifier's logit is invariant to these changes, i.e., $z(x^{\\mathrm{att}}) = z(x)$. This is because the deleted pixels are not in the border set $\\mathcal{B}$. Therefore, $\\Delta_{\\mathrm{att}} = 0$ for all test cases.\n\n- **Border Deletion**: A new image $x^{\\mathrm{bor}}$ is created by setting all pixels in the border set $\\mathcal{B}$ to $\\rho$. The change in output is $\\Delta_{\\mathrm{bor}} = f(x) - f(x^{\\mathrm{bor}})$. The logit changes from $z(x) = \\alpha_b \\sum_{\\mathcal{B}} x_{ij} + \\beta$ to $z(x^{\\mathrm{bor}}) = \\alpha_b \\sum_{\\mathcal{B}} \\rho + \\beta$. Unless the original border intensity $s$ equals $\\rho$, $\\Delta_{\\mathrm{bor}}$ will be non-zero.\n\nThe faithfulness decision $F$ is the truth value of the comparison $\\Delta_{\\mathrm{att}} \\ge \\Delta_{\\mathrm{bor}}$. Substituting $\\Delta_{\\mathrm{att}}=0$, this simplifies to $F = [0 \\ge \\Delta_{\\mathrm{bor}}]$. An attention map is considered faithful if deleting its highlighted features has a greater or equal impact than deleting other features. In our counterexample, we expect this to be false, demonstrating the failure of the visually plausible attention map.\n\n**5. Analysis of Test Cases**\n\n- **Case 1**: $s=1.0, \\rho=0.5, \\alpha_b>0$. The border pixel values ($1.0$) are replaced with a lower value ($0.5$). This decreases the logit $z(x)$, so $f(x) > f(x^{\\mathrm{bor}})$ and $\\Delta_{\\mathrm{bor}} > 0$. The decision $F = [0 \\ge \\Delta_{\\mathrm{bor}}]$ is `False`.\n- **Case 2**: $s=0.5, \\rho=0.5$. The border pixel values are replaced with the same value. Thus, $x = x^{\\mathrm{bor}}$, so $\\Delta_{\\mathrm{bor}} = 0$. The decision $F = [0 \\ge 0]$ is `True`.\n- **Case 3**: $s=1.0, \\rho=0.5, \\alpha_b>0$. Similar to Case 1, $\\Delta_{\\mathrm{bor}} > 0$. The decision $F$ is `False`.\n- **Case 4**: $s=0.1, \\rho=0.5, \\alpha_b>0$. The border pixel values ($0.1$) are replaced with a higher value ($0.5$). This increases the logit $z(x)$, so $f(x) < f(x^{\\mathrm{bor}})$ and $\\Delta_{\\mathrm{bor}} < 0$. The decision $F = [0 \\ge \\Delta_{\\mathrm{bor}}]$ is `True`, as $0$ is greater than a negative number.\n\nThe results confirm that the faithfulness test correctly identifies the model's reliance on the border. When removing the border has a significant effect ($\\Delta_{\\mathrm{bor}} \\ne 0$), the visually plausible attention fails the test (results are `False` or `True` depending on the sign of $\\Delta_{\\mathrm{bor}}$), except in the degenerate case where the border intensity matches the replacement value.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a faithfulness criterion for a synthetic XAI counterexample\n    in a remote sensing context.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'H': 32, 'W': 32, 'r': 8, 'mu_1': 0.8, 'mu_0': 0.2, 'sigma': 0.01,\n         'w_bord': 1, 's': 1.0, 'alpha_b': 0.03, 'alpha_c': 0.0, 'beta': -1.0,\n         'phi': 0.15, 'rho': 0.5},\n        # Case 2\n        {'H': 32, 'W': 32, 'r': 8, 'mu_1': 0.8, 'mu_0': 0.2, 'sigma': 0.01,\n         'w_bord': 1, 's': 0.5, 'alpha_b': 0.03, 'alpha_c': 0.0, 'beta': -1.0,\n         'phi': 0.15, 'rho': 0.5},\n        # Case 3\n        {'H': 32, 'W': 32, 'r': 8, 'mu_1': 0.8, 'mu_0': 0.2, 'sigma': 0.01,\n         'w_bord': 2, 's': 1.0, 'alpha_b': 0.02, 'alpha_c': 0.0, 'beta': -1.0,\n         'phi': 0.15, 'rho': 0.5},\n        # Case 4\n        {'H': 32, 'W': 32, 'r': 8, 'mu_1': 0.8, 'mu_0': 0.2, 'sigma': 0.01,\n         'w_bord': 1, 's': 0.1, 'alpha_b': 0.03, 'alpha_c': 0.0, 'beta': -1.0,\n         'phi': 0.15, 'rho': 0.5},\n    ]\n\n    # Common parameters across all cases\n    y_true = 1\n    y_spur = 1\n    mu_bg = 0.5\n\n    results = []\n    np.random.seed(0)  # For reproducible noise generation\n\n    for params in test_cases:\n        H, W, r = params['H'], params['W'], params['r']\n        mu_1, sigma = params['mu_1'], params['sigma']\n        w_bord, s = params['w_bord'], params['s']\n        alpha_b, alpha_c, beta = params['alpha_b'], params['alpha_c'], params['beta']\n        phi, rho = params['phi'], params['rho']\n        \n        # --- 1. Define Pixel Sets ---\n        i, j = np.mgrid[0:H, 0:W]\n        i0, j0 = (H - 1) / 2.0, (W - 1) / 2.0\n\n        # Border set B\n        dist_to_boundary = np.minimum.reduce([i, j, H - 1 - i, W - 1 - j])\n        mask_b = dist_to_boundary  w_bord\n\n        # Center disk set C\n        dist_from_center = np.sqrt((i - i0)**2 + (j - j0)**2)\n        mask_c = dist_from_center = r\n\n        # --- 2. Generate Synthetic Image x ---\n        x = np.full((H, W), mu_bg)\n        \n        # Set center disk intensity\n        num_center_pixels = np.sum(mask_c)\n        noise = np.random.normal(0, sigma, size=num_center_pixels)\n        x[mask_c] = mu_1 + noise\n        \n        # Set border intensity\n        x[mask_b] = s * y_spur\n\n        # --- 3. Define Classifier ---\n        def sigmoid(z):\n            return 1.0 / (1.0 + np.exp(-z))\n\n        def predict_proba(image):\n            sum_b = np.sum(image[mask_b])\n            sum_c = np.sum(image[mask_c])\n            z = alpha_b * sum_b + alpha_c * sum_c + beta\n            return sigmoid(z)\n\n        # --- 4. Generate Attention Map A(x) ---\n        sigma_g = r / 2.0\n        G = np.exp(-((i - i0)**2 + (j - j0)**2) / (2 * sigma_g**2))\n        S = np.abs(x - mu_bg)\n        A_unnormalized = G * S\n        A = A_unnormalized / np.max(A_unnormalized)\n\n        # --- 5. Perform Deletion Tests ---\n        f_x = predict_proba(x)\n\n        # Attention deletion\n        x_att = x.copy()\n        num_pixels_to_delete = int(np.round(phi * H * W))\n        flat_indices = np.argpartition(A.flatten(), -num_pixels_to_delete)[-num_pixels_to_delete:]\n        np.put(x_att, flat_indices, rho)\n        f_x_att = predict_proba(x_att)\n        delta_att = f_x - f_x_att\n        \n        # Border deletion\n        x_bor = x.copy()\n        x_bor[mask_b] = rho\n        f_x_bor = predict_proba(x_bor)\n        delta_bor = f_x - f_x_bor\n        \n        # --- 6. Make Faithfulness Decision ---\n        F = delta_att >= delta_bor\n        results.append(F.item())\n\n    # Format output as a list of boolean strings\n    boolean_strings = [str(r) for r in results]\n    print(f\"[{','.join(boolean_strings)}]\")\n\nsolve()\n```"
        }
    ]
}