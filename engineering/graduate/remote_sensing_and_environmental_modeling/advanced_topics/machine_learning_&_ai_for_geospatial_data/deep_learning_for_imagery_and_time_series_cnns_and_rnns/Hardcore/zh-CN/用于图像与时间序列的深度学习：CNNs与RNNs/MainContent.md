## 引言
深度学习，特别是卷积神经网络（CNNs）和循环神经网络（RNNs），已成为数据分析领域的一场革命，在处理复杂的遥感影像和环境时间序列数据方面展现出巨大潜力。然而，有效应用这些模型远不止是简单地使用现有工具。它要求从业者不仅深入理解其内部机制，还需掌握将科学领域知识巧妙融入模型设计的艺术——这正是许多研究人员和工程师面临的知识鸿沟。本文旨在弥合理论与实践之间的差距。我们首先将在“原理与机制”一章中，系统地剖析构成现代[深度学习](@entry_id:142022)应用基石的CNNs和RNNs，从第一性原理出发揭示它们的工作方式。接着，在“应用与跨学科连接”一章中，我们将展示如何通过[数据预处理](@entry_id:197920)、架构设计和[损失函数](@entry_id:634569)定制，将领域知识嵌入模型中，以解决真实的科学挑战。最后，“动手实践”部分将通过具体的编程练习，帮助读者巩固所学知识。通过这一结构化的学习路径，读者将能掌握从基础理论到高级应用的全面技能，从而构建出既强大又具备科学严谨性的[深度学习模型](@entry_id:635298)。

## 原理与机制

在上一章介绍的基础上，本章将深入探讨构成现代遥感与环境建模深度学习应用核心的原理与机制。我们将系统性地剖析[卷积神经网络](@entry_id:178973)（CNNs）和[循环神经网络](@entry_id:634803)（RNNs）的基本构件，阐明它们如何从图像和时间序列数据中提取有意义的模式。我们将从基本定义出发，逐步构建起对高级结构（如[注意力机制](@entry_id:917648)和[门控循环单元](@entry_id:1125510)）的深刻理解，并展示这些技术如何解决遥感数据分析中的具体挑战。

### 卷积神经网络（CNNs）的核心机制

卷积神经网络已成为处理网格状数据（如遥感影像）的基石。其核心在于通过学习局部模式并将其组合成更复杂的特征，从而实现对空间信息的层级化抽象。

#### 卷积操作：[空间特征](@entry_id:151354)提取

卷积操作是CNN的核心。一个卷积层通过在输入数据上滑动一个或多个称为**核（kernel）**或**滤波器（filter）**的小型权重矩阵来工作。在每个位置，核与其覆盖的输入区域之间进行逐元素乘积并求和，生成输出**[特征图](@entry_id:637719)（feature map）**中的一个激活值。这一过程在整个输入空间中重复，从而产生一个代表特定空间模式（如边缘、纹理或更复杂的形状）存在的[特征图](@entry_id:637719)。

对于多光谱或高光谱遥感影像，数据通常表示为一个三维张量（高度$H \times$宽度$W \times$波段数$B$）。在这种情况下，我们必须根据数据和任务的内在结构选择合适的卷积维度。

- **二维空间卷积（2D Convolution）**：这是最常见的类型，其核的尺寸为$k \times k \times B_{in}$，其中$B_{in}$是输入通道数。核在空间维度（$x, y$）上滑动，但在每个空间位置上，它会聚合所有输入通道的信息。这种卷积非常适合于当光谱信息本身区分度不强，而类别主要通过空间纹理、形状和上下文来区分的场景。例如，在区分具有相似光谱特征但纹理迥异的城市地物（如不同类型的屋顶）时，2D卷积是理想的选择。

- **一维光谱卷积（1D Convolution）**：在此模式下，卷积核的长度为$s$，并独立地应用于每个像素的光谱向量上。它沿着光谱轴滑动，从而学习波段间的局部相关性。这种方法在类别主要由其独特的光谱特征曲线定义时非常有效。例如，在对[光谱特征](@entry_id:1132105)鲜明的大片均质农田进行分类时，1D光谱卷积可以有效利用光谱信息。更重要的是，由于它独立处理每个像素，它对波段间的微小空间共[配准](@entry_id:1122567)误差不敏感，从而避免了在类别边界处放大噪声。

- **三维时空/时谱卷积（3D Convolution）**：这种卷积使用一个三维核（例如$k \times k \times s$），在所有三个维度（空间和光谱/时间）上联合滑动。它能够捕捉时空或时谱的局部[交互作用](@entry_id:164533)。在处理混合像元问题时，例如半干旱区的灌木-草地镶嵌体，地物的区分性可能既不纯粹在空间上，也不纯粹在光谱上，而是体现在两者微妙的耦合关系中（如冠层与阴影混合导致的光谱-空间模式）。在这种情况下，3D卷积是唯一能够直接对这种联合特征进行建模的架构。

#### 控制空间维度与分辨率

卷积层的几个关键参数——**步幅（stride）**、**填充（padding）**和**扩张（dilation）**——共同决定了输出[特征图](@entry_id:637719)的空间维度和有效分辨率。

**[步幅与填充](@entry_id:635382)**

步幅$s$定义了核在输入网格上滑动的步长。填充$p$是指在输入图像的边界周围添加额外的像素（通常为零），以控制输出尺寸并处理边界效应。

从[离散卷积](@entry_id:160939)的定义出发，我们可以推导出输出[特征图](@entry_id:637719)的高度$H'$和宽度$W'$的通用公式。对于一个宽度为$W$的输入，应用对称的[零填充](@entry_id:637925)$p$后，其有效宽度变为$W + 2p$。一个宽度为$k$的核在上面滑动，步幅为$s$。输出的宽度$W'$等于核可以放置的有效位置数。第一个位置的左上角在索引1，后续位置为$1+s, 1+2s, \dots, 1+N_W s$。最后一个位置必须保证整个核仍在填充后的图像内，即$(1+N_W s) + k - 1 \le W+2p$。由此可解得总步数$N_W = \lfloor \frac{W+2p-k}{s} \rfloor$，总位置数即输出宽度为$W' = N_W + 1$。因此，输出尺寸由下式给出：

$$H' = \left\lfloor \frac{H + 2p - k}{s} \right\rfloor + 1$$

$$W' = \left\lfloor \frac{W + 2p - k}{s} \right\rfloor + 1$$

例如，对于一个输入尺寸为$1536 \times 1024$的图像，使用$k=7, s=3, p=1$的卷积，其输出尺寸将是$H' = \lfloor \frac{1536+2(1)-7}{3} \rfloor + 1 = 511$和$W' = \lfloor \frac{1024+2(1)-7}{3} \rfloor + 1 = 340$。

在遥感应用中，一个特别重要的概念是**地面采样距离（Ground Sample Distance, GSD）**，即像素中心在地面上的物理间距。当步幅$s > 1$时，卷积层实际上对输入[特征图](@entry_id:637719)进行了下采样。输出[特征图](@entry_id:637719)上相邻像素的中心，对应于输入网格上相隔$s$个像素的位置。因此，如果输入图像的GSD为$G$，则输出[特征图](@entry_id:637719)的有效GSD $G'$将变为：

$$G' = s \times G$$

例如，若输入GSD为10米，步幅为3，则输出[特征图](@entry_id:637719)的有效GSD将变为30米。

在某些任务（如[语义分割](@entry_id:637957)）中，我们希望保持[特征图](@entry_id:637719)的空间分辨率与输入一致。这可以通过设置步幅$s=1$并选择合适的填充来实现。令$H'=H$和$W'=W$，我们可以求解所需的填充$p$。当$s=1$时，公式变为$W = W + 2p - k + 1$，解得$p = \frac{k-1}{2}$。这种设置通常被称为“**相同填充（same padding）**”，它要求核尺寸$k$为奇数，以确保$p$为整数。

**步幅与[混叠](@entry_id:146322)**

将步幅$s>1$的卷积视为下采样操作，为我们提供了从信号处理角度理解其影响的框架。根据**奈奎斯特-香农采样定理**，为了无失真地从样本中重建一个连续信号，采样频率必须大于该信号最大频率的两倍。对于空间信号，这等价于其最大角[空间频率](@entry_id:270500)$\omega_{\max}$必须小于采样网格的[奈奎斯特频率](@entry_id:276417)$\omega_{Nyquist} = \frac{\pi}{\Delta}$，其中$\Delta$是采样间隔（即GSD）。

当CNN层使用步幅$s$时，它创建了一个新的、更稀疏的采样网格，其有效GSD为$\Delta' = s\Delta$。这个新网格的奈奎斯特频率为$\omega'_{Nyquist} = \frac{\pi}{s\Delta}$。为了避免**[混叠](@entry_id:146322)（aliasing）**——即高频信号成分被错误地表示为低频成分——输入到该层的信号的最大频率必须低于这个新的奈奎斯特频率。因此，无[混叠](@entry_id:146322)的条件是：

$$\omega_{\max}  \frac{\pi}{s\Delta}$$

重新整理可得到对步幅$s$的约束：

$$s  \frac{\pi}{\Delta \omega_{\max}}$$

这个不等式表明，最大允许的步幅取决于原始图像的“过采样因子”。如果一个高分辨率航拍影像的GSD为$\Delta = 0.08$米/像素，其内容的最大角[空间频率](@entry_id:270500)经估计为$\omega_{\max} = 15$[弧度](@entry_id:171693)/米，那么为避免[混叠](@entry_id:146322)，步幅必须小于$s_{\max} = \frac{\pi}{0.08 \times 15} \approx 2.618$。在实践中，这意味着使用$s=3$可能会引入[混叠伪影](@entry_id:925293)，而$s=2$则是安全的。这对于保留精细纹理和边界的完整性至关重要。

#### 感受野：理解网络“看”到了什么

一个网络层中神经元的**[感受野](@entry_id:636171)（Receptive Field）**是指输入空间中能够影响该神经元激活值的区域大小。在CNN中，感受野的大小随着[网络深度](@entry_id:635360)的增加而增长，这使得网络能够学习从局部到全局的层级化特征。

我们可以推导出一个由$L$个卷积层组成的堆栈的[有效感受野](@entry_id:637760)尺寸$R$的通用公式。设第$\ell$层（$\ell=1, \dots, L$）的核尺寸为$k_\ell$，步幅为$s_\ell$，扩张率为$d_\ell$。[扩张卷积](@entry_id:636365)通过在核的权重之间插入$d_\ell-1$个零，增大了核的覆盖范围而不增加参数。一个[扩张卷积](@entry_id:636365)核的有效尺寸为$(k_\ell-1)d_\ell+1$。

感受野的尺寸可以通过一个递归关系来计算。第$\ell$层的[感受野](@entry_id:636171)$R_\ell$是在前一层$R_{\ell-1}$的基础上扩展的。[扩展量](@entry_id:178668)取决于第$\ell$层的核覆盖的区域，并按之前所有层的累积步幅进行缩放。最终，一个$L$层网络的[感受野大小](@entry_id:634995)$R$为：

$$R = 1 + \sum_{\ell=1}^{L} \left[ (k_\ell - 1)d_\ell \prod_{i=1}^{\ell-1} s_i \right]$$

其中$\prod_{i=1}^{0} s_i$定义为1。这个公式揭示了步幅和扩张率对感受野增长的乘法效应。例如，一个具有以下参数的4层CNN：
- 第1层: $k_1=3, s_1=1, d_1=1$
- 第2层: $k_2=5, s_2=2, d_2=2$
- 第3层: $k_3=3, s_3=1, d_3=1$
- 第4层: $k_4=3, s_4=2, d_4=3$

其[感受野大小](@entry_id:634995)为$R = 1 + (3-1)\cdot 1 \cdot 1 + (5-1)\cdot 2 \cdot 1 + (3-1)\cdot 1 \cdot (1\cdot 2) + (3-1)\cdot 3 \cdot (1\cdot 2\cdot 1) = 1 + 2 + 8 + 4 + 12 = 27$像素。如果像素的GSD为30米，这个[感受野](@entry_id:636171)对应的物理覆盖范围就是$27 \times 30 = 810$米，即$0.81$公里。

#### 用于密集预测的架构：[编码器-解码器](@entry_id:637839)与[跳跃连接](@entry_id:637548)

对于需要为每个输入像素生成一个输出的密集预测任务（如[土地覆盖](@entry_id:1127047)分割），一种称为**[编码器-解码器](@entry_id:637839)（encoder-decoder）**的架构非常流行。**[U-Net](@entry_id:635895)**是其中的一个典型代表。

编码器部分通常是一个标准的分类CNN，由一系列[卷积和](@entry_id:263238)池化（或大[步幅卷积](@entry_id:637216)）层组成。它逐步降低[空间分辨率](@entry_id:904633)，同时增加通道数（特征的抽象层次），从而在深层[特征图](@entry_id:637719)中捕捉高级的语义信息。然而，这个过程中存在一个“[信息瓶颈](@entry_id:263638)”。每次下采样操作（如步幅为2的[最大池化](@entry_id:636121)）都会将有效GSD加倍，并根据[采样定理](@entry_id:262499)，永久性地丢失高于新[奈奎斯特频率](@entry_id:276417)的[空间频率](@entry_id:270500)信息。对于作物田地边界分割这类任务，边界是锐利的过渡，其信息主要由高频空间分量承载。这些高频分量在编码器的深层很可能被完全丢失。

解码器部分则负责将编码器输出的低分辨率、高语义的[特征图](@entry_id:637719)[上采样](@entry_id:275608)回原始输入分辨率。它通常使用**[转置卷积](@entry_id:636519)（transposed convolutions）**或其他的上[采样方法](@entry_id:141232)。然而，由于信息已经在编码器中丢失，仅靠解码器本身无法恢复精确的、高频的边界细节。它最多只能根据上下文“猜测”边界的大致位置，导致输出模糊。

[U-Net](@entry_id:635895)的精髓在于引入了**[跳跃连接](@entry_id:637548)（skip connections）**。这些连接将编码器中不同分辨率阶段的[特征图](@entry_id:637719)直接传递并拼接（concatenate）到解码器中相应分辨率的层上。这样做的好处是：

1.  **信息恢复**：[跳跃连接](@entry_id:637548)为解码器提供了高分辨率、高频的局部信息“快捷方式”，这些信息绕过了[信息瓶颈](@entry_id:263638)。解码器可以利用这些信息来精确定位边界，从而显著提高分割的准确性。
2.  **特征融合**：将编码器的[特征图](@entry_id:637719)与解码器的[上采样](@entry_id:275608)[特征图](@entry_id:637719)沿通道维度进行拼接，而不是逐元素相加，被认为是一种更优的融合策略。拼接保留了两种特征（一种是低级、高分辨率的空间细节，另一种是高级、低分辨率的语义上下文）的独立性，允许后续的卷积层以数据驱动的方式学习如何最佳地融合它们，而不是强制进行破坏性的直接相加。

### [循环神经网络](@entry_id:634803)（RNNs）的核心机制

当数据具有序列结构时，如[环境监测](@entry_id:196500)的时间序列数据，[循环神经网络](@entry_id:634803)（RNNs）是自然的选择。RNN通过在时间步之间共享权重并维持一个内部**[隐藏状态](@entry_id:634361)（hidden state）**来处理可变长度的序列。

#### [长期依赖](@entry_id:637847)的挑战：[梯度消失与爆炸](@entry_id:634312)

一个简单的RNN在时间步$t$的[隐藏状态](@entry_id:634361)$h_t$是前一时刻状态$h_{t-1}$和当前输入$x_t$的函数：$h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$。在训练过程中，通常使用**通过时间反向传播（Backpropagation Through Time, [BPTT](@entry_id:633900)）**算法来计算[损失函数](@entry_id:634569)对参数的梯度。[BPTT](@entry_id:633900)将RNN沿时间维度展开成一个[深度前馈网络](@entry_id:635356)，然后应用标准的[反向传播](@entry_id:199535)。

然而，当序列很长时，BPTT面临两个严峻的挑战。首先是**内存瓶颈**。为了计算梯度，需要存储[前向传播](@entry_id:193086)过程中计算出的所有中间激活值（主要是[隐藏状态](@entry_id:634361)）。对于一个长度为$T$的序列，[批量大小](@entry_id:174288)为$B$，隐藏层维度为$D$的网络，完整BPTT的内存需求与$B \cdot T \cdot D$成正比，这对于长序列来说是不可接受的。

更根本的问题是**[梯度消失与爆炸](@entry_id:634312)（vanishing and exploding gradients）**。在反向传播过程中，梯度需要通过[链式法则](@entry_id:190743)从后向前传递。从时刻$t$的损失对时刻$t-k$的[隐藏状态](@entry_id:634361)求导，会涉及到$k$个[雅可比矩阵](@entry_id:178326)$\frac{\partial h_i}{\partial h_{i-1}}$的连乘。如果这些矩阵的范数持续小于1，梯度会呈指数级衰减至零（梯度消失），使得网络无法学习到[长期依赖](@entry_id:637847)关系。反之，如果范数持续大于1, 梯度会指数级增长（[梯度爆炸](@entry_id:635825)），导致训练不稳定。

为了解决内存问题，一种常见的实用技术是**截断通过时间[反向传播](@entry_id:199535)（Truncated [BPTT](@entry_id:633900), TBPTT）**。TBPTT将[前向传播](@entry_id:193086)完整执行，但在[反向传播](@entry_id:199535)时，只回传固定的$W$步。这使得内存需求降低到与$B \cdot W \cdot D$成正比，与总序列长度$T$无关。然而，TBPTT通过截断[梯度流](@entry_id:635964)，只能近似真实梯度。在一个简化的线性RNN中可以证明，截断梯度与完整梯度之比恰好为$\frac{W}{T}$。这意味着T[BPTT](@entry_id:633900)学习到的依赖关系范围被人为地限制在了窗口$W$之内。

#### 长短期记忆网络（LSTM）：门控解决方案

为了从根本上解决[梯度消失问题](@entry_id:144098)，**[长短期记忆](@entry_id:637886)（Long Short-Term Memory, [LSTM](@entry_id:635790)）**网络被提了出来。LSTM是一种特殊的RNN，其核心思想是引入一个独立的**细胞状态（cell state）**$c_t$和三个精巧的**门（gate）**结构来控制信息流。

一个[LSTM单元](@entry_id:636128)的更新过程如下：

1.  **[遗忘门](@entry_id:637423)（Forget Gate）**$f_t$：决定从上一个细胞状态$c_{t-1}$中丢弃多少信息。
    $$f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)$$

2.  **输入门（Input Gate）**$i_t$和**候选状态（Candidate State）**$g_t$：决定将哪些新信息存入细胞状态。
    $$i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)$$
    $$g_t = \tanh(W_g x_t + U_g h_{t-1} + b_g)$$

3.  **细胞状态更新**：结合遗忘和输入，更新细胞状态。
    $$c_t = f_t \odot c_{t-1} + i_t \odot g_t$$

4.  **[输出门](@entry_id:634048)（Output Gate）**$o_t$：决定从细胞状态中输出哪些信息作为新的[隐藏状态](@entry_id:634361)。
    $$o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)$$
    $$h_t = o_t \odot \tanh(c_t)$$

其中$\sigma$是logistic sigmoid函数，$\tanh$是[双曲正切函数](@entry_id:634307)，$\odot$代表逐元素乘积。

[LSTM](@entry_id:635790)缓解梯度消失的关键在于细胞状态的更新方式。梯度沿时间反向传播时，$\frac{\partial L}{\partial c_{t-k}}$的路径上包含一个连乘项$\prod_{j=t-k+1}^{t} f_j$。由于[遗忘门](@entry_id:637423)$f_j$的输出在$(0,1)$之间，网络可以通过学习将$f_j$的值设置在接近1的水平，来为梯度创建一个“信息高速公路”。当$f_j \approx 1$时，梯度可以几乎无衰减地流过多达数个时间步，从而使得网络能够学习到[长期依赖](@entry_id:637847)。例如，在模拟NDVI的年度季节性周期时，[LSTM](@entry_id:635790)可以通过将[遗忘门](@entry_id:637423)在整个季节内保持“打开”（$f_t \approx 1$），来记住季节初期的状态，从而预测季节[末期](@entry_id:169480)的植被状况。反之，当遇到无关信息（如云层污染的数据点）时，网络可以学习将$f_t$设置为接近0，从而“忘记”当前状态，保护[长期记忆](@entry_id:169849)不受干扰。

#### [时间卷积网络](@entry_id:1132914)（TCN）：一种序列的卷积方法

作为RNN的替代方案，**[时间卷积网络](@entry_id:1132914)（Temporal Convolutional Network, TCN）**利用卷积结构来处理[序列数据](@entry_id:636380)，并展现出卓越的性能。TCN的核心是**因果卷积（causal convolutions）**和**[扩张卷积](@entry_id:636365)（dilated convolutions）**。

因果卷积确保了在时刻$t$的输出只依赖于时刻$t$及之前的输入，这对于预测任务是必不可少的。[扩张卷积](@entry_id:636365)则通过指数级增加的扩张因子，使得网络能够以很少的层数获得巨大的感受野，从而有效捕捉[长期依赖](@entry_id:637847)。

一个拥有$L$层、核大小为$k$、扩张[基数](@entry_id:754020)为$r$（即第$l$层的扩张率$d_l = r^l$）的TCN，其[感受野](@entry_id:636171)长度$R$为：

$$R(L,k,r) = 1 + (k-1)\frac{r^L - 1}{r - 1} \quad (\text{对于 } r > 1)$$

这个公式表明，TCN的感受野随层数$L$呈指数增长。例如，在一个小时径流预测任务中，若需要捕捉大约两周（336小时）的水文记忆，使用核大小$k=3$、扩张[基数](@entry_id:754020)$r=2$的TCN，其[感受野](@entry_id:636171)为$R(L,3,2) = 2^{L+1}-1$。为使$R \ge 336$，我们需要$2^{L+1} \ge 337$，解得最小的整数层数$L=8$。仅用8层，TCN就能看到超过500个时间步之前的信息，这展示了其捕捉[长期依赖](@entry_id:637847)的强大能力。

### 高级机制与训练技术

除了CNN和RNN的核心构件外，一些高级机制和训练技术对于构建高性能的现代深度学习模型至关重要。

#### [注意力机制](@entry_id:917648)：聚焦于重要信息

**[注意力机制](@entry_id:917648)（Attention Mechanism）**允许模型在处理输入时动态地将[焦点](@entry_id:174388)放在最相关的部分。**[缩放点积注意力](@entry_id:636814)（Scaled Dot-Product Attention）**是其中的一种关键形式，是Transformer等现代架构的核心。

其原理可以从三个基本概念推导得出：

1.  **相似度**：给定一个**查询（Query）**向量$q$（代表当前上下文）和一组**键（Key）**向量$k_i$（代表输入序列中的各个元素），它们之间的相似度或相关性可以通过点积$q^T k_i$来衡量。
2.  **权重分配**：利用**[Softmax](@entry_id:636766)**函数将这些原始的相似度得分转换成一组和为1的概率权重$\alpha_i$，即**注意力权重**。$\alpha_i = \frac{\exp(s_i)}{\sum_j \exp(s_j)}$。
3.  **稳定性缩放**：当键向量的维度$d_k$很大时，点积$q^T k_i$的方差会与$d_k$成正比地增长。这会导致[Softmax函数](@entry_id:143376)进入饱和区，使得梯度变得极小，从而妨碍学习。为了解决这个问题，需要将点积得分除以一个缩放因子$\sqrt{d_k}$。

综合起来，注意力权重的计算公式为：

$$\alpha_i = \text{softmax}\left(\frac{q^T k_i}{\sqrt{d_k}}\right)$$

最终的输出是**值（Value）**向量$v_i$（代表输入元素的内容）的加权和：$\text{output} = \sum_i \alpha_i v_i$。

在野火风险预测的例子中，查询$q$可以编码当前时刻的目标位置气象信息，键$k_i$可以编码不同历史时刻和空间区域的遥感特征（如燃料湿度），值$v_i$则是这些时空候选区域对风险的基准贡献。[注意力机制](@entry_id:917648)通过计算$q$和$k_i$的相似度，动态地为那些与当前查询最相关的历史时空区域分配更高的权重，从而形成一个更精准、更具解释性的[风险评估](@entry_id:170894)。

#### [稳定训练](@entry_id:635987)：[批量归一化](@entry_id:634986)

深度神经网络的训练面临一个被称为**[内部协变量偏移](@entry_id:637601)（Internal Covariate Shift, ICS）**的问题，即在训练过程中，由于前层网络参数的更新，导致后层网络输入的分布发生变化。这使得后层网络需要不断适应新的输入分布，从而减慢了训练速度。

**[批量归一化](@entry_id:634986)（Batch Normalization, BN）**是一种旨在缓解ICS的技术。其核心思想是在网络的每一层[激活函数](@entry_id:141784)之前，对该层的输入进行归一化处理。对于一个小批量（mini-batch）的激活，BN会计算其均值$\mu$和方差$\sigma^2$，然后对每个激活值$x$进行[标准化](@entry_id:637219)：

$$\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}$$

其中$\epsilon$是一个很小的正常数，用于防止除以零。

然而，强制将每层输入的均值和方差固定为0和1可能会限制网络的表达能力。因此，BN引入了两个可学习的参数——缩放因子$\gamma$和平移因子$\beta$——来对归一化后的激活进行[仿射变换](@entry_id:144885)：

$$y = \gamma \hat{x} + \beta$$

这使得网络可以学习恢复原始激活的[表示能力](@entry_id:636759)，甚至学习到对后续层更有利的分布。

在训练遥感影像分类CNN时，对来自不同地理位置、不同时间拍摄的影像块（patches）进行归一化，可以使模型对光照、大气条件等变化更加鲁棒，从而稳定并加速训练过程。例如，对于一个包含8个近红外波段激活值的小批量$\{0.74, \dots, 0.66\}$，我们可以计算其均值$\mu=0.70$和方差$\sigma^2=0.0016$。然后，利用这些统计量和学习到的$\gamma, \beta$参数，对任何一个激活值进行变换，使其进入一个更稳定的[数值范围](@entry_id:752817)。