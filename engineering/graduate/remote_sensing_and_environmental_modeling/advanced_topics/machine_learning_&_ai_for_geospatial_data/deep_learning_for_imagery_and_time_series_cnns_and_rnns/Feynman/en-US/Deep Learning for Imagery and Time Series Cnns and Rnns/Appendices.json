{
    "hands_on_practices": [
        {
            "introduction": "Designing deep learning models for high-dimensional remote sensing data, such as hyperspectral imagery, requires a firm grasp of computational efficiency. This exercise guides you through a first-principles analysis of a 3D convolutional layer, a common building block for spatio-spectral feature extraction. By deriving the number of parameters and computational cost for both a standard and a separable convolution, you will gain quantitative insight into architectural trade-offs that are critical for developing scalable and practical models.",
            "id": "3805474",
            "problem": "Consider a hyperspectral image cube represented as a three-dimensional tensor $X \\in \\mathbb{R}^{H \\times W \\times C}$, where $H$ and $W$ are the spatial height and width, and $C$ is the number of spectral bands. You design a layer that performs three-dimensional convolution with $F$ output filters, stride $1$ in all dimensions, and padding chosen so that the output has the same spatial and spectral dimensions as the input (often called “same” padding). Each filter uses a spatial-spectral kernel of size $(k_{s}, k_{s}, k_{\\lambda})$, and a single scalar bias is added per output filter after the convolution. You are to quantify the model complexity and computational burden from first principles.\n\nStart from the definition of discrete convolution as a sum of products over the kernel support, and define computational cost as the total number of scalar multiply-accumulate pairs (one multiplication followed by one addition, commonly abbreviated as multiply-accumulate (MAC)) performed in a single forward pass over one input cube $X$. Treat the bias addition as negligible for MAC counting.\n\nNow consider an alternative design in which each three-dimensional kernel for an output filter is restricted to be spatial-spectral separable in the sense that the kernel equals the outer product of a spatial two-dimensional kernel $A \\in \\mathbb{R}^{k_{s} \\times k_{s}}$ and a spectral one-dimensional kernel $b \\in \\mathbb{R}^{k_{\\lambda}}$. Implement this separable kernel as two successive convolutions: first apply a two-dimensional convolution with $A$ independently at each spectral index, then apply a one-dimensional convolution with $b$ along the spectral dimension independently at each spatial location, followed by a single scalar bias per output filter. Assume stride $1$ and padding selected to preserve dimensions at each stage.\n\nDerive symbolic expressions for the total number of trainable parameters and the total number of MACs in one forward pass for both the dense three-dimensional convolution and the separable spatial-spectral design, in terms of $H$, $W$, $C$, $F$, $k_{s}$, and $k_{\\lambda}$. Finally, provide the two ratios as closed-form analytic expressions: the ratio of dense to separable parameter counts, and the ratio of dense to separable MAC counts. Express your final answer as these two ratios only, in exact symbolic form, with no numerical substitution or rounding.",
            "solution": "This problem requires a first-principles derivation of the number of trainable parameters and the computational complexity (measured in multiply-accumulate operations, or MACs) for two types of three-dimensional convolutional layers applied to a hyperspectral data cube. The input is a tensor $X \\in \\mathbb{R}^{H \\times W \\times C}$, where $H$ and $W$ are spatial dimensions and $C$ is the spectral dimension. The layer has $F$ output filters. In both cases, stride is $1$ and padding is chosen to preserve the input dimensions $H, W, C$ for each output feature map.\n\nFirst, we analyze the standard (dense) three-dimensional convolution.\n\nA dense 3D convolution layer with $F$ filters has $F$ distinct kernels. Each kernel, let's denote it $K_f$ for filter $f \\in \\{1, \\dots, F\\}$, is a three-dimensional tensor of size $k_s \\times k_s \\times k_\\lambda$. Additionally, each filter $f$ has a single scalar bias term, $b_f$.\n\nThe number of trainable parameters for a single filter $f$ is the sum of the number of elements in its kernel and its bias. The kernel $K_f$ has $k_s \\times k_s \\times k_\\lambda = k_s^2 k_\\lambda$ weight parameters. The bias $b_f$ is $1$ parameter. Thus, one filter has $(k_s^2 k_\\lambda + 1)$ parameters. Since there are $F$ such filters, each with its own independent set of parameters, the total number of trainable parameters for the dense convolution layer, $P_{\\text{dense}}$, is:\n$$P_{\\text{dense}} = F \\cdot (k_s^2 k_\\lambda + 1)$$\n\nNext, we calculate the number of MACs for a single forward pass. A MAC is defined as one multiplication followed by one addition. The computation of each element in the output feature map involves a dot product between the kernel and a corresponding patch of the input tensor. For a kernel of size $k_s \\times k_s \\times k_\\lambda$, this dot product requires $k_s^2 k_\\lambda$ multiplications. Following the problem's definition, this corresponds to $k_s^2 k_\\lambda$ MACs (as bias addition is considered negligible for MAC counting).\n\nThe layer produces $F$ output feature maps. Due to the \"same\" padding and stride of $1$, each feature map has dimensions $H \\times W \\times C$. Therefore, the total number of elements in the output tensor is $F \\times H \\times W \\times C$. Since each of these elements requires $k_s^2 k_\\lambda$ MACs to compute, the total number of MACs for the dense convolution layer, $M_{\\text{dense}}$, is:\n$$M_{\\text{dense}} = (F \\cdot H \\cdot W \\cdot C) \\cdot (k_s^2 k_\\lambda) = F \\cdot HWC \\cdot k_s^2 k_\\lambda$$\n\nSecond, we analyze the spatial-spectral separable convolution.\n\nIn this design, each of the $F$ filters is parameterized not by a dense 3D kernel, but by a pair of smaller kernels: a spatial kernel $A_f \\in \\mathbb{R}^{k_s \\times k_s}$ and a spectral kernel $b_f \\in \\mathbb{R}^{k_\\lambda}$. The dense 3D kernel is implicitly defined as the outer product of $A_f$ and $b_f$.\n\nThe number of trainable parameters for a single filter $f$ is the sum of the elements in $A_f$, the elements in $b_f$, and the single scalar bias. The spatial kernel $A_f$ has $k_s^2$ parameters. The spectral kernel $b_f$ has $k_\\lambda$ parameters. The bias is $1$ parameter. Thus, one separable filter has $(k_s^2 + k_\\lambda + 1)$ parameters. For $F$ filters, the total number of parameters, $P_{\\text{sep}}$, is:\n$$P_{\\text{sep}} = F \\cdot (k_s^2 + k_\\lambda + 1)$$\n\nThe computational cost is calculated by analyzing the two-stage process described.\nStage 1: A 2D spatial convolution is applied. For each of the $F$ filters, its spatial kernel $A_f$ is convolved with each of the $C$ spectral slices of the input tensor $X$. To produce one intermediate feature map of size $H \\times W \\times C$ for a filter $f$, we perform $C$ independent 2D convolutions (one for each slice). The cost to compute one pixel in one of these 2D convolutions is $k_s^2$ MACs. Over the $H \\times W$ spatial dimensions and $C$ slices for a single filter $f$, the cost is $H \\cdot W \\cdot C \\cdot k_s^2$. For all $F$ filters, the total MACs for Stage 1, $M_1$, is:\n$$M_1 = F \\cdot HWC \\cdot k_s^2$$\n\nStage 2: A 1D spectral convolution is applied. For each filter $f$, its spectral kernel $b_f$ is convolved with the intermediate feature map generated in Stage 1. This convolution is performed along the spectral dimension, independently for each of the $H \\times W$ spatial locations. The cost to compute one final output pixel is a 1D convolution of length $k_\\lambda$, which requires $k_\\lambda$ MACs. The final output has $F \\cdot H \\cdot W \\cdot C$ elements. Thus, the total MACs for Stage 2, $M_2$, is:\n$$M_2 = F \\cdot HWC \\cdot k_\\lambda$$\n\nThe total number of MACs for the separable design, $M_{\\text{sep}}$, is the sum of the MACs from both stages:\n$$M_{\\text{sep}} = M_1 + M_2 = F \\cdot HWC \\cdot k_s^2 + F \\cdot HWC \\cdot k_\\lambda = F \\cdot HWC \\cdot (k_s^2 + k_\\lambda)$$\n\nFinally, we compute the required ratios.\n\nThe ratio of the number of parameters is:\n$$\\text{Ratio}_P = \\frac{P_{\\text{dense}}}{P_{\\text{sep}}} = \\frac{F (k_s^2 k_\\lambda + 1)}{F (k_s^2 + k_\\lambda + 1)} = \\frac{k_s^2 k_\\lambda + 1}{k_s^2 + k_\\lambda + 1}$$\n\nThe ratio of the number of MACs is:\n$$\\text{Ratio}_M = \\frac{M_{\\text{dense}}}{M_{\\text{sep}}} = \\frac{F \\cdot HWC \\cdot k_s^2 k_\\lambda}{F \\cdot HWC \\cdot (k_s^2 + k_\\lambda)} = \\frac{k_s^2 k_\\lambda}{k_s^2 + k_\\lambda}$$\n\nThese two expressions represent the reduction in model complexity and computational cost, respectively, achieved by using a separable convolution instead of a dense one.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{k_s^{2} k_{\\lambda} + 1}{k_s^{2} + k_{\\lambda} + 1}  \\frac{k_s^{2} k_{\\lambda}}{k_s^{2} + k_{\\lambda}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In applied deep learning, seemingly minor implementation details can introduce significant and systematic biases in model predictions. This practice problem explores the impact of different padding schemes—zero, reflect, and replicate—on the performance of a CNN at image boundaries. By analyzing a simplified coastal segmentation scenario, you will see how these choices can create or suppress edge features, leading to false positives or negatives and affecting the scientific integrity of the model's output.",
            "id": "3805476",
            "problem": "A coastal water segmentation model built with a Convolutional Neural Network (CNN) must generate reliable land-water boundary predictions at image borders in nadir-view multispectral satellite imagery used for environmental modeling of shoreline change. Consider a simplified analysis that isolates the first convolutional layer’s behavior along a single image row at the left border to understand how padding choices influence border predictions. Let the raw radiance along the row be modeled by a one-dimensional signal $I[i]$ with pixel index $i \\in \\mathbb{Z}$, where smaller values correspond to darker water and larger values to brighter land. Assume water has mean radiance $r_{w} = 0.2$ and land has mean radiance $r_{l} = 0.8$, which are plausible, normalized values for a near-infrared band where water appears dark and land bright.\n\nSuppose the first convolutional layer implements a discrete cross-correlation with an odd-symmetric $3$-tap kernel $h = [-1, 0, 1]$ to approximate a first derivative (a common primitive for boundary-sensitive features), producing a feature response\n$$\ny[i] = \\sum_{k=-1}^{1} h[k] \\, I[i+k] = I[i+1] - I[i-1],\n$$\nwith the understanding that values $I[i]$ outside the image are defined by a padding scheme. Consider three standard padding schemes: zero padding, reflect padding, and replicate padding, defined as follows for the left border at index $i=0$:\n- Zero padding: $I[-1] = 0$.\n- Reflect padding: $I[-1] = I[1]$.\n- Replicate padding: $I[-1] = I[0]$.\n\nAssume a simple boundary detector flags a shoreline at the leftmost pixel when $y[0] \\ge \\tau$ with threshold $\\tau = 0.1$. Analyze two practically relevant left-border scenarios for coastal imagery:\n- Scenario $S_{1}$ (no shoreline at the border): $I[0] = r_{w}$ and $I[1] = r_{w}$, representing open water continuing away from the border.\n- Scenario $S_{2}$ (shoreline passes immediately at the border): $I[0] = r_{w}$ and $I[1] = r_{l}$, representing a water-to-land transition at the first interior pixel.\n\nBased only on the discrete cross-correlation definition above, the padding definitions, and the stated radiance values and threshold, which of the following statements are correct regarding the impact of padding on the border feature response and the induced shoreline flags?\n\nA. In Scenario $S_{1}$, zero padding produces a strictly positive $y[0]$ that exceeds the threshold and therefore yields a false positive shoreline flag at the left border, whereas reflect and replicate padding both yield $y[0] = 0$ and no false positive.\n\nB. In Scenario $S_{2}$, reflect padding suppresses the left-border response to $y[0] = 0$ and thus misses a real shoreline at the border, while replicate padding yields a positive response smaller than the zero-padded response but still above threshold, thereby preserving a true positive at the border.\n\nC. Under the stated detector and scenarios, replicate padding is the only scheme among the three that simultaneously suppresses the false positive in Scenario $S_{1}$ and preserves the true positive in Scenario $S_{2}$ at the leftmost pixel, making it preferable for unbiased shoreline flags specifically at the image border.\n\nD. Any padding-induced border artifact can be learned away by the CNN with sufficient training data, so zero padding is preferable for unbiased border predictions in practice.",
            "solution": "We begin from the definition of discrete cross-correlation for the $3$-tap odd-symmetric kernel $h = [-1, 0, 1]$, which yields\n$$\ny[i] = \\sum_{k=-1}^{1} h[k] \\, I[i+k] = (-1)\\,I[i-1] + 0 \\cdot I[i] + 1 \\cdot I[i+1] = I[i+1] - I[i-1].\n$$\nAt the left border $i=0$, the value $I[-1]$ is not present in the image and is supplied by the padding scheme. Thus,\n$$\ny[0] = I[1] - I[-1],\n$$\nwith $I[-1]$ given by the chosen padding.\n\nWe analyze the two scenarios:\n\nScenario $S_{1}$: $I[0] = r_{w} = 0.2$, $I[1] = r_{w} = 0.2$ (open water).\n- Zero padding: $I[-1] = 0$. Then $y[0] = I[1] - I[-1] = 0.2 - 0 = 0.2$. Since $\\tau = 0.1$, we have $y[0] = 0.2 \\ge \\tau$, which falsely triggers a shoreline flag; this is a false positive produced by the artificial dark band introduced outside the image.\n- Reflect padding: $I[-1] = I[1] = 0.2$. Then $y[0] = 0.2 - 0.2 = 0$. Since $0  \\tau$, no shoreline is flagged; there is no false positive.\n- Replicate padding: $I[-1] = I[0] = 0.2$. Then $y[0] = 0.2 - 0.2 = 0$. Again, no shoreline is flagged; there is no false positive.\n\nTherefore, in Scenario $S_{1}$, zero padding creates a spurious positive response that exceeds threshold, while reflect and replicate yield zero response.\n\nScenario $S_{2}$: $I[0] = r_{w} = 0.2$, $I[1] = r_{l} = 0.8$ (water-to-land transition at the first interior pixel).\n- Zero padding: $I[-1] = 0$. Then $y[0] = 0.8 - 0 = 0.8$. Since $0.8 \\ge 0.1$, a shoreline is correctly flagged with a strong response.\n- Reflect padding: $I[-1] = I[1] = 0.8$. Then $y[0] = 0.8 - 0.8 = 0$. This entirely suppresses the edge response at the border, yielding no shoreline flag at $i=0$ despite the real transition; this is a false negative at the border under the stated rule.\n- Replicate padding: $I[-1] = I[0] = 0.2$. Then $y[0] = 0.8 - 0.2 = 0.6$. Since $0.6 \\ge 0.1$, a shoreline is correctly flagged at the border with a response that is smaller than under zero padding but still comfortably above threshold.\n\nThese outcomes can be understood from first principles: zero padding inserts an unphysical dark value that inflates odd-symmetric gradient responses at the image boundary, reflect padding enforces even symmetry across the border that annihilates odd-symmetric responses such as first derivatives at the border, and replicate padding continues the border pixel value, preserving a one-sided contrast without creating an artificial step to zero.\n\nOption-by-option analysis:\n- Option A: In Scenario $S_{1}$, we computed $y[0] = 0.2$ for zero padding, which exceeds $\\tau = 0.1$ and triggers a false shoreline. For reflect and replicate, $y[0] = 0$, yielding no false positive. This matches the statement. Verdict — Correct.\n- Option B: In Scenario $S_{2}$, reflect padding gives $y[0] = 0$, missing the true shoreline at the border, while replicate yields $y[0] = 0.6$, smaller than the zero-padded $0.8$ but still above threshold, thus preserving a true positive. This matches the statement. Verdict — Correct.\n- Option C: Under the specified detector and scenarios, replicate padding uniquely suppresses the false positive in Scenario $S_{1}$ and preserves a true positive in Scenario $S_{2}$ at the leftmost pixel. Zero padding fails the first requirement (it yields a false positive in Scenario $S_{1}$), and reflect padding fails the second (it yields a false negative in Scenario $S_{2}$). Thus replicate is the only padding that meets both criteria, making it preferable for unbiased shoreline flags specifically at the image border under the stated rule. Verdict — Correct.\n- Option D: The padding rule is part of the forward computation at inference; it deterministically changes $I[-1]$ and hence $y[0]$. No amount of data can make the zero-padded input at $i=-1$ equal to the physically plausible continuation at the border. While a network can partially compensate statistically, the sign and magnitude biases shown above are structural consequences of the padding choice. Therefore, claiming zero padding is preferable for unbiased border predictions because the model can learn away its artifacts is not supported by the first-principles analysis. Verdict — Incorrect.\n\nBroader implication for remote sensing and environmental modeling: padding-induced biases near image borders can systematically shift shoreline localization and class probabilities, impacting derived coastal metrics. Reflect padding reduces spurious edges but can under-detect true border-adjacent transitions for odd-symmetric filters, while replicate padding often provides a better compromise at the border by preserving one-sided contrast without introducing unphysical zeros. These effects persist in deeper CNNs because the first-layer boundary conditions propagate through subsequent layers unless explicitly mitigated by training strategies or architectural design.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "A cornerstone of trustworthy scientific modeling is rigorous and unbiased evaluation of a model's generalization performance. When working with geospatial data, the common assumption of independent and identically distributed samples is violated by spatial autocorrelation, which can lead to dangerously optimistic error estimates from standard cross-validation. This exercise challenges you to confront this issue directly by deriving a spatially blocked cross-validation scheme that accounts for both the model's receptive field and the data's correlation structure, ensuring a more realistic assessment of model skill.",
            "id": "3805490",
            "problem": "A land cover classification model is trained on multispectral satellite imagery and monthly time series to predict discrete land cover classes at the pixel scale over a large agricultural region. Inputs are two-dimensional spatial patches extracted from Sentinel-2 data and twelve-month temporal sequences. The model architecture is a hybrid Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN): three convolutional layers with kernel sizes $5$, $3$, and $3$ and stride $1$ feed into a gated recurrent unit over $12$ time steps. Pixels are at $10\\ \\mathrm{m}$ resolution in both $x$ and $y$ directions. The study area exhibits isotropic, stationary spatial autocorrelation in both spectral features and labels with correlation function $\\rho(d) = \\exp\\!\\left(-d/\\lambda\\right)$, where $d$ is Euclidean distance in meters and $\\lambda = 150\\ \\mathrm{m}$ is the correlation length estimated from an empirical semivariogram. The cross-validation risk estimator for generalization error in classification is defined via average loss over held-out folds.\n\nExplain, from first principles, why random $K$-fold cross-validation at the pixel level can lead to optimistic bias in the estimated generalization error when there is positive spatial autocorrelation and a model with nonzero spatial receptive field. Then, derive a two-dimensional spatially blocked cross-validation scheme that reduces this optimistic bias by ensuring approximate independence between training and test sets at the boundaries. The derivation must begin from definitions of independence, spatial autocorrelation, and the effective receptive field of the CNN. Specifically:\n\n- Derive the effective spatial receptive field radius $r$ (in meters) of the CNN given the specified architecture and pixel resolution, interpreting the receptive field as the largest radius within which input pixels can influence the prediction at a central pixel.\n- Using the autocorrelation function $\\rho(d)$ and an independence threshold $\\epsilon = 0.05$, derive a sufficient buffer width $h$ (in meters) around test blocks such that the maximum cross-correlation between any training pixel that can influence a test prediction and the test pixel itself is bounded by $\\rho(h - r) \\le \\epsilon$.\n- State how blocks should be constructed and assigned to folds to produce approximately unbiased cross-validation risk estimates under the given autocorrelation model. Provide any necessary conditions on block size relative to $\\lambda$ and $r$, and explain how the temporal sequences are handled in the folds.\n\nCompute the numerical values of $r$ and a minimal $h$ satisfying the bound. Then select the option that correctly specifies a spatially blocked scheme consistent with your derivations.\n\nOptions:\nA. Perform random pixel-level $K$-fold cross-validation with class-stratified sampling to ensure per-fold class balance; do not use spatial blocks or buffers. This maintains independent and identically distributed samples and yields unbiased risk estimates.\n\nB. Partition the map into non-overlapping square test tiles of side length equal to the correlation length $\\lambda$; for each fold, train on all pixels outside the test tiles with no buffer. Keep all temporal sequences intact within tiles.\n\nC. Use two-dimensional spatial $h$-block cross-validation: define square test blocks whose side length is at least $3\\lambda$, assign blocks to $K$ folds, and for each fold train only on pixels that are at least $h$ meters away from any test block, where $h \\ge r + \\lambda \\ln(1/\\epsilon)$ computed from the receptive field radius $r$ and the correlation function $\\rho(d)$; keep the full $12$-month sequences intact in both train and test blocks. This yields approximately unbiased risk estimates under the isotropic autocorrelation model.\n\nD. Perform temporal blocked cross-validation by holding out alternating months across the entire region, training on the remaining months. Spatial locations are shared across train and test months; no spatial buffering is applied.\n\nE. Use spatially blocked cross-validation with blocks of side length at least $3\\lambda$ and a buffer of width equal to the receptive field radius $r$ only; assign blocks to folds and keep time sequences intact. This ensures that train and test pixels are never within one receptive field and is sufficient to eliminate optimistic bias.",
            "solution": "The user-provided problem statement is valid. It is scientifically grounded in the principles of geostatistics and machine learning, well-posed with a clear objective and sufficient information, and uses objective, precise language. The scenario described is a canonical problem in the application of deep learning to geospatial data. I will now proceed with the solution.\n\nThe problem asks for an explanation of why standard, random pixel-level cross-validation leads to biased error estimates in the presence of spatial autocorrelation, and for the derivation of a spatially blocked cross-validation scheme that mitigates this bias.\n\n### 1. The Optimistic Bias of Random Pixel-Level Cross-Validation\n\nThe fundamental assumption of $K$-fold cross-validation is that the training and testing datasets for each fold are independent draws from the underlying data distribution. When this assumption holds, the average error across the folds provides an unbiased estimate of the model's generalization error.\n\nThe problem states that there is positive spatial autocorrelation in the data, described by the function $\\rho(d) = \\exp(-d/\\lambda)$, where $d$ is distance. This means that pixels that are close to each other are not independent; their feature values and class labels are correlated. The closer two pixels are, the higher their correlation.\n\nWhen performing random $K$-fold cross-validation at the pixel level, each pixel is assigned to a fold independently of its spatial location. Consequently, for any given pixel in the test set of a fold, there will almost certainly be pixels from the training set in its immediate vicinity.\n\nThe convolutional neural network (CNN) component of the model makes predictions based on a spatial neighborhood of input pixels, defined by its receptive field. Due to spatial autocorrelation, the training pixels that are close to a test pixel carry information that is highly redundant with the information in the test pixel and its neighborhood. The model is therefore trained on data that is not truly independent of the test data. This phenomenon is often called \"information leakage.\"\n\nAs a result, the model's performance on the held-out test data will be artificially inflated, because it is not being evaluated on genuinely unseen, independent examples. The resulting estimate of the generalization error is systematically lower than the true error that would be observed on a geographically separate, independent dataset. This downward bias is referred to as \"optimistic bias.\"\n\n### 2. Derivation of a Spatially Blocked Cross-Validation Scheme\n\nTo obtain a more accurate estimate of generalization error, the training and test sets must be rendered approximately independent. This can be achieved by enforcing a minimum spatial separation between them.\n\n#### 2.1. Effective Receptive Field Radius ($r$)\n\nThe receptive field of a CNN is the size of the input region that affects a single output unit. The prediction for a central pixel is influenced by input pixels within a certain radius. We must first calculate the size of this receptive field in pixels and then convert it to meters.\n\nThe network has three sequential convolutional layers with strides of $s=1$. The receptive field size ($RF$) after layer $i$ is given by the recursive formula:\n$$RF_i = RF_{i-1} + (k_i - 1) \\prod_{j=1}^{i-1} s_j$$\nGiven that all strides are $s_j = 1$, the formula simplifies to $RF_i = RF_{i-1} + (k_i - 1)$. The input layer has a receptive field of $RF_0 = 1$.\n\n- After Layer $1$ (kernel size $k_1 = 5$):\n  $$RF_1 = RF_0 + (k_1 - 1) = 1 + (5 - 1) = 5$$\n- After Layer $2$ (kernel size $k_2 = 3$):\n  $$RF_2 = RF_1 + (k_2 - 1) = 5 + (3 - 1) = 7$$\n- After Layer $3$ (kernel size $k_3 = 3$):\n  $$RF_3 = RF_2 + (k_3 - 1) = 7 + (3 - 1) = 9$$\n\nThe final receptive field is a $9 \\times 9$ block of pixels. The problem defines the radius $r$ as the largest radius from a central pixel to an influencing input pixel. For a $9 \\times 9$ field, the field extends $(9-1)/2 = 4$ pixels from the center along the primary axes.\nThe pixel resolution is $10\\ \\mathrm{m}$. Therefore, the receptive field radius $r$ in meters is:\n$$r = 4\\ \\text{pixels} \\times 10\\ \\mathrm{m/pixel} = 40\\ \\mathrm{m}$$\n\n#### 2.2. Sufficient Buffer Width ($h$)\n\nA spatially blocked cross-validation scheme introduces a buffer zone of width $h$ between any training pixel and any test pixel. Let $p_{test}$ be a pixel in the test block and $p_{train}$ be a pixel in the training set. The minimum distance between them is $h$.\n\nThe prediction at $p_{test}$ is influenced by input pixels in its receptive field, i.e., at a distance of at most $r$ from $p_{test}$. Let $p_{input}$ be such an input pixel. We want to bound the correlation between the training data and the data used to make test predictions. The \"worst-case\" scenario, which maximizes correlation, occurs when the distance between a training pixel and an input pixel for a test prediction is minimized.\n\nBy the triangle inequality, the distance between any input pixel $p_{input}$ influencing the prediction at $p_{test}$ and any training pixel $p_{train}$ is:\n$$\\text{dist}(p_{input}, p_{train}) \\ge \\text{dist}(p_{test}, p_{train}) - \\text{dist}(p_{test}, p_{input})$$\nThe minimum distance occurs when $p_{test}$ is on the edge of the test block, and $p_{input}$ and $p_{train}$ are on a line extending from $p_{test}$, with $p_{input}$ inside the receptive field and $p_{train}$ outside the buffer.\nThe minimum distance is $\\text{dist}(p_{test}, p_{train}) \\ge h$. The maximum distance is $\\text{dist}(p_{test}, p_{input}) \\le r$.\nTherefore, the minimum distance between any input pixel for a test prediction and any training pixel is $h - r$.\n\nThe problem requires that the maximum cross-correlation be bounded by $\\epsilon = 0.05$. The autocorrelation function is monotonically decreasing, so this condition is met if the correlation at the minimum possible distance is less than or equal to $\\epsilon$:\n$$\\rho(h - r) \\le \\epsilon$$\nSubstituting the given autocorrelation function $\\rho(d) = \\exp(-d/\\lambda)$:\n$$\\exp\\left(-\\frac{h - r}{\\lambda}\\right) \\le \\epsilon$$\nTo solve for $h$, we take the natural logarithm of both sides:\n$$-\\frac{h - r}{\\lambda} \\le \\ln(\\epsilon)$$\nMultiplying by $-\\lambda$ (a positive constant) reverses the inequality:\n$$h - r \\ge -\\lambda \\ln(\\epsilon)$$\n$$h - r \\ge \\lambda \\ln\\left(\\frac{1}{\\epsilon}\\right)$$\n$$h \\ge r + \\lambda \\ln\\left(\\frac{1}{\\epsilon}\\right)$$\nThis is the derived condition for the sufficient buffer width $h$.\n\n#### 2.3. Numerical Computation of $r$ and $h$\n\nFrom the previous section, the receptive field radius is:\n$$r = 40\\ \\mathrm{m}$$\n\nNow, we compute the minimal $h$ that satisfies the inequality. We are given:\n- $r = 40\\ \\mathrm{m}$\n- $\\lambda = 150\\ \\mathrm{m}$\n- $\\epsilon = 0.05$\n\n$$h \\ge 40\\ \\mathrm{m} + (150\\ \\mathrm{m}) \\ln\\left(\\frac{1}{0.05}\\right)$$\n$$h \\ge 40 + 150 \\ln(20)$$\nUsing $\\ln(20) \\approx 2.9957$:\n$$h \\ge 40 + 150 \\times 2.9957$$\n$$h \\ge 40 + 449.355$$\n$$h \\ge 489.355\\ \\mathrm{m}$$\nA minimal sufficient buffer width is approximately $h = 490\\ \\mathrm{m}$.\n\n#### 2.4. Block Construction and Assignment\n\nThe derived scheme is a form of spatial $h$-block cross-validation. The procedure is as follows:\n1.  **Partitioning**: The study area is partitioned into non-overlapping spatial blocks. To minimize the proportion of data discarded in buffer zones, these blocks should be large relative to the correlation length $\\lambda$ and buffer $h$. A side length of at least $3\\lambda$ is a reasonable heuristic.\n2.  **Fold Assignment**: The blocks are assigned to $K$ folds. For a given fold $k$, the test set consists of all pixels within the blocks assigned to fold $k$.\n3.  **Training Set Definition**: The training set for fold $k$ consists of all pixels from blocks not assigned to fold $k$ *and* that are at a distance of at least $h$ from any test block in fold $k$. Pixels within the buffer zones are excluded from both training and testing for that fold.\n4.  **Temporal Data Handling**: The model uses a $12$-month time series. Spatial partitioning must not break these temporal sequences. Therefore, for any given pixel, its entire $12$-month sequence is assigned as a unit to either the training set, the test set, or the buffer zone.\n\n### 3. Evaluation of Options\n\n**A. Perform random pixel-level $K$-fold cross-validation with class-stratified sampling...**\nThis method explicitly ignores spatial autocorrelation, which is the central issue. It falsely claims to maintain IID samples and yield unbiased estimates. As explained in section 1, this leads to optimistic bias.\n**Verdict: Incorrect.**\n\n**B. Partition the map into non-overlapping square test tiles of side length equal to the correlation length $\\lambda$; for each fold, train on all pixels outside the test tiles with no buffer...**\nThis method uses spatial blocking but critically omits the buffer zone. Without a buffer, training pixels can be adjacent to test pixels, leading to significant information leakage due to both the model's receptive field and a high degree of spatial correlation at close distances, violating the independence assumption.\n**Verdict: Incorrect.**\n\n**C. Use two-dimensional spatial $h$-block cross-validation: define square test blocks whose side length is at least $3\\lambda$, assign blocks to $K$ folds, and for each fold train only on pixels that are at least $h$ meters away from any test block, where $h \\ge r + \\lambda \\ln(1/\\epsilon)$ computed from the receptive field radius $r$ and the correlation function $\\rho(d)$; keep the full $12$-month sequences intact in both train and test blocks...**\nThis option correctly describes the entire derived procedure. It specifies using $h$-block CV, provides a reasonable condition for block size (at least $3\\lambda$), correctly defines the training set using a buffer $h$, provides the exact formula for $h$ derived from first principles ($h \\ge r + \\lambda \\ln(1/\\epsilon)$), and correctly handles the temporal sequences. This procedure will yield approximately unbiased risk estimates.\n**Verdict: Correct.**\n\n**D. Perform temporal blocked cross-validation by holding out alternating months...**\nThis method addresses temporal autocorrelation, which is not the stated problem. The problem is spatial autocorrelation. This approach does not perform any spatial separation and would suffer from massive information leakage, making the error estimate highly biased.\n**Verdict: Incorrect.**\n\n**E. Use spatially blocked cross-validation with blocks of side length at least $3\\lambda$ and a buffer of width equal to the receptive field radius $r$ only...**\nThis method uses a buffer, but the buffer width is set to $h=r$. This is insufficient. A buffer of width $r$ only prevents the receptive field of a test prediction from containing training pixels. It does not account for the spatial correlation that persists over much longer distances. With $h=r=40\\ \\mathrm{m}$ and $\\lambda=150\\ \\mathrm{m}$, the correlation between a training pixel at the buffer edge and a test pixel at the block edge would be $\\rho(40) = \\exp(-40/150) \\approx 0.766$, which is very high. The claim that this is sufficient to eliminate bias is false.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}