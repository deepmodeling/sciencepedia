{
    "hands_on_practices": [
        {
            "introduction": "Before applying machine learning, we must ensure our data preprocessing respects the underlying physics. This exercise challenges you to critically evaluate common image processing techniques, like histogram equalization, and contrast them with methods that preserve the radiometric integrity of satellite imagery . Understanding this distinction is fundamental for building models that can reliably transfer across different scenes and sensors.",
            "id": "3862730",
            "problem": "A multi-spectral satellite sensor records per-pixel digital numbers that are converted to at-sensor spectral radiance via a linear calibration of the form $L_{i,b} = g_b \\cdot \\mathrm{DN}_{i,b} + o_b$, where $i$ indexes the scene, $b \\in \\{1,\\dots,B\\}$ indexes the spectral band, $g_b$ is the band-specific gain, and $o_b$ is the band-specific offset. Under clear-sky conditions, the top-of-atmosphere reflectance $\\rho_{i,b}$ is defined by radiometry as $\\rho_{i,b} = \\dfrac{\\pi \\, L_{i,b} \\, d_i^2}{E_{0,b} \\cos \\theta_i}$, where $d_i$ is the Earth–Sun distance factor for scene $i$, $E_{0,b}$ is the extraterrestrial solar irradiance for band $b$, and $\\theta_i$ is the solar zenith angle for scene $i$. This definition follows from conservation of energy and the bidirectional reflectance distribution assumptions implicit in the top-of-atmosphere model and provides a physically meaningful, dimensionless quantity $\\rho_{i,b}$ that, in the absence of atmospheric and adjacency effects, is proportional to surface spectral reflectance.\n\nConsider using histogram equalization as a preprocessing step for training models with transfer learning across multiple geographic regions. Histogram equalization for band $b$ in scene $i$ constructs a data-dependent, strictly increasing mapping $H_{i,b}$ from the empirical cumulative distribution function of $x_{i,b}$ (either $\\mathrm{DN}_{i,b}$, $L_{i,b}$, or $\\rho_{i,b}$) so that $y_{i,b} = H_{i,b}(x_{i,b})$ yields an approximately uniform histogram on a target range. In contrast, a per-band linear scaling applies $y_{i,b} = a_b x_{i,b} + b_b$ with constants $a_b > 0$ and $b_b$ chosen once and reused consistently across scenes.\n\nIn the context of remote sensing for environmental modeling, and aiming to support transfer learning without violating physical radiometry, select all statements that correctly explain why histogram equalization may break radiometric fidelity and identify alternatives that preserve relative reflectance in a scientifically meaningful sense. For this problem, interpret “preserve relative reflectance” to mean preserving per-band ordering across pixels and preserving proportionality up to a fixed band-specific scale (i.e., preserving ratios $\\rho_{i,b}(p)/\\rho_{i,b}(q)$ within band $b$ if and only if $b_b = 0$), and interpret “consistency across scenes” to mean that transform parameters do not depend on the particular scene $i$.\n\nA. Histogram equalization is a data-dependent, non-linear mapping $y_{i,b} = H_{i,b}(x_{i,b})$ whose shape changes with the scene’s histogram, so it destroys the fixed linear relationship between $x_{i,b}$ and $\\rho_{i,b}$ and breaks cross-scene comparability of radiometry. A physically respectful alternative is to first compute top-of-atmosphere reflectance $\\rho_{i,b}$ via $L_{i,b} = g_b \\cdot \\mathrm{DN}_{i,b} + o_b$ and $\\rho_{i,b} = \\dfrac{\\pi \\, L_{i,b} \\, d_i^2}{E_{0,b} \\cos \\theta_i}$, then apply a per-band linear scaling $y_{i,b} = a_b \\rho_{i,b} + b_b$ with $a_b > 0$ and $b_b$ fixed across scenes; setting $b_b = 0$ additionally preserves within-band reflectance ratios across pixels.\n\nB. Using histogram equalization with a single, fixed lookup table constructed from one large global dataset yields a monotone mapping that is the same in every scene; monotonicity alone guarantees preservation of physical radiometry, so this approach is preferred over any linear scaling.\n\nC. Per-image standardization $y_{i,b} = \\dfrac{x_{i,b} - \\mu_{i,b}}{\\sigma_{i,b}}$ with $\\mu_{i,b}$ and $\\sigma_{i,b}$ computed from each scene, and contrast-limited adaptive histogram equalization (CLAHE), preserve relative reflectance because both are monotone in $x_{i,b}$; these methods are therefore suitable for physically consistent transfer learning.\n\nD. After radiometric calibration to top-of-atmosphere or surface reflectance, choose fixed per-band physical bounds (e.g., plausible reflectance limits over the source domain) and apply a per-band min–max scaling $y_{i,b} = a_b \\rho_{i,b} + b_b$ with $a_b > 0$ and $b_b$ computed once from those bounds, or apply global per-band standardization $y_{i,b} = \\dfrac{\\rho_{i,b} - \\mu_b}{\\sigma_b}$ with $\\mu_b, \\sigma_b$ computed over the source domain. Both are consistent across scenes, preserve per-band ordering, and with $b_b = 0$ preserve within-band reflectance ratios; they avoid the data-dependent non-linearity of histogram equalization, reducing domain shift in transfer learning.\n\nE. Cross-band whitening that mixes channels (for example, Principal Component Analysis (PCA) whitening) is preferred over per-band linear scaling because it rotates the spectral space to decorrelate bands while preserving bandwise reflectance ratios and physical interpretability.\n\nSelect all that apply.",
            "solution": "The problem asks us to evaluate statements regarding the use of histogram equalization versus other preprocessing techniques for remote sensing data, specifically in the context of transfer learning and maintaining radiometric fidelity.\n\nFirst, let's establish the fundamental principles based on the problem description.\n\n1.  **Radiometric Quantities**: The raw data are digital numbers ($\\mathrm{DN}_{i,b}$), which are converted to a physical quantity, at-sensor spectral radiance ($L_{i,b}$), via a linear transformation:\n    $$L_{i,b} = g_b \\cdot \\mathrm{DN}_{i,b} + o_b$$\n    This radiance is then converted to top-of-atmosphere (TOA) reflectance ($\\rho_{i,b}$), a dimensionless quantity that normalizes for solar illumination geometry and intensity:\n    $$\\rho_{i,b} = \\dfrac{\\pi \\, L_{i,b} \\, d_i^2}{E_{0,b} \\cos \\theta_i}$$\n    Combining these, the relationship between $\\mathrm{DN}_{i,b}$ and $\\rho_{i,b}$ is also linear for a given scene $i$:\n    $$\\rho_{i,b} = \\left( \\dfrac{\\pi \\, g_b \\, d_i^2}{E_{0,b} \\cos \\theta_i} \\right) \\mathrm{DN}_{i,b} + \\left( \\dfrac{\\pi \\, o_b \\, d_i^2}{E_{0,b} \\cos \\theta_i} \\right)$$\n    This can be written as $\\rho_{i,b} = C_{1,i,b} \\cdot \\mathrm{DN}_{i,b} + C_{2,i,b}$, where the coefficients $C_{1,i,b}$ and $C_{2,i,b}$ depend on the scene ($i$) and band ($b$). The quantity $\\rho_{i,b}$ is considered physically meaningful as it is, under ideal conditions, proportional to the intrinsic reflectance of the surface.\n\n2.  **Criteria for Good Preprocessing (as defined in the problem)**:\n    *   **Preserve Relative Reflectance**: This is defined as a two-part criterion for a transformation $T$:\n        1.  **Preserving Order**: The transformation must be strictly increasing (monotone). If $\\rho_1 > \\rho_2$, then $T(\\rho_1) > T(\\rho_2)$.\n        2.  **Preserving Proportionality (Ratios)**: The transformation should ideally be a simple scaling, $y = a_b \\rho$, to preserve ratios $\\rho(p)/\\rho(q) = y(p)/y(q)$. A general linear transform $y = a_b \\rho + b_b$ with $b_b \\ne 0$ does not preserve ratios.\n    *   **Consistency Across Scenes**: For transfer learning, the transformation parameters must not depend on the specific scene $i$. A transformation $y_{i,b} = f(x_{i,b})$ is consistent if the function $f$ is the same for all scenes. A transformation $y_{i,b} = f_i(x_{i,b})$ where the function $f_i$ changes with the scene is inconsistent.\n\nNow we analyze the transformations in question.\n\n*   **Histogram Equalization**: For a given scene $i$ and band $b$, this method computes the empirical cumulative distribution function (CDF) $F_{i,b}(x)$ from the scene's data. The transformation is $y_{i,b} = H_{i,b}(x_{i,b}) \\propto F_{i,b}(x_{i,b})$.\n    *   **Non-linear**: The CDF is generally a non-linear function of the input values. This destroys the linear relationship to the physical quantity $\\rho_{i,b}$ and does not preserve ratios or even differences.\n    *   **Scene-Dependent (Inconsistent)**: The function $F_{i,b}$ and thus the mapping $H_{i,b}$ is constructed from the histogram of scene $i$. A different scene $j$ with a different histogram (e.g., a dark forest vs. a bright desert) will produce a completely different mapping function $H_{j,b}$. A specific reflectance value, say $\\rho = 0.2$, could be mapped to a high output value in the dark scene (as it's a relatively bright pixel) and a low output value in the bright scene. This breaks cross-scene comparability.\n    *   **Monotonicity**: The CDF is a non-decreasing function; for continuous variables, it is strictly increasing. Thus, histogram equalization does preserve the order of pixel values *within* a single scene.\n\n*   **Per-band Linear Scaling**: This applies $y_{i,b} = a_b x_{i,b} + b_b$.\n    *   **Consistency**: If the parameters $a_b$ and $b_b$ are chosen once and fixed across all scenes, this method is consistent by definition.\n    *   **Radiometric Preservation**: Being linear, it preserves linear relationships. As $a_b > 0$, it is strictly increasing and preserves order. If $b_b=0$, it also preserves ratios, fully satisfying the problem's definition of preserving relative reflectance. If $b_b \\ne 0$, it maintains a simple affine relationship to the physical quantity, which is far superior to a complex non-linear mapping.\n\nWith this framework, we can evaluate each option.\n\n**A. Histogram equalization is a data-dependent, non-linear mapping $y_{i,b} = H_{i,b}(x_{i,b})$ whose shape changes with the scene’s histogram, so it destroys the fixed linear relationship between $x_{i,b}$ and $\\rho_{i,b}$ and breaks cross-scene comparability of radiometry. A physically respectful alternative is to first compute top-of-atmosphere reflectance $\\rho_{i,b}$ via $L_{i,b} = g_b \\cdot \\mathrm{DN}_{i,b} + o_b$ and $\\rho_{i,b} = \\dfrac{\\pi \\, L_{i,b} \\, d_i^2}{E_{0,b} \\cos \\theta_i}$, then apply a per-band linear scaling $y_{i,b} = a_b \\rho_{i,b} + b_b$ with $a_b > 0$ and $b_b$ fixed across scenes; setting $b_b = 0$ additionally preserves within-band reflectance ratios across pixels.**\nThis statement is a precise and correct summary of the issues with per-scene histogram equalization and the benefits of a fixed linear scaling applied to a physical quantity ($\\rho_{i,b}$). The description of histogram equalization as data-dependent, non-linear, and scene-variable is accurate. This scene-dependency breaks cross-scene comparability, which is essential for transfer learning. The proposed alternative—radiometric correction followed by a fixed linear scaling—is a standard, physically sound approach. It is consistent across scenes and preserves radiometric relationships in a simple, linear fashion. The final clause about $b_b=0$ preserving ratios is also correct.\n**Verdict: Correct.**\n\n**B. Using histogram equalization with a single, fixed lookup table constructed from one large global dataset yields a monotone mapping that is the same in every scene; monotonicity alone guarantees preservation of physical radiometry, so this approach is preferred over any linear scaling.**\nThis describes a \"global\" histogram equalization. While this approach creates a fixed, scene-independent mapping, solving the consistency problem, it remains a non-linear transformation. The claim that \"monotonicity alone guarantees preservation of physical radiometry\" is false. Physical radiometry involves magnitude and proportionality, not just order. A non-linear mapping, even if monotone, warps the radiometric scale. For instance, two intervals of reflectance of the same width, $[\\rho_1, \\rho_1+\\delta]$ and $[\\rho_2, \\rho_2+\\delta]$, will be mapped to intervals of different widths. This distortion is generally undesirable. Linear scaling preserves this relationship. Therefore, the conclusion that this non-linear approach is \"preferred over any linear scaling\" is incorrect.\n**Verdict: Incorrect.**\n\n**C. Per-image standardization $y_{i,b} = \\dfrac{x_{i,b} - \\mu_{i,b}}{\\sigma_{i,b}}$ with $\\mu_{i,b}$ and $\\sigma_{i,b}$ computed from each scene, and contrast-limited adaptive histogram equalization (CLAHE), preserve relative reflectance because both are monotone in $x_{i,b}$; these methods are therefore suitable for physically consistent transfer learning.**\nThis statement contains multiple errors. First, per-image standardization and CLAHE are scene-dependent transformations. The parameters ($\\mu_{i,b}, \\sigma_{i,b}$ or the local histograms for CLAHE) are calculated for each scene $i$. This violates the \"consistency across scenes\" requirement and makes them generally unsuitable for transfer learning without causing domain shift. A pixel value of $y=0$ after per-image standardization implies the pixel had the mean value *for that scene*, which is a different physical reflectance value from scene to scene. Second, the justification that they \"preserve relative reflectance because both are monotone\" repeats the fallacy from option B: monotonicity is not sufficient. CLAHE is also non-linear. These methods are not suitable for \"physically consistent transfer learning.\"\n**Verdict: Incorrect.**\n\n**D. After radiometric calibration to top-of-atmosphere or surface reflectance, choose fixed per-band physical bounds (e.g., plausible reflectance limits over the source domain) and apply a per-band min–max scaling $y_{i,b} = a_b \\rho_{i,b} + b_b$ with $a_b > 0$ and $b_b$ computed once from those bounds, or apply global per-band standardization $y_{i,b} = \\dfrac{\\rho_{i,b} - \\mu_b}{\\sigma_b}$ with $\\mu_b, \\sigma_b$ computed over the source domain. Both are consistent across scenes, preserve per-band ordering, and with $b_b = 0$ preserve within-band reflectance ratios; they avoid the data-dependent non-linearity of histogram equalization, reducing domain shift in transfer learning.**\nThis statement correctly identifies two valid methods for data preprocessing that align with the problem's goals. Both methods—min-max scaling with fixed bounds and standardization with global statistics—are special cases of the scene-independent linear scaling $y_{i,b} = a_b \\rho_{i,b} + b_b$. Because the parameters ($a_b, b_b$, or $\\mu_b, \\sigma_b$) are fixed across all scenes, they are \"consistent across scenes\" and thus suitable for transfer learning by reducing domain shift. They are linear transformations with positive scaling factors, so they preserve order. The statement correctly identifies that they avoid the non-linear, scene-dependent issues of histogram equalization. The clause \"with $b_b = 0$ preserve within-band reflectance ratios\" is a correct statement about a special case of such linear transformations.\n**Verdict: Correct.**\n\n**E. Cross-band whitening that mixes channels (for example, Principal Component Analysis (PCA) whitening) is preferred over per-band linear scaling because it rotates the spectral space to decorrelate bands while preserving bandwise reflectance ratios and physical interpretability.**\nThis statement is fundamentally incorrect. PCA whitening involves a transformation matrix that mixes spectral bands: $y_k = \\sum_{j=1}^B W_{kj} (\\rho_j - \\mu_j)$. The output channel $y_k$ is a linear combination of all input reflectance bands. This explicitly destroys the physical interpretability of the individual channels; $y_k$ is no longer the reflectance in a specific spectral band. Furthermore, it does not preserve bandwise reflectance ratios. The ratio of two pixels in a whitened channel, $y_k(p) / y_k(q)$, is a ratio of linear combinations of reflectances and has no direct relationship to the ratio of reflectances in any single original band, $\\rho_b(p) / \\rho_b(q)$. Therefore, whitening is contrary to the goal of preserving physical radiometry and interpretability.\n**Verdict: Incorrect.**\n\nFinal selection: Options A and D are the only correct statements.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Building on the principle of radiometric fidelity, this practice guides you through the implementation of a physically-grounded data augmentation. You will translate sensor characteristics, such as gain uncertainty and signal-to-noise ratio, into a realistic simulation of radiometric variability . This approach creates a more robust training dataset by teaching the model to be invariant to plausible calibration and noise effects.",
            "id": "3862712",
            "problem": "You are tasked with designing a physically grounded radiometric augmentation for multispectral surface reflectance in remote sensing. The augmentation perturbs each band’s reflectance by a multiplicative factor to simulate plausible radiometric calibration variability. Your derivation and implementation must begin from fundamental measurement and uncertainty principles in radiometry and must not rely on shortcut formulas.\n\nBegin from the following base facts and definitions:\n\n- Let $D_b$ denote the digital number for band $b$ and $L_b$ the at-sensor radiance for band $b$. A standard linear calibration model relates these as $L_b = g_b \\,(D_b - d_b) + o_b$, where $g_b$ is the gain, $d_b$ is the dark level, and $o_b$ is the offset. Assume the offset is negligible after preprocessing, so $L_b \\approx g_b\\,(D_b - d_b)$.\n- The unitless surface reflectance for band $b$, denoted $\\rho_b$, relates to at-sensor radiance via a well-tested radiometric relation of the form $\\rho_b = \\kappa_b \\, L_b$, where $\\kappa_b$ aggregates deterministic terms such as extraterrestrial irradiance, solar zenith angle, atmospheric transmittance, and geometric factors after atmospheric correction, so that multiplicative calibration errors in $L_b$ propagate multiplicatively to $\\rho_b$.\n- Let the gain uncertainty be represented as a relative standard uncertainty $u_b$ (one standard deviation), and let the Signal-to-Noise Ratio (SNR) for band $b$ be $s_b$ (defined as the mean signal divided by the standard deviation of noise, i.e., $s_b = \\mu_b/\\sigma_b$).\n- For a $95\\%$ confidence-like bound in a Gaussian approximation, a two-standard-deviation bound is used. Under conservative addition of independent relative uncertainties (triangle inequality), the total relative perturbation bound is\n$$\n\\alpha_b = \\min\\!\\left(2\\,u_b + \\frac{2}{s_b}, \\, \\alpha_{\\mathrm{cap}}\\right),\n$$\nwhere $\\alpha_{\\mathrm{cap}}$ is a hard cap chosen to avoid unrealistic perturbations.\n- Surface reflectance is physically unitless and lies in the closed interval $[0,1]$. For a known per-band maximum expected reflectance $r^{\\max}_b \\in (0,1]$ (derived from land surface properties and illumination conditions), bounding the multiplicative augmentation factor $f_b$ must ensure that $\\rho_b' = f_b \\,\\rho_b \\in [0,1]$ for all $\\rho_b \\in [0, r_b^{\\max}]$. This implies\n$$\nf_b \\in \\left[\\max\\!\\left(1 - \\alpha_b,\\, 0\\right),\\; \\min\\!\\left(1 + \\alpha_b,\\, \\frac{1}{r_b^{\\max}}\\right)\\right].\n$$\n- The augmentation should sample $f_b$ uniformly in the above interval for each band $b$ and then form augmented reflectance $\\rho_b' = \\mathrm{clip}(f_b \\rho_b,\\, 0,\\, 1)$ to conservatively enforce physical bounds.\n\nYour program must implement the above logic. Use the following fixed constants for all test cases:\n- Confidence scaling factor $2$ for both calibration and noise contributions.\n- Augmentation cap $\\alpha_{\\mathrm{cap}} = 0.25$.\n- Random number generator seed set to $2025$ for determinism in sampling.\n- Sampling distribution for each $f_b$ is uniform on the per-band interval defined above.\n\nInput is implicit in the provided test suite and no external input should be read. Reflectance values are unitless and must be treated as such. All relative uncertainties and $s_b$ values are dimensionless.\n\nTest Suite. For each case $i$, you are given arrays for $u_b$, $s_b$, $r_b^{\\max}$, and an observed reflectance vector $\\rho_b$ of the same length. Compute per-band bounds and one realization of augmented reflectance by sampling factors with the fixed seed:\n- Case $1$ (happy path, moderate uncertainties and high Signal-to-Noise Ratio):\n  - $u_b = [0.02,\\, 0.02,\\, 0.03,\\, 0.03]$,\n  - $s_b = [200,\\, 220,\\, 180,\\, 150]$,\n  - $r_b^{\\max} = [0.6,\\, 0.7,\\, 0.8,\\, 0.5]$,\n  - $\\rho_b = [0.1,\\, 0.2,\\, 0.3,\\, 0.4]$.\n- Case $2$ (upper bound constrained by high maximum reflectance close to unity):\n  - $u_b = [0.03,\\, 0.03]$,\n  - $s_b = [250,\\, 250]$,\n  - $r_b^{\\max} = [0.95,\\, 0.95]$,\n  - $\\rho_b = [0.9,\\, 0.95]$.\n- Case $3$ (low Signal-to-Noise Ratio bands with larger uncertainty, but below cap):\n  - $u_b = [0.05,\\, 0.04,\\, 0.05]$,\n  - $s_b = [30,\\, 40,\\, 20]$,\n  - $r_b^{\\max} = [0.7,\\, 0.6,\\, 0.4]$,\n  - $\\rho_b = [0.5,\\, 0.3,\\, 0.2]$.\n- Case $4$ (cap-active extreme, very large combined uncertainty before capping):\n  - $u_b = [0.2]$,\n  - $s_b = [10]$,\n  - $r_b^{\\max} = [0.8]$,\n  - $\\rho_b = [0.5]$.\n\nOutput requirements:\n- For each case, compute per-band bounds $[L_b, U_b]$ with $L_b = \\max(1 - \\alpha_b, 0)$ and $U_b = \\min(1 + \\alpha_b, 1/r_b^{\\max})$.\n- Using the fixed seed, sample one $f_b \\sim \\mathrm{Uniform}(L_b, U_b)$ per band and compute $\\rho_b' = \\mathrm{clip}(f_b \\rho_b, 0, 1)$.\n- Round all floating-point outputs to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of length $4$ (one per case). Each element is a list of two elements:\n  1. A list of per-band two-element lists $[L_b, U_b]$ for that case.\n  2. A list of augmented reflectances $\\rho_b'$ for that case.\n- The line must be printed as a JSON-like list with no spaces, for example: \n  - $[ [[L_1,U_1],\\ldots], [\\rho'_1,\\ldots] ]$ per case, aggregated into a top-level list of the $4$ cases.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of radiometry and error propagation, well-posed with all necessary information provided, and objective in its formulation. We will now proceed with a complete solution.\n\nThe objective is to formulate and implement a physically-grounded radiometric augmentation for multispectral surface reflectance, $\\rho_b$, where $b$ indexes the spectral band. The augmentation simulates variability arising from sensor calibration uncertainty and noise by applying a random multiplicative factor, $f_b$, to the reflectance in each band. The derivation proceeds from first principles.\n\nFirst, we establish the total relative uncertainty that bounds the augmentation. The problem provides two primary sources of uncertainty:\n$1$. The sensor gain, $g_b$, has a relative standard uncertainty of $u_b$.\n$2$. The signal itself is subject to noise, characterized by the Signal-to-Noise Ratio, $s_b$. The relative uncertainty due to noise can be taken as the inverse of the SNR, $1/s_b$.\n\nThe problem specifies using a $95\\%$ confidence-like interval, approximated by a two-standard-deviation ($2\\sigma$) bound. The uncertainties from gain and noise are treated as independent and their effects are added linearly, representing a conservative (worst-case) summation. Thus, the combined relative perturbation bound, $\\alpha_b$, for each band $b$ is constructed by summing the $2\\sigma$ contributions from both sources. This total perturbation is then capped at a maximum value, $\\alpha_{\\mathrm{cap}}$, to prevent unphysical augmentations. This is formalized as:\n$$\n\\alpha_b = \\min\\!\\left(2\\,u_b + \\frac{2}{s_b}, \\, \\alpha_{\\mathrm{cap}}\\right)\n$$\nThe value of $\\alpha_{\\mathrm{cap}}$ is given as $0.25$, and the confidence scaling factor is $2$.\n\nNext, we derive the valid interval for the multiplicative augmentation factor, $f_b$. The factor $f_b$ is intended to perturb the reflectance, so it will be sampled from an interval centered around $1$, with a width related to $\\alpha_b$. The augmented reflectance is $\\rho_b' = f_b \\rho_b$. A critical physical constraint is that any valid surface reflectance, original or augmented, must lie within the closed interval $[0, 1]$. This constraint must hold not just for the given $\\rho_b$, but for any physically plausible reflectance up to a per-band maximum expected value, $r^{\\max}_b \\in (0,1]$.\n\nThis leads to two constraints on $f_b$:\n$1$. **Lower Bound**: Since $\\rho_b \\ge 0$, to ensure $\\rho_b' = f_b \\rho_b \\ge 0$, we must have $f_b \\ge 0$. The lower bound derived from the uncertainty model is $1 - \\alpha_b$. Combining these, the lower bound for $f_b$, denoted $L_b$, is $L_b = \\max(1 - \\alpha_b, 0)$.\n$2$. **Upper Bound**: To ensure $\\rho_b' = f_b \\rho_b \\le 1$ for all possible reflectances $\\rho_b \\in [0, r_b^{\\max}]$, the most stringent condition is imposed at the maximum reflectance, $\\rho_b = r_b^{\\max}$. This gives $f_b r_b^{\\max} \\le 1$, which implies $f_b \\le 1/r_b^{\\max}$. The upper bound from the uncertainty model is $1 + \\alpha_b$. To satisfy both conditions, we take the minimum of the two. Thus, the upper bound for $f_b$, denoted $U_b$, is $U_b = \\min(1 + \\alpha_b, 1/r_b^{\\max})$.\n\nCombining these, the sampling interval for the multiplicative factor $f_b$ for each band is:\n$$\nf_b \\in [L_b, U_b] = \\left[\\max\\!\\left(1 - \\alpha_b, 0\\right), \\min\\!\\left(1 + \\alpha_b, \\frac{1}{r_b^{\\max}}\\right)\\right]\n$$\n\nThe augmentation procedure is as follows:\n$1$. For each band $b$ in a given multispectral observation, calculate the total relative perturbation bound $\\alpha_b$.\n$2$. Using $\\alpha_b$ and the per-band maximum reflectance $r_b^{\\max}$, determine the sampling interval $[L_b, U_b]$.\n$3$. For each band $b$, draw a random sample $f_b$ from a uniform distribution over its corresponding interval $[L_b, U_b]$. To ensure reproducibility, a fixed seed ($2025$) is used for the random number generator.\n$4$. Compute the augmented reflectance as $\\rho_b' = f_b \\rho_b$.\n$5$. As a final safeguard, clip the result to the physical range $[0, 1]$: $\\rho_b' = \\mathrm{clip}(f_b \\rho_b, 0, 1)$.\n\nThe program will implement this logic for the four provided test cases. For each case, it will compute the per-band bounds $[L_b, U_b]$ and a single realization of the augmented reflectance vector $\\rho_b'$. All numerical results will be rounded to $6$ decimal places and formatted into a single-line JSON-like string as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a physically grounded radiometric augmentation for multispectral surface reflectance.\n    \"\"\"\n    # Define fixed constants\n    confidence_factor = 2.0\n    alpha_cap = 0.25\n    seed = 2025\n\n    # Initialize a random number generator with a fixed seed for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # Test suite as defined in the problem statement\n    test_cases = [\n        {\n            \"u_b\": [0.02, 0.02, 0.03, 0.03],\n            \"s_b\": [200, 220, 180, 150],\n            \"r_max_b\": [0.6, 0.7, 0.8, 0.5],\n            \"rho_b\": [0.1, 0.2, 0.3, 0.4],\n        },\n        {\n            \"u_b\": [0.03, 0.03],\n            \"s_b\": [250, 250],\n            \"r_max_b\": [0.95, 0.95],\n            \"rho_b\": [0.9, 0.95],\n        },\n        {\n            \"u_b\": [0.05, 0.04, 0.05],\n            \"s_b\": [30, 40, 20],\n            \"r_max_b\": [0.7, 0.6, 0.4],\n            \"rho_b\": [0.5, 0.3, 0.2],\n        },\n        {\n            \"u_b\": [0.2],\n            \"s_b\": [10],\n            \"r_max_b\": [0.8],\n            \"rho_b\": [0.5],\n        },\n    ]\n\n    all_results = []\n\n    for case_data in test_cases:\n        # Extract data and convert to NumPy arrays for vectorized operations\n        u_b = np.array(case_data[\"u_b\"], dtype=float)\n        s_b = np.array(case_data[\"s_b\"], dtype=float)\n        r_max_b = np.array(case_data[\"r_max_b\"], dtype=float)\n        rho_b = np.array(case_data[\"rho_b\"], dtype=float)\n\n        # Step 1: Calculate the total relative perturbation bound, alpha_b\n        alpha_b = np.minimum(confidence_factor * u_b + confidence_factor / s_b, alpha_cap)\n\n        # Step 2: Calculate the bounds [L_b, U_b] for the multiplicative factor f_b\n        L_b = np.maximum(1.0 - alpha_b, 0.0)\n        U_b = np.minimum(1.0 + alpha_b, 1.0 / r_max_b)\n\n        # Step 3: Sample f_b from a uniform distribution over [L_b, U_b]\n        f_b = rng.uniform(L_b, U_b, size=u_b.shape)\n\n        # Step 4: Compute the augmented reflectance and clip to the physical range [0, 1]\n        rho_prime_b = np.clip(f_b * rho_b, 0.0, 1.0)\n        \n        # Step 5: Round results to 6 decimal places as required\n        L_b_rounded = np.round(L_b, 6)\n        U_b_rounded = np.round(U_b, 6)\n        rho_prime_b_rounded = np.round(rho_prime_b, 6)\n\n        # Structure the results for the current case\n        bounds_list = [[l, u] for l, u in zip(L_b_rounded, U_b_rounded)]\n        rhos_list = rho_prime_b_rounded.tolist()\n        \n        case_result = [bounds_list, rhos_list]\n        all_results.append(case_result)\n\n    # Print the final output in the specified compact JSON-like format\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While careful augmentation enhances robustness, transfer learning often requires explicitly addressing the domain shift between source and target datasets. This exercise introduces Correlation Alignment (CORAL), a technique designed to minimize this shift by aligning the second-order statistics of feature distributions . By implementing the CORAL loss, $L_{\\text{CORAL}}=\\lVert C_S - C_T \\rVert_F^2$, from scratch, you will gain a practical understanding of how to encourage a model to learn domain-invariant features.",
            "id": "3862784",
            "problem": "You are given two finite sets of feature vectors representing encoded crop patches from two distinct continents, one set designated as the source domain and the other as the target domain. Your task is to compute the Correlation Alignment (CORAL) loss, which penalizes the discrepancy in second-order statistics between source and target feature distributions. The CORAL loss is defined as the squared Frobenius norm of the difference between the source and target covariance matrices.\n\nStarting from the following fundamental definitions:\n\n- Let $X \\in \\mathbb{R}^{n \\times d}$ denote a data matrix with $n$ samples (rows) and $d$ features (columns).\n- The sample mean vector is $\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i$, where $x_i$ is the $i$-th row of $X$.\n- The centered data matrix is $X_c = X - \\mathbf{1}_n \\mu^\\top$, where $\\mathbf{1}_n \\in \\mathbb{R}^{n \\times 1}$ is a column vector of ones.\n- The unbiased sample covariance matrix (when $n \\ge 2$) is $C = \\frac{1}{n-1} X_c^\\top X_c \\in \\mathbb{R}^{d \\times d}$. For $n \\le 1$, use the convention $C = 0_{d \\times d}$.\n- The Frobenius norm is $\\lVert A \\rVert_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$ for any matrix $A$.\n- The CORAL loss between source and target feature sets with covariances $C_S$ and $C_T$ is\n$$\nL_{\\text{CORAL}} = \\lVert C_S - C_T \\rVert_F^2 = \\operatorname{trace}\\!\\big((C_S - C_T)^\\top(C_S - C_T)\\big).\n$$\n\nIn transfer learning, features are obtained by applying a pretrained encoder to raw inputs. In this problem, model the encoder as an affine-free linear mapping followed by a pointwise activation function. Specifically, given raw inputs $X \\in \\mathbb{R}^{n \\times m}$ and an encoder weight matrix $W \\in \\mathbb{R}^{m \\times d}$, define the encoded features as\n$$\nF = \\phi(X W),\n$$\nwhere $\\phi$ is either the identity activation $\\phi(z) = z$ applied elementwise, or the Rectified Linear Unit (ReLU), $\\phi(z) = \\max(0, z)$ applied elementwise.\n\nFor each test case below, you must:\n$1.$ Compute $F_S = \\phi(X_S W)$ and $F_T = \\phi(X_T W)$.\n$2.$ Compute unbiased sample covariance matrices $C_S$ and $C_T$ from $F_S$ and $F_T$ using $C = \\frac{1}{n-1} X_c^\\top X_c$ when $n \\ge 2$, and $C = 0_{d \\times d}$ when $n \\le 1$.\n$3.$ Compute $L_{\\text{CORAL}} = \\lVert C_S - C_T \\rVert_F^2$.\nReturn each $L_{\\text{CORAL}}$ as a floating-point number rounded to $6$ decimal places.\n\nTest suite (all matrices are given explicitly):\n\n- Case $1$ (general, identity activation):\n  - $X_S^{(1)} = \\begin{bmatrix}\n  1 & 2 & 3 \\\\\n  2 & 1 & 0 \\\\\n  0 & 1 & 2 \\\\\n  3 & 5 & 7\n  \\end{bmatrix}$,\n  $X_T^{(1)} = \\begin{bmatrix}\n  2 & 0 & 1 \\\\\n  1 & 1 & 1 \\\\\n  4 & 2 & 0 \\\\\n  0 & 3 & 6\n  \\end{bmatrix}$,\n  $W^{(1)} = \\begin{bmatrix}\n  1 & 0 \\\\\n  0 & 1 \\\\\n  1 & -1\n  \\end{bmatrix}$,\n  $\\phi$ is identity.\n\n- Case $2$ (identical domains, ReLU):\n  - $X_S^{(2)} = \\begin{bmatrix}\n  1 & 0 & 1 \\\\\n  0 & 1 & 1 \\\\\n  2 & 2 & 2\n  \\end{bmatrix}$,\n  $X_T^{(2)} = \\begin{bmatrix}\n  1 & 0 & 1 \\\\\n  0 & 1 & 1 \\\\\n  2 & 2 & 2\n  \\end{bmatrix}$,\n  $W^{(2)} = \\begin{bmatrix}\n  1 & 0 \\\\\n  0 & 1 \\\\\n  1 & -1\n  \\end{bmatrix}$,\n  $\\phi$ is ReLU, $\\phi(z) = \\max(0, z)$ elementwise.\n\n- Case $3$ (one-dimensional encoded features with ReLU, zero-variance source):\n  - $X_S^{(3)} = \\begin{bmatrix}\n  1 & 1 \\\\\n  2 & 1 \\\\\n  0 & 2\n  \\end{bmatrix}$,\n  $X_T^{(3)} = \\begin{bmatrix}\n  3 & 1 \\\\\n  1 & 0 \\\\\n  0 & 3\n  \\end{bmatrix}$,\n  $W^{(3)} = \\begin{bmatrix}\n  1 \\\\\n  -2\n  \\end{bmatrix}$,\n  $\\phi$ is ReLU, $\\phi(z) = \\max(0, z)$ elementwise.\n\n- Case $4$ (boundary condition with single source sample, identity activation):\n  - $X_S^{(4)} = \\begin{bmatrix}\n  10 & 10\n  \\end{bmatrix}$,\n  $X_T^{(4)} = \\begin{bmatrix}\n  9 & 11 \\\\\n  11 & 9\n  \\end{bmatrix}$,\n  $W^{(4)} = \\begin{bmatrix}\n  1 & 0 \\\\\n  0 & 1\n  \\end{bmatrix}$,\n  $\\phi$ is identity.\n\nImplementation requirements:\n- Use the unbiased covariance estimator $C = \\frac{1}{n-1} X_c^\\top X_c$ when $n \\ge 2$, and $C = 0_{d \\times d}$ when $n \\le 1$.\n- The final output must be a single line containing a list of the four results in order, rounded to $6$ decimal places, and formatted as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3,r_4]$ with no spaces.\n- No physical units apply in this problem. No angles are involved. Do not express any quantity as a percentage; all outputs must be floating-point numbers.\n\nYour program must compute $L_{\\text{CORAL}}$ for each case and print the single-line aggregated result as specified.",
            "solution": "The problem is well-posed and scientifically sound. Its resolution requires the systematic application of standard definitions from linear algebra and statistics. The objective is to compute the Correlation Alignment (CORAL) loss, defined as the squared Frobenius norm of the difference between the source and target sample covariance matrices: $L_{\\text{CORAL}} = \\lVert C_S - C_T \\rVert_F^2$.\n\nThe computational procedure consists of three sequential steps:\n$1$. **Feature Encoding**: The raw input data matrices, $X_S$ and $X_T$, are transformed into encoded feature matrices, $F_S$ and $F_T$, respectively. This is achieved through the specified mapping $F = \\phi(XW)$, where $\\phi$ is an element-wise activation function (either identity or ReLU).\n$2$. **Covariance Matrix Computation**: The unbiased sample covariance matrices, $C_S$ and $C_T$, are calculated from the encoded features $F_S$ and $F_T$. For a given feature matrix $F \\in \\mathbb{R}^{n \\times d}$ with $n$ samples and $d$ features, the covariance matrix $C$ is computed as follows:\n- If $n \\ge 2$, the unbiased sample covariance is $C = \\frac{1}{n-1} F_c^\\top F_c$, where $F_c = F - \\mathbf{1}_n \\mu^\\top$ is the centered data matrix and $\\mu$ is the sample mean vector of $F$.\n- If $n \\le 1$, the covariance matrix is defined as the $d \\times d$ zero matrix, $C = 0_{d \\times d}$.\n$3$. **Loss Calculation**: The CORAL loss is the sum of the squared elements of the difference matrix $C_S - C_T$. That is, $L_{\\text{CORAL}} = \\sum_{i,j} ( (C_S - C_T)_{ij} )^2$.\n\nWe now apply this procedure to each of the four test cases provided.\n\n**Case 1: General case with identity activation**\nGiven $X_S^{(1)}$, $X_T^{(1)}$, $W^{(1)}$, and $\\phi(z)=z$.\nThe encoded features are $F_S^{(1)} = X_S^{(1)} W^{(1)}$ and $F_T^{(1)} = X_T^{(1)} W^{(1)}$.\n$$\nF_S^{(1)} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 1 & 0 \\\\ 0 & 1 & 2 \\\\ 3 & 5 & 7 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{bmatrix} = \\begin{bmatrix} 4 & -1 \\\\ 2 & 1 \\\\ 2 & -1 \\\\ 10 & -2 \\end{bmatrix}\n$$\n$$\nF_T^{(1)} = \\begin{bmatrix} 2 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 4 & 2 & 0 \\\\ 0 & 3 & 6 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{bmatrix} = \\begin{bmatrix} 3 & -1 \\\\ 2 & 0 \\\\ 4 & 2 \\\\ 6 & -3 \\end{bmatrix}\n$$\nThe sample sizes are $n_S=4$ and $n_T=4$, both greater than or equal to $2$. We compute the unbiased sample covariance matrices.\nFor the source, $\\mu_S = [4.5, -0.75]$. The covariance matrix is:\n$$ C_S^{(1)} = \\frac{1}{3} \\begin{bmatrix} 43 & -10.5 \\\\ -10.5 & 4.75 \\end{bmatrix} = \\begin{bmatrix} 14.333\\dots & -3.5 \\\\ -3.5 & 1.5833\\dots \\end{bmatrix} $$\nFor the target, $\\mu_T = [3.75, -0.5]$. The covariance matrix is:\n$$ C_T^{(1)} = \\frac{1}{3} \\begin{bmatrix} 8.75 & -5.5 \\\\ -5.5 & 13 \\end{bmatrix} = \\begin{bmatrix} 2.9166\\dots & -1.8333\\dots \\\\ -1.8333\\dots & 4.3333\\dots \\end{bmatrix} $$\nThe CORAL loss is the squared Frobenius norm of $C_S^{(1)} - C_T^{(1)}$:\n$$ L_{\\text{CORAL}}^{(1)} = \\left\\lVert \\frac{1}{3} \\begin{bmatrix} 34.25 & -5 \\\\ -5 & -8.25 \\end{bmatrix} \\right\\rVert_F^2 = \\frac{1}{9} (34.25^2 + (-5)^2 + (-5)^2 + (-8.25)^2) = \\frac{1291.125}{9} = 143.458333\\dots $$\n\n**Case 2: Identical domains with ReLU activation**\nGiven $X_S^{(2)} = X_T^{(2)}$, $W^{(2)}$, and $\\phi(z)=\\max(0,z)$.\nSince the source and target raw inputs are identical ($X_S^{(2)} = X_T^{(2)}$) and they are processed by the same encoder ($W^{(2)}$ and $\\phi$), their feature representations will also be identical: $F_S^{(2)} = \\phi(X_S^{(2)}W^{(2)}) = \\phi(X_T^{(2)}W^{(2)}) = F_T^{(2)}$. Consequently, their covariance matrices must be equal, $C_S^{(2)} = C_T^{(2)}$.\nThe difference matrix $C_S^{(2)} - C_T^{(2)}$ is the zero matrix. Therefore, the loss is zero.\n$$ L_{\\text{CORAL}}^{(2)} = \\lVert 0 \\rVert_F^2 = 0 $$\n\n**Case 3: One-dimensional features with ReLU activation**\nGiven $X_S^{(3)}$, $X_T^{(3)}$, $W^{(3)}$, and $\\phi(z)=\\max(0,z)$. The encoded features will be one-dimensional ($d=1$).\n$$ Z_S^{(3)} = X_S^{(3)} W^{(3)} = \\begin{bmatrix} 1 & 1 \\\\ 2 & 1 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 0 \\\\ -4 \\end{bmatrix} \\implies F_S^{(3)} = \\phi(Z_S^{(3)}) = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} $$\nSince all source features are $0$, the variance is $0$. $C_S^{(3)} = [0]$.\n$$ Z_T^{(3)} = X_T^{(3)} W^{(3)} = \\begin{bmatrix} 3 & 1 \\\\ 1 & 0 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ -6 \\end{bmatrix} \\implies F_T^{(3)} = \\phi(Z_T^{(3)}) = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} $$\nFor the target features, $n_T=3$, $\\mu_T = [2/3]$. The centered data is $[1/3, 1/3, -2/3]^\\top$. The covariance (which is the variance for $d=1$) is:\n$$ C_T^{(3)} = \\frac{1}{3-1} \\left( (1/3)^2 + (1/3)^2 + (-2/3)^2 \\right) = \\frac{1}{2} \\left( \\frac{6}{9} \\right) = \\left[\\frac{1}{3}\\right] $$\nThe CORAL loss is:\n$$ L_{\\text{CORAL}}^{(3)} = \\lVert [0] - [1/3] \\rVert_F^2 = (-1/3)^2 = 1/9 = 0.111111\\dots $$\n\n**Case 4: Boundary condition with a single source sample**\nGiven $X_S^{(4)}$, $X_T^{(4)}$, $W^{(4)}$, and $\\phi(z)=z$.\nThe source domain has a single sample, $n_S = 1$. According to the problem definition, the covariance matrix for $n \\le 1$ is the zero matrix. The feature dimension is $d=2$.\n$$ C_S^{(4)} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} $$\nFor the target domain, $n_T = 2$. With identity activation and $W^{(4)}$ as the identity matrix, $F_T^{(4)} = X_T^{(4)} = \\begin{bmatrix} 9 & 11 \\\\ 11 & 9 \\end{bmatrix}$. The mean is $\\mu_T = [10, 10]$. The centered data is $F_{T,c}^{(4)} = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix}$. The covariance matrix is:\n$$ C_T^{(4)} = \\frac{1}{2-1} (F_{T,c}^{(4)})^\\top F_{T,c}^{(4)} = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} = \\begin{bmatrix} 2 & -2 \\\\ -2 & 2 \\end{bmatrix} $$\nThe CORAL loss is:\n$$ L_{\\text{CORAL}}^{(4)} = \\left\\lVert \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} - \\begin{bmatrix} 2 & -2 \\\\ -2 & 2 \\end{bmatrix} \\right\\rVert_F^2 = (-2)^2 + 2^2 + 2^2 + (-2)^2 = 4+4+4+4=16 $$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_covariance(F):\n    \"\"\"\n    Computes the unbiased sample covariance matrix for a feature matrix F.\n    Handles the case n <= 1 as specified in the problem.\n    \"\"\"\n    if F.ndim == 1:\n        F = F.reshape(-1, 1)\n    \n    n, d = F.shape\n\n    if n <= 1:\n        return np.zeros((d, d))\n    else:\n        mu = np.mean(F, axis=0)\n        F_c = F - mu\n        # Using (F_c.T @ F_c) / (n - 1) as per the definition\n        C = np.dot(F_c.T, F_c) / (n - 1)\n        return C\n\ndef compute_coral_loss(X_S, X_T, W, activation):\n    \"\"\"\n    Computes the CORAL loss for given source and target data, and an encoder.\n    \"\"\"\n    X_S_np = np.array(X_S, dtype=np.float64)\n    X_T_np = np.array(X_T, dtype=np.float64)\n    W_np = np.array(W, dtype=np.float64)\n\n    # 1. Encode features\n    Z_S = X_S_np @ W_np\n    Z_T = X_T_np @ W_np\n\n    if activation == 'relu':\n        F_S = np.maximum(0, Z_S)\n        F_T = np.maximum(0, Z_T)\n    else:  # identity\n        F_S = Z_S\n        F_T = Z_T\n\n    # 2. Compute covariance matrices\n    C_S = compute_covariance(F_S)\n    C_T = compute_covariance(F_T)\n\n    # 3. Compute CORAL loss\n    C_diff = C_S - C_T\n    loss = np.sum(C_diff**2)  # Squared Frobenius norm\n\n    return loss\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            [[1, 2, 3], [2, 1, 0], [0, 1, 2], [3, 5, 7]], \n            [[2, 0, 1], [1, 1, 1], [4, 2, 0], [0, 3, 6]],\n            [[1, 0], [0, 1], [1, -1]], \n            'identity'\n        ),\n        # Case 2\n        (\n            [[1, 0, 1], [0, 1, 1], [2, 2, 2]],\n            [[1, 0, 1], [0, 1, 1], [2, 2, 2]],\n            [[1, 0], [0, 1], [1, -1]],\n            'relu'\n        ),\n        # Case 3\n        (\n            [[1, 1], [2, 1], [0, 2]],\n            [[3, 1], [1, 0], [0, 3]],\n            [[1], [-2]],\n            'relu'\n        ),\n        # Case 4\n        (\n            [[10, 10]],\n            [[9, 11], [11, 9]],\n            [[1, 0], [0, 1]],\n            'identity'\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        X_S, X_T, W, activation = case\n        coral_loss = compute_coral_loss(X_S, X_T, W, activation)\n        results.append(coral_loss)\n\n    # Final print statement in the exact required format.\n    # Results are formatted to 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}