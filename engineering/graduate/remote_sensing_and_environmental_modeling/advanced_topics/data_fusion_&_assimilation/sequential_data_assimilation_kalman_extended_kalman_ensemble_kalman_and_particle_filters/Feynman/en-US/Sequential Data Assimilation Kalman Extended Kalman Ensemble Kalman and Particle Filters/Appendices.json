{
    "hands_on_practices": [
        {
            "introduction": "Before we attempt to assimilate data, we must ask a fundamental question: do our measurements contain enough information to determine the state of our system? This is the concept of \"observability,\" a core property of any state-space model that dictates whether state estimation is even possible. This exercise  provides a hands-on method to test for observability in a linear system by constructing the observability matrix, building crucial intuition for how a model's structure and measurement network determine the success of any data assimilation filter.",
            "id": "3846223",
            "problem": "A remote sensing data assimilation system for ecohydrological prediction is modeled by a discrete-time, linear time-invariant state-space representation. The state vector is $\\mathbf{x}_{k} \\in \\mathbb{R}^{3}$, representing anomalies in soil moisture, vegetation biomass, and surface temperature, and the measurement $y_{k} \\in \\mathbb{R}$ is a satellite-derived brightness temperature proxy. The dynamics and observation operator are given by\n$$\n\\mathbf{x}_{k+1} = F \\mathbf{x}_{k}, \\quad y_{k} = H \\mathbf{x}_{k},\n$$\nwith\n$$\nF = \\begin{bmatrix}\n0.9 & 0.1 & 0.0 \\\\\n0.0 & 0.95 & 0.05 \\\\\n0.2 & 0.0 & 0.85\n\\end{bmatrix}, \\quad\nH = \\begin{bmatrix}\n0.6 & 0.0 & 0.4\n\\end{bmatrix}.\n$$\nStarting from the definitions of discrete-time linear systems and the concept of observability, construct the $3 \\times 3$ observability matrix\n$$\nO = \\begin{bmatrix}\nH \\\\\nH F \\\\\nH F^{2}\n\\end{bmatrix},\n$$\nand determine its rank. State the rank condition for observability and, based on this condition, explain how observability affects the feasibility of accurate sequential data assimilation under linear-Gaussian assumptions using the Kalman Filter (KF), and implications for the Extended Kalman Filter (EKF), Ensemble Kalman Filter (EnKF), and Particle Filter (PF) in realistic remote sensing settings. Provide the rank of $O$ as your final numerical answer. No rounding is required, and no physical units are associated with the answer.",
            "solution": "The problem requires an analysis of the observability for a given discrete-time, linear time-invariant (LTI) system, which models a remote sensing data assimilation process. The state vector $\\mathbf{x}_{k} \\in \\mathbb{R}^{n}$ (with $n=3$) evolves according to $\\mathbf{x}_{k+1} = F \\mathbf{x}_{k}$, and the scalar observation is given by $y_{k} = H \\mathbf{x}_{k}$. The system matrices are provided as:\n$$\nF = \\begin{bmatrix}\n0.9 & 0.1 & 0.0 \\\\\n0.0 & 0.95 & 0.05 \\\\\n0.2 & 0.0 & 0.85\n\\end{bmatrix}, \\quad\nH = \\begin{bmatrix}\n0.6 & 0.0 & 0.4\n\\end{bmatrix}\n$$\nA system is defined as observable if the initial state $\\mathbf{x}_{0}$ can be uniquely determined from a sequence of observations $y_{0}, y_{1}, \\dots, y_{n-1}$. For an LTI system, this property is assessed by examining the rank of the observability matrix $O$. For a system with an $n$-dimensional state space, the observability matrix is constructed as:\n$$\nO = \\begin{bmatrix}\nH \\\\\nH F \\\\\n\\vdots \\\\\nH F^{n-1}\n\\end{bmatrix}\n$$\nIn this case, the state dimension is $n=3$, so the observability matrix is:\n$$\nO = \\begin{bmatrix}\nH \\\\\nH F \\\\\nH F^{2}\n\\end{bmatrix}\n$$\nThe first step is to compute the matrix products $H F$ and $H F^2$.\n\nCalculation of $H F$:\n$$\nH F = \\begin{bmatrix}\n0.6 & 0.0 & 0.4\n\\end{bmatrix}\n\\begin{bmatrix}\n0.9 & 0.1 & 0.0 \\\\\n0.0 & 0.95 & 0.05 \\\\\n0.2 & 0.0 & 0.85\n\\end{bmatrix}\n$$\n$$\nH F = \\begin{bmatrix}\n(0.6)(0.9) + (0.0)(0.0) + (0.4)(0.2) & (0.6)(0.1) + (0.0)(0.95) + (0.4)(0.0) & (0.6)(0.0) + (0.0)(0.05) + (0.4)(0.85)\n\\end{bmatrix}\n$$\n$$\nH F = \\begin{bmatrix}\n0.54 + 0.08 & 0.06 & 0.34\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.62 & 0.06 & 0.34\n\\end{bmatrix}\n$$\n\nCalculation of $H F^2$:\nFirst, we compute the matrix $F^2 = F \\times F$.\n$$\nF^2 = \\begin{bmatrix}\n0.9 & 0.1 & 0.0 \\\\\n0.0 & 0.95 & 0.05 \\\\\n0.2 & 0.0 & 0.85\n\\end{bmatrix}\n\\begin{bmatrix}\n0.9 & 0.1 & 0.0 \\\\\n0.0 & 0.95 & 0.05 \\\\\n0.2 & 0.0 & 0.85\n\\end{bmatrix}\n$$\n$$\nF^2 = \\begin{bmatrix}\n(0.9)(0.9) + (0.1)(0.0) + (0.0)(0.2) & (0.9)(0.1) + (0.1)(0.95) + (0.0)(0.0) & (0.9)(0.0) + (0.1)(0.05) + (0.0)(0.85) \\\\\n(0.0)(0.9) + (0.95)(0.0) + (0.05)(0.2) & (0.0)(0.1) + (0.95)(0.95) + (0.05)(0.0) & (0.0)(0.0) + (0.95)(0.05) + (0.05)(0.85) \\\\\n(0.2)(0.9) + (0.0)(0.0) + (0.85)(0.2) & (0.2)(0.1) + (0.0)(0.95) + (0.85)(0.0) & (0.2)(0.0) + (0.0)(0.05) + (0.85)(0.85)\n\\end{bmatrix}\n$$\n$$\nF^2 = \\begin{bmatrix}\n0.81 & 0.09 + 0.095 & 0.005 \\\\\n0.01 & 0.9025 & 0.0475 + 0.0425 \\\\\n0.18 + 0.17 & 0.02 & 0.7225\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.81 & 0.185 & 0.005 \\\\\n0.01 & 0.9025 & 0.09 \\\\\n0.35 & 0.02 & 0.7225\n\\end{bmatrix}\n$$\nNow, we can compute $H F^2 = (H) (F^2)$.\n$$\nH F^2 = \\begin{bmatrix}\n0.6 & 0.0 & 0.4\n\\end{bmatrix}\n\\begin{bmatrix}\n0.81 & 0.185 & 0.005 \\\\\n0.01 & 0.9025 & 0.09 \\\\\n0.35 & 0.02 & 0.7225\n\\end{bmatrix}\n$$\n$$\nH F^2 = \\begin{bmatrix}\n(0.6)(0.81) + (0.4)(0.35) & (0.6)(0.185) + (0.4)(0.02) & (0.6)(0.005) + (0.4)(0.7225)\n\\end{bmatrix}\n$$\n$$\nH F^2 = \\begin{bmatrix}\n0.486 + 0.140 & 0.111 + 0.008 & 0.003 + 0.289\n\\end{bmatrix}\n= \\begin{bmatrix}\n0.626 & 0.119 & 0.292\n\\end{bmatrix}\n$$\nWith the component rows calculated, the observability matrix $O$ is assembled:\n$$\nO = \\begin{bmatrix}\nH \\\\\nH F \\\\\nH F^{2}\n\\end{bmatrix} = \\begin{bmatrix}\n0.6 & 0.0 & 0.4 \\\\\n0.62 & 0.06 & 0.34 \\\\\n0.626 & 0.119 & 0.292\n\\end{bmatrix}\n$$\nThe rank condition for observability states that the system is observable if and only if the observability matrix $O$ has full rank. For this system, full rank means $\\text{rank}(O) = n = 3$. The rank of a square matrix is full if and only if its determinant is non-zero. We calculate the determinant of $O$:\n$$\n\\det(O) = 0.6 \\begin{vmatrix} 0.06 & 0.34 \\\\ 0.119 & 0.292 \\end{vmatrix} - 0.0 \\begin{vmatrix} 0.62 & 0.34 \\\\ 0.626 & 0.292 \\end{vmatrix} + 0.4 \\begin{vmatrix} 0.62 & 0.06 \\\\ 0.626 & 0.119 \\end{vmatrix}\n$$\n$$\n\\det(O) = 0.6((0.06)(0.292) - (0.34)(0.119)) + 0.4((0.62)(0.119) - (0.06)(0.626))\n$$\n$$\n\\det(O) = 0.6(0.01752 - 0.04046) + 0.4(0.07378 - 0.03756)\n$$\n$$\n\\det(O) = 0.6(-0.02294) + 0.4(0.03622)\n$$\n$$\n\\det(O) = -0.013764 + 0.014488 = 0.000724\n$$\nSince $\\det(O) = 0.000724 \\neq 0$, the matrix $O$ is invertible and has full rank. Thus, $\\text{rank}(O) = 3$. According to the rank condition, the system is observable.\n\nThe observability of the system has profound implications for the feasibility of sequential data assimilation.\nFor the Kalman Filter (KF), which is the optimal linear unbiased estimator for linear systems with Gaussian noise, observability is a necessary and sufficient condition for the estimate error covariance matrix to converge to a unique, positive definite steady-state solution. This ensures that the filter's estimate of the state will converge toward the true state over time and that the uncertainty of the estimate will be bounded. If the system were unobservable, certain states or linear combinations of states could not be inferred from the measurements, and the error covariance components associated with these unobservable directions would not be reduced by the assimilation of data.\n\nFor nonlinear systems, as are common in realistic remote sensing settings, filters like the Extended Kalman Filter (EKF), Ensemble Kalman Filter (EnKF), and Particle Filter (PF) are used. The concept of observability remains crucial.\nThe EKF linearizes the system dynamics around the current estimate. Observability is then evaluated for this time-varying linearized system. If the system is persistently \"unobservable\" or \"weakly observable,\" the EKF can become numerically unstable and diverge.\nThe EnKF and PF do not require explicit linearization but are still fundamentally limited by observability. If a state component is unobservable, the measurements provide no information to update it. In the EnKF, this means the ensemble spread for that component is not reduced by the analysis step, and spurious correlations can degrade the estimation of other states. In the PF, the weights of the particles are not influenced by unobservable state components, which can lead to particle degeneracy and consequent filter failure, as resampling will fail to correct the distribution of the unobservable state.\nIn conclusion, the observability of this specific ecohydrological model, as demonstrated by the full rank of its observability matrix, is a prerequisite for the successful application of any of these data assimilation filters to accurately estimate the system state.\nThe rank of the observability matrix $O$ is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "The Ensemble Kalman Filter (EnKF) is a powerful tool for the high-dimensional systems found in environmental modeling, but its reliance on a finite ensemble introduces sampling errors that create spurious, unphysical correlations between distant variables. Covariance localization is the essential technique used to solve this problem by tapering the sample covariance based on physical distance. This hands-on problem  guides you through implementing a realistic localization scheme using the standard Gaspari-Cohn function, demonstrating how it directly modifies the Kalman gain to ensure observations only influence nearby state variables.",
            "id": "3846198",
            "problem": "Consider a one-dimensional gridded environmental model representing a state of soil moisture along a transect, intended for sequential data assimilation in remote sensing. The state vector is denoted by $\\mathbf{x} \\in \\mathbb{R}^{n}$ with $n$ grid points, and there are $m$ observations derived from satellite footprints. The observation operator $\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$ maps the state to observation space as a linear averaging within each footprint. Observational errors are assumed unbiased, independent of $\\mathbf{x}$, and Gaussian with covariance $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$.\n\nThe fundamental base is the Bayesian linear Gaussian update and the least-squares optimality, which together imply that the Kalman Filter (KF) gain emerges from the prior covariance and the linear observation model. The Ensemble Kalman Filter (EnKF) uses a finite ensemble to construct a sample covariance, which can suffer from spurious long-range correlations. Covariance localization mitigates this through a compactly supported correlation function. You are given a Gaspari–Cohn localization function $\\rho(r)$ with localization radius $L>0$, used to construct a localization matrix $\\mathbf{L} \\in \\mathbb{R}^{n \\times n}$ by $\\mathbf{L}_{ij} = \\rho(r_{ij})$, where $r_{ij}$ is the physical distance between grid points $i$ and $j$. The localization is applied to the prior covariance via an element-wise (Hadamard) product.\n\nYour task is to write a complete program that:\n- Constructs the grid and the observation operator.\n- Generates a prior ensemble from a zero-mean Gaussian field with a specified physically plausible covariance function and computes the sample prior covariance $\\mathbf{P}_{xx} \\in \\mathbb{R}^{n \\times n}$.\n- Constructs the localization matrix $\\mathbf{L}$ from $\\rho(r)$ and $L$ for the grid.\n- Computes the unlocalized Kalman gain using the standard KF definition arising from the Bayesian linear Gaussian framework and least-squares optimality, and the localized Kalman gain by applying the localization to the prior covariance.\n- Returns a quantitative comparison metric between the localized and unlocalized gains for each test case.\n\nAll distances in the construction must be treated in kilometers, and angles do not appear. The final metric must be dimensionless (no physical units). The results must be floats.\n\nGrid and observation setup:\n- Let $n = 20$ grid points uniformly spaced by $\\Delta x = 10$ km along a transect from $0$ km to $190$ km.\n- Let $m = 3$ observations with footprint centers at $50$ km, $100$ km, and $150$ km. Each observation averages all grid points within a footprint radius $r_{\\mathrm{fp}} = 15$ km using uniform weights that sum to $1$ across the contributing grid points for each observation row of $\\mathbf{H}$.\n- The observation error covariance is $\\mathbf{R} = \\operatorname{diag}(s^2, s^2, s^2)$ with $s^2 = 0.01$.\n\nPrior ensemble:\n- Construct an ensemble of size $M = 100$ with zero mean and a squared-exponential covariance $C_{ij} = \\sigma^2 \\exp\\!\\left(-\\frac{r_{ij}^2}{2 \\ell^2}\\right)$, where $\\sigma^2 = 0.04$ and correlation length $\\ell = 40$ km. Use this $\\mathbf{C}$ to sample $M$ realizations and compute the sample covariance $\\mathbf{P}_{xx}$.\n\nLocalization and gain computation:\n- Construct $\\mathbf{L}$ using $\\rho(r)$ and radius $L$ as $\\mathbf{L}_{ij} = \\rho(r_{ij})$, where $r_{ij}$ is the physical distance between grid points $i$ and $j$.\n- Compute the unlocalized Kalman gain using standard KF logic from the Bayesian linear Gaussian foundation.\n- Compute the localized Kalman gain by replacing $\\mathbf{P}_{xx}$ with the localized covariance $\\mathbf{P}^{\\mathrm{loc}}_{xx} = \\mathbf{P}_{xx} \\circ \\mathbf{L}$, where $\\circ$ is the Hadamard product.\n- For each test case, compute the Frobenius norm of the difference between the localized and unlocalized gains, namely $\\left\\|\\mathbf{K}_{\\mathrm{loc}} - \\mathbf{K}\\right\\|_{F}$, as a single float.\n\nTest suite:\n- Case $1$: $L = 5$ km.\n- Case $2$: $L = 30$ km.\n- Case $3$: $L = 1000$ km.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[result1,result2,result3]`). The three floats must be the Frobenius norms for the three cases in the order listed above. No additional text should be printed. Randomness must be controlled by fixing a reproducible seed so that the outputs are deterministic across runs.",
            "solution": "The core of this problem lies in computing the Kalman gain, a matrix that optimally combines prior state information with new observations. The optimality is derived from minimizing the analysis error covariance under linear-Gaussian assumptions, which is equivalent to a Bayesian update of the state estimate. The standard Kalman gain $\\mathbf{K}$ is given by:\n$$\n\\mathbf{K} = \\mathbf{P}_{xx} \\mathbf{H}^T (\\mathbf{H} \\mathbf{P}_{xx} \\mathbf{H}^T + \\mathbf{R})^{-1}\n$$\nwhere $\\mathbf{P}_{xx}$ is the prior state error covariance, $\\mathbf{H}$ is the observation operator, and $\\mathbf{R}$ is the observation error covariance. In the context of EnKF, $\\mathbf{P}_{xx}$ is a sample covariance estimated from an ensemble, which can contain spurious long-range correlations due to finite ensemble size. Covariance localization is a technique to mitigate this by filtering the sample covariance.\n\nThe solution is constructed in the following steps:\n1.  Define the model grid and construct the linear observation operator $\\mathbf{H}$.\n2.  Generate a prior ensemble from a specified true covariance function $\\mathbf{C}$ and compute the sample prior covariance $\\mathbf{P}_{xx}$.\n3.  Construct the localization matrix $\\mathbf{L}$ based on the Gaspari-Cohn function for each specified localization radius.\n4.  Compute the unlocalized Kalman gain $\\mathbf{K}$ and the localized gain $\\mathbf{K}_{\\mathrm{loc}}$.\n5.  Calculate the Frobenius norm of the difference, $\\|\\mathbf{K}_{\\mathrm{loc}} - \\mathbf{K}\\|_{F}$, for each test case.\n\n**1. Grid and Observation Operator $\\mathbf{H}$**\n\nThe state vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ represents soil moisture on a one-dimensional grid of $n=20$ points. The grid points are uniformly spaced by $\\Delta x = 10 \\text{ km}$, with positions $x_i = i \\cdot \\Delta x$ for $i = 0, 1, \\dots, 19$. The physical domain spans from $0 \\text{ km}$ to $190 \\text{ km}$.\n\nThe observation operator $\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$ with $m=3$ and $n=20$ maps the state space to the observation space. Each of the $m=3$ observations represents an average of the state values over a specific footprint. The footprint centers are located at $c_1=50 \\text{ km}$, $c_2=100 \\text{ km}$, and $c_3=150 \\text{ km}$. Each footprint has a radius of $r_{\\mathrm{fp}} = 15 \\text{ km}$.\n\nAn element $H_{ji}$ of the observation operator is non-zero if grid point $i$ (at position $x_i$) falls within the footprint of observation $j$ (centered at $c_j$), i.e., if $|x_i - c_j| \\le r_{\\mathrm{fp}}$. The weights are uniform, so for each observation $j$, if $N_j$ is the number of grid points within its footprint, then $H_{ji} = 1/N_j$ for those points and $H_{ji} = 0$ otherwise.\n\n-   **Observation 1 ($c_1=50 \\text{ km}$):** The footprint covers $[35, 65] \\text{ km}$. The grid points in this range are $x_4=40 \\text{ km}$, $x_5=50 \\text{ km}$, and $x_6=60 \\text{ km}$. Thus, $N_1=3$, and the first row of $\\mathbf{H}$ has entries $H_{1,4}=H_{1,5}=H_{1,6}=1/3$.\n-   **Observation 2 ($c_2=100 \\text{ km}$):** The footprint covers $[85, 115] \\text{ km}$. The grid points are $x_9=90 \\text{ km}$, $x_{10}=100 \\text{ km}$, and $x_{11}=110 \\text{ km}$. Thus, $N_2=3$, and the second row has entries $H_{2,9}=H_{2,10}=H_{2,11}=1/3$.\n-   **Observation 3 ($c_3=150 \\text{ km}$):** The footprint covers $[135, 165] \\text{ km}$. The grid points are $x_{14}=140 \\text{ km}$, $x_{15}=150 \\text{ km}$, and $x_{16}=160 \\text{ km}$. Thus, $N_3=3$, and the third row has entries $H_{3,14}=H_{3,15}=H_{3,16}=1/3$.\n\nAll other elements of $\\mathbf{H}$ are zero. The observation error covariance is given as $\\mathbf{R} = s^2 \\mathbf{I}_3$, with $s^2=0.01$, where $\\mathbf{I}_3$ is the $3 \\times 3$ identity matrix.\n\n**2. Prior Ensemble and Sample Covariance $\\mathbf{P}_{xx}$**\n\nThe prior belief about the state is modeled as a zero-mean Gaussian random field. The true covariance between any two grid points $i$ and $j$ is given by the squared-exponential function:\n$$\nC_{ij} = \\sigma^2 \\exp\\left(-\\frac{r_{ij}^2}{2 \\ell^2}\\right)\n$$\nwhere $\\sigma^2 = 0.04$ is the variance, $\\ell = 40 \\text{ km}$ is the correlation length, and $r_{ij} = |x_i - x_j| = |i-j| \\Delta x$ is the physical distance between the points.\n\nAn ensemble of $M=100$ state vectors, $\\{\\mathbf{x}^{(k)}\\}_{k=1}^{M}$, is drawn from the multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\mathbf{C})$. A fixed random seed ensures reproducibility. From this ensemble, the sample covariance matrix $\\mathbf{P}_{xx} \\in \\mathbb{R}^{n \\times n}$ is computed as:\n$$\n\\mathbf{P}_{xx} = \\frac{1}{M-1} \\sum_{k=1}^{M} (\\mathbf{x}^{(k)} - \\bar{\\mathbf{x}})(\\mathbf{x}^{(k)} - \\bar{\\mathbf{x}})^T\n$$\nwhere $\\bar{\\mathbf{x}}$ is the sample mean of the ensemble.\n\n**3. Covariance Localization and the Matrix $\\mathbf{L}$**\n\nTo counteract spurious long-range correlations in $\\mathbf{P}_{xx}$, localization is applied via an element-wise product with a localization matrix $\\mathbf{L}$. The elements of $\\mathbf{L}$ are defined by the Gaspari-Cohn 5th-order compactly supported correlation function, $\\rho(r)$, as $L_{ij} = \\rho(r_{ij})$. This function depends on the physical distance $r_{ij}$ and a specified localization radius $L$. We interpret $L$ as the characteristic scale parameter of the function, such that its support extends to a distance of $2L$. The argument to the polynomial is $z = r/L$.\n\nThe Gaspari-Cohn function is defined piecewise:\n$$\n\\rho(z) =\n\\begin{cases}\n-\\frac{1}{4}z^5 + \\frac{1}{2}z^4 + \\frac{5}{8}z^3 - \\frac{5}{3}z^2 + 1 & \\text{if } 0 \\le z \\le 1 \\\\\n\\frac{1}{12}z^5 - \\frac{1}{2}z^4 + \\frac{5}{8}z^3 + \\frac{5}{3}z^2 - 5z + 4 - \\frac{2}{3z} & \\text{if } 1 < z \\le 2 \\\\\n0 & \\text{if } z > 2\n\\end{cases}\n$$\nFor each test case ($L=5, 30, 1000 \\text{ km}$), a corresponding matrix $\\mathbf{L}$ is constructed by evaluating $\\rho(r_{ij}/L)$ for all pairs of grid points $(i, j)$.\n\n**4. Kalman Gain Computation**\n\nThe unlocalized Kalman gain, $\\mathbf{K}$, is computed using the standard formula with the sample covariance $\\mathbf{P}_{xx}$. The localized covariance is first computed:\n$$\n\\mathbf{P}^{\\mathrm{loc}}_{xx} = \\mathbf{P}_{xx} \\circ \\mathbf{L}\n$$\nwhere $\\circ$ denotes the Hadamard (element-wise) product. This localized covariance is then used to compute the localized Kalman gain, $\\mathbf{K}_{\\mathrm{loc}}$:\n$$\n\\mathbf{K}_{\\mathrm{loc}} = \\mathbf{P}^{\\mathrm{loc}}_{xx} \\mathbf{H}^T (\\mathbf{H} \\mathbf{P}^{\\mathrm{loc}}_{xx} \\mathbf{H}^T + \\mathbf{R})^{-1}\n$$\n\n**5. Comparison Metric**\n\nFinally, for each test case, the quantitative difference between the two gains is measured by the Frobenius norm of their difference matrix:\n$$\n\\text{metric} = \\left\\|\\mathbf{K}_{\\mathrm{loc}} - \\mathbf{K}\\right\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{m} |(K_{\\mathrm{loc}})_{ij} - K_{ij}|^2}\n$$\nThe results for the three test cases are then collected.\n\n-   For $L=5 \\text{ km}$, which is less than the grid spacing of $10 \\text{ km}$, $r_{ij}/L \\ge 2$ for all $i \\neq j$. Thus $\\mathbf{L}$ becomes the identity matrix, localization is maximal, and the difference is expected to be large.\n-   For $L=1000 \\text{ km}$, which is much larger than the domain size of $190 \\text{ km}$, $r_{ij}/L$ is very small for all pairs. Thus $\\rho(r_{ij}/L) \\approx 1$, making $\\mathbf{L}$ a matrix of ones. Localization has minimal effect, and the difference is expected to be very small.\n-   For $L=30 \\text{ km}$, the localization effect is intermediate, resulting in a metric value between the other two cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes an EnKF-related metric for three test cases.\n    \"\"\"\n    # Fix seed for reproducibility\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    # 1. Grid and Observation Setup\n    n = 20  # Number of grid points\n    dx = 10.0  # Grid spacing in km\n    grid_points = np.arange(n) * dx\n\n    m = 3  # Number of observations\n    obs_centers = np.array([50.0, 100.0, 150.0])\n    r_fp = 15.0  # Footprint radius in km\n    s2 = 0.01  # Observation error variance\n\n    # Construct observation operator H\n    H = np.zeros((m, n))\n    for j in range(m):\n        center = obs_centers[j]\n        # Find grid points within the footprint\n        indices = np.where(np.abs(grid_points - center) <= r_fp)[0]\n        if indices.size > 0:\n            H[j, indices] = 1.0 / indices.size\n\n    # Construct observation error covariance matrix R\n    R = np.diag(np.full(m, s2))\n\n    # 2. Prior Ensemble Generation\n    M = 100  # Ensemble size\n    sigma2 = 0.04  # Prior variance\n    ell = 40.0  # Correlation length in km\n\n    # Construct true prior covariance matrix C\n    dist_matrix = np.abs(np.subtract.outer(grid_points, grid_points))\n    C = sigma2 * np.exp(-dist_matrix**2 / (2 * ell**2))\n\n    # Generate ensemble\n    # multivariate_normal returns (size, dim), we want (dim, size) for our convention\n    mean_vec = np.zeros(n)\n    ensemble = rng.multivariate_normal(mean_vec, C, size=M).T\n\n    # Compute sample prior covariance P_xx\n    # np.cov expects (features, samples), which matches our (n, M) shape\n    P_xx = np.cov(ensemble, rowvar=True)\n    \n    # Define the Gaspari-Cohn localization function\n    def gaspari_cohn(r, L):\n        \"\"\"\n        Computes the Gaspari-Cohn 5th-order correlation function.\n        r: distance array\n        L: localization radius parameter\n        \"\"\"\n        # Suppress divide-by-zero warnings for r=0, which is handled\n        with np.errstate(divide='ignore', invalid='ignore'):\n            z = np.abs(r) / L\n        \n        rho = np.zeros_like(z)\n        \n        # Case 1: 0 <= z <= 1\n        mask1 = z <= 1\n        z1 = z[mask1]\n        rho[mask1] = ((-1/4 * z1**5) + (1/2 * z1**4) + (5/8 * z1**3) - \n                      (5/3 * z1**2) + 1)\n        \n        # Case 2: 1 < z <= 2\n        mask2 = (z > 1) & (z <= 2)\n        z2 = z[mask2]\n        # Check for z2 being zero to avoid division by zero, although z > 1\n        # should prevent this. It is good practice.\n        non_zero_z2 = z2 != 0\n        rho_z2 = np.zeros_like(z2)\n        if np.any(non_zero_z2):\n             rho_z2[non_zero_z2] = ((1/12 * z2[non_zero_z2]**5) - \\\n               (1/2 * z2[non_zero_z2]**4) + (5/8 * z2[non_zero_z2]**3) + \\\n               (5/3 * z2[non_zero_z2]**2) - (5 * z2[non_zero_z2]) + 4 - \\\n               (2/3 / z2[non_zero_z2]))\n        rho[mask2] = rho_z2\n        \n        return rho\n\n    # 3. Compute unlocalized Kalman Gain\n    # K = P_xx H^T (H P_xx H^T + R)^-1\n    H_Pxx_HT = H @ P_xx @ H.T\n    inv_term_unloc = np.linalg.inv(H_Pxx_HT + R)\n    K_unloc = P_xx @ H.T @ inv_term_unloc\n\n    # Test suite\n    test_cases = [5.0, 30.0, 1000.0]  # Localization radii L in km\n    results = []\n\n    for L in test_cases:\n        # a. Construct localization matrix L\n        loc_matrix = gaspari_cohn(dist_matrix, L)\n        \n        # b. Compute localized covariance and gain\n        P_loc = P_xx * loc_matrix  # Hadamard product\n        \n        H_Ploc_HT = H @ P_loc @ H.T\n        inv_term_loc = np.linalg.inv(H_Ploc_HT + R)\n        K_loc = P_loc @ H.T @ inv_term_loc\n        \n        # c. Compute the Frobenius norm of the difference\n        diff_norm = np.linalg.norm(K_loc - K_unloc, 'fro')\n        results.append(diff_norm)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Particle filters tackle nonlinearity by representing probability distributions with a cloud of weighted samples, or 'particles'. To remain effective, the filter must periodically focus its computational resources on promising regions of the state space by eliminating particles with low importance and replicating those with high importance. In this exercise , you will implement systematic resampling—an elegant and efficient algorithm for this task—and derive its statistical properties, revealing exactly why it is a preferred method in modern particle filters.",
            "id": "3846225",
            "problem": "You are working with Sequential Importance Resampling (SIR) Particle Filters as used in sequential data assimilation for remote sensing and environmental modeling. Let there be a sequence of normalized importance weights $w_{1},\\dots,w_{N}$ for $N$ particles, where each $w_{i} \\ge 0$ and $\\sum_{i=1}^{N} w_{i} = 1$. Systematic resampling generates $N$ new particle indices using a single random offset $u$ and the cumulative distribution function of the weights. The resampling rule can be formalized through the quantile transform principle applied to a discrete distribution: draw deterministically located points that are uniformly stratified over the unit interval and map them through the inverse cumulative distribution function.\n\nStarting from the following fundamental base:\n- The definition of a discrete probability distribution and its cumulative distribution function $F(i) = \\sum_{k=1}^{i} w_{k}$.\n- The quantile transform characterization of sampling from a distribution by inverting its cumulative distribution function.\n- The fact that a uniform partition of $[0,1)$ into $N$ strata of equal length $1/N$ yields stratified points when one uniform offset is used for all strata.\n\nTasks:\n1) Using only these principles, derive a one-pass algorithm that maps the $N$ stratified points $t_{j} = \\frac{u + j}{N}$ for $j \\in \\{0,1,\\dots,N-1\\}$, where $u \\in [0,1)$, to particle indices via the cumulative distribution function of the weights, thereby returning the vector of offspring counts $N_{1},\\dots,N_{N}$, where $N_{i}$ is the number of stratified points that fall into the interval $[F(i-1), F(i))$ with $F(0) = 0$. Your algorithm must visit the weights and the stratified points in a single linear pass.\n\n2) From first principles, derive the exact distribution of each offspring count $N_{i}$ as a function of $N$ and $w_{i}$ when $u$ is distributed uniformly on $[0,1)$. In particular, derive the support and the associated probabilities. Express your answer as explicit functions of $N$ and $w_{i}$.\n\nYour program must:\n- Implement the systematic resampling offspring count mapping in one pass as specified above.\n- For each particle index $i$, compute the pair consisting of the integer $m_{i} = \\lfloor N w_{i} \\rfloor$ and the probability $p_{i} = \\mathbb{P}\\big(N_{i} = m_{i} + 1\\big)$ under $u \\sim \\mathrm{Uniform}[0,1)$, which fully characterizes the two-point distribution of $N_{i}$.\n- For each test case below, output a list of three items: the realized offspring count vector $[N_{1},\\dots,N_{N}]$ for the provided $u$, the list $[m_{1},\\dots,m_{N}]$, and the list $[p_{1},\\dots,p_{N}]$.\n\nTest suite:\n- Case A (general nonuniform weights): $N = 6$, weights $[0.05, 0.15, 0.2, 0.1, 0.3, 0.2]$, $u = 0.13$.\n- Case B (uniform weights): $N = 5$, weights $[0.2, 0.2, 0.2, 0.2, 0.2]$, $u = 0.7$.\n- Case C (one dominant weight): $N = 8$, weights $[0.7, 0.3/7, 0.3/7, 0.3/7, 0.3/7, 0.3/7, 0.3/7, 0.3/7]$, $u = 0.9$.\n- Case D (zero weights present): $N = 4$, weights $[0.0, 0.5, 0.5, 0.0]$, $u = 0.25$.\n- Case E (singleton boundary case): $N = 1$, weights $[1.0]$, $u = 0.33$.\n\nAll $w_{i}$ are dimensionless and the variable $u$ is dimensionless. There are no physical units in this problem. Angles are not involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of three lists. For example, the output should look like: `[[countsA,mA,pA],[countsB,mB,pB],...]`.\n- All lists should be printed with no spaces.\n- All numerical outputs must be given as plain decimal numbers.\n\nYour code must be self-contained and produce the output for the above test suite exactly as specified. No user input is required.",
            "solution": "### Derivation of the One-Pass Resampling Algorithm\n\nThe goal is to compute the offspring counts $N_i$ for $i \\in \\{1, \\dots, N\\}$. The count $N_i$ is the number of stratified points $t_j = (u+j)/N$ that lie in the interval $[F(i-1), F(i))$. This task requires an algorithm that processes the sequence of weights and the sequence of stratified points in a single linear scan, avoiding nested loops or pre-computation of the full cumulative sum vector.\n\nThe key insight is that both the stratified points $t_j$ and the cumulative sum boundaries $F(i)$ are monotonically non-decreasing. This structure allows for a \"merge-like\" comparison. We can iterate through the stratified points and, for each point, advance a pointer through the cumulative weight intervals until the correct interval is found.\n\nLet us use $0$-based indexing for implementation convenience, so particles are indexed $i \\in \\{0, \\dots, N-1\\}$ and weights are $w_0, \\dots, w_{N-1}$. The cumulative sum is $F(i) = \\sum_{k=0}^{i} w_k$ and the interval for particle $i$ is $[F(i-1), F(i))$, with $F(-1)=0$.\n\nThe algorithm proceeds as follows:\n1. Initialize an array of offspring counts, `counts`, of size $N$ to all zeros.\n2. Initialize the particle index `i = 0`.\n3. Initialize the upper bound of the current particle's interval, `C_upper_bound` $= w_0$. This corresponds to $F(0)$.\n4. Iterate through the new sample indices $j$ from $0$ to $N-1$.\n   a. Calculate the current stratified point: $t_j = (u+j)/N$.\n   b. The point $t_j$ belongs to particle $i$ if $t_j  F(i)$. As we advance $j$, $t_j$ increases. We may need to move to the next particle interval. We do this by advancing $i$ as long as $t_j$ is greater than or equal to the current upper bound `C_upper_bound`.\n      ```\n      while (i  N-1 and t_j >= C_upper_bound):\n          i = i + 1\n          C_upper_bound = C_upper_bound + w[i]\n      ```\n      The condition `i  N-1` prevents an out-of-bounds access on $w_i$ and correctly assigns all remaining points to the last particle.\n   c. Once the `while` loop terminates, we have found the correct particle index $i$ for the point $t_j$. Increment its count: `counts[i] = counts[i] + 1`.\n5. After the loop over $j$ is complete, the `counts` array contains the final offspring counts $N_0, \\dots, N_{N-1}$.\n\nThis algorithm performs a single pass over the stratified points (the $j$ loop) and a single pass over the particle intervals (the $i$ index). The total number of operations is proportional to $N$, making it a linear time, one-pass algorithm.\n\n### Derivation of the Offspring Count Distribution\n\nWe seek the probability distribution of $N_i$ for a given particle $i$, when $u$ is drawn from a uniform distribution $U[0,1)$. The number of offspring $N_i$ is the count of integers $j \\in \\{0, 1, \\dots, N-1\\}$ satisfying the condition:\n$$ F(i-1) \\le \\frac{u+j}{N}  F(i) $$\nThe number of offspring $N_i$ can be expressed as:\n$$ N_i = \\lfloor N \\cdot F(i) - u \\rfloor - \\lfloor N \\cdot F(i-1) - u \\rfloor $$\nLet $X_{i-1} = N \\cdot F(i-1)$ and the interval length $L_i = N \\cdot w_i$. Note that $N \\cdot F(i) = X_{i-1} + L_i$.\nThe expression for $N_i$ becomes:\n$$ N_i = \\lfloor X_{i-1} + L_i - u \\rfloor - \\lfloor X_{i-1} - u \\rfloor $$\nLet $y = X_{i-1} - u$. Then $N_i = \\lfloor y + L_i \\rfloor - \\lfloor y \\rfloor$.\nWe can decompose the length $L_i = N \\cdot w_i$ into its integer and fractional parts: $L_i = \\lfloor N w_i \\rfloor + \\{N w_i\\}$. Let $m_i = \\lfloor N w_i \\rfloor$ and $r_i = \\{N w_i\\}$.\nUsing the property $\\lfloor z+k \\rfloor = \\lfloor z \\rfloor + k$ for any integer $k$:\n$$ N_i = \\lfloor y + r_i + m_i \\rfloor - \\lfloor y \\rfloor = \\lfloor y + r_i \\rfloor + m_i - \\lfloor y \\rfloor $$\nNow, decompose $y$ into its integer and fractional parts: $y = \\lfloor y \\rfloor + \\{y\\}$.\n$$ N_i = m_i + \\lfloor (\\lfloor y \\rfloor + \\{y\\}) + r_i \\rfloor - \\lfloor y \\rfloor = m_i + \\lfloor y \\rfloor + \\lfloor \\{y\\} + r_i \\rfloor - \\lfloor y \\rfloor = m_i + \\lfloor \\{y\\} + r_i \\rfloor $$\nThe variable $y = N \\cdot F(i-1) - u$. Since $u \\sim U[0,1)$, the fractional part $\\{y\\} = \\{N \\cdot F(i-1) - u\\}$ is also uniformly distributed on $[0,1)$. Let's denote this random variable by $\\xi = \\{y\\}$, so $\\xi \\sim U[0,1)$.\nThe expression for the random variable $N_i$ is now $N_i = m_i + \\lfloor \\xi + r_i \\rfloor$.\nSince $0 \\le \\xi  1$ and $0 \\le r_i  1$ (as $r_i$ is a fractional part), the sum $\\xi + r_i$ is in the range $[0, 2)$. Therefore, $\\lfloor \\xi + r_i \\rfloor$ can only take values $0$ or $1$.\nThis implies that $N_i$ can only take values $m_i$ or $m_i+1$. The support of the distribution of $N_i$ is $\\{m_i, m_i+1\\}$.\n\nThe probability that $N_i = m_i+1$ is the probability that $\\lfloor \\xi + r_i \\rfloor = 1$. This occurs if and only if $\\xi + r_i \\ge 1$, or $\\xi \\ge 1 - r_i$.\nSince $\\xi \\sim U[0,1)$ and $0 \\le 1-r_i \\le 1$, the probability is:\n$$ p_i = \\mathbb{P}(N_i = m_i+1) = \\mathbb{P}(\\xi \\ge 1 - r_i) = \\int_{1-r_i}^{1} 1 \\, d\\xi = 1 - (1-r_i) = r_i $$\nSo, $p_i = r_i = \\{N w_i\\} = N w_i - \\lfloor N w_i \\rfloor$.\nThe probability that $N_i = m_i$ is consequently $1-p_i = 1 - r_i$.\n\nIn summary, for each particle $i$:\n- The offspring count $N_i$ follows a two-point distribution on the support $\\{\\lfloor N w_i \\rfloor, \\lfloor N w_i \\rfloor + 1\\}$.\n- Let $m_i = \\lfloor N w_i \\rfloor$.\n- The probability of getting one extra offspring is $p_i = \\mathbb{P}(N_i = m_i+1) = N w_i - m_i$.\n- The probability of getting the base number of offspring is $\\mathbb{P}(N_i = m_i) = 1 - p_i$.\n\nIf $N w_i$ is an integer, then $r_i = 0$, which implies $p_i=0$. In this case, $N_i$ is deterministically equal to $m_i = N w_i$.\nThe problem asks for the computation of $m_i$ and $p_i$ for each particle, which fully characterizes this distribution.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the systematic resampling problem for the given test cases.\n    It implements the one-pass algorithm for offspring counts and calculates\n    the theoretical distribution parameters for each particle.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"N\": 6,\n            \"weights\": [0.05, 0.15, 0.2, 0.1, 0.3, 0.2],\n            \"u\": 0.13,\n        },\n        {\n            \"N\": 5,\n            \"weights\": [0.2, 0.2, 0.2, 0.2, 0.2],\n            \"u\": 0.7,\n        },\n        {\n            \"N\": 8,\n            \"weights\": [0.7, 0.3/7, 0.3/7, 0.3/7, 0.3/7, 0.3/7, 0.3/7, 0.3/7],\n            \"u\": 0.9,\n        },\n        {\n            \"N\": 4,\n            \"weights\": [0.0, 0.5, 0.5, 0.0],\n            \"u\": 0.25,\n        },\n        {\n            \"N\": 1,\n            \"weights\": [1.0],\n            \"u\": 0.33,\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        weights = np.array(case[\"weights\"])\n        u = case[\"u\"]\n\n        # Task 1: Implement the one-pass algorithm for offspring counts\n        counts = np.zeros(N, dtype=int)\n        \n        # Handle the edge case of N=0 or N=1\n        if N  0:\n            particle_idx = 0\n            # Cumulative weight boundary for the current particle interval\n            c_upper_bound = weights[0]\n\n            # Iterate through the N stratified points\n            for j in range(N):\n                t_j = (u + j) / N\n                \n                # Advance particle index until the correct interval is found for t_j\n                # The condition t_j = c_upper_bound correctly places points on a\n                # boundary into the next interval, matching [F(i-1), F(i)).\n                while particle_idx  N - 1 and t_j = c_upper_bound:\n                    particle_idx += 1\n                    c_upper_bound += weights[particle_idx]\n                \n                counts[particle_idx] += 1\n\n        # Task 2: Compute distribution parameters m_i and p_i\n        m_list = []\n        p_list = []\n        for i in range(N):\n            Nw_i = N * weights[i]\n            # m_i is the integer part of N*w_i\n            m_i = math.floor(Nw_i)\n            # p_i is the fractional part of N*w_i, which is P(N_i = m_i + 1)\n            p_i = Nw_i - m_i\n            m_list.append(m_i)\n            p_list.append(p_i)\n        \n        # Format results for the current test case\n        counts_str = f\"[{','.join(map(str, counts))}]\"\n        m_str = f\"[{','.join(map(str, m_list))}]\"\n        # Format probabilities to avoid scientific notation and ensure consistency\n        p_str = f\"[{','.join(map(lambda x: format(x, '.15g'), p_list))}]\"\n\n        all_results.append(f\"[{counts_str},{m_str},{p_str}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}