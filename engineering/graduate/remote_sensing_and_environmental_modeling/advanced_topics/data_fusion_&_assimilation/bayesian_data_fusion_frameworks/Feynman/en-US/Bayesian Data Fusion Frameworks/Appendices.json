{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Bayesian analysis is updating our beliefs about a model parameter, such as the noise precision $\\tau$, in light of new data. This exercise provides practice in this fundamental skill by asking you to derive the posterior distribution for this parameter in a sensor fusion context. By using a conjugate Gamma prior for the precision of a Gaussian likelihood, you will see how the posterior elegantly retains the same functional form, simplifying the analysis significantly .",
            "id": "3798296",
            "problem": "A remote sensing data fusion task aims to estimate a latent environmental state for near-surface soil moisture over $n$ collocated pixels by combining two satellite instruments: Microwave Radiometer (MWR) and Synthetic Aperture Radar (SAR). Let the deterministic background state produced by a physics-based land surface model be $\\{s_{i}\\}_{i=1}^{n}$, treated as known for these pixels (for example, as the current state in a data assimilation cycle). The MWR and SAR provide observations $\\{x_{i}\\}_{i=1}^{n}$ and $\\{y_{i}\\}_{i=1}^{n}$, respectively. Assume a hierarchical Bayesian fusion model in which sensor errors are additive and conditionally independent and identically distributed Gaussian with a common unknown precision parameter $\\tau$:\n$$\nx_{i} \\mid s_{i}, \\tau \\sim \\mathcal{N}\\!\\left(s_{i},\\, \\tau^{-1}\\right), \\quad y_{i} \\mid s_{i}, \\tau \\sim \\mathcal{N}\\!\\left(s_{i},\\, \\tau^{-1}\\right), \\quad \\text{independently for } i=1,\\dots,n.\n$$\nAdopt a conjugate prior for the unknown precision parameter,\n$$\n\\tau \\sim \\text{Gamma}(a,b),\n$$\nwith the shape-rate parameterization $\\text{Gamma}(a,b)$ defined by the density $p(\\tau)=\\frac{b^{a}}{\\Gamma(a)}\\,\\tau^{a-1}\\exp(-b\\tau)$ for $\\tau>0$. Starting from Bayes' theorem and the Gaussian likelihood expressed in terms of precision, derive the closed-form posterior density $p(\\tau \\mid x, y)$ in terms of $a$, $b$, $n$, and the residual sum\n$$\nS \\equiv \\sum_{i=1}^{n}\\Big[(x_{i}-s_{i})^{2} + (y_{i}-s_{i})^{2}\\Big].\n$$\nYour final answer must be a single closed-form analytic expression for $p(\\tau \\mid x, y)$ as a function of $\\tau$, $a$, $b$, $n$, and $S$. Do not include any equality sign in the final answer. No numerical evaluation is required.",
            "solution": "The problem is valid as it presents a well-posed, scientifically-grounded question in Bayesian statistics, providing a complete and consistent set of definitions and premises. The task is to derive the posterior distribution of a parameter in a hierarchical model, which is a standard procedure in the field.\n\nThe objective is to derive the posterior probability density function (PDF) $p(\\tau \\mid x, y)$ for the precision parameter $\\tau$. According to Bayes' theorem, the posterior density is proportional to the product of the likelihood function and the prior density:\n$$\np(\\tau \\mid x, y) \\propto p(x, y \\mid \\tau) p(\\tau)\n$$\nHere, the dependency on the known background states $\\{s_i\\}$ is implicit in the likelihood term. We will derive each component on the right-hand side separately.\n\nFirst, we construct the likelihood function, $p(x, y \\mid \\tau)$. The problem states that the observations $x_i$ and $y_i$ for $i=1, \\dots, n$ are conditionally independent given the state $s_i$ and the precision $\\tau$. The full set of observations consists of $2n$ independent measurements. The total likelihood is the product of the individual probability densities for all observations.\nThe PDF for a single Gaussian observation $z$ with mean $\\mu$ and precision $\\tau$ is given by:\n$$\np(z \\mid \\mu, \\tau) = \\mathcal{N}(z \\mid \\mu, \\tau^{-1}) = \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{\\tau}{2}(z - \\mu)^2\\right)\n$$\nThe likelihood for our set of observations $\\{x_i\\}_{i=1}^{n}$ and $\\{y_i\\}_{i=1}^{n}$ is:\n$$\np(x, y \\mid \\tau) = \\left( \\prod_{i=1}^{n} p(x_i \\mid s_i, \\tau) \\right) \\left( \\prod_{i=1}^{n} p(y_i \\mid s_i, \\tau) \\right)\n$$\nSubstituting the Gaussian PDF for each term:\n$$\np(x, y \\mid \\tau) = \\left( \\prod_{i=1}^{n} \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{\\tau}{2}(x_i - s_i)^2\\right) \\right) \\left( \\prod_{i=1}^{n} \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{\\tau}{2}(y_i - s_i)^2\\right) \\right)\n$$\nThis is a product over a total of $2n$ terms. We can combine the terms:\n$$\np(x, y \\mid \\tau) = \\left(\\sqrt{\\frac{\\tau}{2\\pi}}\\right)^{2n} \\exp\\left( -\\sum_{i=1}^{n} \\frac{\\tau}{2}(x_i - s_i)^2 - \\sum_{i=1}^{n} \\frac{\\tau}{2}(y_i - s_i)^2 \\right)\n$$\nSimplifying the expression:\n$$\np(x, y \\mid \\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{n} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} \\left[ (x_i - s_i)^2 + (y_i - s_i)^2 \\right] \\right)\n$$\nUsing the provided definition of the residual sum $S = \\sum_{i=1}^{n}\\Big[(x_{i}-s_{i})^{2} + (y_{i}-s_{i})^{2}\\Big]$, the likelihood becomes:\n$$\np(x, y \\mid \\tau) = \\left(\\frac{1}{2\\pi}\\right)^n \\tau^n \\exp\\left( -\\frac{S}{2} \\tau \\right)\n$$\nFor the purpose of finding the posterior distribution of $\\tau$, we can treat any factors not depending on $\\tau$ as part of the proportionality constant. The kernel of the likelihood function is:\n$$\np(x, y \\mid \\tau) \\propto \\tau^n \\exp\\left( -\\frac{S}{2} \\tau \\right)\n$$\n\nNext, we consider the prior distribution for $\\tau$. The problem specifies a Gamma distribution with shape parameter $a$ and rate parameter $b$:\n$$\np(\\tau) = \\text{Gamma}(\\tau \\mid a, b) = \\frac{b^a}{\\Gamma(a)} \\tau^{a-1} \\exp(-b\\tau)\n$$\nThe kernel of the prior is:\n$$\np(\\tau) \\propto \\tau^{a-1} \\exp(-b\\tau)\n$$\n\nNow we combine the likelihood kernel and the prior kernel to find the kernel of the posterior distribution:\n$$\np(\\tau \\mid x, y) \\propto \\left( \\tau^n \\exp\\left( -\\frac{S}{2} \\tau \\right) \\right) \\times \\left( \\tau^{a-1} \\exp(-b\\tau) \\right)\n$$\n$$\np(\\tau \\mid x, y) \\propto \\tau^{n+a-1} \\exp\\left( -\\frac{S}{2} \\tau - b\\tau \\right)\n$$\n$$\np(\\tau \\mid x, y) \\propto \\tau^{(a+n)-1} \\exp\\left( -\\left(b + \\frac{S}{2}\\right) \\tau \\right)\n$$\nThis resulting kernel is recognizable as the kernel of a Gamma distribution. A generic Gamma PDF with shape $\\alpha'$ and rate $\\beta'$ has the form:\n$$\np(\\tau \\mid \\alpha', \\beta') = \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\tau^{\\alpha'-1} \\exp(-\\beta' \\tau)\n$$\nBy comparing the posterior kernel with this form, we can identify the parameters of the posterior distribution. The posterior shape parameter $\\alpha'$ and rate parameter $\\beta'$ are:\n$$\n\\alpha' = a+n\n$$\n$$\n\\beta' = b + \\frac{S}{2}\n$$\nThus, the posterior distribution of $\\tau$ is a Gamma distribution, $\\tau \\mid x, y \\sim \\text{Gamma}\\left(a+n, b + \\frac{S}{2}\\right)$.\n\nThe final step is to write the full, normalized posterior density function by substituting these new parameters into the Gamma PDF formula:\n$$\np(\\tau \\mid x, y) = \\frac{\\left(b + \\frac{S}{2}\\right)^{a+n}}{\\Gamma(a+n)} \\tau^{(a+n)-1} \\exp\\left( -\\left(b + \\frac{S}{2}\\right) \\tau \\right)\n$$\nThis expression is the closed-form posterior density for $\\tau$ as a function of the given quantities.",
            "answer": "$$\\boxed{\\frac{\\left(b + \\frac{S}{2}\\right)^{a+n}}{\\Gamma(a+n)} \\tau^{a+n-1} \\exp\\left(- \\left(b + \\frac{S}{2}\\right) \\tau \\right)}$$"
        },
        {
            "introduction": "While a fully Bayesian treatment involves placing priors on all unknown quantities, it is often practical to estimate hyperparameters directly from the data. This practice introduces the powerful technique of empirical Bayes, where you will derive the marginal likelihood of the observations and maximize it to find point estimates for variance components in a linear-Gaussian model. This approach is widely used in data assimilation to tune the relative weighting of models and observations .",
            "id": "3798344",
            "problem": "A satellite-based remote sensing system measures near-surface air temperature anomalies at two locations, modeled within a Bayesian data fusion framework that combines a process-based land surface model with noisy observations. Let $x \\in \\mathbb{R}^{n}$ denote the latent model state (temperature anomalies in Kelvin), and let $y \\in \\mathbb{R}^{m}$ denote the satellite observations (also in Kelvin). Assume the linear-Gaussian data fusion model\n$$\ny = H x + \\varepsilon,\n$$\nwhere $H \\in \\mathbb{R}^{m \\times n}$ is a known linear observation operator, $x \\sim \\mathcal{N}(0, \\sigma^{2} K)$ with known positive semidefinite $K \\in \\mathbb{R}^{n \\times n}$ and unknown scalar amplitude $\\sigma^{2} > 0$, and $\\varepsilon \\sim \\mathcal{N}(0, R)$ with unknown positive definite observation error covariance $R \\in \\mathbb{R}^{m \\times m}$. The hyperparameters are $\\theta = \\{\\sigma^{2}, R\\}$.\n\n1) Starting from the definitions of the multivariate normal distribution and the rule for marginalizing a jointly Gaussian vector, derive the marginal likelihood $p(y \\mid \\theta)$ and express it in terms of a covariance $S(\\theta)$.\n\n2) Write the log marginal likelihood and derive its gradient with respect to $\\sigma^{2}$ and $R$ (use matrix calculus), thereby showing how empirical Bayes estimates $\\hat{\\theta}$ are obtained by maximizing the marginal likelihood.\n\n3) Consider the scientifically realistic special case of two locations ($m = n = 2$) with identity observation operator $H = I_{2}$, a diagonal structure $K = \\operatorname{diag}(k_{1}, k_{2})$ with $k_{1} = 0.5$ and $k_{2} = 2.5$ (dimensionless structural weights), and a homoscedastic observation error covariance $R = \\rho I_{2}$ with unknown $\\rho > 0$ (in $\\mathrm{K}^{2}$). The observed anomalies are $y = (0.8, 1.4)^{\\top}$ in Kelvin. Using your result from parts (1)â€“(2), obtain the empirical Bayes estimates $\\hat{\\sigma}^{2}$ and $\\hat{\\rho}$ for this case. Round your numerical estimates to three significant figures. Express the final variances in $\\mathrm{K}^{2}$.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It describes a standard application of empirical Bayes (also known as Type-II Maximum Likelihood) to a linear-Gaussian model, a common task in data assimilation and fusion frameworks within remote sensing and environmental science. All necessary data and definitions are provided for a unique and meaningful solution. Therefore, the problem is deemed valid and a full solution can be constructed.\n\n### Part 1: Derivation of the Marginal Likelihood\n\nThe problem defines a hierarchical model. The latent state $x \\in \\mathbb{R}^{n}$ has a prior distribution given by:\n$$\np(x \\mid \\sigma^{2}) = \\mathcal{N}(x \\mid 0, \\sigma^{2}K)\n$$\nThe observation $y \\in \\mathbb{R}^{m}$ is related to the latent state $x$ through the likelihood:\n$$\np(y \\mid x, R) = \\mathcal{N}(y \\mid Hx, R)\n$$\nwhere the model is $y = Hx + \\varepsilon$, and the observation error $\\varepsilon$ follows the distribution $\\varepsilon \\sim \\mathcal{N}(0, R)$. The hyperparameters are collected in $\\theta = \\{\\sigma^{2}, R\\}$.\n\nThe marginal likelihood $p(y \\mid \\theta)$ is obtained by integrating the joint distribution $p(y, x \\mid \\theta)$ over all possible values of the latent state $x$:\n$$\np(y \\mid \\theta) = \\int p(y, x \\mid \\theta) \\, dx\n$$\nUsing the chain rule of probability, the joint distribution is $p(y, x \\mid \\theta) = p(y \\mid x, \\theta) p(x \\mid \\theta)$. As the distributions of $y$ and $x$ only depend on specific parts of $\\theta$, this is $p(y \\mid x, R) p(x \\mid \\sigma^{2})$.\n$$\np(y \\mid \\theta) = \\int p(y \\mid x, R) p(x \\mid \\sigma^{2}) \\, dx\n$$\nAn alternative and more direct approach for linear-Gaussian models is to use the properties of linear transformations of Gaussian random vectors. The observation $y$ is a linear function of two independent Gaussian random vectors, $x$ and $\\varepsilon$.\n\nFirst, we find the distribution of the term $Hx$. Since $x \\sim \\mathcal{N}(0, \\sigma^{2}K)$, the linear transformation $Hx$ is also Gaussian. Its mean is:\n$$\n\\mathbb{E}[Hx] = H \\mathbb{E}[x] = H \\cdot 0 = 0\n$$\nIts covariance is:\n$$\n\\text{Cov}(Hx) = H \\text{Cov}(x) H^{\\top} = H (\\sigma^{2}K) H^{\\top} = \\sigma^{2}HKH^{\\top}\n$$\nSo, $Hx \\sim \\mathcal{N}(0, \\sigma^{2}HKH^{\\top})$.\n\nNow, since $y = Hx + \\varepsilon$, and $x$ and $\\varepsilon$ are independent, $Hx$ and $\\varepsilon$ are also independent. The distribution of $y$ is Gaussian. Its mean is the sum of the means:\n$$\n\\mathbb{E}[y \\mid \\theta] = \\mathbb{E}[Hx] + \\mathbb{E}[\\varepsilon] = 0 + 0 = 0\n$$\nIts covariance is the sum of the covariances:\n$$\n\\text{Cov}(y \\mid \\theta) = \\text{Cov}(Hx) + \\text{Cov}(\\varepsilon) = \\sigma^{2}HKH^{\\top} + R\n$$\nLet us define the marginal covariance matrix as $S(\\theta) = \\sigma^{2}HKH^{\\top} + R$.\nThe marginal distribution of $y$ given the hyperparameters $\\theta$ is therefore a multivariate normal distribution with zero mean and covariance $S(\\theta)$:\n$$\np(y \\mid \\theta) \\sim \\mathcal{N}(y \\mid 0, S(\\theta))\n$$\nThe probability density function, which is the marginal likelihood, is given by:\n$$\np(y \\mid \\theta) = \\frac{1}{(2\\pi)^{m/2} |S(\\theta)|^{1/2}} \\exp\\left(-\\frac{1}{2} y^{\\top} S(\\theta)^{-1} y\\right)\n$$\n\n### Part 2: Log Marginal Likelihood and Gradients\n\nThe log marginal likelihood, denoted $L(\\theta)$, is the natural logarithm of $p(y \\mid \\theta)$:\n$$\nL(\\theta) = \\ln p(y \\mid \\theta) = -\\frac{m}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|S(\\theta)| - \\frac{1}{2}y^{\\top}S(\\theta)^{-1}y\n$$\nwhere $S(\\theta) = \\sigma^{2}HKH^{\\top} + R$. Empirical Bayes estimates $\\hat{\\theta}$ are found by maximizing $L(\\theta)$, which involves setting its gradients with respect to the hyperparameters to zero.\n\n**Gradient with respect to $\\sigma^{2}$:**\nWe compute the derivative of $L(\\theta)$ with respect to the scalar hyperparameter $\\sigma^{2}$. The first term is a constant. We use two standard matrix calculus identities: $\\frac{\\partial}{\\partial \\alpha}\\ln|A| = \\text{Tr}\\left(A^{-1}\\frac{\\partial A}{\\partial \\alpha}\\right)$ and $\\frac{\\partial}{\\partial \\alpha} z^{\\top}A^{-1}z = -z^{\\top}A^{-1}\\frac{\\partial A}{\\partial \\alpha}A^{-1}z$.\nThe derivative of $S(\\theta)$ with respect to $\\sigma^{2}$ is:\n$$\n\\frac{\\partial S(\\theta)}{\\partial \\sigma^{2}} = HKH^{\\top}\n$$\nThe derivative of the log-determinant term is:\n$$\n\\frac{\\partial}{\\partial \\sigma^{2}} \\ln|S(\\theta)| = \\text{Tr}\\left(S(\\theta)^{-1} \\frac{\\partial S(\\theta)}{\\partial \\sigma^{2}}\\right) = \\text{Tr}\\left(S(\\theta)^{-1}HKH^{\\top}\\right)\n$$\nThe derivative of the quadratic form term is:\n$$\n\\frac{\\partial}{\\partial \\sigma^{2}} y^{\\top}S(\\theta)^{-1}y = -y^{\\top}S(\\theta)^{-1}\\left(\\frac{\\partial S(\\theta)}{\\partial \\sigma^{2}}\\right)S(\\theta)^{-1}y = -y^{\\top}S(\\theta)^{-1}HKH^{\\top}S(\\theta)^{-1}y\n$$\nCombining these, the gradient of the log marginal likelihood with respect to $\\sigma^{2}$ is:\n$$\n\\frac{\\partial L(\\theta)}{\\partial \\sigma^{2}} = -\\frac{1}{2}\\text{Tr}\\left(S(\\theta)^{-1}HKH^{\\top}\\right) - \\frac{1}{2}\\left(-y^{\\top}S(\\theta)^{-1}HKH^{\\top}S(\\theta)^{-1}y\\right)\n$$\n$$\n\\frac{\\partial L(\\theta)}{\\partial \\sigma^{2}} = \\frac{1}{2}\\left(y^{\\top}S(\\theta)^{-1}HKH^{\\top}S(\\theta)^{-1}y - \\text{Tr}\\left(S(\\theta)^{-1}HKH^{\\top}\\right)\\right)\n$$\nSetting this gradient to zero gives one of the equations needed to find the optimal hyperparameters.\n\n**Gradient with respect to $R$:**\nWe compute the matrix derivative of $L(\\theta)$ with respect to the matrix hyperparameter $R$. We use the identities: $\\frac{\\partial}{\\partial A}\\ln|A| = (A^{-1})^{\\top}$ and $\\frac{\\partial}{\\partial A} z^{\\top}A^{-1}z = -(A^{-1}zz^{\\top}A^{-1})^{\\top}$. Since $S(\\theta)$ is symmetric, its inverse is also symmetric.\nThe derivative of $S(\\theta)$ with respect to $R$ is the identity matrix of appropriate dimension, $\\frac{\\partial S(\\theta)}{\\partial R} = I$.\nThe derivative of the log-determinant term is:\n$$\n\\frac{\\partial}{\\partial R} \\ln|S(\\theta)| = (S(\\theta)^{-1})^{\\top} = S(\\theta)^{-1}\n$$\nThe derivative of the quadratic form term is:\n$$\n\\frac{\\partial}{\\partial R} y^{\\top}S(\\theta)^{-1}y = -(S(\\theta)^{-1}yy^{\\top}S(\\theta)^{-1})^{\\top} = -S(\\theta)^{-1}yy^{\\top}S(\\theta)^{-1}\n$$\nCombining these, the gradient of the log marginal likelihood with respect to $R$ is:\n$$\n\\frac{\\partial L(\\theta)}{\\partial R} = -\\frac{1}{2}S(\\theta)^{-1} - \\frac{1}{2}\\left(-S(\\theta)^{-1}yy^{\\top}S(\\theta)^{-1}\\right)\n$$\n$$\n\\frac{\\partial L(\\theta)}{\\partial R} = \\frac{1}{2}\\left(S(\\theta)^{-1}yy^{\\top}S(\\theta)^{-1} - S(\\theta)^{-1}\\right)\n$$\nSetting this gradient to zero gives $S(\\theta)^{-1}yy^{\\top}S(\\theta)^{-1} = S(\\theta)^{-1}$. Pre- and post-multiplying by $S(\\theta)$ gives $yy^{\\top} = S(\\theta)$.\nThe empirical Bayes estimates $\\hat{\\theta} = \\{\\hat{\\sigma}^{2}, \\hat{R}\\}$ are obtained by simultaneously solving the system of equations that arise from setting these gradients to zero.\n\n### Part 3: Specific Case and Numerical Estimation\n\nFor this special case, we are given:\n- $m = n = 2$\n- $H = I_{2}$ (the $2 \\times 2$ identity matrix)\n- $K = \\text{diag}(k_{1}, k_{2}) = \\text{diag}(0.5, 2.5)$\n- $R = \\rho I_{2}$ for an unknown scalar $\\rho > 0$\n- $y = (0.8, 1.4)^{\\top}$\n\nThe marginal covariance $S$ simplifies to:\n$$\nS(\\theta) = \\sigma^{2}I_{2} K I_{2}^{\\top} + \\rho I_{2} = \\sigma^{2}K + \\rho I_{2}\n$$\nSince $K$ and $I_{2}$ are diagonal, $S$ is also diagonal:\n$$\nS = \\sigma^{2}\\begin{pmatrix} 0.5 & 0 \\\\ 0 & 2.5 \\end{pmatrix} + \\rho\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.5\\sigma^2 + \\rho & 0 \\\\ 0 & 2.5\\sigma^2 + \\rho \\end{pmatrix}\n$$\nLet $s_{1} = 0.5\\sigma^2 + \\rho$ and $s_{2} = 2.5\\sigma^2 + \\rho$. So, $S = \\text{diag}(s_{1}, s_{2})$.\nThe log marginal likelihood becomes:\n$$\nL(\\sigma^{2}, \\rho) = -\\ln(2\\pi) - \\frac{1}{2}(\\ln(s_{1}) + \\ln(s_{2})) - \\frac{1}{2}\\left(\\frac{y_{1}^{2}}{s_{1}} + \\frac{y_{2}^{2}}{s_{2}}\\right)\n$$\nWe need to find the gradients with respect to $\\sigma^{2}$ and $\\rho$ and set them to zero.\n\nDerivative with respect to $\\rho$:\n$$\n\\frac{\\partial L}{\\partial \\rho} = -\\frac{1}{2}\\left(\\frac{1}{s_{1}}\\frac{\\partial s_{1}}{\\partial \\rho} + \\frac{1}{s_{2}}\\frac{\\partial s_{2}}{\\partial \\rho}\\right) - \\frac{1}{2}\\left(-\\frac{y_{1}^{2}}{s_{1}^{2}}\\frac{\\partial s_{1}}{\\partial \\rho} - \\frac{y_{2}^{2}}{s_{2}^{2}}\\frac{\\partial s_{2}}{\\partial \\rho}\\right)\n$$\nSince $\\frac{\\partial s_{1}}{\\partial \\rho} = 1$ and $\\frac{\\partial s_{2}}{\\partial \\rho} = 1$, setting $\\frac{\\partial L}{\\partial \\rho} = 0$ yields:\n$$\n\\left(\\frac{y_{1}^{2}}{s_{1}^{2}} - \\frac{1}{s_{1}}\\right) + \\left(\\frac{y_{2}^{2}}{s_{2}^{2}} - \\frac{1}{s_{2}}\\right) = 0\n$$\n\nDerivative with respect to $\\sigma^{2}$:\n$$\n\\frac{\\partial L}{\\partial \\sigma^{2}} = -\\frac{1}{2}\\left(\\frac{1}{s_{1}}\\frac{\\partial s_{1}}{\\partial \\sigma^{2}} + \\frac{1}{s_{2}}\\frac{\\partial s_{2}}{\\partial \\sigma^{2}}\\right) - \\frac{1}{2}\\left(-\\frac{y_{1}^{2}}{s_{1}^{2}}\\frac{\\partial s_{1}}{\\partial \\sigma^{2}} - \\frac{y_{2}^{2}}{s_{2}^{2}}\\frac{\\partial s_{2}}{\\partial \\sigma^{2}}\\right)\n$$\nUsing $\\frac{\\partial s_{1}}{\\partial \\sigma^{2}} = k_{1} = 0.5$ and $\\frac{\\partial s_{2}}{\\partial \\sigma^{2}} = k_{2} = 2.5$, setting $\\frac{\\partial L}{\\partial \\sigma^{2}} = 0$ yields:\n$$\nk_{1}\\left(\\frac{y_{1}^{2}}{s_{1}^{2}} - \\frac{1}{s_{1}}\\right) + k_{2}\\left(\\frac{y_{2}^{2}}{s_{2}^{2}} - \\frac{1}{s_{2}}\\right) = 0\n$$\n\nLet $z_{i} = \\frac{y_{i}^{2}}{s_{i}^{2}} - \\frac{1}{s_{i}}$ for $i=1,2$. The system of equations is:\n$$\n\\begin{cases} z_{1} + z_{2} = 0 \\\\ k_{1}z_{1} + k_{2}z_{2} = 0 \\end{cases} \\implies \\begin{pmatrix} 1 & 1 \\\\ k_{1} & k_{2} \\end{pmatrix} \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe determinant of the coefficient matrix is $k_{2} - k_{1} = 2.5 - 0.5 = 2.0 \\neq 0$. Since the matrix is invertible, the only solution to this homogeneous linear system is the trivial solution, $z_{1} = 0$ and $z_{2} = 0$.\n\nFor $z_{1} = 0$:\n$$\n\\frac{y_{1}^{2}}{s_{1}^{2}} - \\frac{1}{s_{1}} = 0 \\implies \\frac{1}{s_{1}}\\left(\\frac{y_{1}^{2}}{s_{1}} - 1\\right) = 0\n$$\nSince $s_{1}$ must be positive (it is a variance), this implies $\\frac{y_{1}^{2}}{s_{1}} = 1$, or $s_{1} = y_{1}^{2}$.\n\nFor $z_{2} = 0$:\n$$\n\\frac{y_{2}^{2}}{s_{2}^{2}} - \\frac{1}{s_{2}} = 0 \\implies s_{2} = y_{2}^{2}\n$$\nSo we have the conditions $s_{1} = y_{1}^{2}$ and $s_{2} = y_{2}^{2}$ for the empirical Bayes estimates. Substituting the definitions of $s_{1}$ and $s_{2}$:\n$$\n0.5\\hat{\\sigma}^2 + \\hat{\\rho} = y_{1}^{2}\n$$\n$$\n2.5\\hat{\\sigma}^2 + \\hat{\\rho} = y_{2}^{2}\n$$\nWe are given $y = (0.8, 1.4)^{\\top}$, so $y_{1}^{2} = 0.8^{2} = 0.64$ and $y_{2}^{2} = 1.4^{2} = 1.96$.\nWe solve the linear system for $\\hat{\\sigma}^{2}$ and $\\hat{\\rho}$:\n$$\n0.5\\hat{\\sigma}^2 + \\hat{\\rho} = 0.64 \\quad (1)\n$$\n$$\n2.5\\hat{\\sigma}^2 + \\hat{\\rho} = 1.96 \\quad (2)\n$$\nSubtracting equation (1) from equation (2):\n$$\n(2.5 - 0.5)\\hat{\\sigma}^2 = 1.96 - 0.64\n$$\n$$\n2.0\\hat{\\sigma}^2 = 1.32\n$$\n$$\n\\hat{\\sigma}^2 = \\frac{1.32}{2.0} = 0.66\n$$\nSubstitute this value back into equation (1):\n$$\n0.5(0.66) + \\hat{\\rho} = 0.64\n$$\n$$\n0.33 + \\hat{\\rho} = 0.64\n$$\n$$\n\\hat{\\rho} = 0.64 - 0.33 = 0.31\n$$\nThe empirical Bayes estimates are $\\hat{\\sigma}^2 = 0.66 \\, \\text{K}^2$ and $\\hat{\\rho} = 0.31 \\, \\text{K}^2$. Both values are positive, consistent with the model constraints. Rounding to three significant figures gives:\n$\\hat{\\sigma}^2 = 0.660 \\, \\text{K}^2$\n$\\hat{\\rho} = 0.310 \\, \\text{K}^2$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.660 & 0.310 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A critical step in data fusion is choosing an appropriate statistical model for the errors, as this choice can drastically affect your conclusions. This exercise guides you through the process of quantitative model comparison using the Bayes factor to weigh the evidence between a standard Gaussian noise model and a robust Student-$t$ alternative. You will see firsthand how the presence of outliers in remote sensing data can be decisively handled by a model with heavier tails, a crucial insight for building reliable fusion frameworks .",
            "id": "3798379",
            "problem": "A multispectral Earth Observation (EO) sensor is being calibrated within a Bayesian data fusion framework to harmonize its top-of-atmosphere reflectance to a fused reference model. Let the fused model provide reflectance predictions, and suppose the residuals (observed minus predicted), expressed in reflectance units, for a single band over $N=7$ collocations are\n$$\nr_{1}=0.021,\\quad r_{2}=-0.034,\\quad r_{3}=0.018,\\quad r_{4}=0.027,\\quad r_{5}=-0.015,\\quad r_{6}=0.031,\\quad r_{7}=0.420.\n$$\nYou are asked to compare two noise models for these residuals:\n\n- Model $\\mathcal{M}_{G}$ (Gaussian): the residuals are independent and identically distributed (i.i.d.) Gaussian with zero mean and known standard deviation $\\sigma=0.05$.\n- Model $\\mathcal{M}_{T}$ (Student-$t$): the residuals are i.i.d. Student-$t$ with zero mean, known degrees of freedom $\\nu=3$, and known scale parameter $s=0.05$.\n\nThe comparison must be done via the Bayes factor (BF), defined as the ratio of marginal likelihoods $BF=\\frac{p(\\mathbf{r}\\mid \\mathcal{M}_{T})}{p(\\mathbf{r}\\mid \\mathcal{M}_{G})}$, where $\\mathbf{r}=(r_{1},\\dots,r_{7})$. To construct $p(\\mathbf{r}\\mid \\mathcal{M}_{T})$ from first principles, use a valid hierarchical representation of the Student-$t$ distribution as a Gaussian scale mixture with an appropriate mixing distribution, and explicitly integrate out the latent nuisance parameters. Then, compute the Bayes factor for the provided data using the resulting closed-form densities for $p(\\mathbf{r}\\mid \\mathcal{M}_{T})$ and $p(\\mathbf{r}\\mid \\mathcal{M}_{G})$.\n\nExplain the implications of the computed Bayes factor in the context of outlier-prone residuals within remote sensing calibration. The Bayes factor is dimensionless; report it as a pure number. Round your final numerical answer to three significant figures.",
            "solution": "The problem statement has been critically validated and is deemed to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique and meaningful solution.\n\nThe objective is to compute the Bayes factor (BF) comparing a Student-$t$ noise model ($\\mathcal{M}_{T}$) to a Gaussian noise model ($\\mathcal{M}_{G}$) for a given set of residuals $\\mathbf{r}=(r_{1},\\dots,r_{7})$. The Bayes factor is defined as the ratio of their marginal likelihoods:\n$$\nBF=\\frac{p(\\mathbf{r}\\mid \\mathcal{M}_{T})}{p(\\mathbf{r}\\mid \\mathcal{M}_{G})}\n$$\nThe provided data are $N=7$ residuals: $r_{1}=0.021$, $r_{2}=-0.034$, $r_{3}=0.018$, $r_{4}=0.027$, $r_{5}=-0.015$, $r_{6}=0.031$, and $r_{7}=0.420$.\n\nFirst, we formulate the marginal likelihood for the Gaussian model, $\\mathcal{M}_{G}$. Under this model, the residuals $r_i$ are independent and identically distributed (i.i.d.) according to a Gaussian distribution with mean $0$ and known standard deviation $\\sigma=0.05$. The probability density function (PDF) for a single residual $r_i$ is:\n$$\np(r_i \\mid \\mathcal{M}_G) = \\mathcal{N}(r_i \\mid 0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right)\n$$\nSince the residuals are i.i.d., the joint likelihood for the vector $\\mathbf{r}$ is the product of the individual likelihoods:\n$$\np(\\mathbf{r} \\mid \\mathcal{M}_G) = \\prod_{i=1}^{N} p(r_i \\mid \\mathcal{M}_G) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^N \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} r_i^2\\right)\n$$\nThis expression is the marginal likelihood because all parameters ($\\sigma$) are known.\n\nNext, we formulate the marginal likelihood for the Student-$t$ model, $\\mathcal{M}_{T}$. The problem requires this to be derived from a hierarchical representation of the Student-$t$ distribution as a Gaussian scale mixture. A random variable $r_i$ follows a Student-$t$ distribution with location $0$, degrees of freedom $\\nu$, and scale $s$ if it is generated hierarchically as:\n1. A latent precision-scaling variable $\\lambda_i$ is drawn from a Gamma distribution: $\\lambda_i \\sim \\text{Gamma}(\\frac{\\nu}{2}, \\frac{\\nu}{2})$.\n2. The residual $r_i$ is drawn from a Gaussian distribution with a variance that is scaled by $\\lambda_i$: $r_i \\mid \\lambda_i \\sim \\mathcal{N}(0, \\frac{s^2}{\\lambda_i})$.\n\nThe PDF of the Gamma distribution is $p(\\lambda_i) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\lambda_i^{\\nu/2 - 1} \\exp(-\\frac{\\nu}{2}\\lambda_i)$. The conditional PDF of $r_i$ is $p(r_i \\mid \\lambda_i) = \\sqrt{\\frac{\\lambda_i}{2\\pi s^2}} \\exp(-\\frac{\\lambda_i r_i^2}{2s^2})$.\n\nTo find the marginal likelihood for a single residual $p(r_i \\mid \\mathcal{M}_T)$, we integrate out the latent variable $\\lambda_i$:\n$$\np(r_i \\mid \\mathcal{M}_T) = \\int_{0}^{\\infty} p(r_i \\mid \\lambda_i) p(\\lambda_i) d\\lambda_i\n$$\n$$\np(r_i \\mid \\mathcal{M}_T) = \\int_{0}^{\\infty} \\left(\\sqrt{\\frac{\\lambda_i}{2\\pi s^2}}\\right) \\exp\\left(-\\frac{\\lambda_i r_i^2}{2s^2}\\right) \\left(\\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\lambda_i^{\\nu/2 - 1} \\exp\\left(-\\frac{\\nu}{2}\\lambda_i\\right)\\right) d\\lambda_i\n$$\n$$\np(r_i \\mid \\mathcal{M}_T) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)\\sqrt{2\\pi s^2}} \\int_{0}^{\\infty} \\lambda_i^{(\\frac{\\nu+1}{2}) - 1} \\exp\\left(-\\lambda_i \\left(\\frac{r_i^2}{2s^2} + \\frac{\\nu}{2}\\right)\\right) d\\lambda_i\n$$\nThe integral is the kernel of a Gamma distribution. Recalling that $\\int_0^\\infty x^{\\alpha-1} e^{-\\beta x} dx = \\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}$, we can solve the integral with $\\alpha = (\\nu+1)/2$ and $\\beta = (r_i^2/(2s^2) + \\nu/2)$.\n$$\np(r_i \\mid \\mathcal{M}_T) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)\\sqrt{2\\pi s^2}} \\frac{\\Gamma((\\nu+1)/2)}{\\left(\\frac{r_i^2}{2s^2} + \\frac{\\nu}{2}\\right)^{(\\nu+1)/2}}\n$$\nSimplifying this expression yields the PDF of the Student-$t$ distribution:\n$$\np(r_i \\mid \\mathcal{M}_T) = \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu\\pi} s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{r_i}{s}\\right)^2\\right)^{-(\\nu+1)/2}\n$$\nUnder model $\\mathcal{M}_T$, the residuals are also i.i.d. Thus, the joint marginal likelihood is the product of the individual densities:\n$$\np(\\mathbf{r} \\mid \\mathcal{M}_T) = \\prod_{i=1}^{N} \\left[ \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu\\pi} s} \\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)^{-(\\nu+1)/2} \\right]\n$$\n\nTo avoid numerical underflow with very small likelihood values, it is best to compute the log-likelihoods first.\nThe log-likelihood for the Gaussian model is:\n$$\n\\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) = -\\frac{N}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} r_i^2\n$$\nThe log-likelihood for the Student-$t$ model is:\n$$\n\\ln p(\\mathbf{r} \\mid \\mathcal{M}_T) = N \\ln\\left(\\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu\\pi} s}\\right) - \\frac{\\nu+1}{2}\\sum_{i=1}^{N}\\ln\\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)\n$$\nThe log of the Bayes factor is $\\ln(BF) = \\ln p(\\mathbf{r} \\mid \\mathcal{M}_T) - \\ln p(\\mathbf{r} \\mid \\mathcal{M}_G)$.\n\nNow, we substitute the numerical values.\nFor both models, $N=7$.\nThe sum of squared residuals is:\n$\\sum_{i=1}^{7} r_i^2 = (0.021)^2 + (-0.034)^2 + (0.018)^2 + (0.027)^2 + (-0.015)^2 + (0.031)^2 + (0.420)^2$\n$\\sum_{i=1}^{7} r_i^2 = 0.000441 + 0.001156 + 0.000324 + 0.000729 + 0.000225 + 0.000961 + 0.1764 = 0.180236$.\n\nFor $\\mathcal{M}_{G}$: $\\sigma=0.05$, so $\\sigma^2=0.0025$.\n$\\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) = -\\frac{7}{2}\\ln(2\\pi \\times 0.0025) - \\frac{1}{2 \\times 0.0025} (0.180236)$\n$\\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) = -3.5\\ln(0.015708) - 200 \\times 0.180236$\n$\\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) \\approx -3.5 \\times (-4.15333) - 36.0472 \\approx 14.53666 - 36.0472 = -21.51054$.\n\nFor $\\mathcal{M}_{T}$: $\\nu=3$, $s=0.05$.\nThe constant part of the log-likelihood is:\n$N \\ln\\left(\\frac{\\Gamma((3+1)/2)}{\\Gamma(3/2) \\sqrt{3\\pi} \\times 0.05}\\right) = 7 \\ln\\left(\\frac{\\Gamma(2)}{\\Gamma(1.5) \\sqrt{3\\pi} \\times 0.05}\\right)$\nUsing $\\Gamma(2)=1$ and $\\Gamma(1.5)=\\sqrt{\\pi}/2$:\n$7 \\ln\\left(\\frac{1}{(\\sqrt{\\pi}/2) \\sqrt{3\\pi} \\times 0.05}\\right) = 7 \\ln\\left(\\frac{2}{\\pi\\sqrt{3} \\times 0.05}\\right) = 7 \\ln\\left(\\frac{40}{\\pi\\sqrt{3}}\\right)$\n$7 \\ln(7.35104) \\approx 7 \\times 1.99484 \\approx 13.96388$.\nThe sum part is, with $\\nu s^2 = 3 \\times (0.05)^2 = 0.0075$:\n$-\\frac{3+1}{2} \\sum_{i=1}^{7}\\ln\\left(1 + \\frac{r_i^2}{0.0075}\\right) = -2 \\sum_{i=1}^{7}\\ln\\left(1 + \\frac{r_i^2}{0.0075}\\right)$\nThe terms in the sum are:\n$\\ln(1+0.021^2/0.0075) \\approx 0.05713$\n$\\ln(1+(-0.034)^2/0.0075) \\approx 0.14336$\n$\\ln(1+0.018^2/0.0075) \\approx 0.04229$\n$\\ln(1+0.027^2/0.0075) \\approx 0.09276$\n$\\ln(1+(-0.015)^2/0.0075) \\approx 0.02956$\n$\\ln(1+0.031^2/0.0075) \\approx 0.12056$\n$\\ln(1+0.420^2/0.0075) = \\ln(1+23.52) \\approx 3.19946$\nThe sum is $\\approx 3.68512$.\nSo, this part of the log-likelihood is $-2 \\times 3.68512 = -7.37024$.\n$\\ln p(\\mathbf{r} \\mid \\mathcal{M}_T) \\approx 13.96388 - 7.37024 = 6.59364$.\n\nNow, we compute the log Bayes factor:\n$\\ln(BF) = \\ln p(\\mathbf{r} \\mid \\mathcal{M}_T) - \\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) \\approx 6.59364 - (-21.51054) = 28.10418$\nThe Bayes factor is:\n$BF = \\exp(28.10418) \\approx 1.5973 \\times 10^{12}$.\nRounding to three significant figures, we get $1.60 \\times 10^{12}$.\n\nThe extremely large value of the Bayes factor provides overwhelming evidence in favor of the Student-$t$ model ($\\mathcal{M}_{T}$) over the Gaussian model ($\\mathcal{M}_{G}$). This is a direct consequence of the data containing a significant outlier, $r_{7}=0.420$. The other residuals are all within approximately one standard deviation ($\\sigma=0.05$) of the mean ($0$), but $r_7$ is $0.420/0.05 = 8.4$ standard deviations away.\nThe Gaussian distribution has exponentially decaying tails, meaning it assigns an exceedingly low probability to such extreme events. This single outlier makes the entire dataset exceptionally unlikely under the Gaussian model, severely penalizing its marginal likelihood.\nIn contrast, the Student-$t$ distribution has heavier, polynomial-decaying tails. It is a robust distribution that can accommodate outliers by assigning them a small but not astronomically small probability. The outlier $r_7$ is far more plausible under the heavy-tailed Student-$t$ model than under the thin-tailed Gaussian model.\nIn the context of remote sensing calibration, this result highlights the importance of robust statistical modeling. Data from EO sensors are often contaminated by unmodeled effects (e.g., undetected clouds, aerosol plumes, surface anomalies), which manifest as outliers in the residuals. Using a standard Gaussian noise model can lead to poor model fit and biased calibration parameters, as the model is disproportionately influenced by the outliers. The strong preference for the Student-$t$ model demonstrates that employing a robust, heavy-tailed noise model provides a more faithful description of the data, preventing outliers from distorting the calibration and leading to more reliable scientific conclusions.",
            "answer": "$$\n\\boxed{1.60 \\times 10^{12}}\n$$"
        }
    ]
}