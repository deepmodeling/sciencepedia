{
    "hands_on_practices": [
        {
            "introduction": "At the heart of any Bayesian framework is the process of updating our beliefs in light of new evidence. This first exercise provides a foundational walkthrough of this core mechanic. We will explore how to derive the posterior distribution for a sensor's precision parameter by combining a chosen prior belief with the likelihood of observed data. This practice utilizes the elegant concept of conjugate priors, where the mathematical form of the prior and posterior are in the same family, providing a clear and intuitive analytical solution for the updated parameters .",
            "id": "3798296",
            "problem": "A remote sensing data fusion task aims to estimate a latent environmental state for near-surface soil moisture over $n$ collocated pixels by combining two satellite instruments: Microwave Radiometer (MWR) and Synthetic Aperture Radar (SAR). Let the deterministic background state produced by a physics-based land surface model be $\\{s_{i}\\}_{i=1}^{n}$, treated as known for these pixels (for example, as the current state in a data assimilation cycle). The MWR and SAR provide observations $\\{x_{i}\\}_{i=1}^{n}$ and $\\{y_{i}\\}_{i=1}^{n}$, respectively. Assume a hierarchical Bayesian fusion model in which sensor errors are additive and conditionally independent and identically distributed Gaussian with a common unknown precision parameter $\\tau$:\n$$\nx_{i} \\mid s_{i}, \\tau \\sim \\mathcal{N}\\!\\left(s_{i},\\, \\tau^{-1}\\right), \\quad y_{i} \\mid s_{i}, \\tau \\sim \\mathcal{N}\\!\\left(s_{i},\\, \\tau^{-1}\\right), \\quad \\text{independently for } i=1,\\dots,n.\n$$\nAdopt a conjugate prior for the unknown precision parameter,\n$$\n\\tau \\sim \\text{Gamma}(a,b),\n$$\nwith the shape-rate parameterization $\\text{Gamma}(a,b)$ defined by the density $p(\\tau)=\\frac{b^{a}}{\\Gamma(a)}\\,\\tau^{a-1}\\exp(-b\\tau)$ for $\\tau>0$. Starting from Bayes' theorem and the Gaussian likelihood expressed in terms of precision, derive the closed-form posterior density $p(\\tau \\mid x, y)$ in terms of $a$, $b$, $n$, and the residual sum\n$$\nS \\equiv \\sum_{i=1}^{n}\\Big[(x_{i}-s_{i})^{2} + (y_{i}-s_{i})^{2}\\Big].\n$$\nYour final answer must be a single closed-form analytic expression for $p(\\tau \\mid x, y)$ as a function of $\\tau$, $a$, $b$, $n$, and $S$. Do not include any equality sign in the final answer. No numerical evaluation is required.",
            "solution": "The problem is valid as it presents a well-posed, scientifically-grounded question in Bayesian statistics, providing a complete and consistent set of definitions and premises. The task is to derive the posterior distribution of a parameter in a hierarchical model, which is a standard procedure in the field.\n\nThe objective is to derive the posterior probability density function (PDF) $p(\\tau \\mid x, y)$ for the precision parameter $\\tau$. According to Bayes' theorem, the posterior density is proportional to the product of the likelihood function and the prior density:\n$$\np(\\tau \\mid x, y) \\propto p(x, y \\mid \\tau) p(\\tau)\n$$\nHere, the dependency on the known background states $\\{s_i\\}$ is implicit in the likelihood term. We will derive each component on the right-hand side separately.\n\nFirst, we construct the likelihood function, $p(x, y \\mid \\tau)$. The problem states that the observations $x_i$ and $y_i$ for $i=1, \\dots, n$ are conditionally independent given the state $s_i$ and the precision $\\tau$. The full set of observations consists of $2n$ independent measurements. The total likelihood is the product of the individual probability densities for all observations.\nThe PDF for a single Gaussian observation $z$ with mean $\\mu$ and precision $\\tau$ is given by:\n$$\np(z \\mid \\mu, \\tau) = \\mathcal{N}(z \\mid \\mu, \\tau^{-1}) = \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{\\tau}{2}(z - \\mu)^2\\right)\n$$\nThe likelihood for our set of observations $\\{x_i\\}_{i=1}^{n}$ and $\\{y_i\\}_{i=1}^{n}$ is:\n$$\np(x, y \\mid \\tau) = \\left( \\prod_{i=1}^{n} p(x_i \\mid s_i, \\tau) \\right) \\left( \\prod_{i=1}^{n} p(y_i \\mid s_i, \\tau) \\right)\n$$\nSubstituting the Gaussian PDF for each term:\n$$\np(x, y \\mid \\tau) = \\left( \\prod_{i=1}^{n} \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{\\tau}{2}(x_i - s_i)^2\\right) \\right) \\left( \\prod_{i=1}^{n} \\sqrt{\\frac{\\tau}{2\\pi}} \\exp\\left(-\\frac{\\tau}{2}(y_i - s_i)^2\\right) \\right)\n$$\nThis is a product over a total of $2n$ terms. We can combine the terms:\n$$\np(x, y \\mid \\tau) = \\left(\\sqrt{\\frac{\\tau}{2\\pi}}\\right)^{2n} \\exp\\left( -\\sum_{i=1}^{n} \\frac{\\tau}{2}(x_i - s_i)^2 - \\sum_{i=1}^{n} \\frac{\\tau}{2}(y_i - s_i)^2 \\right)\n$$\nSimplifying the expression:\n$$\np(x, y \\mid \\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{n} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} \\left[ (x_i - s_i)^2 + (y_i - s_i)^2 \\right] \\right)\n$$\nUsing the provided definition of the residual sum $S = \\sum_{i=1}^{n}\\Big[(x_{i}-s_{i})^{2} + (y_{i}-s_{i})^{2}\\Big]$, the likelihood becomes:\n$$\np(x, y \\mid \\tau) = \\left(\\frac{1}{2\\pi}\\right)^n \\tau^n \\exp\\left( -\\frac{S}{2} \\tau \\right)\n$$\nFor the purpose of finding the posterior distribution of $\\tau$, we can treat any factors not depending on $\\tau$ as part of the proportionality constant. The kernel of the likelihood function is:\n$$\np(x, y \\mid \\tau) \\propto \\tau^n \\exp\\left( -\\frac{S}{2} \\tau \\right)\n$$\n\nNext, we consider the prior distribution for $\\tau$. The problem specifies a Gamma distribution with shape parameter $a$ and rate parameter $b$:\n$$\np(\\tau) = \\text{Gamma}(\\tau \\mid a, b) = \\frac{b^a}{\\Gamma(a)} \\tau^{a-1} \\exp(-b\\tau)\n$$\nThe kernel of the prior is:\n$$\np(\\tau) \\propto \\tau^{a-1} \\exp(-b\\tau)\n$$\n\nNow we combine the likelihood kernel and the prior kernel to find the kernel of the posterior distribution:\n$$\np(\\tau \\mid x, y) \\propto \\left( \\tau^n \\exp\\left( -\\frac{S}{2} \\tau \\right) \\right) \\times \\left( \\tau^{a-1} \\exp(-b\\tau) \\right)\n$$\n$$\np(\\tau \\mid x, y) \\propto \\tau^{n+a-1} \\exp\\left( -\\frac{S}{2} \\tau - b\\tau \\right)\n$$\n$$\np(\\tau \\mid x, y) \\propto \\tau^{(a+n)-1} \\exp\\left( -\\left(b + \\frac{S}{2}\\right) \\tau \\right)\n$$\nThis resulting kernel is recognizable as the kernel of a Gamma distribution. A generic Gamma PDF with shape $\\alpha'$ and rate $\\beta'$ has the form:\n$$\np(\\tau \\mid \\alpha', \\beta') = \\frac{(\\beta')^{\\alpha'}}{\\Gamma(\\alpha')} \\tau^{\\alpha'-1} \\exp(-\\beta' \\tau)\n$$\nBy comparing the posterior kernel with this form, we can identify the parameters of the posterior distribution. The posterior shape parameter $\\alpha'$ and rate parameter $\\beta'$ are:\n$$\n\\alpha' = a+n\n$$\n$$\n\\beta' = b + \\frac{S}{2}\n$$\nThus, the posterior distribution of $\\tau$ is a Gamma distribution, $\\tau \\mid x, y \\sim \\text{Gamma}\\left(a+n, b + \\frac{S}{2}\\right)$.\n\nThe final step is to write the full, normalized posterior density function by substituting these new parameters into the Gamma PDF formula:\n$$\np(\\tau \\mid x, y) = \\frac{\\left(b + \\frac{S}{2}\\right)^{a+n}}{\\Gamma(a+n)} \\tau^{(a+n)-1} \\exp\\left( -\\left(b + \\frac{S}{2}\\right) \\tau \\right)\n$$\nThis expression is the closed-form posterior density for $\\tau$ as a function of the given quantities.",
            "answer": "$$\\boxed{\\frac{\\left(b + \\frac{S}{2}\\right)^{a+n}}{\\Gamma(a+n)} \\tau^{(a+n)-1} \\exp\\left(- \\left(b + \\frac{S}{2}\\right) \\tau \\right)}$$"
        },
        {
            "introduction": "After mastering parameter estimation within a single model, a critical next step is learning to compare different models. Real-world remote sensing data is often imperfect and may contain outliers that violate the assumptions of simple models. This practice introduces the Bayes factor as a principled method for model selection, applying it to a common challenge: deciding between a standard Gaussian noise model and a more robust Student-$t$ model. By calculating the marginal likelihood for each model, you will quantify the evidence provided by the data, gaining insight into how to build more resilient data fusion frameworks that can handle outlier-prone observations .",
            "id": "3798379",
            "problem": "A multispectral Earth Observation (EO) sensor is being calibrated within a Bayesian data fusion framework to harmonize its top-of-atmosphere reflectance to a fused reference model. Let the fused model provide reflectance predictions, and suppose the residuals (observed minus predicted), expressed in reflectance units, for a single band over $N=7$ collocations are\n$$\nr_{1}=0.021,\\quad r_{2}=-0.034,\\quad r_{3}=0.018,\\quad r_{4}=0.027,\\quad r_{5}=-0.015,\\quad r_{6}=0.031,\\quad r_{7}=0.420.\n$$\nYou are asked to compare two noise models for these residuals:\n\n- Model $\\mathcal{M}_{G}$ (Gaussian): the residuals are independent and identically distributed (i.i.d.) Gaussian with zero mean and known standard deviation $\\sigma=0.05$.\n- Model $\\mathcal{M}_{T}$ (Student-$t$): the residuals are i.i.d. Student-$t$ with zero mean, known degrees of freedom $\\nu=3$, and known scale parameter $s=0.05$.\n\nThe comparison must be done via the Bayes factor (BF), defined as the ratio of marginal likelihoods $BF=\\frac{p(\\mathbf{r}\\mid \\mathcal{M}_{T})}{p(\\mathbf{r}\\mid \\mathcal{M}_{G})}$, where $\\mathbf{r}=(r_{1},\\dots,r_{7})$. To construct $p(\\mathbf{r}\\mid \\mathcal{M}_{T})$ from first principles, use a valid hierarchical representation of the Student-$t$ distribution as a Gaussian scale mixture with an appropriate mixing distribution, and explicitly integrate out the latent nuisance parameters. Then, compute the Bayes factor for the provided data using the resulting closed-form densities for $p(\\mathbf{r}\\mid \\mathcal{M}_{T})$ and $p(\\mathbf{r}\\mid \\mathcal{M}_{G})$.\n\nExplain the implications of the computed Bayes factor in the context of outlier-prone residuals within remote sensing calibration. The Bayes factor is dimensionless; report it as a pure number. Round your final numerical answer to three significant figures.",
            "solution": "The problem statement has been critically validated and is deemed to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique and meaningful solution.\n\nThe objective is to compute the Bayes factor (BF) comparing a Student-$t$ noise model ($\\mathcal{M}_{T}$) to a Gaussian noise model ($\\mathcal{M}_{G}$) for a given set of residuals $\\mathbf{r}=(r_{1},\\dots,r_{7})$. The Bayes factor is defined as the ratio of their marginal likelihoods:\n$$\nBF=\\frac{p(\\mathbf{r}\\mid \\mathcal{M}_{T})}{p(\\mathbf{r}\\mid \\mathcal{M}_{G})}\n$$\nThe provided data are $N=7$ residuals: $r_{1}=0.021$, $r_{2}=-0.034$, $r_{3}=0.018$, $r_{4}=0.027$, $r_{5}=-0.015$, $r_{6}=0.031$, and $r_{7}=0.420$.\n\nFirst, we formulate the marginal likelihood for the Gaussian model, $\\mathcal{M}_{G}$. Under this model, the residuals $r_i$ are independent and identically distributed (i.i.d.) according to a Gaussian distribution with mean $0$ and known standard deviation $\\sigma=0.05$. The probability density function (PDF) for a single residual $r_i$ is:\n$$\np(r_i \\mid \\mathcal{M}_G) = \\mathcal{N}(r_i \\mid 0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right)\n$$\nSince the residuals are i.i.d., the joint likelihood for the vector $\\mathbf{r}$ is the product of the individual likelihoods:\n$$\np(\\mathbf{r} \\mid \\mathcal{M}_G) = \\prod_{i=1}^{N} p(r_i \\mid \\mathcal{M}_G) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^N \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} r_i^2\\right)\n$$\nThis expression is the marginal likelihood because all parameters ($\\sigma$) are known.\n\nNext, we formulate the marginal likelihood for the Student-$t$ model, $\\mathcal{M}_{T}$. The problem requires this to be derived from a hierarchical representation of the Student-$t$ distribution as a Gaussian scale mixture. A random variable $r_i$ follows a Student-$t$ distribution with location $0$, degrees of freedom $\\nu$, and scale $s$ if it is generated hierarchically as:\n1. A latent precision-scaling variable $\\lambda_i$ is drawn from a Gamma distribution: $\\lambda_i \\sim \\text{Gamma}(\\frac{\\nu}{2}, \\frac{\\nu}{2})$.\n2. The residual $r_i$ is drawn from a Gaussian distribution with a variance that is scaled by $\\lambda_i$: $r_i \\mid \\lambda_i \\sim \\mathcal{N}(0, \\frac{s^2}{\\lambda_i})$.\n\nThe PDF of the Gamma distribution is $p(\\lambda_i) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\lambda_i^{\\nu/2 - 1} \\exp(-\\frac{\\nu}{2}\\lambda_i)$. The conditional PDF of $r_i$ is $p(r_i \\mid \\lambda_i) = \\sqrt{\\frac{\\lambda_i}{2\\pi s^2}} \\exp(-\\frac{\\lambda_i r_i^2}{2s^2})$.\n\nTo find the marginal likelihood for a single residual $p(r_i \\mid \\mathcal{M}_T)$, we integrate out the latent variable $\\lambda_i$:\n$$\np(r_i \\mid \\mathcal{M}_T) = \\int_{0}^{\\infty} p(r_i \\mid \\lambda_i) p(\\lambda_i) d\\lambda_i\n$$\n$$\np(r_i \\mid \\mathcal{M}_T) = \\int_{0}^{\\infty} \\left(\\sqrt{\\frac{\\lambda_i}{2\\pi s^2}}\\right) \\exp\\left(-\\frac{\\lambda_i r_i^2}{2s^2}\\right) \\left(\\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\lambda_i^{\\nu/2 - 1} \\exp\\left(-\\frac{\\nu}{2}\\lambda_i\\right)\\right) d\\lambda_i\n$$\n$$\np(r_i \\mid \\mathcal{M}_T) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)\\sqrt{2\\pi s^2}} \\int_{0}^{\\infty} \\lambda_i^{(\\frac{\\nu+1}{2}) - 1} \\exp\\left(-\\lambda_i \\left(\\frac{r_i^2}{2s^2} + \\frac{\\nu}{2}\\right)\\right) d\\lambda_i\n$$\nThe integral is the kernel of a Gamma distribution. Recalling that $\\int_0^\\infty x^{\\alpha-1} e^{-\\beta x} dx = \\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}$, we can solve the integral with $\\alpha = (\\nu+1)/2$ and $\\beta = (r_i^2/(2s^2) + \\nu/2)$.\n$$\np(r_i \\mid \\mathcal{M}_T) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)\\sqrt{2\\pi s^2}} \\frac{\\Gamma((\\nu+1)/2)}{\\left(\\frac{r_i^2}{2s^2} + \\frac{\\nu}{2}\\right)^{(\\nu+1)/2}}\n$$\nSimplifying this expression yields the PDF of the Student-$t$ distribution:\n$$\np(r_i \\mid \\mathcal{M}_T) = \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu\\pi} s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{r_i}{s}\\right)^2\\right)^{-(\\nu+1)/2}\n$$\nUnder model $\\mathcal{M}_T$, the residuals are also i.i.d. Thus, the joint marginal likelihood is the product of the individual densities:\n$$\np(\\mathbf{r} \\mid \\mathcal{M}_T) = \\prod_{i=1}^{N} \\left[ \\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu\\pi} s} \\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)^{-(\\nu+1)/2} \\right]\n$$\n\nTo avoid numerical underflow with very small likelihood values, it is best to compute the log-likelihoods first.\nThe log-likelihood for the Gaussian model is:\n$$\n\\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) = -\\frac{N}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} r_i^2\n$$\nThe log-likelihood for the Student-$t$ model is:\n$$\n\\ln p(\\mathbf{r} \\mid \\mathcal{M}_T) = N \\ln\\left(\\frac{\\Gamma((\\nu+1)/2)}{\\Gamma(\\nu/2) \\sqrt{\\nu\\pi} s}\\right) - \\frac{\\nu+1}{2}\\sum_{i=1}^{N}\\ln\\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)\n$$\nThe log of the Bayes factor is $\\ln(BF) = \\ln p(\\mathbf{r} \\mid \\mathcal{M}_T) - \\ln p(\\mathbf{r} \\mid \\mathcal{M}_G)$.\n\nNow, we substitute the numerical values.\nFor both models, $N=7$.\nThe sum of squared residuals is:\n$\\sum_{i=1}^{7} r_i^2 = (0.021)^2 + (-0.034)^2 + (0.018)^2 + (0.027)^2 + (-0.015)^2 + (0.031)^2 + (0.420)^2$\n$\\sum_{i=1}^{7} r_i^2 = 0.000441 + 0.001156 + 0.000324 + 0.000729 + 0.000225 + 0.000961 + 0.1764 = 0.180236$.\n\nFor $\\mathcal{M}_{G}$: $\\sigma=0.05$, so $\\sigma^2=0.0025$.\n$\\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) = -\\frac{7}{2}\\ln(2\\pi \\times 0.0025) - \\frac{1}{2 \\times 0.0025} (0.180236)$\n$\\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) = -3.5\\ln(0.015708) - 200 \\times 0.180236$\n$\\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) \\approx -3.5 \\times (-4.15333) - 36.0472 \\approx 14.53666 - 36.0472 = -21.51054$.\n\nFor $\\mathcal{M}_{T}$: $\\nu=3$, $s=0.05$.\nThe constant part of the log-likelihood is:\n$N \\ln\\left(\\frac{\\Gamma((3+1)/2)}{\\Gamma(3/2) \\sqrt{3\\pi} \\times 0.05}\\right) = 7 \\ln\\left(\\frac{\\Gamma(2)}{\\Gamma(1.5) \\sqrt{3\\pi} \\times 0.05}\\right)$\nUsing $\\Gamma(2)=1$ and $\\Gamma(1.5)=\\sqrt{\\pi}/2$:\n$7 \\ln\\left(\\frac{1}{(\\sqrt{\\pi}/2) \\sqrt{3\\pi} \\times 0.05}\\right) = 7 \\ln\\left(\\frac{2}{\\pi\\sqrt{3} \\times 0.05}\\right) = 7 \\ln\\left(\\frac{40}{\\pi\\sqrt{3}}\\right)$\n$7 \\ln(7.35104) \\approx 7 \\times 1.99484 \\approx 13.96388$.\nThe sum part is, with $\\nu s^2 = 3 \\times (0.05)^2 = 0.0075$:\n$-\\frac{3+1}{2} \\sum_{i=1}^{7}\\ln\\left(1 + \\frac{r_i^2}{0.0075}\\right) = -2 \\sum_{i=1}^{7}\\ln\\left(1 + \\frac{r_i^2}{0.0075}\\right)$\nThe terms in the sum are:\n$\\ln(1+0.021^2/0.0075) \\approx 0.05713$\n$\\ln(1+(-0.034)^2/0.0075) \\approx 0.14336$\n$\\ln(1+0.018^2/0.0075) \\approx 0.04229$\n$\\ln(1+0.027^2/0.0075) \\approx 0.09276$\n$\\ln(1+(-0.015)^2/0.0075) \\approx 0.02956$\n$\\ln(1+0.031^2/0.0075) \\approx 0.12056$\n$\\ln(1+0.420^2/0.0075) = \\ln(1+23.52) \\approx 3.19946$\nThe sum is $\\approx 3.68512$.\nSo, this part of the log-likelihood is $-2 \\times 3.68512 = -7.37024$.\n$\\ln p(\\mathbf{r} \\mid \\mathcal{M}_T) \\approx 13.96388 - 7.37024 = 6.59364$.\n\nNow, we compute the log Bayes factor:\n$\\ln(BF) = \\ln p(\\mathbf{r} \\mid \\mathcal{M}_T) - \\ln p(\\mathbf{r} \\mid \\mathcal{M}_G) \\approx 6.59364 - (-21.51054) = 28.10418$\nThe Bayes factor is:\n$BF = \\exp(28.10418) \\approx 1.5973 \\times 10^{12}$.\nRounding to three significant figures, we get $1.60 \\times 10^{12}$.\n\nThe extremely large value of the Bayes factor provides overwhelming evidence in favor of the Student-$t$ model ($\\mathcal{M}_{T}$) over the Gaussian model ($\\mathcal{M}_{G}$). This is a direct consequence of the data containing a significant outlier, $r_{7}=0.420$. The other residuals are all within approximately one standard deviation ($\\sigma=0.05$) of the mean ($0$), but $r_7$ is $0.420/0.05 = 8.4$ standard deviations away.\nThe Gaussian distribution has exponentially decaying tails, meaning it assigns an exceedingly low probability to such extreme events. This single outlier makes the entire dataset exceptionally unlikely under the Gaussian model, severely penalizing its marginal likelihood.\nIn contrast, the Student-$t$ distribution has heavier, polynomial-decaying tails. It is a robust distribution that can accommodate outliers by assigning them a small but not astronomically small probability. The outlier $r_7$ is far more plausible under the heavy-tailed Student-$t$ model than under the thin-tailed Gaussian model.\nIn the context of remote sensing calibration, this result highlights the importance of robust statistical modeling. Data from EO sensors are often contaminated by unmodeled effects (e.g., undetected clouds, aerosol plumes, surface anomalies), which manifest as outliers in the residuals. Using a standard Gaussian noise model can lead to poor model fit and biased calibration parameters, as the model is disproportionately influenced by the outliers. The strong preference for the Student-$t$ model demonstrates that employing a robust, heavy-tailed noise model provides a more faithful description of the data, preventing outliers from distorting the calibration and leading to more reliable scientific conclusions.",
            "answer": "$$\n\\boxed{1.60 \\times 10^{12}}\n$$"
        },
        {
            "introduction": "The previous exercises involved models where exact inference was possible. However, many sophisticated environmental models, such as those describing spatial fields, lead to posterior distributions that are analytically intractable and too high-dimensional to compute directly. This advanced practice introduces a powerful approximation technique: mean-field variational inference. You will derive the update equations for a variational approximation to the posterior in a model involving a Gaussian Markov Random Field (GMRF), a common prior for spatial data. This exercise provides hands-on experience with the computational machinery that makes Bayesian inference feasible for the complex, large-scale problems frequently encountered in modern remote sensing and environmental modeling .",
            "id": "3798329",
            "problem": "A spatial environmental field $x \\in \\mathbb{R}^{n}$ represents the latent, gridded columnar particulate matter concentration over a region of interest. Two heterogeneous remote sensing modalities are available for data fusion: a satellite radiometer and a ground-based Light Detection and Ranging (LIDAR) station network. Each modality provides linear measurements of the latent field with Gaussian noise. Specifically, for modality $i \\in \\{1,2\\}$, the observation model is $y_{i} \\in \\mathbb{R}^{m_{i}}$ with\n$$\ny_{i} \\mid x \\sim \\mathcal{N}\\!\\left(H_{i} x, R_{i}\\right),\n$$\nwhere $H_{i} \\in \\mathbb{R}^{m_{i} \\times n}$ is a known forward operator that maps the latent field to observations, and $R_{i} \\in \\mathbb{R}^{m_{i} \\times m_{i}}$ is a known, symmetric positive definite noise covariance. The latent field is endowed with a Gaussian Markov Random Field (GMRF) prior that penalizes spatial roughness via a graph Laplacian $L \\in \\mathbb{R}^{n \\times n}$, assumed symmetric positive definite, and an unknown scalar precision parameter $\\tau > 0$. The prior is\n$$\nx \\mid \\tau \\sim \\mathcal{N}\\!\\left(0, \\left(\\tau L\\right)^{-1}\\right).\n$$\nThe precision parameter $\\tau$ is given a Gamma prior with shape–rate parameterization,\n$$\n\\tau \\sim \\operatorname{Gamma}\\!\\left(\\alpha_{0}, \\beta_{0}\\right),\n$$\nwith probability density $p(\\tau) = \\frac{\\beta_{0}^{\\alpha_{0}}}{\\Gamma(\\alpha_{0})} \\tau^{\\alpha_{0}-1} \\exp\\!\\left(-\\beta_{0} \\tau\\right)$, for $\\alpha_{0} > 0$ and $\\beta_{0} > 0$.\n\nUsing Bayes’ rule and the factorization $p(y_{1}, y_{2}, x, \\tau) = p(y_{1} \\mid x) p(y_{2} \\mid x) p(x \\mid \\tau) p(\\tau)$, construct a mean-field variational approximation $q(x) q(\\tau)$ to the posterior $p(x, \\tau \\mid y_{1}, y_{2})$ and derive the coordinate ascent variational updates for $q(x)$ and $q(\\tau)$ based on expectations under the variational factors. You must start from first principles of the variational method and standard properties of the multivariate Gaussian and Gamma families. Your derivation must explicitly show how the expectations under $q$ appear in the updates, and must arrive at closed-form expressions for the variational parameters of $q(x)$ and $q(\\tau)$.\n\nExpress the final closed-form expressions for the variational mean and covariance of $q(x)$ and the shape and rate of $q(\\tau)$ using matrix notation and any necessary matrix operations. The final answer must be a single closed-form analytic expression. No numerical substitution is required. If you introduce any new symbols, define them clearly. No hints or intermediate shortcut formulas are permitted in the problem statement. You may assume all matrices involved are of conformable dimensions and symmetric positive definite where required.",
            "solution": "The problem requires the derivation of the coordinate ascent variational inference (CAVI) updates for a hierarchical Bayesian model. The goal is to approximate the true posterior distribution $p(x, \\tau \\mid y_1, y_2)$ with a factorized variational distribution $q(x, \\tau) = q(x)q(\\tau)$. In the mean-field variational framework, the optimal factor $q_j^*(z_j)$ for a partition of latent variables $z = \\{z_j\\}$ is given by\n$$\n\\ln q_j^*(z_j) = \\mathbb{E}_{q_{-j}}[\\ln p(y, z)] + \\text{const}\n$$\nwhere $p(y, z)$ is the joint probability of the data and all latent variables, and $\\mathbb{E}_{q_{-j}}$ denotes the expectation with respect to all other variational factors $q_k(z_k)$ for $k \\neq j$.\n\nThe joint probability of all variables is given by the factorization $p(y_1, y_2, x, \\tau) = p(y_1 \\mid x) p(y_2 \\mid x) p(x \\mid \\tau) p(\\tau)$. The log-joint probability is therefore\n$$\n\\ln p(y_1, y_2, x, \\tau) = \\ln p(y_1 \\mid x) + \\ln p(y_2 \\mid x) + \\ln p(x \\mid \\tau) + \\ln p(\\tau).\n$$\nThe individual log-densities are:\n$1$. For the likelihoods $y_i \\mid x \\sim \\mathcal{N}(H_i x, R_i)$:\n$$\n\\ln p(y_i \\mid x) = -\\frac{1}{2} (y_i - H_i x)^T R_i^{-1} (y_i - H_i x) - \\frac{m_i}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln|R_i|\n$$\n$2$. For the prior on $x \\mid \\tau \\sim \\mathcal{N}(0, (\\tau L)^{-1})$:\n$$\n\\ln p(x \\mid \\tau) = -\\frac{1}{2} x^T (\\tau L) x + \\frac{1}{2} \\ln|\\tau L| - \\frac{n}{2} \\ln(2\\pi) = -\\frac{\\tau}{2} x^T L x + \\frac{n}{2}\\ln\\tau + \\frac{1}{2}\\ln|L| - \\frac{n}{2}\\ln(2\\pi)\n$$\n$3$. For the prior on $\\tau \\sim \\text{Gamma}(\\alpha_0, \\beta_0)$:\n$$\n\\ln p(\\tau) = (\\alpha_0 - 1) \\ln\\tau - \\beta_0 \\tau + \\alpha_0 \\ln\\beta_0 - \\ln\\Gamma(\\alpha_0)\n$$\n\nWe now derive the update for each variational factor.\n\n**Derivation of the update for $q(x)$**\n\nThe optimal variational factor $q^*(x)$ is given by\n$$\n\\ln q^*(x) = \\mathbb{E}_{q(\\tau)}[\\ln p(y_1, y_2, x, \\tau)] + \\text{const}\n$$\nWe collect all terms from the log-joint probability that depend on $x$:\n$$\n\\ln q^*(x) \\propto \\mathbb{E}_{q(\\tau)} \\left[ \\ln p(y_1 \\mid x) + \\ln p(y_2 \\mid x) + \\ln p(x \\mid \\tau) \\right].\n$$\nSubstituting the log-densities and discarding terms not involving $x$:\n$$\n\\ln q^*(x) \\propto \\mathbb{E}_{q(\\tau)} \\left[ -\\frac{1}{2} \\sum_{i=1}^2 (y_i - H_i x)^T R_i^{-1} (y_i - H_i x) - \\frac{\\tau}{2} x^T L x \\right].\n$$\nExpanding the quadratic terms for the likelihoods:\n$$\n(y_i - H_i x)^T R_i^{-1} (y_i - H_i x) = x^T H_i^T R_i^{-1} H_i x - 2 y_i^T R_i^{-1} H_i x + y_i^T R_i^{-1} y_i.\n$$\nThe term $y_i^T R_i^{-1} y_i$ is constant with respect to $x$ and can be absorbed into the constant. Thus,\n$$\n\\ln q^*(x) \\propto \\mathbb{E}_{q(\\tau)} \\left[ -\\frac{1}{2} \\sum_{i=1}^2 (x^T H_i^T R_i^{-1} H_i x - 2 y_i^T R_i^{-1} H_i x) - \\frac{\\tau}{2} x^T L x \\right].\n$$\nTaking the expectation with respect to $q(\\tau)$:\n$$\n\\ln q^*(x) \\propto -\\frac{1}{2} \\sum_{i=1}^2 (x^T H_i^T R_i^{-1} H_i x - 2 y_i^T R_i^{-1} H_i x) - \\frac{\\mathbb{E}_{q(\\tau)}[\\tau]}{2} x^T L x.\n$$\nWe can rearrange this into the canonical form of a multivariate Gaussian log-density, which is proportional to $-\\frac{1}{2} x^T \\Sigma^{-1} x + \\mu^T \\Sigma^{-1} x$:\n$$\n\\ln q^*(x) \\propto -\\frac{1}{2} x^T \\left( \\sum_{i=1}^2 H_i^T R_i^{-1} H_i + \\mathbb{E}_{q(\\tau)}[\\tau] L \\right) x + \\left( \\sum_{i=1}^2 y_i^T R_i^{-1} H_i \\right) x.\n$$\nBy comparing this to the general form, we identify $q^*(x)$ as a Gaussian distribution, $q^*(x) = \\mathcal{N}(x \\mid \\mu_x, \\Sigma_x)$, with precision matrix $\\Sigma_x^{-1}$ and mean $\\mu_x$ given by:\n$$\n\\Sigma_x^{-1} = \\sum_{i=1}^2 H_i^T R_i^{-1} H_i + \\mathbb{E}_{q(\\tau)}[\\tau] L\n$$\n$$\n\\mu_x^T \\Sigma_x^{-1} = \\sum_{i=1}^2 y_i^T R_i^{-1} H_i \\implies \\mu_x = \\Sigma_x \\left( \\sum_{i=1}^2 H_i^T R_i^{-1} y_i \\right)\n$$\nSo, the parameters for the optimal variational distribution $q^*(x)$ are:\n$$\n\\Sigma_x = \\left(\\sum_{i=1}^{2} H_i^T R_i^{-1} H_i + \\mathbb{E}_{q(\\tau)}[\\tau] L\\right)^{-1}\n$$\n$$\n\\mu_x = \\Sigma_x \\left(\\sum_{i=1}^{2} H_i^T R_i^{-1} y_i\\right)\n$$\n\n**Derivation of the update for $q(\\tau)$**\n\nThe optimal variational factor $q^*(\\tau)$ is given by\n$$\n\\ln q^*(\\tau) = \\mathbb{E}_{q(x)}[\\ln p(y_1, y_2, x, \\tau)] + \\text{const}\n$$\nWe collect all terms from the log-joint probability that depend on $\\tau$:\n$$\n\\ln q^*(\\tau) \\propto \\mathbb{E}_{q(x)}[\\ln p(x \\mid \\tau) + \\ln p(\\tau)].\n$$\nSubstituting the log-densities and discarding terms not involving $\\tau$:\n$$\n\\ln q^*(\\tau) \\propto \\mathbb{E}_{q(x)} \\left[ -\\frac{\\tau}{2} x^T L x + \\frac{n}{2}\\ln\\tau + (\\alpha_0 - 1) \\ln\\tau - \\beta_0 \\tau \\right].\n$$\nTaking the expectation with respect to $q(x)$:\n$$\n\\ln q^*(\\tau) \\propto -\\frac{\\tau}{2} \\mathbb{E}_{q(x)}[x^T L x] + \\left(\\frac{n}{2} + \\alpha_0 - 1\\right) \\ln\\tau - \\beta_0 \\tau.\n$$\nThis expression can be rearranged into the form of a Gamma log-density, which is proportional to $(\\alpha - 1)\\ln\\tau - \\beta\\tau$:\n$$\n\\ln q^*(\\tau) \\propto \\left(\\alpha_0 + \\frac{n}{2} - 1\\right) \\ln\\tau - \\left(\\beta_0 + \\frac{1}{2} \\mathbb{E}_{q(x)}[x^T L x]\\right) \\tau.\n$$\nBy comparison, we see that $q^*(\\tau)$ is a Gamma distribution, $q^*(\\tau) = \\text{Gamma}(\\tau \\mid \\alpha_\\tau, \\beta_\\tau)$, with shape parameter $\\alpha_\\tau$ and rate parameter $\\beta_\\tau$.\nThe shape parameter is:\n$$\n\\alpha_\\tau = \\alpha_0 + \\frac{n}{2}\n$$\nThe rate parameter is:\n$$\n\\beta_\\tau = \\beta_0 + \\frac{1}{2} \\mathbb{E}_{q(x)}[x^T L x]\n$$\nTo complete this expression, we need to compute the expectation of the quadratic form $\\mathbb{E}_{q(x)}[x^T L x]$. For a random vector $x \\sim \\mathcal{N}(\\mu, \\Sigma)$ and a matrix $A$, the expectation is $\\mathbb{E}[x^T A x] = \\text{Tr}(A\\Sigma) + \\mu^T A \\mu$. In our case, $q(x) = \\mathcal{N}(x \\mid \\mu_x, \\Sigma_x)$, so\n$$\n\\mathbb{E}_{q(x)}[x^T L x] = \\text{Tr}(L\\Sigma_x) + \\mu_x^T L \\mu_x.\n$$\nSubstituting this back into the expression for $\\beta_\\tau$:\n$$\n\\beta_\\tau = \\beta_0 + \\frac{1}{2} \\left( \\text{Tr}(L\\Sigma_x) + \\mu_x^T L \\mu_x \\right).\n$$\n\n**Summary of Variational Parameter Updates**\n\nThe coordinate ascent algorithm proceeds by iteratively updating the parameters of $q(x)$ and $q(\\tau)$ until convergence. The update for $q(x)$ requires the expectation $\\mathbb{E}_{q(\\tau)}[\\tau]$. Since $q(\\tau) = \\text{Gamma}(\\tau \\mid \\alpha_\\tau, \\beta_\\tau)$, this expectation is $\\mathbb{E}_{q(\\tau)}[\\tau] = \\alpha_\\tau / \\beta_\\tau$. The update for $q(\\tau)$ requires moments of $x$ under $q(x)$.\n\nThe closed-form expressions for the parameters of the optimal variational distributions $q^*(x) = \\mathcal{N}(x|\\mu_x, \\Sigma_x)$ and $q^*(\\tau) = \\text{Gamma}(\\tau|\\alpha_\\tau, \\beta_\\tau)$ are:\nThe parameters for $q^*(x)$:\n- Variational mean $\\mu_x$: $\\mu_x = \\left(\\sum_{i=1}^2 H_i^T R_i^{-1} H_i + \\frac{\\alpha_\\tau}{\\beta_\\tau} L \\right)^{-1} \\left(\\sum_{i=1}^2 H_i^T R_i^{-1} y_i\\right)$\n- Variational covariance $\\Sigma_x$: $\\Sigma_x = \\left(\\sum_{i=1}^2 H_i^T R_i^{-1} H_i + \\frac{\\alpha_\\tau}{\\beta_\\tau} L \\right)^{-1}$\n\nThe parameters for $q^*(\\tau)$:\n- Variational shape $\\alpha_\\tau$: $\\alpha_\\tau = \\alpha_0 + \\frac{n}{2}$\n- Variational rate $\\beta_\\tau$: $\\beta_\\tau = \\beta_0 + \\frac{1}{2} \\left( \\mu_x^T L \\mu_x + \\text{Tr}(L \\Sigma_x) \\right)$\n\nThese equations define the fixed point of the CAVI algorithm.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\mu_x & \\Sigma_x & \\alpha_\\tau & \\beta_\\tau \\end{pmatrix} = \\begin{pmatrix} \\left( \\sum_{i=1}^2 H_i^T R_i^{-1} H_i + \\frac{\\alpha_\\tau}{\\beta_\\tau} L \\right)^{-1} \\left( \\sum_{i=1}^2 H_i^T R_i^{-1} y_i \\right) & \\left( \\sum_{i=1}^2 H_i^T R_i^{-1} H_i + \\frac{\\alpha_\\tau}{\\beta_\\tau} L \\right)^{-1} & \\alpha_0 + \\frac{n}{2} & \\beta_0 + \\frac{1}{2}\\left( \\mu_x^T L \\mu_x + \\text{Tr}\\left(L \\Sigma_x\\right) \\right) \\end{pmatrix}}\n$$"
        }
    ]
}