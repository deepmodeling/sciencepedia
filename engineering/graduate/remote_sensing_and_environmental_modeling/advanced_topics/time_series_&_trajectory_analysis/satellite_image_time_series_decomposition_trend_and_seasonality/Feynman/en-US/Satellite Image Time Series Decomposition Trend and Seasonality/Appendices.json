{
    "hands_on_practices": [
        {
            "introduction": "Before we can decompose a time series, we must first construct it from raw satellite imagery, a process fraught with challenges like cloud cover. Maximum Value Compositing (MVC) is a widely used technique to mitigate cloud contamination by selecting the highest pixel value over a given time interval. This practice guides you through a first-principles derivation to quantify how this common pre-processing step can introduce systematic biases, particularly by artificially inflating the seasonal amplitude in regions with distinct cloudy and clear seasons. Understanding this effect is critical for interpreting time series data in the tropics and beyond .",
            "id": "3843784",
            "problem": "A remote sensing researcher constructs monthly composites of the Normalized Difference Vegetation Index (NDVI) time series over a humid tropical forest using Maximum Value Compositing (MVC). Maximum Value Compositing (MVC) selects, within each month, the largest observed NDVI value among all satellite acquisitions in that month. The underlying NDVI signal for a given season is modeled by the canonical time series decomposition as the sum of a constant trend, a seasonal mean specific to the season, and residual measurement error. Assume the trend is constant across the two seasons under consideration so that the seasonal mean controls the month-to-month variations within a year. Let the seasonal means be $S_{\\mathrm{wet}}$ for the wet season and $S_{\\mathrm{dry}}$ for the dry season, and define the true seasonal amplitude as $A_{\\mathrm{true}} = S_{\\mathrm{dry}} - S_{\\mathrm{wet}}$.\n\nWithin any given month in season $s \\in \\{\\mathrm{wet}, \\mathrm{dry}\\}$, suppose there are $n$ independent satellite acquisitions. Each acquisition is either cloud-free with probability $p_{s}$ or cloud-contaminated with probability $1 - p_{s}$. Cloud contamination reduces the observed NDVI by an additive attenuation $c > 0$ relative to the cloud-free observation for that acquisition. Neglect measurement noise and bidirectional reflectance effects, and assume independence of cloud occurrence across acquisitions within a month.\n\nStarting from the above model assumptions and the definition of MVC as a maximum over the $n$ monthly observations, derive the expected MVC composite for the wet and dry seasons, and then derive the expected MVC-estimated seasonal amplitude $A_{\\mathrm{MVC}}$. Use these to obtain the amplitude bias $\\Delta A = A_{\\mathrm{MVC}} - A_{\\mathrm{true}}$. In your reasoning, explain why MVC can inflate the apparent seasonality in humid tropics when cloud-free probabilities differ by season.\n\nProvide the final answer as a single closed-form analytic expression for the amplitude bias $\\Delta A$ in terms of $c$, $p_{\\mathrm{wet}}$, $p_{\\mathrm{dry}}$, and $n$. No numerical evaluation is required.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of remote sensing and statistical modeling, is well-posed with a clear objective and sufficient information, and uses objective, formal language. There are no contradictions, ambiguities, or violations of scientific principles. We may therefore proceed with a formal solution.\n\nThe core of the problem is to determine how the Maximum Value Compositing (MVC) method introduces a systematic bias in the estimation of seasonal amplitude due to differential cloud cover between seasons. We will derive the expected value of the NDVI composite for each season and then compute the resulting amplitude and its bias.\n\nLet $S_s$ be the true seasonal mean Normalized Difference Vegetation Index (NDVI) for season $s \\in \\{\\mathrm{wet}, \\mathrm{dry}\\}$, relative to a constant trend component which will cancel in all difference calculations. An individual satellite acquisition within a given month of season $s$ is either cloud-free or cloud-contaminated.\nThe observed NDVI for a single acquisition, which we denote as a random variable $X_s$, can take one of two values:\n1.  If the acquisition is cloud-free (with probability $p_s$), the observed NDVI is $S_s$.\n2.  If the acquisition is cloud-contaminated (with probability $1 - p_s$), the observed NDVI is reduced by an attenuation factor $c$, resulting in an observation of $S_s - c$.\n\nThe probability distribution for a single observation $X_s$ is therefore:\n$$\nP(X_s = S_s) = p_s \\\\\nP(X_s = S_s - c) = 1 - p_s\n$$\nwhere $c > 0$.\n\nMaximum Value Compositing (MVC) selects the maximum value from the $n$ independent acquisitions within a month. Let these $n$ independent and identically distributed observations be $X_{s,1}, X_{s,2}, \\dots, X_{s,n}$. The MVC composite value for that month, denoted $M_s$, is given by:\n$$\nM_s = \\max(X_{s,1}, X_{s,2}, \\dots, X_{s,n})\n$$\nThe random variable $M_s$ can also only take two possible values: $S_s$ or $S_s - c$. The value of $M_s$ will be $S_s$ if at least one of the $n$ acquisitions is cloud-free. The value of $M_s$ will be $S_s - c$ only if all $n$ acquisitions are cloud-contaminated.\n\nThe probability that a single acquisition is cloud-contaminated is $1 - p_s$. Since the cloud occurrences are independent across the $n$ acquisitions, the probability that all $n$ acquisitions are cloud-contaminated is $(1 - p_s)^n$.\n$$\nP(M_s = S_s - c) = P(\\text{all } n \\text{ acquisitions are cloudy}) = (1 - p_s)^n\n$$\nThe event that $M_s = S_s$ is the complement, meaning at least one acquisition is cloud-free. The probability of this event is:\n$$\nP(M_s = S_s) = 1 - P(M_s = S_s - c) = 1 - (1 - p_s)^n\n$$\nNow, we can compute the expected value of the MVC composite for season $s$, denoted $E[M_s]$.\n$$\nE[M_s] = (S_s) \\cdot P(M_s = S_s) + (S_s - c) \\cdot P(M_s = S_s - c)\n$$\nSubstituting the probabilities we derived:\n$$\nE[M_s] = S_s \\left( 1 - (1 - p_s)^n \\right) + (S_s - c) \\left( (1 - p_s)^n \\right)\n$$\nExpanding the terms:\n$$\nE[M_s] = S_s - S_s (1 - p_s)^n + S_s (1 - p_s)^n - c (1 - p_s)^n\n$$\nThe $S_s (1 - p_s)^n$ terms cancel, leaving:\n$$\nE[M_s] = S_s - c (1 - p_s)^n\n$$\nThis expression represents the expected monthly NDVI composite for season $s$. It shows that the expected composite is the true seasonal mean $S_s$ minus a negative bias term, $c (1 - p_s)^n$, which depends on the cloud attenuation $c$, the number of looks $n$, and the probability of a clear observation $p_s$.\n\nWe apply this general result to the wet and dry seasons. Let $p_{\\mathrm{wet}}$ and $p_{\\mathrm{dry}}$ be the cloud-free probabilities for the wet and dry seasons, respectively.\nThe expected MVC composite for the wet season is:\n$$\nE[M_{\\mathrm{wet}}] = S_{\\mathrm{wet}} - c (1 - p_{\\mathrm{wet}})^n\n$$\nThe expected MVC composite for the dry season is:\n$$\nE[M_{\\mathrm{dry}}] = S_{\\mathrm{dry}} - c (1 - p_{\\mathrm{dry}})^n\n$$\nThe expected MVC-estimated seasonal amplitude, $A_{\\mathrm{MVC}}$, is the difference between these expected composite values:\n$$\nA_{\\mathrm{MVC}} = E[M_{\\mathrm{dry}}] - E[M_{\\mathrm{wet}}]\n$$\n$$\nA_{\\mathrm{MVC}} = \\left( S_{\\mathrm{dry}} - c (1 - p_{\\mathrm{dry}})^n \\right) - \\left( S_{\\mathrm{wet}} - c (1 - p_{\\mathrm{wet}})^n \\right)\n$$\nRearranging the terms to group the true seasonal means and the bias terms:\n$$\nA_{\\mathrm{MVC}} = (S_{\\mathrm{dry}} - S_{\\mathrm{wet}}) - c \\left( (1 - p_{\\mathrm{dry}})^n - (1 - p_{\\mathrm{wet}})^n \\right)\n$$\nThe true seasonal amplitude is defined as $A_{\\mathrm{true}} = S_{\\mathrm{dry}} - S_{\\mathrm{wet}}$. Substituting this into the expression for $A_{\\mathrm{MVC}}$ gives:\n$$\nA_{\\mathrm{MVC}} = A_{\\mathrm{true}} - c \\left( (1 - p_{\\mathrm{dry}})^n - (1 - p_{\\mathrm{wet}})^n \\right)\n$$\nThe amplitude bias, $\\Delta A$, is defined as $\\Delta A = A_{\\mathrm{MVC}} - A_{\\mathrm{true}}$.\n$$\n\\Delta A = \\left( A_{\\mathrm{true}} - c \\left( (1 - p_{\\mathrm{dry}})^n - (1 - p_{\\mathrm{wet}})^n \\right) \\right) - A_{\\mathrm{true}}\n$$\n$$\n\\Delta A = - c \\left( (1 - p_{\\mathrm{dry}})^n - (1 - p_{\\mathrm{wet}})^n \\right)\n$$\nThis can be written more cleanly by reversing the terms inside the parenthesis:\n$$\n\\Delta A = c \\left( (1 - p_{\\mathrm{wet}})^n - (1 - p_{\\mathrm{dry}})^n \\right)\n$$\nThis is the final expression for the amplitude bias.\n\nTo explain why seasonality is inflated, we consider the typical conditions in a humid tropical forest, where the wet season is significantly cloudier than the dry season. This implies $p_{\\mathrm{wet}}  p_{\\mathrm{dry}}$.\nSince $p_{\\mathrm{wet}}  p_{\\mathrm{dry}}$, it follows that $(1 - p_{\\mathrm{wet}}) > (1 - p_{\\mathrm{dry}})$.\nFor $n \\ge 1$, this inequality is preserved when raised to the power of $n$: $(1 - p_{\\mathrm{wet}})^n > (1 - p_{\\mathrm{dry}})^n$.\nThe quantity $(1 - p_s)^n$ is the probability of failing to obtain a single cloud-free observation in season $s$. This failure is more likely in the cloudier wet season.\nThe term $c (1 - p_s)^n$ represents the magnitude of the negative bias (the expected reduction) in the MVC composite for season $s$. Since $(1 - p_{\\mathrm{wet}})^n > (1 - p_{\\mathrm{dry}})^n$ and $c > 0$, the magnitude of the negative bias is greater for the wet season than for the dry season.\nThis means the MVC procedure systematically underestimates the wet season NDVI more severely than it underestimates the dry season NDVI. When the estimated amplitude $A_{\\mathrm{MVC}}$ is computed by subtracting the more suppressed wet-season value from the less suppressed dry-season value, the resulting difference is larger than the true difference $A_{\\mathrm{true}}$.\nMathematically, since $(1 - p_{\\mathrm{wet}})^n > (1 - p_{\\mathrm{dry}})^n$, the term $(1 - p_{\\mathrm{wet}})^n - (1 - p_{\\mathrm{dry}})^n$ is positive. As $c$ is also positive, the amplitude bias $\\Delta A = c \\left( (1 - p_{\\mathrm{wet}})^n - (1 - p_{\\mathrm{dry}})^n \\right)$ is positive. A positive bias means $A_{\\mathrm{MVC}} > A_{\\mathrm{true}}$, which is an inflation or exaggeration of the seasonal amplitude.",
            "answer": "$$\n\\boxed{c \\left( (1 - p_{\\mathrm{wet}})^{n} - (1 - p_{\\mathrm{dry}})^{n} \\right)}\n$$"
        },
        {
            "introduction": "Once a time series is assembled, the next step is to decompose its constituent parts, but real-world data is rarely perfect. Outliers from residual clouds, sensor noise, or processing artifacts can disproportionately influence standard trend fitting methods like Ordinary Least Squares (OLS), leading to inaccurate results. This hands-on coding exercise introduces a powerful alternative, robust regression, which is designed to downweight the influence of such outliers. You will implement a robust trend estimator using the Huber loss function and compare its performance against OLS, providing a concrete demonstration of how robust methods yield more reliable estimates of both trend and seasonal amplitude .",
            "id": "3843866",
            "problem": "A satellite image time series of surface reflectance (unitless decimal fraction) is modeled as an additive decomposition into a slowly varying trend and a periodic seasonal component. Let the observation at time index $t \\in \\{0,1,\\dots,N-1\\}$ be denoted by $y_t$ and suppose\n$$\ny_t = \\tau(t) + s(t) + \\varepsilon_t + o_t,\n$$\nwhere $\\tau(t)$ is the trend, $s(t)$ is the seasonal component, $\\varepsilon_t$ is zero-mean noise, and $o_t$ represents occasional outliers due to, for example, residual clouds or bidirectional reflectance effects not fully corrected. The seasonal component is assumed to have known period $P$ (in samples) and to be representable by a single harmonic,\n$$\ns(t) = a \\cos\\left(\\frac{2\\pi t}{P}\\right) + b \\sin\\left(\\frac{2\\pi t}{P}\\right),\n$$\nwith amplitude $A = \\sqrt{a^2 + b^2}$, and angles are in radians.\n\nTo reduce the influence of outliers on the trend estimation, you are to construct a robust estimator for $\\tau(t)$ using the Huber loss. Specifically, represent the trend by a first-order polynomial $\\tau(t) = \\beta_0 + \\beta_1 t$, and estimate the parameters $\\beta_0, \\beta_1$ by minimizing the Huber objective\n$$\nJ(\\beta_0,\\beta_1) = \\sum_{t=0}^{N-1} \\rho_\\delta\\!\\left(y_t - \\beta_0 - \\beta_1 t\\right),\n$$\nwhere $\\rho_\\delta(r)$ is the Huber loss with threshold $\\delta > 0$,\n$$\n\\rho_\\delta(r) = \n\\begin{cases}\n\\frac{1}{2} r^2,  \\text{if } |r| \\le \\delta, \\\\\n\\delta \\left(|r| - \\frac{1}{2}\\delta\\right),  \\text{if } |r| > \\delta.\n\\end{cases}\n$$\nAfter estimating the trend, obtain the seasonal amplitude $A$ by detrending the series and performing least squares harmonic regression onto $\\cos\\left(\\frac{2\\pi t}{P}\\right)$ and $\\sin\\left(\\frac{2\\pi t}{P}\\right)$. Repeat the amplitude estimation using a non-robust ordinary least squares trend estimate for comparison. The task is to implement Iteratively Reweighted Least Squares (IRLS) to solve the robust Huber trend estimation problem, then quantify the effect of the robust trend on the estimated seasonal amplitude compared to the least squares trend.\n\nYour program must:\n- Generate synthetic time series $y_t$ according to the specified additive model for each test case, with the following components:\n  - Trend $\\tau(t) = \\beta_0 + \\beta_1 t$.\n  - Seasonal component $s(t) = A_{\\text{true}} \\cos\\left(\\frac{2\\pi t}{P}\\right)$ (use zero phase for data generation).\n  - Gaussian noise $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ with standard deviation $\\sigma$.\n  - Outliers $o_t$ injected uniformly at random at a specified fraction of indices, with magnitudes drawn as independent signs multiplied by a specified outlier magnitude.\n- Estimate the trend coefficients $(\\beta_0,\\beta_1)$ using:\n  - Ordinary least squares (non-robust) on $\\{(t,y_t)\\}$.\n  - Robust Huber regression via Iteratively Reweighted Least Squares on $\\{(t,y_t)\\}$ with threshold $\\delta$.\n- Compute the seasonal amplitude $A$ by least squares harmonic regression on the detrended series for both trend estimates, using the known period $P$.\n- For each test case, return a single float equal to $A_{\\text{Huber}} - A_{\\text{OLS}}$, in reflectance units, expressed as a decimal number (no percentage).\n\nUse the following test suite of parameter values, chosen to cover typical and edge-case conditions in satellite image time series decomposition. For each tuple, the parameters are $(N, P, \\beta_0, \\beta_1, A_{\\text{true}}, \\sigma, f_{\\text{out}}, M_{\\text{out}}, \\delta)$:\n1. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (60, 12, 0.4, 0.0015, 0.15, 0.02, 0.10, 0.35, 0.05)$.\n2. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (60, 12, 0.5, 0.0000, 0.12, 0.01, 0.00, 0.00, 0.05)$.\n3. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (48, 12, 0.3, 0.0020, 0.08, 0.03, 0.25, 0.50, 0.03)$.\n4. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (36, 12, 0.6, -0.0010, 0.10, 0.015, 0.15, 0.40, 0.005)$.\n5. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (24, 12, 0.45, 0.0010, 0.00, 0.02, 0.20, 0.30, 0.05)$.\n\nAll amplitudes and differences must be in reflectance units (unitless decimal fractions). Angles are in radians. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5]$, where $r_i$ is the amplitude difference $A_{\\text{Huber}} - A_{\\text{OLS}}$ for test case $i$.",
            "solution": "The posed problem is valid as it is scientifically grounded in statistical signal processing, well-posed with a clear objective, and provides all necessary information to construct a unique, verifiable solution. The underlying methods—additive time series decomposition, Ordinary Least Squares (OLS), Huber regression, and Iteratively Reweighted Least Squares (IRLS)—are standard and mathematically sound. We proceed with a detailed solution.\n\nThe core of the problem is to compare the impact of a non-robust versus a robust trend estimation on the subsequent estimation of a seasonal signal's amplitude. The time series is modeled as:\n$$\ny_t = \\tau(t) + s(t) + \\varepsilon_t + o_t\n$$\nwhere $y_t$ is the observation at time $t$, $\\tau(t)$ is a linear trend, $s(t)$ is a sinusoidal seasonal component, $\\varepsilon_t$ is Gaussian noise, and $o_t$ represents sporadic outliers.\n\n**Step 1: Synthetic Data Generation**\nFor each test case, we synthesize a time series of length $N$. The time vector is $\\mathbf{t} = [0, 1, \\dots, N-1]^T$.\n1.  **Trend Component**: The linear trend is given by $\\tau(t) = \\beta_0 + \\beta_1 t$.\n2.  **Seasonal Component**: The problem specifies a seasonal signal with a known period $P$ and zero phase for data generation, $s(t) = A_{\\text{true}} \\cos(2\\pi t/P)$.\n3.  **Noise Component**: I.i.d. Gaussian noise samples $\\varepsilon_t$ are drawn from $\\mathcal{N}(0, \\sigma^2)$.\n4.  **Outlier Component**: Outliers are added to a fraction $f_{\\text{out}}$ of the data points. We calculate the number of outliers $N_{\\text{out}} = \\text{round}(N \\cdot f_{\\text{out}})$, randomly select $N_{\\text{out}}$ unique time indices, and at these indices, set $o_t = \\pm M_{\\text{out}}$, with the sign chosen randomly. For all other indices, $o_t = 0$.\n\nThe final observed time series is the sum of these four components: $\\mathbf{y} = \\boldsymbol{\\tau} + \\mathbf{s} + \\boldsymbol{\\varepsilon} + \\mathbf{o}$. All random number generation is performed using a fixed seed to ensure the solution is reproducible.\n\n**Step 2: Trend Estimation**\nThe trend is modeled as a linear function $\\tau(t) = \\beta_0 + \\beta_1 t$. In matrix form, we seek to find the parameter vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ in the model $\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\beta}$, where $\\mathbf{X}$ is the $N \\times 2$ design matrix with columns for the intercept and the linear term:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ \\vdots  \\vdots \\\\ 1  N-1 \\end{pmatrix}\n$$\n\n**A. Ordinary Least Squares (OLS) Trend Estimation**\nThe OLS method finds the parameters $\\boldsymbol{\\beta}$ that minimize the sum of squared residuals, $||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2$. This is a standard linear regression problem whose solution is given by the normal equations:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nThis solution is sensitive to outliers because large residuals are squared, giving them a disproportionately high influence on the fit. The estimated trend is $\\hat{\\boldsymbol{\\tau}}_{\\text{OLS}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$.\n\n**B. Robust Huber Trend Estimation via Iteratively Reweighted Least Squares (IRLS)**\nThe Huber M-estimator minimizes a different objective function, $J(\\boldsymbol{\\beta}) = \\sum_{t=0}^{N-1} \\rho_\\delta(y_t - (\\mathbf{X}\\boldsymbol{\\beta})_t)$, using the Huber loss $\\rho_\\delta(r)$. This loss function behaves quadratically for small residuals ($|r| \\le \\delta$) like OLS, but linearly for large residuals ($|r| > \\delta$), which reduces the influence of outliers.\n\nThis optimization problem is solved using IRLS. The algorithm iteratively solves a weighted least squares problem where the weights adapt to the residuals from the previous iteration. The update step for the parameters is:\n$$\n\\hat{\\boldsymbol{\\beta}}^{(k+1)} = (\\mathbf{X}^T\\mathbf{W}^{(k)}\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{W}^{(k)}\\mathbf{y}\n$$\nThe matrix $\\mathbf{W}^{(k)}$ is a diagonal matrix of weights $w_t^{(k)}$ at iteration $k$. Each weight is a function of the corresponding residual $r_t^{(k)} = y_t - (\\mathbf{X}\\hat{\\boldsymbol{\\beta}}^{(k)})_t$:\n$$\nw_t^{(k)} = \\begin{cases} 1,  \\text{if } |r_t^{(k)}| \\le \\delta \\\\ \\delta/|r_t^{(k)}|,  \\text{if } |r_t^{(k)}| > \\delta \\end{cases}\n$$\nThe algorithm proceeds as follows:\n1.  Initialize parameters $\\hat{\\boldsymbol{\\beta}}^{(0)}$, typically with the OLS solution $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$.\n2.  Iteratively compute residuals, update weights, and solve the weighted least squares problem. For numerical stability, the update is not performed by matrix inversion but by solving an equivalent standard least squares problem.\n3.  The iterations continue until the change in the parameter vector $\\hat{\\boldsymbol{\\beta}}$ between successive iterations falls below a specified tolerance, or a maximum number of iterations is reached.\nThe converged solution $\\hat{\\boldsymbol{\\beta}}_{\\text{Huber}}$ gives the robustly estimated trend $\\hat{\\boldsymbol{\\tau}}_{\\text{Huber}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{Huber}}$.\n\n**Step 3: Seasonal Amplitude Estimation**\nThe estimated trend is subtracted from the original data to obtain a detrended series. This is done for both trend estimates:\n$$\n\\mathbf{d}_{\\text{OLS}} = \\mathbf{y} - \\hat{\\boldsymbol{\\tau}}_{\\text{OLS}} \\quad \\text{and} \\quad \\mathbf{d}_{\\text{Huber}} = \\mathbf{y} - \\hat{\\boldsymbol{\\tau}}_{\\text{Huber}}\n$$\nWe then fit a harmonic model to each detrended series. The model is $d(t) \\approx a \\cos(2\\pi t/P) + b \\sin(2\\pi t/P)$. The coefficients $\\boldsymbol{\\gamma} = [a, b]^T$ are found via least squares. The design matrix for this regression is:\n$$\n\\mathbf{S} = \\begin{pmatrix}\n\\cos(\\frac{2\\pi \\cdot 0}{P})  \\sin(\\frac{2\\pi \\cdot 0}{P}) \\\\\n\\vdots  \\vdots \\\\\n\\cos(\\frac{2\\pi \\cdot (N-1)}{P})  \\sin(\\frac{2\\pi \\cdot (N-1)}{P})\n\\end{pmatrix}\n$$\nThe solution for the coefficients is $\\hat{\\boldsymbol{\\gamma}} = (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{S}^T\\mathbf{d}$. This yields coefficient vectors $\\hat{\\boldsymbol{\\gamma}}_{\\text{OLS}}$ and $\\hat{\\boldsymbol{\\gamma}}_{\\text{Huber}}$. The corresponding amplitudes are the Euclidean norms of these vectors:\n$$\nA_{\\text{OLS}} = ||\\hat{\\boldsymbol{\\gamma}}_{\\text{OLS}}||_2 \\quad \\text{and} \\quad A_{\\text{Huber}} = ||\\hat{\\boldsymbol{\\gamma}}_{\\text{Huber}}||_2\n$$\n\n**Step 4: Final Comparison**\nThe final metric for each test case is the difference between the two estimated amplitudes, which quantifies the effect of the robust trend estimation:\n$$\n\\Delta A = A_{\\text{Huber}} - A_{\\text{OLS}}\n$$\nA non-zero value indicates that the choice of detrending method influences the seasonal analysis. In the presence of outliers, the OLS trend can be significantly biased, leading to a polluted detrended series and an inaccurate amplitude estimate. The Huber regression, being robust, should yield a more accurate trend, and consequently, a more reliable seasonal amplitude.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lstsq\n\ndef generate_time_series(N, P, beta0, beta1, A_true, sigma, f_out, M_out, rng):\n    \"\"\"Generates a synthetic time series based on the additive model.\"\"\"\n    t = np.arange(N)\n    \n    # 1. Trend component\n    trend = beta0 + beta1 * t\n    \n    # 2. Seasonal component\n    seasonal = A_true * np.cos(2 * np.pi * t / P)\n    \n    # 3. Noise component\n    noise = rng.normal(0, sigma, N)\n    \n    # 4. Outlier component\n    outliers = np.zeros(N)\n    n_outliers = int(round(f_out * N))\n    if n_outliers > 0:\n        outlier_indices = rng.choice(N, size=n_outliers, replace=False)\n        outlier_signs = rng.choice([-1, 1], size=n_outliers)\n        outliers[outlier_indices] = outlier_signs * M_out\n        \n    y = trend + seasonal + noise + outliers\n    return t, y\n\ndef estimate_trend_ols(t, y):\n    \"\"\"Estimates trend coefficients using Ordinary Least Squares.\"\"\"\n    X = np.stack([np.ones_like(t), t], axis=1)\n    beta_ols, _, _, _ = lstsq(X, y)\n    return beta_ols\n\ndef estimate_trend_huber_irls(t, y, delta, max_iter=100, tol=1e-6):\n    \"\"\"Estimates trend coefficients using Huber regression via IRLS.\"\"\"\n    X = np.stack([np.ones_like(t), t], axis=1)\n    \n    # Initialize with OLS solution\n    beta = estimate_trend_ols(t, y)\n    \n    for _ in range(max_iter):\n        residuals = y - X @ beta\n        abs_residuals = np.abs(residuals)\n        \n        # Calculate weights based on Huber's psi-function/residual\n        # w(r) = min(1, delta/|r|)\n        # Add a small epsilon to avoid division by zero, although not strictly necessary\n        # as |r| > delta for the second case.\n        weights = np.minimum(1.0, delta / (abs_residuals + 1e-8))\n        \n        # Perform weighted least squares\n        # This is equivalent to solving lstsq for L*y and L*X where L = sqrt(W)\n        L = np.sqrt(weights)\n        X_w = X * L[:, np.newaxis]\n        y_w = y * L\n        \n        beta_new, _, _, _ = lstsq(X_w, y_w)\n        \n        if np.linalg.norm(beta_new - beta)  tol:\n            break\n        beta = beta_new\n        \n    return beta\n\ndef estimate_seasonal_amplitude(t, detrended_y, P):\n    \"\"\"Estimates seasonal amplitude from a detrended series.\"\"\"\n    S = np.stack([\n        np.cos(2 * np.pi * t / P),\n        np.sin(2 * np.pi * t / P)\n    ], axis=1)\n    \n    gamma, _, _, _ = lstsq(S, detrended_y)\n    amplitude = np.linalg.norm(gamma)\n    return amplitude\n\ndef solve():\n    \"\"\"\n    Main function to run the time series decomposition for all test cases.\n    \"\"\"\n    test_cases = [\n        # (N, P, beta0, beta1, A_true, sigma, f_out, M_out, delta)\n        (60, 12, 0.4, 0.0015, 0.15, 0.02, 0.10, 0.35, 0.05),\n        (60, 12, 0.5, 0.0000, 0.12, 0.01, 0.00, 0.00, 0.05),\n        (48, 12, 0.3, 0.0020, 0.08, 0.03, 0.25, 0.50, 0.03),\n        (36, 12, 0.6, -0.0010, 0.10, 0.015, 0.15, 0.40, 0.005),\n        (24, 12, 0.45, 0.0010, 0.00, 0.02, 0.20, 0.30, 0.05),\n    ]\n\n    results = []\n    # Use a fixed seed for reproducibility of random components.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    for params in test_cases:\n        N, P, beta0, beta1, A_true, sigma, f_out, M_out, delta = params\n        \n        # 1. Generate synthetic time series data\n        t, y = generate_time_series(N, P, beta0, beta1, A_true, sigma, f_out, M_out, rng)\n        \n        # 2. Estimate trend using OLS\n        beta_ols = estimate_trend_ols(t, y)\n        tau_ols = beta_ols[0] + beta_ols[1] * t\n        d_ols = y - tau_ols\n        A_ols = estimate_seasonal_amplitude(t, d_ols, P)\n\n        # 3. Estimate trend using Huber regression (IRLS)\n        beta_huber = estimate_trend_huber_irls(t, y, delta)\n        tau_huber = beta_huber[0] + beta_huber[1] * t\n        d_huber = y - tau_huber\n        A_huber = estimate_seasonal_amplitude(t, d_huber, P)\n\n        # 4. Calculate and store the difference\n        results.append(A_huber - A_ols)\n\n    # Print results in the specified format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Fitting a model to a time series is not the final step; validation is essential to ensure the model accurately represents the underlying processes. A well-specified model should capture the systematic patterns in the data, leaving behind only random, unstructured noise in the residuals. This practice focuses on a key diagnostic technique: examining the autocorrelation function (ACF) of the residuals to detect lingering periodic signals that your model may have missed. By deriving the theoretical shape of the residual ACF under model misspecification, you will learn to identify the tell-tale signature of an inadequately captured seasonal component .",
            "id": "3843881",
            "problem": "A monthly Normalized Difference Vegetation Index (NDVI) time series $\\{X_t\\}_{t=1}^{n}$ for a vegetated region is modeled additively as $X_t = T_t + S_t + \\varepsilon_t$, where $T_t$ is a smooth trend, $S_t$ is a seasonal component with period $s=12$, and $\\varepsilon_t$ is a zero-mean, independent and identically distributed Gaussian white noise with variance $\\sigma^{2}$. The seasonal component is generated by a harmonic expansion $S_t = \\sum_{k=1}^{K^{\\star}} A_k \\cos\\!\\left(2\\pi \\frac{k}{12} t + \\phi_k\\right)$, where $K^{\\star} \\in \\mathbb{N}$ is the true number of harmonics, $A_k>0$ are amplitudes, and $\\phi_k \\in \\mathbb{R}$ are phases. Consider a seasonal adjustment performed by harmonic regression that removes $K$ harmonics.\n\nYou are tasked with constructing residual diagnostics to assess the adequacy of the seasonal adjustment and deriving the expected patterns in the residual autocorrelation function (ACF) under model underfit versus overfit. Use only the following foundational bases: the definition of autocovariance and autocorrelation for a (weakly) stationary process, the orthogonality over integer numbers of periods of sinusoidal functions with distinct harmonic frequencies, and the properties of Gaussian white noise.\n\nWork under the large-sample regime where $n$ is large and the fitted seasonal coefficients for any correctly included harmonic converge to their true values so that correctly included harmonics are fully removed from the residuals in probability. Let the residuals after seasonal adjustment be $r_t = X_t - \\widehat{T}_t - \\widehat{S}_t$.\n\nTasks:\n1. From first principles (the definition of autocovariance and the properties stated above), derive an expression for the large-sample expected residual autocorrelation at lag $h$, denoted $\\rho(h)$, in the case where the seasonal model is underfit and omits exactly one true harmonic at frequency $f=\\frac{k}{12}$ with amplitude $A_k$ while correctly removing all other true harmonics and the trend. Your expression must depend on $A_k$, $\\sigma^{2}$, $h$, and $f$ only.\n2. Using your derived expression, specialize to the monthly case where the omitted harmonic is $k=2$ so that $f=\\frac{1}{6}$, with amplitude $A_2=0.15$, and the noise variance is $\\sigma^{2}=0.01$. Compute the expected residual autocorrelation at lag $h=6$.\n3. Briefly state, without computing any additional numeric quantity, what the large-sample expected residual ACF is at any lag if the seasonal model is overfit by including an extra harmonic whose true amplitude is zero (assume all true harmonics were correctly included and removed, and the noise is Gaussian white).\n\nExpress your final answer as the single numerical value of the expected residual autocorrelation at lag $h=6$ for the underfit case in Task $2$. Round your answer to four significant figures. Provide a dimensionless number (no units).",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in time series analysis, well-posed, objective, and contains sufficient information to derive a unique solution.\n\nThe problem asks for an analysis of the residual autocorrelation function (ACF) arising from seasonally adjusting a time series model under different model specification scenarios. The time series is given by the additive model $X_t = T_t + S_t + \\varepsilon_t$, where $T_t$ is the trend, $S_t$ is a seasonal component with period $s=12$, and $\\varepsilon_t$ is Gaussian white noise with mean $0$ and variance $\\sigma^2$. The seasonal component is a sum of $K^\\star$ harmonics: $S_t = \\sum_{k=1}^{K^{\\star}} A_k \\cos(2\\pi \\frac{k}{12} t + \\phi_k)$.\n\n### Task 1: Residual ACF under Model Underfit\n\nIn this case, the seasonal adjustment is underfit, meaning it fails to remove one of the true harmonic components. The problem specifies that the trend $T_t$ and all true harmonics are removed, except for one at frequency $f = \\frac{k}{12}$ with amplitude $A_k$. The residuals after this imperfect adjustment, $r_t = X_t - \\widehat{T}_t - \\widehat{S}_t$, are given by the large-sample limit where fitted components converge to their true values.\n$$r_t = (T_t + S_t + \\varepsilon_t) - T_t - \\left(S_t - A_k \\cos\\left(2\\pi f t + \\phi_k\\right)\\right)$$\n$$r_t = A_k \\cos\\left(2\\pi f t + \\phi_k\\right) + \\varepsilon_t$$\nLet's denote the unmodeled seasonal component as $S_{k,t} = A_k \\cos(2\\pi f t + \\phi_k)$. The residual series is $r_t = S_{k,t} + \\varepsilon_t$.\n\nTo find the autocorrelation function $\\rho(h)$, we first derive the autocovariance function $\\gamma(h)$. We treat the residual series $r_t$ as a realization of a weakly stationary process. This can be conceptualized by assuming the phase $\\phi_k$ is a random variable Uniform($[0, 2\\pi]$), which makes the process $S_{k,t}$ (and thus $r_t$) weakly stationary with a mean of zero.\nThe autocovariance at lag $h$ is defined as $\\gamma(h) = E[r_t r_{t+h}]$.\n$$\\gamma(h) = E[(S_{k,t} + \\varepsilon_t)(S_{k,t+h} + \\varepsilon_{t+h})]$$\nExpanding this expression gives:\n$$\\gamma(h) = E[S_{k,t}S_{k,t+h}] + E[S_{k,t}\\varepsilon_{t+h}] + E[\\varepsilon_t S_{k,t+h}] + E[\\varepsilon_t \\varepsilon_{t+h}]$$\nSince the noise process $\\varepsilon_t$ is independent of the seasonal component $S_{k,t}$, the cross-terms are zero: $E[S_{k,t}\\varepsilon_{t+h}] = E[S_{k,t}]E[\\varepsilon_{t+h}] = 0 \\cdot 0 = 0$.\nThe autocovariance separates into the sum of the autocovariances of the signal and noise:\n$$\\gamma(h) = \\gamma_S(h) + \\gamma_\\varepsilon(h)$$\nThe autocovariance of the white noise process $\\varepsilon_t$ is by definition:\n$$\\gamma_\\varepsilon(h) = E[\\varepsilon_t \\varepsilon_{t+h}] = \\begin{cases} \\sigma^2  \\text{if } h=0 \\\\ 0  \\text{if } h \\neq 0 \\end{cases} \\quad = \\sigma^2 \\delta_{h,0}$$\nwhere $\\delta_{h,0}$ is the Kronecker delta.\n\nThe autocovariance of the unmodeled harmonic $S_{k,t}$ is:\n$$\\gamma_S(h) = E[A_k \\cos(2\\pi f t + \\phi_k) \\cdot A_k \\cos(2\\pi f (t+h) + \\phi_k)]$$\nUsing the trigonometric identity $\\cos(A)\\cos(B) = \\frac{1}{2}[\\cos(A-B) + \\cos(A+B)]$:\n$$\\gamma_S(h) = A_k^2 E\\left[\\frac{1}{2}\\left(\\cos(-2\\pi f h) + \\cos(2\\pi f(2t+h) + 2\\phi_k)\\right)\\right]$$\n$$\\gamma_S(h) = \\frac{A_k^2}{2} \\left(\\cos(2\\pi f h) + E[\\cos(4\\pi f t + 2\\pi f h + 2\\phi_k)]\\right)$$\nThe expectation of the second cosine term, taken over the random phase $\\phi_k$, is zero. This result also emerges from a time-averaging perspective in the large-sample limit, as the average of a sinusoidal function over many periods approaches zero. Thus:\n$$\\gamma_S(h) = \\frac{A_k^2}{2} \\cos(2\\pi f h)$$\nThe total autocovariance of the residual series $r_t$ is:\n$$\\gamma_r(h) = \\frac{A_k^2}{2} \\cos(2\\pi f h) + \\sigma^2 \\delta_{h,0}$$\nThe variance of the series is the autocovariance at lag $h=0$:\n$$\\gamma_r(0) = \\frac{A_k^2}{2} \\cos(0) + \\sigma^2 = \\frac{A_k^2}{2} + \\sigma^2$$\nThe autocorrelation function is $\\rho(h) = \\frac{\\gamma_r(h)}{\\gamma_r(0)}$.\n$$\\rho(h) = \\frac{\\frac{A_k^2}{2} \\cos(2\\pi f h) + \\sigma^2 \\delta_{h,0}}{\\frac{A_k^2}{2} + \\sigma^2}$$\nFor any non-zero lag $h \\neq 0$, the Kronecker delta is zero, and the expression simplifies to:\n$$\\rho(h) = \\frac{\\frac{A_k^2}{2} \\cos(2\\pi f h)}{\\frac{A_k^2}{2} + \\sigma^2}, \\quad h \\neq 0$$\nThis is the required expression. It reveals that the residual ACF will itself be periodic, tracing a cosine wave at the frequency $f$ of the omitted harmonic, with its amplitude dampened by the noise variance.\n\n### Task 2: Numerical Calculation for a Specific Case\n\nWe are asked to compute the expected residual autocorrelation at lag $h=6$ for the specific case:\n- Omitted harmonic: $k=2$\n- Frequency: $f = \\frac{k}{12} = \\frac{2}{12} = \\frac{1}{6}$\n- Amplitude: $A_2 = 0.15$\n- Noise variance: $\\sigma^2 = 0.01$\n- Lag: $h=6$\n\nUsing the derived formula for $\\rho(h)$ with $h \\neq 0$:\n$$\\rho(6) = \\frac{\\frac{A_2^2}{2} \\cos(2\\pi f h)}{\\frac{A_2^2}{2} + \\sigma^2}$$\nSubstitute the given values:\n$$\\rho(6) = \\frac{\\frac{(0.15)^2}{2} \\cos\\left(2\\pi \\cdot \\frac{1}{6} \\cdot 6\\right)}{\\frac{(0.15)^2}{2} + 0.01}$$\nThe argument of the cosine function is $2\\pi \\cdot \\frac{1}{6} \\cdot 6 = 2\\pi$. Since $\\cos(2\\pi)=1$:\n$$\\rho(6) = \\frac{\\frac{0.0225}{2}}{\\frac{0.0225}{2} + 0.01} = \\frac{0.01125}{0.01125 + 0.01}$$\n$$\\rho(6) = \\frac{0.01125}{0.02125}$$\nThis fraction simplifies to $\\frac{1125}{2125} = \\frac{9 \\times 125}{17 \\times 125} = \\frac{9}{17}$.\nAs a decimal, this is:\n$$\\rho(6) = \\frac{9}{17} \\approx 0.52941176...$$\nRounding to four significant figures, we get $0.5294$. This high positive correlation is expected because the lag $h=6$ matches the period of the omitted harmonic ($1/f = 6$ months), so the residuals are strongly correlated with themselves after one full cycle of the unmodeled dynamic.\n\n### Task 3: Residual ACF under Model Overfit\n\nIn the case of overfitting, the model includes an extra harmonic term whose true amplitude is zero. All true harmonics are correctly included. In the large-sample regime, a consistent estimation method like harmonic regression (which is a form of ordinary least squares) will produce coefficient estimates that converge to their true values.\nThe true model is $X_t = T_t + S_t + \\varepsilon_t$. The fitted model includes all components of $T_t$ and $S_t$ plus an extra, non-existent harmonic. As $n \\to \\infty$, the estimated coefficient (amplitude) for this extra harmonic will converge to its true value of zero. Consequently, the fitted trend and seasonality, $\\widehat{T}_t + \\widehat{S}_t$, converge in probability to the true trend and seasonality, $T_t + S_t$.\nThe residuals are therefore:\n$$r_t = X_t - (\\widehat{T}_t + \\widehat{S}_t) \\xrightarrow{n \\to \\infty} (T_t + S_t + \\varepsilon_t) - (T_t + S_t) = \\varepsilon_t$$\nIn the large-sample limit, the residual series $r_t$ converges to the white noise process $\\varepsilon_t$. The autocorrelation function of a white noise process is, by definition, $1$ at lag $h=0$ and $0$ for all non-zero lags $h \\neq 0$.\nTherefore, the large-sample expected residual ACF in the overfit case is $\\rho(h) = 0$ for all $h \\neq 0$.",
            "answer": "$$\\boxed{0.5294}$$"
        }
    ]
}