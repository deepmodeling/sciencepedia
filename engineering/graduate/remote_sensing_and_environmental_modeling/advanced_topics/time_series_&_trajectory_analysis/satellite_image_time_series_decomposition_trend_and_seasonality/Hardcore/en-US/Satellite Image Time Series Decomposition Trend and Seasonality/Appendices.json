{
    "hands_on_practices": [
        {
            "introduction": "Understanding seasonality begins with its mathematical representation. This exercise grounds your knowledge in harmonic regression, a cornerstone for modeling periodic signals in time series data. By deriving the model from first principles and then calculating the amplitude and phase of seasonal cycles from given coefficients, you will gain a deeper appreciation for how we quantify the 'what' and 'when' of environmental phenomena .",
            "id": "3843828",
            "problem": "A single pixel time series of the Normalized Difference Vegetation Index (NDVI) derived from Sentinel-2 surface reflectance is observed at regular daily intervals over multiple years. Assume an additive decomposition of the form $Y_t = \\mu(t) + S(t) + \\varepsilon_t$, where $Y_t$ is the observed NDVI at day $t$, $\\mu(t)$ is a smooth trend, $S(t)$ is a seasonal component that is strictly periodic with fundamental period $P$, and $\\varepsilon_t$ is a zero-mean noise term with finite variance that is uncorrelated across time. The seasonal component $S(t)$ is assumed to be well-approximated by a finite set of harmonics of the fundamental frequency.\n\nStarting from the definitions of periodic functions and the orthogonality of trigonometric functions over integer multiples of the fundamental period, and using the least squares principle under the assumption of independent, identically distributed Gaussian errors, derive a harmonic regression representation for $S(t)$ based on a finite number $K$ of harmonics of the fundamental period. Then, show how the amplitudes and phases of each harmonic can be expressed in terms of the regression coefficients obtained by least squares.\n\nAfter providing the derivation, use the following values (which were obtained by first detrending $Y_t$ to estimate $S(t)$ via least squares over $N$ daily samples spanning an integer number of years with $P = 365$ days) for $K = 3$ harmonics:\n- For $k = 1$: $a_1 = 0.12$, $b_1 = -0.05$.\n- For $k = 2$: $a_2 = 0.08$, $b_2 = 0.02$.\n- For $k = 3$: $a_3 = 0.03$, $b_3 = -0.01$.\n\nCompute the amplitudes and phases for each harmonic in the order $k = 1, 2, 3$. Express amplitudes in reflectance (unitless) and phases in radians. Constrain phases to lie in the interval $[0, 2\\pi)$ by an appropriate mapping. Round your final numerical values to four significant figures.\n\nYour final answer must be a single row matrix containing the six values $(A_1, \\phi_1, A_2, \\phi_2, A_3, \\phi_3)$ in that order, where $A_k$ denotes amplitude and $\\phi_k$ denotes phase for harmonic $k$.",
            "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a complete solution. It describes a standard application of harmonic analysis to time series data in remote sensing, which is a well-established scientific method.\n\nThe problem asks for two main components: first, a derivation of the harmonic regression model for a periodic seasonal component from first principles, and second, a numerical calculation of amplitudes and phases from given regression coefficients.\n\n**Part 1: Derivation of Harmonic Regression and Amplitude-Phase Representation**\n\nWe begin with the additive decomposition model for a time series $Y_t$:\n$$ Y_t = \\mu(t) + S(t) + \\varepsilon_t $$\nwhere $t$ represents time, $\\mu(t)$ is the trend, $S(t)$ is the seasonal component, and $\\varepsilon_t$ is a random error term. The problem states that $S(t)$ is strictly periodic with a fundamental period $P$. Any such well-behaved periodic function can be represented by a Fourier series. The fundamental angular frequency is $\\omega = \\frac{2\\pi}{P}$. The seasonal component $S(t)$ is approximated by a finite sum of $K$ harmonics:\n$$ S(t) \\approx \\sum_{k=1}^{K} \\left( a_k \\cos(k \\omega t) + b_k \\sin(k \\omega t) \\right) $$\nThe problem states that the time series $Y_t$ has been detrended to yield estimates of the seasonal component. Let us denote these detrended observations as $s_t = Y_t - \\hat{\\mu}(t)$. We aim to find the coefficients $a_k$ and $b_k$ that best fit the model to these observations. Under the assumption of independent and identically distributed Gaussian errors, the principle of least squares is equivalent to maximum likelihood estimation. The sum of squared residuals ($SSR$) between the observations $s_t$ and the model is:\n$$ SSR(a_1, ..., a_K, b_1, ..., b_K) = \\sum_{t=1}^{N} \\left( s_t - \\sum_{k=1}^{K} \\left[ a_k \\cos\\left(\\frac{2\\pi k t}{P}\\right) + b_k \\sin\\left(\\frac{2\\pi k t}{P}\\right) \\right] \\right)^2 $$\nTo minimize $SSR$, we take the partial derivative with respect to each coefficient and set it to zero. For an arbitrary coefficient $a_j$ where $j \\in \\{1, ..., K\\}$:\n$$ \\frac{\\partial SSR}{\\partial a_j} = \\sum_{t=1}^{N} 2 \\left( s_t - \\sum_{k=1}^{K} [...] \\right) \\left( - \\cos\\left(\\frac{2\\pi j t}{P}\\right) \\right) = 0 $$\n$$ \\sum_{t=1}^{N} s_t \\cos\\left(\\frac{2\\pi j t}{P}\\right) = \\sum_{t=1}^{N} \\cos\\left(\\frac{2\\pi j t}{P}\\right) \\sum_{k=1}^{K} \\left[ a_k \\cos\\left(\\frac{2\\pi k t}{P}\\right) + b_k \\sin\\left(\\frac{2\\pi k t}{P}\\right) \\right] $$\nThe problem specifies that the data span an integer number of years, $N = mP$ for some integer $m$. For such a case, the discrete trigonometric functions exhibit orthogonality over the interval $t \\in \\{1, ..., N\\}$. The relevant identities are:\n$$ \\sum_{t=1}^{N} \\cos\\left(\\frac{2\\pi k t}{P}\\right) \\sin\\left(\\frac{2\\pi j t}{P}\\right) = 0 \\quad \\text{for all } k, j $$\n$$ \\sum_{t=1}^{N} \\cos\\left(\\frac{2\\pi k t}{P}\\right) \\cos\\left(\\frac{2\\pi j t}{P}\\right) = \\begin{cases} \\frac{N}{2}  \\text{if } k=j \\neq 0, P/2 \\\\ 0  \\text{if } k \\neq j \\end{cases} $$\n$$ \\sum_{t=1}^{N} \\sin\\left(\\frac{2\\pi k t}{P}\\right) \\sin\\left(\\frac{2\\pi j t}{P}\\right) = \\begin{cases} \\frac{N}{2}  \\text{if } k=j \\neq 0, P/2 \\\\ 0  \\text{if } k \\neq j \\end{cases} $$\nApplying these orthogonality relations to the expanded sum for $\\frac{\\partial SSR}{\\partial a_j}$, all terms where $k \\neq j$ become zero. We are left with:\n$$ \\sum_{t=1}^{N} s_t \\cos\\left(\\frac{2\\pi j t}{P}\\right) = \\sum_{t=1}^{N} a_j \\cos^2\\left(\\frac{2\\pi j t}{P}\\right) = a_j \\frac{N}{2} $$\nSolving for $a_j$ gives the least squares estimate:\n$$ a_j = \\frac{2}{N} \\sum_{t=1}^{N} s_t \\cos\\left(\\frac{2\\pi j t}{P}\\right) $$\nSimilarly, differentiating with respect to $b_j$:\n$$ \\frac{\\partial SSR}{\\partial b_j} = \\sum_{t=1}^{N} 2 \\left( s_t - \\sum_{k=1}^{K} [...] \\right) \\left( - \\sin\\left(\\frac{2\\pi j t}{P}\\right) \\right) = 0 $$\nApplying orthogonality yields:\n$$ \\sum_{t=1}^{N} s_t \\sin\\left(\\frac{2\\pi j t}{P}\\right) = \\sum_{t=1}^{N} b_j \\sin^2\\left(\\frac{2\\pi j t}{P}\\right) = b_j \\frac{N}{2} $$\n$$ b_j = \\frac{2}{N} \\sum_{t=1}^{N} s_t \\sin\\left(\\frac{2\\pi j t}{P}\\right) $$\nThis completes the derivation of the harmonic regression coefficients.\n\nNext, we convert the $k$-th harmonic, $S_k(t) = a_k \\cos(k \\omega t) + b_k \\sin(k \\omega t)$, into the amplitude-phase form, $S_k(t) = A_k \\cos(k \\omega t - \\phi_k)$.\nUsing the trigonometric identity $\\cos(X - Y) = \\cos(X)\\cos(Y) + \\sin(X)\\sin(Y)$, we expand the amplitude-phase form:\n$$ A_k \\cos(k \\omega t - \\phi_k) = A_k (\\cos(k \\omega t)\\cos(\\phi_k) + \\sin(k \\omega t)\\sin(\\phi_k)) $$\n$$ S_k(t) = (A_k \\cos(\\phi_k)) \\cos(k \\omega t) + (A_k \\sin(\\phi_k)) \\sin(k \\omega t) $$\nBy comparing the coefficients of $\\cos(k \\omega t)$ and $\\sin(k \\omega t)$ with the original form, we establish the relationships:\n$$ a_k = A_k \\cos(\\phi_k) $$\n$$ b_k = A_k \\sin(\\phi_k) $$\nTo find the amplitude $A_k$, we square and sum these two equations:\n$$ a_k^2 + b_k^2 = A_k^2 \\cos^2(\\phi_k) + A_k^2 \\sin^2(\\phi_k) = A_k^2 (\\cos^2(\\phi_k) + \\sin^2(\\phi_k)) = A_k^2 $$\nAs amplitude is a non-negative quantity, we have:\n$$ A_k = \\sqrt{a_k^2 + b_k^2} $$\nTo find the phase $\\phi_k$, we divide the second equation by the first:\n$$ \\frac{b_k}{a_k} = \\frac{A_k \\sin(\\phi_k)}{A_k \\cos(\\phi_k)} = \\tan(\\phi_k) $$\nThe phase angle $\\phi_k$ must be determined by considering the signs of both $a_k$ (the cosine coefficient, analogous to an x-coordinate) and $b_k$ (the sine coefficient, analogous to a y-coordinate) to place the angle in the correct quadrant. This is achieved using the two-argument arctangent function, often denoted $\\text{atan2}(y, x)$. So, $\\phi_k' = \\text{atan2}(b_k, a_k)$, which typically returns a value in $(-\\pi, \\pi]$.\nThe problem requires the phase to be in the interval $[0, 2\\pi)$. We achieve this with the following mapping:\n$$ \\phi_k = \\begin{cases} \\phi_k'  \\text{if } \\phi_k' \\ge 0 \\\\ \\phi_k' + 2\\pi  \\text{if } \\phi_k'  0 \\end{cases} $$\n\n**Part 2: Numerical Calculation**\n\nWe are given coefficients for $K=3$ harmonics and $P=365$. We compute the amplitude $A_k$ and phase $\\phi_k$ for each harmonic, rounding to four significant figures.\n\nFor $k=1$: $a_1 = 0.12$, $b_1 = -0.05$.\n$$ A_1 = \\sqrt{(0.12)^2 + (-0.05)^2} = \\sqrt{0.0144 + 0.0025} = \\sqrt{0.0169} = 0.13 $$\nTo four significant figures, $A_1 = 0.1300$.\nThe phase is given by $\\phi_1' = \\arctan(\\frac{-0.05}{0.12})$. Since $a_1  0$ and $b_1  0$, the angle is in the fourth quadrant.\n$$ \\phi_1' \\approx -0.39479 \\text{ radians} $$\nMapping to $[0, 2\\pi)$:\n$$ \\phi_1 = -0.39479 + 2\\pi \\approx 5.88839... $$\nTo four significant figures, $\\phi_1 = 5.888$.\n\nFor $k=2$: $a_2 = 0.08$, $b_2 = 0.02$.\n$$ A_2 = \\sqrt{(0.08)^2 + (0.02)^2} = \\sqrt{0.0064 + 0.0004} = \\sqrt{0.0068} \\approx 0.082462... $$\nTo four significant figures, $A_2 = 0.08246$.\nThe phase is given by $\\phi_2' = \\arctan(\\frac{0.02}{0.08})$. Since $a_2  0$ and $b_2  0$, the angle is in the first quadrant.\n$$ \\phi_2' = \\arctan(0.25) \\approx 0.244978... \\text{ radians} $$\nThis value is already in the range $[0, 2\\pi)$, so no mapping is needed.\nTo four significant figures, $\\phi_2 = 0.2450$.\n\nFor $k=3$: $a_3 = 0.03$, $b_3 = -0.01$.\n$$ A_3 = \\sqrt{(0.03)^2 + (-0.01)^2} = \\sqrt{0.0009 + 0.0001} = \\sqrt{0.0010} \\approx 0.031622... $$\nTo four significant figures, $A_3 = 0.03162$.\nThe phase is given by $\\phi_3' = \\arctan(\\frac{-0.01}{0.03})$. Since $a_3  0$ and $b_3  0$, the angle is in the fourth quadrant.\n$$ \\phi_3' \\approx -0.32175... \\text{ radians} $$\nMapping to $[0, 2\\pi)$:\n$$ \\phi_3 = -0.32175 + 2\\pi \\approx 5.96143... $$\nTo four significant figures, $\\phi_3 = 5.961$.\n\nThe final results for $(A_1, \\phi_1, A_2, \\phi_2, A_3, \\phi_3)$ are $(0.1300, 5.888, 0.08246, 0.2450, 0.03162, 5.961)$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0.1300  5.888  0.08246  0.2450  0.03162  5.961 \\end{pmatrix} } $$"
        },
        {
            "introduction": "Theoretical models provide a clean framework, but real-world satellite data is often contaminated by outliers from sources like undetected clouds or sensor noise. This hands-on coding practice  challenges you to implement a robust trend estimator using the Huber loss function, which intelligently down-weights the influence of these outliers. By comparing your robust results with a standard least squares fit, you will directly observe how crucial robust methods are for accurately separating trend from seasonality.",
            "id": "3843866",
            "problem": "A satellite image time series of surface reflectance (unitless decimal fraction) is modeled as an additive decomposition into a slowly varying trend and a periodic seasonal component. Let the observation at time index $t \\in \\{0,1,\\dots,N-1\\}$ be denoted by $y_t$ and suppose\n$$\ny_t = \\tau(t) + s(t) + \\varepsilon_t + o_t,\n$$\nwhere $\\tau(t)$ is the trend, $s(t)$ is the seasonal component, $\\varepsilon_t$ is zero-mean noise, and $o_t$ represents occasional outliers due to, for example, residual clouds or bidirectional reflectance effects not fully corrected. The seasonal component is assumed to have known period $P$ (in samples) and to be representable by a single harmonic,\n$$\ns(t) = a \\cos\\left(\\frac{2\\pi t}{P}\\right) + b \\sin\\left(\\frac{2\\pi t}{P}\\right),\n$$\nwith amplitude $A = \\sqrt{a^2 + b^2}$, and angles are in radians.\n\nTo reduce the influence of outliers on the trend estimation, you are to construct a robust estimator for $\\tau(t)$ using the Huber loss. Specifically, represent the trend by a first-order polynomial $\\tau(t) = \\beta_0 + \\beta_1 t$, and estimate the parameters $\\beta_0, \\beta_1$ by minimizing the Huber objective\n$$\nJ(\\beta_0,\\beta_1) = \\sum_{t=0}^{N-1} \\rho_\\delta\\!\\left(y_t - \\beta_0 - \\beta_1 t\\right),\n$$\nwhere $\\rho_\\delta(r)$ is the Huber loss with threshold $\\delta  0$,\n$$\n\\rho_\\delta(r) = \n\\begin{cases}\n\\frac{1}{2} r^2,  \\text{if } |r| \\le \\delta, \\\\\n\\delta \\left(|r| - \\frac{1}{2}\\delta\\right),  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\nAfter estimating the trend, obtain the seasonal amplitude $A$ by detrending the series and performing least squares harmonic regression onto $\\cos\\left(\\frac{2\\pi t}{P}\\right)$ and $\\sin\\left(\\frac{2\\pi t}{P}\\right)$. Repeat the amplitude estimation using a non-robust ordinary least squares trend estimate for comparison. The task is to implement Iteratively Reweighted Least Squares (IRLS) to solve the robust Huber trend estimation problem, then quantify the effect of the robust trend on the estimated seasonal amplitude compared to the least squares trend.\n\nYour program must:\n- Generate synthetic time series $y_t$ according to the specified additive model for each test case, with the following components:\n  - Trend $\\tau(t) = \\beta_0 + \\beta_1 t$.\n  - Seasonal component $s(t) = A_{\\text{true}} \\cos\\left(\\frac{2\\pi t}{P}\\right)$ (use zero phase for data generation).\n  - Gaussian noise $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ with standard deviation $\\sigma$.\n  - Outliers $o_t$ injected uniformly at random at a specified fraction of indices, with magnitudes drawn as independent signs multiplied by a specified outlier magnitude.\n- Estimate the trend coefficients $(\\beta_0,\\beta_1)$ using:\n  - Ordinary least squares (non-robust) on $\\{(t,y_t)\\}$.\n  - Robust Huber regression via Iteratively Reweighted Least Squares on $\\{(t,y_t)\\}$ with threshold $\\delta$.\n- Compute the seasonal amplitude $A$ by least squares harmonic regression on the detrended series for both trend estimates, using the known period $P$.\n- For each test case, return a single float equal to $A_{\\text{Huber}} - A_{\\text{OLS}}$, in reflectance units, expressed as a decimal number (no percentage).\n\nUse the following test suite of parameter values, chosen to cover typical and edge-case conditions in satellite image time series decomposition. For each tuple, the parameters are $(N, P, \\beta_0, \\beta_1, A_{\\text{true}}, \\sigma, f_{\\text{out}}, M_{\\text{out}}, \\delta)$:\n1. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (60, 12, 0.4, 0.0015, 0.15, 0.02, 0.10, 0.35, 0.05)$.\n2. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (60, 12, 0.5, 0.0000, 0.12, 0.01, 0.00, 0.00, 0.05)$.\n3. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (48, 12, 0.3, 0.0020, 0.08, 0.03, 0.25, 0.50, 0.03)$.\n4. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (36, 12, 0.6, -0.0010, 0.10, 0.015, 0.15, 0.40, 0.005)$.\n5. $(N,P,\\beta_0,\\beta_1,A_{\\text{true}},\\sigma,f_{\\text{out}},M_{\\text{out}},\\delta) = (24, 12, 0.45, 0.0010, 0.00, 0.02, 0.20, 0.30, 0.05)$.\n\nAll amplitudes and differences must be in reflectance units (unitless decimal fractions). Angles are in radians. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4,r_5]$, where $r_i$ is the amplitude difference $A_{\\text{Huber}} - A_{\\text{OLS}}$ for test case $i$.",
            "solution": "The posed problem is valid as it is scientifically grounded in statistical signal processing, well-posed with a clear objective, and provides all necessary information to construct a unique, verifiable solution. The underlying methods—additive time series decomposition, Ordinary Least Squares (OLS), Huber regression, and Iteratively Reweighted Least Squares (IRLS)—are standard and mathematically sound. We proceed with a detailed solution.\n\nThe core of the problem is to compare the impact of a non-robust versus a robust trend estimation on the subsequent estimation of a seasonal signal's amplitude. The time series is modeled as:\n$$\ny_t = \\tau(t) + s(t) + \\varepsilon_t + o_t\n$$\nwhere $y_t$ is the observation at time $t$, $\\tau(t)$ is a linear trend, $s(t)$ is a sinusoidal seasonal component, $\\varepsilon_t$ is Gaussian noise, and $o_t$ represents sporadic outliers.\n\n**Step 1: Synthetic Data Generation**\nFor each test case, we synthesize a time series of length $N$. The time vector is $\\mathbf{t} = [0, 1, \\dots, N-1]^T$.\n1.  **Trend Component**: The linear trend is given by $\\tau(t) = \\beta_0 + \\beta_1 t$.\n2.  **Seasonal Component**: The problem specifies a seasonal signal with a known period $P$ and zero phase for data generation, $s(t) = A_{\\text{true}} \\cos(2\\pi t/P)$.\n3.  **Noise Component**: I.i.d. Gaussian noise samples $\\varepsilon_t$ are drawn from $\\mathcal{N}(0, \\sigma^2)$.\n4.  **Outlier Component**: Outliers are added to a fraction $f_{\\text{out}}$ of the data points. We calculate the number of outliers $N_{\\text{out}} = \\text{round}(N \\cdot f_{\\text{out}})$, randomly select $N_{\\text{out}}$ unique time indices, and at these indices, set $o_t = \\pm M_{\\text{out}}$, with the sign chosen randomly. For all other indices, $o_t = 0$.\n\nThe final observed time series is the sum of these four components: $\\mathbf{y} = \\boldsymbol{\\tau} + \\mathbf{s} + \\boldsymbol{\\varepsilon} + \\mathbf{o}$. All random number generation is performed using a fixed seed to ensure the solution is reproducible.\n\n**Step 2: Trend Estimation**\nThe trend is modeled as a linear function $\\tau(t) = \\beta_0 + \\beta_1 t$. In matrix form, we seek to find the parameter vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ in the model $\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\beta}$, where $\\mathbf{X}$ is the $N \\times 2$ design matrix with columns for the intercept and the linear term:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ \\vdots  \\vdots \\\\ 1  N-1 \\end{pmatrix}\n$$\n\n**A. Ordinary Least Squares (OLS) Trend Estimation**\nThe OLS method finds the parameters $\\boldsymbol{\\beta}$ that minimize the sum of squared residuals, $||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2$. This is a standard linear regression problem whose solution is given by the normal equations:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nThis solution is sensitive to outliers because large residuals are squared, giving them a disproportionately high influence on the fit. The estimated trend is $\\hat{\\boldsymbol{\\tau}}_{\\text{OLS}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$.\n\n**B. Robust Huber Trend Estimation via Iteratively Reweighted Least Squares (IRLS)**\nThe Huber M-estimator minimizes a different objective function, $J(\\boldsymbol{\\beta}) = \\sum_{t=0}^{N-1} \\rho_\\delta(y_t - (\\mathbf{X}\\boldsymbol{\\beta})_t)$, using the Huber loss $\\rho_\\delta(r)$. This loss function behaves quadratically for small residuals ($|r| \\le \\delta$) like OLS, but linearly for large residuals ($|r|  \\delta$), which reduces the influence of outliers.\n\nThis optimization problem is solved using IRLS. The algorithm iteratively solves a weighted least squares problem where the weights adapt to the residuals from the previous iteration. The update step for the parameters is:\n$$\n\\hat{\\boldsymbol{\\beta}}^{(k+1)} = (\\mathbf{X}^T\\mathbf{W}^{(k)}\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{W}^{(k)}\\mathbf{y}\n$$\nThe matrix $\\mathbf{W}^{(k)}$ is a diagonal matrix of weights $w_t^{(k)}$ at iteration $k$. Each weight is a function of the corresponding residual $r_t^{(k)} = y_t - (\\mathbf{X}\\hat{\\boldsymbol{\\beta}}^{(k)})_t$:\n$$\nw_t^{(k)} = \\begin{cases} 1,  \\text{if } |r_t^{(k)}| \\le \\delta \\\\ \\delta/|r_t^{(k)}|,  \\text{if } |r_t^{(k)}|  \\delta \\end{cases}\n$$\nThe algorithm proceeds as follows:\n1.  Initialize parameters $\\hat{\\boldsymbol{\\beta}}^{(0)}$, typically with the OLS solution $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$.\n2.  Iteratively compute residuals, update weights, and solve the weighted least squares problem. For numerical stability, the update is not performed by matrix inversion but by solving an equivalent standard least squares problem.\n3.  The iterations continue until the change in the parameter vector $\\hat{\\boldsymbol{\\beta}}$ between successive iterations falls below a specified tolerance, or a maximum number of iterations is reached.\nThe converged solution $\\hat{\\boldsymbol{\\beta}}_{\\text{Huber}}$ gives the robustly estimated trend $\\hat{\\boldsymbol{\\tau}}_{\\text{Huber}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{Huber}}$.\n\n**Step 3: Seasonal Amplitude Estimation**\nThe estimated trend is subtracted from the original data to obtain a detrended series. This is done for both trend estimates:\n$$\n\\mathbf{d}_{\\text{OLS}} = \\mathbf{y} - \\hat{\\boldsymbol{\\tau}}_{\\text{OLS}} \\quad \\text{and} \\quad \\mathbf{d}_{\\text{Huber}} = \\mathbf{y} - \\hat{\\boldsymbol{\\tau}}_{\\text{Huber}}\n$$\nWe then fit a harmonic model to each detrended series. The model is $d(t) \\approx a \\cos(2\\pi t/P) + b \\sin(2\\pi t/P)$. The coefficients $\\boldsymbol{\\gamma} = [a, b]^T$ are found via least squares. The design matrix for this regression is:\n$$\n\\mathbf{S} = \\begin{pmatrix}\n\\cos(\\frac{2\\pi \\cdot 0}{P})  \\sin(\\frac{2\\pi \\cdot 0}{P}) \\\\\n\\vdots  \\vdots \\\\\n\\cos(\\frac{2\\pi \\cdot (N-1)}{P})  \\sin(\\frac{2\\pi \\cdot (N-1)}{P})\n\\end{pmatrix}\n$$\nThe solution for the coefficients is $\\hat{\\boldsymbol{\\gamma}} = (\\mathbf{S}^T\\mathbf{S})^{-1}\\mathbf{S}^T\\mathbf{d}$. This yields coefficient vectors $\\hat{\\boldsymbol{\\gamma}}_{\\text{OLS}}$ and $\\hat{\\boldsymbol{\\gamma}}_{\\text{Huber}}$. The corresponding amplitudes are the Euclidean norms of these vectors:\n$$\nA_{\\text{OLS}} = ||\\hat{\\boldsymbol{\\gamma}}_{\\text{OLS}}||_2 \\quad \\text{and} \\quad A_{\\text{Huber}} = ||\\hat{\\boldsymbol{\\gamma}}_{\\text{Huber}}||_2\n$$\n\n**Step 4: Final Comparison**\nThe final metric for each test case is the difference between the two estimated amplitudes, which quantifies the effect of the robust trend estimation:\n$$\n\\Delta A = A_{\\text{Huber}} - A_{\\text{OLS}}\n$$\nA non-zero value indicates that the choice of detrending method influences the seasonal analysis. In the presence of outliers, the OLS trend can be significantly biased, leading to a polluted detrended series and an inaccurate amplitude estimate. The Huber regression, being robust, should yield a more accurate trend, and consequently, a more reliable seasonal amplitude.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lstsq\n\ndef generate_time_series(N, P, beta0, beta1, A_true, sigma, f_out, M_out, rng):\n    \"\"\"Generates a synthetic time series based on the additive model.\"\"\"\n    t = np.arange(N)\n    \n    # 1. Trend component\n    trend = beta0 + beta1 * t\n    \n    # 2. Seasonal component\n    seasonal = A_true * np.cos(2 * np.pi * t / P)\n    \n    # 3. Noise component\n    noise = rng.normal(0, sigma, N)\n    \n    # 4. Outlier component\n    outliers = np.zeros(N)\n    n_outliers = int(round(f_out * N))\n    if n_outliers  0:\n        outlier_indices = rng.choice(N, size=n_outliers, replace=False)\n        outlier_signs = rng.choice([-1, 1], size=n_outliers)\n        outliers[outlier_indices] = outlier_signs * M_out\n        \n    y = trend + seasonal + noise + outliers\n    return t, y\n\ndef estimate_trend_ols(t, y):\n    \"\"\"Estimates trend coefficients using Ordinary Least Squares.\"\"\"\n    X = np.stack([np.ones_like(t), t], axis=1)\n    beta_ols, _, _, _ = lstsq(X, y)\n    return beta_ols\n\ndef estimate_trend_huber_irls(t, y, delta, max_iter=100, tol=1e-6):\n    \"\"\"Estimates trend coefficients using Huber regression via IRLS.\"\"\"\n    X = np.stack([np.ones_like(t), t], axis=1)\n    \n    # Initialize with OLS solution\n    beta = estimate_trend_ols(t, y)\n    \n    for _ in range(max_iter):\n        residuals = y - X @ beta\n        abs_residuals = np.abs(residuals)\n        \n        # Calculate weights based on Huber's psi-function/residual\n        # w(r) = min(1, delta/|r|)\n        # Add a small epsilon to avoid division by zero, although not strictly necessary\n        # as |r|  delta for the second case.\n        weights = np.minimum(1.0, delta / (abs_residuals + 1e-8))\n        \n        # Perform weighted least squares\n        # This is equivalent to solving lstsq for L*y and L*X where L = sqrt(W)\n        L = np.sqrt(weights)\n        X_w = X * L[:, np.newaxis]\n        y_w = y * L\n        \n        beta_new, _, _, _ = lstsq(X_w, y_w)\n        \n        if np.linalg.norm(beta_new - beta)  tol:\n            break\n        beta = beta_new\n        \n    return beta\n\ndef estimate_seasonal_amplitude(t, detrended_y, P):\n    \"\"\"Estimates seasonal amplitude from a detrended series.\"\"\"\n    S = np.stack([\n        np.cos(2 * np.pi * t / P),\n        np.sin(2 * np.pi * t / P)\n    ], axis=1)\n    \n    gamma, _, _, _ = lstsq(S, detrended_y)\n    amplitude = np.linalg.norm(gamma)\n    return amplitude\n\ndef solve():\n    \"\"\"\n    Main function to run the time series decomposition for all test cases.\n    \"\"\"\n    test_cases = [\n        # (N, P, beta0, beta1, A_true, sigma, f_out, M_out, delta)\n        (60, 12, 0.4, 0.0015, 0.15, 0.02, 0.10, 0.35, 0.05),\n        (60, 12, 0.5, 0.0000, 0.12, 0.01, 0.00, 0.00, 0.05),\n        (48, 12, 0.3, 0.0020, 0.08, 0.03, 0.25, 0.50, 0.03),\n        (36, 12, 0.6, -0.0010, 0.10, 0.015, 0.15, 0.40, 0.005),\n        (24, 12, 0.45, 0.0010, 0.00, 0.02, 0.20, 0.30, 0.05),\n    ]\n\n    results = []\n    # Use a fixed seed for reproducibility of random components.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    for params in test_cases:\n        N, P, beta0, beta1, A_true, sigma, f_out, M_out, delta = params\n        \n        # 1. Generate synthetic time series data\n        t, y = generate_time_series(N, P, beta0, beta1, A_true, sigma, f_out, M_out, rng)\n        \n        # 2. Estimate trend using OLS\n        beta_ols = estimate_trend_ols(t, y)\n        tau_ols = beta_ols[0] + beta_ols[1] * t\n        d_ols = y - tau_ols\n        A_ols = estimate_seasonal_amplitude(t, d_ols, P)\n\n        # 3. Estimate trend using Huber regression (IRLS)\n        beta_huber = estimate_trend_huber_irls(t, y, delta)\n        tau_huber = beta_huber[0] + beta_huber[1] * t\n        d_huber = y - tau_huber\n        A_huber = estimate_seasonal_amplitude(t, d_huber, P)\n\n        # 4. Calculate and store the difference\n        results.append(A_huber - A_ols)\n\n    # Print results in the specified format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Fitting a model is only half the battle; the other half is verifying its adequacy. This exercise delves into the critical process of model diagnostics by examining the time series of residuals—the data left over after subtracting your estimated trend and seasonality. By deriving the expected shape of the residual autocorrelation function (ACF) when a seasonal component is missed , you will learn to spot the statistical footprints of an underfit model, a fundamental skill for any time series analyst.",
            "id": "3843881",
            "problem": "A monthly Normalized Difference Vegetation Index (NDVI) time series $\\{X_t\\}_{t=1}^{n}$ for a vegetated region is modeled additively as $X_t = T_t + S_t + \\varepsilon_t$, where $T_t$ is a smooth trend, $S_t$ is a seasonal component with period $s=12$, and $\\varepsilon_t$ is a zero-mean, independent and identically distributed Gaussian white noise with variance $\\sigma^{2}$. The seasonal component is generated by a harmonic expansion $S_t = \\sum_{k=1}^{K^{\\star}} A_k \\cos\\!\\left(2\\pi \\frac{k}{12} t + \\phi_k\\right)$, where $K^{\\star} \\in \\mathbb{N}$ is the true number of harmonics, $A_k0$ are amplitudes, and $\\phi_k \\in \\mathbb{R}$ are phases. Consider a seasonal adjustment performed by harmonic regression that removes $K$ harmonics.\n\nYou are tasked with constructing residual diagnostics to assess the adequacy of the seasonal adjustment and deriving the expected patterns in the residual autocorrelation function (ACF) under model underfit versus overfit. Use only the following foundational bases: the definition of autocovariance and autocorrelation for a (weakly) stationary process, the orthogonality over integer numbers of periods of sinusoidal functions with distinct harmonic frequencies, and the properties of Gaussian white noise.\n\nWork under the large-sample regime where $n$ is large and the fitted seasonal coefficients for any correctly included harmonic converge to their true values so that correctly included harmonics are fully removed from the residuals in probability. Let the residuals after seasonal adjustment be $r_t = X_t - \\widehat{T}_t - \\widehat{S}_t$.\n\nTasks:\n1. From first principles (the definition of autocovariance and the properties stated above), derive an expression for the large-sample expected residual autocorrelation at lag $h$, denoted $\\rho(h)$, in the case where the seasonal model is underfit and omits exactly one true harmonic at frequency $f=\\frac{k}{12}$ with amplitude $A_k$ while correctly removing all other true harmonics and the trend. Your expression must depend on $A_k$, $\\sigma^{2}$, $h$, and $f$ only.\n2. Using your derived expression, specialize to the monthly case where the omitted harmonic is $k=2$ so that $f=\\frac{1}{6}$, with amplitude $A_2=0.15$, and the noise variance is $\\sigma^{2}=0.01$. Compute the expected residual autocorrelation at lag $h=6$.\n3. Briefly state, without computing any additional numeric quantity, what the large-sample expected residual ACF is at any lag if the seasonal model is overfit by including an extra harmonic whose true amplitude is zero (assume all true harmonics were correctly included and removed, and the noise is Gaussian white).\n\nExpress your final answer as the single numerical value of the expected residual autocorrelation at lag $h=6$ for the underfit case in Task $2$. Round your answer to four significant figures. Provide a dimensionless number (no units).",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in time series analysis, well-posed, objective, and contains sufficient information to derive a unique solution.\n\nThe problem asks for an analysis of the residual autocorrelation function (ACF) arising from seasonally adjusting a time series model under different model specification scenarios. The time series is given by the additive model $X_t = T_t + S_t + \\varepsilon_t$, where $T_t$ is the trend, $S_t$ is a seasonal component with period $s=12$, and $\\varepsilon_t$ is Gaussian white noise with mean $0$ and variance $\\sigma^2$. The seasonal component is a sum of $K^\\star$ harmonics: $S_t = \\sum_{k=1}^{K^{\\star}} A_k \\cos(2\\pi \\frac{k}{12} t + \\phi_k)$.\n\n### Task 1: Residual ACF under Model Underfit\n\nIn this case, the seasonal adjustment is underfit, meaning it fails to remove one of the true harmonic components. The problem specifies that the trend $T_t$ and all true harmonics are removed, except for one at frequency $f = \\frac{k}{12}$ with amplitude $A_k$. The residuals after this imperfect adjustment, $r_t = X_t - \\widehat{T}_t - \\widehat{S}_t$, are given by the large-sample limit where fitted components converge to their true values.\n$$r_t = (T_t + S_t + \\varepsilon_t) - T_t - \\left(S_t - A_k \\cos\\left(2\\pi f t + \\phi_k\\right)\\right)$$\n$$r_t = A_k \\cos\\left(2\\pi f t + \\phi_k\\right) + \\varepsilon_t$$\nLet's denote the unmodeled seasonal component as $S_{k,t} = A_k \\cos(2\\pi f t + \\phi_k)$. The residual series is $r_t = S_{k,t} + \\varepsilon_t$.\n\nTo find the autocorrelation function $\\rho(h)$, we first derive the autocovariance function $\\gamma(h)$. We treat the residual series $r_t$ as a realization of a weakly stationary process. This can be conceptualized by assuming the phase $\\phi_k$ is a random variable Uniform($[0, 2\\pi]$), which makes the process $S_{k,t}$ (and thus $r_t$) weakly stationary with a mean of zero.\nThe autocovariance at lag $h$ is defined as $\\gamma(h) = E[r_t r_{t+h}]$.\n$$\\gamma(h) = E[(S_{k,t} + \\varepsilon_t)(S_{k,t+h} + \\varepsilon_{t+h})]$$\nExpanding this expression gives:\n$$\\gamma(h) = E[S_{k,t}S_{k,t+h}] + E[S_{k,t}\\varepsilon_{t+h}] + E[\\varepsilon_t S_{k,t+h}] + E[\\varepsilon_t \\varepsilon_{t+h}]$$\nSince the noise process $\\varepsilon_t$ is independent of the seasonal component $S_{k,t}$, the cross-terms are zero: $E[S_{k,t}\\varepsilon_{t+h}] = E[S_{k,t}]E[\\varepsilon_{t+h}] = 0 \\cdot 0 = 0$.\nThe autocovariance separates into the sum of the autocovariances of the signal and noise:\n$$\\gamma(h) = \\gamma_S(h) + \\gamma_\\varepsilon(h)$$\nThe autocovariance of the white noise process $\\varepsilon_t$ is by definition:\n$$\\gamma_\\varepsilon(h) = E[\\varepsilon_t \\varepsilon_{t+h}] = \\begin{cases} \\sigma^2  \\text{if } h=0 \\\\ 0  \\text{if } h \\neq 0 \\end{cases} \\quad = \\sigma^2 \\delta_{h,0}$$\nwhere $\\delta_{h,0}$ is the Kronecker delta.\n\nThe autocovariance of the unmodeled harmonic $S_{k,t}$ is:\n$$\\gamma_S(h) = E[A_k \\cos(2\\pi f t + \\phi_k) \\cdot A_k \\cos(2\\pi f (t+h) + \\phi_k)]$$\nUsing the trigonometric identity $\\cos(A)\\cos(B) = \\frac{1}{2}[\\cos(A-B) + \\cos(A+B)]$:\n$$\\gamma_S(h) = A_k^2 E\\left[\\frac{1}{2}\\left(\\cos(-2\\pi f h) + \\cos(2\\pi f(2t+h) + 2\\phi_k)\\right)\\right]$$\n$$\\gamma_S(h) = \\frac{A_k^2}{2} \\left(\\cos(2\\pi f h) + E[\\cos(4\\pi f t + 2\\pi f h + 2\\phi_k)]\\right)$$\nThe expectation of the second cosine term, taken over the random phase $\\phi_k$, is zero. This result also emerges from a time-averaging perspective in the large-sample limit, as the average of a sinusoidal function over many periods approaches zero. Thus:\n$$\\gamma_S(h) = \\frac{A_k^2}{2} \\cos(2\\pi f h)$$\nThe total autocovariance of the residual series $r_t$ is:\n$$\\gamma_r(h) = \\frac{A_k^2}{2} \\cos(2\\pi f h) + \\sigma^2 \\delta_{h,0}$$\nThe variance of the series is the autocovariance at lag $h=0$:\n$$\\gamma_r(0) = \\frac{A_k^2}{2} \\cos(0) + \\sigma^2 = \\frac{A_k^2}{2} + \\sigma^2$$\nThe autocorrelation function is $\\rho(h) = \\frac{\\gamma_r(h)}{\\gamma_r(0)}$.\n$$\\rho(h) = \\frac{\\frac{A_k^2}{2} \\cos(2\\pi f h) + \\sigma^2 \\delta_{h,0}}{\\frac{A_k^2}{2} + \\sigma^2}$$\nFor any non-zero lag $h \\neq 0$, the Kronecker delta is zero, and the expression simplifies to:\n$$\\rho(h) = \\frac{\\frac{A_k^2}{2} \\cos(2\\pi f h)}{\\frac{A_k^2}{2} + \\sigma^2}, \\quad h \\neq 0$$\nThis is the required expression. It reveals that the residual ACF will itself be periodic, tracing a cosine wave at the frequency $f$ of the omitted harmonic, with its amplitude dampened by the noise variance.\n\n### Task 2: Numerical Calculation for a Specific Case\n\nWe are asked to compute the expected residual autocorrelation at lag $h=6$ for the specific case:\n- Omitted harmonic: $k=2$\n- Frequency: $f = \\frac{k}{12} = \\frac{2}{12} = \\frac{1}{6}$\n- Amplitude: $A_2 = 0.15$\n- Noise variance: $\\sigma^2 = 0.01$\n- Lag: $h=6$\n\nUsing the derived formula for $\\rho(h)$ with $h \\neq 0$:\n$$\\rho(6) = \\frac{\\frac{A_2^2}{2} \\cos(2\\pi f h)}{\\frac{A_2^2}{2} + \\sigma^2}$$\nSubstitute the given values:\n$$\\rho(6) = \\frac{\\frac{(0.15)^2}{2} \\cos\\left(2\\pi \\cdot \\frac{1}{6} \\cdot 6\\right)}{\\frac{(0.15)^2}{2} + 0.01}$$\nThe argument of the cosine function is $2\\pi \\cdot \\frac{1}{6} \\cdot 6 = 2\\pi$. Since $\\cos(2\\pi)=1$:\n$$\\rho(6) = \\frac{\\frac{0.0225}{2}}{\\frac{0.0225}{2} + 0.01} = \\frac{0.01125}{0.01125 + 0.01}$$\n$$\\rho(6) = \\frac{0.01125}{0.02125}$$\nThis fraction simplifies to $\\frac{1125}{2125} = \\frac{9 \\times 125}{17 \\times 125} = \\frac{9}{17}$.\nAs a decimal, this is:\n$$\\rho(6) = \\frac{9}{17} \\approx 0.52941176...$$\nRounding to four significant figures, we get $0.5294$. This high positive correlation is expected because the lag $h=6$ matches the period of the omitted harmonic ($1/f = 6$ months), so the residuals are strongly correlated with themselves after one full cycle of the unmodeled dynamic.\n\n### Task 3: Residual ACF under Model Overfit\n\nIn the case of overfitting, the model includes an extra harmonic term whose true amplitude is zero. All true harmonics are correctly included. In the large-sample regime, a consistent estimation method like harmonic regression (which is a form of ordinary least squares) will produce coefficient estimates that converge to their true values.\nThe true model is $X_t = T_t + S_t + \\varepsilon_t$. The fitted model includes all components of $T_t$ and $S_t$ plus an extra, non-existent harmonic. As $n \\to \\infty$, the estimated coefficient (amplitude) for this extra harmonic will converge to its true value of zero. Consequently, the fitted trend and seasonality, $\\widehat{T}_t + \\widehat{S}_t$, converge in probability to the true trend and seasonality, $T_t + S_t$.\nThe residuals are therefore:\n$$r_t = X_t - (\\widehat{T}_t + \\widehat{S}_t) \\xrightarrow{n \\to \\infty} (T_t + S_t + \\varepsilon_t) - (T_t + S_t) = \\varepsilon_t$$\nIn the large-sample limit, the residual series $r_t$ converges to the white noise process $\\varepsilon_t$. The autocorrelation function of a white noise process is, by definition, $1$ at lag $h=0$ and $0$ for all non-zero lags $h \\neq 0$.\nTherefore, the large-sample expected residual ACF in the overfit case is $\\rho(h) = 0$ for all $h \\neq 0$.",
            "answer": "$$\\boxed{0.5294}$$"
        }
    ]
}