{
    "hands_on_practices": [
        {
            "introduction": "In practice, the noise in satellite time series is rarely uniform; factors like sensor viewing geometry, atmospheric conditions, or snow cover can introduce time-dependent variance (heteroskedasticity). This exercise moves beyond idealized assumptions by employing Weighted Least Squares (WLS), the proper statistical tool for such cases. You will derive the covariance matrix for the harmonic coefficients, which provides the foundation for quantifying the uncertainty of our seasonal model parameters under realistic conditions .",
            "id": "3818979",
            "problem": "A satellite measures a vegetation index over a single hydrological year with nominal period $T$ (in days), producing a time series $\\{y(t_i)\\}_{i=1}^{N}$ at observation times $t_i$ that are not necessarily uniformly spaced. After physically based pre-processing that removes the long-term mean and all harmonics except a selected wavenumber $k$, the residual signal is modeled as\n$$\ny(t_i) \\;=\\; a_k \\,\\cos\\!\\big(\\omega_k\\, t_i\\big) \\;+\\; b_k \\,\\sin\\!\\big(\\omega_k\\, t_i\\big) \\;+\\; \\varepsilon_i,\n$$\nwhere $\\omega_k = 2\\pi k / T$ is the angular frequency expressed in radians per day, $a_k$ and $b_k$ are the unknown harmonic coefficients for the retained wavenumber $k$, and the residuals $\\varepsilon_i$ satisfy $\\mathbb{E}[\\varepsilon_i]=0$, are mutually uncorrelated, and have a known heteroskedastic variance function $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2(t_i)$ determined from sensor geometry and atmospheric conditions. All trigonometric arguments are in radians.\n\nIn environmental modeling practice, when a variance function $\\sigma^2(t)$ is known, the Best Linear Unbiased Estimator is obtained by Weighted Least Squares (WLS) using weights $w(t_i) = 1/\\sigma^2(t_i)$. Let the $N \\times 2$ design matrix for the retained wavenumber be\n$$\n\\mathbf{X}_k \\;=\\;\n\\begin{pmatrix}\n\\cos(\\omega_k t_1) & \\sin(\\omega_k t_1) \\\\\n\\vdots & \\vdots \\\\\n\\cos(\\omega_k t_N) & \\sin(\\omega_k t_N)\n\\end{pmatrix},\n$$\nand let the diagonal weight matrix be $\\mathbf{W} = \\operatorname{diag}\\big(w(t_1),\\ldots,w(t_N)\\big)$ with $w(t_i) = 1/\\sigma^2(t_i)$.\n\nStarting from fundamental linear model assumptions and the properties of WLS under known heteroskedasticity, derive the expected covariance matrix of the WLS estimator of the parameter vector $\\boldsymbol{\\theta}_k = (a_k, b_k)^{\\top}$ for the retained wavenumber $k$, and express your final answer in closed form using the discrete weighted sums\n$$\nS_{cc} \\;=\\; \\sum_{i=1}^{N} w(t_i)\\,\\cos^2(\\omega_k t_i), \\quad\nS_{ss} \\;=\\; \\sum_{i=1}^{N} w(t_i)\\,\\sin^2(\\omega_k t_i), \\quad\nS_{cs} \\;=\\; \\sum_{i=1}^{N} w(t_i)\\,\\cos(\\omega_k t_i)\\,\\sin(\\omega_k t_i).\n$$\n\nYour final answer must be a single closed-form analytic expression for the $2 \\times 2$ covariance matrix of $(a_k,b_k)$ in terms of $S_{cc}$, $S_{ss}$, and $S_{cs}$. No numerical evaluation is required, and no rounding is needed. State the final expression only; do not include units inside the final expression. Angles should be considered in radians throughout.",
            "solution": "The problem requires the derivation of the expected covariance matrix for the Weighted Least Squares (WLS) estimator of the harmonic coefficients $\\boldsymbol{\\theta}_k = (a_k, b_k)^{\\top}$. The derivation will proceed from the fundamental formulation of the problem as a general linear model.\n\nThe given model for a single observation $t_i$ is\n$$\ny(t_i) \\;=\\; a_k \\,\\cos(\\omega_k\\, t_i) \\;+\\; b_k \\,\\sin(\\omega_k\\, t_i) \\;+\\; \\varepsilon_i\n$$\nThis can be expressed in matrix form for all $N$ observations as a linear model $\\mathbf{y} = \\mathbf{X}_k\\boldsymbol{\\theta}_k + \\boldsymbol{\\varepsilon}$, where:\n- $\\mathbf{y} = (y(t_1), \\dots, y(t_N))^{\\top}$ is the $N \\times 1$ vector of observations.\n- $\\mathbf{X}_k$ is the $N \\times 2$ design matrix, as given in the problem statement.\n- $\\boldsymbol{\\theta}_k = (a_k, b_k)^{\\top}$ is the $2 \\times 1$ vector of parameters to be estimated.\n- $\\boldsymbol{\\varepsilon} = (\\varepsilon_1, \\dots, \\varepsilon_N)^{\\top}$ is the $N \\times 1$ vector of random errors.\n\nThe problem states the statistical properties of the error terms:\n1.  The expected value of each error is zero, $\\mathbb{E}[\\varepsilon_i] = 0$. This implies the expected value of the error vector is the zero vector, $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$.\n2.  The errors are mutually uncorrelated, meaning $\\operatorname{Cov}(\\varepsilon_i, \\varepsilon_j) = 0$ for $i \\neq j$.\n3.  The variance of each error is known and heteroskedastic, $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2(t_i)$.\n\nFrom these properties, we can construct the covariance matrix of the error vector $\\boldsymbol{\\varepsilon}$:\n$$\n\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}] = \\operatorname{diag}(\\operatorname{Var}(\\varepsilon_1), \\dots, \\operatorname{Var}(\\varepsilon_N)) = \\operatorname{diag}(\\sigma^2(t_1), \\dots, \\sigma^2(t_N))\n$$\nThe problem defines the weights for WLS as $w(t_i) = 1/\\sigma^2(t_i)$, and the weight matrix as $\\mathbf{W} = \\operatorname{diag}(w(t_1), \\dots, w(t_N))$. It follows directly that the covariance matrix of the errors is the inverse of the weight matrix:\n$$\n\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\mathbf{W}^{-1}\n$$\nThe WLS estimator $\\hat{\\boldsymbol{\\theta}}_k$ for $\\boldsymbol{\\theta}_k$ is the one that minimizes the weighted sum of squared residuals, given by the expression $(\\mathbf{y} - \\mathbf{X}_k\\boldsymbol{\\theta}_k)^{\\top}\\mathbf{W}(\\mathbf{y} - \\mathbf{X}_k\\boldsymbol{\\theta}_k)$. The solution to this minimization problem is the well-known WLS estimator:\n$$\n\\hat{\\boldsymbol{\\theta}}_k = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{y}\n$$\nTo find the covariance matrix of this estimator, we first establish its expected value. Since $\\mathbb{E}[\\mathbf{y}] = \\mathbb{E}[\\mathbf{X}_k\\boldsymbol{\\theta}_k + \\boldsymbol{\\varepsilon}] = \\mathbf{X}_k\\boldsymbol{\\theta}_k + \\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{X}_k\\boldsymbol{\\theta}_k$, we have:\n$$\n\\mathbb{E}[\\hat{\\boldsymbol{\\theta}}_k] = \\mathbb{E}[(\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{y}] = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbb{E}[\\mathbf{y}] = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k) \\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_k\n$$\nThis confirms that the WLS estimator is unbiased. The covariance matrix of $\\hat{\\boldsymbol{\\theta}}_k$ is defined as $\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_k) = \\mathbb{E}[(\\hat{\\boldsymbol{\\theta}}_k - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}_k])(\\hat{\\boldsymbol{\\theta}}_k - \\mathbb{E}[\\hat{\\boldsymbol{\\theta}}_k])^{\\top}]$. Using the unbiasedness, this becomes:\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_k) = \\mathbb{E}[(\\hat{\\boldsymbol{\\theta}}_k - \\boldsymbol{\\theta}_k)(\\hat{\\boldsymbol{\\theta}}_k - \\boldsymbol{\\theta}_k)^{\\top}]\n$$\nWe can express the term $(\\hat{\\boldsymbol{\\theta}}_k - \\boldsymbol{\\theta}_k)$ in terms of the error vector $\\boldsymbol{\\varepsilon}$:\n$$\n\\hat{\\boldsymbol{\\theta}}_k - \\boldsymbol{\\theta}_k = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{y} - \\boldsymbol{\\theta}_k = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W}(\\mathbf{X}_k\\boldsymbol{\\theta}_k + \\boldsymbol{\\varepsilon}) - \\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_k + (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W}\\boldsymbol{\\varepsilon} - \\boldsymbol{\\theta}_k = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W}\\boldsymbol{\\varepsilon}\n$$\nNow we substitute this into the expression for the covariance matrix. Let $\\mathbf{M} = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W}$. Then $\\hat{\\boldsymbol{\\theta}}_k - \\boldsymbol{\\theta}_k = \\mathbf{M}\\boldsymbol{\\varepsilon}$.\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_k) = \\mathbb{E}[(\\mathbf{M}\\boldsymbol{\\varepsilon})(\\mathbf{M}\\boldsymbol{\\varepsilon})^{\\top}] = \\mathbb{E}[\\mathbf{M}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}\\mathbf{M}^{\\top}] = \\mathbf{M} \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\top}] \\mathbf{M}^{\\top} = \\mathbf{M} \\operatorname{Cov}(\\boldsymbol{\\varepsilon}) \\mathbf{M}^{\\top}\n$$\nSubstituting back the expressions for $\\mathbf{M}$ and $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\mathbf{W}^{-1}$:\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_k) = \\left( (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W} \\right) (\\mathbf{W}^{-1}) \\left( (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} \\mathbf{X}_k^{\\top}\\mathbf{W} \\right)^{\\top}\n$$\nThe transpose term is $(\\mathbf{W}^{\\top}\\mathbf{X}_k^{\\top\\top})((\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1})^{\\top} = \\mathbf{W}\\mathbf{X}_k(\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1}$ since $\\mathbf{W}$ is diagonal and $\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k$ is symmetric.\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_k) = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} (\\mathbf{X}_k^{\\top}\\mathbf{W}) (\\mathbf{W}^{-1}) (\\mathbf{W}\\mathbf{X}_k) (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1}\n$$\nUsing associativity of matrix multiplication, the middle part simplifies: $(\\mathbf{X}_k^{\\top}\\mathbf{W}) (\\mathbf{W}^{-1}) (\\mathbf{W}\\mathbf{X}_k) = \\mathbf{X}_k^{\\top}(\\mathbf{W}\\mathbf{W}^{-1})\\mathbf{W}\\mathbf{X}_k = \\mathbf{X}_k^{\\top}\\mathbf{I}\\mathbf{W}\\mathbf{X}_k = \\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k$.\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_k) = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k) (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} = \\mathbf{I} (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1}\n$$\nThe task reduces to computing the matrix $\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k$ and then finding its inverse.\nLet $c_i = \\cos(\\omega_k t_i)$ and $s_i = \\sin(\\omega_k t_i)$.\n$$\n\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k = \\begin{pmatrix} c_1 & \\cdots & c_N \\\\ s_1 & \\cdots & s_N \\end{pmatrix} \\begin{pmatrix} w(t_1) & & 0 \\\\ & \\ddots & \\\\ 0 & & w(t_N) \\end{pmatrix} \\begin{pmatrix} c_1 & s_1 \\\\ \\vdots & \\vdots \\\\ c_N & s_N \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} c_1 & \\cdots & c_N \\\\ s_1 & \\cdots & s_N \\end{pmatrix} \\begin{pmatrix} w(t_1)c_1 & w(t_1)s_1 \\\\ \\vdots & \\vdots \\\\ w(t_N)c_N & w(t_N)s_N \\end{pmatrix}\n= \\begin{pmatrix} \\sum_{i=1}^{N} w(t_i)c_i^2 & \\sum_{i=1}^{N} w(t_i)c_i s_i \\\\ \\sum_{i=1}^{N} w(t_i)s_i c_i & \\sum_{i=1}^{N} w(t_i)s_i^2 \\end{pmatrix}\n$$\nUsing the definitions provided in the problem statement:\n- $S_{cc} = \\sum_{i=1}^{N} w(t_i)\\cos^2(\\omega_k t_i)$\n- $S_{ss} = \\sum_{i=1}^{N} w(t_i)\\sin^2(\\omega_k t_i)$\n- $S_{cs} = \\sum_{i=1}^{N} w(t_i)\\cos(\\omega_k t_i)\\sin(\\omega_k t_i)$\nThe matrix becomes:\n$$\n\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k = \\begin{pmatrix} S_{cc} & S_{cs} \\\\ S_{cs} & S_{ss} \\end{pmatrix}\n$$\nFinally, we compute the inverse of this $2 \\times 2$ matrix. The determinant is $\\det(\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k) = S_{cc}S_{ss} - S_{cs}^2$. The inverse is:\n$$\n\\operatorname{Cov}(\\hat{\\boldsymbol{\\theta}}_k) = (\\mathbf{X}_k^{\\top}\\mathbf{W}\\mathbf{X}_k)^{-1} = \\frac{1}{S_{cc}S_{ss} - S_{cs}^2} \\begin{pmatrix} S_{ss} & -S_{cs} \\\\ -S_{cs} & S_{cc} \\end{pmatrix}\n$$\nThis is the final expression for the covariance matrix of the WLS estimator of $(a_k, b_k)^{\\top}$.",
            "answer": "$$\n\\boxed{\\frac{1}{S_{cc}S_{ss} - S_{cs}^2} \\begin{pmatrix} S_{ss} & -S_{cs} \\\\ -S_{cs} & S_{cc} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Estimating harmonic coefficients is a means to an end; the ultimate goal is to extract scientifically meaningful information, such as the timing of key phenological events. Building upon the concept of coefficient uncertainty, this practice guides you through the process of uncertainty propagation. You will learn how to translate the statistical variance of your model's sine and cosine coefficients into the real-world uncertainty of a derived metric—in this case, the day of the year corresponding to the peak of the seasonal cycle .",
            "id": "3818966",
            "problem": "A single-pixel seasonal vegetation signal observed by a satellite sensor is modeled at time $t$ as a first-harmonic linear combination of cosine and sine:\n$$\ny(t) \\;=\\; \\mu \\;+\\; c \\cos\\!\\big(\\omega t\\big) \\;+\\; s \\sin\\!\\big(\\omega t\\big) \\;+\\; \\varepsilon(t),\n$$\nwhere $\\omega \\;=\\; \\frac{2\\pi}{T}$ is the known annual angular frequency for period $T$, $\\mu$ is a constant offset, and $\\varepsilon(t)$ is zero-mean noise. The cosine and sine coefficients $(c,s)$ are obtained via ordinary least squares (OLS), and their joint uncertainty is summarized by the covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\operatorname{Var}(c) & \\operatorname{Cov}(c,s) \\\\\n\\operatorname{Cov}(c,s) & \\operatorname{Var}(s)\n\\end{pmatrix}.\n$$\nThe amplitude–phase reparameterization is defined by\n$$\nc \\;=\\; A \\cos(\\phi), \\qquad s \\;=\\; A \\sin(\\phi),\n$$\nso that\n$$\ny(t) \\;=\\; \\mu \\;+\\; A \\cos\\!\\big(\\omega t - \\phi\\big),\n$$\nwith phase $\\phi \\in [0,2\\pi)$ measured in radians. The time of peak within the year is defined by the earliest $t$ such that $\\omega t - \\phi \\equiv 0 \\;(\\mathrm{mod}\\; 2\\pi)$, which, on the principal branch, is\n$$\nt_{\\mathrm{peak}} \\;=\\; \\frac{\\phi}{2\\pi}\\,T.\n$$\n\nYou are given the following statistically consistent OLS outputs for a particular pixel and year:\n- Estimated cosine coefficient $\\hat{c} \\;=\\; 0.12$,\n- Estimated sine coefficient $\\hat{s} \\;=\\; 0.16$,\n- Covariance matrix entries $\\operatorname{Var}(c) \\;=\\; 2.0 \\times 10^{-5}$, $\\operatorname{Var}(s) \\;=\\; 3.0 \\times 10^{-5}$, and $\\operatorname{Cov}(c,s) \\;=\\; 0.5 \\times 10^{-5}$,\n- Annual period $T \\;=\\; 365$ days,\n- All angles are in radians.\n\nStarting only from the definitions above, the differentiability of smooth mappings, and first-order (linear) uncertainty propagation for differentiable transformations of random vectors, derive the first-order variance of $\\phi$ from the joint uncertainty of $(c,s)$, then propagate it through $t_{\\mathrm{peak}} \\;=\\; \\frac{\\phi}{2\\pi} T$ to obtain the one-standard-deviation uncertainty $\\sigma_{t_{\\mathrm{peak}}}$ in days. \n\nRound your final numerical answer for $\\sigma_{t_{\\mathrm{peak}}}$ to four significant figures. Express the final time uncertainty in days. Provide only the single numerical value as your final answer. The angle unit is radians.",
            "solution": "The objective is to find the one-standard-deviation uncertainty, $\\sigma_{t_{\\mathrm{peak}}}$, of the time of peak vegetation signal, $t_{\\mathrm{peak}}$. The problem requires a derivation based on first-order uncertainty propagation, also known as the Delta method. This involves two main steps: first, propagating the uncertainty from the cosine and sine coefficients $(c, s)$ to the phase angle $\\phi$, and second, propagating the uncertainty from $\\phi$ to $t_{\\mathrm{peak}}$.\n\nThe phase angle $\\phi$ is a function of the coefficients $(c,s)$ via the transformation $c = A \\cos(\\phi)$ and $s = A \\sin(\\phi)$. This implies $\\tan(\\phi) = s/c$. For the given estimated coefficients $\\hat{c} = 0.12 > 0$ and $\\hat{s} = 0.16 > 0$, the phase angle is in the first quadrant, and the function can be written as $\\phi(c, s) = \\arctan(s/c)$.\n\nThe first-order approximation for the variance of a function $g(X)$ of a random vector $X$ is given by:\n$$\n\\operatorname{Var}(g(X)) \\approx (\\nabla g)^T \\Sigma_X (\\nabla g)\n$$\nwhere $\\nabla g$ is the gradient of the function $g$ evaluated at the mean of $X$, and $\\Sigma_X$ is the covariance matrix of $X$. In our case, $g(c,s) = \\phi(c,s)$, the random vector is $X = (c,s)^T$, and its covariance matrix is $\\Sigma$.\n\nFirst, we compute the gradient of $\\phi(c,s) = \\arctan(s/c)$. The partial derivatives are:\n$$\n\\frac{\\partial\\phi}{\\partial c} = \\frac{\\partial}{\\partial c} \\arctan\\left(\\frac{s}{c}\\right) = \\frac{1}{1 + (s/c)^2} \\cdot \\left(-\\frac{s}{c^2}\\right) = \\frac{c^2}{c^2+s^2} \\cdot \\left(-\\frac{s}{c^2}\\right) = -\\frac{s}{c^2+s^2}\n$$\n$$\n\\frac{\\partial\\phi}{\\partial s} = \\frac{\\partial}{\\partial s} \\arctan\\left(\\frac{s}{c}\\right) = \\frac{1}{1 + (s/c)^2} \\cdot \\left(\\frac{1}{c}\\right) = \\frac{c^2}{c^2+s^2} \\cdot \\left(\\frac{1}{c}\\right) = \\frac{c}{c^2+s^2}\n$$\nThe gradient vector $\\nabla \\phi$ is therefore:\n$$\n\\nabla \\phi = \\begin{pmatrix} \\frac{\\partial\\phi}{\\partial c} \\\\ \\frac{\\partial\\phi}{\\partial s} \\end{pmatrix} = \\frac{1}{c^2+s^2} \\begin{pmatrix} -s \\\\ c \\end{pmatrix}\n$$\nNow, we apply the uncertainty propagation formula to find the variance of $\\phi$, $\\operatorname{Var}(\\phi)$:\n$$\n\\operatorname{Var}(\\phi) \\approx \\left( \\frac{1}{c^2+s^2} \\begin{pmatrix} -s \\\\ c \\end{pmatrix} \\right)^T \\begin{pmatrix} \\operatorname{Var}(c) & \\operatorname{Cov}(c,s) \\\\ \\operatorname{Cov}(c,s) & \\operatorname{Var}(s) \\end{pmatrix} \\left( \\frac{1}{c^2+s^2} \\begin{pmatrix} -s \\\\ c \\end{pmatrix} \\right)\n$$\n$$\n\\operatorname{Var}(\\phi) \\approx \\frac{1}{(c^2+s^2)^2} \\begin{pmatrix} -s & c \\end{pmatrix} \\begin{pmatrix} \\operatorname{Var}(c) & \\operatorname{Cov}(c,s) \\\\ \\operatorname{Cov}(c,s) & \\operatorname{Var}(s) \\end{pmatrix} \\begin{pmatrix} -s \\\\ c \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\n\\begin{pmatrix} -s & c \\end{pmatrix} \\begin{pmatrix} \\operatorname{Var}(c) & \\operatorname{Cov}(c,s) \\\\ \\operatorname{Cov}(c,s) & \\operatorname{Var}(s) \\end{pmatrix} = \\begin{pmatrix} -s\\operatorname{Var}(c) + c\\operatorname{Cov}(c,s) & -s\\operatorname{Cov}(c,s) + c\\operatorname{Var}(s) \\end{pmatrix}\n$$\n$$\n\\left( \\begin{pmatrix} -s\\operatorname{Var}(c) + c\\operatorname{Cov}(c,s) & -s\\operatorname{Cov}(c,s) + c\\operatorname{Var}(s) \\end{pmatrix} \\right) \\begin{pmatrix} -s \\\\ c \\end{pmatrix} = s^2\\operatorname{Var}(c) - sc\\operatorname{Cov}(c,s) - sc\\operatorname{Cov}(c,s) + c^2\\operatorname{Var}(s)\n$$\nThis simplifies to:\n$$\ns^2\\operatorname{Var}(c) + c^2\\operatorname{Var}(s) - 2sc\\operatorname{Cov}(c,s)\n$$\nThus, the derived expression for the variance of the phase is:\n$$\n\\operatorname{Var}(\\phi) \\approx \\frac{s^2\\operatorname{Var}(c) + c^2\\operatorname{Var}(s) - 2sc\\operatorname{Cov}(c,s)}{(c^2+s^2)^2}\n$$\nNow we substitute the given numerical values into this expression, evaluated at the estimates $(\\hat{c}, \\hat{s})$:\n$\\hat{c} = 0.12$, $\\hat{s} = 0.16$, $\\operatorname{Var}(c) = 2.0 \\times 10^{-5}$, $\\operatorname{Var}(s) = 3.0 \\times 10^{-5}$, and $\\operatorname{Cov}(c,s) = 0.5 \\times 10^{-5}$.\n\nWe calculate the components:\n$\\hat{c}^2 = (0.12)^2 = 0.0144$\n$\\hat{s}^2 = (0.16)^2 = 0.0256$\n$\\hat{c}^2 + \\hat{s}^2 = 0.0144 + 0.0256 = 0.04$\n$2\\hat{s}\\hat{c} = 2(0.16)(0.12) = 0.0384$\n\nThe numerator is:\n$N = (0.0256)(2.0 \\times 10^{-5}) + (0.0144)(3.0 \\times 10^{-5}) - (0.0384)(0.5 \\times 10^{-5})$\n$N = (0.0512 \\times 10^{-5}) + (0.0432 \\times 10^{-5}) - (0.0192 \\times 10^{-5})$\n$N = (0.0512 + 0.0432 - 0.0192) \\times 10^{-5} = 0.0752 \\times 10^{-5} = 7.52 \\times 10^{-7}$\n\nThe denominator is:\n$D = (\\hat{c}^2 + \\hat{s}^2)^2 = (0.04)^2 = 0.0016 = 1.6 \\times 10^{-3}$\n\nSo, the variance of $\\phi$ is:\n$$\n\\operatorname{Var}(\\phi) \\approx \\frac{7.52 \\times 10^{-7}}{1.6 \\times 10^{-3}} = 4.7 \\times 10^{-4} \\;\\text{rad}^2\n$$\nThe second step is to propagate this variance to $t_{\\mathrm{peak}}$. The relationship is given as:\n$$\nt_{\\mathrm{peak}} = \\frac{\\phi}{2\\pi} T\n$$\nThis is a linear scaling of the random variable $\\phi$ by a constant factor $k = \\frac{T}{2\\pi}$. For a transformation $Y = kX$, the variance propagates as $\\operatorname{Var}(Y) = k^2 \\operatorname{Var}(X)$. Therefore:\n$$\n\\operatorname{Var}(t_{\\mathrm{peak}}) = \\left(\\frac{T}{2\\pi}\\right)^2 \\operatorname{Var}(\\phi)\n$$\nThe one-standard-deviation uncertainty, $\\sigma_{t_{\\mathrm{peak}}}$, is the square root of the variance:\n$$\n\\sigma_{t_{\\mathrm{peak}}} = \\sqrt{\\operatorname{Var}(t_{\\mathrm{peak}})} = \\sqrt{\\left(\\frac{T}{2\\pi}\\right)^2 \\operatorname{Var}(\\phi)} = \\frac{T}{2\\pi} \\sqrt{\\operatorname{Var}(\\phi)}\n$$\nSubstituting the value for the annual period $T = 365$ days and the calculated $\\operatorname{Var}(\\phi)$:\n$$\n\\sigma_{t_{\\mathrm{peak}}} = \\frac{365}{2\\pi} \\sqrt{4.7 \\times 10^{-4}}\n$$\nWe calculate the numerical value:\n$$\n\\sqrt{4.7 \\times 10^{-4}} \\approx 0.021679483\n$$\n$$\n\\sigma_{t_{\\mathrm{peak}}} \\approx \\frac{365}{2\\pi} \\times 0.021679483 \\approx 58.09155 \\times 0.021679483 \\approx 1.2593683 \\;\\text{days}\n$$\nRounding the final result to four significant figures, as requested, we get $1.259$.",
            "answer": "$$\n\\boxed{1.259}\n$$"
        },
        {
            "introduction": "A single satellite pixel often represents a mosaic of different land cover types, each with its own distinct seasonal rhythm. This exercise addresses such sub-pixel heterogeneity by modeling the observed signal as a linear mixture of underlying components. By applying the principle of superposition, you will explore how the distinct seasonal signals from different eco-climatic regimes combine, allowing you to calculate the amplitude and phase of the resulting composite waveform observed by the sensor .",
            "id": "3818982",
            "problem": "A seasonal vegetation signal observed by satellite for one calendar year is modeled as a truncated Fourier representation of the Normalized Difference Vegetation Index (NDVI), where $t$ denotes time in days and $T = 365$ is the period in days. Consider a $3 \\times 3$ kilometer pixel that is a linear mixture of two sub-pixel eco-climatic regimes, denoted $A$ and $B$, with area fractions $w_{A} = 0.6$ and $w_{B} = 0.4$, respectively. Assume that sub-pixel NDVI seasonal anomalies are additive and well-approximated by their semiannual harmonic only, with angular frequency $\\omega = \\frac{4\\pi}{T}$ and phases given in radians. The sub-pixel semiannual components are\n$$\ns_{A}(t) = A_{A}\\cos\\!\\left(\\omega t + \\phi_{A}\\right), \\quad s_{B}(t) = A_{B}\\cos\\!\\left(\\omega t + \\phi_{B}\\right),\n$$\nwhere $A_{A} = 0.18$, $\\phi_{A} = -\\frac{\\pi}{6}$, $A_{B} = 0.12$, and $\\phi_{B} = \\frac{\\pi}{3}$. The pixel-level signal is modeled as\n$$\nx(t) = w_{A}s_{A}(t) + w_{B}s_{B}(t).\n$$\nThere exists an equivalent single semiannual harmonic $x(t) = A_{\\text{mix}}\\cos\\!\\left(\\omega t + \\phi_{\\text{mix}}\\right)$ that exactly matches the semiannual content of $x(t)$.\n\nUsing the foundations of Fourier analysis and the linear mixture assumption, determine the combined semiannual amplitude $A_{\\text{mix}}$. Round your final numerical answer to four significant figures. Express the amplitude as a dimensionless quantity. Additionally, based on your derivation, interpret the presence and timing of the two peaks of the semiannual component in terms of plausible bimodal climate regimes affecting vegetation growth, but do not include any interpretive text in your final answer.",
            "solution": "The problem requires finding the amplitude $A_{\\text{mix}}$ of a composite signal $x(t)$, which is a weighted sum of two semiannual harmonics, $s_A(t)$ and $s_B(t)$, of the same frequency $\\omega$.\n$$\nx(t) = w_{A}A_{A}\\cos(\\omega t + \\phi_{A}) + w_{B}A_{B}\\cos(\\omega t + \\phi_{B})\n$$\nThe sum of two sinusoidal functions with the same frequency results in another sinusoid of that same frequency. This problem can be solved by treating the two components as vectors (or phasors) in a 2D plane and adding them. The magnitude of the resultant vector will be the amplitude $A_{\\text{mix}}$.\n\nA signal of the form $A\\cos(\\omega t + \\phi)$ can be represented by a phasor of length $A$ at an angle $\\phi$ relative to the positive x-axis. The two weighted components of our signal can be represented by two phasors:\n- Phasor 1: has magnitude $|C_1| = w_A A_A$ and phase angle $\\phi_A$.\n- Phasor 2: has magnitude $|C_2| = w_B A_B$ and phase angle $\\phi_B$.\n\nThe resultant phasor, representing the mixed signal $x(t)$, is the vector sum of these two phasors. The squared magnitude of this resultant phasor, $A_{\\text{mix}}^2$, can be found using the law of cosines:\n$$\nA_{\\text{mix}}^2 = |C_1|^2 + |C_2|^2 + 2|C_1||C_2|\\cos(\\Delta\\phi)\n$$\nwhere $\\Delta\\phi = \\phi_B - \\phi_A$ is the phase difference between the two phasors.\n\nFirst, we calculate the magnitudes of the two weighted component phasors:\n- $|C_1| = w_A A_A = 0.6 \\times 0.18 = 0.108$\n- $|C_2| = w_B A_B = 0.4 \\times 0.12 = 0.048$\n\nNext, we calculate the phase difference, $\\Delta\\phi$:\n$$\n\\Delta\\phi = \\phi_B - \\phi_A = \\frac{\\pi}{3} - \\left(-\\frac{\\pi}{6}\\right) = \\frac{2\\pi}{6} + \\frac{\\pi}{6} = \\frac{3\\pi}{6} = \\frac{\\pi}{2} \\; \\text{radians}\n$$\nNow, we can substitute these values into the law of cosines formula. The cosine of the phase difference is:\n$$\n\\cos(\\Delta\\phi) = \\cos\\left(\\frac{\\pi}{2}\\right) = 0\n$$\nBecause the cosine term is zero, the formula simplifies to the Pythagorean theorem, which is expected when adding two orthogonal vectors:\n$$\nA_{\\text{mix}}^2 = |C_1|^2 + |C_2|^2\n$$\n$$\nA_{\\text{mix}}^2 = (0.108)^2 + (0.048)^2\n$$\n$$\nA_{\\text{mix}}^2 = 0.011664 + 0.002304 = 0.013968\n$$\nFinally, we find the amplitude $A_{\\text{mix}}$ by taking the square root:\n$$\nA_{\\text{mix}} = \\sqrt{0.013968} \\approx 0.1181863...\n$$\nRounding the final numerical answer to four significant figures gives $0.1182$.",
            "answer": "$$\\boxed{0.1182}$$"
        }
    ]
}