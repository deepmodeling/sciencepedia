{
    "hands_on_practices": [
        {
            "introduction": "The first step in any parameter estimation task is to translate a physical model and statistical assumptions into a mathematical objective function that an algorithm can minimize. This exercise demonstrates the construction of a Maximum A Posteriori (MAP) objective function by combining a Gaussian likelihood with a suitable prior distribution. It introduces a crucial technique—reparameterization via a log-transform—to handle physical constraints like positivity, converting a difficult constrained problem into an unconstrained one suitable for standard optimization routines. Mastering this practice  is fundamental to building robust and physically meaningful inversion models.",
            "id": "3831139",
            "problem": "A hyperspectral imaging spectrometer acquires radiance measurements at $m$ wavelengths over a vegetated surface. A simplified linearized forward model relates the measured radiance vector $y \\in \\mathbb{R}^{m}$ to a set of $n$ strictly positive biophysical parameters $\\theta \\in \\mathbb{R}_{>0}^{n}$ (e.g., nonnegative abundance-like scaling coefficients of radiative transfer basis functions) via a known sensitivity matrix $G \\in \\mathbb{R}^{m \\times n}$, with additive, independent Gaussian measurement errors. Specifically, the data model is the conditional distribution $y \\,|\\, \\theta \\sim \\mathcal{N}(G \\theta, \\sigma^{2} I_{m})$, where $\\mathcal{N}(\\mu, \\Sigma)$ denotes the multivariate normal distribution with mean $\\mu$ and covariance $\\Sigma$, $\\sigma^{2} > 0$ is a known noise variance, and $I_{m}$ is the $m \\times m$ identity matrix. To encode physical positivity and prior knowledge, assume the components of $\\theta$ are a priori independent and follow Gamma distributions with shape $k_{i} > 0$ and rate $\\lambda_{i} > 0$, i.e., for each $i \\in \\{1,\\dots,n\\}$, $\\theta_{i} \\sim \\mathrm{Gamma}(k_{i}, \\lambda_{i})$ with density $p(\\theta_{i}) = \\frac{\\lambda_{i}^{k_{i}}}{\\Gamma(k_{i})} \\theta_{i}^{k_{i}-1} \\exp(-\\lambda_{i} \\theta_{i})$ for $\\theta_{i} > 0$, where $\\Gamma(\\cdot)$ is the Gamma function.\n\nUsing Bayes’ theorem, the posterior density satisfies $p(\\theta \\,|\\, y) \\propto p(y \\,|\\, \\theta) \\, p(\\theta)$. Consider Maximum A Posteriori (MAP) estimation in an unconstrained space by introducing the log-transform $\\phi \\in \\mathbb{R}^{n}$ defined componentwise by $\\phi_{i} = \\ln(\\theta_{i})$ for $i = 1,\\dots,n$, so that $\\theta = \\exp(\\phi)$ with the exponential applied elementwise. Treat $\\phi$ as the optimization variable and use the change-of-variables rule for densities to write the posterior density $p(\\phi \\,|\\, y)$ in terms of $p(\\theta \\,|\\, y)$ and the Jacobian of the transformation.\n\nDerive and simplify the negative log-posterior objective $J_{\\phi}(\\phi)$ to be minimized with respect to $\\phi$, expressed entirely in terms of $\\phi$, $y$, $G$, $\\sigma^{2}$, $k_{i}$, and $\\lambda_{i}$. Ignore additive constants that do not depend on $\\phi$. Your final answer must be a single closed-form analytical expression. No numerical approximation is required.",
            "solution": "The user wants to solve the problem. I will now proceed with the solution.\nThe problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   Measured radiance vector: $y \\in \\mathbb{R}^{m}$\n-   Biophysical parameters: $\\theta \\in \\mathbb{R}_{>0}^{n}$\n-   Sensitivity matrix: $G \\in \\mathbb{R}^{m \\times n}$\n-   Data model (likelihood): $y \\,|\\, \\theta \\sim \\mathcal{N}(G \\theta, \\sigma^{2} I_{m})$\n-   Known noise variance: $\\sigma^{2} > 0$\n-   Prior on $\\theta$ components: Independent, with $\\theta_{i} \\sim \\mathrm{Gamma}(k_{i}, \\lambda_{i})$ for $i \\in \\{1,\\dots,n\\}$\n-   Gamma density: $p(\\theta_{i}) = \\frac{\\lambda_{i}^{k_{i}}}{\\Gamma(k_{i})} \\theta_{i}^{k_{i}-1} \\exp(-\\lambda_{i} \\theta_{i})$ for $\\theta_i > 0$, with shape $k_i > 0$ and rate $\\lambda_i > 0$\n-   Bayesian posterior: $p(\\theta \\,|\\, y) \\propto p(y \\,|\\, \\theta) \\, p(\\theta)$\n-   Log-transformation: $\\phi_{i} = \\ln(\\theta_{i})$ for $i=1, \\dots, n$, so $\\theta = \\exp(\\phi)$ (elementwise)\n-   Objective: Derive the negative log-posterior objective function $J_{\\phi}(\\phi)$ to be minimized with respect to $\\phi$, ignoring additive constants independent of $\\phi$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard Bayesian inference task. The formulation uses a linear forward model with additive Gaussian noise, which is common in many physical sciences. The use of Gamma priors for strictly positive quantities is a standard and well-justified choice in statistical modeling. The reparameterization via a log-transform to convert a constrained optimization problem ($\\theta_i > 0$) into an unconstrained one is a widely used and mathematically sound technique. All definitions are precise, and all necessary information is provided. The problem is scientifically grounded, well-posed, objective, and complete.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the Objective Function\nThe goal is to find the Maximum A Posteriori (MAP) estimate of the parameters in the transformed space, which is equivalent to minimizing the negative log-posterior density. We will derive this objective function, $J_{\\phi}(\\phi)$.\n\nFirst, we establish the posterior probability density of $\\theta$ using Bayes' theorem:\n$$\np(\\theta \\,|\\, y) \\propto p(y \\,|\\, \\theta) p(\\theta)\n$$\nThe likelihood term $p(y \\,|\\, \\theta)$ is given by the multivariate normal distribution $y \\,|\\, \\theta \\sim \\mathcal{N}(G \\theta, \\sigma^{2} I_{m})$. The probability density function is:\n$$\np(y \\,|\\, \\theta) = \\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} (y - G\\theta)^{T}(y - G\\theta)\\right) = \\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\theta\\|_2^2\\right)\n$$\nDropping the normalization constant, which is independent of $\\theta$, we have:\n$$\np(y \\,|\\, \\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\theta\\|_2^2\\right)\n$$\nThe prior term $p(\\theta)$ is given by the product of the independent Gamma distributions for each component $\\theta_{i}$:\n$$\np(\\theta) = \\prod_{i=1}^{n} p(\\theta_{i}) = \\prod_{i=1}^{n} \\frac{\\lambda_{i}^{k_{i}}}{\\Gamma(k_{i})} \\theta_{i}^{k_{i}-1} \\exp(-\\lambda_{i} \\theta_{i})\n$$\nAgain, dropping the normalization constants $\\frac{\\lambda_{i}^{k_{i}}}{\\Gamma(k_{i})}$, we get:\n$$\np(\\theta) \\propto \\left( \\prod_{i=1}^{n} \\theta_{i}^{k_{i}-1} \\right) \\exp\\left(-\\sum_{i=1}^{n} \\lambda_{i} \\theta_{i}\\right)\n$$\nThis can be rewritten as:\n$$\np(\\theta) \\propto \\exp\\left(\\sum_{i=1}^{n} (k_{i}-1)\\ln(\\theta_{i})\\right) \\exp\\left(-\\sum_{i=1}^{n} \\lambda_{i} \\theta_{i}\\right)\n$$\nCombining the likelihood and prior, the posterior for $\\theta$ is:\n$$\np(\\theta \\,|\\, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\theta\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\theta_{i} + \\sum_{i=1}^{n} (k_{i}-1)\\ln(\\theta_{i})\\right)\n$$\nNext, we perform the change of variables from $\\theta$ to $\\phi$ where $\\phi_{i} = \\ln(\\theta_{i})$, or $\\theta_{i} = \\exp(\\phi_{i})$. The probability density functions are related by:\n$$\np_{\\phi}(\\phi \\,|\\, y) = p_{\\theta}(\\theta(\\phi) \\,|\\, y) \\left| \\det\\left(J_{\\phi \\to \\theta}\\right) \\right|\n$$\nwhere $J_{\\phi \\to \\theta}$ is the Jacobian matrix of the transformation from $\\phi$ to $\\theta$. The elements of this Jacobian are $(J_{\\phi \\to \\theta})_{ij} = \\frac{\\partial \\theta_i}{\\partial \\phi_j}$. Since $\\theta_i = \\exp(\\phi_i)$, the transformation is component-wise, and the Jacobian is a diagonal matrix:\n$$\n\\frac{\\partial \\theta_i}{\\partial \\phi_j} = \\delta_{ij} \\frac{d}{d\\phi_i}(\\exp(\\phi_i)) = \\delta_{ij} \\exp(\\phi_i)\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The Jacobian matrix is:\n$$\nJ_{\\phi \\to \\theta} = \\mathrm{diag}(\\exp(\\phi_1), \\exp(\\phi_2), \\dots, \\exp(\\phi_n))\n$$\nIts determinant is the product of its diagonal elements:\n$$\n\\det(J_{\\phi \\to \\theta}) = \\prod_{i=1}^{n} \\exp(\\phi_i) = \\exp\\left(\\sum_{i=1}^{n} \\phi_i\\right)\n$$\nSince $\\exp(\\phi_i) > 0$, the absolute value is redundant.\n\nNow, we can write the posterior for $\\phi$. We substitute $\\theta = \\exp(\\phi)$ into the expression for $p(\\theta \\,|\\, y)$ and multiply by the Jacobian determinant.\n$$\np_{\\phi}(\\phi \\,|\\, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) + \\sum_{i=1}^{n} (k_{i}-1)\\ln(\\exp(\\phi_{i}))\\right) \\cdot \\exp\\left(\\sum_{i=1}^{n} \\phi_i\\right)\n$$\nUsing $\\ln(\\exp(\\phi_{i})) = \\phi_i$ and combining all the terms in the exponent:\n$$\np_{\\phi}(\\phi \\,|\\, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) + \\sum_{i=1}^{n} (k_{i}-1)\\phi_{i} + \\sum_{i=1}^{n} \\phi_i\\right)\n$$\nSimplifying the sum over the $\\phi_i$ terms:\n$$\n\\sum_{i=1}^{n} (k_{i}-1)\\phi_{i} + \\sum_{i=1}^{n} \\phi_i = \\sum_{i=1}^{n} (k_{i}-1+1)\\phi_{i} = \\sum_{i=1}^{n} k_{i}\\phi_{i}\n$$\nThe posterior for $\\phi$ is thus:\n$$\np_{\\phi}(\\phi \\,|\\, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) + \\sum_{i=1}^{n} k_{i}\\phi_{i}\\right)\n$$\nThe log-posterior is the natural logarithm of this expression:\n$$\n\\ln(p_{\\phi}(\\phi \\,|\\, y)) = C - \\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 - \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) + \\sum_{i=1}^{n} k_{i}\\phi_{i}\n$$\nwhere $C$ is a constant that does not depend on $\\phi$.\n\nThe objective function to be minimized, $J_{\\phi}(\\phi)$, is the negative of the log-posterior. We can ignore the additive constant $C$:\n$$\nJ_{\\phi}(\\phi) = - \\left( \\ln(p_{\\phi}(\\phi \\,|\\, y)) - C \\right)\n$$\n$$\nJ_{\\phi}(\\phi) = \\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 + \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) - \\sum_{i=1}^{n} k_{i}\\phi_{i}\n$$\nThis expression is the final simplified negative log-posterior objective function, expressed entirely in terms of the required variables. The vector $\\exp(\\phi)$ is defined component-wise as $(\\exp(\\phi))_i = \\exp(\\phi_i)$.",
            "answer": "$$ \\boxed{ \\frac{1}{2\\sigma^{2}} \\|y - G\\exp(\\phi)\\|_2^2 + \\sum_{i=1}^{n} \\lambda_{i} \\exp(\\phi_{i}) - \\sum_{i=1}^{n} k_{i}\\phi_{i} } $$"
        },
        {
            "introduction": "Finding a single \"best-fit\" parameter set is only half the story; a complete scientific analysis requires quantifying the uncertainty associated with that estimate. This practice explores the Laplace approximation, a powerful method for characterizing the posterior probability distribution of the parameters as a Gaussian centered at the MAP estimate. By approximating the negative log-posterior with a quadratic function, you will compute the posterior covariance matrix from its Hessian, providing the \"error bars\" and correlations for your retrieved parameters. This skill  is essential for interpreting the reliability of inversion results and understanding parameter trade-offs.",
            "id": "3831132",
            "problem": "A multispectral satellite sensor is used to infer two nondimensionalized land-surface parameters, $x \\in \\mathbb{R}^{2}$, representing a normalized leaf biochemistry index and a normalized soil moisture index, from three top-of-canopy reflectance measurements, $y \\in \\mathbb{R}^{3}$. Around the Maximum A Posteriori (MAP) point, the radiative transfer forward model is linearized so that the data misfit is controlled by the Jacobian matrix $J \\in \\mathbb{R}^{3 \\times 2}$ evaluated at the MAP. Assume the measurement noise is zero-mean Gaussian with covariance $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$, and the prior on $x$ is Gaussian with zero mean and covariance $\\Gamma \\in \\mathbb{R}^{2 \\times 2}$. All quantities are nondimensional.\n\nThe matrices are\n$$\nJ \\;=\\; \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix}, \n\\qquad\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\frac{1}{4} & 0 & 0 \\\\\n0 & \\frac{1}{9} & 0 \\\\\n0 & 0 & \\frac{1}{16}\n\\end{pmatrix},\n\\qquad\n\\Gamma \\;=\\; \\begin{pmatrix}\n\\frac{1}{2} & 0 \\\\\n0 & \\frac{1}{3}\n\\end{pmatrix}.\n$$\n\nUsing the standard Gaussian (Laplace) approximation of the posterior around the Maximum A Posteriori (MAP) estimate arising from a second-order Taylor expansion of the negative log-posterior and the Gauss–Newton approximation to the data misfit Hessian, compute the approximate posterior covariance matrix for $x$ at the MAP. Express the final covariance matrix entries exactly as fractions, with no rounding. Since all quantities are nondimensional, do not include units in your answer. The final answer must be a single matrix expression.",
            "solution": "The Bayesian inverse problem for parameter estimation begins with Bayes’ rule, which states that the posterior density is proportional to the product of the likelihood and the prior. For additive zero-mean Gaussian measurement noise with covariance $\\Sigma$ and a Gaussian prior with mean zero and covariance $\\Gamma$, the negative log-posterior, up to an additive constant independent of $x$, is\n$$\n\\Phi(x) \\;=\\; \\frac{1}{2}\\,\\|y - f(x)\\|_{\\Sigma^{-1}}^{2} \\;+\\; \\frac{1}{2}\\,\\|x\\|_{\\Gamma^{-1}}^{2},\n$$\nwhere $f(x)$ is the forward model, and $\\|v\\|_{A}^{2} \\equiv v^{\\top} A v$. Around the Maximum A Posteriori (MAP) point, one linearizes the forward model as $f(x) \\approx f(x_{\\text{MAP}}) + J (x - x_{\\text{MAP}})$, where $J$ is the Jacobian at the MAP. Under the Gauss–Newton approximation, the Hessian of the negative log-posterior at the MAP is approximated by\n$$\nH_{\\text{GN}} \\;\\approx\\; J^{\\top} \\Sigma^{-1} J \\;+\\; \\Gamma^{-1}.\n$$\nThe Laplace (Gaussian) approximation to the posterior around the MAP is then $\\mathcal{N}(x_{\\text{MAP}}, \\Gamma_{\\text{post}})$ with covariance\n$$\n\\Gamma_{\\text{post}} \\;\\approx\\; H_{\\text{GN}}^{-1} \\;=\\; \\left(J^{\\top} \\Sigma^{-1} J \\;+\\; \\Gamma^{-1}\\right)^{-1}.\n$$\n\nWe now compute $J^{\\top} \\Sigma^{-1} J$ and $\\Gamma^{-1}$ using the provided matrices. First, compute $\\Sigma^{-1}$:\n$$\n\\Sigma \\;=\\; \\operatorname{diag}\\!\\left(\\frac{1}{4},\\, \\frac{1}{9},\\, \\frac{1}{16}\\right)\n\\;\\;\\Longrightarrow\\;\\;\n\\Sigma^{-1} \\;=\\; \\operatorname{diag}\\!\\left(4,\\, 9,\\, 16\\right).\n$$\nWrite the rows of $J$ as\n$$\nr_{1} \\;=\\; \\begin{pmatrix} 1 & 0 \\end{pmatrix}, \n\\quad\nr_{2} \\;=\\; \\begin{pmatrix} 0 & 1 \\end{pmatrix},\n\\quad\nr_{3} \\;=\\; \\begin{pmatrix} 1 & 1 \\end{pmatrix}.\n$$\nThen\n$$\nJ^{\\top} \\Sigma^{-1} J \n\\;=\\; \\sum_{i=1}^{3} w_{i}\\, r_{i}^{\\top} r_{i}\n\\quad\\text{with}\\quad\n(w_{1}, w_{2}, w_{3}) \\;=\\; (4, 9, 16).\n$$\nCompute each term:\n$$\n4\\, r_{1}^{\\top} r_{1} \\;=\\; 4 \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 4 & 0 \\\\ 0 & 0 \\end{pmatrix},\n$$\n$$\n9\\, r_{2}^{\\top} r_{2} \\;=\\; 9 \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0 & 0 \\\\ 0 & 9 \\end{pmatrix},\n$$\n$$\n16\\, r_{3}^{\\top} r_{3} \\;=\\; 16 \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 16 & 16 \\\\ 16 & 16 \\end{pmatrix}.\n$$\nSumming yields\n$$\nJ^{\\top} \\Sigma^{-1} J \n\\;=\\; \\begin{pmatrix} 4 & 0 \\\\ 0 & 0 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} 0 & 0 \\\\ 0 & 9 \\end{pmatrix}\n\\;+\\; \\begin{pmatrix} 16 & 16 \\\\ 16 & 16 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 20 & 16 \\\\ 16 & 25 \\end{pmatrix}.\n$$\n\nNext, compute $\\Gamma^{-1}$. Given\n$$\n\\Gamma \\;=\\; \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{3} \\end{pmatrix},\n$$\nwe have\n$$\n\\Gamma^{-1} \\;=\\; \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}.\n$$\nTherefore,\n$$\nH_{\\text{GN}} \\;=\\; J^{\\top} \\Sigma^{-1} J \\;+\\; \\Gamma^{-1}\n\\;=\\; \\begin{pmatrix} 20 & 16 \\\\ 16 & 25 \\end{pmatrix} \\;+\\; \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 22 & 16 \\\\ 16 & 28 \\end{pmatrix}.\n$$\n\nFinally, invert the $2 \\times 2$ matrix to obtain $\\Gamma_{\\text{post}}$:\n$$\n\\Gamma_{\\text{post}} \\;\\approx\\; H_{\\text{GN}}^{-1}\n\\;=\\; \\left(\\begin{pmatrix} 22 & 16 \\\\ 16 & 28 \\end{pmatrix}\\right)^{-1}.\n$$\nFor a symmetric $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}$, the inverse is $\\frac{1}{ad - b^{2}} \\begin{pmatrix} d & -b \\\\ -b & a \\end{pmatrix}$. Here, $a \\,=\\, 22$, $b \\,=\\, 16$, $d \\,=\\, 28$, so\n$$\n\\det(H_{\\text{GN}}) \\;=\\; 22 \\cdot 28 \\;-\\; 16^{2} \\;=\\; 616 \\;-\\; 256 \\;=\\; 360.\n$$\nThus,\n$$\n\\Gamma_{\\text{post}} \\;\\approx\\; \\frac{1}{360} \\begin{pmatrix} 28 & -16 \\\\ -16 & 22 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{28}{360} & -\\frac{16}{360} \\\\ -\\frac{16}{360} & \\frac{22}{360} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{7}{90} & -\\frac{2}{45} \\\\ -\\frac{2}{45} & \\frac{11}{180} \\end{pmatrix}.\n$$\n\nThis is the exact fractional form of the approximate posterior covariance matrix under the Laplace approximation around the Maximum A Posteriori (MAP) estimate.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{7}{90} & -\\frac{2}{45} \\\\ -\\frac{2}{45} & \\frac{11}{180} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The quality of a parameter retrieval depends critically on the information content of the measurements. This capstone exercise moves beyond analyzing existing data to proactively designing experiments that yield the most information. This problem introduces optimal experimental design using the D-optimality criterion, which seeks to maximize the determinant of the Bayesian Fisher Information Matrix (FIM). In this practical coding exercise , you will implement an algorithm to select the best subset of sensor channels from a candidate pool, connecting the theoretical concepts of information and uncertainty to the challenge of designing more effective remote sensing systems.",
            "id": "3831124",
            "problem": "You must design and implement a complete, runnable program that, given a pool of candidate remote sensing channels (sensors) measuring an atmospheric column, selects the sensor subset that maximizes a scalar information measure for a Bayesian retrieval of a $2$-parameter state vector. The retrieval parameters are denoted by $\\boldsymbol{\\theta} = [\\theta_1,\\theta_2]^\\top \\in \\mathbb{R}^2$. Each sensor $i$ produces a scalar radiance-like measurement modeled by a differentiable forward function $f_i(\\boldsymbol{\\theta})$ corrupted by zero-mean Gaussian noise with known variance $\\sigma_i^2$. Assume sensor noises are independent across sensors. You are given a Gaussian prior for $\\boldsymbol{\\theta}$ with covariance matrix $\\mathbf{S}_a \\in \\mathbb{R}^{2 \\times 2}$ that is symmetric positive definite. The goal is to evaluate a D-optimal information criterion based on the Fisher Information Matrix and choose the subset of sensors maximizing this criterion.\n\nFundamental base and definitions:\n- Under a Gaussian likelihood with independent noise and a differentiable forward model, the Fisher Information Matrix (FIM) for a subset $\\mathcal{S}$ of sensors at a parameter value $\\boldsymbol{\\theta}$ equals\n$$\n\\mathbf{F}_{\\mathcal{S}}(\\boldsymbol{\\theta}) \\;=\\; \\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta})^\\top \\mathbf{R}_{\\mathcal{S}}^{-1} \\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta}) \\;+\\; \\mathbf{S}_a^{-1},\n$$\nwhere $\\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{|\\mathcal{S}| \\times 2}$ is the Jacobian of the forward model restricted to the selected sensors, and $\\mathbf{R}_{\\mathcal{S}} \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{S}|}$ is the diagonal noise covariance for the selected sensors with diagonal entries $\\sigma_i^2$.\n- The D-optimal scalar information measure at $\\boldsymbol{\\theta}$ is defined as\n$$\nI_{\\mathcal{S}}(\\boldsymbol{\\theta}) \\;=\\; \\log \\det \\big( \\mathbf{F}_{\\mathcal{S}}(\\boldsymbol{\\theta}) \\big).\n$$\n- To represent variability over plausible atmospheric states, the design criterion is the discrete average over a grid $\\{\\boldsymbol{\\theta}^{(m)}\\}_{m=1}^M$:\n$$\n\\bar{I}_{\\mathcal{S}} \\;=\\; \\frac{1}{M} \\sum_{m=1}^M \\log \\det \\big( \\mathbf{F}_{\\mathcal{S}}(\\boldsymbol{\\theta}^{(m)}) \\big).\n$$\n\nForward model and Jacobian:\n- For each sensor $i \\in \\{0,1,2,3,4,5\\}$, the forward model is\n$$\nf_i(\\boldsymbol{\\theta}) \\;=\\; c_{i1}\\,\\exp\\!\\big(-\\lambda_i \\theta_1\\big) \\;+\\; \\frac{c_{i2}}{1 + \\gamma_i \\theta_2} \\;+\\; \\delta_i\\,\\theta_1\\,\\theta_2,\n$$\nwith constants $\\lambda_i, c_{i1}, c_{i2}, \\gamma_i, \\delta_i$ provided below. The gradient with respect to $\\boldsymbol{\\theta}$ is\n$$\n\\nabla_{\\boldsymbol{\\theta}} f_i(\\boldsymbol{\\theta}) \\;=\\;\n\\begin{bmatrix}\n-\\;c_{i1}\\,\\lambda_i\\,\\exp\\!\\big(-\\lambda_i \\theta_1\\big) \\;+\\; \\delta_i \\,\\theta_2 \\\\\n-\\;\\dfrac{c_{i2}\\,\\gamma_i}{\\big(1 + \\gamma_i \\theta_2\\big)^2} \\;+\\; \\delta_i \\,\\theta_1\n\\end{bmatrix}.\n$$\n- Let $\\mathbf{g}_i(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} f_i(\\boldsymbol{\\theta}) \\in \\mathbb{R}^2$. For a subset $\\mathcal{S}$, one may compute the contribution of sensor $i \\in \\mathcal{S}$ to the FIM as $(1/\\sigma_i^2)\\,\\mathbf{g}_i(\\boldsymbol{\\theta})\\,\\mathbf{g}_i(\\boldsymbol{\\theta})^\\top$ and sum these, then add $\\mathbf{S}_a^{-1}$.\n\nCandidate sensor pool:\n- The pool contains $N = 6$ sensors indexed by $i \\in \\{0,1,2,3,4,5\\}$ with parameters\n$\n\\lambda = [\\,0.2,\\,0.5,\\,1.0,\\,1.5,\\,2.0,\\,2.5\\,],\n$\n$\nc_1 = [\\,1.0,\\,0.8,\\,1.2,\\,0.9,\\,0.7,\\,1.1\\,],\n$\n$\nc_2 = [\\,0.5,\\,0.6,\\,0.4,\\,0.7,\\,0.9,\\,0.3\\,],\n$\n$\n\\gamma = [\\,0.3,\\,0.4,\\,0.6,\\,0.8,\\,1.0,\\,1.2\\,],\n$\n$\n\\delta = [\\,0.02,\\,0.01,\\,0.03,\\,0.015,\\,0.025,\\,0.02\\,].\n$\n- The sensor noise standard deviations are\n$\n\\sigma = [\\,0.02,\\,0.03,\\,0.015,\\,0.025,\\,0.04,\\,0.03\\,],\n$\nso that the noise variances are $\\sigma_i^2$.\n\nOptimization task:\n- For each test case below, you must select a subset $\\mathcal{S}$ of size $K$ (exactly $K$ sensors) that maximizes $\\bar{I}_{\\mathcal{S}}$.\n- If multiple subsets achieve the same maximal value up to numerical tolerance, break ties by selecting the subset with the lexicographically smallest ascending list of sensor indices (using $0$-based indexing).\n- Use the exact formulas above without numerical approximations other than standard floating-point arithmetic.\n\nTest suite:\n- Case $1$ (happy path):\n  - $K = 3$.\n  - Prior covariance $\\mathbf{S}_a = \\mathrm{diag}([\\,0.5^2,\\,0.5^2\\,]) = \\mathrm{diag}([\\,0.25,\\,0.25\\,])$.\n  - State grid $\\{\\boldsymbol{\\theta}^{(m)}\\}$ with $M=2$ points: $\\boldsymbol{\\theta}^{(1)} = [\\,0.5,\\,0.8\\,]^\\top$, $\\boldsymbol{\\theta}^{(2)} = [\\,1.0,\\,0.2\\,]^\\top$.\n- Case $2$ (boundary with single sensor):\n  - $K = 1$.\n  - Prior covariance $\\mathbf{S}_a = \\mathrm{diag}([\\,1.0^2,\\,1.0^2\\,]) = \\mathrm{diag}([\\,1.0,\\,1.0\\,])$.\n  - State grid with $M=1$ point: $\\boldsymbol{\\theta}^{(1)} = [\\,0.7,\\,0.7\\,]^\\top$.\n- Case $3$ (strong prior, multiple states):\n  - $K = 2$.\n  - Prior covariance $\\mathbf{S}_a = \\mathrm{diag}([\\,0.1^2,\\,0.1^2\\,]) = \\mathrm{diag}([\\,0.01,\\,0.01\\,])$.\n  - State grid with $M=3$ points: $\\boldsymbol{\\theta}^{(1)} = [\\,0.2,\\,0.2\\,]^\\top$, $\\boldsymbol{\\theta}^{(2)} = [\\,0.4,\\,1.2\\,]^\\top$, $\\boldsymbol{\\theta}^{(3)} = [\\,0.9,\\,0.1\\,]^\\top$.\n- Case $4$ (trivial full selection):\n  - $K = 6$.\n  - Prior covariance $\\mathbf{S}_a = \\mathrm{diag}([\\,0.3^2,\\,0.7^2\\,]) = \\mathrm{diag}([\\,0.09,\\,0.49\\,])$.\n  - State grid with $M=2$ points: $\\boldsymbol{\\theta}^{(1)} = [\\,0.3,\\,0.6\\,]^\\top$, $\\boldsymbol{\\theta}^{(2)} = [\\,1.1,\\,0.4\\,]^\\top$.\n\nRequired output:\n- For each of the $4$ cases above, compute the optimal subset and output the selected sensor indices as an ascending list with $0$-based indexing.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is itself a bracketed list of integers (e.g., \"[0,2,5]\").\n\nAll computations are dimensionless and must be performed in double-precision floating point. No angles or physical units are involved. The program must not read any input and must use the fixed data provided here. The final answers are the index lists as specified, not probabilities or percentages.",
            "solution": "The problem presented is an instance of optimal experimental design, specifically addressing the task of sensor subset selection in the context of a Bayesian parameter estimation framework. We are tasked with selecting an optimal subset of $K$ sensors from a pool of $N=6$ candidates to maximize the information gained about a $2$-parameter state vector $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2]^\\top$. The optimization criterion is based on the D-optimality principle, which seeks to minimize the volume of the posterior parameter uncertainty ellipsoid by maximizing the determinant of the Fisher Information Matrix (FIM).\n\nThe core of the solution is the evaluation of the objective function, $\\bar{I}_{\\mathcal{S}}$, for all possible sensor subsets $\\mathcal{S}$ of size $K$. Given the small size of the sensor pool ($N=6$), an exhaustive search is computationally feasible and guarantees finding the globally optimal subset. The number of subsets of size $K$ to evaluate is given by the binomial coefficient $\\binom{N}{K}$. For the test cases, the maximum number is $\\binom{6}{3} = 20$, which is a trivial computational burden.\n\nThe objective function is defined as the average of a scalar information measure over a discrete grid of $M$ plausible state vectors, $\\{\\boldsymbol{\\theta}^{(m)}\\}_{m=1}^M$:\n$$\n\\bar{I}_{\\mathcal{S}} \\;=\\; \\frac{1}{M} \\sum_{m=1}^M \\log \\det \\big( \\mathbf{F}_{\\mathcal{S}}(\\boldsymbol{\\theta}^{(m)}) \\big)\n$$\nHere, $\\mathbf{F}_{\\mathcal{S}}(\\boldsymbol{\\theta}^{(m)})$ is the Bayesian Fisher Information Matrix for the subset $\\mathcal{S}$ evaluated at the state vector $\\boldsymbol{\\theta}^{(m)}$. This matrix quantifies the total information about $\\boldsymbol{\\theta}$ from both the measurements and the prior.\n\nThe FIM is constructed by summing the information contributions from the likelihood (the measurements) and the prior distribution. For a given state $\\boldsymbol{\\theta}$, it is given by:\n$$\n\\mathbf{F}_{\\mathcal{S}}(\\boldsymbol{\\theta}) \\;=\\; \\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta})^\\top \\mathbf{R}_{\\mathcal{S}}^{-1} \\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta}) \\;+\\; \\mathbf{S}_a^{-1}\n$$\nThe first term, $\\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta})^\\top \\mathbf{R}_{\\mathcal{S}}^{-1} \\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta})$, represents the information from the sensor measurements in subset $\\mathcal{S}$. $\\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta})$ is the $|\\mathcal{S}| \\times 2$ Jacobian matrix of the forward models for the selected sensors, and $\\mathbf{R}_{\\mathcal{S}}$ is the diagonal covariance matrix of the corresponding measurement noises. The second term, $\\mathbf{S}_a^{-1}$, is the inverse of the prior covariance matrix, representing the information available before any measurements are taken.\n\nSince the sensor noises are independent, $\\mathbf{R}_{\\mathcal{S}}$ is diagonal, and the measurement contribution to the FIM can be computed as a sum of rank-one matrices, one for each sensor $i \\in \\mathcal{S}$:\n$$\n\\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta})^\\top \\mathbf{R}_{\\mathcal{S}}^{-1} \\mathbf{J}_{\\mathcal{S}}(\\boldsymbol{\\theta}) \\;=\\; \\sum_{i \\in \\mathcal{S}} \\frac{1}{\\sigma_i^2} \\mathbf{g}_i(\\boldsymbol{\\theta}) \\mathbf{g}_i(\\boldsymbol{\\theta})^\\top\n$$\nwhere $\\mathbf{g}_i(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} f_i(\\boldsymbol{\\theta})$ is the gradient of the forward model for sensor $i$ with respect to the parameters $\\boldsymbol{\\theta}$. This formulation is computationally convenient. The gradient vector for sensor $i$ is given by:\n$$\n\\mathbf{g}_i(\\boldsymbol{\\theta}) \\;=\\; \\nabla_{\\boldsymbol{\\theta}} f_i(\\boldsymbol{\\theta}) \\;=\\;\n\\begin{bmatrix}\n-\\;c_{i1}\\,\\lambda_i\\,\\exp\\!\\big(-\\lambda_i \\theta_1\\big) \\;+\\; \\delta_i \\,\\theta_2 \\\\\n-\\;\\dfrac{c_{i2}\\,\\gamma_i}{\\big(1 + \\gamma_i \\theta_2\\big)^2} \\;+\\; \\delta_i \\,\\theta_1\n\\end{bmatrix}\n$$\n\nThe algorithmic procedure to solve each test case is as follows:\n$1$. For a given test case specified by the subset size $K$, the prior covariance $\\mathbf{S}_a$, and the state grid $\\{\\boldsymbol{\\theta}^{(m)}\\}$, we first compute the constant prior information matrix $\\mathbf{S}_a^{-1}$. Since $\\mathbf{S}_a$ is diagonal, its inverse is simply the diagonal matrix of reciprocal entries.\n\n$2$. We generate all possible subsets of sensor indices $\\{0, 1, 2, 3, 4, 5\\}$ of size $K$. These subsets are generated in lexicographical order to facilitate the specified tie-breaking rule.\n\n$3$. For each candidate subset $\\mathcal{S}$, we calculate the objective function $\\bar{I}_{\\mathcal{S}}$:\n    a. Initialize a running sum for the total log-determinant to $0$.\n    b. For each state vector $\\boldsymbol{\\theta}^{(m)}$ in the given grid:\n        i. Compute the measurement part of the FIM, $\\mathbf{F}_{\\text{sensors}} = \\sum_{i \\in \\mathcal{S}} (1/\\sigma_i^2) \\mathbf{g}_i(\\boldsymbol{\\theta}^{(m)}) \\mathbf{g}_i(\\boldsymbol{\\theta}^{(m)})^\\top$. This involves calculating the gradient $\\mathbf{g}_i$ for each sensor $i \\in \\mathcal{S}$ at $\\boldsymbol{\\theta}^{(m)}$, forming the outer product with itself, scaling by the inverse variance $1/\\sigma_i^2$, and summing up the resulting $2 \\times 2$ matrices.\n        ii. Form the total FIM: $\\mathbf{F}_{\\mathcal{S}}(\\boldsymbol{\\theta}^{(m)}) = \\mathbf{F}_{\\text{sensors}} + \\mathbf{S}_a^{-1}$.\n        iii. Calculate the determinant of this $2 \\times 2$ matrix and its natural logarithm, $\\log(\\det(\\mathbf{F}_{\\mathcal{S}}(\\boldsymbol{\\theta}^{(m)})))$.\n        iv. Add this value to the running sum.\n    c. Divide the total sum by the number of states $M$ to obtain $\\bar{I}_{\\mathcal{S}}$.\n\n$4$. We maintain a record of the maximum objective value found so far and the corresponding subset. As we iterate through the lexicographically ordered subsets, we update the record only if a strictly larger objective value is found. This correctly implements the tie-breaking rule, ensuring that if multiple subsets yield the same maximal information measure, the one with the lexicographically smallest index list is chosen.\n\n$5$. The subset corresponding to the final maximum information value is the solution for the test case. This process is repeated for all four test cases. The implementation uses double-precision floating-point arithmetic as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the sensor selection problem for all four test cases.\n    \"\"\"\n\n    # Candidate sensor pool parameters (N=6 sensors)\n    # Using np.array for vectorized operations where applicable, but primarily for data organization.\n    LAMBDA = np.array([0.2, 0.5, 1.0, 1.5, 2.0, 2.5])\n    C1 = np.array([1.0, 0.8, 1.2, 0.9, 0.7, 1.1])\n    C2 = np.array([0.5, 0.6, 0.4, 0.7, 0.9, 0.3])\n    GAMMA = np.array([0.3, 0.4, 0.6, 0.8, 1.0, 1.2])\n    DELTA = np.array([0.02, 0.01, 0.03, 0.015, 0.025, 0.02])\n    SIGMA = np.array([0.02, 0.03, 0.015, 0.025, 0.04, 0.03])\n    VAR = SIGMA**2\n\n    def calculate_gradient(i, theta):\n        \"\"\"Calculates the gradient vector g_i(theta) for sensor i.\"\"\"\n        theta1, theta2 = theta\n        lmbda_i = LAMBDA[i]\n        c1_i = C1[i]\n        c2_i = C2[i]\n        gamma_i = GAMMA[i]\n        delta_i = DELTA[i]\n\n        g1 = -c1_i * lmbda_i * np.exp(-lmbda_i * theta1) + delta_i * theta2\n        g2 = - (c2_i * gamma_i) / (1 + gamma_i * theta2)**2 + delta_i * theta1\n        return np.array([g1, g2])\n\n    def calculate_average_information(subset, Sa_inv, theta_grid):\n        \"\"\"Calculates the averaged D-optimal information measure for a given subset.\"\"\"\n        total_log_det = 0.0\n        M = len(theta_grid)\n\n        for theta in theta_grid:\n            F_sensors = np.zeros((2, 2))\n            for i in subset:\n                g = calculate_gradient(i, theta)\n                g_outer = np.outer(g, g)\n                F_sensors += g_outer / VAR[i]\n            \n            F_total = F_sensors + Sa_inv\n            det_F = np.linalg.det(F_total)\n            \n            # The determinant of the FIM, being the sum of a PSD and a PD matrix,\n            # must be positive.\n            total_log_det += np.log(det_F)\n\n        return total_log_det / M\n\n    def solve_case(K, Sa_diag, theta_grid):\n        \"\"\"Finds the optimal sensor subset for a given test case.\"\"\"\n        sensor_indices = range(6)\n        all_subsets = combinations(sensor_indices, K)\n        \n        Sa_inv = np.diag(1.0 / Sa_diag)\n        \n        max_I_bar = -np.inf\n        best_subset = None\n        \n        # itertools.combinations produces tuples in lexicographical order.\n        # This, combined with a strict > check, correctly handles the tie-breaking rule.\n        for subset in all_subsets:\n            I_bar = calculate_average_information(subset, Sa_inv, theta_grid)\n            \n            if I_bar > max_I_bar:\n                max_I_bar = I_bar\n                best_subset = subset\n                \n        return list(best_subset)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path)\n        {'K': 3, 'Sa_diag': np.array([0.5**2, 0.5**2]), 'theta_grid': [np.array([0.5, 0.8]), np.array([1.0, 0.2])]},\n        # Case 2 (boundary with single sensor)\n        {'K': 1, 'Sa_diag': np.array([1.0**2, 1.0**2]), 'theta_grid': [np.array([0.7, 0.7])]},\n        # Case 3 (strong prior, multiple states)\n        {'K': 2, 'Sa_diag': np.array([0.1**2, 0.1**2]), 'theta_grid': [np.array([0.2, 0.2]), np.array([0.4, 1.2]), np.array([0.9, 0.1])]},\n        # Case 4 (trivial full selection)\n        {'K': 6, 'Sa_diag': np.array([0.3**2, 0.7**2]), 'theta_grid': [np.array([0.3, 0.6]), np.array([1.1, 0.4])]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case['K'], case['Sa_diag'], case['theta_grid'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    result_strings = [f\"[{','.join(map(str, r))}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}