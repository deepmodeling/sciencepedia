{
    "hands_on_practices": [
        {
            "introduction": "This first practice will ground your understanding in the fundamental calculation and interpretation of the Kappa coefficient. Using a typical confusion matrix from a binary land-cover change analysis, you will compute the Kappa statistic, $\\kappa$, from first principles. This exercise is designed to demonstrate how $\\kappa$ provides a more robust assessment of classifier performance than overall accuracy, particularly in the presence of the class imbalance that often characterizes remote sensing data. ",
            "id": "3796000",
            "problem": "A change detection study in a tropical forest region uses a bi-temporal satellite data stack to map binary land-cover change ($1$ for change, $0$ for no-change). A validation campaign provides a reference map over the same area. The algorithm’s predictions are cross-tabulated against the reference labels, yielding the following confusion counts: predicted change correctly matched to reference change equal to $150$, predicted change mismatched to reference no-change equal to $70$, predicted no-change mismatched to reference change equal to $30$, and predicted no-change correctly matched to reference no-change equal to $750$. Assume the $4$ cells and their row and column totals fully define the joint and marginal distributions of prediction and reference.\n\nStarting from the fundamental definition that agreement beyond chance is quantified by comparing the observed agreement to the agreement expected if prediction and reference were statistically independent given their marginal distributions, derive an explicit expression for the agreement statistic commonly used for nominal categories in terms of the four confusion counts and the total sample size, and compute its value for the given data. Then, using the same counts, compute the detection rate for the change class and the false alarm rate for the no-change class, and briefly explain how the agreement statistic reflects these quantities in the presence of class imbalance.\n\nProvide only the final numeric value of the agreement statistic as your answer, rounded to four significant figures. Express it as a unitless decimal.",
            "solution": "The problem requires the derivation and calculation of a common agreement statistic for nominal categories, its interpretation in the context of class imbalance, and the calculation of related performance metrics. The description of the statistic as quantifying \"agreement beyond chance\" by comparing observed agreement to the agreement expected under statistical independence identifies it as Cohen's Kappa coefficient, denoted by $\\kappa$.\n\nFirst, let us formalize the problem by constructing a confusion matrix. Let the two classes be \"change\" (class $1$) and \"no-change\" (class $0$). We can arrange the given counts into a $2 \\times 2$ matrix where the rows represent the reference data and the columns represent the algorithm's predictions.\n\nLet $x_{ij}$ be the count of samples where the reference class is $i$ and the predicted class is $j$.\n-   $x_{11}$: Reference is change, predicted is change (True Positive, $TP$). Given as $150$.\n-   $x_{01}$: Reference is no-change, predicted is change (False Positive, $FP$). Given as $70$.\n-   $x_{10}$: Reference is change, predicted is no-change (False Negative, $FN$). Given as $30$.\n-   $x_{00}$: Reference is no-change, predicted is no-change (True Negative, $TN$). Given as $750$.\n\nThe confusion matrix is:\n$$\n\\begin{array}{c|cc|c}\n\\text{ }  \\text{Predicted Change}  \\text{Predicted No-Change}  \\text{Total} \\\\\n\\hline\n\\text{Reference Change}  x_{11} = 150  x_{10} = 30  x_{1+} = 180 \\\\\n\\text{Reference No-Change}  x_{01} = 70  x_{00} = 750  x_{0+} = 820 \\\\\n\\hline\n\\text{Total}  x_{+1} = 220  x_{+0} = 780  N = 1000\n\\end{array}\n$$\nThe total number of samples is $N = x_{11} + x_{10} + x_{01} + x_{00} = 150 + 30 + 70 + 750 = 1000$.\nThe row totals are $x_{1+} = x_{11} + x_{10}$ and $x_{0+} = x_{01} + x_{00}$.\nThe column totals are $x_{+1} = x_{11} + x_{01}$ and $x_{+0} = x_{10} + x_{00}$.\n\nThe kappa statistic, $\\kappa$, is defined as:\n$$\n\\kappa = \\frac{P_o - P_e}{1 - P_e}\n$$\nwhere $P_o$ is the observed proportional agreement and $P_e$ is the expected proportional agreement under the hypothesis of statistical independence between the prediction and the reference.\n\nThe observed agreement, $P_o$, is the proportion of samples that were correctly classified. This is the sum of the diagonal elements of the confusion matrix divided by the total number of samples.\n$$\nP_o = \\frac{x_{11} + x_{00}}{N}\n$$\nThe expected agreement, $P_e$, is the probability that the prediction and reference agree by chance. This is the sum of the probabilities of chance agreement for each class. The probability of chance agreement for a class is the product of its marginal probabilities (the proportion of samples in that class for the reference and the prediction, respectively).\n$$\nP_e = P(\\text{agree on change}) + P(\\text{agree on no-change})\n$$\n$$\nP_e = P(\\text{ref=change})P(\\text{pred=change}) + P(\\text{ref=no-change})P(\\text{pred=no-change})\n$$\nIn terms of the counts:\n$$\nP_e = \\left(\\frac{x_{1+}}{N}\\right) \\left(\\frac{x_{+1}}{N}\\right) + \\left(\\frac{x_{0+}}{N}\\right) \\left(\\frac{x_{+0}}{N}\\right) = \\frac{x_{1+}x_{+1} + x_{0+}x_{+0}}{N^2}\n$$\nSubstituting the expressions for $P_o$ and $P_e$ into the formula for $\\kappa$, we obtain the explicit expression for the agreement statistic in terms of the four confusion counts ($x_{11}, x_{10}, x_{01}, x_{00}$) and the total sample size ($N$):\n$$\n\\kappa = \\frac{\\frac{x_{11} + x_{00}}{N} - \\frac{(x_{11}+x_{10})(x_{11}+x_{01}) + (x_{01}+x_{00})(x_{10}+x_{00})}{N^2}}{1 - \\frac{(x_{11}+x_{10})(x_{11}+x_{01}) + (x_{01}+x_{00})(x_{10}+x_{00})}{N^2}}\n$$\nThis can be simplified by multiplying the numerator and denominator by $N^2$:\n$$\n\\kappa = \\frac{N(x_{11} + x_{00}) - \\left( (x_{11}+x_{10})(x_{11}+x_{01}) + (x_{01}+x_{00})(x_{10}+x_{00}) \\right)}{N^2 - \\left( (x_{11}+x_{10})(x_{11}+x_{01}) + (x_{01}+x_{00})(x_{10}+x_{00}) \\right)}\n$$\nNow, we compute its value for the given data.\nThe total number of samples is $N = 1000$.\nThe observed agreement is:\n$$\nP_o = \\frac{150 + 750}{1000} = \\frac{900}{1000} = 0.9\n$$\nThe marginal totals are:\n-   Reference change total, $x_{1+} = 150 + 30 = 180$.\n-   Reference no-change total, $x_{0+} = 70 + 750 = 820$.\n-   Predicted change total, $x_{+1} = 150 + 70 = 220$.\n-   Predicted no-change total, $x_{+0} = 30 + 750 = 780$.\n\nThe expected agreement is:\n$$\nP_e = \\frac{(180)(220) + (820)(780)}{(1000)^2} = \\frac{39600 + 639600}{1000000} = \\frac{679200}{1000000} = 0.6792\n$$\nFinally, we compute the kappa statistic:\n$$\n\\kappa = \\frac{P_o - P_e}{1 - P_e} = \\frac{0.9 - 0.6792}{1 - 0.6792} = \\frac{0.2208}{0.3208} \\approx 0.6882793017...\n$$\nRounding to four significant figures, we get $\\kappa \\approx 0.6883$.\n\nNext, we compute the detection rate for the change class and the false alarm rate for the no-change class.\nThe detection rate for the change class is the True Positive Rate (TPR), which is the proportion of actual change instances that were correctly identified.\n$$\n\\text{Detection Rate (TPR)} = \\frac{x_{11}}{x_{11} + x_{10}} = \\frac{150}{150 + 30} = \\frac{150}{180} = \\frac{5}{6} \\approx 0.8333\n$$\nThe false alarm rate is the False Positive Rate (FPR), which is the proportion of actual no-change instances that were incorrectly classified as change.\n$$\n\\text{False Alarm Rate (FPR)} = \\frac{x_{01}}{x_{01} + x_{00}} = \\frac{70}{70 + 750} = \\frac{70}{820} = \\frac{7}{82} \\approx 0.0854\n$$\nThe problem exhibits significant class imbalance, with many more no-change samples ($820$) than change samples ($180$). In such cases, a high overall accuracy ($P_o = 0.9$) can be misleading, as it is heavily influenced by the correct classification of the majority class ($TN=750$). The kappa statistic provides a more robust measure of performance because it corrects for the agreement that would be expected by chance alone. This chance agreement, $P_e$, is substantially high ($0.6792$) precisely because of the class imbalance; a random classifier preserving the marginal totals would agree on the \"no-change\" class most of the time. By subtracting $P_e$, kappa isolates the classifier's performance beyond this baseline. A high $\\kappa$ value, such as the one calculated, indicates that the classifier's performance is not simply an artifact of correctly classifying the majority class. It reflects a balanced performance across classes, simultaneously achieving a high detection rate for the minority \"change\" class ($TPR \\approx 0.83$) and a low false alarm rate ($FPR \\approx 0.09$), thus capturing both aspects of \"good\" classification in a single metric.",
            "answer": "$$\\boxed{0.6883}$$"
        },
        {
            "introduction": "This exercise explores one of the most discussed nuances of the Kappa statistic, often termed the 'prevalence paradox.' By analyzing two classifiers that achieve identical observed agreement ($p_o$) but operate in regions with vastly different class prevalences, you will discover how their $\\kappa$ values diverge significantly. This practice is crucial for moving beyond a superficial interpretation of $\\kappa$ and appreciating its sensitivity to the marginal distributions of your data. ",
            "id": "3795970",
            "problem": "A satellite-based land-cover classifier produces binary maps indicating the presence ($1$) or absence ($0$) of wetlands across two ecologically distinct regions, denoted Region $A$ and Region $B$. For each region, a stratified random sample of $N=200$ pixels is evaluated against independent field observations judged to be the ground truth. The analyst reports the following confusion-matrix counts for the classifier versus ground truth:\n\n- Region $A$: true positives $TP=90$, false positives $FP=10$, true negatives $TN=90$, false negatives $FN=10$.\n- Region $B$: true positives $TP=10$, false positives $FP=10$, true negatives $TN=170$, false negatives $FN=10$.\n\nAssume that observed agreement is the fraction of correct classifications and that chance agreement is computed under the assumption that classifier predictions and ground-truth labels are independent given their empirical marginal class prevalences in each region. Starting from these definitions, and without invoking any pre-stated formula, derive the agreement coefficient known as Cohen’s kappa coefficient for each region. Then, compute the difference in kappa values between Region $A$ and Region $B$, defined as $\\Delta \\kappa = \\kappa_{A} - \\kappa_{B}$.\n\nProvide the final value of $\\Delta \\kappa$ as an exact fraction (do not round). No units are required.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-contained. The data provided are consistent and sufficient for a unique solution. The task requires deriving the Cohen's kappa coefficient from first principles and then applying it to two given datasets, which is a standard and meaningful problem in statistical analysis and remote sensing.\n\nThe Cohen's kappa coefficient, $\\kappa$, is a statistic that measures inter-rater agreement for categorical items, correcting for agreement that is expected by chance. It is defined as:\n$$ \\kappa = \\frac{p_o - p_e}{1 - p_e} $$\nwhere $p_o$ is the empirically observed agreement, and $p_e$ is the hypothetical probability of chance agreement. We will derive the expressions for $p_o$ and $p_e$ from the given definitions and the confusion matrix.\n\nA confusion matrix for a binary classification problem is a $2 \\times 2$ table that summarizes the performance of a classifier against a ground truth. The elements of the matrix are:\n- $TP$: True Positives (classifier says $1$, ground truth is $1$)\n- $FP$: False Positives (classifier says $1$, ground truth is $0$)\n- $TN$: True Negatives (classifier says $0$, ground truth is $0$)\n- $FN$: False Negatives (classifier says $0$, ground truth is $1$)\n\nThe total number of samples is $N = TP + FP + TN + FN$.\n\nAs per the problem statement, the observed agreement, $p_o$, is \"the fraction of correct classifications.\" Correct classifications occur when the classifier's output matches the ground truth, corresponding to the $TP$ and $TN$ cells. Thus, the proportion of observed agreement is:\n$$ p_o = \\frac{TP + TN}{N} $$\n\nThe chance agreement, $p_e$, is computed \"under the assumption that classifier predictions and ground-truth labels are independent given their empirical marginal class prevalences.\" To calculate this, we first determine the marginal totals from the confusion matrix.\nLet the rows represent the ground truth (GT) and the columns represent the classifier's prediction (P).\n\n| | P = 1 | P = 0 | Total |\n| :--- | :---: | :---: | :---: |\n| GT = 1 | TP | FN | $TP+FN$ |\n| GT = 0 | FP | TN | $FP+TN$ |\n| Total | $TP+FP$ | $FN+TN$ | $N$ |\n\nThe empirical probability that the ground truth is positive (class $1$) is $p_{GT=1} = \\frac{TP + FN}{N}$.\nThe empirical probability that the ground truth is negative (class $0$) is $p_{GT=0} = \\frac{FP + TN}{N}$.\nThe empirical probability that the classifier predicts positive (class $1$) is $p_{P=1} = \\frac{TP + FP}{N}$.\nThe empirical probability that the classifier predicts negative (class $0$) is $p_{P=0} = \\frac{FN + TN}{N}$.\n\nUnder the independence assumption, the probability of both the ground truth and the classifier labeling a sample as positive by chance is the product of their respective marginal probabilities: $p_{\\text{chance agree positive}} = p_{GT=1} \\times p_{P=1}$.\nSimilarly, the probability of both agreeing on a negative label by chance is $p_{\\text{chance agree negative}} = p_{GT=0} \\times p_{P=0}$.\n\nThe total probability of chance agreement, $p_e$, is the sum of these two probabilities:\n$$ p_e = (p_{GT=1} \\times p_{P=1}) + (p_{GT=0} \\times p_{P=0}) $$\n$$ p_e = \\left(\\frac{TP + FN}{N}\\right)\\left(\\frac{TP + FP}{N}\\right) + \\left(\\frac{FP + TN}{N}\\right)\\left(\\frac{FN + TN}{N}\\right) $$\n\nNow we apply these derived formulas to each region.\n\nFor Region $A$:\nThe given counts are $TP_A=90$, $FP_A=10$, $TN_A=90$, $FN_A=10$.\nThe total number of samples is $N_A = 90 + 10 + 90 + 10 = 200$.\n\nThe observed agreement for Region $A$ is:\n$$ p_{o,A} = \\frac{TP_A + TN_A}{N_A} = \\frac{90 + 90}{200} = \\frac{180}{200} = \\frac{9}{10} $$\n\nThe marginal totals for Region $A$ are:\n- Ground truth positive: $TP_A + FN_A = 90 + 10 = 100$\n- Ground truth negative: $FP_A + TN_A = 10 + 90 = 100$\n- Classifier prediction positive: $TP_A + FP_A = 90 + 10 = 100$\n- Classifier prediction negative: $FN_A + TN_A = 10 + 90 = 100$\n\nThe chance agreement for Region $A$ is:\n$$ p_{e,A} = \\left(\\frac{100}{200}\\right)\\left(\\frac{100}{200}\\right) + \\left(\\frac{100}{200}\\right)\\left(\\frac{100}{200}\\right) = \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n\nThe kappa coefficient for Region $A$ is:\n$$ \\kappa_A = \\frac{p_{o,A} - p_{e,A}}{1 - p_{e,A}} = \\frac{\\frac{9}{10} - \\frac{1}{2}}{1 - \\frac{1}{2}} = \\frac{\\frac{9-5}{10}}{\\frac{1}{2}} = \\frac{\\frac{4}{10}}{\\frac{1}{2}} = \\frac{4}{10} \\times 2 = \\frac{8}{10} = \\frac{4}{5} $$\n\nFor Region $B$:\nThe given counts are $TP_B=10$, $FP_B=10$, $TN_B=170$, $FN_B=10$.\nThe total number of samples is $N_B = 10 + 10 + 170 + 10 = 200$.\n\nThe observed agreement for Region $B$ is:\n$$ p_{o,B} = \\frac{TP_B + TN_B}{N_B} = \\frac{10 + 170}{200} = \\frac{180}{200} = \\frac{9}{10} $$\n\nThe marginal totals for Region $B$ are:\n- Ground truth positive: $TP_B + FN_B = 10 + 10 = 20$\n- Ground truth negative: $FP_B + TN_B = 10 + 170 = 180$\n- Classifier prediction positive: $TP_B + FP_B = 10 + 10 = 20$\n- Classifier prediction negative: $FN_B + TN_B = 10 + 170 = 180$\n\nThe chance agreement for Region $B$ is:\n$$ p_{e,B} = \\left(\\frac{20}{200}\\right)\\left(\\frac{20}{200}\\right) + \\left(\\frac{180}{200}\\right)\\left(\\frac{180}{200}\\right) = \\left(\\frac{1}{10}\\right)\\left(\\frac{1}{10}\\right) + \\left(\\frac{9}{10}\\right)\\left(\\frac{9}{10}\\right) = \\frac{1}{100} + \\frac{81}{100} = \\frac{82}{100} = \\frac{41}{50} $$\n\nThe kappa coefficient for Region $B$ is:\n$$ \\kappa_B = \\frac{p_{o,B} - p_{e,B}}{1 - p_{e,B}} = \\frac{\\frac{9}{10} - \\frac{41}{50}}{1 - \\frac{41}{50}} = \\frac{\\frac{45-41}{50}}{\\frac{50-41}{50}} = \\frac{\\frac{4}{50}}{\\frac{9}{50}} = \\frac{4}{9} $$\n\nFinally, we compute the difference $\\Delta \\kappa = \\kappa_A - \\kappa_B$:\n$$ \\Delta \\kappa = \\frac{4}{5} - \\frac{4}{9} = \\frac{4 \\times 9}{5 \\times 9} - \\frac{4 \\times 5}{9 \\times 5} = \\frac{36}{45} - \\frac{20}{45} = \\frac{16}{45} $$\nThis result highlights that while both regions have the same overall accuracy ($p_o = \\frac{9}{10}$), the agreement corrected for chance ($\\kappa$) is substantially different due to the highly imbalanced class distribution in Region $B$.",
            "answer": "$$ \\boxed{\\frac{16}{45}} $$"
        },
        {
            "introduction": "Bridging the gap from statistical analysis to practical model application, this hands-on exercise challenges you to use the Kappa coefficient as an optimization metric. Given a set of probabilistic predictions from a deforestation classifier, you will implement an algorithm to find the decision threshold that maximizes $\\kappa$. This computational task mirrors a critical step in deploying classification models and reinforces the role of $\\kappa$ as a powerful tool for objective model tuning. ",
            "id": "3795987",
            "problem": "You are given a binary deforestation probability map produced by a remote sensing classifier and corresponding reference labels based on field-validated deforestation status for $N=200$ pixels. The classification decision is made by thresholding predicted probabilities: a pixel is predicted as deforestation if its predicted probability is greater than or equal to a threshold and as non-deforestation otherwise. The goal is to determine the threshold that maximizes chance-corrected agreement as quantified by Cohen's Kappa coefficient (CKC), using only the foundational definitions of observed agreement and expected agreement under independent assignment.\n\nStarting from the foundational base that observed agreement is the proportion of identical labels between prediction and reference, and expected agreement assumes independent assignments determined solely by the marginal proportions of predicted and reference classes, execute the following tasks:\n- For each provided test case, sweep all unique predicted probability values as candidate thresholds.\n- At each threshold, compute the observed agreement $p_o$ and the expected agreement $p_e$ for the induced binary classifications.\n- Use these to compute the chance-corrected agreement coefficient $\\kappa$ at each threshold and select the threshold that maximizes $\\kappa$. In case multiple thresholds achieve the same maximum $\\kappa$, select the smallest threshold among them.\n- For all computations, the decision rule must be: predict deforestation if predicted probability $\\ge$ threshold, and predict non-deforestation otherwise.\n- Express all returned floating-point values as decimals (do not use a percentage sign) rounded to six decimal places.\n\nThe test suite contains three deterministic scenarios, each with $N=200$ pixels:\n1. Balanced reference prevalence with moderate separability:\n   - Reference labels: the first $100$ pixels are non-deforestation ($0$), the next $100$ are deforestation ($1$).\n   - Predicted probabilities: for non-deforestation pixels indexed $j=0,\\dots,99$, $p=0.4+0.002j$; for deforestation pixels indexed $j=0,\\dots,99$, $p=0.5+0.002j$.\n2. Imbalanced reference prevalence with overlapping probabilities:\n   - Reference labels: the first $170$ pixels are non-deforestation ($0$), the next $30$ are deforestation ($1$).\n   - Predicted probabilities: for non-deforestation pixels indexed $j=0,\\dots,169$, $p=0.2+0.003(j \\bmod 60)$; for deforestation pixels indexed $j=0,\\dots,29$, $p=0.28+0.005j$.\n3. Degenerate case with uniform predicted probabilities:\n   - Reference labels: the first $100$ pixels are non-deforestation ($0$), the next $100$ are deforestation ($1$).\n   - Predicted probabilities: all pixels have $p=0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result for a test case must be a two-element list containing the selected threshold and the corresponding $\\kappa$, both rounded to six decimals. The final output format must be precisely:\n[[threshold_1,kappa_1],[threshold_2,kappa_2],[threshold_3,kappa_3]]\n\nNo external input is required. All quantities are unitless probabilities. Angles are not involved. Ensure scientific realism by adhering strictly to the decision rule and agreement definitions described above, and by working only with the provided test suite.",
            "solution": "The provided problem is scientifically sound, well-posed, and all necessary data and conditions for its resolution are explicitly stated. It describes a standard task in classification model evaluation: optimizing a decision threshold to maximize a specific performance metric, in this case, Cohen's Kappa coefficient. The definitions and procedures align with established statistical and machine learning principles. Therefore, a full solution is warranted.\n\nThe core of the problem is to find a decision threshold, $\\tau$, for a probabilistic binary classifier that maximizes the chance-corrected agreement, Cohen's Kappa ($\\kappa$), between the classifier's predictions and a set of ground-truth reference labels. The problem is to be solved for three distinct test scenarios.\n\nThe solution methodology proceeds by first establishing the foundational statistical quantities required for calculating $\\kappa$, then outlining the algorithm to find the optimal threshold, and finally applying this algorithm to the given data.\n\n**1. Foundational Concepts: The Confusion Matrix and Agreement Metrics**\n\nFor any given threshold $\\tau$, a set of predicted probabilities $\\{p_i\\}_{i=1}^N$ is converted into a set of binary predictions $\\{y'_i\\}_{i=1}^N$ using the rule: $y'_i = 1$ (deforestation) if $p_i \\ge \\tau$, and $y'_i = 0$ (non-deforestation) otherwise. Comparing these predictions to the reference labels $\\{y_i\\}_{i=1}^N$ allows us to construct a $2 \\times 2$ confusion matrix with the following integer counts:\n\n-   **True Positives ($TP$)**: The number of pixels where $y_i=1$ and $y'_i=1$.\n-   **True Negatives ($TN$)**: The number of pixels where $y_i=0$ and $y'_i=0$.\n-   **False Positives ($FP$)**: The number of pixels where $y_i=0$ and $y'_i=1$.\n-   **False Negatives ($FN$)**: The number of pixels where $y_i=1$ and $y'_i=0$.\n\nThe total number of pixels is $N = TP + TN + FP + FN$. For this problem, $N=200$. These four counts are functions of the chosen threshold $\\tau$.\n\n**Observed Agreement ($p_o$)**\n\nThe observed agreement, $p_o$, is the proportion of pixels where the prediction matches the reference label. It is calculated directly from the confusion matrix:\n$$p_o = \\frac{TP + TN}{N}$$\n\n**Expected Agreement ($p_e$)**\n\nThe expected agreement, $p_e$, quantifies the agreement that would be expected by random chance alone. It is calculated based on the marginal proportions of each class in the reference and predicted sets, under the assumption of statistical independence between the two.\n\nFirst, we define the marginal proportions:\n-   Proportion of reference labels that are positive (class $1$): $P_{ref,1} = \\frac{TP + FN}{N}$\n-   Proportion of reference labels that are negative (class $0$): $P_{ref,0} = \\frac{TN + FP}{N}$\n-   Proportion of predicted labels that are positive (class $1$): $P_{pred,1} = \\frac{TP + FP}{N}$\n-   Proportion of predicted labels that are negative (class $0$): $P_{pred,0} = \\frac{TN + FN}{N}$\n\nThe probability of both independently assigning a positive label is $P_{ref,1} \\cdot P_{pred,1}$. The probability of both independently assigning a negative label is $P_{ref,0} \\cdot P_{pred,0}$. The total expected agreement is the sum of these two probabilities:\n$$p_e = (P_{ref,1} \\cdot P_{pred,1}) + (P_{ref,0} \\cdot P_{pred,0})$$\n\n**Cohen's Kappa Coefficient ($\\kappa$)**\n\nCohen's Kappa coefficient measures the agreement between two raters (here, the classifier and the reference data), while correcting for the agreement that is expected by chance. It is defined as:\n$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\nThe value of $\\kappa$ ranges from $-1$ to $1$. A value of $\\kappa=1$ implies perfect agreement, $\\kappa=0$ implies agreement is no better than random chance, and negative values imply agreement is worse than chance. A special case arises when $p_e=1$. This occurs only when both reference and prediction marginals are identical and pure (e.g., all samples are class $1$ in both). In this situation, observed agreement $p_o$ must also be $1$, and $\\kappa$ is conventionally defined as $1$.\n\n**2. Algorithmic Procedure for Threshold Optimization**\n\nThe problem requires finding the threshold $\\tau$ that maximizes $\\kappa$. The specified algorithm is as follows:\n\n1.  **Identify Candidate Thresholds**: The set of candidate thresholds, $\\mathcal{T}$, comprises all unique predicted probability values found in the dataset. Any threshold value between two consecutive unique probabilities would yield the same binary classification and hence the same $\\kappa$ value, so testing only the unique probabilities is sufficient and exhaustive.\n\n2.  **Iterate and Evaluate**: For each candidate threshold $\\tau \\in \\mathcal{T}$:\n    a.  Generate the binary predictions $\\{y'_i\\}$ based on the rule $p_i \\ge \\tau$.\n    b.  Construct the confusion matrix by comparing $\\{y'_i\\}$ to $\\{y_i\\}$.\n    c.  Calculate $p_o(\\tau)$ and $p_e(\\tau)$ from the confusion matrix.\n    d.  Compute $\\kappa(\\tau) = \\frac{p_o(\\tau) - p_e(\\tau)}{1 - p_e(\\tau)}$, handling the $1 - p_e(\\tau) = 0$ case as described previously.\n\n3.  **Select the Optimal Threshold**:\n    a.  Determine the maximum Kappa value achieved across all candidate thresholds: $\\kappa_{max} = \\max_{\\tau \\in \\mathcal{T}}\\{\\kappa(\\tau)\\}$.\n    b.  Identify the set of all thresholds that yield this maximum value: $\\mathcal{T}_{max} = \\{\\tau \\in \\mathcal{T} \\mid \\kappa(\\tau) = \\kappa_{max}\\}$.\n    c.  Apply the tie-breaking rule: select the smallest threshold from this set. The optimal threshold is $\\tau_{opt} = \\min(\\mathcal{T}_{max})$.\n\n4.  **Output**: For each test case, the final result is the pair $[\\tau_{opt}, \\kappa_{max}]$, with both values rounded to six decimal places.\n\nThis procedure will be implemented and applied to each of the three test cases, generating the required predictions, performing the threshold sweep, and identifying the optimal result according to the specified criteria.",
            "answer": "[[0.598000,0.980000],[0.379000,0.612146],[0.500000,0.000000]]"
        }
    ]
}