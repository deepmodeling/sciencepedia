{
    "hands_on_practices": [
        {
            "introduction": "How can we mathematically diagnose the potential for equifinality in a model? This exercise introduces a cornerstone of local sensitivity analysis: the Fisher Information Matrix (FIM). By computing the FIM and its eigenvalues for a common remote sensing model, you will quantify the concept of \"sloppiness\" and learn how the structure of a model and the range of its inputs can make certain parameter combinations nearly impossible to distinguish. This practice () provides a rigorous, quantitative foundation for understanding non-uniqueness.",
            "id": "3810068",
            "problem": "Consider a stylized, dimensionless forward model for nadir-view bidirectional reflectance that captures saturation with increasing canopy cover, commonly used as a surrogate in remote sensing and environmental modeling. For known, dimensionless illumination or structural drivers $x_i$ and parameters $\\boldsymbol{\\theta} = (a,b,c)$, the model-predicted reflectance at sample $i$ is\n$$\ny_i(\\boldsymbol{\\theta}) = a \\left(1 - e^{-b x_i}\\right) + c \\, ,\n$$\nwith independent, identically distributed Gaussian measurement errors of zero mean and variance $\\sigma^2$. Assume that the measurement operator is unbiased and that the linearization of the model is valid in a neighborhood of a given parameter vector $\\boldsymbol{\\theta}_0$. Under these assumptions, the negative log-likelihood near $\\boldsymbol{\\theta}_0$ is well approximated by a quadratic form derived from the sensitivity (Jacobian) and the Fisher Information Matrix (FIM).\n\nYour tasks are purely mathematical and algorithmic:\n- Using first principles of least squares with Gaussian errors, define the sensitivity Jacobian $J \\in \\mathbb{R}^{N \\times 3}$ at $\\boldsymbol{\\theta}_0$, where $N$ is the number of samples. Then construct the Fisher Information Matrix $F \\in \\mathbb{R}^{3 \\times 3}$ as\n$$\nF = J^\\top W J \\, ,\n$$\nwith $W = \\sigma^{-2} I_N$.\n- From $F$, quantify parameter sloppiness and equifinality in the neighborhood of $\\boldsymbol{\\theta}_0$ via the following metrics:\n    1. The sloppiness ratio $r = \\lambda_{\\min}(F)/\\lambda_{\\max}(F)$, where $\\lambda_{\\min}$ and $\\lambda_{\\max}$ are the smallest and largest eigenvalues of $F$. Declare a boolean `is_sloppy` that is `True` if $r  \\tau$ and `False` otherwise, with threshold $\\tau = 10^{-4}$.\n    2. The spectral condition number $\\kappa = \\lambda_{\\max}(F)/\\lambda_{\\min}(F)$.\n    3. The local equifinality index at misfit tolerance $\\varepsilon > 0$, defined by the Gaussian linear approximation to the $(\\Delta \\chi^2 = \\varepsilon)$ confidence ellipsoid volume:\n$$\nE(\\varepsilon) = \\frac{\\varepsilon^{p/2}}{\\sqrt{\\det(F)}} \\, ,\n$$\nwhere $p = 3$ is the number of parameters. This index is dimensionless and scales monotonically with the volume of indistinguishable parameter sets under the quadratic approximation.\n\nFundamental bases you must rely on:\n- Gaussian error model with independent, identically distributed noise implies a least-squares objective and Fisher Information Matrix $F = J^\\top W J$.\n- Linearization of a differentiable model $y(\\boldsymbol{\\theta})$ near $\\boldsymbol{\\theta}_0$ yields $y(\\boldsymbol{\\theta}_0 + \\delta\\boldsymbol{\\theta}) \\approx y(\\boldsymbol{\\theta}_0) + J \\, \\delta\\boldsymbol{\\theta}$.\n- The equifinality index defined above comes from the fact that, for Gaussian errors and a quadratic approximation, confidence regions are ellipsoids whose volume is proportional to $1/\\sqrt{\\det(F)}$ and scale with $\\varepsilon^{p/2}$.\n\nModel specifics to implement:\n- The Jacobian $J$ has entries computed from partial derivatives of $y_i(\\boldsymbol{\\theta})$ at $\\boldsymbol{\\theta}_0$:\n$$\n\\frac{\\partial y_i}{\\partial a} = 1 - e^{-b x_i}, \\quad\n\\frac{\\partial y_i}{\\partial b} = a \\, x_i \\, e^{-b x_i}, \\quad\n\\frac{\\partial y_i}{\\partial c} = 1 \\, .\n$$\n\nTest suite:\nFor each test case below, compute `is_sloppy`, $\\kappa$, and $E(\\varepsilon)$ using the specified inputs. All quantities are dimensionless. Use $N = 20$ samples in each case, and define $x_i$ as a linearly spaced grid on the stated interval, inclusive of endpoints.\n\n- Case $1$ (well-spread drivers, low noise, moderate nonlinearity):\n    - $x_i$ on $[0,5]$ with $N=20$.\n    - $\\boldsymbol{\\theta}_0 = (a,b,c) = (0.6, 0.8, 0.05)$.\n    - $\\sigma = 0.01$.\n    - $\\varepsilon = 1.0$.\n- Case $2$ (small curvature parameter inducing trade-off sloppiness):\n    - $x_i$ on $[0,1]$ with $N=20$.\n    - $\\boldsymbol{\\theta}_0 = (a,b,c) = (0.6, 0.02, 0.05)$.\n    - $\\sigma = 0.01$.\n    - $\\varepsilon = 1.0$.\n- Case $3$ (high noise, same drivers as Case $1$):\n    - $x_i$ on $[0,5]$ with $N=20$.\n    - $\\boldsymbol{\\theta}_0 = (a,b,c) = (0.6, 0.8, 0.05)$.\n    - $\\sigma = 0.1$.\n    - $\\varepsilon = 1.0$.\n- Case $4$ (narrow driver range causing near-linear indistinguishability):\n    - $x_i$ on $[0,0.1]$ with $N=20$.\n    - $\\boldsymbol{\\theta}_0 = (a,b,c) = (0.6, 0.8, 0.05)$.\n    - $\\sigma = 0.01$.\n    - $\\varepsilon = 1.0$.\n\nFinal output specification:\n- For each case, produce a list `[is_sloppy, kappa, E]`, where `is_sloppy` is a boolean and the two floating-point values are rounded to six significant figures.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, with each case represented by its list and no spaces, for example: `[[True,123.456,0.00123],[False,...,...],...]`.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded exercise in parameter sensitivity analysis and uncertainty quantification based on the Fisher Information Matrix (FIM), a cornerstone of statistical inference for parametric models. All provided information is self-contained, consistent, and mathematically precise.\n\nThe objective is to analyze the parameter identifiability of a nonlinear model using metrics derived from the FIM. The model describes the reflectance $y_i$ as a function of environmental drivers $x_i$ and a parameter vector $\\boldsymbol{\\theta} = (a, b, c)$:\n$$\ny_i(\\boldsymbol{\\theta}) = a \\left(1 - e^{-b x_i}\\right) + c\n$$\nWe assume measurement errors are independent and identically distributed Gaussian variables with mean $0$ and variance $\\sigma^2$. Under this assumption, the FIM provides a second-order approximation to the negative log-likelihood function around a point estimate $\\boldsymbol{\\theta}_0$.\n\nThe analysis proceeds in the following steps:\n\n1.  **Construct the Sensitivity (Jacobian) Matrix $J$**.\n    The Jacobian matrix $J$ contains the first-order partial derivatives of the model output with respect to each parameter, evaluated for each of the $N$ data samples at the parameter vector $\\boldsymbol{\\theta}_0 = (a_0, b_0, c_0)$. The entry $J_{ik}$ is $\\frac{\\partial y_i}{\\partial \\theta_k} |_{\\boldsymbol{\\theta}_0}$. For the given model, the $N \\times 3$ Jacobian has columns corresponding to derivatives with respect to $a$, $b$, and $c$:\n    $$\n    J_{i1} = \\frac{\\partial y_i}{\\partial a} \\bigg|_{\\boldsymbol{\\theta}_0} = 1 - e^{-b_0 x_i}\n    $$\n    $$\n    J_{i2} = \\frac{\\partial y_i}{\\partial b} \\bigg|_{\\boldsymbol{\\theta}_0} = a_0 x_i e^{-b_0 x_i}\n    $$\n    $$\n    J_{i3} = \\frac{\\partial y_i}{\\partial c} \\bigg|_{\\boldsymbol{\\theta}_0} = 1\n    $$\n\n2.  **Construct the Fisher Information Matrix $F$**.\n    The FIM is defined as $F = J^\\top W J$. The weighting matrix $W$ is the inverse of the data covariance matrix. For independent, identically distributed errors with variance $\\sigma^2$, the covariance matrix is $\\sigma^2 I_N$, where $I_N$ is the $N \\times N$ identity matrix. Thus, $W = (\\sigma^2 I_N)^{-1} = \\sigma^{-2} I_N$.\n    Substituting $W$ into the definition of $F$ yields:\n    $$\n    F = J^\\top (\\sigma^{-2} I_N) J = \\frac{1}{\\sigma^2} J^\\top J\n    $$\n    $F$ is a $3 \\times 3$ symmetric, positive semi-definite matrix. Its eigenvalues, $\\lambda_k$, quantify the model's sensitivity to changes in parameters along the directions of the corresponding eigenvectors (the principal axes of the parameter uncertainty ellipsoid).\n\n3.  **Compute Identifiability Metrics from the Eigenvalues of $F$**.\n    Let $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p$ be the ordered eigenvalues of $F$, where $p=3$ is the number of parameters.\n    - **Sloppiness Ratio ($r$) and Condition Number ($\\kappa$)**: A large spread in eigenvalues indicates that the model is \"sloppy\"â€”very sensitive to parameter changes in some directions (large eigenvalues) but very insensitive in others (small eigenvalues). This structural property is quantified by the condition number of the FIM. The problem defines two related metrics:\n        - The sloppiness ratio $r = \\lambda_{\\min}(F) / \\lambda_{\\max}(F)$. A value of $r$ close to $0$ indicates high sloppiness. We declare the model sloppy if $r  \\tau = 10^{-4}$.\n        - The spectral condition number $\\kappa = \\lambda_{\\max}(F) / \\lambda_{\\min}(F) = 1/r$. A large $\\kappa$ indicates an ill-conditioned problem where small changes in data can lead to large changes in parameter estimates, a hallmark of poor identifiability.\n\n    - **Local Equifinality Index $E(\\varepsilon)$**: This metric quantifies the volume of the parameter space that yields model predictions statistically indistinguishable from the predictions at $\\boldsymbol{\\theta}_0$, under a misfit tolerance $\\varepsilon$. For a linearized model with Gaussian errors, the region where the change in chi-squared statistic ($\\Delta \\chi^2$) is less than or equal to $\\varepsilon$ forms an ellipsoid. The volume of this ellipsoid is proportional to $1/\\sqrt{\\det(F)}$. The given index is a formalization of this relationship:\n        $$\n        E(\\varepsilon) = \\frac{\\varepsilon^{p/2}}{\\sqrt{\\det(F)}}\n        $$\n        Since the determinant is the product of the eigenvalues, $\\det(F) = \\prod_{k=1}^p \\lambda_k$, a single very small eigenvalue will make the determinant small, leading to a large equifinality index $E(\\varepsilon)$. This indicates a large volume of \"equifinal\" parameter sets that fit the data almost equally well.\n\nThe algorithm for each test case is as follows:\na.  Generate the vector of $N=20$ drivers $x_i$ on the specified interval.\nb.  Using the provided $\\boldsymbol{\\theta}_0 = (a_0, b_0, c_0)$, construct the three columns of the $20 \\times 3$ Jacobian matrix $J$.\nc.  Compute the FIM $F = \\frac{1}{\\sigma^2} J^\\top J$.\nd.  Calculate the eigenvalues of the symmetric matrix $F$, identifying $\\lambda_{\\min}$ and $\\lambda_{\\max}$.\ne.  Compute $r = \\lambda_{\\min} / \\lambda_{\\max}$ and determine the boolean $\\mathrm{is\\_sloppy}$ by comparing $r$ to the threshold $\\tau=10^{-4}$.\nf.  Compute $\\kappa = \\lambda_{\\max} / \\lambda_{\\min}$.\ng.  Compute the determinant $\\det(F)$ and use it to calculate the equifinality index $E(\\varepsilon)$ with $p=3$ and the specified $\\varepsilon$.\nh.  Format the resulting boolean and two floating-point numbers as required.\n\nThis procedure is systematically applied to each of the four test cases, which are designed to probe different aspects of parameter non-identifiability: structural trade-offs (Case 2, 4) and the effect of measurement noise (Case 3).",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes sloppiness and equifinality metrics for a nonlinear model\n    based on the Fisher Information Matrix for four test cases.\n    \"\"\"\n\n    def round_to_sf(x, sf):\n        \"\"\"\n        Rounds a number x to a specified number of significant figures (sf).\n        \"\"\"\n        if x == 0:\n            return 0.0\n        if not np.isfinite(x):\n            return x\n        order = math.floor(math.log10(abs(x)))\n        decimals = sf - 1 - order\n        return round(x, decimals)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: well-spread drivers, low noise, moderate nonlinearity\n        {'x_interval': [0, 5], 'N': 20, 'theta_0': (0.6, 0.8, 0.05), 'sigma': 0.01, 'epsilon': 1.0, 'p': 3},\n        # Case 2: small curvature parameter inducing trade-off sloppiness\n        {'x_interval': [0, 1], 'N': 20, 'theta_0': (0.6, 0.02, 0.05), 'sigma': 0.01, 'epsilon': 1.0, 'p': 3},\n        # Case 3: high noise, same drivers as Case 1\n        {'x_interval': [0, 5], 'N': 20, 'theta_0': (0.6, 0.8, 0.05), 'sigma': 0.1, 'epsilon': 1.0, 'p': 3},\n        # Case 4: narrow driver range causing near-linear indistinguishability\n        {'x_interval': [0, 0.1], 'N': 20, 'theta_0': (0.6, 0.8, 0.05), 'sigma': 0.01, 'epsilon': 1.0, 'p': 3}\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        x_interval = case['x_interval']\n        N = case['N']\n        a0, b0, c0 = case['theta_0']\n        sigma = case['sigma']\n        epsilon = case['epsilon']\n        p = case['p']\n        tau = 1e-4\n\n        # Generate the driver variable grid\n        x = np.linspace(x_interval[0], x_interval[1], N)\n\n        # 1. Construct the Sensitivity (Jacobian) Matrix J\n        # Partial derivative w.r.t. 'a'\n        j_a = 1 - np.exp(-b0 * x)\n        # Partial derivative w.r.t. 'b'\n        j_b = a0 * x * np.exp(-b0 * x)\n        # Partial derivative w.r.t. 'c'\n        j_c = np.ones(N)\n        \n        J = np.stack([j_a, j_b, j_c], axis=1)\n\n        # 2. Construct the Fisher Information Matrix F\n        F = (1 / sigma**2) * (J.T @ J)\n\n        # 3. Compute metrics from eigenvalues of F\n        # Use eigvalsh for symmetric matrices for numerical stability\n        eigenvalues = np.linalg.eigvalsh(F)\n        lambda_min = np.min(eigenvalues)\n        lambda_max = np.max(eigenvalues)\n\n        # Ensure lambda_min is not zero to avoid division errors\n        if lambda_min = 0:\n            # For this problem setup, eigenvalues should be positive,\n            # but this handles potential numerical underflow.\n            # A non-positive min eigenvalue indicates extreme sloppiness.\n            is_sloppy = True\n            kappa = np.inf\n            E_eps = np.inf\n        else:\n            # Sloppiness ratio and boolean\n            r = lambda_min / lambda_max\n            is_sloppy = r  tau\n\n            # Spectral condition number\n            kappa = lambda_max / lambda_min\n\n            # Local equifinality index\n            det_F = np.linalg.det(F)\n            if det_F = 0:\n                E_eps = np.inf\n            else:\n                E_eps = (epsilon**(p / 2)) / np.sqrt(det_F)\n\n        # Format results to 6 significant figures\n        kappa_rounded = round_to_sf(kappa, 6)\n        E_eps_rounded = round_to_sf(E_eps, 6)\n        \n        case_result = [is_sloppy, kappa_rounded, E_eps_rounded]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as specified (list of lists, no spaces)\n    results_str_list = []\n    for res in all_results:\n        # Manually construct the string for each inner list to control spacing\n        s = f\"[{res[0]},{res[1]},{res[2]}]\"\n        results_str_list.append(s)\n        \n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A good fit to calibration data does not guarantee a model's predictive power, a lesson that equifinality teaches us sharply. In this practice (), you will implement a simple hydrological model and perform a grid-based calibration, identifying a set of \"equifinal\" parameter combinations that all fit the data well. By then testing these \"good\" models against a different validation dataset, you will directly observe how parameter non-uniqueness can lead to high predictive uncertainty, revealing the critical gap that can exist between model fitting and forecasting.",
            "id": "3810082",
            "problem": "Consider a single-layer soil moisture bucket model driven by daily infiltration flux and subject to linear evapotranspiration and thresholded drainage. Let the state variable be volumetric water content $S_t$ in $\\mathrm{m^3\\,m^{-3}}$ at discrete time $t$ (with a daily time step $\\Delta t = 1$ day). The daily infiltration driving sequence is $P_t$ in $\\mathrm{m^3\\,m^{-3}\\,day^{-1}}$. The model removes water through evapotranspiration $E_t$ and drainage $Q_t$, defined by\n$$\nE_t = k_e\\, S_t, \\qquad Q_t = k_q\\, \\max\\!\\left(0,\\, S_t - S_{\\mathrm{fc}}\\right),\n$$\nwith parameters $k_e$ and $k_q$ in $\\mathrm{day^{-1}}$ and $S_{\\mathrm{fc}}$ in $\\mathrm{m^3\\,m^{-3}}$. The prognostic update (Forward Euler with clamping to physical bounds) is\n$$\nS_{t+1} = \\operatorname{clip}\\!\\left(S_t + P_t - E_t - Q_t,\\, 0,\\, S_{\\max}\\right),\n$$\nwhere $\\operatorname{clip}(x,0,S_{\\max}) = \\min\\!\\left(\\max\\!\\left(0, x\\right), S_{\\max}\\right)$ and $S_{\\max}$ is the maximum volumetric water content. The remote sensing observation operator is taken as direct volumetric water content with negligible measurement noise, i.e.,\n$$\ny_t = S_t.\n$$\n\nThis formulation embodies equifinality (non-uniqueness in model solutions) because when $S_t  S_{\\mathrm{fc}}$ over a period, the drainage $Q_t$ is identically zero, and $k_q$ and $S_{\\mathrm{fc}}$ are unidentifiable from those observations, so multiple parameter combinations yield indistinguishable trajectories that fit the calibration data equally well.\n\nYou must implement a program that performs calibration and validation under equifinality using a grid search over parameter ranges, computes the equifinal set for a specified calibration tolerance, and quantifies how equifinality affects validation performance.\n\nFundamental base and definitions to be used:\n- Water balance at the control volume level: $dS/dt = P - E - Q$.\n- Discrete-time approximation via Forward Euler: $S_{t+1} = S_t + P_t - E_t - Q_t$ followed by clamping.\n- Remote sensing observation operator identity: $y_t = S_t$.\n- Root Mean Square Error (RMSE): for a sequence of model predictions $\\hat{y}_t$ and observations $y_t$ over $T$ time steps, the RMSE is\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\hat{y}_t - y_t\\right)^2}.\n$$\n\nUse the following physically consistent constants and sequences (all in $\\mathrm{m^3\\,m^{-3}}$ unless otherwise noted):\n- Maximum volumetric water content: $S_{\\max} = 0.45$.\n- Initial condition: $S_0 = 0.15$.\n- Ground-truth parameters used to generate observations: $k_e^\\star = 0.06$, $k_q^\\star = 0.18$, $S_{\\mathrm{fc}}^\\star = 0.32$.\n\nCalibration forcing sequence (length $12$):\n$$\nP^{\\mathrm{cal}} = [0.015,\\, 0.000,\\, 0.010,\\, 0.000,\\, 0.020,\\, 0.000,\\, 0.015,\\, 0.000,\\, 0.010,\\, 0.000,\\, 0.020,\\, 0.000].\n$$\n\nValidation forcing sequence (length $12$):\n$$\nP^{\\mathrm{val}} = [0.050,\\, 0.040,\\, 0.000,\\, 0.030,\\, 0.025,\\, 0.000,\\, 0.060,\\, 0.000,\\, 0.040,\\, 0.030,\\, 0.000,\\, 0.050].\n$$\n\nObservations are generated by simulating the model with the ground-truth parameters on the respective forcing sequences, i.e., $y_t^{\\mathrm{cal}}$ and $y_t^{\\mathrm{val}}$ are the resulting $S_t$ trajectories under $P^{\\mathrm{cal}}$ and $P^{\\mathrm{val}}$ with $(k_e^\\star, k_q^\\star, S_{\\mathrm{fc}}^\\star)$, $S_0$, and $S_{\\max}$.\n\nFor each test case defined below, perform the following steps:\n1. Construct uniform grids for $k_e$, $k_q$, and $S_{\\mathrm{fc}}$ using the given minimum, maximum, and count for each parameter. The grid points must include both endpoints and be linearly spaced.\n2. For every parameter triple $(k_e, k_q, S_{\\mathrm{fc}})$ in the grid, simulate the model on $P^{\\mathrm{cal}}$ to produce $\\hat{y}_t^{\\mathrm{cal}}$ and compute the calibration $\\mathrm{RMSE}_{\\mathrm{cal}}$ against $y_t^{\\mathrm{cal}}$.\n3. Define the equifinal set $\\mathcal{E}$ as all parameter triples whose $\\mathrm{RMSE}_{\\mathrm{cal}}$ is less than or equal to the test case tolerance $\\tau$.\n4. For every parameter triple in $\\mathcal{E}$, simulate the model on $P^{\\mathrm{val}}$ to produce $\\hat{y}_t^{\\mathrm{val}}$ and compute the validation $\\mathrm{RMSE}_{\\mathrm{val}}$ against $y_t^{\\mathrm{val}}$.\n5. Compute the following summary metrics for the test case:\n   - The integer cardinality $N = |\\mathcal{E}|$.\n   - The mean of the validation RMSE values over $\\mathcal{E}$, denoted $\\overline{R}_{\\mathrm{val}}$ (a float in $\\mathrm{m^3\\,m^{-3}}$).\n   - The variance of the validation RMSE values over $\\mathcal{E}$, denoted $\\mathrm{Var}(R_{\\mathrm{val}})$ (a float in $\\mathrm{m^3\\,m^{-3}}$ squared).\n   - A boolean diagnostic indicating calibration-validation mismatch: let $(k_e^{\\dagger}, k_q^{\\dagger}, S_{\\mathrm{fc}}^{\\dagger})$ be the lexicographically first parameter triple among those achieving the minimal $\\mathrm{RMSE}_{\\mathrm{cal}}$ over the entire grid (i.e., choose the triple with the smallest $k_e$, then, if tied, the smallest $k_q$, then, if tied, the smallest $S_{\\mathrm{fc}}$ among all triples with the minimal $\\mathrm{RMSE}_{\\mathrm{cal}}$). Let $R_{\\mathrm{val}}^{\\dagger}$ be its validation RMSE. The diagnostic is `True` if $R_{\\mathrm{val}}^{\\dagger}$ is strictly greater than the minimal validation RMSE over $\\mathcal{E}$; otherwise `False`.\n6. Edge case handling: if $N = 0$, set $\\overline{R}_{\\mathrm{val}} = -1.0$, $\\mathrm{Var}(R_{\\mathrm{val}}) = -1.0$, and the boolean diagnostic to `False`.\n\nTest suite (three test cases):\n- Test case $1$ (happy path equifinality):\n  - $k_e$ grid: minimum $0.06$, maximum $0.06$, count $1$.\n  - $k_q$ grid: minimum $0.00$, maximum $0.30$, count $7$.\n  - $S_{\\mathrm{fc}}$ grid: minimum $0.25$, maximum $0.35$, count $6$.\n  - Calibration tolerance: $\\tau = 0.0$.\n- Test case $2$ (boundary case: no equifinal solutions):\n  - $k_e$ grid: minimum $0.02$, maximum $0.055$, count $8$.\n  - $k_q$ grid: minimum $0.00$, maximum $0.30$, count $7$.\n  - $S_{\\mathrm{fc}}$ grid: minimum $0.25$, maximum $0.35$, count $6$.\n  - Calibration tolerance: $\\tau = 0.0$.\n- Test case $3$ (moderate tolerance: many equifinal solutions):\n  - $k_e$ grid: minimum $0.03$, maximum $0.09$, count $7$.\n  - $k_q$ grid: minimum $0.00$, maximum $0.30$, count $7$.\n  - $S_{\\mathrm{fc}}$ grid: minimum $0.25$, maximum $0.35$, count $6$.\n  - Calibration tolerance: $\\tau = 0.005$.\n\nRequired units:\n- Report $\\overline{R}_{\\mathrm{val}}$ and $\\mathrm{Var}(R_{\\mathrm{val}})$ in $\\mathrm{m^3\\,m^{-3}}$ and $\\mathrm{(m^3\\,m^{-3})^2}$, respectively.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list of the form $[N, \\overline{R}_{\\mathrm{val}}, \\mathrm{Var}(R_{\\mathrm{val}}), \\text{boolean}]$. For example, a valid output format is `[[3,0.0042,1.6e-06,True],[0,-1.0,-1.0,False],[12,0.0068,2.1e-06,True]]`.",
            "solution": "The problem requires the implementation and analysis of a single-layer soil moisture bucket model to demonstrate the concept of equifinality in environmental modeling. The process involves parameter calibration and validation using a grid search methodology.\n\nThe model's core is the water balance equation for the volumetric soil water content, $S_t$, at discrete daily time steps $t$. The change in storage is driven by an external infiltration flux $P_t$ and depleted by two internal processes: evapotranspiration $E_t$ and drainage $Q_t$. The prognostic equation is a Forward Euler discretization of the water balance differential equation:\n\n$$\n\\frac{dS}{dt} = P - E - Q\n$$\n\nThis is approximated as:\n$$\nS_{t+1}^{\\text{raw}} = S_t + (P_t - E_t - Q_t) \\Delta t\n$$\nSince the time step $\\Delta t$ is given as $1$ day, and the units of $P,E,Q$ are given in $\\mathrm{day^{-1}}$, the equation simplifies to:\n$$\nS_{t+1}^{\\text{raw}} = S_t + P_t - E_t - Q_t\n$$\n\nThe constituent fluxes are defined as:\n1.  Evapotranspiration $E_t = k_e S_t$: a linear function of the current water content, parameterized by the rate constant $k_e$.\n2.  Drainage $Q_t = k_q \\max(0, S_t - S_{\\mathrm{fc}})$: a thresholded linear function, which is zero until the water content exceeds a field capacity threshold $S_{\\mathrm{fc}}$. The drainage rate is controlled by the parameter $k_q$.\n\nTo maintain physical realism, the resulting water content $S_{t+1}$ is clamped between $0$ (completely dry) and the maximum saturation capacity $S_{\\max}$:\n$$\nS_{t+1} = \\operatorname{clip}(S_{t+1}^{\\text{raw}}, 0, S_{\\max}) = \\min(\\max(0, S_{t+1}^{\\text{raw}}), S_{\\max})\n$$\n\nThe overall procedure is as follows:\nFirst, we establish a \"ground truth\" by defining a known set of parameters $(k_e^\\star, k_q^\\star, S_{\\mathrm{fc}}^\\star)$. Using these true parameters, we simulate the model with two distinct forcing sequences, $P^{\\mathrm{cal}}$ and $P^{\\mathrm{val}}$, to generate synthetic observation datasets, $y_t^{\\mathrm{cal}}$ and $y_t^{\\mathrm{val}}$. These represent the data we would hypothetically collect for calibration and validation, respectively.\n\nNext, for each test case, we perform a grid search over specified ranges for the parameters $k_e$, $k_q$, and $S_{\\mathrm{fc}}$. For every parameter combination $(k_e, k_q, S_{\\mathrm{fc}})$ in the grid:\n1.  We simulate the model using the calibration forcing $P^{\\mathrm{cal}}$ to produce a predicted trajectory $\\hat{y}_t^{\\mathrm{cal}}$.\n2.  We quantify the model's performance by computing the Root Mean Square Error (RMSE) between the prediction and the ground-truth observations, $\\mathrm{RMSE}_{\\mathrm{cal}} = \\sqrt{\\frac{1}{T}\\sum_{t=1}^{T}(\\hat{y}_t^{\\mathrm{cal}} - y_t^{\\mathrm{cal}})^2}$.\n\nThe concept of equifinality is central here. It refers to the phenomenon where multiple different parameter sets can produce model outputs that are equally good (or at least acceptably good) fits to the calibration data. We formalize this by defining an \"equifinal set\" $\\mathcal{E}$ as the collection of all parameter triples whose $\\mathrm{RMSE}_{\\mathrm{cal}}$ is below a given tolerance $\\tau$. The size of this set, $N=|\\mathcal{E}|$, is a direct measure of the degree of equifinality.\n\nThe crucial step is to assess the implications of this equifinality on the model's predictive capability. For every parameter set within $\\mathcal{E}$, we perform a validation run using the separate forcing sequence $P^{\\mathrm{val}}$ and compute the validation error, $\\mathrm{RMSE}_{\\mathrm{val}}$. The distribution of these validation errors reveals the uncertainty in our predictions. We calculate the mean $\\overline{R}_{\\mathrm{val}}$ and variance $\\mathrm{Var}(R_{\\mathrm{val}})$ of these errors. High variance indicates that while many parameter sets fit the calibration data, their predictions diverge significantly under different conditions, posing a challenge for reliable forecasting.\n\nFinally, we compute a diagnostic to check for a calibration-validation mismatch. We identify the single \"best\" parameter set from the calibration phase, $(k_e^{\\dagger}, k_q^{\\dagger}, S_{\\mathrm{fc}}^{\\dagger})$, defined as the one achieving the minimum $\\mathrm{RMSE}_{\\mathrm{cal}}$ (with lexicographical tie-breaking). We then check if this parameter set also yields the best performance in the validation phase (i.e., the minimum $\\mathrm{RMSE}_{\\mathrm{val}}$ among all sets in $\\mathcal{E}$). If its validation error $R_{\\mathrm{val}}^{\\dagger}$ is strictly greater than the minimum possible validation error from a member of $\\mathcal{E}$, it highlights a critical issue in model selection: the best-calibrated model is not necessarily the best predictive model.\n\nThe specific design of the test cases is intended to highlight these principles. Test Case 1 uses a calibration forcing $P^{\\mathrm{cal}}$ that never causes the soil moisture $S_t$ to exceed the field capacity threshold $S_{\\mathrm{fc}}$. Consequently, the drainage term $Q_t$ is always zero, rendering the parameters $k_q$ and $S_{\\mathrm{fc}}$ unidentifiable. This leads to a large equifinal set. The validation forcing $P^{\\mathrm{val}}$ is stronger, activating drainage and thus revealing the performance differences among the equifinal parameter sets. Test Case 2 uses a parameter grid that misses the true parameter $k_e^\\star$ and a strict tolerance $\\tau=0.0$, resulting in an empty equfinal set. Test Case 3 uses a wider grid and a relaxed tolerance, representing a more typical calibration scenario.\n\nThe implementation will proceed by first defining a model-simulation function. Then, for each test case, we generate the ground truth, perform the grid search, identify the equifinal set, compute validation statistics, and determine the diagnostic boolean, meticulously following the specified logic.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform model calibration and validation under equifinality.\n    \"\"\"\n    # === Fundamental Constants and Forcing Sequences ===\n    S_max = 0.45\n    S0 = 0.15\n    ground_truth_params = (0.06, 0.18, 0.32)\n    P_cal = np.array([0.015, 0.000, 0.010, 0.000, 0.020, 0.000, 0.015, 0.000, 0.010, 0.000, 0.020, 0.000])\n    P_val = np.array([0.050, 0.040, 0.000, 0.030, 0.025, 0.000, 0.060, 0.000, 0.040, 0.030, 0.000, 0.050])\n\n    # === Test Suite Definition ===\n    test_cases = [\n        # Test Case 1: Happy path equifinality\n        {'ke_grid': (0.06, 0.06, 1), 'kq_grid': (0.00, 0.30, 7), 'sfc_grid': (0.25, 0.35, 6), 'tau': 0.0},\n        # Test Case 2: Boundary case, no equifinal solutions\n        {'ke_grid': (0.02, 0.055, 8), 'kq_grid': (0.00, 0.30, 7), 'sfc_grid': (0.25, 0.35, 6), 'tau': 0.0},\n        # Test Case 3: Moderate tolerance, many equifinal solutions\n        {'ke_grid': (0.03, 0.09, 7), 'kq_grid': (0.00, 0.30, 7), 'sfc_grid': (0.25, 0.35, 6), 'tau': 0.005}\n    ]\n\n    # === Helper Functions ===\n    def run_model(params, s0_val, p_seq, s_max_val):\n        \"\"\"Simulates the soil moisture model for a given parameter set and forcing.\"\"\"\n        ke, kq, sfc = params\n        s_t = s0_val\n        s_history = []\n        for p_t in p_seq:\n            e_t = ke * s_t\n            q_t = kq * max(0, s_t - sfc)\n            s_t_plus_1_raw = s_t + p_t - e_t - q_t\n            s_t_plus_1 = np.clip(s_t_plus_1_raw, 0, s_max_val)\n            s_history.append(s_t_plus_1)\n            s_t = s_t_plus_1\n        return np.array(s_history)\n\n    def rmse(y_hat, y_true):\n        \"\"\"Computes the Root Mean Square Error.\"\"\"\n        return np.sqrt(np.mean((y_hat - y_true)**2))\n\n    # === Generate Ground Truth Observations ===\n    y_cal_true = run_model(ground_truth_params, S0, P_cal, S_max)\n    y_val_true = run_model(ground_truth_params, S0, P_val, S_max)\n    \n    final_results = []\n    \n    # === Main Loop over Test Cases ===\n    for case in test_cases:\n        # Step 1: Construct parameter grids\n        ke_grid = np.linspace(case['ke_grid'][0], case['ke_grid'][1], case['ke_grid'][2])\n        kq_grid = np.linspace(case['kq_grid'][0], case['kq_grid'][1], case['kq_grid'][2])\n        sfc_grid = np.linspace(case['sfc_grid'][0], case['sfc_grid'][1], case['sfc_grid'][2])\n        tau = case['tau']\n\n        # Step 2: Grid search and calibration RMSE calculation\n        calibration_results = []\n        for ke in ke_grid:\n            for kq in kq_grid:\n                for sfc in sfc_grid:\n                    params = (ke, kq, sfc)\n                    y_hat_cal = run_model(params, S0, P_cal, S_max)\n                    rmse_cal = rmse(y_hat_cal, y_cal_true)\n                    calibration_results.append({'params': params, 'rmse_cal': rmse_cal})\n\n        # Step 3: Define equifinal set\n        equifinal_set = [res for res in calibration_results if res['rmse_cal'] = tau]\n        \n        # Step 5: Compute cardinality N\n        N = len(equifinal_set)\n\n        # Handle edge case N=0\n        if N == 0:\n            final_results.append([0, -1.0, -1.0, False])\n            continue\n            \n        # Step 4: Compute validation RMSE for equifinal set\n        validation_rmse_values = []\n        for res in equifinal_set:\n            y_hat_val = run_model(res['params'], S0, P_val, S_max)\n            rmse_val = rmse(y_hat_val, y_val_true)\n            res['rmse_val'] = rmse_val  # Augment dict with validation result\n            validation_rmse_values.append(rmse_val)\n        \n        # Step 5: Compute summary metrics\n        validation_rmse_values = np.array(validation_rmse_values)\n        R_val_mean = np.mean(validation_rmse_values)\n        R_val_var = np.var(validation_rmse_values)\n\n        # Boolean diagnostic\n        min_rmse_cal_val = min(res['rmse_cal'] for res in calibration_results)\n        \n        best_cal_params_candidates = [\n            res['params'] for res in calibration_results if np.isclose(res['rmse_cal'], min_rmse_cal_val)\n        ]\n        \n        best_cal_params_candidates.sort()\n        params_dagger = best_cal_params_candidates[0]\n        \n        # Find R_val_dagger, which is the validation RMSE for params_dagger\n        R_val_dagger = -1.0\n        # This parameter set must be in the equifinal set if min_rmse_cal_val = tau\n        for res in equifinal_set:\n            if res['params'] == params_dagger:\n                R_val_dagger = res['rmse_val']\n                break\n        \n        min_R_val_in_E = np.min(validation_rmse_values)\n        \n        diagnostic = R_val_dagger > min_R_val_in_E\n\n        final_results.append([N, R_val_mean, R_val_var, diagnostic])\n\n    # === Final Output Formatting ===\n    # Convert list of lists to the required string format\n    output_str = \"[\"\n    for i, res in enumerate(final_results):\n        N, R_mean, R_var, diag = res\n        # Format floats to reasonable precision for consistent output\n        diag_str = 'True' if diag else 'False'\n        output_str += f\"[{N},{R_mean},{R_var},{diag_str}]\"\n        if i  len(final_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Diagnosing equifinality is crucial, but how can we proactively reduce it? This advanced practice () moves from analysis to action by introducing the principles of optimal experimental design. You will implement a greedy algorithm to intelligently select a sequence of new measurements that will most efficiently shrink the parameter uncertainty volume, demonstrating a powerful strategy for planning data acquisition campaigns to overcome non-uniqueness and build more robust models.",
            "id": "3810094",
            "problem": "You are tasked with designing and implementing a complete, runnable program that performs optimal experimental design to mitigate equifinality and non-uniqueness in parameter estimation for a linearized remote sensing forward model. The program must select, from a finite set of candidate observation configurations, a subset that maximizes information gain about model parameters under Gaussian noise and a Gaussian prior. Your implementation must be derived from first principles and must not rely on any pre-supplied shortcut formulas in this statement.\n\nConsider a remote sensing forward model with parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^p$. The observation model for a single candidate measurement configuration $i$ is the linearized form\n$y_i \\approx \\mathbf{s}_i^\\top \\boldsymbol{\\theta} + \\varepsilon_i,$\nwhere $\\mathbf{s}_i \\in \\mathbb{R}^p$ is the local sensitivity vector of the observation with respect to the parameters, and $\\varepsilon_i$ is zero-mean Gaussian noise with variance $\\sigma_i^2$. Measurements are assumed independent across configurations. A Gaussian prior encodes environmental knowledge on parameters: $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\mathbf{C}_\\text{prior})$, where $\\mathbf{C}_\\text{prior} \\in \\mathbb{R}^{p \\times p}$ is symmetric positive definite.\n\nYour goal is to select $m$ measurement configurations from a finite candidate set to reduce equifinality and non-uniqueness in the posterior estimate of $\\boldsymbol{\\theta}$. Equifinality refers to the presence of multiple distinct parameter vectors that produce indistinguishably similar outputs due to aligned or correlated sensitivity directions, which manifests mathematically as near-singularity or ill-conditioning in the information matrix. To design the selection rule, start from the following fundamental base:\n\n- Gaussian likelihood for independent measurements,\n- The definition of Fisher information for linear Gaussian models,\n- Incorporation of the Gaussian prior via Bayesian updating,\n- The relationship between the determinant of the information matrix and the volume of the confidence ellipsoid.\n\nDerive, from these bases, a greedy selection rule that at each step chooses the next measurement to maximize the marginal increase in a determinant-based optimality criterion appropriate for mitigating equifinality. Specifically, use determinant maximization of the Bayesian information matrix to counteract non-uniqueness by spreading information across independent directions in parameter space.\n\nImplement the resulting greedy algorithm as a function that takes:\n- The parameter dimension $p$,\n- The prior covariance matrix $\\mathbf{C}_\\text{prior}$,\n- A list of candidate sensitivity vectors $\\{\\mathbf{s}_i\\}_{i=1}^L$,\n- A list of corresponding noise variances $\\{\\sigma_i^2\\}_{i=1}^L$,\n- The selection budget $m$,\n\nand returns the indices of the selected configurations in the order they are chosen.\n\nYour program must solve the following test suite. Each test case is specified as a tuple containing $p$, $\\mathbf{C}_\\text{prior}$, the list of candidate $\\mathbf{s}_i$, the list of $\\sigma_i^2$, and $m$. All matrices and vectors are dimensionally consistent, and all noise variances are positive.\n\nTest Suite:\n1. Happy-path with moderate prior and mixed redundancies:\n   - $p = 4$,\n   - $\\mathbf{C}_\\text{prior} = \\operatorname{diag}(10.0, 10.0, 10.0, 10.0)$,\n   - Sensitivities:\n     $\\mathbf{s}_0 = [1.0, 0.0, 0.1, 0.0]$,\n     $\\mathbf{s}_1 = [0.9, 0.1, 0.0, 0.0]$,\n     $\\mathbf{s}_2 = [0.0, 1.0, 0.0, 0.2]$,\n     $\\mathbf{s}_3 = [0.0, 0.0, 1.0, 0.0]$,\n     $\\mathbf{s}_4 = [0.0, 0.0, 0.0, 1.0]$,\n     $\\mathbf{s}_5 = [0.95, 0.05, 0.0, 0.0]$,\n   - Noise variances:\n     $\\sigma^2 = [0.04, 0.05, 0.04, 0.02, 0.02, 0.05]$,\n   - $m = 3$.\n\n2. Equifinality-heavy case with one low-noise measurement breaking non-uniqueness:\n   - $p = 3$,\n   - $\\mathbf{C}_\\text{prior} = \\operatorname{diag}(50.0, 50.0, 50.0)$,\n   - Sensitivities:\n     $\\mathbf{s}_0 = [1.0, 0.0, 0.0]$,\n     $\\mathbf{s}_1 = [0.9, 0.1, 0.0]$,\n     $\\mathbf{s}_2 = [0.8, 0.2, 0.0]$,\n     $\\mathbf{s}_3 = [0.0, 1.0, 0.0]$,\n     $\\mathbf{s}_4 = [0.0, 0.0, 1.0]$,\n     $\\mathbf{s}_5 = [0.1, 0.1, 0.0]$,\n   - Noise variances:\n     $\\sigma^2 = [0.2, 0.2, 0.2, 0.2, 0.05, 0.2]$,\n   - $m = 3$.\n\n3. Boundary case with a single measurement:\n   - $p = 3$,\n   - $\\mathbf{C}_\\text{prior} = \\operatorname{diag}(1.0, 1.0, 1.0)$,\n   - Sensitivities:\n     $\\mathbf{s}_0 = [1.0, 0.0, 0.0]$,\n     $\\mathbf{s}_1 = [0.0, 1.0, 0.0]$,\n     $\\mathbf{s}_2 = [0.0, 0.0, 1.0]$,\n     $\\mathbf{s}_3 = [0.5, 0.5, 0.0]$,\n     $\\mathbf{s}_4 = [0.0, 0.5, 0.5]$,\n   - Noise variances:\n     $\\sigma^2 = [0.1, 0.1, 0.1, 0.1, 0.1]$,\n   - $m = 1$.\n\n4. Prior-dominated anisotropy emphasizing one poorly constrained parameter:\n   - $p = 4$,\n   - $\\mathbf{C}_\\text{prior} = \\operatorname{diag}(1.0, 1.0, 1.0, 100.0)$,\n   - Sensitivities:\n     $\\mathbf{s}_0 = [0.0, 0.0, 0.0, 1.0]$,\n     $\\mathbf{s}_1 = [0.1, 0.0, 0.0, 0.9]$,\n     $\\mathbf{s}_2 = [1.0, 0.0, 0.0, 0.0]$,\n     $\\mathbf{s}_3 = [0.0, 1.0, 0.0, 0.0]$,\n     $\\mathbf{s}_4 = [0.0, 0.0, 1.0, 0.0]$,\n   - Noise variances:\n     $\\sigma^2 = [0.05, 0.05, 0.05, 0.05, 0.05]$,\n   - $m = 2$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Each test case result must be a list of the selected indices in selection order. For example, the final output format must look like\n$$\n[ [i_{1,1}, i_{1,2}, \\dots], [i_{2,1}, i_{2,2}, \\dots], [i_{3,1}, \\dots], [i_{4,1}, i_{4,2}] ].\n$$\nSpaces are not required; lists must be syntactically valid with integers. No physical units are required for this problem since all quantities are dimensionless within the mathematical specification.",
            "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in Bayesian inference and optimal experimental design, well-posed with a clear objective and constraints, and formulated with objective, precise language. The problem is non-trivial and requires a correct derivation from first principles, which is what is provided below.\n\n### Derivation of the Greedy Selection Algorithm\n\nThe objective is to select a subset of $m$ measurements from a candidate pool to maximally reduce the uncertainty in the parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^p$. The reduction in uncertainty is quantified by maximizing the determinant of the posterior information matrix, a criterion known as D-optimality. This criterion is chosen because the volume of the posterior credible ellipsoid for $\\boldsymbol{\\theta}$ is inversely proportional to the square root of the determinant of the posterior information matrix. Maximizing this determinant aggressively shrinks the volume of parameter space consistent with the data and prior, thus mitigating equifinality and non-uniqueness.\n\n**1. Bayesian Framework and Information Matrices**\n\nThe foundation of our approach is Bayes' theorem, which for our Gaussian-assumed problem, can be conveniently expressed in terms of information matrices. The posterior probability distribution of the parameters $\\boldsymbol{\\theta}$ given a set of measurements $\\mathcal{S}$ is proportional to the product of the likelihood and the prior: $p(\\boldsymbol{\\theta} | \\{y_j\\}_{j \\in \\mathcal{S}}) \\propto p(\\{y_j\\}_{j \\in \\mathcal{S}} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})$. Since the distributions are Gaussian, their exponents add, which means their information (precision) matrices add.\n\nThe posterior information matrix, $\\mathbf{I}_\\text{post}$, is the sum of the prior information matrix, $\\mathbf{I}_\\text{prior}$, and the information matrix from the data, $\\mathbf{I}_\\text{data}$:\n$$\n\\mathbf{I}_\\text{post}(\\mathcal{S}) = \\mathbf{I}_\\text{prior} + \\mathbf{I}_\\text{data}(\\mathcal{S})\n$$\n\n**2. Prior Information**\n\nThe prior belief about the parameters is given by a Gaussian distribution $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\mathbf{C}_\\text{prior})$. The prior information matrix is the inverse of the prior covariance matrix:\n$$\n\\mathbf{I}_\\text{prior} = \\mathbf{C}_\\text{prior}^{-1}\n$$\nThis matrix quantifies our knowledge about $\\boldsymbol{\\theta}$ before any new measurements are made.\n\n**3. Likelihood and Data Information (Fisher Information)**\n\nThe observation model for a single measurement $i$ is $y_i \\approx \\mathbf{s}_i^\\top \\boldsymbol{\\theta} + \\varepsilon_i$, with noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$. The log-likelihood function for this measurement is:\n$$\n\\ln p(y_i | \\boldsymbol{\\theta}) = -\\frac{1}{2\\sigma_i^2} (y_i - \\mathbf{s}_i^\\top \\boldsymbol{\\theta})^2 - \\frac{1}{2}\\ln(2\\pi\\sigma_i^2)\n$$\nThe Fisher Information matrix for this single measurement is defined as the negative expectation of the Hessian (second derivative) of the log-likelihood with respect to $\\boldsymbol{\\theta}$. For a linear Gaussian model, this simplifies to the Hessian of the quadratic term in $\\boldsymbol{\\theta}$:\n$$\n\\mathbf{I}_i = -\\mathbb{E}\\left[\\nabla_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}^\\top \\ln p(y_i | \\boldsymbol{\\theta}) \\right] = \\nabla_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}^\\top \\left( \\frac{1}{2\\sigma_i^2} (\\mathbf{s}_i^\\top \\boldsymbol{\\theta})^2 \\right) = \\frac{1}{\\sigma_i^2} \\mathbf{s}_i \\mathbf{s}_i^\\top\n$$\nThis is a rank-$1$ matrix representing the information contributed by measurement $i$. Since measurements are independent, the total information from a set of measurements $\\mathcal{S}$ is additive:\n$$\n\\mathbf{I}_\\text{data}(\\mathcal{S}) = \\sum_{j \\in \\mathcal{S}} \\mathbf{I}_j = \\sum_{j \\in \\mathcal{S}} \\frac{1}{\\sigma_j^2} \\mathbf{s}_j \\mathbf{s}_j^\\top\n$$\n\n**4. The Greedy Selection Criterion**\n\nWe will build the set of selected measurements $\\mathcal{S}$ iteratively. Let $\\mathcal{S}_{k-1}$ be the set of $k-1$ measurements already chosen, and let $\\mathbf{I}^{(k-1)} = \\mathbf{I}_\\text{post}(\\mathcal{S}_{k-1})$ be the corresponding posterior information matrix. At step $k$, we want to select the next measurement $j^*$ from the set of available candidates $\\mathcal{A}_{k-1}$ that provides the maximum marginal increase in the determinant of the information matrix.\nThe new information matrix after adding measurement $j$ is:\n$$\n\\mathbf{I}^{(k-1, j)} = \\mathbf{I}^{(k-1)} + \\mathbf{I}_j = \\mathbf{I}^{(k-1)} + \\frac{1}{\\sigma_j^2} \\mathbf{s}_j \\mathbf{s}_j^\\top\n$$\nWe seek the measurement $j^*$ that solves:\n$$\nj^* = \\arg\\max_{j \\in \\mathcal{A}_{k-1}} \\det(\\mathbf{I}^{(k-1, j)})\n$$\nTo evaluate this efficiently, we use the matrix determinant lemma: $\\det(\\mathbf{A} + \\mathbf{u}\\mathbf{v}^\\top) = (1 + \\mathbf{v}^\\top \\mathbf{A}^{-1}\\mathbf{u})\\det(\\mathbf{A})$.\nLetting $\\mathbf{A} = \\mathbf{I}^{(k-1)}$ and $\\mathbf{u} = \\mathbf{v} = \\frac{\\mathbf{s}_j}{\\sigma_j}$, we get:\n$$\n\\det(\\mathbf{I}^{(k-1, j)}) = \\det(\\mathbf{I}^{(k-1)}) \\left( 1 + \\left(\\frac{\\mathbf{s}_j}{\\sigma_j}\\right)^\\top (\\mathbf{I}^{(k-1)})^{-1} \\left(\\frac{\\mathbf{s}_j}{\\sigma_j}\\right) \\right) = \\det(\\mathbf I^{(k-1)}) \\left( 1 + \\frac{1}{\\sigma_j^2} \\mathbf{s}_j^\\top (\\mathbf{I}^{(k-1)})^{-1} \\mathbf{s}_j \\right)\n$$\nMaximizing this expression is equivalent to maximizing the term being added. Let $\\mathbf{C}^{(k-1)} = (\\mathbf{I}^{(k-1)})^{-1}$ be the posterior covariance matrix after $k-1$ selections. The selection rule is thus:\n$$\nj^* = \\arg\\max_{j \\in \\mathcal{A}_{k-1}} \\frac{\\mathbf{s}_j^\\top \\mathbf{C}^{(k-1)} \\mathbf{s}_j}{\\sigma_j^2}\n$$\nThe term $\\mathbf{s}_j^\\top \\mathbf{C}^{(k-1)} \\mathbf{s}_j$ is the variance of the predicted measurement $y_j = \\mathbf{s}_j^\\top \\boldsymbol{\\theta}$ under the current posterior. The criterion therefore selects the measurement that is most uncertain, normalized by its own intrinsic noise variance.\n\n**5. Algorithmic Implementation and Optimization**\n\nA direct implementation would involve re-computing the matrix inverse $\\mathbf{C}^{(k)} = (\\mathbf{I}^{(k)})^{-1}$ at each step, which is computationally expensive ($O(p^3)$). A more efficient approach is to update the covariance matrix $\\mathbf{C}^{(k-1)}$ directly to $\\mathbf{C}^{(k)}$ using the Sherman-Morrison formula for the inverse of a rank-$1$ update:\n$$\n(\\mathbf{A} + \\mathbf{u}\\mathbf{v}^\\top)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{v}^\\top\\mathbf{A}^{-1}}{1 + \\mathbf{v}^\\top\\mathbf{A}^{-1}\\mathbf{u}}\n$$\nApplying this to our covariance update, we have $\\mathbf{C}^{(k)} = (\\mathbf{I}^{(k-1)} + \\frac{1}{\\sigma_{j^*}^2}\\mathbf{s}_{j^*}\\mathbf{s}_{j^*}^\\top)^{-1}$. This gives:\n$$\n\\mathbf{C}^{(k)} = \\mathbf{C}^{(k-1)} - \\frac{\\mathbf{C}^{(k-1)} \\mathbf{s}_{j^*} \\mathbf{s}_{j^*}^\\top \\mathbf{C}^{(k-1)}}{\\sigma_{j^*}^2 + \\mathbf{s}_{j^*}^\\top \\mathbf{C}^{(k-1)} \\mathbf{s}_{j^*}}\n$$\nThis update only requires matrix-vector multiplications and an outer product, reducing the complexity of each step's update to $O(p^2)$.\n\nThe final algorithm is as follows:\n1.  Initialize `selected_indices` to an empty list, `candidate_indices` to the set of all measurement indices, and the current posterior covariance `C_current` to `C_prior`.\n2.  For $k$ from $1$ to $m$:\n    a. For each candidate index $j$ in `candidate_indices`, compute the score $S_j = (\\mathbf{s}_j^\\top \\mathbf{C}_\\text{current} \\mathbf{s}_j) / \\sigma_j^2$.\n    b. Find the index $j^*$ with the maximum score. In case of ties, the smallest index is chosen.\n    c. Add $j^*$ to `selected_indices` and remove it from `candidate_indices`.\n    d. Update `C_current` using the Sherman-Morrison formula with $\\mathbf{s}_{j^*}$ and $\\sigma_{j^*}^2$.\n3.  Return the `selected_indices` list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef greedy_selection(p, c_prior, sensitivities, variances, m):\n    \"\"\"\n    Performs greedy selection of m measurements to maximize the determinant\n    of the posterior information matrix.\n\n    Args:\n        p (int): The dimension of the parameter space.\n        c_prior (np.ndarray): The prior covariance matrix (p x p).\n        sensitivities (list): A list of candidate sensitivity vectors (numpy arrays).\n        variances (list): A list of corresponding noise variances.\n        m (int): The number of measurements to select.\n\n    Returns:\n        list: A list of indices of the selected measurements.\n    \"\"\"\n    num_candidates = len(sensitivities)\n    candidate_indices = set(range(num_candidates))\n    selected_indices = []\n\n    c_current = c_prior.copy()\n\n    for _ in range(m):\n        if not candidate_indices:\n            break\n\n        best_j = -1\n        max_score = -1.0\n\n        scores = {}\n        # Iterate through candidates in sorted order to ensure deterministic tie-breaking\n        for j in sorted(list(candidate_indices)):\n            s_j = sensitivities[j]\n            sigma_sq_j = variances[j]\n            \n            # Criterion: s_j^T * C_current * s_j / sigma_j^2\n            score = (s_j.T @ c_current @ s_j) / sigma_sq_j\n            \n            if score > max_score:\n                max_score = score\n                best_j = j\n        \n        if best_j == -1: # Should not happen with valid inputs\n            break\n\n        selected_indices.append(best_j)\n        candidate_indices.remove(best_j)\n\n        # Update the covariance matrix using the Sherman-Morrison formula\n        s_best = sensitivities[best_j]\n        sigma_sq_best = variances[best_j]\n\n        v = c_current @ s_best\n        denominator = sigma_sq_best + s_best.T @ v\n        \n        # Numerator is an outer product: v @ v.T\n        # In numpy, this is np.outer(v, v)\n        c_current -= np.outer(v, v) / denominator\n        \n    return selected_indices\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (\n            4, # p\n            np.diag([10.0, 10.0, 10.0, 10.0]), # C_prior\n            [\n                np.array([1.0, 0.0, 0.1, 0.0]),\n                np.array([0.9, 0.1, 0.0, 0.0]),\n                np.array([0.0, 1.0, 0.0, 0.2]),\n                np.array([0.0, 0.0, 1.0, 0.0]),\n                np.array([0.0, 0.0, 0.0, 1.0]),\n                np.array([0.95, 0.05, 0.0, 0.0]),\n            ], # sensitivities\n            [0.04, 0.05, 0.04, 0.02, 0.02, 0.05], # variances\n            3 # m\n        ),\n        # Test Case 2\n        (\n            3, # p\n            np.diag([50.0, 50.0, 50.0]), # C_prior\n            [\n                np.array([1.0, 0.0, 0.0]),\n                np.array([0.9, 0.1, 0.0]),\n                np.array([0.8, 0.2, 0.0]),\n                np.array([0.0, 1.0, 0.0]),\n                np.array([0.0, 0.0, 1.0]),\n                np.array([0.1, 0.1, 0.0]),\n            ], # sensitivities\n            [0.2, 0.2, 0.2, 0.2, 0.05, 0.2], # variances\n            3 # m\n        ),\n        # Test Case 3\n        (\n            3, # p\n            np.diag([1.0, 1.0, 1.0]), # C_prior\n            [\n                np.array([1.0, 0.0, 0.0]),\n                np.array([0.0, 1.0, 0.0]),\n                np.array([0.0, 0.0, 1.0]),\n                np.array([0.5, 0.5, 0.0]),\n                np.array([0.0, 0.5, 0.5]),\n            ], # sensitivities\n            [0.1, 0.1, 0.1, 0.1, 0.1], # variances\n            1 # m\n        ),\n        # Test Case 4\n        (\n            4, # p\n            np.diag([1.0, 1.0, 1.0, 100.0]), # C_prior\n            [\n                np.array([0.0, 0.0, 0.0, 1.0]),\n                np.array([0.1, 0.0, 0.0, 0.9]),\n                np.array([1.0, 0.0, 0.0, 0.0]),\n                np.array([0.0, 1.0, 0.0, 0.0]),\n                np.array([0.0, 0.0, 1.0, 0.0]),\n            ], # sensitivities\n            [0.05, 0.05, 0.05, 0.05, 0.05], # variances\n            2 # m\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        p, c_prior, sensitivities, variances, m = case\n        result = greedy_selection(p, c_prior, sensitivities, variances, m)\n        results.append(result)\n\n    # Format the output string to match the problem specification\n    # e.g., [[3,4,2],[4,0,3],[0],[0,2]]\n    str_results = [str(res).replace(\" \", \"\") for res in results]\n    final_output_string = f\"[{','.join(str_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}