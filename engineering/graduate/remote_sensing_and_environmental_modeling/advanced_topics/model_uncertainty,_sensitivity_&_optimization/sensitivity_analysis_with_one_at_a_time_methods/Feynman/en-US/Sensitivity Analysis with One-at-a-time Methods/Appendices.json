{
    "hands_on_practices": [
        {
            "introduction": "When performing a sensitivity analysis using numerical finite differences, the first practical question is always: 'how large should the parameter perturbation $h$ be?' This exercise dives into the fundamental trade-off that governs this choice, which is the balance between truncation error from the mathematical approximation and round-off error from finite-precision computer arithmetic. By analyzing these competing errors from first principles, you will derive a widely-used heuristic for selecting a near-optimal step size, a foundational skill for any numerical modeling work. ",
            "id": "3926791",
            "problem": "Consider a deterministic biomedical systems model $y(\\mathbf{p})$ that maps a vector of physiological parameters $\\mathbf{p} \\in \\mathbb{R}^m$ to a scalar biomarker prediction $y \\in \\mathbb{R}$. For local one-at-a-time (OAT) sensitivity analysis with respect to parameter $p_j$, a common finite-difference estimator is the forward difference\n$$\n\\widehat{S}_j(h) \\equiv \\frac{y(\\mathbf{p} + h \\,\\mathbf{e}_j) - y(\\mathbf{p})}{h},\n$$\nwhere $\\mathbf{e}_j$ is the $j$-th standard basis vector and $h>0$ is the perturbation step size in the $p_j$ direction. To choose $h$, you are asked to start from first principles:\n\n- Use the Taylor expansion of $y(\\mathbf{p} + h \\,\\mathbf{e}_j)$ about $\\mathbf{p}$ to justify a truncation error contribution that scales linearly in $h$ for the forward difference derivative approximation.\n- Use the standard floating-point model $\\operatorname{fl}(z) = z(1 + \\delta)$ with $|\\delta| \\le \\epsilon_{\\text{mach}}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon of the floating-point arithmetic, to justify a round-off error contribution that scales inversely with $h$ in the computed difference quotient.\n- Combine these contributions to motivate an error model of the form\n$$\nE(h) \\approx C_1 \\, h + \\frac{C_2}{h},\n$$\nfor suitable positive coefficients $C_1$ and $C_2$ that depend on local properties of $y$ and the arithmetic.\n\nYour task is to select the option that most accurately states the perturbation $h$ that minimizes the above model, together with a practically implementable heuristic for $h$ in terms of machine precision and parameter scaling, suitable for Institute of Electrical and Electronics Engineers (IEEE) 754 double precision arithmetic. The heuristic must be robust across parameters that vary in magnitude, respect physiological bounds (e.g., positivity constraints), and avoid underflow/overflow in $y(\\mathbf{p} + h \\,\\mathbf{e}_j)$. For concreteness, assume a nominal parameter value $p_j = 0.1$ and IEEE 754 double precision with machine epsilon $\\epsilon_{\\text{mach}} = 2^{-53} \\approx 1.11 \\times 10^{-16}$, and report the numerical value of the recommended $h$ for this case.\n\nWhich option is most appropriate?\n\nA. The minimizer of $E(h)$ satisfies $h^{\\star} = \\sqrt{C_2 / C_1}$. Since $C_1$ and $C_2$ are rarely known a priori in biomedical models, a robust heuristic is $h_j = \\sqrt{\\epsilon_{\\text{mach}}} \\,\\max\\!\\big(|p_j|,\\,1\\big)$, which blends relative perturbations for large $|p_j|$ and absolute perturbations for small $|p_j|$. Under IEEE 754 double, $\\sqrt{\\epsilon_{\\text{mach}}} = \\sqrt{2^{-53}} \\approx 1.05 \\times 10^{-8}$, so for $p_j = 0.1$ the recommended $h$ is $h \\approx 1.05 \\times 10^{-8}$, with subsequent clipping if needed to maintain physiological feasibility.\n\nB. The minimizer of $E(h)$ satisfies $h^{\\star} = C_2 / C_1$. To ensure safety against round-off, choose $h_j = \\epsilon_{\\text{mach}} \\,\\max\\!\\big(|p_j|,\\,1\\big)$. Under IEEE 754 double, $\\epsilon_{\\text{mach}} = 2^{-53} \\approx 1.11 \\times 10^{-16}$, so for $p_j = 0.1$ the recommended $h$ is $h \\approx 1.11 \\times 10^{-16}$.\n\nC. The minimizer of $E(h)$ satisfies $h^{\\star} = \\epsilon_{\\text{mach}}^{1/3}$. Use the central-difference optimal step universally even for forward differences: $h_j = \\epsilon_{\\text{mach}}^{1/3} \\,\\max\\!\\big(|p_j|,\\,1\\big)$. Under IEEE 754 double, $\\epsilon_{\\text{mach}}^{1/3} \\approx 4.80 \\times 10^{-6}$, so for $p_j = 0.1$ the recommended $h$ is $h \\approx 4.80 \\times 10^{-6}$.\n\nD. The minimizer of $E(h)$ satisfies $h^{\\star} = \\sqrt{C_1 / C_2}$. Because sensitivities are derivatives with respect to $p_j$, scale $h$ by the parameter: use $h_j = \\epsilon_{\\text{mach}} \\,|p_j|$. Under IEEE 754 double, for $p_j = 0.1$ the recommended $h$ is $h \\approx 1.11 \\times 10^{-17}$.",
            "solution": "The problem requires us to determine the optimal perturbation step size, $h$, for a forward-difference approximation of a derivative in the context of sensitivity analysis. This involves analyzing the trade-off between truncation error and round-off error.\n\n### Step 1: Derivation from First Principles\n\n**1. Truncation Error Analysis**\n\nThe problem statement asks to justify a truncation error that scales linearly with the step size $h$. We begin with the Taylor series expansion of the function $y(\\mathbf{p})$ around the point $\\mathbf{p}$, perturbed in the direction of the $j$-th parameter, $p_j$. Let us consider the single-variable function $f(p_j) = y(p_1, \\dots, p_j, \\dots, p_m)$. The expansion of $f(p_j+h)$ around $p_j$ is:\n$$\nf(p_j+h) = f(p_j) + h f'(p_j) + \\frac{h^2}{2!} f''(p_j) + \\frac{h^3}{3!} f'''(p_j) + \\dots\n$$\nIn the notation of the problem, this is:\n$$\ny(\\mathbf{p} + h\\mathbf{e}_j) = y(\\mathbf{p}) + h \\frac{\\partial y}{\\partial p_j}(\\mathbf{p}) + \\frac{h^2}{2} \\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p}) + O(h^3)\n$$\nThe true derivative (sensitivity) is $S_j = \\frac{\\partial y}{\\partial p_j}(\\mathbf{p})$. The forward difference estimator is defined as:\n$$\n\\widehat{S}_j(h) = \\frac{y(\\mathbf{p} + h\\mathbf{e}_j) - y(\\mathbf{p})}{h}\n$$\nSubstituting the Taylor expansion into the numerator:\n$$\n\\widehat{S}_j(h) = \\frac{\\left( y(\\mathbf{p}) + h \\frac{\\partial y}{\\partial p_j}(\\mathbf{p}) + \\frac{h^2}{2} \\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p}) + O(h^3) \\right) - y(\\mathbf{p})}{h}\n$$\n$$\n\\widehat{S}_j(h) = \\frac{h \\frac{\\partial y}{\\partial p_j}(\\mathbf{p}) + \\frac{h^2}{2} \\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p}) + O(h^3)}{h}\n$$\n$$\n\\widehat{S}_j(h) = \\frac{\\partial y}{\\partial p_j}(\\mathbf{p}) + \\frac{h}{2} \\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p}) + O(h^2)\n$$\nThe truncation error, $E_{\\text{trunc}}(h)$, is the difference between the approximation and the true value, $\\widehat{S}_j(h) - S_j$:\n$$\nE_{\\text{trunc}}(h) = \\frac{h}{2} \\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p}) + O(h^2)\n$$\nThe dominant term of the truncation error is linear in $h$. The magnitude of this error can be written as $|E_{\\text{trunc}}(h)| \\approx C_1 h$, where $C_1 = \\frac{1}{2} \\left| \\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p}) \\right|$. This confirms the first condition.\n\n**2. Round-off Error Analysis**\n\nThe problem asks to justify a round-off error that scales inversely with $h$. We use the floating-point arithmetic model $\\operatorname{fl}(z) = z(1+\\delta)$, with $|\\delta| \\le \\epsilon_{\\text{mach}}$.\nWhen the function values $y(\\mathbf{p})$ and $y(\\mathbf{p} + h\\mathbf{e}_j)$ are computed and stored, they are subject to round-off error:\n$$\n\\operatorname{fl}(y(\\mathbf{p})) = y(\\mathbf{p})(1+\\delta_1)\n$$\n$$\n\\operatorname{fl}(y(\\mathbf{p} + h\\mathbf{e}_j)) = y(\\mathbf{p} + h\\mathbf{e}_j)(1+\\delta_2)\n$$\nwhere $|\\delta_1|, |\\delta_2| \\le \\epsilon_{\\text{mach}}$.\nThe computed difference in the numerator is:\n$$\n\\Delta \\tilde{y} = \\operatorname{fl}(y(\\mathbf{p} + h\\mathbf{e}_j)) - \\operatorname{fl}(y(\\mathbf{p})) = y(\\mathbf{p} + h\\mathbf{e}_j)(1+\\delta_2) - y(\\mathbf{p})(1+\\delta_1)\n$$\nFor small $h$, $y(\\mathbf{p} + h\\mathbf{e}_j) \\approx y(\\mathbf{p})$. The error in the numerator is dominated by the term $y(\\mathbf{p}+h\\mathbf{e}_j)\\delta_2 - y(\\mathbf{p})\\delta_1 \\approx y(\\mathbf{p})(\\delta_2-\\delta_1)$. The magnitude of this error is bounded by $|y(\\mathbf{p})|(|\\delta_1|+|\\delta_2|) \\le 2|y(\\mathbf{p})|\\epsilon_{\\text{mach}}$. This error reflects the phenomenon of subtractive cancellation, where subtracting two nearly equal numbers results in a loss of relative precision.\nThe round-off error in the final computed quotient, $E_{\\text{round}}(h)$, is this numerator error divided by $h$:\n$$\n|E_{\\text{round}}(h)| \\approx \\frac{2 |y(\\mathbf{p})| \\epsilon_{\\text{mach}}}{h}\n$$\nThis can be written as $|E_{\\text{round}}(h)| \\approx \\frac{C_2}{h}$, where $C_2 = 2 |y(\\mathbf{p})| \\epsilon_{\\text{mach}}$. This confirms the second condition, showing that the round-off error component scales inversely with $h$.\n\n**3. Optimal Step Size Calculation**\n\nThe total absolute error, $E(h)$, is the sum of the magnitudes of the truncation and round-off errors:\n$$\nE(h) \\approx |E_{\\text{trunc}}(h)| + |E_{\\text{round}}(h)| = C_1 h + \\frac{C_2}{h}\n$$\nTo find the step size $h^{\\star}$ that minimizes this total error, we differentiate $E(h)$ with respect to $h$ and set the result to zero:\n$$\n\\frac{dE}{dh} = C_1 - \\frac{C_2}{h^2} = 0\n$$\n$$\nC_1 = \\frac{C_2}{h^2} \\implies h^2 = \\frac{C_2}{C_1}\n$$\nThus, the optimal step size is:\n$$\nh^{\\star} = \\sqrt{\\frac{C_2}{C_1}}\n$$\nThe second derivative is $\\frac{d^2E}{dh^2} = \\frac{2C_2}{h^3}$, which is positive for $h>0$, confirming that this is a minimum.\n\n**4. Heuristic for Practical Implementation**\n\nThe optimal step size $h^{\\star}$ depends on $C_1$ and $C_2$, which in turn depend on the second derivative and the value of the function, quantities that are typically unknown.\n$$\nh^{\\star} = \\sqrt{\\frac{2 |y(\\mathbf{p})| \\epsilon_{\\text{mach}}}{\\frac{1}{2}\\left| \\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p}) \\right|}} = \\sqrt{\\epsilon_{\\text{mach}}} \\sqrt{\\frac{4|y(\\mathbf{p})|}{\\left| \\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p}) \\right|}}\n$$\nA common heuristic assumes that for a well-scaled problem, the quantities $|y(\\mathbf{p})|$ and $|\\frac{\\partial^2 y}{\\partial p_j^2}(\\mathbf{p})|$ are of order $1$. This leads to $h^{\\star} \\propto \\sqrt{\\epsilon_{\\text{mach}}}$.\n\nTo be robust to the scale of the parameter $p_j$ itself, the step size should be relative for large $|p_j|$ and absolute for small $|p_j|$. A purely relative step $h=\\eta|p_j|$ would fail for $p_j=0$. A purely absolute step $h=\\eta$ would be too large for parameters whose natural scale is much less than $1$, and too small for parameters with a large scale. A practical way to combine these is:\n$$\nh_j = \\eta \\cdot \\max(|p_j|, \\text{typical_scale})\n$$\nSetting $\\eta = \\sqrt{\\epsilon_{\\text{mach}}}$ and using a default typical scale of $1$ gives the robust heuristic:\n$$\nh_j = \\sqrt{\\epsilon_{\\text{mach}}} \\cdot \\max(|p_j|, 1)\n$$\n\n**5. Application to the Concrete Case**\nGiven $p_j = 0.1$ and IEEE 754 double precision with $\\epsilon_{\\text{mach}} = 2^{-53} \\approx 1.11 \\times 10^{-16}$.\nFirst, calculate $\\sqrt{\\epsilon_{\\text{mach}}}$:\n$$\n\\sqrt{\\epsilon_{\\text{mach}}} = \\sqrt{2^{-53}} = 2^{-26.5} \\approx 1.054 \\times 10^{-8}\n$$\nNext, apply the heuristic:\n$$\nh = \\sqrt{\\epsilon_{\\text{mach}}} \\cdot \\max(|0.1|, 1) = (1.054 \\times 10^{-8}) \\cdot 1 = 1.054 \\times 10^{-8}\n$$\nRounding to two significant figures, $h \\approx 1.05 \\times 10^{-8}$.\n\n### Option-by-Option Analysis\n\n**A. The minimizer of $E(h)$ satisfies $h^{\\star} = \\sqrt{C_2 / C_1}$. Since $C_1$ and $C_2$ are rarely known a priori in biomedical models, a robust heuristic is $h_j = \\sqrt{\\epsilon_{\\text{mach}}} \\,\\max\\!\\big(|p_j|,\\,1\\big)$, which blends relative perturbations for large $|p_j|$ and absolute perturbations for small $|p_j|$. Under IEEE 754 double, $\\sqrt{\\epsilon_{\\text{mach}}} = \\sqrt{2^{-53}} \\approx 1.05 \\times 10^{-8}$, so for $p_j = 0.1$ the recommended $h$ is $h \\approx 1.05 \\times 10^{-8}$, with subsequent clipping if needed to maintain physiological feasibility.**\n- **Minimizer formula**: $h^{\\star} = \\sqrt{C_2 / C_1}$ is correct, as derived above.\n- **Heuristic**: The formula $h_j = \\sqrt{\\epsilon_{\\text{mach}}} \\,\\max\\!\\big(|p_j|,\\,1\\big)$ is a standard, well-justified, and robust heuristic.\n- **Rationale**: The explanation of blending relative and absolute perturbations is correct.\n- **Calculation**: $\\sqrt{\\epsilon_{\\text{mach}}} \\approx 1.05 \\times 10^{-8}$ is correct. For $p_j=0.1$, $h = (1.05 \\times 10^{-8}) \\cdot \\max(0.1, 1) = 1.05 \\times 10^{-8}$. The calculation is correct.\n- **Practicality**: The mention of clipping for physiological feasibility enhances the correctness of the option in a biomedical context.\n**Verdict:** **Correct**.\n\n**B. The minimizer of $E(h)$ satisfies $h^{\\star} = C_2 / C_1$. To ensure safety against round-off, choose $h_j = \\epsilon_{\\text{mach}} \\,\\max\\!\\big(|p_j|,\\,1\\big)$. Under IEEE 754 double, $\\epsilon_{\\text{mach}} = 2^{-53} \\approx 1.11 \\times 10^{-16}$, so for $p_j = 0.1$ the recommended $h$ is $h \\approx 1.11 \\times 10^{-16}$.**\n- **Minimizer formula**: $h^{\\star} = C_2 / C_1$ is incorrect. It is missing the square root. The correct formula is $h^{\\star} = \\sqrt{C_2 / C_1}$.\n- **Heuristic**: The proportionality factor is given as $\\epsilon_{\\text{mach}}$ instead of $\\sqrt{\\epsilon_{\\text{mach}}}$. This would result in an extremely small step size, dominated by round-off error.\n**Verdict:** **Incorrect**.\n\n**C. The minimizer of $E(h)$ satisfies $h^{\\star} = \\epsilon_{\\text{mach}}^{1/3}$. Use the central-difference optimal step universally even for forward differences: $h_j = \\epsilon_{\\text{mach}}^{1/3} \\,\\max\\!\\big(|p_j|,\\,1\\big)$. Under IEEE 754 double, $\\epsilon_{\\text{mach}}^{1/3} \\approx 4.80 \\times 10^{-6}$, so for $p_j = 0.1$ the recommended $h$ is $h \\approx 4.80 \\times 10^{-6}$.**\n- **Minimizer formula and heuristic**: The optimal step size for a forward difference scheme scales with $\\epsilon_{\\text{mach}}^{1/2}$, not $\\epsilon_{\\text{mach}}^{1/3}$. The scaling $\\epsilon_{\\text{mach}}^{1/3}$ is appropriate for a central difference scheme, where the truncation error is $O(h^2)$. The problem explicitly specifies a forward difference.\n**Verdict:** **Incorrect**.\n\n**D. The minimizer of $E(h)$ satisfies $h^{\\star} = \\sqrt{C_1 / C_2}$. Because sensitivities are derivatives with respect to $p_j$, scale $h$ by the parameter: use $h_j = \\epsilon_{\\text{mach}} \\,|p_j|$. Under IEEE 754 double, for $p_j = 0.1$ the recommended $h$ is $h \\approx 1.11 \\times 10^{-17}$.**\n- **Minimizer formula**: $h^{\\star} = \\sqrt{C_1 / C_2}$ is incorrect. It is the reciprocal of the correct formula.\n- **Heuristic**: The formula uses $\\epsilon_{\\text{mach}}$ instead of $\\sqrt{\\epsilon_{\\text{mach}}}$, which is incorrect. It also uses a purely relative step $|p_j|$, which is not robust for $p_j$ near zero.\n**Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Building on the theory of optimal step size, we now consider broader strategic choices for robust One-at-a-Time (OAT) analysis in the context of complex models. Real-world systems, whether in biology or environmental science, often have parameters that must remain positive or that vary by many orders of magnitude, challenges that can easily derail a naive sensitivity analysis. This exercise guides you to evaluate and select robust strategies, such as logarithmic parameter transformations, that are essential for obtaining reliable sensitivity estimates from realistic models. ",
            "id": "3926736",
            "problem": "A receptorâ€“ligand binding system is modeled by ordinary differential equations (ODEs) under mass-action kinetics. Let $R$ denote free receptor, $L$ ligand (assumed constant), and $C$ complex. The total receptor is $R_{\\mathrm{T}}$, so $R = R_{\\mathrm{T}} - C$. The binding kinetics are governed by strictly positive rate constants $k_{\\mathrm{on}}$ and $k_{\\mathrm{off}}$, and the complex evolves according to\n$$\n\\frac{dC}{dt} = k_{\\mathrm{on}}\\,L\\,(R_{\\mathrm{T}} - C) - k_{\\mathrm{off}}\\,C,\\quad C(0) = 0,\\quad k_{\\mathrm{on}} > 0,\\; k_{\\mathrm{off}} > 0.\n$$\nAn experiment reports the occupancy $y(t^\\ast) = C(t^\\ast)/R_{\\mathrm{T}}$ at a fixed time $t^\\ast > 0$. For a nominal parameter vector $\\theta_0 = (k_{\\mathrm{on},0}, k_{\\mathrm{off},0})$, a local one-at-a-time sensitivity analysis of $y(t^\\ast)$ with respect to each parameter is planned, using finite-difference approximations to partial derivatives. You are told that the system can exhibit saturation of occupancy for large $k_{\\mathrm{on}}\\,L/k_{\\mathrm{off}}$, and that numerical solvers may introduce finite precision effects for very small perturbations.\n\nUnder the constraints that $k_{\\mathrm{on}}$ and $k_{\\mathrm{off}}$ must remain strictly positive during perturbations, and recognizing that $y(t^\\ast)$ may be in a saturating regime, which of the following step-selection strategies are both theoretically justified from first principles and practically robust for estimating local, one-at-a-time parameter sensitivities at $\\theta_0$? Select all that apply.\n\nA. For each parameter $\\theta_i \\in \\{k_{\\mathrm{on}}, k_{\\mathrm{off}}\\}$, transform to $\\phi_i = \\ln \\theta_i$ and use a small symmetric perturbation $\\pm h$ in $\\phi_i$ with the other parameter fixed at its nominal value, where $h$ is chosen the same for both parameters (e.g., $h$ in the range $0.01$ to $0.1$), then compute and report sensitivities with respect to $\\phi_i$.\n\nB. For each parameter, use a fixed absolute central difference step $\\pm \\delta$ in the original parameter, with the same $\\delta$ for $k_{\\mathrm{on}}$ and $k_{\\mathrm{off}}$, selected as $\\delta = 10^{-6}$ in the corresponding units, regardless of the nominal parameter magnitudes.\n\nC. Use a multiplicative step of factor $10$ (i.e., $+h$ corresponds to multiplying by $10$ and $-h$ by dividing by $10$) in the log-parameters to avoid underflow, reasoning that larger steps improve numerical stability and overcome saturation-induced flatness.\n\nD. Choose the perturbation in $\\phi_i = \\ln \\theta_i$ adaptively so that the resulting change in occupancy magnitude satisfies $|\\Delta y| \\approx 0.01\\,\\max\\{1, |y|\\}$ when stepping by $\\pm h$ in $\\phi_i$ (with other parameters fixed), and verify near-symmetry between the $+h$ and $-h$ responses; if asymmetry is substantial, reduce $h$ until the responses are approximately symmetric.\n\nE. Enforce positivity by reparameterizing as $\\psi_i = \\theta_i^2$ and applying a symmetric central difference $\\pm \\eta$ in $\\psi_i$ with the same $\\eta$ for both parameters, then mapping the resulting derivative back to the original parameters.",
            "solution": "The objective is to identify theoretically justified and practically robust strategies for local, one-at-a-time sensitivity analysis of the output $y(t^\\ast)$ with respect to parameters $\\theta_i \\in \\{k_{\\mathrm{on}}, k_{\\mathrm{off}}\\}$. The estimation is to be done using finite-difference approximations. The key challenges to address are:\n1.  The positivity constraint: $k_{\\mathrm{on}} > 0$ and $k_{\\mathrm{off}} > 0$.\n2.  Potential for vastly different parameter scales (magnitudes) for $k_{\\mathrm{on}}$ and $k_{\\mathrm{off}}$.\n3.  Potential for output saturation, where the output $y(t^\\ast)$ is nearly flat with respect to parameter changes, making numerical differentiation prone to catastrophic cancellation errors.\n4.  The need for the analysis to be *local*, meaning the derivative is approximated at the nominal point $\\theta_0$, not over a large interval.\n\nWe will now evaluate each proposed strategy against these principles.\n\n**A. For each parameter $\\theta_i \\in \\{k_{\\mathrm{on}}, k_{\\mathrm{off}}\\}$, transform to $\\phi_i = \\ln \\theta_i$ and use a small symmetric perturbation $\\pm h$ in $\\phi_i$ ...**\n*   **Analysis**: This strategy employs a logarithmic transformation of the parameters. A perturbation $\\phi_i \\pm h$ corresponds to a multiplicative perturbation $\\theta_i' = e^{\\phi_i \\pm h} = e^{\\phi_i}e^{\\pm h} = \\theta_i e^{\\pm h}$ in the original parameter space. This naturally enforces the positivity constraint ($e^x > 0$ for any real $x$). Using a fixed step $h$ in log-space corresponds to a fixed *relative* perturbation in the original parameters, which is the standard and correct way to handle parameters that may span several orders of magnitude. The proposed step sizes are small enough to approximate a local derivative. This is a standard, robust, and theoretically sound method.\n*   **Verdict**: **Correct**.\n\n**B. For each parameter, use a fixed absolute central difference step $\\pm \\delta$ in the original parameter...**\n*   **Analysis**: This strategy uses a fixed absolute perturbation $\\theta_i \\pm \\delta$. It fails on two counts. First, it can violate the positivity constraint if a parameter's nominal value is small (e.g., $\\theta_{i,0} - \\delta \\le 0$). Second, it does not respect parameter scale. Using the same absolute step for parameters that differ by orders of magnitude results in a perturbation that is either too small (causing numerical error) or too large (violating locality) for at least one of the parameters.\n*   **Verdict**: **Incorrect**.\n\n**C. Use a multiplicative step of factor $10$ ...**\n*   **Analysis**: This strategy proposes perturbing a parameter by an order of magnitude ($\\theta_i \\to 10\\theta_i$ and $\\theta_i \\to 0.1\\theta_i$). While this overcomes numerical cancellation on a flat response surface, it is a very large step. It violates the core principle of *local* sensitivity analysis, which aims to approximate the derivative at a single point. This method calculates the slope of a secant over a vast interval, which is not the local sensitivity.\n*   **Verdict**: **Incorrect**.\n\n**D. Choose the perturbation in $\\phi_i = \\ln \\theta_i$ adaptively ... and verify near-symmetry ...**\n*   **Analysis**: This describes a sophisticated adaptive strategy. It correctly starts with the log-transform $\\phi_i = \\ln \\theta_i$, inheriting the benefits of positivity enforcement and appropriate scaling. The key idea is to adapt the step size $h$ to achieve a target change in the output. This robustly handles the saturation problem by automatically choosing a larger step if the response is flat, but only as large as needed. The check for response symmetry is a crucial self-consistency test to ensure the step size remains in the local, linear regime. If asymmetry is found, the strategy correctly prescribes reducing $h$. This represents a best-practices approach that is both theoretically justified and practically robust.\n*   **Verdict**: **Correct**.\n\n**E. Enforce positivity by reparameterizing as $\\psi_i = \\theta_i^2$ ...**\n*   **Analysis**: This suggests a quadratic reparameterization, $\\theta_i = \\sqrt{\\psi_i}$. While it can enforce positivity (if handled carefully to avoid $\\psi_i \\pm \\eta$ becoming negative), it has poor scaling properties. A fixed perturbation $\\eta$ in the squared parameter $\\psi_i$ leads to a perturbation in $\\theta_i$ of size $\\Delta\\theta_i \\approx \\pm \\frac{\\eta}{2\\theta_i}$. The step size in the original parameter is *inversely* proportional to its magnitude. For a parameter with a large magnitude, the step is minuscule; for a parameter with a small magnitude, the step is enormous. This is the opposite of what is required and exacerbates scaling issues.\n*   **Verdict**: **Incorrect**.\n\nIn conclusion, strategies A and D are both theoretically sound and practically robust methods for performing the requested sensitivity analysis. Strategy A is a widely used standard practice, and strategy D is a more advanced, adaptive version of A that is even more robust.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "While finite differences are common, a more elegant and often more accurate approach is the sensitivity equation method, which calculates sensitivities by solving an augmented system of differential equations directly. This hands-on coding exercise challenges you to implement this advanced method for a dynamic system, completely bypassing the issue of step-size selection. Furthermore, you will use the computed sensitivity matrix to investigate parameter identifiability, a critical application of sensitivity analysis that helps determine which parameters can be reliably estimated from a given dataset. ",
            "id": "3926762",
            "problem": "Consider the Bergman minimal glucose-insulin model for an intravenous glucose tolerance test in which the plasma glucose concentration is represented by the state variable $G(t)$ and the remote insulin effect is represented by the state variable $X(t)$. The governing ordinary differential equations are\n$$\n\\frac{dG}{dt} = -p_1\\,(G - G_b) - X\\,G,\n$$\n$$\n\\frac{dX}{dt} = -p_2\\,X + p_3\\,(I(t) - I_b),\n$$\nwhere $p_1, p_2, p_3$ are unknown parameters to be analyzed, $G_b$ is the basal glucose concentration, $I_b$ is the basal insulin concentration, and $I(t)$ is a known exogenous insulin input function. Assume the initial conditions $G(0) = G_0$ and $X(0) = X_0$ are independent of the parameters $p_1, p_2, p_3$. The measured output of interest is the glucose trajectory sampled at specified times, i.e., $y_j = G(t_j)$.\n\nLocal one-at-a-time sensitivity analysis for the parameters $p_1, p_2, p_3$ is to be performed via sensitivity equations. For each parameter $p_i$, define sensitivity variables $S_{G,p_i}(t) = \\frac{\\partial G(t)}{\\partial p_i}$ and $S_{X,p_i}(t) = \\frac{\\partial X(t)}{\\partial p_i}$. These sensitivities satisfy first-order ordinary differential equations obtained by differentiating the model with respect to $p_i$. Using these sensitivity equations, construct numerically the Jacobian matrix $J(\\theta^*) \\in \\mathbb{R}^{m \\times 3}$, where $\\theta^* = (p_1, p_2, p_3)$ and $m$ is the number of sampling times, with entries\n$$\nJ_{j,i}(\\theta^*) = \\left.\\frac{\\partial G(t_j)}{\\partial p_i}\\right|_{\\theta = \\theta^*} = S_{G,p_i}(t_j).\n$$\nDetermine the matrix rank of $J(\\theta^*)$.\n\nYour program must implement the sensitivity equations rigorously (not by finite differences) and compute $J(\\theta^*)$ based on an integration of the augmented system consisting of the original state equations and the sensitivity equations. Use precise numerical integration appropriate for stiff or non-stiff systems as needed.\n\nFor all cases below, set the basal values to $G_b = 90$ and $I_b = 10$, and use parameter vector $\\theta^* = (p_1, p_2, p_3) = (0.01, 0.02, 0.0005)$. The insulin input is modeled as\n$$\nI(t) = I_b + A\\,e^{-\\alpha t},\n$$\nwhere $A$ and $\\alpha$ are specified per test case.\n\nTest Suite:\n- Case 1 (general case): Sampling times $t_j = [5, 10, 20, 40, 60]$ (minutes), initial conditions $G_0 = 180$, $X_0 = 0$, insulin parameters $A = 50$, $\\alpha = 0.1$.\n- Case 2 (boundary case with no insulin variation): Sampling times $t_j = [5, 10, 20, 40, 60]$, initial conditions $G_0 = 150$, $X_0 = 0$, insulin parameters $A = 0$, $\\alpha = 0.1$.\n- Case 3 (edge case with sparse sampling): Sampling times $t_j = [1, 2]$, initial conditions $G_0 = 200$, $X_0 = 0$, insulin parameters $A = 50$, $\\alpha = 0.1$.\n- Case 4 (early-time sampling with small insulin perturbation): Sampling times $t_j = [0.5, 1.0, 1.5, 2.0, 3.0]$, initial conditions $G_0 = 90$, $X_0 = 0$, insulin parameters $A = 1$, $\\alpha = 0.2$.\n\nFor each case, construct $J(\\theta^*)$ from the computed sensitivities at the specified sampling times and return the matrix rank as an integer. The final output must be a single line containing the ranks of the four cases as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4]\"), where each $r_k$ is an integer. No physical units are required in the output, and no angles or percentages are involved. The program must be complete and runnable as-is, with no external input required.",
            "solution": "The core of this problem is to perform a local sensitivity analysis of the Bergman minimal model with respect to its parameters $p_1, p_2, p_3$. The sensitivity of the state variables to these parameters is quantified by their partial derivatives, which are themselves functions of time. The governing equations for these sensitivity functions are derived by differentiating the original model's ODEs with respect to each parameter. This results in a larger, augmented system of ODEs that can be solved numerically.\n\nThe state variables are $G(t)$ and $X(t)$, governed by:\n$$\n\\frac{dG}{dt} = f_G(G, X, p_1, p_2, p_3) = -p_1(G - G_b) - XG\n$$\n$$\n\\frac{dX}{dt} = f_X(X, p_1, p_2, p_3, t) = -p_2 X + p_3(I(t) - I_b)\n$$\nThe sensitivity variables are defined as $S_{G,p_i} = \\frac{\\partial G}{\\partial p_i}$ and $S_{X,p_i} = \\frac{\\partial X}{\\partial p_i}$ for $i=1,2,3$. Their dynamics are found by applying the chain rule, for example $\\frac{d}{dt} S_{G,p_i} = \\frac{\\partial f_G}{\\partial G}S_{G,p_i} + \\frac{\\partial f_G}{\\partial X}S_{X,p_i} + \\frac{\\partial f_G}{\\partial p_i}$. The required partial derivatives of the right-hand side functions are:\n- $\\frac{\\partial f_G}{\\partial G} = -p_1 - X$\n- $\\frac{\\partial f_G}{\\partial X} = -G$\n- $\\frac{\\partial f_G}{\\partial p_1} = -(G - G_b)$\n- $\\frac{\\partial f_G}{\\partial p_2} = 0$\n- $\\frac{\\partial f_G}{\\partial p_3} = 0$\n- $\\frac{\\partial f_X}{\\partial G} = 0$\n- $\\frac{\\partial f_X}{\\partial X} = -p_2$\n- $\\frac{\\partial f_X}{\\partial p_1} = 0$\n- $\\frac{\\partial f_X}{\\partial p_2} = -X$\n- $\\frac{\\partial f_X}{\\partial p_3} = I(t) - I_b$\n\nUsing these, we derive the set of ODEs for the sensitivities:\n1.  **Sensitivity to $p_1$**:\n    $$\n    \\frac{dS_{G,p_1}}{dt} = (-p_1 - X)S_{G,p_1} - G S_{X,p_1} - (G - G_b)\n    $$\n    $$\n    \\frac{dS_{X,p_1}}{dt} = -p_2 S_{X,p_1}\n    $$\n2.  **Sensitivity to $p_2$**:\n    $$\n    \\frac{dS_{G,p_2}}{dt} = (-p_1 - X)S_{G,p_2} - G S_{X,p_2}\n    $$\n    $$\n    \\frac{dS_{X,p_2}}{dt} = -p_2 S_{X,p_2} - X\n    $$\n3.  **Sensitivity to $p_3$**:\n    $$\n    \\frac{dS_{G,p_3}}{dt} = (-p_1 - X)S_{G,p_3} - G S_{X,p_3}\n    $$\n    $$\n    \\frac{dS_{X,p_3}}{dt} = -p_2 S_{X,p_3} + (I(t) - I_b)\n    $$\nThe problem states that the initial conditions $G(0)=G_0$ and $X(0)=X_0$ are independent of the parameters $p_i$. This implies that the initial sensitivities are zero:\n$$\nS_{G,p_i}(0) = \\frac{\\partial G_0}{\\partial p_i} = 0\n$$\n$$\nS_{X,p_i}(0) = \\frac{\\partial X_0}{\\partial p_i} = 0\n$$\nfor $i=1, 2, 3$.\n\nWe now construct an augmented state vector $Y(t) \\in \\mathbb{R}^8$:\n$$\nY(t) = [G, X, S_{G,p_1}, S_{X,p_1}, S_{G,p_2}, S_{X,p_2}, S_{G,p_3}, S_{X,p_3}]^T\n$$\nThe dynamics of this augmented system, $\\frac{dY}{dt} = F(t, Y)$, are given by the two original state equations and the six sensitivity equations. The initial condition vector is $Y(0) = [G_0, X_0, 0, 0, 0, 0, 0, 0]^T$.\n\nTo solve the problem, we integrate this $8$-dimensional ODE system from $t=0$ to the latest sampling time for each test case. We use a robust numerical solver, such as `scipy.integrate.solve_ivp`, which can handle potential stiffness in the system. The solver is configured to output the solution at the specific sampling times $t_j$.\n\nFrom the numerical solution, we extract the values of the glucose sensitivities $S_{G,p_i}(t_j)$ for $i=1,2,3$ at each sampling time $t_j$ for $j=1, \\dots, m$. These values form the $m \\times 3$ Jacobian matrix $J$:\n$$\nJ = \n\\begin{pmatrix}\n S_{G,p_1}(t_1) & S_{G,p_2}(t_1) & S_{G,p_3}(t_1) \\\\\n S_{G,p_1}(t_2) & S_{G,p_2}(t_2) & S_{G,p_3}(t_2) \\\\\n \\vdots & \\vdots & \\vdots \\\\\n S_{G,p_1}(t_m) & S_{G,p_2}(t_m) & S_{G,p_3}(t_m)\n\\end{pmatrix}\n$$\nThe final step is to compute the rank of this matrix $J$ using a standard numerical linear algebra routine, such as `numpy.linalg.matrix_rank`, which typically employs Singular Value Decomposition (SVD) for a numerically stable computation. This process is repeated for each of the four test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the Bergman minimal model sensitivity analysis problem for four test cases.\n    \"\"\"\n    \n    # Global parameters and constants\n    p1_star, p2_star, p3_star = 0.01, 0.02, 0.0005\n    Gb, Ib = 90.0, 10.0\n\n    # Test cases as defined in the problem statement\n    test_cases = [\n        {\n            \"t_j\": [5, 10, 20, 40, 60],\n            \"G0\": 180.0, \"X0\": 0.0,\n            \"A\": 50.0, \"alpha\": 0.1\n        },\n        {\n            \"t_j\": [5, 10, 20, 40, 60],\n            \"G0\": 150.0, \"X0\": 0.0,\n            \"A\": 0.0, \"alpha\": 0.1\n        },\n        {\n            \"t_j\": [1.0, 2.0],\n            \"G0\": 200.0, \"X0\": 0.0,\n            \"A\": 50.0, \"alpha\": 0.1\n        },\n        {\n            \"t_j\": [0.5, 1.0, 1.5, 2.0, 3.0],\n            \"G0\": 90.0, \"X0\": 0.0,\n            \"A\": 1.0, \"alpha\": 0.2\n        }\n    ]\n\n    def ode_system(t, y, p1, p2, p3, Gb_val, Ib_val, A_val, alpha_val):\n        \"\"\"\n        Defines the augmented ODE system including states and sensitivities.\n        State vector y:\n        y[0]: G(t)\n        y[1]: X(t)\n        y[2]: S_G,p1(t)\n        y[3]: S_X,p1(t)\n        y[4]: S_G,p2(t)\n        y[5]: S_X,p2(t)\n        y[6]: S_G,p3(t)\n        y[7]: S_X,p3(t)\n        \"\"\"\n        G, X, SGp1, SXp1, SGp2, SXp2, SGp3, SXp3 = y\n        \n        # Insulin input\n        I_minus_Ib = A_val * np.exp(-alpha_val * t)\n\n        # Original state equations\n        dG_dt = -p1 * (G - Gb_val) - X * G\n        dX_dt = -p2 * X + p3 * I_minus_Ib\n        \n        # Sensitivity equations\n        # w.r.t p1\n        dSGp1_dt = (-p1 - X) * SGp1 - G * SXp1 - (G - Gb_val)\n        dS_Xp1_dt = -p2 * SXp1\n        \n        # w.r.t p2\n        dSGp2_dt = (-p1 - X) * SGp2 - G * SXp2\n        dSXp2_dt = -p2 * SXp2 - X\n\n        # w.r t p3\n        dSGp3_dt = (-p1 - X) * SGp3 - G * SXp3\n        dSXp3_dt = -p2 * SXp3 + I_minus_Ib\n\n        return [dG_dt, dX_dt, dSGp1_dt, dS_Xp1_dt, dSGp2_dt, dSXp2_dt, dSGp3_dt, dSXp3_dt]\n\n    results = []\n    \n    for case in test_cases:\n        # Unpack case parameters\n        t_j = case[\"t_j\"]\n        G0, X0 = case[\"G0\"], case[\"X0\"]\n        A, alpha = case[\"A\"], case[\"alpha\"]\n\n        # Initial conditions for the augmented system\n        y0 = [G0, X0, 0, 0, 0, 0, 0, 0]\n        \n        # Time span for integration\n        t_span = (0, max(t_j))\n        \n        # Parameters to pass to the ODE function\n        args = (p1_star, p2_star, p3_star, Gb, Ib, A, alpha)\n        \n        # Solve the augmented ODE system\n        # LSODA is a good choice for systems that may be stiff or non-stiff.\n        sol = solve_ivp(\n            ode_system, \n            t_span, \n            y0, \n            method='LSODA', \n            t_eval=t_j, \n            args=args,\n            rtol=1e-6, \n            atol=1e-9\n        )\n        \n        # Extract sensitivities for glucose w.r.t. p1, p2, p3\n        # SGp1 is at index 2, SGp2 at 4, SGp3 at 6\n        SGp1_vals = sol.y[2]\n        SGp2_vals = sol.y[4]\n        SGp3_vals = sol.y[6]\n        \n        # Construct the Jacobian (Sensitivity) matrix J\n        J = np.vstack([SGp1_vals, SGp2_vals, SGp3_vals]).T\n        \n        # Compute the rank of J\n        rank = np.linalg.matrix_rank(J)\n        results.append(rank)\n\n    # Print the final result in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}