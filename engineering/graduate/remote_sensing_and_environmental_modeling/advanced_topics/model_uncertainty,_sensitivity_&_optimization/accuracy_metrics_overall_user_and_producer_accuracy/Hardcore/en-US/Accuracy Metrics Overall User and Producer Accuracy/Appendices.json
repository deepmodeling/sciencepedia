{
    "hands_on_practices": [
        {
            "introduction": "The confusion matrix is the fundamental tool for evaluating a classification's performance. This first exercise  will ground your understanding by having you derive the estimators for Overall Accuracy ($OA$), User's Accuracy ($UA$), and Producer's Accuracy ($PA$) directly from their definitions as conditional probabilities. By working through a validation scenario based on Simple Random Sampling, you will not only calculate these critical metrics but also justify the statistical assumptions that underpin their confidence intervals.",
            "id": "3794298",
            "problem": "A land cover classification for a coastal watershed is produced from multispectral satellite imagery and validated against expert-interpreted reference labels derived from high-resolution aerial photography. Validation points are selected using Simple Random Sampling (SRS), with equal inclusion probability over the mapped area and a minimum separation distance to suppress short-range spatial autocorrelation. Each validation point carries two categorical labels: the map-predicted class and the independently determined reference class. Four classes are considered: Forest (F), Cropland (C), Wetland (W), and Built-up (B). Rows enumerate reference classes, columns enumerate map-predicted classes, in the order $(F,C,W,B)$. The resulting $4 \\times 4$ sample confusion matrix for $N=500$ validated points is:\n$$\n\\begin{array}{c|cccc}\n & F & C & W & B \\\\\n\\hline\nF & 170 & 12 & 18 & 5 \\\\\nC & 14 & 120 & 9 & 7 \\\\\nW & 8 & 11 & 62 & 4 \\\\\nB & 3 & 9 & 7 & 41 \\\\\n\\end{array}\n$$\nStarting from first principles of probability and sampling under SRS, use the core definitions of accuracy as conditional probabilities to derive the estimators for Overall Accuracy (OA), User’s Accuracy (UA), and Producer’s Accuracy (PA), and then compute their values from the given matrix. Express all accuracy metrics as decimal numbers and round your answers to four significant figures. In your derivation, explicitly justify why, under this SRS design, binomial confidence intervals are appropriate for UA and PA.\n\nReport the final answer as a single row matrix in the following order: $($OA, UA for $F$, UA for $C$, UA for $W$, UA for $B$, PA for $F$, PA for $C$, PA for $W$, PA for $B)$. No units should be included in the final numerical answer.",
            "solution": "The problem is valid as it represents a standard, well-posed analysis of a confusion matrix derived from a Simple Random Sampling (SRS) design in the context of remote sensing accuracy assessment. All necessary data are provided, and the task requires deriving and applying fundamental statistical concepts.\n\nThe problem statement and the main text of this article adopt the convention where rows of the confusion matrix represent the **reference** classes and columns represent the **map-predicted** classes.\n\nLet the set of land cover classes be $\\mathcal{C} = \\{F, C, W, B\\}$. Let $i$ index the reference class (rows) and $j$ index the map class (columns), where $i, j \\in \\{1, 2, 3, 4\\}$ correspond to the ordered tuple $(F, C, W, B)$. The confusion matrix provides the sample counts $n_{ij}$ for each combination of reference class $i$ and map class $j$. The total number of validation points is $N = \\sum_{i} \\sum_{j} n_{ij} = 500$.\n\nLet $n_{i+} = \\sum_{j} n_{ij}$ be the total count for reference class $i$ (row sum) and $n_{+j} = \\sum_{i} n_{ij}$ be the total count for map-predicted class $j$ (column sum).\n\nThe given matrix of counts is:\n$$\n\\mathbf{N} =\n\\begin{pmatrix}\nn_{11} & n_{12} & n_{13} & n_{14} \\\\\nn_{21} & n_{22} & n_{23} & n_{24} \\\\\nn_{31} & n_{32} & n_{33} & n_{34} \\\\\nn_{41} & n_{42} & n_{43} & n_{44}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n170 & 12 & 18 & 5 \\\\\n14 & 120 & 9 & 7 \\\\\n8 & 11 & 62 & 4 \\\\\n3 & 9 & 7 & 41\n\\end{pmatrix}\n$$\n\nThe row sums (reference totals) are:\n$n_{1+} = n_{F+} = 170 + 12 + 18 + 5 = 205$\n$n_{2+} = n_{C+} = 14 + 120 + 9 + 7 = 150$\n$n_{3+} = n_{W+} = 8 + 11 + 62 + 4 = 85$\n$n_{4+} = n_{B+} = 3 + 9 + 7 + 41 = 60$\n\nThe column sums (map totals) are:\n$n_{+1} = n_{+F} = 170 + 14 + 8 + 3 = 195$\n$n_{+2} = n_{+C} = 12 + 120 + 11 + 9 = 152$\n$n_{+3} = n_{+W} = 18 + 9 + 62 + 7 = 96$\n$n_{+4} = n_{+B} = 5 + 7 + 4 + 41 = 57$\n\nThe sum of both row and column totals is $N=500$.\n\n**Derivation of Accuracy Estimators**\nUnder Simple Random Sampling (SRS), each point in the study area has an equal probability of being selected. The sample proportions are therefore unbiased estimators of the population proportions. Let $\\text{Ref}_i$ be the event that a randomly selected point has reference class $i$, and $\\text{Map}_j$ be the event that it is mapped as class $j$. The sample-based estimator for the probability of any joint event $(\\text{Ref}_i, \\text{Map}_j)$ is $\\hat{P}(\\text{Ref}_i \\cap \\text{Map}_j) = \\frac{n_{ij}}{N}$.\n\n1.  **Overall Accuracy (OA)**: OA is the probability that a randomly selected validation point is correctly classified. This corresponds to the event where the map class and reference class are the same. The estimator for OA, denoted $\\widehat{OA}$, is the ratio of the number of correctly classified samples to the total number of samples.\n    $$\n    \\widehat{OA} = \\frac{\\sum_{k=1}^{4} n_{kk}}{N}\n    $$\n    This is an estimator of $P(\\text{Map}_k = \\text{Ref}_k)$ over all classes.\n\n2.  **Producer's Accuracy (PA)**: PA for a class $i$ is the conditional probability that a sample point will be correctly classified as class $i$ given that its true (reference) class is $i$. It measures how well the map producer has represented the true classes on the ground.\n    $$\n    PA_i = P(\\text{Map}_i | \\text{Ref}_i) = \\frac{P(\\text{Map}_i \\cap \\text{Ref}_i)}{P(\\text{Ref}_i)}\n    $$\n    Under SRS, the probabilities are estimated by the sample proportions. The estimator for PA for class $i$, $\\widehat{PA}_i$, is:\n    $$\n    \\widehat{PA}_i = \\frac{n_{ii}/N}{n_{i+}/N} = \\frac{n_{ii}}{n_{i+}}\n    $$\n\n3.  **User's Accuracy (UA)**: UA for a class $j$ is the conditional probability that a sample point is truly of class $j$ given that it has been classified as class $j$ on the map. It measures the reliability of the map from the user's perspective.\n    $$\n    UA_j = P(\\text{Ref}_j | \\text{Map}_j) = \\frac{P(\\text{Ref}_j \\cap \\text{Map}_j)}{P(\\text{Map}_j)}\n    $$\n    The estimator for UA for class $j$, $\\widehat{UA}_j$, is:\n    $$\n    \\widehat{UA}_j = \\frac{n_{jj}/N}{n_{+j}/N} = \\frac{n_{jj}}{n_{+j}}\n    $$\n\n**Justification for Binomial Confidence Intervals**\nThe use of binomial confidence intervals for PA and UA under an SRS design is justified as follows:\n\nFor **Producer's Accuracy** of class $i$ ($\\widehat{PA}_i = \\frac{n_{ii}}{n_{i+}}$), we consider the sub-sample of $n_{i+}$ points whose reference label is class $i$. Each of these $n_{i+}$ points can be viewed as an independent trial. For each trial, there are two outcomes: the map label matches the reference label (a \"success\") or it does not (a \"failure\"). Since the overall sampling was SRS, each of these $n_{i+}$ trials is independent and has the same probability of success, which is the true population producer's accuracy $PA_i$. The number of successes, $n_{ii}$, thus follows a binomial distribution $B(n_{i+}, PA_i)$. Therefore, constructing a confidence interval for the proportion parameter of a binomial distribution is the appropriate statistical procedure.\n\nFor **User's Accuracy** of class $j$ ($\\widehat{UA}_j = \\frac{n_{jj}}{n_{+j}}$), a symmetric argument applies. We consider the sub-sample of $n_{+j}$ points that the map has labeled as class $j$. Each of these points represents an independent trial. A \"success\" occurs if the reference label correctly matches the map label. The number of successes, $n_{jj}$, follows a binomial distribution $B(n_{+j}, UA_j)$, where $UA_j$ is the true population user's accuracy. Thus, binomial confidence intervals are also appropriate for UA.\n\nThe problem mentions a \"minimum separation distance,\" which is a slight departure from pure SRS. This is a practical measure to mitigate spatial autocorrelation, which itself violates the independence assumption of SRS. By enforcing a minimum distance, the design aims to make the samples *effectively* independent. In standard practice, unless this constraint induces significant and systematic bias, the samples are treated as independent, and the binomial model is considered a valid and robust approximation.\n\n**Computations**\nThe diagonal elements sum to $\\sum n_{kk} = 170 + 120 + 62 + 41 = 393$.\n\n1.  **Overall Accuracy:**\n    $\\widehat{OA} = \\frac{393}{500} = 0.786$. Rounded to four significant figures, this is $0.7860$.\n\n2.  **User's Accuracies (from column totals):**\n    $\\widehat{UA}_F = \\frac{n_{FF}}{n_{+F}} = \\frac{170}{195} \\approx 0.871794 \\to 0.8718$\n    $\\widehat{UA}_C = \\frac{n_{CC}}{n_{+C}} = \\frac{120}{152} \\approx 0.789473 \\to 0.7895$\n    $\\widehat{UA}_W = \\frac{n_{WW}}{n_{+W}} = \\frac{62}{96} \\approx 0.645833 \\to 0.6458$\n    $\\widehat{UA}_B = \\frac{n_{BB}}{n_{+B}} = \\frac{41}{57} \\approx 0.719298 \\to 0.7193$\n\n3.  **Producer's Accuracies (from row totals):**\n    $\\widehat{PA}_F = \\frac{n_{FF}}{n_{F+}} = \\frac{170}{205} \\approx 0.829268 \\to 0.8293$\n    $\\widehat{PA}_C = \\frac{n_{CC}}{n_{C+}} = \\frac{120}{150} = 0.8$. Rounded to four significant figures, this is $0.8000$.\n    $\\widehat{PA}_W = \\frac{n_{WW}}{n_{W+}} = \\frac{62}{85} \\approx 0.729411 \\to 0.7294$\n    $\\widehat{PA}_B = \\frac{n_{BB}}{n_{B+}} = \\frac{41}{60} \\approx 0.683333 \\to 0.6833$\n\nThe final values, rounded to four significant figures, are OA: $0.7860$; UAs: $(0.8718, 0.7895, 0.6458, 0.7193)$; PAs: $(0.8293, 0.8000, 0.7294, 0.6833)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.7860 & 0.8718 & 0.7895 & 0.6458 & 0.7193 & 0.8293 & 0.8000 & 0.7294 & 0.6833\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Accuracy metrics are not static properties of a model but are a direct consequence of the decision threshold used for classification. This practice  explores this dynamic relationship by challenging you to analyze how making a classifier more 'strict' or 'lenient' creates a trade-off between different types of errors. You will see firsthand how User's Accuracy can be improved at the expense of Producer's Accuracy, a fundamental concept in tuning a model for a specific operational goal.",
            "id": "3794259",
            "problem": "A supervised classifier trained on multispectral satellite imagery produces, for each pixel, a posterior probability $\\hat{p}$ of belonging to the class \"wetland\" versus \"non-wetland.\" A validation sample contains $N = 10{,}000$ pixels, with reference labels: $N^{+} = 2{,}000$ true wetlands and $N^{-} = 8{,}000$ true non-wetlands. Pixels are mapped as wetland if $\\hat{p} \\ge t$ and as non-wetland otherwise. Consider two decision thresholds: $t_{1} = 0.4$ and $t_{2} = 0.7$.\n\nAt threshold $t_{1}$, the confusion outcomes for the wetland class are:\n- True positives: $\\mathrm{TP}_{1} = 1{,}700$\n- False positives: $\\mathrm{FP}_{1} = 900$\n- False negatives: $\\mathrm{FN}_{1} = 300$\n- True negatives: $\\mathrm{TN}_{1} = 7{,}100$\n\nAt threshold $t_{2}$, the confusion outcomes for the wetland class are:\n- True positives: $\\mathrm{TP}_{2} = 1{,}500$\n- False positives: $\\mathrm{FP}_{2} = 400$\n- False negatives: $\\mathrm{FN}_{2} = 500$\n- True negatives: $\\mathrm{TN}_{2} = 7{,}600$\n\nUsing only the core definitions of the confusion matrix and class-conditional accuracies that are standard in remote sensing map accuracy assessment, determine which option correctly characterizes how a stricter decision rule (raising the threshold from $t_{1}$ to $t_{2}$) trades commission error against omission error for the wetland class, and what that implies for the user's accuracy, producer's accuracy, and overall accuracy. Your answer should rely on the logic that tightening the threshold changes the set of pixels mapped as wetland and should reconcile the observed changes with class prevalence in the validation data.\n\nChoose the single best option.\n\nA. Raising the threshold from $t_{1}$ to $t_{2}$ reduces commission error for wetlands while increasing omission error. The user's accuracy for wetlands increases from approximately $0.654$ to approximately $0.789$, the producer's accuracy decreases from $0.85$ to $0.75$, and the overall accuracy increases from $0.88$ to $0.91$ because many false positives are removed in a dataset dominated by non-wetlands.\n\nB. Raising the threshold from $t_{1}$ to $t_{2}$ reduces both commission and omission errors for wetlands. The user's accuracy decreases while the producer's accuracy increases, and the overall accuracy decreases because fewer pixels are mapped as wetlands.\n\nC. Raising the threshold from $t_{1}$ to $t_{2}$ increases commission error but decreases omission error for wetlands. Both user's and producer's accuracies increase, and the overall accuracy would increase only if the prevalence of wetlands exceeds that of non-wetlands.\n\nD. Raising the threshold from $t_{1}$ to $t_{2}$ reduces commission error while increasing omission error for wetlands. The user's accuracy decreases due to fewer mapped wetlands, the producer's accuracy increases as mislabeling is constrained, and the overall accuracy remains unchanged because the total number of correct classifications is conserved under threshold shifts.",
            "solution": "The user has requested a rigorous validation and solution for a problem concerning classification accuracy metrics in remote sensing.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem statement provides the following data and conditions:\n- Total validation sample size: $N = 10{,}000$ pixels.\n- Reference data composition:\n    - True positives (wetland): $N^{+} = 2{,}000$\n    - True negatives (non-wetland): $N^{-} = 8{,}000$\n- Classification rule: A pixel is classified as \"wetland\" if its posterior probability $\\hat{p}$ is greater than or equal to a threshold $t$ ($\\hat{p} \\ge t$).\n- For threshold $t_{1} = 0.4$, the confusion matrix outcomes are:\n    - $\\mathrm{TP}_{1} = 1{,}700$\n    - $\\mathrm{FP}_{1} = 900$\n    - $\\mathrm{FN}_{1} = 300$\n    - $\\mathrm{TN}_{1} = 7{,}100$\n- For threshold $t_{2} = 0.7$, the confusion matrix outcomes are:\n    - $\\mathrm{TP}_{2} = 1{,}500$\n    - $\\mathrm{FP}_{2} = 400$\n    - $\\mathrm{FN}_{2} = 500$\n    - $\\mathrm{TN}_{2} = 7{,}600$\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientific Grounding:** The problem is firmly based on the standard and fundamental principles of classification accuracy assessment, using a confusion matrix, decision thresholds, and derived metrics like user's, producer's, and overall accuracy. These concepts are central to fields like remote sensing, machine learning, and statistics. The problem is scientifically sound.\n\n2.  **Well-Posed:** The problem is well-posed. It provides all the necessary numerical data to calculate the required metrics and to evaluate the claims made in the options.\n\n3.  **Objective:** The problem is stated using precise, quantitative, and objective language.\n\n4.  **Internal Consistency:** The provided numbers must be consistent with the given totals for reference classes.\n    - For threshold $t_{1}$:\n        - Sum of reference positives: $\\mathrm{TP}_{1} + \\mathrm{FN}_{1} = 1{,}700 + 300 = 2{,}000$, which matches the given $N^{+}$.\n        - Sum of reference negatives: $\\mathrm{FP}_{1} + \\mathrm{TN}_{1} = 900 + 7{,}100 = 8{,}000$, which matches the given $N^{-}$.\n        - Total samples: $\\mathrm{TP}_{1} + \\mathrm{FP}_{1} + \\mathrm{FN}_{1} + \\mathrm{TN}_{1} = 1{,}700 + 900 + 300 + 7{,}100 = 10{,}000$, which matches the given $N$.\n    - For threshold $t_{2}$:\n        - Sum of reference positives: $\\mathrm{TP}_{2} + \\mathrm{FN}_{2} = 1{,}500 + 500 = 2{,}000$, which matches the given $N^{+}$.\n        - Sum of reference negatives: $\\mathrm{FP}_{2} + \\mathrm{TN}_{2} = 400 + 7{,}600 = 8{,}000$, which matches the given $N^{-}$.\n        - Total samples: $\\mathrm{TP}_{2} + \\mathrm{FP}_{2} + \\mathrm{FN}_{2} + \\mathrm{TN}_{2} = 1{,}500 + 400 + 500 + 7{,}600 = 10{,}000$, which matches the given $N$.\n    The data is internally consistent.\n\n5.  **Logical Coherence:** Raising the decision threshold from $t_1=0.4$ to $t_2=0.7$ imposes a stricter criterion for classifying a pixel as \"wetland\". This should decrease the total number of pixels classified as wetland.\n    - Mapped wetland pixels at $t_1$: $\\mathrm{TP}_{1} + \\mathrm{FP}_{1} = 1{,}700 + 900 = 2{,}600$.\n    - Mapped wetland pixels at $t_2$: $\\mathrm{TP}_{2} + \\mathrm{FP}_{2} = 1{,}500 + 400 = 1{,}900$.\n    The number of mapped wetland pixels decreases from $2{,}600$ to $1{,}900$, which is logically consistent with a stricter threshold.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and internally consistent. I will now proceed with the solution derivation.\n\n### Solution Derivation\n\nThe analysis requires calculating and comparing three key accuracy metrics for the \"wetland\" class at both thresholds. The \"wetland\" class is the positive class.\n\n**Definitions of Metrics:**\n- **User's Accuracy (UA)**, or Precision, measures the probability that a pixel mapped as the class of interest is actually that class on the ground. It is an indicator of commission error.\n    $$ \\mathrm{UA} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} $$\n- **Producer's Accuracy (PA)**, or Recall/Sensitivity, measures the probability that a ground reference pixel of a certain class is correctly mapped as such. It is an indicator of omission error.\n    $$ \\mathrm{PA} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} = \\frac{\\mathrm{TP}}{N^{+}} $$\n- **Overall Accuracy (OA)** measures the proportion of all pixels that are correctly classified.\n    $$ \\mathrm{OA} = \\frac{\\mathrm{TP} + \\mathrm{TN}}{N} $$\n\n**Calculations for Threshold $t_{1} = 0.4$**\nUsing the given confusion outcomes: $\\mathrm{TP}_{1} = 1{,}700$, $\\mathrm{FP}_{1} = 900$, $\\mathrm{FN}_{1} = 300$, $\\mathrm{TN}_{1} = 7{,}100$.\n\n- **User's Accuracy at $t_{1}$:**\n  $$ \\mathrm{UA}_{1} = \\frac{\\mathrm{TP}_{1}}{\\mathrm{TP}_{1} + \\mathrm{FP}_{1}} = \\frac{1{,}700}{1{,}700 + 900} = \\frac{1{,}700}{2{,}600} \\approx 0.6538 $$\n- **Producer's Accuracy at $t_{1}$:**\n  $$ \\mathrm{PA}_{1} = \\frac{\\mathrm{TP}_{1}}{\\mathrm{TP}_{1} + \\mathrm{FN}_{1}} = \\frac{1{,}700}{1{,}700 + 300} = \\frac{1{,}700}{2{,}000} = 0.85 $$\n- **Overall Accuracy at $t_{1}$:**\n  $$ \\mathrm{OA}_{1} = \\frac{\\mathrm{TP}_{1} + \\mathrm{TN}_{1}}{N} = \\frac{1{,}700 + 7{,}100}{10{,}000} = \\frac{8{,}800}{10{,}000} = 0.88 $$\n\n**Calculations for Threshold $t_{2} = 0.7$**\nUsing the given confusion outcomes: $\\mathrm{TP}_{2} = 1{,}500$, $\\mathrm{FP}_{2} = 400$, $\\mathrm{FN}_{2} = 500$, $\\mathrm{TN}_{2} = 7{,}600$.\n\n- **User's Accuracy at $t_{2}$:**\n  $$ \\mathrm{UA}_{2} = \\frac{\\mathrm{TP}_{2}}{\\mathrm{TP}_{2} + \\mathrm{FP}_{2}} = \\frac{1{,}500}{1{,}500 + 400} = \\frac{1{,}500}{1{,}900} \\approx 0.7895 $$\n- **Producer's Accuracy at $t_{2}$:**\n  $$ \\mathrm{PA}_{2} = \\frac{\\mathrm{TP}_{2}}{\\mathrm{TP}_{2} + \\mathrm{FN}_{2}} = \\frac{1{,}500}{1{,}500 + 500} = \\frac{1{,}500}{2{,}000} = 0.75 $$\n- **Overall Accuracy at $t_{2}$:**\n  $$ \\mathrm{OA}_{2} = \\frac{\\mathrm{TP}_{2} + \\mathrm{TN}_{2}}{N} = \\frac{1{,}500 + 7{,}600}{10{,}000} = \\frac{9{,}100}{10{,}000} = 0.91 $$\n\n**Analysis of Changes from $t_{1}$ to $t_{2}$**\n\n1.  **Commission vs. Omission Error:**\n    - Commission errors are false positives ($\\mathrm{FP}$). The count decreases from $\\mathrm{FP}_{1} = 900$ to $\\mathrm{FP}_{2} = 400$. Thus, **commission error is reduced**.\n    - Omission errors are false negatives ($\\mathrm{FN}$). The count increases from $\\mathrm{FN}_{1} = 300$ to $\\mathrm{FN}_{2} = 500$. Thus, **omission error is increased**.\n    This demonstrates the classic trade-off: a stricter classification rule for the 'wetland' class reduces false alarms (commission) at the expense of missing more true instances (omission).\n\n2.  **User's Accuracy (Wetland):**\n    - $\\mathrm{UA}_{1} \\approx 0.654$ and $\\mathrm{UA}_{2} \\approx 0.789$. User's accuracy **increases**.\n\n3.  **Producer's Accuracy (Wetland):**\n    - $\\mathrm{PA}_{1} = 0.85$ and $\\mathrm{PA}_{2} = 0.75$. Producer's accuracy **decreases**.\n\n4.  **Overall Accuracy:**\n    - $\\mathrm{OA}_{1} = 0.88$ and $\\mathrm{OA}_{2} = 0.91$. Overall accuracy **increases**.\n    - The reason for the increase in overall accuracy can be understood by examining the change in the number of correctly classified pixels:\n      $$ \\Delta(\\text{Correct}) = (\\mathrm{TP}_{2} + \\mathrm{TN}_{2}) - (\\mathrm{TP}_{1} + \\mathrm{TN}_{1}) = 9{,}100 - 8{,}800 = +300 $$\n      This net gain occurs because the stricter threshold reclassifies some pixels from wetland to non-wetland.\n      - The number of true wetlands reclassified (formerly $\\mathrm{TP}$, now $\\mathrm{FN}$) is $\\mathrm{TP}_{1} - \\mathrm{TP}_{2} = 1{,}700 - 1{,}500 = 200$. This is a loss of $200$ correct classifications.\n      - The number of true non-wetlands reclassified (formerly $\\mathrm{FP}$, now $\\mathrm{TN}$) is $\\mathrm{FP}_{1} - \\mathrm{FP}_{2} = 900 - 400 = 500$. This is a gain of $500$ correct classifications.\n      - The net change is a gain of $500 - 200 = 300$ correct classifications. This outcome is a direct result of the high prevalence of the non-wetland class ($N^{-} = 8{,}000$) compared to the wetland class ($N^{+} = 2{,}000$). Correcting the classification of the more abundant class has a larger impact on overall accuracy.\n\n### Evaluation of Options\n\n**A. Raising the threshold from $t_{1}$ to $t_{2}$ reduces commission error for wetlands while increasing omission error. The user's accuracy for wetlands increases from approximately $0.654$ to approximately $0.789$, the producer's accuracy decreases from $0.85$ to $0.75$, and the overall accuracy increases from $0.88$ to $0.91$ because many false positives are removed in a dataset dominated by non-wetlands.**\n- **Analysis:** This statement perfectly matches all derived results. The trade-off between commission and omission error is correctly stated. The user's accuracy increases (from $0.6538$ to $0.7895$). The producer's accuracy decreases (from $0.85$ to $0.75$). The overall accuracy increases (from $0.88$ to $0.91$). The reasoning provided—that many false positives are removed in a dataset dominated by non-wetlands—is also correct, as shown in the analysis above.\n- **Verdict:** Correct.\n\n**B. Raising the threshold from $t_{1}$ to $t_{2}$ reduces both commission and omission errors for wetlands. The user's accuracy decreases while the producer's accuracy increases, and the overall accuracy decreases because fewer pixels are mapped as wetlands.**\n- **Analysis:** This is incorrect on multiple counts. Omission error increases, it does not decrease ($\\mathrm{FN}_{1}=300$ vs $\\mathrm{FN}_{2}=500$). User's accuracy increases, not decreases. Producer's accuracy decreases, not increases. Overall accuracy increases, not decreases.\n- **Verdict:** Incorrect.\n\n**C. Raising the threshold from $t_{1}$ to $t_{2}$ increases commission error but decreases omission error for wetlands. Both user's and producer's accuracies increase, and the overall accuracy would increase only if the prevalence of wetlands exceeds that of non-wetlands.**\n- **Analysis:** The description of the error trade-off is the reverse of what occurs. Commission error decreases and omission error increases. Producer's accuracy decreases, it does not increase. The reasoning regarding prevalence is also flawed; overall accuracy increased precisely because the non-wetland class is prevalent.\n- **Verdict:** Incorrect.\n\n**D. Raising the threshold from $t_{1}$ to $t_{2}$ reduces commission error while increasing omission error for wetlands. The user's accuracy decreases due to fewer mapped wetlands, the producer's accuracy increases as mislabeling is constrained, and the overall accuracy remains unchanged because the total number of correct classifications is conserved under threshold shifts.**\n- **Analysis:** While this option correctly identifies the trade-off between commission and omission error, it incorrectly states that user's accuracy decreases and producer's accuracy increases. Furthermore, the claim that overall accuracy remains unchanged is false; it increased from $0.88$ to $0.91$. The total number of correct classifications is not conserved when the threshold changes.\n- **Verdict:** Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Real-world mapping tasks often involve challenges like class imbalance, where rare but critical classes are difficult to classify accurately. This final exercise  presents a realistic scenario of refining a map of a rare land-cover type through a post-processing filter designed to reduce false positives. Your task is to quantify the distinct impacts of this correction on User's Accuracy and Producer's Accuracy, revealing how targeted improvements to a map can lead to a significant, albeit asymmetric, enhancement in its reliability.",
            "id": "3794220",
            "problem": "A land-cover map is produced from satellite imagery to delineate a rare peat bog class. In an independent pixel-wise validation set of size $N=200000$, the class prevalence is $p=5 \\times 10^{-3}$, so the number of truly positive (peat bog) pixels is $P=pN$. A baseline classifier produces $TP_0=600$ true positives for the peat bog class, and $FP_0=2400$ false positives. A spatial post-processing screen is then applied that removes a subset of predicted peat bog pixels known (from an auxiliary audit) to contain exactly $\\frac{1}{2}$ of all false positives and $\\frac{1}{20}$ of all true positives for this class.\n\nStarting strictly from the foundational definitions of the confusion matrix and the accuracy metrics used in remote sensing, namely User’s accuracy (UA) and Producer’s accuracy (PA), compute the absolute change in UA and the absolute change in PA induced by the post-processing, and then compute the ratio $R$ defined as the absolute change in UA divided by the absolute change in PA. Express $R$ as a decimal (no percentage sign) and round your answer to four significant figures.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established principles of classification accuracy assessment in remote sensing, is well-posed with all necessary data provided, and is expressed in objective, unambiguous language. The conditions are internally consistent and allow for a unique, meaningful solution.\n\nThe fundamental task is to compute the change in User's Accuracy (UA) and Producer's Accuracy (PA) for a specific land-cover class after a post-processing step is applied to a baseline classification. We begin by defining these metrics based on the components of a confusion matrix: True Positives ($TP$), False Positives ($FP$), and False Negatives ($FN$).\n\nProducer's Accuracy ($PA$), also known as recall or sensitivity, measures the proportion of actual positives that are correctly identified. It is defined as:\n$$\nPA = \\frac{TP}{TP + FN} = \\frac{TP}{P}\n$$\nwhere $P = TP + FN$ is the total number of actual positive instances in the validation set (a column total in the confusion matrix).\n\nUser's Accuracy ($UA$), also known as precision, measures the proportion of predicted positives that are actually correct. It is defined as:\n$$\nUA = \\frac{TP}{TP + FP}\n$$\nwhere $TP + FP$ is the total number of instances predicted as positive by the classifier (a row total in the confusion matrix).\n\nFirst, we establish the parameters of the validation set. The total size is $N=200000$ and the prevalence of the peat bog class (the \"positive\" class) is $p=5 \\times 10^{-3}$. The total number of actual positive pixels, $P$, is therefore:\n$$\nP = p \\times N = (5 \\times 10^{-3}) \\times 200000 = 1000\n$$\nThis value $P$ is a characteristic of the ground-truth data and remains constant throughout the analysis.\n\nNext, we analyze the baseline classifier (state $0$). We are given:\n-   True positives: $TP_0 = 600$\n-   False positives: $FP_0 = 2400$\n\nUsing these values, we can compute the initial accuracy metrics. The total number of pixels predicted as positive is $TP_0 + FP_0 = 600 + 2400 = 3000$.\nThe initial User's Accuracy, $UA_0$, is:\n$$\nUA_0 = \\frac{TP_0}{TP_0 + FP_0} = \\frac{600}{3000} = \\frac{1}{5} = 0.2\n$$\nThe initial Producer's Accuracy, $PA_0$, is:\n$$\nPA_0 = \\frac{TP_0}{P} = \\frac{600}{1000} = 0.6\n$$\n\nNow, we consider the effect of the spatial post-processing screen. This screen removes a subset of pixels that were initially predicted as positive. The problem states that the removed set contains:\n-   $\\frac{1}{20}$ of the initial true positives ($TP_0$).\n-   $\\frac{1}{2}$ of the initial false positives ($FP_0$).\n\nA true positive pixel that is removed from the positive prediction set is re-classified as negative, thus becoming a false negative ($FN$).\nA false positive pixel that is removed from the positive prediction set is re-classified as negative, thus becoming a true negative ($TN$).\n\nLet's calculate the number of $TP$s and $FP$s removed.\n-   Number of $TP$s removed: $\\Delta TP_{rem} = \\frac{1}{20} \\times TP_0 = \\frac{1}{20} \\times 600 = 30$.\n-   Number of $FP$s removed: $\\Delta FP_{rem} = \\frac{1}{2} \\times FP_0 = \\frac{1}{2} \\times 2400 = 1200$.\n\nWe can now find the new values for the confusion matrix components after post-processing (state $1$):\n-   New true positives: $TP_1 = TP_0 - \\Delta TP_{rem} = 600 - 30 = 570$.\n-   New false positives: $FP_1 = FP_0 - \\Delta FP_{rem} = 2400 - 1200 = 1200$.\n\nUsing these new values, we compute the final accuracy metrics. The new total number of pixels predicted as positive is $TP_1 + FP_1 = 570 + 1200 = 1770$.\nThe final User's Accuracy, $UA_1$, is:\n$$\nUA_1 = \\frac{TP_1}{TP_1 + FP_1} = \\frac{570}{1770} = \\frac{57}{177}\n$$\nThe final Producer's Accuracy, $PA_1$, is calculated using the unchanged value of $P=1000$:\n$$\nPA_1 = \\frac{TP_1}{P} = \\frac{570}{1000} = 0.57\n$$\n\nThe next step is to compute the absolute change in each metric.\nThe absolute change in User's Accuracy, $\\Delta UA$, is:\n$$\n\\Delta UA = |UA_1 - UA_0| = \\left| \\frac{57}{177} - 0.2 \\right| = \\left| \\frac{57}{177} - \\frac{1}{5} \\right| = \\left| \\frac{57 \\times 5 - 177 \\times 1}{177 \\times 5} \\right| = \\left| \\frac{285 - 177}{885} \\right| = \\frac{108}{885}\n$$\nThe absolute change in Producer's Accuracy, $\\Delta PA$, is:\n$$\n\\Delta PA = |PA_1 - PA_0| = |0.57 - 0.60| = |-0.03| = 0.03\n$$\n\nFinally, we compute the ratio $R$ as the absolute change in UA divided by the absolute change in PA:\n$$\nR = \\frac{\\Delta UA}{\\Delta PA} = \\frac{108/885}{0.03}\n$$\nTo simplify the calculation:\n$$\nR = \\frac{108}{885 \\times 0.03} = \\frac{108}{26.55} \\approx 4.0677966...\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $4.067$. The fifth significant digit is $7$, which is greater than or equal to $5$, so we round up the last digit.\n$$\nR \\approx 4.068\n$$",
            "answer": "$$\\boxed{4.068}$$"
        }
    ]
}