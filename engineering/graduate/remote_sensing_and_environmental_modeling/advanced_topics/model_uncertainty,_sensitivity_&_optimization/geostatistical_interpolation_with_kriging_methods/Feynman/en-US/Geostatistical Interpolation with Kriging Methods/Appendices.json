{
    "hands_on_practices": [
        {
            "introduction": "While simple kriging provides a theoretical foundation, its requirement of a known mean is often unrealistic in practice. This is where ordinary kriging becomes the essential tool for geostatisticians. This exercise  will guide you through the process of making a prediction when the mean is unknown, demonstrating how to derive and apply the unbiasedness constraint to estimate particulate matter concentration.",
            "id": "3817477",
            "problem": "A remote sensing campaign monitors fine particulate matter concentration, denoted by the random function $Z(\\mathbf{s})$ in $\\mathrm{mg\\,m^{-3}}$, over a $100\\,\\mathrm{m} \\times 100\\,\\mathrm{m}$ urban block. Assume intrinsic stationarity with an unknown constant local mean and an isotropic exponential semivariogram with nugget given by\n$$\n\\gamma(h) \\;=\\; c_{0} \\;+\\; c \\left(1 - \\exp\\!\\left(-\\frac{h}{a}\\right)\\right),\n$$\nwhere $c_{0} = 0.2\\,\\mathrm{(mg\\,m^{-3})^{2}}$, $c = 1.0\\,\\mathrm{(mg\\,m^{-3})^{2}}$, and $a = 50\\,\\mathrm{m}$. Three in situ reference measurements with precise geolocation are available:\n- $Z(\\mathbf{s}_{1}) = 12\\,\\mathrm{mg\\,m^{-3}}$ at $\\mathbf{s}_{1} = (0, 50)\\,\\mathrm{m}$,\n- $Z(\\mathbf{s}_{2}) = 10\\,\\mathrm{mg\\,m^{-3}}$ at $\\mathbf{s}_{2} = (50, 0)\\,\\mathrm{m}$,\n- $Z(\\mathbf{s}_{3}) = 15\\,\\mathrm{mg\\,m^{-3}}$ at $\\mathbf{s}_{3} = (50, 50)\\,\\mathrm{m}$.\n\nYou are to estimate the concentration at the target location $\\mathbf{s}_{0} = (0, 0)\\,\\mathrm{m}$ using ordinary kriging. Starting from the definition of the semivariogram for an intrinsically stationary process,\n$$\n\\gamma(\\mathbf{h}) \\;=\\; \\frac{1}{2}\\,\\mathbb{E}\\!\\left[\\big(Z(\\mathbf{s}+\\mathbf{h}) - Z(\\mathbf{s})\\big)^{2}\\right],\n$$\nand the unbiasedness condition for ordinary kriging with an unknown constant mean, derive the ordinary kriging weights that minimize the estimation variance, and compute the ordinary kriging estimate $\\hat{Z}(\\mathbf{s}_{0})$. Treat $\\gamma(0)$ as $0$ consistently with the definition. Express the final estimate in $\\mathrm{mg\\,m^{-3}}$ and round your answer to four significant figures.",
            "solution": "We model $Z(\\mathbf{s})$ as an intrinsically stationary random function with unknown constant local mean $m$ and semivariogram $\\gamma(h)$ as specified. The ordinary kriging estimator at $\\mathbf{s}_{0}$ is\n$$\n\\hat{Z}(\\mathbf{s}_{0}) \\;=\\; \\sum_{i=1}^{n} w_{i}\\, Z(\\mathbf{s}_{i}),\n$$\nwith $n = 3$ and weights $\\{w_{i}\\}$ chosen to be unbiased and to minimize the estimation variance. For ordinary kriging, unbiasedness under an unknown constant mean requires\n$$\n\\sum_{i=1}^{n} w_{i} \\;=\\; 1.\n$$\nThe estimation variance for an intrinsically stationary process can be expressed in terms of the semivariogram as\n$$\n\\sigma^{2}_{\\mathrm{OK}}(\\mathbf{s}_{0}) \\;=\\; \\mathrm{Var}\\!\\left(\\hat{Z}(\\mathbf{s}_{0}) - Z(\\mathbf{s}_{0})\\right)\n\\;=\\; 2 \\sum_{i=1}^{n} w_{i}\\, \\gamma\\!\\left(\\|\\mathbf{s}_{i} - \\mathbf{s}_{0}\\|\\right) - \\sum_{i=1}^{n}\\sum_{j=1}^{n} w_{i} w_{j}\\, \\gamma\\!\\left(\\|\\mathbf{s}_{i} - \\mathbf{s}_{j}\\|\\right).\n$$\nTo enforce the unbiasedness constraint while minimizing $\\sigma^{2}_{\\mathrm{OK}}(\\mathbf{s}_{0})$, we form the Lagrangian\n$$\n\\mathcal{L}(\\mathbf{w}, \\mu) \\;=\\; 2 \\sum_{i=1}^{n} w_{i}\\, \\gamma\\!\\left(\\|\\mathbf{s}_{i} - \\mathbf{s}_{0}\\|\\right) - \\sum_{i=1}^{n}\\sum_{j=1}^{n} w_{i} w_{j}\\, \\gamma\\!\\left(\\|\\mathbf{s}_{i} - \\mathbf{s}_{j}\\|\\right)\n- 2\\mu \\left(\\sum_{i=1}^{n} w_{i} - 1\\right),\n$$\nand set partial derivatives with respect to $w_{k}$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_{k}} \\;=\\; 2\\, \\gamma\\!\\left(\\|\\mathbf{s}_{k} - \\mathbf{s}_{0}\\|\\right) - 2 \\sum_{j=1}^{n} w_{j}\\, \\gamma\\!\\left(\\|\\mathbf{s}_{k} - \\mathbf{s}_{j}\\|\\right)\n- 2\\mu \\;=\\; 0.\n$$\nDividing by $-2$ yields the ordinary kriging system in semivariogram form:\n$$\n\\sum_{j=1}^{n} w_{j}\\, \\gamma\\!\\left(\\|\\mathbf{s}_{k} - \\mathbf{s}_{j}\\|\\right) + \\mu \\;=\\; \\gamma\\!\\left(\\|\\mathbf{s}_{k} - \\mathbf{s}_{0}\\|\\right), \\quad k = 1, \\dots, n,\n$$\nwith the constraint $\\sum_{i=1}^{n} w_{i} = 1$. Because $\\gamma(0) = 0$, the diagonal elements satisfy $\\gamma(\\|\\mathbf{s}_{i} - \\mathbf{s}_{i}\\|) = 0$.\n\nWe compute all required distances. The target is $\\mathbf{s}_{0} = (0, 0)\\,\\mathrm{m}$, and the data locations are $\\mathbf{s}_{1} = (0, 50)\\,\\mathrm{m}$, $\\mathbf{s}_{2} = (50, 0)\\,\\mathrm{m}$, $\\mathbf{s}_{3} = (50, 50)\\,\\mathrm{m}$. The pairwise distances are:\n- $\\|\\mathbf{s}_{1} - \\mathbf{s}_{0}\\| = \\sqrt{(0-0)^{2} + (50-0)^{2}} = 50\\,\\mathrm{m}$,\n- $\\|\\mathbf{s}_{2} - \\mathbf{s}_{0}\\| = \\sqrt{(50-0)^{2} + (0-0)^{2}} = 50\\,\\mathrm{m}$,\n- $\\|\\mathbf{s}_{3} - \\mathbf{s}_{0}\\| = \\sqrt{(50-0)^{2} + (50-0)^{2}} = 50\\sqrt{2}\\,\\mathrm{m}$,\n- $\\|\\mathbf{s}_{1} - \\mathbf{s}_{2}\\| = \\sqrt{(0-50)^{2} + (50-0)^{2}} = 50\\sqrt{2}\\,\\mathrm{m}$,\n- $\\|\\mathbf{s}_{1} - \\mathbf{s}_{3}\\| = \\sqrt{(0-50)^{2} + (50-50)^{2}} = 50\\,\\mathrm{m}$,\n- $\\|\\mathbf{s}_{2} - \\mathbf{s}_{3}\\| = \\sqrt{(50-50)^{2} + (0-50)^{2}} = 50\\,\\mathrm{m}$.\n\nDefine\n$$\n\\gamma_{50} \\;=\\; \\gamma(50) \\;=\\; c_{0} + c\\left(1 - \\exp\\!\\left(-\\frac{50}{a}\\right)\\right)\n\\;=\\; 0.2 + 1.0\\left(1 - \\exp(-1)\\right)\n\\;=\\; 1.2 - \\exp(-1),\n$$\n$$\n\\gamma_{70} \\;=\\; \\gamma(50\\sqrt{2}) \\;=\\; c_{0} + c\\left(1 - \\exp\\!\\left(-\\frac{50\\sqrt{2}}{a}\\right)\\right)\n\\;=\\; 0.2 + 1.0\\left(1 - \\exp(-\\sqrt{2})\\right)\n\\;=\\; 1.2 - \\exp(-\\sqrt{2}).\n$$\nNumerically,\n$$\n\\exp(-1) \\approx 0.367879441, \\quad \\exp(-\\sqrt{2}) \\approx 0.243116734,\n$$\nso\n$$\n\\gamma_{50} \\approx 0.832120559, \\quad \\gamma_{70} \\approx 0.956883266.\n$$\n\nLet the weights be $w_{1}, w_{2}, w_{3}$, and the Lagrange multiplier be $\\mu$. The ordinary kriging equations become\n$$\n\\begin{aligned}\n&\\gamma(\\|\\mathbf{s}_{1}-\\mathbf{s}_{2}\\|) w_{2} + \\gamma(\\|\\mathbf{s}_{1}-\\mathbf{s}_{3}\\|) w_{3} + \\mu \\;=\\; \\gamma(\\|\\mathbf{s}_{1}-\\mathbf{s}_{0}\\|), \\\\\n&\\gamma(\\|\\mathbf{s}_{2}-\\mathbf{s}_{1}\\|) w_{1} + \\gamma(\\|\\mathbf{s}_{2}-\\mathbf{s}_{3}\\|) w_{3} + \\mu \\;=\\; \\gamma(\\|\\mathbf{s}_{2}-\\mathbf{s}_{0}\\|), \\\\\n&\\gamma(\\|\\mathbf{s}_{3}-\\mathbf{s}_{1}\\|) w_{1} + \\gamma(\\|\\mathbf{s}_{3}-\\mathbf{s}_{2}\\|) w_{2} + \\mu \\;=\\; \\gamma(\\|\\mathbf{s}_{3}-\\mathbf{s}_{0}\\|), \\\\\n&w_{1} + w_{2} + w_{3} \\;=\\; 1,\n\\end{aligned}\n$$\nwhich, after substituting distances, is\n$$\n\\begin{aligned}\n&\\gamma_{70}\\, w_{2} + \\gamma_{50}\\, w_{3} + \\mu \\;=\\; \\gamma_{50}, \\\\\n&\\gamma_{70}\\, w_{1} + \\gamma_{50}\\, w_{3} + \\mu \\;=\\; \\gamma_{50}, \\\\\n&\\gamma_{50}\\, w_{1} + \\gamma_{50}\\, w_{2} + \\mu \\;=\\; \\gamma_{70}, \\\\\n&w_{1} + w_{2} + w_{3} \\;=\\; 1.\n\\end{aligned}\n$$\nBy symmetry of the geometry relative to $\\mathbf{s}_{0}$, we have $w_{1} = w_{2} = w$. Let $w_{3} = v$. The system reduces to\n$$\n\\begin{aligned}\n&\\gamma_{70}\\, w + \\gamma_{50}\\, v + \\mu \\;=\\; \\gamma_{50}, \\\\\n&2\\,\\gamma_{50}\\, w + \\mu \\;=\\; \\gamma_{70}, \\\\\n&2w + v \\;=\\; 1.\n\\end{aligned}\n$$\nSubtracting the second equation from the first eliminates $\\mu$:\n$$\n\\left(\\gamma_{70} - 2\\gamma_{50}\\right) w + \\gamma_{50}\\, v \\;=\\; \\gamma_{50} - \\gamma_{70}.\n$$\nUsing $v = 1 - 2w$ from the constraint, we obtain\n$$\n\\left(\\gamma_{70} - 2\\gamma_{50}\\right) w + \\gamma_{50} (1 - 2w) \\;=\\; \\gamma_{50} - \\gamma_{70},\n$$\nwhich simplifies to\n$$\n\\left(\\gamma_{70} - 2\\gamma_{50} - 2\\gamma_{50}\\right) w + \\gamma_{50} \\;=\\; \\gamma_{50} - \\gamma_{70},\n$$\n$$\n\\left(\\gamma_{70} - 4\\gamma_{50}\\right) w \\;=\\; -\\gamma_{70}.\n$$\nEquivalently (retaining the earlier aggregated coefficients),\n$$\n-2.37159897\\, w \\;\\approx\\; -0.956883266 \\quad\\Rightarrow\\quad w \\;\\approx\\; 0.403477.\n$$\nThen\n$$\nv \\;=\\; 1 - 2w \\;\\approx\\; 1 - 0.806954 \\;\\approx\\; 0.193046.\n$$\nThe ordinary kriging estimate is\n$$\n\\hat{Z}(\\mathbf{s}_{0}) \\;=\\; w\\, Z(\\mathbf{s}_{1}) + w\\, Z(\\mathbf{s}_{2}) + v\\, Z(\\mathbf{s}_{3})\n\\;=\\; w\\,(12 + 10) + v\\,(15)\n\\;=\\; 22w + 15v.\n$$\nNumerically,\n$$\n22w \\;\\approx\\; 22 \\times 0.403477 \\;\\approx\\; 8.876494, \\quad 15v \\;\\approx\\; 15 \\times 0.193046 \\;\\approx\\; 2.895690,\n$$\nso\n$$\n\\hat{Z}(\\mathbf{s}_{0}) \\;\\approx\\; 8.876494 + 2.895690 \\;\\approx\\; 11.772184\\,\\mathrm{mg\\,m^{-3}}.\n$$\nRounded to four significant figures and expressed in $\\mathrm{mg\\,m^{-3}}$, the ordinary kriging estimate is $11.77\\,\\mathrm{mg\\,m^{-3}}$.",
            "answer": "$$\\boxed{11.77}$$"
        },
        {
            "introduction": "A geostatistical model is only as good as its ability to make accurate predictions, but how do we measure this accuracy? This practice  introduces a fundamental model validation technique: leave-one-out cross-validation (LOOCV). By systematically removing one data point at a time and predicting its value using the others, you will learn to compute key diagnostic metrics like the Mean Squared Standardized Error to assess the performance of your chosen spatial model.",
            "id": "3817496",
            "problem": "A remote sensing team uses Synthetic Aperture Radar (SAR) to estimate surface soil moisture fraction over a small agricultural field. Three ground validation sites are arranged in an equilateral triangle with side length $1\\,\\mathrm{km}$ at coordinates $(0,0)$, $(1,0)$, and $(\\frac{1}{2},\\frac{\\sqrt{3}}{2})$, with measured soil moisture fractions $(z_{1},z_{2},z_{3})=(0.32,0.40,0.28)$. Assume the soil moisture is a second-order stationary Gaussian random field with isotropic exponential covariance function $C(h)=\\sigma^{2}\\exp(-\\frac{h}{\\phi})$, zero nugget effect, partial sill $\\sigma^{2}=0.02$, and range parameter $\\phi=1\\,\\mathrm{km}$. \n\nUsing ordinary kriging (unknown constant mean), conduct leave-one-out cross-validation (LOOCV) at each site: when predicting at location $\\mathbf{x}_{i}$, use only the remaining two points as the training set. Starting solely from the definition of ordinary kriging as the best linear unbiased estimator and the principle of variance minimization under the unbiasedness constraint, derive the ordinary kriging system, determine the weights for each LOOCV prediction, derive the corresponding kriging variance at the LOOCV target, and compute the Mean Squared Standardized Error (MSSE)\n$$\n\\mathrm{MSSE}=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\left(z_{i}-\\hat{z}_{-i}\\right)^{2}}{\\sigma_{K,-i}^{2}},\n$$\nwhere $n=3$, $\\hat{z}_{-i}$ is the LOOCV ordinary kriging prediction at $\\mathbf{x}_{i}$ using the other two points, and $\\sigma_{K,-i}^{2}$ is the associated LOOCV ordinary kriging variance. Round your final MSSE to four significant figures and express it as a unitless decimal.",
            "solution": "The task is to compute the Mean Squared Standardized Error (MSSE) from a leave-one-out cross-validation (LOOCV) procedure using ordinary kriging. We begin by deriving the ordinary kriging system from first principles.\n\nLet the random field be $Z(\\mathbf{x})$, which is assumed to be second-order stationary with a constant but unknown mean $E[Z(\\mathbf{x})] = \\mu$ and a known covariance function $\\mathrm{Cov}(Z(\\mathbf{x}), Z(\\mathbf{y})) = C(\\mathbf{x}-\\mathbf{y})$.\nThe objective is to predict the value $Z(\\mathbf{x}_0)$ at a location $\\mathbf{x}_0$ using a linear combination of observed values $z_i = Z(\\mathbf{x}_i)$ at $N$ locations $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$. The estimator is\n$$\n\\hat{Z}(\\mathbf{x}_0) = \\sum_{i=1}^N w_i z_i\n$$\nwhere $w_i$ are the weights to be determined.\n\nFor the estimator to be unbiased, i.e., $E[\\hat{Z}(\\mathbf{x}_0) - Z(\\mathbf{x}_0)] = 0$, we require:\n$$\nE[\\hat{Z}(\\mathbf{x}_0)] = E\\left[\\sum_{i=1}^N w_i Z(\\mathbf{x}_i)\\right] = \\sum_{i=1}^N w_i E[Z(\\mathbf{x}_i)] = \\mu \\sum_{i=1}^N w_i\n$$\nSince $E[Z(\\mathbf{x}_0)] = \\mu$, the unbiasedness condition demands that $\\mu \\sum_{i=1}^N w_i = \\mu$. As this must hold for any value of the unknown mean $\\mu$, we must impose the constraint:\n$$\n\\sum_{i=1}^N w_i = 1\n$$\nThe kriging method determines the weights by minimizing the estimation variance, $\\sigma_K^2 = E[(\\hat{Z}(\\mathbf{x}_0) - Z(\\mathbf{x}_0))^2]$, subject to this unbiasedness constraint. The estimation variance can be expanded as:\n$$\n\\sigma_K^2 = \\mathrm{Var}(\\hat{Z}(\\mathbf{x}_0) - Z(\\mathbf{x}_0)) = \\mathrm{Cov}\\left(\\sum_i w_i Z_i - Z_0, \\sum_j w_j Z_j - Z_0\\right)\n$$\n$$\n\\sigma_K^2 = \\sum_{i=1}^N \\sum_{j=1}^N w_i w_j C_{ij} - 2\\sum_{i=1}^N w_i C_{i0} + C_{00}\n$$\nwhere $Z_i = Z(\\mathbf{x}_i)$, $C_{ij} = \\mathrm{Cov}(Z_i, Z_j)$, $C_{i0} = \\mathrm{Cov}(Z_i, Z_0)$, and $C_{00} = \\mathrm{Var}(Z_0)$.\n\nTo solve this constrained minimization problem, we use the method of Lagrange multipliers. We define the Lagrangian $\\mathcal{L}$:\n$$\n\\mathcal{L}(\\mathbf{w}, \\lambda) = \\sum_{i=1}^N \\sum_{j=1}^N w_i w_j C_{ij} - 2\\sum_{i=1}^N w_i C_{i0} + C_{00} + 2\\lambda\\left(\\sum_{i=1}^N w_i - 1\\right)\n$$\nSetting the partial derivatives with respect to each weight $w_k$ to zero gives:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial w_k} = 2\\sum_{i=1}^N w_i C_{ik} - 2C_{k0} + 2\\lambda = 0 \\implies \\sum_{i=1}^N w_i C_{ik} + \\lambda = C_{k0} \\quad (k=1, \\dots, N)\n$$\nThe derivative with respect to $\\lambda$ recovers the constraint $\\sum w_i = 1$. The resulting set of $N+1$ linear equations forms the ordinary kriging system, which in matrix form is:\n$$\n\\begin{pmatrix}\nC_{11} & \\cdots & C_{1N} & 1 \\\\\n\\vdots & \\ddots & \\vdots & \\vdots \\\\\nC_{N1} & \\cdots & C_{NN} & 1 \\\\\n1 & \\cdots & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nw_1 \\\\\n\\vdots \\\\\nw_N \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nC_{10} \\\\\n\\vdots \\\\\nC_{N0} \\\\\n1\n\\end{pmatrix}\n$$\nThe minimized kriging variance is given by $\\sigma_K^2 = C_{00} - \\sum_{i=1}^N w_i C_{i0} - \\lambda$.\n\nNow, we apply this framework to the specific problem. The coordinates are $\\mathbf{x}_1=(0,0)$, $\\mathbf{x}_2=(1,0)$, and $\\mathbf{x}_3=(\\frac{1}{2},\\frac{\\sqrt{3}}{2})$. The distance between any two of these points is $1\\,\\mathrm{km}$. The isotropic exponential covariance function is $C(h) = \\sigma^2 \\exp(-h/\\phi)$, with partial sill $\\sigma^2 = 0.02$ and range $\\phi = 1\\,\\mathrm{km}$. Thus, $C(h) = 0.02 \\exp(-h)$.\nThe variance of the field is $C(0) = 0.02$. The covariance between any two of the three sites is $C(1) = 0.02 \\exp(-1)$. Let's denote these as $C_0 = 0.02$ and $C_1 = 0.02e^{-1}$.\n\nWe perform leave-one-out cross-validation for each of the $n=3$ sites. Due to the equilateral geometry, the setup for each LOOCV prediction is identical. For instance, when predicting at $\\mathbf{x}_1$ (the target, $\\mathbf{x}_0$) using data from $\\mathbf{x}_2$ and $\\mathbf{x}_3$ ($N=2$), the kriging system is:\n$$\n\\begin{pmatrix}\nC(\\mathbf{x}_2, \\mathbf{x}_2) & C(\\mathbf{x}_2, \\mathbf{x}_3) & 1 \\\\\nC(\\mathbf{x}_3, \\mathbf{x}_2) & C(\\mathbf{x}_3, \\mathbf{x}_3) & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nw_2 \\\\\nw_3 \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nC(\\mathbf{x}_2, \\mathbf{x}_1) \\\\\nC(\\mathbf{x}_3, \\mathbf{x}_1) \\\\\n1\n\\end{pmatrix}\n$$\nThe distances are all $h=1\\,\\mathrm{km}$ for pairs of distinct points. Therefore, $C(\\mathbf{x}_i, \\mathbf{x}_i) = C_0$ and $C(\\mathbf{x}_i, \\mathbf{x}_j) = C_1$ for $i \\neq j$. The system becomes:\n$$\n\\begin{pmatrix}\nC_0 & C_1 & 1 \\\\\nC_1 & C_0 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nw_2 \\\\\nw_3 \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nC_1 \\\\\nC_1 \\\\\n1\n\\end{pmatrix}\n$$\nThis yields three equations:\n1. $w_2 C_0 + w_3 C_1 + \\lambda = C_1$\n2. $w_2 C_1 + w_3 C_0 + \\lambda = C_1$\n3. $w_2 + w_3 = 1$\n\nSubtracting the second equation from the first yields $w_2(C_0 - C_1) - w_3(C_0 - C_1) = 0$. Since $C_0 \\neq C_1$, we can divide by $(C_0 - C_1)$ to get $w_2 = w_3$. Substituting this into the third equation gives $2w_2 = 1$, so $w_2 = w_3 = 0.5$.\nDue to symmetry, for any LOOCV prediction, the two available data points will always receive a weight of $0.5$.\n\nThe kriging variance $\\sigma_{K,-i}^2$ will also be the same for each LOOCV fold. Let's calculate it for the prediction at $\\mathbf{x}_1$:\n$$ \\sigma_{K,-1}^2 = C(\\mathbf{x}_1, \\mathbf{x}_1) - (w_2 C(\\mathbf{x}_2, \\mathbf{x}_1) + w_3 C(\\mathbf{x}_3, \\mathbf{x}_1)) - \\lambda $$\nTo find $\\lambda$, we use the first equation: $0.5 C_0 + 0.5 C_1 + \\lambda = C_1 \\implies \\lambda = 0.5 C_1 - 0.5 C_0$.\n$$ \\sigma_{K,-1}^2 = C_0 - (0.5 C_1 + 0.5 C_1) - (0.5 C_1 - 0.5 C_0) = C_0 - C_1 - 0.5 C_1 + 0.5 C_0 $$\n$$ \\sigma_{K,-1}^2 = 1.5 C_0 - 1.5 C_1 = 1.5(C_0 - C_1) $$\nSubstituting the values $C_0 = 0.02$ and $C_1 = 0.02e^{-1}$:\n$$ \\sigma_{K,-1}^2 = 1.5(0.02 - 0.02e^{-1}) = 0.03(1 - e^{-1}) $$\nThis constant kriging variance is denoted $\\sigma_{K}^2$ for all three folds.\n\nNow we compute the LOOCV predictions and errors:\nThe measured values are $(z_1, z_2, z_3) = (0.32, 0.40, 0.28)$.\n1.  Prediction at $\\mathbf{x}_1$: $\\hat{z}_{-1} = 0.5 z_2 + 0.5 z_3 = 0.5(0.40 + 0.28) = 0.34$.\n    Squared error: $(z_1 - \\hat{z}_{-1})^2 = (0.32 - 0.34)^2 = (-0.02)^2 = 0.0004$.\n2.  Prediction at $\\mathbf{x}_2$: $\\hat{z}_{-2} = 0.5 z_1 + 0.5 z_3 = 0.5(0.32 + 0.28) = 0.30$.\n    Squared error: $(z_2 - \\hat{z}_{-2})^2 = (0.40 - 0.30)^2 = (0.10)^2 = 0.01$.\n3.  Prediction at $\\mathbf{x}_3$: $\\hat{z}_{-3} = 0.5 z_1 + 0.5 z_2 = 0.5(0.32 + 0.40) = 0.36$.\n    Squared error: $(z_3 - \\hat{z}_{-3})^2 = (0.28 - 0.36)^2 = (-0.08)^2 = 0.0064$.\n\nFinally, we compute the Mean Squared Standardized Error (MSSE):\n$$ \\mathrm{MSSE} = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{(z_i - \\hat{z}_{-i})^2}{\\sigma_{K,-i}^2} $$\nSince $n=3$ and $\\sigma_{K,-i}^2 = \\sigma_{K}^2 = 0.03(1 - e^{-1})$ for all $i$:\n$$ \\mathrm{MSSE} = \\frac{1}{3 \\sigma_{K}^2} \\sum_{i=1}^3 (z_i - \\hat{z}_{-i})^2 $$\nThe sum of squared errors is $0.0004 + 0.01 + 0.0064 = 0.0168$.\n$$ \\mathrm{MSSE} = \\frac{0.0168}{3 \\times 0.03(1 - e^{-1})} = \\frac{0.0168}{0.09(1 - e^{-1})} $$\nNumerically evaluating this expression:\n$$ \\mathrm{MSSE} = \\frac{0.0168}{0.09 \\times (1 - \\exp(-1))} \\approx \\frac{0.0168}{0.09 \\times (1 - 0.367879)} = \\frac{0.0168}{0.09 \\times 0.632121} \\approx \\frac{0.0168}{0.0568909} \\approx 0.295311 $$\nRounding to four significant figures, we get $0.2953$.",
            "answer": "$$\n\\boxed{0.2953}\n$$"
        },
        {
            "introduction": "Kriging provides the single best linear unbiased estimate, but it doesn't capture the full picture of spatial uncertainty. To explore the range of possible realities that a spatial field might exhibit, we turn to stochastic simulation. This advanced computational practice  challenges you to implement the Sequential Gaussian Simulation (SGS) algorithm, a powerful method for generating multiple spatial realizations that honor both the data and the underlying covariance structure, providing a richer understanding of spatial variability.",
            "id": "3817488",
            "problem": "You are given a task in remote sensing and environmental modeling to perform conditional simulation using Sequential Gaussian Simulation (SGS). Assume the environmental attribute has been transformed to a standard Gaussian distribution with zero mean and unit variance, and is modeled as a stationary Gaussian random field. The interpolation at an unsampled location is performed using ordinary kriging, and the SGS generates realizations by sampling from the conditional Gaussian distribution at each target location sequentially, conditioning on both the original measurements and previously simulated values.\n\nStarting from the core definitions for Gaussian random fields and linear unbiased estimation, derive the ordinary kriging system from the constraints of unbiasedness and minimum estimation variance, and establish the conditional distribution used in SGS. Use the isotropic exponential covariance model with sill, range, and nugget parameters. Implement the SGS by visiting target locations in a prescribed order, at each step computing the ordinary kriging estimate and kriging variance using the current conditioning set, then drawing a sample from the corresponding conditional Gaussian distribution.\n\nFundamental base and assumptions:\n- A stationary Gaussian random field with zero mean is assumed. The covariance function is isotropic and given by the exponential model.\n- Ordinary kriging is defined by the linear estimator whose weights satisfy unbiasedness and minimize estimation variance under the given covariance.\n- Sequential Gaussian Simulation (SGS) is defined as simulating the Gaussian field node-by-node by drawing from the conditional distribution at each location given all currently available conditioning data (original samples and previously simulated nodes).\n\nDefinitions to use:\n- Let $Z(\\mathbf{x})$ denote the Gaussian random field at spatial location $\\mathbf{x}\\in\\mathbb{R}^{2}$.\n- The covariance for two locations $\\mathbf{x}$ and $\\mathbf{y}$ depends on the separation distance $h=\\|\\mathbf{x}-\\mathbf{y}\\|$ as $C(h)=s^{2}\\exp\\left(-\\frac{h}{a}\\right)$, where $s^{2}$ is the sill and $a$ is the range. The nugget effect $\\tau^{2}$ represents microscale variability and measurement error, added only on the diagonal of the data-data covariance matrix in ordinary kriging.\n- Ordinary kriging uses weights $\\{w_{i}\\}_{i=1}^{n}$ applied to values $\\{z_{i}\\}_{i=1}^{n}$ observed at locations $\\{\\mathbf{x}_{i}\\}_{i=1}^{n}$ to estimate $Z$ at an unsampled location $\\mathbf{x}_{0}$: $\\hat{Z}(\\mathbf{x}_{0})=\\sum_{i=1}^{n}w_{i}z_{i}$, with the unbiasedness constraint $\\sum_{i=1}^{n}w_{i}=1$ and weights chosen to minimize the estimation variance.\n\nYour program must implement the following steps for each test case:\n1. Form the conditioning set of observed locations and values. All coordinates are provided in meters and must be treated numerically in meters ($\\mathrm{m}$).\n2. For each target location in the specified sequential path order:\n   - Construct the ordinary kriging linear system using the current conditioning set:\n     - Build the $n\\times n$ covariance matrix $\\mathbf{C}$ of conditioning locations using the structured covariance $C(h)=s^{2}\\exp\\left(-\\frac{h}{a}\\right)$ and add the nugget $\\tau^{2}$ on the diagonal.\n     - Form the augmented $(n+1)\\times(n+1)$ system by appending a row and column of ones to enforce the unbiasedness constraint.\n     - Compute the covariance vector $\\mathbf{c}$ between the target and the conditioning locations using $C(h)$ without nugget.\n     - Solve the system for the kriging weights and the Lagrange multiplier, compute the kriging estimate and variance.\n   - Sample a realization at the target location from the conditional Gaussian distribution with mean equal to the kriging estimate and variance equal to the kriging variance computed from the structured covariance model (using $C(0)=s^{2}$).\n   - Append the simulated target and its simulated value to the conditioning set before moving to the next target in the path.\n\nImportant implementation guidance:\n- The ordinary kriging variance must be computed using only the structured covariance at zero-lag, $C(0)=s^{2}$, not including the nugget at the target location.\n- The nugget $\\tau^{2}$ is added only to the diagonal of the data-data covariance matrix $\\mathbf{C}$.\n- If numerical round-off produces a non-positive kriging variance, clip it to a small positive value to maintain a valid Gaussian distribution.\n\nTest suite:\n- Case $1$ (happy path):\n  - Observed locations and values: $(\\mathbf{x}_{1},z_{1})=((0.0,0.0),0.2)$, $(\\mathbf{x}_{2},z_{2})=((50.0,0.0),-0.1)$, $(\\mathbf{x}_{3},z_{3})=((0.0,50.0),0.3)$, $(\\mathbf{x}_{4},z_{4})=((50.0,50.0),-0.2)$.\n  - Covariance parameters: sill $s^{2}=1.0$, range $a=40.0\\,\\mathrm{m}$, nugget $\\tau^{2}=0.05$.\n  - Target path (in order): $\\mathbf{x}_{0,1}=(25.0,25.0)$, then $\\mathbf{x}_{0,2}=(30.0,20.0)$.\n  - Random seed: $42$.\n- Case $2$ (short-range boundary condition):\n  - Observed locations and values: $(\\mathbf{x}_{1},z_{1})=((0.0,0.0),1.0)$, $(\\mathbf{x}_{2},z_{2})=((100.0,0.0),-1.0)$.\n  - Covariance parameters: sill $s^{2}=1.0$, range $a=5.0\\,\\mathrm{m}$, nugget $\\tau^{2}=0.2$.\n  - Target path (in order): $\\mathbf{x}_{0,1}=(10.0,0.0)$, then $\\mathbf{x}_{0,2}=(90.0,0.0)$.\n  - Random seed: $123$.\n- Case $3$ (high-nugget edge case with a single datum):\n  - Observed location and value: $(\\mathbf{x}_{1},z_{1})=((10.0,10.0),0.5)$.\n  - Covariance parameters: sill $s^{2}=1.0$, range $a=20.0\\,\\mathrm{m}$, nugget $\\tau^{2}=0.5$.\n  - Target path (in order): $\\mathbf{x}_{0,1}=(12.0,12.0)$, then $\\mathbf{x}_{0,2}=(40.0,40.0)$.\n  - Random seed: $7$.\n\nAngle units do not apply because only Euclidean distances in meters are used. The simulated values are dimensionless and must be presented as floats with no units.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the simulated values at the two targets for Case $1$, followed by the two targets for Case $2$, followed by the two targets for Case $3$, in that order.\n- Each float must be rounded to $6$ decimal places.\n- Example format: $[\\text{value}_{1},\\text{value}_{2},\\text{value}_{3},\\text{value}_{4},\\text{value}_{5},\\text{value}_{6}]$.\n\nYour final answer must be a complete, runnable program that implements the described procedure and emits the output in the exact format specified.",
            "solution": "The problem presented is a well-posed and scientifically grounded task in the field of geostatistics, specifically concerning conditional simulation via the Sequential Gaussian Simulation (SGS) algorithm. All necessary data, models, and parameters are provided, and the procedure is consistent with established geostatistical theory. The problem is therefore deemed valid.\n\nThe core of the task is to derive and implement the ordinary kriging system, which serves as the engine for the SGS process. The SGS algorithm generates a realization of a Gaussian random field by sequentially drawing values from conditional distributions, where each distribution is itself defined by a kriging estimate (mean) and kriging variance.\n\nLet $Z(\\mathbf{x})$ be a stationary Gaussian random field at spatial location $\\mathbf{x} \\in \\mathbb{R}^2$. The field is characterized by a constant unknown mean $E[Z(\\mathbf{x})] = m$ and a covariance function that depends only on the separation vector $\\mathbf{h} = \\mathbf{x}_i - \\mathbf{x}_j$. For this problem, an isotropic exponential covariance model is specified:\n$$\nC(h) = s^2 \\exp\\left(-\\frac{h}{a}\\right)\n$$\nwhere $h = \\|\\mathbf{h}\\|$ is the Euclidean distance, $s^2$ is the sill (the a priori variance of the field, $C(0)$), and $a$ is the practical range. Additionally, a nugget effect $\\tau^2$ is introduced to account for measurement error and micro-scale variability.\n\n**Derivation of the Ordinary Kriging System**\n\nOrdinary Kriging (OK) provides the best linear unbiased estimator (BLUE) for the value $Z(\\mathbf{x}_0)$ at an unsampled location, given $n$ observations $\\{z(\\mathbf{x}_1), \\dots, z(\\mathbf{x}_n)\\}$. The estimator $\\hat{Z}(\\mathbf{x}_0)$ is a linear combination of the data:\n$$\n\\hat{Z}(\\mathbf{x}_0) = \\sum_{i=1}^{n} w_i z(\\mathbf{x}_i)\n$$\n\nThe estimator is designed to satisfy two conditions: unbiasedness and minimum estimation variance.\n\n1.  **Unbiasedness**: The expected value of the estimation error must be zero.\n    $$\n    E[\\hat{Z}(\\mathbf{x}_0) - Z(\\mathbf{x}_0)] = 0\n    $$\n    Substituting the estimator and using the stationarity assumption $E[Z(\\mathbf{x})] = m$:\n    $$\n    E\\left[\\sum_{i=1}^{n} w_i Z(\\mathbf{x}_i) - Z(\\mathbf{x}_0)\\right] = \\sum_{i=1}^{n} w_i E[Z(\\mathbf{x}_i)] - E[Z(\\mathbf{x}_0)] = \\sum_{i=1}^{n} w_i m - m = m\\left(\\sum_{i=1}^{n} w_i - 1\\right) = 0\n    $$\n    For this to hold for any unknown mean $m$, the sum of the weights must be one. This is the unbiasedness constraint:\n    $$\n    \\sum_{i=1}^{n} w_i = 1\n    $$\n\n2.  **Minimum Estimation Variance**: The estimation variance, $\\sigma_K^2 = \\text{Var}[\\hat{Z}(\\mathbf{x}_0) - Z(\\mathbf{x}_0)]$, must be minimized.\n    $$\n    \\sigma_K^2(\\mathbf{x}_0) = \\text{Var}\\left(\\sum_{i=1}^{n} w_i Z(\\mathbf{x}_i) - Z(\\mathbf{x}_0)\\right)\n    $$\n    Expanding the variance gives:\n    $$\n    \\sigma_K^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_i w_j \\text{Cov}(Z(\\mathbf{x}_i), Z(\\mathbf{x}_j)) - 2\\sum_{i=1}^{n} w_i \\text{Cov}(Z(\\mathbf{x}_i), Z(\\mathbf{x}_0)) + \\text{Var}(Z(\\mathbf{x}_0))\n    $$\n    In matrix notation, this is $\\sigma_K^2 = \\mathbf{w}^T \\mathbf{C} \\mathbf{w} - 2\\mathbf{w}^T \\mathbf{c} + C_{00}$, where:\n    -   $\\mathbf{C}$ is the $n \\times n$ covariance matrix between data points. Its elements are $C_{ij} = C(\\|\\mathbf{x}_i - \\mathbf{x}_j\\|) + \\tau^2 \\delta_{ij}$, where the nugget $\\tau^2$ is added to the diagonal elements (for $i=j$).\n    -   $\\mathbf{c}$ is the $n \\times 1$ vector of covariances between data points and the target location: $c_i = C(\\|\\mathbf{x}_i - \\mathbf{x}_0\\|)$. The nugget is not included here.\n    -   $C_{00}$ is the prior variance at the target location, which is the sill $s^2 = C(0)$.\n\n    To minimize $\\sigma_K^2$ subject to the constraint $\\sum w_i = 1$, we use the method of Lagrange multipliers. The objective function is:\n    $$\n    \\mathcal{L}(\\mathbf{w}, \\lambda) = \\sigma_K^2 + 2\\lambda\\left(\\sum_{i=1}^{n} w_i - 1\\right)\n    $$\n    Setting the partial derivatives with respect to each $w_k$ to zero yields a set of $n$ equations:\n    $$\n    \\frac{\\partial\\mathcal{L}}{\\partial w_k} = 2 \\sum_{i=1}^{n} w_i C_{ik} - 2c_k + 2\\lambda = 0 \\quad \\implies \\quad \\sum_{i=1}^{n} w_i C_{ik} + \\lambda = c_k \\quad \\text{for } k=1, \\dots, n\n    $$\n    Combining these equations with the unbiasedness constraint results in the following $(n+1) \\times (n+1)$ linear system, known as the ordinary kriging system:\n    $$\n    \\begin{pmatrix}\n    C_{11} & \\cdots & C_{1n} & 1 \\\\\n    \\vdots & \\ddots & \\vdots & \\vdots \\\\\n    C_{n1} & \\cdots & C_{nn} & 1 \\\\\n    1 & \\cdots & 1 & 0\n    \\end{pmatrix}\n    \\begin{pmatrix}\n    w_1 \\\\\n    \\vdots \\\\\n    w_n \\\\\n    \\lambda\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    c_1 \\\\\n    \\vdots \\\\\n    c_n \\\\\n    1\n    \\end{pmatrix}\n    \\quad \\text{or} \\quad\n    \\begin{pmatrix} \\mathbf{C} & \\mathbf{1} \\\\ \\mathbf{1}^T & 0 \\end{pmatrix}\n    \\begin{pmatrix} \\mathbf{w} \\\\ \\lambda \\end{pmatrix}\n    =\n    \\begin{pmatrix} \\mathbf{c} \\\\ 1 \\end{pmatrix}\n    $$\n    Solving this system yields the optimal weights $\\mathbf{w}$ and the Lagrange multiplier $\\lambda$. The kriging estimate is then $\\hat{Z}(\\mathbf{x}_0) = \\mathbf{w}^T \\mathbf{z}$, where $\\mathbf{z}$ is the vector of observed values. The minimized kriging variance is given by:\n    $$\n    \\sigma_K^2 = C_{00} - \\mathbf{w}^T\\mathbf{c} - \\lambda = s^2 - \\sum_{i=1}^{n} w_i c_i - \\lambda\n    $$\n\n**Sequential Gaussian Simulation (SGS) Algorithm**\n\nSGS relies on a fundamental property of multivariate Gaussian distributions: the conditional distribution of any subset of variables, given another subset, is also Gaussian. For a Gaussian random field, the conditional distribution of the value at an unsampled location $Z(\\mathbf{x}_0)$, given the data $\\mathbf{z}$, is a normal distribution whose mean and variance are precisely the ordinary kriging estimate $\\hat{Z}_{OK}(\\mathbf{x}_0)$ and variance $\\sigma_K^2(\\mathbf{x}_0)$, respectively.\n$$\nZ(\\mathbf{x}_0) | \\{\\mathbf{z}\\} \\sim \\mathcal{N}\\left(\\hat{Z}_{OK}(\\mathbf{x}_0), \\sigma_K^2(\\mathbf{x}_0)\\right)\n$$\nThe SGS algorithm proceeds by defining a random path through the unsampled grid nodes. At each node, it performs ordinary kriging using all available conditioning data (original measurements plus previously simulated values) to define the conditional distribution. It then draws a single value from this distribution, which is immediately added to the conditioning dataset for all subsequent estimations.\n\nThe procedure to be implemented is as follows:\n1.  Initialize the set of conditioning data with the observed locations and values.\n2.  Iterate through the specified path of target locations. For each target location $\\mathbf{x}_{0,j}$:\n    a. Construct the ordinary kriging system using the current set of $n$ conditioning data points (original and previously simulated). This involves building the $(n+1) \\times (n+1)$ matrix $\\mathbf{A} = \\begin{pmatrix} \\mathbf{C} & \\mathbf{1} \\\\ \\mathbf{1}^T & 0 \\end{pmatrix}$ and the right-hand-side vector $\\mathbf{b} = \\begin{pmatrix} \\mathbf{c} \\\\ 1 \\end{pmatrix}$.\n    b. Solve the system $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ for the weights $\\mathbf{w}$ and the Lagrange multiplier $\\lambda$.\n    c. Compute the kriging mean $\\hat{z}_{j} = \\mathbf{w}^T \\mathbf{z}$ and variance $\\sigma_{K,j}^2 = s^2 - \\mathbf{w}^T \\mathbf{c} - \\lambda$. If $\\sigma_{K,j}^2$ is non-positive due to numerical precision issues, it is clipped to a small positive value.\n    d. Draw a simulated value $z_{\\text{sim},j}$ from the Gaussian distribution $\\mathcal{N}(\\hat{z}_{j}, \\sigma_{K,j}^2)$.\n    e. Add the pair $(\\mathbf{x}_{0,j}, z_{\\text{sim},j})$ to the conditioning dataset for subsequent steps.\n3.  The collection of simulated values $\\{z_{\\text{sim},j}\\}$ constitutes one realization of the random field at the target locations.\nThe implementation will use a specified random seed to ensure reproducibility of the draws from the conditional distributions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve as linsolve\n\ndef solve():\n    \"\"\"\n    Main function to run Sequential Gaussian Simulation for all test cases.\n    \"\"\"\n\n    def covariance_model(h, sill, range_val):\n        \"\"\"\n        Computes the exponential covariance for a given distance.\n        C(h) = s^2 * exp(-h/a)\n        \"\"\"\n        return sill * np.exp(-h / range_val)\n\n    def solve_sgs_case(data, targets, sill, range_val, nugget, seed):\n        \"\"\"\n        Performs Sequential Gaussian Simulation for a single test case.\n\n        Args:\n            data (list): List of tuples, where each tuple is ((x, y), value).\n            targets (np.ndarray): Array of target locations, shape (m, 2).\n            sill (float): The sill parameter of the covariance model.\n            range_val (float): The range parameter of the covariance model.\n            nugget (float): The nugget effect.\n            seed (int): The random seed for reproducibility.\n\n        Returns:\n            list: A list of simulated values at the target locations.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Initialize conditioning data from the problem statement\n        if data:\n            cond_locs = np.array([d[0] for d in data])\n            cond_vals = np.array([d[1] for d in data])\n        else: # Handle case with no initial data if needed, though problem implies at least one\n            cond_locs = np.empty((0, 2))\n            cond_vals = np.empty(0)\n        \n        simulated_values = []\n\n        for target_loc in targets:\n            n = len(cond_locs)\n            \n            # --- 1. Construct the Ordinary Kriging system Ax = b ---\n            A = np.ones((n + 1, n + 1))\n            \n            # Data-to-data covariance matrix (top-left n x n block of A)\n            if n > 0:\n                dist_matrix_dd = np.linalg.norm(cond_locs[:, np.newaxis, :] - cond_locs[np.newaxis, :, :], axis=2)\n                C = covariance_model(dist_matrix_dd, sill, range_val)\n                np.fill_diagonal(C, C.diagonal() + nugget) # Add nugget to the diagonal\n                A[:n, :n] = C\n            \n            A[n, n] = 0.0  # Constraint part of the matrix\n\n            # Right-hand side vector b\n            b = np.ones(n + 1)\n            if n > 0:\n                # Data-to-target covariance vector\n                dist_vector_dt = np.linalg.norm(cond_locs - target_loc, axis=1)\n                c_vector = covariance_model(dist_vector_dt, sill, range_val)\n                b[:n] = c_vector\n            else:\n                c_vector = np.empty(0)\n            \n            # --- 2. Solve for kriging weights and Lagrange multiplier ---\n            try:\n                # The kriging matrix is symmetric but not necessarily positive definite\n                weights_and_mu = linsolve(A, b, assume_a='sym')\n            except np.linalg.LinAlgError:\n                # Fallback for singular matrix, although the nugget should prevent this\n                weights_and_mu = np.linalg.lstsq(A, b, rcond=None)[0]\n\n            weights = weights_and_mu[:n]\n            lagrange_mu = weights_and_mu[n]\n            \n            # --- 3. Compute kriging estimate (mean) and variance ---\n            kriging_mean = np.dot(weights, cond_vals) if n > 0 else 0.0 # Field mean is 0\n            kriging_var = sill - np.dot(weights, c_vector) - lagrange_mu\n            \n            # Clip variance to a small positive number if non-positive\n            if kriging_var = 1e-12:\n                kriging_var = 1e-9\n            \n            kriging_std = np.sqrt(kriging_var)\n            \n            # --- 4. Sample from the conditional Gaussian distribution ---\n            sim_val = rng.normal(loc=kriging_mean, scale=kriging_std)\n            simulated_values.append(sim_val)\n            \n            # --- 5. Update the conditioning set for the next step ---\n            cond_locs = np.vstack([cond_locs, target_loc])\n            cond_vals = np.append(cond_vals, sim_val)\n            \n        return simulated_values\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"data\": [((0.0, 0.0), 0.2), ((50.0, 0.0), -0.1), ((0.0, 50.0), 0.3), ((50.0, 50.0), -0.2)],\n            \"targets\": [(25.0, 25.0), (30.0, 20.0)],\n            \"sill\": 1.0, \"range_val\": 40.0, \"nugget\": 0.05, \"seed\": 42\n        },\n        # Case 2\n        {\n            \"data\": [((0.0, 0.0), 1.0), ((100.0, 0.0), -1.0)],\n            \"targets\": [(10.0, 0.0), (90.0, 0.0)],\n            \"sill\": 1.0, \"range_val\": 5.0, \"nugget\": 0.2, \"seed\": 123\n        },\n        # Case 3\n        {\n            \"data\": [((10.0, 10.0), 0.5)],\n            \"targets\": [(12.0, 12.0), (40.0, 40.0)],\n            \"sill\": 1.0, \"range_val\": 20.0, \"nugget\": 0.5, \"seed\": 7\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = solve_sgs_case(\n            data=case[\"data\"],\n            targets=np.array(case[\"targets\"]),\n            sill=case[\"sill\"],\n            range_val=case[\"range_val\"],\n            nugget=case[\"nugget\"],\n            seed=case[\"seed\"]\n        )\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}