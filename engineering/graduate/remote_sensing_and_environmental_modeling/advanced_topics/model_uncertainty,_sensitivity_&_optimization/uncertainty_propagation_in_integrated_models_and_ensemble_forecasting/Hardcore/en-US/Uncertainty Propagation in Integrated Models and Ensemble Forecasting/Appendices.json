{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational practice in uncertainty analysis: propagating errors through a model using analytical methods. This exercise explores how uncertainties in the parameters of a Bidirectional Reflectance Distribution Function (BRDF) model, along with uncertainty in an input variable, combine to affect the final derived quantity, in this case, the black-sky albedo. By applying the first-order Taylor series expansion, or the law of propagation of uncertainty, you will gain hands-on experience with the mechanics of error propagation, including the critical role of the covariance matrix in accounting for correlated parameter uncertainties .",
            "id": "3863122",
            "problem": "Consider a parameterized bidirectional reflectance distribution function (BRDF) for a homogeneous land surface, modeled at a single spectral band as $r(\\theta_s, \\theta_v, \\phi) = a + b \\cos(\\theta_v) + c \\cos(\\theta_s)$, where $r$ is the bidirectional reflectance factor, $\\theta_s$ is the solar zenith angle, $\\theta_v$ is the view zenith angle, and $\\phi$ is the relative azimuth angle. The directional-hemispherical reflectance (black-sky albedo) at solar zenith $\\theta_s$ is defined by the hemispherical integral $A(\\theta_s) = \\int_{\\Omega_{v}} r(\\theta_s, \\theta_v, \\phi) \\cos(\\theta_v) \\, \\mathrm{d}\\Omega_{v}$ over the viewing hemisphere, where $\\mathrm{d}\\Omega_{v} = \\sin(\\theta_v) \\, \\mathrm{d}\\theta_v \\, \\mathrm{d}\\phi$ and the integration domain is $\\theta_v \\in [0, \\pi/2]$, $\\phi \\in [0, 2\\pi]$. A remote sensing inversion has produced parameter estimates $a$, $b$, $c$ with joint uncertainty characterized by the covariance matrix $\\Sigma$:\n$$\n\\Sigma = \\begin{pmatrix}\n0.0004 & 0.0001 & -0.00005 \\\\\n0.0001 & 0.0009 & 0.00002 \\\\\n-0.00005 & 0.00002 & 0.00025\n\\end{pmatrix},\n$$\nand mean parameter values $a = 0.12$, $b = 0.08$, $c = 0.05$. The observed solar zenith angle is $\\theta_s = 0.7$ and its measurement uncertainty is characterized by a standard deviation $\\sigma_{\\theta} = 0.02$. Assume that the parameter uncertainties and the solar zenith angle uncertainty are statistically independent and that the law of propagation of uncertainty via first-order linearization (Taylor series) around the observed angle applies. Using these data and assumptions, compute the standard uncertainty (one standard deviation) of the black-sky albedo $A(\\theta_s)$ at the observed solar zenith angle. All angles are in radians, and you may treat $\\pi$ as the circle constant. Express the final uncertainty as a decimal fraction with no unit and round your answer to four significant figures.",
            "solution": "The problem requires the computation of the standard uncertainty of the black-sky albedo, $A(\\theta_s)$, based on the uncertainties of its input variables. The albedo is a function of the bidirectional reflectance distribution function (BRDF) parameters $a$, $b$, $c$ and the solar zenith angle $\\theta_s$. The uncertainty in $A$ will be determined using the law of propagation of uncertainty, which is justified by the problem's statement to use a first-order Taylor series linearization.\n\nFirst, we must derive the analytical expression for the black-sky albedo, $A(\\theta_s)$. It is defined as the directional-hemispherical reflectance, given by the integral of the BRDF over the viewing hemisphere:\n$$ A(\\theta_s) = \\int_{\\Omega_{v}} r(\\theta_s, \\theta_v, \\phi) \\cos(\\theta_v) \\, \\mathrm{d}\\Omega_{v} $$\nThe differential solid angle for the viewing direction is $\\mathrm{d}\\Omega_{v} = \\sin(\\theta_v) \\, \\mathrm{d}\\theta_v \\, \\mathrm{d}\\phi$, with integration limits for the hemisphere being $\\theta_v \\in [0, \\pi/2]$ and $\\phi \\in [0, 2\\pi]$. The given BRDF model is $r(\\theta_s, \\theta_v, \\phi) = a + b \\cos(\\theta_v) + c \\cos(\\theta_s)$.\n\nWe substitute the BRDF model into the integral for $A(\\theta_s)$:\n$$ A(\\theta_s) = \\int_{0}^{2\\pi} \\int_{0}^{\\pi/2} (a + b \\cos(\\theta_v) + c \\cos(\\theta_s)) \\cos(\\theta_v) \\sin(\\theta_v) \\, \\mathrm{d}\\theta_v \\, \\mathrm{d}\\phi $$\nThe integrand does not depend on the relative azimuth angle $\\phi$, so the integration over $\\phi$ from $0$ to $2\\pi$ introduces a factor of $2\\pi$:\n$$ A(\\theta_s) = 2\\pi \\int_{0}^{\\pi/2} (a \\cos(\\theta_v) \\sin(\\theta_v) + b \\cos^2(\\theta_v) \\sin(\\theta_v) + c \\cos(\\theta_s) \\cos(\\theta_v) \\sin(\\theta_v)) \\, \\mathrm{d}\\theta_v $$\nTo evaluate the integral over $\\theta_v$, we perform a substitution with $u = \\cos(\\theta_v)$, which implies $\\mathrm{d}u = -\\sin(\\theta_v) \\mathrm{d}\\theta_v$. The integration limits transform accordingly: when $\\theta_v=0$, $u=1$, and when $\\theta_v=\\pi/2$, $u=0$.\n$$ A(\\theta_s) = 2\\pi \\int_{1}^{0} (a u + b u^2 + c u \\cos(\\theta_s)) (-\\mathrm{d}u) = 2\\pi \\int_{0}^{1} (a u + b u^2 + c u \\cos(\\theta_s)) \\, \\mathrm{d}u $$\nPerforming the polynomial integration yields:\n$$ A(\\theta_s) = 2\\pi \\left[ \\frac{a u^2}{2} + \\frac{b u^3}{3} + \\frac{c u^2}{2} \\cos(\\theta_s) \\right]_{0}^{1} $$\n$$ A(\\theta_s) = 2\\pi \\left( \\frac{a}{2} + \\frac{b}{3} + \\frac{c}{2} \\cos(\\theta_s) \\right) $$\nThis simplifies to the final analytical expression for the albedo:\n$$ A(a,b,c,\\theta_s) = \\pi a + \\frac{2\\pi}{3} b + \\pi c \\cos(\\theta_s) $$\n\nThe problem states that the uncertainties in the parameters $(a,b,c)$ and the uncertainty in the angle $\\theta_s$ are statistically independent. Therefore, the total variance of $A$, denoted $\\sigma_A^2$, is the sum of the variances contributed by these independent sources:\n$$ \\sigma_A^2 = \\sigma_{A, \\text{params}}^2 + \\sigma_{A, \\theta_s}^2 $$\nThe first term, $\\sigma_{A, \\text{params}}^2$, arises from the parameter uncertainties and is calculated as $J \\Sigma J^T$, where $J$ is the Jacobian row vector of the partial derivatives of $A$ with respect to the parameters $(a,b,c)$, and $\\Sigma$ is their given covariance matrix. The second term, $\\sigma_{A, \\theta_s}^2$, arises from the angle uncertainty and is calculated as $(\\frac{\\partial A}{\\partial \\theta_s})^2 \\sigma_{\\theta}^2$, where $\\sigma_{\\theta}$ is the standard deviation of $\\theta_s$.\n\nWe compute the necessary partial derivatives of $A(a,b,c,\\theta_s)$:\n$$ \\frac{\\partial A}{\\partial a} = \\pi $$\n$$ \\frac{\\partial A}{\\partial b} = \\frac{2\\pi}{3} $$\n$$ \\frac{\\partial A}{\\partial c} = \\pi \\cos(\\theta_s) $$\n$$ \\frac{\\partial A}{\\partial \\theta_{s}} = -\\pi c \\sin(\\theta_s) $$\n\nThe variance from the parameters is:\n$$ \\sigma_{A, \\text{params}}^2 = \\begin{pmatrix} \\frac{\\partial A}{\\partial a} & \\frac{\\partial A}{\\partial b} & \\frac{\\partial A}{\\partial c} \\end{pmatrix} \\Sigma \\begin{pmatrix} \\frac{\\partial A}{\\partial a} \\\\ \\frac{\\partial A}{\\partial b} \\\\ \\frac{\\partial A}{\\partial c} \\end{pmatrix} $$\nWe evaluate the derivatives at the given value $\\theta_s = 0.7$ radians. The variance expression can be expanded as:\n$$ \\sigma_{A, \\text{params}}^2 = \\left(\\frac{\\partial A}{\\partial a}\\right)^2 \\Sigma_{aa} + \\left(\\frac{\\partial A}{\\partial b}\\right)^2 \\Sigma_{bb} + \\left(\\frac{\\partial A}{\\partial c}\\right)^2 \\Sigma_{cc} + 2\\frac{\\partial A}{\\partial a}\\frac{\\partial A}{\\partial b}\\Sigma_{ab} + 2\\frac{\\partial A}{\\partial a}\\frac{\\partial A}{\\partial c}\\Sigma_{ac} + 2\\frac{\\partial A}{\\partial b}\\frac{\\partial A}{\\partial c}\\Sigma_{bc} $$\nSubstituting the derivatives and the components of $\\Sigma = \\begin{pmatrix} 0.0004 & 0.0001 & -0.00005 \\\\ 0.0001 & 0.0009 & 0.00002 \\\\ -0.00005 & 0.00002 & 0.00025 \\end{pmatrix}$:\n$$ \\sigma_{A, \\text{params}}^2 = \\pi^2 (0.0004) + \\left(\\frac{2\\pi}{3}\\right)^2 (0.0009) + (\\pi\\cos(0.7))^2 (0.00025) + 2(\\pi)\\left(\\frac{2\\pi}{3}\\right)(0.0001) + 2(\\pi)(\\pi\\cos(0.7))(-0.00005) + 2\\left(\\frac{2\\pi}{3}\\right)(\\pi\\cos(0.7))(0.00002) $$\n$$ \\sigma_{A, \\text{params}}^2 = \\pi^2 \\left[ 0.0004 + \\frac{4}{9}(0.0009) + \\cos^2(0.7)(0.00025) + \\frac{4}{3}(0.0001) - 2\\cos(0.7)(0.00005) + \\frac{4}{3}\\cos(0.7)(0.00002) \\right] $$\nUsing $\\theta_s = 0.7$, we find $\\cos(0.7) \\approx 0.764842$:\n$$ \\sigma_{A, \\text{params}}^2 \\approx \\pi^2 [ 0.0004 + 0.0004 + (0.58498)(0.00025) + 0.00013333 - (1.52968)(0.00005) + (1.01979)(0.00002) ] $$\n$$ \\sigma_{A, \\text{params}}^2 \\approx \\pi^2 [ 0.0008 + 0.00014625 + 0.00013333 - 0.00007648 + 0.00002040 ] $$\n$$ \\sigma_{A, \\text{params}}^2 \\approx \\pi^2 [ 0.0010235 ] \\approx 0.0100998 $$\n\nThe variance from the angle uncertainty, with $c=0.05$ and $\\sigma_{\\theta}=0.02$, is:\n$$ \\sigma_{A, \\theta_s}^2 = \\left( \\frac{\\partial A}{\\partial \\theta_s} \\right)^2 \\sigma_{\\theta}^2 = (-\\pi c \\sin(\\theta_s))^2 \\sigma_{\\theta}^2 = \\pi^2 c^2 \\sin^2(\\theta_s) \\sigma_{\\theta}^2 $$\nUsing $\\sin(0.7) \\approx 0.644218$:\n$$ \\sigma_{A, \\theta_s}^2 = \\pi^2 (0.05)^2 \\sin^2(0.7) (0.02)^2 \\approx \\pi^2 (0.0025) (0.644218)^2 (0.0004) $$\n$$ \\sigma_{A, \\theta_s}^2 \\approx \\pi^2 (0.0025) (0.415015) (0.0004) \\approx (9.8696) (4.15015 \\times 10^{-7}) \\approx 0.000004096 $$\n\nThe total variance is the sum of these two components:\n$$ \\sigma_A^2 = \\sigma_{A, \\text{params}}^2 + \\sigma_{A, \\theta_s}^2 \\approx 0.0100998 + 0.000004096 = 0.0101039 $$\nThe standard uncertainty, $\\sigma_A$, is the square root of the total variance:\n$$ \\sigma_A = \\sqrt{\\sigma_A^2} \\approx \\sqrt{0.0101039} \\approx 0.100518 $$\nRounding the result to four significant figures as requested gives $0.1005$.",
            "answer": "$$\\boxed{0.1005}$$"
        },
        {
            "introduction": "Having established the mechanics of uncertainty propagation, we now apply these concepts within the context of an integrated model. This practice delves into a common scenario in data assimilation: fusing a model's background forecast with multiple, correlated observations . By analytically deriving the posterior variance under both correct and naive assumptions about the observation error structure, you will quantify the critical impact of neglecting error correlations—a frequent oversight that can lead to significant overconfidence in the assimilated state.",
            "id": "3863088",
            "problem": "A land surface data assimilation system integrates a thermodynamic land surface model with satellite thermal infrared radiances to estimate a scalar near-surface kinetic temperature $x$ over a homogeneous pixel. The forecast step of the integrated model produces a background (prior) ensemble with mean $x_{b}$ and variance $\\sigma_{b}^{2}$, and the observation operator is linear, $h(x)=x$, as the radiance-to-temperature retrieval has already been performed. Two co-orbital satellite retrievals $y_{1}$ and $y_{2}$ are available for the same pixel within a short time window. Due to common atmospheric correction and shared forward-model components, the retrieval errors are correlated. Let the observation error vector be $\\mathbf{\\varepsilon}=\\begin{pmatrix}\\varepsilon_{1} & \\varepsilon_{2}\\end{pmatrix}^{\\top}$ with $\\mathbb{E}[\\varepsilon_{i}]=0$, $\\mathrm{Var}(\\varepsilon_{i})=\\sigma_{o}^{2}$, and $\\mathrm{Corr}(\\varepsilon_{1},\\varepsilon_{2})=\\rho$ with $-1<\\rho<1$. The true observation error covariance is thus $R_{\\mathrm{true}}=\\sigma_{o}^{2}\\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}$.\n\nAssume linear-Gaussian data assimilation and derive the analysis (posterior) variance for $x$ from first principles by applying Bayes’ rule to the Gaussian prior $x\\sim\\mathcal{N}(x_{b},\\sigma_{b}^{2})$ and the Gaussian likelihood $\\mathbf{y}|x \\sim \\mathcal{N}(H x, R)$, where $H=\\begin{pmatrix}1 & 1\\end{pmatrix}^{\\top}$. First, obtain the analysis variance $\\sigma_{a,\\mathrm{true}}^{2}$ using $R=R_{\\mathrm{true}}$. Then, obtain the analysis variance $\\sigma_{a,\\mathrm{diag}}^{2}$ under a diagonal covariance assumption that neglects correlation, i.e., $R_{\\mathrm{diag}}=\\sigma_{o}^{2}I_{2}$.\n\nUsing these results, show how neglecting error correlation inflates perceived observation information content by comparing the observation-induced precision increment terms, and quantify the resulting posterior variance bias. Specifically, provide a closed-form analytic expression for the multiplicative bias factor in the posterior variance,\n$$B=\\frac{\\sigma_{a,\\mathrm{diag}}^{2}}{\\sigma_{a,\\mathrm{true}}^{2}},$$\nin terms of $\\sigma_{b}^{2}$, $\\sigma_{o}^{2}$, and $\\rho$. Your final answer must be a single closed-form analytic expression for $B$. No rounding is required. Because $B$ is dimensionless, do not attach units to your final answer.",
            "solution": "The analysis is grounded in the principles of linear-Gaussian Bayesian inference. The posterior probability distribution for the state variable $x$ given the measurements $\\mathbf{y}$ is proportional to the product of the prior distribution and the likelihood function, $p(x|\\mathbf{y}) \\propto p(\\mathbf{y}|x)p(x)$. Since both the prior and the likelihood are Gaussian, the posterior is also a Gaussian distribution, $x|\\mathbf{y} \\sim \\mathcal{N}(x_{a}, \\sigma_{a}^{2})$. The posterior (analysis) variance, $\\sigma_{a}^{2}$, is determined by the prior (background) variance, $\\sigma_{b}^{2}$, and the observation uncertainty. The general formula for the inverse of the analysis variance, known as the analysis precision, is given by:\n$$(\\sigma_{a}^{2})^{-1} = (\\sigma_{b}^{2})^{-1} + H^{\\top} R^{-1} H$$\nwhere $H$ is the observation operator matrix and $R$ is the observation error covariance matrix. We will apply this formula for the two specified cases for $R$.\n\nFirst, we derive the analysis variance $\\sigma_{a,\\mathrm{true}}^{2}$ using the true observation error covariance matrix, $R_{\\mathrm{true}}$.\nThe given quantities are:\nThe observation operator matrix for the scalar state $x$ and two observations is $H = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe true observation error covariance matrix is $R_{\\mathrm{true}} = \\sigma_{o}^{2}\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$.\n\nTo calculate the term $H^{\\top} R_{\\mathrm{true}}^{-1} H$, we first need the inverse of $R_{\\mathrm{true}}$. The determinant of $R_{\\mathrm{true}}$ is $\\det(R_{\\mathrm{true}}) = (\\sigma_{o}^{2})^{2}(1) - (\\rho\\sigma_{o}^{2})^{2} = \\sigma_{o}^{4}(1-\\rho^{2})$.\nThe inverse is:\n$$R_{\\mathrm{true}}^{-1} = \\frac{1}{\\sigma_{o}^{4}(1-\\rho^{2})} \\begin{pmatrix} \\sigma_{o}^{2} & -\\rho\\sigma_{o}^{2} \\\\ -\\rho\\sigma_{o}^{2} & \\sigma_{o}^{2} \\end{pmatrix} = \\frac{1}{\\sigma_{o}^{2}(1-\\rho^{2})} \\begin{pmatrix} 1 & -\\rho \\\\ -\\rho & 1 \\end{pmatrix}$$\nNow, we compute the observation-induced precision increment, $H^{\\top} R_{\\mathrm{true}}^{-1} H$:\n$$H^{\\top} R_{\\mathrm{true}}^{-1} H = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\left( \\frac{1}{\\sigma_{o}^{2}(1-\\rho^{2})} \\begin{pmatrix} 1 & -\\rho \\\\ -\\rho & 1 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n$$= \\frac{1}{\\sigma_{o}^{2}(1-\\rho^{2})} \\begin{pmatrix} 1-\\rho & 1-\\rho \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n$$= \\frac{1}{\\sigma_{o}^{2}(1-\\rho^{2})} ((1-\\rho) + (1-\\rho)) = \\frac{2(1-\\rho)}{\\sigma_{o}^{2}(1-\\rho)(1+\\rho)} = \\frac{2}{\\sigma_{o}^{2}(1+\\rho)}$$\nThe analysis precision is the sum of the background precision and this observation precision increment:\n$$(\\sigma_{a,\\mathrm{true}}^{2})^{-1} = \\frac{1}{\\sigma_{b}^{2}} + \\frac{2}{\\sigma_{o}^{2}(1+\\rho)} = \\frac{\\sigma_{o}^{2}(1+\\rho) + 2\\sigma_{b}^{2}}{\\sigma_{b}^{2}\\sigma_{o}^{2}(1+\\rho)}$$\nInverting this expression gives the true analysis variance:\n$$\\sigma_{a,\\mathrm{true}}^{2} = \\frac{\\sigma_{b}^{2}\\sigma_{o}^{2}(1+\\rho)}{\\sigma_{o}^{2}(1+\\rho) + 2\\sigma_{b}^{2}}$$\n\nNext, we derive the analysis variance $\\sigma_{a,\\mathrm{diag}}^{2}$ under the incorrect assumption that the observation errors are uncorrelated. This corresponds to using the diagonal covariance matrix $R_{\\mathrm{diag}} = \\sigma_{o}^{2}I_{2}$.\n$$R_{\\mathrm{diag}} = \\sigma_{o}^{2}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\sigma_{o}^{2} & 0 \\\\ 0 & \\sigma_{o}^{2} \\end{pmatrix}$$\nThe inverse is straightforward:\n$$R_{\\mathrm{diag}}^{-1} = \\frac{1}{\\sigma_{o}^{2}}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\nThe perceived observation-induced precision increment is:\n$$H^{\\top} R_{\\mathrm{diag}}^{-1} H = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\left( \\frac{1}{\\sigma_{o}^{2}}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sigma_{o}^{2}}\\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sigma_{o}^{2}}(1+1) = \\frac{2}{\\sigma_{o}^{2}}$$\nComparing this perceived precision increment, $\\frac{2}{\\sigma_{o}^{2}}$, with the true precision increment, $\\frac{2}{\\sigma_{o}^{2}(1+\\rho)}$, reveals how neglecting correlation affects the perceived information content. If the errors are positively correlated ($\\rho > 0$), then $1+\\rho > 1$, and the true precision increment $\\frac{2}{\\sigma_{o}^{2}(1+\\rho)}$ is smaller than the perceived increment $\\frac{2}{\\sigma_{o}^{2}}$. This means that ignoring positive correlation leads to an overestimation, or inflation, of the information content provided by the observations. The ratio of perceived to true information content is $(\\frac{2}{\\sigma_{o}^{2}}) / (\\frac{2}{\\sigma_{o}^{2}(1+\\rho)}) = 1+\\rho$.\n\nThe analysis precision under the diagonal assumption is:\n$$(\\sigma_{a,\\mathrm{diag}}^{2})^{-1} = \\frac{1}{\\sigma_{b}^{2}} + \\frac{2}{\\sigma_{o}^{2}} = \\frac{\\sigma_{o}^{2} + 2\\sigma_{b}^{2}}{\\sigma_{b}^{2}\\sigma_{o}^{2}}$$\nInverting this expression gives the analysis variance under the diagonal assumption:\n$$\\sigma_{a,\\mathrm{diag}}^{2} = \\frac{\\sigma_{b}^{2}\\sigma_{o}^{2}}{\\sigma_{o}^{2} + 2\\sigma_{b}^{2}}$$\n\nFinally, we compute the multiplicative bias factor $B$ by taking the ratio of the calculated variances:\n$$B = \\frac{\\sigma_{a,\\mathrm{diag}}^{2}}{\\sigma_{a,\\mathrm{true}}^{2}} = \\frac{\\frac{\\sigma_{b}^{2}\\sigma_{o}^{2}}{\\sigma_{o}^{2} + 2\\sigma_{b}^{2}}}{\\frac{\\sigma_{b}^{2}\\sigma_{o}^{2}(1+\\rho)}{\\sigma_{o}^{2}(1+\\rho) + 2\\sigma_{b}^{2}}}$$\nWe can cancel the common term $\\sigma_{b}^{2}\\sigma_{o}^{2}$ from the numerator of both fractions:\n$$B = \\frac{1/(\\sigma_{o}^{2} + 2\\sigma_{b}^{2})}{(1+\\rho)/(\\sigma_{o}^{2}(1+\\rho) + 2\\sigma_{b}^{2})} = \\frac{\\sigma_{o}^{2}(1+\\rho) + 2\\sigma_{b}^{2}}{(\\sigma_{o}^{2} + 2\\sigma_{b}^{2})(1+\\rho)}$$\nThis is the final closed-form analytic expression for the multiplicative bias factor.",
            "answer": "$$\\boxed{\\frac{\\sigma_{o}^{2}(1+\\rho) + 2\\sigma_{b}^{2}}{(\\sigma_{o}^{2} + 2\\sigma_{b}^{2})(1+\\rho)}}$$"
        },
        {
            "introduction": "The final practice moves from improving a single model to optimally combining forecasts from multiple models, a cornerstone of modern ensemble forecasting. This exercise tasks you with developing a dynamic weighting scheme from first principles, using a strictly proper scoring rule to evaluate the historical performance of each model . This approach, rooted in Bayesian Model Averaging, provides a theoretically sound method for constructing a superior consensus forecast by rewarding models based on their demonstrated probabilistic skill.",
            "id": "3863176",
            "problem": "You are given a set of probabilistic models that provide predictive distributions for an environmental quantity observed over multiple time points in an integrated modeling context combining remote sensing data and process-based models. The models are to be combined into a multi-model ensemble with dynamically adjusted weights based on their probabilistic skill scores. Your task is to design and implement a program that, for each test case, computes the final ensemble weights from a calibration period by using a strictly proper scoring rule as the skill metric, and then outputs the resulting weights. All required quantities are dimensionless, and the outputs must be decimal numbers (not fractions or percentages).\n\nThe ensemble must be justified by a proper scoring rule that is strictly proper for probabilistic forecasts. You must construct the weights so that they are consistent with maximizing the expected utility under that proper scoring rule across the calibration period, assuming that the calibration observations are independent conditional on each model’s predictive distribution. Your program must implement this principle without invoking any domain-specific shortcuts. The resulting weights must be nonnegative and sum to $1$ for each test case.\n\nThe ensemble is formed from models that, at each calibration time index $t \\in \\{1,\\dots,T\\}$, output a Gaussian predictive distribution with mean $\\mu_{i,t}$ and variance $\\sigma_{i,t}^{2}$ for model index $i \\in \\{1,\\dots,M\\}$. The observation at time $t$ is $y_{t}$. You must use a strictly proper scoring rule for continuous probabilistic forecasts to assess skill per model across the calibration period. Then, you must derive a dynamic weighting scheme that aggregates model skill over the calibration period to produce final weights $w_{i}$ satisfying $\\sum_{i=1}^{M} w_{i} = 1$ and $w_{i} \\ge 0$ for all $i$. The derivation must be based on first principles of strictly proper scoring rules; the weighting scheme must be justified as achieving the maximization of expected scoring-rule utility under the independence assumption across time.\n\nYour program must implement the following steps for each test case:\n- Accept as input parameters (hard-coded within the program) the calibration observations $\\{y_{t}\\}_{t=1}^{T}$ and, for each model $i$, the sequences $\\{\\mu_{i,t}\\}_{t=1}^{T}$ and $\\{\\sigma_{i,t}\\}_{t=1}^{T}$ with $\\sigma_{i,t} > 0$ for all $i$ and $t$.\n- Compute per-model aggregate skill across the calibration period using a strictly proper scoring rule suitable for continuous predictive densities.\n- Derive and compute final ensemble weights $\\{w_{i}\\}_{i=1}^{M}$ that reflect dynamic adjustment based on the aggregate skill, ensuring they lie on the probability simplex.\n- Output the weights for each test case in the required final output format described below.\n\nTest suite specification:\n- Test Case $1$ (happy path): $M = 2$, $T = 5$.\n  - Observations: $y = [0.9, 0.6, -0.2, 0.4, 0.7]$.\n  - Model $1$ means: $\\mu_{1} = [0.85, 0.62, -0.19, 0.35, 0.75]$.\n  - Model $1$ standard deviations: $\\sigma_{1} = [0.25, 0.25, 0.30, 0.25, 0.25]$.\n  - Model $2$ means: $\\mu_{2} = [0.30, 0.10, -0.50, 0.00, 0.20]$.\n  - Model $2$ standard deviations: $\\sigma_{2} = [0.40, 0.40, 0.50, 0.40, 0.40]$.\n- Test Case $2$ (near-tie among models): $M = 3$, $T = 4$.\n  - Observations: $y = [2.0, 2.5, 1.5, 2.2]$.\n  - Model $1$ means: $\\mu_{1} = [1.9, 2.6, 1.6, 2.1]$.\n  - Model $1$ standard deviations: $\\sigma_{1} = [0.30, 0.30, 0.30, 0.30]$.\n  - Model $2$ means: $\\mu_{2} = [2.1, 2.4, 1.4, 2.3]$.\n  - Model $2$ standard deviations: $\\sigma_{2} = [0.30, 0.30, 0.30, 0.30]$.\n  - Model $3$ means: $\\mu_{3} = [2.0, 2.5, 1.5, 2.2]$.\n  - Model $3$ standard deviations: $\\sigma_{3} = [0.35, 0.35, 0.35, 0.35]$.\n- Test Case $3$ (edge case with an overconfident biased model): $M = 2$, $T = 5$.\n  - Observations: $y = [-0.5, -0.4, -0.6, -0.55, -0.45]$.\n  - Model $1$ means: $\\mu_{1} = [-0.45, -0.38, -0.62, -0.50, -0.47]$.\n  - Model $1$ standard deviations: $\\sigma_{1} = [0.15, 0.15, 0.15, 0.15, 0.15]$.\n  - Model $2$ means: $\\mu_{2} = [-0.10, -0.10, -0.10, -0.10, -0.10]$.\n  - Model $2$ standard deviations: $\\sigma_{2} = [0.05, 0.05, 0.05, 0.05, 0.05]$.\n\nAnswer specification:\n- For each test case, your program must compute the final ensemble weights $\\{w_{i}\\}_{i=1}^{M}$ based on the strictly proper scoring-rule principle described above and output them as decimal numbers.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list of weights for one test case. For example, the output should look like $[[w_{1}^{(1)}, w_{2}^{(1)}], [w_{1}^{(2)}, w_{2}^{(2)}, w_{3}^{(2)}], [w_{1}^{(3)}, w_{2}^{(3)}]]$, with no extra text. Each $w_{i}^{(j)}$ must be a decimal number in $[0,1]$ and the weights in each inner list must sum to $1$.",
            "solution": "The user wants me to solve a problem about creating ensemble weights for probabilistic forecasts.\n\n### Step 1: Extract Givens\n- **Context**: An integrated modeling scenario combining remote sensing and process-based models to predict an environmental quantity.\n- **Ensemble structure**: A multi-model ensemble with dynamically adjusted weights.\n- **Ensemble components**: $M$ models, each providing a predictive distribution at $T$ time points.\n- **Predictive distribution form**: Gaussian, with model $i$ at time $t$ providing mean $\\mu_{i,t}$ and variance $\\sigma_{i,t}^{2}$. ($ \\sigma_{i,t} > 0 $)\n- **Calibration data**: A set of observations $\\{y_t\\}_{t=1}^T$.\n- **Weighting principle**:\n    1.  Use a strictly proper scoring rule for continuous probabilistic forecasts to assess skill.\n    2.  Weights must be consistent with maximizing the expected utility under this scoring rule over the calibration period.\n    3.  The calibration observations are independent conditional on each model’s predictive distribution.\n    4.  The derivation must be from first principles.\n- **Weight constraints**: Final weights $w_i$ must be non-negative ($w_i \\ge 0$) and sum to unity ($\\sum_{i=1}^M w_i = 1$).\n- **Test Case 1**: $M=2$, $T=5$.\n    - $y = [0.9, 0.6, -0.2, 0.4, 0.7]$.\n    - $\\mu_1 = [0.85, 0.62, -0.19, 0.35, 0.75]$, $\\sigma_1 = [0.25, 0.25, 0.30, 0.25, 0.25]$.\n    - $\\mu_2 = [0.30, 0.10, -0.50, 0.00, 0.20]$, $\\sigma_2 = [0.40, 0.40, 0.50, 0.40, 0.40]$.\n- **Test Case 2**: $M=3, T=4$.\n    - $y = [2.0, 2.5, 1.5, 2.2]$.\n    - $\\mu_1 = [1.9, 2.6, 1.6, 2.1]$, $\\sigma_1 = [0.30, 0.30, 0.30, 0.30]$.\n    - $\\mu_2 = [2.1, 2.4, 1.4, 2.3]$, $\\sigma_2 = [0.30, 0.30, 0.30, 0.30]$.\n    - $\\mu_3 = [2.0, 2.5, 1.5, 2.2]$, $\\sigma_3 = [0.35, 0.35, 0.35, 0.35]$.\n- **Test Case 3**: $M=2, T=5$.\n    - $y = [-0.5, -0.4, -0.6, -0.55, -0.45]$.\n    - $\\mu_1 = [-0.45, -0.38, -0.62, -0.50, -0.47]$, $\\sigma_1 = [0.15, 0.15, 0.15, 0.15, 0.15]$.\n    - $\\mu_2 = [-0.10, -0.10, -0.10, -0.10, -0.10]$, $\\sigma_2 = [0.05, 0.05, 0.05, 0.05, 0.05]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n- **Scientifically Grounded**: The problem is rooted in the established statistical fields of probabilistic forecasting, ensemble methods, and scoring rules. The use of Gaussian distributions, proper scoring rules, and Bayesian model averaging (which, as will be shown, is the principled approach required) are standard and sound scientific practices. The problem is scientifically valid.\n- **Well-Posed**: The problem requests a weight-derivation scheme from \"first principles\" that is \"consistent with maximizing... utility\". This points strongly towards a specific, well-defined theoretical framework, namely Bayesian Model Averaging (BMA). Under this interpretation, a unique and meaningful solution exists. The inputs are sufficient for this purpose. The problem is well-posed.\n- **Objective**: All inputs are numerical, and the desired output is a set of numerical weights. The language is precise and free of subjectivity. The problem is objective.\n- **Completeness and Consistency**: The problem provides all necessary data (observations, means, standard deviations) for each test case. The constraints ($\\sigma_{i,t} > 0$, weights on the simplex) are clear and consistent. The problem is complete and consistent.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being unrealistic, ill-structured, trivial, or outside scientific verifiability.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe problem requires constructing ensemble weights $\\{w_i\\}_{i=1}^M$ for $M$ probabilistic models based on their performance over a calibration period of $T$ observations. The derivation must be from first principles using a strictly proper scoring rule.\n\n**1. Choice of Scoring Rule**\nA strictly proper scoring rule ensures that a forecaster is uniquely incentivized to state their true belief. For continuous probabilistic forecasts, such as the Gaussian distributions $N(\\mu, \\sigma^2)$ provided by the models, the most fundamental strictly proper scoring rule is the **Logarithmic Score** (LogS). The LogS is simply the logarithm of the probability density function (PDF) of the forecast, evaluated at the observed outcome. For a single forecast distribution $P$ and observation $y$, the score is $S(P, y) = \\log(p(y))$, where $p(y)$ is the PDF corresponding to $P$. Higher scores are better.\n\nFor a Gaussian forecast $p_{i,t} \\sim N(\\mu_{i,t}, \\sigma_{i,t}^2)$, the Logarithmic Score for an observation $y_t$ is:\n$$ S(p_{i,t}, y_t) = \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_{i,t}^2}} \\exp\\left(-\\frac{(y_t-\\mu_{i,t})^2}{2\\sigma_{i,t}^2}\\right) \\right) $$\n$$ S(p_{i,t}, y_t) = -\\frac{1}{2}\\log(2\\pi\\sigma_{i,t}^2) - \\frac{(y_t-\\mu_{i,t})^2}{2\\sigma_{i,t}^2} $$\n\n**2. Aggregation of Skill**\nThe problem states that observations $\\{y_t\\}$ are independent conditional on a given model's forecasts. This assumption allows us to aggregate the skill of each model over the calibration period by summing their individual Logarithmic Scores. The aggregate score for model $i$, denoted $S_i$, is:\n$$ S_i = \\sum_{t=1}^{T} S(p_{i,t}, y_t) = \\sum_{t=1}^{T} \\log(p_{i,t}(y_t)) $$\n\n**3. Derivation of Weights from First Principles**\nThe most principled method to derive weights from model performance is Bayesian Model Averaging (BMA). In this framework, the weights $w_i$ are interpreted as the posterior probability of model $M_i$ being the \"true\" data-generating process, given the observed calibration data $D = \\{y_t\\}_{t=1}^T$.\n\nAccording to Bayes' theorem, the posterior probability of model $M_i$ is:\n$$ w_i \\equiv P(M_i | D) = \\frac{P(D | M_i) P(M_i)}{P(D)} = \\frac{P(D | M_i) P(M_i)}{\\sum_{j=1}^{M} P(D | M_j) P(M_j)} $$\nHere, $P(M_i)$ is the prior probability of model $M_i$, and $P(D | M_i)$ is the marginal likelihood or \"evidence\" for model $M_i$.\n\n- **Prior Probability $P(M_i)$**: In the absence of prior information favoring any particular model, we assign a non-informative, uniform prior: $P(M_i) = 1/M$ for all $i = 1, \\dots, M$.\n\n- **Marginal Likelihood $P(D | M_i)$**: Due to the assumption of temporal independence of observations conditional on the model, the likelihood of the entire dataset for model $M_i$ is the product of the individual likelihoods (densities):\n$$ P(D | M_i) = \\prod_{t=1}^{T} p_{i,t}(y_t) $$\nTaking the logarithm of this expression reveals its direct connection to the aggregate Logarithmic Score $S_i$:\n$$ \\log(P(D | M_i)) = \\sum_{t=1}^{T} \\log(p_{i,t}(y_t)) = S_i $$\nTherefore, the marginal likelihood is simply the exponentiated aggregate score: $P(D | M_i) = \\exp(S_i)$.\n\nSubstituting these back into the formula for the weights gives:\n$$ w_i = \\frac{\\exp(S_i) \\cdot (1/M)}{\\sum_{j=1}^{M} \\exp(S_j) \\cdot (1/M)} = \\frac{\\exp(S_i)}{\\sum_{j=1}^{M} \\exp(S_j)} $$\nThis is the softmax function applied to the vector of aggregate log-scores. This derivation satisfies all problem requirements: it is based on first principles (Bayes' theorem), uses a strictly proper scoring rule (LogS), and produces weights $w_i$ that are non-negative and sum to $1$. The \"dynamic adjustment\" described in the problem is implicitly captured, as this batch formulation gives the same result as an online Bayesian update starting from uniform priors. This method is consistent with finding the optimal BMA predictive distribution, which maximizes expected utility in a Bayesian sense.\n\n**4. Algorithmic Implementation**\nThe algorithm proceeds as follows for each test case:\n1.  For each model $i \\in \\{1, \\dots, M\\}$, compute the aggregate log-score $S_i = \\sum_{t=1}^{T} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma_{i,t}^2) - \\frac{(y_t-\\mu_{i,t})^2}{2\\sigma_{i,t}^2} \\right)$.\n2.  The aggregate scores $S_i$ can be large negative numbers, so calculating $\\exp(S_i)$ directly can cause numerical underflow. To ensure stability, we use the log-sum-exp trick. Let $S_{\\max} = \\max_j \\{S_j\\}$. The weights are calculated as:\n    $$ w_i = \\frac{\\exp(S_i - S_{\\max})}{\\sum_{j=1}^{M} \\exp(S_j - S_{\\max})} $$\n    This computation is numerically stable and mathematically equivalent to the original formula.\n3.  The resulting vector of weights $\\{w_i\\}$ is the final answer for the test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ensemble weighting problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"y\": np.array([0.9, 0.6, -0.2, 0.4, 0.7]),\n            \"mus\": np.array([\n                [0.85, 0.62, -0.19, 0.35, 0.75],\n                [0.30, 0.10, -0.50, 0.00, 0.20]\n            ]),\n            \"sigmas\": np.array([\n                [0.25, 0.25, 0.30, 0.25, 0.25],\n                [0.40, 0.40, 0.50, 0.40, 0.40]\n            ])\n        },\n        {\n            \"y\": np.array([2.0, 2.5, 1.5, 2.2]),\n            \"mus\": np.array([\n                [1.9, 2.6, 1.6, 2.1],\n                [2.1, 2.4, 1.4, 2.3],\n                [2.0, 2.5, 1.5, 2.2]\n            ]),\n            \"sigmas\": np.array([\n                [0.30, 0.30, 0.30, 0.30],\n                [0.30, 0.30, 0.30, 0.30],\n                [0.35, 0.35, 0.35, 0.35]\n            ])\n        },\n        {\n            \"y\": np.array([-0.5, -0.4, -0.6, -0.55, -0.45]),\n            \"mus\": np.array([\n                [-0.45, -0.38, -0.62, -0.50, -0.47],\n                [-0.10, -0.10, -0.10, -0.10, -0.10]\n            ]),\n            \"sigmas\": np.array([\n                [0.15, 0.15, 0.15, 0.15, 0.15],\n                [0.05, 0.05, 0.05, 0.05, 0.05]\n            ])\n        }\n    ]\n\n    def calculate_weights(y, mus, sigmas):\n        \"\"\"\n        Calculates ensemble weights based on the Bayesian Model Averaging principle\n        using the Logarithmic Score.\n\n        Args:\n            y (np.ndarray): Array of observations, shape (T,).\n            mus (np.ndarray): Array of model means, shape (M, T).\n            sigmas (np.ndarray): Array of model standard deviations, shape (M, T).\n\n        Returns:\n            np.ndarray: Final ensemble weights, shape (M,).\n        \"\"\"\n        # Get number of models M and time points T.\n        M, T = mus.shape\n\n        # Reshape y for broadcasting with (M, T) arrays.\n        y_broadcast = y.reshape(1, T)\n\n        # Calculate variances from standard deviations.\n        variances = sigmas**2\n\n        # Calculate the log-likelihood (Logarithmic Score) for each model at each time point.\n        # This is a fully vectorized operation.\n        # log(p(y)) = -0.5 * log(2*pi*sigma^2) - (y - mu)^2 / (2*sigma^2)\n        log_likelihoods = -0.5 * np.log(2 * np.pi * variances) - \\\n                          (y_broadcast - mus)**2 / (2 * variances)\n\n        # Aggregate the scores for each model by summing over the time dimension (axis=1).\n        aggregate_log_scores = np.sum(log_likelihoods, axis=1)\n\n        # To compute weights via softmax, use the log-sum-exp trick for numerical stability.\n        # Subtract the max log score before exponentiating.\n        max_log_score = np.max(aggregate_log_scores)\n        \n        # Calculate unnormalized weights.\n        unnormalized_weights = np.exp(aggregate_log_scores - max_log_score)\n\n        # Normalize to get the final weights.\n        sum_of_weights = np.sum(unnormalized_weights)\n        weights = unnormalized_weights / sum_of_weights\n        \n        return weights\n\n    results = []\n    for case in test_cases:\n        weights = calculate_weights(case[\"y\"], case[\"mus\"], case[\"sigmas\"])\n        results.append(weights)\n\n    # Format the output string precisely as required, with no spaces inside inner lists.\n    results_str_list = []\n    for res in results:\n        # Convert numpy array elements to string and join with commas.\n        weights_str = \",\".join(map(str, res))\n        # Enclose in brackets to form a list-like string.\n        results_str_list.append(f\"[{weights_str}]\")\n    \n    # Join all test case results with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}