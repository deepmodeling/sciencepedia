{
    "hands_on_practices": [
        {
            "introduction": "To effectively evaluate any classification model, one must first master the fundamental language of accuracy assessment: the error matrix. This exercise provides the essential practice for computing the three cornerstone metrics: Overall Accuracy ($OA$), Producer's Accuracy ($PA$), and User's Accuracy ($UA$). By working through this binary classification example, you will build the foundational skills needed to translate raw confusion counts into meaningful measures of map quality and reliability .",
            "id": "3793901",
            "problem": "A binary land-cover classification derived from satellite imagery has been assessed using a sample-based error matrix whose rows correspond to mapped classes and whose columns correspond to reference (ground truth) classes. Let $n_{ij}$ denote the count of samples whose mapped class is $i$ and reference class is $j$, with $i \\in \\{1,2\\}$ and $j \\in \\{1,2\\}$. The observed counts are $n_{11}=50$, $n_{12}=8$, $n_{21}=10$, and $n_{22}=32$. Assume a simple random sample design so that cell proportions estimated by $n_{ij}$ normalized by the appropriate totals are unbiased for the corresponding probabilities.\n\nUsing only the foundational definitions below, compute the Producer’s Accuracy for class $1$ and class $2$ (Producer’s Accuracy (PA) is the proportion of reference samples of a class that are correctly labeled by the map), the User’s Accuracy for class $1$ and class $2$ (User’s Accuracy (UA) is the proportion of mapped samples of a class that truly belong to that class in reference), and the Overall Accuracy (OA) (Overall Accuracy is the proportion of all samples that are correctly classified). Express each accuracy as a decimal in the interval $[0,1]$, rounded to $4$ significant figures.\n\nReport your final answer as the ordered tuple $\\big(PA_{1},\\,PA_{2},\\,UA_{1},\\,UA_{2},\\,OA\\big)$.",
            "solution": "The problem statement has been validated and is deemed valid. It is a scientifically grounded, well-posed, and objective problem in remote sensing accuracy assessment. All necessary data and definitions are provided, and there are no internal contradictions or ambiguities.\n\nThe problem requires the calculation of several accuracy metrics from a sample-based error matrix for a binary classification. The provided counts, $n_{ij}$, represent the number of samples with mapped class $i$ and reference class $j$. The problem specifies the convention where rows are mapped classes and columns are reference classes. The provided data are:\n- $n_{11} = 50$ (correctly classified as class $1$)\n- $n_{12} = 8$ (reference class $2$, mapped as class $1$; error of commission for class $1$)\n- $n_{21} = 10$ (reference class $1$, mapped as class $2$; error of omission for class $1$)\n- $n_{22} = 32$ (correctly classified as class $2$)\n\nThe error matrix $N$ is structured with mapped classes as rows and reference classes as columns:\n$$\nN = \\begin{pmatrix} n_{11} & n_{12} \\\\ n_{21} & n_{22} \\end{pmatrix} = \\begin{pmatrix} 50 & 8 \\\\ 10 & 32 \\end{pmatrix}\n$$\n\nTo calculate the required accuracy metrics, we first compute the marginal totals.\nThe row totals, representing the total number of mapped samples for each class, are denoted by $n_{i.}$.\n$$\nn_{1.} = n_{11} + n_{12} = 50 + 8 = 58\n$$\n$$\nn_{2.} = n_{21} + n_{22} = 10 + 32 = 42\n$$\n\nThe column totals, representing the total number of reference samples for each class, are denoted by $n_{.j}$.\n$$\nn_{.1} = n_{11} + n_{21} = 50 + 10 = 60\n$$\n$$\nn_{.2} = n_{12} + n_{22} = 8 + 32 = 40\n$$\n\nThe grand total number of samples, $N_{total}$, is the sum of all elements in the matrix:\n$$\nN_{total} = n_{11} + n_{12} + n_{21} + n_{22} = 50 + 8 + 10 + 32 = 100\n$$\n\nNow we apply the definitions provided in the problem statement.\n\n**User’s Accuracy (UA)**\nUser’s Accuracy for a class $i$ ($UA_{i}$) is defined as the proportion of mapped samples of that class that truly belong to that class in reference. This corresponds to the number of correctly classified samples in a class ($n_{ii}$) divided by the total number of samples mapped to that class (the row total $n_{i.}$).\nThe formula is $UA_{i} = \\frac{n_{ii}}{n_{i.}}$.\n\nFor class $1$:\n$$\nUA_{1} = \\frac{n_{11}}{n_{1.}} = \\frac{50}{58} = \\frac{25}{29} \\approx 0.862068...\n$$\nRounded to $4$ significant figures, $UA_{1} = 0.8621$.\n\nFor class $2$:\n$$\nUA_{2} = \\frac{n_{22}}{n_{2.}} = \\frac{32}{42} = \\frac{16}{21} \\approx 0.761904...\n$$\nRounded to $4$ significant figures, $UA_{2} = 0.7619$.\n\n**Producer’s Accuracy (PA)**\nProducer’s Accuracy for a class $j$ ($PA_{j}$) is defined as the proportion of reference samples of that class that are correctly labeled by the map. This corresponds to the number of correctly classified samples in a class ($n_{jj}$) divided by the total number of reference samples for that class (the column total $n_{.j}$).\nThe formula is $PA_{j} = \\frac{n_{jj}}{n_{.j}}$.\n\nFor class $1$:\n$$\nPA_{1} = \\frac{n_{11}}{n_{.1}} = \\frac{50}{60} = \\frac{5}{6} \\approx 0.83333...\n$$\nRounded to $4$ significant figures, $PA_{1} = 0.8333$.\n\nFor class $2$:\n$$\nPA_{2} = \\frac{n_{22}}{n_{.2}} = \\frac{32}{40} = \\frac{4}{5} = 0.8\n$$\nExpressed with $4$ significant figures, $PA_{2} = 0.8000$.\n\n**Overall Accuracy (OA)**\nOverall Accuracy is defined as the proportion of all samples that are correctly classified. This is the sum of the diagonal elements (correctly classified samples) divided by the grand total number of samples.\nThe formula is $OA = \\frac{\\sum_{k=1}^{2} n_{kk}}{N_{total}}$.\n$$\nOA = \\frac{n_{11} + n_{22}}{N_{total}} = \\frac{50 + 32}{100} = \\frac{82}{100} = 0.82\n$$\nExpressed with $4$ significant figures, $OA = 0.8200$.\n\nThe final results are compiled into the required ordered tuple $\\big(PA_{1},\\,PA_{2},\\,UA_{1},\\,UA_{2},\\,OA\\big)$.\n- $PA_{1} = 0.8333$\n- $PA_{2} = 0.8000$\n- $UA_{1} = 0.8621$\n- $UA_{2} = 0.7619$\n- $OA = 0.8200$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.8333 & 0.8000 & 0.8621 & 0.7619 & 0.8200\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A high Overall Accuracy can often be deceptive, particularly in real-world scenarios with imbalanced class distributions. This practice moves beyond simple calculation to critical interpretation, presenting a case where a classifier achieves a very high $OA$ while completely failing to identify a rare but critical class. Engaging with this problem will underscore the epistemic limitations of relying on a single aggregate metric and highlight the necessity of examining per-class accuracies to gain a true understanding of model performance .",
            "id": "3794289",
            "problem": "A remote sensing team evaluates a supervised land-cover classifier for a multispectral satellite scene with four classes: Forest, Cropland, Urban, and Wetland. The evaluation uses an independent set of reference samples, building a confusion matrix where rows represent predicted class labels and columns represent reference class instances. The total number of evaluated reference samples is $10000$. The following per-row counts are given, where each bullet lists the reference class distribution for a single predicted class:\n\n- Predicted Forest (5960 total): 5840 were reference Forest, 80 were reference Cropland, 10 were reference Urban, and 30 were reference Wetland.\n- Predicted Cropland (2995 total): 120 were reference Forest, 2850 were reference Cropland, 5 were reference Urban, and 20 were reference Wetland.\n- Predicted Urban (1010 total): 20 were reference Forest, 60 were reference Cropland, 930 were reference Urban, and 0 were reference Wetland.\n- Predicted Wetland (35 total): 20 were reference Forest, 10 were reference Cropland, 5 were reference Urban, and 0 were reference Wetland.\n\nFrom first principles of accuracy assessment in remote sensing, derive the Overall Accuracy (OA), the Producer’s Accuracy (PA) per class, and the User’s Accuracy (UA) per class for this multi-class confusion matrix. Then, select the single option that most accurately characterizes both the computed values and the epistemic limitation of OA in the presence of severe class imbalance.\n\nA. OA $= 0.962$; Wetland PA $= 0$; Wetland UA $= 0$; the high OA masks catastrophic failure for the low-prevalence Wetland class, so OA is epistemically insufficient for judging reliability on rare classes in imbalanced settings.\n\nB. OA $= 0.962$; Wetland PA $= 0.4$; Wetland UA $= 0$; the high OA guarantees acceptable per-class performance, hence OA is epistemically sufficient even with imbalance.\n\nC. OA $= 0.962$; Wetland PA $= 0$; Wetland UA $= 1$; OA remains high because the classifier predicts Wetland only when certain, making OA epistemically reliable for rare classes.\n\nD. OA $= 0.60$; Wetland PA $= 0$; Wetland UA $= 0$; OA penalizes rare-class errors heavily, so OA is epistemically sensitive to imbalance.\n\nE. OA $= 0.962$; Wetland PA $= 0$; Wetland UA $= 0$; OA equals the weighted average of Producer’s Accuracy across classes and therefore a catastrophic PA inevitably forces OA to be low, implying OA is epistemically aligned with rare-class risk.",
            "solution": "The problem statement is a valid exercise in the accuracy assessment of a land-cover classification, a standard procedure in remote sensing and geographic information science. All necessary data are provided, the quantities are consistent, and the question is well-posed. We may proceed with the derivation.\n\nFirst, we construct the confusion matrix, denoted as $M$. The problem specifies the convention where rows represent the predicted classification labels and columns represent the reference data (ground truth). The classes are Forest ($F$), Cropland ($C$), Urban ($U$), and Wetland ($W$). The element $M_{ij}$ is the count of samples predicted as class $i$ that belong to reference class $j$.\n\nFrom the provided data:\n- Predicted Forest (row 1): $M_{FF}=5840$, $M_{FC}=80$, $M_{FU}=10$, $M_{FW}=30$. Row sum: $5840+80+10+30 = 5960$.\n- Predicted Cropland (row 2): $M_{CF}=120$, $M_{CC}=2850$, $M_{CU}=5$, $M_{CW}=20$. Row sum: $120+2850+5+20 = 2995$.\n- Predicted Urban (row 3): $M_{UF}=20$, $M_{UC}=60$, $M_{UU}=930$, $M_{UW}=0$. Row sum: $20+60+930+0 = 1010$.\n- Predicted Wetland (row 4): $M_{WF}=20$, $M_{WC}=10$, $M_{WU}=5$, $M_{WW}=0$. Row sum: $20+10+5+0 = 35$.\n\nThe confusion matrix $M$ is:\n$$\nM = \\begin{pmatrix}\n & \\text{Ref } F & \\text{Ref } C & \\text{Ref } U & \\text{Ref } W \\\\\n\\text{Pred } F & 5840 & 80 & 10 & 30 \\\\\n\\text{Pred } C & 120 & 2850 & 5 & 20 \\\\\n\\text{Pred } U & 20 & 60 & 930 & 0 \\\\\n\\text{Pred } W & 20 & 10 & 5 & 0\n\\end{pmatrix}\n$$\n\nTo calculate Producer's Accuracy, we need the column sums (total number of reference samples for each class):\n- Column sum $C_F = 5840 + 120 + 20 + 20 = 6000$\n- Column sum $C_C = 80 + 2850 + 60 + 10 = 3000$\n- Column sum $C_U = 10 + 5 + 930 + 5 = 950$\n- Column sum $C_W = 30 + 20 + 0 + 0 = 50$\nThe total number of samples is $N = 6000 + 3000 + 950 + 50 = 10000$, which is consistent with the problem statement.\n\nNow we derive the accuracy metrics from first principles.\n\n**Overall Accuracy (OA)**\nThe Overall Accuracy is the proportion of all reference samples that were correctly classified. It is the sum of the diagonal elements of the confusion matrix divided by the total number of samples $N$.\n$$\nOA = \\frac{\\sum_{i=1}^{k} M_{ii}}{N}\n$$\nFor this problem, with $k=4$ classes:\n$$\nOA = \\frac{M_{FF} + M_{CC} + M_{UU} + M_{WW}}{N} = \\frac{5840 + 2850 + 930 + 0}{10000} = \\frac{9620}{10000} = 0.962\n$$\n\n**User's Accuracy (UA)**\nUser's Accuracy for a predicted class $i$, also known as precision, measures how often a prediction of class $i$ is actually correct. It is the number of correctly classified samples of class $i$ ($M_{ii}$) divided by the total number of samples predicted as class $i$ (the row sum $R_i$). It quantifies errors of commission.\n$$\nUA_i = \\frac{M_{ii}}{R_i}\n$$\nWe are particularly interested in the Wetland class ($i=W$):\n- Total predicted as Wetland, $R_W = 35$.\n- Correctly classified as Wetland, $M_{WW} = 0$.\n$$\nUA_W = \\frac{M_{WW}}{R_W} = \\frac{0}{35} = 0\n$$\n\n**Producer's Accuracy (PA)**\nProducer's Accuracy for a reference class $j$, also known as recall, measures how well reference samples of a given class are correctly classified. It is the number of correctly classified samples of class $j$ ($M_{jj}$) divided by the total number of reference samples of class $j$ (the column sum $C_j$). It quantifies errors of omission.\n$$\nPA_j = \\frac{M_{jj}}{C_j}\n$$\nFor the Wetland class ($j=W$):\n- Reference total for Wetland, $C_W = 50$.\n- Correctly classified as Wetland, $M_{WW} = 0$.\n$$\nPA_W = \\frac{M_{WW}}{C_W} = \\frac{0}{50} = 0\n$$\n\nSummary of calculated values:\n- Overall Accuracy ($OA$) = $0.962$\n- Wetland Producer's Accuracy ($PA_W$) = $0$\n- Wetland User's Accuracy ($UA_W$) = $0$\n\nNow we evaluate each option.\n\n**A. OA $= 0.962$; Wetland PA $= 0$; Wetland UA $= 0$; the high OA masks catastrophic failure for the low-prevalence Wetland class, so OA is epistemically insufficient for judging reliability on rare classes in imbalanced settings.**\n- The numerical values are correct: $OA = 0.962$, $PA_W = 0$, $UA_W = 0$.\n- The epistemic claim is that the high OA masks the poor performance on the rare Wetland class. The Wetland class accounts for only $50/10000 = 0.5\\%$ of the samples. The classifier failed completely on this class ($PA_W=0$ and $UA_W=0$). However, the OA is extremely high ($96.2\\%$). This is because OA is a weighted average of per-class accuracies, where the weights are the class proportions. The dominant classes, Forest ($60\\%$) and Cropland ($30\\%$), are classified with high accuracy, inflating the OA. Therefore, OA is not a reliable indicator of performance on individual, especially rare, classes. This statement is a correct and fundamental critique of OA in the context of class imbalance.\n- Verdict: **Correct**.\n\n**B. OA $= 0.962$; Wetland PA $= 0.4$; Wetland UA $= 0$; the high OA guarantees acceptable per-class performance, hence OA is epistemically sufficient even with imbalance.**\n- The value for Wetland PA is given as $0.4$, which contradicts our calculation of $PA_W=0$.\n- The epistemic claim that \"high OA guarantees acceptable per-class performance\" is demonstrably false. Our results show a high OA of $0.962$ coexisting with a complete failure on the Wetland class ($PA_W=0, UA_W=0$).\n- Verdict: **Incorrect**.\n\n**C. OA $= 0.962$; Wetland PA $= 0$; Wetland UA $= 1$; OA remains high because the classifier predicts Wetland only when certain, making OA epistemically reliable for rare classes.**\n- The value for Wetland UA is given as $1$, which contradicts our calculation of $UA_W=0$. A UA of $1$ would imply all $35$ samples predicted as Wetland were indeed Wetland. The data shows none of them were.\n- The epistemic claim is based on this false UA value and is consequently flawed. The classifier is never correct when it predicts \"Wetland\".\n- Verdict: **Incorrect**.\n\n**D. OA $= 0.60$; Wetland PA $= 0$; Wetland UA $= 0$; OA penalizes rare-class errors heavily, so OA is epistemically sensitive to imbalance.**\n- The value for OA is given as $0.60$, which contradicts our calculation of $OA = 0.962$.\n- The epistemic claim that \"OA penalizes rare-class errors heavily\" is false. OA is a prevalence-weighted average, so it is insensitive to errors on rare classes, as this very problem demonstrates. The contribution of the Wetland class to OA is minimal.\n- Verdict: **Incorrect**.\n\n**E. OA $= 0.962$; Wetland PA $= 0$; Wetland UA $= 0$; OA equals the weighted average of Producer’s Accuracy across classes and therefore a catastrophic PA inevitably forces OA to be low, implying OA is epistemically aligned with rare-class risk.**\n- The numerical values are correct.\n- The statement that \"OA equals the weighted average of Producer’s Accuracy across classes\" is true, with weights being the class prevalences: $OA = \\sum_j (C_j/N) \\cdot PA_j$.\n- However, the inference that \"a catastrophic PA inevitably forces OA to be low\" is false. If the class with catastrophic PA is rare (has a small weight $C_j/N$), its impact on the weighted sum is negligible. In our case, the Wetland class has a weight of $50/10000 = 0.005$. Its contribution to the OA is $0.005 \\times PA_W = 0.005 \\times 0 = 0$. This does not force OA to be low. The claim that OA is \"epistemically aligned with rare-class risk\" is the opposite of the truth.\n- Verdict: **Incorrect**.\n\nBased on the analysis, option A is the only one that presents both the correct numerical results and a sound epistemic interpretation of their meaning.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Accuracy assessment in remote sensing rarely relies on simple random sampling; instead, more efficient stratified sampling designs are common. This advanced problem bridges the gap between textbook theory and professional practice by demonstrating how to properly calculate accuracy metrics when the reference data is collected via a stratified design. Working through this example will teach you how to incorporate stratum weights ($W_i$) to derive unbiased estimates of producer's accuracy, a crucial skill for ensuring the statistical validity of your assessment results in large-area mapping projects .",
            "id": "3793883",
            "problem": "A remote sensing land-cover map with $3$ mapped classes is validated using a stratified random sample allocated by mapped class. Let the mapped classes be indexed by $i \\in \\{1,2,3\\}$ and the reference (field-verified) classes be indexed by $j \\in \\{1,2,3\\}$. The sampling strata are the mapped classes, with known mapped area proportions $W_{1}=0.60$, $W_{2}=0.30$, and $W_{3}=0.10$, which sum to $1$. Within each mapped class $i$, a simple random sample of $n_{i+}$ pixels is taken and labeled against reference data to produce the $3 \\times 3$ sample error matrix $N=[n_{ij}]$ with rows indexed by mapped class and columns indexed by reference class:\n$$\nN \\;=\\;\n\\begin{pmatrix}\n40 & 8 & 2 \\\\\n9 & 18 & 3 \\\\\n4 & 4 & 12\n\\end{pmatrix},\n\\quad\n(n_{1+},n_{2+},n_{3+})=(50,30,20).\n$$\nUsing only foundational definitions of conditional probability and design-based unbiased estimation under stratified sampling by mapped class, do the following:\n- Compute the per-class user’s accuracy (UA), $UA_{i}$, interpreted as $P(\\text{Reference}=i \\mid \\text{Mapped}=i)$, for $i \\in \\{1,2,3\\}$.\n- Compute the per-class producer’s accuracy (PA), $PA_{i}$, interpreted as $P(\\text{Mapped}=i \\mid \\text{Reference}=i)$, for $i \\in \\{1,2,3\\}$, taking into account the unequal mapped area proportions $W_{i}$ implied by the stratified design.\n- For each class $i$, define the commission error as $C_{i}=1-UA_{i}$ and the omission error as $O_{i}=1-PA_{i}$. Let $\\Delta_{i}=C_{i}-O_{i}=PA_{i}-UA_{i}$. A class is said to have misclassification dominated by commission errors if $\\Delta_{i}>0$ and by omission errors if $\\Delta_{i}<0$.\n\nReport a single scalar: the index $k \\in \\{1,2,3\\}$ of the class with the largest positive value of $\\Delta_{i}$; if no $\\Delta_{i}$ is positive, report $0$. The final answer must be this single index.",
            "solution": "The problem is determined to be valid as it represents a standard, well-posed problem in remote sensing accuracy assessment with a complete and consistent set of givens. We proceed with the solution.\n\nThe analysis hinges on design-based statistical estimation, where the sampling design dictates the form of the estimators. The sampling is stratified by mapped class, with known stratum area proportions $W_i$.\n\nFirst, we compute the estimator for the per-class user’s accuracy, $\\widehat{UA}_i$. User's accuracy for class $i$, denoted $UA_i$, is the conditional probability $P(\\text{Reference}=i \\mid \\text{Mapped}=i)$. Because the sampling is performed independently within each mapped class stratum (the rows of the error matrix), the estimator $\\widehat{UA}_i$ is the proportion of correctly classified pixels within the sample from stratum $i$:\n$$ \\widehat{UA}_i = \\frac{n_{ii}}{n_{i+}} $$\nThe given sample error matrix is $N=[n_{ij}] = \\begin{pmatrix} 40 & 8 & 2 \\\\ 9 & 18 & 3 \\\\ 4 & 4 & 12 \\end{pmatrix}$ with row totals $(n_{1+},n_{2+},n_{3+})=(50,30,20)$.\nFor each class $i \\in \\{1, 2, 3\\}$, we have:\n$$ \\widehat{UA}_1 = \\frac{n_{11}}{n_{1+}} = \\frac{40}{50} = 0.80 $$\n$$ \\widehat{UA}_2 = \\frac{n_{22}}{n_{2+}} = \\frac{18}{30} = 0.60 $$\n$$ \\widehat{UA}_3 = \\frac{n_{33}}{n_{3+}} = \\frac{12}{20} = 0.60 $$\n\nNext, we compute the estimator for the per-class producer’s accuracy, $\\widehat{PA}_j$. Producer's accuracy for class $j$, denoted $PA_j$, is the conditional probability $P(\\text{Mapped}=j \\mid \\text{Reference}=j)$. To estimate this from a sample stratified by mapped class, we must first estimate the true area proportions for each cell $(i,j)$ of the population error matrix. The unbiased estimator for the proportion of the total area belonging to mapped class $i$ and reference class $j$, denoted $\\widehat{A}_{ij}$, is given by weighting the sample proportion from stratum $i$ by that stratum's area proportion $W_i$:\n$$ \\widehat{A}_{ij} = W_i \\frac{n_{ij}}{n_{i+}} $$\nThe estimator for the total proportion of area belonging to reference class $j$, denoted $\\widehat{A}_{+j}$, is the sum of these estimates over all mapped classes (rows):\n$$ \\widehat{A}_{+j} = \\sum_{k=1}^3 \\widehat{A}_{kj} = \\sum_{k=1}^3 W_k \\frac{n_{kj}}{n_{k+}} $$\nThe estimator for producer's accuracy for class $j$ is then the ratio of the estimated area correctly classified as class $j$ to the estimated total area of reference class $j$:\n$$ \\widehat{PA}_j = \\frac{\\widehat{A}_{jj}}{\\widehat{A}_{+j}} = \\frac{W_j \\frac{n_{jj}}{n_{j+}}}{\\sum_{k=1}^3 W_k \\frac{n_{kj}}{n_{k+}}} $$\nThe stratum area proportions are given as $W_1 = 0.60$, $W_2 = 0.30$, and $W_3 = 0.10$.\nWe compute the estimated reference class proportions, $\\widehat{A}_{+j}$:\n$$ \\widehat{A}_{+1} = W_1 \\frac{n_{11}}{n_{1+}} + W_2 \\frac{n_{21}}{n_{2+}} + W_3 \\frac{n_{31}}{n_{3+}} = (0.60)\\frac{40}{50} + (0.30)\\frac{9}{30} + (0.10)\\frac{4}{20} = 0.48 + 0.09 + 0.02 = 0.59 $$\n$$ \\widehat{A}_{+2} = W_1 \\frac{n_{12}}{n_{1+}} + W_2 \\frac{n_{22}}{n_{2+}} + W_3 \\frac{n_{32}}{n_{3+}} = (0.60)\\frac{8}{50} + (0.30)\\frac{18}{30} + (0.10)\\frac{4}{20} = 0.096 + 0.18 + 0.02 = 0.296 $$\n$$ \\widehat{A}_{+3} = W_1 \\frac{n_{13}}{n_{1+}} + W_2 \\frac{n_{23}}{n_{2+}} + W_3 \\frac{n_{33}}{n_{3+}} = (0.60)\\frac{2}{50} + (0.30)\\frac{3}{30} + (0.10)\\frac{12}{20} = 0.024 + 0.03 + 0.06 = 0.114 $$\nNow we compute the producer's accuracy estimates:\n$$ \\widehat{PA}_1 = \\frac{W_1 (n_{11}/n_{1+})}{\\widehat{A}_{+1}} = \\frac{0.60 (40/50)}{0.59} = \\frac{0.48}{0.59} = \\frac{48}{59} $$\n$$ \\widehat{PA}_2 = \\frac{W_2 (n_{22}/n_{2+})}{\\widehat{A}_{+2}} = \\frac{0.30 (18/30)}{0.296} = \\frac{0.18}{0.296} = \\frac{180}{296} = \\frac{45}{74} $$\n$$ \\widehat{PA}_3 = \\frac{W_3 (n_{33}/n_{3+})}{\\widehat{A}_{+3}} = \\frac{0.10 (12/20)}{0.114} = \\frac{0.06}{0.114} = \\frac{60}{114} = \\frac{10}{19} $$\nFinally, we compute the quantity $\\Delta_i = \\widehat{PA}_i - \\widehat{UA}_i$ for each class. Note the problem asks to compare these values for each class index $i$.\nFor class $1$:\n$$ \\Delta_1 = \\widehat{PA}_1 - \\widehat{UA}_1 = \\frac{48}{59} - 0.80 = \\frac{48}{59} - \\frac{4}{5} = \\frac{48 \\times 5 - 4 \\times 59}{295} = \\frac{240 - 236}{295} = \\frac{4}{295} $$\nSince $\\Delta_1 > 0$, this class's misclassification is dominated by commission errors.\nFor class $2$:\n$$ \\Delta_2 = \\widehat{PA}_2 - \\widehat{UA}_2 = \\frac{45}{74} - 0.60 = \\frac{45}{74} - \\frac{3}{5} = \\frac{45 \\times 5 - 3 \\times 74}{370} = \\frac{225 - 222}{370} = \\frac{3}{370} $$\nSince $\\Delta_2 > 0$, this class's misclassification is also dominated by commission errors.\nFor class $3$:\n$$ \\Delta_3 = \\widehat{PA}_3 - \\widehat{UA}_3 = \\frac{10}{19} - 0.60 = \\frac{10}{19} - \\frac{3}{5} = \\frac{10 \\times 5 - 3 \\times 19}{95} = \\frac{50 - 57}{95} = -\\frac{7}{95} $$\nSince $\\Delta_3 < 0$, this class's misclassification is dominated by omission errors.\n\nThe problem requires identifying the class index $k$ corresponding to the largest positive value of $\\Delta_i$. We must compare the positive values $\\Delta_1 = \\frac{4}{295}$ and $\\Delta_2 = \\frac{3}{370}$. We can compare the two fractions by cross-multiplication:\n$$ 4 \\times 370 = 1480 $$\n$$ 3 \\times 295 = 885 $$\nSince $1480 > 885$, it follows that $\\frac{4}{295} > \\frac{3}{370}$, and thus $\\Delta_1 > \\Delta_2$.\nThe largest positive value is $\\Delta_1$. The index of this class is $k=1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}