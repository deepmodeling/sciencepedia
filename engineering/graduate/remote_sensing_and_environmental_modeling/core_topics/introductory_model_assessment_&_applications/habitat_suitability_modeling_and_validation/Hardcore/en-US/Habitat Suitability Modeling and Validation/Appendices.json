{
    "hands_on_practices": [
        {
            "introduction": "This first practice introduces the fundamental concept of a bioclimatic envelope model, one of the simplest and most transparent forms of habitat suitability modeling. By translating a species' known physiological tolerances into a set of logical rules, you will construct a suitability index from gridded environmental data. This exercise builds foundational skills in spatial data manipulation and the application of boolean logic to define ecological niches .",
            "id": "3818680",
            "problem": "You are given gridded environmental data for a landscape, represented as arrays of equal shape for air temperature, annual precipitation, and a biotic exclusion factor. The objective is to compute a cell-wise habitat suitability index, classify pixels according to a threshold, and summarize the fraction of the landscape meeting the threshold. The suitability index must be constructed strictly from the following base principles.\n\nFundamental base and definitions:\n- Let a pixel location be denoted by the symbol $x$. Each pixel holds three values: air temperature $T(x)$, annual precipitation $P(x)$, and a biotic exclusion factor $b(x)$.\n- The species is climate-limited to closed intervals $[10,30]$ degrees Celsius and $[500,1500]$ millimeters. All temperatures must be treated and reported in degrees Celsius, and all precipitations must be treated and reported in millimeters.\n- Use the indicator function definition. For any statement $A$, define the indicator $\\mathbb{I}(A)$ by $\\mathbb{I}(A)=1$ if $A$ is true and $\\mathbb{I}(A)=0$ otherwise.\n- Biotic exclusion is represented by $b(x)$ as a fractional unavailability in the unit interval $[0,1]$, where $b(x)=0$ means no biotic exclusion and $b(x)=1$ means complete exclusion. If any $b(x)$ values lie outside $[0,1]$, clip them into $[0,1]$ before further computation.\n- Invalid or missing pixels are represented by the special value $\\mathrm{NaN}$ (Not-a-Number). A pixel $x$ is considered valid if and only if $T(x)$, $P(x)$, and $b(x)$ are all finite real numbers (i.e., none is $\\mathrm{NaN}$). Pixels that are not valid must be excluded from both the numerator and denominator of any fraction.\n\nTask:\n1. For each pixel $x$, define a climate feasibility indicator via the intersection of interval constraints:\n   - The temperature feasibility indicator is $\\mathbb{I}\\left(10 \\leq T(x) \\leq 30\\right)$.\n   - The precipitation feasibility indicator is $\\mathbb{I}\\left(500 \\leq P(x) \\leq 1500\\right)$.\n   - The climate feasibility indicator is the product $\\mathbb{I}\\left(10 \\leq T(x) \\leq 30\\right)\\cdot \\mathbb{I}\\left(500 \\leq P(x) \\leq 1500\\right)$.\n2. For each valid pixel $x$, define the suitability index $s(x)$ by the combination of climate feasibility and the complement of the biotic exclusion:\n   - If the climate feasibility indicator equals $0$, then $s(x)=0$.\n   - If the climate feasibility indicator equals $1$, then $s(x)=1-b(x)$, where $b(x)$ has been clipped into $[0,1]$.\n3. Using a suitability threshold $\\tau_s=0.8$ (dimensionless), count how many valid pixels satisfy $s(x)\\geq \\tau_s$. Compute the fraction of the landscape satisfying the threshold as\n   $$f=\\frac{\\text{number of valid pixels with } s(x)\\geq \\tau_s}{\\text{number of valid pixels}}.$$\n   If there are no valid pixels, define $f=0.0$ by convention.\n4. For each provided test case, compute $f$ and report the result as a floating-point number rounded to six decimal places.\n\nAngle units are not involved in this problem. All percentages must be reported as decimals.\n\nTest suite:\nUse the following four cases. In each case, all arrays have the same dimensions and refer to the same pixel grid. Use $\\mathrm{NaN}$ for missing values exactly where shown.\n\n- Test Case 1 (general mixed case):\n  - Temperature $T^{(1)}$ (degrees Celsius):\n    $$\\begin{bmatrix}\n    12 & 25 & 5 & 31 \\\\\n    10 & 30 & 20 & \\mathrm{NaN} \\\\\n    15 & 9 & 28 & 11\n    \\end{bmatrix}$$\n  - Precipitation $P^{(1)}$ (millimeters):\n    $$\\begin{bmatrix}\n    600 & 1400 & 600 & 600 \\\\\n    500 & 1500 & 1000 & 900 \\\\\n    499 & 800 & 1600 & \\mathrm{NaN}\n    \\end{bmatrix}$$\n  - Biotic exclusion $b^{(1)}$ (fraction, to be clipped into $[0,1]$ before use):\n    $$\\begin{bmatrix}\n    0.1 & 0.05 & 0.3 & 0.1 \\\\\n    0.2 & 0.0 & 0.19 & 0.9 \\\\\n    0.0 & 0.15 & 0.2 & 0.1\n    \\end{bmatrix}$$\n\n- Test Case 2 (boundary conditions and clipping):\n  - Temperature $T^{(2)}$ (degrees Celsius):\n    $$\\begin{bmatrix}\n    10 & 30 & 20 & 10 \\\\\n    30 & 10 & 30 & 25\n    \\end{bmatrix}$$\n  - Precipitation $P^{(2)}$ (millimeters):\n    $$\\begin{bmatrix}\n    500 & 1500 & 1000 & 500 \\\\\n    1500 & 500 & 1500 & 700\n    \\end{bmatrix}$$\n  - Biotic exclusion $b^{(2)}$ (fraction, clip to $[0,1]$):\n    $$\\begin{bmatrix}\n    0.2 & 0.2 & 0.8 & 0.0 \\\\\n    0.2000001 & -0.1 & 1.2 & 0.2\n    \\end{bmatrix}$$\n\n- Test Case 3 (all climate-infeasible):\n  - Temperature $T^{(3)}$ (degrees Celsius):\n    $$\\begin{bmatrix}\n    9 & 35 \\\\\n    8 & 31\n    \\end{bmatrix}$$\n  - Precipitation $P^{(3)}$ (millimeters):\n    $$\\begin{bmatrix}\n    400 & 1600 \\\\\n    200 & 2000\n    \\end{bmatrix}$$\n  - Biotic exclusion $b^{(3)}$ (fraction):\n    $$\\begin{bmatrix}\n    0.0 & 0.0 \\\\\n    0.0 & 0.0\n    \\end{bmatrix}$$\n\n- Test Case 4 (no valid pixels):\n  - Temperature $T^{(4)}$ (degrees Celsius):\n    $$\\begin{bmatrix}\n    \\mathrm{NaN} & \\mathrm{NaN} \\\\\n    \\mathrm{NaN} & \\mathrm{NaN}\n    \\end{bmatrix}$$\n  - Precipitation $P^{(4)}$ (millimeters):\n    $$\\begin{bmatrix}\n    \\mathrm{NaN} & \\mathrm{NaN} \\\\\n    \\mathrm{NaN} & \\mathrm{NaN}\n    \\end{bmatrix}$$\n  - Biotic exclusion $b^{(4)}$ (fraction):\n    $$\\begin{bmatrix}\n    \\mathrm{NaN} & \\mathrm{NaN} \\\\\n    \\mathrm{NaN} & \\mathrm{NaN}\n    \\end{bmatrix}$$\n\nRequired final output format:\nYour program must compute $f$ for each of the four test cases in order and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each floating-point result rounded to six decimal places. For example, an output with four results must look like\n$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$.",
            "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent set of definitions and data for a standard habitat suitability modeling exercise.\n\nThe solution proceeds by implementing the specified algorithm step-by-step for an arbitrary set of gridded data layers. Let the input arrays for temperature, precipitation, and biotic exclusion be denoted by $T$, $P$, and $b$ respectively.\n\n1.  **Identification of Valid Pixels**: The first step is to establish which pixels in the grid contain valid data. According to the problem definition, a pixel at location $x$ is valid if and only if the values for temperature $T(x)$, precipitation $P(x)$, and the biotic factor $b(x)$ are all finite real numbers (i.e., not $\\mathrm{NaN}$). This can be implemented by creating a boolean mask, let's call it $M_{valid}$, where an element is `True` if the corresponding pixel is valid and `False` otherwise. The total number of valid pixels, $N_{valid}$, is the sum of `True` elements in this mask.\n    $$M_{valid}(x) = \\mathbb{I}(\\neg\\mathrm{isNaN}(T(x)) \\land \\neg\\mathrm{isNaN}(P(x)) \\land \\neg\\mathrm{isNaN}(b(x)))$$\n    $$N_{valid} = \\sum_{x} M_{valid}(x)$$\n    If $N_{valid} = 0$, the problem states that the fraction $f$ is defined as $0.0$, and the calculation terminates.\n\n2.  **Clipping of Biotic Exclusion Factor**: The biotic exclusion factor $b(x)$ is defined as a fractional unavailability and must be within the closed unit interval $[0, 1]$. The problem requires clipping any values outside this range. We define a new grid, $b_{clipped}$, where for each pixel $x$:\n    $$b_{clipped}(x) = \\max(0, \\min(1, b(x)))$$\n    This operation is applied to the entire $b$ grid, including pixels that might later be identified as invalid; the invalid pixel mask $M_{valid}$ will ensure these locations are correctly ignored in final calculations.\n\n3.  **Calculation of Climate Feasibility**: A climate feasibility indicator, $C(x)$, is computed for each pixel. This indicator is $1$ if both temperature and precipitation are within their specified favorable ranges, and $0$ otherwise.\n    The temperature feasibility is given by the indicator mask $C_T(x) = \\mathbb{I}(10 \\leq T(x) \\leq 30)$.\n    The precipitation feasibility is given by the indicator mask $C_P(x) = \\mathbb{I}(500 \\leq P(x) \\leq 1500)$.\n    The combined climate feasibility is the product of these two indicators:\n    $$C(x) = C_T(x) \\cdot C_P(x) = \\mathbb{I}\\left((10 \\leq T(x) \\leq 30) \\land (500 \\leq P(x) \\leq 1500)\\right)$$\n\n4.  **Computation of the Suitability Index**: The habitat suitability index, $s(x)$, is now calculated for each pixel based on the climate feasibility $C(x)$ and the clipped biotic factor $b_{clipped}(x)$.\n    - If $C(x) = 0$, the climate is unsuitable, so $s(x) = 0$.\n    - If $C(x) = 1$, the climate is suitable, and the suitability is determined by the biotic factor: $s(x) = 1 - b_{clipped}(x)$.\n    This logic can be expressed concisely for the entire grid as:\n    $$s(x) = C(x) \\cdot (1 - b_{clipped}(x))$$\n\n5.  **Classification and Aggregation**: The final step is to determine the fraction of the valid landscape that is highly suitable. A pixel is classified as highly suitable if its suitability index $s(x)$ is greater than or equal to the threshold $\\tau_s = 0.8$. We count the number of pixels that satisfy this condition *and* are also valid. Let this count be $N_{suitable}$.\n    $$N_{suitable} = \\sum_{x} \\mathbb{I}(s(x) \\geq \\tau_s) \\cdot M_{valid}(x)$$\n    Note the multiplication by $M_{valid}(x)$ which ensures that only valid pixels are counted in the numerator, as stipulated by the problem.\n\n6.  **Calculation of the Final Fraction**: The fraction $f$ is the ratio of the number of highly suitable valid pixels to the total number of valid pixels.\n    $$f = \\frac{N_{suitable}}{N_{valid}}$$\n    As established in Step 1, if $N_{valid} = 0$, then $f = 0.0$. The final result is rounded to six decimal places.\n\nLet's apply this logic to Test Case 1:\n-   $$T^{(1)} = \\begin{bmatrix} 12 & 25 & 5 & 31 \\\\ 10 & 30 & 20 & \\mathrm{NaN} \\\\ 15 & 9 & 28 & 11 \\end{bmatrix}$$\n-   $$P^{(1)} = \\begin{bmatrix} 600 & 1400 & 600 & 600 \\\\ 500 & 1500 & 1000 & 900 \\\\ 499 & 800 & 1600 & \\mathrm{NaN} \\end{bmatrix}$$\n-   $$b^{(1)} = \\begin{bmatrix} 0.1 & 0.05 & 0.3 & 0.1 \\\\ 0.2 & 0.0 & 0.19 & 0.9 \\\\ 0.0 & 0.15 & 0.2 & 0.1 \\end{bmatrix}$$\n\n1.  **Valid Pixels**: Two pixels have $\\mathrm{NaN}$ values: at index $(1, 3)$ from $T^{(1)}$ and at index $(2, 3)$ from $P^{(1)}$. All other $12-2=10$ pixels are valid. Thus, $N_{valid} = 10$.\n2.  **Clip Biotic Factor**: All values in $b^{(1)}$ are already in $[0, 1]$, so $b_{clipped}^{(1)} = b^{(1)}$.\n3.  **Climate Feasibility**:\n    $$C_T = \\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 1 & 0 \\\\ 1 & 0 & 1 & 1 \\end{bmatrix}$$, $$C_P = \\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\\\ 0 & 1 & 0 & 0 \\end{bmatrix}$$\n    $$C = C_T \\cdot C_P = \\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$\n4.  **Suitability Index**: $s(x) = C(x) \\cdot (1 - b_{clipped}(x))$.\n    $$s = \\begin{bmatrix} 1\\cdot(1-0.1) & 1\\cdot(1-0.05) & 0 & 0 \\\\ 1\\cdot(1-0.2) & 1\\cdot(1-0.0) & 1\\cdot(1-0.19) & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 0.9 & 0.95 & 0 & 0 \\\\ 0.8 & 1.0 & 0.81 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$\n5.  **Classification**: Find where $s(x) \\geq 0.8$.\n    $$\\mathbb{I}(s(x) \\geq 0.8) = \\begin{bmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$\n    The number of highly suitable valid pixels, $N_{suitable}$, is the number of `1`s in this grid, excluding the positions of invalid pixels (none of which overlap here). There are $5$ such pixels. $N_{suitable} = 5$.\n6.  **Final Fraction**: $f = \\frac{N_{suitable}}{N_{valid}} = \\frac{5}{10} = 0.5$. Rounded to six decimal places, this is $0.500000$.\nThe same procedure is applied to all other test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the fraction of a landscape satisfying a habitat suitability threshold\n    for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: General mixed case\n        {\n            \"T\": np.array([\n                [12, 25, 5, 31],\n                [10, 30, 20, np.nan],\n                [15, 9, 28, 11]\n            ]),\n            \"P\": np.array([\n                [600, 1400, 600, 600],\n                [500, 1500, 1000, 900],\n                [499, 800, 1600, np.nan]\n            ]),\n            \"b\": np.array([\n                [0.1, 0.05, 0.3, 0.1],\n                [0.2, 0.0, 0.19, 0.9],\n                [0.0, 0.15, 0.2, 0.1]\n            ])\n        },\n        # Test Case 2: Boundary conditions and clipping\n        {\n            \"T\": np.array([\n                [10, 30, 20, 10],\n                [30, 10, 30, 25]\n            ]),\n            \"P\": np.array([\n                [500, 1500, 1000, 500],\n                [1500, 500, 1500, 700]\n            ]),\n            \"b\": np.array([\n                [0.2, 0.2, 0.8, 0.0],\n                [0.2000001, -0.1, 1.2, 0.2]\n            ])\n        },\n        # Test Case 3: All climate-infeasible\n        {\n            \"T\": np.array([\n                [9, 35],\n                [8, 31]\n            ]),\n            \"P\": np.array([\n                [400, 1600],\n                [200, 2000]\n            ]),\n            \"b\": np.array([\n                [0.0, 0.0],\n                [0.0, 0.0]\n            ])\n        },\n        # Test Case 4: No valid pixels\n        {\n            \"T\": np.array([\n                [np.nan, np.nan],\n                [np.nan, np.nan]\n            ]),\n            \"P\": np.array([\n                [np.nan, np.nan],\n                [np.nan, np.nan]\n            ]),\n            \"b\": np.array([\n                [np.nan, np.nan],\n                [np.nan, np.nan]\n            ])\n        }\n    ]\n\n    results = []\n    # Suitability threshold\n    tau_s = 0.8\n\n    for case in test_cases:\n        T, P, b = case[\"T\"], case[\"P\"], case[\"b\"]\n        \n        # 1. Identify valid pixels (not NaN in any layer)\n        valid_pixels_mask = ~np.isnan(T) & ~np.isnan(P) & ~np.isnan(b)\n        num_valid_pixels = np.sum(valid_pixels_mask)\n\n        # Handle case with no valid pixels\n        if num_valid_pixels == 0:\n            results.append(0.0)\n            continue\n\n        # 2. Clip biotic exclusion factor b to [0, 1]\n        b_clipped = np.clip(b, 0, 1)\n\n        # 3. Calculate climate feasibility indicator C(x)\n        # C(x) is 1 if T and P are within bounds, 0 otherwise.\n        temp_feasibility = (T >= 10) & (T <= 30)\n        precip_feasibility = (P >= 500) & (P <= 1500)\n        climate_feasibility_mask = temp_feasibility & precip_feasibility\n\n        # 4. Compute the suitability index s(x)\n        # s(x) = C(x) * (1 - b_clipped)\n        # This works because the boolean mask acts as 0s and 1s.\n        suitability_grid = climate_feasibility_mask * (1 - b_clipped)\n        \n        # 5. Count pixels where s(x) >= tau_s, but only for valid pixels\n        suitable_mask = suitability_grid >= tau_s\n        # The number of suitable pixels is the sum of pixels that are\n        # both suitable AND valid.\n        num_suitable_pixels = np.sum(suitable_mask & valid_pixels_mask)\n\n        # 6. Compute the final fraction f\n        f = num_suitable_pixels / num_valid_pixels\n        \n        # Store the result, rounded to six decimal places\n        results.append(round(f, 6))\n\n    # Format the final output string exactly as required, ensuring six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond fixed rules, this practice delves into correlative modeling by fitting a statistical relationship between environmental variables and species presence-absence data. You will implement a penalized logistic regression, a cornerstone of modern species distribution modeling, to learn habitat preferences directly from observations. This exercise provides hands-on experience with maximum likelihood estimation and regularization techniques that are essential for building robust statistical models in ecology .",
            "id": "3818652",
            "problem": "You are given a binary presence–absence dataset and environmental covariates from remote sensing for habitat suitability modeling. Assume that each observation $(y_i, x_i)$ satisfies a Bernoulli data-generating process with probability $p_i$ and a logistic link: $y_i \\sim \\mathrm{Bernoulli}(p_i)$ with $p_i = \\mathrm{logit}^{-1}(\\eta_i)$ and $\\eta_i = \\beta_0 + \\beta^\\top x_i$. Here, $x_i \\in \\mathbb{R}^d$ are covariates (without the intercept), $\\beta_0 \\in \\mathbb{R}$ is the intercept, and $\\beta \\in \\mathbb{R}^d$ are slope coefficients. The inverse logit function is defined by $\\mathrm{logit}^{-1}(z) = \\frac{1}{1 + e^{-z}}$.\n\nYour task is to write a complete program that, for each test case provided below, fits the coefficient vector $(\\beta_0, \\beta)$ by maximizing the Bernoulli log-likelihood under a logistic link, computes predicted suitability values on a supplied test grid of covariates, and reports the fitted coefficient estimates and the average predicted suitability across the grid. To ensure identifiability and numerical stability under quasi-separation or collinearity, use an isotropic Gaussian prior on the slopes (but not the intercept), which is equivalent to adding an $\\ell_2$ ridge penalty of strength $\\lambda$ to the slopes only. Concretely, the objective is the penalized log-likelihood\n$$\n\\ell(\\beta_0,\\beta) = \\sum_{i=1}^n \\left[ y_i \\,\\eta_i - \\log\\left(1 + e^{\\eta_i}\\right) \\right] - \\frac{\\lambda}{2}\\,\\|\\beta\\|_2^2,\n$$\nwith no penalty on $\\beta_0$. Use $\\lambda = 10^{-4}$. Do not apply any other regularization, weighting, or feature transformations.\n\nFundamental base to use: Bernoulli likelihood, the definition of the logistic link, and the Maximum Likelihood Estimation principle (optionally Maximum A Posteriori under a Gaussian prior, which yields the specified $\\ell_2$ penalty).\n\nImplement a numerically stable and convergent solver based on first principles, such as a Newton–Raphson method for Generalized Linear Models (GLMs), ensuring that the Hessian is adjusted to include the penalty on the slopes only. You may assume that the Hessian is strictly positive-definite due to the ridge penalty and that convergence can be tested by the maximum absolute change in parameters.\n\nFor each test case, compute:\n- The fitted coefficients $(\\hat{\\beta}_0, \\hat{\\beta})$.\n- The predicted suitability values $p_g = \\mathrm{logit}^{-1}(\\hat{\\beta}_0 + \\hat{\\beta}^\\top x_g)$ for each grid covariate $x_g$ given.\n- The average predicted suitability across the grid, $\\bar{p} = \\frac{1}{m} \\sum_{g=1}^m p_g$.\n\nAll outputs are dimensionless probabilities, so no physical units are required. If any quantity could be interpreted as a rate or proportion, express it as a decimal in $[0,1]$.\n\nTest suite of datasets and grids:\n- Test case $1$ (happy path, two covariates):\n  - Covariates per observation $(\\mathrm{NDVI}, \\mathrm{distance\\_to\\_water})$:\n    - $x_1 = (0.75, 0.2),\\; x_2 = (0.68, 0.5),\\; x_3 = (0.80, 1.2),\\; x_4 = (0.55, 2.0),$\n    - $x_5 = (0.40, 3.5),\\; x_6 = (0.30, 4.0),\\; x_7 = (0.62, 1.5),\\; x_8 = (0.48, 2.8),$\n    - $x_9 = (0.72, 0.8),\\; x_{10} = (0.66, 1.0)$.\n  - Presence–absence responses:\n    - $y = (1, 1, 1, 0, 0, 0, 1, 0, 1, 1)$.\n  - Test grid covariates:\n    - $g_1 = (0.60, 1.0),\\; g_2 = (0.50, 3.0),\\; g_3 = (0.80, 0.5),\\; g_4 = (0.35, 4.5)$.\n- Test case $2$ (quasi-separation, two covariates):\n  - Covariates:\n    - $x_1 = (0.82, 3.0),\\; x_2 = (0.78, 2.5),\\; x_3 = (0.76, 1.0),\\; x_4 = (0.20, 0.8),$\n    - $x_5 = (0.30, 1.5),\\; x_6 = (0.35, 2.0),\\; x_7 = (0.70, 4.0),\\; x_8 = (0.45, 3.5)$.\n  - Responses:\n    - $y = (1, 1, 1, 0, 0, 0, 1, 0)$.\n  - Test grid:\n    - $g_1 = (0.25, 1.0),\\; g_2 = (0.65, 2.0),\\; g_3 = (0.85, 0.5)$.\n- Test case $3$ (high collinearity, two covariates):\n  - Covariates:\n    - $x_1 = (0.20, 0.402),\\; x_2 = (0.25, 0.499),\\; x_3 = (0.30, 0.601),\\; x_4 = (0.35, 0.699),$\n    - $x_5 = (0.40, 0.800),\\; x_6 = (0.45, 0.901),\\; x_7 = (0.50, 1.001),\\; x_8 = (0.55, 1.099),$\n    - $x_9 = (0.60, 1.199),\\; x_{10} = (0.65, 1.300)$.\n  - Responses:\n    - $y = (0, 0, 0, 0, 1, 1, 1, 1, 1, 1)$.\n  - Test grid:\n    - $g_1 = (0.33, 0.66),\\; g_2 = (0.58, 1.16)$.\n\nProgram requirements:\n- Implement the maximization of the penalized Bernoulli log-likelihood using a Newton–Raphson procedure with the Hessian equal to $X^\\top W X + \\Lambda$, where $W$ is diagonal with entries $p_i (1 - p_i)$ and $\\Lambda$ is a diagonal matrix with entries $(0, \\lambda, \\lambda)$ so that only the slopes are penalized.\n- Use $\\lambda = 10^{-4}$ and initialize $(\\beta_0, \\beta)$ to zeros.\n- Convergence criterion: stop when the maximum absolute parameter update is less than $10^{-9}$ or after $100$ iterations, whichever comes first.\n- For each test case, compute and report a list with $4$ values: $[\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\bar{p}]$, where $\\bar{p}$ is the average predicted suitability across the grid for that test case.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list of lists with each value rounded to $6$ decimal places, for example, $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],[c_1,c_2,c_3,c_4]]$.",
            "solution": "We start from the Bernoulli likelihood for binary presence–absence data and the logistic link. For each observation $i \\in \\{1,\\dots,n\\}$ with response $y_i \\in \\{0,1\\}$ and covariate vector $x_i \\in \\mathbb{R}^d$ (without intercept), the model assumes $y_i \\sim \\mathrm{Bernoulli}(p_i)$ with $p_i = \\mathrm{logit}^{-1}(\\eta_i)$ and $\\eta_i = \\beta_0 + \\beta^\\top x_i$. The inverse logit function is $\\mathrm{logit}^{-1}(z) = \\frac{1}{1 + e^{-z}}$, which maps $\\mathbb{R}$ to $(0,1)$.\n\nThe log-likelihood of parameters $(\\beta_0, \\beta)$ is\n$$\n\\ell_{\\text{ll}}(\\beta_0,\\beta) = \\sum_{i=1}^n \\left[ y_i \\,\\eta_i - \\log\\!\\left(1 + e^{\\eta_i}\\right) \\right].\n$$\nTo address quasi-separation and collinearity and ensure numerical stability, we adopt a Gaussian prior on the slopes (but not on the intercept), equivalent to adding an $\\ell_2$ penalty to the slopes only. Let $\\lambda > 0$ denote the penalty strength and define the penalized log-likelihood\n$$\n\\ell(\\beta_0,\\beta) = \\ell_{\\text{ll}}(\\beta_0,\\beta) - \\frac{\\lambda}{2}\\,\\|\\beta\\|_2^2.\n$$\nThis is equivalent to Maximum A Posteriori estimation under $\\beta \\sim \\mathcal{N}(0, \\lambda^{-1} I)$ with no prior penalty on $\\beta_0$. The maximizer $(\\hat{\\beta}_0,\\hat{\\beta})$ can be obtained by Newton–Raphson iterations, which use the gradient and Hessian of $\\ell$.\n\nLet $X \\in \\mathbb{R}^{n \\times (d+1)}$ denote the design matrix with the first column equal to ones (the intercept) and the remaining $d$ columns equal to the covariates. Denote the full parameter vector by $\\theta = (\\beta_0, \\beta_1, \\dots, \\beta_d)^\\top \\in \\mathbb{R}^{d+1}$. Let $p \\in \\mathbb{R}^n$ be the vector with entries $p_i = \\mathrm{logit}^{-1}(x_i^\\top \\theta)$, where $x_i$ now refers to the full row of $X$ including the intercept. The gradient of the unpenalized log-likelihood is\n$$\n\\nabla \\ell_{\\text{ll}}(\\theta) = X^\\top (y - p),\n$$\nwhere $y \\in \\mathbb{R}^n$ is the response vector and the subtraction is elementwise. The Hessian of the unpenalized log-likelihood is\n$$\n\\nabla^2 \\ell_{\\text{ll}}(\\theta) = -X^\\top W X,\n$$\nwhere $W \\in \\mathbb{R}^{n \\times n}$ is diagonal with entries $W_{ii} = p_i (1 - p_i)$. For the penalized objective with a penalty applied to slopes only, define $\\Lambda \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ as a diagonal matrix with diagonal entries $(0, \\lambda, \\dots, \\lambda)$, where the zero on the first entry ensures no penalty on the intercept. The gradient and Hessian of the penalized log-posterior are then\n$$\n\\nabla \\ell(\\theta) = X^\\top (y - p) - \\Lambda \\theta,\\quad\n\\nabla^2 \\ell(\\theta) = -X^\\top W X - \\Lambda.\n$$\nThe Newton–Raphson update solves for the increment $\\Delta \\theta$ in the linear system\n$$\n\\big(X^\\top W X + \\Lambda\\big)\\, \\Delta \\theta = X^\\top (y - p) - \\Lambda \\theta,\n$$\nand then updates $\\theta \\leftarrow \\theta + \\Delta \\theta$. This is an Iteratively Reweighted Least Squares (IRLS) style update adapted for the ridge-penalized Generalized Linear Model (GLM). Because $\\Lambda$ is positive semidefinite with strictly positive entries for the slope components, $X^\\top W X + \\Lambda$ will be positive definite provided that at least one observation has $p_i \\in (0,1)$ and $\\lambda > 0$, which guarantees a unique solution for $\\Delta \\theta$ and good numerical conditioning.\n\nAlgorithmic steps for each test case:\n- Form $X$ by concatenating a column of ones with the covariate matrix.\n- Initialize $\\theta^{(0)} = 0 \\in \\mathbb{R}^{d+1}$.\n- For iteration $t = 0, 1, \\dots$ up to a maximum of $100$:\n  - Compute $\\eta^{(t)} = X \\theta^{(t)}$, then $p^{(t)} = \\mathrm{logit}^{-1}(\\eta^{(t)})$.\n  - Form $W^{(t)}$ with $W_{ii}^{(t)} = p_i^{(t)} \\big( 1 - p_i^{(t)} \\big)$.\n  - Compute gradient $g^{(t)} = X^\\top \\big( y - p^{(t)} \\big) - \\Lambda \\theta^{(t)}$.\n  - Compute Hessian-like matrix $H^{(t)} = X^\\top W^{(t)} X + \\Lambda$.\n  - Solve $H^{(t)} \\Delta \\theta^{(t)} = g^{(t)}$ and update $\\theta^{(t+1)} = \\theta^{(t)} + \\Delta \\theta^{(t)}$.\n  - Stop when $\\|\\Delta \\theta^{(t)}\\|_\\infty < 10^{-9}$.\n- Let $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ be the components of $\\hat{\\theta}$ for $d = 2$ covariates.\n- For each grid covariate $x_g \\in \\mathbb{R}^2$, compute $p_g = \\mathrm{logit}^{-1}\\big(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{g1} + \\hat{\\beta}_2 x_{g2}\\big)$.\n- Compute $\\bar{p} = \\frac{1}{m} \\sum_{g=1}^m p_g$ where $m$ is the number of grid points and all weights are equal.\n\nNumerical stability considerations:\n- To avoid overflow in computing $\\mathrm{logit}^{-1}(z)$, compute it in a numerically stable manner, for example by clipping $z$ to a bounded interval such as $[-35, 35]$ before applying $1/(1+e^{-z})$ or by using a stable implementation of the logistic function.\n- The ridge penalty with $\\lambda = 10^{-4}$ avoids singularity in $H^{(t)}$ in the presence of high collinearity or near-separation, ensuring invertibility in the linear solve at each iteration.\n- The intercept is not penalized, consistent with standard practice in regularized GLMs.\n\nOutput specification:\n- For each test case, produce the list $[\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\bar{p}]$ with all values rounded to $6$ decimal places.\n- Aggregate all test case results into a single list of lists and print that list on a single line as the program’s only output, for example: $[[b_{01}, b_{11}, b_{21}, \\bar{p}_1], [b_{02}, b_{12}, b_{22}, \\bar{p}_2], [b_{03}, b_{13}, b_{23}, \\bar{p}_3]]$.\n\nThis procedure directly implements habitat suitability modeling as a logistic regression on remote sensing covariates, computing predicted suitability across a test grid that represents the study area and reporting both fitted coefficients and the mean predicted suitability.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef stable_logistic(z):\n    # Compute logistic function in a numerically stable way.\n    # Clip z to avoid overflow in exp.\n    z = np.clip(z, -35.0, 35.0)\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef fit_logistic_ridge(X, y, lam=1e-4, tol=1e-9, max_iter=100):\n    \"\"\"\n    Fit penalized logistic regression with ridge penalty on slopes only.\n    X: (n, d+1) design matrix with intercept in the first column (ones).\n    y: (n,) binary responses.\n    lam: ridge penalty strength applied to slopes only.\n    Returns theta of shape (d+1,).\n    \"\"\"\n    n, p = X.shape\n    theta = np.zeros(p, dtype=float)\n    # Penalty matrix: 0 on intercept, lam on slopes.\n    Lambda = np.diag([0.0] + [lam] * (p - 1))\n\n    for _ in range(max_iter):\n        eta = X @ theta\n        p_hat = stable_logistic(eta)\n        W = p_hat * (1.0 - p_hat)  # (n,)\n        # Form X^T W X efficiently\n        # H = X^T W X + Lambda\n        # Use broadcasting to multiply rows of X by W\n        XW = X * W[:, None]\n        H = X.T @ XW + Lambda\n        # gradient: X^T(y - p) - Lambda theta\n        g = X.T @ (y - p_hat) - Lambda @ theta\n        try:\n            delta = np.linalg.solve(H, g)\n        except np.linalg.LinAlgError:\n            # Fallback to least squares solution if needed\n            delta = np.linalg.lstsq(H, g, rcond=None)[0]\n        theta_new = theta + delta\n        if np.max(np.abs(delta)) < tol:\n            theta = theta_new\n            break\n        theta = theta_new\n    return theta\n\ndef predict_logistic(theta, X_grid):\n    \"\"\"\n    Predict probabilities for grid covariates without intercept column.\n    theta: (d+1,), X_grid: (m, d)\n    \"\"\"\n    intercept = theta[0]\n    beta = theta[1:]\n    eta = intercept + X_grid @ beta\n    return stable_logistic(eta)\n\ndef solve():\n    lam = 1e-4\n\n    # Test case 1\n    X1_cov = np.array([\n        [0.75, 0.2],\n        [0.68, 0.5],\n        [0.80, 1.2],\n        [0.55, 2.0],\n        [0.40, 3.5],\n        [0.30, 4.0],\n        [0.62, 1.5],\n        [0.48, 2.8],\n        [0.72, 0.8],\n        [0.66, 1.0],\n    ], dtype=float)\n    y1 = np.array([1,1,1,0,0,0,1,0,1,1], dtype=float)\n    G1 = np.array([\n        [0.60, 1.0],\n        [0.50, 3.0],\n        [0.80, 0.5],\n        [0.35, 4.5],\n    ], dtype=float)\n\n    # Test case 2\n    X2_cov = np.array([\n        [0.82, 3.0],\n        [0.78, 2.5],\n        [0.76, 1.0],\n        [0.20, 0.8],\n        [0.30, 1.5],\n        [0.35, 2.0],\n        [0.70, 4.0],\n        [0.45, 3.5],\n    ], dtype=float)\n    y2 = np.array([1,1,1,0,0,0,1,0], dtype=float)\n    G2 = np.array([\n        [0.25, 1.0],\n        [0.65, 2.0],\n        [0.85, 0.5],\n    ], dtype=float)\n\n    # Test case 3\n    X3_cov = np.array([\n        [0.20, 0.402],\n        [0.25, 0.499],\n        [0.30, 0.601],\n        [0.35, 0.699],\n        [0.40, 0.800],\n        [0.45, 0.901],\n        [0.50, 1.001],\n        [0.55, 1.099],\n        [0.60, 1.199],\n        [0.65, 1.300],\n    ], dtype=float)\n    y3 = np.array([0,0,0,0,1,1,1,1,1,1], dtype=float)\n    G3 = np.array([\n        [0.33, 0.66],\n        [0.58, 1.16],\n    ], dtype=float)\n\n    test_cases = [\n        (X1_cov, y1, G1),\n        (X2_cov, y2, G2),\n        (X3_cov, y3, G3),\n    ]\n\n    results = []\n    for X_cov, y, G in test_cases:\n        n, d = X_cov.shape\n        # Build design matrix with intercept\n        X = np.hstack([np.ones((n, 1)), X_cov])\n        theta_hat = fit_logistic_ridge(X, y, lam=lam, tol=1e-9, max_iter=100)\n        # Predictions on grid\n        p_grid = predict_logistic(theta_hat, G)\n        avg_p = float(np.mean(p_grid))\n        # Collect [beta0, beta1, beta2, avg_p]\n        res = [float(theta_hat[0]), float(theta_hat[1]), float(theta_hat[2]), avg_p]\n        results.append(res)\n\n    # Format results: each value rounded to 6 decimals\n    def fmt(x):\n        return f\"{x:.6f}\"\n\n    out = \"[\" + \",\".join(\"[\" + \",\".join(fmt(v) for v in row) + \"]\" for row in results) + \"]\"\n    print(out)\n\nsolve()\n```"
        },
        {
            "introduction": "A critical task in habitat modeling is projecting a trained model into new areas or time periods, such as future climate scenarios, which carries the risk of extrapolation. This exercise equips you with tools to assess this challenge, including the Multivariate Environmental Similarity Surface (MESS), to identify where your model is predicting into novel environmental conditions. You will also implement 'clamping' to constrain predictions and quantify its impact, fostering a critical perspective on model transferability .",
            "id": "3818689",
            "problem": "You are modeling habitat suitability for a species using a set of environmental predictors derived from remote sensing and climate data to assess future projections. You will implement a rigorous procedure to compute a Multivariate Environmental Similarity Surface (MESS) map, identify extrapolation, apply clamping on predictors to the training range, and quantify changes in predicted suitability under a future scenario. All computations must be grounded in fundamental definitions of normalization on bounded intervals and logistic transformation and must be expressed mathematically.\n\nLet the predictor vector be $\\mathbf{z} = (z_1, z_2, z_3)$, where $z_1$ is mean annual temperature in degrees Celsius, $z_2$ is annual precipitation in millimeters per year, and $z_3$ is Normalized Difference Vegetation Index (NDVI, unitless). The training environmental space is summarized by lower bounds $\\mathbf{a} = (a_1, a_2, a_3)$ and upper bounds $\\mathbf{b} = (b_1, b_2, b_3)$, the training means $\\boldsymbol{\\mu} = (\\mu_1, \\mu_2, \\mu_3)$, and the training standard deviations $\\boldsymbol{\\sigma} = (\\sigma_1, \\sigma_2, \\sigma_3)$. The elements are:\n- Temperature: $a_1 = 12$ degrees Celsius, $b_1 = 20$ degrees Celsius, $\\mu_1 = 16$ degrees Celsius, $\\sigma_1 = 2.5$ degrees Celsius.\n- Precipitation: $a_2 = 800$ millimeters per year, $b_2 = 1200$ millimeters per year, $\\mu_2 = 1000$ millimeters per year, $\\sigma_2 = 120$ millimeters per year.\n- NDVI: $a_3 = 0.4$ (unitless), $b_3 = 0.7$ (unitless), $\\mu_3 = 0.55$ (unitless), $\\sigma_3 = 0.08$ (unitless).\n\nDefine, for each predictor $i \\in \\{1,2,3\\}$, the normalized position within the training interval as\n$$\ns_i(z_i) = \\frac{z_i - a_i}{b_i - a_i}.\n$$\nDefine the per-variable centrality-based similarity function $c_i(z_i)$ by the piecewise expression\n$$\nc_i(z_i) =\n\\begin{cases}\n1 - 2\\left| s_i(z_i) - \\frac{1}{2} \\right|, & \\text{if } a_i \\le z_i \\le b_i, \\\\\n-\\dfrac{a_i - z_i}{b_i - a_i}, & \\text{if } z_i < a_i, \\\\\n-\\dfrac{z_i - b_i}{b_i - a_i}, & \\text{if } z_i > b_i.\n\\end{cases}\n$$\nThis function yields values in the interval $[-1,1]$, equals $1$ at the interval center, equals $0$ at the bounds, and is negative outside the training range, proportional to the exceedance distance relative to the interval width.\n\nDefine the Multivariate Environmental Similarity Surface (MESS) score as\n$$\nM(\\mathbf{z}) = \\min\\{c_1(z_1), c_2(z_2), c_3(z_3)\\}.\n$$\nDefine extrapolation as the condition\n$$\nE(\\mathbf{z}) = \\left( \\exists i \\in \\{1,2,3\\} \\text{ such that } z_i < a_i \\text{ or } z_i > b_i \\right),\n$$\nequivalently $E(\\mathbf{z})$ is true if and only if $M(\\mathbf{z}) < 0$.\n\nTo limit extrapolation impacts on predictions, define clamping to the training interval for each predictor:\n$$\n\\tilde{z}_i = \\min\\{ \\max\\{ z_i, a_i \\}, b_i \\}, \\quad i \\in \\{1,2,3\\}.\n$$\nLet the standardized predictors be\n$$\nz_i^{\\star} = \\frac{z_i - \\mu_i}{\\sigma_i}, \\quad \\tilde{z}_i^{\\star} = \\frac{\\tilde{z}_i - \\mu_i}{\\sigma_i}.\n$$\nLet suitability be modeled by a logistic function (a Generalized Linear Model (GLM) with a logit link) using fixed coefficients $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)$:\n$$\n\\beta_0 = -0.5, \\quad \\beta_1 = 0.9, \\quad \\beta_2 = 0.6, \\quad \\beta_3 = 1.2.\n$$\nDefine the linear predictors\n$$\nf(\\mathbf{z}) = \\beta_0 + \\beta_1 z_1^{\\star} + \\beta_2 z_2^{\\star} + \\beta_3 z_3^{\\star}, \\quad\nf(\\tilde{\\mathbf{z}}) = \\beta_0 + \\beta_1 \\tilde{z}_1^{\\star} + \\beta_2 \\tilde{z}_2^{\\star} + \\beta_3 \\tilde{z}_3^{\\star},\n$$\nand the suitability values\n$$\nS(\\mathbf{z}) = \\frac{1}{1 + e^{-f(\\mathbf{z})}}, \\quad S(\\tilde{\\mathbf{z}}) = \\frac{1}{1 + e^{-f(\\tilde{\\mathbf{z}})}}.\n$$\nQuantify the clamping-induced change in predicted suitability as\n$$\n\\Delta S = S(\\tilde{\\mathbf{z}}) - S(\\mathbf{z}).\n$$\n\nImplement a program that, for a set of future climate scenario pixels, computes $M(\\mathbf{z})$, $E(\\mathbf{z})$, and $\\Delta S$. All raw predictors must be handled using the physical units specified above. Report floating-point answers rounded to $6$ decimal places. Angles do not appear in this problem. No percentages are used.\n\nUse the following test suite of future predictor vectors $\\mathbf{z}$:\n- Test $1$: $\\mathbf{z} = (16, 1000, 0.55)$ with units as specified above.\n- Test $2$: $\\mathbf{z} = (24, 900, 0.60)$ with units as specified above.\n- Test $3$: $\\mathbf{z} = (12, 800, 0.40)$ with units as specified above.\n- Test $4$: $\\mathbf{z} = (10, 1300, 0.20)$ with units as specified above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of the form $[M(\\mathbf{z}), E(\\mathbf{z}), \\Delta S]$. For example, the output format must be of the form $[[m_1,e_1,d_1],[m_2,e_2,d_2],[m_3,e_3,d_3],[m_4,e_4,d_4]]$, with all floating-point values rounded to $6$ decimal places.",
            "solution": "The problem requires the implementation of a procedure to assess habitat suitability under a future environmental scenario, involving the calculation of a Multivariate Environmental Similarity Surface (MESS), identification of extrapolation, application of predictor clamping, and quantification of the resulting change in predicted suitability. The problem is scientifically sound, well-posed, and all necessary parameters and definitions are provided.\n\nFirst, we establish the constants and parameters provided. The environmental predictors are $\\mathbf{z} = (z_1, z_2, z_3)$. The training data is summarized by lower bounds $\\mathbf{a} = (12, 800, 0.4)$, upper bounds $\\mathbf{b} = (20, 1200, 0.7)$, means $\\boldsymbol{\\mu} = (16, 1000, 0.55)$, and standard deviations $\\boldsymbol{\\sigma} = (2.5, 120, 0.08)$. The logistic model coefficients are $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\beta_3) = (-0.5, 0.9, 0.6, 1.2)$.\n\nThe solution process for any given future predictor vector $\\mathbf{z}$ involves the following sequential steps:\n\n1.  **Compute the per-variable similarity scores $c_i(z_i)$ for $i \\in \\{1,2,3\\}$.**\n    This requires first calculating the normalized position $s_i(z_i) = \\frac{z_i - a_i}{b_i - a_i}$. The similarity $c_i(z_i)$ is then determined by the piecewise function:\n    $$\n    c_i(z_i) =\n    \\begin{cases}\n    1 - 2\\left| s_i(z_i) - \\frac{1}{2} \\right|, & \\text{if } a_i \\le z_i \\le b_i \\\\\n    -\\dfrac{a_i - z_i}{b_i - a_i} = s_i(z_i), & \\text{if } z_i < a_i \\\\\n    -\\dfrac{z_i - b_i}{b_i - a_i} = -(s_i(z_i) - 1), & \\text{if } z_i > b_i\n    \\end{cases}\n    $$\n    Note that the expressions for $z_i < a_i$ and $z_i > b_i$ can also be expressed in terms of $s_i(z_i)$. For $z_i < a_i$, $s_i(z_i) = \\frac{z_i-a_i}{b_i-a_i} < 0$, and $-\\frac{a_i-z_i}{b_i-a_i} = \\frac{z_i-a_i}{b_i-a_i} = s_i(z_i)$.\n\n2.  **Compute the MESS score $M(\\mathbf{z})$ and determine extrapolation $E(\\mathbf{z})$.**\n    The MESS score is the minimum of the individual similarity scores:\n    $$\n    M(\\mathbf{z}) = \\min\\{c_1(z_1), c_2(z_2), c_3(z_3)\\}\n    $$\n    Extrapolation $E(\\mathbf{z})$ occurs if any predictor is outside its training range, which is equivalent to the MESS score being negative. We will represent $E(\\mathbf{z})$ as $1$ if true (extrapolation) and $0$ if false (interpolation or on the boundary).\n    $$\n    E(\\mathbf{z}) =\n    \\begin{cases}\n    1 & \\text{if } M(\\mathbf{z}) < 0 \\\\\n    0 & \\text{if } M(\\mathbf{z}) \\ge 0\n    \\end{cases}\n    $$\n\n3.  **Perform predictor clamping.**\n    To mitigate the effects of extrapolation, each predictor $z_i$ is clamped to its training range $[a_i, b_i]$:\n    $$\n    \\tilde{z}_i = \\min\\{ \\max\\{ z_i, a_i \\}, b_i \\}\n    $$\n    This creates a clamped predictor vector $\\tilde{\\mathbf{z}} = (\\tilde{z}_1, \\tilde{z}_2, \\tilde{z}_3)$.\n\n4.  **Standardize the original and clamped predictors.**\n    Both the original vector $\\mathbf{z}$ and the clamped vector $\\tilde{\\mathbf{z}}$ are standardized using the training mean $\\boldsymbol{\\mu}$ and standard deviation $\\boldsymbol{\\sigma}$:\n    $$\n    z_i^{\\star} = \\frac{z_i - \\mu_i}{\\sigma_i}, \\quad \\tilde{z}_i^{\\star} = \\frac{\\tilde{z}_i - \\mu_i}{\\sigma_i}\n    $$\n\n5.  **Calculate suitability scores $S(\\mathbf{z})$ and $S(\\tilde{\\mathbf{z}})$.**\n    First, the linear predictors are computed using the coefficients $\\boldsymbol{\\beta}$:\n    $$\n    f(\\mathbf{z}) = \\beta_0 + \\beta_1 z_1^{\\star} + \\beta_2 z_2^{\\star} + \\beta_3 z_3^{\\star}\n    $$\n    $$\n    f(\\tilde{\\mathbf{z}}) = \\beta_0 + \\beta_1 \\tilde{z}_1^{\\star} + \\beta_2 \\tilde{z}_2^{\\star} + \\beta_3 \\tilde{z}_3^{\\star}\n    $$\n    Then, the logistic function is applied to obtain the suitability scores:\n    $$\n    S(\\mathbf{z}) = \\frac{1}{1 + e^{-f(\\mathbf{z})}}, \\quad S(\\tilde{\\mathbf{z}}) = \\frac{1}{1 + e^{-f(\\tilde{\\mathbf{z}})}}\n    $$\n\n6.  **Quantify the change in suitability $\\Delta S$.**\n    The change is the difference between the clamped and original suitability predictions:\n    $$\n    \\Delta S = S(\\tilde{\\mathbf{z}}) - S(\\mathbf{z})\n    $$\n\nLet's walk through an example calculation for **Test Case 2**: $\\mathbf{z} = (24, 900, 0.60)$.\n\n1.  **Similarity scores $c_i(z_i)$**:\n    *   $z_1 = 24 > b_1 = 20$. Extrapolation. $c_1(24) = -\\frac{24 - 20}{20 - 12} = -\\frac{4}{8} = -0.5$.\n    *   $a_2 = 800 \\le z_2 = 900 \\le b_2 = 1200$. Interpolation. $s_2(900) = \\frac{900 - 800}{1200 - 800} = \\frac{100}{400} = 0.25$.\n        $c_2(900) = 1 - 2|0.25 - 0.5| = 1 - 2(0.25) = 0.5$.\n    *   $a_3 = 0.4 \\le z_3 = 0.6 \\le b_3 = 0.7$. Interpolation. $s_3(0.6) = \\frac{0.6 - 0.4}{0.7 - 0.4} = \\frac{0.2}{0.3} = \\frac{2}{3}$.\n        $c_3(0.6) = 1 - 2|\\frac{2}{3} - 0.5| = 1 - 2(\\frac{1}{6}) = \\frac{2}{3} \\approx 0.666667$.\n\n2.  **MESS score $M(\\mathbf{z})$ and Extrapolation $E(\\mathbf{z})$**:\n    *   $M(\\mathbf{z}) = \\min\\{-0.5, 0.5, \\frac{2}{3}\\} = -0.5$.\n    *   Since $M(\\mathbf{z}) < 0$, we have extrapolation, so $E(\\mathbf{z}) = 1$.\n\n3.  **Predictor clamping**:\n    *   $\\tilde{z}_1 = \\min\\{\\max\\{24, 12\\}, 20\\} = \\min\\{24, 20\\} = 20$.\n    *   $\\tilde{z}_2 = \\min\\{\\max\\{900, 800\\}, 1200\\} = \\min\\{900, 1200\\} = 900$.\n    *   $\\tilde{z}_3 = \\min\\{\\max\\{0.6, 0.4\\}, 0.7\\} = \\min\\{0.6, 0.7\\} = 0.6$.\n    *   $\\tilde{\\mathbf{z}} = (20, 900, 0.6)$.\n\n4.  **Standardization**:\n    *   $z_1^{\\star} = \\frac{24 - 16}{2.5} = 3.2$, $z_2^{\\star} = \\frac{900 - 1000}{120} = -\\frac{5}{6}$, $z_3^{\\star} = \\frac{0.6 - 0.55}{0.08} = 0.625$.\n    *   $\\tilde{z}_1^{\\star} = \\frac{20 - 16}{2.5} = 1.6$, $\\tilde{z}_2^{\\star} = \\frac{900 - 1000}{120} = -\\frac{5}{6}$, $\\tilde{z}_3^{\\star} = \\frac{0.6 - 0.55}{0.08} = 0.625$.\n\n5.  **Suitability calculation**:\n    *   $f(\\mathbf{z}) = -0.5 + 0.9(3.2) + 0.6(-\\frac{5}{6}) + 1.2(0.625) = -0.5 + 2.88 - 0.5 + 0.75 = 2.63$.\n    *   $S(\\mathbf{z}) = \\frac{1}{1 + e^{-2.63}} \\approx 0.932717$.\n    *   $f(\\tilde{\\mathbf{z}}) = -0.5 + 0.9(1.6) + 0.6(-\\frac{5}{6}) + 1.2(0.625) = -0.5 + 1.44 - 0.5 + 0.75 = 1.19$.\n    *   $S(\\tilde{\\mathbf{z}}) = \\frac{1}{1 + e^{-1.19}} \\approx 0.766755$.\n\n6.  **Change $\\Delta S$**:\n    *   $\\Delta S = S(\\tilde{\\mathbf{z}}) - S(\\mathbf{z}) \\approx 0.766755 - 0.932717 = -0.165962$.\n\nThe final result for this test case, after rounding to $6$ decimal places, is $[M(\\mathbf{z}), E(\\mathbf{z}), \\Delta S] = [-0.5, 1, -0.165962]$. This process is repeated for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes habitat suitability metrics for a set of future climate scenarios.\n    \"\"\"\n    \n    # Define constants from the problem statement\n    # Lower bounds [temperature, precipitation, NDVI]\n    a = np.array([12.0, 800.0, 0.4])\n    # Upper bounds\n    b = np.array([20.0, 1200.0, 0.7])\n    # Training means\n    mu = np.array([16.0, 1000.0, 0.55])\n    # Training standard deviations\n    sigma = np.array([2.5, 120.0, 0.08])\n    # GLM coefficients [intercept, beta1, beta2, beta3]\n    beta = np.array([-0.5, 0.9, 0.6, 1.2])\n\n    test_cases = [\n        np.array([16.0, 1000.0, 0.55]),  # Test 1\n        np.array([24.0, 900.0, 0.60]),   # Test 2\n        np.array([12.0, 800.0, 0.40]),   # Test 3\n        np.array([10.0, 1300.0, 0.20])   # Test 4\n    ]\n\n    all_results = []\n\n    for z in test_cases:\n        # Step 1: Compute per-variable centrality-based similarity c_i(z_i)\n        c = np.zeros(3)\n        interval_width = b - a\n        \n        for i in range(3):\n            if z[i] < a[i]:\n                # Extrapolation below the lower bound\n                c[i] = -(a[i] - z[i]) / interval_width[i]\n            elif z[i] > b[i]:\n                # Extrapolation above the upper bound\n                c[i] = -(z[i] - b[i]) / interval_width[i]\n            else:\n                # Interpolation within the training range\n                s_i = (z[i] - a[i]) / interval_width[i]\n                c[i] = 1.0 - 2.0 * np.abs(s_i - 0.5)\n\n        # Step 2: Compute MESS score M(z) and determine extrapolation E(z)\n        m_z = np.min(c)\n        e_z = 1 if m_z < 0 else 0\n\n        # Step 3: Perform predictor clamping\n        z_clamped = np.clip(z, a, b)\n        \n        # Step 4: Standardize original and clamped predictors\n        z_star = (z - mu) / sigma\n        z_clamped_star = (z_clamped - mu) / sigma\n\n        # Step 5: Calculate suitability scores S(z) and S(~z)\n        # Add intercept term for linear predictor calculation\n        z_star_with_intercept = np.insert(z_star, 0, 1)\n        z_clamped_star_with_intercept = np.insert(z_clamped_star, 0, 1)\n        \n        # We need to adjust the beta vector to use with the inserted '1'\n        # f(z) = beta0 + beta1*z1* + ...\n        # Let's compute manually to avoid confusion\n        f_z = beta[0] + np.dot(beta[1:], z_star)\n        f_z_clamped = beta[0] + np.dot(beta[1:], z_clamped_star)\n\n        # Logistic function S(x) = 1 / (1 + exp(-f(x)))\n        s_z = 1.0 / (1.0 + np.exp(-f_z))\n        s_z_clamped = 1.0 / (1.0 + np.exp(-f_z_clamped))\n\n        # Step 6: Quantify clamping-induced change\n        delta_s = s_z_clamped - s_z\n        \n        # Round results to 6 decimal places\n        result_tuple = [\n            round(m_z, 6),\n            e_z,\n            round(delta_s, 6)\n        ]\n        all_results.append(result_tuple)\n        \n    # Final print statement in the exact required format.\n    # The requirement `[[m1,e1,d1],[m2,e2,d2],...]` is the default string representation of a list of lists.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}