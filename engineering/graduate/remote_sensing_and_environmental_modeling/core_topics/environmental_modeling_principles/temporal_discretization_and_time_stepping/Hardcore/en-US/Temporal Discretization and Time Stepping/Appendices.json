{
    "hands_on_practices": [
        {
            "introduction": "This practice provides a foundational exercise in numerical analysis, where you will empirically verify the performance of common time-stepping schemes. By implementing the Explicit Euler, Implicit Euler, and Heun's methods on a standard test problem, you will compute their observed order of accuracy and witness the effects of equation stiffness on numerical stability . This hands-on verification is a crucial skill for any modeler, bridging the gap between theoretical properties and practical behavior.",
            "id": "3859481",
            "problem": "Consider the linear ordinary differential equation (ODE) initial value problem $y'(t)=-\\lambda y(t)$ for $t \\in [0,T]$ with initial condition $y(0)=y_0$, where $\\lambda0$. The exact solution is $y(t)=y_0 \\exp(-\\lambda t)$. This linear test problem is a standard proxy for the temporal behavior of linearized relaxation processes in environmental models and serves to evaluate numerical time-stepping schemes that are also used in data-driven state estimation informed by remote sensing. Your task is to design a program that, for a given set of parameters, computes the observed order of accuracy at the final time by comparing the global error across two successive step-size halvings for each of the following three methods:\n\n- Explicit (forward) Euler: $y_{n+1} = y_n + h f(t_n,y_n)$.\n- Implicit (backward) Euler: $y_{n+1} = y_n + h f(t_{n+1},y_{n+1})$.\n- Heun’s method (explicit trapezoidal, a two-stage Runge–Kutta method): $y_{n+1} = y_n + \\dfrac{h}{2}\\left(f(t_n,y_n)+f(t_n+h,y_n+h f(t_n,y_n))\\right)$.\n\nHere $f(t,y)=-\\lambda y$, $h$ is the time step, and the numerical solution at the final time $T$ is obtained after $N$ steps with $h = T/N$. For a given base step count $N_0$, define two refinements by doubling the number of steps each time, i.e., use $N_0$, $2N_0$, and $4N_0$, which correspond to $h$, $h/2$, and $h/4$. For each method and each parameter set, compute the global error $E(h)=\\lvert y_N - y(T)\\rvert$ at the final time for $h$, $h/2$, and $h/4$. Then estimate the observed order of accuracy by comparing errors between successive refinements and averaging the two estimates obtained from the pairs $(h,h/2)$ and $(h/2,h/4)$. Do not use any analytic error constants or pre-derived error order formulas in your implementation; instead, base your estimate solely on the errors obtained from numerical runs at the three resolutions.\n\nScientific basis you must adhere to:\n- Use the definition of the global error at a fixed final time $T$ and the notion that for a consistent method there exists a constant $C$ and an order $p0$ such that $E(h) \\approx C h^p$ as $h \\to 0$.\n- Use only the problem’s exact solution $y(T)=y_0 \\exp(-\\lambda T)$ to compute errors; do not use manufactured analyses or internal error estimators.\n\nTest suite:\nUse $y_0 = 1$ for all cases. For each case, the program must first set the base number of steps $N_0$ as the nearest integer to $T/h_0$, i.e., $N_0 = \\operatorname{round}(T/h_0)$, then use $N_0$, $2N_0$, and $4N_0$. This guarantees the final time $T$ is hit exactly at each resolution.\n\nProvide observed orders for the following four parameter sets, chosen to probe nonstiff, moderately stiff, stiff-within-stability, and stiff-near-instability regimes:\n- Case A (nonstiff): $\\lambda = 1$, $T = 1$, $h_0 = 0.2$.\n- Case B (moderately stiff, boundary for explicit methods): $\\lambda = 20$, $T = 1$, $h_0 = 0.1$.\n- Case C (stiff but within explicit stability when refined): $\\lambda = 100$, $T = 1$, $h_0 = 0.015$.\n- Case D (very stiff with coarse unstable explicit steps but short horizon): $\\lambda = 500$, $T = 0.1$, $h_0 = 0.05$.\n\nYour program must:\n- Implement the three methods exactly as defined above.\n- For each case, compute the observed order of accuracy for each method by averaging the two refinement-based estimates obtained from the three resolutions $N_0$, $2N_0$, and $4N_0$.\n- Return a single line of output containing the $12$ floating-point results as a comma-separated list enclosed in square brackets. The sequence must be, for each case in the order A, B, C, D, the observed orders for explicit Euler, implicit Euler, and Heun’s method, in that order within each case. For example, the output format is $[\\text{A\\_EE},\\text{A\\_IE},\\text{A\\_HM},\\text{B\\_EE},\\text{B\\_IE},\\text{B\\_HM},\\text{C\\_EE},\\text{C\\_IE},\\text{C\\_HM},\\text{D\\_EE},\\text{D\\_IE},\\text{D\\_HM}]$, where $\\text{EE}$ denotes explicit Euler, $\\text{IE}$ denotes implicit Euler, and $\\text{HM}$ denotes Heun’s method. Round each reported value numerically to $6$ decimal places before outputting.\n\nAngle units are not applicable. No physical units are involved; all quantities are dimensionless. The final output must be a single line with the exact list syntax shown above.",
            "solution": "### **Problem Validation**\n\n#### **Step 1: Extract Givens**\n\nThe problem statement provides the following information:\n- **ODE**: $y'(t)=-\\lambda y(t)$ for $t \\in [0,T]$.\n- **Initial Condition**: $y(0)=y_0$.\n- **Parameter Constraint**: $\\lambda0$.\n- **Exact Solution**: $y(t)=y_0 \\exp(-\\lambda t)$.\n- **Function Definition**: $f(t,y)=-\\lambda y$.\n- **Numerical Methods**:\n    1.  **Explicit (forward) Euler**: $y_{n+1} = y_n + h f(t_n,y_n)$.\n    2.  **Implicit (backward) Euler**: $y_{n+1} = y_n + h f(t_{n+1},y_{n+1})$.\n    3.  **Heun’s method (explicit trapezoidal)**: $y_{n+1} = y_n + \\dfrac{h}{2}\\left(f(t_n,y_n)+f(t_n+h,y_n+h f(t_n,y_n))\\right)$.\n- **Time Discretization**: $h = T/N$, where $N$ is the number of steps.\n- **Error Calculation**: Global error $E(h)=\\lvert y_N - y(T)\\rvert$ at the final time $T$.\n- **Order of Accuracy Estimation**: For resolutions corresponding to step counts $N_0, 2N_0, 4N_0$, the observed order is the average of two estimates.\n- **Scientific Basis**: $E(h) \\approx C h^p$ for small $h$, and error computation must use the provided exact solution.\n- **Test Suite Setup**: $y_0 = 1$ for all cases. Base step count $N_0 = \\operatorname{round}(T/h_0)$.\n- **Parameter Sets**:\n    -   Case A: $\\lambda = 1$, $T = 1$, $h_0 = 0.2$.\n    -   Case B: $\\lambda = 20$, $T = 1$, $h_0 = 0.1$.\n    -   Case C: $\\lambda = 100$, $T = 1$, $h_0 = 0.015$.\n    -   Case D: $\\lambda = 500$, $T = 0.1$, $h_0 = 0.05$.\n- **Output Format**: A single line containing a comma-separated list of $12$ floating-point results, rounded to $6$ decimal places, enclosed in square brackets. The order is specified as (A_EE, A_IE, A_HM, B_EE, B_IE, B_HM, C_EE, C_IE, C_HM, D_EE, D_IE, D_HM).\n\n#### **Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is based on the linear test equation $y' = -\\lambda y$, a fundamental tool in the numerical analysis of ordinary differential equations for studying the stability and accuracy of time integration schemes. The methods provided (Explicit Euler, Implicit Euler, Heun's) are standard, canonical examples of numerical integrators. This problem is firmly rooted in established numerical analysis principles.\n- **Well-Posed**: The problem is specified with precision. All constants, equations, initial conditions, and procedures are explicitly defined. For each parameter set, the instructions lead to a single, unique, and meaningful numerical result (the observed order of accuracy).\n- **Objective**: The problem is stated in precise mathematical and computational terms, free of any subjectivity, ambiguity, or opinion-based claims.\n\nThe problem does not exhibit any of the invalidity flaws:\n1.  **Scientific/Factual Unsoundness**: No violations of mathematical logic or scientific principles exist.\n2.  **Non-Formalizable/Irrelevant**: The problem is a formal, quantifiable task directly relevant to temporal discretization in modeling.\n3.  **Incomplete/Contradictory Setup**: All necessary information is provided, and there are no contradictions.\n4.  **Unrealistic/Infeasible**: The parameters are chosen to explore different stiffness regimes, which is realistic in the context of numerical analysis. The computations are feasible.\n5.  **Ill-Posed/Poorly Structured**: The procedure for calculating the observed order is well-defined and leads to a unique result.\n6.  **Pseudo-Profound/Trivial**: The problem requires a correct implementation of numerical methods and an understanding of how to assess their empirical performance, which is a non-trivial task that illustrates important concepts of stability and convergence.\n7.  **Outside Scientific Verifiability**: The results are computationally verifiable.\n\n#### **Step 3: Verdict and Action**\n\nThe problem statement is **valid**. A solution will be provided.\n\n### **Solution**\n\nThe problem requires us to compute the observed order of accuracy for three numerical methods—Explicit Euler, Implicit Euler, and Heun's method—on the linear test ODE $y'(t) = -\\lambda y(t)$ with $y(0) = y_0$. The exact solution is $y(t) = y_0 \\exp(-\\lambda t)$.\n\nFor a given method, the numerical solution after one time step $h$ can be written as $y_{n+1} = g(h\\lambda) y_n$, where $g(z)$ is the method's amplification factor and $z=h\\lambda$. We derive $g(z)$ for each method with $f(t,y) = -\\lambda y$:\n\n1.  **Explicit Euler**:\n    $y_{n+1} = y_n + h(-\\lambda y_n) = (1 - h\\lambda) y_n$.\n    The amplification factor is $g_{EE}(z) = 1 - z$.\n\n2.  **Implicit Euler**:\n    $y_{n+1} = y_n + h(-\\lambda y_{n+1}) \\implies y_{n+1}(1+h\\lambda) = y_n \\implies y_{n+1} = \\frac{1}{1+h\\lambda} y_n$.\n    The amplification factor is $g_{IE}(z) = \\frac{1}{1+z}$.\n\n3.  **Heun’s Method**:\n    $y_{n+1} = y_n + \\frac{h}{2} \\left( (-\\lambda y_n) + (-\\lambda(y_n + h(-\\lambda y_n))) \\right)$\n    $y_{n+1} = y_n + \\frac{h}{2} \\left( -\\lambda y_n - \\lambda y_n + h\\lambda^2 y_n \\right)$\n    $y_{n+1} = y_n \\left( 1 - h\\lambda + \\frac{h^2\\lambda^2}{2} \\right)$.\n    The amplification factor is $g_{HM}(z) = 1 - z + \\frac{z^2}{2}$.\n\nAfter $N$ steps to reach the final time $T=Nh$, the numerical solution is $y_N = y_0 \\cdot [g(h\\lambda)]^N$. This closed-form expression is more efficient and numerically robust than a step-by-step loop for this linear problem.\n\nThe core of the task is to find the observed order of accuracy, $p$. For a method of order $p$, the global error $E(h)$ at a fixed final time $T$ behaves as $E(h) \\approx C h^p$ for sufficiently small step size $h$. Given errors from two different step sizes, $h_1$ and $h_2$, we have:\n$$ \\frac{E(h_1)}{E(h_2)} \\approx \\frac{C h_1^p}{C h_2^p} = \\left(\\frac{h_1}{h_2}\\right)^p $$\nSolving for $p$ gives:\n$$ p \\approx \\frac{\\log(E(h_1)/E(h_2))}{\\log(h_1/h_2)} $$\nIn our case, we perform a sequence of halvings, so $h_1/h_2 = 2$. This simplifies to $p \\approx \\log_2(E(h_1)/E(h_2))$.\n\nThe procedure is as follows for each test case and method:\n1.  Given $(\\lambda, T, h_0)$ and $y_0 = 1$, calculate the base number of steps $N_0 = \\operatorname{round}(T/h_0)$.\n2.  Define three resolutions with step counts $N_1 = N_0$, $N_2 = 2N_0$, and $N_3 = 4N_0$.\n3.  For each resolution $k \\in \\{1,2,3\\}$, calculate the step size $h_k = T/N_k$ and the numerical solution $y_{N_k}$.\n4.  Compute the global error for each resolution: $E_k = |y_{N_k} - y(T)|$, where $y(T) = y_0 \\exp(-\\lambda T)$.\n5.  Calculate two estimates for the order of accuracy:\n    $p_1 = \\log_2(E_1/E_2)$\n    $p_2 = \\log_2(E_2/E_3)$\n6.  The final observed order is the average of these two estimates: $p_{obs} = (p_1 + p_2)/2$.\n\nThis procedure is applied to all twelve combinations of test cases and methods. The parameter sets are designed to test the methods from non-stiff to very stiff regimes. For stiff problems (large $\\lambda$), explicit methods like Forward Euler and Heun's can become unstable if the step size $h$ is too large (specifically, if $h\\lambda  2$). In such unstable cases, the error will grow as $h$ is reduced, leading to a negative observed order of accuracy. The implicit Euler method is A-stable and will remain stable for any $h  0$, correctly handling stiff equations, although its accuracy characteristics might deviate from the classical non-stiff theory. The computation will capture these behaviors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the observed order of accuracy for three numerical methods\n    on the linear test ODE y' = -lambda*y across four different parameter sets.\n    \"\"\"\n\n    def compute_numerical_solution(method_name, lam, y0, T, N):\n        \"\"\"\n        Calculates the numerical solution y(T) using the amplification factor.\n        \n        Args:\n            method_name (str): Identifier for the method ('EE', 'IE', 'HM').\n            lam (float): The lambda parameter of the ODE.\n            y0 (float): Initial condition y(0).\n            T (float): Final time.\n            N (int): Number of time steps.\n        \n        Returns:\n            float: The numerical solution at time T.\n        \"\"\"\n        h = T / N\n        z = h * lam\n        \n        if method_name == 'EE': # Explicit Euler\n            # Amplification factor: g(z) = 1 - z\n            # Solution: y_N = y_0 * (1 - h*lam)^N\n            amp_factor = 1.0 - z\n        elif method_name == 'IE': # Implicit Euler\n            # Amplification factor: g(z) = 1 / (1 + z)\n            # Solution: y_N = y_0 * (1 + h*lam)^(-N)\n            amp_factor = 1.0 / (1.0 + z)\n        elif method_name == 'HM': # Heun's Method\n            # Amplification factor: g(z) = 1 - z + z^2/2\n            # Solution: y_N = y_0 * (1 - h*lam + 0.5*(h*lam)^2)^N\n            amp_factor = 1.0 - z + 0.5 * z**2\n        else:\n            raise ValueError(\"Unknown method name\")\n        \n        # Using the power of the amplification factor can be unstable for large N\n        # with negative bases. We must handle complex numbers that arise from\n        # negative bases in the amplification factor for explicit methods.\n        # The result of y_N should be real.\n        if isinstance(amp_factor, complex) or amp_factor  0:\n            # y_N = y_0 * (g)^N. If g is negative, result is y_0 * |g|^N * (-1)^N\n            # We take the real part, which is y_0 * g^N\n            return y0 * (amp_factor**N).real\n        else:\n            return y0 * amp_factor**N\n\n    def calculate_observed_order(method_name, lam, y0, T, h0):\n        \"\"\"\n        Calculates the observed order of accuracy for a given method and parameters.\n        \n        Args:\n            method_name (str): Identifier for the method ('EE', 'IE', 'HM').\n            lam (float): The lambda parameter of the ODE.\n            y0 (float): Initial condition y(0).\n            T (float): Final time.\n            h0 (float): Base step size parameter.\n        \n        Returns:\n            float: The observed order of accuracy.\n        \"\"\"\n        N0 = int(round(T / h0))\n        Ns = [N0, 2 * N0, 4 * N0]\n        \n        y_exact = y0 * np.exp(-lam * T)\n        \n        errors = []\n        for N in Ns:\n            if N == 0:\n                # Avoid division by zero if T/h0 rounds to 0. Not expected for given test cases.\n                errors.append(np.inf)\n                continue\n            y_num = compute_numerical_solution(method_name, lam, y0, T, N)\n            error = np.abs(y_num - y_exact)\n            errors.append(error)\n            \n        E1, E2, E3 = errors\n\n        # Handle cases where error is zero to avoid division by zero or log(0)\n        if E2 == 0 or E3 == 0:\n            # If error becomes 0, this implies very fast convergence or machine precision limit.\n            # The order is effectively infinite, but the problem's formula would break down.\n            # We return a large number or NaN as a sentinel. For this problem, it is not expected.\n            return np.nan\n\n        p1 = np.log2(E1 / E2)\n        p2 = np.log2(E2 / E3)\n        \n        p_obs = (p1 + p2) / 2.0\n        return p_obs\n\n    y0 = 1.0 # Initial condition y(0)=1 for all cases.\n\n    # Test cases: (lambda, T, h0)\n    test_cases = [\n        (1.0, 1.0, 0.2),      # Case A: nonstiff\n        (20.0, 1.0, 0.1),     # Case B: moderately stiff\n        (100.0, 1.0, 0.015),  # Case C: stiff but stable\n        (500.0, 0.1, 0.05),   # Case D: very stiff, unstable explicit\n    ]\n    \n    method_names = ['EE', 'IE', 'HM']\n    \n    results = []\n    for lam, T, h0 in test_cases:\n        for method_name in method_names:\n            order = calculate_observed_order(method_name, lam, y0, T, h0)\n            results.append(round(order, 6))\n\n    # Format the final output string as per the requirements\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from pure numerical properties to physical realism, this exercise addresses a critical constraint in environmental modeling: positivity. Many state variables, such as pollutant concentrations, must remain non-negative to be physically meaningful. This practice guides you through the process of deriving a sharp time-step restriction for an explicit method to guarantee positivity, illustrating how physical constraints can impose stricter limits on $\\Delta t$ than stability alone .",
            "id": "3859476",
            "problem": "A single-grid-cell atmospheric mass balance is used to model a nonnegative concentration $y(t)$ of a chemically reactive pollutant whose emissions are inferred from spaceborne remote sensing observations. The temporal evolution is governed by the conservation of mass: the rate of change equals sources minus sinks. Specifically, assume\n$$\\frac{dy}{dt} = e(t) - \\lambda\\, y(t),$$\nwhere $e(t) \\ge 0$ represents an emission source term derived from remote sensing retrievals and $0  \\lambda$ is a constant first-order loss rate representing combined deposition and chemical removal. The time advancement is performed with a single-step explicit method that evaluates the right-hand side at the previous time level, with time steps $\\Delta t_n  0$. The concentration is known to be nonnegative at the current step, $y_n \\ge 0$, and emission estimates satisfy $e_n \\ge 0$.\n\nStarting from the conservation equation and the definition of the derivative, derive a sharp restriction on the explicit time step $\\Delta t_n$ in terms of $\\lambda$ that guarantees positivity preservation in the update, namely $y_{n+1} \\ge 0$ whenever $y_n \\ge 0$ and $e_n \\ge 0$. Additionally, derive a one-line positivity-preserving limiter strategy of the form $y_{n+1}^{\\mathrm{lim}} = y_n + \\phi\\,\\Delta t_n\\,f(y_n)$ for a general explicit update $y_{n+1} = y_n + \\Delta t_n f(y_n)$ that ensures $y_{n+1}^{\\mathrm{lim}} \\ge 0$ when $y_n \\ge 0$, without requiring reduction of $\\Delta t_n$. You must justify the sharpness of the time-step restriction.\n\nFinally, evaluate the maximum allowable time step $\\Delta t_{\\max}$ for $\\lambda = 1.25 \\times 10^{-5}\\,\\mathrm{s}^{-1}$. Express your final numeric answer in seconds and round to four significant figures.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard mass-balance equation from environmental modeling and asks for a rigorous derivation of a positivity constraint on a numerical time-stepping scheme and the formulation of a positivity-preserving limiter, followed by a numerical calculation. All provided information is self-contained and consistent. Therefore, the problem is valid, and I shall proceed with the solution.\n\nThe temporal evolution of the pollutant concentration $y(t)$ is given by the linear first-order ordinary differential equation:\n$$\n\\frac{dy}{dt} = e(t) - \\lambda y(t)\n$$\nwith the constraints $y(t) \\ge 0$, $e(t) \\ge 0$, and $\\lambda  0$.\n\nThe numerical scheme is a single-step explicit method, which evaluates the right-hand side, $f(t, y) = e(t) - \\lambda y(t)$, at the previous time level $t_n$. This corresponds to the Forward or Explicit Euler method. The discrete update rule is:\n$$\n\\frac{y_{n+1} - y_n}{\\Delta t_n} = e_n - \\lambda y_n\n$$\nwhere $y_n \\equiv y(t_n)$, $e_n \\equiv e(t_n)$, and $\\Delta t_n = t_{n+1} - t_n  0$. Rearranging for $y_{n+1}$, we obtain:\n$$\ny_{n+1} = y_n + \\Delta t_n (e_n - \\lambda y_n)\n$$\nWe can collect the terms involving $y_n$:\n$$\ny_{n+1} = y_n(1 - \\lambda \\Delta t_n) + e_n \\Delta t_n\n$$\nWe are given that at the current step, the concentration $y_n \\ge 0$ and the emission term $e_n \\ge 0$. The time step $\\Delta t_n$ is also positive. Thus, the term $e_n \\Delta t_n$ is always non-negative.\n\n**Part 1: Derivation of the Time-Step Restriction**\n\nTo guarantee positivity preservation, we must ensure that $y_{n+1} \\ge 0$ for any valid inputs ($y_n \\ge 0, e_n \\ge 0$). Since $e_n \\Delta t_n \\ge 0$, the condition $y_{n+1} \\ge 0$ is guaranteed if the term $y_n(1 - \\lambda \\Delta t_n)$ is also non-negative. As we must guarantee this for any $y_n \\ge 0$, the coefficient of $y_n$ must be non-negative.\n$$\n1 - \\lambda \\Delta t_n \\ge 0\n$$\nSolving for $\\Delta t_n$:\n$$\n1 \\ge \\lambda \\Delta t_n \\implies \\Delta t_n \\le \\frac{1}{\\lambda}\n$$\nThis is the restriction on the time step required to unconditionally guarantee $y_{n+1} \\ge 0$.\n\nTo demonstrate that this restriction is sharp, we must show that if the condition is violated, there exists at least one set of valid inputs for which $y_{n+1}  0$. Let's assume $\\Delta t_n  1/\\lambda$. We can write $\\Delta t_n = (1+\\epsilon)/\\lambda$ for some small positive number $\\epsilon  0$. Substituting this into the update equation:\n$$\ny_{n+1} = y_n\\left(1 - \\lambda \\frac{1+\\epsilon}{\\lambda}\\right) + e_n \\Delta t_n = y_n(1 - (1+\\epsilon)) + e_n \\Delta t_n = - \\epsilon y_n + e_n \\Delta t_n\n$$\nNow, consider a physically plausible scenario with no emissions, $e_n = 0$, and a non-zero initial concentration, $y_n  0$. In this case, the equation for $y_{n+1}$ simplifies to:\n$$\ny_{n+1} = - \\epsilon y_n\n$$\nSince $\\epsilon  0$ and $y_n  0$, it is clear that $y_{n+1}  0$. This violates the physical requirement of non-negative concentration. Therefore, the condition $\\Delta t_n \\le 1/\\lambda$ is not merely sufficient but also necessary to guarantee positivity for all possible valid conditions, establishing its sharpness.\n\n**Part 2: Derivation of the Positivity-Preserving Limiter Strategy**\n\nWe are asked to derive a limiter for a general explicit update $y_{n+1} = y_n + \\Delta t_n f(y_n)$ that enforces $y_{n+1}^{\\mathrm{lim}} \\ge 0$ when $y_n \\ge 0$. The limiter must be of the form $y_{n+1}^{\\mathrm{lim}} = y_n + \\phi\\,\\Delta t_n\\,f(y_n)$.\n\nThe goal is to find a scaling factor $\\phi$ that modifies the update step $\\Delta y_n = \\Delta t_n f(y_n)$ to prevent $y_{n+1}$ from becoming negative. We require $y_{n+1}^{\\mathrm{lim}} \\ge 0$, which means:\n$$\ny_n + \\phi \\Delta t_n f(y_n) \\ge 0\n$$\nWe analyze two cases based on the sign of the tendency term $f(y_n)$:\n\nCase 1: $f(y_n) \\ge 0$.\nThe concentration is non-decreasing. Since $y_n \\ge 0$ and $\\Delta t_n  0$, the un-limited update $y_{n+1} = y_n + \\Delta t_n f(y_n)$ will always be greater than or equal to $y_n$, thus remaining non-negative. No limitation is necessary. To maintain the accuracy of the original scheme, we should not alter the update. This is achieved by setting $\\phi = 1$.\n\nCase 2: $f(y_n)  0$.\nThe concentration is decreasing, and $y_{n+1}$ may become negative if the time step is too large. We must enforce $y_n + \\phi \\Delta t_n f(y_n) \\ge 0$. Rearranging for $\\phi$:\n$$\n\\phi \\Delta t_n f(y_n) \\ge -y_n\n$$\nSince $\\Delta t_n  0$ and $f(y_n)  0$, the term $\\Delta t_n f(y_n)$ is negative. Dividing by this negative quantity reverses the inequality sign:\n$$\n\\phi \\le \\frac{-y_n}{\\Delta t_n f(y_n)}\n$$\nLet's define the ratio $r = \\frac{-y_n}{\\Delta t_n f(y_n)}$. Note that since $y_n \\ge 0$ and the denominator is negative, $r \\ge 0$. The condition for positivity is $\\phi \\le r$. To stay as close as possible to the original, unlimited scheme (where $\\phi = 1$), we should choose the largest possible value for $\\phi$ that satisfies this constraint.\n\nIf $r \\ge 1$, this implies $\\frac{-y_n}{\\Delta t_n f(y_n)} \\ge 1$, which is equivalent to $-y_n \\ge \\Delta t_n f(y_n)$ or $y_n + \\Delta t_n f(y_n) \\ge 0$. The un-limited step already preserves positivity. In this situation, we can choose $\\phi = 1$, which satisfies $\\phi \\le r$.\n\nIf $0 \\le r  1$, the un-limited step would result in a negative concentration. To guarantee positivity while making the smallest possible modification to the update, we must choose the largest allowable value for $\\phi$, which is $\\phi = r$. This choice leads to $y_{n+1}^{\\mathrm{lim}} = y_n + r (\\Delta t_n f(y_n)) = y_n + \\frac{-y_n}{\\Delta t_n f(y_n)} (\\Delta t_n f(y_n)) = y_n - y_n = 0$.\n\nCombining these sub-cases for $f(y_n)  0$, the optimal choice for $\\phi$ that preserves accuracy as much as possible is $\\phi = \\min(1, r)$.\n\nTherefore, the complete \"one-line\" positivity-preserving limiter strategy is $y_{n+1}^{\\mathrm{lim}} = y_n + \\phi\\,\\Delta t_n\\,f(y_n)$, where the factor $\\phi$ is defined as follows:\n$$\n\\phi = \\begin{cases} 1  \\text{if } f(y_n) \\ge 0 \\\\ \\min\\left(1, \\frac{-y_n}{\\Delta t_n f(y_n)}\\right)  \\text{if } f(y_n)  0 \\end{cases}\n$$\nThis strategy ensures $y_{n+1}^{\\mathrm{lim}} \\ge 0$ for any time step $\\Delta t_n  0$, without requiring its reduction.\n\n**Part 3: Numerical Evaluation**\n\nFinally, we are asked to evaluate the maximum allowable time step, $\\Delta t_{\\max}$, that guarantees positivity without a limiter. From Part 1, this is given by the sharp bound:\n$$\n\\Delta t_{\\max} = \\frac{1}{\\lambda}\n$$\nGiven the loss rate constant $\\lambda = 1.25 \\times 10^{-5}\\,\\mathrm{s}^{-1}$:\n$$\n\\Delta t_{\\max} = \\frac{1}{1.25 \\times 10^{-5}\\,\\mathrm{s}^{-1}} = \\frac{1}{1.25} \\times 10^5\\,\\mathrm{s}\n$$\nSince $1.25 = 5/4$, we have $1/1.25 = 4/5 = 0.8$.\n$$\n\\Delta t_{\\max} = 0.8 \\times 10^5\\,\\mathrm{s} = 80000\\,\\mathrm{s}\n$$\nThe problem requires the answer to be rounded to four significant figures.\n$$\n\\Delta t_{\\max} = 8.000 \\times 10^4\\,\\mathrm{s}\n$$",
            "answer": "$$\\boxed{8.000 \\times 10^4}$$"
        },
        {
            "introduction": "This final practice tackles a sophisticated challenge at the intersection of modeling and remote sensing: accounting for measurement noise. Real-world satellite data is not perfect, and representing this uncertainty often requires moving from deterministic to stochastic differential equations (SDEs). In this exercise, you will discretize an Ornstein-Uhlenbeck process, a common model for correlated noise, and determine the time step needed to accurately resolve its statistical properties, providing essential skills for data assimilation and uncertainty quantification .",
            "id": "3859431",
            "problem": "A remote sensing instrument onboard a satellite produces soil moisture retrievals contaminated by correlated measurement noise. In environmental modeling, a physically defensible noise model is the Ornstein–Uhlenbeck (OU) process driven by a Wiener process (standard Brownian motion). Let the measurement noise be denoted by $\\,\\eta(t)\\,$ with units of volumetric water content $\\,\\text{m}^3/\\text{m}^3\\,$, and let its correlation time be $\\,\\tau\\,$ (in seconds) and its stationary standard deviation be $\\,\\sigma\\,$ (in $\\,\\text{m}^3/\\text{m}^3\\,$). The OU process is defined by the Stochastic Differential Equation (SDE)\n$$\n\\mathrm{d}\\eta(t) \\;=\\; -\\frac{1}{\\tau}\\,\\eta(t)\\,\\mathrm{d}t \\;+\\; \\sqrt{\\frac{2\\,\\sigma^2}{\\tau}}\\,\\mathrm{d}W_t,\n$$\nwhere $\\,W_t\\,$ is a standard Wiener process.\n\nYou are tasked to perform temporal discretization and time stepping of this SDE using the Euler–Maruyama method and to determine the time step size $\\,\\Delta t\\,$ (in seconds) required to resolve the noise-driven variability in the sense that the discrete-time approximation attains a stationary variance within a prescribed relative error tolerance $\\,\\epsilon\\,$ (expressed as a decimal) compared to the continuous-time stationary variance. Additionally, to ensure adequate temporal resolution of the correlation structure, you must enforce that at least $\\,N_c\\,$ samples fall within one correlation time $\\,\\tau\\,$, i.e., you must constrain $\\,\\Delta t\\,$ so that $\\,\\Delta t \\le \\tau / N_c\\,$. Starting only from the OU SDE given above, the definition of the Euler–Maruyama discretization, and the properties of autoregressive processes, derive from first principles a quantitative bound on $\\,\\Delta t\\,$ that guarantees the stationary variance of the Euler–Maruyama scheme lies within the relative error tolerance $\\,\\epsilon\\,$ of $\\,\\sigma^2\\,$, and simultaneously meets the sampling constraint $\\,\\Delta t \\le \\tau / N_c\\,$. Your final step size must be the smallest $\\,\\Delta t\\,$ that satisfies both constraints.\n\nYour program must implement this derivation as a function that, for each test case, computes the required $\\,\\Delta t\\,$ in seconds and returns it as a floating-point number. Express each final answer in seconds, rounded to $\\,6\\,$ decimal places.\n\nTest Suite:\n- Case $\\,1\\,$ (typical orbital pass): $\\sigma = 0.1\\,\\text{m}^3/\\text{m}^3$, $\\tau = 7200\\,\\text{s}$, $\\epsilon = 0.05$, $N_c = 10$.\n- Case $\\,2\\,$ (short correlation, loose variance tolerance): $\\sigma = 0.2\\,\\text{m}^3/\\text{m}^3$, $\\tau = 3600\\,\\text{s}$, $\\epsilon = 0.9$, $N_c = 3$.\n- Case $\\,3\\,$ (near-white noise, tight variance tolerance): $\\sigma = 0.05\\,\\text{m}^3/\\text{m}^3$, $\\tau = 60\\,\\text{s}$, $\\epsilon = 0.01$, $N_c = 20$.\n- Case $\\,4\\,$ (long correlation, very tight variance tolerance): $\\sigma = 0.15\\,\\text{m}^3/\\text{m}^3$, $\\tau = 86400\\,\\text{s}$, $\\epsilon = 10^{-4}$, $N_c = 50$.\n- Case $\\,5\\,$ (very short correlation, moderate tolerance, minimal sampling): $\\sigma = 0.3\\,\\text{m}^3/\\text{m}^3$, $\\tau = 1\\,\\text{s}$, $\\epsilon = 0.5$, $N_c = 1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above (e.g., $[\\Delta t_1,\\Delta t_2,\\Delta t_3,\\Delta t_4,\\Delta t_5]$), where each $\\,\\Delta t_i\\,$ is in seconds, rounded to $\\,6\\,$ decimal places.",
            "solution": "The problem requires the derivation of a time step size, $\\Delta t$, for the temporal discretization of an Ornstein–Uhlenbeck (OU) process. The derived $\\Delta t$ must satisfy two distinct constraints: one related to the accuracy of the stationary variance of the numerical approximation, and another related to the temporal sampling resolution of the process's correlation structure. The final step size will be the largest value that satisfies both conditions simultaneously.\n\nThe governing Stochastic Differential Equation (SDE) for the measurement noise $\\eta(t)$ is given as:\n$$\n\\mathrm{d}\\eta(t) = -\\frac{1}{\\tau}\\eta(t)\\mathrm{d}t + \\sqrt{\\frac{2\\sigma^2}{\\tau}}\\mathrm{d}W_t\n$$\nwhere $\\tau$ is the correlation time, $\\sigma^2$ is the stationary variance of the continuous-time process, and $W_t$ is a standard Wiener process.\n\nOur derivation proceeds in the following steps:\n1.  Discretize the SDE using the Euler–Maruyama method to obtain a discrete-time representation of the process.\n2.  Calculate the stationary variance of this discrete-time process.\n3.  Derive an upper bound on $\\Delta t$ from the constraint on the relative error of the stationary variance.\n4.  Derive an upper bound on $\\Delta t$ from the constraint on the number of samples per correlation time.\n5.  Combine these two bounds to determine the final required time step size.\n\n**Step 1: Euler–Maruyama Discretization**\n\nThe Euler–Maruyama method approximates the SDE over a small time interval $\\Delta t = t_{n+1} - t_n$. The general form of the method for an SDE $\\mathrm{d}X_t = a(X_t, t)\\mathrm{d}t + b(X_t, t)\\mathrm{d}W_t$ is $X_{n+1} = X_n + a(X_n, t_n)\\Delta t + b(X_n, t_n)\\Delta W_n$.\n\nFor the given OU process, the drift term is $a(\\eta, t) = -\\frac{1}{\\tau}\\eta(t)$ and the diffusion term is $b(\\eta, t) = \\sqrt{\\frac{2\\sigma^2}{\\tau}}$. Applying the discretization yields:\n$$\n\\eta_{n+1} = \\eta_n - \\frac{1}{\\tau}\\eta_n\\Delta t + \\sqrt{\\frac{2\\sigma^2}{\\tau}}\\Delta W_n\n$$\nwhere $\\eta_n$ is the approximation of $\\eta(t_n)$ and $\\Delta W_n = W_{t_{n+1}} - W_{t_n}$ is an increment of the Wiener process. These increments are independent, identically distributed normal random variables with mean $0$ and variance $\\Delta t$, i.e., $\\Delta W_n \\sim \\mathcal{N}(0, \\Delta t)$. We can express $\\Delta W_n$ as $\\sqrt{\\Delta t}Z_n$, where $Z_n$ is a standard normal random variable, $Z_n \\sim \\mathcal{N}(0, 1)$.\n\nSubstituting this into the discretized equation:\n$$\n\\eta_{n+1} = \\eta_n - \\frac{\\Delta t}{\\tau}\\eta_n + \\sqrt{\\frac{2\\sigma^2}{\\tau}}\\sqrt{\\Delta t}Z_n\n$$\n$$\n\\eta_{n+1} = \\left(1 - \\frac{\\Delta t}{\\tau}\\right)\\eta_n + \\sqrt{\\frac{2\\sigma^2\\Delta t}{\\tau}}Z_n\n$$\nThis equation is in the form of a first-order autoregressive process, AR($1$): $\\eta_{n+1} = \\phi\\eta_n + \\varepsilon_n$, where the autoregressive coefficient is $\\phi = 1 - \\frac{\\Delta t}{\\tau}$ and the innovation term is $\\varepsilon_n = \\sqrt{\\frac{2\\sigma^2\\Delta t}{\\tau}}Z_n$.\n\n**Step 2: Stationary Variance of the Discrete-Time Process**\n\nAn AR($1$) process is stationary if and only if $|\\phi|  1$. In our case, this means $|1 - \\frac{\\Delta t}{\\tau}|  1$. Since $\\Delta t  0$ and $\\tau  0$, we have $1 - \\frac{\\Delta t}{\\tau}  1$. The other part of the inequality requires $1 - \\frac{\\Delta t}{\\tau}  -1$, which implies $2  \\frac{\\Delta t}{\\tau}$, or $\\Delta t  2\\tau$. This is the stability condition for the numerical scheme.\n\nThe variance of the innovation term is:\n$$\n\\text{Var}(\\varepsilon_n) = \\text{E}[\\varepsilon_n^2] - (\\text{E}[\\varepsilon_n])^2 = \\text{E}\\left[\\left(\\sqrt{\\frac{2\\sigma^2\\Delta t}{\\tau}}Z_n\\right)^2\\right] - 0^2 = \\frac{2\\sigma^2\\Delta t}{\\tau}\\text{E}[Z_n^2]\n$$\nSince $\\text{E}[Z_n^2] = \\text{Var}(Z_n) + (\\text{E}[Z_n])^2 = 1 + 0^2 = 1$, the innovation variance is $\\text{Var}(\\varepsilon_n) = \\frac{2\\sigma^2\\Delta t}{\\tau}$.\n\nThe stationary variance of a stationary AR($1$) process, which we denote as $\\sigma_{\\text{discrete}}^2$, is given by the formula $\\sigma_{\\text{discrete}}^2 = \\frac{\\text{Var}(\\varepsilon_n)}{1-\\phi^2}$. Substituting our expressions for $\\phi$ and $\\text{Var}(\\varepsilon_n)$:\n$$\n\\sigma_{\\text{discrete}}^2 = \\frac{\\frac{2\\sigma^2\\Delta t}{\\tau}}{1 - \\left(1 - \\frac{\\Delta t}{\\tau}\\right)^2}\n$$\nExpanding the denominator:\n$$\n1 - \\left(1 - \\frac{2\\Delta t}{\\tau} + \\frac{\\Delta t^2}{\\tau^2}\\right) = \\frac{2\\Delta t}{\\tau} - \\frac{\\Delta t^2}{\\tau^2} = \\frac{\\Delta t}{\\tau}\\left(2 - \\frac{\\Delta t}{\\tau}\\right)\n$$\nSubstituting this back into the expression for the variance:\n$$\n\\sigma_{\\text{discrete}}^2 = \\frac{\\frac{2\\sigma^2\\Delta t}{\\tau}}{\\frac{\\Delta t}{\\tau}\\left(2 - \\frac{\\Delta t}{\\tau}\\right)} = \\frac{2\\sigma^2}{2 - \\frac{\\Delta t}{\\tau}} = \\sigma^2 \\left(\\frac{1}{1 - \\frac{\\Delta t}{2\\tau}}\\right)\n$$\n\n**Step 3: Deriving the Variance Constraint Bound**\n\nThe problem requires that the stationary variance of the discrete-time approximation lies within a prescribed relative error tolerance $\\epsilon$ of the continuous-time stationary variance $\\sigma^2$. This is expressed as:\n$$\n\\frac{|\\sigma_{\\text{discrete}}^2 - \\sigma^2|}{\\sigma^2} \\le \\epsilon\n$$\nSubstituting the derived expression for $\\sigma_{\\text{discrete}}^2$:\n$$\n\\frac{\\left| \\sigma^2 \\left(\\frac{1}{1 - \\frac{\\Delta t}{2\\tau}}\\right) - \\sigma^2 \\right|}{\\sigma^2} \\le \\epsilon\n$$\nThe $\\sigma^2$ terms cancel, leaving:\n$$\n\\left| \\frac{1}{1 - \\frac{\\Delta t}{2\\tau}} - 1 \\right| \\le \\epsilon\n$$\nCombining the terms inside the absolute value:\n$$\n\\left| \\frac{1 - \\left(1 - \\frac{\\Delta t}{2\\tau}\\right)}{1 - \\frac{\\Delta t}{2\\tau}} \\right| = \\left| \\frac{\\frac{\\Delta t}{2\\tau}}{1 - \\frac{\\Delta t}{2\\tau}} \\right| \\le \\epsilon\n$$\nFor the scheme to be stable, $\\Delta t  2\\tau$, which ensures the denominator $1 - \\frac{\\Delta t}{2\\tau}$ is positive. Since $\\Delta t$, $\\tau$, and $2$ are also positive, the entire expression inside the absolute value is positive. We can therefore drop the absolute value signs:\n$$\n\\frac{\\frac{\\Delta t}{2\\tau}}{1 - \\frac{\\Delta t}{2\\tau}} \\le \\epsilon\n$$\nNow, we solve for $\\Delta t$:\n$$\n\\frac{\\Delta t}{2\\tau} \\le \\epsilon \\left(1 - \\frac{\\Delta t}{2\\tau}\\right)\n$$\n$$\n\\frac{\\Delta t}{2\\tau} \\le \\epsilon - \\epsilon \\frac{\\Delta t}{2\\tau}\n$$\n$$\n\\frac{\\Delta t}{2\\tau} \\left(1 + \\epsilon\\right) \\le \\epsilon\n$$\n$$\n\\frac{\\Delta t}{2\\tau} \\le \\frac{\\epsilon}{1 + \\epsilon}\n$$\nThis gives the first upper bound on $\\Delta t$, which we denote $\\Delta t_{\\text{variance}}$:\n$$\n\\Delta t \\le \\frac{2\\tau\\epsilon}{1 + \\epsilon} \\equiv \\Delta t_{\\text{variance}}\n$$\n\n**Step 4: Deriving the Sampling Constraint Bound**\n\nThe second constraint requires that at least $N_c$ samples fall within one correlation time $\\tau$. The number of samples in an interval of length $\\tau$ is $\\tau / \\Delta t$. Thus, the constraint is:\n$$\n\\frac{\\tau}{\\Delta t} \\ge N_c\n$$\nSolving for $\\Delta t$ gives the second upper bound, which we denote $\\Delta t_{\\text{sampling}}$:\n$$\n\\Delta t \\le \\frac{\\tau}{N_c} \\equiv \\Delta t_{\\text{sampling}}\n$$\n\n**Step 5: Final Time Step Calculation**\n\nTo satisfy both constraints simultaneously, the chosen time step $\\Delta t$ must be less than or equal to both upper bounds. The problem asks for the single required time step, which is logically interpreted as the largest possible value of $\\Delta t$ that meets all conditions. This value is the minimum of the two derived bounds:\n$$\n\\Delta t_{\\text{final}} = \\min(\\Delta t_{\\text{variance}}, \\Delta t_{\\text{sampling}})\n$$\n$$\n\\Delta t_{\\text{final}} = \\min\\left(\\frac{2\\tau\\epsilon}{1 + \\epsilon}, \\frac{\\tau}{N_c}\\right)\n$$\nThis final expression provides the required time step size as a function of the given parameters $\\tau$, $\\epsilon$, and $N_c$. Note that the standard deviation $\\sigma$ does not appear in the final formula, as it cancels out during the relative error calculation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the required time step size for discretizing an Ornstein-Uhlenbeck\n    process based on variance accuracy and temporal sampling constraints.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (sigma, tau, epsilon, Nc)\n    # sigma is in m^3/m^3, tau is in seconds, epsilon is dimensionless, Nc is dimensionless.\n    test_cases = [\n        # Case 1 (typical orbital pass)\n        (0.1, 7200.0, 0.05, 10.0),\n        # Case 2 (short correlation, loose variance tolerance)\n        (0.2, 3600.0, 0.9, 3.0),\n        # Case 3 (near-white noise, tight variance tolerance)\n        (0.05, 60.0, 0.01, 20.0),\n        # Case 4 (long correlation, very tight variance tolerance)\n        (0.15, 86400.0, 1e-4, 50.0),\n        # Case 5 (very short correlation, moderate tolerance, minimal sampling)\n        (0.3, 1.0, 0.5, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        sigma_val, tau, epsilon, Nc = case\n\n        # Constraint 1: The stationary variance of the Euler-Maruyama scheme\n        # must be within a relative error tolerance 'epsilon' of the true variance.\n        # This leads to the bound: dt = (2 * tau * epsilon) / (1 + epsilon)\n        dt_variance = (2.0 * tau * epsilon) / (1.0 + epsilon)\n\n        # Constraint 2: At least 'Nc' samples must fall within one correlation time 'tau'.\n        # This leads to the bound: dt = tau / Nc\n        dt_sampling = tau / Nc\n\n        # The final time step must satisfy both constraints. Therefore, it must be\n        # less than or equal to the minimum of the two bounds. We choose the largest\n        # possible step size, which is the minimum of these two upper bounds.\n        delta_t = min(dt_variance, dt_sampling)\n        \n        results.append(delta_t)\n\n    # Format the results to 6 decimal places and create the final output string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}