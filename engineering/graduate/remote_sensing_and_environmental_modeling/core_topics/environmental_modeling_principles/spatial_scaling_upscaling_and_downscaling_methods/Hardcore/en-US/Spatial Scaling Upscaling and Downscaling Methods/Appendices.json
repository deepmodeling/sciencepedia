{
    "hands_on_practices": [
        {
            "introduction": "Aggregating fine-resolution raster data to coarser, often irregular, polygonal units is a fundamental task in environmental analysis. The core principle guiding this process is conservation: the total amount of a quantity (like mass or population) should not change simply because we changed its spatial representation. This practice  challenges you to implement and contrast a naive centroid-based method with a rigorous overlay method, providing a concrete understanding of why conservative remapping is crucial and how to quantify the errors, or \"leakage,\" introduced by simpler approaches.",
            "id": "3851372",
            "problem": "Consider a spatial scaling scenario common in remote sensing and environmental modeling where a physical field is represented on a fine raster grid and aggregated to coarse, non-aligned polygons. Let the rectangular spatial domain be $\\Omega = [0,L_x] \\times [0,L_y]$ in meters, and let the fine raster be a uniform grid of $N_x \\times N_y$ axis-aligned rectangular cells that partition $\\Omega$. Assume a deterministic, continuous areal density field $\\rho(x,y)$, in kilograms per square meter, defined by $\\rho(x,y) = \\alpha + \\beta x + \\gamma y$, with $\\alpha$ in kilograms per square meter, and $\\beta$ and $\\gamma$ in kilograms per cubic meter, so that $\\beta x$ and $\\gamma y$ have units of kilograms per square meter.\n\nThe conservation principle for change of support states that the mass over any measurable subset $S \\subseteq \\Omega$ is \n$$\nM(S) = \\iint_S \\rho(x,y)\\, \\mathrm{d}x\\, \\mathrm{d}y.\n$$\nCoarse supports are given as a list of axis-aligned rectangular polygons $P_1,\\dots,P_m$, each specified by its $x$-interval $[a_x^{(p)},b_x^{(p)}]$ and $y$-interval $[a_y^{(p)},b_y^{(p)}]$ in meters. Define the following two upscaling methods:\n\n1. A centroid-assignment method: for each fine cell $C$, compute its centroid $(x_c,y_c)$ and assign the entire cell mass to the first polygon $P_p$ whose interior contains $(x_c,y_c)$ (if none contain the centroid, the cell contributes zero to all polygons). Denote the aggregated mass for polygon $P_p$ by $A_p^{\\mathrm{cen}}$.\n\n2. A corrective exact-overlay method: for each fine cell $C$ and polygon $P_p$, compute the rectangular intersection $I = C \\cap P_p$. If $I$ has positive area, add the exact integral $\\iint_I \\rho(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y$ to the polygon total. Denote the aggregated mass for polygon $P_p$ by $A_p^{\\mathrm{exact}}$.\n\nDefine the polygon truth $T_p = M(P_p)$ using the conservation principle. Quantify leakage using both a redistribution error magnitude and an aggregate non-conservation metric:\n- Redistribution error magnitude:\n$$\nE(A) = \\sum_{p=1}^m \\left| A_p - T_p \\right|,\n$$\nwhere $A_p$ is either $A_p^{\\mathrm{cen}}$ or $A_p^{\\mathrm{exact}}$.\n- Aggregate non-conservation across polygons:\n$$\nC(A) = \\sum_{p=1}^m A_p - \\sum_{p=1}^m T_p.\n$$\nAll masses and leakage metrics must be expressed in kilograms.\n\nYour task is to implement a program that, for each test case below, constructs the fine grid, computes the exact mass per cell from the given $\\rho(x,y)$ by integrating over each cell, upscales to polygons using both methods, computes $T_p$ for each polygon from first principles, and reports $E(A)$ and $C(A)$ for both methods.\n\nTest suite (four cases):\n- Case 1 (misaligned partition):\n    - $L_x = 1000$, $L_y = 1000$ meters; $N_x = 10$, $N_y = 10$.\n    - $\\alpha = 2$ kilograms per square meter, $\\beta = 0.001$ kilograms per cubic meter, $\\gamma = -0.0005$ kilograms per cubic meter.\n    - Polygons: $P_1 = [0,430] \\times [0,1000]$, $P_2 = [430,1000] \\times [0,1000]$.\n- Case 2 (aligned partition):\n    - $L_x = 1000$, $L_y = 1000$ meters; $N_x = 10$, $N_y = 10$.\n    - $\\alpha = 2$ kilograms per square meter, $\\beta = 0.001$ kilograms per cubic meter, $\\gamma = -0.0005$ kilograms per cubic meter.\n    - Polygons: $P_1 = [0,500] \\times [0,1000]$, $P_2 = [500,1000] \\times [0,1000]$.\n- Case 3 (overlap):\n    - $L_x = 1000$, $L_y = 1000$ meters; $N_x = 10$, $N_y = 10$.\n    - $\\alpha = 2$ kilograms per square meter, $\\beta = 0.001$ kilograms per cubic meter, $\\gamma = -0.0005$ kilograms per cubic meter.\n    - Polygons: $P_1 = [200,700] \\times [200,700]$, $P_2 = [500,900] \\times [100,600]$.\n- Case 4 (gap):\n    - $L_x = 1000$, $L_y = 1000$ meters; $N_x = 10$, $N_y = 10$.\n    - $\\alpha = 2$ kilograms per square meter, $\\beta = 0.001$ kilograms per cubic meter, $\\gamma = -0.0005$ kilograms per cubic meter.\n    - Polygons: $P_1 = [0,400] \\times [0,1000]$, $P_2 = [600,1000] \\times [0,1000]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a four-element list $[E(A^{\\mathrm{cen}}),C(A^{\\mathrm{cen}}),E(A^{\\mathrm{exact}}),C(A^{\\mathrm{exact}})]$, all in kilograms, as a list of lists. For example, the final output format must be $[[e_1,c_1,e'_1,c'_1],[e_2,c_2,e'_2,c'_2],[e_3,c_3,e'_3,c'_3],[e_4,c_4,e'_4,c'_4]]$ where each $e_i$, $c_i$, $e'_i$, $c'_i$ is a real number in kilograms.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in integral calculus and principles of mass conservation, well-posed with complete and consistent data, and objectively formulated. The task requires the implementation and comparison of two spatial upscaling methods, a common problem in remote sensing and environmental modeling. We will proceed with a full solution.\n\nThe core of the problem involves calculating the mass over various rectangular domains by integrating a given areal density field $\\rho(x,y)$. The density field is a linear function of the spatial coordinates:\n$$\n\\rho(x,y) = \\alpha + \\beta x + \\gamma y\n$$\nwhere $\\alpha$ is in kg/m², and $\\beta, \\gamma$ are in kg/m³, ensuring dimensional consistency.\n\nA key step is to derive a general formula for the integral of $\\rho(x,y)$ over an arbitrary axis-aligned rectangular region $R = [x_1, x_2] \\times [y_1, y_2]$. The mass $M(R)$ is given by:\n$$\nM(R) = \\iint_R \\rho(x,y) \\, \\mathrm{d}x\\mathrm{d}y = \\int_{y_1}^{y_2} \\int_{x_1}^{x_2} (\\alpha + \\beta x + \\gamma y) \\, \\mathrm{d}x\\mathrm{d}y\n$$\nPerforming the integration first with respect to $x$:\n$$\n\\int_{x_1}^{x_2} (\\alpha + \\beta x + \\gamma y) \\, \\mathrm{d}x = \\left[ \\alpha x + \\frac{\\beta}{2} x^2 + \\gamma y x \\right]_{x_1}^{x_2} = \\alpha(x_2 - x_1) + \\frac{\\beta}{2}(x_2^2 - x_1^2) + \\gamma y(x_2 - x_1)\n$$\nNow, integrating the result with respect to $y$:\n$$\n\\int_{y_1}^{y_2} \\left( \\alpha(x_2 - x_1) + \\frac{\\beta}{2}(x_2^2 - x_1^2) + \\gamma y(x_2 - x_1) \\right) \\, \\mathrm{d}y = \\left[ \\alpha(x_2 - x_1)y + \\frac{\\beta}{2}(x_2^2 - x_1^2)y + \\frac{\\gamma}{2}(x_2 - x_1)y^2 \\right]_{y_1}^{y_2}\n$$\n$$\nM(R) = \\alpha(x_2 - x_1)(y_2 - y_1) + \\frac{\\beta}{2}(x_2^2 - x_1^2)(y_2 - y_1) + \\frac{\\gamma}{2}(x_2 - x_1)(y_2^2 - y_1^2)\n$$\nThis expression can be simplified by introducing the centroid of the rectangle, $(\\bar{x}, \\bar{y}) = \\left(\\frac{x_1+x_2}{2}, \\frac{y_1+y_2}{2}\\right)$, and its area, $A_R = (x_2 - x_1)(y_2 - y_1)$. Using the identities $x_2^2 - x_1^2 = (x_2 - x_1)(x_2 + x_1) = (x_2 - x_1)(2\\bar{x})$ and $y_2^2 - y_1^2 = (y_2 - y_1)(2\\bar{y})$, the expression becomes:\n$$\nM(R) = \\alpha A_R + \\frac{\\beta}{2}(2\\bar{x}(x_2-x_1))(y_2 - y_1) + \\frac{\\gamma}{2}(x_2 - x_1)(2\\bar{y}(y_2-y_1))\n$$\n$$\nM(R) = \\alpha A_R + \\beta \\bar{x} A_R + \\gamma \\bar{y} A_R = (\\alpha + \\beta \\bar{x} + \\gamma \\bar{y}) A_R = \\rho(\\bar{x}, \\bar{y}) A_R\n$$\nThis is a notable result: for a linear density field, the exact integral over a rectangle is equal to the density evaluated at the rectangle's centroid multiplied by its area. This simplifies all mass calculations.\n\nThe solution proceeds as follows for each test case:\n1.  **Grid and Parameter Definition**: The domain $\\Omega = [0, L_x] \\times [0, L_y]$ is partitioned into an $N_x \\times N_y$ grid of fine cells. The cell dimensions are $\\Delta x_{cell} = L_x/N_x$ and $\\Delta y_{cell} = L_y/N_y$. The parameters $\\alpha, \\beta, \\gamma$ and the list of coarse polygons $\\{P_p\\}$ are defined.\n\n2.  **True Polygon Mass Calculation ($T_p$)**: The true mass for each polygon $P_p = [a_x^{(p)},b_x^{(p)}] \\times [a_y^{(p)},b_y^{(p)}]$ is calculated directly using the derived integral formula:\n    $$\n    T_p = M(P_p) = \\rho(\\bar{x}_p, \\bar{y}_p) \\cdot \\text{Area}(P_p)\n    $$\n    where $(\\bar{x}_p, \\bar{y}_p)$ is the centroid of polygon $P_p$.\n\n3.  **Centroid-Assignment Upscaling ($A_p^{\\mathrm{cen}}$)**:\n    a. The mass of each fine grid cell $C_{ij}$ is first calculated as $M(C_{ij})$.\n    b. The centroid of each cell $(x_c, y_c)$ is determined.\n    c. For each cell, the list of polygons $\\{P_p\\}$ is searched. The entire mass $M(C_{ij})$ is added to the total mass $A_p^{\\mathrm{cen}}$ of the *first* polygon $P_p$ whose interior contains the cell's centroid. If no polygon contains the centroid, the cell's mass is effectively lost from the aggregation, contributing to non-conservation.\n\n4.  **Exact-Overlay Upscaling ($A_p^{\\mathrm{exact}}$)**:\n    a. An accumulated mass $A_p^{\\mathrm{exact}}$ is initialized to zero for each polygon $P_p$.\n    b. For each combination of a fine cell $C$ and a polygon $P_p$, their geometric intersection $I = C \\cap P_p$ is computed. This intersection is also an axis-aligned rectangle.\n    c. If the intersection $I$ has a positive area, its mass $M(I)$ is calculated using the integral formula and added to $A_p^{\\mathrm{exact}}$.\n    d. This method is conservative by construction. Since the set of fine cells $\\{C\\}$ forms a partition of the domain $\\Omega$, the sum of integrals over the intersections with a polygon $P_p$ must equal the integral over the entire polygon:\n       $$\n       A_p^{\\mathrm{exact}} = \\sum_C M(C \\cap P_p) = \\sum_C \\iint_{C \\cap P_p} \\rho \\, dA = \\iint_{\\bigcup_C (C \\cap P_p)} \\rho \\, dA = \\iint_{P_p} \\rho \\, dA = T_p\n       $$\n       Therefore, we predict that for the exact-overlay method, $A_p^{\\mathrm{exact}} = T_p$ for all $p$. Consequently, the redistribution error $E(A^{\\mathrm{exact}})$ and the aggregate non-conservation $C(A^{\\mathrm{exact}})$ must both be zero, apart from negligible floating-point computational errors. This serves as a robust internal check for the correctness of the implementation.\n\n5.  **Error Metric Calculation**: Finally, the two error metrics are computed for both upscaling methods:\n    - Redistribution error magnitude: $E(A) = \\sum_{p=1}^m | A_p - T_p |$\n    - Aggregate non-conservation: $C(A) = \\sum_{p=1}^m A_p - \\sum_{p=1}^m T_p$\n\nThe provided test cases explore different spatial arrangements of polygons relative to the fine grid (misaligned, aligned, overlapping, and with gaps), which systematically expose the strengths and weaknesses of each upscaling approach. The centroid method is expected to show significant error when polygon boundaries are not aligned with cell boundaries, or when there are gaps and overlaps. The exact-overlay method is expected to be perfectly accurate in all cases, as it is designed to be the ground truth for this aggregation problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the spatial scaling problem for a suite of test cases.\n    \"\"\"\n    \n    # Test suite definition\n    test_cases = [\n        {\n            \"L_x\": 1000, \"L_y\": 1000, \"N_x\": 10, \"N_y\": 10,\n            \"alpha\": 2, \"beta\": 0.001, \"gamma\": -0.0005,\n            \"polygons\": [\n                (0, 430, 0, 1000),  # P1 = [0,430] x [0,1000]\n                (430, 1000, 0, 1000) # P2 = [430,1000] x [0,1000]\n            ]\n        },\n        {\n            \"L_x\": 1000, \"L_y\": 1000, \"N_x\": 10, \"N_y\": 10,\n            \"alpha\": 2, \"beta\": 0.001, \"gamma\": -0.0005,\n            \"polygons\": [\n                (0, 500, 0, 1000),   # P1 = [0,500] x [0,1000]\n                (500, 1000, 0, 1000) # P2 = [500,1000] x [0,1000]\n            ]\n        },\n        {\n            \"L_x\": 1000, \"L_y\": 1000, \"N_x\": 10, \"N_y\": 10,\n            \"alpha\": 2, \"beta\": 0.001, \"gamma\": -0.0005,\n            \"polygons\": [\n                (200, 700, 200, 700), # P1 = [200,700] x [200,700]\n                (500, 900, 100, 600)  # P2 = [500,900] x [100,600]\n            ]\n        },\n        {\n            \"L_x\": 1000, \"L_y\": 1000, \"N_x\": 10, \"N_y\": 10,\n            \"alpha\": 2, \"beta\": 0.001, \"gamma\": -0.0005,\n            \"polygons\": [\n                (0, 400, 0, 1000),   # P1 = [0,400] x [0,1000]\n                (600, 1000, 0, 1000) # P2 = [600,1000] x [0,1000]\n            ]\n        }\n    ]\n\n    def mass_integral(x1, y1, x2, y2, alpha, beta, gamma):\n        \"\"\"\n        Calculates the exact integral of rho(x,y) over a rectangle [x1,x2]x[y1,y2].\n        \"\"\"\n        if x1 >= x2 or y1 >= y2:\n            return 0.0\n        area = (x2 - x1) * (y2 - y1)\n        centroid_x = (x1 + x2) / 2.0\n        centroid_y = (y1 + y2) / 2.0\n        rho_at_centroid = alpha + beta * centroid_x + gamma * centroid_y\n        return rho_at_centroid * area\n\n    all_results = []\n    for case in test_cases:\n        Lx, Ly, Nx, Ny = case[\"L_x\"], case[\"L_y\"], case[\"N_x\"], case[\"N_y\"]\n        alpha, beta, gamma = case[\"alpha\"], case[\"beta\"], case[\"gamma\"]\n        polygons = [p for p in case[\"polygons\"]]\n        \n        dx_cell = Lx / Nx\n        dy_cell = Ly / Ny\n        num_polygons = len(polygons)\n\n        # 1. Compute true polygon mass T_p\n        T_p = np.zeros(num_polygons)\n        for p_idx, poly in enumerate(polygons):\n            T_p[p_idx] = mass_integral(poly[0], poly[2], poly[1], poly[3], alpha, beta, gamma)\n\n        # Pre-compute cell masses\n        cell_masses = np.zeros((Nx, Ny))\n        for i in range(Nx):\n            for j in range(Ny):\n                x1_cell, x2_cell = i * dx_cell, (i + 1) * dx_cell\n                y1_cell, y2_cell = j * dy_cell, (j + 1) * dy_cell\n                cell_masses[i, j] = mass_integral(x1_cell, y1_cell, x2_cell, y2_cell, alpha, beta, gamma)\n\n        # 2. Centroid-assignment method A_cen\n        A_cen = np.zeros(num_polygons)\n        for i in range(Nx):\n            for j in range(Ny):\n                x1_cell, x2_cell = i * dx_cell, (i + 1) * dx_cell\n                y1_cell, y2_cell = j * dy_cell, (j + 1) * dy_cell\n                centroid_x = (x1_cell + x2_cell) / 2.0\n                centroid_y = (y1_cell + y2_cell) / 2.0\n                mass = cell_masses[i, j]\n\n                for p_idx, poly in enumerate(polygons):\n                    ax, bx, ay, by = poly[0], poly[1], poly[2], poly[3]\n                    if ax  centroid_x  bx and ay  centroid_y  by:\n                        A_cen[p_idx] += mass\n                        break\n\n        # 3. Exact-overlay method A_exact\n        A_exact = np.zeros(num_polygons)\n        for p_idx, poly in enumerate(polygons):\n            poly_x1, poly_x2, poly_y1, poly_y2 = poly\n            for i in range(Nx):\n                for j in range(Ny):\n                    cell_x1, cell_x2 = i * dx_cell, (i + 1) * dx_cell\n                    cell_y1, cell_y2 = j * dy_cell, (j + 1) * dy_cell\n\n                    inter_x1 = max(cell_x1, poly_x1)\n                    inter_x2 = min(cell_x2, poly_x2)\n                    inter_y1 = max(cell_y1, poly_y1)\n                    inter_y2 = min(cell_y2, poly_y2)\n                    \n                    mass_in_intersection = mass_integral(inter_x1, inter_y1, inter_x2, inter_y2, alpha, beta, gamma)\n                    A_exact[p_idx] += mass_in_intersection\n        \n        # 4. Compute error metrics\n        E_cen = np.sum(np.abs(A_cen - T_p))\n        C_cen = np.sum(A_cen) - np.sum(T_p)\n        \n        E_exact = np.sum(np.abs(A_exact - T_p))\n        C_exact = np.sum(A_exact) - np.sum(T_p)\n        \n        # Consistent with theory, E_exact and C_exact should be ~0.\n        # We present the calculated floating point values.\n        \n        case_results = [E_cen, C_cen, E_exact, C_exact]\n        all_results.append(case_results)\n\n    # Format output string\n    result_strings = []\n    for res in all_results:\n        result_strings.append(f\"[{','.join(f'{x:.17g}' for x in res)}]\")\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Environmental models often involve nonlinear relationships, meaning the output is not directly proportional to the input. When upscaling, applying a nonlinear function to an averaged input is not the same as averaging the results of the function applied at the fine scale. This discrepancy, known as aggregation bias and related to Jensen's inequality, is a critical source of error that must be understood and quantified. This analytical exercise  asks you to derive the exact bias for an exponential transform and compare it with a common approximation, deepening your understanding of how sub-grid variability impacts large-scale estimates.",
            "id": "3851419",
            "problem": "In a land surface remote sensing workflow, a fine-resolution field $X$ represents the logarithm of a radiative quantity (for example, the log of path radiance or the log of column optical depth) whose spatial variability over a coarse-grid cell is modeled as Gaussian with mean $\\mu$ and variance $\\sigma^{2}$, i.e., $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. The Level-2 product maps this field to a physical quantity via the exponential transform $f(x)=\\exp(x)$ before aggregation. The aggregation bias of upscaling this nonlinear transform is defined as the difference between the coarse-cell expectation of the transformed fine-scale field and the transform of the coarse-cell mean of the fine-scale field, namely\n$$\nB \\equiv \\mathbb{E}[f(X)] - f\\!\\left(\\mathbb{E}[X]\\right).\n$$\nStarting from the core definitions of expectation, the properties of the Gaussian distribution, and Taylor series expansion, derive the exact closed-form expression for $B$ and its second-order approximation obtained by truncating the Taylor expansion of $f$ around $x=\\mu$ after the quadratic term. Then, report the single closed-form analytic expression for the ratio\n$$\nR \\equiv \\frac{B}{B_{\\text{(second-order)}}},\n$$\nin terms of $\\mu$ and $\\sigma^{2}$. No numerical evaluation is required and no rounding is needed. Express your final answer as a symbolic expression with no units.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in spatial statistics and remote sensing concerning the effects of nonlinearity on spatial aggregation. All required information is provided, and the problem is free of contradictions or ambiguities.\n\nThe objective is to find the ratio $R \\equiv \\frac{B}{B_{\\text{(second-order)}}}$, where $B$ is the exact aggregation bias and $B_{\\text{(second-order)}}$ is its second-order approximation. The field $X$ is a random variable following a Gaussian distribution, $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, and the nonlinear transformation is $f(x) = \\exp(x)$.\n\nThe derivation proceeds in three steps: first, the calculation of the exact bias $B$; second, the calculation of the approximate bias $B_{\\text{(second-order)}}$; and third, the calculation of their ratio $R$.\n\n**Step 1: Derivation of the Exact Aggregation Bias $B$**\n\nThe aggregation bias $B$ is defined as:\n$$\nB \\equiv \\mathbb{E}[f(X)] - f(\\mathbb{E}[X])\n$$\nThe random variable $X$ is given to follow a normal distribution $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$. By definition, the expectation of $X$ is:\n$$\n\\mathbb{E}[X] = \\mu\n$$\nThe transformation is $f(x) = \\exp(x)$. Applying this transformation to the expectation of $X$ gives:\n$$\nf(\\mathbb{E}[X]) = f(\\mu) = \\exp(\\mu)\n$$\nNext, we compute the expectation of the transformed variable, $\\mathbb{E}[f(X)]$:\n$$\n\\mathbb{E}[f(X)] = \\mathbb{E}[\\exp(X)]\n$$\nThis expression is, by definition, the moment-generating function (MGF) of the random variable $X$, denoted $M_X(t)$, evaluated at $t=1$. The MGF for a normal distribution $X \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ is given by the standard formula:\n$$\nM_X(t) = \\mathbb{E}[\\exp(tX)] = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^{2}t^{2}\\right)\n$$\nEvaluating the MGF at $t=1$:\n$$\n\\mathbb{E}[\\exp(X)] = M_X(1) = \\exp\\left(\\mu \\cdot 1 + \\frac{1}{2}\\sigma^{2}\\cdot 1^{2}\\right) = \\exp\\left(\\mu + \\frac{1}{2}\\sigma^{2}\\right)\n$$\nThis is the well-known formula for the mean of a log-normal distribution.\n\nNow, we can assemble the expression for the exact bias $B$:\n$$\nB = \\mathbb{E}[\\exp(X)] - \\exp(\\mathbb{E}[X]) = \\exp\\left(\\mu + \\frac{1}{2}\\sigma^{2}\\right) - \\exp(\\mu)\n$$\nBy factoring out the term $\\exp(\\mu)$, we arrive at the final expression for the exact bias:\n$$\nB = \\exp(\\mu)\\left(\\exp\\left(\\frac{1}{2}\\sigma^{2}\\right) - 1\\right)\n$$\n\n**Step 2: Derivation of the Second-Order Approximate Bias $B_{\\text{(second-order)}}$**\n\nThe second-order approximation is derived by taking the expectation of the Taylor series expansion of $f(x)$ around the mean $\\mu$, truncated after the quadratic term. The Taylor expansion of a function $f(x)$ around $x=\\mu$ is:\n$$\nf(x) = f(\\mu) + f'(\\mu)(x-\\mu) + \\frac{f''(\\mu)}{2!}(x-\\mu)^{2} + \\mathcal{O}\\left((x-\\mu)^{3}\\right)\n$$\nFor the function $f(x) = \\exp(x)$, the derivatives are $f'(x) = \\exp(x)$ and $f''(x) = \\exp(x)$. Evaluating these derivatives at $x=\\mu$ gives $f'(\\mu) = \\exp(\\mu)$ and $f''(\\mu) = \\exp(\\mu)$.\nSubstituting these into the Taylor expansion and truncating at the second order yields the approximation for $f(X)$:\n$$\nf(X) \\approx \\exp(\\mu) + \\exp(\\mu)(X-\\mu) + \\frac{\\exp(\\mu)}{2}(X-\\mu)^{2}\n$$\nThe expectation of this approximate expression, denoted $\\mathbb{E}[f(X)]_{\\text{approx}}$, is found using the linearity of the expectation operator:\n$$\n\\mathbb{E}[f(X)]_{\\text{approx}} = \\mathbb{E}\\left[\\exp(\\mu) + \\exp(\\mu)(X-\\mu) + \\frac{\\exp(\\mu)}{2}(X-\\mu)^{2}\\right]\n$$\n$$\n\\mathbb{E}[f(X)]_{\\text{approx}} = \\exp(\\mu) + \\exp(\\mu)\\mathbb{E}[X-\\mu] + \\frac{\\exp(\\mu)}{2}\\mathbb{E}\\left[(X-\\mu)^{2}\\right]\n$$\nWe use two fundamental properties of the distribution of $X$:\n1. The expectation of the deviation from the mean is zero: $\\mathbb{E}[X-\\mu] = \\mathbb{E}[X] - \\mu = \\mu - \\mu = 0$.\n2. The expectation of the squared deviation from the mean is the variance: $\\mathbb{E}\\left[(X-\\mu)^{2}\\right] = \\sigma^{2}$.\n\nSubstituting these results into the expression for $\\mathbb{E}[f(X)]_{\\text{approx}}$:\n$$\n\\mathbb{E}[f(X)]_{\\text{approx}} = \\exp(\\mu) + \\exp(\\mu) \\cdot 0 + \\frac{\\exp(\\mu)}{2} \\cdot \\sigma^{2} = \\exp(\\mu)\\left(1 + \\frac{1}{2}\\sigma^{2}\\right)\n$$\nThe second-order approximate bias, $B_{\\text{(second-order)}}$, is the difference between this approximate expectation and the transformed mean:\n$$\nB_{\\text{(second-order)}} = \\mathbb{E}[f(X)]_{\\text{approx}} - f(\\mathbb{E}[X]) = \\exp(\\mu)\\left(1 + \\frac{1}{2}\\sigma^{2}\\right) - \\exp(\\mu)\n$$\n$$\nB_{\\text{(second-order)}} = \\exp(\\mu) + \\frac{1}{2}\\exp(\\mu)\\sigma^{2} - \\exp(\\mu) = \\frac{1}{2}\\exp(\\mu)\\sigma^{2}\n$$\n\n**Step 3: Calculation of the Ratio $R$**\n\nThe final step is to compute the ratio $R = \\frac{B}{B_{\\text{(second-order)}}}$. We substitute the expressions derived in the previous steps:\n$$\nR = \\frac{\\exp(\\mu)\\left(\\exp\\left(\\frac{1}{2}\\sigma^{2}\\right) - 1\\right)}{\\frac{1}{2}\\exp(\\mu)\\sigma^{2}}\n$$\nThe term $\\exp(\\mu)$ appears in both the numerator and the denominator, and since $\\exp(\\mu)  0$, it can be cancelled. This leaves an expression that depends only on the variance $\\sigma^{2}$:\n$$\nR = \\frac{\\exp\\left(\\frac{1}{2}\\sigma^{2}\\right) - 1}{\\frac{1}{2}\\sigma^{2}}\n$$\nThis can be written in a slightly cleaner form by letting the denominator be a single term:\n$$\nR = \\frac{\\exp\\left(\\frac{\\sigma^{2}}{2}\\right) - 1}{\\frac{\\sigma^{2}}{2}}\n$$\nThis is the final, exact closed-form expression for the required ratio.",
            "answer": "$$\n\\boxed{\\frac{\\exp\\left(\\frac{\\sigma^{2}}{2}\\right) - 1}{\\frac{\\sigma^{2}}{2}}}\n$$"
        },
        {
            "introduction": "While upscaling aggregates data, downscaling disaggregates it—a common need when coarse model outputs must be mapped to a finer grid for local analysis. A primary challenge in downscaling is ensuring the new high-resolution field honors the original coarse data totals. Pycnophylactic (\"mass-preserving\") interpolation is an elegant iterative technique that solves this by alternating between smoothing the field and rescaling it to enforce the coarse-level conservation laws. In this coding practice , you will implement this classic algorithm, gaining hands-on experience with methods that balance physical constraints with desirable properties like smoothness.",
            "id": "3851410",
            "problem": "Consider a fine-resolution rectangular grid with $m \\times n$ cells, indexed by integer pairs $(i,j)$ with $i \\in \\{0,\\dots,m-1\\}$ and $j \\in \\{0,\\dots,n-1\\}$. A nonnegative scalar field $x$ defined on this grid is represented by an array $x_{i,j} \\in \\mathbb{R}_{\\ge 0}$. The grid is partitioned into $B$ disjoint zones by a given zone label array $z_{i,j} \\in \\{0,\\dots,B-1\\}$, and each zone $b \\in \\{0,\\dots,B-1\\}$ has a specified coarse total $M_b \\in \\mathbb{R}_{\\ge 0}$, which is the target sum of the fine-resolution values inside that zone. The goal of pycnophylactic interpolation is to construct a smooth fine-resolution field $x$ while exactly conserving the zone totals, meaning that for every zone $b$,\n$$\n\\sum_{\\{(i,j) \\mid z_{i,j} = b\\}} x_{i,j} = M_b.\n$$\n\nStarting from an initial nonnegative field $x^{(0)}$, define a smoothing operator $S$ by the $5$-point average with homogeneous Neumann (reflecting) boundary conditions:\n$$\n\\left(Sx\\right)_{i,j} = \\frac{1}{5}\\left(x_{i,j} + x_{\\max(i-1,0),j} + x_{\\min(i+1,m-1),j} + x_{i,\\max(j-1,0)} + x_{i,\\min(j+1,n-1)}\\right).\n$$\nPycnophylactic interpolation iteratively alternates smoothing with exact enforcement of zone-sum conservation via zone-wise rescaling. Let $Z_b = \\{(i,j) \\mid z_{i,j}=b\\}$ denote the index set of zone $b$. Denote $s_b(x) = \\sum_{(p,q)\\in Z_b} x_{p,q}$ the sum over zone $b$. The rescaling operator $R$ acts as\n$$\n\\left(Ry\\right)_{i,j} =\n\\begin{cases}\n\\displaystyle r_b \\, y_{i,j}  \\text{if } z_{i,j} = b, \\\\\n\\end{cases}\n\\quad \\text{where} \\quad r_b = \n\\begin{cases}\n\\displaystyle \\frac{M_b}{s_b(y)}  \\text{if } s_b(y)  0, \\\\\n\\displaystyle 0  \\text{if } s_b(y) = 0 \\text{ and } M_b = 0.\n\\end{cases}\n$$\nThe pycnophylactic update is then\n$$\nx^{(k+1)} = R\\big(S(x^{(k)})\\big),\n$$\nwhich enforces $s_b\\!\\left(x^{(k+1)}\\right) = M_b$ for all $b$ after each iteration while encouraging smoothness through $S$.\n\nYour task is to:\n- Derive from first principles why the update $x^{(k+1)} = R\\big(S(x^{(k)})\\big)$ preserves the exact coarse totals for all $k$, and explain the connection to minimizing a discrete Dirichlet energy subject to linear conservation constraints.\n- Implement the iterative pycnophylactic interpolation algorithm using the $5$-point average $S$ and the zone-wise rescaling $R$ as defined above.\n- For each provided test case, run the algorithm until convergence or until a fixed iteration cap, whichever occurs first. Convergence can be detected by the maximum absolute change $\\max_{i,j} \\left|x^{(k+1)}_{i,j} - x^{(k)}_{i,j}\\right|$ falling below a tolerance $\\varepsilon$.\n- For each test case, compute:\n  $1)$ a boolean indicating whether all zone sums of the final field equal their target totals within an absolute tolerance $\\varepsilon$, and\n  $2)$ the ratio of the final to initial discrete Dirichlet energy,\n$$\nE(x) = \\sum_{i=1}^{m-1}\\sum_{j=0}^{n-1} \\left(x_{i,j} - x_{i-1,j}\\right)^2 + \\sum_{i=0}^{m-1}\\sum_{j=1}^{n-1} \\left(x_{i,j} - x_{i,j-1}\\right)^2,\n$$\nrounded to six decimal places. If $E\\!\\left(x^{(0)}\\right) = 0$, define the ratio to be $0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). For $N$ test cases, the output should be ordered as $[b_1,r_1,b_2,r_2,\\dots,b_N,r_N]$, where $b_t$ is the boolean for test case $t$ and $r_t$ is the energy ratio for test case $t$ rounded to six decimals.\n\nUse the following test suite. In all cases, use $\\varepsilon = 10^{-9}$, an iteration cap of $k_{\\max} = 300$, and initialize $x^{(0)}$ exactly as specified.\n\n- Test case $1$ (happy path, multi-zone rectangular partition):\n  - Grid size: $m = 4$, $n = 4$.\n  - Zone labels $z$ (rows listed top to bottom) with $B = 4$ zones:\n    $$\n    \\begin{aligned}\n    \\left[\\,0,\\,0,\\,1,\\,1\\,\\right],\\\\\n    \\left[\\,0,\\,0,\\,1,\\,1\\,\\right],\\\\\n    \\left[\\,2,\\,2,\\,3,\\,3\\,\\right],\\\\\n    \\left[\\,2,\\,2,\\,3,\\,3\\,\\right].\n    \\end{aligned}\n    $$\n  - Coarse totals: $M_0 = 10$, $M_1 = 20$, $M_2 = 30$, $M_3 = 40$.\n  - Initial field $x^{(0)}$ (rows listed top to bottom):\n    $$\n    \\begin{aligned}\n    \\left[\\,1.0,\\,1.2,\\,0.8,\\,0.9\\,\\right],\\\\\n    \\left[\\,1.1,\\,0.9,\\,1.3,\\,0.7\\,\\right],\\\\\n    \\left[\\,0.6,\\,0.5,\\,1.4,\\,1.1\\,\\right],\\\\\n    \\left[\\,0.4,\\,0.6,\\,1.0,\\,1.2\\,\\right].\n    \\end{aligned}\n    $$\n\n- Test case $2$ (boundary condition with a zero-total zone):\n  - Grid size: $m = 2$, $n = 4$.\n  - Zone labels $z$ with $B = 2$ zones:\n    $$\n    \\begin{aligned}\n    \\left[\\,0,\\,0,\\,1,\\,1\\,\\right],\\\\\n    \\left[\\,0,\\,0,\\,1,\\,1\\,\\right].\n    \\end{aligned}\n    $$\n  - Coarse totals: $M_0 = 0$, $M_1 = 50$.\n  - Initial field $x^{(0)}$:\n    $$\n    \\begin{aligned}\n    \\left[\\,0.5,\\,0.2,\\,1.0,\\,1.5\\,\\right],\\\\\n    \\left[\\,0.4,\\,0.3,\\,0.8,\\,2.0\\,\\right].\n    \\end{aligned}\n    $$\n\n- Test case $3$ (irregular zone geometry):\n  - Grid size: $m = 3$, $n = 5$.\n  - Zone labels $z$ with $B = 3$ zones:\n    $$\n    \\begin{aligned}\n    \\left[\\,0,\\,0,\\,1,\\,1,\\,1\\,\\right],\\\\\n    \\left[\\,0,\\,2,\\,2,\\,1,\\,1\\,\\right],\\\\\n    \\left[\\,0,\\,2,\\,2,\\,2,\\,1\\,\\right].\n    \\end{aligned}\n    $$\n  - Coarse totals: $M_0 = 5$, $M_1 = 15$, $M_2 = 10$.\n  - Initial field $x^{(0)}$:\n    $$\n    \\begin{aligned}\n    \\left[\\,1.0,\\,0.5,\\,2.0,\\,1.0,\\,0.5\\,\\right],\\\\\n    \\left[\\,0.8,\\,3.0,\\,1.0,\\,2.0,\\,1.5\\,\\right],\\\\\n    \\left[\\,0.6,\\,0.5,\\,0.5,\\,3.0,\\,0.7\\,\\right].\n    \\end{aligned}\n    $$\n\n- Test case $4$ (single-zone domain):\n  - Grid size: $m = 3$, $n = 3$.\n  - Zone labels $z$ with $B = 1$ zone:\n    $$\n    \\begin{aligned}\n    \\left[\\,0,\\,0,\\,0\\,\\right],\\\\\n    \\left[\\,0,\\,0,\\,0\\,\\right],\\\\\n    \\left[\\,0,\\,0,\\,0\\,\\right].\n    \\end{aligned}\n    $$\n  - Coarse total: $M_0 = 100$.\n  - Initial field $x^{(0)}$:\n    $$\n    \\begin{aligned}\n    \\left[\\,1.0,\\,2.0,\\,3.0\\,\\right],\\\\\n    \\left[\\,4.0,\\,5.0,\\,6.0\\,\\right],\\\\\n    \\left[\\,7.0,\\,8.0,\\,9.0\\,\\right].\n    \\end{aligned}\n    $$\n\nImplement the algorithm exactly as specified. Your program should output the list $[b_1,r_1,b_2,r_2,b_3,r_3,b_4,r_4]$ on a single line, where each $b_t$ is a boolean and each $r_t$ is a float rounded to six decimal places. No other output is permitted.",
            "solution": "The problem of pycnophylactic interpolation is to construct a smooth, fine-resolution scalar field $x$ on a grid that adheres to specified coarse-resolution zone totals. The problem is well-defined, scientifically grounded in established spatial analysis techniques, and provides all necessary data and parameters for a computational solution. Therefore, the problem is deemed valid.\n\n### Theoretical Foundation\n\n#### 1. Preservation of Coarse Totals\n\nThe iterative update rule is given by $x^{(k+1)} = R\\big(S(x^{(k)})\\big)$. We must show that for any $k \\ge 0$, the resulting field $x^{(k+1)}$ satisfies the conservation constraint $\\sum_{\\{(i,j) \\mid z_{i,j} = b\\}} x^{(k+1)}_{i,j} = M_b$ for every zone $b \\in \\{0, \\dots, B-1\\}$.\n\nLet $y = S(x^{(k)})$ be the field after the smoothing step. The next iterate is $x^{(k+1)} = R(y)$. To verify the conservation property, we compute the sum of the values of $x^{(k+1)}$ over an arbitrary zone $b$. Let $Z_b = \\{(i,j) \\mid z_{i,j}=b\\}$ be the set of cell indices for zone $b$.\n\nThe sum over zone $b$ is:\n$$ \\sum_{(i,j) \\in Z_b} x^{(k+1)}_{i,j} = \\sum_{(i,j) \\in Z_b} (Ry)_{i,j} $$\n\nBy the definition of the rescaling operator $R$, for any cell $(i,j)$ within zone $b$, the value $(Ry)_{i,j}$ is given by $r_b \\, y_{i,j}$, where $r_b$ is a constant scaling factor for the entire zone. Therefore, we can factor $r_b$ out of the summation:\n$$ \\sum_{(i,j) \\in Z_b} (Ry)_{i,j} = \\sum_{(i,j) \\in Z_b} r_b \\, y_{i,j} = r_b \\sum_{(i,j) \\in Z_b} y_{i,j} $$\n\nThe sum $\\sum_{(i,j) \\in Z_b} y_{i,j}$ is precisely the sum of the smoothed field over zone $b$, which is denoted by $s_b(y)$. Thus, the expression becomes:\n$$ \\sum_{(i,j) \\in Z_b} x^{(k+1)}_{i,j} = r_b \\, s_b(y) $$\n\nWe now analyze this based on the definition of the scaling factor $r_b$:\n- **Case 1: $s_b(y)  0$.** In this case, $r_b = \\frac{M_b}{s_b(y)}$. Substituting this into our equation gives:\n  $$ \\sum_{(i,j) \\in Z_b} x^{(k+1)}_{i,j} = \\left(\\frac{M_b}{s_b(y)}\\right) s_b(y) = M_b $$\n- **Case 2: $s_b(y) = 0$ and $M_b = 0$.** In this case, $r_b = 0$. The equation gives:\n  $$ \\sum_{(i,j) \\in Z_b} x^{(k+1)}_{i,j} = (0) \\cdot (0) = 0 = M_b $$\n\nIn all well-defined cases, the sum of values in $x^{(k+1)}$ over zone $b$ is exactly $M_b$. The operator $R$ is constructed precisely to enforce this property. Since the update $x^{(k+1)}$ is the direct output of the operator $R$, it must satisfy the zone-sum constraints for all $k$, provided the iteration starts with a non-negative field $x^{(0)}$ and smoothing does not result in the undefined case where $s_b(y) = 0$ while $M_b  0$. The given smoothing operator $S$, being an average, preserves non-negativity and spreads values, making this pathological case highly unlikely with the provided initial data.\n\n#### 2. Connection to Constrained Energy Minimization\n\nThe pycnophylactic problem can be formulated as a constrained optimization problem. The goal is to find a field $x$ that is as \"smooth\" as possible while satisfying the given constraints. A standard measure of a field's lack of smoothness (or \"roughness\") is the discrete Dirichlet energy:\n$$ E(x) = \\sum_{i=1}^{m-1}\\sum_{j=0}^{n-1} \\left(x_{i,j} - x_{i-1,j}\\right)^2 + \\sum_{i=0}^{m-1}\\sum_{j=1}^{n-1} \\left(x_{i,j} - x_{i,j-1}\\right)^2 $$\nThis energy is the sum of squared differences between adjacent cell values. A smoother field has smaller local variations and thus a lower Dirichlet energy.\n\nThe optimization problem is:\n$$ \\text{Minimize} \\quad E(x) \\\\ \\text{subject to: } \\quad \\sum_{\\{(i,j) \\mid z_{i,j} = b\\}} x_{i,j} = M_b \\quad \\text{for all } b \\in \\{0, \\dots, B-1\\} \\\\ \\text{and} \\quad x_{i,j} \\ge 0 \\quad \\text{for all } i,j. $$\n\nThis is a convex optimization problem, as it involves minimizing a convex function ($E(x)$ is quadratic) over a convex feasible set (defined by linear equality constraints and non-negativity).\n\nThe iterative algorithm $x^{(k+1)} = R(S(x^{(k)}))$ is a form of an alternating projection method (or more generally, an operator splitting method) for solving this problem. The two main operations, smoothing and rescaling, can be interpreted as steps that address the two competing demands of the problem: smoothness and constraint satisfaction.\n\n1.  **Smoothing Step $y = S(x^{(k)})$**: This step aims to reduce the Dirichlet energy. The update rule for the smoothing operator $S$ is a 5-point average, which is equivalent to one iteration of the Jacobi method for solving the discrete Laplace equation $\\nabla^2 x = 0$. The steady-state solution to this equation (with Neumann boundaries) is a constant field, which has the minimum possible Dirichlet energy of $0$. Thus, applying $S$ moves the field $x^{(k)}$ towards a smoother configuration.\n\n2.  **Rescaling Step $x^{(k+1)} = R(y)$**: The smoothed field $y$ will generally not satisfy the zone-sum constraints. The operator $R$ projects the field $y$ back onto the feasible set defined by the linear conservation constraints. By scaling all values within a zone $b$ by the single factor $r_b$, it restores the required sum $M_b$. This multiplicative projection is particularly suited for problems that also include a non-negativity constraint.\n\nIn summary, the algorithm iteratively alternates between a step that promotes the objective (minimizing energy) and a step that enforces the constraints. While a single iteration does not guarantee a decrease in energy (the projection step $R$ might increase it), such alternating schemes are known to converge to a fixed point which, in this context, represents the optimal solution that balances maximum smoothness with exact preservation of mass.\n\n### Implementation Strategy\n\nThe algorithm will be implemented in Python using the `numpy` library for efficient array operations. A function will encapsulate the logic for a single test case.\n\n1.  **Initialization**: The initial field $x^{(0)}$, zone labels $z$, and coarse totals $M$ are initialized as `numpy` arrays.\n2.  **Iteration**: A loop runs for a maximum of $k_{\\max} = 300$ iterations.\n3.  **Smoothing ($S$)**: The $5$-point average with reflecting (Neumann) boundary conditions is implemented efficiently. The field `x` is padded with a $1$-cell border using `np.pad(x, 1, mode='edge')`. The smoothed field `y` is then computed using vectorized operations on the padded array, summing the values from the five corresponding positions (center, up, down, left, right) for each cell and dividing by $5$.\n4.  **Rescaling ($R$)**: The new field `x_next` is initialized. A loop iterates through each zone $b \\in \\{0, \\dots, B-1\\}$. For each zone, a boolean mask identifies the cells belonging to it. The sum of the smoothed field `y` over the zone, $s_b(y)$, is calculated. The scaling ratio $r_b$ is determined based on the problem's definition. The values of `x_next` within the zone are then set to the corresponding values of `y` multiplied by $r_b$. After the loop, the main field `x` is updated to `x_next`.\n5.  **Convergence**: After each iteration, the maximum absolute difference between the current field $x$ and the previous field $x_{\\text{prev}}$ is computed. If this difference falls below the tolerance $\\varepsilon = 10^{-9}$, the loop terminates.\n6.  **Final Calculations**:\n    - **Conservation Check**: After the loop, the final sum for each zone is computed and compared against its target total $M_b$. A boolean flag is set to `True` if all zone sums are within the tolerance $\\varepsilon$ of their targets, and `False` otherwise.\n    - **Energy Ratio**: A helper function computes the discrete Dirichlet energy $E(x)$ using `numpy` slicing to find horizontal and vertical differences. The ratio $E(x^{(\\text{final})}) / E(x^{(0)})$ is calculated, handling the case where $E(x^{(0)}) = 0$. The result is rounded to six decimal places.\n\nThe results from each test case (the conservation boolean and the energy ratio) are collected and formatted into the required single-line output string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for pycnophylactic interpolation\n    and print the results in the specified format.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (happy path, multi-zone rectangular partition)\n        {\n            \"m\": 4, \"n\": 4,\n            \"z\": np.array([\n                [0, 0, 1, 1],\n                [0, 0, 1, 1],\n                [2, 2, 3, 3],\n                [2, 2, 3, 3]\n            ]),\n            \"M\": np.array([10.0, 20.0, 30.0, 40.0]),\n            \"x0\": np.array([\n                [1.0, 1.2, 0.8, 0.9],\n                [1.1, 0.9, 1.3, 0.7],\n                [0.6, 0.5, 1.4, 1.1],\n                [0.4, 0.6, 1.0, 1.2]\n            ])\n        },\n        # Test case 2 (boundary condition with a zero-total zone)\n        {\n            \"m\": 2, \"n\": 4,\n            \"z\": np.array([\n                [0, 0, 1, 1],\n                [0, 0, 1, 1]\n            ]),\n            \"M\": np.array([0.0, 50.0]),\n            \"x0\": np.array([\n                [0.5, 0.2, 1.0, 1.5],\n                [0.4, 0.3, 0.8, 2.0]\n            ])\n        },\n        # Test case 3 (irregular zone geometry)\n        {\n            \"m\": 3, \"n\": 5,\n            \"z\": np.array([\n                [0, 0, 1, 1, 1],\n                [0, 2, 2, 1, 1],\n                [0, 2, 2, 2, 1]\n            ]),\n            \"M\": np.array([5.0, 15.0, 10.0]),\n            \"x0\": np.array([\n                [1.0, 0.5, 2.0, 1.0, 0.5],\n                [0.8, 3.0, 1.0, 2.0, 1.5],\n                [0.6, 0.5, 0.5, 3.0, 0.7]\n            ])\n        },\n        # Test case 4 (single-zone domain)\n        {\n            \"m\": 3, \"n\": 3,\n            \"z\": np.array([\n                [0, 0, 0],\n                [0, 0, 0],\n                [0, 0, 0]\n            ]),\n            \"M\": np.array([100.0]),\n            \"x0\": np.array([\n                [1.0, 2.0, 3.0],\n                [4.0, 5.0, 6.0],\n                [7.0, 8.0, 9.0]\n            ])\n        }\n    ]\n\n    # Global parameters\n    k_max = 300\n    epsilon = 1e-9\n    \n    results = []\n    for case in test_cases:\n        conserved, ratio = run_pycno(\n            case[\"z\"], case[\"M\"], case[\"x0\"], k_max, epsilon\n        )\n        results.append(str(conserved))\n        results.append(f\"{ratio:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef E_dirichlet(x: np.ndarray) - float:\n    \"\"\"Computes the discrete Dirichlet energy of a 2D field.\"\"\"\n    if x.shape[0]  2 and x.shape[1]  2:\n        return 0.0\n    \n    diff_h = 0.0\n    if x.shape[0] > 1:\n        diff_h = np.sum((x[1:, :] - x[:-1, :])**2)\n    \n    diff_v = 0.0\n    if x.shape[1] > 1:\n        diff_v = np.sum((x[:, 1:] - x[:, :-1])**2)\n        \n    return float(diff_h + diff_v)\n\ndef run_pycno(z: np.ndarray, M: np.ndarray, x0: np.ndarray, k_max: int, eps: float):\n    \"\"\"\n    Performs pycnophylactic interpolation for a single test case.\n\n    Args:\n        z: Zone label array.\n        M: Array of coarse totals for each zone.\n        x0: Initial field.\n        k_max: Maximum number of iterations.\n        eps: Convergence tolerance.\n\n    Returns:\n        A tuple containing:\n        - A boolean indicating if zone sums are conserved.\n        - The ratio of final to initial Dirichlet energy.\n    \"\"\"\n    x = np.copy(x0)\n    num_zones = len(M)\n\n    for _ in range(k_max):\n        x_prev = np.copy(x)\n\n        # Smoothing step (S)\n        # Pad the array to handle Neumann boundary conditions\n        padded_x = np.pad(x, 1, mode='edge')\n        \n        # Apply the 5-point average stencil using vectorized operations\n        y = (padded_x[1:-1, 1:-1] +   # Center\n             padded_x[:-2, 1:-1] +    # Up\n             padded_x[2:, 1:-1] +     # Down\n             padded_x[1:-1, :-2] +    # Left\n             padded_x[1:-1, 2:]) / 5.0\n\n        # Rescaling step (R)\n        x_next = np.zeros_like(x)\n        for b in range(num_zones):\n            mask = (z == b)\n            if not np.any(mask):\n                continue\n            \n            s_b = np.sum(y[mask])\n            M_b = M[b]\n\n            r_b = 0.0\n            if s_b > 0:\n                r_b = M_b / s_b\n            elif M_b == 0:  # This covers the case s_b == 0 and M_b == 0\n                r_b = 0.0\n            # The case s_b == 0 and M_b > 0 would be a division by zero.\n            # This is not expected given the smoothing operator and positive initial fields.\n\n            x_next[mask] = y[mask] * r_b\n        \n        x = x_next\n\n        # Check for convergence\n        max_change = np.max(np.abs(x - x_prev))\n        if max_change  eps:\n            break\n\n    # Calculate final metrics after iteration\n    \n    # 1. Check if all zone sums are conserved within tolerance\n    conserved = True\n    for b in range(num_zones):\n        mask = (z == b)\n        if not np.any(mask):\n            continue\n        final_sum = np.sum(x[mask])\n        if np.abs(final_sum - M[b]) > eps:\n            conserved = False\n            break\n\n    # 2. Calculate the ratio of final to initial discrete Dirichlet energy\n    E_initial = E_dirichlet(x0)\n    E_final = E_dirichlet(x)\n\n    if E_initial  eps: # Treat as zero if very small\n        energy_ratio = 0.0\n    else:\n        energy_ratio = E_final / E_initial\n\n    return conserved, energy_ratio\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}