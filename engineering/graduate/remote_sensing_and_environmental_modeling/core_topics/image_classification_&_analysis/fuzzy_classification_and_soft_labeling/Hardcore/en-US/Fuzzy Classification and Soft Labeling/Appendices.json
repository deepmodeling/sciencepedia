{
    "hands_on_practices": [
        {
            "introduction": "In fuzzy classification, a pixel is not assigned to a single class but is described by a \"soft label\"—a vector of membership probabilities across all possible classes. A fundamental step in working with these labels is to quantify their inherent uncertainty. This exercise introduces Shannon entropy, a key concept from information theory, as the principled way to measure the ambiguity of a classification, providing a vital metric for assessing model confidence at the pixel level .",
            "id": "3814939",
            "problem": "A multispectral pixel in a coastal wetland scene is modeled as a convex mixture of $K=3$ land cover classes (open water, emergent vegetation, and exposed sediment). A supervised probabilistic classifier outputs a soft label vector $p=(p_{1},p_{2},p_{3})=(0.2,0.5,0.3)$ representing the posterior class membership probabilities for this pixel. In fuzzy classification for remote sensing, a valid soft label must lie in the probability simplex, meaning each component is nonnegative and the components sum to $1$. First, assess whether the given $p$ satisfies this normalization property using only the core definition of a probability measure on a finite sample space.\n\nTo quantify classification uncertainty for environmental modeling, adopt the uncertainty functional characterized by the Shannon–Khinchin axioms (continuity, maximality at the uniform distribution, expandability, and additivity). Using the natural logarithm so that the uncertainty is measured in nats, compute the uncertainty of the soft label $p$ for this pixel. Round your final numerical result to four significant figures and express the answer in nats. Provide only the numeric value as your final answer.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\nThe givens are as follows:\n- A system with $K=3$ distinct land cover classes.\n- A soft label vector representing posterior class membership probabilities: $p=(p_{1},p_{2},p_{3})=(0.2,0.5,0.3)$.\n- The definition of a valid soft label: it must lie in the probability simplex, meaning $p_i \\ge 0$ for all $i$ and $\\sum_i p_i = 1$.\n- The first task is to assess if the given vector $p$ satisfies this property based on the core definition of a probability measure.\n- The second task is to compute the classification uncertainty using the functional defined by the Shannon–Khinchin axioms.\n- The uncertainty is to be measured in nats, implying the use of the natural logarithm.\n- The final numerical result must be rounded to four significant figures.\n\nThe problem is scientifically grounded, well-posed, and objective. It is based on established principles of probability theory (probability simplex) and information theory (Shannon entropy, which is uniquely characterized by the Shannon-Khinchin axioms). The context of fuzzy classification in remote sensing is a standard and realistic application domain. The data provided are complete, consistent, and sufficient to arrive at a unique solution. The problem does not violate any of the specified invalidity criteria. Therefore, the problem is deemed valid and a full solution is warranted.\n\nThe solution proceeds in two parts as requested.\n\nFirst, we validate the given soft label vector $p=(0.2, 0.5, 0.3)$. The problem asks for this validation to be based on the core definition of a probability measure on a finite sample space. Let the set of mutually exclusive and exhaustive classes be $\\Omega = \\{C_1, C_2, C_3\\}$. The posterior probabilities assigned to each class are $p_i = P(C_i)$, where $P$ is a probability measure. For a finite sample space, the Kolmogorov axioms of probability require:\n1.  Non-negativity: The probability of any event is non-negative. For the elementary events $C_i$, this translates to $p_i \\ge 0$ for all $i \\in \\{1, 2, 3\\}$.\n2.  Normalization: The probability of the entire sample space is $1$. Since the classes are exhaustive, this means $P(\\Omega) = P(C_1 \\cup C_2 \\cup C_3) = P(C_1) + P(C_2) + P(C_3) = \\sum_{i=1}^{3} p_i = 1$.\n\nWe check these two conditions for the given vector $p$:\n1.  Non-negativity: The components of the vector are $p_1 = 0.2$, $p_2 = 0.5$, and $p_3 = 0.3$. All three values are greater than or equal to $0$. This condition is satisfied.\n2.  Normalization: The sum of the components is calculated as:\n$$ \\sum_{i=1}^{3} p_i = p_1 + p_2 + p_3 = 0.2 + 0.5 + 0.3 = 1.0 $$\nThis condition is also satisfied.\nSince both axiomatic requirements are met, the vector $p$ is a valid probability distribution and lies within the probability simplex for $K=3$.\n\nSecond, we compute the classification uncertainty. The problem specifies using the uncertainty functional characterized by the Shannon–Khinchin axioms. For a discrete probability distribution $p = (p_1, p_2, \\dots, p_K)$, this functional is the Shannon entropy, $H(p)$, defined as:\n$$ H(p) = - \\sum_{i=1}^{K} p_i \\log_b(p_i) $$\nwhere $b$ is the base of the logarithm. The unit \"nats\" indicates that the natural logarithm (base $e$) must be used. Thus, the formula becomes:\n$$ H(p) = - \\sum_{i=1}^{K} p_i \\ln(p_i) $$\nThe convention that $0 \\ln(0) = 0$ is applied if any $p_i$ were $0$.\n\nFor the given probability distribution $p=(0.2, 0.5, 0.3)$ with $K=3$, the uncertainty $H(p)$ is:\n$$ H(p) = - \\left( p_1 \\ln(p_1) + p_2 \\ln(p_2) + p_3 \\ln(p_3) \\right) $$\nSubstituting the numerical values:\n$$ H(p) = - \\left( 0.2 \\ln(0.2) + 0.5 \\ln(0.5) + 0.3 \\ln(0.3) \\right) $$\nWe compute the values of the natural logarithms:\n$\\ln(0.2) \\approx -1.6094379$\n$\\ln(0.5) \\approx -0.6931472$\n$\\ln(0.3) \\approx -1.2039728$\n\nNow, we calculate each term in the sum:\n$0.2 \\ln(0.2) \\approx 0.2 \\times (-1.6094379) \\approx -0.32188758$\n$0.5 \\ln(0.5) \\approx 0.5 \\times (-0.6931472) \\approx -0.34657360$\n$0.3 \\ln(0.3) \\approx 0.3 \\times (-1.2039728) \\approx -0.36119184$\n\nSumming these terms:\n$$ \\sum_{i=1}^{3} p_i \\ln(p_i) \\approx -0.32188758 - 0.34657360 - 0.36119184 \\approx -1.02965302 $$\nThe uncertainty is the negative of this sum:\n$$ H(p) \\approx -(-1.02965302) \\approx 1.02965302 \\text{ nats} $$\nThe problem requires the result to be rounded to four significant figures. The first four significant figures are $1$, $0$, $2$, and $9$. The fifth significant digit is $6$, which is $5$ or greater, so we round up the last significant digit. Thus, $1.02965...$ rounds to $1.030$.",
            "answer": "$$\\boxed{1.030}$$"
        },
        {
            "introduction": "Beyond analyzing individual pixels, a common task in environmental modeling is to characterize larger areas, such as a map tile. This practice demonstrates how to derive a tile-level soft label by combining prior knowledge about class proportions with new observational data from pixel counts. You will apply the elegant framework of Bayesian inference using the Dirichlet-multinomial model, a cornerstone for updating beliefs about proportions in a statistically robust manner .",
            "id": "3814923",
            "problem": "A land-cover classification workflow for remote sensing and environmental modeling aggregates pixel-wise class assignments within a square tile of side length $30$ meters into a tile-level soft label for three land-cover classes. Let the unknown class-proportion vector for the tile be $p=(p_{1},p_{2},p_{3})$ on the $3$-simplex, with a Dirichlet prior $\\mathrm{Dirichlet}(\\alpha)$ capturing historical class mixing, where $\\alpha=(2,2,2)$. Within the tile, an independent pixel sample of size $N$ is modeled as a multinomial draw given $p$, resulting in observed counts $\\mathbf{n}=(n_{1},n_{2},n_{3})=(10,5,3)$ for the three classes. Using Bayesian updating under the conjugate Dirichlet–multinomial model, derive the posterior distribution for $p$ and then compute the posterior mean soft label $\\hat{p}$ for the tile.\n\nProvide the final answer as a single row matrix containing, in order, the three posterior Dirichlet parameters followed by the three components of the posterior mean soft label, that is,\n$$\n\\begin{pmatrix}\n\\alpha_{1}^{\\text{post}} & \\alpha_{2}^{\\text{post}} & \\alpha_{3}^{\\text{post}} & \\hat{p}_{1} & \\hat{p}_{2} & \\hat{p}_{3}\n\\end{pmatrix}.\n$$\nExpress the components of the posterior mean soft label as exact rational numbers; do not round. No units are required.",
            "solution": "The user has provided a Bayesian inference problem situated in the context of remote sensing and land-cover classification.\n\n### Step 1: Extract Givens\n- **Problem Domain:** Land-cover classification, remote sensing, environmental modeling.\n- **Spatial Unit:** A square tile of side length $30$ meters.\n- **Number of Classes:** $K=3$.\n- **Model Parameter:** The class-proportion vector for the tile is $p = (p_1, p_2, p_3)$, constrained to the $3$-simplex (i.e., $p_i \\ge 0$ for $i=1,2,3$ and $\\sum_{i=1}^3 p_i = 1$).\n- **Prior Distribution:** The prior distribution for the parameter $p$ is a Dirichlet distribution, $p \\sim \\mathrm{Dirichlet}(\\alpha)$.\n- **Prior Hyperparameters:** The vector of prior hyperparameters is $\\alpha = (\\alpha_1, \\alpha_2, \\alpha_3) = (2, 2, 2)$.\n- **Data Model (Likelihood):** A sample of $N$ pixels is modeled as a multinomial draw with parameter $p$. The observed counts are $\\mathbf{n} = (n_1, n_2, n_3)$.\n- **Observed Data:** The counts are given as $\\mathbf{n} = (10, 5, 3)$.\n- **Task:**\n    1. Derive the posterior distribution for $p$.\n    2. Compute the posterior mean soft label $\\hat{p} = E[p | \\mathbf{n}]$.\n- **Output Format:** A single row matrix containing the posterior Dirichlet parameters $(\\alpha_1^{\\text{post}}, \\alpha_2^{\\text{post}}, \\alpha_3^{\\text{post}})$ followed by the components of the posterior mean soft label $(\\hat{p}_1, \\hat{p}_2, \\hat{p}_3)$. Components of $\\hat{p}$ must be exact rational numbers.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is firmly grounded in Bayesian statistics. The Dirichlet-multinomial model is the canonical conjugate model for inference on proportions. Its application to soft-labeling in land-cover classification is a standard and scientifically sound practice in remote sensing and spatial statistics. The tile size of $30$ meters is characteristic of common satellite imagery (e.g., Landsat), making the context realistic.\n2.  **Well-Posed:** The problem is well-posed. Given a Dirichlet prior and multinomial data, the posterior distribution is known to be a Dirichlet distribution with parameters that can be calculated directly. The posterior mean is a well-defined property of the Dirichlet distribution. A unique solution exists and can be derived analytically.\n3.  **Objective:** The problem is stated in precise, objective mathematical language. All terms are standard in statistics and machine learning. There is no ambiguity or subjectivity.\n4.  **Completeness:** The problem is self-contained and complete. It provides the prior distribution, its parameters, the likelihood model, and the observed data, which are all the necessary components for performing the Bayesian update.\n5.  **Consistency:** The provided data and models are internally consistent. The number of classes matches the dimensions of the prior parameter vector and the data vector.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard application of Bayesian inference using a conjugate prior-likelihood pair. The problem is well-defined, scientifically sound, and all necessary information is provided. I will now proceed with the solution.\n\n### Derivation of Solution\nThe problem requires performing a Bayesian update for the class-proportion vector $p$ of a land-cover tile. The framework is the conjugate Dirichlet-multinomial model.\n\nThe prior distribution for the proportion vector $p = (p_1, p_2, p_3)$ is given as a Dirichlet distribution, parameterized by $\\alpha = (\\alpha_1, \\alpha_2, \\alpha_3)$:\n$$ p \\sim \\mathrm{Dirichlet}(\\alpha) $$\nThe probability density function (PDF) of the prior is:\n$$ f(p | \\alpha) = \\frac{\\Gamma(\\sum_{i=1}^3 \\alpha_i)}{\\prod_{i=1}^3 \\Gamma(\\alpha_i)} \\prod_{i=1}^3 p_i^{\\alpha_i - 1} $$\nFor our problem, $\\alpha = (2, 2, 2)$, so the prior PDF is proportional to:\n$$ f(p | \\alpha) \\propto p_1^{2-1} p_2^{2-1} p_3^{2-1} = p_1^{1} p_2^{1} p_3^{1} $$\n\nThe data consists of pixel counts $\\mathbf{n} = (n_1, n_2, n_3) = (10, 5, 3)$. The total number of sampled pixels is $N = n_1 + n_2 + n_3 = 10 + 5 + 3 = 18$. The likelihood of observing this data given the proportion vector $p$ is described by a multinomial distribution:\n$$ L(\\mathbf{n} | p) = P(\\mathbf{n} | p, N) = \\frac{N!}{n_1! n_2! n_3!} p_1^{n_1} p_2^{n_2} p_3^{n_3} $$\nAs a function of $p$, the likelihood is proportional to:\n$$ L(\\mathbf{n} | p) \\propto p_1^{n_1} p_2^{n_2} p_3^{n_3} = p_1^{10} p_2^{5} p_3^{3} $$\n\nAccording to Bayes' theorem, the posterior distribution of $p$ is proportional to the product of the prior and the likelihood:\n$$ f(p | \\mathbf{n}, \\alpha) \\propto L(\\mathbf{n} | p) f(p | \\alpha) $$\n$$ f(p | \\mathbf{n}, \\alpha) \\propto \\left( \\prod_{i=1}^3 p_i^{n_i} \\right) \\left( \\prod_{i=1}^3 p_i^{\\alpha_i - 1} \\right) $$\n$$ f(p | \\mathbf{n}, \\alpha) \\propto \\prod_{i=1}^3 p_i^{n_i + \\alpha_i - 1} $$\nThis resulting functional form is the kernel of a Dirichlet distribution. Therefore, the posterior distribution is also a Dirichlet distribution, a property known as conjugacy. The posterior distribution is:\n$$ p | \\mathbf{n} \\sim \\mathrm{Dirichlet}(\\alpha^{\\text{post}}) $$\nwhere the posterior hyperparameters $\\alpha^{\\text{post}}$ are obtained by adding the observed counts to the prior hyperparameters:\n$$ \\alpha^{\\text{post}} = \\alpha + \\mathbf{n} = (\\alpha_1 + n_1, \\alpha_2 + n_2, \\alpha_3 + n_3) $$\n\nWe can now compute the specific values for the posterior parameters:\n$$ \\alpha_1^{\\text{post}} = \\alpha_1 + n_1 = 2 + 10 = 12 $$\n$$ \\alpha_2^{\\text{post}} = \\alpha_2 + n_2 = 2 + 5 = 7 $$\n$$ \\alpha_3^{\\text{post}} = \\alpha_3 + n_3 = 2 + 3 = 5 $$\nThus, the posterior distribution is $p | \\mathbf{n} \\sim \\mathrm{Dirichlet}(12, 7, 5)$. These are the first three values for the final answer.\n\nNext, we must compute the posterior mean soft label, $\\hat{p}$. This is the expected value of the posterior distribution of $p$. For a general Dirichlet distribution $p \\sim \\mathrm{Dirichlet}(\\beta)$, the expected value of the $i$-th component is given by:\n$$ E[p_i] = \\frac{\\beta_i}{\\sum_{j=1}^K \\beta_j} $$\nIn our case, the parameters are the posterior parameters $\\beta = \\alpha^{\\text{post}}$. Let $\\alpha_0^{\\text{post}}$ be the sum of the posterior parameters:\n$$ \\alpha_0^{\\text{post}} = \\sum_{i=1}^3 \\alpha_i^{\\text{post}} = 12 + 7 + 5 = 24 $$\nThe components of the posterior mean vector $\\hat{p} = (\\hat{p}_1, \\hat{p}_2, \\hat{p}_3)$ are:\n$$ \\hat{p}_1 = E[p_1 | \\mathbf{n}] = \\frac{\\alpha_1^{\\text{post}}}{\\alpha_0^{\\text{post}}} = \\frac{12}{24} = \\frac{1}{2} $$\n$$ \\hat{p}_2 = E[p_2 | \\mathbf{n}] = \\frac{\\alpha_2^{\\text{post}}}{\\alpha_0^{\\text{post}}} = \\frac{7}{24} $$\n$$ \\hat{p}_3 = E[p_3 | \\mathbf{n}] = \\frac{\\alpha_3^{\\text{post}}}{\\alpha_0^{\\text{post}}} = \\frac{5}{24} $$\nThese are the last three values for the final answer. They are expressed as exact rational numbers as required.\n\nThe final answer is the row matrix combining the posterior parameters and the posterior mean components:\n$$ \\begin{pmatrix} \\alpha_{1}^{\\text{post}} & \\alpha_{2}^{\\text{post}} & \\alpha_{3}^{\\text{post}} & \\hat{p}_{1} & \\hat{p}_{2} & \\hat{p}_{3} \\end{pmatrix} = \\begin{pmatrix} 12 & 7 & 5 & \\frac{1}{2} & \\frac{7}{24} & \\frac{5}{24} \\end{pmatrix} $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n12 & 7 & 5 & \\frac{1}{2} & \\frac{7}{24} & \\frac{5}{24}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Evaluating the accuracy of a fuzzy classification requires metrics that can compare two soft labels, rather than just tallying right or wrong hard assignments. This exercise introduces the Hellinger distance, a true metric on the space of probability distributions, to quantify the dissimilarity between a predicted soft label and a reference label. By learning to compute and aggregate this distance, you will gain a powerful tool for nuanced accuracy assessment that properly accounts for partial agreements and disagreements in soft classification outputs .",
            "id": "3814924",
            "problem": "A multispectral Earth observation image is analyzed for land cover using a fuzzy classifier that produces soft label vectors for each pixel, where each vector is a set of class membership values that sum to one. Consider four land cover classes (water, vegetation, bare soil, built-up). A high-resolution fractional map provides reference soft labels as subpixel fractions for the same classes. For three representative pixels in a scene, the classifier predictions and reference fractions are as follows:\n- Pixel $1$: predicted $p^{(1)} = [0.50, 0.30, 0.15, 0.05]$, reference $q^{(1)} = [0.40, 0.35, 0.20, 0.05]$.\n- Pixel $2$: predicted $p^{(2)} = [0.05, 0.25, 0.60, 0.10]$, reference $q^{(2)} = [0.10, 0.20, 0.65, 0.05]$.\n- Pixel $3$: predicted $p^{(3)} = [0.20, 0.50, 0.25, 0.05]$, reference $q^{(3)} = [0.25, 0.45, 0.25, 0.05]$.\n\nSuppose the three pixels have reliability weights (e.g., derived from cloud screening and signal-to-noise estimates) $w^{(1)} = 0.5$, $w^{(2)} = 0.3$, and $w^{(3)} = 0.2$, with $\\sum_{i=1}^{3} w^{(i)} = 1$.\n\nStarting from the basic definition of the Bhattacharyya coefficient for discrete distributions and the induced Hellinger distance on the probability simplex, do the following:\n1. Derive the discrete-pixel expression for the Hellinger distance between the predicted and reference soft label vectors for a single pixel.\n2. Explain how to aggregate the pixel-level Hellinger distances over a spatial scene using a weighting function and the concept of a normalized integral (or expectation) over the spatial domain.\n3. Compute the scene-level aggregated Hellinger distance for the three pixels given above, using the provided weights and your derivation.\n\nExpress the final aggregated scene-level Hellinger distance as a decimal, and round your answer to four significant figures. No physical units are required because this distance is dimensionless.",
            "solution": "The user-provided problem is assessed as valid and well-posed. All data and definitions required for a unique solution are present and self-consistent. The problem is scientifically grounded in the fields of remote sensing, information theory, and statistics.\n\nThe solution will be developed in three parts as requested by the problem statement.\n\n### 1. Derivation of the Hellinger Distance for a Single Pixel\n\nThe problem requires deriving the discrete Hellinger distance, starting from the Bhattacharyya coefficient. Let $p = \\{p_j\\}_{j=1}^K$ and $q = \\{q_j\\}_{j=1}^K$ be two discrete probability distributions over the same set of $K$ outcomes. In this context, $p$ is the predicted soft label vector and $q$ is the reference soft label vector for a single pixel, and $K=4$ is the number of land cover classes. The components $p_j$ and $q_j$ represent the membership values for the $j$-th class, with $\\sum_{j=1}^K p_j = 1$ and $\\sum_{j=1}^K q_j = 1$.\n\nThe Bhattacharyya coefficient, $BC(p, q)$, for these discrete distributions is defined as the sum of the geometric means of the corresponding probabilities:\n$$\nBC(p, q) = \\sum_{j=1}^{K} \\sqrt{p_j q_j}\n$$\nThis coefficient measures the overlap between the two distributions. Its value ranges from $0$ (for disjoint distributions) to $1$ (for identical distributions).\n\nThe Hellinger distance, $H(p, q)$, is a metric defined on the probability simplex and is induced by the Bhattacharyya coefficient. Its definition is:\n$$\nH(p, q) = \\sqrt{1 - BC(p, q)}\n$$\nSubstituting the expression for the Bhattacharyya coefficient into this definition yields the discrete-pixel expression for the Hellinger distance between the predicted soft label vector $p$ and the reference soft label vector $q$:\n$$\nH(p, q) = \\sqrt{1 - \\sum_{j=1}^{K} \\sqrt{p_j q_j}}\n$$\nThis expression provides the distance for a single pixel.\n\n### 2. Aggregation of Pixel-Level Hellinger Distances\n\nTo obtain a single performance metric for an entire scene composed of multiple pixels, the pixel-level Hellinger distances must be aggregated. The problem specifies that this should be done using a weighting function. For a discrete set of $N$ pixels, indexed by $i=1, \\dots, N$, this corresponds to a weighted average.\n\nLet $H^{(i)} = H(p^{(i)}, q^{(i)})$ be the Hellinger distance for the $i$-th pixel, and let $w^{(i)}$ be the reliability weight associated with that pixel. The weights are given such that they are normalized, i.e., $\\sum_{i=1}^N w^{(i)} = 1$.\n\nThe scene-level aggregated Hellinger distance, $H_{agg}$, is the weighted arithmetic mean of the individual pixel-level distances:\n$$\nH_{agg} = \\sum_{i=1}^{N} w^{(i)} H^{(i)}\n$$\nThis formulation is equivalent to the expectation of the Hellinger distance, where the set of pixels is treated as a sample space and the weights $w^{(i)}$ define the probability mass function over that space. In the continuous case, this would be a normalized integral over the spatial domain $\\Omega$, $H_{agg} = \\int_{\\Omega} w(x) H(p(x), q(x)) dx / \\int_{\\Omega} w(x) dx$, where $x$ is a spatial coordinate. The discrete sum is the direct analogue for the given problem with $N=3$ pixels.\n\n### 3. Computation of the Aggregated Hellinger Distance\n\nWe now compute the aggregated Hellinger distance for the three given pixels using the derived formulas. The number of classes is $K=4$.\n\n**For Pixel 1:**\n$p^{(1)} = [0.50, 0.30, 0.15, 0.05]$\n$q^{(1)} = [0.40, 0.35, 0.20, 0.05]$\n$w^{(1)} = 0.5$\n\nThe Bhattacharyya coefficient is:\n$$\nBC^{(1)} = \\sqrt{0.50 \\times 0.40} + \\sqrt{0.30 \\times 0.35} + \\sqrt{0.15 \\times 0.20} + \\sqrt{0.05 \\times 0.05}\n$$\n$$\nBC^{(1)} = \\sqrt{0.20} + \\sqrt{0.105} + \\sqrt{0.03} + 0.05\n$$\n$$\nBC^{(1)} \\approx 0.4472136 + 0.3240370 + 0.1732051 + 0.05 = 0.9944557\n$$\nThe Hellinger distance for pixel 1 is:\n$$\nH^{(1)} = \\sqrt{1 - BC^{(1)}} \\approx \\sqrt{1 - 0.9944557} = \\sqrt{0.0055443} \\approx 0.0744600\n$$\n\n**For Pixel 2:**\n$p^{(2)} = [0.05, 0.25, 0.60, 0.10]$\n$q^{(2)} = [0.10, 0.20, 0.65, 0.05]$\n$w^{(2)} = 0.3$\n\nThe Bhattacharyya coefficient is:\n$$\nBC^{(2)} = \\sqrt{0.05 \\times 0.10} + \\sqrt{0.25 \\times 0.20} + \\sqrt{0.60 \\times 0.65} + \\sqrt{0.10 \\times 0.05}\n$$\n$$\nBC^{(2)} = \\sqrt{0.005} + \\sqrt{0.05} + \\sqrt{0.39} + \\sqrt{0.005}\n$$\n$$\nBC^{(2)} \\approx 0.0707107 + 0.2236068 + 0.6244998 + 0.0707107 = 0.9895280\n$$\nThe Hellinger distance for pixel 2 is:\n$$\nH^{(2)} = \\sqrt{1 - BC^{(2)}} \\approx \\sqrt{1 - 0.9895280} = \\sqrt{0.0104720} \\approx 0.1023330\n$$\n\n**For Pixel 3:**\n$p^{(3)} = [0.20, 0.50, 0.25, 0.05]$\n$q^{(3)} = [0.25, 0.45, 0.25, 0.05]$\n$w^{(3)} = 0.2$\n\nThe Bhattacharyya coefficient is:\n$$\nBC^{(3)} = \\sqrt{0.20 \\times 0.25} + \\sqrt{0.50 \\times 0.45} + \\sqrt{0.25 \\times 0.25} + \\sqrt{0.05 \\times 0.05}\n$$\n$$\nBC^{(3)} = \\sqrt{0.05} + \\sqrt{0.225} + 0.25 + 0.05\n$$\n$$\nBC^{(3)} \\approx 0.2236068 + 0.4743416 + 0.25 + 0.05 = 0.9979484\n$$\nThe Hellinger distance for pixel 3 is:\n$$\nH^{(3)} = \\sqrt{1 - BC^{(3)}} \\approx \\sqrt{1 - 0.9979484} = \\sqrt{0.0020516} \\approx 0.0452946\n$$\n\n**Final Aggregation:**\nNow, we compute the aggregated Hellinger distance using the weights $w^{(1)}=0.5$, $w^{(2)}=0.3$, and $w^{(3)}=0.2$.\n$$\nH_{agg} = w^{(1)} H^{(1)} + w^{(2)} H^{(2)} + w^{(3)} H^{(3)}\n$$\nUsing the more precise values for the intermediate calculations:\n$H^{(1)} \\approx 0.0744600047$\n$H^{(2)} \\approx 0.1023329969$\n$H^{(3)} \\approx 0.0452940756$\n$$\nH_{agg} \\approx (0.5 \\times 0.0744600047) + (0.3 \\times 0.1023329969) + (0.2 \\times 0.0452940756)\n$$\n$$\nH_{agg} \\approx 0.0372300024 + 0.0306998991 + 0.0090588151\n$$\n$$\nH_{agg} \\approx 0.0769887166\n$$\nThe problem requires rounding the final answer to four significant figures. The first significant figure is $7$, followed by $6$, $9$, and $8$. The fifth significant figure is $8$, which is $\\ge 5$, so we round up the fourth significant figure. The number $0.07698$ becomes $0.07699$.\n$$\nH_{agg} \\approx 0.07699\n$$",
            "answer": "$$\n\\boxed{0.07699}\n$$"
        }
    ]
}