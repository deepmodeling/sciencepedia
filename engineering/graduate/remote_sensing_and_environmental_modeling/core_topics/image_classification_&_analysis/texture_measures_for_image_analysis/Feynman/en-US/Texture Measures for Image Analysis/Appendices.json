{
    "hands_on_practices": [
        {
            "introduction": "The Gray-Level Co-occurrence Matrix (GLCM) is a cornerstone of statistical texture analysis, providing a powerful way to quantify the spatial relationships between pixel intensities. This practice focuses on building the GLCM from first principles and deriving several key texture measures. By systematically varying the lag distance $d$, you will gain a deeper understanding of how these features capture textural information at different spatial scales, from fine-grained patterns to coarser structures .",
            "id": "3859963",
            "problem": "Consider a two-dimensional discrete grayscale image modeled as a function $I(x,y)$ on a rectangular lattice, where each pixel takes a value in $\\{0,1,\\dots,L-1\\}$ for some integer number of gray levels $L$. Second-order texture is characterized by spatial gray-level interactions quantified by the Gray-Level Co-occurrence Matrix (GLCM). For a strictly positive integer lag distance $d$ and an orientation angle $\\theta$ (in degrees), define the co-occurrence relation between pixel pairs separated by the vector of length $d$ at angle $\\theta$. From this relation, define the co-occurrence count matrix, and then the normalized joint probability function $p_{d,\\theta}(i,j)$ by dividing counts by the total number of valid pixel pairs.\n\nStarting from the fundamental definitions above, derive the standard second-order texture measures as functionals of the joint probability $p_{d,\\theta}(i,j)$. Your derivation must be grounded in the probability interpretation of $p_{d,\\theta}(i,j)$, using expectations over appropriate functions of the random variables corresponding to gray levels of paired pixels. You must ensure your definitions are well-posed for any integer $L \\geq 2$, any image domain, any strictly positive integer lag distance $d$, and angle $\\theta$ expressed in degrees. For the special case of degenerate images where variance is zero, define the correlation to be $0$.\n\nWrite a complete program that:\n- Constructs the GLCM for a given image at orientation $\\theta = 0^\\circ$ (horizontal direction) and at specified lag distances $d$, by counting all valid ordered pixel pairs $(I(x,y), I(x,y+d))$ where indices are within bounds, and normalizing to obtain $p_{d,0^\\circ}(i,j)$.\n- Computes, from first principles, the following texture features from $p_{d,0^\\circ}(i,j)$: contrast, homogeneity, energy, entropy, and correlation, each defined via expectations with respect to $p_{d,0^\\circ}(i,j)$.\n- Quantifies the effect of varying the lag distance $d$ by computing the differences $\\Delta$ between features at $d = 4$ and $d = 1$, i.e., for each feature $f$, return $f(d=4) - f(d=1)$.\n\nAngle must be treated in degrees throughout. No physical units apply. All computations must be expressed as floating-point numbers.\n\nTest Suite:\nUse the following images and parameters to exercise different texture regimes, boundary conditions, and coarse-versus-fine-scale patterns:\n- Test case $1$: An $8 \\times 8$ binary checkerboard image with $L=2$, defined by $I(x,y) = ((x+y) \\bmod 2)$. Use lag distances $d \\in \\{1,4\\}$, $\\theta = 0^\\circ$.\n- Test case $2$: A $32 \\times 32$ image with $L=8$ gray levels produced by pseudo-random integers in $\\{0,1,\\dots,7\\}$ using a fixed seed $s = 42$ for reproducibility. Use lag distances $d \\in \\{1,4\\}$, $\\theta = 0^\\circ$.\n- Test case $3$: A $16 \\times 16$ binary image with vertical stripes of width $4$, so $I(x,y) = ((y \\div 4) \\bmod 2)$ where $\\div$ denotes integer division, and $L=2$. Use lag distances $d \\in \\{1,4\\}$, $\\theta = 0^\\circ$.\n- Test case $4$: A $16 \\times 16$ uniform binary image with all pixels equal to $1$, $L=2$. Use lag distances $d \\in \\{1,4\\}$, $\\theta = 0^\\circ$.\n\nFor each test case, compute the feature differences $[\\Delta\\text{contrast}, \\Delta\\text{homogeneity}, \\Delta\\text{energy}, \\Delta\\text{entropy}, \\Delta\\text{correlation}]$ where each $\\Delta$ is $f(d=4) - f(d=1)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. Each per-test-case result must be a list of five floats rounded to six decimal places, in the order $[\\Delta\\text{contrast},\\Delta\\text{homogeneity},\\Delta\\text{energy},\\Delta\\text{entropy},\\Delta\\text{correlation}]$. For example, the output structure must look like $[[a_1,a_2,a_3,a_4,a_5],[b_1,b_2,b_3,b_4,b_5],[c_1,c_2,c_3,c_4,c_5],[d_1,d_2,d_3,d_4,d_5]]$, where each $a_i$, $b_i$, $c_i$, $d_i$ is a float rounded to six decimal places.",
            "solution": "We begin with a formal definition of the Gray-Level Co-occurrence Matrix (GLCM) grounded in probability. Let $I(x,y)$ be a discrete grayscale image with pixel values in $\\{0,1,\\dots,L-1\\}$, with $L \\geq 2$. Fix an orientation angle $\\theta$ in degrees and a strictly positive integer lag distance $d$. Consider all ordered pairs of pixels $(I(x,y), I(x',y'))$ such that the displacement from $(x,y)$ to $(x',y')$ has length $d$ and angle $\\theta$. For the horizontal direction with $\\theta = 0^\\circ$, the displacement is $(\\Delta x, \\Delta y) = (0,d)$, so $(x',y') = (x, y + d)$.\n\nDefine the co-occurrence count matrix $P_{d,\\theta}(i,j)$ by\n$$\nP_{d,\\theta}(i,j) = \\#\\{(x,y) \\mid I(x,y) = i,\\, I(x',y') = j,\\, (x',y') = (x,y) + (0,d),\\, \\text{in bounds}\\}.\n$$\nLet $N_{d,\\theta} = \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} P_{d,\\theta}(i,j)$ be the total number of valid pairs. The normalized joint probability mass function (pmf) is\n$$\np_{d,\\theta}(i,j) = \\frac{P_{d,\\theta}(i,j)}{N_{d,\\theta}}, \\quad \\text{for } i,j \\in \\{0,1,\\dots,L-1\\}.\n$$\nThis $p_{d,\\theta}(i,j)$ is a bona fide pmf over the discrete random variables $(X,Y)$ representing gray levels of the pair at the specified displacement, because $p_{d,\\theta}(i,j) \\geq 0$ and $\\sum_{i,j} p_{d,\\theta}(i,j) = 1$.\n\nFrom $p_{d,\\theta}(i,j)$, we obtain marginal pmfs $p_X(i) = \\sum_{j=0}^{L-1} p_{d,\\theta}(i,j)$ and $p_Y(j) = \\sum_{i=0}^{L-1} p_{d,\\theta}(i,j)$. Their expectations and variances are\n$$\n\\mu_X = \\sum_{i=0}^{L-1} i\\, p_X(i), \\quad \\mu_Y = \\sum_{j=0}^{L-1} j\\, p_Y(j),\n$$\n$$\n\\sigma_X^2 = \\sum_{i=0}^{L-1} (i - \\mu_X)^2\\, p_X(i), \\quad \\sigma_Y^2 = \\sum_{j=0}^{L-1} (j - \\mu_Y)^2\\, p_Y(j).\n$$\n\nWe now define the standard texture measures via expectations over $(X,Y)$ with respect to $p_{d,\\theta}(i,j)$:\n\n- Contrast quantifies average squared gray-level difference:\n$$\n\\text{contrast} = \\mathbb{E}\\left[(X - Y)^2\\right] = \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} (i - j)^2\\, p_{d,\\theta}(i,j).\n$$\n\n- Homogeneity (inverse difference moment) penalizes larger gray-level differences:\n$$\n\\text{homogeneity} = \\mathbb{E}\\left[\\frac{1}{1 + (X - Y)^2}\\right] = \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} \\frac{p_{d,\\theta}(i,j)}{1 + (i - j)^2}.\n$$\n\n- Energy (angular second moment) measures concentration of the pmf:\n$$\n\\text{energy} = \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} \\left(p_{d,\\theta}(i,j)\\right)^2.\n$$\n\n- Entropy measures uncertainty of the pmf:\n$$\n\\text{entropy} = - \\sum_{i=0}^{L-1} \\sum_{j=0}^{L-1} p_{d,\\theta}(i,j) \\log p_{d,\\theta}(i,j),\n$$\nwith the convention that terms with $p_{d,\\theta}(i,j) = 0$ contribute $0$.\n\n- Correlation measures linear association between $X$ and $Y$:\n$$\n\\text{correlation} = \\frac{\\mathbb{E}\\left[(X - \\mu_X)(Y - \\mu_Y)\\right]}{\\sigma_X \\sigma_Y} = \\frac{\\sum_{i=0}^{L-1}\\sum_{j=0}^{L-1} (i - \\mu_X)(j - \\mu_Y)\\, p_{d,\\theta}(i,j)}{\\sigma_X \\sigma_Y}.\n$$\nFor degenerate cases where $\\sigma_X = 0$ or $\\sigma_Y = 0$, we define correlation to be $0$.\n\nThese measures are derived directly from the pmf $p_{d,\\theta}(i,j)$, tying them to the fundamental probabilistic characterization of spatial gray-level interactions.\n\nAlgorithmic design:\n\n1. For a given image $I$ and $L$, construct $P_{d,0^\\circ}(i,j)$ by counting all ordered pairs $(I(x,y), I(x,y+d))$ with $x \\in \\{0,\\dots,H-1\\}$, $y \\in \\{0,\\dots,W-d-1\\}$ for image height $H$ and width $W$. Each observed pair $(i,j)$ increments $P_{d,0^\\circ}(i,j)$ by $1$.\n\n2. Normalize counts to probabilities: compute $N_{d,0^\\circ}$ as the sum of all counts and set $p_{d,0^\\circ}(i,j) = P_{d,0^\\circ}(i,j)/N_{d,0^\\circ}$.\n\n3. Compute the features as shown above using $p_{d,0^\\circ}(i,j)$, its marginals, and derived statistics. Use the natural logarithm in entropy, and exclude zero-probability terms from the summation to avoid undefined expressions.\n\n4. Quantify the effect of lag distance by computing differences $\\Delta f = f(d=4) - f(d=1)$ for each feature $f$ in $\\{\\text{contrast}, \\text{homogeneity}, \\text{energy}, \\text{entropy}, \\text{correlation}\\}$.\n\n5. Apply this procedure to the specified test suite:\n   - For the $8 \\times 8$ checkerboard ($L=2$), increasing $d$ from $1$ to $4$ flips the co-occurrence from predominantly cross-level pairs to predominantly same-level pairs, decreasing contrast, increasing homogeneity, leaving energy and entropy unchanged (due to symmetric two-state pmf), and increasing correlation from negative to positive.\n   - For the $32 \\times 32$ seeded random image ($L=8$), co-occurrence is approximately independent across $d$, leading to small $\\Delta$ values across features.\n   - For the $16 \\times 16$ vertical stripes of width $4$ ($L=2$), $d=1$ stays mostly within stripes (same-level pairs), while $d=4$ aligns with transitions between stripes (cross-level pairs), increasing contrast and decreasing homogeneity; energy and entropy adjust modestly; correlation becomes more negative.\n   - For the uniform image ($L=2$), the pmf is concentrated on $(i,j) = (1,1)$ for all $d$, yielding no change across features (all $\\Delta$ are $0$).\n\n6. Round each computed difference to six decimal places and output the per-test-case results in the specified single-line, no-spaces, bracketed, comma-separated format.\n\nThis design adheres to first principles by deriving all texture features from the joint probability function determined by the GLCM, and it isolates the effect of increasing lag distance $d$ on coarse-scale versus fine-scale texture expression.",
            "answer": "```python\nimport numpy as np\n\ndef build_glcm_horizontal(image: np.ndarray, levels: int, d: int) -> np.ndarray:\n    \"\"\"\n    Build the GLCM for horizontal orientation (theta = 0 degrees) at lag distance d.\n    image: 2D numpy array of integer gray levels in {0, ..., levels-1}\n    levels: number of gray levels L\n    d: positive integer lag\n    Returns: L x L normalized joint probability matrix p(i,j)\n    \"\"\"\n    H, W = image.shape\n    if d <= 0:\n        raise ValueError(\"Lag distance d must be a strictly positive integer.\")\n    if W - d <= 0:\n        # No valid pairs; return uniform zero-probability matrix (handled upstream by choosing d < W)\n        return np.zeros((levels, levels), dtype=float)\n\n    left = image[:, :W - d]\n    right = image[:, d:]\n\n    # Flatten pairs\n    i_vals = left.ravel()\n    j_vals = right.ravel()\n\n    # Accumulate counts\n    counts = np.zeros((levels, levels), dtype=float)\n    # Use np.add.at to handle repeated indices\n    np.add.at(counts, (i_vals, j_vals), 1.0)\n    total = counts.sum()\n    if total == 0:\n        return np.zeros((levels, levels), dtype=float)\n    return counts / total\n\ndef texture_features_from_p(p: np.ndarray) -> dict:\n    \"\"\"\n    Compute texture features from normalized joint probability matrix p(i,j).\n    Returns a dict with keys: contrast, homogeneity, energy, entropy, correlation.\n    \"\"\"\n    L = p.shape[0]\n    # Indices grid\n    i_idx = np.arange(L)\n    j_idx = np.arange(L)\n    I, J = np.meshgrid(i_idx, j_idx, indexing='ij')\n\n    # Contrast: E[(I-J)^2]\n    diff_sq = (I - J) ** 2\n    contrast = float((diff_sq * p).sum())\n\n    # Homogeneity: E[1/(1+(I-J)^2)]\n    homogeneity = float((p / (1.0 + diff_sq)).sum())\n\n    # Energy: sum p^2\n    energy = float((p ** 2).sum())\n\n    # Entropy: - sum p log p (natural log), ignoring p=0\n    nonzero = p > 0\n    entropy = float(-(p[nonzero] * np.log(p[nonzero])).sum())\n\n    # Marginals\n    p_i = p.sum(axis=1)\n    p_j = p.sum(axis=0)\n\n    mu_i = float((i_idx * p_i).sum())\n    mu_j = float((j_idx * p_j).sum())\n\n    var_i = float(((i_idx - mu_i) ** 2 * p_i).sum())\n    var_j = float(((j_idx - mu_j) ** 2 * p_j).sum())\n\n    sigma_i = np.sqrt(var_i)\n    sigma_j = np.sqrt(var_j)\n\n    # Numerator for correlation: sum (i - mu_i)(j - mu_j) p(i,j)\n    corr_num = float(((I - mu_i) * (J - mu_j) * p).sum())\n    denom = sigma_i * sigma_j\n    if denom == 0.0:\n        correlation = 0.0\n    else:\n        correlation = float(corr_num / denom)\n\n    return {\n        \"contrast\": contrast,\n        \"homogeneity\": homogeneity,\n        \"energy\": energy,\n        \"entropy\": entropy,\n        \"correlation\": correlation\n    }\n\ndef image_checkerboard(size: int) -> np.ndarray:\n    \"\"\"Generate an size x size checkerboard with values {0,1}.\"\"\"\n    x = np.arange(size).reshape(-1, 1)\n    y = np.arange(size).reshape(1, -1)\n    return ((x + y) % 2).astype(int)\n\ndef image_random_ints(h: int, w: int, levels: int, seed: int) -> np.ndarray:\n    \"\"\"Generate h x w random integers in {0, ..., levels-1} with fixed seed.\"\"\"\n    rng = np.random.default_rng(seed)\n    return rng.integers(low=0, high=levels, size=(h, w), endpoint=False).astype(int)\n\ndef image_vertical_stripes(h: int, w: int, stripe_width: int) -> np.ndarray:\n    \"\"\"Generate binary vertical stripes of given width.\"\"\"\n    y = np.arange(w)\n    stripe = ((y // stripe_width) % 2).astype(int)\n    return np.tile(stripe, (h, 1))\n\ndef image_uniform(h: int, w: int, value: int = 1) -> np.ndarray:\n    \"\"\"Generate a uniform image with a given value.\"\"\"\n    return np.full((h, w), value, dtype=int)\n\ndef compute_feature_differences(image: np.ndarray, levels: int, d_small: int = 1, d_large: int = 4) -> list:\n    \"\"\"\n    Compute feature differences f(d_large) - f(d_small) for contrast, homogeneity, energy, entropy, correlation.\n    Returns a list of five floats rounded to six decimal places.\n    \"\"\"\n    p_small = build_glcm_horizontal(image, levels, d_small)\n    p_large = build_glcm_horizontal(image, levels, d_large)\n\n    feats_small = texture_features_from_p(p_small)\n    feats_large = texture_features_from_p(p_large)\n\n    keys = [\"contrast\", \"homogeneity\", \"energy\", \"entropy\", \"correlation\"]\n    diffs = [feats_large[k] - feats_small[k] for k in keys]\n    # Round to six decimals as required\n    diffs = [round(x, 6) for x in diffs]\n    return diffs\n\ndef format_nested_list_no_spaces(nested: list) -> str:\n    \"\"\"\n    Format a nested list (list of lists of floats) into a string with no spaces,\n    as required: [[a1,a2,...],[b1,b2,...],...]\n    \"\"\"\n    inner_strs = []\n    for inner in nested:\n        inner_str = \",\".join(f\"{v:.6f}\" if isinstance(v, float) else str(v) for v in inner)\n        inner_strs.append(f\"[{inner_str}]\")\n    return f\"[{','.join(inner_strs)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (image_generator_function, parameters_dict, levels)\n        (\"checkerboard\", {\"size\": 8}, 2),\n        (\"random\", {\"h\": 32, \"w\": 32, \"levels\": 8, \"seed\": 42}, 8),\n        (\"vertical_stripes\", {\"h\": 16, \"w\": 16, \"stripe_width\": 4}, 2),\n        (\"uniform\", {\"h\": 16, \"w\": 16, \"value\": 1}, 2),\n    ]\n\n    results = []\n    for name, params, levels in test_cases:\n        if name == \"checkerboard\":\n            img = image_checkerboard(params[\"size\"])\n        elif name == \"random\":\n            img = image_random_ints(params[\"h\"], params[\"w\"], params[\"levels\"], params[\"seed\"])\n        elif name == \"vertical_stripes\":\n            img = image_vertical_stripes(params[\"h\"], params[\"w\"], params[\"stripe_width\"])\n        elif name == \"uniform\":\n            img = image_uniform(params[\"h\"], params[\"w\"], params[\"value\"])\n        else:\n            raise ValueError(\"Unknown test case.\")\n\n        # Ensure values are within [0, levels-1]\n        img = img.astype(int)\n        if img.min() < 0 or img.max() >= levels:\n            raise ValueError(\"Image values out of expected range for specified levels.\")\n\n        diffs = compute_feature_differences(img, levels, d_small=1, d_large=4)\n        results.append(diffs)\n\n    # Final print statement in the exact required format (no spaces).\n    print(format_nested_list_no_spaces(results))\n\nsolve()\n```"
        },
        {
            "introduction": "Filter-bank methods offer an alternative and complementary approach to texture analysis, using a set of specialized filters to decompose an image into different texture components. This exercise guides you through the implementation of Laws' texture energy measures, a classic filter-bank technique . You will not only compute these texture features but also empirically investigate the critical concept of illumination invariance by observing how local mean normalization affects feature stability under changing brightness and contrast.",
            "id": "3859964",
            "problem": "You are given a two-dimensional discrete grayscale image represented as a real-valued function $I:\\mathbb{Z}^2\\to\\mathbb{R}$ on a finite grid. Consider the standard Laws' texture measure construction based on one-dimensional kernels of length $5$: $L_5$, $E_5$, $S_5$, $W_5$, and $R_5$. The two-dimensional masks are formed by outer products of these kernels, yielding $25$ masks of size $5\\times 5$. For each mask $M$, define the linear response $R_M$ as the two-dimensional discrete convolution of the image with the mask, written as\n$$\nR_M(x,y) = \\sum_{u=-2}^{2}\\sum_{v=-2}^{2} I(x-u,y-v)\\,M(u,v),\n$$\nand the windowed energy $E_M$ as the sum of squared responses over a square window $\\mathcal{W}_{w}(x,y)$ of side length $w$ centered at $(x,y)$,\n$$\nE_M(x,y) = \\sum_{(i,j)\\in \\mathcal{W}_{w}(x,y)} \\left( R_M(i,j)\\right)^2.\n$$\nYou will implement a program that computes, for a given image and window size $w$, the spatial average of the windowed energy for each mask, aggregating to a $25$-dimensional feature vector of global energies, one per mask. To study illumination invariance, you will investigate two forms of image formation changes: an additive bias and a multiplicative scale. You will also consider zero-mean normalization performed locally by subtracting the local mean within the same window size $w$ prior to filtering. Formally, the locally zero-mean normalized image $\\tilde{I}$ is\n$$\n\\tilde{I}(x,y) = I(x,y) - \\frac{1}{w^2}\\sum_{(i,j)\\in \\mathcal{W}_{w}(x,y)} I(i,j).\n$$\nYou will quantify invariance for a pair of images $I_\\text{base}$ and $I_\\text{mod}$ through the maximum relative deviation across masks,\n$$\n\\Delta = \\max_{M} \\frac{\\left| \\bar{E}_M(I_\\text{mod}) - \\bar{E}_M(I_\\text{base}) \\right|}{\\left|\\bar{E}_M(I_\\text{base})\\right| + \\varepsilon},\n$$\nwhere $\\bar{E}_M$ denotes the spatial average of $E_M$ over all pixel locations, and $\\varepsilon$ is a small positive number to avoid division by zero. The objective is to compute $\\Delta$ for several test cases, with and without zero-mean normalization, and thereby empirically examine the effect of zero-mean normalization on illumination invariance.\n\nFundamental base and definitions to be used:\n- Discrete convolution and linear shift-invariant filtering: the convolution operator is linear and commutes with translations, and the mask response is obtained by the sum of products of the neighborhood intensities with the mask coefficients.\n- Energy as squared magnitude aggregated over a window: squaring the filter response and summing over a spatial support quantifies local texture strength, rooted in the notion of power in linear systems.\n- Local mean as a smoothing operator: the local mean over a window is the output of convolution with a normalized box filter.\n\nYour program must implement the following tasks strictly from these bases:\n1. Construct the one-dimensional Laws' kernels $L_5=\\left[1,4,6,4,1\\right]$, $E_5=\\left[-1,-2,0,2,1\\right]$, $S_5=\\left[-1,0,2,0,-1\\right]$, $W_5=\\left[-1,2,0,-2,1\\right]$, and $R_5=\\left[1,-4,6,-4,1\\right]$. Form all $25$ two-dimensional masks by outer products of the one-dimensional kernels.\n2. For a given image and window size $w$, compute the spatially averaged windowed energy for each mask by convolving the image with each mask, squaring the response, and convolving with a $w\\times w$ all-ones window to obtain $E_M(x,y)$, then averaging $E_M$ over all $(x,y)$.\n3. Implement local zero-mean normalization by subtracting from the image its local mean computed with a $w\\times w$ uniform kernel, and recompute the energies accordingly when the normalization flag is set.\n4. For each test case, compute the maximum relative deviation $\\Delta$ between the global energy vectors of a base image and a modified image.\n\nTest suite:\n- Case $1$ (additive bias without normalization): base image size $64\\times 64$ defined by\n$$\nI_\\text{base}(x,y) = 0.5\\sin\\left(\\frac{2\\pi x}{8}\\right) + 0.3\\cos\\left(\\frac{2\\pi y}{16}\\right) + 0.2\\sin\\left(\\frac{2\\pi(x+y)}{10}\\right),\n$$\nmodified image $I_\\text{mod}(x,y)=I_\\text{base}(x,y)+0.3$, window size $w=15$, zero-mean normalization disabled.\n- Case $2$ (additive bias with normalization): same $I_\\text{base}$ and $I_\\text{mod}$ as Case $1$, window size $w=15$, zero-mean normalization enabled.\n- Case $3$ (multiplicative scale without normalization): $I_\\text{mod}(x,y)=2.0\\,I_\\text{base}(x,y)$, window size $w=15$, zero-mean normalization disabled.\n- Case $4$ (multiplicative scale with normalization): same as Case $3$ but zero-mean normalization enabled.\n- Case $5$ (constant image with additive bias and normalization): $I_\\text{base}(x,y)=0.2$ for all $(x,y)$ on a $64\\times 64$ grid, $I_\\text{mod}(x,y)=I_\\text{base}(x,y)+0.1$, window size $w=15$, zero-mean normalization enabled.\n\nNumerical and output requirements:\n- Use $\\varepsilon=10^{-12}$ as the small positive number in the relative deviation formula.\n- Angles in trigonometric functions are to be in radians.\n- All outputs must be expressed as unitless real numbers.\n- For each case, the program must compute a single real number $\\Delta$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), in the order of the cases $1$ through $5$.",
            "solution": "The problem requires the implementation and analysis of Laws' texture measures to empirically study their invariance properties with respect to simple illumination changes, namely additive bias and multiplicative scaling. The analysis involves comparing feature vectors derived from a base image and a modified image, with and without a preceding local zero-mean normalization step. The solution is constructed by following the definitions and tasks provided.\n\nThe core of Laws' method lies in a set of one-dimensional, zero-sum kernels of length $5$, which are designed to detect basic structural patterns like levels, edges, spots, waves, and ripples. The five fundamental kernels are:\n- Level: $L_5 = [1, 4, 6, 4, 1]$\n- Edge: $E_5 = [-1, -2, 0, 2, 1]$\n- Spot: $S_5 = [-1, 0, 2, 0, -1]$\n- Wave: $W_5 = [-1, 2, 0, -2, 1]$\n- Ripple: $R_5 = [1, -4, 6, -4, 1]$\n\nFrom these five one-dimensional kernels, a set of $5 \\times 5 = 25$ two-dimensional convolution masks, $M_{ab}$, are generated by computing the outer product of pairs of 1D kernels, $M_{ab} = k_a^T k_b$, where $k_a, k_b \\in \\{L_5, E_5, S_5, W_5, R_5\\}$.\n\nFor each test case, we must process a base image $I_\\text{base}$ and a modified image $I_\\text{mod}$ to produce a $25$-dimensional feature vector for each. This vector consists of the spatially averaged windowed energy, $\\bar{E}_M$, for each of the $25$ masks. The procedure for calculating this feature vector for a given image $I$ is as follows.\n\nFirst, an optional local zero-mean normalization may be applied. When this option is enabled, the original image $I$ is transformed into a normalized image $\\tilde{I}$. This normalization is designed to remove local variations in brightness. For a given window size $w$, the local mean at each pixel $(x,y)$ is calculated by averaging the pixel values within a $w \\times w$ window centered at $(x,y)$. The normalized image is then given by:\n$$\n\\tilde{I}(x,y) = I(x,y) - \\frac{1}{w^2}\\sum_{(i,j)\\in \\mathcal{W}_{w}(x,y)} I(i,j)\n$$\nThis operation is efficiently implemented by convolving the image $I$ with a normalized $w \\times w$ box filter, $K_\\text{avg}$, where each element is $1/w^2$, and subtracting the resulting local-mean image from the original image: $\\tilde{I} = I - (I * K_\\text{avg})$. If normalization is disabled, the original image $I$ is used directly in the subsequent steps. Let's denote the image being processed (either $I$ or $\\tilde{I}$) as $I'$.\n\nNext, for each of the $25$ masks $M$, the following steps are performed to compute $\\bar{E}_M(I')$:\n1.  **Filtering**: The image $I'$ is convolved with the mask $M$ to produce a response image $R_M$. This is a linear filtering operation defined as:\n    $$\n    R_M(x,y) = (I' * M)(x,y) = \\sum_{u=-2}^{2}\\sum_{v=-2}^{2} I'(x-u,y-v)\\,M(u,v)\n    $$\n2.  **Energy Calculation**: The response image $R_M$ is squared element-wise to produce a raw energy map, $(R_M(x,y))^2$.\n3.  **Windowed Aggregation**: The squared responses are summed over a $w \\times w$ window $\\mathcal{W}_w$ to produce the windowed energy map, $E_M$.\n    $$\n    E_M(x,y) = \\sum_{(i,j)\\in \\mathcal{W}_{w}(x,y)} \\left( R_M(i,j)\\right)^2\n    $$\n    This summation is computationally equivalent to convolving the squared response map with a $w \\times w$ kernel of all ones, let's call it $K_\\text{sum}$: $E_M = (R_M)^2 * K_\\text{sum}$.\n4.  **Spatial Averaging**: The final feature for the mask $M$, denoted $\\bar{E}_M$, is the spatial average of the windowed energy map $E_M(x,y)$ over all pixel coordinates $(x,y)$ in the image grid.\n\nThis process yields two $25$-dimensional feature vectors, one for $I_\\text{base}$ and one for $I_\\text{mod}$.\n\nFinally, to quantify the invariance of the features, the maximum relative deviation $\\Delta$ between the two feature vectors is computed. Let $\\bar{E}_M(I_\\text{base})$ and $\\bar{E}_M(I_\\text{mod})$ be the features for a given mask $M$. The deviation is defined as:\n$$\n\\Delta = \\max_{M} \\frac{\\left| \\bar{E}_M(I_\\text{mod}) - \\bar{E}_M(I_\\text{base}) \\right|}{\\left|\\bar{E}_M(I_\\text{base})\\right| + \\varepsilon}\n$$\nwhere $\\varepsilon=10^{-12}$ is a small constant to prevent division by zero, particularly for masks that might yield zero energy.\n\nAll convolution operations are performed such that the output image has the same dimensions as the input image. To handle boundaries, a symmetric padding scheme is employed, which is a standard choice for minimizing edge artifacts in image processing. The implementation will process each of the five specified test cases and calculate the corresponding $\\Delta$ value.",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def compute_features(img, w, normalize):\n        \"\"\"\n        Computes the 25-dimensional Laws' texture energy feature vector for a given image.\n\n        Args:\n            img (np.ndarray): The input 2D image.\n            w (int): The window size for energy aggregation and local mean.\n            normalize (bool): Flag to enable/disable local zero-mean normalization.\n\n        Returns:\n            np.ndarray: A 25-element vector of spatially averaged texture energies.\n        \"\"\"\n        if normalize:\n            # Local zero-mean normalization\n            local_mean_kernel = np.ones((w, w)) / (w * w)\n            local_mean = convolve2d(img, local_mean_kernel, mode='same', boundary='symm')\n            proc_img = img - local_mean\n        else:\n            proc_img = img\n\n        # Define 1D Laws' kernels\n        L5 = np.array([1, 4, 6, 4, 1])\n        E5 = np.array([-1, -2, 0, 2, 1])\n        S5 = np.array([-1, 0, 2, 0, -1])\n        W5 = np.array([-1, 2, 0, -2, 1])\n        R5 = np.array([1, -4, 6, -4, 1])\n        kernels_1d = [L5, E5, S5, W5, R5]\n\n        # Generate 2D masks from outer products\n        masks_2d = [np.outer(k1, k2) for k1 in kernels_1d for k2 in kernels_1d]\n\n        energy_vector = []\n        window_sum_kernel = np.ones((w, w))\n\n        for mask in masks_2d:\n            # Step 1: Convolve image with mask\n            response = convolve2d(proc_img, mask, mode='same', boundary='symm')\n            \n            # Step 2: Square the response\n            response_sq = np.square(response)\n            \n            # Step 3: Compute windowed energy via convolution\n            energy_map = convolve2d(response_sq, window_sum_kernel, mode='same', boundary='symm')\n            \n            # Step 4: Spatially average the energy\n            avg_energy = np.mean(energy_map)\n            energy_vector.append(avg_energy)\n\n        return np.array(energy_vector)\n\n    # Common parameters for test cases\n    size = 64\n    w = 15\n    epsilon = 1e-12\n\n    # Generate base images\n    y, x = np.meshgrid(np.arange(size), np.arange(size), indexing='ij')\n    I_base_sinusoid = (0.5 * np.sin(2 * np.pi * x / 8) +\n                       0.3 * np.cos(2 * np.pi * y / 16) +\n                       0.2 * np.sin(2 * np.pi * (x + y) / 10))\n    I_base_constant = np.full((size, size), 0.2)\n\n    # Define test cases\n    test_cases = [\n        # Case 1: Additive bias, no normalization\n        {'I_base': I_base_sinusoid, 'I_mod': I_base_sinusoid + 0.3, 'w': w, 'normalize': False},\n        # Case 2: Additive bias, with normalization\n        {'I_base': I_base_sinusoid, 'I_mod': I_base_sinusoid + 0.3, 'w': w, 'normalize': True},\n        # Case 3: Multiplicative scale, no normalization\n        {'I_base': I_base_sinusoid, 'I_mod': I_base_sinusoid * 2.0, 'w': w, 'normalize': False},\n        # Case 4: Multiplicative scale, with normalization\n        {'I_base': I_base_sinusoid, 'I_mod': I_base_sinusoid * 2.0, 'w': w, 'normalize': True},\n        # Case 5: Constant image, additive bias, with normalization\n        {'I_base': I_base_constant, 'I_mod': I_base_constant + 0.1, 'w': w, 'normalize': True},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Compute feature vectors for base and modified images\n        E_base = compute_features(case['I_base'], case['w'], case['normalize'])\n        E_mod = compute_features(case['I_mod'], case['w'], case['normalize'])\n        \n        # Calculate maximum relative deviation Delta\n        numerator = np.abs(E_mod - E_base)\n        denominator = np.abs(E_base) + epsilon\n        relative_deviations = numerator / denominator\n        delta = np.max(relative_deviations)\n        results.append(delta)\n\n    # Format the final output string\n    output_str = f\"[{','.join(f'{r:.10f}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world texture analysis often generates a large number of correlated features, leading to high dimensionality and redundancy. This practice addresses the essential task of feature space analysis and reduction using Principal Component Analysis (PCA) . You will apply PCA to a set of texture features to identify the principal modes of variation, interpret these components in terms of underlying texture processes like roughness and periodicity, and ultimately propose a compact, minimally redundant feature set for efficient modeling.",
            "id": "3860042",
            "problem": "A researcher is investigating texture measures in remote sensing imagery and seeks a principled reduction of a high-dimensional feature set. Consider a data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ containing $p$ texture features measured across $n$ image patches. The objective is to apply Principal Component Analysis (PCA) to standardized features, interpret principal components in terms of underlying texture processes, and propose a reduced feature set that preserves variance and minimizes redundancy.\n\nFoundational base:\n- Each feature vector $\\mathbf{x}_i \\in \\mathbb{R}^n$ is standardized to a zero-mean, unit-variance vector $\\mathbf{z}_i$ via $z_{ti} = (x_{ti} - \\mu_i)/\\sigma_i$ where $\\mu_i$ is the sample mean and $\\sigma_i$ is the sample standard deviation of feature $i$. Features with sample variance below a small threshold $\\delta$ are treated as zero-variance and must be excluded.\n- The sample covariance of standardized features is the sample correlation matrix $\\mathbf{S} = \\frac{1}{n-1} \\mathbf{Z}^\\top \\mathbf{Z}$, with $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p'}$ the standardized matrix after any exclusions ($p'$ is the number of retained features).\n- Principal components are defined as orthonormal directions $\\{\\mathbf{v}_j\\}$ that maximize projected variance. These are obtained by the eigen-decomposition $\\mathbf{S}\\mathbf{v}_j = \\lambda_j \\mathbf{v}_j$ with eigenvalues ordered $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_{p'} \\ge 0$. The explained variance ratio of component $j$ is $r_j = \\lambda_j / \\sum_{k=1}^{p'} \\lambda_k$.\n- The correlation loading of feature $i$ on principal component $j$ is given by $\\ell_{ij} = \\sqrt{\\lambda_j} v_{ij}$. These loadings quantify how strongly each feature aligns with each principal component.\n\nComponent retention rule:\n- Use the Kaiser criterion to retain components satisfying $\\lambda_j \\ge 1$, because $\\mathbf{S}$ is a correlation matrix of standardized variables. Also enforce a cumulative explained variance threshold $\\tau = 0.9$: retain at least the smallest number $k$ such that $\\sum_{j=1}^{k} r_j \\ge \\tau$. Let $k$ be the maximum of the Kaiser count and the cumulative threshold count, capped at $p'$.\n\nInterpretation mapping:\n- Interpret retained principal components in terms of four texture process categories: roughness, periodicity, smoothness, and randomness. Define a weight vector $\\mathbf{w}^{(c)} \\in \\mathbb{R}^{p}$ for each category $c$ over the original $p$ features, reflecting qualitative associations:\n    - Roughness: positive weights for features like Gray-Level Co-occurrence Matrix contrast, dissimilarity, entropy, and fractal dimension; negative weights for homogeneity, energy, and Local Binary Pattern uniformity.\n    - Periodicity: positive weights for spectral ratio of Fourier magnitudes at dominant frequency; moderate positive weights for homogeneity and Local Binary Pattern uniformity; negative weight for entropy.\n    - Smoothness: the opposite of roughness; positive weights for homogeneity, energy, and Local Binary Pattern uniformity; negative weights for contrast, dissimilarity, entropy, and fractal dimension.\n    - Randomness: positive weight for entropy and negative weight for spectral ratio; other weights neutral.\n- For each retained component $j$, compute category scores $s_c(j) = \\sum_{i \\in \\mathcal{I}} w^{(c)}_i \\cdot |\\ell_{ij}|$, where $\\mathcal{I}$ indexes the retained features (after excluding zero-variance features). Assign component $j$ to the category with the largest score. In the event of ties, break ties deterministically with the priority order roughness $\\rightarrow$ periodicity $\\rightarrow$ smoothness $\\rightarrow$ randomness and map these categories to integer codes $1,2,3,4$ respectively.\n\nReduced feature set proposal:\n- From the retained components, choose a set of representative features with minimal redundancy by selecting, for each component $j$, the retained feature $i$ that maximizes $|\\ell_{ij}|$, skipping any feature already selected by earlier components. Continue selecting the next-highest loading feature until one unique representative has been chosen for each retained component. Report the final set of selected feature indices as zero-based indices with respect to the original $p$ features (excluding any features dropped for zero variance). If fewer than $k$ unique features exist after exclusion, cap the selection accordingly.\n\nYour program must implement this procedure and apply it to the following test suite. The full original feature list (in order) is:\n1. Gray-Level Co-occurrence Matrix contrast\n2. Gray-Level Co-occurrence Matrix dissimilarity\n3. Gray-Level Co-occurrence Matrix homogeneity\n4. Gray-Level Co-occurrence Matrix energy\n5. Gray-Level Co-occurrence Matrix entropy\n6. Fourier spectral ratio at the dominant frequency\n7. Local Binary Pattern uniformity\n8. Fractal dimension estimate\n\nUse the following category weight vectors $\\mathbf{w}^{(c)}$ over the original eight features (in the order above):\n- Roughness ($c=1$): $\\mathbf{w}^{(\\text{roughness})} = [1.0, 1.0, -1.0, -0.8, 0.8, 0.0, -0.6, 0.9]$.\n- Periodicity ($c=2$): $\\mathbf{w}^{(\\text{periodicity})} = [0.0, 0.0, 0.3, 0.0, -0.5, 1.0, 0.3, 0.0]$.\n- Smoothness ($c=3$): $\\mathbf{w}^{(\\text{smoothness})} = [-1.0, -0.8, 1.0, 1.0, -0.5, 0.2, 0.7, -0.5]$.\n- Randomness ($c=4$): $\\mathbf{w}^{(\\text{randomness})} = [0.0, 0.0, 0.0, 0.0, 1.0, -0.5, 0.0, 0.0]$.\n\nTest suite:\n- Case A (balanced latent roughness and periodicity): $n=20$, noise standard deviation $\\sigma=0.05$, seed $s=12345$. Let latent processes $R_t \\sim \\mathcal{N}(0,1)$ and $P_t \\sim \\mathcal{N}(0,1)$ be independent for $t=1,\\dots,n$. Generate features:\n    1. contrast: $0.9 R_t + 0.1 P_t + \\varepsilon_{1t}$,\n    2. dissimilarity: $0.8 R_t + 0.15 P_t + \\varepsilon_{2t}$,\n    3. homogeneity: $-0.75 R_t - 0.1 P_t + \\varepsilon_{3t}$,\n    4. energy: $-0.65 R_t - 0.1 P_t + \\varepsilon_{4t}$,\n    5. entropy: $0.7 R_t + 0.2 P_t + \\varepsilon_{5t}$,\n    6. spectral ratio: $0.1 R_t + 0.9 P_t + \\varepsilon_{6t}$,\n    7. Local Binary Pattern uniformity: $-0.55 R_t + 0.15 P_t + \\varepsilon_{7t}$,\n    8. fractal dimension: $0.6 R_t + 0.05 P_t + \\varepsilon_{8t}$,\n    where $\\varepsilon_{it} \\sim \\mathcal{N}(0,\\sigma^2)$ are independent.\n- Case B (edge case with a zero-variance feature): $n=15$, $\\sigma=0.05$, seed $s=54321$. Generate features as in Case A except set energy to a constant $1.0$ for all samples. This feature must be excluded prior to PCA due to zero sample variance.\n- Case C (high collinearity among features): $n=25$, $\\sigma=0.05$, seed $s=24680$. Generate latent $R_t, P_t$ as in Case A and construct:\n    1. contrast: $0.85 R_t + 0.1 P_t + \\varepsilon_{1t}$,\n    2. dissimilarity: $\\text{contrast} + \\eta_{2t}$ with $\\eta_{2t} \\sim \\mathcal{N}(0,0.01^2)$,\n    3. homogeneity: $-0.7 R_t - 0.1 P_t + \\varepsilon_{3t}$,\n    4. energy: $-\\text{homogeneity} + \\eta_{4t}$ with $\\eta_{4t} \\sim \\mathcal{N}(0,0.01^2)$,\n    5. entropy: $0.65 R_t + 0.2 P_t + \\varepsilon_{5t}$,\n    6. spectral ratio: $0.1 R_t + 0.85 P_t + \\varepsilon_{6t}$,\n    7. Local Binary Pattern uniformity: $0.7 \\cdot \\text{spectral ratio} + \\eta_{7t}$ with $\\eta_{7t} \\sim \\mathcal{N}(0,0.01^2)$,\n    8. fractal dimension: $\\text{entropy} + \\eta_{8t}$ with $\\eta_{8t} \\sim \\mathcal{N}(0,0.01^2)$.\n\nImplementation requirements:\n- Standardize each feature as described. Exclude features with variance less than $\\delta = 10^{-12}$ before PCA.\n- Compute $\\mathbf{S}$, its eigenvalues $\\{\\lambda_j\\}$ and eigenvectors $\\{\\mathbf{v}_j\\}$, sort by descending $\\lambda_j$, and compute correlation loadings $\\ell_{ij} = \\sqrt{\\lambda_j} v_{ij}$.\n- Determine $k$ using the rule described and propose the reduced feature set. Interpret each retained component into integer codes $1$ (roughness), $2$ (periodicity), $3$ (smoothness), or $4$ (randomness).\n- Use zero-based indexing for feature indices relative to the original list of eight features.\n\nFinal output specification:\n- For each test case, your program must output a list containing three elements: the retained component count $k$ (an integer), the list of selected feature indices (a list of integers), and the list of interpretation codes for each retained component ordered by descending explained variance (a list of integers).\n- Your program should produce a single line of output containing the results for all three cases as a comma-separated list enclosed in square brackets, where each per-case result is itself enclosed in square brackets. For example: \"[[k1,[i1,i2,...],[c1,c2,...]], [k2,[...],[...]], [k3,[...],[...]]]\".\n- No physical units or angles are involved in this task.",
            "solution": "The problem requires the implementation of a structured, multi-stage procedure for dimensionality reduction and interpretation of a texture feature set using Principal Component Analysis (PCA). The methodology is applied to synthetically generated datasets representing typical scenarios in remote sensing image analysis. The entire process can be deconstructed into the following principal steps.\n\n**1. Data Standardization and Pre-processing**\n\nThe initial data is provided in a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, comprising $p=8$ texture features measured for $n$ distinct image patches. To ensure that features with larger variance do not disproportionately influence the PCA, each feature vector is standardized. For each feature $i$, its values $\\{x_{ti}\\}_{t=1}^n$ are transformed into a new feature vector $\\mathbf{z}_i$ with a sample mean of zero and a sample standard deviation of one. The transformation is given by:\n$$\nz_{ti} = \\frac{x_{ti} - \\mu_i}{\\sigma_i}\n$$\nwhere $\\mu_i$ is the sample mean and $\\sigma_i$ is the sample standard deviation (computed with a denominator of $n-1$) of the $i$-th feature.\n\nA critical pre-processing step is the identification and exclusion of features with near-zero variance, as these carry no information and can cause numerical instability. Any feature with a sample variance below a specified threshold $\\delta = 10^{-12}$ is removed from the dataset prior to PCA. This results in a standardized data matrix $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p'}$, where $p' \\le p$ is the number of features retained.\n\n**2. Covariance Analysis via Principal Component Analysis (PCA)**\n\nPCA is applied to uncover the latent structure within the feature set. Since the features have been standardized, their sample covariance matrix is equivalent to their sample correlation matrix, $\\mathbf{S}$. This matrix is computed as:\n$$\n\\mathbf{S} = \\frac{1}{n-1} \\mathbf{Z}^\\top \\mathbf{Z}\n$$\nPCA proceeds by performing an eigen-decomposition of this $p' \\times p'$ symmetric matrix $\\mathbf{S}$:\n$$\n\\mathbf{S}\\mathbf{v}_j = \\lambda_j \\mathbf{v}_j\n$$\nThe eigenvectors $\\{\\mathbf{v}_j\\}_{j=1}^{p'}$ are the principal components, which represent a new set of orthonormal basis vectors for the feature space. The corresponding eigenvalues $\\{\\lambda_j\\}_{j=1}^{p'}$, sorted in descending order $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_{p'} \\ge 0$, quantify the variance of the data when projected onto each principal component. The total variance in the standardized data is $\\sum_{j=1}^{p'} \\lambda_j = \\text{trace}(\\mathbf{S}) = p'$.\n\n**3. Component Retention Criterion**\n\nTo achieve dimensionality reduction, only a subset of $k$ principal components is retained. The value of $k$ is determined by a hybrid rule:\n- **Kaiser Criterion**: Retain all components for which the eigenvalue $\\lambda_j \\ge 1$. The rationale is that since the total variance is $p'$, each original standardized feature contributes a variance of $1$ on average. A principal component with $\\lambda_j < 1$ explains less variance than a single original feature.\n- **Cumulative Explained Variance**: Retain the minimum number of components required to explain at least a proportion $\\tau = 0.9$ of the total variance. The explained variance ratio for component $j$ is $r_j = \\lambda_j / \\sum_{i=1}^{p'} \\lambda_i$. We find the smallest $k'$ such that $\\sum_{j=1}^{k'} r_j \\ge \\tau$.\n\nThe final number of retained components, $k$, is the maximum of the counts determined by these two rules, but not exceeding the total number of available components, $p'$.\n\n**4. Interpretation of Retained Components**\n\nThe retained abstract principal components are interpreted by relating them back to the original features. This is accomplished using correlation loadings, $\\ell_{ij}$, which measure the correlation between the original feature $i$ and the principal component $j$. The loading is calculated as:\n$$\n\\ell_{ij} = \\sqrt{\\lambda_j} v_{ij}\n$$\nwhere $v_{ij}$ is the $i$-th element of the eigenvector $\\mathbf{v}_j$.\n\nFor each of the $k$ retained components, a set of scores is computed to associate it with one of four predefined texture categories: roughness ($c=1$), periodicity ($c=2$), smoothness ($c=3$), and randomness ($c=4$). Each category is defined by a weight vector $\\mathbf{w}^{(c)}$ over the original $p$ features. The score for component $j$ and category $c$ is:\n$$\ns_c(j) = \\sum_{i \\in \\mathcal{I}} w^{(c)}_i \\cdot |\\ell_{ij}|\n$$\nwhere $\\mathcal{I}$ is the set of indices of the $p'$ retained features. The component is assigned to the category with the highest score. Any ties are resolved deterministically using the priority order: roughness $\\rightarrow$ periodicity $\\rightarrow$ smoothness $\\rightarrow$ randomness.\n\n**5. Reduced Feature Set Proposal**\n\nThe final step is to propose a reduced set of original features that are representative of the retained principal components while minimizing redundancy. For each retained component $j$ (from $1$ to $k$ in order of decreasing variance), the algorithm selects the single original feature $i$ that exhibits the highest absolute loading $|\\ell_{ij}|$. To ensure the selected features are not redundant, a feature already chosen to represent a previous component is skipped, and the feature with the next-highest absolute loading for the current component is considered. This process continues until a unique representative feature has been selected for each of the $k$ components, resulting in a final set of $k$ feature indices (reported zero-based).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _format_result(res):\n    \"\"\"Helper to format the list results into a string without spaces.\"\"\"\n    if res is None:\n        return \"None\"\n    k, i_list, c_list = res\n    i_str = f\"[{','.join(map(str, i_list))}]\"\n    c_str = f\"[{','.join(map(str, c_list))}]\"\n    return f\"[{k},{i_str},{c_str}]\"\n\ndef analyze_pca(X, weights, delta=1e-12, tau=0.9):\n    \"\"\"\n    Performs the full PCA analysis pipeline on a data matrix X.\n    \"\"\"\n    n, p = X.shape\n\n    # Step 1: Standardization and Filtering\n    variances = np.var(X, axis=0, ddof=1)\n    retained_mask = variances > delta\n    original_indices = np.arange(p)[retained_mask]\n    \n    if not np.any(retained_mask):\n        return [0, [], []]\n\n    X_filtered = X[:, retained_mask]\n    n_filtered, p_prime = X_filtered.shape\n\n    # Standardize the filtered data using sample statistics (ddof=1)\n    means = np.mean(X_filtered, axis=0)\n    stds = np.std(X_filtered, axis=0, ddof=1)\n    \n    # Avoid division by zero if a std is extremely small\n    stds[stds < delta] = 1.0 \n    Z = (X_filtered - means) / stds\n\n    # Step 2: PCA on the correlation matrix\n    # For data Z standardized with sample stddev, its sample covariance matrix\n    # is the desired correlation matrix.\n    if p_prime == 1:\n        S = np.array([[1.0]])\n    else:\n        # Use ddof=1 for sample covariance matrix (normalization by n-1)\n        S = np.cov(Z, rowvar=False, ddof=1)\n\n    # Eigen-decomposition. eigh returns eigenvalues in ascending order.\n    eigenvalues, eigenvectors = np.linalg.eigh(S)\n    \n    # Sort in descending order\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Step 3: Component Retention\n    # Kaiser criterion\n    k_kaiser = np.sum(eigenvalues >= 1.0)\n\n    # Cumulative variance criterion\n    total_variance = np.sum(eigenvalues)\n    if total_variance < delta:\n        explained_variance_ratio = np.zeros_like(eigenvalues)\n    else:\n        explained_variance_ratio = eigenvalues / total_variance\n    \n    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n    \n    k_tau_candidates = np.where(cumulative_variance_ratio >= tau)[0]\n    if k_tau_candidates.size > 0:\n        k_tau = k_tau_candidates[0] + 1\n    else:\n        k_tau = p_prime\n    \n    k = int(max(k_kaiser, k_tau))\n    k = min(k, p_prime)\n\n    if k == 0:\n        return [0, [], []]\n        \n    retained_lambdas = eigenvalues[:k]\n    retained_vectors = eigenvectors[:, :k]\n\n    # Step 4: Loadings and Interpretation\n    loadings = retained_vectors * np.sqrt(retained_lambdas) # shape (p', k)\n\n    interpretation_codes = []\n    filtered_weights = weights[:, original_indices] # shape (4, p')\n    \n    for j in range(k):\n        component_loadings = np.abs(loadings[:, j])\n        scores = np.sum(filtered_weights * component_loadings, axis=1)\n        \n        # Tie-breaking is handled by argmax due to priority order of weights\n        best_category_idx = np.argmax(scores)\n        interpretation_codes.append(best_category_idx + 1)\n        \n    # Step 5: Feature Selection\n    selected_feature_indices = []\n    used_original_indices = set()\n\n    for j in range(k):\n        component_loadings_abs = np.abs(loadings[:, j])\n        sorted_feature_idx_local = np.argsort(component_loadings_abs)[::-1]\n        \n        for local_idx in sorted_feature_idx_local:\n            original_idx = original_indices[local_idx]\n            if original_idx not in used_original_indices:\n                selected_feature_indices.append(int(original_idx))\n                used_original_indices.add(original_idx)\n                break\n    \n    return [k, selected_feature_indices, interpretation_codes]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    weights = np.array([\n        [1.0, 1.0, -1.0, -0.8, 0.8, 0.0, -0.6, 0.9],   # Roughness (1)\n        [0.0, 0.0, 0.3, 0.0, -0.5, 1.0, 0.3, 0.0],    # Periodicity (2)\n        [-1.0, -0.8, 1.0, 1.0, -0.5, 0.2, 0.7, -0.5], # Smoothness (3)\n        [0.0, 0.0, 0.0, 0.0, 1.0, -0.5, 0.0, 0.0]     # Randomness (4)\n    ])\n    \n    test_cases_params = [\n        {'n': 20, 'sigma': 0.05, 'seed': 12345, 'case_type': 'A'},\n        {'n': 15, 'sigma': 0.05, 'seed': 54321, 'case_type': 'B'},\n        {'n': 25, 'sigma': 0.05, 'seed': 24680, 'case_type': 'C'},\n    ]\n    \n    results = []\n\n    for params in test_cases_params:\n        n, sigma, seed, case_type = params['n'], params['sigma'], params['seed'], params['case_type']\n        rng = np.random.default_rng(seed)\n        \n        X = np.zeros((n, 8))\n        \n        if case_type in ['A', 'B']:\n            R = rng.normal(0, 1, n)\n            P = rng.normal(0, 1, n)\n            epsilons = rng.normal(0, sigma, (n, 8))\n            \n            X[:, 0] = 0.9 * R + 0.1 * P + epsilons[:, 0]\n            X[:, 1] = 0.8 * R + 0.15 * P + epsilons[:, 1]\n            X[:, 2] = -0.75 * R - 0.1 * P + epsilons[:, 2]\n            X[:, 3] = -0.65 * R - 0.1 * P + epsilons[:, 3]\n            X[:, 4] = 0.7 * R + 0.2 * P + epsilons[:, 4]\n            X[:, 5] = 0.1 * R + 0.9 * P + epsilons[:, 5]\n            X[:, 6] = -0.55 * R + 0.15 * P + epsilons[:, 6]\n            X[:, 7] = 0.6 * R + 0.05 * P + epsilons[:, 7]\n\n            if case_type == 'B':\n                X[:, 3] = 1.0\n\n        elif case_type == 'C':\n            R = rng.normal(0, 1, n)\n            P = rng.normal(0, 1, n)\n            epsilons = rng.normal(0, sigma, (n, 8))\n            etas = rng.normal(0, 0.01, (n, 8))\n\n            contrast = 0.85 * R + 0.1 * P + epsilons[:, 0]\n            homogeneity = -0.7 * R - 0.1 * P + epsilons[:, 2]\n            entropy = 0.65 * R + 0.2 * P + epsilons[:, 4]\n            spectral_ratio = 0.1 * R + 0.85 * P + epsilons[:, 5]\n            \n            X[:, 0] = contrast\n            X[:, 1] = contrast + etas[:, 1]\n            X[:, 2] = homogeneity\n            X[:, 3] = -homogeneity + etas[:, 3]\n            X[:, 4] = entropy\n            X[:, 5] = spectral_ratio\n            X[:, 6] = 0.7 * spectral_ratio + etas[:, 6]\n            X[:, 7] = entropy + etas[:, 7]\n            \n        result = analyze_pca(X, weights)\n        results.append(result)\n\n    formatted_results = [_format_result(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}