{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in any quantitative remote sensing workflow is atmospheric correction, which converts the radiance measured by a satellite sensor into the actual reflectance of the Earth's surface. This practice guides you through implementing a physically-based correction using a Lookup Table (LUT), a computationally efficient method at the heart of many operational processing chains. By calculating surface reflectance and its sensitivity to atmospheric aerosols, you will gain a foundational understanding of how we obtain reliable data for classification .",
            "id": "3860408",
            "problem": "You are given a simplified, physically consistent atmospheric correction setup for a single spectral band in passive optical remote sensing. The goal is to compute the bidirectional reflectance factor at wavelength $\\lambda$, denoted $\\rho_\\lambda$, for a pixel with known Top-Of-Atmosphere (TOA) radiance $L_\\lambda^{TOA}$, by interpolating the total atmospheric transmittance $T_\\lambda$ from a Lookup Table (LUT), and then to quantify the local sensitivity of $\\rho_\\lambda$ to aerosol optical thickness via the partial derivative $\\partial \\rho_\\lambda / \\partial \\tau_a$. The interpolation must be trilinear over aerosol optical thickness $\\tau_a$, column water vapor $w$, and solar zenith angle $\\theta_s$.\n\nFundamental base:\n- Under the Lambertian approximation, the bidirectional reflectance factor $\\rho_\\lambda$ is defined by the relation between radiance and irradiance,\n$$\n\\rho_\\lambda \\equiv \\frac{\\pi\\,L_\\lambda}{E_\\lambda^{d}},\n$$\nwhere $L_\\lambda$ is the radiance leaving the surface and $E_\\lambda^{d}$ is the downwelling irradiance at the surface.\n- For TOA measurements in a plane-parallel atmosphere under a single-scattering and negligible path radiance approximation, $L_\\lambda^{TOA}$ is attenuated by an upward transmittance, while the irradiance at the surface is attenuated by a downward transmittance. Denoting the product of downward and upward transmittances by $T_\\lambda$, and taking extraterrestrial solar irradiance $E_{0,\\lambda}$ as the top-of-atmosphere irradiance, we use the well-tested relation $E_\\lambda^{d} = E_{0,\\lambda}\\,\\cos\\theta_s\\,T_\\lambda$ and treat $L_\\lambda^{TOA}$ as a proxy for surface-leaving radiance times the upward transmittance under the stated approximation. This yields the working formula\n$$\n\\rho_\\lambda = \\frac{\\pi\\,L_\\lambda^{TOA}}{E_{0,\\lambda}\\,\\cos\\theta_s\\,T_\\lambda}.\n$$\n\nAtmospheric transmittance model for LUT synthesis:\n- Total transmittance $T_\\lambda$ is modeled via the Beer–Lambert law with slant-path airmasses. Let $m_s \\equiv \\sec\\theta_s$ be the solar airmass and $m_v \\equiv \\sec\\theta_v$ be the sensor-view airmass. Consider a nadir view with $\\theta_v = 0^\\circ$ so that $m_v = 1$. Then\n$$\nT_\\lambda(\\tau_a, w, \\theta_s) = \\exp\\left(-m_s\\left(\\alpha_\\lambda\\,\\tau_a + k_{\\lambda,d}\\,w + \\tau_{R,\\lambda,d}\\right) - m_v\\left(\\beta_\\lambda\\,\\tau_a + k_{\\lambda,u}\\,w + \\tau_{R,\\lambda,u}\\right)\\right),\n$$\nwhere $\\alpha_\\lambda$, $\\beta_\\lambda$ represent aerosol extinction weighting for downwelling and upwelling paths, $k_{\\lambda,d}$ and $k_{\\lambda,u}$ represent effective water vapor extinction coefficients for downwelling and upwelling paths, and $\\tau_{R,\\lambda,d}$, $\\tau_{R,\\lambda,u}$ represent Rayleigh optical thickness contributions for downwelling and upwelling paths, respectively.\n\nConstants for LUT synthesis at the given band:\n- $\\alpha_\\lambda = 0.3$, $\\beta_\\lambda = 0.15$,\n- $k_{\\lambda,d} = 0.004$, $k_{\\lambda,u} = 0.002$,\n- $\\tau_{R,\\lambda,d} = 0.02$, $\\tau_{R,\\lambda,u} = 0.01$,\n- $\\theta_v = 0^\\circ$ so $m_v = 1$.\n\nGrids defining the LUT:\n- Aerosol optical thickness grid (dimensionless): $\\tau_a \\in \\{\\,0.0,\\,0.2,\\,0.5,\\,1.0\\,\\}$,\n- Column water vapor grid (centimeters of precipitable water): $w \\in \\{\\,0.5,\\,1.5,\\,3.0,\\,5.0\\,\\}$,\n- Solar zenith angle grid (degrees): $\\theta_s \\in \\{\\,0,\\,30,\\,60,\\,75\\,\\}$.\n\nExtraterrestrial solar irradiance:\n- Use $E_{0,\\lambda} = 1850$ in units of W·m$^{-2}$·µm$^{-1}$.\n\nUnits and angle conventions:\n- Radiance $L_\\lambda^{TOA}$ must be provided in W·m$^{-2}$·sr$^{-1}$·µm$^{-1}$.\n- Column water vapor $w$ must be provided in centimeters of precipitable water.\n- Aerosol optical thickness $\\tau_a$ is dimensionless.\n- Solar zenith angle $\\theta_s$ must be specified in degrees; conversions to radians for trigonometric functions are required internally.\n- The reflectance $\\rho_\\lambda$ is dimensionless.\n- The sensitivity $\\partial \\rho_\\lambda / \\partial \\tau_a$ is dimensionless per unit aerosol optical thickness.\n- Express all angles in degrees on input; internally use radians when evaluating $\\cos\\theta_s$ and $\\sec\\theta_s$.\n\nSensitivity definition:\n- Using the chain rule, the sensitivity at a given state $(\\tau_a, w, \\theta_s)$ is\n$$\n\\frac{\\partial \\rho_\\lambda}{\\partial \\tau_a} = -\\frac{\\pi\\,L_\\lambda^{TOA}}{E_{0,\\lambda}\\,\\cos\\theta_s}\\,\\frac{1}{T_\\lambda(\\tau_a,w,\\theta_s)^2}\\,\\frac{\\partial T_\\lambda}{\\partial \\tau_a},\n$$\nwhere $\\partial T_\\lambda/\\partial \\tau_a$ must be obtained consistently with the LUT interpolation. To ensure generality with respect to LUT construction, compute $\\partial T_\\lambda/\\partial \\tau_a$ numerically by finite differences applied to the interpolated function $T_\\lambda(\\tau_a,w,\\theta_s)$, using a small step $\\delta\\tau_a$ on the $\\tau_a$ axis:\n- If $\\tau_a$ is strictly interior to the LUT domain, use a centered difference with step $\\delta\\tau_a$:\n$$\n\\frac{\\partial T_\\lambda}{\\partial \\tau_a} \\approx \\frac{T_\\lambda(\\tau_a+\\delta\\tau_a,w,\\theta_s) - T_\\lambda(\\tau_a-\\delta\\tau_a,w,\\theta_s)}{2\\,\\delta\\tau_a}.\n$$\n- If $\\tau_a$ is at the lower boundary, use a forward difference:\n$$\n\\frac{\\partial T_\\lambda}{\\partial \\tau_a} \\approx \\frac{T_\\lambda(\\tau_a+\\delta\\tau_a,w,\\theta_s) - T_\\lambda(\\tau_a,w,\\theta_s)}{\\delta\\tau_a}.\n$$\n- If $\\tau_a$ is at the upper boundary, use a backward difference:\n$$\n\\frac{\\partial T_\\lambda}{\\partial \\tau_a} \\approx \\frac{T_\\lambda(\\tau_a,w,\\theta_s) - T_\\lambda(\\tau_a-\\delta\\tau_a,w,\\theta_s)}{\\delta\\tau_a}.\n$$\nChoose $\\delta\\tau_a = 10^{-3}$.\n\nAlgorithmic requirements:\n- Synthesize the LUT by evaluating $T_\\lambda(\\tau_a,w,\\theta_s)$ on the specified grids using the provided constants and the Beer–Lambert expression. Interpolate $T_\\lambda$ at arbitrary $(\\tau_a,w,\\theta_s)$ using trilinear interpolation over the regular grid.\n- For each test case, compute $\\rho_\\lambda$ using the interpolated $T_\\lambda$, and compute $\\partial \\rho_\\lambda / \\partial \\tau_a$ using finite differences in $\\tau_a$ applied to the interpolated $T_\\lambda$ as described above.\n\nTest suite:\nProvide results for the following five test cases, each given as a tuple $(L_\\lambda^{TOA}, \\tau_a, w, \\theta_s)$ with units and conventions as specified:\n- Case $1$: $(80.0,\\;0.3,\\;2.0,\\;40)$,\n- Case $2$: $(50.0,\\;0.0,\\;1.0,\\;0)$,\n- Case $3$: $(30.0,\\;1.0,\\;5.0,\\;75)$,\n- Case $4$: $(60.0,\\;0.5,\\;4.0,\\;60)$,\n- Case $5$: $(70.0,\\;0.2,\\;1.5,\\;30)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of lists, where each inner list contains two floats $[\\rho_\\lambda,\\;\\partial \\rho_\\lambda/\\partial \\tau_a]$ corresponding to one test case, in the same order as specified above. For example, the output should have the form\n$$\n\\big[\\, [\\rho_1,\\,d\\rho_1],\\;[\\rho_2,\\,d\\rho_2],\\;[\\rho_3,\\,d\\rho_3],\\;[\\rho_4,\\,d\\rho_4],\\;[\\rho_5,\\,d\\rho_5]\\,\\big],\n$$\nwith numeric values replacing the symbolic entries.",
            "solution": "The problem requires the computation of the surface bidirectional reflectance factor, denoted $\\rho_\\lambda$, and its sensitivity to aerosol optical thickness, $\\partial \\rho_\\lambda / \\partial \\tau_a$, for a set of atmospheric and viewing conditions. The methodology is based on a simplified atmospheric radiative transfer model, where the total atmospheric transmittance $T_\\lambda$ is pre-computed and stored in a Lookup Table (LUT). This LUT is then used for rapid interpolation of $T_\\lambda$ for arbitrary conditions, from which $\\rho_\\lambda$ and its derivative are derived. The process is outlined in the following steps.\n\n**Step 1: Synthesis of the Transmittance Lookup Table (LUT)**\n\nThe foundation of this method is the accurate and efficient calculation of the total atmospheric transmittance, $T_\\lambda$. The problem provides a physically-based model for $T_\\lambda$ grounded in the Beer–Lambert law, which describes the attenuation of light as it passes through a medium. The total transmittance is given by the product of the transmittance along the sun-to-surface path and the surface-to-sensor path. It is a function of three variables: aerosol optical thickness ($\\tau_a$), column water vapor ($w$), and solar zenith angle ($\\theta_s$).\n\nThe model is expressed as:\n$$\nT_\\lambda(\\tau_a, w, \\theta_s) = \\exp\\left(-m_s\\left(\\alpha_\\lambda\\,\\tau_a + k_{\\lambda,d}\\,w + \\tau_{R,\\lambda,d}\\right) - m_v\\left(\\beta_\\lambda\\,\\tau_a + k_{\\lambda,u}\\,w + \\tau_{R,\\lambda,u}\\right)\\right)\n$$\nwhere $m_s = \\sec\\theta_s$ is the solar airmass and $m_v = \\sec\\theta_v$ is the sensor-view airmass. The problem specifies a nadir-viewing sensor, for which $\\theta_v = 0^\\circ$ and thus $m_v = 1$. The constants for the given spectral band $\\lambda$ are provided: $\\alpha_\\lambda = 0.3$, $\\beta_\\lambda = 0.15$, $k_{\\lambda,d} = 0.004$, $k_{\\lambda,u} = 0.002$, $\\tau_{R,\\lambda,d} = 0.02$, and $\\tau_{R,\\lambda,u} = 0.01$.\n\nSubstituting these values, the specific transmittance function to be used is:\n$$\nT_\\lambda(\\tau_a, w, \\theta_s) = \\exp\\left( - \\frac{1}{\\cos\\theta_s} \\left(0.3\\,\\tau_a + 0.004\\,w + 0.02\\right) - \\left(0.15\\,\\tau_a + 0.002\\,w + 0.01\\right) \\right)\n$$\nIt is imperative to note that the trigonometric function $\\cos\\theta_s$ must be evaluated with $\\theta_s$ in radians.\n\nThe LUT is constructed by evaluating this $T_\\lambda$ function at each node of a three-dimensional grid. The grid axes are defined by the given sets of values:\n-   Aerosol optical thickness: $\\tau_a \\in \\{\\,0.0,\\,0.2,\\,0.5,\\,1.0\\,\\}$\n-   Column water vapor: $w \\in \\{\\,0.5,\\,1.5,\\,3.0,\\,5.0\\,\\}$\n-   Solar zenith angle: $\\theta_s \\in \\{\\,0,\\,30,\\,60,\\,75\\,\\}$ degrees\n\nThis results in a $4 \\times 4 \\times 4$ LUT storing the values of $T_\\lambda$.\n\n**Step 2: Trilinear Interpolation of Transmittance $T_\\lambda$**\n\nFor a given test case with arbitrary parameters $(\\tau_{a,q}, w_q, \\theta_{s,q})$, the corresponding transmittance $T_\\lambda$ must be interpolated from the discrete values in the LUT. The specified method is trilinear interpolation. This method treats the query point as being located within a rectilinear grid cell (a cuboid) defined by the $8$ nearest grid points. The interpolated value is a weighted average of the values at these $8$ corners, where the weights are determined by the point's relative position within the cuboid.\n\nLet the query point be $(x_q, y_q, z_q)$ and the surrounding grid cell be defined by corners $(x_i, y_j, z_k)$ and $(x_{i+1}, y_{j+1}, z_{k+1})$. First, we determine the fractional distances of the query point along each axis:\n$$\nd_x = \\frac{x_q - x_i}{x_{i+1} - x_i}, \\quad d_y = \\frac{y_q - y_j}{y_{j+1} - y_j}, \\quad d_z = \\frac{z_q - z_k}{z_{k+1} - z_k}\n$$\nThe value $V$ at the query point is then found by a sequence of linear interpolations. For instance, one can first interpolate along the $x$-axis at the four corners of the cell's base in the $y-z$ plane, then interpolate these four results along the $y$-axis to obtain two values, and finally interpolate these two values along the $z$-axis. Computationally, this is efficiently handled by libraries equipped for multivariate interpolation on regular grids, even if the grid spacing is non-uniform, as is the case here.\n\n**Step 3: Calculation of Surface Reflectance $\\rho_\\lambda$**\n\nOnce the interpolated transmittance $T_\\lambda(\\tau_{a,q}, w_q, \\theta_{s,q})$ is obtained, the surface reflectance $\\rho_\\lambda$ is calculated using the provided simplified radiative transfer equation:\n$$\n\\rho_\\lambda = \\frac{\\pi\\,L_\\lambda^{TOA}}{E_{0,\\lambda}\\,\\cos\\theta_s\\,T_\\lambda}\n$$\nHere, $L_\\lambda^{TOA}$ is the measured Top-Of-Atmosphere radiance for the given pixel, and $E_{0,\\lambda}$ is the extraterrestrial solar irradiance, given as $1850$ W·m$^{-2}$·µm$^{-1}$. As before, $\\theta_s$ must be in radians for the cosine function.\n\n**Step 4: Calculation of Reflectance Sensitivity $\\partial \\rho_\\lambda / \\partial \\tau_a$**\n\nThe final step is to quantify the local sensitivity of the calculated reflectance to changes in aerosol optical thickness. This is given by the partial derivative $\\partial \\rho_\\lambda / \\partial \\tau_a$. Applying the chain rule to the expression for $\\rho_\\lambda$:\n$$\n\\frac{\\partial \\rho_\\lambda}{\\partial \\tau_a} = \\frac{\\partial}{\\partial \\tau_a} \\left( \\frac{\\pi\\,L_\\lambda^{TOA}}{E_{0,\\lambda}\\,\\cos\\theta_s} \\cdot T_\\lambda^{-1} \\right) = \\frac{\\pi\\,L_\\lambda^{TOA}}{E_{0,\\lambda}\\,\\cos\\theta_s} \\left( -T_\\lambda^{-2} \\frac{\\partial T_\\lambda}{\\partial \\tau_a} \\right)\n$$\nThis can be expressed more concisely by substituting the definition of $\\rho_\\lambda$:\n$$\n\\frac{\\partial \\rho_\\lambda}{\\partial \\tau_a} = -\\frac{\\rho_\\lambda}{T_\\lambda} \\frac{\\partial T_\\lambda}{\\partial \\tau_a}\n$$\nThe derivative of the transmittance, $\\partial T_\\lambda / \\partial \\tau_a$, is computed numerically using a finite difference scheme applied to the *interpolated* transmittance function. The choice of scheme depends on the position of the query point $\\tau_{a,q}$ relative to the boundaries of the $\\tau_a$ grid $[0.0, 1.0]$. A small perturbation $\\delta\\tau_a = 10^{-3}$ is used.\n\n-   If $\\tau_{a,q}$ is an interior point ($0.0 < \\tau_{a,q} < 1.0$), a second-order accurate centered difference is used:\n    $$\n    \\frac{\\partial T_\\lambda}{\\partial \\tau_a} \\approx \\frac{T_\\lambda(\\tau_a+\\delta\\tau_a,w,\\theta_s) - T_\\lambda(\\tau_a-\\delta\\tau_a,w,\\theta_s)}{2\\,\\delta\\tau_a}\n    $$\n-   If $\\tau_{a,q}$ is at the lower boundary ($\\tau_{a,q} = 0.0$), a first-order forward difference is required:\n    $$\n    \\frac{\\partial T_\\lambda}{\\partial \\tau_a} \\approx \\frac{T_\\lambda(\\tau_a+\\delta\\tau_a,w,\\theta_s) - T_\\lambda(\\tau_a,w,\\theta_s)}{\\delta\\tau_a}\n    $$\n-   If $\\tau_{a,q}$ is at the upper boundary ($\\tau_{a,q} = 1.0$), a first-order backward difference is used:\n    $$\n    \\frac{\\partial T_\\lambda}{\\partial \\tau_a} \\approx \\frac{T_\\lambda(\\tau_a,w,\\theta_s) - T_\\lambda(\\tau_a-\\delta\\tau_a,w,\\theta_s)}{\\delta\\tau_a}\n    $$\nEach evaluation of $T_\\lambda$ in these formulas is performed using the trilinear interpolation function established in Step 2. With $\\partial T_\\lambda / \\partial \\tau_a$ determined, the final sensitivity $\\partial \\rho_\\lambda / \\partial \\tau_a$ is readily calculated. This completes the entire prescribed algorithm.",
            "answer": "```python\nimport numpy as np\nfrom scipy.interpolate import RegularGridInterpolator\n\ndef solve():\n    \"\"\"\n    Computes surface reflectance and its sensitivity to aerosol optical thickness\n    using a LUT-based atmospheric correction approach.\n    \"\"\"\n\n    # --- Problem Constants ---\n    # Atmospheric model parameters\n    ALPHA_LAMBDA = 0.3\n    BETA_LAMBDA = 0.15\n    K_LAMBDA_D = 0.004\n    K_LAMBDA_U = 0.002\n    TAU_R_LAMBDA_D = 0.02\n    TAU_R_LAMBDA_U = 0.01\n\n    # View geometry\n    THETA_V_DEG = 0.0\n    M_V = 1.0 / np.cos(np.deg2rad(THETA_V_DEG)) # sec(theta_v)\n\n    # Solar irradiance\n    E0_LAMBDA = 1850.0\n\n    # Finite difference step for sensitivity\n    DELTA_TAU_A = 1e-3\n\n    # --- LUT Grid Definition ---\n    TAU_A_GRID = np.array([0.0, 0.2, 0.5, 1.0])\n    W_GRID = np.array([0.5, 1.5, 3.0, 5.0])\n    THETA_S_GRID = np.array([0.0, 30.0, 60.0, 75.0])\n    GRIDS = (TAU_A_GRID, W_GRID, THETA_S_GRID)\n    \n    # --- Test Cases ---\n    test_cases = [\n        # (L_toa, tau_a, w, theta_s)\n        (80.0, 0.3, 2.0, 40.0),\n        (50.0, 0.0, 1.0, 0.0),\n        (30.0, 1.0, 5.0, 75.0),\n        (60.0, 0.5, 4.0, 60.0),\n        (70.0, 0.2, 1.5, 30.0),\n    ]\n\n    def transmittance_model(tau_a, w, theta_s_deg):\n        \"\"\"\n        Calculates atmospheric transmittance based on the provided model.\n        \"\"\"\n        # Convert solar zenith angle to radians for trig functions\n        theta_s_rad = np.deg2rad(theta_s_deg)\n        # Handle the case theta_s = 90 deg, though not in grid\n        cos_theta_s = np.cos(theta_s_rad)\n        if np.isclose(cos_theta_s, 0):\n            # Physical limit, transmittance is zero\n            return 0.0\n        m_s = 1.0 / cos_theta_s  # sec(theta_s)\n        \n        downwelling_exponent = m_s * (ALPHA_LAMBDA * tau_a + K_LAMBDA_D * w + TAU_R_LAMBDA_D)\n        upwelling_exponent = M_V * (BETA_LAMBDA * tau_a + K_LAMBDA_U * w + TAU_R_LAMBDA_U)\n        \n        total_exponent = downwelling_exponent + upwelling_exponent\n        \n        return np.exp(-total_exponent)\n\n    def synthesize_lut(grids):\n        \"\"\"\n        Generates the 3D LUT for transmittance.\n        \"\"\"\n        tau_grid, w_grid, theta_grid = grids\n        lut = np.zeros((len(tau_grid), len(w_grid), len(theta_grid)))\n        \n        for i, tau_a in enumerate(tau_grid):\n            for j, w in enumerate(w_grid):\n                for k, theta_s in enumerate(theta_grid):\n                    lut[i, j, k] = transmittance_model(tau_a, w, theta_s)\n        return lut\n\n    # --- Main Calculation ---\n    \n    # 1. Synthesize the LUT\n    lut_values = synthesize_lut(GRIDS)\n    \n    # 2. Create the trilinear interpolator object\n    interpolator_T = RegularGridInterpolator(GRIDS, lut_values, method='linear')\n\n    results = []\n    for case in test_cases:\n        l_toa, tau_a, w, theta_s = case\n        \n        # 3. Interpolate transmittance T_lambda for the given point\n        query_point = np.array([tau_a, w, theta_s])\n        T_lambda = interpolator_T(query_point)[0]\n\n        # 4. Calculate surface reflectance rho_lambda\n        cos_theta_s = np.cos(np.deg2rad(theta_s))\n        rho_lambda = (np.pi * l_toa) / (E0_LAMBDA * cos_theta_s * T_lambda)\n        \n        # 5. Calculate sensitivity d(rho)/d(tau_a) via numerical derivative of T_lambda\n        if tau_a == TAU_A_GRID[0]:\n            # Forward difference at lower boundary\n            T_plus = interpolator_T([tau_a + DELTA_TAU_A, w, theta_s])[0]\n            T_center = T_lambda\n            dT_dtau = (T_plus - T_center) / DELTA_TAU_A\n        elif tau_a == TAU_A_GRID[-1]:\n            # Backward difference at upper boundary\n            T_minus = interpolator_T([tau_a - DELTA_TAU_A, w, theta_s])[0]\n            T_center = T_lambda\n            dT_dtau = (T_center - T_minus) / DELTA_TAU_A\n        else:\n            # Centered difference for interior points\n            T_plus = interpolator_T([tau_a + DELTA_TAU_A, w, theta_s])[0]\n            T_minus = interpolator_T([tau_a - DELTA_TAU_A, w, theta_s])[0]\n            dT_dtau = (T_plus - T_minus) / (2 * DELTA_TAU_A)\n            \n        drho_dtau = - (rho_lambda / T_lambda) * dT_dtau\n        \n        results.append([rho_lambda, drho_dtau])\n        \n    # --- Final Output ---\n    # Convert list of lists to the required string format\n    # The default string representation of a list of lists matches the desired output format\n    print(str(results).replace(\" \", \"\"))\n\n# We call the main function to execute the logic.\n# The problem template had this structure, but we will call it directly\n# for cleaner scoping according to standard Python practices. The solve()\n# function encapsulates all logic and can be called from the global scope.\n# After further checks, the requested output format is `[[v1,v2],[v3,v4]...]` without spaces\n# `str(results).replace(\" \", \"\")` achieves this robustly.\nsolve()\n```"
        },
        {
            "introduction": "Modern remote sensing datasets often contain a high number of correlated features, making dimensionality reduction a key step in the classification workflow. This exercise delves into Principal Component Analysis (PCA), not just as a tool for data compression, but as a method whose impact must be carefully measured. You will implement PCA and quantify the resulting information loss in terms of class separability, a critical skill for designing efficient and effective feature sets .",
            "id": "3860432",
            "problem": "You are given labeled feature matrices representing remotely sensed observations used for land-cover image classification. Your task is to implement the dimensionality reduction and reconstruction stages of the image classification workflow using Principal Component Analysis (PCA), then quantify the information loss for class separability due to dimensionality reduction. You must base your derivation and algorithm on the following foundational definitions and widely accepted formulas.\n\nFundamental base:\n- Principal Component Analysis (PCA): Given a standardized data matrix $Z \\in \\mathbb{R}^{n \\times d}$ with $n$ samples and $d$ features, the sample covariance matrix is $$C = \\frac{1}{n-1} Z^\\top Z.$$ PCA finds orthonormal eigenvectors of $C$ and their corresponding eigenvalues. If $C P = P \\Lambda$ where $P \\in \\mathbb{R}^{d \\times d}$ has columns that are eigenvectors and $\\Lambda \\in \\mathbb{R}^{d \\times d}$ is diagonal with nonnegative eigenvalues, then the explained variance of the $i$-th principal component is the $i$-th diagonal element of $\\Lambda$. The explained variance ratio of the first $k$ components is $$r_k = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i},$$ where eigenvalues are sorted in nonincreasing order, $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$.\n- Reconstruction from top components: Let $P_k \\in \\mathbb{R}^{d \\times k}$ be the matrix of the top $k$ eigenvectors. The projection scores are $$T_k = Z P_k,$$ and the rank-$k$ reconstruction in standardized space is $$\\hat{Z} = T_k P_k^\\top.$$ If $k = 0$, define $P_0$ to be empty and $\\hat{Z}$ to be the zero matrix of shape $n \\times d$.\n- Fisher’s class scatter ratio for separability: Given class labels $y \\in \\{0, 1, \\dots\\}$, define the overall mean $$\\mu = \\frac{1}{n} \\sum_{i=1}^n z_i,$$ and for each class $c$ the mean $$\\mu_c = \\frac{1}{n_c} \\sum_{i \\in \\mathcal{I}_c} z_i,$$ where $\\mathcal{I}_c$ is the index set of samples in class $c$ and $n_c = |\\mathcal{I}_c|$. Define the within-class scatter trace $$\\mathrm{tr}(S_W) = \\sum_{c} \\sum_{i \\in \\mathcal{I}_c} \\| z_i - \\mu_c \\|_2^2,$$ and the between-class scatter trace $$\\mathrm{tr}(S_B) = \\sum_{c} n_c \\| \\mu_c - \\mu \\|_2^2.$$ The Fisher separability criterion is $$J = \\frac{\\mathrm{tr}(S_B)}{\\mathrm{tr}(S_W)}.$$ If $\\mathrm{tr}(S_W) = 0$, define $J = 0$ to avoid division by zero.\n- Information loss due to reconstruction: For original standardized features $Z$ and reconstructed standardized features $\\hat{Z}$, define the information loss as the relative drop in $J$: $$L = \\begin{cases} \\max\\left(0, \\dfrac{J(Z) - J(\\hat{Z})}{J(Z)} \\right), & \\text{if } J(Z) > 0, \\\\ 0, & \\text{if } J(Z) = 0. \\end{cases}$$\n\nWorkflow to implement:\n1. Standardize each feature of the input matrix $X \\in \\mathbb{R}^{n \\times d}$ to obtain $Z$ with zero mean and unit unbiased standard deviation per feature. Use the sample standard deviation with denominator $(n-1)$ (that is, the unbiased estimator) for scaling; if a feature has zero standard deviation, divide by $1$ instead to avoid division by zero.\n2. Compute $C$, perform the eigen-decomposition of $C$, sort eigenpairs by nonincreasing eigenvalues, and compute cumulative explained variance ratios $r_k$.\n3. Given a target variance fraction $t \\in [0, 1]$ (expressed as a decimal), select the smallest integer $k \\in \\{0,1,\\dots,d\\}$ such that $r_k \\ge t$ (use a numerical tolerance of $10^{-12}$ to account for floating-point rounding). If $t = 0$, let $k = 0$.\n4. Reconstruct $\\hat{Z}$ from the top $k$ components and compute $J(Z)$ and $J(\\hat{Z})$, then compute $L$.\n\nTest suite:\nFor each test case, you are given ($X, y, t$) where $X$ is the feature matrix, $y$ are class labels, and $t$ is the target variance fraction as a decimal.\n\n- Test case $1$ (happy path, moderately high variance target):\n  - $X_1 = \\begin{bmatrix}\n  $0.12$ & $0.38$ & $0.50$ \\\\\n  $0.10$ & $0.35$ & $0.48$ \\\\\n  $0.15$ & $0.40$ & $0.52$ \\\\\n  $0.30$ & $0.55$ & $0.60$ \\\\\n  $0.28$ & $0.50$ & $0.58$ \\\\\n  $0.32$ & $0.57$ & $0.62$\n  \\end{bmatrix}$,\n  $y_1 = [$0$, $0$, $0$, $1$, $1$, $1$],\n  $t_1 = $0.90$.\n- Test case $2$ (boundary case, full variance target):\n  - $X_2 = \\begin{bmatrix}\n  $0.05$ & $0.20$ \\\\\n  $0.07$ & $0.22$ \\\\\n  $0.50$ & $0.45$ \\\\\n  $0.48$ & $0.40$ \\\\\n  $0.52$ & $0.47$\n  \\end{bmatrix}$,\n  $y_2 = [$0$, $0$, $1$, $1$, $1$],\n  $t_2 = $1.00$.\n- Test case $3$ (edge case, zero variance target):\n  - $X_3 = \\begin{bmatrix}\n  $0.20$ & $0.30$ & $0.25$ & $0.35$ \\\\\n  $0.22$ & $0.32$ & $0.27$ & $0.36$ \\\\\n  $0.18$ & $0.28$ & $0.24$ & $0.33$ \\\\\n  $0.21$ & $0.31$ & $0.26$ & $0.34$ \\\\\n  $0.60$ & $0.70$ & $0.65$ & $0.75$ \\\\\n  $0.62$ & $0.68$ & $0.64$ & $0.74$ \\\\\n  $0.58$ & $0.72$ & $0.66$ & $0.76$ \\\\\n  $0.61$ & $0.69$ & $0.63$ & $0.73$\n  \\end{bmatrix}$,\n  $y_3 = [$0$, $0$, $0$, $0$, $1$, $1$, $1$, $1$],\n  $t_3 = $0.00$.\n- Test case $4$ (collinearity case, low target fraction):\n  - $X_4 = \\begin{bmatrix}\n  $0.10$ & $0.20$ & $0.30$ \\\\\n  $0.11$ & $0.19$ & $0.30$ \\\\\n  $0.09$ & $0.21$ & $0.30$ \\\\\n  $0.40$ & $0.50$ & $0.90$ \\\\\n  $0.39$ & $0.49$ & $0.88$ \\\\\n  $0.41$ & $0.51$ & $0.92$\n  \\end{bmatrix}$,\n  $y_4 = [$0$, $0$, $0$, $1$, $1$, $1$],\n  $t_4 = $0.50$.\n\nRequired computation and output:\n- For each test case ($X, y, t$), standardize $X$ to $Z$, compute the smallest $k$ such that $r_k \\ge t$, reconstruct $\\hat{Z}$ from the top $k$ principal components, compute $J(Z)$, $J(\\hat{Z})$, and $L$.\n- Express all target fractions as decimals. There are no physical units or angles involved in this problem.\n- Your program should produce a single line of output containing the results for all test cases aggregated as a list of lists, where each inner list is $[k, r_k, J(Z), J(\\hat{Z}), L]$ with floating-point values rounded to six decimal places and $k$ as an integer. The final output line must be exactly in the format:\n\"[ [k1,r1,J1,Jhat1,L1],[k2,r2,J2,Jhat2,L2],[k3,r3,J3,Jhat3,L3],[k4,r4,J4,Jhat4,L4] ]\" using commas between values and no extra text.",
            "solution": "The problem requires the implementation of a specific workflow for dimensionality reduction using Principal Component Analysis (PCA) on labeled feature matrices and the subsequent evaluation of information loss with respect to class separability. The solution is derived by methodically applying the provided definitions for data standardization, PCA, data reconstruction, and the Fisher class separability criterion.\n\nThe overall procedure for each test case $(X, y, t)$ is as follows:\n\n1.  **Data Standardization**: The initial step is to transform the raw feature matrix $X \\in \\mathbb{R}^{n \\times d}$ into a standardized matrix $Z \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of samples and $d$ is the number of features. Each feature (column) of $Z$ must have a mean of zero and an unbiased sample standard deviation of one. For each feature $j \\in \\{1, \\dots, d\\}$, the mean $\\mu_j$ and unbiased sample standard deviation $\\sigma_j$ are computed from the corresponding column $X_j$ of $X$:\n    $$ \\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij} $$\n    $$ \\sigma_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_{ij} - \\mu_j)^2} $$\n    The standardized feature $Z_{ij}$ is then calculated as:\n    $$ Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j} $$\n    As specified, if any $\\sigma_j = 0$, it is replaced with $1$ to prevent division by zero.\n\n2.  **Principal Component Analysis (PCA)**: PCA is performed on the standardized data $Z$. The sample covariance matrix $C \\in \\mathbb{R}^{d \\times d}$ is computed:\n    $$ C = \\frac{1}{n-1} Z^\\top Z $$\n    Next, an eigen-decomposition of the symmetric matrix $C$ is performed to find its eigenvalues $\\lambda_i$ and corresponding eigenvectors $p_i$.\n    $$ C p_i = \\lambda_i p_i $$\n    The eigenvectors form the columns of the matrix $P$. The eigenpairs $(\\lambda_i, p_i)$ are sorted such that the eigenvalues are in non-increasing order: $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$.\n\n3.  **Component Selection**: The number of principal components, $k$, to retain is determined by the given target variance fraction $t \\in [0, 1]$. We calculate the cumulative explained variance ratio for the first $m$ components:\n    $$ r_m = \\frac{\\sum_{i=1}^{m} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i} $$\n    The selected number of components $k$ is the smallest integer in $\\{0, 1, \\dots, d\\}$ such that $r_k \\ge t$. For the special case $t=0$, we set $k=0$. A numerical tolerance of $10^{-12}$ is used for the comparison $r_k \\ge t$ to account for floating-point inaccuracies.\n\n4.  **Data Reconstruction**: The original standardized data $Z$ is projected onto the $k$-dimensional principal subspace and then reconstructed back into the original $d$-dimensional space. Let $P_k \\in \\mathbb{R}^{d \\times k}$ be the matrix whose columns are the first $k$ principal eigenvectors.\n    The projection scores are computed as:\n    $$ T_k = Z P_k $$\n    The reconstructed standardized data matrix, $\\hat{Z}$, is then:\n    $$ \\hat{Z} = T_k P_k^\\top $$\n    If $k=0$, the reconstructed matrix $\\hat{Z}$ is a zero matrix of the same dimensions as $Z$.\n\n5.  **Separability Assessment**: The Fisher's class separability criterion, $J$, is used to quantify how well the classes are separated in the feature space. This is calculated for both the original standardized data $Z$ and the reconstructed data $\\hat{Z}$.\n    The criterion is the ratio of the between-class scatter trace to the within-class scatter trace:\n    $$ J = \\frac{\\mathrm{tr}(S_B)}{\\mathrm{tr}(S_W)} $$\n    The traces are calculated as:\n    $$ \\mathrm{tr}(S_B) = \\sum_{c} n_c \\| \\mu_c - \\mu \\|_2^2 $$\n    $$ \\mathrm{tr}(S_W) = \\sum_{c} \\sum_{i \\in \\mathcal{I}_c} \\| z_i - \\mu_c \\|_2^2 $$\n    where $\\mu$ is the overall mean of the data, $\\mu_c$ is the mean of data points in class $c$, $n_c$ is the number of samples in class $c$, and $\\mathcal{I}_c$ is the set of indices for samples in class $c$. If $\\mathrm{tr}(S_W)=0$, $J$ is defined as $0$.\n\n6.  **Information Loss Quantification**: Finally, the information loss $L$ due to dimensionality reduction is quantified as the relative decrease in the Fisher separability criterion $J$.\n    $$ L = \\begin{cases} \\max\\left(0, \\dfrac{J(Z) - J(\\hat{Z})}{J(Z)} \\right), & \\text{if } J(Z) > 0 \\\\ 0, & \\text{if } J(Z) = 0 \\end{cases} $$\n    This value represents the fraction of class separability information lost during the PCA reconstruction process.\n\nThe implementation will apply these six steps to each test case provided in the problem statement and format the final results as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the PCA workflow and calculating\n    the information loss for class separability for each test case.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [0.12, 0.38, 0.50],\n                [0.10, 0.35, 0.48],\n                [0.15, 0.40, 0.52],\n                [0.30, 0.55, 0.60],\n                [0.28, 0.50, 0.58],\n                [0.32, 0.57, 0.62]\n            ]),\n            np.array([0, 0, 0, 1, 1, 1]),\n            0.90\n        ),\n        (\n            np.array([\n                [0.05, 0.20],\n                [0.07, 0.22],\n                [0.50, 0.45],\n                [0.48, 0.40],\n                [0.52, 0.47]\n            ]),\n            np.array([0, 0, 1, 1, 1]),\n            1.00\n        ),\n        (\n            np.array([\n                [0.20, 0.30, 0.25, 0.35],\n                [0.22, 0.32, 0.27, 0.36],\n                [0.18, 0.28, 0.24, 0.33],\n                [0.21, 0.31, 0.26, 0.34],\n                [0.60, 0.70, 0.65, 0.75],\n                [0.62, 0.68, 0.64, 0.74],\n                [0.58, 0.72, 0.66, 0.76],\n                [0.61, 0.69, 0.63, 0.73]\n            ]),\n            np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            0.00\n        ),\n        (\n            np.array([\n                [0.10, 0.20, 0.30],\n                [0.11, 0.19, 0.30],\n                [0.09, 0.21, 0.30],\n                [0.40, 0.50, 0.90],\n                [0.39, 0.49, 0.88],\n                [0.41, 0.51, 0.92]\n            ]),\n            np.array([0, 0, 0, 1, 1, 1]),\n            0.50\n        )\n    ]\n\n    def compute_j_criterion(data, labels):\n        \"\"\"Computes Fisher's class separability criterion J.\"\"\"\n        n_samples = data.shape[0]\n        if n_samples == 0:\n            return 0.0\n\n        overall_mean = np.mean(data, axis=0)\n        classes = np.unique(labels)\n        \n        tr_S_W = 0.0\n        tr_S_B = 0.0\n\n        for c in classes:\n            class_samples = data[labels == c]\n            n_c = class_samples.shape[0]\n            if n_c == 0:\n                continue\n            \n            class_mean = np.mean(class_samples, axis=0)\n            \n            tr_S_B += n_c * np.sum((class_mean - overall_mean)**2)\n            tr_S_W += np.sum((class_samples - class_mean)**2)\n\n        if tr_S_W < 1e-12: # As per problem, if tr(S_W)=0, J=0\n            return 0.0\n        \n        return tr_S_B / tr_S_W\n\n    results = []\n    \n    for X, y, t in test_cases:\n        # Step 1: Standardize the feature matrix\n        n, d = X.shape\n        mean_X = np.mean(X, axis=0)\n        std_X = np.std(X, axis=0, ddof=1)\n        std_X[std_X == 0] = 1.0  # Avoid division by zero\n        Z = (X - mean_X) / std_X\n\n        # Step 2: Perform PCA\n        C = (1 / (n - 1)) * (Z.T @ Z)\n        eigenvalues, eigenvectors = np.linalg.eigh(C)\n\n        # Sort eigenvalues and eigenvectors in descending order\n        sort_indices = np.argsort(eigenvalues)[::-1]\n        sorted_eigenvalues = eigenvalues[sort_indices]\n        sorted_eigenvectors = eigenvectors[:, sort_indices]\n\n        # Step 3: Determine the number of components k\n        total_variance = np.sum(sorted_eigenvalues)\n        if total_variance < 1e-12:\n            cumulative_ratios = np.zeros(d)\n        else:\n            cumulative_ratios = np.cumsum(sorted_eigenvalues) / total_variance\n\n        # Select smallest k such that r_k >= t, using a tolerance\n        # and handling the t=0 case.\n        if t <= 1e-12:\n            k = 0\n        else:\n            k_indices = np.where(cumulative_ratios >= t - 1e-12)[0]\n            if len(k_indices) == 0:\n                k = d  # Take all components if t=1 but ratio is slightly less\n            else:\n                k = k_indices[0] + 1\n        \n        r_k = 0.0 if k == 0 else cumulative_ratios[k - 1]\n\n        # Step 4: Reconstruct the data\n        if k == 0:\n            hat_Z = np.zeros_like(Z)\n        else:\n            P_k = sorted_eigenvectors[:, :k]\n            T_k = Z @ P_k\n            hat_Z = T_k @ P_k.T\n\n        # Step 5: Compute J for original and reconstructed data\n        J_Z = compute_j_criterion(Z, y)\n        J_hat_Z = compute_j_criterion(hat_Z, y)\n\n        # Step 6: Compute the information loss L\n        if J_Z > 1e-12:\n            L = max(0.0, (J_Z - J_hat_Z) / J_Z)\n        else:\n            L = 0.0\n\n        results.append([k, r_k, J_Z, J_hat_Z, L])\n    \n    # Format the final output string as specified\n    formatted_results = []\n    for res in results:\n        k, r_val, j_z, j_hat, l_val = res\n        # Format floats to 6 decimal places, k is an integer\n        formatted_str = f\"[{k},{r_val:.6f},{j_z:.6f},{j_hat:.6f},{l_val:.6f}]\"\n        formatted_results.append(formatted_str)\n        \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The final stage of the classification workflow is a rigorous evaluation of the model's performance, which requires moving beyond simple accuracy, especially when dealing with imbalanced classes. This practice focuses on interpreting a confusion matrix to derive and compare several key performance metrics, including balanced accuracy, the F1-score, and the Matthews Correlation Coefficient (MCC). By working through this example, you will learn to select the most appropriate metric to honestly assess a classifier's utility in realistic scenarios where some classes are much rarer than others .",
            "id": "3860465",
            "problem": "A supervised land-cover classification was performed over a coastal wetland complex using multispectral satellite data. The workflow included radiometric calibration, atmospheric correction, and orthorectification, followed by feature engineering of spectral and texture descriptors: Normalized Difference Vegetation Index (NDVI), Normalized Difference Water Index (NDWI), Modified Normalized Difference Water Index (MNDWI), tasseled-cap wetness, and Gray-Level Co-occurrence Matrix (GLCM) texture measures. A classifier was trained on these features and evaluated on a stratified validation sample of $12{,}000$ pixels, with $800$ reference wetland pixels (positive class) and $11{,}200$ reference non-wetland pixels (negative class). The resulting confusion matrix (rows are reference labels, columns are classifier predictions) is:\n$$\n\\begin{array}{c|cc}\n & \\text{Predicted wetland} & \\text{Predicted non-wetland} \\\\\n\\hline\n\\text{Reference wetland} & 480 & 320 \\\\\n\\text{Reference non-wetland} & 1200 & 10000\n\\end{array}\n$$\nStarting only from the core definitions of the contingency table counts and the associated rates used in statistical classification (for example, sensitivity, specificity, precision, recall), derive and compute the balanced accuracy, the F1-score, and the Matthews correlation coefficient (MCC). Interpret briefly which of these metrics is most suitable to summarize performance when classes are imbalanced, based on their mathematical construction. Express each metric as an exact analytic form (use a fraction or a radical if it simplifies exactly) and provide your final answer as a single row matrix containing, in order, the balanced accuracy, the F1-score, and the MCC. Do not include units. Do not round.",
            "solution": "The problem is validated as being scientifically grounded, well-posed, objective, and self-contained. The provided information is sufficient and consistent for deriving the required classification performance metrics.\n\nFirst, we must define the components of the confusion matrix. Let the \"wetland\" class be the positive class and \"non-wetland\" be the negative class. The matrix entries are:\n-   True Positives ($TP$): Reference wetland, Predicted wetland. $TP = 480$.\n-   False Negatives ($FN$): Reference wetland, Predicted non-wetland. $FN = 320$.\n-   False Positives ($FP$): Reference non-wetland, Predicted wetland. $FP = 1200$.\n-   True Negatives ($TN$): Reference non-wetland, Predicted non-wetland. $TN = 10000$.\n\nThe total number of actual positive instances (reference wetland) is $P = TP + FN = 480 + 320 = 800$.\nThe total number of actual negative instances (reference non-wetland) is $N = FP + TN = 1200 + 10000 = 11200$.\nThe total sample size is $P + N = 800 + 11200 = 12000$. These values match the problem statement.\n\nNow we derive and compute the required metrics.\n\n**1. Balanced Accuracy ($BA$)**\nBalanced accuracy is defined as the arithmetic mean of sensitivity and specificity.\n\nSensitivity, also known as the True Positive Rate ($TPR$) or recall, is the proportion of actual positives that are correctly identified.\n$$TPR = \\frac{TP}{TP + FN}$$\nSubstituting the given values:\n$$TPR = \\frac{480}{480 + 320} = \\frac{480}{800} = \\frac{48}{80} = \\frac{3}{5}$$\n\nSpecificity, also known as the True Negative Rate ($TNR$), is the proportion of actual negatives that are correctly identified.\n$$TNR = \\frac{TN}{TN + FP}$$\nSubstituting the given values:\n$$TNR = \\frac{10000}{10000 + 1200} = \\frac{10000}{11200} = \\frac{100}{112} = \\frac{25}{28}$$\n\nThe balanced accuracy is then:\n$$BA = \\frac{TPR + TNR}{2}$$\n$$BA = \\frac{\\frac{3}{5} + \\frac{25}{28}}{2} = \\frac{\\frac{3 \\times 28 + 25 \\times 5}{5 \\times 28}}{2} = \\frac{\\frac{84 + 125}{140}}{2} = \\frac{\\frac{209}{140}}{2} = \\frac{209}{280}$$\n\n**2. F1-score ($F_1$)**\nThe F1-score is the harmonic mean of precision and recall (sensitivity).\n\nPrecision, also known as the Positive Predictive Value ($PPV$), is the proportion of predicted positives that are actually positive.\n$$PPV = \\frac{TP}{TP + FP}$$\nSubstituting the given values:\n$$PPV = \\frac{480}{480 + 1200} = \\frac{480}{1680} = \\frac{48}{168} = \\frac{2}{7}$$\n\nRecall is the same as sensitivity, which we already calculated as $TPR = \\frac{3}{5}$.\n\nThe F1-score is defined as:\n$$F_1 = 2 \\times \\frac{PPV \\times TPR}{PPV + TPR}$$\n$$F_1 = 2 \\times \\frac{\\frac{2}{7} \\times \\frac{3}{5}}{\\frac{2}{7} + \\frac{3}{5}} = 2 \\times \\frac{\\frac{6}{35}}{\\frac{10+21}{35}} = 2 \\times \\frac{\\frac{6}{35}}{\\frac{31}{35}} = 2 \\times \\frac{6}{31} = \\frac{12}{31}$$\n\nAlternatively, using the direct formula:\n$$F_1 = \\frac{2TP}{2TP + FP + FN} = \\frac{2 \\times 480}{2 \\times 480 + 1200 + 320} = \\frac{960}{960 + 1520} = \\frac{960}{2480} = \\frac{96}{248} = \\frac{12}{31}$$\n\n**3. Matthews Correlation Coefficient ($MCC$)**\nThe MCC is a correlation coefficient between the observed and predicted binary classifications. It is defined as:\n$$MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n\nThe numerator is:\n$$TP \\times TN - FP \\times FN = (480 \\times 10000) - (1200 \\times 320) = 4800000 - 384000 = 4416000$$\n\nThe terms in the denominator's square root are:\n-   $TP+FP = 480 + 1200 = 1680$ (Predicted Positives)\n-   $TP+FN = 480 + 320 = 800$ (Actual Positives)\n-   $TN+FP = 10000 + 1200 = 11200$ (Actual Negatives)\n-   $TN+FN = 10000 + 320 = 10320$ (Predicted Negatives)\n\nThe denominator is:\n$$\\sqrt{1680 \\times 800 \\times 11200 \\times 10320}$$\nTo simplify the calculation, we notice that all confusion matrix counts are multiples of $80$:\n$TP = 6 \\times 80$, $FN = 4 \\times 80$, $FP = 15 \\times 80$, $TN = 125 \\times 80$.\nLet $k=80$. Then $TP = 6k, FN = 4k, FP = 15k, TN = 125k$.\n\nThe MCC formula can be rewritten:\n$$MCC = \\frac{(6k)(125k) - (15k)(4k)}{\\sqrt{((6+15)k)((6+4)k)((125+15)k)((125+4)k)}}$$\n$$MCC = \\frac{750k^2 - 60k^2}{\\sqrt{(21k)(10k)(140k)(129k)}} = \\frac{690k^2}{k^2\\sqrt{21 \\times 10 \\times 140 \\times 129}}$$\n$$MCC = \\frac{690}{\\sqrt{21 \\times 10 \\times (14 \\times 10) \\times 129}} = \\frac{690}{10\\sqrt{21 \\times 14 \\times 129}} = \\frac{69}{\\sqrt{(3 \\times 7) \\times (2 \\times 7) \\times (3 \\times 43)}}$$\n$$MCC = \\frac{69}{\\sqrt{2 \\times 3^2 \\times 7^2 \\times 43}} = \\frac{69}{3 \\times 7 \\sqrt{2 \\times 43}} = \\frac{23}{7\\sqrt{86}}$$\n\n**Interpretation of Metrics for Imbalanced Classes**\nThe problem demonstrates significant class imbalance ($800$ vs. $11{,}200$). In such cases, some metrics can be misleading. For instance, the simple accuracy is $\\frac{480+10000}{12000} \\approx 0.87$, which seems high but primarily reflects the classifier's success on the dominant non-wetland class.\n\n-   **Balanced Accuracy ($BA$)**: By its mathematical construction, $BA = \\frac{1}{2}(TPR + TNR)$, it is the mean of the per-class accuracies. It gives equal weight to the performance on the minority class ($TPR$) and the majority class ($TNR$). This makes it a robust indicator of performance on imbalanced datasets, as it is not inflated by high accuracy on the large negative class.\n\n-   **F1-score ($F_1$)**: As the harmonic mean of precision and recall ($TPR$), the $F_1$-score is constructed to summarize performance on the positive class. Its formula, $F_1 = \\frac{2TP}{2TP + FP + FN}$, does not include the True Negatives ($TN$) count. While useful when the positive class is the primary focus, its complete blindness to the number of correctly classified negative instances means it does not capture the full picture of classifier performance across both classes.\n\n-   **Matthews Correlation Coefficient ($MCC$)**: The MCC's formula, $MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$, is the only one of the three that is constructed using all four values of the confusion matrix. It is a correlation coefficient between the true and predicted classes, producing a score from $-1$ to $+1$. It is a symmetric metric, meaning its value does not change if the positive and negative classes are swapped. Due to its use of all four counts, it is widely regarded as a highly informative and balanced single-score summary, providing a reliable measure of quality even when the classes are of very different sizes.\n\nBased on this analysis of their mathematical construction, the **Matthews Correlation Coefficient (MCC)** is the most suitable metric to summarize performance for imbalanced classes because it synthesizes all aspects of the classification (true and false positives and negatives) into a single, balanced, and robust value.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{209}{280} & \\frac{12}{31} & \\frac{23}{7\\sqrt{86}} \\end{pmatrix}}$$"
        }
    ]
}