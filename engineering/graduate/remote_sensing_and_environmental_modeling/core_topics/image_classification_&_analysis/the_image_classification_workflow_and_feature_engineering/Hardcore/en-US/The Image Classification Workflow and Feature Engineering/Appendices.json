{
    "hands_on_practices": [
        {
            "introduction": "Before investing significant computational resources into training a classifier, it's wise to first assess the quality of the engineered features. This practice explores a fundamental question: how well can our chosen features inherently separate the target classes? We will use the Bhattacharyya distance, a statistical measure that quantifies the overlap between two probability distributions. By modeling our classes as multivariate Gaussian distributions in the feature space, you will not only calculate the overall separability but also decompose the metric to pinpoint which features are the most powerful discriminators, a crucial skill for effective feature engineering .",
            "id": "3860480",
            "problem": "A land-cover classification study in remote sensing and environmental modeling aims to separate riparian forest from irrigated cropland using a three-dimensional engineered feature vector comprising Normalized Difference Vegetation Index (NDVI), Normalized Difference Water Index (NDWI), and Gray Level Co-occurrence Matrix (GLCM) homogeneity. Assume class-conditional feature vectors are modeled as multivariate Gaussian random variables based on training samples. The estimated class means and covariances are:\n$$\n\\boldsymbol{\\mu}_{\\text{forest}} = \\begin{pmatrix} 0.62 \\\\ -0.15 \\\\ 0.45 \\end{pmatrix}, \\quad\n\\boldsymbol{\\mu}_{\\text{cropland}} = \\begin{pmatrix} 0.47 \\\\ 0.05 \\\\ 0.30 \\end{pmatrix},\n$$\n$$\n\\boldsymbol{\\Sigma}_{\\text{forest}} = \\begin{pmatrix}\n0.0225 & 0 & 0 \\\\\n0 & 0.04 & 0 \\\\\n0 & 0 & 0.01\n\\end{pmatrix}, \\quad\n\\boldsymbol{\\Sigma}_{\\text{cropland}} = \\begin{pmatrix}\n0.04 & 0 & 0 \\\\\n0 & 0.01 & 0 \\\\\n0 & 0 & 0.0225\n\\end{pmatrix}.\n$$\nUsing a separability measure grounded in the Bhattacharyya framework between multivariate Gaussian densities and the pooled within-class covariance, compute the separability between these two classes. Then, from first principles, decompose the mean-difference contribution to this separability into additive feature-wise components under the given diagonal pooled-covariance structure, and identify which feature contributes most to discrimination. Round the separability to four significant figures. Express the final separability as a dimensionless number.",
            "solution": "The problem requires the computation of a separability measure between two classes, 'forest' and 'cropland', whose feature distributions are modeled as multivariate Gaussians. The specified measure is based on the Bhattacharyya framework and utilizes the pooled within-class covariance. This points to the Bhattacharyya distance, $D_B$, which for two multivariate normal distributions $\\mathcal{N}(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)$ and $\\mathcal{N}(\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)$, is given by:\n$$\nD_B = \\frac{1}{8}(\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_j)^T \\left( \\frac{\\boldsymbol{\\Sigma}_i + \\boldsymbol{\\Sigma}_j}{2} \\right)^{-1} (\\boldsymbol{\\mu}_i - \\boldsymbol{\\mu}_j) + \\frac{1}{2} \\ln \\left( \\frac{\\det\\left(\\frac{\\boldsymbol{\\Sigma}_i + \\boldsymbol{\\Sigma}_j}{2}\\right)}{\\sqrt{\\det(\\boldsymbol{\\Sigma}_i) \\det(\\boldsymbol{\\Sigma}_j)}} \\right)\n$$\nThe term $\\frac{\\boldsymbol{\\Sigma}_i + \\boldsymbol{\\Sigma}_j}{2}$ is the pooled within-class covariance matrix, assuming equal prior probabilities for the two classes, which we will denote as $\\boldsymbol{\\Sigma}_P$. Let the two classes be denoted by subscripts $f$ (forest) and $c$ (cropland). The separability measure $D_B$ can be separated into two additive components: a term due to the difference in mean vectors, $D_{B, \\text{mean}}$, and a term due to the difference in covariance matrices, $D_{B, \\text{cov}}$.\n$$\nD_B = D_{B, \\text{mean}} + D_{B, \\text{cov}}\n$$\nWhere:\n$$\nD_{B, \\text{mean}} = \\frac{1}{8}(\\boldsymbol{\\mu}_f - \\boldsymbol{\\mu}_c)^T \\boldsymbol{\\Sigma}_P^{-1} (\\boldsymbol{\\mu}_f - \\boldsymbol{\\mu}_c)\n$$\n$$\nD_{B, \\text{cov}} = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\boldsymbol{\\Sigma}_P)}{\\sqrt{\\det(\\boldsymbol{\\Sigma}_f) \\det(\\boldsymbol{\\Sigma}_c)}} \\right)\n$$\n\nFirst, we calculate the necessary components from the given data.\nThe given mean vectors are:\n$$\n\\boldsymbol{\\mu}_f = \\begin{pmatrix} 0.62 \\\\ -0.15 \\\\ 0.45 \\end{pmatrix}, \\quad\n\\boldsymbol{\\mu}_c = \\begin{pmatrix} 0.47 \\\\ 0.05 \\\\ 0.30 \\end{pmatrix}\n$$\nThe difference between the mean vectors is:\n$$\n\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_f - \\boldsymbol{\\mu}_c = \\begin{pmatrix} 0.62 - 0.47 \\\\ -0.15 - 0.05 \\\\ 0.45 - 0.30 \\end{pmatrix} = \\begin{pmatrix} 0.15 \\\\ -0.20 \\\\ 0.15 \\end{pmatrix}\n$$\nThe given covariance matrices are:\n$$\n\\boldsymbol{\\Sigma}_f = \\begin{pmatrix}\n0.0225 & 0 & 0 \\\\\n0 & 0.04 & 0 \\\\\n0 & 0 & 0.01\n\\end{pmatrix}, \\quad\n\\boldsymbol{\\Sigma}_c = \\begin{pmatrix}\n0.04 & 0 & 0 \\\\\n0 & 0.01 & 0 \\\\\n0 & 0 & 0.0225\n\\end{pmatrix}\n$$\nThe pooled covariance matrix $\\boldsymbol{\\Sigma}_P$ is:\n$$\n\\boldsymbol{\\Sigma}_P = \\frac{1}{2}(\\boldsymbol{\\Sigma}_f + \\boldsymbol{\\Sigma}_c) = \\frac{1}{2} \\begin{pmatrix}\n0.0225+0.04 & 0 & 0 \\\\\n0 & 0.04+0.01 & 0 \\\\\n0 & 0 & 0.01+0.0225\n\\end{pmatrix} = \\begin{pmatrix}\n0.03125 & 0 & 0 \\\\\n0 & 0.025 & 0 \\\\\n0 & 0 & 0.01625\n\\end{pmatrix}\n$$\nSince $\\boldsymbol{\\Sigma}_P$ is a diagonal matrix, its inverse is a diagonal matrix with the reciprocals of the diagonal elements:\n$$\n\\boldsymbol{\\Sigma}_P^{-1} = \\begin{pmatrix}\n1/0.03125 & 0 & 0 \\\\\n0 & 1/0.025 & 0 \\\\\n0 & 0 & 1/0.01625\n\\end{pmatrix} = \\begin{pmatrix}\n32 & 0 & 0 \\\\\n0 & 40 & 0 \\\\\n0 & 800/13\n\\end{pmatrix}\n$$\nNow, we can compute the mean-difference contribution, $D_{B, \\text{mean}}$. The quadratic form $(\\Delta\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}_P^{-1} (\\Delta\\boldsymbol{\\mu})$ becomes a sum due to the diagonal structure:\n$$\n(\\Delta\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}_P^{-1} (\\Delta\\boldsymbol{\\mu}) = \\sum_{i=1}^3 \\frac{(\\Delta\\mu_i)^2}{[\\boldsymbol{\\Sigma}_P]_{ii}}\n$$\nThis allows us to decompose the total mean-difference contribution into additive components for each feature as requested. Let $C_i = \\frac{(\\Delta\\mu_i)^2}{[\\boldsymbol{\\Sigma}_P]_{ii}}$ be the contribution from feature $i$.\nFor feature 1 (NDVI):\n$$\nC_1 = \\frac{(0.15)^2}{0.03125} = \\frac{0.0225}{0.03125} = 0.72\n$$\nFor feature 2 (NDWI):\n$$\nC_2 = \\frac{(-0.20)^2}{0.025} = \\frac{0.04}{0.025} = 1.6\n$$\nFor feature 3 (GLCM homogeneity):\n$$\nC_3 = \\frac{(0.15)^2}{0.01625} = \\frac{0.0225}{0.01625} = \\frac{18}{13} \\approx 1.3846\n$$\nComparing these values, $C_2 > C_3 > C_1$. Therefore, the NDWI feature contributes the most to the mean-difference separability.\n\nThe total mean-difference contribution to the Bhattacharyya distance is:\n$$\nD_{B, \\text{mean}} = \\frac{1}{8}(C_1 + C_2 + C_3) = \\frac{1}{8} \\left( 0.72 + 1.6 + \\frac{18}{13} \\right) = \\frac{1}{8} \\left( 2.32 + \\frac{18}{13} \\right)\n$$\n$$\nD_{B, \\text{mean}} = \\frac{1}{8} \\left( \\frac{58}{25} + \\frac{18}{13} \\right) = \\frac{1}{8} \\left( \\frac{58 \\times 13 + 18 \\times 25}{25 \\times 13} \\right) = \\frac{1}{8} \\left( \\frac{754 + 450}{325} \\right) = \\frac{1204}{2600} = \\frac{301}{650} \\approx 0.463077\n$$\nNext, we compute the covariance-difference contribution, $D_{B, \\text{cov}}$. We need the determinants of the covariance matrices.\n$$\n\\det(\\boldsymbol{\\Sigma}_f) = (0.0225)(0.04)(0.01) = 9 \\times 10^{-6}\n$$\n$$\n\\det(\\boldsymbol{\\Sigma}_c) = (0.04)(0.01)(0.0225) = 9 \\times 10^{-6}\n$$\n$$\n\\det(\\boldsymbol{\\Sigma}_P) = (0.03125)(0.025)(0.01625) = \\left(\\frac{1}{32}\\right)\\left(\\frac{1}{40}\\right)\\left(\\frac{13}{800}\\right) = \\frac{13}{1024000}\n$$\nThe ratio inside the logarithm is:\n$$\n\\frac{\\det(\\boldsymbol{\\Sigma}_P)}{\\sqrt{\\det(\\boldsymbol{\\Sigma}_f) \\det(\\boldsymbol{\\Sigma}_c)}} = \\frac{13/1024000}{\\sqrt{(9\\times 10^{-6})^2}} = \\frac{13/1024000}{9 \\times 10^{-6}} = \\frac{13/1024000}{9/1000000} = \\frac{13}{1024000} \\frac{1000000}{9} = \\frac{13000}{9216} = \\frac{1625}{1152}\n$$\nNow, we compute $D_{B, \\text{cov}}$:\n$$\nD_{B, \\text{cov}} = \\frac{1}{2} \\ln \\left( \\frac{1625}{1152} \\right) \\approx \\frac{1}{2} \\ln(1.41059) \\approx \\frac{1}{2}(0.344015) \\approx 0.172008\n$$\nThe total separability $D_B$ is the sum of the two components:\n$$\nD_B = D_{B, \\text{mean}} + D_{B, \\text{cov}} = \\frac{301}{650} + \\frac{1}{2} \\ln \\left( \\frac{1625}{1152} \\right)\n$$\n$$\nD_B \\approx 0.463077 + 0.172008 = 0.635085\n$$\nRounding to four significant figures, the separability is $0.6351$.",
            "answer": "$$\n\\boxed{0.6351}\n$$"
        },
        {
            "introduction": "Remote sensing datasets are often characterized by high dimensionality, with numerous spectral bands and derived features. This can lead to computational challenges and the risk of overfitting, a phenomenon known as the \"curse of dimensionality.\" This exercise tackles this issue head-on using Principal Component Analysis (PCA), a cornerstone technique for dimensionality reduction. You will implement the complete workflow of projecting data into a lower-dimensional space that preserves maximum variance and then reconstructing it, allowing for a quantitative assessment of the trade-offs involved . Crucially, you will evaluate the \"information loss\" not just in terms of variance, but in terms of class separability, linking the abstract goal of dimensionality reduction to the concrete task of classification.",
            "id": "3860432",
            "problem": "You are given labeled feature matrices representing remotely sensed observations used for land-cover image classification. Your task is to implement the dimensionality reduction and reconstruction stages of the image classification workflow using Principal Component Analysis (PCA), then quantify the information loss for class separability due to dimensionality reduction. You must base your derivation and algorithm on the following foundational definitions and widely accepted formulas.\n\nFundamental base:\n- Principal Component Analysis (PCA): Given a standardized data matrix $Z \\in \\mathbb{R}^{n \\times d}$ with $n$ samples and $d$ features, the sample covariance matrix is $$C = \\frac{1}{n-1} Z^\\top Z.$$ PCA finds orthonormal eigenvectors of $C$ and their corresponding eigenvalues. If $C P = P \\Lambda$ where $P \\in \\mathbb{R}^{d \\times d}$ has columns that are eigenvectors and $\\Lambda \\in \\mathbb{R}^{d \\times d}$ is diagonal with nonnegative eigenvalues, then the explained variance of the $i$-th principal component is the $i$-th diagonal element of $\\Lambda$. The explained variance ratio of the first $k$ components is $$r_k = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i},$$ where eigenvalues are sorted in nonincreasing order, $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$.\n- Reconstruction from top components: Let $P_k \\in \\mathbb{R}^{d \\times k}$ be the matrix of the top $k$ eigenvectors. The projection scores are $$T_k = Z P_k,$$ and the rank-$k$ reconstruction in standardized space is $$\\hat{Z} = T_k P_k^\\top.$$ If $k = 0$, define $P_0$ to be empty and $\\hat{Z}$ to be the zero matrix of shape $n \\times d$.\n- Fisher’s class scatter ratio for separability: Given class labels $y \\in \\{0, 1, \\dots\\}$, define the overall mean $$\\mu = \\frac{1}{n} \\sum_{i=1}^n z_i,$$ and for each class $c$ the mean $$\\mu_c = \\frac{1}{n_c} \\sum_{i \\in \\mathcal{I}_c} z_i,$$ where $\\mathcal{I}_c$ is the index set of samples in class $c$ and $n_c = |\\mathcal{I}_c|$. Define the within-class scatter trace $$\\mathrm{tr}(S_W) = \\sum_{c} \\sum_{i \\in \\mathcal{I}_c} \\| z_i - \\mu_c \\|_2^2,$$ and the between-class scatter trace $$\\mathrm{tr}(S_B) = \\sum_{c} n_c \\| \\mu_c - \\mu \\|_2^2.$$ The Fisher separability criterion is $$J = \\frac{\\mathrm{tr}(S_B)}{\\mathrm{tr}(S_W)}.$$ If $\\mathrm{tr}(S_W) = 0$, define $J = 0$ to avoid division by zero.\n- Information loss due to reconstruction: For original standardized features $Z$ and reconstructed standardized features $\\hat{Z}$, define the information loss as the relative drop in $J$: $$L = \\begin{cases} \\max\\left(0, \\dfrac{J(Z) - J(\\hat{Z})}{J(Z)} \\right), & \\text{if } J(Z) > 0, \\\\ 0, & \\text{if } J(Z) = 0. \\end{cases}$$\n\nWorkflow to implement:\n1. Standardize each feature of the input matrix $X \\in \\mathbb{R}^{n \\times d}$ to obtain $Z$ with zero mean and unit unbiased standard deviation per feature. Use the sample standard deviation with denominator $(n-1)$ (that is, the unbiased estimator) for scaling; if a feature has zero standard deviation, divide by $1$ instead to avoid division by zero.\n2. Compute $C$, perform the eigen-decomposition of $C$, sort eigenpairs by nonincreasing eigenvalues, and compute cumulative explained variance ratios $r_k$.\n3. Given a target variance fraction $t \\in [0, 1]$ (expressed as a decimal), select the smallest integer $k \\in \\{0,1,\\dots,d\\}$ such that $r_k \\ge t$ (use a numerical tolerance of $10^{-12}$ to account for floating-point rounding). If $t = 0$, let $k = 0$.\n4. Reconstruct $\\hat{Z}$ from the top $k$ components and compute $J(Z)$ and $J(\\hat{Z})$, then compute $L$.\n\nTest suite:\nFor each test case, you are given $(X, y, t)$ where $X$ is the feature matrix, $y$ are class labels, and $t$ is the target variance fraction as a decimal.\n\n- Test case $1$ (happy path, moderately high variance target):\n  $$X_1 = \\begin{bmatrix}\n  0.12 & 0.38 & 0.50 \\\\\n  0.10 & 0.35 & 0.48 \\\\\n  0.15 & 0.40 & 0.52 \\\\\n  0.30 & 0.55 & 0.60 \\\\\n  0.28 & 0.50 & 0.58 \\\\\n  0.32 & 0.57 & 0.62\n  \\end{bmatrix}$$,\n  $y_1 = [0, 0, 0, 1, 1, 1]$,\n  $t_1 = 0.90$.\n- Test case $2$ (boundary case, full variance target):\n  $$X_2 = \\begin{bmatrix}\n  0.05 & 0.20 \\\\\n  0.07 & 0.22 \\\\\n  0.50 & 0.45 \\\\\n  0.48 & 0.40 \\\\\n  0.52 & 0.47\n  \\end{bmatrix}$$,\n  $y_2 = [0, 0, 1, 1, 1]$,\n  $t_2 = 1.00$.\n- Test case $3$ (edge case, zero variance target):\n  $$X_3 = \\begin{bmatrix}\n  0.20 & 0.30 & 0.25 & 0.35 \\\\\n  0.22 & 0.32 & 0.27 & 0.36 \\\\\n  0.18 & 0.28 & 0.24 & 0.33 \\\\\n  0.21 & 0.31 & 0.26 & 0.34 \\\\\n  0.60 & 0.70 & 0.65 & 0.75 \\\\\n  0.62 & 0.68 & 0.64 & 0.74 \\\\\n  0.58 & 0.72 & 0.66 & 0.76 \\\\\n  0.61 & 0.69 & 0.63 & 0.73\n  \\end{bmatrix}$$,\n  $y_3 = [0, 0, 0, 0, 1, 1, 1, 1]$,\n  $t_3 = 0.00$.\n- Test case $4$ (collinearity case, low target fraction):\n  $$X_4 = \\begin{bmatrix}\n  0.10 & 0.20 & 0.30 \\\\\n  0.11 & 0.19 & 0.30 \\\\\n  0.09 & 0.21 & 0.30 \\\\\n  0.40 & 0.50 & 0.90 \\\\\n  0.39 & 0.49 & 0.88 \\\\\n  0.41 & 0.51 & 0.92\n  \\end{bmatrix}$$,\n  $y_4 = [0, 0, 0, 1, 1, 1]$,\n  $t_4 = 0.50$.\n\nRequired computation and output:\n- For each test case $(X, y, t)$, standardize $X$ to $Z$, compute the smallest $k$ such that $r_k \\ge t$, reconstruct $\\hat{Z}$ from the top $k$ principal components, compute $J(Z)$, $J(\\hat{Z})$, and $L$.\n- Express all target fractions as decimals. There are no physical units or angles involved in this problem.\n- Your program should produce a single line of output containing the results for all test cases aggregated as a list of lists, where each inner list is $[k, r_k, J(Z), J(\\hat{Z}), L]$ with floating-point values rounded to six decimal places and $k$ as an integer. The final output line must be exactly in the format:\n\"[ [k1,r1,J1,Jhat1,L1],[k2,r2,J2,Jhat2,L2],[k3,r3,J3,Jhat3,L3],[k4,r4,J4,Jhat4,L4] ]\" using commas between values and no extra text.",
            "solution": "The problem requires the implementation of a specific workflow for dimensionality reduction using Principal Component Analysis (PCA) on labeled feature matrices and the subsequent evaluation of information loss with respect to class separability. The solution is derived by methodically applying the provided definitions for data standardization, PCA, data reconstruction, and the Fisher class separability criterion.\n\nThe overall procedure for each test case $(X, y, t)$ is as follows:\n\n1.  **Data Standardization**: The initial step is to transform the raw feature matrix $X \\in \\mathbb{R}^{n \\times d}$ into a standardized matrix $Z \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of samples and $d$ is the number of features. Each feature (column) of $Z$ must have a mean of zero and an unbiased sample standard deviation of one. For each feature $j \\in \\{1, \\dots, d\\}$, the mean $\\mu_j$ and unbiased sample standard deviation $\\sigma_j$ are computed from the corresponding column $X_j$ of $X$:\n    $$ \\mu_j = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij} $$\n    $$ \\sigma_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_{ij} - \\mu_j)^2} $$\n    The standardized feature $Z_{ij}$ is then calculated as:\n    $$ Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j} $$\n    As specified, if any $\\sigma_j = 0$, it is replaced with $1$ to prevent division by zero.\n\n2.  **Principal Component Analysis (PCA)**: PCA is performed on the standardized data $Z$. The sample covariance matrix $C \\in \\mathbb{R}^{d \\times d}$ is computed:\n    $$ C = \\frac{1}{n-1} Z^\\top Z $$\n    Next, an eigen-decomposition of the symmetric matrix $C$ is performed to find its eigenvalues $\\lambda_i$ and corresponding eigenvectors $p_i$.\n    $$ C p_i = \\lambda_i p_i $$\n    The eigenvectors form the columns of the matrix $P$. The eigenpairs $(\\lambda_i, p_i)$ are sorted such that the eigenvalues are in non-increasing order: $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$.\n\n3.  **Component Selection**: The number of principal components, $k$, to retain is determined by the given target variance fraction $t \\in [0, 1]$. We calculate the cumulative explained variance ratio for the first $m$ components:\n    $$ r_m = \\frac{\\sum_{i=1}^{m} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i} $$\n    The selected number of components $k$ is the smallest integer in $\\{0, 1, \\dots, d\\}$ such that $r_k \\ge t$. For the special case $t=0$, we set $k=0$. A numerical tolerance of $10^{-12}$ is used for the comparison $r_k \\ge t$ to account for floating-point inaccuracies.\n\n4.  **Data Reconstruction**: The original standardized data $Z$ is projected onto the $k$-dimensional principal subspace and then reconstructed back into the original $d$-dimensional space. Let $P_k \\in \\mathbb{R}^{d \\times k}$ be the matrix whose columns are the first $k$ principal eigenvectors.\n    The projection scores are computed as:\n    $$ T_k = Z P_k $$\n    The reconstructed standardized data matrix, $\\hat{Z}$, is then:\n    $$ \\hat{Z} = T_k P_k^\\top $$\n    If $k=0$, the reconstructed matrix $\\hat{Z}$ is a zero matrix of the same dimensions as $Z$.\n\n5.  **Separability Assessment**: The Fisher's class separability criterion, $J$, is used to quantify how well the classes are separated in the feature space. This is calculated for both the original standardized data $Z$ and the reconstructed data $\\hat{Z}$.\n    The criterion is the ratio of the between-class scatter trace to the within-class scatter trace:\n    $$ J = \\frac{\\mathrm{tr}(S_B)}{\\mathrm{tr}(S_W)} $$\n    The traces are calculated as:\n    $$ \\mathrm{tr}(S_B) = \\sum_{c} n_c \\| \\mu_c - \\mu \\|_2^2 $$\n    $$ \\mathrm{tr}(S_W) = \\sum_{c} \\sum_{i \\in \\mathcal{I}_c} \\| z_i - \\mu_c \\|_2^2 $$\n    where $\\mu$ is the overall mean of the data, $\\mu_c$ is the mean of data points in class $c$, $n_c$ is the number of samples in class $c$, and $\\mathcal{I}_c$ is the set of indices for samples in class $c$. If $\\mathrm{tr}(S_W)=0$, $J$ is defined as $0$.\n\n6.  **Information Loss Quantification**: Finally, the information loss $L$ due to dimensionality reduction is quantified as the relative decrease in the Fisher separability criterion $J$.\n    $$ L = \\begin{cases} \\max\\left(0, \\dfrac{J(Z) - J(\\hat{Z})}{J(Z)} \\right), & \\text{if } J(Z) > 0 \\\\ 0, & \\text{if } J(Z) = 0 \\end{cases} $$\n    This value represents the fraction of class separability information lost during the PCA reconstruction process.\n\nThe implementation will apply these six steps to each test case provided in the problem statement and format the final results as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the PCA workflow and calculating\n    the information loss for class separability for each test case.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [0.12, 0.38, 0.50],\n                [0.10, 0.35, 0.48],\n                [0.15, 0.40, 0.52],\n                [0.30, 0.55, 0.60],\n                [0.28, 0.50, 0.58],\n                [0.32, 0.57, 0.62]\n            ]),\n            np.array([0, 0, 0, 1, 1, 1]),\n            0.90\n        ),\n        (\n            np.array([\n                [0.05, 0.20],\n                [0.07, 0.22],\n                [0.50, 0.45],\n                [0.48, 0.40],\n                [0.52, 0.47]\n            ]),\n            np.array([0, 0, 1, 1, 1]),\n            1.00\n        ),\n        (\n            np.array([\n                [0.20, 0.30, 0.25, 0.35],\n                [0.22, 0.32, 0.27, 0.36],\n                [0.18, 0.28, 0.24, 0.33],\n                [0.21, 0.31, 0.26, 0.34],\n                [0.60, 0.70, 0.65, 0.75],\n                [0.62, 0.68, 0.64, 0.74],\n                [0.58, 0.72, 0.66, 0.76],\n                [0.61, 0.69, 0.63, 0.73]\n            ]),\n            np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            0.00\n        ),\n        (\n            np.array([\n                [0.10, 0.20, 0.30],\n                [0.11, 0.19, 0.30],\n                [0.09, 0.21, 0.30],\n                [0.40, 0.50, 0.90],\n                [0.39, 0.49, 0.88],\n                [0.41, 0.51, 0.92]\n            ]),\n            np.array([0, 0, 0, 1, 1, 1]),\n            0.50\n        )\n    ]\n\n    def compute_j_criterion(data, labels):\n        \"\"\"Computes Fisher's class separability criterion J.\"\"\"\n        n_samples = data.shape[0]\n        if n_samples == 0:\n            return 0.0\n\n        overall_mean = np.mean(data, axis=0)\n        classes = np.unique(labels)\n        \n        tr_S_W = 0.0\n        tr_S_B = 0.0\n\n        for c in classes:\n            class_samples = data[labels == c]\n            n_c = class_samples.shape[0]\n            if n_c == 0:\n                continue\n            \n            class_mean = np.mean(class_samples, axis=0)\n            \n            tr_S_B += n_c * np.sum((class_mean - overall_mean)**2)\n            tr_S_W += np.sum((class_samples - class_mean)**2)\n\n        if tr_S_W < 1e-12: # As per problem, if tr(S_W)=0, J=0\n            return 0.0\n        \n        return tr_S_B / tr_S_W\n\n    results = []\n    \n    for X, y, t in test_cases:\n        # Step 1: Standardize the feature matrix\n        n, d = X.shape\n        mean_X = np.mean(X, axis=0)\n        std_X = np.std(X, axis=0, ddof=1)\n        std_X[std_X == 0] = 1.0  # Avoid division by zero\n        Z = (X - mean_X) / std_X\n\n        # Step 2: Perform PCA\n        C = (1 / (n - 1)) * (Z.T @ Z)\n        eigenvalues, eigenvectors = np.linalg.eigh(C)\n\n        # Sort eigenvalues and eigenvectors in descending order\n        sort_indices = np.argsort(eigenvalues)[::-1]\n        sorted_eigenvalues = eigenvalues[sort_indices]\n        sorted_eigenvectors = eigenvectors[:, sort_indices]\n\n        # Step 3: Determine the number of components k\n        total_variance = np.sum(sorted_eigenvalues)\n        if total_variance < 1e-12:\n            cumulative_ratios = np.zeros(d)\n        else:\n            cumulative_ratios = np.cumsum(sorted_eigenvalues) / total_variance\n\n        # Select smallest k such that r_k >= t, using a tolerance\n        # and handling the t=0 case.\n        if t <= 1e-12:\n            k = 0\n        else:\n            k_indices = np.where(cumulative_ratios >= t - 1e-12)[0]\n            if len(k_indices) == 0:\n                k = d  # Take all components if t=1 but ratio is slightly less\n            else:\n                k = k_indices[0] + 1\n        \n        r_k = 0.0 if k == 0 else cumulative_ratios[k - 1]\n\n        # Step 4: Reconstruct the data\n        if k == 0:\n            hat_Z = np.zeros_like(Z)\n        else:\n            P_k = sorted_eigenvectors[:, :k]\n            T_k = Z @ P_k\n            hat_Z = T_k @ P_k.T\n\n        # Step 5: Compute J for original and reconstructed data\n        J_Z = compute_j_criterion(Z, y)\n        J_hat_Z = compute_j_criterion(hat_Z, y)\n\n        # Step 6: Compute the information loss L\n        if J_Z > 1e-12:\n            L = max(0.0, (J_Z - J_hat_Z) / J_Z)\n        else:\n            L = 0.0\n\n        results.append([k, r_k, J_Z, J_hat_Z, L])\n    \n    # Format the final output string as specified\n    formatted_results = []\n    for res in results:\n        k, r_val, j_z, j_hat, l_val = res\n        # Format floats to 6 decimal places, k is an integer\n        formatted_str = f\"[{k},{r_val:.6f},{j_z:.6f},{j_hat:.6f},{l_val:.6f}]\"\n        formatted_results.append(formatted_str)\n        \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A trained classifier is only as good as our ability to accurately measure its performance. In many environmental modeling scenarios, relying on simple accuracy can be dangerously misleading due to significant class imbalance—for instance, a rare habitat may cover only a tiny fraction of the study area. This practice moves beyond naive metrics by focusing on the confusion matrix as the source of truth for classifier evaluation. By deriving and comparing metrics like the F1-score, balanced accuracy, and the Matthews Correlation Coefficient (MCC), you will develop a nuanced understanding of how to critically assess and report a model's performance, especially when dealing with the imbalanced datasets that are common in the real world .",
            "id": "3860465",
            "problem": "A supervised land-cover classification was performed over a coastal wetland complex using multispectral satellite data. The workflow included radiometric calibration, atmospheric correction, and orthorectification, followed by feature engineering of spectral and texture descriptors: Normalized Difference Vegetation Index (NDVI), Normalized Difference Water Index (NDWI), Modified Normalized Difference Water Index (MNDWI), tasseled-cap wetness, and Gray-Level Co-occurrence Matrix (GLCM) texture measures. A classifier was trained on these features and evaluated on a stratified validation sample of $12{,}000$ pixels, with $800$ reference wetland pixels (positive class) and $11{,}200$ reference non-wetland pixels (negative class). The resulting confusion matrix (rows are reference labels, columns are classifier predictions) is:\n$$\n\\begin{array}{c|cc}\n & \\text{Predicted wetland} & \\text{Predicted non-wetland} \\\\\n\\hline\n\\text{Reference wetland} & 480 & 320 \\\\\n\\text{Reference non-wetland} & 1200 & 10000\n\\end{array}\n$$\nStarting only from the core definitions of the contingency table counts and the associated rates used in statistical classification (for example, sensitivity, specificity, precision, recall), derive and compute the balanced accuracy, the F1-score, and the Matthews correlation coefficient (MCC). Interpret briefly which of these metrics is most suitable to summarize performance when classes are imbalanced, based on their mathematical construction. Express each metric as an exact analytic form (use a fraction or a radical if it simplifies exactly) and provide your final answer as a single row matrix containing, in order, the balanced accuracy, the F1-score, and the MCC. Do not include units. Do not round.",
            "solution": "The problem is validated as being scientifically grounded, well-posed, objective, and self-contained. The provided information is sufficient and consistent for deriving the required classification performance metrics.\n\nFirst, we must define the components of the confusion matrix. Let the \"wetland\" class be the positive class and \"non-wetland\" be the negative class. The matrix entries are:\n-   True Positives ($TP$): Reference wetland, Predicted wetland. $TP = 480$.\n-   False Negatives ($FN$): Reference wetland, Predicted non-wetland. $FN = 320$.\n-   False Positives ($FP$): Reference non-wetland, Predicted wetland. $FP = 1200$.\n-   True Negatives ($TN$): Reference non-wetland, Predicted non-wetland. $TN = 10000$.\n\nThe total number of actual positive instances (reference wetland) is $P = TP + FN = 480 + 320 = 800$.\nThe total number of actual negative instances (reference non-wetland) is $N = FP + TN = 1200 + 10000 = 11200$.\nThe total sample size is $P + N = 800 + 11200 = 12000$. These values match the problem statement.\n\nNow we derive and compute the required metrics.\n\n**1. Balanced Accuracy ($BA$)**\nBalanced accuracy is defined as the arithmetic mean of sensitivity and specificity.\n\nSensitivity, also known as the True Positive Rate ($TPR$) or recall, is the proportion of actual positives that are correctly identified.\n$$TPR = \\frac{TP}{TP + FN}$$\nSubstituting the given values:\n$$TPR = \\frac{480}{480 + 320} = \\frac{480}{800} = \\frac{48}{80} = \\frac{3}{5}$$\n\nSpecificity, also known as the True Negative Rate ($TNR$), is the proportion of actual negatives that are correctly identified.\n$$TNR = \\frac{TN}{TN + FP}$$\nSubstituting the given values:\n$$TNR = \\frac{10000}{10000 + 1200} = \\frac{10000}{11200} = \\frac{100}{112} = \\frac{25}{28}$$\n\nThe balanced accuracy is then:\n$$BA = \\frac{TPR + TNR}{2}$$\n$$BA = \\frac{\\frac{3}{5} + \\frac{25}{28}}{2} = \\frac{\\frac{3 \\times 28 + 25 \\times 5}{5 \\times 28}}{2} = \\frac{\\frac{84 + 125}{140}}{2} = \\frac{\\frac{209}{140}}{2} = \\frac{209}{280}$$\n\n**2. F1-Score ($F_1$)**\nThe F1-score is the harmonic mean of precision and recall (sensitivity).\n\nPrecision, also known as the Positive Predictive Value ($PPV$), is the proportion of predicted positives that are actually positive.\n$$PPV = \\frac{TP}{TP + FP}$$\nSubstituting the given values:\n$$PPV = \\frac{480}{480 + 1200} = \\frac{480}{1680} = \\frac{48}{168} = \\frac{2}{7}$$\n\nRecall is the same as sensitivity, which we already calculated as $TPR = \\frac{3}{5}$.\n\nThe F1-score is defined as:\n$$F_1 = 2 \\times \\frac{PPV \\times TPR}{PPV + TPR}$$\n$$F_1 = 2 \\times \\frac{\\frac{2}{7} \\times \\frac{3}{5}}{\\frac{2}{7} + \\frac{3}{5}} = 2 \\times \\frac{\\frac{6}{35}}{\\frac{10+21}{35}} = 2 \\times \\frac{\\frac{6}{35}}{\\frac{31}{35}} = 2 \\times \\frac{6}{31} = \\frac{12}{31}$$\n\nAlternatively, using the direct formula:\n$$F_1 = \\frac{2TP}{2TP + FP + FN} = \\frac{2 \\times 480}{2 \\times 480 + 1200 + 320} = \\frac{960}{960 + 1520} = \\frac{960}{2480} = \\frac{96}{248} = \\frac{12}{31}$$\n\n**3. Matthews Correlation Coefficient ($MCC$)**\nThe MCC is a correlation coefficient between the observed and predicted binary classifications. It is defined as:\n$$MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n\nThe numerator is:\n$$TP \\times TN - FP \\times FN = (480 \\times 10000) - (1200 \\times 320) = 4800000 - 384000 = 4416000$$\n\nThe terms in the denominator's square root are:\n-   $TP+FP = 480 + 1200 = 1680$ (Predicted Positives)\n-   $TP+FN = 480 + 320 = 800$ (Actual Positives)\n-   $TN+FP = 10000 + 1200 = 11200$ (Actual Negatives)\n-   $TN+FN = 10000 + 320 = 10320$ (Predicted Negatives)\n\nThe denominator is:\n$$\\sqrt{1680 \\times 800 \\times 11200 \\times 10320}$$\nTo simplify the calculation, we notice that all confusion matrix counts are multiples of $80$:\n$TP = 6 \\times 80$, $FN = 4 \\times 80$, $FP = 15 \\times 80$, $TN = 125 \\times 80$.\nLet $k=80$. Then $TP = 6k, FN = 4k, FP = 15k, TN = 125k$.\n\nThe MCC formula can be rewritten:\n$$MCC = \\frac{(6k)(125k) - (15k)(4k)}{\\sqrt{((6+15)k)((6+4)k)((125+15)k)((125+4)k)}}$$\n$$MCC = \\frac{750k^2 - 60k^2}{\\sqrt{(21k)(10k)(140k)(129k)}} = \\frac{690k^2}{k^2\\sqrt{21 \\times 10 \\times 140 \\times 129}}$$\n$$MCC = \\frac{690}{\\sqrt{21 \\times 10 \\times (14 \\times 10) \\times 129}} = \\frac{690}{10\\sqrt{21 \\times 14 \\times 129}} = \\frac{69}{\\sqrt{(3 \\times 7) \\times (2 \\times 7) \\times (3 \\times 43)}}$$\n$$MCC = \\frac{69}{\\sqrt{2 \\times 3^2 \\times 7^2 \\times 43}} = \\frac{69}{3 \\times 7 \\sqrt{2 \\times 43}} = \\frac{23}{7\\sqrt{86}}$$\n\n**Interpretation of Metrics for Imbalanced Classes**\nThe problem demonstrates significant class imbalance ($800$ vs. $11{,}200$). In such cases, some metrics can be misleading. For instance, the simple accuracy is $\\frac{480+10000}{12000} \\approx 0.87$, which seems high but primarily reflects the classifier's success on the dominant non-wetland class.\n\n-   **Balanced Accuracy ($BA$)**: By its mathematical construction, $BA = \\frac{1}{2}(TPR + TNR)$, it is the mean of the per-class accuracies. It gives equal weight to the performance on the minority class ($TPR$) and the majority class ($TNR$). This makes it a robust indicator of performance on imbalanced datasets, as it is not inflated by high accuracy on the large negative class.\n\n-   **F1-Score ($F_1$)**: As the harmonic mean of precision and recall ($TPR$), the $F_1$-score is constructed to summarize performance on the positive class. Its formula, $F_1 = \\frac{2TP}{2TP + FP + FN}$, does not include the True Negatives ($TN$) count. While useful when the positive class is the primary focus, its complete blindness to the number of correctly classified negative instances means it does not capture the full picture of classifier performance across both classes.\n\n-   **Matthews Correlation Coefficient ($MCC$)**: The MCC's formula, $MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$, is the only one of the three that is constructed using all four values of the confusion matrix. It is a correlation coefficient between the true and predicted classes, producing a score from $-1$ to $+1$. It is a symmetric metric, meaning its value does not change if the positive and negative classes are swapped. Due to its use of all four counts, it is widely regarded as a highly informative and balanced single-score summary, providing a reliable measure of quality even when the classes are of very different sizes.\n\nBased on this analysis of their mathematical construction, the **Matthews Correlation Coefficient (MCC)** is the most suitable metric to summarize performance for imbalanced classes because it synthesizes all aspects of the classification (true and false positives and negatives) into a single, balanced, and robust value.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{209}{280} & \\frac{12}{31} & \\frac{23}{7\\sqrt{86}} \\end{pmatrix}}$$"
        }
    ]
}