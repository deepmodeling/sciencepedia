{
    "hands_on_practices": [
        {
            "introduction": "Hyperspectral datasets often contain hundreds of correlated spectral bands, posing challenges for classification algorithms due to the curse of dimensionality. Principal Component Analysis (PCA) is a foundational technique to address this by projecting the data into a smaller set of uncorrelated components that capture most of the data's variance. This exercise  provides hands-on practice in implementing the core logic of PCA-based dimensionality reduction: determining the number of components needed to retain a specific amount of information, a critical first step in many remote sensing workflows.",
            "id": "3852805",
            "problem": "Consider a hyperspectral data cube represented as a two-dimensional matrix of measurements with $p$ spatial samples (pixels) and $n$ spectral bands, denoted $X \\in \\mathbb{R}^{p \\times n}$. In Hyperspectral Imaging (HSI), each column of $X$ corresponds to a spectral band and each row to a spatial observation. Principal Component Analysis (PCA) is a widely used strategy for spectral dimensionality reduction in Remote Sensing and Environmental Modeling, where the objective is to project the data onto a lower-dimensional subspace spanned by a subset of orthonormal eigenvectors associated with the covariance matrix of the centered data.\n\nStarting from the following fundamental base:\n- The centered data matrix is $X_c = X - \\mathbf{1}\\mu^\\top$, where $\\mu \\in \\mathbb{R}^{n}$ is the vector of band means and $\\mathbf{1} \\in \\mathbb{R}^{p}$ is the vector of ones.\n- The sample covariance matrix is $S = \\frac{1}{p-1} X_c^\\top X_c \\in \\mathbb{R}^{n \\times n}$.\n- The covariance matrix $S$ is symmetric and positive semidefinite, admitting an eigen-decomposition $S = V \\Lambda V^\\top$, where $V \\in \\mathbb{R}^{n \\times n}$ is orthonormal and $\\Lambda = \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n)$ contains the nonnegative eigenvalues.\n- The explained variance ratio of the $i$-th principal component is $r_i = \\lambda_i \\big/ \\sum_{j=1}^{n} \\lambda_j$, and the cumulative explained variance up to $k$ components is $R_k = \\sum_{i=1}^{k} r_i$.\n\nYour task is to construct a PCA-based spectral dimensionality reduction for a hyperspectral cube with $n = 200$ bands by selecting the top $k$ eigenvectors such that the cumulative explained variance $R_k$ is at least $0.95$. You must compute the minimal integer $k$ directly from a provided vector of eigenvalues, without assuming any prior sorting or normalization.\n\nDesign a program that, given a set of eigenvalue sequences for $n = 200$, computes the minimal $k$ such that $R_k \\ge 0.95$ for each sequence. The program must:\n- Sort the eigenvalues in nonincreasing order before computing cumulative explained variance.\n- Handle zero and tied eigenvalues robustly.\n- Return $k = 0$ if the sum of eigenvalues is $0$.\n\nUse the following test suite of eigenvalue sequences (each sequence has length $n = 200$):\n- Test case A (exponential spectral energy decay): $\\lambda_i = \\exp\\!\\big(-\\alpha (i-1)\\big)$ with $\\alpha = 0.1$ for $i = 1,\\dots,200$.\n- Test case B (flat spectrum): $\\lambda_i = 1$ for $i = 1,\\dots,200$.\n- Test case C (rank-deficient with $50$ nonzero components): $\\lambda_i = 1$ for $i \\le 50$ and $\\lambda_i = 0$ for $i > 50$.\n- Test case D (spiked spectrum with top $5$ dominant components): $\\lambda_1 = 500$, $\\lambda_2 = 400$, $\\lambda_3 = 300$, $\\lambda_4 = 200$, $\\lambda_5 = 100$, and $\\lambda_i = 0.1$ for $i \\ge 6$.\n- Test case E (exact threshold at $k = 95$): $\\lambda_i = 1$ for $i \\le 100$ and $\\lambda_i = 0$ for $i > 100$.\n- Test case F (unsorted input): same as Test case A but randomly permuted, i.e., the input eigenvalues are in arbitrary order.\n\nFor each test case, the answer must be the computed minimal integer $k$. There are no physical units involved in this computation, and no angles are used.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$). The output must aggregate the integers $k$ for the test cases in the order A, B, C, D, E, F.",
            "solution": "The problem requires the design of an algorithm to determine the minimum number of principal components, denoted by $k$, required to achieve a cumulative explained variance of at least $0.95$. The input is a sequence of eigenvalues from the spectral covariance matrix of a hyperspectral data cube with $n=200$ bands.\n\nThe core principle of Principal Component Analysis (PCA) for dimensionality reduction is to project data onto a lower-dimensional subspace that captures the maximum possible variance. This subspace is spanned by the eigenvectors of the data's covariance matrix, $S$. The variance captured by each eigenvector (principal component) is given by its corresponding eigenvalue, $\\lambda_i$. The total variance in the dataset is the trace of the covariance matrix, which is equivalent to the sum of all its eigenvalues, $T = \\operatorname{Tr}(S) = \\sum_{j=1}^{n} \\lambda_j$.\n\nTo capture the maximum variance with a limited number of $k$ components, one must choose the eigenvectors associated with the $k$ largest eigenvalues. Therefore, the first step of any such procedure must be to sort the provided eigenvalues in nonincreasing order. Let the sorted sequence of eigenvalues be $\\lambda'_1 \\ge \\lambda'_2 \\ge \\dots \\ge \\lambda'_n \\ge 0$.\n\nThe explained variance ratio for the $i$-th principal component in this sorted list is $r'_i = \\lambda'_i / T$. The cumulative explained variance for the first $k$ components is the sum of their individual ratios:\n$$R_k = \\sum_{i=1}^{k} r'_i = \\frac{\\sum_{i=1}^{k} \\lambda'_i}{\\sum_{j=1}^{n} \\lambda'_j}$$\n\nThe objective is to find the smallest integer $k \\ge 0$ such that $R_k \\ge \\tau$, where the specified threshold is $\\tau = 0.95$.\n\nThe algorithm to compute this minimal $k$ is as follows:\n\n1.  **Receive Input**: The input is a sequence of $n$ non-negative eigenvalues $(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)$.\n\n2.  **Calculate Total Variance**: Compute the sum of all eigenvalues, $T = \\sum_{j=1}^{n} \\lambda_j$.\n\n3.  **Handle Zero Variance Edge Case**: A special case arises if $T = 0$. This implies all eigenvalues are $0$, meaning there is no variance in the data (all spatial samples are identical). In this scenario, no components are needed to explain any variance. As per the problem specification, the minimal number of components is $k=0$.\n\n4.  **Sort Eigenvalues**: If $T > 0$, sort the input eigenvalues in nonincreasing (descending) order to obtain the sequence $\\lambda'_1, \\lambda'_2, \\dots, \\lambda'_n$. This step is critical because it ensures that as we add components one by one, we are always including the one that explains the largest amount of the remaining variance. This greedy approach guarantees that the cumulative variance grows as quickly as possible, ensuring the minimality of $k$.\n\n5.  **Iterate and Accumulate**: Initialize a cumulative variance sum, $C = 0$, and a component counter, $k=0$. Iterate through the sorted eigenvalues $\\lambda'_i$ for $i = 1, 2, \\dots, n$:\n    a. Increment the component counter: $k \\leftarrow k + 1$.\n    b. Add the current eigenvalue to the cumulative sum: $C \\leftarrow C + \\lambda'_i$.\n    c. Check if the cumulative variance threshold is met: $C \\ge \\tau \\times T$. In this problem, $\\tau = 0.95$.\n    d. If the condition is met, the current value of $k$ is the minimal number of components required. The algorithm terminates and returns this value of $k$.\n\nThis iterative process guarantees finding the smallest $k$. For instance, if the condition is first met at step $k=k^*$, we know it was not met at step $k=k^*-1$, thus $k^*$ is minimal. Since $\\sum_{i=1}^{n} \\lambda'_i = T$, the condition will always be met by the time $k=n$ (as $T \\ge 0.95 \\times T$), assuming $T>0$.\n\nApplication to the test cases with $n=200$ and $\\tau=0.95$:\n\n-   **Test Case A (Exponential Decay)**: The eigenvalues $\\lambda_i = \\exp(-0.1(i-1))$ are already sorted. The algorithm will sum them until the cumulative sum reaches $95\\%$ of the total. This requires a numerical calculation.\n-   **Test Case B (Flat Spectrum)**: All $\\lambda_i = 1$. The total variance is $T = 200 \\times 1 = 200$. The target variance is $0.95 \\times 200 = 190$. Since each component adds $1$ to the sum, we need exactly $190$ components. Thus, $k=190$.\n-   **Test Case C (Rank-Deficient)**: $50$ eigenvalues are $1$, the rest are $0$. Total variance $T = 50 \\times 1 = 50$. Target variance is $0.95 \\times 50 = 47.5$. After sorting, the first $50$ eigenvalues are $1$. To exceed a sum of $47.5$, we need to sum $48$ of these eigenvalues. Thus, $k=48$.\n-   **Test Case D (Spiked Spectrum)**: The eigenvalues must first be sorted. The large values ($500, 400, 300, 200, 100$) will be at the start. Total variance is $T = (500+400+300+200+100) + (195 \\times 0.1) = 1500 + 19.5 = 1519.5$. The target variance is $0.95 \\times 1519.5 = 1443.525$.\n    -   $k=1: C_1 = 500$\n    -   $k=2: C_2 = 900$\n    -   $k=3: C_3 = 1200$\n    -   $k=4: C_4 = 1400$ ($< 1443.525$)\n    -   $k=5: C_5 = 1500$ ($\\ge 1443.525$)\n    So, the minimal number of components is $k=5$.\n-   **Test Case E (Exact Threshold)**: $100$ eigenvalues are $1$, the rest are $0$. Total variance $T = 100 \\times 1 = 100$. Target variance is $0.95 \\times 100 = 95$. We need to sum $95$ of the unit eigenvalues to reach the target exactly. Thus, $k=95$.\n-   **Test Case F (Unsorted)**: The eigenvalues are a random permutation of Case A. The sorting step in the algorithm makes the calculation identical to that of Case A, so the result for $k$ will be the same.\n\nThe implementation will follow this logic precisely, ensuring robustness for various eigenvalue distributions and adherence to all specified constraints.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_k(eigenvalues: np.ndarray, threshold: float = 0.95) -> int:\n    \"\"\"\n    Computes the minimal number of principal components (k) to explain a\n    given cumulative variance threshold.\n\n    Args:\n        eigenvalues: A numpy array of eigenvalues.\n        threshold: The cumulative explained variance ratio to achieve.\n\n    Returns:\n        The minimal integer k.\n    \"\"\"\n    # Calculate total variance\n    total_variance = np.sum(eigenvalues)\n\n    # Handle the edge case where there is no variance in the data.\n    if total_variance == 0:\n        return 0\n\n    # Sort eigenvalues in nonincreasing (descending) order\n    sorted_eigenvalues = np.sort(eigenvalues)[::-1]\n\n    # Calculate the target variance to be explained\n    target_variance = threshold * total_variance\n\n    # Iterate to find the minimal k\n    cumulative_variance = 0.0\n    k = 0\n    for eig_val in sorted_eigenvalues:\n        cumulative_variance += eig_val\n        k += 1\n        if cumulative_variance >= target_variance:\n            return k\n    \n    # This part should theoretically not be reached if total_variance > 0,\n    # as the loop will always return a k <= n.\n    return len(eigenvalues)\n\ndef solve():\n    \"\"\"\n    Defines, runs, and formats the output for all test cases.\n    \"\"\"\n    n = 200\n    \n    # Test case A (exponential spectral energy decay)\n    alpha_A = 0.1\n    i_A = np.arange(n)\n    eig_A = np.exp(-alpha_A * i_A)\n\n    # Test case B (flat spectrum)\n    eig_B = np.ones(n)\n\n    # Test case C (rank-deficient with 50 nonzero components)\n    eig_C = np.zeros(n)\n    eig_C[:50] = 1.0\n\n    # Test case D (spiked spectrum with top 5 dominant components)\n    eig_D = np.full(n, 0.1)\n    eig_D[0] = 500.0\n    eig_D[1] = 400.0\n    eig_D[2] = 300.0\n    eig_D[3] = 200.0\n    eig_D[4] = 100.0\n    \n    # Test case E (exact threshold at k = 95)\n    eig_E = np.zeros(n)\n    eig_E[:100] = 1.0\n\n    # Test case F (unsorted input, same as A but permuted)\n    # Use a fixed seed for reproducibility of the permutation\n    rng = np.random.default_rng(seed=42)\n    eig_F = rng.permutation(eig_A)\n\n    test_cases = [\n        eig_A,\n        eig_B,\n        eig_C,\n        eig_D,\n        eig_E,\n        eig_F,\n    ]\n\n    results = []\n    for case_eigenvalues in test_cases:\n        k = compute_k(case_eigenvalues)\n        results.append(k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Classifications based solely on spectral information often result in noisy \"salt-and-pepper\" maps, as they ignore the spatial context that neighboring pixels tend to belong to the same land cover class. Markov Random Fields (MRFs) offer a principled way to integrate this spatial dependency, balancing the evidence from the spectral data with a prior belief in spatial smoothness. In this exercise , you will perform a single optimization step using the $\\alpha$-expansion algorithm, providing a concrete example of how spatial context can be used to refine and improve an initial pixel-wise classification.",
            "id": "3852860",
            "problem": "A multispectral remote sensing classifier provides per-pixel posterior class probabilities for a one-dimensional transect of six adjacent pixels across a vegetation–bare soil transition. Let the two classes be Vegetation ($V$) and Bare Soil ($B$). The spatial prior for class smoothness is modeled as a pairwise Markov Random Field (MRF) with a Potts model penalty. The overall Maximum A Posteriori (MAP) energy for a labeling $l = (l_{1},\\dots,l_{6})$ on the chain graph with four-neighbor structure (adjacent pixels) is\n$$\nE(l) \\;=\\; \\sum_{i=1}^{6} \\Big(-\\ln p_{i}(l_{i})\\Big) \\;+\\; \\lambda \\sum_{(i,j)\\in \\mathcal{N}} \\mathbf{1}\\{l_{i}\\neq l_{j}\\},\n$$\nwhere $p_{i}(c)$ is the posterior probability of class $c\\in\\{V,B\\}$ at pixel $i$, $\\mathcal{N}$ contains the adjacent index pairs $\\{(1,2),(2,3),(3,4),(4,5),(5,6)\\}$, $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, and $\\lambda$ is a nonnegative scalar.\n\nYou are given the following posterior probabilities produced by a spectral classifier:\n- Pixel $1$: $p_{1}(V)=0.65$, $p_{1}(B)=0.35$.\n- Pixel $2$: $p_{2}(V)=0.60$, $p_{2}(B)=0.40$.\n- Pixel $3$: $p_{3}(V)=0.45$, $p_{3}(B)=0.55$.\n- Pixel $4$: $p_{4}(V)=0.40$, $p_{4}(B)=0.60$.\n- Pixel $5$: $p_{5}(V)=0.70$, $p_{5}(B)=0.30$.\n- Pixel $6$: $p_{6}(V)=0.66$, $p_{6}(B)=0.34$.\n\nThe Potts model weight is $\\lambda=0.40$. The initial labeling $l^{(0)}$ is the pixel-wise classification (argmax of $p_{i}(c)$ at each pixel).\n\nStarting from $l^{(0)}$, perform one $\\alpha$-expansion move with $\\alpha=V$ (Vegetation), which is solvable exactly by a single graph cut because the Potts term is a metric. Compute the energy $E(l^{(1)})$ of the labeling after this single $\\alpha$-expansion move. Use the natural logarithm $\\ln$. Express your final answer as a single real number with no units, rounded to four significant figures.",
            "solution": "The user wants me to solve a MAP estimation problem on a Markov Random Field (MRF) using the $\\alpha$-expansion algorithm.\n\n### Step 1: Extract Givens\n-   **Classes**: Vegetation ($V$) and Bare Soil ($B$).\n-   **Pixels**: A one-dimensional transect of $6$ pixels, indexed $i=1, \\dots, 6$.\n-   **Energy Function**: The Maximum A Posteriori (MAP) energy for a labeling $l = (l_1, \\dots, l_6)$ is given by\n    $$\n    E(l) \\;=\\; \\sum_{i=1}^{6} \\Big(-\\ln p_{i}(l_{i})\\Big) \\;+\\; \\lambda \\sum_{(i,j)\\in \\mathcal{N}} \\mathbf{1}\\{l_{i}\\neq l_{j}\\},\n    $$\n    where $p_i(c)$ is the posterior probability of class $c$ at pixel $i$, $\\mathcal{N} = \\{(1,2),(2,3),(3,4),(4,5),(5,6)\\}$ is the set of adjacent pixel pairs, $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n-   **Potts Model Weight**: $\\lambda = 0.40$.\n-   **Posterior Probabilities**:\n    -   $p_1(V)=0.65$, $p_1(B)=0.35$.\n    -   $p_2(V)=0.60$, $p_2(B)=0.40$.\n    -   $p_3(V)=0.45$, $p_3(B)=0.55$.\n    -   $p_4(V)=0.40$, $p_4(B)=0.60$.\n    -   $p_5(V)=0.70$, $p_5(B)=0.30$.\n    -   $p_6(V)=0.66$, $p_6(B)=0.34$.\n-   **Initial Condition**: The initial labeling $l^{(0)}$ is the pixel-wise classification, i.e., $l_i^{(0)} = \\arg\\max_{c \\in \\{V, B\\}} p_i(c)$ for each pixel $i$.\n-   **Task**: Perform one $\\alpha$-expansion move with $\\alpha=V$, starting from $l^{(0)}$, to obtain the new labeling $l^{(1)}$. Then, compute the energy $E(l^{(1)})$ and round the result to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded. It describes a standard application of Markov Random Fields with a Potts model prior for image classification, a well-established technique in remote sensing, computer vision, and related fields. The energy function is a common form for MAP estimation. The $\\alpha$-expansion algorithm is a standard, provably correct method for minimizing such energy functions when the smoothness term is a metric, which the Potts model is. The problem is well-posed, with all necessary data and parameters provided. The definitions are clear and unambiguous. The problem is objective and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution.\n\n### Detailed Solution\n\nThe solution proceeds in four steps:\n1.  Determine the initial labeling $l^{(0)}$.\n2.  Formulate the energy minimization problem for the $\\alpha$-expansion move with $\\alpha=V$.\n3.  Solve the minimization problem to find the new labeling $l^{(1)}$.\n4.  Calculate the final energy $E(l^{(1)})$.\n\n**1. Determine the Initial Labeling $l^{(0)}$**\n\nThe initial labeling $l^{(0)}$ is obtained by selecting the class with the highest posterior probability at each pixel.\n\n-   Pixel $1$: $p_{1}(V) = 0.65 > p_{1}(B)=0.35 \\implies l_{1}^{(0)} = V$.\n-   Pixel $2$: $p_{2}(V) = 0.60 > p_{2}(B)=0.40 \\implies l_{2}^{(0)} = V$.\n-   Pixel $3$: $p_{3}(B) = 0.55 > p_{3}(V)=0.45 \\implies l_{3}^{(0)} = B$.\n-   Pixel $4$: $p_{4}(B) = 0.60 > p_{4}(V)=0.40 \\implies l_{4}^{(0)} = B$.\n-   Pixel $5$: $p_{5}(V) = 0.70 > p_{5}(B)=0.30 \\implies l_{5}^{(0)} = V$.\n-   Pixel $6$: $p_{6}(V) = 0.66 > p_{6}(B)=0.34 \\implies l_{6}^{(0)} = V$.\n\nThus, the initial labeling is $l^{(0)} = (V, V, B, B, V, V)$.\n\n**2. Formulate the $\\alpha$-Expansion Problem**\n\nWe perform one $\\alpha$-expansion move with $\\alpha=V$. This means we seek a new labeling $l^{(1)}$ that minimizes the energy $E(l)$, under the constraint that for each pixel $i$, its new label $l_i^{(1)}$ can either be its current label $l_i^{(0)}$ or the expansion label $\\alpha=V$.\n\nFor pixels $i \\in \\{1, 2, 5, 6\\}$, the initial label is $l_i^{(0)} = V$. Since $\\alpha=V$, the only possible new label is $V$. Thus, their labels are fixed: $l_1^{(1)}=V$, $l_2^{(1)}=V$, $l_5^{(1)}=V$, $l_6^{(1)}=V$.\n\nFor pixels $i \\in \\{3, 4\\}$, the initial label is $l_i^{(0)} = B$. The new label $l_i^{(1)}$ can be either $B$ or $V$. We must find the combination of labels for pixels $3$ and $4$ that minimizes the total energy.\n\nThe total energy for a new labeling $l^{(1)} = (V, V, l_3, l_4, V, V)$, where $l_3, l_4 \\in \\{B, V\\}$, is:\n$$\nE(l^{(1)}) = \\sum_{i=1}^{6} \\Big(-\\ln p_{i}(l_i^{(1)})\\Big) + \\lambda \\sum_{(i,j)\\in \\mathcal{N}} \\mathbf{1}\\{l_i^{(1)} \\neq l_j^{(1)}\\}\n$$\nThe energy can be decomposed into a constant part and a variable part that depends on the choices for $l_3$ and $l_4$.\nThe constant part consists of terms involving only pixels $1, 2, 5, 6$. The variable part, which we must minimize, is:\n$$\nE_{\\text{sub}}(l_3, l_4) = \\big(-\\ln p_3(l_3)\\big) + \\big(-\\ln p_4(l_4)\\big) + \\lambda \\Big( \\mathbf{1}\\{l_2^{(1)}\\neq l_3\\} + \\mathbf{1}\\{l_3\\neq l_4\\} + \\mathbf{1}\\{l_4\\neq l_5^{(1)}\\} \\Big)\n$$\nSubstituting $l_2^{(1)}=V$ and $l_5^{(1)}=V$, we get:\n$$\nE_{\\text{sub}}(l_3, l_4) = -\\ln p_3(l_3) - \\ln p_4(l_4) + \\lambda \\Big( \\mathbf{1}\\{V\\neq l_3\\} + \\mathbf{1}\\{l_3\\neq l_4\\} + \\mathbf{1}\\{l_4\\neq V\\} \\Big)\n$$\n\n**3. Solve the Minimization Problem**\n\nWe evaluate $E_{\\text{sub}}(l_3, l_4)$ for the four possible combinations of $(l_3, l_4)$. We use the given probabilities and $\\lambda = 0.40$. The data terms are:\n-   $-\\ln p_3(B) = -\\ln(0.55) \\approx 0.5978$\n-   $-\\ln p_3(V) = -\\ln(0.45) \\approx 0.7985$\n-   $-\\ln p_4(B) = -\\ln(0.60) \\approx 0.5108$\n-   $-\\ln p_4(V) = -\\ln(0.40) \\approx 0.9163$\n\n-   **Case 1:** $(l_3, l_4) = (B, B)$\n    $E_{\\text{sub}}(B, B) = -\\ln(0.55) - \\ln(0.60) + 0.40 \\cdot (\\mathbf{1}\\{V\\neq B\\} + \\mathbf{1}\\{B\\neq B\\} + \\mathbf{1}\\{B\\neq V\\})$\n    $E_{\\text{sub}}(B, B) = 0.5978 + 0.5108 + 0.40 \\cdot (1 + 0 + 1) = 1.1086 + 0.8 = 1.9086$\n\n-   **Case 2:** $(l_3, l_4) = (B, V)$\n    $E_{\\text{sub}}(B, V) = -\\ln(0.55) - \\ln(0.40) + 0.40 \\cdot (\\mathbf{1}\\{V\\neq B\\} + \\mathbf{1}\\{B\\neq V\\} + \\mathbf{1}\\{V\\neq V\\})$\n    $E_{\\text{sub}}(B, V) = 0.5978 + 0.9163 + 0.40 \\cdot (1 + 1 + 0) = 1.5141 + 0.8 = 2.3141$\n\n-   **Case 3:** $(l_3, l_4) = (V, B)$\n    $E_{\\text{sub}}(V, B) = -\\ln(0.45) - \\ln(0.60) + 0.40 \\cdot (\\mathbf{1}\\{V\\neq V\\} + \\mathbf{1}\\{V\\neq B\\} + \\mathbf{1}\\{B\\neq V\\})$\n    $E_{\\text{sub}}(V, B) = 0.7985 + 0.5108 + 0.40 \\cdot (0 + 1 + 1) = 1.3093 + 0.8 = 2.1093$\n\n-   **Case 4:** $(l_3, l_4) = (V, V)$\n    $E_{\\text{sub}}(V, V) = -\\ln(0.45) - \\ln(0.40) + 0.40 \\cdot (\\mathbf{1}\\{V\\neq V\\} + \\mathbf{1}\\{V\\neq V\\} + \\mathbf{1}\\{V\\neq V\\})$\n    $E_{\\text{sub}}(V, V) = 0.7985 + 0.9163 + 0.40 \\cdot (0 + 0 + 0) = 1.7148$\n\nComparing the four values, the minimum energy is $1.7148$, which corresponds to the case $(l_3, l_4) = (V, V)$.\nTherefore, the $\\alpha$-expansion move changes the labels of pixels $3$ and $4$ to $V$. The resulting labeling is $l^{(1)} = (V, V, V, V, V, V)$.\n\n**4. Calculate the Final Energy $E(l^{(1)})$}\n\nWe now compute the total energy for the optimal labeling $l^{(1)} = (V, V, V, V, V, V)$.\nThe energy simplifies because all labels are identical, making the smoothness term zero.\n$$\nE(l^{(1)}) = \\sum_{i=1}^{6} \\Big(-\\ln p_{i}(V)\\Big) + \\lambda \\sum_{(i,j)\\in \\mathcal{N}} \\mathbf{1}\\{V\\neq V\\}\n$$\n$$\nE(l^{(1)}) = \\sum_{i=1}^{6} \\Big(-\\ln p_{i}(V)\\Big) + 0\n$$\n$$\nE(l^{(1)}) = \\big(-\\ln p_1(V)\\big) + \\big(-\\ln p_2(V)\\big) + \\big(-\\ln p_3(V)\\big) + \\big(-\\ln p_4(V)\\big) + \\big(-\\ln p_5(V)\\big) + \\big(-\\ln p_6(V)\\big)\n$$\nSubstituting the given probabilities:\n$$\nE(l^{(1)}) = \\big(-\\ln(0.65)\\big) + \\big(-\\ln(0.60)\\big) + \\big(-\\ln(0.45)\\big) + \\big(-\\ln(0.40)\\big) + \\big(-\\ln(0.70)\\big) + \\big(-\\ln(0.66)\\big)\n$$\nUsing the natural logarithm:\n$$\nE(l^{(1)}) \\approx 0.43078 + 0.51083 + 0.79851 + 0.91629 + 0.35667 + 0.41551\n$$\n$$\nE(l^{(1)}) \\approx 3.42859\n$$\nRounding the result to four significant figures, we get $3.429$.",
            "answer": "$$\\boxed{3.429}$$"
        },
        {
            "introduction": "Creating a land cover map is only half the task; rigorously assessing its accuracy is equally critical for its scientific and practical application. A confusion matrix serves as the foundation for a suite of metrics that each highlight different aspects of a classifier's performance, from overall correctness to class-specific errors. This practice  offers a hands-on opportunity to move from raw validation results to a comprehensive quantitative assessment, calculating and interpreting essential metrics like the F1-score and Cohen's $\\kappa$ to understand a classifier's true performance.",
            "id": "3852862",
            "problem": "A land-cover classifier for a multispectral satellite image integrates both spectral signatures (e.g., reflectance in shortwave infrared, near infrared, and red bands) and spatial-context features (e.g., local texture statistics computed over a moving window). The classifier is applied to map four classes: Water ($W$), Forest ($F$), Urban ($U$), and Agriculture ($A$). An independent validation sample of $N = 713$ pixels is collected via stratified random sampling across $4$ eco-regions, with ground truth derived from high-resolution aerial interpretation. The following aggregated outcomes summarize the predicted labels produced by the classifier for each ground-truth class, where counts are the number of validation pixels with the specified reference and predicted labels:\n\n- Ground truth $W$: predicted $W = 140$, $F = 3$, $U = 2$, $A = 5$ (row sum $= 150$).\n- Ground truth $F$: predicted $W = 6$, $F = 180$, $U = 12$, $A = 22$ (row sum $= 220$).\n- Ground truth $U$: predicted $W = 1$, $F = 15$, $U = 110$, $A = 14$ (row sum $= 140$).\n- Ground truth $A$: predicted $W = 2$, $F = 18$, $U = 13$, $A = 170$ (row sum $= 203$).\n\nAssume the sampling is design-unbiased for the mapped area, and that the validation pixels are independent draws from the underlying land-cover distribution. In addition, field notes indicate that errors exhibit spatial clustering in agricultural valleys (confusion between $A$ and $F$) and along mixed urban-fringe regions (confusion between $U$ and $F$), while $W$ is relatively spectrally distinct but occasionally confused near shorelines.\n\nStarting from the fundamental definitions of contingency tables and event probabilities in classification assessment, perform the following:\n\n1. Construct the $4 \\times 4$ confusion matrix with classes ordered $[W, F, U, A]$, and compute the row and column marginals.\n2. Derive and compute the overall accuracy as the proportion of correct classifications.\n3. For each class $c \\in \\{W, F, U, A\\}$, define true positives, false positives, and false negatives based on the one-versus-all perspective, and derive the class-wise F1-score using the definitions of precision and recall.\n4. Using the definition of expected agreement under independent labeling given the observed marginals, derive and compute Cohen’s $\\kappa$.\n\nBriefly interpret how overall accuracy, per-class F1-scores, and Cohen’s $\\kappa$ differentially reflect the observed error structures and class prevalence in environmental maps, particularly under spatially clustered misclassification.\n\nReport only Cohen’s $\\kappa$ as the final numeric answer. Round your reported $\\kappa$ to four significant figures and express it as a decimal.",
            "solution": "The problem statement provides a summary of classification results for a land-cover mapping task and asks for a quantitative assessment using standard statistical metrics. The problem is scientifically grounded, well-posed, objective, and internally consistent. The total number of validation pixels, $N = 713$, matches the sum of the provided ground-truth row counts ($150 + 220 + 140 + 203 = 713$). The instruction to assume independent draws allows for the standard application of statistical metrics despite the qualitative note on spatial error clustering. Therefore, the problem is valid and a solution can be derived.\n\n**1. Confusion Matrix and Marginals**\n\nA confusion matrix, denoted $C$, is a square matrix where the element $C_{ij}$ is the number of observations known to be in group $i$ (ground truth) but predicted to be in group $j$. The classes are ordered as $[W, F, U, A]$. The rows represent the ground-truth labels and the columns represent the predicted labels.\n\nThe given data are used to construct the $4 \\times 4$ confusion matrix:\n$$\nC =\n\\begin{pmatrix}\n140 & 3 & 2 & 5 \\\\\n6 & 180 & 12 & 22 \\\\\n1 & 15 & 110 & 14 \\\\\n2 & 18 & 13 & 170\n\\end{pmatrix}\n$$\n\nThe row marginals, representing the total number of pixels for each ground-truth class, are given as:\n- $N_{W, \\text{true}} = 140 + 3 + 2 + 5 = 150$\n- $N_{F, \\text{true}} = 6 + 180 + 12 + 22 = 220$\n- $N_{U, \\text{true}} = 1 + 15 + 110 + 14 = 140$\n- $N_{A, \\text{true}} = 2 + 18 + 13 + 170 = 203$\nThe total number of samples is $N = \\sum_{i} N_{i, \\text{true}} = 150 + 220 + 140 + 203 = 713$.\n\nThe column marginals, representing the total number of pixels predicted for each class, are calculated by summing the columns of $C$:\n- $N_{W, \\text{pred}} = 140 + 6 + 1 + 2 = 149$\n- $N_{F, \\text{pred}} = 3 + 180 + 15 + 18 = 216$\n- $N_{U, \\text{pred}} = 2 + 12 + 110 + 13 = 137$\n- $N_{A, \\text{pred}} = 5 + 22 + 14 + 170 = 211$\nThe sum of column marginals also equals the total number of samples: $149 + 216 + 137 + 211 = 713$.\n\n**2. Overall Accuracy**\n\nThe overall accuracy ($OA$) is the proportion of validation pixels that are correctly classified. It is derived by summing the number of correct classifications (the diagonal elements of the confusion matrix) and dividing by the total number of pixels, $N$.\n$$\nOA = \\frac{\\sum_{i=1}^{k} C_{ii}}{N}\n$$\nwhere $k=4$ is the number of classes.\nThe number of correctly classified pixels is the sum of the diagonal elements of $C$:\n$$\n\\sum_{i=1}^{4} C_{ii} = 140 + 180 + 110 + 170 = 600\n$$\nThe overall accuracy is therefore:\n$$\nOA = \\frac{600}{713} \\approx 0.8415\n$$\n\n**3. Class-wise F1-Score**\n\nFor each class $i$, we adopt a one-versus-all perspective to define true positives ($TP_i$), false positives ($FP_i$), and false negatives ($FN_i$).\n- $TP_i$: Pixels of class $i$ correctly classified as class $i$. $TP_i = C_{ii}$.\n- $FP_i$: Pixels of other classes incorrectly classified as class $i$. $FP_i = (\\sum_{j=1}^{k} C_{ji}) - C_{ii} = N_{i, \\text{pred}} - C_{ii}$.\n- $FN_i$: Pixels of class $i$ incorrectly classified as other classes. $FN_i = (\\sum_{j=1}^{k} C_{ij}) - C_{ii} = N_{i, \\text{true}} - C_{ii}$.\n\nFrom these, we derive Precision ($P_i$) and Recall ($R_i$):\n- Precision ($P_i$): The probability that a pixel predicted as class $i$ is actually class $i$. $P_i = \\frac{TP_i}{TP_i + FP_i} = \\frac{C_{ii}}{N_{i, \\text{pred}}}$.\n- Recall ($R_i$): The probability that a pixel of class $i$ is correctly identified. $R_i = \\frac{TP_i}{TP_i + FN_i} = \\frac{C_{ii}}{N_{i, \\text{true}}}$.\n\nThe F1-score is the harmonic mean of precision and recall, providing a single metric that balances both:\n$$\nF1_i = 2 \\cdot \\frac{P_i \\cdot R_i}{P_i + R_i}\n$$\nThe computations for each class are as follows:\n- **Water (W)**: $TP_W=140, FP_W=9, FN_W=10$.\n  $P_W = \\frac{140}{149}$, $R_W = \\frac{140}{150}$.\n  $F1_W = 2 \\cdot \\frac{\\frac{140}{149} \\cdot \\frac{140}{150}}{\\frac{140}{149} + \\frac{140}{150}} \\approx 0.9364$\n- **Forest (F)**: $TP_F=180, FP_F=36, FN_F=40$.\n  $P_F = \\frac{180}{216}$, $R_F = \\frac{180}{220}$.\n  $F1_F = 2 \\cdot \\frac{\\frac{180}{216} \\cdot \\frac{180}{220}}{\\frac{180}{216} + \\frac{180}{220}} \\approx 0.8257$\n- **Urban (U)**: $TP_U=110, FP_U=27, FN_U=30$.\n  $P_U = \\frac{110}{137}$, $R_U = \\frac{110}{140}$.\n  $F1_U = 2 \\cdot \\frac{\\frac{110}{137} \\cdot \\frac{110}{140}}{\\frac{110}{137} + \\frac{110}{140}} \\approx 0.7942$\n- **Agriculture (A)**: $TP_A=170, FP_A=41, FN_A=33$.\n  $P_A = \\frac{170}{211}$, $R_A = \\frac{170}{203}$.\n  $F1_A = 2 \\cdot \\frac{\\frac{170}{211} \\cdot \\frac{170}{203}}{\\frac{170}{211} + \\frac{170}{203}} \\approx 0.8212$\n\n**4. Cohen’s Kappa Coefficient ($\\kappa$)**\n\nCohen's $\\kappa$ measures the agreement between predicted and true labels, correcting for the agreement that would be expected by chance. It is defined as:\n$$\n\\kappa = \\frac{p_o - p_e}{1 - p_e}\n$$\nHere, $p_o$ is the observed probability of agreement, which is the overall accuracy:\n$$\np_o = OA = \\frac{600}{713}\n$$\nThe quantity $p_e$ is the hypothetical probability of chance agreement. It is calculated from the row and column marginals. For each class $i$, the probability of a random pixel being in class $i$ is $\\frac{N_{i, \\text{true}}}{N}$, and the probability of a random pixel being classified as class $i$ is $\\frac{N_{i, \\text{pred}}}{N}$. The probability of these two agreeing by chance for class $i$ is their product. Summing over all classes gives $p_e$:\n$$\np_e = \\sum_{i=1}^{k} \\frac{N_{i, \\text{true}}}{N} \\cdot \\frac{N_{i, \\text{pred}}}{N} = \\frac{1}{N^2} \\sum_{i=1}^{k} (N_{i, \\text{true}} \\cdot N_{i, \\text{pred}})\n$$\nUsing the previously computed marginals ($N=713$):\n$N_{W, \\text{true}} = 150, N_{F, \\text{true}} = 220, N_{U, \\text{true}} = 140, N_{A, \\text{true}} = 203$.\n$N_{W, \\text{pred}} = 149, N_{F, \\text{pred}} = 216, N_{U, \\text{pred}} = 137, N_{A, \\text{pred}} = 211$.\n\nThe sum of products of marginals is:\n$$\n\\sum (\\text{marginals}) = (150 \\times 149) + (220 \\times 216) + (140 \\times 137) + (203 \\times 211)\n$$\n$$\n\\sum (\\text{marginals}) = 22350 + 47520 + 19180 + 42833 = 131883\n$$\nNow, we compute $p_e$:\n$$\np_e = \\frac{131883}{713^2} = \\frac{131883}{508369}\n$$\nNow we can compute $\\kappa$ using the fractional forms of $p_o$ and $p_e$ for precision:\n$$\np_o \\approx 0.84151473 \\quad \\text{and} \\quad p_e \\approx 0.25942475\n$$\n$$\n\\kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{0.84151473 - 0.25942475}{1 - 0.25942475} = \\frac{0.58208998}{0.74057525} \\approx 0.785994\n$$\nRounding to four significant figures, Cohen's $\\kappa$ is $0.7860$.\n\n**Interpretation**\n\n- **Overall Accuracy ($OA \\approx 0.84$)**: This metric suggests a high level of performance, indicating that about $84\\%$ of the landscape is correctly mapped. However, $OA$ can be influenced by class prevalence and does not reveal the nature of the errors.\n- **F1-Scores**: These class-specific scores reveal performance differences. The high F1-score for Water ($F1_W \\approx 0.94$) confirms the field note that it is spectrally distinct. The lower scores for Forest ($F1_F \\approx 0.83$), Urban ($F1_U \\approx 0.79$), and Agriculture ($F1_A \\approx 0.82$) highlight classification challenges. These lower scores are a direct consequence of higher confusion rates among these classes, which corroborates the field notes about errors in agricultural valleys ($A-F$) and urban fringes ($U-F$). The confusion matrix shows notable error counts for $F \\rightarrow A$ ($22$), $A \\rightarrow F$ ($18$), $U \\rightarrow F$ ($15$), and $F \\rightarrow U$ ($12$).\n- **Cohen’s $\\kappa$ ($\\kappa \\approx 0.786$)**: This value represents a substantial or excellent level of agreement. It improves upon $OA$ by accounting for the agreement expected purely by chance, given the observed class frequencies (the marginals). In this case, since the classes are relatively balanced, the $\\kappa$ value is not drastically different from $OA$, but it remains a more robust indicator of classifier skill. The note on *spatially clustered* misclassification implies that the validation samples are not truly independent, which violates a core assumption of standard significance tests for $\\kappa$. While the calculation of $\\kappa$ itself is arithmetically correct under the problem's explicit assumption of independence, a rigorous analysis would require spatial statistics to account for this autocorrelation, which could lead to wider confidence intervals for the metric. In essence, $\\kappa$ quantifies the skill above chance, while the F1-scores diagnose where that skill is lacking, and the qualitative field notes suggest a spatial, rather than random, pattern to those shortcomings.",
            "answer": "$$\\boxed{0.7860}$$"
        }
    ]
}