{
    "hands_on_practices": [
        {
            "introduction": "The parallelepiped classifier is conceptually one of the simplest methods for supervised classification, defining an acceptance region as a hyperrectangle in the feature space. While intuitive, its performance in high-dimensional settings, typical of modern multispectral sensors, can be surprisingly poor. This exercise explores the 'curse of dimensionality' by demonstrating how requiring a pixel to be 'typical' across many spectral bands simultaneously leads to a dramatic drop in expected classification accuracy, a foundational concept for any remote sensing analyst .",
            "id": "3830385",
            "problem": "A multispectral imaging instrument acquires $d$-dimensional feature vectors $\\mathbf{X} \\in \\mathbb{R}^{d}$ of surface reflectance after atmospheric correction. For a land cover class $C$, a parallelepiped classifier defines an axis-aligned acceptance region $A = \\prod_{j=1}^{d} I_{j}$, where each $I_{j} \\subset \\mathbb{R}$ is a closed interval along band $j$ estimated from training data. After a linear whitening transform based on Principal Component Analysis (PCA), assume that the transformed class-conditional features are standardized and statistically independent across bands. The empirical marginal coverage of $I_{j}$ for class $C$ is $p_{j} \\in (0,1)$, defined as the probability that a class-$C$ pixel’s band-$j$ value falls inside $I_{j}$.\n\nStarting only from the definition of joint probability on product spaces, the independence of transformed bands, and the definition of $p_{j}$ as a marginal coverage, derive the expected fraction of class-$C$ pixels accepted by the parallelepiped as a function of $\\{p_{j}\\}_{j=1}^{d}$. Use geometric measure to explain why increasing $d$ simultaneously inflates the absolute volume of $A$ and yet reduces the expected joint coverage under the class distribution. Briefly contrast this with the behavior of a minimum distance classifier (which assigns every pixel to the nearest class mean and does not include a reject option) in terms of coverage.\n\nThen, for a vegetation class in $d = 6$ whitened bands with empirically measured marginal coverages\n$$\np_{1} = 0.93,\\quad p_{2} = 0.89,\\quad p_{3} = 0.90,\\quad p_{4} = 0.88,\\quad p_{5} = 0.95,\\quad p_{6} = 0.87,\n$$\ncompute the expected acceptance probability of the parallelepiped classifier for class $C$. Round your final probability to $4$ significant figures and express it as a decimal.",
            "solution": "The problem asks for a derivation of the expected acceptance probability for a parallelepiped classifier under the assumption of statistically independent features, an explanation of the classifier's behavior in high dimensions, a brief comparison with a minimum distance classifier, and a specific numerical calculation. The problem is well-posed, scientifically grounded, and provides all necessary information for a complete solution.\n\nFirst, we derive the general expression for the expected fraction of class-$C$ pixels accepted by the classifier. Let $\\mathbf{X} = (X_{1}, X_{2}, \\dots, X_{d})$ be the $d$-dimensional random vector representing the features of a pixel belonging to class $C$. The parallelepiped classifier accepts this pixel if its feature vector falls within the acceptance region $A$. The region $A$ is defined as the Cartesian product of $d$ closed intervals, $A = \\prod_{j=1}^{d} I_{j}$, where $I_{j}$ is the acceptance interval for the $j$-th band.\n\nA pixel is accepted if and only if its feature value $X_{j}$ falls within the interval $I_{j}$ for all bands $j = 1, \\dots, d$. The expected fraction of accepted pixels is the probability of this joint event, which we denote as $P_{\\text{accept}}$.\n$$\nP_{\\text{accept}} = P(\\mathbf{X} \\in A) = P(X_{1} \\in I_{1} \\text{ and } X_{2} \\in I_{2} \\text{ and } \\dots \\text{ and } X_{d} \\in I_{d})\n$$\nThe problem statement specifies that after a whitening transformation, the class-conditional features are statistically independent across bands. The definition of statistical independence for a set of events is that the probability of their joint occurrence is the product of their individual probabilities. Therefore, we can write:\n$$\nP_{\\text{accept}} = P(X_{1} \\in I_{1}) \\times P(X_{2} \\in I_{2}) \\times \\dots \\times P(X_{d} \\in I_{d})\n$$\nThe problem defines the empirical marginal coverage $p_{j}$ as the probability that a class-$C$ pixel’s band-$j$ value falls inside the interval $I_{j}$. That is, $p_{j} = P(X_{j} \\in I_{j})$. Substituting this definition into the expression for $P_{\\text{accept}}$, we arrive at the desired result:\n$$\nP_{\\text{accept}} = \\prod_{j=1}^{d} p_{j}\n$$\nThis expression gives the expected acceptance probability for class $C$ as a function of the marginal coverages $\\{p_{j}\\}_{j=1}^{d}$.\n\nNext, we address the apparent paradox of how increasing the dimensionality $d$ can inflate the volume of the acceptance region $A$ while simultaneously reducing the expected joint coverage, $P_{\\text{accept}}$.\nThe geometric measure, or volume, of the axis-aligned parallelepiped $A$ is the product of the lengths of its defining intervals:\n$$\n\\text{Vol}(A) = \\prod_{j=1}^{d} \\text{Length}(I_{j})\n$$\nFor the features to be standardized, their distributions typically have a finite standard deviation, such as $\\sigma_{j}=1$. To achieve a high marginal coverage $p_{j}$ (e.g., $p_{j} = 0.95$), the interval $I_{j}$ must span a considerable range. For instance, if the whitened features follow a standard normal distribution, an interval capturing $p_{j}=0.95$ of the probability mass must have a length of approximately $3.92$. Since this length is greater than $1$, the volume $\\text{Vol}(A)$ will grow exponentially with increasing dimensionality $d$, as $(\\text{Length})^{d}$. This leads to a rapid inflation of the absolute volume of the acceptance region.\nConversely, the joint coverage, or acceptance probability, is $P_{\\text{accept}} = \\prod_{j=1}^{d} p_{j}$. Since each $p_{j}$ is a probability, it must be that $p_{j} \\in (0, 1)$. The product of multiple numbers less than $1$ results in a smaller number. If we assume for simplicity that all marginal coverages are equal to a constant $p  1$, then $P_{\\text{accept}} = p^{d}$. This value decreases exponentially towards $0$ as $d$ increases. This phenomenon is a manifestation of the \"curse of dimensionality\": while the hyper-rectangular region $A$ encompasses a large volume in feature space, this volume is mostly \"empty\" corner space where the probability density is vanishingly small. The actual probability mass of the class distribution becomes increasingly concentrated in a region that is poorly approximated by a hyperrectangle, and the requirement for a sample to be \"typical\" in all dimensions simultaneously becomes exceedingly strict.\n\nNow, we briefly contrast the behavior of the parallelepiped classifier with a minimum distance classifier. The minimum distance classifier partitions the entire feature space $\\mathbb{R}^{d}$ into decision regions, one for each class. Specifically, every point $\\mathbf{x} \\in \\mathbb{R}^{d}$ is assigned to the class $C_{k}$ whose mean vector $\\boldsymbol{\\mu}_{k}$ is closest to $\\mathbf{x}$. This means there is no \"reject\" option; every pixel is classified. The total coverage over all classes is thus $100 \\%$. In contrast, the parallelepiped classifier defines explicit, bounded acceptance regions for each class. Any pixel falling outside all defined parallelepipeds is rejected or left unclassified. Therefore, its total coverage is typically well below $100 \\%$. While the minimum distance classifier is also affected by the curse of dimensionality (distances in high-dimensional spaces can become less meaningful), its behavior with respect to coverage is fundamentally different, as it enforces a complete tessellation of the feature space.\n\nFinally, we compute the expected acceptance probability for the given vegetation class. The dimensionality is $d=6$, and the marginal coverages are:\n$p_{1} = 0.93$\n$p_{2} = 0.89$\n$p_{3} = 0.90$\n$p_{4} = 0.88$\n$p_{5} = 0.95$\n$p_{6} = 0.87$\n\nUsing the derived formula, the expected acceptance probability is the product of these values:\n$$\nP_{\\text{accept}} = p_{1} \\times p_{2} \\times p_{3} \\times p_{4} \\times p_{5} \\times p_{6}\n$$\n$$\nP_{\\text{accept}} = 0.93 \\times 0.89 \\times 0.90 \\times 0.88 \\times 0.95 \\times 0.87\n$$\nCalculating this product gives:\n$$\nP_{\\text{accept}} \\approx 0.5418024876\n$$\nThe problem requires this value to be rounded to $4$ significant figures. The first four significant figures are $5$, $4$, $1$, and $8$. The fifth digit is $0$, so we round down.\n$$\nP_{\\text{accept}} \\approx 0.5418\n$$\nThis demonstrates that even with high marginal coverages in each band (all close to $0.90$), the probability of a pixel satisfying all criteria simultaneously drops to just over $0.54$.",
            "answer": "$$\n\\boxed{0.5418}\n$$"
        },
        {
            "introduction": "The practical performance of a classifier depends heavily on how its parameters are estimated from training data, which can be contaminated with outliers. This practice delves into the crucial concept of robustness by comparing two common methods for defining a parallelepiped classifier: one based on the absolute minimum and maximum values, and another based on the sample mean and standard deviation. By analyzing their response to a single extreme outlier, you will gain insight into the stability of these statistical estimators and their impact on classifier design .",
            "id": "3830449",
            "problem": "A remote sensing analyst is designing a supervised classifier for multispectral satellite imagery to delineate a land cover class $k$ (e.g., temperate broadleaf vegetation) in a feature space of $p$ spectral bands. For each class $k$, training data consist of $n$ pixels with spectral vectors $\\mathbf{x}^{(i)} \\in \\mathbb{R}^p$, $i = 1, \\dots, n$. The analyst considers non-parametric parallelepiped acceptance rules and a minimum distance rule defined as follows.\n\nDefine, for each band $j \\in \\{1, \\dots, p\\}$, the sample minimum $a_{kj} = \\min_{1 \\le i \\le n} x^{(i)}_j$, sample maximum $b_{kj} = \\max_{1 \\le i \\le n} x^{(i)}_j$, sample mean $m_{kj} = \\frac{1}{n}\\sum_{i=1}^n x^{(i)}_j$, and sample standard deviation $s_{kj} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n \\left(x^{(i)}_j - m_{kj}\\right)^2}$. Consider two parallelepiped acceptance regions for class $k$:\n- Min–max parallelepiped: $\\mathbf{x} \\in \\mathcal{P}_k^{\\mathrm{minmax}}$ if and only if $a_{kj} \\le x_j \\le b_{kj}$ for all $j$.\n- Mean–standard deviation parallelepiped: for a chosen scalar $\\alpha > 0$, $\\mathbf{x} \\in \\mathcal{P}_k^{\\alpha}$ if and only if $m_{kj} - \\alpha s_{kj} \\le x_j \\le m_{kj} + \\alpha s_{kj}$ for all $j$.\n\nAdditionally, consider the minimum distance classifier that assigns $\\mathbf{x}$ to the class $k$ minimizing the Euclidean distance $d_k(\\mathbf{x}) = \\lVert \\mathbf{x} - \\boldsymbol{\\mu}_k \\rVert_2$, where $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^p$ denotes a representative class center taken to be the sample mean vector $\\boldsymbol{\\mu}_k = (m_{k1}, \\dots, m_{kp})^\\top$.\n\nSuppose for a specific class $k$ and a particular band $j^\\star$, the class-conditional distribution of clean (non-contaminated) training and test pixels is well approximated by an independent Gaussian model across bands, i.e., $X_j \\sim \\mathcal{N}(\\mu_{kj}, \\sigma_{kj}^2)$ independently for $j = 1, \\dots, p$. Now consider contamination of the training set by a single saturated pixel in band $j^\\star$ with value $M \\gg \\mu_{kj^\\star} + 10\\sigma_{kj^\\star}$, leaving all other bands and samples unchanged. The contaminated training set thus has one extreme outlier on band $j^\\star$ and no outliers on the other bands. Assume test pixels remain clean and follow the stated Gaussian model.\n\nBased on the fundamental definitions above and well-tested properties of Gaussian models and sample statistics, which of the following statements are correct?\n\nA. With even a single extreme outlier on band $j^\\star$, the min–max bounds on $j^\\star$ expand to include that outlier exactly, potentially inflating $\\mathcal{P}_k^{\\mathrm{minmax}}$ arbitrarily along $j^\\star$. In contrast, the mean–standard deviation bounds inflate through the influence of the outlier on $m_{kj^\\star}$ and $s_{kj^\\star}$, which scales with its leverage on these sample statistics and the training size $n$; hence, mean–standard deviation parallelepipeds are generally more robust to single extreme outliers than min–max parallelepipeds.\n\nB. Under the Gaussian class-conditional model and independence across bands, choosing $\\alpha$ to target nominal coverage (e.g., $95\\%$ per band) guarantees that the true acceptance probability for clean test pixels under $m_{kj} \\pm \\alpha s_{kj}$ is exactly $95\\%$ per band regardless of training sample size $n$ and regardless of outlier contamination, whereas min–max parallelepipeds always have acceptance probability at most $95\\%$.\n\nC. Minimum distance classification is unaffected by outliers because it uses only class means and Euclidean distances; therefore it is strictly more robust than either min–max or mean–standard deviation parallelepipeds.\n\nD. For heavy-tailed or skewed spectral distributions, replacing $m_{kj} \\pm \\alpha s_{kj}$ with a robust center and scale, such as the component-wise median and a scaled Median Absolute Deviation (MAD), yields a parallelepiped-like acceptance region that is less sensitive to outliers, at the cost of reduced efficiency under Gaussian assumptions.\n\nE. For the $m_{kj} \\pm \\alpha s_{kj}$ rule, increasing $\\alpha$ monotonically increases the acceptance probability for clean class-$k$ pixels and also increases the chance of including pixels from other classes; for min–max parallelepipeds, acceptance is determined by the training extremes and lacks a continuous tuning parameter analogous to $\\alpha$, so the bias–variance trade-off in acceptance cannot be smoothly adjusted without modifying the training set or adopting additional heuristics.",
            "solution": "This problem assesses the robustness of different classifier construction methods in the presence of a single extreme outlier. Let's analyze the impact of the outlier on each method and evaluate the given statements. The outlier is a single large value $M$ for band $j^\\star$ in one training sample.\n\n**Analysis of Statement A:**\n*   **Min-max parallelepiped:** The bounds are defined by the absolute minimum and maximum values in the training data. The upper bound for band $j^\\star$, $b_{kj^\\star}$, will become exactly equal to the outlier value $M$. This means the acceptance region expands dramatically along that one axis, making the method extremely sensitive to the outlier.\n*   **Mean-standard deviation parallelepiped:** The sample mean $m_{kj^\\star}$ and sample standard deviation $s_{kj^\\star}$ are both influenced by the outlier. The mean is pulled towards $M$ with an effect proportional to $1/n$. The standard deviation is grossly inflated by the term $(M - m'_{kj^\\star})^2$, scaling roughly as $M/\\sqrt{n-1}$. While both statistics are affected, their change is moderated by the sample size $n$.\n*   **Comparison:** The min-max bound is directly determined by the outlier's value, representing a breakdown point of $1/n$. The mean-std-dev method is also non-robust, but the outlier's influence is averaged down. Therefore, the mean-standard deviation method is considered more robust to a *single* extreme outlier than the min-max method for any reasonably large training set.\n*   **Conclusion:** Statement A is correct.\n\n**Analysis of Statement B:**\n*   This statement claims that the interval $m_{kj} \\pm \\alpha s_{kj}$ guarantees a fixed coverage (e.g., 95%) for clean test pixels, regardless of sample size or outliers. This is false. The interval is constructed from sample statistics, which are random variables. The actual coverage of such an interval for a new observation is described by a tolerance interval, which depends on the sample size $n$ (related to the t-distribution). Furthermore, the presence of an outlier completely distorts $m_{kj^\\star}$ and $s_{kj^\\star}$, invalidating any coverage claims for clean data. The assertion about min-max parallelepipeds is also incorrect; their coverage for new data is not capped at 95%.\n*   **Conclusion:** Statement B is incorrect.\n\n**Analysis of Statement C:**\n*   This statement claims the minimum distance classifier is unaffected by outliers because it uses means. This is false. The sample mean is a non-robust statistic and is highly sensitive to outliers. The outlier $M$ will pull the class centroid $\\boldsymbol{\\mu}_k$ towards it, which in turn shifts all the decision boundaries associated with that class. Therefore, the classifier is not robust to outliers.\n*   **Conclusion:** Statement C is incorrect.\n\n**Analysis of Statement D:**\n*   This statement proposes using robust estimators like the median (for center) and Median Absolute Deviation (MAD, for scale) instead of the mean and standard deviation. This is a standard technique in robust statistics. The median and MAD have high breakdown points (50%), meaning they are highly insensitive to a small fraction of extreme outliers. Using them would create a parallelepiped far more stable in the presence of the outlier $M$. The statement also correctly notes the trade-off: these robust estimators are less statistically efficient (i.e., have higher variance) than the mean and standard deviation when the data is perfectly Gaussian and free of outliers.\n*   **Conclusion:** Statement D is correct.\n\n**Analysis of Statement E:**\n*   This statement correctly identifies a key difference in the flexibility of the two parallelepiped methods. For the mean-std-dev rule, $\\alpha$ is a continuous tuning parameter that directly controls the size of the acceptance region, allowing a user to smoothly adjust the trade-off between accepting more true positives and accepting more false positives. The min-max rule has no such parameter; its boundaries are fixed by the data extremes. To adjust its behavior, one would need to modify the training set (e.g., by trimming outliers) or use other ad-hoc rules.\n*   **Conclusion:** Statement E is correct.\n\nBased on this analysis, the correct statements are A, D, and E.",
            "answer": "$$\n\\boxed{ADE}\n$$"
        },
        {
            "introduction": "While simple heuristics like the minimum distance-to-mean classifier are widely used, a deeper understanding requires knowing when they approximate theoretically optimal solutions. This exercise bridges the gap between heuristic methods and formal decision theory by deriving the conditions under which the minimum distance rule aligns with the Bayes optimal classifier. This analysis reveals the implicit assumptions about data structure—specifically regarding class priors and variance—that make this simple classifier a powerful and justifiable choice .",
            "id": "3830408",
            "problem": "A multispectral remote sensing analyst is classifying two land-cover classes using a two-stage non-parametric procedure: a parallelepiped rejector followed by a minimum distance-to-mean rule. Let the image feature vector be $\\mathbf{x} \\in \\mathbb{R}^{p}$, and suppose the class-conditional distributions are multivariate normal with means $\\boldsymbol{\\mu}_{1}, \\boldsymbol{\\mu}_{2} \\in \\mathbb{R}^{p}$ and a common, spherical covariance $\\sigma^{2} I_{p}$, where $I_{p}$ is the $p \\times p$ identity matrix and $\\sigma^{2} > 0$. The prior probabilities are $\\pi_{1}$ and $\\pi_{2}$ with $\\pi_{1} + \\pi_{2} = 1$ and $\\pi_{1}, \\pi_{2} \\in (0,1)$. The parallelepiped rejector uses per-band intervals that are sufficiently wide so that, along the line segment joining $\\boldsymbol{\\mu}_{1}$ and $\\boldsymbol{\\mu}_{2}$, it does not truncate the decision boundary region of interest. The minimum distance-to-mean rule assigns $\\mathbf{x}$ to the class whose mean is nearest in Euclidean distance.\n\nStarting only from Bayes decision theory and the Gaussian likelihood, derive the Bayes decision boundary for these two classes and express the signed offset, measured along the unit vector in the direction of $\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{2}$, between this Bayes boundary and the perpendicular bisector of the segment joining $\\boldsymbol{\\mu}_{1}$ and $\\boldsymbol{\\mu}_{2}$. Let $D = \\|\\boldsymbol{\\mu}_{1} - \\boldsymbol{\\mu}_{2}\\|_{2}$ denote the inter-mean separation and let $\\alpha \\in (0,1)$ be a specified tolerance. Impose the requirement that the magnitude of this offset does not exceed $\\alpha$ times half the inter-mean separation, so that the minimum distance-to-mean decision boundary approximates the Bayes boundary to within the specified tolerance along that axis. Under this requirement, determine the maximum allowable prior odds factor\n$$\\kappa_{\\max} = \\max\\left\\{\\frac{\\pi_{1}}{\\pi_{2}}, \\frac{\\pi_{2}}{\\pi_{1}}\\right\\}$$\nas a closed-form symbolic expression in terms of $\\sigma^{2}$, $D$, and $\\alpha$. The final answer must be this single expression for $\\kappa_{\\max}$. Express the final answer as a dimensionless analytic expression. No rounding is required.",
            "solution": "The objective is to find the maximum allowable prior odds factor, $\\kappa_{\\max}$, given a constraint on the deviation between the Bayes optimal decision boundary and the minimum distance-to-mean decision boundary. The derivation begins from Bayes decision theory for two classes with Gaussian distributions.\n\nThe Bayes decision rule states that a feature vector $\\mathbf{x} \\in \\mathbb{R}^{p}$ is assigned to the class $\\omega_i$ that maximizes the posterior probability $P(\\omega_i | \\mathbf{x})$. The decision boundary is the set of points where the posterior probabilities are equal, which is equivalent to the condition $\\pi_1 p(\\mathbf{x}|\\omega_1) = \\pi_2 p(\\mathbf{x}|\\omega_2)$, where $\\pi_i$ is the prior probability of class $\\omega_i$ and $p(\\mathbf{x}|\\omega_i)$ is the class-conditional probability density function (likelihood).\n\nTaking the natural logarithm of the decision boundary condition gives:\n$$\\ln(\\pi_1) + \\ln(p(\\mathbf{x}|\\omega_1)) = \\ln(\\pi_2) + \\ln(p(\\mathbf{x}|\\omega_2))$$\n\nThe likelihoods are given as multivariate normal distributions $N(\\boldsymbol{\\mu}_i, \\sigma^2 I_p)$, so the probability density for class $\\omega_i$ is:\n$$p(\\mathbf{x}|\\omega_i) = \\frac{1}{(2\\pi \\sigma^2)^{p/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\|\\mathbf{x}-\\boldsymbol{\\mu}_i\\|_2^2\\right)$$\nThe corresponding log-likelihood is:\n$$\\ln(p(\\mathbf{x}|\\omega_i)) = -\\frac{p}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\|\\mathbf{x}-\\boldsymbol{\\mu}_i\\|_2^2$$\nSubstituting this into the boundary equation:\n$$\\ln(\\pi_1) - \\frac{p}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\|\\mathbf{x}-\\boldsymbol{\\mu}_1\\|_2^2 = \\ln(\\pi_2) - \\frac{p}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{1}{2\\sigma^{2}}\\|\\mathbf{x}-\\boldsymbol{\\mu}_2\\|_2^2$$\nThe constant term $-\\frac{p}{2}\\ln(2\\pi\\sigma^{2})$ cancels. Rearranging the remaining terms gives:\n$$\\|\\mathbf{x}-\\boldsymbol{\\mu}_2\\|_2^2 - \\|\\mathbf{x}-\\boldsymbol{\\mu}_1\\|_2^2 = 2\\sigma^{2}(\\ln(\\pi_2) - \\ln(\\pi_1))$$\nExpanding the squared norms $(\\mathbf{x}^\\top \\mathbf{x} - 2\\mathbf{x}^\\top\\boldsymbol{\\mu}_i + \\boldsymbol{\\mu}_i^\\top\\boldsymbol{\\mu}_i)$:\n$$(\\mathbf{x}^\\top \\mathbf{x} - 2\\mathbf{x}^\\top\\boldsymbol{\\mu}_2 + \\boldsymbol{\\mu}_2^\\top\\boldsymbol{\\mu}_2) - (\\mathbf{x}^\\top \\mathbf{x} - 2\\mathbf{x}^\\top\\boldsymbol{\\mu}_1 + \\boldsymbol{\\mu}_1^\\top\\boldsymbol{\\mu}_1) = 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\nThe $\\mathbf{x}^\\top \\mathbf{x}$ terms cancel, leaving the equation of a hyperplane:\n$$2\\mathbf{x}^\\top(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) - (\\boldsymbol{\\mu}_1^\\top\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2^\\top\\boldsymbol{\\mu}_2) = 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\n$$2\\mathbf{x}^\\top(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = \\|\\boldsymbol{\\mu}_1\\|_2^2 - \\|\\boldsymbol{\\mu}_2\\|_2^2 + 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\nThis is the equation for the Bayes optimal decision boundary.\n\nThe minimum distance-to-mean decision boundary is defined by the condition $\\|\\mathbf{x}-\\boldsymbol{\\mu}_1\\|_2 = \\|\\mathbf{x}-\\boldsymbol{\\mu}_2\\|_2$, or $\\|\\mathbf{x}-\\boldsymbol{\\mu}_1\\|_2^2 = \\|\\mathbf{x}-\\boldsymbol{\\mu}_2\\|_2^2$. This corresponds to the case where the right-hand side of the general Bayes boundary equation is zero, which happens when the priors are equal. The equation for this boundary is:\n$$2\\mathbf{x}^\\top(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = \\|\\boldsymbol{\\mu}_1\\|_2^2 - \\|\\boldsymbol{\\mu}_2\\|_2^2$$\nThis is the equation of the perpendicular bisector of the line segment connecting $\\boldsymbol{\\mu}_1$ and $\\boldsymbol{\\mu}_2$.\n\nBoth boundaries are hyperplanes with the same normal vector $(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)$. The signed offset, $d$, between the two parallel hyperplanes, measured along the unit normal vector $\\mathbf{u} = \\frac{\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2}{\\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2}$, can be found by considering a point $\\mathbf{x}_B$ on the Bayes boundary and a point $\\mathbf{x}_{MD}$ on the minimum distance boundary. The vector $\\mathbf{x}_B - \\mathbf{x}_{MD}$ can be written as $d \\cdot \\mathbf{u}$.\nSubtracting the minimum distance boundary equation from the Bayes boundary equation gives:\n$$2\\mathbf{x}_B^\\top(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) - 2\\mathbf{x}_{MD}^\\top(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\n$$2(\\mathbf{x}_B - \\mathbf{x}_{MD})^\\top(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\nSubstituting $\\mathbf{x}_B - \\mathbf{x}_{MD} = d \\cdot \\mathbf{u} = d \\frac{\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2}{D}$, where $D = \\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2$:\n$$2\\left(d \\frac{\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2}{D}\\right)^\\top(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\n$$\\frac{2d}{D} (\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2)^\\top(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2) = 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\n$$\\frac{2d}{D} \\|\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_2\\|_2^2 = 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\n$$\\frac{2dD^{2}}{D} = 2\\sigma^{2}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\nSolving for the signed offset $d$:\n$$d = \\frac{\\sigma^{2}}{D}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)$$\n\nThe problem requires that the magnitude of this offset does not exceed $\\alpha$ times half the inter-mean separation:\n$$|d| \\le \\alpha \\frac{D}{2}$$\nSubstituting the expression for $d$:\n$$\\left|\\frac{\\sigma^{2}}{D}\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)\\right| \\le \\frac{\\alpha D}{2}$$\nSince $\\sigma^{2} > 0$ and $D > 0$, we can write:\n$$\\frac{\\sigma^{2}}{D}\\left|\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)\\right| \\le \\frac{\\alpha D}{2}$$\nIsolating the logarithm term:\n$$\\left|\\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right)\\right| \\le \\frac{\\alpha D^{2}}{2\\sigma^{2}}$$\nThis is equivalent to the inequality:\n$$-\\frac{\\alpha D^{2}}{2\\sigma^{2}} \\le \\ln\\left(\\frac{\\pi_2}{\\pi_1}\\right) \\le \\frac{\\alpha D^{2}}{2\\sigma^{2}}$$\nExponentiating all parts gives the range for the odds ratio $\\frac{\\pi_2}{\\pi_1}$:\n$$\\exp\\left(-\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right) \\le \\frac{\\pi_2}{\\pi_1} \\le \\exp\\left(\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right)$$\n\nThe quantity to be determined is $\\kappa_{\\max} = \\max\\{\\frac{\\pi_1}{\\pi_2}, \\frac{\\pi_2}{\\pi_1}\\}$.\nIf $\\frac{\\pi_2}{\\pi_1} \\ge 1$, then $\\kappa = \\frac{\\pi_2}{\\pi_1}$. The inequality provides the upper bound $\\kappa \\le \\exp\\left(\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right)$.\nIf $\\frac{\\pi_2}{\\pi_1}  1$, then $\\frac{\\pi_1}{\\pi_2}  1$, so $\\kappa = \\frac{\\pi_1}{\\pi_2}$. From the left side of the inequality, $\\frac{\\pi_2}{\\pi_1} \\ge \\exp\\left(-\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right)$. Taking the reciprocal reverses the inequality: $\\frac{\\pi_1}{\\pi_2} \\le \\frac{1}{\\exp\\left(-\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right)} = \\exp\\left(\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right)$. So, $\\kappa \\le \\exp\\left(\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right)$.\nIn both cases, the maximum allowable value for $\\kappa$ is the same. Therefore,\n$$\\kappa_{\\max} = \\exp\\left(\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right)$$",
            "answer": "$$\\boxed{\\exp\\left(\\frac{\\alpha D^{2}}{2\\sigma^{2}}\\right)}$$"
        }
    ]
}