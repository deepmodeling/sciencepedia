## Introduction
In the field of remote sensing, supervised classification is a fundamental task for converting sensor data into meaningful thematic maps, such as land cover types. The core challenge lies in accurately modeling the statistical characteristics of each class from a limited set of training examples. While parametric classifiers assume data follows a specific mathematical distribution, this assumption often fails in the face of complex, real-world spectral data. This knowledge gap creates a need for methods that are robust and make fewer assumptions about the underlying [data structure](@entry_id:634264).

This article delves into two foundational **non-parametric classifiers**: the minimum distance and parallelepiped classifiers. Valued for their simplicity, [computational efficiency](@entry_id:270255), and intuitive geometric basis, these methods offer powerful alternatives when class distributions are unknown or non-Gaussian. By exploring their mechanisms, strengths, and weaknesses, you will gain a comprehensive understanding of how to apply them effectively and navigate their inherent trade-offs.

The following chapters will guide you through a complete learning journey. In **Principles and Mechanisms**, we will dissect the decision rules, geometric interpretations, and theoretical underpinnings of both classifiers, including their connection to Bayesian theory and their vulnerability to the curse of dimensionality. Next, **Applications and Interdisciplinary Connections** will ground these concepts in real-world remote sensing workflows, exploring the critical roles of atmospheric correction, [feature engineering](@entry_id:174925), spatial context, and validation. Finally, **Hands-On Practices** will provide practical exercises to solidify your understanding of key concepts like robustness and the impact of classifier assumptions.

## Principles and Mechanisms

In the domain of supervised classification, our objective is to assign a [feature vector](@entry_id:920515) $\mathbf{x} \in \mathbb{R}^{d}$ to one of $C$ predefined classes, $\omega_1, \dots, \omega_C$. Statistical decision theory provides a foundational framework for this task, suggesting that the optimal decision, under a simple zero-one loss function, is to assign $\mathbf{x}$ to the class with the highest [posterior probability](@entry_id:153467), $P(\omega_j \mid \mathbf{x})$. Through Bayes' theorem, this [posterior probability](@entry_id:153467) is proportional to the product of the class-[conditional probability density function](@entry_id:190422) (PDF), $p(\mathbf{x} \mid \omega_j)$, and the class [prior probability](@entry_id:275634), $\pi_j$. The central challenge in classification thus often reduces to estimating or modeling the class-conditional PDF, $p(\mathbf{x} \mid \omega_j)$, from a [finite set](@entry_id:152247) of labeled training samples.

Classifiers can be broadly categorized based on how they approach this challenge. **Parametric classifiers** assume that $p(\mathbf{x} \mid \omega_j)$ follows a specific mathematical form, such as a multivariate normal (Gaussian) distribution, defined by a fixed set of parameters (e.g., a [mean vector](@entry_id:266544) $\boldsymbol{\mu}_j$ and a covariance matrix $\boldsymbol{\Sigma}_j$). The training process involves estimating these parameters from the data. While statistically efficient if the underlying distributional assumptions are correct, [parametric models](@entry_id:170911) can perform poorly if the true data distribution deviates significantly from the assumed model.

In contrast, **non-parametric classifiers** avoid making explicit assumptions about the functional form of the class-conditional PDFs. Instead, they construct decision rules directly from the training samples, relying on [empirical distributions](@entry_id:274074) or geometric constructs. This chapter explores two of the most fundamental non-parametric classifiers in remote sensing: the [minimum distance classifier](@entry_id:1127934) and the parallelepiped classifier. These methods are valued for their simplicity, computational speed, and robustness, especially when training data is limited or class distributions are unknown or non-Gaussian  .

### The Minimum Distance Classifier

The **minimum-distance-to-mean classifier**, often simply called the [minimum distance classifier](@entry_id:1127934), represents one of the most intuitive approaches to classification. Each class $\omega_j$ is represented by a single prototype vector, which is almost always the [mean vector](@entry_id:266544) (or [centroid](@entry_id:265015)) $\boldsymbol{\mu}_j$ of its training samples. A new, unlabeled pixel with [feature vector](@entry_id:920515) $\mathbf{x}$ is assigned to the class whose centroid is closest in the feature space.

#### Decision Rule and Geometric Interpretation

Formally, the decision rule is to assign $\mathbf{x}$ to class $\omega_k$ if its distance to $\boldsymbol{\mu}_k$ is less than its distance to any other centroid $\boldsymbol{\mu}_j$:
$$ k = \arg\min_{j \in \{1,\dots,C\}} d(\mathbf{x}, \boldsymbol{\mu}_j) $$
While any distance metric can be used, the standard choice is the Euclidean distance. The squared Euclidean distance, $d^2(\mathbf{x}, \boldsymbol{\mu}_j) = \|\mathbf{x} - \boldsymbol{\mu}_j\|_2^2$, is often used for computational convenience, as it avoids the square root operation. Since squaring is a [monotonic function](@entry_id:140815) for non-negative distances, it yields the exact same classification result .

To illustrate, consider a three-band sensor and three classes (Water, Vegetation, Soil) with the following centroids estimated from training data:
$$ \boldsymbol{\mu}_{\mathrm{W}}=(0.05, 0.04, 0.08), \quad \boldsymbol{\mu}_{\mathrm{V}}=(0.04, 0.06, 0.42), \quad \boldsymbol{\mu}_{\mathrm{S}}=(0.10, 0.12, 0.20) $$
Given a test pixel $\mathbf{x}=(0.07, 0.09, 0.22)$, we compute the squared Euclidean distances:
$$ d^2(\mathbf{x}, \boldsymbol{\mu}_{\mathrm{W}}) = (0.07-0.05)^2 + (0.09-0.04)^2 + (0.22-0.08)^2 = 0.0225 $$
$$ d^2(\mathbf{x}, \boldsymbol{\mu}_{\mathrm{V}}) = (0.07-0.04)^2 + (0.09-0.06)^2 + (0.22-0.42)^2 = 0.0418 $$
$$ d^2(\mathbf{x}, \boldsymbol{\mu}_{\mathrm{S}}) = (0.07-0.10)^2 + (0.09-0.12)^2 + (0.22-0.20)^2 = 0.0022 $$
The smallest squared distance is to the Soil [centroid](@entry_id:265015), so the pixel $\mathbf{x}$ is assigned to the Soil class .

The geometry of the decision regions produced by this classifier is elegant and well-defined. The decision boundary between any two classes, $\omega_i$ and $\omega_j$, is the set of points equidistant from their respective centroids, $\boldsymbol{\mu}_i$ and $\boldsymbol{\mu}_j$. The equation for this boundary is:
$$ \|\mathbf{x} - \boldsymbol{\mu}_i\|_2^2 = \|\mathbf{x} - \boldsymbol{\mu}_j\|_2^2 $$
Expanding this equation and simplifying reveals a linear equation in $\mathbf{x}$, which defines a **[hyperplane](@entry_id:636937)**:
$$ \mathbf{x}^{\top}(\boldsymbol{\mu}_i-\boldsymbol{\mu}_j) = \frac{1}{2}\left(\|\boldsymbol{\mu}_i\|_2^2 - \|\boldsymbol{\mu}_j\|_2^2\right) $$
This [hyperplane](@entry_id:636937) is the [perpendicular bisector](@entry_id:176427) of the line segment connecting the two centroids.

The entire feature space is partitioned by these hyperplane boundaries into a set of convex decision regions. The region for each class is a **[convex polyhedron](@entry_id:170947)**, and the complete partition is known as a **Voronoi tessellation** (or Voronoi diagram) generated by the set of centroids. A key characteristic of this partition is that it is exhaustive; every point in the feature space is assigned to a class, leaving no unclassified gaps  .

#### Relationship to Bayesian Classification

Although applied as a simple geometric rule, the [minimum distance classifier](@entry_id:1127934) has a profound connection to parametric Bayes theory. It can be shown that the minimum Euclidean distance rule is mathematically equivalent to the optimal Bayes classifier under the specific and restrictive assumptions that:
1.  All class-conditional distributions $p(\mathbf{x} \mid \omega_j)$ are multivariate Gaussian.
2.  All classes have equal prior probabilities, $\pi_j = 1/C$.
3.  All classes share the same isotropic (spherical) covariance matrix, $\boldsymbol{\Sigma}_j = \sigma^2 \mathbf{I}$, where $\mathbf{I}$ is the identity matrix  .

This equivalence reveals the implicit assumption of the [minimum distance classifier](@entry_id:1127934): it is best suited for scenarios where class clusters are compact, hyperspherical, and of similar size . When these conditions are not met (e.g., if classes have different variances or are elongated due to correlation), the classifier becomes suboptimal.

#### Practical Considerations

A crucial practical consideration for the [minimum distance classifier](@entry_id:1127934) is its **sensitivity to [feature scaling](@entry_id:271716)**. The Euclidean distance calculation sums the squared differences along each feature axis. If one band has a much larger range of values or higher variance than others, it will dominate the distance calculation, effectively diminishing the contribution of the other bands. Therefore, a standard and essential preprocessing step is to **standardize** the features, for example, by transforming each band to have [zero mean](@entry_id:271600) and unit variance across the entire dataset before computing distances and classifying pixels .

### The Parallelepiped Classifier

The **parallelepiped classifier**, also known as the "box" classifier, is another simple non-[parametric method](@entry_id:137438). It defines the decision region for each class as an axis-aligned hyper-rectangle in the feature space.

#### Decision Rule and Construction

For each class $\omega_k$, and for each spectral band $j \in \{1, \dots, d\}$, the minimum and maximum reflectance values are determined from the set of training samples for that class. Let these bounds be $l_{kj}$ and $u_{kj}$.
$$ l_{kj} = \min_{i} x_j^{(i)}, \quad u_{kj} = \max_{i} x_j^{(i)} \quad \text{for training samples } \mathbf{x}^{(i)} \text{ in class } \omega_k $$
This procedure defines the smallest axis-aligned hyper-rectangle that encloses all the training data for class $\omega_k$. It is a purely non-parametric approach as it is derived directly from the empirical [order statistics](@entry_id:266649) of the data without assuming any underlying PDF. This choice is principled because, as the number of training samples increases, these empirical extrema converge to the true endpoints of the distribution's support .

The classification rule is then straightforward: a new pixel $\mathbf{x}$ is assigned to class $\omega_k$ if and only if its value in every band falls within the bounds defined for that class:
$$ \mathbf{x} \text{ is assigned to } \omega_k \iff l_{kj} \leq x_j \leq u_{kj} \text{ for all } j=1, \dots, d $$
The acceptance region for a single class, $\mathcal{B}_k = \{\mathbf{x} : l_{kj} \le x_j \le u_{kj} \forall j\}$, is formally the Cartesian product of closed intervals, $\mathcal{B}_k = \prod_{j=1}^{d}[l_{kj}, u_{kj}]$. As the intersection of half-spaces, each such hyper-rectangle is a **[convex set](@entry_id:268368)** .

#### Fundamental Limitations and Solutions

Despite its simplicity, the parallelepiped classifier suffers from two major, well-known weaknesses.

1.  **Gaps and Overlap:** The set of all parallelepipeds does not form an exhaustive partition of the feature space. It is common for a pixel's feature vector to fall outside of all defined boxes; such pixels are left **unclassified**, creating "gaps" in the classification map. Conversely, if the training data for different classes are not well-separated, their bounding boxes can **overlap**. A pixel falling into an overlapping region satisfies the criteria for multiple classes, leading to an **ambiguous** classification. Thus, the scheme is inherently neither exhaustive nor mutually exclusive without additional rules   .

2.  **Sensitivity to Correlation:** The classifier's boundaries are, by definition, aligned with the feature axes. This means it implicitly assumes that the spectral bands are uncorrelated. If there is strong correlation between bands, the data for a class will form an elongated, "tilted" cluster in the feature space. An axis-aligned box is a very poor geometric fit for such a cluster, leading to high rates of misclassification. It will either be too large, including many background pixels (commission error), or too small, excluding a large portion of the true class distribution (omission error) .

A powerful technique to mitigate the problem of correlation is to first apply a feature transformation that decorrelates the data, such as **Principal Component Analysis (PCA)**. PCA rotates the coordinate system so that the new axes (principal components) align with the directions of maximum variance in the data. In this new, rotated space, the data cluster becomes axis-aligned, and a parallelepiped classifier can now effectively bound the data. This pre-processing step creates a much "tighter" decision boundary. For instance, for data with correlation $\rho$, applying a parallelepiped classifier in the PCA-transformed space can reduce the area of the acceptance region by a factor of $\sqrt{1 - \rho^2}$ compared to the original space, thereby significantly reducing the probability of falsely accepting background pixels  .

#### Advanced Topic: Resolving Ambiguity via Risk Minimization

The problem of overlapping regions can be addressed in a principled manner using [statistical decision theory](@entry_id:174152). When a pixel $\mathbf{x}$ falls into the acceptance regions of multiple classes (the candidate set $\mathcal{S}(x)$), we can resolve the ambiguity by selecting the class that minimizes the **expected conditional risk** .

The risk of deciding class $m$ is $R(m \mid \mathbf{x}) = \sum_{n=1}^C C_{mn} P(\omega_n \mid \mathbf{x})$, where $C_{mn}$ are misclassification costs. To compute the posterior probabilities $P(\omega_n \mid \mathbf{x})$ in a manner consistent with the [non-parametric model](@entry_id:752596), we can assume a uniform probability density within each box: $f_n(\mathbf{x}) \propto 1/V_n$, where $V_n$ is the volume of the box for class $\omega_n$. This leads to a [posterior probability](@entry_id:153467) for any competing class $n \in \mathcal{S}(x)$ of:
$$ P(\omega_n \mid \mathbf{x}) = \frac{\pi_n/V_n}{\sum_{r \in \mathcal{S}(x)} \pi_r/V_r} $$
The ambiguity is resolved by choosing the class $m \in \mathcal{S}(x)$ that minimizes this risk. In the common case of symmetric [0-1 loss](@entry_id:173640), minimizing risk is equivalent to maximizing the [posterior probability](@entry_id:153467). This simplifies the decision rule to choosing the class that maximizes the quantity $\pi_m/V_m$, which can be interpreted as the "density of [prior probability](@entry_id:275634)." If this rule still results in a tie (e.g., if competing classes have equal priors and volumes), the minimum distance rule can be employed as a final, heuristic tie-breaker .

### Comparative Analysis and Practical Trade-offs

The choice between the minimum distance and parallelepiped classifiers, or more complex methods, depends on the nature of the data and the goals of the analysis. This choice can be understood through the lens of the **[bias-variance trade-off](@entry_id:141977)** .

-   **Parallelepiped and Minimum Distance classifiers** are simple models. Their rigid geometric assumptions (axis-aligned boxes or spherical clusters) introduce **high bias**â€”they may be a poor approximation of the true, complex class distributions. However, because they have very few parameters to estimate (extrema or a [mean vector](@entry_id:266544)), they exhibit **low variance**. Their estimates are stable and do not change drastically with small changes in the training data. This makes them robust to noise, [outliers](@entry_id:172866) (especially quantile-based parallelepipeds), and small [training set](@entry_id:636396) sizes.

-   **Parametric classifiers** like Gaussian Maximum Likelihood with full covariance matrices are complex models. They have **low bias** if the Gaussian assumption holds, as they can flexibly model elliptical, rotated clusters. However, they have **high variance**. Estimating a full $d \times d$ covariance matrix requires a large number of training samples ($n_k \gg d^2$). With limited data, these estimates are highly unstable, making the classifier unreliable and sensitive to outliers.

This trade-off means that while a simple rule like minimum distance may be asymptotically suboptimal compared to a correctly specified parametric model, its robustness in finite-sample, non-ideal conditions often makes it a pragmatic choice .

In practice :
-   The **parallelepiped classifier** is preferable for classes with signatures that are known to be constrained by sharp, physically-motivated, and uncorrelated bounds (e.g., the strong absorption of water in NIR/SWIR bands). It is also advantageous when a conservative classification is desired, allowing ambiguous or atypical pixels to remain unclassified.
-   The **[minimum distance classifier](@entry_id:1127934)** is preferable when class clusters are known to be spectrally compact and roughly isotropic (spherical), and when a complete labeling of the entire image domain (with no gaps) is a requirement.

### The Curse of Dimensionality

A final, critical consideration for these classifiers is their performance in high-dimensional spaces, a scenario common with modern hyperspectral sensors. The "curse of dimensionality" refers to various phenomena that arise when analyzing data in high dimensions, and it severely impacts both classifiers .

For the **parallelepiped classifier**, the issue is one of volume and probability. To be accepted, a pixel must satisfy the boundary conditions in *all* $d$ dimensions simultaneously. If the probability of satisfying the condition in a single band is $\alpha  1$, the probability of satisfying it in all $d$ independent bands is $\alpha^d$. This value decays exponentially to zero as $d$ increases. Consequently, in high dimensions, the acceptance region becomes an infinitesimally small fraction of the space, and the classifier will reject nearly every pixel, including those from the correct class (omission error approaches 100%).

For the **[minimum distance classifier](@entry_id:1127934)**, the problem manifests differently. As dimensionality $d$ increases, the Euclidean distances between any two points in the space tend to become more uniform. The ratio of the distance to the farthest point and the nearest point approaches 1. This means that the distinction between "near" and "far" centroids becomes less meaningful. More formally, when many non-informative (noise) bands are added, the variance of the distance comparison accumulates noise from all $d$ dimensions, while the "signal" (the mean difference) comes only from the few informative bands. The signal-to-noise ratio of the decision variable decays, typically as $1/\sqrt{d}$, and the classification error rate approaches that of random guessing (50% for two classes) .

In conclusion, while the parallelepiped and minimum distance classifiers are powerful and intuitive tools for low-dimensional multispectral data, their direct application to high-dimensional hyperspectral data is fraught with peril. Effective use in such contexts mandates a preceding step of feature selection or dimensionality reduction to identify and operate within a lower-dimensional subspace containing the relevant discriminatory information.