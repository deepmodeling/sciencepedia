{
    "hands_on_practices": [
        {
            "introduction": "This first exercise establishes a foundational understanding of the parallelepiped classifier and its behavior in high-dimensional feature spaces. By deriving the joint acceptance probability from individual band probabilities, you will confront a core challenge in remote sensing known as the \"curse of dimensionality.\" This practice illustrates how, even with high accuracy in each spectral band, overall classifier performance can degrade surprisingly quickly as more bands are added, a critical insight for designing effective classifiers for multispectral and hyperspectral imagery .",
            "id": "3830385",
            "problem": "A multispectral imaging instrument acquires $d$-dimensional feature vectors $\\mathbf{X} \\in \\mathbb{R}^{d}$ of surface reflectance after atmospheric correction. For a land cover class $C$, a parallelepiped classifier defines an axis-aligned acceptance region $A = \\prod_{j=1}^{d} I_{j}$, where each $I_{j} \\subset \\mathbb{R}$ is a closed interval along band $j$ estimated from training data. After a linear whitening transform based on Principal Component Analysis (PCA), assume that the transformed class-conditional features are standardized and statistically independent across bands. The empirical marginal coverage of $I_{j}$ for class $C$ is $p_{j} \\in (0,1)$, defined as the probability that a class-$C$ pixel’s band-$j$ value falls inside $I_{j}$.\n\nStarting only from the definition of joint probability on product spaces, the independence of transformed bands, and the definition of $p_{j}$ as a marginal coverage, derive the expected fraction of class-$C$ pixels accepted by the parallelepiped as a function of $\\{p_{j}\\}_{j=1}^{d}$. Use geometric measure to explain why increasing $d$ simultaneously inflates the absolute volume of $A$ and yet reduces the expected joint coverage under the class distribution. Briefly contrast this with the behavior of a minimum distance classifier (which assigns every pixel to the nearest class mean and does not include a reject option) in terms of coverage.\n\nThen, for a vegetation class in $d = 6$ whitened bands with empirically measured marginal coverages\n$$\np_{1} = 0.93,\\quad p_{2} = 0.89,\\quad p_{3} = 0.90,\\quad p_{4} = 0.88,\\quad p_{5} = 0.95,\\quad p_{6} = 0.87,\n$$\ncompute the expected acceptance probability of the parallelepiped classifier for class $C$. Round your final probability to $4$ significant figures and express it as a decimal.",
            "solution": "The problem asks for a derivation of the expected acceptance probability for a parallelepiped classifier under the assumption of statistically independent features, an explanation of the classifier's behavior in high dimensions, a brief comparison with a minimum distance classifier, and a specific numerical calculation.\n\nFirst, we derive the general expression for the expected fraction of class-$C$ pixels accepted by the classifier. Let $\\mathbf{X} = (X_{1}, X_{2}, \\dots, X_{d})$ be the $d$-dimensional random vector representing the features of a pixel belonging to class $C$. The parallelepiped classifier accepts this pixel if its feature vector falls within the acceptance region $A$. The region $A$ is defined as the Cartesian product of $d$ closed intervals, $A = \\prod_{j=1}^{d} I_{j}$, where $I_{j}$ is the acceptance interval for the $j$-th band.\n\nA pixel is accepted if and only if its feature value $X_{j}$ falls within the interval $I_{j}$ for all bands $j = 1, \\dots, d$. The expected fraction of accepted pixels is the probability of this joint event, which we denote as $P_{\\text{accept}}$.\n$$\nP_{\\text{accept}} = P(\\mathbf{X} \\in A) = P(X_{1} \\in I_{1} \\text{ and } X_{2} \\in I_{2} \\text{ and } \\dots \\text{ and } X_{d} \\in I_{d})\n$$\nThe problem statement specifies that after a whitening transformation, the class-conditional features are statistically independent across bands. The definition of statistical independence for a set of events is that the probability of their joint occurrence is the product of their individual probabilities. Therefore, we can write:\n$$\nP_{\\text{accept}} = P(X_{1} \\in I_{1}) \\times P(X_{2} \\in I_{2}) \\times \\dots \\times P(X_{d} \\in I_{d})\n$$\nThe problem defines the empirical marginal coverage $p_{j}$ as the probability that a class-$C$ pixel’s band-$j$ value falls inside the interval $I_{j}$. That is, $p_{j} = P(X_{j} \\in I_{j})$. Substituting this definition into the expression for $P_{\\text{accept}}$, we arrive at the desired result:\n$$\nP_{\\text{accept}} = \\prod_{j=1}^{d} p_{j}\n$$\nThis expression gives the expected acceptance probability for class $C$ as a function of the marginal coverages $\\{p_{j}\\}_{j=1}^{d}$.\n\nNext, we address the apparent paradox of how increasing the dimensionality $d$ can inflate the volume of the acceptance region $A$ while simultaneously reducing the expected joint coverage, $P_{\\text{accept}}$.\nThe geometric measure, or volume, of the axis-aligned parallelepiped $A$ is the product of the lengths of its defining intervals:\n$$\n\\text{Vol}(A) = \\prod_{j=1}^{d} \\text{Length}(I_{j})\n$$\nFor the features to be standardized, their distributions typically have a finite standard deviation, such as $\\sigma_{j}=1$. To achieve a high marginal coverage $p_{j}$ (e.g., $p_{j} = 0.95$), the interval $I_{j}$ must span a considerable range. For instance, if the whitened features follow a standard normal distribution, an interval capturing $p_{j}=0.95$ of the probability mass must have a length of approximately $3.92$. Since this length is greater than $1$, the volume $\\text{Vol}(A)$ will grow exponentially with increasing dimensionality $d$, as $(\\text{Length})^{d}$. This leads to a rapid inflation of the absolute volume of the acceptance region.\nConversely, the joint coverage, or acceptance probability, is $P_{\\text{accept}} = \\prod_{j=1}^{d} p_{j}$. Since each $p_{j}$ is a probability, it must be that $p_{j} \\in (0, 1)$. The product of multiple numbers less than $1$ results in a smaller number. If we assume for simplicity that all marginal coverages are equal to a constant $p < 1$, then $P_{\\text{accept}} = p^{d}$. This value decreases exponentially towards $0$ as $d$ increases. This phenomenon is a manifestation of the \"curse of dimensionality\": while the hyper-rectangular region $A$ encompasses a large volume in feature space, this volume is mostly \"empty\" corner space where the probability density is vanishingly small. The actual probability mass of the class distribution becomes increasingly concentrated in a region that is poorly approximated by a hyperrectangle, and the requirement for a sample to be \"typical\" in all dimensions simultaneously becomes exceedingly strict.\n\nNow, we briefly contrast the behavior of the parallelepiped classifier with a minimum distance classifier. The minimum distance classifier partitions the entire feature space $\\mathbb{R}^{d}$ into decision regions, one for each class. Specifically, every point $\\mathbf{x} \\in \\mathbb{R}^{d}$ is assigned to the class $C_{k}$ whose mean vector $\\boldsymbol{\\mu}_{k}$ is closest to $\\mathbf{x}$. This means there is no \"reject\" option; every pixel is classified. The total coverage over all classes is thus $100 \\%$. In contrast, the parallelepiped classifier defines explicit, bounded acceptance regions for each class. Any pixel falling outside all defined parallelepipeds is rejected or left unclassified. Therefore, its total coverage is typically well below $100 \\%$. While the minimum distance classifier is also affected by the curse of dimensionality (distances in high-dimensional spaces can become less meaningful), its behavior with respect to coverage is fundamentally different, as it enforces a complete tessellation of the feature space.\n\nFinally, we compute the expected acceptance probability for the given vegetation class. The dimensionality is $d=6$, and the marginal coverages are:\n$p_{1} = 0.93$\n$p_{2} = 0.89$\n$p_{3} = 0.90$\n$p_{4} = 0.88$\n$p_{5} = 0.95$\n$p_{6} = 0.87$\n\nUsing the derived formula, the expected acceptance probability is the product of these values:\n$$\nP_{\\text{accept}} = p_{1} \\times p_{2} \\times p_{3} \\times p_{4} \\times p_{5} \\times p_{6}\n$$\n$$\nP_{\\text{accept}} = 0.93 \\times 0.89 \\times 0.90 \\times 0.88 \\times 0.95 \\times 0.87\n$$\nCalculating this product gives:\n$$\nP_{\\text{accept}} \\approx 0.5418024876\n$$\nThe problem requires this value to be rounded to $4$ significant figures. The first four significant figures are $5$, $4$, $1$, and $8$. The fifth digit is $0$, so we round down.\n$$\nP_{\\text{accept}} \\approx 0.5418\n$$\nThis demonstrates that even with high marginal coverages in each band (all close to $0.90$), the probability of a pixel satisfying all criteria simultaneously drops to just over $0.54$.",
            "answer": "$$\n\\boxed{0.5418}\n$$"
        },
        {
            "introduction": "Moving from theoretical properties to practical implementation, this exercise examines how the choice of statistical estimators affects a classifier's performance, particularly when the training data is imperfect. You will analyze the robustness of different methods for constructing parallelepiped classifiers when confronted with outliers, a frequent issue in satellite imagery due to sensor saturation or other anomalies. This practice underscores the critical need to consider data quality and select robust statistical methods when training models from real-world samples .",
            "id": "3830449",
            "problem": "A remote sensing analyst is designing a supervised classifier for multispectral satellite imagery to delineate a land cover class $k$ (e.g., temperate broadleaf vegetation) in a feature space of $p$ spectral bands. For each class $k$, training data consist of $n$ pixels with spectral vectors $x^{(i)} \\in \\mathbb{R}^p$, $i = 1, \\dots, n$. The analyst considers non-parametric parallelepiped acceptance rules and a minimum distance rule defined as follows.\n\nDefine, for each band $j \\in \\{1, \\dots, p\\}$, the sample minimum $a_{kj} = \\min_{1 \\le i \\le n} x^{(i)}_j$, sample maximum $b_{kj} = \\max_{1 \\le i \\le n} x^{(i)}_j$, sample mean $m_{kj} = \\frac{1}{n}\\sum_{i=1}^n x^{(i)}_j$, and sample standard deviation $s_{kj} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n \\left(x^{(i)}_j - m_{kj}\\right)^2}$. Consider two parallelepiped acceptance regions for class $k$:\n- Min–max parallelepiped: $x \\in \\mathcal{P}_k^{\\mathrm{minmax}}$ if and only if $a_{kj} \\le x_j \\le b_{kj}$ for all $j$.\n- Mean–standard deviation parallelepiped: for a chosen scalar $\\alpha > 0$, $x \\in \\mathcal{P}_k^{\\alpha}$ if and only if $m_{kj} - \\alpha s_{kj} \\le x_j \\le m_{kj} + \\alpha s_{kj}$ for all $j$.\n\nAdditionally, consider the minimum distance classifier that assigns $x$ to the class $k$ minimizing the Euclidean distance $d_k(x) = \\lVert x - \\mu_k \\rVert_2$, where $\\mu_k \\in \\mathbb{R}^p$ denotes a representative class center taken to be the sample mean vector $\\mu_k = (m_{k1}, \\dots, m_{kp})^\\top$.\n\nSuppose for a specific class $k$ and a particular band $j^\\star$, the class-conditional distribution of clean (non-contaminated) training and test pixels is well approximated by an independent Gaussian model across bands, i.e., $X_j \\sim \\mathcal{N}(\\mu_{kj}, \\sigma_{kj}^2)$ independently for $j = 1, \\dots, p$. Now consider contamination of the training set by a single saturated pixel in band $j^\\star$ with value $M \\gg \\mu_{kj^\\star} + 10\\sigma_{kj^\\star}$, leaving all other bands and samples unchanged. The contaminated training set thus has one extreme outlier on band $j^\\star$ and no outliers on the other bands. Assume test pixels remain clean and follow the stated Gaussian model.\n\nBased on the fundamental definitions above and well-tested properties of Gaussian models and sample statistics, which of the following statements are correct?\n\nA. With even a single extreme outlier on band $j^\\star$, the min–max bounds on $j^\\star$ expand to include that outlier exactly, potentially inflating $\\mathcal{P}_k^{\\mathrm{minmax}}$ arbitrarily along $j^\\star$. In contrast, the mean–standard deviation bounds inflate through the influence of the outlier on $m_{kj^\\star}$ and $s_{kj^\\star}$, which scales with its leverage on these sample statistics and the training size $n$; hence, mean–standard deviation parallelepipeds are generally more robust to single extreme outliers than min–max parallelepipeds.\n\nB. Under the Gaussian class-conditional model and independence across bands, choosing $\\alpha$ to target nominal coverage (e.g., $95\\%$ per band) guarantees that the true acceptance probability for clean test pixels under $m_{kj} \\pm \\alpha s_{kj}$ is exactly $95\\%$ per band regardless of training sample size $n$ and regardless of outlier contamination, whereas min–max parallelepipeds always have acceptance probability at most $95\\%$.\n\nC. Minimum distance classification is unaffected by outliers because it uses only class means and Euclidean distances; therefore it is strictly more robust than either min–max or mean–standard deviation parallelepipeds.\n\nD. For heavy-tailed or skewed spectral distributions, replacing $m_{kj} \\pm \\alpha s_{kj}$ with a robust center and scale, such as the component-wise median and a scaled Median Absolute Deviation (MAD), yields a parallelepiped-like acceptance region that is less sensitive to outliers, at the cost of reduced efficiency under Gaussian assumptions.\n\nE. For the $m_{kj} \\pm \\alpha s_{kj}$ rule, increasing $\\alpha$ monotonically increases the acceptance probability for clean class-$k$ pixels and also increases the chance of including pixels from other classes; for min–max parallelepipeds, acceptance is determined by the training extremes and lacks a continuous tuning parameter analogous to $\\alpha$, so the bias–variance trade-off in acceptance cannot be smoothly adjusted without modifying the training set or adopting additional heuristics.",
            "solution": "This problem tests the understanding of how different statistical estimators used in classifier construction respond to outliers. We analyze each statement based on the defined scenario of a single extreme outlier in one band of the training data.\n\n*   **A is correct.** The min-max parallelepiped's boundaries are defined by the absolute minimum and maximum values in the training data. A single extreme outlier $M$ will directly set one of the boundaries (e.g., $b_{kj^\\star} = M$), causing the acceptance region to expand dramatically along that axis. This makes it extremely sensitive to outliers. The mean-standard deviation rule uses $m_{kj}$ and $s_{kj}$. While both are sensitive to outliers, their values are averaged over the entire sample of size $n$. The outlier's influence is thus tempered by a factor related to $1/n$ for the mean and $1/\\sqrt{n}$ for the standard deviation. Therefore, for a reasonably large training set, the inflation of the mean-std-dev bounds is less extreme than for the min-max bounds, making the former comparatively more robust.\n\n*   **B is incorrect.** The interval $m_{kj} \\pm \\alpha s_{kj}$ is a random interval based on a sample. The probability that it covers a new observation from the same distribution (a prediction interval) is not constant and depends on the sample size $n$. More importantly, the presence of an outlier contaminates the estimates $m_{kj^\\star}$ and $s_{kj^\\star}$, invalidating the statistical assumptions required for any nominal coverage guarantee on clean test data. The claim about min-max parallelepipeds having at most 95% coverage is also baseless; by construction, they cover 100% of the training data.\n\n*   **C is incorrect.** The premise that the minimum distance classifier is unaffected by outliers is false. The classifier relies on the sample mean vector $\\mu_k = (m_{k1}, \\dots, m_{kp})^\\top$ to define the class center. The sample mean is not a robust statistic and is strongly influenced by outliers. The single outlier $M$ will pull the centroid for class $k$ in its direction, thereby shifting all decision boundaries associated with that class.\n\n*   **D is correct.** This statement correctly describes a fundamental principle of robust statistics. For distributions with heavy tails or outliers, robust estimators of location (like the median) and scale (like the Median Absolute Deviation, or MAD) are preferable to the sample mean and standard deviation. Replacing the non-robust estimators with these robust counterparts creates a classifier that is significantly less sensitive to outliers. The trade-off is that these robust estimators are less statistically efficient (i.e., have higher variance) than the mean and standard deviation when the data is perfectly Gaussian and free of contamination.\n\n*   **E is correct.** The parameter $\\alpha$ in the mean-standard deviation rule acts as a direct, continuous tuning parameter. Increasing $\\alpha$ expands the acceptance box, increasing both the true positive rate (sensitivity) and the false positive rate. This allows for a smooth adjustment of the classifier's bias-variance trade-off. The min-max parallelepiped lacks such a parameter; its boundaries are rigidly determined by the training data extremes. Adjusting its behavior requires modifying the training set (e.g., by trimming outliers) or using other heuristics, not simply tuning a parameter.\n\nTherefore, the correct statements are A, D, and E.",
            "answer": "$$\n\\boxed{ADE}\n$$"
        },
        {
            "introduction": "Beyond theoretical accuracy and statistical robustness, the utility of a classification algorithm in remote sensing hinges on its computational efficiency when applied to massive datasets. This final practice focuses on performance analysis, asking you to derive the computational complexity of the parallelepiped and minimum-distance-to-mean rules. By evaluating how these algorithms scale with data size and dimensionality, and considering modern optimization strategies, you will develop an essential skill for designing workflows that are both accurate and scalable for large-area analysis .",
            "id": "3830387",
            "problem": "A remote sensing analyst must classify a large scene of $N$ pixels, each pixel represented by a $d$-dimensional spectral vector $\\mathbf{x} \\in \\mathbb{R}^{d}$, into one of $K$ land-cover classes using two non-parametric decision rules commonly used in environmental modeling. The parallelepiped rule assumes that class $k \\in \\{1,\\dots,K\\}$ is defined by per-band lower and upper thresholds $\\{L_{k,i},U_{k,i}\\}_{i=1}^{d}$, and declares that $\\mathbf{x}$ is consistent with class $k$ if and only if $L_{k,i} \\le x_i \\le U_{k,i}$ for all $i \\in \\{1,\\dots,d\\}$. The minimum-distance-to-means rule assumes known class means $\\{\\boldsymbol{\\mu}_k\\}_{k=1}^{K}$ in $\\mathbb{R}^{d}$ and assigns $\\mathbf{x}$ to the class minimizing the Euclidean distance $\\|\\mathbf{x} - \\boldsymbol{\\mu}_k\\|_{2}$. Assume a standard unit-cost model in which primitive floating-point comparisons, additions, and multiplications each cost constant time, and use the conventional Big-O notation $\\mathcal{O}(\\cdot)$ for asymptotic costs.\n\nFrom first principles, derive the per-pixel runtime scaling as a function of $K$ and $d$ for each rule by counting primitive operations implied by their definitions. Then consider implementation at scene scale ($N$ large) and reason about methods that change constants versus asymptotic complexity. Based on your derivations, which of the following statements about computational complexity and scalable implementation are correct? Select all that apply.\n\nA. For a single pixel, the worst-case cost of testing the parallelepiped rule against all $K$ classes scales as $\\mathcal{O}(K d)$ primitive comparisons; because each band requires testing $x_i$ against both $L_{k,i}$ and $U_{k,i}$, there are up to $2d$ comparisons per class, and short-circuit rejection can reduce the average cost when bounds are tight.\n\nB. For a single pixel under minimum-distance-to-means, if one evaluates squared Euclidean distances and pre-stores $\\|\\boldsymbol{\\mu}_k\\|_2^2$ for all $k$, the dominant computation reduces to $K$ dot products $\\mathbf{x}^{\\top}\\boldsymbol{\\mu}_k$, giving $\\mathcal{O}(K d)$ per-pixel arithmetic, and the common term $\\|\\mathbf{x}\\|_2^2$ can be dropped without changing the minimizing class.\n\nC. Building a spatial index such as a $k$-$d$ tree over the $K$ class means guarantees sublinear-in-$K$ worst-case per-pixel time for exact nearest-mean classification, irrespective of $d$.\n\nD. For a scene with $N$ pixels, arranging pixels into tiles of $B$ pixels and computing all class scores for a tile as a dense matrix-matrix multiplication between a $B \\times d$ tile and a $d \\times K$ mean matrix can substantially reduce wall-clock time on modern Graphics Processing Units (GPU) due to higher arithmetic intensity, while the asymptotic arithmetic count remains $\\mathcal{O}(N K d)$.\n\nE. If the class bounds $\\{L_{k,i},U_{k,i}\\}$ are pre-sorted per band, the worst-case per-pixel complexity of the parallelepiped rule becomes exactly $\\mathcal{O}(d \\log K)$ because one can locate the containing interval by binary search in each band and intersect the results.\n\nF. Any orthogonal linear transform that reduces dimensionality from $d$ to $d' < d$, such as Principal Component Analysis (PCA), preserves all pairwise Euclidean distances exactly; therefore, minimum-distance classification is unchanged while per-pixel cost drops to $\\mathcal{O}(K d')$.\n\nG. A branchless implementation of the parallelepiped test that accumulates a boolean mask via vectorized comparisons across bands and classes using Single Instruction Multiple Data (SIMD) instructions can improve throughput on modern processors without changing the asymptotic $\\mathcal{O}(K d)$ per-pixel complexity.\n\nH. In minimum-distance-to-means, computing the square root is necessary to avoid bias in the decision; omitting the square root and comparing squared distances changes the identity of the minimizing class, so the per-class cost must include a square root in addition to the $\\mathcal{O}(d)$ multiply-adds.",
            "solution": "We begin from the definitions of the two rules and a unit-cost model for primitive operations.\n\nParallelepiped rule: For class $k$, the membership test demands $L_{k,i} \\le x_i \\le U_{k,i}$ for all $i \\in \\{1,\\dots,d\\}$. For a fixed $k$, this requires, in the absence of early termination, $2$ comparisons per band ($x_i \\ge L_{k,i}$ and $x_i \\le U_{k,i}$), for a total of $2d$ comparisons. To decide the pixel’s class, one must evaluate this condition for all $K$ classes if no class is accepted early or if the implementation insists on checking all classes to detect ambiguities. Therefore, the worst-case per-pixel comparison count scales as $2 K d$, which is $\\mathcal{O}(K d)$. In practice, short-circuit evaluation can terminate evaluation within a class after the first failed band and can stop scanning classes once an acceptance policy is satisfied, potentially reducing the average number of comparisons, but it does not alter the worst-case scaling.\n\nMinimum-distance-to-means rule: For class $k$, the Euclidean distance is $\\|\\mathbf{x} - \\boldsymbol{\\mu}_k\\|_2 = \\sqrt{\\sum_{i=1}^{d} (x_i - \\mu_{k,i})^2}$. Computing this naively involves $d$ subtractions, $d$ squarings (multiplications), $d-1$ additions, and one square root per class. However, since the square root is a strictly increasing function on $[0,\\infty)$, the minimizer of $\\|\\mathbf{x} - \\boldsymbol{\\mu}_k\\|_2$ over $k$ is identical to the minimizer of the squared distance $\\|\\mathbf{x} - \\boldsymbol{\\mu}_k\\|_2^2 = \\sum_{i=1}^{d} (x_i - \\mu_{k,i})^2$. Expanding yields\n$$\n\\|\\mathbf{x} - \\boldsymbol{\\mu}_k\\|_2^2 \\;=\\; \\mathbf{x}^{\\top}\\mathbf{x} - 2\\,\\boldsymbol{\\mu}_k^{\\top}\\mathbf{x} + \\boldsymbol{\\mu}_k^{\\top}\\boldsymbol{\\mu}_k.\n$$\nThe term $\\mathbf{x}^{\\top}\\mathbf{x}$ is independent of $k$ and thus does not affect the argmin over $k$. If we precompute and store $\\boldsymbol{\\mu}_k^{\\top}\\boldsymbol{\\mu}_k$ for all $k$, the per-class computation reduces to evaluating the dot product $\\boldsymbol{\\mu}_k^{\\top}\\mathbf{x}$ (cost $\\mathcal{O}(d)$) and a constant number of scalar operations. Therefore, per pixel, the dominant arithmetic is $K$ dot products, for a total of $\\mathcal{O}(K d)$.\n\nScaling to a scene with $N$ pixels: A straightforward per-pixel loop yields $\\mathcal{O}(N K d)$ primitive operations for the minimum-distance rule and $\\mathcal{O}(N K d)$ comparisons for the parallelepiped rule in the worst case. Implementation strategies that improve memory locality and vectorization can reduce constants and improve wall-clock time while leaving the asymptotic arithmetic count unchanged. For example, batching pixels into tiles of size $B$ and forming a dense matrix-matrix product between a $B \\times d$ tile and the $d \\times K$ matrix of class means computes all $B \\cdot K$ dot products with high arithmetic intensity and efficient cache reuse, which maps well to Graphics Processing Unit (GPU) architectures, yet still performs $\\Theta(B K d)$ multiply-adds per tile, preserving the overall $\\mathcal{O}(N K d)$ complexity.\n\nWe now analyze each option:\n\nA. The argument above yields $2d$ comparisons per class in the absence of early exit, and up to $K$ classes, for a worst-case of $2 K d$ comparisons, i.e., $\\mathcal{O}(K d)$. Short-circuit rejection is a valid constant-factor optimization affecting the average case. Verdict — Correct.\n\nB. By expanding the squared Euclidean distances, dropping the constant $\\mathbf{x}^{\\top}\\mathbf{x}$, and precomputing $\\|\\boldsymbol{\\mu}_k\\|_2^2$, we reduce the per-class work to computing $\\boldsymbol{\\mu}_k^{\\top}\\mathbf{x}$ and a few scalar operations, which is $\\mathcal{O}(d)$ per class and $\\mathcal{O}(K d)$ per pixel. The identity of the minimizer is unchanged when omitting the square root because the square root is monotone. Verdict — Correct.\n\nC. Space-partitioning data structures such as $k$-$d$ trees can provide average-case sublinear behavior for nearest-neighbor queries in low dimensions, but they do not guarantee sublinear worst-case time for exact search; in fact, worst-case queries degrade to visiting $\\Theta(K)$ nodes, especially as $d$ grows. Therefore, there is no worst-case sublinear guarantee “irrespective of $d$.” Verdict — Incorrect.\n\nD. Casting the $B$-pixel block computation as a dense matrix-matrix multiply between a $B \\times d$ tile and a $d \\times K$ matrix of means performs exactly $2 B K d$ floating-point operations (counting multiply-adds) and leverages high-performance kernels on Graphics Processing Units (GPU), often reducing wall-clock time substantially. However, the arithmetic count remains proportional to $N K d$. Verdict — Correct.\n\nE. Pre-sorting intervals per band allows one to identify candidate classes containing $x_i$ in band $i$ via binary search in $\\mathcal{O}(\\log K)$ time per band, but the overall decision requires intersecting the candidate sets across $d$ bands. In the worst case, many or all classes can remain candidates after each band (e.g., if most intervals are wide), forcing $\\Theta(K d)$ checks. Thus, an $\\mathcal{O}(d \\log K)$ worst-case bound is not guaranteed. Verdict — Incorrect.\n\nF. An orthogonal transform that reduces dimensionality to $d' < d$ cannot preserve all Euclidean distances exactly unless all vectors of interest lie within the $d'$-dimensional subspace. Principal Component Analysis (PCA) preserves distances only approximately when truncated. Therefore, minimum-distance decisions can change under dimensionality reduction, even though the per-pixel cost would drop to $\\mathcal{O}(K d')$. Verdict — Incorrect.\n\nG. Implementing the parallelepiped test in a branchless, vectorized fashion—e.g., evaluating comparisons for multiple bands and classes in parallel using Single Instruction Multiple Data (SIMD) and combining results via bitwise operations—can improve throughput by reducing branch mispredictions and increasing data-level parallelism. This optimization does not alter the asymptotic $\\mathcal{O}(K d)$ comparison count. Verdict — Correct.\n\nH. Because the square root is strictly increasing on nonnegative reals, the minimizer of $\\|\\mathbf{x} - \\boldsymbol{\\mu}_k\\|_2$ equals the minimizer of $\\|\\mathbf{x} - \\boldsymbol{\\mu}_k\\|_2^2$. Thus, computing square roots is unnecessary for classification decisions and would only add overhead. Verdict — Incorrect.\n\nThe correct choices are A, B, D, and G.",
            "answer": "$$\\boxed{ABDG}$$"
        }
    ]
}