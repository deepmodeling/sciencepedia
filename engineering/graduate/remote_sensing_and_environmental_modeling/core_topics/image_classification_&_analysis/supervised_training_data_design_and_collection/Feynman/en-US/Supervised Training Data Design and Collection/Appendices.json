{
    "hands_on_practices": [
        {
            "introduction": "The familiar $N$ in statistical formulas often implicitly assumes that each data point provides a full, independent piece of information. However, in remote sensing and environmental science, data is rarely independent due to spatial autocorrelation. This exercise challenges you to move beyond naively counting samples by deriving the *effective sample size* ($n_{\\mathrm{eff}}$), a foundational concept in geostatistics that quantifies the true information content of a spatially correlated dataset. By working through this problem , you will gain a deeper intuition for how spatial structure impacts data value and learn to assess the statistical power of your training data more rigorously.",
            "id": "3856352",
            "problem": "You are designing a supervised training dataset for a remote sensing regression task over a two-dimensional domain with area $A$ (for example, an image mosaic covering a large region). Let $Z(\\mathbf{s})$ denote a second-order stationary and isotropic random field representing the target variable (for example, soil moisture) at spatial location $\\mathbf{s} \\in \\mathbb{R}^{2}$, with mean $\\mu$ and covariance function $C(h)=\\sigma^{2}\\exp(-h/\\phi)$, where $h$ is the separation distance, $\\sigma^{2}$ is the marginal variance, and $\\phi$ is the correlation length. You collect labels at a very high and approximately uniform spatial density (for example, all pixels at a resolution much finer than $\\phi$), and you will use them as if they were independent when training.\n\nDefine the effective sample size $n_{\\mathrm{eff}}$ of your training set as the size of an independent and identically distributed sample from a random variable with variance $\\sigma^{2}$ that yields the same variance for the spatial areal average as your actual, spatially correlated training data. Starting from fundamental definitions for second-order stationary random fields and the variance of an areal average, and assuming $A \\gg \\phi^{2}$ so that edge effects are negligible, derive a closed-form analytic expression for $n_{\\mathrm{eff}}$ in terms of $A$ and $\\phi$. Conclude from your expression how $n_{\\mathrm{eff}}$ scales with the domain size and the correlation length. Provide only the final analytic expression for $n_{\\mathrm{eff}}$ as your answer. No numerical substitution is required, and no rounding is needed.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of geostatistics and random field theory, well-posed with sufficient information and a clear objective, and expressed in precise, objective language.\n\nThe objective is to derive an expression for the effective sample size, $n_{\\mathrm{eff}}$, for a spatially correlated dataset over a two-dimensional domain of area $A$. The random field $Z(\\mathbf{s})$ representing the variable of interest is second-order stationary and isotropic, with a mean $\\mu$ and an exponential covariance function $C(h) = \\sigma^{2}\\exp(-h/\\phi)$, where $h$ is the Euclidean distance, $\\sigma^2$ is the variance, and $\\phi$ is the correlation length.\n\nThe effective sample size $n_{\\mathrm{eff}}$ is defined as the size of a hypothetical independent and identically distributed (i.i.d.) sample that would yield the same variance for its sample mean as the variance of the spatial areal average of the correlated field $Z(\\mathbf{s})$.\n\nFirst, consider a set of $n$ i.i.d. random variables, $\\{X_1, X_2, \\ldots, X_n\\}$, each with variance $\\sigma^2$. The variance of their sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$, is given by:\n$$ \\mathrm{Var}(\\bar{X}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(X_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n} $$\nBy the definition provided in the problem, for an effective sample size of $n_{\\mathrm{eff}}$, this variance is $\\frac{\\sigma^2}{n_{\\mathrm{eff}}}$.\n\nNext, we must calculate the variance of the spatial areal average of the random field $Z(\\mathbf{s})$ over the domain, which we denote by $D$, with area $A = \\int_D d\\mathbf{s}$. The spatial areal average, $\\bar{Z}_A$, is defined as:\n$$ \\bar{Z}_A = \\frac{1}{A} \\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s} $$\nThe variance of $\\bar{Z}_A$ is:\n$$ \\mathrm{Var}(\\bar{Z}_A) = \\mathrm{Var}\\left(\\frac{1}{A} \\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) = \\frac{1}{A^2} \\mathrm{Var}\\left(\\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) $$\nFor a second-order stationary random field, the variance of the integral is given by the double integral of the covariance function over the domain:\n$$ \\mathrm{Var}\\left(\\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) = \\int_{D} \\int_{D} \\mathrm{Cov}(Z(\\mathbf{s}_1), Z(\\mathbf{s}_2)) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 $$\nThe covariance is given by $C(\\|\\mathbf{s}_1 - \\mathbf{s}_2\\|)$. Thus,\n$$ \\mathrm{Var}(\\bar{Z}_A) = \\frac{1}{A^2} \\int_{D} \\int_{D} C(\\|\\mathbf{s}_1 - \\mathbf{s}_2\\|) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 $$\nEvaluating this four-dimensional integral is generally complex. However, the problem states the assumption that the area of the domain $A$ is much larger than the square of the correlation length, $A \\gg \\phi^2$. This implies that the spatial extent of the domain is much larger than the range over which data points are significantly correlated. This allows us to neglect edge effects.\n\nWe can simplify the integral by a change of variables. Let $\\mathbf{h} = \\mathbf{s}_2 - \\mathbf{s}_1$. The integral becomes:\n$$ \\int_{D} \\int_{D} C(\\|\\mathbf{s}_2 - \\mathbf{s}_1\\|) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 = \\int_{D} \\left( \\int_{D-\\mathbf{s}_1} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\right) d\\mathbf{s}_1 $$\nwhere $D-\\mathbf{s}_1$ is the domain $D$ shifted by the vector $-\\mathbf{s}_1$. For any point $\\mathbf{s}_1$ that is not close to the boundary of $D$ (relative to $\\phi$), the covariance function $C(\\|\\mathbf{h}\\|)$ will decay to approximately zero well within the integration domain $D-\\mathbf{s}_1$. Given the assumption $A \\gg \\phi^2$, the contribution from points $\\mathbf{s}_1$ near the boundary is negligible. We can therefore approximate the inner integral by extending its domain to all of $\\mathbb{R}^2$:\n$$ \\int_{D-\\mathbf{s}_1} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\approx \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\nThis approximation makes the inner integral independent of $\\mathbf{s}_1$. The outer integral then becomes:\n$$ \\int_{D} \\left(\\ldots\\right) d\\mathbf{s}_1 \\approx A \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\nSubstituting this back into the expression for the variance of the areal average:\n$$ \\mathrm{Var}(\\bar{Z}_A) \\approx \\frac{1}{A^2} \\left( A \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\right) = \\frac{1}{A} \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\nNow we must evaluate the integral of the given covariance function, $C(h) = \\sigma^2 \\exp(-h/\\phi)$, over $\\mathbb{R}^2$. Since the function is isotropic, we convert to polar coordinates $(h, \\theta)$, where $h$ is the radial distance and $d\\mathbf{h} = h \\, dh \\, d\\theta$.\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = \\int_{0}^{2\\pi} \\int_{0}^{\\infty} \\sigma^2 \\exp(-h/\\phi) \\, h \\, dh \\, d\\theta $$\nThe integral over $\\theta$ from $0$ to $2\\pi$ yields a factor of $2\\pi$:\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = 2\\pi\\sigma^2 \\int_{0}^{\\infty} h \\exp(-h/\\phi) \\, dh $$\nWe solve the remaining integral using integration by parts, with $u=h$ and $dv = \\exp(-h/\\phi) \\, dh$. This gives $du=dh$ and $v = -\\phi \\exp(-h/\\phi)$.\n\\begin{align*}\n\\int_{0}^{\\infty} h \\exp(-h/\\phi) \\, dh &= \\left[ -h\\phi\\exp(-h/\\phi) \\right]_0^{\\infty} - \\int_{0}^{\\infty} (-\\phi) \\exp(-h/\\phi) \\, dh \\\\\n&= (0 - 0) + \\phi \\int_{0}^{\\infty} \\exp(-h/\\phi) \\, dh \\\\\n&= \\phi \\left[ -\\phi\\exp(-h/\\phi) \\right]_0^{\\infty} \\\\\n&= \\phi (0 - (-\\phi \\exp(0))) \\\\\n&= \\phi^2\n\\end{align*}\nTherefore, the integral of the covariance function is:\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = 2\\pi\\sigma^2\\phi^2 $$\nSubstituting this result into our expression for $\\mathrm{Var}(\\bar{Z}_A)$:\n$$ \\mathrm{Var}(\\bar{Z}_A) \\approx \\frac{2\\pi\\sigma^2\\phi^2}{A} $$\nFinally, we equate this with the variance for an i.i.d. sample of effective size $n_{\\mathrm{eff}}$:\n$$ \\frac{\\sigma^2}{n_{\\mathrm{eff}}} = \\frac{2\\pi\\sigma^2\\phi^2}{A} $$\nAssuming $\\sigma^2 \\neq 0$, we can cancel $\\sigma^2$ from both sides:\n$$ \\frac{1}{n_{\\mathrm{eff}}} = \\frac{2\\pi\\phi^2}{A} $$\nSolving for $n_{\\mathrm{eff}}$ gives the closed-form expression:\n$$ n_{\\mathrm{eff}} = \\frac{A}{2\\pi\\phi^2} $$\nFrom this expression, we conclude that the effective sample size $n_{\\mathrm{eff}}$ scales linearly with the domain area $A$ ($n_{\\mathrm{eff}} \\propto A$) and inversely with the square of the correlation length $\\phi$ ($n_{\\mathrm{eff}} \\propto \\phi^{-2}$). The term $2\\pi\\phi^2$ can be interpreted as the effective area of a single independent sample.",
            "answer": "$$\\boxed{\\frac{A}{2\\pi\\phi^{2}}}$$"
        },
        {
            "introduction": "Perfect data is a rarity in the real world, and a critical skill for a scientist is to understand and model the sources of error. In remote sensing, associating field observations with image pixels is fundamental, but geolocation systems have inherent inaccuracies. This practice  guides you through a formal analysis of how geolocation error propagates to create label noise, particularly near the sensitive boundaries between classes. By deriving the expected confusion matrix, you will develop a quantitative framework for assessing the reliability of training labels and for making informed decisions about data collection protocols in complex landscapes.",
            "id": "3856402",
            "problem": "A remote sensing team is designing a supervised training dataset for a two-class land-cover map (classes $A$ and $B$) in a region where the class boundary is locally smooth. Training labels are assigned by overlaying each field observation’s geolocation on the map; however, the geolocation has error due to Global Navigation Satellite System (GNSS) drift, modeled as a bivariate Gaussian random vector $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$ with covariance matrix $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$. Locally, approximate the class boundary by a straight line with unit normal vector $\\mathbf{n} \\in \\mathbb{R}^{2}$. A field observation with true position $\\mathbf{r}$ has signed normal distance $d = \\mathbf{n}^{\\mathsf{T}}\\mathbf{r}$ from the boundary; by convention, $d \\geq 0$ denotes class $A$ and $d < 0$ denotes class $B$. A training label is assigned by the sign of the observed position’s signed distance $d_{\\text{obs}} = \\mathbf{n}^{\\mathsf{T}}(\\mathbf{r} + \\boldsymbol{\\epsilon})$.\n\nAssume the team intentionally samples points near the boundary to characterize confusion: conditional on the true class, the signed normal distance $d$ is uniformly distributed within a strip of width $2D$ centered on the boundary, i.e., $d \\sim \\text{Uniform}([0, D])$ for class $A$ and $d \\sim \\text{Uniform}([-D, 0])$ for class $B$, with equal sampling effort for the two classes. Under this sampling scheme and error model, define the expected $2 \\times 2$ confusion matrix $C(\\Sigma, D, \\mathbf{n})$ whose entry $C_{ij}$ is the probability that a true class $i \\in \\{A,B\\}$ observation is assigned the label $j \\in \\{A,B\\}$ after geolocation error.\n\nStarting only from the fundamental definitions of the multivariate normal distribution, the property that linear functionals of Gaussian random vectors are Gaussian, and the definition of the confusion matrix as class-conditional assignment probabilities averaged over the sampling distribution of $d$, derive a closed-form analytic expression for $C(\\Sigma, D, \\mathbf{n})$. You may express your answer using the standard normal cumulative distribution function $\\Phi(\\cdot)$ and the standard normal probability density function $\\phi(\\cdot)$, and the scalar $\\sigma_{\\mathbf{n}}^{2} = \\mathbf{n}^{\\mathsf{T}}\\Sigma \\mathbf{n}$.\n\nProvide your final result as the complete $2 \\times 2$ matrix $C(\\Sigma, D, \\mathbf{n})$. No numerical evaluation is required; leave the expression in closed form.",
            "solution": "The user has requested a detailed derivation of an expected confusion matrix under a specific sampling and error model.\n\n_Step 1: Problem Validation_\n\n**Extracted Givens:**\n*   **Classes:** Two classes, denoted $A$ and $B$.\n*   **Class Boundary:** A local linear approximation with unit normal vector $\\mathbf{n} \\in \\mathbb{R}^{2}$.\n*   **True Position and Distance:** A point has a true position $\\mathbf{r}$ and a signed normal distance from the boundary $d = \\mathbf{n}^{\\mathsf{T}}\\mathbf{r}$.\n*   **True Class Definition:** Class $A$ if $d \\geq 0$, Class $B$ if $d < 0$.\n*   **Geolocation Error Model:** The error is a random vector $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$, where $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ is the covariance matrix.\n*   **Observed Position and Distance:** The observed position is $\\mathbf{r} + \\boldsymbol{\\epsilon}$, and the observed signed distance is $d_{\\text{obs}} = \\mathbf{n}^{\\mathsf{T}}(\\mathbf{r} + \\boldsymbol{\\epsilon})$.\n*   **Label Assignment Rule:** Label $A$ if $d_{\\text{obs}} \\geq 0$, Label $B$ if $d_{\\text{obs}} < 0$.\n*   **Sampling Distribution of True Distance ($d$):**\n    *   For true class $A$: $d \\sim \\text{Uniform}([0, D])$.\n    *   For true class $B$: $d \\sim \\text{Uniform}([-D, 0])$.\n*   **Target Output:** A closed-form expression for the $2 \\times 2$ expected confusion matrix $C(\\Sigma, D, \\mathbf{n})$, where $C_{ij}$ is the probability of assigning label $j$ to a true class $i$ observation.\n*   **Allowed Functions/Symbols:** Standard normal CDF $\\Phi(\\cdot)$, standard normal PDF $\\phi(\\cdot)$, and $\\sigma_{\\mathbf{n}}^{2} = \\mathbf{n}^{\\mathsf{T}}\\Sigma \\mathbf{n}$.\n\n**Validation Analysis:**\nThe problem is scientifically grounded, well-posed, and objective. It provides a clear, self-contained set of mathematical definitions and statistical models commonly used in geolocation-dependent data analysis, such as in remote sensing. The assumptions (linear boundary, Gaussian error, uniform sampling) are explicit simplifications for analytical tractability, not scientific flaws. The problem asks for a mathematical derivation based on these assumptions, which is a standard task in quantitative sciences. All terms are formally defined, and there are no contradictions, ambiguities, or missing information that would prevent a unique solution.\n\n**Verdict:** The problem is valid.\n\n_Step 2: Derivation of the Solution_\n\nThe observed signed distance $d_{\\text{obs}}$ can be expressed as the sum of the true signed distance $d$ and a projected error term:\n$$d_{\\text{obs}} = \\mathbf{n}^{\\mathsf{T}}(\\mathbf{r} + \\boldsymbol{\\epsilon}) = \\mathbf{n}^{\\mathsf{T}}\\mathbf{r} + \\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon} = d + \\epsilon_n$$\nwhere we define the scalar error component along the normal vector $\\mathbf{n}$ as $\\epsilon_n = \\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon}$.\n\nThe geolocation error $\\boldsymbol{\\epsilon}$ is a multivariate normal random vector, $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$. Since $\\epsilon_n$ is a linear transformation of $\\boldsymbol{\\epsilon}$, $\\epsilon_n$ is also a normally distributed random variable. Its mean is:\n$$E[\\epsilon_n] = E[\\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon}] = \\mathbf{n}^{\\mathsf{T}}E[\\boldsymbol{\\epsilon}] = \\mathbf{n}^{\\mathsf{T}}\\mathbf{0} = 0$$\nIts variance is:\n$$\\text{Var}(\\epsilon_n) = E[\\epsilon_n^2] - (E[\\epsilon_n])^2 = E[(\\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon})(\\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon})^{\\mathsf{T}}] = E[\\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\mathsf{T}}\\mathbf{n}] = \\mathbf{n}^{\\mathsf{T}}E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\mathsf{T}}]\\mathbf{n}$$\nGiven that $E[\\boldsymbol{\\epsilon}] = \\mathbf{0}$, the covariance matrix is $\\Sigma = E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\mathsf{T}}]$. Therefore, the variance is $\\text{Var}(\\epsilon_n) = \\mathbf{n}^{\\mathsf{T}}\\Sigma\\mathbf{n}$, which is defined as $\\sigma_{\\mathbf{n}}^2$.\nThus, the error component normal to the boundary follows a one-dimensional normal distribution: $\\epsilon_n \\sim \\mathcal{N}(0, \\sigma_{\\mathbf{n}}^2)$.\n\nThe confusion matrix $C$ has entries $C_{ij} = P(\\text{label}=j | \\text{true class}=i)$. The rows of the matrix must sum to $1$, so $C_{AB} = 1 - C_{AA}$ and $C_{BA} = 1 - C_{BB}$. We need to compute the diagonal entries $C_{AA}$ and $C_{BB}$.\n\n**Calculation of $C_{AA}$ (True Positive Rate for Class A):**\n$C_{AA}$ is the probability that an observation from true class $A$ is labeled as $A$. This is obtained by marginalizing over the sampling distribution of the true distance $d$ for class $A$ samples.\n$$C_{AA} = P(d_{\\text{obs}} \\geq 0 | \\text{true class A})$$\nBy the law of total probability, we integrate over all possible values of $d$ for class $A$:\n$$C_{AA} = \\int_{0}^{D} P(d_{\\text{obs}} \\geq 0 | d) \\cdot p(d | \\text{true class A}) \\, \\mathrm{d}d$$\nGiven $p(d | \\text{true class A}) = \\frac{1}{D}$ for $d \\in [0, D]$, this becomes:\n$$C_{AA} = \\frac{1}{D} \\int_{0}^{D} P(d + \\epsilon_n \\geq 0) \\, \\mathrm{d}d$$\nThe probability inside the integral is:\n$$P(d + \\epsilon_n \\geq 0) = P(\\epsilon_n \\geq -d)$$\nLet $Z = \\epsilon_n / \\sigma_{\\mathbf{n}}$ be a standard normal variable, $Z \\sim \\mathcal{N}(0, 1)$.\n$$P(\\epsilon_n \\geq -d) = P\\left(Z \\geq -\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) = 1 - P\\left(Z < -\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) = 1 - \\Phi\\left(-\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) = \\Phi\\left(\\frac{d}{\\sigma_{\\mathbf{n}}}\\right)$$\nSubstituting this back into the integral for $C_{AA}$:\n$$C_{AA} = \\frac{1}{D} \\int_{0}^{D} \\Phi\\left(\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) \\, \\mathrm{d}d$$\nTo evaluate this integral, we make the substitution $x = d/\\sigma_{\\mathbf{n}}$, which gives $\\mathrm{d}d = \\sigma_{\\mathbf{n}}\\mathrm{d}x$. The limits of integration change from $[0, D]$ to $[0, D/\\sigma_{\\mathbf{n}}]$.\n$$C_{AA} = \\frac{1}{D} \\int_{0}^{D/\\sigma_{\\mathbf{n}}} \\Phi(x) (\\sigma_{\\mathbf{n}} \\mathrm{d}x) = \\frac{\\sigma_{\\mathbf{n}}}{D} \\int_{0}^{D/\\sigma_{\\mathbf{n}}} \\Phi(x) \\, \\mathrm{d}x$$\nWe use integration by parts, $\\int u \\, \\mathrm{d}v = uv - \\int v \\, \\mathrm{d}u$, with $u = \\Phi(x)$ and $\\mathrm{d}v = \\mathrm{d}x$. This gives $\\mathrm{d}u = \\phi(x)\\mathrm{d}x$ and $v = x$. The indefinite integral is:\n$$\\int \\Phi(x) \\, \\mathrm{d}x = x\\Phi(x) - \\int x\\phi(x) \\, \\mathrm{d}x$$\nSince the derivative of $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-x^2/2)$ is $\\phi'(x) = -x\\phi(x)$, the integral $\\int x\\phi(x) \\, \\mathrm{d}x = -\\phi(x)$.\n$$\\int \\Phi(x) \\, \\mathrm{d}x = x\\Phi(x) + \\phi(x) + \\text{const.}$$\nApplying the limits of integration:\n$$\\int_{0}^{D/\\sigma_{\\mathbf{n}}} \\Phi(x) \\, \\mathrm{d}x = \\left[x\\Phi(x) + \\phi(x)\\right]_{0}^{D/\\sigma_{\\mathbf{n}}} = \\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right)\\right) - \\left(0 \\cdot \\Phi(0) + \\phi(0)\\right)$$\nSubstituting this result back into the expression for $C_{AA}$:\n$$C_{AA} = \\frac{\\sigma_{\\mathbf{n}}}{D} \\left[ \\frac{D}{\\sigma_{\\mathbf{n}}}\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right]$$\nSimplifying yields the final expression for $C_{AA}$:\n$$C_{AA} = \\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)$$\n\n**Calculation of $C_{BB}$ (True Positive Rate for Class B):**\n$C_{BB}$ is the probability that an observation from true class $B$ is labeled as $B$. The sampling distribution is $p(d | \\text{true class B}) = \\frac{1}{D}$ for $d \\in [-D, 0)$.\n$$C_{BB} = P(d_{\\text{obs}} < 0 | \\text{true class B}) = \\frac{1}{D} \\int_{-D}^{0} P(d + \\epsilon_n < 0) \\, \\mathrm{d}d$$\nThe probability inside the integral is:\n$$P(d + \\epsilon_n < 0) = P(\\epsilon_n < -d) = P\\left(Z < -\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) = \\Phi\\left(-\\frac{d}{\\sigma_{\\mathbf{n}}}\\right)$$\nThe integral for $C_{BB}$ is:\n$$C_{BB} = \\frac{1}{D} \\int_{-D}^{0} \\Phi\\left(-\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) \\, \\mathrm{d}d$$\nDue to the symmetry of the problem setup, we expect $C_{BB} = C_{AA}$. We can prove this by substitution. Let $u = -d$, so $\\mathrm{d}u = -\\mathrm{d}d$. The integration limits for $u$ become $[D, 0]$.\n$$C_{BB} = \\frac{1}{D} \\int_{D}^{0} \\Phi\\left(\\frac{u}{\\sigma_{\\mathbf{n}}}\\right) (-\\mathrm{d}u) = \\frac{1}{D} \\int_{0}^{D} \\Phi\\left(\\frac{u}{\\sigma_{\\mathbf{n}}}\\right) \\, \\mathrm{d}u$$\nThis integral is identical in form to the one for $C_{AA}$, therefore $C_{BB} = C_{AA}$.\n\n**Final Confusion Matrix:**\nThe confusion matrix is symmetric with diagonal entries equal to each other, and off-diagonal entries equal to each other.\nLet $P_{\\text{correct}} = C_{AA} = C_{BB} = \\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)$.\nThe off-diagonal entries are $P_{\\text{error}} = C_{AB} = C_{BA} = 1 - P_{\\text{correct}}$.\nThe complete confusion matrix is:\n$$C(\\Sigma, D, \\mathbf{n}) = \\begin{pmatrix} P_{\\text{correct}} & 1 - P_{\\text{correct}} \\\\ 1 - P_{\\text{correct}} & P_{\\text{correct}} \\end{pmatrix}$$\nSubstituting the full expression for $P_{\\text{correct}}$ gives the final result.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right) & 1 - \\left(\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)\\right) \\\\\n1 - \\left(\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)\\right) & \\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With a limited budget for fieldwork, where should you collect the next training label to most improve your model? This question is the focus of active learning, a strategy for intelligent and adaptive data collection. This problem  combines theory and implementation, first asking you to derive a powerful scoring function—the expected model change—to identify the most informative unlabeled locations. You will then translate this theory into a practical greedy algorithm that selects a batch of new samples, balancing the quest for information with the real-world constraint of ensuring spatial diversity in your sampling campaign.",
            "id": "3856366",
            "problem": "Consider a binary land-cover classification task in remote sensing and environmental modeling, where the goal is to decide whether vegetation is present at a location based on features derived from multispectral imagery and terrain data. You are planning a supervised training data collection campaign and must select unlabeled locations to visit. Each candidate unlabeled location has a feature vector derived from common remote sensing indices and terrain metrics, such as the Normalized Difference Vegetation Index (NDVI), the Normalized Difference Water Index (NDWI), elevation and slope, and a two-dimensional coordinate representing its geographic position in a projected coordinate system in meters.\n\nAssume a logistic regression model with parameter vector $w \\in \\mathbb{R}^d$ is currently trained on some preliminary data. The model defines the probability of vegetation presence at feature vector $x \\in \\mathbb{R}^d$ as $P(y=1 \\mid x, w) = \\sigma(w^\\top x)$, where $\\sigma(\\cdot)$ is the logistic sigmoid function. The model is trained using cross-entropy loss, and parameters are updated by gradient descent with learning rate $\\eta > 0$ using an observed label $y \\in \\{0,1\\}$ at a candidate location.\n\nYour tasks are:\n1. Starting from the definitions of logistic regression, the logistic sigmoid function, and cross-entropy loss, derive the expression for the expected squared magnitude of the model parameter update for a single gradient descent step at a candidate location $(x, u)$, where $u \\in \\mathbb{R}^2$ are the spatial coordinates in meters. The expectation is taken with respect to the model’s current predictive distribution over the unknown label at that location. Express your result purely in terms of $w$, $x$, and $\\eta$.\n2. Using the derived expected change as a scoring function, design and implement a batch selection procedure that chooses a set of indices corresponding to unlabeled locations to visit that maximizes the sum of these scores subject to a spatial diversity constraint: for any two selected locations with coordinates $u_i$ and $u_j$, their Euclidean distance must be at least $D_{\\min}$ meters (that is, the constraint is $\\|u_i - u_j\\|_2 \\ge D_{\\min}$). If it is not possible to select the desired batch size due to the spatial constraint, select as many as possible without violating the constraint. Indices are zero-based.\n\nYou must write a complete, runnable program that:\n- Computes the expected squared change score for each candidate location from the given data and a fixed parameter vector $w$ and learning rate $\\eta$.\n- Selects a batch greedily by descending score while enforcing the pairwise spatial diversity constraint (distance at least $D_{\\min}$ meters).\n- Processes the following test suite. For each test case, output the selected indices as a list. Aggregate the results for all test cases into a single line: a comma-separated list of the three index lists enclosed in square brackets (for example, $[ [i_1,i_2], [j_1,j_2,j_3], [k_1] ]$ must be printed as $[[i_1,i_2],[j_1,j_2,j_3],[k_1]]$ with no spaces).\n\nUse the following fixed model parameter and learning rate for all test cases:\n$$\nw = \\begin{bmatrix} 0.15 & 0.65 & -0.45 & 0.25 & 0.10 \\end{bmatrix}, \\quad \\eta = 0.05.\n$$\n\nTest Case $1$ (general case with a moderate diversity threshold):\n- Feature dimension $d = 5$ with bias included as the first element of each feature vector (that is, the first feature is $1$). Eight candidate locations with features and projected coordinates in meters:\n$$\n\\begin{aligned}\nx_0 &= \\begin{bmatrix} 1 & 0.6 & -0.2 & 0.1 & 0.05 \\end{bmatrix}, & u_0 &= \\begin{bmatrix} 1000 & 1000 \\end{bmatrix}, \\\\\nx_1 &= \\begin{bmatrix} 1 & 0.1 & 0.3 & -0.4 & -0.1 \\end{bmatrix}, & u_1 &= \\begin{bmatrix} 1100 & 1600 \\end{bmatrix}, \\\\\nx_2 &= \\begin{bmatrix} 1 & 0.9 & -0.7 & 0.5 & 0.2 \\end{bmatrix}, & u_2 &= \\begin{bmatrix} 1300 & 1200 \\end{bmatrix}, \\\\\nx_3 &= \\begin{bmatrix} 1 & -0.3 & 0.8 & -0.2 & 0.6 \\end{bmatrix}, & u_3 &= \\begin{bmatrix} 2000 & 900 \\end{bmatrix}, \\\\\nx_4 &= \\begin{bmatrix} 1 & 0.2 & -0.1 & 0.0 & -0.05 \\end{bmatrix}, & u_4 &= \\begin{bmatrix} 2100 & 900 \\end{bmatrix}, \\\\\nx_5 &= \\begin{bmatrix} 1 & -0.8 & 0.4 & 0.3 & -0.3 \\end{bmatrix}, & u_5 &= \\begin{bmatrix} 3000 & 3000 \\end{bmatrix}, \\\\\nx_6 &= \\begin{bmatrix} 1 & 0.4 & 0.2 & -0.1 & 0.4 \\end{bmatrix}, & u_6 &= \\begin{bmatrix} 3200 & 2900 \\end{bmatrix}, \\\\\nx_7 &= \\begin{bmatrix} 1 & -0.1 & -0.5 & 0.7 & -0.2 \\end{bmatrix}, & u_7 &= \\begin{bmatrix} 3300 & 1000 \\end{bmatrix}.\n\\end{aligned}\n$$\nBatch size:\n$$\nK = 3, \\quad D_{\\min} = 150 \\text{ meters}.\n$$\n\nTest Case $2$ (boundary case where some pairs are exactly at the threshold and are permitted):\n- Six candidate locations:\n$$\n\\begin{aligned}\nx_0 &= \\begin{bmatrix} 1 & 0.7 & -0.3 & 0.2 & 0.1 \\end{bmatrix}, & u_0 &= \\begin{bmatrix} 1000 & 1000 \\end{bmatrix}, \\\\\nx_1 &= \\begin{bmatrix} 1 & -0.2 & 0.6 & 0.1 & -0.2 \\end{bmatrix}, & u_1 &= \\begin{bmatrix} 1200 & 1000 \\end{bmatrix}, \\\\\nx_2 &= \\begin{bmatrix} 1 & 0.5 & 0.4 & -0.2 & 0.3 \\end{bmatrix}, & u_2 &= \\begin{bmatrix} 1500 & 1400 \\end{bmatrix}, \\\\\nx_3 &= \\begin{bmatrix} 1 & -0.6 & 0.1 & 0.4 & -0.1 \\end{bmatrix}, & u_3 &= \\begin{bmatrix} 1700 & 1200 \\end{bmatrix}, \\\\\nx_4 &= \\begin{bmatrix} 1 & 0.3 & -0.4 & 0.6 & 0.0 \\end{bmatrix}, & u_4 &= \\begin{bmatrix} 2100 & 1600 \\end{bmatrix}, \\\\\nx_5 &= \\begin{bmatrix} 1 & -0.1 & 0.2 & -0.5 & 0.2 \\end{bmatrix}, & u_5 &= \\begin{bmatrix} 2300 & 1600 \\end{bmatrix}.\n\\end{aligned}\n$$\nBatch size and diversity threshold:\n$$\nK = 4, \\quad D_{\\min} = 200 \\text{ meters}.\n$$\n\nTest Case $3$ (edge case where a large diversity threshold prevents filling the full batch):\n- Seven candidate locations, five forming a tight cluster and two far apart:\n$$\n\\begin{aligned}\nx_0 &= \\begin{bmatrix} 1 & 0.2 & 0.1 & -0.1 & 0.05 \\end{bmatrix}, & u_0 &= \\begin{bmatrix} 1000 & 1000 \\end{bmatrix}, \\\\\nx_1 &= \\begin{bmatrix} 1 & 0.21 & 0.12 & -0.11 & 0.06 \\end{bmatrix}, & u_1 &= \\begin{bmatrix} 1020 & 1010 \\end{bmatrix}, \\\\\nx_2 &= \\begin{bmatrix} 1 & 0.19 & 0.09 & -0.12 & 0.04 \\end{bmatrix}, & u_2 &= \\begin{bmatrix} 990 & 1030 \\end{bmatrix}, \\\\\nx_3 &= \\begin{bmatrix} 1 & 0.18 & 0.15 & -0.10 & 0.03 \\end{bmatrix}, & u_3 &= \\begin{bmatrix} 1010 & 980 \\end{bmatrix}, \\\\\nx_4 &= \\begin{bmatrix} 1 & 0.22 & 0.11 & -0.13 & 0.07 \\end{bmatrix}, & u_4 &= \\begin{bmatrix} 980 & 990 \\end{bmatrix}, \\\\\nx_5 &= \\begin{bmatrix} 1 & -0.9 & 0.4 & 0.2 & -0.3 \\end{bmatrix}, & u_5 &= \\begin{bmatrix} 5000 & 5000 \\end{bmatrix}, \\\\\nx_6 &= \\begin{bmatrix} 1 & 0.8 & -0.6 & 0.5 & 0.2 \\end{bmatrix}, & u_6 &= \\begin{bmatrix} 6000 & 7000 \\end{bmatrix}.\n\\end{aligned}\n$$\nBatch size and diversity threshold:\n$$\nK = 4, \\quad D_{\\min} = 500 \\text{ meters}.\n$$\n\nProgram requirements:\n- Implement the logistic sigmoid and compute $p_i = \\sigma(w^\\top x_i)$ for each candidate.\n- Compute the expected squared parameter update magnitude score for each candidate based on your derivation and use it to rank candidates.\n- Select batches for each test case with the greedy procedure that respects the spatial constraint $\\|u_i - u_j\\|_2 \\ge D_{\\min}$; if a tie on scores occurs, break ties by smaller index.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each element is a list of selected indices for one test case, for example:\n$$\n[[i_1,i_2,i_3],[j_1,j_2,j_3,j_4],[k_1,k_2]]\n$$\nIndices must be integers and zero-based.",
            "solution": "The problem requires a two-part solution: first, the derivation of a scoring function based on the expected impact of a new labeled point on a logistic regression model, and second, the design of a greedy algorithm to select a batch of sampling locations that maximizes this score under a spatial diversity constraint.\n\n### Part 1: Derivation of the Expected Squared Parameter Update\n\nThe goal is to derive the expression for the expected squared magnitude of the model parameter update for a single gradient descent step, $\\mathbb{E}[\\|\\Delta w\\|^2]$, at a candidate location with feature vector $x$. The expectation is over the model's current predictive distribution for the unknown binary label $y \\in \\{0, 1\\}$.\n\n**1. Logistic Regression Model and Loss Function**\nThe model predicts the probability of the positive class ($y=1$) as $p = P(y=1 \\mid x, w) = \\sigma(w^\\top x)$, where $\\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic sigmoid function. The probability of the negative class ($y=0$) is $1-p$. The learning process aims to minimize the binary cross-entropy loss for a single labeled data point $(x, y)$:\n$$\nL(w; x, y) = -[y \\log(p) + (1-y) \\log(1-p)]\n$$\n\n**2. Gradient of the Loss Function**\nTo perform gradient descent, we need the gradient of the loss $L$ with respect to the model parameters $w$. A key property of the sigmoid function is that its derivative is $\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1-\\sigma(z))$. Let $z = w^\\top x$. Using the chain rule, we compute the gradient $\\nabla_w L$:\n$$\n\\nabla_w L = \\frac{\\partial L}{\\partial p} \\frac{\\partial p}{\\partial z} \\frac{\\partial z}{\\partial w}\n$$\nThe components are:\n- $\\frac{\\partial L}{\\partial p} = -\\left(\\frac{y}{p} - \\frac{1-y}{1-p}\\right) = -\\frac{y-p}{p(1-p)}$\n- $\\frac{\\partial p}{\\partial z} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$\n- $\\frac{\\partial z}{\\partial w} = \\frac{\\partial(w^\\top x)}{\\partial w} = x$\n\nCombining these terms yields the remarkably simple expression for the gradient:\n$$\n\\nabla_w L = \\left(-\\frac{y-p}{p(1-p)}\\right) (p(1-p)) (x) = -(y-p)x = (p-y)x\n$$\n\n**3. Gradient Descent Parameter Update**\nA single step of gradient descent updates the parameters $w$ with a learning rate $\\eta > 0$ as follows:\n$$\nw_{\\text{new}} = w - \\eta \\nabla_w L\n$$\nThe parameter update vector, $\\Delta w = w_{\\text{new}} - w$, is therefore:\n$$\n\\Delta w = -\\eta \\nabla_w L = -\\eta (p-y)x = \\eta(y-p)x\n$$\n\n**4. Expected Squared Magnitude of the Parameter Update**\nThe label $y$ for a candidate location is unknown. We take the expectation of the squared L2-norm of the update vector, $\\|\\Delta w\\|^2_2$, with respect to the model's predictive distribution for $y$. This distribution is a Bernoulli trial with success probability $p$:\n- $y=1$ with probability $p$\n- $y=0$ with probability $1-p$\n\nThe expectation $\\mathbb{E}_{y \\sim \\text{Bernoulli}(p)}[\\|\\Delta w\\|^2_2]$ is calculated as:\n$$\n\\mathbb{E}[\\|\\Delta w\\|^2_2] = P(y=1) \\cdot \\|\\Delta w|_{y=1}\\|^2_2 + P(y=0) \\cdot \\|\\Delta w|_{y=0}\\|^2_2\n$$\nFirst, we find the squared norm for each possible outcome of $y$:\n- If $y=1$: $\\Delta w = \\eta(1-p)x$. Then $\\|\\Delta w\\|^2_2 = \\|\\eta(1-p)x\\|^2_2 = \\eta^2 (1-p)^2 \\|x\\|^2_2$.\n- If $y=0$: $\\Delta w = \\eta(0-p)x = -\\eta px$. Then $\\|\\Delta w\\|^2_2 = \\|-\\eta px\\|^2_2 = \\eta^2 p^2 \\|x\\|^2_2$.\n\nSubstituting these into the expectation formula:\n$$\n\\begin{aligned}\n\\mathbb{E}[\\|\\Delta w\\|^2_2] &= p \\cdot \\left(\\eta^2 (1-p)^2 \\|x\\|^2_2\\right) + (1-p) \\cdot \\left(\\eta^2 p^2 \\|x\\|^2_2\\right) \\\\\n&= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)^2 + (1-p)p^2\\right] \\\\\n&= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)( (1-p) + p )\\right] \\\\\n&= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)(1)\\right] \\\\\n&= \\eta^2 p(1-p) \\|x\\|^2_2\n\\end{aligned}\n$$\nSubstituting $p = \\sigma(w^\\top x)$, the final expression for the scoring function $S(x)$ is:\n$$\nS(x) = \\eta^2 \\sigma(w^\\top x) (1 - \\sigma(w^\\top x)) \\|x\\|^2_2\n$$\nThis score represents the expected squared magnitude of the change in model weights if we were to sample the label at location $x$ and perform one gradient update. It combines model uncertainty (the term $p(1-p)$ is maximized at $p=0.5$) with the feature vector's magnitude ($\\|x\\|^2_2$), favoring uncertain points that are also influential.\n\n### Part 2: Batch Selection Algorithm\n\nThe second task is to implement a batch selection procedure using the derived score $S(x_i)$ for each candidate location $i$. The procedure must select a batch of $K$ indices that maximizes the sum of scores while adhering to a spatial diversity constraint: any two selected locations $i$ and $j$ must be separated by a Euclidean distance of at least $D_{\\min}$, i.e., $\\|u_i - u_j\\|_2 \\ge D_{\\min}$.\n\nThe specified algorithm is a greedy selection process:\n1.  For each candidate location $i$ with feature vector $x_i$ and coordinates $u_i$, compute its score $S_i = \\eta^2 \\sigma(w^\\top x_i) (1 - \\sigma(w^\\top x_i)) \\|x_i\\|^2_2$.\n2.  Create a list of candidate indices and sort them in descending order based on their scores. Ties in scores are broken by choosing the smaller index first.\n3.  Initialize an empty list for the selected batch, `selected_indices`.\n4.  Iterate through the sorted list of candidate indices. For each candidate index $j$:\n    a. Check if adding location $j$ to the batch violates the spatial constraint. This is done by calculating the Euclidean distance from $u_j$ to the coordinates $u_k$ of every location $k$ already in `selected_indices`.\n    b. If for all $k \\in \\text{selected\\_indices}$, the distance $\\|u_j - u_k\\|_2 \\ge D_{\\min}$, then candidate $j$ is valid.\n    c. If candidate $j$ is valid, add its index to `selected_indices`.\n    d. If the size of `selected_indices` reaches the desired batch size $K$, terminate the process.\n5.  If the loop finishes before the batch is full (i.e., size less than $K$), the procedure concludes with the partially filled batch. This happens when no remaining candidates can satisfy the spatial constraint with the already-selected locations.\n6.  The final output for each test case is the list `selected_indices` containing the zero-based indices of the chosen locations in the order they were selected.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the batch selection problem for three test cases.\n    \"\"\"\n    \n    # Fixed model parameter and learning rate for all test cases\n    w = np.array([0.15, 0.65, -0.45, 0.25, 0.10])\n    eta = 0.05\n    \n    test_cases = [\n        {\n            \"K\": 3,\n            \"D_min\": 150.0,\n            \"X\": np.array([\n                [1, 0.6, -0.2, 0.1, 0.05],\n                [1, 0.1, 0.3, -0.4, -0.1],\n                [1, 0.9, -0.7, 0.5, 0.2],\n                [1, -0.3, 0.8, -0.2, 0.6],\n                [1, 0.2, -0.1, 0.0, -0.05],\n                [1, -0.8, 0.4, 0.3, -0.3],\n                [1, 0.4, 0.2, -0.1, 0.4],\n                [1, -0.1, -0.5, 0.7, -0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1100, 1600], [1300, 1200], [2000, 900],\n                [2100, 900], [3000, 3000], [3200, 2900], [3300, 1000]\n            ])\n        },\n        {\n            \"K\": 4,\n            \"D_min\": 200.0,\n            \"X\": np.array([\n                [1, 0.7, -0.3, 0.2, 0.1],\n                [1, -0.2, 0.6, 0.1, -0.2],\n                [1, 0.5, 0.4, -0.2, 0.3],\n                [1, -0.6, 0.1, 0.4, -0.1],\n                [1, 0.3, -0.4, 0.6, 0.0],\n                [1, -0.1, 0.2, -0.5, 0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1200, 1000], [1500, 1400],\n                [1700, 1200], [2100, 1600], [2300, 1600]\n            ])\n        },\n        {\n            \"K\": 4,\n            \"D_min\": 500.0,\n            \"X\": np.array([\n                [1, 0.2, 0.1, -0.1, 0.05],\n                [1, 0.21, 0.12, -0.11, 0.06],\n                [1, 0.19, 0.09, -0.12, 0.04],\n                [1, 0.18, 0.15, -0.10, 0.03],\n                [1, 0.22, 0.11, -0.13, 0.07],\n                [1, -0.9, 0.4, 0.2, -0.3],\n                [1, 0.8, -0.6, 0.5, 0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1020, 1010], [990, 1030],\n                [1010, 980], [980, 990], [5000, 5000], [6000, 7000]\n            ])\n        }\n    ]\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    all_results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        U = case[\"U\"]\n        K = case[\"K\"]\n        D_min = case[\"D_min\"]\n        num_candidates = X.shape[0]\n\n        # 1. Compute scores for all candidates\n        z = X @ w\n        p = sigmoid(z)\n        p_variance = p * (1 - p)\n        x_norm_sq = np.sum(X**2, axis=1)\n        scores = (eta**2) * p_variance * x_norm_sq\n\n        # 2. Sort candidate indices by score (desc) and original index (asc) for tie-breaking\n        candidate_indices = sorted(range(num_candidates), key=lambda i: (-scores[i], i))\n\n        # 3. Greedy selection with spatial constraint\n        selected_indices = []\n        for idx in candidate_indices:\n            if len(selected_indices) >= K:\n                break\n\n            is_valid = True\n            current_u = U[idx]\n            for selected_idx in selected_indices:\n                dist = np.linalg.norm(current_u - U[selected_idx])\n                if dist < D_min:\n                    is_valid = False\n                    break\n            \n            if is_valid:\n                selected_indices.append(idx)\n        \n        all_results.append(selected_indices)\n\n    # Format the output string to match the required format: [[...],[...],...]\n    result_str = \"[\" + \",\".join([str(sublist).replace(\" \", \"\") for sublist in all_results]) + \"]\"\n    print(result_str)\n\n\nsolve()\n```"
        }
    ]
}