{
    "hands_on_practices": [
        {
            "introduction": "在地理空间分析中，一个普遍的误解是样本越多越好。然而，由于空间自相关性的存在，邻近的样本点往往包含冗余信息，这违背了传统统计学中独立同分布（i.i.d.）的基本假设。本练习  引导你从第一性原理出发，推导“有效样本量” ($n_{\\mathrm{eff}}$) 的概念，从而量化一个空间相关数据集中真正独立的信息量。掌握这一技能对于准确评估模型置信度、避免对数据信息量的盲目乐观至关重要。",
            "id": "3856352",
            "problem": "您正在为一个在面积为 $A$ 的二维域（例如，覆盖大片区域的影像镶嵌图）上的遥感回归任务设计一个监督训练数据集。设 $Z(\\mathbf{s})$ 表示一个二阶平稳且各向同性的随机场，代表在空间位置 $\\mathbf{s} \\in \\mathbb{R}^{2}$ 处的目标变量（例如，土壤湿度），其均值为 $\\mu$，协方差函数为 $C(h)=\\sigma^{2}\\exp(-h/\\phi)$，其中 $h$ 是分离距离，$\\sigma^{2}$ 是边际方差，$\\phi$ 是相关长度。您以非常高且近似均匀的空间密度（例如，分辨率远高于 $\\phi$ 的所有像素）收集标签，并在训练时将它们视为独立的。\n\n将您的训练集的有效样本量 $n_{\\mathrm{eff}}$ 定义为一个来自方差为 $\\sigma^{2}$ 的随机变量的独立同分布样本的大小，该样本为空间面积平均值产生的方差与您实际的空间相关训练数据所产生的方差相同。从二阶平稳随机场和面积平均方差的基本定义出发，并假设 $A \\gg \\phi^{2}$ 以至于边缘效应可以忽略不计，推导出 $n_{\\mathrm{eff}}$ 关于 $A$ 和 $\\phi$ 的闭式解析表达式。根据您的表达式，得出 $n_{\\mathrm{eff}}$ 如何随域大小和相关长度而变化。请仅提供 $n_{\\mathrm{eff}}$ 的最终解析表达式作为您的答案。无需进行数值代入，也无需四舍五入。",
            "solution": "这个问题是有效的。它在科学上基于地质统计学和随机场理论的原理，问题提法得当，信息充分，目标明确，并以精确、客观的语言表述。\n\n目标是为一个在面积为 $A$ 的二维域上的空间相关数据集推导有效样本量 $n_{\\mathrm{eff}}$ 的表达式。代表目标变量的随机场 $Z(\\mathbf{s})$ 是二阶平稳且各向同性的，其均值为 $\\mu$，指数协方差函数为 $C(h) = \\sigma^{2}\\exp(-h/\\phi)$，其中 $h$ 是欧几里得距离，$\\sigma^2$ 是方差，$\\phi$ 是相关长度。\n\n有效样本量 $n_{\\mathrm{eff}}$ 被定义为一个假设的独立同分布（i.i.d.）样本的大小，该样本的样本均值的方差，与相关场 $Z(\\mathbf{s})$ 的空间面积平均的方差相同。\n\n首先，考虑一组 $n$ 个独立同分布的随机变量 $\\{X_1, X_2, \\ldots, X_n\\}$，每个变量的方差为 $\\sigma^2$。它们的样本均值 $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$ 的方差由下式给出：\n$$ \\mathrm{Var}(\\bar{X}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(X_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n} $$\n根据问题中提供的定义，对于有效样本量 $n_{\\mathrm{eff}}$，该方差为 $\\frac{\\sigma^2}{n_{\\mathrm{eff}}}$。\n\n接下来，我们必须计算随机场 $Z(\\mathbf{s})$ 在域上的空间面积平均的方差，我们将该域记为 $D$，其面积为 $A = \\int_D d\\mathbf{s}$。空间面积平均 $\\bar{Z}_A$ 定义为：\n$$ \\bar{Z}_A = \\frac{1}{A} \\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s} $$\n$\\bar{Z}_A$ 的方差为：\n$$ \\mathrm{Var}(\\bar{Z}_A) = \\mathrm{Var}\\left(\\frac{1}{A} \\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) = \\frac{1}{A^2} \\mathrm{Var}\\left(\\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) $$\n对于二阶平稳随机场，该积分的方差由协方差函数在域上的双重积分给出：\n$$ \\mathrm{Var}\\left(\\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) = \\int_{D} \\int_{D} \\mathrm{Cov}(Z(\\mathbf{s}_1), Z(\\mathbf{s}_2)) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 $$\n协方差由 $C(\\|\\mathbf{s}_1 - \\mathbf{s}_2\\|)$ 给出。因此，\n$$ \\mathrm{Var}(\\bar{Z}_A) = \\frac{1}{A^2} \\int_{D} \\int_{D} C(\\|\\mathbf{s}_1 - \\mathbf{s}_2\\|) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 $$\n计算这个四维积分通常很复杂。然而，问题陈述了假设，即域的面积 $A$ 远大于相关长度的平方，$A \\gg \\phi^2$。这意味着域的空间范围远大于数据点显著相关的范围。这使我们可以忽略边缘效应。\n\n我们可以通过变量替换来简化积分。令 $\\mathbf{h} = \\mathbf{s}_2 - \\mathbf{s}_1$。积分变为：\n$$ \\int_{D} \\int_{D} C(\\|\\mathbf{s}_2 - \\mathbf{s}_1\\|) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 = \\int_{D} \\left( \\int_{D-\\mathbf{s}_1} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\right) d\\mathbf{s}_1 $$\n其中 $D-\\mathbf{s}_1$ 是由向量 $-\\mathbf{s}_1$ 平移后的域 $D$。对于任何不靠近 $D$ 边界（相对于 $\\phi$）的点 $\\mathbf{s}_1$，协方差函数 $C(\\|\\mathbf{h}\\|)$ 将在积分域 $D-\\mathbf{s}_1$ 内部很好地衰减到近似为零。鉴于假设 $A \\gg \\phi^2$，靠近边界的点 $\\mathbf{s}_1$ 的贡献可以忽略不计。因此，我们可以通过将其定义域扩展到整个 $\\mathbb{R}^2$ 来近似内层积分：\n$$ \\int_{D-\\mathbf{s}_1} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\approx \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\n这种近似使得内层积分独立于 $\\mathbf{s}_1$。外层积分则变为：\n$$ \\int_{D} \\left(\\ldots\\right) d\\mathbf{s}_1 \\approx A \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\n将此代回面积平均方差的表达式中：\n$$ \\mathrm{Var}(\\bar{Z}_A) \\approx \\frac{1}{A^2} \\left( A \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\right) = \\frac{1}{A} \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\n现在我们必须计算给定协方差函数 $C(h) = \\sigma^2 \\exp(-h/\\phi)$ 在 $\\mathbb{R}^2$ 上的积分。由于该函数是各向同性的，我们转换为极坐标 $(h, \\theta)$，其中 $h$ 是径向距离，$d\\mathbf{h} = h \\, dh \\, d\\theta$。\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = \\int_{0}^{2\\pi} \\int_{0}^{\\infty} \\sigma^2 \\exp(-h/\\phi) \\, h \\, dh \\, d\\theta $$\n从 $0$ 到 $2\\pi$ 对 $\\theta$ 的积分得到一个因子 $2\\pi$：\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = 2\\pi\\sigma^2 \\int_{0}^{\\infty} h \\exp(-h/\\phi) \\, dh $$\n我们使用分部积分法求解剩余的积分，设 $u=h$ 且 $dv = \\exp(-h/\\phi) \\, dh$。这得到 $du=dh$ 和 $v = -\\phi \\exp(-h/\\phi)$。\n\\begin{align*}\n\\int_{0}^{\\infty} h \\exp(-h/\\phi) \\, dh = \\left[ -h\\phi\\exp(-h/\\phi) \\right]_0^{\\infty} - \\int_{0}^{\\infty} (-\\phi) \\exp(-h/\\phi) \\, dh \\\\\n= (0 - 0) + \\phi \\int_{0}^{\\infty} \\exp(-h/\\phi) \\, dh \\\\\n= \\phi \\left[ -\\phi\\exp(-h/\\phi) \\right]_0^{\\infty} \\\\\n= \\phi (0 - (-\\phi \\exp(0))) \\\\\n= \\phi^2\n\\end{align*}\n因此，协方差函数的积分为：\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = 2\\pi\\sigma^2\\phi^2 $$\n将此结果代入我们对 $\\mathrm{Var}(\\bar{Z}_A)$ 的表达式中：\n$$ \\mathrm{Var}(\\bar{Z}_A) \\approx \\frac{2\\pi\\sigma^2\\phi^2}{A} $$\n最后，我们将此与有效样本量为 $n_{\\mathrm{eff}}$ 的独立同分布样本的方差相等同：\n$$ \\frac{\\sigma^2}{n_{\\mathrm{eff}}} = \\frac{2\\pi\\sigma^2\\phi^2}{A} $$\n假设 $\\sigma^2 \\neq 0$，我们可以从两边消去 $\\sigma^2$：\n$$ \\frac{1}{n_{\\mathrm{eff}}} = \\frac{2\\pi\\phi^2}{A} $$\n解出 $n_{\\mathrm{eff}}$ 得到闭式表达式：\n$$ n_{\\mathrm{eff}} = \\frac{A}{2\\pi\\phi^2} $$\n从这个表达式中，我们得出结论，有效样本量 $n_{\\mathrm{eff}}$ 与域面积 $A$ 成线性比例（$n_{\\mathrm{eff}} \\propto A$），并与相关长度 $\\phi$ 的平方成反比（$n_{\\mathrm{eff}} \\propto \\phi^{-2}$）。$2\\pi\\phi^2$ 这一项可以解释为单个独立样本的有效面积。",
            "answer": "$$\\boxed{\\frac{A}{2\\pi\\phi^{2}}}$$"
        },
        {
            "introduction": "高质量的训练数据是监督学习成功的基石，但“地面真实”数据本身也可能存在误差。在遥感应用中，一个常见的误差来源是外业采样点的地理定位不精确，尤其是在地物类别边界附近，微小的定位漂移就可能导致标签分配错误。本练习  将挑战你对这一过程进行数学建模，通过分析地理定位误差如何转化为标签噪声，最终推导出预期的混淆矩阵。这项实践能深化你对训练数据质量控制的理解，并为你评估和减小标签误差提供定量工具。",
            "id": "3856402",
            "problem": "一个遥感团队正在为一个类别边界局部平滑的区域设计一个用于双类别土地覆盖图（类别 $A$ 和 $B$）的监督训练数据集。训练标签是通过将每个野外观测点的地理位置叠加在地图上进行分配的；然而，由于全球导航卫星系统（GNSS）的漂移，地理位置存在误差，该误差被建模为一个二元高斯随机向量 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$，其协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$。在局部，类别边界被近似为一条单位法向量为 $\\mathbf{n} \\in \\mathbb{R}^{2}$ 的直线。一个真实位置为 $\\mathbf{r}$ 的野外观测点，其到边界的有符号法向距离为 $d = \\mathbf{n}^{\\mathsf{T}}\\mathbf{r}$；按照惯例，$d \\geq 0$ 表示类别 $A$，$d  0$ 表示类别 $B$。训练标签根据观测位置的有符号距离 $d_{\\text{obs}} = \\mathbf{n}^{\\mathsf{T}}(\\mathbf{r} + \\boldsymbol{\\epsilon})$ 的符号来分配。\n\n假设团队为表征混淆情况而有意在边界附近采样点：在给定真实类别的条件下，有符号法向距离 $d$ 在一个以边界为中心、宽度为 $2D$ 的条带内均匀分布，即对于类别 $A$，$d \\sim \\text{Uniform}([0, D])$；对于类别 $B$，$d \\sim \\text{Uniform}([-D, 0])$，且对两个类别的采样投入相等。在此采样方案和误差模型下，定义期望的 $2 \\times 2$ 混淆矩阵 $C(\\Sigma, D, \\mathbf{n})$，其元素 $C_{ij}$ 表示一个真实类别为 $i \\in \\{A,B\\}$ 的观测在存在地理位置误差的情况下被分配标签 $j \\in \\{A,B\\}$ 的概率。\n\n仅从多元正态分布的基本定义、高斯随机向量的线性泛函是高斯分布的性质，以及混淆矩阵作为在 $d$ 的采样分布上平均的类别条件分配概率的定义出发，推导 $C(\\Sigma, D, \\mathbf{n})$ 的闭式解析表达式。您可以使用标准正态累积分布函数 $\\Phi(\\cdot)$、标准正态概率密度函数 $\\phi(\\cdot)$ 以及标量 $\\sigma_{\\mathbf{n}}^{2} = \\mathbf{n}^{\\mathsf{T}}\\Sigma \\mathbf{n}$ 来表示您的答案。\n\n请以完整的 $2 \\times 2$ 矩阵 $C(\\Sigma, D, \\mathbf{n})$ 的形式提供您的最终结果。无需进行数值计算；请将表达式保留为闭式形式。",
            "solution": "用户请求了在一个特定的采样和误差模型下，对期望混淆矩阵进行详细推导。\n\n_步骤1：问题验证_\n\n**提取的已知条件：**\n*   **类别：** 两个类别，记为 $A$ 和 $B$。\n*   **类别边界：** 一个单位法向量为 $\\mathbf{n} \\in \\mathbb{R}^{2}$ 的局部线性近似。\n*   **真实位置和距离：** 一个点的真实位置为 $\\mathbf{r}$，其到边界的有符号法向距离为 $d = \\mathbf{n}^{\\mathsf{T}}\\mathbf{r}$。\n*   **真实类别定义：** 若 $d \\geq 0$ 则为类别 $A$，若 $d  0$ 则为类别 $B$。\n*   **地理位置误差模型：** 误差是一个随机向量 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$，其中 $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ 是协方差矩阵。\n*   **观测位置和距离：** 观测位置为 $\\mathbf{r} + \\boldsymbol{\\epsilon}$，观测到的有符号距离为 $d_{\\text{obs}} = \\mathbf{n}^{\\mathsf{T}}(\\mathbf{r} + \\boldsymbol{\\epsilon})$。\n*   **标签分配规则：** 若 $d_{\\text{obs}} \\geq 0$ 则标记为 $A$，若 $d_{\\text{obs}}  0$ 则标记为 $B$。\n*   **真实距离($d$)的采样分布：**\n    *   对于真实类别 $A$：$d \\sim \\text{Uniform}([0, D])$。\n    *   对于真实类别 $B$：$d \\sim \\text{Uniform}([-D, 0])$。\n*   **目标输出：** $2 \\times 2$ 期望混淆矩阵 $C(\\Sigma, D, \\mathbf{n})$ 的闭式表达式，其中 $C_{ij}$ 是将标签 $j$ 分配给真实类别为 $i$ 的观测的概率。\n*   **允许使用的函数/符号：** 标准正态累积分布函数 $\\Phi(\\cdot)$，标准正态概率密度函数 $\\phi(\\cdot)$，以及 $\\sigma_{\\mathbf{n}}^{2} = \\mathbf{n}^{\\mathsf{T}}\\Sigma \\mathbf{n}$。\n\n**验证分析：**\n该问题具有科学依据、提法明确且客观。它提供了一套清晰、自洽的数学定义和统计模型，这些模型在依赖地理位置的数据分析（如遥感领域）中很常用。这些假设（线性边界、高斯误差、均匀采样）是为了分析上的易处理性而做出的明确简化，并非科学上的缺陷。问题要求基于这些假设进行数学推导，这在量化科学中是一项标准任务。所有术语都有正式定义，不存在会妨碍得到唯一解的矛盾、歧义或缺失信息。\n\n**结论：** 问题有效。\n\n_步骤2：解题推导_\n\n观测到的有符号距离 $d_{\\text{obs}}$ 可以表示为真实有符号距离 $d$ 与一个投影误差项的和：\n$$d_{\\text{obs}} = \\mathbf{n}^{\\mathsf{T}}(\\mathbf{r} + \\boldsymbol{\\epsilon}) = \\mathbf{n}^{\\mathsf{T}}\\mathbf{r} + \\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon} = d + \\epsilon_n$$\n其中，我们将沿法向量 $\\mathbf{n}$ 的标量误差分量定义为 $\\epsilon_n = \\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon}$。\n\n地理位置误差 $\\boldsymbol{\\epsilon}$ 是一个多元正态随机向量，$\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$。由于 $\\epsilon_n$ 是 $\\boldsymbol{\\epsilon}$ 的线性变换，因此 $\\epsilon_n$ 也是一个服从正态分布的随机变量。其均值为：\n$$E[\\epsilon_n] = E[\\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon}] = \\mathbf{n}^{\\mathsf{T}}E[\\boldsymbol{\\epsilon}] = \\mathbf{n}^{\\mathsf{T}}\\mathbf{0} = 0$$\n其方差为：\n$$\\text{Var}(\\epsilon_n) = E[\\epsilon_n^2] - (E[\\epsilon_n])^2 = E[(\\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon})(\\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon})^{\\mathsf{T}}] = E[\\mathbf{n}^{\\mathsf{T}}\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\mathsf{T}}\\mathbf{n}] = \\mathbf{n}^{\\mathsf{T}}E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\mathsf{T}}]\\mathbf{n}$$\n已知 $E[\\boldsymbol{\\epsilon}] = \\mathbf{0}$，协方差矩阵为 $\\Sigma = E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{\\mathsf{T}}]$。因此，方差为 $\\text{Var}(\\epsilon_n) = \\mathbf{n}^{\\mathsf{T}}\\Sigma\\mathbf{n}$，该值被定义为 $\\sigma_{\\mathbf{n}}^2$。\n因此，垂直于边界的误差分量服从一维正态分布：$\\epsilon_n \\sim \\mathcal{N}(0, \\sigma_{\\mathbf{n}}^2)$。\n\n混淆矩阵 $C$ 的元素为 $C_{ij} = P(\\text{标签}=j | \\text{真实类别}=i)$。矩阵的各行之和必须为 1，所以 $C_{AB} = 1 - C_{AA}$ 且 $C_{BA} = 1 - C_{BB}$。我们需要计算对角线元素 $C_{AA}$ 和 $C_{BB}$。\n\n**$C_{AA}$ 的计算（类别A的真阳性率）：**\n$C_{AA}$ 是一个来自真实类别 $A$ 的观测被标记为 $A$ 的概率。这可以通过对类别 $A$ 样本的真实距离 $d$ 的采样分布进行边缘化来求得。\n$$C_{AA} = P(d_{\\text{obs}} \\geq 0 | \\text{true class A})$$\n根据全概率定律，我们对类别 $A$ 的所有可能 $d$ 值进行积分：\n$$C_{AA} = \\int_{0}^{D} P(d_{\\text{obs}} \\geq 0 | d) \\cdot p(d | \\text{true class A}) \\, \\mathrm{d}d$$\n给定对于 $d \\in [0, D]$ 有 $p(d | \\text{真实类别 A}) = \\frac{1}{D}$，上式变为：\n$$C_{AA} = \\frac{1}{D} \\int_{0}^{D} P(d + \\epsilon_n \\geq 0) \\, \\mathrm{d}d$$\n积分内的概率是：\n$$P(d + \\epsilon_n \\geq 0) = P(\\epsilon_n \\geq -d)$$\n令 $Z = \\epsilon_n / \\sigma_{\\mathbf{n}}$ 为一个标准正态变量，$Z \\sim \\mathcal{N}(0, 1)$。\n$$P(\\epsilon_n \\geq -d) = P\\left(Z \\geq -\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) = 1 - P\\left(Z  -\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) = 1 - \\Phi\\left(-\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) = \\Phi\\left(\\frac{d}{\\sigma_{\\mathbf{n}}}\\right)$$\n将此代回 $C_{AA}$ 的积分中：\n$$C_{AA} = \\frac{1}{D} \\int_{0}^{D} \\Phi\\left(\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) \\, \\mathrm{d}d$$\n为计算此积分，我们进行换元 $x = d/\\sigma_{\\mathbf{n}}$，得到 $\\mathrm{d}d = \\sigma_{\\mathbf{n}}\\mathrm{d}x$。积分限从 $[0, D]$ 变为 $[0, D/\\sigma_{\\mathbf{n}}]$。\n$$C_{AA} = \\frac{1}{D} \\int_{0}^{D/\\sigma_{\\mathbf{n}}} \\Phi(x) (\\sigma_{\\mathbf{n}} \\mathrm{d}x) = \\frac{\\sigma_{\\mathbf{n}}}{D} \\int_{0}^{D/\\sigma_{\\mathbf{n}}} \\Phi(x) \\, \\mathrm{d}x$$\n我们使用分部积分法 $\\int u \\, \\mathrm{d}v = uv - \\int v \\, \\mathrm{d}u$，取 $u = \\Phi(x)$ 和 $\\mathrm{d}v = \\mathrm{d}x$。这得到 $\\mathrm{d}u = \\phi(x)\\mathrm{d}x$ 和 $v = x$。不定积分为：\n$$\\int \\Phi(x) \\, \\mathrm{d}x = x\\Phi(x) - \\int x\\phi(x) \\, \\mathrm{d}x$$\n由于 $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-x^2/2)$ 的导数是 $\\phi'(x) = -x\\phi(x)$，所以积分 $\\int x\\phi(x) \\, \\mathrm{d}x = -\\phi(x)$。\n$$\\int \\Phi(x) \\, \\mathrm{d}x = x\\Phi(x) + \\phi(x) + \\text{const.}$$\n应用积分限：\n$$\\int_{0}^{D/\\sigma_{\\mathbf{n}}} \\Phi(x) \\, \\mathrm{d}x = \\left[x\\Phi(x) + \\phi(x)\\right]_{0}^{D/\\sigma_{\\mathbf{n}}} = \\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right)\\right) - \\left(0 \\cdot \\Phi(0) + \\phi(0)\\right)$$\n将此结果代回 $C_{AA}$ 的表达式中：\n$$C_{AA} = \\frac{\\sigma_{\\mathbf{n}}}{D} \\left[ \\frac{D}{\\sigma_{\\mathbf{n}}}\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right]$$\n化简后得到 $C_{AA}$ 的最终表达式：\n$$C_{AA} = \\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)$$\n\n**$C_{BB}$ 的计算（类别B的真阳性率）：**\n$C_{BB}$ 是一个来自真实类别 $B$ 的观测被标记为 $B$ 的概率。其采样分布为，对于 $d \\in [-D, 0)$，有 $p(d | \\text{真实类别 B}) = \\frac{1}{D}$。\n$$C_{BB} = P(d_{\\text{obs}}  0 | \\text{true class B}) = \\frac{1}{D} \\int_{-D}^{0} P(d + \\epsilon_n  0) \\, \\mathrm{d}d$$\n积分内的概率为：\n$$P(d + \\epsilon_n  0) = P(\\epsilon_n  -d) = P\\left(Z  -\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) = \\Phi\\left(-\\frac{d}{\\sigma_{\\mathbf{n}}}\\right)$$\n$C_{BB}$ 的积分为：\n$$C_{BB} = \\frac{1}{D} \\int_{-D}^{0} \\Phi\\left(-\\frac{d}{\\sigma_{\\mathbf{n}}}\\right) \\, \\mathrm{d}d$$\n由于问题设置的对称性，我们预期 $C_{BB} = C_{AA}$。我们可以通过换元来证明这一点。令 $u = -d$，则 $\\mathrm{d}u = -\\mathrm{d}d$。$u$ 的积分限变为 $[D, 0]$。\n$$C_{BB} = \\frac{1}{D} \\int_{D}^{0} \\Phi\\left(\\frac{u}{\\sigma_{\\mathbf{n}}}\\right) (-\\mathrm{d}u) = \\frac{1}{D} \\int_{0}^{D} \\Phi\\left(\\frac{u}{\\sigma_{\\mathbf{n}}}\\right) \\, \\mathrm{d}u$$\n这个积分在形式上与 $C_{AA}$ 的积分相同，因此 $C_{BB} = C_{AA}$。\n\n**最终混淆矩阵：**\n混淆矩阵是对称的，其对角线元素彼此相等，非对角线元素也彼此相等。\n令 $P_{\\text{correct}} = C_{AA} = C_{BB} = \\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)$。\n非对角线元素为 $P_{\\text{error}} = C_{AB} = C_{BA} = 1 - P_{\\text{correct}}$。\n完整的混淆矩阵是：\n$$C(\\Sigma, D, \\mathbf{n}) = \\begin{pmatrix} P_{\\text{correct}}  1 - P_{\\text{correct}} \\\\ 1 - P_{\\text{correct}}  P_{\\text{correct}} \\end{pmatrix}$$\n将 $P_{\\text{correct}}$ 的完整表达式代入，得到最终结果。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)  1 - \\left(\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)\\right) \\\\\n1 - \\left(\\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)\\right)  \\Phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) + \\frac{\\sigma_{\\mathbf{n}}}{D} \\left( \\phi\\left(\\frac{D}{\\sigma_{\\mathbf{n}}}\\right) - \\phi(0) \\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在了解了空间冗余和标签噪声等挑战后，下一个关键问题是如何高效地指导未来的数据采集工作，以最少的成本获得最大的模型性能提升。主动学习（Active Learning）为此提供了一个强大的框架，它旨在智能地选择信息量最丰富的未标记样本进行标注。本练习  结合了理论推导与编程实践，要求你首先推导出一个衡量样本信息量的指标——预期模型更新的大小，然后设计并实现一个贪心算法，该算法在选择高信息量样本的同时，还强制执行空间多样性约束，以确保新样本提供的是新颖而非冗余的信息。",
            "id": "3856366",
            "problem": "考虑一个遥感和环境建模中的二元土地覆盖分类任务，其目标是根据从多光谱影像和地形数据中提取的特征，判断一个位置是否存在植被。您正在规划一个监督式训练数据收集活动，并且必须选择未标记的位置进行实地考察。每个候选的未标记位置都有一个特征向量，该向量从常见的遥感指数和地形度量（如归一化植被指数(NDVI)、归一化差异水体指数(NDWI)、海拔和坡度）中派生得出，以及一个表示其在米制投影坐标系中地理位置的二维坐标。\n\n假设当前有一个逻辑回归模型，其参数向量为 $w \\in \\mathbb{R}^d$，该模型已在一些初步数据上进行了训练。该模型将特征向量为 $x \\in \\mathbb{R}^d$ 的位置存在植被的概率定义为 $P(y=1 \\mid x, w) = \\sigma(w^\\top x)$，其中 $\\sigma(\\cdot)$ 是逻辑S型函数。模型使用交叉熵损失进行训练，并使用在候选位置观察到的标签 $y \\in \\{0,1\\}$，通过学习率为 $\\eta  0$ 的梯度下降法来更新参数。\n\n您的任务是：\n1. 从逻辑回归、逻辑S型函数和交叉熵损失的定义出发，推导在候选位置 $(x, u)$（其中 $u \\in \\mathbb{R}^2$ 是以米为单位的空间坐标）处进行单步梯度下降时，模型参数更新的期望平方范数的表达式。该期望是针对模型当前对该位置未知标签的预测分布计算的。请用纯粹的 $w$、$x$ 和 $\\eta$ 来表示您的结果。\n2. 使用推导出的期望变化作为评分函数，设计并实现一个批量选择程序，该程序选择一组对应于要访问的未标记位置的索引，以在满足空间多样性约束的条件下最大化这些分数的总和：对于任意两个选定的位置，其坐标为 $u_i$ 和 $u_j$，它们的欧几里得距离必须至少为 $D_{\\min}$ 米（即约束为 $||u_i - u_j||_2 \\ge D_{\\min}$）。如果由于空间约束无法选择所需的批量大小，则在不违反约束的情况下选择尽可能多的位置。索引是从零开始的。\n\n您必须编写一个完整的、可运行的程序，该程序：\n- 根据给定的数据以及固定的参数向量 $w$ 和学习率 $\\eta$，为每个候选位置计算期望平方变化得分。\n- 通过按分数降序贪婪地选择一个批次，同时强制执行成对空间多样性约束（距离至少为 $D_{\\min}$ 米）。\n- 处理以下测试套件。对于每个测试用例，将选定的索引输出为一个列表。将所有测试用例的结果汇总到一行：一个逗号分隔的列表，包含三个索引列表，并用方括号括起来（例如，$[ [i_1,i_2], [j_1,j_2,j_3], [k_1] ]$ 必须打印为 $[[i_1,i_2],[j_1,j_2,j_3],[k_1]]$，不带空格）。\n\n对所有测试用例使用以下固定的模型参数和学习率：\n$$\nw = \\begin{bmatrix} 0.15  0.65  -0.45  0.25  0.10 \\end{bmatrix}, \\quad \\eta = 0.05.\n$$\n\n测试用例 $1$（具有中等多样性阈值的一般情况）：\n- 特征维度 $d = 5$，偏差项作为每个特征向量的第一个元素（即第一个特征为 $1$）。八个候选位置的特征和以米为单位的投影坐标如下：\n$$\n\\begin{aligned}\nx_0 = \\begin{bmatrix} 1  0.6  -0.2  0.1  0.05 \\end{bmatrix}, u_0 = \\begin{bmatrix} 1000  1000 \\end{bmatrix}, \\\\\nx_1 = \\begin{bmatrix} 1  0.1  0.3  -0.4  -0.1 \\end{bmatrix}, u_1 = \\begin{bmatrix} 1100  1600 \\end{bmatrix}, \\\\\nx_2 = \\begin{bmatrix} 1  0.9  -0.7  0.5  0.2 \\end{bmatrix}, u_2 = \\begin{bmatrix} 1300  1200 \\end{bmatrix}, \\\\\nx_3 = \\begin{bmatrix} 1  -0.3  0.8  -0.2  0.6 \\end{bmatrix}, u_3 = \\begin{bmatrix} 2000  900 \\end{bmatrix}, \\\\\nx_4 = \\begin{bmatrix} 1  0.2  -0.1  0.0  -0.05 \\end{bmatrix}, u_4 = \\begin{bmatrix} 2100  900 \\end{bmatrix}, \\\\\nx_5 = \\begin{bmatrix} 1  -0.8  0.4  0.3  -0.3 \\end{bmatrix}, u_5 = \\begin{bmatrix} 3000  3000 \\end{bmatrix}, \\\\\nx_6 = \\begin{bmatrix} 1  0.4  0.2  -0.1  0.4 \\end{bmatrix}, u_6 = \\begin{bmatrix} 3200  2900 \\end{bmatrix}, \\\\\nx_7 = \\begin{bmatrix} 1  -0.1  -0.5  0.7  -0.2 \\end{bmatrix}, u_7 = \\begin{bmatrix} 3300  1000 \\end{bmatrix}.\n\\end{aligned}\n$$\n批量大小：\n$$\nK = 3, \\quad D_{\\min} = 150 \\text{ meters}.\n$$\n\n测试用例 $2$（边界情况，其中一些点对的距离恰好在阈值上且被允许）：\n- 六个候选位置：\n$$\n\\begin{aligned}\nx_0 = \\begin{bmatrix} 1  0.7  -0.3  0.2  0.1 \\end{bmatrix}, u_0 = \\begin{bmatrix} 1000  1000 \\end{bmatrix}, \\\\\nx_1 = \\begin{bmatrix} 1  -0.2  0.6  0.1  -0.2 \\end{bmatrix}, u_1 = \\begin{bmatrix} 1200  1000 \\end{bmatrix}, \\\\\nx_2 = \\begin{bmatrix} 1  0.5  0.4  -0.2  0.3 \\end{bmatrix}, u_2 = \\begin{bmatrix} 1500  1400 \\end{bmatrix}, \\\\\nx_3 = \\begin{bmatrix} 1  -0.6  0.1  0.4  -0.1 \\end{bmatrix}, u_3 = \\begin{bmatrix} 1700  1200 \\end{bmatrix}, \\\\\nx_4 = \\begin{bmatrix} 1  0.3  -0.4  0.6  0.0 \\end{bmatrix}, u_4 = \\begin{bmatrix} 2100  1600 \\end{bmatrix}, \\\\\nx_5 = \\begin{bmatrix} 1  -0.1  0.2  -0.5  0.2 \\end{bmatrix}, u_5 = \\begin{bmatrix} 2300  1600 \\end{bmatrix}.\n\\end{aligned}\n$$\n批量大小和多样性阈值：\n$$\nK = 4, \\quad D_{\\min} = 200 \\text{ meters}.\n$$\n\n测试用例 $3$（边缘情况，其中大的多样性阈值导致无法填满整个批次）：\n- 七个候选位置，其中五个形成一个紧密的簇，另外两个相距较远：\n$$\n\\begin{aligned}\nx_0 = \\begin{bmatrix} 1  0.2  0.1  -0.1  0.05 \\end{bmatrix}, u_0 = \\begin{bmatrix} 1000  1000 \\end{bmatrix}, \\\\\nx_1 = \\begin{bmatrix} 1  0.21  0.12  -0.11  0.06 \\end{bmatrix}, u_1 = \\begin{bmatrix} 1020  1010 \\end{bmatrix}, \\\\\nx_2 = \\begin{bmatrix} 1  0.19  0.09  -0.12  0.04 \\end{bmatrix}, u_2 = \\begin{bmatrix} 990  1030 \\end{bmatrix}, \\\\\nx_3 = \\begin{bmatrix} 1  0.18  0.15  -0.10  0.03 \\end{bmatrix}, u_3 = \\begin{bmatrix} 1010  980 \\end{bmatrix}, \\\\\nx_4 = \\begin{bmatrix} 1  0.22  0.11  -0.13  0.07 \\end{bmatrix}, u_4 = \\begin{bmatrix} 980  990 \\end{bmatrix}, \\\\\nx_5 = \\begin{bmatrix} 1  -0.9  0.4  0.2  -0.3 \\end{bmatrix}, u_5 = \\begin{bmatrix} 5000  5000 \\end{bmatrix}, \\\\\nx_6 = \\begin{bmatrix} 1  0.8  -0.6  0.5  0.2 \\end{bmatrix}, u_6 = \\begin{bmatrix} 6000  7000 \\end{bmatrix}.\n\\end{aligned}\n$$\n批量大小和多样性阈值：\n$$\nK = 4, \\quad D_{\\min} = 500 \\text{ meters}.\n$$\n\n程序要求：\n- 实现逻辑S型函数并为每个候选位置计算 $p_i = \\sigma(w^\\top x_i)$。\n- 根据您的推导，为每个候选位置计算期望的参数更新平方范数得分，并用它来对候选位置进行排序。\n- 为每个测试用例选择批次，采用尊重空间约束 $||u_i - u_j||_2 \\ge D_{\\min}$ 的贪婪程序；如果出现分数相同的情况，则通过较小的索引来打破平局。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，不带空格，其中每个元素是针对一个测试用例的选定索引列表，例如：\n$$\n[[i_1,i_2,i_3],[j_1,j_2,j_3,j_4],[k_1,k_2]]\n$$\n索引必须是整数且从零开始。",
            "solution": "该问题需要一个由两部分组成的解决方案：首先，基于新标记点对逻辑回归模型的期望影响，推导出一个评分函数；其次，设计一个贪婪算法来选择一批采样位置，以在空间多样性约束下最大化此分数。\n\n### 第1部分：期望参数更新平方范数的推导\n\n目标是推导在候选位置（特征向量为 $x$）处进行单步梯度下降时，模型参数更新的期望平方范数 $\\mathbb{E}[\\|\\Delta w\\|^2]$ 的表达式。该期望是针对模型当前对未知二元标签 $y \\in \\{0, 1\\}$ 的预测分布计算的。\n\n**1. 逻辑回归模型与损失函数**\n模型预测正类（$y=1$）的概率为 $p = P(y=1 \\mid x, w) = \\sigma(w^\\top x)$，其中 $\\sigma(z) = (1 + e^{-z})^{-1}$ 是逻辑S型函数。负类（$y=0$）的概率是 $1-p$。学习过程旨在最小化单个标记数据点 $(x, y)$ 的二元交叉熵损失：\n$$\nL(w; x, y) = -[y \\log(p) + (1-y) \\log(1-p)]\n$$\n\n**2. 损失函数的梯度**\n为了执行梯度下降，我们需要损失 $L$ 相对于模型参数 $w$ 的梯度。S型函数的一个关键性质是其导数为 $\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1-\\sigma(z))$。令 $z = w^\\top x$。使用链式法则，我们计算梯度 $\\nabla_w L$：\n$$\n\\nabla_w L = \\frac{\\partial L}{\\partial p} \\frac{\\partial p}{\\partial z} \\frac{\\partial z}{\\partial w}\n$$\n各个分量为：\n- $\\frac{\\partial L}{\\partial p} = -\\left(\\frac{y}{p} - \\frac{1-y}{1-p}\\right) = -\\frac{y-p}{p(1-p)}$\n- $\\frac{\\partial p}{\\partial z} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$\n- $\\frac{\\partial z}{\\partial w} = \\frac{\\partial(w^\\top x)}{\\partial w} = x$\n\n将这些项组合起来，得到一个非常简洁的梯度表达式：\n$$\n\\nabla_w L = \\left(-\\frac{y-p}{p(1-p)}\\right) (p(1-p)) (x) = -(y-p)x = (p-y)x\n$$\n\n**3. 梯度下降参数更新**\n单步梯度下降使用学习率 $\\eta  0$ 按如下方式更新参数 $w$：\n$$\nw_{\\text{new}} = w - \\eta \\nabla_w L\n$$\n因此，参数更新向量 $\\Delta w = w_{\\text{new}} - w$ 为：\n$$\n\\Delta w = -\\eta \\nabla_w L = -\\eta (p-y)x = \\eta(y-p)x\n$$\n\n**4. 参数更新的期望平方范数**\n候选位置的标签 $y$ 是未知的。我们计算更新向量的L2范数平方 $\\|\\Delta w\\|^2_2$ 相对于模型对 $y$ 的预测分布的期望。该分布是一个成功概率为 $p$ 的伯努利试验：\n- $y=1$ 的概率为 $p$\n- $y=0$ 的概率为 $1-p$\n\n期望 $\\mathbb{E}_{y \\sim \\text{Bernoulli}(p)}[\\|\\Delta w\\|^2_2]$ 计算如下：\n$$\n\\mathbb{E}[\\|\\Delta w\\|^2_2] = P(y=1) \\cdot \\|\\Delta w|_{y=1}\\|^2_2 + P(y=0) \\cdot \\|\\Delta w|_{y=0}\\|^2_2\n$$\n首先，我们计算每种可能的 $y$ 结果对应的范数平方：\n- 如果 $y=1$：$\\Delta w = \\eta(1-p)x$。则 $\\|\\Delta w\\|^2_2 = \\|\\eta(1-p)x\\|^2_2 = \\eta^2 (1-p)^2 \\|x\\|^2_2$。\n- 如果 $y=0$：$\\Delta w = \\eta(0-p)x = -\\eta px$。则 $\\|\\Delta w\\|^2_2 = \\|-\\eta px\\|^2_2 = \\eta^2 p^2 \\|x\\|^2_2$。\n\n将这些代入期望公式：\n$$\n\\begin{aligned}\n\\mathbb{E}[\\|\\Delta w\\|^2_2] = p \\cdot \\left(\\eta^2 (1-p)^2 \\|x\\|^2_2\\right) + (1-p) \\cdot \\left(\\eta^2 p^2 \\|x\\|^2_2\\right) \\\\\n= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)^2 + (1-p)p^2\\right] \\\\\n= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)( (1-p) + p )\\right] \\\\\n= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)(1)\\right] \\\\\n= \\eta^2 p(1-p) \\|x\\|^2_2\n\\end{aligned}\n$$\n代入 $p = \\sigma(w^\\top x)$，评分函数 $S(x)$ 的最终表达式为：\n$$\nS(x) = \\eta^2 \\sigma(w^\\top x) (1 - \\sigma(w^\\top x)) \\|x\\|^2_2\n$$\n这个分数表示，如果我们在位置 $x$ 采样标签并执行一次梯度更新，模型权重的期望变化平方范数。它结合了模型的不确定性（项 $p(1-p)$ 在 $p=0.5$ 时最大化）和特征向量的范数（$\\|x\\|^2_2$），偏好于那些不确定且具有影响力的点。\n\n### 第2部分：批量选择算法\n\n第二个任务是使用为每个候选位置 $i$ 推导出的分数 $S(x_i)$ 来实现一个批量选择程序。该程序必须选择一批大小为 $K$ 的索引，以最大化分数总和，同时遵守空间多样性约束：任意两个选定的位置 $i$ 和 $j$ 必须由至少 $D_{\\min}$ 的欧几里得距离隔开，即 $\\|u_i - u_j\\|_2 \\ge D_{\\min}$。\n\n指定的算法是一个贪婪选择过程：\n1.  对于每个具有特征向量 $x_i$ 和坐标 $u_i$ 的候选位置 $i$，计算其分数 $S_i = \\eta^2 \\sigma(w^\\top x_i) (1 - \\sigma(w^\\top x_i)) \\|x_i\\|^2_2$。\n2.  创建一个候选索引列表，并根据它们的分数按降序排序。分数相同时，优先选择索引较小的。\n3.  初始化一个空列表 `selected_indices` 用于存放选定的批次。\n4.  遍历排序后的候选索引列表。对于每个候选索引 $j$：\n    a. 检查将位置 $j$ 添加到批次中是否违反空间约束。这通过计算 $u_j$ 与 `selected_indices` 中每个已选位置 $k$ 的坐标 $u_k$ 之间的欧几里得距离来完成。\n    b. 如果对于所有 $k \\in \\text{selected\\_indices}$，距离 $\\|u_j - u_k\\|_2 \\ge D_{\\min}$，则候选者 $j$ 是有效的。\n    c. 如果候选者 $j$ 有效，则将其索引添加到 `selected_indices`。\n    d. 如果 `selected_indices` 的大小达到所需的批量大小 $K$，则终止该过程。\n5.  如果在批次被填满（即大小小于 $K$）之前循环结束，则该过程以部分填充的批次结束。当没有剩余的候选者可以满足与已选位置的空间约束时，会发生这种情况。\n6.  每个测试用例的最终输出是列表 `selected_indices`，其中包含按选择顺序排列的所选位置的从零开始的索引。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the batch selection problem for three test cases.\n    \"\"\"\n    \n    # Fixed model parameter and learning rate for all test cases\n    w = np.array([0.15, 0.65, -0.45, 0.25, 0.10])\n    eta = 0.05\n    \n    test_cases = [\n        {\n            \"K\": 3,\n            \"D_min\": 150.0,\n            \"X\": np.array([\n                [1, 0.6, -0.2, 0.1, 0.05],\n                [1, 0.1, 0.3, -0.4, -0.1],\n                [1, 0.9, -0.7, 0.5, 0.2],\n                [1, -0.3, 0.8, -0.2, 0.6],\n                [1, 0.2, -0.1, 0.0, -0.05],\n                [1, -0.8, 0.4, 0.3, -0.3],\n                [1, 0.4, 0.2, -0.1, 0.4],\n                [1, -0.1, -0.5, 0.7, -0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1100, 1600], [1300, 1200], [2000, 900],\n                [2100, 900], [3000, 3000], [3200, 2900], [3300, 1000]\n            ])\n        },\n        {\n            \"K\": 4,\n            \"D_min\": 200.0,\n            \"X\": np.array([\n                [1, 0.7, -0.3, 0.2, 0.1],\n                [1, -0.2, 0.6, 0.1, -0.2],\n                [1, 0.5, 0.4, -0.2, 0.3],\n                [1, -0.6, 0.1, 0.4, -0.1],\n                [1, 0.3, -0.4, 0.6, 0.0],\n                [1, -0.1, 0.2, -0.5, 0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1200, 1000], [1500, 1400],\n                [1700, 1200], [2100, 1600], [2300, 1600]\n            ])\n        },\n        {\n            \"K\": 4,\n            \"D_min\": 500.0,\n            \"X\": np.array([\n                [1, 0.2, 0.1, -0.1, 0.05],\n                [1, 0.21, 0.12, -0.11, 0.06],\n                [1, 0.19, 0.09, -0.12, 0.04],\n                [1, 0.18, 0.15, -0.10, 0.03],\n                [1, 0.22, 0.11, -0.13, 0.07],\n                [1, -0.9, 0.4, 0.2, -0.3],\n                [1, 0.8, -0.6, 0.5, 0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1020, 1010], [990, 1030],\n                [1010, 980], [980, 990], [5000, 5000], [6000, 7000]\n            ])\n        }\n    ]\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    all_results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        U = case[\"U\"]\n        K = case[\"K\"]\n        D_min = case[\"D_min\"]\n        num_candidates = X.shape[0]\n\n        # 1. Compute scores for all candidates\n        z = X @ w\n        p = sigmoid(z)\n        p_variance = p * (1 - p)\n        x_norm_sq = np.sum(X**2, axis=1)\n        scores = (eta**2) * p_variance * x_norm_sq\n\n        # 2. Sort candidate indices by score (desc) and original index (asc) for tie-breaking\n        candidate_indices = sorted(range(num_candidates), key=lambda i: (-scores[i], i))\n\n        # 3. Greedy selection with spatial constraint\n        selected_indices = []\n        for idx in candidate_indices:\n            if len(selected_indices) >= K:\n                break\n\n            is_valid = True\n            current_u = U[idx]\n            for selected_idx in selected_indices:\n                dist = np.linalg.norm(current_u - U[selected_idx])\n                if dist  D_min:\n                    is_valid = False\n                    break\n            \n            if is_valid:\n                selected_indices.append(idx)\n        \n        all_results.append(selected_indices)\n\n    # Format the output string to match the required format: [[...],[...],...]\n    result_str = \"[\" + \",\".join([str(sublist).replace(\" \", \"\") for sublist in all_results]) + \"]\"\n    print(result_str)\n\n\nsolve()\n```"
        }
    ]
}