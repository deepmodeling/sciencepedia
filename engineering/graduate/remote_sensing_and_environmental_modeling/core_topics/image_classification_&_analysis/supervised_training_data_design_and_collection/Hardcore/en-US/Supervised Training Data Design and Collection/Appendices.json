{
    "hands_on_practices": [
        {
            "introduction": "A common pitfall in remote sensing is to equate the number of pixels with the sample size. However, strong spatial autocorrelation means that nearby pixels provide redundant information. This exercise introduces the crucial geostatistical concept of \"effective sample size\" ($n_{\\text{eff}}$), which corrects for this redundancy . By deriving how $n_{\\text{eff}}$ scales with the domain area and the correlation length of the data, you will develop a quantitative intuition for the true informational content of spatially continuous data.",
            "id": "3856352",
            "problem": "You are designing a supervised training dataset for a remote sensing regression task over a two-dimensional domain with area $A$ (for example, an image mosaic covering a large region). Let $Z(\\mathbf{s})$ denote a second-order stationary and isotropic random field representing the target variable (for example, soil moisture) at spatial location $\\mathbf{s} \\in \\mathbb{R}^{2}$, with mean $\\mu$ and covariance function $C(h)=\\sigma^{2}\\exp(-h/\\phi)$, where $h$ is the separation distance, $\\sigma^{2}$ is the marginal variance, and $\\phi$ is the correlation length. You collect labels at a very high and approximately uniform spatial density (for example, all pixels at a resolution much finer than $\\phi$), and you will use them as if they were independent when training.\n\nDefine the effective sample size $n_{\\mathrm{eff}}$ of your training set as the size of an independent and identically distributed sample from a random variable with variance $\\sigma^{2}$ that yields the same variance for the spatial areal average as your actual, spatially correlated training data. Starting from fundamental definitions for second-order stationary random fields and the variance of an areal average, and assuming $A \\gg \\phi^{2}$ so that edge effects are negligible, derive a closed-form analytic expression for $n_{\\mathrm{eff}}$ in terms of $A$ and $\\phi$. Conclude from your expression how $n_{\\mathrm{eff}}$ scales with the domain size and the correlation length. Provide only the final analytic expression for $n_{\\mathrm{eff}}$ as your answer. No numerical substitution is required, and no rounding is needed.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of geostatistics and random field theory, well-posed with sufficient information and a clear objective, and expressed in precise, objective language.\n\nThe objective is to derive an expression for the effective sample size, $n_{\\mathrm{eff}}$, for a spatially correlated dataset over a two-dimensional domain of area $A$. The random field $Z(\\mathbf{s})$ representing the variable of interest is second-order stationary and isotropic, with a mean $\\mu$ and an exponential covariance function $C(h) = \\sigma^{2}\\exp(-h/\\phi)$, where $h$ is the Euclidean distance, $\\sigma^2$ is the variance, and $\\phi$ is the correlation length.\n\nThe effective sample size $n_{\\mathrm{eff}}$ is defined as the size of a hypothetical independent and identically distributed (i.i.d.) sample that would yield the same variance for its sample mean as the variance of the spatial areal average of the correlated field $Z(\\mathbf{s})$.\n\nFirst, consider a set of $n$ i.i.d. random variables, $\\{X_1, X_2, \\ldots, X_n\\}$, each with variance $\\sigma^2$. The variance of their sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i$, is given by:\n$$ \\mathrm{Var}(\\bar{X}) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(X_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n} $$\nBy the definition provided in the problem, for an effective sample size of $n_{\\mathrm{eff}}$, this variance is $\\frac{\\sigma^2}{n_{\\mathrm{eff}}}$.\n\nNext, we must calculate the variance of the spatial areal average of the random field $Z(\\mathbf{s})$ over the domain, which we denote by $D$, with area $A = \\int_D d\\mathbf{s}$. The spatial areal average, $\\bar{Z}_A$, is defined as:\n$$ \\bar{Z}_A = \\frac{1}{A} \\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s} $$\nThe variance of $\\bar{Z}_A$ is:\n$$ \\mathrm{Var}(\\bar{Z}_A) = \\mathrm{Var}\\left(\\frac{1}{A} \\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) = \\frac{1}{A^2} \\mathrm{Var}\\left(\\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) $$\nFor a second-order stationary random field, the variance of the integral is given by the double integral of the covariance function over the domain:\n$$ \\mathrm{Var}\\left(\\int_{D} Z(\\mathbf{s}) \\, d\\mathbf{s}\\right) = \\int_{D} \\int_{D} \\mathrm{Cov}(Z(\\mathbf{s}_1), Z(\\mathbf{s}_2)) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 $$\nThe covariance is given by $C(\\|\\mathbf{s}_1 - \\mathbf{s}_2\\|)$. Thus,\n$$ \\mathrm{Var}(\\bar{Z}_A) = \\frac{1}{A^2} \\int_{D} \\int_{D} C(\\|\\mathbf{s}_1 - \\mathbf{s}_2\\|) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 $$\nEvaluating this four-dimensional integral is generally complex. However, the problem states the assumption that the area of the domain $A$ is much larger than the square of the correlation length, $A \\gg \\phi^2$. This implies that the spatial extent of the domain is much larger than the range over which data points are significantly correlated. This allows us to neglect edge effects.\n\nWe can simplify the integral by a change of variables. Let $\\mathbf{h} = \\mathbf{s}_2 - \\mathbf{s}_1$. The integral becomes:\n$$ \\int_{D} \\int_{D} C(\\|\\mathbf{s}_2 - \\mathbf{s}_1\\|) \\, d\\mathbf{s}_1 \\, d\\mathbf{s}_2 = \\int_{D} \\left( \\int_{D-\\mathbf{s}_1} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\right) d\\mathbf{s}_1 $$\nwhere $D-\\mathbf{s}_1$ is the domain $D$ shifted by the vector $-\\mathbf{s}_1$. For any point $\\mathbf{s}_1$ that is not close to the boundary of $D$ (relative to $\\phi$), the covariance function $C(\\|\\mathbf{h}\\|)$ will decay to approximately zero well within the integration domain $D-\\mathbf{s}_1$. Given the assumption $A \\gg \\phi^2$, the contribution from points $\\mathbf{s}_1$ near the boundary is negligible. We can therefore approximate the inner integral by extending its domain to all of $\\mathbb{R}^2$:\n$$ \\int_{D-\\mathbf{s}_1} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\approx \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\nThis approximation makes the inner integral independent of $\\mathbf{s}_1$. The outer integral then becomes:\n$$ \\int_{D} \\left(\\ldots\\right) d\\mathbf{s}_1 \\approx A \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\nSubstituting this back into the expression for the variance of the areal average:\n$$ \\mathrm{Var}(\\bar{Z}_A) \\approx \\frac{1}{A^2} \\left( A \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} \\right) = \\frac{1}{A} \\int_{\\mathbb{R}^2} C(\\|\\mathbf{h}\\|) \\, d\\mathbf{h} $$\nNow we must evaluate the integral of the given covariance function, $C(h) = \\sigma^2 \\exp(-h/\\phi)$, over $\\mathbb{R}^2$. Since the function is isotropic, we convert to polar coordinates $(h, \\theta)$, where $h$ is the radial distance and $d\\mathbf{h} = h \\, dh \\, d\\theta$.\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = \\int_{0}^{2\\pi} \\int_{0}^{\\infty} \\sigma^2 \\exp(-h/\\phi) \\, h \\, dh \\, d\\theta $$\nThe integral over $\\theta$ from $0$ to $2\\pi$ yields a factor of $2\\pi$:\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = 2\\pi\\sigma^2 \\int_{0}^{\\infty} h \\exp(-h/\\phi) \\, dh $$\nWe solve the remaining integral using integration by parts, with $u=h$ and $dv = \\exp(-h/\\phi) \\, dh$. This gives $du=dh$ and $v = -\\phi \\exp(-h/\\phi)$.\n\\begin{align*}\n\\int_{0}^{\\infty} h \\exp(-h/\\phi) \\, dh = \\left[ -h\\phi\\exp(-h/\\phi) \\right]_0^{\\infty} - \\int_{0}^{\\infty} (-\\phi) \\exp(-h/\\phi) \\, dh \\\\\n= (0 - 0) + \\phi \\int_{0}^{\\infty} \\exp(-h/\\phi) \\, dh \\\\\n= \\phi \\left[ -\\phi\\exp(-h/\\phi) \\right]_0^{\\infty} \\\\\n= \\phi (0 - (-\\phi \\exp(0))) \\\\\n= \\phi^2\n\\end{align*}\nTherefore, the integral of the covariance function is:\n$$ \\int_{\\mathbb{R}^2} C(h) \\, d\\mathbf{h} = 2\\pi\\sigma^2\\phi^2 $$\nSubstituting this result into our expression for $\\mathrm{Var}(\\bar{Z}_A)$:\n$$ \\mathrm{Var}(\\bar{Z}_A) \\approx \\frac{2\\pi\\sigma^2\\phi^2}{A} $$\nFinally, we equate this with the variance for an i.i.d. sample of effective size $n_{\\mathrm{eff}}$:\n$$ \\frac{\\sigma^2}{n_{\\mathrm{eff}}} = \\frac{2\\pi\\sigma^2\\phi^2}{A} $$\nAssuming $\\sigma^2 \\neq 0$, we can cancel $\\sigma^2$ from both sides:\n$$ \\frac{1}{n_{\\mathrm{eff}}} = \\frac{2\\pi\\phi^2}{A} $$\nSolving for $n_{\\mathrm{eff}}$ gives the closed-form expression:\n$$ n_{\\mathrm{eff}} = \\frac{A}{2\\pi\\phi^2} $$\nFrom this expression, we conclude that the effective sample size $n_{\\mathrm{eff}}$ scales linearly with the domain area $A$ ($n_{\\mathrm{eff}} \\propto A$) and inversely with the square of the correlation length $\\phi$ ($n_{\\mathrm{eff}} \\propto \\phi^{-2}$). The term $2\\pi\\phi^2$ can be interpreted as the effective area of a single independent sample.",
            "answer": "$$\\boxed{\\frac{A}{2\\pi\\phi^{2}}}$$"
        },
        {
            "introduction": "Designing a data collection campaign requires balancing costs, benefits, and logistical constraints. Not all classification errors are equally consequential, and some data are more expensive to acquire than others. This practice guides you through a formal optimization problem to create a cost-sensitive sampling plan . You will learn to allocate a fixed budget across different land-cover classes to minimize the expected financial or ecological risk, a vital skill for maximizing the impact of limited resources.",
            "id": "3856398",
            "problem": "A national wetland monitoring program uses supervised classification of multispectral satellite images to identify ephemeral wetlands and non-wetland background. The program wishes to design a cost-sensitive training data collection plan that maximizes expected utility for the final operational map under asymmetric misclassification costs and limited field-labeling resources. The following scenario is scientifically realistic and self-consistent.\n\nConsider two land-cover classes: wetlands and background. Let $p_{w} \\in (0,1)$ denote the prevalence of wetlands in the area of interest and $p_{b} = 1 - p_{w}$ the prevalence of background. Let $C_{w} > 0$ denote the per-pixel cost incurred when a wetland pixel is misclassified (false negative), and let $C_{b} > 0$ denote the per-pixel cost incurred when a background pixel is misclassified (false positive). Assume the operational classifier’s class-conditional generalization error rates for wetlands and background, denoted $e_{w}$ and $e_{b}$ respectively, decrease with the number of class-specific training labels according to an empirically observed learning curve of inverse proportionality:\n$$\ne_{w}(n_{w}) = \\frac{\\alpha_{w}}{n_{w}}, \\quad e_{b}(n_{b}) = \\frac{\\alpha_{b}}{n_{b}},\n$$\nfor $n_{w} \\ge 1$ and $n_{b} \\ge 1$, where $\\alpha_{w} > 0$ and $\\alpha_{b} > 0$ are constants that reflect class complexity, separability, and sensor noise. Each labeled training sample for wetlands costs $k_{w} > 0$ units of field effort and each labeled training sample for background costs $k_{b} > 0$ units, with a fixed labeling budget $B > 0$ that imposes the constraint\n$$\nk_{w} n_{w} + k_{b} n_{b} = B.\n$$\n\nDefine expected utility as the negative of the expected misclassification cost over the area, so maximizing expected utility is equivalent to minimizing the expected risk\n$$\nR(n_{w}, n_{b}) = p_{w} C_{w} e_{w}(n_{w}) + p_{b} C_{b} e_{b}(n_{b}).\n$$\n\nUsing only these definitions and the constraint above, formulate the constrained optimization problem that maximizes expected utility and derive, from first principles, the closed-form expression for the optimal sampling ratio $n_{w}/n_{b}$ that allocates training labels between wetlands and background under the given asymmetric misclassification costs and class prevalences. Treat $n_{w}$ and $n_{b}$ as positive real decision variables. Express your final answer as a single analytic expression for $n_{w}/n_{b}$ in terms of $p_{w}$, $p_{b}$, $C_{w}$, $C_{b}$, $\\alpha_{w}$, $\\alpha_{b}$, $k_{w}$, and $k_{b}$. No rounding is required, and no units should be included in the final answer.",
            "solution": "The task is to design a sampling plan that maximizes expected utility under asymmetric misclassification costs and a fixed labeling budget. By the definition of expected utility as the negative of expected misclassification cost, maximizing utility is equivalent to minimizing the expected risk. The foundational base comprises:\n\n- The definition of expected risk for a two-class classification problem under class prevalences and per-pixel misclassification costs,\n- The empirical learning curve relation that class-conditional generalization error decreases approximately as the inverse of the number of class-specific training examples,\n- The linear budget constraint on labeling costs.\n\nLet $n_{w}  0$ and $n_{b}  0$ denote the numbers of labeled training samples collected for wetlands and background, respectively. The expected risk, by definition of class prevalences and costs, is\n$$\nR(n_{w}, n_{b}) = p_{w} C_{w} e_{w}(n_{w}) + p_{b} C_{b} e_{b}(n_{b}).\n$$\nUnder the stated learning curve model,\n$$\ne_{w}(n_{w}) = \\frac{\\alpha_{w}}{n_{w}}, \\quad e_{b}(n_{b}) = \\frac{\\alpha_{b}}{n_{b}}.\n$$\nTherefore, the expected risk becomes\n$$\nR(n_{w}, n_{b}) = p_{w} C_{w} \\frac{\\alpha_{w}}{n_{w}} + p_{b} C_{b} \\frac{\\alpha_{b}}{n_{b}}.\n$$\n\nThe labeling budget constraint is\n$$\nk_{w} n_{w} + k_{b} n_{b} = B.\n$$\n\nWe seek to minimize $R(n_{w}, n_{b})$ subject to this equality constraint. This is a convex optimization problem because each term $p_{i} C_{i} \\alpha_{i}/n_{i}$ is convex in $n_{i}  0$ and the constraint is linear. The method of Lagrange multipliers yields the necessary and sufficient conditions for optimality.\n\nForm the Lagrangian\n$$\n\\mathcal{L}(n_{w}, n_{b}, \\lambda) = p_{w} C_{w} \\frac{\\alpha_{w}}{n_{w}} + p_{b} C_{b} \\frac{\\alpha_{b}}{n_{b}} + \\lambda \\left( k_{w} n_{w} + k_{b} n_{b} - B \\right).\n$$\n\nTake partial derivatives with respect to $n_{w}$ and $n_{b}$ and set them equal to zero at an interior optimum:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{w}} = - p_{w} C_{w} \\frac{\\alpha_{w}}{n_{w}^{2}} + \\lambda k_{w} = 0,\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{b}} = - p_{b} C_{b} \\frac{\\alpha_{b}}{n_{b}^{2}} + \\lambda k_{b} = 0.\n$$\n\nSolving each for $\\lambda$ gives\n$$\n\\lambda = \\frac{p_{w} C_{w} \\alpha_{w}}{k_{w} n_{w}^{2}} = \\frac{p_{b} C_{b} \\alpha_{b}}{k_{b} n_{b}^{2}}.\n$$\n\nEquating the two expressions yields a relation between $n_{w}$ and $n_{b}$:\n$$\n\\frac{p_{w} C_{w} \\alpha_{w}}{k_{w} n_{w}^{2}} = \\frac{p_{b} C_{b} \\alpha_{b}}{k_{b} n_{b}^{2}}.\n$$\n\nRearrange to isolate the ratio $n_{w}/n_{b}$:\n$$\n\\frac{n_{w}^{2}}{n_{b}^{2}} = \\frac{p_{w} C_{w} \\alpha_{w} k_{b}}{p_{b} C_{b} \\alpha_{b} k_{w}} \\quad \\Longrightarrow \\quad \\frac{n_{w}}{n_{b}} = \\sqrt{ \\frac{p_{w} C_{w} \\alpha_{w} k_{b}}{p_{b} C_{b} \\alpha_{b} k_{w}} }.\n$$\n\nThis expression is the optimal sampling ratio that balances the marginal reduction in expected misclassification cost per unit labeling effort between the two classes. It increases with the wetland prevalence $p_{w}$, wetland misclassification cost $C_{w}$, wetland learning curve coefficient $\\alpha_{w}$, and background sample cost $k_{b}$; it decreases with the background prevalence $p_{b}$, background misclassification cost $C_{b}$, background learning curve coefficient $\\alpha_{b}$, and wetland sample cost $k_{w}$. The budget $B$ determines absolute sample sizes but cancels from the ratio, so the ratio is invariant to the total budget as long as the interior solution with $n_{w}, n_{b}  0$ is feasible.\n\nThus, the optimal cost-sensitive sampling plan under the given assumptions allocates training labels between wetlands and background according to the closed-form ratio derived above.",
            "answer": "$$\\boxed{\\sqrt{\\frac{p_{w} C_{w} \\alpha_{w} k_{b}}{p_{b} C_{b} \\alpha_{b} k_{w}}}}$$"
        },
        {
            "introduction": "Beyond static sampling plans, modern data collection can be adaptive, intelligently selecting the most informative locations to label. This hands-on coding exercise introduces a powerful active learning strategy based on expected model impact . You will first derive a score to quantify how much a potential new label is expected to change the model, then implement a practical algorithm to select a batch of sampling sites that are not only highly informative but also spatially diverse, reflecting real-world field survey constraints.",
            "id": "3856366",
            "problem": "Consider a binary land-cover classification task in remote sensing and environmental modeling, where the goal is to decide whether vegetation is present at a location based on features derived from multispectral imagery and terrain data. You are planning a supervised training data collection campaign and must select unlabeled locations to visit. Each candidate unlabeled location has a feature vector derived from common remote sensing indices and terrain metrics, such as the Normalized Difference Vegetation Index (NDVI), the Normalized Difference Water Index (NDWI), elevation and slope, and a two-dimensional coordinate representing its geographic position in a projected coordinate system in meters.\n\nAssume a logistic regression model with parameter vector $w \\in \\mathbb{R}^d$ is currently trained on some preliminary data. The model defines the probability of vegetation presence at feature vector $x \\in \\mathbb{R}^d$ as $P(y=1 \\mid x, w) = \\sigma(w^\\top x)$, where $\\sigma(\\cdot)$ is the logistic sigmoid function. The model is trained using cross-entropy loss, and parameters are updated by gradient descent with learning rate $\\eta  0$ using an observed label $y \\in \\{0,1\\}$ at a candidate location.\n\nYour tasks are:\n1. Starting from the definitions of logistic regression, the logistic sigmoid function, and cross-entropy loss, derive the expression for the expected squared magnitude of the model parameter update for a single gradient descent step at a candidate location $(x, u)$, where $u \\in \\mathbb{R}^2$ are the spatial coordinates in meters. The expectation is taken with respect to the model’s current predictive distribution over the unknown label at that location. Express your result purely in terms of $w$, $x$, and $\\eta$.\n2. Using the derived expected change as a scoring function, design and implement a batch selection procedure that chooses a set of indices corresponding to unlabeled locations to visit that maximizes the sum of these scores subject to a spatial diversity constraint: for any two selected locations with coordinates $u_i$ and $u_j$, their Euclidean distance must be at least $D_{\\min}$ meters (that is, the constraint is $||u_i - u_j||_2 \\ge D_{\\min}$). If it is not possible to select the desired batch size due to the spatial constraint, select as many as possible without violating the constraint. Indices are zero-based.\n\nYou must write a complete, runnable program that:\n- Computes the expected squared change score for each candidate location from the given data and a fixed parameter vector $w$ and learning rate $\\eta$.\n- Selects a batch greedily by descending score while enforcing the pairwise spatial diversity constraint (distance at least $D_{\\min}$ meters).\n- Processes the following test suite. For each test case, output the selected indices as a list. Aggregate the results for all test cases into a single line: a comma-separated list of the three index lists enclosed in square brackets (for example, $[ [i_1,i_2], [j_1,j_2,j_3], [k_1] ]$ must be printed as $[[i_1,i_2],[j_1,j_2,j_3],[k_1]]$ with no spaces).\n\nUse the following fixed model parameter and learning rate for all test cases:\n$$\nw = \\begin{bmatrix} 0.15  0.65  -0.45  0.25  0.10 \\end{bmatrix}, \\quad \\eta = 0.05.\n$$\n\nTest Case $1$ (general case with a moderate diversity threshold):\n- Feature dimension $d = 5$ with bias included as the first element of each feature vector (that is, the first feature is $1$). Eight candidate locations with features and projected coordinates in meters:\n$$\n\\begin{aligned}\nx_0 = \\begin{bmatrix} 1  0.6  -0.2  0.1  0.05 \\end{bmatrix},  u_0 = \\begin{bmatrix} 1000  1000 \\end{bmatrix}, \\\\\nx_1 = \\begin{bmatrix} 1  0.1  0.3  -0.4  -0.1 \\end{bmatrix},  u_1 = \\begin{bmatrix} 1100  1600 \\end{bmatrix}, \\\\\nx_2 = \\begin{bmatrix} 1  0.9  -0.7  0.5  0.2 \\end{bmatrix},  u_2 = \\begin{bmatrix} 1300  1200 \\end{bmatrix}, \\\\\nx_3 = \\begin{bmatrix} 1  -0.3  0.8  -0.2  0.6 \\end{bmatrix},  u_3 = \\begin{bmatrix} 2000  900 \\end{bmatrix}, \\\\\nx_4 = \\begin{bmatrix} 1  0.2  -0.1  0.0  -0.05 \\end{bmatrix},  u_4 = \\begin{bmatrix} 2100  900 \\end{bmatrix}, \\\\\nx_5 = \\begin{bmatrix} 1  -0.8  0.4  0.3  -0.3 \\end{bmatrix},  u_5 = \\begin{bmatrix} 3000  3000 \\end{bmatrix}, \\\\\nx_6 = \\begin{bmatrix} 1  0.4  0.2  -0.1  0.4 \\end{bmatrix},  u_6 = \\begin{bmatrix} 3200  2900 \\end{bmatrix}, \\\\\nx_7 = \\begin{bmatrix} 1  -0.1  -0.5  0.7  -0.2 \\end{bmatrix},  u_7 = \\begin{bmatrix} 3300  1000 \\end{bmatrix}.\n\\end{aligned}\n$$\nBatch size:\n$$\nK = 3, \\quad D_{\\min} = 150 \\text{ meters}.\n$$\n\nTest Case $2$ (boundary case where some pairs are exactly at the threshold and are permitted):\n- Six candidate locations:\n$$\n\\begin{aligned}\nx_0 = \\begin{bmatrix} 1  0.7  -0.3  0.2  0.1 \\end{bmatrix},  u_0 = \\begin{bmatrix} 1000  1000 \\end{bmatrix}, \\\\\nx_1 = \\begin{bmatrix} 1  -0.2  0.6  0.1  -0.2 \\end{bmatrix},  u_1 = \\begin{bmatrix} 1200  1000 \\end{bmatrix}, \\\\\nx_2 = \\begin{bmatrix} 1  0.5  0.4  -0.2  0.3 \\end{bmatrix},  u_2 = \\begin{bmatrix} 1500  1400 \\end{bmatrix}, \\\\\nx_3 = \\begin{bmatrix} 1  -0.6  0.1  0.4  -0.1 \\end{bmatrix},  u_3 = \\begin{bmatrix} 1700  1200 \\end{bmatrix}, \\\\\nx_4 = \\begin{bmatrix} 1  0.3  -0.4  0.6  0.0 \\end{bmatrix},  u_4 = \\begin{bmatrix} 2100  1600 \\end{bmatrix}, \\\\\nx_5 = \\begin{bmatrix} 1  -0.1  0.2  -0.5  0.2 \\end{bmatrix},  u_5 = \\begin{bmatrix} 2300  1600 \\end{bmatrix}.\n\\end{aligned}\n$$\nBatch size and diversity threshold:\n$$\nK = 4, \\quad D_{\\min} = 200 \\text{ meters}.\n$$\n\nTest Case $3$ (edge case where a large diversity threshold prevents filling the full batch):\n- Seven candidate locations, five forming a tight cluster and two far apart:\n$$\n\\begin{aligned}\nx_0 = \\begin{bmatrix} 1  0.2  0.1  -0.1  0.05 \\end{bmatrix},  u_0 = \\begin{bmatrix} 1000  1000 \\end{bmatrix}, \\\\\nx_1 = \\begin{bmatrix} 1  0.21  0.12  -0.11  0.06 \\end{bmatrix},  u_1 = \\begin{bmatrix} 1020  1010 \\end{bmatrix}, \\\\\nx_2 = \\begin{bmatrix} 1  0.19  0.09  -0.12  0.04 \\end{bmatrix},  u_2 = \\begin{bmatrix} 990  1030 \\end{bmatrix}, \\\\\nx_3 = \\begin{bmatrix} 1  0.18  0.15  -0.10  0.03 \\end{bmatrix},  u_3 = \\begin{bmatrix} 1010  980 \\end{bmatrix}, \\\\\nx_4 = \\begin{bmatrix} 1  0.22  0.11  -0.13  0.07 \\end{bmatrix},  u_4 = \\begin{bmatrix} 980  990 \\end{bmatrix}, \\\\\nx_5 = \\begin{bmatrix} 1  -0.9  0.4  0.2  -0.3 \\end{bmatrix},  u_5 = \\begin{bmatrix} 5000  5000 \\end{bmatrix}, \\\\\nx_6 = \\begin{bmatrix} 1  0.8  -0.6  0.5  0.2 \\end{bmatrix},  u_6 = \\begin{bmatrix} 6000  7000 \\end{bmatrix}.\n\\end{aligned}\n$$\nBatch size and diversity threshold:\n$$\nK = 4, \\quad D_{\\min} = 500 \\text{ meters}.\n$$\n\nProgram requirements:\n- Implement the logistic sigmoid and compute $p_i = \\sigma(w^\\top x_i)$ for each candidate.\n- Compute the expected squared parameter update magnitude score for each candidate based on your derivation and use it to rank candidates.\n- Select batches for each test case with the greedy procedure that respects the spatial constraint $||u_i - u_j||_2 \\ge D_{\\min}$; if a tie on scores occurs, break ties by smaller index.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each element is a list of selected indices for one test case, for example:\n$$\n[[i_1,i_2,i_3],[j_1,j_2,j_3,j_4],[k_1,k_2]]\n$$\nIndices must be integers and zero-based.",
            "solution": "The problem requires a two-part solution: first, the derivation of a scoring function based on the expected impact of a new labeled point on a logistic regression model, and second, the design of a greedy algorithm to select a batch of sampling locations that maximizes this score under a spatial diversity constraint.\n\n### Part 1: Derivation of the Expected Squared Parameter Update\n\nThe goal is to derive the expression for the expected squared magnitude of the model parameter update for a single gradient descent step, $\\mathbb{E}[\\|\\Delta w\\|^2]$, at a candidate location with feature vector $x$. The expectation is over the model's current predictive distribution for the unknown binary label $y \\in \\{0, 1\\}$.\n\n**1. Logistic Regression Model and Loss Function**\nThe model predicts the probability of the positive class ($y=1$) as $p = P(y=1 \\mid x, w) = \\sigma(w^\\top x)$, where $\\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic sigmoid function. The probability of the negative class ($y=0$) is $1-p$. The learning process aims to minimize the binary cross-entropy loss for a single labeled data point $(x, y)$:\n$$\nL(w; x, y) = -[y \\log(p) + (1-y) \\log(1-p)]\n$$\n\n**2. Gradient of the Loss Function**\nTo perform gradient descent, we need the gradient of the loss $L$ with respect to the model parameters $w$. A key property of the sigmoid function is that its derivative is $\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1-\\sigma(z))$. Let $z = w^\\top x$. Using the chain rule, we compute the gradient $\\nabla_w L$:\n$$\n\\nabla_w L = \\frac{\\partial L}{\\partial p} \\frac{\\partial p}{\\partial z} \\frac{\\partial z}{\\partial w}\n$$\nThe components are:\n- $\\frac{\\partial L}{\\partial p} = -\\left(\\frac{y}{p} - \\frac{1-y}{1-p}\\right) = -\\frac{y-p}{p(1-p)}$\n- $\\frac{\\partial p}{\\partial z} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$\n- $\\frac{\\partial z}{\\partial w} = \\frac{\\partial(w^\\top x)}{\\partial w} = x$\n\nCombining these terms yields the remarkably simple expression for the gradient:\n$$\n\\nabla_w L = \\left(-\\frac{y-p}{p(1-p)}\\right) (p(1-p)) (x) = -(y-p)x = (p-y)x\n$$\n\n**3. Gradient Descent Parameter Update**\nA single step of gradient descent updates the parameters $w$ with a learning rate $\\eta > 0$ as follows:\n$$\nw_{\\text{new}} = w - \\eta \\nabla_w L\n$$\nThe parameter update vector, $\\Delta w = w_{\\text{new}} - w$, is therefore:\n$$\n\\Delta w = -\\eta \\nabla_w L = -\\eta (p-y)x = \\eta(y-p)x\n$$\n\n**4. Expected Squared Magnitude of the Parameter Update**\nThe label $y$ for a candidate location is unknown. We take the expectation of the squared L2-norm of the update vector, $\\|\\Delta w\\|^2_2$, with respect to the model's predictive distribution for $y$. This distribution is a Bernoulli trial with success probability $p$:\n- $y=1$ with probability $p$\n- $y=0$ with probability $1-p$\n\nThe expectation $\\mathbb{E}_{y \\sim \\text{Bernoulli}(p)}[\\|\\Delta w\\|^2_2]$ is calculated as:\n$$\n\\mathbb{E}[\\|\\Delta w\\|^2_2] = P(y=1) \\cdot \\|\\Delta w|_{y=1}\\|^2_2 + P(y=0) \\cdot \\|\\Delta w|_{y=0}\\|^2_2\n$$\nFirst, we find the squared norm for each possible outcome of $y$:\n- If $y=1$: $\\Delta w = \\eta(1-p)x$. Then $\\|\\Delta w\\|^2_2 = \\|\\eta(1-p)x\\|^2_2 = \\eta^2 (1-p)^2 \\|x\\|^2_2$.\n- If $y=0$: $\\Delta w = \\eta(0-p)x = -\\eta px$. Then $\\|\\Delta w\\|^2_2 = \\|-\\eta px\\|^2_2 = \\eta^2 p^2 \\|x\\|^2_2$.\n\nSubstituting these into the expectation formula:\n$$\n\\begin{aligned}\n\\mathbb{E}[\\|\\Delta w\\|^2_2] = p \\cdot \\left(\\eta^2 (1-p)^2 \\|x\\|^2_2\\right) + (1-p) \\cdot \\left(\\eta^2 p^2 \\|x\\|^2_2\\right) \\\\\n= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)^2 + (1-p)p^2\\right] \\\\\n= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)( (1-p) + p )\\right] \\\\\n= \\eta^2 \\|x\\|^2_2 \\left[p(1-p)(1)\\right] \\\\\n= \\eta^2 p(1-p) \\|x\\|^2_2\n\\end{aligned}\n$$\nSubstituting $p = \\sigma(w^\\top x)$, the final expression for the scoring function $S(x)$ is:\n$$\nS(x) = \\eta^2 \\sigma(w^\\top x) (1 - \\sigma(w^\\top x)) \\|x\\|^2_2\n$$\nThis score represents the expected squared magnitude of the change in model weights if we were to sample the label at location $x$ and perform one gradient update. It combines model uncertainty (the term $p(1-p)$ is maximized at $p=0.5$) with the feature vector's magnitude ($\\|x\\|^2_2$), favoring uncertain points that are also influential.\n\n### Part 2: Batch Selection Algorithm\n\nThe second task is to implement a batch selection procedure using the derived score $S(x_i)$ for each candidate location $i$. The procedure must select a batch of $K$ indices that maximizes the sum of scores while adhering to a spatial diversity constraint: any two selected locations $i$ and $j$ must be separated by a Euclidean distance of at least $D_{\\min}$, i.e., $\\|u_i - u_j\\|_2 \\ge D_{\\min}$.\n\nThe specified algorithm is a greedy selection process:\n1.  For each candidate location $i$ with feature vector $x_i$ and coordinates $u_i$, compute its score $S_i = \\eta^2 \\sigma(w^\\top x_i) (1 - \\sigma(w^\\top x_i)) \\|x_i\\|^2_2$.\n2.  Create a list of candidate indices and sort them in descending order based on their scores. Ties in scores are broken by choosing the smaller index first.\n3.  Initialize an empty list for the selected batch, `selected_indices`.\n4.  Iterate through the sorted list of candidate indices. For each candidate index $j$:\n    a. Check if adding location $j$ to the batch violates the spatial constraint. This is done by calculating the Euclidean distance from $u_j$ to the coordinates $u_k$ of every location $k$ already in `selected_indices`.\n    b. If for all $k \\in \\text{selected\\_indices}$, the distance $\\|u_j - u_k\\|_2 \\ge D_{\\min}$, then candidate $j$ is valid.\n    c. If candidate $j$ is valid, add its index to `selected_indices`.\n    d. If the size of `selected_indices` reaches the desired batch size $K$, terminate the process.\n5.  If the loop finishes before the batch is full (i.e., size less than $K$), the procedure concludes with the partially filled batch. This happens when no remaining candidates can satisfy the spatial constraint with the already-selected locations.\n6.  The final output for each test case is the list `selected_indices` containing the zero-based indices of the chosen locations in the order they were selected.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the batch selection problem for three test cases.\n    \"\"\"\n    \n    # Fixed model parameter and learning rate for all test cases\n    w = np.array([0.15, 0.65, -0.45, 0.25, 0.10])\n    eta = 0.05\n    \n    test_cases = [\n        {\n            \"K\": 3,\n            \"D_min\": 150.0,\n            \"X\": np.array([\n                [1, 0.6, -0.2, 0.1, 0.05],\n                [1, 0.1, 0.3, -0.4, -0.1],\n                [1, 0.9, -0.7, 0.5, 0.2],\n                [1, -0.3, 0.8, -0.2, 0.6],\n                [1, 0.2, -0.1, 0.0, -0.05],\n                [1, -0.8, 0.4, 0.3, -0.3],\n                [1, 0.4, 0.2, -0.1, 0.4],\n                [1, -0.1, -0.5, 0.7, -0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1100, 1600], [1300, 1200], [2000, 900],\n                [2100, 900], [3000, 3000], [3200, 2900], [3300, 1000]\n            ])\n        },\n        {\n            \"K\": 4,\n            \"D_min\": 200.0,\n            \"X\": np.array([\n                [1, 0.7, -0.3, 0.2, 0.1],\n                [1, -0.2, 0.6, 0.1, -0.2],\n                [1, 0.5, 0.4, -0.2, 0.3],\n                [1, -0.6, 0.1, 0.4, -0.1],\n                [1, 0.3, -0.4, 0.6, 0.0],\n                [1, -0.1, 0.2, -0.5, 0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1200, 1000], [1500, 1400],\n                [1700, 1200], [2100, 1600], [2300, 1600]\n            ])\n        },\n        {\n            \"K\": 4,\n            \"D_min\": 500.0,\n            \"X\": np.array([\n                [1, 0.2, 0.1, -0.1, 0.05],\n                [1, 0.21, 0.12, -0.11, 0.06],\n                [1, 0.19, 0.09, -0.12, 0.04],\n                [1, 0.18, 0.15, -0.10, 0.03],\n                [1, 0.22, 0.11, -0.13, 0.07],\n                [1, -0.9, 0.4, 0.2, -0.3],\n                [1, 0.8, -0.6, 0.5, 0.2]\n            ]),\n            \"U\": np.array([\n                [1000, 1000], [1020, 1010], [990, 1030],\n                [1010, 980], [980, 990], [5000, 5000], [6000, 7000]\n            ])\n        }\n    ]\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    all_results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        U = case[\"U\"]\n        K = case[\"K\"]\n        D_min = case[\"D_min\"]\n        num_candidates = X.shape[0]\n\n        # 1. Compute scores for all candidates\n        z = X @ w\n        p = sigmoid(z)\n        p_variance = p * (1 - p)\n        x_norm_sq = np.sum(X**2, axis=1)\n        scores = (eta**2) * p_variance * x_norm_sq\n\n        # 2. Sort candidate indices by score (desc) and original index (asc) for tie-breaking\n        candidate_indices = sorted(range(num_candidates), key=lambda i: (-scores[i], i))\n\n        # 3. Greedy selection with spatial constraint\n        selected_indices = []\n        for idx in candidate_indices:\n            if len(selected_indices) = K:\n                break\n\n            is_valid = True\n            current_u = U[idx]\n            for selected_idx in selected_indices:\n                dist = np.linalg.norm(current_u - U[selected_idx])\n                if dist  D_min:\n                    is_valid = False\n                    break\n            \n            if is_valid:\n                selected_indices.append(idx)\n        \n        all_results.append(selected_indices)\n\n    # Format the output string to match the required format: [[...],[...],...]\n    result_str = \"[\" + \",\".join([str(sublist).replace(\" \", \"\") for sublist in all_results]) + \"]\"\n    print(result_str)\n\n\nsolve()\n```"
        }
    ]
}