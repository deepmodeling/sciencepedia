{
    "hands_on_practices": [
        {
            "introduction": "This first practice guides you through the complete Maximum Likelihood Classification workflow from the ground up. Starting with raw training data for two distinct land-cover classes, you will derive the statistical parameters—the mean vector and covariance matrix—that define each class under a Gaussian assumption. This exercise  solidifies the foundational skills of parameter estimation via Maximum Likelihood and the subsequent calculation of the log-likelihood ratio used for classification.",
            "id": "3826538",
            "problem": "A multispectral satellite image provides three calibrated reflectance bands: blue, red, and near-infrared (NIR). Consider two land-cover classes, water and vegetation, to be modeled by class-conditional trivariate normal distributions with unknown mean vectors and covariance matrices. You are provided training samples for each class, collected from homogeneous regions and preprocessed to remove atmospheric and illumination artifacts. Let the water class training samples be\n$$\n(0.05,\\,0.03,\\,0.02),\\quad\n(0.06,\\,0.03,\\,0.02),\\quad\n(0.04,\\,0.03,\\,0.02),\\quad\n(0.05,\\,0.04,\\,0.02),\\quad\n(0.05,\\,0.02,\\,0.02),\\quad\n(0.05,\\,0.03,\\,0.03),\\quad\n(0.05,\\,0.03,\\,0.01),\n$$\nand the vegetation class training samples be\n$$\n(0.06,\\,0.08,\\,0.45),\\quad\n(0.08,\\,0.08,\\,0.45),\\quad\n(0.04,\\,0.08,\\,0.45),\\quad\n(0.06,\\,0.11,\\,0.45),\\quad\n(0.06,\\,0.05,\\,0.45),\\quad\n(0.06,\\,0.08,\\,0.50),\\quad\n(0.06,\\,0.08,\\,0.40).\n$$\nA new pixel with reflectance vector\n$$\n\\mathbf{x}=(0.052,\\,0.031,\\,0.022)\n$$\nis to be classified by maximum likelihood classification (ignore any class priors and misclassification costs). Starting from the probability density function of the multivariate normal distribution and the maximum likelihood principle, derive the maximum likelihood estimators for the mean and covariance of each class, estimate these parameters from the given training data, and then compute the exact log-likelihood ratio\n$$\n\\Lambda(\\mathbf{x})=\\ln p(\\mathbf{x}\\mid \\text{water})-\\ln p(\\mathbf{x}\\mid \\text{vegetation}),\n$$\nevaluated at the estimated parameters.\n\nExpress the final value of $\\Lambda(\\mathbf{x})$ as a dimensionless real number and round your answer to four significant figures.",
            "solution": "The problem asks us to classify a new pixel using Maximum Likelihood Classification, which involves calculating the log-likelihood ratio of the pixel's reflectance vector belonging to the 'water' class versus the 'vegetation' class. The class-conditional probability distributions are modeled as trivariate normal distributions.\n\nFirst, we must validate the problem statement.\nThe givens are:\n- Two classes: water ($\\omega_1$) and vegetation ($\\omega_2$).\n- The data is $d=3$ dimensional, representing blue, red, and NIR reflectance.\n- The class-conditional probability density function (PDF) for each class $i$ is a trivariate normal distribution $p(\\mathbf{x}|\\omega_i) \\sim N(\\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)$.\n- There are $N_1=7$ training samples for the water class:\n$(0.05, 0.03, 0.02)$, $(0.06, 0.03, 0.02)$, $(0.04, 0.03, 0.02)$, $(0.05, 0.04, 0.02)$, $(0.05, 0.02, 0.02)$, $(0.05, 0.03, 0.03)$, $(0.05, 0.03, 0.01)$.\n- There are $N_2=7$ training samples for the vegetation class:\n$(0.06, 0.08, 0.45)$, $(0.08, 0.08, 0.45)$, $(0.04, 0.08, 0.45)$, $(0.06, 0.11, 0.45)$, $(0.06, 0.05, 0.45)$, $(0.06, 0.08, 0.50)$, $(0.06, 0.08, 0.40)$.\n- A new pixel with reflectance vector $\\mathbf{x}=(0.052, 0.031, 0.022)$ needs to be classified.\n- The task is to compute the log-likelihood ratio $\\Lambda(\\mathbf{x})=\\ln p(\\mathbf{x}\\mid \\text{water})-\\ln p(\\mathbf{x}\\mid \\text{vegetation})$, ignoring class priors.\n- The final answer must be rounded to four significant figures.\n\nThe problem is scientifically grounded, as it represents a standard task in remote sensing image analysis. The reflectance values are physically plausible for water and vegetation. The use of multivariate normal models is a common and appropriate statistical assumption. The problem is well-posed, providing all necessary data and a clear objective. There are no contradictions or ambiguities. The number of samples ($N=7$) is greater than the dimensionality ($d=3$), ensuring the sample covariance matrices will be non-singular. Therefore, the problem is valid.\n\nWe proceed with the solution.\n\n**Step 1: Derive the Maximum Likelihood Estimators (MLEs)**\nLet a set of $N$ i.i.d. samples $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ be drawn from a $d$-dimensional multivariate normal distribution $N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. The PDF is:\n$$p(\\mathbf{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\right)$$\nThe log-likelihood function for the $N$ samples is:\n$$\n\\ln L(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\sum_{k=1}^{N} \\ln p(\\mathbf{x}_k | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = -\\frac{Nd}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}\\sum_{k=1}^{N} (\\mathbf{x}_k-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}_k-\\boldsymbol{\\mu})\n$$\nTo find the MLE for the mean $\\boldsymbol{\\mu}$, we take the gradient with respect to $\\boldsymbol{\\mu}$ and set it to zero:\n$$\n\\nabla_{\\boldsymbol{\\mu}} \\ln L = \\sum_{k=1}^{N} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_k - \\boldsymbol{\\mu}) = \\boldsymbol{0} \\implies \\sum_{k=1}^{N} (\\mathbf{x}_k - \\boldsymbol{\\mu}) = \\boldsymbol{0} \\implies N\\hat{\\boldsymbol{\\mu}} = \\sum_{k=1}^{N} \\mathbf{x}_k\n$$\nThis gives the MLE for the mean, which is the sample mean:\n$$\\hat{\\boldsymbol{\\mu}} = \\frac{1}{N} \\sum_{k=1}^{N} \\mathbf{x}_k$$\nTo find the MLE for the covariance $\\boldsymbol{\\Sigma}$, we differentiate $\\ln L$ with respect to $\\boldsymbol{\\Sigma}^{-1}$ and set the result to zero. This yields:\n$$\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N} \\sum_{k=1}^{N} (\\mathbf{x}_k - \\hat{\\boldsymbol{\\mu}})(\\mathbf{x}_k - \\hat{\\boldsymbol{\\mu}})^T$$\nThis is the sample covariance matrix (with $1/N$ scaling).\n\n**Step 2: Estimate Parameters for Each Class**\nLet $\\omega_1$ be the water class and $\\omega_2$ be the vegetation class. Here, $d=3$ and $N_1=N_2=7$.\n\nFor the water class ($ \\omega_1 $):\nThe sum of the sample vectors is $\\sum_{k=1}^{7} \\mathbf{x}_{1,k} = (0.35, 0.21, 0.14)^T$.\nThe MLE of the mean is:\n$$\\hat{\\boldsymbol{\\mu}}_1 = \\frac{1}{7}(0.35, 0.21, 0.14)^T = (0.05, 0.03, 0.02)^T$$\nThe centered vectors $(\\mathbf{x}_{1,k} - \\hat{\\boldsymbol{\\mu}}_1)$ are: $(0,0,0)^T$, $(0.01,0,0)^T$, $(-0.01,0,0)^T$, $(0,0.01,0)^T$, $(0,-0.01,0)^T$, $(0,0,0.01)^T$, $(0,0,-0.01)^T$.\nThe sum of the outer products is:\n$$ \\sum_{k=1}^{7} (\\mathbf{x}_{1,k} - \\hat{\\boldsymbol{\\mu}}_1)(\\mathbf{x}_{1,k} - \\hat{\\boldsymbol{\\mu}}_1)^T = \\begin{pmatrix} 2(0.01^2) & 0 & 0 \\\\ 0 & 2(0.01^2) & 0 \\\\ 0 & 0 & 2(0.01^2) \\end{pmatrix} = \\begin{pmatrix} 0.0002 & 0 & 0 \\\\ 0 & 0.0002 & 0 \\\\ 0 & 0 & 0.0002 \\end{pmatrix} $$\nThe MLE of the covariance matrix is:\n$$ \\hat{\\boldsymbol{\\Sigma}}_1 = \\frac{1}{7} \\begin{pmatrix} 0.0002 & 0 & 0 \\\\ 0 & 0.0002 & 0 \\\\ 0 & 0 & 0.0002 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{70000} & 0 & 0 \\\\ 0 & \\frac{2}{70000} & 0 \\\\ 0 & 0 & \\frac{2}{70000} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{35000} & 0 & 0 \\\\ 0 & \\frac{1}{35000} & 0 \\\\ 0 & 0 & \\frac{1}{35000} \\end{pmatrix} $$\n\nFor the vegetation class ($ \\omega_2 $):\nThe sum of the sample vectors is $\\sum_{k=1}^{7} \\mathbf{x}_{2,k} = (0.42, 0.56, 3.15)^T$.\nThe MLE of the mean is:\n$$\\hat{\\boldsymbol{\\mu}}_2 = \\frac{1}{7}(0.42, 0.56, 3.15)^T = (0.06, 0.08, 0.45)^T$$\nThe centered vectors $(\\mathbf{x}_{2,k} - \\hat{\\boldsymbol{\\mu}}_2)$ have non-zero components: $(0.02,0,0)^T$, $(-0.02,0,0)^T$, $(0,0.03,0)^T$, $(0,-0.03,0)^T$, $(0,0,0.05)^T$, $(0,0,-0.05)^T$.\nThe sum of the outer products is:\n$$ \\sum_{k=1}^{7} (\\mathbf{x}_{2,k} - \\hat{\\boldsymbol{\\mu}}_2)(\\mathbf{x}_{2,k} - \\hat{\\boldsymbol{\\mu}}_2)^T = \\begin{pmatrix} 2(0.02^2) & 0 & 0 \\\\ 0 & 2(0.03^2) & 0 \\\\ 0 & 0 & 2(0.05^2) \\end{pmatrix} = \\begin{pmatrix} 0.0008 & 0 & 0 \\\\ 0 & 0.0018 & 0 \\\\ 0 & 0 & 0.0050 \\end{pmatrix} $$\nThe MLE of the covariance matrix is:\n$$ \\hat{\\boldsymbol{\\Sigma}}_2 = \\frac{1}{7} \\begin{pmatrix} 0.0008 & 0 & 0 \\\\ 0 & 0.0018 & 0 \\\\ 0 & 0 & 0.0050 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{70000} & 0 & 0 \\\\ 0 & \\frac{18}{70000} & 0 \\\\ 0 & 0 & \\frac{50}{70000} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{35000} & 0 & 0 \\\\ 0 & \\frac{9}{35000} & 0 \\\\ 0 & 0 & \\frac{25}{35000} \\end{pmatrix} $$\n\n**Step 3: Compute the Log-Likelihood Ratio**\nThe log-likelihood ratio $\\Lambda(\\mathbf{x})$ is given by:\n$$\n\\Lambda(\\mathbf{x}) = \\ln p(\\mathbf{x}\\mid \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}}_1) - \\ln p(\\mathbf{x}\\mid \\hat{\\boldsymbol{\\mu}}_2, \\hat{\\boldsymbol{\\Sigma}}_2)\n$$\nSubstituting the log-PDF expression:\n$$\n\\Lambda(\\mathbf{x}) = \\left[-\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\hat{\\boldsymbol{\\Sigma}}_1| - \\frac{1}{2}D_1^2(\\mathbf{x})\\right] - \\left[-\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\hat{\\boldsymbol{\\Sigma}}_2| - \\frac{1}{2}D_2^2(\\mathbf{x})\\right]\n$$\nwhere $D_i^2(\\mathbf{x}) = (\\mathbf{x}-\\hat{\\boldsymbol{\\mu}}_i)^T \\hat{\\boldsymbol{\\Sigma}}_i^{-1} (\\mathbf{x}-\\hat{\\boldsymbol{\\mu}}_i)$ is the squared Mahalanobis distance.\nSimplifying, we get:\n$$\n\\Lambda(\\mathbf{x}) = \\frac{1}{2}\\ln\\left(\\frac{|\\hat{\\boldsymbol{\\Sigma}}_2|}{|\\hat{\\boldsymbol{\\Sigma}}_1|}\\right) - \\frac{1}{2}\\left(D_1^2(\\mathbf{x}) - D_2^2(\\mathbf{x})\\right)\n$$\nThe pixel to classify is $\\mathbf{x}=(0.052, 0.031, 0.022)^T$.\n\nFirst, compute the ratio of determinants:\n$|\\hat{\\boldsymbol{\\Sigma}}_1| = (\\frac{1}{35000})^3$\n$|\\hat{\\boldsymbol{\\Sigma}}_2| = (\\frac{4}{35000})(\\frac{9}{35000})(\\frac{25}{35000}) = \\frac{900}{35000^3}$\n$\\frac{|\\hat{\\boldsymbol{\\Sigma}}_2|}{|\\hat{\\boldsymbol{\\Sigma}}_1|} = 900$. So, the first term is $\\frac{1}{2}\\ln(900) = \\ln(\\sqrt{900}) = \\ln(30)$.\n\nNext, compute the squared Mahalanobis distances. Since the covariance matrices are diagonal, $D_i^2(\\mathbf{x}) = \\sum_{j=1}^{3} \\frac{(x_j - \\hat{\\mu}_{i,j})^2}{\\hat{\\sigma}_{i,j}^2}$.\n\nFor water ($\\omega_1$):\n$\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_1 = (0.052 - 0.05, 0.031 - 0.03, 0.022 - 0.02)^T = (0.002, 0.001, 0.002)^T$.\nThe variance is constant $\\hat{\\sigma}_{1,j}^2 = 1/35000$.\n$$ D_1^2(\\mathbf{x}) = \\frac{(0.002)^2}{1/35000} + \\frac{(0.001)^2}{1/35000} + \\frac{(0.002)^2}{1/35000} = 35000 \\times (0.000004 + 0.000001 + 0.000004) = 35000 \\times 9 \\times 10^{-6} = 0.315 $$\n\nFor vegetation ($\\omega_2$):\n$\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_2 = (0.052 - 0.06, 0.031 - 0.08, 0.022 - 0.45)^T = (-0.008, -0.049, -0.428)^T$.\nThe variances are $\\hat{\\sigma}_{2,1}^2=4/35000$, $\\hat{\\sigma}_{2,2}^2=9/35000$, $\\hat{\\sigma}_{2,3}^2=25/35000$.\n$$ D_2^2(\\mathbf{x}) = \\frac{(-0.008)^2}{4/35000} + \\frac{(-0.049)^2}{9/35000} + \\frac{(-0.428)^2}{25/35000} $$\n$$ D_2^2(\\mathbf{x}) = \\frac{0.000064 \\times 35000}{4} + \\frac{0.002401 \\times 35000}{9} + \\frac{0.183184 \\times 35000}{25} $$\n$$ D_2^2(\\mathbf{x}) = 0.56 + \\frac{84.035}{9} + \\frac{6411.44}{25} = 0.56 + 9.33722... + 256.4576 = 266.35482... $$\nNow, assemble the log-likelihood ratio:\n$$ \\Lambda(\\mathbf{x}) = \\ln(30) - \\frac{1}{2}(0.315 - 266.35482...) = \\ln(30) - \\frac{1}{2}(-266.03982...) $$\n$$ \\Lambda(\\mathbf{x}) = \\ln(30) + 133.01991... \\approx 3.401197 + 133.019911 = 136.421108... $$\nRounding to four significant figures, we get $136.4$.",
            "answer": "$$\\boxed{136.4}$$"
        },
        {
            "introduction": "Building on the fundamentals of parameter estimation, this practice explores the geometry of the classifier's decision boundary. You will derive the general quadratic discriminant function and discover how it simplifies to a linear form under specific assumptions about class covariance . This exercise provides a deeper intuition for how Maximum Likelihood Classification partitions the feature space and highlights the distinction between quadratic and linear discriminant analysis.",
            "id": "3826536",
            "problem": "A two-class Maximum Likelihood Classification (MLC) problem is posed for a multispectral remote sensing scene in two bands, red and near-infrared. Consider the feature vector $x = (x_{\\mathrm{R}}, x_{\\mathrm{NIR}})^{\\top}$ that represents top-of-atmosphere reflectance (unitless fraction) in the red and near-infrared bands. Suppose the class-conditional distributions for emergent vegetation (class $\\mathcal{V}$) and clear water (class $\\mathcal{W}$) are modeled as multivariate Gaussian (Normal) distributions with parameters estimated from training data:\n$$\n\\mu_{\\mathcal{V}} = \\begin{pmatrix} 0.08 \\\\ 0.45 \\end{pmatrix}, \\quad \\Sigma_{\\mathcal{V}} = \\begin{pmatrix} 0.0025 & 0 \\\\ 0 & 0.0121 \\end{pmatrix},\n$$\n$$\n\\mu_{\\mathcal{W}} = \\begin{pmatrix} 0.05 \\\\ 0.06 \\end{pmatrix}, \\quad \\Sigma_{\\mathcal{W}} = \\begin{pmatrix} 0.0016 & 0 \\\\ 0 & 0.0025 \\end{pmatrix},\n$$\nand prior probabilities\n$$\nP(\\mathcal{V}) = 0.55, \\quad P(\\mathcal{W}) = 0.45.\n$$\nAssume independence across bands is a reasonable approximation so the covariance matrices are diagonal as given. A pixel with reflectance $x_{0} = (0.07, 0.40)^{\\top}$ is observed.\n\nStarting only from Bayes decision rule for minimum probability of error classification and the multivariate Gaussian probability density function, derive the difference of the log-discriminant functions $\\Delta(x) = g_{\\mathcal{V}}(x) - g_{\\mathcal{W}}(x)$ for the general heteroscedastic case (class-specific covariance matrices), making explicit any terms that depend on $x$, the means, the covariance matrices, and the priors. Then, under the homoscedastic assumption (equal covariance across classes) with a common covariance\n$$\n\\Sigma = \\begin{pmatrix} 0.0020 & 0 \\\\ 0 & 0.0040 \\end{pmatrix},\n$$\nshow that the discriminant difference reduces to a linear function in $x$ and identify the weight vector and bias in terms of $\\mu_{\\mathcal{V}}$, $\\mu_{\\mathcal{W}}$, $\\Sigma$, and the priors.\n\nFinally, evaluate the heteroscedastic discriminant difference $\\Delta(x_{0})$ numerically using the parameters provided above for $\\Sigma_{\\mathcal{V}}$ and $\\Sigma_{\\mathcal{W}}$. Express the final value of $\\Delta(x_{0})$ as a single real number, rounded to four significant figures. The answer is unitless.",
            "solution": "The problem is valid as it is scientifically grounded in statistical pattern recognition and Bayesian decision theory, well-posed with all necessary data provided, and objective in its formulation. We can proceed with the solution.\n\nThe problem asks for three tasks: first, to derive the difference of the log-discriminant functions $\\Delta(x) = g_{\\mathcal{V}}(x) - g_{\\mathcal{W}}(x)$ for the general heteroscedastic case; second, to show this simplifies to a linear function for the homoscedastic case and identify the constituent terms; and third, to evaluate $\\Delta(x_{0})$ numerically for a specific observation $x_{0}$.\n\n**Part 1: Derivation of the Heteroscedastic Discriminant Difference**\n\nBayes' decision rule for minimum classification error is to assign a feature vector $x$ to the class $\\omega_i$ that has the maximum a posteriori probability, $P(\\omega_i | x)$. Using Bayes' theorem, this posterior probability is given by:\n$$\nP(\\omega_i | x) = \\frac{p(x | \\omega_i) P(\\omega_i)}{p(x)}\n$$\nwhere $p(x | \\omega_i)$ is the class-conditional probability density function, $P(\\omega_i)$ is the prior probability of class $\\omega_i$, and $p(x)$ is the evidence. Since $p(x)$ is the same for all classes, maximizing the posterior is equivalent to maximizing the product $p(x | \\omega_i) P(\\omega_i)$.\n\nFor computational convenience, we work with the logarithm of this product, as the logarithm is a monotonically increasing function. This defines the discriminant function $g_i(x)$:\n$$\ng_i(x) = \\ln\\left(p(x | \\omega_i) P(\\omega_i)\\right) = \\ln(p(x | \\omega_i)) + \\ln(P(\\omega_i))\n$$\nThe problem states that the class-conditional densities are multivariate Gaussian. For a feature vector $x$ of dimension $d$, the probability density function for class $\\omega_i$ is:\n$$\np(x | \\omega_i) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_i|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_i)^{\\top} \\Sigma_i^{-1} (x - \\mu_i)\\right)\n$$\nSubstituting this into the discriminant function expression, we get:\n$$\ng_i(x) = \\ln\\left(\\frac{1}{(2\\pi)^{d/2} |\\Sigma_i|^{1/2}}\\right) - \\frac{1}{2} (x - \\mu_i)^{\\top} \\Sigma_i^{-1} (x - \\mu_i) + \\ln(P(\\omega_i))\n$$\n$$\ng_i(x) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(|\\Sigma_i|) - \\frac{1}{2}(x - \\mu_i)^{\\top} \\Sigma_i^{-1} (x - \\mu_i) + \\ln(P(\\omega_i))\n$$\nWe are asked to find the difference of the discriminant functions for class $\\mathcal{V}$ (vegetation) and class $\\mathcal{W}$ (water), denoted as $\\Delta(x) = g_{\\mathcal{V}}(x) - g_{\\mathcal{W}}(x)$.\n$$\ng_{\\mathcal{V}}(x) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(|\\Sigma_{\\mathcal{V}}|) - \\frac{1}{2}(x - \\mu_{\\mathcal{V}})^{\\top} \\Sigma_{\\mathcal{V}}^{-1} (x - \\mu_{\\mathcal{V}}) + \\ln(P(\\mathcal{V}))\n$$\n$$\ng_{\\mathcal{W}}(x) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(|\\Sigma_{\\mathcal{W}}|) - \\frac{1}{2}(x - \\mu_{\\mathcal{W}})^{\\top} \\Sigma_{\\mathcal{W}}^{-1} (x - \\mu_{\\mathcal{W}}) + \\ln(P(\\mathcal{W}))\n$$\nSubtracting $g_{\\mathcal{W}}(x)$ from $g_{\\mathcal{V}}(x)$, the common term $-\\frac{d}{2}\\ln(2\\pi)$ cancels out:\n$$\n\\Delta(x) = \\left( - \\frac{1}{2}\\ln(|\\Sigma_{\\mathcal{V}}|) - \\frac{1}{2}(x - \\mu_{\\mathcal{V}})^{\\top} \\Sigma_{\\mathcal{V}}^{-1} (x - \\mu_{\\mathcal{V}}) + \\ln(P(\\mathcal{V})) \\right) - \\left( - \\frac{1}{2}\\ln(|\\Sigma_{\\mathcal{W}}|) - \\frac{1}{2}(x - \\mu_{\\mathcal{W}})^{\\top} \\Sigma_{\\mathcal{W}}^{-1} (x - \\mu_{\\mathcal{W}}) + \\ln(P(\\mathcal{W})) \\right)\n$$\nRearranging the terms, we arrive at the general heteroscedastic discriminant difference:\n$$\n\\Delta(x) = \\frac{1}{2}\\left[ (x - \\mu_{\\mathcal{W}})^{\\top} \\Sigma_{\\mathcal{W}}^{-1} (x - \\mu_{\\mathcal{W}}) - (x - \\mu_{\\mathcal{V}})^{\\top} \\Sigma_{\\mathcal{V}}^{-1} (x - \\mu_{\\mathcal{V}}) \\right] + \\frac{1}{2}\\ln\\left(\\frac{|\\Sigma_{\\mathcal{W}}|}{|\\Sigma_{\\mathcal{V}}|}\\right) + \\ln\\left(\\frac{P(\\mathcal{V})}{P(\\mathcal{W})}\\right)\n$$\nThis expression contains a quadratic term in $x$ (since $\\Sigma_{\\mathcal{V}} \\neq \\Sigma_{\\mathcal{W}}$), as well as terms dependent on the means, covariance matrices, and prior probabilities.\n\n**Part 2: Reduction to the Linear Case (Homoscedastic Assumption)**\n\nUnder the homoscedastic assumption, the covariance matrices for all classes are equal, i.e., $\\Sigma_{\\mathcal{V}} = \\Sigma_{\\mathcal{W}} = \\Sigma$. Let's analyze the effect on $\\Delta(x)$.\nThe log-determinant term becomes:\n$$\n\\frac{1}{2}\\ln\\left(\\frac{|\\Sigma|}{|\\Sigma|}\\right) = \\frac{1}{2}\\ln(1) = 0\n$$\nTo see how the quadratic term simplifies, we expand it:\n$$\n(x - \\mu)^{\\top}\\Sigma^{-1}(x - \\mu) = x^{\\top}\\Sigma^{-1}x - 2\\mu^{\\top}\\Sigma^{-1}x + \\mu^{\\top}\\Sigma^{-1}\\mu\n$$\nSubstituting this into the first part of $\\Delta(x)$ with $\\Sigma_{\\mathcal{V}}^{-1} = \\Sigma_{\\mathcal{W}}^{-1} = \\Sigma^{-1}$:\n$$\n\\frac{1}{2}\\left[ (x^{\\top}\\Sigma^{-1}x - 2\\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}x + \\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}}) - (x^{\\top}\\Sigma^{-1}x - 2\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}x + \\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}}) \\right]\n$$\nThe quadratic term in $x$, $x^{\\top}\\Sigma^{-1}x$, cancels out. The expression simplifies to:\n$$\n\\frac{1}{2}\\left[ 2\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}x - 2\\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}x + \\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}} - \\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}} \\right] = (\\mu_{\\mathcal{V}} - \\mu_{\\mathcal{W}})^{\\top}\\Sigma^{-1}x - \\frac{1}{2}(\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}} - \\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}})\n$$\nCombining this with the prior probability term, the discriminant difference reduces to a linear function of $x$:\n$$\n\\Delta(x) = (\\mu_{\\mathcal{V}} - \\mu_{\\mathcal{W}})^{\\top}\\Sigma^{-1}x - \\frac{1}{2}\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}} + \\frac{1}{2}\\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}} + \\ln\\left(\\frac{P(\\mathcal{V})}{P(\\mathcal{W})}\\right)\n$$\nThis is a linear function of the form $\\Delta(x) = w^{\\top}x + b$, where:\nThe weight vector is $w = \\Sigma^{-1}(\\mu_{\\mathcal{V}} - \\mu_{\\mathcal{W}})$.\nThe bias term is $b = -\\frac{1}{2}\\mu_{\\mathcal{V}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{V}} + \\frac{1}{2}\\mu_{\\mathcal{W}}^{\\top}\\Sigma^{-1}\\mu_{\\mathcal{W}} + \\ln\\left(\\frac{P(\\mathcal{V})}{P(\\mathcal{W})}\\right)$.\n\n**Part 3: Numerical Evaluation of $\\Delta(x_0)$**\n\nWe now evaluate $\\Delta(x_0)$ using the heteroscedastic formula and the given parameters for a pixel $x_0 = (0.07, 0.40)^{\\top}$.\nThe parameters are:\n$$\nx_0 = \\begin{pmatrix} 0.07 \\\\ 0.40 \\end{pmatrix}, \\quad \\mu_{\\mathcal{V}} = \\begin{pmatrix} 0.08 \\\\ 0.45 \\end{pmatrix}, \\quad \\mu_{\\mathcal{W}} = \\begin{pmatrix} 0.05 \\\\ 0.06 \\end{pmatrix}\n$$\n$$\n\\Sigma_{\\mathcal{V}} = \\begin{pmatrix} 0.0025 & 0 \\\\ 0 & 0.0121 \\end{pmatrix}, \\quad \\Sigma_{\\mathcal{W}} = \\begin{pmatrix} 0.0016 & 0 \\\\ 0 & 0.0025 \\end{pmatrix}\n$$\n$$\nP(\\mathcal{V}) = 0.55, \\quad P(\\mathcal{W}) = 0.45\n$$\nFirst, we compute the inverse and determinant of the covariance matrices. Since they are diagonal, these are straightforward.\n$$\n\\Sigma_{\\mathcal{V}}^{-1} = \\begin{pmatrix} \\frac{1}{0.0025} & 0 \\\\ 0 & \\frac{1}{0.0121} \\end{pmatrix} = \\begin{pmatrix} 400 & 0 \\\\ 0 & \\frac{10000}{121} \\end{pmatrix}\n$$\n$$\n|\\Sigma_{\\mathcal{V}}| = 0.0025 \\times 0.0121 = 0.00003025\n$$\n$$\n\\Sigma_{\\mathcal{W}}^{-1} = \\begin{pmatrix} \\frac{1}{0.0016} & 0 \\\\ 0 & \\frac{1}{0.0025} \\end{pmatrix} = \\begin{pmatrix} 625 & 0 \\\\ 0 & 400 \\end{pmatrix}\n$$\n$$\n|\\Sigma_{\\mathcal{W}}| = 0.0016 \\times 0.0025 = 0.000004\n$$\nNext, we compute the Mahalanobis distance squared terms, which are the quadratic forms $(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu)$:\n$$\nx_0 - \\mu_{\\mathcal{V}} = \\begin{pmatrix} 0.07 - 0.08 \\\\ 0.40 - 0.45 \\end{pmatrix} = \\begin{pmatrix} -0.01 \\\\ -0.05 \\end{pmatrix}\n$$\n$$\n(x_0 - \\mu_{\\mathcal{V}})^{\\top}\\Sigma_{\\mathcal{V}}^{-1}(x_0 - \\mu_{\\mathcal{V}}) = 400(-0.01)^2 + \\frac{10000}{121}(-0.05)^2 = 400(0.0001) + \\frac{10000}{121}(0.0025) = 0.04 + \\frac{25}{121} \\approx 0.24661\n$$\n$$\nx_0 - \\mu_{\\mathcal{W}} = \\begin{pmatrix} 0.07 - 0.05 \\\\ 0.40 - 0.06 \\end{pmatrix} = \\begin{pmatrix} 0.02 \\\\ 0.34 \\end{pmatrix}\n$$\n$$\n(x_0 - \\mu_{\\mathcal{W}})^{\\top}\\Sigma_{\\mathcal{W}}^{-1}(x_0 - \\mu_{\\mathcal{W}}) = 625(0.02)^2 + 400(0.34)^2 = 625(0.0004) + 400(0.1156) = 0.25 + 46.24 = 46.49\n$$\nNow, we compute the logarithmic terms:\n$$\n\\frac{1}{2}\\ln\\left(\\frac{|\\Sigma_{\\mathcal{W}}|}{|\\Sigma_{\\mathcal{V}}|}\\right) = \\frac{1}{2}\\ln\\left(\\frac{0.000004}{0.00003025}\\right) = \\frac{1}{2}\\ln\\left(\\frac{16}{121}\\right) = \\ln\\left(\\frac{4}{11}\\right) \\approx -1.01160\n$$\n$$\n\\ln\\left(\\frac{P(\\mathcal{V})}{P(\\mathcal{W})}\\right) = \\ln\\left(\\frac{0.55}{0.45}\\right) = \\ln\\left(\\frac{11}{9}\\right) \\approx 0.20067\n$$\nFinally, we assemble the parts to compute $\\Delta(x_0)$:\n$$\n\\Delta(x_0) = \\frac{1}{2}\\left[ 46.49 - \\left(0.04 + \\frac{25}{121}\\right) \\right] + \\ln\\left(\\frac{4}{11}\\right) + \\ln\\left(\\frac{11}{9}\\right)\n$$\n$$\n\\Delta(x_0) \\approx \\frac{1}{2}[46.49 - 0.24661] - 1.01160 + 0.20067\n$$\n$$\n\\Delta(x_0) \\approx \\frac{1}{2}[46.24339] - 1.01160 + 0.20067\n$$\n$$\n\\Delta(x_0) \\approx 23.12170 - 1.01160 + 0.20067 = 22.31077\n$$\nRounding the result to four significant figures gives $22.31$.\nThe large positive value of $\\Delta(x_0)$ indicates that the posterior probability for class $\\mathcal{V}$ (vegetation) is much higher than for class $\\mathcal{W}$ (water), so the pixel would be classified as vegetation.",
            "answer": "$$\\boxed{22.31}$$"
        },
        {
            "introduction": "Real-world sensor data is often imperfect and may contain outliers that violate the assumptions of a standard Gaussian model. This final, implementation-focused practice challenges you to build and compare a classical Gaussian classifier with a robust version based on the multivariate Student-t distribution . Through this coding exercise, you will directly observe how robust statistical modeling can mitigate the influence of outliers and lead to more reliable classifications.",
            "id": "3826533",
            "problem": "A remote sensing analyst is building a maximum likelihood classifier for land-cover mapping in a multispectral feature space. Each pixel is represented by a vector in $\\mathbb{R}^d$, where $d$ is the number of bands. The analyst considers two land-cover classes with equal prior probabilities and wishes to implement two generative classifiers: a classical parametric classifier assuming a multivariate normal family and a robust parametric classifier assuming a multivariate Student-$t$ family to mitigate the impact of outliers. The goal is to derive the decision rules from first principles and implement them in a program that evaluates a specified test suite.\n\nStart from the following fundamental base: the definition of a generative classifier that assigns a pixel feature vector $x \\in \\mathbb{R}^d$ to the class $c \\in \\{0,1\\}$ that maximizes the class-conditional likelihood $p(x \\mid \\theta_c)$, where $\\theta_c$ denotes the class parameters, under equal prior probabilities. Use the definitions of the multivariate normal family and the multivariate Student-$t$ family, and the principle of Maximum Likelihood Estimation (MLE). No shortcut formulas are to be assumed in the problem statement; the solution must derive the discriminant rules and estimators from the stated base.\n\nImplement two classifiers:\n- A classical maximum likelihood classifier under a multivariate normal family, with parameters for each class estimated by MLE from the training samples. To ensure numerical stability in boundary conditions, apply a fixed diagonal regularization $\\lambda I_d$ to the covariance estimate, with $\\lambda = 10^{-4}$ and $I_d$ the $d \\times d$ identity matrix.\n- A robust maximum likelihood classifier under a multivariate Student-$t$ family with fixed degrees of freedom $\\nu = 4$, with parameters for each class estimated by maximizing the Student-$t$ likelihood. Use an iterative reweighting scheme consistent with the MLE of the Student-$t$ family and apply the same diagonal regularization $\\lambda I_d$ at each iteration.\n\nAssume equal class priors. In the event of an exact tie in the discriminant scores, break ties by choosing the class with the larger index.\n\nYour program must evaluate the following test suite. For each test case, the training samples for class $0$, the training samples for class $1$, and the test pixels are given explicitly as $3$-dimensional vectors. All numbers below are in unitless reflectance-like feature space and are to be treated as pure numbers without physical units.\n\nTest case $1$ (happy path with moderate outliers):\n- Class $0$ training samples:\n$\\{(0.18, 0.52, 0.31), (0.21, 0.49, 0.29), (0.20, 0.51, 0.33), (0.19, 0.50, 0.28), (0.90, 0.90, 0.90)\\}$.\n- Class $1$ training samples:\n$\\{(0.48, 0.31, 0.21), (0.51, 0.29, 0.19), (0.50, 0.30, 0.22), (0.52, 0.28, 0.20), (0.00, 0.00, 0.00)\\}$.\n- Test pixels:\n$\\{(0.20, 0.50, 0.30), (0.50, 0.30, 0.20), (0.85, 0.85, 0.85)\\}$.\n\nTest case $2$ (boundary condition with nearly singular covariances):\n- Class $0$ training samples:\n$\\{(0.10, 0.10, 0.10), (0.11, 0.11, 0.11), (0.09, 0.09, 0.09)\\}$.\n- Class $1$ training samples:\n$\\{(0.60, 0.60, 0.60), (0.61, 0.61, 0.61), (0.59, 0.59, 0.59)\\}$.\n- Test pixels:\n$\\{(0.10, 0.10, 0.10), (0.60, 0.60, 0.60), (0.35, 0.35, 0.35)\\}$.\n\nTest case $3$ (heavy contamination by extreme outliers):\n- Class $0$ training samples:\n$\\{(0.25, 0.55, 0.35), (0.22, 0.53, 0.32), (0.24, 0.54, 0.34), (0.23, 0.52, 0.31)\\}$.\n- Class $1$ training samples:\n$\\{(0.55, 0.25, 0.15), (0.56, 0.24, 0.16), (10.00, 10.00, 10.00)\\}$.\n- Test pixels:\n$\\{(0.24, 0.53, 0.33), (0.55, 0.24, 0.16), (9.00, 9.00, 9.00)\\}$.\n\nAlgorithmic requirements:\n- For the multivariate normal classifier, estimate for each class $c$ the MLE mean $\\mu_c$ and MLE covariance $\\Sigma_c$ from its training samples, apply $\\Sigma_c \\leftarrow \\Sigma_c + \\lambda I_d$, and classify each test pixel by maximizing the corresponding class-conditional likelihood.\n- For the multivariate Student-$t$ classifier with degrees of freedom $\\nu = 4$, estimate for each class $c$ the parameters $(\\mu_c, \\Sigma_c)$ by maximizing the Student-$t$ likelihood using an iterative reweighting scheme consistent with the MLE, ensuring that at each iteration $\\Sigma_c$ is regularized as $\\Sigma_c \\leftarrow \\Sigma_c + \\lambda I_d$. Classify each test pixel by maximizing the corresponding class-conditional likelihood under the Student-$t$ family.\n\nFinal output format:\n- For each test case in order, produce a list of two lists: the first list contains the predicted class labels for the test pixels under the multivariate normal classifier, and the second list contains the predicted class labels for the same test pixels under the multivariate Student-$t$ classifier. Class labels must be integers in $\\{0,1\\}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Concretely, the output must have the form\n$[ [ [g_{1,1}, g_{1,2}, g_{1,3}], [t_{1,1}, t_{1,2}, t_{1,3}] ], [ [g_{2,1}, g_{2,2}, g_{2,3}], [t_{2,1}, t_{2,2}, t_{2,3}] ], [ [g_{3,1}, g_{3,2}, g_{3,3}], [t_{3,1}, t_{3,2}, t_{3,3}] ] ]$,\nwhere $g_{i,j}$ denotes the multivariate normal prediction and $t_{i,j}$ denotes the multivariate Student$-$t prediction for test pixel $j$ in test case $i$.\n\nAngles and physical units do not apply; all quantities are pure numbers. The output elements are integers as specified.",
            "solution": "The user-provided problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent setup for a standard problem in statistical pattern recognition.\n\n### **1. Theoretical Framework for Maximum Likelihood Classification**\n\nThe objective is to classify a pixel feature vector $x \\in \\mathbb{R}^d$ into one of two classes, $c \\in \\{0, 1\\}$. A generative classifier models the class-conditional probability density $p(x | c, \\theta_c)$, where $\\theta_c$ are the parameters of the model for class $c$. According to Bayes' theorem, the posterior probability of a class $c$ given a feature vector $x$ is:\n$$ P(c | x) = \\frac{p(x | c) P(c)}{p(x)} $$\nwhere $P(c)$ is the prior probability of class $c$. The decision rule is to choose the class that maximizes the posterior probability:\n$$ \\hat{c} = \\arg\\max_{c \\in \\{0, 1\\}} P(c | x) = \\arg\\max_{c \\in \\{0, 1\\}} p(x | c) P(c) $$\nThe term $p(x)$ is a normalizing constant independent of the class and can be ignored for maximization. The problem states that the prior probabilities are equal, i.e., $P(0) = P(1)$. Therefore, the decision rule simplifies to choosing the class that maximizes the class-conditional likelihood:\n$$ \\hat{c} = \\arg\\max_{c \\in \\{0, 1\\}} p(x | c, \\theta_c) $$\nFor numerical stability and computational convenience, it is equivalent to maximize the log-likelihood, $\\log p(x | c, \\theta_c)$. The parameters $\\theta_c$ for each class are estimated from the provided training samples for that class using Maximum Likelihood Estimation (MLE).\n\n### **2. Classifier 1: Multivariate Normal (MVN) Parametric Classifier**\n\nThis classifier assumes that the feature vectors for each class are drawn from a multivariate normal (or Gaussian) distribution.\n\n**2.1. Probability Density and Discriminant Function**\nThe probability density function (PDF) of a $d$-dimensional multivariate normal distribution with mean vector $\\mu \\in \\mathbb{R}^d$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is:\n$$ p(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right) $$\nThe log-likelihood, ignoring constant terms that are the same for all classes, is:\n$$ \\log p(x | \\mu, \\Sigma) \\propto -\\frac{1}{2} \\log(|\\Sigma|) - \\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) $$\nMaximizing the log-likelihood is equivalent to minimizing its negative. We define the discriminant function $g_c(x)$ for class $c$ with parameters $(\\mu_c, \\Sigma_c)$ as:\n$$ g_c(x) = \\log(|\\Sigma_c|) + (x - \\mu_c)^T \\Sigma_c^{-1} (x - \\mu_c) $$\nThe term $(x - \\mu_c)^T \\Sigma_c^{-1} (x - \\mu_c)$ is the squared Mahalanobis distance from $x$ to $\\mu_c$. The classification rule is then:\n$$ \\hat{c} = \\arg\\min_{c \\in \\{0, 1\\}} g_c(x) $$\n\n**2.2. Parameter Estimation (MLE)**\nGiven a set of $N$ training samples $\\{x_1, \\dots, x_N\\}$ for a single class, the MLE for the parameters $(\\mu, \\Sigma)$ are the sample mean and the sample covariance:\n$$ \\hat{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} x_i $$\n$$ \\hat{\\Sigma} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{\\mu})(x_i - \\hat{\\mu})^T $$\nThe problem requires applying a fixed diagonal regularization to the covariance estimate to ensure it is well-conditioned and invertible. With regularization constant $\\lambda = 10^{-4}$ and identity matrix $I_d$:\n$$ \\hat{\\Sigma}_{\\text{reg}} = \\hat{\\Sigma} + \\lambda I_d $$\nThese regularized estimates $\\hat{\\mu}_c$ and $\\hat{\\Sigma}_{\\text{reg},c}$ are computed for each class $c$ and used in the discriminant function $g_c(x)$.\n\n### **3. Classifier 2: Robust Multivariate Student-t (MVT) Parametric Classifier**\n\nThis classifier uses the multivariate Student-t distribution, which has heavier tails than the normal distribution, making it more robust to outliers.\n\n**3.1. Probability Density and Discriminant Function**\nThe PDF of a $d$-dimensional multivariate Student-t distribution with location parameter $\\mu \\in \\mathbb{R}^d$, scale matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, and $\\nu$ degrees of freedom is:\n$$ p(x | \\mu, \\Sigma, \\nu) = \\frac{\\Gamma((\\nu+d)/2)}{\\Gamma(\\nu/2) (\\nu\\pi)^{d/2} |\\Sigma|^{1/2}} \\left(1 + \\frac{1}{\\nu} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)^{-(\\nu+d)/2} $$\nwhere $\\Gamma(\\cdot)$ is the gamma function. The degrees of freedom $\\nu$ are fixed at $\\nu=4$. Taking the logarithm and ignoring constant terms, we get:\n$$ \\log p(x | \\mu, \\Sigma, \\nu) \\propto -\\frac{1}{2} \\log(|\\Sigma|) - \\frac{\\nu+d}{2} \\log\\left(1 + \\frac{1}{\\nu} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right) $$\nSimilar to the MVN case, we define a discriminant function $t_c(x)$ to be minimized:\n$$ t_c(x) = \\log(|\\Sigma_c|) + (\\nu+d) \\log\\left(1 + \\frac{1}{\\nu} (x - \\mu_c)^T \\Sigma_c^{-1} (x - \\mu_c)\\right) $$\nThe classification rule is:\n$$ \\hat{c} = \\arg\\min_{c \\in \\{0, 1\\}} t_c(x) $$\n\n**3.2. Parameter Estimation (Iterative Reweighting)**\nThe MLE for the Student-t distribution parameters do not have a closed-form solution and are found iteratively. The specified iterative reweighting scheme is an instance of the Expectation-Maximization (EM) algorithm. It works by viewing the Student-t distribution as a continuous mixture of Gaussian distributions. The algorithm for a given class is as follows:\n\nLet $\\{x_1, \\dots, x_N\\}$ be the training samples, $d$ be the dimension, and $\\nu$ be the degrees of freedom.\n1.  **Initialization**: Initialize parameters $\\mu^{(0)}$ and $\\Sigma^{(0)}$. A robust choice is to use the sample mean and regularized sample covariance as in the MVN case.\n2.  **Iteration**: For $k = 0, 1, 2, \\dots$ until convergence (or for a fixed number of iterations):\n    a.  **E-step (Weighting)**: For each sample $x_i$, compute its squared Mahalanobis distance using the current parameters:\n        $$ \\delta_i^{(k)} = (x_i - \\mu^{(k)})^T (\\Sigma^{(k)})^{-1} (x_i - \\mu^{(k)}) $$\n        Then, compute a weight for each sample:\n        $$ w_i^{(k+1)} = \\frac{\\nu+d}{\\nu + \\delta_i^{(k)}} $$\n        Outliers with large $\\delta_i^{(k)}$ will receive a smaller weight.\n    b.  **M-step (Update)**: Update the parameters using weighted MLE formulas for a Gaussian distribution:\n        $$ \\mu^{(k+1)} = \\frac{\\sum_{i=1}^N w_i^{(k+1)} x_i}{\\sum_{i=1}^N w_i^{(k+1)}} $$\n        $$ \\Sigma_{\\text{unreg}}^{(k+1)} = \\frac{1}{N} \\sum_{i=1}^N w_i^{(k+1)} (x_i - \\mu^{(k+1)})(x_i - \\mu^{(k+1)})^T $$\n        Apply the specified regularization at each step:\n        $$ \\Sigma^{(k+1)} = \\Sigma_{\\text{unreg}}^{(k+1)} + \\lambda I_d $$\n3.  **Termination**: The final parameters $(\\hat{\\mu}, \\hat{\\Sigma}_{\\text{reg}})$ are the values from the last iteration. These are computed for each class and used in the discriminant function $t_c(x)$.\n\n### **4. Tie-Breaking Rule**\nFor both classifiers, if the discriminant scores are exactly equal for both classes ($g_0(x) = g_1(x)$ or $t_0(x) = t_1(x)$), the tie is broken by assigning the pixel to the class with the larger index, which is class $1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run the classifiers on all test cases and print results.\n    \"\"\"\n    # --- Problem Parameters ---\n    LAMBDA_REG = 1e-4\n    NU_STUDENT_T = 4\n    ITERATIONS_STUDENT_T = 100\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        # Test case 1 (happy path with moderate outliers)\n        {\n            \"train0\": np.array([(0.18, 0.52, 0.31), (0.21, 0.49, 0.29), (0.20, 0.51, 0.33), (0.19, 0.50, 0.28), (0.90, 0.90, 0.90)]),\n            \"train1\": np.array([(0.48, 0.31, 0.21), (0.51, 0.29, 0.19), (0.50, 0.30, 0.22), (0.52, 0.28, 0.20), (0.00, 0.00, 0.00)]),\n            \"test_pixels\": np.array([(0.20, 0.50, 0.30), (0.50, 0.30, 0.20), (0.85, 0.85, 0.85)])\n        },\n        # Test case 2 (boundary condition with nearly singular covariances)\n        {\n            \"train0\": np.array([(0.10, 0.10, 0.10), (0.11, 0.11, 0.11), (0.09, 0.09, 0.09)]),\n            \"train1\": np.array([(0.60, 0.60, 0.60), (0.61, 0.61, 0.61), (0.59, 0.59, 0.59)]),\n            \"test_pixels\": np.array([(0.10, 0.10, 0.10), (0.60, 0.60, 0.60), (0.35, 0.35, 0.35)])\n        },\n        # Test case 3 (heavy contamination by extreme outliers)\n        {\n            \"train0\": np.array([(0.25, 0.55, 0.35), (0.22, 0.53, 0.32), (0.24, 0.54, 0.34), (0.23, 0.52, 0.31)]),\n            \"train1\": np.array([(0.55, 0.25, 0.15), (0.56, 0.24, 0.16), (10.00, 10.00, 10.00)]),\n            \"test_pixels\": np.array([(0.24, 0.53, 0.33), (0.55, 0.24, 0.16), (9.00, 9.00, 9.00)])\n        }\n    ]\n\n    # --- Helper Functions ---\n    def fit_gaussian(train_data):\n        n_samples, d = train_data.shape\n        mu = np.mean(train_data, axis=0)\n        # Use bias=True for MLE (division by N)\n        sigma = np.cov(train_data, rowvar=False, bias=True)\n        sigma_reg = sigma + LAMBDA_REG * np.identity(d)\n        return mu, sigma_reg\n\n    def fit_student_t(train_data):\n        n_samples, d = train_data.shape\n        # Initialize with robust Gaussian estimates\n        mu, sigma = fit_gaussian(train_data)\n        \n        for _ in range(ITERATIONS_STUDENT_T):\n            # E-Step: Calculate weights\n            inv_sigma = np.linalg.inv(sigma)\n            diffs = train_data - mu\n            mahalanobis_sq_dists = np.sum((diffs @ inv_sigma) * diffs, axis=1)\n            \n            weights = (NU_STUDENT_T + d) / (NU_STUDENT_T + mahalanobis_sq_dists)\n            \n            # M-Step: Update parameters\n            mu = np.sum(weights[:, np.newaxis] * train_data, axis=0) / np.sum(weights)\n            \n            diffs_new_mu = train_data - mu\n            weighted_diffs = weights[:, np.newaxis] * diffs_new_mu\n            sigma_unreg = (weighted_diffs.T @ diffs_new_mu) / n_samples\n            \n            sigma = sigma_unreg + LAMBDA_REG * np.identity(d)\n            \n        return mu, sigma\n\n    def classify(test_pixels, params_c0, params_c1, model_type):\n        mu0, sigma0 = params_c0\n        mu1, sigma1 = params_c1\n        \n        inv_sigma0 = np.linalg.inv(sigma0)\n        inv_sigma1 = np.linalg.inv(sigma1)\n        \n        log_det0 = np.linalg.slogdet(sigma0)[1]\n        log_det1 = np.linalg.slogdet(sigma1)[1]\n\n        predictions = []\n        for x in test_pixels:\n            mahal0 = (x - mu0).T @ inv_sigma0 @ (x - mu0)\n            mahal1 = (x - mu1).T @ inv_sigma1 @ (x - mu1)\n            \n            if model_type == 'gaussian':\n                score0 = log_det0 + mahal0\n                score1 = log_det1 + mahal1\n            elif model_type == 'student_t':\n                d = len(x)\n                score0 = log_det0 + (NU_STUDENT_T + d) * np.log(1 + mahal0 / NU_STUDENT_T)\n                score1 = log_det1 + (NU_STUDENT_T + d) * np.log(1 + mahal1 / NU_STUDENT_T)\n            \n            # Tie-breaking rule: choose class 1 if scores are equal\n            if score1 = score0:\n                predictions.append(1)\n            else:\n                predictions.append(0)\n        return predictions\n\n    # --- Main Logic ---\n    all_results = []\n    for case in test_cases:\n        train0 = case[\"train0\"]\n        train1 = case[\"train1\"]\n        test_pixels = case[\"test_pixels\"]\n\n        # Gaussian Classifier\n        params_g0 = fit_gaussian(train0)\n        params_g1 = fit_gaussian(train1)\n        preds_g = classify(test_pixels, params_g0, params_g1, 'gaussian')\n\n        # Student-t Classifier\n        params_t0 = fit_student_t(train0)\n        params_t1 = fit_student_t(train1)\n        preds_t = classify(test_pixels, params_t0, params_t1, 'student_t')\n        \n        all_results.append([preds_g, preds_t])\n\n    # Final print statement in the exact required format.\n    # The str() function on a list of lists creates the required nested structure.\n    # The replace call removes spaces for compact formatting.\n    print(str(all_results).replace(' ', ''))\n\nsolve()\n```"
        }
    ]
}