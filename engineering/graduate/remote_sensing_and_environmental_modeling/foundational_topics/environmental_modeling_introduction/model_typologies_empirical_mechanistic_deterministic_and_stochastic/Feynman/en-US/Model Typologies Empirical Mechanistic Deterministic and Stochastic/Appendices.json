{
    "hands_on_practices": [
        {
            "introduction": "To truly understand a model, we must first learn how to probe its behavior. A fundamental technique for this is sensitivity analysis, which quantifies how a model's output responds to small changes in its parameters. This practice  provides a concrete exercise in first-order sensitivity analysis by calculating the Jacobian matrix for a simplified, yet physically-grounded, radiative transfer model. By completing this exercise, you will gain hands-on experience in evaluating how atmospheric and surface properties jointly influence the top-of-atmosphere reflectance signal seen by a satellite.",
            "id": "3828596",
            "problem": "Consider a mechanistic deterministic forward model for remote sensing Top-Of-Atmosphere (TOA) reflectance $y$ over a Lambertian surface under single-scattering in a plane-parallel atmosphere. The model uses Beer–Lambert transmittances and an isotropic single-scattering path term and is written as\n$$\ny(\\rho,\\tau;\\mu_{0},\\mu,\\omega,P) \\;=\\; \\rho\\,\\exp\\!\\left(-\\frac{\\tau}{\\mu_{0}}\\right)\\exp\\!\\left(-\\frac{\\tau}{\\mu}\\right) \\;+\\; \\frac{\\omega\\,P}{4\\,\\mu_{0}\\,\\mu}\\,\\tau,\n$$\nwhere $\\rho$ is the surface albedo (Lambertian bidirectional reflectance factor), $\\tau$ is the aerosol optical depth, $\\mu_{0}=\\cos(\\theta_{0})$ and $\\mu=\\cos(\\theta)$ are the solar and view direction cosines defined by the solar zenith angle $\\theta_{0}$ and the view zenith angle $\\theta$, $\\omega$ is the single-scattering albedo, and $P$ is the phase function value (set to $P=1$ for isotropic scattering). The angle unit is degrees.\n\n- Geometry and optical properties: $\\theta_{0}=60^{\\circ}$, $\\theta=0^{\\circ}$, $\\omega=0.95$, $P=1$, so that $\\mu_{0}=\\cos(60^{\\circ})$ and $\\mu=\\cos(0^{\\circ})$.\n- Parameter vector: $\\theta=\\begin{pmatrix}\\rho \\\\ \\tau\\end{pmatrix}$ with nominal values $\\rho^{\\star}=0.2$ and $\\tau^{\\star}=0.3$.\n- Deterministic perturbation: $\\Delta\\theta=\\begin{pmatrix}\\Delta\\rho \\\\ \\Delta\\tau\\end{pmatrix}$ with $\\Delta\\rho=+0.01$ and $\\Delta\\tau=-0.02$.\n\nStarting from the Beer–Lambert law for transmittance and the single-scattering approximation for path reflectance in a plane-parallel atmosphere, derive the Jacobian row vector $J(\\theta^{\\star})=\\begin{pmatrix}\\frac{\\partial y}{\\partial \\rho} & \\frac{\\partial y}{\\partial \\tau}\\end{pmatrix}\\bigg|_{\\theta^{\\star}}$ and evaluate the deterministic first-order sensitivity $\\Delta y \\approx J(\\theta^{\\star})\\,\\Delta\\theta$. Round your final numerical result for $\\Delta y$ to four significant figures. Express the final reflectance change as a unitless decimal. Then briefly interpret whether the reflectance is locally more sensitive to $\\rho$ or $\\tau$ under the given conditions and comment on the parametric stability implied by the magnitude of $\\Delta y$ relative to $y(\\theta^{\\star})$.",
            "solution": "The problem is evaluated to be scientifically grounded, well-posed, objective, and complete. It represents a standard first-order sensitivity analysis of a simplified radiative transfer model, a common task in remote sensing and environmental modeling. All necessary data and definitions are provided, and no contradictions or physically implausible assumptions are present. Therefore, the problem is valid, and a solution can be derived.\n\nThe forward model for Top-Of-Atmosphere (TOA) reflectance $y$ is given by:\n$$\ny(\\rho,\\tau;\\mu_{0},\\mu,\\omega,P) \\;=\\; \\rho\\,\\exp\\!\\left(-\\frac{\\tau}{\\mu_{0}}\\right)\\exp\\!\\left(-\\frac{\\tau}{\\mu}\\right) \\;+\\; \\frac{\\omega\\,P}{4\\,\\mu_{0}\\,\\mu}\\,\\tau\n$$\nThis expression can be simplified by combining the exponential terms, representing the two-way atmospheric transmittance (downward and upward paths):\n$$\ny(\\rho,\\tau;\\mu_{0},\\mu,\\omega,P) \\;=\\; \\rho\\,\\exp\\!\\left(-\\tau\\left(\\frac{1}{\\mu_{0}} + \\frac{1}{\\mu}\\right)\\right) \\;+\\; \\frac{\\omega\\,P}{4\\,\\mu_{0}\\,\\mu}\\,\\tau\n$$\n\nFirst, we substitute the given constant values for geometry and optical properties:\n- Solar zenith angle $\\theta_{0} = 60^{\\circ}$, so $\\mu_{0} = \\cos(60^{\\circ}) = 0.5$.\n- View zenith angle $\\theta = 0^{\\circ}$, so $\\mu = \\cos(0^{\\circ}) = 1$.\n- Single-scattering albedo $\\omega = 0.95$.\n- Isotropic phase function value $P = 1$.\n\nSubstituting these values into the model equation simplifies it to a function of only $\\rho$ and $\\tau$:\n$$\ny(\\rho, \\tau) = \\rho\\,\\exp\\!\\left(-\\tau\\left(\\frac{1}{0.5} + \\frac{1}{1}\\right)\\right) + \\frac{0.95 \\times 1}{4 \\times 0.5 \\times 1}\\,\\tau\n$$\n$$\ny(\\rho, \\tau) = \\rho\\,\\exp(-3\\tau) + \\frac{0.95}{2}\\,\\tau\n$$\n$$\ny(\\rho, \\tau) = \\rho\\,\\exp(-3\\tau) + 0.475\\,\\tau\n$$\n\nThe goal is to compute the first-order change $\\Delta y$ due to a perturbation $\\Delta\\theta = \\begin{pmatrix}\\Delta\\rho \\\\ \\Delta\\tau\\end{pmatrix}$ around a nominal point $\\theta^{\\star} = \\begin{pmatrix}\\rho^{\\star} \\\\ \\tau^{\\star}\\end{pmatrix}$. This is given by the Taylor expansion truncated to the first order:\n$$\n\\Delta y \\approx \\frac{\\partial y}{\\partial \\rho}\\bigg|_{\\theta^{\\star}}\\Delta\\rho + \\frac{\\partial y}{\\partial \\tau}\\bigg|_{\\theta^{\\star}}\\Delta\\tau\n$$\nThis can be written in matrix form as $\\Delta y \\approx J(\\theta^{\\star})\\,\\Delta\\theta$, where $J(\\theta^{\\star})$ is the Jacobian row vector evaluated at the nominal point $\\theta^{\\star}$.\n\nThe Jacobian row vector is $J(\\rho, \\tau) = \\begin{pmatrix}\\frac{\\partial y}{\\partial \\rho} & \\frac{\\partial y}{\\partial \\tau}\\end{pmatrix}$. We compute the partial derivatives of $y(\\rho, \\tau)$:\n\n1.  Partial derivative with respect to surface albedo $\\rho$:\n    $$\n    \\frac{\\partial y}{\\partial \\rho} = \\frac{\\partial}{\\partial \\rho} \\left( \\rho\\,\\exp(-3\\tau) + 0.475\\,\\tau \\right) = \\exp(-3\\tau)\n    $$\n\n2.  Partial derivative with respect to aerosol optical depth $\\tau$:\n    $$\n    \\frac{\\partial y}{\\partial \\tau} = \\frac{\\partial}{\\partial \\tau} \\left( \\rho\\,\\exp(-3\\tau) + 0.475\\,\\tau \\right) = \\rho\\,\\exp(-3\\tau)(-3) + 0.475 = -3\\rho\\,\\exp(-3\\tau) + 0.475\n    $$\n\nSo, the Jacobian vector is:\n$$\nJ(\\rho, \\tau) = \\begin{pmatrix} \\exp(-3\\tau) & -3\\rho\\,\\exp(-3\\tau) + 0.475 \\end{pmatrix}\n$$\n\nNext, we evaluate this Jacobian at the nominal parameter values $\\theta^{\\star} = \\begin{pmatrix}\\rho^{\\star} \\\\ \\tau^{\\star}\\end{pmatrix} = \\begin{pmatrix}0.2 \\\\ 0.3\\end{pmatrix}$:\n$$\nJ(\\theta^{\\star}) = J(0.2, 0.3) = \\begin{pmatrix} \\exp(-3 \\times 0.3) & -3(0.2)\\,\\exp(-3 \\times 0.3) + 0.475 \\end{pmatrix}\n$$\n$$\nJ(\\theta^{\\star}) = \\begin{pmatrix} \\exp(-0.9) & -0.6\\,\\exp(-0.9) + 0.475 \\end{pmatrix}\n$$\n\nNow, we compute the first-order sensitivity $\\Delta y$ using the perturbation vector $\\Delta\\theta = \\begin{pmatrix}\\Delta\\rho \\\\ \\Delta\\tau\\end{pmatrix} = \\begin{pmatrix}+0.01 \\\\ -0.02\\end{pmatrix}$:\n$$\n\\Delta y \\approx J(\\theta^{\\star})\\,\\Delta\\theta = \\begin{pmatrix} \\exp(-0.9) & -0.6\\,\\exp(-0.9) + 0.475 \\end{pmatrix} \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix}\n$$\n$$\n\\Delta y \\approx (0.01)\\exp(-0.9) + (-0.02)(-0.6\\,\\exp(-0.9) + 0.475)\n$$\n$$\n\\Delta y \\approx 0.01\\,\\exp(-0.9) + 0.012\\,\\exp(-0.9) - (0.02)(0.475)\n$$\n$$\n\\Delta y \\approx 0.022\\,\\exp(-0.9) - 0.0095\n$$\nUsing the numerical value for $\\exp(-0.9) \\approx 0.40656966$:\n$$\n\\Delta y \\approx 0.022 \\times 0.40656966 - 0.0095\n$$\n$$\n\\Delta y \\approx 0.0089445325 - 0.0095\n$$\n$$\n\\Delta y \\approx -0.0005554675\n$$\nRounding the result for $\\Delta y$ to four significant figures gives:\n$$\n\\Delta y \\approx -0.0005555\n$$\n\n**Interpretation:**\n1.  **Sensitivity:** To determine whether the reflectance $y$ is locally more sensitive to $\\rho$ or $\\tau$, we compare the magnitudes of the Jacobian components at $\\theta^{\\star}$:\n    -   $J_{\\rho} = \\frac{\\partial y}{\\partial \\rho} \\bigg|_{\\theta^{\\star}} = \\exp(-0.9) \\approx 0.4066$\n    -   $J_{\\tau} = \\frac{\\partial y}{\\partial \\tau} \\bigg|_{\\theta^{\\star}} = -0.6\\,\\exp(-0.9) + 0.475 \\approx -0.2440 + 0.475 = 0.2310$\n    Since $|J_{\\rho}| \\approx 0.4066 > |J_{\\tau}| \\approx 0.2310$, the TOA reflectance is locally more sensitive to a unit change in surface albedo $\\rho$ than to a unit change in aerosol optical depth $\\tau$.\n\n2.  **Parametric Stability:** To assess stability, we compare the magnitude of the predicted change $|\\Delta y|$ to the nominal reflectance value $y(\\theta^{\\star})$.\n    First, we calculate $y(\\theta^{\\star})$:\n    $$\n    y(\\theta^{\\star}) = y(0.2, 0.3) = 0.2\\,\\exp(-3 \\times 0.3) + 0.475 \\times 0.3\n    $$\n    $$\n    y(\\theta^{\\star}) = 0.2\\,\\exp(-0.9) + 0.1425 \\approx 0.2 \\times 0.4066 + 0.1425 = 0.08132 + 0.1425 = 0.22382\n    $$\n    The relative change is:\n    $$\n    \\frac{|\\Delta y|}{|y(\\theta^{\\star})|} \\approx \\frac{0.0005555}{0.22382} \\approx 0.002482\n    $$\n    This corresponds to a change of approximately $0.25\\%$. The relative input perturbations were $\\Delta\\rho/\\rho^{\\star} = 0.01/0.2 = 5\\%$ and $|\\Delta\\tau/\\tau^{\\star}| = |-0.02/0.3| \\approx 6.7\\%$. Since the relative output change ($0.25\\%$) is much smaller than the relative input perturbations, the model is considered parametrically stable around this operating point.",
            "answer": "$$\n\\boxed{-0.0005555}\n$$"
        },
        {
            "introduction": "While sensitivity tells us how much a model's output changes with a parameter, practical identifiability asks a more critical question: can we actually estimate this parameter from noisy measurements? A parameter's effect, no matter how large in theory, is irrelevant if it is drowned out by measurement noise. This practice  directly addresses this by contrasting the parameter identifiability of an empirical polynomial model with a mechanistic one, introducing a noise-constrained sensitivity score to make the comparison quantitative. This exercise will sharpen your ability to assess not just if a model is sensitive, but if its parameters are practically recoverable in a real-world setting.",
            "id": "3828554",
            "problem": "Consider a remote sensing application where the target variable is remote sensing reflectance, defined as the ratio of water-leaving radiance to downwelling irradiance, with units of steradian inverse ($\\mathrm{sr}^{-1}$). You will compare an empirical deterministic model and a mechanistic deterministic model for reflectance as functions of a single environmental driver $x$ (e.g., chlorophyll concentration in $\\mathrm{mg}\\,\\mathrm{m}^{-3}$), while explicitly accounting for a stochastic observation process. The goal is to apply sensitivity analysis to determine the most identifiable parameter under measurement noise constraints by ranking the magnitude of parameter sensitivities.\n\nFundamental base:\n- Deterministic model definition: a deterministic model maps inputs $x$ and parameters $\\boldsymbol{\\theta}$ to a reflectance $R(x;\\boldsymbol{\\theta})$.\n- Stochastic observation model: measurements $y$ are modeled as $y = R(x;\\boldsymbol{\\theta}) + \\varepsilon$, where $\\varepsilon$ is a random variable with $ \\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2)$ and $\\sigma_y$ is the standard deviation of measurement noise in units of $\\mathrm{sr}^{-1}$.\n- Sensitivity definition: the local parameter sensitivity of $R$ with respect to parameter $\\theta_i$ at a given $x$ is the partial derivative $\\partial R / \\partial \\theta_i$ evaluated at the specified parameter values.\n\nModel typologies to be used:\n- Empirical deterministic model: a polynomial approximation $R_{\\mathrm{E}}(x;\\boldsymbol{\\theta}_{\\mathrm{E}})$ with parameter vector $\\boldsymbol{\\theta}_{\\mathrm{E}} = (\\theta_{\\mathrm{E},0}, \\theta_{\\mathrm{E},1}, \\theta_{\\mathrm{E},2})$ and scalar input $x$.\n- Mechanistic deterministic model: a simplified radiative transfer-inspired saturation-plus-reciprocal form $R_{\\mathrm{M}}(x;\\boldsymbol{\\theta}_{\\mathrm{M}})$ with parameter vector $\\boldsymbol{\\theta}_{\\mathrm{M}} = (\\alpha, \\beta, \\gamma, \\delta)$ and scalar input $x$.\n\nYour task:\n1. Starting from the above definitions and the standard rules of differential calculus, derive the analytic expressions for the partial derivatives $\\partial R_{\\mathrm{E}}/\\partial \\theta_{\\mathrm{E},i}$ and $\\partial R_{\\mathrm{M}}/\\partial \\theta_{\\mathrm{M},i}$ for the following model forms:\n   - Empirical deterministic model: $R_{\\mathrm{E}}(x;\\boldsymbol{\\theta}_{\\mathrm{E}}) = \\theta_{\\mathrm{E},0} + \\theta_{\\mathrm{E},1}\\,x + \\theta_{\\mathrm{E},2}\\,x^2$.\n   - Mechanistic deterministic model: $R_{\\mathrm{M}}(x;\\boldsymbol{\\theta}_{\\mathrm{M}}) = \\alpha\\left(1 - e^{-\\beta x}\\right) + \\dfrac{\\gamma}{1 + \\delta x}$.\n2. Using the stochastic observation model $y = R + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2)$, define the noise-constrained identifiability ranking score for parameter $\\theta_i$ as $S_i = \\left|\\dfrac{\\partial R}{\\partial \\theta_i}\\right|\\,/\\,\\sigma_y$. This score measures the magnitude of the parameter’s effect on the reflectance relative to the observation noise, in units of $(\\mathrm{sr}^{-1})/(\\mathrm{sr}^{-1})$.\n3. For each test case, compute $S_i$ for all parameters of both models and select the index of the most identifiable parameter (the parameter with the largest $S_i$). In case of ties, select the smallest index. Use zero-based indexing for parameter indices. No angles are involved. All reflectance and noise quantities are in $\\mathrm{sr}^{-1}$; no unit conversion is needed.\n4. Implement a program that evaluates the following four test cases and outputs the indices of the most identifiable parameters as a single list of integers in the order: empirical index then mechanistic index for each test case. The program must produce the final output exactly as a single line containing a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Test case $1$ (happy path):\n  - $x = 2.0$\n  - $\\boldsymbol{\\theta}_{\\mathrm{E}} = (0.005, 0.002, 0.0001)$\n  - $\\boldsymbol{\\theta}_{\\mathrm{M}} = (0.02, 0.7, 0.015, 0.1)$\n  - $\\sigma_y = 0.001$ $\\mathrm{sr}^{-1}$\n- Test case $2$ (large noise):\n  - $x = 10.0$\n  - $\\boldsymbol{\\theta}_{\\mathrm{E}} = (0.005, 0.002, 0.0001)$\n  - $\\boldsymbol{\\theta}_{\\mathrm{M}} = (0.02, 0.7, 0.015, 0.1)$\n  - $\\sigma_y = 0.02$ $\\mathrm{sr}^{-1}$\n- Test case $3$ (boundary input):\n  - $x = 0.0$\n  - $\\boldsymbol{\\theta}_{\\mathrm{E}} = (0.005, 0.002, 0.0001)$\n  - $\\boldsymbol{\\theta}_{\\mathrm{M}} = (0.02, 0.7, 0.015, 0.1)$\n  - $\\sigma_y = 0.001$ $\\mathrm{sr}^{-1}$\n- Test case $4$ (tie in empirical sensitivities):\n  - $x = 1.0$\n  - $\\boldsymbol{\\theta}_{\\mathrm{E}} = (0.000, 0.003, 0.001)$\n  - $\\boldsymbol{\\theta}_{\\mathrm{M}} = (0.1, 0.0, 0.1, 0.0)$\n  - $\\sigma_y = 0.01$ $\\mathrm{sr}^{-1}$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$), where the elements are integers in the order $[\\text{empirical}_1,\\text{mechanistic}_1,\\text{empirical}_2,\\text{mechanistic}_2,\\text{empirical}_3,\\text{mechanistic}_3,\\text{empirical}_4,\\text{mechanistic}_4]$ corresponding to the four test cases.",
            "solution": "The problem has been validated and is deemed valid. It is scientifically grounded in standard principles of mathematical modeling and sensitivity analysis, well-posed with a clear objective and sufficient data, and expressed in objective, unambiguous language. The solution proceeds by first deriving the analytical sensitivities and then implementing a computational procedure to evaluate the specified test cases.\n\nThe objective is to determine the most identifiable parameter for two different models of remote sensing reflectance, $R$, under a stochastic observation process. Identifiability is quantified by a noise-constrained sensitivity score. This requires deriving the partial derivatives of each model with respect to its parameters.\n\n**1. Derivation of Analytic Sensitivities**\n\nThe local sensitivity of a model output $R$ to a parameter $\\theta_i$ is defined as the partial derivative $\\frac{\\partial R}{\\partial \\theta_i}$. We derive these sensitivities for both the empirical and mechanistic models.\n\n**1.1. Empirical Deterministic Model**\n\nThe empirical model is a quadratic polynomial of the form:\n$$\nR_{\\mathrm{E}}(x;\\boldsymbol{\\theta}_{\\mathrm{E}}) = \\theta_{\\mathrm{E},0} + \\theta_{\\mathrm{E},1}\\,x + \\theta_{\\mathrm{E},2}\\,x^2\n$$\nwhere $\\boldsymbol{\\theta}_{\\mathrm{E}} = (\\theta_{\\mathrm{E},0}, \\theta_{\\mathrm{E},1}, \\theta_{\\mathrm{E},2})$ is the parameter vector. The sensitivities are found by taking the partial derivative with respect to each parameter in turn, treating other parameters and the input $x$ as constants.\n\nThe sensitivity to $\\theta_{\\mathrm{E},0}$ is:\n$$\n\\frac{\\partial R_{\\mathrm{E}}}{\\partial \\theta_{\\mathrm{E},0}} = \\frac{\\partial}{\\partial \\theta_{\\mathrm{E},0}} \\left( \\theta_{\\mathrm{E},0} + \\theta_{\\mathrm{E},1}\\,x + \\theta_{\\mathrm{E},2}\\,x^2 \\right) = 1\n$$\nThis indicates that a change in $\\theta_{\\mathrm{E},0}$ results in a constant offset to the reflectance, independent of $x$.\n\nThe sensitivity to $\\theta_{\\mathrm{E},1}$ is:\n$$\n\\frac{\\partial R_{\\mathrm{E}}}{\\partial \\theta_{\\mathrm{E},1}} = \\frac{\\partial}{\\partial \\theta_{\\mathrm{E},1}} \\left( \\theta_{\\mathrm{E},0} + \\theta_{\\mathrm{E},1}\\,x + \\theta_{\\mathrm{E},2}\\,x^2 \\right) = x\n$$\nThis sensitivity is linearly dependent on the input variable $x$.\n\nThe sensitivity to $\\theta_{\\mathrm{E},2}$ is:\n$$\n\\frac{\\partial R_{\\mathrm{E}}}{\\partial \\theta_{\\mathrm{E},2}} = \\frac{\\partial}{\\partial \\theta_{\\mathrm{E},2}} \\left( \\theta_{\\mathrm{E},0} + \\theta_{\\mathrm{E},1}\\,x + \\theta_{\\mathrm{E},2}\\,x^2 \\right) = x^2\n$$\nThis sensitivity depends on the square of the input variable $x$.\n\n**1.2. Mechanistic Deterministic Model**\n\nThe mechanistic model is given by a saturation-plus-reciprocal form:\n$$\nR_{\\mathrm{M}}(x;\\boldsymbol{\\theta}_{\\mathrm{M}}) = \\alpha\\left(1 - e^{-\\beta x}\\right) + \\dfrac{\\gamma}{1 + \\delta x}\n$$\nwhere $\\boldsymbol{\\theta}_{\\mathrm{M}} = (\\alpha, \\beta, \\gamma, \\delta)$ is the parameter vector. The sensitivities are derived using the sum rule, product rule, and chain rule of differentiation.\n\nThe sensitivity to $\\alpha$ is:\n$$\n\\frac{\\partial R_{\\mathrm{M}}}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left[ \\alpha\\left(1 - e^{-\\beta x}\\right) + \\frac{\\gamma}{1 + \\delta x} \\right] = 1 - e^{-\\beta x}\n$$\nThis pertains to the saturation term's amplitude.\n\nThe sensitivity to $\\beta$ is found using the chain rule for the exponential term:\n$$\n\\frac{\\partial R_{\\mathrm{M}}}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left[ \\alpha\\left(1 - e^{-\\beta x}\\right) \\right] = \\alpha \\left( -e^{-\\beta x} \\cdot \\frac{\\partial}{\\partial \\beta}(-\\beta x) \\right) = \\alpha \\left( -e^{-\\beta x} \\cdot (-x) \\right) = \\alpha x e^{-\\beta x}\n$$\nThis relates to the rate of saturation.\n\nThe sensitivity to $\\gamma$ is:\n$$\n\\frac{\\partial R_{\\mathrm{M}}}{\\partial \\gamma} = \\frac{\\partial}{\\partial \\gamma} \\left[ \\frac{\\gamma}{1 + \\delta x} \\right] = \\frac{1}{1 + \\delta x}\n$$\nThis pertains to the amplitude of the reciprocal term.\n\nThe sensitivity to $\\delta$ is found using the power rule and chain rule on the term $(1 + \\delta x)^{-1}$:\n$$\n\\frac{\\partial R_{\\mathrm{M}}}{\\partial \\delta} = \\frac{\\partial}{\\partial \\delta} \\left[ \\gamma(1 + \\delta x)^{-1} \\right] = \\gamma \\left( -1 \\cdot (1 + \\delta x)^{-2} \\cdot \\frac{\\partial}{\\partial \\delta}(1 + \\delta x) \\right) = \\gamma \\left( -(1 + \\delta x)^{-2} \\cdot x \\right) = \\frac{-\\gamma x}{(1 + \\delta x)^2}\n$$\nThis relates to the decay rate of the reciprocal term.\n\n**2. Identifiability Score and Evaluation**\n\nThe problem defines a noise-constrained identifiability ranking score for a parameter $\\theta_i$ as:\n$$\nS_i = \\frac{\\left|\\frac{\\partial R}{\\partial \\theta_i}\\right|}{\\sigma_y}\n$$\nwhere $\\sigma_y$ is the standard deviation of the measurement noise. This score normalizes the magnitude of the model's sensitivity to a parameter by the level of observation noise. A higher score implies that the effect of a change in the parameter is large relative to the noise, making the parameter more \"identifiable\" from noisy data.\n\nFor each test case, we perform the following steps for both the empirical and mechanistic models:\n1.  Evaluate the vector of analytical sensitivities, $[\\frac{\\partial R}{\\partial \\theta_0}, \\frac{\\partial R}{\\partial \\theta_1}, \\dots]$, using the given values of $x$ and the model parameters $\\boldsymbol{\\theta}$.\n2.  Calculate the vector of scores, $[S_0, S_1, \\dots]$, by taking the absolute value of each sensitivity and dividing by the given $\\sigma_y$.\n3.  Determine the index of the largest score in the vector. This index corresponds to the most identifiable parameter for the given conditions. The problem specifies that in case of a tie, the smallest index should be chosen.\n\nThis procedure is implemented programmatically to process the four test cases provided in the problem statement. The final output is an ordered list of these indices.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# SciPy is not needed for this problem as numpy contains all required functions.\n\ndef solve():\n    \"\"\"\n    Computes the index of the most identifiable parameter for empirical and\n    mechanistic models across several test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (happy path)\n        {'x': 2.0, 'theta_E': (0.005, 0.002, 0.0001), 'theta_M': (0.02, 0.7, 0.015, 0.1), 'sigma_y': 0.001},\n        # Test case 2 (large noise)\n        {'x': 10.0, 'theta_E': (0.005, 0.002, 0.0001), 'theta_M': (0.02, 0.7, 0.015, 0.1), 'sigma_y': 0.02},\n        # Test case 3 (boundary input)\n        {'x': 0.0, 'theta_E': (0.005, 0.002, 0.0001), 'theta_M': (0.02, 0.7, 0.015, 0.1), 'sigma_y': 0.001},\n        # Test case 4 (tie in empirical sensitivities)\n        {'x': 1.0, 'theta_E': (0.000, 0.003, 0.001), 'theta_M': (0.1, 0.0, 0.1, 0.0), 'sigma_y': 0.01},\n    ]\n\n    results = []\n\n    def calculate_empirical_sensitivities(x):\n        \"\"\"\n        Calculates the sensitivities for the empirical model.\n        dR/d(theta_E0) = 1\n        dR/d(theta_E1) = x\n        dR/d(theta_E2) = x^2\n        \"\"\"\n        return np.array([1.0, x, x**2])\n\n    def calculate_mechanistic_sensitivities(x, theta_M):\n        \"\"\"\n        Calculates the sensitivities for the mechanistic model.\n        theta_M = (alpha, beta, gamma, delta)\n        \"\"\"\n        alpha, beta, gamma, delta = theta_M\n        \n        # dR/d(alpha) = 1 - exp(-beta * x)\n        sens_alpha = 1.0 - np.exp(-beta * x)\n        \n        # dR/d(beta) = alpha * x * exp(-beta * x)\n        sens_beta = alpha * x * np.exp(-beta * x)\n        \n        # dR/d(gamma) = 1 / (1 + delta * x)\n        sens_gamma = 1.0 / (1.0 + delta * x)\n        \n        # dR/d(delta) = -gamma * x / (1 + delta * x)^2\n        sens_delta = -gamma * x / ((1.0 + delta * x)**2)\n        \n        return np.array([sens_alpha, sens_beta, sens_gamma, sens_delta])\n\n    for case in test_cases:\n        # Unpack case data\n        x = case['x']\n        theta_E = case['theta_E']\n        theta_M = case['theta_M']\n        sigma_y = case['sigma_y']\n\n        # 1. Empirical Model Calculation\n        # Calculate sensitivities\n        sens_E = calculate_empirical_sensitivities(x)\n        # Calculate scores S_i = |sensitivity| / sigma_y\n        scores_E = np.abs(sens_E) / sigma_y\n        # Find index of max score. np.argmax handles ties by taking the first occurrence.\n        most_identifiable_E_idx = np.argmax(scores_E)\n        results.append(most_identifiable_E_idx)\n        \n        # 2. Mechanistic Model Calculation\n        # Calculate sensitivities\n        sens_M = calculate_mechanistic_sensitivities(x, theta_M)\n        # Calculate scores\n        scores_M = np.abs(sens_M) / sigma_y\n        # Find index of max score\n        most_identifiable_M_idx = np.argmax(scores_M)\n        results.append(most_identifiable_M_idx)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Environmental systems and our knowledge of them are inherently stochastic. Consequently, model inputs and parameters are often not known perfectly but are better described by probability distributions. This exercise  shifts our focus from deterministic sensitivity to stochastic uncertainty propagation. You will derive and apply the first-order approximation to calculate how the variance of an input variable, Leaf Area Index (LAI), propagates through the nonlinear Beer-Lambert model to create variance in the output. This practice is essential for quantifying confidence in model predictions and understanding the limits of linear approximations in nonlinear systems.",
            "id": "3828545",
            "problem": "Consider a single-parameter mechanistic canopy radiation model widely used in remote sensing of vegetation, where the fraction of absorbed photosynthetically active radiation (fAPAR) is related to the Leaf Area Index (LAI) through the Beer–Lambert law: $$f(X) = 1 - \\exp(-k X),$$ where $X$ is LAI and $k$ is the light extinction coefficient. Suppose an airborne or satellite pixel contains sub-pixel variability in LAI modeled as a random variable $X$ with mean $\\mathbb{E}[X] = \\mu$ and variance $\\operatorname{Var}(X) = \\sigma^{2}$. For a particular biome and sun-sensor geometry, take $k = 0.5$ (unitless). Assume field data for this site give $\\mu = 3$ and $\\sigma^{2} = 0.16$ for LAI, both unitless.\n\nTasks:\n1. Starting from a first-order Taylor expansion of $f(X)$ about $X = \\mu$ and the definition of variance, derive the first-order uncertainty propagation expression $$\\operatorname{Var}(f(X)) \\approx \\big(f'(\\mu)\\big)^{2}\\,\\operatorname{Var}(X).$$ Avoid invoking any pre-derived propagation rules beyond the Taylor expansion and properties of variance.\n2. Using the derived expression, write the approximate variance of $f(X)$ for this model in terms of $\\mu$, $\\sigma^{2}$, and $k$, and then evaluate it numerically for $k = 0.5$, $\\mu = 3$, and $\\sigma^{2} = 0.16$.\n3. To assess the validity of the linear approximation for this specific nonlinearity, define the dimensionless validity indicator $$\\varepsilon = \\frac{|f''(\\mu)|\\,\\sigma}{|f'(\\mu)|}.$$ Compute $\\varepsilon$ for the given $k$, $\\mu$, and $\\sigma^{2}$. Interpretations such as “small $\\varepsilon$ implies high validity” may be used in your reasoning, but for your final answer only report $\\varepsilon$ as a pure number (no units). No rounding is required for the final answer.\n\nNote: If you additionally assume $X$ is Gaussian, you may compute the exact variance of $f(X)$ to compare against the first-order approximation as part of your assessment, but you must still derive the first-order expression from first principles as requested in part 1. The final answer to submit is the single numerical value of $\\varepsilon$ only, with no units.",
            "solution": "The problem as stated is scientifically sound, mathematically well-posed, and contains all necessary information to proceed with a solution. The model is a standard application of the Beer-Lambert law in remote sensing, and the tasks involve standard mathematical procedures in uncertainty analysis. Therefore, the problem is valid.\n\nThe core of the problem is to analyze the propagation of uncertainty for the function $f(X) = 1 - \\exp(-kX)$, where $X$ is a random variable with mean $\\mathbb{E}[X] = \\mu$ and variance $\\operatorname{Var}(X) = \\sigma^2$.\n\n**Task 1: Derivation of the First-Order Uncertainty Propagation Expression**\n\nWe start with a first-order Taylor series expansion of the function $f(X)$ around the point $X = \\mu$:\n$$f(X) \\approx f(\\mu) + f'(\\mu)(X-\\mu)$$\nwhere $f'(\\mu)$ is the first derivative of $f(X)$ with respect to $X$, evaluated at $X = \\mu$.\n\nThe variance of a function $g(X)$ is defined as $\\operatorname{Var}(g(X)) = \\mathbb{E}[(g(X) - \\mathbb{E}[g(X)])^2]$. To find the approximate variance of $f(X)$, we apply the variance operator, $\\operatorname{Var}(\\cdot)$, to the Taylor expansion.\n$$\\operatorname{Var}(f(X)) \\approx \\operatorname{Var}\\left(f(\\mu) + f'(\\mu)(X-\\mu)\\right)$$\nWe can rearrange the terms inside the variance operator:\n$$\\operatorname{Var}(f(X)) \\approx \\operatorname{Var}\\left(f'(\\mu)X + (f(\\mu) - f'(\\mu)\\mu)\\right)$$\nWe use the fundamental property of variance, which states that for any random variable $Z$ and constants $a$ and $b$, $\\operatorname{Var}(aZ + b) = a^2\\operatorname{Var}(Z)$. In our expression, $X$ is the random variable, $f'(\\mu)$ is a constant (as it is the derivative evaluated at a specific point $\\mu$), and the entire term $(f(\\mu) - f'(\\mu)\\mu)$ is also a constant.\nApplying this property, we identify $a = f'(\\mu)$ and $b = f(\\mu) - f'(\\mu)\\mu$. This gives:\n$$\\operatorname{Var}(f'(\\mu)X + (f(\\mu) - f'(\\mu)\\mu)) = \\left(f'(\\mu)\\right)^2 \\operatorname{Var}(X)$$\nTherefore, by substituting back, we arrive at the first-order approximation for the propagation of variance:\n$$\\operatorname{Var}(f(X)) \\approx \\left(f'(\\mu)\\right)^2 \\operatorname{Var}(X)$$\nThis is the required derivation based on a first-order Taylor expansion and the definition of variance.\n\n**Task 2: Application to the fAPAR Model and Numerical Evaluation**\n\nThe given model is $f(X) = 1 - \\exp(-kX)$. First, we must find its derivative with respect to $X$.\n$$f'(X) = \\frac{d}{dX} \\left(1 - \\exp(-kX)\\right) = 0 - \\exp(-kX) \\cdot (-k) = k\\exp(-kX)$$\nNext, we evaluate this derivative at the mean LAI, $X = \\mu$:\n$$f'(\\mu) = k\\exp(-k\\mu)$$\nSubstituting this expression into the derived variance formula from Task 1, where $\\operatorname{Var}(X) = \\sigma^2$:\n$$\\operatorname{Var}(f(X)) \\approx \\left(k\\exp(-k\\mu)\\right)^2 \\sigma^2 = k^2\\exp(-2k\\mu)\\sigma^2$$\nThis is the approximate variance of $f(X)$ in terms of $\\mu$, $\\sigma^2$, and $k$.\n\nNow, we evaluate this expression numerically using the given values: $k = 0.5$, $\\mu = 3$, and $\\sigma^2 = 0.16$.\n$$\\operatorname{Var}(f(X)) \\approx (0.5)^2 \\exp(-2 \\cdot 0.5 \\cdot 3) \\cdot 0.16$$\n$$\\operatorname{Var}(f(X)) \\approx 0.25 \\cdot \\exp(-3) \\cdot 0.16$$\n$$\\operatorname{Var}(f(X)) \\approx 0.04 \\exp(-3)$$\nNumerically, this value is approximately $0.04 \\times 0.049787 \\approx 0.001991$.\n\n**Task 3: Computation of the Dimensionless Validity Indicator $\\varepsilon$**\n\nThe validity indicator is defined as $\\varepsilon = \\frac{|f''(\\mu)|\\,\\sigma}{|f'(\\mu)|}$. This requires the second derivative of $f(X)$.\nWe differentiate $f'(X) = k\\exp(-kX)$ with respect to $X$:\n$$f''(X) = \\frac{d}{dX} \\left(k\\exp(-kX)\\right) = k \\cdot \\exp(-kX) \\cdot (-k) = -k^2\\exp(-kX)$$\nNow, we evaluate the first and second derivatives at $X = \\mu$:\n$$f'(\\mu) = k\\exp(-k\\mu)$$\n$$f''(\\mu) = -k^2\\exp(-k\\mu)$$\nWe take the absolute value of the second derivative. Since $k^2$ and $\\exp(-k\\mu)$ are always positive, this gives:\n$$|f''(\\mu)| = |-k^2\\exp(-k\\mu)| = k^2\\exp(-k\\mu)$$\nThe standard deviation $\\sigma$ is the square root of the variance: $\\sigma = \\sqrt{\\sigma^2} = \\sqrt{0.16} = 0.4$.\n\nNow we substitute these components into the expression for $\\varepsilon$:\n$$\\varepsilon = \\frac{\\left(k^2\\exp(-k\\mu)\\right) \\cdot \\sigma}{k\\exp(-k\\mu)}$$\nWe can simplify this expression by canceling common terms. The term $\\exp(-k\\mu)$ appears in both the numerator and the denominator and cancels out. One factor of $k$ also cancels.\n$$\\varepsilon = \\frac{k^2 \\sigma}{k} = k\\sigma$$\nThis elegant result shows that for this specific exponential model, the validity of the linear approximation depends only on the product of the extinction coefficient and the standard deviation of the input variable.\n\nFinally, we compute the numerical value of $\\varepsilon$ using $k = 0.5$ and $\\sigma = 0.4$:\n$$\\varepsilon = 0.5 \\cdot 0.4 = 0.2$$\nThe value of $\\varepsilon$ is $0.2$. This dimensionless number provides a measure of the importance of the second-order (nonlinear) term relative to the first-order (linear) term in the Taylor expansion. A value of $\\varepsilon \\ll 1$ is typically taken to indicate that the first-order approximation is reasonable. Here, $\\varepsilon = 0.2$ suggests that the nonlinearity is present but the linear approximation is likely adequate for many purposes, though not perfectly accurate.",
            "answer": "$$\\boxed{0.2}$$"
        }
    ]
}