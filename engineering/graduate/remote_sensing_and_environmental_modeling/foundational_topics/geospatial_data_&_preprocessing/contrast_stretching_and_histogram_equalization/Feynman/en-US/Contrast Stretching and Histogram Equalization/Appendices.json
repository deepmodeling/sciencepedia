{
    "hands_on_practices": [
        {
            "introduction": "To truly master histogram equalization, it is essential to move beyond the abstract formula and understand its mechanics step-by-step. This practice provides a concrete, small-scale histogram, allowing you to manually trace the transformation from raw pixel counts to cumulative probabilities and, finally, to the new, equalized digital numbers. By performing this core calculation, you will build a foundational understanding of how this powerful non-linear contrast enhancement technique works from first principles. ",
            "id": "3802113",
            "problem": "An airborne multispectral imaging radiometer used in remote sensing of vegetation canopies records radiance as an $8$-bit Digital Number (DN) after on-board linear quantization. In environmental modeling workflows, a common pre-processing step is global histogram equalization to enhance contrast while preserving the order of DN levels. Consider a small subset of a flightline where only the following DN levels are present in a single band, with their corresponding pixel counts (the rest of the $256$ possible DN levels do not occur in this subset):\n\n- $DN=12$ has $184$ pixels.\n- $DN=35$ has $792$ pixels.\n- $DN=87$ has $115$ pixels.\n- $DN=123$ has $58$ pixels.\n- $DN=156$ has $1610$ pixels.\n- $DN=201$ has $927$ pixels.\n- $DN=245$ has $1214$ pixels.\n\nAssume global histogram equalization to an output dynamic range of $L'=256$ levels is applied to this subset, treating the empirical distribution formed by the listed counts as the image histogram. Using only the definitions of the histogram, the cumulative distribution, and a monotone mapping defined by the normalized cumulative distribution, do the following:\n\n- Compute the cumulative counts in ascending DN order for the listed bins.\n- From first principles, derive the discrete equalization mapping based on the cumulative distribution and apply it to compute the equalized output level for each listed DN bin.\n\nFinally, report the equalized output level for the bin at $DN=201$ as your single numerical answer. The final answer is a single integer level; no unit specification or significant-figure rounding is required beyond the conventional rounding inherent to the equalization mapping.",
            "solution": "The problem requires the computation of an equalized output Digital Number (DN) for a specific input DN, based on a provided sparse histogram from a remote sensing image subset. The process involves histogram equalization, a standard technique for contrast enhancement in image processing.\n\nFirst, we must formalize the problem by defining the components of histogram equalization from first principles.\n\nLet the set of DN levels present in the image be $\\{r_j\\}$, where $j$ indexes the distinct, ordered levels. Let $n(r_j)$ be the number of pixels (pixel count) for each level $r_j$. The image is described as $8$-bit, so the full dynamic range is $L=256$ levels, from $0$ to $255$. The problem specifies an output dynamic range of $L' = 256$ levels, which corresponds to an output range of $[0, 255]$.\n\nThe total number of pixels in the image subset, $N$, is the sum of all pixel counts:\n$$N = \\sum_{j} n(r_j)$$\n\nThe provided data for the non-zero DN bins are:\n- $r_1 = 12$, $n(r_1) = 184$\n- $r_2 = 35$, $n(r_2) = 792$\n- $r_3 = 87$, $n(r_3) = 115$\n- $r_4 = 123$, $n(r_4) = 58$\n- $r_5 = 156$, $n(r_5) = 1610$\n- $r_6 = 201$, $n(r_6) = 927$\n- $r_7 = 245$, $n(r_7) = 1214$\n\nFirst, we calculate the total number of pixels, $N$:\n$$N = 184 + 792 + 115 + 58 + 1610 + 927 + 1214 = 4900$$\n\nThe next step, as requested, is to compute the cumulative counts. The cumulative count for a given DN level $r_k$, denoted $C(r_k)$, is the sum of pixel counts for all levels less than or equal to $r_k$.\n$$C(r_k) = \\sum_{j: r_j \\le r_k} n(r_j)$$\n\nWe calculate the cumulative counts for each present DN level in ascending order:\n- For $r_1=12$: $C(12) = n(12) = 184$\n- For $r_2=35$: $C(35) = n(12) + n(35) = 184 + 792 = 976$\n- For $r_3=87$: $C(87) = C(35) + n(87) = 976 + 115 = 1091$\n- For $r_4=123$: $C(123) = C(87) + n(123) = 1091 + 58 = 1149$\n- For $r_5=156$: $C(156) = C(123) + n(156) = 1149 + 1610 = 2759$\n- For $r_6=201$: $C(201) = C(156) + n(201) = 2759 + 927 = 3686$\n- For $r_7=245$: $C(245) = C(201) + n(245) = 3686 + 1214 = 4900$\n\nThe cumulative count for the highest DN level equals the total number of pixels $N$, which serves as a consistency check.\n\nThe principle of histogram equalization is to apply a monotone mapping $T(r)$ that transforms the input pixel levels $r$ to output levels $s$ such that the histogram of the output levels is as flat (uniform) as possible. For discrete levels, this transformation is defined using the normalized cumulative distribution function (CDF). The CDF at a level $r_k$ is given by $C(r_k)/N$. The mapping function scales this CDF to the new dynamic range $[0, L'-1]$.\n\nThe discrete equalization mapping is therefore:\n$$s_k = T(r_k) = \\text{round}\\left( (L' - 1) \\times \\frac{C(r_k)}{N} \\right)$$\nwhere `round` denotes rounding to the nearest integer.\n\nGiven $L' = 256$, the maximum output level is $L'-1 = 255$. The total number of pixels is $N=4900$. The transformation becomes:\n$$s_k = \\text{round}\\left( 255 \\times \\frac{C(r_k)}{4900} \\right)$$\n\nThe problem asks for the equalized output level for the bin at $DN=201$. We use the input level $r_k = 201$. From our prior calculation, the cumulative count for this level is $C(201) = 3686$.\n\nSubstituting these values into the transformation function:\n$$s_{201} = \\text{round}\\left( 255 \\times \\frac{3686}{4900} \\right)$$\n\nWe perform the calculation:\n$$s_{201} = \\text{round}\\left( 255 \\times 0.75224489... \\right)$$\n$$s_{201} = \\text{round}\\left( 191.82244897... \\right)$$\n\nApplying conventional rounding to the nearest integer, we get:\n$$s_{201} = 192$$\n\nThus, the input DN level of $201$ is mapped to the output DN level of $192$ after histogram equalization.",
            "answer": "$$\\boxed{192}$$"
        },
        {
            "introduction": "Theoretical exercises often use clean, idealized data, but real-world remote sensing images are rarely so simple. They frequently contain special pixel values, such as a `nodata` code for areas obscured by clouds or outside the sensor's swath. This practice challenges you to quantify the analytical bias introduced when such `nodata` values are incorrectly included in a histogram equalization, providing a powerful demonstration of why data masking is a critical preprocessing step for maintaining the integrity of quantitative analyses. ",
            "id": "3802037",
            "problem": "A remote sensing production pipeline creates an $8$-bit surface reflectance image with $L=256$ discrete digital levels $r \\in \\{0,1,2,\\dots,255\\}$, where $r=0$ is reserved as a nodata code representing clouds and saturated radiances. In a particular tile, the nodata fraction is $f_{0}=0.35$ of all pixels, and the valid reflectance pixels are uniformly distributed across the levels $r \\in \\{1,2,\\dots,255\\}$.\n\nTwo histogram equalization pipelines are applied to the valid pixels:\n\n- Pipeline A computes the cumulative mass for equalization by including the nodata code $r=0$ among the counts, treating all pixels as part of the cumulative mass.\n- Pipeline B computes the cumulative mass by excluding the nodata code through a mask, so only valid pixels contribute to the cumulative mass.\n\nFor a valid pixel with level $r \\in \\{1,2,\\dots,255\\}$, let $z_{\\text{A}}(r)$ denote the output level from Pipeline A and $z_{\\text{B}}(r)$ denote the output level from Pipeline B, both obtained by classic histogram equalization that maps $r$ to an output level proportional to the cumulative mass up to $r$. Let $R$ be a random valid pixel level drawn from the uniform distribution on $\\{1,2,\\dots,255\\}$.\n\nUsing only fundamental definitions of probability mass, cumulative distribution, and the mapping principle of histogram equalization, compute the expected bias introduced by including nodata in the cumulative mass, namely the quantity $\\mathbb{E}\\left[z_{\\text{A}}(R)-z_{\\text{B}}(R)\\right]$. Express your final answer as a number of output digital levels. No units are required, and no rounding is necessary.\n\nFinally, provide a brief justification, grounded in the derivation, for why masks are required in remote sensing production pipelines when performing histogram-based contrast adjustments on images that contain nodata values.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It is a formalizable problem in statistical image processing. We may proceed with the solution.\n\nThe problem asks for the expected bias $\\mathbb{E}\\left[z_{\\text{A}}(R)-z_{\\text{B}}(R)\\right]$ introduced by including nodata values in a histogram equalization calculation. We begin by formalizing the probability distributions and transformations for each pipeline.\n\nThe image is $8$-bit, so there are $L=256$ digital levels, $r \\in \\{0, 1, 2, \\dots, 255\\}$. The level $r=0$ is a nodata code, and its fraction of the total pixels is $f_0 = 0.35$. The remaining fraction of pixels, $1-f_0 = 0.65$, are valid and are uniformly distributed across the levels $r \\in \\{1, 2, \\dots, 255\\}$.\n\nLet $p(r)$ be the probability mass function (PMF) for a randomly selected pixel from the entire image.\nThe probability of a pixel being nodata is $p(0) = f_0 = 0.35$.\nFor the $L-1 = 255$ valid levels, the remaining probability $1-f_0$ is distributed uniformly. Thus, for any $r \\in \\{1, 2, \\dots, L-1\\}$:\n$$p(r) = \\frac{1-f_0}{L-1} = \\frac{0.65}{255}$$\n\nThe standard histogram equalization transformation maps an input level $r$ to an output level $z(r)$ that is proportional to the cumulative mass function (CMF) of the pixel levels. The mapping is given by $z(r) = (L-1) \\times CMF(r)$.\n\nFirst, we analyze Pipeline B, which correctly excludes the nodata pixels.\n**Pipeline B: Nodata Excluded**\nThis pipeline considers only the valid pixels, for which the levels are uniformly distributed on $\\{1, 2, \\dots, 255\\}$. The PMF for this conditional distribution, let's call it $p_v(r)$, is:\n$$p_v(r) = \\frac{1}{L-1} \\quad \\text{for } r \\in \\{1, 2, \\dots, L-1\\}$$\nThe CMF for the valid pixels, $CMF_{\\text{B}}(r)$, is the sum of probabilities up to level $r$:\n$$CMF_{\\text{B}}(r) = \\sum_{k=1}^{r} p_v(k) = \\sum_{k=1}^{r} \\frac{1}{L-1} = \\frac{r}{L-1}$$\nThe output level $z_{\\text{B}}(r)$ is then:\n$$z_{\\text{B}}(r) = (L-1) \\times CMF_{\\text{B}}(r) = (L-1) \\left(\\frac{r}{L-1}\\right) = r$$\nThis result is expected: applying histogram equalization to an already uniform distribution results in an identity mapping, as the data is already \"equalized\".\n\n**Pipeline A: Nodata Included**\nThis pipeline incorrectly includes all pixels, including nodata, in its statistical basis. The CMF, $CMF_{\\text{A}}(r)$, is calculated from the overall PMF, $p(r)$. We are interested in the transformation for valid input pixels, so we evaluate for $r \\in \\{1, 2, \\dots, L-1\\}$:\n$$CMF_{\\text{A}}(r) = \\sum_{k=0}^{r} p(k) = p(0) + \\sum_{k=1}^{r} p(k) = f_0 + \\sum_{k=1}^{r} \\frac{1-f_0}{L-1} = f_0 + \\frac{r(1-f_0)}{L-1}$$\nThe output level $z_{\\text{A}}(r)$ is:\n$$z_{\\text{A}}(r) = (L-1) \\times CMF_{\\text{A}}(r) = (L-1) \\left( f_0 + \\frac{r(1-f_0)}{L-1} \\right) = (L-1)f_0 + r(1-f_0)$$\n\n**Expected Bias Calculation**\nThe bias for a specific valid pixel level $r$ is the difference $\\Delta(r) = z_{\\text{A}}(r) - z_{\\text{B}}(r)$.\n$$\\Delta(r) = \\left( (L-1)f_0 + r(1-f_0) \\right) - r = (L-1)f_0 + r - rf_0 - r = f_0(L-1-r)$$\nWe need to find the expected value of this bias, $\\mathbb{E}[\\Delta(R)]$, where $R$ is a random variable representing a valid pixel level. The problem states $R$ is drawn from the uniform distribution on $\\{1, 2, \\dots, 255\\}$, i.e., $\\{1, 2, \\dots, L-1\\}$.\nThe expectation is:\n$$\\mathbb{E}[\\Delta(R)] = \\mathbb{E}[f_0(L-1-R)] = f_0 \\mathbb{E}[L-1-R] = f_0 \\left( (L-1) - \\mathbb{E}[R] \\right)$$\nFor a discrete uniform random variable on the integers $\\{a, b\\}$, the expected value is $\\frac{a+b}{2}$. Here, $R$ is uniform on $\\{1, L-1\\}$, so its expectation is:\n$$\\mathbb{E}[R] = \\frac{1 + (L-1)}{2} = \\frac{L}{2}$$\nSubstituting this into the expression for the expected bias:\n$$\\mathbb{E}[\\Delta(R)] = f_0 \\left( (L-1) - \\frac{L}{2} \\right) = f_0 \\left( \\frac{2L-2-L}{2} \\right) = f_0 \\left( \\frac{L-2}{2} \\right) = f_0 \\left( \\frac{L}{2} - 1 \\right)$$\nNow, we substitute the given numerical values, $L=256$ and $f_0=0.35$:\n$$\\mathbb{E}[\\Delta(R)] = 0.35 \\left( \\frac{256}{2} - 1 \\right) = 0.35 (128 - 1) = 0.35 \\times 127$$\n$$0.35 \\times 127 = 44.45$$\nThe expected bias is $44.45$ digital levels.\n\n**Justification for Masks in Remote Sensing**\nThe derivation provides a clear, quantitative justification for using masks. The transformation in Pipeline A, $z_{\\text{A}}(r) = (L-1)f_0 + (1-f_0)r$, shows that the presence of nodata values fundamentally alters the contrast stretch.\nWith $f_0 = 0.35$ and $L=256$, the mapping is $z_{\\text{A}}(r) = 255 \\times 0.35 + (1-0.35)r = 89.25 + 0.65r$.\nThe correct mapping, $z_{\\text{B}}(r)=r$, preserves the full dynamic range $[1, 255]$ of the valid data. In contrast, Pipeline A maps the same input range $[1, 255]$ to the output range $[89.25 + 0.65, 89.25 + 0.65 \\times 255] = [89.9, 255]$.\nThis means that all valid pixels, particularly the darker ones, are significantly brightened, and the entire data signal is compressed into a smaller upper portion of the available dynamic range (from a width of $254$ to $255-89.9 \\approx 165.1$). This crushes the contrast in the darker parts of the scene, obscuring details. This artifact arises because the large pixel count at the nodata level $r=0$ creates a large initial jump in the CMF, $CMF_{\\text{A}}(0) = f_0 = 0.35$. Consequently, the dimmest valid pixel $r=1$ is mapped to a bright level of $z_A(1) \\approx (L-1)f_0 \\approx 89.9$.\nMasks are therefore required to isolate the valid pixels from nodata, ensuring that the statistical basis for histogram-based adjustments reflects the true data distribution. This allows for a meaningful contrast enhancement that reveals features across the full dynamic range of the actual surface reflectances, which is the primary goal of the technique. The calculated expected bias of $44.45$ quantifies the average magnitude of this undesirable distortion.",
            "answer": "$$\n\\boxed{44.45}\n$$"
        },
        {
            "introduction": "A correct mathematical formula is only as good as its computational implementation, a principle that is especially true in remote sensing where datasets are massive. This hands-on coding exercise explores the crucial issue of numerical stability when calculating the empirical cumulative distribution function ($\\hat{F}$) for millions of pixels. You will implement and contrast a naive, error-prone algorithm against a robust method, gaining practical insight into how floating-point arithmetic can impact scientific results and why stable algorithm design is paramount for large-scale data processing. ",
            "id": "3802107",
            "problem": "You are given a discrete intensity domain for a single-band remote sensing image and must compute a numerically stable empirical cumulative distribution function used in histogram equalization and contrast stretching at very large sample sizes. Let the intensity levels be $r \\in \\{0,1,\\dots,L-1\\}$ for some integer $L \\ge 2$. Consider a set of $N$ observed pixels with digital numbers in this domain, summarized by a histogram $\\{h_k\\}_{k=0}^{L-1}$ where $h_k$ is the nonnegative integer count in bin $k$ and $\\sum_{k=0}^{L-1} h_k = N$. The empirical cumulative distribution function (CDF) is defined for bin index $k$ as\n$$\n\\hat{F}(k) \\triangleq \\frac{1}{N} \\sum_{i=0}^{k} h_i.\n$$\nIn the histogram equalization workflow, $\\hat{F}$ is subsequently used to map intensities to a target domain via a monotonic transformation, and in contrast stretching via quantile-based limits; both steps are sensitive to numerical error when $N$ is large.\n\nStarting solely from the following fundamental bases:\n- The definition of the empirical cumulative distribution function as a cumulative sum of frequencies divided by the total count.\n- The rules of probability on discrete finite sample spaces.\n- The IEEE-754 floating-point rounding model where summing many small positive floating values in sequence can accumulate rounding error.\n\nDerive an algorithmic approach to compute $\\hat{F}$ that remains numerically stable for very large $N$ (for example, $N \\ge 10^7$) and avoids error amplification from cumulative rounding. Then implement and validate it on synthetic, scientifically plausible histograms that mimic remote sensing brightness distributions.\n\nYour implementation must:\n1. Implement two estimators of $\\hat{F}$ from the same histogram $\\{h_k\\}$:\n   - A naive single-pass floating-point estimator that first forms bin probabilities $p_k = h_k / N$ in a low-precision floating-point type and then computes $\\hat{F}(k) = \\sum_{i=0}^k p_i$ by sequential summation in that same low precision.\n   - A numerically stable estimator that first accumulates the cumulative counts $\\sum_{i=0}^k h_i$ exactly in integer arithmetic of sufficiently wide width and then performs a single division by $N$ in high precision floating-point to produce $\\hat{F}(k)$.\n2. Use $\\hat{F}$ to define the histogram equalization mapping $y_k$ for each bin $k$ as the real-valued transform $y_k \\gets \\hat{F}(k)$, which lies in $[0,1]$.\n3. Validate and quantify stability and correctness on the following test suite. Each test case specifies $(L, N, \\text{probability model})$ used to generate a histogram by a multinomial draw of $N$ samples:\n   - Case A (happy path mixture, moderate dynamic range): $L = 4096$, $N = 2000000$. The true underlying $p_k$ arises from a mixture of three truncated Gaussian components over $\\{0,\\dots,L-1\\}$ with plausibly remote-sensing-like modes and spreads. Generate the histogram by a multinomial draw from these $p_k$. Output the maximum absolute difference, as a floating-point number, between the naive and stable $\\hat{F}$ over all bins. This must be expressed as a bare decimal (no percent sign).\n   - Case B (extremely skewed with a dominant dark bin): $L = 4096$, $N = 1000000$. Let $p_0 = 0.999$ and distribute the remaining mass $0.001$ uniformly over bins $\\{1,\\dots,L-1\\}$. Generate counts multinomially. Output the absolute difference, as a floating-point number, between the naive and stable $\\hat{F}$ evaluated at $k=0$.\n   - Case C (uniform reflectance): $L = 4096$, $N = 500000$. Set $p_k = 1/L$ for all $k$. Using the stable $\\hat{F}$ and the equalization mapping $y_k = \\hat{F}(k)$, compute the mean mapped value $\\bar{y} = \\sum_{k=0}^{L-1} y_k \\, (h_k/N)$. Output the absolute deviation $|\\bar{y} - 0.5|$ as a floating-point number.\n   - Case D (multi-scale mixture stressing small-bin probabilities): $L = 4096$, $N = 1500000$. Use a mixture of three truncated Gaussian components with substantially different standard deviations to create very small per-bin probabilities in the tails. Generate the histogram. Output the maximum absolute difference, as a floating-point number, between the naive and stable $\\hat{F}$ over all bins.\n   - Case E (monotonicity property): Using the histogram from Case A and the stable estimator, check whether the mapping $k \\mapsto y_k$ is non-decreasing for all $k$; output the Boolean result.\n4. For all random draws, fix the random seed internally to guarantee reproducibility.\n5. Requirements for the final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order: $[\\text{result}_{A}, \\text{result}_{B}, \\text{result}_{C}, \\text{result}_{D}, \\text{result}_{E}]$. Here, $\\text{result}_{A}$, $\\text{result}_{B}$, $\\text{result}_{C}$, and $\\text{result}_{D}$ are floating-point numbers, and $\\text{result}_{E}$ is a Boolean.\n\nNotes:\n- There are no physical units in this problem because intensities are dimensionless digital numbers. All angles, if any, are to be measured in radians; however, no angles are required here.\n- All probabilities must be represented as decimals (not percentages).\n- Your algorithm must be self-contained and must not read any input. The probability models for the Gaussian mixtures should be constructed in a scientifically plausible way (for example, by setting means and standard deviations as fractions of $L$ and normalizing a truncated Gaussian on the discrete grid).",
            "solution": "The problem requires the derivation and implementation of a numerically stable algorithm for computing the empirical cumulative distribution function (CDF) of a discrete intensity distribution, a common task in remote sensing image processing for contrast stretching and histogram equalization. We will first validate the problem statement, then derive the algorithmic approaches, and finally detail the implementation for the required test cases.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n- Intensity domain: $r \\in \\{0, 1, \\dots, L-1\\}$ with integer $L \\ge 2$.\n- Sample size: $N$ observed pixels.\n- Histogram: $\\{h_k\\}_{k=0}^{L-1}$, where $h_k$ is the non-negative integer count for bin $k$, and $\\sum_{k=0}^{L-1} h_k = N$.\n- Empirical CDF definition: $\\hat{F}(k) \\triangleq \\frac{1}{N} \\sum_{i=0}^{k} h_i$.\n- Algorithmic bases: The definition of the empirical CDF, rules of probability, and the IEEE-754 floating-point rounding model.\n- Task 1 (Implementation): Implement two estimators for $\\hat{F}(k)$: a naive single-pass floating-point version and a numerically stable integer-based accumulation version.\n- Task 2 (Application): Use $\\hat{F}(k)$ for the histogram equalization mapping $y_k \\gets \\hat{F}(k)$.\n- Task 3 (Validation Suite):\n    - Case A: $(L, N) = (4096, 2 \\times 10^6)$, 3-component Gaussian mixture. Output: $\\max_k |\\hat{F}_{naive}(k) - \\hat{F}_{stable}(k)|$.\n    - Case B: $(L, N) = (4096, 10^6)$, highly skewed distribution with $p_0=0.999$. Output: $|\\hat{F}_{naive}(0) - \\hat{F}_{stable}(0)|$.\n    - Case C: $(L, N) = (4096, 5 \\times 10^5)$, uniform distribution. Output: $|\\bar{y} - 0.5|$, where $\\bar{y}$ is the mean of the mapped values.\n    - Case D: $(L, N) = (4096, 1.5 \\times 10^6)$, multi-scale Gaussian mixture. Output: $\\max_k |\\hat{F}_{naive}(k) - \\hat{F}_{stable}(k)|$.\n    - Case E: Using Case A data, check if $k \\mapsto y_k$ is non-decreasing. Output: Boolean result.\n- Task 4 (Reproducibility): Fix the random seed for all random draws.\n- Task 5 (Output Format): A single-line list of results: $[\\text{result}_{A}, \\text{result}_{B}, \\text{result}_{C}, \\text{result}_{D}, \\text{result}_{E}]$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically and mathematically sound. It addresses a fundamental issue in numerical computation—the accumulation of floating-point rounding error—within the valid and relevant context of statistical estimation for image processing. The parameters, models (Gaussian mixtures, skewed distributions), and tasks are well-defined, objective, and realistic for the domain of remote sensing. The problem is self-contained, with no missing information or contradictions. It is well-posed, leading to a unique and verifiable set of numerical results.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. We proceed to the solution.\n\n### Derivation of Algorithmic Approaches\n\nThe core task is to compute the empirical CDF, $\\hat{F}(k) = \\frac{1}{N} \\sum_{i=0}^{k} h_i$, for $k \\in \\{0, 1, \\dots, L-1\\}$. We analyze two computational strategies as stipulated.\n\n**1. The Naive Floating-Point Estimator**\n\nThis approach follows the definition of a sum of probabilities. First, one computes the empirical probability $p_k$ for each bin $k$:\n$$\np_k = \\frac{h_k}{N}\n$$\nThese probabilities are then summed cumulatively:\n$$\n\\hat{F}_{naive}(k) = \\sum_{i=0}^{k} p_i\n$$\nThe numerical instability arises from the properties of floating-point arithmetic. If these operations are performed in a fixed, finite precision (e.g., 32-bit float), each $p_i$ may already contain a small rounding error from the division. More significantly, the sequential summation $\\sum p_i$ is prone to error accumulation. As the running sum $S_j = \\sum_{i=0}^{j} p_i$ grows, adding a subsequent small value $p_{j+1}$ can lead to a loss of precision. In floating-point representation, numbers are stored as a significand and an exponent. To add two numbers with different exponents, the smaller one's significand must be shifted, potentially losing its least significant bits. For a long sum of positive numbers, where the partial sum becomes much larger than subsequent terms, this effect, known as \"absorption,\" can become pronounced, leading to a significant deviation from the true mathematical sum.\n\nThe algorithm is as follows:\n1. For each $k \\in \\{0, \\dots, L-1\\}$, compute $p_k = (float)h_k / (float)N$ using low-precision floating-point arithmetic.\n2. Initialize an accumulator $S = 0.0$ in the same low precision.\n3. For $k = 0, \\dots, L-1$:\n   a. Update the accumulator: $S \\gets S + p_k$.\n   b. Store the result: $\\hat{F}_{naive}(k) = S$.\n\n**2. The Numerically Stable Estimator**\n\nThis approach is designed to circumvent the issues of floating-point summation. It leverages the fact that the histogram counts $h_k$ are exact integers. The key insight is to perform the entire summation using exact integer arithmetic before any floating-point division occurs.\n\nFirst, we compute the cumulative counts, $C_k$:\n$$\nC_k = \\sum_{i=0}^{k} h_i\n$$\nSince each $h_i$ is an integer and their sum $N$ can be large (e.g., $> 10^7$), this summation must be performed using an integer data type of sufficient width to hold $N$ without overflow (e.g., a 64-bit integer). This summation is exact.\n\nOnly after the full cumulative count $C_k$ is obtained for each bin $k$ do we perform a division to get the final CDF value:\n$$\n\\hat{F}_{stable}(k) = \\frac{C_k}{N}\n$$\nIn this method, for each $k$, there is only a single floating-point operation (division). The error from this single operation is bounded by the machine epsilon of the floating-point type used (preferably a high-precision one, like a 64-bit float). This avoids the *accumulation* of errors inherent in the naive approach.\n\nThe algorithm is as follows:\n1. Initialize a cumulative count accumulator $C = 0$ using a wide integer type.\n2. For $k = 0, \\dots, L-1$:\n   a. Update the accumulator: $C \\gets C + h_k$.\n   b. Compute the CDF value $\\hat{F}_{stable}(k) = (double)C / (double)N$ using high-precision floating-point arithmetic.\n\n**Application to Histogram Equalization**\n\nHistogram equalization is a technique to improve image contrast by redistributing pixel intensities. The transformation function is precisely the CDF of the intensity distribution. For a discrete histogram, the value of the empirical CDF, $\\hat{F}(k)$, gives the transformed intensity value for all pixels originally in bin $k$. The mapping is:\n$$\ny_k = \\hat{F}(k)\n$$\nThis maps the original intensities from domain $\\{0, \\dots, L-1\\}$ to new real-valued levels in $[0, 1]$. A crucial property of this transformation is that it must be monotonic (non-decreasing), meaning if intensity $k_1 < k_2$, then their transformed values must satisfy $y_{k_1} \\le y_{k_2}$. This is guaranteed by the definition of the CDF, as $\\hat{F}(k+1) - \\hat{F}(k) = h_{k+1}/N \\ge 0$, since histogram counts $h_{k+1}$ are non-negative. The stable estimator preserves this property exactly.\n\n### Test Case Implementation Plan\n\nWe will now implement these two algorithms and evaluate them on the specified test suite. A fixed random seed will ensure reproducibility. Histograms will be generated using a multinomial distribution draw.\n\n- **For Gaussian Mixtures (Cases A, D):** We construct a probability vector $\\{p_k\\}_{k=0}^{L-1}$ where $p_k$ is proportional to a weighted sum of Gaussian probability density functions evaluated at $k$. The resulting vector is normalized to sum to $1$.\n- **For Low-Precision (Naive Estimator):** We will use the `numpy.float32` data type.\n- **For High-Precision (Stable Estimator):** We will use Python's native `int` (or `numpy.int64`) for accumulation and `numpy.float64` for the final division and result storage.\n\nThe five required outputs will be calculated as described in the problem statement and formatted into a single list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes and validates two estimators for the empirical CDF of image histograms,\n    quantifying the numerical stability of each approach.\n    \"\"\"\n    # Set a global seed for reproducibility of random draws.\n    SEED = 42\n    RNG = np.random.default_rng(SEED)\n\n    def generate_histogram(L, N, p_model, rng):\n        \"\"\"\n        Generates a histogram by a multinomial draw from a given probability model.\n\n        Args:\n            L (int): Number of intensity levels (bins).\n            N (int): Total number of samples (pixels).\n            p_model (dict): Specification for the probability distribution.\n            rng (np.random.Generator): A NumPy random number generator.\n\n        Returns:\n            np.ndarray: The generated histogram of counts {h_k}.\n        \"\"\"\n        k = np.arange(L)\n        p = np.zeros(L, dtype=np.float64)\n\n        if p_model['type'] == 'gaussian_mixture':\n            means = p_model['means']\n            stds = p_model['stds']\n            weights = p_model['weights']\n            \n            pdf_sum = np.zeros(L, dtype=np.float64)\n            for w, mu, sigma in zip(weights, means, stds):\n                pdf_sum += w * norm.pdf(k, loc=mu, scale=sigma)\n            \n            # Normalize to ensure probabilities sum to 1.\n            p = pdf_sum / np.sum(pdf_sum)\n            \n        elif p_model['type'] == 'skewed':\n            # Highly skewed distribution.\n            rem_prob = 1.0 - 0.999\n            p.fill(rem_prob / (L - 1))\n            p[0] = 0.999\n            # Renormalize to correct any floating point inaccuracies.\n            p /= np.sum(p)\n            \n        elif p_model['type'] == 'uniform':\n            # Uniform distribution.\n            p.fill(1.0 / L)\n            \n        else:\n            raise ValueError(\"Unknown probability model type\")\n\n        # Generate histogram via multinomial draw.\n        h = rng.multinomial(N, p)\n        return h\n\n    def naive_cdf(h, N):\n        \"\"\"\n        Computes the naive empirical CDF using low-precision floats (float32).\n        This method is prone to cumulative rounding errors.\n        \"\"\"\n        p_naive = h.astype(np.float32) / np.float32(N)\n        f_naive = np.cumsum(p_naive)\n        return f_naive\n\n    def stable_cdf(h, N):\n        \"\"\"\n        Computes the stable empirical CDF using exact integer accumulation\n        followed by a single high-precision division.\n        \"\"\"\n        # Summation is done with 64-bit integers to prevent any overflow.\n        cumulative_counts = np.cumsum(h.astype(np.int64))\n        # Division is done in high precision (float64 is the default).\n        f_stable = cumulative_counts / np.float64(N)\n        return f_stable\n\n    # --- Test Case A: Happy path mixture, moderate dynamic range ---\n    L_A, N_A = 4096, 2000000\n    p_model_A = {\n        'type': 'gaussian_mixture',\n        'means': [0.1 * L_A, 0.4 * L_A, 0.7 * L_A],\n        'stds': [0.05 * L_A, 0.1 * L_A, 0.08 * L_A],\n        'weights': [0.3, 0.5, 0.2]\n    }\n    h_A = generate_histogram(L_A, N_A, p_model_A, RNG)\n    f_naive_A = naive_cdf(h_A, N_A)\n    f_stable_A = stable_cdf(h_A, N_A)\n    # The difference is computed in high precision (float64) for accuracy.\n    result_A = np.max(np.abs(f_naive_A - f_stable_A))\n\n    # --- Test Case B: Extremely skewed with a dominant dark bin ---\n    L_B, N_B = 4096, 1000000\n    p_model_B = {'type': 'skewed'}\n    h_B = generate_histogram(L_B, N_B, p_model_B, RNG)\n    f_naive_B = naive_cdf(h_B, N_B)\n    f_stable_B = stable_cdf(h_B, N_B)\n    result_B = np.abs(f_naive_B[0] - f_stable_B[0])\n\n    # --- Test Case C: Uniform reflectance ---\n    L_C, N_C = 4096, 500000\n    p_model_C = {'type': 'uniform'}\n    h_C = generate_histogram(L_C, N_C, p_model_C, RNG)\n    f_stable_C = stable_cdf(h_C, N_C)\n    y_C = f_stable_C  # Histogram equalization mapping\n    # Empirical probability p_hat(k) = h_k / N\n    p_empirical_C = h_C / np.float64(N_C)\n    # Mean mapped value y_bar = E[y_k] = sum(y_k * p_hat(k))\n    y_bar_C = np.sum(y_C * p_empirical_C)\n    result_C = np.abs(y_bar_C - 0.5)\n\n    # --- Test Case D: Multi-scale mixture stressing small-bin probabilities ---\n    L_D, N_D = 4096, 1500000\n    p_model_D = {\n        'type': 'gaussian_mixture',\n        'means': [0.2 * L_D, 0.5 * L_D, 0.7 * L_D],\n        'stds': [0.01 * L_D, 0.05 * L_D, 0.2 * L_D], # Substantially different stds\n        'weights': [0.4, 0.4, 0.2]\n    }\n    h_D = generate_histogram(L_D, N_D, p_model_D, RNG)\n    f_naive_D = naive_cdf(h_D, N_D)\n    f_stable_D = stable_cdf(h_D, N_D)\n    result_D = np.max(np.abs(f_naive_D - f_stable_D))\n\n    # --- Test Case E: Monotonicity property ---\n    # Use the stable CDF from Case A.\n    y_A_stable = f_stable_A\n    # The difference y[k+1] - y[k] must be non-negative.\n    # np.diff computes this array of differences.\n    # Due to the nature of the stable calculation, this should be exact.\n    is_monotonic = np.all(np.diff(y_A_stable) >= 0.0)\n    result_E = bool(is_monotonic)\n\n    # --- Final Output Formatting ---\n    results = [result_A, result_B, result_C, result_D, result_E]\n    \n    # Custom mapping to format floats with full precision and bool as lowercase\n    def format_value(v):\n        if isinstance(v, bool):\n            return str(v).lower()\n        if isinstance(v, (np.floating, float)):\n            return f\"{v:.17g}\" # Use general format decimal representation\n        return str(v)\n\n    print(f\"[{','.join(map(format_value, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}