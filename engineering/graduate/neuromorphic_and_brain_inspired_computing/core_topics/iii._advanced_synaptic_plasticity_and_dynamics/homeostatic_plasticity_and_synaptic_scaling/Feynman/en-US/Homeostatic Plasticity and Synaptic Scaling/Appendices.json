{
    "hands_on_practices": [
        {
            "introduction": "To begin, we will ground our understanding of synaptic scaling in a direct and tangible calculation. This first practice invites you to derive the scaling factor required to adjust a neuron's firing rate to a specific target, using a simplified linear rate model. By working through this exercise , you will gain a concrete grasp of the fundamental mathematical operation at the heart of this homeostatic mechanism.",
            "id": "4047498",
            "problem": "Consider a single neuron in a neuromorphic system implementing homeostatic plasticity through synaptic scaling. The neuron receives inputs from $N$ presynaptic channels with average input rates $\\bar{x}_{i}$ and synaptic weights $w_{i}$, and its average firing rate over a slow homeostatic timescale is modeled by the linear rate equation $r=\\sum_{i=1}^{N}w_{i}\\bar{x}_{i}$. A homeostatic controller seeks to achieve a target average firing rate $r^{*}$ by multiplicatively scaling all synaptic weights by a common positive factor $\\alpha$, while preserving the ratios of synaptic strengths. Assume that over the timescale on which the controller acts, the presynaptic averages $\\bar{x}_{i}$ remain fixed, and that the neuron's transfer function is well approximated by a linear mapping in this operating regime.\n\nDerive, from these premises alone, an analytical expression for the scaling factor $\\alpha$ that achieves the target rate $r^{*}$. Then, evaluate $\\alpha$ for the specific case with $N=3$, presynaptic averages $\\bar{x}_{1}=5\\,\\text{Hz}$, $\\bar{x}_{2}=7\\,\\text{Hz}$, $\\bar{x}_{3}=3\\,\\text{Hz}$, synaptic weights $w_{1}=0.8$, $w_{2}=0.5$, $w_{3}=-0.2$, and target average rate $r^{*}=10\\,\\text{Hz}$. Round your final numerical answer to four significant figures. The final answer should be reported as the single value of $\\alpha$.",
            "solution": "The problem is deemed valid as it is scientifically grounded in a standard model of homeostatic plasticity, well-posed with a clear objective and sufficient data, and expressed in objective, formalizable language.\n\nThe initial average firing rate of the neuron, which we can denote as $r_{\\text{initial}}$, is given by the linear rate equation:\n$$r_{\\text{initial}} = \\sum_{i=1}^{N} w_{i} \\bar{x}_{i}$$\nHere, $w_{i}$ are the initial synaptic weights and $\\bar{x}_{i}$ are the average presynaptic input rates.\n\nHomeostatic plasticity adjusts the synaptic weights to achieve a target average firing rate, $r^{*}$. The mechanism specified is a multiplicative scaling of all synaptic weights by a common positive factor, $\\alpha$. The new weights, $w'_{i}$, are therefore given by:\n$$w'_{i} = \\alpha w_{i}$$\nThe problem states that this scaling should result in the neuron's average firing rate becoming $r^{*}$. Using the same rate equation with the new weights, we have:\n$$r^{*} = \\sum_{i=1}^{N} w'_{i} \\bar{x}_{i}$$\nSubstituting the expression for the new weights, $w'_{i}$, into this equation:\n$$r^{*} = \\sum_{i=1}^{N} (\\alpha w_{i}) \\bar{x}_{i}$$\nSince the scaling factor $\\alpha$ is a constant common to all terms in the summation, it can be factored out:\n$$r^{*} = \\alpha \\left( \\sum_{i=1}^{N} w_{i} \\bar{x}_{i} \\right)$$\nThe term in the parentheses is the initial average firing rate, $r_{\\text{initial}}$. Thus, the relationship is:\n$$r^{*} = \\alpha \\cdot r_{\\text{initial}}$$\nTo find the required scaling factor $\\alpha$, we can rearrange this equation, assuming $r_{\\text{initial}} \\neq 0$:\n$$\\alpha = \\frac{r^{*}}{r_{\\text{initial}}} = \\frac{r^{*}}{\\sum_{i=1}^{N} w_{i} \\bar{x}_{i}}$$\nThis is the general analytical expression for the scaling factor $\\alpha$.\n\nNow, we evaluate this expression for the specific case provided. The givens are:\nNumber of inputs, $N=3$.\nPresynaptic average rates: $\\bar{x}_{1}=5\\,\\text{Hz}$, $\\bar{x}_{2}=7\\,\\text{Hz}$, $\\bar{x}_{3}=3\\,\\text{Hz}$.\nInitial synaptic weights: $w_{1}=0.8$, $w_{2}=0.5$, $w_{3}=-0.2$.\nTarget average rate: $r^{*}=10\\,\\text{Hz}$.\n\nFirst, we calculate the initial firing rate, $r_{\\text{initial}}$, which is the denominator in our expression for $\\alpha$:\n$$r_{\\text{initial}} = \\sum_{i=1}^{3} w_{i} \\bar{x}_{i} = w_{1}\\bar{x}_{1} + w_{2}\\bar{x}_{2} + w_{3}\\bar{x}_{3}$$\nSubstituting the numerical values:\n$$r_{\\text{initial}} = (0.8)(5) + (0.5)(7) + (-0.2)(3)$$\n$$r_{\\text{initial}} = 4.0 + 3.5 - 0.6 = 6.9\\,\\text{Hz}$$\nSince $r_{\\text{initial}} = 6.9 \\neq 0$, the scaling factor $\\alpha$ is well-defined.\n\nNow, we can calculate $\\alpha$ using the target rate $r^{*}=10\\,\\text{Hz}$:\n$$\\alpha = \\frac{r^{*}}{r_{\\text{initial}}} = \\frac{10}{6.9}$$\n$$\\alpha \\approx 1.44927536...$$\nThe problem requires rounding the final numerical answer to four significant figures.\nThe first four significant figures are $1$, $4$, $4$, and $9$. The fifth digit is $2$, which is less than $5$, so we round down (i.e., we do not change the fourth digit).\nTherefore, the value of $\\alpha$ rounded to four significant figures is $1.449$.",
            "answer": "$$\\boxed{1.449}$$"
        },
        {
            "introduction": "Having established the basic mechanism, a crucial question arises: why is *multiplicative* scaling the preferred model for this form of plasticity? This practice challenges you to think like a theorist by comparing multiplicative scaling with a plausible alternative, additive scaling. By analyzing how each rule affects the relative strengths of synapses , you will uncover the profound reason why multiplicative updates are essential for preserving learned information within a neural circuit.",
            "id": "3989724",
            "problem": "Consider a single conductance-based neuron receiving inputs from $N$ excitatory synapses indexed by $i \\in \\{1,\\dots,N\\}$ with baseline synaptic weights $w_i \\ge 0$ and fixed presynaptic firing rates $r_i \\ge 0$. Assume linear summation of synaptic drive so that the mean excitatory drive is $I = \\sum_{i=1}^{N} w_i r_i$. A homeostatic mechanism aims to adjust all synaptic weights globally to match a strictly positive target mean drive $I^{\\star} > 0$ while minimizing disruption of relative synaptic efficacies.\n\nTwo global update classes are considered: multiplicative scaling with a common factor $\\alpha \\in \\mathbb{R}$ applied as $w_i' = \\alpha w_i$ for all $i$, and additive scaling with a common offset $\\beta \\in \\mathbb{R}$ applied as $w_i' = w_i + \\beta$ for all $i$. For both updates, impose the following desiderata:\n- Non-negativity: all adjusted weights must satisfy $w_i' \\ge 0$ for all $i$.\n- Proportionality preservation: the relative efficacy ratios must be preserved, that is $w_i'/w_j' = w_i/w_j$ for all $i,j$ with $w_j > 0$.\n\nStarting from the definitions above, and using only fundamental modeling assumptions for synaptic integration and homeostatic control, do the following:\n1. Derive necessary and sufficient conditions on $\\alpha$ for multiplicative scaling to satisfy non-negativity and proportionality preservation simultaneously for arbitrary nonnegative $\\{w_i\\}$. Repeat the derivation for additive scaling to obtain necessary and sufficient conditions on $\\beta$ that ensure non-negativity and proportionality preservation simultaneously for arbitrary nonnegative $\\{w_i\\}$. Explicitly state any degenerate cases you identify.\n2. Enforce the homeostatic target $\\sum_{i=1}^{N} w_i' r_i = I^{\\star}$ under each update class and solve symbolically for the parameter ($\\alpha$ in the multiplicative case and $\\beta$ in the additive case) in terms of $\\{w_i\\}$, $\\{r_i\\}$, and $I^{\\star}$. State clearly any regularity conditions required for existence and uniqueness of the solutions you derive.\n3. By analyzing the effect of each update on the ratios $w_i'/w_j'$, show rigorously that additive scaling distorts these ratios for generic weight configurations except when $\\beta = 0$; characterize any non-generic, degenerate sets of $\\{w_i\\}$ for which additive scaling does not distort ratios.\n\nAssume $\\sum_{i=1}^{N} r_i > 0$ and $\\sum_{i=1}^{N} w_i r_i > 0$ so that both the baseline drive and the total presynaptic drive are strictly positive. Provide your reasoning and intermediate steps. Finally, report the closed-form analytic expression for the unique multiplicative scaling factor $\\alpha$ that achieves the target $I^{\\star}$ while satisfying the desiderata above. The final answer must be a single closed-form expression with no units.",
            "solution": "The problem statement is a valid exercise in computational neuroscience, presenting a well-posed and scientifically grounded scenario for analyzing homeostatic synaptic plasticity. It is self-contained, with all necessary definitions and assumptions provided to derive a unique and meaningful solution. The problem is free of scientific inaccuracies, contradictions, and ambiguities. We may therefore proceed with a formal solution.\n\nThe analysis is structured into three parts as requested by the problem statement, followed by the determination of the final expression for the multiplicative scaling factor.\n\n**Part 1: Derivation of Necessary and Sufficient Conditions for Desiderata**\n\nWe analyze the two proposed update classes, multiplicative and additive scaling, against the two desiderata: non-negativity ($w_i' \\ge 0$) and proportionality preservation ($w_i'/w_j' = w_i/w_j$ for $w_j  0$). These conditions must hold for an arbitrary set of initial non-negative weights $\\{w_i\\}$.\n\n**Multiplicative Scaling:**\nThe update rule is $w_i' = \\alpha w_i$ for $\\alpha \\in \\mathbb{R}$.\n\n1.  **Non-negativity:** The condition is $w_i' = \\alpha w_i \\ge 0$. Since this must hold for any arbitrary initial weight set $\\{w_i\\}$ where $w_i \\ge 0$, we can consider a non-degenerate case where at least one weight $w_k  0$. For this weight, the condition becomes $\\alpha w_k \\ge 0$, which implies $\\alpha \\ge 0$. This is a necessary condition. It is also sufficient, because if $\\alpha \\ge 0$ and $w_i \\ge 0$ for all $i \\in \\{1, \\dots, N\\}$, then their product $\\alpha w_i$ is guaranteed to be non-negative.\n    Thus, the necessary and sufficient condition for non-negativity is $\\alpha \\ge 0$.\n\n2.  **Proportionality Preservation:** The condition is $w_i'/w_j' = w_i/w_j$ for any pair $i, j$ where $w_j  0$. Substituting the update rule, we have:\n    $$ \\frac{\\alpha w_i}{\\alpha w_j} = \\frac{w_i}{w_j} $$\n    For this equation to be meaningful and for the fraction on the left to be well-defined, the denominator $\\alpha w_j$ cannot be zero. Since we consider cases where $w_j  0$, we must have $\\alpha \\ne 0$. If $\\alpha \\ne 0$, we can cancel $\\alpha$ from the numerator and denominator, yielding the identity $w_i/w_j = w_i/w_j$, which is always true. Therefore, proportionality is preserved if and only if $\\alpha \\ne 0$. A degenerate case occurs if all $w_i=0$, in which case the condition is vacuously satisfied for any $\\alpha$, but the problem assumes a non-trivial baseline drive $\\sum w_i r_i  0$, so not all $w_i$ can be zero if the corresponding $r_i$ are positive.\n\n3.  **Simultaneous Conditions:** To satisfy both non-negativity ($\\alpha \\ge 0$) and proportionality preservation ($\\alpha \\ne 0$) simultaneously, the scaling factor must satisfy $\\alpha  0$.\n\n**Additive Scaling:**\nThe update rule is $w_i' = w_i + \\beta$ for $\\beta \\in \\mathbb{R}$.\n\n1.  **Non-negativity:** The condition is $w_i' = w_i + \\beta \\ge 0$. This must hold for all $w_i \\ge 0$. The most restrictive case is for the smallest possible initial weight, which is $w_i = 0$. For non-negativity to hold, we must have $0 + \\beta \\ge 0$, which implies $\\beta \\ge 0$. This is a necessary condition. It is also sufficient, as if $\\beta \\ge 0$ and $w_i \\ge 0$, their sum $w_i + \\beta$ is always non-negative.\n    Thus, the necessary and sufficient condition for non-negativity is $\\beta \\ge 0$.\n\n2.  **Proportionality Preservation:** The condition is $w_i'/w_j' = w_i/w_j$ for $w_j  0$.\n    $$ \\frac{w_i + \\beta}{w_j + \\beta} = \\frac{w_i}{w_j} $$\n    Cross-multiplying gives:\n    $$ w_j (w_i + \\beta) = w_i (w_j + \\beta) $$\n    $$ w_i w_j + \\beta w_j = w_i w_j + \\beta w_i $$\n    $$ \\beta w_j = \\beta w_i $$\n    $$ \\beta (w_j - w_i) = 0 $$\n    This equation must hold for an arbitrary set of non-negative weights $\\{w_i\\}$. In a generic configuration, we can choose weights $w_i$ and $w_j$ such that $w_i \\ne w_j$. For the equation to hold in this general case, we must have $\\beta = 0$.\n\n3.  **Simultaneous Conditions:** To satisfy both non-negativity ($\\beta \\ge 0$) and proportionality preservation ($\\beta = 0$), the only possible value for the additive offset is $\\beta = 0$. This corresponds to a trivial update where the weights are not changed. A degenerate case where proportionality is preserved for any $\\beta \\ge 0$ occurs if all initial weights are identical, $w_i = c$ for all $i$. In this specific scenario, $w_j - w_i = 0$, and the condition $\\beta(w_j-w_i)=0$ is satisfied for any $\\beta$.\n\n**Part 2: Solving for Scaling Parameters**\n\nWe now enforce the homeostatic target $I' = \\sum_{i=1}^{N} w_i' r_i = I^{\\star}$.\n\n**Multiplicative Scaling:**\nThe target equation is $\\sum_{i=1}^{N} (\\alpha w_i) r_i = I^{\\star}$. Factoring out the constant $\\alpha$:\n$$ \\alpha \\sum_{i=1}^{N} w_i r_i = I^{\\star} $$\nThe sum term is the baseline drive, $I = \\sum_{i=1}^{N} w_i r_i$. So, $\\alpha I = I^{\\star}$.\nSolving for $\\alpha$:\n$$ \\alpha = \\frac{I^{\\star}}{I} = \\frac{I^{\\star}}{\\sum_{i=1}^{N} w_i r_i} $$\nFor this solution to exist and be unique, the denominator must be non-zero. The problem explicitly assumes that the baseline drive is strictly positive, i.e., $\\sum_{i=1}^{N} w_i r_i  0$. Since the target drive $I^{\\star}$ is also strictly positive, the resulting $\\alpha$ will be strictly positive ($\\alpha  0$). This is fully consistent with the necessary and sufficient condition derived in Part 1 for satisfying both desiderata.\n\n**Additive Scaling:**\nThe target equation is $\\sum_{i=1}^{N} (w_i + \\beta) r_i = I^{\\star}$. Distributing the sum:\n$$ \\sum_{i=1}^{N} w_i r_i + \\sum_{i=1}^{N} \\beta r_i = I^{\\star} $$\n$$ I + \\beta \\sum_{i=1}^{N} r_i = I^{\\star} $$\nSolving for $\\beta$:\n$$ \\beta = \\frac{I^{\\star} - I}{\\sum_{i=1}^{N} r_i} = \\frac{I^{\\star} - \\sum_{i=1}^{N} w_i r_i}{\\sum_{i=1}^{N} r_i} $$\nFor this solution to exist and be unique, the denominator must be non-zero. The problem explicitly assumes $\\sum_{i=1}^{N} r_i  0$. Therefore, a unique solution for $\\beta$ always exists. However, as shown in Part 1, additive scaling only preserves proportionality for generic weights if $\\beta = 0$, which implies $I^{\\star} = I$. Thus, additive scaling cannot, in general, reach an arbitrary target $I^{\\star} \\ne I$ without distorting the relative weight structure. Furthermore, for non-negativity we need $\\beta \\ge 0$, which requires $I^{\\star} \\ge I$. If the target drive is lower than the current drive, additive scaling would require making some weights negative, violating the non-negativity constraint for synapses with initially small weights.\n\n**Part 3: Analysis of Ratio Distortion by Additive Scaling**\n\nAs derived in Part 1, the condition for additive scaling to preserve the ratio $w_i/w_j$ is $\\beta(w_j - w_i) = 0$.\nFor a generic set of weights $\\{w_k\\}$, not all weights are equal. This means there exists at least one pair of indices $(i, j)$ for which $w_i \\ne w_j$. For such a pair, the term $(w_j - w_i)$ is non-zero. Consequently, for the equation $\\beta(w_j - w_i) = 0$ to hold, it must be that $\\beta = 0$.\nA non-zero additive shift ($\\beta \\ne 0$) will therefore distort the ratio for any pair of synapses with initially different weights. Let $R_{ij} = w_i/w_j$. The new ratio is $R'_{ij} = (w_i+\\beta)/(w_j+\\beta)$. If we assume $\\beta  0$ and $w_i  w_j  0$, then we can show $R'_{ij}  R_{ij}$. The transformation compresses the ratios towards $1$.\n\nThe only non-generic, degenerate set of weights $\\{w_i\\}$ for which additive scaling preserves all ratios for any $\\beta$ is the set where all weights are equal.\nLet $w_i = c$ for all $i \\in \\{1, \\dots, N\\}$ for some constant $c \\ge 0$.\nThen for any pair $(i, j)$ with $c0$, the initial ratio is $w_i/w_j = c/c = 1$.\nThe updated weights are $w_i' = c+\\beta$. The new ratio is $w_i'/w_j' = (c+\\beta)/(c+\\beta) = 1$.\nThe ratio is preserved. If $c=0$, all weights are zero and the concept of ratio is ill-defined. In summary, additive scaling is structure-preserving only for the highly non-generic case of a completely uniform weight distribution.\n\n**Conclusion and Final Answer Formulation:**\nThe analysis rigorously demonstrates that only multiplicative scaling with a factor $\\alpha  0$ can achieve the homeostatic target $I^\\star$ while strictly preserving the non-negativity of weights and the relative efficacy of all synapses for an arbitrary initial weight distribution. The unique scaling factor that accomplishes this is derived in Part 2. The final answer required is the closed-form expression for this factor.",
            "answer": "$$\n\\boxed{\\frac{I^{\\star}}{\\sum_{i=1}^{N} w_i r_i}}\n$$"
        },
        {
            "introduction": "The elegant mathematical models we study must ultimately be implemented on physical hardware, which introduces its own set of constraints and non-idealities. This final practice explores the practical consequences of implementing synaptic scaling using fixed-point arithmetic, a common approach in neuromorphic engineering. You will model and analyze how quantization error accumulates over time , providing critical insight into the trade-offs between computational precision and resource efficiency in building robust neuromorphic systems.",
            "id": "4047583",
            "problem": "In a neuromorphic substrate implementing homeostatic synaptic scaling, a population of $N$ synapses undergoes global multiplicative renormalization to stabilize firing. At discrete update times $t \\in \\{1,2,\\dots,T\\}$, the ideal update would scale each synaptic weight $w_i(t-1)$ by a factor $0\\alpha1$, yielding $w_i^{\\star}(t)=\\alpha\\,w_i^{\\star}(t-1)$ with $w_i^{\\star}(0)=w_i(0)$. However, the hardware stores each weight in fixed-point with quantization step $q0$ and rounds every update to the nearest multiple of $q$ with ties broken to even. Each representable weight has the form $w_i(t)=k_i(t)\\,q$ for some integer $k_i(t)$.\n\nAssume:\n- The initial weights are exactly representable, so $w_i(0)=k_i(0)\\,q$ for integers $k_i(0)$.\n- The hardware implements the update $w_i(t)=\\mathcal{Q}\\!\\left(\\alpha\\,w_i(t-1)\\right)$, where $\\mathcal{Q}$ denotes rounding to the nearest multiple of $q$ with ties-to-even and no saturation occurs.\n- Under the standard additive noise model for rounding-to-nearest without overload and with signal-quantizer independence, the scalar quantization error at each synapse and step, $e_i(t)=w_i(t)-\\alpha\\,w_i(t-1)$, is an independent, zero-mean random variable uniformly distributed on $[-q/2,+q/2]$, independent across $i$ and $t$.\n\nTasks:\n1) Write the digital update rule in the integer code domain, i.e., an expression for $k_i(t)$ in terms of $k_i(t-1)$ and $\\alpha$.\n\n2) Let the cumulative quantization error vector at time $t$ be $\\boldsymbol{\\delta}(t)=\\big(w_1(t)-w_1^{\\star}(t),\\dots,w_N(t)-w_N^{\\star}(t)\\big)$. Starting from the assumptions above and fundamental definitions of rounding and uniform quantization error, derive a closed-form analytic expression for the expected squared Euclidean norm $\\mathbb{E}\\!\\left[\\|\\boldsymbol{\\delta}(T)\\|_2^2\\right]$ as a function of $N$, $q$, $\\alpha$, and $T$, with $0\\alpha1$ and positive integer $T$.\n\nProvide your final answer as a single closed-form expression in symbolic form. Do not include units. Do not provide an inequality or an equation; provide only the requested expression.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A population of $N$ synapses.\n- Discrete update times $t \\in \\{1,2,\\dots,T\\}$.\n- Ideal synaptic weight update: $w_i^{\\star}(t)=\\alpha\\,w_i^{\\star}(t-1)$.\n- Ideal update scaling factor: $0\\alpha1$.\n- Ideal initial condition: $w_i^{\\star}(0)=w_i(0)$.\n- Hardware representation: fixed-point with quantization step $q0$.\n- Hardware rounding rule: round to the nearest multiple of $q$ with ties-to-even.\n- Representable weight form: $w_i(t)=k_i(t)\\,q$ for an integer $k_i(t)$.\n- Initial condition for hardware: $w_i(0)=k_i(0)\\,q$ for integers $k_i(0)$.\n- Hardware update rule: $w_i(t)=\\mathcal{Q}\\!\\left(\\alpha\\,w_i(t-1)\\right)$, where $\\mathcal{Q}$ is the rounding operator.\n- Saturation: No saturation occurs.\n- Quantization error model: $e_i(t)=w_i(t)-\\alpha\\,w_i(t-1)$.\n- Error statistics: $e_i(t)$ is an independent, zero-mean random variable uniformly distributed on $[-q/2,+q/2]$.\n- Error independence: $e_i(t)$ is independent across synapse index $i$ and time step $t$.\n- Task 1: Find the update rule for $k_i(t)$ in terms of $k_i(t-1)$ and $\\alpha$.\n- Task 2: Derive a closed-form expression for the expected squared Euclidean norm of the cumulative quantization error vector, $\\mathbb{E}\\!\\left[\\|\\boldsymbol{\\delta}(T)\\|_2^2\\right]$, where $\\boldsymbol{\\delta}(t)=\\big(w_1(t)-w_1^{\\star}(t),\\dots,w_N(t)-w_N^{\\star}(t)\\big)$.\n- Constraints on parameters: $0\\alpha1$ and $T$ is a positive integer.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It formalizes a standard problem in digital signal processing and neuromorphic engineering, specifically the analysis of error accumulation in a linear time-invariant system subject to quantization. The use of a uniform additive noise model for rounding error is a widely accepted approximation for analytical tractability. The problem statement is self-contained, with all necessary variables, definitions, and statistical assumptions provided. There are no contradictions, ambiguities, or pseudo-scientific claims.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds by first addressing Task 1 to establish the update rule in the integer domain, and then addressing Task 2 by deriving a recurrence relation for the cumulative error, solving it, and calculating the required expected value.\n\n**Task 1: Digital Update Rule in the Integer Code Domain**\n\nThe hardware represents each weight $w_i(t)$ as an integer multiple of the quantization step $q$, such that $w_i(t) = k_i(t)q$. The hardware update rule is given by $w_i(t) = \\mathcal{Q}(\\alpha w_i(t-1))$, where $\\mathcal{Q}(x)$ rounds the value $x$ to the nearest multiple of $q$. Let $\\mathcal{R}(y)$ denote the operation of rounding a real number $y$ to the nearest integer. Then, the quantization operator can be written as $\\mathcal{Q}(x) = q \\cdot \\mathcal{R}(x/q)$.\n\nBy substituting the integer-coded representations of the weights into the hardware update rule, we obtain:\n$$\nk_i(t)q = \\mathcal{Q}(\\alpha (k_i(t-1)q))\n$$\nUsing the definition of $\\mathcal{Q}(\\cdot)$:\n$$\nk_i(t)q = q \\cdot \\mathcal{R}\\left(\\frac{\\alpha k_i(t-1)q}{q}\\right)\n$$\nDividing both sides by $q$ yields the update rule for the integer code $k_i(t)$:\n$$\nk_i(t) = \\mathcal{R}(\\alpha k_i(t-1))\n$$\nThis equation describes the evolution of the integer representation of the synaptic weight at each time step.\n\n**Task 2: Expected Squared Euclidean Norm of Cumulative Error**\n\nThe primary goal is to compute $\\mathbb{E}\\!\\left[\\|\\boldsymbol{\\delta}(T)\\|_2^2\\right]$. The squared Euclidean norm of the cumulative error vector $\\boldsymbol{\\delta}(T)$ is given by $\\|\\boldsymbol{\\delta}(T)\\|_2^2 = \\sum_{i=1}^{N} \\delta_i(T)^2$, where $\\delta_i(t) = w_i(t) - w_i^{\\star}(t)$ is the cumulative error for the $i$-th synapse at time $t$.\n\nBy linearity of expectation:\n$$\n\\mathbb{E}\\!\\left[\\|\\boldsymbol{\\delta}(T)\\|_2^2\\right] = \\mathbb{E}\\!\\left[\\sum_{i=1}^{N} \\delta_i(T)^2\\right] = \\sum_{i=1}^{N} \\mathbb{E}\\!\\left[\\delta_i(T)^2\\right]\n$$\nThe problem states that the quantization errors $e_i(t)$ are independent and identically distributed for all synapses $i$ and time steps $t$. This implies that the statistical properties of $\\delta_i(T)$ are identical for all $i$. Therefore, $\\mathbb{E}\\!\\left[\\delta_i(T)^2\\right]$ is the same for all $i$. Let us denote this common value as $\\mathbb{E}\\!\\left[\\delta(T)^2\\right]$. The expression simplifies to:\n$$\n\\mathbb{E}\\!\\left[\\|\\boldsymbol{\\delta}(T)\\|_2^2\\right] = N \\cdot \\mathbb{E}\\!\\left[\\delta(T)^2\\right]\n$$\nWe now derive an expression for $\\mathbb{E}\\!\\left[\\delta(T)^2\\right]$. We will drop the synapse index $i$ for clarity. The cumulative error is $\\delta(t) = w(t) - w^{\\star}(t)$.\nThe ideal update is $w^{\\star}(t) = \\alpha w^{\\star}(t-1)$.\nThe actual hardware update can be described by the additive noise model: $w(t) = \\alpha w(t-1) + e(t)$, where $e(t)$ is the quantization error at step $t$.\n\nSubtracting the ideal update from the actual update equation, we get a recurrence relation for the cumulative error $\\delta(t)$:\n$$\nw(t) - w^{\\star}(t) = (\\alpha w(t-1) + e(t)) - (\\alpha w^{\\star}(t-1))\n$$\n$$\n\\delta(t) = \\alpha (w(t-1) - w^{\\star}(t-1)) + e(t)\n$$\n$$\n\\delta(t) = \\alpha \\delta(t-1) + e(t)\n$$\nThis is a first-order linear recurrence relation. We can solve it by unrolling the recursion.\nThe initial condition is $\\delta(0) = w(0) - w^{\\star}(0)$. The problem states that $w(0) = w^{\\star}(0)$, so $\\delta(0)=0$.\nFor $t=1$, $\\delta(1) = \\alpha\\delta(0) + e(1) = e(1)$.\nFor $t=2$, $\\delta(2) = \\alpha\\delta(1) + e(2) = \\alpha e(1) + e(2)$.\nFor a general time step $t$, the solution is found by induction:\n$$\n\\delta(t) = \\sum_{j=1}^{t} \\alpha^{t-j} e(j)\n$$\nWe need to find the expected value of the square of this expression at time $T$:\n$$\n\\mathbb{E}\\!\\left[\\delta(T)^2\\right] = \\mathbb{E}\\!\\left[ \\left(\\sum_{j=1}^{T} \\alpha^{T-j} e(j)\\right)^2 \\right] = \\mathbb{E}\\!\\left[ \\left(\\sum_{j=1}^{T} \\alpha^{T-j} e(j)\\right) \\left(\\sum_{k=1}^{T} \\alpha^{T-k} e(k)\\right) \\right]\n$$\n$$\n\\mathbb{E}\\!\\left[\\delta(T)^2\\right] = \\mathbb{E}\\!\\left[ \\sum_{j=1}^{T} \\sum_{k=1}^{T} \\alpha^{2T-j-k} e(j) e(k) \\right]\n$$\nUsing the linearity of expectation, we move the expectation operator inside the summations:\n$$\n\\mathbb{E}\\!\\left[\\delta(T)^2\\right] = \\sum_{j=1}^{T} \\sum_{k=1}^{T} \\alpha^{2T-j-k} \\mathbb{E}[e(j) e(k)]\n$$\nThe problem states that the errors $e(t)$ are independent across time and have zero mean, i.e., $\\mathbb{E}[e(t)]=0$ for all $t$.\nThis implies that for $j \\neq k$, the errors $e(j)$ and $e(k)$ are uncorrelated, and thus $\\mathbb{E}[e(j) e(k)] = \\mathbb{E}[e(j)] \\mathbb{E}[e(k)] = 0 \\cdot 0 = 0$.\nFor $j=k$, the expectation is $\\mathbb{E}[e(j)^2]$. Since the mean is zero, this is equal to the variance of the error, $\\mathbb{E}[e(j)^2] = \\text{Var}(e(j))$.\n\nThe error $e(j)$ is uniformly distributed on $[-q/2, +q/2]$. The variance of a uniform distribution over an interval $[a, b]$ is $(b-a)^2/12$.\n$$\n\\text{Var}(e(j)) = \\frac{(q/2 - (-q/2))^2}{12} = \\frac{q^2}{12}\n$$\nThis variance is constant for all time steps $j$. Let's denote it by $\\sigma_e^2 = q^2/12$. The double summation simplifies to a single summation over the terms where $j=k$:\n$$\n\\mathbb{E}\\!\\left[\\delta(T)^2\\right] = \\sum_{j=1}^{T} \\alpha^{2(T-j)} \\mathbb{E}[e(j)^2] = \\sigma_e^2 \\sum_{j=1}^{T} (\\alpha^2)^{T-j}\n$$\nThis is a geometric series. To simplify, we can change the index of summation. Let $m = T-j$. As $j$ goes from $1$ to $T$, $m$ goes from $T-1$ down to $0$.\n$$\n\\sum_{j=1}^{T} (\\alpha^2)^{T-j} = \\sum_{m=0}^{T-1} (\\alpha^2)^m\n$$\nThe sum of a finite geometric series is given by $\\sum_{k=0}^{n-1} r^k = \\frac{1-r^n}{1-r}$. In our case, the common ratio is $r = \\alpha^2$ and the number of terms is $n = T$.\n$$\n\\sum_{m=0}^{T-1} (\\alpha^2)^m = \\frac{1 - (\\alpha^2)^T}{1 - \\alpha^2} = \\frac{1 - \\alpha^{2T}}{1 - \\alpha^2}\n$$\nSubstituting this back into the expression for $\\mathbb{E}\\!\\left[\\delta(T)^2\\right]$:\n$$\n\\mathbb{E}\\!\\left[\\delta(T)^2\\right] = \\sigma_e^2 \\left( \\frac{1 - \\alpha^{2T}}{1 - \\alpha^2} \\right) = \\frac{q^2}{12} \\frac{1 - \\alpha^{2T}}{1 - \\alpha^2}\n$$\nFinally, we can find the total expected squared Euclidean norm for the population of $N$ synapses:\n$$\n\\mathbb{E}\\!\\left[\\|\\boldsymbol{\\delta}(T)\\|_2^2\\right] = N \\cdot \\mathbb{E}\\!\\left[\\delta(T)^2\\right] = N \\frac{q^2}{12} \\frac{1 - \\alpha^{2T}}{1 - \\alpha^2}\n$$\nThis is the closed-form analytic expression for the expected cumulative error as a function of $N$, $q$, $\\alpha$, and $T$.",
            "answer": "$$\n\\boxed{N \\frac{q^{2}}{12} \\frac{1 - \\alpha^{2T}}{1 - \\alpha^{2}}}\n$$"
        }
    ]
}