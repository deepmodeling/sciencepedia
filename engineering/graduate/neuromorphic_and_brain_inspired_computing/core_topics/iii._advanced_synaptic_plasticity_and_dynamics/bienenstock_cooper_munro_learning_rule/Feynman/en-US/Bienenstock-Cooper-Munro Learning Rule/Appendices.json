{
    "hands_on_practices": [
        {
            "introduction": "Before exploring the rich dynamics of any physical model, a crucial first step is to verify its internal consistency. This practice invites you to perform a dimensional analysis on the core BCM equation, a fundamental skill that ensures the mathematical formulation is physically and dimensionally sound. By confirming that the units on both sides of the equation match, you build confidence in the model's structure .",
            "id": "4037165",
            "problem": "Consider a single synapse in a neuromorphic circuit implementing the Bienenstock-Cooper-Munro (BCM) learning rule, where the synaptic weight dynamics are given by the differential equation\n$$\n\\dot{w}_i=\\eta\\, x_i\\, y\\, \\bigl(y-\\theta\\bigr).\n$$\nAssume the following:\n- The presynaptic firing rate $x_i$ and postsynaptic firing rate $y$ are measured as rates in hertz, so $[x_i]=[y]=\\mathrm{s}^{-1}$.\n- The synaptic weight $w_i$ is dimensionless, and the postsynaptic activity-dependent threshold $\\theta$ must be chosen so that $y-\\theta$ is well-defined.\n- The left-hand side $\\dot{w}_i$ is the time derivative of a dimensionless quantity and therefore has units of $\\mathrm{s}^{-1}$.\n\nUsing only fundamental dimensional analysis principles (that a sum requires commensurate units and that a time derivative of a dimensionless variable has units $\\mathrm{s}^{-1}$), determine the exponent $p$ such that the learning rate constant $\\eta$ has units $\\mathrm{s}^{p}$ that guarantee the equation is dimensionally consistent. Report your final answer as the single number $p$ (no units). No rounding is required.",
            "solution": "The problem requires the determination of the exponent $p$ for the units of the learning rate constant $\\eta$, which are given as $\\mathrm{s}^{p}$, to ensure the dimensional consistency of the Bienenstock-Cooper-Munro (BCM) learning rule. The governing differential equation is:\n$$\n\\dot{w}_i=\\eta\\, x_i\\, y\\, \\bigl(y-\\theta\\bigr)\n$$\nThe fundamental principle we must apply is that of dimensional homogeneity. This principle mandates that for any physically meaningful equation, the dimensions of the expression on the left-hand side (LHS) must be identical to the dimensions of the expression on the right-hand side (RHS).\n\nLet us denote the dimensions of a quantity $Q$ by $[Q]$. We are given the following information about the dimensions of the variables:\n\\begin{itemize}\n    \\item The synaptic weight $w_i$ is a dimensionless quantity. Therefore, $[w_i] = 1$. The time derivative, $\\dot{w}_i = \\frac{dw_i}{dt}$, must have dimensions of inverse time. Given that time is measured in seconds ($\\mathrm{s}$), the dimensions of the LHS are $[\\dot{w}_i] = \\mathrm{s}^{-1}$.\n    \\item The presynaptic firing rate $x_i$ has units of hertz, so $[x_i] = \\mathrm{s}^{-1}$.\n    \\item The postsynaptic firing rate $y$ also has units of hertz, so $[y] = \\mathrm{s}^{-1}$.\n    \\item The learning rate constant $\\eta$ has units of $\\mathrm{s}^{p}$, so $[\\eta] = \\mathrm{s}^{p}$. Our goal is to find $p$.\n\\end{itemize}\n\nNext, we must determine the dimensions of the term $(y-\\theta)$. The principle of dimensional homogeneity also requires that any quantities that are added or subtracted must have the same dimensions. The expression $y-\\theta$ involves the subtraction of the threshold $\\theta$ from the postsynaptic rate $y$. For this operation to be valid, the dimensions of $\\theta$ must be the same as the dimensions of $y$.\n$$\n[\\theta] = [y] = \\mathrm{s}^{-1}\n$$\nConsequently, the dimensions of the entire parenthetical term are also the same:\n$$\n[y-\\theta] = \\mathrm{s}^{-1}\n$$\n\nNow we can set up the dimensional equation by equating the dimensions of the LHS and RHS.\n$$\n[\\dot{w}_i] = [\\eta] [x_i] [y] [y-\\theta]\n$$\nSubstituting the known dimensions for each term into this equation:\n$$\n\\mathrm{s}^{-1} = [\\eta] \\cdot (\\mathrm{s}^{-1}) \\cdot (\\mathrm{s}^{-1}) \\cdot (\\mathrm{s}^{-1})\n$$\nWe are given that $[\\eta] = \\mathrm{s}^{p}$. Substituting this into the equation gives:\n$$\n\\mathrm{s}^{-1} = (\\mathrm{s}^{p}) \\cdot \\mathrm{s}^{-1-1-1}\n$$\nSimplifying the product of terms on the RHS by adding the exponents:\n$$\n\\mathrm{s}^{-1} = \\mathrm{s}^{p} \\cdot \\mathrm{s}^{-3}\n$$\n$$\n\\mathrm{s}^{-1} = \\mathrm{s}^{p-3}\n$$\nFor this equation to hold true, the exponents of the base unit (seconds, $\\mathrm{s}$) on both sides must be equal.\n$$\n-1 = p - 3\n$$\nSolving for $p$:\n$$\np = 3 - 1\n$$\n$$\np = 2\n$$\nThus, for the BCM equation to be dimensionally consistent under the given conditions, the learning rate constant $\\eta$ must have units of $\\mathrm{s}^{2}$. The value of the exponent $p$ is $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "The elegance of the BCM rule lies in its ability to bidirectionally modify synaptic weights, inducing either long-term potentiation (LTP) or depression (LTD). This exercise guides you through an analysis of a simplified BCM model to reveal its underlying computational principle. You will demonstrate how the rule effectively implements a thresholded Hebbian mechanism, where the fate of the synapse—strengthening or weakening—is determined by whether the postsynaptic activity surpasses a critical threshold .",
            "id": "4037134",
            "problem": "Consider a single-rate neuron driven by one presynaptic channel with activity $x(t)$ and one synaptic weight $w(t)$. Assume a rate-coded postsynaptic response $y(t)$ that is linearly proportional to the net input, $y(t)=w(t)\\,x(t)$, and that the synaptic weight is constrained by a projection to the nonnegative orthant, $w(t)\\geq 0$, as is common in rectifying neuromorphic circuits. Let $x(t)\\equiv x>0$ be constant over the time scale of synaptic changes. The Bienenstock-Cooper-Munro (BCM) learning rule for the synaptic weight is defined by an activity-gated update of the form $\\dot{w}(t)=\\eta\\,x(t)\\,y(t)\\big(y(t)-\\theta\\big)$, where $\\eta>0$ is a learning-rate constant and $\\theta>0$ is a fixed, small threshold parameter with the same units as $y(t)$. Starting from the assumptions of rate coding, nonnegativity of $w(t)$, and the BCM update structure above, derive the autonomous differential equation for $w(t)$ and perform a sign analysis of its right-hand side to identify the unique nonzero critical weight $w_{\\mathrm{c}}$ at which the plasticity switches from depression to potentiation. Use this sign analysis to justify that, for a fixed small $\\theta$, the BCM rule approximates a thresholded Hebbian rule that depresses synapses when the postsynaptic activity is low (that is, when $y(t)<\\theta$) and potentiates when $y(t)>\\theta$. Provide the closed-form analytic expression for the switching weight $w_{\\mathrm{c}}$ in terms of $x$ and $\\theta$ as your final answer. No numerical approximation is required.",
            "solution": "The problem provides the Bienenstock-Cooper-Munro (BCM) learning rule for a synaptic weight $w(t)$ and asks for the derivation and analysis of its dynamics under specific conditions. We begin by stating the given equations and constraints.\n\nThe postsynaptic response $y(t)$ is linearly proportional to the presynaptic input $x(t)$ and the synaptic weight $w(t)$:\n$$y(t) = w(t) x(t)$$\nThe synaptic weight is constrained to be non-negative:\n$$w(t) \\ge 0$$\nThe presynaptic input is assumed to be constant and positive:\n$$x(t) \\equiv x, \\quad \\text{with } x > 0$$\nUnder this condition, the postsynaptic response simplifies to:\n$$y(t) = w(t) x$$\nThe BCM learning rule is given by:\n$$\\dot{w}(t) = \\eta \\, x(t) \\, y(t) \\big(y(t) - \\theta\\big)$$\nwhere $\\dot{w}(t)$ is the time derivative of the weight, $\\eta > 0$ is the learning rate, and $\\theta > 0$ is a fixed activity threshold.\n\nThe first step is to derive the autonomous differential equation for $w(t)$. We substitute the expressions for $x(t)$ and $y(t)$ into the BCM rule:\n$$\\dot{w}(t) = \\eta \\, x \\, (w(t)x) \\big(w(t)x - \\theta\\big)$$\nBy rearranging terms, we obtain the autonomous ordinary differential equation (ODE) for $w(t)$, as the right-hand side depends only on $w(t)$ and not explicitly on time $t$:\n$$\\dot{w}(t) = \\eta x^2 w(t) \\big(w(t)x - \\theta\\big)$$\nLet's define the right-hand side function as $F(w) = \\eta x^2 w (wx - \\theta)$. The sign of $\\dot{w}(t) = F(w(t))$ determines whether the synapse undergoes potentiation ($\\dot{w} > 0$) or depression ($\\dot{w} < 0$).\n\nNext, we perform a sign analysis of $F(w)$. The critical points (or fixed points) of the dynamics occur when $\\dot{w}(t) = 0$, which means $F(w) = 0$.\n$$\\eta x^2 w (wx - \\theta) = 0$$\nSince we are given that $\\eta > 0$ and $x > 0$, the term $\\eta x^2$ is strictly positive. Therefore, the equation is satisfied if either $w = 0$ or $(wx - \\theta) = 0$.\nThis gives two critical points:\n1. $w_1 = 0$\n2. $wx - \\theta = 0 \\implies w_2 = \\frac{\\theta}{x}$\n\nThe problem asks for the unique nonzero critical weight $w_{\\mathrm{c}}$ where plasticity switches from depression to potentiation. This corresponds to $w_2$.\n$$w_{\\mathrm{c}} = \\frac{\\theta}{x}$$\nSince $\\theta > 0$ and $x > 0$, $w_{\\mathrm{c}}$ is a positive weight, consistent with the constraint $w(t) \\ge 0$.\n\nNow we analyze the sign of $\\dot{w}$ in the regions defined by these critical points for $w > 0$. The sign of $\\dot{w} = \\eta x^2 w (wx - \\theta)$ is determined by the sign of the term $(wx - \\theta)$, because $\\eta x^2 > 0$ and we are considering $w > 0$.\n\nCase 1: Depression ($\\dot{w} < 0$).\nThis occurs when $(wx - \\theta) < 0$, which implies $wx < \\theta$. Solving for $w$ gives:\n$$w < \\frac{\\theta}{x} \\implies w < w_{\\mathrm{c}}$$\nSo, for any weight $w$ in the interval $0 < w < w_{\\mathrm{c}}$, the synapse will weaken ($\\dot{w} < 0$).\n\nCase 2: Potentiation ($\\dot{w} > 0$).\nThis occurs when $(wx - \\theta) > 0$, which implies $wx > \\theta$. Solving for $w$ gives:\n$$w > \\frac{\\theta}{x} \\implies w > w_{\\mathrm{c}}$$\nFor any weight $w > w_{\\mathrm{c}}$, the synapse will strengthen ($\\dot{w} > 0$).\n\nThis sign analysis justifies the interpretation of the BCM rule as a thresholded Hebbian rule. The post-synaptic activity is $y = wx$. The condition for switching from depression to potentiation, $w = w_{\\mathrm{c}} = \\frac{\\theta}{x}$, is equivalent to the postsynaptic activity reaching the threshold:\n$$y = w_{\\mathrm{c}}x = \\left(\\frac{\\theta}{x}\\right)x = \\theta$$\nThus, the BCM rule implements the following logic:\n- If postsynaptic activity is low ($y < \\theta$), the synapse depresses. This corresponds to Long-Term Depression (LTD).\n- If postsynaptic activity is high ($y > \\theta$), the synapse potentiates. This corresponds to Long-Term Potentiation (LTP).\n\nThe Hebbian character of the rule is evident from the multiplicative dependence on presynaptic ($x$) and postsynaptic ($y$) activity in the term $\\eta x y$. The factor $(y - \\theta)$ introduces the threshold that governs the direction of plasticity, making it a \"thresholded Hebbian rule\". The critical weight $w_{\\mathrm{c}}$ is the specific weight value at which, for a given input $x$, the postsynaptic response $y$ equals the threshold $\\theta$, marking the boundary between depression and potentiation.\n\nThe final answer required is the closed-form analytic expression for this switching weight $w_{\\mathrm{c}}$. As derived from the analysis of the critical points of the autonomous ODE, this expression is:\n$$w_{\\mathrm{c}} = \\frac{\\theta}{x}$$",
            "answer": "$$\\boxed{\\frac{\\theta}{x}}$$"
        },
        {
            "introduction": "Having established the foundational mechanics of BCM, we now apply it to a scenario that mimics a neuron learning from its environment. This practice challenges you to compute the expected synaptic changes when a neuron is exposed to two different input patterns. By calculating the trajectory of the synaptic weights, you will directly observe how BCM dynamics lead to the emergence of neuronal selectivity, a key process for feature detection and learning in the brain .",
            "id": "4037209",
            "problem": "Consider a single linear neuron with synaptic weight vector $w \\in \\mathbb{R}^{2}$ receiving presynaptic input patterns $x \\in \\mathbb{R}^{2}$. The output of the neuron is $y = w^{\\top} x$. Two distinct, fixed input patterns $x^{(1)}$ and $x^{(2)}$ are presented with equal frequency, so that the input $x$ is drawn independently from $\\{x^{(1)}, x^{(2)}\\}$ with probability $1/2$ each. Assume synaptic plasticity follows the Bienenstock-Cooper-Munro (BCM) learning rule: synaptic change is proportional to presynaptic activity multiplied by a modification function that depends on postsynaptic activity relative to a sliding threshold, and the sliding threshold on a slower timescale equilibrates to the long-term average of the squared postsynaptic activity under the current distribution of inputs. The learning rate constant is $1$.\n\nLet the initial synaptic weight be $w = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$, and the two input patterns be\n$$\nx^{(1)} = \\begin{pmatrix} 2 \\\\ \\frac{2}{5} \\end{pmatrix}, \\qquad x^{(2)} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\end{pmatrix}.\n$$\nTreat the threshold as equilibrated to the current expected squared output computed with this $w$ and the given input distribution. Using the BCM framework and the equal-frequency input distribution, compute:\n\n1. The expected instantaneous weight change vector $\\mathbb{E}\\!\\left[\\frac{dw}{dt}\\right]$ at the stated $w$ and inputs.\n2. Which of the two patterns the neuron will become selective for under BCM dynamics, encoded as $k = 1$ if selective for $x^{(1)}$ and $k = 2$ if selective for $x^{(2)}$.\n\nReport your final answer as a single row matrix containing the two components of $\\mathbb{E}\\!\\left[\\frac{dw}{dt}\\right]$ followed by $k$, in the form $\\begin{pmatrix} \\Delta w_{1} & \\Delta w_{2} & k \\end{pmatrix}$. Provide exact values; do not round.",
            "solution": "The user wants me to solve a problem based on the Bienenstock-Cooper-Munro (BCM) learning rule.\n\n### Step 1: Extract Givens\n- Neuron: Single linear neuron.\n- Synaptic weight vector: $w \\in \\mathbb{R}^{2}$, initial value $w = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$.\n- Presynaptic input patterns: $x \\in \\mathbb{R}^{2}$.\n- Neuron output: $y = w^{\\top} x$.\n- Input distribution: Two fixed patterns, $x^{(1)} = \\begin{pmatrix} 2 \\\\ \\frac{2}{5} \\end{pmatrix}$ and $x^{(2)} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\end{pmatrix}$, occur with equal frequency, $P(x=x^{(1)}) = P(x=x^{(2)}) = \\frac{1}{2}$.\n- Learning rule: BCM rule, $\\frac{dw}{dt} \\propto x \\cdot \\phi(y, \\theta_M)$. The standard form is $\\frac{dw}{dt} = \\eta x y (y - \\theta_M)$.\n- Learning rate constant: $\\eta = 1$.\n- Sliding threshold $\\theta_M$: Equilibrated to the long-term average of the squared postsynaptic activity, $\\theta_M = \\mathbb{E}[y^2]$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes a canonical application of the BCM learning rule, a well-established model in computational neuroscience. All premises are based on standard formulations of this model.\n- **Well-Posed:** The problem provides all necessary initial conditions, parameters, and definitions to compute the requested quantities. A unique solution exists.\n- **Objective:** The problem is stated using precise mathematical language and contains no subjective or ambiguous terms.\n- **Completeness:** The problem is self-contained. All variables and constants are defined.\n- **Consistency:** The provided data and conditions are internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nThe BCM learning rule for synaptic weight change is given by:\n$$\n\\frac{dw}{dt} = \\eta y \\phi(y, \\theta_M) x\n$$\nThe problem description implies the standard modification function $\\phi(y, \\theta_M) = y - \\theta_M$, which results in the full learning rule:\n$$\n\\frac{dw}{dt} = \\eta y (y - \\theta_M) x\n$$\nThe problem states that the learning rate constant is $\\eta = 1$. The sliding threshold $\\theta_M$ is the long-term average of the squared postsynaptic activity:\n$$\n\\theta_M = \\mathbb{E}[y^2]\n$$\nThe neuron's output is $y = w^{\\top} x$. We are given the initial weight vector $w = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$.\n\nFirst, we compute the postsynaptic activity for each of the two input patterns:\n$$\ny^{(1)} = w^{\\top} x^{(1)} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 \\\\ \\frac{2}{5} \\end{pmatrix} = \\frac{1}{2}(2) + \\frac{1}{2}\\left(\\frac{2}{5}\\right) = 1 + \\frac{1}{5} = \\frac{6}{5}\n$$\n$$\ny^{(2)} = w^{\\top} x^{(2)} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\end{pmatrix} = \\frac{1}{2}\\left(\\frac{1}{10}\\right) + \\frac{1}{2}\\left(\\frac{1}{10}\\right) = \\frac{1}{20} + \\frac{1}{20} = \\frac{2}{20} = \\frac{1}{10}\n$$\n\nNext, we compute the threshold $\\theta_M$. Since the patterns are presented with equal probability $P(x^{(1)}) = P(x^{(2)}) = \\frac{1}{2}$, the expectation is:\n$$\n\\theta_M = \\mathbb{E}[y^2] = P(x^{(1)}) (y^{(1)})^2 + P(x^{(2)}) (y^{(2)})^2\n$$\n$$\n\\theta_M = \\frac{1}{2} \\left(\\frac{6}{5}\\right)^2 + \\frac{1}{2} \\left(\\frac{1}{10}\\right)^2 = \\frac{1}{2} \\left(\\frac{36}{25}\\right) + \\frac{1}{2} \\left(\\frac{1}{100}\\right) = \\frac{18}{25} + \\frac{1}{200}\n$$\nTo sum these fractions, we find a common denominator of $200$:\n$$\n\\theta_M = \\frac{18 \\times 8}{25 \\times 8} + \\frac{1}{200} = \\frac{144}{200} + \\frac{1}{200} = \\frac{145}{200} = \\frac{29}{40}\n$$\n\n### Part 1: Compute the expected instantaneous weight change\nThe expected weight change is found by taking the expectation of the learning rule over the input distribution:\n$$\n\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\mathbb{E}\\left[y (y - \\theta_M) x\\right] = \\sum_{i=1}^{2} P(x^{(i)}) y^{(i)} (y^{(i)} - \\theta_M) x^{(i)}\n$$\n$$\n\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\frac{1}{2} y^{(1)} (y^{(1)} - \\theta_M) x^{(1)} + \\frac{1}{2} y^{(2)} (y^{(2)} - \\theta_M) x^{(2)}\n$$\nWe first compute the modification terms $(y - \\theta_M)$:\n$$\ny^{(1)} - \\theta_M = \\frac{6}{5} - \\frac{29}{40} = \\frac{48}{40} - \\frac{29}{40} = \\frac{19}{40}\n$$\n$$\ny^{(2)} - \\theta_M = \\frac{1}{10} - \\frac{29}{40} = \\frac{4}{40} - \\frac{29}{40} = -\\frac{25}{40} = -\\frac{5}{8}\n$$\nNow we substitute these values into the expression for the expected weight change:\n$$\n\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\frac{1}{2} \\left(\\frac{6}{5}\\right) \\left(\\frac{19}{40}\\right) x^{(1)} + \\frac{1}{2} \\left(\\frac{1}{10}\\right) \\left(-\\frac{5}{8}\\right) x^{(2)}\n$$\n$$\n\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\left(\\frac{57}{200}\\right) x^{(1)} - \\left(\\frac{5}{160}\\right) x^{(2)} = \\left(\\frac{57}{200}\\right) x^{(1)} - \\left(\\frac{1}{32}\\right) x^{(2)}\n$$\nNow, we substitute the input vectors $x^{(1)}$ and $x^{(2)}$:\n$$\n\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\frac{57}{200} \\begin{pmatrix} 2 \\\\ \\frac{2}{5} \\end{pmatrix} - \\frac{1}{32} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{114}{200} \\\\ \\frac{114}{1000} \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{320} \\\\ \\frac{1}{320} \\end{pmatrix} = \\begin{pmatrix} \\frac{57}{100} \\\\ \\frac{57}{500} \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{320} \\\\ \\frac{1}{320} \\end{pmatrix}\n$$\nWe compute each component separately.\nFor the first component, $\\Delta w_1$:\n$$\n\\Delta w_1 = \\frac{57}{100} - \\frac{1}{320}\n$$\nThe least common multiple of $100$ and $320$ is $1600$.\n$$\n\\Delta w_1 = \\frac{57 \\times 16}{1600} - \\frac{1 \\times 5}{1600} = \\frac{912 - 5}{1600} = \\frac{907}{1600}\n$$\nFor the second component, $\\Delta w_2$:\n$$\n\\Delta w_2 = \\frac{57}{500} - \\frac{1}{320}\n$$\nThe least common multiple of $500$ and $320$ is $8000$.\n$$\n\\Delta w_2 = \\frac{57 \\times 16}{8000} - \\frac{1 \\times 25}{8000} = \\frac{912 - 25}{8000} = \\frac{887}{8000}\n$$\nSo, the expected instantaneous weight change vector is $\\mathbb{E}\\left[\\frac{dw}{dt}\\right] = \\begin{pmatrix} \\frac{907}{1600} \\\\ \\frac{887}{8000} \\end{pmatrix}$.\n\n### Part 2: Determine neuronal selectivity\nThe BCM rule leads to synaptic potentiation (LTP) when postsynaptic activity is high and depression (LTD) when it is low. The threshold $\\theta_M$ separates these two regimes. The sign of the modification term $y(y - \\theta_M)$ determines the direction of plasticity.\nFor pattern $x^{(1)}$:\nThe modification factor is $y^{(1)}(y^{(1)} - \\theta_M) = \\frac{6}{5}\\left(\\frac{19}{40}\\right) = \\frac{114}{200} = \\frac{57}{100} > 0$.\nA positive factor means that when pattern $x^{(1)}$ is presented, the weight vector $w$ is moved in the direction of $x^{(1)}$, strengthening the synapses and increasing the neuron's response to this pattern. This corresponds to LTP.\n\nFor pattern $x^{(2)}$:\nThe modification factor is $y^{(2)}(y^{(2)} - \\theta_M) = \\frac{1}{10}\\left(-\\frac{5}{8}\\right) = -\\frac{5}{80} = -\\frac{1}{16} < 0$.\nA negative factor means that when pattern $x^{(2)}$ is presented, the weight vector $w$ is moved in the direction opposite to $x^{(2)}$, weakening the synapses and decreasing the neuron's response to this pattern. This corresponds to LTD.\n\nSince the dynamics reinforce the response to $x^{(1)}$ and suppress the response to $x^{(2)}$, the neuron will become selective for pattern $x^{(1)}$. As per the problem's encoding, this corresponds to $k=1$.\n\nThe final answer requires the two components of the expected weight change vector, $\\Delta w_1$ and $\\Delta w_2$, and the selectivity indicator $k$.\n$\\Delta w_1 = \\frac{907}{1600}$\n$\\Delta w_2 = \\frac{887}{8000}$\n$k = 1$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{907}{1600} & \\frac{887}{8000} & 1 \\end{pmatrix}}\n$$"
        }
    ]
}