{
    "hands_on_practices": [
        {
            "introduction": "We begin by forging a crucial link between the neurobiological concept of Reward-Modulated Spike-Timing-Dependent Plasticity (R-STDP) and the abstract framework of reinforcement learning. This exercise  walks through a minimalist two-state task to demonstrate how the weight updates generated by a three-factor synaptic plasticity rule are mathematically equivalent to those of Temporal-Difference (TD) learning with eligibility traces. Understanding this equivalence is fundamental to viewing R-STDP as a plausible neural implementation of sophisticated learning algorithms.",
            "id": "4057751",
            "problem": "Consider a minimal episodic task modeled as a two-state Markov chain with states $A$ and $B$ and a terminal absorbing state $\\perp$. The episode begins at time $t=0$ in state $A$, transitions deterministically to state $B$ at $t=1$, and terminates at $t=2$ in state $\\perp$. A scalar reward $R$ is delivered only at termination; that is, $r_0=0$, $r_1=R$, and $r_2$ is undefined because the episode ends at $t=2$ upon entering $\\perp$. This task is used to train a single-parameter linear value function $V(s;w)=w\\,x(s)$, where $w\\in\\mathbb{R}$ is the synaptic weight to be adapted, and $x(s)\\in\\mathbb{R}$ is a fixed feature of state $s$. Assume $x(A)=x_0$ and $x(B)=x_1$ are given constants. The discount factor is $\\gamma\\in(0,1)$, and the eligibility trace parameter is $\\lambda\\in[0,1]$. The learning rate is $\\eta0$.\n\nYou will compute the one-episode synaptic weight update using Temporal-Difference (TD) learning of order zero, also known as Temporal-Difference (TD(0)), with eligibility traces, and then verify its equivalence to a reward-modulated three-factor Spike-Timing-Dependent Plasticity (STDP) rule. Use the following fundamental definitions:\n\n- The Temporal-Difference error at time $t$ is $\\delta_t=r_t+\\gamma\\,V(s_{t+1};w)-V(s_t;w)$, with the convention $V(\\perp;w)=0$.\n- The eligibility trace obeys $e_t=\\gamma\\,\\lambda\\,e_{t-1}+x(s_t)$, with $e_{-1}=0$.\n\nIn a neuromorphic formulation of reward-modulated STDP, the synaptic update is a three-factor product, where the global modulatory signal $M_t$ gates a local eligibility $e_t$ accrued from pre- and post-synaptic activity; in this task, identify $M_t$ with the Temporal-Difference error, $M_t \\equiv \\delta_t$, consistent with reward prediction error signaling.\n\nStarting from these definitions and the Markov chain structure described above, derive the single-episode synaptic weight increment $\\Delta w$ produced by TD(0) with eligibility traces and explicitly show that the same $\\Delta w$ results from the three-factor reward-modulated STDP rule with $M_t=\\delta_t$. Express your final answer as a closed-form analytic expression in terms of the symbols $\\eta$, $\\gamma$, $\\lambda$, $R$, $w$, $x_0$, and $x_1$. No numerical evaluation is required, and no units need be attached to the final expression. The final answer must be a single closed-form expression; do not present it as an equation to be solved.",
            "solution": "The problem statement is subjected to validation before proceeding with a solution.\n\n### Step 1: Extract Givens\n- **Task Structure**: A two-state Markov chain with states $A$ and $B$, and a terminal absorbing state $\\perp$.\n- **Episode Dynamics**: The episode starts at $t=0$ in state $s_0=A$, transitions to $s_1=B$ at $t=1$, and terminates in $s_2=\\perp$ at $t=2$.\n- **Reward Schedule**: $r_0=0$, $r_1=R$.\n- **Value Function**: A single-parameter linear value function $V(s;w)=w\\,x(s)$, where $w \\in \\mathbb{R}$ is the weight and $x(s) \\in \\mathbb{R}$ is a state feature.\n- **State Features**: $x(A)=x_0$ and $x(B)=x_1$.\n- **Learning Parameters**: Discount factor $\\gamma \\in (0,1)$, eligibility trace parameter $\\lambda \\in [0,1]$, learning rate $\\eta  0$.\n- **Temporal-Difference (TD) Error**: $\\delta_t=r_t+\\gamma\\,V(s_{t+1};w)-V(s_t;w)$, with the convention $V(\\perp;w)=0$.\n- **Eligibility Trace**: $e_t=\\gamma\\,\\lambda\\,e_{t-1}+x(s_t)$, with $e_{-1}=0$.\n- **Three-Factor Rule Formulation**: The synaptic update is described as a product of a global modulatory signal $M_t$ and a local eligibility $e_t$.\n- **Identification**: The modulatory signal is identified with the TD error, $M_t \\equiv \\delta_t$.\n- **Objective**: Derive the single-episode weight increment $\\Delta w$ using TD learning with eligibility traces, and show its equivalence to the specified three-factor rule. The result must be a closed-form expression in terms of $\\eta, \\gamma, \\lambda, R, w, x_0, x_1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard exercise in reinforcement learning theory (TD($\\lambda$)) and its application in computational neuroscience as a model for reward-modulated synaptic plasticity. All terms are formally defined, and the components (Markov chain, linear value function, learning rules) are standard in the literature. The problem is well-posed, self-contained, and objective, with all necessary parameters and equations provided for a unique solution. There are no scientific or factual unsoundness, no contradictions, and no ambiguities in the definitions. The problem is a formalizable and substantive mathematical derivation.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A complete, reasoned solution follows.\n\n### Derivation of the Synaptic Weight Update\nThe total change in the synaptic weight $w$ over a single episode is given by the sum of updates at each time step. The learning rule is Temporal-Difference learning with eligibility traces, commonly known as TD($\\lambda$). The update rule is:\n$$\n\\Delta w = \\eta \\sum_{t=0}^{T-1} \\delta_t e_t\n$$\nIn this problem, the episode consists of transitions from $t=0$ to $t=1$, with termination at $t=2$. Thus, the summation is over $t=0, 1$.\n$$\n\\Delta w = \\eta (\\delta_0 e_0 + \\delta_1 e_1)\n$$\nTo calculate $\\Delta w$, we must first determine the values of the eligibility traces $e_0$ and $e_1$, and the TD errors $\\delta_0$ and $\\delta_1$.\n\n**1. Calculation of Eligibility Traces ($e_t$)**\nThe eligibility trace evolves according to the recurrence relation $e_t=\\gamma\\,\\lambda\\,e_{t-1}+x(s_t)$, with the initial condition $e_{-1}=0$.\n\nFor $t=0$:\nThe state is $s_0=A$, so the feature is $x(s_0)=x_0$.\n$$\ne_0 = \\gamma\\,\\lambda\\,e_{-1} + x(s_0) = \\gamma\\,\\lambda\\,(0) + x_0 = x_0\n$$\n\nFor $t=1$:\nThe state is $s_1=B$, so the feature is $x(s_1)=x_1$.\n$$\ne_1 = \\gamma\\,\\lambda\\,e_{0} + x(s_1) = \\gamma\\,\\lambda\\,x_0 + x_1\n$$\n\n**2. Calculation of TD Errors ($\\delta_t$)**\nThe TD error is defined as $\\delta_t=r_t+\\gamma\\,V(s_{t+1};w)-V(s_t;w)$. The value function is $V(s;w)=w\\,x(s)$.\n\nFor $t=0$:\nThe states are $s_0=A$ and $s_1=B$. The reward is $r_0=0$.\nThe value functions are $V(s_0;w) = V(A;w) = w\\,x_0$ and $V(s_1;w) = V(B;w) = w\\,x_1$.\n$$\n\\delta_0 = r_0 + \\gamma\\,V(s_1;w) - V(s_0;w) = 0 + \\gamma\\,(w\\,x_1) - w\\,x_0 = w(\\gamma\\,x_1 - x_0)\n$$\n\nFor $t=1$:\nThe states are $s_1=B$ and the terminal state $s_2=\\perp$. The reward is $r_1=R$.\nThe value functions are $V(s_1;w) = V(B;w) = w\\,x_1$ and by convention, $V(s_2;w) = V(\\perp;w) = 0$.\n$$\n\\delta_1 = r_1 + \\gamma\\,V(s_2;w) - V(s_1;w) = R + \\gamma\\,(0) - w\\,x_1 = R - w\\,x_1\n$$\n\n**3. Calculation of the Total Weight Update ($\\Delta w$)**\nNow we substitute the expressions for $e_0, e_1, \\delta_0, \\delta_1$ into the equation for $\\Delta w$:\n$$\n\\Delta w = \\eta (\\delta_0 e_0 + \\delta_1 e_1)\n$$\n$$\n\\Delta w = \\eta \\left[ \\left( w(\\gamma\\,x_1 - x_0) \\right) (x_0) + \\left( R - w\\,x_1 \\right) (\\gamma\\,\\lambda\\,x_0 + x_1) \\right]\n$$\nWe expand the terms inside the brackets:\n$$\n\\Delta w = \\eta \\left[ (w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2) + (R(\\gamma\\,\\lambda\\,x_0 + x_1) - w\\,x_1(\\gamma\\,\\lambda\\,x_0 + x_1)) \\right]\n$$\n$$\n\\Delta w = \\eta \\left[ w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2 + R\\,\\gamma\\,\\lambda\\,x_0 + R\\,x_1 - w\\,\\gamma\\,\\lambda\\,x_0\\,x_1 - w\\,x_1^2 \\right]\n$$\nTo obtain the final closed-form expression, we group terms involving the reward $R$ and terms involving the current weight $w$:\n$$\n\\Delta w = \\eta \\left[ (R\\,\\gamma\\,\\lambda\\,x_0 + R\\,x_1) + (w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2 - w\\,\\gamma\\,\\lambda\\,x_0\\,x_1 - w\\,x_1^2) \\right]\n$$\nFactoring out $R$ and $w$:\n$$\n\\Delta w = \\eta \\left[ R(\\gamma\\,\\lambda\\,x_0 + x_1) + w(\\gamma\\,x_0\\,x_1 - x_0^2 - \\gamma\\,\\lambda\\,x_0\\,x_1 - x_1^2) \\right]\n$$\nThe term multiplying $w$ can be rearranged for clarity:\n$$\n\\gamma\\,x_0\\,x_1 - x_0^2 - \\gamma\\,\\lambda\\,x_0\\,x_1 - x_1^2 = -(x_0^2 + x_1^2 - \\gamma\\,x_0\\,x_1 + \\gamma\\,\\lambda\\,x_0\\,x_1) = -(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0\\,x_1)\n$$\nSubstituting this back gives the final expression:\n$$\n\\Delta w = \\eta \\left[ R(\\gamma\\,\\lambda\\,x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0\\,x_1) \\right]\n$$\n\n**4. Equivalence with the Three-Factor Rule**\nThe problem defines the reward-modulated three-factor STDP rule as a product of a global modulatory signal $M_t$ gating a local eligibility $e_t$. The total update is the sum of these products over time, scaled by a learning rate $\\eta$:\n$$\n\\Delta w = \\eta \\sum_t M_t e_t\n$$\nThe problem explicitly provides the identification $M_t \\equiv \\delta_t$. Substituting this into the three-factor rule gives:\n$$\n\\Delta w = \\eta \\sum_t \\delta_t e_t\n$$\nThis is precisely the definition of the update rule for TD($\\lambda$) that was used as the starting point of the derivation. Therefore, the equivalence is established by the definitions provided in the problem statement. The calculation performed above for the TD($\\lambda$) update is, by this identification, simultaneously the calculation for the specified three-factor STDP rule. The problem reduces to deriving the single-episode weight update under the TD($\\lambda$) framework, which has been completed.",
            "answer": "$$\n\\boxed{\\eta \\left[ R(\\gamma\\lambda x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0 x_1) \\right]}\n$$"
        },
        {
            "introduction": "For a learning agent to perform robustly, its learning rule must effectively handle the stochasticity of rewards from the environment. This practice  addresses this challenge by exploring the concept of a reward baseline, which is subtracted from the raw reward to generate a more informative prediction error signal. You will perform a key optimization by deriving the constant baseline value that minimizes the variance of synaptic updates, a fundamental technique for accelerating convergence and stabilizing learning.",
            "id": "4057771",
            "problem": "A single synapse in a reward-modulated Spike-Timing-Dependent Plasticity (STDP) rule updates its weight according to $\\Delta w = \\eta\\, e\\,(R - b)$, where $\\eta$ is a fixed learning rate, $e$ is a binary eligibility flag indicating the presence of a causal pre–post spike-timing event within a prescribed window (so $e \\in \\{0,1\\}$ and $e^2=e$), $R$ is a bounded scalar reward delivered after the event, and $b$ is a constant reward baseline. Across trials, $(e,R)$ are independent and identically distributed samples from a stationary joint distribution with finite second moments, and $\\mathbb{E}[e] \\neq 0$. The baseline $b$ is constrained to be a constant that does not depend on trial outcomes.\n\nStarting from the definitions of variance and expectation, and without assuming any additional structure on the joint distribution beyond the conditions stated, derive the value of the constant baseline $b^{*}$ that minimizes the variance $\\mathrm{Var}\\!\\big(e\\,(R - b)\\big)$. Express your final answer as a closed-form symbolic expression in terms of expectations of $e$ and $R$ only.\n\nThen, briefly justify how $b^{*}$ can be estimated online from data using a stable recursive scheme that does not require storing the entire history, and indicate how to guard against division by zero when $\\mathbb{E}[e]$ is small.\n\nProvide your final answer for $b^{*}$ as a single analytical expression. No numerical approximation is required, and no units are needed.",
            "solution": "The problem requires the derivation of the constant reward baseline $b$ that minimizes the variance of the synaptic weight update term, $\\mathrm{Var}\\!\\big(e\\,(R - b)\\big)$. We are given that the learning rate $\\eta$ is a constant, so minimizing $\\mathrm{Var}\\!\\big(\\eta\\,e\\,(R - b)\\big)$ is equivalent to minimizing $\\mathrm{Var}\\!\\big(e\\,(R - b)\\big)$, as $\\mathrm{Var}(cX) = c^2\\mathrm{Var}(X)$.\n\nLet the random variable for the update term be $Y(b) = e(R - b)$. We wish to find the value of the constant $b$, denoted $b^*$, that minimizes $V(b) = \\mathrm{Var}(Y(b))$.\nWe can write $Y(b)$ as $eR - be$. The term $eR$ is a random variable, and the term $be$ is a constant $b$ multiplying the random variable $e$. We can use the property of variance for a linear combination of random variables. For random variables $X_1$ and $X_2$ and a constant $c$, $\\mathrm{Var}(X_1 - cX_2) = \\mathrm{Var}(X_1) - 2c\\mathrm{Cov}(X_1, X_2) + c^2\\mathrm{Var}(X_2)$.\n\nLet's identify $X_1 = eR$ and $X_2 = e$, with the constant being $b$. The variance we want to minimize is:\n$$V(b) = \\mathrm{Var}(eR - be) = \\mathrm{Var}(eR) - 2b\\mathrm{Cov}(eR, e) + b^2\\mathrm{Var}(e)$$\nThis expression for $V(b)$ is a quadratic function of $b$. To find the value of $b$ that minimizes $V(b)$, we can take the derivative with respect to $b$ and set it to zero.\n$$\\frac{d V(b)}{db} = -2\\mathrm{Cov}(eR, e) + 2b\\mathrm{Var}(e)$$\nSetting the derivative to zero to find the minimum:\n$$ -2\\mathrm{Cov}(eR, e) + 2b^*\\mathrm{Var}(e) = 0 $$\n$$ 2b^*\\mathrm{Var}(e) = 2\\mathrm{Cov}(eR, e) $$\nProvided that $\\mathrm{Var}(e) \\neq 0$, we can solve for $b^*$:\n$$ b^* = \\frac{\\mathrm{Cov}(eR, e)}{\\mathrm{Var}(e)} $$\nThe second derivative is $\\frac{d^2 V(b)}{db^2} = 2\\mathrm{Var}(e)$. Since $e$ is a binary random variable, $e \\in \\{0, 1\\}$, its variance is $\\mathrm{Var}(e) = \\mathbb{E}[e](1 - \\mathbb{E}[e])$. The problem states $\\mathbb{E}[e] \\neq 0$. If $\\mathbb{E}[e]=1$, $e$ is a constant ($e=1$ almost surely), so $\\mathrm{Var}(e)=0$. In this case, the variance $V(b) = \\mathrm{Var}(R-b) = \\mathrm{Var}(R)$, which does not depend on $b$, and any $b$ is a minimizer. The problem phrasing \"the value\" implies a unique solution, so we must assume $\\mathbb{E}[e] \\in (0, 1)$, which makes $\\mathrm{Var}(e)  0$. Therefore, the second derivative is positive, and our solution for $b^*$ corresponds to a unique minimum.\n\nNow, we must express $b^*$ in terms of expectations of $e$ and $R$. We use the definitions of variance and covariance.\nThe variance of $e$ is:\n$$ \\mathrm{Var}(e) = \\mathbb{E}[e^2] - (\\mathbb{E}[e])^2 $$\nSince $e$ is a binary flag with $e \\in \\{0, 1\\}$, we have $e^2 = e$. Thus,\n$$ \\mathrm{Var}(e) = \\mathbb{E}[e] - (\\mathbb{E}[e])^2 = \\mathbb{E}[e](1 - \\mathbb{E}[e]) $$\nThe covariance between $eR$ and $e$ is:\n$$ \\mathrm{Cov}(eR, e) = \\mathbb{E}[(eR)e] - \\mathbb{E}[eR]\\mathbb{E}[e] $$\nAgain, using $e^2 = e$, this simplifies to:\n$$ \\mathrm{Cov}(eR, e) = \\mathbb{E}[e^2R] - \\mathbb{E}[eR]\\mathbb{E}[e] = \\mathbb{E}[eR] - \\mathbb{E}[eR]\\mathbb{E}[e] = \\mathbb{E}[eR](1 - \\mathbb{E}[e]) $$\nSubstituting these expressions back into the equation for $b^*$:\n$$ b^* = \\frac{\\mathbb{E}[eR](1 - \\mathbb{E}[e])}{\\mathbb{E}[e](1 - \\mathbb{E}[e])} $$\nSince we have established that $\\mathbb{E}[e] \\in (0, 1)$, the term $(1 - \\mathbb{E}[e])$ is non-zero, and we can cancel it from the numerator and denominator. This yields the final expression for the optimal baseline:\n$$ b^* = \\frac{\\mathbb{E}[eR]}{\\mathbb{E}[e]} $$\nThis expression has an intuitive interpretation as the conditional expectation of the reward $R$ given that a causal spike-timing event has occurred, i.e., $b^* = \\mathbb{E}[R|e=1]$.\n\nFor the second part of the question, we are asked to justify how $b^*$ can be estimated online. The optimal baseline $b^*$ is a ratio of two expectations, $\\mathbb{E}[eR]$ and $\\mathbb{E}[e]$. Given a stream of data samples $(e_t, R_t)$ from trials $t=1, 2, \\dots$, one can estimate each expectation recursively using an exponentially weighted moving average (EWMA). Let $\\hat{\\mu}_{eR, t}$ and $\\hat{\\mu}_{e, t}$ be the estimates at trial $t$, and $\\alpha$ be a small learning rate:\n$$ \\hat{\\mu}_{eR, t} = \\hat{\\mu}_{eR, t-1} + \\alpha(e_t R_t - \\hat{\\mu}_{eR, t-1}) $$\n$$ \\hat{\\mu}_{e, t} = \\hat{\\mu}_{e, t-1} + \\alpha(e_t - \\hat{\\mu}_{e, t-1}) $$\nThe baseline is then estimated as the ratio $\\hat{b}_t = \\frac{\\hat{\\mu}_{eR, t}}{\\hat{\\mu}_{e, t}}$. This scheme is recursive, stable, and does not require storing the data history.\n\nTo guard against division by zero or by a very small number (which can happen if $\\mathbb{E}[e]$ is small, especially early in the estimation), one can regularize the denominator by adding a small positive constant $\\epsilon$: $\\hat{b}_t = \\frac{\\hat{\\mu}_{eR, t}}{\\hat{\\mu}_{e, t} + \\epsilon}$.\nAlternatively, a more elegant online scheme arises from noting that the derived $b^*$ is also the value that ensures the average synaptic update is zero: $\\mathbb{E}[\\Delta w] \\propto \\mathbb{E}[e(R-b)] = 0$. This suggests a direct stochastic approximation algorithm for $b^*$ itself:\n$$ \\hat{b}_t = \\hat{b}_{t-1} + \\alpha_b \\, e_t (R_t - \\hat{b}_{t-1}) $$\nwhere $\\alpha_b$ is a learning rate for the baseline. The fixed point of this update occurs when $\\mathbb{E}[e(R-b)]=0$, which solves to $b = \\frac{\\mathbb{E}[eR]}{\\mathbb{E}[e]} = b^*$. This update rule is recursive, inherently avoids division, and remains numerically stable even if $\\mathbb{E}[e]$ is small, although convergence will be slower in that case.",
            "answer": "$$\\boxed{\\frac{\\mathbb{E}[eR]}{\\mathbb{E}[e]}}$$"
        },
        {
            "introduction": "This final practice transitions from pen-and-paper theory to hands-on computational neuroscience, challenging you to build a spiking neural network that learns. In this simulation , you will implement a complete R-STDP learning system—including Poisson spike generation, eligibility traces, and action selection—to solve a classic two-armed bandit problem. The core task is to empirically investigate how the variance of the reward baseline, a parameter you analyzed theoretically in , directly impacts the learning speed and performance of the network.",
            "id": "4057793",
            "problem": "Consider a two-action bandit-like spiking network controlled by Reward-Modulated Spike-Timing-Dependent Plasticity (STDP). There are $N_{\\mathrm{pre}}$ independent Poisson input neurons feeding two competing output (action) neurons. On each trial, input neurons emit spikes and the action neurons produce at most one spike; the action with the earliest spike is taken. The obtained scalar reward modulates synaptic plasticity via a baseline-subtracted factor.\n\nUse the following fundamental bases and definitions to construct and analyze the dynamics:\n\n- Spike-Timing-Dependent Plasticity (STDP): For a presynaptic spike at time $t_{\\mathrm{pre}}$ and a postsynaptic spike at time $t_{\\mathrm{post}}$, define the STDP window function $F(\\Delta t)$ with $\\Delta t = t_{\\mathrm{post}} - t_{\\mathrm{pre}}$ as\n$$\nF(\\Delta t) = \n\\begin{cases}\nA_{+} \\exp\\left(-\\frac{\\Delta t}{\\tau_{+}}\\right),  \\Delta t  0 \\\\\n- A_{-} \\exp\\left(\\frac{\\Delta t}{\\tau_{-}}\\right),  \\Delta t  0\n\\end{cases}\n$$\nwhere $A_{+}  0$, $A_{-}  0$, $\\tau_{+}  0$, and $\\tau_{-}  0$.\n\n- Reward-Modulated Plasticity: Let $R \\in \\{0,1\\}$ denote the reward outcome of the chosen action on a trial, and let $b$ denote a baseline estimate intended to reduce variance. The synaptic update for weight $w_{ji}$ from presynaptic neuron $i$ to action neuron $j$ is defined as\n$\\Delta w_{ji} = \\eta \\, (R - b) \\, e_{ji}$,\nwhere $\\eta  0$ is the learning rate and $e_{ji}$ is the eligibility signal constructed from spike pairs,\n$e_{ji} = \\sum_{t_{\\mathrm{pre}} \\in \\mathcal{S}_i} F(t_{\\mathrm{post}} - t_{\\mathrm{pre}})$,\nwith $\\mathcal{S}_i$ the set of presynaptic spike times from neuron $i$ on that trial and $t_{\\mathrm{post}}$ the postsynaptic spike time of the chosen action neuron. If no postsynaptic spike occurs during the trial, take $e_{ji} = 0$.\n\n- Action Selection by Competing Hazards: For each action neuron $j \\in \\{0,1\\}$, define its hazard rate $\\lambda_j$ as an increasing function of aggregate presynaptic drive,\n$\\lambda_j = \\alpha \\max\\!\\left( \\sum_{i=1}^{N_{\\mathrm{pre}}} w_{ji} x_i, \\, \\varepsilon \\right)$,\nwhere $\\alpha  0$ is a gain, $\\varepsilon  0$ ensures strictly positive rate, and $x_i$ is the number of spikes emitted by presynaptic neuron $i$ on that trial. Draw a candidate spike time $t_j$ from an exponential distribution with rate $\\lambda_j$. The chosen action is the one with the smallest $t_j$ not exceeding the trial duration $T$. If neither candidate time falls within $[0,T]$, select stochastically with probability proportional to $\\lambda_j$, and set $e_{ji} = 0$ if no postsynaptic spike occurs.\n\n- Bandit Environment: Action $j=0$ (optimal) returns $R=1$ with probability $p_{\\mathrm{opt}}$ and $R=0$ otherwise. Action $j=1$ returns $R=1$ with probability $p_{\\mathrm{non}}$ and $R=0$ otherwise, with $p_{\\mathrm{opt}}  p_{\\mathrm{non}}$. Rewards are independent across trials conditioned on the chosen action.\n\n- Baseline Subtraction: The baseline $b$ on each trial is defined as $b = \\mu + \\xi$, where $\\mu$ is a running estimate of expected reward updated by exponential averaging\n$\\mu \\leftarrow (1 - \\gamma) \\mu + \\gamma R$,\nwith $0  \\gamma  1$, and $\\xi$ is zero-mean Gaussian noise independent of $R$ and spike timing, with variance $\\sigma_b^2$. The quantity $\\sigma_b^2$ parameterizes baseline variance.\n\nYou must simulate $M$ independent learning trials for each baseline variance parameter and measure the learning time as the number of trials until the moving fraction of selecting the optimal action, computed over the last $W$ trials, first exceeds a threshold $\\theta$. If the threshold is not reached within a cap of $K$ trials, report $K$.\n\nThe simulation parameters to use are:\n- Number of presynaptic neurons: $N_{\\mathrm{pre}} = 30$.\n- Trial duration: $T = 0.2$.\n- Presynaptic rate per neuron: $r_{\\mathrm{in}} = 25$.\n- STDP parameters: $A_{+} = 1.0$, $A_{-} = 0.5$, $\\tau_{+} = 0.02$, $\\tau_{-} = 0.02$.\n- Hazard gain and floor: $\\alpha = 30.0$, $\\varepsilon = 10^{-12}$.\n- Learning rate and decay: $\\eta = 2 \\times 10^{-4}$, with a small weight decay implemented as $- \\lambda_w w_{ji}$ per trial where $\\lambda_w = 10^{-4}$.\n- Reward probabilities: $p_{\\mathrm{opt}} = 0.8$, $p_{\\mathrm{non}} = 0.2$.\n- Baseline averaging rate: $\\gamma = 0.01$.\n- Sliding window and threshold: $W = 100$, $\\theta = 0.9$.\n- Weight bounds: initialize $w_{ji}$ with small random positive values and clip all weights to $[0, 0.05]$ after each update.\n- Trial cap: $K = 2000$.\n\nTest Suite:\n- Baseline variance values to test: $\\sigma_b^2 \\in \\{0.0, 0.0025, 0.04, 0.25\\}$.\n\nFor each value of $\\sigma_b^2$ in the test suite, simulate the learning process and compute the learning time as an integer number of trials. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[n1,n2,n3,n4]\"), where each $n_i$ is the learning time corresponding to the $i$-th baseline variance test case. No other output is permitted.",
            "solution": "The problem asks for a simulation of a two-action reinforcement learning task performed by a spiking neural network. The synaptic weights of the network are updated via a reward-modulated spike-timing-dependent plasticity (R-STDP) rule. The objective is to determine the learning time for different levels of variance in a modulatory baseline signal. The problem is scientifically grounded, well-posed, and contains all necessary information to construct a simulation.\n\nThe simulation is structured as a series of trials, repeated for each specified baseline variance $\\sigma_b^2$. For each value of $\\sigma_b^2$, the system is initialized and then evolved for a maximum of $K=2000$ trials. To ensure a fair comparison, the random number generator is reset to the same state at the beginning of each simulation run, isolating the effect of $\\sigma_b^2$.\n\nA single trial within the simulation proceeds as follows:\n\n1.  **Presynaptic Spike Generation**: For each of the $N_{\\mathrm{pre}}=30$ input neurons, the number of spikes, $x_i$, to be fired during the trial of duration $T=0.2$ s is drawn from a Poisson distribution with mean $\\lambda = r_{\\mathrm{in}} T = 25 \\times 0.2 = 5$. The timings of these $x_i$ spikes, denoted by the set $\\mathcal{S}_i$, are drawn from a uniform distribution over the interval $[0, T]$.\n\n2.  **Action Selection**: The two output neurons, indexed by $j \\in \\{0, 1\\}$, compete to fire first. The instantaneous probability of firing for each neuron $j$ is governed by a hazard rate, $\\lambda_j$. This rate is a function of the total weighted input drive it receives:\n    $$\n    \\lambda_j = \\alpha \\max\\left( \\sum_{i=1}^{N_{\\mathrm{pre}}} w_{ji} x_i, \\, \\varepsilon \\right)\n    $$\n    where $w_{ji}$ is the synaptic weight from input neuron $i$ to output neuron $j$, $\\alpha=30.0$ is a gain factor, and $\\varepsilon=10^{-12}$ is a small positive constant to prevent rates from being zero. A candidate spike time $t_j$ for each neuron is then drawn from an exponential distribution with rate $\\lambda_j$. The winning action, $j_{\\text{chosen}}$, corresponds to the neuron with the minimum candidate time, $t_{\\text{post}} = \\min(t_0, t_1)$.\n    If $t_{\\text{post}} > T$, no spike occurs within the trial period. In this case, an action is chosen stochastically with probabilities proportional to the hazard rates, $P(\\text{choose } j) = \\lambda_j / (\\lambda_0 + \\lambda_1)$.\n\n3.  **Eligibility Trace Calculation**: An eligibility trace, $e_{ji}$, quantifies the causal link between presynaptic spikes and a postsynaptic spike, serving as a memory for later weight updates. It is non-zero only for the synapses of the winning neuron, and only if it fired within the trial duration ($t_{\\text{post}} \\le T$). For the winning neuron $j_{\\text{chosen}}$, the eligibility trace for each input $i$ is:\n    $$\n    e_{j_{\\text{chosen}}, i} = \\sum_{t_{\\mathrm{pre}} \\in \\mathcal{S}_i} F(t_{\\text{post}} - t_{\\mathrm{pre}})\n    $$\n    where $F(\\Delta t)$ is the STDP window function defined with parameters $A_{+}=1.0$, $A_{-}=0.5$, and $\\tau_{+}=\\tau_{-}=0.02$. For the losing neuron, or if no spike occurred, all eligibility traces are $0$.\n\n4.  **Reward and Learning Signal**: After an action $j_{\\text{chosen}}$ is taken, the environment provides a stochastic reward $R \\in \\{0, 1\\}$. Action $j=0$ is optimal, yielding $R=1$ with probability $p_{\\mathrm{opt}}=0.8$. Action $j=1$ is suboptimal, with $p_{\\mathrm{non}}=0.2$. A learning signal is formed by comparing the reward $R$ to a baseline $b = \\mu + \\xi$. Here, $\\mu$ is an exponential moving average of past rewards, and $\\xi$ is zero-mean Gaussian noise with variance $\\sigma_b^2$. The variance $\\sigma_b^2$ is the parameter we investigate.\n\n5.  **Weight and State Update**: The synaptic weights are updated based on the reward prediction error $(R-b)$ and the eligibility trace $e_{ji}$:\n    $$\n    w_{ji}(t+1) = (1 - \\lambda_w) w_{ji}(t) + \\eta (R - b) e_{ji}(t)\n    $$\n    This update combines the R-STDP rule (with learning rate $\\eta = 2 \\times 10^{-4}$) and a weight decay term ($\\lambda_w = 10^{-4}$). After the update, all weights are clipped to the range $[0, 0.05]$. The running average reward $\\mu$ is also updated: $\\mu \\leftarrow (1-\\gamma)\\mu + \\gamma R$ with $\\gamma=0.01$.\n\n6.  **Performance Evaluation**: The learning process is monitored by calculating the fraction of optimal actions ($j=0$) chosen over a sliding window of the last $W=100$ trials. The simulation for a given $\\sigma_b^2$ terminates when this fraction first exceeds the threshold $\\theta=0.9$. The trial number at which this occurs is the learning time. If the threshold is not reached within $K=2000$ trials, the learning time is recorded as $K$.\n\nThis entire procedure is implemented in a Python script. The simulation is run for each value of $\\sigma_b^2$ in the test suite $\\{0.0, 0.0025, 0.04, 0.25\\}$, and the resulting integer learning times are collected and formatted as the final output.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(sigma_b_sq, params, rng):\n    \"\"\"\n    Runs a single full simulation for a given baseline variance.\n\n    Args:\n        sigma_b_sq (float): The variance of the baseline noise.\n        params (tuple): A tuple containing all simulation parameters.\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        int: The learning time in number of trials.\n    \"\"\"\n    # Unpack parameters\n    (N_pre, T, r_in, A_plus, A_minus, tau_plus, tau_minus,\n     alpha, epsilon, eta, lambda_w, p_opt, p_non,\n     gamma, W, theta, K, w_max) = params\n\n    sigma_b = np.sqrt(sigma_b_sq)\n\n    # Initialization\n    w = rng.uniform(0, 0.01, size=(2, N_pre))\n    mu = (p_opt + p_non) / 2.0\n    choice_history = np.zeros(W, dtype=int)\n\n    for trial in range(1, K + 1):\n        # 1. Generate presynaptic spikes\n        spike_counts = rng.poisson(r_in * T, size=N_pre)\n        presynaptic_spikes = [sorted(rng.uniform(0, T, size=n)) for n in spike_counts]\n\n        # 2. Action selection\n        drive = w @ spike_counts\n        lambdas = alpha * np.maximum(drive, epsilon)\n        candidate_times = rng.exponential(1.0 / lambdas)\n        t_post = np.min(candidate_times)\n        chosen_action = np.argmin(candidate_times)\n\n        eligibility_traces = np.zeros_like(w)\n\n        if t_post = T:\n            # 3. Calculate eligibility traces for the winning neuron\n            e_ji = np.zeros(N_pre)\n            for i in range(N_pre):\n                temp_e = 0.0\n                for t_pre in presynaptic_spikes[i]:\n                    delta_t = t_post - t_pre\n                    if delta_t  0:\n                        temp_e += A_plus * np.exp(-delta_t / tau_plus)\n                    elif delta_t  0:\n                        temp_e += -A_minus * np.exp(delta_t / tau_minus)\n                e_ji[i] = temp_e\n            eligibility_traces[chosen_action, :] = e_ji\n        else:\n            # No spike occurred: stochastic choice, eligibility remains zero\n            probs = lambdas / (np.sum(lambdas) + 1e-20)\n            chosen_action = rng.choice([0, 1], p=probs)\n\n        # 4. Determine Reward and Baseline\n        reward_prob = p_opt if chosen_action == 0 else p_non\n        R = 1 if rng.random()  reward_prob else 0\n        xi = rng.normal(0, sigma_b)\n        b = mu + xi\n\n        # 5. Update Weights\n        w = (1 - lambda_w) * w + eta * (R - b) * eligibility_traces\n        w = np.clip(w, 0, w_max)\n\n        # 6. Update Mean Reward\n        mu = (1 - gamma) * mu + gamma * R\n\n        # 7. Check Learning Criterion\n        choice_history[(trial - 1) % W] = 1 if chosen_action == 0 else 0\n        if trial = W:\n            performance = np.mean(choice_history)\n            if performance  theta:\n                return trial\n\n    return K\n\ndef solve():\n    \"\"\"\n    Main function to set up and run the simulations for the test suite.\n    \"\"\"\n    # Simulation Parameters\n    params = (\n        30,      # N_pre: Number of presynaptic neurons\n        0.2,     # T: Trial duration\n        25.0,    # r_in: Presynaptic rate per neuron\n        1.0,     # A_plus: STDP parameter\n        0.5,     # A_minus: STDP parameter\n        0.02,    # tau_plus: STDP parameter\n        0.02,    # tau_minus: STDP parameter\n        30.0,    # alpha: Hazard gain\n        1e-12,   # epsilon: Hazard floor\n        2e-4,    # eta: Learning rate\n        1e-4,    # lambda_w: Weight decay\n        0.8,     # p_opt: Reward probability for optimal action\n        0.2,     # p_non: Reward probability for non-optimal action\n        0.01,    # gamma: Baseline averaging rate\n        100,     # W: Sliding window size\n        0.9,     # theta: Performance threshold\n        2000,    # K: Trial cap\n        0.05     # w_max: Maximum weight\n    )\n\n    # Test Suite\n    test_cases = [0.0, 0.0025, 0.04, 0.25]\n    results = []\n\n    # Using the same seed for each run ensures that the only difference\n    # between simulations is the parameter being tested (sigma_b^2),\n    # allowing for a fair comparison.\n    seed = 42\n    \n    for sigma_b_sq in test_cases:\n        rng = np.random.default_rng(seed=seed)\n        learning_time = run_simulation(sigma_b_sq, params, rng)\n        results.append(learning_time)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}