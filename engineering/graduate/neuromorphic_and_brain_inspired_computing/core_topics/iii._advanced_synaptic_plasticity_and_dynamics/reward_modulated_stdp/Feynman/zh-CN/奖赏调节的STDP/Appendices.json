{
    "hands_on_practices": [
        {
            "introduction": "要深入理解一个学习规则，第一步往往是分析其在理想化条件下的平均行为。这个练习将指导你推导在两个独立的泊松放电神经元模型下，突触权重的期望变化率。通过这个推导，你将清晰地看到前突触和后突触的放电率、STDP时间窗口的形状以及全局奖励信号是如何共同决定学习方向和速度的，这是理解奖励调制STDP动力学的基础。",
            "id": "4057782",
            "problem": "考虑一个正在经历奖励调制的脉冲时间依赖可塑性（Spike-Timing-Dependent Plasticity, STDP）的单个突触。突触前神经元和突触后神经元分别以速率 $\\lambda_{\\text{pre}}$ 和 $\\lambda_{\\text{post}}$ 发放独立的、均匀的泊松脉冲序列。可塑性是“全对全”（all-to-all）且基于脉冲对的：每个有序的突触前-后脉冲对 $(t_{\\text{pre}}, t_{\\text{post}})$ 都会贡献一个由 STDP 窗口 $F(\\Delta t)$ 指定的资格增量，其中 $\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$，并且 $F$ 是绝对可积的。该可塑性由一个持续的、平稳的标量奖励信号进行调制，在突触变化的时间尺度上，该信号可被视为一个常数 $R_{0}$。学习率是 $\\eta$。假设权重动态没有额外的约束（没有硬边界，也没有显式的权重依赖性）。\n\n从均匀泊松过程和基于脉冲对的 STDP 的定义出发，并且仅使用独立泊松过程的基本性质和期望的线性性质，推导出在长时间极限下单位时间内的预期突触权重变化，并用 $\\lambda_{\\text{pre}}$、$\\lambda_{\\text{post}}$、$\\eta$、$R_{0}$ 和 $F$ 表示。然后，说明在没有稳定机制的情况下，突触权重发散到 $+\\infty$（以及发散到 $-\\infty$）的条件。你的最终数值或解析答案必须是单位时间内的预期突觸权重变化；不要在最终答案框中包含任何单位。",
            "solution": "该问题要求推导在特定的奖励调制脉冲时间依赖可塑性（STDP）规则下，单个突触的预期权重变化率 $\\frac{d\\langle w \\rangle}{dt}$。核心假设是突触前和突触后神经元以独立的、均匀的泊松过程发放脉冲，并且可塑性规则是基于脉冲对的。\n\n设突触前和突触后脉冲序列分别由脉冲时间集合 $\\{t_i^{\\text{pre}}\\}$ 和 $\\{t_j^{\\text{post}}\\}$ 表示。相应的发放速率是恒定的，由 $\\lambda_{\\text{pre}}$ 和 $\\lambda_{\\text{post}}$ 给出。可塑性规则规定，对于每个有序的突触前脉冲（时刻 $t_{\\text{pre}}$）和突触后脉冲（时刻 $t_{\\text{post}}$）对，突触权重 $w$ 会更新一个量 $\\Delta w$。该更新由以下公式给出：\n$$ \\Delta w = \\eta R_{0} F(\\Delta t) $$\n其中 $\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$ 是脉冲之间的时间差，$\\eta$ 是一个恒定的学习率，$R_{0}$ 是一个恒定的标量奖励信号，而 $F(\\Delta t)$ 是 STDP 窗口函数，它根据脉冲时序确定更新量。\n\n我们的目标是计算在长时间极限下权重的预期变化率 $\\frac{d\\langle w \\rangle}{dt}$。鉴于底层的脉冲序列统计是平稳的（均匀泊松过程），并且参数 $\\eta$ 和 $R_{0}$ 是恒定的，预期的变化率也将是一个常数。我们可以通过对所有可能的时间差积分所有可能的脉冲配对的预期贡献来计算这个速率。\n\n两个速率分别为 $\\lambda_{1}$ 和 $\\lambda_{2}$ 的独立均匀泊松过程的一个基本性质是，来自每个过程的一个事件构成的时间间隔在无穷小区间 $[\\tau, \\tau + d\\tau]$ 内的事件对的发生率为 $\\lambda_{1} \\lambda_{2} d\\tau$。这个结果直接源于过程的独立性。第一个过程在小时间窗 $dt$ 内发生一个事件的概率是 $\\lambda_{1} dt$。第二个过程在延迟 $\\tau$ 后的时间窗 $d\\tau$ 内发生一个事件的概率是 $\\lambda_{2} d\\tau$。这个特定事件对的联合概率是 $(\\lambda_{1} dt)(\\lambda_{2} d\\tau)$。通过除以 $dt$ 和区间 $d\\tau$，可以得到这类事件对的速率密度为 $\\lambda_{1}\\lambda_{2}$。\n\n将此原理应用于给定的突触，时间差 $\\Delta t = t_{\\text{post}} - t_{\\text{pre}}$ 落在区间 $[\\tau, \\tau + d\\tau]$ 内的（突触前，突触后）脉冲对的发生率为 $\\lambda_{\\text{pre}} \\lambda_{\\text{post}} d\\tau$。\n\n每个这样的脉冲对对总权重变化的贡献量为 $\\eta R_{0} F(\\tau)$。由时间延迟在 $[\\tau, \\tau + d\\tau]$ 内的脉冲对引起的权重变化率的预期微分贡献是这些脉冲对的发生率与每次配对引起的权重变化的乘积：\n$$ d\\left(\\frac{d\\langle w \\rangle}{dt}\\right) = (\\eta R_{0} F(\\tau)) \\times (\\lambda_{\\text{pre}} \\lambda_{\\text{post}} d\\tau) $$\n为了得到总的预期变化率 $\\frac{d\\langle w \\rangle}{dt}$，我们必须对所有可能的时间延迟 $\\tau$ 积分这一贡献。可塑性规则的“全对全”性质意味着积分范围是从 $-\\infty$ 到 $\\infty$：\n$$ \\frac{d\\langle w \\rangle}{dt} = \\int_{-\\infty}^{\\infty} \\eta R_{0} \\lambda_{\\text{pre}} \\lambda_{\\text{post}} F(\\tau) d\\tau $$\n由于 $\\eta$、$R_{0}$、$\\lambda_{\\text{pre}}$ 和 $\\lambda_{\\text{post}}$ 都是常数，它们可以从积分中提取出来，从而得到预期变化率的最终表达式：\n$$ \\frac{d\\langle w \\rangle}{dt} = \\eta R_{0} \\lambda_{\\text{pre}} \\lambda_{\\text{post}} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau $$\n问题指出 $F(\\tau)$ 是绝对可积的，这保证了积分 $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau$ 收敛到一个有限值，从而使该变化率是良定义的。\n\n问题的第二部分要求给出突触权重发散的条件。在没有任何稳定机制（如权重依赖性或硬边界）的情况下，预期权重随时间线性演化：$\\langle w(t) \\rangle = \\langle w(0) \\rangle + t \\cdot \\frac{d\\langle w \\rangle}{dt}$。如果变化率 $\\frac{d\\langle w \\rangle}{dt}$ 为正，权重将发散到 $+\\infty$；如果变化率为负，则发散到 $-\\infty$。\n\n设 $C = \\eta R_{0} \\lambda_{\\text{pre}} \\lambda_{\\text{post}} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau$。\n学习率 $\\eta$ 和发放速率 $\\lambda_{\\text{pre}}$、$\\lambda_{\\text{post}}$ 是非负的。我们假设它们是严格为正的，以便发生任何变化。因此，$C$ 的符号由奖励 $R_{0}$ 和 STDP 窗口函数积分的乘积的符号决定。\n\n发散到 $+\\infty$ 的条件是 $C > 0$，这意味着：\n$$ R_{0} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau > 0 $$\n当（$R_{0} > 0$ 且 $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau > 0$）或（$R_{0} < 0$ 且 $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau < 0$）时，此条件成立。\n\n发散到 $-\\infty$ 的条件是 $C < 0$，这意味着：\n$$ R_{0} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau < 0 $$\n当（$R_{0} > 0$ 且 $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau < 0$）或（$R_{0} < 0$ 且 $\\int_{-\\infty}^{\\infty} F(\\tau) d\\tau > 0$）时，此条件成立。\n如果该乘积为零，则预期权重是稳定的。\n问题要求将单位时间内的预期突触权重变化作为最终答案。",
            "answer": "$$\n\\boxed{\\eta R_{0} \\lambda_{\\text{pre}} \\lambda_{\\text{post}} \\int_{-\\infty}^{\\infty} F(\\tau) d\\tau}\n$$"
        },
        {
            "introduction": "奖励调制的STDP不仅仅是一个受生物学启发的启发式规则，它与强化学习的规范理论之间存在着深刻的联系。这项实践将通过一个简单而典型的例子，揭示R-STDP如何能够实现时序差分（TD）学习，这是现代人工智能的基石之一。完成这个练习将帮助你巩固对R-STDP作为解决时间信用分配问题的计算算法的理解。",
            "id": "4057751",
            "problem": "考虑一个最小的情景式任务，该任务被建模为一个具有状态 $A$ 和 $B$ 以及一个终止吸收状态 $\\perp$ 的两状态马尔可夫链。该情景在时间 $t=0$ 时从状态 $A$ 开始，在 $t=1$ 时确定性地转移到状态 $B$，并在 $t=2$ 时在状态 $\\perp$ 终止。一个标量奖励 $R$ 仅在终止时给出；也就是说，$r_{0}=0$，$r_{1}=R$，而 $r_{2}$ 未定义，因为该情景在 $t=2$ 进入 $\\perp$ 时结束。该任务用于训练一个单参数线性价值函数 $V(s;w)=w\\,x(s)$，其中 $w\\in\\mathbb{R}$ 是待调整的突触权重，$x(s)\\in\\mathbb{R}$ 是状态 $s$ 的一个固定特征。假设 $x(A)=x_{0}$ 和 $x(B)=x_{1}$ 是给定的常数。折扣因子为 $\\gamma\\in(0,1)$，资格迹参数为 $\\lambda\\in[0,1]$。学习率为 $\\eta>0$。\n\n您将使用零阶时序差分（TD）学习（也称为时序差分TD(0)）与资格迹来计算单次情景的突触权重更新，然后验证其与奖励调控的三因子脉冲时间依赖可塑性（STDP）规则的等价性。使用以下基本定义：\n\n- 时间 $t$ 的时序差分误差为 $\\delta_{t}=r_{t}+\\gamma\\,V(s_{t+1};w)-V(s_{t};w)$，约定 $V(\\perp;w)=0$。\n- 资格迹遵循 $e_{t}=\\gamma\\,\\lambda\\,e_{t-1}+x(s_{t})$，且 $e_{-1}=0$。\n\n在奖励调控的STDP的神经形态学表述中，突触更新是一个三因子乘积，其中全局调制信号 $M_{t}$ 门控由突触前和突触后活动累积的局部资格 $e_{t}$；在本任务中，将 $M_{t}$ 等同于时序差分误差，即 $M_{t}\\equiv\\delta_{t}$，这与奖励预测误差信号一致。\n\n从这些定义和上述马尔可夫链结构出发，推导由带资格迹的TD(0)产生的单次情景突触权重增量 $\\Delta w$，并明确证明，当 $M_{t}=\\delta_{t}$ 时，从三因子奖励调控的STDP规则中可以得到相同的 $\\Delta w$。将您的最终答案表示为关于符号 $\\eta$、$\\gamma$、$\\lambda$、$R$、$w$、$x_{0}$ 和 $x_{1}$ 的封闭形式解析表达式。不需要进行数值计算，最终表达式也无需附加单位。最终答案必须是单个封闭形式的表达式；不要将其呈现为一个待解方程。",
            "solution": "单个情景中突触权重 $w$ 的总变化量由每个时间步的更新总和给出。带资格迹的时序差分（TD($\\lambda$)）学习的更新规则是：\n$$\n\\Delta w = \\eta \\sum_{t=0}^{T-1} \\delta_t e_t\n$$\n在这个问题中，情景包含从 $t=0$ 到 $t=1$ 的转移，并在 $t=2$ 终止。因此，求和范围是 $t=0, 1$。\n$$\n\\Delta w = \\eta (\\delta_0 e_0 + \\delta_1 e_1)\n$$\n要计算 $\\Delta w$，我们必须首先确定资格迹 $e_0$ 和 $e_1$ 以及 TD 误差 $\\delta_0$ 和 $\\delta_1$ 的值。\n\n**1. 资格迹 ($e_t$) 的计算**\n资格迹根据递推关系 $e_{t}=\\gamma\\,\\lambda\\,e_{t-1}+x(s_{t})$ 演变，初始条件为 $e_{-1}=0$。\n\n对于 $t=0$：\n状态为 $s_0=A$，所以特征是 $x(s_0)=x_0$。\n$$\ne_0 = \\gamma\\,\\lambda\\,e_{-1} + x(s_0) = \\gamma\\,\\lambda\\,(0) + x_0 = x_0\n$$\n\n对于 $t=1$：\n状态为 $s_1=B$，所以特征是 $x(s_1)=x_1$。\n$$\ne_1 = \\gamma\\,\\lambda\\,e_{0} + x(s_1) = \\gamma\\,\\lambda\\,x_0 + x_1\n$$\n\n**2. TD 误差 ($\\delta_t$) 的计算**\nTD 误差定义为 $\\delta_{t}=r_{t}+\\gamma\\,V(s_{t+1};w)-V(s_{t};w)$。价值函数为 $V(s;w)=w\\,x(s)$。\n\n对于 $t=0$：\n状态为 $s_0=A$ 和 $s_1=B$。奖励为 $r_0=0$。\n价值函数为 $V(s_0;w) = V(A;w) = w\\,x_0$ 和 $V(s_1;w) = V(B;w) = w\\,x_1$。\n$$\n\\delta_0 = r_0 + \\gamma\\,V(s_1;w) - V(s_0;w) = 0 + \\gamma\\,(w\\,x_1) - w\\,x_0 = w(\\gamma\\,x_1 - x_0)\n$$\n\n对于 $t=1$：\n状态为 $s_1=B$ 和终止状态 $s_2=\\perp$。奖励为 $r_1=R$。\n价值函数为 $V(s_1;w) = V(B;w) = w\\,x_1$，并且根据约定，$V(s_2;w) = V(\\perp;w) = 0$。\n$$\n\\delta_1 = r_1 + \\gamma\\,V(s_2;w) - V(s_1;w) = R + \\gamma\\,(0) - w\\,x_1 = R - w\\,x_1\n$$\n\n**3. 总权重更新 ($\\Delta w$) 的计算**\n现在我们将 $e_0, e_1, \\delta_0, \\delta_1$ 的表达式代入 $\\Delta w$ 的方程中：\n$$\n\\Delta w = \\eta (\\delta_0 e_0 + \\delta_1 e_1)\n$$\n$$\n\\Delta w = \\eta \\left[ \\left( w(\\gamma\\,x_1 - x_0) \\right) (x_0) + \\left( R - w\\,x_1 \\right) (\\gamma\\,\\lambda\\,x_0 + x_1) \\right]\n$$\n我们展开括号内的项：\n$$\n\\Delta w = \\eta \\left[ (w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2) + (R(\\gamma\\,\\lambda\\,x_0 + x_1) - w\\,x_1(\\gamma\\,\\lambda\\,x_0 + x_1)) \\right]\n$$\n$$\n\\Delta w = \\eta \\left[ w\\,\\gamma\\,x_0\\,x_1 - w\\,x_0^2 + R\\,\\gamma\\,\\lambda\\,x_0 + R\\,x_1 - w\\,\\gamma\\,\\lambda\\,x_0\\,x_1 - w\\,x_1^2 \\right]\n$$\n为了得到最终的封闭形式表达式，我们对包含奖励 $R$ 的项和包含当前权重 $w$ 的项进行分组：\n$$\n\\Delta w = \\eta \\left[ (R\\,\\gamma\\,\\lambda\\,x_0 + R\\,x_1) - (w\\,x_0^2 + w\\,x_1^2 - w\\,\\gamma\\,x_0\\,x_1 + w\\,\\gamma\\,\\lambda\\,x_0\\,x_1) \\right]\n$$\n提出因子 $R$ 和 $w$：\n$$\n\\Delta w = \\eta \\left[ R(\\gamma\\,\\lambda\\,x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma\\,x_0\\,x_1 + \\gamma\\,\\lambda\\,x_0\\,x_1) \\right]\n$$\n为了清晰起见，可以重新整理乘以 $w$ 的项：\n$$\n\\Delta w = \\eta \\left[ R(\\gamma\\lambda x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0 x_1) \\right]\n$$\n\n**4. 与三因子规则的等价性**\n问题将奖励调控的三因子STDP规则定义为全局调制信号 $M_t$ 门控局部资格 $e_t$ 的乘积。总更新是这些乘积随时间的总和，并按学习率 $\\eta$ 进行缩放：\n$$\n\\Delta w = \\eta \\sum_t M_t e_t\n$$\n问题明确给出了等同关系 $M_t \\equiv \\delta_t$。将其代入三因子规则，得到：\n$$\n\\Delta w = \\eta \\sum_t \\delta_t e_t\n$$\n这正是作为推导起点的 TD($\\lambda$) 更新规则的定义。因此，根据问题陈述中给出的定义，等价性得以确立。通过这种等同关系，上述为 TD($\\lambda$) 更新所做的计算，同时也是为指定的三因子STDP规则所做的计算。",
            "answer": "$$\n\\boxed{\\eta \\left[ R(\\gamma\\lambda x_0 + x_1) - w(x_0^2 + x_1^2 - \\gamma(1-\\lambda)x_0 x_1) \\right]}\n$$"
        },
        {
            "introduction": "理论分析固然强大，但模拟能让我们直观地观察学习动态并验证关于性能的假设。这项实践要求你构建一个学习“赌博机”任务的脉冲神经网络。你的目标是编程实现奖励调制的STDP，并量化一个关键的优化技术——基线减法（baseline subtraction）——对学习速度和稳定性的影响，从而将理论与实际应用联系起来。",
            "id": "4057793",
            "problem": "考虑一个由奖励调制的脉冲时间依赖可塑性（STDP）控制的双动作老虎机式脉冲网络。有 $N_{\\mathrm{pre}}$ 个独立的泊松输入神经元，为两个相互竞争的输出（动作）神经元提供输入。在每次试验中，输入神经元发放脉冲，而动作神经元最多产生一个脉冲；具有最早脉冲的动作被执行。获得的标量奖励通过一个减去基线的因子来调制突触可塑性。\n\n使用以下基本依据和定义来构建和分析其动力学：\n\n- 脉冲时间依赖可塑性 (STDP)：对于在时间 $t_{\\mathrm{pre}}$ 的突触前脉冲和在时间 $t_{\\mathrm{post}}$ 的突触后脉冲，定义 STDP 时间窗函数 $F(\\Delta t)$，其中 $\\Delta t = t_{\\mathrm{post}} - t_{\\mathrm{pre}}$，如下：\n$$\nF(\\Delta t) = \n\\begin{cases}\nA_{+} \\exp\\left(-\\frac{\\Delta t}{\\tau_{+}}\\right), & \\Delta t > 0 \\\\\n- A_{-} \\exp\\left(\\frac{\\Delta t}{\\tau_{-}}\\right), & \\Delta t < 0\n\\end{cases}\n$$\n其中 $A_{+} > 0$，$A_{-} > 0$，$\\tau_{+} > 0$ 且 $\\tau_{-} > 0$。\n\n- 奖励调制的可塑性：令 $R \\in \\{0,1\\}$ 表示一次试验中所选动作的奖励结果，令 $b$ 表示一个旨在减少方差的基线估计。从突触前神经元 $i$ 到动作神经元 $j$ 的权重 $w_{ji}$ 的突触更新定义为：\n$$\n\\Delta w_{ji} = \\eta \\, (R - b) \\, e_{ji},\n$$\n其中 $\\eta > 0$ 是学习率，$e_{ji}$ 是由脉冲对构建的资格信号，\n$$\ne_{ji} = \\sum_{t_{\\mathrm{pre}} \\in \\mathcal{S}_i} F(t_{\\mathrm{post}} - t_{\\mathrm{pre}}),\n$$\n其中 $\\mathcal{S}_i$ 是该次试验中来自神经元 $i$ 的突触前脉冲时间的集合，$t_{\\mathrm{post}}$ 是所选动作神经元的突触后脉冲时间。如果在试验期间没有发生突触后脉冲，则取 $e_{ji} = 0$。\n\n- 通过竞争风险进行动作选择：对于每个动作神经元 $j \\in \\{0,1\\}$，将其风险率 $\\lambda_j$ 定义为总突触前驱动的增函数，\n$$\n\\lambda_j = \\alpha \\max\\!\\left( \\sum_{i=1}^{N_{\\mathrm{pre}}} w_{ji} x_i, \\, \\varepsilon \\right),\n$$\n其中 $\\alpha > 0$ 是一个增益，$\\varepsilon > 0$ 确保率是严格正值，$x_i$ 是该次试验中突触前神经元 $i$ 发放的脉冲数量。从速率为 $\\lambda_j$ 的指数分布中抽取一个候选脉冲时间 $t_j$。所选动作是具有不超过试验时长 $T$ 的最小 $t_j$ 的那个。如果两个候选时间都不在 $[0,T]$ 范围内，则以与 $\\lambda_j$ 成正比的概率进行随机选择，并且如果没有发生突触后脉冲，则设 $e_{ji} = 0$。\n\n- 老虎机环境：动作 $j=0$（最优）以概率 $p_{\\mathrm{opt}}$ 返回 $R=1$，否则返回 $R=0$。动作 $j=1$ 以概率 $p_{\\mathrm{non}}$ 返回 $R=1$，否则返回 $R=0$，其中 $p_{\\mathrm{opt}} > p_{\\mathrm{non}}$。在给定所选动作的条件下，各次试验的奖励是独立的。\n\n- 基线减法：每次试验的基线 $b$ 定义为 $b = \\mu + \\xi$，其中 $\\mu$ 是通过指数平均更新的期望奖励的运行估计\n$$\n\\mu \\leftarrow (1 - \\gamma) \\mu + \\gamma R,\n$$\n其中 $0 < \\gamma < 1$，$\\xi$ 是独立于 $R$ 和脉冲时间的零均值高斯噪声，其方差为 $\\sigma_b^2$。量 $\\sigma_b^2$ 参数化了基线方差。\n\n你必须为每个基线方差参数模拟 $M$ 次独立的学习试验，并测量学习时间。学习时间定义为：在最近 $W$ 次试验中计算的选择最优动作的移动比例首次超过阈值 $\\theta$ 所需的试验次数。如果在 $K$ 次试验的上限内未能达到该阈值，则报告 $K$。\n\n要使用的模拟参数是：\n- 突触前神经元数量：$N_{\\mathrm{pre}} = 30$。\n- 试验时长：$T = 0.2$。\n- 每个神经元的突触前发放率：$r_{\\mathrm{in}} = 25$。\n- STDP 参数：$A_{+} = 1.0$, $A_{-} = 0.5$, $\\tau_{+} = 0.02$, $\\tau_{-} = 0.02$。\n- 风险增益和下限：$\\alpha = 30.0$, $\\varepsilon = 10^{-12}$。\n- 学习率和衰减：$\\eta = 2 \\times 10^{-4}$，并实现一个小的权重衰减，即每次试验 $- \\lambda_w w_{ji}$，其中 $\\lambda_w = 10^{-4}$。\n- 奖励概率：$p_{\\mathrm{opt}} = 0.8$, $p_{\\mathrm{non}} = 0.2$。\n- 基线平均率：$\\gamma = 0.01$。\n- 滑动窗口和阈值：$W = 100$, $\\theta = 0.9$。\n- 权重边界：用小的随机正值初始化 $w_{ji}$，并在每次更新后将所有权重裁剪到 $[0, 0.05]$ 范围内。\n- 试验上限：$K = 2000$。\n\n测试套件：\n- 待测试的基线方差值：$\\sigma_b^2 \\in \\{0.0, 0.0025, 0.04, 0.25\\}$。\n\n对于测试套件中的每个 $\\sigma_b^2$ 值，模拟学习过程并以整数试验次数计算学习时间。你的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表的结果（例如，\"[n1,n2,n3,n4]\"），其中每个 $n_i$ 是对应于第 $i$ 个基线方差测试用例的学习时间。不允许有其他输出。",
            "solution": "该问题要求模拟一个由脉冲神经网络执行的双动作强化学习任务。网络的突触权重通过奖励调制的脉冲时间依赖可塑性（R-STDP）规则进行更新。目标是确定在调制性基线信号具有不同方差水平下的学习时间。该问题具有科学依据，是适定的，并包含了构建模拟所需的所有必要信息。\n\n模拟的结构是一系列试验，对每个指定的基线方差 $\\sigma_b^2$ 重复进行。对于每个 $\\sigma_b^2$ 值，系统被初始化，然后演化最多 $K=2000$ 次试验。为确保公平比较，在每次模拟运行开始时，随机数生成器被重置到相同的状态，从而隔离 $\\sigma_b^2$ 的影响。\n\n模拟中的单次试验按以下步骤进行：\n\n1.  **突触前脉冲生成**：对于 $N_{\\mathrm{pre}}=30$ 个输入神经元中的每一个，在时长为 $T=0.2$ 秒的试验期间将要发放的脉冲数量 $x_i$ 从均值为 $\\lambda = r_{\\mathrm{in}} T = 25 \\times 0.2 = 5$ 的泊松分布中抽取。这些 $x_i$ 个脉冲的时间，用集合 $\\mathcal{S}_i$ 表示，从区间 $[0, T]$ 上的均匀分布中抽取。\n\n2.  **动作选择**：两个由 $j \\in \\{0, 1\\}$ 索引的输出神经元竞争首先发放脉冲。每个神经元 $j$ 的瞬时发放概率由一个风险率 $\\lambda_j$ 控制。该率是其接收到的加权总输入驱动的函数：\n    $$\n    \\lambda_j = \\alpha \\max\\left( \\sum_{i=1}^{N_{\\mathrm{pre}}} w_{ji} x_i, \\, \\varepsilon \\right)\n    $$\n    其中 $w_{ji}$ 是从输入神经元 $i$ 到输出神经元 $j$ 的突触权重，$\\alpha=30.0$ 是一个增益因子，$\\varepsilon=10^{-12}$ 是一个防止率为零的小正数常量。然后为每个神经元从速率为 $\\lambda_j$ 的指数分布中抽取一个候选脉冲时间 $t_j$。获胜动作 $j_{\\text{chosen}}$ 对应于具有最小候选时间的神经元，$t_{\\text{post}} = \\min(t_0, t_1)$。\n    如果 $t_{\\text{post}} > T$，则在试验周期内没有脉冲发生。在这种情况下，以与风险率成正比的概率 $P(\\text{选择 } j) = \\lambda_j / (\\lambda_0 + \\lambda_1)$ 随机选择一个动作。\n\n3.  **资格迹计算**：资格迹 $e_{ji}$ 量化了突触前脉冲和突触后脉冲之间的因果联系，作为后续权重更新的记忆。它仅对获胜神经元的突触非零，且仅当该神经元在试验时长内（$t_{\\text{post}} \\le T$）发放了脉冲时才非零。对于获胜神经元 $j_{\\text{chosen}}$，每个输入 $i$ 的资格迹为：\n    $$\n    e_{j_{\\text{chosen}}, i} = \\sum_{t_{\\mathrm{pre}} \\in \\mathcal{S}_i} F(t_{\\text{post}} - t_{\\mathrm{pre}})\n    $$\n    其中 $F(\\Delta t)$ 是用参数 $A_{+}=1.0$，$A_{-}=0.5$ 和 $\\tau_{+}=\\tau_{-}=0.02$ 定义的 STDP 时间窗函数。对于失败的神经元，或者如果没有脉冲发生，所有资格迹均为 $0$。\n\n4.  **奖励与学习信号**：在执行动作 $j_{\\text{chosen}}$ 后，环境提供一个随机奖励 $R \\in \\{0, 1\\}$。动作 $j=0$ 是最优的，以概率 $p_{\\mathrm{opt}}=0.8$ 产生 $R=1$。动作 $j=1$ 是次优的，概率为 $p_{\\mathrm{non}}=0.2$。通过将奖励 $R$ 与基线 $b = \\mu + \\xi$ 进行比较，形成一个学习信号。这里，$\\mu$ 是过去奖励的指数移动平均，$\\xi$ 是方差为 $\\sigma_b^2$ 的零均值高斯噪声。方差 $\\sigma_b^2$ 是我们研究的参数。\n\n5.  **权重与状态更新**：突触权重根据奖励预测误差 $(R-b)$ 和资格迹 $e_{ji}$ 进行更新：\n    $$\n    w_{ji}(t+1) = (1 - \\lambda_w) w_{ji}(t) + \\eta (R - b) e_{ji}(t)\n    $$\n    此更新结合了 R-STDP 规则（学习率为 $\\eta = 2 \\times 10^{-4}$）和一个权重衰减项（$\\lambda_w = 10^{-4}$）。更新后，所有权重被裁剪到 $[0, 0.05]$ 的范围内。运行平均奖励 $\\mu$ 也被更新：$\\mu \\leftarrow (1-\\gamma)\\mu + \\gamma R$，其中 $\\gamma=0.01$。\n\n6.  **性能评估**：通过计算在最近 $W=100$ 次试验的滑动窗口上选择的最优动作（$j=0$）的比例来监控学习过程。对于给定的 $\\sigma_b^2$，当这个比例首次超过阈值 $\\theta=0.9$ 时，模拟终止。发生这种情况时的试验次数即为学习时间。如果在 $K=2000$ 次试验内未达到该阈值，则学习时间记录为 $K$。\n\n整个过程在一个 Python 脚本中实现。对测试套件 $\\{0.0, 0.0025, 0.04, 0.25\\}$ 中的每个 $\\sigma_b^2$ 值运行模拟，并将得到的整数学习时间收集并格式化为最终输出。",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(sigma_b_sq, params, rng):\n    \"\"\"\n    Runs a single full simulation for a given baseline variance.\n\n    Args:\n        sigma_b_sq (float): The variance of the baseline noise.\n        params (tuple): A tuple containing all simulation parameters.\n        rng (np.random.Generator): The random number generator instance.\n\n    Returns:\n        int: The learning time in number of trials.\n    \"\"\"\n    # Unpack parameters\n    (N_pre, T, r_in, A_plus, A_minus, tau_plus, tau_minus,\n     alpha, epsilon, eta, lambda_w, p_opt, p_non,\n     gamma, W, theta, K, w_max) = params\n\n    sigma_b = np.sqrt(sigma_b_sq)\n\n    # Initialization\n    w = rng.uniform(0, 0.01, size=(2, N_pre))\n    mu = (p_opt + p_non) / 2.0\n    choice_history = np.zeros(W, dtype=int)\n\n    for trial in range(1, K + 1):\n        # 1. Generate presynaptic spikes\n        spike_counts = rng.poisson(r_in * T, size=N_pre)\n        presynaptic_spikes = [sorted(rng.uniform(0, T, size=n)) for n in spike_counts]\n\n        # 2. Action selection\n        drive = w @ spike_counts\n        lambdas = alpha * np.maximum(drive, epsilon)\n        candidate_times = rng.exponential(1.0 / lambdas)\n        t_post = np.min(candidate_times)\n        chosen_action = np.argmin(candidate_times)\n\n        eligibility_traces = np.zeros_like(w)\n\n        if t_post = T:\n            # 3. Calculate eligibility traces for the winning neuron\n            e_ji = np.zeros(N_pre)\n            for i in range(N_pre):\n                temp_e = 0.0\n                for t_pre in presynaptic_spikes[i]:\n                    delta_t = t_post - t_pre\n                    if delta_t > 0:\n                        temp_e += A_plus * np.exp(-delta_t / tau_plus)\n                    elif delta_t  0:\n                        temp_e += -A_minus * np.exp(delta_t / tau_minus)\n                e_ji[i] = temp_e\n            eligibility_traces[chosen_action, :] = e_ji\n        else:\n            # No spike occurred: stochastic choice, eligibility remains zero\n            probs = lambdas / (np.sum(lambdas) + 1e-20)\n            chosen_action = rng.choice([0, 1], p=probs)\n\n        # 4. Determine Reward and Baseline\n        reward_prob = p_opt if chosen_action == 0 else p_non\n        R = 1 if rng.random()  reward_prob else 0\n        xi = rng.normal(0, sigma_b)\n        b = mu + xi\n\n        # 5. Update Weights\n        w = (1 - lambda_w) * w + eta * (R - b) * eligibility_traces\n        w = np.clip(w, 0, w_max)\n\n        # 6. Update Mean Reward\n        mu = (1 - gamma) * mu + gamma * R\n\n        # 7. Check Learning Criterion\n        choice_history[(trial - 1) % W] = 1 if chosen_action == 0 else 0\n        if trial >= W:\n            performance = np.mean(choice_history)\n            if performance > theta:\n                return trial\n\n    return K\n\ndef solve():\n    \"\"\"\n    Main function to set up and run the simulations for the test suite.\n    \"\"\"\n    # Simulation Parameters\n    params = (\n        30,      # N_pre: Number of presynaptic neurons\n        0.2,     # T: Trial duration\n        25.0,    # r_in: Presynaptic rate per neuron\n        1.0,     # A_plus: STDP parameter\n        0.5,     # A_minus: STDP parameter\n        0.02,    # tau_plus: STDP parameter\n        0.02,    # tau_minus: STDP parameter\n        30.0,    # alpha: Hazard gain\n        1e-12,   # epsilon: Hazard floor\n        2e-4,    # eta: Learning rate\n        1e-4,    # lambda_w: Weight decay\n        0.8,     # p_opt: Reward probability for optimal action\n        0.2,     # p_non: Reward probability for non-optimal action\n        0.01,    # gamma: Baseline averaging rate\n        100,     # W: Sliding window size\n        0.9,     # theta: Performance threshold\n        2000,    # K: Trial cap\n        0.05     # w_max: Maximum weight\n    )\n\n    # Test Suite\n    test_cases = [0.0, 0.0025, 0.04, 0.25]\n    results = []\n\n    # Using the same seed for each run ensures that the only difference\n    # between simulations is the parameter being tested (sigma_b^2),\n    # allowing for a fair comparison.\n    seed = 42\n    \n    for sigma_b_sq in test_cases:\n        rng = np.random.default_rng(seed=seed)\n        learning_time = run_simulation(sigma_b_sq, params, rng)\n        results.append(learning_time)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}