{
    "hands_on_practices": [
        {
            "introduction": "Graph partitioning is a cornerstone of mapping algorithms, essential for minimizing costly inter-core communication by intelligently assigning neurons to different hardware cores. The Fiduccia-Mattheyses (FM) algorithm provides an efficient and widely used heuristic for this task, iteratively improving a partition by moving nodes based on a \"gain\" metric. This exercise provides concrete, hands-on experience with the core mechanic of the FM algorithm, demonstrating how to calculate the gain of a potential move and adapt the method for practical hardware constraints like per-core neuron capacity .",
            "id": "4050853",
            "problem": "Consider mapping a Spiking Neural Network (SNN) onto two neuromorphic cores, with the objective of minimizing inter-core communication by partitioning the network while respecting per-core capacities. Let the effective undirected inter-neuron coupling weights be represented by the symmetric adjacency matrix $W \\in \\mathbb{R}^{6 \\times 6}$ for neurons labeled $\\{1,2,3,4,5,6\\}$:\n$$\nW \\;=\\;\n\\begin{pmatrix}\n0 & 2 & 1 & 3 & 0 & 0 \\\\\n2 & 0 & 2 & 1 & 1 & 0 \\\\\n1 & 2 & 0 & 4 & 2 & 1 \\\\\n3 & 1 & 4 & 0 & 2 & 1 \\\\\n0 & 1 & 2 & 2 & 0 & 3 \\\\\n0 & 0 & 1 & 1 & 3 & 0\n\\end{pmatrix}.\n$$\nAssume an initial two-way partition onto cores $A$ and $B$ defined by $P_A = \\{1,2,3\\}$ and $P_B = \\{4,5,6\\}$. The per-core neuron capacities are $N_{\\mathrm{cap}}^A = 3$ and $N_{\\mathrm{cap}}^B = 3$. The mapping objective penalizes capacity overflow using a scalar penalty coefficient $\\lambda = 2$, measured in the same units as the cut weight. Define the total cut weight as the sum of weights of all edges crossing the partition, and define the Fiduccia–Mattheyses (FM) gain for moving a single neuron $v$ from its current core to the other core as the reduction in cut weight induced by the move. To incorporate capacity constraints, define the effective gain for moving a neuron $v$ as the FM gain minus a capacity overflow penalty, where the penalty is $\\lambda$ times the sum of positive overflows after the move, namely\n$$\n\\lambda \\left( \\max\\{0,\\,N_A^{\\mathrm{after}} - N_{\\mathrm{cap}}^A\\} + \\max\\{0,\\,N_B^{\\mathrm{after}} - N_{\\mathrm{cap}}^B\\} \\right),\n$$\nwith $N_A^{\\mathrm{after}}$ and $N_B^{\\mathrm{after}}$ denoting the neuron counts on cores $A$ and $B$ after the move.\n\nCompute the effective gain, as a single real number, for moving neuron $3$ from core $A$ to core $B$ in one iteration of FM refinement. Express your final answer as a real number without units. No rounding is required.",
            "solution": "The problem is well-posed and contains all necessary information to compute a unique solution. We are asked to compute the effective gain for moving neuron $3$ from core A to core B. The effective gain, $G_{\\mathrm{eff}}(v)$, for moving a neuron $v$ is defined as the Fiduccia–Mattheyses (FM) gain, $G_{\\mathrm{FM}}(v)$, minus a capacity overflow penalty, $P_{\\mathrm{cap}}$.\n$$\nG_{\\mathrm{eff}}(v) = G_{\\mathrm{FM}}(v) - P_{\\mathrm{cap}}\n$$\n\nFirst, we calculate the FM gain, $G_{\\mathrm{FM}}(3)$, for moving neuron $3$. The FM gain is defined as the reduction in the total cut weight. This can be calculated as the sum of the weights of the connections from the neuron to the destination partition (external connections) minus the sum of the weights of the connections from the neuron to other neurons in its source partition (internal connections).\n\nThe neuron to be moved is $v=3$. Its initial partition is core $A$, defined by the set of neurons $P_A = \\{1, 2, 3\\}$. The destination partition is core $B$, with neurons $P_B = \\{4, 5, 6\\}$.\n\nThe weights of the connections are given by the adjacency matrix $W$:\n$$\nW \\;=\\;\n\\begin{pmatrix}\n0 & 2 & 1 & 3 & 0 & 0 \\\\\n2 & 0 & 2 & 1 & 1 & 0 \\\\\n1 & 2 & 0 & 4 & 2 & 1 \\\\\n3 & 1 & 4 & 0 & 2 & 1 \\\\\n0 & 1 & 2 & 2 & 0 & 3 \\\\\n0 & 0 & 1 & 1 & 3 & 0\n\\end{pmatrix}\n$$\n\nThe internal connections for neuron $3$ are its connections to other neurons in partition $P_A$, which are neurons $1$ and $2$. The sum of the weights of these internal connections, $I_3$, is:\n$$\nI_3 = W_{31} + W_{32} = 1 + 2 = 3\n$$\nThe external connections for neuron $3$ are its connections to all neurons in partition $P_B$, which are neurons $4$, $5$, and $6$. The sum of the weights of these external connections, $E_3$, is:\n$$\nE_3 = W_{34} + W_{35} + W_{36} = 4 + 2 + 1 = 7\n$$\nThe FM gain for moving neuron $3$ is the difference between the external and internal connection weights:\n$$\nG_{\\mathrm{FM}}(3) = E_3 - I_3 = 7 - 3 = 4\n$$\nThis value represents the reduction in the cut weight if neuron $3$ were to be moved.\n\nNext, we calculate the capacity overflow penalty, $P_{\\mathrm{cap}}$. The penalty is given by the formula:\n$$\nP_{\\mathrm{cap}} = \\lambda \\left( \\max\\{0,\\,N_A^{\\mathrm{after}} - N_{\\mathrm{cap}}^A\\} + \\max\\{0,\\,N_B^{\\mathrm{after}} - N_{\\mathrm{cap}}^B\\} \\right)\n$$\nwhere $\\lambda = 2$ is the penalty coefficient.\n\nThe initial state of the partitions is:\n- Core $A$: $P_A = \\{1, 2, 3\\}$, so the number of neurons is $N_A^{\\mathrm{initial}} = |P_A| = 3$.\n- Core $B$: $P_B = \\{4, 5, 6\\}$, so the number of neurons is $N_B^{\\mathrm{initial}} = |P_B| = 3$.\n\nThe per-core capacities are given as $N_{\\mathrm{cap}}^A = 3$ and $N_{\\mathrm{cap}}^B = 3$.\n\nWhen we move neuron $3$ from core $A$ to core $B$, the number of neurons in each core changes:\n- Core $A$ after move: $N_A^{\\mathrm{after}} = N_A^{\\mathrm{initial}} - 1 = 3 - 1 = 2$.\n- Core $B$ after move: $N_B^{\\mathrm{after}} = N_B^{\\mathrm{initial}} + 1 = 3 + 1 = 4$.\n\nNow, we can calculate the overflow for each core:\n- Overflow on core $A$: $\\max\\{0,\\, N_A^{\\mathrm{after}} - N_{\\mathrm{cap}}^A\\} = \\max\\{0,\\, 2 - 3\\} = \\max\\{0, -1\\} = 0$.\n- Overflow on core $B$: $\\max\\{0,\\, N_B^{\\mathrm{after}} - N_{\\mathrm{cap}}^B\\} = \\max\\{0,\\, 4 - 3\\} = \\max\\{0, 1\\} = 1$.\n\nThe total overflow is the sum of the overflows on each core, which is $0 + 1 = 1$.\nThe capacity overflow penalty is then:\n$$\nP_{\\mathrm{cap}} = \\lambda \\times (\\text{Total Overflow}) = 2 \\times 1 = 2\n$$\n\nFinally, we compute the effective gain for moving neuron $3$ by subtracting the capacity penalty from the FM gain:\n$$\nG_{\\mathrm{eff}}(3) = G_{\\mathrm{FM}}(3) - P_{\\mathrm{cap}} = 4 - 2 = 2\n$$\nThe effective gain is $2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "Effectively mapping a neural network often requires adapting its structure to the physical limitations of the target hardware, such as a maximum number of synaptic inputs (fan-in) per hardware neuron. When a neuron in the source model exceeds this limit, a common strategy is to split it into multiple sub-neurons, but this transformation is not exact and can introduce computational errors. This practice will guide you through using formal mathematical tools, specifically Taylor's theorem, to analyze and derive a bound for the approximation error introduced by such essential network transformations .",
            "id": "4050904",
            "problem": "A single computational neuron in a rate-based neuromorphic model is described by an input current $u$ and a static nonlinear firing-rate transfer function $g$, so that the firing rate is $r = g(u)$. The current is $u = \\sum_{i=1}^{N} w_i x_i + b$, where $w_i$ are synaptic efficacies, $x_i$ are input rates, and $b$ is a bias current. To map this neuron onto neuromorphic hardware with limited fan-in, you split it into $K$ sub-neurons that each receive a disjoint subset of the inputs and a fraction of the bias, forming sub-currents $u_k = \\sum_{i \\in \\mathcal{S}_k} w_i x_i + b/K$, with $\\{\\mathcal{S}_k\\}$ a partition of $\\{1,\\dots,N\\}$. Each sub-neuron has the same nonlinearity $g$ and produces rate $r_k = g(u_k)$. The split neuron’s output is recombined by summation, $r_{\\mathrm{split}} = \\sum_{k=1}^{K} r_k$. By construction, $\\sum_{k=1}^{K} u_k = u$.\n\nAssume $g$ is twice continuously differentiable, satisfies $g(0) = 0$, and for all $z$ in the interval between $0$ and each of $\\{u_1,\\dots,u_K,u\\}$, the second derivative is bounded in magnitude by a known constant $M > 0$, that is, $|g''(z)| \\le M$.\n\nUsing only these assumptions and the Taylor theorem with the Lagrange form of the remainder about $0$, derive a bound $B(u_1,\\dots,u_K)$ such that the absolute deviation in firing rate after splitting obeys $|r_{\\mathrm{split}} - r| \\le B(u_1,\\dots,u_K)$, expressed in terms of $M$, $\\{u_k\\}$, and $u$. Then, under a balanced mapping strategy in which the partition is chosen to equalize the sub-currents so that $u_k = u/K$ for all $k$, simplify your bound to a closed-form expression depending only on $M$, $u$, and $K$.\n\nProvide the simplified closed-form expression for the balanced case as your final answer. Do not include units. No numerical rounding is required.",
            "solution": "The objective is to find a bound on the absolute deviation in firing rate, $|r_{\\mathrm{split}} - r|$, where $r=g(u)$ is the rate of the original neuron and $r_{\\mathrm{split}}=\\sum_{k=1}^{K} g(u_k)$ is the rate of the split neuron. The input currents are related by $u = \\sum_{k=1}^{K} u_k$.\n\nThe problem specifies using the Taylor theorem with the Lagrange form of the remainder about the point $z=0$. For a function $f(z)$ that is at least twice continuously differentiable, this is given by:\n$$f(z) = f(0) + f'(0)z + \\frac{f''(\\xi)}{2!}z^2$$\nwhere $\\xi$ is some value between $0$ and $z$.\n\nThis theorem is to be applied to the firing-rate transfer function $g(z)$. We are given that $g(0) = 0$. Therefore, the Taylor expansion of $g(z)$ about $z=0$ simplifies to:\n$$g(z) = g'(0)z + \\frac{g''(\\xi_z)}{2}z^2$$\nwhere $\\xi_z$ is a value between $0$ and $z$.\n\nWe can apply this expansion to the total rate $r$ and the sub-rates $r_k$:\nFor the original neuron with input current $u$, the rate $r$ is:\n$$r = g(u) = g'(0)u + \\frac{g''(\\xi_0)}{2}u^{2}$$\nwhere $\\xi_0$ is some value between $0$ and $u$.\n\nFor each sub-neuron $k$ with input current $u_k$, the rate $r_k$ is:\n$$r_k = g(u_k) = g'(0)u_k + \\frac{g''(\\xi_k)}{2}u_k^{2}$$\nwhere $\\xi_k$ is some value between $0$ and $u_k$.\n\nThe total rate of the split neuron, $r_{\\mathrm{split}}$, is the sum of the sub-rates:\n$$r_{\\mathrm{split}} = \\sum_{k=1}^{K} r_k = \\sum_{k=1}^{K} \\left( g'(0)u_k + \\frac{g''(\\xi_k)}{2}u_k^{2} \\right)$$\nBy distributing the summation, we get:\n$$r_{\\mathrm{split}} = g'(0) \\left( \\sum_{k=1}^{K} u_k \\right) + \\frac{1}{2} \\sum_{k=1}^{K} g''(\\xi_k)u_k^{2}$$\nBy construction, we have the identity $\\sum_{k=1}^{K} u_k = u$. Substituting this into the expression for $r_{\\mathrm{split}}$ yields:\n$$r_{\\mathrm{split}} = g'(0)u + \\frac{1}{2} \\sum_{k=1}^{K} g''(\\xi_k)u_k^{2}$$\n\nThe deviation in firing rate is the difference $r_{\\mathrm{split}} - r$:\n$$r_{\\mathrm{split}} - r = \\left( g'(0)u + \\frac{1}{2} \\sum_{k=1}^{K} g''(\\xi_k)u_k^{2} \\right) - \\left( g'(0)u + \\frac{g''(\\xi_0)}{2}u^{2} \\right)$$\nThe linear terms in $u$ cancel, resulting in:\n$$r_{\\mathrm{split}} - r = \\frac{1}{2} \\left( \\sum_{k=1}^{K} g''(\\xi_k)u_k^{2} - g''(\\xi_0)u^{2} \\right)$$\n\nWe now seek a bound on the absolute value of this deviation, $|r_{\\mathrm{split}} - r|$.\n$$|r_{\\mathrm{split}} - r| = \\frac{1}{2} \\left| \\sum_{k=1}^{K} g''(\\xi_k)u_k^{2} - g''(\\xi_0)u^{2} \\right|$$\nUsing the triangle inequality, $|a - b| \\le |a| + |b|$, we obtain:\n$$|r_{\\mathrm{split}} - r| \\le \\frac{1}{2} \\left( \\left| \\sum_{k=1}^{K} g''(\\xi_k)u_k^{2} \\right| + \\left| -g''(\\xi_0)u^{2} \\right| \\right)$$\nA second application of the triangle inequality to the sum, $\\left| \\sum z_i \\right| \\le \\sum |z_i|$, gives:\n$$|r_{\\mathrm{split}} - r| \\le \\frac{1}{2} \\left( \\sum_{k=1}^{K} |g''(\\xi_k)u_k^{2}| + |g''(\\xi_0)|u^{2} \\right)$$\n$$|r_{\\mathrm{split}} - r| \\le \\frac{1}{2} \\left( \\sum_{k=1}^{K} |g''(\\xi_k)|u_k^{2} + |g''(\\xi_0)|u^{2} \\right)$$\nWe are given the condition that $|g''(z)| \\le M$ for a positive constant $M$ over the relevant intervals. This applies to all $\\xi_k$ and $\\xi_0$. Substituting this bound, we have:\n$$|r_{\\mathrm{split}} - r| \\le \\frac{1}{2} \\left( \\sum_{k=1}^{K} M u_k^{2} + M u^{2} \\right)$$\nFactoring out the constant $M$ yields the general bound $B(u_1, \\dots, u_K)$:\n$$B(u_1, \\dots, u_K) = \\frac{M}{2} \\left( \\sum_{k=1}^{K} u_k^{2} + u^{2} \\right)$$\n\nNext, we simplify this bound for the balanced mapping case, where $u_k = u/K$ for all $k \\in \\{1, \\dots, K\\}$. We first compute the sum of squares, $\\sum_{k=1}^{K} u_k^{2}$:\n$$\\sum_{k=1}^{K} u_k^{2} = \\sum_{k=1}^{K} \\left(\\frac{u}{K}\\right)^{2} = \\sum_{k=1}^{K} \\frac{u^{2}}{K^{2}} = K \\left( \\frac{u^{2}}{K^{2}} \\right) = \\frac{u^{2}}{K}$$\nSubstituting this result into the general bound expression:\n$$|r_{\\mathrm{split}} - r| \\le \\frac{M}{2} \\left( \\frac{u^{2}}{K} + u^{2} \\right)$$\nWe can factor out $u^{2}$ and simplify the expression in parentheses:\n$$|r_{\\mathrm{split}} - r| \\le \\frac{M u^{2}}{2} \\left( \\frac{1}{K} + 1 \\right) = \\frac{M u^{2}}{2} \\left( \\frac{1+K}{K} \\right)$$\nThis gives the simplified closed-form expression for the bound in the balanced case:\n$$\\frac{M u^{2} (K+1)}{2K}$$\nThis expression depends only on $M$, $u$, and $K$, as requested by the problem.",
            "answer": "$$\\boxed{\\frac{M u^{2} (K+1)}{2K}}$$"
        },
        {
            "introduction": "The ultimate goal of a mapping algorithm is to find an optimal deployment, but \"optimal\" is rarely a single number; it's a balance of competing objectives like energy, latency, and area. This exercise contrasts two fundamental strategies for multi-objective optimization: a simple weighted-sum scalarization and the more comprehensive concept of a Pareto-optimal front. Through a carefully constructed scenario involving a biased cost model, you will discover the potential pitfalls of scalarization and gain a deeper appreciation for why Pareto dominance is often a more robust criterion for evaluating complex design trade-offs in neuromorphic engineering .",
            "id": "4050920",
            "problem": "A neuromorphic mapping tool evaluates candidate mappings of a spiking neural network to a crossbar-based substrate using four objectives: energy per inference $E$ (in $\\mathrm{mJ}$), latency $L$ (in $\\mathrm{ms}$), silicon area $A$ (in $\\mathrm{mm}^2$), and inter-core communication $C$ (in $\\mathrm{MB}$). All four objectives are to be minimized. Consider two feasible mappings $M_1$ and $M_2$ with true objective vectors\n- $M_1$: $(E_1, L_1, A_1, C_1) = (1.20\\,\\mathrm{mJ}, 7\\,\\mathrm{ms}, 16\\,\\mathrm{mm}^2, 9\\,\\mathrm{MB})$,\n- $M_2$: $(E_2, L_2, A_2, C_2) = (1.00\\,\\mathrm{mJ}, 6\\,\\mathrm{ms}, 14\\,\\mathrm{mm}^2, 8\\,\\mathrm{MB})$.\n\nTo combine the objectives into a single score for selection, the tool applies a normalized weighted sum (scalarization). Let the normalization scales be $(s_E, s_L, s_A, s_C) = (1\\,\\mathrm{mJ}, 10\\,\\mathrm{ms}, 20\\,\\mathrm{mm}^2, 10\\,\\mathrm{MB})$. Let the weights be $(w_E, w_L, w_A, w_C) = (0.4, 0.2, 0.2, 0.2)$, with $w_E + w_L + w_A + w_C = 1$. The scalarized score of mapping $M_i$ under the cost model is\n$$\nJ_i = w_E \\frac{\\widehat{E}_i}{s_E} + w_L \\frac{\\widehat{L}_i}{s_L} + w_A \\frac{\\widehat{A}_i}{s_A} + w_C \\frac{\\widehat{C}_i}{s_C},\n$$\nwhere $\\widehat{(\\cdot)}$ denotes the estimated metrics used by the tool.\n\nSuppose the cost model is unbiased for $E$, $L$, and $A$, but systematically underestimates communication for $M_1$ by a multiplicative factor $\\gamma \\in (0,1)$, while it is accurate for $M_2$:\n- $\\widehat{E}_i = E_i$ for $i \\in \\{1,2\\}$,\n- $\\widehat{L}_i = L_i$ for $i \\in \\{1,2\\}$,\n- $\\widehat{A}_i = A_i$ for $i \\in \\{1,2\\}$,\n- $\\widehat{C}_1 = \\gamma C_1$ and $\\widehat{C}_2 = C_2$.\n\nA Pareto-front method based on the true objectives would declare $M_2$ strictly dominating $M_1$ and discard $M_1$. By contrast, the scalarization-based selector will choose the mapping with the smaller $J_i$ under the possibly biased estimates.\n\nUsing only the foundational definitions of Pareto dominance and normalized weighted-sum scalarization, and the data given above, derive the exact supremum value $\\gamma^{\\star}$ such that for all $\\gamma \\in (0, \\gamma^{\\star})$, the scalarized selector (operating on the biased estimates) erroneously prefers $M_1$ even though $M_1$ is dominated by $M_2$ in the true objectives. Provide your final answer as the exact value of $\\gamma^{\\star}$ with no rounding.",
            "solution": "The problem asks for the supremum value of a bias factor, $\\gamma^{\\star}$, for which a scalarization-based selection algorithm erroneously prefers a mapping $M_1$ over another mapping $M_2$, despite $M_2$ being Pareto-superior to $M_1$.\n\nFirst, we validate the premise of Pareto dominance. The true objective vectors for the mappings $M_1$ and $M_2$ are given as:\n$M_1$: $(E_1, L_1, A_1, C_1) = (1.20, 7, 16, 9)$\n$M_2$: $(E_2, L_2, A_2, C_2) = (1.00, 6, 14, 8)$\nThe units are $\\mathrm{mJ}$ for energy $E$, $\\mathrm{ms}$ for latency $L$, $\\mathrm{mm}^2$ for area $A$, and $\\mathrm{MB}$ for communication $C$. All four objectives are to be minimized.\nA solution $M_2$ strictly dominates a solution $M_1$ if $M_2$ is at least as good as $M_1$ in all objectives and strictly better in at least one objective. Since we are minimizing, 'better' means having a smaller value.\nComparing the objectives:\n$E_2 = 1.00 < E_1 = 1.20$\n$L_2 = 6 < L_1 = 7$\n$A_2 = 14 < A_1 = 16$\n$C_2 = 8 < C_1 = 9$\nSince all objectives for $M_2$ have smaller values than their counterparts for $M_1$, $M_2$ strictly dominates $M_1$. A Pareto-based method would correctly identify $M_2$ as the superior solution and discard $M_1$.\n\nNext, we analyze the scalarization-based selector. The score for a mapping $M_i$ is given by the weighted sum of its normalized estimated objectives:\n$$J_i = w_E \\frac{\\widehat{E}_i}{s_E} + w_L \\frac{\\widehat{L}_i}{s_L} + w_A \\frac{\\widehat{A}_i}{s_A} + w_C \\frac{\\widehat{C}_i}{s_C}$$\nThe normalization scales are $(s_E, s_L, s_A, s_C) = (1, 10, 20, 10)$ and the weights are $(w_E, w_L, w_A, w_C) = (0.4, 0.2, 0.2, 0.2)$.\nThe estimated metrics $\\widehat{(\\cdot)}$ are given as:\n$\\widehat{E}_i = E_i$, $\\widehat{L}_i = L_i$, $\\widehat{A}_i = A_i$ for $i \\in \\{1,2\\}$.\n$\\widehat{C}_1 = \\gamma C_1$ and $\\widehat{C}_2 = C_2$, where $\\gamma \\in (0,1)$.\n\nThe scalarized selector erroneously prefers $M_1$ over $M_2$ if the calculated score for $M_1$ is less than the score for $M_2$, i.e., $J_1 < J_2$. We must find the supremum value $\\gamma^{\\star}$ such that this inequality holds for all $\\gamma \\in (0, \\gamma^{\\star})$.\n\nLet's write the expressions for $J_1$ and $J_2$ using the given values and the biased estimate for $C_1$:\nFor $M_1$:\n$J_1 = w_E \\frac{E_1}{s_E} + w_L \\frac{L_1}{s_L} + w_A \\frac{A_1}{s_A} + w_C \\frac{\\gamma C_1}{s_C}$\nSubstituting the values:\n$J_1 = (0.4) \\frac{1.20}{1} + (0.2) \\frac{7}{10} + (0.2) \\frac{16}{20} + (0.2) \\frac{\\gamma \\cdot 9}{10}$\n$J_1 = (0.4)(1.20) + (0.2)(0.7) + (0.2)(0.8) + (0.2)(0.9)\\gamma$\n$J_1 = 0.48 + 0.14 + 0.16 + 0.18\\gamma$\n$J_1 = 0.78 + 0.18\\gamma$\n\nFor $M_2$:\n$J_2 = w_E \\frac{E_2}{s_E} + w_L \\frac{L_2}{s_L} + w_A \\frac{A_2}{s_A} + w_C \\frac{C_2}{s_C}$\nSubstituting the values:\n$J_2 = (0.4) \\frac{1.00}{1} + (0.2) \\frac{6}{10} + (0.2) \\frac{14}{20} + (0.2) \\frac{8}{10}$\n$J_2 = (0.4)(1.00) + (0.2)(0.6) + (0.2)(0.7) + (0.2)(0.8)$\n$J_2 = 0.40 + 0.12 + 0.14 + 0.16$\n$J_2 = 0.82$\n\nThe condition for erroneously preferring $M_1$ is $J_1 < J_2$. We set up and solve this inequality for $\\gamma$:\n$0.78 + 0.18\\gamma < 0.82$\n$0.18\\gamma < 0.82 - 0.78$\n$0.18\\gamma < 0.04$\n$\\gamma < \\frac{0.04}{0.18}$\n$\\gamma < \\frac{4}{18}$\n$\\gamma < \\frac{2}{9}$\n\nThe problem states that this erroneous preference for $M_1$ occurs for all $\\gamma \\in (0, \\gamma^{\\star})$. The inequality we derived, $\\gamma < 2/9$, defines the range of $\\gamma$ for which $J_1 < J_2$. Since the problem specifies $\\gamma \\in (0,1)$ and $2/9 < 1$, the interval of interest is $\\gamma \\in (0, 2/9)$.\n\nThe question asks for the supremum of this interval. The supremum of the set $\\{\\gamma \\in \\mathbb{R} \\mid 0 < \\gamma < 2/9\\}$ is the least upper bound of the set, which is $2/9$.\nTherefore, the supremum value is $\\gamma^{\\star} = 2/9$. At $\\gamma = 2/9$, the scores would be equal ($J_1=J_2$), and for any value of $\\gamma$ smaller than this, the biased cost model will lead to the incorrect selection of $M_1$.\nThe final answer must be the exact value.",
            "answer": "$$\\boxed{\\frac{2}{9}}$$"
        }
    ]
}