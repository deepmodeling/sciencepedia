{
    "hands_on_practices": [
        {
            "introduction": "A primary motivation for neuromorphic computing is energy efficiency. To create energy-aware mapping algorithms, we must first be able to accurately model and predict the energy consumption of a given network on target hardware. This practice guides you through constructing a foundational energy model from the bottom up, linking the energy costs of individual neural and communication events to the total expected energy consumption of a network layer . Developing this skill is essential for analyzing and optimizing the performance of Spiking Neural Networks on Network-on-Chip (NoC) architectures.",
            "id": "4050840",
            "problem": "Consider a single feedforward layer of a Spiking Neural Network (SNN) mapped onto a two-dimensional mesh Network-on-Chip (NoC). The NoC comprises $K \\times K$ identical routing tiles arranged on integer coordinates $(x,y)$ with $x,y \\in \\{1,2,\\dots,K\\}$, and packets traverse links along shortest Manhattan paths. Each spike generated by a source neuron is replicated into independent unicast packets to its postsynaptic targets by the NoC’s routing logic. Each neuron spike event consumes an energy $E_{\\text{neuron}}$ per spike at the neuron’s source tile. Each per-packet router processing step (injection or forwarding at a router) consumes $E_{\\text{router}}$ per packet per router, and each per-packet link traversal consumes $E_{\\text{link}}$ per hop. Assume links and routers are identical and their energy costs do not depend on load. By a mapping policy, each source neuron’s targets are chosen independently and uniformly at random among all tiles (excluding the source tile), and routing uses independent shortest Manhattan paths for each unicast packet.\n\nLet $m$ denote the multicast degree of a spike (the number of target destinations for that spike), and let $\\ell$ denote the aggregated hop count of that spike (the sum of link hops across all $m$ unicast paths). Treat the $m$ unicast paths as independent shortest paths to uniformly random destinations. A spike emitted by neuron $i$ at rate $r_i$ is modeled as a Poisson process over a time window $T$, and spikes from distinct neurons are independent. Let there be $N$ neurons in the layer, each assigned to a tile uniformly at random and independently of other neurons.\n\nStarting only from additivity of energy across independent events and the definitions above:\n\n1. Derive a closed-form expression for the expected aggregated hop count $\\mathbb{E}[\\ell \\mid m,K]$ for one spike as a function of $m$ and $K$.\n2. Using energy additivity, derive a symbolic expression for the expected per-spike energy in terms of $E_{\\text{neuron}}$, $E_{\\text{router}}$, $E_{\\text{link}}$, $m$, and $K$.\n3. Compose the result to obtain a closed-form expression for the expected total energy of the layer over the window $T$ in joules in terms of $E_{\\text{neuron}}$, $E_{\\text{router}}$, $E_{\\text{link}}$, $K$, $T$, $\\{r_i\\}_{i=1}^{N}$, and the average multicast degree $\\bar{m}$ of spikes in the layer. Assume the multicast degrees are independent of spike timing and identically distributed across neurons with finite mean $\\bar{m}$.\n\nExpress your final answer as one analytical expression. If units are needed, express the energy in joules. No numerical approximation or rounding is required.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the principles of neuromorphic computing, computer architecture, and probability theory. It is well-posed, objective, and contains sufficient information to derive a unique solution. The model, while simplified, is self-consistent and does not violate any fundamental laws.\n\nThe solution is derived in three parts as requested by the problem statement.\n\n### Part 1: Expected Aggregated Hop Count\n\nLet $\\ell$ be the aggregated hop count for a single spike with multicast degree $m$. It is the sum of the hop counts $h_j$ for each of the $m$ unicast packets:\n$$\n\\ell = \\sum_{j=1}^{m} h_j\n$$\nBy the linearity of expectation, the expected aggregated hop count is:\n$$\n\\mathbb{E}[\\ell \\mid m, K] = \\mathbb{E}\\left[\\sum_{j=1}^{m} h_j \\mid m, K\\right] = \\sum_{j=1}^{m} \\mathbb{E}[h_j \\mid m, K]\n$$\nThe problem states that the $m$ target destinations are chosen independently and uniformly at random from all tiles excluding the source tile. This implies that the distribution of the hop count $h_j$ is identical for all packets $j \\in \\{1, \\dots, m\\}$. Let $\\mathbb{E}[h]$ denote this common expected hop count. Then,\n$$\n\\mathbb{E}[\\ell \\mid m, K] = m \\, \\mathbb{E}[h]\n$$\nTo find $\\mathbb{E}[h]$, we must calculate the expected Manhattan distance between two distinct tiles chosen uniformly at random on the $K \\times K$ grid. Let a source tile be at coordinates $(x_s, y_s)$ and a destination tile be at $(x_d, y_d)$, where $x_s, y_s, x_d, y_d \\in \\{1, 2, \\dots, K\\}$. The shortest Manhattan path has a hop count (distance) of $h = |x_s - x_d| + |y_s - y_d|$.\n\nBy linearity of expectation, $\\mathbb{E}[h] = \\mathbb{E}[|x_s - x_d|] + \\mathbb{E}[|y_s - y_d|]$. Due to the symmetry of the grid, $\\mathbb{E}[|x_s - x_d|] = \\mathbb{E}[|y_s - y_d|]$. We need to compute this value, averaged over all possible distinct source and destination tile pairs.\n\nThe total number of tile pairs is $K^4$. The number of distinct pairs (source $\\neq$ destination) is $K^2(K^2 - 1)$. The expectation of the one-dimensional distance $|x_s - x_d|$ is the sum of $|x_s - x_d|$ over all distinct source/destination tile pairs, divided by the number of such pairs.\n$$\n\\mathbb{E}[|x_s - x_d|] = \\frac{\\sum_{\\text{source } s \\neq \\text{dest } d} |x_s - x_d|}{K^2(K^2 - 1)}\n$$\nThe summation in the numerator can be expanded:\n$$\n\\sum_{s \\neq d} |x_s - x_d| = \\sum_{s} \\sum_{d \\neq s} |x_s - x_d|\n$$\nSince $|x_s - x_s| = 0$, we can write $\\sum_{d \\neq s} |x_s - x_d| = \\sum_{d} |x_s - x_d|$.\n$$\n\\sum_{s} \\sum_{d} |x_s - x_d| = \\sum_{(x_s, y_s)} \\sum_{(x_d, y_d)} |x_s - x_d| = K^2 \\sum_{x_s=1}^K \\sum_{x_d=1}^K |x_s - x_d|\n$$\nThe one-dimensional sum is a standard result:\n$$\n\\sum_{i=1}^K \\sum_{j=1}^K |i - j| = 2 \\sum_{i=1}^K \\sum_{j=1}^{i-1} (i - j) = \\frac{K(K^2 - 1)}{3}\n$$\nSubstituting this back, the total sum of all one-dimensional distances along x is:\n$$\n\\sum_{s \\neq d} |x_s - x_d| = K^2 \\cdot \\frac{K(K^2 - 1)}{3} = \\frac{K^3(K^2 - 1)}{3}\n$$\nTherefore, the expected one-dimensional distance is:\n$$\n\\mathbb{E}[|x_s - x_d|] = \\frac{K^3(K^2 - 1) / 3}{K^2(K^2 - 1)} = \\frac{K}{3}\n$$\nThe expected Manhattan distance is then:\n$$\n\\mathbb{E}[h] = \\mathbb{E}[|x_s - x_d|] + \\mathbb{E}[|y_s - y_d|] = \\frac{K}{3} + \\frac{K}{3} = \\frac{2K}{3}\n$$\nFinally, the expected aggregated hop count for a spike with multicast degree $m$ is:\n$$\n\\mathbb{E}[\\ell \\mid m, K] = m \\frac{2K}{3}\n$$\n\n### Part 2: Expected Per-Spike Energy\n\nThe total energy for a single spike event, $E_{\\text{spike}}$, is the sum of the neuron's spiking energy and the communication energy for its $m$ packets.\n$$\nE_{\\text{spike}} = E_{\\text{neuron}} + E_{\\text{comm}}\n$$\nThe communication energy $E_{\\text{comm}}$ is the sum of energies for all $m$ unicast packets. For a single packet traversing a path of $h_j$ hops, it crosses $h_j$ links and is processed by $h_j+1$ routers (one injection, $h_j-1$ forwarding steps, and one absorption, which we assume falls under the router processing definition). The energy for packet $j$ is:\n$$\nE_{\\text{packet}, j} = h_j E_{\\text{link}} + (h_j + 1) E_{\\text{router}}\n$$\nThe total communication energy for the spike is:\n$$\nE_{\\text{comm}} = \\sum_{j=1}^m E_{\\text{packet}, j} = \\sum_{j=1}^m (h_j E_{\\text{link}} + (h_j + 1) E_{\\text{router}})\n$$\nWe seek the expected per-spike energy, conditioned on $m$ and $K$. Using linearity of expectation:\n$$\n\\mathbb{E}[E_{\\text{spike}} \\mid m, K] = E_{\\text{neuron}} + \\mathbb{E}\\left[\\sum_{j=1}^m (h_j E_{\\text{link}} + (h_j + 1) E_{\\text{router}}) \\mid m, K\\right]\n$$\n$$\n\\mathbb{E}[E_{\\text{spike}} \\mid m, K] = E_{\\text{neuron}} + \\sum_{j=1}^m \\mathbb{E}[h_j E_{\\text{link}} + (h_j + 1) E_{\\text{router}} \\mid m, K]\n$$\nSince $E_{\\text{link}}$ and $E_{\\text{router}}$ are constants and $\\mathbb{E}[h_j] = \\mathbb{E}[h] = \\frac{2K}{3}$ for all $j$:\n$$\n\\mathbb{E}[E_{\\text{spike}} \\mid m, K] = E_{\\text{neuron}} + \\sum_{j=1}^m \\left( \\mathbb{E}[h] E_{\\text{link}} + (\\mathbb{E}[h] + 1) E_{\\text{router}} \\right)\n$$\n$$\n\\mathbb{E}[E_{\\text{spike}} \\mid m, K] = E_{\\text{neuron}} + m \\left( \\frac{2K}{3} E_{\\text{link}} + \\left(\\frac{2K}{3} + 1\\right) E_{\\text{router}} \\right)\n$$\n\n### Part 3: Expected Total Energy over Time T\n\nThe total energy $E_{\\text{total}}$ is the sum of energies of all spikes occurring in the time window $T$. Let $S$ be the total number of spikes in the window from all $N$ neurons. The energy of the $k$-th spike is $E_{\\text{spike}, k}$.\n$$\nE_{\\text{total}} = \\sum_{k=1}^S E_{\\text{spike}, k}\n$$\nThe number of spikes from neuron $i$, $S_i$, follows a Poisson distribution with mean $r_i T$. The total number of spikes $S = \\sum_{i=1}^N S_i$ is also a Poisson random variable (sum of independent Poissons) with mean $\\mathbb{E}[S] = \\sum_{i=1}^N r_i T = T \\sum_{i=1}^N r_i$.\n\nThe energy of each spike is a random variable, dependent on its multicast degree $m$. The problem states that multicast degrees are independent of spike timing and are identically distributed with mean $\\bar{m}$. This means the energy of each spike, $E_{\\text{spike}, k}$, is an independent and identically distributed random variable, and its value is independent of the total number of spikes $S$. Therefore, we can apply Wald's identity:\n$$\n\\mathbb{E}[E_{\\text{total}}] = \\mathbb{E}[S] \\cdot \\mathbb{E}[E_{\\text{spike}}]\n$$\nWe need the unconditional expected energy per spike, $\\mathbb{E}[E_{\\text{spike}}]$. This is found by taking the expectation of the result from Part 2 over the distribution of $m$:\n$$\n\\mathbb{E}[E_{\\text{spike}}] = \\mathbb{E}_m[\\mathbb{E}[E_{\\text{spike}} \\mid m, K]] = \\mathbb{E}_m\\left[ E_{\\text{neuron}} + m \\left( \\frac{2K}{3} E_{\\text{link}} + \\left(\\frac{2K}{3} + 1\\right) E_{\\text{router}} \\right) \\right]\n$$\nUsing linearity of expectation and $\\mathbb{E}[m] = \\bar{m}$:\n$$\n\\mathbb{E}[E_{\\text{spike}}] = E_{\\text{neuron}} + \\bar{m} \\left( \\frac{2K}{3} E_{\\text{link}} + \\left(\\frac{2K}{3} + 1\\right) E_{\\text{router}} \\right)\n$$\nCombining these results, the expected total energy over the window $T$ is:\n$$\n\\mathbb{E}[E_{\\text{total}}] = \\left(T \\sum_{i=1}^N r_i\\right) \\left[ E_{\\text{neuron}} + \\bar{m} \\left( \\left(\\frac{2K}{3} + 1\\right) E_{\\text{router}} + \\frac{2K}{3} E_{\\text{link}} \\right) \\right]\n$$\nThis is the final closed-form expression for the expected total energy.",
            "answer": "$$ \\boxed{ T \\left( \\sum_{i=1}^{N} r_i \\right) \\left[ E_{\\text{neuron}} + \\bar{m} \\left( \\left( \\frac{2K}{3} + 1 \\right) E_{\\text{router}} + \\frac{2K}{3} E_{\\text{link}} \\right) \\right] } $$"
        },
        {
            "introduction": "After defining objectives like minimizing communication, the next step is to use an algorithm to find a mapping that achieves this goal. This exercise provides a concrete, hands-on application of the Fiduccia-Mattheyses (FM) algorithm, a cornerstone heuristic for graph partitioning that is widely used in VLSI design and network mapping. By manually calculating the \"gain\" from moving a single neuron between hardware cores, you will gain a practical intuition for how iterative refinement algorithms intelligently explore the solution space to minimize inter-core traffic while adhering to critical hardware capacity constraints .",
            "id": "4050853",
            "problem": "Consider mapping a Spiking Neural Network (SNN) onto two neuromorphic cores, with the objective of minimizing inter-core communication by partitioning the network while respecting per-core capacities. Let the effective undirected inter-neuron coupling weights be represented by the symmetric adjacency matrix $W \\in \\mathbb{R}^{6 \\times 6}$ for neurons labeled $\\{1,2,3,4,5,6\\}$:\n$$\nW \\;=\\;\n\\begin{pmatrix}\n0 & 2 & 1 & 3 & 0 & 0 \\\\\n2 & 0 & 2 & 1 & 1 & 0 \\\\\n1 & 2 & 0 & 4 & 2 & 1 \\\\\n3 & 1 & 4 & 0 & 2 & 1 \\\\\n0 & 1 & 2 & 2 & 0 & 3 \\\\\n0 & 0 & 1 & 1 & 3 & 0\n\\end{pmatrix}.\n$$\nAssume an initial two-way partition onto cores $A$ and $B$ defined by $P_A = \\{1,2,3\\}$ and $P_B = \\{4,5,6\\}$. The per-core neuron capacities are $N_{\\mathrm{cap}}^A = 3$ and $N_{\\mathrm{cap}}^B = 3$. The mapping objective penalizes capacity overflow using a scalar penalty coefficient $\\lambda = 2$, measured in the same units as the cut weight. Define the total cut weight as the sum of weights of all edges crossing the partition, and define the Fiduccia–Mattheyses (FM) gain for moving a single neuron $v$ from its current core to the other core as the reduction in cut weight induced by the move. To incorporate capacity constraints, define the effective gain for moving a neuron $v$ as the FM gain minus a capacity overflow penalty, where the penalty is $\\lambda$ times the sum of positive overflows after the move, namely\n$$\n\\lambda \\left( \\max\\{0,\\,N_A^{\\mathrm{after}} - N_{\\mathrm{cap}}^A\\} + \\max\\{0,\\,N_B^{\\mathrm{after}} - N_{\\mathrm{cap}}^B\\} \\right),\n$$\nwith $N_A^{\\mathrm{after}}$ and $N_B^{\\mathrm{after}}$ denoting the neuron counts on cores $A$ and $B$ after the move.\n\nCompute the effective gain, as a single real number, for moving neuron $3$ from core $A$ to core $B$ in one iteration of FM refinement. Express your final answer as a real number without units. No rounding is required.",
            "solution": "The problem is well-posed and contains all necessary information to compute a unique solution. We are asked to compute the effective gain for moving neuron $3$ from core A to core B. The effective gain, $G_{\\mathrm{eff}}(v)$, for moving a neuron $v$ is defined as the Fiduccia–Mattheyses (FM) gain, $G_{\\mathrm{FM}}(v)$, minus a capacity overflow penalty, $P_{\\mathrm{cap}}$.\n$$\nG_{\\mathrm{eff}}(v) = G_{\\mathrm{FM}}(v) - P_{\\mathrm{cap}}\n$$\n\nFirst, we calculate the FM gain, $G_{\\mathrm{FM}}(3)$, for moving neuron $3$. The FM gain is defined as the reduction in the total cut weight. This can be calculated as the sum of the weights of the connections from the neuron to the destination partition (external connections) minus the sum of the weights of the connections from the neuron to other neurons in its source partition (internal connections).\n\nThe neuron to be moved is $v=3$. Its initial partition is core $A$, defined by the set of neurons $P_A = \\{1, 2, 3\\}$. The destination partition is core $B$, with neurons $P_B = \\{4, 5, 6\\}$.\n\nThe weights of the connections are given by the adjacency matrix $W$:\n$$\nW \\;=\\;\n\\begin{pmatrix}\n0 & 2 & 1 & 3 & 0 & 0 \\\\\n2 & 0 & 2 & 1 & 1 & 0 \\\\\n1 & 2 & 0 & 4 & 2 & 1 \\\\\n3 & 1 & 4 & 0 & 2 & 1 \\\\\n0 & 1 & 2 & 2 & 0 & 3 \\\\\n0 & 0 & 1 & 1 & 3 & 0\n\\end{pmatrix}\n$$\n\nThe internal connections for neuron $3$ are its connections to other neurons in partition $P_A$, which are neurons $1$ and $2$. The sum of the weights of these internal connections, $I_3$, is:\n$$\nI_3 = W_{31} + W_{32} = 1 + 2 = 3\n$$\nThe external connections for neuron $3$ are its connections to all neurons in partition $P_B$, which are neurons $4$, $5$, and $6$. The sum of the weights of these external connections, $E_3$, is:\n$$\nE_3 = W_{34} + W_{35} + W_{36} = 4 + 2 + 1 = 7\n$$\nThe FM gain for moving neuron $3$ is the difference between the external and internal connection weights:\n$$\nG_{\\mathrm{FM}}(3) = E_3 - I_3 = 7 - 3 = 4\n$$\nThis value represents the reduction in the cut weight if neuron $3$ were to be moved.\n\nNext, we calculate the capacity overflow penalty, $P_{\\mathrm{cap}}$. The penalty is given by the formula:\n$$\nP_{\\mathrm{cap}} = \\lambda \\left( \\max\\{0,\\,N_A^{\\mathrm{after}} - N_{\\mathrm{cap}}^A\\} + \\max\\{0,\\,N_B^{\\mathrm{after}} - N_{\\mathrm{cap}}^B\\} \\right)\n$$\nwhere $\\lambda = 2$ is the penalty coefficient.\n\nThe initial state of the partitions is:\n- Core $A$: $P_A = \\{1, 2, 3\\}$, so the number of neurons is $N_A^{\\mathrm{initial}} = |P_A| = 3$.\n- Core $B$: $P_B = \\{4, 5, 6\\}$, so the number of neurons is $N_B^{\\mathrm{initial}} = |P_B| = 3$.\n\nThe per-core capacities are given as $N_{\\mathrm{cap}}^A = 3$ and $N_{\\mathrm{cap}}^B = 3$.\n\nWhen we move neuron $3$ from core $A$ to core $B$, the number of neurons in each core changes:\n- Core $A$ after move: $N_A^{\\mathrm{after}} = N_A^{\\mathrm{initial}} - 1 = 3 - 1 = 2$.\n- Core $B$ after move: $N_B^{\\mathrm{after}} = N_B^{\\mathrm{initial}} + 1 = 3 + 1 = 4$.\n\nNow, we can calculate the overflow for each core:\n- Overflow on core $A$: $\\max\\{0,\\, N_A^{\\mathrm{after}} - N_{\\mathrm{cap}}^A\\} = \\max\\{0,\\, 2 - 3\\} = \\max\\{0, -1\\} = 0$.\n- Overflow on core $B$: $\\max\\{0,\\, N_B^{\\mathrm{after}} - N_{\\mathrm{cap}}^B\\} = \\max\\{0,\\, 4 - 3\\} = \\max\\{0, 1\\} = 1$.\n\nThe total overflow is the sum of the overflows on each core, which is $0 + 1 = 1$.\nThe capacity overflow penalty is then:\n$$\nP_{\\mathrm{cap}} = \\lambda \\times (\\text{Total Overflow}) = 2 \\times 1 = 2\n$$\n\nFinally, we compute the effective gain for moving neuron $3$ by subtracting the capacity penalty from the FM gain:\n$$\nG_{\\mathrm{eff}}(3) = G_{\\mathrm{FM}}(3) - P_{\\mathrm{cap}} = 4 - 2 = 2\n$$\nThe effective gain is $2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "Selecting an optimal mapping is rarely about a single metric; it almost always involves balancing conflicting objectives such as energy, latency, and area. This final practice delves into the crucial challenge of multi-objective optimization, contrasting a simple weighted-sum method (scalarization) with the more robust concept of Pareto dominance. By analyzing a hypothetical scenario where a biased cost model leads to a demonstrably suboptimal choice, you will learn to critically evaluate selection strategies and appreciate the importance of considering the entire Pareto front of non-dominated solutions .",
            "id": "4050920",
            "problem": "A neuromorphic mapping tool evaluates candidate mappings of a spiking neural network to a crossbar-based substrate using four objectives: energy per inference $E$ (in $\\mathrm{mJ}$), latency $L$ (in $\\mathrm{ms}$), silicon area $A$ (in $\\mathrm{mm}^2$), and intercore communication $C$ (in $\\mathrm{MB}$). All four objectives are to be minimized. Consider two feasible mappings $M_1$ and $M_2$ with true objective vectors\n- $M_1$: $(E_1, L_1, A_1, C_1) = (1.20\\,\\mathrm{mJ}, 7\\,\\mathrm{ms}, 16\\,\\mathrm{mm}^2, 9\\,\\mathrm{MB})$,\n- $M_2$: $(E_2, L_2, A_2, C_2) = (1.00\\,\\mathrm{mJ}, 6\\,\\mathrm{ms}, 14\\,\\mathrm{mm}^2, 8\\,\\mathrm{MB})$.\n\nTo combine the objectives into a single score for selection, the tool applies a normalized weighted sum (scalarization). Let the normalization scales be $(s_E, s_L, s_A, s_C) = (1\\,\\mathrm{mJ}, 10\\,\\mathrm{ms}, 20\\,\\mathrm{mm}^2, 10\\,\\mathrm{MB})$. Let the weights be $(w_E, w_L, w_A, w_C) = (0.4, 0.2, 0.2, 0.2)$, with $w_E + w_L + w_A + w_C = 1$. The scalarized score of mapping $M_i$ under the cost model is\n$$\nJ_i = w_E \\frac{\\widehat{E}_i}{s_E} + w_L \\frac{\\widehat{L}_i}{s_L} + w_A \\frac{\\widehat{A}_i}{s_A} + w_C \\frac{\\widehat{C}_i}{s_C},\n$$\nwhere $\\widehat{(\\cdot)}$ denotes the estimated metrics used by the tool.\n\nSuppose the cost model is unbiased for $E$, $L$, and $A$, but systematically underestimates communication for $M_1$ by a multiplicative factor $\\gamma \\in (0,1)$, while it is accurate for $M_2$:\n- $\\widehat{E}_i = E_i$ for $i \\in \\{1,2\\}$,\n- $\\widehat{L}_i = L_i$ for $i \\in \\{1,2\\}$,\n- $\\widehat{A}_i = A_i$ for $i \\in \\{1,2\\}$,\n- $\\widehat{C}_1 = \\gamma C_1$ and $\\widehat{C}_2 = C_2$.\n\nA Pareto-front method based on the true objectives would declare $M_2$ strictly dominating $M_1$ and discard $M_1$. By contrast, the scalarization-based selector will choose the mapping with the smaller $J_i$ under the possibly biased estimates.\n\nUsing only the foundational definitions of Pareto dominance and normalized weighted-sum scalarization, and the data given above, derive the exact supremum value $\\gamma^{\\star}$ such that for all $\\gamma \\in (0, \\gamma^{\\star})$, the scalarized selector (operating on the biased estimates) erroneously prefers $M_1$ even though $M_1$ is dominated by $M_2$ in the true objectives. Provide your final answer as the exact value of $\\gamma^{\\star}$ with no rounding.",
            "solution": "The problem asks for the supremum value of a bias factor, $\\gamma^{\\star}$, for which a scalarization-based selection algorithm erroneously prefers a mapping $M_1$ over another mapping $M_2$, despite $M_2$ being Pareto-superior to $M_1$.\n\nFirst, we validate the premise of Pareto dominance. The true objective vectors for the mappings $M_1$ and $M_2$ are given as:\n$M_1$: $(E_1, L_1, A_1, C_1) = (1.20, 7, 16, 9)$\n$M_2$: $(E_2, L_2, A_2, C_2) = (1.00, 6, 14, 8)$\nThe units are $\\mathrm{mJ}$ for energy $E$, $\\mathrm{ms}$ for latency $L$, $\\mathrm{mm}^2$ for area $A$, and $\\mathrm{MB}$ for communication $C$. All four objectives are to be minimized.\nA solution $M_2$ strictly dominates a solution $M_1$ if $M_2$ is at least as good as $M_1$ in all objectives and strictly better in at least one objective. Since we are minimizing, 'better' means having a smaller value.\nComparing the objectives:\n$E_2 = 1.00 < E_1 = 1.20$\n$L_2 = 6 < L_1 = 7$\n$A_2 = 14 < A_1 = 16$\n$C_2 = 8 < C_1 = 9$\nSince all objectives for $M_2$ have smaller values than their counterparts for $M_1$, $M_2$ strictly dominates $M_1$. A Pareto-based method would correctly identify $M_2$ as the superior solution and discard $M_1$.\n\nNext, we analyze the scalarization-based selector. The score for a mapping $M_i$ is given by the weighted sum of its normalized estimated objectives:\n$$J_i = w_E \\frac{\\widehat{E}_i}{s_E} + w_L \\frac{\\widehat{L}_i}{s_L} + w_A \\frac{\\widehat{A}_i}{s_A} + w_C \\frac{\\widehat{C}_i}{s_C}$$\nThe normalization scales are $(s_E, s_L, s_A, s_C) = (1, 10, 20, 10)$ and the weights are $(w_E, w_L, w_A, w_C) = (0.4, 0.2, 0.2, 0.2)$.\nThe estimated metrics $\\widehat{(\\cdot)}$ are given as:\n$\\widehat{E}_i = E_i$, $\\widehat{L}_i = L_i$, $\\widehat{A}_i = A_i$ for $i \\in \\{1,2\\}$.\n$\\widehat{C}_1 = \\gamma C_1$ and $\\widehat{C}_2 = C_2$, where $\\gamma \\in (0,1)$.\n\nThe scalarized selector erroneously prefers $M_1$ over $M_2$ if the calculated score for $M_1$ is less than the score for $M_2$, i.e., $J_1 < J_2$. We must find the supremum value $\\gamma^{\\star}$ such that this inequality holds for all $\\gamma \\in (0, \\gamma^{\\star})$.\n\nLet's write the expressions for $J_1$ and $J_2$ using the given values and the biased estimate for $C_1$:\nFor $M_1$:\n$J_1 = w_E \\frac{E_1}{s_E} + w_L \\frac{L_1}{s_L} + w_A \\frac{A_1}{s_A} + w_C \\frac{\\gamma C_1}{s_C}$\nSubstituting the values:\n$J_1 = (0.4) \\frac{1.20}{1} + (0.2) \\frac{7}{10} + (0.2) \\frac{16}{20} + (0.2) \\frac{\\gamma \\cdot 9}{10}$\n$J_1 = (0.4)(1.20) + (0.2)(0.7) + (0.2)(0.8) + (0.2)(0.9)\\gamma$\n$J_1 = 0.48 + 0.14 + 0.16 + 0.18\\gamma$\n$J_1 = 0.78 + 0.18\\gamma$\n\nFor $M_2$:\n$J_2 = w_E \\frac{E_2}{s_E} + w_L \\frac{L_2}{s_L} + w_A \\frac{A_2}{s_A} + w_C \\frac{C_2}{s_C}$\nSubstituting the values:\n$J_2 = (0.4) \\frac{1.00}{1} + (0.2) \\frac{6}{10} + (0.2) \\frac{14}{20} + (0.2) \\frac{8}{10}$\n$J_2 = (0.4)(1.00) + (0.2)(0.6) + (0.2)(0.7) + (0.2)(0.8)$\n$J_2 = 0.40 + 0.12 + 0.14 + 0.16$\n$J_2 = 0.82$\n\nThe condition for erroneously preferring $M_1$ is $J_1 < J_2$. We set up and solve this inequality for $\\gamma$:\n$0.78 + 0.18\\gamma < 0.82$\n$0.18\\gamma < 0.82 - 0.78$\n$0.18\\gamma < 0.04$\n$\\gamma < \\frac{0.04}{0.18}$\n$\\gamma < \\frac{4}{18}$\n$\\gamma < \\frac{2}{9}$\n\nThe problem states that this erroneous preference for $M_1$ occurs for all $\\gamma \\in (0, \\gamma^{\\star})$. The inequality we derived, $\\gamma < 2/9$, defines the range of $\\gamma$ for which $J_1 < J_2$. Since the problem specifies $\\gamma \\in (0,1)$ and $2/9 < 1$, the interval of interest is $\\gamma \\in (0, 2/9)$.\n\nThe question asks for the supremum of this interval. The supremum of the set $\\{\\gamma \\in \\mathbb{R} \\mid 0 < \\gamma < 2/9\\}$ is the least upper bound of the set, which is $2/9$.\nTherefore, the supremum value is $\\gamma^{\\star} = 2/9$. At $\\gamma = 2/9$, the scores would be equal ($J_1=J_2$), and for any value of $\\gamma$ smaller than this, the biased cost model will lead to the incorrect selection of $M_1$.\nThe final answer must be the exact value.",
            "answer": "$$\\boxed{\\frac{2}{9}}$$"
        }
    ]
}