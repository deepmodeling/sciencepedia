{
    "hands_on_practices": [
        {
            "introduction": "To truly benchmark a neuromorphic system, we must connect high-level performance metrics to the underlying physical operations. This practice builds this connection from first principles by quantifying the total energy of an inference based on the discrete energy cost of individual synaptic events. By working through this calculation , you will not only compute a key efficiency metric but also develop a conceptual understanding of how a network's architectural properties, such as fan-in and fan-out, directly determine its computational workload and resulting energy consumption.",
            "id": "4036973",
            "problem": "A neuromorphic accelerator executes inference for a Spiking Neural Network (SNN) using event-driven synaptic operations. Consider the following measurement setup and assumptions:\n\n- Each synaptic event incurs a dynamic switching energy of $E_{\\text{syn}} = 10\\,\\text{pJ/event}$ determined by on-chip measurements under nominal voltage and temperature.\n- The average synaptic activity per inference across a representative dataset is $5 \\times 10^{6}$ events, measured after the network has converged and with the same input distribution used for accuracy benchmarking.\n- You may neglect leakage (static) energy, neuron update energy, and communication overheads; consider only dynamic synaptic energy from synaptic events.\n\nStarting only from the fundamental definitions of energy per discrete switching event and the notion of counting events, derive an expression for the dynamic synaptic energy per inference and compute its value. Express your final energy in joules using scientific notation and round your answer to three significant figures.\n\nThen, justify from first principles how neuron fan-in and fan-out determine the synaptic event count under fixed spike statistics and how this propagates to the energy and latency metrics while holding functional accuracy constant. Your discussion should be grounded in definitions (e.g., events as edge-triggered synaptic operations) and resource constraints (e.g., sustained synaptic processing rate), without invoking any unprovided shortcut formulas.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All necessary data are provided, and the task is to perform a calculation and provide a conceptual justification based on first principles, which is a valid scientific problem. The model presented is a standard and simplified but physically sound representation of energy consumption in event-driven neuromorphic hardware.\n\n**Part 1: Derivation and Calculation of Dynamic Synaptic Energy**\n\nThe problem asks for the dynamic synaptic energy per inference. This energy is stipulated to arise exclusively from discrete synaptic events. The fundamental principle for calculating the total energy of a collection of discrete, independent processes is the principle of superposition. The total energy is the sum of the energies of each individual process.\n\nLet $E_{\\text{total}}$ be the total dynamic synaptic energy per inference.\nLet $N_{\\text{events}}$ be the total number of synaptic events per inference.\nLet $E_{\\text{event}}$ be the energy consumed per single synaptic event.\n\nAssuming that each synaptic event is identical in its energy consumption, the total energy is the product of the number of events and the energy per event.\n$$E_{\\text{total}} = N_{\\text{events}} \\times E_{\\text{event}}$$\n\nFrom the problem statement, we are given:\nThe average synaptic activity per inference, which corresponds to the number of events:\n$$N_{\\text{events}} = 5 \\times 10^{6} \\text{ events}$$\nThe dynamic switching energy per synaptic event:\n$$E_{\\text{event}} = E_{\\text{syn}} = 10 \\text{ pJ/event}$$\n\nWe must first convert the energy per event into the base SI unit of joules ($J$). The prefix 'pico' (p) corresponds to a factor of $10^{-12}$.\n$$E_{\\text{syn}} = 10 \\times 10^{-12} \\frac{\\text{J}}{\\text{event}}$$\n\nNow, we can substitute the given values into our derived expression for the total energy:\n$$E_{\\text{total}} = (5 \\times 10^{6} \\text{ events}) \\times \\left(10 \\times 10^{-12} \\frac{\\text{J}}{\\text{event}}\\right)$$\n\nPerforming the multiplication:\n$$E_{\\text{total}} = (5 \\times 10) \\times (10^{6} \\times 10^{-12}) \\text{ J}$$\n$$E_{\\text{total}} = 50 \\times 10^{-6} \\text{ J}$$\n\nTo express this in standard scientific notation ($a \\times 10^b$ where $1 \\le |a| < 10$), we adjust the mantissa and exponent:\n$$E_{\\text{total}} = 5.0 \\times 10^{1} \\times 10^{-6} \\text{ J}$$\n$$E_{\\text{total}} = 5.0 \\times 10^{-5} \\text{ J}$$\n\nThe problem requires the answer to be rounded to three significant figures.\n$$E_{\\text{total}} = 5.00 \\times 10^{-5} \\text{ J}$$\n\n**Part 2: Justification of the Role of Fan-in and Fan-out**\n\nWe are asked to justify from first principles how neuron fan-in and fan-out affect the synaptic event count, and consequently, energy and latency, while holding accuracy constant.\n\n**First Principles and Definitions:**\n1.  A **Spiking Neural Network (SNN)** consists of neurons connected by synapses.\n2.  A **spike** is a discrete event in time, representing a signal from a pre-synaptic neuron.\n3.  A **synaptic event** is the fundamental operation triggered by a pre-synaptic spike arriving at a synapse. This operation involves retrieving the synaptic weight and updating the state of the post-synaptic neuron. The total number of such events during an inference is what we denote as the synaptic event count, $S$.\n4.  **Fan-out** ($N_{\\text{out}}$) of a neuron is the number of post-synaptic neurons it projects to. It represents the number of synapses that are sourced by this single neuron.\n5.  **Fan-in** ($N_{\\text{in}}$) of a neuron is the number of pre-synaptic neurons that project to it. It represents the number of synapses that target this single neuron.\n\n**Relationship between Fan-out and Synaptic Event Count ($S$):**\nConsider a single pre-synaptic neuron, indexed by $i$. When neuron $i$ generates a single spike, this spike is propagated to all its downstream targets. By definition, the number of these targets is its fan-out, $N_{\\text{out},i}$. Therefore, one spike from neuron $i$ generates exactly $N_{\\text{out},i}$ synaptic events.\n\nLet $k_i$ be the number of times neuron $i$ spikes during a single inference. The total number of synaptic events generated by neuron $i$ is the product $k_i \\times N_{\\text{out},i}$.\nThe total synaptic event count for the entire network, $S$, is the sum of events generated by all neurons in the network:\n$$S = \\sum_{i \\in \\text{neurons}} k_i \\cdot N_{\\text{out},i}$$\n\nThe problem states we must consider \"fixed spike statistics\". This is a crucial constraint, implying that the firing pattern, and thus the set of values $\\{k_i\\}$, is held constant across the networks being compared. Under this assumption, the equation shows that $S$ is a weighted sum of the fan-outs of all neurons. If the average fan-out, $\\bar{N}_{\\text{out}}$, increases, the total synaptic event count $S$ will increase proportionally, assuming the distribution of spike counts remains the same. The same logic applies to fan-in, as the sum of all fan-ins across a network equals the sum of all fan-outs (i.e., $\\sum N_{\\text{in},j} = \\sum N_{\\text{out},i}$), both being equal to the total number of synapses. Thus, for a fixed number of neurons and fixed spike statistics, the total synaptic event count $S$ is directly proportional to the average network connectivity, which is characterized by fan-in and fan-out.\n\n**Propagation to Energy:**\nAs established in Part 1, the total dynamic synaptic energy ($E_{\\text{total}}$) is directly proportional to the total synaptic event count ($S$):\n$$E_{\\text{total}} = S \\times E_{\\text{syn}}$$\nSince we have shown that $S$ is proportional to the average fan-in/fan-out (under the given constraints), it follows directly that the total energy is also proportional to the average fan-in/fan-out. A network with higher connectivity requires more synaptic operations for the same amount of neural activity, and thus consumes more dynamic energy.\n\n**Propagation to Latency:**\nLatency ($T_{\\text{latency}}$) is the time required to complete the inference task. A neuromorphic accelerator has a finite capacity for processing synaptic events, defined by its maximum sustained synaptic processing rate, $R_{\\text{syn}}$, measured in synaptic events per second (SEPS). This rate is a hardware resource constraint.\nThe minimum time required to process a total of $S$ synaptic events is therefore bounded by this rate:\n$$T_{\\text{latency}} \\ge \\frac{S}{R_{\\text{syn}}}$$\nThis lower bound represents the computational latency. Since $S$ is proportional to the average fan-in/fan-out, the minimum latency is also proportional to the average fan-in/fan-out:\n$$T_{\\text{latency}} \\propto S \\propto \\text{average fan-in/fan-out}$$\nTherefore, increasing the network connectivity (fan-in/fan-out) increases the total computational workload ($S$), which, for a processor with a fixed throughput ($R_{\\text{syn}}$), results in a longer processing time, i.e., higher latency.\n\n**Constraint of Constant Accuracy:**\nThe condition \"while holding functional accuracy constant\" is non-trivial. Fan-in and fan-out are fundamental properties of the network's topology. Altering them changes the model. To maintain accuracy, a network with a different connectivity would likely need to be re-trained, which could in turn alter the learned spike statistics $\\{k_i\\}$. The assumption of \"fixed spike statistics\" is therefore a strong idealization used to isolate the direct impact of connectivity on workload. In this idealized scenario, where two networks with different connectivities can achieve the same accuracy with identical firing patterns, the one with higher fan-in/fan-out will inherently have a higher synaptic event count, leading to greater energy consumption and latency.",
            "answer": "$$\\boxed{5.00 \\times 10^{-5}}$$"
        },
        {
            "introduction": "Optimizing a neuromorphic system is rarely about maximizing a single metric; it is about navigating a complex space of trade-offs. A configuration that achieves the lowest latency might do so at an unacceptable energy cost, and vice versa. This exercise  introduces a standard method for evaluating this balance by using a composite metric, the Energy-Delay Product ($EDP$). By calculating and comparing the $EDP$ across several distinct system configurations, you will learn how to make a holistic and quantitatively justified choice for the most efficient overall design.",
            "id": "4036901",
            "problem": "A neuromorphic spiking classifier is evaluated under five hardware-software configurations, indexed by $i=1,2,3,4,5$. For each configuration $i$, the per-inference energy $E_i$ (in Joules), per-inference latency $L_i$ (in seconds), and top-$1$ classification accuracy $A_i$ (as a decimal fraction) are measured on the same test set. The standard benchmarking practice in neuromorphic and brain-inspired computing uses composite metrics that combine energy and latency to capture energy-time trade-offs. Using the standard definition of the Energy-Delay Product (EDP), compute the EDP for each configuration from first principles and determine the configuration index $i$ that minimizes EDP. Then, report the corresponding accuracy $A_i$ for that configuration.\n\nThe measured values are:\n- Configuration $1$: $E_1=3.2\\times10^{-5}$, $L_1=8.0\\times10^{-3}$, $A_1=0.926$.\n- Configuration $2$: $E_2=2.5\\times10^{-5}$, $L_2=1.2\\times10^{-2}$, $A_2=0.913$.\n- Configuration $3$: $E_3=1.8\\times10^{-5}$, $L_3=9.0\\times10^{-3}$, $A_3=0.901$.\n- Configuration $4$: $E_4=1.6\\times10^{-5}$, $L_4=1.7\\times10^{-2}$, $A_4=0.934$.\n- Configuration $5$: $E_5=1.2\\times10^{-5}$, $L_5=1.1\\times10^{-2}$, $A_5=0.889$.\n\nWork from core definitions for energy and latency to justify the EDP calculation, and ensure scientific consistency in units. Express the final answer as a row matrix containing two entries: the minimizing configuration index $i$ and its accuracy $A_i$. Round the reported accuracy to four significant figures, and include no physical units in the final answer.",
            "solution": "The problem statement has been validated and is determined to be sound. It is scientifically grounded, well-posed, objective, complete, and contains no internal contradictions or unrealistic data. The task is a standard calculation in the field of computer architecture and neuromorphic engineering.\n\nThe problem requires the calculation of the Energy-Delay Product (EDP) for five distinct hardware-software configurations of a neuromorphic spiking classifier. The goal is to identify the configuration that yields the minimum EDP and report its corresponding classification accuracy.\n\nFirst, we define the fundamental quantities involved.\nEnergy, denoted by $E$, is the work done or heat produced by a system. In this context, it is the total energy consumed by the neuromorphic hardware to perform a single classification task (one inference). Its SI unit is the Joule ($J$).\nLatency, denoted by $L$, is the time delay between the initiation and completion of a process. Here, it represents the time taken to perform a single inference. Its SI unit is the second ($s$).\n\nThe Energy-Delay Product (EDP) is a figure of merit used to evaluate the efficiency of a computational system, capturing the trade-off between performance (low latency) and energy consumption. A lower EDP signifies a more efficient design. From first principles, the EDP for a given configuration $i$ is defined as the product of the energy consumed per operation and the latency of that operation.\nThe formula is:\n$$\n\\text{EDP}_i = E_i \\times L_i\n$$\nThe resulting unit for EDP is Joule-second ($J \\cdot s$). We will now compute the EDP for each of the five given configurations.\n\nThe provided data are:\n- Configuration $1$: $E_1 = 3.2 \\times 10^{-5} \\, J$, $L_1 = 8.0 \\times 10^{-3} \\, s$, $A_1 = 0.926$.\n- Configuration $2$: $E_2 = 2.5 \\times 10^{-5} \\, J$, $L_2 = 1.2 \\times 10^{-2} \\, s$, $A_2 = 0.913$.\n- Configuration $3$: $E_3 = 1.8 \\times 10^{-5} \\, J$, $L_3 = 9.0 \\times 10^{-3} \\, s$, $A_3 = 0.901$.\n- Configuration $4$: $E_4 = 1.6 \\times 10^{-5} \\, J$, $L_4 = 1.7 \\times 10^{-2} \\, s$, $A_4 = 0.934$.\n- Configuration $5$: $E_5 = 1.2 \\times 10^{-5} \\, J$, $L_5 = 1.1 \\times 10^{-2} \\, s$, $A_5 = 0.889$.\n\nThe EDP for each configuration is calculated as follows:\n\nFor configuration $i=1$:\n$$\n\\text{EDP}_1 = E_1 \\times L_1 = (3.2 \\times 10^{-5}) \\times (8.0 \\times 10^{-3}) = 25.6 \\times 10^{-8} = 2.56 \\times 10^{-7} \\, J \\cdot s\n$$\n\nFor configuration $i=2$:\n$$\n\\text{EDP}_2 = E_2 \\times L_2 = (2.5 \\times 10^{-5}) \\times (1.2 \\times 10^{-2}) = 3.0 \\times 10^{-7} \\, J \\cdot s\n$$\n\nFor configuration $i=3$:\n$$\n\\text{EDP}_3 = E_3 \\times L_3 = (1.8 \\times 10^{-5}) \\times (9.0 \\times 10^{-3}) = 16.2 \\times 10^{-8} = 1.62 \\times 10^{-7} \\, J \\cdot s\n$$\n\nFor configuration $i=4$:\n$$\n\\text{EDP}_4 = E_4 \\times L_4 = (1.6 \\times 10^{-5}) \\times (1.7 \\times 10^{-2}) = 2.72 \\times 10^{-7} \\, J \\cdot s\n$$\n\nFor configuration $i=5$:\n$$\n\\text{EDP}_5 = E_5 \\times L_5 = (1.2 \\times 10^{-5}) \\times (1.1 \\times 10^{-2}) = 1.32 \\times 10^{-7} \\, J \\cdot s\n$$\n\nTo determine the optimal configuration, we compare the calculated EDP values:\n- $\\text{EDP}_1 = 2.56 \\times 10^{-7}$\n- $\\text{EDP}_2 = 3.00 \\times 10^{-7}$\n- $\\text{EDP}_3 = 1.62 \\times 10^{-7}$\n- $\\text{EDP}_4 = 2.72 \\times 10^{-7}$\n- $\\text{EDP}_5 = 1.32 \\times 10^{-7}$\n\nThe minimum value among these is $\\text{EDP}_5 = 1.32 \\times 10^{-7} \\, J \\cdot s$. This corresponds to the configuration with index $i=5$.\n\nThe problem requires reporting the configuration index $i$ that minimizes the EDP and its corresponding accuracy $A_i$. The minimizing index is $i=5$. The accuracy for this configuration is $A_5 = 0.889$. The problem specifies that the accuracy should be rounded to four significant figures. Therefore, $A_5 = 0.8890$.\n\nThe final answer comprises the minimizing configuration index, $5$, and its corresponding accuracy, $0.8890$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n5 & 0.8890\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A high overall accuracy can be a deceptive metric, often masking poor performance on the very cases that matter most. This is especially true in real-world scenarios where data is imbalanced, with some classes appearing far more frequently than others. This practice  provides a critical lesson in performance evaluation by demonstrating how a seemingly good accuracy score can hide a model's complete failure on minority classes. By computing and contrasting overall accuracy with the macro-averaged $F_1$ score, you will learn why robust, class-sensitive metrics are essential for a faithful and reliable assessment of a system's capabilities.",
            "id": "4036959",
            "problem": "A spiking neural network (SNN) deployed on a neuromorphic substrate is evaluated on a multiclass classification task with pronounced class imbalance. The test set contains $10{,}000$ labeled samples across $5$ classes. The evaluation produces the following confusion matrix $C \\in \\mathbb{N}^{5 \\times 5}$, where rows index the true class and columns index the predicted class:\n$$\nC \\;=\\; \\begin{pmatrix}\n7400 & 200 & 150 & 150 & 100 \\\\\n500 & 150 & 80 & 40 & 30 \\\\\n420 & 100 & 120 & 40 & 20 \\\\\n200 & 30 & 30 & 25 & 15 \\\\\n140 & 20 & 20 & 10 & 10\n\\end{pmatrix}.\n$$\nUsing core definitions in multiclass classification, compute the overall accuracy and the macro-averaged $F_1$ score. Treat per-class precision as the fraction of correctly predicted samples of that class among all samples predicted as that class, per-class recall as the fraction of correctly predicted samples of that class among all true samples of that class, and per-class $F_1$ as the harmonic mean of its precision and recall. The macro-averaged $F_1$ is the unweighted mean of the per-class $F_1$ values across the $5$ classes. Express both metrics as unitless decimals, and round both to four significant figures. Additionally, based on first principles of these definitions and the presented data, explain why overall accuracy alone may be misleading under class imbalance in neuromorphic evaluation scenarios, even when latency and energy per inference appear acceptable. The final numerical answer must contain only the two requested metrics.",
            "solution": "The problem requires the calculation of two classification metrics—overall accuracy and macro-averaged $F_1$ score—from a given confusion matrix, and an explanation of the potential for the accuracy metric to be misleading in the context of class imbalance. The problem is well-posed and scientifically grounded.\n\nFirst, we establish the necessary components from the provided confusion matrix $C$:\n$$\nC \\;=\\; \\begin{pmatrix}\n7400 & 200 & 150 & 150 & 100 \\\\\n500 & 150 & 80 & 40 & 30 \\\\\n420 & 100 & 120 & 40 & 20 \\\\\n200 & 30 & 30 & 25 & 15 \\\\\n140 & 20 & 20 & 10 & 10\n\\end{pmatrix}\n$$\nThe element $C_{ij}$ represents the number of samples from true class $i$ that were predicted as class $j$. The total number of samples $N$ is the sum of all elements in the matrix:\n$N = \\sum_{i=1}^{5} \\sum_{j=1}^{5} C_{ij} = (7400+200+150+150+100) + (500+150+80+40+30) + (420+100+120+40+20) + (200+30+30+25+15) + (140+20+20+10+10) = 8000 + 800 + 700 + 300 + 200 = 10000$.\nThis confirms the given total test set size of $10,000$ samples.\n\n**Calculation of Overall Accuracy**\n\nThe overall accuracy is the ratio of the number of correctly classified samples to the total number of samples. The correctly classified samples correspond to the diagonal elements of the confusion matrix (the trace of $C$).\n$$\n\\text{Accuracy} = \\frac{\\sum_{i=1}^{5} C_{ii}}{N} = \\frac{\\text{Tr}(C)}{N}\n$$\nThe sum of the diagonal elements is:\n$$\n\\text{Tr}(C) = 7400 + 150 + 120 + 25 + 10 = 7705\n$$\nTherefore, the overall accuracy is:\n$$\n\\text{Accuracy} = \\frac{7705}{10000} = 0.7705\n$$\nThis value is already expressed to four significant figures.\n\n**Calculation of Macro-Averaged $F_1$ Score**\n\nTo compute the macro-averaged $F_1$ score, we must first compute the precision ($P_i$), recall ($R_i$), and $F_1$ score ($F_{1,i}$) for each of the $k=5$ classes.\n\nFor each class $i$, we define:\n- True Positives ($TP_i$): $C_{ii}$\n- False Positives ($FP_i$): $\\sum_{j=1, j\\neq i}^{k} C_{ji}$ (sum of column $i$, excluding the diagonal)\n- False Negatives ($FN_i$): $\\sum_{j=1, j\\neq i}^{k} C_{ij}$ (sum of row $i$, excluding the diagonal)\n\nPrecision for class $i$ is $P_i = \\frac{TP_i}{TP_i + FP_i}$, which is the number of correct predictions for class $i$ divided by the total number of predictions for class $i$.\nRecall for class $i$ is $R_i = \\frac{TP_i}{TP_i + FN_i}$, which is the number of correct predictions for class $i$ divided by the total number of actual samples in class $i$.\nThe $F_1$ score for class $i$ is the harmonic mean of its precision and recall: $F_{1,i} = 2 \\frac{P_i R_i}{P_i + R_i}$.\n\nLet's compute these values for each class:\nThe row sums (total actual samples per class) are: $S_{R,1}=8000$, $S_{R,2}=800$, $S_{R,3}=700$, $S_{R,4}=300$, $S_{R,5}=200$.\nThe column sums (total predicted samples per class) are:\n$S_{C,1}=7400+500+420+200+140 = 8660$\n$S_{C,2}=200+150+100+30+20 = 500$\n$S_{C,3}=150+80+120+30+20 = 400$\n$S_{C,4}=150+40+40+25+10 = 265$\n$S_{C,5}=100+30+20+15+10 = 175$\n\n- **Class 1:**\n  $TP_1 = 7400$\n  $P_1 = \\frac{7400}{8660} = \\frac{370}{433}$\n  $R_1 = \\frac{7400}{8000} = \\frac{37}{40}$\n  $F_{1,1} = 2 \\frac{\\frac{370}{433} \\cdot \\frac{37}{40}}{\\frac{370}{433} + \\frac{37}{40}} = 2 \\frac{370 \\cdot 37}{370 \\cdot 40 + 37 \\cdot 433} = 2 \\frac{13690}{14800 + 16021} = \\frac{27380}{30821} = \\frac{740}{833} \\approx 0.888355...$\n\n- **Class 2:**\n  $TP_2 = 150$\n  $P_2 = \\frac{150}{500} = \\frac{3}{10}$\n  $R_2 = \\frac{150}{800} = \\frac{3}{16}$\n  $F_{1,2} = 2 \\frac{\\frac{3}{10} \\cdot \\frac{3}{16}}{\\frac{3}{10} + \\frac{3}{16}} = 2 \\frac{\\frac{9}{160}}{\\frac{48+30}{160}} = \\frac{18}{78} = \\frac{3}{13} \\approx 0.230769...$\n\n- **Class 3:**\n  $TP_3 = 120$\n  $P_3 = \\frac{120}{400} = \\frac{3}{10}$\n  $R_3 = \\frac{120}{700} = \\frac{6}{35}$\n  $F_{1,3} = 2 \\frac{\\frac{3}{10} \\cdot \\frac{6}{35}}{\\frac{3}{10} + \\frac{6}{35}} = 2 \\frac{\\frac{18}{350}}{\\frac{21+12}{70}} = 2 \\frac{18}{350} \\frac{70}{33} = \\frac{36}{5 \\cdot 33} = \\frac{12}{55} \\approx 0.218181...$\n\n- **Class 4:**\n  $TP_4 = 25$\n  $P_4 = \\frac{25}{265} = \\frac{5}{53}$\n  $R_4 = \\frac{25}{300} = \\frac{1}{12}$\n  $F_{1,4} = 2 \\frac{\\frac{5}{53} \\cdot \\frac{1}{12}}{\\frac{5}{53} + \\frac{1}{12}} = 2 \\frac{\\frac{5}{636}}{\\frac{60+53}{636}} = \\frac{10}{113} \\approx 0.088495...$\n\n- **Class 5:**\n  $TP_5 = 10$\n  $P_5 = \\frac{10}{175} = \\frac{2}{35}$\n  $R_5 = \\frac{10}{200} = \\frac{1}{20}$\n  $F_{1,5} = 2 \\frac{\\frac{2}{35} \\cdot \\frac{1}{20}}{\\frac{2}{35} + \\frac{1}{20}} = 2 \\frac{\\frac{2}{700}}{\\frac{40+35}{700}} = \\frac{4}{75} \\approx 0.053333...$\n\nThe macro-averaged $F_1$ score is the unweighted arithmetic mean of the per-class $F_1$ scores:\n$$\nF_{1,\\text{macro}} = \\frac{1}{5} \\sum_{i=1}^{5} F_{1,i} = \\frac{1}{5} \\left( \\frac{740}{833} + \\frac{3}{13} + \\frac{12}{55} + \\frac{10}{113} + \\frac{4}{75}\\right)\n$$\n$$\nF_{1,\\text{macro}} \\approx \\frac{1}{5} (0.888355... + 0.230769... + 0.218181... + 0.088495... + 0.053333...)\n$$\n$$\nF_{1,\\text{macro}} \\approx \\frac{1}{5} (1.479135...) \\approx 0.295827...\n$$\nRounding to four significant figures, the macro-averaged $F_1$ score is $0.2958$.\n\n**Explanation of Misleading Accuracy under Class Imbalance**\n\nThe calculated metrics present a stark contrast: the overall accuracy is a seemingly reasonable $0.7705$, while the macro-averaged $F_1$ score is a very poor $0.2958$. This discrepancy arises directly from the severe class imbalance in the test set and the definitions of the metrics.\n\nThe total number of samples is $10,000$, with the class distribution being $8000$, $800$, $700$, $300$, and $200$ samples for classes $1$ through $5$, respectively. Class $1$ constitutes $80\\%$ of the entire dataset.\n\nOverall accuracy is a weighted average of per-class performance, where the weights are the class sizes. The high number of true positives for the majority class ($TP_1 = 7400$) dominates the numerator of the accuracy calculation ($\\sum TP_i = 7705$). Consequently, the overall accuracy is heavily skewed by the model's performance on this single, oversized class, giving a deceptively optimistic view of the model's capabilities. A trivial model that predicts \"class 1\" for every sample would still achieve an accuracy of $8000/10000 = 0.80$, which is higher than the SNN's performance, but would be useless in practice.\n\nThe macro-averaged $F_1$ score, in contrast, computes the $F_1$ score for each class independently and then takes the unweighted average. This gives equal importance to each class, regardless of its prevalence. The per-class $F_1$ scores for the four minority classes ($0.2308$, $0.2182$, $0.0885$, $0.0533$) are extremely low. This reveals that the SNN model has almost no practical ability to correctly identify samples from these classes. The low macro-F1 score of $0.2958$ accurately reflects this poor performance across the minority classes, which the overall accuracy metric completely masks.\n\nIn neuromorphic evaluation, focusing only on efficiency metrics like low latency and energy per inference alongside a high overall accuracy would be profoundly misleading. The data shows that the SNN could be achieving its efficiency simply by learning a simplistic strategy biased toward the majority class. The model is not solving the intended classification problem but rather a biased, simpler version of it. Therefore, for a scientifically valid assessment of a neuromorphic system on an imbalanced task, it is critical to use metrics like macro-averaged F1-score, which are sensitive to the performance on minority classes, to ensure that the reported efficiency is not a byproduct of a functionally useless model.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.7705 & 0.2958 \\end{pmatrix}}\n$$"
        }
    ]
}