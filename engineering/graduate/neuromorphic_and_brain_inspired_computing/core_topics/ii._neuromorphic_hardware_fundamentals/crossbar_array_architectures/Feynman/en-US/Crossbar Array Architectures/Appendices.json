{
    "hands_on_practices": [
        {
            "introduction": "In an ideal crossbar array, reading a single memristive cell would only involve current passing through that specific device. However, the interconnected nature of the grid creates parasitic \"sneak paths\" through unselected cells, which can corrupt the measurement and increase power consumption. This practice delves into a common biasing scheme designed to mitigate this issue, challenging you to apply circuit analysis principles to quantify the total current drawn, including these sneak path components, and understand its dependence on array size .",
            "id": "112899",
            "problem": "A memristor crossbar array is a promising architecture for high-density, non-volatile memory and neuromorphic computing systems. It consists of a grid of perpendicular conducting wires, with a two-terminal memristive device at each intersection. In this problem, we consider an $N \\times N$ memristor crossbar array. The memristors can be switched between a low-resistance state (LRS), denoted by resistance $R_{ON}$, and a high-resistance state (HRS), denoted by resistance $R_{OFF}$.\n\nAssume that the memristor at the intersection of the first wordline ($WL_1$) and the first bitline ($BL_1$) is in the ON state ($R_{ON}$). All other $N^2 - 1$ memristors in the array are in the OFF state ($R_{OFF}$).\n\nTo read the state of the target cell (1,1), the following voltage-biasing scheme is applied:\n1.  The selected wordline, $WL_1$, is biased with a read voltage $V_{read}$.\n2.  The selected bitline, $BL_1$, is connected to ground (0 V).\n3.  All unselected wordlines ($WL_i$ for $i \\in \\{2, ..., N\\}$) are biased at $V_{read}/2$.\n4.  All unselected bitlines ($BL_j$ for $j \\in \\{2, ..., N\\}$) are biased at $V_{read}/2$.\n\nA major challenge in such arrays is the presence of \"sneak paths,\" where current flows through unselected OFF-state cells, potentially leading to read errors and increased power consumption.\n\nDerive a closed-form expression for the total current, $I_{total}$, supplied by the voltage source connected to the selected wordline $WL_1$. This current is the sum of the current through the target cell and all sneak path currents originating from $WL_1$. Your answer should be expressed in terms of $V_{read}$, $R_{ON}$, $R_{OFF}$, and $N$.",
            "solution": "1. The current through the selected cell (1,1) is\n$$\nI_{sel}=\\frac{V_{read}-0}{R_{ON}}=\\frac{V_{read}}{R_{ON}}.\n$$\n2. Each unselected bitline $BL_j$ ($j=2,\\dots,N$) is held at $V_{read}/2$, so the sneak‐path current from $WL_1$ through the OFF‐state memristor at $(1,j)$ is\n$$\nI_{s,j}=\\frac{V_{read}-\\tfrac{V_{read}}{2}}{R_{OFF}}\n=\\frac{V_{read}/2}{R_{OFF}}.\n$$\n3. There are $N-1$ such unselected bitlines, giving total sneak current\n$$\nI_{s}=(N-1)\\frac{V_{read}/2}{R_{OFF}}\n=\\frac{(N-1)V_{read}}{2\\,R_{OFF}}.\n$$\n4. Summing,\n$$\nI_{total}=I_{sel}+I_{s}\n=\\frac{V_{read}}{R_{ON}}+\\frac{(N-1)V_{read}}{2\\,R_{OFF}}\n=V_{read}\\Bigl(\\frac{1}{R_{ON}}+\\frac{N-1}{2\\,R_{OFF}}\\Bigr).\n$$",
            "answer": "$$\\boxed{V_{read}\\Bigl(\\frac{1}{R_{ON}}+\\frac{N-1}{2\\,R_{OFF}}\\Bigr)}$$"
        },
        {
            "introduction": "Beyond structural challenges, the physical behavior of devices and circuits introduces further non-idealities. This practice examines two fundamental and competing sources of error: device nonlinearity, which typically worsens with larger signal voltages, and thermal noise, whose relative impact grows as signal voltages shrink. You will derive the optimal read voltage $V_{\\mathrm{opt}}$ that balances these two effects to maximize the Signal-to-Noise-and-Distortion Ratio (SNDR), a crucial metric for analog signal integrity .",
            "id": "4042191",
            "problem": "Consider a resistive crossbar performing an analog dot product, where a column integrates currents from active rows under a read voltage $V$. Model the column as an effective conductance $G$ that captures the sum of the contributing device conductances at the chosen operating point. The device current-voltage characteristic around the operating point exhibits weak odd-order nonlinearity, modeled by the cubic expansion $I(V) \\approx G V \\left(1 + \\beta V^{2}\\right)$, where $\\beta$ is a small, real nonlinearity coefficient with units $\\mathrm{V}^{-2}$ and $|\\beta| V^{2} \\ll 1$. The ideal linear signal current is $I_{\\mathrm{sig}}(V) = G V$. The nonlinearity-induced distortion current is the deviation from linearity, $\\Delta I_{\\mathrm{nl}}(V) = I(V) - G V$.\n\nAssume the random current noise at the column is dominated by Johnson–Nyquist thermal noise of the effective conductance $G$ integrated over a measurement bandwidth $B$, with one-sided mean-square current noise $i_{\\mathrm{th}}^{2} = 4 k_{B} T G B$, where $k_{B}$ is the Boltzmann constant and $T$ is the absolute temperature. Neglect amplifier noise and any other noise sources. Treat the nonlinearity-induced distortion as an additive, uncorrelated error with root-mean-square magnitude equal to $|\\Delta I_{\\mathrm{nl}}(V)|$. Define the total error as the root-sum-square of the random noise and the distortion.\n\nUsing only the above modeling assumptions and well-tested laws (including Johnson–Nyquist noise), do the following:\n\n$1.$ Derive how the relative nonlinearity error $\\varepsilon_{\\mathrm{nl}}(V) \\equiv |\\Delta I_{\\mathrm{nl}}(V)|/I_{\\mathrm{sig}}(V)$ scales with $V$, and how the relative thermal noise $\\varepsilon_{\\mathrm{th}}(V) \\equiv i_{\\mathrm{th}}/I_{\\mathrm{sig}}(V)$ scales with $V$. Explain why operating at lower $V$ reduces nonlinearity error but increases relative noise.\n\n$2.$ Define the Signal-to-Noise-and-Distortion Ratio (SNDR) as $\\mathrm{SNDR}(V) \\equiv I_{\\mathrm{sig}}(V)/\\sqrt{i_{\\mathrm{th}}^{2} + \\left(\\Delta I_{\\mathrm{nl}}(V)\\right)^{2}}$. Derive the read voltage $V_{\\mathrm{opt}}$ that maximizes $\\mathrm{SNDR}(V)$ in closed form as a function of $k_{B}$, $T$, $B$, $G$, and $\\beta$.\n\nExpress your final answer for the optimal read voltage as a single closed-form analytic expression in volts. No numerical evaluation is required and no rounding is needed.",
            "solution": "We begin from the given small-signal current-voltage characteristic with weak cubic nonlinearity,\n$$\nI(V) \\approx G V \\left(1 + \\beta V^{2}\\right),\n$$\nwhere $G$ is the effective conductance of the column and $\\beta$ is the cubic nonlinearity coefficient with $|\\beta| V^{2} \\ll 1$. The ideal linear signal current is\n$$\nI_{\\mathrm{sig}}(V) = G V,\n$$\nand the nonlinearity-induced distortion current is\n$$\n\\Delta I_{\\mathrm{nl}}(V) = I(V) - G V = G \\beta V^{3}.\n$$\n\nFor the random current noise, we invoke Johnson–Nyquist thermal noise for a conductance, which is a well-tested principle stating that the one-sided current noise spectral density is $S_{i} = 4 k_{B} T G$ in units of $\\mathrm{A}^{2}/\\mathrm{Hz}$. Integrated over a measurement bandwidth $B$, the mean-square current noise is\n$$\ni_{\\mathrm{th}}^{2} = 4 k_{B} T G B,\n$$\nand the root-mean-square (RMS) thermal noise is $i_{\\mathrm{th}} = \\sqrt{4 k_{B} T G B}$.\n\n$1.$ Relative scalings versus $V$:\n\n- The relative nonlinearity error is\n$$\n\\varepsilon_{\\mathrm{nl}}(V) \\equiv \\frac{|\\Delta I_{\\mathrm{nl}}(V)|}{I_{\\mathrm{sig}}(V)} = \\frac{|G \\beta V^{3}|}{G V} = |\\beta| V^{2}.\n$$\nThus, $\\varepsilon_{\\mathrm{nl}}(V)$ scales as $V^{2}$ and decreases quadratically as $V$ is reduced.\n\n- The relative thermal noise is\n$$\n\\varepsilon_{\\mathrm{th}}(V) \\equiv \\frac{i_{\\mathrm{th}}}{I_{\\mathrm{sig}}(V)} = \\frac{\\sqrt{4 k_{B} T G B}}{G V} = \\sqrt{\\frac{4 k_{B} T B}{G}} \\cdot \\frac{1}{V}.\n$$\nThus, $\\varepsilon_{\\mathrm{th}}(V)$ scales as $V^{-1}$ and increases inversely as $V$ is reduced.\n\nThese scalings demonstrate the trade-off: decreasing $V$ reduces nonlinearity error (since $|\\beta| V^{2}$ shrinks) but increases the relative impact of thermal noise (since $1/V$ grows).\n\n$2.$ Maximizing the Signal-to-Noise-and-Distortion Ratio (SNDR):\n\nWe define the total error as the root-sum-square of the uncorrelated random noise and distortion:\n$$\ni_{\\mathrm{tot}}(V) = \\sqrt{i_{\\mathrm{th}}^{2} + \\left(\\Delta I_{\\mathrm{nl}}(V)\\right)^{2}} = \\sqrt{4 k_{B} T G B + \\left(G \\beta V^{3}\\right)^{2}}.\n$$\nThe Signal-to-Noise-and-Distortion Ratio (SNDR) is\n$$\n\\mathrm{SNDR}(V) \\equiv \\frac{I_{\\mathrm{sig}}(V)}{i_{\\mathrm{tot}}(V)} = \\frac{G V}{\\sqrt{4 k_{B} T G B + G^{2} \\beta^{2} V^{6}}}.\n$$\nMaximizing $\\mathrm{SNDR}(V)$ is equivalent to maximizing its square to avoid the square root:\n$$\nF(V) \\equiv \\mathrm{SNDR}(V)^{2} = \\frac{G^{2} V^{2}}{4 k_{B} T G B + G^{2} \\beta^{2} V^{6}}.\n$$\nLet\n$$\na \\equiv 4 k_{B} T G B, \\quad c \\equiv G^{2} \\beta^{2},\n$$\nso that\n$$\nF(V) = \\frac{G^{2} V^{2}}{a + c V^{6}}.\n$$\nWe differentiate with respect to $V$ and set the derivative to zero for an interior maximum. Using the quotient rule, with $N(V) = G^{2} V^{2}$ and $D(V) = a + c V^{6}$, we have\n$$\n\\frac{\\mathrm{d}F}{\\mathrm{d}V} = \\frac{N'(V) D(V) - N(V) D'(V)}{D(V)^{2}}.\n$$\nSetting the numerator to zero,\n$$\nN'(V) D(V) - N(V) D'(V) = 0.\n$$\nCompute derivatives:\n$$\nN'(V) = 2 G^{2} V, \\quad D'(V) = 6 c V^{5}.\n$$\nThus,\n$$\n2 G^{2} V \\left(a + c V^{6}\\right) - G^{2} V^{2} \\left(6 c V^{5}\\right) = 0.\n$$\nDivide both sides by $G^{2} V$ (valid for $V > 0$ in a read operation):\n$$\n2 \\left(a + c V^{6}\\right) - 6 c V^{6} = 0 \\quad \\Longrightarrow \\quad 2 a - 4 c V^{6} = 0.\n$$\nSolving for $V^{6}$ gives\n$$\nV^{6} = \\frac{a}{2 c} = \\frac{4 k_{B} T G B}{2 G^{2} \\beta^{2}} = \\frac{2 k_{B} T B}{G \\beta^{2}}.\n$$\nTaking the positive real sixth root (the physically relevant branch for a positive read voltage),\n$$\nV_{\\mathrm{opt}} = \\left(\\frac{2 k_{B} T B}{G \\beta^{2}}\\right)^{1/6}.\n$$\nThis $V_{\\mathrm{opt}}$ maximizes $\\mathrm{SNDR}(V)$ under the stated model. A second-derivative test confirms that this stationary point is a maximum because for $V \\to 0^{+}$, $F(V) \\to 0$, and for $V \\to \\infty$, $F(V) \\sim G^{2} V^{2}/(c V^{6}) \\to 0$, so the positive interior stationary point must be the global maximum.\n\nTherefore, the optimal read voltage that balances nonlinearity-induced distortion against thermal noise to maximize the Signal-to-Noise-and-Distortion Ratio is\n$$\nV_{\\mathrm{opt}} = \\left(\\frac{2 k_{B} T B}{G \\beta^{2}}\\right)^{1/6}.\n$$",
            "answer": "$$\\boxed{\\left(\\frac{2 k_{B} T B}{G \\beta^{2}}\\right)^{1/6}}$$"
        },
        {
            "introduction": "Neuromorphic systems operate at the interface of analog computation and digital control. In practice, this means both the input signals from Digital-to-Analog Converters (DACs) and the programmed device conductances have finite precision. This exercise guides you through a statistical analysis to quantify the mean-square error in the computed analog dot product arising from this quantization. By linking the number of available conductance levels to a target model accuracy, this problem provides a concrete example of co-designing hardware to meet system-level requirements .",
            "id": "4042173",
            "problem": "A resistive crossbar array implements an analog matrix–vector multiplication in each column: the column current is the weighted sum of input voltages and device conductances, modeled as $I = \\sum_{j=1}^{M} G_j V_j$, where $M$ is the number of rows, $G_j$ is the conductance of the $j$-th device, and $V_j$ is the input voltage applied at the $j$-th row. In a practical neuromorphic system, both the conductances and the input voltages are quantized. Device conductances $G_j$ are programmed to one of $2^{b_G}$ discrete levels over the range $[0, G_{\\max}]$, and input voltages $V_j$ are provided by a Digital-to-Analog Converter (DAC) with $b_V$ bits over the symmetric range $[-V_{\\max}, V_{\\max}]$. Assume the following foundational facts:\n- The quantized levels are uniformly spaced. Let the conductance quantization step be $\\Delta_G = \\frac{G_{\\max}}{2^{b_G}-1}$ and the voltage quantization step be $\\Delta_V = \\frac{2 V_{\\max}}{2^{b_V}-1}$.\n- Quantization error for a uniform mid-rise quantizer can be modeled as independent, zero-mean random variables uniformly distributed in $[-\\Delta_G/2, \\Delta_G/2]$ for conductance and $[-\\Delta_V/2, \\Delta_V/2]$ for voltage, with variances $\\sigma_G^2 = \\Delta_G^2/12$ and $\\sigma_V^2 = \\Delta_V^2/12$.\n- The random variables $\\{G_j\\}$ and $\\{V_j\\}$ are independent across $j$, with $G_j$ independently and identically distributed (i.i.d.) uniformly on $[0, G_{\\max}]$ and $V_j$ i.i.d. uniformly on $[-V_{\\max}, V_{\\max}]$.\n\nStarting only from the definitions above and the independence assumptions, do the following:\n1. Derive a symbolic expression for the expected mean-square quantization error in the column current, $\\mathbb{E}[(\\Delta I)^2]$, where $\\Delta I$ is the error in $I$ due to conductance and voltage quantization. Do not neglect second-order terms arising from the product of conductance and voltage quantization errors.\n2. Derive a symbolic expression for the relative mean-square error $\\rho = \\frac{\\mathbb{E}[(\\Delta I)^2]}{\\mathbb{E}[I^2]}$ in terms of $b_G$, $b_V$, $G_{\\max}$, and $V_{\\max}$, and the uniform-distribution second moments of $G_j$ and $V_j$.\n3. Using your derived expression, specialize to the following hardware and workload parameters: $M = 512$, $V_{\\max} = 0.2 \\text{ V}$, $G_{\\max} = 20 \\,\\mu\\text{S}$, and $b_V = 8$. Suppose the trained model’s accuracy is preserved if the relative mean-square error satisfies $\\rho \\leq \\alpha$, with $\\alpha = 1.0 \\times 10^{-4}$ expressed as a decimal. Determine the smallest integer number of conductance bits $b_G$ that meets this requirement. Express your final answer as an integer number of bits, without units.\n\nIf any numerical approximation is needed mid-derivation, carry it out only at the final step; keep all intermediate expressions symbolic. The final answer does not require rounding by significant figures because it is an integer.",
            "solution": "Let the true (unquantized) conductances and voltages be $G_j$ and $V_j$, respectively. The quantized values are $\\hat{G}_j = G_j + \\delta G_j$ and $\\hat{V}_j = V_j + \\delta V_j$, where $\\delta G_j$ and $\\delta V_j$ are the respective quantization errors.\n\nBased on the problem statement, we have the following statistical properties:\n- The signals $G_j$ are i.i.d. with a uniform distribution on $[0, G_{\\max}]$. Their mean and second moment are:\n$$ \\mathbb{E}[G_j] = \\frac{G_{\\max}}{2} $$\n$$ \\mathbb{E}[G_j^2] = \\int_{0}^{G_{\\max}} g^2 \\frac{1}{G_{\\max}} dg = \\frac{G_{\\max}^2}{3} $$\n- The signals $V_j$ are i.i.d. with a uniform distribution on $[-V_{\\max}, V_{\\max}]$. Their mean and second moment are:\n$$ \\mathbb{E}[V_j] = 0 $$\n$$ \\mathbb{E}[V_j^2] = \\int_{-V_{\\max}}^{V_{\\max}} v^2 \\frac{1}{2V_{\\max}} dv = \\frac{V_{\\max}^2}{3} $$\n- The quantization errors $\\delta G_j$ and $\\delta V_j$ are modeled as independent, zero-mean random variables. Thus, $\\mathbb{E}[\\delta G_j]=0$ and $\\mathbb{E}[\\delta V_j]=0$. They are also independent of the signals $G_j$ and $V_j$.\n- The variances of the quantization errors are given as $\\sigma_G^2 = \\mathbb{E}[(\\delta G_j)^2]$ and $\\sigma_V^2 = \\mathbb{E}[(\\delta V_j)^2]$. Specifically:\n$$ \\sigma_G^2 = \\frac{\\Delta_G^2}{12} = \\frac{1}{12} \\left( \\frac{G_{\\max}}{2^{b_G}-1} \\right)^2 = \\frac{G_{\\max}^2}{12(2^{b_G}-1)^2} $$\n$$ \\sigma_V^2 = \\frac{\\Delta_V^2}{12} = \\frac{1}{12} \\left( \\frac{2V_{\\max}}{2^{b_V}-1} \\right)^2 = \\frac{4V_{\\max}^2}{12(2^{b_V}-1)^2} = \\frac{V_{\\max}^2}{3(2^{b_V}-1)^2} $$\n\n### 1. Derivation of the Mean-Square Quantization Error $\\mathbb{E}[(\\Delta I)^2]$\n\nThe true column current is $I = \\sum_{j=1}^{M} G_j V_j$. The current computed with quantized values is $\\hat{I} = \\sum_{j=1}^{M} \\hat{G}_j \\hat{V}_j$. The error in the current, $\\Delta I$, is:\n$$ \\Delta I = \\hat{I} - I = \\sum_{j=1}^{M} (\\hat{G}_j \\hat{V}_j - G_j V_j) = \\sum_{j=1}^{M} \\left( (G_j + \\delta G_j)(V_j + \\delta V_j) - G_j V_j \\right) $$\n$$ \\Delta I = \\sum_{j=1}^{M} (G_j V_j + G_j \\delta V_j + V_j \\delta G_j + \\delta G_j \\delta V_j - G_j V_j) = \\sum_{j=1}^{M} (G_j \\delta V_j + V_j \\delta G_j + \\delta G_j \\delta V_j) $$\nLet $E_j = G_j \\delta V_j + V_j \\delta G_j + \\delta G_j \\delta V_j$. Then $\\Delta I = \\sum_{j=1}^{M} E_j$.\nThe mean-square error is $\\mathbb{E}[(\\Delta I)^2] = \\mathbb{E}[(\\sum_{j=1}^{M} E_j)^2]$.\n\nFirst, we find the expected value of $E_j$. Due to the independence of signals and errors, and the zero-mean property of errors:\n$$ \\mathbb{E}[E_j] = \\mathbb{E}[G_j \\delta V_j] + \\mathbb{E}[V_j \\delta G_j] + \\mathbb{E}[\\delta G_j \\delta V_j] $$\n$$ \\mathbb{E}[E_j] = \\mathbb{E}[G_j]\\mathbb{E}[\\delta V_j] + \\mathbb{E}[V_j]\\mathbb{E}[\\delta G_j] + \\mathbb{E}[\\delta G_j]\\mathbb{E}[\\delta V_j] = (\\frac{G_{\\max}}{2})(0) + (0)(0) + (0)(0) = 0 $$\nSince the terms $E_j$ are zero-mean and are independent for different indices $j$ (as all constituent variables are independent across $j$), the expectation of the cross-products $\\mathbb{E}[E_j E_k]$ for $j \\neq k$ is $\\mathbb{E}[E_j]\\mathbb{E}[E_k] = 0$.\nTherefore, the mean-square error simplifies:\n$$ \\mathbb{E}[(\\Delta I)^2] = \\mathbb{E}\\left[\\sum_{j=1}^{M} E_j^2 + \\sum_{j \\neq k} E_j E_k\\right] = \\sum_{j=1}^{M} \\mathbb{E}[E_j^2] $$\nSince the components are i.i.d., $\\mathbb{E}[E_j^2]$ is the same for all $j$.\n$$ \\mathbb{E}[(\\Delta I)^2] = M \\cdot \\mathbb{E}[E_j^2] $$\nNow we compute $\\mathbb{E}[E_j^2]$:\n$$ E_j^2 = (G_j \\delta V_j + V_j \\delta G_j + \\delta G_j \\delta V_j)^2 $$\n$$ E_j^2 = (G_j \\delta V_j)^2 + (V_j \\delta G_j)^2 + (\\delta G_j \\delta V_j)^2 + 2G_j V_j \\delta G_j \\delta V_j + 2G_j \\delta G_j (\\delta V_j)^2 + 2V_j \\delta V_j (\\delta G_j)^2 $$\nTaking the expectation of each term, and using independence and the zero-mean property of errors:\n$$ \\mathbb{E}[(G_j \\delta V_j)^2] = \\mathbb{E}[G_j^2] \\mathbb{E}[(\\delta V_j)^2] = \\mathbb{E}[G_j^2]\\sigma_V^2 $$\n$$ \\mathbb{E}[(V_j \\delta G_j)^2] = \\mathbb{E}[V_j^2] \\mathbb{E}[(\\delta G_j)^2] = \\mathbb{E}[V_j^2]\\sigma_G^2 $$\n$$ \\mathbb{E}[(\\delta G_j \\delta V_j)^2] = \\mathbb{E}[(\\delta G_j)^2] \\mathbb{E}[(\\delta V_j)^2] = \\sigma_G^2\\sigma_V^2 $$\nThe expectations of all cross-product terms are zero, e.g.:\n$$ \\mathbb{E}[2G_j V_j \\delta G_j \\delta V_j] = 2\\mathbb{E}[G_j]\\mathbb{E}[V_j]\\mathbb{E}[\\delta G_j]\\mathbb{E}[\\delta V_j] = 2(\\frac{G_{\\max}}{2})(0)(0)(0) = 0 $$\nSumming the non-zero terms:\n$$ \\mathbb{E}[E_j^2] = \\mathbb{E}[G_j^2]\\sigma_V^2 + \\mathbb{E}[V_j^2]\\sigma_G^2 + \\sigma_G^2\\sigma_V^2 $$\nThus, the mean-square quantization error in the column current is:\n$$ \\mathbb{E}[(\\Delta I)^2] = M \\left( \\mathbb{E}[G_j^2]\\sigma_V^2 + \\mathbb{E}[V_j^2]\\sigma_G^2 + \\sigma_G^2\\sigma_V^2 \\right) $$\n\n### 2. Derivation of the Relative Mean-Square Error $\\rho$\n\nThe relative mean-square error is $\\rho = \\frac{\\mathbb{E}[(\\Delta I)^2]}{\\mathbb{E}[I^2]}$. We need to find the denominator $\\mathbb{E}[I^2]$.\n$$ I = \\sum_{j=1}^{M} G_j V_j \\implies \\mathbb{E}[I^2] = \\mathbb{E}\\left[\\left(\\sum_{j=1}^{M} G_j V_j\\right)^2\\right] = \\sum_{j=1}^{M}\\sum_{k=1}^{M} \\mathbb{E}[G_j V_j G_k V_k] $$\nFor the cross-terms where $j \\neq k$, all variables are independent:\n$$ \\mathbb{E}[G_j V_j G_k V_k] = \\mathbb{E}[G_j]\\mathbb{E}[V_j]\\mathbb{E}[G_k]\\mathbb{E}[V_k] = \\left(\\frac{G_{\\max}}{2}\\right)(0)\\left(\\frac{G_{\\max}}{2}\\right)(0) = 0 $$\nThe cross-terms vanish. We are left with the diagonal terms ($j=k$):\n$$ \\mathbb{E}[I^2] = \\sum_{j=1}^{M} \\mathbb{E}[(G_j V_j)^2] $$\nAssuming $G_j$ and $V_j$ are independent for the same index $j$ (a standard assumption in this context), we have $\\mathbb{E}[(G_j V_j)^2] = \\mathbb{E}[G_j^2]\\mathbb{E}[V_j^2]$. Since all terms are i.i.d.:\n$$ \\mathbb{E}[I^2] = M \\cdot \\mathbb{E}[G_j^2]\\mathbb{E}[V_j^2] $$\nNow we can form the ratio $\\rho$:\n$$ \\rho = \\frac{M \\left( \\mathbb{E}[G_j^2]\\sigma_V^2 + \\mathbb{E}[V_j^2]\\sigma_G^2 + \\sigma_G^2\\sigma_V^2 \\right)}{M \\left( \\mathbb{E}[G_j^2]\\mathbb{E}[V_j^2] \\right)} $$\n$$ \\rho = \\frac{\\sigma_V^2}{\\mathbb{E}[V_j^2]} + \\frac{\\sigma_G^2}{\\mathbb{E}[G_j^2]} + \\frac{\\sigma_G^2\\sigma_V^2}{\\mathbb{E}[G_j^2]\\mathbb{E}[V_j^2]} $$\nSubstituting the expressions for the moments and variances:\n$$ \\mathbb{E}[G_j^2] = \\frac{G_{\\max}^2}{3}, \\quad \\mathbb{E}[V_j^2] = \\frac{V_{\\max}^2}{3} $$\n$$ \\sigma_G^2 = \\frac{G_{\\max}^2}{12(2^{b_G}-1)^2}, \\quad \\sigma_V^2 = \\frac{V_{\\max}^2}{3(2^{b_V}-1)^2} $$\nThe terms in $\\rho$ become:\n$$ \\frac{\\sigma_V^2}{\\mathbb{E}[V_j^2]} = \\frac{V_{\\max}^2 / (3(2^{b_V}-1)^2)}{V_{\\max}^2 / 3} = \\frac{1}{(2^{b_V}-1)^2} $$\n$$ \\frac{\\sigma_G^2}{\\mathbb{E}[G_j^2]} = \\frac{G_{\\max}^2 / (12(2^{b_G}-1)^2)}{G_{\\max}^2 / 3} = \\frac{3}{12(2^{b_G}-1)^2} = \\frac{1}{4(2^{b_G}-1)^2} $$\nThe third term is the product of these two:\n$$ \\frac{\\sigma_G^2\\sigma_V^2}{\\mathbb{E}[G_j^2]\\mathbb{E}[V_j^2]} = \\left(\\frac{\\sigma_G^2}{\\mathbb{E}[G_j^2]}\\right)\\left(\\frac{\\sigma_V^2}{\\mathbb{E}[V_j^2]}\\right) = \\frac{1}{4(2^{b_G}-1)^2(2^{b_V}-1)^2} $$\nSo the final symbolic expression for $\\rho$ is:\n$$ \\rho = \\frac{1}{(2^{b_V}-1)^2} + \\frac{1}{4(2^{b_G}-1)^2} \\left( 1 + \\frac{1}{(2^{b_V}-1)^2} \\right) $$\n\n### 3. Determination of the Smallest Integer $b_G$\n\nWe are given $b_V = 8$ and the constraint $\\rho \\leq \\alpha = 1.0 \\times 10^{-4}$.\nLet's substitute $b_V=8$:\n$$ 2^{b_V}-1 = 2^8 - 1 = 256 - 1 = 255 $$\nThe expression for $\\rho$ becomes:\n$$ \\rho = \\frac{1}{255^2} + \\frac{1}{4(2^{b_G}-1)^2} \\left( 1 + \\frac{1}{255^2} \\right) $$\nWe apply the constraint $\\rho \\leq 10^{-4}$:\n$$ \\frac{1}{255^2} + \\frac{1}{4(2^{b_G}-1)^2} \\left( 1 + \\frac{1}{255^2} \\right) \\leq 10^{-4} $$\nIsolate the term containing $b_G$:\n$$ \\frac{1}{4(2^{b_G}-1)^2} \\left( 1 + \\frac{1}{255^2} \\right) \\leq 10^{-4} - \\frac{1}{255^2} $$\nLet's compute the numerical values:\n$$ \\frac{1}{255^2} = \\frac{1}{65025} \\approx 1.53787 \\times 10^{-5} $$\nThe inequality becomes:\n$$ \\frac{1}{4(2^{b_G}-1)^2} \\left( 1 + \\frac{1}{65025} \\right) \\leq 10^{-4} - \\frac{1}{65025} $$\n$$ \\frac{1}{4(2^{b_G}-1)^2} \\left( \\frac{65026}{65025} \\right) \\leq \\frac{10^{-4} \\times 65025 - 1}{65025} = \\frac{6.5025 - 1}{65025} = \\frac{5.5025}{65025} $$\nMultiply both sides by $\\frac{65025}{65026}$:\n$$ \\frac{1}{4(2^{b_G}-1)^2} \\leq \\frac{5.5025}{65025} \\times \\frac{65025}{65026} = \\frac{5.5025}{65026} $$\nNow, rearrange to solve for $(2^{b_G}-1)^2$:\n$$ (2^{b_G}-1)^2 \\geq \\frac{65026}{4 \\times 5.5025} = \\frac{65026}{22.01} \\approx 2954.384 $$\nTake the square root of both sides:\n$$ 2^{b_G}-1 \\geq \\sqrt{2954.384} \\approx 54.354 $$\n$$ 2^{b_G} \\geq 55.354 $$\nTo find the integer $b_G$, we take the logarithm base $2$:\n$$ b_G \\geq \\log_2(55.354) = \\frac{\\ln(55.354)}{\\ln(2)} \\approx \\frac{4.0137}{0.6931} \\approx 5.790 $$\nSince $b_G$ must be an integer, the smallest integer value that satisfies this inequality is $6$.",
            "answer": "$$\\boxed{6}$$"
        }
    ]
}