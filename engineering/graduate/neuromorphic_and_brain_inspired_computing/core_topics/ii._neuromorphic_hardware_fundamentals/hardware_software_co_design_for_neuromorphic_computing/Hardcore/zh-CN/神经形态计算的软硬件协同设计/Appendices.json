{
    "hands_on_practices": [
        {
            "introduction": "为了在硬件上高效实现神经网络，一个核心的协同设计策略是降低突触权重的数值精度，即量化。这个练习引导我们从基本原理出发，探讨一个关键的权衡：硬件资源的节省与算法精度的损失。通过推导权重量化引入的输出误差 $L$，我们能够精确地理解硬件参数（如比特宽度 $b$）如何直接影响模型的性能，这是硬件-软件协同设计中的一个中心议题。",
            "id": "4046637",
            "problem": "一个神经形态加速器设计师必须协同优化突触权重精度和算法准确性。考虑一个单一线性神经元，其突触权重向量为 $\\mathbf{w} \\in \\mathbb{R}^{d}$，其分量在有界均匀先验 $w_{i} \\sim \\mathcal{U}(-W, W)$（其中 $W  0$）下是独立同分布的。该硬件为权重采用一个均匀标量量化器，它有 $2^{b}$ 个量化区间，这些区间将区间 $[-W, W]$ 精确地划分为等宽的子区间，每个量化值等于其所在区间的中心（一个中升型均匀量化器）。设量化器表示为 $Q(\\cdot)$，量化误差表示为 $\\mathbf{e} = Q(\\mathbf{w}) - \\mathbf{w}$。\n\n该神经元接收一个输入 $\\mathbf{x} \\in \\mathbb{R}^{d}$，该输入是零均值的，协方差矩阵为 $\\Sigma_{x} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$，且与 $\\mathbf{w}$ 无关，因此也与 $\\mathbf{e}$ 无关。量化前的输出为 $y = \\mathbf{w}^{\\top}\\mathbf{x}$，量化后的输出为 $\\tilde{y} = Q(\\mathbf{w})^{\\top}\\mathbf{x}$。在线性化机制下（无额外非线性），将准确性损失定义为由于权重量化导致的神经元输出均方误差（MSE）的期望增量：\n$$\nL \\triangleq \\mathbb{E}\\big[(\\tilde{y} - y)^{2}\\big] = \\mathbb{E}\\big[(\\mathbf{e}^{\\top}\\mathbf{x})^{2}\\big].\n$$\n\n仅从均匀量化、均匀分布、协方差和期望的线性性的核心定义出发，并且不使用任何未经证明的简化公式，推导 L 作为 b、W 和 $\\operatorname{tr}(\\Sigma_{x})$ 函数的封闭形式表达式。将您的最终答案表示为单个解析表达式。不需要进行数值评估或四舍五入。",
            "solution": "用户希望我首先进行**问题验证**，然后如果问题有效，则提供完整的解决方案。\n\n### 步骤 1：提取已知条件\n-   神经元权重向量：$\\mathbf{w} \\in \\mathbb{R}^{d}$。\n-   权重分布：$w_{i} \\sim \\mathcal{U}(-W, W)$ 是独立同分布的（i.i.d.），对于给定的 $W  0$。\n-   量化器：一个均匀标量量化器 $Q(\\cdot)$，有 $2^{b}$ 个量化区间，划分了 $[-W, W]$。它是一个中升型量化器。\n-   量化误差：$\\mathbf{e} = Q(\\mathbf{w}) - \\mathbf{w}$。\n-   神经元输入向量：$\\mathbf{x} \\in \\mathbb{R}^{d}$。\n-   输入统计量：$\\mathbb{E}[\\mathbf{x}] = \\mathbf{0}$ 和 $\\Sigma_{x} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$。\n-   独立性：$\\mathbf{x}$ 与 $\\mathbf{w}$ 无关，因此也与 $\\mathbf{e}$ 无关。\n-   量化前输出：$y = \\mathbf{w}^{\\top}\\mathbf{x}$。\n-   量化后输出：$\\tilde{y} = Q(\\mathbf{w})^{\\top}\\mathbf{x}$。\n-   准确性损失定义：$L \\triangleq \\mathbb{E}\\big[(\\tilde{y} - y)^{2}\\big] = \\mathbb{E}\\big[(\\mathbf{e}^{\\top}\\mathbf{x})^{2}\\big]$。\n-   目标：推导 L 作为 b、W 和 $\\operatorname{tr}(\\Sigma_{x})$ 函数的封闭形式表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学上合理**：该问题模拟了权重量化对线性神经元输出的影响。这是数字信号处理、机器学习硬件和神经形态工程中的一个标准和基本问题。该模型虽然简化，但基于量化理论和统计学的既定原则。**(有效)**\n2.  **定义明确**：问题提供了明确的目标和所有必要的定义（$\\mathcal{U}(-W, W)$、中升型量化器、L），以推导出唯一的解析解。**(有效)**\n3.  **客观性**：问题以精确的数学和技术语言陈述，没有任何主观或有偏见的措辞。**(有效)**\n4.  **完整性和一致性**：所提供的信息（分布、独立性、定义）是充分且相互一致的。没有遗漏关键信息，也没有矛盾之处。**(有效)**\n5.  **现实性和可行性**：该模型是分析量化神经网络时常用的现实抽象。这些条件在物理上是合理的。**(有效)**\n\n### 步骤 3：结论与行动\n问题被判定为**有效**。我将继续推导解决方案。\n\n### 推导\n准确性损失 $L$ 定义为量化后输出和量化前输出之间均方误差的期望值。\n$$\nL \\triangleq \\mathbb{E}\\big[(\\tilde{y} - y)^{2}\\big]\n$$\n代入 $\\tilde{y}$ 和 $y$ 的表达式：\n$$\nL = \\mathbb{E}\\big[(Q(\\mathbf{w})^{\\top}\\mathbf{x} - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big] = \\mathbb{E}\\big[((Q(\\mathbf{w}) - \\mathbf{w})^{\\top}\\mathbf{x})^{2}\\big]\n$$\n使用量化误差向量的定义 $\\mathbf{e} = Q(\\mathbf{w}) - \\mathbf{w}$：\n$$\nL = \\mathbb{E}\\big[(\\mathbf{e}^{\\top}\\mathbf{x})^{2}\\big]\n$$\n项 $\\mathbf{e}^{\\top}\\mathbf{x}$ 是一个标量。我们可以将其平方写为 $(\\mathbf{e}^{\\top}\\mathbf{x})(\\mathbf{e}^{\\top}\\mathbf{x}) = (\\mathbf{e}^{\\top}\\mathbf{x})(\\mathbf{x}^{\\top}\\mathbf{e}) = \\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{e}$。\n$$\nL = \\mathbb{E}[\\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{e}]\n$$\n由于期望内的表达式是一个标量，我们可以应用迹算子而不改变其值，$\\operatorname{tr}(c) = c$。\n$$\nL = \\mathbb{E}[\\operatorname{tr}(\\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{e})]\n$$\n利用迹的循环性质 $\\operatorname{tr}(ABCD) = \\operatorname{tr}(DABC)$：\n$$\nL = \\mathbb{E}[\\operatorname{tr}(\\mathbf{e}\\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top})]\n$$\n根据期望和迹算子的线性性，我们可以交换它们的顺序：\n$$\nL = \\operatorname{tr}\\big(\\mathbb{E}[\\mathbf{e}\\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}]\\big)\n$$\n问题陈述输入 $\\mathbf{x}$ 与权重 $\\mathbf{w}$ 无关，并且由于 $\\mathbf{e}$ 是 $\\mathbf{w}$ 的函数，$\\mathbf{x}$ 也与 $\\mathbf{e}$ 无关。因此，乘积的期望等于期望的乘积：\n$$\nL = \\operatorname{tr}\\big(\\mathbb{E}[\\mathbf{e}\\mathbf{e}^{\\top}] \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\big)\n$$\n我们识别出 $\\Sigma_{x} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$，并将量化误差的协方差矩阵定义为 $\\Sigma_{e} = \\mathbb{E}[\\mathbf{e}\\mathbf{e}^{\\top}]$。\n$$\nL = \\operatorname{tr}(\\Sigma_{e} \\Sigma_{x})\n$$\n接下来，我们必须确定 $\\Sigma_{e}$ 的结构。该矩阵的元素是 $(\\Sigma_{e})_{ij} = \\mathbb{E}[e_{i}e_{j}]$。误差分量是 $e_{i} = Q(w_{i}) - w_{i}$。\n权重分量 $w_{i}$ 是独立同分布的。由于标量量化器 $Q(\\cdot)$ 是逐元素应用的，因此对于 $i \\neq j$，误差分量 $e_i$ 也是两两独立的。\n这意味着对于 $i \\neq j$，有 $\\mathbb{E}[e_{i}e_{j}] = \\mathbb{E}[e_{i}]\\mathbb{E}[e_{j}]$。\n\n让我们计算单个误差分量的期望值 $\\mathbb{E}[e_{i}]$。\n$$\n\\mathbb{E}[e_i] = \\mathbb{E}[Q(w_i) - w_i] = \\mathbb{E}[Q(w_i)] - \\mathbb{E}[w_i]\n$$\n权重 $w_i$ 根据 $\\mathcal{U}(-W, W)$ 分布，这是一个关于 0 对称的分布。因此，它们的期望是 $\\mathbb{E}[w_i] = 0$。\n量化器是对称区间 $[-W, W]$ 上的中升型均匀量化器。对于对称范围内的偶数个量化级（$N=2^b$），中升型量化器是一个奇函数，$Q(-w) = -Q(w)$。$w_i$ 的概率密度函数 $p(w_i)$ 是一个偶函数（在对称区间上为常数）。期望 $\\mathbb{E}[Q(w_i)]$ 是一个奇函数（$Q(w_i)p(w_i)$）在对称区间上的积分，结果为零。\n因此，$\\mathbb{E}[e_i] = 0 - 0 = 0$。\n这意味着对于 $i \\neq j$，有 $(\\Sigma_{e})_{ij} = \\mathbb{E}[e_{i}]\\mathbb{E}[e_{j}] = 0 \\cdot 0 = 0$。所以，$\\Sigma_{e}$ 是一个对角矩阵。\n\n对角线元素是 $(\\Sigma_{e})_{ii} = \\mathbb{E}[e_{i}^{2}]$。由于 $\\mathbb{E}[e_i]=0$，这就是误差的方差 $\\operatorname{Var}(e_i)$。因为 $w_i$ 是同分布的，并且对每个分量应用相同的量化器，所以方差 $\\sigma_{e}^{2} \\triangleq \\mathbb{E}[e_{i}^{2}]$ 对所有 $i$ 都是相同的。\n所以，$\\Sigma_{e}$ 是一个缩放的单位矩阵：\n$$\n\\Sigma_{e} = \\sigma_{e}^{2} I_d\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n\n将此代回 $L$ 的表达式中：\n$$\nL = \\operatorname{tr}(\\sigma_{e}^{2} I_d \\Sigma_{x}) = \\sigma_{e}^{2} \\operatorname{tr}(I_d \\Sigma_{x}) = \\sigma_{e}^{2} \\operatorname{tr}(\\Sigma_{x})\n$$\n最后一步是计算 $\\sigma_{e}^{2}$。这是单个标量变量 $w \\sim \\mathcal{U}(-W, W)$ 的量化误差的方差。区间 $[-W, W]$ 的宽度为 $2W$。它被划分为 $2^b$ 个等宽的量化区间。量化步长 $\\Delta$ 为：\n$$\n\\Delta = \\frac{2W}{2^b}\n$$\n量化误差方差由平方误差在 $w$ 的分布上的积分给出：\n$$\n\\sigma_{e}^{2} = \\mathbb{E}[(Q(w)-w)^2] = \\int_{-W}^{W} (Q(w)-w)^2 p(w) dw\n$$\n均匀分布的概率密度函数是 $p(w) = \\frac{1}{2W}$，对于 $w \\in [-W, W]$。\n$$\n\\sigma_{e}^{2} = \\frac{1}{2W} \\int_{-W}^{W} (Q(w)-w)^2 dw\n$$\n我们可以将此积分拆分为对 $2^b$ 个量化区间的求和。对于任何给定的区间，比如 $B_k$，$Q(w)$ 是一个等于该区间中心 $c_k$ 的常数。设该区间由 $[c_k - \\Delta/2, c_k + \\Delta/2]$ 定义。对这个单个区间的积分是：\n$$\n\\int_{c_k - \\Delta/2}^{c_k + \\Delta/2} (c_k - w)^2 dw\n$$\n我们进行换元，令 $u = w - c_k$，则 $du = dw$。积分上下限变为 $-\\Delta/2$ 和 $\\Delta/2$。\n$$\n\\int_{-\\Delta/2}^{\\Delta/2} (-u)^2 du = \\int_{-\\Delta/2}^{\\Delta/2} u^2 du = \\left[ \\frac{u^3}{3} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{(\\Delta/2)^3 - (-\\Delta/2)^3}{3} = \\frac{2(\\Delta^3/8)}{3} = \\frac{\\Delta^3}{12}\n$$\n这个结果对所有 $2^b$ 个区间都是相同的。总积分是所有区间的总和：\n$$\n\\int_{-W}^{W} (Q(w)-w)^2 dw = \\sum_{k=1}^{2^b} \\frac{\\Delta^3}{12} = 2^b \\frac{\\Delta^3}{12}\n$$\n现在我们将此代回 $\\sigma_{e}^{2}$ 的表达式中：\n$$\n\\sigma_{e}^{2} = \\frac{1}{2W} \\left( 2^b \\frac{\\Delta^3}{12} \\right)\n$$\n使用关系式 $\\Delta = \\frac{2W}{2^b}$，这意味着 $2W = 2^b \\Delta$：\n$$\n\\sigma_{e}^{2} = \\frac{1}{2^b \\Delta} \\left( 2^b \\frac{\\Delta^3}{12} \\right) = \\frac{\\Delta^2}{12}\n$$\n最后，我们将 $\\Delta = \\frac{2W}{2^b}$ 代入这个方差表达式中：\n$$\n\\sigma_{e}^{2} = \\frac{1}{12} \\left( \\frac{2W}{2^b} \\right)^2 = \\frac{4W^2}{12 \\cdot (2^b)^2} = \\frac{W^2}{3 \\cdot 2^{2b}}\n$$\n我们现在可以写出准确性损失 $L$ 的最终表达式：\n$$\nL = \\sigma_{e}^{2} \\operatorname{tr}(\\Sigma_{x}) = \\frac{W^2}{3 \\cdot 2^{2b}} \\operatorname{tr}(\\Sigma_{x})\n$$\n这个表达式将准确性损失直接与量化位深度 $b$、突触权重的范围 $W$ 以及输入协方差矩阵的迹 $\\operatorname{tr}(\\Sigma_{x})$ 联系起来。",
            "answer": "$$\\boxed{\\frac{W^2 \\operatorname{tr}(\\Sigma_{x})}{3 \\cdot 2^{2b}}}$$"
        },
        {
            "introduction": "在理解了单个硬件参数的影响后，我们将分析尺度扩展到整个网络的单次推理过程。本练习旨在构建一个系统级的能量消耗模型，这在神经形态芯片设计中至关重要。我们将学习如何基于网络活动（如脉冲发放率 $\\rho$ 和扇出）以及具体的硬件参数（如存储器层次和单次操作能耗），自下而上地估算总体能耗 $E_{\\text{tot}}$。 这种建模方法使我们能够预测系统性能，并识别能耗瓶颈，为软硬件联合优化提供了坚实的量化基础。",
            "id": "4046656",
            "problem": "一个脉冲神经网络 (SNN) 在一个神经形态系统上执行，该系统具有事件驱动计算和一个由静态随机存取存储器 (SRAM) 和动态随机存取存储器 (DRAM) 组成的双层存储器层次结构。系统级能耗模型遵循独立事件能量的相加性以及事件计数与活动性的正比关系：单次推理的总能耗是控制开销、脉冲路由、神经元状态更新、突触状态更新和权重获取的能耗总和，每项都按事件发生。假设脉冲计数随一个无量纲发放率乘子 $\\rho$ 线性缩放，使得第 $l$ 层的脉冲计数变为 $s_{l}(\\rho) = \\rho s_{l}$。\n\n网络规格：\n- 每次推理的基准脉冲数：$s_{1} = 20000$， $s_{2} = 50000$， $s_{3} = 10000$。\n- 每个脉冲的平均扇出（突触目标）：$k_{1} = 64$， $k_{2} = 128$， $k_{3} = 256$。\n\n硬件能耗参数（所有能量单位为纳焦耳 (nJ)）：\n- 每次推理的固定控制开销：$E_{0} = 50000$。\n- 每个脉冲事件的路由器能耗：$E_{\\text{route}} = 5$。\n- 每个脉冲的神经元状态更新能耗：$E_{\\text{neuron}} = 3$。\n- 每个 $32$ 位字的 SRAM 读取能耗：$E_{\\text{SRAM,r}} = 10$。\n- 每个 $32$ 位字的 SRAM 写入能耗：$E_{\\text{SRAM,w}} = 20$。\n- 每个 $32$ 位字的 DRAM 读取能耗：$E_{\\text{DRAM,r}} = 1000$。\n- 每个突触事件的突触累加（算术）能耗：$E_{\\text{acc}} = 4$。\n\n软硬件映射假设（协同设计约束）：\n- 突触后膜状态存储在片上；每个突触事件执行一次状态的 SRAM 读取和一次更新后的 SRAM 回写。\n- 第 $1$ 层和第 $2$ 层的权重主要存储在 SRAM 中，但第 $2$ 层由于容量限制经历了分块处理，权重读取的 DRAM 溢出分数为 $p_{2} = 0.1$。第 $1$ 层的 $p_{1} = 0$。第 $3$ 层的权重完全存放在 DRAM 中，其 $p_{3} = 1$。\n- 第 $l$ 层中一个突触事件的能量成本包括一次权重读取、一次突触后状态读取、一次累加操作和一次状态写入。脉冲路由和神经元更新在每个脉冲发出时产生一次。\n\n仅使用上述基本定义和约束，推导总推理能耗 $E_{\\text{tot}}(\\rho)$（单位：纳焦耳）作为发放率乘子 $\\rho$ 的函数的闭式表达式，以及定义为在 $\\rho = 1$ 处求值的导数 $\\frac{d E_{\\text{tot}}}{d \\rho}$ 的灵敏度。您的最终答案必须呈现：\n- $E_{\\text{tot}}(\\rho)$ 的解析表达式（单位：纳焦耳）。\n- $\\left.\\frac{d E_{\\text{tot}}}{d \\rho}\\right|_{\\rho=1}$ 的单个实数值（单位：纳焦耳）。\n无需四舍五入；请给出精确的整数值。",
            "solution": "问题陈述已经过严格验证，被认为是有效的。它在科学上基于神经形态计算和数字系统能耗建模的原理，参数完整且一致，问题设定适定，并且其表述是客观的。因此，我们可以进行正式求解。\n\n单次推理的总能耗 $E_{\\text{tot}}$ 是一个固定的控制开销和一个活动相关分量的总和。活动又由发放率乘子 $\\rho$ 线性缩放。总能耗可以表示为：\n$$E_{\\text{tot}}(\\rho) = E_{0} + E_{\\text{activity}}(\\rho)$$\n其中 $E_{0}$ 是固定的控制开销能耗，给定为 $E_{0} = 50000$ nJ。\n\n活动相关能耗 $E_{\\text{activity}}(\\rho)$ 由两部分主要贡献组成：脉冲事件（路由和神经元更新）消耗的能量和突触事件（权重访问和状态更新）消耗的能量。\n$$E_{\\text{activity}}(\\rho) = E_{\\text{spike}}(\\rho) + E_{\\text{synapse}}(\\rho)$$\n\n首先，我们对与脉冲事件相关的能耗进行建模。第 $l$ 层的脉冲数由 $s_l(\\rho) = \\rho s_l$ 给出。网络中的总脉冲数是所有 $3$ 层的总和：\n$$S_{\\text{tot}}(\\rho) = \\sum_{l=1}^{3} s_{l}(\\rho) = \\rho \\sum_{l=1}^{3} s_{l}$$\n每个脉冲都会产生路由 ($E_{\\text{route}}$) 和神经元状态更新 ($E_{\\text{neuron}}$) 的能量成本。所有脉冲事件的总能耗是：\n$$E_{\\text{spike}}(\\rho) = S_{\\text{tot}}(\\rho) (E_{\\text{route}} + E_{\\text{neuron}}) = \\rho \\left( \\sum_{l=1}^{3} s_{l} \\right) (E_{\\text{route}} + E_{\\text{neuron}})$$\n\n接下来，我们对突触事件的能耗进行建模。一个源于第 $l$ 层的脉冲会产生 $k_l$ 个突触事件，其中 $k_l$ 是该层的平均扇出。第 $l$ 层的突触事件数量是：\n$$N_{\\text{syn}, l}(\\rho) = s_{l}(\\rho) k_{l} = \\rho s_{l} k_{l}$$\n总突触能耗是所有层中所有突触事件能耗的总和：\n$$E_{\\text{synapse}}(\\rho) = \\sum_{l=1}^{3} N_{\\text{syn}, l}(\\rho) E_{\\text{syn}, l} = \\rho \\sum_{l=1}^{3} s_{l} k_{l} E_{\\text{syn}, l}$$\n其中 $E_{\\text{syn}, l}$ 是第 $l$ 层中每个突触事件的能耗。\n\n能量 $E_{\\text{syn}, l}$ 由硬件映射决定。对于每个突触事件，成本包括：一次权重读取、一次突触后状态读取、一次累加操作和一次突触后状态写入。突触后状态存储在 SRAM 中，因此其访问能耗为 $E_{\\text{SRAM,r}} + E_{\\text{SRAM,w}}$。累加操作的能耗是 $E_{\\text{acc}}$。权重读取能耗取决于该层的内存分配，其特征是 DRAM 溢出分数 $p_l$。在第 $l$ 层中，一次权重读取的平均能耗是 $(1-p_l)E_{\\text{SRAM,r}} + p_l E_{\\text{DRAM,r}}$。\n因此，第 $l$ 层中每个突触事件的能耗是：\n$$E_{\\text{syn}, l} = \\left( (1-p_l)E_{\\text{SRAM,r}} + p_l E_{\\text{DRAM,r}} \\right) + E_{\\text{SRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}}$$\n$$E_{\\text{syn}, l} = (2-p_l)E_{\\text{SRAM,r}} + p_l E_{\\text{DRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}}$$\n\n结合所有分量，总能耗 $E_{\\text{tot}}(\\rho)$ 是：\n$$E_{\\text{tot}}(\\rho) = E_{0} + \\rho \\left[ \\left( \\sum_{l=1}^{3} s_{l} \\right) (E_{\\text{route}} + E_{\\text{neuron}}) + \\sum_{l=1}^{3} s_{l} k_{l} E_{\\text{syn}, l} \\right]$$\n\n现在，我们代入给定的值。\n脉冲计数为 $s_{1} = 20000$，$s_{2} = 50000$ 和 $s_{3} = 10000$。\n扇出为 $k_{1} = 64$，$k_{2} = 128$ 和 $k_{3} = 256$。\n能耗参数为 $E_{0}=50000$，$E_{\\text{route}}=5$，$E_{\\text{neuron}}=3$，$E_{\\text{SRAM,r}}=10$，$E_{\\text{SRAM,w}}=20$，$E_{\\text{DRAM,r}}=1000$ 和 $E_{\\text{acc}}=4$（所有单位均为 nJ）。\nDRAM 溢出分数为 $p_{1} = 0$，$p_{2} = 0.1$ 和 $p_{3} = 1$。\n\n首先，计算每层每个突触事件的能耗：\n对于第 $l=1$ 层：$p_{1} = 0$。\n$$E_{\\text{syn}, 1} = (2-0)E_{\\text{SRAM,r}} + 0 \\cdot E_{\\text{DRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}} = 2(10) + 20 + 4 = 44 \\text{ nJ}$$\n对于第 $l=2$ 层：$p_{2} = 0.1$。\n$$E_{\\text{syn}, 2} = (2-0.1)E_{\\text{SRAM,r}} + 0.1 E_{\\text{DRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}} = 1.9(10) + 0.1(1000) + 20 + 4 = 19 + 100 + 24 = 143 \\text{ nJ}$$\n对于第 $l=3$ 层：$p_{3} = 1$。\n$$E_{\\text{syn}, 3} = (2-1)E_{\\text{SRAM,r}} + 1 \\cdot E_{\\text{DRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}} = 1(10) + 1(1000) + 20 + 4 = 10 + 1000 + 24 = 1034 \\text{ nJ}$$\n\n接下来，我们计算 $\\rho$ 的系数，它代表每单位基准活动的总能耗。\n基准状态下的总脉冲数：$\\sum s_l = 20000 + 50000 + 10000 = 80000$。\n每个脉冲事件的能耗：$E_{\\text{route}} + E_{\\text{neuron}} = 5 + 3 = 8$ nJ。\n与脉冲相关的总能耗系数：\n$$C_{\\text{spike}} = \\left( \\sum_{l=1}^{3} s_{l} \\right) (E_{\\text{route}} + E_{\\text{neuron}}) = 80000 \\times 8 = 640000 \\text{ nJ}$$\n与突触相关的总能耗系数：\n$$C_{\\text{synapse}} = \\sum_{l=1}^{3} s_{l} k_{l} E_{\\text{syn}, l} = s_{1}k_{1}E_{\\text{syn}, 1} + s_{2}k_{2}E_{\\text{syn}, 2} + s_{3}k_{3}E_{\\text{syn}, 3}$$\n$$C_{\\text{synapse}} = (20000 \\times 64 \\times 44) + (50000 \\times 128 \\times 143) + (10000 \\times 256 \\times 1034)$$\n$$C_{\\text{synapse}} = 56320000 + 915200000 + 2647040000 = 3618560000 \\text{ nJ}$$\n总的活动相关能耗系数是这两者之和：\n$$C_{\\text{activity}} = C_{\\text{spike}} + C_{\\text{synapse}} = 640000 + 3618560000 = 3619200000 \\text{ nJ}$$\n\n因此，总推理能耗的闭式表达式为：\n$$E_{\\text{tot}}(\\rho) = 50000 + 3619200000 \\rho$$\n\n灵敏度是总能耗关于发放率乘子 $\\rho$ 的导数：\n$$\\frac{d E_{\\text{tot}}}{d \\rho} = \\frac{d}{d \\rho} (50000 + 3619200000 \\rho) = 3619200000$$\n这个导数是常数。其在 $\\rho = 1$ 处的值是：\n$$\\left.\\frac{d E_{\\text{tot}}}{d \\rho}\\right|_{\\rho=1} = 3619200000$$\n\n第一个结果是 $E_{\\text{tot}}(\\rho)$ 的解析表达式，第二个结果是在 $\\rho=1$ 处灵敏度的数值。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n50000 + 3619200000 \\rho  3619200000\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "最后的练习将我们带入一个更高级、更动态的协同设计场景：片上学习。这个挑战的核心是在有限的片上存储资源 $M$ 和学习算法的数值精度之间做出权衡。我们将为一种名为“随时间反向传播”（BPTT）的算法，通过分析其截断长度 $L$ 对梯度偏差和内存占用的双重影响，设计一个最优的截断方案。 这个练习促使我们思考如何根据动态变化的系统状态和约束来实时调整算法参数，这是实现高效自适应神经形态系统的关键一步。",
            "id": "4046578",
            "problem": "考虑一个循环脉冲神经网络，其离散时间隐藏状态在其工作点附近遵循一个稳定的线性化循环，因此在所有时间 $t$，局部状态雅可比矩阵 $J_t$ 满足 $\\|J_t\\|_2 \\leq \\alpha$，其中 $\\alpha \\in (0,1)$ 是一个固定的收缩因子。设标量损失为 $\\mathcal{L} = \\sum_{t=1}^{T} \\ell_t$，并假设 $\\ell_t$ 相对于权重矩阵 $W$ 的梯度可以通过链式法则写成一个时间索引项的和，这些项涉及雅可比矩阵的乘积。假设存在一个统一常数 $\\gamma  0$，使得每一步对梯度范数的贡献有界，即对所有 $t$ 都有 $\\|\\nabla_W \\ell_t\\|_2 \\leq \\gamma$。随时间反向传播 (BPTT) 通过在 $L$ 步后分离计算图来截断梯度，忽略了来自早于 $t-L$ 时刻的贡献的尾部。将在给定时间 $t$ 的截断引起的梯度偏差大小定义为完整梯度和截断梯度在 $t$ 处的 $\\ell_2$ 范数差，记为 $B(L)$。\n\n您正在为一个神经形态处理器协同设计一个片上学习程序，该处理器在片上内存约束下执行截断的 BPTT。该处理器以事件压缩形式存储梯度计算所需的最少充分状态。对于一个拥有 $N$ 个神经元的网络，事件稀疏度为 $s \\in (0,1)$（每个时间步激活的神经元比例），每个存储的激活值占用 $b$ 字节，则每个时间步的存储成本为 $s N b$。有一个与截断长度 $L$ 无关的固定开销 $c$ 字节。设可用的片上内存预算为 $M$ 字节；进一步假设一个外部感知管道消耗一个随时间变化的内存量 $m_{\\mathrm{other}}(t)$，这将可用于学习的内存减少到 $M - m_{\\mathrm{other}}(t)$。\n\n对于给定的时间任务，一个长度为 $T$ 的序列中有两个机制：机制 1 适用于时间 $t \\in \\{1,2,\\dots,T/2\\}$，有效收缩因子为 $\\alpha_1$，界为 $\\gamma_1$；机制 2 适用于时间 $t \\in \\{T/2+1,\\dots,T\\}$，有效收缩因子为 $\\alpha_2$，界为 $\\gamma_2$。假设 $T$ 足够大，使得截断尾部可以用该界所隐含的无穷几何级数来建模。机制 1 中的可用内存为 $M_1 = M - m_{\\mathrm{other},1}$，机制 2 中的可用内存为 $M_2 = M - m_{\\mathrm{other},2}$。您需要为这两个机制设计一个截断方案 $L_1$ 和 $L_2$（以时间步为单位），该方案在满足每个机制的内存约束的同时，最小化截断引起的梯度偏差，并且还满足指定的偏差容限 $\\epsilon_1$ 和 $\\epsilon_2$。\n\n使用以下参数值：\n- $N = 10^{4}$，$s = 0.1$，$b = 2$ (字节)，$c = 10^{5}$ (字节)，$M = 1.5 \\times 10^{6}$ (字节)。\n- $m_{\\mathrm{other},1} = 3.0 \\times 10^{5}$ (字节)，$m_{\\mathrm{other},2} = 1.0 \\times 10^{5}$ (字节)。\n- $\\alpha_1 = 0.95$，$\\gamma_1 = 1.2$，$\\epsilon_1 = 2.0 \\times 10^{-2}$。\n- $\\alpha_2 = 0.80$，$\\gamma_2 = 0.80$，$\\epsilon_2 = 1.0 \\times 10^{-2}$。\n\n任务：\n1. 从链式法则和算子范数的次乘法性质出发，推导单一机制下 $B(L)$ 关于 $\\alpha$ 和 $\\gamma$ 的一个界。解释为什么这个界呈现为几何级数所隐含的形式，并以 $L$、$\\alpha$ 和 $\\gamma$ 的函数形式给出 $B(L)$ 的闭式界。\n2. 给定机制 $i \\in \\{1,2\\}$ 的界和偏差容限 $\\epsilon_i$，推导确保 $B(L) \\leq \\epsilon_i$ 的最小截断长度 $L_{\\mathrm{bias},i}$。\n3. 给定机制 $i$ 中的片上内存限制 $M_i$ 和存储模型，推导满足 $c + s N b \\, L \\leq M_i$ 的最大可行截断长度 $L_{\\mathrm{max},i}$。\n4. 通过为每个机制 $i$ 选择 $L_i = \\min\\{L_{\\mathrm{bias},i}, L_{\\mathrm{max},i}\\}$，规定在尊重内存的同时最小化偏差的截断方案 $(L_1, L_2)$。\n\n使用说明中描述的行矩阵格式，将您的最终答案表示为时间步长对 $(L_1, L_2)$。不需要四舍五入；$L_1$ 和 $L_2$ 均为整数。",
            "solution": "该问题将根据既定标准进行验证。\n\n### 问题验证\n\n**步骤 1：提取给定信息**\n\n- **网络动态**：循环脉冲神经网络，具有离散时间隐藏状态。\n- **雅可比界**：局部状态雅可比矩阵 $J_t$ 满足 $\\|J_t\\|_2 \\leq \\alpha$，其中 $\\alpha \\in (0,1)$ 是一个固定的收缩因子。\n- **损失函数**：$\\mathcal{L} = \\sum_{t=1}^{T} \\ell_t$。\n- **梯度结构**：$\\ell_t$ 相对于权重矩阵 $W$ 的梯度涉及雅可比矩阵的乘积。\n- **梯度范数界**：$\\|\\nabla_W \\ell_t\\|_2 \\leq \\gamma$，其中 $\\gamma  0$ 为常数。这被表述为“每一步对梯度范数的贡献”。\n- **BPTT 截断**：随时间反向传播 (BPTT) 被截断为 $L$ 步。\n- **梯度偏差**：$B(L)$ 是在时间 $t$ 处完整梯度和截断梯度之间的 $\\ell_2$ 范数差。\n- **硬件参数**：\n    - 神经元数量：$N = 10^{4}$。\n    - 事件稀疏度：$s = 0.1$。\n    - 每个激活值的字节数：$b = 2$。\n    - 每个时间步的存储成本：$s N b$。\n    - 固定存储开销：$c = 10^{5}$ 字节。\n    - 总内存预算：$M = 1.5 \\times 10^{6}$ 字节。\n    - 外部内存使用：$m_{\\mathrm{other}}(t)$。\n- **操作机制**：\n    - **机制 1** ($t \\in \\{1, \\dots, T/2\\}$)：\n        - 收缩因子：$\\alpha_1 = 0.95$。\n        - 梯度界：$\\gamma_1 = 1.2$。\n        - 偏差容限：$\\epsilon_1 = 2.0 \\times 10^{-2}$。\n        - 外部内存：$m_{\\mathrm{other},1} = 3.0 \\times 10^{5}$ 字节。\n        - 可用内存：$M_1 = M - m_{\\mathrm{other},1}$。\n    - **机制 2** ($t \\in \\{T/2+1, \\dots, T\\}$)：\n        - 收缩因子：$\\alpha_2 = 0.80$。\n        - 梯度界：$\\gamma_2 = 0.80$。\n        - 偏差容限：$\\epsilon_2 = 1.0 \\times 10^{-2}$。\n        - 外部内存：$m_{\\mathrm{other},2} = 1.0 \\times 10^{5}$ 字节。\n        - 可用内存：$M_2 = M - m_{\\mathrm{other},2}$。\n- **假设**：$T$ 足够大，可以用无穷几何级数来建模截断尾部。\n- **任务**：\n    1. 推导 $B(L)$ 关于 $\\alpha$ 和 $\\gamma$ 的一个界。\n    2. 为每个机制 $i$ 推导满足偏差容限 $\\epsilon_i$ 的最小截断长度 $L_{\\mathrm{bias},i}$。\n    3. 为每个机制 $i$ 根据内存限制 $M_i$ 推导最大可行截断长度 $L_{\\mathrm{max},i}$。\n    4. 使用规则 $L_i = \\min\\{L_{\\mathrm{bias},i}, L_{\\mathrm{max},i}\\}$ 确定方案 $(L_1, L_2)$。\n\n**步骤 2：使用提取的给定信息进行验证**\n\n- **科学基础**：该问题在训练循环神经网络的理论方面有很好的基础。BPTT、梯度消失（与 $\\alpha  1$ 相关）、雅可比矩阵和梯度截断等概念是机器学习中的标准内容。硬件内存模型是在协同设计分析中使用的有效且常见的简化。该问题在科学上是合理的。\n- **良定性**：该问题是自包含的，提供了所有必要的参数和方程。目标定义清晰：根据一组约束找到一个最优的截断方案 $(L_1, L_2)$。任务的结构逻辑地导向最终解决方案，确保存在唯一答案。\n- **客观性**：该问题以精确、客观的数学和技术语言陈述，没有任何主观或模棱两可的术语。\n\n问题陈述没有表现出验证标准中列出的任何缺陷（例如，科学不合理、不完整、矛盾、不可行）。\n\n**步骤 3：结论与行动**\n\n此问题是**有效的**。将提供完整的解决方案。\n\n### 解决方案\n\n解决方案按顺序处理四个任务。\n\n**任务 1：截断引起的梯度偏差 $B(L)$ 的界**\n\n损失 $\\mathcal{L}$ 相对于权重 $W$ 的梯度是使用链式法则计算的，该法则展开了网络的循环依赖关系。完整梯度累积了来自所有先前时间步的影响。对于损失项 $\\ell_t$ 的梯度计算涉及依赖于状态 $h_{t-1}, h_{t-2}, \\dots$ 的项。一个表示长度为 $k$（从时间 $t-k$ 到 $t$）的依赖路径的项包含 $k$ 个雅可比矩阵的乘积：$J_t J_{t-1} \\cdots J_{t-k+1}$。\n\n这样一个乘积的范数可以使用矩阵范数的次乘法性质和给定的条件 $\\|J_\\tau\\|_2 \\leq \\alpha$（对所有 $\\tau$）来界定：\n$$\n\\left\\| \\prod_{j=t-k+1}^{t} J_j \\right\\|_2 \\leq \\prod_{j=t-k+1}^{t} \\|J_j\\|_2 \\leq \\alpha^k\n$$\n这表明过去状态对当前梯度的影响随着时间距离呈指数衰减，这是网络动态具有收缩性的直接结果。\n\n使用长度为 $L$ 的窗口进行截断的 BPTT 会忽略所有长度为 $k \\geq L$ 的依赖路径。截断引起的梯度偏差 $B(L)$ 是所有被忽略的梯度贡献之和的 $\\ell_2$ 范数。设对应于长度为 $k$ 的依赖路径的梯度部分表示为 $\\mathcal{G}_k$。则偏差为 $B(L) = \\| \\sum_{k=L}^{\\infty} \\mathcal{G}_k \\|_2$。\n\n使用三角不等式，我们可以用范数的和来界定和的范数：\n$$\nB(L) \\leq \\sum_{k=L}^{\\infty} \\| \\mathcal{G}_k \\|_2\n$$\n问题指出存在一个统一常数 $\\gamma  0$，使得“每一步对梯度范数的贡献”有界。我们在 BPTT 偏差分析的标准上下文中解释这一点，其中来自长度为 $k$ 的依赖的贡献的范数可以被界定为 $\\|\\mathcal{G}_k\\|_2 \\leq \\gamma \\alpha^k$。这个模型结合了基础局部梯度幅度（与 $\\gamma$ 相关）和指数衰减（与 $\\alpha^k$ 相关）。\n\n将此代入 $B(L)$ 的不等式中，得到一个几何级数：\n$$\nB(L) \\leq \\sum_{k=L}^{\\infty} \\gamma \\alpha^k = \\gamma \\left( \\alpha^L + \\alpha^{L+1} + \\alpha^{L+2} + \\dots \\right)\n$$\n这个无穷几何级数的首项为 $a = \\gamma \\alpha^L$，公比为 $r = \\alpha$。由于 $0  \\alpha  1$，级数收敛。其和由 $\\frac{a}{1-r}$ 给出。\n因此，截断引起的梯度偏差的闭式界为：\n$$\nB(L) \\leq \\frac{\\gamma \\alpha^L}{1-\\alpha}\n$$\n\n**任务 2：根据偏差容限确定的最小截断长度 $L_{\\mathrm{bias},i}$**\n\n对于每个机制 $i \\in \\{1, 2\\}$，我们必须找到满足偏差容限 $\\epsilon_i$ 的最小整数截断长度 $L_{\\mathrm{bias},i}$。这要求 $B(L) \\leq \\epsilon_i$。使用推导出的界：\n$$\n\\frac{\\gamma_i \\alpha_i^L}{1-\\alpha_i} \\leq \\epsilon_i\n$$\n我们解出 $L$：\n$$\n\\alpha_i^L \\leq \\frac{\\epsilon_i(1-\\alpha_i)}{\\gamma_i}\n$$\n对两边取自然对数。由于 $\\alpha_i \\in (0,1)$，$\\ln(\\alpha_i)$ 是负数，所以不等式反转：\n$$\nL \\ln(\\alpha_i) \\leq \\ln\\left( \\frac{\\epsilon_i(1-\\alpha_i)}{\\gamma_i} \\right) \\implies L \\geq \\frac{\\ln\\left( \\frac{\\epsilon_i(1-\\alpha_i)}{\\gamma_i} \\right)}{\\ln(\\alpha_i)}\n$$\n由于 $L$ 必须是整数，所需的最小长度是此表达式的向上取整。\n$$\nL_{\\mathrm{bias},i} = \\left\\lceil \\frac{\\ln\\left( \\frac{\\epsilon_i(1-\\alpha_i)}{\\gamma_i} \\right)}{\\ln(\\alpha_i)} \\right\\rceil\n$$\n对于机制 1：$\\alpha_1=0.95$，$\\gamma_1=1.2$，$\\epsilon_1=2.0 \\times 10^{-2}$。\n$$\nL_{\\mathrm{bias},1} = \\left\\lceil \\frac{\\ln\\left( \\frac{(2.0 \\times 10^{-2})(1-0.95)}{1.2} \\right)}{\\ln(0.95)} \\right\\rceil = \\left\\lceil \\frac{\\ln\\left( \\frac{0.001}{1.2} \\right)}{\\ln(0.95)} \\right\\rceil \\approx \\left\\lceil \\frac{-7.090}{-0.05129} \\right\\rceil = \\lceil 138.23 \\rceil = 139\n$$\n对于机制 2：$\\alpha_2=0.80$，$\\gamma_2=0.80$，$\\epsilon_2=1.0 \\times 10^{-2}$。\n$$\nL_{\\mathrm{bias},2} = \\left\\lceil \\frac{\\ln\\left( \\frac{(1.0 \\times 10^{-2})(1-0.80)}{0.80} \\right)}{\\ln(0.80)} \\right\\rceil = \\left\\lceil \\frac{\\ln(0.0025)}{\\ln(0.80)} \\right\\rceil \\approx \\left\\lceil \\frac{-5.991}{-0.2231} \\right\\rceil = \\lceil 26.85 \\rceil = 27\n$$\n\n**任务 3：根据内存限制确定的最大截断长度 $L_{\\mathrm{max},i}$**\n\n用于存储截断长度为 $L$ 的激活值的总内存被建模为固定开销 $c$ 加上每时间步成本 $s N b$ 乘以 $L$。这个总量必须在可用内存预算 $M_i = M - m_{\\mathrm{other},i}$ 之内。\n$$\nc + s N b L \\leq M_i\n$$\n解出最大长度 $L$：\n$$\ns N b L \\leq M_i - c \\implies L \\leq \\frac{M_i - c}{s N b}\n$$\n由于 $L$ 必须是整数，最大可行长度是此表达式的向下取整。\n$$\nL_{\\mathrm{max},i} = \\left\\lfloor \\frac{M_i - c}{s N b} \\right\\rfloor\n$$\n首先，我们计算每个时间步的恒定存储成本 $sNb$：\n$$\ns N b = 0.1 \\times 10^4 \\times 2 = 2000 \\text{ 字节/时间步}\n$$\n接下来，我们计算每个机制的可用内存：\n$$\nM_1 = M - m_{\\mathrm{other},1} = 1.5 \\times 10^6 - 3.0 \\times 10^5 = 1.2 \\times 10^6 \\text{ 字节}\n$$\n$$\nM_2 = M - m_{\\mathrm{other},2} = 1.5 \\times 10^6 - 1.0 \\times 10^5 = 1.4 \\times 10^6 \\text{ 字节}\n$$\n现在，我们计算每个机制的最大长度 $L_{\\mathrm{max},i}$。\n对于机制 1：\n$$\nL_{\\mathrm{max},1} = \\left\\lfloor \\frac{M_1 - c}{s N b} \\right\\rfloor = \\left\\lfloor \\frac{1.2 \\times 10^6 - 10^5}{2000} \\right\\rfloor = \\left\\lfloor \\frac{1.1 \\times 10^6}{2000} \\right\\rfloor = \\lfloor 550 \\rfloor = 550\n$$\n对于机制 2：\n$$\nL_{\\mathrm{max},2} = \\left\\lfloor \\frac{M_2 - c}{s N b} \\right\\rfloor = \\left\\lfloor \\frac{1.4 \\times 10^6 - 10^5}{2000} \\right\\rfloor = \\left\\lfloor \\frac{1.3 \\times 10^6}{2000} \\right\\rfloor = \\lfloor 650 \\rfloor = 650\n$$\n\n**任务 4：规定最终截断方案 $(L_1, L_2)$**\n\n问题要求一个方案 $(L_1, L_2)$，该方案在尊重内存约束的同时最小化偏差。这转化为选择尽可能长的截断长度，直到达到偏差容限或内存预算所施加的限制。规则是 $L_i = \\min\\{L_{\\mathrm{bias},i}, L_{\\mathrm{max},i}\\}$。这里，$L_{\\mathrm{bias},i}$ 是满足偏差目标的最小长度，而 $L_{\\mathrm{max},i}$ 是内存可以支持的最大长度。选择它们的最小值确保两个约束都得到满足。\n\n对于机制 1：\n$$\nL_1 = \\min\\{L_{\\mathrm{bias},1}, L_{\\mathrm{max},1}\\} = \\min\\{139, 550\\} = 139\n$$\n在此机制中，期望的偏差容限是限制因素。片上内存足以支持所需的截断长度。\n\n对于机制 2：\n$$\nL_2 = \\min\\{L_{\\mathrm{bias},2}, L_{\\mathrm{max},2}\\} = \\min\\{27, 650\\} = 27\n$$\n同样，在此机制中，偏差容限是限制因素。由于收缩因子更强（$\\alpha_2  \\alpha_1$），所需的截断长度要短得多，并且内存充足。\n\n最终规定的截断方案是 $(L_1, L_2) = (139, 27)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n139  27\n\\end{pmatrix}\n}\n$$"
        }
    ]
}