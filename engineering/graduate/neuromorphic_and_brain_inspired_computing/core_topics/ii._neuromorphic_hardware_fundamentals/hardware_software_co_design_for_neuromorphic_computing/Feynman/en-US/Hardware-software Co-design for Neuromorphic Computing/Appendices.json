{
    "hands_on_practices": [
        {
            "introduction": "Effective hardware-software co-design often begins at the most fundamental level: the representation of information. In neuromorphic hardware, reducing the numerical precision of synaptic weights is a powerful technique for saving memory and energy. This practice guides you through a foundational analysis to quantify the trade-off, specifically deriving the accuracy loss resulting from weight quantization . By connecting a hardware parameter, the bit-width $b$, to an algorithmic metric, the Mean Squared Error, you will develop a core skill for optimizing the balance between hardware efficiency and model performance.",
            "id": "4046637",
            "problem": "A neuromorphic accelerator designer must co-optimize synaptic weight precision and algorithmic accuracy. Consider a single linear neuron with synaptic weight vector $\\mathbf{w} \\in \\mathbb{R}^{d}$ whose entries are independent and identically distributed under a bounded uniform prior $w_{i} \\sim \\mathcal{U}(-W, W)$, where $W > 0$. The hardware employs a uniform scalar quantizer for weights with $2^{b}$ bins that exactly partition the interval $[-W, W]$ into equal-width subintervals, with each quantized value equal to the center of its bin (a mid-rise uniform quantizer). Let the quantizer be denoted by $Q(\\cdot)$ and the quantization error by $\\mathbf{e} = Q(\\mathbf{w}) - \\mathbf{w}$.\n\nThe neuron receives an input $\\mathbf{x} \\in \\mathbb{R}^{d}$ that is zero-mean with covariance matrix $\\Sigma_{x} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$, independent of $\\mathbf{w}$ and hence independent of $\\mathbf{e}$. The pre-quantization output is $y = \\mathbf{w}^{\\top}\\mathbf{x}$ and the post-quantization output is $\\tilde{y} = Q(\\mathbf{w})^{\\top}\\mathbf{x}$. In a linearized regime (no additional nonlinearity), define the accuracy loss as the expected increase in Mean Squared Error (MSE) at the neuron output due to weight quantization:\n$$\nL \\triangleq \\mathbb{E}\\big[(\\tilde{y} - y)^{2}\\big] = \\mathbb{E}\\big[(\\mathbf{e}^{\\top}\\mathbf{x})^{2}\\big].\n$$\n\nStarting only from core definitions of uniform quantization, uniform distributions, covariance, and linearity of expectation, and without invoking any unproven shortcut formulas, derive a closed-form expression for $L$ as a function of $b$, $W$, and $\\operatorname{tr}(\\Sigma_{x})$. Express your final answer as a single analytic expression. No numerical evaluation or rounding is required.",
            "solution": "The accuracy loss $L$ is defined as the expected squared error between the post-quantization and pre-quantization outputs.\n$$\nL \\triangleq \\mathbb{E}\\big[(\\tilde{y} - y)^{2}\\big]\n$$\nSubstituting the expressions for $\\tilde{y}$ and $y$:\n$$\nL = \\mathbb{E}\\big[(Q(\\mathbf{w})^{\\top}\\mathbf{x} - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big] = \\mathbb{E}\\big[((Q(\\mathbf{w}) - \\mathbf{w})^{\\top}\\mathbf{x})^{2}\\big]\n$$\nUsing the definition of the quantization error vector, $\\mathbf{e} = Q(\\mathbf{w}) - \\mathbf{w}$:\n$$\nL = \\mathbb{E}\\big[(\\mathbf{e}^{\\top}\\mathbf{x})^{2}\\big]\n$$\nThe term $\\mathbf{e}^{\\top}\\mathbf{x}$ is a scalar. We can write its square as $(\\mathbf{e}^{\\top}\\mathbf{x})(\\mathbf{e}^{\\top}\\mathbf{x}) = (\\mathbf{e}^{\\top}\\mathbf{x})(\\mathbf{x}^{\\top}\\mathbf{e}) = \\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{e}$.\n$$\nL = \\mathbb{E}[\\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{e}]\n$$\nSince the expression inside the expectation is a scalar, we can apply the trace operator without changing its value, $\\operatorname{tr}(c) = c$.\n$$\nL = \\mathbb{E}[\\operatorname{tr}(\\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}\\mathbf{e})]\n$$\nUsing the cyclic property of the trace, $\\operatorname{tr}(ABCD) = \\operatorname{tr}(DABC)$:\n$$\nL = \\mathbb{E}[\\operatorname{tr}(\\mathbf{e}\\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top})]\n$$\nBy the linearity of both expectation and trace operators, we can swap their order:\n$$\nL = \\operatorname{tr}\\big(\\mathbb{E}[\\mathbf{e}\\mathbf{e}^{\\top}\\mathbf{x}\\mathbf{x}^{\\top}]\\big)\n$$\nThe problem states that the input $\\mathbf{x}$ is independent of the weights $\\mathbf{w}$, and since $\\mathbf{e}$ is a function of $\\mathbf{w}$, $\\mathbf{x}$ is also independent of $\\mathbf{e}$. Therefore, the expectation of the product is the product of the expectations:\n$$\nL = \\operatorname{tr}\\big(\\mathbb{E}[\\mathbf{e}\\mathbf{e}^{\\top}] \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]\\big)\n$$\nWe recognize $\\Sigma_{x} = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$ and we define the covariance matrix of the quantization error as $\\Sigma_{e} = \\mathbb{E}[\\mathbf{e}\\mathbf{e}^{\\top}]$.\n$$\nL = \\operatorname{tr}(\\Sigma_{e} \\Sigma_{x})\n$$\nNext, we must determine the structure of $\\Sigma_{e}$. The entries of this matrix are $(\\Sigma_{e})_{ij} = \\mathbb{E}[e_{i}e_{j}]$. The error components are $e_{i} = Q(w_{i}) - w_{i}$.\nThe weight components $w_{i}$ are given as independent and identically distributed. Since the scalar quantizer $Q(\\cdot)$ is applied element-wise, the error components $e_i$ are also pairwise independent for $i \\neq j$.\nThis means that for $i \\neq j$, $\\mathbb{E}[e_{i}e_{j}] = \\mathbb{E}[e_{i}]\\mathbb{E}[e_{j}]$.\n\nLet's compute the expected value of a single error component, $\\mathbb{E}[e_{i}]$.\n$$\n\\mathbb{E}[e_i] = \\mathbb{E}[Q(w_i) - w_i] = \\mathbb{E}[Q(w_i)] - \\mathbb{E}[w_i]\n$$\nThe weights $w_i$ are distributed according to $\\mathcal{U}(-W, W)$, a symmetric distribution around $0$. Thus, their expectation is $\\mathbb{E}[w_i] = 0$.\nThe quantizer is a mid-rise uniform quantizer over the symmetric interval $[-W, W]$. A mid-rise quantizer for an even number of levels ($N=2^b$) over a symmetric range is an odd function, $Q(-w) = -Q(w)$. The probability density function of $w_i$, $p(w_i)$, is an even function (constant over a symmetric interval). The expectation $\\mathbb{E}[Q(w_i)]$ is the integral of an odd function ($Q(w_i)p(w_i)$) over a symmetric interval, which is zero.\nTherefore, $\\mathbb{E}[e_i] = 0 - 0 = 0$.\nThis implies that for $i \\neq j$, $(\\Sigma_{e})_{ij} = \\mathbb{E}[e_{i}]\\mathbb{E}[e_{j}] = 0 \\cdot 0 = 0$. So, $\\Sigma_{e}$ is a diagonal matrix.\n\nThe diagonal elements are $(\\Sigma_{e})_{ii} = \\mathbb{E}[e_{i}^{2}]$. Since $\\mathbb{E}[e_i]=0$, this is the variance of the error, $\\operatorname{Var}(e_i)$. As the $w_i$ are identically distributed and the same quantizer is applied to each, the variance $\\sigma_{e}^{2} \\triangleq \\mathbb{E}[e_{i}^{2}]$ is the same for all $i$.\nSo, $\\Sigma_{e}$ is a scaled identity matrix:\n$$\n\\Sigma_{e} = \\sigma_{e}^{2} I_d\n$$\nwhere $I_d$ is the $d \\times d$ identity matrix.\n\nSubstituting this back into the expression for $L$:\n$$\nL = \\operatorname{tr}(\\sigma_{e}^{2} I_d \\Sigma_{x}) = \\sigma_{e}^{2} \\operatorname{tr}(I_d \\Sigma_{x}) = \\sigma_{e}^{2} \\operatorname{tr}(\\Sigma_{x})\n$$\nThe final step is to calculate $\\sigma_{e}^{2}$. This is the variance of the quantization error for a single scalar variable $w \\sim \\mathcal{U}(-W, W)$. The interval $[-W, W]$ has a width of $2W$. It is partitioned into $2^b$ bins of equal width. The quantization step size $\\Delta$ is:\n$$\n\\Delta = \\frac{2W}{2^b}\n$$\nThe quantization error variance is given by the integral of the squared error over the distribution of $w$:\n$$\n\\sigma_{e}^{2} = \\mathbb{E}[(Q(w)-w)^2] = \\int_{-W}^{W} (Q(w)-w)^2 p(w) dw\n$$\nThe probability density function for the uniform distribution is $p(w) = \\frac{1}{2W}$ for $w \\in [-W, W]$.\n$$\n\\sigma_{e}^{2} = \\frac{1}{2W} \\int_{-W}^{W} (Q(w)-w)^2 dw\n$$\nWe can split this integral into a sum over the $2^b$ quantization bins. For any given bin, say $B_k$, $Q(w)$ is a constant equal to the bin's center, $c_k$. Let the bin be defined by the interval $[c_k - \\Delta/2, c_k + \\Delta/2]$. The integral over this single bin is:\n$$\n\\int_{c_k - \\Delta/2}^{c_k + \\Delta/2} (c_k - w)^2 dw\n$$\nLet's make a substitution $u = w - c_k$, so $du = dw$. The limits become $-\\Delta/2$ and $\\Delta/2$.\n$$\n\\int_{-\\Delta/2}^{\\Delta/2} (-u)^2 du = \\int_{-\\Delta/2}^{\\Delta/2} u^2 du = \\left[ \\frac{u^3}{3} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{(\\Delta/2)^3 - (-\\Delta/2)^3}{3} = \\frac{2(\\Delta^3/8)}{3} = \\frac{\\Delta^3}{12}\n$$\nThis result is the same for all $2^b$ bins. The total integral is the sum over all bins:\n$$\n\\int_{-W}^{W} (Q(w)-w)^2 dw = \\sum_{k=1}^{2^b} \\frac{\\Delta^3}{12} = 2^b \\frac{\\Delta^3}{12}\n$$\nNow we substitute this back into the expression for $\\sigma_{e}^{2}$:\n$$\n\\sigma_{e}^{2} = \\frac{1}{2W} \\left( 2^b \\frac{\\Delta^3}{12} \\right)\n$$\nUsing the relation $\\Delta = \\frac{2W}{2^b}$, which means $2W = 2^b \\Delta$:\n$$\n\\sigma_{e}^{2} = \\frac{1}{2^b \\Delta} \\left( 2^b \\frac{\\Delta^3}{12} \\right) = \\frac{\\Delta^2}{12}\n$$\nFinally, we substitute $\\Delta = \\frac{2W}{2^b}$ into this expression for the variance:\n$$\n\\sigma_{e}^{2} = \\frac{1}{12} \\left( \\frac{2W}{2^b} \\right)^2 = \\frac{4W^2}{12 \\cdot (2^b)^2} = \\frac{W^2}{3 \\cdot 2^{2b}}\n$$\nWe can now write the final expression for the accuracy loss $L$:\n$$\nL = \\sigma_{e}^{2} \\operatorname{tr}(\\Sigma_{x}) = \\frac{W^2}{3 \\cdot 2^{2b}} \\operatorname{tr}(\\Sigma_{x})\n$$\nThis expression relates the accuracy loss directly to the quantization bit depth $b$, the range of the synaptic weights $W$, and the trace of the input covariance matrix $\\operatorname{tr}(\\Sigma_{x})$.",
            "answer": "$$\\boxed{\\frac{W^2 \\operatorname{tr}(\\Sigma_{x})}{3 \\cdot 2^{2b}}}$$"
        },
        {
            "introduction": "As we scale up from individual neurons to large networks, communication becomes a critical system bottleneck. Neuromorphic systems rely on efficient interconnects, like those using Address-Event Representation (AER), to route spike information. This exercise applies the principles of queueing theory to model a shared AER bus, allowing you to predict performance metrics like collision probability and latency as a function of network activity . Mastering this analysis is essential for designing communication fabrics that can support the demands of complex spiking neural network algorithms without becoming a bottleneck.",
            "id": "4046607",
            "problem": "A shared Address-Event Representation (AER) interconnect aggregates spikes from $N$ neuromorphic cores into a single contention-based bus. Each core produces spikes as a sparse point process, and by superposition the aggregate spike stream at the bus input is well modeled as a Poisson process of rate $\\Lambda$ events per second. The bus transports one fixed-length address-event at a time using a handshake protocol with deterministic transfer time $s$ seconds per event, independent of the source. When a spike arrives while the bus is already serving another event, it is deemed to experience a contention (operationally, a “collision”) and must reattempt transmission, which we approximate using queueing with eventual service. Assume stability so that $\\Lambda s < 1$.\n\nModel the bus as an $M/D/1$ queue (Markovian arrivals/Deterministic service/one server) with arrival rate $\\Lambda$ and deterministic service time $s$. Using only well-tested foundations of queueing with Poisson arrivals and deterministic service, and without assuming any formula specific to this particular system beyond these foundations:\n\n1. Starting from the Poisson Arrivals See Time Averages (PASTA) property and the definition of server utilization, derive the analytic expression for the collision probability $p_{\\mathrm{col}}(\\Lambda)$, defined as the probability that an arriving event finds the bus busy.\n\n2. Starting from the Pollaczek–Khinchine result for mean waiting in an $M/G/1$ queue and specializing it to deterministic service, derive the analytic expression for the expected reattempt delay $D_{\\mathrm{re}}(\\Lambda)$, defined as the mean time an arriving event spends waiting due to contention before its service on the bus begins.\n\nProvide your final answer as a single closed-form analytic expression for $D_{\\mathrm{re}}(\\Lambda)$ in seconds, expressed only in terms of $\\Lambda$ and $s$. No numerical evaluation is required, and no additional parameters may appear in the final expression.",
            "solution": "The problem requires the derivation of two key performance metrics for a shared bus modeled as an $M/D/1$ queue: the collision probability $p_{\\text{col}}(\\Lambda)$ and the expected reattempt delay $D_{\\text{re}}(\\Lambda)$. The system has a Poisson arrival process with rate $\\Lambda$ and a deterministic service time $s$. The stability of the queue is ensured by the condition $\\Lambda s < 1$.\n\nFirst, we derive the expression for the collision probability, $p_{\\text{col}}(\\Lambda)$. This quantity is defined as the probability that an arriving event finds the bus busy. The derivation must be based on the PASTA property and the definition of server utilization.\n\nThe server utilization, denoted by $\\rho$, represents the long-run fraction of time the server is occupied. For a single-server queue, it is calculated as the product of the arrival rate, $\\Lambda$, and the mean service time, $E[S]$. In this $M/D/1$ system, the service time is a constant value $s$, so the mean service time is $E[S] = s$. The utilization is therefore:\n$$ \\rho = \\Lambda E[S] = \\Lambda s $$\nThe arrival process is a Poisson process. A fundamental result for systems with Poisson arrivals is the PASTA property (Poisson Arrivals See Time Averages). This theorem states that the probability that an arriving customer finds the system in a particular state is equal to the long-run proportion of time the system is in that same state.\n\nThe collision probability $p_{\\text{col}}(\\Lambda)$ is precisely the probability that an arriving event finds the server (the bus) busy. Applying the PASTA property, this probability is equal to the steady-state probability that the bus is busy, as observed at an arbitrary instant in time.\n$$ p_{\\text{col}}(\\Lambda) = P(\\text{arrival finds bus busy}) \\overset{\\text{PASTA}}{=} P(\\text{bus is busy at an arbitrary time}) $$\nBy definition, the long-run proportion of time the bus is busy is the server utilization, $\\rho$. Thus:\n$$ P(\\text{bus is busy at an arbitrary time}) = \\rho $$\nCombining these statements, we conclude that the collision probability is equal to the server utilization:\n$$ p_{\\text{col}}(\\Lambda) = \\rho = \\Lambda s $$\nThis result, mandated by the stability condition $\\Lambda s < 1$, provides the probability of contention for an arriving spike.\n\nNext, we derive the expression for the expected reattempt delay, $D_{\\text{re}}(\\Lambda)$. This is defined as the mean time an arriving event must wait before its service begins, which is the standard definition of the mean waiting time in the queue, $W_q$. The derivation must start from the Pollaczek–Khinchine (P-K) formula for an $M/G/1$ queue.\n\nThe $M/G/1$ queue models a system with Poisson arrivals, a general service time distribution, and one server. The P-K formula for the mean waiting time in the queue, $W_q$, is given by:\n$$ W_q = \\frac{\\Lambda E[S^2]}{2(1 - \\rho)} $$\nwhere $\\Lambda$ is the arrival rate, $E[S^2]$ is the second moment of the service time distribution, and $\\rho = \\Lambda E[S]$ is the server utilization.\n\nTo apply this formula to our $M/D/1$ system, we must specialize the service time moments for a deterministic service time $s$. Let $S$ be the random variable representing the service time.\nFor a deterministic process, $S$ is a constant, so $S=s$.\nThe first moment, or mean, of the service time distribution is:\n$$ E[S] = s $$\nThe second moment of the service time distribution is:\n$$ E[S^2] = s^2 $$\nThe server utilization is $\\rho = \\Lambda E[S] = \\Lambda s$, which is consistent with our prior derivation.\n\nWe now substitute these specific values for the $M/D/1$ case into the general P-K formula. The expected reattempt delay $D_{\\text{re}}(\\Lambda)$ is equivalent to $W_q$:\n$$ D_{\\text{re}}(\\Lambda) = W_q = \\frac{\\Lambda E[S^2]}{2(1 - \\Lambda E[S])} = \\frac{\\Lambda (s^2)}{2(1 - \\Lambda s)} $$\nThis provides the final analytical expression for the expected reattempt delay:\n$$ D_{\\text{re}}(\\Lambda) = \\frac{\\Lambda s^2}{2(1 - \\Lambda s)} $$\nThis expression is solely in terms of the given parameters $\\Lambda$ and $s$, as requested. The stability condition $\\Lambda s < 1$ ensures a finite, non-negative waiting time.",
            "answer": "$$\\boxed{\\frac{\\Lambda s^2}{2(1 - \\Lambda s)}}$$"
        },
        {
            "introduction": "A key objective in neuromorphic engineering is minimizing energy consumption. This capstone practice integrates the concepts of computation, memory access, and communication into a comprehensive system-level energy model for a spiking neural network inference task . You will derive an expression for total energy that accounts for a multi-level memory hierarchy and activity-dependent costs, providing a powerful tool for evaluating design choices. This analysis demonstrates how co-design decisions, such as where to store synaptic weights, directly impact the overall energy efficiency of the system.",
            "id": "4046656",
            "problem": "A spiking neural network (SNN) is executed on a neuromorphic system with event-driven computation and a two-level memory hierarchy comprising Static Random Access Memory (SRAM) and Dynamic Random Access Memory (DRAM). The system-level energy model follows additivity of independent event energies and proportionality of event counts to activity: the total energy for one inference is the sum of energy for control overhead, spike routing, neuron state updates, synaptic state updates, and weight fetches, each incurred per event. Assume linear scaling of spike counts with a dimensionless firing-rate multiplier $\\rho$, such that the spike count in layer $l$ becomes $s_{l}(\\rho) = \\rho s_{l}$.\n\nNetwork specification:\n- Baseline spikes per inference: $s_{1} = 20000$, $s_{2} = 50000$, $s_{3} = 10000$.\n- Average fan-out (synaptic targets) per spike: $k_{1} = 64$, $k_{2} = 128$, $k_{3} = 256$.\n\nHardware-energy parameters (express all energies in nanojoules (nJ)):\n- Fixed control overhead per inference: $E_{0} = 50000$.\n- Router energy per spike event: $E_{\\text{route}} = 5$.\n- Neuron state update energy per spike: $E_{\\text{neuron}} = 3$.\n- SRAM read energy per $32$-bit word: $E_{\\text{SRAM,r}} = 10$.\n- SRAM write energy per $32$-bit word: $E_{\\text{SRAM,w}} = 20$.\n- DRAM read energy per $32$-bit word: $E_{\\text{DRAM,r}} = 1000$.\n- Synaptic accumulate (arithmetic) energy per synaptic event: $E_{\\text{acc}} = 4$.\n\nSoftware-hardware mapping assumptions (co-design constraints):\n- Post-synaptic membrane state is stored on-chip; each synaptic event performs one SRAM read of the state and one SRAM write back after update.\n- Layer $1$ and layer $2$ weights are stored primarily in SRAM, but layer $2$ experiences capacity-induced tiling with a DRAM spill fraction $p_{2} = 0.1$ for weight reads. Layer $1$ has $p_{1} = 0$. Layer $3$ weights reside entirely in DRAM with $p_{3} = 1$.\n- The energy cost of a synaptic event in layer $l$ includes one weight read, one post-synaptic state read, one accumulate operation, and one state write. Spike routing and neuron updates are incurred once per emitted spike.\n\nUsing only the above fundamental definitions and constraints, derive a closed-form expression for the total inference energy $E_{\\text{tot}}(\\rho)$ in nanojoules as a function of the firing-rate multiplier $\\rho$, and the sensitivity defined as the derivative $\\frac{d E_{\\text{tot}}}{d \\rho}$ evaluated at $\\rho = 1$. Your final answer must present:\n- The analytic expression for $E_{\\text{tot}}(\\rho)$ in nanojoules.\n- The single real-valued number for $\\left.\\frac{d E_{\\text{tot}}}{d \\rho}\\right|_{\\rho=1}$ in nanojoules.\nNo rounding is required; present exact integer values.",
            "solution": "The total energy for one inference, $E_{\\text{tot}}$, is the sum of a fixed control overhead and an activity-dependent component. The activity, in turn, is linearly scaled by a firing-rate multiplier $\\rho$. The total energy can be expressed as:\n$$E_{\\text{tot}}(\\rho) = E_{0} + E_{\\text{activity}}(\\rho)$$\nwhere $E_{0}$ is the fixed control overhead energy, given as $E_{0} = 50000$ nJ.\n\nThe activity-dependent energy, $E_{\\text{activity}}(\\rho)$, consists of two main contributions: energy consumed by spike events (routing and neuron updates) and energy consumed by synaptic events (weight access and state updates).\n$$E_{\\text{activity}}(\\rho) = E_{\\text{spike}}(\\rho) + E_{\\text{synapse}}(\\rho)$$\n\nFirst, we model the energy related to spike events. The number of spikes in layer $l$ is given by $s_l(\\rho) = \\rho s_l$. The total number of spikes in the network is the sum over all $3$ layers:\n$$S_{\\text{tot}}(\\rho) = \\sum_{l=1}^{3} s_{l}(\\rho) = \\rho \\sum_{l=1}^{3} s_{l}$$\nEach spike incurs an energy cost for routing ($E_{\\text{route}}$) and for the neuron state update ($E_{\\text{neuron}}$). The total energy for all spike events is:\n$$E_{\\text{spike}}(\\rho) = S_{\\text{tot}}(\\rho) (E_{\\text{route}} + E_{\\text{neuron}}) = \\rho \\left( \\sum_{l=1}^{3} s_{l} \\right) (E_{\\text{route}} + E_{\\text{neuron}})$$\n\nNext, we model the energy for synaptic events. A spike originating from layer $l$ generates $k_l$ synaptic events, where $k_l$ is the average fan-out for that layer. The number of synaptic events for layer $l$ is:\n$$N_{\\text{syn}, l}(\\rho) = s_{l}(\\rho) k_{l} = \\rho s_{l} k_{l}$$\nThe total synaptic energy is the sum of energies of all synaptic events across all layers:\n$$E_{\\text{synapse}}(\\rho) = \\sum_{l=1}^{3} N_{\\text{syn}, l}(\\rho) E_{\\text{syn}, l} = \\rho \\sum_{l=1}^{3} s_{l} k_{l} E_{\\text{syn}, l}$$\nwhere $E_{\\text{syn}, l}$ is the energy per synaptic event in layer $l$.\n\nThe energy $E_{\\text{syn}, l}$ is determined by the hardware mapping. For each synaptic event, the costs are: one weight read, one post-synaptic state read, one accumulate operation, and one post-synaptic state write. The post-synaptic state is stored in SRAM, so its access energy is $E_{\\text{SRAM,r}} + E_{\\text{SRAM,w}}$. The accumulate operation energy is $E_{\\text{acc}}$. The weight read energy depends on the layer's memory allocation, characterized by the DRAM spill fraction $p_l$. The average energy for a weight read in layer $l$ is $(1-p_l)E_{\\text{SRAM,r}} + p_l E_{\\text{DRAM,r}}$.\nTherefore, the energy per synaptic event for layer $l$ is:\n$$E_{\\text{syn}, l} = \\left( (1-p_l)E_{\\text{SRAM,r}} + p_l E_{\\text{DRAM,r}} \\right) + E_{\\text{SRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}}$$\n$$E_{\\text{syn}, l} = (2-p_l)E_{\\text{SRAM,r}} + p_l E_{\\text{DRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}}$$\n\nCombining all components, the total energy $E_{\\text{tot}}(\\rho)$ is:\n$$E_{\\text{tot}}(\\rho) = E_{0} + \\rho \\left[ \\left( \\sum_{l=1}^{3} s_{l} \\right) (E_{\\text{route}} + E_{\\text{neuron}}) + \\sum_{l=1}^{3} s_{l} k_{l} E_{\\text{syn}, l} \\right]$$\n\nNow, we substitute the given values.\nThe spike counts are $s_{1} = 20000$, $s_{2} = 50000$, and $s_{3} = 10000$.\nThe fan-outs are $k_{1} = 64$, $k_{2} = 128$, and $k_{3} = 256$.\nThe energy parameters are $E_{0}=50000$, $E_{\\text{route}}=5$, $E_{\\text{neuron}}=3$, $E_{\\text{SRAM,r}}=10$, $E_{\\text{SRAM,w}}=20$, $E_{\\text{DRAM,r}}=1000$, and $E_{\\text{acc}}=4$ (all in nJ).\nThe DRAM spill fractions are $p_{1} = 0$, $p_{2} = 0.1$, and $p_{3} = 1$.\n\nFirst, calculate the energy per synaptic event for each layer:\nFor layer $l=1$: $p_{1} = 0$.\n$$E_{\\text{syn}, 1} = (2-0)E_{\\text{SRAM,r}} + 0 \\cdot E_{\\text{DRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}} = 2(10) + 20 + 4 = 44 \\text{ nJ}$$\nFor layer $l=2$: $p_{2} = 0.1$.\n$$E_{\\text{syn}, 2} = (2-0.1)E_{\\text{SRAM,r}} + 0.1 E_{\\text{DRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}} = 1.9(10) + 0.1(1000) + 20 + 4 = 19 + 100 + 24 = 143 \\text{ nJ}$$\nFor layer $l=3$: $p_{3} = 1$.\n$$E_{\\text{syn}, 3} = (2-1)E_{\\text{SRAM,r}} + 1 \\cdot E_{\\text{DRAM,r}} + E_{\\text{SRAM,w}} + E_{\\text{acc}} = 1(10) + 1(1000) + 20 + 4 = 10 + 1000 + 24 = 1034 \\text{ nJ}$$\n\nNext, we calculate the coefficient of $\\rho$, which represents the total energy per unit of baseline activity.\nTotal spikes at baseline: $\\sum s_l = 20000 + 50000 + 10000 = 80000$.\nEnergy per spike event: $E_{\\text{route}} + E_{\\text{neuron}} = 5 + 3 = 8$ nJ.\nTotal spike-related energy coefficient:\n$$C_{\\text{spike}} = \\left( \\sum_{l=1}^{3} s_{l} \\right) (E_{\\text{route}} + E_{\\text{neuron}}) = 80000 \\times 8 = 640000 \\text{ nJ}$$\nTotal synapse-related energy coefficient:\n$$C_{\\text{synapse}} = \\sum_{l=1}^{3} s_{l} k_{l} E_{\\text{syn}, l} = s_{1}k_{1}E_{\\text{syn}, 1} + s_{2}k_{2}E_{\\text{syn}, 2} + s_{3}k_{3}E_{\\text{syn}, 3}$$\n$$C_{\\text{synapse}} = (20000 \\times 64 \\times 44) + (50000 \\times 128 \\times 143) + (10000 \\times 256 \\times 1034)$$\n$$C_{\\text{synapse}} = 56320000 + 915200000 + 2647040000 = 3618560000 \\text{ nJ}$$\nThe total activity-dependent energy coefficient is the sum of these two:\n$$C_{\\text{activity}} = C_{\\text{spike}} + C_{\\text{synapse}} = 640000 + 3618560000 = 3619200000 \\text{ nJ}$$\n\nThe closed-form expression for the total inference energy is thus:\n$$E_{\\text{tot}}(\\rho) = 50000 + 3619200000 \\rho$$\n\nThe sensitivity is the derivative of the total energy with respect to the firing-rate multiplier $\\rho$:\n$$\\frac{d E_{\\text{tot}}}{d \\rho} = \\frac{d}{d \\rho} (50000 + 3619200000 \\rho) = 3619200000$$\nThis derivative is constant. Its value evaluated at $\\rho = 1$ is:\n$$\\left.\\frac{d E_{\\text{tot}}}{d \\rho}\\right|_{\\rho=1} = 3619200000$$\n\nThe first result is the analytic expression for $E_{\\text{tot}}(\\rho)$, and the second is the numerical value for the sensitivity at $\\rho=1$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n50000 + 3619200000 \\rho & 3619200000\n\\end{pmatrix}\n}\n$$"
        }
    ]
}