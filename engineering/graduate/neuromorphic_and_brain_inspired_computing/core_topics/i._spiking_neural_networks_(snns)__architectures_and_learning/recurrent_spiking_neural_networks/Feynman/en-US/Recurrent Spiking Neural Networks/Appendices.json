{
    "hands_on_practices": [
        {
            "introduction": "A robust simulation of a spiking neuron is the bedrock of any SNN model. This first practice focuses on the numerical methods used to integrate the neuron's continuous-time dynamics. You will derive and implement both an exact integration scheme and the more common, but less stable, forward Euler approximation for a conductance-based neuron, giving you direct insight into the critical trade-offs between computational accuracy and efficiency that underpin all SNN research. ",
            "id": "4056993",
            "problem": "Consider a single-compartment leaky integrate-and-fire neuron with conductance-based synaptic input embedded in a recurrent spiking neural network (SNN). The neuron has membrane capacitance $C$ and leak conductance $g_L$ with leak reversal potential $E_L$. A single effective synaptic conductance $g(t)$ drives the membrane toward a synaptic reversal potential $E_{\\mathrm{rev}}$. The synaptic conductance evolves according to first-order decay with time constant $\\tau_s$ and receives impulsive increments proportional to a synaptic weight $w$ upon presynaptic spike arrivals represented by a discrete spike indicator $s_t \\in \\{0,1\\}$ at discrete time $t$. The membrane voltage is denoted $V(t)$. Assume that spike generation and reset are disabled for the purpose of stability analysis (that is, the system is analyzed in the subthreshold regime without threshold or reset).\n\nStarting from first principles and well-tested facts:\n- The membrane obeys the current balance equation $C \\frac{dV}{dt} = -g_L \\left(V - E_L\\right) - g(t)\\left(V - E_{\\mathrm{rev}}\\right) + I_{\\mathrm{ext}}$, where $I_{\\mathrm{ext}}$ is an external injected current.\n- The synaptic conductance obeys first-order decay between spikes.\n- Use a discrete-time step of size $\\Delta t > 0$ and assume an operator-splitting scheme in which $g(t)$ is treated as piecewise constant over each time step when updating $V(t)$.\n\nTasks:\n1. Derive a closed-form discrete-time update for the synaptic conductance that captures exponential decay over one time step $\\Delta t$ and instantaneous increments due to $s_t$. Express $g_{t+1}$ in terms of $g_t$, $\\Delta t$, $\\tau_s$, $w$, and $s_t$.\n2. Derive the exact discrete-time update of the membrane voltage over $\\Delta t$ under the assumption that $g(t)$ is frozen at $g_t$ during the voltage update. Express $V_{t+1}$ in terms of $V_t$, $g_t$, $g_L$, $E_L$, $E_{\\mathrm{rev}}$, $I_{\\mathrm{ext}}$, $C$, and $\\Delta t$. Your derivation should start from the linear ordinary differential equation and use exact integration without introducing auxiliary shortcut formulas not derived from the stated base.\n3. Derive the explicit forward Euler update for $V_{t+1}$ corresponding to the same continuous-time equation. Starting from the definition of the forward Euler method as a first-order discretization of the derivative, give the resulting discrete-time map.\n4. Analyze the stability of the voltage updates under large synaptic conductance $g_t$. For the exact update, determine whether the mapping is contractive for any $g_t \\ge 0$ and $\\Delta t > 0$. For the forward Euler update, derive a condition on $\\Delta t$, $C$, $g_L$, and $g_t$ under which the mapping is stable in the sense that small perturbations in $V_t$ decay over iterations. Provide the explicit upper bound on $\\Delta t$ that guarantees stability for given $C$, $g_L$, and $g_t$.\n5. Implement a program that computes, for each test case below:\n   - The synaptic conductance $g_{t+1}$ in siemens ($\\mathrm{S}$).\n   - The exact voltage update $V_{t+1}^{\\mathrm{exact}}$ in volts ($\\mathrm{V}$).\n   - The forward Euler voltage update $V_{t+1}^{\\mathrm{Euler}}$ in volts ($\\mathrm{V}$).\n   - A boolean $s_{\\mathrm{exact}}$ indicating whether the exact update is contractive (stable) for the given parameters.\n   - A boolean $s_{\\mathrm{Euler}}$ indicating whether the forward Euler update is stable for the given parameters according to your derived condition.\n   - The maximum stable step size $\\Delta t_{\\max}^{\\mathrm{Euler}}$ for forward Euler in seconds ($\\mathrm{s}$), expressed as a nonnegative real number.\n\nExpress voltages in volts ($\\mathrm{V}$), conductances in siemens ($\\mathrm{S}$), capacitance in farads ($\\mathrm{F}$), time in seconds ($\\mathrm{s}$), and current in amperes ($\\mathrm{A}$). All outputs must be real numbers or booleans; do not use percentage signs.\n\nTest suite:\n- Case A (nominal excitatory input, stable forward Euler): $\\Delta t = 10^{-4}\\,\\mathrm{s}$, $\\tau_s = 5\\times 10^{-3}\\,\\mathrm{s}$, $g_L = 10\\times 10^{-9}\\,\\mathrm{S}$, $C = 200\\times 10^{-12}\\,\\mathrm{F}$, $E_L = -6.5\\times 10^{-2}\\,\\mathrm{V}$, $E_{\\mathrm{rev}} = 0\\,\\mathrm{V}$, $I_{\\mathrm{ext}} = 0\\,\\mathrm{A}$, $w = 5\\times 10^{-9}\\,\\mathrm{S}$, $s_t = 1$, $g_t = 30\\times 10^{-9}\\,\\mathrm{S}$, $V_t = -7.0\\times 10^{-2}\\,\\mathrm{V}$.\n- Case B (large synaptic conductance, forward Euler unstable): $\\Delta t = 5\\times 10^{-4}\\,\\mathrm{s}$, $\\tau_s = 5\\times 10^{-3}\\,\\mathrm{s}$, $g_L = 10\\times 10^{-9}\\,\\mathrm{S}$, $C = 200\\times 10^{-12}\\,\\mathrm{F}$, $E_L = -6.5\\times 10^{-2}\\,\\mathrm{V}$, $E_{\\mathrm{rev}} = 0\\,\\mathrm{V}$, $I_{\\mathrm{ext}} = 0\\,\\mathrm{A}$, $w = 5\\times 10^{-9}\\,\\mathrm{S}$, $s_t = 0$, $g_t = 2\\times 10^{-6}\\,\\mathrm{S}$, $V_t = -7.0\\times 10^{-2}\\,\\mathrm{V}$.\n- Case C (boundary condition for forward Euler stability with zero synaptic conductance): $\\Delta t = \\frac{2C}{g_L}\\,\\mathrm{s}$, $\\tau_s = 5\\times 10^{-3}\\,\\mathrm{s}$, $g_L = 10\\times 10^{-9}\\,\\mathrm{S}$, $C = 200\\times 10^{-12}\\,\\mathrm{F}$, $E_L = -6.5\\times 10^{-2}\\,\\mathrm{V}$, $E_{\\mathrm{rev}} = -8.0\\times 10^{-2}\\,\\mathrm{V}$, $I_{\\mathrm{ext}} = 0\\,\\mathrm{A}$, $w = 5\\times 10^{-9}\\,\\mathrm{S}$, $s_t = 0$, $g_t = 0\\,\\mathrm{S}$, $V_t = -6.5\\times 10^{-2}\\,\\mathrm{V}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, append the six quantities in the order specified in Task 5. The final output should therefore be a flat list of length $18$ containing, in order, the six results from Case A, followed by the six results from Case B, followed by the six results from Case C. For example, the output must look like $[x_1,x_2,x_3,x_4,x_5,x_6,x_7,\\ldots,x_{18}]$ where each $x_i$ is either a boolean or a real number expressed in SI units as appropriate.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It presents a standard subthreshold model of a leaky integrate-and-fire neuron with conductance-based synapses and asks for specific, formalizable derivations and calculations related to its discrete-time simulation and numerical stability. The parameters provided are physically realistic and the tasks are logically consistent. The problem is deemed valid.\n\nHerein, we derive the requested update equations and stability conditions from first principles.\n\n### Task 1: Synaptic Conductance Update\n\nThe dynamics of the synaptic conductance $g(t)$ are governed by first-order decay between spikes. The corresponding ordinary differential equation (ODE) is:\n$$\n\\frac{dg}{dt} = -\\frac{g}{\\tau_s}\n$$\nwhere $\\tau_s$ is the synaptic time constant. To find the evolution of $g$ over a discrete time step $\\Delta t$, we integrate this ODE from time $t$ to $t+\\Delta t$. Separating variables, we get:\n$$\n\\int_{g_t}^{g(t+\\Delta t)} \\frac{dg}{g} = -\\frac{1}{\\tau_s} \\int_{t}^{t+\\Delta t} dt'\n$$\n$$\n\\ln(g(t+\\Delta t)) - \\ln(g_t) = -\\frac{\\Delta t}{\\tau_s}\n$$\n$$\n\\ln\\left(\\frac{g(t+\\Delta t)}{g_t}\\right) = -\\frac{\\Delta t}{\\tau_s}\n$$\nExponentiating both sides yields the decay part of the update:\n$$\ng(t+\\Delta t) = g_t e^{-\\Delta t/\\tau_s}\n$$\nThe problem states that upon the arrival of a presynaptic spike at time $t$, indicated by $s_t=1$, there is an impulsive increment of size $w$. This increment is added to the decayed conductance to obtain the value at the next time step, $g_{t+1}$. The full discrete-time update equation for the synaptic conductance $g_t$ is therefore:\n$$\ng_{t+1} = g_t e^{-\\Delta t/\\tau_s} + w s_t\n$$\n\n### Task 2: Exact Membrane Voltage Update\n\nThe membrane potential $V(t)$ is governed by the current balance equation:\n$$\nC \\frac{dV}{dt} = -g_L (V - E_L) - g(t)(V - E_{\\mathrm{rev}}) + I_{\\mathrm{ext}}\n$$\nAccording to the operator-splitting scheme, we assume the synaptic conductance $g(t)$ is held constant at its value $g_t$ over the interval $[t, t+\\Delta t]$. The ODE for the voltage becomes a linear first-order equation:\n$$\nC \\frac{dV}{dt} = -g_L (V - E_L) - g_t(V - E_{\\mathrm{rev}}) + I_{\\mathrm{ext}}\n$$\nWe rearrange the terms to group the multiples of $V$:\n$$\nC \\frac{dV}{dt} = -g_L V + g_L E_L - g_t V + g_t E_{\\mathrm{rev}} + I_{\\mathrm{ext}}\n$$\n$$\nC \\frac{dV}{dt} = -(g_L + g_t)V + (g_L E_L + g_t E_{\\mathrm{rev}} + I_{\\mathrm{ext}})\n$$\nLet's define the total effective conductance $G_{\\mathrm{tot}} = g_L + g_t$ and the total input current term $I_{\\mathrm{eff}} = g_L E_L + g_t E_{\\mathrm{rev}} + I_{\\mathrm{ext}}$. The equation simplifies to:\n$$\n\\frac{dV}{dt} = -\\frac{G_{\\mathrm{tot}}}{C}V + \\frac{I_{\\mathrm{eff}}}{C}\n$$\nThis is a standard linear ODE of the form $\\frac{dV}{dt} = aV+b$. Let's rewrite it to highlight the steady-state voltage $V_{\\infty}$ and effective time constant $\\tau_{\\mathrm{eff}}$:\n$$\n\\frac{dV}{dt} = -\\frac{g_L+g_t}{C} \\left( V - \\frac{g_L E_L + g_t E_{\\mathrm{rev}} + I_{\\mathrm{ext}}}{g_L+g_t} \\right)\n$$\nHere, the effective time constant is $\\tau_{\\mathrm{eff}} = \\frac{C}{g_L+g_t}$ and the steady-state voltage is $V_{\\infty} = \\frac{g_L E_L + g_t E_{\\mathrm{rev}} + I_{\\mathrm{ext}}}{g_L+g_t}$. The ODE is $\\frac{dV}{dt} = -\\frac{1}{\\tau_{\\mathrm{eff}}}(V - V_{\\infty})$.\nThe exact solution to this ODE with initial condition $V(t) = V_t$ is:\n$$\nV(t') = V_{\\infty} + (V_t - V_{\\infty}) e^{-(t'-t)/\\tau_{\\mathrm{eff}}}\n$$\nTo find the voltage at the next time step, we evaluate this at $t' = t+\\Delta t$:\n$$\nV_{t+1} = V_{\\infty} + (V_t - V_{\\infty}) e^{-\\Delta t/\\tau_{\\mathrm{eff}}}\n$$\nSubstituting back the definitions of $V_{\\infty}$ and $\\tau_{\\mathrm{eff}}$, we obtain the exact discrete-time update rule for the voltage:\n$$\nV_{t+1} = \\frac{g_L E_L + g_t E_{\\mathrm{rev}} + I_{\\mathrm{ext}}}{g_L+g_t} + \\left( V_t - \\frac{g_L E_L + g_t E_{\\mathrm{rev}} + I_{\\mathrm{ext}}}{g_L+g_t} \\right) \\exp\\left(-\\frac{(g_L+g_t)\\Delta t}{C}\\right)\n$$\nThis update is valid provided $g_L+g_t \\neq 0$. In the problem, $g_L > 0$, so this is always satisfied.\n\n### Task 3: Forward Euler Voltage Update\n\nThe forward Euler method approximates the solution to an ODE $\\frac{dy}{dt} = f(y,t)$ using the discrete update $y_{t+1} = y_t + \\Delta t \\cdot f(y_t, t)$. For our voltage equation, the function $f(V_t, t)$ is:\n$$\nf(V_t, t) = \\frac{dV}{dt}\\bigg|_{V=V_t} = \\frac{1}{C}\\left(-g_L(V_t - E_L) - g_t(V_t - E_{\\mathrm{rev}}) + I_{\\mathrm{ext}}\\right)\n$$\nApplying the forward Euler formula, we get:\n$$\nV_{t+1}^{\\mathrm{Euler}} = V_t + \\Delta t \\cdot \\frac{1}{C}\\left(-g_L(V_t - E_L) - g_t(V_t - E_{\\mathrm{rev}}) + I_{\\mathrm{ext}}\\right)\n$$\nThis is the explicit forward Euler update for the membrane voltage.\n\n### Task 4: Stability Analysis\n\nA discrete-time linear map of the form $x_{k+1} = A x_k + B$ is stable if small perturbations decay, which requires the magnitude of the multiplier $A$ to be strictly less than $1$, i.e., $|A|<1$. Such a map is known as a contraction mapping.\n\n**Stability of the Exact Update:**\nThe exact update rule can be written in the form $V_{t+1} = A_{\\mathrm{exact}}V_t + B_{\\mathrm{exact}}$, where the multiplier is:\n$$\nA_{\\mathrm{exact}} = \\exp\\left(-\\frac{(g_L+g_t)\\Delta t}{C}\\right)\n$$\nAccording to the problem statement and typical physical constraints, we have $C > 0$, $\\Delta t > 0$, $g_L > 0$, and $g_t \\ge 0$. Therefore, the total conductance $g_L+g_t$ is strictly positive. The argument of the exponential, $-\\frac{(g_L+g_t)\\Delta t}{C}$, is always a finite, strictly negative number. For any strictly negative argument $x < 0$, its exponential $e^x$ satisfies $0 < e^x < 1$. Thus, we have:\n$$\n0 < A_{\\mathrm{exact}} < 1\n$$\nSince $|A_{\\mathrm{exact}}| < 1$ for all valid parameter sets, the exact integration scheme is always contractive and therefore unconditionally stable.\n\n**Stability of the Forward Euler Update:**\nThe forward Euler update can be rearranged into the form $V_{t+1}^{\\mathrm{Euler}} = A_{\\mathrm{Euler}}V_t + B_{\\mathrm{Euler}}$. First, we isolate the terms multiplying $V_t$:\n$$\nV_{t+1}^{\\mathrm{Euler}} = V_t - \\frac{\\Delta t}{C}(g_L+g_t)V_t + \\frac{\\Delta t}{C}(g_L E_L + g_t E_{\\mathrm{rev}} + I_{\\mathrm{ext}})\n$$\nThe multiplier is:\n$$\nA_{\\mathrm{Euler}} = 1 - \\frac{(g_L+g_t)\\Delta t}{C}\n$$\nFor stability, we require $|A_{\\mathrm{Euler}}| < 1$, which translates to the two inequalities:\n$$\n-1 < 1 - \\frac{(g_L+g_t)\\Delta t}{C} \\quad \\text{and} \\quad 1 - \\frac{(g_L+g_t)\\Delta t}{C} < 1\n$$\nThe second inequality, $-\\frac{(g_L+g_t)\\Delta t}{C} < 0$, is always true since $g_L+g_t > 0$, $\\Delta t > 0$, and $C>0$.\nThe first inequality yields the stability condition:\n$$\n-2 < -\\frac{(g_L+g_t)\\Delta t}{C}\n$$\n$$\n2 > \\frac{(g_L+g_t)\\Delta t}{C}\n$$\n$$\n\\Delta t < \\frac{2C}{g_L+g_t}\n$$\nThis is the condition on the time step $\\Delta t$ for the forward Euler scheme to be stable for given $C$, $g_L$, and $g_t$. The explicit upper bound on $\\Delta t$ for stability is:\n$$\n\\Delta t_{\\max}^{\\mathrm{Euler}} = \\frac{2C}{g_L+g_t}\n$$\nThe forward Euler update is stable if $\\Delta t < \\Delta t_{\\max}^{\\mathrm{Euler}}$. At $\\Delta t = \\Delta t_{\\max}^{\\mathrm{Euler}}$, the multiplier $A_{\\mathrm{Euler}}$ becomes $-1$, leading to marginal stability (oscillations) where perturbations do not decay.\n\n### Task 5: Implementation\n\nThe following program implements the derived formulas to compute the six required quantities for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes dynamics and stability for a leaky integrate-and-fire neuron model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Nominal excitatory input, stable Forward Euler\n        {\n            \"delta_t\": 1e-4, \"tau_s\": 5e-3, \"g_L\": 10e-9, \"C\": 200e-12,\n            \"E_L\": -6.5e-2, \"E_rev\": 0.0, \"I_ext\": 0.0, \"w\": 5e-9,\n            \"s_t\": 1, \"g_t\": 30e-9, \"V_t\": -7.0e-2\n        },\n        # Case B: Large synaptic conductance, Forward Euler unstable\n        {\n            \"delta_t\": 5e-4, \"tau_s\": 5e-3, \"g_L\": 10e-9, \"C\": 200e-12,\n            \"E_L\": -6.5e-2, \"E_rev\": 0.0, \"I_ext\": 0.0, \"w\": 5e-9,\n            \"s_t\": 0, \"g_t\": 2e-6, \"V_t\": -7.0e-2\n        },\n        # Case C: Boundary condition for Forward Euler stability\n        {\n            # delta_t is calculated from other params for this case\n            \"tau_s\": 5e-3, \"g_L\": 10e-9, \"C\": 200e-12,\n            \"E_L\": -6.5e-2, \"E_rev\": -8.0e-2, \"I_ext\": 0.0, \"w\": 5e-9,\n            \"s_t\": 0, \"g_t\": 0.0, \"V_t\": -6.5e-2\n        }\n    ]\n    # Special calculation for case C delta_t\n    test_cases[2][\"delta_t\"] = 2 * test_cases[2][\"C\"] / test_cases[2][\"g_L\"]\n\n    results = []\n    for params in test_cases:\n        delta_t = params[\"delta_t\"]\n        tau_s = params[\"tau_s\"]\n        g_L = params[\"g_L\"]\n        C = params[\"C\"]\n        E_L = params[\"E_L\"]\n        E_rev = params[\"E_rev\"]\n        I_ext = params[\"I_ext\"]\n        w = params[\"w\"]\n        s_t = params[\"s_t\"]\n        g_t = params[\"g_t\"]\n        V_t = params[\"V_t\"]\n\n        # 1. Synaptic conductance g_{t+1}\n        g_tp1 = g_t * np.exp(-delta_t / tau_s) + w * s_t\n\n        # 2. Exact voltage update V_{t+1}^{exact}\n        G_tot = g_L + g_t\n        if G_tot > 0:\n            V_inf = (g_L * E_L + g_t * E_rev + I_ext) / G_tot\n            tau_eff = C / G_tot\n            V_tp1_exact = V_inf + (V_t - V_inf) * np.exp(-delta_t / tau_eff)\n        else: # Should not happen with g_L > 0\n            V_tp1_exact = V_t + (I_ext / C) * delta_t\n\n        # 3. Forward Euler voltage update V_{t+1}^{Euler}\n        dVdt_term = -g_L * (V_t - E_L) - g_t * (V_t - E_rev) + I_ext\n        V_tp1_euler = V_t + (delta_t / C) * dVdt_term\n\n        # 4. Stability of exact update (s_exact)\n        # As derived, it is unconditionally stable for g_L > 0, C > 0, delta_t > 0\n        # The multiplier is strictly between 0 and 1, so it is contractive.\n        s_exact = True\n\n        # 5. Stability of Forward Euler update (s_Euler) and max step size\n        # Must compute dt_max first to determine s_Euler\n        if G_tot > 0:\n            dt_max_euler = 2 * C / G_tot\n        else: # Unconditionally stable\n            dt_max_euler = float('inf')\n        \n        # Stability requires strict inequality\n        s_euler = delta_t  dt_max_euler\n        \n        # The order for results is specified in the problem\n        results.append(g_tp1)\n        results.append(V_tp1_exact)\n        results.append(V_tp1_euler)\n        results.append(s_exact)\n        results.append(s_euler)\n        results.append(dt_max_euler)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we can simulate a neuron, the next step is to enable it to learn. This exercise tackles one of the central challenges in training SNNs: the non-differentiable nature of the spike generation process. You will implement a Straight-Through Estimator (STE), a foundational surrogate gradient technique, and analyze its effect on gradient propagation, providing a tangible understanding of how to apply gradient-based learning to recurrent SNNs and mitigate the problem of vanishing gradients. ",
            "id": "4056952",
            "problem": "Consider a single-neuron recurrent Spiking Neural Network (SNN), where the membrane potential in discrete time is governed by the leaky recurrence\n$$\nV_{t+1} = \\alpha V_t + w_r\\, s(V_t) + b,\n$$\nfor $t = 0,1,\\dots,T-1$, with initial potential $V_0$. Here $s(V)$ is the binary spike function given by the Heaviside step function $H(\\cdot)$, namely $s(V) = H(V - V_{th})$ where $V_{th}$ is the spike threshold. The constants are the leak factor $\\alpha \\in (0,1)$, the recurrent weight $w_r \\in \\mathbb{R}$, and the bias $b \\in \\mathbb{R}$. We consider Backpropagation Through Time (BPTT) to analyze vanishing gradients.\n\nIntroduce a Straight-Through Estimator (STE) for the derivative of the spike function such that\n$$\n\\frac{\\partial s}{\\partial V}(V) \\approx g(V) = \\begin{cases}\n1,  \\text{if } |V - V_{th}| \\le \\delta,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\nwith window half-width $\\delta > 0$. This surrogate derivative models learnability only when the membrane potential is in a small window around threshold. For gradient flow analysis across time, consider the Jacobian factor at time $t$\n$$\nJ_t = \\frac{\\partial V_{t+1}}{\\partial V_t}.\n$$\nUse only fundamental principles, specifically the chain rule from calculus and the recurrence definition, to build an explicit computational method for the magnitude of the gradient propagated through time,\n$$\nG = \\left|\\frac{\\partial V_T}{\\partial V_0}\\right|.\n$$\nAlso compute the baseline magnitude $G_0$ obtained when no surrogate gradient is used, i.e., when $\\partial s/\\partial V \\equiv 0$. Finally, report the ratio\n$$\nR = \\frac{G}{G_0},\n$$\nwhich quantifies the effect of the STE on vanishing gradients across $T$ timesteps.\n\nYour program must:\n- Implement the forward dynamics for $V_t$ exactly as specified.\n- Implement the surrogate derivative $g(V)$ with the indicator window defined above.\n- Compute $J_t$ from first principles using the above definitions, then compute $G$ via the chain rule as the product of $|J_t|$ across time, and compute $G_0$ consistently with the no-STE setting.\n- Return the ratio $R$ for each test case.\n\nYou must not introduce any additional dynamics (e.g., hard or soft reset terms) beyond what is specified.\n\nTest suite:\nProvide the results for the following five parameter sets $(T,\\alpha,w_r,b,V_{th},\\delta,V_0)$:\n\n1. General case with threshold crossing and positive recurrent feedback (happy path):\n   - $T = 60$, $\\alpha = 0.9$, $w_r = 0.4$, $b = 0.12$, $V_{th} = 1.0$, $\\delta = 0.05$, $V_0 = 0.0$.\n\n2. Window deactivated (boundary case for STE neutrality):\n   - $T = 60$, $\\alpha = 0.9$, $w_r = 0.4$, $b = 0.12$, $V_{th} = 1.0$, $\\delta = 0.0$, $V_0 = 0.0$.\n\n3. Strong vanishing baseline with large horizon (edge case):\n   - $T = 100$, $\\alpha = 0.5$, $w_r = 0.8$, $b = 0.8$, $V_{th} = 1.0$, $\\delta = 0.02$, $V_0 = 0.0$.\n\n4. Negative recurrent weight (adverse effect case):\n   - $T = 80$, $\\alpha = 0.9$, $w_r = -0.7$, $b = 0.15$, $V_{th} = 1.0$, $\\delta = 0.05$, $V_0 = 0.0$.\n\n5. No recurrent feedback (control case):\n   - $T = 120$, $\\alpha = 0.99$, $w_r = 0.0$, $b = 0.0105$, $V_{th} = 1.0$, $\\delta = 0.05$, $V_0 = 0.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be the float $R$ for the corresponding test case, in the same order as listed above. No other text should be printed.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of computational neuroscience and machine learning, well-posed with a clear and unique solvable structure, and objective in its mathematical formulation. We will proceed with a full solution.\n\nThe problem requires a quantitative analysis of the Straight-Through Estimator (STE) on gradient propagation in a single-neuron recurrent Spiking Neural Network (SNN). The core of the analysis is to compute the ratio, $R$, of the gradient magnitude with the STE to a baseline gradient magnitude without it.\n\nFirst, we formalize the components of the problem. The neuron's membrane potential, $V_t$, evolves according to the discrete-time recurrence relation:\n$$\nV_{t+1} = \\alpha V_t + w_r s(V_t) + b\n$$\nwhere $t \\in \\{0, 1, \\dots, T-1\\}$ is the time step, $\\alpha \\in (0,1)$ is the leak factor, $w_r$ is the recurrent synaptic weight, and $b$ is a constant bias current. The spiking behavior is governed by the function $s(V_t)$, which is defined using the Heaviside step function $H(\\cdot)$ as:\n$$\ns(V_t) = H(V_t - V_{th}) = \\begin{cases}\n1,  \\text{if } V_t \\ge V_{th} \\\\\n0,  \\text{if } V_t  V_{th}\n\\end{cases}\n$$\nHere, $V_{th}$ is the fixed spike threshold.\n\nTo train such a network using gradient-based methods, we need to backpropagate gradients through time (BPTT). The quantity of interest is the gradient of the final state $V_T$ with respect to the initial state $V_0$, which is $\\frac{\\partial V_T}{\\partial V_0}$. Using the chain rule, this gradient is expressed as a product of Jacobian factors:\n$$\n\\frac{\\partial V_T}{\\partial V_0} = \\frac{\\partial V_T}{\\partial V_{T-1}} \\frac{\\partial V_{T-1}}{\\partial V_{T-2}} \\cdots \\frac{\\partial V_1}{\\partial V_0} = \\prod_{t=0}^{T-1} \\frac{\\partial V_{t+1}}{\\partial V_t}\n$$\nLet's define the Jacobian factor at time $t$ as $J_t = \\frac{\\partial V_{t+1}}{\\partial V_t}$. We can compute this by differentiating the recurrence relation with respect to $V_t$:\n$$\nJ_t = \\frac{\\partial}{\\partial V_t} (\\alpha V_t + w_r s(V_t) + b) = \\alpha + w_r \\frac{\\partial s(V_t)}{\\partial V_t}\n$$\nThe derivative of the Heaviside step function, $\\frac{\\partial s(V_t)}{\\partial V_t}$, is zero everywhere except at the threshold, where it is undefined (mathematically, a Dirac delta function). In a practical BPTT implementation, this derivative is taken to be zero, which stalls learning. The Straight-Through Estimator (STE) is a technique to overcome this by substituting a surrogate function for the derivative. The problem specifies the following surrogate derivative, $g(V)$:\n$$\ng(V) = \\frac{\\partial s}{\\partial V}(V) \\approx \\begin{cases}\n1,  \\text{if } |V - V_{th}| \\le \\delta \\\\\n0,  \\text{otherwise}\n\\end{cases}\n$$\nwhere $\\delta > 0$ defines a small window around the threshold $V_{th}$. Substituting $g(V_t)$ for $\\frac{\\partial s(V_t)}{\\partial V_t}$, the Jacobian factor becomes:\n$$\nJ_t = \\alpha + w_r g(V_t)\n$$\nThe total gradient magnitude propagated through $T$ time steps, which we denote as $G$, is the absolute value of the product of these Jacobian factors:\n$$\nG = \\left| \\prod_{t=0}^{T-1} J_t \\right| = \\prod_{t=0}^{T-1} |J_t| = \\prod_{t=0}^{T-1} |\\alpha + w_r g(V_t)|\n$$\nTo compute $G$, we must first perform a forward pass to simulate the trajectory of $V_t$ for $t \\in [0, T]$. Then, for each time step $t \\in [0, T-1]$, we evaluate $g(V_t)$ and compute the corresponding term in the product.\n\nNext, we must compute the baseline gradient magnitude, $G_0$. This is defined as the case where no surrogate gradient is used, explicitly stated as $\\frac{\\partial s}{\\partial V} \\equiv 0$. This implies that for the baseline case, $g(V_t) = 0$ for all $t$. The Jacobian factor, $J_{0,t}$, simplifies to:\n$$\nJ_{0,t} = \\alpha + w_r \\cdot 0 = \\alpha\n$$\nThe baseline gradient magnitude $G_0$ is then:\n$$\nG_0 = \\prod_{t=0}^{T-1} |\\alpha| = |\\alpha|^T\n$$\nSince the problem states $\\alpha \\in (0,1)$, we have $G_0 = \\alpha^T$. This term decays exponentially with $T$, representing the canonical vanishing gradient problem in recurrent networks.\n\nFinally, the problem asks for the ratio $R = \\frac{G}{G_0}$, which quantifies the impact of the STE on gradient flow:\n$$\nR = \\frac{\\prod_{t=0}^{T-1} |\\alpha + w_r g(V_t)|}{\\alpha^T}\n$$\nThis ratio measures how many times the STE either amplifies ($|J_t| > \\alpha$) or attenuates ($|J_t|  \\alpha$) the gradient flow compared to the baseline decay. The computational procedure is as follows:\n1.  For a given set of parameters $(T, \\alpha, w_r, b, V_{th}, \\delta, V_0)$, simulate the neuron's dynamics from $t=0$ to $T-1$ to obtain the full membrane potential trajectory $\\{V_0, V_1, \\dots, V_T\\}$.\n2.  Calculate $G$ by iterating from $t=0$ to $T-1$, evaluating $g(V_t)$ at each step, computing $J_t = \\alpha + w_r g(V_t)$, and multiplying the accumulated product by $|J_t|$.\n3.  Calculate $G_0 = \\alpha^T$.\n4. Compute the ratio $R = G / G_0$.\nThis procedure is implemented for each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ratio of gradient magnitudes for a single-neuron SNN\n    with and without a Straight-Through Estimator (STE).\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, alpha, w_r, b, V_th, delta, V_0)\n        (60, 0.9, 0.4, 0.12, 1.0, 0.05, 0.0),      # Case 1: General case\n        (60, 0.9, 0.4, 0.12, 1.0, 0.0, 0.0),       # Case 2: Window deactivated\n        (100, 0.5, 0.8, 0.8, 1.0, 0.02, 0.0),     # Case 3: Strong vanishing baseline\n        (80, 0.9, -0.7, 0.15, 1.0, 0.05, 0.0),     # Case 4: Negative recurrent weight\n        (120, 0.99, 0.0, 0.0105, 1.0, 0.05, 0.0)  # Case 5: No recurrent feedback\n    ]\n\n    results = []\n    for case in test_cases:\n        T, alpha, w_r, b, V_th, delta, V_0 = case\n\n        # Step 1: Forward pass to compute the membrane potential trajectory V_t.\n        # V is a numpy array of size T+1 to store V_0 to V_T.\n        V = np.zeros(T + 1)\n        V[0] = V_0\n\n        for t in range(T):\n            # The spike function s(V) is the Heaviside step function H(V - V_th).\n            # H(x) is 1 if x = 0, and 0 otherwise.\n            spike = 1.0 if V[t] = V_th else 0.0\n            \n            # Apply the leaky recurrence relation.\n            V[t+1] = alpha * V[t] + w_r * spike + b\n\n        # Step 2: Compute G, the gradient magnitude with the STE.\n        # This requires a \"backward\" pass over the V trajectory.\n        G = 1.0\n        for t in range(T):\n            # The surrogate derivative g(V).\n            # g(V) is 1 if |V - V_th| = delta, and 0 otherwise.\n            surrogate_grad = 1.0 if np.abs(V[t] - V_th) = delta else 0.0\n\n            # The Jacobian factor J_t.\n            J_t = alpha + w_r * surrogate_grad\n\n            # The total gradient magnitude is the product of the absolute values.\n            G *= np.abs(J_t)\n\n        # Step 3: Compute G_0, the baseline gradient magnitude.\n        # This corresponds to the case where the derivative of s(V) is always zero.\n        # G_0 = alpha^T.\n        G_0 = alpha ** T\n\n        # Step 4: Compute the ratio R = G / G_0.\n        # We check for G_0 being zero, although alpha in (0,1) prevents this.\n        if G_0 == 0.0:\n            # This case is not expected with the problem constraints.\n            R = float('inf') if G  0 else 1.0\n        else:\n            R = G / G_0\n\n        results.append(R)\n\n    # Final print statement in the exact required format.\n    # We format the floats to ensure consistent and clean output.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from a single neuron to a network, this final practice explores how collective dynamics give rise to powerful computation. You will build a recurrent SNN reservoir and investigate how a key structural parameter—connectivity sparsity—influences the network's ability to create separable representations of different inputs. This simulation provides a hands-on demonstration of the reservoir computing paradigm, illustrating how the rich, high-dimensional dynamics of recurrent networks can be harnessed for complex information processing. ",
            "id": "4056967",
            "problem": "Consider a recurrent Spiking Neural Network (SNN) reservoir modeled with Leaky Integrate-and-Fire (LIF) neurons. The goal is to analyze how connectivity sparsity in the recurrent reservoir affects the separability of input-evoked state trajectories. Separability will be quantified via pairwise Euclidean distances between trajectory vectors corresponding to distinct input stimuli. The analysis must compare these pairwise distances under different connectivity densities.\n\nFundamental base:\n- Define the membrane potential dynamics of a LIF neuron by the differential equation $$\\tau_m \\frac{dV_i(t)}{dt} = -\\left(V_i(t) - V_{\\text{rest}}\\right) + R_m I_i(t),$$ where $V_i(t)$ is the membrane potential of neuron $i$, $I_i(t)$ is the total synaptic current into neuron $i$, $\\tau_m$ is the membrane time constant, $R_m$ is the membrane resistance, and $V_{\\text{rest}}$ is the resting potential. A spike is emitted when $V_i(t)$ crosses a threshold $V_{\\text{thr}}$, after which $V_i(t)$ is set to $V_{\\text{reset}}$ and held there for a refractory period.\n- Model the synaptic current with an exponentially decaying Post-Synaptic Current (PSC): $$\\tau_s \\frac{dI_i(t)}{dt} = -I_i(t) + \\sum_{j=1}^{N} W_{ij} s_j(t) + w_{\\text{in}} x_i(t),$$ where $\\tau_s$ is the synaptic time constant, $W_{ij}$ is the recurrent weight from neuron $j$ to neuron $i$, $s_j(t)$ is the spike train of neuron $j$, $x_i(t)$ is the external input spike train to neuron $i$, and $w_{\\text{in}}$ is the feedforward input weight. The external input spike trains $x_i(t)$ are generated from independent Poisson processes with specified rates (in Hertz).\n- To construct a trajectory representation that captures the dynamics, define a filtered spike state $$\\tau_f \\frac{dy_i(t)}{dt} = -y_i(t) + s_i(t),$$ where $y_i(t)$ is the low-pass filtered spike train for neuron $i$ and $\\tau_f$ is a filtering time constant. The reservoir trajectory vector for an input stimulus is the flattening of $y_i(t)$ across all neurons and time steps.\n\nReservoir construction and connectivity sparsity:\n- The reservoir has $N$ LIF neurons. Each presynaptic neuron $j$ is either excitatory or inhibitory with a fixed sign according to Dale’s law. The recurrent connectivity is random with connectivity density $d \\in [0,1]$, defining the probability that a connection $W_{ij}$ is present for $i \\neq j$. The magnitude of present connections is positive and drawn from an exponential distribution and then signed by the presynaptic type. Self-connections are forbidden, i.e., $W_{ii} = 0$.\n- To make comparisons across densities $d$ meaningful, scale the weight magnitudes by a factor inversely proportional to $\\sqrt{N d}$ when $d > 0$, so that the total incoming variance remains approximately controlled as $d$ changes.\n\nDiscrete-time simulation:\n- Use a time discretization with step size $\\Delta t$. The membrane and current updates are carried out via Euler integration. The spike state $s_i(t)$ is binary ($0$ or $1$) per time step. The external input spike train $x_i(t)$ is independently generated for each neuron using a Bernoulli process with success probability $p = r \\Delta t$, where $r$ is the input rate in Hertz and $\\Delta t$ is in seconds.\n\nTrajectory separability:\n- For each connectivity density $d$, simulate the reservoir for multiple input stimuli with distinct Poisson rates. For each stimulus, obtain the trajectory vector by flattening the filtered spike states $y_i(t)$ across all neurons and time steps. Compute pairwise Euclidean distances between trajectory vectors corresponding to different stimuli. The separability score for density $d$ is defined as the mean of these pairwise distances.\n\nPhysical units and numerical values:\n- Time $\\Delta t$ is $1$ millisecond ($\\Delta t = 1$ ms), total simulated duration is $T = 500$ milliseconds ($T = 500$ ms), and angles are not applicable. Potentials are in millivolts (mV), but the output focuses on dimensionless distances between trajectory vectors.\n- Membrane parameters: $\\tau_m = 20$ ms, $V_{\\text{rest}} = -65$ mV, $V_{\\text{thr}} = -50$ mV, $V_{\\text{reset}} = -65$ mV, refractory period $= 2$ ms, and $R_m = 1$ (dimensionless scale).\n- Synaptic and filtering parameters: $\\tau_s = 5$ ms, $\\tau_f = 20$ ms, $w_{\\text{in}} = 1.0$ (dimensionless scale).\n- Reservoir size: $N = 100$, fraction of excitatory neurons $p_{\\text{exc}} = 0.8$, so the number of inhibitory neurons is $N_{\\text{inh}} = N (1 - p_{\\text{exc}})$.\n- Weight magnitude base scale for exponential distribution: $b = 0.2$ (dimensionless), scaled by $1/\\sqrt{N d}$ when $d > 0$; for $d = 0$, set $W_{ij} = 0$.\n\nInput stimuli:\n- Use input rates $r \\in \\{5, 20, 50, 80\\}$ Hertz.\n\nTest suite:\n- Connectivity densities $d$ to test: $\\{0.0, 0.05, 0.2, 0.5, 1.0\\}$.\n- For each $d$ in the above set, simulate the reservoir with all input rates in $\\{5, 20, 50, 80\\}$ Hertz and compute the mean pairwise Euclidean distance across all distinct stimulus pairs.\n\nAlgorithmic task:\n- Implement the above discrete-time LIF reservoir dynamics and trajectory processing.\n- For each $d$ in the test suite, output the separability score as a floating-point number: the mean pairwise Euclidean distance between trajectory vectors for the four stimuli.\n- Randomness must be controlled via a fixed seed so that results are deterministic.\n\nFinal output format:\n- Your program should produce a single line of output containing the separability scores for the connectivity densities in the specified order as a comma-separated list enclosed in square brackets, for example, $[s_{d1}, s_{d2}, \\dots, s_{d5}]$ where each $s_{dk}$ is the mean pairwise distance for the $k$-th density in the test suite.",
            "solution": "The problem requires an analysis of how the connectivity sparsity of a recurrent Spiking Neural Network (SNN) reservoir affects the separability of its responses to different input stimuli. The SNN is composed of Leaky Integrate-and-Fire (LIF) neurons. The analysis will be performed by simulating the network's dynamics under various connectivity densities and quantifying the separability of the resulting state-space trajectories.\n\nFirst, we formalize the discrete-time simulation model based on the provided continuous-time differential equations. The simulation proceeds in discrete time steps of size $\\Delta t = 1$ ms. We use the forward Euler method for integration as specified.\n\nThe membrane potential $V_i$ of neuron $i$ at time step $t+1$ is updated based on its value and the synaptic current $I_i$ at step $t$. The provided equation is $\\tau_m \\frac{dV_i}{dt} = -(V_i - V_{\\text{rest}}) + R_m I_i$. With $R_m=1$, its discrete form is:\n$$V_i[t+1] = V_i[t] + \\frac{\\Delta t}{\\tau_m} \\left( -(V_i[t] - V_{\\text{rest}}) + I_i[t] \\right)$$\nwhich can be rearranged to:\n$$V_i[t+1] = V_i[t] \\left(1 - \\frac{\\Delta t}{\\tau_m}\\right) + \\frac{\\Delta t}{\\tau_m} \\left( V_{\\text{rest}} + I_i[t] \\right)$$\n\nThe synaptic current $I_i$ is driven by recurrent spikes from within the reservoir and external input spikes. The governing equation is $\\tau_s \\frac{dI_i}{dt} = -I_i + \\sum_{j=1}^{N} W_{ij} s_j(t) + w_{\\text{in}} x_i(t)$. Its Euler discretization is:\n$$I_i[t+1] = I_i[t] \\left(1 - \\frac{\\Delta t}{\\tau_s}\\right) + \\frac{\\Delta t}{\\tau_s} \\left( \\sum_{j=1}^{N} W_{ij} s_j[t] + w_{\\text{in}} x_i[t] \\right)$$\nHere, $s_j[t]$ is a binary variable, equal to $1$ if neuron $j$ spikes at time step $t$, and $0$ otherwise. Similarly, $x_i[t]$ is the binary external input spike to neuron $i$ at step $t$.\n\nA spike is emitted by neuron $i$ at step $t$ if its membrane potential $V_i[t]$ crosses the threshold $V_{\\text{thr}}$. Upon spiking, its potential is reset to $V_{\\text{reset}}$ and the neuron enters a refractory period of $2$ ms ($2$ time steps), during which its potential is clamped at $V_{\\text{reset}}$.\n\nThe state-space trajectory is constructed from a low-pass filtered version of the spike trains, $y_i(t)$. The dynamics $\\tau_f \\frac{dy_i}{dt} = -y_i + s_i(t)$ are discretized as:\n$$y_i[t+1] = y_i[t] \\left(1 - \\frac{\\Delta t}{\\tau_f}\\right) + \\frac{\\Delta t}{\\tau_f} s_i[t]$$\n\nThe algorithmic procedure is as follows:\n\n1.  **Initialization**: We set a global random seed for reproducibility. All time constants ($\\tau_m$, $\\tau_s$, $\\tau_f$) are converted into numbers of time steps (e.g., $\\tau_m / \\Delta t = 20 \\text{ ms} / 1 \\text{ ms} = 20$). The four distinct external input spike trains, one for each rate $r \\in \\{5, 20, 50, 80\\}$ Hz, are pre-generated. Each train is an $N \\times T$ matrix where each entry is an independent Bernoulli trial with success probability $p = r \\Delta t$. Generating these inputs beforehand ensures that the same set of stimuli is used to test the reservoir under each connectivity density, providing a controlled comparison.\n\n2.  **Reservoir Construction**: For each connectivity density $d \\in \\{0.0, 0.05, 0.2, 0.5, 1.0\\}$, we construct a corresponding recurrent weight matrix $W$ of size $N \\times N$, where $N=100$.\n    -   Neuron types are assigned: $80\\%$ excitatory ($N_{\\text{exc}}=80$) and $20\\%$ inhibitory ($N_{\\text{inh}}=20$).\n    -   A connection from neuron $j$ to $i$ exists with probability $d$. Self-connections ($W_{ii}$) are forbidden.\n    -   If $d0$, the magnitudes of existing weights are drawn from an exponential distribution with scale parameter $b=0.2$.\n    -   The sign of a weight $W_{ij}$ is determined by the presynaptic neuron $j$: positive if $j$ is excitatory, negative if inhibitory (Dale's Law).\n    -   To maintain network stability across different densities, weight magnitudes are scaled by $1 / \\sqrt{Nd}$. For $d=0$, all weights $W_{ij}$ are set to $0$.\n\n3.  **Network Simulation**: For a given density $d$ and its associated weight matrix $W$, the SNN's response to each of the four pre-generated input stimuli is simulated for $T=500$ ms ($500$ steps).\n    -   At each time step $t$, the simulation proceeds causally:\n        a.  The potentials $V_i[t]$ of non-refractory neurons are updated based on the currents $I_i[t]$.\n        b.  Neurons whose potential crosses $V_{\\text{thr}}$ are marked as spiking, and their spike variable $s_i[t]$ is set to $1$.\n        c.  The potentials of spiking neurons are reset to $V_{\\text{reset}}$, and their refractory counters are initiated.\n        d.  The synaptic currents $I_i[t+1]$ and filtered states $y_i[t+1]$ are updated using the spike vector $s[t]$ and the external input vector $x[t]$.\n    -   The history of the filtered state vectors, $y_i[t]$, is stored for all neurons and time steps.\n\n4.  **Separability Calculation**: After simulating the network for all four input rates for a fixed density $d$, we obtain four trajectory matrices. Each matrix is flattened into a single vector of size $N \\times T = 100 \\times 500 = 50000$.\n    -   The separability of these four trajectory vectors is quantified by their pairwise Euclidean distances. There are $\\binom{4}{2}=6$ unique pairs.\n    -   The separability score for density $d$ is the arithmetic mean of these six Euclidean distances.\n\nThis process is repeated for each density in the test suite. The final output is the list of separability scores, one for each density.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the recurrent SNN trajectory separability problem.\n    \"\"\"\n    # Use a fixed seed for reproducibility as required.\n    SEED = 0\n    rng = np.random.default_rng(SEED)\n\n    # Physical units and numerical values\n    DT_MS = 1.0\n    T_MS = 500.0\n    TAU_M_MS = 20.0\n    V_REST = -65.0\n    V_THR = -50.0\n    V_RESET = -65.0\n    REFRACTORY_PERIOD_MS = 2.0\n    R_M = 1.0  # Assumed dimensionless as per problem, folded into I\n    TAU_S_MS = 5.0\n    TAU_F_MS = 20.0\n    W_IN = 1.0\n    N = 100\n    P_EXC = 0.8\n    N_EXC = int(N * P_EXC)\n    WEIGHT_SCALE_BASE = 0.2\n\n    # Simulation parameters in terms of time steps\n    T_STEPS = int(T_MS / DT_MS)\n    REFRACTORY_STEPS = int(REFRACTORY_PERIOD_MS / DT_MS)\n    \n    # Pre-calculated Euler update coefficients\n    ALPHA_M = DT_MS / TAU_M_MS\n    ALPHA_S = DT_MS / TAU_S_MS\n    ALPHA_F = DT_MS / TAU_F_MS\n\n    # Test suite\n    densities_to_test = [0.0, 0.05, 0.2, 0.5, 1.0]\n    input_rates_hz = [5.0, 20.0, 50.0, 80.0]\n\n    # Pre-generate input spike trains for all stimuli to ensure fair comparison across densities\n    input_trains = []\n    dt_s = DT_MS / 1000.0\n    for r in input_rates_hz:\n        p = r * dt_s\n        train = rng.binomial(1, p, size=(T_STEPS, N)).astype(np.float64)\n        input_trains.append(train)\n\n    # Define neuron types for Dale's Law\n    neuron_signs = np.ones(N, dtype=np.float64)\n    neuron_signs[N_EXC:] = -1.0\n\n    separability_scores = []\n\n    for d in densities_to_test:\n        # 1. Reservoir construction\n        W = np.zeros((N, N), dtype=np.float64)\n        if d > 0:\n            # Create connectivity mask\n            mask = rng.random(size=(N, N))  d\n            np.fill_diagonal(mask, False)\n            \n            # Generate weights from exponential distribution\n            W_mag = rng.exponential(scale=WEIGHT_SCALE_BASE, size=(N, N))\n            \n            # Apply Dale's law and scaling\n            scaling_factor = 1.0 / np.sqrt(N * d)\n            W = mask * W_mag * scaling_factor * neuron_signs.reshape(1, -1)\n\n        trajectories = []\n        for x_train in input_trains:\n            # 2. Simulation for one stimulus\n            v = np.full(N, V_REST, dtype=np.float64)\n            i_syn = np.zeros(N, dtype=np.float64)\n            y_filtered = np.zeros(N, dtype=np.float64)\n            ref_counters = np.zeros(N, dtype=np.int32)\n            \n            y_history = np.zeros((T_STEPS, N), dtype=np.float64)\n\n            for t in range(T_STEPS):\n                is_refractory = ref_counters > 0\n                \n                # Update membrane potential for non-refractory neurons\n                dv = ALPHA_M * (-(v - V_REST) + R_M * i_syn)\n                v[~is_refractory] += dv[~is_refractory]\n                \n                # Decrement refractory counters and clamp potential\n                v[is_refractory] = V_RESET\n                ref_counters[is_refractory] -= 1\n\n                # Detect spikes\n                spiked = (v >= V_THR)  (~is_refractory)\n                s_t = spiked.astype(np.float64)\n                \n                # Reset potential and set refractory period for spiking neurons\n                v[spiked] = V_RESET\n                ref_counters[spiked] = REFRACTORY_STEPS\n                \n                # Update synaptic current\n                recurrent_input = W @ s_t\n                external_input = W_IN * x_train[t, :]\n                total_input = recurrent_input + external_input\n                di_syn = ALPHA_S * (-i_syn + total_input)\n                i_syn += di_syn\n                \n                # Update filtered spike state\n                dy_filtered = ALPHA_F * (-y_filtered + s_t)\n                y_filtered += dy_filtered\n                \n                y_history[t, :] = y_filtered\n\n            # Flatten trajectory and store\n            trajectories.append(y_history.flatten())\n\n        # 3. Trajectory separability calculation for the current density\n        if len(trajectories) > 1:\n            distances = pdist(np.array(trajectories), 'euclidean')\n            mean_distance = np.mean(distances)\n        else:\n            mean_distance = 0.0 # Should not happen with >1 stimuli rates\n            \n        separability_scores.append(mean_distance)\n\n    # Final output format\n    print(f\"[{','.join(map(str, separability_scores))}]\")\n\nsolve()\n```"
        }
    ]
}