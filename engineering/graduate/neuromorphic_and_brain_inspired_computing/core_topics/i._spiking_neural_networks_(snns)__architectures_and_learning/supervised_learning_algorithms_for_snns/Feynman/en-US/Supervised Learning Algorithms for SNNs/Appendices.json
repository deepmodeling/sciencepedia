{
    "hands_on_practices": [
        {
            "introduction": "A core challenge in training Spiking Neural Networks (SNNs) is the non-differentiable nature of the spike generation mechanism. This exercise provides a foundational, first-principles derivation of a supervised learning rule using the surrogate gradient method, which is the cornerstone of modern SNN training. By working through the mathematics for the biologically-inspired van Rossum distance loss, you will gain a deep understanding of how error signals are calculated and propagated to update synaptic weights .",
            "id": "4061662",
            "problem": "Consider a single-output neuron in a Spiking Neural Network (SNN). The neuron is modeled by the Spike Response Model (SRM) in discrete time with uniform sampling interval $\\Delta t$. Let the presynaptic spike train be $\\{S^{\\mathrm{pre}}_{t}\\}_{t=1}^{T}$, where $S^{\\mathrm{pre}}_{t} \\in \\{0,1\\}$ indicates a spike at time index $t$. The neuron's membrane potential is given by a linear synaptic response\n$$\nV_{t} = w\\,(\\epsilon * S^{\\mathrm{pre}})_{t},\n$$\nwhere $w \\in \\mathbb{R}$ is a synaptic weight, $(\\epsilon * S^{\\mathrm{pre}})_{t}$ denotes the discrete-time causal convolution $(\\epsilon * S^{\\mathrm{pre}})_{t} = \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$, and $\\{\\epsilon_{j}\\}_{j \\ge 0}$ is a fixed synaptic kernel sequence. The neuron emits an output spike train $\\{S_{t}\\}_{t=1}^{T}$ according to the threshold relation $S_{t} = H(V_{t} - V_{\\theta})$, where $H(\\cdot)$ is the Heaviside step function and $V_{\\theta}$ is a fixed threshold. For supervised learning, a target spike train $\\{S^{\\mathrm{tar}}_{t}\\}_{t=1}^{T}$ is provided. The loss is the van Rossum loss defined by\n$$\nL = \\frac{1}{2} \\sum_{t=1}^{T} \\Big( (\\kappa * S)_{t} - (\\kappa * S^{\\mathrm{tar}})_{t} \\Big)^{2},\n$$\nwhere $(\\kappa * S)_{t} = \\sum_{j=0}^{t-1} \\kappa_{j}\\,S_{t-j}$ is a causal convolution with a fixed van Rossum kernel $\\{\\kappa_{j}\\}_{j \\ge 0}$, for example the discrete exponential kernel $\\kappa_{j} = \\frac{\\Delta t}{\\tau_{r}} \\exp\\!\\big(-\\frac{j\\,\\Delta t}{\\tau_{r}}\\big)$ for $j \\ge 0$ with time constant $\\tau_{r} > 0$. To enable gradient-based optimization, use a surrogate gradient for the spike nonlinearity: replace the ill-defined derivative $\\partial S_{t}/\\partial V_{t}$ by a prescribed surrogate function $\\phi(V_{t})$ that is nonzero in a neighborhood of $V_{\\theta}$ and zero otherwise.\n\nStarting only from the definitions of causal discrete convolution, the chain rule, and the above loss and neuron model, derive an explicit closed-form analytic expression for the gradient $\\partial L / \\partial w$ in terms of the sequences $\\{\\kappa_{j}\\}$, $\\{\\epsilon_{j}\\}$, $\\{S_{t}\\}$, $\\{S^{\\mathrm{tar}}_{t}\\}$, and $\\{S^{\\mathrm{pre}}_{t}\\}$. Your final expression must show the explicit dependence on a filtered error signal that arises from the van Rossum loss and the causal convolution structure. Express the final answer as a single analytic expression. Do not provide numerical values.",
            "solution": "The problem asks for the derivation of the gradient of the van Rossum loss function, $L$, with respect to a synaptic weight, $w$, for a neuron described by the Spike Response Model (SRM). The solution requires a rigorous application of the chain rule of calculus in the context of discrete-time signals and convolutions, incorporating the concept of a surrogate gradient for the non-differentiable spiking mechanism.\n\nLet us begin by restating the core definitions provided:\nThe membrane potential at time step $t$ is:\n$$\nV_{t} = w\\,(\\epsilon * S^{\\mathrm{pre}})_{t} = w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\n$$\nThe output spike at time step $t$ is generated by:\n$$\nS_{t} = H(V_{t} - V_{\\theta})\n$$\nwhere $H(\\cdot)$ is the Heaviside step function. The derivative of this function is ill-defined. As instructed, we use a surrogate gradient by making the substitution:\n$$\n\\frac{\\partial S_{t}}{\\partial V_{t}} \\approx \\phi(V_{t})\n$$\nwhere $\\phi(\\cdot)$ is a suitable function that approximates the derivative, for instance, a narrow pulse centered at the threshold.\n\nThe loss function is given by:\n$$\nL = \\frac{1}{2} \\sum_{k=1}^{T} \\Big( (\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} \\Big)^{2}\n$$\nFor convenience, let us define the filtered spike trains as $s_{k} = (\\kappa * S)_{k}$ and $s^{\\mathrm{tar}}_{k} = (\\kappa * S^{\\mathrm{tar}})_{k}$. The loss can then be written as:\n$$\nL = \\frac{1}{2} \\sum_{k=1}^{T} (s_{k} - s^{\\mathrm{tar}}_{k})^{2}\n$$\nOur objective is to compute the gradient $\\frac{\\partial L}{\\partial w}$. The weight $w$ influences the membrane potentials $\\{V_{t}\\}$, which in turn determine the output spikes $\\{S_{t}\\}$, which are then filtered to produce $\\{s_{k}\\}$ that appear in the loss $L$. The chain of dependencies is $w \\rightarrow \\{V_{t}\\} \\rightarrow \\{S_{t}\\} \\rightarrow \\{s_{k}\\} \\rightarrow L$. We apply the multivariate chain rule, summing the influence of $w$ on $L$ through each spike $S_{t}$:\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial S_{t}} \\frac{\\partial S_{t}}{\\partial w}\n$$\nThis decomposition separates the problem into two parts:\n1.  Computing the derivative of a spike $S_{t}$ with respect to the weight $w$, which involves the neuron's dynamics.\n2.  Computing the derivative of the loss $L$ with respect to a spike $S_{t}$, which involves the structure of the loss function.\n\nLet us address the first part, $\\frac{\\partial S_{t}}{\\partial w}$. Using the chain rule for the neuron dynamics:\n$$\n\\frac{\\partial S_{t}}{\\partial w} = \\frac{\\partial S_{t}}{\\partial V_{t}} \\frac{\\partial V_{t}}{\\partial w}\n$$\nWe substitute the surrogate gradient for the first term: $\\frac{\\partial S_{t}}{\\partial V_{t}} \\approx \\phi(V_{t})$. For the second term, we differentiate the expression for $V_{t}$ with respect to $w$:\n$$\n\\frac{\\partial V_{t}}{\\partial w} = \\frac{\\partial}{\\partial w} \\left( w\\,(\\epsilon * S^{\\mathrm{pre}})_{t} \\right) = (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\nCombining these results gives the derivative of the spike with respect to the weight:\n$$\n\\frac{\\partial S_{t}}{\\partial w} = \\phi(V_{t}) \\, (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\nThis term couples the postsynaptic neuron's state (via $\\phi(V_{t})$) with the presynaptic input activity (via the filtered presynaptic spike train $(\\epsilon * S^{\\mathrm{pre}})_{t}$).\n\nNow, we address the second part, $\\frac{\\partial L}{\\partial S_{t}}$. The loss $L$ depends on $S_{t}$ through the filtered spike train $s_{k} = (\\kappa*S)_{k}$. We apply the chain rule again:\n$$\n\\frac{\\partial L}{\\partial S_{t}} = \\sum_{k=1}^{T} \\frac{\\partial L}{\\partial s_{k}} \\frac{\\partial s_{k}}{\\partial S_{t}}\n$$\nThe derivative of $L$ with respect to $s_{k}$ is straightforward:\n$$\n\\frac{\\partial L}{\\partial s_{k}} = \\frac{\\partial}{\\partial s_{k}} \\left[ \\frac{1}{2} \\sum_{m=1}^{T} (s_{m} - s^{\\mathrm{tar}}_{m})^{2} \\right] = s_{k} - s^{\\mathrm{tar}}_{k}\n$$\nNext, we compute the derivative of the convolution $s_{k}$ with respect to a single spike $S_{t}$:\n$$\ns_{k} = (\\kappa * S)_{k} = \\sum_{j=0}^{k-1} \\kappa_{j}\\,S_{k-j}\n$$\nThe derivative $\\frac{\\partial s_{k}}{\\partial S_{t}}$ is non-zero only if one of the terms in the sum contains $S_{t}$. This occurs when $k-j=t$, or $j=k-t$. For this to be a valid index in the sum, we must have $0 \\le k-t \\le k-1$, which simplifies to $t \\ge 1$ and $k \\ge t$. If this condition is met, the derivative is the coefficient of $S_{t}$, which is $\\kappa_{k-t}$.\n$$\n\\frac{\\partial s_{k}}{\\partial S_{t}} =\n\\begin{cases}\n\\kappa_{k-t}  \\text{if } k \\ge t \\\\\n0  \\text{if } k  t\n\\end{cases}\n$$\nSubstituting these two derivatives back, we find the influence of a spike $S_{t}$ on the total loss:\n$$\n\\frac{\\partial L}{\\partial S_{t}} = \\sum_{k=t}^{T} (s_{k} - s^{\\mathrm{tar}}_{k}) \\kappa_{k-t} = \\sum_{k=t}^{T} \\Big( (\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} \\Big) \\kappa_{k-t}\n$$\nThis term represents an error signal that is filtered and propagated backward in time from the future ($k \\ge t$) to the present ($t$). Let us denote this \"filtered error signal\" by $\\delta_{t}$:\n$$\n\\delta_{t} = \\sum_{k=t}^{T} \\kappa_{k-t} \\Big( (\\kappa * (S - S^{\\mathrm{tar}}))_{k} \\Big)\n$$\nFinally, we assemble the complete gradient $\\frac{\\partial L}{\\partial w}$ by substituting our results for $\\frac{\\partial S_{t}}{\\partial w}$ and $\\frac{\\partial L}{\\partial S_{t}}$ into the original sum:\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\delta_{t} \\frac{\\partial S_{t}}{\\partial w} = \\sum_{t=1}^{T} \\delta_{t} \\, \\phi(V_{t}) \\, (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\nTo obtain the final explicit expression as required, we substitute the full definitions for $\\delta_t$, $V_t$, and the convolutions. The filtered presynaptic input is $(\\epsilon * S^{\\mathrm{pre}})_{t} = \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$. The membrane potential is $V_{t} = w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$. The filtered error signal contains the convolution difference $(\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} = \\sum_{l=0}^{k-1} \\kappa_l (S_{k-l} - S^{\\mathrm{tar}}_{k-l})$.\n\nCombining everything yields the final analytic expression for the gradient:\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\phi\\left(w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\\right) \\left(\\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\\right) \\left( \\sum_{k=t}^{T} \\kappa_{k-t} \\left[ \\sum_{l=0}^{k-1} \\kappa_{l}(S_{k-l} - S^{\\mathrm{tar}}_{k-l}) \\right] \\right)\n$$\nThis expression conforms to the three-factor learning rule structure often found in neuroscience, where the weight update is proportional to the product of a presynaptic trace, a postsynaptic state-dependent term, and a future-dependent error signal.",
            "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\phi\\left(w \\sum_{j=0}^{t-1} \\epsilon_j S^{\\mathrm{pre}}_{t-j}\\right) \\left(\\sum_{j=0}^{t-1} \\epsilon_j S^{\\mathrm{pre}}_{t-j}\\right) \\left( \\sum_{k=t}^{T} \\kappa_{k-t} \\left[ \\sum_{l=0}^{k-1} \\kappa_l (S_{k-l} - S^{\\mathrm{tar}}_{k-l}) \\right] \\right)}\n$$"
        },
        {
            "introduction": "Beyond the mathematical correctness of a learning algorithm lies the crucial question of its practical feasibility. This practice shifts our focus to the computational and memory costs associated with different training strategies, specifically comparing the canonical Backpropagation Through Time (BPTT) with the more biologically plausible Eligibility Propagation (e-prop). Analyzing the asymptotic complexity of these algorithms is an essential skill for any researcher or engineer, as it informs the choice of method based on network size, simulation duration, and hardware constraints .",
            "id": "4061601",
            "problem": "Consider a fully connected recurrent spiking neural network (SNN) with $N$ leaky integrate-and-fire neurons and discrete-time dynamics over $T$ steps. There is a synapse from every neuron to every neuron (including or excluding self-connections does not affect leading-order scaling), so the number of trainable synaptic weights is $M = N^2$. Assume that the per-synapse and per-neuron forward computations at each time step require a constant number of floating-point operations that does not depend on $N$ or $T$, and that storing any scalar state variable requires a constant number of machine words, independent of $N$ and $T$. Training uses supervised learning with a scalar loss that depends on network activity over time.\n\nTwo training algorithms are used:\n\n- Backpropagation Through Time (BPTT): Backpropagation Through Time (BPTT) stores the forward-pass neuron states needed to compute exact gradients via a reverse-time pass. Assume that BPTT does not maintain any per-synapse eligibility traces during the forward pass.\n\n- Eligibility Propagation (e-prop): Eligibility propagation (e-prop) updates gradients online using per-synapse eligibility traces that evolve forward in time. Assume that e-prop does not store past neuron states beyond what is needed to advance to the next time step.\n\nAdopt the following conventions to make the accounting unambiguous:\n\n- Count the peak algorithm-specific working memory as the additional memory beyond the storage of trainable parameters and their accumulated gradients. Thus, do not include the $M$ trainable weights, any optimizer states directly tied to those weights, or the final gradient tensors in the peak memory count; only include the extra temporal or per-synapse state that is specific to the algorithmâ€™s mechanics.\n\n- Count the total arithmetic operations over the entire training pass (one forward and, where applicable, one backward sweep) as the compute complexity. Treat the per-synapse and per-neuron operation counts per time step as constants independent of $N$ and $T$.\n\nUnder these assumptions, derive the asymptotic scaling, in terms of $N$ and $T$, of the following four quantities:\n\n$1.$ Peak algorithm-specific working memory of BPTT.\n\n$2.$ Peak algorithm-specific working memory of e-prop.\n\n$3.$ Total arithmetic operation count of BPTT over the $T$-step sequence.\n\n$4.$ Total arithmetic operation count of e-prop over the $T$-step sequence.\n\nProvide the final answer as a single row matrix containing, in order, the four asymptotic expressions as functions of $N$ and $T$ using Big-O notation. Ignore constant factors and lower-order terms. No numerical rounding is required. Express the final result in exact symbolic form.",
            "solution": "The problem requires the derivation of the asymptotic scaling for peak working memory and total computational complexity for two supervised learning algorithms in a fully connected recurrent spiking neural network (SNN): Backpropagation Through Time (BPTT) and Eligibility Propagation (e-prop). The network consists of $N$ neurons and runs for $T$ discrete time steps. The number of synapses is $M = N^2$.\n\nLet us define the scope of the analysis based on the provided assumptions:\n- The network is fully connected, so there are $M = O(N^2)$ synaptic weights.\n- The arithmetic operations for updating a single neuron's state or a single synapse's contribution at each time step are constant, i.e., $O(1)$.\n- The memory required to store any single scalar state variable (e.g., membrane potential, eligibility trace value) is constant, i.e., $O(1)$.\n- \"Algorithm-specific working memory\" excludes the storage for the $M$ trainable weights and their accumulated gradients.\n- \"Total arithmetic operations\" covers the entire training pass (forward and backward, if applicable).\n\nWe will derive the four requested quantities in order.\n\n$1.$ **Peak algorithm-specific working memory of BPTT**\n\nThe BPTT algorithm functions in two passes: a forward pass and a backward pass. To compute the exact gradients of the loss function with respect to the network's parameters and states at a given time step $t$, the backward pass requires access to the neuron states computed during the forward pass at that same time step $t$ and potentially $t+1$. Therefore, the algorithm must store the history of neuron states throughout the forward pass.\n\nThe problem specifies that BPTT \"stores the forward-pass neuron states\". At each time step $t \\in \\{1, 2, ..., T\\}$, the state of each of the $N$ neurons must be saved. The state of a neuron (e.g., its membrane potential, spike output) can be represented by a constant number of scalar variables. Per the problem's assumption, storing these requires $O(1)$ memory per neuron.\n\n- Memory to store states for all $N$ neurons at a single time step: $N \\times O(1) = O(N)$.\n- To perform the full backward pass from $t=T$ to $t=1$, the states from all $T$ time steps must be available. The peak memory usage occurs at the end of the forward pass (at time $T$), when the entire history has been recorded.\n- Total memory required: (Number of time steps) $\\times$ (Memory per time step) $= T \\times O(N) = O(NT)$.\n\nThis memory is algorithm-specific as it is used to bridge the forward and backward passes, and it is distinct from the memory for parameters or final gradients. Thus, the peak algorithm-specific working memory for BPTT scales as $O(NT)$.\n\n$2.$ **Peak algorithm-specific working memory of e-prop**\n\nThe e-prop algorithm is designed to compute approximate gradients online, avoiding the need for a full backward pass through time. It achieves this by maintaining a \"per-synapse eligibility trace\". This trace, for each synapse, keeps a running track of the causal link between a synaptic weight and the eventual loss. The problem states that e-prop \"updates gradients online using per-synapse eligibility traces\" and \"does not store past neuron states beyond what is needed to advance to the next time step\".\n\nThe dominant algorithm-specific memory cost for e-prop is the storage of these eligibility traces.\n- Number of synapses in the network: $M = N^2$.\n- Each synapse requires an eligibility trace, which is a scalar or a small vector of constant size. The memory to store one trace is therefore $O(1)$.\n- Total memory for all eligibility traces: (Number of synapses) $\\times$ (Memory per trace) $= M \\times O(1) = O(N^2)$.\n\nSince e-prop is an online algorithm, these traces are updated at each time step, but the total number of traces remains constant throughout the process. The memory requirement does not grow with the number of time steps $T$. Therefore, the peak algorithm-specific working memory for e-prop scales as $O(N^2)$.\n\n$3.$ **Total arithmetic operation count of BPTT**\n\nThe total computation for BPTT is the sum of the operations in the forward pass and the backward pass.\n\n- **Forward Pass:** At each time step $t \\in \\{1, 2, ..., T\\}$, the state of each of the $N$ neurons is updated. This update depends on the inputs received from all other neurons.\n    - Each of the $M = N^2$ synapses contributes to a postsynaptic neuron's state. The computation per synapse is $O(1)$. Total synaptic computation per time step: $O(N^2)$.\n    - Each of the $N$ neurons also undergoes an internal state update (e.g., membrane potential decay), which is an $O(1)$ operation per neuron. Total neuron-specific computation per time step: $O(N)$.\n    - Total computation per time step: $O(N^2) + O(N) = O(N^2)$.\n    - Total forward pass computation over $T$ steps: $T \\times O(N^2) = O(N^2 T)$.\n\n- **Backward Pass:** The backward pass computes gradients by propagating error signals backward through the unrolled computation graph, from $t=T$ down to $t=1$. The structure of the computation graph for the backward pass is symmetric to the forward pass.\n    - At each time step $t$, the algorithm computes the gradient of the loss with respect to the neuron states and weights. This involves backpropagating errors across all $M = N^2$ synaptic connections.\n    - The computational cost per time step in the backward pass is also dominated by synaptic calculations, resulting in $O(N^2)$ operations.\n    - Total backward pass computation over $T$ steps: $T \\times O(N^2) = O(N^2 T)$.\n\n- **Total BPTT Computation:** (Forward Pass) + (Backward Pass) = $O(N^2 T) + O(N^2 T) = O(N^2 T)$.\n\n$4.$ **Total arithmetic operation count of e-prop**\n\nE-prop computes gradients in a single forward pass. At each time step, it must perform the standard forward dynamics simulation and additionally update its internal data structures for gradient estimation.\n\n- At each time step $t \\in \\{1, 2, ..., T\\}$:\n    - **Forward Dynamics:** This is identical to the BPTT forward pass. It involves computing the effect of $N^2$ synapses and updating $N$ neurons. The complexity is $O(N^2)$.\n    - **Eligibility Trace Update:** The algorithm must update the eligibility trace for each of the $M = N^2$ synapses. The update rule for a single trace is an $O(1)$ operation, depending on local pre- and post-synaptic quantities. The total computational cost for updating all traces is $O(N^2)$.\n    - **Gradient Accumulation:** Based on the eligibility traces and an error signal, the gradient for each of the $M = N^2$ weights is updated. This is also an $O(N^2)$ operation per time step.\n- **Total computation per time step:** (Forward Dynamics) + (Trace Update) + (Gradient Accumulation) = $O(N^2) + O(N^2) + O(N^2) = O(N^2)$.\n- **Total e-prop Computation:** The operations are repeated for $T$ time steps.\n    - Total computation: $T \\times (\\text{per-step computation}) = T \\times O(N^2) = O(N^2 T)$.\n\nIn summary, the four requested quantities have the following asymptotic scaling:\n1.  BPTT Memory: $O(NT)$\n2.  e-prop Memory: $O(N^2)$\n3.  BPTT Compute: $O(N^2 T)$\n4.  e-prop Compute: $O(N^2 T)$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nO(NT)  O(N^2)  O(N^2 T)  O(N^2 T)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Bridging the gap between theory and application, this final practice challenges you to implement a learning rule that respects the fundamental constraints of neuromorphic computing. The goal is to construct a supervised learning procedure that is both local and event-driven, meaning updates occur only at spike times and depend solely on information available at the neuron. This exercise provides hands-on experience in designing algorithms that are not only effective but also computationally efficient and suitable for deployment on specialized, event-based hardware .",
            "id": "4061647",
            "problem": "Consider a single postsynaptic neuron in a Spiking Neural Network (SNN) receiving $N$ presynaptic spike trains. The neuron produces an output spike train while a supervisory signal provides a target spike train. The training must be supervised and adhere to event-driven computation constraints: weight updates are allowed only at spike events and must depend solely on local state variables available at the neuron, without continuous-time integration or nonlocal information. The goal is to derive a training procedure that satisfies these constraints and implement it to compute weight updates for given spike train sequences.\n\nFundamental base and modeling assumptions:\n- Spikes are represented as sums of Dirac delta functions. For presynaptic neuron $i$, the spike train is $s_i(t) = \\sum_k \\delta(t - t_k^{(i)})$, where $t_k^{(i)}$ are the presynaptic spike times.\n- Define a synaptic eligibility trace per input as $x_i(t)$ driven by presynaptic spikes via a first-order filter:\n$$\\frac{d x_i(t)}{d t} = -\\frac{x_i(t)}{\\tau_s} + s_i(t), \\quad x_i(0) = 0,$$\nwhere $\\tau_s$ is the synaptic time constant in seconds. This implies $x_i(t)$ decays exponentially with time constant $\\tau_s$ and jumps by $1$ at each presynaptic spike time.\n- Define low-pass filtered versions of the actual and target output spike trains as $z(t)$ and $z^\\ast(t)$, respectively:\n$$\\frac{d z(t)}{d t} = -\\frac{z(t)}{\\tau_o} + y(t), \\quad z(0) = 0,$$\n$$\\frac{d z^\\ast(t)}{d t} = -\\frac{z^\\ast(t)}{\\tau_o} + y^\\ast(t), \\quad z^\\ast(0) = 0,$$\nwhere $y(t) = \\sum_j \\delta(t - t_j^{\\text{post}})$ is the actual postsynaptic spike train, $y^\\ast(t) = \\sum_m \\delta(t - t_m^{\\text{targ}})$ is the target spike train, and $\\tau_o$ is the output filter time constant in seconds.\n- The supervised objective is the squared error between filtered actual and target outputs:\n$$\\mathcal{L} = \\frac{1}{2}\\int_{0}^{T} \\left(z(t) - z^\\ast(t)\\right)^2 \\, dt,$$\ndefined over a finite horizon $[0, T]$ with $T$ in seconds. The learning rate is $\\eta > 0$ (unitless).\n\nEvent-driven constraint and derivation target:\n- Derive a local, supervised update for synaptic weights $w_i$ that depends only on spike events and locally available state variables. The update must occur only at spike times from the union of postsynaptic and target spikes and must not require nonlocal information or continuous-time adjoint integration.\n- The update for $w_i$ must be expressible at an event time $t_e$ as a function of the instantaneous error signal $e(t_e)$ and the local eligibility trace $x_i(t_e)$, where $e(t) \\triangleq z(t) - z^\\ast(t)$. All quantities must be computable using only local state and spike events.\n- For simultaneous postsynaptic and target spikes at the same time $t_e$, both $z(t)$ and $z^\\ast(t)$ receive identical instantaneous increments, so the error $e(t_e)$ must be computed after applying both increments, resulting in no artificial bias due to ordering.\n\nAlgorithmic implementation constraints:\n- The computation must proceed by iterating through the sorted union of all event times (i.e., all presynaptic spikes across inputs, all postsynaptic spikes, and all target spikes).\n- Between events, each state variable decays exponentially: for a time increment $\\Delta t$, $x_i \\leftarrow x_i \\cdot \\exp(-\\Delta t/\\tau_s)$ and $z \\leftarrow z \\cdot \\exp(-\\Delta t/\\tau_o)$, $z^\\ast \\leftarrow z^\\ast \\cdot \\exp(-\\Delta t/\\tau_o)$.\n- At an event time $t_e$, apply all jumps from presynaptic, postsynaptic, and target spikes that occur at $t_e$, and then compute the error $e(t_e) = z(t_e) - z^\\ast(t_e)$. Weight updates are allowed only at $t_e$ if a postsynaptic or target spike occurs at $t_e$.\n- Time quantities must be expressed in seconds. The final output should be numerical floats without units.\n\nTest suite specification:\n- You must implement the derived event-driven weight update procedure and compute the total weight increment vector $\\Delta \\mathbf{w} = [\\Delta w_0, \\Delta w_1, \\dots, \\Delta w_{N-1}]$ for each test case. Use the following parameter values uniformly across test cases: $\\tau_s = 0.020$ (seconds), $\\tau_o = 0.050$ (seconds), and learning rate $\\eta = 0.1$ (unitless). All spike times are in seconds.\n- For each test case, the presynaptic spike times are provided per input, the postsynaptic actual spike times, and the target spike times:\n\nTest case A (general case):\n- $N = 2$\n- Presynaptic spikes: $S_0 = [0.010, 0.030, 0.080]$, $S_1 = [0.015, 0.060]$\n- Postsynaptic spikes: $Y = [0.070, 0.120]$\n- Target spikes: $Y^\\ast = [0.050, 0.100, 0.150]$\n\nTest case B (boundary case with no spikes):\n- $N = 2$\n- Presynaptic spikes: $S_0 = []$, $S_1 = []$\n- Postsynaptic spikes: $Y = []$\n- Target spikes: $Y^\\ast = []$\n\nTest case C (edge case with simultaneous events):\n- $N = 2$\n- Presynaptic spikes: $S_0 = [0.040, 0.100]$, $S_1 = [0.040, 0.100]$\n- Postsynaptic spikes: $Y = [0.100]$\n- Target spikes: $Y^\\ast = [0.070, 0.100]$\n\nFinal output specification:\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of floats representing the weight increments for that case. For example, the format should be:\n$$[\\,[\\Delta w_0^{(A)},\\Delta w_1^{(A)}],\\,[\\Delta w_0^{(B)},\\Delta w_1^{(B)}],\\,[\\Delta w_0^{(C)},\\Delta w_1^{(C)}]\\,].$$",
            "solution": "The problem is well-posed and scientifically grounded within the domain of computational neuroscience and neuromorphic engineering. It describes a supervised learning scenario for a single spiking neuron, rooted in established models using filtered spike trains and synaptic eligibility traces. The objective function is a standard-squared error, and the constraints enforce an event-driven, local learning rule, which is a key feature of biologically plausible and computationally efficient algorithms for Spiking Neural Networks (SNNs). All necessary parameters and initial conditions are provided to ensure a unique, deterministic solution.\n\nWe will first deduce the learning rule that satisfies the problem's constraints and then outline the event-driven algorithm for its implementation.\n\n**Derivation of the Event-Driven Learning Rule**\n\nThe objective is to minimize the loss function $\\mathcal{L}$ via gradient descent with respect to the synaptic weights $w_i$. The loss is given by:\n$$\n\\mathcal{L} = \\frac{1}{2}\\int_{0}^{T} \\left(z(t) - z^\\ast(t)\\right)^2 \\, dt = \\frac{1}{2}\\int_{0}^{T} e(t)^2 \\, dt\n$$\nwhere $e(t) \\triangleq z(t) - z^\\ast(t)$ is the instantaneous error. A standard gradient descent update for a weight $w_i$ would be proportional to $-\\frac{\\partial \\mathcal{L}}{\\partial w_i}$. The partial derivative is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\int_{0}^{T} e(t) \\frac{\\partial z(t)}{\\partial w_i} \\, dt\n$$\nThe variable $z(t)$ is the low-pass filtered version of the postsynaptic spike train $y(t)$. The generation of $y(t)$ depends on the weighted sum of presynaptic inputs, and thus on the weights $w_i$. However, the problem statement does not provide a neuron model that specifies this dependency, i.e., it does not define the relationship between $y(t)$ and the weights $w_i$. This omission makes a rigorous, first-principles derivation of $\\frac{\\partial z(t)}{\\partial w_i}$ and, consequently, the learning rule, impossible.\n\nNonetheless, the problem provides a set of strong constraints that allow us to construct the intended learning rule. The rule must be:\n1. Event-driven, with updates occurring only at the times of postsynaptic spikes ($t_j^{\\text{post}}$) or target spikes ($t_m^{\\text{targ}}$).\n2. Local, depending only on state variables available at the neuron, specifically the presynaptic eligibility trace $x_i(t_e)$ and the error signal $e(t_e)$ at an event time $t_e$.\n\nThese constraints guide us toward a heuristic rule that approximates gradient descent and is consistent with established supervised learning rules for SNNs (e.g., ReSuMe, SuperSpike). The total weight change, $\\Delta w_i$, is a sum of discrete updates, $\\delta w_i(t_e)$, at each learning event time $t_e$ from the set of all postsynaptic and target spike times, denoted $T_{events} = \\{t_j^{\\text{post}}\\} \\cup \\{t_m^{\\text{targ}}\\}$.\n$$\n\\Delta w_i = \\sum_{t_e \\in T_{events}} \\delta w_i(t_e)\n$$\nWe propose an update form that combines the local eligibility $x_i(t_e)$ with the local error $e(t_e)$:\n$$\n\\delta w_i(t_e) = -\\eta \\cdot e(t_e) \\cdot x_i(t_e)\n$$\nLet us analyze this rule's behavior:\n- If a target spike occurs at $t_e$ without a corresponding actual spike, $z^\\ast(t_e)$ increases, making $e(t_e) = z(t_e) - z^\\ast(t_e)$ negative. The update is $\\delta w_i(t_e) > 0$. This potentiates the synapse (increases $w_i$) if it was recently active (i.e., $x_i(t_e)$ is large), which is a Hebbian mechanism to encourage a spike at this time in the future.\n- If an actual spike occurs at $t_e$ without a corresponding target spike, $z(t_e)$ increases, making $e(t_e)$ positive. The update is $\\delta w_i(t_e)  0$. This depresses the synapse (decreases $w_i$) if it was recently active, which is an anti-Hebbian mechanism to suppress erroneous spikes.\n- If both an actual and a target spike occur at $t_e$, the problem specifies that $e(t_e)$ is computed after both jumps. The new state values are $z(t_e) = z(t_e^-) + 1$ and $z^\\ast(t_e) = z^\\ast(t_e^-) + 1$, where $t_e^-$ is the time just before the event. The error is then $e(t_e) = (z(t_e^-) + 1) - (z^\\ast(t_e^-) + 1) = z(t_e^-) - z^\\ast(t_e^-) = e(t_e^-)$. The update depends on the error just prior to the correctly timed spike, driving the system to correct any residual timing mismatch.\n\nThis rule satisfies all problem constraints and implements a plausible learning mechanism. Therefore, the total synaptic weight update for neuron $i$ is:\n$$\n\\Delta w_i = -\\eta \\sum_{t_e \\in T_{events}} e(t_e) x_i(t_e)\n$$\nwhere all quantities in the sum are evaluated at their respective event times $t_e$.\n\n**Algorithmic Procedure**\n\nThe computation must be performed in an event-driven manner. The algorithm is as follows:\n1.  Collect all unique event times from presynaptic spikes ($S_i$), postsynaptic spikes ($Y$), and target spikes ($Y^\\ast$) into a single timeline, sorted in ascending order.\n2.  Initialize the state variables: synaptic traces $x_i(0) = 0$ for all $i$, filtered output $z(0) = 0$, filtered target $z^\\ast(0) = 0$, and total weight changes $\\Delta w_i(0) = 0$.\n3.  Iterate through the sorted event times. For each event at time $t_k$, let the previous event time be $t_{k-1}$.\n4.  Calculate the time interval $\\Delta t = t_k - t_{k-1}$.\n5.  Evolve the state variables over $\\Delta t$ by applying exponential decay:\n    $$\n    x_i(t_k^-) = x_i(t_{k-1}) \\exp(-\\Delta t / \\tau_s) \\\\\n    z(t_k^-) = z(t_{k-1}) \\exp(-\\Delta t / \\tau_o) \\\\\n    z^\\ast(t_k^-) = z^\\ast(t_{k-1}) \\exp(-\\Delta t / \\tau_o)\n    $$\n6.  At time $t_k$, apply the instantaneous jumps for all spikes occurring at this time. The order of application for different state variables does not matter.\n    - For each presynaptic spike from input $i$ at $t_k$: $x_i(t_k) = x_i(t_k^-) + 1$.\n    - If a postsynaptic spike occurs at $t_k$: $z(t_k) = z(t_k^-) + 1$.\n    - If a target spike occurs at $t_k$: $z^\\ast(t_k) = z^\\ast(t_k^-) + 1$.\n    State variables not affected by a jump at $t_k$ retain their decayed value, e.g., $x_j(t_k) = x_j(t_k^-)$ if input $j$ does not spike.\n7.  If the event at $t_k$ is a learning event (i.e., a spike occurred in $Y$ or $Y^\\ast$), calculate the weight update:\n    a. Compute the error signal after all jumps: $e(t_k) = z(t_k) - z^\\ast(t_k)$.\n    b. Update the total weight change for each synapse $i$: $\\Delta w_i \\leftarrow \\Delta w_i - \\eta \\cdot e(t_k) \\cdot x_i(t_k)$.\n8.  Repeat for all event times. The final values of $\\Delta w_i$ are the result.\n\nThis procedure strictly adheres to the event-driven and locality constraints of the problem.",
            "answer": "[[0.0632350,0.0381016],[0.0000000,0.0000000],[0.0157977,0.0157977]]"
        }
    ]
}