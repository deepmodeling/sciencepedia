{
    "hands_on_practices": [
        {
            "introduction": "要掌握脉冲神经网络的监督学习，我们首先必须理解脉冲的全或无特性所带来的根本挑战。第一个练习将引导你通过比较两种不同的神经元输出读出方法——一种基于离散的脉冲计数，另一种基于连续的膜电位——来探讨这个问题。通过推导每种方法的梯度敏感度，你将揭示为何基于脉冲的目标函数常导致梯度消失，从而理解我们接下来将要探讨的专门技术的必要性 。",
            "id": "4061639",
            "problem": "考虑一个接收突触前脉冲序列的单个突触后漏积分-发放 (LIF) 神经元。设膜电位由 $V(t)$ 表示，膜时间常数由 $\\tau_{\\mathrm{m}}$ 表示，输入电阻由 $R$ 表示。在任意两个脉冲之间的区间上，其阈下膜动力学满足以下线性常微分方程\n$$\\tau_{\\mathrm{m}} \\frac{dV(t)}{dt} = -V(t) + R \\sum_{j} w_j x_j(t),$$\n其中 $w_j$ 是突触权重，$x_j(t)$ 是通过将突触前脉冲序列 $s_j(t) = \\sum_{m} \\delta\\!\\left(t - t^{(j)}_m\\right)$ 与指数突触核 $\\epsilon(t) = \\frac{1}{\\tau_{\\mathrm{s}}} e^{-t/\\tau_{\\mathrm{s}}} H(t)$ 进行卷积得到的突触电流，其中突触时间常数 $\\tau_{\\mathrm{s}} > 0$，$H(t)$ 为亥维赛德阶跃函数。当 $V(t)$ 达到阈值 $V_{\\mathrm{th}}$ 时，神经元发放一个脉冲，且 $V(t)$ 被重置为 $V_{\\mathrm{r}}$；假设存在一个绝对不应期以防止立即再次发放脉冲。\n\n在一个固定窗口 $[0, T]$ 内，考虑两种监督式读出：\n(i) 脉冲计数读出 $r_{\\mathrm{sc}} = N(T)$，即在 $[0, T]$ 内发放的总脉冲数；\n(ii) 基于电压的读出 $r_{\\mathrm{v}} = \\int_{0}^{T} V(t)\\, dt$。\n\n对于一个目标值 $r^{\\ast}$，定义二次损失 $L = \\frac{1}{2}\\left(r - r^{\\ast}\\right)^2$，其中 $r$ 为 $r_{\\mathrm{sc}}$ 或 $r_{\\mathrm{v}}$。考虑在基准权重 $w_k$ 周围对单个突触权重进行无穷小扰动 $w_k \\mapsto w_k + \\delta w$，该扰动在 $[0, T]$ 内产生固定的脉冲序列和固定的重置时间。\n\n根据控制动力学和读出定义，推导在两种读出下的梯度敏感度 $\\frac{\\partial L}{\\partial w_k}$，并比较它们在不改变脉冲时间的小扰动 $\\delta w$ 下的行为。然后，在每种情况下，为基于梯度的监督式训练的适用性找出正确的推论。\n\n下列陈述中哪些是正确的？\n\nA) 在精确的脉冲非线性下，对于在 $[0, T]$ 内不改变阈值穿越次数的所有扰动 $\\delta w$，$\\frac{\\partial r_{\\mathrm{sc}}}{\\partial w_k} = 0$；因此，对于脉冲计数读出，$\\frac{\\partial L}{\\partial w_k}$ 在权重空间中几乎处处为零，若无额外技术，则会阻碍标准的梯度下降。\n\nB) 对于基于电压的读出，$\\frac{\\partial L}{\\partial w_k} = \\left(r_{\\mathrm{v}} - r^{\\ast}\\right) \\int_{0}^{T} \\frac{\\partial V(t)}{\\partial w_k}\\, dt$，其中 $\\frac{\\partial V(t)}{\\partial w_k}$ 在每个脉冲间歇期上求解线性敏感度方程 $\\tau_{\\mathrm{m}} \\frac{d}{dt}\\left(\\frac{\\partial V(t)}{\\partial w_k}\\right) = -\\frac{\\partial V(t)}{\\partial w_k} + R\\, x_k(t)$，并在脉冲时间固定时，在重置点具有适当的连续性；因此梯度存在、有界，并随膜和突触时间常数缩放，有利于稳定的基于梯度的训练。\n\nC) 如果脉冲生成非线性被一个在阈值附近具有非零斜率的可微代理 $\\sigma\\!\\left(V(t) - V_{\\mathrm{th}}\\right)$ 替代，那么脉冲计数梯度将与基于电压的梯度在一个常数乘法因子内变得相同，这意味着两种读出具有等效的梯度稳定性。\n\nD) 由于 $V(t)$ 中由重置引起的不连续性，基于电压的读出不可避免地会遭受梯度爆炸的影响，使得 $\\frac{\\partial V(t)}{\\partial w_k}$ 无界，并导致梯度下降不稳定。\n\nE) 对于脉冲计数读出，可以通过无穷小有限差分获得无偏梯度估计，而无需代理平滑，因为差商的极限几乎处处等于真实的非零导数。\n\n选择所有正确选项。请将您的答案以大写字母按字母升序连接的形式提供。",
            "solution": "用户希望对所提供的问题陈述进行批判性验证，然后进行完整的推导和选项评估。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n*   **神经元模型**：漏积分-发放 (LIF)。\n*   **状态变量**：膜电位，$V(t)$。\n*   **参数**：膜时间常数 $\\tau_{\\mathrm{m}}$，输入电阻 $R$，突触时间常数 $\\tau_{\\mathrm{s}}$，发放阈值 $V_{\\mathrm{th}}$，重置电位 $V_{\\mathrm{r}}$。\n*   **阈下动力学**：$\\tau_{\\mathrm{m}} \\frac{dV(t)}{dt} = -V(t) + R \\sum_{j} w_j x_j(t)$。\n*   **突触输入**：突触电流 $x_j(t) = (s_j \\ast \\epsilon)(t)$，其中 $s_j(t) = \\sum_{m} \\delta\\!\\left(t - t^{(j)}_m\\right)$ 是突触前脉冲序列，$\\epsilon(t) = \\frac{1}{\\tau_{\\mathrm{s}}} e^{-t/\\tau_{\\mathrm{s}}} H(t)$ 是突触核。$w_j$ 是突触权重。\n*   **脉冲发放与重置**：当 $V(t)$ 达到 $V_{\\mathrm{th}}$ 时，发放一个脉冲，$V(t)$ 重置为 $V_{\\mathrm{r}}$。假设有绝对不应期。\n*   **观测窗口**：$[0, T]$。\n*   **读出**：\n    *   (i) 脉冲计数读出：$r_{\\mathrm{sc}} = N(T)$，在 $[0, T]$ 内的总脉冲数。\n    *   (ii) 基于电压的读出：$r_{\\mathrm{v}} = \\int_{0}^{T} V(t)\\, dt$。\n*   **损失函数**：$L = \\frac{1}{2}\\left(r - r^{\\ast}\\right)^2$，其中 $r \\in \\{r_{\\mathrm{sc}}, r_{\\mathrm{v}}\\}$。\n*   **分析条件**：分析针对单个权重 $w_k$ 的无穷小扰动，这些扰动不改变 $[0, T]$ 内的脉冲序列或脉冲时间。\n\n**步骤2：使用提取的已知条件进行验证**\n\n*   **科学依据**：该问题牢固地植根于计算神经科学和神经形态计算领域。LIF 神经元、指数突触模型以及监督学习任务的构建都是标准且被广泛研究的概念。其所探讨的核心问题——脉冲生成事件的不可微性——是训练脉冲神经网络 (SNN) 的一个核心挑战。\n*   **适定性**：这是一个适定问题。动力学由脉冲间的线性常微分方程 (ODE) 描述。读出是电压轨迹的良定义泛函。问题要求在明确说明的（尽管是简化的）固定脉冲时间假设下求损失函数的梯度。这使得我们能够对感兴趣的量进行唯一且有意义的推导。\n*   **客观性**：该问题以精确、客观的数学语言陈述。所有术语在该领域内都是标准且明确的。\n\n问题陈述没有科学或事实上的不健全之处，是可形式化的，对于所提问题是完整的，在其理论背景下并非不切实际，结构良好，并探讨了一个非平凡的问题。固定脉冲时间的假设是一种刻意的简化，旨在揭示不同读出方式下梯度的内在属性，这是一种常见的分析技术。\n\n**步骤3：结论与行动**\n\n该问题是**有效的**。可以继续进行分析。\n\n### 推导与选项分析\n\n二次损失 $L = \\frac{1}{2}(r - r^{\\ast})^2$ 关于突触权重 $w_k$ 的梯度由链式法则给出：\n$$ \\frac{\\partial L}{\\partial w_k} = \\frac{\\partial L}{\\partial r} \\frac{\\partial r}{\\partial w_k} = (r - r^{\\ast}) \\frac{\\partial r}{\\partial w_k} $$\n问题的核心是评估在两种不同读出（$r_{\\mathrm{sc}}$ 和 $r_{\\mathrm{v}}$）下，以及在无穷小权重扰动不改变脉冲时间的假设下，项 $\\frac{\\partial r}{\\partial w_k}$ 的值。\n\n**情况1：脉冲计数读出 ($r = r_{\\mathrm{sc}}$)**\n\n脉冲计数读出为 $r_{\\mathrm{sc}} = N(T)$，即在区间 $[0, T]$ 内的总脉冲数。根据其性质，$N(T)$ 是一个整值函数。作为权重 $w_k$ 的函数，$N(T; w_k)$ 是一个阶跃函数。它在 $w_k$ 的连续区间内是常数，仅在特定的 $w_k$ 值处发生不连续跳跃，在这些点上权重的改变会导致一个脉冲被增加或移除。\n\n问题明确假设我们分析的扰动“不改变脉冲时间”。这直接意味着脉冲数 $N(T)$ 对于这些扰动保持不变。因此，对于一个小的变化 $\\delta w$，读出的变化为 $\\delta r_{\\mathrm{sc}} = 0$。\n\n导数被定义为差商的极限：\n$$ \\frac{\\partial r_{\\mathrm{sc}}}{\\partial w_k} = \\lim_{\\delta w \\to 0} \\frac{N(T; w_k + \\delta w) - N(T; w_k)}{\\delta w} $$\n根据问题的假设，对于任何足够小且不跨越不连续点的 $\\delta w$，分子为零。因此，导数为：\n$$ \\frac{\\partial r_{\\mathrm{sc}}}{\\partial w_k} = 0 $$\n这在除了一个测度为零的不连续点集合之外的所有 $w_k$ 上都成立。因此，脉冲计数读出的损失梯度为：\n$$ \\frac{\\partial L}{\\partial w_k} = (r_{\\mathrm{sc}} - r^{\\ast}) \\cdot 0 = 0 \\quad (\\text{几乎处处}) $$\n这就是在基于脉冲时间或计数的 SNN 训练中著名的“梯度消失”问题。\n\n**情况2：基于电压的读出 ($r = r_{\\mathrm{v}}$)**\n\n基于电压的读出为 $r_{\\mathrm{v}} = \\int_{0}^{T} V(t)\\, dt$。它关于 $w_k$ 的梯度是：\n$$ \\frac{\\partial r_{\\mathrm{v}}}{\\partial w_k} = \\frac{\\partial}{\\partial w_k} \\int_{0}^{T} V(t; w_k)\\, dt = \\int_{0}^{T} \\frac{\\partial V(t; w_k)}{\\partial w_k}\\, dt $$\n在这里，微分和积分的交换是允许的，因为积分限是固定的，并且 $V(t)$ 在脉冲之间是 $w_k$ 的连续函数。我们定义电压敏感度为 $S_k(t) = \\frac{\\partial V(t)}{\\partial w_k}$。\n\n为了找到 $S_k(t)$ 的方程，我们对阈下动力学方程关于 $w_k$ 求导：\n$$ \\frac{\\partial}{\\partial w_k} \\left( \\tau_{\\mathrm{m}} \\frac{dV(t)}{dt} = -V(t) + R \\sum_{j} w_j x_j(t) \\right) $$\n假设平滑性允许交换导数，我们得到：\n$$ \\tau_{\\mathrm{m}} \\frac{d}{dt} \\left(\\frac{\\partial V(t)}{\\partial w_k}\\right) = -\\frac{\\partial V(t)}{\\partial w_k} + R \\frac{\\partial}{\\partial w_k}} \\left( \\sum_{j} w_j x_j(t) \\right) $$\n和的导数为：\n$$ \\frac{\\partial}{\\partial w_k} \\left( \\sum_{j} w_j x_j(t) \\right) = \\sum_{j} \\frac{\\partial(w_j)}{\\partial w_k} x_j(t) = x_k(t) $$\n因为 $x_j(t)$ 不依赖于突触后权重 $w_j$。因此，敏感度 $S_k(t)$ 服从线性 ODE：\n$$ \\tau_{\\mathrm{m}} \\frac{dS_k(t)}{dt} = -S_k(t) + R x_k(t) $$\n该方程在每个脉冲间歇期都成立。现在，考虑在固定脉冲时间 $t=t_f$ 处的重置效应。脉冲后，电位被重置为一个常数值：$V(t_f^+) = V_{\\mathrm{r}}$。重置后瞬间的敏感度为：\n$$ S_k(t_f^+) = \\frac{\\partial V(t_f^+)}{\\partial w_k} = \\frac{\\partial V_{\\mathrm{r}}}{\\partial w_k} = 0 $$\n因为 $V_{\\mathrm{r}}$ 是一个固定参数。因此，在每个脉冲时间，敏感度 $S_k(t)$ 被重置为 $0$。这起到稳定机制的作用。输入 $R x_k(t)$ 是衰减指数的和，并且是有界的。$S_k(t)$ 的 ODE 是稳定的，并由有界输入驱动。周期性地重置为 $0$ 可以防止 $S_k(t)$ 无界增长。因此，$S_k(t)$ 是时间的有界函数。\n\n所以，积分 $\\int_{0}^{T} S_k(t)\\, dt$ 是有限且良定义的。基于电压的读出的损失梯度为：\n$$ \\frac{\\partial L}{\\partial w_k} = (r_{\\mathrm{v}} - r^{\\ast}) \\int_{0}^{T} S_k(t)\\, dt $$\n该梯度通常是连续且非零的，使其适用于标准的基于梯度的优化方法。\n\n### 选项评估\n\n**A) 在精确的脉冲非线性下，对于在 $[0, T]$ 内不改变阈值穿越次数的所有扰动 $\\delta w$，$\\frac{\\partial r_{\\mathrm{sc}}}{\\partial w_k} = 0$；因此，对于脉冲计数读出，$\\frac{\\partial L}{\\partial w_k}$ 在权重空间中几乎处处为零，若无额外技术，则会阻碍标准的梯度下降。**\n\n此陈述准确反映了脉冲计数读出的推导。整值脉冲计数的导数几乎处处为零。因此，损失梯度也几乎处处为零，这意味着标准梯度下降算法将无法找到有用的学习信号。为了克服这一点，需要诸如代理梯度之类的额外技术。\n**结论：正确。**\n\n**B) 对于基于电压的读出，$\\frac{\\partial L}{\\partial w_k} = \\left(r_{\\mathrm{v}} - r^{\\ast}\\right) \\int_{0}^{T} \\frac{\\partial V(t)}{\\partial w_k}\\, dt$，其中 $\\frac{\\partial V(t)}{\\partial w_k}$ 在每个脉冲间歇期上求解线性敏感度方程 $\\tau_{\\mathrm{m}} \\frac{d}{dt}\\left(\\frac{\\partial V(t)}{\\partial w_k}\\right) = -\\frac{\\partial V(t)}{\\partial w_k} + R\\, x_k(t)$，并在脉冲时间固定时，在重置点具有适当的连续性；因此梯度存在、有界，并随膜和突触时间常数缩放，有利于稳定的基于梯度的训练。**\n\n此陈述正确地指出了梯度的形式和线性敏感度方程。“在重置点具有适当的连续性”这一说法略有不精确——敏感度 $\\frac{\\partial V(t)}{\\partial w_k}$ 本身是不连续的，被重置为 $0$。然而，在固定脉冲时间的假设下，这个重置条件是处理动力学的“适当”方式，允许分段积分进行。梯度存在、有界（由于稳定的动力学和重置），并且适用于基于梯度的训练，这一结论是正确的。与时间常数的缩放关系源于 ODE 的形式。\n**结论：正确。**\n\n**C) 如果脉冲生成非线性被一个在阈值附近具有非零斜率的可微代理 $\\sigma\\!\\left(V(t) - V_{\\mathrm{th}}\\right)$ 替代，那么脉冲计数梯度将与基于电压的梯度在一个常数乘法因子内变得相同，这意味着两种读出具有等效的梯度稳定性。**\n\n如果我们使用代理方法，其中“脉冲计数”被建模为 $r'_{\\mathrm{sc}} = \\int_0^T \\sigma(V(t)-V_{\\mathrm{th}}) dt$，其关于 $w_k$ 的导数是 $\\frac{\\partial r'_{\\mathrm{sc}}}{\\partial w_k} = \\int_0^T \\sigma'(V(t)-V_{\\mathrm{th}}) \\frac{\\partial V(t)}{\\partial w_k} dt$。基于电压的读出的导数是 $\\frac{\\partial r_{\\mathrm{v}}}{\\partial w_k} = \\int_0^T \\frac{\\partial V(t)}{\\partial w_k} dt$。项 $\\sigma'(V(t)-V_{\\mathrm{th}})$ 是时间的函数，因为它依赖于 $V(t)$。它在积分内部充当一个时变加权因子。因此，两个梯度并非在一个常数因子内相同。该陈述在数学上是错误的。\n**结论：不正确。**\n\n**D) 由于 $V(t)$ 中由重置引起的不连续性，基于电压的读出不可避免地会遭受梯度爆炸的影响，使得 $\\frac{\\partial V(t)}{\\partial w_k}$ 无界，并导致梯度下降不稳定。**\n\n这与事实恰恰相反。虽然电压 $V(t)$ 是不连续的，但其敏感度 $\\frac{\\partial V(t)}{\\partial w_k}$ 并非无界。正如我们的推导所示，在固定脉冲时间的假设下，重置机制在每次脉冲时将敏感度强制置为 $0$。这种周期性重置是一种稳定特性，可以防止敏感度爆炸。\n**结论：不正确。**\n\n**E) 对于脉冲计数读出，可以通过无穷小有限差分获得无偏梯度估计，而无需代理平滑，因为差商的极限几乎处处等于真实的非零导数。**\n\n该陈述包含一个根本性错误。对于像脉冲计数这样的阶跃函数，定义导数的差商的极限几乎处处为零。它并非“几乎处处非零”。虽然有限差分计算会得到该导数的无偏估计，但该估计值将几乎处处为0，无法为学习提供任何信息。问题不在于估计方法，而在于真实梯度本身的性质。\n**结论：不正确。**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "在明确了不可微性的问题之后，我们现在转向最主流的解决方案：代理梯度法。本练习将带你深入该技术的核心数学机制。你将为一个衡量脉冲序列相似度的 van Rossum 损失函数，严格推导其权重更新规则，通过应用链式法则和代理导数来处理脉冲神经元的时间动态 。",
            "id": "4061662",
            "problem": "考虑一个脉冲神经网络（SNN）中的单输出神经元。该神经元由离散时间下的脉冲响应模型（SRM）建模，采样间隔为均匀的 $\\Delta t$。设突触前脉冲序列为 $\\{S^{\\mathrm{pre}}_{t}\\}_{t=1}^{T}$，其中 $S^{\\mathrm{pre}}_{t} \\in \\{0,1\\}$ 表示在时间索引 $t$ 处有脉冲。神经元的膜电位由线性突触响应给出\n$$\nV_{t} = w\\,(\\epsilon * S^{\\mathrm{pre}})_{t},\n$$\n其中 $w \\in \\mathbb{R}$ 是一个突触权重，$(\\epsilon * S^{\\mathrm{pre}})_{t}$ 表示离散时间因果卷积 $(\\epsilon * S^{\\mathrm{pre}})_{t} = \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$，而 $\\{\\epsilon_{j}\\}_{j \\ge 0}$ 是一个固定的突触核序列。神经元根据阈值关系 $S_{t} = H(V_{t} - V_{\\theta})$ 发出一个输出脉冲序列 $\\{S_{t}\\}_{t=1}^{T}$，其中 $H(\\cdot)$ 是 Heaviside 阶跃函数，$V_{\\theta}$ 是一个固定的阈值。对于监督学习，提供一个目标脉冲序列 $\\{S^{\\mathrm{tar}}_{t}\\}_{t=1}^{T}$。损失函数为 van Rossum 损失，定义如下\n$$\nL = \\frac{1}{2} \\sum_{t=1}^{T} \\Big( (\\kappa * S)_{t} - (\\kappa * S^{\\mathrm{tar}})_{t} \\Big)^{2},\n$$\n其中 $(\\kappa * S)_{t} = \\sum_{j=0}^{t-1} \\kappa_{j}\\,S_{t-j}$ 是与固定的 van Rossum 核 $\\{\\kappa_{j}\\}_{j \\ge 0}$ 的因果卷积，例如离散指数核 $\\kappa_{j} = \\frac{\\Delta t}{\\tau_{r}} \\exp\\!\\big(-\\frac{j\\,\\Delta t}{\\tau_{r}}\\big)$（对于 $j \\ge 0$），时间常数为 $\\tau_{r} > 0$。为了能够进行基于梯度的优化，对脉冲非线性使用代理梯度：将不明确的导数 $\\partial S_{t}/\\partial V_{t}$ 替换为一个预设的代理函数 $\\phi(V_{t})$，该函数在 $V_{\\theta}$ 的邻域内非零，在其他地方为零。\n\n仅从因果离散卷积、链式法则以及上述损失和神经元模型的定义出发，推导梯度 $\\partial L / \\partial w$ 的显式闭式解析表达式，该表达式应使用序列 $\\{\\kappa_{j}\\}$、$\\{\\epsilon_{j}\\}$、$\\{S_{t}\\}$、$\\{S^{\\mathrm{tar}}_{t}\\}$ 和 $\\{S^{\\mathrm{pre}}_{t}\\}$ 来表示。您的最终表达式必须显式地展示出对一个滤波后误差信号的依赖，该信号源于 van Rossum 损失和因果卷积结构。将最终答案表示为单个解析表达式。不要提供数值。",
            "solution": "问题要求推导 van Rossum 损失函数 $L$ 相对于突触权重 $w$ 的梯度，该神经元由脉冲响应模型（SRM）描述。该求解过程要求在离散时间信号和卷积的背景下严格应用微积分的链式法则，并引入代理梯度的概念来处理不可微的脉冲发放机制。\n\n让我们首先重述所提供的核心定义：\n在时间步 $t$ 的膜电位是：\n$$\nV_{t} = w\\,(\\epsilon * S^{\\mathrm{pre}})_{t} = w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\n$$\n在时间步 $t$ 的输出脉冲由以下方式生成：\n$$\nS_{t} = H(V_{t} - V_{\\theta})\n$$\n其中 $H(\\cdot)$ 是 Heaviside 阶跃函数。此函数的导数是不明确的。按照指示，我们通过以下替换来使用代理梯度：\n$$\n\\frac{\\partial S_{t}}{\\partial V_{t}} \\approx \\phi(V_{t})\n$$\n其中 $\\phi(\\cdot)$ 是一个近似导数的合适函数，例如一个以阈值为中心的窄脉冲。\n\n损失函数由下式给出：\n$$\nL = \\frac{1}{2} \\sum_{k=1}^{T} \\Big( (\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} \\Big)^{2}\n$$\n为方便起见，我们将滤波后的脉冲序列定义为 $s_{k} = (\\kappa * S)_{k}$ 和 $s^{\\mathrm{tar}}_{k} = (\\kappa * S^{\\mathrm{tar}})_{k}$。那么损失可以写成：\n$$\nL = \\frac{1}{2} \\sum_{k=1}^{T} (s_{k} - s^{\\mathrm{tar}}_{k})^{2}\n$$\n我们的目标是计算梯度 $\\frac{\\partial L}{\\partial w}$。权重 $w$ 影响膜电位 $\\{V_{t}\\}$，膜电位进而决定输出脉冲 $\\{S_{t}\\}$，输出脉冲再经过滤波产生出现在损失 $L$ 中的 $\\{s_{k}\\}$。依赖链为 $w \\rightarrow \\{V_{t}\\} \\rightarrow \\{S_{t}\\} \\rightarrow \\{s_{k}\\} \\rightarrow L$。我们应用多元链式法则，将 $w$ 通过每个脉冲 $S_{t}$ 对 $L$ 的影响求和：\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial S_{t}} \\frac{\\partial S_{t}}{\\partial w}\n$$\n这个分解将问题分为两部分：\n1.  计算一个脉冲 $S_{t}$ 相对于权重 $w$ 的导数，这涉及神经元的动力学。\n2.  计算损失 $L$ 相对于一个脉冲 $S_{t}$ 的导数，这涉及损失函数的结构。\n\n我们先来解决第一部分，$\\frac{\\partial S_{t}}{\\partial w}$。对神经元动力学使用链式法则：\n$$\n\\frac{\\partial S_{t}}{\\partial w} = \\frac{\\partial S_{t}}{\\partial V_{t}} \\frac{\\partial V_{t}}{\\partial w}\n$$\n我们将第一项替换为代理梯度：$\\frac{\\partial S_{t}}{\\partial V_{t}} \\approx \\phi(V_{t})$。对于第二项，我们将 $V_{t}$ 的表达式对 $w$ 求导：\n$$\n\\frac{\\partial V_{t}}{\\partial w} = \\frac{\\partial}{\\partial w} \\left( w\\,(\\epsilon * S^{\\mathrm{pre}})_{t} \\right) = (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\n结合这些结果，得到脉冲相对于权重的导数：\n$$\n\\frac{\\partial S_{t}}{\\partial w} = \\phi(V_{t}) \\, (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\n这一项将突触后神经元的状态（通过 $\\phi(V_{t})$）与突触前输入活动（通过滤波后的突触前脉冲序列 $(\\epsilon * S^{\\mathrm{pre}})_{t}$）耦合起来。\n\n现在，我们来解决第二部分，$\\frac{\\partial L}{\\partial S_{t}}$。损失 $L$ 通过滤波后的脉冲序列 $s_{k} = (\\kappa*S)_{k}$ 依赖于 $S_{t}$。我们再次应用链式法则：\n$$\n\\frac{\\partial L}{\\partial S_{t}} = \\sum_{k=1}^{T} \\frac{\\partial L}{\\partial s_{k}} \\frac{\\partial s_{k}}{\\partial S_{t}}\n$$\n$L$ 相对于 $s_{k}$ 的导数很简单：\n$$\n\\frac{\\partial L}{\\partial s_{k}} = \\frac{\\partial}{\\partial s_{k}} \\left[ \\frac{1}{2} \\sum_{m=1}^{T} (s_{m} - s^{\\mathrm{tar}}_{m})^{2} \\right] = s_{k} - s^{\\mathrm{tar}}_{k}\n$$\n接下来，我们计算卷积 $s_{k}$ 相对于单个脉冲 $S_{t}$ 的导数：\n$$\ns_{k} = (\\kappa * S)_{k} = \\sum_{j=0}^{k-1} \\kappa_{j}\\,S_{k-j}\n$$\n仅当求和中的某一项包含 $S_{t}$ 时，导数 $\\frac{\\partial s_{k}}{\\partial S_{t}}$ 才非零。这种情况发生在 $k-j=t$ 时，即 $j=k-t$。为了使这是一个有效的求和索引，我们必须有 $0 \\le k-t \\le k-1$，这可以简化为 $t \\ge 1$ 和 $k \\ge t$。如果满足此条件，导数就是 $S_{t}$ 的系数，即 $\\kappa_{k-t}$。\n$$\n\\frac{\\partial s_{k}}{\\partial S_{t}} =\n\\begin{cases}\n\\kappa_{k-t}  &\\text{if } k \\ge t \\\\\n0  &\\text{if } k  t\n\\end{cases}\n$$\n将这两个导数代回，我们得到一个脉冲 $S_{t}$ 对总损失的影响：\n$$\n\\frac{\\partial L}{\\partial S_{t}} = \\sum_{k=t}^{T} (s_{k} - s^{\\mathrm{tar}}_{k}) \\kappa_{k-t} = \\sum_{k=t}^{T} \\Big( (\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} \\Big) \\kappa_{k-t}\n$$\n这一项表示一个误差信号，它被滤波并从未来（$k \\ge t$）向现在（$t$）反向传播。我们用 $\\delta_{t}$ 表示这个“滤波后的误差信号”：\n$$\n\\delta_{t} = \\sum_{k=t}^{T} \\kappa_{k-t} \\Big( (\\kappa * (S - S^{\\mathrm{tar}}))_{k} \\Big)\n$$\n最后，我们将 $\\frac{\\partial S_{t}}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial S_{t}}$ 的结果代入原始的求和式中，从而组合出完整的梯度 $\\frac{\\partial L}{\\partial w}$：\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\delta_{t} \\frac{\\partial S_{t}}{\\partial w} = \\sum_{t=1}^{T} \\delta_{t} \\, \\phi(V_{t}) \\, (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\n为了按要求获得最终的显式表达式，我们代入 $\\delta_t$、$V_t$ 和卷积的完整定义。滤波后的突触前输入为 $(\\epsilon * S^{\\mathrm{pre}})_{t} = \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$。膜电位为 $V_{t} = w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$。滤波后的误差信号包含卷积差 $(\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} = \\sum_{l=0}^{k-1} \\kappa_l (S_{k-l} - S^{\\mathrm{tar}}_{k-l})$。\n\n将所有部分组合起来，得到梯度的最终解析表达式：\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\phi\\left(w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\\right) \\left(\\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\\right) \\left( \\sum_{k=t}^{T} \\kappa_{k-t} \\left[ \\sum_{l=0}^{k-1} \\kappa_{l}(S_{k-l} - S^{\\mathrm{tar}}_{k-l}) \\right] \\right)\n$$\n该表达式符合神经科学中常见的三因子学习规则结构，其中权重更新与突触前轨迹、依赖于突触后状态的项以及依赖于未来的误差信号的乘积成正比。",
            "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\phi\\left(w \\sum_{j=0}^{t-1} \\epsilon_j S^{\\mathrm{pre}}_{t-j}\\right) \\left(\\sum_{j=0}^{t-1} \\epsilon_j S^{\\mathrm{pre}}_{t-j}\\right) \\left( \\sum_{k=t}^{T} \\kappa_{k-t} \\left[ \\sum_{l=0}^{k-1} \\kappa_l (S_{k-l} - S^{\\mathrm{tar}}_{k-l}) \\right] \\right)}\n$$"
        },
        {
            "introduction": "理论联系实践方能彰显其力量。最后一个动手练习要求你将基于梯度的学习原理转化为一种计算高效的、事件驱动的算法。你将设计并实现一个遵循神经形态硬件约束的局部学习规则，通过从零开始构建一个功能性仿真来巩固你的理解 。",
            "id": "4061647",
            "problem": "考虑一个尖峰神经网络（SNN）中的单个突触后神经元，它接收$N$个突触前尖峰序列。该神经元产生一个输出尖峰序列，同时有一个监督信号提供目标尖峰序列。训练必须是监督式的，并遵守事件驱动的计算约束：权重更新只允许在尖峰事件发生时进行，并且必须仅依赖于神经元上可用的局部状态变量，无需连续时间积分或非局部信息。目标是推导一个满足这些约束的训练过程，并实现它来为给定的尖峰序列计算权重更新。\n\n基本基础和建模假设：\n- 尖峰表示为狄拉克δ函数的和。对于突触前神经元$i$，其尖峰序列为$s_i(t) = \\sum_k \\delta(t - t_k^{(i)})$，其中$t_k^{(i)}$是突触前尖峰时间。\n- 为每个输入定义一个突触资格迹$x_i(t)$，由突触前尖峰通过一阶滤波器驱动：\n$$\\frac{d x_i(t)}{d t} = -\\frac{x_i(t)}{\\tau_s} + s_i(t), \\quad x_i(0) = 0,$$\n其中$\\tau_s$是突触时间常数，单位为秒。这意味着$x_i(t)$以时间常数$\\tau_s$指数衰减，并在每个突触前尖峰时间点跳变增加1。\n- 将实际输出尖峰序列和目标输出尖峰序列的低通滤波版本分别定义为$z(t)$和$z^\\ast(t)$：\n$$\\frac{d z(t)}{d t} = -\\frac{z(t)}{\\tau_o} + y(t), \\quad z(0) = 0,$$\n$$\\frac{d z^\\ast(t)}{d t} = -\\frac{z^\\ast(t)}{\\tau_o} + y^\\ast(t), \\quad z^\\ast(0) = 0,$$\n其中$y(t) = \\sum_j \\delta(t - t_j^{\\text{post}})$是实际的突触后尖峰序列，$y^\\ast(t) = \\sum_m \\delta(t - t_m^{\\text{targ}})$是目标尖峰序列，$\\tau_o$是输出滤波器的时间常数，单位为秒。\n- 监督目标是滤波后的实际输出和目标输出之间的平方误差：\n$$\\mathcal{L} = \\frac{1}{2}\\int_{0}^{T} \\left(z(t) - z^\\ast(t)\\right)^2 \\, dt,$$\n定义在有限时间范围$[0, T]$内，其中$T$的单位为秒。学习率$\\eta  0$（无单位）。\n\n事件驱动约束和推导目标：\n- 推导一个用于突触权重$w_i$的局部、监督式更新规则，该规则仅依赖于尖峰事件和局部可用的状态变量。更新必须只在突触后尖峰和目标尖峰的并集中的尖峰时间点发生，并且不能需要非局部信息或连续时间的伴随积分。\n- $w_i$的更新必须能够在事件时间$t_e$表示为瞬时误差信号$e(t_e)$和局部资格迹$x_i(t_e)$的函数，其中$e(t) \\triangleq z(t) - z^\\ast(t)$。所有量必须仅使用局部状态和尖峰事件即可计算。\n- 对于在同一时间$t_e$发生的突触后尖峰和目标尖峰， $z(t)$和$z^\\ast(t)$都接收到相同的瞬时增量，因此误差$e(t_e)$必须在应用两个增量后计算，从而不会因排序而产生人为偏差。\n\n算法实现约束：\n- 计算必须通过迭代所有事件时间（即所有输入的突触前尖峰、所有突触后尖峰和所有目标尖峰）的排序并集来执行。\n- 在事件之间，每个状态变量都呈指数衰减：对于时间增量$\\Delta t$，$x_i \\leftarrow x_i \\cdot \\exp(-\\Delta t/\\tau_s)$，$z \\leftarrow z \\cdot \\exp(-\\Delta t/\\tau_o)$，$z^\\ast \\leftarrow z^\\ast \\cdot \\exp(-\\Delta t/\\tau_o)$。\n- 在事件时间$t_e$，应用在$t_e$发生的所有突触前、突触后和目标尖峰的跳变，然后计算误差$e(t_e) = z(t_e) - z^\\ast(t_e)$。只有当在$t_e$有突触后或目标尖峰发生时，才允许进行权重更新。\n- 时间量必须以秒表示。最终输出应为无单位的数值浮点数。\n\n测试套件规范：\n- 您必须实现所推导的事件驱动权重更新过程，并为每个测试用例计算总权重增量向量$\\Delta \\mathbf{w} = [\\Delta w_0, \\Delta w_1, \\dots, \\Delta w_{N-1}]$。在所有测试用例中统一使用以下参数值：$\\tau_s = 0.020$（秒），$\\tau_o = 0.050$（秒），以及学习率$\\eta = 0.1$（无单位）。所有尖峰时间均以秒为单位。\n- 对于每个测试用例，提供每个输入的突触前尖峰时间、突触后实际尖峰时间以及目标尖峰时间：\n\n测试用例 A（一般情况）：\n- $N = 2$\n- 突触前尖峰：$S_0 = [0.010, 0.030, 0.080]$，$S_1 = [0.015, 0.060]$\n- 突触后尖峰：$Y = [0.070, 0.120]$\n- 目标尖峰：$Y^\\ast = [0.050, 0.100, 0.150]$\n\n测试用例 B（无尖峰的边界情况）：\n- $N = 2$\n- 突触前尖峰：$S_0 = []$，$S_1 = []$\n- 突触后尖峰：$Y = []$\n- 目标尖峰：$Y^\\ast = []$\n\n测试用例 C（事件同时发生的边缘情况）：\n- $N = 2$\n- 突触前尖峰：$S_0 = [0.040, 0.100]$，$S_1 = [0.040, 0.100]$\n- 突触后尖峰：$Y = [0.100]$\n- 目标尖峰：$Y^\\ast = [0.070, 0.100]$\n\n最终输出规范：\n- 您的程序必须产生一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个测试用例的结果本身是一个浮点数列表，表示该案例的权重增量。例如，格式应为：\n$$[\\,[\\Delta w_0^{(A)},\\Delta w_1^{(A)}],\\,[\\Delta w_0^{(B)},\\Delta w_1^{(B)}],\\,[\\Delta w_0^{(C)},\\Delta w_1^{(C)}]\\,].$$",
            "solution": "该问题定义明确，在计算神经科学和神经形态工程领域具有坚实的科学基础。它描述了单个尖峰神经元的监督学习场景，根植于使用滤波后尖峰序列和突触资格迹的既定模型。目标函数是标准的平方误差，而约束强制要求事件驱动的局部学习规则，这是生物学上合理且计算高效的尖峰神经网络（SNN）算法的一个关键特征。所有必要的参数和初始条件都已提供，以确保一个唯一的、确定性的解决方案。\n\n我们将首先推导满足问题约束的学习规则，然后概述其实现的事件驱动算法。\n\n**事件驱动学习规则的推导**\n\n目标是通过梯度下降法最小化关于突触权重$w_i$的损失函数$\\mathcal{L}$。损失函数由下式给出：\n$$\n\\mathcal{L} = \\frac{1}{2}\\int_{0}^{T} \\left(z(t) - z^\\ast(t)\\right)^2 \\, dt = \\frac{1}{2}\\int_{0}^{T} e(t)^2 \\, dt\n$$\n其中$e(t) \\triangleq z(t) - z^\\ast(t)$是瞬时误差。对于权重$w_i$的标准梯度下降更新将与$-\\frac{\\partial \\mathcal{L}}{\\partial w_i}$成正比。偏导数为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\int_{0}^{T} e(t) \\frac{\\partial z(t)}{\\partial w_i} \\, dt\n$$\n变量$z(t)$是突触后尖峰序列$y(t)$的低通滤波版本。$y(t)$的生成取决于突触前输入的加权和，因此也取决于权重$w_i$。然而，问题陈述并未提供指定此依赖关系的神经元模型，即它没有定义$y(t)$和权重$w_i$之间的关系。这一遗漏使得从第一性原理严格推导$\\frac{\\partial z(t)}{\\partial w_i}$以及学习规则变得不可能。\n\n尽管如此，问题提供了一系列强约束，使我们能够构建出预期的学习规则。该规则必须是：\n1. 事件驱动的，更新仅在突触后尖峰（$t_j^{\\text{post}}$）或目标尖峰（$t_m^{\\text{targ}}$）的时刻发生。\n2. 局部的，仅依赖于神经元上可用的状态变量，特别是在事件时间$t_e$的突触前资格迹$x_i(t_e)$和误差信号$e(t_e)$。\n\n这些约束引导我们走向一个近似梯度下降的启发式规则，该规则与已建立的SNN监督学习规则（例如ReSuMe，SuperSpike）一致。总权重变化$\\Delta w_i$是在所有突触后和目标尖峰时间集合（表示为$T_{events} = \\{t_j^{\\text{post}}\\} \\cup \\{t_m^{\\text{targ}}\\}$）中每个学习事件时间$t_e$的离散更新$\\delta w_i(t_e)$之和。\n$$\n\\Delta w_i = \\sum_{t_e \\in T_{events}} \\delta w_i(t_e)\n$$\n我们提出一个结合了局部资格迹$x_i(t_e)$和局部误差$e(t_e)$的更新形式：\n$$\n\\delta w_i(t_e) = -\\eta \\cdot e(t_e) \\cdot x_i(t_e)\n$$\n让我们分析一下这个规则的行为：\n- 如果在$t_e$时刻发生了一个目标尖峰但没有相应的实际尖峰，$z^\\ast(t_e)$会增加，使得$e(t_e) = z(t_e) - z^\\ast(t_e)$为负。更新量$\\delta w_i(t_e) > 0$。如果该突触最近是活跃的（即$x_i(t_e)$较大），这将增强该突触（增加$w_i$），这是一种赫布机制，旨在鼓励未来在这个时间点产生一个尖峰。\n- 如果在$t_e$时刻发生了一个实际尖峰但没有相应的目标尖峰，$z(t_e)$会增加，使得$e(t_e)$为正。更新量$\\delta w_i(t_e)  0$。如果该突触最近是活跃的，这将削弱该突触（减少$w_i$），这是一种反赫布机制，旨在抑制错误的尖峰。\n- 如果在$t_e$时刻同时发生了一个实际尖峰和一个目标尖峰，问题规定$e(t_e)$在两个跳变都发生后计算。新的状态值为$z(t_e) = z(t_e^-) + 1$和$z^\\ast(t_e) = z^\\ast(t_e^-) + 1$，其中$t_e^-$是事件发生前一刻的时间。那么误差为$e(t_e) = (z(t_e^-) + 1) - (z^\\ast(t_e^-) + 1) = z(t_e^-) - z^\\ast(t_e^-) = e(t_e^-)$。更新取决于正确时间尖峰发生前的误差，驱动系统纠正任何残留的时间不匹配。\n\n该规则满足问题的所有约束，并实现了一种合理的学习机制。因此，神经元$i$的总突触权重更新为：\n$$\n\\Delta w_i = -\\eta \\sum_{t_e \\in T_{events}} e(t_e) x_i(t_e)\n$$\n其中，总和中的所有量都在它们各自的事件时间$t_e$进行评估。\n\n**算法流程**\n\n计算必须以事件驱动的方式执行。算法如下：\n1.  将来自突触前尖峰（$S_i$）、突触后尖峰（$Y$）和目标尖峰（$Y^\\ast$）的所有唯一事件时间收集到一个时间线中，并按升序排序。\n2.  初始化状态变量：所有$i$的突触迹$x_i(0) = 0$，滤波后输出$z(0) = 0$，滤波后目标$z^\\ast(0) = 0$，以及总权重变化$\\Delta w_i(0) = 0$。\n3.  遍历排序后的事件时间。对于每个在时间$t_k$的事件，让前一个事件时间为$t_{k-1}$。\n4.  计算时间间隔$\\Delta t = t_k - t_{k-1}$。\n5.  通过应用指数衰减来演化状态变量超过$\\Delta t$：\n    $$\n    x_i(t_k^-) = x_i(t_{k-1}) \\exp(-\\Delta t / \\tau_s) \\\\\n    z(t_k^-) = z(t_{k-1}) \\exp(-\\Delta t / \\tau_o) \\\\\n    z^\\ast(t_k^-) = z^\\ast(t_{k-1}) \\exp(-\\Delta t / \\tau_o)\n    $$\n6.  在时间$t_k$，应用在该时间发生的所有尖峰的瞬时跳变。不同状态变量的应用顺序无关紧要。\n    - 对于在$t_k$时刻来自输入$i$的每个突触前尖峰：$x_i(t_k) = x_i(t_k^-) + 1$。\n    - 如果在$t_k$时刻有突触后尖峰发生：$z(t_k) = z(t_k^-) + 1$。\n    - 如果在$t_k$时刻有目标尖峰发生：$z^\\ast(t_k) = z^\\ast(t_k^-) + 1$。\n    未受$t_k$时刻跳变影响的状态变量保持其衰减后的值，例如，如果输入$j$没有尖峰，则$x_j(t_k) = x_j(t_k^-)$。\n7.  如果$t_k$时刻的事件是学习事件（即在$Y$或$Y^\\ast$中有尖峰发生），则计算权重更新：\n    a. 在所有跳变后计算误差信号：$e(t_k) = z(t_k) - z^\\ast(t_k)$。\n    b. 更新每个突触$i$的总权重变化：$\\Delta w_i \\leftarrow \\Delta w_i - \\eta \\cdot e(t_k) \\cdot x_i(t_k)$。\n8.  对所有事件时间重复此过程。$\\Delta w_i$的最终值即为结果。\n\n此过程严格遵守了问题的事件驱动和局部性约束。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to compute weight updates for supervised SNN learning.\n    \"\"\"\n    \n    # Uniform parameters for all test cases\n    tau_s = 0.020  # seconds\n    tau_o = 0.050  # seconds\n    eta = 0.1      # unitless\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # Test case A (general case)\n        {\n            \"N\": 2,\n            \"S\": [[0.010, 0.030, 0.080], [0.015, 0.060]],\n            \"Y\": [0.070, 0.120],\n            \"Y_star\": [0.050, 0.100, 0.150]\n        },\n        # Test case B (boundary case with no spikes)\n        {\n            \"N\": 2,\n            \"S\": [[], []],\n            \"Y\": [],\n            \"Y_star\": []\n        },\n        # Test case C (edge case with simultaneous events)\n        {\n            \"N\": 2,\n            \"S\": [[0.040, 0.100], [0.040, 0.100]],\n            \"Y\": [0.100],\n            \"Y_star\": [0.070, 0.100]\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        S_spikes = case[\"S\"]\n        Y_spikes = case[\"Y\"]\n        Y_star_spikes = case[\"Y_star\"]\n\n        # 1. Collect all event times and create a sorted unique list\n        event_times = set()\n        for s_train in S_spikes:\n            event_times.update(s_train)\n        event_times.update(Y_spikes)\n        event_times.update(Y_star_spikes)\n        \n        sorted_event_times = sorted(list(event_times))\n\n        # 2. Initialize state variables\n        x = np.zeros(N)\n        z = 0.0\n        z_star = 0.0\n        delta_w = np.zeros(N)\n        current_time = 0.0\n\n        # 3. Iterate through event-driven simulation\n        for event_time in sorted_event_times:\n            # 4. Calculate time interval and 5. Evolve states via decay\n            delta_t = event_time - current_time\n            if delta_t > 0:\n                x *= np.exp(-delta_t / tau_s)\n                z *= np.exp(-delta_t / tau_o)\n                z_star *= np.exp(-delta_t / tau_o)\n            \n            # Keep track if a learning event occurs at this time\n            is_learning_event = False\n\n            # 6. Apply instantaneous jumps\n            # Presynaptic spikes\n            for i in range(N):\n                if event_time in S_spikes[i]:\n                    # Using a tolerance for float comparison is robust\n                    if any(np.isclose(event_time, t) for t in S_spikes[i]):\n                        x[i] += 1.0\n            \n            # Postsynaptic spike\n            if any(np.isclose(event_time, t) for t in Y_spikes):\n                z += 1.0\n                is_learning_event = True\n\n            # Target spike\n            if any(np.isclose(event_time, t) for t in Y_star_spikes):\n                z_star += 1.0\n                is_learning_event = True\n\n            # 7. If it's a learning event, compute and apply weight update\n            if is_learning_event:\n                error = z - z_star\n                update = -eta * error * x\n                delta_w += update\n            \n            current_time = event_time\n\n        all_results.append(delta_w.tolist())\n\n    # Format the final output string exactly as specified\n    result_str = \",\".join([f\"[{','.join(map(lambda v: f'{v:.7f}', r))}]\" for r in all_results])\n    print(f\"[[{result_str}]]\")\n\nsolve()\n```"
        }
    ]
}