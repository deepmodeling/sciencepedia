{
    "hands_on_practices": [
        {
            "introduction": "在为脉冲神经网络（SNN）设计监督学习任务时，首要的决策之一是选择合适的损失函数或“读出”机制。这个选择至关重要，因为它直接影响梯度是否能够有效流过网络。本练习将通过对比基于脉冲计数和基于膜电位的两种不同读出方式，帮助您亲身体会脉冲发放的非微分特性所带来的“梯度消失”挑战，从而深刻理解为何需要更复杂的梯度估计算法。",
            "id": "4061639",
            "problem": "考虑一个接收突触前脉冲序列的突触后 Leaky Integrate-and-Fire (LIF) 神经元。设膜电位表示为 $V(t)$，膜时间常数为 $\\tau_{\\mathrm{m}}$，输入电阻为 $R$。在任意两个脉冲之间的区间上，亚阈值膜动力学满足线性常微分方程\n$$\\tau_{\\mathrm{m}} \\frac{dV(t)}{dt} = -V(t) + R \\sum_{j} w_j x_j(t),$$\n其中 $w_j$ 是突触权重，$x_j(t)$ 是通过将突触前脉冲序列 $s_j(t) = \\sum_{m} \\delta\\!\\left(t - t^{(j)}_m\\right)$ 与指数突触核 $\\epsilon(t) = \\frac{1}{\\tau_{\\mathrm{s}}} e^{-t/\\tau_{\\mathrm{s}}} H(t)$ 进行卷积得到的突触电流，其中突触时间常数 $\\tau_{\\mathrm{s}} > 0$，$H(t)$ 为 Heaviside 阶跃函数。当 $V(t)$ 达到阈值 $V_{\\mathrm{th}}$ 时，会发放一个脉冲，并且 $V(t)$ 被重置为 $V_{\\mathrm{r}}$；假设存在一个绝对不应期，以防止立即再次发放脉冲。\n\n在一个固定的时间窗口 $[0, T]$ 内考虑两种监督式读出：\n(i) 脉冲计数读出 $r_{\\mathrm{sc}} = N(T)$，即在 $[0, T]$ 内发放的脉冲总数；\n(ii) 基于电压的读出 $r_{\\mathrm{v}} = \\int_{0}^{T} V(t)\\, dt$。\n\n对于一个目标值 $r^{\\ast}$，定义二次损失函数 $L = \\frac{1}{2}\\left(r - r^{\\ast}\\right)^2$，其中 $r$ 为 $r_{\\mathrm{sc}}$ 或 $r_{\\mathrm{v}}$。考虑在基准权重 $w_k$ 周围对单个突触权重进行无穷小扰动 $w_k \\mapsto w_k + \\delta w$，该扰动在 $[0, T]$ 内产生固定的脉冲序列和固定的重置时间。\n\n根据控制动力学和读出定义，推导两种读出下的梯度敏感度 $\\frac{\\partial L}{\\partial w_k}$，并比较它们在不改变脉冲时间的微小扰动 $\\delta w$ 下的行为。然后，为每种情况确定基于梯度的监督式训练适用性的正确推论。\n\n下列陈述中，哪些是正确的？\n\nA) 在精确的脉冲非线性下，对于所有不改变 $[0, T]$ 内阈值穿越次数的扰动 $\\delta w$，都有 $\\frac{\\partial r_{\\mathrm{sc}}}{\\partial w_k} = 0$；因此，对于脉冲计数读出，$\\frac{\\partial L}{\\partial w_k}$ 在权重空间中几乎处处为零，这阻碍了标准梯度下降法的应用，除非采用额外技术。\n\nB) 对于基于电压的读出，$\\frac{\\partial L}{\\partial w_k} = \\left(r_{\\mathrm{v}} - r^{\\ast}\\right) \\int_{0}^{T} \\frac{\\partial V(t)}{\\partial w_k}\\, dt$，其中 $\\frac{\\partial V(t)}{\\partial w_k}$ 在每个脉冲间期求解线性敏感度方程 $\\tau_{\\mathrm{m}} \\frac{d}{dt}\\left(\\frac{\\partial V(t)}{\\partial w_k}\\right) = -\\frac{\\partial V(t)}{\\partial w_k} + R\\, x_k(t)$，并在脉冲时间固定时，跨重置点具有适当的连续性；因此梯度存在、有界，并与膜和突触时间常数成比例，有利于稳定的基于梯度的训练。\n\nC) 如果脉冲生成非线性被一个在阈值附近具有非零斜率的可微代理函数 $\\sigma\\!\\left(V(t) - V_{\\mathrm{th}}\\right)$ 替代，那么脉冲计数梯度将与基于电压的梯度在相差一个常数乘法因子的情况下变得相同，这意味着两种读出具有等效的梯度稳定性。\n\nD) 基于电压的读出不可避免地会因 $V(t)$ 中由重置引起的不连续性而遭受梯度爆炸问题，使得 $\\frac{\\partial V(t)}{\\partial w_k}$ 无界，并导致梯度下降不稳定。\n\nE) 对于脉冲计数读出，可以通过无穷小有限差分获得无偏梯度估计，而无需代理平滑，因为差商的极限几乎处处等于真实的非零导数。\n\n选择所有正确选项。将您的答案以大写字母按字母升序连接的形式提供。",
            "solution": "用户需要对所提供的问题陈述进行批判性验证，然后对选项进行全面推导和评估。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **神经元模型**：Leaky Integrate-and-Fire (LIF)。\n*   **状态变量**：膜电位 $V(t)$。\n*   **参数**：膜时间常数 $\\tau_{\\mathrm{m}}$，输入电阻 $R$，突触时间常数 $\\tau_{\\mathrm{s}}$，发放阈值 $V_{\\mathrm{th}}$，重置电位 $V_{\\mathrm{r}}$。\n*   **亚阈值动力学**：$\\tau_{\\mathrm{m}} \\frac{dV(t)}{dt} = -V(t) + R \\sum_{j} w_j x_j(t)$。\n*   **突触输入**：突触电流 $x_j(t) = (s_j \\ast \\epsilon)(t)$，其中 $s_j(t) = \\sum_{m} \\delta\\!\\left(t - t^{(j)}_m\\right)$ 是突触前脉冲序列，$\\epsilon(t) = \\frac{1}{\\tau_{\\mathrm{s}}} e^{-t/\\tau_{\\mathrm{s}}} H(t)$ 是突触核。$w_j$ 是突触权重。\n*   **脉冲发放与重置**：当 $V(t)$ 达到 $V_{\\mathrm{th}}$ 时，发放一个脉冲，且 $V(t)$ 重置为 $V_{\\mathrm{r}}$。假设有绝对不应期。\n*   **观测窗口**：$[0, T]$。\n*   **读出**：\n    *   (i) 脉冲计数读出：$r_{\\mathrm{sc}} = N(T)$，在 $[0, T]$ 内的总脉冲数。\n    *   (ii) 基于电压的读出：$r_{\\mathrm{v}} = \\int_{0}^{T} V(t)\\, dt$。\n*   **损失函数**：$L = \\frac{1}{2}\\left(r - r^{\\ast}\\right)^2$，其中 $r \\in \\{r_{\\mathrm{sc}}, r_{\\mathrm{v}}\\}$。\n*   **分析条件**：分析涉及对单个权重 $w_k$ 进行无穷小扰动，且该扰动不改变 $[0, T]$ 内的脉冲序列或脉冲时间。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n*   **科学依据**：该问题牢固地植根于计算神经科学和神经形态计算。LIF 神经元、指数突触模型以及监督学习任务的表述都是标准且被广泛研究的概念。所解决的核心问题——脉冲生成事件的不可微性——是训练脉冲神经网络 (SNN) 的核心挑战。\n*   **适定性**：该问题是适定的。动力学由脉冲之间的线性常微分方程 (ODE) 描述。读出是电压轨迹的明确定义的泛函。问题要求在明确说明的（尽管是简化的）固定脉冲时间的假设下，求损失函数的梯度。这允许对感兴趣的量进行唯一且有意义的推导。\n*   **客观性**：问题以精确、客观的数学语言陈述。所有术语在该领域内都是标准的且无歧义的。\n\n问题陈述没有科学或事实上的不健全之处，是可形式化的，对于所提问题是完整的，在其理论背景下并非不切实际，结构良好，并解决了一个非平凡的问题。固定脉冲时间的假设是一种刻意的简化，旨在揭示不同读出的梯度基本属性，这是一种常见的分析技术。\n\n**步骤 3：结论与行动**\n\n问题是有效的。可以继续进行分析。\n\n### 推导与选项分析\n\n二次损失 $L = \\frac{1}{2}(r - r^{\\ast})^2$ 关于突触权重 $w_k$ 的梯度由链式法则给出：\n$$ \\frac{\\partial L}{\\partial w_k} = \\frac{\\partial L}{\\partial r} \\frac{\\partial r}{\\partial w_k} = (r - r^{\\ast}) \\frac{\\partial r}{\\partial w_k} $$\n问题的核心是在无穷小权重扰动不改变脉冲时间的既定假设下，评估两种不同读出（$r_{\\mathrm{sc}}$ 和 $r_{\\mathrm{v}}$）的导数项 $\\frac{\\partial r}{\\partial w_k}$。\n\n**情况 1：脉冲计数读出 ($r = r_{\\mathrm{sc}}$)**\n\n脉冲计数读出是 $r_{\\mathrm{sc}} = N(T)$，即在区间 $[0, T]$ 内的脉冲总数。就其本质而言，$N(T)$ 是一个整数值函数。作为权重 $w_k$ 的函数，$N(T; w_k)$ 是一个阶跃函数。它在 $w_k$ 的连续范围内是常数，并且在特定的 $w_k$ 值处不连续地跳跃，在这些值处权重的改变会导致一个脉冲被添加或移除。\n\n问题明确假设我们分析的扰动“不改变脉冲时间”。这直接意味着对于这些扰动，脉冲数量 $N(T)$ 保持不变。因此，对于一个微小的变化 $\\delta w$，读出的变化为 $\\delta r_{\\mathrm{sc}} = 0$。\n\n导数定义为差商的极限：\n$$ \\frac{\\partial r_{\\mathrm{sc}}}{\\partial w_k} = \\lim_{\\delta w \\to 0} \\frac{N(T; w_k + \\delta w) - N(T; w_k)}{\\delta w} $$\n在问题的假设下，对于任何不跨越不连续点的足够小的 $\\delta w$，分子都为零。因此，导数为：\n$$ \\frac{\\partial r_{\\mathrm{sc}}}{\\partial w_k} = 0 $$\n这对所有 $w_k$ 都成立，除了一个测度为零的函数不连续点集。因此，脉冲计数读出的损失梯度为：\n$$ \\frac{\\partial L}{\\partial w_k} = (r_{\\mathrm{sc}} - r^{\\ast}) \\cdot 0 = 0 \\quad (\\text{几乎处处}) $$\n这就是在基于脉冲时间或计数的 SNN 训练中众所周知的“梯度消失”问题。\n\n**情况 2：基于电压的读出 ($r = r_{\\mathrm{v}}$)**\n\n基于电压的读出是 $r_{\\mathrm{v}} = \\int_{0}^{T} V(t)\\, dt$。它关于 $w_k$ 的梯度为：\n$$ \\frac{\\partial r_{\\mathrm{v}}}{\\partial w_k} = \\frac{\\partial}{\\partial w_k} \\int_{0}^{T} V(t; w_k)\\, dt = \\int_{0}^{T} \\frac{\\partial V(t; w_k)}{\\partial w_k}\\, dt $$\n在这里，微分和积分的交换是允许的，因为积分限是固定的，并且 $V(t)$ 在脉冲之间是 $w_k$ 的连续函数。我们将电压敏感度定义为 $S_k(t) = \\frac{\\partial V(t)}{\\partial w_k}$。\n\n为了找到 $S_k(t)$ 的方程，我们对亚阈值动力学关于 $w_k$ 进行微分：\n$$ \\frac{\\partial}{\\partial w_k} \\left( \\tau_{\\mathrm{m}} \\frac{dV(t)}{dt} = -V(t) + R \\sum_{j} w_j x_j(t) \\right) $$\n假设平滑性允许交换导数，我们得到：\n$$ \\tau_{\\mathrm{m}} \\frac{d}{dt} \\left(\\frac{\\partial V(t)}{\\partial w_k}\\right) = -\\frac{\\partial V(t)}{\\partial w_k} + R \\frac{\\partial}{\\partial w_k}} \\left( \\sum_{j} w_j x_j(t) \\right) $$\n和的导数为：\n$$ \\frac{\\partial}{\\partial w_k} \\left( \\sum_{j} w_j x_j(t) \\right) = \\sum_{j} \\frac{\\partial(w_j)}{\\partial w_k} x_j(t) = x_k(t) $$\n因为 $x_j(t)$ 不依赖于突触后权重 $w_j$。因此，敏感度 $S_k(t)$ 服从线性常微分方程：\n$$ \\tau_{\\mathrm{m}} \\frac{dS_k(t)}{dt} = -S_k(t) + R x_k(t) $$\n这个方程在每个脉冲间期都成立。现在，考虑在固定的脉冲时间 $t=t_f$ 处重置的影响。脉冲之后，电位被重置为一个常数值：$V(t_f^+) = V_{\\mathrm{r}}$。重置后瞬间的敏感度是：\n$$ S_k(t_f^+) = \\frac{\\partial V(t_f^+)}{\\partial w_k} = \\frac{\\partial V_{\\mathrm{r}}}{\\partial w_k} = 0 $$\n因为 $V_{\\mathrm{r}}$ 是一个固定参数。因此，在每个脉冲时间点，敏感度 $S_k(t)$ 被重置为 0。这起到了一种稳定机制的作用。输入 $R x_k(t)$ 是衰减指数的和，是有界的。$S_k(t)$ 的常微分方程是稳定的，并由有界输入驱动。周期性地重置为 0 防止了 $S_k(t)$ 无界增长。因此，$S_k(t)$ 是时间的有界函数。\n\n因此，积分 $\\int_{0}^{T} S_k(t)\\, dt$ 是有限且良定义的。基于电压的读出的损失梯度为：\n$$ \\frac{\\partial L}{\\partial w_k} = (r_{\\mathrm{v}} - r^{\\ast}) \\int_{0}^{T} S_k(t)\\, dt $$\n这个梯度通常是连续且非零的，使其适用于标准的基于梯度的优化方法。\n\n### 选项评估\n\n**A) 该陈述准确地反映了脉冲计数读出的推导过程。整数值脉冲计数的导数几乎处处为零。因此，损失梯度也几乎处处为零，这意味着标准梯度下降算法将无法找到有用的学习信号。为了克服这个问题，需要像代理梯度这样的额外技术。**\n**结论：正确。**\n\n**B) 该陈述正确地指出了梯度的形式和线性敏感度方程。短语“跨重置点具有适当的连续性”略有不精确——敏感度 $\\frac{\\partial V(t)}{\\partial w_k}$ 本身是不连续的，因为它被重置为 0。然而，在固定脉冲时间的假设下，这个重置条件是处理动力学的“适当”方式，使得分段积分得以进行。结论是梯度存在、有界（由于稳定的动力学和重置），并且适用于基于梯度的训练，这是正确的。与时间常数的比例关系源于常微分方程的形式。**\n**结论：正确。**\n\n**C) 如果我们使用代理方法，其中“脉冲计数”被建模为 $r'_{\\mathrm{sc}} = \\int_0^T \\sigma(V(t)-V_{\\mathrm{th}}) dt$，其关于 $w_k$ 的导数是 $\\frac{\\partial r'_{\\mathrm{sc}}}{\\partial w_k} = \\int_0^T \\sigma'(V(t)-V_{\\mathrm{th}}) \\frac{\\partial V(t)}{\\partial w_k} dt$。基于电压的读出的导数是 $\\frac{\\partial r_{\\mathrm{v}}}{\\partial w_k} = \\int_0^T \\frac{\\partial V(t)}{\\partial w_k} dt$。项 $\\sigma'(V(t)-V_{\\mathrm{th}})$ 是时间的函数，因为它依赖于 $V(t)$。它在积分内部充当一个时变权重因子。因此，这两个梯度并非仅相差一个常数因子。该陈述在数学上是错误的。**\n**结论：不正确。**\n\n**D) 这与事实恰恰相反。虽然电压 $V(t)$ 是不连续的，但其敏感度 $\\frac{\\partial V(t)}{\\partial w_k}$ 并非无界。如我们的推导所示，在固定脉冲时间的假设下，当重置机制应用于敏感度动力学时，它会在每次脉冲时将敏感度强制置为 0。这种周期性重置是一种稳定特性，可以防止敏感度爆炸。**\n**结论：不正确。**\n\n**E) 该陈述包含一个根本性错误。定义导数的差商的极限，对于像脉冲计数这样的阶跃函数来说，几乎处处为零。它不是“几乎处处非零”。虽然有限差分计算可以得出该导数的无偏估计，但该估计值几乎处处为 0，无法为学习提供任何信息。问题不在于估计方法，而在于真实梯度本身的性质。**\n**结论：不正确。**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "认识到简单的脉冲计数目标存在问题后，我们自然会问：如何为更复杂的、基于脉冲序列的损失函数（如 van Rossum 距离）计算梯度？本练习将引导您运用替代梯度法这一核心技术，从第一性原理出发，为基于脉冲响应模型（SRM）的神经元推导出完整的学习规则。通过这个推导，您将揭示许多生物可塑性学习规则中常见的“三因子”结构是如何从数学上自然产生的。",
            "id": "4061662",
            "problem": "考虑一个脉冲神经网络（SNN）中的单输出神经元。该神经元由离散时间下的脉冲响应模型（SRM）建模，采样间隔为均匀的 $\\Delta t$。设突触前脉冲序列为 $\\{S^{\\mathrm{pre}}_{t}\\}_{t=1}^{T}$，其中 $S^{\\mathrm{pre}}_{t} \\in \\{0,1\\}$ 表示在时间索引 $t$ 处有脉冲。该神经元的膜电位由线性突触响应给出\n$$\nV_{t} = w\\,(\\epsilon * S^{\\mathrm{pre}})_{t},\n$$\n其中 $w \\in \\mathbb{R}$ 是一个突触权重，$(\\epsilon * S^{\\mathrm{pre}})_{t}$ 表示离散时间因果卷积 $(\\epsilon * S^{\\mathrm{pre}})_{t} = \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$，而 $\\{\\epsilon_{j}\\}_{j \\ge 0}$ 是一个固定的突触核序列。神经元根据阈值关系 $S_{t} = H(V_{t} - V_{\\theta})$ 发出输出脉冲序列 $\\{S_{t}\\}_{t=1}^{T}$，其中 $H(\\cdot)$ 是亥维赛阶跃函数，$V_{\\theta}$ 是一个固定阈值。对于监督学习，提供了一个目标脉冲序列 $\\{S^{\\mathrm{tar}}_{t}\\}_{t=1}^{T}$。损失函数是 van Rossum 损失，定义为\n$$\nL = \\frac{1}{2} \\sum_{t=1}^{T} \\Big( (\\kappa * S)_{t} - (\\kappa * S^{\\mathrm{tar}})_{t} \\Big)^{2},\n$$\n其中 $(\\kappa * S)_{t} = \\sum_{j=0}^{t-1} \\kappa_{j}\\,S_{t-j}$ 是与一个固定的 van Rossum 核 $\\{\\kappa_{j}\\}_{j \\ge 0}$ 的因果卷积，例如离散指数核 $\\kappa_{j} = \\frac{\\Delta t}{\\tau_{r}} \\exp\\!\\big(-\\frac{j\\,\\Delta t}{\\tau_{r}}\\big)$（对于 $j \\ge 0$），时间常数为 $\\tau_{r}  0$。为了能够进行基于梯度的优化，对脉冲非线性使用替代梯度：将不明确的导数 $\\partial S_{t}/\\partial V_{t}$ 替换为一个指定的替代函数 $\\phi(V_{t})$，该函数在 $V_{\\theta}$ 的一个邻域内非零，在其他地方为零。\n\n仅从因果离散卷积、链式法则以及上述损失和神经元模型的定义出发，推导梯度 $\\partial L / \\partial w$ 关于序列 $\\{\\kappa_{j}\\}$、$\\{\\epsilon_{j}\\}$、$\\{S_{t}\\}$、$\\{S^{\\mathrm{tar}}_{t}\\}$ 和 $\\{S^{\\mathrm{pre}}_{t}\\}$ 的显式闭式解析表达式。你的最终表达式必须显示出对一个滤波后误差信号的显式依赖，该信号源于 van Rossum 损失和因果卷积结构。将最终答案表示为单个解析表达式。不要提供数值。",
            "solution": "该问题要求推导 van Rossum 损失函数 $L$ 相对于一个由脉冲响应模型（SRM）描述的神经元的突触权重 $w$ 的梯度。该求解过程需要在离散时间信号和卷积的背景下严格应用微积分的链式法则，并结合针对不可微的脉冲生成机制的替代梯度概念。\n\n我们首先重申所提供的核心定义：\n在时间步 $t$ 的膜电位是：\n$$\nV_{t} = w\\,(\\epsilon * S^{\\mathrm{pre}})_{t} = w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\n$$\n在时间步 $t$ 的输出脉冲由以下公式生成：\n$$\nS_{t} = H(V_{t} - V_{\\theta})\n$$\n其中 $H(\\cdot)$ 是亥维赛阶跃函数。该函数的导数是不明确的。按照指示，我们通过以下替换来使用替代梯度：\n$$\n\\frac{\\partial S_{t}}{\\partial V_{t}} \\approx \\phi(V_{t})\n$$\n其中 $\\phi(\\cdot)$ 是一个近似导数的合适函数，例如一个以阈值为中心的窄脉冲。\n\n损失函数由下式给出：\n$$\nL = \\frac{1}{2} \\sum_{k=1}^{T} \\Big( (\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} \\Big)^{2}\n$$\n为方便起见，我们将滤波后的脉冲序列定义为 $s_{k} = (\\kappa * S)_{k}$ 和 $s^{\\mathrm{tar}}_{k} = (\\kappa * S^{\\mathrm{tar}})_{k}$。损失函数可以写成：\n$$\nL = \\frac{1}{2} \\sum_{k=1}^{T} (s_{k} - s^{\\mathrm{tar}}_{k})^{2}\n$$\n我们的目标是计算梯度 $\\frac{\\partial L}{\\partial w}$。权重 $w$ 影响膜电位 $\\{V_{t}\\}$，膜电位进而决定输出脉冲 $\\{S_{t}\\}$，输出脉冲再被滤波产生出现在损失 $L$ 中的 $\\{s_{k}\\}$。依赖链是 $w \\rightarrow \\{V_{t}\\} \\rightarrow \\{S_{t}\\} \\rightarrow \\{s_{k}\\} \\rightarrow L$。我们应用多元链式法则，将 $w$ 通过每个脉冲 $S_{t}$ 对 $L$ 的影响求和：\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial S_{t}} \\frac{\\partial S_{t}}{\\partial w}\n$$\n这个分解将问题分为两部分：\n1.  计算一个脉冲 $S_{t}$ 相对于权重 $w$ 的导数，这涉及到神经元的动力学。\n2.  计算损失 $L$ 相对于一个脉冲 $S_{t}$ 的导数，这涉及到损失函数的结构。\n\n我们先处理第一部分，$\\frac{\\partial S_{t}}{\\partial w}$。对神经元动力学使用链式法则：\n$$\n\\frac{\\partial S_{t}}{\\partial w} = \\frac{\\partial S_{t}}{\\partial V_{t}} \\frac{\\partial V_{t}}{\\partial w}\n$$\n我们将第一项替换为替代梯度：$\\frac{\\partial S_{t}}{\\partial V_{t}} \\approx \\phi(V_{t})$。对于第二项，我们将 $V_{t}$ 的表达式对 $w$ 求导：\n$$\n\\frac{\\partial V_{t}}{\\partial w} = \\frac{\\partial}{\\partial w} \\left( w\\,(\\epsilon * S^{\\mathrm{pre}})_{t} \\right) = (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\n结合这些结果，得到脉冲相对于权重的导数：\n$$\n\\frac{\\partial S_{t}}{\\partial w} = \\phi(V_{t}) \\, (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\n这一项将突触后神经元的状态（通过 $\\phi(V_{t})$）与突触前输入活动（通过滤波后的突触前脉冲序列 $(\\epsilon * S^{\\mathrm{pre}})_{t}$）耦合起来。\n\n现在，我们处理第二部分，$\\frac{\\partial L}{\\partial S_{t}}$。损失 $L$ 通过滤波后的脉冲序列 $s_{k} = (\\kappa*S)_{k}$ 依赖于 $S_{t}$。我们再次应用链式法则：\n$$\n\\frac{\\partial L}{\\partial S_{t}} = \\sum_{k=1}^{T} \\frac{\\partial L}{\\partial s_{k}} \\frac{\\partial s_{k}}{\\partial S_{t}}\n$$\n$L$ 相对于 $s_{k}$ 的导数很简单：\n$$\n\\frac{\\partial L}{\\partial s_{k}} = \\frac{\\partial}{\\partial s_{k}} \\left[ \\frac{1}{2} \\sum_{m=1}^{T} (s_{m} - s^{\\mathrm{tar}}_{m})^{2} \\right] = s_{k} - s^{\\mathrm{tar}}_{k}\n$$\n接下来，我们计算卷积 $s_{k}$ 相对于单个脉冲 $S_{t}$ 的导数：\n$$\ns_{k} = (\\kappa * S)_{k} = \\sum_{j=0}^{k-1} \\kappa_{j}\\,S_{k-j}\n$$\n导数 $\\frac{\\partial s_{k}}{\\partial S_{t}}$ 仅当求和中的某一项包含 $S_{t}$ 时才非零。这发生在 $k-j=t$ 时，即 $j=k-t$。要使其成为求和中的有效索引，必须满足 $0 \\le k-t \\le k-1$，这简化为 $t \\ge 1$ 和 $k \\ge t$。如果满足此条件，导数就是 $S_{t}$ 的系数，即 $\\kappa_{k-t}$。\n$$\n\\frac{\\partial s_{k}}{\\partial S_{t}} =\n\\begin{cases}\n\\kappa_{k-t}  \\text{if } k \\ge t \\\\\n0  \\text{if } k  t\n\\end{cases}\n$$\n将这两个导数代回，我们得到一个脉冲 $S_{t}$ 对总损失的影响：\n$$\n\\frac{\\partial L}{\\partial S_{t}} = \\sum_{k=t}^{T} (s_{k} - s^{\\mathrm{tar}}_{k}) \\kappa_{k-t} = \\sum_{k=t}^{T} \\Big( (\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} \\Big) \\kappa_{k-t}\n$$\n这一项代表了一个从未来（$k \\ge t$）向现在（$t$）反向传播并被滤波的误差信号。我们用 $\\delta_{t}$ 来表示这个“滤波后的误差信号”：\n$$\n\\delta_{t} = \\sum_{k=t}^{T} \\kappa_{k-t} \\Big( (\\kappa * (S - S^{\\mathrm{tar}}))_{k} \\Big)\n$$\n最后，我们将 $\\frac{\\partial S_{t}}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial S_{t}}$ 的结果代入最初的求和式中，从而组合出完整的梯度 $\\frac{\\partial L}{\\partial w}$：\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\delta_{t} \\frac{\\partial S_{t}}{\\partial w} = \\sum_{t=1}^{T} \\delta_{t} \\, \\phi(V_{t}) \\, (\\epsilon * S^{\\mathrm{pre}})_{t}\n$$\n为了按要求获得最终的显式表达式，我们代入 $\\delta_t$、$V_t$ 和卷积的完整定义。滤波后的突触前输入是 $(\\epsilon * S^{\\mathrm{pre}})_{t} = \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$。膜电位是 $V_{t} = w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}$。滤波后的误差信号包含卷积差 $(\\kappa * S)_{k} - (\\kappa * S^{\\mathrm{tar}})_{k} = \\sum_{l=0}^{k-1} \\kappa_l (S_{k-l} - S^{\\mathrm{tar}}_{k-l})$。\n\n将所有部分组合起来，得到梯度的最终解析表达式：\n$$\n\\frac{\\partial L}{\\partial w} = \\sum_{t=1}^{T} \\phi\\left(w \\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\\right) \\left(\\sum_{j=0}^{t-1} \\epsilon_{j}\\,S^{\\mathrm{pre}}_{t-j}\\right) \\left( \\sum_{k=t}^{T} \\kappa_{k-t} \\left[ \\sum_{l=0}^{k-1} \\kappa_{l}(S_{k-l} - S^{\\mathrm{tar}}_{k-l}) \\right] \\right)\n$$\n这个表达式符合神经科学中常见的三因子学习法则结构，即权重更新与突触前迹、突触后状态依赖项和未来依赖的误差信号的乘积成正比。",
            "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\phi\\left(w \\sum_{j=0}^{t-1} \\epsilon_j S^{\\mathrm{pre}}_{t-j}\\right) \\left(\\sum_{j=0}^{t-1} \\epsilon_j S^{\\mathrm{pre}}_{t-j}\\right) \\left( \\sum_{k=t}^{T} \\kappa_{k-t} \\left[ \\sum_{l=0}^{k-1} \\kappa_l (S_{k-l} - S^{\\mathrm{tar}}_{k-l}) \\right] \\right)}\n$$"
        },
        {
            "introduction": "我们已经知道如何为单个神经元定义和计算梯度，但要训练一个完整的循环网络，我们还需要高效的算法来处理时间和空间上的依赖关系。本练习将视角从数学推导转向算法分析，要求您评估两种主流训练算法——时间反向传播（BPTT）和资格传播（e-prop）——在计算和内存方面的复杂度。理解这些算法的伸缩特性对于在实际应用中根据网络规模和硬件限制选择合适的训练策略至关重要。",
            "id": "4061601",
            "problem": "考虑一个全连接的循环脉冲神经网络 (SNN)，其包含 $N$ 个渗漏积分-发放神经元，并在 $T$ 个离散时间步长上具有离散时间动态。每个神经元到其他所有神经元都存在一个突触（包含或排除自连接不影响主阶缩放），因此可训练的突触权重数量为 $M = N^{2}$。假设在每个时间步，每个突触和每个神经元的前向计算需要恒定数量的浮点运算，该数量不依赖于 $N$ 或 $T$；并且存储任何标量状态变量需要恒定数量的机器字，也独立于 $N$ 和 $T$。训练使用监督学习，其标量损失取决于网络随时间的活动。\n\n使用两种训练算法：\n\n- 时间反向传播 (BPTT)：时间反向传播 (BPTT) 存储前向传播过程中的神经元状态，以便通过时间逆向过程计算精确梯度。假设 BPTT 在前向传播过程中不维护任何每个突触的资格迹。\n\n- 资格传播 (e-prop)：资格传播 (e-prop) 使用随时间前向演化的、每个突触的资格迹来在线更新梯度。假设 e-prop 不存储除了进入下一时间步所需之外的过去神经元状态。\n\n为使计算明确，采用以下约定：\n\n- 将算法特定的峰值工作内存计为超出可训练参数及其累积梯度存储之外的额外内存。因此，峰值内存计数中不包括 $M$ 个可训练权重、任何与这些权重直接相关的优化器状态或最终的梯度张量；仅包括算法机制特定的额外时间状态或每个突触的状态。\n\n- 将整个训练过程（一次前向扫描以及，如适用，一次后向扫描）的总算术运算次数计为计算复杂度。将每个时间步中每个突触和每个神经元的运算次数视为独立于 $N$ 和 $T$ 的常数。\n\n在这些假设下，推导以下四个量关于 $N$ 和 $T$ 的渐近缩放：\n\n$1.$ BPTT 的算法特定峰值工作内存。\n\n$2.$ e-prop 的算法特定峰值工作内存。\n\n$3.$ BPTT 在 $T$ 步序列上的总算术运算次数。\n\n$4.$ e-prop 在 $T$ 步序列上的总算术运算次数。\n\n最终答案以单行矩阵的形式给出，其中按顺序包含四个使用大O表示法表示的、作为 $N$ 和 $T$ 的函数的渐近表达式。忽略常数因子和低阶项。不需要数值四舍五入。以精确的符号形式表示最终结果。",
            "solution": "该问题要求推导在一个全连接循环脉冲神经网络 (SNN) 中，两种监督学习算法——时间反向传播 (BPTT) 和资格传播 (e-prop)——的峰值工作内存和总计算复杂度的渐近缩放。该网络包含 $N$ 个神经元，并运行 $T$ 个离散时间步。突触数量为 $M = N^2$。\n\n让我们根据给定的假设来定义分析范围：\n- 网络是全连接的，因此有 $M = O(N^2)$ 个突触权重。\n- 在每个时间步更新单个神经元状态或单个突触贡献的算术运算是常数，即 $O(1)$。\n- 存储任何单个标量状态变量（例如，膜电位、资格迹值）所需的内存是常数，即 $O(1)$。\n- “算法特定的工作内存”不包括 $M$ 个可训练权重及其累积梯度的存储。\n- “总算术运算次数”涵盖整个训练过程（前向和后向，如果适用）。\n\n我们将按顺序推导这四个量。\n\n$1.$ **BPTT 的算法特定峰值工作内存**\n\nBPTT 算法分两个过程运行：一个前向过程和一个后向过程。为了计算在给定时间步 $t$ 时损失函数关于网络参数和状态的精确梯度，后向过程需要访问在前向过程中于同一时间步 $t$ 以及可能在 $t+1$ 计算出的神经元状态。因此，该算法必须在整个前向过程中存储神经元状态的历史记录。\n\n问题明确指出 BPTT “存储前向传播过程中的神经元状态”。在每个时间步 $t \\in \\{1, 2, ..., T\\}$，必须保存 $N$ 个神经元中每一个的状态。一个神经元的状态（例如，其膜电位、脉冲输出）可以用恒定数量的标量变量来表示。根据问题的假设，存储这些状态每个神经元需要 $O(1)$ 的内存。\n\n- 在单个时间步存储所有 $N$ 个神经元状态所需的内存：$N \\times O(1) = O(N)$。\n- 为了执行从 $t=T$ 到 $t=1$ 的完整后向过程，所有 $T$ 个时间步的状态都必须可用。峰值内存使用量出现在前向过程结束时（在时间 $T$），此时整个历史记录都已被保存。\n- 所需总内存：（时间步数） $\\times$ （每个时间步的内存） $= T \\times O(N) = O(NT)$。\n\n这部分内存是算法特定的，因为它用于连接前向和后向过程，并且与参数或最终梯度的内存是分开的。因此，BPTT 的算法特定峰值工作内存的缩放级别为 $O(NT)$。\n\n$2.$ **e-prop 的算法特定峰值工作内存**\n\ne-prop 算法旨在在线计算近似梯度，从而避免了完整的时间后向过程。它通过维护一个“每个突触的资格迹”来实现这一点。对于每个突触，这个迹会持续追踪突触权重与最终损失之间的因果联系。问题指出，e-prop “使用每个突触的资格迹在线更新梯度”并且“不存储除了进入下一时间步所需之外的过去神经元状态”。\n\ne-prop 的主要算法特定内存成本是这些资格迹的存储。\n- 网络中的突触数量：$M = N^2$。\n- 每个突触都需要一个资格迹，它是一个标量或一个大小恒定的小向量。因此，存储一个迹的内存为 $O(1)$。\n- 所有资格迹的总内存：（突触数量） $\\times$ （每个迹的内存） $= M \\times O(1) = O(N^2)$。\n\n由于 e-prop 是一种在线算法，这些迹在每个时间步都会更新，但迹的总数在整个过程中保持不变。内存需求不随时间步数 $T$ 增长。因此，e-prop 的算法特定峰值工作内存的缩放级别为 $O(N^2)$。\n\n$3.$ **BPTT 的总算术运算次数**\n\nBPTT 的总计算量是前向过程和后向过程中运算的总和。\n\n- **前向过程：** 在每个时间步 $t \\in \\{1, 2, ..., T\\}$， $N$ 个神经元中每一个的状态都会更新。此更新取决于从所有其他神经元接收到的输入。\n    - $M = N^2$ 个突触中的每一个都对突触后神经元的状态有贡献。每个突触的计算量是 $O(1)$。每个时间步的总突触计算量为：$O(N^2)$。\n    - $N$ 个神经元中的每一个也进行内部状态更新（例如，膜电位衰减），这是每个神经元 $O(1)$ 的运算。每个时间步的总神经元特定计算量为：$O(N)$。\n    - 每个时间步的总计算量：$O(N^2) + O(N) = O(N^2)$。\n    - $T$ 个步长上的总前向过程计算量：$T \\times O(N^2) = O(N^2 T)$。\n\n- **后向过程：** 后向过程通过将误差信号从 $t=T$ 向下传播到 $t=1$，在展开的计算图中反向传播来计算梯度。后向过程的计算图结构与前向过程对称。\n    - 在每个时间步 $t$，算法计算损失关于神经元状态和权重的梯度。这涉及到通过所有 $M = N^2$ 个突触连接反向传播误差。\n    - 后向过程中每个时间步的计算成本也由突触计算主导，导致 $O(N^2)$ 次运算。\n    - $T$ 个步长上的总后向过程计算量：$T \\times O(N^2) = O(N^2 T)$。\n\n- **BPTT 总计算量：** （前向过程） + （后向过程） = $O(N^2 T) + O(N^2 T) = O(N^2 T)$。\n\n$4.$ **e-prop 的总算术运算次数**\n\nE-prop 在单次前向过程中计算梯度。在每个时间步，它必须执行标准的前向动态模拟，并额外更新其用于梯度估计的内部数据结构。\n\n- 在每个时间步 $t \\in \\{1, 2, ..., T\\}$：\n    - **前向动态：** 这与 BPTT 的前向过程相同。它涉及计算 $N^2$ 个突触的影响并更新 $N$ 个神经元。复杂度为 $O(N^2)$。\n    - **资格迹更新：** 算法必须更新 $M = N^2$ 个突触中每一个的资格迹。单个迹的更新规则是一个 $O(1)$ 的运算，取决于局部的突触前和突触后量。更新所有迹的总计算成本为 $O(N^2)$。\n    - **梯度累积：** 基于资格迹和误差信号，更新 $M = N^2$ 个权重中每一个的梯度。这也是每个时间步 $O(N^2)$ 的运算。\n- **每个时间步的总计算量：** （前向动态） + （迹更新） + （梯度累积） = $O(N^2) + O(N^2) + O(N^2) = O(N^2)$。\n- **e-prop 总计算量：** 这些操作重复 $T$ 个时间步。\n    - 总计算量：$T \\times (\\text{每步计算量}) = T \\times O(N^2) = O(N^2 T)$。\n\n总而言之，所要求的四个量具有以下渐近缩放：\n1.  BPTT 内存：$O(NT)$\n2.  e-prop 内存：$O(N^2)$\n3.  BPTT 计算量：$O(N^2 T)$\n4.  e-prop 计算量：$O(N^2 T)$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nO(NT)  O(N^2)  O(N^2 T)  O(N^2 T)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}