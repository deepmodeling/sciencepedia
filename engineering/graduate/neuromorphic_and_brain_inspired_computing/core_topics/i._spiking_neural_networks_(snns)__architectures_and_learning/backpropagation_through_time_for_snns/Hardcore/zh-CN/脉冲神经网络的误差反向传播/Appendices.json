{
    "hands_on_practices": [
        {
            "introduction": "在应用时间反向传播（BPTT）之前，我们必须首先精确地模拟神经元的前向动态。本练习将通过实现一个离散时间的渗漏整合发放（LIF）神经元模型，为后续的梯度计算构建计算图。掌握如何将连续时间的神经元动力学方程转化为离散时间的模拟，是计算神经科学中的一项基本技能，也是训练脉冲神经网络的基础。",
            "id": "4036252",
            "problem": "考虑一个在已知输入电流序列下，在离散时间内演化的单个 Leaky Integrate-and-Fire (LIF) 神经元。连续时间内的膜电位 $v(t)$ 服从常微分方程 (ODE) $dv/dt = -\\frac{1}{\\tau} v(t) + k I(t)$，其中 $\\tau$ 是膜时间常数，$k$ 是一个增益（可以吸收膜电阻等生物物理常数），$I(t)$ 是输入电流。当膜电位达到或超过一个阈值时，会发出一个脉冲。脉冲过后，膜电位被重置为一个指定的重置值。用于脉冲神经网络 (SNNs) 的随时间反向传播 (BPTT) 算法需要一个精确的、随时间展开的前向状态序列来定义计算图。\n\n假设采用均匀的离散时间步长，并使用指数欧拉方法对 LIF 动力学进行离散化。令 $\\alpha \\in (0,1)$ 表示对应于单步连续时间衰减的离散时间泄漏因子，$\\theta$ 表示发放阈值，$V_{\\text{reset}}$ 表示硬重置电位。令 $I_t$ 表示在步骤 $t$ 的离散时间驱动，其已经被缩放，以使其贡献能够如所选离散化方法所定义的那样，线性地进入阈值前膜电位。定义 Heaviside 阶跃函数 $H(x)$，当 $x \\ge 0$ 时返回 $1$，否则返回 $0$。在每个离散步骤中，神经元按以下方式演化：从时间索引 $t$ 的膜电位 $v_t$ 开始，形成一个阈值前电位，根据是否越过阈值来确定一个脉冲，并应用硬重置以获得下一步的膜电位 $v_{t+1}$。确切的顺序必须是：泄漏与输入整合、阈值判断、然后重置。在每个步骤 $t$，随时间反向传播 (BPTT) 所需的展开状态必须记录整合前状态 $v_t$、阈值前整合电位、脉冲指示符 $s_t \\in \\{0,1\\}$ 以及重置后状态 $v_{t+1}$。\n\n你的任务是：\n- 从连续时间 LIF ODE $dv/dt = -\\frac{1}{\\tau} v(t) + k I(t)$ 出发，在所述的指数欧拉离散化和硬重置条件下，推导出阈值前电位的离散时间递推公式和脉冲-重置规则，用给定的参数 $\\alpha$、$\\theta$、$V_{\\text{reset}}$ 和序列 $I_t$ 表示。\n- 实现一个程序，给定 $(\\alpha,\\theta,V_{\\text{reset}})$、初始膜电位 $v_0$ 以及一个由 $t=0,\\dots,T$ 索引的长度为 $T+1$ 的序列 $I_t$，计算并记录每个 $t=0,\\dots,T$ 的：整合前膜电位 $v_t$、阈值前电位、脉冲指示符 $s_t \\in \\{0,1\\}$ 和重置后膜电位 $v_{t+1}$。遵循确切的顺序：整合、阈值判断、重置。当阈值前电位恰好等于 $\\theta$ 时，使用临界情况处理规则 $s_t = 1$。\n- 对于下面的每个测试用例，报告 BPTT 所需的精确展开状态列表，其形式为列表的列表。其中第 $t$ 个条目是 $[t, v_t, \\text{pre\\_threshold}, s_t, v_{t+1}]$，$t$ 为整数，$v_t$ 和阈值前电位为浮点值，$s_t$ 为整数 $0$ 或 $1$，$v_{t+1}$ 为浮点值。\n\n使用以下具有科学合理性和自洽性参数值的测试套件：\n1. 典型脉冲发放情况：\n   - $\\alpha = 0.9$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.0$, $v_0 = 0.0$, $T = 5$, $I_t$ 对于 $t=0,\\dots,5$ 等于 $[0.5, 0.6, 0.0, 0.0, 0.0, 0.0]$。\n2. 边界阈值相等情况（测试临界情况处理 $H(0)=1$）：\n   - $\\alpha = 0.5$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.0$, $v_0 = 2.0$, $T = 2$, $I_t$ 对于 $t=0,\\dots,2$ 等于 $[0.0, 0.0, 0.0]$。\n3. 泄漏下的无脉冲累积：\n   - $\\alpha = 0.95$, $\\theta = 1.2$, $V_{\\text{reset}} = 0.0$, $v_0 = 0.1$, $T = 4$, $I_t$ 对于 $t=0,\\dots,4$ 等于 $[0.1, 0.1, 0.1, 0.1, 0.1]$。\n4. 带有非零重置的多次脉冲发放：\n   - $\\alpha = 0.8$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.2$, $v_0 = 0.0$, $T = 6$, $I_t$ 对于 $t=0,\\dots,6$ 等于 $[1.0, 0.9, 0.9, 0.9, 0.0, 1.2, 0.0]$。\n\n最终输出格式要求：\n- 你的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的结果列表。此顶层列表中的每个元素对应一个测试用例，并且其本身必须是一个包含 $T+1$ 个条目的带括号列表，条目形式为 $[t,v_t,\\text{pre\\_threshold},s_t,v_{t+1}]$，并遵循该顺序。例如，外层列表必须看起来像 $[ \\text{case1}, \\text{case2}, \\dots ]$，不含任何附加文本。此问题不使用角度，也不要求物理单位。所有报告的值必须是指定类型的数值。",
            "solution": "该问题已经过验证，被认为是可靠、适定且有科学依据的。它提出了计算神经科学中的一个标准任务：在离散时间内模拟 Leaky Integrate-and-Fire (LIF) 神经元模型，这是像随时间反向传播 (BPTT) 这类算法的一个基本步骤。\n\n### 离散时间递推公式的推导\n\n膜电位 $v(t)$ 的动力学由以下连续时间常微分方程 (ODE) 给出：\n$$ \\frac{dv}{dt} = -\\frac{1}{\\tau} v(t) + k I(t) $$\n这是一个一阶线性 ODE。为了在持续时间为 $\\Delta t$ 的时间步长上对其进行离散化，我们使用指数欧拉方法。该方法在输入电流 $I(t)$ 在该区间上为常数的假设下，提供了 ODE 在区间 $[t, t+\\Delta t]$ 上的精确解。\n\n齐次方程 $\\frac{dv}{dt} = -\\frac{1}{\\tau} v(t)$ 的解是 $v(t) = v(t_0) e^{-(t-t_0)/\\tau}$。在一个时间步长 $\\Delta t$ 内，电位以 $e^{-\\Delta t/\\tau}$ 的因子衰减。问题将此离散时间泄漏因子定义为 $\\alpha$：\n$$ \\alpha = e^{-\\Delta t/\\tau} $$\n对于完整的非齐次方程，假设在此区间内 $I(t)$ 是一个常数 $I_t$，则从 $t$ 到 $t+\\Delta t$ 的单个时间步长内的精确解为：\n$$ v(t+\\Delta t) = v(t) e^{-\\Delta t/\\tau} + k\\tau(1 - e^{-\\Delta t/\\tau}) I_t $$\n代入 $\\alpha$ 得：\n$$ v(t+\\Delta t) = \\alpha v(t) + k\\tau(1 - \\alpha) I_t $$\n问题陈述，离散时间输入驱动 $I_t$ 是“已经被缩放，以使其贡献能够线性地进入”。这意味着项 $k\\tau(1 - \\alpha)$ 被吸收到所提供的输入序列 $I_t$ 中。因此，在考虑脉冲和重置之前，膜电位的离散时间更新规则就是泄漏后的先前电位与缩放后输入的和。\n\n令 $v_t$ 为时间步 $t$ 开始时的膜电位。遵循指定的操作顺序（整合、阈值判断、重置），我们首先计算阈值前电位，我们将其表示为 $u_t$：\n$$ u_t = \\alpha v_t + I_t $$\n此步骤代表泄漏和输入电流的整合。\n\n接下来，我们应用阈值判断规则。如果阈值前电位 $u_t$ 达到或超过阈值 $\\theta$，则发出一个脉冲 $s_t$。使用 Heaviside 阶跃函数 $H(x)$（定义为当 $x \\ge 0$ 时为 $1$，否则为 $0$），脉冲指示符 $s_t \\in \\{0, 1\\}$ 为：\n$$ s_t = H(u_t - \\theta) $$\n这个公式正确地实现了当 $u_t = \\theta$ 时生成脉冲的临界情况处理规则。\n\n最后，我们应用硬重置机制来确定下一个时间步开始时的膜电位 $v_{t+1}$。如果生成了脉冲（$s_t = 1$），电位将重置为 $V_{\\text{reset}}$。如果没有发生脉冲（$s_t = 0$），电位将从其阈值前的值 $u_t$ 延续。这可以表示为单个方程：\n$$ v_{t+1} = (1 - s_t) u_t + s_t V_{\\text{reset}} $$\n\n### 模拟算法\n\n模拟从初始状态 $v_0$ 开始，对时间步 $t = 0, 1, \\dots, T$ 进行迭代。对于每个时间步 $t$：\n\n1.  **检索状态和输入**：该步骤开始时的状态是 $v_t$。此步骤的输入是 $I_t$。\n2.  **整合**：计算阈值前电位 $u_t = \\alpha v_t + I_t$。\n3.  **阈值判断**：如果 $u_t \\ge \\theta$，则确定脉冲输出 $s_t = 1$，否则 $s_t = 0$。\n4.  **重置**：计算下一个时间步的重置后电位，$v_{t+1} = (1 - s_t) u_t + s_t V_{\\text{reset}}$。\n5.  **为 BPTT 记录状态**：存储元组 $[t, v_t, u_t, s_t, v_{t+1}]$。\n6.  **推进时间**：下一步的电位 $v_{t+1}$ 成为 $t+1$ 迭代的起始电位。\n\n此过程生成 BPTT 所需的完整展开状态序列。实现将对每个提供的测试用例执行此算法。",
            "answer": "```python\n# 完整且可运行的 Python 3 代码位于此处。\n# 导入必须遵守指定的执行环境。\nimport numpy as np\n# 此问题不需要其他库。\n\ndef solve():\n    \"\"\"\n    运行所有测试用例的LIF神经元模拟并打印结果的主函数。\n    \"\"\"\n    test_cases = [\n        # 1. 典型脉冲发放情况\n        {\n            \"alpha\": 0.9, \"theta\": 1.0, \"v_reset\": 0.0, \"v0\": 0.0, \"T\": 5,\n            \"I\": [0.5, 0.6, 0.0, 0.0, 0.0, 0.0]\n        },\n        # 2. 边界阈值相等情况\n        {\n            \"alpha\": 0.5, \"theta\": 1.0, \"v_reset\": 0.0, \"v0\": 2.0, \"T\": 2,\n            \"I\": [0.0, 0.0, 0.0]\n        },\n        # 3. 泄漏下的无脉冲累积\n        {\n            \"alpha\": 0.95, \"theta\": 1.2, \"v_reset\": 0.0, \"v0\": 0.1, \"T\": 4,\n            \"I\": [0.1, 0.1, 0.1, 0.1, 0.1]\n        },\n        # 4. 带有非零重置的多次脉冲发放\n        {\n            \"alpha\": 0.8, \"theta\": 1.0, \"v_reset\": 0.2, \"v0\": 0.0, \"T\": 6,\n            \"I\": [1.0, 0.9, 0.9, 0.9, 0.0, 1.2, 0.0]\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        alpha = case_params[\"alpha\"]\n        theta = case_params[\"theta\"]\n        v_reset = case_params[\"v_reset\"]\n        v0 = case_params[\"v0\"]\n        I_seq = case_params[\"I\"]\n        T = case_params[\"T\"]\n        \n        case_result = simulate_lif(alpha, theta, v_reset, v0, I_seq, T)\n        results.append(case_result)\n\n    # 最终的打印语句采用所要求的精确格式。\n    # 列表的默认 str() 表示形式符合所要求的输出格式。\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_lif(alpha, theta, v_reset, v0, I_seq, T):\n    \"\"\"\n    模拟一个离散时间的渗漏整合发放（LIF）神经元。\n\n    Args:\n        alpha (float): 离散时间泄漏因子。\n        theta (float): 发放阈值。\n        v_reset (float): 脉冲后的重置电位。\n        v0 (float): 初始膜电位。\n        I_seq (list of float): 长度为 T+1 的输入电流序列。\n        T (int): 模拟的最终时间索引（总步数为 T+1）。\n\n    Returns:\n        list of lists: BPTT 所需的展开状态历史记录。每个内部列表的形式为\n                       [t, v_t, pre_threshold_potential, s_t, v_{t+1}]。\n    \"\"\"\n    recorded_states = []\n    v_current = float(v0)\n    \n    # 模拟从 t=0 到 T（含）运行。\n    for t in range(T + 1):\n        # 当前时间步的输入\n        I_t = float(I_seq[t])\n        \n        # v_t 是该步骤开始时的电位。\n        v_t = v_current\n        \n        # 1. 整合：泄漏和输入得到阈值前电位。\n        # 在推导中表示为 u_t。\n        pre_threshold_potential = alpha * v_t + I_t\n        \n        # 2. 阈值判断：检查是否发放脉冲。s_t 是脉冲指示符。\n        # 条件包含等号，实现 H(x) 其中 H(0)=1。\n        s_t = 1 if pre_threshold_potential = theta else 0\n        \n        # 3. 重置：确定下一个状态 v_{t+1}。\n        if s_t == 1:\n            v_next = v_reset\n        else:\n            v_next = pre_threshold_potential\n        \n        # 记录此步骤的 BPTT 状态。\n        # 类型为：int, float, float, int, float。\n        recorded_states.append([t, v_t, pre_threshold_potential, s_t, float(v_next)])\n        \n        # 为下一次迭代更新当前电位。\n        v_current = float(v_next)\n        \n    return recorded_states\n\nsolve()\n```"
        },
        {
            "introduction": "利用梯度下降法训练脉冲神经网络（SNNs）的核心挑战在于脉冲发放的不可微特性。本练习将深入探讨此问题的解决方案：代理梯度（surrogate gradient）。通过亲手实现并比较两种常见的代理梯度方法，您将直观地理解它们如何为不可微的脉冲函数生成一个有效的、可用于学习的梯度信号。",
            "id": "4036243",
            "problem": "考虑一个单神经元脉冲神经网络（SNN），其包含一个离散时间的漏积分-发放（LIF）神经元。膜电位 $v_t$ 根据以下差分方程演化\n$$v_{t+1} = \\beta v_t + w x_t - s_t \\theta,$$\n其中 $t \\in \\{0,1,\\dots,T-1\\}$，$v_0 = 0$，$w$ 是一个标量突触权重，$x_t$ 是一个已知的输入序列，$\\beta \\in (0,1)$ 是泄漏因子，$\\theta  0$ 是脉冲阈值，而 $s_t \\in \\{0,1\\}$ 是在时间 $t$ 产生的脉冲，由赫维赛德阶跃函数 $s_t = H(v_t - \\theta)$ 给出。损失函数定义为\n$$L = \\frac{1}{2}\\sum_{t=0}^{T-1} \\left(s_t - y_t\\right)^2,$$\n其中 $y_t \\in \\{0,1\\}$ 是一个预设的目标脉冲序列。任务是使用两种不同的代理导数（surrogate derivative）来计算 $\\partial s_t / \\partial v_t$，并通过时间反向传播（BPTT）计算梯度 $\\partial L / \\partial w$：\n1. 直通估计器（Straight-Through Estimator, STE）：$$\\frac{\\partial s_t}{\\partial v_t} \\approx \\mathbf{1}_{|v_t - \\theta|  \\delta},$$ 其中 $\\delta  0$ 定义了指示器窗口。\n2. 平滑函数导数：定义 $h_k(v) = \\sigma(k(v - \\theta))$，其中 $\\sigma(u) = \\frac{1}{1+e^{-u}}$，并使用 $$\\frac{d h_k(v)}{dv} = k \\sigma(k(v - \\theta))\\left(1 - \\sigma(k(v - \\theta))\\right)$$ 作为 $\\frac{\\partial s_t}{\\partial v_t}$ 的代理，其中陡峭度参数 $k  0$。\n\n仅从给定的神经元动力学、赫维赛德定义和损失函数出发，推导出 $\\partial v_t / \\partial w$ 的 BPTT 递归关系式，以及用所选代理导数表示的 $\\partial L / \\partial w$ 表达式。实现这两种梯度计算，并通过为每个测试案例报告一个标量来分析其引起的梯度不匹配：\n$$r = \\frac{\\left|\\left(\\partial L / \\partial w\\right)_{\\text{STE}} - \\left(\\partial L / \\partial w\\right)_{k}\\right|}{\\left|\\left(\\partial L / \\partial w\\right)_{k}\\right| + \\varepsilon},$$\n其中 $\\varepsilon = 10^{-12}$ 是一个数值稳定器。\n\n在所有测试案例中，使用以下固定的 SNN 参数：$T = 12$，$\\beta = 0.9$，$\\theta = 1.0$，$v_0 = 0$，以及一个固定的目标脉冲序列 $y_t$，在时间 $t = 4$ 和 $t = 9$（从零开始索引）处有单位脉冲，其他时间 $y_t = 0$。\n\n对于输入序列，使用基于类型标签和振幅 $A$ 的确定性、时间索引的定义：\n- 类型 \"pulses2\"：$x_t = A$ 当 $t \\in \\{3,7\\}$，否则 $x_t = 0$。\n- 类型 \"pulses4\"：$x_t = A$ 当 $t \\in \\{2,4,6,8\\}$，否则 $x_t = 0$。\n- 类型 \"single\"：$x_t = A$ 当 $t = 4$，否则 $x_t = 0$。\n- 类型 \"zero\"：对所有 $t$，$x_t = 0$。\n- 类型 \"constant\"：对所有 $t$，$x_t = A$。\n\n你的程序必须为以下每个测试案例计算 $r$，每个案例指定为 $(\\delta, k, w, \\text{type}, A)$：\n- 案例 1：$(0.05, 100.0, 0.8, \\text{\"pulses2\"}, 1.2)$。\n- 案例 2：$(0.2, 100.0, 0.8, \\text{\"pulses2\"}, 1.2)$。\n- 案例 3：$(0.05, 20.0, 1.2, \\text{\"pulses4\"}, 1.5)$。\n- 案例 4：$(0.5, 10.0, 0.3, \\text{\"single\"}, 2.0)$。\n- 案例 5：$(0.01, 200.0, 0.8, \\text{\"zero\"}, 0.0)$。\n- 案例 6：$(0.2, 50.0, 1.0, \\text{\"constant\"}, 0.5)$。\n\n你的程序应该生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,\\dots,r_6]$）。每个 $r_i$ 都必须是浮点数。不应打印任何其他文本。",
            "solution": "此问题已经过验证，被认为是可靠、适定且具有科学依据的。它提出了神经形态计算领域的一个标准任务：使用基于梯度的方法训练脉冲神经网络（SNN），这需要解决脉冲生成机制的不可微性问题。所有参数、模型和目标都已明确定义。\n\n问题的核心是计算损失函数 $L$ 相对于突触权重 $w$ 的梯度，记为 $\\frac{\\partial L}{\\partial w}$。损失函数定义在一个脉冲序列上：\n$$L = \\frac{1}{2}\\sum_{t=0}^{T-1} (s_t - y_t)^2$$\n其中 $s_t$ 是神经元在时间 $t$ 的输出脉冲，$y_t$ 是目标脉冲。\n\n使用链式法则，梯度可以表示为：\n$$\\frac{\\partial L}{\\partial w} = \\sum_{t=0}^{T-1} \\frac{\\partial L}{\\partial s_t} \\frac{d s_t}{d w}$$\n第一项可以从损失函数的定义直接计算得出：\n$$\\frac{\\partial L}{\\partial s_t} = s_t - y_t$$\n第二项 $\\frac{d s_t}{d w}$ 需要进一步展开。输出脉冲 $s_t$ 是膜电位 $v_t$ 的函数，而膜电位 $v_t$ 又是权重 $w$ 的函数。再次应用链式法则：\n$$\\frac{d s_t}{d w} = \\frac{d s_t}{d v_t} \\frac{d v_t}{d w}$$\n项 $\\frac{d s_t}{d v_t}$ 表示脉冲生成函数 $s_t = H(v_t - \\theta)$ 的导数，其中 $H$ 是赫维赛德阶跃函数。其数学导数几乎处处为零，并且在阈值处是一个未定义的狄拉克δ函数，这不适用于基于梯度的学习。为了解决这个问题，我们用一个连续的代理函数来替代它，我们将其表示为 $\\psi(v_t)$：\n$$\\frac{d s_t}{d v_t} \\approx \\psi(v_t)$$\n该问题要求为 $\\psi(v_t)$ 使用两种不同的代理：\n1.  **直通估计器 (STE)**：$\\psi(v_t) = \\mathbf{1}_{|v_t - \\theta|  \\delta}$，其中 $\\mathbf{1}$ 是指示函数。这将导数近似为在阈值 $\\theta$ 周围一个宽度为 $2\\delta$ 的小窗口内的常数值 $1$，在窗口外则为 $0$。\n2.  **平滑 Sigmoid 导数**：$\\psi(v_t) = \\frac{d h_k(v_t)}{d v_t} = k \\sigma(k(v_t - \\theta))(1 - \\sigma(k(v_t - \\theta)))$，其中 $\\sigma(u) = (1+e^{-u})^{-1}$ 是 logistic sigmoid 函数。这使用了阶跃函数的一个平滑近似的导数。\n\n剩下需要确定的项是 $\\frac{d v_t}{d w}$，它是时间 $t$ 的膜电位相对于权重 $w$ 的全导数。我们可以通过对神经元的动力学方程求导来推导出该项的递归关系：\n$$v_{t+1} = \\beta v_t + w x_t - s_t \\theta$$\n让我们定义 $g_t := \\frac{d v_t}{d w}$。对动力学方程关于 $w$ 求导得出：\n$$\\frac{d v_{t+1}}{d w} = \\frac{d}{d w} (\\beta v_t + w x_t - s_t \\theta)$$\n$$g_{t+1} = \\beta \\frac{d v_t}{d w} + x_t - \\theta \\frac{d s_t}{d w}$$\n代入 $\\frac{d s_t}{d w} = \\psi(v_t) \\frac{d v_t}{d w} = \\psi(v_t) g_t$：\n$$g_{t+1} = \\beta g_t + x_t - \\theta \\psi(v_t) g_t$$\n这就给出了 $g_t$ 的前向递归关系：\n$$g_{t+1} = (\\beta - \\theta \\psi(v_t)) g_t + x_t$$\n初始条件基于 $v_0 = 0$，它是一个常数，不依赖于 $w$。因此，该递归的初始条件是 $g_0 = \\frac{d v_0}{d w} = 0$。\n\n将这些部分组合起来，总梯度的最终表达式为：\n$$\\frac{\\partial L}{\\partial w} \\approx \\sum_{t=0}^{T-1} (s_t - y_t) \\psi(v_t) g_t$$\n\n计算梯度的整体算法如下：\n\n1.  **前向传播**：模拟神经元动力学以获得膜电位 $v_t$ 和脉冲 $s_t$ 在 $t \\in \\{0, 1, \\dots, T-1\\}$ 期间的历史记录。\n    - 初始化 $v_0 = 0$。\n    - 对于 $t = 0, \\dots, T-1$：\n        - 如果 $v_t \\ge \\theta$，则 $s_t = 1$，否则 $s_t = 0$。\n        - $v_{t+1} = \\beta v_t + w x_t - s_t \\theta$。\n    - 存储序列 $v_0, \\dots, v_{T-1}$ 和 $s_0, \\dots, s_{T-1}$。\n\n2.  **梯度计算**：计算每个时间步的梯度贡献并进行累加。这涉及到第二次前向传播来计算序列 $g_t$。\n    - 初始化总梯度 $\\frac{\\partial L}{\\partial w} = 0$。\n    - 初始化电位的梯度 $g_0 = 0$。\n    - 对于 $t = 0, \\dots, T-1$：\n        - 根据所选方法（STE 或 Smoothed）计算代理导数 $\\psi(v_t)$。\n        - 更新总梯度：$\\frac{\\partial L}{\\partial w} \\leftarrow \\frac{\\partial L}{\\partial w} + (s_t - y_t) \\psi(v_t) g_t$。\n        - 更新用于下一步的电位梯度：$g_{t+1} = (\\beta - \\theta \\psi(v_t)) g_t + x_t$。\n\n对每种代理梯度方法执行一次此过程，以获得 $(\\partial L / \\partial w)_{\\text{STE}}$ 和 $(\\partial L / \\partial w)_{k}$。然后按规定计算相对差异度量 $r$。",
            "answer": "```python\n# 完整且可运行的 Python 3 代码位于此处。\n# 导入必须遵守指定的执行环境。\nimport numpy as np\nfrom scipy.special import expit\n\ndef generate_input_sequence(seq_type, A, T):\n    \"\"\"生成输入序列 x_t。\"\"\"\n    x = np.zeros(T)\n    if seq_type == \"pulses2\":\n        if 3  T: x[3] = A\n        if 7  T: x[7] = A\n    elif seq_type == \"pulses4\":\n        indices = [2, 4, 6, 8]\n        for i in indices:\n            if i  T:\n                x[i] = A\n    elif seq_type == \"single\":\n        if 4  T: x[4] = A\n    elif seq_type == \"zero\":\n        pass  # x 已经是全零\n    elif seq_type == \"constant\":\n        x[:] = A\n    else:\n        raise ValueError(\"未知的序列类型\")\n    return x\n\ndef calculate_gradient(w, seq_type, A, surrogate_choice, surrogate_param):\n    \"\"\"\n    计算单个神经元 SNN 的梯度 dL/dw。\n    \n    Args:\n        w (float): 突触权重。\n        seq_type (str): 输入序列的类型。\n        A (float): 输入序列的振幅。\n        surrogate_choice (str): 'STE' 或 'Smoothed'。\n        surrogate_param (float): STE 的 delta 或 Smoothed 的 k。\n\n    Returns:\n        float: 计算出的梯度 dL/dw。\n    \"\"\"\n    # 固定的 SNN 参数\n    T = 12\n    beta = 0.9\n    theta = 1.0\n    v0 = 0.0\n\n    # 目标脉冲序列\n    y = np.zeros(T)\n    y[4] = 1.0\n    y[9] = 1.0\n\n    # 输入序列\n    x = generate_input_sequence(seq_type, A, T)\n\n    # --- 1. 前向传播：模拟神经元动力学 ---\n    v_hist = np.zeros(T)\n    s_hist = np.zeros(T)\n    v = v0\n\n    for t in range(T):\n        v_hist[t] = v\n        s = 1.0 if v = theta else 0.0\n        s_hist[t] = s\n        v = beta * v + w * x[t] - s * theta\n\n    # --- 2. 梯度计算（前向模式）---\n    grad_L_w = 0.0\n    g = 0.0  # g_0 = d(v_0)/dw = 0\n\n    for t in range(T):\n        v_t = v_hist[t]\n        \n        # 计算代理导数 psi(v_t)\n        if surrogate_choice == 'STE':\n            delta = surrogate_param\n            psi = 1.0 if np.abs(v_t - theta)  delta else 0.0\n        elif surrogate_choice == 'Smoothed':\n            k = surrogate_param\n            u = k * (v_t - theta)\n            sigma_u = expit(u)\n            psi = k * sigma_u * (1.0 - sigma_u)\n        else:\n            raise ValueError(\"无效的代理选择\")\n            \n        # 累积梯度\n        grad_L_w += (s_hist[t] - y[t]) * psi * g\n        \n        # 更新用于下一个时间步的 g\n        g = (beta - theta * psi) * g + x[t]\n        \n    return grad_L_w\n    \n\ndef solve():\n    \"\"\"\n    运行所有测试用例并计算梯度不匹配率 r 的主函数。\n    \"\"\"\n    test_cases = [\n        # (delta, k, w, type, A)\n        (0.05, 100.0, 0.8, \"pulses2\", 1.2),\n        (0.2, 100.0, 0.8, \"pulses2\", 1.2),\n        (0.05, 20.0, 1.2, \"pulses4\", 1.5),\n        (0.5, 10.0, 0.3, \"single\", 2.0),\n        (0.01, 200.0, 0.8, \"zero\", 0.0),\n        (0.2, 50.0, 1.0, \"constant\", 0.5),\n    ]\n\n    results = []\n    epsilon = 1e-12\n\n    for case in test_cases:\n        delta, k, w, seq_type, A = case\n        \n        grad_ste = calculate_gradient(w, seq_type, A, 'STE', delta)\n        grad_k = calculate_gradient(w, seq_type, A, 'Smoothed', k)\n        \n        r = np.abs(grad_ste - grad_k) / (np.abs(grad_k) + epsilon)\n        results.append(r)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "确保 BPTT 算法的正确性对于模型训练至关重要，而手动实现的复杂性使其容易出错。本高级练习将介绍一种严谨的验证方法，即通过从零开始构建一个通用自动微分（AD）引擎，来检验手动推导的 BPTT 算法计算出的伴随变量序列。这个过程不仅能验证代码的正确性，更能加深您对 BPTT 本质的理解——它是在时间展开图上应用链式法则（即反向模式自动微分）的一个特例。",
            "id": "4036240",
            "problem": "设计并实现一个完整的、可运行的程序，该程序构建一个单神经元离散时间脉冲神经网络 (SNN)，该网络具有用于脉冲生成机制的可微代理函数，并使用随时间反向传播 (BPTT) 验证伴随变量的反向递推。验证必须通过将从第一性原理计算出的伴随序列与通用的反向模式自动微分 (AD) 引擎在相同可微代理模型上产生的梯度进行比较来执行。\n\n基本核心定义：\n- 考虑一个单神经元，其膜电位序列 $\\{v_t\\}_{t=0}^{T}$ 由一个离散时间的泄露积分并重置递推关系控制。该神经元由外部输入序列 $\\{x_t\\}_{t=0}^{T-1}$ 驱动。\n- 脉冲生成由一个替代亥维赛德阶跃函数的可微代理函数建模。设 $\\sigma_\\beta(u) = \\frac{1}{1 + e^{-\\beta u}}$ 为逻辑斯谛函数，其斜率参数为 $\\beta$，该函数可微，其导数为 $\\sigma'_\\beta(u) = \\beta \\sigma_\\beta(u)\\left(1 - \\sigma_\\beta(u)\\right)$。\n- 离散时间膜动态定义为\n$$\nv_{t+1} = \\alpha \\, v_t + w \\, x_t - r \\, \\sigma_\\beta\\!\\left(v_t - \\theta\\right),\n$$\n其中 $t \\in \\{0,1,\\dots,T-1\\}$，$\\alpha \\in (0,1)$ 是泄露因子，$w \\in \\mathbb{R}$ 是突触权重，$r \\in \\mathbb{R}_{\\ge 0}$ 是重置幅度，$\\theta \\in \\mathbb{R}$ 是阈值。\n- 标量损失定义为\n$$\nL = \\sum_{t=0}^{T-1} \\frac{\\gamma}{2} \\left(\\sigma_\\beta\\!\\left(v_t - \\theta\\right) - y_t\\right)^2 \\;+\\; \\frac{\\rho}{2} \\, v_T^2,\n$$\n其中 $\\gamma \\in \\mathbb{R}_{0}$ 和 $\\rho \\in \\mathbb{R}_{\\ge 0}$，以及一个参考序列 $\\{y_t\\}_{t=0}^{T-1}$。\n\n任务要求：\n1. 实现一个通用的反向模式自动微分引擎，该引擎作用于由基本运算（加法、减法、与标量相乘、平方和逻辑斯谛函数 $\\sigma_\\beta$）组成的计算图。使用它来计算上述可微代理模型的梯度 $\\left\\{\\frac{\\partial L}{\\partial v_t}\\right\\}_{t=0}^{T}$。\n2. 严格从链式法则和所提供的递推关系出发，独立推导、实现并计算伴随序列 $\\{\\delta^v_t\\}_{t=0}^{T}$。伴随变量 $\\delta^v_t$ 定义为 $\\delta^v_t = \\frac{\\partial L}{\\partial v_t}$。你绝不能使用任何预封装的梯度函数或依赖任何外部自动微分工具；BPTT 递推必须从第一性原理获得。\n3. 设计一个单元测试，针对每个提供的测试用例，验证通过 BPTT 计算的伴随序列与 AD 引擎产生的梯度序列在指定的数值容差内匹配。比较指标必须是所有时间索引 $t \\in \\{0,\\dots,T\\}$ 上的最大绝对差。\n4. 对于所有测试用例，你的程序必须输出单行内容，其中包含一个方括号括起来的逗号分隔列表，每个条目是一个布尔值，指示该测试用例是否通过 (true) 或失败 (false)。\n\n测试套件规范：\n使用以下四个测试用例。对于每个用例，膜电位的初始条件为 $v_0 = 0$。\n\n- 用例 1 (一般成功路径):\n  - $T = 8$\n  - $\\alpha = 0.9$\n  - $w = 0.5$\n  - $\\theta = 0.2$\n  - $r = 0.2$\n  - $\\beta = 8.0$\n  - $\\gamma = 1.3$\n  - $\\rho = 0.7$\n  - $x = [0.1, 0.0, 0.4, 0.0, 0.3, 0.5, 0.2, 0.0]$\n  - $y = [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]$\n\n- 用例 2 (因高阈值而有效无脉冲的边界情况):\n  - $T = 6$\n  - $\\alpha = 0.8$\n  - $w = 0.3$\n  - $\\theta = 5.0$\n  - $r = 0.5$\n  - $\\beta = 8.0$\n  - $\\gamma = 1.0$\n  - $\\rho = 0.0$\n  - $x = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1]$\n  - $y = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$\n\n- 用例 3 (零泄露的边缘情况):\n  - $T = 5$\n  - $\\alpha = 0.0$\n  - $w = 1.0$\n  - $\\theta = 0.1$\n  - $r = 0.1$\n  - $\\beta = 5.0$\n  - $\\gamma = 0.7$\n  - $\\rho = 0.2$\n  - $x = [0.5, 0.5, 0.5, 0.5, 0.5]$\n  - $y = [1.0, 1.0, 0.0, 1.0, 0.0]$\n\n- 用例 4 (带负权重的抑制性输入):\n  - $T = 7$\n  - $\\alpha = 0.95$\n  - $w = -0.7$\n  - $\\theta = 0.25$\n  - $r = 0.25$\n  - $\\beta = 6.0$\n  - $\\gamma = 0.9$\n  - $\\rho = 0.4$\n  - $x = [0.2, 0.2, 0.0, 0.3, 0.1, 0.0, 0.2]$\n  - $y = [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]$\n\n数值容差：\n- 设通过 BPTT 得到的伴随序列与通过 AD 得到的梯度序列之间的最大绝对差表示为 $\\Delta = \\max_{t \\in \\{0,\\dots,T\\}} \\left| \\delta^v_t - \\frac{\\partial L}{\\partial v_t} \\right|$。\n- 如果 $\\Delta \\le 10^{-6}$，则测试用例通过；否则，失败。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表（例如，$[\\text{true},\\text{false},\\text{true},\\text{true}]$），每个条目是对应测试用例的布尔值，顺序与上述用例指定的一致。不应打印任何额外的文本或行。",
            "solution": "该问题要求设计并实现一个验证框架，用于验证应用于单神经元脉冲神经网络 (SNN) 的随时间反向传播 (BPTT) 算法。验证方法是将基于第一性原理推导的 BPTT 计算出的伴随序列与一个通用的、从零开始实现的反向模式自动微分 (AD) 引擎计算出的梯度进行比较。\n\n### 1. 数学模型构建\n\n所考虑的系统是一个离散时间的单神经元模型，由一组递推关系控制。\n\n神经元的膜电位，用序列 $\\{v_t\\}_{t=0}^{T}$ 表示，根据泄露积分并重置动态演化：\n$$\nv_{t+1} = \\alpha \\, v_t + w \\, x_t - r \\, \\sigma_\\beta(v_t - \\theta), \\quad \\text{for } t \\in \\{0, 1, \\dots, T-1\\}\n$$\n其中：\n- $v_t \\in \\mathbb{R}$：在时间 $t$ 的膜电位。\n- $x_t \\in \\mathbb{R}$：在时间 $t$ 的外部输入。\n- $\\alpha \\in (0,1)$：膜电位泄露因子。\n- $w \\in \\mathbb{R}$：突触权重。\n- $r \\in \\mathbb{R}_{\\ge 0}$：脉冲发放后的重置幅度。\n- $\\theta \\in \\mathbb{R}$：膜电位阈值。\n- $v_0$：初始膜电位，指定为 $v_0 = 0$。\n\n理想脉冲的不可微特性通过使用一个可微代理函数——逻辑斯谛函数 $\\sigma_\\beta(u)$ 来解决：\n$$\ns_t = \\sigma_\\beta(v_t - \\theta) = \\frac{1}{1 + e^{-\\beta (v_t - \\theta)}}\n$$\n其中 $s_t$ 表示神经元在时间 $t$ 的“发放率”或脉冲概率，$\\beta$ 是一个斜率参数。逻辑斯谛函数的导数对于基于梯度的优化方法至关重要，其表达式为：\n$$\n\\sigma'_\\beta(u) = \\frac{d\\sigma_\\beta(u)}{du} = \\beta \\, \\sigma_\\beta(u) \\left(1 - \\sigma_\\beta(u)\\right)\n$$\n\n神经元的性能由一个标量损失函数 $L$ 来量化，该函数是随时间变化的脉冲匹配目标与对最终膜电位的正则化项之和：\n$$\nL = \\sum_{t=0}^{T-1} \\frac{\\gamma}{2} \\left(\\sigma_\\beta(v_t - \\theta) - y_t\\right)^2 + \\frac{\\rho}{2} \\, v_T^2\n$$\n其中：\n- $\\{y_t\\}_{t=0}^{T-1}$：目标脉冲序列。\n- $\\gamma \\in \\mathbb{R}_{0}$：脉冲匹配误差的权重。\n- $\\rho \\in \\mathbb{R}_{\\ge 0}$：最终电位正则化项的权重。\n\n我们的主要目标是计算梯度序列 $\\left\\{\\frac{\\partial L}{\\partial v_t}\\right\\}_{t=0}^{T}$。\n\n### 2. BPTT 伴随方程的推导\n\nBPTT 是链式法则在时间计算图上的应用。我们将伴随变量 $\\delta^v_t$ 定义为损失 $L$ 相对于状态变量 $v_t$ 的全导数：\n$$\n\\delta^v_t = \\frac{dL}{dv_t}\n$$\n$L$ 的值既通过时间 $t$ 的损失项直接依赖于 $v_t$，也通过 $v_t$ 对所有后续状态 $v_{t+1}, \\dots, v_T$ 的影响而间接依赖于 $v_t$。链式法则为伴随变量提供了一个递归关系：\n$$\n\\delta^v_t = \\frac{\\partial L}{\\partial v_t} + \\frac{dL}{dv_{t+1}}\\frac{\\partial v_{t+1}}{\\partial v_t} = \\frac{\\partial L}{\\partial v_t} + \\delta^v_{t+1} \\frac{\\partial v_{t+1}}{\\partial v_t}\n$$\n这个递推关系在时间上反向操作，从 $t=T$ 向下到 $t=0$。\n\n为了构建 BPTT 算法，我们必须推导递推关系中的两个偏导数的表达式。\n\n**终端条件 (在 $t=T$ 时)**：\n递推在最终时间步 $T$ 进行初始化。损失 $L$ 中唯一显式依赖于 $v_T$ 的项是 $\\frac{\\rho}{2}v_T^2$。\n$$\n\\delta^v_T = \\frac{dL}{dv_T} = \\frac{\\partial}{\\partial v_T} \\left( \\frac{\\rho}{2} v_T^2 \\right) = \\rho \\, v_T\n$$\n\n**局部梯度项 ($\\frac{\\partial L}{\\partial v_t}$，对于 $t  T$)**：\n对于任何时间步 $t \\in \\{0, \\dots, T-1\\}$，$L$ 对 $v_t$ 的显式依赖来自于脉冲匹配项 $\\frac{\\gamma}{2} (\\sigma_\\beta(v_t - \\theta) - y_t)^2$。\n$$\n\\frac{\\partial L}{\\partial v_t} = \\frac{\\partial}{\\partial v_t} \\left[ \\frac{\\gamma}{2} \\left(\\sigma_\\beta(v_t - \\theta) - y_t\\right)^2 \\right]\n$$\n应用链式法则：\n$$\n\\frac{\\partial L}{\\partial v_t} = \\gamma \\left(\\sigma_\\beta(v_t - \\theta) - y_t\\right) \\cdot \\frac{\\partial}{\\partial v_t} \\left(\\sigma_\\beta(v_t - \\theta)\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial v_t} = \\gamma \\left(s_t - y_t\\right) \\cdot \\sigma'_\\beta(v_t - \\theta) \\cdot \\frac{\\partial}{\\partial v_t}(v_t - \\theta)\n$$\n$$\n\\frac{\\partial L}{\\partial v_t} = \\gamma \\left(s_t - y_t\\right) \\sigma'_\\beta(v_t - \\theta)\n$$\n为方便表示，令 $s'_t = \\sigma'_\\beta(v_t - \\theta)$。则 $\\frac{\\partial L}{\\partial v_t} = \\gamma (s_t - y_t) s'_t$。\n\n**状态转移雅可比 ($\\frac{\\partial v_{t+1}}{\\partial v_t}$)**：\n该项捕捉了 $v_t$ 的变化如何传播到 $v_{t+1}$。它从膜动态方程中推导得出：\n$$\n\\frac{\\partial v_{t+1}}{\\partial v_t} = \\frac{\\partial}{\\partial v_t} \\left( \\alpha v_t + w x_t - r \\sigma_\\beta(v_t - \\theta) \\right)\n$$\n$$\n\\frac{\\partial v_{t+1}}{\\partial v_t} = \\alpha - r \\cdot \\frac{\\partial}{\\partial v_t} \\left(\\sigma_\\beta(v_t - \\theta)\\right)\n$$\n$$\n\\frac{\\partial v_{t+1}}{\\partial v_t} = \\alpha - r \\, \\sigma'_\\beta(v_t - \\theta) = \\alpha - r s'_t\n$$\n\n**BPTT 递推总结**：\n结合这些部分，我们得到完整的 BPTT 算法：\n1.  **前向传播**：通过迭代动态方程，计算并存储从 $t \\in \\{0, \\dots, T\\}$ 的状态和脉冲序列。\n    - $v_0 = 0$\n    - 对于 $t = 0, \\dots, T-1$:\n        - $s_t = \\sigma_\\beta(v_t - \\theta)$\n        - $s'_t = \\beta s_t (1 - s_t)$\n        - $v_{t+1} = \\alpha v_t + w x_t - r s_t$\n2.  **反向传播**：通过时间反向迭代，计算伴随序列 $\\{\\delta^v_t\\}_{t=0}^{T}$。\n    - 初始化: $\\delta^v_T = \\rho v_T$\n    - 对于 $t = T-1, \\dots, 0$:\n        - $\\delta^v_t = \\gamma (s_t - y_t) s'_t + \\delta^v_{t+1} (\\alpha - r s'_t)$\n\n### 3. 用于验证的自动微分框架\n\n为了验证 BPTT 推导和实现的正确性，我们构建一个独立的方法来计算相同的梯度。一个通用的反向模式自动微分 (AD) 引擎可以用于此目的。反向模式 AD 的原理是首先在前向传播过程中构建一个计算图，记录所有基本运算及其输入。然后，通过反向遍历此图，使用链式法则将导数从输出传播回输入。\n\n我们的实现包括：\n- **`ADVar` 类**: 一个用于表示计算图中节点的数据结构。每个 `ADVar` 对象存储其标量 `value` 和其 `grad`（最终损失相对于其值的导数），`grad` 初始化为 $0$。\n- **计算带 (Computational Tape)**：一个充当“磁带”的全局列表。在前向传播过程中，每执行一个操作，一个相应的反向函数（一个闭包）就会被附加到这个磁带上。这个函数封装了如何将梯度传播回操作输入的信息。\n- **基本运算**: 一组在 `ADVar` 对象上操作的函数（`ad_add`, `ad_sub`, `ad_mul`, `ad_square`, `ad_sigma`）。每个函数计算输出值，将其包装在一个新的 `ADVar` 中，并将相应的梯度传播规则添加到磁带上。例如，对于加法 $z = x + y$，规则是 $\\frac{\\partial L}{\\partial x} \\mathrel{+}= \\frac{\\partial L}{\\partial z}$ 和 $\\frac{\\partial L}{\\partial y} \\mathrel{+}= \\frac{\\partial L}{\\partial z}$。\n- **反向执行**: 在前向传播构建了整个计算图并计算出最终损失 $L_{AD}$ 之后，其梯度被设为 $1$。然后反向传播按相反顺序遍历磁带，执行每个存储的函数。这个过程系统地在参与计算的每个 `ADVar` 的 `.grad` 属性中累积正确的梯度。\n\n### 4. 验证方法\n\n验证过程严格比较了两种独立梯度计算方法的输出。\n\n1.  **AD 计算**: 使用 AD 框架的基本运算逐步构建 SNN 动态和损失函数。这会自动构建完整的计算图。然后从最终损失变量开始进行反向传播，填充每个 $v_t$ 变量的 `.grad` 属性，从而得到序列 $\\{\\frac{\\partial L}{\\partial v_t}_{\\text{AD}}\\}_{t=0}^{T}$。\n2.  **BPTT 计算**: 执行从第一性原理推导出的 BPTT 算法。这包括一个标准的数值前向传播来记录状态，然后是 BPTT 反向递推来计算伴随序列 $\\{\\delta^v_t\\}_{t=0}^{T}$。\n3.  **比较**: 使用最大绝对差作为误差度量来比较两个得到的梯度序列：\n    $$\n    \\Delta = \\max_{t \\in \\{0,\\dots,T\\}} \\left| \\delta^v_t - \\frac{\\partial L}{\\partial v_t}_{\\text{AD}} \\right|\n    $$\n    如果此误差在指定的数值容差 $\\Delta \\le 10^{-6}$ 内，则认为测试用例“通过”。此容差考虑了两种计算路径之间可能存在的微小浮点差异。成功的验证为 BPTT 推导及其实现的正确性提供了高度的置信度。",
            "answer": "```python\n# 完整且可运行的 Python 3 代码位于此处。\nimport numpy as np\n\n# 用于自动微分引擎的简单全局计算带\n_AD_TAPE = []\n\nclass ADVar:\n    \"\"\"表示用于自动微分的计算图中的一个变量。\"\"\"\n    def __init__(self, value):\n        self.value = np.float64(value)\n        self.grad = np.float64(0.0)\n\ndef _reset_ad():\n    \"\"\"清除全局计算带并重置梯度。\"\"\"\n    global _AD_TAPE\n    _AD_TAPE = []\n\ndef _backward_ad(root_var):\n    \"\"\"执行自动微分引擎的反向传播过程。\"\"\"\n    root_var.grad = np.float64(1.0)\n    for backward_fn in reversed(_AD_TAPE):\n        backward_fn()\n\n# --- 自动微分引擎的基本运算 ---\n\ndef ad_add(a: ADVar, b: ADVar) - ADVar:\n    \"\"\"自动微分的加法运算。\"\"\"\n    out = ADVar(a.value + b.value)\n    def _backward_fn():\n        a.grad += out.grad\n        b.grad += out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\ndef ad_sub(a: ADVar, b: ADVar) - ADVar:\n    \"\"\"自动微分的减法运算。\"\"\"\n    out = ADVar(a.value - b.value)\n    def _backward_fn():\n        a.grad += out.grad\n        b.grad -= out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\ndef ad_mul(a: ADVar, b: ADVar) - ADVar:\n    \"\"\"自动微分的乘法运算。\"\"\"\n    out = ADVar(a.value * b.value)\n    def _backward_fn():\n        a.grad += b.value * out.grad\n        b.grad += a.value * out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\ndef ad_square(a: ADVar) - ADVar:\n    \"\"\"自动微分的平方运算。\"\"\"\n    out = ADVar(a.value ** 2)\n    def _backward_fn():\n        a.grad += 2 * a.value * out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\ndef _logistic(u, beta):\n    \"\"\"逻辑斯谛 Sigmoid 函数。\"\"\"\n    return 1.0 / (1.0 + np.exp(-beta * u))\n\ndef ad_sigma(u_var: ADVar, beta: float) - ADVar:\n    \"\"\"自动微分的逻辑斯谛代理脉冲函数运算。\"\"\"\n    s_val = _logistic(u_var.value, beta)\n    out = ADVar(s_val)\n    def _backward_fn():\n        s_prime = beta * s_val * (1.0 - s_val)\n        u_var.grad += s_prime * out.grad\n    _AD_TAPE.append(_backward_fn)\n    return out\n\n# --- 验证实现 ---\n\ndef get_grads_with_ad(params: dict) - np.ndarray:\n    \"\"\"使用通用自动微分引擎计算梯度。\"\"\"\n    _reset_ad()\n    T = params['T']\n    alpha, w, theta, r = params['alpha'], params['w'], params['theta'], params['r']\n    beta, gamma, rho = params['beta'], params['gamma'], params['rho']\n    x, y = params['x'], params['y']\n\n    # --- AD 前向传播 ---\n    v_ad = [None] * (T + 1)\n    v_ad[0] = ADVar(0.0)\n    s_ad = [None] * T\n\n    # 神经元动力学\n    for t in range(T):\n        u_t = ad_sub(v_ad[t], ADVar(theta))\n        s_ad[t] = ad_sigma(u_t, beta)\n        \n        term1 = ad_mul(ADVar(alpha), v_ad[t])\n        term2 = ad_mul(ADVar(w), ADVar(x[t]))\n        term3 = ad_mul(ADVar(r), s_ad[t])\n        \n        v_ad[t+1] = ad_sub(ad_add(term1, term2), term3)\n\n    # 损失计算\n    total_loss = ADVar(0.0)\n    \n    # 脉冲匹配损失分量\n    if gamma  0:\n        spike_loss = ADVar(0.0)\n        for t in range(T):\n            err_t = ad_sub(s_ad[t], ADVar(y[t]))\n            loss_t = ad_square(err_t)\n            spike_loss = ad_add(spike_loss, loss_t)\n        scaled_spike_loss = ad_mul(ADVar(gamma / 2.0), spike_loss)\n        total_loss = ad_add(total_loss, scaled_spike_loss)\n        \n    # 最终电位正则化分量\n    if rho  0:\n        final_loss_term = ad_mul(ADVar(rho / 2.0), ad_square(v_ad[T]))\n        total_loss = ad_add(total_loss, final_loss_term)\n        \n    # --- AD 反向传播 ---\n    _backward_ad(total_loss)\n    \n    return np.array([v.grad for v in v_ad], dtype=np.float64)\n\ndef get_grads_with_bptt(params: dict) - np.ndarray:\n    \"\"\"使用推导的 BPTT 算法计算梯度。\"\"\"\n    T = params['T']\n    alpha, w, theta, r = params['alpha'], params['w'], params['theta'], params['r']\n    beta, gamma, rho = params['beta'], params['gamma'], params['rho']\n    x, y = params['x'], params['y']\n\n    # --- BPTT 前向传播（记录状态）---\n    v = np.zeros(T + 1, dtype=np.float64)\n    s = np.zeros(T, dtype=np.float64)\n    s_prime = np.zeros(T, dtype=np.float64)\n    v[0] = 0.0\n\n    for t in range(T):\n        u = v[t] - theta\n        s_val = _logistic(u, beta)\n        s[t] = s_val\n        s_prime[t] = beta * s_val * (1.0 - s_val)\n        v[t+1] = alpha * v[t] + w * x[t] - r * s[t]\n\n    # --- BPTT 反向传播（计算伴随变量）---\n    delta_v = np.zeros(T + 1, dtype=np.float64)\n    delta_v[T] = rho * v[T]\n\n    for t in range(T - 1, -1, -1):\n        local_grad = gamma * (s[t] - y[t]) * s_prime[t]\n        propagated_grad = delta_v[t+1] * (alpha - r * s_prime[t])\n        delta_v[t] = local_grad + propagated_grad\n\n    return delta_v\n\ndef solve():\n    \"\"\"运行测试用例并生成最终输出的主函数。\"\"\"\n    test_cases = [\n        {\n            \"T\": 8, \"alpha\": 0.9, \"w\": 0.5, \"theta\": 0.2, \"r\": 0.2, \"beta\": 8.0,\n            \"gamma\": 1.3, \"rho\": 0.7,\n            \"x\": [0.1, 0.0, 0.4, 0.0, 0.3, 0.5, 0.2, 0.0],\n            \"y\": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n        },\n        {\n            \"T\": 6, \"alpha\": 0.8, \"w\": 0.3, \"theta\": 5.0, \"r\": 0.5, \"beta\": 8.0,\n            \"gamma\": 1.0, \"rho\": 0.0,\n            \"x\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n            \"y\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n        },\n        {\n            \"T\": 5, \"alpha\": 0.0, \"w\": 1.0, \"theta\": 0.1, \"r\": 0.1, \"beta\": 5.0,\n            \"gamma\": 0.7, \"rho\": 0.2,\n            \"x\": [0.5, 0.5, 0.5, 0.5, 0.5],\n            \"y\": [1.0, 1.0, 0.0, 1.0, 0.0]\n        },\n        {\n            \"T\": 7, \"alpha\": 0.95, \"w\": -0.7, \"theta\": 0.25, \"r\": 0.25, \"beta\": 6.0,\n            \"gamma\": 0.9, \"rho\": 0.4,\n            \"x\": [0.2, 0.2, 0.0, 0.3, 0.1, 0.0, 0.2],\n            \"y\": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]\n        }\n    ]\n\n    results = []\n    tolerance = 1e-6\n\n    for case_params in test_cases:\n        grads_ad = get_grads_with_ad(case_params)\n        grads_bptt = get_grads_with_bptt(case_params)\n        \n        max_abs_diff = np.max(np.abs(grads_ad - grads_bptt))\n        \n        passed = max_abs_diff = tolerance\n        results.append(str(passed).lower())\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}