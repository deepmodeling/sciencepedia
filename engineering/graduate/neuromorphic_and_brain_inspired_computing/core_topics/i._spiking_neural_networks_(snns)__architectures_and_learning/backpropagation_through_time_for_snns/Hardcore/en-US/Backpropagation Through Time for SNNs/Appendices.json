{
    "hands_on_practices": [
        {
            "introduction": "Before tackling backpropagation, we must first understand and implement the forward propagation of signals in a Spiking Neural Network. This exercise focuses on simulating the dynamics of a single Leaky Integrate-and-Fire (LIF) neuron, the fundamental building block of many SNNs. By translating the neuron's continuous-time differential equation into a discrete-time update rule, you will build a simulation that tracks the membrane potential and generates spikes over time. Mastering this forward pass is essential, as Backpropagation Through Time requires the complete, unrolled history of neuron states to compute gradients during the subsequent backward pass .",
            "id": "4036252",
            "problem": "Consider a single Leaky Integrate-and-Fire (LIF) neuron evolving in discrete time under a known input current sequence. The membrane potential $v(t)$ in continuous time obeys the ordinary differential equation (ODE) $dv/dt = -\\frac{1}{\\tau} v(t) + k I(t)$, where $\\tau$ is a membrane time constant, $k$ is a gain (which can absorb biophysical constants such as membrane resistance), and $I(t)$ is the input current. A spike is emitted when the membrane potential reaches or exceeds a threshold. After a spike, the membrane potential is reset to a specified reset value. Backpropagation Through Time (BPTT) for Spiking Neural Networks (SNNs) requires an exact unrolled sequence of forward states over time to define the computational graph.\n\nAssume a uniform discrete-time step with an exponential-Euler discretization of the LIF dynamics. Let $\\alpha \\in (0,1)$ denote the discrete-time leak factor corresponding to the continuous-time decay over one step, $\\theta$ denote the firing threshold, and $V_{\\text{reset}}$ denote the hard reset potential. Let $I_t$ denote the discrete-time drive at step $t$, already scaled so that its contribution enters linearly in the pre-threshold membrane as defined by the chosen discretization. Define the Heaviside step function $H(x)$ to return $1$ when $x \\ge 0$ and $0$ otherwise. The neuron evolves as follows each discrete step: starting from the membrane potential $v_t$ at time index $t$, form a pre-threshold potential, determine a spike according to threshold crossing, and apply a hard reset to obtain the next-step membrane $v_{t+1}$. The exact ordering must be leak-and-input integration, thresholding, then reset. The unrolled state required by Backpropagation Through Time (BPTT) at each step $t$ must record the pre-integration state $v_t$, the pre-threshold integrated potential, the spike indicator $s_t$, and the post-reset state $v_{t+1}$.\n\nYour task is:\n- From the continuous-time LIF ODE $dv/dt = -\\frac{1}{\\tau} v(t) + k I(t)$, derive the discrete-time recurrence for the pre-threshold potential and the spike-reset rule under the stated exponential-Euler discretization and hard reset, expressed in terms of the given parameters $\\alpha$, $\\theta$, and $V_{\\text{reset}}$, and the sequence $I_t$.\n- Implement a program that, given $(\\alpha,\\theta,V_{\\text{reset}})$, an initial membrane potential $v_0$, and a sequence $I_t$ of length $T+1$ indexed by $t=0,\\dots,T$, computes and records for each $t=0,\\dots,T$: the pre-integration membrane $v_t$, the pre-threshold potential, the spike indicator $s_t \\in \\{0,1\\}$, and the post-reset membrane $v_{t+1}$, following the exact ordering: integrate, threshold, reset. Use the tie-breaking rule $s_t = 1$ when the pre-threshold potential equals $\\theta$ exactly.\n- Report, for each test case below, the exact unrolled state list required by BPTT as a list of lists, where the $t$-th entry is $[t, v_t, \\text{pre\\_threshold}, s_t, v_{t+1}]$ with $t$ as an integer, $v_t$ and the pre-threshold potential as floating-point values, $s_t$ as an integer $0$ or $1$, and $v_{t+1}$ as a floating-point value.\n\nUse the following test suite with scientifically plausible and self-consistent parameter values:\n1. Typical spiking case:\n   - $\\alpha = 0.9$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.0$, $v_0 = 0.0$, $T = 5$, $I_t$ for $t=0,\\dots,5$ equals $[0.5, 0.6, 0.0, 0.0, 0.0, 0.0]$.\n2. Boundary threshold equality case (tests tie-breaking $H(0)=1$):\n   - $\\alpha = 0.5$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.0$, $v_0 = 2.0$, $T = 2$, $I_t$ for $t=0,\\dots,2$ equals $[0.0, 0.0, 0.0]$.\n3. No-spike accumulation under leak:\n   - $\\alpha = 0.95$, $\\theta = 1.2$, $V_{\\text{reset}} = 0.0$, $v_0 = 0.1$, $T = 4$, $I_t$ for $t=0,\\dots,4$ equals $[0.1, 0.1, 0.1, 0.1, 0.1]$.\n4. Multiple spikes with nonzero reset:\n   - $\\alpha = 0.8$, $\\theta = 1.0$, $V_{\\text{reset}} = 0.2$, $v_0 = 0.0$, $T = 6$, $I_t$ for $t=0,\\dots,6$ equals $[1.0, 0.9, 0.9, 0.9, 0.0, 1.2, 0.0]$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element in this top-level list corresponds to one test case and must itself be a bracketed list containing $T+1$ entries of the form $[t,v_t,\\text{pre\\_threshold},s_t,v_{t+1}]$ in that order. For example, the outer list must look like $[ \\text{case1}, \\text{case2}, \\dots ]$ with no additional text. Angles are not used in this problem, and no physical units are required. All reported values must be numerical in the specified types.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard task in computational neuroscience: the simulation of a Leaky Integrate-and-Fire (LIF) neuron model in discrete time, which is a fundamental step for algorithms like Backpropagation Through Time (BPTT).\n\n### Derivation of the Discrete-Time Recurrence\n\nThe dynamics of the membrane potential $v(t)$ are given by the continuous-time ordinary differential equation (ODE):\n$$ \\frac{dv}{dt} = -\\frac{1}{\\tau} v(t) + k I(t) $$\nThis is a first-order linear ODE. To discretize it over a time step of duration $\\Delta t$, we use the exponential-Euler method. This method provides an exact solution to the ODE over the interval $[t, t+\\Delta t]$ under the assumption that the input current $I(t)$ is constant over that interval.\n\nThe solution to the homogeneous equation $\\frac{dv}{dt} = -\\frac{1}{\\tau} v(t)$ is $v(t) = v(t_0) e^{-(t-t_0)/\\tau}$. Over one time step $\\Delta t$, the potential decays by a factor of $e^{-\\Delta t/\\tau}$. The problem defines this discrete-time leak factor as $\\alpha$:\n$$ \\alpha = e^{-\\Delta t/\\tau} $$\nFor the full non-homogeneous equation, the exact solution over a single time step from $t$ to $t+\\Delta t$, assuming $I(t)$ is a constant $I_t$ during this interval, is:\n$$ v(t+\\Delta t) = v(t) e^{-\\Delta t/\\tau} + k\\tau(1 - e^{-\\Delta t/\\tau}) I_t $$\nSubstituting $\\alpha$ gives:\n$$ v(t+\\Delta t) = \\alpha v(t) + k\\tau(1 - \\alpha) I_t $$\nThe problem states that the discrete-time input drive $I_t$ is \"already scaled so that its contribution enters linearly\". This implies that the term $k\\tau(1 - \\alpha)$ is absorbed into the provided input sequence $I_t$. Thus, the discrete-time update rule for the membrane potential before considering spiking and reset is simply the sum of the leaked prior potential and the scaled input.\n\nLet $v_t$ be the membrane potential at the beginning of time step $t$. Following the specified order of operations (integrate, threshold, reset), we first calculate the pre-threshold potential, which we will denote as $u_t$:\n$$ u_t = \\alpha v_t + I_t $$\nThis step represents the leak and integration of input current.\n\nNext, we apply the thresholding rule. A spike $s_t$ is emitted if the pre-threshold potential $u_t$ reaches or exceeds the threshold $\\theta$. Using the Heaviside step function $H(x)$, defined as $1$ for $x \\ge 0$ and $0$ otherwise, the spike indicator $s_t \\in \\{0, 1\\}$ is:\n$$ s_t = H(u_t - \\theta) $$\nThis formulation correctly implements the tie-breaking rule that a spike is generated if $u_t = \\theta$.\n\nFinally, we apply the hard reset mechanism to determine the membrane potential $v_{t+1}$ for the beginning of the next time step. If a spike was generated ($s_t = 1$), the potential is reset to $V_{\\text{reset}}$. If no spike occurred ($s_t = 0$), the potential carries over from its pre-threshold value $u_t$. This can be expressed as a single equation:\n$$ v_{t+1} = (1 - s_t) u_t + s_t V_{\\text{reset}} $$\n\n### Simulation Algorithm\n\nThe simulation proceeds iteratively from an initial state $v_0$ for time steps $t = 0, 1, \\dots, T$. For each time step $t$:\n\n1.  **Retrieve State and Input**: The state at the start of the step is $v_t$. The input for this step is $I_t$.\n2.  **Integrate**: Calculate the pre-threshold potential $u_t = \\alpha v_t + I_t$.\n3.  **Threshold**: Determine the spike output $s_t = 1$ if $u_t \\ge \\theta$, and $s_t = 0$ otherwise.\n4.  **Reset**: Calculate the post-reset potential for the next time step, $v_{t+1} = (1 - s_t) u_t + s_t V_{\\text{reset}}$.\n5.  **Record State for BPTT**: Store the tuple $[t, v_t, u_t, s_t, v_{t+1}]$.\n6.  **Advance Time**: The potential for the next step, $v_{t+1}$, becomes the starting potential for the iteration at $t+1$.\n\nThis procedure generates the complete unrolled sequence of states required for BPTT. The implementation will execute this algorithm for each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are required for this problem.\n\ndef solve():\n    \"\"\"\n    Main function to run the LIF neuron simulations for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # 1. Typical spiking case\n        {\n            \"alpha\": 0.9, \"theta\": 1.0, \"v_reset\": 0.0, \"v0\": 0.0, \"T\": 5,\n            \"I\": [0.5, 0.6, 0.0, 0.0, 0.0, 0.0]\n        },\n        # 2. Boundary threshold equality case\n        {\n            \"alpha\": 0.5, \"theta\": 1.0, \"v_reset\": 0.0, \"v0\": 2.0, \"T\": 2,\n            \"I\": [0.0, 0.0, 0.0]\n        },\n        # 3. No-spike accumulation under leak\n        {\n            \"alpha\": 0.95, \"theta\": 1.2, \"v_reset\": 0.0, \"v0\": 0.1, \"T\": 4,\n            \"I\": [0.1, 0.1, 0.1, 0.1, 0.1]\n        },\n        # 4. Multiple spikes with nonzero reset\n        {\n            \"alpha\": 0.8, \"theta\": 1.0, \"v_reset\": 0.2, \"v0\": 0.0, \"T\": 6,\n            \"I\": [1.0, 0.9, 0.9, 0.9, 0.0, 1.2, 0.0]\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        alpha = case_params[\"alpha\"]\n        theta = case_params[\"theta\"]\n        v_reset = case_params[\"v_reset\"]\n        v0 = case_params[\"v0\"]\n        I_seq = case_params[\"I\"]\n        T = case_params[\"T\"]\n        \n        case_result = simulate_lif(alpha, theta, v_reset, v0, I_seq, T)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation of lists is compliant with the required output format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_lif(alpha, theta, v_reset, v0, I_seq, T):\n    \"\"\"\n    Simulates a discrete-time Leaky Integrate-and-Fire (LIF) neuron.\n\n    Args:\n        alpha (float): Discrete-time leak factor.\n        theta (float): Firing threshold.\n        v_reset (float): Reset potential after a spike.\n        v0 (float): Initial membrane potential.\n        I_seq (list of float): Sequence of input currents of length T+1.\n        T (int): The final time index for the simulation (total steps is T+1).\n\n    Returns:\n        list of lists: The unrolled state history required for BPTT. Each inner list\n                       is of the form [t, v_t, pre_threshold_potential, s_t, v_{t+1}].\n    \"\"\"\n    recorded_states = []\n    v_current = float(v0)\n    \n    # The simulation runs for t from 0 to T, inclusive.\n    for t in range(T + 1):\n        # Input for the current time step\n        I_t = float(I_seq[t])\n        \n        # v_t is the potential at the beginning of the step.\n        v_t = v_current\n        \n        # 1. Integrate: leak and input to get pre-threshold potential.\n        # This is denoted as u_t in the derivation.\n        pre_threshold_potential = alpha * v_t + I_t\n        \n        # 2. Threshold: check for spike. s_t is the spike indicator.\n        # The condition includes equality, implementing H(x) where H(0)=1.\n        s_t = 1 if pre_threshold_potential >= theta else 0\n        \n        # 3. Reset: determine the next state v_{t+1}.\n        if s_t == 1:\n            v_next = v_reset\n        else:\n            v_next = pre_threshold_potential\n        \n        # Record the BPTT state for this step.\n        # Types are: int, float, float, int, float.\n        recorded_states.append([t, v_t, pre_threshold_potential, s_t, float(v_next)])\n        \n        # Update the current potential for the next iteration.\n        v_current = float(v_next)\n        \n    return recorded_states\n\nsolve()\n```"
        },
        {
            "introduction": "Having implemented the forward dynamics, we now address the central challenge of training SNNs: the non-differentiable nature of spike events. This exercise introduces the concept of surrogate gradients, a key technique that replaces the mathematically problematic derivative of the spiking function with a well-behaved approximation during the backward pass. You will implement and compare two popular surrogates: the simple Straight-Through Estimator (STE) and a derivative based on a smooth sigmoid function. By deriving the BPTT update rule and analyzing these methods, you will gain hands-on experience with the core mechanism that enables gradient-based learning in SNNs .",
            "id": "4036243",
            "problem": "Consider a single-neuron Spiking Neural Network (SNN) with a discrete-time Leaky Integrate-and-Fire (LIF) neuron. The membrane potential $v_t$ evolves according to the difference equation\n$$v_{t+1} = \\beta v_t + w x_t - s_t \\theta,$$\nwhere $t \\in \\{0,1,\\dots,T-1\\}$, $v_0 = 0$, $w$ is a scalar synaptic weight, $x_t$ is a known input sequence, $\\beta \\in (0,1)$ is the leak factor, $\\theta > 0$ is the spike threshold, and $s_t \\in \\{0,1\\}$ is the spike at time $t$ given by the Heaviside step function $s_t = H(v_t - \\theta)$. The loss is defined as\n$$L = \\frac{1}{2}\\sum_{t=0}^{T-1} \\left(s_t - y_t\\right)^2,$$\nwhere $y_t \\in \\{0,1\\}$ is a prescribed target spike train. The task is to compute the gradient $\\partial L / \\partial w$ via Backpropagation Through Time (BPTT) using two different surrogate derivatives for $\\partial s_t / \\partial v_t$:\n1. The Straight-Through Estimator (STE): $$\\frac{\\partial s_t}{\\partial v_t} \\approx \\mathbf{1}_{|v_t - \\theta| < \\delta},$$ where $\\delta > 0$ defines the indicator window.\n2. A smoothed approximation to the true spike function: define $h_k(v) = \\sigma(k(v - \\theta))$ with $\\sigma(u) = \\frac{1}{1+e^{-u}}$ and use $$\\frac{d h_k(v)}{dv} = k \\sigma(k(v - \\theta))\\left(1 - \\sigma(k(v - \\theta))\\right)$$ as the surrogate for $\\frac{\\partial s_t}{\\partial v_t}$, for steepness parameter $k > 0$.\n\nStarting only from the given neuron dynamics, the Heaviside definition, and the loss function, derive the BPTT recursion for $\\partial v_t / \\partial w$ and the expression for $\\partial L / \\partial w$ in terms of the chosen surrogate derivative. Implement both gradient computations and analyze the induced gradient mismatch by reporting a single scalar per test case:\n$$r = \\frac{\\left|\\left(\\partial L / \\partial w\\right)_{\\text{STE}} - \\left(\\partial L / \\partial w\\right)_{k}\\right|}{\\left|\\left(\\partial L / \\partial w\\right)_{k}\\right| + \\varepsilon},$$\nwhere $\\varepsilon = 10^{-12}$ is a numerical stabilizer.\n\nUse the following fixed SNN parameters across all test cases: $T = 12$, $\\beta = 0.9$, $\\theta = 1.0$, $v_0 = 0$, and a fixed target spike train $y_t$ with unit spikes at times $t = 4$ and $t = 9$ (zero-based indexing), and $y_t = 0$ elsewhere.\n\nFor the input sequences, use deterministic, time-indexed definitions based on a type label and amplitude $A$:\n- Type \"pulses2\": $x_t = A$ for $t \\in \\{3,7\\}$ and $x_t = 0$ otherwise.\n- Type \"pulses4\": $x_t = A$ for $t \\in \\{2,4,6,8\\}$ and $x_t = 0$ otherwise.\n- Type \"single\": $x_t = A$ for $t = 4$ and $x_t = 0$ otherwise.\n- Type \"zero\": $x_t = 0$ for all $t$.\n- Type \"constant\": $x_t = A$ for all $t$.\n\nYour program must compute $r$ for each of the following test cases, each specified as $(\\delta, k, w, \\text{type}, A)$:\n- Case $1$: $(0.05, 100.0, 0.8, \\text{\"pulses2\"}, 1.2)$.\n- Case $2$: $(0.2, 100.0, 0.8, \\text{\"pulses2\"}, 1.2)$.\n- Case $3$: $(0.05, 20.0, 1.2, \\text{\"pulses4\"}, 1.5)$.\n- Case $4$: $(0.5, 10.0, 0.3, \\text{\"single\"}, 2.0)$.\n- Case $5$: $(0.01, 200.0, 0.8, \\text{\"zero\"}, 0.0)$.\n- Case $6$: $(0.2, 50.0, 1.0, \\text{\"constant\"}, 0.5)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_6]$). Each $r_i$ must be a floating-point number. No additional text should be printed.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard task in the field of neuromorphic computing: training a Spiking Neural Network (SNN) using gradient-based methods, which requires addressing the non-differentiability of the spike generation mechanism. All parameters, models, and objectives are clearly defined.\n\nThe core of the problem is to compute the gradient of a loss function $L$ with respect to a synaptic weight $w$, denoted as $\\frac{\\partial L}{\\partial w}$. The loss function is defined over a sequence of spikes:\n$$L = \\frac{1}{2}\\sum_{t=0}^{T-1} (s_t - y_t)^2$$\nwhere $s_t$ is the neuron's output spike at time $t$ and $y_t$ is the target spike.\n\nUsing the chain rule, the gradient can be expressed as:\n$$\\frac{\\partial L}{\\partial w} = \\sum_{t=0}^{T-1} \\frac{\\partial L}{\\partial s_t} \\frac{d s_t}{d w}$$\nThe first term is straightforward to compute from the loss function definition:\n$$\\frac{\\partial L}{\\partial s_t} = s_t - y_t$$\nThe second term, $\\frac{d s_t}{d w}$, requires further expansion. The output spike $s_t$ is a function of the membrane potential $v_t$, which in turn is a function of the weight $w$. Applying the chain rule again:\n$$\\frac{d s_t}{d w} = \\frac{d s_t}{d v_t} \\frac{d v_t}{d w}$$\nThe term $\\frac{d s_t}{d v_t}$ represents the derivative of the spike generation function, $s_t = H(v_t - \\theta)$, where $H$ is the Heaviside step function. The mathematical derivative is zero almost everywhere and is an undefined Dirac delta function at the threshold, which is unsuitable for gradient-based learning. To resolve this, we replace it with a continuous surrogate function, which we denote as $\\psi(v_t)$:\n$$\\frac{d s_t}{d v_t} \\approx \\psi(v_t)$$\nThe problem requires using two distinct surrogates for $\\psi(v_t)$:\n1.  **Straight-Through Estimator (STE)**: $\\psi(v_t) = \\mathbf{1}_{|v_t - \\theta| < \\delta}$, where $\\mathbf{1}$ is the indicator function. This approximates the derivative as a constant value of $1$ within a small window of width $2\\delta$ around the threshold $\\theta$, and $0$ otherwise.\n2.  **Smoothed Sigmoid Derivative**: $\\psi(v_t) = \\frac{d h_k(v_t)}{d v_t} = k \\sigma(k(v_t - \\theta))(1 - \\sigma(k(v_t - \\theta)))$, where $\\sigma(u) = (1+e^{-u})^{-1}$ is the logistic sigmoid function. This uses the derivative of a smooth approximation of the step function.\n\nThe remaining term to be determined is $\\frac{d v_t}{d w}$, which is the total derivative of the membrane potential at time $t$ with respect to the weight $w$. We can derive a recurrence relation for this term by differentiating the neuron's dynamic equation:\n$$v_{t+1} = \\beta v_t + w x_t - s_t \\theta$$\nLet us define $g_t := \\frac{d v_t}{d w}$. Differentiating the dynamics with respect to $w$ yields:\n$$\\frac{d v_{t+1}}{d w} = \\frac{d}{d w} (\\beta v_t + w x_t - s_t \\theta)$$\n$$g_{t+1} = \\beta \\frac{d v_t}{d w} + x_t - \\theta \\frac{d s_t}{d w}$$\nSubstituting $\\frac{d s_t}{d w} = \\psi(v_t) \\frac{d v_t}{d w} = \\psi(v_t) g_t$:\n$$g_{t+1} = \\beta g_t + x_t - \\theta \\psi(v_t) g_t$$\nThis gives the forward recurrence relation for $g_t$:\n$$g_{t+1} = (\\beta - \\theta \\psi(v_t)) g_t + x_t$$\nThe initial condition is based on $v_0 = 0$, which is a constant and does not depend on $w$. Therefore, the initial condition for the recurrence is $g_0 = \\frac{d v_0}{d w} = 0$.\n\nCombining these pieces, the final expression for the total gradient is:\n$$\\frac{\\partial L}{\\partial w} \\approx \\sum_{t=0}^{T-1} (s_t - y_t) \\psi(v_t) g_t$$\n\nThe overall algorithm for computing the gradient is as follows:\n\n1.  **Forward Pass**: Simulate the neuron dynamics to obtain the history of membrane potentials $v_t$ and spikes $s_t$ for $t \\in \\{0, 1, \\dots, T-1\\}$.\n    - Initialize $v_0 = 0$.\n    - For $t = 0, \\dots, T-1$:\n        - $s_t = 1$ if $v_t \\ge \\theta$, otherwise $s_t = 0$.\n        - $v_{t+1} = \\beta v_t + w x_t - s_t \\theta$.\n    - Store the sequences $v_0, \\dots, v_{T-1}$ and $s_0, \\dots, s_{T-1}$.\n\n2.  **Gradient Calculation**: Compute the gradient contribution at each time step and accumulate it. This involves a second forward pass to compute the sequence $g_t$.\n    - Initialize total gradient $\\frac{\\partial L}{\\partial w} = 0$.\n    - Initialize the potential's gradient $g_0 = 0$.\n    - For $t = 0, \\dots, T-1$:\n        - Compute the surrogate derivative $\\psi(v_t)$ based on the chosen method (STE or Smoothed).\n        - Update the total gradient: $\\frac{\\partial L}{\\partial w} \\leftarrow \\frac{\\partial L}{\\partial w} + (s_t - y_t) \\psi(v_t) g_t$.\n        - Update the potential's gradient for the next step: $g_{t+1} = (\\beta - \\theta \\psi(v_t)) g_t + x_t$.\n\nThis procedure is executed once for each surrogate gradient method to obtain $(\\partial L / \\partial w)_{\\text{STE}}$ and $(\\partial L / \\partial w)_{k}$. The relative difference metric $r$ is then computed as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef generate_input_sequence(seq_type, A, T):\n    \"\"\"Generates the input sequence x_t.\"\"\"\n    x = np.zeros(T)\n    if seq_type == \"pulses2\":\n        if 3 < T: x[3] = A\n        if 7 < T: x[7] = A\n    elif seq_type == \"pulses4\":\n        indices = [2, 4, 6, 8]\n        for i in indices:\n            if i < T:\n                x[i] = A\n    elif seq_type == \"single\":\n        if 4 < T: x[4] = A\n    elif seq_type == \"zero\":\n        pass  # x is already all zeros\n    elif seq_type == \"constant\":\n        x[:] = A\n    else:\n        raise ValueError(\"Unknown sequence type\")\n    return x\n\ndef calculate_gradient(w, seq_type, A, surrogate_choice, surrogate_param):\n    \"\"\"\n    Calculates the gradient dL/dw for a single-neuron SNN.\n    \n    Args:\n        w (float): The synaptic weight.\n        seq_type (str): The type of input sequence.\n        A (float): The amplitude of the input sequence.\n        surrogate_choice (str): 'STE' or 'Smoothed'.\n        surrogate_param (float): Delta for STE or k for Smoothed.\n\n    Returns:\n        float: The calculated gradient dL/dw.\n    \"\"\"\n    # Fixed SNN parameters\n    T = 12\n    beta = 0.9\n    theta = 1.0\n    v0 = 0.0\n\n    # Target spike train\n    y = np.zeros(T)\n    y[4] = 1.0\n    y[9] = 1.0\n\n    # Input sequence\n    x = generate_input_sequence(seq_type, A, T)\n\n    # --- 1. Forward Pass: Simulate neuron dynamics ---\n    v_hist = np.zeros(T)\n    s_hist = np.zeros(T)\n    v = v0\n\n    for t in range(T):\n        v_hist[t] = v\n        s = 1.0 if v >= theta else 0.0\n        s_hist[t] = s\n        v = beta * v + w * x[t] - s * theta\n\n    # --- 2. Gradient Calculation (Forward Mode) ---\n    grad_L_w = 0.0\n    g = 0.0  # g_0 = d(v_0)/dw = 0\n\n    for t in range(T):\n        v_t = v_hist[t]\n        \n        # Calculate surrogate derivative psi(v_t)\n        if surrogate_choice == 'STE':\n            delta = surrogate_param\n            psi = 1.0 if np.abs(v_t - theta) < delta else 0.0\n        elif surrogate_choice == 'Smoothed':\n            k = surrogate_param\n            u = k * (v_t - theta)\n            sigma_u = expit(u)\n            psi = k * sigma_u * (1.0 - sigma_u)\n        else:\n            raise ValueError(\"Invalid surrogate choice\")\n            \n        # Accumulate gradient\n        grad_L_w += (s_hist[t] - y[t]) * psi * g\n        \n        # Update g for the next time step\n        g = (beta - theta * psi) * g + x[t]\n        \n    return grad_L_w\n    \n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and compute the gradient mismatch ratio r.\n    \"\"\"\n    test_cases = [\n        # (delta, k, w, type, A)\n        (0.05, 100.0, 0.8, \"pulses2\", 1.2),\n        (0.2, 100.0, 0.8, \"pulses2\", 1.2),\n        (0.05, 20.0, 1.2, \"pulses4\", 1.5),\n        (0.5, 10.0, 0.3, \"single\", 2.0),\n        (0.01, 200.0, 0.8, \"zero\", 0.0),\n        (0.2, 50.0, 1.0, \"constant\", 0.5),\n    ]\n\n    results = []\n    epsilon = 1e-12\n\n    for case in test_cases:\n        delta, k, w, seq_type, A = case\n        \n        grad_ste = calculate_gradient(w, seq_type, A, 'STE', delta)\n        grad_k = calculate_gradient(w, seq_type, A, 'Smoothed', k)\n        \n        r = np.abs(grad_ste - grad_k) / (np.abs(grad_k) + epsilon)\n        results.append(r)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Implementing complex algorithms like BPTT can be susceptible to subtle errors. A crucial skill for any machine learning practitioner is the ability to verify the correctness of a gradient implementation. This practice introduces the gradient check, a powerful and widely used debugging technique. You will create a fully differentiable version of the SNN model and compare the gradient computed by your analytical BPTT implementation against a numerical estimate obtained via the finite-difference method. This process provides a reliable way to validate your BPTT code, ensuring it accurately reflects the derived mathematics before you apply it to more complex problems .",
            "id": "4036196",
            "problem": "Consider a single-neuron discrete-time Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) with a single synaptic weight $w$, driven by an external input sequence. Use Backpropagation Through Time (BPTT) with a differentiable surrogate for the spike-generation nonlinearity to compute the analytic gradient with respect to $w$, and compare it to a finite-difference estimate of the same gradient. Then quantify the relative error between the two gradient estimates.\n\nFundamental base and definitions: The LIF neuron state evolves according to the following discrete-time update at step $t$:\n1. The pre-reset membrane potential is $V_t^{\\mathrm{pre}} = \\alpha V_{t-1} + w x_t$, where $V_{t-1}$ is the post-reset membrane potential from the previous step, $\\alpha \\in (0,1)$ is a leak factor, and $x_t$ is the input at time $t$. The initial membrane potential is $V_0 = 0$.\n2. The spike activation is $z_t = s(u_t)$, where $u_t = V_t^{\\mathrm{pre}} - V_{\\mathrm{th}}$, $V_{\\mathrm{th}}$ is a fixed threshold, and $s(u)$ is a differentiable surrogate for the Heaviside step function. Use the standard logistic function $s(u) = \\frac{1}{1 + \\exp(-\\beta u)}$, with slope parameter $\\beta > 0$; its derivative is $s'(u) = \\beta s(u)(1 - s(u))$.\n3. A soft reset is applied by gating the membrane with the surrogate spike activation: $V_t = (1 - z_t) V_t^{\\mathrm{pre}}$.\n\nThe loss over a time horizon of length $T$ is the quadratic mismatch between the spike activation and a target sequence $y_t$:\n$$\n\\mathcal{L}(w) = \\frac{1}{2} \\sum_{t=1}^T \\left(z_t - y_t\\right)^2.\n$$\n\nTasks:\n1. Implement the forward dynamics described above using the differentiable surrogate $s(u)$ with slope $\\beta$.\n2. Derive and implement Backpropagation Through Time (BPTT) to compute the analytic gradient $\\frac{\\partial \\mathcal{L}}{\\partial w}$ by applying the chain rule across time on the computational graph defined by $V_t^{\\mathrm{pre}}$, $z_t$, and $V_t$.\n3. Compute a central finite-difference estimate of the gradient using a perturbation $\\varepsilon > 0$:\n$$\ng_{\\mathrm{fd}}(w) = \\frac{\\mathcal{L}(w + \\varepsilon) - \\mathcal{L}(w - \\varepsilon)}{2 \\varepsilon}.\n$$\n4. Compute the relative error between the analytic gradient $g_{\\mathrm{bptt}}(w)$ and the finite-difference estimate $g_{\\mathrm{fd}}(w)$ as\n$$\n\\mathrm{err} = \\frac{\\left|g_{\\mathrm{bptt}}(w) - g_{\\mathrm{fd}}(w)\\right|}{\\left|g_{\\mathrm{fd}}(w)\\right| + 10^{-12}}.\n$$\n5. For each test case, output the relative error as a floating-point number.\n\nScientific realism: Use fixed sequences and parameters that are plausible and self-consistent for the stated model. Note that the differentiable surrogate in the forward pass is essential for meaningful finite-difference gradient checking.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n\nTest suite:\nUse the following parameter sets to test different regimes and edge cases. In all cases the initial membrane potential is $V_0 = 0$, the leak is $\\alpha = 0.9$, the threshold is $V_{\\mathrm{th}} = 1.0$, and the time horizon length is $T = 6$, unless otherwise specified. The input and target sequences for $t = 1, \\ldots, T$ are\n$$\nx_{1:T} = [0, 1, 0.5, 0, 1, 0], \\quad y_{1:T} = [0, 1, 0, 0, 1, 0].\n$$\n\n- Case 1 (happy path, moderate slope and finite difference step): $w = 0.8$, $\\beta = 4.0$, $\\varepsilon = 10^{-4}$.\n- Case 2 (finite-difference boundary with very small perturbation): $w = 0.8$, $\\beta = 4.0$, $\\varepsilon = 10^{-8}$.\n- Case 3 (stiff surrogate, approaching hard threshold): $w = 0.8$, $\\beta = 50.0$, $\\varepsilon = 10^{-4}$.\n- Case 4 (near-threshold weight to induce sensitivity): $w = 1.2$, $\\beta = 4.0$, $\\varepsilon = 10^{-4}$.\n\nAnswer specification:\n- For each case, compute the single floating-point value $\\mathrm{err}$ as defined above.\n- The final output must be a single line with a list of the four errors, in the order of the cases above, formatted as \"[e1,e2,e3,e4]\".",
            "solution": "The problem is assessed as valid. It is scientifically grounded in the domain of computational neuroscience, specifically the training of spiking neural networks. The problem is well-posed, with all necessary mathematical definitions, parameters, and inputs provided for a unique and verifiable solution. The language is objective and formal.\n\nThe core of the problem is to compute the gradient of a loss function $\\mathcal{L}$ with respect to a synaptic weight $w$ in a single-neuron Leaky Integrate-and-Fire (LIF) model. This will be done analytically using Backpropagation Through Time (BPTT) and numerically using the finite-difference method.\n\nFirst, we establish the model dynamics and the loss function. The state of the neuron evolves over discrete time steps $t=1, \\dots, T$.\n\nThe pre-reset membrane potential at time $t$ is given by:\n$$\nV_t^{\\mathrm{pre}} = \\alpha V_{t-1} + w x_t\n$$\nwhere $V_{t-1}$ is the post-reset membrane potential from the previous step ($V_0 = 0$), $\\alpha$ is the leak factor, $w$ is the weight, and $x_t$ is the external input.\n\nThe surrogate spike activation $z_t$ is a differentiable function of $V_t^{\\mathrm{pre}}$:\n$$\nz_t = s(u_t) = s(V_t^{\\mathrm{pre}} - V_{\\mathrm{th}}) = \\frac{1}{1 + \\exp(-\\beta (V_t^{\\mathrm{pre}} - V_{\\mathrm{th}}))}\n$$\nThe derivative of the sigmoid function $s(u)$ is $s'(u) = \\beta s(u)(1 - s(u))$, which can be written in terms of $z_t$ as $s'(u_t) = \\beta z_t(1 - z_t)$.\n\nThe membrane potential is updated using a soft reset mechanism:\n$$\nV_t = (1 - z_t) V_t^{\\mathrm{pre}}\n$$\n\nThe total loss $\\mathcal{L}$ over the time horizon $T$ is the sum of squared errors between the spike activation $z_t$ and a target sequence $y_t$:\n$$\n\\mathcal{L}(w) = \\sum_{t=1}^T \\mathcal{L}_t = \\frac{1}{2} \\sum_{t=1}^T (z_t - y_t)^2\n$$\n\nOur objective is to compute the gradient $\\frac{\\partial \\mathcal{L}}{\\partial w}$. Since the loss is a sum over time, its gradient is also a sum:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\sum_{t=1}^T \\frac{\\partial \\mathcal{L}}{\\partial w} (\\text{at time } t)\n$$\nWe use the chain rule to trace the influence of $w$ on the loss. The weight $w$ influences $\\mathcal{L}$ at each time step $t$ through the pre-reset membrane potential $V_t^{\\mathrm{pre}}$.\nThe total gradient can thus be written as:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w} = \\sum_{t=1}^T \\delta_t^{V^{\\mathrm{pre}}} x_t\n$$\nwhere we define $\\delta_t^{V^{\\mathrm{pre}}} := \\frac{\\partial \\mathcal{L}}{\\partial V_t^{\\mathrm{pre}}}$ as the gradient of the total loss with respect to the pre-reset potential at time $t$. The BPTT algorithm provides a method to compute these $\\delta_t^{V^{\\mathrm{pre}}}$ terms efficiently by propagating gradients backward in time.\n\nLet's derive the recurrence relation for the gradients. The total loss $\\mathcal{L}$ depends on $V_t^{\\mathrm{pre}}$ through two paths:\n1.  Directly through $z_t$, which affects the loss term $\\mathcal{L}_t$.\n2.  Indirectly through $V_t$, which affects all future loss terms $\\mathcal{L}_{t+1}, \\dots, \\mathcal{L}_T$.\n\nSo, we can write $\\delta_t^{V^{\\mathrm{pre}}}$ as:\n$$\n\\delta_t^{V^{\\mathrm{pre}}} = \\frac{\\partial \\mathcal{L}}{\\partial V_t^{\\mathrm{pre}}} = \\frac{\\partial \\mathcal{L}_t}{\\partial V_t^{\\mathrm{pre}}} + \\frac{\\partial \\left( \\sum_{k=t+1}^T \\mathcal{L}_k \\right)}{\\partial V_t^{\\mathrm{pre}}}\n$$\nThe first term is the gradient from the current time step's loss:\n$$\n\\frac{\\partial \\mathcal{L}_t}{\\partial V_t^{\\mathrm{pre}}} = \\frac{\\partial \\mathcal{L}_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial u_t} \\frac{\\partial u_t}{\\partial V_t^{\\mathrm{pre}}} = (z_t - y_t) \\cdot s'(u_t) \\cdot 1 = (z_t - y_t) s'(u_t)\n$$\nThe second term represents the gradient flowing back from the future. This gradient is propagated via the post-reset potential $V_t$. Let's define $\\delta_t^V := \\frac{\\partial \\mathcal{L}}{\\partial V_t}$ as the gradient of the total loss with respect to $V_t$. The chain rule gives:\n$$\n\\frac{\\partial \\left( \\sum_{k=t+1}^T \\mathcal{L}_k \\right)}{\\partial V_t^{\\mathrm{pre}}} = \\frac{\\partial \\mathcal{L}}{\\partial V_t} \\frac{\\partial V_t}{\\partial V_t^{\\mathrm{pre}}} = \\delta_t^V \\frac{\\partial V_t}{\\partial V_t^{\\mathrm{pre}}}\n$$\nWe compute the partial derivative of the reset mechanism:\n$$\n\\frac{\\partial V_t}{\\partial V_t^{\\mathrm{pre}}} = \\frac{\\partial}{\\partial V_t^{\\mathrm{pre}}} \\left[ (1 - z_t) V_t^{\\mathrm{pre}} \\right] = (1 - z_t) \\cdot 1 + V_t^{\\mathrm{pre}} \\cdot \\left( -\\frac{\\partial z_t}{\\partial V_t^{\\mathrm{pre}}} \\right) = (1 - z_t) - V_t^{\\mathrm{pre}} s'(u_t)\n$$\nCombining these terms gives the expression for $\\delta_t^{V^{\\mathrm{pre}}}$:\n$$\n\\delta_t^{V^{\\mathrm{pre}}} = (z_t - y_t) s'(u_t) + \\delta_t^V \\left[ (1 - z_t) - V_t^{\\mathrm{pre}} s'(u_t) \\right]\n$$\nTo complete the recurrence, we need to relate $\\delta_t^V$ to the gradients at time $t+1$. The potential $V_t$ only influences the future through $V_{t+1}^{\\mathrm{pre}}$.\n$$\n\\delta_t^V = \\frac{\\partial \\mathcal{L}}{\\partial V_t} = \\frac{\\partial \\mathcal{L}}{\\partial V_{t+1}^{\\mathrm{pre}}} \\frac{\\partial V_{t+1}^{\\mathrm{pre}}}{\\partial V_t} = \\delta_{t+1}^{V^{\\mathrm{pre}}} \\cdot \\alpha\n$$\nThis gives us a way to compute $\\delta_t^V$ from the $\\delta_{t+1}^{V^{\\mathrm{pre}}}$ we would have computed in the previous step of the backward pass.\n\nThe BPTT algorithm proceeds as follows:\n1.  **Forward Pass**: For $t = 1, \\dots, T$, compute and store the sequences $V_t^{\\mathrm{pre}}$, $u_t$, $z_t$, and $V_t$ using the model's dynamical equations. The initial state is $V_0 = 0$.\n2.  **Backward Pass**: Initialize the total gradient $\\frac{\\partial \\mathcal{L}}{\\partial w} = 0$ and the incoming potential gradient $\\delta_T^V = \\frac{\\partial \\mathcal{L}}{\\partial V_T} = 0$. Iterate backward from $t = T$ down to $1$:\n    a. Compute $\\delta_t^{V^{\\mathrm{pre}}}$ using the formula derived above, with $\\delta_t^V$ being the gradient propagated from step $t+1$.\n    b. Update the total weight gradient: $\\frac{\\partial \\mathcal{L}}{\\partial w} \\leftarrow \\frac{\\partial \\mathcal{L}}{\\partial w} + \\delta_t^{V^{\\mathrm{pre}}} x_t$.\n    c. Compute the gradient to be passed to the previous step: $\\delta_{t-1}^V = \\delta_t^{V^{\\mathrm{pre}}} \\alpha$. This becomes the new $\\delta_t^V$ for the next iteration of the loop (at time $t-1$).\n\nThe resulting value of $\\frac{\\partial \\mathcal{L}}{\\partial w}$ is the analytic gradient, $g_{\\mathrm{bptt}}(w)$.\n\nFor verification, we compute a numerical gradient using the central finite-difference formula:\n$$\ng_{\\mathrm{fd}}(w) = \\frac{\\mathcal{L}(w + \\varepsilon) - \\mathcal{L}(w - \\varepsilon)}{2 \\varepsilon}\n$$\nThis involves running the forward simulation twice with perturbed weights $w+\\varepsilon$ and $w-\\varepsilon$ to find the corresponding losses.\n\nFinally, the relative error between the analytic and numerical estimates is calculated as:\n$$\n\\mathrm{err} = \\frac{\\left|g_{\\mathrm{bptt}}(w) - g_{\\mathrm{fd}}(w)\\right|}{\\left|g_{\\mathrm{fd}}(w)\\right| + 10^{-12}}\n$$\nThe small constant $10^{-12}$ in the denominator ensures numerical stability if $g_{\\mathrm{fd}}(w)$ is close to zero. The implementation will execute this entire procedure for each of the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the BPTT for LIF neuron problem for a suite of test cases.\n    \"\"\"\n    \n    # Fixed parameters for all test cases as per the problem statement\n    ALPHA = 0.9\n    V_TH = 1.0\n    T = 6\n    # Sequences are padded with a 0 at index 0 to align with 1-based time indexing t=1..T\n    X_SEQ = np.array([0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0])\n    Y_SEQ = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0])\n    V0 = 0.0\n\n    def sigmoid(u, beta):\n        \"\"\"Computes the logistic sigmoid function.\"\"\"\n        # Note: Using np.exp is essential for vectorized operations with numpy arrays.\n        return 1.0 / (1.0 + np.exp(-beta * u))\n\n    def run_simulation(w, beta):\n        \"\"\"\n        Performs the forward pass of the LIF neuron simulation.\n        \n        Args:\n            w (float): The synaptic weight.\n            beta (float): The slope parameter of the surrogate spike function.\n\n        Returns:\n            tuple: A tuple containing:\n                - loss (float): The total loss over the time horizon.\n                - history (dict): A dictionary of stored states (V, V_pre, u, z).\n        \"\"\"\n        V_hist = np.zeros(T + 1)\n        V_pre_hist = np.zeros(T + 1)\n        u_hist = np.zeros(T + 1)\n        z_hist = np.zeros(T + 1)\n\n        V_hist[0] = V0\n\n        # Forward pass from t=1 to T\n        for t in range(1, T + 1):\n            V_pre_hist[t] = ALPHA * V_hist[t - 1] + w * X_SEQ[t]\n            u_hist[t] = V_pre_hist[t] - V_TH\n            z_hist[t] = sigmoid(u_hist[t], beta)\n            V_hist[t] = (1 - z_hist[t]) * V_pre_hist[t]\n\n        # Calculate loss. Slicing with [1:] ignores the dummy 0th element.\n        loss = 0.5 * np.sum((z_hist[1:] - Y_SEQ[1:])**2)\n\n        history = {\n            \"V_pre\": V_pre_hist,\n            \"u\": u_hist,\n            \"z\": z_hist,\n        }\n        \n        return loss, history\n\n    def compute_bptt_gradient(w, beta):\n        \"\"\"\n        Computes the analytic gradient dL/dw using Backpropagation Through Time.\n        \n        Args:\n            w (float): The synaptic weight.\n            beta (float): The slope parameter.\n\n        Returns:\n            float: The analytic gradient g_bptt(w).\n        \"\"\"\n        _, history = run_simulation(w, beta)\n        \n        V_pre = history[\"V_pre\"]\n        z = history[\"z\"]\n\n        dL_dw = 0.0\n        # dL_dV is the gradient dL/dV_t propagated from the future (t+1) to the present (t).\n        # We start with dL/dV_T = 0, as V_T has no future influence.\n        dL_dV = 0.0\n        \n        # Backward pass from t=T down to 1\n        for t in range(T, 0, -1):\n            s_prime = beta * z[t] * (1.0 - z[t])\n            \n            # Gradient of loss w.r.t. V_pre_t.\n            # This combines the local gradient from z_t and the propagated gradient from V_t.\n            dL_dV_pre = (z[t] - Y_SEQ[t]) * s_prime + dL_dV * ((1.0 - z[t]) - V_pre[t] * s_prime)\n\n            # Accumulate gradient dL/dw from the current time step\n            dL_dw += dL_dV_pre * X_SEQ[t]\n            \n            # Propagate gradient to V_{t-1} for the next iteration.\n            # This becomes the new dL_dV for time step t-1.\n            dL_dV = dL_dV_pre * ALPHA\n            \n        return dL_dw\n\n    def compute_fd_gradient(w, beta, epsilon):\n        \"\"\"\n        Computes the numerical gradient dL/dw using the central finite-difference method.\n        \n        Args:\n            w (float): The synaptic weight.\n            beta (float): The slope parameter.\n            epsilon (float): The perturbation for the finite difference.\n\n        Returns:\n            float: The numerical gradient g_fd(w).\n        \"\"\"\n        loss_plus, _ = run_simulation(w + epsilon, beta)\n        loss_minus, _ = run_simulation(w - epsilon, beta)\n        \n        g_fd = (loss_plus - loss_minus) / (2.0 * epsilon)\n        return g_fd\n\n    # Test suite from the problem statement\n    test_cases = [\n        # (w, beta, epsilon)\n        (0.8, 4.0, 1e-4),  # Case 1\n        (0.8, 4.0, 1e-8),  # Case 2\n        (0.8, 50.0, 1e-4), # Case 3\n        (1.2, 4.0, 1e-4),  # Case 4\n    ]\n\n    results = []\n    \n    for w, beta, epsilon in test_cases:\n        g_bptt = compute_bptt_gradient(w, beta)\n        g_fd = compute_fd_gradient(w, beta, epsilon)\n        \n        # Compute relative error\n        err = np.abs(g_bptt - g_fd) / (np.abs(g_fd) + 1e-12)\n        results.append(err)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}