{
    "hands_on_practices": [
        {
            "introduction": "The core function of a reservoir is to create a rich, high-dimensional representation of a time-varying input, which is achieved through the \"memory\" of its constituent nodes. This exercise  provides a hands-on look at how a simple synaptic model, based on an exponential filter, transforms a sequence of discrete events like spikes into a continuous state that retains information about the past. Mastering this is foundational to understanding how reservoirs process temporal information.",
            "id": "4050175",
            "problem": "Consider a reservoir node in a Liquid State Machine (LSM) or an Echo State Network (ESN) that receives a univariate spike train and exposes a continuous-time filtered state to a linear readout. In a commonly used synaptic model for such reservoirs, the post-synaptic current evoked by a single presynaptic spike is modeled by a causal exponential kernel with time constant $\\,\\tau\\,$. Specifically, let the kernel be $\\,k(t)=\\exp(-t/\\tau)\\,H(t)\\,$, where $\\,H(t)\\,$ is the Heaviside step function. Let the input spike train be $\\,s(t)=\\sum_{i=1}^{N}\\delta(t-t_i)\\,$, where $\\,\\delta(\\cdot)\\,$ is the Dirac delta distribution and $\\,t_1,\\dots,t_N\\,$ are spike times. The filtered state used as a readout feature is the convolution $\\,z(t)=(k*s)(t)\\,$.\n\nStarting only from the definitions of linear time-invariant convolution and the sifting property of the Dirac delta distribution, and without assuming any other specialized results, do the following:\n\n1) Derive a closed-form expression for $\\,z(t)\\,$ in terms of $\\,\\{t_i\\}\\,$ and $\\,\\tau\\,$ that is valid for arbitrary $\\,t\\,$.\n\n2) For a particular realization with $\\,N=4\\,$ spikes at times $\\,t_1=2\\,\\mathrm{ms}\\,$, $\\,t_2=7\\,\\mathrm{ms}\\,$, $\\,t_3=9\\,\\mathrm{ms}\\,$, and $\\,t_4=15\\,\\mathrm{ms}\\,$, compute $\\,z(T)\\,$ at observation time $\\,T=20\\,\\mathrm{ms}\\,$ as a closed-form analytic expression in terms of $\\,\\tau\\,$, where $\\,\\tau\\,$ is measured in $\\,\\mathrm{ms}\\,$ so that the argument of the exponential is dimensionless.\n\n3) To make precise how $\\,\\tau\\,$ tunes the reservoir’s effective memory, define the relative weight $\\,w(L;\\tau)\\,$ of a spike that occurred $\\,L\\,$ units of time in the past as the ratio of its contribution to $\\,z(t)\\,$ at time $\\,t\\,$ to that of an infinitesimally recent spike at the same time $\\,t\\,$. Using the same model, derive $\\,w(L;\\tau)\\,$ and then derive the effective memory horizon $\\,L_{\\epsilon}(\\tau)\\,$ such that spikes older than $\\,L_{\\epsilon}(\\tau)\\,$ are attenuated to at most a fraction $\\,\\epsilon\\,$ of a recent spike, where $\\,0\\epsilon1\\,$.\n\nProvide your final answer as the closed-form analytic expression for $\\,z(T)\\,$ from part $\\,2\\,$, expressed as a sum of exponentials in terms of $\\,\\tau\\,$ only. Do not include units in your final answer.",
            "solution": "The problem is deemed valid as it is scientifically grounded in the established theory of linear time-invariant systems and its application to simplified neural modeling, is well-posed with sufficient and consistent information, and is expressed in objective, formal language. We proceed with the solution.\n\nThe problem is divided into three parts. First, we derive a general expression for the filtered state $z(t)$. Second, we apply this expression to a specific case. Third, we analyze the memory properties of the system.\n\nPart 1: Derivation of the filtered state $z(t)$\n\nThe filtered state $z(t)$ is given by the convolution of the kernel $k(t)$ with the input spike train $s(t)$. The definition of convolution for linear time-invariant systems is:\n$$z(t) = (k*s)(t) = \\int_{-\\infty}^{\\infty} k(t-\\lambda)s(\\lambda)d\\lambda$$\nThe givens are the kernel $k(t) = \\exp(-t/\\tau)H(t)$ and the spike train $s(t) = \\sum_{i=1}^{N}\\delta(t-t_i)$, where $H(t)$ is the Heaviside step function and $\\delta(t)$ is the Dirac delta distribution.\n\nSubstituting the expressions for $k(t)$ and $s(t)$ into the convolution integral:\n$$z(t) = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{t-\\lambda}{\\tau}\\right)H(t-\\lambda) \\left( \\sum_{i=1}^{N}\\delta(\\lambda-t_i) \\right) d\\lambda$$\nDue to the linearity of the integral, we can interchange the summation and integration operators:\n$$z(t) = \\sum_{i=1}^{N} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{t-\\lambda}{\\tau}\\right)H(t-\\lambda) \\delta(\\lambda-t_i) d\\lambda$$\nThe problem requires using the sifting property of the Dirac delta distribution, which states that for a function $f(\\lambda)$, $\\int_{-\\infty}^{\\infty} f(\\lambda)\\delta(\\lambda-a)d\\lambda = f(a)$, provided $f$ is continuous at $a$. In our case, the function is $f(\\lambda) = \\exp\\left(-\\frac{t-\\lambda}{\\tau}\\right)H(t-\\lambda)$ and the point of evaluation is $a=t_i$. Applying the sifting property to each integral in the sum:\n$$\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{t-\\lambda}{\\tau}\\right)H(t-\\lambda) \\delta(\\lambda-t_i) d\\lambda = \\exp\\left(-\\frac{t-t_i}{\\tau}\\right)H(t-t_i)$$\nSubstituting this back into the summation gives the closed-form expression for $z(t)$:\n$$z(t) = \\sum_{i=1}^{N} \\exp\\left(-\\frac{t-t_i}{\\tau}\\right)H(t-t_i)$$\nThis expression shows that the state $z(t)$ is a linear superposition of the exponentially decaying responses to each past spike. The Heaviside function $H(t-t_i)$ enforces causality, ensuring that a spike at time $t_i$ contributes to the state only for times $t \\ge t_i$.\n\nPart 2: Computation of $z(T)$ for a specific spike train\n\nWe are given $N=4$ spikes at times $t_1=2\\,\\mathrm{ms}$, $t_2=7\\,\\mathrm{ms}$, $t_3=9\\,\\mathrm{ms}$, and $t_4=15\\,\\mathrm{ms}$. We need to compute the state $z(T)$ at the observation time $T=20\\,\\mathrm{ms}$. The time constant $\\tau$ is also measured in $\\mathrm{ms}$, making the arguments of the exponentials dimensionless.\n\nUsing the formula derived in Part 1 with $t=T=20$:\n$$z(20) = \\sum_{i=1}^{4} \\exp\\left(-\\frac{20-t_i}{\\tau}\\right)H(20-t_i)$$\nWe evaluate each term in the sum:\nFor $i=1$: $t_1=2$. The argument of the Heaviside function is $20-2=18 > 0$, so $H(18)=1$. The term is $\\exp\\left(-\\frac{18}{\\tau}\\right)$.\nFor $i=2$: $t_2=7$. The argument of the Heaviside function is $20-7=13 > 0$, so $H(13)=1$. The term is $\\exp\\left(-\\frac{13}{\\tau}\\right)$.\nFor $i=3$: $t_3=9$. The argument of the Heaviside function is $20-9=11 > 0$, so $H(11)=1$. The term is $\\exp\\left(-\\frac{11}{\\tau}\\right)$.\nFor $i=4$: $t_4=15$. The argument of the Heaviside function is $20-15=5 > 0$, so $H(5)=1$. The term is $\\exp\\left(-\\frac{5}{\\tau}\\right)$.\n\nSumming these contributions gives the final expression for $z(20)$:\n$$z(20) = \\exp\\left(-\\frac{18}{\\tau}\\right) + \\exp\\left(-\\frac{13}{\\tau}\\right) + \\exp\\left(-\\frac{11}{\\tau}\\right) + \\exp\\left(-\\frac{5}{\\tau}\\right)$$\n\nPart 3: Derivation of effective memory measures\n\nFirst, we derive the relative weight $w(L;\\tau)$. This is defined as the ratio of the contribution to $z(t)$ from a spike at time $t_{past} = t-L$ to that of an infinitesimally recent spike at time $t$. Let the recent spike occur at $t_{recent} = t-\\delta$, where $\\delta \\to 0^{+}$.\n\nThe contribution of the past spike at $t-L$ is found by applying the result from Part 1 for a single spike ($N=1$):\n$$C_{past} = \\exp\\left(-\\frac{t-(t-L)}{\\tau}\\right)H(t-(t-L)) = \\exp\\left(-\\frac{L}{\\tau}\\right)H(L)$$\nSince $L$ represents a duration in the past, $L0$, so $H(L)=1$. Thus, $C_{past} = \\exp(-L/\\tau)$.\n\nThe contribution of the infinitesimally recent spike at $t-\\delta$ is:\n$$C_{recent} = \\exp\\left(-\\frac{t-(t-\\delta)}{\\tau}\\right)H(t-(t-\\delta)) = \\exp\\left(-\\frac{\\delta}{\\tau}\\right)H(\\delta)$$\nAs $\\delta \\to 0^{+}$ (representing an infinitesimally small delay), $H(\\delta)=1$ and $\\exp(-\\delta/\\tau) \\to \\exp(0) = 1$. So, $C_{recent} = 1$.\n\nThe relative weight $w(L;\\tau)$ is the ratio:\n$$w(L;\\tau) = \\frac{C_{past}}{C_{recent}} = \\frac{\\exp(-L/\\tau)}{1} = \\exp\\left(-\\frac{L}{\\tau}\\right)$$\n\nNext, we derive the effective memory horizon $L_{\\epsilon}(\\tau)$. This is the time $L$ such that any spike older than $L$ is attenuated to a fraction $\\epsilon$ or less of a recent spike. This means we are looking for the value $L_{\\epsilon}$ that satisfies the condition $w(L; \\tau) \\le \\epsilon$ for all $L > L_{\\epsilon}$. Since $w(L;\\tau)$ is a monotonically decreasing function of $L$ (for $L0, \\tau0$), this condition is satisfied by finding the boundary value $L_{\\epsilon}$ where the equality holds:\n$$w(L_{\\epsilon};\\tau) = \\epsilon$$\n$$\\exp\\left(-\\frac{L_{\\epsilon}}{\\tau}\\right) = \\epsilon$$\nTo solve for $L_{\\epsilon}$, we take the natural logarithm of both sides:\n$$-\\frac{L_{\\epsilon}}{\\tau} = \\ln(\\epsilon)$$\n$$L_{\\epsilon}(\\tau) = -\\tau\\ln(\\epsilon)$$\nSince the problem states $0  \\epsilon  1$, we have $\\ln(\\epsilon)  0$, which ensures that $L_{\\epsilon}(\\tau)$ is a positive quantity. This can also be written as $L_{\\epsilon}(\\tau) = \\tau\\ln(1/\\epsilon)$. This result quantifies how the time constant $\\tau$ scales the memory of the system; a larger $\\tau$ leads to a longer memory horizon for a fixed attenuation factor $\\epsilon$.\n\nThe final answer requested is the expression for $z(T)$ from Part 2.",
            "answer": "$$\n\\boxed{\\exp\\left(-\\frac{18}{\\tau}\\right) + \\exp\\left(-\\frac{13}{\\tau}\\right) + \\exp\\left(-\\frac{11}{\\tau}\\right) + \\exp\\left(-\\frac{5}{\\tau}\\right)}\n$$"
        },
        {
            "introduction": "A functional reservoir must possess the \"echo state property,\" ensuring that its response is uniquely determined by the input history and not by its initial state. This requires the network's internal dynamics to be stable. This practice  moves beyond analyzing a given network to proactively designing one, using the elegant Gershgorin Circle Theorem to construct a recurrent weight matrix that is guaranteed to be stable—a crucial skill for building reliable reservoir computers.",
            "id": "4050223",
            "problem": "Consider a discrete-time linearized reservoir in the linear regime of an Echo State Network (ESN), where the state update is given by $x_{t+1} = W x_t + U u_t$, and the input $u_t$ is bounded. The Echo State Property (ESP) requires that the reservoir dynamics be asymptotically contractive for fixed input, which in the linear case is guaranteed if the spectral radius of $W$ is strictly less than $1$. You are tasked with designing a recurrent weight matrix that guarantees stability in the linear regime using the Gershgorin Circle Theorem and then validating the design.\n\nLet the recurrent weight matrix be parameterized as $W(s) = (1 - \\ell) I + s S$, where $\\ell \\in (0, 1)$ is the leak parameter, $s \\geq 0$ is a scalar gain to be designed, $I$ is the identity matrix, and $S$ is a fixed, strictly upper-triangular connectivity template with zero diagonal:\n$$\nS \\;=\\;\n\\begin{pmatrix}\n0  \\frac{3}{10}  -\\frac{1}{5}  0 \\\\\n0  0  \\frac{1}{4}  -\\frac{1}{4} \\\\\n0  0  0  \\frac{11}{20} \\\\\n0  0  0  0\n\\end{pmatrix},\n\\qquad\n\\ell \\;=\\; \\frac{1}{5}.\n$$\nDesign $s$ by enforcing that all Gershgorin disks of $W(s)$ lie within the unit circle in the complex plane, thereby guaranteeing that all eigenvalues of $W(s)$ lie within the unit circle. Then validate the choice by computing the eigenvalues of $W(s)$ at the designed $s$ and argue that the ESP holds in the linear regime for bounded inputs by considering the difference dynamics between two trajectories driven by the same input.\n\nProvide the largest $s$ that satisfies the Gershgorin-based design requirement as a single real number or a single closed-form analytic expression. No rounding is required and no units are involved. The final boxed answer must be this value of $s$ only.",
            "solution": "The user's problem is first subjected to a validation check.\n\n### Step 1: Extract Givens\n-   State update equation: $x_{t+1} = W x_t + U u_t$\n-   Recurrent weight matrix parameterization: $W(s) = (1 - \\ell) I + s S$\n-   Leak parameter: $\\ell = \\frac{1}{5}$\n-   Scalar gain: $s \\geq 0$\n-   Identity matrix: $I$\n-   Connectivity template matrix:\n    $$\n    S \\;=\\;\n    \\begin{pmatrix}\n    0  \\frac{3}{10}  -\\frac{1}{5}  0 \\\\\n    0  0  \\frac{1}{4}  -\\frac{1}{4} \\\\\n    0  0  0  \\frac{11}{20} \\\\\n    0  0  0  0\n    \\end{pmatrix}\n    $$\n-   Design constraint: All Gershgorin disks of $W(s)$ must lie within the unit circle in the complex plane.\n-   Task: Find the largest $s$ satisfying the design constraint, then validate the choice and argue for the Echo State Property (ESP).\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is firmly rooted in the theory of reservoir computing, specifically Echo State Networks (ESNs). The concepts of the Echo State Property, spectral radius, state-space models, and stability are fundamental to this field. The use of the Gershgorin Circle Theorem is a standard and valid mathematical technique for bounding the eigenvalues of a matrix. The problem is scientifically sound.\n-   **Well-Posed:** The problem clearly defines the matrix structure, provides all necessary parameters, and specifies a clear objective: to find the largest gain $s$ that satisfies a given stability criterion. The subsequent validation step is also well-defined. A unique solution for $s$ exists.\n-   **Objective:** The problem is stated using precise mathematical language and definitions, free of any subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is scientifically grounded, well-posed, and objective. It is deemed **valid**. The solution process will now proceed.\n\nThe goal is to design the scalar gain $s$ such that the spectral radius of the recurrent weight matrix $W(s)$ is strictly less than $1$, i.e., $\\rho(W(s))  1$. This guarantees the Echo State Property (ESP) for the linearized system. The design is to be performed using the Gershgorin Circle Theorem.\n\nFirst, we construct the matrix $W(s)$. With the given leak parameter $\\ell = \\frac{1}{5}$, the term $(1-\\ell)$ becomes $1 - \\frac{1}{5} = \\frac{4}{5}$.\nThe matrix $W(s)$ is given by:\n$$\nW(s) = (1 - \\ell) I + s S = \\frac{4}{5} I + s S\n$$\nSubstituting the matrices $I$ and $S$:\n$$\nW(s) = \\frac{4}{5}\n\\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1\n\\end{pmatrix}\n+ s\n\\begin{pmatrix}\n0  \\frac{3}{10}  -\\frac{1}{5}  0 \\\\\n0  0  \\frac{1}{4}  -\\frac{1}{4} \\\\\n0  0  0  \\frac{11}{20} \\\\\n0  0  0  0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{4}{5}  \\frac{3s}{10}  -\\frac{s}{5}  0 \\\\\n0  \\frac{4}{5}  \\frac{s}{4}  -\\frac{s}{4} \\\\\n0  0  \\frac{4}{5}  \\frac{11s}{20} \\\\\n0  0  0  \\frac{4}{5}\n\\end{pmatrix}\n$$\nNext, we apply the Gershgorin Circle Theorem. For a square matrix $A = (A_{ij})$, the theorem states that every eigenvalue lies within the union of the Gershgorin disks $D(A_{ii}, R_i)$ in the complex plane, where the center of the $i$-th disk is the diagonal element $A_{ii}$ and its radius is $R_i = \\sum_{j \\neq i} |A_{ij}|$.\n\nFor our matrix $W(s)$, all diagonal elements are identical: $W_{ii}(s) = \\frac{4}{5}$ for $i=1, 2, 3, 4$. Thus, all Gershgorin disks are centered at the real number $\\frac{4}{5}$. We calculate the radii $R_i$ for each row. Since $s \\ge 0$, we have $|s|=s$.\n-   $R_1 = |\\frac{3s}{10}| + |-\\frac{s}{5}| + |0| = \\frac{3s}{10} + \\frac{s}{5} = \\frac{3s+2s}{10} = \\frac{5s}{10} = \\frac{s}{2}$.\n-   $R_2 = |0| + |\\frac{s}{4}| + |-\\frac{s}{4}| = \\frac{s}{4} + \\frac{s}{4} = \\frac{2s}{4} = \\frac{s}{2}$.\n-   $R_3 = |0| + |0| + |\\frac{11s}{20}| = \\frac{11s}{20}$.\n-   $R_4 = |0| + |0| + |0| = 0$.\n\nThe design requirement states that all Gershgorin disks must lie strictly within the unit circle $|z|=1$. For a disk centered at a positive real number $c$ with radius $R$, this condition is $c+R  1$. In our case, the center is $c = \\frac{4}{5}$. We must satisfy this inequality for the largest radius among $R_1, R_2, R_3, R_4$.\nWe must enforce the condition for each radius:\n1.  For $R_1 = \\frac{s}{2}$: $\\frac{4}{5} + \\frac{s}{2}  1 \\implies \\frac{s}{2}  1 - \\frac{4}{5} \\implies \\frac{s}{2}  \\frac{1}{5} \\implies s  \\frac{2}{5}$.\n2.  For $R_2 = \\frac{s}{2}$: This gives the same condition, $s  \\frac{2}{5}$.\n3.  For $R_3 = \\frac{11s}{20}$: $\\frac{4}{5} + \\frac{11s}{20}  1 \\implies \\frac{11s}{20}  \\frac{1}{5} \\implies s  \\frac{20}{5 \\times 11} \\implies s  \\frac{4}{11}$.\n4.  For $R_4 = 0$: $\\frac{4}{5} + 0  1$, which is always true and places no constraint on $s$.\n\nTo ensure all disks are within the unit circle, we must satisfy all constraints on $s$ simultaneously. This requires $s$ to be smaller than the minimum of the derived upper bounds. We compare the bounds:\n$$\n\\frac{2}{5} = \\frac{22}{55} \\quad \\text{and} \\quad \\frac{4}{11} = \\frac{20}{55}\n$$\nThe most restrictive condition is $s  \\frac{4}{11}$. The problem asks for the largest value of $s$ that satisfies this requirement. This corresponds to the supremum of the interval $[0, \\frac{4}{11})$, which is $s = \\frac{4}{11}$. We select this boundary value for our design.\n\nNow, we validate this choice. We set $s = \\frac{4}{11}$ and examine the matrix $W(\\frac{4}{11})$.\n$$\nW(\\frac{4}{11}) =\n\\begin{pmatrix}\n\\frac{4}{5}  \\frac{3}{10}(\\frac{4}{11})  -\\frac{1}{5}(\\frac{4}{11})  0 \\\\\n0  \\frac{4}{5}  \\frac{1}{4}(\\frac{4}{11})  -\\frac{1}{4}(\\frac{4}{11}) \\\\\n0  0  \\frac{4}{5}  \\frac{11}{20}(\\frac{4}{11}) \\\\\n0  0  0  \\frac{4}{5}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{4}{5}  \\frac{6}{55}  -\\frac{4}{55}  0 \\\\\n0  \\frac{4}{5}  \\frac{1}{11}  -\\frac{1}{11} \\\\\n0  0  \\frac{4}{5}  \\frac{1}{5} \\\\\n0  0  0  \\frac{4}{5}\n\\end{pmatrix}\n$$\nThis matrix is upper triangular. The eigenvalues of a triangular matrix are its diagonal entries. Therefore, all eigenvalues of $W(\\frac{4}{11})$ are $\\lambda = \\frac{4}{5}$.\nThe spectral radius is $\\rho(W) = \\max_i |\\lambda_i| = |\\frac{4}{5}| = \\frac{4}{5}$.\nSince $\\rho(W) = \\frac{4}{5}  1$, the condition for the ESP is satisfied. The Gershgorin-based design is thus successful.\n\nFinally, we argue that the ESP holds for bounded inputs by considering the difference dynamics. The Echo State Property requires that for a given input signal $u_t$, the reservoir state $x_t$ asymptotically forgets its initial state $x_0$.\nConsider two state trajectories, $x_t$ and $x'_t$, starting from different initial states $x_0$ and $x'_0$ respectively, but driven by the same input sequence $u_t$.\n$$\nx_{t+1} = W x_t + U u_t\n$$\n$$\nx'_{t+1} = W x'_t + U u_t\n$$\nLet $\\delta_t = x_t - x'_t$ be the difference between the two states. The dynamics of this difference are given by:\n$$\n\\delta_{t+1} = x_{t+1} - x'_{t+1} = (W x_t + U u_t) - (W x'_t + U u_t) = W(x_t - x'_t) = W \\delta_t\n$$\nThis is an autonomous linear discrete-time system. Its solution is $\\delta_t = W^t \\delta_0$. The state difference $\\delta_t$ will converge to zero as $t \\to \\infty$ for any initial difference $\\delta_0$ if and only if the matrix $W$ is stable, which means all of its eigenvalues have a magnitude strictly less than $1$. This condition is equivalent to requiring the spectral radius $\\rho(W)$ to be strictly less than $1$.\nIf $\\lim_{t \\to \\infty} \\delta_t = 0$, then $\\lim_{t \\to \\infty} (x_t - x'_t) = 0$, which signifies that the two trajectories converge. The asymptotic behavior of the state is thus independent of the initial conditions, depending only on the history of the input signal $u_t$. This is the definition of the Echo State Property.\nOur design yielded $s = \\frac{4}{11}$, for which we calculated the spectral radius to be $\\rho(W(\\frac{4}{11})) = \\frac{4}{5}$. Since $\\frac{4}{5}  1$, the difference dynamics are asymptotically stable, and the ESP is guaranteed.\n\nThe largest value for $s$ is $\\frac{4}{11}$.",
            "answer": "$$\\boxed{\\frac{4}{11}}$$"
        },
        {
            "introduction": "Once a reservoir generates complex state dynamics, a simple linear readout is trained to perform a task. While the reservoir itself is often seen as a \"black box,\" its output can be surprisingly interpretable. This final exercise  delves into the modern field of explainable AI (XAI), applying methods like standardized coefficients and SHAP-like attributions to understand which reservoir features are most important for a given prediction and demystifying the link between the reservoir's internal state and its final output.",
            "id": "4050137",
            "problem": "Consider a discrete-time Echo State Network (ESN) with $N=2$ reservoir units, used as a simplified model of a Liquid State Machine (LSM). The reservoir state $\\mathbf{r}_t \\in \\mathbb{R}^2$ evolves according to the linear recurrence\n$$\n\\mathbf{r}_t = W \\mathbf{r}_{t-1} + U u_t,\n$$\nwith $\\mathbf{r}_0 = \\mathbf{0}$, scalar input $u_t$, reservoir matrix $W = \\mathrm{diag}(\\lambda_1, \\lambda_2)$, and input coupling vector $U = (\\gamma_1, \\gamma_2)^\\top$. The linear readout produces the output\n$$\ny_t = \\mathbf{w}^\\top \\mathbf{r}_t + b,\n$$\nwith readout weight vector $\\mathbf{w} = (w_1, w_2)^\\top$ and bias $b$. Assume the input $u_t$ is a zero-mean, independent and identically distributed (i.i.d.) white-noise process with variance $\\sigma_u^2 = 1$, and the spectral radius condition $|\\lambda_j|  1$ holds for $j \\in \\{1,2\\}$.\n\nThe following numerical parameters are fixed: $\\lambda_1 = \\frac{1}{2}$, $\\lambda_2 = \\frac{1}{3}$, $\\gamma_1 = 1$, $\\gamma_2 = 2$, $\\mathbf{w} = (3,-1)^\\top$, and $b=0$.\n\nYou will investigate readout explainability using two complementary notions:\n\n(1) Standardized Coefficients: Define the standardized coefficient for feature $j$ as $s_j = w_j \\, \\sigma(r_j)/\\sigma(y)$, where $\\sigma(r_j)$ is the standard deviation of $r_{t,j}$ under the stationary distribution induced by the white-noise drive, and $\\sigma(y)$ is the standard deviation of $y_t$ under the same drive. Feature importance is assessed via the magnitude $|s_j|$.\n\n(2) SHapley Additive exPlanations (SHAP): Under a baseline in which missing features are set to their expected values and an independent background (consistent with a white-noise drive and linear reservoir), derive the SHAP-like attribution $\\phi_j(T)$ for feature $j$ to the time-$T$ output $y_T$ in this ESN.\n\nTasks:\n\n(a) Starting from the recurrence and the white-noise assumptions, derive $\\sigma^2(r_1)$, $\\sigma^2(r_2)$, and $\\operatorname{Cov}(r_1,r_2)$, then compute $\\sigma(y)$ from these quantities. Using these, compute $s_1$ and $s_2$ in closed form and determine which reservoir unit $j^\\star \\in \\{1,2\\}$ has the larger absolute standardized coefficient $|s_{j^\\star}|$.\n\n(b) For the finite input sequence with $T=4$ given by $u_1=2$, $u_2=-1$, $u_3=0$, $u_4=3$, derive the SHAP-like attribution $\\phi_j(4)$ for each unit $j$ under the specified baseline and independence assumptions. Report the attribution $\\phi_{j^\\star}(4)$ for the unit $j^\\star$ identified in part (a).\n\nDiscuss briefly any interpretability limits that arise from correlated reservoir states and time-aggregation in reservoir computing.\n\nYour final reported answer must be the single real number $\\phi_{j^\\star}(4)$, rounded to $4$ significant figures. Express the final answer without units.",
            "solution": "The problem statement is analyzed and found to be valid. It is scientifically grounded in the theory of reservoir computing and machine learning interpretability, well-posed with sufficient and consistent information, and formalizable using standard mathematical methods. We may proceed with the solution.\n\nThe problem is divided into two main parts. Part (a) requires the calculation of stationary statistics for the reservoir states to determine feature importance via standardized coefficients. Part (b) requires the calculation of SHAP-like attributions for a specific input sequence.\n\n### Part (a): Stationary Statistics and Standardized Coefficients\n\nThe evolution of the reservoir state components is given by the uncoupled recurrences:\n$$\nr_{t,j} = \\lambda_j r_{t-1, j} + \\gamma_j u_t, \\quad \\text{for } j \\in \\{1,2\\}\n$$\nWith the initial condition $\\mathbf{r}_0 = \\mathbf{0}$, the state at time $t$ can be expressed as a sum over the history of inputs:\n$$\nr_{t,j} = \\sum_{k=1}^t \\lambda_j^{t-k} \\gamma_j u_k\n$$\nSince the spectral radius condition $|\\lambda_j|  1$ is satisfied, for large $t$ the process becomes stationary. The state can be represented as an infinite moving average process:\n$$\nr_{t,j} = \\gamma_j \\sum_{k=0}^{\\infty} \\lambda_j^{k} u_{t-k}\n$$\nThe input $u_t$ is a zero-mean, i.i.d. process with variance $\\sigma_u^2 = 1$. The expected value of any reservoir state component is therefore zero:\n$$\nE[r_{t,j}] = \\gamma_j \\sum_{k=0}^{\\infty} \\lambda_j^{k} E[u_{t-k}] = 0\n$$\n\n**1. Derivation of Variances and Covariance**\n\nThe variance of the $j$-th reservoir state component is:\n$$\n\\sigma^2(r_j) = \\operatorname{Var}(r_{t,j}) = E[r_{t,j}^2] = E\\left[ \\left( \\gamma_j \\sum_{k=0}^{\\infty} \\lambda_j^{k} u_{t-k} \\right)^2 \\right]\n$$\n$$\n\\sigma^2(r_j) = \\gamma_j^2 E\\left[ \\sum_{k=0}^{\\infty} \\sum_{l=0}^{\\infty} \\lambda_j^{k} \\lambda_j^{l} u_{t-k} u_{t-l} \\right]\n$$\nSince the inputs are i.i.d. and zero-mean, $E[u_{t-k}u_{t-l}] = \\sigma_u^2 \\delta_{kl}$, where $\\delta_{kl}$ is the Kronecker delta.\n$$\n\\sigma^2(r_j) = \\gamma_j^2 \\sum_{k=0}^{\\infty} (\\lambda_j^2)^k \\sigma_u^2 = \\frac{\\gamma_j^2 \\sigma_u^2}{1-\\lambda_j^2}\n$$\nThe covariance between the two reservoir state components is:\n$$\n\\operatorname{Cov}(r_1, r_2) = E[r_{t,1} r_{t,2}] = E\\left[ \\left(\\gamma_1 \\sum_{k=0}^{\\infty} \\lambda_1^k u_{t-k}\\right) \\left(\\gamma_2 \\sum_{l=0}^{\\infty} \\lambda_2^l u_{t-l}\\right) \\right]\n$$\n$$\n\\operatorname{Cov}(r_1, r_2) = \\gamma_1 \\gamma_2 E\\left[ \\sum_{k=0}^{\\infty} \\sum_{l=0}^{\\infty} \\lambda_1^k \\lambda_2^l u_{t-k} u_{t-l} \\right] = \\gamma_1 \\gamma_2 \\sigma_u^2 \\sum_{k=0}^{\\infty} (\\lambda_1 \\lambda_2)^k = \\frac{\\gamma_1 \\gamma_2 \\sigma_u^2}{1-\\lambda_1 \\lambda_2}\n$$\n\n**2. Numerical Calculation of Statistics**\n\nUsing the given parameters $\\lambda_1 = \\frac{1}{2}$, $\\lambda_2 = \\frac{1}{3}$, $\\gamma_1 = 1$, $\\gamma_2 = 2$, and $\\sigma_u^2 = 1$:\n$$\n\\sigma^2(r_1) = \\frac{1^2 \\cdot 1}{1 - (\\frac{1}{2})^2} = \\frac{1}{1 - \\frac{1}{4}} = \\frac{1}{\\frac{3}{4}} = \\frac{4}{3}\n$$\n$$\n\\sigma^2(r_2) = \\frac{2^2 \\cdot 1}{1 - (\\frac{1}{3})^2} = \\frac{4}{1 - \\frac{1}{9}} = \\frac{4}{\\frac{8}{9}} = \\frac{36}{8} = \\frac{9}{2}\n$$\n$$\n\\operatorname{Cov}(r_1, r_2) = \\frac{1 \\cdot 2 \\cdot 1}{1 - (\\frac{1}{2})(\\frac{1}{3})} = \\frac{2}{1 - \\frac{1}{6}} = \\frac{2}{\\frac{5}{6}} = \\frac{12}{5}\n$$\nThe output is $y_t = \\mathbf{w}^\\top \\mathbf{r}_t = w_1 r_{t,1} + w_2 r_{t,2}$. The variance of the output is:\n$$\n\\sigma^2(y) = \\operatorname{Var}(y_t) = w_1^2 \\sigma^2(r_1) + w_2^2 \\sigma^2(r_2) + 2 w_1 w_2 \\operatorname{Cov}(r_1, r_2)\n$$\nWith $\\mathbf{w} = (3, -1)^\\top$, so $w_1 = 3$ and $w_2 = -1$:\n$$\n\\sigma^2(y) = 3^2 \\left(\\frac{4}{3}\\right) + (-1)^2 \\left(\\frac{9}{2}\\right) + 2(3)(-1)\\left(\\frac{12}{5}\\right) = 9\\left(\\frac{4}{3}\\right) + \\frac{9}{2} - \\frac{72}{5} = 12 + \\frac{9}{2} - \\frac{72}{5}\n$$\n$$\n\\sigma^2(y) = \\frac{120}{10} + \\frac{45}{10} - \\frac{144}{10} = \\frac{165 - 144}{10} = \\frac{21}{10} = 2.1\n$$\n\n**3. Standardized Coefficients and Feature Importance**\n\nThe standardized coefficient is $s_j = w_j \\sigma(r_j) / \\sigma(y)$. We compare the magnitudes $|s_j|$. It is equivalent to compare $|s_j|^2 = w_j^2 \\sigma^2(r_j) / \\sigma^2(y)$.\n$$\n|s_1|^2 = \\frac{w_1^2 \\sigma^2(r_1)}{\\sigma^2(y)} = \\frac{3^2 \\cdot \\frac{4}{3}}{\\frac{21}{10}} = \\frac{12}{\\frac{21}{10}} = \\frac{120}{21} = \\frac{40}{7}\n$$\n$$\n|s_2|^2 = \\frac{w_2^2 \\sigma^2(r_2)}{\\sigma^2(y)} = \\frac{(-1)^2 \\cdot \\frac{9}{2}}{\\frac{21}{10}} = \\frac{\\frac{9}{2}}{\\frac{21}{10}} = \\frac{9}{2} \\cdot \\frac{10}{21} = \\frac{90}{42} = \\frac{15}{7}\n$$\nSince $\\frac{40}{7}  \\frac{15}{7}$, we have $|s_1|^2  |s_2|^2$, which implies $|s_1|  |s_2|$. Therefore, the reservoir unit with the larger absolute standardized coefficient is $j^\\star = 1$.\n\n### Part (b): SHAP-like Attribution\n\nThe problem asks for the SHAP attribution $\\phi_j(T)$ for feature $j$ (which is $r_{T,j}$) to the output $y_T$. The model is a linear readout $y_T = w_1 r_{T,1} + w_2 r_{T,2}$. The problem specifies a baseline where missing features are set to their expected values ($E[r_j] = 0$) and an \"independent background\". For a linear model, this directive simplifies the SHAP value calculation to:\n$$\n\\phi_j(T) = w_j (r_{T,j} - E[r_{T,j}]) = w_j r_{T,j}\n$$\nWe need to compute the reservoir state $\\mathbf{r}_T$ for $T=4$ given the input sequence $u_1=2$, $u_2=-1$, $u_3=0$, $u_4=3$. We start with $\\mathbf{r}_0 = (0, 0)^\\top$ and iterate the state update equation.\n\nFor unit $j=1$ ($\\lambda_1 = 1/2, \\gamma_1 = 1$):\n- $r_{1,1} = (\\frac{1}{2})r_{0,1} + (1)u_1 = (\\frac{1}{2})(0) + (1)(2) = 2$\n- $r_{2,1} = (\\frac{1}{2})r_{1,1} + (1)u_2 = (\\frac{1}{2})(2) + (1)(-1) = 1 - 1 = 0$\n- $r_{3,1} = (\\frac{1}{2})r_{2,1} + (1)u_3 = (\\frac{1}{2})(0) + (1)(0) = 0$\n- $r_{4,1} = (\\frac{1}{2})r_{3,1} + (1)u_4 = (\\frac{1}{2})(0) + (1)(3) = 3$\n\nFor unit $j=2$ ($\\lambda_2 = 1/3, \\gamma_2 = 2$):\n- $r_{1,2} = (\\frac{1}{3})r_{0,2} + (2)u_1 = (\\frac{1}{3})(0) + (2)(2) = 4$\n- $r_{2,2} = (\\frac{1}{3})r_{1,2} + (2)u_2 = (\\frac{1}{3})(4) + (2)(-1) = \\frac{4}{3} - 2 = -\\frac{2}{3}$\n- $r_{3,2} = (\\frac{1}{3})r_{2,2} + (2)u_3 = (\\frac{1}{3})(-\\frac{2}{3}) + (2)(0) = -\\frac{2}{9}$\n- $r_{4,2} = (\\frac{1}{3})r_{3,2} + (2)u_4 = (\\frac{1}{3})(-\\frac{2}{9}) + (2)(3) = -\\frac{2}{27} + 6 = \\frac{-2+162}{27} = \\frac{160}{27}$\n\nThe reservoir state at $T=4$ is $\\mathbf{r}_4 = (3, 160/27)^\\top$.\nThe SHAP-like attributions are:\n$$\n\\phi_1(4) = w_1 r_{4,1} = (3)(3) = 9\n$$\n$$\n\\phi_2(4) = w_2 r_{4,2} = (-1)\\left(\\frac{160}{27}\\right) = -\\frac{160}{27}\n$$\nThe problem asks for the attribution $\\phi_{j^\\star}(4)$, where $j^\\star=1$. Thus, the required value is $\\phi_1(4) = 9$.\n\n### Discussion of Interpretability Limits\n\n1.  **Correlated Reservoir States**: As shown in part (a), $\\operatorname{Cov}(r_1, r_2) = 12/5 \\neq 0$. The reservoir states are correlated because they are driven by the same input signal $u_t$. Both interpretability methods used have limitations in this context. Standardized coefficients assess the importance of each feature marginally, ignoring joint effects, which can be misleading under multicollinearity. The simplified SHAP formula $\\phi_j=w_j(r_j - E[r_j])$ is derived assuming feature independence. Applying it to correlated features, as dictated by the problem, corresponds to a particular version of SHAP (interventional) that can yield attributions for \"unrealistic\" feature combinations that do not respect the data's correlation structure.\n\n2.  **Time Aggregation**: The reservoir states $r_{t,j}$ are exponentially weighted moving averages of the entire input history. An attribution to $r_{T,j}$ is an attribution to a complex, time-aggregated feature. It explains how this aggregated history, as captured by neuron $j$, contributes to the output $y_T$. However, it does not directly reveal the importance of a specific input $u_k$ at a past time step $k \\le T$. The explanation is at the level of the reservoir's internal representation, not the original input features, thus limiting the interpretability of the model's behavior in terms of its raw inputs.\n\nThe final answer required is $\\phi_{j^\\star}(4)$. Since $j^\\star=1$, this is $\\phi_1(4) = 9$. Rounded to $4$ significant figures, this is $9.000$.",
            "answer": "$$\n\\boxed{9.000}\n$$"
        }
    ]
}