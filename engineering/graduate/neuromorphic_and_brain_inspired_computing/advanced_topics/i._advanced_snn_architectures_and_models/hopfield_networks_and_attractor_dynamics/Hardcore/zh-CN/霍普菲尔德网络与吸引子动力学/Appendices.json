{
    "hands_on_practices": [
        {
            "introduction": "掌握Hopfield网络的关键在于理解其权重如何决定吸引子。本练习将引导你从头开始，为一个小型网络手动构建权重矩阵，以创建特定的稳定状态。通过这个基础性的构建过程，你将具体地体会到赫布学习规则（Hebbian learning rule）如何塑造网络的能量景观，从而形成指定的吸引子。",
            "id": "4047996",
            "problem": "考虑一个具有 $N=4$ 个神经元的全连接二元状态 Hopfield 网络 (HN)，其状态向量为 $s \\in \\{-1,+1\\}^{4}$，对称突触权重矩阵为 $W \\in \\mathbb{R}^{4 \\times 4}$ 且 $W_{ii}=0$，阈值向量为 $\\theta \\in \\mathbb{R}^{4}$。异步更新规则为 $s_{i} \\leftarrow \\operatorname{sgn}\\!\\left(\\sum_{j=1}^{4} W_{ij} s_{j} - \\theta_{i}\\right)$，一个不动点 $s^{\\star}$ 对所有 $i$ 都满足 $s_{i}^{\\star} = \\operatorname{sgn}\\!\\left(\\sum_{j=1}^{4} W_{ij} s_{j}^{\\star} - \\theta_{i}\\right)$。Hopfield 能量函数为\n$$\nE(s) = -\\frac{1}{2} \\sum_{i \\neq j} W_{ij} s_{i} s_{j} + \\sum_{i=1}^{4} \\theta_{i} s_{i}.\n$$\n您的任务是：\n1) 构造一个显式的对称矩阵 $W$（其中 $W_{ii}=0$）和一个阈值向量 $\\theta$，使得该动力学系统在 $\\{-1,+1\\}^{4}$ 上恰好有两个不动点吸引子。请明确给出 $W$ 和 $\\theta$。\n2) 从不动点条件 $s_{i} = \\operatorname{sgn}\\!\\left(\\sum_{j} W_{ij} s_{j} - \\theta_{i}\\right)$ 直接证明，这两个且仅有两个状态是不动点。\n3) 使用上述能量函数，计算任一不动点吸引子与从该吸引子出发经单次自旋翻转可达的能量最低的非不动点构型之间的能量差 $\\Delta E$。若您在构造中引入了正耦合强度 $J$，请用 $J$ 将答案表示为一个闭式解析表达式。无需四舍五入。最终答案必须是单一的无单位闭式表达式。",
            "solution": "这个问题是有效的，因为它科学上基于 Hopfield 网络的标准理论，是适定的、客观的，并包含了一个严谨解所需的所有必要信息。我们将通过构造网络、证明其性质，然后计算所需的能量差来进行解答。\n\n该问题描述了一个由 $N=4$ 个神经元组成的 Hopfield 网络，其状态为 $s_i \\in \\{-1, +1\\}$。异步动力学遵循规则 $s_{i} \\leftarrow \\operatorname{sgn}(\\sum_{j=1}^{4} W_{ij} s_{j} - \\theta_{i})$，其中符号函数 $\\operatorname{sgn}(x)$ 在 $x>0$ 时返回 $+1$，在 $x0$ 时返回 $-1$。问题没有指明 $x=0$ 时的行为，但我们将证明，在我们的构造中，这种情况永远不会发生。一个状态 $s$ 是不动点，如果对所有 $i=1, 2, 3, 4$ 都满足 $s_{i} = \\operatorname{sgn}(\\sum_{j=1}^{4} W_{ij} s_{j} - \\theta_{i})$。当符号函数的参数不为零时，这等价于对所有 $i$ 都成立的稳定性条件 $s_{i} \\left(\\sum_{j=1}^{4} W_{ij} s_{j} - \\theta_{i}\\right) > 0$。\n\n**1) 权重矩阵和阈值向量的构造**\n\n为了创建恰好两个不动点吸引子，我们可以设计网络来存储单个模式。最简单的模式是所有自旋都对齐的模式。令该模式为 $\\xi = (+1, +1, +1, +1)^T$。由于系统在阈值为零时的对称性，我们期望其反向模式 $-\\xi = (-1, -1, -1, -1)^T$ 也是一个吸引子。让我们将其形式化。\n\n我们使用 Hebbian 学习规则为单个存储的模式 $\\xi$ 构造权重矩阵 $W$。我们引入一个正耦合常数 $J > 0$。\n$$\nW_{ij} = \\begin{cases} J \\xi_i \\xi_j  \\text{for } i \\neq j \\\\ 0  \\text{for } i = j \\end{cases}\n$$\n由于对所有 $i$ 都有 $\\xi_i = +1$，我们得到对所有 $i \\neq j$ 有 $W_{ij} = J$ 且 $W_{ii} = 0$。得到的权重矩阵为：\n$$\nW = J \\begin{pmatrix} 0  1  1  1 \\\\ 1  0  1  1 \\\\ 1  1  0  1 \\\\ 1  1  1  0 \\end{pmatrix}\n$$\n该矩阵是对称的且对角线元素为零，符合要求。对于阈值向量，我们做最简单的选择，即 $\\theta = (0, 0, 0, 0)^T$。\n\n因此，我们的显式构造是：对于任何 $J>0$，$W$ 如上所示，且对所有 $i$ 有 $\\theta_{i} = 0$。两个预期的不动点是 $s^{(1)} = (+1, +1, +1, +1)^T$ 和 $s^{(2)} = (-1, -1, -1, -1)^T$。\n\n**2) 恰好有两个不动点的证明**\n\n我们必须证明 $s^{(1)}$ 和 $s^{(2)}$ 是不动点，并且 $\\{-1, +1\\}^4$ 空间中没有其他状态是不动点。神经元 $i$ 的局部场（符号函数的参数）是 $h_i(s) = \\sum_{j=1}^4 W_{ij} s_j - \\theta_i = \\sum_{j \\neq i} J s_j$。不动点条件是 $s_i = \\operatorname{sgn}(h_i(s))$，或者说 $s_i h_i(s) > 0$。\n\n首先，我们来验证两个预期的不动点：\n- 对于 $s^{(1)} = (+1, +1, +1, +1)^T$：\n对于任何神经元 $i$，$s_i^{(1)} = +1$。局部场为 $h_i(s^{(1)}) = \\sum_{j \\neq i} J s_j^{(1)} = J(+1) + J(+1) + J(+1) = 3J$。\n稳定性条件为 $s_i^{(1)} h_i(s^{(1)}) = (+1)(3J) = 3J$。因为 $J>0$，所以 $3J>0$，因此该条件对所有 $i$ 都满足。所以，$s^{(1)}$ 是一个不动点。\n\n- 对于 $s^{(2)} = (-1, -1, -1, -1)^T$：\n对于任何神经元 $i$，$s_i^{(2)} = -1$。局部场为 $h_i(s^{(2)}) = \\sum_{j \\neq i} J s_j^{(2)} = J(-1) + J(-1) + J(-1) = -3J$。\n稳定性条件为 $s_i^{(2)} h_i(s^{(2)}) = (-1)(-3J) = 3J$。因为 $J>0$，所以 $3J>0$，因此该条件对所有 $i$ 都满足。所以，$s^{(2)}$ 是一个不动点。\n\n接下来，我们必须证明剩下的 $2^4 - 2 = 14$ 个状态不是不动点。我们可以根据这些状态包含的 $-1$ 自旋的数量（我们记为 $k$）对它们进行分类。\n- 情况 $k=1$：有 $\\binom{4}{1}=4$ 个这样的状态。根据对称性，它们是等价的。我们分析 $s = (-1, +1, +1, +1)^T$。检查神经元 $i=1$ 的稳定性，其状态为 $s_1 = -1$。\n$h_1(s) = \\sum_{j \\neq 1} J s_j = J(s_2 + s_3 + s_4) = J(+1 + 1 + 1) = 3J$。\n该神经元的稳定性条件是 $s_1 h_1(s) > 0$。\n然而，我们发现 $s_1 h_1(s) = (-1)(3J) = -3J  0$。条件被违反。因此，$s_1$ 是不稳定的，将翻转为 $+1$。该状态不是不动点。根据对称性，任何 $k=1$ 的状态都不是不动点。\n\n- 情况 $k=2$：有 $\\binom{4}{2}=6$ 个这样的状态。我们分析 $s = (-1, -1, +1, +1)^T$。检查神经元 $i=1$ 的稳定性，其状态为 $s_1 = -1$。\n$h_1(s) = \\sum_{j \\neq 1} J s_j = J(s_2 + s_3 + s_4) = J(-1 + 1 + 1) = J$。\n稳定性条件的乘积为 $s_1 h_1(s) = (-1)(J) = -J  0$。条件被违反。神经元 $s_1$ 是不稳定的。该状态不是不动点。根据对称性，任何 $k=2$ 的状态都不是不动点。\n\n- 情况 $k=3$：有 $\\binom{4}{3}=4$ 个这样的状态。这些是 $k=1$ 状态的反向状态。我们分析 $s = (-1, -1, -1, +1)^T$。检查神经元 $i=4$ 的稳定性，其状态为 $s_4 = +1$。\n$h_4(s) = \\sum_{j \\neq 4} J s_j = J(s_1 + s_2 + s_3) = J(-1 - 1 - 1) = -3J$。\n稳定性条件的乘积为 $s_4 h_4(s) = (+1)(-3J) = -3J  0$。条件被违反。神经元 $s_4$ 是不稳定的。该状态不是不动点。根据对称性，任何 $k=3$ 的状态都不是不动点。\n\n我们已经穷尽地检查了所有 $16$ 种可能的状态，并证明了只有 $s^{(1)}$ 和 $s^{(2)}$ 是不动点。对于所有状态，局部场 $h_i$ 都是 $J$ 的非零倍数（具体来说是 $\\pm J$ 或 $\\pm 3J$），因此对于此构造，$\\operatorname{sgn}(0)$ 的模糊性不是一个问题。\n\n**3) 能量差 $\\Delta E$ 的计算**\n\nHopfield 能量由 $E(s) = -\\frac{1}{2} \\sum_{i \\neq j} W_{ij} s_{i} s_{j} + \\sum_{i=1}^{4} \\theta_{i} s_{i}$ 给出。\n在我们的构造中（$i \\neq j$ 时 $\\theta=0$ 且 $W_{ij}=J$），这可以简化为 $E(s) = -\\frac{J}{2} \\sum_{i \\neq j} s_{i} s_{j}$。\n\n首先，我们计算不动点吸引子的能量。根据对称性，它们的能量必然相同。\n对于 $s^{(1)} = (+1, +1, +1, +1)^T$，对所有 $i,j$ 都有 $s_i^{(1)} s_j^{(1)} = (+1)(+1) = 1$。求和 $\\sum_{i \\neq j}$ 中的项数为 $N(N-1) = 4(3) = 12$。\n$$\nE_{\\text{attractor}} = E(s^{(1)}) = -\\frac{J}{2} \\sum_{i \\neq j} (1) = -\\frac{J}{2} \\times 12 = -6J\n$$\n作为检验，对于 $s^{(2)} = (-1, -1, -1, -1)^T$，$s_i^{(2)} s_j^{(2)} = (-1)(-1) = 1$，得出相同的能量 $E(s^{(2)}) = -6J$。\n\n接下来，我们需要计算“从该吸引子出发经单次自旋翻转可达的能量最低的非不动点构型”的能量。考虑一个与 $s^{(1)}$ 相差一次自旋翻转的状态 $s'$。根据对称性，所有这样的状态都是等价的。我们选择 $s' = (-1, +1, +1, +1)^T$。从第 2 部分我们知道这是一个非不动点。\n为了计算其能量 $E(s')$，我们可以使用恒等式 $\\sum_{i \\neq j} s_i s_j = (\\sum_i s_i)^2 - \\sum_i (s_i)^2$。由于 $s_i \\in \\{-1, +1\\}$，我们有 $(s_i)^2=1$，所以 $\\sum_i (s_i)^2 = N=4$。\n对于 $s'$，自旋之和为 $\\sum_i s'_i = -1 + 1 + 1 + 1 = 2$。\n因此，$\\sum_{i \\neq j} s'_{i} s'_{j} = (\\sum_i s'_i)^2 - N = (2)^2 - 4 = 4 - 4 = 0$。\n这个相邻状态的能量是：\n$$\nE_{\\text{neighbor}} = E(s') = -\\frac{J}{2} \\times (0) = 0\n$$\n由于根据对称性，所有单次自旋翻转的邻居都是等价的，它们都具有相同的能量 $0$。因此，这就是从吸引子出发经单次自旋翻转可达的任何（以及所有）能量最低的非不动点构型的能量。\n\n最后，能量差 $\\Delta E$ 是相邻状态能量与吸引子能量之差：\n$$\n\\Delta E = E_{\\text{neighbor}} - E_{\\text{attractor}} = 0 - (-6J) = 6J\n$$\n这个能量差代表了系统要因单个神经元翻转而自发地从吸引子状态跃迁出来所必须克服的能垒。",
            "answer": "$$\n\\boxed{6J}\n$$"
        },
        {
            "introduction": "在构建了网络之后，下一步自然是观察其动态演化过程。这个练习的核心是实现异步更新规则，这是驱动网络状态向其存储的记忆（即吸引子）收敛的核心机制。通过逐步跟踪状态的变化，你将把能量最小化这一抽象概念转化为一个可触摸、可观察的计算过程。",
            "id": "4047998",
            "problem": "考虑一个由二元神经元组成的全连接 Hopfield 网络 (HN)，其状态向量为 $s \\in \\{-1,+1\\}^N$，权重矩阵为 $W \\in \\mathbb{R}^{N \\times N}$，偏置向量为 $\\theta \\in \\mathbb{R}^N$。一次完整的异步扫描会按索引顺序逐个顺序更新神经元，在更新每个神经元之前，使用当前状态计算其局部场。神经元 $i$ 的局部场定义为来自所有其他神经元和偏置的净输入。计算任务是，从一个基本基础上，推导并实现异步更新规则，该规则需保证对于对角线为零的对称矩阵 $W$，Hopfield 能量不会增加，并用它来模拟几个测试用例的一次完整扫描。\n\n基本基础：\n- Hopfield 能量函数为 $s \\in \\{-1,+1\\}^N$ 定义为\n$$\nE(s) = -\\frac{1}{2} s^\\top W s + \\theta^\\top s,\n$$\n其中 $W$ 是对称的，且对于所有 $i \\in \\{1,\\dots,N\\}$，$w_{ii} = 0$。这是一个经过充分检验的目标函数，在适当的局部确定性更新下会减小。\n- 给定当前状态 $s$，神经元 $i$ 的局部场定义为\n$$\nh_i(s) = \\sum_{j=1}^N w_{ij} s_j - \\theta_i.\n$$\n\n在一次异步扫描中，您必须按升序索引 $i=1,2,\\dots,N$ 处理神经元。在每个索引 $i$ 处，根据当前状态计算 $h_i(s)$，并将 $s_i$ 更新为一个不会增加 $E(s)$ 的值。如果 $h_i(s) > 0$，选择 $s_i \\leftarrow +1$；如果 $h_i(s)  0$，选择 $s_i \\leftarrow -1$；如果 $h_i(s) = 0$，则保持 $s_i$ 不变（在对称、零对角线的情况下，此平局规则是保证确定性和能量不增所必需的）。将扫描过程中按计算顺序列出的局部场序列记录为 $\\{h_i\\}_{i=1}^N$。\n\n您必须实现一个程序，对于下面指定的每个测试用例，接收 $(W,\\theta,s)$，执行恰好一次如上所述的完整异步扫描，并输出：\n- 扫描后得到的状态 $s' \\in \\{-1,+1\\}^N$，以及\n- 每步计算出的局部场序列 $\\{h_i\\}_{i=1}^N$（每个 $h_i \\in \\mathbb{R}$）。\n\n所有值都是无单位的（没有物理单位）。不涉及角度。不涉及百分比。\n\n测试套件（每个案例指定 $N$、$W$、$\\theta$ 和 $s$）：\n\n案例 1（一般对称情况，非零偏置）：\n- $N = 4$，\n- $W = \\begin{bmatrix}\n0  1  -1  0 \\\\\n1  0  1  -1 \\\\\n-1  1  0  1 \\\\\n0  -1  1  0\n\\end{bmatrix}$，\n- $\\theta = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.0 \\\\ 0.2 \\end{bmatrix}$，\n- $s = \\begin{bmatrix} +1 \\\\ -1 \\\\ +1 \\\\ -1 \\end{bmatrix}$。\n\n案例 2（局部场等于零的边界平局情况）：\n- $N = 3$，\n- $W = \\begin{bmatrix}\n0  2  -2 \\\\\n2  0  0 \\\\\n-2  0  0\n\\end{bmatrix}$，\n- $\\theta = \\begin{bmatrix} -4 \\\\ 0 \\\\ 0 \\end{bmatrix}$，\n- $s = \\begin{bmatrix} +1 \\\\ -1 \\\\ +1 \\end{bmatrix}$。\n\n案例 3（通过 Hebbian 构造的单个存储模式的稳定吸引子）：\n- $N = 5$，\n- $W = \\begin{bmatrix}\n0  -0.2  0.2  -0.2  0.2 \\\\\n-0.2  0  -0.2  0.2  -0.2 \\\\\n0.2  -0.2  0  -0.2  0.2 \\\\\n-0.2  0.2  -0.2  0  -0.2 \\\\\n0.2  -0.2  0.2  -0.2  0\n\\end{bmatrix}$，\n- $\\theta = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，\n- $s = \\begin{bmatrix} +1 \\\\ -1 \\\\ +1 \\\\ -1 \\\\ +1 \\end{bmatrix}$。\n\n案例 4（双模式 Hebbian 权重，零偏置；稀疏强耦合）：\n- $N = 4$，\n- $W = \\begin{bmatrix}\n0  0  0  -0.5 \\\\\n0  0  -0.5  0 \\\\\n0  -0.5  0  0 \\\\\n-0.5  0  0  0\n\\end{bmatrix}$，\n- $\\theta = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，\n- $s = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ -1 \\end{bmatrix}$。\n\n您的程序应生成单行输出，其中包含所有四个测试用例的结果，聚合为一个用方括号括起来、无空格的逗号分隔列表。每个测试用例的结果必须是一个双元素列表，其第一个元素是结果状态 $s'$，表示为 $\\{-1,+1\\}$ 中的整数列表，第二个元素是局部场列表 $\\{h_i\\}$，表示为浮点数列表。例如，总输出格式必须为\n$$\n[ [s'_1,h^{(1)}], [s'_2,h^{(2)}], [s'_3,h^{(3)}], [s'_4,h^{(4)}] ]\n$$\n但字符串中任何地方都不能有空格，并且每个 $s'_k$ 和 $h^{(k)}$ 都是用方括号和逗号分隔条目表示的列表。每个 $h^{(k)}$ 内的局部场顺序必须与该案例中的神经元更新顺序 $i=1,2,\\dots,N$ 相匹配。",
            "solution": "该问题要求推导并实现一个用于 Hopfield 网络的确定性异步更新规则，该规则保证网络的能量不会增加。该网络由 $N$ 个状态为 $s_i \\in \\{-1, +1\\}$ 的二元神经元、一个对角线为零的对称权重矩阵 $W$（$w_{ij} = w_{ji}$ 且 $w_{ii} = 0$）以及一个偏置向量 $\\theta$ 组成。\n\n首先，我们将从提供的基本定义中推导更新规则。对于状态向量 $s \\in \\{-1, +1\\}^N$，Hopfield 能量函数 $E(s)$ 由下式给出：\n$$\nE(s) = -\\frac{1}{2} s^\\top W s + \\theta^\\top s = -\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N w_{ij} s_i s_j + \\sum_{i=1}^N \\theta_i s_i\n$$\n异步更新规则考虑改变单个神经元（比如神经元 $k$）的状态，从 $s_k$ 变为新值 $s'_k \\in \\{-1, +1\\}$，而所有其他神经元的状态 $s_j$（对于 $j \\neq k$）保持不变。设 $s$ 为更新前的状态向量，$s'$ 为更新后的状态向量。能量变化 $\\Delta E_k$ 为：\n$$\n\\Delta E_k = E(s') - E(s)\n$$\n能量函数中不涉及 $s_k$ 的项将相互抵消。能量变化仅由包含 $s_k$ 的项引起。\n$$\n\\Delta E_k = \\left(-\\frac{1}{2} \\sum_{i,j} w_{ij} s'_i s'_j + \\sum_i \\theta_i s'_i\\right) - \\left(-\\frac{1}{2} \\sum_{i,j} w_{ij} s_i s_j + \\sum_i \\theta_i s_i\\right)\n$$\n偏置项的变化很简单 $\\theta_k s'_k - \\theta_k s_k = \\theta_k (s'_k - s_k)$。\n\n对于二次项，我们分离出涉及索引 $k$ 的分量：\n$$\n\\sum_{i,j} w_{ij} s_i s_j = \\sum_{i\\neq k, j\\neq k} w_{ij} s_i s_j + \\sum_{j\\neq k} w_{kj} s_k s_j + \\sum_{i\\neq k} w_{ik} s_i s_k + w_{kk} s_k^2\n$$\n鉴于 $W$ 是对称的（$w_{ik} = w_{ki}$）并且对角线为零（$w_{kk}=0$），这可以简化为：\n$$\n\\sum_{i,j} w_{ij} s_i s_j = \\sum_{i\\neq k, j\\neq k} w_{ij} s_i s_j + 2 \\sum_{j\\neq k} w_{kj} s_k s_j\n$$\n因此，二次项的变化是：\n$$\n-\\frac{1}{2} \\left[ \\left( \\sum_{i,j} w_{ij} s'_i s'_j \\right) - \\left( \\sum_{i,j} w_{ij} s_i s_j \\right) \\right] = -\\frac{1}{2} \\left[ \\left( 2 \\sum_{j\\neq k} w_{kj} s'_k s_j \\right) - \\left( 2 \\sum_{j\\neq k} w_{kj} s_k s_j \\right) \\right]\n$$\n$$\n= - \\sum_{j\\neq k} w_{kj} s_j (s'_k - s_k)\n$$\n因为 $w_{kk}=0$，所以和 $\\sum_{j \\neq k} w_{kj} s_j$ 等于 $\\sum_{j=1}^N w_{kj} s_j$。结合二次项和偏置项的变化：\n$$\n\\Delta E_k = - (s'_k - s_k) \\sum_{j=1}^N w_{kj} s_j + (s'_k - s_k) \\theta_k\n$$\n$$\n\\Delta E_k = - (s'_k - s_k) \\left( \\sum_{j=1}^N w_{kj} s_j - \\theta_k \\right)\n$$\n问题将神经元 $k$ 的局部场定义为 $h_k(s) = \\sum_{j=1}^N w_{kj} s_j - \\theta_k$。代入此定义，我们得到能量变化的表达式：\n$$\n\\Delta E_k = - (s'_k - s_k) h_k(s)\n$$\n为保证能量不增加，我们必须确保 $\\Delta E_k \\le 0$。这意味着我们必须选择新状态 $s'_k$ 以使得 $(s'_k - s_k) h_k(s) \\ge 0$。我们来分析 $s'_k$ 的条件：\n1.  如果 $h_k(s) > 0$：我们需要 $s'_k - s_k \\ge 0$。由于 $s_k, s'_k \\in \\{-1, +1\\}$，如果 $s_k = +1$，$s'_k$ 必须是 $+1$，导致 $s'_k-s_k=0$。如果 $s_k=-1$，$s'_k$ 必须是 $+1$，导致 $s'_k-s_k=2$。在这两种子情况下，选择 $s'_k = +1$ 都满足条件。\n2.  如果 $h_k(s)  0$：我们需要 $s'_k - s_k \\le 0$。通过类似的推理，选择 $s'_k = -1$ 满足此条件。如果 $s_k=-1$，变化为 $0$。如果 $s_k=+1$，变化为 $-2$。\n3.  如果 $h_k(s) = 0$：无论 $s'_k$ 的值是多少，能量变化 $\\Delta E_k$ 均为 $0$。问题指定了一个确定性的平局打破规则，即保持状态不变，即 $s'_k = s_k$。这导致 $\\Delta E_k = 0$。\n\n结合这些情况，我们得到了更新规则，该规则保证能量不增：\n$$\ns'_k =\n\\begin{cases}\n    +1  \\text{if } h_k(s) > 0 \\\\\n    -1  \\text{if } h_k(s)  0 \\\\\n    s_k  \\text{if } h_k(s) = 0\n\\end{cases}\n$$\n此推导证实了问题中指定的更新规则的有效性和理论合理性。\n\n计算任务是按指定顺序 $k=1, 2, \\ldots, N$ 对神经元执行一次完整的异步扫描。在异步更新中，神经元 $k$ 的状态是基于网络的当前状态进行更新的，该当前状态包括了同一次扫描中所有先前神经元 $1, \\ldots, k-1$ 的更新。这通过维护一个单一的状态向量 $s$ 并对其进行就地修改来实现。对于从 $1$ 到 $N$ 的每个神经元 $k$：\n1.  使用当前状态向量 $s$ 计算局部场 $h_k(s)$。\n2.  记录 $h_k(s)$ 的值。\n3.  根据上面推导的规则更新神经元 $k$ 的状态 $s_k$，从而修改向量 $s$。\n\n将此过程应用于提供的每个测试用例。为每个案例收集最终的状态向量 $s'$ 和计算出的局部场序列 $\\{h_k\\}_{k=1}^N$。实现时需要仔细处理数组索引和就地更新，以正确模拟异步扫描的顺序性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Hopfield network simulation problem for all specified test cases.\n    \"\"\"\n\n    def perform_sweep(W, theta, s_initial):\n        \"\"\"\n        Performs one full asynchronous sweep on a Hopfield network.\n\n        Args:\n            W (np.ndarray): The N x N weight matrix.\n            theta (np.ndarray): The N x 1 bias vector.\n            s_initial (np.ndarray): The N x 1 initial state vector.\n\n        Returns:\n            tuple: A tuple containing:\n                - s_final (list): The final state vector as a list of integers.\n                - local_fields (list): The sequence of computed local fields.\n        \"\"\"\n        s_current = s_initial.copy().astype(float)\n        N = len(s_current)\n        local_fields = []\n\n        for i in range(N):\n            # Compute local field using the current state of the network\n            h_i = np.dot(W[i, :], s_current) - theta[i]\n            local_fields.append(h_i)\n\n            # Apply the update rule\n            if h_i > 0:\n                s_current[i] = 1.0\n            elif h_i  0:\n                s_current[i] = -1.0\n            # If h_i is 0, s_current[i] remains unchanged as per the rule.\n\n        s_final = [int(x) for x in s_current]\n        return s_final, local_fields\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            np.array([\n                [0, 1, -1, 0],\n                [1, 0, 1, -1],\n                [-1, 1, 0, 1],\n                [0, -1, 1, 0]\n            ]),\n            np.array([0.5, -0.5, 0.0, 0.2]),\n            np.array([1, -1, 1, -1])\n        ),\n        # Case 2\n        (\n            np.array([\n                [0, 2, -2],\n                [2, 0, 0],\n                [-2, 0, 0]\n            ]),\n            np.array([-4, 0, 0]),\n            np.array([1, -1, 1])\n        ),\n        # Case 3\n        (\n            np.array([\n                [0, -0.2, 0.2, -0.2, 0.2],\n                [-0.2, 0, -0.2, 0.2, -0.2],\n                [0.2, -0.2, 0, -0.2, 0.2],\n                [-0.2, 0.2, -0.2, 0, -0.2],\n                [0.2, -0.2, 0.2, -0.2, 0]\n            ]),\n            np.array([0, 0, 0, 0, 0]),\n            np.array([1, -1, 1, -1, 1])\n        ),\n        # Case 4\n        (\n            np.array([\n                [0, 0, 0, -0.5],\n                [0, 0, -0.5, 0],\n                [0, -0.5, 0, 0],\n                [-0.5, 0, 0, 0]\n            ]),\n            np.array([0, 0, 0, 0]),\n            np.array([1, 1, 1, -1])\n        ),\n    ]\n\n    results_as_strs = []\n    for W, theta, s_init in test_cases:\n        s_prime, h_fields = perform_sweep(W, theta, s_init)\n        \n        # Format the output strings for s' and h to have no spaces\n        s_str = f\"[{','.join(map(str, s_prime))}]\"\n        h_str = f\"[{','.join(map(str, h_fields))}]\"\n        \n        # Combine them into the format [[s_list],[h_list]]\n        case_result_str = f\"[{s_str},{h_str}]\"\n        results_as_strs.append(case_result_str)\n\n    # Final print statement in the exact required format with no spaces anywhere.\n    print(f\"[{','.join(results_as_strs)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "这个高级练习将带领我们从简单地识别吸引子，深入到量化其稳定性的层面。通过计算稳定性边界（stability margin）并估算吸引盆（basin of attraction）的半径，你将学会如何分析一个记忆模式对噪声或信息损坏的抵抗能力。这直接将网络的理论结构与其作为内容可寻址存储器的实际性能联系起来。",
            "id": "4047971",
            "problem": "考虑一个全连接的 Hopfield 网络，它有 $N=5$ 个二元神经元 $s_{i} \\in \\{-1,+1\\}$，对称权重 $w_{ij}=w_{ji}$ 且 $w_{ii}=0$，没有外部偏置。其异步动态由局部更新规则 $s_{i} \\leftarrow \\mathrm{sign}(h_{i})$ 定义，其中局部场为 $h_{i} = \\sum_{j \\neq i} w_{ij} s_{j}$。Lyapunov（能量）函数为 $E(s) = -\\frac{1}{2} \\sum_{i \\neq j} w_{ij} s_{i} s_{j}$，任何将神经元 $s_{i}$ 翻转到其局部场符号相反方向的异步更新都会严格减小该函数值。权重矩阵为\n$$\nW = \\begin{pmatrix}\n0  3  2  -1  -2 \\\\\n3  0  2  -2  -1 \\\\\n2  2  0  -1  -3 \\\\\n-1  -2  -1  0  3 \\\\\n-2  -1  -3  3  0\n\\end{pmatrix}.\n$$\n假设候选不动点是 $s^{*} = (1,1,1,-1,-1)$。仅从上述定义出发，完成以下任务：\n- 推导局部场 $h_{i}(s^{*})$ 并计算稳定性边界 $m = \\min_{i} s^{*}_{i} h_{i}(s^{*})$。\n- 使用第一性原理推导，说明翻转一组自旋 $S$ 会如何改变每个局部场，从而导出一个充分条件，保证在异步更新下，与 $s^*$ 的汉明距离至多为 $d$ 的每个状态 $s$ 都在 $s^*$ 的吸引盆内。具体地，使用最坏情况下的界限：翻转一个自旋 $j$ 会使 $s^{*}_{i} h_{i}$ 的变化量至多为 $2 |w_{ij}|$，而翻转 $d$ 个自旋会使其变化量至多为第 $i$ 行中 $d$ 个最大 $|w_{ij}|$ 之和的 $2$ 倍。基于此充分条件，计算对于给定的 $W$ 所能保证的最大整数 $d$。\n以精确形式将最终答案表示为数对 $\\left(m, d\\right)$。无需四舍五入，也没有物理单位。您的最终答案必须按规定以一个单行矩阵的形式放在方框内。",
            "solution": "用户要求我解决一个关于 Hopfield 网络的问题。\n\n### 第 1 部分：问题验证\n\n**步骤 1：提取已知条件**\n\n*   网络：全连接的 Hopfield 网络，有 $N=5$ 个二元神经元。\n*   神经元状态：$s_{i} \\in \\{-1, +1\\}$。\n*   权重：对称（$w_{ij}=w_{ji}$），无自连接（$w_{ii}=0$）。\n*   偏置：无外部偏置。\n*   动态：异步更新规则 $s_{i} \\leftarrow \\mathrm{sign}(h_{i})$。\n*   局部场：$h_{i} = \\sum_{j \\neq i} w_{ij} s_{j}$。\n*   Lyapunov 函数：$E(s) = -\\frac{1}{2} \\sum_{i \\neq j} w_{ij} s_{i} s_{j}$。\n*   权重矩阵 $W$：\n    $$\n    W = \\begin{pmatrix}\n    0  3  2  -1  -2 \\\\\n    3  0  2  -2  -1 \\\\\n    2  2  0  -1  -3 \\\\\n    -1  -2  -1  0  3 \\\\\n    -2  -1  -3  3  0\n    \\end{pmatrix}\n    $$\n*   候选不动点：$s^{*} = (1, 1, 1, -1, -1)$。\n*   任务 1：推导局部场 $h_{i}(s^{*})$ 并计算稳定性边界 $m = \\min_{i} s^{*}_{i} h_{i}(s^{*})$。\n*   任务 2：使用局部场变化的最坏情况界限，导出一个充分条件，使得与 $s^*$ 的汉明距离至多为 $d$ 的状态都在其吸引盆内。计算此条件所能保证的最大整数 $d$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n该问题具有科学依据，提法恰当且客观。它描述了一个标准的 Hopfield 网络，这是计算神经科学和物理学中一个成熟的模型。所有定义，包括局部场、更新规则和能量函数，都是标准的。所提供的权重矩阵 $W$ 与描述一致（$5 \\times 5$ 大小，对称，对角线为零）。任务具体、明确，需要基于给定原理进行计算。没有矛盾、歧义或缺失信息。该问题是分析 Hopfield 网络吸引子动力学的一个标准且不平凡的练习。\n\n**步骤 3：结论与行动**\n\n问题被判定为**有效**。我将进行完整解答。\n\n### 第 2 部分：详细解答\n\n问题要求计算两个量：候选不动点 $s^*$ 的稳定性边界 $m$，以及基于特定最坏情况分析所保证的吸引半径 $d$。\n\n**计算稳定性边界 $m$**\n\n首先，我们验证 $s^* = (1, 1, 1, -1, -1)$ 是否为一个不动点（一个稳定状态）。如果对于每个神经元 $i$，其状态 $s_i^*$ 都与其局部场的符号相匹配，即 $s_i^* = \\mathrm{sign}(h_i(s^*))$，则状态 $s^*$ 是一个不动点。在状态 $s^*$ 下，神经元 $i$ 的局部场由 $h_i(s^*) = \\sum_{j \\neq i} w_{ij} s_j^*$ 给出。\n\n让我们计算每个神经元 $i=1, \\dots, 5$ 的局部场：\n*   对于 $i=1$：\n    $h_1(s^*) = w_{12}s_2^* + w_{13}s_3^* + w_{14}s_4^* + w_{15}s_5^* = (3)(1) + (2)(1) + (-1)(-1) + (-2)(-1) = 3 + 2 + 1 + 2 = 8$。\n*   对于 $i=2$：\n    $h_2(s^*) = w_{21}s_1^* + w_{23}s_3^* + w_{24}s_4^* + w_{25}s_5^* = (3)(1) + (2)(1) + (-2)(-1) + (-1)(-1) = 3 + 2 + 2 + 1 = 8$。\n*   对于 $i=3$：\n    $h_3(s^*) = w_{31}s_1^* + w_{32}s_2^* + w_{34}s_4^* + w_{35}s_5^* = (2)(1) + (2)(1) + (-1)(-1) + (-3)(-1) = 2 + 2 + 1 + 3 = 8$。\n*   对于 $i=4$：\n    $h_4(s^*) = w_{41}s_1^* + w_{42}s_2^* + w_{43}s_3^* + w_{45}s_5^* = (-1)(1) + (-2)(1) + (-1)(1) + (3)(-1) = -1 - 2 - 1 - 3 = -7$。\n*   对于 $i=5$：\n    $h_5(s^*) = w_{51}s_1^* + w_{52}s_2^* + w_{53}s_3^* + w_{54}s_4^* = (-2)(1) + (-1)(1) + (-3)(1) + (3)(-1) = -2 - 1 - 3 - 3 = -9$。\n\n现在我们检查稳定性条件 $s_i^* = \\mathrm{sign}(h_i(s^*))$：\n*   $s_1^*=1$, $\\mathrm{sign}(h_1(s^*))=\\mathrm{sign}(8)=1$。正确。\n*   $s_2^*=1$, $\\mathrm{sign}(h_2(s^*))=\\mathrm{sign}(8)=1$。正确。\n*   $s_3^*=1$, $\\mathrm{sign}(h_3(s^*))=\\mathrm{sign}(8)=1$。正确。\n*   $s_4^*=-1$, $\\mathrm{sign}(h_4(s^*))=\\mathrm{sign}(-7)=-1$。正确。\n*   $s_5^*=-1$, $\\mathrm{sign}(h_5(s^*))=\\mathrm{sign}(-9)=-1$。正确。\n由于该条件对所有 $i$ 都成立，因此 $s^*$ 确实是一个不动点。\n\n稳定性边界 $m$ 定义为 $m = \\min_{i} s_i^* h_i(s^*)$。这个量衡量了整个网络中自旋与其局部场的最小“对齐”程度。较大的 $m$ 值表示一个更鲁棒的稳定不动点。\n*   $s_1^*h_1(s^*) = (1)(8) = 8$。\n*   $s_2^*h_2(s^*) = (1)(8) = 8$。\n*   $s_3^*h_3(s^*) = (1)(8) = 8$。\n*   $s_4^*h_4(s^*) = (-1)(-7) = 7$。\n*   $s_5^*h_5(s^*) = (-1)(-9) = 9$。\n\n这些值的最小值是 $m = \\min\\{8, 8, 8, 7, 9\\} = 7$。\n\n**计算保证的吸引半径 $d$**\n\n我们需要找到最大整数 $d$，使得任何与 $s^*$ 的汉明距离 $d_H(s, s^*) \\leq d$ 的状态 $s$ 都保证位于 $s^*$ 的吸引盆内。一个保证这一点的强充分条件是，要求对于任何这样的状态 $s$，对任何神经元 $i$ 的异步更新都会将其状态设置为 $s_i^*$。这确保了任何更新要么保持“正确”的自旋不变，要么将“不正确”的自旋翻转为正确的，从而严格减小与 $s^*$ 的汉明距离，直到距离变为 $0$。\n\n实现这一点的条件是，对于所有 $i \\in \\{1, \\dots, N\\}$ 和所有 $d_H(s, s^*) \\le d$ 的状态 $s$，都有 $\\mathrm{sign}(h_i(s)) = s_i^*$。这等价于要求 $s_i^*h_i(s) > 0$。\n\n设状态 $s$ 在索引集合 $S_{flip}$ 处与 $s^*$ 不同，其中 $|S_{flip}| = d' \\le d$。对于 $j \\in S_{flip}$，我们有 $s_j = -s_j^*$，这意味着 $s_j - s_j^* = -2s_j^*$。对于 $j \\notin S_{flip}$，有 $s_j = s_j^*$。\n\n状态 $s$ 下的局部场是：\n$$\nh_i(s) = \\sum_{j \\neq i} w_{ij}s_j = \\sum_{j \\neq i} w_{ij}s_j^* + \\sum_{j \\neq i} w_{ij}(s_j - s_j^*) = h_i(s^*) - 2\\sum_{j \\in S_{flip}, j \\neq i} w_{ij}s_j^*\n$$\n稳定性条件 $s_i^*h_i(s) > 0$ 变为：\n$$\ns_i^*h_i(s^*) - 2s_i^*\\sum_{j \\in S_{flip}, j \\neq i} w_{ij}s_j^* > 0\n$$\n这个不等式必须对任何大小为 $d' \\le d$ 的集合 $S_{flip}$ 都成立。为保证这一点，我们必须确保初始稳定性 $s_i^*h_i(s^*)$ 大于扰动项 $2s_i^*\\sum_{j \\in S_{flip}, j \\neq i} w_{ij}s_j^*$ 可能造成的最大“损害”。\n\n题目指示我们使用扰动幅度的最坏情况界限。通过翻转一个包含 $d'$ 个自旋的集合 $S_{flip}$， $s_i^*h_i$ 的总变化量是 $s_i^*\\Delta h_i = -2s_i^*\\sum_{j \\in S_{flip}, j \\neq i} w_{ij}s_j^*$。这个变化量的幅度有如下界限：\n$$\n\\left| -2s_i^*\\sum_{j \\in S_{flip}, j \\neq i} w_{ij}s_j^* \\right| \\leq 2\\sum_{j \\in S_{flip}, j \\neq i} |w_{ij}|\n$$\n对于给定的 $d'$，当 $S_{flip}$ 由对应于最大 $|w_{ij}|$ 值的 $d'$ 个索引（$i$ 除外）组成时，会发生最坏情况（最大）的损害。由于这个条件必须对任何 $d' \\le d$ 都成立，我们用可能的最大损害进行检验，这发生在 $d'=d$ 时。\n\n因此，我们将要检查的充分条件是：对于所有 $i \\in \\{1, \\dots, N\\}$，\n$$\ns_i^* h_i(s^*) > 2 \\sum_{k=1}^{d} (\\text{对于 } j \\neq i, |w_{ij}| \\text{ 的第 k 大值})\n$$\n\n让我们对递增的整数 $d$ 值测试这个条件。\n\n**测试 $d=1$：**\n条件是 $s_i^*h_i(s^*) > 2 \\max_{j \\neq i} |w_{ij}|$。我们对每个神经元 $i$ 进行评估：\n*   $i=1$：$s_1^*h_1(s^*) = 8$。$|W|$ 的第一行元素为 $\\{3, 2, 1, 2\\}$，所以 $\\max_j |w_{1j}|=3$。条件：$8 > 2 \\times 3 = 6$。成立。\n*   $i=2$：$s_2^*h_2(s^*) = 8$。$|W|$ 的第二行元素为 $\\{3, 2, 2, 1\\}$，所以 $\\max_j |w_{2j}|=3$。条件：$8 > 2 \\times 3 = 6$。成立。\n*   $i=3$：$s_3^*h_3(s^*) = 8$。$|W|$ 的第三行元素为 $\\{2, 2, 1, 3\\}$，所以 $\\max_j |w_{3j}|=3$。条件：$8 > 2 \\times 3 = 6$。成立。\n*   $i=4$：$s_4^*h_4(s^*) = 7$。$|W|$ 的第四行元素为 $\\{1, 2, 1, 3\\}$，所以 $\\max_j |w_{4j}|=3$。条件：$7 > 2 \\times 3 = 6$。成立。\n*   $i=5$：$s_5^*h_5(s^*) = 9$。$|W|$ 的第五行元素为 $\\{2, 1, 3, 3\\}$，所以 $\\max_j |w_{5j}|=3$。条件：$9 > 2 \\times 3 = 6$。成立。\n由于当 $d=1$ 时该条件对所有 $i$ 都成立，我们可以保证吸引半径为 $d=1$。\n\n**测试 $d=2$：**\n条件是 $s_i^*h_i(s^*) > 2 \\times (\\text{对于 } j \\neq i, \\text{两个最大的 } |w_{ij}| \\text{ 之和})$。我们只需要找到一个不满足此条件的神经元。\n让我们检查 $i=1$：\n*   $s_1^*h_1(s^*) = 8$。对于 $j \\neq 1$，$|w_{1j}|$ 的值是 $\\{3, 2, 1, 2\\}$。两个最大的是 $3$ 和 $2$。\n*   条件是 $8 > 2 \\times (3 + 2) = 10$。这不成立。\n\n由于对于 $d=2$（特别是对于神经元 $i=1$），充分条件不成立，我们无法用此方法保证吸引半径为 $d=2$。因此，在指定条件下所能保证的最大整数 $d$ 是 $1$。\n\n最终的数对是 $(m, d) = (7, 1)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 7  1 \\end{pmatrix}}\n$$"
        }
    ]
}