{
    "hands_on_practices": [
        {
            "introduction": "掌握抽象模型的最佳方式之一是构建一个具体的例子。这项练习将指导您从零开始，使用赫布（Hebbian）学习规则来构建一个虽小但功能齐全的霍普菲尔德网络。通过分析其完整的状态空间，您将对吸引子是什么以及能量景观如何支配网络行为获得一个切实而深刻的理解。",
            "id": "4047996",
            "problem": "考虑一个具有 $N=4$ 个神经元的全连接二元状态Hopfield网络 (HN)，其状态向量为 $s \\in \\{-1,+1\\}^{4}$，对称突触权重矩阵为 $W \\in \\mathbb{R}^{4 \\times 4}$ 且 $W_{ii}=0$，阈值向量为 $\\theta \\in \\mathbb{R}^{4}$。异步更新规则为 $s_{i} \\leftarrow \\operatorname{sgn}\\!\\left(\\sum_{j=1}^{4} W_{ij} s_{j} - \\theta_{i}\\right)$，不动点 $s^{\\star}$ 对所有 $i$ 满足 $s_{i}^{\\star} = \\operatorname{sgn}\\!\\left(\\sum_{j=1}^{4} W_{ij} s_{j}^{\\star} - \\theta_{i}\\right)$。Hopfield能量函数为\n$$\nE(s) = -\\frac{1}{2} \\sum_{i \\neq j} W_{ij} s_{i} s_{j} + \\sum_{i=1}^{4} \\theta_{i} s_{i}.\n$$\n您的任务是：\n1) 构造一个显式的对称矩阵 $W$（其中 $W_{ii}=0$）和一个阈值向量 $\\theta$，使得该动力学系统在 $\\{-1,+1\\}^{4}$ 上恰好有两个不动点吸引子。请明确地给出 $W$ 和 $\\theta$。\n2) 根据不动点条件 $s_{i} = \\operatorname{sgn}\\!\\left(\\sum_{j} W_{ij} s_{j} - \\theta_{i}\\right)$ 直接证明，这两个且仅有两个状态是不动点。\n3) 使用上述能量函数，计算任一不动点吸引子与从该吸引子出发经单自旋翻转可达的能量最低的非不动点构型之间的能量差 $\\Delta E$。如果您的构造中引入了正耦合强度 $J$，请用 $J$ 将答案表示为一个闭式解析表达式。无需四舍五入。最终答案必须是无单位的单个闭式表达式。",
            "solution": "该问题是有效的，因为它在科学上基于Hopfield网络的标准理论，是适定的、客观的，并且包含严格求解所需的所有信息。我们将通过构造网络、证明其性质，然后计算所需的能量差来进行求解。\n\n问题描述了一个包含 $N=4$ 个神经元的Hopfield网络，其状态为 $s_i \\in \\{-1, +1\\}$。异步动力学遵循规则 $s_{i} \\leftarrow \\operatorname{sgn}(\\sum_{j=1}^{4} W_{ij} s_{j} - \\theta_{i})$，其中符号函数 $\\operatorname{sgn}(x)$ 在 $x>0$ 时返回 $+1$，在 $x0$ 时返回 $-1$。问题没有规定 $x=0$ 时的行为，但我们将证明，在我们的构造中，这种情况永远不会发生。一个状态 $s$ 是不动点，如果对所有 $i=1, 2, 3, 4$ 都满足 $s_{i} = \\operatorname{sgn}(\\sum_{j=1}^{4} W_{ij} s_{j} - \\theta_{i})$，这在符号函数的参数不为零的情况下，等价于对所有 $i$ 都满足稳定性条件 $s_{i} \\left(\\sum_{j=1}^{4} W_{ij} s_{j} - \\theta_{i}\\right)  0$。\n\n**1) 权重矩阵和阈值向量的构造**\n\n为了创建恰好两个不动点吸引子，我们可以设计网络来存储单个模式。最简单的模式是所有自旋都对齐的模式。设这个模式为 $\\xi = (+1, +1, +1, +1)^T$。由于系统在零阈值下的对称性，我们预期其负模式 $-\\xi = (-1, -1, -1, -1)^T$ 也是一个吸引子。让我们将其形式化。\n\n我们使用Hebbian学习规则为单个存储的模式 $\\xi$ 构造权重矩阵 $W$。我们引入一个正耦合常数 $J  0$。\n$$\nW_{ij} = \\begin{cases} J \\xi_i \\xi_j  \\text{if } i \\neq j \\\\ 0  \\text{if } i = j \\end{cases}\n$$\n由于对所有 $i$ 都有 $\\xi_i = +1$，我们得到对所有 $i \\neq j$ 都有 $W_{ij} = J$，且 $W_{ii} = 0$。得到的权重矩阵是：\n$$\nW = J \\begin{pmatrix} 0  1  1  1 \\\\ 1  0  1  1 \\\\ 1  1  0  1 \\\\ 1  1  1  0 \\end{pmatrix}\n$$\n该矩阵是对称的且对角线为零，符合要求。对于阈值向量，我们做最简单的选择，即 $\\theta = (0, 0, 0, 0)^T$。\n\n因此，我们的显式构造是：对于任何 $J>0$，$W$ 如上所示，且对所有 $i$ 都有 $\\theta_{i} = 0$。两个预期的不动点是 $s^{(1)} = (+1, +1, +1, +1)^T$ 和 $s^{(2)} = (-1, -1, -1, -1)^T$。\n\n**2) 恰有两个不动点的证明**\n\n我们必须证明 $s^{(1)}$ 和 $s^{(2)}$ 是不动点，并且在 $\\{-1, +1\\}^4$ 空间中没有其他状态是不动点。神经元 $i$ 的局部场（符号函数的参数）是 $h_i(s) = \\sum_{j=1}^4 W_{ij} s_j - \\theta_i = \\sum_{j \\neq i} J s_j$。不动点条件是 $s_i = \\operatorname{sgn}(h_i(s))$，或者说 $s_i h_i(s)  0$。\n\n首先，我们来验证这两个预期的不动点：\n- 对于 $s^{(1)} = (+1, +1, +1, +1)^T$：对任意神经元 $i$，$s_i^{(1)} = +1$。局部场为 $h_i(s^{(1)}) = \\sum_{j \\neq i} J s_j^{(1)} = J(+1) + J(+1) + J(+1) = 3J$。稳定性条件为 $s_i^{(1)} h_i(s^{(1)}) = (+1)(3J) = 3J$。因为 $J>0$，所以 $3J>0$，故对所有 $i$ 该条件都满足。因此，$s^{(1)}$ 是一个不动点。\n\n- 对于 $s^{(2)} = (-1, -1, -1, -1)^T$：对任意神经元 $i$，$s_i^{(2)} = -1$。局部场为 $h_i(s^{(2)}) = \\sum_{j \\neq i} J s_j^{(2)} = J(-1) + J(-1) + J(-1) = -3J$。稳定性条件为 $s_i^{(2)} h_i(s^{(2)}) = (-1)(-3J) = 3J$。因为 $J>0$，所以 $3J>0$，故对所有 $i$ 该条件都满足。因此，$s^{(2)}$ 是一个不动点。\n\n接下来，我们必须证明剩余的 $2^4 - 2 = 14$ 个状态不是不动点。我们可以根据它们包含的 $-1$ 自旋的数量对这些状态进行分类，我们将其记为 $k$。\n- 情况 $k=1$：有 $\\binom{4}{1}=4$ 个这样的状态。根据对称性，它们是等价的。让我们分析 $s = (-1, +1, +1, +1)^T$。我们来检查神经元 $i=1$ 的稳定性，其状态为 $s_1 = -1$。$h_1(s) = \\sum_{j \\neq 1} J s_j = J(s_2 + s_3 + s_4) = J(+1 + 1 + 1) = 3J$。该神经元的稳定性条件是 $s_1 h_1(s)  0$。然而，我们发现 $s_1 h_1(s) = (-1)(3J) = -3J  0$。条件被违反了。因此，$s_1$ 是不稳定的，将翻转为 $+1$。该状态不是不动点。根据对称性，任何 $k=1$ 的状态都不是不动点。\n\n- 情况 $k=2$：有 $\\binom{4}{2}=6$ 个这样的状态。让我们分析 $s = (-1, -1, +1, +1)^T$。我们来检查神经元 $i=1$ 的稳定性，其状态为 $s_1 = -1$。$h_1(s) = \\sum_{j \\neq 1} J s_j = J(s_2 + s_3 + s_4) = J(-1 + 1 + 1) = J$。稳定性条件的乘积为 $s_1 h_1(s) = (-1)(J) = -J  0$。条件被违反了。神经元 $s_1$ 是不稳定的。该状态不是不动点。根据对称性，任何 $k=2$ 的状态都不是不动点。\n\n- 情况 $k=3$：有 $\\binom{4}{3}=4$ 个这样的状态。这些是 $k=1$ 状态的负状态。让我们分析 $s = (-1, -1, -1, +1)^T$。我们来检查神经元 $i=4$ 的稳定性，其状态为 $s_4 = +1$。$h_4(s) = \\sum_{j \\neq 4} J s_j = J(s_1 + s_2 + s_3) = J(-1 - 1 - 1) = -3J$。稳定性条件的乘积为 $s_4 h_4(s) = (+1)(-3J) = -3J  0$。条件被违反了。神经元 $s_4$ 是不稳定的。该状态不是不动点。根据对称性，任何 $k=3$ 的状态都不是不动点。\n\n我们已经穷举检查了所有16种可能的状态，并证明了只有 $s^{(1)}$ 和 $s^{(2)}$ 是不动点。对于所有状态，局部场 $h_i$ 都是 $J$ 的非零倍数（具体来说是 $\\pm J$ 或 $\\pm 3J$），因此 $\\operatorname{sgn}(0)$ 的模糊性对于这个构造不是一个问题。\n\n**3) 能量差 $\\Delta E$ 的计算**\n\nHopfield能量由 $E(s) = -\\frac{1}{2} \\sum_{i \\neq j} W_{ij} s_{i} s_{j} + \\sum_{i=1}^{4} \\theta_{i} s_{i}$ 给出。在我们的构造下（对 $i \\neq j$ 有 $\\theta=0$ 和 $W_{ij}=J$），这简化为 $E(s) = -\\frac{J}{2} \\sum_{i \\neq j} s_{i} s_{j}$。\n\n首先，我们计算不动点吸引子的能量。根据对称性，它们必须具有相同的能量。\n对于 $s^{(1)} = (+1, +1, +1, +1)^T$，对所有 $i,j$ 我们有 $s_i^{(1)} s_j^{(1)} = (+1)(+1) = 1$。求和 $\\sum_{i \\neq j}$ 中的项数是 $N(N-1) = 4(3) = 12$。\n$$\nE_{\\text{attractor}} = E(s^{(1)}) = -\\frac{J}{2} \\sum_{i \\neq j} (1) = -\\frac{J}{2} \\times 12 = -6J\n$$\n作为验证，对于 $s^{(2)} = (-1, -1, -1, -1)^T$，$s_i^{(2)} s_j^{(2)} = (-1)(-1) = 1$，得到相同的能量 $E(s^{(2)}) = -6J$。\n\n接下来，我们需要“从该吸引子出发经单自旋翻转可达的能量最低的非不动点构型”的能量。让我们考虑一个与 $s^{(1)}$ 相差一个单自旋翻转的状态 $s'$。根据对称性，所有这些状态都是等价的。我们选择 $s' = (-1, +1, +1, +1)^T$。从第2部分我们知道这是一个非不动点。\n为了计算其能量 $E(s')$，我们可以使用恒等式 $\\sum_{i \\neq j} s_i s_j = (\\sum_i s_i)^2 - \\sum_i (s_i)^2$。由于 $s_i \\in \\{-1, +1\\}$，我们有 $(s_i)^2=1$，所以 $\\sum_i (s_i)^2 = N=4$。\n对于 $s'$，自旋之和是 $\\sum_i s'_i = -1 + 1 + 1 + 1 = 2$。因此，$\\sum_{i \\neq j} s'_{i} s'_{j} = (\\sum_i s'_i)^2 - N = (2)^2 - 4 = 4 - 4 = 0$。\n这个相邻状态的能量是：\n$$\nE_{\\text{neighbor}} = E(s') = -\\frac{J}{2} \\times (0) = 0\n$$\n由于所有单自旋翻转的邻居根据对称性是等价的，它们都具有相同的能量 $0$。因此，这就是任何（以及所有）经单自旋翻转可达的能量最低的非不动点构型的能量。\n\n最后，能量差 $\\Delta E$ 是相邻状态的能量与吸引子能量之间的差值：\n$$\n\\Delta E = E_{\\text{neighbor}} - E_{\\text{attractor}} = 0 - (-6J) = 6J\n$$\n这个能量差代表了系统因单个神经元翻转而要从一个吸引子状态自发跃迁出来所必须克服的能量势垒。",
            "answer": "$$\n\\boxed{6J}\n$$"
        },
        {
            "introduction": "在通过手动分析掌握了网络的基本构造之后，下一步是将其动态过程转化为计算实现。这项练习要求您编写程序来模拟霍普菲尔德网络的核心动态——异步更新。通过模拟网络状态随时间的逐步演化，您将亲身体验系统是如何朝着吸引子收敛的，这是理解其作为内容可寻址内存功能的关键一步。",
            "id": "4047998",
            "problem": "考虑一个由二元神经元组成的全连接 Hopfield 网络 (HN)，其状态向量为 $s \\in \\{-1,+1\\}^N$，权重矩阵为 $W \\in \\mathbb{R}^{N \\times N}$，偏置向量为 $\\theta \\in \\mathbb{R}^N$。一次完整的异步扫描会按索引顺序，每次一个地依次更新神经元，在更新每个神经元之前，使用当前状态计算其局部场。神经元 $i$ 处的局部场定义为来自所有其他神经元和偏置的净输入。本计算任务要求从基本原理出发，推导并实现一个异步更新规则，该规则在权重矩阵 $W$ 对称且对角线元素为零的情况下，保证不增加 Hopfield 能量。然后，使用此规则为几个测试用例模拟一次完整的扫描。\n\n基本原理：\n- 对于 $s \\in \\{-1,+1\\}^N$，Hopfield 能量函数定义为\n$$\nE(s) = -\\frac{1}{2} s^\\top W s + \\theta^\\top s,\n$$\n其中 $W$ 是对称的，且对于所有 $i \\in \\{1,\\dots,N\\}$ 都有 $w_{ii} = 0$。这是一个经过充分检验的目标函数，在适当的局部确定性更新下，其值会减小。\n- 在给定当前状态 $s$ 时，神经元 $i$ 处的局部场定义为\n$$\nh_i(s) = \\sum_{j=1}^N w_{ij} s_j - \\theta_i.\n$$\n\n在一次异步扫描中，您必须按索引升序 $i=1,2,\\dots,N$ 处理神经元。对于每个索引 $i$，根据当前状态计算 $h_i(s)$，并将 $s_i$ 更新为一个不会增加 $E(s)$ 的值。如果 $h_i(s)  0$，则选择 $s_i \\leftarrow +1$；如果 $h_i(s)  0$，则选择 $s_i \\leftarrow -1$；如果 $h_i(s) = 0$，则保持 $s_i$ 不变（在对称、零对角线的情况下，此平局规则是保证确定性和能量不增加所必需的）。将扫描过程中计算出的局部场序列按计算顺序记录为 $\\{h_i\\}_{i=1}^N$。\n\n您必须实现一个程序，对于下面指定的每个测试用例，该程序接收 $(W,\\theta,s)$，执行恰好一次如上所述的完整异步扫描，并输出：\n- 扫描后的结果状态 $s' \\in \\{-1,+1\\}^N$，以及\n- 在每一步计算出的局部场序列 $\\{h_i\\}_{i=1}^N$（每个 $h_i \\in \\mathbb{R}$）。\n\n所有值都是无单位的（没有物理单位）。不涉及角度。不涉及百分比。\n\n测试套件（每个用例指定 $N$、$W$、$\\theta$ 和 $s$）：\n\n用例 1（一般对称情况，非零偏置）：\n- $N = 4$,\n- $W = \\begin{bmatrix}\n0  1  -1  0 \\\\\n1  0  1  -1 \\\\\n-1  1  0  1 \\\\\n0  -1  1  0\n\\end{bmatrix}$,\n- $\\theta = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.0 \\\\ 0.2 \\end{bmatrix}$,\n- $s = \\begin{bmatrix} +1 \\\\ -1 \\\\ +1 \\\\ -1 \\end{bmatrix}$.\n\n用例 2（局部场等于零的边界平局情况）：\n- $N = 3$,\n- $W = \\begin{bmatrix}\n0  2  -2 \\\\\n2  0  0 \\\\\n-2  0  0\n\\end{bmatrix}$,\n- $\\theta = \\begin{bmatrix} -4 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n- $s = \\begin{bmatrix} +1 \\\\ -1 \\\\ +1 \\end{bmatrix}$.\n\n用例 3（通过 Hebbian 构建的单个存储模式的稳定吸引子）：\n- $N = 5$,\n- $W = \\begin{bmatrix}\n0  -0.2  0.2  -0.2  0.2 \\\\\n-0.2  0  -0.2  0.2  -0.2 \\\\\n0.2  -0.2  0  -0.2  0.2 \\\\\n-0.2  0.2  -0.2  0  -0.2 \\\\\n0.2  -0.2  0.2  -0.2  0\n\\end{bmatrix}$,\n- $\\theta = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n- $s = \\begin{bmatrix} +1 \\\\ -1 \\\\ +1 \\\\ -1 \\\\ +1 \\end{bmatrix}$.\n\n用例 4（双模式 Hebbian 权重，零偏置；稀疏强耦合）：\n- $N = 4$,\n- $W = \\begin{bmatrix}\n0  0  0  -0.5 \\\\\n0  0  -0.5  0 \\\\\n0  -0.5  0  0 \\\\\n-0.5  0  0  0\n\\end{bmatrix}$,\n- $\\theta = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n- $s = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\\\ -1 \\end{bmatrix}$.\n\n您的程序应生成单行输出，其中包含所有四个测试用例的结果，这些结果聚合为一个用方括号括起来的、不含空格的逗号分隔列表。每个测试用例的结果必须是一个双元素列表，其第一个元素是结果状态 $s'$（一个包含 $\\{-1,+1\\}$ 中整数的列表），第二个元素是局部场列表 $\\{h_i\\}$（一个包含浮点数的列表）。例如，总体输出格式必须为\n$$\n[ [s'_1,h^{(1)}], [s'_2,h^{(2)}], [s'_3,h^{(3)}], [s'_4,h^{(4)}] ]\n$$\n但字符串中任何地方都没有空格，并且每个 $s'_k$ 和 $h^{(k)}$ 都是用方括号和逗号分隔条目表示的列表。每个 $h^{(k)}$ 内的局部场顺序必须与该用例内的神经元更新顺序 $i=1,2,\\dots,N$ 相匹配。",
            "solution": "问题要求推导并实现一个用于 Hopfield 网络的确定性异步更新规则，该规则能保证网络能量不增加。该网络由 $N$ 个二元神经元组成，其状态为 $s_i \\in \\{-1, +1\\}$，一个对角线为零的对称权重矩阵 $W$（$w_{ij} = w_{ji}$ 且 $w_{ii} = 0$），以及一个偏置向量 $\\theta$。\n\n首先，我们从提供的基本定义推导更新规则。对于状态向量 $s \\in \\{-1, +1\\}^N$，Hopfield 能量函数 $E(s)$ 由下式给出：\n$$\nE(s) = -\\frac{1}{2} s^\\top W s + \\theta^\\top s = -\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N w_{ij} s_i s_j + \\sum_{i=1}^N \\theta_i s_i\n$$\n异步更新规则考虑改变单个神经元（例如神经元 $k$）的状态，从 $s_k$ 变为新值 $s'_k \\in \\{-1, +1\\}$，而所有其他神经元状态 $s_j$（对于 $j \\neq k$）保持不变。设 $s$ 为更新前的状态向量，$s'$ 为更新后的状态向量。能量的变化量 $\\Delta E_k$ 为：\n$$\n\\Delta E_k = E(s') - E(s)\n$$\n能量函数中不涉及 $s_k$ 的项将被抵消。变化仅由包含 $s_k$ 的项引起。\n$$\n\\Delta E_k = \\left(-\\frac{1}{2} \\sum_{i,j} w_{ij} s'_i s'_j + \\sum_i \\theta_i s'_i\\right) - \\left(-\\frac{1}{2} \\sum_{i,j} w_{ij} s_i s_j + \\sum_i \\theta_i s_i\\right)\n$$\n偏置项的变化很简单，为 $\\theta_k s'_k - \\theta_k s_k = \\theta_k (s'_k - s_k)$。\n\n对于二次项，我们分离出涉及索引 $k$ 的分量：\n$$\n\\sum_{i,j} w_{ij} s_i s_j = \\sum_{i\\neq k, j\\neq k} w_{ij} s_i s_j + \\sum_{j\\neq k} w_{kj} s_k s_j + \\sum_{i\\neq k} w_{ik} s_i s_k + w_{kk} s_k^2\n$$\n鉴于 $W$ 是对称的（$w_{ik} = w_{ki}$）并且对角线为零（$w_{kk}=0$），上式可简化为：\n$$\n\\sum_{i,j} w_{ij} s_i s_j = \\sum_{i\\neq k, j\\neq k} w_{ij} s_i s_j + 2 \\sum_{j\\neq k} w_{kj} s_k s_j\n$$\n因此，二次项的变化为：\n$$\n-\\frac{1}{2} \\left[ \\left( \\sum_{i,j} w_{ij} s'_i s'_j \\right) - \\left( \\sum_{i,j} w_{ij} s_i s_j \\right) \\right] = -\\frac{1}{2} \\left[ \\left( 2 \\sum_{j\\neq k} w_{kj} s'_k s_j \\right) - \\left( 2 \\sum_{j\\neq k} w_{kj} s_k s_j \\right) \\right]\n$$\n$$\n= - \\sum_{j\\neq k} w_{kj} s_j (s'_k - s_k)\n$$\n因为 $w_{kk}=0$，所以求和 $\\sum_{j \\neq k} w_{kj} s_j$ 等于 $\\sum_{j=1}^N w_{kj} s_j$。结合二次项和偏置项的变化：\n$$\n\\Delta E_k = - (s'_k - s_k) \\sum_{j=1}^N w_{kj} s_j + (s'_k - s_k) \\theta_k\n$$\n$$\n\\Delta E_k = - (s'_k - s_k) \\left( \\sum_{j=1}^N w_{kj} s_j - \\theta_k \\right)\n$$\n问题将神经元 $k$ 处的局部场定义为 $h_k(s) = \\sum_{j=1}^N w_{kj} s_j - \\theta_k$。代入此定义，我们得到能量变化的表达式：\n$$\n\\Delta E_k = - (s'_k - s_k) h_k(s)\n$$\n为保证能量不增加，我们必须确保 $\\Delta E_k \\le 0$。这意味着我们必须选择新状态 $s'_k$，使得 $(s'_k - s_k) h_k(s) \\ge 0$。我们来分析一下对 $s'_k$ 的条件：\n1.  如果 $h_k(s)  0$：我们需要 $s'_k - s_k \\ge 0$。由于 $s_k, s'_k \\in \\{-1, +1\\}$，如果 $s_k = +1$，则 $s'_k$ 必须是 $+1$，导致 $s'_k-s_k=0$。如果 $s_k=-1$，则 $s'_k$ 必须是 $+1$，导致 $s'_k-s_k=2$。在这两种子情况下，选择 $s'_k = +1$ 都满足条件。\n2.  如果 $h_k(s)  0$：我们需要 $s'_k - s_k \\le 0$。通过类似推理，选择 $s'_k = -1$ 满足此条件。如果 $s_k=-1$，变化为 $0$。如果 $s_k=+1$，变化为 $-2$。\n3.  如果 $h_k(s) = 0$：无论 $s'_k$ 的值是多少，能量变化 $\\Delta E_k$ 均为 $0$。问题指定了一个确定性的平局打破规则，即保持状态不变，即 $s'_k = s_k$。这导致 $\\Delta E_k = 0$。\n\n综合这些情况，我们得到更新规则，该规则保证能量不增加：\n$$\ns'_k =\n\\begin{cases}\n    +1  \\text{if } h_k(s)  0 \\\\\n    -1  \\text{if } h_k(s)  0 \\\\\n    s_k  \\text{if } h_k(s) = 0\n\\end{cases}\n$$\n此推导证实了问题中指定的更新规则的有效性和理论完备性。\n\n计算任务是按指定顺序 $k=1, 2, \\ldots, N$ 对神经元执行一次完整的异步扫描。在异步更新中，神经元 $k$ 的状态是基于网络的当前状态更新的，该当前状态包括了同一次扫描中所有先前神经元 $1, \\ldots, k-1$ 的更新。这通过维护一个被原地修改的单一状态向量 $s$ 来实现。对于从 $1$ 到 $N$ 的每个神经元 $k$：\n1.  使用当前状态向量 $s$ 计算局部场 $h_k(s)$。\n2.  记录 $h_k(s)$ 的值。\n3.  根据上面推导的规则更新神经元 $k$ 的状态 $s_k$，从而修改向量 $s$。\n\n将此过程应用于提供的每个测试用例。收集每个用例的最终状态向量 $s'$ 和计算出的局部场序列 $\\{h_k\\}_{k=1}^N$。实现时需要仔细处理数组索引和原地更新，以正确模拟异步扫描的顺序性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Hopfield network simulation problem for all specified test cases.\n    \"\"\"\n\n    def perform_sweep(W, theta, s_initial):\n        \"\"\"\n        Performs one full asynchronous sweep on a Hopfield network.\n\n        Args:\n            W (np.ndarray): The N x N weight matrix.\n            theta (np.ndarray): The N x 1 bias vector.\n            s_initial (np.ndarray): The N x 1 initial state vector.\n\n        Returns:\n            tuple: A tuple containing:\n                - s_final (list): The final state vector as a list of integers.\n                - local_fields (list): The sequence of computed local fields.\n        \"\"\"\n        s_current = s_initial.copy().astype(float)\n        N = len(s_current)\n        local_fields = []\n\n        for i in range(N):\n            # Compute local field using the current state of the network\n            h_i = np.dot(W[i, :], s_current) - theta[i]\n            local_fields.append(h_i)\n\n            # Apply the update rule\n            if h_i > 0:\n                s_current[i] = 1.0\n            elif h_i  0:\n                s_current[i] = -1.0\n            # If h_i is 0, s_current[i] remains unchanged as per the rule.\n\n        s_final = [int(x) for x in s_current]\n        return s_final, local_fields\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            np.array([\n                [0, 1, -1, 0],\n                [1, 0, 1, -1],\n                [-1, 1, 0, 1],\n                [0, -1, 1, 0]\n            ]),\n            np.array([0.5, -0.5, 0.0, 0.2]),\n            np.array([1, -1, 1, -1])\n        ),\n        # Case 2\n        (\n            np.array([\n                [0, 2, -2],\n                [2, 0, 0],\n                [-2, 0, 0]\n            ]),\n            np.array([-4, 0, 0]),\n            np.array([1, -1, 1])\n        ),\n        # Case 3\n        (\n            np.array([\n                [0, -0.2, 0.2, -0.2, 0.2],\n                [-0.2, 0, -0.2, 0.2, -0.2],\n                [0.2, -0.2, 0, -0.2, 0.2],\n                [-0.2, 0.2, -0.2, 0, -0.2],\n                [0.2, -0.2, 0.2, -0.2, 0]\n            ]),\n            np.array([0, 0, 0, 0, 0]),\n            np.array([1, -1, 1, -1, 1])\n        ),\n        # Case 4\n        (\n            np.array([\n                [0, 0, 0, -0.5],\n                [0, 0, -0.5, 0],\n                [0, -0.5, 0, 0],\n                [-0.5, 0, 0, 0]\n            ]),\n            np.array([0, 0, 0, 0]),\n            np.array([1, 1, 1, -1])\n        ),\n    ]\n\n    results_as_strs = []\n    for W, theta, s_init in test_cases:\n        s_prime, h_fields = perform_sweep(W, theta, s_init)\n        \n        # Format the output strings for s' and h to have no spaces\n        s_str = f\"[{','.join(map(str, s_prime))}]\"\n        h_str = f\"[{','.join(map(str, h_fields))}]\"\n        \n        # Combine them into the format [[s_list],[h_list]]\n        case_result_str = f\"[{s_str},{h_str}]\"\n        results_as_strs.append(case_result_str)\n\n    # Final print statement in the exact required format with no spaces anywhere.\n    print(f\"[{','.join(results_as_strs)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "模型的行为往往取决于一些看似微小的细节，更新方案就是一个典型的例子。虽然异步更新能保证霍普菲尔德网络收敛到稳定点，但同步更新却可能导致完全不同的结果。这项练习通过一个具体案例，揭示了在同步更新下网络如何陷入振荡而非稳定记忆，从而强调了异步更新规则对于网络实现联想记忆功能的重要性。",
            "id": "4047941",
            "problem": "考虑一个全连接的 Hopfield 网络，它有 $N=2$ 个二元神经元，其状态 $s_{i} \\in \\{-1, +1\\}$，阈值 $\\theta_{i}=0$，以及一个对角线元素为零的对称权重矩阵，由下式给出\n$$\nW=\\begin{pmatrix}\n0  a \\\\\na  0\n\\end{pmatrix},\n$$\n其中 $a=\\frac{7}{4}$。该网络使用同步更新，意味着下一状态 $s(t+1)$ 是通过将 $s_{i}(t+1)=\\operatorname{sign}\\left(\\sum_{j} w_{ij} s_{j}(t)\\right)$ 同时应用于所有神经元得到的，其中如果 $x0$，则 $\\operatorname{sign}(x)=+1$，如果 $x0$，则 $\\operatorname{sign}(x)=-1$；本问题中不会出现 $x=0$ 的情况。\n\n从初始状态 $s(0)=\\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix}$ 开始，根据 Hopfield 网络的基本原理分析同步更新引起的动力学。使用标准的 Hopfield 能量函数\n$$\nE(s)=-\\frac{1}{2} s^{\\top} W s + \\sum_{i} \\theta_{i} s_{i},\n$$\n并在给定条件下，推导出必要的映射关系，以表明轨迹是否进入一个 2-周期环。通过计算环上每个状态的 $E(s)$ 值来明确计算其能量，从而验证振荡。\n\n将在 2-周期环上达到的唯一共同能量值作为您的最终答案。请以精确值的形式表示最终答案，无需四舍五入。",
            "solution": "首先对问题进行验证，以确保其在科学上是合理的、良置的且客观的。\n\n### 步骤 1：提取已知条件\n- 网络大小：$N=2$。\n- 神经元状态：$s_{i} \\in \\{-1, +1\\}$。\n- 神经元阈值：对于 $i=1, 2$，$\\theta_{i}=0$。\n- 权重矩阵：$W=\\begin{pmatrix} 0  a \\\\ a  0 \\end{pmatrix}$，其中 $a=\\frac{7}{4}$。该矩阵是对称的（$w_{ij} = w_{ji}$），且对角线元素为零（$w_{ii}=0$）。\n- 更新规则：同步更新，下一状态 $s(t+1)$ 通过将 $s_{i}(t+1)=\\operatorname{sign}\\left(\\sum_{j} w_{ij} s_{j}(t)\\right)$ 同时应用于所有神经元来确定。\n- 符号函数定义：如果 $x0$，则 $\\operatorname{sign}(x)=+1$；如果 $x0$，则 $\\operatorname{sign}(x)=-1$。题目说明不会出现 $x=0$ 的情况。\n- 初始状态：$s(0)=\\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix}$。\n- 能量函数：$E(s)=-\\frac{1}{2} s^{\\top} W s + \\sum_{i} \\theta_{i} s_{i}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题描述了一个标准的 Hopfield 网络，这是计算神经科学中一个成熟的模型。所有参数都已明确定义，其动力学由标准的确定性规则支配。关于不会出现 $x=0$ 的说辞是可验证的：神经元 $i$ 的符号函数输入为 $h_i = \\sum_j w_{ij} s_j$。对于 $i=1$，$h_1 = w_{12}s_2 = \\frac{7}{4}s_2$。由于 $s_2 \\in \\{-1, +1\\}$，$h_1$ 永远不为零。类似地，$h_2 = w_{21}s_1 = \\frac{7}{4}s_1$，也永远不为零。该问题是自洽的，有科学依据的，且是良置的。\n\n### 步骤 3：结论与行动\n问题被评估为 **有效**。将提供完整的解法。\n\n### 解题推导\nHopfield 网络由 $N=2$ 个神经元组成，其状态为 $s = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix}$，其中 $s_1, s_2 \\in \\{-1, +1\\}$。权重矩阵由下式给出：\n$$\nW = \\begin{pmatrix}\n0  \\frac{7}{4} \\\\\n\\frac{7}{4}  0\n\\end{pmatrix}\n$$\n在时间 $t$ 的状态向量 $s(t)$ 的更新规则是同步的：\n$$\ns(t+1) = \\operatorname{sign}(W s(t))\n$$\n其中符号函数是逐元素应用的。初始状态由下式给出：\n$$\ns(0) = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix}\n$$\n我们从 $s(0)$ 开始分析网络的轨迹。\n\n在时间步 $t=1$，状态 $s(1)$ 的计算如下：\n首先，我们计算激活向量 $h(0) = W s(0)$：\n$$\nh(0) = \\begin{pmatrix} 0  \\frac{7}{4} \\\\ \\frac{7}{4}  0 \\end{pmatrix} \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (0)(+1) + (\\frac{7}{4})(-1) \\\\ (\\frac{7}{4})(+1) + (0)(-1) \\end{pmatrix} = \\begin{pmatrix} -\\frac{7}{4} \\\\ +\\frac{7}{4} \\end{pmatrix}\n$$\n接下来，我们将符号函数应用于 $h(0)$ 的每个分量以得到 $s(1)$：\n$$\ns(1) = \\operatorname{sign}(h(0)) = \\begin{pmatrix} \\operatorname{sign}(-\\frac{7}{4}) \\\\ \\operatorname{sign}(+\\frac{7}{4}) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ +1 \\end{pmatrix}\n$$\n现在，我们使用状态 $s(1)$ 计算时间步 $t=2$ 时的状态：\n$$\nh(1) = W s(1) = \\begin{pmatrix} 0  \\frac{7}{4} \\\\ \\frac{7}{4}  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ +1 \\end{pmatrix} = \\begin{pmatrix} (0)(-1) + (\\frac{7}{4})(+1) \\\\ (\\frac{7}{4})(-1) + (0)(+1) \\end{pmatrix} = \\begin{pmatrix} +\\frac{7}{4} \\\\ -\\frac{7}{4} \\end{pmatrix}\n$$\n将符号函数应用于 $h(1)$ 得到 $s(2)$：\n$$\ns(2) = \\operatorname{sign}(h(1)) = \\begin{pmatrix} \\operatorname{sign}(+\\frac{7}{4}) \\\\ \\operatorname{sign}(-\\frac{7}{4}) \\end{pmatrix} = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix}\n$$\n我们观察到 $s(2) = s(0)$。这证实了网络已进入一个周期为 2 的极限环，即一个 2-周期环。轨迹在以下两个状态之间振荡：\n$$\ns_A = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix} \\quad \\text{和} \\quad s_B = \\begin{pmatrix} -1 \\\\ +1 \\end{pmatrix}\n$$\n映射关系为 $s_A \\to s_B$ 和 $s_B \\to s_A$。\n\n接下来，我们计算这个环中每个状态的 Hopfield 能量。能量函数由下式给出：\n$$\nE(s) = -\\frac{1}{2} s^{\\top} W s + \\sum_{i} \\theta_{i} s_{i}\n$$\n由于所有阈值都为零，$\\theta_i = 0$，能量函数简化为：\n$$\nE(s) = -\\frac{1}{2} s^{\\top} W s\n$$\n对 $N=2$ 的情况展开此式：\n$$\nE(s) = -\\frac{1}{2} \\begin{pmatrix} s_1  s_2 \\end{pmatrix} \\begin{pmatrix} 0  a \\\\ a  0 \\end{pmatrix} \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = -\\frac{1}{2} \\begin{pmatrix} s_1  s_2 \\end{pmatrix} \\begin{pmatrix} a s_2 \\\\ a s_1 \\end{pmatrix} = -\\frac{1}{2} (a s_1 s_2 + a s_2 s_1) = - a s_1 s_2\n$$\n当 $a = \\frac{7}{4}$ 时，该网络的具体能量函数为：\n$$\nE(s_1, s_2) = -\\frac{7}{4} s_1 s_2\n$$\n我们现在计算环中两个状态的能量。\n对于状态 $s_A = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix}$：\n$$\nE(s_A) = E(+1, -1) = -\\frac{7}{4} (+1)(-1) = \\frac{7}{4}\n$$\n对于状态 $s_B = \\begin{pmatrix} -1 \\\\ +1 \\end{pmatrix}$：\n$$\nE(s_B) = E(-1, +1) = -\\frac{7}{4} (-1)(+1) = \\frac{7}{4}\n$$\n在这个 2-周期环中，两个状态的能量是相同的，$E(s_A) = E(s_B) = \\frac{7}{4}$。在环中能量值恒定是同步更新 Hopfield 网络中振荡行为的特征。问题要求的就是这个唯一的共同能量值。",
            "answer": "$$\\boxed{\\frac{7}{4}}$$"
        }
    ]
}