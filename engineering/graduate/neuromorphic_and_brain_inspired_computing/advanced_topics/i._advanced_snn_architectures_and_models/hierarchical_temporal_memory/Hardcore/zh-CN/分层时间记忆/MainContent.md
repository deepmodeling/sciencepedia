## 引言
分层时间记忆（Hierarchical Temporal Memory, HTM）是一个受大脑新皮层结构与功能启发的计算框架，旨在揭示智能的生物学原理并将其应用于机器。作为神经科学和人工智能交叉领域的产物，HTM直面传统[计算模型](@entry_id:637456)在处理[时间[序列数](@entry_id:262935)据](@entry_id:636380)、持续学习和鲁棒性方面遇到的挑战，例如循环神经网络的梯度问题和许多模型中的[灾难性遗忘](@entry_id:636297)现象。它提出了一套独特的解决方案，其核心在于模拟大脑处理信息的方式——通过稀疏的、层次化的和基于预测的机制来理解世界。

本文旨在为读者提供一个关于HTM的全面而深入的指南。我们将分步构建HTM的理论大厦，引领您穿越其复杂的内部世界。
-   在“**原理与机制**”一章中，我们将首先剖析其信息表示的基础——[稀疏分布式表示](@entry_id:1132024)（SDR），然后详细阐述[空间池化器](@entry_id:1132049)如何从输入中学习特征，以及时间记忆如何掌握序列的动态并做出预测。
-   接下来，在“**应用与跨学科联系**”一章中，我们将展示这些理论如何在现实世界中转化为强大的工具，涵盖从数据编码、在线异常检测到[机器人学](@entry_id:150623)中的感觉运动推理等广泛领域。
-   最后，通过“**动手实践**”部分，我们将理论与实践相结合，提供具体的计算练习，帮助您巩固对HTM关键组件的理解。

通过这一结构化的学习路径，您将不仅掌握HTM的核心思想，还将理解其作为新一代智能计算范式的潜力与优势。

## 原理与机制

本章深入探讨分层时间记忆（HTM）的核心原理和基本机制。我们将从其基础数据结构——[稀疏分布式表示](@entry_id:1132024)（SDR）开始，逐步构建[空间池化器](@entry_id:1132049)（Spatial Pooler）和时间记忆（Temporal Memory）这两个关键组件。最后，我们将探讨这些组件如何组合成一个层次化的[预测学](@entry_id:1130218)习系统，并分析其系统级属性，如[持续学习](@entry_id:634283)能力和与其他主流模型的区别。

### [稀疏分布式表示](@entry_id:1132024) (SDR): [神经计算](@entry_id:154058)的基础

HT[M理论](@entry_id:161892)的基石是其信息表示方式，即**[稀疏分布式表示](@entry_id:1132024) (Sparse Distributed Representation, SDR)**。SDR不仅是一种数据结构，更是一种蕴含着深刻计算原理的编码方案。

#### 什么是SDR？

从数学上讲，一个SDR是一个长度为 $n$ 的二元向量，其中恰好有 $k$ 个位为1（称为**激活位**），其余 $n-k$ 个位为0。其核心特征是**[稀疏性](@entry_id:136793)**，即激活位的数量 $k$ 远小于向量的总长度 $n$ ($k \ll n$)。例如，在一个典型的HTM系统中，参数可能是 $n=2048$ 和 $k=40$，稀疏度仅为约2%。

与传统的**密集表示**（dense representation，如深度学习中常见的嵌入向量）相比，SDR的每个位都没有独立的含义。信息的含义（或语义）编码在激活位的特定组合中。这种“分布式”的特性意味着信息的表示是稳健的，单个位的错误不会摧毁整个信息的含义。

SDR的表示空间极其巨大。对于给定的 $n$ 和 $k$，可能存在的SDR总数由[二项式系数](@entry_id:261706) $\binom{n}{k}$ 给出。对于 $n=2048$ 和 $k=40$ 的情况，这个数字是一个天文数字，远超宇宙中的原子总数。这为系统提供了几乎无限的容量来表示海量的信息模式。

#### SDR的语义：重叠与相似性

HTM系统通过比较SDR之间的**重叠 (overlap)** 来度量它们的语义相似性。两个SDR向量 $x$ 和 $y$ 的重叠被定义为它们对应位乘[积之和](@entry_id:266697)，也即它们的点积，它计算了两个向量共同拥有的激活位的数量：
$$S(x, y) = x \cdot y = \sum_{i=1}^{n} x_i y_i$$
重叠值越大，代表两个SDR表示的原始信息在语义上越相似。

重叠与另一个经典的[编码理论](@entry_id:141926)度量——**[汉明距离](@entry_id:157657) (Hamming distance)** 密切相关。[汉明距离](@entry_id:157657) $d_H(x, y)$ 是两个二元向量在对应位置上值不同的位的数量。对于两个权重（激活位数）均为 $k$ 的SDR，[汉明距离](@entry_id:157657)和重叠之间存在一个简单的线性关系：
$$d_H(x, y) = 2(k - S(x, y))$$
这个关系表明，对于固定权重的SDR，最大化重叠等价于最小化[汉明距离](@entry_id:157657)。

#### SDR的鲁棒性

SDR的稀疏性赋予了其两个至关重要的鲁棒性特质：抗噪声能力和低误匹配率。

首先，SDR对噪声具有很强的**抗噪性**。假设一个SDR $x$ 因噪声而被随机翻转了 $r$ 个位，变成了 $x'$。根据定义，$x$ 和 $x'$ 之间的[汉明距离](@entry_id:157657)就是 $r$。我们可以计算 $x$ 和 $x'$ 之[间期](@entry_id:157879)望的重叠值。每次翻转有 $k/n$ 的概率命中一个激活位（将其从1变为0），有 $(n-k)/n$ 的概率命中一个非激活位（将其从0变为1）。因此，平均而言，翻转会减少 $r \cdot (k/n)$ 个重叠位。原始重叠为 $k$（$x$ 与自身的重叠），因此噪声SDR与原始SDR的期望重叠为：
$$E[S(x, x')] = k - r \frac{k}{n} = k \left(1 - \frac{r}{n}\right)$$
以 $n=2048, k=40, r=10$ 为例，期望重叠约为 $39.8$。这个值与原始重叠 $40$ 非常接近，远高于一个有意义的相似性阈值。这表明即使存在少量噪声，SDR仍然能被可靠地识别为其自身。

其次，两个随机选择的、不相关的SDR具有极低的**误匹配率**。它们之间意外产生高重叠的概率微乎其微。这个问题可以精确地用**[超几何分布](@entry_id:193745)**来建模。如果我们固定一个有 $k$ 个激活位的SDR，然后从 $n$ 个位中随机选择 $k$ 个位来构成第二个SDR，那么两个SDR的重叠数 $M$ 服从[超几何分布](@entry_id:193745)。其期望重叠为：
$$E[M] = \frac{k^2}{n}$$
对于 $n=2048, k=40$，期望的随机重叠仅为 $1600 / 2048 \approx 0.78$。要让随机重叠达到一个例如 $\theta=20$ 的阈值，其概率是“天文数字般的小”。  这一特性保证了系统可以非常可靠地区分不同的模式，而不会将它们混淆。

### [空间池化器](@entry_id:1132049) (Spatial Pooler): 从输入到SDR

[空间池化器](@entry_id:1132049)（SP）是HTM的第一个主要处理阶段。它的核心功能是将任意的输入数据（可能是密集的、有噪声的）转换成一个固定的、稀疏的SDR。

#### 结构与机制

SP由一组**微柱 (minicolumns)** 组成，每个微柱可以被看作一个[特征检测](@entry_id:265858)器。每个微柱 $j$ 通过一组**近端树突 (proximal dendrites)** 上的突触与输入空间建立连接。每个突触 $s$ 都有一个**持久性值 (permanence value)** $p_{js} \in [0,1]$，表示其连接的强度。

一个突触被认为是“连接的”，如果其持久性值超过一个固定的**连接阈值** $p_{\text{th}}$。当一个输入向量 $x$呈现给SP时，每个微柱 $j$ 计算其**重叠分数** $o_j(x)$，即其所有连接的、且接收到激活输入的突触数量：
$$o_j(\mathbf{x}) = \sum_{s \in \mathcal{I}_j} c_{js}\, x_s$$
其中 $c_{js} = \mathbf{1}[p_{js} \ge p_{\text{th}}]$ 是一个指示函数，表示突触是否连接。 重叠分数越高的微柱，表示其“[感受野](@entry_id:636171)”与当前输入模式的匹配度越高。

#### 竞争与学习：形成稀疏表征

为了产生稀疏的输出，SP采用了一种基于**抑制 (inhibition)** 的竞争机制。最常见的机制是**k-赢家通吃 (k-Winners-Take-All, k-WTA)**。在这种机制下，只有重叠分数最高的少数微柱会胜出并变为激活状态，而其他微柱则被抑制。

抑制可以是**全局的 (global)** 或**局部的 (local)**。在全局抑制中，所有微柱相互竞争，最终整个网络中只有重叠分数最高的 $k$ 个微柱被激活。这种情况下，输出SDR的[稀疏性](@entry_id:136793)是固定的，激活位数为 $k$。

在局部抑制中，竞争只发生在每个微柱的**抑制邻域 (inhibition neighborhood)** 内。这个邻域通常定义为微柱阵列中一个以该微柱为中心、半径为 $r$ 的圆形区域。每个邻域内都会选出 $k$ 个赢家。局部抑制的优点在于它能更好地保留输入的**拓扑结构 (topography)**。如果输入特征在空间上是相关的，那么将竞争限制在局部可以确保邻近的微柱响应邻近的特征，从而在微柱阵列中形成对输入空间的有序映射。如果抑制半径 $r$ 过大，远处的、不相关的特征响应可能会抑制局部的响应，从而破坏这种拓扑映射。 局部抑制下，网络的整体稀疏度由局部稀疏度 $k/n_r$ 决定，其中 $n_r$ 是一个邻域内的平均微柱数。

SP的学习是一个在线的、无监督的过程。它通过调整突触的持久性值来实现。一个简化的赫布学习规则是：对于激活的微柱，增加那些连接到激活输入位的突触的持久性，同时减少那些连接到非激活输入位的突触的持久性。这使得每个微柱逐渐专精于检测输入空间中的特定模式。

#### 与其他方法的区别

SP的机制与一些其他机器学习算法有本质区别。它不同于 **k-近邻 (k-NN)**，因为SP不是通过计算与存储样本的距离来分类，而是通过学习输入的内在特征结构来生成编码。它也不同于**[字典学习](@entry_id:748389) (dictionary learning)** 或**稀疏编码 (sparse coding)**，因为SP的目标不是用稀疏系数和字典来重建输入，而是通过[竞争性学习](@entry_id:1122716)生成一个具有固定[稀疏性](@entry_id:136793)的SDR表征。

### 时间记忆 (Temporal Memory): 学习与预测序列

时间记忆（TM）是HTM的核心，负责学习时间序列并进行预测。它接收来自SP（或其他TM层）的SDR序列作为输入，并学习这些SDR之间的时间转换关系。

#### 结构：柱内细胞与远端连接

TM的结构比SP更复杂。每个微柱不仅是一个单元，而是包含多个（例如 $C$ 个）**细胞 (cells)**。虽然同一个微柱内的所有细胞共享相同的近端输入（即它们代表相同的瞬时输入特征），但每个细胞可以通过其**远端树突段 (distal dendritic segments)** 学习不同的时间上下文。

这些远端树突段是TM学习时间依赖关系的关键。每个树突段上分布着大量突触，这些突触连接到网络中其他可能在之前时间步激活的细胞。从生物学角度看，HTM的远端树突段和其预测机制是对皮层锥体神经元中**[主动树突](@entry_id:193434) (active dendrites)** 功能的一种抽象建模。在生物神经元中，远端树突上的多个同步输入可以触发局部的**[树突棘波](@entry_id:165333) (dendritic spike, 如NMDA spike)**，这种局部事件使细胞体去极化，进入一种“准备放电”的状态，但这并不直接导致细胞体发放[动作电位](@entry_id:138506)。这与HTM中远端段激活细胞进入“预测状态”的功能高度相似。因此，HTM中的**段[激活阈值](@entry_id:635336)** $\theta$（需要激活的突触数量）应与生物学上的[树突棘波](@entry_id:165333)阈值（如10-20个同步输入）相比较，而不是与细胞体的[动作电位](@entry_id:138506)发放阈值（一个电压值）相比较。

#### 预测与激活机制

TM的运行基于两个核心状态：**预测状态 (predictive state)** 和**激活状态 (active state)**。

在每个时间步 $t$，一个细胞 $c$ 是否进入预测状态取决于其远端树突段的活动。如果一个细胞的任何一个远端树突段 $s$ 在前一时间步 $t-1$ 接收到了足够多的来自当时激活细胞的输入（即激活的突触数量超过了阈值 $\theta$），那么这个树突段就会被激活，从而使细胞 $c$ 在时间步 $t$ 进入预测状态。一个细胞的预测状态 $P(c,t)$ 可以用以下嵌套的[指示函数](@entry_id:186820)精确表示：
$$P(c,t) = \mathbb{1}\left[\sum_{s \in \mathcal{S}(c)} \mathbb{1}\left[\sum_{j \in s} a_{j}(t-1) \ge \theta\right] \ge 1\right]$$
其中 $\mathcal{S}(c)$ 是细胞 $c$ 的远端段集合， $a_j(t-1)$ 是前一时刻突触前细胞 $j$ 的激活状态。

当SP在时间步 $t$ 确定了激活的微[柱集](@entry_id:180956)合后，TM的细胞激活规则如下：
1.  对于一个被激活的微柱，如果它内部有任何细胞处于预测状态，那么**只有**这些处于预测状态的细胞会变为激活状态。
2.  如果一个被激活的微柱内部**没有**任何细胞处于预测状态，这表示一个未被预料到的转换发生了。此时，该微柱会**爆发 (burst)**，即该微柱内的**所有**细胞都暂时变为激活状态。

微柱的爆发是一个关键信号，它既表示了对当前输入的明确表征（因为整个微柱都激活了），也标志着一个**新奇事件 (novelty signal)**，从而触发新的学习。

#### 学习高阶序列

TM通过在每个微柱中使用多个细胞来学习**高阶序列 (higher-order sequences)**。高阶序列是指序列中的下一个元素不仅依赖于当前元素，还依赖于更早的元素（即上下文）。

考虑一个经典的例子：系统被训练学习两个序列，$S_1: A \to B \to A$ 和 $S_2: C \to B \to C$。在这里，元素 $B$ 出现在两种不同的上下文中。一个简单的一阶[马尔可夫模型](@entry_id:899700)无法解决这种模糊性，因为它只能根据当前的 $B$ 来预测下一步。

HTM的TM机制可以优雅地解决这个问题。假设 $A, B, C$ 分别由微柱 $Col_A, Col_B, Col_C$ 表示。
-   当学习 $A \to B$ 时，$Col_B$ 会爆发，其内部的一个细胞（比如 $cell_{B,1}$）会被选中，并在其远端树突上形成连接到 $Col_A$ 激活细胞的突触。
-   当学习 $C \to B$ 时，$Col_B$ 再次爆发（因为来自 $C$ 的上下文是新的），另一个细胞（比如 $cell_{B,2}$）会被选中，并学习连接到 $Col_C$ 的激活细胞。

经过学习后，当序列 $A \to B$ 出现时，来自 $A$ 的激活会使 $cell_{B,1}$ 进入预测状态，因此只有 $cell_{B,1}$ 激活。而当 $C \to B$ 出现时，只有 $cell_{B,2}$ 激活。这样，虽然外部输入都是 $B$，但TM内部的激活细胞是不同的，它们分别编码了“在A之后的B”和“在C之后的B”这两个不同的高阶上下文。这些特定的细胞激活状态接着会产生对下一步（分别是 $A$ 和 $C$）的精确预测。

一个微柱内细胞的数量 $n$ 决定了该微柱能为对应输入符号唯一表示的**上下文容量 (contextual capacity)**。理论上，它最多可以区分 $n$ 种不同的时间上下文。

#### 突触可塑性：学习规则

TM的学习发生在远端树突段上，通过调整突触的持久性值来实现。学习规则是局部的、在线的，并且基于预测的成功与否。
当一个远端段因为预测了后续的细胞活动而被激活时（即 $z(t-1)=1$），其上的突触持久性将按以下规则更新：
-   **正确的预测** (Correct Prediction, $y(t)=1$): 如果该段所在的细胞确实在当前时间步 $t$ 变得活跃，则这是一个成功的预测。系统会奖励那些促成此次预测的突触。具体来说，对于该段上连接到前一时刻 ($t-1$) **激活**突触前细胞的突触，其持久性增加一个量 $\alpha$；而对于连接到**非激活**突触前细胞的突触，其持久性减少一个量 $\beta$。这个过程强化了正确的模式并削弱了无关的连接。
-   **错误的预测** (Incorrect Prediction, $y(t)=0$): 如果该段所在的细胞在时间步 $t$ 没有变得活跃，则这是一个失败的预测。系统会惩罚那些导致这次错误预测的突触。具体来说，对于该段上连接到前一时刻**激活**突触前细胞的突触，其持久性减少一个量 $\beta$。

这些规则可以被形式化为持久性值的[更新方程](@entry_id:264802)。例如，在正确的预测下，突触 $j$ 的持久性更新为 $p_j(t) = \Pi_{[0,1]}(p_j(t-1) + \alpha s_j(t-1) - \beta(1-s_j(t-1)))$，其中 $s_j(t-1)$ 是突触前细胞的激活状态，$\Pi_{[0,1]}$ 是将值裁剪到 $[0,1]$ 区间的操作。

### 层次结构与系统属性

H[TM模](@entry_id:266144)型的核心思想之一是**层次化 (Hierarchy)**。大脑皮层是一个层次化结构，HTM通过堆叠多层SP和TM区域来模拟这种结构，从而学习日益复杂的空间和时间模式。

#### 层次结构：时间池化与[不变性](@entry_id:140168)

在一个多层HTM架构中，低层区域的输出作为高层区域的输入。高层区域通过一个称为**时间池化 (temporal pooling)** 的过程，学习对低层模式序列的**不变性表示 (invariant representation)**。

假设一个高层细胞接收来自低层区域的SDR序列。它不会对每个瞬时SDR做出反应，而是将其输入在一个时间窗口 $W$ 内进行整合。如果在这个窗口内，它所连接的低层微[柱集](@entry_id:180956)合被足够频繁地激活（即总的**池化重叠**超过一个阈值），那么这个高层细胞就会变得活跃。这种机制使得高层细胞的激活状态在一段时间内保持稳定，即使低层的SDR在不断变化。它学会了识别一个完整的序列片段，而不是单个的元素。

为了实现稳定的[不变性](@entry_id:140168)表示并能在不同序列之间实现清晰的转换，时间窗口 $W$ 的选择至关重要。它通常需要小于序列片段的典型持续时间 $D_s$ ($W  D_s$)，这样可以避免窗口跨越序列边界，混淆不同序列的统计特性。通过学习，系统能确保对于一个给定的序列，只有一个（或少数几个）高层细胞的池化重叠能稳定地超过阈值，从而形成一个对该序列的独特、稳定的“名称”。

#### [持续学习](@entry_id:634283)：稳定性与可塑性的权衡

HTM被设计用于**持续学习 (continual learning)**，即在一个连续的数据流中不断学习新知识，同时不忘记旧知识。与传统的深度神经网络相比，HTM在避免**[灾难性遗忘](@entry_id:636297) (catastrophic forgetting)** 方面具有天然优势。这主要归功于其两个特性：[稀疏分布式表示](@entry_id:1132024)和局部学习规则。SDR的稀疏性大大减少了不同模式（任务）表征之间的重叠和干扰。同时，其**局部学习规则**（突触的改变只依赖于突触前后的局部活动）意味着学习新任务 $T_2$ 时，只会影响到与 $T_2$ 相关的少数突触，而不会像反向传播那样对整个网络的权重进行全局性调整，从而保护了为任务 $T_1$ 编码的权重。

然而，HTM并不能完全消除学习中的**[稳定性-可塑性困境](@entry_id:1132257) (stability-plasticity dilemma)**。稳定性指的是保持已有知识的能力，而可塑性指的是学习新知识的能力。这是一个固有的权衡。在HTM中，如果学习率（即持久性更新的步长 $\alpha$ 和 $\beta$）设置得过高，系统将能快速学习新序列，但旧序列的突触连接可能会因为随机波动或[资源竞争](@entry_id:191325)而被削弱或遗忘。反之，如果学习率过低，系统将能很好地保持旧知识，但学习新知识的速度会非常缓慢。因此，HTM并非解决了这个困境，而是提供了一个可以通过调整其参数（如[学习率](@entry_id:140210)、突触数量限制等）来有效管理这一权衡的框架。

#### 与其他序列模型的比较

最后，将HTM与两种广为人知的序列模型——**循环神经网络 (RNNs)** 和 **[隐马尔可夫模型](@entry_id:275059) (HMMs)** 进行比较，可以更好地理解其独特之处。

-   **与RNN的比较**:
    -   **表示**: HTM使用稀疏、二元的SDR；RNN使用密集、实值的隐藏状态向量。
    -   **学习**: HTM使用局部的、在线的、类赫布式的突触持久性更新规则；RNN使用全局的、通常是批处理的**通过时间[反向传播](@entry_id:199535) ([BPTT](@entry_id:633900))** 算法，这可能导致梯度消失或爆炸问题。
    -   **推理**: HTM通过激活特定的细胞集合来形成明确的、可解释的**预测状态**，可以同时表示多个可能的未来；RNN的预测则蕴含在其动态演化的、难以直接解释的[隐藏状态](@entry_id:634361)向量中。

-   **与HMM的比较**:
    -   **表示**: HTM通过多个细胞编码高阶、可变长度的时间上下文；标准的HMM遵循一阶**马尔可夫假设**，即未来只依赖于当前状态。
    -   **学习**: HTM是在线的、增量的学习；HMM通常使用**[期望最大化 (EM)](@entry_id:637213)** 算法进行批处理学习，该算法需要遍历整个数据集来计算期望统计量。
    -   **推理**: HTM的推理是前向的、确定性的[模式匹配](@entry_id:137990)过程；HMM的推理（如计算[后验概率](@entry_id:153467)）则依赖于**[前向-后向算法](@entry_id:194772) (forward-backward algorithm)**。

综上所述，HTM作为一个受大脑新皮层启发的模型，在表示、学习和推理的诸多方面都提供了与主流[机器学习模型](@entry_id:262335)截然不同的机制和原理。