{
    "hands_on_practices": [
        {
            "introduction": "在我们依赖神经采样器进行推断之前，必须首先理解其产生的样本的统计特性。这个练习探讨了神经动力学（如样本之间的时间相关性）如何影响我们对均值和方差等基本统计量的估计。通过从第一性原理推导这些估计量的偏差和方差，您将深入理解为何在解释来自神经形态系统的数据时，必须考虑时间相关性和“神经温度”等非理想特性 。",
            "id": "4052509",
            "problem": "考虑一个由神经形态神经采样器表示的标量潜变量 $x$，其离散时间动态为一个一阶自回归过程 (AR(1))：$$x_{t+1} = \\mu + \\rho\\left(x_t - \\mu\\right) + \\varepsilon_t,$$ 其中 $t \\in \\{0,1,2,\\ldots\\}$，$|\\rho| < 1$，并且新息 $\\varepsilon_t$ 是独立同分布 (i.i.d.) 的高斯变量，其均值为零，方差的选择使得 $x_t$ 的平稳方差为 $$\\gamma_0 = \\sigma^2 T,$$ 其中 $\\mu$ 是目标均值，$\\sigma^2$ 是目标方差，$T$ 是神经温度参数，它用于缩放采样器的平稳方差与目标分布的相对关系。此 AR(1) 过程是严平稳的，其自协方差函数为 $$\\gamma_k = \\operatorname{Cov}(x_t, x_{t+k}) = \\gamma_0 \\rho^k.$$ 您从该平稳过程中观测到 $n$ 个连续样本 $\\{x_1, x_2, \\ldots, x_n\\}$，并使用常规估计量来估计均值和方差：$$\\hat{m} = \\frac{1}{n}\\sum_{t=1}^n x_t, \\quad \\hat{s}^2 = \\frac{1}{n-1}\\sum_{t=1}^n \\left(x_t - \\hat{m}\\right)^2.$$ 从基本原理出发——即期望、方差、协方差和平稳性的定义——在不假设样本之间独立性的前提下，推导以下量关于 $n$、$\\mu$、$\\sigma^2$、$T$ 和 $\\rho$ 的表达式：\n\n1. 样本均值估计量的偏差，定义为 $$\\operatorname{Bias}(\\hat{m}) = \\mathbb{E}[\\hat{m}] - \\mu.$$\n\n2. 样本均值估计量的方差，定义为 $$\\operatorname{Var}(\\hat{m}) = \\mathbb{E}\\left[(\\hat{m} - \\mathbb{E}[\\hat{m}])^2\\right].$$\n\n3. 样本方差估计量的期望值，定义为 $$\\mathbb{E}[\\hat{s}^2].$$\n\n然后，将样本方差解释为忽略神经温度 ($T$) 和时间相关性 ($\\rho$) 的目标方差 $\\sigma^2$ 的一个朴素估计量。使用您推导出的表达式，计算这个朴素方差估计量的偏差：$$\\operatorname{Bias}_{\\text{naive}} = \\mathbb{E}[\\hat{s}^2] - \\sigma^2.$$\n\n您的程序必须实现您推导出的公式，以计算每个测试用例的以下三个输出：\n\n- 样本均值估计量的偏差 $\\operatorname{Bias}(\\hat{m})$。\n- 朴素方差估计量的偏差 $\\operatorname{Bias}_{\\text{naive}}$。\n- 样本均值估计量的方差 $\\operatorname{Var}(\\hat{m})$。\n\n不涉及物理单位。不涉及角度。所有输出均表示为实数（浮点数）。使用以下测试套件，其中每个元组为 $(n, \\mu, \\sigma, T, \\rho)$：\n\n- 测试用例 A（独立性，理想路径）：$(1000, 0.0, 1.0, 1.0, 0.0)$。\n- 测试用例 B（强正相关，大 $n$）：$(1000, 0.0, 2.0, 1.0, 0.95)$。\n- 测试用例 C（负相关，中等 $n$）：$(200, 1.0, 1.5, 1.0, -0.5)$。\n- 测试用例 D（温度不匹配，中等相关性）：$(500, 0.5, 1.0, 1.5, 0.7)$。\n- 测试用例 E（小样本，高相关性）：$(20, -1.0, 1.0, 1.0, 0.9)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按上述指定顺序输出三个浮点数，并将所有测试用例的输出连接成一个单一的扁平列表。例如，输出格式应为 $$\\texttt{[b\\_A^{(m)},b\\_A^{(v)},v\\_A^{(m)},b\\_B^{(m)},b\\_B^{(v)},v\\_B^{(m)},\\ldots]}$$ 其中 $b^{(m)}$ 是均值的偏差，$b^{(v)}$ 是朴素方差估计量的偏差，$v^{(m)}$ 是样本均值的方差。",
            "solution": "用户提供的问题将经过严格的验证程序。\n\n### 第 1 步：提取已知条件\n- **过程动态**：一阶自回归过程 AR(1) 定义为 $x_{t+1} = \\mu + \\rho(x_t - \\mu) + \\varepsilon_t$。\n- **参数和条件**：\n    - 时间索引：$t \\in \\{0,1,2,\\ldots\\}$。\n    - 自回归系数：$|\\rho|  1$。\n    - 新息项：$\\varepsilon_t$ 是独立同分布的高斯变量，$\\mathbb{E}[\\varepsilon_t] = 0$。\n    - 目标均值：$\\mu$。\n    - 目标方差：$\\sigma^2$。\n    - 神经温度：$T$。\n- **平稳性质**：\n    - 过程 $x_t$ 是严平稳的。\n    - 平稳均值：$\\mathbb{E}[x_t] = \\mu$。\n    - 平稳方差：$\\gamma_0 = \\operatorname{Var}(x_t) = \\sigma^2 T$。\n    - 自协方差函数：$\\gamma_k = \\operatorname{Cov}(x_t, x_{t+k}) = \\gamma_0 \\rho^k$，对于 $k \\ge 0$。由此可推断，$\\operatorname{Cov}(x_i, x_j) = \\gamma_0 \\rho^{|i-j|}$。\n- **观测值**：$n$ 个连续样本 $\\{x_1, x_2, \\ldots, x_n\\}$。\n- **估计量**：\n    - 样本均值：$\\hat{m} = \\frac{1}{n}\\sum_{t=1}^n x_t$。\n    - 样本方差：$\\hat{s}^2 = \\frac{1}{n-1}\\sum_{t=1}^n (x_t - \\hat{m})^2$。\n- **待推导的量**：\n    1. 样本均值的偏差：$\\operatorname{Bias}(\\hat{m}) = \\mathbb{E}[\\hat{m}] - \\mu$。\n    2. 样本均值的方差：$\\operatorname{Var}(\\hat{m}) = \\mathbb{E}[(\\hat{m} - \\mathbb{E}[\\hat{m}])^2]$。\n    3. 样本方差的期望值：$\\mathbb{E}[\\hat{s}^2]$。\n    4. 朴素方差估计量的偏差：$\\operatorname{Bias}_{\\text{naive}} = \\mathbb{E}[\\hat{s}^2] - \\sigma^2$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学基础**：该问题在随机过程和统计估计理论中有坚实的基础。AR(1) 过程是时间序列分析中的一个经典模型，其在神经采样中的应用是神经形态计算中公认的范式。所有定义都是标准且正确的。\n- **适定性**：问题是适定的。它提供了推导所需量的所有必要参数和定义。这些推导是数理统计中的标准练习，存在唯一且稳定的解。\n- **客观性**：语言正式、精确，且无主观论断。\n- **缺陷清单**：\n    1. **科学/事实不健全**：无。模型及其性质与既定理论一致。条件 $|\\rho|  1$ 正确地确保了平稳性。\n    2. **不可形式化/不相关**：无。问题是一个形式化的数学任务，直接关系到神经采样器统计特性的表征。\n    3. **不完整/矛盾的设置**：无。所有需要的信息都已提供且内部一致。\n    4. **不切实际/不可行**：无。模型参数和测试用例在物理上和数值上都是合理的。\n    5. **不适定/结构不良**：无。目标陈述清晰，可以得出唯一、可验证的结果。\n    6. **伪深刻/琐碎**：无。虽然均值的偏差是一个简单的结果，但均值的方差，尤其是相关过程样本方差的期望，需要进行不那么琐碎的推导。\n    7. **超出科学可验证性**：无。推导过程在数学上是可验证的。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。它是一个定义明确、科学合理的统计分析问题。将提供一个完整的、有理有据的解决方案。\n\n---\n\n### 从基本原理出发的推导\n\n将基于期望、方差和协方差的基本定义，以及平稳 AR(1) 过程的性质，对每个所要求的量进行推导。平稳均值为 $\\mathbb{E}[x_t] = \\mu$，自协方差为 $\\operatorname{Cov}(x_i, x_j) = \\gamma_0 \\rho^{|i-j|}$，其中 $\\gamma_0 = \\sigma^2 T$。\n\n**1. 样本均值估计量的偏差，$\\operatorname{Bias}(\\hat{m})$**\n\n偏差定义为 $\\operatorname{Bias}(\\hat{m}) = \\mathbb{E}[\\hat{m}] - \\mu$。我们首先计算样本均值 $\\hat{m}$ 的期望值。\n\n根据期望算子的线性性：\n$$\n\\mathbb{E}[\\hat{m}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^n x_t\\right] = \\frac{1}{n}\\sum_{t=1}^n \\mathbb{E}[x_t]\n$$\n由于过程是平稳的，对于所有 $t$，$\\mathbb{E}[x_t] = \\mu$。\n$$\n\\mathbb{E}[\\hat{m}] = \\frac{1}{n}\\sum_{t=1}^n \\mu = \\frac{1}{n}(n\\mu) = \\mu\n$$\n因此，样本均值的偏差是：\n$$\n\\operatorname{Bias}(\\hat{m}) = \\mu - \\mu = 0\n$$\n样本均值 $\\hat{m}$ 是过程均值 $\\mu$ 的无偏估计量，无论时间相关性 $\\rho$ 或神经温度 $T$ 如何。\n\n**2. 样本均值估计量的方差，$\\operatorname{Var}(\\hat{m})$**\n\n$\\hat{m}$ 的方差定义为 $\\operatorname{Var}(\\hat{m}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{t=1}^n x_t\\right)$。\n\n使用相关随机变量和的方差性质：\n$$\n\\operatorname{Var}(\\hat{m}) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{t=1}^n x_t\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(x_i, x_j)\n$$\n代入自协方差函数 $\\operatorname{Cov}(x_i, x_j) = \\gamma_0 \\rho^{|i-j|}$：\n$$\n\\operatorname{Var}(\\hat{m}) = \\frac{\\gamma_0}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\rho^{|i-j|}\n$$\n这个双重求和是托普利茨矩阵的标准结果，可简化为：\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n \\rho^{|i-j|} = n \\frac{1+\\rho}{1-\\rho} - \\frac{2\\rho(1-\\rho^n)}{(1-\\rho)^2}\n$$\n将此结果代入 $\\operatorname{Var}(\\hat{m})$ 的表达式中：\n$$\n\\operatorname{Var}(\\hat{m}) = \\frac{\\gamma_0}{n^2} \\left[ n \\frac{1+\\rho}{1-\\rho} - \\frac{2\\rho(1-\\rho^n)}{(1-\\rho)^2} \\right]\n$$\n提出 $n$ 并代入 $\\gamma_0 = \\sigma^2 T$，我们得到最终公式：\n$$\n\\operatorname{Var}(\\hat{m}) = \\frac{\\sigma^2 T}{n} \\left[ \\frac{1+\\rho}{1-\\rho} - \\frac{2\\rho(1-\\rho^n)}{n(1-\\rho)^2} \\right]\n$$\n注意，在独立样本的情况下（$\\rho=0$），该公式正确地简化为 $\\operatorname{Var}(\\hat{m}) = \\frac{\\sigma^2 T}{n} [1 - 0] = \\frac{\\gamma_0}{n}$。\n\n**3. 样本方差估计量的期望值，$\\mathbb{E}[\\hat{s}^2]$**\n\n我们从 $\\hat{s}^2$ 的定义开始，并使用一个常见的恒等式：\n$$\n\\hat{s}^2 = \\frac{1}{n-1}\\sum_{t=1}^n (x_t - \\hat{m})^2 = \\frac{1}{n-1} \\left( \\sum_{t=1}^n x_t^2 - n\\hat{m}^2 \\right)\n$$\n根据期望的线性性：\n$$\n\\mathbb{E}[\\hat{s}^2] = \\frac{1}{n-1} \\left( \\sum_{t=1}^n \\mathbb{E}[x_t^2] - n\\mathbb{E}[\\hat{m}^2] \\right)\n$$\n我们计算这两个期望。\n首先，根据方差的定义 $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$：\n$\\mathbb{E}[x_t^2] = \\operatorname{Var}(x_t) + (\\mathbb{E}[x_t])^2 = \\gamma_0 + \\mu^2$。\n$\\mathbb{E}[\\hat{m}^2] = \\operatorname{Var}(\\hat{m}) + (\\mathbb{E}[\\hat{m}])^2 = \\operatorname{Var}(\\hat{m}) + \\mu^2$。\n\n将这些代入 $\\mathbb{E}[\\hat{s}^2]$ 的表达式中：\n$$\n\\mathbb{E}[\\hat{s}^2] = \\frac{1}{n-1} \\left( \\sum_{t=1}^n (\\gamma_0 + \\mu^2) - n(\\operatorname{Var}(\\hat{m}) + \\mu^2) \\right)\n$$\n$$\n= \\frac{1}{n-1} \\left( n(\\gamma_0 + \\mu^2) - n\\operatorname{Var}(\\hat{m}) - n\\mu^2 \\right)\n$$\n$$\n= \\frac{1}{n-1} \\left( n\\gamma_0 + n\\mu^2 - n\\operatorname{Var}(\\hat{m}) - n\\mu^2 \\right)\n$$\n$$\n= \\frac{n}{n-1} \\left( \\gamma_0 - \\operatorname{Var}(\\hat{m}) \\right)\n$$\n这个简洁的结果将样本方差的期望与真实过程方差 $\\gamma_0$ 和样本均值的方差 $\\operatorname{Var}(\\hat{m})$ 联系起来。使用 $\\gamma_0 = \\sigma^2 T$ 和先前推导的 $\\operatorname{Var}(\\hat{m})$ 公式，这个量就完全确定了。对于独立同分布样本，$\\operatorname{Var}(\\hat{m}) = \\gamma_0/n$，并且 $\\mathbb{E}[\\hat{s}^2] = \\frac{n}{n-1}(\\gamma_0 - \\gamma_0/n) = \\frac{n}{n-1}(\\frac{n-1}{n}\\gamma_0) = \\gamma_0$，这证实了 $\\hat{s}^2$ 在独立同分布情况下是过程方差的无偏估计量。\n\n**4. 朴素方差估计量的偏差，$\\operatorname{Bias}_{\\text{naive}}$**\n\n问题将目标方差 $\\sigma^2$ 的朴素估计量定义为样本方差 $\\hat{s}^2$，它忽略了神经温度 $T$ 和时间相关性 $\\rho$ 的影响。这个朴素估计量的偏差是：\n$$\n\\operatorname{Bias}_{\\text{naive}} = \\mathbb{E}[\\hat{s}^2] - \\sigma^2\n$$\n代入我们对 $\\mathbb{E}[\\hat{s}^2]$ 的结果：\n$$\n\\operatorname{Bias}_{\\text{naive}} = \\frac{n}{n-1} \\left( \\gamma_0 - \\operatorname{Var}(\\hat{m}) \\right) - \\sigma^2\n$$\n代入 $\\gamma_0 = \\sigma^2 T$：\n$$\n\\operatorname{Bias}_{\\text{naive}} = \\frac{n}{n-1} \\left( \\sigma^2 T - \\operatorname{Var}(\\hat{m}) \\right) - \\sigma^2\n$$\n这个偏差有两个来源。第一个是温度不匹配，它贡献了一个与 $\\sigma^2(T-1)$ 相关的项。第二个是时间相关性，它通过 $\\operatorname{Var}(\\hat{m})$ 项进入。对于正的 $\\rho$，$\\operatorname{Var}(\\hat{m})$ 比独立同分布情况下要大，这会减小 $\\mathbb{E}[\\hat{s}^2]$，从而对偏差贡献一个负分量。对于负的 $\\rho$，情况则相反。\n\n现在将实施这些推导出的公式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes statistical properties of estimators for an AR(1) process\n    based on derived analytical formulas.\n    \"\"\"\n    # Test suite: (n, mu, sigma, T, rho)\n    test_cases = [\n        (1000, 0.0, 1.0, 1.0, 0.0),    # A: independence, happy path\n        (1000, 0.0, 2.0, 1.0, 0.95),   # B: strong positive correlation, large n\n        (200, 1.0, 1.5, 1.0, -0.5),   # C: negative correlation, moderate n\n        (500, 0.5, 1.0, 1.5, 0.7),    # D: temperature mismatch, moderate correlation\n        (20, -1.0, 1.0, 1.0, 0.9),    # E: small sample, high correlation\n    ]\n\n    results = []\n    for case in test_cases:\n        n, mu, sigma, T, rho = case\n        \n        sigma_sq = sigma**2\n        gamma_0 = sigma_sq * T\n\n        # 1. Bias of the sample mean estimator\n        # As derived, E[m_hat] = mu, so the bias is always 0.\n        bias_m = 0.0\n\n        # 2. Variance of the sample mean estimator\n        if n == 0:\n            # Not applicable, but handle for robustness\n            var_m = np.nan\n        elif rho == 0.0:\n            # i.i.d. case\n            var_m = gamma_0 / n\n        else:\n            # Correlated case, |rho|  1\n            if abs(rho) >= 1:\n                # Should not happen based on problem constraints\n                var_m = np.inf\n            else:\n                one_minus_rho = 1.0 - rho\n                # Var(m_hat) = (gamma_0 / n) * [ (1+rho)/(1-rho) - (2*rho*(1-rho^n))/(n*(1-rho)^2) ]\n                term1 = (1.0 + rho) / one_minus_rho\n                term2 = (2.0 * rho * (1.0 - np.power(rho, n))) / (n * np.power(one_minus_rho, 2))\n                var_m = (gamma_0 / n) * (term1 - term2)\n\n        # 3. Expected value of the sample variance estimator\n        if n == 1:\n            # Bessel's correction undefined, but handle for robustness\n            E_s_sq = np.nan\n        else:\n            # E[s^2] = (n / (n-1)) * (gamma_0 - Var(m_hat))\n            E_s_sq = (n / (n - 1.0)) * (gamma_0 - var_m)\n\n        # 4. Bias of the naive variance estimator\n        # Bias_naive = E[s^2] - sigma^2\n        bias_v_naive = E_s_sq - sigma_sq\n\n        # Append results in the specified order\n        results.append(bias_m)\n        results.append(bias_v_naive)\n        results.append(var_m)\n\n    # Format the final output as a single-line string\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理论模型是理想化的，但真实的硬件却存在各种非理想因素。本实践练习聚焦于一个核心的实际任务：校准一个基于硬件的、其响应函数偏离理想模型的尖峰神经元。通过学习如何估计和校正这些由增益和偏移引起的失真，我们将能把神经形态硬件转变为可靠的概率计算工具 。这个过程利用了对数几率（logit）变换和Kullback-Leibler散度等关键技术。",
            "id": "4052528",
            "problem": "给定一个神经形态硬件上的二进制脉冲神经采样器，它在固定的观测窗口内，以一个由输入场的 logistic 函数建模的概率发射脉冲。在一个理想的 logistic 二进制采样器中，脉冲概率作为标量场 $h$ 的函数是 $p^{\\ast}(h) = \\sigma(h)$，其中 $\\sigma(u) = 1/(1 + e^{-u})$ 是 logistic 函数。在实际硬件上，会发生未知的增益和偏移失真，因此采样器实际上实现的是 $p(h) = \\sigma(\\alpha h + \\gamma)$，其中 $\\alpha$ 和 $\\gamma$ 是未知的实数，可能会随时间或配置而漂移。目标是通过从测量的训练响应中估计 $(\\alpha, \\gamma)$ 来校准采样器，然后通过量化其与理想采样器的散度，在留出的测试集上验证校准后的模型。\n\n基本原理：\n- 一个具有脉冲概率 $p$ 的二进制随机变量 $x \\in \\{0,1\\}$ 服从伯努利分布 $\\mathsf{Bernoulli}(p)$。\n- logistic 链接定义为 $\\sigma(u) = 1/(1+e^{-u})$，适用于任何实数 $u$。\n- 对数几率 (logit) 函数为 $\\operatorname{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$，适用于任何 $p \\in (0,1)$。\n- 从 $\\mathsf{Bernoulli}(p)$ 到 $\\mathsf{Bernoulli}(q)$ 的 Kullback–Leibler 散度 (KLD) 是 $\\mathrm{KL}(p \\Vert q) = p \\ln\\left(\\frac{p}{q}\\right) + (1-p)\\ln\\left(\\frac{1-p}{1-q}\\right)$。\n\n校准问题：\n- 你将获得训练测量数据，包括一小组场 $h_i$，以及对于每个 $h_i$，在 $n_i$ 次试验中观测到的脉冲数 $k_i$。使用参数为 $\\delta$ 的 Laplace 平滑来避免接近 $0$ 和 $1$ 的退化概率，定义经验概率估计 $\\hat{p}_i = \\frac{k_i + \\delta}{n_i + 2\\delta}$。假设硬件遵循 logistic 模型 $p(h) = \\sigma(\\alpha h + \\gamma)$。在 logit 域中，这意味着 $\\operatorname{logit}(\\hat{p}_i) \\approx \\alpha h_i + \\gamma$ 加上采样噪声。通过在 logit 域中对训练数据进行线性最小二乘法来估计 $(\\alpha, \\gamma)$。\n\n验证问题：\n- 给定留出的测试场 $h_j$ 和在 $n_j$ 次试验中测得的脉冲计数 $k_j$（使用相同的平滑处理），计算两个验证指标：\n  1. 理想目标概率 $p^{\\ast}(h_j) = \\sigma(h_j)$ 与校准模型的预测概率 $q(h_j) = \\sigma(\\alpha h_j + \\gamma)$ 之间的平均 Kullback–Leibler 散度：\n  $$\\overline{\\mathrm{KL}} = \\frac{1}{m}\\sum_{j=1}^{m}\\left[p^{\\ast}(h_j)\\ln\\left(\\frac{p^{\\ast}(h_j)}{q(h_j)}\\right) + (1-p^{\\ast}(h_j))\\ln\\left(\\frac{1-p^{\\ast}(h_j)}{1-q(h_j)}\\right)\\right],$$\n  其中 $m$ 是测试场的数量。\n  2. 测试测量上的最大绝对 logit 残差，定义为：\n  $$R_{\\max} = \\max_{j}\\left|\\operatorname{logit}\\!\\left(\\frac{k_j+\\delta}{n_j+2\\delta}\\right) - (\\alpha h_j + \\gamma)\\right|.$$\n\n你的任务是编写一个完整的、可运行的程序，为每个提供的测试用例执行校准和验证。该程序必须：\n- 在 logit 域中实现 logistic $\\sigma(u)$、logit、Laplace 平滑和线性最小二乘法。\n- 对于每个测试用例，在训练集上估计 $(\\alpha, \\gamma)$，在测试集上使用校准模型与理想目标计算 $\\overline{\\mathrm{KL}}$，并使用测量的测试计数计算 $R_{\\max}$。\n- 通过检查 $\\overline{\\mathrm{KL}}$ 是否小于或等于特定于用例的容差 $\\tau$ 来决定每个用例的通过/失败。\n\n测试套件：\n- 提供了三个测试用例。每个用例都提供 $h_{\\text{train}}$、$k_{\\text{train}}$、$n_{\\text{train}}$、$h_{\\text{test}}$、$k_{\\text{test}}$、$n_{\\text{test}}$ 和一个容差 $\\tau$，所有用例中使用的平滑参数 $\\delta = 1$。所有 $h$ 值均为纯实数（无物理单位）。测试套件涵盖了一个典型的条件良好的情景、一个饱和边界情况和一个漂移/噪声情况。\n\n用例 A（条件良好）：\n- $h_{\\text{train}} = [-2.0, 0.0, 2.0]$\n- $n_{\\text{train}} = [1000, 1000, 1000]$\n- $k_{\\text{train}} = [154, 525, 870]$\n- $h_{\\text{test}} = [-3.0, -1.0, 1.0, 3.0]$\n- $n_{\\text{test}} = [1000, 1000, 1000, 1000]$\n- $k_{\\text{test}} = [69, 310, 731, 943]$\n- $\\tau = 0.01$\n\n用例 B（饱和边界）：\n- $h_{\\text{train}} = [-1.0, 0.0, 1.0]$\n- $n_{\\text{train}} = [200, 200, 200]$\n- $k_{\\text{train}} = [4, 100, 196]$\n- $h_{\\text{test}} = [-2.0, -0.5, 0.5, 2.0]$\n- $n_{\\text{test}} = [200, 200, 200, 200]$\n- $k_{\\text{test}} = [0, 24, 176, 200]$\n- $\\tau = 0.05$\n\n用例 C（漂移/噪声情况）：\n- $h_{\\text{train}} = [-1.0, 0.0, 1.0]$\n- $n_{\\text{train}} = [500, 500, 500]$\n- $k_{\\text{train}} = [99, 225, 366]$\n- $h_{\\text{test}} = [-1.0, 0.0, 1.0, 2.0]$\n- $n_{\\text{test}} = [500, 500, 500, 500]$\n- $k_{\\text{test}} = [60, 189, 366, 462]$\n- $\\tau = 0.03$\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含三个用例的结果，形式为一个用方括号括起来的逗号分隔列表，每个用例的结果本身是一个包含四个值的列表 $[\\alpha, \\gamma, \\overline{\\mathrm{KL}}, \\text{pass}]$。例如：$[[\\alpha_1,\\gamma_1,\\mathrm{KL}_1,\\text{True}],[\\alpha_2,\\gamma_2,\\mathrm{KL}_2,\\text{False}],[\\alpha_3,\\gamma_3,\\mathrm{KL}_3,\\text{True}]]$。数值结果应为标准十进制浮点数，通过/失败应为布尔值。此问题中不出现角度，也没有物理单位。",
            "solution": "该问题要求对一个二进制脉冲神经采样器进行校准和验证。该过程涉及估计一个失真的 logistic 响应模型的参数，然后量化该模型与理想采样器的偏差。解决方案分三个阶段进行：首先，定义必要的数学函数和模型；其次，指定用于估计未知硬件参数的校准程序；第三，详细说明用于评估校准模型的验证指标。\n\n首先，我们定义模型的核心组成部分。采样器的脉冲概率由 logistic 函数 $\\sigma(u) = (1 + e^{-u})^{-1}$ 建模。理想采样器的脉冲概率为 $p^{\\ast}(h) = \\sigma(h)$，其中 $h$ 是输入场。硬件实现存在失真，其响应为 $p(h) = \\sigma(\\alpha h + \\gamma)$，其中 $\\alpha$ 和 $\\gamma$ 分别是未知的增益和偏移参数。分析在对数几率 (logit) 域中进行，使用函数 $\\operatorname{logit}(p) = \\ln(p/(1-p))$，它是 logistic 函数的反函数。这种变换是关键，因为它使响应模型线性化：$\\operatorname{logit}(p(h)) = \\alpha h + \\gamma$。\n\n对于给定的场 $h_i$，从 $n_i$ 次试验中获得的脉冲计数数据得到 $k_i$ 个脉冲。脉冲的经验概率估计为 $\\hat{p}_i$。为了处理 $k_i=0$ 或 $k_i=n_i$ 的情况（这会导致未定义的 logit 值），我们使用 Laplace 平滑。对于平滑参数 $\\delta$，平滑后的经验概率由下式给出：\n$$ \\hat{p}_i = \\frac{k_i + \\delta}{n_i + 2\\delta} $$\n对于此问题，指定 $\\delta=1$。这确保了 $\\hat{p}_i \\in (0, 1)$。\n\n第二阶段是校准，我们使用一组训练数据 $\\{ (h_i, k_i, n_i) \\}_{i=1}^N$ 来估计 $(\\alpha, \\gamma)$。我们将经验概率转换到 logit 域，创建一组数据点 $(h_i, y_i)$，其中 $y_i = \\operatorname{logit}(\\hat{p}_i)$。基于我们的线性化模型，我们期望关系式 $y_i \\approx \\alpha h_i + \\gamma$ 成立。我们可以通过最小化误差平方和 $S(\\alpha, \\gamma) = \\sum_{i=1}^N (y_i - (\\alpha h_i + \\gamma))^2$ 来找到最佳拟合参数 $(\\alpha, \\gamma)$。这是一个标准的线性最小二乘回归问题。我们寻求找到一个参数向量 $\\mathbf{x} = [\\alpha, \\gamma]^T$，以最佳方式求解线性方程组 $\\mathbf{A}\\mathbf{x} \\approx \\mathbf{y}$，其中：\n$$ \\mathbf{A} = \\begin{pmatrix} h_1  1 \\\\ h_2  1 \\\\ \\vdots  \\vdots \\\\ h_N  1 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix} $$\n最小二乘解由正规方程给出，$\\mathbf{x} = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{y}$。这提供了硬件参数的估计值 $(\\hat{\\alpha}, \\hat{\\gamma})$。为简洁起见，我们将其称为 $(\\alpha, \\gamma)$。\n\n第三阶段是验证。使用估计的参数 $(\\alpha, \\gamma)$ 和一个留出的测试数据集 $\\{ (h_j, k_j, n_j) \\}_{j=1}^m$，我们计算两个指标。\n\n第一个指标是平均 Kullback–Leibler (KL) 散度，$\\overline{\\mathrm{KL}}$。它衡量了当使用校准模型 $q(h_j) = \\sigma(\\alpha h_j + \\gamma)$ 来近似理想模型 $p^{\\ast}(h_j) = \\sigma(h_j)$ 时的平均信息损失。单个脉冲事件是一个伯努利试验，从 $\\mathsf{Bernoulli}(p)$ 分布到 $\\mathsf{Bernoulli}(q)$ 分布的 KL 散度是 $\\mathrm{KL}(p \\Vert q) = p \\ln(p/q) + (1-p)\\ln((1-p)/(1-q))$。在 $m$ 个测试场上的平均 KL 散度为：\n$$ \\overline{\\mathrm{KL}} = \\frac{1}{m}\\sum_{j=1}^{m} \\mathrm{KL}(p^{\\ast}(h_j) \\Vert q(h_j)) = \\frac{1}{m}\\sum_{j=1}^{m} \\left[ p^{\\ast}(h_j)\\ln\\left(\\frac{p^{\\ast}(h_j)}{q(h_j)}\\right) + (1-p^{\\ast}(h_j))\\ln\\left(\\frac{1-p^{\\ast}(h_j)}{1-q(h_j)}\\right) \\right] $$\n如果这个 $\\overline{\\mathrm{KL}}$ 值小于或等于指定的容差 $\\tau$，则认为该测试用例的采样器性能是可接受的。\n\n第二个指标是最大绝对 logit 残差 $R_{\\max}$，它量化了校准后的线性模型对经验测试数据的拟合优度。它是模型在 logit 域的预测与测得的测试概率的 logit 之间的最大偏差：\n$$ R_{\\max} = \\max_{j}\\left|\\operatorname{logit}\\!\\left(\\frac{k_j+\\delta}{n_j+2\\delta}\\right) - (\\alpha h_j + \\gamma)\\right| $$\n虽然该指标提供了关于模型与数据不匹配的宝贵诊断信息，但在此问题中它不用于通过/失败标准。\n\n实现将通过定义 $\\sigma(u)$、$\\operatorname{logit}(p)$、Laplace 平滑和 KL 散度的函数来进行。对于每个测试用例，它将：\n1.  获取训练数据 $(h_{\\text{train}}, k_{\\text{train}}, n_{\\text{train}})$ 和 $\\delta=1$。\n2.  计算平滑概率 $\\hat{p}_i$ 及其 logit 变换 $y_i$。\n3.  对 $y_i$ 关于 $h_i$ 进行线性最小二乘回归，以找到 $(\\alpha, \\gamma)$。\n4.  获取测试数据 $(h_{\\text{test}}, k_{\\text{test}}, n_{\\text{test}})$ 和估计的 $(\\alpha, \\gamma)$。\n5.  计算理想概率 $p^{\\ast}(h_j)$ 和校准模型概率 $q(h_j)$。\n6.  通过对所有测试点的 $\\mathrm{KL}(p^{\\ast}(h_j) \\Vert q(h_j))$ 求平均来计算平均 KL 散度 $\\overline{\\mathrm{KL}}$。\n7.  将 $\\overline{\\mathrm{KL}}$ 与特定于用例的容差 $\\tau$ 进行比较，以确定通过/失败状态。\n8.  为每个用例组装结果 $[\\alpha, \\gamma, \\overline{\\mathrm{KL}}, \\text{pass}]$，并将其格式化为指定的最终输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs calibration and validation for a binary spiking neural sampler\n    across multiple test cases as specified in the problem statement.\n    \"\"\"\n    delta = 1.0\n\n    test_cases = [\n        {\n            \"name\": \"Case A (well-conditioned)\",\n            \"h_train\": np.array([-2.0, 0.0, 2.0]),\n            \"n_train\": np.array([1000, 1000, 1000]),\n            \"k_train\": np.array([154, 525, 870]),\n            \"h_test\": np.array([-3.0, -1.0, 1.0, 3.0]),\n            \"n_test\": np.array([1000, 1000, 1000, 1000]),\n            \"k_test\": np.array([69, 310, 731, 943]),\n            \"tau\": 0.01\n        },\n        {\n            \"name\": \"Case B (saturation boundary)\",\n            \"h_train\": np.array([-1.0, 0.0, 1.0]),\n            \"n_train\": np.array([200, 200, 200]),\n            \"k_train\": np.array([4, 100, 196]),\n            \"h_test\": np.array([-2.0, -0.5, 0.5, 2.0]),\n            \"n_test\": np.array([200, 200, 200, 200]),\n            \"k_test\": np.array([0, 24, 176, 200]),\n            \"tau\": 0.05\n        },\n        {\n            \"name\": \"Case C (drift/noise case)\",\n            \"h_train\": np.array([-1.0, 0.0, 1.0]),\n            \"n_train\": np.array([500, 500, 500]),\n            \"k_train\": np.array([99, 225, 366]),\n            \"h_test\": np.array([-1.0, 0.0, 1.0, 2.0]),\n            \"n_test\": np.array([500, 500, 500, 500]),\n            \"k_test\": np.array([60, 189, 366, 462]),\n            \"tau\": 0.03\n        }\n    ]\n\n    def sigma(u):\n        \"\"\"Logistic function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-u))\n\n    def logit(p):\n        \"\"\"Logit function.\"\"\"\n        # Add small epsilon to avoid log(0) if p is exactly 0 or 1,\n        # though Laplace smoothing should prevent this.\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        return np.log(p / (1.0 - p))\n\n    def laplace_smooth(k, n, delta_val):\n        \"\"\"Empirical probability with Laplace smoothing.\"\"\"\n        return (k + delta_val) / (n + 2.0 * delta_val)\n    \n    def kl_divergence_bernoulli(p, q):\n        \"\"\"KL divergence between two Bernoulli distributions.\"\"\"\n        # Clip probabilities to avoid log(0)\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        q = np.clip(q, 1e-15, 1 - 1e-15)\n        return p * np.log(p / q) + (1.0 - p) * np.log((1.0 - p) / (1.0 - q))\n\n    all_results = []\n\n    for case in test_cases:\n        # --- Calibration Step ---\n        # 1. Calculate smoothed empirical probabilities\n        p_hat_train = laplace_smooth(case[\"k_train\"], case[\"n_train\"], delta)\n\n        # 2. Transform to logit domain\n        y_train = logit(p_hat_train)\n        \n        # 3. Perform linear least squares to find alpha and gamma\n        # We are fitting y = alpha * h + gamma\n        A = np.vstack([case[\"h_train\"], np.ones_like(case[\"h_train\"])]).T\n        solution, _, _, _ = np.linalg.lstsq(A, y_train, rcond=None)\n        alpha, gamma = solution[0], solution[1]\n\n        # --- Validation Step ---\n        h_test = case[\"h_test\"]\n        \n        # 1. Calculate Mean KL Divergence\n        p_star = sigma(h_test)  # Ideal probabilities\n        q_calibrated = sigma(alpha * h_test + gamma) # Calibrated model probabilities\n        \n        kl_divs = kl_divergence_bernoulli(p_star, q_calibrated)\n        mean_kl = np.mean(kl_divs)\n\n        # 2. (Optional, for completeness) Calculate Max Absolute Logit Residual\n        p_hat_test = laplace_smooth(case[\"k_test\"], case[\"n_test\"], delta)\n        logit_p_test = logit(p_hat_test)\n        model_logit_preds = alpha * h_test + gamma\n        r_max = np.max(np.abs(logit_p_test - model_logit_preds))\n\n        # 3. Pass/Fail decision\n        passed = mean_kl = case[\"tau\"]\n        \n        all_results.append([alpha, gamma, mean_kl, passed])\n\n    # --- Final Output Formatting ---\n    # Manually format the string to avoid spaces after commas\n    case_strings = []\n    for result in all_results:\n        # str(boolean) gives 'True' or 'False' as required\n        case_str = f\"[{result[0]},{result[1]},{result[2]},{str(result[3])}]\"\n        case_strings.append(case_str)\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "本练习将我们的讨论提升到一个更高的层次，展示了如何利用神经动力学来实现复杂的采样算法。我们将探讨如何精确地调整一个尖峰神经元的概率行为，以实现Metropolis-Hastings算法中的接受（acceptance）机制，这是马尔可夫链蒙特卡洛（MCMC）方法的基石。这个练习将底层的神经元机制与高层的概率推断算法联系起来，揭示了构建复杂推断机器的神经计算原理 。",
            "id": "4052517",
            "problem": "考虑一个神经形态脉冲网络，该网络通过 Metropolis–Hastings (MH) 算法实现马尔可夫链蒙特卡洛 (MCMC) 采样，以近似二元潜在状态上的后验分布。在状态向量 $\\mathbf{s} = (s_1, s_2)$（其中 $s_i \\in \\{0,1\\}$）上的目标分布由一个基于能量的模型定义，其对数概率与 $-\\!U(\\mathbf{s})$ 成正比，其中能量函数为\n$$\nU(\\mathbf{s}) = \\theta_1 s_1 + \\theta_2 s_2 + W s_1 s_2.\n$$\n假设该电路的目标是平稳分布 $\\pi(\\mathbf{s}) \\propto \\exp\\!\\big(-U(\\mathbf{s})\\big)$。\n\n提议的生成方式是：首先均匀选择一个坐标 $i \\in \\{1,2\\}$，然后根据一个有偏的局部规则随机设置 $s_i$：当坐标 $i$ 被选中时，电路以概率 $p_i$ 提议 $s_i' = 1$，以概率 $1 - p_i$ 提议 $s_i' = 0$，这与当前的 $s_i$ 无关。这导致了一个非对称的提议分布 $q(\\mathbf{s}' \\mid \\mathbf{s})$。\n\n为确保该马尔可夫链以 $\\pi(\\mathbf{s})$ 为其平稳分布，接受机制必须满足相对于 $\\pi(\\mathbf{s})$ 的细致平衡条件。每个提议的接受事件由一个“接受”神经元介导，该神经元在长度为 $T$ 的固定决策窗口内，根据速率为 $\\lambda$ 的齐次泊松过程发放脉冲。当且仅当窗口内至少出现一个接受脉冲时，该提议被接受。对于一个齐次泊松过程，在时长为 $T$ 的窗口内至少出现一个脉冲的概率是 $1 - \\exp(-\\lambda T)$。\n\n从具有转移核 $P(\\mathbf{s} \\to \\mathbf{s}')$ 的时间齐次马尔可夫链的细致平衡定义出发，推导在使用给定的非对称提议 $q(\\mathbf{s}' \\mid \\mathbf{s})$ 进行 MH 采样时所需的接受概率 $\\alpha(\\mathbf{s} \\to \\mathbf{s}')$，然后将其与神经元接受概率 $1 - \\exp(-\\lambda T)$ 相等，以确定实现正确接受概率所需的速率 $\\lambda$。\n\n最后，对于以下特定的转移和参数，数值计算 $\\lambda$：\n- 当前状态 $\\mathbf{x} = (1,0)$ 和提议状态 $\\mathbf{x}' = (1,1)$，\n- 能量参数 $\\theta_1 = 0.7$, $\\theta_2 = -0.3$, $W = 1.1$，\n- 提议偏置 $p_1 = 0.5$, $p_2 = 0.8$，\n- 决策窗口 $T = 20\\,\\text{ms}$。\n\n将您计算出的 $\\lambda$ 的最终数值四舍五入到四位有效数字，并以赫兹 (Hz) 为单位表示。您的最终答案必须是一个实数。",
            "solution": "该问题要求推导一个“接受”神经元的发放速率 $\\lambda$，该速率能在一个二元网络中为特定的状态转移正确地实现 Metropolis-Hastings (MH) 接受概率。\n\n首先，我们为 MH 算法建立理论基础。一个具有转移概率 $P(\\mathbf{s} \\to \\mathbf{s}')$ 的马尔可夫链，如果满足细致平衡条件，则其拥有平稳分布 $\\pi(\\mathbf{s})$：\n$$\n\\pi(\\mathbf{s}) P(\\mathbf{s} \\to \\mathbf{s}') = \\pi(\\mathbf{s}') P(\\mathbf{s}' \\to \\mathbf{s})\n$$\n在 MH 算法中，转移是一个两步过程：首先根据一个提议分布 $q(\\mathbf{s}' | \\mathbf{s})$ 从当前状态 $\\mathbf{s}$ 提议一个新状态 $\\mathbf{s}'$，然后以概率 $\\alpha(\\mathbf{s} \\to \\mathbf{s}')$ 接受这个提议。因此，对于 $\\mathbf{s}' \\neq \\mathbf{s}$，总转移概率为 $P(\\mathbf{s} \\to \\mathbf{s}') = q(\\mathbf{s}' | \\mathbf{s}) \\alpha(\\mathbf{s} \\to \\mathbf{s}')$。\n\n将此代入细致平衡方程，得到：\n$$\n\\pi(\\mathbf{s}) q(\\mathbf{s}' | \\mathbf{s}) \\alpha(\\mathbf{s} \\to \\mathbf{s}') = \\pi(\\mathbf{s}') q(\\mathbf{s} | \\mathbf{s}') \\alpha(\\mathbf{s}' \\to \\mathbf{s})\n$$\n对接受概率的比率进行整理，得到：\n$$\n\\frac{\\alpha(\\mathbf{s} \\to \\mathbf{s}')}{\\alpha(\\mathbf{s}' \\to \\mathbf{s})} = \\frac{\\pi(\\mathbf{s}') q(\\mathbf{s} | \\mathbf{s}')}{\\pi(\\mathbf{s}) q(\\mathbf{s}' | \\mathbf{s})}\n$$\n满足此关系的 Metropolis-Hastings 接受概率的标准选择是：\n$$\n\\alpha(\\mathbf{s} \\to \\mathbf{s}') = \\min\\left(1, \\frac{\\pi(\\mathbf{s}') q(\\mathbf{s} | \\mathbf{s}')}{\\pi(\\mathbf{s}) q(\\mathbf{s}' | \\mathbf{s})}\\right)\n$$\n问题陈述，目标平稳分布 $\\pi(\\mathbf{s})$ 与 $\\exp(-U(\\mathbf{s}))$ 成正比，其中 $U(\\mathbf{s})$ 是状态 $\\mathbf{s}$ 的能量。因此，目标分布的概率比为：\n$$\n\\frac{\\pi(\\mathbf{s}')}{\\pi(\\mathbf{s})} = \\frac{\\exp(-U(\\mathbf{s}'))}{\\exp(-U(\\mathbf{s}))} = \\exp\\big(-(U(\\mathbf{s}') - U(\\mathbf{s}))\\big) = \\exp(-\\Delta U)\n$$\n其中 $\\Delta U = U(\\mathbf{s}') - U(\\mathbf{s})$。接受概率可以写为：\n$$\n\\alpha(\\mathbf{s} \\to \\mathbf{s}') = \\min\\left(1, \\exp(-\\Delta U) \\frac{q(\\mathbf{s} | \\mathbf{s}')}{q(\\mathbf{s}' | \\mathbf{s})}\\right)\n$$\n接下来，我们必须确定从状态 $\\mathbf{x} = (1,0)$ 到 $\\mathbf{x}' = (1,1)$ 的特定转移的提议概率比率。提议机制包括两个步骤：首先，以 $1/2$ 的概率均匀选择一个坐标 $i \\in \\{1,2\\}$；其次，根据一个有偏规则（以概率 $p_i$ 提议 $s_i'=1$，以概率 $1-p_i$ 提议 $s_i'=0$）为 $s_i$ 提议一个新值。\n\n转移是从 $\\mathbf{s} = \\mathbf{x} = (1,0)$ 到 $\\mathbf{s}' = \\mathbf{x}' = (1,1)$。这些状态仅在第二个坐标（$i=2$）上不同。\n对于正向提议 $q(\\mathbf{x}'|\\mathbf{x})$，必须选择坐标 $i=2$（概率为 $1/2$），并且该坐标的新状态必须被提议为 $s_2' = 1$（概率为 $p_2$）。因此：\n$$\nq(\\mathbf{x}'|\\mathbf{x}) = q((1,1)|(1,0)) = \\frac{1}{2} \\times p_2\n$$\n对于反向提议 $q(\\mathbf{x}|\\mathbf{x}')$，从 $\\mathbf{x}'=(1,1)$ 开始要到达 $\\mathbf{x}=(1,0)$，必须再次选择坐标 $i=2$（概率为 $1/2$），并且新状态必须被提议为 $s_2 = 0$（概率为 $1-p_2$）。因此：\n$$\nq(\\mathbf{x}|\\mathbf{x}') = q((1,0)|(1,1)) = \\frac{1}{2} \\times (1-p_2)\n$$\n提议概率的比率为：\n$$\n\\frac{q(\\mathbf{x}|\\mathbf{x}')}{q(\\mathbf{x}'|\\mathbf{x})} = \\frac{\\frac{1}{2}(1-p_2)}{\\frac{1}{2}p_2} = \\frac{1-p_2}{p_2}\n$$\n现在我们计算能量变化 $\\Delta U = U(\\mathbf{x}') - U(\\mathbf{x})$。能量函数为 $U(\\mathbf{s}) = \\theta_1 s_1 + \\theta_2 s_2 + W s_1 s_2$。\n对于当前状态 $\\mathbf{x}=(1,0)$：\n$$\nU(\\mathbf{x}) = U(1,0) = \\theta_1(1) + \\theta_2(0) + W(1)(0) = \\theta_1\n$$\n对于提议状态 $\\mathbf{x}'=(1,1)$：\n$$\nU(\\mathbf{x}') = U(1,1) = \\theta_1(1) + \\theta_2(1) + W(1)(1) = \\theta_1 + \\theta_2 + W\n$$\n能量变化为：\n$$\n\\Delta U = U(\\mathbf{x}') - U(\\mathbf{x}) = (\\theta_1 + \\theta_2 + W) - \\theta_1 = \\theta_2 + W\n$$\n将这些结果代入接受概率公式：\n$$\n\\alpha(\\mathbf{x} \\to \\mathbf{x}') = \\min\\left(1, \\exp(-(\\theta_2 + W)) \\frac{1-p_2}{p_2}\\right)\n$$\n问题陈述，接受的神经元实现是基于泊松过程的，其中接受概率是在时间窗口 $T$ 内至少出现一个脉冲的概率，由 $1 - \\exp(-\\lambda T)$ 给出。为确保细致平衡，这个神经元接受概率必须等于推导出的 MH 接受概率：\n$$\n1 - \\exp(-\\lambda T) = \\alpha(\\mathbf{x} \\to \\mathbf{x}')\n$$\n解出 $\\lambda$：\n$$\n\\exp(-\\lambda T) = 1 - \\alpha(\\mathbf{x} \\to \\mathbf{x}')\n$$\n$$\n-\\lambda T = \\ln\\big(1 - \\alpha(\\mathbf{x} \\to \\mathbf{x}')\\big)\n$$\n$$\n\\lambda = -\\frac{1}{T} \\ln\\big(1 - \\alpha(\\mathbf{x} \\to \\mathbf{x}')\\big)\n$$\n代入 $\\alpha(\\mathbf{x} \\to \\mathbf{x}')$ 的表达式：\n$$\n\\lambda = -\\frac{1}{T} \\ln\\left(1 - \\min\\left(1, \\exp(-(\\theta_2 + W)) \\frac{1-p_2}{p_2}\\right)\\right)\n$$\n现在我们可以用给定的参数对此表达式进行数值计算：$\\theta_2 = -0.3$，$W = 1.1$，$p_2 = 0.8$，以及 $T = 20\\,\\text{ms} = 20 \\times 10^{-3}\\,\\text{s}$。\n首先，计算 $\\min$ 函数的参数：\n$$\n\\exp(-(\\theta_2 + W)) \\frac{1-p_2}{p_2} = \\exp(-(-0.3 + 1.1)) \\frac{1-0.8}{0.8} = \\exp(-0.8) \\frac{0.2}{0.8} = 0.25\\exp(-0.8)\n$$\n数值上，$\\exp(-0.8) \\approx 0.449329$。所以该项约等于 $0.25 \\times 0.449329 = 0.11233225$。因为这个值小于 1，所以 $\\min$ 函数返回这个值。\n因此，接受概率为 $\\alpha(\\mathbf{x} \\to \\mathbf{x}')=0.25\\exp(-0.8)$。\n现在我们可以计算 $\\lambda$：\n$$\n\\lambda = -\\frac{1}{20 \\times 10^{-3}} \\ln\\big(1 - 0.25\\exp(-0.8)\\big)\n$$\n$$\n\\lambda = -50 \\ln\\big(1 - 0.25\\exp(-0.8)\\big)\n$$\n$$\n\\lambda \\approx -50 \\ln(1 - 0.11233225) = -50 \\ln(0.88766775)\n$$\n$$\n\\lambda \\approx -50 \\times (-0.119159) \\approx 5.95795 \\, \\text{Hz}\n$$\n四舍五入到四位有效数字，我们得到 $\\lambda = 5.958\\,\\text{Hz}$。",
            "answer": "$$\\boxed{5.958}$$"
        }
    ]
}