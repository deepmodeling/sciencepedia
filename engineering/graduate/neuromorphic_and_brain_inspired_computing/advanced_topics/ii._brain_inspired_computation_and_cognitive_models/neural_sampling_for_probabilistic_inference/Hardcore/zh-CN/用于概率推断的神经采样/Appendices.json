{
    "hands_on_practices": [
        {
            "introduction": "神经采样器随时间生成状态序列，但与理想的随机数生成器不同，这些样本之间通常存在关联性。本练习探讨了这种以自回归过程为模型的时间相关性，如何影响样本均值和方差等估计量的统计特性。理解这些影响对于正确解读神经形态系统的输出以及设计更高效的采样器至关重要。",
            "id": "4052509",
            "problem": "考虑一个标量潜变量 $x$，由一个神经形态神经采样器表示，其离散时间动态是一个一阶自回归过程 (AR(1))：$$x_{t+1} = \\mu + \\rho\\left(x_t - \\mu\\right) + \\varepsilon_t,$$ 其中 $t \\in \\{0,1,2,\\ldots\\}$, $|\\rho| < 1$，并且新息 $\\varepsilon_t$ 是独立同分布 (i.i.d.) 的高斯分布，其均值为零，方差的选择使得 $x_t$ 的平稳方差为 $$\\gamma_0 = \\sigma^2 T,$$ 其中 $\\mu$ 是目标均值，$\\sigma^2$ 是目标方差，$T$ 是神经温度参数，它相对于目标分布缩放采样器的平稳方差。这个 AR(1) 过程是严格平稳的，其自协方差函数为 $$\\gamma_k = \\operatorname{Cov}(x_t, x_{t+k}) = \\gamma_0 \\rho^k.$$ 您从该平稳过程中观测到 $n$ 个连续样本 $\\{x_1, x_2, \\ldots, x_n\\}$，并使用常规估计量来估计均值和方差 $$\\hat{m} = \\frac{1}{n}\\sum_{t=1}^n x_t, \\quad \\hat{s}^2 = \\frac{1}{n-1}\\sum_{t=1}^n \\left(x_t - \\hat{m}\\right)^2.$$ 从第一性原理出发——即期望、方差、协方差和平稳性的定义——推导下列量的表达式（用 $n$、$\\mu$、$\\sigma^2$、$T$ 和 $\\rho$ 表示），且不假设样本之间相互独立：\n\n1. 样本均值估计量的偏差，定义为 $$\\operatorname{Bias}(\\hat{m}) = \\mathbb{E}[\\hat{m}] - \\mu.$$\n\n2. 样本均值估计量的方差，定义为 $$\\operatorname{Var}(\\hat{m}) = \\mathbb{E}\\left[(\\hat{m} - \\mathbb{E}[\\hat{m}])^2\\right].$$\n\n3. 样本方差估计量的期望值，定义为 $$\\mathbb{E}[\\hat{s}^2].$$\n\n然后，将样本方差解释为目标方差 $\\sigma^2$ 的一个朴素估计量，该估计量忽略了神经温度 ($T$) 和时间相关性 ($\\rho$)。使用您推导的表达式，计算这个朴素方差估计量的偏差，$$\\operatorname{Bias}_{\\text{naive}} = \\mathbb{E}[\\hat{s}^2] - \\sigma^2.$$\n\n您的程序必须实现您推导的公式，以计算每个测试用例的以下三个输出：\n\n- 样本均值估计量的偏差 $\\operatorname{Bias}(\\hat{m})$。\n- 朴素方差估计量的偏差 $\\operatorname{Bias}_{\\text{naive}}$。\n- 样本均值估计量的方差 $\\operatorname{Var}(\\hat{m})$。\n\n不涉及物理单位。不涉及角度。所有输出均表示为实数（浮点数）。使用以下测试套件，其中每个元组为 $(n, \\mu, \\sigma, T, \\rho)$：\n\n- 测试用例 A (独立性，理想路径)： $(1000, 0.0, 1.0, 1.0, 0.0)$。\n- 测试用例 B (强正相关，大 $n$)： $(1000, 0.0, 2.0, 1.0, 0.95)$。\n- 测试用例 C (负相关，中等 $n$)： $(200, 1.0, 1.5, 1.0, -0.5)$。\n- 测试用例 D (温度不匹配，中等相关性)： $(500, 0.5, 1.0, 1.5, 0.7)$。\n- 测试用例 E (小样本，高相关性)： $(20, -1.0, 1.0, 1.0, 0.9)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按上述顺序输出三个浮点数，并将所有测试用例的输出连接成一个扁平列表。例如，输出格式应为 $$\\texttt{[b\\_A^{(m)},b\\_A^{(v)},v\\_A^{(m)},b\\_B^{(m)},b\\_B^{(v)},v\\_B^{(m)},\\ldots]}$$ 其中 $b^{(m)}$ 是均值的偏差，$b^{(v)}$ 是朴素方差估计量的偏差，$v^{(m)}$ 是样本均值的方差。",
            "solution": "对用户提供的问题进行严格的验证程序。\n\n### 步骤 1：提取已知条件\n- **过程动态**：一阶自回归过程 AR($1$) 定义为 `$x_{t+1} = \\mu + \\rho(x_t - \\mu) + \\varepsilon_t$`。\n- **参数和条件**：\n    - 时间指数：`$t \\in \\{0,1,2,\\ldots\\}`。\n    - 自回归系数：`$|\\rho| < 1$`。\n    - 新息项：`$\\varepsilon_t$` 是独立同分布的高斯分布，且 `$\\mathbb{E}[\\varepsilon_t] = 0$`。\n    - 目标均值：`$\\mu$`。\n    - 目标方差：`$\\sigma^2$`。\n    - 神经温度：`$T$`。\n- **平稳性质**：\n    - 过程 `$x_t$` 是严格平稳的。\n    - 平稳均值：`$\\mathbb{E}[x_t] = \\mu$`。\n    - 平稳方差：`$\\gamma_0 = \\operatorname{Var}(x_t) = \\sigma^2 T$`。\n    - 自协方差函数：对于 `$k \\ge 0$`，`$\\gamma_k = \\operatorname{Cov}(x_t, x_{t+k}) = \\gamma_0 \\rho^k$`。由此可推得 `$\\operatorname{Cov}(x_i, x_j) = \\gamma_0 \\rho^{|i-j|}$`。\n- **观测值**：`$n$` 个连续样本 `$\\{x_1, x_2, \\ldots, x_n\\}$`。\n- **估计量**：\n    - 样本均值：`$\\hat{m} = \\frac{1}{n}\\sum_{t=1}^n x_t$`。\n    - 样本方差：`$\\hat{s}^2 = \\frac{1}{n-1}\\sum_{t=1}^n (x_t - \\hat{m})^2$`。\n- **待推导的量**：\n    1. 样本均值的偏差：`$\\operatorname{Bias}(\\hat{m}) = \\mathbb{E}[\\hat{m}] - \\mu$`。\n    2. 样本均值的方差：`$\\operatorname{Var}(\\hat{m}) = \\mathbb{E}[(\\hat{m} - \\mathbb{E}[\\hat{m}])^2]$`。\n    3. 样本方差的期望值：`$\\mathbb{E}[\\hat{s}^2]$`。\n    4. 朴素方差估计量的偏差：`$\\operatorname{Bias}_{\\text{naive}} = \\mathbb{E}[\\hat{s}^2] - \\sigma^2$`。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据**：该问题在随机过程和统计估计理论中有充分的依据。AR($1$) 过程是时间序列分析中的一个典型模型，其在神经采样中的应用是神经形态计算中一个公认的范式。所有定义都是标准且正确的。\n- **适定性**：该问题是适定的。它提供了推导所需量的所有必要参数和定义。这些推导是数理统计中的标准练习，并且存在唯一的、稳定的解。\n- **客观性**：语言正式、精确，没有主观论断。\n- **缺陷清单**：\n    1. **科学/事实不健全**：无。该模型及其性质与既定理论一致。条件 `$|\\rho| < 1$` 正确地确保了平稳性。\n    2. **不可形式化/不相关**：无。该问题是一个形式化的数学任务，直接关系到对神经采样器统计特性的表征。\n    3. **不完整/矛盾的设置**：无。所有必需信息都已提供且内部一致。\n    4. **不切实际/不可行**：无。模型参数和测试用例在物理上和数值上都是合理的。\n    5. **不适定/结构不良**：无。目标陈述清晰，能导出唯一的、可验证的结果。\n    6. **伪深刻/琐碎**：无。虽然均值的偏差是一个简单的结果，但相关过程的均值方差以及样本方差的期望需要进行不平凡的推导。\n    7. **超出科学可验证性**：无。推导过程在数学上是可验证的。\n\n### 步骤 3：结论和行动\n此问题是 **有效的**。这是一个定义明确、科学合理的统计分析问题。将提供一个完整的、有理有据的解决方案。\n\n---\n\n### 从第一性原理推导\n\n将基于期望、方差和协方差的基本定义以及平稳 AR($1$) 过程的性质，对每个请求的量进行推导。平稳均值为 `$\\mathbb{E}[x_t] = \\mu$`，自协方差为 `$\\operatorname{Cov}(x_i, x_j) = \\gamma_0 \\rho^{|i-j|}$`，其中 `$\\gamma_0 = \\sigma^2 T$`。\n\n**1. 样本均值估计量的偏差, `$\\operatorname{Bias}(\\hat{m})$`**\n\n偏差定义为 `$\\operatorname{Bias}(\\hat{m}) = \\mathbb{E}[\\hat{m}] - \\mu$`。我们首先计算样本均值 `$\\hat{m}$` 的期望值。\n\n根据期望算子的线性性质：\n$$\n\\mathbb{E}[\\hat{m}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{t=1}^n x_t\\right] = \\frac{1}{n}\\sum_{t=1}^n \\mathbb{E}[x_t]\n$$\n由于该过程是平稳的，对所有 `$t$` 都有 `$\\mathbb{E}[x_t] = \\mu$`。\n$$\n\\mathbb{E}[\\hat{m}] = \\frac{1}{n}\\sum_{t=1}^n \\mu = \\frac{1}{n}(n\\mu) = \\mu\n$$\n因此，样本均值的偏差为：\n$$\n\\operatorname{Bias}(\\hat{m}) = \\mu - \\mu = 0\n$$\n样本均值 `$\\hat{m}$` 是过程均值 `$\\mu$` 的无偏估计量，无论时间相关性 `$\\rho$` 或神经温度 `$T$` 如何。\n\n**2. 样本均值估计量的方差, `$\\operatorname{Var}(\\hat{m})$`**\n\n`$\\hat{m}$` 的方差定义为 `$\\operatorname{Var}(\\hat{m}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{t=1}^n x_t\\right)$`。\n\n使用相关随机变量之和的方差性质：\n$$\n\\operatorname{Var}(\\hat{m}) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{t=1}^n x_t\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(x_i, x_j)\n$$\n代入自协方差函数 `$\\operatorname{Cov}(x_i, x_j) = \\gamma_0 \\rho^{|i-j|}$`：\n$$\n\\operatorname{Var}(\\hat{m}) = \\frac{\\gamma_0}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\rho^{|i-j|}\n$$\n对于一个托普利茨矩阵，该双重求和有一个标准结果，可简化为：\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n \\rho^{|i-j|} = n \\frac{1+\\rho}{1-\\rho} - \\frac{2\\rho(1-\\rho^n)}{(1-\\rho)^2}\n$$\n将此结果代回 `$\\operatorname{Var}(\\hat{m})$` 的表达式中：\n$$\n\\operatorname{Var}(\\hat{m}) = \\frac{\\gamma_0}{n^2} \\left[ n \\frac{1+\\rho}{1-\\rho} - \\frac{2\\rho(1-\\rho^n)}{(1-\\rho)^2} \\right]\n$$\n提取公因子 `$n$` 并代入 `$\\gamma_0 = \\sigma^2 T$`，我们得到最终公式：\n$$\n\\operatorname{Var}(\\hat{m}) = \\frac{\\sigma^2 T}{n} \\left[ \\frac{1+\\rho}{1-\\rho} - \\frac{2\\rho(1-\\rho^n)}{n(1-\\rho)^2} \\right]\n$$\n注意，在独立样本的情况下（`$\\rho=0$`），该公式正确地简化为 `$\\operatorname{Var}(\\hat{m}) = \\frac{\\sigma^2 T}{n} [1 - 0] = \\frac{\\gamma_0}{n}$`。\n\n**3. 样本方差估计量的期望值, `$\\mathbb{E}[\\hat{s}^2]$`**\n\n我们从 `$\\hat{s}^2$` 的定义开始，并使用一个常用的恒等式：\n$$\n\\hat{s}^2 = \\frac{1}{n-1}\\sum_{t=1}^n (x_t - \\hat{m})^2 = \\frac{1}{n-1} \\left( \\sum_{t=1}^n x_t^2 - n\\hat{m}^2 \\right)\n$$\n根据期望的线性性质：\n$$\n\\mathbb{E}[\\hat{s}^2] = \\frac{1}{n-1} \\left( \\sum_{t=1}^n \\mathbb{E}[x_t^2] - n\\mathbb{E}[\\hat{m}^2] \\right)\n$$\n我们计算这两个期望值。\n首先，根据方差的定义 `$\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$`：\n`$\\mathbb{E}[x_t^2] = \\operatorname{Var}(x_t) + (\\mathbb{E}[x_t])^2 = \\gamma_0 + \\mu^2$`。\n`$\\mathbb{E}[\\hat{m}^2] = \\operatorname{Var}(\\hat{m}) + (\\mathbb{E}[\\hat{m}])^2 = \\operatorname{Var}(\\hat{m}) + \\mu^2$`。\n\n将这些代入 `$\\mathbb{E}[\\hat{s}^2]$` 的表达式中：\n$$\n\\mathbb{E}[\\hat{s}^2] = \\frac{1}{n-1} \\left( \\sum_{t=1}^n (\\gamma_0 + \\mu^2) - n(\\operatorname{Var}(\\hat{m}) + \\mu^2) \\right)\n$$\n$$\n= \\frac{1}{n-1} \\left( n(\\gamma_0 + \\mu^2) - n\\operatorname{Var}(\\hat{m}) - n\\mu^2 \\right)\n$$\n$$\n= \\frac{1}{n-1} \\left( n\\gamma_0 + n\\mu^2 - n\\operatorname{Var}(\\hat{m}) - n\\mu^2 \\right)\n$$\n$$\n= \\frac{n}{n-1} \\left( \\gamma_0 - \\operatorname{Var}(\\hat{m}) \\right)\n$$\n这个简洁的结果将样本方差的期望值与真实过程方差 `$\\gamma_0$` 以及样本均值的方差 `$\\operatorname{Var}(\\hat{m})$` 联系起来。使用 `$\\gamma_0 = \\sigma^2 T$` 和先前推导的 `$\\operatorname{Var}(\\hat{m})$` 公式，这个量就完全确定了。对于独立同分布的样本，`$\\operatorname{Var}(\\hat{m}) = \\gamma_0/n$`，并且 `$\\mathbb{E}[\\hat{s}^2] = \\frac{n}{n-1}(\\gamma_0 - \\gamma_0/n) = \\frac{n}{n-1}(\\frac{n-1}{n}\\gamma_0) = \\gamma_0$`，证实了在独立同分布情况下，`$\\hat{s}^2$` 是过程方差的无偏估计量。\n\n**4. 朴素方差估计量的偏差, `$\\operatorname{Bias}_{\\text{naive}}$`**\n\n问题将目标方差 `$\\sigma^2$` 的朴素估计量定义为样本方差 `$\\hat{s}^2$`，它忽略了神经温度 `$T$` 和时间相关性 `$\\rho$` 的影响。这个朴素估计量的偏差是：\n$$\n\\operatorname{Bias}_{\\text{naive}} = \\mathbb{E}[\\hat{s}^2] - \\sigma^2\n$$\n代入我们关于 `$\\mathbb{E}[\\hat{s}^2]$` 的结果：\n$$\n\\operatorname{Bias}_{\\text{naive}} = \\frac{n}{n-1} \\left( \\gamma_0 - \\operatorname{Var}(\\hat{m}) \\right) - \\sigma^2\n$$\n代入 `$\\gamma_0 = \\sigma^2 T$`：\n$$\n\\operatorname{Bias}_{\\text{naive}} = \\frac{n}{n-1} \\left( \\sigma^2 T - \\operatorname{Var}(\\hat{m}) \\right) - \\sigma^2\n$$\n该偏差有两个来源。第一个是温度不匹配，它贡献了一个与 `$\\sigma^2(T-1)$` 相关的项。第二个是时间相关性，它通过 `$\\operatorname{Var}(\\hat{m})$` 项进入。对于正的 `$\\rho$`，`$\\operatorname{Var}(\\hat{m})` 大于独立同分布情况下的值，这会减小 `$\\mathbb{E}[\\hat{s}^2]$`，从而对偏差产生一个负的分量。对于负的 `$\\rho$`，情况则相反。\n\n现在将实现这些推导出的公式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes statistical properties of estimators for an AR(1) process\n    based on derived analytical formulas.\n    \"\"\"\n    # Test suite: (n, mu, sigma, T, rho)\n    test_cases = [\n        (1000, 0.0, 1.0, 1.0, 0.0),    # A: independence, happy path\n        (1000, 0.0, 2.0, 1.0, 0.95),   # B: strong positive correlation, large n\n        (200, 1.0, 1.5, 1.0, -0.5),   # C: negative correlation, moderate n\n        (500, 0.5, 1.0, 1.5, 0.7),    # D: temperature mismatch, moderate correlation\n        (20, -1.0, 1.0, 1.0, 0.9),    # E: small sample, high correlation\n    ]\n\n    results = []\n    for case in test_cases:\n        n, mu, sigma, T, rho = case\n        \n        sigma_sq = sigma**2\n        gamma_0 = sigma_sq * T\n\n        # 1. Bias of the sample mean estimator\n        # As derived, E[m_hat] = mu, so the bias is always 0.\n        bias_m = 0.0\n\n        # 2. Variance of the sample mean estimator\n        if n == 0:\n            # Not applicable, but handle for robustness\n            var_m = np.nan\n        elif rho == 0.0:\n            # i.i.d. case\n            var_m = gamma_0 / n\n        else:\n            # Correlated case, |rho|  1\n            if abs(rho) >= 1:\n                # Should not happen based on problem constraints\n                var_m = np.inf\n            else:\n                one_minus_rho = 1.0 - rho\n                # Var(m_hat) = (gamma_0 / n) * [ (1+rho)/(1-rho) - (2*rho*(1-rho^n))/(n*(1-rho)^2) ]\n                term1 = (1.0 + rho) / one_minus_rho\n                term2 = (2.0 * rho * (1.0 - np.power(rho, n))) / (n * np.power(one_minus_rho, 2))\n                var_m = (gamma_0 / n) * (term1 - term2)\n\n        # 3. Expected value of the sample variance estimator\n        if n == 1:\n            # Bessel's correction undefined, but handle for robustness\n            E_s_sq = np.nan\n        else:\n            # E[s^2] = (n / (n-1)) * (gamma_0 - Var(m_hat))\n            E_s_sq = (n / (n - 1.0)) * (gamma_0 - var_m)\n\n        # 4. Bias of the naive variance estimator\n        # Bias_naive = E[s^2] - sigma^2\n        bias_v_naive = E_s_sq - sigma_sq\n\n        # Append results in the specified order\n        results.append(bias_m)\n        results.append(bias_v_naive)\n        results.append(var_m)\n\n    # Format the final output as a single-line string\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "类脑计算的一个核心挑战是将抽象的概率算法映射到神经回路的动态上。本练习演示了如何使用尖峰神经元来实现马尔可夫链蒙特卡洛（MCMC）方法中的基石——Metropolis-Hastings算法。其核心任务是推导出一个“接受”神经元所需的放电率，以确保满足细致平衡条件，从而将 MCMC 的计算原理转化为生物物理上可行的机制。",
            "id": "4052517",
            "problem": "考虑一个神经形态脉冲网络，它通过 Metropolis-Hastings (MH) 算法实现马尔可夫链蒙特卡洛 (MCMC) 采样，以近似二元潜在状态的后验分布。状态向量 $\\mathbf{s} = (s_1, s_2)$（其中 $s_i \\in \\{0,1\\}$）上的目标分布由一个基于能量的模型定义，其对数概率与 $-\\!U(\\mathbf{s})$ 成正比，能量函数为\n$$\nU(\\mathbf{s}) = \\theta_1 s_1 + \\theta_2 s_2 + W s_1 s_2.\n$$\n假设该电路的目标是稳态分布 $\\pi(\\mathbf{s}) \\propto \\exp\\!\\big(-U(\\mathbf{s})\\big)$。\n\n提议是通过均匀选择一个坐标 $i \\in \\{1,2\\}$ 并根据一个有偏的局部规则随机设置 $s_i$ 来生成的：当坐标 $i$ 被选中时，电路以概率 $p_i$ 提议 $s_i' = 1$，以概率 $1 - p_i$ 提议 $s_i' = 0$，这与当前的 $s_i$ 无关。这导致了一个非对称的提议分布 $q(\\mathbf{s}' \\mid \\mathbf{s})$。\n\n为确保该链以 $\\pi(\\mathbf{s})$ 为其稳态分布，接受机制必须满足关于 $\\pi(\\mathbf{s})$ 的细致平衡条件。对每个提议的接受事件由一个“接受”神经元介导，该神经元在长度为 $T$ 的固定决策窗口内，根据速率为 $\\lambda$ 的齐次泊松过程发放脉冲。当且仅当窗口内至少出现一个接受脉冲时，该提议被接受。对于一个齐次泊松过程，在持续时间为 $T$ 的窗口内至少出现一个脉冲的概率是 $1 - \\exp(-\\lambda T)$。\n\n从转移核为 $P(\\mathbf{s} \\to \\mathbf{s}')$ 的时齐马尔可夫链的细致平衡定义出发，推导在使用给定的非对称提议 $q(\\mathbf{s}' \\mid \\mathbf{s})$ 进行 MH 采样时所需的接受概率 $\\alpha(\\mathbf{s} \\to \\mathbf{s}')$，然后将其与神经接受概率 $1 - \\exp(-\\lambda T)$ 相等，以确定实现正确接受概率所需的速率 $\\lambda$。\n\n最后，针对以下特定的转移和参数，数值计算 $\\lambda$：\n- 当前状态 $\\mathbf{x} = (1,0)$ 和提议状态 $\\mathbf{x}' = (1,1)$，\n- 能量参数 $\\theta_1 = 0.7$，$\\theta_2 = -0.3$，$W = 1.1$，\n- 提议偏置 $p_1 = 0.5$，$p_2 = 0.8$，\n- 决策窗口 $T = 20\\,\\text{ms}$。\n\n将您最终的 $\\lambda$ 数值四舍五入到四位有效数字，并以赫兹 (Hz) 为单位表示。您的最终答案必须是一个实数。",
            "solution": "该问题要求推导一个“接受”神经元的发放速率 $\\lambda$，该速率能在一个二元网络中为特定的状态转移正确地实现 Metropolis-Hastings (MH) 接受概率。\n\n首先，我们为 MH 算法建立理论基础。一个转移概率为 $P(\\mathbf{s} \\to \\mathbf{s}')$ 的马尔可夫链，如果满足细致平衡条件，则其具有稳态分布 $\\pi(\\mathbf{s})$：\n$$\n\\pi(\\mathbf{s}) P(\\mathbf{s} \\to \\mathbf{s}') = \\pi(\\mathbf{s}') P(\\mathbf{s}' \\to \\mathbf{s})\n$$\n在 MH 算法中，转移是一个两步过程：首先根据提议分布 $q(\\mathbf{s}' | \\mathbf{s})$ 从当前状态 $\\mathbf{s}$ 提议一个新状态 $\\mathbf{s}'$，然后以概率 $\\alpha(\\mathbf{s} \\to \\mathbf{s}')$ 接受这个提议。因此，对于 $\\mathbf{s}' \\neq \\mathbf{s}$，总的转移概率是 $P(\\mathbf{s} \\to \\mathbf{s}') = q(\\mathbf{s}' | \\mathbf{s}) \\alpha(\\mathbf{s} \\to \\mathbf{s}')$。\n\n将此代入细致平衡方程，得到：\n$$\n\\pi(\\mathbf{s}) q(\\mathbf{s}' | \\mathbf{s}) \\alpha(\\mathbf{s} \\to \\mathbf{s}') = \\pi(\\mathbf{s}') q(\\mathbf{s} | \\mathbf{s}') \\alpha(\\mathbf{s}' \\to \\mathbf{s})\n$$\n对接受概率的比率进行整理，得到：\n$$\n\\frac{\\alpha(\\mathbf{s} \\to \\mathbf{s}')}{\\alpha(\\mathbf{s}' \\to \\mathbf{s})} = \\frac{\\pi(\\mathbf{s}') q(\\mathbf{s} | \\mathbf{s}')}{\\pi(\\mathbf{s}) q(\\mathbf{s}' | \\mathbf{s})}\n$$\n满足此关系的、标准的 Metropolis-Hastings 接受概率选择为：\n$$\n\\alpha(\\mathbf{s} \\to \\mathbf{s}') = \\min\\left(1, \\frac{\\pi(\\mathbf{s}') q(\\mathbf{s} | \\mathbf{s}')}{\\pi(\\mathbf{s}) q(\\mathbf{s}' | \\mathbf{s})}\\right)\n$$\n问题陈述，目标稳态分布 $\\pi(\\mathbf{s})$ 与 $\\exp(-U(\\mathbf{s}))$ 成正比，其中 $U(\\mathbf{s})$ 是状态 $\\mathbf{s}$ 的能量。因此，目标分布的概率比率为：\n$$\n\\frac{\\pi(\\mathbf{s}')}{\\pi(\\mathbf{s})} = \\frac{\\exp(-U(\\mathbf{s}'))}{\\exp(-U(\\mathbf{s}))} = \\exp\\big(-(U(\\mathbf{s}') - U(\\mathbf{s}))\\big) = \\exp(-\\Delta U)\n$$\n其中 $\\Delta U = U(\\mathbf{s}') - U(\\mathbf{s})$。接受概率可以写为：\n$$\n\\alpha(\\mathbf{s} \\to \\mathbf{s}') = \\min\\left(1, \\exp(-\\Delta U) \\frac{q(\\mathbf{s} | \\mathbf{s}')}{q(\\mathbf{s}' | \\mathbf{s})}\\right)\n$$\n接下来，我们必须确定从状态 $\\mathbf{x} = (1,0)$ 到 $\\mathbf{x}' = (1,1)$ 的特定转移的提议概率比率。提议机制包括两个步骤：首先，以概率 $1/2$ 均匀选择一个坐标 $i \\in \\{1,2\\}$；其次，根据一个有偏规则（以概率 $p_i$ 提议 $s_i'=1$，以概率 $1-p_i$ 提议 $s_i'=0$）为 $s_i$ 提议一个新值。\n\n转移是从 $\\mathbf{s} = \\mathbf{x} = (1,0)$ 到 $\\mathbf{s}' = \\mathbf{x}' = (1,1)$。这些状态仅在第二个坐标（$i=2$）上不同。\n对于正向提议 $q(\\mathbf{x}'|\\mathbf{x})$，必须选择坐标 $i=2$（概率为 $1/2$），并且该坐标的新状态必须被提议为 $s_2' = 1$（概率为 $p_2$）。因此：\n$$\nq(\\mathbf{x}'|\\mathbf{x}) = q((1,1)|(1,0)) = \\frac{1}{2} \\times p_2\n$$\n对于反向提议 $q(\\mathbf{x}|\\mathbf{x}')$，从 $\\mathbf{x}'=(1,1)$ 到达 $\\mathbf{x}=(1,0)$，必须再次选择坐标 $i=2$（概率为 $1/2$），并且新状态必须被提议为 $s_2 = 0$（概率为 $1-p_2$）。因此：\n$$\nq(\\mathbf{x}|\\mathbf{x}') = q((1,0)|(1,1)) = \\frac{1}{2} \\times (1-p_2)\n$$\n提议概率的比率为：\n$$\n\\frac{q(\\mathbf{x}|\\mathbf{x}')}{q(\\mathbf{x}'|\\mathbf{x})} = \\frac{\\frac{1}{2}(1-p_2)}{\\frac{1}{2}p_2} = \\frac{1-p_2}{p_2}\n$$\n现在我们计算能量变化 $\\Delta U = U(\\mathbf{x}') - U(\\mathbf{x})$。能量函数为 $U(\\mathbf{s}) = \\theta_1 s_1 + \\theta_2 s_2 + W s_1 s_2$。\n对于当前状态 $\\mathbf{x}=(1,0)$：\n$$\nU(\\mathbf{x}) = U(1,0) = \\theta_1(1) + \\theta_2(0) + W(1)(0) = \\theta_1\n$$\n对于提议状态 $\\mathbf{x}'=(1,1)$：\n$$\nU(\\mathbf{x}') = U(1,1) = \\theta_1(1) + \\theta_2(1) + W(1)(1) = \\theta_1 + \\theta_2 + W\n$$\n能量变化为：\n$$\n\\Delta U = U(\\mathbf{x}') - U(\\mathbf{x}) = (\\theta_1 + \\theta_2 + W) - \\theta_1 = \\theta_2 + W\n$$\n将这些结果代入接受概率公式：\n$$\n\\alpha(\\mathbf{x} \\to \\mathbf{x}') = \\min\\left(1, \\exp(-(\\theta_2 + W)) \\frac{1-p_2}{p_2}\\right)\n$$\n问题陈述，接受的神经实现是基于泊松过程的，其中接受概率是在时间窗口 $T$ 内至少出现一个脉冲的概率，由 $1 - \\exp(-\\lambda T)$ 给出。为确保细致平衡，这个神经接受概率必须等于推导出的 MH 接受概率：\n$$\n1 - \\exp(-\\lambda T) = \\alpha(\\mathbf{x} \\to \\mathbf{x}')\n$$\n求解 $\\lambda$：\n$$\n\\exp(-\\lambda T) = 1 - \\alpha(\\mathbf{x} \\to \\mathbf{x}')\n$$\n$$\n-\\lambda T = \\ln\\big(1 - \\alpha(\\mathbf{x} \\to \\mathbf{x}')\\big)\n$$\n$$\n\\lambda = -\\frac{1}{T} \\ln\\big(1 - \\alpha(\\mathbf{x} \\to \\mathbf{x}')\\big)\n$$\n代入 $\\alpha(\\mathbf{x} \\to \\mathbf{x}')$ 的表达式：\n$$\n\\lambda = -\\frac{1}{T} \\ln\\left(1 - \\min\\left(1, \\exp(-(\\theta_2 + W)) \\frac{1-p_2}{p_2}\\right)\\right)\n$$\n现在我们可以用给定的参数数值计算这个表达式：$\\theta_2 = -0.3$，$W = 1.1$，$p_2 = 0.8$ 以及 $T = 20\\,\\text{ms} = 20 \\times 10^{-3}\\,\\text{s}$。\n首先，计算 $\\min$ 函数的参数：\n$$\n\\exp(-(\\theta_2 + W)) \\frac{1-p_2}{p_2} = \\exp(-(-0.3 + 1.1)) \\frac{1-0.8}{0.8} = \\exp(-0.8) \\frac{0.2}{0.8} = 0.25\\exp(-0.8)\n$$\n数值上，$\\exp(-0.8) \\approx 0.449329$。所以该项约等于 $0.25 \\times 0.449329 = 0.11233225$。由于这个值小于 1，$\\min$ 函数返回此值。\n因此，接受概率为 $\\alpha(\\mathbf{x} \\to \\mathbf{x}')=0.25\\exp(-0.8)$。\n现在我们可以计算 $\\lambda$：\n$$\n\\lambda = -\\frac{1}{20 \\times 10^{-3}} \\ln\\big(1 - 0.25\\exp(-0.8)\\big)\n$$\n$$\n\\lambda = -50 \\ln\\big(1 - 0.25\\exp(-0.8)\\big)\n$$\n$$\n\\lambda \\approx -50 \\ln(1 - 0.11233225) = -50 \\ln(0.88766775)\n$$\n$$\n\\lambda \\approx -50 \\times (-0.119159) \\approx 5.95795 \\, \\text{Hz}\n$$\n四舍五入到四位有效数字，我们得到 $\\lambda = 5.958\\,\\text{Hz}$。",
            "answer": "$$\\boxed{5.958}$$"
        },
        {
            "introduction": "神经采样器的理论模型是一种理想化情况；真实的神经形态硬件会表现出增益和偏移失真等不完美之处。这项动手任务将引导您完成校准和验证基于硬件的二元采样器的完整工作流程。该过程利用在 logit 变换空间中的线性回归来估计硬件参数，并使用像 KL 散度这样的信息论度量来量化校准后模型的性能，为您提供了处理和表征真实神经形态系统所必需的实用方法。",
            "id": "4052528",
            "problem": "给定一个神经形态硬件上的二进制脉冲神经采样器，在一个固定的观测窗口内，它以一个由输入场的logistic函数建模的概率发射脉冲。在一个理想的logistic二进制采样器中，作为标量场 $h$ 的函数，脉冲概率为 $p^{\\ast}(h) = \\sigma(h)$，其中 $\\sigma(u) = 1/(1 + e^{-u})$ 是logistic函数。在真实硬件上，会出现未知的增益和偏移失真，因此采样器实际上实现的是 $p(h) = \\sigma(\\alpha h + \\gamma)$，其中 $\\alpha$ 和 $\\gamma$ 是未知的实数，可能会随时间或配置而漂移。目标是通过测量的训练响应来估计 $(\\alpha, \\gamma)$ 以校准采样器，然后通过量化其与理想采样器的散度，在留出测试集上验证校准后的模型。\n\n基本概念：\n- 一个脉冲概率为 $p$ 的二进制随机变量 $x \\in \\{0,1\\}$ 服从伯努利分布 $\\mathsf{Bernoulli}(p)$。\n- logistic链接定义为 $\\sigma(u) = 1/(1+e^{-u})$，适用于任何实数 $u$。\n- 对数几率（logit）函数为 $\\operatorname{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$，适用于任何 $p \\in (0,1)$。\n- 从 $\\mathsf{Bernoulli}(p)$ 到 $\\mathsf{Bernoulli}(q)$ 的Kullback–Leibler散度（KLD）为 $\\mathrm{KL}(p \\Vert q) = p \\ln\\left(\\frac{p}{q}\\right) + (1-p)\\ln\\left(\\frac{1-p}{1-q}\\right)$。\n\n校准问题：\n- 您将获得由一小组场 $h_i$ 组成的训练测量数据，以及对于每个 $h_i$，在 $n_i$ 次试验中观测到的脉冲数 $k_i$。使用参数为 $\\delta$ 的拉普拉斯平滑来避免接近0和1的退化概率，定义经验概率估计为 $\\hat{p}_i = \\frac{k_i + \\delta}{n_i + 2\\delta}$。假设硬件遵循logistic模型 $p(h) = \\sigma(\\alpha h + \\gamma)$。在logit域中，这意味着 $\\operatorname{logit}(\\hat{p}_i) \\approx \\alpha h_i + \\gamma$ 加上采样噪声。通过在logit域中对訓練數據应用线性最小二乘法来估计 $(\\alpha, \\gamma)$。\n\n验证问题：\n- 给定留出的测试场 $h_j$ 和在 $n_j$ 次试验中测得的脉冲数 $k_j$（使用相同的平滑方法），计算两个验证指标：\n  1. 理想目标概率 $p^{\\ast}(h_j) = \\sigma(h_j)$ 与校准模型的预测概率 $q(h_j) = \\sigma(\\alpha h_j + \\gamma)$ 之间的平均Kullback–Leibler散度：\n  $$\\overline{\\mathrm{KL}} = \\frac{1}{m}\\sum_{j=1}^{m}\\left[p^{\\ast}(h_j)\\ln\\left(\\frac{p^{\\ast}(h_j)}{q(h_j)}\\right) + (1-p^{\\ast}(h_j))\\ln\\left(\\frac{1-p^{\\ast}(h_j)}{1-q(h_j)}\\right)\\right],$$\n  其中 $m$ 是测试场的数量。\n  2. 测试测量数据上的最大绝对logit残差，定义为：\n  $$R_{\\max} = \\max_{j}\\left|\\operatorname{logit}\\!\\left(\\frac{k_j+\\delta}{n_j+2\\delta}\\right) - (\\alpha h_j + \\gamma)\\right|.$$\n\n您的任务是编写一个完整的、可运行的程序，为每个提供的测试用例执行校准和验证。该程序必须：\n- 在logit域中实现logistic函数 $\\sigma(u)$、logit函数、拉普拉斯平滑和线性最小二乘法。\n- 对于每个测试用例，在训练集上估计 $(\\alpha, \\gamma)$，使用校准模型与理想目标在测试集上计算 $\\overline{\\mathrm{KL}}$，并使用测量的测试计数计算 $R_{\\max}$。\n- 通过检查 $\\overline{\\mathrm{KL}}$ 是否小于或等于特定用例的容差 $\\tau$ 来决定每个用例的通过/不通过状态。\n\n测试套件：\n- 提供了三个测试用例。每个用例都提供 $h_{\\text{train}}$、$k_{\\text{train}}$、$n_{\\text{train}}$、$h_{\\text{test}}$、$k_{\\text{test}}$、$n_{\\text{test}}$ 和一个容差 $\\tau$，并且所有用例都使用平滑参数 $\\delta = 1$。所有 $h$ 值都是纯实数（没有物理单位）。测试套件涵盖了一个典型的良态场景、一个饱和边界情况以及一个漂移/噪声情况。\n\n用例A（良态）：\n- $h_{\\text{train}} = [-2.0, 0.0, 2.0]$\n- $n_{\\text{train}} = [1000, 1000, 1000]$\n- $k_{\\text{train}} = [154, 525, 870]$\n- $h_{\\text{test}} = [-3.0, -1.0, 1.0, 3.0]$\n- $n_{\\text{test}} = [1000, 1000, 1000, 1000]$\n- $k_{\\text{test}} = [69, 310, 731, 943]$\n- $\\tau = 0.01$\n\n用例B（饱和边界）：\n- $h_{\\text{train}} = [-1.0, 0.0, 1.0]$\n- $n_{\\text{train}} = [200, 200, 200]$\n- $k_{\\text{train}} = [4, 100, 196]$\n- $h_{\\text{test}} = [-2.0, -0.5, 0.5, 2.0]$\n- $n_{\\text{test}} = [200, 200, 200, 200]$\n- $k_{\\text{test}} = [0, 24, 176, 200]$\n- $\\tau = 0.05$\n\n用例C（漂移/噪声情况）：\n- $h_{\\text{train}} = [-1.0, 0.0, 1.0]$\n- $n_{\\text{train}} = [500, 500, 500]$\n- $k_{\\text{train}} = [99, 225, 366]$\n- $h_{\\text{test}} = [-1.0, 0.0, 1.0, 2.0]$\n- $n_{\\text{test}} = [500, 500, 500, 500]$\n- $k_{\\text{test}} = [60, 189, 366, 462]$\n- $\\tau = 0.03$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个用例的结果。结果是一个用方括号括起来的逗号分隔列表，其中每个用例的结果本身是一个包含四个值的列表 $[\\alpha, \\gamma, \\overline{\\mathrm{KL}}, \\text{pass}]$。例如：$[[\\alpha_1,\\gamma_1,\\mathrm{KL}_1,\\text{True}],[\\alpha_2,\\gamma_2,\\mathrm{KL}_2,\\text{False}],[\\alpha_3,\\gamma_3,\\mathrm{KL}_3,\\text{True}]]$。数值结果应为标准十进制浮点数，通过/不通过状态应为布尔值。此问题中不涉及角度，也没有物理单位。",
            "solution": "该问题要求对一个二进制脉冲神经采样器进行校准和验证。该过程涉及估计失真logistic响应模型的参数，然后量化该模型与理想采样器之间的偏差。解决方案分三个阶段进行：首先，定义必要的数学函数和模型；其次，指定用于估计未知硬件参数的校准程序；第三，详细说明用于评估校准模型的验证指标。\n\n首先，我们定义模型的核心组件。采样器的脉冲概率由logistic函数 $\\sigma(u) = (1 + e^{-u})^{-1}$ 建模。理想采样器的脉冲概率为 $p^{\\ast}(h) = \\sigma(h)$，其中 $h$ 是输入场。硬件实现存在失真，其响应为 $p(h) = \\sigma(\\alpha h + \\gamma)$，其中 $\\alpha$ 和 $\\gamma$ 分别是未知的增益和偏移参数。分析在对数几率（logit）域中进行，使用函数 $\\operatorname{logit}(p) = \\ln(p/(1-p))$，该函数是logistic函数的逆函数。这一变换是关键，因为它使响应模型线性化：$\\operatorname{logit}(p(h)) = \\alpha h + \\gamma$。\n\n对于给定的场 $h_i$，来自 $n_i$ 次试验的脉冲计数数据得到 $k_i$ 个脉冲。脉冲的经验概率估计为 $\\hat{p}_i$。为了处理 $k_i=0$ 或 $k_i=n_i$ 的情况（这会导致未定义的logit值），我们使用拉普拉斯平滑。使用平滑参数 $\\delta$，平滑后的经验概率由下式给出：\n$$ \\hat{p}_i = \\frac{k_i + \\delta}{n_i + 2\\delta} $$\n对于此问题，指定 $\\delta=1$。这确保了 $\\hat{p}_i \\in (0, 1)$。\n\n第二阶段是校准，我们使用一组训练数据 $\\{ (h_i, k_i, n_i) \\}_{i=1}^N$ 来估计 $(\\alpha, \\gamma)$。我们将经验概率转换到logit域，创建一组数据点 $(h_i, y_i)$，其中 $y_i = \\operatorname{logit}(\\hat{p}_i)$。基于我们的线性化模型，我们期望关系式为 $y_i \\approx \\alpha h_i + \\gamma$。我们可以通过最小化平方误差和 $S(\\alpha, \\gamma) = \\sum_{i=1}^N (y_i - (\\alpha h_i + \\gamma))^2$ 来找到最佳拟合参数 $(\\alpha, \\gamma)$。这是一个标准的线性最小二乘回归问题。我们寻求找到一个参数向量 $\\mathbf{x} = [\\alpha, \\gamma]^T$，以最佳方式求解线性方程组 $\\mathbf{A}\\mathbf{x} \\approx \\mathbf{y}$，其中：\n$$ \\mathbf{A} = \\begin{pmatrix} h_1  1 \\\\ h_2  1 \\\\ \\vdots  \\vdots \\\\ h_N  1 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix} $$\n最小二乘解由正规方程给出，$\\mathbf{x} = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{y}$。这为硬件参数提供了估计值 $(\\hat{\\alpha}, \\hat{\\gamma})$。为简洁起见，我们将这些估计值称为 $(\\alpha, \\gamma)$。\n\n第三阶段是验证。使用估计的参数 $(\\alpha, \\gamma)$ 和一个留出的测试数据集 $\\{ (h_j, k_j, n_j) \\}_{j=1}^m$，我们计算两个指标。\n\n第一个指标是平均Kullback–Leibler（KL）散度, $\\overline{\\mathrm{KL}}$。它衡量了使用校准模型 $q(h_j) = \\sigma(\\alpha h_j + \\gamma)$ 来近似理想模型 $p^{\\ast}(h_j) = \\sigma(h_j)$ 时的平均信息损失。单个脉冲事件是一个伯努利试验，从 $\\mathsf{Bernoulli}(p)$ 分布到 $\\mathsf{Bernoulli}(q)$ 分布的KL散度为 $\\mathrm{KL}(p \\Vert q) = p \\ln(p/q) + (1-p)\\ln((1-p)/(1-q))$。在 $m$ 个测试场上的平均KL散度为：\n$$ \\overline{\\mathrm{KL}} = \\frac{1}{m}\\sum_{j=1}^{m} \\mathrm{KL}(p^{\\ast}(h_j) \\Vert q(h_j)) = \\frac{1}{m}\\sum_{j=1}^{m} \\left[ p^{\\ast}(h_j)\\ln\\left(\\frac{p^{\\ast}(h_j)}{q(h_j)}\\right) + (1-p^{\\ast}(h_j))\\ln\\left(\\frac{1-p^{\\ast}(h_j)}{1-q(h_j)}\\right) \\right] $$\n如果这个 $\\overline{\\mathrm{KL}}$ 值小于或等于指定的容差 $\\tau$，则认为采样器在给定测试用例下的性能是可接受的。\n\n第二个指标，最大绝对logit残差 $R_{\\max}$，量化了校准线性模型对经验测试数据的拟合优度。它是模型在logit域中的预测与测量的测试概率的logit值之间的最大偏差：\n$$ R_{\\max} = \\max_{j}\\left|\\operatorname{logit}\\!\\left(\\frac{k_j+\\delta}{n_j+2\\delta}\\right) - (\\alpha h_j + \\gamma)\\right| $$\n虽然该指标提供了关于模型与数据不匹配的有价值的诊断信息，但在此问题中它不用于通过/不通过的判断标准。\n\n实现将首先为 $\\sigma(u)$、$\\operatorname{logit}(p)$、拉普拉斯平滑和KL散度定义函数。对于每个测试用例，程序将：\n1. 获取训练数据 $(h_{\\text{train}}, k_{\\text{train}}, n_{\\text{train}})$ 和 $\\delta=1$。\n2. 计算平滑概率 $\\hat{p}_i$ 及其logit变换 $y_i$。\n3. 对 $h_i$ 上的 $y_i$ 执行线性最小二乘回归以找到 $(\\alpha, \\gamma)$。\n4. 获取测试数据 $(h_{\\text{test}}, k_{\\text{test}}, n_{\\text{test}})$ 和估计的 $(\\alpha, \\gamma)$。\n5. 计算理想概率 $p^{\\ast}(h_j)$ 和校准模型概率 $q(h_j)$。\n6. 通过对所有测试点上的 $\\mathrm{KL}(p^{\\ast}(h_j) \\Vert q(h_j))$求平均来计算平均KL散度 $\\overline{\\mathrm{KL}}$。\n7. 将 $\\overline{\\mathrm{KL}}$ 与特定用例的容差 $\\tau$ 进行比较，以确定通过/不通过状态。\n8. 为每个用例组装结果 $[\\alpha, \\gamma, \\overline{\\mathrm{KL}}, \\text{pass}]$，并将其格式化为指定的最终输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs calibration and validation for a binary spiking neural sampler\n    across multiple test cases as specified in the problem statement.\n    \"\"\"\n    delta = 1.0\n\n    test_cases = [\n        {\n            \"name\": \"Case A (well-conditioned)\",\n            \"h_train\": np.array([-2.0, 0.0, 2.0]),\n            \"n_train\": np.array([1000, 1000, 1000]),\n            \"k_train\": np.array([154, 525, 870]),\n            \"h_test\": np.array([-3.0, -1.0, 1.0, 3.0]),\n            \"n_test\": np.array([1000, 1000, 1000, 1000]),\n            \"k_test\": np.array([69, 310, 731, 943]),\n            \"tau\": 0.01\n        },\n        {\n            \"name\": \"Case B (saturation boundary)\",\n            \"h_train\": np.array([-1.0, 0.0, 1.0]),\n            \"n_train\": np.array([200, 200, 200]),\n            \"k_train\": np.array([4, 100, 196]),\n            \"h_test\": np.array([-2.0, -0.5, 0.5, 2.0]),\n            \"n_test\": np.array([200, 200, 200, 200]),\n            \"k_test\": np.array([0, 24, 176, 200]),\n            \"tau\": 0.05\n        },\n        {\n            \"name\": \"Case C (drift/noise case)\",\n            \"h_train\": np.array([-1.0, 0.0, 1.0]),\n            \"n_train\": np.array([500, 500, 500]),\n            \"k_train\": np.array([99, 225, 366]),\n            \"h_test\": np.array([-1.0, 0.0, 1.0, 2.0]),\n            \"n_test\": np.array([500, 500, 500, 500]),\n            \"k_test\": np.array([60, 189, 366, 462]),\n            \"tau\": 0.03\n        }\n    ]\n\n    def sigma(u):\n        \"\"\"Logistic function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-u))\n\n    def logit(p):\n        \"\"\"Logit function.\"\"\"\n        # Add small epsilon to avoid log(0) if p is exactly 0 or 1,\n        # though Laplace smoothing should prevent this.\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        return np.log(p / (1.0 - p))\n\n    def laplace_smooth(k, n, delta_val):\n        \"\"\"Empirical probability with Laplace smoothing.\"\"\"\n        return (k + delta_val) / (n + 2.0 * delta_val)\n    \n    def kl_divergence_bernoulli(p, q):\n        \"\"\"KL divergence between two Bernoulli distributions.\"\"\"\n        # Clip probabilities to avoid log(0)\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        q = np.clip(q, 1e-15, 1 - 1e-15)\n        return p * np.log(p / q) + (1.0 - p) * np.log((1.0 - p) / (1.0 - q))\n\n    all_results = []\n\n    for case in test_cases:\n        # --- Calibration Step ---\n        # 1. Calculate smoothed empirical probabilities\n        p_hat_train = laplace_smooth(case[\"k_train\"], case[\"n_train\"], delta)\n\n        # 2. Transform to logit domain\n        y_train = logit(p_hat_train)\n        \n        # 3. Perform linear least squares to find alpha and gamma\n        # We are fitting y = alpha * h + gamma\n        A = np.vstack([case[\"h_train\"], np.ones_like(case[\"h_train\"])]).T\n        solution, _, _, _ = np.linalg.lstsq(A, y_train, rcond=None)\n        alpha, gamma = solution[0], solution[1]\n\n        # --- Validation Step ---\n        h_test = case[\"h_test\"]\n        \n        # 1. Calculate Mean KL Divergence\n        p_star = sigma(h_test)  # Ideal probabilities\n        q_calibrated = sigma(alpha * h_test + gamma) # Calibrated model probabilities\n        \n        kl_divs = kl_divergence_bernoulli(p_star, q_calibrated)\n        mean_kl = np.mean(kl_divs)\n\n        # 2. (Optional, for completeness) Calculate Max Absolute Logit Residual\n        p_hat_test = laplace_smooth(case[\"k_test\"], case[\"n_test\"], delta)\n        logit_p_test = logit(p_hat_test)\n        model_logit_preds = alpha * h_test + gamma\n        r_max = np.max(np.abs(logit_p_test - model_logit_preds))\n\n        # 3. Pass/Fail decision\n        passed = mean_kl = case[\"tau\"]\n        \n        all_results.append([alpha, gamma, mean_kl, passed])\n\n    # --- Final Output Formatting ---\n    # Manually format the string to avoid spaces after commas\n    case_strings = []\n    for result in all_results:\n        # str(boolean) gives 'True' or 'False' as required\n        case_str = f\"[{result[0]},{result[1]},{result[2]},{str(result[3])}]\"\n        case_strings.append(case_str)\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}