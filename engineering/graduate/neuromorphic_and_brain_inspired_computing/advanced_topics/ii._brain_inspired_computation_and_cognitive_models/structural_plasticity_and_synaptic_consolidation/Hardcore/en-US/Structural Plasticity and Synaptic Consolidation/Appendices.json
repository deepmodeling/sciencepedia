{
    "hands_on_practices": [
        {
            "introduction": "To build a robust understanding of synaptic consolidation, we begin with its most fundamental mathematical description: the cascade model. This exercise explores a simplified synapse where a fast, labile efficacy trace $w(t)$ is stabilized by a slow, long-term consolidation trace $z(t)$. By solving the coupled differential equations that govern their interaction in response to an idealized learning event, you will derive the system's impulse response, a core skill in analyzing the dynamics of any linear time-invariant system in neuroscience and engineering. ",
            "id": "4060985",
            "problem": "Consider a neuromorphic synapse modeled with two coupled state variables: a fast synaptic efficacy $w(t)$ and a slow consolidation trace $z(t)$. The dynamics are governed by the coupled linear ordinary differential equations\n$$\\dot{w}(t)=-\\lambda\\big(w(t)-z(t)\\big)+\\eta\\,\\Delta(t),\\qquad \\dot{z}(t)=-\\mu\\,z(t)+\\xi\\,\\Delta(t),$$\nwhere $\\lambda>0$ and $\\mu>0$ are rate constants for fast and slow processes, respectively, and $\\eta$ and $\\xi$ are scalar gains scaling the immediate effect of a learning event $\\Delta(t)$ on $w(t)$ and $z(t)$, respectively. The learning event is instantaneous and is modeled as the Dirac delta distribution $\\Delta(t)=\\delta(t)$, and the synapse is initially quiescent with causal initial conditions $w(0^{-})=0$ and $z(0^{-})=0$. Assume $\\lambda\\neq\\mu$.\n\nStarting from the fundamental properties of linear differential equations and the Dirac delta distribution, derive the causal impulse response of the fast efficacy $w(t)$ to this single learning event. Express your final answer as a single closed-form analytic expression in terms of $\\lambda$, $\\mu$, $\\eta$, $\\xi$, and $t$, optionally using the Heaviside step function $H(t)$ to enforce causality. No rounding is required, and no physical units need to be reported.",
            "solution": "### Step 1: Find Initial Conditions at t=0+\nThe presence of the Dirac delta function $\\delta(t)$ as a forcing term will cause an instantaneous jump in the state variables at $t=0$. We determine the values of $w(0^{+})$ and $z(0^{+})$ by integrating the differential equations from $t=0^{-}$ to $t=0^{+}$.\n\nFor the $z(t)$ equation:\n$$ \\int_{0^{-}}^{0^{+}} \\dot{z}(t) \\,dt = \\int_{0^{-}}^{0^{+}} \\big(-\\mu\\,z(t)+\\xi\\,\\delta(t)\\big) \\,dt $$\n$$ z(0^{+}) - z(0^{-}) = -\\mu \\int_{0^{-}}^{0^{+}} z(t) \\,dt + \\xi \\int_{0^{-}}^{0^{+}} \\delta(t) \\,dt $$\nGiven $z(0^{-})=0$, and noting that the integral of a finite function over an infinitesimal interval is zero while the integral of the delta function is one, we get:\n$$ z(0^{+}) - 0 = 0 + \\xi(1) \\implies z(0^{+}) = \\xi $$\n\nFor the $w(t)$ equation:\n$$ \\int_{0^{-}}^{0^{+}} \\dot{w}(t) \\,dt = \\int_{0^{-}}^{0^{+}} \\Big(-\\lambda\\big(w(t)-z(t)\\big)+\\eta\\,\\delta(t)\\Big) \\,dt $$\n$$ w(0^{+}) - w(0^{-}) = -\\lambda \\int_{0^{-}}^{0^{+}} \\big(w(t)-z(t)\\big) \\,dt + \\eta \\int_{0^{-}}^{0^{+}} \\delta(t) \\,dt $$\nGiven $w(0^{-})=0$, we similarly find:\n$$ w(0^{+}) - 0 = 0 + \\eta(1) \\implies w(0^{+}) = \\eta $$\n\n### Step 2: Solve the Homogeneous System for t > 0\nFor $t>0$, the Dirac delta term is zero, and the system of ODEs becomes:\n$$ \\dot{w}(t) = -\\lambda w(t) + \\lambda z(t) \\quad (1) $$\n$$ \\dot{z}(t) = -\\mu z(t) \\quad (2) $$\nThis system is to be solved with the initial conditions $w(0^{+}) = \\eta$ and $z(0^{+}) = \\xi$.\n\nFirst, we solve equation (2), which is a simple first-order homogeneous ODE. Its solution is $z(t) = A \\exp(-\\mu t)$. Using the initial condition $z(0^{+}) = \\xi$:\n$$ z(t) = \\xi \\exp(-\\mu t) \\quad \\text{for } t > 0 $$\n\nNext, substitute this expression for $z(t)$ into equation (1):\n$$ \\dot{w}(t) = -\\lambda w(t) + \\lambda \\xi \\exp(-\\mu t) $$\nThis is a first-order linear inhomogeneous ODE: $\\dot{w}(t) + \\lambda w(t) = \\lambda \\xi \\exp(-\\mu t)$. We solve it using an integrating factor $I(t) = \\exp(\\lambda t)$.\n$$ \\frac{d}{dt} \\big(w(t) \\exp(\\lambda t)\\big) = \\lambda \\xi \\exp((\\lambda - \\mu)t) $$\nIntegrating both sides (and using the given condition $\\lambda \\neq \\mu$):\n$$ w(t) \\exp(\\lambda t) = \\int \\lambda \\xi \\exp((\\lambda - \\mu)t) \\,dt = \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp((\\lambda - \\mu)t) + C $$\nSolving for $w(t)$:\n$$ w(t) = \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(-\\mu t) + C \\exp(-\\lambda t) $$\n\n### Step 3: Apply Initial Condition to Find the Final Solution\nWe use the initial condition $w(0^{+}) = \\eta$ to find the integration constant $C$:\n$$ w(0^{+}) = \\eta = \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(0) + C \\exp(0) $$\n$$ C = \\eta - \\frac{\\lambda \\xi}{\\lambda - \\mu} $$\nSubstituting $C$ back into the expression for $w(t)$:\n$$ w(t) = \\left(\\eta - \\frac{\\lambda \\xi}{\\lambda - \\mu}\\right) \\exp(-\\lambda t) + \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(-\\mu t) $$\nThis solution is valid for $t>0$. To express the causal impulse response for all $t$, we multiply by the Heaviside step function $H(t)$.",
            "answer": "$$ \\boxed{\\left[ \\left( \\eta - \\frac{\\lambda \\xi}{\\lambda - \\mu} \\right) \\exp(-\\lambda t) + \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(-\\mu t) \\right] H(t)} $$"
        },
        {
            "introduction": "While a simple two-state model captures the essence of consolidation, real biological memory is thought to involve a multitude of processes with a wide spectrum of timescales. This practice generalizes the cascade concept to a many-layered system, demonstrating how the collective behavior of numerous simple exponential decays can give rise to a power-law decay in the aggregate memory signal . This is a profound principle that explains how systems built from simple components can exhibit complex, scale-free dynamics, and mastering the integral approximation technique used here is key to analyzing such emergent phenomena.",
            "id": "4060979",
            "problem": "Consider a synaptic consolidation cascade used in neuromorphic and brain-inspired computing to stabilize memories under ongoing random plasticity. A memory is encoded at time $t=0$ as a unit impulse, and the subsequent interference is modeled as a stationary stochastic process that produces independent exponential relaxations at each consolidation level. The cascade consists of $K$ levels indexed by $\\ell=0,1,\\dots,K-1$, each contributing a component characterized by a relaxation time constant $\\tau_{\\ell}$ and an initial weight $w_{\\ell}$ that reflects the probability and magnitude that the encoded trace penetrates to level $\\ell$. Assume the following structural features:\n\n- The time constants are geometrically spaced, $\\tau_{\\ell}=\\tau_{0}\\lambda^{\\ell}$ with $\\lambda>1$ and $\\tau_{0}>0$.\n- The initial weights are geometrically decaying with level, $w_{\\ell}=A\\lambda^{-\\mu\\ell}$ with $A>0$ and real $\\mu$.\n\nDefine the memory signal trace $S(t)$ at retrieval time $t>0$ as the sum of the levelwise exponential decays:\n$$\nS(t)=\\sum_{\\ell=0}^{K-1}w_{\\ell}\\exp\\!\\left(-\\frac{t}{\\tau_{\\ell}}\\right).\n$$\n\nStarting from fundamental facts about linear systems with multiple timescales and the properties of sums and integrals of exponential responses, derive the leading-order asymptotic scaling law of $S(t)$ for large times in the regime $\\tau_{0}\\ll t\\ll\\tau_{K-1}$, treating $K$ as large enough that the level index can be approximated as continuous. Show under what conditions on the parameters $\\lambda$ and $\\mu$ this asymptotic behavior is a power-law decay in $t$. Your final answer must be a single closed-form analytic expression for the leading-order term of $S(t)$ as a function of $t$, $A$, $\\tau_{0}$, $\\lambda$, and $\\mu$. No rounding is required.",
            "solution": "### Step 1: Approximate the Sum with an Integral\nThe memory signal trace is given by the discrete sum:\n$$ S(t) = \\sum_{\\ell=0}^{K-1} w_{\\ell} \\exp\\left(-\\frac{t}{\\tau_{\\ell}}\\right) = \\sum_{\\ell=0}^{K-1} A\\lambda^{-\\mu\\ell} \\exp\\left(-\\frac{t}{\\tau_{0}\\lambda^{\\ell}}\\right) $$\nFor a large number of levels $K$, we can approximate the discrete sum over the index $\\ell$ with an integral over a continuous variable $x$. Since the regime of interest is for intermediate times, we can extend the integration limits from $0$ to $\\infty$ to find the leading-order behavior.\n$$ S(t) \\approx A \\int_{0}^{\\infty} \\lambda^{-\\mu x} \\exp\\left(-\\frac{t}{\\tau_{0}\\lambda^{x}}\\right) dx $$\n\n### Step 2: Change of Variables to Time Constants\nTo solve the integral, we perform a change of variables from the level index $x$ to the time constant $\\tau = \\tau_x = \\tau_{0}\\lambda^{x}$.\nFrom this, we have $\\lambda^{x} = \\tau/\\tau_{0}$, which implies $x \\ln(\\lambda) = \\ln(\\tau/\\tau_{0})$, so $x = \\frac{\\ln(\\tau/\\tau_{0})}{\\ln(\\lambda)}$.\nThe differential is $dx = \\frac{1}{\\tau \\ln(\\lambda)} d\\tau$.\nThe weight term becomes $\\lambda^{-\\mu x} = (\\lambda^{x})^{-\\mu} = (\\tau/\\tau_{0})^{-\\mu}$.\nSubstituting these into the integral, the limits of integration for $\\tau$ become $[\\tau_0, \\infty)$:\n$$ S(t) \\approx A \\int_{\\tau_{0}}^{\\infty} \\left(\\frac{\\tau}{\\tau_{0}}\\right)^{-\\mu} \\exp\\left(-\\frac{t}{\\tau}\\right) \\frac{1}{\\tau \\ln(\\lambda)} d\\tau $$\nRearranging the terms gives:\n$$ S(t) \\approx \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} \\int_{\\tau_{0}}^{\\infty} \\tau^{-\\mu-1} \\exp\\left(-\\frac{t}{\\tau}\\right) d\\tau $$\n\n### Step 3: Second Change of Variables and Asymptotic Analysis\nWe perform another change of variables to simplify the exponential. Let $z = t/\\tau$.\nThis implies $\\tau = t/z$, and the differential is $d\\tau = -\\frac{t}{z^{2}} dz$.\nThe new limits of integration are: when $\\tau=\\tau_{0}$, $z = t/\\tau_{0}$; when $\\tau \\to \\infty$, $z \\to 0$.\nSubstituting these yields:\n$$ S(t) \\approx \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} \\int_{t/\\tau_{0}}^{0} \\left(\\frac{t}{z}\\right)^{-\\mu-1} \\exp(-z) \\left(-\\frac{t}{z^{2}}\\right) dz $$\nUsing the negative sign to reverse the limits of integration and simplifying:\n$$ S(t) \\approx \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} \\int_{0}^{t/\\tau_{0}} t^{-\\mu-1} z^{\\mu+1} \\exp(-z) \\frac{t}{z^{2}} dz = \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} t^{-\\mu} \\int_{0}^{t/\\tau_{0}} z^{\\mu-1} \\exp(-z) dz $$\nThe integral is the lower incomplete gamma function, $\\gamma(\\mu, t/\\tau_{0})$.\nFor the asymptotic behavior at large times ($t \\gg \\tau_{0}$), the upper limit of the integral $t/\\tau_{0} \\to \\infty$. The integral converges to the complete Gamma function, $\\Gamma(\\mu) = \\int_{0}^{\\infty} z^{\\mu-1} \\exp(-z) dz$, provided that the integral is convergent at $z=0$. This requires the exponent $\\mu - 1 > -1$, or $\\mu > 0$.\nUnder this condition, the integral term becomes a constant, $\\Gamma(\\mu)$, and the signal trace has the leading-order asymptotic form:\n$$ S(t) \\approx \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} t^{-\\mu} \\Gamma(\\mu) $$\nThis expression shows a power-law decay in time, $S(t) \\propto t^{-\\mu}$.",
            "answer": "$$\n\\boxed{\\frac{A \\Gamma(\\mu) \\tau_{0}^{\\mu}}{(\\ln \\lambda) t^{\\mu}}}\n$$"
        },
        {
            "introduction": "Having explored the dynamics of individual memory traces, we now scale up to the network level, where the interplay of thousands of plastic synapses determines collective stability. This exercise delves into how structural plasticity, specifically the activity-dependent deletion of synapses, acts as a crucial homeostatic mechanism to prevent runaway excitation in a recurrent network . By applying principles from random matrix theory, you will derive a concrete condition for network stability, quantitatively linking the statistics of microscopic synaptic changes to the macroscopic requirement for a functional and stable computational system.",
            "id": "4061004",
            "problem": "Consider a large recurrent neuromorphic network with $N$ neurons, whose linearized discrete-time dynamics are given by $x(t+1)=W x(t)$, where $W \\in \\mathbb{R}^{N \\times N}$ is the recurrent connectivity matrix. Stability of the linearized dynamics requires the spectral radius $\\rho(W)$ to satisfy $\\rho(W)<1$. Assume the following statistically homogeneous and scientifically realistic setup:\n- Pre-plasticity connectivity is generated by an Erdős–Rényi adjacency mask with independent entries $M_{ij} \\sim \\mathrm{Bernoulli}(c)$ for a fixed connection probability $c \\in (0,1)$, independent of weights.\n- Conditional on $M_{ij}=1$, synaptic weights are independent and identically distributed with zero mean and second moment $\\mathbb{E}[W_{ij}^{2} \\mid M_{ij}=1] = v/N$ for some $v>0$; otherwise $W_{ij}=0$. This yields entrywise variance $\\mathbb{E}[W_{ij}^{2}] = c v/N$.\n- Structural plasticity implements activity-dependent deletion: an existing synapse $(i,j)$ is deleted with probability $d(a_i,a_j)$ that depends on the pre- and post-synaptic activities $a_i$ and $a_j$. Let the expected deletion fraction among existing synapses be $\\phi = \\mathbb{E}[d(a_i,a_j) \\mid M_{ij}=1]$, with $0<\\phi<1$.\n- Synaptic consolidation rescales the surviving weights by a deterministic factor $s>0$ applied uniformly to all surviving synapses.\n\nWork in the large-$N$ limit under the standard independence assumptions and moment conditions of random matrix theory that guarantee the circular law for non-Hermitian matrices with independent and identically distributed entries of variance $\\sigma^{2}/N$. Starting from first principles (linear stability via spectral radius, the definition of entrywise variance under masking and rescaling, and the circular law for the limiting spectral distribution), derive the stability condition on the post-plasticity spectral radius $\\rho(W')$ after activity-dependent deletion and consolidation. Then, assuming $c v s^{2} > 1$ so that stability is not already guaranteed without deletion, determine the minimal expected deletion fraction $\\phi_{\\min}$ required to ensure $\\rho(W')<1$.\n\nExpress your final answer as a single closed-form symbolic expression for $\\phi_{\\min}$ in terms of $c$, $v$, and $s$. No numerical rounding is required. No units are needed.",
            "solution": "### Step 1: Relate Stability to Spectral Radius and Variance\nThe stability of the discrete-time linear dynamics $x(t+1)=W' x(t)$ requires the spectral radius of the post-plasticity matrix $W'$ to be less than one: $\\rho(W') < 1$.\nThe problem states to use the circular law from random matrix theory. For a large random matrix with independent, zero-mean entries of variance $\\sigma^2/N$, the spectral radius is given by $\\rho = \\sigma$.\nTherefore, our task is to find the entrywise variance of the post-plasticity matrix $W'$, which we denote as $\\mathrm{Var}(W'_{ij}) = (\\sigma')^2/N$, and then enforce the stability condition $\\sigma' < 1$.\n\n### Step 2: Calculate the Variance of Post-Plasticity Weights\nLet's determine the statistical properties of the entries $W'_{ij}$ of the post-plasticity matrix. An entry $W'_{ij}$ is non-zero only if an original synapse existed ($M_{ij}=1$), survived the deletion process, and was then rescaled.\n- The probability of a synapse existing initially is $P(M_{ij}=1) = c$.\n- Given a synapse exists, the probability it survives deletion is $1-\\phi$.\n- Thus, the probability that a synapse at position $(i,j)$ exists and survives is $P(\\text{survives}) = c(1-\\phi)$.\n- If the synapse survives, its new weight is $s W_{ij}$. If it does not, its weight is 0.\n\nFirst, we find the mean of the new weights, $E[W'_{ij}]$. Since the original weights $W_{ij}$ have zero mean, and the scaling and deletion processes are independent of the sign of the weight, the new weights will also have zero mean: $E[W'_{ij}]=0$.\n\nNext, we calculate the variance, which is equal to the second moment $E[(W'_{ij})^2]$ since the mean is zero. We use the law of total expectation:\n$$ E[(W'_{ij})^2] = P(\\text{survives}) \\cdot E[(W'_{ij})^2 | \\text{survives}] + P(\\text{doesn't survive}) \\cdot E[(W'_{ij})^2 | \\text{doesn't survive}] $$\nThe second term is zero. The value of $W'_{ij}$ given survival is $s W_{ij}$. The expectation is conditioned on the synapse existing in the first place, so we use the conditional second moment of the original weights.\n$$ E[(W'_{ij})^2] = c(1-\\phi) \\cdot E[(s W_{ij})^2 | M_{ij}=1] $$\n$$ E[(W'_{ij})^2] = c(1-\\phi) s^2 \\cdot E[W_{ij}^2 | M_{ij}=1] $$\nWe are given $E[W_{ij}^{2} \\mid M_{ij}=1] = v/N$. Substituting this in:\n$$ \\mathrm{Var}(W'_{ij}) = E[(W'_{ij})^2] = c(1-\\phi) s^2 \\frac{v}{N} $$\n\n### Step 3: Apply the Circular Law and Solve for $\\phi_{\\min}$\nBy comparing our result with the general form $\\mathrm{Var}(W'_{ij}) = (\\sigma')^2/N$, we identify $(\\sigma')^2 = c s^2 v (1-\\phi)$.\nThe spectral radius is therefore $\\rho(W') = \\sigma' = \\sqrt{c s^2 v (1-\\phi)}$.\nThe stability condition $\\rho(W') < 1$ becomes:\n$$ \\sqrt{c s^2 v (1-\\phi)} < 1 $$\nSquaring both sides (all terms are non-negative):\n$$ c s^2 v (1-\\phi) < 1 $$\nThe problem states that $c v s^2 > 1$, so the network would be unstable without deletion ($\\phi=0$). We need to find the minimal $\\phi$ to ensure stability. We rearrange the inequality to solve for $\\phi$:\n$$ 1-\\phi < \\frac{1}{c s^2 v} $$\n$$ -\\phi < \\frac{1}{c s^2 v} - 1 $$\n$$ \\phi > 1 - \\frac{1}{c s^2 v} $$\nThis inequality gives the range of deletion fractions that stabilize the network. The minimal required deletion fraction, $\\phi_{\\min}$, is the lower bound of this range.\n$$ \\phi_{\\min} = 1 - \\frac{1}{c s^2 v} $$",
            "answer": "$$\n\\boxed{1 - \\frac{1}{cvs^{2}}}\n$$"
        }
    ]
}