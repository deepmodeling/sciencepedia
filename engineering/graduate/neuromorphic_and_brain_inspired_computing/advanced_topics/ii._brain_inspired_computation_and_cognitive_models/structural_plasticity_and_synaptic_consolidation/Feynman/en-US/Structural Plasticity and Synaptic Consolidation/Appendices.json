{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of computational memory research is the idea that learning unfolds over multiple timescales. This concept is often captured by \"cascade models,\" where a volatile, short-term memory trace gradually consolidates into a more stable, long-term form. This exercise explores the simplest version of such a model, using a system of two coupled linear ordinary differential equations to represent a \"fast\" synaptic efficacy $w(t)$ and a \"slow\" consolidation trace $z(t)$. By deriving the system's impulse response to a single, instantaneous learning event , you will uncover the fundamental dynamics of how a memory is initially captured and subsequently stabilized over time, providing a crucial foundation for understanding more complex theories of synaptic consolidation.",
            "id": "4060985",
            "problem": "Consider a neuromorphic synapse modeled with two coupled state variables: a fast synaptic efficacy $w(t)$ and a slow consolidation trace $z(t)$. The dynamics are governed by the coupled linear ordinary differential equations\n$$\\dot{w}(t)=-\\lambda\\big(w(t)-z(t)\\big)+\\eta\\,\\Delta(t),\\qquad \\dot{z}(t)=-\\mu\\,z(t)+\\xi\\,\\Delta(t),$$\nwhere $\\lambda>0$ and $\\mu>0$ are rate constants for fast and slow processes, respectively, and $\\eta$ and $\\xi$ are scalar gains scaling the immediate effect of a learning event $\\Delta(t)$ on $w(t)$ and $z(t)$, respectively. The learning event is instantaneous and is modeled as the Dirac delta distribution $\\Delta(t)=\\delta(t)$, and the synapse is initially quiescent with causal initial conditions $w(0^{-})=0$ and $z(0^{-})=0$. Assume $\\lambda\\neq\\mu$.\n\nStarting from the fundamental properties of linear differential equations and the Dirac delta distribution, derive the causal impulse response of the fast efficacy $w(t)$ to this single learning event. Express your final answer as a single closed-form analytic expression in terms of $\\lambda$, $\\mu$, $\\eta$, $\\xi$, and $t$, optionally using the Heaviside step function $H(t)$ to enforce causality. No rounding is required, and no physical units need to be reported.",
            "solution": "The objective is to find the impulse response for $w(t)$. The presence of the Dirac delta function $\\delta(t)$ as a forcing term will cause an instantaneous change in the state variables at $t=0$. We can determine the values of $w(0^{+})$ and $z(0^{+})$ by integrating the given differential equations across the origin, from $t=0^{-}$ to $t=0^{+}$.\n\nFirst, consider the equation for $z(t)$:\n$$ \\dot{z}(t)=-\\mu\\,z(t)+\\xi\\,\\delta(t) $$\nIntegrating from $t=0^{-}$ to $t=0^{+}$:\n$$ \\int_{0^{-}}^{0^{+}} \\dot{z}(t) \\,dt = \\int_{0^{-}}^{0^{+}} \\big(-\\mu\\,z(t)+\\xi\\,\\delta(t)\\big) \\,dt $$\n$$ z(0^{+}) - z(0^{-}) = -\\mu \\int_{0^{-}}^{0^{+}} z(t) \\,dt + \\xi \\int_{0^{-}}^{0^{+}} \\delta(t) \\,dt $$\nGiven the initial condition $z(0^{-})=0$, and noting that the integral of a finite-valued function over an infinitesimal interval is zero, while $\\int_{0^{-}}^{0^{+}} \\delta(t) \\,dt = 1$, we get:\n$$ z(0^{+}) - 0 = 0 + \\xi(1) \\implies z(0^{+}) = \\xi $$\n\nNext, we follow the same procedure for the $w(t)$ equation:\n$$ \\dot{w}(t)=-\\lambda\\big(w(t)-z(t)\\big)+\\eta\\,\\delta(t) $$\nIntegrating from $t=0^{-}$ to $t=0^{+}$:\n$$ w(0^{+}) - w(0^{-}) = -\\lambda \\int_{0^{-}}^{0^{+}} \\big(w(t)-z(t)\\big) \\,dt + \\eta \\int_{0^{-}}^{0^{+}} \\delta(t) \\,dt $$\nGiven $w(0^{-})=0$, this yields:\n$$ w(0^{+}) - 0 = 0 + \\eta(1) \\implies w(0^{+}) = \\eta $$\n\nNow, for $t>0$, the Dirac delta term is zero, and the system of ODEs becomes:\n$$ \\dot{w}(t) = -\\lambda w(t) + \\lambda z(t) \\quad (1) $$\n$$ \\dot{z}(t) = -\\mu z(t) \\quad (2) $$\nThis system is solved for $t>0$ with the new initial conditions $w(0^{+}) = \\eta$ and $z(0^{+}) = \\xi$.\n\nWe solve equation $(2)$ first. Its solution is $z(t) = C_z \\exp(-\\mu t)$. Using the initial condition $z(0^{+}) = \\xi$, we find $C_z=\\xi$. So, for $t>0$:\n$$ z(t) = \\xi \\exp(-\\mu t) $$\n\nNow, substitute this into equation $(1)$:\n$$ \\dot{w}(t) + \\lambda w(t) = \\lambda \\xi \\exp(-\\mu t) $$\nThis is a first-order linear inhomogeneous ODE. The solution is the sum of the homogeneous solution $w_h(t) = C_w \\exp(-\\lambda t)$ and a particular solution. We seek a particular solution of the form $w_p(t) = A \\exp(-\\mu t)$. Substituting this into the equation gives:\n$$ -A\\mu \\exp(-\\mu t) + \\lambda A \\exp(-\\mu t) = \\lambda \\xi \\exp(-\\mu t) $$\n$$ A(\\lambda - \\mu) = \\lambda \\xi \\implies A = \\frac{\\lambda \\xi}{\\lambda - \\mu} $$\nThis is valid as $\\lambda \\neq \\mu$ is given. The general solution for $w(t)$ is:\n$$ w(t) = C_w \\exp(-\\lambda t) + \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(-\\mu t) $$\nTo find the constant $C_w$, we apply the initial condition $w(0^{+}) = \\eta$:\n$$ \\eta = C_w \\exp(0) + \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(0) \\implies C_w = \\eta - \\frac{\\lambda \\xi}{\\lambda - \\mu} $$\nSubstituting $C_w$ back, we get the solution for $t>0$:\n$$ w(t) = \\left(\\eta - \\frac{\\lambda \\xi}{\\lambda - \\mu}\\right) \\exp(-\\lambda t) + \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(-\\mu t) $$\nThe causal impulse response is zero for $t<0$. We can write a single expression using the Heaviside step function $H(t)$:\n$$ w(t) = \\left[ \\left(\\eta - \\frac{\\lambda \\xi}{\\lambda - \\mu}\\right) \\exp(-\\lambda t) + \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(-\\mu t) \\right] H(t) $$",
            "answer": "$$ \\boxed{\\left[ \\left( \\eta - \\frac{\\lambda \\xi}{\\lambda - \\mu} \\right) \\exp(-\\lambda t) + \\frac{\\lambda \\xi}{\\lambda - \\mu} \\exp(-\\mu t) \\right] H(t)} $$"
        },
        {
            "introduction": "While simple models with a few timescales can explain short-term consolidation, biological memory is characterized by its remarkable persistence over days, months, and years, often exhibiting a power-law decay rather than a simple exponential one. This exercise delves into a powerful theoretical model that explains how such long-term stability can emerge from a \"deep\" cascade of synaptic processes. By analyzing a system with many consolidation levels whose time constants are geometrically spaced , you will use integral approximation techniques to show how a sum of exponential decays can give rise to power-law forgetting, a key mechanism for achieving robust long-term memory in both biological and neuromorphic systems.",
            "id": "4060979",
            "problem": "Consider a synaptic consolidation cascade used in neuromorphic and brain-inspired computing to stabilize memories under ongoing random plasticity. A memory is encoded at time $t=0$ as a unit impulse, and the subsequent interference is modeled as a stationary stochastic process that produces independent exponential relaxations at each consolidation level. The cascade consists of $K$ levels indexed by $\\ell=0,1,\\dots,K-1$, each contributing a component characterized by a relaxation time constant $\\tau_{\\ell}$ and an initial weight $w_{\\ell}$ that reflects the probability and magnitude that the encoded trace penetrates to level $\\ell$. Assume the following structural features:\n\n- The time constants are geometrically spaced, $\\tau_{\\ell}=\\tau_{0}\\lambda^{\\ell}$ with $\\lambda>1$ and $\\tau_{0}>0$.\n- The initial weights are geometrically decaying with level, $w_{\\ell}=A\\lambda^{-\\mu\\ell}$ with $A>0$ and real $\\mu$.\n\nDefine the memory signal trace $S(t)$ at retrieval time $t>0$ as the sum of the levelwise exponential decays:\n$$\nS(t)=\\sum_{\\ell=0}^{K-1}w_{\\ell}\\exp\\!\\left(-\\frac{t}{\\tau_{\\ell}}\\right).\n$$\n\nStarting from fundamental facts about linear systems with multiple timescales and the properties of sums and integrals of exponential responses, derive the leading-order asymptotic scaling law of $S(t)$ for large times in the regime $\\tau_{0}\\ll t\\ll\\tau_{K-1}$, treating $K$ as large enough that the level index can be approximated as continuous. Show under what conditions on the parameters $\\lambda$ and $\\mu$ this asymptotic behavior is a power-law decay in $t$. Your final answer must be a single closed-form analytic expression for the leading-order term of $S(t)$ as a function of $t$, $A$, $\\tau_{0}$, $\\lambda$, and $\\mu$. No rounding is required.",
            "solution": "To find the asymptotic scaling law, we first approximate the discrete sum over levels $\\ell$ with an integral over a continuous variable $x$. The sum is given by:\n$$ S(t) = \\sum_{\\ell=0}^{K-1} A\\lambda^{-\\mu\\ell} \\exp\\left(-\\frac{t}{\\tau_{0}\\lambda^{\\ell}}\\right) $$\nFor a large number of levels $K$, the sum can be approximated by an integral where the summation index $\\ell$ is treated as a continuous variable $x$. The limits of summation $[0, K-1]$ can be extended to $[0, \\infty)$ to find the leading-order asymptotic behavior in the specified time regime.\n$$ S(t) \\approx A \\int_{0}^{\\infty} \\lambda^{-\\mu x} \\exp\\left(-\\frac{t}{\\tau_{0}\\lambda^{x}}\\right) dx $$\nWe perform a change of variables to the time constant, letting $y = \\tau_{x} = \\tau_{0}\\lambda^{x}$. This gives $x = \\frac{\\ln(y/\\tau_{0})}{\\ln(\\lambda)}$ and the differential $dx = \\frac{dy}{y\\ln(\\lambda)}$. The weight term becomes $\\lambda^{-\\mu x} = (\\lambda^{x})^{-\\mu} = (y/\\tau_{0})^{-\\mu}$. The integral transforms to:\n$$ S(t) \\approx A \\int_{\\tau_{0}}^{\\infty} \\left(\\frac{y}{\\tau_{0}}\\right)^{-\\mu} \\exp\\left(-\\frac{t}{y}\\right) \\frac{1}{y \\ln(\\lambda)} dy $$\nRearranging the terms, we get:\n$$ S(t) \\approx \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} \\int_{\\tau_{0}}^{\\infty} y^{-\\mu-1} \\exp\\left(-\\frac{t}{y}\\right) dy $$\nNext, we introduce another substitution, $z = t/y$, which implies $y=t/z$ and $dy = -t/z^2 dz$. The integration limits change from $[\\tau_0, \\infty)$ to $[t/\\tau_0, 0)$.\n$$ S(t) \\approx \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} \\int_{t/\\tau_{0}}^{0} \\left(\\frac{t}{z}\\right)^{-\\mu-1} \\exp(-z) \\left(-\\frac{t}{z^{2}}\\right) dz $$\nReversing the integration limits and simplifying the terms involving $t$ and $z$:\n$$ S(t) \\approx \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} \\int_{0}^{t/\\tau_{0}} t^{-\\mu-1} z^{\\mu+1} \\exp(-z) \\frac{t}{z^{2}} dz = \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} t^{-\\mu} \\int_{0}^{t/\\tau_{0}} z^{\\mu-1} \\exp(-z) dz $$\nThe integral is the lower incomplete gamma function, $\\gamma(\\mu, t/\\tau_{0})$. For the asymptotic behavior at large times ($t \\gg \\tau_0$), the upper limit of the integral $t/\\tau_0 \\to \\infty$. The integral converges to the complete gamma function $\\Gamma(\\mu) = \\int_{0}^{\\infty} z^{\\mu-1} \\exp(-z) dz$, provided the integral converges at its lower bound $z=0$. This convergence requires the exponent of $z$ to be greater than $-1$, i.e., $\\mu-1 > -1$, which simplifies to $\\mu > 0$.\n\nUnder this condition, the leading-order asymptotic scaling law is:\n$$ S(t) \\approx \\frac{A \\tau_{0}^{\\mu}}{\\ln(\\lambda)} t^{-\\mu} \\Gamma(\\mu) $$\nThis shows a power-law decay $S(t) \\propto t^{-\\mu}$ for $\\mu > 0$. The expression can be written as:\n$$ S(t) \\approx \\frac{A \\Gamma(\\mu) \\tau_{0}^{\\mu}}{(\\ln \\lambda) t^{\\mu}} $$",
            "answer": "$$\n\\boxed{\\frac{A \\Gamma(\\mu) \\tau_{0}^{\\mu}}{(\\ln \\lambda) t^{\\mu}}}\n$$"
        },
        {
            "introduction": "Moving from the scale of a single synapse to a large network, structural plasticity presents a fundamental challenge: how to enable learning and memory formation without compromising the stability of the entire system. In recurrent networks, where neurons feed signals back to one another, uncontrolled changes in synaptic strength can lead to explosive, unstable activity. This practice addresses this critical issue by combining models of structural plasticity with powerful tools from random matrix theory . You will derive the stability condition for a large network where synapses are probabilistically deleted and the remaining ones are strengthened, determining the minimal pruning rate required to keep the network's dynamics stable. This exercise bridges the gap between microscopic synaptic rules and macroscopic network function.",
            "id": "4061004",
            "problem": "Consider a large recurrent neuromorphic network with $N$ neurons, whose linearized discrete-time dynamics are given by $x(t+1)=W x(t)$, where $W \\in \\mathbb{R}^{N \\times N}$ is the recurrent connectivity matrix. Stability of the linearized dynamics requires the spectral radius $\\rho(W)$ to satisfy $\\rho(W)<1$. Assume the following statistically homogeneous and scientifically realistic setup:\n- Pre-plasticity connectivity is generated by an Erdős–Rényi adjacency mask with independent entries $M_{ij} \\sim \\mathrm{Bernoulli}(c)$ for a fixed connection probability $c \\in (0,1)$, independent of weights.\n- Conditional on $M_{ij}=1$, synaptic weights are independent and identically distributed with zero mean and second moment $\\mathbb{E}[W_{ij}^{2} \\mid M_{ij}=1] = v/N$ for some $v>0$; otherwise $W_{ij}=0$. This yields entrywise variance $\\mathbb{E}[W_{ij}^{2}] = c v/N$.\n- Structural plasticity implements activity-dependent deletion: an existing synapse $(i,j)$ is deleted with probability $d(a_i,a_j)$ that depends on the pre- and post-synaptic activities $a_i$ and $a_j$. Let the expected deletion fraction among existing synapses be $\\phi = \\mathbb{E}[d(a_i,a_j) \\mid M_{ij}=1]$, with $0<\\phi<1$.\n- Synaptic consolidation rescales the surviving weights by a deterministic factor $s>0$ applied uniformly to all surviving synapses.\n\nWork in the large-$N$ limit under the standard independence assumptions and moment conditions of random matrix theory that guarantee the circular law for non-Hermitian matrices with independent and identically distributed entries of variance $\\sigma^{2}/N$. Starting from first principles (linear stability via spectral radius, the definition of entrywise variance under masking and rescaling, and the circular law for the limiting spectral distribution), derive the stability condition on the post-plasticity spectral radius $\\rho(W')$ after activity-dependent deletion and consolidation. Then, assuming $c v s^{2} > 1$ so that stability is not already guaranteed without deletion, determine the minimal expected deletion fraction $\\phi_{\\min}$ required to ensure $\\rho(W')<1$.\n\nExpress your final answer as a single closed-form symbolic expression for $\\phi_{\\min}$ in terms of $c$, $v$, and $s$. No numerical rounding is required. No units are needed.",
            "solution": "The stability of the network is determined by the spectral radius $\\rho(W')$ of the post-plasticity connectivity matrix $W'$. In the large-$N$ limit, the circular law for random matrices states that for a matrix with independent, zero-mean entries of variance $(\\sigma')^2/N$, the spectral radius is $\\rho(W') = \\sigma'$. Our goal is to compute this variance for $W'$ and apply the stability condition $\\sigma' < 1$.\n\nLet's define the post-plasticity weight $W'_{ij}$. A synapse $(i,j)$ is present in the final matrix if it was present initially (with probability $c$) and it survived the deletion process (with probability $1-\\phi$). The probability of a synapse existing in $W'$ is thus $p' = c(1-\\phi)$. If a synapse exists, its weight is $s \\cdot W_{ij}$, where $W_{ij}$ is the original weight. Otherwise, $W'_{ij}=0$.\n\nFirst, we compute the mean of the entries of $W'$. Since the original weights have zero mean ($E[W_{ij}]=0$), the rescaled weights also have zero mean:\n$$ E[W'_{ij}] = 0 $$\n\nNext, we compute the variance, which is equal to the second moment $E[(W'_{ij})^2]$ since the mean is zero. The expectation is taken over the distribution of initial connections, deletions, and weight values. A non-zero weight $W'_{ij}$ occurs only if an initial connection exists ($M_{ij}=1$) and it survives deletion (an event with probability $1-\\phi$).\nThe unconditional second moment is:\n$$ E[(W'_{ij})^2] = P(\\text{synapse exists in } W') \\cdot E[(W'_{ij})^2 \\mid \\text{synapse exists in } W'] + P(\\text{synapse absent}) \\cdot 0 $$\nThe probability of a synapse existing in the final matrix is $p' = c(1-\\phi)$. If it exists, its value is $s \\cdot W_{ij}$, and the conditional expectation of its square is $E[s^2 W_{ij}^2 \\mid M_{ij}=1] = s^2 E[W_{ij}^2 \\mid M_{ij}=1] = s^2 (v/N)$.\nTherefore, the unconditional variance is:\n$$ \\mathrm{Var}(W'_{ij}) = E[(W'_{ij})^2] = c(1-\\phi) \\cdot s^2 \\frac{v}{N} = \\frac{cs^2v(1-\\phi)}{N} $$\n\nAccording to the circular law, the squared spectral radius $(\\sigma')^2$ is the numerator of this variance expression:\n$$ (\\sigma')^2 = cs^2v(1-\\phi) $$\nThe stability condition $\\rho(W')  1$ translates to $(\\sigma')^2  1$:\n$$ cs^2v(1-\\phi)  1 $$\n\nWe are asked to find the minimal deletion fraction $\\phi_{\\min}$ that ensures stability, given that the initial system is unstable ($cvs^2 > 1$). We solve the inequality for $\\phi$:\n$$ 1-\\phi  \\frac{1}{cs^2v} $$\n$$ -\\phi  \\frac{1}{cs^2v} - 1 $$\n$$ \\phi > 1 - \\frac{1}{cs^2v} $$\nThe minimal required deletion fraction is the boundary of this condition. Any value of $\\phi$ greater than this boundary value will stabilize the network.\n$$ \\phi_{\\min} = 1 - \\frac{1}{cs^2v} $$",
            "answer": "$$\n\\boxed{1 - \\frac{1}{cvs^{2}}}\n$$"
        }
    ]
}