{
    "hands_on_practices": [
        {
            "introduction": "The Free-energy Principle is a normative theory, but how might the brain actually implement it? Predictive coding offers a compelling process theory, suggesting that the brain is a hierarchical prediction machine constantly working to minimize sensory prediction errors. This exercise  takes you to the heart of this mechanism, asking you to derive and implement the fundamental update rule for belief updating, providing a concrete look at how abstract principles of inference can be translated into neurally plausible computations.",
            "id": "4028586",
            "problem": "Consider a linear-Gaussian generative model suitable for predictive coding under the Free-Energy Principle. Let the latent state vector be $\\mathbf{s} \\in \\mathbb{R}^m$ with a zero-mean Gaussian prior $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, I)$, where $I$ is the $m \\times m$ identity matrix. Let the observation vector be $\\mathbf{y} \\in \\mathbb{R}^n$, generated according to $\\mathbf{y} \\mid \\mathbf{s} \\sim \\mathcal{N}(W \\mathbf{s}, \\Sigma)$, where $W \\in \\mathbb{R}^{n \\times m}$ is a known generative mapping and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a known positive-definite observation noise covariance. Consider a Gaussian variational posterior $q(\\mathbf{s}) = \\mathcal{N}(\\mu_q, \\Sigma_q)$ with mean $\\mu_q \\in \\mathbb{R}^m$ and positive-definite covariance $\\Sigma_q \\in \\mathbb{R}^{m \\times m}$.\n\nFrom first principles, relying only on Bayes' rule for linear-Gaussian models, the definition of the negative variational free energy as a lower bound on the log model evidence, and the Laplace approximation (Gaussian approximation around the mode), derive the continuous-time predictive coding dynamics for the mean parameter $\\mu_q$ under a natural gradient preconditioning by $\\Sigma_q$ and identity prior covariance $I$. Use these dynamics to define a single discrete update that corresponds to one explicit Euler step of size $\\eta = 1$, and compute the change in the mean parameter, denoted $\\Delta \\mu_q$, for one iteration as applied to the model above. Your program must implement the computation of $\\Delta \\mu_q$ given $(W, \\Sigma, \\mu_q, \\Sigma_q, \\mathbf{y})$, and it must not rely on any formula not derivable from the fundamental laws and definitions described.\n\nImplement the computation for the following test suite. Each case provides $(W, \\Sigma, \\mu_q, \\Sigma_q, \\mathbf{y})$ with dimensions $n = 2$ and $m = 2$:\n\n- Case $1$ (general well-conditioned case):\n  - $W = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.0 & 1.0 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.5 & 0.0 \\\\ 0.0 & 0.5 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.8 & 0.0 \\\\ 0.0 & 0.6 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.5 \\\\ -0.5 \\end{bmatrix}$.\n\n- Case $2$ (high observation noise):\n  - $W = \\begin{bmatrix} 1.0 & -0.5 \\\\ 0.3 & 1.2 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 5.0 & 0.0 \\\\ 0.0 & 5.0 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} -0.3 \\\\ 0.4 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 1.1 & 0.0 \\\\ 0.0 & 0.9 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 0.2 \\\\ -1.0 \\end{bmatrix}$.\n\n- Case $3$ (correlated posterior covariance):\n  - $W = \\begin{bmatrix} 0.7 & 0.4 \\\\ -0.2 & 1.3 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.6 & 0.0 \\\\ 0.0 & 0.4 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 0.5 \\\\ 0.1 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.7 & 0.3 \\\\ 0.3 & 0.9 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$.\n\n- Case $4$ (rank-deficient generative mapping):\n  - $W = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.1 & 0.0 \\\\ 0.0 & 0.1 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$.\n\n- Case $5$ (strong sensory precision):\n  - $W = \\begin{bmatrix} 0.9 & -0.1 \\\\ 0.2 & 0.8 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.01 & 0.0 \\\\ 0.0 & 0.02 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} -0.1 \\\\ 0.2 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.5 & 0.0 \\\\ 0.0 & 0.5 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the two-dimensional change vector $\\Delta \\mu_q$ for a test case, rounded to six decimal places, represented as a Python-style list. For example, the format should be $[\\,[\\delta\\mu_{1,1}, \\delta\\mu_{1,2}], [\\delta\\mu_{2,1}, \\delta\\mu_{2,2}], \\dots\\,]$. There are no physical units involved. Angles are not used. All numerical outputs must be real-valued floats. The algorithm must be implemented from the foundational definitions given, without invoking any pre-specified shortcut formulas.",
            "solution": "We begin with a linear-Gaussian generative model and a Gaussian variational posterior. The latent state is $\\mathbf{s} \\in \\mathbb{R}^m$ with prior $p(\\mathbf{s}) = \\mathcal{N}(\\mathbf{0}, I)$, where $I$ is the $m \\times m$ identity. The likelihood is $p(\\mathbf{y} \\mid \\mathbf{s}) = \\mathcal{N}(W \\mathbf{s}, \\Sigma)$, with $W \\in \\mathbb{R}^{n \\times m}$ and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ a positive-definite covariance. The variational posterior is $q(\\mathbf{s}) = \\mathcal{N}(\\mu_q, \\Sigma_q)$, with $\\mu_q \\in \\mathbb{R}^m$ and $\\Sigma_q \\in \\mathbb{R}^{m \\times m}$ positive-definite.\n\nThe Free-Energy Principle employs variational free energy to bound the log-evidence. The negative variational free energy (the evidence lower bound) is given by\n$$\n\\mathcal{L}(\\mu_q, \\Sigma_q) = \\mathbb{E}_{q(\\mathbf{s})}[\\log p(\\mathbf{y}, \\mathbf{s})] - \\mathbb{E}_{q(\\mathbf{s})}[\\log q(\\mathbf{s})].\n$$\nUnder the Laplace approximation and natural gradient dynamics, predictive coding emerges by performing gradient ascent on $\\mathcal{L}$ with respect to the variational parameters, often preconditioned by the Fisher information metric for Gaussian families, which here is represented by $\\Sigma_q$ for the mean parameter. To derive the update, we work from fundamental probabilistic definitions.\n\nFirst, write the joint density:\n$$\n\\log p(\\mathbf{y}, \\mathbf{s}) = \\log p(\\mathbf{y}\\mid \\mathbf{s}) + \\log p(\\mathbf{s}).\n$$\nFor Gaussian terms,\n$$\n\\log p(\\mathbf{y}\\mid \\mathbf{s}) = -\\tfrac{1}{2}(\\mathbf{y} - W \\mathbf{s})^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mathbf{s}) + \\text{const},\n$$\n$$\n\\log p(\\mathbf{s}) = -\\tfrac{1}{2}\\mathbf{s}^\\top I \\mathbf{s} + \\text{const} = -\\tfrac{1}{2}\\mathbf{s}^\\top \\mathbf{s} + \\text{const}.\n$$\nThe variational posterior is\n$$\n\\log q(\\mathbf{s}) = -\\tfrac{1}{2}(\\mathbf{s} - \\mu_q)^\\top \\Sigma_q^{-1}(\\mathbf{s} - \\mu_q) + \\text{const}.\n$$\nWe focus on the mean update. Under the Laplace approximation for the mean, consider the negative free energy (which is equivalent, up to constants, to the expected negative log posterior) as a function of $\\mu_q$, approximating the expectation by evaluating at the mean (this is standard in Laplace variational schemes). Define the energy function\n$$\nF(\\mu_q) = \\tfrac{1}{2}(\\mathbf{y} - W \\mu_q)^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) + \\tfrac{1}{2}\\mu_q^\\top I \\mu_q,\n$$\nwhich is the negative log posterior evaluated at $\\mu_q$ (the first term corresponds to the likelihood and the second to the prior, with $I$ as the prior precision). Minimizing $F(\\mu_q)$ corresponds to maximizing the posterior probability and, equivalently, maximizing the evidence lower bound with respect to $\\mu_q$ under the Laplace approximation.\n\nCompute the gradient of $F(\\mu_q)$ with respect to $\\mu_q$. Using standard matrix calculus,\n$$\n\\nabla_{\\mu_q} F(\\mu_q) = - W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) + I \\mu_q.\n$$\nPredictive coding can be expressed as gradient descent on $F(\\mu_q)$ or gradient ascent on $\\mathcal{L}$ with a natural gradient preconditioning by $\\Sigma_q$. The natural gradient step for the mean parameter in an exponential family Gaussian can be approximated by premultiplying the ordinary Euclidean gradient by $\\Sigma_q$, yielding a direction that accounts for the local curvature implied by the posterior covariance. Thus, the continuous-time dynamics for $\\mu_q$ under natural gradient ascent on $\\mathcal{L}$ (equivalently, natural gradient descent on $F$) becomes\n$$\n\\frac{d\\mu_q}{dt} = \\Sigma_q\\left[ W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) - I \\mu_q \\right].\n$$\nTaking a single explicit Euler step with step size $\\eta = 1$ yields the discrete change\n$$\n\\Delta \\mu_q = \\Sigma_q\\left[ W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) - I \\mu_q \\right].\n$$\nThis $\\Delta \\mu_q$ is the requested change in the posterior mean for one predictive coding iteration under the stipulated assumptions: linear-Gaussian model, identity prior covariance, Laplace approximation, and natural gradient preconditioning by $\\Sigma_q$.\n\nAlgorithmic design:\n- Input: matrices $W$, $\\Sigma$, $\\Sigma_q$ and vectors $\\mu_q$, $\\mathbf{y}$.\n- Validate dimensions: $W$ is $n \\times m$, $\\Sigma$ is $n \\times n$, $\\Sigma_q$ is $m \\times m$, $\\mu_q \\in \\mathbb{R}^m$, $\\mathbf{y} \\in \\mathbb{R}^n$.\n- Compute the sensory prediction error $\\mathbf{e}_y = \\mathbf{y} - W \\mu_q$.\n- Compute the precision-weighted error $\\mathbf{r}_y = \\Sigma^{-1} \\mathbf{e}_y$.\n- Compute the back-propagated precision-weighted error at latent level $\\mathbf{g} = W^\\top \\mathbf{r}_y$.\n- Compute the prior contribution $\\mathbf{p} = I \\mu_q = \\mu_q$.\n- Combine to form the Euclidean gradient direction $\\mathbf{d} = \\mathbf{g} - \\mathbf{p}$.\n- Precondition by $\\Sigma_q$ to obtain the natural gradient step $\\Delta \\mu_q = \\Sigma_q \\mathbf{d}$.\n- Output $\\Delta \\mu_q$ with numerical rounding as specified.\n\nEdge cases are included in the test suite:\n- Case $1$ verifies a general, well-conditioned scenario.\n- Case $2$ uses large observation noise $\\Sigma$, suppressing the sensory term relative to the prior term.\n- Case $3$ uses correlated $\\Sigma_q$, exercising natural gradient preconditioning across dimensions.\n- Case $4$ uses a rank-deficient $W$, restricting sensory information to a subspace.\n- Case $5$ uses very small observation noise (high precision), emphasizing the sensory term and leading to a large correction.\n\nThe final program implements the above steps for each test case and prints the results in the required single-line format, as a comma-separated list of lists with each component rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef predictive_coding_delta_mu(W, Sigma, mu_q, Sigma_q, y):\n    \"\"\"\n    Compute one predictive coding iteration change in posterior mean (Delta mu_q)\n    under a linear-Gaussian generative model with identity prior covariance,\n    Laplace approximation, and natural gradient preconditioning by Sigma_q.\n\n    Delta mu_q = Sigma_q * [ W^T * Sigma^{-1} * (y - W * mu_q) - mu_q ]\n    \"\"\"\n    # Sensory prediction error\n    e_y = y - W @ mu_q\n    # Precision-weighted error\n    r_y = np.linalg.solve(Sigma, e_y)\n    # Backpropagated precision-weighted error to latent space\n    g = W.T @ r_y\n    # Prior term (identity prior precision)\n    p = mu_q\n    # Euclidean gradient direction\n    d = g - p\n    # Natural gradient step preconditioned by Sigma_q\n    delta_mu = Sigma_q @ d\n    return delta_mu\n\ndef format_vector(vec):\n    # Round to six decimals and format as Python-style list\n    return \"[\" + \",\".join(f\"{x:.6f}\" for x in vec.tolist()) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            np.array([[1.0, 0.2],\n                      [0.0, 1.0]]),\n            np.array([[0.5, 0.0],\n                      [0.0, 0.5]]),\n            np.array([0.1, -0.2]),\n            np.array([[0.8, 0.0],\n                      [0.0, 0.6]]),\n            np.array([1.5, -0.5])\n        ),\n        # Case 2\n        (\n            np.array([[1.0, -0.5],\n                      [0.3,  1.2]]),\n            np.array([[5.0, 0.0],\n                      [0.0, 5.0]]),\n            np.array([-0.3, 0.4]),\n            np.array([[1.1, 0.0],\n                      [0.0, 0.9]]),\n            np.array([0.2, -1.0])\n        ),\n        # Case 3\n        (\n            np.array([[ 0.7, 0.4],\n                      [-0.2, 1.3]]),\n            np.array([[0.6, 0.0],\n                      [0.0, 0.4]]),\n            np.array([0.5, 0.1]),\n            np.array([[0.7, 0.3],\n                      [0.3, 0.9]]),\n            np.array([1.0, 0.0])\n        ),\n        # Case 4\n        (\n            np.array([[1.0, 0.0],\n                      [0.0, 0.0]]),\n            np.array([[0.1, 0.0],\n                      [0.0, 0.1]]),\n            np.array([2.0, -1.0]),\n            np.array([[1.0, 0.0],\n                      [0.0, 1.0]]),\n            np.array([1.0, 0.0])\n        ),\n        # Case 5\n        (\n            np.array([[0.9, -0.1],\n                      [0.2,  0.8]]),\n            np.array([[0.01, 0.0],\n                      [0.0,  0.02]]),\n            np.array([-0.1, 0.2]),\n            np.array([[0.5, 0.0],\n                      [0.0, 0.5]]),\n            np.array([0.0, 1.0])\n        ),\n    ]\n\n    results = []\n    for W, Sigma, mu_q, Sigma_q, y in test_cases:\n        delta_mu = predictive_coding_delta_mu(W, Sigma, mu_q, Sigma_q, y)\n        results.append(format_vector(delta_mu))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond passive perception, the Free-energy Principle also provides a formal basis for action selection, known as active inference. In this view, actions are not chosen to maximize reward, but to minimize the expected free energy of future outcomes. This practice  guides you through the calculation of expected free energy for different policies, demonstrating how agents must balance the pursuit of preferred outcomes (extrinsic value) with the drive to resolve uncertainty about the world (epistemic value).",
            "id": "4028595",
            "problem": "Consider a single-step decision problem formulated under the Free-Energy Principle (FEP) in brain modeling and computational neuroscience. Hidden states are binary, $s \\in \\{s_{0}, s_{1}\\}$, with a current belief (recognition density) $Q(s)$ given by $Q(s_{0}) = 0.6$ and $Q(s_{1}) = 0.4$. Observations are binary, $o \\in \\{0,1\\}$, and there are two candidate one-step policies $\\pi_{1}$ and $\\pi_{2}$ corresponding to two actions $a_{1}$ and $a_{2}$ that modulate the likelihood mapping $P(o \\mid s, a)$ as follows:\n- Under action $a_{1}$: $P(o=1 \\mid s_{0}, a_{1}) = 0.8$, $P(o=0 \\mid s_{0}, a_{1}) = 0.2$, $P(o=1 \\mid s_{1}, a_{1}) = 0.3$, $P(o=0 \\mid s_{1}, a_{1}) = 0.7$.\n- Under action $a_{2}$: $P(o=1 \\mid s_{0}, a_{2}) = 0.6$, $P(o=0 \\mid s_{0}, a_{2}) = 0.4$, $P(o=1 \\mid s_{1}, a_{2}) = 0.1$, $P(o=0 \\mid s_{1}, a_{2}) = 0.9$.\n\nPreferred outcomes are encoded by a prior preference distribution $P^{\\ast}(o)$ with $P^{\\ast}(1) = 0.9$ and $P^{\\ast}(0) = 0.1$. Assume a single time step, and that the prior over hidden states used in scoring expected free energy matches the current belief, i.e., $P(s) = Q(s)$. Use the natural logarithm $\\ln$.\n\nStarting from the definition of variational free energy for a single observation in terms of a recognition distribution $Q(s \\mid o, \\pi)$ and a generative model with outcome preferences $P^{\\ast}(o)$, and using only core definitions from variational Bayesian inference (e.g., Kullback–Leibler divergence (KLD), mutual information, and law of total expectation), derive an expression for the one-step expected free energy $\\mathcal{G}(\\pi)$ of a policy $\\pi$ that separates into (i) an extrinsic term that depends on $P^{\\ast}(o)$ and the predictive distribution $Q(o \\mid \\pi)$ and (ii) an epistemic term that is the mutual information between hidden states and outcomes under $\\pi$. Then compute $\\mathcal{G}(\\pi_{1})$ and $\\mathcal{G}(\\pi_{2})$ numerically, and determine which policy minimizes $\\mathcal{G}(\\pi)$.\n\nAs your final answer, report the index $i \\in \\{1,2\\}$ of the minimizing policy $\\pi_{i}$. No rounding instruction is needed for the final index. If intermediate numerical computations are required, use exact arithmetic and $\\ln$ evaluations; any approximations should be clearly indicated within your derivation.",
            "solution": "The user wants me to solve a problem based on the Free-Energy Principle.\n\n### Step 1: Extract Givens\n-   **Hidden States:** $s \\in \\{s_{0}, s_{1}\\}$.\n-   **Recognition Density (Current Belief):** $Q(s_{0}) = 0.6$, $Q(s_{1}) = 0.4$.\n-   **Observations:** $o \\in \\{0, 1\\}$.\n-   **Policies:** $\\pi_1$ (action $a_1$) and $\\pi_2$ (action $a_2$).\n-   **Likelihood mapping for action $a_1$:**\n    -   $P(o=1 \\mid s_{0}, a_{1}) = 0.8$, $P(o=0 \\mid s_{0}, a_{1}) = 0.2$.\n    -   $P(o=1 \\mid s_{1}, a_{1}) = 0.3$, $P(o=0 \\mid s_{1}, a_{1}) = 0.7$.\n-   **Likelihood mapping for action $a_2$:**\n    -   $P(o=1 \\mid s_{0}, a_{2}) = 0.6$, $P(o=0 \\mid s_{0}, a_{2}) = 0.4$.\n    -   $P(o=1 \\mid s_{1}, a_{2}) = 0.1$, $P(o=0 \\mid s_{1}, a_{2}) = 0.9$.\n-   **Preferred Outcomes (Prior Preferences):** $P^{\\ast}(1) = 0.9$, $P^{\\ast}(0) = 0.1$.\n-   **Assumption:** The prior over hidden states for scoring EFE is the current belief: $P(s) = Q(s)$.\n-   **Logarithm:** Use the natural logarithm $\\ln$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within the established framework of active inference and the free-energy principle. The terminology and formulation are standard in computational neuroscience. The problem is well-posed, providing all necessary numerical values and a clear, unambiguous objective. The provided probabilities for all conditional distributions sum to $1$, and there are no internal contradictions. The data is consistent. The problem is objective and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Expected Free Energy Expression\n\nThe expected free energy $\\mathcal{G}(\\pi)$ of a policy $\\pi$ is the free energy expected under that policy, averaged over the outcomes $o$ predicted by the policy. This is written as:\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[F(o|\\pi)] $$\nwhere $F(o|\\pi)$ is the variational free energy associated with a particular future outcome $o$. The problem states that we should start from a definition of free energy involving a recognition distribution $Q(s|o,\\pi)$ and a generative model with outcome preferences $P^\\ast(o)$. The free energy is a functional of a recognition density $Q$ and a generative model $P$, defined as $F = \\mathbb{E}_{Q}[\\ln Q - \\ln P]$. For active inference, the generative model embodies the agent's goals. We define a simple generative model where states and outcomes are independent, with outcomes following the preference distribution $P^\\ast(o)$ and states following a prior distribution $P(s)$. Thus, the joint probability is $P(s,o) = P(s)P^\\ast(o)$.\n\nThe free energy for a future outcome $o$ is the Kullback-Leibler (KL) divergence between the approximate posterior belief about states $Q(s|o,\\pi)$ and this goal-directed generative model $P(s,o)$:\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln Q(s|o,\\pi) - \\ln P(s,o)] $$\nSubstituting $P(s,o) = P(s)P^\\ast(o)$:\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln Q(s|o,\\pi) - \\ln(P(s)P^\\ast(o))] $$\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln \\frac{Q(s|o,\\pi)}{P(s)}] - \\ln P^\\ast(o) $$\nThe first term is the KL divergence $D_{KL}[Q(s|o,\\pi) || P(s)]$. So:\n$$ F(o|\\pi) = D_{KL}[Q(s|o,\\pi) || P(s)] - \\ln P^\\ast(o) $$\nNow, we compute the expected free energy $\\mathcal{G}(\\pi)$ by taking the expectation of $F(o|\\pi)$ over the predictive distribution of outcomes $Q(o|\\pi) = \\sum_s P(o|s,\\pi)Q(s)$:\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)] - \\ln P^\\ast(o)] $$\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)]] + \\mathbb{E}_{Q(o|\\pi)}[-\\ln P^\\ast(o)] $$\nThis expression separates into two terms.\n\nThe first term is the **epistemic term**, which quantifies the expected information gain about the hidden states:\n$$ \\mathcal{G}_I(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)]] $$\nUsing the problem's condition that the prior over states is the current belief, $P(s) = Q(s)$, this term becomes:\n$$ \\mathcal{G}_I(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || Q(s)]] = \\sum_o Q(o|\\pi) \\sum_s Q(s|o,\\pi) \\ln\\frac{Q(s|o,\\pi)}{Q(s)} $$\nThis is, by definition, the mutual information $I(s;o|\\pi)$ between hidden states $s$ and outcomes $o$ under the policy $\\pi$.\n\nThe second term is the **extrinsic term** (or pragmatic value), which quantifies how much the expected outcomes diverge from preferred outcomes:\n$$ \\mathcal{G}_E(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[-\\ln P^\\ast(o)] = \\sum_o Q(o|\\pi) (-\\ln P^\\ast(o)) $$\nThis is the cross-entropy between the predictive distribution $Q(o|\\pi)$ and the preference distribution $P^\\ast(o)$. This term depends on $P^{\\ast}(o)$ and $Q(o|\\pi)$ as required.\n\nTherefore, the one-step expected free energy to be minimized is:\n$$ \\mathcal{G}(\\pi) = \\mathcal{G}_E(\\pi) + \\mathcal{G}_I(\\pi) $$\n\n### Computation for Policy $\\pi_1$ (Action $a_1$)\n\nThe current belief is $Q(s_0) = 0.6$ and $Q(s_1) = 0.4$.\n\n1.  **Predictive outcome distribution $Q(o|\\pi_1)$**:\n    $Q(o=1|\\pi_1) = \\sum_s P(o=1|s, a_1)Q(s) = (0.8)(0.6) + (0.3)(0.4) = 0.48 + 0.12 = 0.6$.\n    $Q(o=0|\\pi_1) = \\sum_s P(o=0|s, a_1)Q(s) = (0.2)(0.6) + (0.7)(0.4) = 0.12 + 0.28 = 0.4$.\n\n2.  **Extrinsic Term $\\mathcal{G}_E(\\pi_1)$**:\n    $\\mathcal{G}_E(\\pi_1) = Q(o=0|\\pi_1)(-\\ln P^\\ast(0)) + Q(o=1|\\pi_1)(-\\ln P^\\ast(1))$\n    $\\mathcal{G}_E(\\pi_1) = 0.4(-\\ln 0.1) + 0.6(-\\ln 0.9) = 0.4\\ln(10) - 0.6\\ln(0.9)$.\n\n3.  **Epistemic Term $\\mathcal{G}_I(\\pi_1)$**:\n    First, we find the posterior beliefs $Q(s|o, \\pi_1) = \\frac{P(o|s,a_1)Q(s)}{Q(o|\\pi_1)}$.\n    For $o=1$:\n    $Q(s_0|o=1, \\pi_1) = \\frac{0.8 \\times 0.6}{0.6} = 0.8$.\n    $Q(s_1|o=1, \\pi_1) = \\frac{0.3 \\times 0.4}{0.6} = 0.2$.\n    For $o=0$:\n    $Q(s_0|o=0, \\pi_1) = \\frac{0.2 \\times 0.6}{0.4} = 0.3$.\n    $Q(s_1|o=0, \\pi_1) = \\frac{0.7 \\times 0.4}{0.4} = 0.7$.\n\n    Next, we compute the KL divergences:\n    $D_{KL}[Q(s|o=1,\\pi_1) || Q(s)] = 0.8\\ln(\\frac{0.8}{0.6}) + 0.2\\ln(\\frac{0.2}{0.4}) = 0.8\\ln(\\frac{4}{3}) + 0.2\\ln(\\frac{1}{2})$.\n    $D_{KL}[Q(s|o=0,\\pi_1) || Q(s)] = 0.3\\ln(\\frac{0.3}{0.6}) + 0.7\\ln(\\frac{0.7}{0.4}) = 0.3\\ln(\\frac{1}{2}) + 0.7\\ln(\\frac{7}{4})$.\n\n    $\\mathcal{G}_I(\\pi_1) = Q(o=1|\\pi_1) D_{KL}[Q(s|o=1,\\pi_1) || Q(s)] + Q(o=0|\\pi_1) D_{KL}[Q(s|o=0,\\pi_1) || Q(s)]$\n    $\\mathcal{G}_I(\\pi_1) = 0.6(0.8\\ln(\\frac{4}{3}) + 0.2\\ln(\\frac{1}{2})) + 0.4(0.3\\ln(\\frac{1}{2}) + 0.7\\ln(\\frac{7}{4}))$\n    $\\mathcal{G}_I(\\pi_1) = 0.48\\ln(\\frac{4}{3}) + 0.12\\ln(\\frac{1}{2}) + 0.12\\ln(\\frac{1}{2}) + 0.28\\ln(\\frac{7}{4})$\n    $\\mathcal{G}_I(\\pi_1) = 0.48\\ln(\\frac{4}{3}) + 0.24\\ln(\\frac{1}{2}) + 0.28\\ln(\\frac{7}{4})$.\n\n4.  **Total Expected Free Energy $\\mathcal{G}(\\pi_1)$**:\n    $\\mathcal{G}(\\pi_1) = \\mathcal{G}_E(\\pi_1) + \\mathcal{G}_I(\\pi_1)$\n    $\\mathcal{G}(\\pi_1) = (0.4\\ln(10) - 0.6\\ln(0.9)) + (0.48\\ln(\\frac{4}{3}) - 0.24\\ln(2) + 0.28\\ln(\\frac{7}{4}))$\n    Using numerical values: $\\ln(10) \\approx 2.3026$, $\\ln(0.9) \\approx -0.1054$, $\\ln(4/3) \\approx 0.2877$, $\\ln(2) \\approx 0.6931$, $\\ln(7/4) \\approx 0.5596$.\n    $\\mathcal{G}_E(\\pi_1) \\approx 0.4(2.3026) + 0.6(0.1054) = 0.92104 + 0.06324 = 0.98428$.\n    $\\mathcal{G}_I(\\pi_1) \\approx 0.48(0.2877) - 0.24(0.6931) + 0.28(0.5596) = 0.13810 - 0.16634 + 0.15669 = 0.12845$.\n    $\\mathcal{G}(\\pi_1) \\approx 0.98428 + 0.12845 = 1.11273$.\n\n### Computation for Policy $\\pi_2$ (Action $a_2$)\n\n1.  **Predictive outcome distribution $Q(o|\\pi_2)$**:\n    $Q(o=1|\\pi_2) = \\sum_s P(o=1|s, a_2)Q(s) = (0.6)(0.6) + (0.1)(0.4) = 0.36 + 0.04 = 0.4$.\n    $Q(o=0|\\pi_2) = \\sum_s P(o=0|s, a_2)Q(s) = (0.4)(0.6) + (0.9)(0.4) = 0.24 + 0.36 = 0.6$.\n\n2.  **Extrinsic Term $\\mathcal{G}_E(\\pi_2)$**:\n    $\\mathcal{G}_E(\\pi_2) = Q(o=0|\\pi_2)(-\\ln P^\\ast(0)) + Q(o=1|\\pi_2)(-\\ln P^\\ast(1))$\n    $\\mathcal{G}_E(\\pi_2) = 0.6(-\\ln 0.1) + 0.4(-\\ln 0.9) = 0.6\\ln(10) - 0.4\\ln(0.9)$.\n\n3.  **Epistemic Term $\\mathcal{G}_I(\\pi_2)$**:\n    First, we find the posterior beliefs $Q(s|o, \\pi_2) = \\frac{P(o|s,a_2)Q(s)}{Q(o|\\pi_2)}$.\n    For $o=1$:\n    $Q(s_0|o=1, \\pi_2) = \\frac{0.6 \\times 0.6}{0.4} = 0.9$.\n    $Q(s_1|o=1, \\pi_2) = \\frac{0.1 \\times 0.4}{0.4} = 0.1$.\n    For $o=0$:\n    $Q(s_0|o=0, \\pi_2) = \\frac{0.4 \\times 0.6}{0.6} = 0.4$.\n    $Q(s_1|o=0, \\pi_2) = \\frac{0.9 \\times 0.4}{0.6} = 0.6$.\n\n    Next, we compute the KL divergences:\n    $D_{KL}[Q(s|o=1,\\pi_2) || Q(s)] = 0.9\\ln(\\frac{0.9}{0.6}) + 0.1\\ln(\\frac{0.1}{0.4}) = 0.9\\ln(1.5) + 0.1\\ln(0.25)$.\n    $D_{KL}[Q(s|o=0,\\pi_2) || Q(s)] = 0.4\\ln(\\frac{0.4}{0.6}) + 0.6\\ln(\\frac{0.6}{0.4}) = 0.4\\ln(\\frac{2}{3}) + 0.6\\ln(1.5) = 0.2\\ln(1.5)$.\n\n    $\\mathcal{G}_I(\\pi_2) = Q(o=1|\\pi_2) D_{KL}[Q(s|o=1,\\pi_2) || Q(s)] + Q(o=0|\\pi_2) D_{KL}[Q(s|o=0,\\pi_2) || Q(s)]$\n    $\\mathcal{G}_I(\\pi_2) = 0.4(0.9\\ln(1.5) + 0.1\\ln(0.25)) + 0.6(0.2\\ln(1.5))$\n    $\\mathcal{G}_I(\\pi_2) = 0.36\\ln(1.5) + 0.04\\ln(0.25) + 0.12\\ln(1.5) = 0.48\\ln(1.5) + 0.04\\ln(0.25)$.\n\n4.  **Total Expected Free Energy $\\mathcal{G}(\\pi_2)$**:\n    $\\mathcal{G}(\\pi_2) = \\mathcal{G}_E(\\pi_2) + \\mathcal{G}_I(\\pi_2)$\n    $\\mathcal{G}(\\pi_2) = (0.6\\ln(10) - 0.4\\ln(0.9)) + (0.48\\ln(1.5) + 0.04\\ln(0.25))$\n    Using numerical values: $\\ln(1.5) \\approx 0.4055$, $\\ln(0.25) \\approx -1.3863$.\n    $\\mathcal{G}_E(\\pi_2) \\approx 0.6(2.3026) + 0.4(0.1054) = 1.38156 + 0.04216 = 1.42372$.\n    $\\mathcal{G}_I(\\pi_2) \\approx 0.48(0.4055) + 0.04(-1.3863) = 0.19464 - 0.05545 = 0.13919$.\n    $\\mathcal{G}(\\pi_2) \\approx 1.42372 + 0.13919 = 1.56291$.\n\n### Conclusion\n\nComparing the total expected free energy for both policies:\n$$ \\mathcal{G}(\\pi_1) \\approx 1.11273 $$\n$$ \\mathcal{G}(\\pi_2) \\approx 1.56291 $$\nSince an agent following the free-energy principle selects the policy that minimizes the expected free energy, we compare the two values. We find that $\\mathcal{G}(\\pi_1) < \\mathcal{G}(\\pi_2)$.\nTherefore, policy $\\pi_1$ is the optimal policy. The index of the minimizing policy is $i=1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "An agent's ability to make sense of the world depends critically on its internal generative model. But how does an agent determine which model is best? This final practice  demonstrates that the Free-energy Principle offers an elegant solution by framing model selection as a form of inference. By computing the free energy—or its analytic equivalent, the negative log model evidence—you will see how this framework inherently balances a model's accuracy against its complexity, providing a first-principles implementation of Occam's razor.",
            "id": "4063890",
            "problem": "You will implement and analyze model selection under the free-energy principle in a linear Gaussian generative model, focusing on the accuracy–complexity trade-off. Consider a univariate latent variable $s \\in \\mathbb{R}$ and an observed variable $o \\in \\mathbb{R}$. A generative model $M$ is specified by a Gaussian prior and a Gaussian likelihood: a prior $p(s \\mid M) = \\mathcal{N}(s ; \\mu_0, v_0)$ with mean $\\mu_0 \\in \\mathbb{R}$ and variance $v_0 \\in \\mathbb{R}_{>0}$, and a likelihood $p(o \\mid s, M) = \\mathcal{N}(o ; s, v_s)$ with variance $v_s \\in \\mathbb{R}_{>0}$. The approximate posterior $q(s \\mid o, M)$ is constrained to be Gaussian. In the conjugate Gaussian case, the exact posterior $p(s \\mid o, M)$ is Gaussian and belongs to this family.\n\nYour tasks are:\n- Starting from Bayes’ rule $p(s \\mid o, M) \\propto p(o \\mid s, M) \\, p(s \\mid M)$, and the normalization of Gaussian densities, derive the exact posterior $p(s \\mid o, M)$ in closed form, including its mean and variance, as functions of $\\mu_0$, $v_0$, $v_s$, and $o$.\n- Using the definitions of the expected value and Kullback–Leibler divergence, define two functionals of $q(s \\mid o, M)$: the accuracy $\\mathcal{A}(M, o)$ as the expected log likelihood $\\mathbb{E}_{q}[\\log p(o \\mid s, M)]$, and the complexity $\\mathcal{C}(M, o)$ as the divergence $\\mathrm{KL}(q(s \\mid o, M) \\, \\| \\, p(s \\mid M))$. Express $\\mathcal{A}(M, o)$ and $\\mathcal{C}(M, o)$ in closed form when $q(s \\mid o, M)$ is Gaussian. All logarithms are natural logarithms and all information-theoretic quantities are in nats.\n- Define the variational free energy $\\mathcal{F}(M, o)$ as the difference $\\mathcal{C}(M, o) - \\mathcal{A}(M, o)$. Implement a program that, for each observed $o$, evaluates $\\mathcal{F}(M, o)$ for each model $M$ in a given set, and selects the model index that minimizes $\\mathcal{F}(M, o)$. In case of exact equality of $\\mathcal{F}$ values across multiple models, break ties by choosing the smallest index.\n\nUse only the following fixed test suite of models and observations:\n- Models (indexed by $i \\in \\{1, 2, 3, 4\\}$), each with parameters $(\\mu_0, v_0, v_s)$:\n  - $M_1$: $(\\mu_0, v_0, v_s) = (0, 1, 4)$.\n  - $M_2$: $(\\mu_0, v_0, v_s) = (0, 1, 0.25)$.\n  - $M_3$: $(\\mu_0, v_0, v_s) = (0, 10, 0.25)$.\n  - $M_4$: $(\\mu_0, v_0, v_s) = (0, 0.1, 0.25)$.\n- Observations (in order): $o \\in \\{0, 2.5, -1.5, 8\\}$.\n\nYour program must:\n- For each observation $o$ in the list, compute $\\mathcal{F}(M_i, o)$ for $i \\in \\{1, 2, 3, 4\\}$ using the closed-form expressions you derive from first principles for the Gaussian case with $q(s \\mid o, M) = p(s \\mid o, M)$.\n- Produce a single line of output containing the results as a comma-separated list of the selected model indices, enclosed in square brackets (for example, $[1,2,3,4]$). The output must correspond to the observations in the order given above, and contain exactly one integer per observation.\n\nNo physical units are involved. Angles do not appear. All returned values are dimensionless. Ensure numerical stability by respecting positivity of variances $v_0$ and $v_s$ and using natural logarithms. The final outputs must be integers. The program must not require any input. The evaluation must be done in natural units (nats).",
            "solution": "The problem is valid and well-posed, grounded in standard Bayesian inference and information theory. We proceed with a step-by-step derivation of the required quantities and the subsequent computational method.\n\n### 1. Derivation of the Exact Posterior Distribution\n\nThe generative model $M$ is defined by a Gaussian prior and a Gaussian likelihood:\n- Prior: $p(s \\mid M) = \\mathcal{N}(s ; \\mu_0, v_0)$\n- Likelihood: $p(o \\mid s, M) = \\mathcal{N}(o ; s, v_s)$\n\nAccording to Bayes' rule, the posterior distribution of the latent variable $s$ given an observation $o$ is proportional to the product of the likelihood and the prior:\n$$p(s \\mid o, M) \\propto p(o \\mid s, M) p(s \\mid M)$$\nSince the prior and likelihood are both Gaussian, their product is also proportional to a Gaussian function, implying the posterior is Gaussian. We can identify its parameters by examining the exponent of the resulting distribution. The log-posterior is given by:\n$$\\log p(s \\mid o, M) = \\log p(o \\mid s, M) + \\log p(s \\mid M) + \\text{const.}$$\nSubstituting the probability density functions for the Gaussian distributions (and ignoring normalization constants):\n$$\\log p(s \\mid o, M) = -\\frac{(o-s)^2}{2v_s} - \\frac{(s-\\mu_0)^2}{2v_0} + \\text{const.}$$\nTo find the posterior mean and variance, we expand the quadratic terms and collect terms in $s^2$ and $s$:\n$$\\log p(s \\mid o, M) = -\\frac{1}{2v_s}(o^2 - 2os + s^2) - \\frac{1}{2v_0}(s^2 - 2s\\mu_0 + \\mu_0^2) + \\text{const.}$$\n$$= s^2 \\left(-\\frac{1}{2v_s} - \\frac{1}{2v_0}\\right) + s \\left(\\frac{o}{v_s} + \\frac{\\mu_0}{v_0}\\right) + \\text{const.}$$\nThis is a quadratic form in $s$, characteristic of a Gaussian log-density. A generic Gaussian posterior $p(s \\mid o, M) = \\mathcal{N}(s; \\mu_p, v_p)$ has a log-density of the form:\n$$\\log \\mathcal{N}(s; \\mu_p, v_p) = -\\frac{(s-\\mu_p)^2}{2v_p} + \\text{const.} = s^2\\left(-\\frac{1}{2v_p}\\right) + s\\left(\\frac{\\mu_p}{v_p}\\right) + \\text{const.}$$\nBy comparing the coefficients of the $s^2$ term, we find the posterior precision (inverse variance):\n$$\\frac{1}{v_p} = \\frac{1}{v_0} + \\frac{1}{v_s} = \\frac{v_0 + v_s}{v_0 v_s}$$\nThus, the posterior variance $v_p$ is:\n$$v_p = \\frac{v_0 v_s}{v_0 + v_s}$$\nBy comparing the coefficients of the $s$ term, we find the posterior mean $\\mu_p$:\n$$\\frac{\\mu_p}{v_p} = \\frac{\\mu_0}{v_0} + \\frac{o}{v_s}$$\n$$\\mu_p = v_p \\left( \\frac{\\mu_0}{v_0} + \\frac{o}{v_s} \\right) = \\frac{v_0 v_s}{v_0 + v_s} \\left( \\frac{\\mu_0 v_s + o v_0}{v_0 v_s} \\right)$$\nThus, the posterior mean $\\mu_p$ is a precision-weighted average of the prior mean and the observation:\n$$\\mu_p = \\frac{v_s \\mu_0 + v_0 o}{v_0 + v_s}$$\nThe problem specifies using an approximate posterior $q(s \\mid o, M)$ which is the exact posterior in this conjugate case. Let $q(s \\mid o, M) = \\mathcal{N}(s; \\mu_q, v_q)$, where $\\mu_q = \\mu_p$ and $v_q = v_p$.\n\n### 2. Derivation of Accuracy and Complexity\n\n**Accuracy $\\mathcal{A}(M, o)$** is the expected log-likelihood under the approximate posterior $q$:\n$$\\mathcal{A}(M, o) = \\mathbb{E}_{q}[\\log p(o \\mid s, M)]$$\nThe log-likelihood is:\n$$\\log p(o \\mid s, M) = \\log\\left(\\frac{1}{\\sqrt{2\\pi v_s}}\\exp\\left(-\\frac{(o-s)^2}{2v_s}\\right)\\right) = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{(o-s)^2}{2v_s}$$\nTaking the expectation with respect to $q(s \\mid o, M) = \\mathcal{N}(s; \\mu_q, v_q)$:\n$$\\mathcal{A}(M, o) = \\mathbb{E}_{q}\\left[-\\frac{1}{2}\\log(2\\pi v_s) - \\frac{(o-s)^2}{2v_s}\\right] = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{1}{2v_s}\\mathbb{E}_{q}[(o-s)^2]$$\nThe expected value of the squared error is:\n$$\\mathbb{E}_{q}[(o-s)^2] = \\mathbb{E}_{q}[(s-o)^2] = \\mathbb{E}_{q}[(s-\\mu_q + \\mu_q-o)^2]$$\n$$= \\mathbb{E}_{q}[(s-\\mu_q)^2 + 2(s-\\mu_q)(\\mu_q-o) + (\\mu_q-o)^2]$$\n$$= \\mathbb{E}_{q}[(s-\\mu_q)^2] + 2(\\mu_q-o)\\mathbb{E}_{q}[s-\\mu_q] + (\\mu_q-o)^2$$\nSince $\\mathbb{E}_{q}[s-\\mu_q] = 0$ and $\\mathbb{E}_{q}[(s-\\mu_q)^2] = v_q$, this simplifies to:\n$$\\mathbb{E}_{q}[(o-s)^2] = v_q + (\\mu_q-o)^2$$\nSubstituting this back, the closed-form expression for accuracy is:\n$$\\mathcal{A}(M, o) = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{v_q + (o-\\mu_q)^2}{2v_s}$$\n\n**Complexity $\\mathcal{C}(M, o)$** is the Kullback-Leibler (KL) divergence from the prior to the posterior:\n$$\\mathcal{C}(M, o) = \\mathrm{KL}(q(s \\mid o, M) \\, \\| \\, p(s \\mid M))$$\nFor two univariate Gaussian distributions, $q(s) = \\mathcal{N}(s; \\mu_q, v_q)$ and $p(s) = \\mathcal{N}(s; \\mu_0, v_0)$, the KL divergence is given by the standard formula:\n$$\\mathrm{KL}(q \\| p) = \\frac{1}{2} \\left( \\log\\frac{v_0}{v_q} + \\frac{v_q}{v_0} + \\frac{(\\mu_q - \\mu_0)^2}{v_0} - 1 \\right)$$\nThus, the closed-form expression for complexity is:\n$$\\mathcal{C}(M, o) = \\frac{1}{2} \\left( \\log\\left(\\frac{v_0}{v_q}\\right) + \\frac{v_q + (\\mu_q - \\mu_0)^2}{v_0} - 1 \\right)$$\n\n### 3. Variational Free Energy and Computational Strategy\n\nThe variational free energy $\\mathcal{F}(M, o)$ is defined as:\n$$\\mathcal{F}(M, o) = \\mathcal{C}(M, o) - \\mathcal{A}(M, o)$$\nThis quantity is an upper bound on the negative log-evidence, $-\\log p(o \\mid M)$. When the approximate posterior is the exact posterior, i.e., $q(s \\mid o, M) = p(s \\mid o, M)$, this bound becomes an equality:\n$$\\mathcal{F}(M, o) = -\\log p(o \\mid M)$$\nThis provides a much simpler expression for computation. The marginal likelihood or model evidence $p(o \\mid M)$ is obtained by marginalizing the joint distribution $p(o,s \\mid M) = p(o \\mid s, M)p(s \\mid M)$ over the latent variable $s$:\n$$p(o \\mid M) = \\int p(o \\mid s, M) p(s \\mid M) \\,ds$$\nThis integral represents the convolution of two Gaussian distributions: $\\mathcal{N}(s; \\mu_0, v_0)$ and a kernel for $o$ given $s$ corresponding to $\\mathcal{N}(o; s, v_s)$, which can be seen as $o = s+\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0,v_s)$. The resulting distribution for $o$ is also Gaussian. Its mean is the sum of the means ($\\mu_0 + 0 = \\mu_0$), and its variance is the sum of the variances ($v_0 + v_s$), as $s$ and $\\epsilon$ are independent.\n$$p(o \\mid M) = \\mathcal{N}(o; \\mu_0, v_0+v_s)$$\nTherefore, the variational free energy is simply the negative log of this marginal likelihood:\n$$\\mathcal{F}(M, o) = -\\log \\left( \\frac{1}{\\sqrt{2\\pi(v_0+v_s)}} \\exp\\left(-\\frac{(o-\\mu_0)^2}{2(v_0+v_s)}\\right) \\right)$$\n$$\\mathcal{F}(M, o) = \\frac{(o-\\mu_0)^2}{2(v_0+v_s)} + \\frac{1}{2}\\log(2\\pi(v_0+v_s))$$\nThis expression represents a trade-off. The first term, $\\frac{(o-\\mu_0)^2}{2(v_0+v_s)}$, is an accuracy or prediction error term penalizing models whose prior prediction mean $\\mu_0$ is far from the observation $o$, modulated by the predictive uncertainty $v_0+v_s$. The second term, $\\frac{1}{2}\\log(2\\pi(v_0+v_s))$, acts as a complexity penalty, penalizing models with large predictive uncertainty. Minimizing $\\mathcal{F}$ achieves an optimal balance between accuracy and complexity, which is the principle of Bayesian model selection.\n\nThe computational procedure is as follows:\n1.  For each observation $o$ from the set $\\{0, 2.5, -1.5, 8\\}$.\n2.  For each model $M_i$ from the set $\\{M_1, M_2, M_3, M_4\\}$, with their respective parameters $(\\mu_0, v_0, v_s)$.\n3.  Calculate $\\mathcal{F}(M_i, o)$ using the simplified expression derived above.\n4.  For a given $o$, select the model index $i$ that yields the minimum value of $\\mathcal{F}(M_i, o)$.\n5.  If multiple models yield the same minimum value, the tie is broken by selecting the model with the smallest index (e.g., $M_1$ over $M_2$).\n6.  Collect the winning model indices for each observation in the specified order.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements model selection under the free-energy principle for a set of\n    linear Gaussian models and observations.\n    \"\"\"\n    # Define the models as a list of tuples (mu_0, v_0, v_s).\n    # Indices are 1-based, so models[0] corresponds to M_1.\n    models = [\n        (0.0, 1.0, 4.0),     # M1\n        (0.0, 1.0, 0.25),    # M2\n        (0.0, 10.0, 0.25),   # M3\n        (0.0, 0.1, 0.25),    # M4\n    ]\n\n    # Define the list of observations.\n    observations = [0.0, 2.5, -1.5, 8.0]\n\n    def calculate_free_energy(o, mu0, v0, vs):\n        \"\"\"\n        Calculates the variational free energy F(M, o), which for an exact\n        posterior equals the negative log model evidence -log p(o|M).\n\n        The formula is F = (o-mu0)^2 / (2*(v0+vs)) + 0.5*log(2*pi*(v0+vs)).\n\n        Args:\n            o (float): The observation.\n            mu0 (float): The prior mean.\n            v0 (float): The prior variance.\n            vs (float): The likelihood variance.\n\n        Returns:\n            float: The variational free energy.\n        \"\"\"\n        predictive_variance = v0 + vs\n        \n        # Accuracy term (or prediction error)\n        accuracy_term = (o - mu0)**2 / (2 * predictive_variance)\n        \n        # Complexity term\n        complexity_term = 0.5 * np.log(2 * np.pi * predictive_variance)\n        \n        return accuracy_term + complexity_term\n\n    selected_model_indices = []\n    \n    # Iterate through each observation\n    for o in observations:\n        free_energies = []\n        \n        # Evaluate free energy for each model given the observation\n        for mu0, v0, vs in models:\n            f = calculate_free_energy(o, mu0, v0, vs)\n            free_energies.append(f)\n        \n        # Find the index of the model that minimizes the free energy.\n        # np.argmin returns the index of the first minimum, which handles the tie-breaking rule.\n        # Add 1 to convert from 0-based index to 1-based model index.\n        best_model_index = np.argmin(free_energies) + 1\n        selected_model_indices.append(best_model_index)\n\n    # Format the output as a comma-separated list in square brackets.\n    output_str = f\"[{','.join(map(str, selected_model_indices))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}