{
    "hands_on_practices": [
        {
            "introduction": "主动推理的核心在于选择能最小化期望自由能的行动。这个练习提供了一个具体的、分步的计算过程，使这一抽象原则变得切实可感。通过这个练习，你将学会把期望自由能分解为两个关键组成部分：实用价值（实现偏好的结果）和认知价值（减少不确定性），并理解在一个简单场景中它们如何结合起来指导智能体的选择。",
            "id": "4028595",
            "problem": "考虑一个根据大脑建模和计算神经科学中的自由能原理（FEP）构建的单步决策问题。隐状态是二元的，$s \\in \\{s_{0}, s_{1}\\}$，其当前信念（识别密度）$Q(s)$ 由 $Q(s_{0}) = 0.6$ 和 $Q(s_{1}) = 0.4$ 给出。观测值是二元的，$o \\in \\{0,1\\}$，存在两个候选的单步策略 $\\pi_{1}$ 和 $\\pi_{2}$，分别对应两个行动 $a_{1}$ 和 $a_{2}$，它们如下调节似然映射 $P(o \\mid s, a)$：\n- 在行动 $a_{1}$ 下：$P(o=1 \\mid s_{0}, a_{1}) = 0.8$, $P(o=0 \\mid s_{0}, a_{1}) = 0.2$, $P(o=1 \\mid s_{1}, a_{1}) = 0.3$, $P(o=0 \\mid s_{1}, a_{1}) = 0.7$。\n- 在行动 $a_{2}$ 下：$P(o=1 \\mid s_{0}, a_{2}) = 0.6$, $P(o=0 \\mid s_{0}, a_{2}) = 0.4$, $P(o=1 \\mid s_{1}, a_{2}) = 0.1$, $P(o=0 \\mid s_{1}, a_{2}) = 0.9$。\n\n偏好的结果由先验偏好分布 $P^{\\ast}(o)$ 编码，其中 $P^{\\ast}(1) = 0.9$ 和 $P^{\\ast}(0) = 0.1$。假设只有一个时间步，并且用于评估期望自由能的隐状态先验与当前信念相匹配，即 $P(s) = Q(s)$。使用自然对数 $\\ln$。\n\n从单个观测的变分自由能的定义出发（该定义用识别分布 $Q(s \\mid o, \\pi)$ 和带有结果偏好 $P^{\\ast}(o)$ 的生成模型表示），并且仅使用变分贝叶斯推断的核心定义（例如，KL散度（Kullback–Leibler divergence, KLD）、互信息和全期望定律），推导出一个策略 $\\pi$ 的单步期望自由能 $\\mathcal{G}(\\pi)$ 的表达式，该表达式可以分解为 (i) 一个依赖于 $P^{\\ast}(o)$ 和预测分布 $Q(o \\mid \\pi)$ 的外在项，以及 (ii) 一个在策略 $\\pi$ 下隐状态和结果之间的互信息的认知项。然后，数值计算 $\\mathcal{G}(\\pi_{1})$ 和 $\\mathcal{G}(\\pi_{2})$，并确定哪个策略能最小化 $\\mathcal{G}(\\pi)$。\n\n作为最终答案，报告最小化策略 $\\pi_{i}$ 的索引 $i \\in \\{1,2\\}$。最终索引无需四舍五入。如果需要中间数值计算，请使用精确算术和 $\\ln$ 求值；任何近似值都应在推导过程中明确指出。",
            "solution": "用户希望我解决一个基于自由能原理的问题。\n\n### 第1步：提取已知条件\n-   **隐状态：** $s \\in \\{s_{0}, s_{1}\\}$。\n-   **识别密度（当前信念）：** $Q(s_{0}) = 0.6$, $Q(s_{1}) = 0.4$。\n-   **观测值：** $o \\in \\{0, 1\\}$。\n-   **策略：** $\\pi_1$（行动 $a_1$）和 $\\pi_2$（行动 $a_2$）。\n-   **行动 $a_1$ 的似然映射：**\n    -   $P(o=1 \\mid s_{0}, a_{1}) = 0.8$, $P(o=0 \\mid s_{0}, a_{1}) = 0.2$。\n    -   $P(o=1 \\mid s_{1}, a_{1}) = 0.3$, $P(o=0 \\mid s_{1}, a_{1}) = 0.7$。\n-   **行动 $a_2$ 的似然映射：**\n    -   $P(o=1 \\mid s_{0}, a_{2}) = 0.6$, $P(o=0 \\mid s_{0}, a_{2}) = 0.4$。\n    -   $P(o=1 \\mid s_{1}, a_{2}) = 0.1$, $P(o=0 \\mid s_{1}, a_{2}) = 0.9$。\n-   **偏好结果（先验偏好）：** $P^{\\ast}(1) = 0.9$, $P^{\\ast}(0) = 0.1$。\n-   **假设：** 用于评估EFE的隐状态先验是当前信念：$P(s) = Q(s)$。\n-   **对数：** 使用自然对数 $\\ln$。\n\n### 第2步：使用提取的已知条件进行验证\n该问题在主动推断和自由能原理的既定框架内具有科学依据。其术语和公式在计算神经科学中是标准的。该问题是适定的，提供了所有必要的数值和一个清晰、明确的目标。所提供的所有条件分布的概率总和为 $1$，并且没有内部矛盾。数据是一致的。问题是客观且可形式化的。\n\n### 第3步：结论和行动\n问题有效。将提供完整解答。\n\n### 期望自由能表达式的推导\n\n一个策略 $\\pi$ 的期望自由能 $\\mathcal{G}(\\pi)$ 是在该策略下预期的自由能，对该策略预测的结果 $o$ 进行平均。这可以写成：\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[F(o|\\pi)] $$\n其中 $F(o|\\pi)$ 是与特定未来结果 $o$ 相关的变分自由能。问题要求我们从一个涉及识别分布 $Q(s|o,\\pi)$ 和带有结果偏好 $P^\\ast(o)$ 的生成模型的自由能定义出发。自由能是识别密度 $Q$ 和生成模型 $P$ 的一个泛函，定义为 $F = \\mathbb{E}_{Q}[\\ln Q - \\ln P]$。对于主动推断，生成模型体现了智能体的目标。我们定义一个简单的生成模型，其中状态和结果是独立的，结果遵循偏好分布 $P^\\ast(o)$，状态遵循先验分布 $P(s)$。因此，联合概率为 $P(s,o) = P(s)P^\\ast(o)$。\n\n未来结果 $o$ 的自由能是关于状态的近似后验信念 $Q(s|o,\\pi)$ 与这个目标导向的生成模型 $P(s,o)$ 之间的KL散度（Kullback-Leibler, KL）：\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln Q(s|o,\\pi) - \\ln P(s,o)] $$\n代入 $P(s,o) = P(s)P^\\ast(o)$：\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln Q(s|o,\\pi) - \\ln(P(s)P^\\ast(o))] $$\n$$ F(o|\\pi) = \\mathbb{E}_{Q(s|o,\\pi)}[\\ln \\frac{Q(s|o,\\pi)}{P(s)}] - \\ln P^\\ast(o) $$\n第一项是KL散度 $D_{KL}[Q(s|o,\\pi) || P(s)]$。所以：\n$$ F(o|\\pi) = D_{KL}[Q(s|o,\\pi) || P(s)] - \\ln P^\\ast(o) $$\n现在，我们通过对 $F(o|\\pi)$ 在结果的预测分布 $Q(o|\\pi) = \\sum_s P(o|s,\\pi)Q(s)$ 上取期望来计算期望自由能 $\\mathcal{G}(\\pi)$：\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)] - \\ln P^\\ast(o)] $$\n$$ \\mathcal{G}(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)]] + \\mathbb{E}_{Q(o|\\pi)}[-\\ln P^\\ast(o)] $$\n该表达式可分为两项。\n\n第一项是**认知项**，它量化了关于隐状态的预期信息增益：\n$$ \\mathcal{G}_I(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || P(s)]] $$\n使用问题中的条件，即状态的先验是当前信念 $P(s) = Q(s)$，该项变为：\n$$ \\mathcal{G}_I(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[D_{KL}[Q(s|o,\\pi) || Q(s)]] = \\sum_o Q(o|\\pi) \\sum_s Q(s|o,\\pi) \\ln\\frac{Q(s|o,\\pi)}{Q(s)} $$\n根据定义，这是在策略 $\\pi$ 下，隐状态 $s$ 和结果 $o$ 之间的互信息 $I(s;o|\\pi)$。\n\n第二项是**外在项**（或实用价值），它量化了预期结果与偏好结果的偏离程度：\n$$ \\mathcal{G}_E(\\pi) = \\mathbb{E}_{Q(o|\\pi)}[-\\ln P^\\ast(o)] = \\sum_o Q(o|\\pi) (-\\ln P^\\ast(o)) $$\n这是预测分布 $Q(o|\\pi)$ 和偏好分布 $P^\\ast(o)$ 之间的交叉熵。该项如要求的那样依赖于 $P^{\\ast}(o)$ 和 $Q(o|\\pi)$。\n\n因此，需要最小化的单步期望自由能是：\n$$ \\mathcal{G}(\\pi) = \\mathcal{G}_E(\\pi) + \\mathcal{G}_I(\\pi) $$\n\n### 策略 $\\pi_1$（行动 $a_1$）的计算\n\n当前信念为 $Q(s_0) = 0.6$ 和 $Q(s_1) = 0.4$。\n\n1.  **预测结果分布 $Q(o|\\pi_1)$**：\n    $Q(o=1|\\pi_1) = \\sum_s P(o=1|s, a_1)Q(s) = (0.8)(0.6) + (0.3)(0.4) = 0.48 + 0.12 = 0.6$。\n    $Q(o=0|\\pi_1) = \\sum_s P(o=0|s, a_1)Q(s) = (0.2)(0.6) + (0.7)(0.4) = 0.12 + 0.28 = 0.4$。\n\n2.  **外在项 $\\mathcal{G}_E(\\pi_1)$**：\n    $\\mathcal{G}_E(\\pi_1) = Q(o=0|\\pi_1)(-\\ln P^\\ast(0)) + Q(o=1|\\pi_1)(-\\ln P^\\ast(1))$\n    $\\mathcal{G}_E(\\pi_1) = 0.4(-\\ln 0.1) + 0.6(-\\ln 0.9) = 0.4\\ln(10) - 0.6\\ln(0.9)$。\n\n3.  **认知项 $\\mathcal{G}_I(\\pi_1)$**：\n    首先，我们计算后验信念 $Q(s|o, \\pi_1) = \\frac{P(o|s,a_1)Q(s)}{Q(o|\\pi_1)}$。\n    对于 $o=1$：\n    $Q(s_0|o=1, \\pi_1) = \\frac{0.8 \\times 0.6}{0.6} = 0.8$。\n    $Q(s_1|o=1, \\pi_1) = \\frac{0.3 \\times 0.4}{0.6} = 0.2$。\n    对于 $o=0$：\n    $Q(s_0|o=0, \\pi_1) = \\frac{0.2 \\times 0.6}{0.4} = 0.3$。\n    $Q(s_1|o=0, \\pi_1) = \\frac{0.7 \\times 0.4}{0.4} = 0.7$。\n\n    接下来，我们计算KL散度：\n    $D_{KL}[Q(s|o=1,\\pi_1) || Q(s)] = 0.8\\ln(\\frac{0.8}{0.6}) + 0.2\\ln(\\frac{0.2}{0.4}) = 0.8\\ln(\\frac{4}{3}) + 0.2\\ln(\\frac{1}{2})$。\n    $D_{KL}[Q(s|o=0,\\pi_1) || Q(s)] = 0.3\\ln(\\frac{0.3}{0.6}) + 0.7\\ln(\\frac{0.7}{0.4}) = 0.3\\ln(\\frac{1}{2}) + 0.7\\ln(\\frac{7}{4})$。\n\n    $\\mathcal{G}_I(\\pi_1) = Q(o=1|\\pi_1) D_{KL}[Q(s|o=1,\\pi_1) || Q(s)] + Q(o=0|\\pi_1) D_{KL}[Q(s|o=0,\\pi_1) || Q(s)]$\n    $\\mathcal{G}_I(\\pi_1) = 0.6(0.8\\ln(\\frac{4}{3}) + 0.2\\ln(\\frac{1}{2})) + 0.4(0.3\\ln(\\frac{1}{2}) + 0.7\\ln(\\frac{7}{4}))$\n    $\\mathcal{G}_I(\\pi_1) = 0.48\\ln(\\frac{4}{3}) + 0.12\\ln(\\frac{1}{2}) + 0.12\\ln(\\frac{1}{2}) + 0.28\\ln(\\frac{7}{4})$\n    $\\mathcal{G}_I(\\pi_1) = 0.48\\ln(\\frac{4}{3}) + 0.24\\ln(\\frac{1}{2}) + 0.28\\ln(\\frac{7}{4})$。\n\n4.  **总期望自由能 $\\mathcal{G}(\\pi_1)$**：\n    $\\mathcal{G}(\\pi_1) = \\mathcal{G}_E(\\pi_1) + \\mathcal{G}_I(\\pi_1)$\n    $\\mathcal{G}(\\pi_1) = (0.4\\ln(10) - 0.6\\ln(0.9)) + (0.48\\ln(\\frac{4}{3}) - 0.24\\ln(2) + 0.28\\ln(\\frac{7}{4}))$\n    使用数值： $\\ln(10) \\approx 2.3026$, $\\ln(0.9) \\approx -0.1054$, $\\ln(4/3) \\approx 0.2877$, $\\ln(2) \\approx 0.6931$, $\\ln(7/4) \\approx 0.5596$。\n    $\\mathcal{G}_E(\\pi_1) \\approx 0.4(2.3026) + 0.6(0.1054) = 0.92104 + 0.06324 = 0.98428$。\n    $\\mathcal{G}_I(\\pi_1) \\approx 0.48(0.2877) - 0.24(0.6931) + 0.28(0.5596) = 0.13810 - 0.16634 + 0.15669 = 0.12845$。\n    $\\mathcal{G}(\\pi_1) \\approx 0.98428 + 0.12845 = 1.11273$。\n\n### 策略 $\\pi_2$（行动 $a_2$）的计算\n\n1.  **预测结果分布 $Q(o|\\pi_2)$**：\n    $Q(o=1|\\pi_2) = \\sum_s P(o=1|s, a_2)Q(s) = (0.6)(0.6) + (0.1)(0.4) = 0.36 + 0.04 = 0.4$。\n    $Q(o=0|\\pi_2) = \\sum_s P(o=0|s, a_2)Q(s) = (0.4)(0.6) + (0.9)(0.4) = 0.24 + 0.36 = 0.6$。\n\n2.  **外在项 $\\mathcal{G}_E(\\pi_2)$**：\n    $\\mathcal{G}_E(\\pi_2) = Q(o=0|\\pi_2)(-\\ln P^\\ast(0)) + Q(o=1|\\pi_2)(-\\ln P^\\ast(1))$\n    $\\mathcal{G}_E(\\pi_2) = 0.6(-\\ln 0.1) + 0.4(-\\ln 0.9) = 0.6\\ln(10) - 0.4\\ln(0.9)$。\n\n3.  **认知项 $\\mathcal{G}_I(\\pi_2)$**：\n    首先，我们计算后验信念 $Q(s|o, \\pi_2) = \\frac{P(o|s,a_2)Q(s)}{Q(o|\\pi_2)}$。\n    对于 $o=1$：\n    $Q(s_0|o=1, \\pi_2) = \\frac{0.6 \\times 0.6}{0.4} = 0.9$。\n    $Q(s_1|o=1, \\pi_2) = \\frac{0.1 \\times 0.4}{0.4} = 0.1$。\n    对于 $o=0$：\n    $Q(s_0|o=0, \\pi_2) = \\frac{0.4 \\times 0.6}{0.6} = 0.4$。\n    $Q(s_1|o=0, \\pi_2) = \\frac{0.9 \\times 0.4}{0.6} = 0.6$。\n\n    接下来，我们计算KL散度：\n    $D_{KL}[Q(s|o=1,\\pi_2) || Q(s)] = 0.9\\ln(\\frac{0.9}{0.6}) + 0.1\\ln(\\frac{0.1}{0.4}) = 0.9\\ln(1.5) + 0.1\\ln(0.25)$。\n    $D_{KL}[Q(s|o=0,\\pi_2) || Q(s)] = 0.4\\ln(\\frac{0.4}{0.6}) + 0.6\\ln(\\frac{0.6}{0.4}) = 0.4\\ln(\\frac{2}{3}) + 0.6\\ln(1.5) = 0.2\\ln(1.5)$。\n\n    $\\mathcal{G}_I(\\pi_2) = Q(o=1|\\pi_2) D_{KL}[Q(s|o=1,\\pi_2) || Q(s)] + Q(o=0|\\pi_2) D_{KL}[Q(s|o=0,\\pi_2) || Q(s)]$\n    $\\mathcal{G}_I(\\pi_2) = 0.4(0.9\\ln(1.5) + 0.1\\ln(0.25)) + 0.6(0.2\\ln(1.5))$\n    $\\mathcal{G}_I(\\pi_2) = 0.36\\ln(1.5) + 0.04\\ln(0.25) + 0.12\\ln(1.5) = 0.48\\ln(1.5) + 0.04\\ln(0.25)$。\n\n4.  **总期望自由能 $\\mathcal{G}(\\pi_2)$**：\n    $\\mathcal{G}(\\pi_2) = \\mathcal{G}_E(\\pi_2) + \\mathcal{G}_I(\\pi_2)$\n    $\\mathcal{G}(\\pi_2) = (0.6\\ln(10) - 0.4\\ln(0.9)) + (0.48\\ln(1.5) + 0.04\\ln(0.25))$\n    使用数值：$\\ln(1.5) \\approx 0.4055$, $\\ln(0.25) \\approx -1.3863$。\n    $\\mathcal{G}_E(\\pi_2) \\approx 0.6(2.3026) + 0.4(0.1054) = 1.38156 + 0.04216 = 1.42372$。\n    $\\mathcal{G}_I(\\pi_2) \\approx 0.48(0.4055) + 0.04(-1.3863) = 0.19464 - 0.05545 = 0.13919$。\n    $\\mathcal{G}(\\pi_2) \\approx 1.42372 + 0.13919 = 1.56291$。\n\n### 结论\n\n比较两种策略的总期望自由能：\n$$ \\mathcal{G}(\\pi_1) \\approx 1.11273 $$\n$$ \\mathcal{G}(\\pi_2) \\approx 1.56291 $$\n由于遵循自由能原理的智能体选择最小化期望自由能的策略，我们比较这两个值。我们发现 $\\mathcal{G}(\\pi_1)  \\mathcal{G}(\\pi_2)$。\n因此，策略 $\\pi_1$ 是最优策略。最小化策略的索引是 $i=1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "在理解了指导行动的原则（最小化期望自由能）之后，下一个问题是，大脑究竟是如何执行必要的计算的？这个实践将深入探讨预测性编码，这是一个关于大脑如何实现自由能最小化的重要过程理论。通过编程实现预测性编码更新规则的一个步骤，你将深入了解神经群体之间传递信息以持续抑制预测误差的机制，从而更新对世界的信念。",
            "id": "4028586",
            "problem": "考虑一个适用于自由能原理下预测编码的线性高斯生成模型。设潜状态向量为 $\\mathbf{s} \\in \\mathbb{R}^m$，其先验为零均值高斯分布 $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, I)$，其中 $I$ 是 $m \\times m$ 的单位矩阵。设观测向量为 $\\mathbf{y} \\in \\mathbb{R}^n$，其根据 $\\mathbf{y} \\mid \\mathbf{s} \\sim \\mathcal{N}(W \\mathbf{s}, \\Sigma)$ 生成，其中 $W \\in \\mathbb{R}^{n \\times m}$ 是一个已知的生成映射，$\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是一个已知的正定观测噪声协方差。考虑一个高斯变分后验 $q(\\mathbf{s}) = \\mathcal{N}(\\mu_q, \\Sigma_q)$，其均值为 $\\mu_q \\in \\mathbb{R}^m$，协方差为正定的 $\\Sigma_q \\in \\mathbb{R}^{m \\times m}$。\n\n从第一性原理出发，仅依赖于线性高斯模型的 Bayes 规则、作为对数模型证据下界的负变分自由能的定义，以及 Laplace 近似（在众数附近的高斯近似），推导在自然梯度预处理（由 $\\Sigma_q$ 进行）和单位先验协方差 $I$ 下，均值参数 $\\mu_q$ 的连续时间预测编码动力学。使用这些动力学来定义一个单一的离散更新，该更新对应于步长 $\\eta = 1$ 的一个显式 Euler 步，并计算上述模型应用一次迭代后均值参数的变化，记为 $\\Delta \\mu_q$。您的程序必须实现给定 $(W, \\Sigma, \\mu_q, \\Sigma_q, \\mathbf{y})$ 时 $\\Delta \\mu_q$ 的计算，并且不得依赖任何无法从所述基本定律和定义推导出的公式。\n\n为以下测试套件实现计算。每个案例提供维度为 $n = 2$ 和 $m = 2$ 的 $(W, \\Sigma, \\mu_q, \\Sigma_q, \\mathbf{y})$：\n\n- 案例 1 (一般良态情况):\n  - $W = \\begin{bmatrix} 1.0  0.2 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.5  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.8  0.0 \\\\ 0.0  0.6 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.5 \\\\ -0.5 \\end{bmatrix}$。\n\n- 案例 2 (高观测噪声):\n  - $W = \\begin{bmatrix} 1.0  -0.5 \\\\ 0.3  1.2 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 5.0  0.0 \\\\ 0.0  5.0 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} -0.3 \\\\ 0.4 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 1.1  0.0 \\\\ 0.0  0.9 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 0.2 \\\\ -1.0 \\end{bmatrix}$。\n\n- 案例 3 (相关的后验协方差):\n  - $W = \\begin{bmatrix} 0.7  0.4 \\\\ -0.2  1.3 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.6  0.0 \\\\ 0.0  0.4 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 0.5 \\\\ 0.1 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.7  0.3 \\\\ 0.3  0.9 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$。\n\n- 案例 4 (秩亏的生成映射):\n  - $W = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.1  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$。\n\n- 案例 5 (强感觉精度):\n  - $W = \\begin{bmatrix} 0.9  -0.1 \\\\ 0.2  0.8 \\end{bmatrix}$,\n  - $\\Sigma = \\begin{bmatrix} 0.01  0.0 \\\\ 0.0  0.02 \\end{bmatrix}$,\n  - $\\mu_q = \\begin{bmatrix} -0.1 \\\\ 0.2 \\end{bmatrix}$,\n  - $\\Sigma_q = \\begin{bmatrix} 0.5  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个条目是对应一个测试案例的二维变化向量 $\\Delta \\mu_q$，四舍五入到六位小数，并表示为 Python 风格的列表。例如，格式应为 $[\\,[\\delta\\mu_{1,1}, \\delta\\mu_{1,2}], [\\delta\\mu_{2,1}, \\delta\\mu_{2,2}], \\dots\\,]$。不涉及物理单位。不使用角度。所有数值输出必须是实值浮点数。算法必须根据给定的基础定义实现，不得调用任何预先指定的快捷公式。",
            "solution": "我们从一个线性高斯生成模型和一个高斯变分后验开始。潜状态为 $\\mathbf{s} \\in \\mathbb{R}^m$，其先验为 $p(\\mathbf{s}) = \\mathcal{N}(\\mathbf{0}, I)$，其中 $I$ 是 $m \\times m$ 的单位矩阵。似然函数为 $p(\\mathbf{y} \\mid \\mathbf{s}) = \\mathcal{N}(W \\mathbf{s}, \\Sigma)$，其中 $W \\in \\mathbb{R}^{n \\times m}$，$\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是一个正定协方差。变分后验为 $q(\\mathbf{s}) = \\mathcal{N}(\\mu_q, \\Sigma_q)$，其中 $\\mu_q \\in \\mathbb{R}^m$，$\\Sigma_q \\in \\mathbb{R}^{m \\times m}$ 是正定的。\n\n自由能原理采用变分自由能来界定对数证据。负变分自由能（证据下界）由下式给出\n$$\n\\mathcal{L}(\\mu_q, \\Sigma_q) = \\mathbb{E}_{q(\\mathbf{s})}[\\log p(\\mathbf{y}, \\mathbf{s})] - \\mathbb{E}_{q(\\mathbf{s})}[\\log q(\\mathbf{s})].\n$$\n在 Laplace 近似和自然梯度动力学下，通过对变分参数执行关于 $\\mathcal{L}$ 的梯度上升，预测编码得以产生。这个过程通常由高斯族分布的 Fisher 信息度量进行预处理，在此，对于均值参数，该度量由 $\\Sigma_q$ 表示。为推导此更新，我们从基本的概率定义入手。\n\n首先，写出联合密度：\n$$\n\\log p(\\mathbf{y}, \\mathbf{s}) = \\log p(\\mathbf{y}\\mid \\mathbf{s}) + \\log p(\\mathbf{s}).\n$$\n对于高斯项，\n$$\n\\log p(\\mathbf{y}\\mid \\mathbf{s}) = -\\tfrac{1}{2}(\\mathbf{y} - W \\mathbf{s})^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mathbf{s}) + \\text{const},\n$$\n$$\n\\log p(\\mathbf{s}) = -\\tfrac{1}{2}\\mathbf{s}^\\top I \\mathbf{s} + \\text{const} = -\\tfrac{1}{2}\\mathbf{s}^\\top \\mathbf{s} + \\text{const}.\n$$\n变分后验为\n$$\n\\log q(\\mathbf{s}) = -\\tfrac{1}{2}(\\mathbf{s} - \\mu_q)^\\top \\Sigma_q^{-1}(\\mathbf{s} - \\mu_q) + \\text{const}.\n$$\n我们关注均值的更新。在对均值使用 Laplace 近似时，将负自由能（在常数项之外，等价于期望的负对数后验）视为 $\\mu_q$ 的函数，并通过在均值处求值来近似期望（这是 Laplace 变分方案中的标准做法）。定义能量函数\n$$\nF(\\mu_q) = \\tfrac{1}{2}(\\mathbf{y} - W \\mu_q)^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) + \\tfrac{1}{2}\\mu_q^\\top I \\mu_q,\n$$\n这是在 $\\mu_q$ 处求值的负对数后验（第一项对应似然，第二项对应先验，其中 $I$ 是先验精度）。最小化 $F(\\mu_q)$ 对应于最大化后验概率，并且等价于在 Laplace 近似下，关于 $\\mu_q$ 最大化证据下界。\n\n计算 $F(\\mu_q)$ 关于 $\\mu_q$ 的梯度。使用标准矩阵微积分，\n$$\n\\nabla_{\\mu_q} F(\\mu_q) = - W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) + I \\mu_q.\n$$\n预测编码可以表示为对 $F(\\mu_q)$ 的梯度下降或对 $\\mathcal{L}$ 的梯度上升，并用 $\\Sigma_q$ 进行自然梯度预处理。指数族高斯分布中均值参数的自然梯度步可以通过将普通的欧几里得梯度左乘 $\\Sigma_q$ 来近似，从而产生一个考虑了后验协方差所隐含的局部曲率的方向。因此，在对 $\\mathcal{L}$ 进行自然梯度上升（等价于对 $F$ 进行自然梯度下降）时，$\\mu_q$ 的连续时间动力学变为\n$$\n\\frac{d\\mu_q}{dt} = \\Sigma_q\\left[ W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) - I \\mu_q \\right].\n$$\n取步长 $\\eta = 1$ 的一个显式 Euler 步，可得离散变化\n$$\n\\Delta \\mu_q = \\Sigma_q\\left[ W^\\top \\Sigma^{-1}(\\mathbf{y} - W \\mu_q) - I \\mu_q \\right].\n$$\n这个 $\\Delta \\mu_q$ 就是在规定假设（线性高斯模型、单位先验协方差、Laplace 近似以及由 $\\Sigma_q$ 进行的自然梯度预处理）下，一次预测编码迭代中后验均值的所求变化。\n\n算法设计：\n- 输入：矩阵 $W$、$\\Sigma$、$\\Sigma_q$ 和向量 $\\mu_q$、$\\mathbf{y}$。\n- 验证维度：$W$ 是 $n \\times m$，$ \\Sigma$ 是 $n \\times n$，$ \\Sigma_q$ 是 $m \\times m$，$\\mu_q \\in \\mathbb{R}^m$，$\\mathbf{y} \\in \\mathbb{R}^n$。\n- 计算感觉预测误差 $\\mathbf{e}_y = \\mathbf{y} - W \\mu_q$。\n- 计算精度加权误差 $\\mathbf{r}_y = \\Sigma^{-1} \\mathbf{e}_y$。\n- 计算反向传播到潜层的精度加权误差 $\\mathbf{g} = W^\\top \\mathbf{r}_y$。\n- 计算先验贡献 $\\mathbf{p} = I \\mu_q = \\mu_q$。\n- 组合以形成欧几里得梯度方向 $\\mathbf{d} = \\mathbf{g} - \\mathbf{p}$。\n- 通过 $\\Sigma_q$ 进行预处理以获得自然梯度步 $\\Delta \\mu_q = \\Sigma_q \\mathbf{d}$。\n- 按规定对 $\\Delta \\mu_q$ 进行数值四舍五入并输出。\n\n测试套件中包含了边缘情况：\n- 案例 1 验证一个一般性的、良态的场景。\n- 案例 2 使用大的观测噪声 $\\Sigma$，相对于先验项，抑制了感觉项。\n- 案例 3 使用相关的 $\\Sigma_q$，以在不同维度上运用自然梯度预处理。\n- 案例 4 使用秩亏的 $W$，将感觉信息限制在一个子空间内。\n- 案例 5 使用非常小的观测噪声（高精度），强调了感觉项，并导致较大的校正。\n\n最终程序为每个测试案例实现上述步骤，并以要求的单行格式打印结果，即一个由列表组成的逗号分隔列表，其中每个分量都四舍五入到六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef predictive_coding_delta_mu(W, Sigma, mu_q, Sigma_q, y):\n    \"\"\"\n    Compute one predictive coding iteration change in posterior mean (Delta mu_q)\n    under a linear-Gaussian generative model with identity prior covariance,\n    Laplace approximation, and natural gradient preconditioning by Sigma_q.\n\n    Delta mu_q = Sigma_q * [ W^T * Sigma^{-1} * (y - W * mu_q) - mu_q ]\n    \"\"\"\n    # Sensory prediction error\n    e_y = y - W @ mu_q\n    # Precision-weighted error\n    r_y = np.linalg.solve(Sigma, e_y)\n    # Backpropagated precision-weighted error to latent space\n    g = W.T @ r_y\n    # Prior term (identity prior precision)\n    p = mu_q\n    # Euclidean gradient direction\n    d = g - p\n    # Natural gradient step preconditioned by Sigma_q\n    delta_mu = Sigma_q @ d\n    return delta_mu\n\ndef format_vector(vec):\n    # Round to six decimals and format as Python-style list\n    return \"[\" + \",\".join(f\"{x:.6f}\" for x in vec.tolist()) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            np.array([[1.0, 0.2],\n                      [0.0, 1.0]]),\n            np.array([[0.5, 0.0],\n                      [0.0, 0.5]]),\n            np.array([0.1, -0.2]),\n            np.array([[0.8, 0.0],\n                      [0.0, 0.6]]),\n            np.array([1.5, -0.5])\n        ),\n        # Case 2\n        (\n            np.array([[1.0, -0.5],\n                      [0.3,  1.2]]),\n            np.array([[5.0, 0.0],\n                      [0.0, 5.0]]),\n            np.array([-0.3, 0.4]),\n            np.array([[1.1, 0.0],\n                      [0.0, 0.9]]),\n            np.array([0.2, -1.0])\n        ),\n        # Case 3\n        (\n            np.array([[ 0.7, 0.4],\n                      [-0.2, 1.3]]),\n            np.array([[0.6, 0.0],\n                      [0.0, 0.4]]),\n            np.array([0.5, 0.1]),\n            np.array([[0.7, 0.3],\n                      [0.3, 0.9]]),\n            np.array([1.0, 0.0])\n        ),\n        # Case 4\n        (\n            np.array([[1.0, 0.0],\n                      [0.0, 0.0]]),\n            np.array([[0.1, 0.0],\n                      [0.0, 0.1]]),\n            np.array([2.0, -1.0]),\n            np.array([[1.0, 0.0],\n                      [0.0, 1.0]]),\n            np.array([1.0, 0.0])\n        ),\n        # Case 5\n        (\n            np.array([[0.9, -0.1],\n                      [0.2,  0.8]]),\n            np.array([[0.01, 0.0],\n                      [0.0,  0.02]]),\n            np.array([-0.1, 0.2]),\n            np.array([[0.5, 0.0],\n                      [0.0, 0.5]]),\n            np.array([0.0, 1.0])\n        ),\n    ]\n\n    results = []\n    for W, Sigma, mu_q, Sigma_q, y in test_cases:\n        delta_mu = predictive_coding_delta_mu(W, Sigma, mu_q, Sigma_q, y)\n        results.append(format_vector(delta_mu))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了推断状态和选择行动，一个成熟的智能体还必须能够在不同的世界模型之间进行选择。这个练习展示了自由能原理如何为模型选择提供一个自然的框架，即在模型的准确性和复杂性之间进行权衡。你将通过编写程序来观察，在给定一些数据的情况下，智能体如何利用自由能最小化原则在关于其环境结构的竞争性假设之间做出裁决。",
            "id": "4063890",
            "problem": "您将实现并分析线性高斯生成模型中基于自由能原理的模型选择，重点关注准确性与复杂度的权衡。考虑一个单变量潜变量 $s \\in \\mathbb{R}$ 和一个观测变量 $o \\in \\mathbb{R}$。一个生成模型 $M$ 由高斯先验和高斯似然指定：先验 $p(s \\mid M) = \\mathcal{N}(s ; \\mu_0, v_0)$，其均值为 $\\mu_0 \\in \\mathbb{R}$，方差为 $v_0 \\in \\mathbb{R}_{0}$；似然 $p(o \\mid s, M) = \\mathcal{N}(o ; s, v_s)$，其方差为 $v_s \\in \\mathbb{R}_{0}$。近似后验 $q(s \\mid o, M)$ 被约束为高斯分布。在共轭高斯情况下，精确后验 $p(s \\mid o, M)$ 是高斯分布，并且属于该族。\n\n您的任务是：\n- 从贝叶斯法则 $p(s \\mid o, M) \\propto p(o \\mid s, M) \\, p(s \\mid M)$ 和高斯密度的归一化出发，推导出精确后验 $p(s \\mid o, M)$ 的闭式解，包括其均值和方差，表示为 $\\mu_0$、$v_0$、$v_s$ 和 $o$ 的函数。\n- 使用期望值和 Kullback-Leibler 散度的定义，定义 $q(s \\mid o, M)$ 的两个泛函：准确性 $\\mathcal{A}(M, o)$ 定义为期望对数似然 $\\mathbb{E}_{q}[\\log p(o \\mid s, M)]$，复杂度 $\\mathcal{C}(M, o)$ 定义为散度 $\\mathrm{KL}(q(s \\mid o, M) \\, \\| \\, p(s \\mid M))$。当 $q(s \\mid o, M)$ 是高斯分布时，以闭式形式表示 $\\mathcal{A}(M, o)$ 和 $\\mathcal{C}(M, o)$。所有对数均为自然对数，所有信息论量均以奈特 (nats) 为单位。\n- 将变分自由能 $\\mathcal{F}(M, o)$ 定义为差值 $\\mathcal{C}(M, o) - \\mathcal{A}(M, o)$。实现一个程序，对于每个观测值 $o$，评估给定集合中每个模型 $M$ 的 $\\mathcal{F}(M, o)$，并选择使 $\\mathcal{F}(M, o)$ 最小化的模型索引。如果多个模型的 $\\mathcal{F}$ 值完全相等，则通过选择最小的索引来打破平局。\n\n仅使用以下固定的模型和观测值测试套件：\n- 模型（由 $i \\in \\{1, 2, 3, 4\\}$ 索引），每个模型具有参数 $(\\mu_0, v_0, v_s)$：\n  - $M_1$：$(\\mu_0, v_0, v_s) = (0, 1, 4)$。\n  - $M_2$：$(\\mu_0, v_0, v_s) = (0, 1, 0.25)$。\n  - $M_3$：$(\\mu_0, v_0, v_s) = (0, 10, 0.25)$。\n  - $M_4$：$(\\mu_0, v_0, v_s) = (0, 0.1, 0.25)$。\n- 观测值（按顺序）：$o \\in \\{0, 2.5, -1.5, 8\\}$。\n\n您的程序必须：\n- 对于列表中的每个观测值 $o$，使用您从第一性原理为高斯情况（其中 $q(s \\mid o, M) = p(s \\mid o, M)$）推导出的闭式表达式，计算 $i \\in \\{1, 2, 3, 4\\}$ 的 $\\mathcal{F}(M_i, o)$。\n- 生成一行输出，其中包含所选模型索引的逗号分隔列表，并用方括号括起来（例如，$[1,2,3,4]$）。输出必须与上面给出的观测值顺序相对应，并且每个观测值恰好包含一个整数。\n\n不涉及物理单位。不出现角度。所有返回值都是无量纲的。通过确保方差 $v_0$ 和 $v_s$ 的正性并使用自然对数来保证数值稳定性。最终输出必须是整数。程序不得要求任何输入。评估必须以自然单位（奈特）进行。",
            "solution": "该问题是有效且适定的，其基础是标准的贝叶斯推断和信息论。我们逐步推导所需的量和后续的计算方法。\n\n### 1. 精确后验分布的推导\n\n生成模型 $M$ 由高斯先验和高斯似然定义：\n- 先验：$p(s \\mid M) = \\mathcal{N}(s ; \\mu_0, v_0)$\n- 似然：$p(o \\mid s, M) = \\mathcal{N}(o ; s, v_s)$\n\n根据贝叶斯法则，给定观测值 $o$ 的潜变量 $s$ 的后验分布与似然和先验的乘积成正比：\n$$p(s \\mid o, M) \\propto p(o \\mid s, M) p(s \\mid M)$$\n由于先验和似然都是高斯分布，它们的乘积也与一个高斯函数成正比，这意味着后验分布是高斯分布。我们可以通过检查所得分布的指数来确定其参数。对数后验由下式给出：\n$$\\log p(s \\mid o, M) = \\log p(o \\mid s, M) + \\log p(s \\mid M) + \\text{const.}$$\n代入高斯分布的概率密度函数（并忽略归一化常数）：\n$$\\log p(s \\mid o, M) = -\\frac{(o-s)^2}{2v_s} - \\frac{(s-\\mu_0)^2}{2v_0} + \\text{const.}$$\n为了找到后验均值和方差，我们展开二次项并收集 $s^2$ 和 $s$ 的项：\n$$\\log p(s \\mid o, M) = -\\frac{1}{2v_s}(o^2 - 2os + s^2) - \\frac{1}{2v_0}(s^2 - 2s\\mu_0 + \\mu_0^2) + \\text{const.}$$\n$$= s^2 \\left(-\\frac{1}{2v_s} - \\frac{1}{2v_0}\\right) + s \\left(\\frac{o}{v_s} + \\frac{\\mu_0}{v_0}\\right) + \\text{const.}$$\n这是 $s$ 的二次型，是高斯对数密度的特征。一个通用的高斯后验 $p(s \\mid o, M) = \\mathcal{N}(s; \\mu_p, v_p)$ 具有以下形式的对数密度：\n$$\\log \\mathcal{N}(s; \\mu_p, v_p) = -\\frac{(s-\\mu_p)^2}{2v_p} + \\text{const.} = s^2\\left(-\\frac{1}{2v_p}\\right) + s\\left(\\frac{\\mu_p}{v_p}\\right) + \\text{const.}$$\n通过比较 $s^2$ 项的系数，我们找到后验精度（方差的倒数）：\n$$\\frac{1}{v_p} = \\frac{1}{v_0} + \\frac{1}{v_s} = \\frac{v_0 + v_s}{v_0 v_s}$$\n因此，后验方差 $v_p$ 为：\n$$v_p = \\frac{v_0 v_s}{v_0 + v_s}$$\n通过比较 $s$ 项的系数，我们找到后验均值 $\\mu_p$：\n$$\\frac{\\mu_p}{v_p} = \\frac{\\mu_0}{v_0} + \\frac{o}{v_s}$$\n$$\\mu_p = v_p \\left( \\frac{\\mu_0}{v_0} + \\frac{o}{v_s} \\right) = \\frac{v_0 v_s}{v_0 + v_s} \\left( \\frac{\\mu_0 v_s + o v_0}{v_0 v_s} \\right)$$\n因此，后验均值 $\\mu_p$ 是先验均值和观测值的精度加权平均值：\n$$\\mu_p = \\frac{v_s \\mu_0 + v_0 o}{v_0 + v_s}$$\n问题指定使用近似后验 $q(s \\mid o, M)$，在这个共轭情况下它就是精确后验。设 $q(s \\mid o, M) = \\mathcal{N}(s; \\mu_q, v_q)$，其中 $\\mu_q = \\mu_p$ 且 $v_q = v_p$。\n\n### 2. 准确性与复杂度的推导\n\n**准确性 $\\mathcal{A}(M, o)$** 是在近似后验 $q$ 下的期望对数似然：\n$$\\mathcal{A}(M, o) = \\mathbb{E}_{q}[\\log p(o \\mid s, M)]$$\n对数似然为：\n$$\\log p(o \\mid s, M) = \\log\\left(\\frac{1}{\\sqrt{2\\pi v_s}}\\exp\\left(-\\frac{(o-s)^2}{2v_s}\\right)\\right) = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{(o-s)^2}{2v_s}$$\n关于 $q(s \\mid o, M) = \\mathcal{N}(s; \\mu_q, v_q)$ 取期望：\n$$\\mathcal{A}(M, o) = \\mathbb{E}_{q}\\left[-\\frac{1}{2}\\log(2\\pi v_s) - \\frac{(o-s)^2}{2v_s}\\right] = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{1}{2v_s}\\mathbb{E}_{q}[(o-s)^2]$$\n平方误差的期望值为：\n$$\\mathbb{E}_{q}[(o-s)^2] = \\mathbb{E}_{q}[(s-o)^2] = \\mathbb{E}_{q}[(s-\\mu_q + \\mu_q-o)^2]$$\n$$= \\mathbb{E}_{q}[(s-\\mu_q)^2 + 2(s-\\mu_q)(\\mu_q-o) + (\\mu_q-o)^2]$$\n$$= \\mathbb{E}_{q}[(s-\\mu_q)^2] + 2(\\mu_q-o)\\mathbb{E}_{q}[s-\\mu_q] + (\\mu_q-o)^2$$\n由于 $\\mathbb{E}_{q}[s-\\mu_q] = 0$ 且 $\\mathbb{E}_{q}[(s-\\mu_q)^2] = v_q$，这可以简化为：\n$$\\mathbb{E}_{q}[(o-s)^2] = v_q + (\\mu_q-o)^2$$\n将其代回，准确性的闭式表达式为：\n$$\\mathcal{A}(M, o) = -\\frac{1}{2}\\log(2\\pi v_s) - \\frac{v_q + (o-\\mu_q)^2}{2v_s}$$\n\n**复杂度 $\\mathcal{C}(M, o)$** 是从先验到后验的 Kullback-Leibler (KL) 散度：\n$$\\mathcal{C}(M, o) = \\mathrm{KL}(q(s \\mid o, M) \\, \\| \\, p(s \\mid M))$$\n对于两个单变量高斯分布 $q(s) = \\mathcal{N}(s; \\mu_q, v_q)$ 和 $p(s) = \\mathcal{N}(s; \\mu_0, v_0)$，KL 散度由标准公式给出：\n$$\\mathrm{KL}(q \\| p) = \\frac{1}{2} \\left( \\log\\frac{v_0}{v_q} + \\frac{v_q}{v_0} + \\frac{(\\mu_q - \\mu_0)^2}{v_0} - 1 \\right)$$\n因此，复杂度的闭式表达式为：\n$$\\mathcal{C}(M, o) = \\frac{1}{2} \\left( \\log\\left(\\frac{v_0}{v_q}\\right) + \\frac{v_q + (\\mu_q - \\mu_0)^2}{v_0} - 1 \\right)$$\n\n### 3. 变分自由能与计算策略\n\n变分自由能 $\\mathcal{F}(M, o)$ 定义为：\n$$\\mathcal{F}(M, o) = \\mathcal{C}(M, o) - \\mathcal{A}(M, o)$$\n这个量是负对数证据 $-\\log p(o \\mid M)$ 的一个上界。当近似后验是精确后验时，即 $q(s \\mid o, M) = p(s \\mid o, M)$，这个界变成一个等式：\n$$\\mathcal{F}(M, o) = -\\log p(o \\mid M)$$\n这为计算提供了一个更简单的表达式。边缘似然或模型证据 $p(o \\mid M)$ 是通过对潜变量 $s$ 上的联合分布 $p(o,s \\mid M) = p(o \\mid s, M)p(s \\mid M)$ 进行边缘化得到的：\n$$p(o \\mid M) = \\int p(o \\mid s, M) p(s \\mid M) \\,ds$$\n这个积分代表了两个高斯分布的卷积：$\\mathcal{N}(s; \\mu_0, v_0)$ 和一个给定 $s$ 的 $o$ 的核，对应于 $\\mathcal{N}(o; s, v_s)$，这可以看作是 $o = s+\\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0,v_s)$。$o$ 的结果分布也是高斯分布。其均值是均值之和（$\\mu_0 + 0 = \\mu_0$），其方差是方差之和（$v_0 + v_s$），因为 $s$ 和 $\\epsilon$ 是独立的。\n$$p(o \\mid M) = \\mathcal{N}(o; \\mu_0, v_0+v_s)$$\n因此，变分自由能就是该边缘似然的负对数：\n$$\\mathcal{F}(M, o) = -\\log \\left( \\frac{1}{\\sqrt{2\\pi(v_0+v_s)}} \\exp\\left(-\\frac{(o-\\mu_0)^2}{2(v_0+v_s)}\\right) \\right)$$\n$$\\mathcal{F}(M, o) = \\frac{(o-\\mu_0)^2}{2(v_0+v_s)} + \\frac{1}{2}\\log(2\\pi(v_0+v_s))$$\n该表达式代表了一种权衡。第一项 $\\frac{(o-\\mu_0)^2}{2(v_0+v_s)}$ 是一个准确性或预测误差项，它惩罚那些先验预测均值 $\\mu_0$ 远离观测值 $o$ 的模型，并由预测不确定性 $v_0+v_s$ 进行调节。第二项 $\\frac{1}{2}\\log(2\\pi(v_0+v_s))$ 充当复杂度惩罚，惩罚具有较大预测不确定性的模型。最小化 $\\mathcal{F}$ 实现了准确性与复杂度之间的最佳平衡，这就是贝叶斯模型选择的原理。\n\n计算过程如下：\n1.  对于集合 $\\{0, 2.5, -1.5, 8\\}$ 中的每个观测值 $o$。\n2.  对于集合 $\\{M_1, M_2, M_3, M_4\\}$ 中的每个模型 $M_i$，及其各自的参数 $(\\mu_0, v_0, v_s)$。\n3.  使用上面推导的简化表达式计算 $\\mathcal{F}(M_i, o)$。\n4.  对于给定的 $o$，选择产生 $\\mathcal{F}(M_i, o)$ 最小值的模型索引 $i$。\n5.  如果多个模型产生相同的最小值，则通过选择具有最小索引的模型来打破平局（例如，选择 $M_1$ 而非 $M_2$）。\n6.  按指定顺序收集每个观测值的获胜模型索引。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements model selection under the free-energy principle for a set of\n    linear Gaussian models and observations.\n    \"\"\"\n    # Define the models as a list of tuples (mu_0, v_0, v_s).\n    # Indices are 1-based, so models[0] corresponds to M_1.\n    models = [\n        (0.0, 1.0, 4.0),     # M1\n        (0.0, 1.0, 0.25),    # M2\n        (0.0, 10.0, 0.25),   # M3\n        (0.0, 0.1, 0.25),    # M4\n    ]\n\n    # Define the list of observations.\n    observations = [0.0, 2.5, -1.5, 8.0]\n\n    def calculate_free_energy(o, mu0, v0, vs):\n        \"\"\"\n        Calculates the variational free energy F(M, o), which for an exact\n        posterior equals the negative log model evidence -log p(o|M).\n\n        The formula is F = (o-mu0)^2 / (2*(v0+vs)) + 0.5*log(2*pi*(v0+vs)).\n\n        Args:\n            o (float): The observation.\n            mu0 (float): The prior mean.\n            v0 (float): The prior variance.\n            vs (float): The likelihood variance.\n\n        Returns:\n            float: The variational free energy.\n        \"\"\"\n        predictive_variance = v0 + vs\n        \n        # Accuracy term (or prediction error)\n        accuracy_term = (o - mu0)**2 / (2 * predictive_variance)\n        \n        # Complexity term\n        complexity_term = 0.5 * np.log(2 * np.pi * predictive_variance)\n        \n        return accuracy_term + complexity_term\n\n    selected_model_indices = []\n    \n    # Iterate through each observation\n    for o in observations:\n        free_energies = []\n        \n        # Evaluate free energy for each model given the observation\n        for mu0, v0, vs in models:\n            f = calculate_free_energy(o, mu0, v0, vs)\n            free_energies.append(f)\n        \n        # Find the index of the model that minimizes the free energy.\n        # np.argmin returns the index of the first minimum, which handles the tie-breaking rule.\n        # Add 1 to convert from 0-based index to 1-based model index.\n        best_model_index = np.argmin(free_energies) + 1\n        selected_model_indices.append(best_model_index)\n\n    # Format the output as a comma-separated list in square brackets.\n    output_str = f\"[{','.join(map(str, selected_model_indices))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}