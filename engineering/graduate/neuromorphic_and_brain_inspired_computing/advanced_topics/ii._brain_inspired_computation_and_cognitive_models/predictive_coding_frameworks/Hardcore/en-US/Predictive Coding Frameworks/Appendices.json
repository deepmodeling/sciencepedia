{
    "hands_on_practices": [
        {
            "introduction": "At its core, predictive coding formalizes the 'Bayesian brain' hypothesis, where the brain continuously updates its internal model of the world. This first exercise provides a foundational mathematical look at this process. By deriving the posterior distribution for a simple linear-Gaussian model, you will see precisely how a prior belief is combined with sensory evidence to form an updated belief, and how the reliability (or precision) of each source of information dictates its influence on the final estimate .",
            "id": "5052192",
            "problem": "Consider a single-latent-variable generative model commonly used to formalize Predictive Coding and Bayesian brain frameworks in neurobiology. A latent cause $x$ represents the brain’s prediction about a sensory input, and a measurement $y$ is the observed sensory signal. Assume a Gaussian prior on the latent cause, $x \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$, and a Gaussian likelihood for the observation given the cause, $y \\mid x \\sim \\mathcal{N}(x, \\sigma_{y}^{2})$, where $\\sigma_{0}^{2} > 0$ and $\\sigma_{y}^{2} > 0$ are known constants. Using Bayes’ rule and the well-tested fact that products of Gaussian densities yield another Gaussian density in the variable of interest, derive the posterior density $p(x \\mid y)$ in closed form and show that it is Gaussian. Then express the posterior mean and posterior variance using precision-weighted combinations of the prior mean and the observation. Define precision as the inverse of variance, so that $\\tau_{0} = 1 / \\sigma_{0}^{2}$ and $\\tau_{y} = 1 / \\sigma_{y}^{2}$. Provide the final expressions for the posterior mean and the posterior variance in terms of $\\mu_{0}$, $y$, $\\tau_{0}$, and $\\tau_{y}$. No numerical approximation is required; present exact symbolic expressions. The final answer must be a calculation and should be expressed as a single row matrix containing the posterior mean and posterior variance.",
            "solution": "The problem statement is evaluated and found to be valid. It is a well-posed problem in Bayesian statistics, grounded in established mathematical principles and directly applicable to the specified neurobiological framework of predictive coding. All necessary information is provided, and the terms are unambiguous.\n\nThe task is to derive the posterior probability density function (PDF) $p(x \\mid y)$ for a latent cause $x$ given an observation $y$. We are given the prior distribution for $x$ and the likelihood function for $y$ given $x$.\n\nThe prior is a Gaussian distribution:\n$$p(x) = \\mathcal{N}(x \\mid \\mu_{0}, \\sigma_{0}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\nThe likelihood is also a Gaussian distribution:\n$$p(y \\mid x) = \\mathcal{N}(y \\mid x, \\sigma_{y}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y}^{2}}} \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_{y}^{2}}\\right)$$\nAccording to Bayes' rule, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\nSubstituting the given distributions, we have:\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_{y}^{2}}\\right) \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\nWe can combine the arguments of the exponential functions:\n$$p(x \\mid y) \\propto \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_{y}^{2}} - \\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}} \\right)$$\nTo find the form of the posterior, we analyze the exponent, focusing on the terms that depend on $x$. Let $E(x)$ be the argument of the exponential:\n$$E(x) = -\\frac{1}{2} \\left( \\frac{(y - x)^2}{\\sigma_{y}^{2}} + \\frac{(x - \\mu_{0})^2}{\\sigma_{0}^{2}} \\right)$$\nExpanding the squared terms:\n$$E(x) = -\\frac{1}{2} \\left( \\frac{y^2 - 2yx + x^2}{\\sigma_{y}^{2}} + \\frac{x^2 - 2x\\mu_{0} + \\mu_{0}^2}{\\sigma_{0}^{2}} \\right)$$\nWe now group the terms by powers of $x$:\n$$E(x) = -\\frac{1}{2} \\left[ x^2 \\left(\\frac{1}{\\sigma_{0}^{2}} + \\frac{1}{\\sigma_{y}^{2}}\\right) - 2x \\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{y}{\\sigma_{y}^{2}}\\right) + \\left(\\frac{\\mu_{0}^2}{\\sigma_{0}^{2}} + \\frac{y^2}{\\sigma_{y}^{2}}\\right) \\right]$$\nThe problem defines precision as the inverse of variance: $\\tau_{0} = 1/\\sigma_{0}^{2}$ and $\\tau_{y} = 1/\\sigma_{y}^{2}$. Substituting these into the expression:\n$$E(x) = -\\frac{1}{2} \\left[ x^2 (\\tau_{0} + \\tau_{y}) - 2x (\\tau_{0}\\mu_{0} + \\tau_{y}y) + (\\tau_{0}\\mu_{0}^2 + \\tau_{y}y^2) \\right]$$\nThis expression is a quadratic function of $x$. This implies that the posterior distribution $p(x \\mid y)$ is a Gaussian distribution, as stated in the problem. A generic Gaussian PDF for a variable $x$ with mean $\\mu_{\\text{post}}$ and variance $\\sigma_{\\text{post}}^2$ has an exponent of the form:\n$$-\\frac{(x - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2} = -\\frac{1}{2\\sigma_{\\text{post}}^2} (x^2 - 2x\\mu_{\\text{post}} + \\mu_{\\text{post}}^2) = -\\frac{1}{2} (\\tau_{\\text{post}}x^2 - 2\\tau_{\\text{post}}\\mu_{\\text{post}}x + \\text{const})$$\nwhere $\\tau_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ is the posterior precision.\n\nBy comparing the coefficients of the powers of $x$ in our derived exponent $E(x)$ with the general form, we can identify the parameters of the posterior distribution.\nComparing the coefficients of the $x^2$ term:\n$$\\tau_{\\text{post}} = \\tau_{0} + \\tau_{y}$$\nThis shows that the posterior precision is the sum of the prior precision and the likelihood precision. The posterior variance, $\\sigma_{\\text{post}}^2$, is the inverse of the posterior precision:\n$$\\sigma_{\\text{post}}^2 = \\frac{1}{\\tau_{\\text{post}}} = \\frac{1}{\\tau_{0} + \\tau_{y}}$$\nNow, comparing the coefficients of the $x$ term:\n$$2\\tau_{\\text{post}}\\mu_{\\text{post}} = 2(\\tau_{0}\\mu_{0} + \\tau_{y}y)$$\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{\\text{post}}}$$\nSubstituting the expression for $\\tau_{\\text{post}}$:\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{0} + \\tau_{y}}$$\nThe posterior mean is a precision-weighted average of the prior mean $\\mu_{0}$ and the observed data $y$. The terms in $E(x)$ that do not depend on $x$ are absorbed into the normalization constant of the posterior Gaussian PDF.\n\nThus, the posterior distribution $p(x \\mid y)$ is a Gaussian $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$ with the derived mean and variance.\n\nThe posterior mean is:\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{0} + \\tau_{y}}$$\nThe posterior variance is:\n$$\\sigma_{\\text{post}}^2 = \\frac{1}{\\tau_{0} + \\tau_{y}}$$\nThese are the final symbolic expressions required.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\tau_{0} \\mu_{0} + \\tau_{y} y}{\\tau_{0} + \\tau_{y}} & \\frac{1}{\\tau_{0} + \\tau_{y}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from the static Bayesian solution to the dynamic process of inference, this practice explores how predictive coding can be implemented as an online, iterative algorithm. Here, you will work with a nonlinear generative model, which is more typical of complex biological systems. The task is to derive the gradient-based update rule that minimizes variational free energy, thereby connecting the abstract principle of prediction error minimization to a concrete computational step used to update the brain's internal estimates .",
            "id": "4011082",
            "problem": "Consider a single-level nonlinear generative model used in predictive coding frameworks for brain modeling and computational neuroscience. An observation $y$ is generated from a latent state $x$ via a differentiable nonlinear observation function $g(x)$ with additive Gaussian noise. Assume the observation function is the logistic sigmoid $g(x) = \\frac{1}{1 + \\exp(-x)}$. The observation model is $y \\mid x \\sim \\mathcal{N}(g(x), \\Pi_{s}^{-1})$, where $\\Pi_{s} > 0$ is the sensory precision (inverse variance). The latent state has a Gaussian prior $x \\sim \\mathcal{N}(\\mu_{0}, R^{-1})$, where $R > 0$ is the prior precision. \n\nUsing the principle of Variational Free Energy (VFE) minimization within Predictive Coding (PC), one can update an estimate of $x$ by performing gradient ascent on the log joint density $\\ln p(y, x)$ with a small positive step size $\\gamma$, that is, $x \\leftarrow x + \\gamma \\nabla_{x} \\ln p(y, x)$.\n\nStarting from first principles of Gaussian likelihoods and priors, derive the gradient $\\nabla_{x} \\ln p(y, x)$ for this model in terms of $y$, $x$, $g(x)$, $g'(x)$, $\\Pi_{s}$, and $R$. Then, for the specific values $x = 0$, $y = 0.8$, $\\mu_{0} = 0.1$, $R = 5$, $\\Pi_{s} = 20$, and $\\gamma = 0.05$, compute the one-step updated state $x_{\\text{new}}$ explicitly. You must compute and use the exact derivative $g'(x)$ at the given $x$. Express your final answer as a single real number. No rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We can therefore proceed with the derivation and calculation.\n\nThe core of the problem is to compute the gradient of the log joint probability density, $\\nabla_{x} \\ln p(y, x)$, and then use it to perform a one-step update of the latent state estimate $x$. The joint probability $p(y, x)$ is given by the product of the likelihood $p(y|x)$ and the prior $p(x)$, based on the chain rule of probability:\n$$p(y, x) = p(y | x) p(x)$$\nTaking the natural logarithm of this expression, we get:\n$$\\ln p(y, x) = \\ln p(y | x) + \\ln p(x)$$\nWe will now define the explicit forms for the log-likelihood and log-prior.\n\nFirst, we consider the likelihood $p(y|x)$. The problem states that the observation $y$ is drawn from a normal distribution with mean $g(x)$ and precision $\\Pi_{s}$ (which corresponds to a variance of $\\Pi_{s}^{-1}$). The probability density function (PDF) is:\n$$p(y | x) = \\mathcal{N}(y; g(x), \\Pi_{s}^{-1}) = \\frac{1}{\\sqrt{2\\pi (\\Pi_{s}^{-1})}} \\exp\\left(-\\frac{(y - g(x))^2}{2 (\\Pi_{s}^{-1})}\\right)$$\nSimplifying the expression:\n$$p(y | x) = \\sqrt{\\frac{\\Pi_{s}}{2\\pi}} \\exp\\left(-\\frac{\\Pi_{s}}{2} (y - g(x))^2\\right)$$\nThe log-likelihood is therefore:\n$$\\ln p(y | x) = \\ln\\left(\\sqrt{\\frac{\\Pi_{s}}{2\\pi}}\\right) - \\frac{\\Pi_{s}}{2} (y - g(x))^2$$\nThe first term is a constant with respect to $x$.\n\nSecond, we consider the prior $p(x)$. The problem states that the latent state $x$ is drawn from a normal distribution with mean $\\mu_{0}$ and precision $R$ (variance $R^{-1}$). The PDF is:\n$$p(x) = \\mathcal{N}(x; \\mu_{0}, R^{-1}) = \\frac{1}{\\sqrt{2\\pi (R^{-1})}} \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2 (R^{-1})}\\right)$$\nSimplifying the expression:\n$$p(x) = \\sqrt{\\frac{R}{2\\pi}} \\exp\\left(-\\frac{R}{2} (x - \\mu_{0})^2\\right)$$\nThe log-prior is:\n$$\\ln p(x) = \\ln\\left(\\sqrt{\\frac{R}{2\\pi}}\\right) - \\frac{R}{2} (x - \\mu_{0})^2$$\nAgain, the first term is a constant with respect to $x$.\n\nCombining the log-likelihood and log-prior, the log joint probability density is:\n$$\\ln p(y, x) = -\\frac{\\Pi_{s}}{2} (y - g(x))^2 - \\frac{R}{2} (x - \\mu_{0})^2 + C$$\nwhere $C = \\ln\\left(\\sqrt{\\frac{\\Pi_{s}}{2\\pi}}\\right) + \\ln\\left(\\sqrt{\\frac{R}{2\\pi}}\\right)$ is a constant that does not depend on $x$.\n\nTo find the gradient $\\nabla_{x} \\ln p(y, x)$, we differentiate this expression with respect to $x$.\n$$\\nabla_{x} \\ln p(y, x) = \\frac{d}{dx} \\left( -\\frac{\\Pi_{s}}{2} (y - g(x))^2 - \\frac{R}{2} (x - \\mu_{0})^2 \\right)$$\nUsing the chain rule for each term:\n\\begin{align*} \\nabla_{x} \\ln p(y, x) &= -\\frac{\\Pi_{s}}{2} \\cdot 2(y - g(x)) \\cdot \\frac{d}{dx}(-g(x)) - \\frac{R}{2} \\cdot 2(x - \\mu_{0}) \\cdot \\frac{d}{dx}(x) \\\\ &= -\\Pi_{s}(y - g(x))(-g'(x)) - R(x - \\mu_{0}) \\\\ &= \\Pi_{s}(y - g(x))g'(x) - R(x - \\mu_{0}) \\end{align*}\nThis is the general expression for the gradient. In the context of predictive coding, the term $\\Pi_{s}(y - g(x))$ represents the precision-weighted sensory prediction error, and the term $-R(x - \\mu_{0})$ represents the precision-weighted prior prediction error.\n\nNow, we must compute the numerical value of the updated state $x_{\\text{new}}$. The update rule is given by:\n$$x_{\\text{new}} = x + \\gamma \\nabla_{x} \\ln p(y, x)$$\nWe are given the initial state $x=0$, and the parameters $y=0.8$, $\\mu_{0}=0.1$, $R=5$, $\\Pi_{s}=20$, and $\\gamma=0.05$.\n\nFirst, we evaluate $g(x)$ and its derivative $g'(x)$ at $x=0$.\nThe observation function is the logistic sigmoid: $g(x) = \\frac{1}{1 + \\exp(-x)}$.\nAt $x=0$:\n$$g(0) = \\frac{1}{1 + \\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2} = 0.5$$\nThe derivative of the sigmoid function is $g'(x) = g(x)(1-g(x))$.\nAt $x=0$:\n$$g'(0) = g(0)(1-g(0)) = \\frac{1}{2}\\left(1-\\frac{1}{2}\\right) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4} = 0.25$$\n\nNext, we substitute these values and the given parameters into the gradient expression:\n\\begin{align*} \\nabla_{x} \\ln p(y, x) \\Big|_{x=0} &= \\Pi_{s}(y - g(0))g'(0) - R(0 - \\mu_{0}) \\\\ &= 20(0.8 - 0.5)(0.25) - 5(0 - 0.1) \\\\ &= 20(0.3)(0.25) - 5(-0.1) \\\\ &= 6(0.25) + 0.5 \\\\ &= 1.5 + 0.5 \\\\ &= 2 \\end{align*}\nThe value of the gradient at $x=0$ is $2$.\n\nFinally, we apply the update rule to find $x_{\\text{new}}$:\n$$x_{\\text{new}} = x + \\gamma \\cdot (\\text{gradient})$$\n$$x_{\\text{new}} = 0 + 0.05 \\cdot 2$$\n$$x_{\\text{new}} = 0.1$$\nThe updated state after one step is $0.1$.",
            "answer": "$$\\boxed{0.1}$$"
        },
        {
            "introduction": "Predictive coding is not limited to passive perception; its most powerful extension is Active Inference, which explains how we choose actions. This principle suggests that we act to minimize our future uncertainty about the world. This hands-on problem demonstrates this concept by asking you to calculate the expected information gain (or reduction in entropy) for different actions, allowing an agent to select the action that will most effectively resolve ambiguity about its environment .",
            "id": "4011073",
            "problem": "Consider a binary hidden-state generative model in the tradition of predictive coding and Active Inference (AI), where the agent can take actions to influence the informativeness of subsequent sensory data. The hidden state is denoted by $s \\in \\{0,1\\}$, the observation by $o \\in \\{0,1\\}$, and the action by $a \\in \\{0,1\\}$, where $a=0$ indicates a less informative (passive) sampling and $a=1$ indicates a more informative (active) sampling. The agent's prior belief over the hidden state is $p(s=1)=\\pi$ and $p(s=0)=1-\\pi$.\n\nThe likelihood model of observations given the hidden state and action is parameterized by an action-dependent accuracy parameter $\\alpha_a \\in [0.5,1]$, with the following symmetry:\n- $p(o=1 \\mid s=1,a)=\\alpha_a$, $p(o=0 \\mid s=1,a)=1-\\alpha_a$,\n- $p(o=0 \\mid s=0,a)=\\alpha_a$, $p(o=1 \\mid s=0,a)=1-\\alpha_a$.\n\nThe task is to demonstrate how Active Inference resolves ambiguous sensory input via action that disambiguates states, by computing the expected reduction in posterior entropy after taking an action. Use the following fundamental bases:\n- Bayes' rule: $p(s \\mid o,a) = \\dfrac{p(o \\mid s,a)\\,p(s)}{p(o \\mid a)}$, with $p(o \\mid a)=\\sum_{s} p(o \\mid s,a)\\,p(s)$.\n- Shannon entropy (in nats): for a binary distribution $p(s)$, $H[p(s)] = -\\sum_{s \\in \\{0,1\\}} p(s)\\,\\ln p(s)$.\n\nDefine the expected reduction in posterior entropy for action $a$ as the difference between the prior entropy and the expected posterior entropy after observing $o$ generated under action $a$, namely:\n- prior entropy: $H[p(s)]$,\n- posterior entropy given an observation: $H[p(s \\mid o,a)]$,\n- expected posterior entropy: $\\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$,\n- expected reduction: $H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$.\n\nYou must:\n1. Derive from first principles the computation of the expected reduction in posterior entropy for any given $(\\pi,\\alpha_0,\\alpha_1)$ using only Bayes' rule and Shannon entropy.\n2. Implement a program that, for each test case, computes the expected reduction in posterior entropy for $a=0$ and $a=1$, selects the action that maximizes the expected reduction (tie-breaking in favor of the smaller index), and outputs the result for each test case as a list $[R_0,R_1,A^\\ast]$ where $R_0$ and $R_1$ are the expected reductions (in nats) for $a=0$ and $a=1$ respectively (rounded to six decimal places), and $A^\\ast \\in \\{0,1\\}$ is the selected action index.\n\nUse the following test suite:\n- Case $1$: $\\pi=0.5$, $\\alpha_0=0.6$, $\\alpha_1=0.9$.\n- Case $2$: $\\pi=0.99$, $\\alpha_0=0.8$, $\\alpha_1=0.95$.\n- Case $3$: $\\pi=0.5$, $\\alpha_0=0.5$, $\\alpha_1=0.5$.\n- Case $4$: $\\pi=0.7$, $\\alpha_0=0.55$, $\\alpha_1=0.75$.\n\nAll entropy quantities must be expressed in nats and rounded to six decimal places. Your program should produce a single line of output containing the results for the above cases as a comma-separated list enclosed in square brackets, where each element is itself a list in the form $[R_0,R_1,A^\\ast]$. For example: $[[0.123456,0.234567,1],[\\dots]]$.\n\nNo external input is allowed; hard-code the test suite in the program. The solution must be applicable in purely mathematical terms and solvable using any modern programming language. Ensure numerical stability and scientific realism by not introducing invalid probabilities (e.g., probabilities outside $[0,1]$).",
            "solution": "The user-provided problem is first validated against the established criteria.\n\n### Step 1: Extract Givens\n- **Hidden state:** $s \\in \\{0,1\\}$\n- **Observation:** $o \\in \\{0,1\\}$\n- **Action:** $a \\in \\{0,1\\}$\n- **Prior belief:** $p(s=1) = \\pi$, $p(s=0) = 1-\\pi$.\n- **Likelihood model:** This is a symmetric binary channel where the probability of a correct observation is $\\alpha_a \\in [0.5, 1]$, dependent on action $a$.\n  - $p(o=1 \\mid s=1,a) = \\alpha_a$\n  - $p(o=0 \\mid s=1,a) = 1-\\alpha_a$\n  - $p(o=0 \\mid s=0,a) = \\alpha_a$\n  - $p(o=1 \\mid s=0,a) = 1-\\alpha_a$\n- **Fundamental principles:**\n  - Bayes' rule: $p(s \\mid o,a) = \\frac{p(o \\mid s,a)\\,p(s)}{p(o \\mid a)}$, where $p(o \\mid a) = \\sum_{s} p(o \\mid s,a)\\,p(s)$.\n  - Shannon entropy (in nats): $H[p(x)] = -\\sum_i p(x_i)\\,\\ln p(x_i)$.\n- **Objective function:** The expected reduction in posterior entropy for action $a$, denoted $R_a$, is defined as:\n  $R_a = H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$, where:\n  - $H[p(s)]$ is the entropy of the prior distribution over states.\n  - $H[p(s \\mid o,a)]$ is the entropy of the posterior distribution over states after taking action $a$ and observing $o$.\n  - $\\mathbb{E}_{o \\sim p(o \\mid a)}[\\cdot]$ denotes the expectation over observations $o$ drawn from the distribution $p(o \\mid a)$.\n- **Action selection:** The optimal action $A^\\ast$ is the one that maximizes the expected reduction in entropy, with ties broken in favor of the smaller action index ($a=0$). $A^\\ast = \\arg\\max_a R_a$.\n- **Test cases:**\n    1. $\\pi=0.5$, $\\alpha_0=0.6$, $\\alpha_1=0.9$\n    2. $\\pi=0.99$, $\\alpha_0=0.8$, $\\alpha_1=0.95$\n    3. $\\pi=0.5$, $\\alpha_0=0.5$, $\\alpha_1=0.5$\n    4. $\\pi=0.7$, $\\alpha_0=0.55$, $\\alpha_1=0.75$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in Bayesian probability and information theory, which are foundational to computational neuroscience and machine learning. The model, though simple, is a standard formulation used to illustrate the principles of Active Inference and predictive coding, where agents act to reduce their uncertainty about the world. It is scientifically sound.\n- **Well-Posed:** All necessary parameters ($\\pi, \\alpha_0, \\alpha_1$), definitions (entropy, Bayes' rule), and objectives are explicitly provided. The problem is self-contained and has a unique, computable solution for each test case.\n- **Objective:** The problem is stated using precise mathematical language, free from any subjective or ambiguous terminology.\n- **Other criteria:** The problem is not contradictory, incomplete, or based on unrealistic physics. It is a formalizable and non-trivial computational task.\n\n### Step 3: Verdict and Action\nThe problem is valid. A step-by-step derivation of the solution is provided below.\n\n### Derivation of the Solution\n\nThe solution requires computing the expected reduction in posterior entropy, $R_a$, for each action $a \\in \\{0,1\\}$. This quantity is also known as the expected information gain or the mutual information $I(S;O|a)$.\n\n**1. Prior Entropy**\nThe prior distribution over the binary state $s$ is a Bernoulli distribution with parameter $\\pi$. Its Shannon entropy, $H[p(s)]$, is:\n$$ H[p(s)] = -\\left[ p(s=0)\\ln(p(s=0)) + p(s=1)\\ln(p(s=1)) \\right] = -\\left[ (1-\\pi)\\ln(1-\\pi) + \\pi\\ln(\\pi) \\right] $$\nFor brevity, we can use the binary entropy function $h(x) = -x\\ln(x) - (1-x)\\ln(1-x)$. Thus, $H[p(s)] = h(\\pi)$. By convention, $0\\ln(0)=0$, so if $\\pi=0$ or $\\pi=1$, the entropy is $0$.\n\n**2. Marginal Probability of Observations**\nFor a given action $a$ with accuracy parameter $\\alpha_a$, we compute the probability of each observation $o$ by marginalizing over the hidden states $s$:\n$$ p(o \\mid a) = \\sum_{s \\in \\{0,1\\}} p(o \\mid s, a) p(s) $$\nFor $o=1$:\n$$ p(o=1 \\mid a) = p(o=1 \\mid s=1, a)p(s=1) + p(o=1 \\mid s=0, a)p(s=0) = \\alpha_a \\pi + (1-\\alpha_a)(1-\\pi) $$\nFor $o=0$:\n$$ p(o=0 \\mid a) = p(o=0 \\mid s=1, a)p(s=1) + p(o=0 \\mid s=0, a)p(s=0) = (1-\\alpha_a)\\pi + \\alpha_a(1-\\pi) $$\nIt can be verified that $p(o=1 \\mid a) + p(o=0 \\mid a) = 1$.\n\n**3. Posterior Probability of Hidden States**\nUsing Bayes' rule, we find the posterior probability of the state $s=1$ given an observation $o$ and action $a$, which we denote $\\pi'_{o,a}$:\n$$ \\pi'_{o,a} = p(s=1 \\mid o, a) = \\frac{p(o \\mid s=1, a)p(s=1)}{p(o \\mid a)} $$\nIf $o=1$:\n$$ \\pi'_{1,a} = \\frac{\\alpha_a \\pi}{p(o=1 \\mid a)} = \\frac{\\alpha_a \\pi}{\\alpha_a \\pi + (1-\\alpha_a)(1-\\pi)} $$\nIf $o=0$:\n$$ \\pi'_{0,a} = \\frac{(1-\\alpha_a) \\pi}{p(o=0 \\mid a)} = \\frac{(1-\\alpha_a) \\pi}{(1-\\alpha_a)\\pi + \\alpha_a(1-\\pi)} $$\nThe posterior probability of $s=0$ is then $1 - \\pi'_{o,a}$.\n\n**4. Expected Posterior Entropy**\nThe entropy of the posterior distribution $p(s \\mid o, a)$ is $H[p(s \\mid o, a)] = h(\\pi'_{o,a})$. The expected posterior entropy is the average of these posterior entropies, weighted by the probability of each observation:\n$$ \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] = p(o=0 \\mid a)H[p(s \\mid o=0, a)] + p(o=1 \\mid a)H[p(s \\mid o=1, a)] $$\n$$ \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] = p(o=0 \\mid a) \\cdot h(\\pi'_{0,a}) + p(o=1 \\mid a) \\cdot h(\\pi'_{1,a}) $$\n\n**5. Expected Reduction in Entropy**\nThe final value, $R_a$, is the difference between the prior entropy and the expected posterior entropy:\n$$ R_a = H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] $$\nThis calculation is performed for both $a=0$ (with $\\alpha_0$) and $a=1$ (with $\\alpha_1$) to obtain $R_0$ and $R_1$.\n\n**6. Action Selection**\nThe agent chooses the action that maximizes the expected information gain.\n$$ A^\\ast = \\begin{cases} 1 & \\text{if } R_1 > R_0 \\\\ 0 & \\text{if } R_1 \\le R_0 \\end{cases} $$\nThis adheres to the specified tie-breaking rule. The result for each test case is the triplet $[R_0, R_1, A^\\ast]$. An implementation of this procedure will be provided in the following section.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Active Inference problem by calculating expected entropy reduction for given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (pi, alpha_0, alpha_1)\n        (0.5, 0.6, 0.9),\n        (0.99, 0.8, 0.95),\n        (0.5, 0.5, 0.5),\n        (0.7, 0.55, 0.75),\n    ]\n\n    def binary_entropy(p):\n        \"\"\"\n        Calculates the Shannon entropy for a binary probability distribution p(x=1)=p.\n        H(p) = -p*ln(p) - (1-p)*ln(1-p) in nats.\n        Handles edge cases where p=0 or p=1.\n        \"\"\"\n        if p == 0 or p == 1:\n            return 0.0\n        return -p * np.log(p) - (1 - p) * np.log(1 - p)\n\n    def calculate_expected_reduction(pi, alpha_a):\n        \"\"\"\n        Computes the expected reduction in posterior entropy for a given prior pi and action accuracy alpha_a.\n        \"\"\"\n        # 1. Prior Entropy\n        prior_entropy = binary_entropy(pi)\n        \n        # If prior is certain, no uncertainty to reduce.\n        if prior_entropy == 0.0:\n            return 0.0\n\n        # Special case: if action is uninformative, posterior equals prior, so no reduction.\n        if alpha_a == 0.5:\n            return 0.0\n\n        # 2. Marginal Probability of Observations\n        # p(o=1 | a)\n        p_o1_a = alpha_a * pi + (1 - alpha_a) * (1 - pi)\n        # p(o=0 | a)\n        p_o0_a = 1.0 - p_o1_a\n\n        # 3. Posterior Probabilities of Hidden States\n        # p(s=1 | o=1, a)\n        # Handle cases where p_o1_a might be zero if pi=0 or pi=1.\n        if p_o1_a > 0:\n            post_s1_o1 = (alpha_a * pi) / p_o1_a\n        else: # This happens if pi=0 and alpha_a=1, or pi=1 and alpha_a=0 (not in domain)\n            post_s1_o1 = 0\n            \n        # p(s=1 | o=0, a)\n        if p_o0_a > 0:\n            post_s1_o0 = ((1 - alpha_a) * pi) / p_o0_a\n        else: # This happens if pi=1 and alpha_a=1, or pi=0 and alpha_a=0 (not in domain)\n            post_s1_o0 = 0\n\n        # 4. Expected Posterior Entropy\n        # H[p(s|o=1,a)]\n        post_entropy_o1 = binary_entropy(post_s1_o1)\n        # H[p(s|o=0,a)]\n        post_entropy_o0 = binary_entropy(post_s1_o0)\n        \n        expected_posterior_entropy = p_o1_a * post_entropy_o1 + p_o0_a * post_entropy_o0\n\n        # 5. Expected Reduction in Entropy\n        reduction = prior_entropy - expected_posterior_entropy\n        \n        return reduction\n\n    \n    results_str_list = []\n    for case in test_cases:\n        pi, alpha_0, alpha_1 = case\n        \n        # Compute reduction for action a=0\n        R0 = calculate_expected_reduction(pi, alpha_0)\n        \n        # Compute reduction for action a=1\n        R1 = calculate_expected_reduction(pi, alpha_1)\n        \n        # Select action that maximizes reduction, tie-break to a=0\n        A_star = 1 if R1 > R0 else 0\n        \n        # Format results as specified\n        formatted_r0 = f\"{round(R0, 6):.6f}\"\n        formatted_r1 = f\"{round(R1, 6):.6f}\"\n        result_str = f\"[{formatted_r0},{formatted_r1},{A_star}]\"\n        results_str_list.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        }
    ]
}