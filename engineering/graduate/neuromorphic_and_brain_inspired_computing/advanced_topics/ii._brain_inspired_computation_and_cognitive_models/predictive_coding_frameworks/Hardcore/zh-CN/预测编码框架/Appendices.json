{
    "hands_on_practices": [
        {
            "introduction": "预测编码的核心在于大脑如何将先验信念与感官证据相结合，以形成对外部世界的最佳估计。这个练习将带你推导贝叶斯推断中的一个基本结果，展示后验信念如何成为先验信念和感官输入的精确度加权平均。通过这个推导，你将掌握预测编码框架的数学基石，理解大脑在不确定性下整合信息的基本原理。",
            "id": "5052192",
            "problem": "考虑一个在神经生物学中常用于形式化预测编码和贝叶斯大脑框架的单潜变量生成模型。潜在原因 $x$ 代表大脑对感觉输入的预测，测量值 $y$ 是观测到的感觉信号。假设潜在原因服从高斯先验分布，$x \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$，给定原因的观测值服从高斯似然，$y \\mid x \\sim \\mathcal{N}(x, \\sigma_{y}^{2})$，其中 $\\sigma_{0}^{2} > 0$ 和 $\\sigma_{y}^{2} > 0$ 是已知常数。使用贝叶斯法则以及“高斯密度函数的乘积会产生关于目标变量的另一个高斯密度函数”这一经过充分检验的事实，以闭合形式推导后验密度 $p(x \\mid y)$，并证明其为高斯分布。然后，使用先验均值和观测值的精度加权组合来表示后验均值和后验方差。定义精度为方差的倒数，即 $\\tau_{0} = 1 / \\sigma_{0}^{2}$ 和 $\\tau_{y} = 1 / \\sigma_{y}^{2}$。提供以后验均值和后验方差的最终表达式，用 $\\mu_{0}$、$y$、$\\tau_{0}$ 和 $\\tau_{y}$ 表示。无需数值近似；请给出精确的符号表达式。最终答案必须是一个计算结果，并应表示为包含后验均值和后验方差的单行矩阵。",
            "solution": "问题陈述经评估有效。这是一个贝叶斯统计中的适定问题，基于已建立的数学原理，并直接适用于指定的预测编码神经生物学框架。所有必要信息均已提供，且术语无歧义。\n\n任务是推导给定观测值 $y$ 的情况下潜在原因 $x$ 的后验概率密度函数 (PDF) $p(x \\mid y)$。我们已知 $x$ 的先验分布和给定 $x$ 的 $y$ 的似然函数。\n\n先验分布是一个高斯分布：\n$$p(x) = \\mathcal{N}(x \\mid \\mu_{0}, \\sigma_{0}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\n似然函数也是一个高斯分布：\n$$p(y \\mid x) = \\mathcal{N}(y \\mid x, \\sigma_{y}^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y}^{2}}} \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_{y}^{2}}\\right)$$\n根据贝叶斯法则，后验分布正比于似然函数与先验分布的乘积：\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\n代入给定的分布，我们得到：\n$$p(x \\mid y) \\propto \\exp\\left(-\\frac{(y - x)^2}{2\\sigma_{y}^{2}}\\right) \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}}\\right)$$\n我们可以合并指数函数的指数部分：\n$$p(x \\mid y) \\propto \\exp\\left( -\\frac{(y - x)^2}{2\\sigma_{y}^{2}} - \\frac{(x - \\mu_{0})^2}{2\\sigma_{0}^{2}} \\right)$$\n为了找到后验分布的形式，我们分析指数部分，重点关注依赖于 $x$ 的项。设 $E(x)$ 为指数函数的指数部分：\n$$E(x) = -\\frac{1}{2} \\left( \\frac{(y - x)^2}{\\sigma_{y}^{2}} + \\frac{(x - \\mu_{0})^2}{\\sigma_{0}^{2}} \\right)$$\n展开平方项：\n$$E(x) = -\\frac{1}{2} \\left( \\frac{y^2 - 2yx + x^2}{\\sigma_{y}^{2}} + \\frac{x^2 - 2x\\mu_{0} + \\mu_{0}^2}{\\sigma_{0}^{2}} \\right)$$\n我们现在按 $x$ 的幂次对各项进行分组：\n$$E(x) = -\\frac{1}{2} \\left[ x^2 \\left(\\frac{1}{\\sigma_{0}^{2}} + \\frac{1}{\\sigma_{y}^{2}}\\right) - 2x \\left(\\frac{\\mu_{0}}{\\sigma_{0}^{2}} + \\frac{y}{\\sigma_{y}^{2}}\\right) + \\left(\\frac{\\mu_{0}^2}{\\sigma_{0}^{2}} + \\frac{y^2}{\\sigma_{y}^{2}}\\right) \\right]$$\n问题将精度定义为方差的倒数：$\\tau_{0} = 1/\\sigma_{0}^{2}$ 和 $\\tau_{y} = 1/\\sigma_{y}^{2}$。将它们代入表达式中：\n$$E(x) = -\\frac{1}{2} \\left[ x^2 (\\tau_{0} + \\tau_{y}) - 2x (\\tau_{0}\\mu_{0} + \\tau_{y}y) + (\\tau_{0}\\mu_{0}^2 + \\tau_{y}y^2) \\right]$$\n这个表达式是关于 $x$ 的一个二次函数。这意味着后验分布 $p(x \\mid y)$ 是一个高斯分布，正如问题中所述。对于一个均值为 $\\mu_{\\text{post}}$、方差为 $\\sigma_{\\text{post}}^2$ 的变量 $x$，其通用高斯 PDF 的指数部分形式如下：\n$$-\\frac{(x - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2} = -\\frac{1}{2\\sigma_{\\text{post}}^2} (x^2 - 2x\\mu_{\\text{post}} + \\mu_{\\text{post}}^2) = -\\frac{1}{2} (\\tau_{\\text{post}}x^2 - 2\\tau_{\\text{post}}\\mu_{\\text{post}}x + \\text{const})$$\n其中 $\\tau_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ 是后验精度。\n\n通过比较我们推导出的指数 $E(x)$ 中 $x$ 的各次幂的系数与一般形式，我们可以确定后验分布的参数。\n比较 $x^2$ 项的系数：\n$$\\tau_{\\text{post}} = \\tau_{0} + \\tau_{y}$$\n这表明后验精度是先验精度和似然精度之和。后验方差 $\\sigma_{\\text{post}}^2$ 是后验精度的倒数：\n$$\\sigma_{\\text{post}}^2 = \\frac{1}{\\tau_{\\text{post}}} = \\frac{1}{\\tau_{0} + \\tau_{y}}$$\n现在，比较 $x$ 项的系数：\n$$2\\tau_{\\text{post}}\\mu_{\\text{post}} = 2(\\tau_{0}\\mu_{0} + \\tau_{y}y)$$\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{\\text{post}}}$$\n代入 $\\tau_{\\text{post}}$ 的表达式：\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{0} + \\tau_{y}}$$\n后验均值是先验均值 $\\mu_{0}$ 和观测数据 $y$ 的精度加权平均值。$E(x)$ 中不依赖于 $x$ 的项被吸收到后验高斯 PDF 的归一化常数中。\n\n因此，后验分布 $p(x \\mid y)$ 是一个高斯分布 $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$，其均值和方差如上推导。\n\n后验均值为：\n$$\\mu_{\\text{post}} = \\frac{\\tau_{0}\\mu_{0} + \\tau_{y}y}{\\tau_{0} + \\tau_{y}}$$\n后验方差为：\n$$\\sigma_{\\text{post}}^2 = \\frac{1}{\\tau_{0} + \\tau_{y}}$$\n这些就是所要求的最终符号表达式。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\tau_{0} \\mu_{0} + \\tau_{y} y}{\\tau_{0} + \\tau_{y}}  \\frac{1}{\\tau_{0} + \\tau_{y}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "现实世界的生成过程很少是线性的，因此预测编码模型必须能够处理非线性关系。这个练习将线性高斯模型推广到非线性情况，并引入通过梯度下降最小化变分自由能（$VFE$）的推断过程。你将亲手计算一个具体的更新步骤，这有助于直观地理解预测编码作为一种过程理论，其中神经活动被视为通过迭代更新来不断减少预测误差的过程。",
            "id": "4011082",
            "problem": "考虑一个用于大脑建模和计算神经科学中预测编码框架的单层非线性生成模型。观测值 $y$ 是通过一个可微非线性观测函数 $g(x)$ 从潜在状态 $x$ 生成的，并带有加性高斯噪声。假设观测函数是 logistic sigmoid 函数 $g(x) = \\frac{1}{1 + \\exp(-x)}$。观测模型为 $y \\mid x \\sim \\mathcal{N}(g(x), \\Pi_{s}^{-1})$，其中 $\\Pi_{s}  0$ 是感官精度（方差的倒数）。潜在状态具有高斯先验 $x \\sim \\mathcal{N}(\\mu_{0}, R^{-1})$，其中 $R  0$ 是先验精度。\n\n在预测编码（PC）中，使用变分自由能（VFE）最小化原理，可以通过对对数联合密度 $\\ln p(y, x)$ 进行梯度上升来更新对 $x$ 的估计，步长为一个小的正数 $\\gamma$，即 $x \\leftarrow x + \\gamma \\nabla_{x} \\ln p(y, x)$。\n\n从高斯似然和先验的第一性原理出发，推导该模型的梯度 $\\nabla_{x} \\ln p(y, x)$，用 $y$、$x$、$g(x)$、$g'(x)$、$\\Pi_{s}$ 和 $R$ 表示。然后，对于具体值 $x = 0$、$y = 0.8$、$\\mu_{0} = 0.1$、$R = 5$、$\\Pi_{s} = 20$ 和 $\\gamma = 0.05$，显式计算单步更新后的状态 $x_{\\text{new}}$。你必须计算并使用给定 $x$ 处的精确导数 $g'(x)$。将最终答案表示为单个实数。不需要四舍五入。",
            "solution": "该问题表述清晰，具有科学依据，并为获得唯一解提供了所有必要信息。因此，我们可以进行推导和计算。\n\n问题的核心是计算对数联合概率密度 $\\nabla_{x} \\ln p(y, x)$ 的梯度，然后用它来对潜在状态估计值 $x$ 执行单步更新。根据概率链式法则，联合概率 $p(y, x)$ 由似然 $p(y|x)$ 和先验 $p(x)$ 的乘积给出：\n$$p(y, x) = p(y | x) p(x)$$\n对该表达式取自然对数，我们得到：\n$$\\ln p(y, x) = \\ln p(y | x) + \\ln p(x)$$\n我们现在将定义对数似然和对数先验的显式形式。\n\n首先，我们考虑似然 $p(y|x)$。问题陈述，观测值 $y$ 来自一个均值为 $g(x)$、精度为 $\\Pi_{s}$（对应方差为 $\\Pi_{s}^{-1}$）的正态分布。其概率密度函数（PDF）为：\n$$p(y | x) = \\mathcal{N}(y; g(x), \\Pi_{s}^{-1}) = \\frac{1}{\\sqrt{2\\pi (\\Pi_{s}^{-1})}} \\exp\\left(-\\frac{(y - g(x))^2}{2 (\\Pi_{s}^{-1})}\\right)$$\n简化表达式：\n$$p(y | x) = \\sqrt{\\frac{\\Pi_{s}}{2\\pi}} \\exp\\left(-\\frac{\\Pi_{s}}{2} (y - g(x))^2\\right)$$\n因此，对数似然为：\n$$\\ln p(y | x) = \\ln\\left(\\sqrt{\\frac{\\Pi_{s}}{2\\pi}}\\right) - \\frac{\\Pi_{s}}{2} (y - g(x))^2$$\n第一项是关于 $x$ 的常数。\n\n其次，我们考虑先验 $p(x)$。问题陈述，潜在状态 $x$ 来自一个均值为 $\\mu_{0}$、精度为 $R$（方差为 $R^{-1}$）的正态分布。其概率密度函数为：\n$$p(x) = \\mathcal{N}(x; \\mu_{0}, R^{-1}) = \\frac{1}{\\sqrt{2\\pi (R^{-1})}} \\exp\\left(-\\frac{(x - \\mu_{0})^2}{2 (R^{-1})}\\right)$$\n简化表达式：\n$$p(x) = \\sqrt{\\frac{R}{2\\pi}} \\exp\\left(-\\frac{R}{2} (x - \\mu_{0})^2\\right)$$\n对数先验为：\n$$\\ln p(x) = \\ln\\left(\\sqrt{\\frac{R}{2\\pi}}\\right) - \\frac{R}{2} (x - \\mu_{0})^2$$\n同样，第一项是关于 $x$ 的常数。\n\n结合对数似然和对数先验，对数联合概率密度为：\n$$\\ln p(y, x) = -\\frac{\\Pi_{s}}{2} (y - g(x))^2 - \\frac{R}{2} (x - \\mu_{0})^2 + C$$\n其中 $C = \\ln\\left(\\sqrt{\\frac{\\Pi_{s}}{2\\pi}}\\right) + \\ln\\left(\\sqrt{\\frac{R}{2\\pi}}\\right)$ 是一个不依赖于 $x$ 的常数。\n\n为了求梯度 $\\nabla_{x} \\ln p(y, x)$，我们将此表达式对 $x$ 求导。\n$$\\nabla_{x} \\ln p(y, x) = \\frac{d}{dx} \\left( -\\frac{\\Pi_{s}}{2} (y - g(x))^2 - \\frac{R}{2} (x - \\mu_{0})^2 \\right)$$\n对每一项使用链式法则：\n\\begin{align*} \\nabla_{x} \\ln p(y, x) = -\\frac{\\Pi_{s}}{2} \\cdot 2(y - g(x)) \\cdot \\frac{d}{dx}(-g(x)) - \\frac{R}{2} \\cdot 2(x - \\mu_{0}) \\cdot \\frac{d}{dx}(x) \\\\ = -\\Pi_{s}(y - g(x))(-g'(x)) - R(x - \\mu_{0}) \\\\ = \\Pi_{s}(y - g(x))g'(x) - R(x - \\mu_{0}) \\end{align*}\n这是梯度的一般表达式。在预测编码的背景下，项 $\\Pi_{s}(y - g(x))$ 代表精度加权的感觉预测误差，而项 $-R(x - \\mu_{0})$ 代表精度加权的先验预测误差。\n\n现在，我们必须计算更新后状态 $x_{\\text{new}}$ 的数值。更新规则由下式给出：\n$$x_{\\text{new}} = x + \\gamma \\nabla_{x} \\ln p(y, x)$$\n我们给定的初始状态为 $x=0$，参数为 $y=0.8$、$\\mu_{0}=0.1$、$R=5$、$\\Pi_{s}=20$ 和 $\\gamma=0.05$。\n\n首先，我们在 $x=0$ 处计算 $g(x)$ 及其导数 $g'(x)$。\n观测函数是 logistic sigmoid：$g(x) = \\frac{1}{1 + \\exp(-x)}$。\n在 $x=0$ 处：\n$$g(0) = \\frac{1}{1 + \\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2} = 0.5$$\nsigmoid 函数的导数是 $g'(x) = g(x)(1-g(x))$。\n在 $x=0$ 处：\n$$g'(0) = g(0)(1-g(0)) = \\frac{1}{2}\\left(1-\\frac{1}{2}\\right) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4} = 0.25$$\n\n接下来，我们将这些值和给定的参数代入梯度表达式中：\n\\begin{align*} \\nabla_{x} \\ln p(y, x) \\Big|_{x=0} = \\Pi_{s}(y - g(0))g'(0) - R(0 - \\mu_{0}) \\\\ = 20(0.8 - 0.5)(0.25) - 5(0 - 0.1) \\\\ = 20(0.3)(0.25) - 5(-0.1) \\\\ = 6(0.25) + 0.5 \\\\ = 1.5 + 0.5 \\\\ = 2 \\end{align*}\n在 $x=0$ 处的梯度值为 $2$。\n\n最后，我们应用更新规则来找到 $x_{\\text{new}}$：\n$$x_{\\text{new}} = x + \\gamma \\cdot (\\text{gradient})$$\n$$x_{\\text{new}} = 0 + 0.05 \\cdot 2$$\n$$x_{\\text{new}} = 0.1$$\n单步更新后的状态是 $0.1$。",
            "answer": "$$\\boxed{0.1}$$"
        },
        {
            "introduction": "预测编码不仅能解释被动感知，还能通过主动推断（Active Inference）的理念，将感知与行动统一起来。这个练习探讨了一个核心思想：智能体如何通过选择行动来主动收集信息，以减少关于世界状态的不确定性。你将计算并比较不同行动带来的预期信息增益，从而理解最小化不确定性这一原则是如何驱动智能体与环境进行有目的的互动。",
            "id": "4011073",
            "problem": "考虑一个遵循预测编码（predictive coding）和主动推断（Active Inference, AI）传统的二元隐状态生成模型，其中智能体可以采取行动来影响后续感官数据的信息量。隐状态表示为 $s \\in \\{0,1\\}$，观测表示为 $o \\in \\{0,1\\}$，行动表示为 $a \\in \\{0,1\\}$，其中 $a=0$ 表示信息量较少（被动）的采样，$a=1$ 表示信息量较多（主动）的采样。智能体对隐状态的先验信念为 $p(s=1)=\\pi$ 和 $p(s=0)=1-\\pi$。\n\n在给定隐状态和行动的情况下，观测的似然模型由一个依赖于行动的准确度参数 $\\alpha_a \\in [0.5,1]$ 参数化，并具有以下对称性：\n- $p(o=1 \\mid s=1,a)=\\alpha_a$, $p(o=0 \\mid s=1,a)=1-\\alpha_a$,\n- $p(o=0 \\mid s=0,a)=\\alpha_a$, $p(o=1 \\mid s=0,a)=1-\\alpha_a$.\n\n任务是通过计算采取行动后后验熵的期望减少量，来展示主动推断如何通过消除状态歧义的行动来解决模糊的感官输入。使用以下基本原理：\n- 贝叶斯法则：$p(s \\mid o,a) = \\dfrac{p(o \\mid s,a)\\,p(s)}{p(o \\mid a)}$，其中 $p(o \\mid a)=\\sum_{s} p(o \\mid s,a)\\,p(s)$。\n- 香农熵（以奈特为单位）：对于二元分布 $p(s)$，$H[p(s)] = -\\sum_{s \\in \\{0,1\\}} p(s)\\,\\ln p(s)$。\n\n将行动 $a$ 的后验熵期望减少量定义为先验熵与在行动 $a$ 下生成观测 $o$ 后的期望后验熵之差，即：\n- 先验熵：$H[p(s)]$，\n- 给定观测的后验熵：$H[p(s \\mid o,a)]$，\n- 期望后验熵：$\\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$，\n- 期望减少量：$H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$。\n\n您必须：\n1. 仅使用贝叶斯法则和香农熵，从第一性原理推导对于任何给定的 $(\\pi,\\alpha_0,\\alpha_1)$ 计算后验熵期望减少量的方法。\n2. 实现一个程序，为每个测试用例计算 $a=0$ 和 $a=1$ 的后验熵期望减少量，选择使期望减少量最大化的行动（平局时选择较小的索引），并为每个测试用例输出一个列表形式的结果 $[R_0,R_1,A^\\ast]$，其中 $R_0$ 和 $R_1$ 分别是 $a=0$ 和 $a=1$ 的期望减少量（以奈特为单位，四舍五入到六位小数），$A^\\ast \\in \\{0,1\\}$ 是所选行动的索引。\n\n使用以下测试套件：\n- 用例 1：$\\pi=0.5$, $\\alpha_0=0.6$, $\\alpha_1=0.9$。\n- 用例 2：$\\pi=0.99$, $\\alpha_0=0.8$, $\\alpha_1=0.95$。\n- 用例 3：$\\pi=0.5$, $\\alpha_0=0.5$, $\\alpha_1=0.5$。\n- 用例 4：$\\pi=0.7$, $\\alpha_0=0.55$, $\\alpha_1=0.75$。\n\n所有熵值必须以奈特表示，并四舍五入到六位小数。您的程序应生成单行输出，其中包含上述用例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个元素本身就是 $[R_0,R_1,A^\\ast]$ 形式的列表。例如：$[[0.123456,0.234567,1],[\\dots]]$。\n\n不允许外部输入；请将测试套件硬编码在程序中。解决方案必须在纯数学上适用，并能用任何现代编程语言解决。请确保数值稳定性与科学真实性，不要引入无效概率（例如，$[0,1]$ 范围之外的概率）。",
            "solution": "首先根据既定标准对用户提供的问题进行验证。\n\n### 步骤 1：提取已知条件\n- **隐状态：** $s \\in \\{0,1\\}$\n- **观测：** $o \\in \\{0,1\\}$\n- **行动：** $a \\in \\{0,1\\}$\n- **先验信念：** $p(s=1) = \\pi$, $p(s=0) = 1-\\pi$。\n- **似然模型：** 这是一个对称二元信道，其中正确观测的概率为 $\\alpha_a \\in [0.5, 1]$，依赖于行动 $a$。\n  - $p(o=1 \\mid s=1,a) = \\alpha_a$\n  - $p(o=0 \\mid s=1,a) = 1-\\alpha_a$\n  - $p(o=0 \\mid s=0,a) = \\alpha_a$\n  - $p(o=1 \\mid s=0,a) = 1-\\alpha_a$\n- **基本原理：**\n  - 贝叶斯法则：$p(s \\mid o,a) = \\frac{p(o \\mid s,a)\\,p(s)}{p(o \\mid a)}$，其中 $p(o \\mid a) = \\sum_{s} p(o \\mid s,a)\\,p(s)$。\n  - 香农熵（以奈特为单位）：$H[p(x)] = -\\sum_i p(x_i)\\,\\ln p(x_i)$。\n- **目标函数：** 行动 $a$ 的后验熵期望减少量，记为 $R_a$，定义为：\n  $R_a = H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]]$，其中：\n  - $H[p(s)]$ 是状态先验分布的熵。\n  - $H[p(s \\mid o,a)]$ 是在采取行动 $a$ 并观测到 $o$ 之后状态后验分布的熵。\n  - $\\mathbb{E}_{o \\sim p(o \\mid a)}[\\cdot]$ 表示对从分布 $p(o \\mid a)$ 中抽取的观测 $o$ 的期望。\n- **行动选择：** 最优行动 $A^\\ast$ 是使熵的期望减少量最大化的行动，平局时选择较小的行动索引（$a=0$）。$A^\\ast = \\arg\\max_a R_a$。\n- **测试用例：**\n    1. $\\pi=0.5$, $\\alpha_0=0.6$, $\\alpha_1=0.9$\n    2. $\\pi=0.99$, $\\alpha_0=0.8$, $\\alpha_1=0.95$\n    3. $\\pi=0.5$, $\\alpha_0=0.5$, $\\alpha_1=0.5$\n    4. $\\pi=0.7$, $\\alpha_0=0.55$, $\\alpha_1=0.75$\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学基础：** 该问题牢固地植根于贝叶斯概率论和信息论，这两者是计算神经科学和机器学习的基础。该模型虽然简单，却是一个标准范式，用于阐释主动推断和预测编码的原理，在这些原理中，智能体通过行动来减少其对世界的不确定性。该问题在科学上是合理的。\n- **适定性：** 所有必要的参数（$\\pi, \\alpha_0, \\alpha_1$）、定义（熵、贝叶斯法则）和目标都已明确给出。问题是自包含的，并且对每个测试用例都有唯一的、可计算的解。\n- **客观性：** 问题使用精确的数学语言陈述，没有任何主观或模糊的术语。\n- **其他标准：** 该问题没有矛盾、不完整或基于不切实际的物理学。它是一个可形式化且不平凡的计算任务。\n\n### 步骤 3：结论与行动\n问题有效。下面提供了解决方案的逐步推导。\n\n### 解的推导\n\n该解法需要为每个行动 $a \\in \\{0,1\\}$ 计算后验熵的期望减少量 $R_a$。这个量也称为期望信息增益或互信息 $I(S;O|a)$。\n\n**1. 先验熵**\n关于二元状态 $s$ 的先验分布是参数为 $\\pi$ 的伯努利分布。其香农熵 $H[p(s)]$ 为：\n$$ H[p(s)] = -\\left[ p(s=0)\\ln(p(s=0)) + p(s=1)\\ln(p(s=1)) \\right] = -\\left[ (1-\\pi)\\ln(1-\\pi) + \\pi\\ln(\\pi) \\right] $$\n为简洁起见，我们可以使用二元熵函数 $h(x) = -x\\ln(x) - (1-x)\\ln(1-x)$。因此，$H[p(s)] = h(\\pi)$。按照惯例，$0\\ln(0)=0$，所以如果 $\\pi=0$ 或 $\\pi=1$，熵为 $0$。\n\n**2. 观测的边际概率**\n对于给定的行动 $a$ 和准确度参数 $\\alpha_a$，我们通过对隐状态 $s$ 进行边缘化来计算每个观测 $o$ 的概率：\n$$ p(o \\mid a) = \\sum_{s \\in \\{0,1\\}} p(o \\mid s, a) p(s) $$\n对于 $o=1$:\n$$ p(o=1 \\mid a) = p(o=1 \\mid s=1, a)p(s=1) + p(o=1 \\mid s=0, a)p(s=0) = \\alpha_a \\pi + (1-\\alpha_a)(1-\\pi) $$\n对于 $o=0$:\n$$ p(o=0 \\mid a) = p(o=0 \\mid s=1, a)p(s=1) + p(o=0 \\mid s=0, a)p(s=0) = (1-\\alpha_a)\\pi + \\alpha_a(1-\\pi) $$\n可以验证 $p(o=1 \\mid a) + p(o=0 \\mid a) = 1$。\n\n**3. 隐状态的后验概率**\n使用贝叶斯法则，我们求出在给定观测 $o$ 和行动 $a$ 的情况下状态 $s=1$ 的后验概率，我们将其记为 $\\pi'_{o,a}$：\n$$ \\pi'_{o,a} = p(s=1 \\mid o, a) = \\frac{p(o \\mid s=1, a)p(s=1)}{p(o \\mid a)} $$\n如果 $o=1$:\n$$ \\pi'_{1,a} = \\frac{\\alpha_a \\pi}{p(o=1 \\mid a)} = \\frac{\\alpha_a \\pi}{\\alpha_a \\pi + (1-\\alpha_a)(1-\\pi)} $$\n如果 $o=0$:\n$$ \\pi'_{0,a} = \\frac{(1-\\alpha_a) \\pi}{p(o=0 \\mid a)} = \\frac{(1-\\alpha_a) \\pi}{(1-\\alpha_a)\\pi + \\alpha_a(1-\\pi)} $$\n那么 $s=0$ 的后验概率就是 $1 - \\pi'_{o,a}$。\n\n**4. 期望后验熵**\n后验分布 $p(s \\mid o, a)$ 的熵是 $H[p(s \\mid o, a)] = h(\\pi'_{o,a})$。期望后验熵是这些后验熵的加权平均值，权重为每个观测的概率：\n$$ \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] = p(o=0 \\mid a)H[p(s \\mid o=0, a)] + p(o=1 \\mid a)H[p(s \\mid o=1, a)] $$\n$$ \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] = p(o=0 \\mid a) \\cdot h(\\pi'_{0,a}) + p(o=1 \\mid a) \\cdot h(\\pi'_{1,a}) $$\n\n**5. 熵的期望减少量**\n最终值 $R_a$ 是先验熵与期望后验熵之差：\n$$ R_a = H[p(s)] - \\mathbb{E}_{o \\sim p(o \\mid a)}[H[p(s \\mid o,a)]] $$\n对 $a=0$（使用 $\\alpha_0$）和 $a=1$（使用 $\\alpha_1$）都执行此计算，以获得 $R_0$ 和 $R_1$。\n\n**6. 行动选择**\n智能体选择使期望信息增益最大化的行动。\n$$ A^\\ast = \\begin{cases} 1  \\text{if } R_1  R_0 \\\\ 0  \\text{if } R_1 \\le R_0 \\end{cases} $$\n这遵循了指定的平局打破规则。每个测试用例的结果是一个三元组 $[R_0, R_1, A^\\ast]$。此过程的实现将在下一节中提供。",
            "answer": "[[0.020136,0.368064,1],[0.009801,0.026938,1],[0.000000,0.000000,0],[0.002534,0.126466,1]]"
        }
    ]
}