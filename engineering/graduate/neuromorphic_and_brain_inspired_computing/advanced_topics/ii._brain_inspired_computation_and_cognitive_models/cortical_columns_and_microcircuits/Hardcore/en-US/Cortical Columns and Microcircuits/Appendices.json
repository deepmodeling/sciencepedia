{
    "hands_on_practices": [
        {
            "introduction": "Before we can understand the complex dynamics of a microcircuit, we must first master the behavior of its fundamental component: the single neuron. The Leaky Integrate-and-Fire (LIF) model provides a tractable yet insightful abstraction for how a neuron integrates synaptic inputs over time to produce an output spike. This first practice invites you to derive the input-current-to-firing-rate relationship—the $f$-$I$ curve—for an LIF neuron from first principles, a cornerstone calculation in computational neuroscience that builds intuition for how a neuron's biophysical properties shape its response function. ",
            "id": "4041431",
            "problem": "In a canonical cortical column microcircuit, the pooled synaptic drive to a representative excitatory neuron can be approximated as a constant current during sustained activity. Model a single layer $2/3$ pyramidal neuron as a Leaky Integrate-and-Fire (LIF) unit with membrane capacitance $C$, membrane time constant $\\tau_m$, resting potential $v_{rest}$, spike threshold $v_{th}$, and reset potential $v_{reset}$. The membrane potential $v(t)$ evolves according to the current-balance equation\n$$\nC \\frac{dv}{dt} = -\\frac{C}{\\tau_m}\\bigl(v - v_{rest}\\bigr) + I,\n$$\nwhere $I$ is a constant input current representing the net effect of many presynaptic neurons within the microcircuit. Upon reaching $v_{th}$, the neuron emits a spike, is instantly reset to $v_{reset}$, and then undergoes an absolute refractory period of duration $\\tau_{ref}$ during which it does not integrate input.\n\nUsing first-passage time reasoning in the deterministic limit (no noise), derive the steady-state firing rate $r(I)$, expressed in $\\mathrm{s}^{-1}$, as a closed-form analytic expression in terms of $I$, $C$, $\\tau_m$, $v_{rest}$, $v_{th}$, $v_{reset}$, and $\\tau_{ref}$. Assume the constant input is sufficiently strong to drive repetitive spiking, i.e., the effective steady-state potential under constant drive satisfies $v_{rest} + \\tau_m I / C > v_{th}$. Provide your final answer as a single explicit expression for $r(I)$ with no inequalities. Do not perform any numerical substitution or rounding; give the exact symbolic expression.",
            "solution": "The user wants to derive the steady-state firing rate $r(I)$ for a Leaky Integrate-and-Fire (LIF) neuron model subjected to a constant input current $I$. The problem is scientifically grounded, well-posed, and all necessary parameters are provided. I will proceed with the derivation.\n\nThe firing rate $r$ is the inverse of the inter-spike interval, $T_{ISI}$. The inter-spike interval is the sum of two periods: the absolute refractory period, $\\tau_{ref}$, during which the neuron is inactive, and the charging time, $T_{charge}$, which is the time it takes for the membrane potential $v(t)$ to rise from the reset potential $v_{reset}$ to the threshold potential $v_{th}$.\n$$\nr(I) = \\frac{1}{T_{ISI}} = \\frac{1}{T_{charge} + \\tau_{ref}}\n$$\nOur primary task is to find an expression for $T_{charge}$. This time is determined by the dynamics of the membrane potential, which are governed by the given ordinary differential equation (ODE):\n$$\nC \\frac{dv}{dt} = -\\frac{C}{\\tau_m}\\bigl(v - v_{rest}\\bigr) + I\n$$\nTo solve this equation, we can first rearrange it into a more standard linear first-order form. Dividing by the capacitance $C$, we get:\n$$\n\\frac{dv}{dt} = -\\frac{1}{\\tau_m}\\bigl(v - v_{rest}\\bigr) + \\frac{I}{C}\n$$\nRearranging the terms to isolate the derivative:\n$$\n\\frac{dv}{dt} = -\\frac{1}{\\tau_m} v + \\frac{v_{rest}}{\\tau_m} + \\frac{I}{C}\n$$\nMultiplying by $\\tau_m$ provides a cleaner structure:\n$$\n\\tau_m \\frac{dv}{dt} = -v + v_{rest} + \\frac{\\tau_m I}{C}\n$$\nWe can define an effective steady-state potential, $v_{\\infty}$, which is the potential the membrane would approach if there were no firing threshold:\n$$\nv_{\\infty} = v_{rest} + \\frac{\\tau_m I}{C}\n$$\nThe problem states that $v_{\\infty} > v_{th}$, which guarantees that the neuron will always reach the threshold and fire repetitively. Substituting $v_{\\infty}$ into the ODE yields:\n$$\n\\tau_m \\frac{dv}{dt} = -(v - v_{\\infty})\n$$\nThis is a separable differential equation. We can solve it to find the evolution of $v(t)$ during the charging period. We set the initial condition for this period at time $t=0$ to be the reset potential, $v(0) = v_{reset}$. We want to find the time $T_{charge}$ at which the potential reaches the threshold, $v(T_{charge}) = v_{th}$.\nSeparating variables gives:\n$$\n\\frac{dv}{v - v_{\\infty}} = -\\frac{dt}{\\tau_m}\n$$\nWe integrate this equation from the start of the charging period (time $t=0$, potential $v=v_{reset}$) to a general time $t$ where the potential is $v(t)$:\n$$\n\\int_{v_{reset}}^{v(t)} \\frac{1}{v' - v_{\\infty}} dv' = \\int_{0}^{t} -\\frac{1}{\\tau_m} dt'\n$$\nPerforming the integration gives:\n$$\n\\Bigl[ \\ln|v' - v_{\\infty}| \\Bigr]_{v_{reset}}^{v(t)} = -\\frac{t}{\\tau_m}\n$$\n$$\n\\ln|v(t) - v_{\\infty}| - \\ln|v_{reset} - v_{\\infty}| = -\\frac{t}{\\tau_m}\n$$\nGiven the condition $v_{\\infty} > v_{th}$ and the fact that for an LIF neuron $v_{th} > v_{reset}$, the potential $v(t)$ during the charging phase is always less than $v_{\\infty}$. Therefore, the terms inside the absolute value are always negative, so we can write $|x| = -x$.\n$$\n\\ln(v_{\\infty} - v(t)) - \\ln(v_{\\infty} - v_{reset}) = -\\frac{t}{\\tau_m}\n$$\nUsing the properties of logarithms:\n$$\n\\ln\\left(\\frac{v_{\\infty} - v(t)}{v_{\\infty} - v_{reset}}\\right) = -\\frac{t}{\\tau_m}\n$$\nNow, we solve for the specific time $t = T_{charge}$ when the potential reaches the threshold, $v(T_{charge}) = v_{th}$:\n$$\n\\ln\\left(\\frac{v_{\\infty} - v_{th}}{v_{\\infty} - v_{reset}}\\right) = -\\frac{T_{charge}}{\\tau_m}\n$$\nSolving for $T_{charge}$:\n$$\nT_{charge} = -\\tau_m \\ln\\left(\\frac{v_{\\infty} - v_{th}}{v_{\\infty} - v_{reset}}\\right)\n$$\nUsing the logarithmic identity $-\\ln(a/b) = \\ln(b/a)$, we can write the expression for $T_{charge}$ in a form where the argument of the logarithm is greater than $1$:\n$$\nT_{charge} = \\tau_m \\ln\\left(\\frac{v_{\\infty} - v_{reset}}{v_{\\infty} - v_{th}}\\right)\n$$\nThe condition $v_{\\infty} > v_{th} > v_{reset}$ ensures that the argument of the logarithm is a real number greater than $1$, so $T_{charge}$ is a positive real value, as expected.\n\nNext, we substitute the definition of $v_{\\infty}$ back into the expression for $T_{charge}$:\n$$\nT_{charge} = \\tau_m \\ln\\left(\\frac{(v_{rest} + \\frac{\\tau_m I}{C}) - v_{reset}}{(v_{rest} + \\frac{\\tau_m I}{C}) - v_{th}}\\right)\n$$\nNow we can express the total inter-spike interval $T_{ISI}$:\n$$\nT_{ISI} = \\tau_{ref} + T_{charge} = \\tau_{ref} + \\tau_m \\ln\\left(\\frac{v_{rest} - v_{reset} + \\frac{\\tau_m I}{C}}{v_{rest} - v_{th} + \\frac{\\tau_m I}{C}}\\right)\n$$\nFinally, the steady-state firing rate $r(I)$ is the reciprocal of $T_{ISI}$:\n$$\nr(I) = \\frac{1}{T_{ISI}} = \\frac{1}{\\tau_{ref} + \\tau_m \\ln\\left(\\frac{v_{rest} - v_{reset} + \\frac{\\tau_m I}{C}}{v_{rest} - v_{th} + \\frac{\\tau_m I}{C}}\\right)}\n$$\nThis is the closed-form analytic expression for the firing rate as a function of the input current $I$ and the neuron's parameters.",
            "answer": "$$\n\\boxed{\\frac{1}{\\tau_{ref} + \\tau_m \\ln\\left(\\frac{v_{rest} - v_{reset} + \\frac{\\tau_m I}{C}}{v_{rest} - v_{th} + \\frac{\\tau_m I}{C}}\\right)}}\n$$"
        },
        {
            "introduction": "Having characterized the neuron, we now turn to the connections between them. Synapses in the cortex are not static amplifiers but dynamic computational elements that exhibit short-term plasticity (STP), where synaptic strength changes over hundreds of milliseconds depending on recent presynaptic activity. This practice delves into the widely-used Tsodyks-Markram model to explore how STP shapes synaptic transmission, asking you to derive the average synaptic response under stochastic Poisson drive. This exercise is crucial for understanding how microcircuits can process information encoded in the *rate* and *timing* of incoming spikes. ",
            "id": "4041433",
            "problem": "Consider a synapse within a cortical microcircuit in a cortical column, modeled by the Tsodyks–Markram short-term plasticity (STP) model. Let the presynaptic spike train be a homogeneous Poisson process with rate $\\lambda$, so that inter-spike intervals $t$ are independent and exponentially distributed with probability density $p(t) = \\lambda \\exp(-\\lambda t)$. The synapse is characterized by a utilization variable $u(t)$ and an available resource fraction $x(t)$. Between spikes, the dynamics follow the standard Tsodyks–Markram laws: $\\mathrm{d}u/\\mathrm{d}t = -u/\\tau_{fac}$ and $\\mathrm{d}x/\\mathrm{d}t = (1 - x)/\\tau_{rec}$. At the moment of a presynaptic spike at time $t_i$, $u$ is incremented according to $u \\leftarrow u + U(1 - u)$, and a fraction $u$ of the available resources is released, so $x \\leftarrow x(1 - u)$ immediately after the spike. The amplitude of the postsynaptic response to a spike at $t_i$ is proportional to the released fraction, and we take the proportionality constant to be $1$ for simplicity, so the amplitude is $a_i = u(t_i^{+}) x(t_i^{-})$.\n\nAssume stationarity under Poisson drive and adopt the commonly used mean-field closure that replaces products of fluctuating quantities by products of their steady-state expectations. Starting from these model definitions and the properties of a Poisson process, derive a closed-form analytic expression for the steady-state expected amplitude per presynaptic spike, $\\mathcal{A}_{ss}(\\lambda)$, as a function of $U$, $\\tau_{rec}$, $\\tau_{fac}$, and $\\lambda$. Express the final answer as a single simplified analytic expression. No rounding is required, and no units should be included in the final expression.",
            "solution": "The user wants to solve the problem by following these steps:\n1.  **Problem Validation**: The problem statement is first validated for scientific soundness, consistency, and completeness.\n2.  **Solution Derivation**: If the problem is valid, a detailed derivation is provided.\n3.  **Final Answer Formulation**: The final answer is presented in the specified format.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens**\n-   **Model**: Tsodyks–Markram (TM) model of short-term synaptic plasticity.\n-   **Presynaptic input**: Homogeneous Poisson process with rate $\\lambda$. Inter-spike interval (ISI) distribution is $p(t) = \\lambda \\exp(-\\lambda t)$.\n-   **State variables**: utilization $u(t)$, available resources $x(t)$.\n-   **Parameters**: facilitation time constant $\\tau_{fac}$, recovery time constant $\\tau_{rec}$, baseline utilization $U$.\n-   **Dynamics between spikes**: $\\frac{\\mathrm{d}u}{\\mathrm{d}t} = -\\frac{u}{\\tau_{fac}}$ and $\\frac{\\mathrm{d}x}{\\mathrm{d}t} = \\frac{1 - x}{\\tau_{rec}}$.\n-   **Dynamics at a spike at time $t_i$**:\n    -   $u$ is updated: $u(t_i^+) = u(t_i^-) + U(1 - u(t_i^-))$.\n    -   $x$ is updated: $x(t_i^+) = x(t_i^-)(1 - u(t_i^+))$.\n-   **Postsynaptic amplitude**: $a_i = u(t_i^+) x(t_i^-)$ (proportionality constant is $1$).\n-   **Assumptions**: Stationarity and mean-field closure ($\\langle AB \\rangle \\approx \\langle A \\rangle \\langle B \\rangle$).\n-   **Objective**: Derive the steady-state expected amplitude per spike, $\\mathcal{A}_{ss}(\\lambda)$, as a function of $U$, $\\tau_{rec}$, $\\tau_{fac}$, and $\\lambda$.\n\n**1.2. Validation against Criteria**\n-   **Scientific Grounding**: The problem is based on the Tsodyks–Markram model, a standard and widely accepted model in computational neuroscience for describing short-term synaptic plasticity. The use of a Poisson process for spike trains is also a common and valid simplification. The problem is scientifically grounded.\n-   **Well-Posedness**: The problem is well-posed. It provides all dynamical equations, statistical properties of the input, and the necessary mean-field approximation to derive a unique analytical solution.\n-   **Objectivity**: The problem is stated in precise mathematical and scientific language, free from any subjective or ambiguous terms.\n-   **Consistency and Completeness**: The model equations and update rules are consistent with the standard formulation of the TM model. All necessary parameters and assumptions are provided.\n-   **Other Flaws**: The problem does not exhibit any other flaws such as being unrealistic, ill-posed, trivial, or unverifiable.\n\n**1.3. Verdict**\nThe problem is valid and can be solved as stated.\n\n### Step 2: Solution Derivation\n\nThe goal is to compute the steady-state expected amplitude per presynaptic spike, $\\mathcal{A}_{ss}(\\lambda)$. This quantity is defined as the expectation of the amplitude $a_i$ over the stationary distribution of states and spike timings.\n$$\n\\mathcal{A}_{ss}(\\lambda) = \\langle a_i \\rangle = \\langle u(t_i^+) x(t_i^-) \\rangle\n$$\nThe problem specifies the use of a mean-field closure, which allows us to approximate the expectation of a product as the product of expectations:\n$$\n\\mathcal{A}_{ss}(\\lambda) \\approx \\langle u(t_i^+) \\rangle \\langle x(t_i^-) \\rangle\n$$\nLet us denote the steady-state expectation of a variable $z$ just before a spike as $\\bar{z}^-$ and just after a spike as $\\bar{z}^+$. Our goal is to find $\\mathcal{A}_{ss}(\\lambda) = \\bar{u}^+ \\bar{x}^-$.\n\nLet's consider the evolution of the system from one spike to the next. Let the $k$-th spike occur at time $t_k$, and the next at $t_{k+1}$, with inter-spike interval $\\Delta t_k = t_{k+1} - t_k$.\nThe update rules at spike $k$ are:\n$u_k^+ = u_k^- + U(1 - u_k^-)$\n$x_k^+ = x_k^-(1 - u_k^+)$\nBetween spikes, the variables evolve according to:\n$u_{k+1}^- = u_k^+ \\exp(-\\Delta t_k / \\tau_{fac})$\n$x_{k+1}^- = 1 - (1 - x_k^+) \\exp(-\\Delta t_k / \\tau_{rec})$\n\nTo find the steady-state expectations, we average these equations over the distribution of ISIs, $\\Delta t_k$, which is $p(t) = \\lambda \\exp(-\\lambda t)$. In steady state, $\\langle z_{k+1} \\rangle = \\langle z_k \\rangle = \\bar{z}$.\n\n**1. Derivation of $\\bar{u}^+$**\nTaking the expectation of the update and evolution equations for $u$:\n$$\n\\bar{u}^+ = \\langle u^- + U(1 - u^-) \\rangle = \\bar{u}^- + U(1 - \\bar{u}^-) = (1 - U)\\bar{u}^- + U\n$$\n$$\n\\bar{u}^- = \\langle u^+ \\exp(-\\Delta t / \\tau_{fac}) \\rangle\n$$\nApplying the mean-field approximation, we separate the expectation over the state variables from the expectation over the ISI duration:\n$$\n\\bar{u}^- \\approx \\langle u^+ \\rangle \\langle \\exp(-\\Delta t / \\tau_{fac}) \\rangle = \\bar{u}^+ \\langle \\exp(-t / \\tau_{fac}) \\rangle\n$$\nThe expectation of the exponential term is calculated by integrating over the ISI probability distribution:\n$$\n\\langle \\exp(-t / \\tau_{fac}) \\rangle = \\int_0^\\infty \\exp(-t/\\tau_{fac}) p(t) dt = \\int_0^\\infty \\exp(-t/\\tau_{fac}) \\lambda \\exp(-\\lambda t) dt\n$$\n$$\n= \\lambda \\int_0^\\infty \\exp\\left(-t \\left(\\frac{1}{\\tau_{fac}} + \\lambda\\right)\\right) dt = \\frac{\\lambda}{\\frac{1}{\\tau_{fac}} + \\lambda} = \\frac{\\lambda \\tau_{fac}}{1 + \\lambda \\tau_{fac}}\n$$\nSo, we have a system of two linear equations for $\\bar{u}^+$ and $\\bar{u}^-$:\n$$\n\\begin{cases}\n\\bar{u}^+ = (1-U)\\bar{u}^- + U \\\\\n\\bar{u}^- = \\bar{u}^+ \\frac{\\lambda \\tau_{fac}}{1 + \\lambda \\tau_{fac}}\n\\end{cases}\n$$\nSubstituting the second equation into the first:\n$$\n\\bar{u}^+ = (1-U) \\left(\\bar{u}^+ \\frac{\\lambda \\tau_{fac}}{1 + \\lambda \\tau_{fac}}\\right) + U\n$$\n$$\n\\bar{u}^+ \\left(1 - (1-U)\\frac{\\lambda \\tau_{fac}}{1 + \\lambda \\tau_{fac}}\\right) = U\n$$\n$$\n\\bar{u}^+ \\left(\\frac{1 + \\lambda \\tau_{fac} - (1-U)\\lambda \\tau_{fac}}{1 + \\lambda \\tau_{fac}}\\right) = U\n$$\n$$\n\\bar{u}^+ \\left(\\frac{1 + \\lambda \\tau_{fac} - \\lambda \\tau_{fac} + U\\lambda \\tau_{fac}}{1 + \\lambda \\tau_{fac}}\\right) = U\n$$\n$$\n\\bar{u}^+ \\left(\\frac{1 + U \\lambda \\tau_{fac}}{1 + \\lambda \\tau_{fac}}\\right) = U\n$$\nSolving for $\\bar{u}^+$:\n$$\n\\bar{u}^+ = \\frac{U(1 + \\lambda \\tau_{fac})}{1 + U \\lambda \\tau_{fac}}\n$$\n\n**2. Derivation of $\\bar{x}^-$**\nWe follow a similar procedure for $x$. Taking expectations:\n$$\n\\bar{x}^- = \\langle 1 - (1 - x^+) \\exp(-\\Delta t / \\tau_{rec}) \\rangle\n$$\nUsing the mean-field approximation:\n$$\n\\bar{x}^- \\approx 1 - \\langle (1-x^+) \\rangle \\langle \\exp(-\\Delta t / \\tau_{rec}) \\rangle = 1 - (1-\\bar{x}^+) \\langle \\exp(-t / \\tau_{rec}) \\rangle\n$$\nThe expectation term is analogous to the one for $\\tau_{fac}$:\n$$\n\\langle \\exp(-t / \\tau_{rec}) \\rangle = \\frac{\\lambda \\tau_{rec}}{1 + \\lambda \\tau_{rec}}\n$$\nSo, $\\bar{x}^- = 1 - (1-\\bar{x}^+) \\frac{\\lambda \\tau_{rec}}{1 + \\lambda \\tau_{rec}}$.\nNow we need an equation relating $\\bar{x}^+$ and $\\bar{x}^-$. From the update rule $x^+ = x^-(1-u^+)$, we take the expectation and apply the mean-field approximation:\n$$\n\\bar{x}^+ = \\langle x^-(1-u^+) \\rangle \\approx \\langle x^- \\rangle \\langle (1-u^+) \\rangle = \\bar{x}^- (1-\\bar{u}^+)\n$$\nWe now solve the system for $\\bar{x}^-$:\n$$\n\\begin{cases}\n\\bar{x}^- = 1 - (1-\\bar{x}^+) \\frac{\\lambda \\tau_{rec}}{1 + \\lambda \\tau_{rec}} \\\\\n\\bar{x}^+ = \\bar{x}^-(1-\\bar{u}^+)\n\\end{cases}\n$$\nSubstituting the second equation into the first:\n$$\n\\bar{x}^- = 1 - (1 - \\bar{x}^-(1-\\bar{u}^+)) \\frac{\\lambda \\tau_{rec}}{1 + \\lambda \\tau_{rec}}\n$$\n$$\n\\bar{x}^- (1 + \\lambda \\tau_{rec}) = (1 + \\lambda \\tau_{rec}) - (1 - \\bar{x}^-(1-\\bar{u}^+))\\lambda \\tau_{rec}\n$$\n$$\n\\bar{x}^- + \\bar{x}^- \\lambda \\tau_{rec} = 1 + \\lambda \\tau_{rec} - \\lambda \\tau_{rec} + \\bar{x}^-(1-\\bar{u}^+) \\lambda \\tau_{rec}\n$$\n$$\n\\bar{x}^- + \\bar{x}^- \\lambda \\tau_{rec} = 1 + \\bar{x}^-\\lambda \\tau_{rec} - \\bar{x}^-\\bar{u}^+\\lambda \\tau_{rec}\n$$\n$$\n\\bar{x}^- = 1 - \\bar{x}^-\\bar{u}^+\\lambda \\tau_{rec}\n$$\n$$\n\\bar{x}^- (1 + \\bar{u}^+ \\lambda \\tau_{rec}) = 1\n$$\nSolving for $\\bar{x}^-$:\n$$\n\\bar{x}^- = \\frac{1}{1 + \\bar{u}^+ \\lambda \\tau_{rec}}\n$$\n\n**3. Final Expression for $\\mathcal{A}_{ss}(\\lambda)$**\nNow we combine the results for $\\bar{u}^+$ and $\\bar{x}^-$.\n$$\n\\mathcal{A}_{ss}(\\lambda) = \\bar{u}^+ \\bar{x}^- = \\bar{u}^+ \\left(\\frac{1}{1 + \\bar{u}^+ \\lambda \\tau_{rec}}\\right) = \\frac{\\bar{u}^+}{1 + \\bar{u}^+ \\lambda \\tau_{rec}}\n$$\nSubstitute the expression for $\\bar{u}^+$:\n$$\n\\mathcal{A}_{ss}(\\lambda) = \\frac{\\frac{U(1 + \\lambda \\tau_{fac})}{1 + U \\lambda \\tau_{fac}}}{1 + \\left(\\frac{U(1 + \\lambda \\tau_{fac})}{1 + U \\lambda \\tau_{fac}}\\right) \\lambda \\tau_{rec}}\n$$\nTo simplify, multiply the numerator and denominator by $(1 + U \\lambda \\tau_{fac})$:\n$$\n\\mathcal{A}_{ss}(\\lambda) = \\frac{U(1 + \\lambda \\tau_{fac})}{(1 + U \\lambda \\tau_{fac}) + U(1 + \\lambda \\tau_{fac}) \\lambda \\tau_{rec}}\n$$\nExpanding the denominator:\n$$\nD = 1 + U \\lambda \\tau_{fac} + U\\lambda\\tau_{rec} + U\\lambda^2\\tau_{fac}\\tau_{rec}\n$$\nWe can group the terms by powers of $\\lambda$:\n$$\nD = 1 + \\lambda U(\\tau_{fac} + \\tau_{rec}) + \\lambda^2 U \\tau_{fac} \\tau_{rec}\n$$\nThus, the final simplified expression for the steady-state expected amplitude is:\n$$\n\\mathcal{A}_{ss}(\\lambda) = \\frac{U(1 + \\lambda \\tau_{fac})}{1 + \\lambda U(\\tau_{fac} + \\tau_{rec}) + \\lambda^2 U \\tau_{fac} \\tau_{rec}}\n$$",
            "answer": "$$\n\\boxed{\\frac{U(1 + \\lambda \\tau_{fac})}{1 + \\lambda U(\\tau_{fac} + \\tau_{rec}) + \\lambda^2 U \\tau_{fac} \\tau_{rec}}}\n$$"
        },
        {
            "introduction": "With an understanding of individual neurons and dynamic synapses, we can now assemble them into a functional microcircuit. This final practice takes a system-level view, framing the cortical microcircuit as a 'reservoir' of rich, recurrent dynamics capable of performing complex computations. You will implement a complete reservoir computing system—from constructing a biologically-inspired network adhering to Dale's principle to training a linear readout with ridge regression—to solve a temporal classification task. This hands-on project demonstrates how the transient dynamics of a random recurrent network can be harnessed for powerful information processing, offering a compelling model for computation in the cortex. ",
            "id": "4041506",
            "problem": "You are to implement a computational model inspired by cortical column microcircuits to perform temporal classification using a reservoir computing approach and ridge regression for the readout. The reservoir represents a simplified, rate-based microcircuit with excitatory and inhibitory neurons obeying Dale’s principle. Your program must construct the reservoir, process input streams, compute readout weights via ridge regression, and evaluate classification performance on specified test cases. All angles are to be treated in radians. No physical units are involved in this problem.\n\nFundamental base and definitions:\n- A cortical column microcircuit can be abstracted as a recurrent network of neurons with sparse connectivity, with approximately a fraction of neurons being excitatory and the remainder inhibitory. Dale’s principle states that the outgoing synaptic weights of each neuron are either all nonnegative (excitatory) or all nonpositive (inhibitory).\n- A rate-based reservoir model uses a leaky update equation. For reservoir state vector $x_t \\in \\mathbb{R}^{N}$ at discrete time $t$, input vector $u_t \\in \\mathbb{R}^{d}$, recurrent weight matrix $W \\in \\mathbb{R}^{N \\times N}$, input weight matrix $W_{\\text{in}} \\in \\mathbb{R}^{N \\times d}$, and leak rate $0 < \\alpha \\leq 1$, the update is\n$$\nx_{t+1} = (1 - \\alpha)x_t + \\alpha \\tanh\\!\\left(W x_t + W_{\\text{in}} u_t\\right).\n$$\n- The spectral radius of a matrix is the largest absolute value of its eigenvalues, denoted $\\rho(W)$. Reservoirs are commonly scaled to a target spectral radius to control dynamics.\n- Ridge regression solves for readout weights $W_{\\text{out}}$ by minimizing the penalized least squares objective\n$$\n\\min_{W_{\\text{out}}} \\ \\|X W_{\\text{out}} - Y\\|_F^2 + \\lambda \\|W_{\\text{out}}\\|_F^2,\n$$\nwhere $X \\in \\mathbb{R}^{M \\times (N+1)}$ are feature vectors constructed from reservoir states appended with a bias term, $Y \\in \\mathbb{R}^{M \\times C}$ are one-hot class targets, $M$ is the number of sequences, $C$ is the number of classes, and $\\lambda \\ge 0$ is the regularization coefficient. The closed-form solution is\n$$\nW_{\\text{out}} = (X^\\top X + \\lambda I)^{-1} X^\\top Y.\n$$\n\nProblem setup:\n- Reservoir construction:\n  - The reservoir has $N$ neurons with a fraction $r_{\\text{exc}}$ designated as excitatory and the remainder inhibitory, enforcing that the entire row of outgoing weights from an excitatory neuron are nonnegative and from an inhibitory neuron are nonpositive.\n  - Connectivity is sparse with connection density $p \\in (0,1)$; absent connections are set to $0$.\n  - The recurrent weight matrix $W$ is scaled so that $\\rho(W)$ equals a specified target spectral radius.\n  - The input weight matrix $W_{\\text{in}}$ is drawn randomly and scaled by an input scaling factor $s_{\\text{in}}$.\n- Dynamics:\n  - For each input sequence, initialize $x_0 = \\mathbf{0}$ and update using the leaky reservoir equation above.\n  - After processing the full sequence of length $T$, construct a feature vector $s = [x_T; 1] \\in \\mathbb{R}^{N+1}$ by appending a bias term $1$.\n- Readout training and inference:\n  - Construct a training set of $M_{\\text{train}}$ sequences balanced across $C = 3$ classes, and a test set of $M_{\\text{test}}$ sequences similarly balanced.\n  - Train the readout $W_{\\text{out}} \\in \\mathbb{R}^{(N+1) \\times C}$ using ridge regression with regularization parameter $\\lambda$. If $\\lambda = 0$ and the system is singular, use the Moore–Penrose pseudoinverse to obtain the Least Squares (LS) solution.\n  - For a test sequence feature vector $s$, compute output scores $o = s^\\top W_{\\text{out}} \\in \\mathbb{R}^{C}$ and predict the class as the index of the largest component of $o$.\n- Input generation:\n  - There are $d = 2$ input channels.\n  - For class $0$: channel $0$ is a sinusoid with frequency $f_{\\text{high}}$ cycles per sample and channel $1$ is a sinusoid with frequency $f_{\\text{low}}$ cycles per sample for all times $t \\in \\{0,1,\\ldots,T-1\\}$.\n  - For class $1$: channel $0$ uses $f_{\\text{low}}$ and channel $1$ uses $f_{\\text{high}}$ for all times $t$.\n  - For class $2$: both channels use $f_{\\text{low}}$ for $t < T/2$ and $f_{\\text{high}}$ for $t \\ge T/2$.\n  - Each sinusoid is $\\sin(2\\pi f t)$ with angles in radians, and independent Gaussian noise with standard deviation $\\sigma$ is added to each channel at each time step.\n- Frequencies:\n  - Use $f_{\\text{high}} = 0.15$ and $f_{\\text{low}} = 0.03$ cycles per sample. The angular frequency is $\\omega = 2\\pi f$.\n\nYour task is to implement this model and evaluate test accuracies for the following test suite of parameter sets. Each parameter set is a tuple $(N, p, r_{\\text{exc}}, \\rho_{\\text{target}}, \\alpha, s_{\\text{in}}, \\lambda, \\sigma, T, M_{\\text{train}}, M_{\\text{test}}, \\text{seed})$:\n- Case $1$ (general case): $(200, 0.10, 0.80, 0.90, 0.30, 1.00, 0.01, 0.05, 100, 300, 120, 41)$.\n- Case $2$ (boundary $\\lambda = 0$): $(200, 0.10, 0.80, 0.90, 0.30, 1.00, 0.00, 0.05, 100, 300, 120, 42)$.\n- Case $3$ (edge input scale $s_{\\text{in}} = 0$): $(200, 0.10, 0.80, 0.90, 0.30, 0.00, 0.10, 0.05, 100, 300, 120, 43)$.\n- Case $4$ (near-instability $\\rho_{\\text{target}} = 1.20$ with smaller leak, heavy regularization): $(200, 0.10, 0.80, 1.20, 0.10, 1.00, 1.00, 0.05, 100, 300, 120, 44)$.\n- Case $5$ (short sequences): $(200, 0.10, 0.80, 0.90, 0.50, 1.00, 0.01, 0.05, 20, 300, 120, 45)$.\n\nDesign for coverage:\n- Case $1$ is the happy path for a well-conditioned reservoir.\n- Case $2$ tests the boundary condition with no regularization, requiring a robust handling of possible singularities.\n- Case $3$ tests the degenerate case with no input drive, where the reservoir cannot distinguish classes and performance should degrade toward chance level.\n- Case $4$ tests high spectral radius with small leak and strong regularization.\n- Case $5$ tests reduced temporal memory due to short sequence length.\n\nFinal output specification:\n- Your program should produce a single line of output containing the test accuracies as a comma-separated list of floats rounded to six decimal places and enclosed in square brackets. For example, the output format must be like $[a_1,a_2,a_3,a_4,a_5]$, where each $a_i$ is the accuracy for case $i$ expressed as a decimal fraction rounded to six decimal places.",
            "solution": "The problem is valid as it presents a scientifically grounded, well-posed, and objective computational task based on established principles in neuromorphic computing and machine learning. All necessary parameters and definitions for implementing a reservoir computing model for temporal classification are provided. The procedure is deterministic given the specified random seeds, ensuring a unique and verifiable solution.\n\nThe solution involves a step-by-step implementation and evaluation of this model. The core components are: input signal generation, reservoir construction adhering to Dale's principle, simulation of reservoir dynamics, training of a linear readout using ridge regression, and performance evaluation.\n\n### 1. Input Data Generation\n\nThe model must classify temporal patterns defined across $d=2$ input channels and $C=3$ classes. For each class $c \\in \\{0, 1, 2\\}$, we generate input sequences $\\mathbf{u}^{(c)} = \\{u_0, u_1, \\ldots, u_{T-1}\\}$ of length $T$, where $u_t \\in \\mathbb{R}^2$. The signals are based on sinusoids with two distinct frequencies, a high frequency $f_{\\text{high}} = 0.15$ cycles/sample and a low frequency $f_{\\text{low}} = 0.03$ cycles/sample. The corresponding angular frequencies are $\\omega_{\\text{high}} = 2\\pi f_{\\text{high}}$ and $\\omega_{\\text{low}} = 2\\pi f_{\\text{low}}$. Independent Gaussian noise, $n_t \\sim \\mathcal{N}(0, \\sigma^2 I)$, is added at each time step.\n\nThe input vector $u_t$ for each class is defined as follows:\n- **Class 0:** $u_t = [\\sin(\\omega_{\\text{high}} t), \\sin(\\omega_{\\text{low}} t)]^\\top + n_t$\n- **Class 1:** $u_t = [\\sin(\\omega_{\\text{low}} t), \\sin(\\omega_{\\text{high}} t)]^\\top + n_t$\n- **Class 2:** $u_t = \\begin{cases} [\\sin(\\omega_{\\text{low}} t), \\sin(\\omega_{\\text{low}} t)]^\\top + n_t & \\text{if } t < T/2 \\\\ [\\sin(\\omega_{\\text{high}} t), \\sin(\\omega_{\\text{high}} t)]^\\top + n_t & \\text{if } t \\ge T/2 \\end{cases}$\n\nFor each test case, we generate $M_{\\text{train}}$ training sequences and $M_{\\text{test}}$ testing sequences, balanced across the three classes.\n\n### 2. Reservoir Construction\n\nThe reservoir is a recurrent neural network of $N$ neurons. Its structure is defined by the recurrent weight matrix $W \\in \\mathbb{R}^{N \\times N}$ and the input weight matrix $W_{\\text{in}} \\in \\mathbb{R}^{N \\times d}$.\n\n**Recurrent Weight Matrix ($W$):**\n1.  **Initialization:** A dense matrix $W_{\\text{rand}} \\in \\mathbb{R}^{N \\times N}$ is initialized with values drawn from a standard normal distribution $\\mathcal{N}(0, 1)$.\n2.  **Sparsity:** A binary mask $M_{\\text{sparse}} \\in \\{0,1\\}^{N \\times N}$ is created, where each element is $1$ with probability $p$ (the connection density) and $0$ otherwise. The sparse matrix is $W_{\\text{sparse}} = W_{\\text{rand}} \\odot M_{\\text{sparse}}$, where $\\odot$ is the Hadamard (element-wise) product.\n3.  **Dale's Principle:** The $N$ neurons are partitioned into $N_{\\text{exc}} = \\lfloor N \\cdot r_{\\text{exc}} \\rfloor$ excitatory and $N_{\\text{inh}} = N - N_{\\text{exc}}$ inhibitory neurons. For each neuron $i$, its outgoing weights (row $i$ of $W$) are made either all non-negative (if excitatory) or all non-positive (if inhibitory). This is achieved by taking the absolute value of the weights in $W_{\\text{sparse}}$ and then multiplying each row by $+1$ or $-1$ corresponding to the neuron type.\n4.  **Spectral Radius Scaling:** The spectral radius $\\rho(W)$, which is the maximum absolute eigenvalue of the resulting matrix, governs the network's dynamics. To ensure desired properties (e.g., the echo state property), the matrix is scaled to a target spectral radius $\\rho_{\\text{target}}$. The final recurrent matrix is $W = \\rho_{\\text{target}} \\frac{W_{\\text{dale}}}{\\rho(W_{\\text{dale}})}$, assuming $\\rho(W_{\\text{dale}}) \\neq 0$.\n\n**Input Weight Matrix ($W_{\\text{in}}$):**\nThe input matrix $W_{\\text{in}}$ is constructed by drawing elements from a uniform distribution $U(-1, 1)$ and scaling the entire matrix by the input scaling factor $s_{\\text{in}}$.\n\n### 3. Reservoir Dynamics and Feature Extraction\n\nThe state of the reservoir, $x_t \\in \\mathbb{R}^{N}$, evolves over time according to the leaky rate-based update equation:\n$$\nx_{t+1} = (1 - \\alpha)x_t + \\alpha \\tanh(W x_t + W_{\\text{in}} u_t)\n$$\nwhere $\\alpha$ is the leak rate. For each input sequence, the reservoir is initialized with a zero state vector, $x_0 = \\mathbf{0}$, and driven by the input for $T$ time steps. The final state of the reservoir, $x_T$, captures a non-linear projection of the input history. This state is augmented with a bias term to form the feature vector for classification: $s = [x_T^\\top, 1]^\\top \\in \\mathbb{R}^{N+1}$.\n\n### 4. Readout Training via Ridge Regression\n\nThe readout mechanism is a linear classifier trained to map the reservoir's feature vectors to class labels. We collect the feature vectors from all $M_{\\text{train}}$ training sequences into a feature matrix $X \\in \\mathbb{R}^{M_{\\text{train}} \\times (N+1)}$. The corresponding class labels are represented in a one-hot encoded target matrix $Y \\in \\mathbb{R}^{M_{\\text{train}} \\times C}$.\n\nThe readout weights $W_{\\text{out}} \\in \\mathbb{R}^{(N+1) \\times C}$ are found by solving the ridge regression problem, which minimizes a combination of the squared error and a penalty on the magnitude of the weights:\n$$\n\\min_{W_{\\text{out}}} \\|X W_{\\text{out}} - Y\\|_F^2 + \\lambda \\|W_{\\text{out}}\\|_F^2\n$$\nThe regularization parameter $\\lambda \\ge 0$ controls the trade-off. The closed-form solution is:\n$$\nW_{\\text{out}} = (X^\\top X + \\lambda I)^{-1} X^\\top Y\n$$\nFor the case where $\\lambda = 0$, the problem reduces to ordinary least squares. If $X^\\top X$ is singular, the inverse does not exist. As specified, the solution is then found using the Moore-Penrose pseudoinverse, $X^\\dagger$. The solution for the weights becomes $W_{\\text{out}} = X^\\dagger Y$. This provides the minimum-norm solution to the least-squares problem.\n\n### 5. Inference and Evaluation\n\nOnce $W_{\\text{out}}$ is trained, the model's performance is evaluated on the $M_{\\text{test}}$ test sequences. For each test sequence, we compute its feature vector $s_{\\text{test}}$ and calculate the output scores $o = s_{\\text{test}}^\\top W_{\\text{out}}$. The predicted class is the one corresponding to the highest score: $\\hat{y} = \\text{argmax}_{i} (o_i)$.\n\nThe final metric is the test accuracy, defined as the fraction of test sequences that are correctly classified. This procedure is repeated for each parameter set provided in the test suite.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a reservoir computing model for temporal classification.\n    \"\"\"\n    test_cases = [\n        # (N, p, r_exc, rho_target, alpha, s_in, lambda, sigma, T, M_train, M_test, seed)\n        (200, 0.10, 0.80, 0.90, 0.30, 1.00, 0.01, 0.05, 100, 300, 120, 41),\n        (200, 0.10, 0.80, 0.90, 0.30, 1.00, 0.00, 0.05, 100, 300, 120, 42),\n        (200, 0.10, 0.80, 0.90, 0.30, 0.00, 0.10, 0.05, 100, 300, 120, 43),\n        (200, 0.10, 0.80, 1.20, 0.10, 1.00, 1.00, 0.05, 100, 300, 120, 44),\n        (200, 0.10, 0.80, 0.90, 0.50, 1.00, 0.01, 0.05, 20, 300, 120, 45),\n    ]\n\n    results = []\n\n    f_high = 0.15\n    f_low = 0.03\n    d = 2\n    C = 3\n\n    for params in test_cases:\n        N, p, r_exc, rho_target, alpha, s_in, lam, sigma, T, M_train, M_test, seed = params\n        rng = np.random.default_rng(seed)\n\n        # 1. Reservoir Construction\n        N_exc = int(N * r_exc)\n        \n        # Initialize W\n        W = rng.standard_normal(size=(N, N))\n        \n        # Sparsity\n        sparsity_mask = rng.random(size=(N, N)) < p\n        W *= sparsity_mask\n\n        # Dale's Principle\n        exc_indices = rng.choice(N, size=N_exc, replace=False)\n        inh_indices = np.setdiff1d(np.arange(N), exc_indices)\n        dale_mask = np.ones((N, 1))\n        dale_mask[inh_indices] = -1\n        W = np.abs(W) * dale_mask\n        \n        # Scale spectral radius\n        try:\n            eigenvalues = np.linalg.eigvals(W)\n            rho_W = np.max(np.abs(eigenvalues))\n            if rho_W > 1e-9: # Avoid division by zero\n                W = W * (rho_target / rho_W)\n        except np.linalg.LinAlgError:\n            # This can happen for pathological matrices, though unlikely here.\n            # If so, we proceed with the unscaled matrix.\n            pass\n\n        # Input weights\n        W_in = rng.uniform(-1, 1, size=(N, d)) * s_in\n\n        # 2. Data Generation & Feature Extraction\n        def generate_data_and_features(M):\n            X_features = np.zeros((M, N + 1))\n            Y_labels = np.zeros((M,), dtype=int)\n            \n            samples_per_class = M // C\n            time_vec = np.arange(T)\n            s_high = np.sin(2 * np.pi * f_high * time_vec)\n            s_low = np.sin(2 * np.pi * f_low * time_vec)\n            \n            for i in range(M):\n                class_idx = i // samples_per_class\n                Y_labels[i] = class_idx\n                \n                u = np.zeros((T, d))\n                if class_idx == 0:\n                    u[:, 0] = s_high\n                    u[:, 1] = s_low\n                elif class_idx == 1:\n                    u[:, 0] = s_low\n                    u[:, 1] = s_high\n                else: # class_idx == 2\n                    split_idx = T // 2\n                    u[:split_idx, :] = np.vstack([s_low[:split_idx], s_low[:split_idx]]).T\n                    u[split_idx:, :] = np.vstack([s_high[split_idx:], s_high[split_idx:]]).T\n                \n                noise = rng.normal(0, sigma, size=(T, d))\n                u += noise\n                \n                # Run reservoir dynamics\n                x = np.zeros(N)\n                for t in range(T):\n                    x = (1 - alpha) * x + alpha * np.tanh(W @ x + W_in @ u[t])\n                \n                # Store final state with bias\n                X_features[i, :N] = x\n                X_features[i, N] = 1.0 # Bias term\n            \n            return X_features, Y_labels\n\n        X_train, y_train_labels = generate_data_and_features(M_train)\n        X_test, y_test_labels = generate_data_and_features(M_test)\n        \n        # One-hot encode training labels\n        Y_train_onehot = np.zeros((M_train, C))\n        Y_train_onehot[np.arange(M_train), y_train_labels] = 1\n\n        # 3. Readout Training\n        if lam == 0.0:\n            # Use Moore-Penrose pseudoinverse for Ordinary Least Squares\n            try:\n                X_pinv = np.linalg.pinv(X_train)\n                W_out = X_pinv @ Y_train_onehot\n            except np.linalg.LinAlgError:\n                # Should not be reached with pinv\n                W_out = np.zeros((N + 1, C))\n        else:\n            # Ridge Regression\n            I = np.identity(N + 1)\n            term1 = np.linalg.inv(X_train.T @ X_train + lam * I)\n            term2 = X_train.T @ Y_train_onehot\n            W_out = term1 @ term2\n        \n        # 4. Inference and Evaluation\n        output_scores = X_test @ W_out\n        predicted_classes = np.argmax(output_scores, axis=1)\n        \n        accuracy = np.mean(predicted_classes == y_test_labels)\n        results.append(accuracy)\n\n    # Final print statement\n    print(f\"[{','.join(f'{acc:.6f}' for acc in results)}]\")\n\nsolve()\n```"
        }
    ]
}