{
    "hands_on_practices": [
        {
            "introduction": "To understand the brain's hierarchical organization, we first need a method to measure it. This practice introduces a foundational technique based on neuroanatomical data, specifically the distinct laminar patterns of connections between cortical areas. By applying a quantitative index and mathematical optimization, you will learn how to transform raw connectivity counts into a ranked hierarchical structure, a fundamental skill in systems neuroscience. ",
            "id": "4055874",
            "problem": "Consider a network of $3$ neocortical areas, labeled $A$, $B$, and $C$. For each ordered pair of areas $i \\to j$, two counts are measured: the number of supragranular-originating neurons $S_{ij}$ (layers II/III) and the number of infragranular-originating neurons $I_{ij}$ (layers V/VI) that project from area $i$ to area $j$. The supragranular-to-infragranular laminar index for projection $i \\to j$ is defined as\n$$\nr_{ij} \\equiv \\frac{S_{ij} - I_{ij}}{S_{ij} + I_{ij}},\n$$\nwith $-1 < r_{ij} < 1$. In the established laminar model of cortical hierarchy, a projection with $r_{ij} > 0$ is laminar feedforward (originating predominantly in supragranular layers) and is taken to go from a lower-hierarchy area to a higher-hierarchy area, whereas a projection with $r_{ij} < 0$ is laminar feedback and goes from a higher-hierarchy area to a lower-hierarchy area. Furthermore, a well-tested generative model relates $r_{ij}$ monotonically to the hierarchical distance between areas, and for small to moderate distances one may linearize this relationship so that\n$$\nr_{ij} \\approx h_{j} - h_{i},\n$$\nwhere $h_{i}$ is the (unknown) continuous hierarchy value of area $i$. The hierarchy values are defined up to an additive constant; to fix the gauge, impose $\\sum_{i \\in \\{A,B,C\\}} h_{i} = 0$.\n\nYou are given the following laminar connectivity matrices (zeros on the diagonal):\n$$\nS \\;=\\; \\begin{pmatrix}\n0 & 3 & 11 \\\\\n1 & 0 & 2 \\\\\n1 & 1 & 0\n\\end{pmatrix},\n\\qquad\nI \\;=\\; \\begin{pmatrix}\n0 & 1 & 1 \\\\\n3 & 0 & 1 \\\\\n11 & 2 & 0\n\\end{pmatrix}.\n$$\n\nTasks:\n- Compute all $r_{ij}$ for $i \\neq j$.\n- Using the linearized model $r_{ij} \\approx h_{j} - h_{i}$, estimate $(h_{A}, h_{B}, h_{C})$ by solving the least-squares problem that minimizes $\\sum_{i \\neq j} \\big(r_{ij} - (h_{j} - h_{i})\\big)^{2}$ subject to $h_{A} + h_{B} + h_{C} = 0$.\n- Validate the inferred ranking by constructing the directed graph on $\\{A, B, C\\}$ with an edge $i \\to j$ whenever $r_{ij} > 0$, and performing a topological sort of this graph to confirm that the order is consistent with the ascending values of $(h_{A}, h_{B}, h_{C})$.\n\nReport the final hierarchy values as a row vector $\\big(h_{A}, h_{B}, h_{C}\\big)$ in exact form. Do not round. The final answer must be given as a single row vector using the $\\pmatrix$ notation. No units are required.",
            "solution": "The problem requires the determination of hierarchy values $(h_A, h_B, h_C)$ for three neocortical areas, based on neuron projection counts, and subsequent validation of the result. The process involves three main tasks: computing the laminar index for each projection, solving a constrained least-squares problem to estimate the hierarchy values, and validating the ordering using a graph-based topological sort.\n\nLet the areas be indexed as $1$ for $A$, $2$ for $B$, and $3$ for $C$. The given data for supragranular ($S$) and infragranular ($I$) projection neuron counts are:\n$$\nS \\;=\\; \\begin{pmatrix}\n0 & 3 & 11 \\\\\n1 & 0 & 2 \\\\\n1 & 1 & 0\n\\end{pmatrix},\n\\qquad\nI \\;=\\; \\begin{pmatrix}\n0 & 1 & 1 \\\\\n3 & 0 & 1 \\\\\n11 & 2 & 0\n\\end{pmatrix}\n$$\nThese matrices represent $S_{ij}$ and $I_{ij}$ where $i, j \\in \\{A, B, C\\}$.\n\nTask 1: Compute all $r_{ij}$ for $i \\neq j$.\nThe supragranular-to-infragranular laminar index $r_{ij}$ is defined as $r_{ij} = \\frac{S_{ij} - I_{ij}}{S_{ij} + I_{ij}}$. We compute the off-diagonal elements of the matrix $R = (r_{ij})$:\n\n$r_{AB} = r_{12} = \\frac{S_{12} - I_{12}}{S_{12} + I_{12}} = \\frac{3 - 1}{3 + 1} = \\frac{2}{4} = \\frac{1}{2}$\n\n$r_{AC} = r_{13} = \\frac{S_{13} - I_{13}}{S_{13} + I_{13}} = \\frac{11 - 1}{11 + 1} = \\frac{10}{12} = \\frac{5}{6}$\n\n$r_{BA} = r_{21} = \\frac{S_{21} - I_{21}}{S_{21} + I_{21}} = \\frac{1 - 3}{1 + 3} = \\frac{-2}{4} = -\\frac{1}{2}$\n\n$r_{BC} = r_{23} = \\frac{S_{23} - I_{23}}{S_{23} + I_{23}} = \\frac{2 - 1}{2 + 1} = \\frac{1}{3}$\n\n$r_{CA} = r_{31} = \\frac{S_{31} - I_{31}}{S_{31} + I_{31}} = \\frac{1 - 11}{1 + 11} = \\frac{-10}{12} = -\\frac{5}{6}$\n\n$r_{CB} = r_{32} = \\frac{S_{32} - I_{32}}{S_{32} + I_{32}} = \\frac{1 - 2}{1 + 2} = \\frac{-1}{3}$\n\nThe resulting matrix of laminar indices is:\n$$\nR \\;=\\; \\begin{pmatrix}\n0 & \\frac{1}{2} & \\frac{5}{6} \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{3} \\\\\n-\\frac{5}{6} & -\\frac{1}{3} & 0\n\\end{pmatrix}\n$$\nWe observe that $r_{ji} = -r_{ij}$, which is consistent with the given data where $S = I^T$.\n\nTask 2: Estimate $(h_A, h_B, h_C)$ using a least-squares fit.\nWe need to find the values of $h_A, h_B, h_C$ that minimize the sum of squared errors between the measured $r_{ij}$ values and the model $h_j - h_i$. The objective function is:\n$$\nE(h_A, h_B, h_C) = \\sum_{i \\neq j} \\big(r_{ij} - (h_{j} - h_{i})\\big)^{2}\n$$\nsubject to the constraint $h_A + h_B + h_C = 0$.\n\nSince $r_{ji} = -r_{ij}$ and $(h_i - h_j) = -(h_j - h_i)$, the term for $(j,i)$ in the sum is identical to the term for $(i,j)$:\n$$\n(r_{ji} - (h_i - h_j))^2 = (-r_{ij} - (-(h_j - h_i)))^2 = (r_{ij} - (h_j-h_i))^2\n$$\nThus, the sum can be written as:\n$$\nE = 2 \\left[ (r_{AB} - (h_B - h_A))^2 + (r_{AC} - (h_C - h_A))^2 + (r_{BC} - (h_C - h_B))^2 \\right]\n$$\nMinimizing $E$ is equivalent to minimizing the simpler function $E'$:\n$$\nE'(h_A, h_B, h_C) = (r_{AB} - h_B + h_A)^2 + (r_{AC} - h_C + h_A)^2 + (r_{BC} - h_C + h_B)^2\n$$\nTo solve this constrained optimization problem, we use the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(h_A, h_B, h_C, \\lambda) = E'(h_A, h_B, h_C) + \\lambda (h_A + h_B + h_C)\n$$\nWe set the partial derivatives with respect to each $h_i$ to zero:\n$\\frac{\\partial \\mathcal{L}}{\\partial h_A} = 2(r_{AB} - h_B + h_A) + 2(r_{AC} - h_C + h_A) + \\lambda = 0 \\implies 2h_A - h_B - h_C = -r_{AB} - r_{AC} - \\lambda/2$\n$\\frac{\\partial \\mathcal{L}}{\\partial h_B} = -2(r_{AB} - h_B + h_A) + 2(r_{BC} - h_C + h_B) + \\lambda = 0 \\implies -h_A + 2h_B - h_C = r_{AB} - r_{BC} - \\lambda/2$\n$\\frac{\\partial \\mathcal{L}}{\\partial h_C} = -2(r_{AC} - h_C + h_A) - 2(r_{BC} - h_C + h_B) + \\lambda = 0 \\implies -h_A - h_B + 2h_C = r_{AC} + r_{BC} - \\lambda/2$\n\nSumming these three equations yields $0 = 0 - 3\\lambda/2$, which implies $\\lambda=0$. The system simplifies to:\n$1) \\quad 2h_A - h_B - h_C = -r_{AB} - r_{AC}$\n$2) \\quad -h_A + 2h_B - h_C = r_{AB} - r_{BC}$\n$3) \\quad -h_A - h_B + 2h_C = r_{AC} + r_{BC}$\nWe solve this system along with the constraint $4) \\quad h_A + h_B + h_C = 0$.\n\nFrom the constraint, we have $h_B + h_C = -h_A$. Substituting this into equation (1):\n$2h_A - (-h_A) = -r_{AB} - r_{AC} \\implies 3h_A = -r_{AB} - r_{AC} \\implies h_A = -\\frac{1}{3}(r_{AB} + r_{AC})$\n\nSimilarly, from the constraint, $h_A + h_C = -h_B$. Substituting this into equation (2):\n$2h_B - (h_A + h_C) = r_{AB} - r_{BC} \\implies 2h_B - (-h_B) = r_{AB} - r_{BC} \\implies 3h_B = r_{AB} - r_{BC} \\implies h_B = \\frac{1}{3}(r_{AB} - r_{BC})$\n\nFinally, from the constraint, we find $h_C$:\n$h_C = -h_A - h_B = \\frac{1}{3}(r_{AB} + r_{AC}) - \\frac{1}{3}(r_{AB} - r_{BC}) = \\frac{1}{3}(r_{AC} + r_{BC})$\n\nNow, we substitute the numerical values of $r_{ij}$:\n$h_A = -\\frac{1}{3} \\left( \\frac{1}{2} + \\frac{5}{6} \\right) = -\\frac{1}{3} \\left( \\frac{3}{6} + \\frac{5}{6} \\right) = -\\frac{1}{3} \\left( \\frac{8}{6} \\right) = -\\frac{1}{3} \\left( \\frac{4}{3} \\right) = -\\frac{4}{9}$\n\n$h_B = \\frac{1}{3} \\left( \\frac{1}{2} - \\frac{1}{3} \\right) = \\frac{1}{3} \\left( \\frac{3}{6} - \\frac{2}{6} \\right) = \\frac{1}{3} \\left( \\frac{1}{6} \\right) = \\frac{1}{18}$\n\n$h_C = \\frac{1}{3} \\left( \\frac{5}{6} + \\frac{1}{3} \\right) = \\frac{1}{3} \\left( \\frac{5}{6} + \\frac{2}{6} \\right) = \\frac{1}{3} \\left( \\frac{7}{6} \\right) = \\frac{7}{18}$\n\nThe estimated hierarchy values are $(h_A, h_B, h_C) = (-\\frac{4}{9}, \\frac{1}{18}, \\frac{7}{18})$.\nLet's check the constraint: $h_A + h_B + h_C = -\\frac{4}{9} + \\frac{1}{18} + \\frac{7}{18} = -\\frac{8}{18} + \\frac{8}{18} = 0$. The constraint is satisfied.\n\nTask 3: Validate the inferred ranking.\nWe construct a directed graph where an edge $i \\to j$ exists if $r_{ij} > 0$.\n- $r_{AB} = \\frac{1}{2} > 0 \\implies$ edge $A \\to B$.\n- $r_{AC} = \\frac{5}{6} > 0 \\implies$ edge $A \\to C$.\n- $r_{BC} = \\frac{1}{3} > 0 \\implies$ edge $B \\to C$.\nThe other $r_{ij}$ values are negative, so there are no other edges. The graph has vertices $\\{A, B, C\\}$ and edges $\\{(A,B), (A,C), (B,C)\\}$.\n\nTo perform a topological sort, we use Kahn's algorithm. We find nodes with an in-degree of $0$.\n- In-degree of $A$: $0$.\n- In-degree of $B$: $1$ (from $A$).\n- In-degree of $C$: $2$ (from $A$ and $B$).\nThe only node with an in-degree of $0$ is $A$. So, it is the first in the sort. We remove $A$ and its outgoing edges from the graph. The remaining graph has vertices $\\{B, C\\}$ and one edge $(B, C)$.\nThe in-degrees in the new graph are:\n- In-degree of $B$: $0$.\n- In-degree of $C$: $1$ (from $B$).\nThe only node with in-degree $0$ is $B$. So it is next. Removing $B$ leaves only $C$.\nThe unique topological sort of the graph is the sequence $A, B, C$.\n\nThis order should be consistent with the ascending values of the hierarchy levels. The computed values are:\n$h_A = -\\frac{4}{9} = -\\frac{8}{18} \\approx -0.444$\n$h_B = \\frac{1}{18} \\approx 0.056$\n$h_C = \\frac{7}{18} \\approx 0.389$\nThe values are ordered as $h_A < h_B < h_C$. This corresponds to the hierarchy $A \\to B \\to C$.\nThe topological sort $A, B, C$ is fully consistent with the hierarchy ordering inferred from the least-squares estimation. The validation is successful.\n\nThe final answer is the row vector of the estimated hierarchy values.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} -\\frac{4}{9} & \\frac{1}{18} & \\frac{7}{18} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While anatomical studies reveal a hierarchical structure, how does such an organization arise? This exercise explores the principle of self-organization, demonstrating how a simple, local learning rule—Oja's rule for Hebbian plasticity—can cause a neuron to learn the principal component of its inputs. You will analyze how this process naturally gives rise to an ordered feature hierarchy based on the statistical regularities of the environment, linking synaptic-level dynamics to system-level organization. ",
            "id": "4055805",
            "problem": "Consider a linear neuron receiving zero-mean inputs $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ drawn independently and identically distributed from a stationary distribution with covariance matrix $C = \\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{\\top}]$. The neuron's output is $y = \\boldsymbol{w}^{\\top}\\boldsymbol{x}$, where $\\boldsymbol{w} \\in \\mathbb{R}^{n}$ is the synaptic weight vector. The synapses update according to Oja's rule, a normalized form of Hebbian learning: $\\Delta \\boldsymbol{w} = \\eta\\, y\\, \\boldsymbol{x} - \\eta\\, y^{2}\\, \\boldsymbol{w}$, with small constant learning rate $\\eta > 0$. Principal Component Analysis (PCA) refers to extracting orthogonal directions that maximize the variance of the projected data, ordered by eigenvalues of $C$.\n\nStarting from the definitions of covariance and the stated learning rule, derive the expected continuous-time weight dynamics as a function of $C$ and $\\boldsymbol{w}$. Explain the fixed points of this dynamical system and their stability in terms of the eigen-decomposition of $C$, and thereby relate convergence of Hebbian learning to principal components. Use this relationship to connect the emergence of ordered feature hierarchies to the eigenvalue spectrum via mode-specific convergence rates.\n\nNow specialize to the case $n = 3$ with\n$$\nC = \\begin{pmatrix}\n4 & 2 & 0 \\\\\n2 & 3 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nLet $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3} > 0$ denote the eigenvalues of $C$. Linearize the Oja dynamics around the stable fixed point aligned with the principal eigenvector and define the mode-specific time constants $\\tau_{i}$ of convergence along the eigendirections associated with $\\lambda_{i}$ by the inverse of the corresponding linearized decay rates. Compute the ratio\n$$\n\\rho \\equiv \\frac{\\tau_{\\text{second-largest}}}{\\tau_{\\text{smallest}}} = \\frac{\\tau_{2}}{\\tau_{3}}\n$$\nfor the given $C$, and provide its numerical value rounded to four significant figures. Express the final answer as a dimensionless number with no units.",
            "solution": "The problem asks for a derivation of the continuous-time dynamics of Oja's learning rule, an analysis of its fixed points and their stability, and a specific calculation for a given covariance matrix $C$.\n\nFirst, we derive the continuous-time dynamics. For a small learning rate $\\eta$, the discrete updates can be averaged over the input distribution to find the expected change, which approximates a continuous flow. We define a continuous time variable $t$ that scales with the number of updates, such that $\\frac{d\\boldsymbol{w}}{dt}$ is proportional to the expected update $\\mathbb{E}[\\Delta\\boldsymbol{w}]$. By convention, we absorb the learning rate into the time scaling, so $\\frac{d\\boldsymbol{w}}{dt} = \\mathbb{E}[\\frac{\\Delta\\boldsymbol{w}}{\\eta}]$.\n$$\n\\frac{d\\boldsymbol{w}}{dt} = \\mathbb{E}[y\\, \\boldsymbol{x} - y^{2}\\, \\boldsymbol{w}] = \\mathbb{E}[y\\, \\boldsymbol{x}] - \\mathbb{E}[y^{2}]\\,\\boldsymbol{w}\n$$\nWe evaluate the two expectation terms.\nThe first term is the Hebbian term:\n$$\n\\mathbb{E}[y\\, \\boldsymbol{x}] = \\mathbb{E}[(\\boldsymbol{w}^{\\top}\\boldsymbol{x})\\, \\boldsymbol{x}] = \\mathbb{E}[\\boldsymbol{x}(\\boldsymbol{x}^{\\top}\\boldsymbol{w})] = \\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{\\top}]\\boldsymbol{w} = C\\boldsymbol{w}\n$$\nThe second term is the normalization or decay term:\n$$\n\\mathbb{E}[y^{2}] = \\mathbb{E}[(\\boldsymbol{w}^{\\top}\\boldsymbol{x})^{2}] = \\mathbb{E}[(\\boldsymbol{w}^{\\top}\\boldsymbol{x})(\\boldsymbol{x}^{\\top}\\boldsymbol{w})] = \\boldsymbol{w}^{\\top}\\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{\\top}]\\boldsymbol{w} = \\boldsymbol{w}^{\\top}C\\boldsymbol{w}\n$$\nSubstituting these back into the differential equation gives the continuous-time dynamics for the weight vector $\\boldsymbol{w}$:\n$$\n\\frac{d\\boldsymbol{w}}{dt} = C\\boldsymbol{w} - (\\boldsymbol{w}^{\\top}C\\boldsymbol{w})\\boldsymbol{w}\n$$\nNext, we find the fixed points $\\boldsymbol{w}^*$ of this system by setting $\\frac{d\\boldsymbol{w}}{dt} = 0$:\n$$\nC\\boldsymbol{w}^* = (\\boldsymbol{w}^{*\\top}C\\boldsymbol{w}^*)\\boldsymbol{w}^*\n$$\nThis equation has the form $C\\boldsymbol{v} = \\alpha\\boldsymbol{v}$, which is the definition of an eigenvector. Thus, any fixed point $\\boldsymbol{w}^*$ must be an eigenvector of the covariance matrix $C$. Let $\\boldsymbol{v}_i$ be an eigenvector of $C$ with eigenvalue $\\lambda_i$, so $C\\boldsymbol{v}_i = \\lambda_i \\boldsymbol{v}_i$. If we set $\\boldsymbol{w}^* = c\\boldsymbol{v}_i$ for some scalar $c$, the equation becomes:\n$$\nC(c\\boldsymbol{v}_i) = ((c\\boldsymbol{v}_i)^{\\top}C(c\\boldsymbol{v}_i))(c\\boldsymbol{v}_i) \\implies c\\lambda_i\\boldsymbol{v}_i = (c^2 \\boldsymbol{v}_i^{\\top}\\lambda_i\\boldsymbol{v}_i)(c\\boldsymbol{v}_i) = c^3 \\lambda_i (\\boldsymbol{v}_i^{\\top}\\boldsymbol{v}_i)\\boldsymbol{v}_i\n$$\nAssuming $\\lambda_i \\neq 0$ and $\\boldsymbol{v}_i \\neq \\boldsymbol{0}$, we get $c = c^3 \\lambda_i ||\\boldsymbol{v}_i||^2$. One solution is $c=0$, so $\\boldsymbol{w}^* = \\boldsymbol{0}$ is a trivial fixed point. If $c \\neq 0$, then $1 = c^2 \\lambda_i ||\\boldsymbol{v}_i||^2$. If we use normalized eigenvectors such that $||\\boldsymbol{v}_i|| = 1$, the fixed points are the unit eigenvectors $\\boldsymbol{v}_i$ of $C$.\n\nTo analyze stability, we consider a small perturbation $\\boldsymbol{\\epsilon}(t)$ around a fixed point $\\boldsymbol{w}^* = \\boldsymbol{v}_i$, where $\\boldsymbol{v}_i$ is a unit eigenvector with eigenvalue $\\lambda_i$. Let $\\boldsymbol{w}(t) = \\boldsymbol{v}_i + \\boldsymbol{\\epsilon}(t)$. The linearized dynamics for $\\boldsymbol{\\epsilon}$ are:\n$$\n\\frac{d\\boldsymbol{\\epsilon}}{dt} \\approx (C - \\lambda_i I)\\boldsymbol{\\epsilon} - 2\\lambda_i(\\boldsymbol{v}_i^{\\top}\\boldsymbol{\\epsilon})\\boldsymbol{v}_i\n$$\nSince $C$ is a symmetric matrix, its eigenvectors $\\{\\boldsymbol{v}_j\\}$ form an orthonormal basis. We can express the perturbation $\\boldsymbol{\\epsilon}$ in this basis: $\\boldsymbol{\\epsilon}(t) = \\sum_{j=1}^{n} c_j(t) \\boldsymbol{v}_j$. Substituting this into the linearized equation and taking the dot product with $\\boldsymbol{v}_k$ yields the dynamics for the components $c_k$:\nFor $k=i$: $\\frac{dc_i}{dt} = -2\\lambda_i c_i$. This component decays, corresponding to the restoration of the vector's norm.\nFor $k \\neq i$: $\\frac{dc_k}{dt} = (\\lambda_k - \\lambda_i) c_k$.\nFor the fixed point $\\boldsymbol{v}_i$ to be stable, all perturbations must decay to zero. This requires the coefficients $(\\lambda_k-\\lambda_i)$ to be negative for all $k \\neq i$. This condition, $\\lambda_k < \\lambda_i$ for all $k \\neq i$, is only met if $\\lambda_i$ is the largest eigenvalue, which we denote $\\lambda_1$. Therefore, the only stable fixed point is $\\boldsymbol{v}_1$, the unit eigenvector corresponding to the largest eigenvalue. All other eigenvector fixed points are unstable (saddle points), and the origin is also unstable.\n\nThis directly relates Oja's rule to PCA. The weight vector $\\boldsymbol{w}$ converges to the first principal component of the input data, which is the direction of maximum variance. The rate of convergence is determined by the eigenvalue spectrum. The component of the weight vector along any other eigendirection $\\boldsymbol{v}_k$ (where $k > 1$) decays exponentially with a rate of $\\lambda_1 - \\lambda_k > 0$. The time constant for this decay is $\\tau_k = 1/(\\lambda_1-\\lambda_k)$. Slower convergence occurs for modes whose eigenvalues $\\lambda_k$ are close to $\\lambda_1$. This explains how a learning hierarchy can emerge: the most prominent features (highest variance) are learned first, and the speed of learning subsequent features is governed by the separation in their corresponding variances (eigenvalues).\n\nNow we specialize to the given case with $n=3$ and $C = \\begin{pmatrix} 4 & 2 & 0 \\\\ 2 & 3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\nThe matrix is block-diagonal, so one eigenvalue is immediately $\\lambda_3=1$, with eigenvector $\\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}^{\\top}$. The other two eigenvalues are from the submatrix $C' = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}$. The characteristic equation is $\\det(C' - \\lambda I)=0$:\n$$\n(4-\\lambda)(3-\\lambda) - (2)(2) = 0 \\\\\n\\lambda^2 - 7\\lambda + 12 - 4 = 0 \\\\\n\\lambda^2 - 7\\lambda + 8 = 0\n$$\nThe roots are given by the quadratic formula:\n$$\n\\lambda = \\frac{7 \\pm \\sqrt{(-7)^2 - 4(1)(8)}}{2} = \\frac{7 \\pm \\sqrt{49 - 32}}{2} = \\frac{7 \\pm \\sqrt{17}}{2}\n$$\nThe eigenvalues are $\\lambda_1 = \\frac{7+\\sqrt{17}}{2}$, $\\lambda_2 = \\frac{7-\\sqrt{17}}{2}$, and $\\lambda_3=1$.\nNumerically, $\\sqrt{17} \\approx 4.123$, so $\\lambda_1 \\approx 5.56$, $\\lambda_2 \\approx 1.44$, $\\lambda_3=1.00$. The ordering $\\lambda_1 > \\lambda_2 > \\lambda_3$ holds.\n\nThe stable fixed point is the eigenvector $\\boldsymbol{v}_1$ associated with $\\lambda_1$. We are asked to compute the ratio of time constants of convergence for the other modes. The time constant $\\tau_i$ for the decay of the component along eigenvector $\\boldsymbol{v}_i$ during convergence to $\\boldsymbol{v}_1$ is the inverse of the decay rate, $\\lambda_1 - \\lambda_i$.\nThe time constant associated with the second-largest eigenvalue, $\\lambda_2$, is:\n$$\n\\tau_2 = \\frac{1}{\\lambda_1 - \\lambda_2}\n$$\nThe time constant associated with the smallest eigenvalue, $\\lambda_3$, is:\n$$\n\\tau_3 = \\frac{1}{\\lambda_1 - \\lambda_3}\n$$\nThe desired ratio is $\\rho = \\frac{\\tau_2}{\\tau_3}$:\n$$\n\\rho = \\frac{1/(\\lambda_1 - \\lambda_2)}{1/(\\lambda_1 - \\lambda_3)} = \\frac{\\lambda_1 - \\lambda_3}{\\lambda_1 - \\lambda_2}\n$$\nWe now compute the differences in eigenvalues:\n$$\n\\lambda_1 - \\lambda_2 = \\left(\\frac{7+\\sqrt{17}}{2}\\right) - \\left(\\frac{7-\\sqrt{17}}{2}\\right) = \\frac{2\\sqrt{17}}{2} = \\sqrt{17}\n$$\n$$\n\\lambda_1 - \\lambda_3 = \\left(\\frac{7+\\sqrt{17}}{2}\\right) - 1 = \\frac{7+\\sqrt{17}-2}{2} = \\frac{5+\\sqrt{17}}{2}\n$$\nNow, we compute the ratio $\\rho$:\n$$\n\\rho = \\frac{(5+\\sqrt{17})/2}{\\sqrt{17}} = \\frac{5+\\sqrt{17}}{2\\sqrt{17}}\n$$\nTo obtain the numerical value, we use $\\sqrt{17} \\approx 4.1231056$:\n$$\n\\rho \\approx \\frac{5 + 4.1231056}{2 \\times 4.1231056} = \\frac{9.1231056}{8.2462112} \\approx 1.106339\n$$\nRounding to four significant figures, we get $\\rho \\approx 1.106$.",
            "answer": "$$\\boxed{1.106}$$"
        },
        {
            "introduction": "Hierarchical organization is not just an anatomical curiosity; it serves a crucial computational function. This hands-on coding exercise puts the efficient coding hypothesis to the test, exploring how a hierarchical sparse encoder can balance the competing demands of accurate signal representation and minimal metabolic energy. By implementing this model and analyzing the accuracy-energy trade-off, you will gain practical insight into why hierarchical processing is a ubiquitous strategy in both biological and artificial neural systems. ",
            "id": "4055862",
            "problem": "You are given a synthetic dataset, a two-layer hierarchical sparse encoder, and an energy model inspired by the efficient coding principle. The encoder follows a cascade consistent with hierarchical organization: the first layer rotates the input to a coefficient domain and applies sparsity through soft-thresholding, and the second layer does the same to the first layer’s code. The reconstruction is obtained by inverting the transforms in order. Your task is to compute, for a set of sparsity penalty values, the trade-off between accuracy and energy, and then compute the Pareto front of non-dominated solutions.\n\nBegin from the efficient coding principle: sparse codes reduce energy consumption by limiting spiking activity while preserving information. Use the following core definitions and well-tested facts as the foundational base for your derivation and implementation:\n- The Least Absolute Shrinkage and Selection Operator (LASSO) optimization with an orthonormal dictionary $D$ and objective $J(v) = \\frac{1}{2}\\lVert u - D v \\rVert_2^2 + \\lambda \\lVert v \\rVert_1$ has the coordinate-wise closed-form solution $v^\\star = S_\\lambda(D^\\top u)$, where the soft-threshold operator $S_\\lambda$ is defined by $S_\\lambda(z)_i = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\lambda, 0\\}$.\n- Energy in spiking models is approximately proportional to the number of active coefficients (a proxy for spikes), scaled by per-spike energy constants.\n\nHierarchical encoder specification:\n- Dimension of each input vector is $n = 4$.\n- First-layer dictionary $D_1 \\in \\mathbb{R}^{4 \\times 4}$ is the scaled Hadamard matrix\n$$\nD_1 = \\frac{1}{2}\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 \\\\\n1 & 1 & -1 & -1 \\\\\n1 & -1 & -1 & 1\n\\end{bmatrix},\n$$\nwhich is orthonormal (i.e., $D_1^\\top D_1 = I$).\n- Second-layer dictionary $D_2 \\in \\mathbb{R}^{4 \\times 4}$ is block-diagonal with two planar rotations in radians:\n$$\nD_2 = \n\\begin{bmatrix}\n\\cos\\varphi & -\\sin\\varphi & 0 & 0 \\\\\n\\sin\\varphi & \\cos\\varphi & 0 & 0 \\\\\n0 & 0 & \\cos\\psi & -\\sin\\psi \\\\\n0 & 0 & \\sin\\psi & \\cos\\psi\n\\end{bmatrix},\n$$\nwith $\\varphi = \\pi/4$ and $\\psi = \\pi/3$, both in radians. This matrix is orthonormal (i.e., $D_2^\\top D_2 = I$).\n\nEncoder and decoder:\n- For each input $x \\in \\mathbb{R}^4$, the first-layer code is $y_1 = S_\\lambda(D_1^\\top x)$.\n- The second-layer code is $y_2 = S_\\lambda(D_2^\\top y_1)$.\n- The reconstruction is $x_{\\text{hat}} = D_1 D_2 y_2$.\n\nAccuracy:\n- Define the per-input accuracy as $a(x,\\lambda) = 1 - \\frac{\\lVert x - x_{\\text{hat}} \\rVert_2^2}{\\lVert x \\rVert_2^2}$, which lies in $[0,1]$ for $x \\neq 0$.\n- The reported accuracy for a given $\\lambda$ is the average of $a(x,\\lambda)$ over the dataset.\n\nEnergy:\n- Per-spike energy costs are $e_1 = 10$ picojoules and $e_2 = 20$ picojoules for the first and second layers, respectively.\n- For a given input, the energy is $E(x,\\lambda) = e_1 \\lVert y_1 \\rVert_0 + e_2 \\lVert y_2 \\rVert_0$, where $\\lVert \\cdot \\rVert_0$ denotes the count of nonzero entries.\n- The reported energy for a given $\\lambda$ is the average of $E(x,\\lambda)$ over the dataset, expressed in picojoules per input vector.\n\nDataset:\n- The dataset contains $5$ input vectors in $\\mathbb{R}^4$:\n$$\nx_1 = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.5 \\\\ 0.0 \\end{bmatrix},\\quad\nx_2 = \\begin{bmatrix} -0.5 \\\\ 1.5 \\\\ 0.0 \\\\ -1.0 \\end{bmatrix},\\quad\nx_3 = \\begin{bmatrix} 0.25 \\\\ -0.75 \\\\ 0.5 \\\\ 1.25 \\end{bmatrix},\\quad\nx_4 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix},\\quad\nx_5 = \\begin{bmatrix} -1.0 \\\\ -0.5 \\\\ 0.5 \\\\ 0.0 \\end{bmatrix}.\n$$\n\nTrade-off curve:\n- For each sparsity parameter $\\lambda$ in the test suite, compute the average accuracy and average energy. The test suite of $\\lambda$ values is $\\{0.0, 0.25, 0.5, 1.0, 2.5\\}$.\n\nPareto front:\n- A solution $(\\text{accuracy}, \\text{energy})$ dominates $(\\text{accuracy}', \\text{energy}')$ if $\\text{accuracy} \\ge \\text{accuracy}'$ and $\\text{energy} \\le \\text{energy}'$, with at least one strict inequality. The Pareto front is the set of all solutions not dominated by any other.\n- Compute the Pareto front over the set of points obtained from the specified $\\lambda$ values. Report the front sorted by increasing energy.\n\nUnits and angle specification:\n- Energies must be reported in picojoules per input vector.\n- Angles $\\varphi$ and $\\psi$ are in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing two lists enclosed in square brackets. The first list contains the trade-off curve as ordered pairs for each $\\lambda$ in the specified order, and the second list contains the Pareto front ordered pairs sorted by increasing energy. Each ordered pair is a two-element list $[\\text{accuracy}, \\text{energy}]$ with both components rounded to $6$ decimal places. For example, the output should look like\n$$\n\\bigl[ [[a_1,E_1],[a_2,E_2],\\dots], [[a_{p1},E_{p1}],[a_{p2},E_{p2}],\\dots] \\bigr].\n$$\n\nTest suite design:\n- Include $\\lambda = 0.0$ as the no-sparsity boundary that yields exact reconstruction with orthonormal transforms.\n- Include intermediate $\\lambda$ values $\\lambda = 0.25$ and $\\lambda = 0.5$ to test typical sparse regimes.\n- Include $\\lambda = 1.0$ to test stronger sparsity.\n- Include $\\lambda = 2.5$ to test the edge case where all codes are zero and reconstruction is the zero vector.\n- The expected outputs are numerical lists: the trade-off curve and Pareto front, both as lists of floating-point ordered pairs.\n\nYour program must implement the above definitions exactly and produce the output in the specified format in a single line.",
            "solution": "The task is to compute the trade-off between reconstruction accuracy and metabolic energy for a two-layer hierarchical sparse encoder, and then to identify the Pareto-optimal solutions from this trade-off. The solution is derived by simulating the encoder's behavior for a given dataset and a specified set of sparsity penalties.\n\nThe process is structured as follows:\n1.  Define the components of the hierarchical model.\n2.  For each specified sparsity penalty $\\lambda$, simulate the encoding-decoding process for every input vector in the dataset.\n3.  Calculate the average accuracy and average energy for each $\\lambda$.\n4.  Identify the non-dominated solutions (the Pareto front) from the set of computed (accuracy, energy) pairs.\n\n**Step 1: Model Definition**\nThe model consists of two sequential sparse encoding layers. Each layer's operation is based on the LASSO problem, which has a closed-form solution involving a soft-thresholding operator when the dictionary is orthonormal.\n\nThe input signal dimension is $n=4$.\nThe first-layer dictionary $D_1$ is the $4 \\times 4$ orthonormal scaled Hadamard matrix:\n$$\nD_1 = \\frac{1}{2}\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 \\\\\n1 & 1 & -1 & -1 \\\\\n1 & -1 & -1 & 1\n\\end{bmatrix}\n$$\nThe second-layer dictionary $D_2$ is an orthonormal block-diagonal rotation matrix with angles $\\varphi = \\pi/4$ and $\\psi = \\pi/3$:\n$$\nD_2 = \n\\begin{bmatrix}\n\\cos(\\pi/4) & -\\sin(\\pi/4) & 0 & 0 \\\\\n\\sin(\\pi/4) & \\cos(\\pi/4) & 0 & 0 \\\\\n0 & 0 & \\cos(\\pi/3) & -\\sin(\\pi/3) \\\\\n0 & 0 & \\sin(\\pi/3) & \\cos(\\pi/3)\n\\end{bmatrix}\n$$\nThe core operation is the soft-thresholding function $S_\\lambda(z)$, applied element-wise:\n$$\nS_\\lambda(z_i) = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\lambda, 0\\}\n$$\nwhere $\\lambda \\ge 0$ is the sparsity penalty.\n\n**Step 2: Encoding, Decoding, and Metrics Calculation**\nFor each input vector $x$ from the dataset and for each value of $\\lambda$ from the test suite $\\{0.0, 0.25, 0.5, 1.0, 2.5\\}$, we perform the following calculations:\na. **Encoding**: The input $x$ is passed through the two-layer encoder.\nThe first-layer code, $y_1$, is obtained by projecting the input $x$ onto the dictionary $D_1$ and applying soft-thresholding:\n$$\ny_1 = S_\\lambda(D_1^\\top x)\n$$\nThe second-layer code, $y_2$, is obtained by treating $y_1$ as the input and repeating the process with dictionary $D_2$:\n$$\ny_2 = S_\\lambda(D_2^\\top y_1)\n$$\nb. **Decoding**: The reconstruction, $x_{\\text{hat}}$, is generated by sequentially applying the inverse transformations (which are simply the dictionaries themselves, since they are orthonormal) starting from the highest-level code $y_2$:\n$$\nx_{\\text{hat}} = D_1 D_2 y_2\n$$\nc. **Accuracy Calculation**: The accuracy of the reconstruction is quantified as the fraction of variance explained:\n$$\na(x,\\lambda) = 1 - \\frac{\\lVert x - x_{\\text{hat}} \\rVert_2^2}{\\lVert x \\rVert_2^2}\n$$\nwhere $\\lVert \\cdot \\rVert_2$ is the Euclidean norm. An accuracy of $1$ signifies perfect reconstruction.\nd. **Energy Calculation**: The energy cost is modeled as being proportional to the number of non-zero coefficients (spikes) in each layer's code. The total energy is:\n$$\nE(x,\\lambda) = e_1 \\lVert y_1 \\rVert_0 + e_2 \\lVert y_2 \\rVert_0\n$$\nwhere $\\lVert \\cdot \\rVert_0$ is the $L_0$-norm (count of non-zero elements), and the energy costs are $e_1 = 10$ pJ and $e_2 = 20$ pJ.\n\n**Step 3: Generating the Trade-off Curve**\nFor each $\\lambda$ in the test suite, the accuracy $a(x,\\lambda)$ and energy $E(x,\\lambda)$ are calculated for every input vector $x_i$ in the dataset ($i=1, \\dots, 5$). The results are then averaged over the dataset to yield a single representative pair of (average accuracy, average energy) for that $\\lambda$:\n$$\n\\text{Accuracy}(\\lambda) = \\frac{1}{5} \\sum_{i=1}^{5} a(x_i, \\lambda)\n$$\n$$\n\\text{Energy}(\\lambda) = \\frac{1}{5} \\sum_{i=1}^{5} E(x_i, \\lambda)\n$$\nThe collection of these pairs, ordered according to the provided sequence of $\\lambda$ values, forms the trade-off curve.\n\n**Step 4: Computing the Pareto Front**\nThe Pareto front consists of all solutions that are not dominated by any other solution. A solution $(A, E)$ dominates another solution $(A', E')$ if $A \\ge A'$ and $E \\le E'$, with at least one inequality being strict. In our context, this means a solution is superior if it offers at least as good accuracy for at most the same energy cost, and is strictly better on at least one of these metrics.\nTo find the front, each computed (accuracy, energy) pair is compared against all others. If a pair is found to be dominated by another, it is excluded from the front. The final set of non-dominated points constitutes the Pareto front. These points are then sorted by increasing energy, as required. The implementation will systematically check for dominance among the calculated trade-off points. Finally, all numerical results are formatted to $6$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical sparse coding problem.\n    \"\"\"\n    # Step 1: Define constants, dictionaries, and dataset\n    n = 4\n    \n    # First-layer dictionary D1 (scaled Hadamard matrix)\n    D1 = 0.5 * np.array([\n        [1, 1, 1, 1],\n        [1, -1, 1, -1],\n        [1, 1, -1, -1],\n        [1, -1, -1, 1]\n    ], dtype=float)\n\n    # Second-layer dictionary D2 (block-diagonal rotation matrix)\n    phi = np.pi / 4\n    psi = np.pi / 3\n    D2 = np.array([\n        [np.cos(phi), -np.sin(phi), 0, 0],\n        [np.sin(phi), np.cos(phi), 0, 0],\n        [0, 0, np.cos(psi), -np.sin(psi)],\n        [0, 0, np.sin(psi), np.cos(psi)]\n    ], dtype=float)\n\n    # Inverses are transposes since dictionaries are orthonormal\n    D1_T = D1.T\n    D2_T = D2.T\n\n    # Energy constants\n    e1 = 10.0  # pJ\n    e2 = 20.0  # pJ\n\n    # Dataset\n    dataset = [\n        np.array([1.0, 0.5, -0.5, 0.0]),\n        np.array([-0.5, 1.5, 0.0, -1.0]),\n        np.array([0.25, -0.75, 0.5, 1.25]),\n        np.array([1.0, 1.0, 1.0, 1.0]),\n        np.array([-1.0, -0.5, 0.5, 0.0])\n    ]\n    num_samples = len(dataset)\n\n    # Sparsity penalty test suite\n    lambda_suite = [0.0, 0.25, 0.5, 1.0, 2.5]\n    \n    # Soft-thresholding operator\n    def soft_threshold(z, lambd):\n        return np.sign(z) * np.maximum(np.abs(z) - lambd, 0)\n\n    # Step 2 & 3: Compute the trade-off curve\n    tradeoff_curve_points = []\n    \n    for lambd in lambda_suite:\n        total_accuracy = 0.0\n        total_energy = 0.0\n        \n        for x in dataset:\n            # Encoding\n            y1 = soft_threshold(D1_T @ x, lambd)\n            y2 = soft_threshold(D2_T @ y1, lambd)\n            \n            # Decoding\n            x_hat = D1 @ D2 @ y2\n            \n            # Accuracy\n            norm_x_sq = np.sum(x**2)\n            if norm_x_sq == 0:\n                # Should not happen with the given data\n                accuracy = 1.0\n            else:\n                error_norm_sq = np.sum((x - x_hat)**2)\n                accuracy = 1.0 - error_norm_sq / norm_x_sq\n            \n            # Energy\n            l0_y1 = np.count_nonzero(y1)\n            l0_y2 = np.count_nonzero(y2)\n            energy = e1 * l0_y1 + e2 * l0_y2\n            \n            total_accuracy += accuracy\n            total_energy += energy\n            \n        avg_accuracy = total_accuracy / num_samples\n        avg_energy = total_energy / num_samples\n        \n        tradeoff_curve_points.append([avg_accuracy, avg_energy])\n\n    # Step 4: Compute the Pareto front\n    pareto_front = []\n    for i, p1 in enumerate(tradeoff_curve_points):\n        is_dominated = False\n        for j, p2 in enumerate(tradeoff_curve_points):\n            if i == j:\n                continue\n            # p2 dominates p1 if acc2 >= acc1 and ene2 <= ene1 (with one strict)\n            if (p2[0] >= p1[0] and p2[1] <= p1[1]) and \\\n               (p2[0] > p1[0] or p2[1] < p1[1]):\n                is_dominated = True\n                break\n        if not is_dominated:\n            pareto_front.append(p1)\n\n    # Sort Pareto front by increasing energy\n    pareto_front.sort(key=lambda p: p[1])\n    \n    # Step 5: Format the output\n    tradeoff_curve_rounded = [[round(p[0], 6), round(p[1], 6)] for p in tradeoff_curve_points]\n    pareto_front_rounded = [[round(p[0], 6), round(p[1], 6)] for p in pareto_front]\n    \n    # This manual string construction ensures there are no spaces after commas.\n    tradeoff_str = f\"[{','.join(str(p).replace(' ', '') for p in tradeoff_curve_rounded)}]\"\n    pareto_str = f\"[{','.join(str(p).replace(' ', '') for p in pareto_front_rounded)}]\"\n\n    print(f\"[{tradeoff_str},{pareto_str}]\")\n\nsolve()\n```"
        }
    ]
}