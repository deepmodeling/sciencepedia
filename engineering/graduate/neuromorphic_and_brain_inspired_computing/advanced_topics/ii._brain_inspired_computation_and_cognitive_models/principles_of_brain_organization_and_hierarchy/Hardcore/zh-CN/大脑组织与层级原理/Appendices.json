{
    "hands_on_practices": [
        {
            "introduction": "大脑皮层区域间的层级结构是其组织的一个基本原则，但这不仅仅是一个抽象概念，而是可以通过解剖学数据进行量化的。一个被广泛接受的模型利用了大脑皮层不同区域间连接的层状模式：前馈连接（从低层级到高层级）主要起源于浅层（supragranular layers），而反馈连接（从高层级到低层级）主要起源于深层（infragranular layers）。本练习将指导你应用这一定量原则，通过分析给定的层状连接数据，并使用最小二乘法估计出一个三区域网络的层级值，从而将抽象的层级概念转化为具体的计算实践 。",
            "id": "4055874",
            "problem": "考虑一个由三个新皮质区域（标记为 $A$、$B$ 和 $C$）组成的网络。对于每对有序区域 $i \\to j$，测量了两个计数：从区域 $i$ 投射到区域 $j$ 的上颗粒层起源神经元（II/III层）数量 $S_{ij}$ 和下颗粒层起源神经元（V/VI层）数量 $I_{ij}$。投射 $i \\to j$ 的上颗粒层-下颗粒层层状指数定义为\n$$\nr_{ij} \\equiv \\frac{S_{ij} - I_{ij}}{S_{ij} + I_{ij}},\n$$\n其中 $-1  r_{ij}  1$。在已建立的皮质层级层状模型中，一个 $r_{ij} > 0$ 的投射是层状前馈（主要起源于上颗粒层），并被认为是从一个较低层级区域到一个较高层级区域的投射；而一个 $r_{ij}  0$ 的投射是层状反馈，从一个较高层级区域到一个较低层级区域。此外，一个经过充分检验的生成模型将 $r_{ij}$ 与区域间的层级距离单调相关联，对于中小型距离，可以线性化这种关系，使得\n$$\nr_{ij} \\approx h_{j} - h_{i},\n$$\n其中 $h_{i}$ 是区域 $i$ 的（未知）连续层级值。层级值的定义允许相差一个任意的加法常数；为了固定规范，施加约束 $\\sum_{i \\in \\{A,B,C\\}} h_{i} = 0$。\n\n给定以下层状连接矩阵（对角线元素为零）：\n$$\nS \\;=\\; \\begin{pmatrix}\n0  3  11 \\\\\n1  0  2 \\\\\n1  1  0\n\\end{pmatrix},\n\\qquad\nI \\;=\\; \\begin{pmatrix}\n0  1  1 \\\\\n3  0  1 \\\\\n11  2  0\n\\end{pmatrix}.\n$$\n\n任务：\n- 计算所有 $i \\neq j$ 的 $r_{ij}$。\n- 使用线性化模型 $r_{ij} \\approx h_{j} - h_{i}$，通过求解最小化 $\\sum_{i \\neq j} \\big(r_{ij} - (h_{j} - h_{i})\\big)^{2}$ 且满足约束 $h_{A} + h_{B} + h_{C} = 0$ 的最小二乘问题，来估计 $(h_{A}, h_{B}, h_{C})$。\n- 通过构建 $\\{A, B, C\\}$ 上的有向图来验证推断的排序，其中只要 $r_{ij}  0$ 就存在一条边 $i \\to j$。然后对该图进行拓扑排序，以确认其顺序与 $(h_{A}, h_{B}, h_{C})$ 的升序值一致。\n\n以精确形式报告最终的层级值，形式为行向量 $\\big(h_{A}, h_{B}, h_{C}\\big)$。不要四舍五入。最终答案必须使用 $\\begin{pmatrix}$ 符号以单个行向量的形式给出。不需要单位。",
            "solution": "该问题要求根据神经元投射计数确定三个新皮质区域的层级值 $(h_A, h_B, h_C)$，并随后验证结果。该过程涉及三个主要任务：计算每个投射的层状指数，求解一个带约束的最小二乘问题以估计层级值，以及使用基于图的拓扑排序来验证排序。\n\n该问题在科学上基于已建立的皮质组织模型，作为一个标准的优化问题是适定的，并且用精确的数据进行了客观的表述。因此，该问题是有效的，并将推导出一个解。\n\n设区域索引为 $1$ 代表 $A$，$2$ 代表 $B$，$3$ 代表 $C$。给定的上颗粒层 ($S$) 和下颗粒层 ($I$) 投射神经元计数数据为：\n$$\nS \\;=\\; \\begin{pmatrix}\n0  3  11 \\\\\n1  0  2 \\\\\n1  1  0\n\\end{pmatrix},\n\\qquad\nI \\;=\\; \\begin{pmatrix}\n0  1  1 \\\\\n3  0  1 \\\\\n11  2  0\n\\end{pmatrix}\n$$\n这些矩阵表示 $S_{ij}$ 和 $I_{ij}$，其中 $i, j \\in \\{A, B, C\\}$。\n\n任务1：计算所有 $i \\neq j$ 的 $r_{ij}$。\n上颗粒层-下颗粒层层状指数 $r_{ij}$ 定义为 $r_{ij} = \\frac{S_{ij} - I_{ij}}{S_{ij} + I_{ij}}$。我们计算矩阵 $R = (r_{ij})$ 的非对角元素：\n\n$r_{AB} = r_{12} = \\frac{S_{12} - I_{12}}{S_{12} + I_{12}} = \\frac{3 - 1}{3 + 1} = \\frac{2}{4} = \\frac{1}{2}$\n\n$r_{AC} = r_{13} = \\frac{S_{13} - I_{13}}{S_{13} + I_{13}} = \\frac{11 - 1}{11 + 1} = \\frac{10}{12} = \\frac{5}{6}$\n\n$r_{BA} = r_{21} = \\frac{S_{21} - I_{21}}{S_{21} + I_{21}} = \\frac{1 - 3}{1 + 3} = \\frac{-2}{4} = -\\frac{1}{2}$\n\n$r_{BC} = r_{23} = \\frac{S_{23} - I_{23}}{S_{23} + I_{23}} = \\frac{2 - 1}{2 + 1} = \\frac{1}{3}$\n\n$r_{CA} = r_{31} = \\frac{S_{31} - I_{31}}{S_{31} + I_{31}} = \\frac{1 - 11}{1 + 11} = \\frac{-10}{12} = -\\frac{5}{6}$\n\n$r_{CB} = r_{32} = \\frac{S_{32} - I_{32}}{S_{32} + I_{32}} = \\frac{1 - 2}{1 + 2} = \\frac{-1}{3}$\n\n得到的层状指数矩阵为：\n$$\nR \\;=\\; \\begin{pmatrix}\n0  \\frac{1}{2}  \\frac{5}{6} \\\\\n-\\frac{1}{2}  0  \\frac{1}{3} \\\\\n-\\frac{5}{6}  -\\frac{1}{3}  0\n\\end{pmatrix}\n$$\n我们观察到 $r_{ji} = -r_{ij}$，这与给定数据中 $S = I^T$ 的情况是一致的。\n\n任务2：使用最小二乘拟合估计 $(h_A, h_B, h_C)$。\n我们需要找到 $h_A, h_B, h_C$ 的值，以最小化测量值 $r_{ij}$ 与模型 $h_j - h_i$ 之间的平方误差和。目标函数为：\n$$\nE(h_A, h_B, h_C) = \\sum_{i \\neq j} \\big(r_{ij} - (h_{j} - h_{i})\\big)^{2}\n$$\n约束条件为 $h_A + h_B + h_C = 0$。\n\n由于 $r_{ji} = -r_{ij}$ 并且 $(h_i - h_j) = -(h_j - h_i)$，求和式中 $(j,i)$ 项与 $(i,j)$ 项相同：\n$$\n(r_{ji} - (h_i - h_j))^2 = (-r_{ij} - (-(h_j - h_i)))^2 = (r_{ij} - (h_j-h_i))^2\n$$\n因此，求和式可以写成：\n$$\nE = 2 \\left[ (r_{AB} - (h_B - h_A))^2 + (r_{AC} - (h_C - h_A))^2 + (r_{BC} - (h_C - h_B))^2 \\right]\n$$\n最小化 $E$ 等价于最小化更简单的函数 $E'$：\n$$\nE'(h_A, h_B, h_C) = (r_{AB} - h_B + h_A)^2 + (r_{AC} - h_C + h_A)^2 + (r_{BC} - h_C + h_B)^2\n$$\n为了解决这个带约束的优化问题，我们使用拉格朗日乘数法。拉格朗日函数为：\n$$\n\\mathcal{L}(h_A, h_B, h_C, \\lambda) = E'(h_A, h_B, h_C) + \\lambda (h_A + h_B + h_C)\n$$\n我们将关于每个 $h_i$ 的偏导数设为零：\n$\\frac{\\partial \\mathcal{L}}{\\partial h_A} = 2(r_{AB} - h_B + h_A) + 2(r_{AC} - h_C + h_A) + \\lambda = 0 \\implies 2h_A - h_B - h_C = -r_{AB} - r_{AC} - \\lambda/2$\n$\\frac{\\partial \\mathcal{L}}{\\partial h_B} = -2(r_{AB} - h_B + h_A) + 2(r_{BC} - h_C + h_B) + \\lambda = 0 \\implies -h_A + 2h_B - h_C = r_{AB} - r_{BC} - \\lambda/2$\n$\\frac{\\partial \\mathcal{L}}{\\partial h_C} = -2(r_{AC} - h_C + h_A) - 2(r_{BC} - h_C + h_B) + \\lambda = 0 \\implies -h_A - h_B + 2h_C = r_{AC} + r_{BC} - \\lambda/2$\n\n将这三个方程相加得到 $0 = 0 - 3\\lambda/2$，这意味着 $\\lambda=0$。该系统简化为：\n$1) \\quad 2h_A - h_B - h_C = -r_{AB} - r_{AC}$\n$2) \\quad -h_A + 2h_B - h_C = r_{AB} - r_{BC}$\n$3) \\quad -h_A - h_B + 2h_C = r_{AC} + r_{BC}$\n我们将此系统与约束条件 $4) \\quad h_A + h_B + h_C = 0$ 一起求解。\n\n根据约束条件，我们有 $h_B + h_C = -h_A$。将其代入方程(1)：\n$2h_A - (-h_A) = -r_{AB} - r_{AC} \\implies 3h_A = -r_{AB} - r_{AC} \\implies h_A = -\\frac{1}{3}(r_{AB} + r_{AC})$\n\n同样，根据约束条件，$h_A + h_C = -h_B$。将其代入方程(2)：\n$2h_B - (h_A + h_C) = r_{AB} - r_{BC} \\implies 2h_B - (-h_B) = r_{AB} - r_{BC} \\implies 3h_B = r_{AB} - r_{BC} \\implies h_B = \\frac{1}{3}(r_{AB} - r_{BC})$\n\n最后，根据约束条件，我们求得 $h_C$：\n$h_C = -h_A - h_B = \\frac{1}{3}(r_{AB} + r_{AC}) - \\frac{1}{3}(r_{AB} - r_{BC}) = \\frac{1}{3}(r_{AC} + r_{BC})$\n\n现在，我们代入 $r_{ij}$ 的数值：\n$h_A = -\\frac{1}{3} \\left( \\frac{1}{2} + \\frac{5}{6} \\right) = -\\frac{1}{3} \\left( \\frac{3}{6} + \\frac{5}{6} \\right) = -\\frac{1}{3} \\left( \\frac{8}{6} \\right) = -\\frac{1}{3} \\left( \\frac{4}{3} \\right) = -\\frac{4}{9}$\n\n$h_B = \\frac{1}{3} \\left( \\frac{1}{2} - \\frac{1}{3} \\right) = \\frac{1}{3} \\left( \\frac{3}{6} - \\frac{2}{6} \\right) = \\frac{1}{3} \\left( \\frac{1}{6} \\right) = \\frac{1}{18}$\n\n$h_C = \\frac{1}{3} \\left( \\frac{5}{6} + \\frac{1}{3} \\right) = \\frac{1}{3} \\left( \\frac{5}{6} + \\frac{2}{6} \\right) = \\frac{1}{3} \\left( \\frac{7}{6} \\right) = \\frac{7}{18}$\n\n估计的层级值为 $(h_A, h_B, h_C) = (-\\frac{4}{9}, \\frac{1}{18}, \\frac{7}{18})$。\n我们来检查约束条件：$h_A + h_B + h_C = -\\frac{4}{9} + \\frac{1}{18} + \\frac{7}{18} = -\\frac{8}{18} + \\frac{8}{18} = 0$。约束条件得到满足。\n\n任务3：验证推断的排序。\n我们构建一个有向图，其中如果 $r_{ij}  0$，则存在一条边 $i \\to j$。\n- $r_{AB} = \\frac{1}{2}  0 \\implies$ 边 $A \\to B$。\n- $r_{AC} = \\frac{5}{6}  0 \\implies$ 边 $A \\to C$。\n- $r_{BC} = \\frac{1}{3}  0 \\implies$ 边 $B \\to C$。\n其他的 $r_{ij}$ 值为负，因此没有其他边。该图有顶点 $\\{A, B, C\\}$ 和边 $\\{(A,B), (A,C), (B,C)\\}$。\n\n为了进行拓扑排序，我们使用 Kahn 算法。我们找到入度为 $0$ 的节点。\n- $A$ 的入度：$0$。\n- $B$ 的入度：$1$ (来自 $A$)。\n- $C$ 的入度：$2$ (来自 $A$ 和 $B$)。\n唯一入度为 $0$ 的节点是 $A$。所以它在排序中是第一个。我们从图中移除 $A$ 及其出边。剩下的图有顶点 $\\{B, C\\}$ 和一条边 $(B, C)$。\n新图中的入度是：\n- $B$ 的入度：$0$。\n- $C$ 的入度：$1$ (来自 $B$)。\n唯一入度为 $0$ 的节点是 $B$。所以它是下一个。移除 $B$ 后只剩下 $C$。\n该图的唯一拓扑排序是序列 $A, B, C$。\n\n这个顺序应该与层级水平的升序值一致。计算出的值为：\n$h_A = -\\frac{4}{9} = -\\frac{8}{18} \\approx -0.444$\n$h_B = \\frac{1}{18} \\approx 0.056$\n$h_C = \\frac{7}{18} \\approx 0.389$\n这些值的顺序是 $h_A  h_B  h_C$。这对应于层级 $A \\to B \\to C$。\n拓扑排序 $A, B, C$ 与从最小二乘估计推断出的层级排序完全一致。验证成功。\n\n最终答案是估计的层级值的行向量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} -\\frac{4}{9}  \\frac{1}{18}  \\frac{7}{18} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在了解了如何从宏观解剖结构中确定大脑层级之后，一个自然的问题是这种有序的结构是如何通过学习形成的。神经科学中的一个核心思想是，简单的局部学习规则能够导致全局有序的特征表示的涌现。本练习将探讨Oja规则，这是一种归一化的赫布学习（Hebbian learning），它揭示了单个神经元如何能够自主地从其输入中提取主成分，即数据方差最大的方向 。通过分析这个学习规则的动态特性，你将理解突触可塑性这一微观机制如何与主成分分析（PCA）这一宏观计算联系起来，并为层级式特征提取的自组织过程提供一个基本模型。",
            "id": "4055805",
            "problem": "考虑一个线性神经元，其接收的输入 $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ 是从一个协方差矩阵为 $C = \\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{\\top}]$ 的平稳分布中独立同分布抽取的零均值样本。该神经元的输出为 $y = \\boldsymbol{w}^{\\top}\\boldsymbol{x}$，其中 $\\boldsymbol{w} \\in \\mathbb{R}^{n}$ 是突触权重向量。突触根据 Oja 法则进行更新，这是一种归一化的赫布学习形式：$\\Delta \\boldsymbol{w} = \\eta\\, y\\, \\boldsymbol{x} - \\eta\\, y^{2}\\, \\boldsymbol{w}$，其中 $\\eta  0$ 是一个小的恒定学习率。主成分分析 (PCA) 是指提取使投影数据方差最大化的正交方向，这些方向按协方差矩阵 $C$ 的特征值排序。\n\n从协方差的定义和给定的学习法则出发，推导期望的连续时间权重动态过程，并将其表示为 $C$ 和 $\\boldsymbol{w}$ 的函数。根据 $C$ 的特征分解，解释该动力学系统的不动点及其稳定性，并由此将赫布学习的收敛性与主成分联系起来。利用这种关系，通过模式特定的收敛速率，将有序特征层级的涌现与特征值谱联系起来。\n\n现在考虑 $n = 3$ 的特殊情况，其中\n$$\nC = \\begin{pmatrix}\n4  2  0 \\\\\n2  3  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n令 $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3}  0$ 表示 $C$ 的特征值。在与主特征向量对齐的稳定不动点附近对 Oja 动态过程进行线性化，并将沿与 $\\lambda_i$ 相关联的特征方向收敛的模式特定时间常数 $\\tau_i$ 定义为相应线性化衰减率的倒数。对于给定的 $C$，计算比率\n$$\n\\rho \\equiv \\frac{\\tau_{\\text{second-largest}}}{\\tau_{\\text{smallest}}} = \\frac{\\tau_{2}}{\\tau_{3}}\n$$\n并给出其数值，四舍五入到四位有效数字。最终答案应表示为无单位的无量纲数。",
            "solution": "该问题要求推导 Oja 学习法则的连续时间动态过程，分析其不动点和稳定性，并对给定的协方差矩阵 $C$ 进行具体计算。\n\n首先，我们验证问题陈述的有效性。\n已知条件如下：\n- 输入向量：$\\boldsymbol{x} \\in \\mathbb{R}^{n}$，零均值，独立同分布 (i.i.d.)。\n- 协方差矩阵：$C = \\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{\\top}]$\n- 神经元输出：$y = \\boldsymbol{w}^{\\top}\\boldsymbol{x}$\n- Oja 法则：$\\Delta \\boldsymbol{w} = \\eta\\, y\\, \\boldsymbol{x} - \\eta\\, y^{2}\\, \\boldsymbol{w}$，其中学习率 $\\eta  0$ 为小常数。\n- 对于特殊情况，$n=3$ 且 $C = \\begin{pmatrix} 4  2  0 \\\\ 2  3  0 \\\\ 0  0  1 \\end{pmatrix}$。\n- 特征值排序为 $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3}  0$。\n- 时间常数 $\\tau_i$ 定义为线性化衰减率的倒数。\n- 目标是计算 $\\rho = \\frac{\\tau_2}{\\tau_3}$，并四舍五入到四位有效数字。\n\n该问题在科学上植根于神经网络和赫布学习理论。Oja 法则及其与 PCA 的联系是计算神经科学中的经典结果。问题是良定的，为求得唯一解提供了所有必要信息。语言客观，数学设定一致。因此，该问题被认为是有效的。\n\n我们首先推导连续时间动态过程。对于小的学习率 $\\eta$，离散更新可以在输入分布上进行平均以求得期望变化，这近似于一个连续流。我们定义一个连续时间变量 $t$，它与更新次数成比例，使得 $\\frac{d\\boldsymbol{w}}{dt}$ 与期望更新 $\\mathbb{E}[\\Delta\\boldsymbol{w}]$ 成正比。按照惯例，我们将学习率吸收到时间尺度中，因此 $\\frac{d\\boldsymbol{w}}{dt} = \\mathbb{E}[\\frac{\\Delta\\boldsymbol{w}}{\\eta}]$。\n$$\n\\frac{d\\boldsymbol{w}}{dt} = \\mathbb{E}[y\\, \\boldsymbol{x} - y^{2}\\, \\boldsymbol{w}] = \\mathbb{E}[y\\, \\boldsymbol{x}] - \\mathbb{E}[y^{2}]\\,\\boldsymbol{w}\n$$\n我们计算这两个期望项。\n第一项是赫布项：\n$$\n\\mathbb{E}[y\\, \\boldsymbol{x}] = \\mathbb{E}[(\\boldsymbol{w}^{\\top}\\boldsymbol{x})\\, \\boldsymbol{x}] = \\mathbb{E}[\\boldsymbol{x}(\\boldsymbol{x}^{\\top}\\boldsymbol{w})] = \\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{\\top}]\\boldsymbol{w} = C\\boldsymbol{w}\n$$\n第二项是归一化项或衰减项：\n$$\n\\mathbb{E}[y^{2}] = \\mathbb{E}[(\\boldsymbol{w}^{\\top}\\boldsymbol{x})^{2}] = \\mathbb{E}[(\\boldsymbol{w}^{\\top}\\boldsymbol{x})(\\boldsymbol{x}^{\\top}\\boldsymbol{w})] = \\boldsymbol{w}^{\\top}\\mathbb{E}[\\boldsymbol{x}\\boldsymbol{x}^{\\top}]\\boldsymbol{w} = \\boldsymbol{w}^{\\top}C\\boldsymbol{w}\n$$\n将这些代回到微分方程中，得到权重向量 $\\boldsymbol{w}$ 的连续时间动态过程：\n$$\n\\frac{d\\boldsymbol{w}}{dt} = C\\boldsymbol{w} - (\\boldsymbol{w}^{\\top}C\\boldsymbol{w})\\boldsymbol{w}\n$$\n接下来，我们通过设置 $\\frac{d\\boldsymbol{w}}{dt} = 0$ 来寻找该系统的不动点 $\\boldsymbol{w}^*$：\n$$\nC\\boldsymbol{w}^* = (\\boldsymbol{w}^{*\\top}C\\boldsymbol{w}^*)\\boldsymbol{w}^*\n$$\n该方程具有 $C\\boldsymbol{v} = \\alpha\\boldsymbol{v}$ 的形式，这是特征向量的定义。因此，任何不动点 $\\boldsymbol{w}^*$ 都必须是协方差矩阵 $C$ 的一个特征向量。设 $\\boldsymbol{v}_i$ 是 $C$ 的一个特征向量，其特征值为 $\\lambda_i$，因此 $C\\boldsymbol{v}_i = \\lambda_i \\boldsymbol{v}_i$。如果我们设 $\\boldsymbol{w}^* = c\\boldsymbol{v}_i$（其中 $c$ 为某个标量），则方程变为：\n$$\nC(c\\boldsymbol{v}_i) = ((c\\boldsymbol{v}_i)^{\\top}C(c\\boldsymbol{v}_i))(c\\boldsymbol{v}_i) \\implies c\\lambda_i\\boldsymbol{v}_i = (c^2 \\boldsymbol{v}_i^{\\top}\\lambda_i\\boldsymbol{v}_i)(c\\boldsymbol{v}_i) = c^3 \\lambda_i (\\boldsymbol{v}_i^{\\top}\\boldsymbol{v}_i)\\boldsymbol{v}_i\n$$\n假设 $\\lambda_i \\neq 0$ 且 $\\boldsymbol{v}_i \\neq \\boldsymbol{0}$，我们得到 $c = c^3 \\lambda_i ||\\boldsymbol{v}_i||^2$。一个解是 $c=0$，因此 $\\boldsymbol{w}^* = \\boldsymbol{0}$ 是一个平凡不动点。如果 $c \\neq 0$，则 $1 = c^2 \\lambda_i ||\\boldsymbol{v}_i||^2$。如果我们使用归一化的特征向量使得 $||\\boldsymbol{v}_i|| = 1$，那么 $c^2 = 1/\\lambda_i$，所以 $c = \\pm 1/\\sqrt{\\lambda_i}$。分析单位向量的动态过程更为方便。Oja 法则会驱使 $||\\boldsymbol{w}||$ 趋向于 1。我们假设不动点是单位向量。如果 $||\\boldsymbol{w}^*||=1$，那么 $\\boldsymbol{w}^*$ 必须是 $C$ 的一个单位特征向量 $\\boldsymbol{v}_i$。不动点方程变为 $\\lambda_i\\boldsymbol{v}_i = (\\boldsymbol{v}_i^{\\top}\\lambda_i\\boldsymbol{v}_i)\\boldsymbol{v}_i = \\lambda_i(\\boldsymbol{v}_i^{\\top}\\boldsymbol{v}_i)\\boldsymbol{v}_i = \\lambda_i\\boldsymbol{v}_i$，这是满足的。所以，不动点是 $C$ 的单位特征向量 $\\boldsymbol{v}_i$。\n\n为了分析稳定性，我们考虑在不动点 $\\boldsymbol{w}^* = \\boldsymbol{v}_i$（其中 $\\boldsymbol{v}_i$ 是特征值为 $\\lambda_i$ 的单位特征向量）附近的一个小扰动 $\\boldsymbol{\\epsilon}(t)$。令 $\\boldsymbol{w}(t) = \\boldsymbol{v}_i + \\boldsymbol{\\epsilon}(t)$。$\\boldsymbol{\\epsilon}$ 的线性化动态过程为：\n$$\n\\frac{d\\boldsymbol{\\epsilon}}{dt} \\approx (C - \\lambda_i I)\\boldsymbol{\\epsilon} - 2\\lambda_i(\\boldsymbol{v}_i^{\\top}\\boldsymbol{\\epsilon})\\boldsymbol{v}_i\n$$\n由于 $C$ 是对称矩阵，其特征向量 $\\{\\boldsymbol{v}_j\\}$ 构成一个标准正交基。我们可以在该基下表示扰动 $\\boldsymbol{\\epsilon}$：$\\boldsymbol{\\epsilon}(t) = \\sum_{j=1}^{n} c_j(t) \\boldsymbol{v}_j$。将此代入线性化方程，并与 $\\boldsymbol{v}_k$ 做点积，可以得到分量 $c_k$ 的动态过程：\n对于 $k=i$：$\\frac{dc_i}{dt} = -2\\lambda_i c_i$。该分量会衰减，这对应于向量范数的恢复。\n对于 $k \\neq i$：$\\frac{dc_k}{dt} = (\\lambda_k - \\lambda_i) c_k$。\n为了使不动点 $\\boldsymbol{v}_i$ 稳定，所有扰动都必须衰减到零。这要求对于所有 $k \\neq i$，系数 $(\\lambda_k-\\lambda_i)$ 都为负。这个条件，即对于所有 $k \\neq i$ 都有 $\\lambda_k  \\lambda_i$，只有当 $\\lambda_i$ 是最大特征值时才满足，我们将其记为 $\\lambda_1$。因此，唯一的稳定不动点是 $\\boldsymbol{v}_1$，即对应于最大特征值的单位特征向量。所有其他特征向量不动点都是不稳定的（鞍点），原点也是不稳定的。\n\n这直接将 Oja 法则与 PCA 联系起来。权重向量 $\\boldsymbol{w}$ 收敛到输入数据的第一主成分，也就是最大方差的方向。收敛速率由特征值谱决定。权重向量沿着任何其他特征方向 $\\boldsymbol{v}_k$（其中 $k1$）的分量以 $\\lambda_1 - \\lambda_k  0$ 的速率指数衰减。此衰减的时间常数为 $\\tau_k = 1/(\\lambda_1-\\lambda_k)$。对于特征值 $\\lambda_k$ 接近 $\\lambda_1$ 的模式，收敛会更慢。这解释了学习层级如何涌现：最显著的特征（方差最大）首先被学习，而学习后续特征的速度则由其对应方差（特征值）的间隔所决定。\n\n现在我们考虑 $n=3$ 和给定矩阵 $C = \\begin{pmatrix} 4  2  0 \\\\ 2  3  0 \\\\ 0  0  1 \\end{pmatrix}$ 的特殊情况。\n该矩阵是块对角的，所以可以立即看出一个特征值是 $\\lambda_3=1$，其特征向量为 $\\begin{pmatrix} 0  0  1 \\end{pmatrix}^{\\top}$。另外两个特征值来自子矩阵 $C' = \\begin{pmatrix} 4  2 \\\\ 2  3 \\end{pmatrix}$。其特征方程为 $\\det(C' - \\lambda I)=0$：\n$$\n(4-\\lambda)(3-\\lambda) - (2)(2) = 0 \\\\\n\\lambda^2 - 7\\lambda + 12 - 4 = 0 \\\\\n\\lambda^2 - 7\\lambda + 8 = 0\n$$\n根由二次方程求根公式给出：\n$$\n\\lambda = \\frac{7 \\pm \\sqrt{(-7)^2 - 4(1)(8)}}{2} = \\frac{7 \\pm \\sqrt{49 - 32}}{2} = \\frac{7 \\pm \\sqrt{17}}{2}\n$$\n特征值为 $\\lambda_1 = \\frac{7+\\sqrt{17}}{2}$，$\\lambda_2 = \\frac{7-\\sqrt{17}}{2}$ 和 $\\lambda_3=1$。数值上，$\\sqrt{17} \\approx 4.123$，所以 $\\lambda_1 \\approx 5.56$，$\\lambda_2 \\approx 1.44$，$\\lambda_3=1.00$。排序 $\\lambda_1 > \\lambda_2 > \\lambda_3$ 成立。\n\n稳定不动点是与 $\\lambda_1$ 相关联的特征向量 $\\boldsymbol{v}_1$。我们需要计算其他模式收敛时间常数的比率。在向 $\\boldsymbol{v}_1$ 收敛过程中，沿特征向量 $\\boldsymbol{v}_i$ 的分量衰减的时间常数 $\\tau_i$ 是衰减率 $\\lambda_1 - \\lambda_i$ 的倒数。\n与第二大特征值 $\\lambda_2$ 相关的时间常数为：\n$$\n\\tau_2 = \\frac{1}{\\lambda_1 - \\lambda_2}\n$$\n与最小特征值 $\\lambda_3$ 相关的时间常数为：\n$$\n\\tau_3 = \\frac{1}{\\lambda_1 - \\lambda_3}\n$$\n所求比率为 $\\rho = \\frac{\\tau_2}{\\tau_3}$：\n$$\n\\rho = \\frac{1/(\\lambda_1 - \\lambda_2)}{1/(\\lambda_1 - \\lambda_3)} = \\frac{\\lambda_1 - \\lambda_3}{\\lambda_1 - \\lambda_2}\n$$\n现在我们计算特征值之差：\n$$\n\\lambda_1 - \\lambda_2 = \\left(\\frac{7+\\sqrt{17}}{2}\\right) - \\left(\\frac{7-\\sqrt{17}}{2}\\right) = \\frac{2\\sqrt{17}}{2} = \\sqrt{17}\n$$\n$$\n\\lambda_1 - \\lambda_3 = \\left(\\frac{7+\\sqrt{17}}{2}\\right) - 1 = \\frac{7+\\sqrt{17}-2}{2} = \\frac{5+\\sqrt{17}}{2}\n$$\n现在，我们计算比率 $\\rho$：\n$$\n\\rho = \\frac{(5+\\sqrt{17})/2}{\\sqrt{17}} = \\frac{5+\\sqrt{17}}{2\\sqrt{17}}\n$$\n为了获得数值，我们使用 $\\sqrt{17} \\approx 4.1231056$：\n$$\n\\rho \\approx \\frac{5 + 4.1231056}{2 \\times 4.1231056} = \\frac{9.1231056}{8.2462112} \\approx 1.106339\n$$\n四舍五入到四位有效数字，我们得到 $\\rho \\approx 1.106$。",
            "answer": "$$\\boxed{1.106}$$"
        },
        {
            "introduction": "我们已经探讨了层级结构的测量方法和一种可能的学习机制，现在让我们构建并评估一个功能性的层级模型。大脑的组织结构被认为遵循着如“高效编码假说”等优化原则，该假说假设系统在信息保真度（准确性）和代谢能量消耗之间进行权衡。本练习将理论付诸实践，要求你实现一个两层层级稀疏编码器，这是一种体现了高效编码思想的强大计算模型 。你将通过编程探索在不同稀疏度下，模型重建精度与能量消耗之间的权衡关系，并确定构成帕累托前沿（Pareto front）的最优解集合，从而深入理解设计神经形态系统时面临的核心挑战。",
            "id": "4055862",
            "problem": "给定一个合成数据集、一个两层分层稀疏编码器以及一个受高效编码原理启发的能量模型。该编码器遵循与分层组织一致的级联结构：第一层将输入旋转到系数域，并通过软阈值化施加稀疏性；第二层对第一层的编码执行相同的操作。通过按顺序反转变换获得重构。您的任务是针对一组稀疏惩罚值，计算准确度与能量之间的权衡，然后计算非支配解的帕累托前沿。\n\n从高效编码原理开始：稀疏编码通过限制尖峰活动来降低能量消耗，同时保存信息。使用以下核心定义和经过充分检验的事实作为您推导和实现的基础：\n- 使用正交归一字典 $D$ 和目标函数 $J(v) = \\frac{1}{2}\\lVert u - D v \\rVert_2^2 + \\lambda \\lVert v \\rVert_1$ 的最小绝对收缩和选择算子 (LASSO) 优化问题，其坐标级的闭式解为 $v^\\star = S_\\lambda(D^\\top u)$，其中软阈值算子 $S_\\lambda$ 定义为 $S_\\lambda(z)_i = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\lambda, 0\\}$。\n- 在尖峰模型中，能量消耗近似正比于活跃系数的数量（作为尖峰的代理），并按每个尖峰的能量常数进行缩放。\n\n分层编码器规范：\n- 每个输入向量的维度为 $n = 4$。\n- 第一层字典 $D_1 \\in \\mathbb{R}^{4 \\times 4}$ 是缩放的哈达玛矩阵\n$$\nD_1 = \\frac{1}{2}\n\\begin{bmatrix}\n1  1  1  1 \\\\\n1  -1  1  -1 \\\\\n1  1  -1  -1 \\\\\n1  -1  -1  1\n\\end{bmatrix},\n$$\n它是正交归一的（即 $D_1^\\top D_1 = I$）。\n- 第二层字典 $D_2 \\in \\mathbb{R}^{4 \\times 4}$ 是一个块对角矩阵，包含两个以弧度为单位的平面旋转：\n$$\nD_2 = \n\\begin{bmatrix}\n\\cos\\varphi  -\\sin\\varphi  0  0 \\\\\n\\sin\\varphi  \\cos\\varphi  0  0 \\\\\n0  0  \\cos\\psi  -\\sin\\psi \\\\\n0  0  \\sin\\psi  \\cos\\psi\n\\end{bmatrix},\n$$\n其中 $\\varphi = \\pi/4$ 和 $\\psi = \\pi/3$，单位均为弧度。该矩阵是正交归一的（即 $D_2^\\top D_2 = I$）。\n\n编码器和解码器：\n- 对于每个输入 $x \\in \\mathbb{R}^4$，第一层编码为 $y_1 = S_\\lambda(D_1^\\top x)$。\n- 第二层编码为 $y_2 = S_\\lambda(D_2^\\top y_1)$。\n- 重构为 $x_{\\text{hat}} = D_1 D_2 y_2$。\n\n准确度：\n- 将单个输入的准确度定义为 $a(x,\\lambda) = 1 - \\frac{\\lVert x - x_{\\text{hat}} \\rVert_2^2}{\\lVert x \\rVert_2^2}$，对于 $x \\neq 0$，其值在 $[0,1]$ 区间内。\n- 对于给定的 $\\lambda$，报告的准确度是 $a(x,\\lambda)$ 在整个数据集上的平均值。\n\n能量：\n- 第一层和第二层每个尖峰的能量成本分别为 $e_1 = 10$ 皮焦耳和 $e_2 = 20$ 皮焦耳。\n- 对于给定输入，能量为 $E(x,\\lambda) = e_1 \\lVert y_1 \\rVert_0 + e_2 \\lVert y_2 \\rVert_0$，其中 $\\lVert \\cdot \\rVert_0$ 表示非零项的数量。\n- 对于给定的 $\\lambda$，报告的能量是 $E(x,\\lambda)$ 在整个数据集上的平均值，单位为皮焦耳/每输入向量。\n\n数据集：\n- 数据集包含 $5$ 个 $\\mathbb{R}^4$ 中的输入向量：\n$$\nx_1 = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.5 \\\\ 0.0 \\end{bmatrix},\\quad\nx_2 = \\begin{bmatrix} -0.5 \\\\ 1.5 \\\\ 0.0 \\\\ -1.0 \\end{bmatrix},\\quad\nx_3 = \\begin{bmatrix} 0.25 \\\\ -0.75 \\\\ 0.5 \\\\ 1.25 \\end{bmatrix},\\quad\nx_4 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix},\\quad\nx_5 = \\begin{bmatrix} -1.0 \\\\ -0.5 \\\\ 0.5 \\\\ 0.0 \\end{bmatrix}.\n$$\n\n权衡曲线：\n- 对于测试集中的每个稀疏参数 $\\lambda$，计算平均准确度和平均能量。$\\lambda$ 值的测试集为 $\\{0.0, 0.25, 0.5, 1.0, 2.5\\}$。\n\n帕累托前沿：\n- 如果一个解 $(\\text{accuracy}, \\text{energy})$ 满足 $\\text{accuracy} \\ge \\text{accuracy}'$ 且 $\\text{energy} \\le \\text{energy}'$，并且至少有一个不等式是严格的，那么它就支配另一个解 $(\\text{accuracy}', \\text{energy}')$。帕累托前沿是所有不被任何其他解所支配的解的集合。\n- 在从指定的 $\\lambda$ 值获得的点集上计算帕累托前沿。报告按能量递增排序的前沿。\n\n单位和角度规范：\n- 能量必须以皮焦耳/每输入向量为单位报告。\n- 角度 $\\varphi$ 和 $\\psi$ 以弧度为单位。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含两个用方括号括起来的列表。第一个列表包含按指定顺序排列的每个 $\\lambda$ 的权衡曲线（以有序对形式），第二个列表包含按能量递增排序的帕累托前沿有序对。每个有序对是一个双元素列表 $[\\text{accuracy}, \\text{energy}]$，两个分量都四舍五入到 $6$ 位小数。例如，输出应如下所示\n$$\n\\bigl[ [[a_1,E_1],[a_2,E_2],\\dots], [[a_{p1},E_{p1}],[a_{p2},E_{p2}],\\dots] \\bigr].\n$$\n\n测试集设计：\n- 包括 $\\lambda = 0.0$ 作为无稀疏性边界，在此边界下使用正交归一变换可实现精确重构。\n- 包括中间 $\\lambda$ 值 $\\lambda = 0.25$ 和 $\\lambda = 0.5$ 以测试典型的稀疏机制。\n- 包括 $\\lambda = 1.0$ 以测试更强的稀疏性。\n- 包括 $\\lambda = 2.5$ 以测试所有编码均为零且重构为零向量的边界情况。\n- 预期的输出是数值列表：权衡曲线和帕累托前沿，两者均以浮点数有序对的列表形式表示。\n\n您的程序必须精确地实现上述定义，并以指定的格式在单行中生成输出。",
            "solution": "该问题是有效的，因为它在科学上基于高效编码和稀疏表示的原理，在数学上是适定的，提供了所有必要的数据和定义，并且完全客观。在验证过程中未发现任何缺陷。\n\n任务是计算一个两层分层稀疏编码器的重构准确度与代谢能量之间的权衡，然后从这种权衡中识别出帕累托最优解。解决方案是通过模拟编码器在给定数据集和一组指定的稀疏惩罚下的行为来得出的。\n\n过程结构如下：\n1.  定义分层模型的组件。\n2.  对于每个指定的稀疏惩罚 $\\lambda$，为数据集中的每个输入向量模拟编码-解码过程。\n3.  计算每个 $\\lambda$ 的平均准确度和平均能量。\n4.  从计算出的 (准确度, 能量) 对集合中识别出非支配解（帕累托前沿）。\n\n**步骤 1：模型定义**\n该模型由两个顺序稀疏编码层组成。每一层的操作都基于 LASSO 问题，当字典是正交归一的时，该问题有一个涉及软阈值算子的闭式解。\n\n输入信号维度为 $n=4$。\n第一层字典 $D_1$ 是 $4 \\times 4$ 的正交归一缩放哈达玛矩阵：\n$$\nD_1 = \\frac{1}{2}\n\\begin{bmatrix}\n1  1  1  1 \\\\\n1  -1  1  -1 \\\\\n1  1  -1  -1 \\\\\n1  -1  -1  1\n\\end{bmatrix}\n$$\n第二层字典 $D_2$ 是一个正交归一的块对角旋转矩阵，其角度为 $\\varphi = \\pi/4$ 和 $\\psi = \\pi/3$：\n$$\nD_2 = \n\\begin{bmatrix}\n\\cos(\\pi/4)  -\\sin(\\pi/4)  0  0 \\\\\n\\sin(\\pi/4)  \\cos(\\pi/4)  0  0 \\\\\n0  0  \\cos(\\pi/3)  -\\sin(\\pi/3) \\\\\n0  0  \\sin(\\pi/3)  \\cos(\\pi/3)\n\\end{bmatrix}\n$$\n核心操作是逐元素应用的软阈值函数 $S_\\lambda(z)$：\n$$\nS_\\lambda(z_i) = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\lambda, 0\\}\n$$\n其中 $\\lambda \\ge 0$ 是稀疏惩罚。\n\n**步骤 2：编码、解码和指标计算**\n对于数据集中的每个输入向量 $x$ 和测试集 $\\{0.0, 0.25, 0.5, 1.0, 2.5\\}$ 中的每个 $\\lambda$ 值，我们执行以下计算：\na. **编码**：输入 $x$ 通过两层编码器。\n第一层编码 $y_1$ 是通过将输入 $x$ 投影到字典 $D_1$ 上并应用软阈值化得到的：\n$$\ny_1 = S_\\lambda(D_1^\\top x)\n$$\n第二层编码 $y_2$ 是通过将 $y_1$ 作为输入并使用字典 $D_2$ 重复此过程得到的：\n$$\ny_2 = S_\\lambda(D_2^\\top y_1)\n$$\nb. **解码**：重构 $x_{\\text{hat}}$ 是通过从最高层编码 $y_2$ 开始，顺序应用逆变换（由于字典是正交归一的，逆变换就是字典本身）生成的：\n$$\nx_{\\text{hat}} = D_1 D_2 y_2\n$$\nc. **准确度计算**：重构的准确度被量化为方差解释比例：\n$$\na(x,\\lambda) = 1 - \\frac{\\lVert x - x_{\\text{hat}} \\rVert_2^2}{\\lVert x \\rVert_2^2}\n$$\n其中 $\\lVert \\cdot \\rVert_2$ 是欧几里得范数。准确度为 $1$ 意味着完美重构。\nd. **能量计算**：能量成本被建模为与每层编码中非零系数（尖峰）的数量成正比。总能量为：\n$$\nE(x,\\lambda) = e_1 \\lVert y_1 \\rVert_0 + e_2 \\lVert y_2 \\rVert_0\n$$\n其中 $\\lVert \\cdot \\rVert_0$ 是 $L_0$范数（非零元素的数量），能量成本为 $e_1 = 10$ pJ 和 $e_2 = 20$ pJ。\n\n**步骤 3：生成权衡曲线**\n对于测试集中的每个 $\\lambda$，为数据集中的每个输入向量 $x_i$（$i=1, \\dots, 5$）计算准确度 $a(x_i,\\lambda)$ 和能量 $E(x_i,\\lambda)$。然后将结果在整个数据集上取平均，得到该 $\\lambda$ 对应的一个代表性对（平均准确度，平均能量）：\n$$\n\\text{Accuracy}(\\lambda) = \\frac{1}{5} \\sum_{i=1}^{5} a(x_i, \\lambda)\n$$\n$$\n\\text{Energy}(\\lambda) = \\frac{1}{5} \\sum_{i=1}^{5} E(x_i, \\lambda)\n$$\n这些对的集合，按照给定的 $\\lambda$ 值顺序排列，构成了权衡曲线。\n\n**步骤 4：计算帕累托前沿**\n帕累托前沿由所有不被任何其他解所支配的解组成。如果一个解 $(A, E)$ 满足 $A \\ge A'$ 且 $E \\le E'$，并且至少有一个不等式是严格的，那么它就支配另一个解 $(A', E')$。在我们的情境中，这意味着如果一个解能以至多相同的能量成本提供至少同样好的准确度，并且在这些指标中至少有一个上严格更优，那么这个解就是更优的。\n为了找到前沿，每个计算出的 (准确度, 能量) 对都将与所有其他对进行比较。如果发现某个对被另一个对支配，则将其从前沿中排除。最终的非支配点集合构成了帕累托前沿。然后按照要求，将这些点按能量递增排序。实现将系统地检查计算出的权衡点之间的支配关系。最后，所有数值结果都格式化为 $6$ 位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical sparse coding problem.\n    \"\"\"\n    # Step 1: Define constants, dictionaries, and dataset\n    n = 4\n    \n    # First-layer dictionary D1 (scaled Hadamard matrix)\n    D1 = 0.5 * np.array([\n        [1, 1, 1, 1],\n        [1, -1, 1, -1],\n        [1, 1, -1, -1],\n        [1, -1, -1, 1]\n    ], dtype=float)\n\n    # Second-layer dictionary D2 (block-diagonal rotation matrix)\n    phi = np.pi / 4\n    psi = np.pi / 3\n    D2 = np.array([\n        [np.cos(phi), -np.sin(phi), 0, 0],\n        [np.sin(phi), np.cos(phi), 0, 0],\n        [0, 0, np.cos(psi), -np.sin(psi)],\n        [0, 0, np.sin(psi), np.cos(psi)]\n    ], dtype=float)\n\n    # Inverses are transposes since dictionaries are orthonormal\n    D1_T = D1.T\n    D2_T = D2.T\n\n    # Energy constants\n    e1 = 10.0  # pJ\n    e2 = 20.0  # pJ\n\n    # Dataset\n    dataset = [\n        np.array([1.0, 0.5, -0.5, 0.0]),\n        np.array([-0.5, 1.5, 0.0, -1.0]),\n        np.array([0.25, -0.75, 0.5, 1.25]),\n        np.array([1.0, 1.0, 1.0, 1.0]),\n        np.array([-1.0, -0.5, 0.5, 0.0])\n    ]\n    num_samples = len(dataset)\n\n    # Sparsity penalty test suite\n    lambda_suite = [0.0, 0.25, 0.5, 1.0, 2.5]\n    \n    # Soft-thresholding operator\n    def soft_threshold(z, lambd):\n        return np.sign(z) * np.maximum(np.abs(z) - lambd, 0)\n\n    # Step 2  3: Compute the trade-off curve\n    tradeoff_curve_points = []\n    \n    for lambd in lambda_suite:\n        total_accuracy = 0.0\n        total_energy = 0.0\n        \n        for x in dataset:\n            # Encoding\n            y1 = soft_threshold(D1_T @ x, lambd)\n            y2 = soft_threshold(D2_T @ y1, lambd)\n            \n            # Decoding\n            x_hat = D1 @ D2 @ y2\n            \n            # Accuracy\n            norm_x_sq = np.sum(x**2)\n            if norm_x_sq == 0:\n                # Should not happen with the given data\n                accuracy = 1.0\n            else:\n                error_norm_sq = np.sum((x - x_hat)**2)\n                accuracy = 1.0 - error_norm_sq / norm_x_sq\n            \n            # Energy\n            l0_y1 = np.count_nonzero(y1)\n            l0_y2 = np.count_nonzero(y2)\n            energy = e1 * l0_y1 + e2 * l0_y2\n            \n            total_accuracy += accuracy\n            total_energy += energy\n            \n        avg_accuracy = total_accuracy / num_samples\n        avg_energy = total_energy / num_samples\n        \n        tradeoff_curve_points.append([avg_accuracy, avg_energy])\n\n    # Step 4: Compute the Pareto front\n    pareto_front = []\n    for i, p1 in enumerate(tradeoff_curve_points):\n        is_dominated = False\n        for j, p2 in enumerate(tradeoff_curve_points):\n            if i == j:\n                continue\n            # p2 dominates p1 if acc2 >= acc1 and ene2 = ene1 (with one strict)\n            if (p2[0] >= p1[0] and p2[1] = p1[1]) and \\\n               (p2[0] > p1[0] or p2[1]  p1[1]):\n                is_dominated = True\n                break\n        if not is_dominated:\n            pareto_front.append(p1)\n\n    # Sort Pareto front by increasing energy\n    pareto_front.sort(key=lambda p: p[1])\n    \n    # Step 5: Format the output\n    tradeoff_curve_rounded = [[round(p[0], 6), round(p[1], 6)] for p in tradeoff_curve_points]\n    pareto_front_rounded = [[round(p[0], 6), round(p[1], 6)] for p in pareto_front]\n    \n    # This manual string construction ensures there are no spaces after commas.\n    tradeoff_str = f\"[{','.join(str(p).replace(' ', '') for p in tradeoff_curve_rounded)}]\"\n    pareto_str = f\"[{','.join(str(p).replace(' ', '') for p in pareto_front_rounded)}]\"\n\n    print(f\"[{tradeoff_str},{pareto_str}]\")\n\nsolve()\n```"
        }
    ]
}