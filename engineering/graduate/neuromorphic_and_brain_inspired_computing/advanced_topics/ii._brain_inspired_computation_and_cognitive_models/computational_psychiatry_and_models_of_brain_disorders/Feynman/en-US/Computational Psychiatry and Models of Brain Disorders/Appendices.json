{
    "hands_on_practices": [
        {
            "introduction": "Many brain disorders, particularly major depression, are increasingly understood as disruptions of large-scale brain network organization. Network science provides a powerful toolkit to formalize and test this idea, and modularity ($Q$) is a cornerstone metric. This exercise provides direct, hands-on experience in calculating the modularity of a given brain network partition, grounding the abstract concept of \"functional segregation\" in a concrete computation. ",
            "id": "4039887",
            "problem": "A mesoscale functional brain network is represented by an undirected, weighted adjacency matrix $\\mathbf{A} \\in \\mathbb{R}^{6 \\times 6}$ with nodes corresponding to cortical parcels. Consider the following empirically plausible connectivity (weights are Fisher-transformed correlations), with stronger intra-network coupling and weaker inter-network coupling:\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n0 & 3.0 & 2.5 & 0.3 & 0.4 & 0.2 \\\\\n3.0 & 0 & 2.0 & 0.5 & 0.3 & 0.4 \\\\\n2.5 & 2.0 & 0 & 0.2 & 0.4 & 0.3 \\\\\n0.3 & 0.5 & 0.2 & 0 & 3.2 & 2.8 \\\\\n0.4 & 0.3 & 0.4 & 3.2 & 0 & 3.1 \\\\\n0.2 & 0.4 & 0.3 & 2.8 & 3.1 & 0\n\\end{pmatrix}.\n$$\nAssume a two-module community assignment in which nodes $\\{1,2,3\\}$ belong to community $\\mathcal{C}_{1}$ and nodes $\\{4,5,6\\}$ belong to community $\\mathcal{C}_{2}$. Use the standard Newman–Girvan modularity for undirected, weighted networks with a strength-preserving null model. Specifically, treat the off-diagonal entries as edge weights, the node strength $s_{i}$ as the sum of weights incident to node $i$, and the total edge weight $w$ as half the sum of all entries of $\\mathbf{A}$. Compute the modularity $Q$ of this partition.\n\nRound your final numerical value of $Q$ to four significant figures. No units are required.\n\nAfter computing $Q$, briefly state, in one sentence, how a reduced $Q$ would be interpreted in the context of major depressive disorder as it relates to functional segregation. (Your interpretation will not be graded numerically; only the value of $Q$ will be used for the final answer.)",
            "solution": "The problem is valid as it is scientifically grounded in network science, well-posed with all necessary information provided, and objective in its formulation. We can proceed with the calculation.\n\nThe modularity $Q$ for a partition of a weighted, undirected network is given by the Newman–Girvan formula:\n$$ Q = \\frac{1}{2w} \\sum_{i,j} \\left[ A_{ij} - \\frac{s_i s_j}{2w} \\right] \\delta(c_i, c_j) $$\nwhere $\\mathbf{A}$ is the adjacency matrix with entries $A_{ij}$, $s_i = \\sum_j A_{ij}$ is the strength of node $i$, $w = \\frac{1}{2} \\sum_{i,j} A_{ij}$ is the total weight of all edges in the network, $c_i$ is the community of node $i$, and $\\delta(c_i, c_j)$ is the Kronecker delta, which is $1$ if nodes $i$ and $j$ are in the same community and $0$ otherwise.\n\nA more computationally convenient form of the modularity equation is:\n$$ Q = \\sum_{c=1}^{N_c} \\left[ \\frac{w_c}{w} - \\left(\\frac{S_c}{2w}\\right)^2 \\right] $$\nwhere the sum is over the $N_c$ communities, $w_c = \\frac{1}{2} \\sum_{i,j \\in c} A_{ij}$ is the sum of weights of the links inside community $c$, and $S_c = \\sum_{i \\in c} s_i$ is the sum of the strengths of the nodes in community $c$.\n\nFirst, we calculate the strength $s_i$ for each node by summing the rows of the given adjacency matrix $\\mathbf{A}$:\n$$\n\\mathbf{A} \\;=\\;\n\\begin{pmatrix}\n0 & 3.0 & 2.5 & 0.3 & 0.4 & 0.2 \\\\\n3.0 & 0 & 2.0 & 0.5 & 0.3 & 0.4 \\\\\n2.5 & 2.0 & 0 & 0.2 & 0.4 & 0.3 \\\\\n0.3 & 0.5 & 0.2 & 0 & 3.2 & 2.8 \\\\\n0.4 & 0.3 & 0.4 & 3.2 & 0 & 3.1 \\\\\n0.2 & 0.4 & 0.3 & 2.8 & 3.1 & 0\n\\end{pmatrix}\n$$\n$s_1 = 3.0 + 2.5 + 0.3 + 0.4 + 0.2 = 6.4$\n$s_2 = 3.0 + 2.0 + 0.5 + 0.3 + 0.4 = 6.2$\n$s_3 = 2.5 + 2.0 + 0.2 + 0.4 + 0.3 = 5.4$\n$s_4 = 0.3 + 0.5 + 0.2 + 3.2 + 2.8 = 7.0$\n$s_5 = 0.4 + 0.3 + 0.4 + 3.2 + 3.1 = 7.4$\n$s_6 = 0.2 + 0.4 + 0.3 + 2.8 + 3.1 = 6.8$\n\nNext, we calculate the total edge weight $w$ of the network, which is half the sum of all node strengths:\n$$ w = \\frac{1}{2} \\sum_{i=1}^{6} s_i = \\frac{1}{2} (6.4 + 6.2 + 5.4 + 7.0 + 7.4 + 6.8) = \\frac{1}{2} (39.2) = 19.6 $$\nTherefore, the term $2w$ is $39.2$.\n\nNow, we analyze the two communities, $\\mathcal{C}_{1} = \\{1,2,3\\}$ and $\\mathcal{C}_{2} = \\{4,5,6\\}$.\n\nFor community $\\mathcal{C}_{1}$:\nThe sum of internal edge weights, $w_1$, is the sum of weights of edges connecting nodes $\\{1,2,3\\}$ to each other.\n$$ w_1 = A_{12} + A_{13} + A_{23} = 3.0 + 2.5 + 2.0 = 7.5 $$\nThe sum of node strengths, $S_1$, is:\n$$ S_1 = s_1 + s_2 + s_3 = 6.4 + 6.2 + 5.4 = 18.0 $$\n\nFor community $\\mathcal{C}_{2}$:\nThe sum of internal edge weights, $w_2$, is the sum of weights of edges connecting nodes $\\{4,5,6\\}$ to each other.\n$$ w_2 = A_{45} + A_{46} + A_{56} = 3.2 + 2.8 + 3.1 = 9.1 $$\nThe sum of node strengths, $S_2$, is:\n$$ S_2 = s_4 + s_5 + s_6 = 7.0 + 7.4 + 6.8 = 21.2 $$\n\nNow we can compute the modularity $Q$ by summing the contributions from each community:\n$$ Q = \\left[ \\frac{w_1}{w} - \\left(\\frac{S_1}{2w}\\right)^2 \\right] + \\left[ \\frac{w_2}{w} - \\left(\\frac{S_2}{2w}\\right)^2 \\right] $$\nSubstituting the calculated values:\n$$ Q = \\left[ \\frac{7.5}{19.6} - \\left(\\frac{18.0}{39.2}\\right)^2 \\right] + \\left[ \\frac{9.1}{19.6} - \\left(\\frac{21.2}{39.2}\\right)^2 \\right] $$\n$$ Q = \\frac{7.5 + 9.1}{19.6} - \\left( \\frac{18.0^2}{39.2^2} + \\frac{21.2^2}{39.2^2} \\right) $$\n$$ Q = \\frac{16.6}{19.6} - \\frac{324 + 449.44}{1536.64} $$\n$$ Q = \\frac{16.6}{19.6} - \\frac{773.44}{1536.64} $$\nEvaluating the fractions:\n$$ Q \\approx 0.84693877... - 0.50334975... $$\n$$ Q \\approx 0.34358902... $$\nRounding to four significant figures, we get $Q \\approx 0.3436$.\n\nA reduced modularity $Q$ in major depressive disorder would be interpreted as a decrease in functional segregation, reflecting a breakdown of the typical community structure of the brain into less distinct functional modules.",
            "answer": "$$\\boxed{0.3436}$$"
        },
        {
            "introduction": "How do false or distorted perceptions, a hallmark of psychosis, arise from neural computations? The Bayesian Brain hypothesis posits that such symptoms can result from an imbalance between prior beliefs and incoming sensory evidence. This practice guides you through simulating a simple Bayesian agent to see firsthand how overly strong prior expectations, combined with noisy sensory input, can cause the agent to \"perceive\" a stimulus that is not actually present. By implementing this model, you will gain a practical understanding of how the brain's inferential machinery can generate false alarms, providing a computational account for positive symptoms. ",
            "id": "4039878",
            "problem": "Consider a single latent continuous cause $x \\in \\mathbb{R}$ representing a stimulus intensity. In a predictive coding view of Bayesian perception, an agent maintains a Gaussian prior over $x$, namely $x \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$, and receives a sensory observation $y \\in \\mathbb{R}$ generated from a Gaussian likelihood $y \\sim \\mathcal{N}(x, \\sigma^2)$. The sensory precision is defined as $\\pi_s = 1/\\sigma^2$ and the prior precision is defined as $\\pi_0 = 1/\\sigma_0^2$. The agent updates from the prior to a posterior over $x$ by applying Bayes' rule under the normal-normal conjugate model, and uses the posterior mean as a point estimate of $x$. The agent then makes a binary detection decision for the presence of a stimulus by comparing the posterior mean to a decision threshold $\\tau$: detect if posterior mean $>$ $\\tau$, otherwise do not detect.\n\nYour task is to construct a simulation in which true stimuli are absent (that is, the ground-truth latent $x$ equals $0$ in all trials) and quantify the false-positive detection rate (false alarm rate) under different combinations of sensory and prior precisions. The simulation should proceed as follows:\n- For each parameter set, draw $N$ independent observations $y$ from the absence generative model $y \\sim \\mathcal{N}(0, \\sigma^2)$.\n- For each observation, compute the posterior mean under the normal-normal model using Bayes' rule.\n- Apply the detection decision rule with threshold $\\tau$.\n- Report the false alarm rate as the fraction of trials for which a detection occurs despite the stimulus being absent.\n- Use a fixed random seed equal to $42$ for reproducibility.\n- Express each false alarm rate as a decimal between $0$ and $1$, rounded to $4$ decimal places.\n\nThe simulation must implement the Bayesian update from first principles for the normal-normal conjugate model. It must not assume any heuristic weighing scheme other than the normative Bayesian update implied by the specified Gaussian prior and Gaussian likelihood.\n\nTest suite:\nCompute the false alarm rate for each of the following parameter sets $(\\sigma^2, \\sigma_0^2, \\mu_0, \\tau, N)$:\n1. $(1.0, 1.0, 0.5, 0.2, 100000)$: a general case with moderate sensory precision and moderate prior precision with a positive prior mean.\n2. $(25.0, 0.04, 1.0, 0.2, 100000)$: low sensory precision combined with high prior precision with a strongly positive prior mean.\n3. $(0.01, 10.0, 1.0, 0.2, 100000)$: high sensory precision combined with low prior precision, with a positive prior mean.\n4. $(1.0, 1.0, 0.5, 1.0, 100000)$: a conservative decision threshold with moderate sensory and prior precisions.\n5. $(1.0, 1.0, 0.5, -0.1, 100000)$: a liberal decision threshold with moderate sensory and prior precisions.\n6. $(25.0, 0.04, 0.0, 0.2, 100000)$: low sensory precision combined with high prior precision but a neutral prior mean.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite above. For example, $[\\text{result1},\\text{result2},\\dots]$, where each entry is the false alarm rate rounded to $4$ decimal places as specified.",
            "solution": "The problem requires the simulation of a Bayesian perceptual model to determine the rate of false-positive detections (false alarms) under various parametric conditions. The model is based on the normal-normal conjugate framework, which is a cornerstone of Bayesian statistics. The solution proceeds by first deriving the analytical form of the posterior belief and then using this result to construct a numerical simulation.\n\nA rational agent aims to infer a latent stimulus intensity $x \\in \\mathbb{R}$. The agent's prior belief about $x$ is described by a Gaussian distribution with mean $\\mu_0$ and variance $\\sigma_0^2$:\n$$p(x) = \\mathcal{N}(x | \\mu_0, \\sigma_0^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}\\right)$$\nUpon receiving a sensory observation $y \\in \\mathbb{R}$, the agent updates its belief. The observation $y$ is assumed to be generated from a Gaussian likelihood function centered on the true latent cause $x$, with variance $\\sigma^2$:\n$$p(y|x) = \\mathcal{N}(y | x, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right)$$\nThe agent combines the prior belief with the sensory evidence using Bayes' rule to form a posterior belief over $x$:\n$$p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$$\nwhere $p(y) = \\int p(y|x)p(x) dx$ is the marginal likelihood or evidence. Since we are dealing with conjugate distributions (a Gaussian prior and a Gaussian likelihood), the posterior will also be a Gaussian distribution, $p(x|y) = \\mathcal{N}(x | \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$. The parameters of this posterior distribution can be derived by examining the terms in the exponent of the product $p(y|x)p(x)$:\n$$p(x|y) \\propto \\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}\\right)$$\n$$= \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(x-y)^2}{\\sigma^2} + \\frac{(x-\\mu_0)^2}{\\sigma_0^2} \\right] \\right)$$\nExpanding the quadratic terms in $x$ in the exponent:\n$$-\\frac{1}{2} \\left[ \\frac{x^2 - 2xy + y^2}{\\sigma^2} + \\frac{x^2 - 2x\\mu_0 + \\mu_0^2}{\\sigma_0^2} \\right]$$\n$$= -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)x^2 - 2\\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)x + \\text{constants} \\right]$$\nBy completing the square for $x$, we can identify the posterior precision $\\pi_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ and posterior mean $\\mu_{\\text{post}}$. The posterior precision is the sum of the likelihood precision $\\pi_s = 1/\\sigma^2$ and the prior precision $\\pi_0 = 1/\\sigma_0^2$:\n$$\\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\sigma_0^2}$$\nThe posterior mean $\\mu_{\\text{post}}$ is a precision-weighted average of the sensory observation $y$ (which acts as the data-driven mean) and the prior mean $\\mu_0$:\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) = \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)^{-1} \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)$$\nSimplifying this expression yields the formula for the posterior mean, which serves as the agent's point estimate of the stimulus intensity:\n$$\\mu_{\\text{post}} = \\frac{\\sigma_0^2 y + \\sigma^2 \\mu_0}{\\sigma^2 + \\sigma_0^2}$$\nThe problem defines a simulation where the true stimulus is always absent, i.e., the ground-truth value is $x=0$. In each trial, a sensory observation $y$ is generated from the likelihood model with $x=0$:\n$$y \\sim \\mathcal{N}(0, \\sigma^2)$$\nFor each such observation $y$, the agent computes the posterior mean $\\mu_{\\text{post}}$. A detection is triggered if this estimate exceeds a decision threshold $\\tau$:\n$$\\text{Detect if } \\mu_{\\text{post}} > \\tau$$\nSince the stimulus is actually absent ($x=0$), any detection is a false alarm. The false alarm rate is the proportion of trials in which $\\mu_{\\text{post}} > \\tau$.\n\nThe simulation algorithm is as follows for each parameter set $(\\sigma^2, \\sigma_0^2, \\mu_0, \\tau, N)$:\n1.  Initialize a pseudo-random number generator with a fixed seed of $42$ for reproducibility.\n2.  Generate a vector of $N$ independent sensory observations, $\\{y_i\\}_{i=1}^N$, by drawing from the distribution $\\mathcal{N}(0, \\sigma^2)$. The standard deviation used for sampling is $\\sigma = \\sqrt{\\sigma^2}$.\n3.  For each observation $y_i$, compute the corresponding posterior mean $\\mu_{\\text{post},i}$ using the derived formula:\n    $$\\mu_{\\text{post},i} = \\frac{\\sigma_0^2 y_i + \\sigma^2 \\mu_0}{\\sigma^2 + \\sigma_0^2}$$\n    This calculation is performed efficiently for all $N$ observations using vectorized operations.\n4.  Count the number of trials where the detection condition is met: $C = \\sum_{i=1}^N \\mathbb{I}(\\mu_{\\text{post},i} > \\tau)$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n5.  The false alarm rate is computed as the ratio of the count of false alarms to the total number of trials: $\\text{FAR} = C/N$.\n6.  The result is rounded to $4$ decimal places as required.\n\nThis procedure is repeated for all test cases provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a Bayesian perception model to calculate false alarm rates\n    under different parameter sets.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (sigma_sq, sigma0_sq, mu0, tau, N)\n    test_cases = [\n        (1.0, 1.0, 0.5, 0.2, 100000),\n        (25.0, 0.04, 1.0, 0.2, 100000),\n        (0.01, 10.0, 1.0, 0.2, 100000),\n        (1.0, 1.0, 0.5, 1.0, 100000),\n        (1.0, 1.0, 0.5, -0.1, 100000),\n        (25.0, 0.04, 0.0, 0.2, 100000)\n    ]\n\n    results = []\n    \n    # Set the fixed random seed for reproducibility.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    for case in test_cases:\n        sigma_sq, sigma0_sq, mu0, tau, N = case\n\n        # Step 1: Draw N independent observations from the absence generative model.\n        # The model is y ~ N(0, sigma_sq).\n        # np.random.normal takes standard deviation (sigma), not variance (sigma_sq).\n        sigma = np.sqrt(sigma_sq)\n        y_obs = rng.normal(loc=0, scale=sigma, size=N)\n\n        # Step 2: Compute the posterior mean for each observation.\n        # This is the implementation of the formula derived from first principles:\n        # mu_post = (sigma0_sq * y + sigma_sq * mu0) / (sigma_sq + sigma0_sq)\n        numerator = sigma0_sq * y_obs + sigma_sq * mu0\n        denominator = sigma_sq + sigma0_sq\n        mu_post = numerator / denominator\n\n        # Step 3: Apply the detection decision rule and count false alarms.\n        # A false alarm occurs if the posterior mean > tau, as the true stimulus is absent.\n        false_alarms = np.sum(mu_post > tau)\n\n        # Step 4: Calculate the false alarm rate.\n        false_alarm_rate = false_alarms / N\n\n        # Round the result to 4 decimal places as specified.\n        rounded_rate = round(false_alarm_rate, 4)\n        results.append(rounded_rate)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Computational psychiatry often presents us with multiple, competing mechanistic models to explain the same set of behavioral or neural data. A crucial scientific skill is to adjudicate between these rival theories in a principled manner. This advanced practice challenges you to build a complete Bayesian model comparison pipeline, which involves computing the \"model evidence\" to quantitatively compare two different reinforcement learning models of decision-making. Completing this exercise will equip you with a powerful methodological framework, advancing your skills from simply simulating a single model to formally evaluating which theory best explains the data. ",
            "id": "4039932",
            "problem": "You are asked to design and implement a complete, self-contained model comparison pipeline for computational psychiatry within neuromorphic and brain-inspired computing. The goal is to adjudicate between two competing mechanistic accounts of patient decision-making using Bayesian model evidence and compute Bayes factors. Your program must be a complete, runnable implementation of the model comparison, producing a single-line output as specified below.\n\nTwo mechanistic accounts are to be compared:\n\n- Mechanistic account $\\mathcal{M}_1$ (asymmetric learning rates): a neuromorphic reinforcement learning policy with separate learning rates for positive and negative prediction errors, capturing a negativity bias often implicated in mood disorders. The learning rates are $(\\alpha_{+}, \\alpha_{-}) \\in [0,1]^2$. The latent value for each action is updated using a standard temporal-difference rule, with the choice probability governed by a softmax policy on neuromorphic value units.\n\n- Mechanistic account $\\mathcal{M}_2$ (dopaminergic reward attenuation): a neuromorphic reinforcement learning policy with a single learning rate $\\alpha \\in [0,1]$ and a reward attenuation parameter $\\lambda \\in [0,1]$ that scales the received reward before updating the value. This captures anhedonia-like mechanisms where rewards are perceived as less salient.\n\nFoundational base and definitions to be used:\n\n- Bayesian model comparison is grounded in probability theory and Bayes' theorem. For any model $\\mathcal{M}$ with parameters $\\theta$ and data $D$, Bayes' theorem states $p(\\theta \\mid D, \\mathcal{M}) \\propto p(D \\mid \\theta, \\mathcal{M}) p(\\theta \\mid \\mathcal{M})$. The Bayesian model evidence (also called the marginal likelihood) is defined as\n$$p(D \\mid \\mathcal{M}) = \\int p(D \\mid \\theta, \\mathcal{M}) \\, p(\\theta \\mid \\mathcal{M}) \\, d\\theta.$$\nThe Bayes factor comparing $\\mathcal{M}_1$ to $\\mathcal{M}_2$ is defined as\n$$\\mathrm{BF}_{1,2} = \\frac{p(D \\mid \\mathcal{M}_1)}{p(D \\mid \\mathcal{M}_2)}.$$\n\n- The neuromorphic reinforcement learning policy uses a softmax decision rule. For two actions with current value estimates $(Q_0, Q_1)$ and inverse temperature $\\kappa > 0$, the probability of choosing action $c_t \\in \\{0,1\\}$ at time $t$ is\n$$p(c_t \\mid Q_t, \\kappa) = \\frac{\\exp(\\kappa Q_{t, c_t})}{\\exp(\\kappa Q_{t, 0}) + \\exp(\\kappa Q_{t, 1})}.$$\nAfter observing reward $r_t \\in [0,1]$, the value update for the chosen action depends on the mechanistic account:\n- For $\\mathcal{M}_1$: define the prediction error $\\delta_t = r_t - Q_{t, c_t}$. Use $\\alpha_{+}$ if $\\delta_t \\ge 0$ and $\\alpha_{-}$ otherwise, with the update\n$$Q_{t+1, c_t} = Q_{t, c_t} + \\alpha_{\\pm} \\, \\delta_t,$$\nand the unchosen value remains unchanged, $Q_{t+1, \\text{other}} = Q_{t, \\text{other}}$.\n- For $\\mathcal{M}_2$: compute an attenuated reward $r'_t = \\lambda \\, r_t$. Then the prediction error is $\\delta_t = r'_t - Q_{t, c_t}$ and the update rule is\n$$Q_{t+1, c_t} = Q_{t, c_t} + \\alpha \\, \\delta_t,$$\nwith $Q_{t+1, \\text{other}} = Q_{t, \\text{other}}$.\n\n- The initial values are $Q_{0,0} = 0$ and $Q_{0,1} = 0$. The inverse temperature is fixed across all cases and models at $\\kappa = 5.0$ (dimensionless). All quantities above are dimensionless; no physical units are required.\n\n- Priors: For $\\mathcal{M}_1$, use independent Beta priors for the learning rates:\n$$\\alpha_{+} \\sim \\mathrm{Beta}(2,2), \\quad \\alpha_{-} \\sim \\mathrm{Beta}(2,2).$$\nFor $\\mathcal{M}_2$, use independent Beta priors\n$$\\alpha \\sim \\mathrm{Beta}(2,2), \\quad \\lambda \\sim \\mathrm{Beta}(3,2).$$\nThe Beta density for $x \\in (0,1)$ with shape $(a,b)$ is\n$$f(x \\mid a,b) = \\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)},$$\nwhere $B(a,b)$ is the Beta function.\n\nLikelihood construction for a dataset $D=\\{(c_t, r_t)\\}_{t=1}^T$:\n\n- For any parameter $\\theta$ of a given model, the likelihood $p(D \\mid \\theta, \\mathcal{M})$ is the product over trials of the choice probabilities given the current values and softmax rule,\n$$p(D \\mid \\theta, \\mathcal{M}) = \\prod_{t=1}^T p(c_t \\mid Q_t(\\theta), \\kappa),$$\nwith $Q_t(\\theta)$ evolving according to the model's update rule using the observed rewards $r_t$.\n\nNumerical evaluation requirements:\n\n- Compute the Bayesian model evidence $p(D \\mid \\mathcal{M})$ for each model by numerical quadrature over the parameter domain, using a uniform grid of $N = 101$ points per parameter dimension over the interval $[\\epsilon, 1-\\epsilon]$ with $\\epsilon = 10^{-6}$. The discrete approximation is\n$$p(D \\mid \\mathcal{M}) \\approx \\sum_{i} \\sum_{j} \\cdots \\left( p(D \\mid \\theta_{i,j,\\ldots}, \\mathcal{M}) \\, p(\\theta_{i,j,\\ldots} \\mid \\mathcal{M}) \\right) \\prod_{d} \\Delta_d,$$\nwhere $\\Delta_d$ is the grid step size for dimension $d$. For numerical stability, compute sums in the log-domain using the log-sum-exp operator.\n\nTest suite:\n\nProvide results for the following three datasets (each is a sequence over $T=8$ trials). Each dataset consists of choices and rewards:\n- Case $1$ (mixed rewards and choices): choices $[0,0,1,1,0,1,0,1]$, rewards $[1,0,1,0,1,1,0,0]$.\n- Case $2$ (boundary, zero rewards): choices $[0,1,0,1,0,1,0,1]$, rewards $[0,0,0,0,0,0,0,0]$.\n- Case $3$ (consistent reward on one action, mostly choosing that action): choices $[0,0,0,0,1,0,0,0]$, rewards $[1,1,1,1,0,1,1,1]$.\n\nYour program must:\n- Implement both mechanistic accounts as described.\n- For each dataset, compute the Bayesian model evidence for $\\mathcal{M}_1$ and $\\mathcal{M}_2$ via grid-based numerical integration with $N=101$ and $\\epsilon = 10^{-6}$.\n- Compute the Bayes factor $\\mathrm{BF}_{1,2}$ for each dataset.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the Bayes factors for the three test cases as a comma-separated list enclosed in square brackets. Each Bayes factor must be a float rounded to six decimal places, in the order of cases $1$, $2$, $3$, for example, $[bf_1,bf_2,bf_3]$, where each $bf_i$ is a decimal number rounded to six places.",
            "solution": "The objective is to perform a Bayesian model comparison between two mechanistic models of reinforcement learning, designated $\\mathcal{M}_1$ and $\\mathcal{M}_2$, which represent competing hypotheses about dysfunctions in decision-making. The comparison is arbitrated using the Bayes factor, $\\mathrm{BF}_{1,2}$, which quantifies the evidence provided by a given dataset $D$ in favor of $\\mathcal{M}_1$ over $\\mathcal{M}_2$.\n\nThe core of Bayesian model comparison is the Bayes factor, defined as the ratio of the model evidences (or marginal likelihoods):\n$$\n\\mathrm{BF}_{1,2} = \\frac{p(D \\mid \\mathcal{M}_1)}{p(D \\mid \\mathcal{M}_2)}\n$$\nThis requires the computation of the model evidence, $p(D \\mid \\mathcal{M})$, for each model. The evidence is obtained by integrating the likelihood of the data, $p(D \\mid \\theta, \\mathcal{M})$, over the entire parameter space $\\theta$, weighted by the prior probability of the parameters, $p(\\theta \\mid \\mathcal{M})$:\n$$\np(D \\mid \\mathcal{M}) = \\int p(D \\mid \\theta, \\mathcal{M}) \\, p(\\theta \\mid \\mathcal{M}) \\, d\\theta\n$$\nFor this problem, the two components of the integrand, the likelihood and the prior, are explicitly defined.\n\nThe likelihood function, $p(D \\mid \\theta, \\mathcal{M})$, is derived from the agent's decision-making process. For a dataset $D = \\{(c_t, r_t)\\}_{t=1}^T$ comprising a sequence of choices $c_t$ and rewards $r_t$, the total likelihood is the product of the probabilities of each individual choice, conditioned on the history of actions and rewards up to that point. This history is encapsulated in the learned action-value estimates, $Q_t$.\n$$\np(D \\mid \\theta, \\mathcal{M}) = \\prod_{t=1}^T p(c_t \\mid Q_t(\\theta), \\kappa)\n$$\nThe calculation of this likelihood necessitates a trial-by-trial simulation of the agent's learning process for a given set of parameters $\\theta$. The process is as follows:\n1. Initialize the action values to zero: $Q_{0,0} = 0.0$ and $Q_{0,1} = 0.0$.\n2. For each trial $t$ from $1$ to $T$:\n    a. The probability of the agent making the observed choice $c_t$ is calculated using the softmax function, with a fixed inverse temperature $\\kappa = 5.0$:\n    $$p(c_t \\mid Q_t, \\kappa) = \\frac{\\exp(\\kappa Q_{t, c_t})}{\\exp(\\kappa Q_{t, 0}) + \\exp(\\kappa Q_{t, 1})}$$\n    b. The value of the chosen action, $Q_{t, c_t}$, is updated according to the rules of the specific model, $\\mathcal{M}_1$ or $\\mathcal{M}_2$. The value of the unchosen action remains unchanged.\n\nThe two models differ in their value update mechanisms:\n- **Model $\\mathcal{M}_1$ (asymmetric learning rates):** This model has parameters $\\theta_1 = (\\alpha_+, \\alpha_-)$. The prediction error is $\\delta_t = r_t - Q_{t,c_t}$. The update rule is $Q_{t+1, c_t} = Q_{t, c_t} + \\alpha_{\\pm} \\delta_t$, where $\\alpha_+$ is used if $\\delta_t \\ge 0$ and $\\alpha_-$ is used if $\\delta_t < 0$.\n- **Model $\\mathcal{M}_2$ (reward attenuation):** This model has parameters $\\theta_2 = (\\alpha, \\lambda)$. The reward is first attenuated, $r'_t = \\lambda r_t$. The prediction error is then $\\delta_t = r'_t - Q_{t,c_t}$, and the update rule is $Q_{t+1, c_t} = Q_{t, c_t} + \\alpha \\delta_t$.\n\nFor numerical stability, all likelihood calculations are performed in the logarithmic domain. The total log-likelihood for a given $\\theta$ is the sum of the log-probabilities of each choice: $\\mathcal{L}(\\theta) = \\sum_{t=1}^T \\log p(c_t \\mid Q_t(\\theta), \\kappa)$.\n\nThe prior distributions, $p(\\theta \\mid \\mathcal{M})$, encode our beliefs about the parameters before observing the data. The problem specifies Beta distributions for all parameters:\n- For $\\mathcal{M}_1$: $\\alpha_+ \\sim \\mathrm{Beta}(2,2)$ and $\\alpha_- \\sim \\mathrm{Beta}(2,2)$. The joint prior is $p(\\theta_1 \\mid \\mathcal{M}_1) = p(\\alpha_+ \\mid a=2, b=2) p(\\alpha_- \\mid a=2, b=2)$.\n- For $\\mathcal{M}_2$: $\\alpha \\sim \\mathrm{Beta}(2,2)$ and $\\lambda \\sim \\mathrm{Beta}(3,2)$. The joint prior is $p(\\theta_2 \\mid \\mathcal{M}_2) = p(\\alpha \\mid a=2, b=2) p(\\lambda \\mid a=3, b=2)$.\n\nThe model evidence integral is analytically intractable, so we employ numerical quadrature. The two-dimensional parameter space for each model is discretized into a uniform grid of $N \\times N$ points, with $N=101$. The grid spans the interval $[\\epsilon, 1-\\epsilon]$ for each parameter, where $\\epsilon = 10^{-6}$. The integral is approximated by a sum over all grid points $(\\theta_{ij})$:\n$$p(D \\mid \\mathcal{M}) \\approx \\sum_{i=1}^N \\sum_{j=1}^N p(D \\mid \\theta_{ij}, \\mathcal{M}) p(\\theta_{ij} \\mid \\mathcal{M}) \\Delta^2$$\nwhere $\\Delta = (1 - 2\\epsilon) / (N-1)$ is the grid spacing. To avert underflow and other numerical issues, this sum is computed in the log domain. The log-evidence is:\n$$\n\\log p(D \\mid \\mathcal{M}) \\approx \\log \\left( \\sum_{i,j} \\exp(\\mathcal{L}(\\theta_{ij}) + \\log p(\\theta_{ij} \\mid \\mathcal{M})) \\right) + 2\\log(\\Delta)\n$$\nThe summation term is calculated using the log-sum-exp algorithm, which provides numerical stability.\n\nThe algorithmic implementation follows this principled design. A main function iterates through the three provided datasets. For each dataset, it calls a dedicated function, `calculate_log_evidence`, for each of the two models. This function constructs the parameter grid, and for each point on the grid, it computes the log-prior (using `scipy.stats.beta.logpdf`) and the log-likelihood. The log-likelihood function itself simulates the agent's trial-by-trial behavior. The resulting log-posterior terms (log-likelihood plus log-prior) are aggregated using `scipy.special.logsumexp` to compute the log-evidence. Finally, the Bayes factor is computed from the log-evidences: $\\mathrm{BF}_{1,2} = \\exp(\\log p(D \\mid \\mathcal{M}_1) - \\log p(D \\mid \\mathcal{M}_2))$. The result for each case is then rounded and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\nfrom scipy.special import logsumexp\n\ndef calculate_log_likelihood_m1(data, params, kappa):\n    \"\"\"\n    Calculates the total log-likelihood for Model 1 (asymmetric learning rates).\n    \n    Args:\n        data (tuple): A tuple of (choices, rewards).\n        params (tuple): A tuple of (alpha_pos, alpha_neg).\n        kappa (float): The inverse temperature parameter.\n\n    Returns:\n        float: The total log-likelihood of the data given the parameters.\n    \"\"\"\n    alpha_pos, alpha_neg = params\n    choices, rewards = data\n    \n    Q = np.array([0.0, 0.0])\n    total_log_likelihood = 0.0\n\n    for t in range(len(choices)):\n        c_t, r_t = choices[t], rewards[t]\n        \n        # Calculate log-probability of the choice\n        kappa_Q = kappa * Q\n        log_softmax_den = logsumexp(kappa_Q)\n        log_prob_choice = kappa_Q[c_t] - log_softmax_den\n        total_log_likelihood += log_prob_choice\n        \n        # Update Q-value based on M1's rule\n        delta = r_t - Q[c_t]\n        alpha = alpha_pos if delta >= 0.0 else alpha_neg\n        Q[c_t] += alpha * delta\n        \n    return total_log_likelihood\n\ndef calculate_log_likelihood_m2(data, params, kappa):\n    \"\"\"\n    Calculates the total log-likelihood for Model 2 (reward attenuation).\n\n    Args:\n        data (tuple): A tuple of (choices, rewards).\n        params (tuple): A tuple of (alpha, lambda).\n        kappa (float): The inverse temperature parameter.\n\n    Returns:\n        float: The total log-likelihood of the data given the parameters.\n    \"\"\"\n    alpha, lam = params\n    choices, rewards = data\n    \n    Q = np.array([0.0, 0.0])\n    total_log_likelihood = 0.0\n\n    for t in range(len(choices)):\n        c_t, r_t = choices[t], rewards[t]\n        \n        # Calculate log-probability of the choice\n        kappa_Q = kappa * Q\n        log_softmax_den = logsumexp(kappa_Q)\n        log_prob_choice = kappa_Q[c_t] - log_softmax_den\n        total_log_likelihood += log_prob_choice\n        \n        # Update Q-value based on M2's rule\n        r_prime = lam * r_t\n        delta = r_prime - Q[c_t]\n        Q[c_t] += alpha * delta\n        \n    return total_log_likelihood\n\ndef compute_log_evidence(data, priors_ab, likelihood_func, n_grid, epsilon, kappa):\n    \"\"\"\n    Computes the log model evidence via grid-based numerical integration.\n\n    Args:\n        data (tuple): A tuple of (choices, rewards).\n        priors_ab (list): A list of [a,b] for the Beta priors of the two parameters.\n        likelihood_func (function): The function to compute log-likelihood.\n        n_grid (int): The number of points on the grid for each parameter.\n        epsilon (float): The small offset for the grid boundaries.\n        kappa (float): The inverse temperature.\n\n    Returns:\n        float: The log model evidence.\n    \"\"\"\n    grid_points = np.linspace(epsilon, 1.0 - epsilon, n_grid)\n    delta = (1.0 - 2.0 * epsilon) / (n_grid - 1.0)\n    log_delta = np.log(delta)\n\n    priors = [(priors_ab[0][0], priors_ab[0][1]), (priors_ab[1][0], priors_ab[1][1])]\n    \n    log_prior1_vals = beta.logpdf(grid_points, priors[0][0], priors[0][1])\n    log_prior2_vals = beta.logpdf(grid_points, priors[1][0], priors[1][1])\n    \n    # Using a meshgrid is more efficient than nested Python loops\n    p1_grid, p2_grid = np.meshgrid(grid_points, grid_points, indexing='ij')\n\n    log_prior_grid = log_prior1_vals[:, np.newaxis] + log_prior2_vals[np.newaxis, :]\n    \n    # Vectorize the likelihood calculation over the grid\n    # This involves a Python loop, but it's cleaner than passing the whole grid\n    # to the likelihood function, which is stateful (trial-by-trial).\n    log_likelihood_grid = np.zeros_like(p1_grid)\n    for i in range(n_grid):\n        for j in range(n_grid):\n            params = (p1_grid[i, j], p2_grid[i, j])\n            log_likelihood_grid[i, j] = likelihood_func(data, params, kappa)\n\n    log_posterior_terms = log_likelihood_grid + log_prior_grid\n    \n    # Sum over the 2D grid and add the log of the volume element (Delta^2)\n    log_sum_evidence = logsumexp(log_posterior_terms)\n    log_evidence = log_sum_evidence + 2.0 * log_delta\n    \n    return log_evidence\n\ndef solve():\n    \"\"\"\n    Main function to run the model comparison pipeline for the test cases.\n    \"\"\"\n    KAPPA = 5.0\n    N_GRID = 101\n    EPSILON = 1e-6\n\n    test_cases = [\n        # Case 1: mixed rewards and choices\n        ((0,0,1,1,0,1,0,1), (1,0,1,0,1,1,0,0)),\n        # Case 2: boundary, zero rewards\n        ((0,1,0,1,0,1,0,1), (0,0,0,0,0,0,0,0)),\n        # Case 3: consistent reward on one action\n        ((0,0,0,0,1,0,0,0), (1,1,1,1,0,1,1,1)),\n    ]\n\n    priors_m1 = [[2.0, 2.0], [2.0, 2.0]] # (alpha+, alpha-)\n    priors_m2 = [[2.0, 2.0], [3.0, 2.0]] # (alpha, lambda)\n\n    results = []\n    for data in test_cases:\n        # Pre-process data for convenience\n        choices, rewards = np.array(data[0]), np.array(data[1])\n        formatted_data = (choices, rewards)\n\n        # Compute log evidence for Model 1\n        log_evidence_m1 = compute_log_evidence(\n            formatted_data, priors_m1, calculate_log_likelihood_m1,\n            N_GRID, EPSILON, KAPPA\n        )\n\n        # Compute log evidence for Model 2\n        log_evidence_m2 = compute_log_evidence(\n            formatted_data, priors_m2, calculate_log_likelihood_m2,\n            N_GRID, EPSILON, KAPPA\n        )\n        \n        # Compute Bayes Factor BF_{1,2}\n        log_bf_12 = log_evidence_m1 - log_evidence_m2\n        bf_12 = np.exp(log_bf_12)\n        \n        results.append(f\"{bf_12:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}