{
    "hands_on_practices": [
        {
            "introduction": "This first exercise explores the link between biological rhythms and mood disorders, a key area of research in computational psychiatry. You will use a simplified mathematical model of a circadian oscillator to quantify how a small phase shift, perhaps induced by a therapeutic intervention, is predicted to alter mood symptom severity. This practice  demonstrates how fundamental analytical tools, like linearization, can generate testable hypotheses about the relationship between physiological dynamics and clinical outcomes.",
            "id": "4039927",
            "problem": "A neuromorphic circadian module intended to emulate the Suprachiasmatic Nucleus (SCN) is modeled by a single-phase oscillator whose observable state (a proxy for circadian arousal) is given by $s(t) = A \\cos(\\omega t + \\phi)$, where $A$ is the amplitude, $\\omega$ is the angular frequency, $t$ is clock time measured from local midnight, and $\\phi$ is the phase. In a computational psychiatry model of mood disorder severity, the instantaneous mood symptom severity $M(t)$ is approximated near a baseline operating point by an affine readout $M(t) \\approx M_{0} + \\alpha s(t)$, where $\\alpha$ is a gain converting circadian arousal to severity units and $M_{0}$ is a baseline term. The system is probed at a fixed measurement time $t_{m}$ each day.\n\nA light-based zeitgeber intervention induces a small phase shift $\\Delta \\phi$ in the oscillator, so that the post-intervention state becomes $s_{\\text{post}}(t) = A \\cos(\\omega t + \\phi_{0} + \\Delta \\phi)$, where $\\phi_{0}$ is the pre-intervention baseline phase. Assume $\\Delta \\phi$ is sufficiently small that a first-order approximation around $\\Delta \\phi = 0$ is valid.\n\nStarting only from well-tested facts about harmonic oscillators and first-order Taylor expansions, derive the linearized change in mood severity at the measurement time $t_{m}$, defined as $\\Delta M = M_{\\text{post}}(t_{m}) - M_{\\text{pre}}(t_{m})$, in terms of the given parameters. Then evaluate $\\Delta M$ for the following scientifically plausible parameters: amplitude $A = 0.7$, gain $\\alpha = 4.3$, angular frequency $\\omega = \\frac{2\\pi}{24}$ (radians per hour), measurement time $t_{m} = 8$ (hours after local midnight), baseline phase $\\phi_{0} = -\\frac{\\pi}{6}$ (radians), and phase shift $\\Delta \\phi = 0.27$ (radians). Express the final $\\Delta M$ as a dimensionless quantity and round your answer to four significant figures.",
            "solution": "The problem requires the derivation of the linearized change in mood severity, $\\Delta M$, resulting from a small phase shift, $\\Delta \\phi$, in a circadian oscillator model, and then to evaluate this change for a given set of parameters.\n\nFirst, we establish the definitions provided in the problem statement. The state of the circadian oscillator is given by a harmonic function:\n$$s(t) = A \\cos(\\omega t + \\phi)$$\nwhere $A$ is the amplitude, $\\omega$ is the angular frequency, $t$ is time, and $\\phi$ is the phase.\n\nThe instantaneous mood symptom severity, $M(t)$, is given by an affine transformation of the oscillator's state:\n$$M(t) = M_{0} + \\alpha s(t)$$\nwhere $M_{0}$ is a baseline severity and $\\alpha$ is a gain factor.\n\nMeasurements are taken at a fixed time each day, $t_{m}$. The pre-intervention mood severity at this time, $M_{\\text{pre}}(t_{m})$, is based on the initial phase $\\phi_{0}$.\nThe pre-intervention state is $s_{\\text{pre}}(t) = A \\cos(\\omega t + \\phi_{0})$.\nThus, the pre-intervention mood severity at the measurement time is:\n$$M_{\\text{pre}}(t_{m}) = M_{0} + \\alpha s_{\\text{pre}}(t_{m}) = M_{0} + \\alpha A \\cos(\\omega t_{m} + \\phi_{0})$$\n\nA light-based intervention induces a small phase shift $\\Delta \\phi$. The post-intervention state is $s_{\\text{post}}(t) = A \\cos(\\omega t + \\phi_{0} + \\Delta \\phi)$.\nThe post-intervention mood severity at the measurement time is:\n$$M_{\\text{post}}(t_{m}) = M_{0} + \\alpha s_{\\text{post}}(t_{m}) = M_{0} + \\alpha A \\cos(\\omega t_{m} + \\phi_{0} + \\Delta \\phi)$$\n\nThe change in mood severity, $\\Delta M$, is the difference between the post-intervention and pre-intervention mood severities at the measurement time $t_{m}$:\n$$\\Delta M = M_{\\text{post}}(t_{m}) - M_{\\text{pre}}(t_{m})$$\nSubstituting the expressions for $M_{\\text{post}}$ and $M_{\\text{pre}}$:\n$$\\Delta M = \\left( M_{0} + \\alpha A \\cos(\\omega t_{m} + \\phi_{0} + \\Delta \\phi) \\right) - \\left( M_{0} + \\alpha A \\cos(\\omega t_{m} + \\phi_{0}) \\right)$$\nThe baseline term $M_{0}$ cancels out:\n$$\\Delta M = \\alpha A \\left( \\cos(\\omega t_{m} + \\phi_{0} + \\Delta \\phi) - \\cos(\\omega t_{m} + \\phi_{0}) \\right)$$\n\nThe problem states that $\\Delta \\phi$ is small, warranting a first-order Taylor expansion around $\\Delta \\phi = 0$. Let us define a function $f(\\delta) = \\cos(C + \\delta)$, where $C = \\omega t_{m} + \\phi_{0}$ is a constant phase angle and $\\delta$ is the small perturbation, corresponding to $\\Delta \\phi$. The first-order Taylor expansion of $f(\\delta)$ around $\\delta = 0$ is:\n$$f(\\delta) \\approx f(0) + f'(0) \\delta$$\nThe function and its derivative are:\n$$f(\\delta) = \\cos(C + \\delta) \\implies f(0) = \\cos(C)$$\n$$f'(\\delta) = -\\sin(C + \\delta) \\implies f'(0) = -\\sin(C)$$\nThus, the approximation is:\n$$\\cos(C + \\delta) \\approx \\cos(C) - \\delta \\sin(C)$$\nSubstituting $C = \\omega t_{m} + \\phi_{0}$ and $\\delta = \\Delta \\phi$, we get:\n$$\\cos(\\omega t_{m} + \\phi_{0} + \\Delta \\phi) \\approx \\cos(\\omega t_{m} + \\phi_{0}) - \\Delta \\phi \\sin(\\omega t_{m} + \\phi_{0})$$\n\nNow, we substitute this linearized expression back into the equation for $\\Delta M$:\n$$\\Delta M \\approx \\alpha A \\left( \\left( \\cos(\\omega t_{m} + \\phi_{0}) - \\Delta \\phi \\sin(\\omega t_{m} + \\phi_{0}) \\right) - \\cos(\\omega t_{m} + \\phi_{0}) \\right)$$\nThe term $\\cos(\\omega t_{m} + \\phi_{0})$ cancels, yielding the final expression for the linearized change in mood severity:\n$$\\Delta M = - \\alpha A \\sin(\\omega t_{m} + \\phi_{0}) \\Delta \\phi$$\nThis completes the derivation part of the problem.\n\nNext, we evaluate $\\Delta M$ using the provided parameters:\n$A = 0.7$\n$\\alpha = 4.3$\n$\\omega = \\frac{2\\pi}{24} = \\frac{\\pi}{12}$ radians/hour\n$t_{m} = 8$ hours\n$\\phi_{0} = -\\frac{\\pi}{6}$ radians\n$\\Delta \\phi = 0.27$ radians\n\nFirst, we calculate the total phase argument of the sine function, $\\omega t_{m} + \\phi_{0}$:\n$$\\omega t_{m} + \\phi_{0} = \\left(\\frac{\\pi}{12} \\text{ rad/hr}\\right) \\times (8 \\text{ hr}) + \\left(-\\frac{\\pi}{6} \\text{ rad}\\right)$$\n$$\\omega t_{m} + \\phi_{0} = \\frac{8\\pi}{12} - \\frac{\\pi}{6} = \\frac{2\\pi}{3} - \\frac{\\pi}{6}$$\nTo subtract, we find a common denominator:\n$$\\omega t_{m} + \\phi_{0} = \\frac{4\\pi}{6} - \\frac{\\pi}{6} = \\frac{3\\pi}{6} = \\frac{\\pi}{2} \\text{ radians}$$\n\nNow, we evaluate the sine of this angle:\n$$\\sin(\\omega t_{m} + \\phi_{0}) = \\sin\\left(\\frac{\\pi}{2}\\right) = 1$$\n\nFinally, we substitute all the numerical values into the expression for $\\Delta M$:\n$$\\Delta M = - \\alpha A \\sin(\\omega t_{m} + \\phi_{0}) \\Delta \\phi$$\n$$\\Delta M = - (4.3) \\times (0.7) \\times (1) \\times (0.27)$$\n$$\\Delta M = - (3.01) \\times (0.27)$$\n$$\\Delta M = -0.8127$$\nThe problem specifies the answer should be rounded to four significant figures. The calculated value $-0.8127$ already has four significant figures. The quantity is dimensionless as requested, assuming the mood severity scale is itself a dimensionless score.",
            "answer": "$$\\boxed{-0.8127}$$"
        },
        {
            "introduction": "This hands-on practice moves from analytical modeling to computational simulation, tackling a central theme in computational psychiatry: the Bayesian brain hypothesis. You will implement a simple perceptual agent to explore how it combines prior beliefs with noisy sensory evidence to make a decision. This simulation  directly tests a key hypothesis about psychosis, demonstrating how an over-weighting of prior expectations relative to sensory input can lead to false inferences, or \"hallucinations,\" even in the absence of a true stimulus.",
            "id": "4039878",
            "problem": "Consider a single latent continuous cause $x \\in \\mathbb{R}$ representing a stimulus intensity. In a predictive coding view of Bayesian perception, an agent maintains a Gaussian prior over $x$, namely $x \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$, and receives a sensory observation $y \\in \\mathbb{R}$ generated from a Gaussian likelihood $y \\sim \\mathcal{N}(x, \\sigma^2)$. The sensory precision is defined as $\\pi_s = 1/\\sigma^2$ and the prior precision is defined as $\\pi_0 = 1/\\sigma_0^2$. The agent updates from the prior to a posterior over $x$ by applying Bayes' rule under the normal-normal conjugate model, and uses the posterior mean as a point estimate of $x$. The agent then makes a binary detection decision for the presence of a stimulus by comparing the posterior mean to a decision threshold $\\tau$: detect if posterior mean $$ $\\tau$, otherwise do not detect.\n\nYour task is to construct a simulation in which true stimuli are absent (that is, the ground-truth latent $x$ equals $0$ in all trials) and quantify the false-positive detection rate (false alarm rate) under different combinations of sensory and prior precisions. The simulation should proceed as follows:\n- For each parameter set, draw $N$ independent observations $y$ from the absence generative model $y \\sim \\mathcal{N}(0, \\sigma^2)$.\n- For each observation, compute the posterior mean under the normal-normal model using Bayes' rule.\n- Apply the detection decision rule with threshold $\\tau$.\n- Report the false alarm rate as the fraction of trials for which a detection occurs despite the stimulus being absent.\n- Use a fixed random seed equal to $42$ for reproducibility.\n- Express each false alarm rate as a decimal between $0$ and $1$, rounded to $4$ decimal places.\n\nThe simulation must implement the Bayesian update from first principles for the normal-normal conjugate model. It must not assume any heuristic weighing scheme other than the normative Bayesian update implied by the specified Gaussian prior and Gaussian likelihood.\n\nTest suite:\nCompute the false alarm rate for each of the following parameter sets $(\\sigma^2, \\sigma_0^2, \\mu_0, \\tau, N)$:\n1. $(1.0, 1.0, 0.5, 0.2, 100000)$: a general case with moderate sensory precision and moderate prior precision with a positive prior mean.\n2. $(25.0, 0.04, 1.0, 0.2, 100000)$: low sensory precision combined with high prior precision with a strongly positive prior mean.\n3. $(0.01, 10.0, 1.0, 0.2, 100000)$: high sensory precision combined with low prior precision, with a positive prior mean.\n4. $(1.0, 1.0, 0.5, 1.0, 100000)$: a conservative decision threshold with moderate sensory and prior precisions.\n5. $(1.0, 1.0, 0.5, -0.1, 100000)$: a liberal decision threshold with moderate sensory and prior precisions.\n6. $(25.0, 0.04, 0.0, 0.2, 100000)$: low sensory precision combined with high prior precision but a neutral prior mean.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite above. For example, $[\\text{result1},\\text{result2},\\dots]$, where each entry is the false alarm rate rounded to $4$ decimal places as specified.",
            "solution": "The problem requires the simulation of a Bayesian perceptual model to determine the rate of false-positive detections (false alarms) under various parametric conditions. The model is based on the normal-normal conjugate framework, which is a cornerstone of Bayesian statistics. The solution proceeds by first deriving the analytical form of the posterior belief and then using this result to construct a numerical simulation.\n\nA rational agent aims to infer a latent stimulus intensity $x \\in \\mathbb{R}$. The agent's prior belief about $x$ is described by a Gaussian distribution with mean $\\mu_0$ and variance $\\sigma_0^2$:\n$$p(x) = \\mathcal{N}(x | \\mu_0, \\sigma_0^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}\\right)$$\nUpon receiving a sensory observation $y \\in \\mathbb{R}$, the agent updates its belief. The observation $y$ is assumed to be generated from a Gaussian likelihood function centered on the true latent cause $x$, with variance $\\sigma^2$:\n$$p(y|x) = \\mathcal{N}(y | x, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right)$$\nThe agent combines the prior belief with the sensory evidence using Bayes' rule to form a posterior belief over $x$:\n$$p(x|y) = \\frac{p(y|x)p(x)}{p(y)}$$\nwhere $p(y) = \\int p(y|x)p(x) dx$ is the marginal likelihood or evidence. Since we are dealing with conjugate distributions (a Gaussian prior and a Gaussian likelihood), the posterior will also be a Gaussian distribution, $p(x|y) = \\mathcal{N}(x | \\mu_{\\text{post}}, \\sigma_{\\text{post}}^2)$. The parameters of this posterior distribution can be derived by examining the terms in the exponent of the product $p(y|x)p(x)$:\n$$p(x|y) \\propto \\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right) \\exp\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}\\right)$$\n$$= \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(x-y)^2}{\\sigma^2} + \\frac{(x-\\mu_0)^2}{\\sigma_0^2} \\right] \\right)$$\nExpanding the quadratic terms in $x$ in the exponent:\n$$-\\frac{1}{2} \\left[ \\frac{x^2 - 2xy + y^2}{\\sigma^2} + \\frac{x^2 - 2x\\mu_0 + \\mu_0^2}{\\sigma_0^2} \\right]$$\n$$= -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)x^2 - 2\\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)x + \\text{constants} \\right]$$\nBy completing the square for $x$, we can identify the posterior precision $\\pi_{\\text{post}} = 1/\\sigma_{\\text{post}}^2$ and posterior mean $\\mu_{\\text{post}}$. The posterior precision is the sum of the likelihood precision $\\pi_s = 1/\\sigma^2$ and the prior precision $\\pi_0 = 1/\\sigma_0^2$:\n$$\\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\sigma_0^2}$$\nThe posterior mean $\\mu_{\\text{post}}$ is a precision-weighted average of the sensory observation $y$ (which acts as the data-driven mean) and the prior mean $\\mu_0$:\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) = \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)^{-1} \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)$$\nSimplifying this expression yields the formula for the posterior mean, which serves as the agent's point estimate of the stimulus intensity:\n$$\\mu_{\\text{post}} = \\frac{\\sigma_0^2 y + \\sigma^2 \\mu_0}{\\sigma^2 + \\sigma_0^2}$$\nThe problem defines a simulation where the true stimulus is always absent, i.e., the ground-truth value is $x=0$. In each trial, a sensory observation $y$ is generated from the likelihood model with $x=0$:\n$$y \\sim \\mathcal{N}(0, \\sigma^2)$$\nFor each such observation $y$, the agent computes the posterior mean $\\mu_{\\text{post}}$. A detection is triggered if this estimate exceeds a decision threshold $\\tau$:\n$$\\text{Detect if } \\mu_{\\text{post}}  \\tau$$\nSince the stimulus is actually absent ($x=0$), any detection is a false alarm. The false alarm rate is the proportion of trials in which $\\mu_{\\text{post}}  \\tau$.\n\nThe simulation algorithm is as follows for each parameter set $(\\sigma^2, \\sigma_0^2, \\mu_0, \\tau, N)$:\n1.  Initialize a pseudo-random number generator with a fixed seed of $42$ for reproducibility.\n2.  Generate a vector of $N$ independent sensory observations, $\\{y_i\\}_{i=1}^N$, by drawing from the distribution $\\mathcal{N}(0, \\sigma^2)$. The standard deviation used for sampling is $\\sigma = \\sqrt{\\sigma^2}$.\n3.  For each observation $y_i$, compute the corresponding posterior mean $\\mu_{\\text{post},i}$ using the derived formula:\n    $$\\mu_{\\text{post},i} = \\frac{\\sigma_0^2 y_i + \\sigma^2 \\mu_0}{\\sigma^2 + \\sigma_0^2}$$\n    This calculation is performed efficiently for all $N$ observations using vectorized operations.\n4.  Count the number of trials where the detection condition is met: $C = \\sum_{i=1}^N \\mathbb{I}(\\mu_{\\text{post},i}  \\tau)$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n5.  The false alarm rate is computed as the ratio of the count of false alarms to the total number of trials: $\\text{FAR} = C/N$.\n6.  The result is rounded to $4$ decimal places as required.\n\nThis procedure is repeated for all test cases provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a Bayesian perception model to calculate false alarm rates\n    under different parameter sets.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (sigma_sq, sigma0_sq, mu0, tau, N)\n    test_cases = [\n        (1.0, 1.0, 0.5, 0.2, 100000),\n        (25.0, 0.04, 1.0, 0.2, 100000),\n        (0.01, 10.0, 1.0, 0.2, 100000),\n        (1.0, 1.0, 0.5, 1.0, 100000),\n        (1.0, 1.0, 0.5, -0.1, 100000),\n        (25.0, 0.04, 0.0, 0.2, 100000)\n    ]\n\n    results = []\n    \n    # Set the fixed random seed for reproducibility.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    for case in test_cases:\n        sigma_sq, sigma0_sq, mu0, tau, N = case\n\n        # Step 1: Draw N independent observations from the absence generative model.\n        # The model is y ~ N(0, sigma_sq).\n        # np.random.normal takes standard deviation (sigma), not variance (sigma_sq).\n        sigma = np.sqrt(sigma_sq)\n        y_obs = rng.normal(loc=0, scale=sigma, size=N)\n\n        # Step 2: Compute the posterior mean for each observation.\n        # This is the implementation of the formula derived from first principles:\n        # mu_post = (sigma0_sq * y + sigma_sq * mu0) / (sigma_sq + sigma0_sq)\n        numerator = sigma0_sq * y_obs + sigma_sq * mu0\n        denominator = sigma_sq + sigma0_sq\n        mu_post = numerator / denominator\n\n        # Step 3: Apply the detection decision rule and count false alarms.\n        # A false alarm occurs if the posterior mean  tau, as the true stimulus is absent.\n        false_alarms = np.sum(mu_post  tau)\n\n        # Step 4: Calculate the false alarm rate.\n        false_alarm_rate = false_alarms / N\n\n        # Round the result to 4 decimal places as specified.\n        rounded_rate = round(false_alarm_rate, 4)\n        results.append(rounded_rate)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final, advanced practice simulates one of the most critical tasks in computational psychiatry: formally comparing competing theories of brain dysfunction. You will implement a full Bayesian model comparison pipeline to adjudicate between two distinct reinforcement learning accounts of maladaptive decision-making, using hypothetical behavioral data. By calculating the model evidence and the resulting Bayes factor , you will gain hands-on experience with the rigorous process of using data to select the most plausible mechanistic explanation for observed behavior, a cornerstone of the field.",
            "id": "4039932",
            "problem": "You are asked to design and implement a complete, self-contained model comparison pipeline for computational psychiatry within neuromorphic and brain-inspired computing. The goal is to adjudicate between two competing mechanistic accounts of patient decision-making using Bayesian model evidence and compute Bayes factors. Your program must be a complete, runnable implementation of the model comparison, producing a single-line output as specified below.\n\nTwo mechanistic accounts are to be compared:\n\n- Mechanistic account $\\mathcal{M}_1$ (asymmetric learning rates): a neuromorphic reinforcement learning policy with separate learning rates for positive and negative prediction errors, capturing a negativity bias often implicated in mood disorders. The learning rates are $(\\alpha_{+}, \\alpha_{-}) \\in [0,1]^2$. The latent value for each action is updated using a standard temporal-difference rule, with the choice probability governed by a softmax policy on neuromorphic value units.\n\n- Mechanistic account $\\mathcal{M}_2$ (dopaminergic reward attenuation): a neuromorphic reinforcement learning policy with a single learning rate $\\alpha \\in [0,1]$ and a reward attenuation parameter $\\lambda \\in [0,1]$ that scales the received reward before updating the value. This captures anhedonia-like mechanisms where rewards are perceived as less salient.\n\nFoundational base and definitions to be used:\n\n- Bayesian model comparison is grounded in probability theory and Bayes' theorem. For any model $\\mathcal{M}$ with parameters $\\theta$ and data $D$, Bayes' theorem states $p(\\theta \\mid D, \\mathcal{M}) \\propto p(D \\mid \\theta, \\mathcal{M}) p(\\theta \\mid \\mathcal{M})$. The Bayesian model evidence (also called the marginal likelihood) is defined as\n$$p(D \\mid \\mathcal{M}) = \\int p(D \\mid \\theta, \\mathcal{M}) \\, p(\\theta \\mid \\mathcal{M}) \\, d\\theta.$$\nThe Bayes factor comparing $\\mathcal{M}_1$ to $\\mathcal{M}_2$ is defined as\n$$\\mathrm{BF}_{1,2} = \\frac{p(D \\mid \\mathcal{M}_1)}{p(D \\mid \\mathcal{M}_2)}.$$\n\n- The neuromorphic reinforcement learning policy uses a softmax decision rule. For two actions with current value estimates $(Q_0, Q_1)$ and inverse temperature $\\kappa  0$, the probability of choosing action $c_t \\in \\{0,1\\}$ at time $t$ is\n$$p(c_t \\mid Q_t, \\kappa) = \\frac{\\exp(\\kappa Q_{t, c_t})}{\\exp(\\kappa Q_{t, 0}) + \\exp(\\kappa Q_{t, 1})}.$$\nAfter observing reward $r_t \\in [0,1]$, the value update for the chosen action depends on the mechanistic account:\n- For $\\mathcal{M}_1$: define the prediction error $\\delta_t = r_t - Q_{t, c_t}$. Use $\\alpha_{+}$ if $\\delta_t \\ge 0$ and $\\alpha_{-}$ otherwise, with the update\n$$Q_{t+1, c_t} = Q_{t, c_t} + \\alpha_{\\pm} \\, \\delta_t,$$\nand the unchosen value remains unchanged, $Q_{t+1, \\text{other}} = Q_{t, \\text{other}}$.\n- For $\\mathcal{M}_2$: compute an attenuated reward $r'_t = \\lambda \\, r_t$. Then the prediction error is $\\delta_t = r'_t - Q_{t, c_t}$ and the update rule is\n$$Q_{t+1, c_t} = Q_{t, c_t} + \\alpha \\, \\delta_t,$$\nwith $Q_{t+1, \\text{other}} = Q_{t, \\text{other}}$.\n\n- The initial values are $Q_{0,0} = 0$ and $Q_{0,1} = 0$. The inverse temperature is fixed across all cases and models at $\\kappa = 5.0$ (dimensionless). All quantities above are dimensionless; no physical units are required.\n\n- Priors: For $\\mathcal{M}_1$, use independent Beta priors for the learning rates:\n$$\\alpha_{+} \\sim \\mathrm{Beta}(2,2), \\quad \\alpha_{-} \\sim \\mathrm{Beta}(2,2).$$\nFor $\\mathcal{M}_2$, use independent Beta priors\n$$\\alpha \\sim \\mathrm{Beta}(2,2), \\quad \\lambda \\sim \\mathrm{Beta}(3,2).$$\nThe Beta density for $x \\in (0,1)$ with shape $(a,b)$ is\n$$f(x \\mid a,b) = \\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)},$$\nwhere $B(a,b)$ is the Beta function.\n\nLikelihood construction for a dataset $D=\\{(c_t, r_t)\\}_{t=1}^T$:\n\n- For any parameter $\\theta$ of a given model, the likelihood $p(D \\mid \\theta, \\mathcal{M})$ is the product over trials of the choice probabilities given the current values and softmax rule,\n$$p(D \\mid \\theta, \\mathcal{M}) = \\prod_{t=1}^T p(c_t \\mid Q_t(\\theta), \\kappa),$$\nwith $Q_t(\\theta)$ evolving according to the model's update rule using the observed rewards $r_t$.\n\nNumerical evaluation requirements:\n\n- Compute the Bayesian model evidence $p(D \\mid \\mathcal{M})$ for each model by numerical quadrature over the parameter domain, using a uniform grid of $N = 101$ points per parameter dimension over the interval $[\\epsilon, 1-\\epsilon]$ with $\\epsilon = 10^{-6}$. The discrete approximation is\n$$p(D \\mid \\mathcal{M}) \\approx \\sum_{i} \\sum_{j} \\cdots \\left( p(D \\mid \\theta_{i,j,\\ldots}, \\mathcal{M}) \\, p(\\theta_{i,j,\\ldots} \\mid \\mathcal{M}) \\right) \\prod_{d} \\Delta_d,$$\nwhere $\\Delta_d$ is the grid step size for dimension $d$. For numerical stability, compute sums in the log-domain using the log-sum-exp operator.\n\nTest suite:\n\nProvide results for the following three datasets (each is a sequence over $T=8$ trials). Each dataset consists of choices and rewards:\n- Case $1$ (mixed rewards and choices): choices $[0,0,1,1,0,1,0,1]$, rewards $[1,0,1,0,1,1,0,0]$.\n- Case $2$ (boundary, zero rewards): choices $[0,1,0,1,0,1,0,1]$, rewards $[0,0,0,0,0,0,0,0]$.\n- Case $3$ (consistent reward on one action, mostly choosing that action): choices $[0,0,0,0,1,0,0,0]$, rewards $[1,1,1,1,0,1,1,1]$.\n\nYour program must:\n- Implement both mechanistic accounts as described.\n- For each dataset, compute the Bayesian model evidence for $\\mathcal{M}_1$ and $\\mathcal{M}_2$ via grid-based numerical integration with $N=101$ and $\\epsilon = 10^{-6}$.\n- Compute the Bayes factor $\\mathrm{BF}_{1,2}$ for each dataset.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the Bayes factors for the three test cases as a comma-separated list enclosed in square brackets. Each Bayes factor must be a float rounded to six decimal places, in the order of cases $1$, $2$, $3$, for example, $[bf_1,bf_2,bf_3]$, where each $bf_i$ is a decimal number rounded to six places.",
            "solution": "The objective is to perform a Bayesian model comparison between two mechanistic models of reinforcement learning, designated $\\mathcal{M}_1$ and $\\mathcal{M}_2$, which represent competing hypotheses about dysfunctions in decision-making. The comparison is arbitrated using the Bayes factor, $\\mathrm{BF}_{1,2}$, which quantifies the evidence provided by a given dataset $D$ in favor of $\\mathcal{M}_1$ over $\\mathcal{M}_2$.\n\nThe core of Bayesian model comparison is the Bayes factor, defined as the ratio of the model evidences (or marginal likelihoods):\n$$\n\\mathrm{BF}_{1,2} = \\frac{p(D \\mid \\mathcal{M}_1)}{p(D \\mid \\mathcal{M}_2)}\n$$\nThis requires the computation of the model evidence, $p(D \\mid \\mathcal{M})$, for each model. The evidence is obtained by integrating the likelihood of the data, $p(D \\mid \\theta, \\mathcal{M})$, over the entire parameter space $\\theta$, weighted by the prior probability of the parameters, $p(\\theta \\mid \\mathcal{M})$:\n$$\np(D \\mid \\mathcal{M}) = \\int p(D \\mid \\theta, \\mathcal{M}) \\, p(\\theta \\mid \\mathcal{M}) \\, d\\theta\n$$\nFor this problem, the two components of the integrand, the likelihood and the prior, are explicitly defined.\n\nThe likelihood function, $p(D \\mid \\theta, \\mathcal{M})$, is derived from the agent's decision-making process. For a dataset $D = \\{(c_t, r_t)\\}_{t=1}^T$ comprising a sequence of choices $c_t$ and rewards $r_t$, the total likelihood is the product of the probabilities of each individual choice, conditioned on the history of actions and rewards up to that point. This history is encapsulated in the learned action-value estimates, $Q_t$.\n$$\np(D \\mid \\theta, \\mathcal{M}) = \\prod_{t=1}^T p(c_t \\mid Q_t(\\theta), \\kappa)\n$$\nThe calculation of this likelihood necessitates a trial-by-trial simulation of the agent's learning process for a given set of parameters $\\theta$. The process is as follows:\n1. Initialize the action values to zero: $Q_{0,0} = 0.0$ and $Q_{0,1} = 0.0$.\n2. For each trial $t$ from $1$ to $T$:\n    a. The probability of the agent making the observed choice $c_t$ is calculated using the softmax function, with a fixed inverse temperature $\\kappa = 5.0$:\n    $$p(c_t \\mid Q_t, \\kappa) = \\frac{\\exp(\\kappa Q_{t, c_t})}{\\exp(\\kappa Q_{t, 0}) + \\exp(\\kappa Q_{t, 1})}$$\n    b. The value of the chosen action, $Q_{t, c_t}$, is updated according to the rules of the specific model, $\\mathcal{M}_1$ or $\\mathcal{M}_2$. The value of the unchosen action remains unchanged.\n\nThe two models differ in their value update mechanisms:\n- **Model $\\mathcal{M}_1$ (asymmetric learning rates):** This model has parameters $\\theta_1 = (\\alpha_+, \\alpha_-)$. The prediction error is $\\delta_t = r_t - Q_{t,c_t}$. The update rule is $Q_{t+1, c_t} = Q_{t, c_t} + \\alpha_{\\pm} \\delta_t$, where $\\alpha_+$ is used if $\\delta_t \\ge 0$ and $\\alpha_-$ is used if $\\delta_t  0$.\n- **Model $\\mathcal{M}_2$ (reward attenuation):** This model has parameters $\\theta_2 = (\\alpha, \\lambda)$. The reward is first attenuated, $r'_t = \\lambda r_t$. The prediction error is then $\\delta_t = r'_t - Q_{t,c_t}$, and the update rule is $Q_{t+1, c_t} = Q_{t, c_t} + \\alpha \\delta_t$.\n\nFor numerical stability, all likelihood calculations are performed in the logarithmic domain. The total log-likelihood for a given $\\theta$ is the sum of the log-probabilities of each choice: $\\mathcal{L}(\\theta) = \\sum_{t=1}^T \\log p(c_t \\mid Q_t(\\theta), \\kappa)$.\n\nThe prior distributions, $p(\\theta \\mid \\mathcal{M})$, encode our beliefs about the parameters before observing the data. The problem specifies Beta distributions for all parameters:\n- For $\\mathcal{M}_1$: $\\alpha_+ \\sim \\mathrm{Beta}(2,2)$ and $\\alpha_- \\sim \\mathrm{Beta}(2,2)$. The joint prior is $p(\\theta_1 \\mid \\mathcal{M}_1) = p(\\alpha_+ \\mid a=2, b=2) p(\\alpha_- \\mid a=2, b=2)$.\n- For $\\mathcal{M}_2$: $\\alpha \\sim \\mathrm{Beta}(2,2)$ and $\\lambda \\sim \\mathrm{Beta}(3,2)$. The joint prior is $p(\\theta_2 \\mid \\mathcal{M}_2) = p(\\alpha \\mid a=2, b=2) p(\\lambda \\mid a=3, b=2)$.\n\nThe model evidence integral is analytically intractable, so we employ numerical quadrature. The two-dimensional parameter space for each model is discretized into a uniform grid of $N \\times N$ points, with $N=101$. The grid spans the interval $[\\epsilon, 1-\\epsilon]$ for each parameter, where $\\epsilon = 10^{-6}$. The integral is approximated by a sum over all grid points $(\\theta_{ij})$:\n$$p(D \\mid \\mathcal{M}) \\approx \\sum_{i=1}^N \\sum_{j=1}^N p(D \\mid \\theta_{ij}, \\mathcal{M}) p(\\theta_{ij} \\mid \\mathcal{M}) \\Delta^2$$\nwhere $\\Delta = (1 - 2\\epsilon) / (N-1)$ is the grid spacing. To avert underflow and other numerical issues, this sum is computed in the log domain. The log-evidence is:\n$$\n\\log p(D \\mid \\mathcal{M}) \\approx \\log \\left( \\sum_{i,j} \\exp(\\mathcal{L}(\\theta_{ij}) + \\log p(\\theta_{ij} \\mid \\mathcal{M})) \\right) + 2\\log(\\Delta)\n$$\nThe summation term is calculated using the log-sum-exp algorithm, which provides numerical stability.\n\nThe algorithmic implementation follows this principled design. A main function iterates through the three provided datasets. For each dataset, it calls a dedicated function, `calculate_log_evidence`, for each of the two models. This function constructs the parameter grid, and for each point on the grid, it computes the log-prior (using `scipy.stats.beta.logpdf`) and the log-likelihood. The log-likelihood function itself simulates the agent's trial-by-trial behavior. The resulting log-posterior terms (log-likelihood plus log-prior) are aggregated using `scipy.special.logsumexp` to compute the log-evidence. Finally, the Bayes factor is computed from the log-evidences: $\\mathrm{BF}_{1,2} = \\exp(\\log p(D \\mid \\mathcal{M}_1) - \\log p(D \\mid \\mathcal{M}_2))$. The result for each case is then rounded and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\nfrom scipy.special import logsumexp\n\ndef calculate_log_likelihood_m1(data, params, kappa):\n    \"\"\"\n    Calculates the total log-likelihood for Model 1 (asymmetric learning rates).\n    \n    Args:\n        data (tuple): A tuple of (choices, rewards).\n        params (tuple): A tuple of (alpha_pos, alpha_neg).\n        kappa (float): The inverse temperature parameter.\n\n    Returns:\n        float: The total log-likelihood of the data given the parameters.\n    \"\"\"\n    alpha_pos, alpha_neg = params\n    choices, rewards = data\n    \n    Q = np.array([0.0, 0.0])\n    total_log_likelihood = 0.0\n\n    for t in range(len(choices)):\n        c_t, r_t = choices[t], rewards[t]\n        \n        # Calculate log-probability of the choice\n        kappa_Q = kappa * Q\n        log_softmax_den = logsumexp(kappa_Q)\n        log_prob_choice = kappa_Q[c_t] - log_softmax_den\n        total_log_likelihood += log_prob_choice\n        \n        # Update Q-value based on M1's rule\n        delta = r_t - Q[c_t]\n        alpha = alpha_pos if delta = 0.0 else alpha_neg\n        Q[c_t] += alpha * delta\n        \n    return total_log_likelihood\n\ndef calculate_log_likelihood_m2(data, params, kappa):\n    \"\"\"\n    Calculates the total log-likelihood for Model 2 (reward attenuation).\n\n    Args:\n        data (tuple): A tuple of (choices, rewards).\n        params (tuple): A tuple of (alpha, lambda).\n        kappa (float): The inverse temperature parameter.\n\n    Returns:\n        float: The total log-likelihood of the data given the parameters.\n    \"\"\"\n    alpha, lam = params\n    choices, rewards = data\n    \n    Q = np.array([0.0, 0.0])\n    total_log_likelihood = 0.0\n\n    for t in range(len(choices)):\n        c_t, r_t = choices[t], rewards[t]\n        \n        # Calculate log-probability of the choice\n        kappa_Q = kappa * Q\n        log_softmax_den = logsumexp(kappa_Q)\n        log_prob_choice = kappa_Q[c_t] - log_softmax_den\n        total_log_likelihood += log_prob_choice\n        \n        # Update Q-value based on M2's rule\n        r_prime = lam * r_t\n        delta = r_prime - Q[c_t]\n        Q[c_t] += alpha * delta\n        \n    return total_log_likelihood\n\ndef compute_log_evidence(data, priors_ab, likelihood_func, n_grid, epsilon, kappa):\n    \"\"\"\n    Computes the log model evidence via grid-based numerical integration.\n\n    Args:\n        data (tuple): A tuple of (choices, rewards).\n        priors_ab (list): A list of [a,b] for the Beta priors of the two parameters.\n        likelihood_func (function): The function to compute log-likelihood.\n        n_grid (int): The number of points on the grid for each parameter.\n        epsilon (float): The small offset for the grid boundaries.\n        kappa (float): The inverse temperature.\n\n    Returns:\n        float: The log model evidence.\n    \"\"\"\n    grid_points = np.linspace(epsilon, 1.0 - epsilon, n_grid)\n    delta = (1.0 - 2.0 * epsilon) / (n_grid - 1.0)\n    log_delta = np.log(delta)\n\n    priors = [(priors_ab[0][0], priors_ab[0][1]), (priors_ab[1][0], priors_ab[1][1])]\n    \n    log_prior1_vals = beta.logpdf(grid_points, priors[0][0], priors[0][1])\n    log_prior2_vals = beta.logpdf(grid_points, priors[1][0], priors[1][1])\n    \n    # Using a meshgrid is more efficient than nested Python loops\n    p1_grid, p2_grid = np.meshgrid(grid_points, grid_points, indexing='ij')\n\n    log_prior_grid = log_prior1_vals[:, np.newaxis] + log_prior2_vals[np.newaxis, :]\n    \n    # Vectorize the likelihood calculation over the grid\n    # This involves a Python loop, but it's cleaner than passing the whole grid\n    # to the likelihood function, which is stateful (trial-by-trial).\n    log_likelihood_grid = np.zeros_like(p1_grid)\n    for i in range(n_grid):\n        for j in range(n_grid):\n            params = (p1_grid[i, j], p2_grid[i, j])\n            log_likelihood_grid[i, j] = likelihood_func(data, params, kappa)\n\n    log_posterior_terms = log_likelihood_grid + log_prior_grid\n    \n    # Sum over the 2D grid and add the log of the volume element (Delta^2)\n    log_sum_evidence = logsumexp(log_posterior_terms)\n    log_evidence = log_sum_evidence + 2.0 * log_delta\n    \n    return log_evidence\n\ndef solve():\n    \"\"\"\n    Main function to run the model comparison pipeline for the test cases.\n    \"\"\"\n    KAPPA = 5.0\n    N_GRID = 101\n    EPSILON = 1e-6\n\n    test_cases = [\n        # Case 1: mixed rewards and choices\n        ((0,0,1,1,0,1,0,1), (1,0,1,0,1,1,0,0)),\n        # Case 2: boundary, zero rewards\n        ((0,1,0,1,0,1,0,1), (0,0,0,0,0,0,0,0)),\n        # Case 3: consistent reward on one action\n        ((0,0,0,0,1,0,0,0), (1,1,1,1,0,1,1,1)),\n    ]\n\n    priors_m1 = [[2.0, 2.0], [2.0, 2.0]] # (alpha+, alpha-)\n    priors_m2 = [[2.0, 2.0], [3.0, 2.0]] # (alpha, lambda)\n\n    results = []\n    for data in test_cases:\n        # Pre-process data for convenience\n        choices, rewards = np.array(data[0]), np.array(data[1])\n        formatted_data = (choices, rewards)\n\n        # Compute log evidence for Model 1\n        log_evidence_m1 = compute_log_evidence(\n            formatted_data, priors_m1, calculate_log_likelihood_m1,\n            N_GRID, EPSILON, KAPPA\n        )\n\n        # Compute log evidence for Model 2\n        log_evidence_m2 = compute_log_evidence(\n            formatted_data, priors_m2, calculate_log_likelihood_m2,\n            N_GRID, EPSILON, KAPPA\n        )\n        \n        # Compute Bayes Factor BF_{1,2}\n        log_bf_12 = log_evidence_m1 - log_evidence_m2\n        bf_12 = np.exp(log_bf_12)\n        \n        results.append(f\"{bf_12:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}