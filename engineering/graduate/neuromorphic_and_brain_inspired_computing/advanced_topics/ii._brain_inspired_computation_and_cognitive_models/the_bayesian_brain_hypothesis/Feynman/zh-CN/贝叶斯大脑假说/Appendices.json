{
    "hands_on_practices": [
        {
            "introduction": "贝叶斯大脑假说的核心思想是，大脑通过结合先验信念与感官证据来进行概率推断。本练习将引导你完成这一过程的基础数学推导，展示当先验和似然都服从高斯分布时，后验估计如何形成一个“精度加权平均” 。通过这个实践，你将掌握贝叶斯整合的基本机制，理解大脑如何根据信息的可靠性（即精度）来权衡其内部模型和外部世界的数据。",
            "id": "4063569",
            "problem": "在贝叶斯大脑假说中，皮层计算通常被建模为一种概率推断，它将内部生成的预测（先验）与传入的感官证据（似然）相结合，其相对影响由精度（方差的倒数）来调节。考虑一个用于单一潜在原因 $s$ 的神经形态感官估计模块，其先验为 $p(s)=\\mathcal{N}(0,1)$，并有一个标量观测值 $x$，由 $p(x\\mid s)=\\mathcal{N}(s,0.25)$ 条件生成。仅从贝叶斯法则 $p(s\\mid x)\\propto p(x\\mid s)\\,p(s)$ 和精度是方差倒数的定义出发，通过在高斯乘积的指数上显式地进行配方，推导出后验分布 $p(s\\mid x)$。然后，计算给定 $x$ 时 $s$ 的后验均值和后验方差，并将后验均值解释为先验均值和观测值的精度加权平均。\n\n将您的最终答案表示为一个单行矩阵，其中包含后验均值和后验方差，以 $x$ 的函数形式给出精确的符号表达式。无需四舍五入。不涉及物理单位。",
            "solution": "该问题要求基于指定的先验 $p(s)$ 和似然 $p(x \\mid s)$，推导给定观测值 $x$ 时潜变量 $s$ 的后验分布 $p(s \\mid x)$。推导过程必须从贝叶斯法则出发，通过对高斯密度函数乘积的指数进行配方来完成。随后，需要计算并解释后验均值和方差。\n\n给定的分布是：\n1.  $s$ 的先验分布：$p(s) = \\mathcal{N}(\\mu_{prior}, \\sigma_{prior}^2)$，其中均值为 $\\mu_{prior} = 0$，方差为 $\\sigma_{prior}^2 = 1$。\n2.  给定 $s$ 时观测值 $x$ 的似然：$p(x \\mid s) = \\mathcal{N}(s, \\sigma_{like}^2)$，其中方差为 $\\sigma_{like}^2 = 0.25$。请注意，该分布的均值是潜变量 $s$。\n\n一般高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的概率密度函数 (PDF) 由 $f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)$ 给出。\n因此，先验和似然的具体 PDF 分别为：\n$p(s) = \\frac{1}{\\sqrt{2\\pi(1)}} \\exp\\left(-\\frac{(s-0)^2}{2(1)}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{s^2}{2}\\right)$\n$p(x \\mid s) = \\frac{1}{\\sqrt{2\\pi(0.25)}} \\exp\\left(-\\frac{(x-s)^2}{2(0.25)}\\right) = \\frac{1}{\\sqrt{0.5\\pi}} \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right)$\n\n根据贝叶斯法则，后验分布 $p(s \\mid x)$ 与似然和先验的乘积成正比：\n$p(s \\mid x) \\propto p(x \\mid s) p(s)$\n\n代入 PDF 的表达式，我们得到：\n$p(s \\mid x) \\propto \\left[ \\frac{1}{\\sqrt{0.5\\pi}} \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{s^2}{2}\\right) \\right]$\n\n由于我们处理的是比例关系，可以忽略常数归一化因子。后验分布与指数部分成正比：\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{(x-s)^2}{0.5}\\right) \\exp\\left(-\\frac{s^2}{2}\\right) = \\exp\\left(-\\frac{(x-s)^2}{0.5} - \\frac{s^2}{2}\\right)$\n\n让我们分析指数部分，我们将其表示为 $\\Phi(s)$：\n$\\Phi(s) = -\\left(\\frac{(s-x)^2}{0.5} + \\frac{s^2}{2}\\right) = -\\left(2(s-x)^2 + \\frac{s^2}{2}\\right)$\n\n为了确定后验分布的形式，我们展开指数中的项并收集 $s$ 的幂。高斯形式 $\\mathcal{N}(\\mu_{post}, \\sigma_{post}^2)$ 的后验分布将具有 $-\\frac{(s-\\mu_{post})^2}{2\\sigma_{post}^2} + C$ 形式的指数，其中 $C$ 是一个关于 $s$ 的常数。\n$\\Phi(s) = -\\left(2(s^2 - 2sx + x^2) + \\frac{s^2}{2}\\right)$\n$\\Phi(s) = -\\left(2s^2 - 4sx + 2x^2 + \\frac{s^2}{2}\\right)$\n$\\Phi(s) = -\\left(\\left(2 + \\frac{1}{2}\\right)s^2 - 4sx + 2x^2\\right)$\n$\\Phi(s) = -\\left(\\frac{5}{2}s^2 - 4sx + 2x^2\\right)$\n\n现在，我们对包含 $s$ 的项进行配方。一般形式为 $As^2+Bs+C$。我们从涉及 $s$ 的项中提出因子 $A$：$A(s^2 + \\frac{B}{A}s) + C = A\\left(s+\\frac{B}{2A}\\right)^2 + C - \\frac{B^2}{4A}$。\n在我们的表达式中，括号内的项是 $\\frac{5}{2}s^2 - 4sx + 2x^2$。这里，$A = \\frac{5}{2}$ 且 $B = -4x$。\n$\\frac{5}{2}s^2 - 4sx + 2x^2 = \\frac{5}{2}\\left(s^2 - \\frac{2}{5}(4x)s\\right) + 2x^2$\n$= \\frac{5}{2}\\left(s^2 - \\frac{8x}{5}s\\right) + 2x^2$\n$= \\frac{5}{2}\\left[\\left(s - \\frac{4x}{5}\\right)^2 - \\left(\\frac{4x}{5}\\right)^2\\right] + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{5}{2}\\left(\\frac{16x^2}{25}\\right) + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{8x^2}{5} + 2x^2$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{8x^2}{5} + \\frac{10x^2}{5}$\n$= \\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 + \\frac{2x^2}{5}$\n\n将此代回 $\\Phi(s)$ 的表达式中：\n$\\Phi(s) = -\\left(\\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 + \\frac{2x^2}{5}\\right) = -\\frac{5}{2}\\left(s - \\frac{4x}{5}\\right)^2 - \\frac{2x^2}{5}$\n\n因此，后验分布为：\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{1}{2} \\cdot 5 \\left(s - \\frac{4x}{5}\\right)^2\\right) \\exp\\left(-\\frac{2x^2}{5}\\right)$\n\n项 $\\exp(-2x^2/5)$ 相对于 $s$ 是一个常数，可以被吸收到归一化常数中。剩余的表达式具有 $s$ 的高斯 PDF 的形式：\n$p(s \\mid x) \\propto \\exp\\left(-\\frac{(s - \\mu_{post})^2}{2\\sigma_{post}^2}\\right)$\n\n通过将推导出的指数与一般形式进行比较，我们可以确定后验均值 $\\mu_{post}$ 和后验方差 $\\sigma_{post}^2$：\n$\\mu_{post} = \\frac{4x}{5}$\n$\\frac{1}{2\\sigma_{post}^2} = \\frac{5}{2} \\implies \\sigma_{post}^2 = \\frac{1}{5}$\n\n所以，后验分布是一个高斯分布：$p(s \\mid x) = \\mathcal{N}\\left(\\frac{4x}{5}, \\frac{1}{5}\\right)$。\n后验均值为 $\\mu_{post} = \\frac{4x}{5}$。\n后验方差为 $\\sigma_{post}^2 = \\frac{1}{5}$。\n\n为了将后验均值解释为精度加权平均，我们首先定义精度。精度 $\\lambda$ 是方差的倒数，即 $\\lambda = 1/\\sigma^2$。\n先验精度：$\\lambda_{prior} = 1/\\sigma_{prior}^2 = 1/1 = 1$。\n似然精度：$\\lambda_{like} = 1/\\sigma_{like}^2 = 1/0.25 = 4$。\n\n在高斯先验和高斯似然的情况下，后验均值 $\\mu_{post}$ 的通用公式是先验均值 $\\mu_{prior}$ 和数据 $x$（即在 $s=x$ 处评估的似然函数的均值）的精度加权平均：\n$\\mu_{post} = \\frac{\\lambda_{prior}\\mu_{prior} + \\lambda_{like}x}{\\lambda_{prior} + \\lambda_{like}}$\n后验精度是先验精度和似然精度的总和：\n$\\lambda_{post} = \\lambda_{prior} + \\lambda_{like}$\n后验方差是后验精度的倒数：\n$\\sigma_{post}^2 = \\frac{1}{\\lambda_{post}} = \\frac{1}{\\lambda_{prior} + \\lambda_{like}}$\n\n让我们用给定的值和这些通用公式来验证我们的结果：\n$\\mu_{prior} = 0$, $\\lambda_{prior} = 1$, $\\lambda_{like} = 4$.\n$\\mu_{post} = \\frac{(1)(0) + (4)x}{1 + 4} = \\frac{4x}{5}$\n$\\sigma_{post}^2 = \\frac{1}{1 + 4} = \\frac{1}{5}$\n\n结果完全匹配。其解释是，后验均值 $\\mu_{post}$ 是先验关于均值的信念（$\\mu_{prior}=0$）和感官证据（$x$）的加权平均。权重分别是各自的精度。数据的精度为 $4$，而先验的精度为 $1$，因此感官数据对最终估计的影响是先验信念的四倍。这可以通过将后验均值写为以下形式看出：\n$\\mu_{post} = \\left(\\frac{\\lambda_{prior}}{\\lambda_{prior}+\\lambda_{like}}\\right)\\mu_{prior} + \\left(\\frac{\\lambda_{like}}{\\lambda_{prior}+\\lambda_{like}}\\right)x = \\left(\\frac{1}{5}\\right)(0) + \\left(\\frac{4}{5}\\right)x = \\frac{4x}{5}$。\n\n最终答案要求将后验均值和后验方差放在一个单行矩阵中。\n后验均值：$\\frac{4x}{5}$\n后验方差：$\\frac{1}{5}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{4x}{5}  \\frac{1}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "理解了贝叶斯推断的静态数学原理后，一个自然的问题是：神经回路如何动态地实现这一计算？预测编码理论提供了一个有力的框架，它假设大脑通过不断比较预测和感觉输入来更新其内部模型。本练习将抽象的理论转化为一个具体的层流预测编码回路的最小动力学系统，要求你推导其动力学方程并分析其稳定性 。这个过程将帮助你连接高级的计算原理与底层的神经动力学，并探索反馈增益等参数如何影响系统的行为，例如在稳定感知和振荡之间切换。",
            "id": "4063576",
            "problem": "在贝叶斯大脑假说中，皮层回路通常被建模为实现预测编码，其中深层表征单元预测感觉输入，而浅层误差单元则发出失配信号。考虑一个针对单一潜在原因的最小层状预测编码回路，该回路基于线性高斯生成模型和变分自由能 (VFE) 上的梯度流。设生成模型为 $y = w x + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$，并有一个高斯先验 $x \\sim \\mathcal{N}(\\mu,\\pi^{-1})$。定义感知精度 $\\rho = \\sigma^{-2}$ 和先验精度 $\\pi  0$。在预测编码中，误差单元计算精度加权的预测误差的估计值，并向深层表征单元发送反馈。\n\n从上述模型以及VFE被定义为预测误差和与先验均值偏差的二次能量出发，推导一个最小的连续时间层状动力学系统，其中：\n- 浅层误差活动 $e$ 是一个漏积分器，以速率 $\\kappa  0$ 趋向于精度加权的预测误差。\n- 深层表征活动 $x$ 由先验精度和来自误差单元的层状反馈驱动，反馈增益为 $g  0$。\n假设时间已被重新标度，使得深层时间常数等于 $1$，并通过在没有外部驱动（$\\delta y = 0$）的情况下对动力学进行线性化，来分析不动点 $x^{\\star} = \\mu$，$y^{\\star} = w \\mu$ 周围的内部稳定性。\n\n从第一性原理出发，构建以偏差 $\\delta x$ 和 $\\delta e$ 表示的线性化二维动力学系统，并推导其雅可比矩阵。计算该雅可比矩阵的特征值，将其表示为反馈增益 $g$ 以及参数 $\\pi$、$\\rho$、$\\kappa$ 和 $w$ 的函数。确定临界反馈增益 $g_{\\star}$，在该增益下特征值从实数转变为复数（即特征多项式的判别式等于零）。以 $\\pi$、$\\rho$、$\\kappa$ 和 $w$ 的单一闭式解析表达式形式给出 $g_{\\star}$ 的最终答案。不需要进行数值计算，也不需要包含单位。",
            "solution": "问题要求在一个最小预测编码回路模型中推导临界反馈增益 $g_{\\star}$。推导过程首先建立系统的连续时间动力学，然后围绕一个特定的不动点对其进行线性化，最后分析所得雅可比矩阵的特征值。\n\n首先，我们根据所给的描述来形式化动力学系统。该系统由一个活动为 $e$ 的浅层误差单元和一个活动为 $x$ 的深层表征单元组成。\n\n误差单元活动 $e$ 被描述为一个漏积分器，以速率 $\\kappa  0$ 趋向于精度加权的预测误差 $\\rho(y-wx)$。这可以转化为以下常微分方程 (ODE)：\n$$\n\\frac{de}{dt} = \\kappa \\left( \\rho(y-wx) - e \\right)\n$$\n\n表征单元活动 $x$ 被描述为由两个项驱动：来自误差单元的反馈和先验的影响。来自误差单元 $e$ 的反馈具有增益 $g  0$，并由突触权重 $w$ 介导。先验的影响将 $x$ 拉向其均值 $\\mu$，其强度由先验精度 $\\pi  0$ 决定。问题指出该单元的时间常数被重新标度为 $1$。这导出了以下 ODE：\n$$\n\\frac{dx}{dt} = gwe - \\pi(x-\\mu)\n$$\n此处，项 $gwe$ 代表反馈驱动，项 $-\\pi(x-\\mu)$ 代表来自先验的驱动，这与在变分自由能 $F = \\frac{1}{2}\\rho(y-wx)^2 + \\frac{1}{2}\\pi(x-\\mu)^2$ 上进行梯度下降是一致的。\n\n因此，完整的动力学系统为：\n$$\n\\begin{cases}\n\\frac{dx}{dt} = gwe - \\pi(x-\\mu) \\\\\n\\frac{de}{dt} = \\kappa(\\rho(y-wx) - e)\n\\end{cases}\n$$\n\n接下来，我们确定要进行线性化的不动点 $(x^{\\star}, e^{\\star})$。问题指明该点对应于先验均值 $x^{\\star} = \\mu$，这在感觉输入为 $y^{\\star} = w\\mu$ 时发生。我们验证对于 $y = y^{\\star}$，$(x, e) = (\\mu, 0)$ 确实是一个不动点：\n令 $\\frac{dx}{dt} = 0$ 和 $\\frac{de}{dt} = 0$：\n$$\ngwe^{\\star} - \\pi(x^{\\star}-\\mu) = 0\n$$\n$$\n\\kappa(\\rho(y^{\\star}-wx^{\\star}) - e^{\\star}) = 0\n$$\n将 $x^{\\star} = \\mu$ 和 $y^{\\star} = w\\mu$ 代入第二个方程得到：\n$$\n\\kappa(\\rho(w\\mu - w\\mu) - e^{\\star}) = 0 \\implies \\kappa(-e^{\\star}) = 0 \\implies e^{\\star} = 0\n$$\n将 $x^{\\star} = \\mu$ 和 $e^{\\star} = 0$ 代入第一个方程得到：\n$$\ngw(0) - \\pi(\\mu-\\mu) = 0 \\implies 0 = 0\n$$\n条件得到满足，因此 $(x^{\\star}, e^{\\star}) = (\\mu, 0)$ 是正确的不动点。\n\n现在，我们围绕这个不动点对系统进行线性化。我们考虑小偏差 $\\delta x = x - x^{\\star}$ 和 $\\delta e = e - e^{\\star}$。分析是针对“内部稳定性”进行的，这意味着外部输入没有变化，因此 $\\delta y = 0$，即 $y$ 保持在 $y^{\\star} = w\\mu$ 不变。\n偏差的系统为：\n$$ \\frac{d}{dt} \\begin{pmatrix} \\delta x \\\\ \\delta e \\end{pmatrix} = J \\begin{pmatrix} \\delta x \\\\ \\delta e \\end{pmatrix} $$\n其中 $J$ 是在不动点 $(x^{\\star}, e^{\\star}) = (\\mu, 0)$ 处计算的雅可比矩阵：\n$$\nJ = \\begin{pmatrix} \\frac{\\partial}{\\partial x}(\\frac{dx}{dt})  \\frac{\\partial}{\\partial e}(\\frac{dx}{dt}) \\\\ \\frac{\\partial}{\\partial x}(\\frac{de}{dt})  \\frac{\\partial}{\\partial e}(\\frac{de}{dt}) \\end{pmatrix}_{x=\\mu, e=0}\n$$\n偏导数是：\n$$\n\\frac{\\partial}{\\partial x}(gwe - \\pi(x-\\mu)) = -\\pi\n$$\n$$\n\\frac{\\partial}{\\partial e}(gwe - \\pi(x-\\mu)) = gw\n$$\n$$\n\\frac{\\partial}{\\partial x}(\\kappa(\\rho(y-wx) - e)) = -\\kappa \\rho w\n$$\n$$\n\\frac{\\partial}{\\partial e}(\\kappa(\\rho(y-wx) - e)) = -\\kappa\n$$\n这些导数是常数，因此雅可比矩阵为：\n$$\nJ = \\begin{pmatrix} -\\pi  gw \\\\ -\\kappa \\rho w  -\\kappa \\end{pmatrix}\n$$\n\n为了确定不动点的稳定性和性质，我们通过求解特征方程 $\\det(J - \\lambda I) = 0$ 来计算雅可比矩阵的特征值 $\\lambda$：\n$$\n\\det \\begin{pmatrix} -\\pi-\\lambda  gw \\\\ -\\kappa \\rho w  -\\kappa-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(-\\pi-\\lambda)(-\\kappa-\\lambda) - (gw)(-\\kappa \\rho w) = 0\n$$\n$$\n(\\pi+\\lambda)(\\kappa+\\lambda) + g\\kappa\\rho w^2 = 0\n$$\n$$\n\\lambda^2 + (\\pi+\\kappa)\\lambda + (\\pi\\kappa + g\\kappa\\rho w^2) = 0\n$$\n这是一个关于 $\\lambda$ 的二次方程。解由二次公式给出：\n$$\n\\lambda = \\frac{-(\\pi+\\kappa) \\pm \\sqrt{(\\pi+\\kappa)^2 - 4(\\pi\\kappa + g\\kappa\\rho w^2)}}{2}\n$$\n当特征多项式的判别式 $\\Delta = (\\pi+\\kappa)^2 - 4(\\pi\\kappa + g\\kappa\\rho w^2)$ 的符号从正变为负时，特征值从实数转变为复数。临界点发生在判别式为零时。我们求解 $\\Delta = 0$ 时的增益 $g = g_{\\star}$：\n$$\n(\\pi+\\kappa)^2 - 4(\\pi\\kappa + g_{\\star}\\kappa\\rho w^2) = 0\n$$\n展开平方项：\n$$\n\\pi^2 + 2\\pi\\kappa + \\kappa^2 - 4\\pi\\kappa - 4g_{\\star}\\kappa\\rho w^2 = 0\n$$\n简化关于 $\\pi$ 和 $\\kappa$ 的多项式：\n$$\n\\pi^2 - 2\\pi\\kappa + \\kappa^2 - 4g_{\\star}\\kappa\\rho w^2 = 0\n$$\n识别出完全平方：\n$$\n(\\pi-\\kappa)^2 - 4g_{\\star}\\kappa\\rho w^2 = 0\n$$\n现在，我们分离出 $g_{\\star}$：\n$$\n4g_{\\star}\\kappa\\rho w^2 = (\\pi - \\kappa)^2\n$$\n假设 $\\kappa, \\rho, w$ 均不为零，我们可以解出 $g_{\\star}$：\n$$\ng_{\\star} = \\frac{(\\pi - \\kappa)^2}{4\\kappa\\rho w^2}\n$$\n这个表达式给出了临界反馈增益，在该增益下，系统的特征值从实数转变为复数，标志着动力学从一个稳定节点转变为一个稳定螺线点（阻尼振荡）。",
            "answer": "$$\\boxed{\\frac{(\\pi - \\kappa)^{2}}{4\\kappa\\rho w^{2}}}$$"
        },
        {
            "introduction": "在处理真实世界的复杂性时，精确的贝叶斯推断往往是计算上不可行的。为了解决这个问题，大脑可能采用近似推断方法，其中变分推断（Variational Inference）是一个核心候选方案。它将推断问题转化为一个优化问题：寻找一个简单的分布来最好地逼近真实的后验分布。本练习将带你深入变分推断的核心，通过推导一个简单生成模型的证据下界（ELBO）及其相对于变分参数的梯度 。掌握ELBO的计算是理解大脑如何通过优化其内部信念来学习和推断的关键一步。",
            "id": "4063529",
            "problem": "考虑一个标量潜因 $s$，它在一个与贝叶斯大脑假说一致的、受大脑启发的生成模型中生成一个标量感觉数据 $x$。大脑对潜因的内部先验是高斯分布，$p(s)=\\mathcal{N}(0,\\sigma_{s}^{2})$，感觉似然也是高斯分布，$p(x \\mid s)=\\mathcal{N}(s,\\sigma_{n}^{2})$，其中 $\\sigma_{s}^{2}0$ 和 $\\sigma_{n}^{2}0$ 是固定的方差，分别代表先验不确定性和感觉噪声。大脑维持一个变分后验 $q(s)=\\mathcal{N}(\\mu_{q},\\sigma_{q}^{2})$，其自由参数为 $\\mu_{q}\\in\\mathbb{R}$ 和 $\\sigma_{q}0$。\n\n从贝叶斯法则 $p(s \\mid x)\\propto p(x \\mid s)p(s)$ 和证据下界 (ELBO) 的定义，即 $\\mathcal{L}(q)=\\mathbb{E}_{q(s)}\\left[\\ln p(x,s)-\\ln q(s)\\right]$ 出发，推导出 $\\mathcal{L}(q)$ 作为 $\\mu_{q}$、$\\sigma_{q}$、$x$、$\\sigma_{s}$ 和 $\\sigma_{n}$ 的函数的解析表达式。然后，在保持观测值 $x$ 固定的情况下，计算 $\\mathcal{L}(q)$ 相对于 $\\mu_{q}$ 和 $\\sigma_{q}$ 的梯度。\n\n假设所有的期望都是在 $q(s)=\\mathcal{N}(\\mu_{q},\\sigma_{q}^{2})$ 下精确计算的，并且对数是自然对数。在首次出现时定义您使用的任何缩写词；例如，证据下界 (ELBO) 和 Kullback–Leibler 散度 (KLD)。使用 `pmatrix` 环境，将您的最终梯度向量表示为行矩阵形式的单个封闭形式解析表达式。不需要四舍五入，也不涉及物理单位。",
            "solution": "该问题是有效的，因为它在贝叶斯统计和计算神经科学方面有科学依据，定义完整、问题明确，并且陈述客观。\n\n任务是推导证据下界 (ELBO) 的解析表达式，记为 $\\mathcal{L}(q)$，然后计算其关于变分参数 $\\mu_{q}$ 和 $\\sigma_{q}$ 的梯度。\n\n给定的量是：\n- 潜因 $s$ 的先验分布：$p(s) = \\mathcal{N}(s; 0, \\sigma_{s}^{2})$\n- 感觉似然：$p(x \\mid s) = \\mathcal{N}(x; s, \\sigma_{n}^{2})$\n- 变分后验分布：$q(s) = \\mathcal{N}(s; \\mu_{q}, \\sigma_{q}^{2})$\n\nELBO 定义为：\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(s)}\\left[\\ln p(x, s) - \\ln q(s)\\right]\n$$\n使用概率的乘法法则，$p(x,s) = p(x \\mid s)p(s)$，我们可以将 ELBO 重写为：\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s) + \\ln p(s) - \\ln q(s)\\right]\n$$\n这个表达式可以分为两部分：一个期望对数似然项和一个与 Kullback-Leibler 散度 (KLD) 相关的项。\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right] - \\mathbb{E}_{q(s)}\\left[\\ln q(s) - \\ln p(s)\\right]\n$$\n第二项是 $q(s)$ 和 $p(s)$ 之间的 KLD：\n$$\n\\mathrm{KLD}\\left(q(s) \\parallel p(s)\\right) = \\mathbb{E}_{q(s)}\\left[\\ln\\frac{q(s)}{p(s)}\\right]\n$$\n因此，ELBO 可以表示为：\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right] - \\mathrm{KLD}\\left(q(s) \\parallel p(s)\\right)\n$$\n我们现在将分别计算每一项。\n\n首先，我们计算期望对数似然，$\\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right]$。\n似然的概率密度函数 (PDF) 是：\n$$\np(x \\mid s) = \\frac{1}{\\sqrt{2\\pi\\sigma_{n}^{2}}} \\exp\\left(-\\frac{(x - s)^{2}}{2\\sigma_{n}^{2}}\\right)\n$$\n取自然对数，我们得到：\n$$\n\\ln p(x \\mid s) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{(x - s)^{2}}{2\\sigma_{n}^{2}}\n$$\n现在，我们对 $s \\sim q(s) = \\mathcal{N}(\\mu_{q}, \\sigma_{q}^{2})$ 取期望：\n$$\n\\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right] = \\mathbb{E}_{q(s)}\\left[-\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{x^{2} - 2xs + s^{2}}{2\\sigma_{n}^{2}}\\right]\n$$\n$$\n= -\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{1}{2\\sigma_{n}^{2}}\\mathbb{E}_{q(s)}\\left[x^{2} - 2xs + s^{2}\\right]\n$$\n使用期望的性质 $\\mathbb{E}_{q(s)}[s] = \\mu_{q}$ 和 $\\mathbb{E}_{q(s)}[s^{2}] = \\mathrm{Var}_{q(s)}[s] + (\\mathbb{E}_{q(s)}[s])^{2} = \\sigma_{q}^{2} + \\mu_{q}^{2}$：\n$$\n\\mathbb{E}_{q(s)}\\left[\\ln p(x \\mid s)\\right] = -\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{1}{2\\sigma_{n}^{2}}\\left(x^{2} - 2x\\mu_{q} + (\\sigma_{q}^{2} + \\mu_{q}^{2})\\right)\n$$\n$$\n= -\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{(x - \\mu_{q})^{2} + \\sigma_{q}^{2}}{2\\sigma_{n}^{2}}\n$$\n\n其次，我们计算两个单变量高斯分布 $q(s) = \\mathcal{N}(\\mu_{q}, \\sigma_{q}^{2})$ 和 $p(s) = \\mathcal{N}(0, \\sigma_{s}^{2})$ 之间的 KLD。通用公式是：\n$$\n\\mathrm{KLD}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\parallel \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\ln\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\n代入我们的参数（$\\mu_1=\\mu_q, \\sigma_1=\\sigma_q, \\mu_2=0, \\sigma_2=\\sigma_s$）：\n$$\n\\mathrm{KLD}\\left(q(s) \\parallel p(s)\\right) = \\ln\\frac{\\sigma_{s}}{\\sigma_{q}} + \\frac{\\sigma_{q}^{2} + (\\mu_{q} - 0)^{2}}{2\\sigma_{s}^{2}} - \\frac{1}{2}\n$$\n$$\n= \\frac{1}{2}\\ln\\left(\\frac{\\sigma_{s}^{2}}{\\sigma_{q}^{2}}\\right) + \\frac{\\sigma_{q}^{2} + \\mu_{q}^{2}}{2\\sigma_{s}^{2}} - \\frac{1}{2}\n$$\n\n现在，我们合并这些项来得到 ELBO 的完整表达式，$\\mathcal{L}(q)$：\n$$\n\\mathcal{L}(q) = \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_{n}^{2}) - \\frac{(x - \\mu_{q})^{2} + \\sigma_{q}^{2}}{2\\sigma_{n}^{2}}\\right) - \\left(\\frac{1}{2}\\ln\\sigma_{s}^{2} - \\frac{1}{2}\\ln\\sigma_{q}^{2} + \\frac{\\sigma_{q}^{2} + \\mu_{q}^{2}}{2\\sigma_{s}^{2}} - \\frac{1}{2}\\right)\n$$\n$$\n\\mathcal{L}(q) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\sigma_{n}^{2}) - \\frac{(x - \\mu_{q})^{2}}{2\\sigma_{n}^{2}} - \\frac{\\sigma_{q}^{2}}{2\\sigma_{n}^{2}} - \\frac{1}{2}\\ln(\\sigma_{s}^{2}) + \\frac{1}{2}\\ln(\\sigma_{q}^{2}) - \\frac{\\sigma_{q}^{2}}{2\\sigma_{s}^{2}} - \\frac{\\mu_{q}^{2}}{2\\sigma_{s}^{2}} + \\frac{1}{2}\n$$\n这是 $\\mathcal{L}(q)$ 的解析表达式。接下来，我们计算关于 $\\mu_{q}$ 和 $\\sigma_{q}$ 的偏导数。\n\n关于 $\\mu_q$ 的梯度：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mu_{q}} = \\frac{\\partial}{\\partial \\mu_{q}}\\left[-\\frac{(x - \\mu_{q})^{2}}{2\\sigma_{n}^{2}} - \\frac{\\mu_{q}^{2}}{2\\sigma_{s}^{2}}\\right]\n$$\n$$\n= -\\frac{2(x - \\mu_{q})(-1)}{2\\sigma_{n}^{2}} - \\frac{2\\mu_{q}}{2\\sigma_{s}^{2}}\n$$\n$$\n= \\frac{x - \\mu_{q}}{\\sigma_{n}^{2}} - \\frac{\\mu_{q}}{\\sigma_{s}^{2}}\n$$\n这也可以写成 $\\frac{x}{\\sigma_{n}^{2}} - \\mu_{q}\\left(\\frac{1}{\\sigma_{n}^{2}} + \\frac{1}{\\sigma_{s}^{2}}\\right)$。\n\n关于 $\\sigma_q$ 的梯度：\n注意到 $\\frac{1}{2}\\ln(\\sigma_{q}^{2}) = \\ln(\\sigma_{q})$，涉及 $\\sigma_q$ 的项是：\n$$\n-\\frac{\\sigma_{q}^{2}}{2\\sigma_{n}^{2}} + \\ln(\\sigma_{q}) - \\frac{\\sigma_{q}^{2}}{2\\sigma_{s}^{2}}\n$$\n偏导数是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_{q}} = \\frac{\\partial}{\\partial \\sigma_{q}}\\left[-\\frac{\\sigma_{q}^{2}}{2\\sigma_{n}^{2}} + \\ln(\\sigma_{q}) - \\frac{\\sigma_{q}^{2}}{2\\sigma_{s}^{2}}\\right]\n$$\n$$\n= -\\frac{2\\sigma_{q}}{2\\sigma_{n}^{2}} + \\frac{1}{\\sigma_{q}} - \\frac{2\\sigma_{q}}{2\\sigma_{s}^{2}}\n$$\n$$\n= \\frac{1}{\\sigma_{q}} - \\frac{\\sigma_{q}}{\\sigma_{n}^{2}} - \\frac{\\sigma_{q}}{\\sigma_{s}^{2}}\n$$\n这也可以写成 $\\frac{1}{\\sigma_{q}} - \\sigma_{q}\\left(\\frac{1}{\\sigma_{n}^{2}} + \\frac{1}{\\sigma_{s}^{2}}\\right)$。\n\n$\\mathcal{L}(q)$ 关于其参数 $(\\mu_{q}, \\sigma_{q})$ 的梯度向量是 $\\nabla \\mathcal{L}(q) = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial \\mu_{q}}  \\frac{\\partial \\mathcal{L}}{\\partial \\sigma_{q}} \\end{pmatrix}$。\n\n结合结果，梯度由以下行矩阵给出：\n$$\n\\nabla_{\\mu_{q}, \\sigma_{q}} \\mathcal{L}(q) = \\begin{pmatrix} \\frac{x - \\mu_{q}}{\\sigma_{n}^{2}} - \\frac{\\mu_{q}}{\\sigma_{s}^{2}}  \\frac{1}{\\sigma_{q}} - \\frac{\\sigma_{q}}{\\sigma_{n}^{2}} - \\frac{\\sigma_{q}}{\\sigma_{s}^{2}} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{x - \\mu_{q}}{\\sigma_{n}^{2}} - \\frac{\\mu_{q}}{\\sigma_{s}^{2}}  \\frac{1}{\\sigma_{q}} - \\frac{\\sigma_{q}}{\\sigma_{n}^{2}} - \\frac{\\sigma_{q}}{\\sigma_{s}^{2}} \\end{pmatrix}}\n$$"
        }
    ]
}