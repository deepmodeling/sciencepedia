{
    "hands_on_practices": [
        {
            "introduction": "One of the most fundamental models of attention posits that it acts as a gain controller, selectively amplifying task-relevant neural signals. This exercise provides a first-principles analysis of this mechanism's functional consequence. By working through this problem , you will derive how a simple multiplicative gain applied to a signal path can enhance the signal-to-noise ratio, providing a quantitative link between a biophysical process and improved perceptual quality.",
            "id": "4051873",
            "problem": "A neuromorphic attention module applies a multiplicative gain to specific feature channels to prioritize task-relevant inputs. Consider a single-channel linear readout modeled as follows. Let the latent stimulus-aligned signal be a zero-mean, second-order stationary process with power $P_{s} = \\mathbb{E}[s^{2}(t)]$ that is independent of noise. The attention mechanism applies a static, nonnegative gain $g \\geq 0$ to the signal path only, producing an output\n$$\ny_{g}(t) = g\\,s(t) + n(t),\n$$\nwhere $n(t)$ is additive white Gaussian noise (AWGN) with zero mean and variance $\\sigma^{2}$ that is independent of $s(t)$ and is not modulated by the gain. The baseline (no-attention) condition corresponds to $g=1$.\n\nUsing the standard definition of signal-to-noise ratio (SNR) as the ratio of signal power to noise power at the output, derive from first principles an exact, closed-form expression for the SNR improvement factor $R(g)$ defined as\n$$\nR(g) \\triangleq \\frac{\\mathrm{SNR}_{\\text{with gain } g}}{\\mathrm{SNR}_{\\text{baseline}}},\n$$\nexpressed solely in terms of $g$. Then, determine the minimal gain threshold $g_{\\text{th}}$ for which the SNR strictly improves relative to baseline.\n\nExpress your final answer as a single row matrix $\\begin{pmatrix} R(g) & g_{\\text{th}} \\end{pmatrix}$. No numerical approximation is required, and no units are needed for the final expression.",
            "solution": "The task is to derive the signal-to-noise ratio (SNR) improvement factor $R(g)$ and the minimal gain threshold $g_{\\text{th}}$ for which the SNR strictly improves. We begin from the first principles laid out in the problem statement.\n\nThe output of the neuromorphic attention module is given by the linear model:\n$$y_{g}(t) = g\\,s(t) + n(t)$$\nwhere $s(t)$ is the stimulus-aligned signal, $n(t)$ is the additive noise, and $g$ is the multiplicative gain applied only to the signal path. The gain is nonnegative, so $g \\geq 0$.\n\nThe problem specifies that the SNR is defined as the ratio of signal power to noise power at the output.\nFirst, let's identify the signal and noise components at the output. Based on the structure of the model, the signal component is $g\\,s(t)$ and the noise component is $n(t)$.\n\nThe power of the signal component at the output, which we denote $P_{\\text{signal}, g}$, is the expected value of its square. Since $g$ is a deterministic constant:\n$$P_{\\text{signal}, g} = \\mathbb{E}\\left[ (g\\,s(t))^{2} \\right] = \\mathbb{E}\\left[ g^{2}s^{2}(t) \\right] = g^{2}\\mathbb{E}\\left[ s^{2}(t) \\right]$$\nThe problem states that the latent signal $s(t)$ is a zero-mean process with power $P_{s} = \\mathbb{E}[s^{2}(t)]$. Substituting this into our expression, we get:\n$$P_{\\text{signal}, g} = g^{2}P_{s}$$\n\nThe power of the noise component at the output, $P_{\\text{noise}, g}$, is the power of the additive noise process $n(t)$. The problem states that $n(t)$ is additive white Gaussian noise (AWGN) with zero mean and variance $\\sigma^{2}$. The power of a zero-mean process is equal to its variance. Therefore:\n$$P_{\\text{noise}, g} = \\mathbb{E}\\left[ n^{2}(t) \\right] = \\sigma^{2}$$\nAs specified, the noise power is not modulated by the gain $g$.\n\nNow, we can express the SNR for a given gain $g$, denoted $\\mathrm{SNR}_{\\text{with gain } g}$, as the ratio of the signal power to the noise power at the output:\n$$\\mathrm{SNR}_{\\text{with gain } g} = \\frac{P_{\\text{signal}, g}}{P_{\\text{noise}, g}} = \\frac{g^{2}P_{s}}{\\sigma^{2}}$$\n\nNext, we establish the baseline SNR. The baseline condition is defined for a gain of $g=1$. Substituting $g=1$ into the general SNR expression yields the baseline SNR, $\\mathrm{SNR}_{\\text{baseline}}$:\n$$\\mathrm{SNR}_{\\text{baseline}} = \\frac{1^{2}P_{s}}{\\sigma^{2}} = \\frac{P_{s}}{\\sigma^{2}}$$\n\nThe SNR improvement factor, $R(g)$, is defined as the ratio of the SNR with gain $g$ to the baseline SNR:\n$$R(g) \\triangleq \\frac{\\mathrm{SNR}_{\\text{with gain } g}}{\\mathrm{SNR}_{\\text{baseline}}}$$\nSubstituting the expressions derived above:\n$$R(g) = \\frac{\\frac{g^{2}P_{s}}{\\sigma^{2}}}{\\frac{P_{s}}{\\sigma^{2}}}$$\nAssuming that the signal power $P_{s}$ and noise power $\\sigma^{2}$ are both greater than zero (which is implicit for a meaningful SNR calculation), we can cancel the term $\\frac{P_{s}}{\\sigma^{2}}$:\n$$R(g) = g^{2}$$\nThis is the required closed-form expression for the SNR improvement factor expressed solely in terms of $g$.\n\nFor the second part of the problem, we must find the minimal gain threshold $g_{\\text{th}}$ for which the SNR strictly improves relative to the baseline. A strict improvement occurs when the SNR with gain $g$ is strictly greater than the baseline SNR, which is equivalent to the condition $R(g) > 1$.\nUsing our derived expression for $R(g)$, we have the inequality:\n$$g^{2} > 1$$\nWe are given that the gain $g$ is a nonnegative value, i.e., $g \\geq 0$. The solution to the inequality $g^{2} > 1$ under the constraint $g \\geq 0$ is:\n$$g > 1$$\nThe problem asks for the minimal gain threshold, $g_{\\text{th}}$, which is the boundary point of the region of strict improvement. For any gain $g$ strictly greater than this threshold, the SNR must improve. The condition $g>1$ implies that this threshold value is:\n$$g_{\\text{th}} = 1$$\nAt $g=1$, $R(1)=1$, representing no change from baseline. For any $g>1$, $R(g)>1$, representing a strict improvement. For any $0 \\leq g < 1$, $R(g)<1$, representing a degradation. Thus, $g_{\\text{th}}=1$ is the threshold.\n\nThe final answer requires expressing $R(g)$ and $g_{\\text{th}}$ as a single row matrix.\nThe two results are $R(g) = g^{2}$ and $g_{\\text{th}} = 1$.\nThe row matrix is $\\begin{pmatrix} g^{2} & 1 \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} g^{2} & 1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving beyond simple signal amplification, a powerful modern perspective models the brain as a Bayesian inference engine. Within this framework, attention is often hypothesized to increase the *precision* of sensory evidence, effectively making measurements more reliable. This practice  asks you to explore this idea by calculating how attentional modulation of measurement precision impacts the brain's posterior belief, demonstrating its role in reducing uncertainty about the state of the world.",
            "id": "4051876",
            "problem": "A neuromorphic observer implements Bayesian inference over a single latent scalar state $x$ that represents the instantaneous activation level in a putative global workspace model of attention and consciousness. The observer has a Gaussian prior $p(x) = \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$ reflecting slow dynamics and homeostatic constraints of cortical activity. A sensory measurement $y$ is generated by a linear Gaussian observation model $y = x + \\eta$, where the measurement noise $\\eta \\sim \\mathcal{N}(0, \\sigma_{y}^{2})$ arises from stochastic spiking and synaptic transmission variability.\n\nAttention is hypothesized to modulate the measurement precision by a multiplicative factor $k \\geq 1$, consistent with gain control and stochastic resonance mechanisms in neuromorphic circuits, so that under attention the measurement noise variance becomes $\\sigma_{y}^{2}/k$. Define the prior precision $\\tau_{0} = 1/\\sigma_{0}^{2}$ and the baseline (pre-attention) measurement precision $\\tau_{y} = 1/\\sigma_{y}^{2}$.\n\nStarting only from Bayesâ€™ theorem and the probability density function of the Gaussian distribution, derive the posterior precision as a function of $k$ and subsequently derive the posterior variance $V(k)$ of $x$ given $y$ under attention. Express your final answer as a single closed-form analytic expression for $V(k)$ in terms of $\\tau_{0}$, $\\tau_{y}$, and $k$. No numerical evaluation is required, and no units are needed for the final expression.",
            "solution": "The problem asks for the derivation of the posterior variance, denoted as $V(k)$, of a latent state $x$ given a sensory measurement $y$, under an attentional modulation factor $k$. The derivation must start from Bayes' theorem and the definition of the Gaussian probability density function (PDF).\n\nFirst, let's establish the prior and the likelihood distributions.\n\nThe prior distribution of the latent state $x$ is given as a Gaussian:\n$$p(x) = \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$$\nThe PDF for this distribution is proportional to:\n$$p(x) \\propto \\exp\\left(-\\frac{(x - \\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\n\nThe sensory measurement $y$ is generated by the linear Gaussian observation model $y = x + \\eta$. The measurement noise $\\eta$ is a Gaussian random variable. Under attentional modulation, the noise distribution is given by $\\eta \\sim \\mathcal{N}(0, \\sigma_{y}^{2}/k)$. From the observation model, for a given state $x$, the measurement $y$ is distributed as a Gaussian centered at $x$ with variance $\\sigma_{y}^{2}/k$. This defines the likelihood function $p(y|x)$:\n$$p(y|x) = \\mathcal{N}(x, \\sigma_{y}^{2}/k)$$\nAs a function of $x$ for a fixed measurement $y$, the likelihood is:\n$$p(y|x) \\propto \\exp\\left(-\\frac{(y - x)^{2}}{2(\\sigma_{y}^{2}/k)}\\right) = \\exp\\left(-\\frac{k(x - y)^{2}}{2\\sigma_{y}^{2}}\\right)$$\n\nAccording to Bayes' theorem, the posterior probability distribution of $x$ given $y$, $p(x|y)$, is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nSubstituting the expressions for the prior and the likelihood:\n$$p(x|y) \\propto \\exp\\left(-\\frac{k(x - y)^{2}}{2\\sigma_{y}^{2}}\\right) \\exp\\left(-\\frac{(x - \\mu_{0})^{2}}{2\\sigma_{0}^{2}}\\right)$$\nWe can combine the exponents:\n$$p(x|y) \\propto \\exp\\left( -\\left[ \\frac{k(x - y)^{2}}{2\\sigma_{y}^{2}} + \\frac{(x - \\mu_{0})^{2}}{2\\sigma_{0}^{2}} \\right] \\right)$$\n\nThe problem defines precision as the reciprocal of variance. The prior precision is $\\tau_{0} = 1/\\sigma_{0}^{2}$, and the baseline measurement precision is $\\tau_{y} = 1/\\sigma_{y}^{2}$. We can define the effective measurement precision under attention as $\\tau_{y,k} = 1/(\\sigma_{y}^{2}/k) = k/\\sigma_{y}^{2} = k\\tau_{y}$.\n\nSubstituting these precision terms into the exponent of the posterior distribution:\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ k\\tau_{y}(x - y)^{2} + \\tau_{0}(x - \\mu_{0})^{2} \\right] \\right)$$\nThe product of two Gaussian distributions is another Gaussian. Therefore, the posterior distribution $p(x|y)$ is a Gaussian, which we can denote as $\\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$. The PDF of this posterior is proportional to:\n$$p(x|y) \\propto \\exp\\left(-\\frac{(x - \\mu_{\\text{post}})^{2}}{2\\sigma_{\\text{post}}^{2}}\\right) = \\exp\\left(-\\frac{1}{2}\\tau_{\\text{post}}(x - \\mu_{\\text{post}})^{2}\\right)$$\nwhere $\\tau_{\\text{post}} = 1/\\sigma_{\\text{post}}^{2}$ is the posterior precision.\n\nTo find the posterior variance (or precision), we can analyze the terms in the exponent of our derived posterior expression that are quadratic in $x$. Let's expand the terms inside the exponent's brackets:\n$$k\\tau_{y}(x^{2} - 2xy + y^{2}) + \\tau_{0}(x^{2} - 2x\\mu_{0} + \\mu_{0}^{2})$$\nCollecting the terms with $x^{2}$:\n$$(k\\tau_{y} + \\tau_{0})x^{2} - 2(k\\tau_{y}y + \\tau_{0}\\mu_{0})x + (\\text{terms independent of } x)$$\nThe general form of the exponent of a Gaussian PDF in terms of its precision $\\tau_{\\text{post}}$ is $-\\frac{1}{2}(\\tau_{\\text{post}}x^2 - 2\\tau_{\\text{post}}\\mu_{\\text{post}}x + \\dots)$. By comparing the coefficient of the $x^{2}$ term in our derived posterior with this general form, we can identify the posterior precision.\nThe coefficient of $x^2$ in the exponent is $-\\frac{1}{2}(k\\tau_{y} + \\tau_{0})$.\nBy comparison, this must be equal to $-\\frac{1}{2}\\tau_{\\text{post}}$.\nTherefore, the posterior precision, as a function of the attentional gain $k$, is:\n$$\\tau_{\\text{post}}(k) = \\tau_{0} + k\\tau_{y}$$\nThis result illustrates a fundamental principle of Bayesian fusion for Gaussian variables: the precision of the posterior is the sum of the precisions of the prior and the likelihood.\n\nThe problem asks for the posterior variance, $V(k)$. The variance is the reciprocal of the precision:\n$$V(k) = \\sigma_{\\text{post}}^{2}(k) = \\frac{1}{\\tau_{\\text{post}}(k)}$$\nSubstituting the expression for $\\tau_{\\text{post}}(k)$:\n$$V(k) = \\frac{1}{\\tau_{0} + k\\tau_{y}}$$\nThis is the final closed-form analytic expression for the posterior variance $V(k)$ in terms of the prior precision $\\tau_{0}$, the baseline measurement precision $\\tau_{y}$, and the attentional modulation factor $k$.",
            "answer": "$$\\boxed{\\frac{1}{\\tau_{0} + k\\tau_{y}}}$$"
        },
        {
            "introduction": "Attention is not a static state but a dynamic process involving competition and coordination across neural populations, which are subject to biophysical constraints like communication delays. This exercise delves into the dynamics of attentional switching by modeling it as a system with delayed feedback. Through a linear stability analysis of a delayed differential equation , you will determine the critical time delay at which a stable attentional focus can give way to oscillatory instabilities, a concept crucial for understanding rhythmic attentional sampling and pathological circuit dynamics.",
            "id": "4051846",
            "problem": "Consider a minimal mean-field model of attention switching between two competing cortical populations coupled through delayed inhibition. Near a symmetric steady state, the deviation of the antisymmetric competition mode, denoted by $a(t)$, is approximated by a linear delayed differential equation (DDE) with a single discrete delay:\n$$\n\\frac{d a(t)}{d t} \\;=\\; -\\alpha\\, a(t) \\;-\\; \\beta\\, a\\!\\left(t-\\tau\\right),\n$$\nwhere $\\,\\alpha>0\\,$ represents leak (intrinsic decay), $\\,\\beta>0\\,$ represents the effective strength of delayed inhibitory feedback, and $\\,\\tau \\ge 0\\,$ is the total synaptic-conduction delay. The symmetric steady state is linearly stable when all characteristic roots have negative real part and loses stability through an oscillatory instability when a pair of complex-conjugate roots crosses the imaginary axis (Hopf bifurcation).\n\nStarting only from the linearization and the exponential trial solution method for linear DDEs, perform a linear stability analysis of the steady state and determine the smallest positive delay $\\,\\tau_c\\,$ at which an oscillatory instability first occurs. Use radians for all angles. Then, evaluate the critical delay for the parameter values $\\,\\alpha = 50\\,\\text{s}^{-1}\\,$ and $\\,\\beta = 80\\,\\text{s}^{-1}\\,$ and provide the smallest positive $\\,\\tau_c\\,$ that destabilizes the steady state by an oscillatory instability. Express the final answer in milliseconds (ms) and round to four significant figures.",
            "solution": "To perform the linear stability analysis, we begin with the given linear delayed differential equation (DDE) for the antisymmetric competition mode, $a(t)$:\n$$\n\\frac{d a(t)}{d t} = -\\alpha a(t) - \\beta a(t-\\tau)\n$$\nWe seek solutions of the form $a(t) = C \\exp(\\lambda t)$, where $C$ is a constant and $\\lambda \\in \\mathbb{C}$ is a characteristic root (eigenvalue). Substituting this ansatz into the DDE gives:\n$$\n\\lambda C \\exp(\\lambda t) = -\\alpha C \\exp(\\lambda t) - \\beta C \\exp(\\lambda(t-\\tau))\n$$\nFor a non-trivial solution ($C \\neq 0$), we can divide the entire equation by $C \\exp(\\lambda t)$:\n$$\n\\lambda = -\\alpha - \\beta \\exp(-\\lambda \\tau)\n$$\nThis leads to the characteristic equation for the system:\n$$\n\\lambda + \\alpha + \\beta \\exp(-\\lambda \\tau) = 0\n$$\nThe symmetric steady state $a=0$ is stable if and only if all roots $\\lambda$ of this equation have a negative real part, i.e., $\\text{Re}(\\lambda) < 0$. An oscillatory instability, also known as a Hopf bifurcation, occurs when a pair of complex-conjugate roots crosses the imaginary axis. At the point of bifurcation, there exists a purely imaginary root $\\lambda = i\\omega$, where $\\omega \\in \\mathbb{R}$ is the angular frequency of the oscillations. Without loss of generality, we consider $\\omega > 0$.\n\nWe substitute $\\lambda = i\\omega$ into the characteristic equation:\n$$\ni\\omega + \\alpha + \\beta \\exp(-i\\omega\\tau) = 0\n$$\nUsing Euler's formula, $\\exp(-i\\omega\\tau) = \\cos(\\omega\\tau) - i\\sin(\\omega\\tau)$, we can expand the equation:\n$$\ni\\omega + \\alpha + \\beta (\\cos(\\omega\\tau) - i \\sin(\\omega\\tau)) = 0\n$$\nTo solve this complex equation, we separate it into its real and imaginary parts:\n$$\n(\\alpha + \\beta \\cos(\\omega\\tau)) + i(\\omega - \\beta \\sin(\\omega\\tau)) = 0\n$$\nBoth the real and imaginary parts must be zero:\n\\begin{enumerate}\n    \\item Real part: $\\alpha + \\beta \\cos(\\omega\\tau) = 0 \\implies \\cos(\\omega\\tau) = -\\frac{\\alpha}{\\beta}$\n    \\item Imaginary part: $\\omega - \\beta \\sin(\\omega\\tau) = 0 \\implies \\sin(\\omega\\tau) = \\frac{\\omega}{\\beta}$\n\\end{enumerate}\nTo find the critical frequency $\\omega_c$ at the instability onset, we use the fundamental trigonometric identity $\\cos^2(\\theta) + \\sin^2(\\theta) = 1$ with $\\theta = \\omega\\tau$:\n$$\n\\left(-\\frac{\\alpha}{\\beta}\\right)^2 + \\left(\\frac{\\omega}{\\beta}\\right)^2 = 1\n$$\n$$\n\\frac{\\alpha^2}{\\beta^2} + \\frac{\\omega^2}{\\beta^2} = 1\n$$\n$$\n\\alpha^2 + \\omega^2 = \\beta^2\n$$\nSolving for $\\omega^2$, we get:\n$$\n\\omega^2 = \\beta^2 - \\alpha^2\n$$\nFor an oscillatory instability to occur, the frequency $\\omega$ must be a real, positive number. This requires $\\omega^2 > 0$, which implies $\\beta^2 > \\alpha^2$. Since $\\alpha$ and $\\beta$ are given as positive, this condition simplifies to $\\beta > \\alpha$. If this condition is not met, a Hopf bifurcation does not occur, and the steady state remains stable for all $\\tau \\ge 0$. The critical frequency is thus:\n$$\n\\omega_c = \\sqrt{\\beta^2 - \\alpha^2}\n$$\nNow we find the critical delay $\\tau_c$. From the real and imaginary parts, we have expressions for $\\cos(\\omega_c \\tau)$ and $\\sin(\\omega_c \\tau)$. The value of the angle $\\omega_c \\tau$ must satisfy both simultaneously. Since $\\alpha, \\beta, \\omega_c > 0$, we have $\\cos(\\omega_c \\tau) < 0$ and $\\sin(\\omega_c \\tau) > 0$. This places the angle $\\omega_c \\tau$ in the second quadrant.\nThe general solution for $\\tau$ is given by:\n$$\n\\omega_c \\tau = \\arccos\\left(-\\frac{\\alpha}{\\beta}\\right) + 2n\\pi, \\quad n \\in \\{0, 1, 2, \\dots\\}\n$$\nThe smallest positive delay, $\\tau_c$, at which an instability first occurs corresponds to the case where $n=0$:\n$$\n\\omega_c \\tau_c = \\arccos\\left(-\\frac{\\alpha}{\\beta}\\right)\n$$\nTherefore, the critical delay $\\tau_c$ is:\n$$\n\\tau_c = \\frac{1}{\\omega_c} \\arccos\\left(-\\frac{\\alpha}{\\beta}\\right) = \\frac{1}{\\sqrt{\\beta^2 - \\alpha^2}} \\arccos\\left(-\\frac{\\alpha}{\\beta}\\right)\n$$\nNow, we evaluate this expression for the given parameter values $\\alpha = 50\\,\\text{s}^{-1}$ and $\\beta = 80\\,\\text{s}^{-1}$. First, we check the condition for instability: $\\beta = 80 > \\alpha = 50$, so an oscillatory instability is possible.\n\nThe critical frequency $\\omega_c$ is:\n$$\n\\omega_c = \\sqrt{(80\\,\\text{s}^{-1})^2 - (50\\,\\text{s}^{-1})^2} = \\sqrt{6400 - 2500}\\,\\text{s}^{-1} = \\sqrt{3900}\\,\\text{s}^{-1} = 10\\sqrt{39}\\,\\text{s}^{-1}\n$$\nThe argument of the arccosine function is:\n$$\n-\\frac{\\alpha}{\\beta} = -\\frac{50}{80} = -0.625\n$$\nThe value of the arccosine, in radians, is:\n$$\n\\arccos(-0.625) \\approx 2.24553255\\,\\text{rad}\n$$\nNow we calculate $\\tau_c$ in seconds:\n$$\n\\tau_c = \\frac{\\arccos(-0.625)}{10\\sqrt{39}\\,\\text{s}^{-1}} \\approx \\frac{2.24553255}{10 \\times 6.244998}\\,\\text{s} \\approx \\frac{2.24553255}{62.44998}\\,\\text{s} \\approx 0.03595729\\,\\text{s}\n$$\nThe problem requires the answer in milliseconds (ms), so we convert the units:\n$$\n\\tau_c \\approx 0.03595729\\,\\text{s} \\times 1000\\,\\frac{\\text{ms}}{\\text{s}} = 35.95729\\,\\text{ms}\n$$\nFinally, rounding to four significant figures, we get:\n$$\n\\tau_c \\approx 35.96\\,\\text{ms}\n$$",
            "answer": "$$\\boxed{35.96}$$"
        }
    ]
}