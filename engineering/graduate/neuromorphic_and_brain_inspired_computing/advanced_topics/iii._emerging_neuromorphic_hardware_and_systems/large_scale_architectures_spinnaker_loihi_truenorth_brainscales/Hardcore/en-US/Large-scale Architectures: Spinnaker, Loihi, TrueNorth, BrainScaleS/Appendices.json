{
    "hands_on_practices": [
        {
            "introduction": "The first practical step in deploying a spiking neural network is ensuring it fits within the hardware's constraints. This exercise guides you through the process of calculating the minimum number of computational cores required to host a neural layer on various large-scale platforms. By balancing per-core limits on neuron populations and synaptic memory, you will gain hands-on experience with the fundamental resource allocation challenges that shape the design and scalability of neuromorphic applications .",
            "id": "4049234",
            "problem": "Consider a single feedforward layer in a spiking neural network that must be mapped onto one of four large-scale neuromorphic platforms: Spiking Neural Network Architecture (SpiNNaker), Intel Loihi, IBM TrueNorth, and BrainScaleS. The layer comprises $N$ post-synaptic neurons, each with fan-in $k$ from a preceding population. Assume the following scientifically grounded mapping model: each synaptic connection consumes one unit of synapse memory; within any single core, presynaptic axon streams can be shared across all neurons placed on that core; there is no cross-core sharing of synapse memory for the layer; and all neurons in the layer are homogeneous, requiring identical fan-in $k$.\n\nEach platform enforces resource limits at the core level:\n- SpiNNaker: a single core can host at most $n_{\\max}^{\\text{Spi}} = 200$ neurons and at most $s_{\\max}^{\\text{Spi}} = 2.0 \\times 10^{5}$ synapses.\n- Loihi: a single core can host at most $n_{\\max}^{\\text{Loi}} = 1024$ neurons and at most $s_{\\max}^{\\text{Loi}} = 3.0 \\times 10^{5}$ synapses.\n- TrueNorth: a single core can host at most $n_{\\max}^{\\text{TN}} = 256$ neurons, at most $s_{\\max}^{\\text{TN}} = 256^{2}$ synapses, and at most $a_{\\max}^{\\text{TN}} = 256$ distinct presynaptic axon streams. For this platform, presynaptic axon capacity must not be exceeded by the fan-in, i.e., the number of distinct input streams required by the layer per core cannot exceed $a_{\\max}^{\\text{TN}}$.\n- BrainScaleS: a single core can host at most $n_{\\max}^{\\text{BSS}} = 512$ neurons and at most $s_{\\max}^{\\text{BSS}} = 1.5 \\times 10^{5}$ synapses.\n\nAssume $N = 5000$ and $k = 180$. Under the stated model and limits, derive from first principles the minimal number of cores needed to host the entire layer for each platform, accounting jointly for neuron-capacity and synapse-capacity constraints, and validating the presynaptic axon constraint for the IBM TrueNorth case. Express your final answer as a single row matrix in the order $\\left(\\text{SpiNNaker}, \\text{Loihi}, \\text{TrueNorth}, \\text{BrainScaleS}\\right)$. No rounding is required beyond exact counting induced by ceiling and floor operations inherent to minimal resource allocation.",
            "solution": "The objective is to find the minimum number of cores, $C$, required to host a neural layer of $N$ neurons, each with a fan-in of $k$, on four different neuromorphic platforms. The total number of neurons in the layer is $N = 5000$ and the fan-in is $k = 180$. The total number of synaptic connections in the layer is $N \\times k = 5000 \\times 180 = 9.0 \\times 10^5$.\n\nTo find the minimal number of cores, we first must determine the maximum number of neurons from this layer, $n_{\\text{per\\_core}}$, that can be placed on a single core of each platform. This number is limited by the platform-specific constraints on the number of neurons and synapses per core. Let $n$ be the number of neurons on a single core. The constraints are:\n$1$. Neuron Capacity: The number of neurons $n$ cannot exceed the maximum number of neurons per core, $n_{\\max}$.\n$$n \\le n_{\\max}$$\n$2$. Synapse Capacity: The total number of synapses required for $n$ neurons is $n \\times k$, since each neuron has $k$ incoming synapses. This must not exceed the maximum synapses per core, $s_{\\max}$.\n$$n \\times k \\le s_{\\max} \\implies n \\le \\frac{s_{\\max}}{k}$$\nSince $n$ must be an integer, the synapse constraint implies $n \\le \\lfloor \\frac{s_{\\max}}{k} \\rfloor$.\n\nThe maximum number of neurons that can be placed on a single core, $n_{\\text{per\\_core}}$, is the largest integer $n$ that satisfies both constraints simultaneously.\n$$n_{\\text{per\\_core}} = \\min(n_{\\max}, \\lfloor \\frac{s_{\\max}}{k} \\rfloor)$$\nOnce $n_{\\text{per\\_core}}$ is determined, the minimum number of cores $C$ required to host all $N$ neurons is the total number of neurons divided by the maximum number per core, rounded up to the nearest integer. This is calculated using the ceiling function.\n$$C = \\lceil \\frac{N}{n_{\\text{per\\_core}}} \\rceil$$\nWe now apply this framework to each platform using the given values $N=5000$ and $k=180$.\n\n$1$. SpiNNaker:\nThe core limits are $n_{\\max}^{\\text{Spi}} = 200$ neurons and $s_{\\max}^{\\text{Spi}} = 2.0 \\times 10^{5}$ synapses.\nThe maximum number of neurons based on synapse capacity is:\n$$n \\le \\lfloor \\frac{s_{\\max}^{\\text{Spi}}}{k} \\rfloor = \\lfloor \\frac{2.0 \\times 10^5}{180} \\rfloor = \\lfloor 1111.11\\dots \\rfloor = 1111$$\nThe limiting factor is the neuron capacity. Thus, the maximum number of neurons per core is:\n$$n_{\\text{per\\_core}}^{\\text{Spi}} = \\min(200, 1111) = 200$$\nThe minimum number of cores required for SpiNNaker is:\n$$C_{\\text{Spi}} = \\lceil \\frac{N}{n_{\\text{per\\_core}}^{\\text{Spi}}} \\rceil = \\lceil \\frac{5000}{200} \\rceil = \\lceil 25 \\rceil = 25$$\n\n$2$. Loihi:\nThe core limits are $n_{\\max}^{\\text{Loi}} = 1024$ neurons and $s_{\\max}^{\\text{Loi}} = 3.0 \\times 10^{5}$ synapses.\nThe maximum number of neurons based on synapse capacity is:\n$$n \\le \\lfloor \\frac{s_{\\max}^{\\text{Loi}}}{k} \\rfloor = \\lfloor \\frac{3.0 \\times 10^5}{180} \\rfloor = \\lfloor 1666.66\\dots \\rfloor = 1666$$\nThe limiting factor is the neuron capacity. Thus, the maximum number of neurons per core is:\n$$n_{\\text{per\\_core}}^{\\text{Loi}} = \\min(1024, 1666) = 1024$$\nThe minimum number of cores required for Loihi is:\n$$C_{\\text{Loi}} = \\lceil \\frac{N}{n_{\\text{per\\_core}}^{\\text{Loi}}} \\rceil = \\lceil \\frac{5000}{1024} \\rceil = \\lceil 4.8828\\dots \\rceil = 5$$\n\n$3$. TrueNorth:\nThe core limits are $n_{\\max}^{\\text{TN}} = 256$ neurons, $s_{\\max}^{\\text{TN}} = 256^2 = 65536$ synapses, and $a_{\\max}^{\\text{TN}} = 256$ distinct presynaptic axon streams.\nFirst, we validate the axon constraint. The problem states that presynaptic axon streams are shared across neurons on a core. The fan-in $k$ represents the number of distinct presynaptic inputs required. We must check if $k \\le a_{\\max}^{\\text{TN}}$.\n$$180 \\le 256$$\nThe axon constraint is satisfied. We can proceed.\nThe maximum number of neurons based on synapse capacity is:\n$$n \\le \\lfloor \\frac{s_{\\max}^{\\text{TN}}}{k} \\rfloor = \\lfloor \\frac{65536}{180} \\rfloor = \\lfloor 364.08\\dots \\rfloor = 364$$\nThe limiting factor is the neuron capacity. Thus, the maximum number of neurons per core is:\n$$n_{\\text{per\\_core}}^{\\text{TN}} = \\min(256, 364) = 256$$\nThe minimum number of cores required for TrueNorth is:\n$$C_{\\text{TN}} = \\lceil \\frac{N}{n_{\\text{per\\_core}}^{\\text{TN}}} \\rceil = \\lceil \\frac{5000}{256} \\rceil = \\lceil 19.53125 \\rceil = 20$$\n\n$4$. BrainScaleS:\nThe core limits are $n_{\\max}^{\\text{BSS}} = 512$ neurons and $s_{\\max}^{\\text{BSS}} = 1.5 \\times 10^{5}$ synapses.\nThe maximum number of neurons based on synapse capacity is:\n$$n \\le \\lfloor \\frac{s_{\\max}^{\\text{BSS}}}{k} \\rfloor = \\lfloor \\frac{1.5 \\times 10^5}{180} \\rfloor = \\lfloor 833.33\\dots \\rfloor = 833$$\nThe limiting factor is the neuron capacity. Thus, the maximum number of neurons per core is:\n$$n_{\\text{per\\_core}}^{\\text{BSS}} = \\min(512, 833) = 512$$\nThe minimum number of cores required for BrainScaleS is:\n$$C_{\\text{BSS}} = \\lceil \\frac{N}{n_{\\text{per\\_core}}^{\\text{BSS}}} \\rceil = \\lceil \\frac{5000}{512} \\rceil = \\lceil 9.765625 \\rceil = 10$$\n\nThe minimal core requirements for the platforms are $25$ for SpiNNaker, $5$ for Loihi, $20$ for TrueNorth, and $10$ for BrainScaleS. The final answer is expressed as a single row matrix in the specified order.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 25  5  20  10 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond simply fitting a network onto cores, performance at scale depends critically on how network components are physically placed across the hardware. This practice addresses the challenge of topology-aware mapping, where the goal is to minimize inter-chip communication costs by aligning the network's communication graph with the hardware's physical topology . By solving this optimization problem, you will quantify the significant performance gains achievable through intelligent placement versus a naive mapping strategy.",
            "id": "4049231",
            "problem": "You are given a simplified abstraction of communication in large-scale neuromorphic architectures such as Spiking Neural Network (SNN) systems deployed on multi-chip platforms including the Scalable Processor for Neural Networks (SpiNNaker), Intel's Loihi Neuromorphic Research Chip (Loihi), International Business Machines' TrueNorth Architecture (TrueNorth), and the Heidelberg University's BrainScaleS system (BrainScaleS). Each chip is connected via a Network-on-Chip (NoC), which can be idealized as a grid topology. We model communication as an undirected weighted graph and use the number of router traversals (hops) in the NoC to measure inter-chip communication cost. Your task is to construct a topology-aware mapping of neural clusters to chips that minimizes inter-chip traffic, and then quantify the reduction in average hop count relative to a naive baseline mapping.\n\nFundamental base and definitions:\n\n- Let there be $N$ neural clusters labeled $\\{0,1,\\dots,N-1\\}$ and $N$ chips labeled $\\{0,1,\\dots,N-1\\}$ arranged on a two-dimensional grid. Chip $j$ has integer coordinates $(x_j,y_j)$.\n- The communication demand is given by a symmetric nonnegative matrix $W \\in \\mathbb{R}^{N \\times N}$, where $W_{uv}$ is the expected inter-cluster spike traffic between cluster $u$ and cluster $v$ per unit time, for $u \\neq v$, and $W_{uu} = 0$.\n- The hop distance between chip $j$ and chip $k$ is modeled as the Manhattan distance $d_{jk} = |x_j - x_k| + |y_j - y_k|$, which counts the number of router traversals along axis-aligned links in a mesh NoC.\n- A mapping $\\pi: \\{0,\\dots,N-1\\} \\to \\{0,\\dots,N-1\\}$ assigns each cluster index $u$ to a chip index $\\pi(u)$. The weighted average hop count induced by $\\pi$ is\n$$\nH(\\pi) = \\frac{\\sum_{0 \\le u  v \\le N-1} W_{uv}\\, d_{\\pi(u),\\pi(v)}}{\\sum_{0 \\le u  v \\le N-1} W_{uv}}.\n$$\n- The baseline mapping $\\pi_0$ is the identity mapping $\\pi_0(u) = u$, which assigns cluster $u$ to chip $u$ with chips ordered in row-major fashion as specified per test case.\n- The topology-aware mapping should minimize the numerator $\\sum_{uv} W_{uv}\\, d_{\\pi(u),\\pi(v)}$, equivalently minimizing $H(\\pi)$ since the denominator is constant for fixed $W$.\n\nTask: For each test case, compute the reduction in average hop count defined as $\\Delta = H(\\pi_0) - H(\\pi^\\star)$, where $\\pi^\\star$ is an optimal mapping that minimizes $H(\\pi)$.\n\nAngle units do not apply. There are no physical units; hop counts are dimensionless. Express each $\\Delta$ as a decimal rounded to exactly $4$ digits after the decimal point.\n\nTest suite:\n\n- Test case $1$ (happy path, $N=4$):\n  - Chip coordinates (row-major on a $2 \\times 2$ grid): chip $0$: $(0,0)$, chip $1$: $(1,0)$, chip $2$: $(0,1)$, chip $3$: $(1,1)$.\n  - Communication matrix $W$ (symmetric, zero diagonal):\n    $$\n    W =\n    \\begin{bmatrix}\n    0  1  1  12 \\\\\n    1  0  12  1 \\\\\n    1  12  0  1 \\\\\n    12  1  1  0\n    \\end{bmatrix}.\n    $$\n- Test case $2$ (boundary case: uniform all-to-all traffic, $N=4$):\n  - Chip coordinates: chip $0$: $(0,0)$, chip $1$: $(1,0)$, chip $2$: $(0,1)$, chip $3$: $(1,1)$.\n  - Communication matrix $W$:\n    $$\n    W =\n    \\begin{bmatrix}\n    0  5  5  5 \\\\\n    5  0  5  5 \\\\\n    5  5  0  5 \\\\\n    5  5  5  0\n    \\end{bmatrix}.\n    $$\n- Test case $3$ (edge case: chain traffic on a $3 \\times 2$ grid, $N=6$):\n  - Chip coordinates (row-major on a $3 \\times 2$ grid): chip $0$: $(0,0)$, chip $1$: $(1,0)$, chip $2$: $(2,0)$, chip $3$: $(0,1)$, chip $4$: $(1,1)$, chip $5$: $(2,1)$.\n  - Communication matrix $W$ with chain edges of weight $20$ and zero elsewhere:\n    $$\n    W_{01} = 20,\\;\n    W_{12} = 20,\\;\n    W_{23} = 20,\\;\n    W_{34} = 20,\\;\n    W_{45} = 20,\n    $$\n    and for all other $u \\neq v$, $W_{uv} = 0$. Symmetry implies $W_{vu} = W_{uv}$.\n- Test case $4$ (star traffic centered on cluster $0$ on a $4 \\times 2$ grid, $N=8$):\n  - Chip coordinates (row-major on a $4 \\times 2$ grid): chip $0$: $(0,0)$, chip $1$: $(1,0)$, chip $2$: $(2,0)$, chip $3$: $(3,0)$, chip $4$: $(0,1)$, chip $5$: $(1,1)$, chip $6$: $(2,1)$, chip $7$: $(3,1)$.\n  - Communication matrix $W$ with star weights $10$ from cluster $0$ to all others and zero elsewhere:\n    $$\n    W_{0v} = 10 \\text{ for } v \\in \\{1,2,3,4,5,6,7\\}, \\text{ and } W_{uv} = 0 \\text{ for all other pairs } u \\neq v.\n    $$\n    Symmetry implies $W_{v0} = 10$.\n\nProgram requirements:\n\n- Implement the computation of $H(\\pi)$ for a given mapping $\\pi$, chip coordinates, and $W$ using Manhattan hop distances.\n- For each test case, compute $H(\\pi_0)$ for the baseline mapping and $H(\\pi^\\star)$ for an optimal mapping $\\pi^\\star$ that minimizes the weighted sum of hop distances. For the given small $N$, an exhaustive search over all bijections $\\pi$ is permitted.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and rounded to exactly $4$ decimal places, in the order of the test cases: $[ \\Delta_1, \\Delta_2, \\Delta_3, \\Delta_4 ]$.",
            "solution": "The user's problem requires the computation of the reduction in average communication cost, $\\Delta$, achieved by an optimal topology-aware mapping of neural clusters to chips compared to a naive baseline mapping. This problem can be formalized as an instance of the Quadratic Assignment Problem (QAP). While the general QAP is NP-hard, the small number of clusters, $N$, in the provided test cases ($N \\le 8$) allows for an optimal solution to be found through an exhaustive search of all possible mappings.\n\nFirst, we formalize the components of the problem. We are given $N$ neural clusters and $N$ chips. The locations of the chips are provided as coordinates $(x_j, y_j)$, from which we can pre-compute a distance matrix $D \\in \\mathbb{R}^{N \\times N}$. The entry $D_{jk}$ is the Manhattan distance $d_{jk} = |x_j - x_k| + |y_j - y_k|$ between chip $j$ and chip $k$. We are also given a symmetric communication demand matrix $W \\in \\mathbb{R}^{N \\times N}$, where $W_{uv}$ represents the traffic between cluster $u$ and cluster $v$.\n\nA mapping is a bijection $\\pi$ from the set of clusters $\\{0, 1, \\dots, N-1\\}$ to the set of chips $\\{0, 1, \\dots, N-1\\}$. The objective is to find a mapping $\\pi^\\star$ that minimizes the weighted average hop count, defined as:\n$$\nH(\\pi) = \\frac{\\sum_{0 \\le u  v \\le N-1} W_{uv}\\, d_{\\pi(u),\\pi(v)}}{\\sum_{0 \\le u  v \\le N-1} W_{uv}}\n$$\nLet the numerator be the total cost function $C(\\pi) = \\sum_{0 \\le u  v \\le N-1} W_{uv}\\, d_{\\pi(u),\\pi(v)}$ and the denominator be the total communication weight $S_W = \\sum_{0 \\le u  v \\le N-1} W_{uv}$. Since $S_W$ is a constant for a given matrix $W$, minimizing the average hop count $H(\\pi)$ is equivalent to minimizing the total cost $C(\\pi)$.\n\nThe solution strategy involves the following steps for each test case:\n$1$. **Construct the distance matrix $D$**: Using the provided chip coordinates, compute the Manhattan distance $d_{jk}$ for all pairs of chips $(j, k)$. This matrix is symmetric with a zero diagonal.\n$2$. **Compute total weight $S_W$**: Sum the elements in the strict upper triangle of the communication matrix $W$ to find $S_W$. If $S_W=0$, the hop count is undefined but can be treated as $0$, leading to $\\Delta=0$. For all provided test cases, $S_W  0$.\n$3$. **Calculate baseline cost**: The baseline mapping is the identity permutation, $\\pi_0(u) = u$. The corresponding total cost is computed as $C(\\pi_0) = \\sum_{0 \\le u  v \\le N-1} W_{uv} d_{uv}$.\n$4$. **Find the optimal cost**: An exhaustive search is performed over all $N!$ permutations of $\\{0, 1, \\dots, N-1\\}$. For each permutation $\\pi$, we calculate its total cost $C(\\pi)$. The minimum cost found among all permutations is designated as $C(\\pi^\\star) = \\min_{\\pi} C(\\pi)$.\n$5$. **Compute the reduction $\\Delta$**: The reduction in average hop count is the difference between the baseline and optimal average hop counts:\n$$\n\\Delta = H(\\pi_0) - H(\\pi^\\star) = \\frac{C(\\pi_0)}{S_W} - \\frac{C(\\pi^\\star)}{S_W} = \\frac{C(\\pi_0) - C(\\pi^\\star)}{S_W}\n$$\nThe final result for each test case is rounded to exactly $4$ digits after the decimal point.\n\nThe implementation encapsulates this logic. A primary function processes each test case by invoking a core calculation routine. This routine uses `itertools.permutations` to generate all possible mappings $\\pi$ and systematically finds the one that minimizes the total cost $C(\\pi)$, thereby ensuring the correctness of the final computed reduction $\\Delta$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import permutations\n\ndef solve():\n    \"\"\"\n    Solves the topology-aware mapping problem for all test cases.\n    \"\"\"\n\n    def create_w_case3(N):\n        \"\"\"Creates the communication matrix W for Test Case 3.\"\"\"\n        W = np.zeros((N, N))\n        edges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n        weight = 20.0\n        for u, v in edges:\n            W[u, v] = W[v, u] = weight\n        return W\n\n    def create_w_case4(N):\n        \"\"\"Creates the communication matrix W for Test Case 4.\"\"\"\n        W = np.zeros((N, N))\n        weight = 10.0\n        for v in range(1, N):\n            W[0, v] = W[v, 0] = weight\n        return W\n\n    test_cases = [\n        {\n            \"N\": 4,\n            \"coords\": np.array([[0, 0], [1, 0], [0, 1], [1, 1]]),\n            \"W\": np.array([\n                [0, 1, 1, 12],\n                [1, 0, 12, 1],\n                [1, 12, 0, 1],\n                [12, 1, 1, 0]\n            ], dtype=float)\n        },\n        {\n            \"N\": 4,\n            \"coords\": np.array([[0, 0], [1, 0], [0, 1], [1, 1]]),\n            \"W\": np.array([\n                [0, 5, 5, 5],\n                [5, 0, 5, 5],\n                [5, 5, 0, 5],\n                [5, 5, 5, 0]\n            ], dtype=float)\n        },\n        {\n            \"N\": 6,\n            \"coords\": np.array([[0, 0], [1, 0], [2, 0], [0, 1], [1, 1], [2, 1]]),\n            \"W\": create_w_case3(6)\n        },\n        {\n            \"N\": 8,\n            \"coords\": np.array([[0, 0], [1, 0], [2, 0], [3, 0], [0, 1], [1, 1], [2, 1], [3, 1]]),\n            \"W\": create_w_case4(8)\n        }\n    ]\n\n    def calculate_delta(N, coords, W):\n        \"\"\"\n        Calculates the reduction in average hop count for a single test case.\n        \"\"\"\n        # Step 1: Construct the distance matrix D using Manhattan distance\n        D = np.zeros((N, N))\n        for j in range(N):\n            for k in range(j + 1, N):\n                dist = np.sum(np.abs(coords[j] - coords[k]))\n                D[j, k] = D[k, j] = dist\n\n        # Step 2: Sum the upper triangle of W to find the total weight S_W\n        upper_tri_indices = np.triu_indices(N, k=1)\n        total_weight = np.sum(W[upper_tri_indices])\n\n        if total_weight == 0:\n            return 0.0\n\n        # Helper function to compute the cost numerator for a given mapping pi\n        def calculate_cost_numerator(pi):\n            cost_num = 0.0\n            for u, v in zip(upper_tri_indices[0], upper_tri_indices[1]):\n                chip_u, chip_v = pi[u], pi[v]\n                cost_num += W[u, v] * D[chip_u, chip_v]\n            return cost_num\n\n        # Step 3: Calculate baseline cost\n        pi_0 = tuple(range(N))\n        cost_base = calculate_cost_numerator(pi_0)\n\n        # Step 4: Find the optimal cost by exhaustive search\n        min_cost = float('inf')\n        for pi in permutations(range(N)):\n            current_cost = calculate_cost_numerator(pi)\n            if current_cost  min_cost:\n                min_cost = current_cost\n        \n        cost_opt = min_cost\n\n        # Step 5: Compute the reduction Delta\n        delta = (cost_base - cost_opt) / total_weight\n        return delta\n\n    results = []\n    for case in test_cases:\n        delta = calculate_delta(case[\"N\"], case[\"coords\"], case[\"W\"])\n        results.append(f\"{delta:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A primary motivation for building neuromorphic hardware is to achieve radical gains in energy efficiency compared to conventional computing. This exercise provides a first-principles method for quantifying this efficiency by calculating the average energy consumed per spike on different platforms . By applying the fundamental relationship between power, energy, and rate, you will compute and compare this key performance metric, revealing the orders-of-magnitude differences that arise from distinct architectural philosophies.",
            "id": "4049260",
            "problem": "Consider a spiking neural network whose aggregate output spike rate across all neurons is steady at $r=10^{6}\\,\\text{s}^{-1}$. The network is executed on four large-scale neuromorphic platforms: Spiking Neural Network Architecture (SpiNNaker), Intel Loihi neuromorphic research chip (Loihi), IBM TrueNorth neuromorphic application-specific integrated circuit (TrueNorth), and Heidelberg BrainScaleS accelerated mixed-signal neuromorphic system (BrainScaleS). For this workload, the dynamic power attributable to spike processing (i.e., excluding platform-specific idle or baseline power) has been measured as $P_{\\text{SpiNNaker}}=2.4\\,\\text{W}$, $P_{\\text{Loihi}}=0.06\\,\\text{W}$, $P_{\\text{TrueNorth}}=0.02\\,\\text{W}$, and $P_{\\text{BrainScaleS}}=0.15\\,\\text{W}$.\n\nStarting only from the fundamental definitions of power as the time rate of energy transfer and spike rate as the time rate of spike count, derive an expression for the average energy per spike in terms of the steady power and the steady spike rate. Then, using the derived expression and the given values, compute the average energy per spike for each platform. Express the energy per spike in $\\text{J}/\\text{spike}$ and round each value to three significant figures. Present your four numerical results as a single row matrix in the order $(\\text{SpiNNaker},\\,\\text{Loihi},\\,\\text{TrueNorth},\\,\\text{BrainScaleS})$.",
            "solution": "The fundamental definition of instantaneous power is $P(t)=\\frac{dE(t)}{dt}$, where $E(t)$ is the cumulative energy consumed up to time $t$. Under steady-state operation, we assume the power is time-invariant so that $P(t)=P$ is constant over the interval of interest. The spike rate is defined as $r(t)=\\frac{dN(t)}{dt}$, where $N(t)$ is the cumulative spike count up to time $t$. Under steady-state operation, the spike rate is also time-invariant so that $r(t)=r$ is constant.\n\nThe average energy per spike, denoted $E_{\\text{spike}}$, is the energy associated with a single spike on average. Formally, consider the ratio of the infinitesimal energy increment to the infinitesimal spike count increment,\n$$\nE_{\\text{spike}}=\\frac{dE}{dN}.\n$$\nUsing the chain rule with respect to time,\n$$\n\\frac{dE}{dN}=\\frac{\\frac{dE}{dt}}{\\frac{dN}{dt}}=\\frac{P}{r}.\n$$\nThus, under steady-state power $P$ and steady spike rate $r$, the average energy per spike is\n$$\nE_{\\text{spike}}=\\frac{P}{r}.\n$$\n\nWe now apply this expression to each platform, using $r=10^{6}\\,\\text{s}^{-1}$ and the given dynamic powers. For SpiNNaker,\n$$\nE_{\\text{spike,SpiNNaker}}=\\frac{P_{\\text{SpiNNaker}}}{r}=\\frac{2.4\\,\\text{W}}{10^{6}\\,\\text{s}^{-1}}=2.4\\times 10^{-6}\\,\\text{J}/\\text{spike}.\n$$\nFor Loihi,\n$$\nE_{\\text{spike,Loihi}}=\\frac{P_{\\text{Loihi}}}{r}=\\frac{0.06\\,\\text{W}}{10^{6}\\,\\text{s}^{-1}}=6.0\\times 10^{-8}\\,\\text{J}/\\text{spike}.\n$$\nFor TrueNorth,\n$$\nE_{\\text{spike,TrueNorth}}=\\frac{P_{\\text{TrueNorth}}}{r}=\\frac{0.02\\,\\text{W}}{10^{6}\\,\\text{s}^{-1}}=2.0\\times 10^{-8}\\,\\text{J}/\\text{spike}.\n$$\nFor BrainScaleS,\n$$\nE_{\\text{spike,BrainScaleS}}=\\frac{P_{\\text{BrainScaleS}}}{r}=\\frac{0.15\\,\\text{W}}{10^{6}\\,\\text{s}^{-1}}=1.5\\times 10^{-7}\\,\\text{J}/\\text{spike}.\n$$\n\nRounding each to three significant figures preserves the displayed forms: $2.40\\times 10^{-6}\\,\\text{J}/\\text{spike}$, $6.00\\times 10^{-8}\\,\\text{J}/\\text{spike}$, $2.00\\times 10^{-8}\\,\\text{J}/\\text{spike}$, and $1.50\\times 10^{-7}\\,\\text{J}/\\text{spike}$. The requested output is the row matrix in the order $(\\text{SpiNNaker},\\,\\text{Loihi},\\,\\text{TrueNorth},\\,\\text{BrainScaleS})$.",
            "answer": "$$\\boxed{\\begin{pmatrix}2.40 \\times 10^{-6}  6.00 \\times 10^{-8}  2.00 \\times 10^{-8}  1.50 \\times 10^{-7}\\end{pmatrix}}$$"
        }
    ]
}