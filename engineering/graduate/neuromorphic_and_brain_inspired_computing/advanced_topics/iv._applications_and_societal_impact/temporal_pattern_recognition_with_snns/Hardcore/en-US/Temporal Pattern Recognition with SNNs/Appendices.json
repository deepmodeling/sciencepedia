{
    "hands_on_practices": [
        {
            "introduction": "This exercise provides a foundational analysis of how recurrent SNNs can inherently store and represent temporal information through their dynamics. By linearizing the network's behavior around a fixed point, you will use eigenvalue analysis to determine the stability of this state . This practice connects the abstract mathematical framework of dynamical systems to the concrete function of memory and pattern maintenance in neural circuits.",
            "id": "4063219",
            "problem": "Consider a recurrent Spiking Neural Network (SNN) performing temporal pattern recognition whose macroscopic firing activity is well-approximated by a smooth, population-averaged firing-rate model obtained from the Leaky Integrate-and-Fire (LIF) neuron under diffusion-like input and coarse-graining assumptions. The network dynamics for the firing-rate vector $r(t) \\in \\mathbb{R}^{3}$ are given by the ordinary differential equation\n$$\n\\tau \\,\\frac{d r(t)}{d t} \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big),\n$$\nwhere $\\tau$ is the effective membrane time constant, $W \\in \\mathbb{R}^{3 \\times 3}$ is the recurrent connectivity matrix, $I_{0} \\in \\mathbb{R}^{3}$ is a constant input vector, and $\\phi(\\cdot)$ is the static current-to-rate transfer function applied element-wise. Assume $\\phi$ is the logistic function\n$$\n\\phi(u) \\;=\\; \\frac{1}{1 + \\exp\\!\\big(-\\beta\\,(u - \\theta)\\big)},\n$$\nwith slope $\\beta$ and threshold $\\theta$.\n\nSuppose the constant input $I_{0}$ has been tuned so that the network has a fixed point $r^{\\star} = \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\end{pmatrix}$ satisfying $r^{\\star} = \\phi\\!\\big(W r^{\\star} + I_{0}\\big)$, and the recurrent connectivity is\n$$\nW \\;=\\; \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix}.\n$$\nTake $\\tau = 0.02$ s and $\\beta = 3$. Using first principles of linearization around a fixed point of a smooth dynamical system, derive the linearized dynamics at $r^{\\star}$, compute the Jacobianâ€™s eigenvalues, and thereby determine the stability of temporal pattern representations encoded by the eigenmodes. Express the three eigenvalues of the linearized system in $\\mathrm{s}^{-1}$, and round your final numerical values to three significant figures.",
            "solution": "The problem asks for an analysis of the stability of a fixed point in a recurrent firing-rate neural network model. The analysis proceeds by linearizing the system dynamics around the given fixed point and then computing the eigenvalues of the resulting Jacobian matrix.\n\nThe dynamics of the firing-rate vector $r(t) \\in \\mathbb{R}^{3}$ are given by the ordinary differential equation (ODE):\n$$\n\\tau \\,\\frac{d r(t)}{d t} \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big)\n$$\nLet us define the function $F(r)$ as the right-hand side of the ODE (pre-multiplication by $1/\\tau$):\n$$\nF(r) \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big)\n$$\nSo the system is $\\frac{dr}{dt} = \\frac{1}{\\tau} F(r)$. A fixed point $r^{\\star}$ of the system satisfies $\\frac{dr}{dt} = 0$, which implies $F(r^{\\star}) = 0$, or explicitly:\n$$\nr^{\\star} \\;=\\; \\phi\\!\\big(W\\,r^{\\star} + I_{0}\\big)\n$$\nThe problem provides that $r^{\\star} = \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\end{pmatrix}$ is such a fixed point.\n\nTo analyze the local stability of this fixed point, we linearize the dynamics around $r^{\\star}$. Let $r(t) = r^{\\star} + x(t)$, where $x(t)$ is a small perturbation vector. Substituting this into the system's ODE:\n$$\n\\tau \\,\\frac{d}{d t}\\big(r^{\\star} + x(t)\\big) \\;=\\; -\\,\\big(r^{\\star} + x(t)\\big) \\;+\\; \\phi\\!\\big(W\\,(r^{\\star} + x(t)) + I_{0}\\big)\n$$\nSince $r^{\\star}$ is constant, $\\frac{dr^{\\star}}{dt} = 0$. The equation for the perturbation $x(t)$ becomes:\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;=\\; -\\,r^{\\star} - x(t) \\;+\\; \\phi\\!\\big(W\\,r^{\\star} + I_{0} + W\\,x(t)\\big)\n$$\nWe perform a first-order Taylor expansion of the nonlinear function $\\phi$ around the input evaluated at the fixed point, $u^{\\star} = W\\,r^{\\star} + I_{0}$. For a small perturbation input $\\Delta u = W\\,x(t)$, we have:\n$$\n\\phi(u^{\\star} + \\Delta u) \\;\\approx\\; \\phi(u^{\\star}) + \\phi'(u^{\\star}) \\Delta u\n$$\nwhere the derivative is applied element-wise. Let $D$ be a diagonal matrix whose entries are the derivatives of $\\phi$ evaluated at each component of $u^{\\star}$: $D_{ii} = \\phi'((u^{\\star})_i)$. The expansion in vector form is:\n$$\n\\phi\\!\\big(W\\,r^{\\star} + I_{0} + W\\,x(t)\\big) \\;\\approx\\; \\phi(W\\,r^{\\star} + I_{0}) + D\\,W\\,x(t)\n$$\nSubstituting this back into the perturbation equation:\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;\\approx\\; -\\,r^{\\star} - x(t) + \\phi(W\\,r^{\\star} + I_{0}) + D\\,W\\,x(t)\n$$\nUsing the fixed point condition $r^{\\star} = \\phi(W\\,r^{\\star} + I_{0})$, the equation simplifies to:\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;\\approx\\; -x(t) + D\\,W\\,x(t) \\;=\\; (D\\,W - I) x(t)\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix. The linearized dynamics are therefore:\n$$\n\\frac{d x(t)}{d t} \\;=\\; J\\,x(t), \\quad \\text{where} \\quad J = \\frac{1}{\\tau} (D\\,W - I)\n$$\nThe matrix $J$ is the Jacobian of the system evaluated at the fixed point $r^{\\star}$. The stability of the fixed point is determined by the eigenvalues of $J$.\n\nWe now compute the components of the Jacobian matrix.\nThe transfer function is the logistic function $\\phi(u) = \\frac{1}{1 + \\exp(-\\beta(u-\\theta))}$. Its derivative is:\n$$\n\\phi'(u) = \\beta\\,\\phi(u)\\,\\big(1 - \\phi(u)\\big)\n$$\nAt the fixed point, the input to neuron $i$, denoted $(u^{\\star})_i = (W\\,r^{\\star} + I_{0})_i$, satisfies $\\phi((u^{\\star})_i) = r^{\\star}_i$. Since every component of $r^{\\star}$ is $r^{\\star}_i = 0.4$, the derivative of $\\phi$ for each neuron is the same:\n$$\n\\phi'((u^{\\star})_i) = \\beta\\,r^{\\star}_i\\,(1 - r^{\\star}_i) = 3 \\times 0.4 \\times (1 - 0.4) = 3 \\times 0.4 \\times 0.6 = 0.72\n$$\nLet this derivative value be $d = 0.72$. The matrix $D$ is then $D = d \\cdot I = 0.72 \\cdot I$.\n\nThe Jacobian is $J = \\frac{1}{\\tau}(d\\,W - I)$. We are given $\\tau=0.02\\,\\mathrm{s}$ and the connectivity matrix:\n$$\nW \\;=\\; \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix}\n$$\nThe matrix inside the parenthesis is $M = d\\,W - I$:\n$$\nM = 0.72 \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n0.576  0.216  0.216 \\\\\n0.216  0.576  0.216 \\\\\n0.216  0.216  0.576\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\n$$\nM = \\begin{pmatrix}\n-0.424  0.216  0.216 \\\\\n0.216  -0.424  0.216 \\\\\n0.216  0.216  -0.424\n\\end{pmatrix}\n$$\nThe eigenvalues of the Jacobian $J$ are $\\frac{1}{\\tau}$ times the eigenvalues of $M$. The matrix $M$ has a symmetric structure of the form $c_1 I + c_2 \\mathbf{1}\\mathbf{1}^T$. Such matrices have one eigenvector parallel to the vector of all ones, $\\mathbf{v}_1 = (1, 1, 1)^T$, and the remaining eigenvectors are in the subspace orthogonal to $\\mathbf{v}_1$.\nLet's find the eigenvalue $\\lambda_{M,1}$ corresponding to $\\mathbf{v}_1$:\n$$\nM \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -0.424 + 0.216 + 0.216 \\\\ 0.216 - 0.424 + 0.216 \\\\ 0.216 + 0.216 - 0.424 \\end{pmatrix} = \\begin{pmatrix} 0.008 \\\\ 0.008 \\\\ 0.008 \\end{pmatrix} = 0.008 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nSo, one eigenvalue of $M$ is $\\lambda_{M,1} = 0.008$.\nThe other two eigenvalues are degenerate due to the symmetry. Any vector orthogonal to $(1, 1, 1)^T$, such as $\\mathbf{v}_2 = (1, -1, 0)^T$, is an eigenvector:\n$$\nM \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -0.424 - 0.216 \\\\ 0.216 + 0.424 \\\\ 0.216 - 0.216 \\end{pmatrix} = \\begin{pmatrix} -0.64 \\\\ 0.64 \\\\ 0 \\end{pmatrix} = -0.64 \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nThe corresponding eigenvalue is $\\lambda_{M,2} = -0.64$. Due to symmetry, the third eigenvalue is also $\\lambda_{M,3} = -0.64$.\n\nThe eigenvalues of the Jacobian $J$ are obtained by multiplying the eigenvalues of $M$ by $\\frac{1}{\\tau} = \\frac{1}{0.02} = 50 \\, \\mathrm{s}^{-1}$:\n$$\n\\lambda_1 = 50 \\times \\lambda_{M,1} = 50 \\times 0.008 = 0.4 \\, \\mathrm{s}^{-1}\n$$\n$$\n\\lambda_2 = \\lambda_3 = 50 \\times \\lambda_{M,2} = 50 \\times (-0.64) = -32 \\, \\mathrm{s}^{-1}\n$$\nThe stability of the fixed point is determined by the signs of the real parts of these eigenvalues. We have one positive real eigenvalue, $\\lambda_1 = 0.4$, and two negative real eigenvalues, $\\lambda_2 = \\lambda_3 = -32$. The presence of a positive eigenvalue indicates that the fixed point $r^{\\star}$ is unstable. Specifically, it is a saddle point.\nPerturbations along the direction of the eigenvector corresponding to $\\lambda_1$ (which is the common mode where all rates change together) will grow exponentially, moving the system state away from the fixed point. Perturbations in the two-dimensional subspace spanned by the eigenvectors for $\\lambda_2$ and $\\lambda_3$ (the differential modes) will decay, returning the state to that subspace. Thus, temporal patterns corresponding to the common mode are unstable, while those corresponding to the differential modes are stable.\n\nThe problem requires the three eigenvalues rounded to three significant figures.\n$\\lambda_1 = 0.4$ becomes $0.400$.\n$\\lambda_2 = -32$ becomes $-32.0$.\n$\\lambda_3 = -32$ becomes $-32.0$.\nThe units are $\\mathrm{s}^{-1}$.\n\nThe three eigenvalues of the linearized system are $\\{0.400, -32.0, -32.0\\}$ in units of $\\mathrm{s}^{-1}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.400  -32.0  -32.0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Effective learning in SNNs often involves balancing performance with biological and hardware constraints, such as metabolic energy or power consumption, which are closely linked to spiking activity. This problem explores the critical trade-off between detection accuracy and spike sparsity by formulating and solving an optimization problem with a sparsity regularizer . By deriving the optimal regularization strength, you will gain a deeper understanding of how to tune training objectives to achieve both high performance and computational efficiency.",
            "id": "4063229",
            "problem": "Consider a single-layer Spiking Neural Network (SNN) designed for temporal motif detection in streaming input. The network emits binary spikes $z_t \\in \\{0,1\\}$ at discrete time steps $t$, and a motif is deemed detected if at least one spike occurs inside a motif window of duration $T_m$. For analytical tractability, model the spike train inside any fixed window as a homogeneous Poisson process with constant rate $r$, so that the probability of observing at least one spike in the window is $1 - \\exp(-r T_m)$. Assume a balanced dataset with equal proportions of motif-present and motif-absent segments.\n\nTraining is performed by minimizing a standard negative log-likelihood detection objective augmented with a spike sparsity penalty $\\lambda \\sum_{t} z_t$, where the sum is over the motif window. Under the Poisson model, the expected penalty inside a motif window is $\\lambda \\mathbb{E}\\left[\\sum_{t} z_t\\right] = \\lambda r T_m$. Empirically, near a stationary operating point, the sparsity regularizer induces a linear reduction in the effective rates for motif-present and motif-absent segments, modeled as $r^{+}(\\lambda) = r_0^{+} - k^{+} \\lambda$ and $r^{-}(\\lambda) = r_0^{-} - k^{-} \\lambda$, respectively, for $\\lambda \\ge 0$, with $r_0^{+}  r_0^{-}  0$ and $k^{+}  k^{-}  0$. Assume $\\lambda$ lies in the interval that keeps both $r^{+}(\\lambda)$ and $r^{-}(\\lambda)$ strictly positive.\n\nDefine the motif detection accuracy as the average of the true positive probability on motif-present segments and the true negative probability on motif-absent segments, namely\n$$\nA(\\lambda) = \\frac{1}{2}\\left[1 - \\exp\\!\\big(-T_m r^{+}(\\lambda)\\big)\\right] + \\frac{1}{2}\\exp\\!\\big(-T_m r^{-}(\\lambda)\\big).\n$$\nStarting from the Poisson spiking model and the regularized objective, derive the form of $A(\\lambda)$, analytically determine the value $\\lambda^{\\star}$ that maximizes $A(\\lambda)$, and then compute $A(\\lambda^{\\star})$ for the following scientifically plausible parameters: $T_m = 20\\,\\mathrm{ms}$, $r_0^{+} = 200\\,\\mathrm{s}^{-1}$, $r_0^{-} = 20\\,\\mathrm{s}^{-1}$, $k^{+} = 80\\,\\mathrm{s}^{-1}$ per unit $\\lambda$, and $k^{-} = 10\\,\\mathrm{s}^{-1}$ per unit $\\lambda$. Express the final accuracy as a decimal and round your answer to four significant figures. No units are required for the final accuracy.",
            "solution": "The problem asks us to find the value of the regularization parameter $\\lambda$ that maximizes the motif detection accuracy $A(\\lambda)$ and to compute this maximum accuracy for a given set of parameters.\n\nFirst, we validate the problem statement.\nThe problem is well-defined within the context of computational neuroscience and machine learning.\nThe modeling of spike trains as a Poisson process is a standard and scientifically grounded simplification. The use of a sparsity regularizer is a common technique. The accuracy metric is appropriate for a balanced binary classification task. All required parameters are provided, and their units are consistent. The product of a rate in $\\mathrm{s}^{-1}$ and a time in $\\mathrm{s}$ (after converting ms to s) is dimensionless, as required for the argument of an exponential function. The constraints on the parameters ensure the model is physically plausible (e.g., positive firing rates). The problem is self-contained, consistent, and scientifically sound. Thus, proceeding with a solution is justified.\n\nThe problem statement provides the accuracy function $A(\\lambda)$. We first re-derive it to confirm its structure. The dataset is balanced, meaning motif-present and motif-absent segments are equally likely. The accuracy $A(\\lambda)$ is the average of the true positive probability, $P(\\text{TP})$, and the true negative probability, $P(\\text{TN})$.\n$A(\\lambda) = \\frac{1}{2}P(\\text{TP}) + \\frac{1}{2}P(\\text{TN})$.\nA true positive occurs when at least one spike is detected in a motif-present window. The spike rate is $r^{+}(\\lambda)$. Using the given Poisson model, the probability of zero spikes in a window of duration $T_m$ is $\\exp(-T_m r^{+}(\\lambda))$. Therefore, the probability of at least one spike is $P(\\text{TP}) = 1 - \\exp(-T_m r^{+}(\\lambda))$.\nA true negative occurs when zero spikes are detected in a motif-absent window. The spike rate is $r^{-}(\\lambda)$. The probability of zero spikes is $P(\\text{TN}) = \\exp(-T_m r^{-}(\\lambda))$.\nSubstituting these probabilities into the accuracy formula yields:\n$$A(\\lambda) = \\frac{1}{2}\\left[1 - \\exp(-T_m r^{+}(\\lambda))\\right] + \\frac{1}{2}\\exp(-T_m r^{-}(\\lambda))$$\nThis confirms the expression provided in the problem.\n\nThe effective rates are given as linear functions of $\\lambda$:\n$r^{+}(\\lambda) = r_0^{+} - k^{+} \\lambda$\n$r^{-}(\\lambda) = r_0^{-} - k^{-} \\lambda$\nSubstituting these into the accuracy function:\n$$A(\\lambda) = \\frac{1}{2} - \\frac{1}{2}\\exp(-T_m (r_0^{+} - k^{+} \\lambda)) + \\frac{1}{2}\\exp(-T_m (r_0^{-} - k^{-} \\lambda))$$\nTo find the value of $\\lambda$ that maximizes $A(\\lambda)$, denoted $\\lambda^{\\star}$, we must compute the derivative of $A(\\lambda)$ with respect to $\\lambda$ and set it to zero.\n$$\\frac{dA}{d\\lambda} = \\frac{d}{d\\lambda} \\left[ \\frac{1}{2} - \\frac{1}{2}\\exp(-T_m r_0^{+} + T_m k^{+} \\lambda) + \\frac{1}{2}\\exp(-T_m r_0^{-} + T_m k^{-} \\lambda) \\right]$$\nUsing the chain rule, we obtain:\n$$\\frac{dA}{d\\lambda} = -\\frac{1}{2} (T_m k^{+}) \\exp(-T_m r_0^{+} + T_m k^{+} \\lambda) + \\frac{1}{2} (T_m k^{-}) \\exp(-T_m r_0^{-} + T_m k^{-} \\lambda)$$\nSetting the derivative to zero to find the critical point $\\lambda^{\\star}$:\n$$T_m k^{-} \\exp(-T_m r_0^{-} + T_m k^{-} \\lambda^{\\star}) = T_m k^{+} \\exp(-T_m r_0^{+} + T_m k^{+} \\lambda^{\\star})$$\n$$k^{-} \\exp(-T_m (r_0^{-} - k^{-} \\lambda^{\\star})) = k^{+} \\exp(-T_m (r_0^{+} - k^{+} \\lambda^{\\star}))$$\nTo solve for $\\lambda^{\\star}$, we take the natural logarithm of both sides:\n$$\\ln(k^{-}) - T_m (r_0^{-} - k^{-} \\lambda^{\\star}) = \\ln(k^{+}) - T_m (r_0^{+} - k^{+} \\lambda^{\\star})$$\n$$\\ln(k^{-}) - T_m r_0^{-} + T_m k^{-} \\lambda^{\\star} = \\ln(k^{+}) - T_m r_0^{+} + T_m k^{+} \\lambda^{\\star}$$\nRearranging the terms to isolate $\\lambda^{\\star}$:\n$$T_m r_0^{+} - T_m r_0^{-} - (\\ln(k^{+}) - \\ln(k^{-})) = T_m k^{+} \\lambda^{\\star} - T_m k^{-} \\lambda^{\\star}$$\n$$T_m (r_0^{+} - r_0^{-}) - \\ln\\left(\\frac{k^{+}}{k^{-}}\\right) = T_m (k^{+} - k^{-}) \\lambda^{\\star}$$\nThis yields the expression for the optimal regularization parameter:\n$$\\lambda^{\\star} = \\frac{T_m (r_0^{+} - r_0^{-}) - \\ln(k^{+}/k^{-})}{T_m (k^{+} - k^{-})}$$\nThe second derivative test would confirm that this critical point corresponds to a maximum, as $k^{+}  k^{-}$.\n\nThe next step is to compute the maximum accuracy $A(\\lambda^{\\star})$ using the provided parameters:\n$T_m = 20\\,\\mathrm{ms} = 0.02\\,\\mathrm{s}$\n$r_0^{+} = 200\\,\\mathrm{s}^{-1}$\n$r_0^{-} = 20\\,\\mathrm{s}^{-1}$\n$k^{+} = 80\\,\\mathrm{s}^{-1}$ per unit $\\lambda$\n$k^{-} = 10\\,\\mathrm{s}^{-1}$ per unit $\\lambda$\n\nTo simplify the calculation of $A(\\lambda^{\\star})$, we use the first-order condition $k^{-} \\exp(-T_m r^{-}(\\lambda^{\\star})) = k^{+} \\exp(-T_m r^{+}(\\lambda^{\\star}))$. This implies $\\exp(-T_m r^{+}(\\lambda^{\\star})) = \\frac{k^{-}}{k^{+}} \\exp(-T_m r^{-}(\\lambda^{\\star}))$. Substituting this into the accuracy formula:\n$$A(\\lambda^{\\star}) = \\frac{1}{2}\\left[1 - \\frac{k^{-}}{k^{+}}\\exp(-T_m r^{-}(\\lambda^{\\star}))\\right] + \\frac{1}{2}\\exp(-T_m r^{-}(\\lambda^{\\star}))$$\n$$A(\\lambda^{\\star}) = \\frac{1}{2}\\left[1 + \\left(1 - \\frac{k^{-}}{k^{+}}\\right) \\exp(-T_m r^{-}(\\lambda^{\\star}))\\right]$$\n$$A(\\lambda^{\\star}) = \\frac{1}{2}\\left[1 + \\left(1 - \\frac{k^{-}}{k^{+}}\\right) \\exp(-T_m(r_0^{-} - k^{-} \\lambda^{\\star}))\\right]$$\nLet's evaluate the argument of the exponential, $-T_m(r_0^{-} - k^{-} \\lambda^{\\star}) = -T_m r_0^{-} + T_m k^{-} \\lambda^{\\star}$. Substituting the expression for $\\lambda^{\\star}$:\n$$-T_m r_0^{-} + T_m k^{-} \\left(\\frac{T_m (r_0^{+} - r_0^{-}) - \\ln(k^{+}/k^{-})}{T_m (k^{+} - k^{-})}\\right) = -T_m r_0^{-} + \\frac{k^{-}}{k^{+} - k^{-}} \\left(T_m (r_0^{+} - r_0^{-}) - \\ln\\left(\\frac{k^{+}}{k^{-}}\\right)\\right)$$\nThis can be simplified to:\n$$\\frac{-T_m r_0^{-} (k^{+} - k^{-}) + T_m k^{-} (r_0^{+} - r_0^{-}) - k^{-} \\ln(k^{+}/k^{-})}{k^{+} - k^{-}} = \\frac{T_m(k^{-}r_0^{+} - k^{+}r_0^{-}) - k^{-} \\ln(k^{+}/k^{-})}{k^{+} - k^{-}}$$\nNow, we substitute the numerical values into this expression for the exponent.\n$k^{-}r_0^{+} - k^{+}r_0^{-} = (10)(200) - (80)(20) = 2000 - 1600 = 400$.\n$T_m(k^{-}r_0^{+} - k^{+}r_0^{-}) = 0.02 \\times 400 = 8$.\n$k^{+} - k^{-} = 80 - 10 = 70$.\n$k^{-} = 10$.\n$k^{+}/k^{-} = 80/10 = 8$.\nThe exponent is:\n$$\\text{Exponent} = \\frac{8 - 10 \\ln(8)}{70}$$\nNow, we can compute the value of $A(\\lambda^{\\star})$:\n$$A(\\lambda^{\\star}) = \\frac{1}{2}\\left[1 + \\left(1 - \\frac{10}{80}\\right) \\exp\\left(\\frac{8 - 10 \\ln(8)}{70}\\right)\\right]$$\n$$A(\\lambda^{\\star}) = \\frac{1}{2}\\left[1 + \\frac{7}{8} \\exp\\left(\\frac{8 - 10 \\ln(8)}{70}\\right)\\right]$$\nWe compute the numerical value:\n$\\ln(8) \\approx 2.07944154$.\nThe exponent is approximately $\\frac{8 - 10(2.07944154)}{70} = \\frac{8 - 20.7944154}{70} = \\frac{-12.7944154}{70} \\approx -0.18277736$.\n$\\exp(-0.18277736) \\approx 0.8329432$.\n$$A(\\lambda^{\\star}) \\approx \\frac{1}{2} \\left[1 + \\frac{7}{8} (0.8329432)\\right] = \\frac{1}{2} [1 + 0.875 \\times 0.8329432] = \\frac{1}{2} [1 + 0.7288253] = \\frac{1.7288253}{2} = 0.86441265$$\nRounding to four significant figures, the maximum accuracy is $0.8644$.",
            "answer": "$$\\boxed{0.8644}$$"
        },
        {
            "introduction": "A key motivation for SNNs is their efficient implementation on neuromorphic hardware, which often imposes constraints like limited numerical precision. This practice addresses the crucial step of deploying a trained model by analyzing the effects of weight quantization on classification performance. You will derive a sufficient condition to guarantee that a learned temporal filter remains effective after its weights are converted to a low-precision fixed-point format, a vital skill for robust hardware design .",
            "id": "4063206",
            "problem": "Consider a single-layer Spiking Neural Network (SNN) performing two-class temporal pattern recognition via a linear temporal filter. Time is discretized into $L$ bins, and each input pattern is represented as a vector $s \\in \\mathbb{R}^{L}$ of binned spike counts. The learned temporal filter is a weight vector $h \\in \\mathbb{R}^{L}$. The decision variable for a pattern $s$ is the linear response $d = \\sum_{t=1}^{L} h_{t} s_{t}$, and the predicted class is the sign of $d$ with threshold at zero. For a labeled example $(s^{(i)}, y^{(i)})$ with label $y^{(i)} \\in \\{ -1, +1 \\}$, the signed margin is $m^{(i)} = y^{(i)} d^{(i)}$, where $d^{(i)} = \\sum_{t=1}^{L} h_{t} s_{t}^{(i)}$.\n\nSuppose the weight vector $h$ must be implemented with limited precision using a uniform symmetric fixed-point quantizer with $b$ bits over a dynamic range $[-W_{\\max}, W_{\\max}]$. The quantizer has $2^{b}$ discrete levels in this range and uses rounding to the nearest level. The quantized weight vector is denoted $\\hat{h}$, the quantization step size is $\\Delta$, and the quantization error per weight is bounded in magnitude by $\\Delta / 2$. Assume no saturation occurs, that is, every true weight satisfies $h_{t} \\in [-W_{\\max}, W_{\\max}]$.\n\nStarting from fundamental definitions of linear filtering and uniform quantization, derive a sufficient condition that guarantees all signed margins remain positive under quantization, that is, $y^{(i)} \\sum_{t=1}^{L} \\hat{h}_{t} s_{t}^{(i)}  0$ for all provided labeled examples. Use the triangle inequality to upper-bound the induced decision perturbation from quantization. Then, express the minimal bit-width $b$ as the smallest integer satisfying this sufficient condition in terms of the learned filter $h$, the labeled input patterns $\\{(s^{(i)}, y^{(i)})\\}$, and the dynamic range parameter $W_{\\max}$.\n\nYour program must implement this sufficient-condition-based minimal-bit computation and produce a single-line output aggregating the results for the following test suite. In each case, compute the minimal integer $b$ required to maintain the original classification on the given labeled examples, assuming rounding-to-nearest and no saturation.\n\nTest Suite:\n- Case $1$:\n  - $L = 6$,\n  - $h^{(1)} = \\left[\\,0.6,\\,0.5,\\,0.2,\\,-0.1,\\,-0.2,\\,-0.3\\,\\right]$,\n  - Labeled patterns\n    - $(s^{(1)}_{1}, y^{(1)}_{1})$ with $s^{(1)}_{1} = \\left[\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ and $y^{(1)}_{1} = +1$,\n    - $(s^{(1)}_{2}, y^{(1)}_{2})$ with $s^{(1)}_{2} = \\left[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1\\,\\right]$ and $y^{(1)}_{2} = -1$,\n    - $(s^{(1)}_{3}, y^{(1)}_{3})$ with $s^{(1)}_{3} = \\left[\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,\\right]$ and $y^{(1)}_{3} = +1$,\n  - $W_{\\max}^{(1)} = 1.0$.\n- Case $2$:\n  - $L = 6$,\n  - $h^{(2)} = \\left[\\,0.5,\\,0.49,\\,0.01,\\,-0.01,\\,-0.49,\\,-0.5\\,\\right]$,\n  - Labeled patterns\n    - $(s^{(2)}_{1}, y^{(2)}_{1})$ with $s^{(2)}_{1} = \\left[\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ and $y^{(2)}_{1} = +1$,\n    - $(s^{(2)}_{2}, y^{(2)}_{2})$ with $s^{(2)}_{2} = \\left[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1\\,\\right]$ and $y^{(2)}_{2} = -1$,\n    - $(s^{(2)}_{3}, y^{(2)}_{3})$ with $s^{(2)}_{3} = \\left[\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,\\right]$ and $y^{(2)}_{3} = -1$,\n  - $W_{\\max}^{(2)} = 0.5$.\n- Case $3$:\n  - $L = 6$,\n  - $h^{(3)} = \\left[\\,0.1,\\,-0.1,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,\\right]$,\n  - Labeled patterns\n    - $(s^{(3)}_{1}, y^{(3)}_{1})$ with $s^{(3)}_{1} = \\left[\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,\\right]$ and $y^{(3)}_{1} = +1$,\n    - $(s^{(3)}_{2}, y^{(3)}_{2})$ with $s^{(3)}_{2} = \\left[\\,0,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ and $y^{(3)}_{2} = -1$,\n  - $W_{\\max}^{(3)} = 2.0$.\n\nImplementation details:\n- Your program must compute, for each case, the signed margins $m^{(i)}$ under full precision, the maximal absolute input sum $S = \\max_{i} \\sum_{t=1}^{L} |s^{(i)}_{t}|$, and then the minimal integer bit-width $b$ required by the sufficient condition you derived.\n- The final output must be a single line containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[\\,b_{1},b_{2},b_{3}\\,\\right]$, where $b_{k}$ is the minimal bit-width for case $k$ computed by your method. The outputs must be integers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\,b_{1},b_{2},b_{3}\\,\\right]$). No other output is allowed.",
            "solution": "The problem requires the derivation of a sufficient condition for the preservation of classification signs in a single-layer Spiking Neural Network (SNN) after its linear temporal filter weights are quantized. From this condition, we must determine the minimal integer bit-width $b$ required.\n\nThe core of the problem lies in analyzing how quantization error propagates through the linear filtering operation and affects the sign of the decision variable. We are given that the initial classification is correct for all examples, meaning the full-precision signed margin $m^{(i)} = y^{(i)} d^{(i)}$ is positive for all examples $i$. We must find a condition on the quantization bit-width $b$ that guarantees the post-quantization signed margin $\\hat{m}^{(i)}$ also remains positive.\n\nLet us begin by formalizing the quantization process and the resulting error. The weight vector $h \\in \\mathbb{R}^{L}$ is quantized to $\\hat{h} \\in \\mathbb{R}^{L}$. The quantizer is a uniform symmetric fixed-point quantizer with $b$ bits, operating over the dynamic range $[-W_{\\max}, W_{\\max}]$. It has $2^b$ discrete levels. A standard interpretation for such a quantizer, which is consistent with the problem statement, defines the quantization step size $\\Delta$ as the total range divided by the number of levels.\n$$ \\Delta = \\frac{W_{\\max} - (-W_{\\max})}{2^b} = \\frac{2W_{\\max}}{2^b} $$\nThe problem states that rounding to the nearest level is used and no saturation occurs. The quantization error for each weight component $h_t$ is the difference $e_t = \\hat{h}_t - h_t$. The magnitude of this error is bounded by half the step size:\n$$ |\\hat{h}_t - h_t| \\le \\frac{\\Delta}{2} $$\n\nNext, we analyze the effect of this quantization error on the decision variable. The full-precision decision variable for an input pattern $s^{(i)}$ is the inner product:\n$$ d^{(i)} = \\sum_{t=1}^{L} h_t s_t^{(i)} $$\nAfter quantization, the decision variable becomes:\n$$ \\hat{d}^{(i)} = \\sum_{t=1}^{L} \\hat{h}_t s_t^{(i)} $$\nThe change, or perturbation, in the decision variable due to quantization is $\\delta_d^{(i)} = \\hat{d}^{(i)} - d^{(i)}$.\n$$ \\delta_d^{(i)} = \\sum_{t=1}^{L} \\hat{h}_t s_t^{(i)} - \\sum_{t=1}^{L} h_t s_t^{(i)} = \\sum_{t=1}^{L} (\\hat{h}_t - h_t) s_t^{(i)} $$\nThe post-quantization signed margin is $\\hat{m}^{(i)} = y^{(i)} \\hat{d}^{(i)}$. We can express this in terms of the original margin $m^{(i)}$ and the perturbation:\n$$ \\hat{m}^{(i)} = y^{(i)} (d^{(i)} + \\delta_d^{(i)}) = y^{(i)} d^{(i)} + y^{(i)} \\delta_d^{(i)} = m^{(i)} + y^{(i)} \\delta_d^{(i)} $$\nFor the classification to be preserved, we require $\\hat{m}^{(i)}  0$. This gives the condition:\n$$ m^{(i)} + y^{(i)} \\delta_d^{(i)}  0 \\implies m^{(i)}  -y^{(i)} \\delta_d^{(i)} $$\nTo establish a sufficient condition, we must ensure this inequality holds even in the worst-case scenario. The worst case occurs when the perturbation term $y^{(i)} \\delta_d^{(i)}$ is maximally negative. The most negative value of this term is $-|y^{(i)} \\delta_d^{(i)}| = -|\\delta_d^{(i)}|$ since $|y^{(i)}|=1$. Therefore, a sufficient condition to guarantee $\\hat{m}^{(i)}  0$ is:\n$$ m^{(i)}  |\\delta_d^{(i)}| $$\nWe now need to find an upper bound on the magnitude of the perturbation, $|\\delta_d^{(i)}|$. Using the triangle inequality on the expression for $\\delta_d^{(i)}$:\n$$ |\\delta_d^{(i)}| = \\left| \\sum_{t=1}^{L} (\\hat{h}_t - h_t) s_t^{(i)} \\right| \\le \\sum_{t=1}^{L} |(\\hat{h}_t - h_t) s_t^{(i)}| = \\sum_{t=1}^{L} |\\hat{h}_t - h_t| |s_t^{(i)}| $$\nSubstituting the quantization error bound $|\\hat{h}_t - h_t| \\le \\frac{\\Delta}{2}$:\n$$ |\\delta_d^{(i)}| \\le \\sum_{t=1}^{L} \\frac{\\Delta}{2} |s_t^{(i)}| = \\frac{\\Delta}{2} \\sum_{t=1}^{L} |s_t^{(i)}| $$\nThe term $\\sum_{t=1}^{L} |s_t^{(i)}|$ is the $L_1$-norm of the input vector $s^{(i)}$. Let's denote it $S^{(i)}$. Our sufficient condition for a single pattern $i$ becomes:\n$$ m^{(i)}  \\frac{\\Delta}{2} S^{(i)} $$\nThis inequality must hold for all labeled examples in the dataset. This can be ensured by deriving a single, more conservative condition. Let $m_{\\min} = \\min_i m^{(i)}$ be the minimum signed margin across all examples, and let $S = \\max_i S^{(i)} = \\max_i \\left( \\sum_{t=1}^{L} |s_t^{(i)}| \\right)$ be the maximum $L_1$-norm of any input pattern.\nFor any example $j$, we have $m^{(j)} \\ge m_{\\min}$ and $S^{(j)} \\le S$. Therefore, the term $\\frac{\\Delta}{2}S$ is an upper bound for all $\\frac{\\Delta}{2}S^{(j)}$. If we enforce the condition $m_{\\min}  \\frac{\\Delta}{2}S$, it guarantees that $m^{(j)} \\ge m_{\\min}  \\frac{\\Delta}{2}S \\ge \\frac{\\Delta}{2}S^{(j)}$, thus satisfying the condition for all $j$. This approach aligns with the problem's request to compute $S$.\n\nOur final sufficient condition is:\n$$ m_{\\min}  \\frac{\\Delta S}{2} $$\nNow we can solve for the minimal bit-width $b$. We substitute the expression for the quantization step $\\Delta = \\frac{2W_{\\max}}{2^b}$:\n$$ m_{\\min}  \\frac{S}{2} \\left( \\frac{2W_{\\max}}{2^b} \\right) = \\frac{S \\cdot W_{\\max}}{2^b} $$\nRearranging the inequality to solve for $2^b$:\n$$ 2^b  \\frac{S \\cdot W_{\\max}}{m_{\\min}} $$\nTo find the smallest integer $b$ that satisfies this strict inequality, we take the base-2 logarithm of both sides:\n$$ b  \\log_2 \\left( \\frac{S \\cdot W_{\\max}}{m_{\\min}} \\right) $$\nLet $X = \\frac{S \\cdot W_{\\max}}{m_{\\min}}$. The smallest integer $b$ greater than $\\log_2(X)$ is given by $\\lfloor \\log_2(X) \\rfloor + 1$. This formula is valid for $X  0$. Given that $S \\ge 0$, $W_{\\max}  0$, and we have confirmed $m_{\\min}  0$ for all test cases, $X$ will be positive. If $X \\le 1$, this would imply that $b  \\log_2(X)$ where $\\log_2(X) \\le 0$. A bit-width of $b=1$ would suffice, and we assume $b \\ge 1$ is a practical minimum.\nThe final expression for the minimal required bit-width is:\n$$ b = \\left\\lfloor \\log_2 \\left( \\frac{W_{\\max} \\cdot \\max_i \\left( \\sum_{t=1}^{L} |s_t^{(i)}| \\right)}{\\min_i \\left( y^{(i)} \\sum_{t=1}^{L} h_t s_t^{(i)} \\right)} \\right) \\right\\rfloor + 1 $$\nThis formula will be implemented to solve for $b$ for each test case.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the minimal integer bit-width `b` required to maintain the\n    original classification on a set of labeled examples, based on a\n    derived sufficient condition.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"h\": np.array([0.6, 0.5, 0.2, -0.1, -0.2, -0.3]),\n            \"patterns\": [\n                (np.array([1, 1, 0, 0, 0, 0]), 1),\n                (np.array([0, 0, 0, 0, 1, 1]), -1),\n                (np.array([0, 1, 0, 1, 0, 1]), 1)\n            ],\n            \"W_max\": 1.0\n        },\n        # Case 2\n        {\n            \"h\": np.array([0.5, 0.49, 0.01, -0.01, -0.49, -0.5]),\n            \"patterns\": [\n                (np.array([1, 1, 0, 0, 0, 0]), 1),\n                (np.array([0, 0, 0, 0, 1, 1]), -1),\n                (np.array([0, 1, 0, 1, 0, 1]), -1)\n            ],\n            \"W_max\": 0.5\n        },\n        # Case 3\n        {\n            \"h\": np.array([0.1, -0.1, 0.0, 0.0, 0.0, 0.0]),\n            \"patterns\": [\n                (np.array([1, 0, 0, 0, 0, 0]), 1),\n                (np.array([0, 1, 0, 0, 0, 0]), -1)\n            ],\n            \"W_max\": 2.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        h = case[\"h\"]\n        patterns = case[\"patterns\"]\n        W_max = case[\"W_max\"]\n\n        # Step 1  2: Calculate all signed margins m_i and find the minimum m_min.\n        margins = []\n        for s_i, y_i in patterns:\n            # Calculate the linear response (decision variable)\n            d_i = np.dot(h, s_i)\n            # Calculate the signed margin\n            m_i = y_i * d_i\n            margins.append(m_i)\n        \n        m_min = min(margins)\n\n        # Step 3  4: Calculate the L1-norm of all inputs S_i and find the maximum S.\n        s_norms = []\n        for s_i, y_i in patterns:\n            s_norm = np.sum(np.abs(s_i))\n            s_norms.append(s_norm)\n            \n        S = max(s_norms)\n\n        # Step 5: Calculate the argument X for the logarithm\n        # The derivation guarantees m_min > 0, so no division by zero is expected.\n        X = (W_max * S) / m_min\n        \n        # Step 6: Calculate the minimal integer bit-width b.\n        # The condition is b > log2(X). The smallest integer b is floor(log2(X)) + 1.\n        # A bit-width b must be at least 1. If X = 1, log2(X) = 0, and the formula\n        # might yield 1 or less. b=1 is the effective minimum.\n        if X = 1:\n            b = 1\n        else:\n            b = math.floor(math.log2(X)) + 1\n        \n        results.append(b)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}