{
    "hands_on_practices": [
        {
            "introduction": "To understand how Spiking Neural Networks (SNNs) process temporal information, we must first analyze the dynamics of their underlying recurrent connections. This practice problem models the network's behavior using a rate-based approximation, allowing you to explore the stability of a neural activity pattern by linearizing the system around a fixed point . By computing the eigenvalues of the system's Jacobian, you will gain direct insight into how network structure determines whether a temporal representation is stably maintained or dynamically transformed.",
            "id": "4063219",
            "problem": "Consider a recurrent Spiking Neural Network (SNN) performing temporal pattern recognition whose macroscopic firing activity is well-approximated by a smooth, population-averaged firing-rate model obtained from the Leaky Integrate-and-Fire (LIF) neuron under diffusion-like input and coarse-graining assumptions. The network dynamics for the firing-rate vector $r(t) \\in \\mathbb{R}^{3}$ are given by the ordinary differential equation\n$$\n\\tau \\,\\frac{d r(t)}{d t} \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big),\n$$\nwhere $\\tau$ is the effective membrane time constant, $W \\in \\mathbb{R}^{3 \\times 3}$ is the recurrent connectivity matrix, $I_{0} \\in \\mathbb{R}^{3}$ is a constant input vector, and $\\phi(\\cdot)$ is the static current-to-rate transfer function applied element-wise. Assume $\\phi$ is the logistic function\n$$\n\\phi(u) \\;=\\; \\frac{1}{1 + \\exp\\!\\big(-\\beta\\,(u - \\theta)\\big)},\n$$\nwith slope $\\beta$ and threshold $\\theta$.\n\nSuppose the constant input $I_{0}$ has been tuned so that the network has a fixed point $r^{\\star} = \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\end{pmatrix}$ satisfying $r^{\\star} = \\phi\\!\\big(W r^{\\star} + I_{0}\\big)$, and the recurrent connectivity is\n$$\nW \\;=\\; \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix}.\n$$\nTake $\\tau = 0.02$ s and $\\beta = 3$. Using first principles of linearization around a fixed point of a smooth dynamical system, derive the linearized dynamics at $r^{\\star}$, compute the Jacobian’s eigenvalues, and thereby determine the stability of temporal pattern representations encoded by the eigenmodes. Express the three eigenvalues of the linearized system in $\\mathrm{s}^{-1}$, and round your final numerical values to three significant figures.",
            "solution": "The problem asks for an analysis of the stability of a fixed point in a recurrent firing-rate neural network model. The analysis proceeds by linearizing the system dynamics around the given fixed point and then computing the eigenvalues of the resulting Jacobian matrix.\n\nThe dynamics of the firing-rate vector $r(t) \\in \\mathbb{R}^{3}$ are given by the ordinary differential equation (ODE):\n$$\n\\tau \\,\\frac{d r(t)}{d t} \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big)\n$$\nLet us define the function $F(r)$ as the right-hand side of the ODE (pre-multiplication by $1/\\tau$):\n$$\nF(r) \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big)\n$$\nSo the system is $\\frac{dr}{dt} = \\frac{1}{\\tau} F(r)$. A fixed point $r^{\\star}$ of the system satisfies $\\frac{dr}{dt} = 0$, which implies $F(r^{\\star}) = 0$, or explicitly:\n$$\nr^{\\star} \\;=\\; \\phi\\!\\big(W\\,r^{\\star} + I_{0}\\big)\n$$\nThe problem provides that $r^{\\star} = \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\end{pmatrix}$ is such a fixed point.\n\nTo analyze the local stability of this fixed point, we linearize the dynamics around $r^{\\star}$. Let $r(t) = r^{\\star} + x(t)$, where $x(t)$ is a small perturbation vector. Substituting this into the system's ODE:\n$$\n\\tau \\,\\frac{d}{d t}\\big(r^{\\star} + x(t)\\big) \\;=\\; -\\,\\big(r^{\\star} + x(t)\\big) \\;+\\; \\phi\\!\\big(W\\,(r^{\\star} + x(t)) + I_{0}\\big)\n$$\nSince $r^{\\star}$ is constant, $\\frac{dr^{\\star}}{dt} = 0$. The equation for the perturbation $x(t)$ becomes:\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;=\\; -\\,r^{\\star} - x(t) \\;+\\; \\phi\\!\\big(W\\,r^{\\star} + I_{0} + W\\,x(t)\\big)\n$$\nWe perform a first-order Taylor expansion of the nonlinear function $\\phi$ around the input evaluated at the fixed point, $u^{\\star} = W\\,r^{\\star} + I_{0}$. For a small perturbation input $\\Delta u = W\\,x(t)$, we have:\n$$\n\\phi(u^{\\star} + \\Delta u) \\;\\approx\\; \\phi(u^{\\star}) + \\phi'(u^{\\star}) \\Delta u\n$$\nwhere the derivative is applied element-wise. Let $D$ be a diagonal matrix whose entries are the derivatives of $\\phi$ evaluated at each component of $u^{\\star}$: $D_{ii} = \\phi'((u^{\\star})_i)$. The expansion in vector form is:\n$$\n\\phi\\!\\big(W\\,r^{\\star} + I_{0} + W\\,x(t)\\big) \\;\\approx\\; \\phi(W\\,r^{\\star} + I_{0}) + D\\,W\\,x(t)\n$$\nSubstituting this back into the perturbation equation:\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;\\approx\\; -\\,r^{\\star} - x(t) + \\phi(W\\,r^{\\star} + I_{0}) + D\\,W\\,x(t)\n$$\nUsing the fixed point condition $r^{\\star} = \\phi(W\\,r^{\\star} + I_{0})$, the equation simplifies to:\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;\\approx\\; -x(t) + D\\,W\\,x(t) \\;=\\; (D\\,W - I) x(t)\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix. The linearized dynamics are therefore:\n$$\n\\frac{d x(t)}{d t} \\;=\\; J\\,x(t), \\quad \\text{where} \\quad J = \\frac{1}{\\tau} (D\\,W - I)\n$$\nThe matrix $J$ is the Jacobian of the system evaluated at the fixed point $r^{\\star}$. The stability of the fixed point is determined by the eigenvalues of $J$.\n\nWe now compute the components of the Jacobian matrix.\nThe transfer function is the logistic function $\\phi(u) = \\frac{1}{1 + \\exp(-\\beta(u-\\theta))}$. Its derivative is:\n$$\n\\phi'(u) = \\beta\\,\\phi(u)\\,\\big(1 - \\phi(u)\\big)\n$$\nAt the fixed point, the input to neuron $i$, denoted $(u^{\\star})_i = (W\\,r^{\\star} + I_{0})_i$, satisfies $\\phi((u^{\\star})_i) = r^{\\star}_i$. Since every component of $r^{\\star}$ is $r^{\\star}_i = 0.4$, the derivative of $\\phi$ for each neuron is the same:\n$$\n\\phi'((u^{\\star})_i) = \\beta\\,r^{\\star}_i\\,(1 - r^{\\star}_i) = 3 \\times 0.4 \\times (1 - 0.4) = 3 \\times 0.4 \\times 0.6 = 0.72\n$$\nLet this derivative value be $d = 0.72$. The matrix $D$ is then $D = d \\cdot I = 0.72 \\cdot I$.\n\nThe Jacobian is $J = \\frac{1}{\\tau}(d\\,W - I)$. We are given $\\tau=0.02\\,\\mathrm{s}$ and the connectivity matrix:\n$$\nW \\;=\\; \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix}\n$$\nThe matrix inside the parenthesis is $M = d\\,W - I$:\n$$\nM = 0.72 \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n0.576  0.216  0.216 \\\\\n0.216  0.576  0.216 \\\\\n0.216  0.216  0.576\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\n$$\nM = \\begin{pmatrix}\n-0.424  0.216  0.216 \\\\\n0.216  -0.424  0.216 \\\\\n0.216  0.216  -0.424\n\\end{pmatrix}\n$$\nThe eigenvalues of the Jacobian $J$ are $\\frac{1}{\\tau}$ times the eigenvalues of $M$. The matrix $M$ has a symmetric structure of the form $c_1 I + c_2 \\mathbf{1}\\mathbf{1}^T$. Such matrices have one eigenvector parallel to the vector of all ones, $\\mathbf{v}_1 = (1, 1, 1)^T$, and the remaining eigenvectors are in the subspace orthogonal to $\\mathbf{v}_1$.\nLet's find the eigenvalue $\\lambda_{M,1}$ corresponding to $\\mathbf{v}_1$:\n$$\nM \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -0.424 + 0.216 + 0.216 \\\\ 0.216 - 0.424 + 0.216 \\\\ 0.216 + 0.216 - 0.424 \\end{pmatrix} = \\begin{pmatrix} 0.008 \\\\ 0.008 \\\\ 0.008 \\end{pmatrix} = 0.008 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nSo, one eigenvalue of $M$ is $\\lambda_{M,1} = 0.008$.\nThe other two eigenvalues are degenerate due to the symmetry. Any vector orthogonal to $(1, 1, 1)^T$, such as $\\mathbf{v}_2 = (1, -1, 0)^T$, is an eigenvector:\n$$\nM \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -0.424 - 0.216 \\\\ 0.216 + 0.424 \\\\ 0.216 - 0.216 \\end{pmatrix} = \\begin{pmatrix} -0.64 \\\\ 0.64 \\\\ 0 \\end{pmatrix} = -0.64 \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nThe corresponding eigenvalue is $\\lambda_{M,2} = -0.64$. Due to symmetry, the third eigenvalue is also $\\lambda_{M,3} = -0.64$.\n\nThe eigenvalues of the Jacobian $J$ are obtained by multiplying the eigenvalues of $M$ by $\\frac{1}{\\tau} = \\frac{1}{0.02} = 50 \\, \\mathrm{s}^{-1}$:\n$$\n\\lambda_1 = 50 \\times \\lambda_{M,1} = 50 \\times 0.008 = 0.4 \\, \\mathrm{s}^{-1}\n$$\n$$\n\\lambda_2 = \\lambda_3 = 50 \\times \\lambda_{M,2} = 50 \\times (-0.64) = -32 \\, \\mathrm{s}^{-1}\n$$\nThe stability of the fixed point is determined by the signs of the real parts of these eigenvalues. We have one positive real eigenvalue, $\\lambda_1 = 0.4$, and two negative real eigenvalues, $\\lambda_2 = \\lambda_3 = -32$. The presence of a positive eigenvalue indicates that the fixed point $r^{\\star}$ is unstable. Specifically, it is a saddle point.\nPerturbations along the direction of the eigenvector corresponding to $\\lambda_1$ (which is the common mode where all rates change together) will grow exponentially, moving the system state away from the fixed point. Perturbations in the two-dimensional subspace spanned by the eigenvectors for $\\lambda_2$ and $\\lambda_3$ (the differential modes) will decay, returning the state to that subspace. Thus, temporal patterns corresponding to the common mode are unstable, while those corresponding to the differential modes are stable.\n\nThe problem requires the three eigenvalues rounded to three significant figures.\n$\\lambda_1 = 0.4$ becomes $0.400$.\n$\\lambda_2 = -32$ becomes $-32.0$.\n$\\lambda_3 = -32$ becomes $-32.0$.\nThe units are $\\mathrm{s}^{-1}$.\n\nThe three eigenvalues of the linearized system are $\\{0.400, -32.0, -32.0\\}$ in units of $\\mathrm{s}^{-1}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.400  -32.0  -32.0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Training SNNs with gradient-based methods is challenging due to the non-differentiable nature of spiking events. This problem introduces the concept of surrogate gradients, a key technique that enables backpropagation by approximating the derivative of the spiking nonlinearity . You will implement and compare several common surrogate functions, analyzing the bias they introduce, which provides essential practical knowledge for effectively training deep SNNs.",
            "id": "4063246",
            "problem": "Consider a discrete-time, two-layer spiking neural network for temporal motif detection. Time is indexed by integer steps $t \\in \\{0,1,\\dots,T-1\\}$ with $T$ specified. The input layer provides a binary spike train $x(t) \\in \\{0,1\\}$ that contains a temporal motif defined by spike occurrences at specified times. The first layer contains a single neuron with a linear synaptic filter and a logistic spiking nonlinearity. The second (output) layer contains a single neuron that linearly aggregates the first layer’s spikes across time and applies a logistic nonlinearity to produce a Bernoulli probability for target classification.\n\nDefinitions and forward model:\n- Let the synaptic kernel be the causal exponential kernel $h(\\Delta t) = \\exp(-\\Delta t/\\tau)$ for $\\Delta t \\ge 0$ and $h(\\Delta t) = 0$ for $\\Delta t  0$, with time constant $\\tau  0$ in discrete steps.\n- The first-layer membrane potential is $u_1(t) = w_1 \\sum_{t'=0}^{t} x(t') h(t-t')$, where $w_1$ is the first-layer synaptic weight.\n- The first-layer spiking probability (logistic nonlinearity) is $s_1(t) = \\sigma\\left(\\frac{u_1(t) - \\theta_1}{\\beta}\\right)$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $\\theta_1$ is the threshold, and $\\beta  0$ is the slope parameter (shared across layers).\n- The second-layer membrane potential is $u_2 = w_2 \\sum_{t=0}^{T-1} s_1(t)$, where $w_2$ is the second-layer synaptic weight.\n- The output probability is $p = \\sigma\\left(\\frac{u_2 - \\theta_2}{\\beta}\\right)$, with threshold $\\theta_2$.\n- The loss for target $y \\in \\{0,1\\}$ is the logistic cross-entropy $\\mathcal{L}(p,y) = -y \\log p - (1-y) \\log(1-p)$.\n\nObjective:\nYou must analyze the bias in estimating the gradient $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$ when using three standard surrogate derivative functions in place of the exact derivative of the first-layer logistic nonlinearity. The ground-truth gradient must be derived under the above stochastic logistic model. The surrogate gradient estimator must be constructed by replacing the exact factor $\\frac{\\partial s_1(t)}{\\partial u_1(t)}$ with a surrogate function $g(u_1(t) - \\theta_1)$, while keeping the forward model (including $s_1(t)$ and $p$) unchanged.\n\nSurrogate derivative functions:\n- Piecewise linear triangular: $g_{\\mathrm{pwl}}(z; a) = \\frac{1}{a} \\max\\left(0, 1 - \\frac{|z|}{a}\\right)$ with width parameter $a  0$.\n- Exponential (Laplace-shaped): $g_{\\mathrm{exp}}(z; a) = \\frac{1}{a} \\exp\\left(-\\frac{|z|}{a}\\right)$ with scale $a  0$.\n- Gaussian: $g_{\\mathrm{gauss}}(z; a) = \\frac{1}{\\sqrt{2\\pi} a} \\exp\\left(-\\frac{z^2}{2 a^2}\\right)$ with standard deviation $a  0$.\n\nBias definition:\nFor each surrogate $g$, define the surrogate-based estimator $\\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g)$ by substituting $g(u_1(t) - \\theta_1)$ for $\\frac{\\partial s_1(t)}{\\partial u_1(t)}$ in the chain rule, and define the bias as $B(g) = \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) - \\frac{\\partial \\mathcal{L}}{\\partial w_1}$.\n\nYour program must implement the forward model, derive and compute the ground-truth gradient $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$ from first principles, and then compute the three surrogate-based estimates and their biases $B(g_{\\mathrm{pwl}})$, $B(g_{\\mathrm{exp}})$, and $B(g_{\\mathrm{gauss}})$.\n\nTest suite:\nUse the following five test cases, each specified by $(T, \\tau, \\text{spike times}, y, w_1, w_2, \\theta_1, \\theta_2, \\beta, a)$:\n1. $(\\;20,\\;5.0,\\;\\{4,9\\},\\;1,\\;1.0,\\;1.2,\\;0.9,\\;1.1,\\;0.25,\\;0.25\\;)$\n2. $(\\;20,\\;5.0,\\;\\{7\\},\\;0,\\;1.0,\\;1.0,\\;1.2,\\;1.0,\\;0.25,\\;0.25\\;)$\n3. $(\\;20,\\;5.0,\\;\\{5,6\\},\\;1,\\;0.8,\\;1.3,\\;0.8,\\;1.0,\\;0.25,\\;0.25\\;)$\n4. $(\\;20,\\;5.0,\\;\\{4,9\\},\\;1,\\;1.0,\\;1.2,\\;0.9,\\;1.1,\\;0.25,\\;0.05\\;)$\n5. $(\\;20,\\;5.0,\\;\\{4,9\\},\\;1,\\;1.0,\\;1.2,\\;0.9,\\;1.1,\\;0.25,\\;1.0\\;)$\n\nIn all cases, construct $x(t)$ by setting $x(t)=1$ at the listed spike times and $x(t)=0$ otherwise. The parameters are dimensionless. For each case, compute and return the list of biases $[B(g_{\\mathrm{pwl}}), B(g_{\\mathrm{exp}}), B(g_{\\mathrm{gauss}})]$ as floating-point numbers.\n\nFinal output format:\nYour program should produce a single line of output containing a list of lists, each inner list corresponding to a test case and containing the three biases in the order specified. The line should be formatted exactly as a comma-separated list enclosed in square brackets, for example, $[\\,[b_{1,\\mathrm{pwl}},b_{1,\\mathrm{exp}},b_{1,\\mathrm{gauss}}],\\dots\\,]$, with numeric values printed in standard floating-point notation. No additional text or lines are permitted.",
            "solution": "The problem requires the calculation of the bias introduced by using surrogate gradient functions for training a two-layer spiking neural network. The bias is defined as the difference between the gradient estimator derived using a surrogate function and the true gradient derived from the defined stochastic model. We will first derive the analytical expression for the true gradient, then for the surrogate-based estimator, and finally for the bias.\n\nThe complete forward model is defined by the following equations:\n- Input spike train: $x(t) \\in \\{0, 1\\}$ for $t \\in \\{0, 1, \\dots, T-1\\}$.\n- Synaptic kernel: $h(\\Delta t) = \\exp(-\\Delta t/\\tau)$ for $\\Delta t \\ge 0$.\n- First-layer membrane potential: $u_1(t) = w_1 \\sum_{t'=0}^{t} x(t') h(t-t')$.\n- First-layer spiking probability: $s_1(t) = \\sigma\\left(\\frac{u_1(t) - \\theta_1}{\\beta}\\right)$, with $\\sigma(z) = (1 + e^{-z})^{-1}$.\n- Second-layer membrane potential: $u_2 = w_2 \\sum_{t=0}^{T-1} s_1(t)$.\n- Output probability: $p = \\sigma\\left(\\frac{u_2 - \\theta_2}{\\beta}\\right)$.\n- Loss function: $\\mathcal{L}(p,y) = -y \\log p - (1-y) \\log(1-p)$.\n\nOur objective is to compute the bias $B(g) = \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) - \\frac{\\partial \\mathcal{L}}{\\partial w_1}$ for the first-layer weight $w_1$.\n\nFirst, we derive the ground-truth gradient, $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$, by applying the chain rule. The gradient calculation follows the path of influence from $w_1$ to $\\mathcal{L}$: $w_1 \\rightarrow u_1(t) \\rightarrow s_1(t) \\rightarrow u_2 \\rightarrow p \\rightarrow \\mathcal{L}$.\n\n1.  Derivative of the loss $\\mathcal{L}$ with respect to the second-layer potential $u_2$:\n    The derivative of the logistic function is $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$.\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial p} = -\\frac{y}{p} + \\frac{1-y}{1-p} $$\n    $$ \\frac{\\partial p}{\\partial u_2} = \\frac{\\partial}{\\partial u_2}\\sigma\\left(\\frac{u_2 - \\theta_2}{\\beta}\\right) = \\sigma'\\left(\\frac{u_2 - \\theta_2}{\\beta}\\right) \\frac{1}{\\beta} = \\frac{p(1-p)}{\\beta} $$\n    Combining these gives a common and simplified result for logistic cross-entropy loss:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial u_2} = \\frac{\\partial \\mathcal{L}}{\\partial p} \\frac{\\partial p}{\\partial u_2} = \\left(-\\frac{y}{p} + \\frac{1-y}{1-p}\\right) \\frac{p(1-p)}{\\beta} = \\frac{-y(1-p) + p(1-y)}{\\beta} = \\frac{p-y}{\\beta} $$\n\n2.  Derivative of $u_2$ with respect to $w_1$:\n    $$ \\frac{\\partial u_2}{\\partial w_1} = \\frac{\\partial}{\\partial w_1} \\left(w_2 \\sum_{t=0}^{T-1} s_1(t)\\right) = w_2 \\sum_{t=0}^{T-1} \\frac{\\partial s_1(t)}{\\partial w_1} $$\n    To find $\\frac{\\partial s_1(t)}{\\partial w_1}$, we apply the chain rule again:\n    $$ \\frac{\\partial s_1(t)}{\\partial w_1} = \\frac{\\partial s_1(t)}{\\partial u_1(t)} \\frac{\\partial u_1(t)}{\\partial w_1} $$\n\n3.  Derivative terms for the first layer:\n    The derivative of the first-layer spiking probability $s_1(t)$ with respect to its potential $u_1(t)$ is:\n    $$ \\frac{\\partial s_1(t)}{\\partial u_1(t)} = \\frac{\\partial}{\\partial u_1(t)} \\sigma\\left(\\frac{u_1(t) - \\theta_1}{\\beta}\\right) = \\sigma'\\left(\\frac{u_1(t) - \\theta_1}{\\beta}\\right) \\frac{1}{\\beta} = \\frac{s_1(t)(1-s_1(t))}{\\beta} $$\n    The derivative of the first-layer potential $u_1(t)$ with respect to the weight $w_1$ is:\n    $$ \\frac{\\partial u_1(t)}{\\partial w_1} = \\frac{\\partial}{\\partial w_1} \\left(w_1 \\sum_{t'=0}^{t} x(t') h(t-t')\\right) = \\sum_{t'=0}^{t} x(t') h(t-t') = \\frac{u_1(t)}{w_1} $$\n\n4.  Assembling the ground-truth gradient $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$:\n    Combining the parts, we get:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{\\partial \\mathcal{L}}{\\partial u_2} \\frac{\\partial u_2}{\\partial w_1} = \\left(\\frac{p-y}{\\beta}\\right) \\left(w_2 \\sum_{t=0}^{T-1} \\frac{\\partial s_1(t)}{\\partial u_1(t)} \\frac{\\partial u_1(t)}{\\partial w_1}\\right) $$\n    Substituting the expressions from step 3:\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{w_2(p-y)}{\\beta} \\sum_{t=0}^{T-1} \\left(\\frac{s_1(t)(1-s_1(t))}{\\beta}\\right) \\left(\\frac{u_1(t)}{w_1}\\right) $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{w_2(p-y)}{w_1 \\beta^2} \\sum_{t=0}^{T-1} u_1(t) s_1(t)(1-s_1(t)) $$\n    This is the exact gradient.\n\nNext, we formulate the surrogate-based gradient estimator, $\\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g)$. This is constructed by replacing the true derivative of the nonlinearity, $\\frac{\\partial s_1(t)}{\\partial u_1(t)}$, with a surrogate function $g(u_1(t) - \\theta_1)$.\n$$ \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) = \\frac{w_2(p-y)}{\\beta} \\sum_{t=0}^{T-1} g\\left(u_1(t) - \\theta_1\\right) \\frac{\\partial u_1(t)}{\\partial w_1} $$\n$$ \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) = \\frac{w_2(p-y)}{w_1 \\beta} \\sum_{t=0}^{T-1} u_1(t) g\\left(u_1(t) - \\theta_1\\right) $$\n\nFinally, the bias $B(g)$ is the difference between the estimator and the true gradient:\n$$ B(g) = \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) - \\frac{\\partial \\mathcal{L}}{\\partial w_1} $$\n$$ B(g) = \\left(\\frac{w_2(p-y)}{w_1 \\beta} \\sum_{t=0}^{T-1} u_1(t) g(u_1(t)-\\theta_1)\\right) - \\left(\\frac{w_2(p-y)}{w_1 \\beta^2} \\sum_{t=0}^{T-1} u_1(t) s_1(t)(1-s_1(t))\\right) $$\nFactoring out common terms yields the final expression for the bias:\n$$ B(g) = \\frac{w_2(p-y)}{w_1 \\beta} \\sum_{t=0}^{T-1} u_1(t) \\left( g(u_1(t)-\\theta_1) - \\frac{s_1(t)(1-s_1(t))}{\\beta} \\right) $$\n\nThe computational procedure to calculate the bias for each surrogate function is as follows:\n1.  For a given test case, construct the input spike train $x(t)$.\n2.  Perform the forward pass:\n    a. Compute the synaptic kernel values $h(\\Delta t)$ for $\\Delta t \\in \\{0, \\dots, T-1\\}$.\n    b. Convolve $x(t)$ with $h(t)$ and scale by $w_1$ to get the first-layer potential $u_1(t)$ for all $t$.\n    c. Apply the logistic function to $u_1(t)$ to get the first-layer spike probabilities $s_1(t)$.\n    d. Sum $s_1(t)$ over time and scale by $w_2$ to get the second-layer potential $u_2$.\n    e. Apply the logistic function to $u_2$ to get the final output probability $p$.\n3.  Calculate the bias using the derived formula:\n    a. Compute the total pre-factor $\\frac{w_2(p-y)}{w_1 \\beta}$.\n    b. For each time step $t$, calculate the difference term: $D(t) = g(u_1(t)-\\theta_1) - \\frac{s_1(t)(1-s_1(t))}{\\beta}$. This is done for each of the three surrogate functions $g_{\\mathrm{pwl}}$, $g_{\\mathrm{exp}}$, and $g_{\\mathrm{gauss}}$.\n    c. Compute the sum $\\sum_{t=0}^{T-1} u_1(t) D(t)$.\n    d. Multiply the sum by the pre-factor to obtain the bias for each surrogate.\nThis procedure is implemented for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of surrogate gradient estimators for a two-layer SNN.\n    \"\"\"\n    # Test cases from the problem statement:\n    # (T, tau, spike_times, y, w1, w2, theta1, theta2, beta, a)\n    test_cases = [\n        (20, 5.0, {4, 9}, 1, 1.0, 1.2, 0.9, 1.1, 0.25, 0.25),\n        (20, 5.0, {7}, 0, 1.0, 1.0, 1.2, 1.0, 0.25, 0.25),\n        (20, 5.0, {5, 6}, 1, 0.8, 1.3, 0.8, 1.0, 0.25, 0.25),\n        (20, 5.0, {4, 9}, 1, 1.0, 1.2, 0.9, 1.1, 0.25, 0.05),\n        (20, 5.0, {4, 9}, 1, 1.0, 1.2, 0.9, 1.1, 0.25, 1.0),\n    ]\n\n    results = []\n    \n    # Define the logistic sigmoid function\n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Define the surrogate derivative functions\n    def g_pwl(z, a):\n        return (1.0 / a) * np.maximum(0, 1 - np.abs(z) / a)\n\n    def g_exp(z, a):\n        return (1.0 / a) * np.exp(-np.abs(z) / a)\n\n    def g_gauss(z, a):\n        return (1.0 / (np.sqrt(2 * np.pi) * a)) * np.exp(-(z**2) / (2 * a**2))\n\n    for case in test_cases:\n        T, tau, spike_times, y, w1, w2, theta1, theta2, beta, a = case\n\n        # 1. Construct input spike train x(t)\n        x = np.zeros(T)\n        for t_spike in spike_times:\n            if 0 = t_spike  T:\n                x[t_spike] = 1.0\n\n        # 2. Perform the forward pass\n        # 2a. Synaptic kernel h(dt)\n        time_steps = np.arange(T)\n        h = np.exp(-time_steps / tau)\n\n        # 2b. First-layer potential u1(t)\n        # u1(t) = w1 * sum_{t'=0 to t} x(t') h(t-t')\n        # This is equivalent to a convolution\n        u1 = w1 * np.convolve(x, h)[:T]\n\n        # 2c. First-layer spiking probability s1(t)\n        z1 = (u1 - theta1) / beta\n        s1 = sigma(z1)\n\n        # 2d. Second-layer potential u2\n        u2 = w2 * np.sum(s1)\n\n        # 2e. Output probability p\n        z2 = (u2 - theta2) / beta\n        p = sigma(z2)\n\n        # 3. Calculate the bias B(g)\n        # B(g) = (w2*(p-y)/(w1*beta)) * sum(u1 * (g(u1-theta1) - s1*(1-s1)/beta))\n        \n        pre_factor = (w2 * (p - y)) / (w1 * beta)\n        \n        # Difference terms\n        z1_shifted = u1 - theta1\n        true_deriv_term = s1 * (1 - s1) / beta\n        \n        # Calculate surrogate values\n        g_pwl_vals = g_pwl(z1_shifted, a)\n        g_exp_vals = g_exp(z1_shifted, a)\n        g_gauss_vals = g_gauss(z1_shifted, a)\n\n        # Calculate difference between surrogate and true derivative\n        diff_pwl = g_pwl_vals - true_deriv_term\n        diff_exp = g_exp_vals - true_deriv_term\n        diff_gauss = g_gauss_vals - true_deriv_term\n        \n        # Calculate sum over time, weighted by u1\n        sum_pwl = np.sum(u1 * diff_pwl)\n        sum_exp = np.sum(u1 * diff_exp)\n        sum_gauss = np.sum(u1 * diff_gauss)\n        \n        # Final bias values\n        bias_pwl = pre_factor * sum_pwl\n        bias_exp = pre_factor * sum_exp\n        bias_gauss = pre_factor * sum_gauss\n        \n        biases_for_case = [bias_pwl, bias_exp, bias_gauss]\n        results.append(biases_for_case)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The successful deployment of SNNs on neuromorphic hardware often depends on representing synaptic weights with limited precision to save energy and area. This exercise explores the trade-off between weight quantization and classification accuracy by tasking you with deriving a sufficient condition to preserve a learned temporal filter's performance . By calculating the minimum bit-width required, you will gain a quantitative understanding of how to design robust SNNs that operate effectively under hardware constraints.",
            "id": "4063206",
            "problem": "Consider a single-layer Spiking Neural Network (SNN) performing two-class temporal pattern recognition via a linear temporal filter. Time is discretized into $L$ bins, and each input pattern is represented as a vector $s \\in \\mathbb{R}^{L}$ of binned spike counts. The learned temporal filter is a weight vector $h \\in \\mathbb{R}^{L}$. The decision variable for a pattern $s$ is the linear response $d = \\sum_{t=1}^{L} h_{t} s_{t}$, and the predicted class is the sign of $d$ with threshold at zero. For a labeled example $(s^{(i)}, y^{(i)})$ with label $y^{(i)} \\in \\{ -1, +1 \\}$, the signed margin is $m^{(i)} = y^{(i)} d^{(i)}$, where $d^{(i)} = \\sum_{t=1}^{L} h_{t} s_{t}^{(i)}$.\n\nSuppose the weight vector $h$ must be implemented with limited precision using a uniform symmetric fixed-point quantizer with $b$ bits over a dynamic range $[-W_{\\max}, W_{\\max}]$. The quantizer has $2^{b}$ discrete levels in this range and uses rounding to the nearest level. The quantized weight vector is denoted $\\hat{h}$, the quantization step size is $\\Delta$, and the quantization error per weight is bounded in magnitude by $\\Delta / 2$. Assume no saturation occurs, that is, every true weight satisfies $h_{t} \\in [-W_{\\max}, W_{\\max}]$.\n\nStarting from fundamental definitions of linear filtering and uniform quantization, derive a sufficient condition that guarantees all signed margins remain positive under quantization, that is, $y^{(i)} \\sum_{t=1}^{L} \\hat{h}_{t} s_{t}^{(i)}  0$ for all provided labeled examples. Use the triangle inequality to upper-bound the induced decision perturbation from quantization. Then, express the minimal bit-width $b$ as the smallest integer satisfying this sufficient condition in terms of the learned filter $h$, the labeled input patterns $\\{(s^{(i)}, y^{(i)})\\}$, and the dynamic range parameter $W_{\\max}$.\n\nYour program must implement this sufficient-condition-based minimal-bit computation and produce a single-line output aggregating the results for the following test suite. In each case, compute the minimal integer $b$ required to maintain the original classification on the given labeled examples, assuming rounding-to-nearest and no saturation.\n\nTest Suite:\n- Case $1$:\n  - $L = 6$,\n  - $h^{(1)} = \\left[\\,0.6,\\,0.5,\\,0.2,\\,-0.1,\\,-0.2,\\,-0.3\\,\\right]$,\n  - Labeled patterns\n    - $(s^{(1)}_{1}, y^{(1)}_{1})$ with $s^{(1)}_{1} = \\left[\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ and $y^{(1)}_{1} = +1$,\n    - $(s^{(1)}_{2}, y^{(1)}_{2})$ with $s^{(1)}_{2} = \\left[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1\\,\\right]$ and $y^{(1)}_{2} = -1$,\n    - $(s^{(1)}_{3}, y^{(1)}_{3})$ with $s^{(1)}_{3} = \\left[\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,\\right]$ and $y^{(1)}_{3} = +1$,\n  - $W_{\\max}^{(1)} = 1.0$.\n- Case $2$:\n  - $L = 6$,\n  - $h^{(2)} = \\left[\\,0.5,\\,0.49,\\,0.01,\\,-0.01,\\,-0.49,\\,-0.5\\,\\right]$,\n  - Labeled patterns\n    - $(s^{(2)}_{1}, y^{(2)}_{1})$ with $s^{(2)}_{1} = \\left[\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ and $y^{(2)}_{1} = +1$,\n    - $(s^{(2)}_{2}, y^{(2)}_{2})$ with $s^{(2)}_{2} = \\left[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1\\,\\right]$ and $y^{(2)}_{2} = -1$,\n    - $(s^{(2)}_{3}, y^{(2)}_{3})$ with $s^{(2)}_{3} = \\left[\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,\\right]$ and $y^{(2)}_{3} = -1$,\n  - $W_{\\max}^{(2)} = 0.5$.\n- Case $3$:\n  - $L = 6$,\n  - $h^{(3)} = \\left[\\,0.1,\\,-0.1,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,\\right]$,\n  - Labeled patterns\n    - $(s^{(3)}_{1}, y^{(3)}_{1})$ with $s^{(3)}_{1} = \\left[\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,\\right]$ and $y^{(3)}_{1} = +1$,\n    - $(s^{(3)}_{2}, y^{(3)}_{2})$ with $s^{(3)}_{2} = \\left[\\,0,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ and $y^{(3)}_{2} = -1$,\n  - $W_{\\max}^{(3)} = 2.0$.\n\nImplementation details:\n- Your program must compute, for each case, the signed margins $m^{(i)}$ under full precision, the maximal absolute input sum $S = \\max_{i} \\sum_{t=1}^{L} |s^{(i)}_{t}|$, and then the minimal integer bit-width $b$ required by the sufficient condition you derived.\n- The final output must be a single line containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[\\,b_{1},b_{2},b_{3}\\,\\right]$, where $b_{k}$ is the minimal bit-width for case $k$ computed by your method. The outputs must be integers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\,b_{1},b_{2},b_{3}\\,\\right]$). No other output is allowed.",
            "solution": "The problem requires the derivation of a sufficient condition for the preservation of classification signs in a single-layer Spiking Neural Network (SNN) after its linear temporal filter weights are quantized. From this condition, we must determine the minimal integer bit-width $b$ required.\n\nThe core of the problem lies in analyzing how quantization error propagates through the linear filtering operation and affects the sign of the decision variable. We are given that the initial classification is correct for all examples, meaning the full-precision signed margin $m^{(i)} = y^{(i)} d^{(i)}$ is positive for all examples $i$. We must find a condition on the quantization bit-width $b$ that guarantees the post-quantization signed margin $\\hat{m}^{(i)}$ also remains positive.\n\nLet us begin by formalizing the quantization process and the resulting error. The weight vector $h \\in \\mathbb{R}^{L}$ is quantized to $\\hat{h} \\in \\mathbb{R}^{L}$. The quantizer is a uniform symmetric fixed-point quantizer with $b$ bits, operating over the dynamic range $[-W_{\\max}, W_{\\max}]$. It has $2^b$ discrete levels. A standard interpretation for such a quantizer, which is consistent with the problem statement, defines the quantization step size $\\Delta$ as the total range divided by the number of intervals, which is the number of levels.\n$$ \\Delta = \\frac{W_{\\max} - (-W_{\\max})}{2^b} = \\frac{2W_{\\max}}{2^b} $$\nThe problem states that rounding to the nearest level is used and no saturation occurs. The quantization error for each weight component $h_t$ is the difference $e_t = \\hat{h}_t - h_t$. The magnitude of this error is bounded by half the step size:\n$$ |\\hat{h}_t - h_t| \\le \\frac{\\Delta}{2} $$\n\nNext, we analyze the effect of this quantization error on the decision variable. The full-precision decision variable for an input pattern $s^{(i)}$ is the inner product:\n$$ d^{(i)} = \\sum_{t=1}^{L} h_t s_t^{(i)} $$\nAfter quantization, the decision variable becomes:\n$$ \\hat{d}^{(i)} = \\sum_{t=1}^{L} \\hat{h}_t s_t^{(i)} $$\nThe change, or perturbation, in the decision variable due to quantization is $\\delta_d^{(i)} = \\hat{d}^{(i)} - d^{(i)}$.\n$$ \\delta_d^{(i)} = \\sum_{t=1}^{L} \\hat{h}_t s_t^{(i)} - \\sum_{t=1}^{L} h_t s_t^{(i)} = \\sum_{t=1}^{L} (\\hat{h}_t - h_t) s_t^{(i)} $$\nThe post-quantization signed margin is $\\hat{m}^{(i)} = y^{(i)} \\hat{d}^{(i)}$. We can express this in terms of the original margin $m^{(i)}$ and the perturbation:\n$$ \\hat{m}^{(i)} = y^{(i)} (d^{(i)} + \\delta_d^{(i)}) = y^{(i)} d^{(i)} + y^{(i)} \\delta_d^{(i)} = m^{(i)} + y^{(i)} \\delta_d^{(i)} $$\nFor the classification to be preserved, we require $\\hat{m}^{(i)}  0$. This gives the condition:\n$$ m^{(i)} + y^{(i)} \\delta_d^{(i)}  0 \\implies m^{(i)}  -y^{(i)} \\delta_d^{(i)} $$\nTo establish a sufficient condition, we must ensure this inequality holds even in the worst-case scenario. The worst case occurs when the perturbation term $y^{(i)} \\delta_d^{(i)}$ is maximally negative. The most negative value of this term is $-|y^{(i)} \\delta_d^{(i)}| = -|\\delta_d^{(i)}|$ since $|y^{(i)}|=1$. Therefore, a sufficient condition to guarantee $\\hat{m}^{(i)}  0$ is:\n$$ m^{(i)}  |\\delta_d^{(i)}| $$\nWe now need to find an upper bound on the magnitude of the perturbation, $|\\delta_d^{(i)}|$. Using the triangle inequality on the expression for $\\delta_d^{(i)}$:\n$$ |\\delta_d^{(i)}| = \\left| \\sum_{t=1}^{L} (\\hat{h}_t - h_t) s_t^{(i)} \\right| \\le \\sum_{t=1}^{L} |(\\hat{h}_t - h_t) s_t^{(i)}| = \\sum_{t=1}^{L} |\\hat{h}_t - h_t| |s_t^{(i)}| $$\nSubstituting the quantization error bound $|\\hat{h}_t - h_t| \\le \\frac{\\Delta}{2}$:\n$$ |\\delta_d^{(i)}| \\le \\sum_{t=1}^{L} \\frac{\\Delta}{2} |s_t^{(i)}| = \\frac{\\Delta}{2} \\sum_{t=1}^{L} |s_t^{(i)}| $$\nThe term $\\sum_{t=1}^{L} |s_t^{(i)}|$ is the $L_1$-norm of the input vector $s^{(i)}$. Let's denote it $S^{(i)}$. Our sufficient condition for a single pattern $i$ becomes:\n$$ m^{(i)}  \\frac{\\Delta}{2} S^{(i)} $$\nThis inequality must hold for all labeled examples in the dataset. This can be ensured by deriving a single, more conservative condition. Let $m_{\\min} = \\min_i m^{(i)}$ be the minimum signed margin across all examples, and let $S = \\max_i S^{(i)} = \\max_i \\left( \\sum_{t=1}^{L} |s_t^{(i)}| \\right)$ be the maximum $L_1$-norm of any input pattern.\nFor any example $j$, we have $m^{(j)} \\ge m_{\\min}$ and $S^{(j)} \\le S$. Therefore, the term $\\frac{\\Delta}{2}S$ is an upper bound for all $\\frac{\\Delta}{2}S^{(j)}$. If we enforce the condition $m_{\\min}  \\frac{\\Delta}{2}S$, it guarantees that $m^{(j)} \\ge m_{\\min}  \\frac{\\Delta}{2}S \\ge \\frac{\\Delta}{2}S^{(j)}$, thus satisfying the condition for all $j$. This approach aligns with the problem's request to compute $S$.\n\nOur final sufficient condition is:\n$$ m_{\\min}  \\frac{\\Delta S}{2} $$\nNow we can solve for the minimal bit-width $b$. We substitute the expression for the quantization step $\\Delta = \\frac{2W_{\\max}}{2^b}$:\n$$ m_{\\min}  \\frac{S}{2} \\left( \\frac{2W_{\\max}}{2^b} \\right) = \\frac{S \\cdot W_{\\max}}{2^b} $$\nRearranging the inequality to solve for $2^b$:\n$$ 2^b  \\frac{S \\cdot W_{\\max}}{m_{\\min}} $$\nTo find the smallest integer $b$ that satisfies this strict inequality, we take the base-2 logarithm of both sides:\n$$ b  \\log_2 \\left( \\frac{S \\cdot W_{\\max}}{m_{\\min}} \\right) $$\nLet $X = \\frac{S \\cdot W_{\\max}}{m_{\\min}}$. The smallest integer $b$ greater than $\\log_2(X)$ is given by $\\lfloor \\log_2(X) \\rfloor + 1$. This formula is valid for $X  0$. Given that $S \\ge 0$, $W_{\\max}  0$, and we have confirmed $m_{\\min}  0$ for all test cases, $X$ will be positive. If $X \\le 1$, this would imply that $b  \\log_2(X)$ where $\\log_2(X) \\le 0$. A bit-width of $b=1$ would suffice, and we assume $b \\ge 1$ is a practical minimum.\nThe final expression for the minimal required bit-width is:\n$$ b = \\left\\lfloor \\log_2 \\left( \\frac{W_{\\max} \\cdot \\max_i \\left( \\sum_{t=1}^{L} |s_t^{(i)}| \\right)}{\\min_i \\left( y^{(i)} \\sum_{t=1}^{L} h_t s_t^{(i)} \\right)} \\right) \\right\\rfloor + 1 $$\nThis formula will be implemented to solve for $b$ for each test case.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the minimal integer bit-width `b` required to maintain the\n    original classification on a set of labeled examples, based on a\n    derived sufficient condition.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"h\": np.array([0.6, 0.5, 0.2, -0.1, -0.2, -0.3]),\n            \"patterns\": [\n                (np.array([1, 1, 0, 0, 0, 0]), 1),\n                (np.array([0, 0, 0, 0, 1, 1]), -1),\n                (np.array([0, 1, 0, 1, 0, 1]), 1)\n            ],\n            \"W_max\": 1.0\n        },\n        # Case 2\n        {\n            \"h\": np.array([0.5, 0.49, 0.01, -0.01, -0.49, -0.5]),\n            \"patterns\": [\n                (np.array([1, 1, 0, 0, 0, 0]), 1),\n                (np.array([0, 0, 0, 0, 1, 1]), -1),\n                (np.array([0, 1, 0, 1, 0, 1]), -1)\n            ],\n            \"W_max\": 0.5\n        },\n        # Case 3\n        {\n            \"h\": np.array([0.1, -0.1, 0.0, 0.0, 0.0, 0.0]),\n            \"patterns\": [\n                (np.array([1, 0, 0, 0, 0, 0]), 1),\n                (np.array([0, 1, 0, 0, 0, 0]), -1)\n            ],\n            \"W_max\": 2.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        h = case[\"h\"]\n        patterns = case[\"patterns\"]\n        W_max = case[\"W_max\"]\n\n        # Step 1  2: Calculate all signed margins m_i and find the minimum m_min.\n        margins = []\n        for s_i, y_i in patterns:\n            # Calculate the linear response (decision variable)\n            d_i = np.dot(h, s_i)\n            # Calculate the signed margin\n            m_i = y_i * d_i\n            margins.append(m_i)\n        \n        m_min = min(margins)\n\n        # Step 3  4: Calculate the L1-norm of all inputs S_i and find the maximum S.\n        s_norms = []\n        for s_i, y_i in patterns:\n            s_norm = np.sum(np.abs(s_i))\n            s_norms.append(s_norm)\n            \n        S = max(s_norms)\n\n        # Step 5: Calculate the argument X for the logarithm\n        # The derivation guarantees m_min > 0, so no division by zero is expected.\n        X = (W_max * S) / m_min\n        \n        # Step 6: Calculate the minimal integer bit-width b.\n        # The condition is b > log2(X). The smallest integer b is floor(log2(X)) + 1.\n        # A bit-width b must be at least 1. If X = 1, log2(X) = 0, and the formula\n        # might yield 1 or less. b=1 is the effective minimum.\n        if X = 1:\n            b = 1\n        else:\n            b = math.floor(math.log2(X)) + 1\n        \n        results.append(b)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}