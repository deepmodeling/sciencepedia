{
    "hands_on_practices": [
        {
            "introduction": "为了让脉冲神经网络（SNN）能够可靠地识别时间模式，其网络活动必须能够稳定地表示这些模式。一种强大的分析方法是将复杂的脉冲动力学简化为连续的平均发放率模型，并研究其不动点的稳定性。本练习将指导您应用线性化这一核心动力学系统工具，通过分析雅可比矩阵的特征值来判断网络表征的稳定性，从而深入理解循环SNN如何维持和处理时间信息。",
            "id": "4063219",
            "problem": "考虑一个执行时间模式识别的循环脉冲神经网络 (SNN)，其宏观发放活动可以通过一个光滑的群体平均发放率模型很好地近似。该模型是在类扩散输入和粗粒化假设下，从泄漏整合发放 (LIF) 神经元导出的。发放率向量 $r(t) \\in \\mathbb{R}^{3}$ 的网络动力学由以下常微分方程给出\n$$\n\\tau \\,\\frac{d r(t)}{d t} \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big),\n$$\n其中 $\\tau$ 是有效膜时间常数，$W \\in \\mathbb{R}^{3 \\times 3}$ 是循环连接矩阵，$I_{0} \\in \\mathbb{R}^{3}$ 是一个恒定输入向量，而 $\\phi(\\cdot)$ 是逐元素应用的静态电流-发放率传递函数。假设 $\\phi$ 是 logistic 函数\n$$\n\\phi(u) \\;=\\; \\frac{1}{1 + \\exp\\!\\big(-\\beta\\,(u - \\theta)\\big)},\n$$\n其斜率为 $\\beta$，阈值为 $\\theta$。\n\n假设恒定输入 $I_{0}$ 已经过调整，使得网络有一个不动点 $r^{\\star} = \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\end{pmatrix}$ 满足 $r^{\\star} = \\phi\\!\\big(W r^{\\star} + I_{0}\\big)$，且循环连接为\n$$\nW \\;=\\; \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix}.\n$$\n取 $\\tau = 0.02$ s 和 $\\beta = 3$。使用光滑动力系统不动点周围线性化的基本原理，推导在 $r^{\\star}$ 处的线性化动力学，计算雅可比矩阵的特征值，并由此确定由特征模态编码的时间模式表示的稳定性。以 $\\mathrm{s}^{-1}$ 为单位表示线性化系统的三个特征值，并将最终数值结果四舍五入到三位有效数字。",
            "solution": "该问题要求分析一个循环发放率神经网络模型中不动点的稳定性。分析过程是通过在给定的不动点周围对系统动力学进行线性化，然后计算所得雅可比矩阵的特征值。\n\n发放率向量 $r(t) \\in \\mathbb{R}^{3}$ 的动力学由以下常微分方程 (ODE) 给出：\n$$\n\\tau \\,\\frac{d r(t)}{d t} \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big)\n$$\n让我们将函数 $F(r)$ 定义为 ODE 的右侧 (预先乘以 $1/\\tau$)：\n$$\nF(r) \\;=\\; -\\,r(t) \\;+\\; \\phi\\!\\big(W\\,r(t) + I_{0}\\big)\n$$\n因此系统为 $\\frac{dr}{dt} = \\frac{1}{\\tau} F(r)$。系统的一个不动点 $r^{\\star}$ 满足 $\\frac{dr}{dt} = 0$，这意味着 $F(r^{\\star}) = 0$，或者明确地写为：\n$$\nr^{\\star} \\;=\\; \\phi\\!\\big(W\\,r^{\\star} + I_{0}\\big)\n$$\n问题给出 $r^{\\star} = \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\end{pmatrix}$ 是这样一个不动点。\n\n为了分析这个不动点的局部稳定性，我们在 $r^{\\star}$ 周围对动力学进行线性化。令 $r(t) = r^{\\star} + x(t)$，其中 $x(t)$ 是一个小的微扰向量。将此代入系统的 ODE：\n$$\n\\tau \\,\\frac{d}{d t}\\big(r^{\\star} + x(t)\\big) \\;=\\; -\\,\\big(r^{\\star} + x(t)\\big) \\;+\\; \\phi\\!\\big(W\\,(r^{\\star} + x(t)) + I_{0}\\big)\n$$\n由于 $r^{\\star}$ 是常数，$\\frac{dr^{\\star}}{dt} = 0$。微扰 $x(t)$ 的方程变为：\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;=\\; -\\,r^{\\star} - x(t) \\;+\\; \\phi\\!\\big(W\\,r^{\\star} + I_{0} + W\\,x(t)\\big)\n$$\n我们对非线性函数 $\\phi$ 在不动点处的输入 $u^{\\star} = W\\,r^{\\star} + I_{0}$ 周围进行一阶泰勒展开。对于一个小的微扰输入 $\\Delta u = W\\,x(t)$，我们有：\n$$\n\\phi(u^{\\star} + \\Delta u) \\;\\approx\\; \\phi(u^{\\star}) + \\phi'(u^{\\star}) \\Delta u\n$$\n其中导数是逐元素应用的。令 $D$ 为一个对角矩阵，其对角线元素是在 $u^{\\star}$ 的每个分量上计算的 $\\phi$ 的导数：$D_{ii} = \\phi'((u^{\\star})_i)$。向量形式的展开为：\n$$\n\\phi\\!\\big(W\\,r^{\\star} + I_{0} + W\\,x(t)\\big) \\;\\approx\\; \\phi(W\\,r^{\\star} + I_{0}) + D\\,W\\,x(t)\n$$\n将此代回微扰方程：\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;\\approx\\; -\\,r^{\\star} - x(t) + \\phi(W\\,r^{\\star} + I_{0}) + D\\,W\\,x(t)\n$$\n使用不动点条件 $r^{\\star} = \\phi(W\\,r^{\\star} + I_{0})$，方程简化为：\n$$\n\\tau \\,\\frac{d x(t)}{d t} \\;\\approx\\; -x(t) + D\\,W\\,x(t) \\;=\\; (D\\,W - I) x(t)\n$$\n其中 $I$ 是 $3 \\times 3$ 的单位矩阵。因此，线性化动力学为：\n$$\n\\frac{d x(t)}{d t} \\;=\\; J\\,x(t), \\quad \\text{其中} \\quad J = \\frac{1}{\\tau} (D\\,W - I)\n$$\n矩阵 $J$ 是在不动点 $r^{\\star}$ 处计算的系统雅可比矩阵。不动点的稳定性由 $J$ 的特征值决定。\n\n我们现在计算雅可比矩阵的各分量。\n传递函数是 logistic 函数 $\\phi(u) = \\frac{1}{1 + \\exp(-\\beta(u-\\theta))}$。其导数为：\n$$\n\\phi'(u) = \\beta\\,\\phi(u)\\,\\big(1 - \\phi(u)\\big)\n$$\n在不动点处，神经元 $i$ 的输入，记为 $(u^{\\star})_i = (W\\,r^{\\star} + I_{0})_i$，满足 $\\phi((u^{\\star})_i) = r^{\\star}_i$。由于 $r^{\\star}$ 的每个分量都是 $r^{\\star}_i = 0.4$，所以对每个神经元而言，$\\phi$ 的导数是相同的：\n$$\n\\phi'((u^{\\star})_i) = \\beta\\,r^{\\star}_i\\,(1 - r^{\\star}_i) = 3 \\times 0.4 \\times (1 - 0.4) = 3 \\times 0.4 \\times 0.6 = 0.72\n$$\n设该导数值为 $d = 0.72$。那么矩阵 $D$ 为 $D = d \\cdot I = 0.72 \\cdot I$。\n\n雅可比矩阵为 $J = \\frac{1}{\\tau}(d\\,W - I)$。给定 $\\tau=0.02\\,\\mathrm{s}$ 和连接矩阵：\n$$\nW \\;=\\; \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix}\n$$\n括号内的矩阵是 $M = d\\,W - I$：\n$$\nM = 0.72 \\begin{pmatrix}\n0.8  0.3  0.3 \\\\\n0.3  0.8  0.3 \\\\\n0.3  0.3  0.8\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n0.576  0.216  0.216 \\\\\n0.216  0.576  0.216 \\\\\n0.216  0.216  0.576\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\n$$\nM = \\begin{pmatrix}\n-0.424  0.216  0.216 \\\\\n0.216  -0.424  0.216 \\\\\n0.216  0.216  -0.424\n\\end{pmatrix}\n$$\n雅可比矩阵 $J$ 的特征值是 $M$ 的特征值乘以 $\\frac{1}{\\tau}$。矩阵 $M$ 具有 $c_1 I + c_2 \\mathbf{1}\\mathbf{1}^T$ 形式的对称结构。这类矩阵有一个与全一向量 $\\mathbf{v}_1 = (1, 1, 1)^T$ 平行的特征向量，其余的特征向量位于与 $\\mathbf{v}_1$ 正交的子空间中。\n让我们找出对应于 $\\mathbf{v}_1$ 的特征值 $\\lambda_{M,1}$：\n$$\nM \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -0.424 + 0.216 + 0.216 \\\\ 0.216 - 0.424 + 0.216 \\\\ 0.216 + 0.216 - 0.424 \\end{pmatrix} = \\begin{pmatrix} 0.008 \\\\ 0.008 \\\\ 0.008 \\end{pmatrix} = 0.008 \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n因此，$M$ 的一个特征值是 $\\lambda_{M,1} = 0.008$。\n由于对称性，另外两个特征值是简并的。任何与 $(1, 1, 1)^T$ 正交的向量，例如 $\\mathbf{v}_2 = (1, -1, 0)^T$，都是一个特征向量：\n$$\nM \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -0.424 - 0.216 \\\\ 0.216 + 0.424 \\\\ 0.216 - 0.216 \\end{pmatrix} = \\begin{pmatrix} -0.64 \\\\ 0.64 \\\\ 0 \\end{pmatrix} = -0.64 \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\n对应的特征值是 $\\lambda_{M,2} = -0.64$。由于对称性，第三个特征值也是 $\\lambda_{M,3} = -0.64$。\n\n雅可比矩阵 $J$ 的特征值通过将 $M$ 的特征值乘以 $\\frac{1}{\\tau} = \\frac{1}{0.02} = 50 \\, \\mathrm{s}^{-1}$ 得到：\n$$\n\\lambda_1 = 50 \\times \\lambda_{M,1} = 50 \\times 0.008 = 0.4 \\, \\mathrm{s}^{-1}\n$$\n$$\n\\lambda_2 = \\lambda_3 = 50 \\times \\lambda_{M,2} = 50 \\times (-0.64) = -32 \\, \\mathrm{s}^{-1}\n$$\n不动点的稳定性由这些特征值实部的符号决定。我们有一个正实数特征值 $\\lambda_1 = 0.4$ 和两个负实数特征值 $\\lambda_2 = \\lambda_3 = -32$。正特征值的存在表明不动点 $r^{\\star}$ 是不稳定的。具体来说，它是一个鞍点。\n沿着与 $\\lambda_1$ 对应的特征向量（即所有发放率一起变化的共模）方向的微扰将呈指数增长，使系统状态偏离不动点。在由 $\\lambda_2$ 和 $\\lambda_3$ 的特征向量张成的二维子空间（差模）中的微扰将会衰减，使状态返回该子空间。因此，与共模对应的时间模式是不稳定的，而与差模对应的则是稳定的。\n\n问题要求将三个特征值四舍五入到三位有效数字。\n$\\lambda_1 = 0.4$ 变为 $0.400$。\n$\\lambda_2 = -32$ 变为 $-32.0$。\n$\\lambda_3 = -32$ 变为 $-32.0$。\n单位是 $\\mathrm{s}^{-1}$。\n\n线性化系统的三个特征值是 $\\{0.400, -32.0, -32.0\\}$，单位为 $\\mathrm{s}^{-1}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.400 & -32.0 & -32.0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "从分析转向学习，一个核心挑战是如何训练SNN。虽然基于梯度的强大方法（如反向传播）是深度学习的基石，但SNN中脉冲的“全或无”特性使其不可微，从而阻碍了梯度的计算。本练习将带您探索代理梯度法，这是克服此问题的标准技术。您将亲手实现并评估不同的代理函数，并通过量化其引入的梯度偏差，深入理解在计算简便性与学习保真度之间的权衡。",
            "id": "4063246",
            "problem": "考虑一个用于检测时间基序的离散时间、双层脉冲神经网络。时间由整数步长 $t \\in \\{0,1,\\dots,T-1\\}$ 索引，其中 $T$ 是指定的。输入层提供一个二元脉冲序列 $x(t) \\in \\{0,1\\}$，其中包含由在指定时间发生的脉冲所定义的时间基序。第一层包含一个神经元，该神经元具有线性突触滤波器和逻辑脉冲非线性。第二层（输出层）包含一个神经元，它线性地聚合第一层随时间发出的脉冲，并应用逻辑非线性来为目标分类生成一个伯努利概率。\n\n定义和前向模型：\n- 设突触核为因果指数核 $h(\\Delta t) = \\exp(-\\Delta t/\\tau)$（对于 $\\Delta t \\ge 0$）和 $h(\\Delta t) = 0$（对于 $\\Delta t < 0$），其中时间常数 $\\tau > 0$ 以离散步长为单位。\n- 第一层膜电位为 $u_1(t) = w_1 \\sum_{t'=0}^{t} x(t') h(t-t')$，其中 $w_1$ 是第一层的突触权重。\n- 第一层脉冲概率（逻辑非线性）为 $s_1(t) = \\sigma\\left(\\frac{u_1(t) - \\theta_1}{\\beta}\\right)$，其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，$\\theta_1$ 是阈值，$\\beta > 0$ 是斜率参数（在各层之间共享）。\n- 第二层膜电位为 $u_2 = w_2 \\sum_{t=0}^{T-1} s_1(t)$，其中 $w_2$ 是第二层的突触权重。\n- 输出概率为 $p = \\sigma\\left(\\frac{u_2 - \\theta_2}{\\beta}\\right)$，其中阈值为 $\\theta_2$。\n- 目标 $y \\in \\{0,1\\}$ 的损失是对数交叉熵 $\\mathcal{L}(p,y) = -y \\log p - (1-y) \\log(1-p)$。\n\n目标：\n您必须分析在使用三个标准代理导数函数来替代第一层逻辑非线性的精确导数时，估计梯度 $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$ 的偏差。真实梯度必须在上述随机逻辑模型下推导得出。代理梯度估计器必须通过将精确因子 $\\frac{\\partial s_1(t)}{\\partial u_1(t)}$ 替换为代理函数 $g(u_1(t) - \\theta_1)$ 来构建，同时保持前向模型（包括 $s_1(t)$ 和 $p$）不变。\n\n代理导数函数：\n- 分段线性三角函数：$g_{\\mathrm{pwl}}(z; a) = \\frac{1}{a} \\max\\left(0, 1 - \\frac{|z|}{a}\\right)$，宽度参数为 $a > 0$。\n- 指数函数（拉普拉斯形状）：$g_{\\mathrm{exp}}(z; a) = \\frac{1}{a} \\exp\\left(-\\frac{|z|}{a}\\right)$，尺度参数为 $a > 0$。\n- 高斯函数：$g_{\\mathrm{gauss}}(z; a) = \\frac{1}{\\sqrt{2\\pi} a} \\exp\\left(-\\frac{z^2}{2 a^2}\\right)$，标准差为 $a > 0$。\n\n偏差定义：\n对于每个代理函数 $g$，通过在链式法则中用 $g(u_1(t) - \\theta_1)$ 替代 $\\frac{\\partial s_1(t)}{\\partial u_1(t)}$ 来定义基于代理的估计器 $\\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g)$，并将偏差定义为 $B(g) = \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) - \\frac{\\partial \\mathcal{L}}{\\partial w_1}$。\n\n您的程序必须实现前向模型，从第一性原理推导并计算真实梯度 $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$，然后计算三个基于代理的估计及其偏差 $B(g_{\\mathrm{pwl}})$、$B(g_{\\mathrm{exp}})$ 和 $B(g_{\\mathrm{gauss}})$。\n\n测试套件：\n使用以下五个测试用例，每个用例由 $(T, \\tau, \\text{脉冲时间}, y, w_1, w_2, \\theta_1, \\theta_2, \\beta, a)$ 指定：\n1. $(\\;20,\\;5.0,\\;\\{4,9\\},\\;1,\\;1.0,\\;1.2,\\;0.9,\\;1.1,\\;0.25,\\;0.25\\;)$\n2. $(\\;20,\\;5.0,\\;\\{7\\},\\;0,\\;1.0,\\;1.0,\\;1.2,\\;1.0,\\;0.25,\\;0.25\\;)$\n3. $(\\;20,\\;5.0,\\;\\{5,6\\},\\;1,\\;0.8,\\;1.3,\\;0.8,\\;1.0,\\;0.25,\\;0.25\\;)$\n4. $(\\;20,\\;5.0,\\;\\{4,9\\},\\;1,\\;1.0,\\;1.2,\\;0.9,\\;1.1,\\;0.25,\\;0.05\\;)$\n5. $(\\;20,\\;5.0,\\;\\{4,9\\},\\;1,\\;1.0,\\;1.2,\\;0.9,\\;1.1,\\;0.25,\\;1.0\\;)$\n\n在所有情况下，通过在列出的脉冲时间设置 $x(t)=1$ 并在其他时间设置 $x(t)=0$ 来构建 $x(t)$。这些参数是无量纲的。对于每个用例，计算并返回偏差列表 $[B(g_{\\mathrm{pwl}}), B(g_{\\mathrm{exp}}), B(g_{\\mathrm{gauss}})]$，结果为浮点数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个列表的列表，每个内部列表对应一个测试用例，并按指定顺序包含三个偏差。该行的格式应完全为方括号内包含的逗号分隔列表，例如 $[\\,[b_{1,\\mathrm{pwl}},b_{1,\\mathrm{exp}},b_{1,\\mathrm{gauss}}],\\dots\\,]$，数值以标准浮点表示法打印。不允许有其他文本或换行。",
            "solution": "该问题要求计算在训练一个双层脉冲神经网络时，因使用代理梯度函数而引入的偏差。偏差定义为使用代理函数推导的梯度估计器与从已定义的随机模型推导的真实梯度之间的差值。我们将首先推导真实梯度的解析表达式，然后是基于代理的估计器的表达式，最后是偏差的表达式。\n\n完整的前向模型由以下方程定义：\n- 输入脉冲序列：$x(t) \\in \\{0, 1\\}$，对于 $t \\in \\{0, 1, \\dots, T-1\\}$。\n- 突触核：$h(\\Delta t) = \\exp(-\\Delta t/\\tau)$，对于 $\\Delta t \\ge 0$。\n- 第一层膜电位：$u_1(t) = w_1 \\sum_{t'=0}^{t} x(t') h(t-t')$。\n- 第一层脉冲概率：$s_1(t) = \\sigma\\left(\\frac{u_1(t) - \\theta_1}{\\beta}\\right)$，其中 $\\sigma(z) = (1 + e^{-z})^{-1}$。\n- 第二层膜电位：$u_2 = w_2 \\sum_{t=0}^{T-1} s_1(t)$。\n- 输出概率：$p = \\sigma\\left(\\frac{u_2 - \\theta_2}{\\beta}\\right)$。\n- 损失函数：$\\mathcal{L}(p,y) = -y \\log p - (1-y) \\log(1-p)$。\n\n我们的目标是计算第一层权重 $w_1$ 的偏差 $B(g) = \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) - \\frac{\\partial \\mathcal{L}}{\\partial w_1}$。\n\n首先，我们通过应用链式法则来推导真实梯度 $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$。梯度计算遵循从 $w_1$ 到 $\\mathcal{L}$ 的影响路径：$w_1 \\rightarrow u_1(t) \\rightarrow s_1(t) \\rightarrow u_2 \\rightarrow p \\rightarrow \\mathcal{L}$。\n\n1.  损失 $\\mathcal{L}$ 相对于第二层电位 $u_2$ 的导数：\n    逻辑函数的导数是 $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$。\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial p} = -\\frac{y}{p} + \\frac{1-y}{1-p} $$\n    $$ \\frac{\\partial p}{\\partial u_2} = \\frac{\\partial}{\\partial u_2}\\sigma\\left(\\frac{u_2 - \\theta_2}{\\beta}\\right) = \\sigma'\\left(\\frac{u_2 - \\theta_2}{\\beta}\\right) \\frac{1}{\\beta} = \\frac{p(1-p)}{\\beta} $$\n    将这些结合起来，得到对数交叉熵损失的一个常见且简化的结果：\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial u_2} = \\frac{\\partial \\mathcal{L}}{\\partial p} \\frac{\\partial p}{\\partial u_2} = \\left(-\\frac{y}{p} + \\frac{1-y}{1-p}\\right) \\frac{p(1-p)}{\\beta} = \\frac{-y(1-p) + p(1-y)}{\\beta} = \\frac{p-y}{\\beta} $$\n\n2.  $u_2$ 相对于 $w_1$ 的导数：\n    $$ \\frac{\\partial u_2}{\\partial w_1} = \\frac{\\partial}{\\partial w_1} \\left(w_2 \\sum_{t=0}^{T-1} s_1(t)\\right) = w_2 \\sum_{t=0}^{T-1} \\frac{\\partial s_1(t)}{\\partial w_1} $$\n    为了求得 $\\frac{\\partial s_1(t)}{\\partial w_1}$，我们再次应用链式法则：\n    $$ \\frac{\\partial s_1(t)}{\\partial w_1} = \\frac{\\partial s_1(t)}{\\partial u_1(t)} \\frac{\\partial u_1(t)}{\\partial w_1} $$\n\n3.  第一层的导数项：\n    第一层脉冲概率 $s_1(t)$ 相对于其电位 $u_1(t)$ 的导数是：\n    $$ \\frac{\\partial s_1(t)}{\\partial u_1(t)} = \\frac{\\partial}{\\partial u_1(t)} \\sigma\\left(\\frac{u_1(t) - \\theta_1}{\\beta}\\right) = \\sigma'\\left(\\frac{u_1(t) - \\theta_1}{\\beta}\\right) \\frac{1}{\\beta} = \\frac{s_1(t)(1-s_1(t))}{\\beta} $$\n    第一层电位 $u_1(t)$ 相对于权重 $w_1$ 的导数是：\n    $$ \\frac{\\partial u_1(t)}{\\partial w_1} = \\frac{\\partial}{\\partial w_1} \\left(w_1 \\sum_{t'=0}^{t} x(t') h(t-t')\\right) = \\sum_{t'=0}^{t} x(t') h(t-t') = \\frac{u_1(t)}{w_1} $$\n\n4.  组合真实梯度 $\\frac{\\partial \\mathcal{L}}{\\partial w_1}$：\n    组合各部分，我们得到：\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{\\partial \\mathcal{L}}{\\partial u_2} \\frac{\\partial u_2}{\\partial w_1} = \\left(\\frac{p-y}{\\beta}\\right) \\left(w_2 \\sum_{t=0}^{T-1} \\frac{\\partial s_1(t)}{\\partial u_1(t)} \\frac{\\partial u_1(t)}{\\partial w_1}\\right) $$\n    代入步骤3中的表达式：\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{w_2(p-y)}{\\beta} \\sum_{t=0}^{T-1} \\left(\\frac{s_1(t)(1-s_1(t))}{\\beta}\\right) \\left(\\frac{u_1(t)}{w_1}\\right) $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{w_2(p-y)}{w_1 \\beta^2} \\sum_{t=0}^{T-1} u_1(t) s_1(t)(1-s_1(t)) $$\n    这就是精确梯度。\n\n接下来，我们构建基于代理的梯度估计器 $\\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g)$。这是通过将非线性的真实导数 $\\frac{\\partial s_1(t)}{\\partial u_1(t)}$ 替换为代理函数 $g(u_1(t) - \\theta_1)$ 来构建的。\n$$ \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) = \\frac{w_2(p-y)}{\\beta} \\sum_{t=0}^{T-1} g\\left(u_1(t) - \\theta_1\\right) \\frac{\\partial u_1(t)}{\\partial w_1} $$\n$$ \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) = \\frac{w_2(p-y)}{w_1 \\beta} \\sum_{t=0}^{T-1} u_1(t) g\\left(u_1(t) - \\theta_1\\right) $$\n\n最后，偏差 $B(g)$ 是估计器与真实梯度之间的差值：\n$$ B(g) = \\widehat{\\frac{\\partial \\mathcal{L}}{\\partial w_1}}(g) - \\frac{\\partial \\mathcal{L}}{\\partial w_1} $$\n$$ B(g) = \\left(\\frac{w_2(p-y)}{w_1 \\beta} \\sum_{t=0}^{T-1} u_1(t) g(u_1(t)-\\theta_1)\\right) - \\left(\\frac{w_2(p-y)}{w_1 \\beta^2} \\sum_{t=0}^{T-1} u_1(t) s_1(t)(1-s_1(t))\\right) $$\n提取公因式得到偏差的最终表达式：\n$$ B(g) = \\frac{w_2(p-y)}{w_1 \\beta} \\sum_{t=0}^{T-1} u_1(t) \\left( g(u_1(t)-\\theta_1) - \\frac{s_1(t)(1-s_1(t))}{\\beta} \\right) $$\n\n为每个代理函数计算偏差的计算过程如下：\n1.  对于给定的测试用例，构建输入脉冲序列 $x(t)$。\n2.  执行前向传播：\n    a. 计算 $\\Delta t \\in \\{0, \\dots, T-1\\}$ 的突触核值 $h(\\Delta t)$。\n    b. 将 $x(t)$ 与 $h(t)$进行卷积，并按 $w_1$ 缩放，以获得所有 $t$ 的第一层电位 $u_1(t)$。\n    c. 对 $u_1(t)$ 应用逻辑函数，以获得第一层脉冲概率 $s_1(t)$。\n    d. 将 $s_1(t)$ 随时间求和，并按 $w_2$ 缩放，以获得第二层电位 $u_2$。\n    e. 对 $u_2$ 应用逻辑函数，以获得最终输出概率 $p$。\n3.  使用推导出的公式计算偏差：\n    a. 计算总前置因子 $\\frac{w_2(p-y)}{w_1 \\beta}$。\n    b. 对于每个时间步 $t$，计算差值项：$D(t) = g(u_1(t)-\\theta_1) - \\frac{s_1(t)(1-s_1(t))}{\\beta}$。对三个代理函数 $g_{\\mathrm{pwl}}$、$g_{\\mathrm{exp}}$ 和 $g_{\\mathrm{gauss}}$ 分别执行此操作。\n    c. 计算总和 $\\sum_{t=0}^{T-1} u_1(t) D(t)$。\n    d. 将总和乘以预因子，以获得每个代理函数的偏差。\n对提供的每个测试用例实施此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of surrogate gradient estimators for a two-layer SNN.\n    \"\"\"\n    # Test cases from the problem statement:\n    # (T, tau, spike_times, y, w1, w2, theta1, theta2, beta, a)\n    test_cases = [\n        (20, 5.0, {4, 9}, 1, 1.0, 1.2, 0.9, 1.1, 0.25, 0.25),\n        (20, 5.0, {7}, 0, 1.0, 1.0, 1.2, 1.0, 0.25, 0.25),\n        (20, 5.0, {5, 6}, 1, 0.8, 1.3, 0.8, 1.0, 0.25, 0.25),\n        (20, 5.0, {4, 9}, 1, 1.0, 1.2, 0.9, 1.1, 0.25, 0.05),\n        (20, 5.0, {4, 9}, 1, 1.0, 1.2, 0.9, 1.1, 0.25, 1.0),\n    ]\n\n    results = []\n    \n    # Define the logistic sigmoid function\n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Define the surrogate derivative functions\n    def g_pwl(z, a):\n        return (1.0 / a) * np.maximum(0, 1 - np.abs(z) / a)\n\n    def g_exp(z, a):\n        return (1.0 / a) * np.exp(-np.abs(z) / a)\n\n    def g_gauss(z, a):\n        return (1.0 / (np.sqrt(2 * np.pi) * a)) * np.exp(-(z**2) / (2 * a**2))\n\n    for case in test_cases:\n        T, tau, spike_times, y, w1, w2, theta1, theta2, beta, a = case\n\n        # 1. Construct input spike train x(t)\n        x = np.zeros(T)\n        for t_spike in spike_times:\n            if 0 = t_spike  T:\n                x[t_spike] = 1.0\n\n        # 2. Perform the forward pass\n        # 2a. Synaptic kernel h(dt)\n        time_steps = np.arange(T)\n        h = np.exp(-time_steps / tau)\n\n        # 2b. First-layer potential u1(t)\n        # u1(t) = w1 * sum_{t'=0 to t} x(t') h(t-t')\n        # This is equivalent to a convolution\n        u1 = w1 * np.convolve(x, h)[:T]\n\n        # 2c. First-layer spiking probability s1(t)\n        z1 = (u1 - theta1) / beta\n        s1 = sigma(z1)\n\n        # 2d. Second-layer potential u2\n        u2 = w2 * np.sum(s1)\n\n        # 2e. Output probability p\n        z2 = (u2 - theta2) / beta\n        p = sigma(z2)\n\n        # 3. Calculate the bias B(g)\n        # B(g) = (w2*(p-y)/(w1*beta)) * sum(u1 * (g(u1-theta1) - s1*(1-s1)/beta))\n        \n        pre_factor = (w2 * (p - y)) / (w1 * beta)\n        \n        # Difference terms\n        z1_shifted = u1 - theta1\n        true_deriv_term = s1 * (1 - s1) / beta\n        \n        # Calculate surrogate values\n        g_pwl_vals = g_pwl(z1_shifted, a)\n        g_exp_vals = g_exp(z1_shifted, a)\n        g_gauss_vals = g_gauss(z1_shifted, a)\n\n        # Calculate difference between surrogate and true derivative\n        diff_pwl = g_pwl_vals - true_deriv_term\n        diff_exp = g_exp_vals - true_deriv_term\n        diff_gauss = g_gauss_vals - true_deriv_term\n        \n        # Calculate sum over time, weighted by u1\n        sum_pwl = np.sum(u1 * diff_pwl)\n        sum_exp = np.sum(u1 * diff_exp)\n        sum_gauss = np.sum(u1 * diff_gauss)\n        \n        # Final bias values\n        bias_pwl = pre_factor * sum_pwl\n        bias_exp = pre_factor * sum_exp\n        bias_gauss = pre_factor * sum_gauss\n        \n        biases_for_case = [bias_pwl, bias_exp, bias_gauss]\n        results.append(biases_for_case)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后一个实践环节关注将训练好的SNN部署到物理硬件时面临的现实约束。为了追求极致的能效和面积效率，神经形态芯片通常使用低精度权重。本练习旨在量化这种权重精度限制对网络性能的影响。您将通过推导一个保证分类性能不变的充分条件，来计算维持网络功能所需的最小比特宽度，从而直接连接理论分析与硬件设计规范。",
            "id": "4063206",
            "problem": "考虑一个单层脉冲神经网络 (SNN)，它通过一个线性时序滤波器执行双类别时序模式识别。时间被离散化为 $L$ 个区间，每个输入模式表示为一个区间化脉冲计数的向量 $s \\in \\mathbb{R}^{L}$。学习到的时序滤波器是一个权重向量 $h \\in \\mathbb{R}^{L}$。对于一个模式 $s$，其决策变量是线性响应 $d = \\sum_{t=1}^{L} h_{t} s_{t}$，预测的类别是 $d$ 的符号，阈值为零。对于一个带标签的样本 $(s^{(i)}, y^{(i)})$，其标签为 $y^{(i)} \\in \\{ -1, +1 \\}$，带符号间隔为 $m^{(i)} = y^{(i)} d^{(i)}$，其中 $d^{(i)} = \\sum_{t=1}^{L} h_{t} s_{t}^{(i)}$。\n\n假设权重向量 $h$ 必须使用一个 $b$ 位的均匀对称定点量化器在动态范围 $[-W_{\\max}, W_{\\max}]$ 内以有限精度实现。该量化器在此范围内有 $2^{b}$ 个离散电平，并使用四舍五入到最近电平的方法。量化后的权重向量记为 $\\hat{h}$，量化步长为 $\\Delta$，每个权重的量化误差的幅值以 $\\Delta / 2$ 为界。假设没有发生饱和，即每个真实权重都满足 $h_{t} \\in [-W_{\\max}, W_{\\max}]$。\n\n从线性滤波和均匀量化的基本定义出发，推导出一个充分条件，以保证在量化后所有带符号间隔都保持为正，即对于所有给定的带标签样本，都有 $y^{(i)} \\sum_{t=1}^{L} \\hat{h}_{t} s_{t}^{(i)}  0$。使用三角不等式为量化引起的决策扰动设定上界。然后，将满足此充分条件的最小位宽 $b$ 表示为学习到的滤波器 $h$、带标签的输入模式 $\\{(s^{(i)}, y^{(i)})\\}$ 和动态范围参数 $W_{\\max}$ 的函数，并取最小整数。\n\n您的程序必须实现这种基于充分条件的最小位宽计算，并为以下测试套件生成一个单行输出，汇总所有结果。在每种情况下，假设采用四舍五入到最近值且无饱和，计算在给定带标签样本上保持原始分类所需的最小整数位宽 $b$。\n\n测试套件：\n- 情况 $1$：\n  - $L = 6$，\n  - $h^{(1)} = \\left[\\,0.6,\\,0.5,\\,0.2,\\,-0.1,\\,-0.2,\\,-0.3\\,\\right]$，\n  - 带标签的模式\n    - $(s^{(1)}_{1}, y^{(1)}_{1})$，其中 $s^{(1)}_{1} = \\left[\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ 且 $y^{(1)}_{1} = +1$，\n    - $(s^{(1)}_{2}, y^{(1)}_{2})$，其中 $s^{(1)}_{2} = \\left[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1\\,\\right]$ 且 $y^{(1)}_{2} = -1$，\n    - $(s^{(1)}_{3}, y^{(1)}_{3})$，其中 $s^{(1)}_{3} = \\left[\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,\\right]$ 且 $y^{(1)}_{3} = +1$，\n  - $W_{\\max}^{(1)} = 1.0$。\n- 情况 $2$：\n  - $L = 6$，\n  - $h^{(2)} = \\left[\\,0.5,\\,0.49,\\,0.01,\\,-0.01,\\,-0.49,\\,-0.5\\,\\right]$，\n  - 带标签的模式\n    - $(s^{(2)}_{1}, y^{(2)}_{1})$，其中 $s^{(2)}_{1} = \\left[\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ 且 $y^{(2)}_{1} = +1$，\n    - $(s^{(2)}_{2}, y^{(2)}_{2})$，其中 $s^{(2)}_{2} = \\left[\\,0,\\,0,\\,0,\\,0,\\,1,\\,1\\,\\right]$ 且 $y^{(2)}_{2} = -1$，\n    - $(s^{(2)}_{3}, y^{(2)}_{3})$，其中 $s^{(2)}_{3} = \\left[\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,\\right]$ 且 $y^{(2)}_{3} = -1$，\n  - $W_{\\max}^{(2)} = 0.5$。\n- 情况 $3$：\n  - $L = 6$，\n  - $h^{(3)} = \\left[\\,0.1,\\,-0.1,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,\\right]$，\n  - 带标签的模式\n    - $(s^{(3)}_{1}, y^{(3)}_{1})$，其中 $s^{(3)}_{1} = \\left[\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,\\right]$ 且 $y^{(3)}_{1} = +1$，\n    - $(s^{(3)}_{2}, y^{(3)}_{2})$，其中 $s^{(3)}_{2} = \\left[\\,0,\\,1,\\,0,\\,0,\\,0,\\,0\\,\\right]$ 且 $y^{(3)}_{2} = -1$，\n  - $W_{\\max}^{(3)} = 2.0$。\n\n实现细节：\n- 您的程序必须为每种情况计算全精度下的带符号间隔 $m^{(i)}$、最大绝对输入和 $S = \\max_{i} \\sum_{t=1}^{L} |s^{(i)}_{t}|$，然后计算您推导的充分条件所需的最小整数位宽 $b$。\n- 最终输出必须是单行文本，包含一个用方括号括起来的逗号分隔列表，例如 $\\left[\\,b_{1},b_{2},b_{3}\\,\\right]$，其中 $b_{k}$ 是您的方法为情况 $k$ 计算出的最小位宽。输出必须是整数。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$\\left[\\,b_{1},b_{2},b_{3}\\,\\right]$）。不允许有其他输出。",
            "solution": "该问题要求推导一个充分条件，以确保单层脉冲神经网络 (SNN) 的线性时序滤波器权重在量化后，其分类符号得以保留。根据这个条件，我们必须确定所需的最小整数位宽 $b$。\n\n问题的核心在于分析量化误差如何通过线性滤波操作传播，并影响决策变量的符号。我们已知对于所有样本，初始分类都是正确的，这意味着对于所有样本 $i$，全精度带符号间隔 $m^{(i)} = y^{(i)} d^{(i)}$ 均为正。我们必须找到一个关于量化位宽 $b$ 的条件，以保证量化后的带符号间隔 $\\hat{m}^{(i)}$ 也保持为正。\n\n让我们首先对量化过程及其产生的误差进行形式化描述。权重向量 $h \\in \\mathbb{R}^{L}$ 被量化为 $\\hat{h} \\in \\mathbb{R}^{L}$。该量化器是一个 $b$ 位的均匀对称定点量化器，作用于动态范围 $[-W_{\\max}, W_{\\max}]$。它有 $2^b$ 个离散电平。与问题陈述一致的对此类量化器的标准解释是，将量化步长 $\\Delta$ 定义为总范围除以电平数。\n$$ \\Delta = \\frac{W_{\\max} - (-W_{\\max})}{2^b} = \\frac{2W_{\\max}}{2^b} $$\n问题陈述指出，使用四舍五入到最近电平的方法，并且没有饱和发生。每个权重分量 $h_t$ 的量化误差是差值 $e_t = \\hat{h}_t - h_t$。该误差的幅值以步长的一半为界：\n$$ |\\hat{h}_t - h_t| \\le \\frac{\\Delta}{2} $$\n\n接下来，我们分析该量化误差对决策变量的影响。对于一个输入模式 $s^{(i)}$，其全精度决策变量是内积：\n$$ d^{(i)} = \\sum_{t=1}^{L} h_t s_t^{(i)} $$\n量化后，决策变量变为：\n$$ \\hat{d}^{(i)} = \\sum_{t=1}^{L} \\hat{h}_t s_t^{(i)} $$\n由于量化导致的决策变量的变化（或扰动）为 $\\delta_d^{(i)} = \\hat{d}^{(i)} - d^{(i)}$。\n$$ \\delta_d^{(i)} = \\sum_{t=1}^{L} \\hat{h}_t s_t^{(i)} - \\sum_{t=1}^{L} h_t s_t^{(i)} = \\sum_{t=1}^{L} (\\hat{h}_t - h_t) s_t^{(i)} $$\n量化后的带符号间隔为 $\\hat{m}^{(i)} = y^{(i)} \\hat{d}^{(i)}$。我们可以用原始间隔 $m^{(i)}$ 和扰动来表示它：\n$$ \\hat{m}^{(i)} = y^{(i)} (d^{(i)} + \\delta_d^{(i)}) = y^{(i)} d^{(i)} + y^{(i)} \\delta_d^{(i)} = m^{(i)} + y^{(i)} \\delta_d^{(i)} $$\n为了保持分类不变，我们要求 $\\hat{m}^{(i)}  0$。这给出了条件：\n$$ m^{(i)} + y^{(i)} \\delta_d^{(i)}  0 \\implies m^{(i)}  -y^{(i)} \\delta_d^{(i)} $$\n为了建立一个充分条件，我们必须确保这个不等式即使在最坏的情况下也成立。当扰动项 $y^{(i)} \\delta_d^{(i)}$ 为最大负值时，出现最坏情况。由于 $|y^{(i)}|=1$，该项的最小值为 $-|y^{(i)} \\delta_d^{(i)}| = -|\\delta_d^{(i)}|$。因此，保证 $\\hat{m}^{(i)}  0$ 的一个充分条件是：\n$$ m^{(i)}  |\\delta_d^{(i)}| $$\n我们现在需要找到扰动幅值 $|\\delta_d^{(i)}|$ 的一个上界。对 $\\delta_d^{(i)}$ 的表达式使用三角不等式：\n$$ |\\delta_d^{(i)}| = \\left| \\sum_{t=1}^{L} (\\hat{h}_t - h_t) s_t^{(i)} \\right| \\le \\sum_{t=1}^{L} |(\\hat{h}_t - h_t) s_t^{(i)}| = \\sum_{t=1}^{L} |\\hat{h}_t - h_t| |s_t^{(i)}| $$\n代入量化误差界 $|\\hat{h}_t - h_t| \\le \\frac{\\Delta}{2}$：\n$$ |\\delta_d^{(i)}| \\le \\sum_{t=1}^{L} \\frac{\\Delta}{2} |s_t^{(i)}| = \\frac{\\Delta}{2} \\sum_{t=1}^{L} |s_t^{(i)}| $$\n项 $\\sum_{t=1}^{L} |s_t^{(i)}|$ 是输入向量 $s^{(i)}$ 的 $L_1$ 范数。我们将其记为 $S^{(i)}$。我们对单个模式 $i$ 的充分条件变为：\n$$ m^{(i)}  \\frac{\\Delta}{2} S^{(i)} $$\n这个不等式必须对数据集中的所有带标签样本都成立。这可以通过推导一个更保守的单一条件来保证。设 $m_{\\min} = \\min_i m^{(i)}$ 为所有样本中的最小带符号间隔，设 $S = \\max_i S^{(i)} = \\max_i \\left( \\sum_{t=1}^{L} |s_t^{(i)}| \\right)$ 为所有输入模式中的最大 $L_1$ 范数。\n对于任何样本 $j$，我们有 $m^{(j)} \\ge m_{\\min}$ 和 $S^{(j)} \\le S$。因此，项 $\\frac{\\Delta}{2}S$ 是所有 $\\frac{\\Delta}{2}S^{(j)}$ 的上界。如果我们强制执行条件 $m_{\\min}  \\frac{\\Delta}{2}S$，它就能保证 $m^{(j)} \\ge m_{\\min}  \\frac{\\Delta}{2}S \\ge \\frac{\\Delta}{2}S^{(j)}$，从而对所有 $j$ 都满足该条件。这种方法与问题中计算 $S$ 的要求一致。\n\n我们的最终充分条件是：\n$$ m_{\\min}  \\frac{\\Delta S}{2} $$\n现在我们可以求解最小位宽 $b$。我们代入量化步长 $\\Delta = \\frac{2W_{\\max}}{2^b}$ 的表达式：\n$$ m_{\\min}  \\frac{S}{2} \\left( \\frac{2W_{\\max}}{2^b} \\right) = \\frac{S \\cdot W_{\\max}}{2^b} $$\n整理不等式以求解 $2^b$：\n$$ 2^b  \\frac{S \\cdot W_{\\max}}{m_{\\min}} $$\n为了找到满足这个严格不等式的最小整数 $b$，我们对两边取以 2 为底的对数：\n$$ b  \\log_2 \\left( \\frac{S \\cdot W_{\\max}}{m_{\\min}} \\right) $$\n设 $X = \\frac{S \\cdot W_{\\max}}{m_{\\min}}$。大于 $\\log_2(X)$ 的最小整数 $b$ 由 $\\lfloor \\log_2(X) \\rfloor + 1$ 给出。该公式在 $X  0$ 时有效。鉴于 $S \\ge 0$，$W_{\\max}  0$，并且我们已经确认所有测试用例的 $m_{\\min}  0$，因此 $X$ 将为正。如果 $X \\le 1$，则意味着 $b  \\log_2(X)$，其中 $\\log_2(X) \\le 0$。位宽 $b=1$ 就足够了，我们假设 $b \\ge 1$ 是一个实际的最小值。\n所需的最小位宽的最终表达式是：\n$$ b = \\left\\lfloor \\log_2 \\left( \\frac{W_{\\max} \\cdot \\max_i \\left( \\sum_{t=1}^{L} |s_t^{(i)}| \\right)}{\\min_i \\left( y^{(i)} \\sum_{t=1}^{L} h_t s_t^{(i)} \\right)} \\right) \\right\\rfloor + 1 $$\n此公式将被实现，以求解每个测试用例的 $b$。",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the minimal integer bit-width `b` required to maintain the\n    original classification on a set of labeled examples, based on a\n    derived sufficient condition.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"h\": np.array([0.6, 0.5, 0.2, -0.1, -0.2, -0.3]),\n            \"patterns\": [\n                (np.array([1, 1, 0, 0, 0, 0]), 1),\n                (np.array([0, 0, 0, 0, 1, 1]), -1),\n                (np.array([0, 1, 0, 1, 0, 1]), 1)\n            ],\n            \"W_max\": 1.0\n        },\n        # Case 2\n        {\n            \"h\": np.array([0.5, 0.49, 0.01, -0.01, -0.49, -0.5]),\n            \"patterns\": [\n                (np.array([1, 1, 0, 0, 0, 0]), 1),\n                (np.array([0, 0, 0, 0, 1, 1]), -1),\n                (np.array([0, 1, 0, 1, 0, 1]), -1)\n            ],\n            \"W_max\": 0.5\n        },\n        # Case 3\n        {\n            \"h\": np.array([0.1, -0.1, 0.0, 0.0, 0.0, 0.0]),\n            \"patterns\": [\n                (np.array([1, 0, 0, 0, 0, 0]), 1),\n                (np.array([0, 1, 0, 0, 0, 0]), -1)\n            ],\n            \"W_max\": 2.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        h = case[\"h\"]\n        patterns = case[\"patterns\"]\n        W_max = case[\"W_max\"]\n\n        # Step 1  2: Calculate all signed margins m_i and find the minimum m_min.\n        margins = []\n        for s_i, y_i in patterns:\n            # Calculate the linear response (decision variable)\n            d_i = np.dot(h, s_i)\n            # Calculate the signed margin\n            m_i = y_i * d_i\n            margins.append(m_i)\n        \n        m_min = min(margins)\n\n        # Step 3  4: Calculate the L1-norm of all inputs S_i and find the maximum S.\n        s_norms = []\n        for s_i, y_i in patterns:\n            s_norm = np.sum(np.abs(s_i))\n            s_norms.append(s_norm)\n            \n        S = max(s_norms)\n\n        # Step 5: Calculate the argument X for the logarithm\n        # The derivation guarantees m_min > 0, so no division by zero is expected.\n        X = (W_max * S) / m_min\n        \n        # Step 6: Calculate the minimal integer bit-width b.\n        # The condition is b > log2(X). The smallest integer b is floor(log2(X)) + 1.\n        # A bit-width b must be at least 1. If X = 1, log2(X) = 0, and the formula\n        # might yield 1 or less. b=1 is the effective minimum.\n        if X = 1:\n            b = 1\n        else:\n            b = math.floor(math.log2(X)) + 1\n        \n        results.append(b)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}