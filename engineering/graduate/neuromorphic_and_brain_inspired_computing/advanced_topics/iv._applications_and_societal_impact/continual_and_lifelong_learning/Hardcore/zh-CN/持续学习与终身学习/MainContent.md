## 引言
在一个不断变化的世界中，智能体若要实现真正的自主性，就必须具备持续适应和学习的能力。然而，对于当前的人工智能系统而言，这种终身学习的能力仍然是一个巨大的挑战。传统的神经网络在按顺序学习一系列任务时，往往会遭遇“[灾难性遗忘](@entry_id:636297)”——新知识的获取会灾难性地破坏或覆盖已经学到的旧知识。这一根本性障碍限制了人工智能在动态、开放式环境中的应用。

本文旨在系统性地剖析并应对这一挑战。我们将带领读者深入[持续学习](@entry_id:634283)的世界，从其核心困境出发，探索前沿的解决方案及其深刻的理论根基。这趟旅程将分为三个章节：
- 在 **“原理与机制”** 中，我们将首先精确定义[灾难性遗忘](@entry_id:636297)与[稳定性-可塑性困境](@entry_id:1132257)，并介绍量化其影响的评估指标。随后，我们将深入探讨三大主流解决方法：基于正则化的方法（如EWC）、基于重放的方法以及基于架构的方法，并触及一个更深层次的现象——表征漂移。
- 在 **“应用与跨学科连接”** 中，我们将视野拓宽，探索这些算法思想如何在神经科学中找到生物学基础，又如何被应用于神经形态硬件、[医疗AI](@entry_id:920780)和关键基础设施等领域，展示持续学习作为一项赋能技术的普适性。
- 最后，在 **“动手实践”** 部分，你将通过一系列精心设计的编程练习，将理论知识转化为实践技能，从分析简单的[线性模型](@entry_id:178302)到构建一个能够持续学习的[脉冲神经网络](@entry_id:1132168)。

通过这三个层次的递进学习，你将构建起对持续学习领域的全面而深入的理解，掌握从理论到实践的关键知识。让我们从探究持续学习的核心原理与机制开始。

## 原理与机制

在对[持续学习](@entry_id:634283)的挑战有了初步了解后，本章将深入探讨其核心原理和为应对这些挑战而设计的关键机制。我们将从[灾难性遗忘](@entry_id:636297)的正式定义及其与稳定性和可塑性之间内在的权衡开始。随后，我们将介绍评估持续学习系统性能的[标准化](@entry_id:637219)指标。在此基础上，我们将探索三种主流的解决方案：基于正则化的方法、基于重放的方法和基于架构的方法。最后，我们将讨论一个更深层次的现象——表征漂移，它挑战了我们对“遗忘”的简单理解。

### 核心挑战：稳定性-可塑性与[灾难性遗忘](@entry_id:636297)

任何旨在实现终身学习的智能系统都必须解决一个根本性的困境：**[稳定性-可塑性困境](@entry_id:1132257) (stability-plasticity dilemma)**。系统必须具有足够的**可塑性 (plasticity)** 以便从新经验中快速学习新知识，同时又必须具有足够的**稳定性 (stability)** 以防止新知识灾难性地破坏或覆盖旧的记忆。在[人工神经网络](@entry_id:140571)中，这种破坏性的遗忘被称为**[灾难性遗忘](@entry_id:636297) (catastrophic forgetting)**。

为了精确地定义这一现象，我们考虑一个按顺序学习一系列任务的[参数模型](@entry_id:170911) $f_{\theta}$。假设模型首先在任务1上训练，其数据分布为 $\mathcal{D}_{\text{old}}$，得到的参数为 $\theta_{\text{old}}$。随后，模型在任务2上训练，其数据分布为 $\mathcal{D}_{\text{new}}$，参数更新为 $\theta_{\text{new}}$。模型的性能通过一个[损失函数](@entry_id:634569) $\ell$ 在某个数据分布 $\mathcal{D}$ 上的[期望风险](@entry_id:634700) $L(\theta; \mathcal{D}) = \mathbb{E}_{(x,y)\sim \mathcal{D}}[\ell(f_{\theta}(x),y)]$ 来衡量。

在这些定义下，[灾难性遗忘](@entry_id:636297)是指在学习任务2之后，模型在任务1上的性能下降。我们可以将其量化为在旧任务数据分布 $\mathcal{D}_{\text{old}}$ 上期望损失的增加量 $\Delta \mathcal{L}$：
$$
\Delta \mathcal{L} \equiv \mathbb{E}_{(x,y)\sim \mathcal{D}_{\text{old}}}\big[\ell(f_{\theta_{\text{new}}}(x),y)-\ell(f_{\theta_{\text{old}}}(x),y)\big]
$$
如果 $\Delta \mathcal{L}$ 显著为正，就意味着发生了[灾难性遗忘](@entry_id:636297) 。值得注意的是，这个定义假设了旧任务的数据分布 $\mathcal{D}_{\text{old}}$ 是固定的。如果 $\mathcal{D}_{\text{old}}$ 自身随时间变化（例如，从 $\mathcal{D}_{\text{old}}$ 变为 $\mathcal{D}'_{\text{old}}$），这种现象被称为**概念漂移 (concept drift)**，这是一个与[灾难性遗忘](@entry_id:636297)相关但截然不同的挑战。此外，遗忘也不同于**容量饱和 (capacity saturation)**，后者指模型由于其有限的表征能力，即使在最优情况下也无法同时为所有任务找到低风险的解决方案 。

[稳定性-可塑性困境](@entry_id:1132257)可以被更形式化地理解为一种类似偏见-方差的权衡。考虑一个小的参数更新 $\Delta \boldsymbol{\theta}$，它旨在最小化新任务（任务1）的损失，同时最小化对旧任务（任务0）性能的破坏。我们可以将旧任务的性能损失（稳定性成本）和新任务的性能提升（可塑性增益）通过围绕旧任务最优参数 $\boldsymbol{\theta}_0$ 的二阶泰勒展开来近似。

假设在 $\boldsymbol{\theta}_0$ 处，旧任务损失的梯度为零。那么，对于小的更新 $\Delta \boldsymbol{\theta}$，旧任务损失的增加（稳定性成本 $S$）近似为 $\Delta L_0 \approx \frac{1}{2} \Delta \boldsymbol{\theta}^{\top} \mathbf{A}_0 \Delta \boldsymbol{\theta}$，而新任务损失的减少（可塑性增益 $I$）则依赖于新任务的梯度 $\mathbf{g}_1$ 和曲率 $\mathbf{A}_1$。其中 $\mathbf{A}_k$ 是任务 $k$ 的高斯-牛顿敏感性矩阵，衡量了模型输出对参数变化的敏感度。一个典型的[持续学习](@entry_id:634283)目标是最小化一个惩罚项，该惩罚项权衡了新任务的学习和旧任务的保持，形式如下：
$$
J(\Delta \boldsymbol{\theta}; \lambda) = \mathbf{g}_{1}^{\top}\Delta \boldsymbol{\theta} + \frac{1}{2} \Delta \boldsymbol{\theta}^{\top}(\mathbf{A}_{1} + \lambda \mathbf{A}_{0})\Delta \boldsymbol{\theta}
$$
其中 $\lambda > 0$ 是一个可调的稳定性权重。求解此目标可以得到最优的参数更新 $\Delta \boldsymbol{\theta}^{\star}(\lambda)$。通过分析稳定性成本 $S(\lambda)$ 和可塑性增益 $I(\lambda)$ 如何随 $\lambda$ 变化，可以揭示一个深刻的权衡关系。增加 $\lambda$ 会减少参数更新的幅度，从而降低对旧任务的破坏（增加稳定性），但同时也会限制在新任务上的学习能力（降低可塑性）。一个理论分析可以表明，在某个[效用函数](@entry_id:137807) $U(\lambda) = I(\lambda) - \mu S(\lambda)$（其中 $\mu$ 是一个固定的偏好权重）下，最优的稳定性权重 $\lambda$ 直接等于 $\mu$ 。这为在稳定性和可塑性之间进行原则性权衡提供了坚实的理论基础。

### 量化持续学习的性能

为了系统地研究和比较不同的[持续学习](@entry_id:634283)算法，我们需要一套[标准化](@entry_id:637219)的评估指标。一个通用的评估协议是，在一个包含 $T$ 个任务的序列上训练模型。每当完成任务 $i$ 的训练后，模型就在所有已见任务 $j \in \{1, \dots, i\}$ 甚至所有任务 $j \in \{1, \dots, T\}$ 的测试集上进行评估。这将产生一个[精度矩阵](@entry_id:264481) $R \in [0,1]^{T \times T}$，其中元素 $R_{i,j}$ 表示在完成任务 $i$ 的训练后，模型在任务 $j$ 上的分类准确率。

基于这个[精度矩阵](@entry_id:264481)，我们可以定义几个关键的性能指标 ：

1.  **平均准确率 (Average Accuracy, ACC)**：在整个任务序列训练完成（即在任务 $T$ 之后）时，模型在所有任务上的平均性能。它衡量了学习系统在学习了所有任务后的最终整体表现。
    $$
    \mathrm{ACC} = \frac{1}{T} \sum_{j=1}^{T} R_{T,j}
    $$

2.  **后向迁移 (Backward Transfer, BWT)**：衡量学习新任务对旧任务性能的平均影响。对于每个任务 $j  T$，它在刚学完时的准确率是 $R_{j,j}$，在整个序列结束后变为 $R_{T,j}$。BWT是这些性能变化的平均值。
    $$
    \mathrm{BWT} = \frac{1}{T-1} \sum_{j=1}^{T-1} (R_{T,j} - R_{j,j})
    $$
    一个正的 BWT 表明学习新任务平均上能帮助提升旧任务的性能（知识协同或正向迁移），而负的 BWT 则直接量化了[灾难性遗忘](@entry_id:636297)的程度。

3.  **遗忘度 (Forgetting, F)**：衡量每个旧任务从其历史最高性能下降的平均程度。对于任务 $j$，其在整个学习过程中达到的最高准确率为 $R_j^* = \max_{k \in \{j, \dots, T\}} R_{k,j}$。遗忘度是最终准确率 $R_{T,j}$ 与这个峰值准确率之差的平均值。
    $$
    \mathrm{F} = \frac{1}{T-1} \sum_{j=1}^{T-1} (R_j^* - R_{T,j})
    $$
    在没有排练机制的典型场景中，旧任务的性能通常单调下降，此时 $R_j^* = R_{j,j}$。在更一般的情况下，这些指标之间存在精确的分析关系。例如，遗忘度 $\mathrm{F}$ 可以表示为后向迁移 $\mathrm{BWT}$ 和任务峰值性能的函数 。这些指标共同提供了一个多维度的视角来评估和理解[持续学习](@entry_id:634283)算法的行为。

### 一个原则性基础：贝叶斯视角

解决[稳定性-可塑性困境](@entry_id:1132257)的一个根本性方法源于[贝叶斯推理](@entry_id:165613)。在一个理想的贝叶斯[在线学习](@entry_id:637955)框架中，知识的整合是自然而无缝的。当一系列数据 $\mathcal{D}_1, \mathcal{D}_2, \dots, \mathcal{D}_t, \dots$ 依次到来时，模型参数 $\theta$ 的后验分布可以被递归地更新。

根据[贝叶斯定理](@entry_id:897366)，在观察到前 $t$ 个任务的数据 $\mathcal{D}_{1:t}$ 后，参数的[后验分布](@entry_id:145605)为 $p(\theta | \mathcal{D}_{1:t})$。这个更新过程可以被写成一个优美的递归形式：
$$
p(\theta | \mathcal{D}_{1:t}) \propto p(\mathcal{D}_t | \theta) p(\theta | \mathcal{D}_{1:t-1})
$$
这个公式的含义是，在 $t-1$ 时刻的**后验分布 $p(\theta | \mathcal{D}_{1:t-1})$**，自然地成为了在 $t$ 时刻更新时的**[先验分布](@entry_id:141376)** 。新数据 $\mathcal{D}_t$ 的[似然](@entry_id:167119) $p(\mathcal{D}_t | \theta)$ 与这个先验相结合，就得到了新的[后验分布](@entry_id:145605)。这个过程完美地平衡了稳定性和可塑性：先验 $p(\theta | \mathcal{D}_{1:t-1})$ 封装了从过去任务中学到的一切知识（稳定性），而[似然](@entry_id:167119)项 $p(\mathcal{D}_t | \theta)$ 则驱动模型适应新任务（可塑性）。

然而，这个理想框架在实践中面临巨大挑战。除了少数特殊情况（例如，当[似然函数](@entry_id:921601)属于[指数族](@entry_id:263444)且先验是其[共轭先验](@entry_id:262304)时），[后验分布](@entry_id:145605) $p(\theta | \mathcal{D}_{1:t-1})$ 通常没有解析形式，并且随着任务的增加会变得越来越复杂，导致计算和存储都变得不可行。

因此，大多数实用的贝叶斯[持续学习](@entry_id:634283)方法都依赖于[近似推断](@entry_id:746496)。常见的近似策略包括 ：
- **假定密度滤波 (Assumed Density Filtering, ADF)**：在每次更新后，将复杂真实的后验投影到一个更简单的[参数化](@entry_id:265163)分布族（如高斯分布）中，通过[矩匹配](@entry_id:144382)等方式实现。
- **期望传播 (Expectation Propagation, EP)**：通过迭代地优化局部近似来逼近全局后验。
- **[拉普拉斯近似](@entry_id:636859) (Laplace's Method)**：将[后验分布近似](@entry_id:753632)为一个以[最大后验概率](@entry_id:268939)（MAP）估计为中心的高斯分布，其协方差由在 MAP 点处[损失函数](@entry_id:634569)的负[海森矩阵](@entry_id:139140)的逆给出。

[拉普拉斯近似](@entry_id:636859)尤其重要，因为它为一类强大的[正则化方法](@entry_id:150559)提供了理论基础。

### [正则化方法](@entry_id:150559)：惩罚重要的变化

[正则化方法](@entry_id:150559)通过在学习新任务时向损失函数添加一个惩罚项来缓解[灾难性遗忘](@entry_id:636297)。这个惩罚项旨在限制对先前任务至关重要的参数发生改变。**弹性权重巩固 (Elastic Weight Consolidation, EWC)** 是这类方法中最具代表性的一个。

EWC 的核心思想源于上述的贝叶斯[在线学习](@entry_id:637955)和[拉普拉斯近似](@entry_id:636859)。它将前一个任务的[后验分布](@entry_id:145605) $p(\theta | \mathcal{D}_{\text{prev}})$ 近似为一个高斯分布，其中心是为该任务找到的最优参数 $\theta^*$，其[精度矩阵](@entry_id:264481)（[协方差矩阵](@entry_id:139155)的逆）由**费雪信息矩阵 (Fisher Information Matrix, FIM)** $\mathbf{F}$ 给出。在新任务的学习中，这个近似的后验就作为先验。在[最大后验概率](@entry_id:268939)（MAP）估计的框架下，这等价于最小化一个新的总损失函数 $\mathcal{L}(\boldsymbol{\theta})$，它由新任务的损失 $\mathcal{L}_{\text{new}}(\boldsymbol{\theta})$ 和一个二次正则化项组成 ：
$$
\mathcal{L}(\boldsymbol{\theta}) = \mathcal{L}_{\text{new}}(\boldsymbol{\theta}) + \sum_{i} \frac{\lambda}{2} F_i (\theta_i - \theta_i^{*})^2
$$
这里，$\lambda$ 是一个超参数，用于平衡新旧任务的重要性，$F_i$ 是[费雪信息矩阵](@entry_id:750640)的对角线元素。这个惩罚项对参数 $\theta_i$ 的改动施加了一个“弹性约束”，约束的“[弹力](@entry_id:175665)系数”就是 $F_i$。

那么，为什么[费雪信息矩阵](@entry_id:750640)能够衡量参数的“重要性”呢？有两个互补的视角 ：

1.  **[损失景观](@entry_id:635571)的曲率**：在标准的[正则性条件](@entry_id:166962)下，费雪信息矩阵等于负[对数似然函数](@entry_id:168593)的[海森矩阵](@entry_id:139140)的[期望值](@entry_id:150961)。因此，$F_i$ 衡量了[损失函数](@entry_id:634569)在参数 $\theta_i$ 方向上的曲率。一个大的 $F_i$ 意味着这个方向非常“陡峭”，对 $\theta_i$ 的微小改动都会导致损失的急剧增加。EWC 通过重罚这些方向上的改动来保护模型在旧任务上的性能。

2.  **参数估计的精度**：根据**[克拉默-拉奥下界](@entry_id:154412) (Cramér-Rao Bound)**，[费雪信息矩阵](@entry_id:750640)的[逆矩阵](@entry_id:140380)为任何[无偏估计量](@entry_id:756290)的协方差提供了一个下界。这意味着，一个大的 $F_i$ 对应于参数 $\theta_i$ 估计方差的下界较小，表明该参数被旧任务的数据“精确地确定”了。因此，这些被精确估计的参数对旧任务至关重要，应当被稳定地保持。

为了使这个概念更具体，考虑一个用泊松[广义线性模型 (GLM)](@entry_id:893670) 建模的脉冲神经元，其放电率 $r(t; \boldsymbol{\theta}) = \exp(\boldsymbol{\theta}^{\top}\mathbf{x}(t))$。在这种情况下，[费雪信息矩阵](@entry_id:750640)的对角元素可以被计算为 $F_i = \mathbb{E}[r(t; \boldsymbol{\theta}^{*}) x_i(t)^2]$ 。这个公式清晰地表明，一个参数的重要性取决于它所连接的输入特征 $x_i(t)$ 的大小以及神经元本身的激活水平 $r(t)$。

### 重放方法：重温过去

另一大类缓解[灾难性遗忘](@entry_id:636297)的方法受到生物学中记忆巩固过程的启发，即通过“重放”(replay) 过去的经验来加深和稳定记忆。在算法上，这对应于在学习新任务的同时，交错地使用存储的旧任务样本进行训练。

**[互补学习系统](@entry_id:926487) (Complementary Learning Systems, CLS)** 理论为这类方法提供了强大的神经科学基础。该理论提出，大脑的学习分为两个互补的系统 ：
- **海马体 (Hippocampus)**：一个快速学习系统，能够一次性地编码和存储具体的、独特的事件（情节记忆）。
- **新皮层 (Neocortex)**：一个慢速学习系统，通过对[海马体](@entry_id:152369)中存储的经验进行反复、交错的重放，逐渐提取出统计规律和抽象知识，并将其整合到长时记忆中。

这个理论可以直接映射到重放式[持续学习](@entry_id:634283)算法：
- **算法中的“[海马体](@entry_id:152369)”**：可以是一个**情节记忆缓冲区 (episodic buffer)**，用于快速存储一小部分来自过去任务的真实数据样本。
- **算法中的“新皮层”**：对应于主要的[参数模型](@entry_id:170911) $f_{\theta}$（例如一个[深度神经网络](@entry_id:636170)），它以较小的[学习率](@entry_id:140210)进行更新，学习的输入是当前任务的新数据和从缓冲区中重放的旧数据的混合。

通过以概率 $\alpha$ 从缓冲区中采样，以概率 $1-\alpha$ 从当前任务数据流中采样，该算法的[随机梯度下降](@entry_id:139134)（SGD）更新在期望上会朝着一个混合[风险函数](@entry_id:166593) $R_{\text{mix}}(\theta) = \alpha R_{\text{text{old}}}(\theta) + (1-\alpha) R_{\text{new}}(\theta)$ 的负梯度方向移动。在适当的条件下（如损失函数是凸的，学习率足够小），这个过程近似于在所有任务的并集上进行联合训练，从而有效地缓解了遗忘 。

除了存储真实样本的**情节重放 (episodic replay)**，另一种强大的变体是**生成式重放 (generative replay)**。该方法不存储真实数据，而是训练一个生成模型（如 VAE 或 GAN），让它学习过去任务的数据分布。在学习新任务时，这个生成模型可以合成“伪样本”用于重放。这种方法的好处是内存开销是固定的，与任务数量无关。

生成式重放的有效性也可以被严格地分析。假设生成器为每个旧任务 $t$ 生成的近似分布 $\widehat{P}_t$ 与真实分布 $P_t$ 之间的差距可以用 **Wasserstein-1 距离** $W_1(P_t, \widehat{P}_t) \leq \varepsilon_t$ 来衡量，并且任务[损失函数](@entry_id:634569)关于输入是 $L$-Lipschitz 连续的。那么，使用生成式重放的联合风险与理想的联合风险之间的绝对差异 $\Delta$ 有一个紧凑的[上界](@entry_id:274738) ：
$$
\Delta(f_{\theta}) \le L \sum_{t=1}^{T} \varepsilon_t
$$
这个界表明，生成式重放的性能直接取决于生成器的质量（由 $\varepsilon_t$ 衡量）。只要能训练出高保真度的生成器，生成式重放就能有效地逼近理想的联合训练，从而防止遗忘。

### 架构方法：隔离参数

第三类方法采取了一种更直接的策略来避免任务间的干扰：通过修改[网络架构](@entry_id:268981)来为不同任务分配专属的参数资源，从而在物理上隔离它们。

这个核心思想可以被形式化地描述为将模型的总参数 $\boldsymbol{\theta}$ 分解为共享参数 $\boldsymbol{\theta}^{\text{shared}}$ 和一系列任务专属参数 $\boldsymbol{\theta}^{(t)}$ 的直接和：$\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{shared}} \oplus \bigoplus_{t=1}^T \boldsymbol{\theta}^{(t)}$。这里的关键是，每个 $\boldsymbol{\theta}^{(t)}$ 的支持（即非零参数的索引集）是互不相交的。当学习任务 $t$ 时，梯度更新被严格限制在 $\boldsymbol{\theta}^{(t)}$（以及可能的 $\boldsymbol{\theta}^{\text{shared}}$）上，而所有其他任务的专属参数 $\boldsymbol{\theta}^{(s)}$ ($s \neq t$) 则保持不变。这可以通过一个**掩码梯度更新**来实现 ：
$$
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \eta \mathbf{P}^{(t)} \nabla_{\boldsymbol{\theta}} \mathcal{L}^{(t)}(\boldsymbol{\theta})
$$
其中 $\mathbf{P}^{(t)}$ 是一个[投影算子](@entry_id:154142)，它将[梯度向量](@entry_id:141180)投影到任务 $t$ 的专属参数子空间上。由于不同任务的子空间是正交的（$\mathbf{P}^{(s)}\mathbf{P}^{(t)}=0$ for $s \neq t$），对任务 $t$ 的更新保证不会影响到任务 $s$ 的参数。

两种著名的算法体现了这一思想：

1.  **渐进式网络 (Progressive Networks, ProgNets)**：当一个新任务到来时，ProgNets 在现有网络旁边“生长”出一个新的网络“列”。所有旧列的参数都被冻结，只有新列的参数被训练。新列可以通过横向连接从旧列的隐藏层中获取信息，实现知识迁移，但反向的参数更新被完全阻断。这种架构的动态扩展直接物理地实现了参数的隔离 。

2.  **PackNet**：该方法在一个固定大小的网络中实现参数隔离。对于第一个任务，它正常训练网络，然后通过**剪枝 (pruning)** 移除不重要的权重（例如，幅度小的权重）。剩下的重要权重被“打包”并冻结，成为任务1的专属参数。对于第二个任务，只在那些被剪掉的“自由”权重上进行训练。训练结束后，再次剪枝并冻结任务2的重要权重。这个过程不断重复。通过这种方式，PackNet 为每个任务分配了一组不相交的权重集合（通过二[进制](@entry_id:634389)掩码定义），从而在不增加网络规模的情况下防止了干扰 。

### 一个更深层次的现象：表征漂移

虽然[灾难性遗忘](@entry_id:636297)是[持续学习](@entry_id:634283)中的一个主要障碍，但生物大脑的记忆似乎远比“稳定不变”要复杂。神经科学研究表明，即使一个记忆被稳定地保持，其底层的神经表征也可能随着时间的推移而缓慢变化。这种现象被称为**表征漂移 (representational drift)**。

表征漂移的定义是：编码任务相关信息的[神经表征](@entry_id:1128614)发生了缓慢的再[参数化](@entry_id:265163)，但任务本身所需的信息被保留了下来。这与[灾难性遗忘](@entry_id:636297)形成了鲜明对比，后者意味着信息的丧失 。

我们可以通过一个简化的模型来理解这一概念。假设一个[分类任务](@entry_id:635433)的类别信息 $Y$ 被一个[特征向量](@entry_id:151813) $R_t$ 编码。在 $t_0$ 时刻，特征的类[条件分布](@entry_id:138367)为 $R_{t_0} | Y=y \sim \mathcal{N}(\mu_y^{(t_0)}, \Sigma^{(t_0)})$。表征漂移可以被建模为一个可逆的[线性变换](@entry_id:149133) $A$，使得在 $t_1$ 时刻的特征 $R_{t_1} \approx A R_{t_0}$。经过这个变换后，新的类[条件分布](@entry_id:138367)变为 $\mathcal{N}(A\mu_y^{(t_0)}, A\Sigma^{(t_0)}A^T)$。

一个关键的数学结果是，在高斯类[条件分布](@entry_id:138367)和共享协方差的情况下，最优分类器的错误率仅取决于类均值之间的**[马氏距离](@entry_id:269828) (Mahalanobis distance)**。而这个[马氏距离](@entry_id:269828)在[可逆线性变换](@entry_id:149915)下是不变的。这意味着，尽管[特征向量](@entry_id:151813) $R_t$ 的具体数值和统计特性（如均值和协方差）已经改变，但理论上可达到的最优分类性能在 $t_0$ 和 $t_1$ 时刻是完全相同的。同样，作为信息论度量，[互信息](@entry_id:138718) $I(Y; R_t)$ 在这种可逆变换下也是不变的 。

表征漂移的实验标志是：
- 一个在 $t_0$ 时刻训练好的**固定解码器**（例如一个[线性分类器](@entry_id:637554)），在 $t_1$ 时刻的性能会显著下降，因为特征的“语言”已经改变。
- 然而，如果在 $t_1$ 时刻**重新训练一个解码器**，它能够恢复到与 $t_0$ 时刻相近的高性能水平，因为解决任务所需的信息仍然存在于新的表征中。

这个现象挑战了将记忆视为静态存储的观点，并暗示了大脑中记忆的动态和自适应本质。对于设计更鲁棒和灵活的神经[形态学](@entry_id:273085)习系统而言，理解并可能利用表征漂移，是一个前沿且富有启发性的研究方向。