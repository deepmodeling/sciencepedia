{
    "hands_on_practices": [
        {
            "introduction": "为了有效解决一个问题，我们首先必须能够准确地理解和量化它。这个练习利用一个简化的线性回归模型，让你通过解析推导来揭示序贯学习（“分块”学习）与联合学习（“交错”学习）之间的性能差异。通过这个实践，你将为持续学习中的核心挑战——灾难性遗忘——建立一个坚实的数学基础，并理解为何简单的任务切换会导致先前知识的灾难性损失 。",
            "id": "4041098",
            "problem": "考虑一个神经形态和类脑计算中的持续学习设置，其中一个单一的突触参数将一对任务（2个任务）建模为具有共享输入的线性回归。设输入 $x$ 从均值为零、方差为 $\\sigma_{x}^{2}$ 的高斯分布中独立同分布地抽取，即 $x \\sim \\mathcal{N}(0,\\sigma_{x}^{2})$。任务 $t \\in \\{1,2\\}$ 由一个生成线性模型 $y_{t} = w_{t} x + \\epsilon_{t}$ 决定，其中特定于任务的真实参数 $w_{t} \\in \\mathbb{R}$ 是固定的，噪声是独立于 $x$ 和 $t' \\neq t$ 时的 $\\epsilon_{t'}$ 的零均值高斯分布，方差为 $\\sigma_{\\epsilon}^{2}$，即 $\\epsilon_{t} \\sim \\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$。学习器是一个单参数线性回归器 $f(x; w) = w x$，使用平方误差损失和无限数据进行训练，学习率足够小，以至于训练收敛到期望损失的最小化器。\n\n将分块学习方案定义为在任务1上训练至收敛，然后在任务2上训练至收敛（从而将参数覆盖为任务2的最小化器）。将交错学习方案定义为在一个以相等概率呈现任务1和任务2样本的混合任务上训练至收敛（从而最小化等权重平均期望损失）。\n\n为衡量持续学习下的性能，将参数 $w$ 在任务 $t$ 上的每个任务均方误差(MSE)定义为 $\\mathrm{MSE}_{t}(w) = \\mathbb{E}\\!\\left[(w x - y_{t})^{2}\\right]$，并将回归的归一化准确率定义为 $a_{t}(w) = 1 - \\frac{\\mathrm{MSE}_{t}(w)}{\\mathrm{MSE}_{t}(0)}$，其中 $\\mathrm{MSE}_{t}(0) = \\mathbb{E}\\!\\left[(0 - y_{t})^{2}\\right]$ 是零预测器的MSE，作为基线。设跨任务的平均准确率为 $A(w) = \\frac{1}{2}\\left(a_{1}(w) + a_{2}(w)\\right)$。\n\n仅从这些定义以及高斯随机变量和最小二乘回归的标准性质出发，推导在分块和交错学习方案下收敛参数的闭式表达式，并计算交错学习相对于分块学习的增益，定义为 $B = A(w_{\\mathrm{I}}) - A(w_{\\mathrm{B}})$，其中 $w_{\\mathrm{B}}$ 和 $w_{\\mathrm{I}}$ 分别表示各自收敛后的参数。将您关于 $B$ 的最终答案表示为仅含 $w_{1}$、$w_{2}$、$\\sigma_{x}^{2}$ 和 $\\sigma_{\\epsilon}^{2}$ 的单一闭式解析表达式。不需要进行数值评估，也不需要四舍五入。",
            "solution": "问题要求在具有两个线性回归任务的持续学习背景下，计算交错训练方案相对于分块训练方案的增益 $B = A(w_{\\mathrm{I}}) - A(w_{\\mathrm{B}})$。为了找到这个值，我们必须首先推导出每种方案下的最终参数 $w_{\\mathrm{I}}$ 和 $w_{\\mathrm{B}}$，然后计算平均准确率 $A(w_{\\mathrm{I}})$ 和 $A(w_{\\mathrm{B}})$。\n\n首先，我们推导每个任务均方误差 $\\mathrm{MSE}_{t}(w)$ 的通用表达式。\n任务 $t$ 的生成模型是 $y_{t} = w_{t} x + \\epsilon_{t}$。学习器的预测是 $w x$。任务 $t$ 的MSE是期望平方误差：\n$$\n\\mathrm{MSE}_{t}(w) = \\mathbb{E}\\!\\left[(w x - y_{t})^{2}\\right] = \\mathbb{E}\\!\\left[(w x - (w_{t} x + \\epsilon_{t}))^{2}\\right] = \\mathbb{E}\\!\\left[((w - w_{t})x - \\epsilon_{t})^{2}\\right]\n$$\n展开平方项并利用期望的线性性质和变量的独立性（$\\mathbb{E}[x\\epsilon_t]=0$），我们得到：\n$$\n\\mathrm{MSE}_{t}(w) = (w - w_{t})^{2}\\mathbb{E}[x^{2}] - 2(w - w_{t})\\mathbb{E}[x\\epsilon_{t}] + \\mathbb{E}[\\epsilon_{t}^{2}]\n$$\n根据已知条件 $\\mathbb{E}[x^{2}]=\\sigma_{x}^{2}$ 和 $\\mathbb{E}[\\epsilon_{t}^{2}]=\\sigma_{\\epsilon}^{2}$，我们有：\n$$\n\\mathrm{MSE}_{t}(w) = (w - w_{t})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}\n$$\n\n接下来，我们确定每种学习方案的收敛参数。\n对于**分块学习方案**，学习器首先在任务1上收敛，然后在任务2上收敛，覆盖了前一个参数。最终的参数是最小化任务2损失的参数，即 $w_2$。因此：\n$$\nw_{\\mathrm{B}} = w_{2}\n$$\n对于**交错学习方案**，学习器最小化两个任务的等权重平均期望损失：\n$$\nL_{\\mathrm{I}}(w) = \\frac{1}{2}\\left(\\mathrm{MSE}_{1}(w) + \\mathrm{MSE}_{2}(w)\\right) = \\frac{1}{2}\\sigma_{x}^{2}\\left((w - w_{1})^{2} + (w - w_{2})^{2}\\right) + \\sigma_{\\epsilon}^{2}\n$$\n为了找到最小化器 $w_{\\mathrm{I}}$，我们将 $L_{\\mathrm{I}}(w)$ 的导数设为零，得到：\n$$\nw_{\\mathrm{I}} = \\frac{w_{1} + w_{2}}{2}\n$$\n\n接下来，我们推导归一化准确率 $a_{t}(w)$ 的表达式。\n$$\na_{t}(w) = 1 - \\frac{\\mathrm{MSE}_{t}(w)}{\\mathrm{MSE}_{t}(0)} = 1 - \\frac{(w - w_{t})^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}{w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}\\left(w_{t}^{2} - (w - w_{t})^{2}\\right)}{w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} = \\frac{\\sigma_{x}^{2}w(2w_{t} - w)}{w_{t}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}\n$$\n\n现在我们计算平均准确率 $A(w_{\\mathrm{B}})$ 和 $A(w_{\\mathrm{I}})$。\n对于**分块学习方案**，$w_{\\mathrm{B}} = w_{2}$，我们有 $A(w_{\\mathrm{B}}) = \\frac{1}{2}\\left(a_{1}(w_{2}) + a_{2}(w_{2})\\right)$，其中：\n$$\na_{1}(w_{2}) = \\frac{\\sigma_{x}^{2}w_{2}(2w_{1} - w_{2})}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}, \\quad a_{2}(w_{2}) = \\frac{\\sigma_{x}^{2}w_{2}^{2}}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}}\n$$\n对于**交错学习方案**，$w_{\\mathrm{I}} = \\frac{w_1+w_2}{2}$，我们有 $A(w_{\\mathrm{I}}) = \\frac{1}{2}\\left(a_{1}(w_{\\mathrm{I}}) + a_{2}(w_{\\mathrm{I}})\\right)$，其中：\n$$\na_{1}(w_{\\mathrm{I}}) = \\frac{\\sigma_{x}^{2}(3w_{1}^{2} + 2w_{1}w_{2} - w_{2}^{2})}{4(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}, \\quad a_{2}(w_{\\mathrm{I}}) = \\frac{\\sigma_{x}^{2}(3w_{2}^{2} + 2w_{1}w_{2} - w_{1}^{2})}{4(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$\n\n最后，我们计算增益 $B = A(w_{\\mathrm{I}}) - A(w_{\\mathrm{B}}) = \\frac{1}{2}\\left[ (a_{1}(w_{\\mathrm{I}}) - a_{1}(w_{\\mathrm{B}})) + (a_{2}(w_{\\mathrm{I}}) - a_{2}(w_{\\mathrm{B}})) \\right]$。\n对于任务1，准确率差异为：\n$$\na_{1}(w_{\\mathrm{I}}) - a_{1}(w_{B}) = \\frac{\\sigma_{x}^{2}}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} \\left[ \\frac{3w_{1}^{2} + 2w_{1}w_{2} - w_{2}^{2}}{4} - w_{2}(2w_{1} - w_{2}) \\right] = \\frac{3\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{4(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$\n对于任务2，准确率差异为：\n$$\na_{2}(w_{\\mathrm{I}}) - a_{2}(w_{B}) = \\frac{\\sigma_{x}^{2}}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} \\left[ \\frac{3w_{2}^{2} + 2w_{1}w_{2} - w_{1}^{2}}{4} - w_{2}^{2} \\right] = \\frac{-\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{4(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$\n将这些差异代入 $B$ 的表达式并化简，我们得到：\n$$\nB = \\frac{1}{2} \\left[ \\frac{3\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{4(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} - \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{4(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} \\right] = \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2}}{8} \\left[ \\frac{3}{w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} - \\frac{1}{w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}} \\right]\n$$\n将括号内的项通分，得到最终答案：\n$$\nB = \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2} \\left( 3(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}) - (w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2}) \\right)}{8(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} = \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2} \\left( (3w_{2}^{2} - w_{1}^{2})\\sigma_{x}^{2} + 2\\sigma_{\\epsilon}^{2} \\right)}{8(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})}\n$$",
            "answer": "$$ \\boxed{ \\frac{\\sigma_{x}^{2}(w_{1} - w_{2})^{2} \\left( (3w_{2}^{2} - w_{1}^{2})\\sigma_{x}^{2} + 2\\sigma_{\\epsilon}^{2} \\right)}{8(w_{1}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})(w_{2}^{2}\\sigma_{x}^{2} + \\sigma_{\\epsilon}^{2})} } $$"
        },
        {
            "introduction": "在量化了灾难性遗忘问题之后，我们来探索一种受神经科学启发的解决方案。弹性权重巩固（Elastic Weight Consolidation, EWC）是一种通过正则化来保护重要突触（参数）的经典方法。在此练习中，你将扮演算法设计者的角色，通过推导来确定最优的权衡参数 $\\lambda$，从而在学习新任务和保留旧知识之间取得最佳平衡 。这个过程将让你直接运用作为EWC核心的费雪信息矩阵（Fisher Information Matrix），加深对正则化方法的理解。",
            "id": "4041100",
            "problem": "考虑一个双参数神经形态模型，其参数矢量为 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$，在两个任务上进行顺序训练。假设通过弹性权重巩固（Elastic Weight Consolidation, EWC）来实施突触巩固，其中惩罚项由权衡参数 $\\lambda \\geq 0$ 进行缩放。在每个任务的最优值附近，通过一个二次型来近似期望负对数似然，该二次型的曲率由经验费雪信息矩阵给出。具体而言，对于任务 $i \\in \\{1,2\\}$，其最优值为 $\\boldsymbol{\\theta}_{i}$，费雪信息矩阵为 $\\mathbf{F}_{i}$，定义局部二次损失为\n$$\nL_{i}(\\boldsymbol{\\theta}) = \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{i})^{\\top}\\mathbf{F}_{i}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{i}).\n$$\n在任务2的训练过程中，EWC目标函数为\n$$\nJ(\\boldsymbol{\\theta};\\lambda) = \\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{2})^{\\top}\\mathbf{F}_{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{2}) + \\frac{\\lambda}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{1})^{\\top}\\mathbf{F}_{1}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{1}),\n$$\n由此得到巩固后的参数 $\\boldsymbol{\\theta}^{\\star}(\\lambda) = \\arg\\min_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta};\\lambda)$。假设两个任务的准确度同等重要，并且在二次近似的主导阶上，通过最小化平均二次损失来最大化平均归一化准确度\n$$\nL_{\\mathrm{avg}}(\\lambda) = \\frac{1}{2}L_{1}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) + \\frac{1}{2}L_{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda)).\n$$\n设最优值和费雪矩阵为\n$$\n\\boldsymbol{\\theta}_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad \\boldsymbol{\\theta}_{2} = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}, \\quad \\mathbf{F}_{1} = \\begin{pmatrix}4  0 \\\\ 0  1\\end{pmatrix}, \\quad \\mathbf{F}_{2} = \\begin{pmatrix}1  0 \\\\ 0  9\\end{pmatrix}.\n$$\n在这些假设下，确定能最大化两个任务平均归一化准确度的权衡参数 $\\lambda$ 的值。将您的最终答案表示为一个无量纲数。无需四舍五入；请提供精确值。",
            "solution": "我们的目标是找到最小化平均二次损失 $L_{\\mathrm{avg}}(\\lambda)$ 的权衡参数 $\\lambda$。为此，我们首先需要找到EWC目标函数 $J(\\boldsymbol{\\theta};\\lambda)$ 的最小化器 $\\boldsymbol{\\theta}^{\\star}(\\lambda)$。\n由于 $J(\\boldsymbol{\\theta};\\lambda)$ 是关于 $\\boldsymbol{\\theta}$ 的二次函数，我们可以通过将其梯度设为零来找到最小值：\n$$ \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta};\\lambda) = \\mathbf{F}_{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{2}) + \\lambda \\mathbf{F}_{1}(\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{1}) = \\mathbf{0} $$\n整理后可得：\n$$ (\\mathbf{F}_{2} + \\lambda \\mathbf{F}_{1})\\boldsymbol{\\theta} = \\mathbf{F}_{2}\\boldsymbol{\\theta}_{2} + \\lambda \\mathbf{F}_{1}\\boldsymbol{\\theta}_{1} $$\n因此，最优参数为：\n$$ \\boldsymbol{\\theta}^{\\star}(\\lambda) = (\\mathbf{F}_{2} + \\lambda \\mathbf{F}_{1})^{-1}(\\mathbf{F}_{2}\\boldsymbol{\\theta}_{2} + \\lambda \\mathbf{F}_{1}\\boldsymbol{\\theta}_{1}) $$\n代入给定的值，由于所有矩阵都是对角矩阵，计算变得简单：\n$$ \\mathbf{F}_{2} + \\lambda \\mathbf{F}_{1} = \\begin{pmatrix}1+4\\lambda & 0 \\\\ 0 & 9+\\lambda\\end{pmatrix} $$\n$$ \\mathbf{F}_{2}\\boldsymbol{\\theta}_{2} + \\lambda \\mathbf{F}_{1}\\boldsymbol{\\theta}_{1} = \\begin{pmatrix}1 & 0 \\\\ 0 & 9\\end{pmatrix}\\begin{pmatrix}0 \\\\ 2\\end{pmatrix} + \\lambda \\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 18\\end{pmatrix} + \\begin{pmatrix}4\\lambda \\\\ 0\\end{pmatrix} = \\begin{pmatrix}4\\lambda \\\\ 18\\end{pmatrix} $$\n于是，我们得到 $\\boldsymbol{\\theta}^{\\star}(\\lambda)$:\n$$ \\boldsymbol{\\theta}^{\\star}(\\lambda) = \\begin{pmatrix}\\frac{1}{1+4\\lambda} & 0 \\\\ 0 & \\frac{1}{9+\\lambda}\\end{pmatrix} \\begin{pmatrix}4\\lambda \\\\ 18\\end{pmatrix} = \\begin{pmatrix} \\frac{4\\lambda}{1+4\\lambda} \\\\ \\frac{18}{9+\\lambda} \\end{pmatrix} $$\n接下来，我们将 $\\boldsymbol{\\theta}^{\\star}(\\lambda)$ 代入 $L_1$ 和 $L_2$ 的表达式中，以计算 $L_{\\mathrm{avg}}(\\lambda)$。\n$$ L_{1}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) = \\frac{1}{2} \\left( \\begin{pmatrix} \\frac{4\\lambda}{1+4\\lambda} - 1 \\\\ \\frac{18}{9+\\lambda} - 0 \\end{pmatrix}^{\\top} \\begin{pmatrix}4 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix} \\frac{-1}{1+4\\lambda} \\\\ \\frac{18}{9+\\lambda} \\end{pmatrix} \\right) = \\frac{1}{2}\\left( \\frac{4}{(1+4\\lambda)^2} + \\frac{324}{(9+\\lambda)^2} \\right) = \\frac{2}{(1+4\\lambda)^2} + \\frac{162}{(9+\\lambda)^2} $$\n$$ L_{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) = \\frac{1}{2} \\left( \\begin{pmatrix} \\frac{4\\lambda}{1+4\\lambda} - 0 \\\\ \\frac{18}{9+\\lambda} - 2 \\end{pmatrix}^{\\top} \\begin{pmatrix}1 & 0 \\\\ 0 & 9\\end{pmatrix} \\begin{pmatrix} \\frac{4\\lambda}{1+4\\lambda} \\\\ \\frac{-2\\lambda}{9+\\lambda} \\end{pmatrix} \\right) = \\frac{1}{2}\\left( \\frac{16\\lambda^2}{(1+4\\lambda)^2} + \\frac{36\\lambda^2 \\cdot 9}{(9+\\lambda)^2} \\right) = \\frac{8\\lambda^2}{(1+4\\lambda)^2} + \\frac{18\\lambda^2}{(9+\\lambda)^2} $$\n平均损失为：\n$$ L_{\\mathrm{avg}}(\\lambda) = \\frac{1}{2}\\left(L_{1}(\\boldsymbol{\\theta}^{\\star}(\\lambda)) + L_{2}(\\boldsymbol{\\theta}^{\\star}(\\lambda))\\right) = \\frac{1+4\\lambda^2}{(1+4\\lambda)^2} + \\frac{81+9\\lambda^2}{(9+\\lambda)^2} $$\n为了最小化 $L_{\\mathrm{avg}}(\\lambda)$，我们对其求导并令其为零：\n$$ \\frac{d L_{\\mathrm{avg}}}{d\\lambda} = \\frac{d}{d\\lambda}\\left(\\frac{1+4\\lambda^2}{(1+4\\lambda)^2}\\right) + \\frac{d}{d\\lambda}\\left(\\frac{81+9\\lambda^2}{(9+\\lambda)^2}\\right) = 0 $$\n分别计算两项的导数：\n$$ \\frac{d}{d\\lambda}\\left(\\frac{1+4\\lambda^2}{(1+4\\lambda)^2}\\right) = \\frac{8(\\lambda-1)}{(1+4\\lambda)^3} $$\n$$ \\frac{d}{d\\lambda}\\left(\\frac{81+9\\lambda^2}{(9+\\lambda)^2}\\right) = \\frac{162(\\lambda-1)}{(9+\\lambda)^3} $$\n将它们相加并设为零：\n$$ \\frac{8(\\lambda-1)}{(1+4\\lambda)^3} + \\frac{162(\\lambda-1)}{(9+\\lambda)^3} = (\\lambda-1) \\left[ \\frac{8}{(1+4\\lambda)^3} + \\frac{162}{(9+\\lambda)^3} \\right] = 0 $$\n对于 $\\lambda \\geq 0$，方括号内的项总是正数。因此，唯一的解是 $\\lambda - 1 = 0$，即 $\\lambda = 1$。\n通过检查二阶导数或分析一阶导数在 $\\lambda=1$ 附近的符号变化，可以确认这是一个最小值。因此，最优的权衡参数是1。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "除了基于正则化的方法，另一大类持续学习算法是基于投影的方法，它们通过将新任务的梯度更新限制在与旧任务知识子空间正交的方向上，从而避免学习过程中的干扰。这个练习将引导你从一个非常实用的角度入手：分析梯度投影算法关键步骤的计算复杂度。通过精确计算每一步所需的浮点运算（flops）次数，你将学会评估算法在神经形态硬件或传统计算机上的实现效率，这是研究生阶段必不可少的一项实践技能 。",
            "id": "4041143",
            "problem": "考虑一个在神经形态和类脑计算中的持续学习设置，其中一个 $\\mathbb{R}^{d}$中的参数向量通过基于随机梯度的规则跨任务更新，同时避免对先前学习任务的干扰。假设先前的 $k$ 个任务已经产生了 $k$ 个线性无关的梯度方向，这些方向作为矩阵 $G \\in \\mathbb{R}^{d \\times k}$ 的列被收集起来，其中 $k \\leq d$。设每次任务切换时，使用 Gram–Schmidt 正交化过程为 $G$ 的列空间获得一个标准正交基 $Q = [q_{1}, \\ldots, q_{k}] \\in \\mathbb{R}^{d \\times k}$。在当前任务的每个训练步骤中，使用 Gram–Schmidt 递推将一个新的梯度 $g \\in \\mathbb{R}^{d}$ 投影到 $G^{\\top}$ 的零空间（即 $G$ 的列所张成的空间的正交补）上\n$$\ng \\leftarrow g - \\sum_{i=1}^{k} \\langle q_{i}, g \\rangle q_{i},\n$$\n因此更新方向为 $g_{\\perp} = g - \\sum_{i=1}^{k} \\langle q_{i}, g \\rangle q_{i}$，它属于 $G^{\\top}$ 的零空间。\n\n采用标准的浮点运算 (flop) 计数模型：每次标量乘法消耗 $1$ flop，每次标量加法或减法消耗 $1$ flop。在此模型下，$\\mathbb{R}^{d}$ 中两个向量的点积消耗 $2d - 1$ flops，与一个 $d \\times r$ 矩阵的矩阵-向量乘积消耗 $d(2r - 1)$ flops，$\\mathbb{R}^{d}$ 中两个向量相减消耗 $d$ flops。\n\n假设 $Q$ 已经被计算并存储，并且 $G$ 具有满列秩 $k$。从第一性原理出发，推导每个训练步骤中使用上述 Gram–Schmidt 迭代计算 $g_{\\perp}$ 所需浮点运算的确切次数，并表示为 $d$ 和 $k$ 的显式封闭形式函数。您的最终答案必须是仅含 $d$ 和 $k$ 的单个解析表达式。不要使用渐近符号进行简化。无需四舍五入。",
            "solution": "我们的目标是计算在每个训练步骤中，将新梯度 $g$ 投影到由标准正交基 $Q = [q_1, \\dots, q_k]$ 所张成空间的正交补上所需的浮点运算（flops）总数。问题中描述的投影过程 $g_{\\perp} = g - \\sum_{i=1}^{k} \\langle q_{i}, g \\rangle q_{i}$ 可以通过一个迭代过程（即改进的格拉姆-施密特法）来高效实现。\n\n设初始向量为 $g^{(0)} = g$。我们通过 $k$ 次迭代来依次减去 $g$ 在每个基向量 $q_i$ 上的分量。第 $i$ 次迭代（$i$ 从 1 到 $k$）的计算如下：\n$$ g^{(i)} = g^{(i-1)} - \\langle q_{i}, g^{(i-1)} \\rangle q_{i} $$\n经过 $k$ 次迭代后，得到的向量 $g^{(k)}$ 就是我们所求的 $g_{\\perp}$。\n\n现在，我们根据给定的 flop 计数模型来分析单次迭代的计算成本：\n\n1.  **计算内积**：计算标量系数 $c_i = \\langle q_i, g^{(i-1)} \\rangle$。这是两个 $d$ 维向量的内积。\n    *   根据定义，这需要 $d$ 次乘法和 $d-1$ 次加法。\n    *   成本 = $d + (d-1) = 2d - 1$ flops。\n\n2.  **计算标量-向量乘法**：计算向量 $v_i = c_i q_i$。这将标量 $c_i$ 与向量 $q_i$ 的 $d$ 个分量相乘。\n    *   成本 = $d$ 次标量乘法 = $d$ flops。\n\n3.  **计算向量减法**：计算更新后的向量 $g^{(i)} = g^{(i-1)} - v_i$。这是两个 $d$ 维向量的减法。\n    *   成本 = $d$ 次标量减法 = $d$ flops。\n\n因此，单次迭代（例如，对于索引 $i$）的总 flop 数是上述三步成本的总和：\n$$ \\text{单次迭代成本} = (2d - 1) + d + d = 4d - 1 $$\n\n由于这个过程需要对 $k$ 个基向量进行迭代，总的 flop 数量就是单次迭代的成本乘以迭代次数 $k$：\n$$ \\text{总 flops} = k \\times (\\text{单次迭代成本}) = k(4d - 1) $$\n\n展开此表达式，我们得到关于 $d$ 和 $k$ 的最终封闭形式函数：\n$$ \\text{总 flops} = 4kd - k $$",
            "answer": "$$\\boxed{4kd - k}$$"
        }
    ]
}