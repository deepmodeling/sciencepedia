{
    "hands_on_practices": [
        {
            "introduction": "Training spiking neural networks (SNNs) with gradient-based methods presents a challenge, as the spiking mechanism is inherently non-differentiable. This exercise introduces the surrogate gradient method, a cornerstone technique that enables backpropagation in SNNs by approximating the derivative of the spike generation function during the backward pass. By deriving the gradient for a single neuron and computing it numerically, you will gain hands-on experience with the core mechanism that underpins learning on neuromorphic hardware in a federated setting .",
            "id": "4045055",
            "problem": "A neuromorphic edge client participating in Federated Learning (FL) trains a single Leaky Integrate-and-Fire (LIF) neuron locally and communicates only weight updates via Federated Averaging (FedAvg). The neuron receives a single presynaptic event over one discrete time step and uses a surrogate gradient to enable gradient-based learning. The neuronâ€™s membrane potential obeys the continuous-time LIF dynamics $$\\tau_{m} \\frac{dV(t)}{dt} = -\\left(V(t) - V_{\\mathrm{rest}}\\right) + R I(t),$$ where $\\tau_{m}$ is the membrane time constant, $V_{\\mathrm{rest}}$ is the resting potential, $R$ is the membrane resistance, and $I(t)$ is the synaptic input current. Using forward Euler with step size $\\Delta t$, the one-step update before any reset is $$V_{1} = \\alpha V_{0} + w x_{1}, \\quad \\text{where} \\quad \\alpha = \\exp\\!\\left(-\\frac{\\Delta t}{\\tau_{m}}\\right),$$ $w$ is the synaptic weight, and $x_{1}$ is the presynaptic input amplitude over the step. The neuron emits a spike if its pre-reset membrane potential crosses threshold, modeled by the Heaviside function $$s_{1} = H\\!\\left(V_{1} - v_{\\mathrm{th}}\\right),$$ where $v_{\\mathrm{th}}$ is the threshold. The local training loss on this client for the single-step sample is $$L = \\frac{1}{2}\\left(s_{1} - y\\right)^{2},$$ with target $y \\in \\{0,1\\}$. To enable differentiability, the Heaviside function is replaced in the backward pass by a logistic surrogate with scale $a  0$, $$\\tilde{H}(u) = \\sigma\\!\\left(\\frac{u}{a}\\right), \\quad \\sigma(z) = \\frac{1}{1 + \\exp(-z)},$$ so that the surrogate derivative used for backpropagation is $$\\rho(u) = \\frac{d}{du}\\tilde{H}(u) = \\frac{1}{a}\\,\\sigma\\!\\left(\\frac{u}{a}\\right)\\left[1 - \\sigma\\!\\left(\\frac{u}{a}\\right)\\right].$$\n\nStarting only from these definitions and the chain rule, derive an analytic expression for the gradient $\\frac{\\partial L}{\\partial w}$ in terms of $x_{1}$, $y$, $v_{\\mathrm{th}}$, $V_{1}$, and $a$. Then, for a single-spike case on this client with parameters $$\\tau_{m} = 20\\,\\mathrm{ms}, \\quad \\Delta t = 1\\,\\mathrm{ms}, \\quad V_{0} = 0, \\quad v_{\\mathrm{th}} = 1.0, \\quad w = 0.8, \\quad x_{1} = 1.5, \\quad y = 0, \\quad a = 0.5,$$ compute the numerical value of $\\frac{\\partial L}{\\partial w}$ using the surrogate gradient and the one-step forward dynamics above. Round your final numerical answer to four significant figures and express it as a dimensionless number.",
            "solution": "The problem asks for two parts: first, to derive an analytic expression for the gradient of the loss function with respect to a synaptic weight, $\\frac{\\partial L}{\\partial w}$, using the surrogate gradient method; and second, to compute the numerical value of this gradient for a specific set of parameters.\n\nThe validation of the problem statement confirms that it is scientifically grounded, well-posed, and contains all necessary information to proceed. The problem describes a standard scenario in training Spiking Neural Networks (SNNs) with surrogate gradients, a valid and established technique in neuromorphic computing.\n\n**Part 1: Analytical Derivation of the Gradient $\\frac{\\partial L}{\\partial w}$**\n\nThe local loss function $L$ is defined as:\n$$L = \\frac{1}{2}(s_1 - y)^2$$\nwhere $s_1$ is the neuron's output spike, $y$ is the target label, $w$ is the synaptic weight, and $x_1$ is the input. The output spike $s_1$ is a function of the membrane potential $V_1$, which in turn is a function of the weight $w$. Specifically:\n$$s_1 = H(V_1 - v_{\\mathrm{th}})$$\n$$V_1 = \\alpha V_0 + w x_1$$\nHere, $H$ is the Heaviside step function, $v_{\\mathrm{th}}$ is the firing threshold, $V_0$ is the initial potential, and $\\alpha$ is the membrane potential decay factor.\n\nWe seek to compute the gradient $\\frac{\\partial L}{\\partial w}$. Using the chain rule for differentiation, we can express this gradient as a product of three partial derivatives:\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial s_1} \\frac{\\partial s_1}{\\partial V_1} \\frac{\\partial V_1}{\\partial w}$$\n\nLet us compute each term individually.\n\n1.  **Derivative of the loss with respect to the output spike, $\\frac{\\partial L}{\\partial s_1}$**:\n    From the definition of $L$, we have:\n    $$\\frac{\\partial L}{\\partial s_1} = \\frac{\\partial}{\\partial s_1} \\left[ \\frac{1}{2}(s_1 - y)^2 \\right] = 2 \\cdot \\frac{1}{2}(s_1 - y) = s_1 - y$$\n\n2.  **Derivative of the membrane potential with respect to the weight, $\\frac{\\partial V_1}{\\partial w}$**:\n    From the one-step update equation for $V_1$, we have:\n    $$\\frac{\\partial V_1}{\\partial w} = \\frac{\\partial}{\\partial w} (\\alpha V_0 + w x_1)$$\n    Since $\\alpha$, $V_0$, and $x_1$ are not functions of $w$, this simplifies to:\n    $$\\frac{\\partial V_1}{\\partial w} = x_1$$\n\n3.  **Derivative of the output spike with respect to the membrane potential, $\\frac{\\partial s_1}{\\partial V_1}$**:\n    The forward-pass output is given by $s_1 = H(V_1 - v_{\\mathrm{th}})$. The derivative of the Heaviside function is the Dirac delta function, which is zero almost everywhere and undefined at the origin. This poses a problem for gradient-based optimization. The surrogate gradient method addresses this by replacing the non-differentiable derivative of $H(u)$ in the backward pass with a continuous, well-behaved surrogate function, $\\rho(u)$. The problem defines this surrogate derivative as:\n    $$\\rho(u) = \\frac{1}{a} \\sigma\\left(\\frac{u}{a}\\right) \\left[1 - \\sigma\\left(\\frac{u}{a}\\right)\\right]$$\n    where $u = V_1 - v_{\\mathrm{th}}$. Therefore, for the purpose of backpropagation, we make the substitution:\n    $$\\frac{\\partial s_1}{\\partial V_1} \\approx \\rho(V_1 - v_{\\mathrm{th}})$$\n\nCombining these three terms, we obtain the expression for the gradient of the loss with respect to the weight:\n$$\\frac{\\partial L}{\\partial w} = (s_1 - y) \\cdot \\rho(V_1 - v_{\\mathrm{th}}) \\cdot x_1$$\nSubstituting the definition of $\\rho(u)$, we get the full analytical expression:\n$$\\frac{\\partial L}{\\partial w} = (s_1 - y) \\left( \\frac{1}{a} \\sigma\\left(\\frac{V_1 - v_{\\mathrm{th}}}{a}\\right) \\left[1 - \\sigma\\left(\\frac{V_1 - v_{\\mathrm{th}}}{a}\\right)\\right] \\right) x_1$$\nHere, $s_1$ is the value computed during the forward pass, i.e., $s_1 = H(V_1 - v_{\\mathrm{th}})$. The expression is in terms of $x_1$, $y$, $v_{\\mathrm{th}}$, $V_1$, and $a$ as required, with the understanding that $s_1$ is determined by $V_1$ and $v_{\\mathrm{th}}$.\n\n**Part 2: Numerical Computation of the Gradient**\n\nWe are given the following parameters:\n$\\tau_{m} = 20\\,\\mathrm{ms}$, $\\Delta t = 1\\,\\mathrm{ms}$, $V_{0} = 0$, $v_{\\mathrm{th}} = 1.0$, $w = 0.8$, $x_{1} = 1.5$, $y = 0$, $a = 0.5$.\n\nThe computation proceeds in steps, following the logic of the forward and backward passes.\n\n1.  **Calculate the membrane potential $V_1$ (Forward Pass)**:\n    First, we determine the decay factor $\\alpha$:\n    $$\\alpha = \\exp\\left(-\\frac{\\Delta t}{\\tau_{m}}\\right) = \\exp\\left(-\\frac{1\\,\\mathrm{ms}}{20\\,\\mathrm{ms}}\\right) = \\exp(-0.05)$$\n    Now, we compute the pre-reset membrane potential $V_1$:\n    $$V_1 = \\alpha V_0 + w x_1 = \\exp(-0.05) \\cdot 0 + (0.8)(1.5) = 1.2$$\n\n2.  **Calculate the output spike $s_1$ (Forward Pass)**:\n    The output spike is determined by comparing $V_1$ to the threshold $v_{\\mathrm{th}}$:\n    $$s_1 = H(V_1 - v_{\\mathrm{th}}) = H(1.2 - 1.0) = H(0.2)$$\n    Since the argument is positive, the Heaviside function evaluates to $1$:\n    $$s_1 = 1$$\n\n3.  **Calculate the gradient components (Backward Pass)**:\n    We now use the derived formula for $\\frac{\\partial L}{\\partial w}$:\n    $$\\frac{\\partial L}{\\partial w} = (s_1 - y) \\cdot \\rho(V_1 - v_{\\mathrm{th}}) \\cdot x_1$$\n    -   The error term is $(s_1 - y) = 1 - 0 = 1$.\n    -   The input term is $x_1 = 1.5$.\n    -   The surrogate gradient term is $\\rho(V_1 - v_{\\mathrm{th}}) = \\rho(0.2)$. Let's compute this.\n        The argument of the sigmoid function is $u/a = (V_1 - v_{\\mathrm{th}})/a = 0.2 / 0.5 = 0.4$.\n        The value of the sigmoid function is:\n        $$\\sigma(0.4) = \\frac{1}{1 + \\exp(-0.4)} \\approx \\frac{1}{1 + 0.670320046} \\approx \\frac{1}{1.670320046} \\approx 0.59868766$$\n        Now we can compute the value of the surrogate derivative $\\rho(0.2)$:\n        $$\\rho(0.2) = \\frac{1}{a} \\sigma(0.4) [1 - \\sigma(0.4)] = \\frac{1}{0.5} (0.59868766) [1 - 0.59868766]$$\n        $$\\rho(0.2) = 2 \\cdot (0.59868766) \\cdot (0.40131234) \\approx 0.4805084$$\n\n4.  **Combine to find the final gradient**:\n    $$\\frac{\\partial L}{\\partial w} = (1) \\cdot (0.4805084) \\cdot (1.5) = 0.7207626$$\n\n5.  **Round to four significant figures**:\n    The computed value is $0.7207626$. Rounding to four significant figures gives $0.7208$.",
            "answer": "$$\n\\boxed{0.7208}\n$$"
        },
        {
            "introduction": "A central question in federated learning is how to combine model updates from multiple clients into a single, coherent global model. This practice explores the foundational principle of Federated Averaging (FedAvg), where the server weights each client's contribution based on the size of its local dataset. Through a formal derivation, you will demonstrate why this weighting scheme is crucial for ensuring that the aggregated gradient provides an unbiased estimate of the centralized gradient, a key property for stable convergence .",
            "id": "4045019",
            "problem": "Consider a synchronous federated learning system composed of $K$ neuromorphic edge clients, each running a spiking neural network trained with surrogate-gradient stochastic gradient descent. Client $k \\in \\{1,\\dots,K\\}$ holds a local dataset $\\mathcal{D}_k$ of size $n_k$, with $\\sum_{k=1}^{K} n_k = N$. Assume the following foundational conditions hold:\n- The data across clients are independent and identically distributed (IID): each $\\mathcal{D}_k$ consists of $n_k$ independent samples drawn from the same distribution over data-label pairs $(\\mathbf{x}, y)$.\n- The per-sample spiking loss $\\ell(w; \\mathbf{x}, y)$ is differentiable in $w$ after surrogate relaxation, and the surrogate gradient estimator $\\widehat{\\nabla}\\ell(w; \\mathbf{x}, y)$ computed on neuromorphic hardware is unbiased in the sense that $\\mathbb{E}[\\widehat{\\nabla}\\ell(w; \\mathbf{x}, y)] = \\nabla \\ell(w; \\mathbf{x}, y)$, where the expectation is with respect to both data sampling and hardware-induced randomness (e.g., quantization and event noise).\n- At a given round, each client computes its local empirical gradient as $g_k(w) = \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\widehat{\\nabla}\\ell(w; z)$, where $z = (\\mathbf{x}, y)$.\n\nThe centralized empirical risk over the union $\\mathcal{D} = \\bigcup_{k=1}^{K} \\mathcal{D}_k$ is $L_N(w) = \\frac{1}{N} \\sum_{z \\in \\mathcal{D}} \\ell(w; z)$, with corresponding centralized empirical gradient $\\nabla L_N(w) = \\frac{1}{N} \\sum_{z \\in \\mathcal{D}} \\nabla \\ell(w; z)$.\n\nA parameter server aggregates the client gradients into a single update direction using weights $p_k \\ge 0$ with $\\sum_{k=1}^{K} p_k = 1$, producing the aggregated gradient $g_{\\mathrm{agg}}(w) = \\sum_{k=1}^{K} p_k \\, g_k(w)$ and the global update $w^{+} = w - \\eta \\, g_{\\mathrm{agg}}(w)$ with a common learning rate $\\eta  0$.\n\nStarting only from the definitions above and the IID and unbiasedness assumptions, determine the weights $p_k$ as a function of the dataset sizes $n_k$ so that the aggregated gradient equals the centralized empirical gradient in expectation, that is, $\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\nabla L_N(w)$. Then verify this equality by explicit derivation.\n\nProvide your final answer as a single closed-form analytic expression for $p_k$ in terms of $\\{n_k\\}_{k=1}^{K}$, with no rounding required and no units. Do not provide any intermediate steps in the final answer.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard theoretical problem in the field of federated learning, specifically concerning the derivation of aggregation weights that ensure the aggregated gradient is an unbiased estimator of the centralized empirical gradient.\n\nThe objective is to find the weighting coefficients $p_k$ for $k \\in \\{1, \\dots, K\\}$ that satisfy the condition $\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\nabla L_N(w)$. The expectation $\\mathbb{E}[\\cdot]$ is taken with respect to the hardware-induced randomness in the surrogate gradient estimators. The datasets $\\{\\mathcal{D}_k\\}_{k=1}^{K}$ are considered fixed for this expectation.\n\nWe begin by expanding the left-hand side (LHS) of the target equality, which is the expected aggregated gradient $\\mathbb{E}[g_{\\mathrm{agg}}(w)]$.\nUsing the definition of the aggregated gradient $g_{\\mathrm{agg}}(w) = \\sum_{k=1}^{K} p_k \\, g_k(w)$ and the linearity of the expectation operator, we have:\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\mathbb{E}\\left[ \\sum_{k=1}^{K} p_k \\, g_k(w) \\right] = \\sum_{k=1}^{K} p_k \\, \\mathbb{E}[g_k(w)]\n$$\nNext, we substitute the definition of the local empirical gradient, $g_k(w) = \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\widehat{\\nabla}\\ell(w; z)$:\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\sum_{k=1}^{K} p_k \\, \\mathbb{E}\\left[ \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\widehat{\\nabla}\\ell(w; z) \\right]\n$$\nBy linearity of expectation, we can move the operator inside the sum over the local dataset $\\mathcal{D}_k$:\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\sum_{k=1}^{K} p_k \\, \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\mathbb{E}[\\widehat{\\nabla}\\ell(w; z)]\n$$\nThe problem states that the surrogate gradient estimator is unbiased for any given data sample $z = (\\mathbf{x}, y)$, such that $\\mathbb{E}[\\widehat{\\nabla}\\ell(w; z)] = \\nabla \\ell(w; z)$. Applying this assumption, we obtain:\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\sum_{k=1}^{K} \\frac{p_k}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z)\n$$\nNow, we analyze the right-hand side (RHS) of the target equality, the centralized empirical gradient $\\nabla L_N(w)$. By its definition, $\\nabla L_N(w) = \\frac{1}{N} \\sum_{z \\in \\mathcal{D}} \\nabla \\ell(w; z)$. The total dataset $\\mathcal{D}$ is the union of the local datasets, $\\mathcal{D} = \\bigcup_{k=1}^{K} \\mathcal{D}_k$. Assuming the local datasets are disjoint, we can rewrite the sum over $\\mathcal{D}$ as a sum over the local datasets:\n$$\n\\nabla L_N(w) = \\frac{1}{N} \\sum_{k=1}^{K} \\sum_{z \\in \\mathcal{D}_k} \\nabla \\ell(w; z)\n$$\nTo satisfy the problem's requirement, we equate the derived expression for the LHS with the RHS:\n$$\n\\sum_{k=1}^{K} \\frac{p_k}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z) = \\frac{1}{N} \\sum_{k=1}^{K} \\sum_{z \\in \\mathcal{D}_k} \\nabla \\ell(w; z)\n$$\nThis equation can be rearranged into a single summation:\n$$\n\\sum_{k=1}^{K} \\left( \\frac{p_k}{n_k} - \\frac{1}{N} \\right) \\left( \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z) \\right) = 0\n$$\nThis equality must hold for any set of local datasets $\\{\\mathcal{D}_k\\}$ and any model parameters $w$. The vectors representing the local gradient sums, $\\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z)$, can be linearly independent. For the total sum to be the zero vector under such general conditions, the scalar coefficient of each vector term must be zero. Therefore, for each client $k \\in \\{1, \\dots, K\\}$, we must have:\n$$\n\\frac{p_k}{n_k} - \\frac{1}{N} = 0\n$$\nSolving for $p_k$ yields:\n$$\np_k = \\frac{n_k}{N}\n$$\nThe problem defines $N = \\sum_{j=1}^{K} n_j$. Hence, the weights are proportional to the size of the local datasets. We verify that these weights satisfy the constraint $\\sum_{k=1}^{K} p_k = 1$:\n$$\n\\sum_{k=1}^{K} p_k = \\sum_{k=1}^{K} \\frac{n_k}{N} = \\frac{1}{N} \\sum_{k=1}^{K} n_k = \\frac{N}{N} = 1\n$$\nThe constraint is satisfied. Since $n_k \\ge 0$ (a dataset size) and $N  0$, the condition $p_k \\ge 0$ is also met. It is noteworthy that the IID data assumption, while given, is not necessary to establish this specific equality between the expected aggregated gradient and the centralized empirical gradient. The IID property is, however, crucial for ensuring that this centralized empirical gradient is itself an unbiased estimate of the true global population gradient.\n\nFinally, we perform the explicit verification required by the problem. We substitute $p_k = n_k/N$ back into the expression for the expected aggregated gradient:\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\sum_{k=1}^{K} p_k \\, \\mathbb{E}[g_k(w)] = \\sum_{k=1}^{K} \\frac{n_k}{N} \\, \\mathbb{E}\\left[ \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\widehat{\\nabla}\\ell(w; z) \\right]\n$$\n$$\n= \\sum_{k=1}^{K} \\frac{n_k}{N} \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\mathbb{E}[\\widehat{\\nabla}\\ell(w; z)] = \\sum_{k=1}^{K} \\frac{1}{N} \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z)\n$$\n$$\n= \\frac{1}{N} \\sum_{k=1}^{K} \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z) = \\frac{1}{N} \\sum_{z \\in \\mathcal{D}} \\nabla\\ell(w; z) = \\nabla L_N(w)\n$$\nThe verification confirms that with the choice $p_k = n_k/N$, the equality $\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\nabla L_N(w)$ holds. The solution is thus $p_k = \\frac{n_k}{\\sum_{j=1}^{K} n_j}$.",
            "answer": "$$\\boxed{\\frac{n_k}{\\sum_{j=1}^{K} n_j}}$$"
        },
        {
            "introduction": "The efficiency of federated learning on neuromorphic hardware hinges on a delicate balance between local on-device computation and costly network communication. This exercise frames this trade-off as a constrained optimization problem, where the goal is to maximize learning progress without exceeding a fixed energy budget. By solving for the optimal number of local epochs and communication rounds, you will engage with a realistic system design challenge that is critical for deploying energy-aware intelligent systems on the edge .",
            "id": "4044985",
            "problem": "Consider a cross-device Federated Learning (FL) system implemented on neuromorphic edge hardware using a Spiking Neural Network (SNN). There are $R$ communication rounds; in each round, a representative client performs $E$ local epochs of training before communicating its model update once. Assume symmetry across clients so that the per-client analysis suffices.\n\nHardware-energy model:\n- Each local epoch processes $n$ samples, and each sample generates on average $s$ synaptic events. The energy per synaptic event is $e_s$ joules, so the compute energy per local epoch is $c \\equiv e_s \\, n \\, s$.\n- In each communication round, the client transmits and receives a total of $L$ bits, and the per-bit communication energy is $e_c$ joules, so the per-round communication energy is $d \\equiv e_c \\, L$.\n\nEnergy budget:\n- The total per-client energy over the full training is $R \\big(E c + d\\big)$, which must not exceed a fixed budget $B$, that is $R \\big(E c + d\\big) \\le B$.\n\nLearning-theoretic objective:\n- Under standard smoothness and bounded-variance assumptions for stochastic optimization with heterogeneous data across clients, a commonly used upper bound on the expected optimality gap after $R$ rounds with $E$ local epochs per round is of the form\n$$\n\\Delta(E,R) \\le \\frac{\\alpha}{R E} + \\beta E,\n$$\nwhere $\\alpha  0$ aggregates smoothness, stepsize, and gradient-noise constants, and $\\beta  0$ aggregates client-drift and heterogeneity constants induced by multiple local steps.\n\nFormulate and solve the following constrained optimization problem to determine the real-valued positive pair $(E^{\\star}, R^{\\star})$ that minimizes the bound subject to the energy budget:\n$$\n\\min_{E  0, \\, R  0} \\; \\frac{\\alpha}{R E} + \\beta E \\quad \\text{subject to} \\quad R \\big(E c + d\\big) \\le B.\n$$\nExpress your final answer as exact closed-form symbolic expressions for $E^{\\star}$ and $R^{\\star}$ in terms of $\\alpha$, $\\beta$, $B$, $c$, and $d$ (and recall $c \\equiv e_s \\, n \\, s$, $d \\equiv e_c \\, L$). No numerical evaluation is required. Provide your answer as a pair in the order $(E^{\\star}, R^{\\star})$. Since the answer is symbolic, no rounding is needed. Units, if any, should not be included in the final expressions.",
            "solution": "The problem as stated is valid. It is a well-posed constrained optimization problem grounded in established models from federated learning and energy consumption analysis. All terms are clearly defined, and no scientific or logical contradictions are present.\n\nThe optimization problem is to find the pair of positive real numbers $(E^{\\star}, R^{\\star})$ that solves:\n$$\n\\min_{E  0, \\, R  0} \\; f(E,R) = \\frac{\\alpha}{R E} + \\beta E\n$$\nsubject to the constraint:\n$$\ng(E,R) = R \\big(E c + d\\big) \\le B\n$$\nAll parameters $\\alpha$, $\\beta$, $B$, $c$, and $d$ are given as positive constants.\n\nFirst, we analyze the structure of the problem. The objective function $f(E,R)$ consists of two terms. For any fixed value of $E  0$, the function $f(E,R)$ is a strictly decreasing function of $R$, because the term $\\frac{\\alpha}{RE}$ decreases as $R$ increases while the term $\\beta E$ remains constant.\n\nThis observation implies that to minimize $f(E,R)$, we should make $R$ as large as the constraint permits. If an optimal solution $(E, R)$ existed such that the constraint was not active, i.e., $R(Ec+d)  B$, we could always increase $R$ to a new value $R' = \\frac{B}{Ec+d}$. This $R'$ would be greater than $R$, satisfying the constraint with equality, and would yield a smaller objective function value, $f(E, R')  f(E, R)$. Therefore, any optimal solution $(E^{\\star}, R^{\\star})$ must lie on the boundary of the feasible set, meaning the energy constraint must be active (saturated).\n$$\nR^{\\star} \\big(E^{\\star} c + d\\big) = B\n$$\nThis allows us to express one variable in terms of the other. We can write $R$ as a function of $E$:\n$$\nR(E) = \\frac{B}{E c + d}\n$$\nWe can now substitute this expression for $R$ into the objective function to transform the constrained two-variable problem into an unconstrained optimization problem in a single variable, $E$.\nLet the new objective function be $h(E) = f(E, R(E))$:\n$$\nh(E) = \\frac{\\alpha}{R(E) E} + \\beta E = \\frac{\\alpha}{\\left(\\frac{B}{Ec + d}\\right) E} + \\beta E\n$$\nSimplifying the expression for $h(E)$:\n$$\nh(E) = \\frac{\\alpha (Ec + d)}{B E} + \\beta E = \\frac{\\alpha Ec}{B E} + \\frac{\\alpha d}{B E} + \\beta E\n$$\n$$\nh(E) = \\frac{\\alpha c}{B} + \\frac{\\alpha d}{B E} + \\beta E\n$$\nTo find the value of $E$ that minimizes $h(E)$, we compute its derivative with respect to $E$ and set it to zero. The term $\\frac{\\alpha c}{B}$ is a constant, so it does not affect the location of the minimum.\n$$\n\\frac{dh}{dE} = \\frac{d}{dE} \\left( \\frac{\\alpha c}{B} + \\frac{\\alpha d}{B} E^{-1} + \\beta E \\right) = -\\frac{\\alpha d}{B} E^{-2} + \\beta\n$$\nSetting the derivative to zero to find the optimal value $E^{\\star}$:\n$$\n-\\frac{\\alpha d}{B (E^{\\star})^2} + \\beta = 0\n$$\n$$\n\\beta = \\frac{\\alpha d}{B (E^{\\star})^2}\n$$\nRearranging to solve for $(E^{\\star})^2$:\n$$\n(E^{\\star})^2 = \\frac{\\alpha d}{\\beta B}\n$$\nSince $E$ must be positive, we take the positive square root:\n$$\nE^{\\star} = \\sqrt{\\frac{\\alpha d}{\\beta B}}\n$$\nTo confirm that this critical point corresponds to a minimum, we examine the second derivative of $h(E)$:\n$$\n\\frac{d^2h}{dE^2} = \\frac{d}{dE} \\left( -\\frac{\\alpha d}{B} E^{-2} + \\beta \\right) = -(-2)\\frac{\\alpha d}{B} E^{-3} = \\frac{2 \\alpha d}{B E^3}\n$$\nGiven that $\\alpha  0$, $d  0$, $B  0$, and we are in the domain $E  0$, the second derivative $\\frac{d^2h}{dE^2}$ is always positive. This confirms that the function $h(E)$ is strictly convex for $E  0$, and thus $E^{\\star}$ is the unique global minimum.\n\nFinally, we find the corresponding optimal value $R^{\\star}$ by substituting $E^{\\star}$ back into the expression derived from the active constraint:\n$$\nR^{\\star} = \\frac{B}{E^{\\star} c + d}\n$$\n$$\nR^{\\star} = \\frac{B}{c \\sqrt{\\frac{\\alpha d}{\\beta B}} + d}\n$$\nThe pair $(E^{\\star}, R^{\\star})$ represents the optimal allocation of local epochs and communication rounds that minimizes the convergence bound under the given energy budget. The expressions are provided in terms of the fundamental parameters of the problem.\nThe solution pair is:\n$$\nE^{\\star} = \\sqrt{\\frac{\\alpha d}{\\beta B}}\n$$\n$$\nR^{\\star} = \\frac{B}{d + c \\sqrt{\\frac{\\alpha d}{\\beta B}}}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{\\frac{\\alpha d}{\\beta B}}  \\frac{B}{d + c \\sqrt{\\frac{\\alpha d}{\\beta B}}} \\end{pmatrix}}\n$$"
        }
    ]
}