{
    "hands_on_practices": [
        {
            "introduction": "漏积分-发放（LIF）神经元是神经形态计算的基石，因其计算效率高而备受青睐。为了构建有效的学习系统，我们必须首先掌握其基本动力学。本练习  将指导您从第一性原理出发，推导膜电位的轨迹，将物理电路模型与其数学描述联系起来。这是理解脉冲神经元中时间信息处理（如首脉冲时间编码）的关键第一步。",
            "id": "4045046",
            "problem": "在一个跨孤岛联邦学习环境中，每个客户端部署一个神经形态核心，该核心实现一个基于电流的漏泄积分-发放 (Leaky Integrate-and-Fire, LIF) 神经元，用于将本地标量信号编码为首次脉冲延迟，该延迟将在服务器端进行聚合。神经元动力学由遵循基尔霍夫电流定律的漏泄阻容膜实现，并通过从阻容电路推导出的常微分方程进行连续建模：膜电容电流加上漏电流等于注入的突触电流。设膜电位为 $V(t)$，静息电位为 $V_{rest}$，漏电阻为 $R$，膜时间常数为 $\\tau_m$，并且当 $t \\ge 0$ 时恒定输入电流为 $I(t)=I_0$。该神经元遵循具有瞬时重置和硬阈值的基于电流的LIF动力学：当 $V(t)$ 从下方达到阈值 $V_{th}$ 时，神经元发放一个脉冲，并且 $V(t)$ 立即重置为 $V_{reset}$。在编码窗口的开始，假设神经元处于重置电位，即 $V(0)=V_{reset}$。\n\n从阻容电路定律出发，并仅使用第一性原理（基尔霍夫电流定律和膜电容 $C_m$ 的定义 $\\tau_m = R C_m$），推导在恒定输入 $I(t)=I_0$ 下，对于 $t \\in [0,t_{sp})$ 的膜电位轨迹 $V(t)$，其中 $t_{sp}$ 是首次穿越阈值的时间。然后，通过强制 $V(t_{sp})=V_{th}$，用给定的参数解析求解首次脉冲时间 $t_{sp}$。假设 $R I_0 > V_{th}-V_{rest}$ 和 $V_{reset}  V_{th}$，以确保在有限时间内发生脉冲。以秒为单位表示 $t_{sp}$。请以单行矩阵的形式提供最终答案，该矩阵包含用 $(\\tau_m, V_{rest}, V_{th}, V_{reset}, R, I_0)$ 和变量 $t$ 表示的 $V(t)$ 的闭式表达式和 $t_{sp}$ 的闭式表达式。不需要进行数值代入，也不需要四舍五入。",
            "solution": "经评估，该问题是有效的，因为它在科学上基于成熟的电路理论和计算神经科学，问题陈述适定，信息充分且一致，并且表述客观。\n\n该问题要求推导一个基于电流的漏泄积分-发放 (LIF) 神经元的膜电位 $V(t)$ 和首次脉冲时间 $t_{sp}$。推导从第一性原理开始，特别是应用于阻容 (RC) 电路的基尔霍夫电流定律。\n\n该模型由一个膜电容 $C_m$ 和一个漏电阻 $R$ 并联组成。注入神经元的总电流 $I(t)$ 分为两条路径：通过电容器的电流 $I_C$ 和通过电阻器的电流 $I_R$。\n\n根据基尔霍夫电流定律，离开一个节点的电流总和必须等于进入该节点的电流：\n$$I(t) = I_C(t) + I_R(t)$$\n通过电容器的电流由 $I_C(t) = C_m \\frac{dV(t)}{dt}$ 给出，其中 $V(t)$ 是膜电位。通过漏电阻的电流由欧姆定律给出，$I_R(t) = \\frac{V(t) - V_{rest}}{R}$，其中 $V_{rest}$ 是膜两端的静息电位。\n\n将这些表达式代入基尔霍夫定律，我们得到控制膜电位的常微分方程 (ODE)：\n$$I(t) = C_m \\frac{dV(t)}{dt} + \\frac{V(t) - V_{rest}}{R}$$\n对于 $t \\ge 0$，输入电流是恒定的 $I(t) = I_0$。方程变为：\n$$I_0 = C_m \\frac{dV(t)}{dt} + \\frac{V(t) - V_{rest}}{R}$$\n为了求解此常微分方程，我们将其重排为标准的一阶线性形式。将整个方程乘以 $R$ 得到：\n$$R I_0 = R C_m \\frac{dV(t)}{dt} + V(t) - V_{rest}$$\n使用给定的膜时间常数定义 $\\tau_m = R C_m$，我们将其代入方程中：\n$$\\tau_m \\frac{dV(t)}{dt} + V(t) = V_{rest} + R I_0$$\n这是一个非齐次一阶线性常微分方程。其通解可以通过多种方法求得。我们定义稳态电位 $V_{\\infty} = V_{rest} + R I_0$，这是在没有脉冲阈值的情况下，当 $t \\to \\infty$ 时电位将达到的值。该常微分方程可以写为：\n$$\\tau_m \\frac{dV(t)}{dt} = -(V(t) - V_{\\infty})$$\n此方程的通解形式为：\n$$V(t) = V_{\\infty} + K \\exp\\left(-\\frac{t}{\\tau_m}\\right)$$\n其中 $K$ 是由初始条件决定的积分常数。问题陈述，神经元在时间 $t=0$ 时处于其重置电位，因此初始条件为 $V(0) = V_{reset}$。应用此条件：\n$$V(0) = V_{reset} = V_{\\infty} + K \\exp(0) = V_{\\infty} + K$$\n求解 $K$：\n$$K = V_{reset} - V_{\\infty} = V_{reset} - (V_{rest} + R I_0)$$\n将 $K$ 和 $V_{\\infty}$ 代回 $V(t)$ 的通解，我们得到在 $t \\in [0, t_{sp})$ 范围内膜电位的具体轨迹：\n$$V(t) = (V_{rest} + R I_0) + (V_{reset} - V_{rest} - R I_0) \\exp\\left(-\\frac{t}{\\tau_m}\\right)$$\n这是所需答案的第一部分。\n\n接下来，我们求解首次脉冲的时间 $t_{sp}$。当膜电位 $V(t)$ 达到阈值电位 $V_{th}$ 时，会产生一个脉冲。因此，我们设 $V(t_{sp}) = V_{th}$ 并求解 $t_{sp}$：\n$$V_{th} = (V_{rest} + R I_0) + (V_{reset} - V_{rest} - R I_0) \\exp\\left(-\\frac{t_{sp}}{\\tau_m}\\right)$$\n我们重排方程以分离指数项：\n$$V_{th} - V_{rest} - R I_0 = (V_{reset} - V_{rest} - R I_0) \\exp\\left(-\\frac{t_{sp}}{\\tau_m}\\right)$$\n$$\\exp\\left(-\\frac{t_{sp}}{\\tau_m}\\right) = \\frac{V_{th} - V_{rest} - R I_0}{V_{reset} - V_{rest} - R I_0}$$\n为了求解 $t_{sp}$，我们对两边取自然对数：\n$$-\\frac{t_{sp}}{\\tau_m} = \\ln\\left(\\frac{V_{th} - V_{rest} - R I_0}{V_{reset} - V_{rest} - R I_0}\\right)$$\n最后，求解 $t_{sp}$：\n$$t_{sp} = -\\tau_m \\ln\\left(\\frac{V_{th} - V_{rest} - R I_0}{V_{reset} - V_{rest} - R I_0}\\right)$$\n使用对数恒等式 $-\\ln(a/b) = \\ln(b/a)$，我们可以将表达式写成一种避免了前导负号的形式：\n$$t_{sp} = \\tau_m \\ln\\left(\\frac{V_{reset} - V_{rest} - R I_0}{V_{th} - V_{rest} - R I_0}\\right)$$\n问题给出了约束条件 $R I_0 > V_{th} - V_{rest}$ 和 $V_{reset}  V_{th}$。第一个约束意味着 $V_{rest} + R I_0 > V_{th}$，确保了稳态电位高于阈值，从而保证了脉冲的发生。这也意味着分母 $V_{th} - V_{rest} - R I_0$ 是负的。由于 $V_{reset}  V_{th}  V_{rest} + R I_0$，分子 $V_{reset} - V_{rest} - R I_0$ 也是负的。因此，对数的自变量是正的。第二个约束 $V_{reset}  V_{th}$ 确保对数的自变量大于1，从而得到一个正的脉冲时间 $t_{sp} > 0$。这就完成了首次脉冲时间的推导。\n\n这两个推导出的表达式是该问题的完整解。",
            "answer": "$$\\boxed{\\begin{pmatrix} V_{rest} + R I_0 + (V_{reset} - V_{rest} - R I_0) \\exp\\left(-\\frac{t}{\\tau_m}\\right)  \\tau_m \\ln\\left(\\frac{V_{reset} - V_{rest} - R I_0}{V_{th} - V_{rest} - R I_0}\\right) \\end{pmatrix}}$$"
        },
        {
            "introduction": "在理解了单个LIF神经元的动力学之后，下一个挑战是如何使其能够从数据中学习。脉冲事件的“全有或全无”特性使其激活函数不可微，这为基于梯度的优化方法带来了巨大障碍。本练习  介绍了代理梯度法，这是一项绕过此问题的关键技术，为您提供了实现脉冲神经元学习规则的实践经验。",
            "id": "4045055",
            "problem": "一个参与联邦学习（FL）的神经形态边缘客户端在本地训练一个漏积分放电（LIF）神经元，并仅通过联邦平均（FedAvg）通信权重更新。该神经元在一个离散时间步内接收单个突触前事件，并使用代理梯度来实现基于梯度的学习。该神经元的膜电位遵循连续时间LIF动力学 $$\\tau_{m} \\frac{dV(t)}{dt} = -\\left(V(t) - V_{\\mathrm{rest}}\\right) + R I(t),$$ 其中 $\\tau_{m}$ 是膜时间常数，$V_{\\mathrm{rest}}$ 是静息电位，$R$ 是膜电阻，$I(t)$ 是突触输入电流。使用步长为 $\\Delta t$ 的前向欧拉法，任何复位前的单步更新为 $$V_{1} = \\alpha V_{0} + w x_{1}, \\quad \\text{其中} \\quad \\alpha = \\exp\\!\\left(-\\frac{\\Delta t}{\\tau_{m}}\\right),$$ $w$ 是突触权重，$x_{1}$ 是该时间步内的突触前输入幅度。如果神经元的复位前膜电位超过阈值，它就会发放一个脉冲，这由赫维赛德函数建模：$$s_{1} = H\\!\\left(V_{1} - v_{\\mathrm{th}}\\right),$$ 其中 $v_{\\mathrm{th}}$ 是阈值。此客户端上单个样本的局部训练损失为 $$L = \\frac{1}{2}\\left(s_{1} - y\\right)^{2},$$ 目标 $y \\in \\{0,1\\}$。为了实现可微性，在反向传播中，赫维赛德函数被一个尺度为 $a  0$ 的逻辑斯蒂代理函数替代，$$\\tilde{H}(u) = \\sigma\\!\\left(\\frac{u}{a}\\right), \\quad \\sigma(z) = \\frac{1}{1 + \\exp(-z)},$$ 因此用于反向传播的代理导数为 $$\\rho(u) = \\frac{d}{du}\\tilde{H}(u) = \\frac{1}{a}\\,\\sigma\\!\\left(\\frac{u}{a}\\right)\\left[1 - \\sigma\\!\\left(\\frac{u}{a}\\right)\\right].$$\n\n仅从这些定义和链式法则出发，推导梯度 $\\frac{\\partial L}{\\partial w}$ 关于 $x_{1}$、$y$、$v_{\\mathrm{th}}$、$V_{1}$ 和 $a$ 的解析表达式。然后，对于该客户端上的一个单脉冲情况，参数如下：$$\\tau_{m} = 20\\,\\mathrm{ms}, \\quad \\Delta t = 1\\,\\mathrm{ms}, \\quad V_{0} = 0, \\quad v_{\\mathrm{th}} = 1.0, \\quad w = 0.8, \\quad x_{1} = 1.5, \\quad y = 0, \\quad a = 0.5,$$ 使用代理梯度和上述单步前向动力学计算 $\\frac{\\partial L}{\\partial w}$ 的数值。将最终数值答案四舍五入到四位有效数字，并表示为一个无量纲数。",
            "solution": "该问题要求完成两个部分：首先，使用代理梯度法推导损失函数相对于突触权重 $\\frac{\\partial L}{\\partial w}$ 的梯度的解析表达式；其次，为一组特定参数计算该梯度的数值。\n\n对问题陈述的验证确认了其科学基础扎实、问题设定良好，并包含所有必要信息。该问题描述了使用代理梯度训练脉冲神经网络（SNN）的标准场景，这是神经形态计算中一种有效且成熟的技术。\n\n**第一部分：梯度 $\\frac{\\partial L}{\\partial w}$ 的解析推导**\n\n局部损失函数 $L$ 定义为：\n$$L = \\frac{1}{2}(s_1 - y)^2$$\n其中 $s_1$ 是神经元的输出脉冲，$y$ 是目标标签，$w$ 是突触权重，$x_1$ 是输入。输出脉冲 $s_1$ 是膜电位 $V_1$ 的函数，而 $V_1$ 又是权重 $w$ 的函数。具体来说：\n$$s_1 = H(V_1 - v_{\\mathrm{th}})$$\n$$V_1 = \\alpha V_0 + w x_1$$\n这里，$H$ 是赫维赛德阶跃函数，$v_{\\mathrm{th}}$ 是放电阈值，$V_0$ 是初始电位，$\\alpha$ 是膜电位衰减因子。\n\n我们寻求计算梯度 $\\frac{\\partial L}{\\partial w}$。使用链式法则，我们可以将此梯度表示为三个偏导数的乘积：\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial s_1} \\frac{\\partial s_1}{\\partial V_1} \\frac{\\partial V_1}{\\partial w}$$\n\n让我们分别计算每一项。\n\n1.  **损失相对于输出脉冲的导数，$\\frac{\\partial L}{\\partial s_1}$**：\n    根据 $L$ 的定义，我们有：\n    $$\\frac{\\partial L}{\\partial s_1} = \\frac{\\partial}{\\partial s_1} \\left[ \\frac{1}{2}(s_1 - y)^2 \\right] = 2 \\cdot \\frac{1}{2}(s_1 - y) = s_1 - y$$\n\n2.  **膜电位相对于权重的导数，$\\frac{\\partial V_1}{\\partial w}$**：\n    根据 $V_1$ 的单步更新方程，我们有：\n    $$\\frac{\\partial V_1}{\\partial w} = \\frac{\\partial}{\\partial w} (\\alpha V_0 + w x_1)$$\n    由于 $\\alpha$、$V_0$ 和 $x_1$ 都不是 $w$ 的函数，这可以简化为：\n    $$\\frac{\\partial V_1}{\\partial w} = x_1$$\n\n3.  **输出脉冲相对于膜电位的导数，$\\frac{\\partial s_1}{\\partial V_1}$**：\n    前向传播的输出由 $s_1 = H(V_1 - v_{\\mathrm{th}})$ 给出。赫维赛德函数的导数是狄拉克δ函数，它几乎处处为零，在原点未定义。这给基于梯度的优化带来了问题。代理梯度法通过在反向传播中用一个连续、表现良好的代理函数 $\\rho(u)$ 替换 $H(u)$ 的不可微导数来解决此问题。问题将此代理导数定义为：\n    $$\\rho(u) = \\frac{1}{a} \\sigma\\left(\\frac{u}{a}\\right) \\left[1 - \\sigma\\left(\\frac{u}{a}\\right)\\right]$$\n    其中 $u = V_1 - v_{\\mathrm{th}}$。因此，为了反向传播，我们进行替换：\n    $$\\frac{\\partial s_1}{\\partial V_1} \\approx \\rho(V_1 - v_{\\mathrm{th}})$$\n\n将这三项组合起来，我们得到损失相对于权重的梯度表达式：\n$$\\frac{\\partial L}{\\partial w} = (s_1 - y) \\cdot \\rho(V_1 - v_{\\mathrm{th}}) \\cdot x_1$$\n代入 $\\rho(u)$ 的定义，我们得到完整的解析表达式：\n$$\\frac{\\partial L}{\\partial w} = (s_1 - y) \\left( \\frac{1}{a} \\sigma\\left(\\frac{V_1 - v_{\\mathrm{th}}}{a}\\right) \\left[1 - \\sigma\\left(\\frac{V_1 - v_{\\mathrm{th}}}{a}\\right)\\right] \\right) x_1$$\n这里，$s_1$ 是在前向传播过程中计算的值，即 $s_1 = H(V_1 - v_{\\mathrm{th}})$。该表达式是关于 $x_1$、$y$、$v_{\\mathrm{th}}$、$V_1$ 和 $a$ 的函数，正如所要求的，同时应理解 $s_1$ 是由 $V_1$ 和 $v_{\\mathrm{th}}$ 决定的。\n\n**第二部分：梯度的数值计算**\n\n给定以下参数：\n$\\tau_{m} = 20\\,\\mathrm{ms}$，$\\Delta t = 1\\,\\mathrm{ms}$，$V_{0} = 0$，$v_{\\mathrm{th}} = 1.0$，$w = 0.8$，$x_{1} = 1.5$，$y = 0$，$a = 0.5$。\n\n计算按步骤进行，遵循前向和反向传播的逻辑。\n\n1.  **计算膜电位 $V_1$（前向传播）**：\n    首先，我们确定衰减因子 $\\alpha$：\n    $$\\alpha = \\exp\\left(-\\frac{\\Delta t}{\\tau_{m}}\\right) = \\exp\\left(-\\frac{1\\,\\mathrm{ms}}{20\\,\\mathrm{ms}}\\right) = \\exp(-0.05)$$\n    现在，我们计算复位前的膜电位 $V_1$：\n    $$V_1 = \\alpha V_0 + w x_1 = \\exp(-0.05) \\cdot 0 + (0.8)(1.5) = 1.2$$\n\n2.  **计算输出脉冲 $s_1$（前向传播）**：\n    通过将 $V_1$ 与阈值 $v_{\\mathrm{th}}$ 进行比较来确定输出脉冲：\n    $$s_1 = H(V_1 - v_{\\mathrm{th}}) = H(1.2 - 1.0) = H(0.2)$$\n    由于参数为正，赫维赛德函数的值为1：\n    $$s_1 = 1$$\n\n3.  **计算梯度分量（反向传播）**：\n    我们现在使用推导出的 $\\frac{\\partial L}{\\partial w}$ 公式：\n    $$\\frac{\\partial L}{\\partial w} = (s_1 - y) \\cdot \\rho(V_1 - v_{\\mathrm{th}}) \\cdot x_1$$\n    -   误差项为 $(s_1 - y) = 1 - 0 = 1$。\n    -   输入项为 $x_1 = 1.5$。\n    -   代理梯度项为 $\\rho(V_1 - v_{\\mathrm{th}}) = \\rho(0.2)$。让我们来计算它。\n        sigmoid函数的参数是 $u/a = (V_1 - v_{\\mathrm{th}})/a = 0.2 / 0.5 = 0.4$。\n        sigmoid函数的值是：\n        $$\\sigma(0.4) = \\frac{1}{1 + \\exp(-0.4)} \\approx \\frac{1}{1 + 0.670320046} \\approx \\frac{1}{1.670320046} \\approx 0.59868766$$\n        现在我们可以计算代理导数 $\\rho(0.2)$ 的值：\n        $$\\rho(0.2) = \\frac{1}{a} \\sigma(0.4) [1 - \\sigma(0.4)] = \\frac{1}{0.5} (0.59868766) [1 - 0.59868766]$$\n        $$\\rho(0.2) = 2 \\cdot (0.59868766) \\cdot (0.40131234) \\approx 0.4805084$$\n\n4.  **组合以求得最终梯度**：\n    $$\\frac{\\partial L}{\\partial w} = (1) \\cdot (0.4805084) \\cdot (1.5) = 0.7207626$$\n\n5.  **四舍五入到四位有效数字**：\n    计算值为 $0.7207626$。四舍五入到四位有效数字得到 $0.7208$。",
            "answer": "$$\n\\boxed{0.7208}\n$$"
        },
        {
            "introduction": "具备了训练单个神经形态客户端的能力后，我们现在可以将其扩展到联邦学习的环境中。在联邦学习中，中央服务器通过聚合来自多个客户端的模型更新来协调学习过程，而无需访问其私有数据。本练习  深入探讨了这一过程的理论核心，指导您推导出标准的聚合权重，以确保全局更新是中心化梯度的无偏估计，这也是著名的联邦平均（FedAvg）算法的基础。",
            "id": "4045019",
            "problem": "考虑一个由 $K$ 个神经形态边缘客户端组成的同步联邦学习系统，每个客户端都运行一个使用代理梯度随机梯度下降训练的脉冲神经网络。客户端 $k \\in \\{1,\\dots,K\\}$ 持有大小为 $n_k$ 的本地数据集 $\\mathcal{D}_k$，且 $\\sum_{k=1}^{K} n_k = N$。假设以下基本条件成立：\n- 各客户端之间的数据是独立同分布 (IID) 的：每个 $\\mathcal{D}_k$ 由 $n_k$ 个从相同的数据-标签对 $(\\mathbf{x}, y)$ 分布中抽取的独立样本组成。\n- 单样本脉冲损失 $\\ell(w; \\mathbf{x}, y)$ 在经过代理松弛后对 $w$ 是可微的，并且在神经形态硬件上计算的代理梯度估计器 $\\widehat{\\nabla}\\ell(w; \\mathbf{x}, y)$ 是无偏的，即 $\\mathbb{E}[\\widehat{\\nabla}\\ell(w; \\mathbf{x}, y)] = \\nabla \\ell(w; \\mathbf{x}, y)$，其中期望是针对数据采样和硬件引起的随机性（例如，量化和事件噪声）计算的。\n- 在给定的一轮中，每个客户端计算其本地经验梯度为 $g_k(w) = \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\widehat{\\nabla}\\ell(w; z)$，其中 $z = (\\mathbf{x}, y)$。\n\n在数据集并集 $\\mathcal{D} = \\bigcup_{k=1}^{K} \\mathcal{D}_k$ 上的中心化经验风险是 $L_N(w) = \\frac{1}{N} \\sum_{z \\in \\mathcal{D}} \\ell(w; z)$，其相应的中心化经验梯度为 $\\nabla L_N(w) = \\frac{1}{N} \\sum_{z \\in \\mathcal{D}} \\nabla \\ell(w; z)$。\n\n参数服务器使用权重 $p_k \\ge 0$（满足 $\\sum_{k=1}^{K} p_k = 1$）将客户端梯度聚合成一个单一的更新方向，产生聚合梯度 $g_{\\mathrm{agg}}(w) = \\sum_{k=1}^{K} p_k \\, g_k(w)$ 和全局更新 $w^{+} = w - \\eta \\, g_{\\mathrm{agg}}(w)$，其中 $\\eta  0$ 是一个共同的学习率。\n\n仅从上述定义以及 IID 和无偏性假设出发，确定权重 $p_k$ 作为数据集大小 $n_k$ 的函数，使得聚合梯度在期望上等于中心化经验梯度，即 $\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\nabla L_N(w)$。然后通过显式推导来验证此等式。\n\n请以关于 $\\{n_k\\}_{k=1}^{K}$ 的单个闭式解析表达式的形式提供您的最终答案，无需四舍五入，也无单位。最终答案中不要提供任何中间步骤。",
            "solution": "问题陈述已经过验证，被认为是有效的。它具有科学依据、是良定的、客观的且内部一致。它提出了联邦学习领域中的一个标准理论问题，具体涉及推导聚合权重以确保聚合梯度是中心化经验梯度的无偏估计量。\n\n目标是找到加权系数 $p_k$（对于 $k \\in \\{1, \\dots, K\\}$），使其满足条件 $\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\nabla L_N(w)$。期望 $\\mathbb{E}[\\cdot]$ 是针对代理梯度估计器中由硬件引起的随机性计算的。对于这个期望而言，数据集 $\\{\\mathcal{D}_k\\}_{k=1}^{K}$ 被视为固定的。\n\n我们首先展开目标等式的左侧 (LHS)，即期望聚合梯度 $\\mathbb{E}[g_{\\mathrm{agg}}(w)]$。使用聚合梯度的定义 $g_{\\mathrm{agg}}(w) = \\sum_{k=1}^{K} p_k \\, g_k(w)$ 和期望算子的线性性质，我们有：\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\mathbb{E}\\left[ \\sum_{k=1}^{K} p_k \\, g_k(w) \\right] = \\sum_{k=1}^{K} p_k \\, \\mathbb{E}[g_k(w)]\n$$\n接下来，我们代入本地经验梯度的定义，$g_k(w) = \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\widehat{\\nabla}\\ell(w; z)$：\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\sum_{k=1}^{K} p_k \\, \\mathbb{E}\\left[ \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\widehat{\\nabla}\\ell(w; z) \\right]\n$$\n根据期望的线性性质，我们可以将期望算子移到对本地数据集 $\\mathcal{D}_k$ 的求和内部：\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\sum_{k=1}^{K} p_k \\, \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\mathbb{E}[\\widehat{\\nabla}\\ell(w; z)]\n$$\n问题陈述指出，对于任意给定的数据样本 $z = (\\mathbf{x}, y)$，代理梯度估计器是无偏的，使得 $\\mathbb{E}[\\widehat{\\nabla}\\ell(w; z)] = \\nabla \\ell(w; z)$。应用此假设，我们得到：\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\sum_{k=1}^{K} \\frac{p_k}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z)\n$$\n现在，我们分析目标等式的右侧 (RHS)，即中心化经验梯度 $\\nabla L_N(w)$。根据其定义，$\\nabla L_N(w) = \\frac{1}{N} \\sum_{z \\in \\mathcal{D}} \\nabla \\ell(w; z)$。总数据集 $\\mathcal{D}$ 是本地数据集的并集，即 $\\mathcal{D} = \\bigcup_{k=1}^{K} \\mathcal{D}_k$。假设本地数据集是不相交的，我们可以将对 $\\mathcal{D}$ 的求和重写为对各个本地数据集的求和：\n$$\n\\nabla L_N(w) = \\frac{1}{N} \\sum_{k=1}^{K} \\sum_{z \\in \\mathcal{D}_k} \\nabla \\ell(w; z)\n$$\n为了满足问题的要求，我们将推导出的 LHS 表达式与 RHS 相等：\n$$\n\\sum_{k=1}^{K} \\frac{p_k}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z) = \\frac{1}{N} \\sum_{k=1}^{K} \\sum_{z \\in \\mathcal{D}_k} \\nabla \\ell(w; z)\n$$\n这个方程可以重新排列成一个单一的求和式：\n$$\n\\sum_{k=1}^{K} \\left( \\frac{p_k}{n_k} - \\frac{1}{N} \\right) \\left( \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z) \\right) = 0\n$$\n这个等式必须对任何本地数据集集合 $\\{\\mathcal{D}_k\\}$ 和任何模型参数 $w$ 都成立。代表本地梯度和的向量 $\\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z)$ 可以是线性无关的。为了在这样的一般条件下使总和为零向量，每个向量项的标量系数必须为零。因此，对于每个客户端 $k \\in \\{1, \\dots, K\\}$，我们必须有：\n$$\n\\frac{p_k}{n_k} - \\frac{1}{N} = 0\n$$\n解出 $p_k$ 可得：\n$$\np_k = \\frac{n_k}{N}\n$$\n问题定义了 $N = \\sum_{j=1}^{K} n_j$。因此，权重与本地数据集的大小成正比。我们验证这些权重是否满足约束条件 $\\sum_{k=1}^{K} p_k = 1$：\n$$\n\\sum_{k=1}^{K} p_k = \\sum_{k=1}^{K} \\frac{n_k}{N} = \\frac{1}{N} \\sum_{k=1}^{K} n_k = \\frac{N}{N} = 1\n$$\n该约束得到满足。由于 $n_k \\ge 0$（数据集大小）且 $N  0$，条件 $p_k \\ge 0$ 也得到满足。值得注意的是，虽然给出了 IID 数据假设，但对于建立期望聚合梯度与中心化经验梯度之间的这一特定等式而言，该假设并非必要。然而，IID 属性对于确保该中心化经验梯度本身是真实全局总体梯度的无偏估计至关重要。\n\n最后，我们执行问题所要求的显式验证。我们将 $p_k = n_k/N$ 代回期望聚合梯度的表达式中：\n$$\n\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\sum_{k=1}^{K} p_k \\, \\mathbb{E}[g_k(w)] = \\sum_{k=1}^{K} \\frac{n_k}{N} \\, \\mathbb{E}\\left[ \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\widehat{\\nabla}\\ell(w; z) \\right]\n$$\n$$\n= \\sum_{k=1}^{K} \\frac{n_k}{N} \\frac{1}{n_k} \\sum_{z \\in \\mathcal{D}_k} \\mathbb{E}[\\widehat{\\nabla}\\ell(w; z)] = \\sum_{k=1}^{K} \\frac{1}{N} \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z)\n$$\n$$\n= \\frac{1}{N} \\sum_{k=1}^{K} \\sum_{z \\in \\mathcal{D}_k} \\nabla\\ell(w; z) = \\frac{1}{N} \\sum_{z \\in \\mathcal{D}} \\nabla\\ell(w; z) = \\nabla L_N(w)\n$$\n验证证实，当选择 $p_k = n_k/N$ 时，等式 $\\mathbb{E}[g_{\\mathrm{agg}}(w)] = \\nabla L_N(w)$ 成立。因此，解为 $p_k = \\frac{n_k}{\\sum_{j=1}^{K} n_j}$。",
            "answer": "$$\\boxed{\\frac{n_k}{\\sum_{j=1}^{K} n_j}}$$"
        }
    ]
}