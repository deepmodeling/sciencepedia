{
    "hands_on_practices": [
        {
            "introduction": "脉冲神经网络（SNN）的对抗性攻击，尤其是基于梯度的攻击，依赖于能够计算损失函数相对于输入脉冲的梯度。这个练习将指导你推导该梯度的解析表达式，从而为理解如何精确计算并应用微小扰动以最大化地影响网络输出奠定数学基础。通过使用代理梯度方法处理脉冲发放的不可微特性，你将深入探究时间反向传播（backpropagation through time）在SNN中的核心机制 。",
            "id": "4034865",
            "problem": "考虑一个单层脉冲神经网络（SNN），用作对抗性鲁棒性分析的特征提取器。有 $N$ 个输入通道，索引为 $i \\in \\{1,\\dots,N\\}$，以及 $T$ 个离散时间步，索引为 $t \\in \\{1,\\dots,T\\}$。输入脉冲序列为 $x_{i,t} \\in \\{0,1\\}$，但为了进行基于梯度的对抗性扰动分析，将 $x_{i,t}$ 视为一个实值变量。突触后神经元的膜电位为 $u_t \\in \\mathbb{R}$，其离散时间泄露积分过程无重置：\n$$\nu_t \\;=\\; \\lambda\\,u_{t-1} \\;+\\; \\sum_{i=1}^{N} w_i\\, x_{i,t}, \\quad u_0 \\;=\\; 0,\n$$\n其中 $\\lambda \\in (0,1)$ 是泄露因子， $w_i \\in \\mathbb{R}$ 是突触权重。神经元通过在 $\\vartheta \\in \\mathbb{R}$ 处的硬阈值化来发放脉冲 $s_t \\in \\{0,1\\}$：\n$$\ns_t \\;=\\; H\\!\\left(u_t - \\vartheta\\right),\n$$\n其中 $H(\\cdot)$ 是 Heaviside 阶跃函数。对于反向传播，使用一个宽度参数为 $\\beta > 0$ 的三角形代理导数来处理阈值非线性：\n$$\n\\psi_{\\beta}(z) \\;=\\; \\begin{cases}\n\\dfrac{1}{\\beta}\\left(1 - \\dfrac{|z|}{\\beta}\\right), & \\text{if } |z| \\le \\beta, \\\\[6pt]\n0, & \\text{if } |z| > \\beta,\n\\end{cases}\n$$\n并用 $\\psi_{\\beta}\\!\\left(u_t - \\vartheta\\right)$ 来近似 $\\dfrac{\\partial s_t}{\\partial u_t}$。\n\n一个线性读出层将脉冲聚合成一个标量计数 $S \\;=\\; \\sum_{t=1}^{T} s_t$，然后生成 $K$ 个类别 logits\n$$\nz_k \\;=\\; \\alpha_k\\, S \\;+\\; b_k, \\quad k \\in \\{1,\\dots,K\\},\n$$\n其中 $\\alpha_k, b_k \\in \\mathbb{R}$。预测的类别概率由 softmax 函数给出\n$$\np_k \\;=\\; \\dfrac{\\exp(z_k)}{\\sum_{\\ell=1}^{K} \\exp(z_{\\ell})},\n$$\n训练目标是针对 one-hot 标签向量 $y \\in \\{0,1\\}^{K}$ 的单样本交叉熵损失，\n$$\n\\mathcal{L} \\;=\\; - \\sum_{k=1}^{K} y_k \\,\\ln p_k.\n$$\n\n在此设定下，计算损失函数关于输入分量 $x_{j,\\tau}$ 的梯度的精确解析表达式（以闭合形式），其中 $j \\in \\{1,\\dots,N\\}$ 和 $\\tau \\in \\{1,\\dots,T\\}$ 是固定的：\n$$\n\\dfrac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} \\quad \\text{作为 } \\left(\\lambda, \\{w_i\\}_{i=1}^{N}, \\vartheta, \\beta, \\{\\alpha_k,b_k\\}_{k=1}^{K}, \\{x_{i,t}\\}, \\{u_t\\}\\right) \\text{ 的函数}。\n$$\n\n您的最终答案必须是单一的闭合形式符号表达式。不需要数值近似或四舍五入。您的答案不应包含任何单位。避免使用任何不等式或方程式作为最终答案；只提供所要求的单一表达式。",
            "solution": "该问题是有效的。它提出了一个定义清晰、自成体系且有科学依据的简单脉冲神经网络（SNN）模型，并要求计算一个特定的梯度。必要方程、参数和初始条件都已给出，任务是应用微分学原理，特别是链式法则，来推导一个解析表达式。对于不可微的脉冲机制，使用代理梯度是训练 SNN 领域的一项标准技术。该问题是适定的，并允许一个唯一的、精确的解。\n\n我们的目标是计算损失函数 $\\mathcal{L}$ 在特定时间步 $\\tau$ 和输入通道 $j$ 上相对于单个输入分量 $x_{j,\\tau}$ 的梯度。这需要沿着网络的计算图，从损失函数反向到输入，应用链式法则。计算路径如下： $\\mathcal{L} \\rightarrow \\{p_k\\} \\rightarrow \\{z_k\\} \\rightarrow S \\rightarrow \\{s_t\\} \\rightarrow \\{u_t\\} \\rightarrow x_{j,\\tau}$。\n\n首先，我们计算交叉熵损失 $\\mathcal{L} = - \\sum_{k=1}^{K} y_k \\ln p_k$ 相对于 logits $z_m$ 的梯度。概率 $p_k$ 由 softmax 函数给出：$p_k = \\frac{\\exp(z_k)}{\\sum_{\\ell=1}^{K} \\exp(z_{\\ell})}$。\n损失函数相对于一个 logit $z_m$ 的导数是一个标准结果：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial z_m} = p_m - y_m.\n$$\n\n接下来，我们求损失函数相对于总脉冲数 $S$ 的梯度。logits 定义为 $z_k = \\alpha_k S + b_k$。使用链式法则，我们对所有依赖于 $S$ 的 logits $z_k$ 进行求和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\frac{\\partial \\mathcal{L}}{\\partial z_k} \\frac{\\partial z_k}{\\partial S}.\n$$\n因为 $\\frac{\\partial z_k}{\\partial S} = \\alpha_k$，我们代入之前的结果：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\alpha_k (p_k - y_k).\n$$\n这一项相对于时间索引 $t$ 是常数。\n\n现在，我们将梯度传播到单个脉冲 $s_t$。总脉冲数是 $S = \\sum_{t=1}^{T} s_t$。损失函数相对于单个脉冲 $s_t$ 的梯度是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_t} = \\frac{\\partial \\mathcal{L}}{\\partial S} \\frac{\\partial S}{\\partial s_t}.\n$$\n考虑到 $\\frac{\\partial S}{\\partial s_t} = 1$，我们有：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_t} = \\frac{\\partial \\mathcal{L}}{\\partial S} = \\sum_{k=1}^{K} \\alpha_k (p_k - y_k).\n$$\n\n下一步是将梯度传播到膜电位 $u_t$。脉冲 $s_t$ 由 $s_t = H(u_t - \\vartheta)$ 生成，其中 $H(\\cdot)$ 是 Heaviside 阶跃函数。由于 $H(\\cdot)$ 的导数几乎处处为零且在原点未定义，我们使用所提供的代理梯度 $\\psi_{\\beta}(z)$。我们近似 $\\frac{\\partial s_t}{\\partial u_t} \\approx \\psi_{\\beta}(u_t - \\vartheta)$。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u_t} = \\frac{\\partial \\mathcal{L}}{\\partial s_t} \\frac{\\partial s_t}{\\partial u_t} \\approx \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\psi_{\\beta}(u_t - \\vartheta).\n$$\n\n最后，我们计算损失函数相对于输入 $x_{j,\\tau}$ 的梯度。在时间 $\\tau$ 的输入会影响所有后续时间 $t \\ge \\tau$ 的膜电位。因此，我们必须通过所有受影响的电位 $\\{u_t\\}_{t=\\tau}^T$ 来累加 $x_{j,\\tau}$ 的贡献。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial u_t} \\frac{\\partial u_t}{\\partial x_{j,\\tau}}.\n$$\n我们需要确定雅可比项 $\\frac{\\partial u_t}{\\partial x_{j,\\tau}}$。膜电位根据 $u_t = \\lambda u_{t-1} + \\sum_{i=1}^{N} w_i x_{i,t}$ 演化，其中 $u_0 = 0$。\n如果 $t  \\tau$，$u_t$ 不依赖于 $x_{j,\\tau}$，所以 $\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = 0$。\n如果 $t = \\tau$，我们有 $u_\\tau = \\lambda u_{\\tau-1} + \\sum_{i=1}^{N} w_i x_{i,\\tau}$。项 $u_{\\tau-1}$ 不依赖于 $x_{j,\\tau}$，所以：\n$$\n\\frac{\\partial u_\\tau}{\\partial x_{j,\\tau}} = w_j.\n$$\n如果 $t  \\tau$，我们对递推关系应用链式法则：\n$$\n\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\frac{\\partial}{\\partial x_{j,\\tau}} \\left( \\lambda u_{t-1} + \\sum_{i=1}^{N} w_i x_{i,t} \\right) = \\lambda \\frac{\\partial u_{t-1}}{\\partial x_{j,\\tau}}.\n$$\n这定义了一个简单的递推关系。我们可以将其展开，为 $t \\ge \\tau$ 找到一个闭合形式的表达式：\n$$\n\\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\lambda \\frac{\\partial u_{t-1}}{\\partial x_{j,\\tau}} = \\lambda^2 \\frac{\\partial u_{t-2}}{\\partial x_{j,\\tau}} = \\dots = \\lambda^{t-\\tau} \\frac{\\partial u_{\\tau}}{\\partial x_{j,\\tau}} = \\lambda^{t-\\tau} w_j.\n$$\n现在我们可以组装梯度的最终表达式。对 $t$ 的求和从 $\\tau$ 开始：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = \\sum_{t=\\tau}^{T} \\frac{\\partial \\mathcal{L}}{\\partial u_t} \\frac{\\partial u_t}{\\partial x_{j,\\tau}} = \\sum_{t=\\tau}^{T} \\left[ \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\psi_{\\beta}(u_t - \\vartheta) \\right] \\left( \\lambda^{t-\\tau} w_j \\right).\n$$\n我们可以将不依赖于求和索引 $t$ 的项提取出来：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{j,\\tau}} = w_j \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\left( \\sum_{t=\\tau}^{T} \\lambda^{t-\\tau} \\psi_{\\beta}(u_t - \\vartheta) \\right).\n$$\n这就是损失函数相对于输入分量 $x_{j,\\tau}$ 的梯度的最终闭合形式表达式。",
            "answer": "$$\n\\boxed{w_j \\left( \\sum_{k=1}^{K} \\alpha_k (p_k - y_k) \\right) \\left( \\sum_{t=\\tau}^{T} \\lambda^{t-\\tau} \\psi_{\\beta}(u_t - \\vartheta) \\right)}\n$$"
        },
        {
            "introduction": "代理梯度虽然让攻击成为可能，但它仅仅是真实SNN中不可微动态的一种近似。这个编码练习要求你实现一个完整的对抗性攻击流程，并量化“迁移差距”——即在代理模型上预测的攻击效果与在真实SNN上产生的实际效果之间的差异。这项实践任务不仅让你亲手评估攻击的真实效力，也揭示了SNN安全领域的一个核心挑战 。",
            "id": "4034858",
            "problem": "您的任务是实现一个完整的、可运行的程序，用以量化使用代理梯度制作的对抗性样本与其对脉冲神经网络（Spiking Neural Network, SNN）的真实影响之间的可迁移性差距。实验协议必须经过数学上的明确指定，然后在代码中实例化。目标系统是一个双输出、单层的 Leaky Integrate-and-Fire (LIF) 神经元网络，该网络接收时间序列输入电流。其中，对抗性扰动使用代理梯度模型生成，并在精确脉冲动力学下进行评估。\n\n基本原理和定义：\n- 脉冲神经网络（SNN）是一个神经元动力学系统，其中每个神经元在其膜电位超过阈值时会发放脉冲。每个神经元被建模为一个 Leaky Integrate-and-Fire (LIF) 单元，其在每个时间步 $t$ 的离散时间更新规则如下：\n  - 重置前膜电位为 $v^{\\text{hat}}_{t,k} = \\lambda v_{t,k} + \\sum_{i=1}^{N_{\\text{in}}} W_{k,i} x_{t,i}$，其中 $k$ 是输出神经元索引，$\\lambda \\in (0,1)$ 是泄漏因子，权重矩阵 $W \\in \\mathbb{R}^{N_{\\text{out}} \\times N_{\\text{in}}}$，输入为 $x_t \\in \\mathbb{R}^{N_{\\text{in}}}$。\n  - 精确脉冲 $z_{t,k}$ 为 $z_{t,k} = H(v^{\\text{hat}}_{t,k} - V_{\\text{th}})$，其中 $H(\\cdot)$ 是 Heaviside 阶跃函数，$V_{\\text{th}}$ 是阈值。\n  - 重置后膜电位为 $v_{t+1,k} = v^{\\text{hat}}_{t,k} - z_{t,k} V_{\\text{reset}}$，其中 $V_{\\text{reset}}$ 是脉冲发放时施加的重置减量。\n  - 在 $T$ 个时间步的范围内，输出 $k$ 的脉冲计数为 $S_k = \\sum_{t=0}^{T-1} z_{t,k}$。\n- 分类结果定义为具有最大脉冲计数 $S_k$ 的索引 $k$，即对输出进行 $\\operatorname{argmax}$ 运算。对于真实类别索引 $y$，损失函数是脉冲计数的 softmax 与独热分布之间的交叉熵：$L = -\\log\\left(\\exp(S_y) / \\sum_{j=1}^{N_{\\text{out}}} \\exp(S_j)\\right)$。\n- 代理梯度建模通过一个平滑函数来替代不可微的 $H(\\cdot)$。我们定义代理脉冲为 $z^{\\text{sur}}_{t,k} = \\sigma(\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}}))$，其中 $\\sigma(\\cdot)$ 是 logistic 函数，$\\beta  0$ 是一个斜率参数，用于控制代理转换的陡峭程度。\n- 对抗性扰动是使用在代理动力学下计算的 Fast Gradient Sign Method (FGSM) 生成的。对于扰动预算 $\\epsilon$，输入序列 $x$ 被扰动为 $\\tilde{x}$，其计算方式为 $\\tilde{x}_{t,i} = \\Pi_{[0,1]}(x_{t,i} + \\epsilon \\cdot \\operatorname{sign}(\\nabla_{x_{t,i}} L))$，其中 $\\Pi_{[0,1]}(\\cdot)$ 将元素逐个投影到有效的输入区间 $[0,1]$ 内。\n\n可迁移性差距度量：\n- 攻击后的代理边距定义为 $\\tilde{m}^{\\text{sur}} = \\tilde{S}^{\\text{sur}}_{y} - \\max_{k \\neq y} \\tilde{S}^{\\text{sur}}_{k}$，其中 $\\tilde{S}^{\\text{sur}}_{k}$ 是在对抗性输入 $\\tilde{x}$ 上应用代理动力学得到的代理脉冲计数。\n- 攻击后的精确边距定义为 $\\tilde{m}^{\\text{exact}} = \\tilde{S}^{\\text{exact}}_{y} - \\max_{k \\neq y} \\tilde{S}^{\\text{exact}}_{k}$，其中 $\\tilde{S}^{\\text{exact}}_{k}$ 是在对抗性输入 $\\tilde{x}$ 上应用 Heaviside 动力学得到的精确脉冲计数。\n- 可迁移性差距为 $g = \\tilde{m}^{\\text{sur}} - \\tilde{m}^{\\text{exact}}$。正值的 $g$ 表示相对于精确脉冲动力学，代理动力学高估了攻击效果。\n\n用于可复现性的固定模型和数据：\n- 输入数量为 $N_{\\text{in}} = 4$，输出数量为 $N_{\\text{out}} = 2$。\n- 权重矩阵固定为 $W = \\begin{bmatrix} 0.9  0.9  -0.9  -0.9 \\\\ -0.9  -0.9  0.9  0.9 \\end{bmatrix}$。\n- 泄漏因子为 $\\lambda = 0.9$，阈值为 $V_{\\text{th}} = 1.0$，重置值为 $V_{\\text{reset}} = 1.0$，真实类别索引为 $y = 0$。\n- 干净输入序列在每个时间步都是一个常数向量：对于每个 $t$，$x_t = [0.6, 0.6, 0.4, 0.4]$。\n- 输入被逐元素地约束在 $[0,1]$ 范围内。\n\n您的程序必须：\n- 为指定的 LIF 模型实现精确脉冲动力学和代理动力学。\n- 在代理动力学下，使用脉冲计数作为 logits 计算交叉熵损失，然后使用代理导数随时间反向传播以获得 $\\nabla_x L$。\n- 在代理动力学下，使用预算 $\\epsilon$ 构建一个单步 FGSM 对抗性样本，并将其投影到 $[0,1]$。\n- 在对抗性输入上评估 $\\tilde{m}^{\\text{sur}}$ 和 $\\tilde{m}^{\\text{exact}}$，并报告每个测试用例的可迁移性差距 $g$。\n\n测试套件：\n- 每个测试用例是一个元组 $(\\epsilon,\\beta,T)$，其中 $\\epsilon$ 是 FGSM 预算，$\\beta$ 是代理斜率，而 $T$ 是时间步数。\n- 使用以下测试用例：\n  - $(0.0, 2.0, 10)$ 代表无攻击。\n  - $(0.05, 2.0, 10)$ 代表小预算和中等陡峭度。\n  - $(0.3, 2.0, 10)$ 代表大预算。\n  - $(0.15, 10.0, 10)$ 代表陡峭的代理。\n  - $(0.15, 0.5, 10)$ 代表平滑的代理。\n  - $(0.15, 2.0, 1)$ 代表单步边界条件。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表必须与上述测试套件的顺序相同，并且每个元素都必须是作为浮点数的可迁移性差距 $g$。例如，输出行类似于 $[g_1,g_2,g_3,g_4,g_5,g_6]$，其中每个 $g_i$ 按所列顺序对应一个测试用例。",
            "solution": "此问题是有效的。这是一个定义明确、有科学依据的计算任务，属于神经形态系统的对抗性机器学习领域。所有参数、模型和目标都经过了充分的数学严谨性定义，从而能够得到唯一的解。\n\n此问题的核心是量化用于基于梯度的对抗性攻击的代理模型与脉冲神经网络（SNN）的真实、不可微动力学之间的差异。这种差异被称为“可迁移性差距”。解决方案涉及一个多步骤的计算过程，该过程基于神经动力学、自动微分和对抗性机器学习的原理。\n\n**1. Leaky Integrate-and-Fire (LIF) 神经元动力学**\nSNN 的基本组成部分是 Leaky Integrate-and-Fire (LIF) 神经元。其行为由一组离散时间动力学方程控制。对于在时间步 $t \\in \\{0, ..., T-1\\}$ 的每个输出神经元 $k \\in \\{0, ..., N_{\\text{out}}-1\\}$，膜电位按如下方式演化：\n- 重置前电位 $v^{\\text{hat}}_{t,k}$ 整合了上一步的衰减电位和加权输入：\n$$v^{\\text{hat}}_{t,k} = \\lambda v_{t,k} + \\sum_{i=1}^{N_{\\text{in}}} W_{k,i} x_{t,i}$$\n此处，$\\lambda \\in (0,1)$ 是泄漏因子，$v_{t,k}$ 是在步骤 $t-1$ 发生电位重置后的膜电位（其中 $v_{0,k} = 0$），$W$ 是权重矩阵，$x_t$ 是在时间 $t$ 的输入向量。\n\n- 如果电位超过阈值 $V_{\\text{th}}$，则会发放一个脉冲。这是一个不可微事件，由 Heaviside 阶跃函数 $H(\\cdot)$ 建模：\n$$z_{t,k} = H(v^{\\text{hat}}_{t,k} - V_{\\text{th}})$$\n其中 $z_{t,k} \\in \\{0, 1\\}$。\n\n- 发放脉冲后，电位会减少 $V_{\\text{reset}}$（减法重置机制）：\n$$v_{t+1,k} = v^{\\text{hat}}_{t,k} - z_{t,k} V_{\\text{reset}}$$\n\n- 在时间范围 $T$ 内，神经元 $k$ 的总脉冲计数为 $S_k = \\sum_{t=0}^{T-1} z_{t,k}$。\n\n**2. 代理梯度原理**\n像 Fast Gradient Sign Method (FGSM) 这样的对抗性攻击需要损失函数相对于输入的梯度 $\\nabla_x L$。在精确脉冲生成中的 Heaviside 函数是不可微的，这使得梯度计算无法进行。为了克服这个问题，我们采用一个代理模型，其中 Heaviside 函数被一个平滑近似函数所替代。问题指定使用 logistic 函数 $\\sigma(\\cdot)$：\n$$z^{\\text{sur}}_{t,k} = \\sigma(\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}})) = \\frac{1}{1 + \\exp(-\\beta (v^{\\text{hat}}_{t,k} - V_{\\text{th}}))}$$\n参数 $\\beta  0$ 控制代理函数的陡峭程度，当 $\\beta \\to \\infty$ 时，它更接近 Heaviside 函数。所有后续的动力学过程，包括重置机制和脉冲计数，都使用这个连续值的“代理脉冲”$z^{\\text{sur}}_{t,k}$ 来保持可微性。\n\n**3. 通过随时间反向传播（BPTT）计算梯度**\n为了计算 $\\nabla_{x_t} L$，我们必须通过代理 SNN 展开的时间动力学来对交叉熵损失进行微分。损失为 $L = -\\log\\left(\\exp(S^{\\text{sur}}_y) / \\sum_{j} \\exp(S^{\\text{sur}}_j)\\right)$，其中 $y$ 是真实类别索引。损失相对于代理脉冲计数 $S^{\\text{sur}}_k$ 的梯度是 $\\frac{\\partial L}{\\partial S^{\\text{sur}}_k} = p_k - \\delta_{ky}$，其中 $p_k$ 是 softmax 概率，$\\delta_{ky}$ 是 Kronecker delta。\n\nBPTT 算法将误差信号随时间向后传播。设 $\\delta v^{\\text{hat}}_{t,k} = \\frac{\\partial L}{\\partial v^{\\text{hat}}_{t,k}}$ 和 $\\delta v_{t,k} = \\frac{\\partial L}{\\partial v_{t,k}}$ 为损失相对于电位的梯度。递推关系如下：\n- $\\delta v^{\\text{hat}}_{t,k} = \\left(\\frac{\\partial L}{\\partial S^{\\text{sur}}_k} \\psi_{t,k}\\right) + \\delta v_{t+1,k} (1 - \\psi_{t,k} V_{\\text{reset}})$\n- $\\delta v_{t,k} = \\lambda \\delta v^{\\text{hat}}_{t,k}$\n其中 $\\psi_{t,k} = \\frac{\\partial z^{\\text{sur}}_{t,k}}{\\partial v^{\\text{hat}}_{t,k}} = \\beta \\sigma'(\\beta(v^{\\text{hat}}_{t,k} - V_{\\text{th}}))$。反向传播从 $\\delta v_{T,k} = 0$ 开始。然后通过链式法则找到在时间 $t$ 相对于输入的梯度：\n$$\\nabla_{x_{t}} L = W^T \\delta v^{\\text{hat}}_{t}$$\n\n**4. 对抗性攻击生成（FGSM）**\n利用从代理模型计算出的梯度 $\\nabla_x L$，我们可以使用单步 FGSM 来制作一个对抗性输入 $\\tilde{x}$：\n$$\\tilde{x}_{t,i} = \\Pi_{[0,1]}\\left(x_{t,i} + \\epsilon \\cdot \\text{sign}((\\nabla_x L)_{t,i})\\right)$$\n其中 $\\epsilon$ 是扰动预算，$\\Pi_{[0,1]}(\\cdot)$ 是一个裁剪函数，将输入投影回有效范围 $[0, 1]$ 内。\n\n**5. 评估可迁移性差距**\n最后一步是衡量攻击的有效性。我们计算决策边距，定义为正确类别的脉冲计数减去任何不正确类别的最大脉冲计数。由于 $N_{\\text{out}}=2$ 且 $y=0$，这简化为 $S_0 - S_1$。我们在代理和精确两种动力学下为对抗性输入 $\\tilde{x}$ 计算此边距：\n- 代理边距：$\\tilde{m}^{\\text{sur}} = \\tilde{S}^{\\text{sur}}_{0} - \\tilde{S}^{\\text{sur}}_{1}$\n- 精确边距：$\\tilde{m}^{\\text{exact}} = \\tilde{S}^{\\text{exact}}_{0} - \\tilde{S}^{\\text{exact}}_{1}$\n\n可迁移性差距 $g = \\tilde{m}^{\\text{sur}} - \\tilde{m}^{\\text{exact}}$ 量化了代理模型对攻击成功率的预测与真实 SNN 中实际结果的偏离程度。正值的 $g$ 表示代理模型高估了攻击的有效性。\n\n该实现将为每个测试用例系统地遵循这些步骤，并将动力学和梯度计算封装在专门的函数中。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the transferability gap for adversarial examples in a Spiking Neural Network.\n    \"\"\"\n\n    # --- Fixed model and data for reproducibility ---\n    N_IN = 4\n    N_OUT = 2\n    W = np.array([\n        [0.9, 0.9, -0.9, -0.9],\n        [-0.9, -0.9, 0.9, 0.9]\n    ], dtype=np.float64)\n    LAMBDA = 0.9\n    V_TH = 1.0\n    V_RESET = 1.0\n    Y_TRUE_CLASS = 0\n    X_CLEAN = np.array([0.6, 0.6, 0.4, 0.4], dtype=np.float64)\n    \n    # --- Test suite ---\n    test_cases = [\n        (0.0, 2.0, 10),      # No attack\n        (0.05, 2.0, 10),     # Small budget, moderate sharpness\n        (0.3, 2.0, 10),      # Large budget\n        (0.15, 10.0, 10),    # Steep surrogate\n        (0.15, 0.5, 10),     # Smooth surrogate\n        (0.15, 2.0, 1),      # One-step boundary condition\n    ]\n\n    def sigmoid(x):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def lif_simulation(x_seq, T, beta, mode, store_history=False):\n        \"\"\"\n        Simulates the LIF SNN for a given input sequence.\n        \n        Args:\n            x_seq (np.ndarray): Input sequence of shape (T, N_in).\n            T (int): Number of time steps.\n            beta (float): Surrogate slope parameter.\n            mode (str): 'exact' for Heaviside spikes, 'surrogate' for sigmoid spikes.\n            store_history (bool): If True, returns intermediate states for BPTT.\n\n        Returns:\n            Tuple[np.ndarray, dict]: Spike counts and history dictionary.\n        \"\"\"\n        v_mem = np.zeros(N_OUT, dtype=np.float64)\n        spike_counts = np.zeros(N_OUT, dtype=np.float64)\n        \n        history = {'v_hat_seq': [], 'z_sur_seq': []} if store_history else None\n\n        for t in range(T):\n            # Input current\n            i_in = W @ x_seq[t]\n            \n            # Pre-reset membrane potential\n            v_hat = LAMBDA * v_mem + i_in\n            \n            # Spike generation\n            if mode == 'exact':\n                spikes = (v_hat = V_TH).astype(np.float64)\n            elif mode == 'surrogate':\n                spikes = sigmoid(beta * (v_hat - V_TH))\n            else:\n                raise ValueError(\"Invalid mode specified. Must be 'exact' or 'surrogate'.\")\n\n            # Post-reset membrane potential\n            v_mem = v_hat - spikes * V_RESET\n            \n            spike_counts += spikes\n\n            if store_history:\n                history['v_hat_seq'].append(v_hat)\n                history['z_sur_seq'].append(spikes) # Surrogate spikes\n        \n        return spike_counts, history\n\n    def compute_gradients(history, S_sur, T, beta):\n        \"\"\"\n        Computes gradients of the loss w.r.t. the input sequence using BPTT.\n        \n        Args:\n            history (dict): Stored states from the surrogate forward pass.\n            S_sur (np.ndarray): Total surrogate spike counts.\n            T (int): Number of time steps.\n            beta (float): Surrogate slope parameter.\n\n        Returns:\n            np.ndarray: Gradient tensor of shape (T, N_in).\n        \"\"\"\n        v_hat_seq = history['v_hat_seq']\n        z_sur_seq = history['z_sur_seq']\n\n        # Gradient of loss w.r.t. spike counts (logits)\n        exp_S = np.exp(S_sur - np.max(S_sur)) # Stabilized softmax\n        p = exp_S / np.sum(exp_S)\n        y_one_hot = np.zeros(N_OUT, dtype=np.float64)\n        y_one_hot[Y_TRUE_CLASS] = 1.0\n        dLdS = p - y_one_hot\n        \n        grad_x = np.zeros((T, N_IN), dtype=np.float64)\n        \n        # BPTT initialization\n        delta_v_next = np.zeros(N_OUT, dtype=np.float64)\n\n        for t in reversed(range(T)):\n            # Gradient of surrogate spike function sigma'(...)\n            psi_t = beta * z_sur_seq[t] * (1.0 - z_sur_seq[t])\n            \n            # Propagate error back to pre-reset potential\n            term1 = dLdS * psi_t\n            term2 = delta_v_next * (1.0 - psi_t * V_RESET)\n            delta_v_hat_t = term1 + term2\n            \n            # Gradient w.r.t. input x_t\n            grad_x[t] = W.T @ delta_v_hat_t\n            \n            # Propagate error to previous membrane potential for next BPTT step\n            delta_v_next = LAMBDA * delta_v_hat_t\n            \n        return grad_x\n\n    results = []\n    for epsilon, beta, T in test_cases:\n        x_clean_seq = np.tile(X_CLEAN, (T, 1))\n\n        if epsilon == 0.0:\n            x_adv = x_clean_seq\n        else:\n            # 1. Run surrogate forward pass on clean input to get history for BPTT\n            S_sur_clean, history = lif_simulation(x_clean_seq, T, beta, 'surrogate', store_history=True)\n\n            # 2. Compute gradients via BPTT\n            grad_x = compute_gradients(history, S_sur_clean, T, beta)\n\n            # 3. Construct adversarial example using FGSM\n            x_adv = x_clean_seq + epsilon * np.sign(grad_x)\n            x_adv = np.clip(x_adv, 0.0, 1.0)\n\n        # 4. Evaluate margins on the adversarial example\n        # Surrogate margin\n        S_sur_adv, _ = lif_simulation(x_adv, T, beta, 'surrogate')\n        m_sur_adv = S_sur_adv[Y_TRUE_CLASS] - S_sur_adv[1 - Y_TRUE_CLASS]\n\n        # Exact margin\n        S_exact_adv, _ = lif_simulation(x_adv, T, beta, 'exact')\n        m_exact_adv = S_exact_adv[Y_TRUE_CLASS] - S_exact_adv[1 - Y_TRUE_CLASS]\n\n        # 5. Compute the transferability gap\n        gap = m_sur_adv - m_exact_adv\n        results.append(gap)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "构建鲁棒的神经网络需要深入理解不同架构组件如何响应攻击。本问题通过一个思想实验，让你对比最大池化（max pooling）与平均池化（average pooling）在面对稀疏脉冲注入攻击时的表现。通过分析攻击对每种池化操作的影响，你将建立起关于局部计算选择如何对网络整体鲁棒性产生深远影响的直观认识 。",
            "id": "4034876",
            "problem": "一个脉冲神经网络 (SNN) 层在固定的观察窗口 $t \\in [0,T]$ 内产生突触前脉冲序列 $\\{s_i(t)\\}_{i=1}^n$，其中每个 $s_i(t)$ 是代表脉冲的狄拉克脉冲之和。定义单元 $i$ 的脉冲计数 $c_i$ 为 $c_i = \\int_0^T s_i(t)\\,\\mathrm{d}t$，因此每个脉冲对 $c_i$ 的贡献为 $1$。考虑一个大小为 $n$ 的池化区域，以及应用于 $\\{c_i\\}_{i=1}^n$ 的两种 ($2$) 脉冲形式的池化函数：\n- 最大池化：$y_{\\max} = \\max_{1 \\le i \\le n} c_i$。\n- 平均池化：$y_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n c_i$。\n\n假设一个最坏情况下的对手，他可以通过向计数中添加一个非负整数向量 $a \\in \\mathbb{Z}_{\\ge 0}^n$ 来插入稀疏事件，从而产生受扰动的计数 $c_i' = c_i + a_i$，并受限于 $\\ell_1$ 预算 $\\|a\\|_1 = \\sum_{i=1}^n a_i \\le s$，其中 $s$ 是在 $[0,T]$ 期间在整个池化区域插入的脉冲总数。在窗口内，$c_i$ 没有上限饱和。\n\n下游决策阶段将池化值 $y$ 与一个固定阈值 $\\theta$ 进行比较：如果 $y \\ge \\theta$，则宣告检测事件发生。对于一个未受扰动的池化值 $y$，其裕度为 $\\Delta = \\theta - y  0$，对手试图通过最少的脉冲插入将 $y$ 提升到至少为 $\\theta$。在此模型和所述定义下，哪个选项最能描述在单个池化区域内，最大池化与平均池化相比对稀疏事件插入的相对对抗鲁棒性？\n\nA. 在最坏情况下，平均池化需要比最大池化多至少 $n$ 倍的插入脉冲才能越过相同的下游阈值，这意味着平均池化对稀疏插入更鲁棒。\n\nB. 最大池化更鲁棒，因为它丢弃了除最大计数之外的所有计数；稀疏的对手无法在不影响大多数单元的情况下显著改变最大计数，因此需要更多的插入脉冲。\n\nC. 最大池化和平均池化在脉冲插入下的鲁棒性相同，因为两种池化函数在脉冲计数上都是单调的。\n\nD. 对稀疏脉冲插入的鲁棒性仅由突触后泄露和不应期动力学决定，而与使用最大池化还是平均池化无关。",
            "solution": "对问题陈述进行严格评估后，认定其是有效的。它在简化的神经网络模型的标准理论框架内具有科学依据，问题设定良好，目标明确，语言客观，并且内部一致。所有进行形式化分析所必需的定义都已提供。\n\n该问题要求在脉冲神经网络 (SNN) 的背景下，比较最大池化与平均池化的对抗鲁棒性。鲁棒性由导致错误分类（即，使池化输出达到阈值 $\\theta$）所需的最少对抗性插入脉冲数来隐含地量化。\n\n设大小为 $n$ 的池化区域中的初始脉冲计数为向量 $c = (c_1, c_2, \\dots, c_n)$，其中 $c_i \\in \\mathbb{Z}_{\\ge 0}$ 是整数。对手添加一个扰动向量 $a = (a_1, a_2, \\dots, a_n)$，其中 $a_i \\in \\mathbb{Z}_{\\ge 0}$，从而得到受扰动的计数 $c' = c + a$。插入的脉冲总数是 $a$ 的 $\\ell_1$ 范数，记为 $S = \\|a\\|_1 = \\sum_{i=1}^n a_i$。\n\n初始状态下，池化输出 $y$ 低于阈值 $\\theta$。也就是说，存在一个裕度 $\\Delta = \\theta - y  0$。对手的目标是找到最小的整数 $S$，使得新的池化输出 $y'$ 满足 $y' \\ge \\theta$。为了使分析易于处理且与问题中基于整数的定义（$c_i, a_i$）一致，我们假设阈值 $\\theta$ 也是一个整数。\n\n**情况 1：最大池化**\n\n输出由 $y_{\\max} = \\max_{1 \\le i \\le n} c_i$ 给出。初始状态满足 $y_{\\max}  \\theta$。\n受扰动的输出为 $y'_{\\max} = \\max_{1 \\le i \\le n} (c_i + a_i)$。\n对手的目标是实现 $y'_{\\max} \\ge \\theta$。这要求至少对于一个神经元 $j$，受扰动的计数满足 $c_j + a_j \\ge \\theta$。\n\n为了最小化插入脉冲的总数 $S = \\sum a_i$，对手应采用最高效的策略。这包括识别出需要最少额外脉冲才能达到阈值的神经元，并将所有扰动集中在该单个神经元上。将神经元 $j$ 推至阈值所需的脉冲数为 $a_j = \\theta - c_j$（因为 $c_j  \\theta$，并且所有量都是整数）。为了最小化此成本，对手应选择具有最高初始脉冲计数的神经元 $k$，即 $c_k = \\max_{i} c_i = y_{\\max}$。\n\n通过设置 $a_k = \\theta - c_k$ 和 $a_i = 0$ (对于所有 $i \\ne k$)，对手以可能的最小总脉冲数实现目标。设这个最小值为 $S_{\\max}^{\\min}$。\n$$S_{\\max}^{\\min} = \\theta - c_k = \\theta - \\max_{1 \\le i \\le n} c_i = \\theta - y_{\\max}$$\n\n**情况 2：平均池化**\n\n输出由 $y_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n c_i$ 给出。初始状态满足 $y_{\\mathrm{avg}}  \\theta$。\n受扰动的输出为 $y'_{\\mathrm{avg}} = \\frac{1}{n} \\sum_{i=1}^n (c_i + a_i)$。\n对手的目标是实现 $y'_{\\mathrm{avg}} \\ge \\theta$。\n$$ \\frac{1}{n} \\sum_{i=1}^n (c_i + a_i) \\ge \\theta $$\n整理不等式：\n$$ \\sum_{i=1}^n c_i + \\sum_{i=1}^n a_i \\ge n \\theta $$\n$$ \\sum_{i=1}^n a_i \\ge n \\theta - \\sum_{i=1}^n c_i $$\n左边的量是脉冲总数 $S$。右边的量 $n \\theta - \\sum c_i$ 是一个整数，因为 $n$、$\\theta$ 和所有的 $c_i$ 都是整数。满足此不等式的最小整数 $S$ 就是右边的值。设这个最小值为 $S_{\\mathrm{avg}}^{\\min}$。\n$$ S_{\\mathrm{avg}}^{\\min} = n \\theta - \\sum_{i=1}^n c_i $$\n这也可以用初始输出 $y_{\\mathrm{avg}}$ 来表示：\n$$ S_{\\mathrm{avg}}^{\\min} = n(\\theta - \\frac{1}{n}\\sum_{i=1}^n c_i) = n(\\theta - y_{\\mathrm{avg}}) $$\n\n**鲁棒性比较**\n\n为了比较鲁棒性，我们比较每种情况下所需的最小脉冲数，$S_{\\max}^{\\min}$ 和 $S_{\\mathrm{avg}}^{\\min}$。我们将评估比率 $S_{\\mathrm{avg}}^{\\min} / S_{\\max}^{\\min}$。\n$$ \\frac{S_{\\mathrm{avg}}^{\\min}}{S_{\\max}^{\\min}} = \\frac{n \\theta - \\sum_{i=1}^n c_i}{\\theta - \\max_{1 \\le i \\le n} c_i} $$\n对于任何一组非负数 $\\{c_i\\}_{i=1}^n$，一个基本性质是其和小于或等于 $n$ 乘以其最大值：\n$$ \\sum_{i=1}^n c_i \\le n \\cdot \\max_{1 \\le i \\le n} c_i $$\n乘以 $-1$ 会反转不等号：\n$$ -\\sum_{i=1}^n c_i \\ge -n \\cdot \\max_{1 \\le i \\le n} c_i $$\n两边都加上 $n\\theta$ 得：\n$$ n \\theta - \\sum_{i=1}^n c_i \\ge n \\theta - n \\cdot \\max_{1 \\le i \\le n} c_i $$\n$$ n \\theta - \\sum_{i=1}^n c_i \\ge n \\left(\\theta - \\max_{1 \\le i \\le n} c_i\\right) $$\n代入最小脉冲数的表达式：\n$$ S_{\\mathrm{avg}}^{\\min} \\ge n \\cdot S_{\\max}^{\\min} $$\n由于我们有一个初始裕度 $\\Delta  0$，所以 $S_{\\max}^{\\min} = \\theta - y_{\\max} \\ge 1$，因此该比较是有意义的。等式 $S_{\\mathrm{avg}}^{\\min} = n \\cdot S_{\\max}^{\\min}$ 成立当且仅当 $\\sum c_i = n \\cdot \\max c_i$，这发生于当且仅当所有 $c_i$ 都相等时。如果计数 $c_i$ 不均匀，则不等式为严格不等式：$S_{\\mathrm{avg}}^{\\min}  n \\cdot S_{\\max}^{\\min}$。\n\n这个结果表明，与最大池化相比，平均池化需要至少 $n$ 倍的脉冲才能被这种类型的攻击攻破。因此，平均池化的鲁棒性要显著更高。\n\n**逐项分析**\n\nA. 在最坏情况下，平均池化需要比最大池化多至少 $n$ 倍的插入脉冲才能越过相同的下游阈值，这意味着平均池化对稀疏插入更鲁棒。\n- 这个陈述准确地反映了我们推导出的不等式 $S_{\\mathrm{avg}}^{\\min} \\ge n \\cdot S_{\\max}^{\\min}$。平均池化更鲁棒的结论直接源于这个数学事实，因为鲁棒性是通过导致失败所需的扰动大小来衡量的。\n- **结论：正确。**\n\nB. 最大池化更鲁棒，因为它丢弃了除最大计数之外的所有计数；稀疏的对手无法在不影响大多数单元的情况下显著改变最大计数，因此需要更多的插入脉冲。\n- “最大池化更鲁棒”的结论与我们的发现相反。其给出的理由是有缺陷的：稀疏对手通过*僅*針對具有最大计数的单个神经元进行攻击，可以非常有效地对抗最大池化，这是一种最高效的单点攻击。它不需要影响大多数单元。\n- **结论：错误。**\n\nC. 最大池化和平均池化在脉冲插入下的鲁棒性相同，因为两种池化函数在脉冲计数上都是单调的。\n- 尽管两种函数确实都是单调不减的，但仅此属性并不能决定鲁棒性的程度。输出对输入变化的敏感度，即与导数（或离散差分）相关的量，才是关键。对于最大池化，单个脉冲可以使输出增加 $1$，而对于平均池化，单个脉冲仅使输出增加 $1/n$。它们的鲁棒性显然不相等。\n- **结论：错误。**\n\nD. 对稀疏脉冲插入的鲁棒性仅由突触后泄露和不应期动力学决定，而与使用最大池化还是平均池化无关。\n- 这个陈述引入了问题明确定义的数学模型之外的概念（泄露、不应期动力学）。分析必须在给定的框架内进行。我们的推导表明，在此模型中，池化函数的选择是决定鲁棒性的决定性因素。\n- **结论：错误。**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}