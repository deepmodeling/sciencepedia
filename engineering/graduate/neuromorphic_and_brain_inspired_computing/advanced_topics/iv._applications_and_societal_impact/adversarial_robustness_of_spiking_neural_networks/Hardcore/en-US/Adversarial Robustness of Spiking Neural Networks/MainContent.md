## Introduction
Spiking Neural Networks (SNNs) represent a paradigm shift in artificial intelligence, drawing inspiration from the brain's energy-efficient, event-driven processing. As these [neuromorphic systems](@entry_id:1128645) move from research labs to real-world applications in robotics, [autonomous systems](@entry_id:173841), and [edge computing](@entry_id:1124150), ensuring their reliability and security becomes paramount. However, like their conventional counterparts, SNNs are susceptible to [adversarial attacks](@entry_id:635501)—subtle, malicious inputs designed to cause misclassification or system failure. The unique spatio-temporal nature of SNNs introduces a new set of vulnerabilities and challenges that demand a specialized understanding. This article addresses this critical knowledge gap by providing a deep dive into the [adversarial robustness](@entry_id:636207) of SNNs.

This article will guide you through the intricate landscape of SNN security across three comprehensive chapters. In "Principles and Mechanisms," we will lay the groundwork by dissecting adversarial threat models, identifying the sources of vulnerability from the single neuron to the network level, and establishing formal methods for certifying robustness. Next, "Applications and Interdisciplinary Connections" will explore these principles in action, examining physical attacks on neuromorphic sensors and revealing profound connections to control theory, Bayesian statistics, and causal inference. Finally, "Hands-On Practices" will provide you with the opportunity to apply these concepts through targeted exercises, solidifying your understanding of how to analyze, attack, and defend these complex systems.

## Principles and Mechanisms

This chapter delves into the foundational principles and mechanisms governing the [adversarial robustness](@entry_id:636207) of Spiking Neural Networks (SNNs). We will dissect the nature of adversarial threats in the context of spatio-temporal event data, explore the intrinsic sources of vulnerability within SNN architectures and learning paradigms, establish formal methods for quantifying and certifying robustness, and examine mechanisms for enhancing [network resilience](@entry_id:265763).

### The Adversarial Landscape in Spiking Systems

To understand robustness, we must first rigorously define the adversary. In the context of SNNs, an adversary's goal is to introduce subtle, often imperceptible perturbations to an input spike train to induce a misclassification or other system failure. The capabilities and knowledge of such an adversary are formalized through threat models.

#### Threat Models: White-Box, Gray-Box, and Black-Box

The effectiveness and nature of an attack are fundamentally determined by the information available to the adversary. This knowledge is stratified into three canonical threat models. Let us consider an SNN represented by a function $f_{\theta}$ with parameters $\theta$ (weights, biases, thresholds, etc.), which maps an input spike train $\mathbf{S}^{\mathrm{in}}$ to an output spike train $\mathbf{S}^{\mathrm{out}}$. The adversary's aim is to find a perturbation $\Delta \mathbf{S}^{\mathrm{in}}$ that maximizes a task loss $L$, subject to a [budget constraint](@entry_id:146950).

The **white-box** model represents the worst-case scenario from a defender's perspective. Here, the adversary possesses complete knowledge of the target system. This includes the [network architecture](@entry_id:268981) $\mathcal{A}$, the full set of trained parameters $\theta$, the exact loss function $L$ used for training, and often the training data distribution $\mathcal{D}$. With this "glass box" view, the adversary can observe all internal network states, such as the event streams at every layer. The most significant capability this knowledge grants is the ability to compute gradients of the loss function with respect to the input, $\nabla_{\mathbf{S}^{\mathrm{in}}} L$. For SNNs trained with backpropagation, this involves leveraging the same surrogate gradient techniques used during training to enable powerful, optimization-based attacks .

The **black-box** model represents the other extreme. The adversary has no knowledge of the model's internal workings. Access is restricted to a query oracle: the attacker can submit inputs to the model $f_{\theta}$ and observe the corresponding outputs (e.g., the final classification label or output spike train). The architecture, parameters, loss function, and internal activities are all opaque. In this setting, gradient-based attacks are impossible. Instead, the adversary must resort to zeroth-order [optimization methods](@entry_id:164468), which estimate gradients through numerous queries (e.g., using [finite differences](@entry_id:167874)), or to transfer attacks, where an attack is crafted on a separate, locally trained surrogate model and then applied to the target model in the hope that the vulnerability "transfers" .

Between these two extremes lies the **gray-box** model, which encompasses a wide spectrum of partial-knowledge scenarios. A common and practical gray-box assumption is that the adversary knows the model architecture $\mathcal{A}$ but not the specific trained parameters $\theta$. Lacking the true parameters, the attacker cannot compute the true gradient $\nabla_{\mathbf{S}^{\mathrm{in}}} L$. However, by training their own surrogate model with the same architecture on a proxy dataset, they can compute a proxy gradient and craft a transfer attack. This approach is often far more efficient than black-box query-based methods .

#### Perturbation Taxonomies and Physical Plausibility

Adversarial perturbations in SNNs can be categorized by how they manipulate the input spike train. These manipulations can occur in the continuous domain or the discrete event domain. The primary types include:

1.  **Additive Current Perturbations**: These attacks modify the continuous-time current $I(t)$ injected into the input neurons, transforming it to $I(t) + \delta I(t)$. This is analogous to traditional adversarial attacks on ANNs that perturb continuous-valued inputs.

2.  **Spike Insertion or Deletion**: These attacks directly alter the discrete set of event times, transforming a spike train $\mathcal{S}$ into a new train $\mathcal{S}'$ by adding or removing spikes.

3.  **Spike-Time Jitter**: This attack shifts the timing of existing spikes, $t_k \mapsto t_k + \delta t_k$, without changing the total number of spikes.

While mathematically simple, a critical consideration for these attacks, especially in [neuromorphic systems](@entry_id:1128645) processing real-world sensor data, is their **physical plausibility**. An attack is physically plausible only if the perturbed spike train could have been realistically produced by the sensor hardware under some physically realizable stimulus.

Consider an event camera, such as a Dynamic Vision Sensor (DVS), which generates spikes in response to logarithmic changes in pixel intensity. Such a device has inherent physical constraints, including a finite analog front-end bandwidth $B$, a per-pixel refractory period $t_{\mathrm{ref}}$, and a maximum event rate $r_{\mathrm{max}}$. A physically plausible perturbation must respect these constraints .

For an **additive current perturbation** to be physically plausible, the perturbation $\delta I(t)$ must itself be a signal that the sensor's analog frontend could produce. This means the signal must be bandlimited, with negligible energy above the sensor's bandwidth $B$. Arbitrarily high-frequency current injections are not physically realizable . If the attack is generated at the stimulus level (i.e., by manipulating the light entering the sensor), the change in [luminance](@entry_id:174173) must be sufficient to trigger the sensor's internal threshold, and the dynamics of this [luminance](@entry_id:174173) change must be physically possible.

For **spike insertion**, a new spike cannot be added at a pixel if it violates that pixel's refractory period $t_{\mathrm{ref}}$. Furthermore, a physically generated spike must correspond to an actual change in the stimulus (e.g., a [luminance](@entry_id:174173) change) that satisfies the sensor's event-generation rule, including the correct event polarity. Similarly, **spike [deletion](@entry_id:149110)** is physically achieved by manipulating the stimulus (e.g., via occlusion or attenuation) to prevent a threshold crossing that would have otherwise occurred .

Finally, **spike-time jitter** is constrained by causality and [sensor dynamics](@entry_id:263688). Shifting a spike to be significantly earlier implies an unnaturally fast-rising signal in the physical world, which may require frequency components that exceed the sensor's bandwidth $B$. Jittered spikes must also respect refractory constraints, ensuring that the new time of a spike does not violate the minimum interval from its predecessor at the same pixel . These considerations are paramount when evaluating the true threat posed by [adversarial attacks](@entry_id:635501) to SNNs deployed in real-world applications.

### Sources of Vulnerability in SNNs

The susceptibility of an SNN to adversarial attacks is not an emergent fluke but is rooted in the fundamental properties of its components and the way it represents and processes information. We now dissect these sources of vulnerability, from the single neuron to the network level.

#### Single-Neuron Dynamics

The robustness of an SNN begins with its most basic processing unit: the neuron. Let us analyze the widely used Leaky Integrate-and-Fire (LIF) model. Its subthreshold membrane potential $V(t)$ is governed by the dynamics:
$$ \tau_m \dot V(t) = -(V(t) - V_{rest}) + R I(t) $$
A spike is fired when $V(t)$ reaches a threshold $V_{th}$, followed by a reset to $V_{reset}$ and a refractory period $\tau_{ref}$. Each of these parameters plays a distinct role in shaping the neuron's susceptibility to an adversarial current perturbation $\delta I(t)$ .

The **[membrane time constant](@entry_id:168069) $\tau_m$** defines the neuron's integration window. It acts as a low-pass filter on the input current. A larger $\tau_m$ leads to slower integration and stronger filtering of high-frequency noise. This decreases the neuron's susceptibility to transient, high-energy ($L_2$-bounded) adversarial current pulses, as the maximal voltage deviation scales with $1/\sqrt{\tau_m}$. For persistent, low-amplitude ($L_\infty$-bounded) perturbations, a larger $\tau_m$ also reduces susceptibility for short durations by slowing the voltage response .

The **[membrane resistance](@entry_id:174729) $R$** acts as a gain factor, directly scaling the input current's effect on the membrane potential. The voltage deviation caused by a perturbation $\delta I(t)$ is directly proportional to $R$. Consequently, decreasing $R$ makes the neuron less responsive to input currents, which improves [adversarial robustness](@entry_id:636207) but comes at the cost of reduced sensitivity to legitimate inputs .

The voltage gap between the **reset potential $V_{reset}$** and the **threshold $V_{th}$** determines the amount of charge that must be integrated to fire a spike. Raising $V_{th}$ or lowering $V_{reset}$ increases this gap, requiring a larger or more sustained adversarial perturbation to induce a spurious spike. Conversely, raising $V_{reset}$ closer to $V_{th}$ makes the neuron more excitable and thus more vulnerable to subsequent perturbations shortly after firing .

Finally, the **refractory period $\tau_{ref}$** imposes a hard limit on the neuron's maximum firing rate. By increasing $\tau_{ref}$, we increase the minimum time between spikes. This directly reduces the maximum number of adversarial spikes that can be induced in a given time window, thereby acting as a powerful constraint on rate-based attacks .

#### Neural Coding and the Threat Surface

Moving from a single neuron to a population, the way information is encoded in spike trains profoundly shapes the network's vulnerability. Different coding schemes have different invariances, which an adversary can exploit.

In **[rate coding](@entry_id:148880)**, information is conveyed by the number of spikes $N_i$ from neuron $i$ within a time window $W$. The feature is proportional to the spike count, $r_i(W) = N_i(W)/|W|$. This code is, by definition, insensitive to the precise timing of spikes within the window. An adversary employing spike-time jitter would have no effect on a pure rate-coded representation. To be effective, an attack must alter the spike count, which requires either inserting new spikes or deleting existing ones .

In **temporal coding**, the precise timing of spikes carries information. A common example is [latency coding](@entry_id:1127087), where the feature is the time of the first spike, $\ell_i(W)$. This scheme is exquisitely sensitive to any perturbation that can advance or delay spike times, such as small current injections or spike-time jitter. Unlike rate coding, an adversary does not need to add or remove spikes; subtle timing shifts are sufficient. However, this code is often insensitive to any spikes occurring after the first one .

In **[rank-order coding](@entry_id:1130566)**, the information lies in the relative firing order of neurons, determined by their first-spike latencies. The feature is the permutation $\pi(W)$ of neuron indices sorted by latency. This code is invariant to any perturbation that affects all neurons equally, such as a uniform delay that shifts all spike times by a constant amount. It is also insensitive to the [absolute magnitude](@entry_id:157959) of the latencies. To be effective, an adversary must induce an "order inversion" by differentially shifting spike times, causing a neuron that fired later to fire earlier than another, and vice-versa .

Understanding the coding scheme used by an SNN is therefore essential for assessing its threat surface—the set of perturbations to which it is most vulnerable.

#### Methodological Vulnerabilities: Surrogate Gradients and ANN-SNN Conversion

Vulnerabilities also arise from the specific methods used to design and train SNNs.

A central challenge in training SNNs is the non-differentiable nature of the [spike generation](@entry_id:1132149) mechanism, typically modeled by a Heaviside [step function](@entry_id:158924). The most common solution is **surrogate gradient training**, where the hard-[threshold function](@entry_id:272436) is used in the forward pass to maintain the network's spiking nature, but its derivative is replaced with a smooth, differentiable proxy $\tilde{\sigma}'(u)$ in the [backward pass](@entry_id:199535). This introduces a fundamental **gradient mismatch**: the gradients used for training (and for crafting white-box attacks) do not correspond to the true gradients of the network, which are zero almost everywhere. The surrogate gradient is non-zero in a neighborhood around the firing threshold, creating a "phantom" sensitivity that suggests a small input change will produce a smooth change in the loss. An attack crafted using this phantom gradient may fail on the true network if the perturbation is not large enough to actually push a neuron's potential across the threshold. This mismatch complicates the generation of effective gradient-based attacks .

Another common practice is **ANN-to-SNN conversion**, where a pre-trained Artificial Neural Network (ANN) is mapped to an SNN. This raises the question of whether [adversarial examples](@entry_id:636615) transfer from the source ANN to the converted SNN. The transferability is not guaranteed and depends on the specifics of the conversion. Consider a ReLU-based ANN converted to a rate-coded SNN, where the input current to an SNN neuron is proportional to the corresponding ANN activation. The SNN's output (spike count) is an integer, resulting from a quantization of the input current over time: $N = \lfloor K \cdot \max\{0, z\} \rfloor$, where $z$ is the ANN logit and $K$ is a conversion constant. This quantization can make the SNN robust to small ANN perturbations that change $z$ but are not large enough to change the floor of $Kz$. However, if an attack on the ANN is powerful enough to flip the sign of the logit from positive to negative, the ReLU ensures the input current to the SNN drops to zero, causing the spike count to become zero. This type of attack is highly likely to transfer and cause a misclassification in the SNN .

### Formalizing and Certifying Robustness

To move beyond qualitative descriptions of vulnerability, we need a mathematical framework to measure perturbations and provide formal guarantees of robustness.

#### Spike Train Metrics

A prerequisite for formal analysis is a metric to quantify the "distance" between two spike trains, which serves as the budget for an adversary. While a simple Euclidean distance on a vectorized spike train is possible, it is not well-suited to the event-based nature of the data. More appropriate are metrics designed specifically for point processes.

The **van Rossum distance** is a powerful metric derived from a plausible biophysical model. It conceives of each spike as triggering a post-synaptic potential (PSP), typically modeled as a decaying exponential kernel. Two spike trains are first converted into continuous signals by convolving them with this kernel. The squared van Rossum distance is then defined as the squared $L^2$ distance between these two continuous signals. The time constant $\tau$ of the kernel is a crucial parameter: a small $\tau$ makes the metric highly sensitive to fine [temporal jitter](@entry_id:1132926), while a large $\tau$ makes it approximate a comparison of spike counts ([rate coding](@entry_id:148880)). Its mathematical form is continuous and [almost everywhere differentiable](@entry_id:200712) with respect to spike times, making it suitable for gradient-based analysis of temporal perturbations .

Another important family of metrics is based on edit distances. The **Victor-Purpura distance**, for instance, defines the distance between two spike trains as the minimum cost to transform one into the other using a set of allowed operations: inserting a spike (cost 1), deleting a spike (cost 1), and shifting a spike in time by $\Delta t$ (cost $\lambda |\Delta t|$). The parameter $\lambda$ controls the relative cost of timing versus spike count changes, providing a tuneable way to define the adversarial threat model .

#### Lipschitz Continuity and Certified Robustness

With a well-defined metric $d_X$ for the input space of spike trains, we can analyze the robustness of an SNN mapping $f$ through the lens of **Lipschitz continuity**. A function $f$ is $L$-Lipschitz if the distance between any two outputs is bounded by a constant $L$ times the distance between the corresponding inputs: $d_Y(f(x), f(x')) \le L \cdot d_X(x, x')$. The Lipschitz constant $L = \sup_{x \neq x'} \frac{d_Y(f(x), f(x'))}{d_X(x, x')}$ is the maximum amplification factor of the network from input perturbations to output perturbations.

The primary utility of the Lipschitz constant is in providing **[certified robustness](@entry_id:637376)**. If we can compute or bound the Lipschitz constant $L$ of an SNN, we can provide a formal guarantee that the network's prediction will not change for any perturbation within a certain radius. For a classification task, let the output metric $d_Y$ be the $\ell_{\infty}$ norm on the logit vector, and define the [classification margin](@entry_id:634496) $m(x)$ as the difference between the logit of the predicted class and the highest logit of any other class. It can be shown that the predicted class is guaranteed to remain unchanged for any perturbation $x'$ satisfying $d_X(x, x') \le \epsilon$ as long as the budget $\epsilon$ is less than a certified radius given by $\epsilon_{\text{cert}} = \frac{m(x)}{2L}$. This provides a worst-case guarantee against *any* possible adversarial attack within the budget $\epsilon$, a much stronger statement than empirical robustness against specific attacks .

### Mechanisms for Enhancing Robustness

Given the multifaceted nature of SNN vulnerabilities, a range of defense strategies have been developed, from principled [optimization techniques](@entry_id:635438) to the leveraging of bio-plausible intrinsic dynamics.

#### Adversarial Training

The most effective and widely studied defense against adversarial attacks is **[adversarial training](@entry_id:635216)**. Instead of training the network only on clean data, this method augments the training process to include [adversarial examples](@entry_id:636615). The objective is formalized as a **[minimax optimization](@entry_id:195173) problem**. The standard [empirical risk minimization](@entry_id:633880) objective, $\min_{\theta} \sum L(f_{\theta}(S_i), y_i)$, is replaced with a [robust optimization](@entry_id:163807) objective:
$$ \min_{\theta} \frac{1}{N} \sum_{i=1}^{N} \max_{S' : \, d(S', S_i) \le \epsilon} \, L\big(f_{\theta}(S'), y_i\big) $$
This objective seeks to find model parameters $\theta$ that minimize the average loss on the worst-case [adversarial examples](@entry_id:636615). The inner maximization problem corresponds to the adversary's goal: for a given model $\theta$ and a clean input $S_i$, find the perturbed input $S'$ within the budget $\epsilon$ (as defined by a spike train metric like Victor-Purpura) that maximizes the loss. The outer minimization then adjusts the model parameters to reduce the loss on these hard examples. This procedure explicitly forces the network to learn representations that are resilient to the specific class of perturbations defined by the threat model .

#### Bio-Plausible Plasticity and Intrinsic Stability

Beyond explicit optimization, the inherent dynamics and learning rules of biological neural systems can offer insights into building more robust SNNs.

**Spike-Timing-Dependent Plasticity (STDP)** is a Hebbian learning rule where synaptic weights are adjusted based on the precise relative timing of pre- and post-synaptic spikes. While a powerful [unsupervised learning](@entry_id:160566) mechanism, its very sensitivity to spike timing makes it a point of vulnerability. A timing-based adversary can manipulate STDP updates during training by introducing small input jitters, which propagate through the network and alter pre-post spike time differences, potentially hijacking the learning process to maliciously strengthen or weaken connections .

However, biological systems couple fast Hebbian learning with slower regulatory mechanisms, collectively known as **homeostatic plasticity**. These mechanisms, such as synaptic scaling (which multiplicatively adjusts all of a neuron's incoming weights) or [intrinsic excitability](@entry_id:911916) adaptation (which adjusts firing thresholds), work to maintain a stable, target level of activity. From a robustness perspective, [homeostasis](@entry_id:142720) acts as a crucial stabilizing force. By preventing neurons from drifting into pathological regimes of hyper- or hypo-activity, it constrains the operating points of the network. This dampens the amplification of timing perturbations and reduces extreme sensitivities, thereby providing an intrinsic source of robustness against adversarial manipulation, both during and after training . Integrating such bio-plausible stabilization mechanisms may be a key pathway toward developing SNNs that are not just accurate, but also inherently resilient.