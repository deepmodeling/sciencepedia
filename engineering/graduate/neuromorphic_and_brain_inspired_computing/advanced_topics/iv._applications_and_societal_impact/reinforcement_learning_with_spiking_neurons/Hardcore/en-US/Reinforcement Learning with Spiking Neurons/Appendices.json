{
    "hands_on_practices": [
        {
            "introduction": "To build intelligent spiking agents, we must first understand how a single neuron can represent a continuous value, such as the intensity of an action. This practice explores the foundational Leaky Integrate-and-Fire (LIF) model, the workhorse of computational neuroscience. By solving the membrane potential dynamics under a constant input current , you will derive the relationship between input strength and a neuron's output firing rate, providing a concrete basis for how actions can be encoded in spike trains.",
            "id": "4057344",
            "problem": "In a policy network for Reinforcement Learning (RL) implemented with spiking neurons, an action channel is emitted by a single Leaky Integrate-and-Fire (LIF) neuron whose inter-spike interval (ISI) encodes the action intensity. The membrane potential $V(t)$ follows the standard LIF dynamics with membrane time constant $\\tau_m$ and membrane resistance $R$ under a constant injected current $I(t)=I_0$, namely\n$$\n\\tau_m \\frac{dV}{dt} = - V + R I_0.\n$$\nA spike is emitted when $V(t)$ reaches the threshold $\\vartheta$, after which the membrane potential is instantaneously reset to $V_\\mathrm{reset}$. Assume no refractory period or additional synaptic dynamics. Immediately after a spike at $t=0$, the membrane potential satisfies $V(0^+) = V_\\mathrm{reset}$. Assume the parameters satisfy $V_\\mathrm{reset}  \\vartheta$ and $R I_0  \\vartheta$ so that a subsequent spike occurs in finite time.\n\nStarting from the given differential equation and these conditions, derive from first principles a closed-form analytic expression for the inter-spike interval $T_{\\mathrm{ISI}}$ (the time from reset at $t=0$ until the next threshold crossing), as a function of $\\tau_m$, $R$, $I_0$, $\\vartheta$, and $V_\\mathrm{reset}$. Express your final answer in seconds as a single closed-form expression. No numerical evaluation or rounding is required.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the established Leaky Integrate-and-Fire (LIF) neuron model, is mathematically well-posed with a specific ordinary differential equation and sufficient initial conditions, and is expressed in objective, formal language. We may, therefore, proceed directly to the derivation.\n\nThe dynamics of the membrane potential $V(t)$ are governed by the differential equation:\n$$\n\\tau_m \\frac{dV}{dt} = - V + R I_0\n$$\nThis is a first-order linear non-homogeneous ordinary differential equation. We can rearrange it into the standard form $\\frac{dV}{dt} + p(t)V = q(t)$:\n$$\n\\frac{dV}{dt} + \\frac{1}{\\tau_m} V(t) = \\frac{R I_0}{\\tau_m}\n$$\nThe general solution to this equation is the sum of the homogeneous solution, $V_h(t)$, and a particular solution, $V_p(t)$.\n\nThe homogeneous equation is $\\frac{dV_h}{dt} + \\frac{1}{\\tau_m} V_h = 0$, which has the solution $V_h(t) = A \\exp(-t/\\tau_m)$, where $A$ is an integration constant.\n\nFor the particular solution, since the input current $I_0$ is constant, the right-hand side of the ODE is constant. We can therefore look for a constant steady-state solution $V_p(t) = V_{\\infty}$. Substituting this into the full ODE, the derivative term becomes zero:\n$$\n\\tau_m \\frac{d(V_{\\infty})}{dt} = 0 = -V_{\\infty} + R I_0\n$$\nThis yields the steady-state potential $V_{\\infty} = R I_0$.\n\nThe complete general solution for $V(t)$ is the sum of the homogeneous and particular solutions:\n$$\nV(t) = V_h(t) + V_p(t) = V_{\\infty} + A \\exp(-t/\\tau_m) = R I_0 + A \\exp(-t/\\tau_m)\n$$\nTo determine the constant $A$, we apply the initial condition. The problem specifies that immediately after a spike at $t=0$, the membrane potential is reset. Thus, $V(0) = V_\\mathrm{reset}$. Substituting this into the general solution:\n$$\nV(0) = V_\\mathrm{reset} = R I_0 + A \\exp(-0/\\tau_m) = R I_0 + A\n$$\nSolving for $A$, we find $A = V_\\mathrm{reset} - R I_0$.\n\nSubstituting this constant back into the general solution gives the explicit trajectory of the membrane potential starting from the reset value:\n$$\nV(t) = R I_0 + (V_\\mathrm{reset} - R I_0) \\exp(-t/\\tau_m)\n$$\nThe inter-spike interval, $T_{\\mathrm{ISI}}$, is the time required for the potential to rise from $V_\\mathrm{reset}$ to the firing threshold $\\vartheta$. We find this time by setting $t = T_{\\mathrm{ISI}}$ and $V(T_{\\mathrm{ISI}}) = \\vartheta$:\n$$\n\\vartheta = R I_0 + (V_\\mathrm{reset} - R I_0) \\exp(-T_{\\mathrm{ISI}}/\\tau_m)\n$$\nWe now solve this equation for $T_{\\mathrm{ISI}}$. First, we isolate the exponential term:\n$$\n\\vartheta - R I_0 = (V_\\mathrm{reset} - R I_0) \\exp(-T_{\\mathrm{ISI}}/\\tau_m)\n$$\n$$\n\\exp(-T_{\\mathrm{ISI}}/\\tau_m) = \\frac{\\vartheta - R I_0}{V_\\mathrm{reset} - R I_0}\n$$\nBy multiplying the numerator and the denominator of the fraction by $-1$, we can express the right-hand side more conveniently:\n$$\n\\exp(-T_{\\mathrm{ISI}}/\\tau_m) = \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}}\n$$\nTo solve for $T_{\\mathrm{ISI}}$, we take the natural logarithm of both sides:\n$$\n\\ln\\left( \\exp(-T_{\\mathrm{ISI}}/\\tau_m) \\right) = \\ln\\left( \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}} \\right)\n$$\n$$\n-\\frac{T_{\\mathrm{ISI}}}{\\tau_m} = \\ln\\left( \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}} \\right)\n$$\nFinally, multiplying by $-\\tau_m$ yields the expression for $T_{\\mathrm{ISI}}$:\n$$\nT_{\\mathrm{ISI}} = -\\tau_m \\ln\\left( \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}} \\right)\n$$\nUsing the logarithmic identity $-\\ln(x/y) = \\ln(y/x)$, we obtain the final closed-form expression:\n$$\nT_{\\mathrm{ISI}} = \\tau_m \\ln\\left( \\frac{R I_0 - V_\\mathrm{reset}}{R I_0 - \\vartheta} \\right)\n$$\nThe conditions $R I_0  \\vartheta$ and $V_\\mathrm{reset}  \\vartheta$ ensure that $R I_0 - V_\\mathrm{reset}  R I_0 - \\vartheta  0$. Consequently, the argument of the logarithm is greater than $1$, which guarantees that $T_{\\mathrm{ISI}}$ is a positive and real-valued time interval, as is physically required.",
            "answer": "$$\n\\boxed{\\tau_m \\ln\\left( \\frac{R I_0 - V_\\mathrm{reset}}{R I_0 - \\vartheta} \\right)}\n$$"
        },
        {
            "introduction": "Reinforcement learning in biological systems is thought to rely on three-factor learning rules, where synaptic changes depend on pre-synaptic activity, post-synaptic activity, and a global reward signal. This exercise provides a concrete, quantitative look at this process by tasking you with computing a synaptic eligibility trace . You will see how local spike timing creates a temporary memory of causal hypotheses, which is then converted into a permanent weight change upon the arrival of a reward, bridging the temporal gap between action and outcome.",
            "id": "4057394",
            "problem": "Consider a single plastic synapse in a three-factor Reinforcement Learning (RL) framework connecting one presynaptic spiking neuron to one postsynaptic spiking neuron. The synapse maintains an eligibility trace $e(t)$ that is shaped by local spike timing via a Spike-Timing-Dependent Plasticity (STDP) mechanism and decays in time. The presynaptic spike train is $s_{\\text{pre}}(t) = \\sum_{i}\\delta(t - t_{i}^{\\text{pre}})$ and the postsynaptic spike train is $s_{\\text{post}}(t) = \\sum_{j}\\delta(t - t_{j}^{\\text{post}})$, where $\\delta(\\cdot)$ denotes the Dirac delta function, and spikes occur at times $t_{i}^{\\text{pre}}$ and $t_{j}^{\\text{post}}$, respectively. Define exponentially decaying spike traces $x(t)$ and $y(t)$ by the differential equations $\\frac{d x(t)}{d t} = -\\frac{x(t)}{\\tau_{+}} + s_{\\text{pre}}(t)$ and $\\frac{d y(t)}{d t} = -\\frac{y(t)}{\\tau_{-}} + s_{\\text{post}}(t)$, with $x(t)$ and $y(t)$ incremented by $1$ at presynaptic and postsynaptic spikes, respectively. The eligibility $e(t)$ obeys the leaky accumulation rule $\\frac{d e(t)}{d t} = -\\frac{e(t)}{\\tau_{e}} + A_{+}\\,x(t)\\,s_{\\text{post}}(t) - A_{-}\\,y(t)\\,s_{\\text{pre}}(t)$, with $e(t0)=0$. The pairwise STDP update kernel $g(\\Delta t)$ associated with a pre-post spike pair with time difference $\\Delta t = t^{\\text{post}} - t^{\\text{pre}}$ is given by $g(\\Delta t) = A_{+}\\exp\\!\\left(-\\frac{\\Delta t}{\\tau_{+}}\\right)$ for $\\Delta t  0$, $g(\\Delta t) = -A_{-}\\exp\\!\\left(\\frac{\\Delta t}{\\tau_{-}}\\right)$ for $\\Delta t  0$, and $g(0)=0$. Assume the following parameters and spike times: $\\tau_{e} = 100\\,\\text{ms}$, $\\tau_{+} = 20\\,\\text{ms}$, $\\tau_{-} = 30\\,\\text{ms}$, $A_{+} = 0.8$, $A_{-} = 0.5$, $t_{i}^{\\text{pre}} \\in \\{10\\,\\text{ms},\\,70\\,\\text{ms}\\}$, and $t_{j}^{\\text{post}} \\in \\{40\\,\\text{ms},\\,90\\,\\text{ms}\\}$. A scalar reward $r$ arrives at time $t_{r} = 150\\,\\text{ms}$ with magnitude $r = 0.4$, and the learning rate is $\\eta = 0.05$. Starting from the provided definitions and without introducing any additional phenomenological shortcut rules, compute the eligibility $e(t_{r})$ produced by this pre-post sequence and then the net weight update $\\Delta w = \\eta\\,r\\,e(t_{r})$. Round your final numeric answer for $\\Delta w$ to six significant figures. Express the final answer without units.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and internally consistent. It presents a standard computational problem in the field of neuromorphic engineering and computational neuroscience, specifically concerning synaptic plasticity in spiking neural networks. All necessary parameters and conditions are provided to compute a unique, meaningful solution. Thus, I will proceed with a full derivation.\n\nThe primary objective is to compute the net weight update $\\Delta w$, defined as $\\Delta w = \\eta\\,r\\,e(t_{r})$, where $\\eta$ is the learning rate, $r$ is the reward magnitude, and $e(t_r)$ is the value of the synaptic eligibility trace at the time of reward, $t_r$. The core of the problem is to determine $e(t_r)$.\n\nThe dynamics of the eligibility trace $e(t)$ are governed by the linear first-order ordinary differential equation (ODE):\n$$ \\frac{d e(t)}{d t} = -\\frac{e(t)}{\\tau_{e}} + W(t) $$\nwhere $W(t) = A_{+}\\,x(t)\\,s_{\\text{post}}(t) - A_{-}\\,y(t)\\,s_{\\text{pre}}(t)$ represents the updates from spike-timing-dependent plasticity (STDP). The initial condition is $e(t0)=0$.\n\nThe formal solution to this ODE with the initial condition $e(0)=0$ is given by the convolution integral:\n$$ e(t) = \\int_{0}^{t} \\exp\\left(-\\frac{t-t'}{\\tau_{e}}\\right) W(t') \\,dt' $$\nThe spike trains $s_{\\text{pre}}(t)$ and $s_{\\text{post}}(t)$ are sums of Dirac delta functions, $\\delta(\\cdot)$. Substituting the expression for $W(t)$ yields:\n$$ e(t) = \\int_{0}^{t} \\exp\\left(-\\frac{t-t'}{\\tau_{e}}\\right) \\left[A_{+}\\,x(t')\\,\\sum_{j}\\delta(t' - t_{j}^{\\text{post}}) - A_{-}\\,y(t')\\,\\sum_{i}\\delta(t' - t_{i}^{\\text{pre}})\\right] dt' $$\nUsing the sifting property of the Dirac delta function, $\\int f(\\tau)\\delta(\\tau-t_0)d\\tau = f(t_0)$, the integral simplifies. When evaluated at $t=t_r$, we sum the contributions from all spikes that occurred before $t_r$:\n$$ e(t_r) = \\sum_{j: t_{j}^{\\text{post}}t_r} A_{+}\\,x(t_{j}^{\\text{post}}) \\exp\\left(-\\frac{t_r - t_{j}^{\\text{post}}}{\\tau_{e}}\\right) - \\sum_{i: t_{i}^{\\text{pre}}t_r} A_{-}\\,y(t_{i}^{\\text{pre}}) \\exp\\left(-\\frac{t_r - t_{i}^{\\text{pre}}}{\\tau_{e}}\\right) $$\nThis equation shows that $e(t_r)$ is the sum of instantaneous changes that occurred at each spike time, each decayed exponentially with time constant $\\tau_e$ until the reward time $t_r$.\n\nThe presynaptic trace $x(t)$ and postsynaptic trace $y(t)$ are themselves solutions to similar ODEs. Given their definitions, their values at any time $t$ are the sum of decaying exponentials from all preceding spikes:\n$$ x(t) = \\sum_{i: t_{i}^{\\text{pre}}t} \\exp\\left(-\\frac{t - t_{i}^{\\text{pre}}}{\\tau_{+}}\\right) $$\n$$ y(t) = \\sum_{j: t_{j}^{\\text{post}}t} \\exp\\left(-\\frac{t - t_{j}^{\\text{post}}}{\\tau_{-}}\\right) $$\nThe initial conditions are $x(t0)=0$ and $y(t0)=0$.\n\nLet's compute $e(t_r)$ using the given parameters and spike times. All time units are in milliseconds (ms), but we will treat them as dimensionless numbers for calculation, as the ratios in the exponents are dimensionless.\nParameters:\n$\\tau_{e} = 100$, $\\tau_{+} = 20$, $\\tau_{-} = 30$, $A_{+} = 0.8$, $A_{-} = 0.5$.\nSpike times:\nPresynaptic: $t_{1}^{\\text{pre}} = 10$, $t_{2}^{\\text{pre}} = 70$.\nPostsynaptic: $t_{1}^{\\text{post}} = 40$, $t_{2}^{\\text{post}} = 90$.\nReward time: $t_r = 150$.\n\nWe evaluate the terms in the expression for $e(t_r)$ chronologically based on spike events:\n\n1.  **Contribution from presynaptic spike at $t_{1}^{\\text{pre}} = 10\\,$ms**: The term is $-A_{-}\\,y(10)\\exp(-(150 - 10)/\\tau_e)$. At $t=10$, no postsynaptic spikes have occurred, so $y(10) = 0$. The contribution is $0$.\n\n2.  **Contribution from postsynaptic spike at $t_{1}^{\\text{post}} = 40\\,$ms**: The term is $A_{+}\\,x(40)\\exp(-(150 - 40)/\\tau_e)$.\n    The presynaptic trace at $t=40$ is due to the spike at $t_{1}^{\\text{pre}}=10$:\n    $x(40) = \\exp(-(40 - 10)/\\tau_{+}) = \\exp(-30/20) = \\exp(-1.5)$.\n    This contribution to $e(150)$ is $A_{+}\\exp(-1.5)\\exp(-110/100) = A_{+}\\exp(-1.5 - 1.1) = A_{+}\\exp(-2.6)$.\n\n3.  **Contribution from presynaptic spike at $t_{2}^{\\text{pre}} = 70\\,$ms**: The term is $-A_{-}\\,y(70)\\exp(-(150 - 70)/\\tau_e)$.\n    The postsynaptic trace at $t=70$ is due to the spike at $t_{1}^{\\text{post}}=40$:\n    $y(70) = \\exp(-(70 - 40)/\\tau_{-}) = \\exp(-30/30) = \\exp(-1)$.\n    This contribution to $e(150)$ is $-A_{-}\\exp(-1)\\exp(-80/100) = -A_{-}\\exp(-1 - 0.8) = -A_{-}\\exp(-1.8)$.\n\n4.  **Contribution from postsynaptic spike at $t_{2}^{\\text{post}} = 90\\,$ms**: The term is $A_{+}\\,x(90)\\exp(-(150 - 90)/\\tau_e)$.\n    The presynaptic trace at $t=90$ is the sum of contributions from spikes at $t_{1}^{\\text{pre}}=10$ and $t_{2}^{\\text{pre}}=70$:\n    $x(90) = \\exp(-(90 - 10)/\\tau_{+}) + \\exp(-(90 - 70)/\\tau_{+}) = \\exp(-80/20) + \\exp(-20/20) = \\exp(-4) + \\exp(-1)$.\n    This contribution to $e(150)$ is $A_{+}(\\exp(-4) + \\exp(-1))\\exp(-60/100) = A_{+}(\\exp(-4) + \\exp(-1))\\exp(-0.6) = A_{+}\\exp(-4.6) + A_{+}\\exp(-1.6)$.\n\nSumming all contributions, we get the analytical expression for $e(150)$:\n$$ e(150) = A_{+}\\exp(-2.6) - A_{-}\\exp(-1.8) + A_{+}\\exp(-4.6) + A_{+}\\exp(-1.6) $$\nSubstituting the numerical values for $A_{+}$ and $A_{-}$:\n$$ e(150) = 0.8\\exp(-2.6) - 0.5\\exp(-1.8) + 0.8\\exp(-4.6) + 0.8\\exp(-1.6) $$\nNow, we compute the numerical value:\n$e(150) \\approx 0.8(0.07427358) - 0.5(0.16529889) + 0.8(0.01005184) + 0.8(0.20189652)$\n$e(150) \\approx 0.05941886 - 0.08264944 + 0.00804147 + 0.16151722$\n$e(150) \\approx 0.14632811$\n\nFinally, we calculate the weight update $\\Delta w$ using $\\eta = 0.05$ and $r = 0.4$:\n$$ \\Delta w = \\eta\\,r\\,e(t_{r}) = (0.05)(0.4)e(150) = 0.02\\,e(150) $$\n$$ \\Delta w \\approx 0.02 \\times 0.14632811 \\approx 0.0029265622 $$\nRounding the result to six significant figures, we get $0.00292656$.",
            "answer": "$$\\boxed{0.00292656}$$"
        },
        {
            "introduction": "This advanced practice moves beyond simple mechanics to address a critical conceptual issue in reinforcement learning: confounding bias. By analyzing a scenario where a naive learning rule fails due to a confounding state variable , you will discover why simply correlating neural activity with reward is insufficient for effective learning. This exercise powerfully motivates the necessity of a baseline, revealing how learning from *surprise*—the reward prediction error—leads to more robust and accurate credit assignment.",
            "id": "4057313",
            "problem": "Consider a single-neuron actor in Reinforcement Learning (RL) implemented with a spiking Generalized Linear Model (GLM). The environment is a single-step contextual bandit with binary state $S \\in \\{0,1\\}$ drawn independently at the beginning of each trial with $\\mathbb{P}(S=1)=\\frac{1}{2}$ and $\\mathbb{P}(S=0)=\\frac{1}{2}$. The neuron receives a constant input $x(S)$ over a fixed trial duration $[0,\\Delta]$, with $x(0)=0$ and $x(1)=1$. Conditional on the state $S$, the neuron emits a spike train $y(t)$ modeled as a Poisson process with constant conditional intensity\n$$\n\\lambda(S) = \\exp\\!\\big(w\\,x(S)\\big),\n$$\nwhere $w \\in \\mathbb{R}$ is the synaptic weight to be learned. Let $N=\\int_{0}^{\\Delta} y(t)\\,dt$ denote the total spike count in the trial. The reward $R$ is generated at the end of the trial according to $R = r(S) + \\xi$, with $r(0)=0$, $r(1)=1$, and $\\xi$ a zero-mean random variable independent of $S$ and $y(t)$, representing exogenous reward noise with finite variance.\n\nA commonly used naive three-factor rule updates the weight at the end of each trial by\n$$\n\\Delta w = \\eta\\,R \\int_{0}^{\\Delta} y(t)\\,x(S)\\,dt = \\eta\\,R\\,x(S)\\,N,\n$$\nwhere $\\eta0$ is a small learning rate. Because the environment’s reward does not causally depend on the neuron’s spiking (the action has no effect on $R$ given $S$), an unbiased update should have zero expectation. However, confounding can arise because both $R$ and $y(t)$ depend on $S$.\n\nStarting from the definitions given above, compute the closed-form analytic expression for the expected naive update $\\mathbb{E}[\\Delta w]$ as a function of $\\eta$, $\\Delta$, and $w$. Your expression must be exact, not empirical. Then, conceptually propose a principled modification of the three-factor update that removes the confounding bias by conditioning out the state-dependent components of the reward and the firing, and briefly justify why its expectation is zero under the present assumptions. Express the final expected naive update in a single closed-form analytic expression. No rounding is required and no physical units need to be reported.",
            "solution": "The problem asks for two things: first, to compute the expected value of a naive three-factor learning rule, $\\mathbb{E}[\\Delta w]$, and second, to propose a modified, unbiased learning rule and justify why its expectation is zero.\n\nFirst, we compute the expectation of the naive weight update $\\Delta w = \\eta\\,R\\,x(S)\\,N$. The expectation is denoted by $\\mathbb{E}[\\Delta w]$.\n$$\n\\mathbb{E}[\\Delta w] = \\mathbb{E}[\\eta\\,R\\,x(S)\\,N] = \\eta\\,\\mathbb{E}[R\\,x(S)\\,N]\n$$\nTo compute the expectation $\\mathbb{E}[R\\,x(S)\\,N]$, we use the law of total expectation by conditioning on the state $S$. The state $S$ can be $0$ or $1$, each with probability $\\frac{1}{2}$.\n$$\n\\mathbb{E}[R\\,x(S)\\,N] = \\mathbb{P}(S=0)\\,\\mathbb{E}[R\\,x(S)\\,N | S=0] + \\mathbb{P}(S=1)\\,\\mathbb{E}[R\\,x(S)\\,N | S=1]\n$$\nWe evaluate the conditional expectation for each case.\n\nCase 1: $S=0$\nGiven $S=0$, the input is $x(S) = x(0) = 0$.\nThe term inside the expectation is $R\\,x(0)\\,N = R \\cdot 0 \\cdot N = 0$.\nTherefore, the conditional expectation is:\n$$\n\\mathbb{E}[R\\,x(S)\\,N | S=0] = \\mathbb{E}[0] = 0\n$$\n\nCase 2: $S=1$\nGiven $S=1$, the input is $x(S) = x(1) = 1$. The reward is $R = r(1) + \\xi = 1 + \\xi$.\nThe term inside the expectation is $R\\,x(1)\\,N = (1+\\xi) \\cdot 1 \\cdot N = (1+\\xi)N$.\nWe need to compute the conditional expectation $\\mathbb{E}[(1+\\xi)N | S=1]$.\nAccording to the problem statement, the reward noise $\\xi$ is independent of the state $S$ and the spike train $y(t)$. Since $N = \\int_{0}^{\\Delta} y(t)\\,dt$, $\\xi$ is also independent of $N$.\nConditional on $S=1$, $N$ and $\\xi$ are independent random variables. Thus, we can separate the expectation of their product:\n$$\n\\mathbb{E}[(1+\\xi)N | S=1] = \\mathbb{E}[1+\\xi | S=1] \\cdot \\mathbb{E}[N | S=1]\n$$\nSince $\\xi$ is independent of $S$, $\\mathbb{E}[1+\\xi | S=1] = \\mathbb{E}[1+\\xi] = 1 + \\mathbb{E}[\\xi]$. We are given that $\\xi$ is a zero-mean random variable, so $\\mathbb{E}[\\xi] = 0$.\n$$\n\\mathbb{E}[1+\\xi | S=1] = 1 + 0 = 1\n$$\nNext, we find $\\mathbb{E}[N | S=1]$. The spike train $y(t)$ is modeled as a Poisson process with constant conditional intensity $\\lambda(S)$. The total spike count $N$ in a duration $\\Delta$ is a Poisson-distributed random variable with mean $\\lambda(S)\\Delta$.\nFor $S=1$, the intensity is $\\lambda(1) = \\exp(w\\,x(1)) = \\exp(w)$.\nSo, the conditional expectation of $N$ is:\n$$\n\\mathbb{E}[N | S=1] = \\lambda(1)\\Delta = \\Delta\\exp(w)\n$$\nCombining these results for the $S=1$ case:\n$$\n\\mathbb{E}[(1+\\xi)N | S=1] = 1 \\cdot \\Delta\\exp(w) = \\Delta\\exp(w)\n$$\n\nNow, we substitute the results for both cases back into the law of total expectation:\n$$\n\\mathbb{E}[R\\,x(S)\\,N] = \\left(\\frac{1}{2}\\right) \\cdot (0) + \\left(\\frac{1}{2}\\right) \\cdot (\\Delta\\exp(w)) = \\frac{\\Delta}{2}\\exp(w)\n$$\nFinally, we find the expected weight update:\n$$\n\\mathbb{E}[\\Delta w] = \\eta \\cdot \\frac{\\Delta}{2}\\exp(w) = \\frac{\\eta\\Delta}{2}\\exp(w)\n$$\nThis is the closed-form expression for the expected naive update. The non-zero result confirms that the update rule is biased. This bias arises because the state $S$ acts as a confounder, simultaneously influencing the reward via $r(S)$ and the neuronal activity via $x(S)$, which creates a spurious correlation between reward and spiking.\n\nSecond, we propose a principled modification to the update rule to remove this bias. In reinforcement learning, this is typically achieved by subtracting a state-dependent baseline, $b(S)$, from the reward term. The update is then driven by the reward prediction error (RPE), $R - b(S)$. The modified update rule is:\n$$\n\\Delta w' = \\eta (R - b(S)) x(S) N\n$$\nThe optimal baseline that removes the confounding and often reduces variance is the expected reward given the state, $b(S) = \\mathbb{E}[R|S]$. We calculate this baseline:\n$$\n\\mathbb{E}[R|S] = \\mathbb{E}[r(S) + \\xi | S] = r(S) + \\mathbb{E}[\\xi|S]\n$$\nSince $\\xi$ is independent of $S$ and has a mean of zero, $\\mathbb{E}[\\xi|S] = \\mathbb{E}[\\xi] = 0$. Thus, the baseline is $b(S) = r(S)$.\nThe modified, unbiased update rule is:\n$$\n\\Delta w' = \\eta (R - r(S)) x(S) N\n$$\nSubstituting $R = r(S) + \\xi$, the RPE term becomes $R - r(S) = (r(S) + \\xi) - r(S) = \\xi$. The rule simplifies to:\n$$\n\\Delta w' = \\eta\\,\\xi\\,x(S)\\,N\n$$\nTo justify that this rule is unbiased, we compute its expectation:\n$$\n\\mathbb{E}[\\Delta w'] = \\mathbb{E}[\\eta\\,\\xi\\,x(S)\\,N] = \\eta\\,\\mathbb{E}[\\xi\\,x(S)\\,N]\n$$\nThe problem states that $\\xi$ is independent of both $S$ and $y(t)$, and therefore also of $N = \\int_{0}^{\\Delta} y(t)\\,dt$. This means $\\xi$ is independent of the product $x(S)N$. We can therefore separate the expectation:\n$$\n\\mathbb{E}[\\xi\\,x(S)\\,N] = \\mathbb{E}[\\xi] \\cdot \\mathbb{E}[x(S)\\,N]\n$$\nSince $\\mathbb{E}[\\xi] = 0$, the entire expression becomes zero:\n$$\n\\mathbb{E}[\\Delta w'] = \\eta \\cdot 0 \\cdot \\mathbb{E}[x(S)\\,N] = 0\n$$\nThe expectation of the modified update is zero, which correctly reflects the fact that the neuron's spiking action has no causal effect on the reward in this environment. By conditioning out the state-dependent component of the reward, $r(S)$, we have removed the confounding bias.",
            "answer": "$$\n\\boxed{\\frac{\\eta\\Delta}{2}\\exp(w)}\n$$"
        }
    ]
}