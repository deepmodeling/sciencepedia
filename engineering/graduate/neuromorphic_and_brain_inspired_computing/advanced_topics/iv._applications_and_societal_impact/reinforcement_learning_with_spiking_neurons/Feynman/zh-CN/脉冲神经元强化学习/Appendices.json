{
    "hands_on_practices": [
        {
            "introduction": "要构建能够学习的脉冲神经网络，我们首先必须理解其基本组成部分——单个脉冲神经元——如何处理信息。此练习将引导您从基本原理出发，推导泄漏整合发放（LIF）神经元模型的输入-输出关系。通过求解在恒定输入电流下神经元的膜电位方程，您将能够分析性地确定其发放频率，从而深入理解神经元参数如何将其输入转化为时间编码的输出信号。",
            "id": "4057344",
            "problem": "在一个用脉冲神经元实现的强化学习(RL)策略网络中，一个动作通道由单个漏电积分发放(LIF)神经元发出，其脉冲间隔(ISI)编码了动作的强度。膜电位 $V(t)$ 在恒定注入电流 $I(t)=I_0$ 的作用下，遵循标准的LIF动力学，其膜时间常数为 $\\tau_m$，膜电阻为 $R$，即\n$$\n\\tau_m \\frac{dV}{dt} = - V + R I_0.\n$$\n当 $V(t)$ 达到阈值 $\\vartheta$ 时，会发出一个脉冲，之后膜电位会瞬时重置为 $V_\\mathrm{reset}$。假设没有不应期或额外的突触动力学。在 $t=0$ 时刻的脉冲之后，膜电位满足 $V(0^+) = V_\\mathrm{reset}$。假设参数满足 $V_\\mathrm{reset}  \\vartheta$ 和 $R I_0 > \\vartheta$，从而确保后续脉冲能在有限时间内发生。\n\n从给定的微分方程和这些条件出发，从第一性原理推导脉冲间隔 $T_{\\mathrm{ISI}}$（从 $t=0$ 时刻重置到下一次越过阈值的时间）的闭式解析表达式，该表达式是 $\\tau_m$、$R$、$I_0$、$\\vartheta$ 和 $V_\\mathrm{reset}$ 的函数。将最终答案以秒为单位表示为单个闭式表达式。不需要进行数值计算或四舍五入。",
            "solution": "膜电位 $V(t)$ 的动力学由以下一阶线性非齐次常微分方程决定：\n$$\n\\tau_m \\frac{dV}{dt} = - V + R I_0\n$$\n我们可以将其整理成标准形式 $\\frac{dV}{dt} + \\frac{1}{\\tau_m} V(t) = \\frac{R I_0}{\\tau_m}$。该方程的通解是齐次解 $V_h(t)$ 和一个特解 $V_p(t)$ 之和。\n\n齐次方程为 $\\frac{dV_h}{dt} + \\frac{1}{\\tau_m} V_h = 0$，其解为 $V_h(t) = A \\exp(-t/\\tau_m)$，其中 $A$ 是一个积分常数。\n\n对于特解，由于输入电流 $I_0$ 是恒定的，我们可以寻找一个恒定的稳态解 $V_p(t) = V_{\\infty}$。将其代入完整的常微分方程，导数项变为零：\n$$\n0 = -V_{\\infty} + R I_0\n$$\n这就得出了稳态电位 $V_{\\infty} = R I_0$。\n\n$V(t)$ 的完整通解是：\n$$\nV(t) = V_{\\infty} + A \\exp(-t/\\tau_m) = R I_0 + A \\exp(-t/\\tau_m)\n$$\n为了确定常数 $A$，我们应用初始条件 $V(0) = V_\\mathrm{reset}$：\n$$\nV(0) = V_\\mathrm{reset} = R I_0 + A \\exp(0) = R I_0 + A\n$$\n解出 $A$，我们得到 $A = V_\\mathrm{reset} - R I_0$。\n\n将这个常数代回通解，得到从重置值开始的膜电位显式轨迹：\n$$\nV(t) = R I_0 + (V_\\mathrm{reset} - R I_0) \\exp(-t/\\tau_m)\n$$\n脉冲间隔 $T_{\\mathrm{ISI}}$ 是电位从 $V_\\mathrm{reset}$ 上升到发放阈值 $\\vartheta$ 所需的时间。我们通过令 $t = T_{\\mathrm{ISI}}$ 和 $V(T_{\\mathrm{ISI}}) = \\vartheta$ 来求出这个时间：\n$$\n\\vartheta = R I_0 + (V_\\mathrm{reset} - R I_0) \\exp(-T_{\\mathrm{ISI}}/\\tau_m)\n$$\n现在我们求解 $T_{\\mathrm{ISI}}$。首先，分离出指数项：\n$$\n\\vartheta - R I_0 = (V_\\mathrm{reset} - R I_0) \\exp(-T_{\\mathrm{ISI}}/\\tau_m)\n$$\n$$\n\\exp(-T_{\\mathrm{ISI}}/\\tau_m) = \\frac{\\vartheta - R I_0}{V_\\mathrm{reset} - R I_0} = \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}}\n$$\n对等式两边取自然对数：\n$$\n-\\frac{T_{\\mathrm{ISI}}}{\\tau_m} = \\ln\\left( \\frac{R I_0 - \\vartheta}{R I_0 - V_\\mathrm{reset}} \\right)\n$$\n最后，乘以 $-\\tau_m$ 并使用对数恒等式 $-\\ln(x/y) = \\ln(y/x)$，我们得到最终的闭式表达式：\n$$\nT_{\\mathrm{ISI}} = \\tau_m \\ln\\left( \\frac{R I_0 - V_\\mathrm{reset}}{R I_0 - \\vartheta} \\right)\n$$\n条件 $R I_0 > \\vartheta$ 和 $V_\\mathrm{reset}  \\vartheta$ 确保了 $R I_0 - V_\\mathrm{reset} > R I_0 - \\vartheta > 0$。因此，对数的自变量大于1，这保证了 $T_{\\mathrm{ISI}}$ 是一个正实数时间间隔，与物理要求相符。",
            "answer": "$$\n\\boxed{\\tau_m \\ln\\left( \\frac{R I_0 - V_\\mathrm{reset}}{R I_0 - \\vartheta} \\right)}\n$$"
        },
        {
            "introduction": "在了解了单个神经元如何发放脉冲后，下一步是探究神经元之间的连接（即突触）如何根据经验进行调整。本练习将深入探讨三因子学习法则，这是一种生物学上可信的强化学习机制，它结合了局部脉冲时序信息和全局奖励信号。您将通过一个具体的脉冲序列计算突触的“资格迹”，亲身体验突触如何“标记”自身以待更新，以及一个延迟的奖励信号如何最终触发权重的实际变化。",
            "id": "4057394",
            "problem": "考虑一个三因子强化学习（RL）框架中的单个可塑性突触，它连接一个突触前尖峰神经元和一个突触后尖峰神经元。该突触维持一个资格迹 $e(t)$，它由局部尖峰时间通过尖峰时间依赖可塑性（STDP）机制塑造，并随时间衰减。突触前尖峰序列为 $s_{\\text{pre}}(t) = \\sum_{i}\\delta(t - t_{i}^{\\text{pre}})$，突触后尖峰序列为 $s_{\\text{post}}(t) = \\sum_{j}\\delta(t - t_{j}^{\\text{post}})$，其中 $\\delta(\\cdot)$ 表示狄拉克δ函数，尖峰分别在时间 $t_{i}^{\\text{pre}}$ 和 $t_{j}^{\\text{post}}$ 发生。通过微分方程 $\\frac{d x(t)}{d t} = -\\frac{x(t)}{\\tau_{+}} + s_{\\text{pre}}(t)$ 和 $\\frac{d y(t)}{d t} = -\\frac{y(t)}{\\tau_{-}} + s_{\\text{post}}(t)$ 定义指数衰减的尖峰迹 $x(t)$ 和 $y(t)$，其中 $x(t)$ 和 $y(t)$ 分别在突触前和突触后尖峰发生时增加1。资格迹 $e(t)$ 遵循泄漏累积规则 $\\frac{d e(t)}{d t} = -\\frac{e(t)}{\\tau_{e}} + A_{+}\\,x(t)\\,s_{\\text{post}}(t) - A_{-}\\,y(t)\\,s_{\\text{pre}}(t)$，且 $e(0)=0$。与时间差为 $\\Delta t = t^{\\text{post}} - t^{\\text{pre}}$ 的突触前-后尖峰对相关的成对STDP更新核 $g(\\Delta t)$ 由 $g(\\Delta t) = A_{+}\\exp\\!\\left(-\\frac{\\Delta t}{\\tau_{+}}\\right)$（当 $\\Delta t > 0$ 时），$g(\\Delta t) = -A_{-}\\exp\\!\\left(\\frac{\\Delta t}{\\tau_{-}}\\right)$（当 $\\Delta t  0$ 时）以及 $g(0)=0$ 给出。假设以下参数和尖峰时间：$\\tau_{e} = 100\\,\\text{ms}$，$\\tau_{+} = 20\\,\\text{ms}$，$\\tau_{-} = 30\\,\\text{ms}$，$A_{+} = 0.8$，$A_{-} = 0.5$，$t^{\\text{pre}} \\in \\{10\\,\\text{ms},\\,70\\,\\text{ms}\\}$，以及 $t^{\\text{post}} \\in \\{40\\,\\text{ms},\\,90\\,\\text{ms}\\}$。一个标量奖励 $r$ 在时间 $t_{r} = 150\\,\\text{ms}$ 到达，其大小为 $r = 0.4$，学习率为 $\\eta = 0.05$。从所提供的定义出发，不引入任何额外的唯象快捷规则，计算由此前-后序列产生的资格迹 $e(t_{r})$，然后计算净权重更新 $\\Delta w = \\eta\\,r\\,e(t_{r})$。将您对 $\\Delta w$ 的最终数值答案四舍五入到六位有效数字。最终答案不带单位。",
            "solution": "我们的目标是计算净权重更新 $\\Delta w = \\eta\\,r\\,e(t_{r})$，这需要计算在奖励到达时刻 $t_r = 150\\,\\text{ms}$ 的资格迹 $e(t_r)$ 的值。资格迹 $e(t)$ 随时间累积来自脉冲时间依赖可塑性 (STDP) 事件的贡献，并以时间常数 $\\tau_e$ 指数衰减。\n\n我们可以通过计算每个突触前-后脉冲对的贡献，并将其从其发生时刻衰减到奖励时刻 $t_r$，然后将所有贡献相加来得到 $e(t_r)$。STDP 事件的贡献由 STDP 核 $g(\\Delta t)$ 给出，并被认为发生在脉冲对中第二个脉冲的时刻。\n\n我们有突触前脉冲在 $\\{10\\,\\text{ms},\\,70\\,\\text{ms}\\}$，突触后脉冲在 $\\{40\\,\\text{ms},\\,90\\,\\text{ms}\\}$。我们分析所有四个脉冲对。\n\n1.  **脉冲对 (前: 10ms, 后: 40ms):**\n    *   时间差：$\\Delta t = 40 - 10 = 30\\,\\text{ms} > 0$ (因果, LTP)。\n    *   STDP 贡献：$g(30) = A_{+}\\exp(-\\Delta t / \\tau_{+}) = 0.8\\exp(-30/20) = 0.8\\exp(-1.5)$。\n    *   此贡献在 $t=40\\,\\text{ms}$ 注入迹中。\n    *   到奖励时刻 $t_r=150\\,\\text{ms}$，它衰减了 $150 - 40 = 110\\,\\text{ms}$。\n    *   在 $t_r$ 的贡献值：$e_1 = 0.8\\exp(-1.5) \\cdot \\exp(-110/\\tau_e) = 0.8\\exp(-1.5)\\exp(-1.1) = 0.8\\exp(-2.6)$。\n\n2.  **脉冲对 (前: 10ms, 后: 90ms):**\n    *   时间差：$\\Delta t = 90 - 10 = 80\\,\\text{ms} > 0$ (因果, LTP)。\n    *   STDP 贡献：$g(80) = A_{+}\\exp(-80/20) = 0.8\\exp(-4)$。\n    *   此贡献在 $t=90\\,\\text{ms}$ 注入。衰减时间为 $150 - 90 = 60\\,\\text{ms}$。\n    *   在 $t_r$ 的贡献值：$e_2 = 0.8\\exp(-4) \\cdot \\exp(-60/100) = 0.8\\exp(-4.6)$。\n\n3.  **脉冲对 (前: 70ms, 后: 40ms):**\n    *   时间差：$\\Delta t = 40 - 70 = -30\\,\\text{ms}  0$ (反因果, LTD)。\n    *   STDP 贡献：$g(-30) = -A_{-}\\exp(\\Delta t / \\tau_{-}) = -0.5\\exp(-30/30) = -0.5\\exp(-1)$。\n    *   此贡献在 $t=70\\,\\text{ms}$ 注入。衰减时间为 $150 - 70 = 80\\,\\text{ms}$。\n    *   在 $t_r$ 的贡献值：$e_3 = -0.5\\exp(-1) \\cdot \\exp(-80/100) = -0.5\\exp(-1.8)$。\n\n4.  **脉冲对 (前: 70ms, 后: 90ms):**\n    *   时间差：$\\Delta t = 90 - 70 = 20\\,\\text{ms} > 0$ (因果, LTP)。\n    *   STDP 贡献：$g(20) = A_{+}\\exp(-20/20) = 0.8\\exp(-1)$。\n    *   此贡献在 $t=90\\,\\text{ms}$ 注入。衰减时间为 $150 - 90 = 60\\,\\text{ms}$。\n    *   在 $t_r$ 的贡献值：$e_4 = 0.8\\exp(-1) \\cdot \\exp(-60/100) = 0.8\\exp(-1.6)$。\n\n奖励时刻的总资格迹是这些衰减后贡献的总和：\n$$\ne(t_r) = e_1 + e_2 + e_3 + e_4 = 0.8e^{-2.6} - 0.5e^{-1.8} + 0.8e^{-4.6} + 0.8e^{-1.6}\n$$\n现在我们计算数值：\n*   $e_1 = 0.8 \\times 0.074273... \\approx 0.0594188$\n*   $e_2 = 0.8 \\times 0.010051... \\approx 0.0080414$\n*   $e_3 = -0.5 \\times 0.165298... \\approx -0.0826495$\n*   $e_4 = 0.8 \\times 0.201896... \\approx 0.1615172$\n$$\ne(150) \\approx 0.0594188 + 0.0080414 - 0.0826495 + 0.1615172 \\approx 0.1463279\n$$\n最后，我们计算权重更新：\n$$\n\\Delta w = \\eta \\cdot r \\cdot e(t_r) = 0.05 \\times 0.4 \\times 0.1463279 = 0.02 \\times 0.1463279 \\approx 0.002926558\n$$\n四舍五入到六位有效数字，得到 $0.00292656$。",
            "answer": "$$\\boxed{0.00292656}$$"
        },
        {
            "introduction": "虽然我们已经学习了如何更新权重，但简单地应用学习规则有时会导致错误的学习结果。这个高级练习揭示了强化学习中的一个关键问题：混淆偏倚，即当状态、动作和奖励之间存在虚假相关时，即使智能体的动作对奖励没有因果影响，学习信号也可能出现偏差。通过推导这种偏倚并设计一个基于状态的基线来进行修正，您将掌握奖励预测误差的核心思想，并将您的理解从应用公式提升到批判性地设计稳健、无偏的学习算法。",
            "id": "4057313",
            "problem": "考虑一个在强化学习 (RL) 中使用尖峰广义线性模型 (GLM) 实现的单神经元行动者。环境是一个单步情境赌博机，其二元状态 $S \\in \\{0,1\\}$ 在每次试验开始时独立抽取，满足 $\\mathbb{P}(S=1)=\\frac{1}{2}$ 和 $\\mathbb{P}(S=0)=\\frac{1}{2}$。在固定的试验时长 $[0,\\Delta]$ 内，神经元接收恒定的输入 $x(S)$，其中 $x(0)=0$ 且 $x(1)=1$。在给定状态 $S$ 的条件下，神经元发出一个尖峰序列 $y(t)$，该序列被建模为一个具有恒定条件强度的泊松过程\n$$\n\\lambda(S) = \\exp\\!\\big(w\\,x(S)\\big),\n$$\n其中 $w \\in \\mathbb{R}$ 是待学习的突触权重。令 $N=\\int_{0}^{\\Delta} y(t)\\,dt$ 表示试验中的总尖峰数。奖励 $R$ 在试验结束时根据 $R = r(S) + \\xi$ 生成，其中 $r(0)=0$，$r(1)=1$，$\\xi$ 是一个均值为零的随机变量，独立于 $S$ 和 $y(t)$，代表具有有限方差的外源性奖励噪声。\n\n一个常用的朴素三因子规则在每次试验结束时通过以下方式更新权重\n$$\n\\Delta w = \\eta\\,R \\int_{0}^{\\Delta} y(t)\\,x(S)\\,dt = \\eta\\,R\\,x(S)\\,N,\n$$\n其中 $\\eta>0$ 是一个小的学习率。由于环境的奖励在因果上不依赖于神经元的尖峰活动（在给定 $S$ 的情况下，行动对 $R$ 没有影响），一个无偏的更新应该具有零期望。然而，由于 $R$ 和 $y(t)$ 都依赖于 $S$，可能会出现混淆。\n\n从上述定义出发，计算朴素更新期望 $\\mathbb{E}[\\Delta w]$ 关于 $\\eta$、$\\Delta$ 和 $w$ 的闭式解析表达式。你的表达式必须是精确的，而非经验性的。然后，从概念上提出一个对三因子更新的原则性修改，通过对奖励和发放率中依赖于状态的分量进行条件化来消除混淆偏差，并简要论证为什么在当前假设下其期望为零。将最终的朴素更新期望用单一的闭式解析表达式表示。不需要四舍五入，也无需报告物理单位。",
            "solution": "朴素权重更新 $\\Delta w = \\eta\\,R\\,x(S)\\,N$ 的期望为 $\\mathbb{E}[\\Delta w] = \\eta\\,\\mathbb{E}[R\\,x(S)\\,N]$。我们使用全期望定律，对状态 $S$ 进行条件化来计算这个期望。\n$$\n\\mathbb{E}[R\\,x(S)\\,N] = \\mathbb{P}(S=0)\\,\\mathbb{E}[R\\,x(S)\\,N | S=0] + \\mathbb{P}(S=1)\\,\\mathbb{E}[R\\,x(S)\\,N | S=1]\n$$\n我们对两种情况分别评估条件期望。\n\n**情况 1: S=0**\n给定 $S=0$，输入为 $x(0)=0$。期望内的项为 $R \\cdot 0 \\cdot N = 0$。因此，$\\mathbb{E}[R\\,x(S)\\,N | S=0] = 0$。\n\n**情况 2: S=1**\n给定 $S=1$，输入为 $x(1)=1$，奖励为 $R = r(1) + \\xi = 1 + \\xi$。期望内的项为 $(1+\\xi)N$。由于奖励噪声 $\\xi$ 和脉冲数 $N$ 在给定 $S=1$ 的条件下是独立的，我们可以分解期望：\n$$\n\\mathbb{E}[(1+\\xi)N | S=1] = \\mathbb{E}[1+\\xi | S=1] \\cdot \\mathbb{E}[N | S=1]\n$$\n我们知道 $\\mathbb{E}[\\xi]=0$ 且 $\\xi$ 独立于 $S$，因此 $\\mathbb{E}[1+\\xi | S=1] = 1 + \\mathbb{E}[\\xi] = 1$。\n脉冲数 $N$ 服从泊松分布，其均值为 $\\lambda(1)\\Delta = \\exp(w \\cdot 1)\\Delta = \\Delta\\exp(w)$。因此，$\\mathbb{E}[N | S=1] = \\Delta\\exp(w)$。\n将两者结合，得到 $\\mathbb{E}[(1+\\xi)N | S=1] = 1 \\cdot \\Delta\\exp(w) = \\Delta\\exp(w)$。\n\n现在，将两种情况的结果代回全期望公式中：\n$$\n\\mathbb{E}[R\\,x(S)\\,N] = \\left(\\frac{1}{2}\\right) \\cdot (0) + \\left(\\frac{1}{2}\\right) \\cdot (\\Delta\\exp(w)) = \\frac{\\Delta}{2}\\exp(w)\n$$\n因此，期望权重更新为：\n$$\n\\mathbb{E}[\\Delta w] = \\frac{\\eta\\Delta}{2}\\exp(w)\n$$\n这个非零结果证实了学习规则是有偏的，因为状态 $S$ 是一个混淆变量，它同时影响奖励和神经元活动，从而在两者之间产生了虚假相关。\n\n为了消除这种混淆偏差，我们引入一个依赖于状态的基线 $b(S)$，并使用奖励预测误差 $R - b(S)$ 来驱动学习。最优的基线是 $b(S) = \\mathbb{E}[R|S] = r(S)$。修正后的更新规则为：\n$$\n\\Delta w' = \\eta (R - r(S)) x(S) N = \\eta \\xi x(S) N\n$$\n这个新规则的期望是：\n$$\n\\mathbb{E}[\\Delta w'] = \\mathbb{E}[\\eta\\,\\xi\\,x(S)\\,N] = \\eta\\,\\mathbb{E}[\\xi] \\cdot \\mathbb{E}[x(S)\\,N]\n$$\n因为 $\\xi$ 是独立于 $S$ 和 $N$ 的，并且 $\\mathbb{E}[\\xi] = 0$，所以整个期望为零：\n$$\n\\mathbb{E}[\\Delta w'] = \\eta \\cdot 0 \\cdot \\mathbb{E}[x(S)\\,N] = 0\n$$\n这正确地反映了在该环境中神经元的动作对奖励没有因果影响，因此期望的更新应该为零。",
            "answer": "$$\n\\boxed{\\frac{\\eta\\Delta}{2}\\exp(w)}\n$$"
        }
    ]
}