{
    "hands_on_practices": [
        {
            "introduction": "The true performance of a parallel processor is not dictated by its peak computational power alone. This exercise challenges you to synthesize two foundational concepts in computer architecture: Amdahl's Law, which governs the limits of parallel speedup, and the Roofline model, which accounts for memory bandwidth constraints. By deriving a unified speedup formula, you will quantify how the von Neumann bottleneck can cap performance long before the processor's arithmetic limits are reached .",
            "id": "4067202",
            "problem": "In a von Neumann architecture, data movement between memory and compute units can limit performance when the available memory bandwidth is insufficient to feed the arithmetic units, a phenomenon often referred to as the von Neumann bottleneck. Consider a workload comprising a total of $W$ floating-point operations. A fraction $\\alpha$ of these operations is inherently serial and must execute on a single core, while the remaining fraction $1-\\alpha$ is perfectly parallelizable across $n$ identical cores.\n\nAssume a roofline-type performance bound in which the attainable performance is limited by the minimum of the peak arithmetic throughput and the bandwidth-limited throughput. Let the aggregate peak compute throughput of the $n$-core processor be $P_{\\text{peak}}$ (in normalized operation-rate units), and let the memory subsystem provide an effective sustained product $BW \\cdot I_{\\text{op}}$ (in the same normalized units), where $BW$ is the sustained memory bandwidth and $I_{\\text{op}}$ is the operational intensity (operations per byte). Assume that all $n$ cores share the same memory bandwidth.\n\nLet the single-core peak throughput be $P_{\\text{core}} = P_{\\text{peak}}/n$. For any segment executing on a single core, the attainable performance is $\\min\\!\\big(P_{\\text{core}},\\, BW \\cdot I_{\\text{op}}\\big)$, and for a perfectly parallel segment executing across $n$ cores, the attainable performance is $\\min\\!\\big(n P_{\\text{core}},\\, BW \\cdot I_{\\text{op}}\\big) = \\min\\!\\big(P_{\\text{peak}},\\, BW \\cdot I_{\\text{op}}\\big)$.\n\nUsing only the basic relations that execution time equals work divided by performance, derive from first principles an expression for the speedup $S$ defined as the ratio of the time to execute the entire workload on a single core to the time on $n$ cores under the shared-bandwidth constraint. Then, for the specific parameters $n=16$, $\\alpha=0.05$, $P_{\\text{peak}}=16$ (normalized units), and $BW \\cdot I_{\\text{op}}=8$ (same units), compute the exact value of the speedup. Additionally, identify within your reasoning whether the multicore execution of the parallel portion is limited by compute or by bandwidth. Give the final numerical speedup as an exact value (no rounding). The speedup is unitless; report only the value.",
            "solution": "The problem requires the derivation of a speedup formula for a multi-core processor whose performance is constrained by both its arithmetic throughput and memory bandwidth, following a Roofline model combined with Amdahl's Law. Subsequently, the speedup is to be calculated for a specific set of parameters.\n\nFirst, we validate the problem statement.\nThe givens are:\n- Total workload: $W$\n- Serial fraction of the workload: $\\alpha$\n- Parallelizable fraction of the workload: $1-\\alpha$\n- Number of cores: $n$\n- Aggregate peak compute throughput: $P_{\\text{peak}}$\n- Memory-bandwidth limited throughput: $BW \\cdot I_{\\text{op}}$\n- Single-core peak throughput: $P_{\\text{core}} = P_{\\text{peak}}/n$\n- Performance on a single core: $\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})$\n- Performance on $n$ cores for the parallel part: $\\min(P_{\\text{peak}}, BW \\cdot I_{\\text{op}})$\n- Speedup definition: $S = T_1 / T_n$\n- Execution time definition: Time = Work / Performance\n- Specific parameters: $n=16$, $\\alpha=0.05$, $P_{\\text{peak}}=16$, $BW \\cdot I_{\\text{op}}=8$.\n\nThe problem is scientifically grounded in established computer architecture principles (Amdahl's Law, Roofline model), is well-posed with sufficient and consistent information, and is stated objectively. The problem is therefore deemed valid.\n\nWe proceed to derive the expression for speedup $S$. The speedup $S$ is defined as the ratio of the execution time on a single core, $T_1$, to the execution time on $n$ cores, $T_n$.\n\n1.  **Calculate the execution time on a single core, $T_1$.**\n    When the entire workload $W$ is executed on a single core, the attainable performance, let's call it $P_1$, is limited by the minimum of the single-core peak throughput and the bandwidth-limited throughput.\n    $$P_1 = \\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})$$\n    The execution time $T_1$ is the total work $W$ divided by this performance:\n    $$T_1 = \\frac{W}{P_1} = \\frac{W}{\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})}$$\n\n2.  **Calculate the execution time on $n$ cores, $T_n$.**\n    The execution on $n$ cores consists of two parts: a serial part and a parallel part.\n    -   The serial part of the work is $\\alpha W$. It must run on a single core. The performance for this part is the same as the single-core performance, $P_1$. The time for the serial part, $T_{n, \\text{serial}}$, is:\n        $$T_{n, \\text{serial}} = \\frac{\\alpha W}{\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})}$$\n    -   The parallel part of the work is $(1-\\alpha)W$. It runs across all $n$ cores. The attainable performance for this parallel execution, let's call it $P_n$, is limited by the minimum of the aggregate peak throughput of all $n$ cores and the shared memory bandwidth throughput.\n        $$P_n = \\min(n P_{\\text{core}}, BW \\cdot I_{\\text{op}}) = \\min(P_{\\text{peak}}, BW \\cdot I_{\\text{op}})$$\n        The time for the parallel part, $T_{n, \\text{parallel}}$, is:\n        $$T_{n, \\text{parallel}} = \\frac{(1-\\alpha)W}{\\min(P_{\\text{peak}}, BW \\cdot I_{\\text{op}})}$$\n    The total execution time on $n$ cores, $T_n$, is the sum of the times for the serial and parallel parts:\n    $$T_n = T_{n, \\text{serial}} + T_{n, \\text{parallel}} = \\frac{\\alpha W}{\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})} + \\frac{(1-\\alpha)W}{\\min(P_{\\text{peak}}, BW \\cdot I_{\\text{op}})}$$\n\n3.  **Derive the speedup expression, $S$.**\n    Using the definition $S = T_1 / T_n$, we have:\n    $$S = \\frac{\\frac{W}{\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})}}{\\frac{\\alpha W}{\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})} + \\frac{(1-\\alpha)W}{\\min(P_{\\text{peak}}, BW \\cdot I_{\\text{op}})}}$$\n    The workload $W$ is a common factor and cancels out:\n    $$S = \\frac{\\frac{1}{\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})}}{\\frac{\\alpha}{\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})} + \\frac{1-\\alpha}{\\min(P_{\\text{peak}}, BW \\cdot I_{\\text{op}})}}$$\n    To simplify this complex fraction, we can multiply the numerator and the denominator by $\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})$:\n    $$S = \\frac{1}{\\alpha + (1-\\alpha) \\frac{\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}})}{\\min(P_{\\text{peak}}, BW \\cdot I_{\\text{op}})}}$$\n    This is the general expression for the speedup under the given model. It is a form of Amdahl's Law where the parallel speedup factor is determined by the Roofline constraints.\n\n4.  **Compute the numerical value of the speedup.**\n    We are given the parameters: $n=16$, $\\alpha=0.05$, $P_{\\text{peak}}=16$, and $BW \\cdot I_{\\text{op}}=8$.\n    First, we determine the single-core peak throughput $P_{\\text{core}}$:\n    $$P_{\\text{core}} = \\frac{P_{\\text{peak}}}{n} = \\frac{16}{16} = 1$$\n    Next, we evaluate the performance for the serial and parallel execution segments.\n    -   Performance of the serial part (and single-core execution):\n        $$\\min(P_{\\text{core}}, BW \\cdot I_{\\text{op}}) = \\min(1, 8) = 1$$\n    -   Performance of the parallel part:\n        $$\\min(P_{\\text{peak}}, BW \\cdot I_{\\text{op}}) = \\min(16, 8) = 8$$\n    Here we identify the performance bottleneck for the parallel portion. Since the attainable performance is $8$, which is equal to $BW \\cdot I_{\\text{op}}$ and less than $P_{\\text{peak}} = 16$, the multicore execution of the parallel portion is limited by memory bandwidth.\n\n    Now, we substitute these performance values and $\\alpha$ into the derived speedup formula:\n    $$S = \\frac{1}{\\alpha + (1-\\alpha) \\frac{1}{8}}$$\n    Substituting $\\alpha = 0.05$:\n    $$S = \\frac{1}{0.05 + (1 - 0.05) \\frac{1}{8}} = \\frac{1}{0.05 + (0.95) \\frac{1}{8}}$$\n    To compute the exact value, we use fractions: $0.05 = \\frac{5}{100} = \\frac{1}{20}$ and $0.95 = \\frac{95}{100} = \\frac{19}{20}$.\n    $$S = \\frac{1}{\\frac{1}{20} + \\frac{19}{20} \\cdot \\frac{1}{8}} = \\frac{1}{\\frac{1}{20} + \\frac{19}{160}}$$\n    We find a common denominator for the terms in the denominator, which is $160$.\n    $$S = \\frac{1}{\\frac{8}{160} + \\frac{19}{160}} = \\frac{1}{\\frac{8+19}{160}} = \\frac{1}{\\frac{27}{160}}$$\n    Therefore, the speedup is:\n    $$S = \\frac{160}{27}$$",
            "answer": "$$\\boxed{\\frac{160}{27}}$$"
        },
        {
            "introduction": "The memory wall does not just create a performance bottleneck; it leads directly to a power and energy efficiency crisis. This practice delves into the thermal constraints of modern processors, a phenomenon sometimes called \"dark silicon,\" where power limits prevent all cores from being active simultaneously. By modeling per-core power consumption, you will analyze how memory-bound workloads, characterized by low utilization, waste significant energy and fundamentally constrain the achievable parallelism in von Neumann systems .",
            "id": "4067232",
            "problem": "Consider a general-purpose Central Processing Unit (CPU) implementing a von Neumann architecture with $M$ identical cores on a single silicon die. The package is constrained by a Thermal Design Power (TDP) budget of $P_{\\text{max}}$. Each powered-on and clocked core (i.e., an active core) dissipates an average per-core power $P_{\\text{core}}$ under a given workload. Assume inactive cores are fully power-gated and draw negligible power. Using only the definitions of power $P = \\mathrm{d}E/\\mathrm{d}t$ and resource constraints, derive a closed-form expression for the maximal fraction $f^{\\star}$ of the die's cores that can be simultaneously active without exceeding $P_{\\text{max}}$. Express your final answer as a dimensionless analytic expression in terms of $P_{\\text{max}}$, $P_{\\text{core}}$, and $M$.\n\nTo analyze the role of von Neumann memory bottlenecks, refine the per-core power model as follows. Let each core’s average power be decomposed into a leakage component $P_{\\text{leak}}$ and a dynamic component $P_{\\text{dyn}}$ scaled by utilization $u \\in [0,1]$ according to $P_{\\text{core}} = P_{\\text{leak}} + u P_{\\text{dyn}}$, where the dynamic component follows Complementary Metal–Oxide–Semiconductor (CMOS) switching power $P_{\\text{dyn}} \\propto C V^{2} f_{\\text{clk}}$ with effective capacitance $C$, supply voltage $V$, and clock frequency $f_{\\text{clk}}$. Assume the memory subsystem comprising the last-level cache, on-die interconnect, and off-chip Dynamic Random-Access Memory (DRAM) interface contributes an additional per-active-core baseline $P_{\\text{mem}}$ that does not vanish during stalls. Using first principles and this refined model, explain how a memory-bound workload (small $u$ due to frequent stalls waiting on memory) affects the achievable fraction $f^{\\star}$ and energy efficiency, and present the corresponding expression for $f^{\\star}$ in terms of $P_{\\text{max}}$, $M$, $P_{\\text{leak}}$, $P_{\\text{dyn}}$, $u$, and $P_{\\text{mem}}$. No numerical rounding is required. The final answer must be a single analytic expression for $f^{\\star}$.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Total number of identical cores: $M$.\n- CPU package Thermal Design Power (TDP) budget: $P_{\\text{max}}$.\n- Average power dissipation per active core (simple model): $P_{\\text{core}}$.\n- Inactive core power: Negligible.\n- Definition of power: $P = \\mathrm{d}E/\\mathrm{d}t$.\n- Goal 1: Derive the maximal fraction of active cores, $f^{\\star}$, in terms of $P_{\\text{max}}$, $P_{\\text{core}}$, and $M$.\n- Refined per-core power model: $P_{\\text{core}} = P_{\\text{leak}} + u P_{\\text{dyn}}$.\n- Core utilization: $u \\in [0,1]$.\n- Dynamic power relation: $P_{\\text{dyn}} \\propto C V^{2} f_{\\text{clk}}$.\n- Additional power from memory subsystem per active core: $P_{\\text{mem}}$.\n- Workload type: Memory-bound, characterized by small $u$.\n- Goal 2: Explain a) the effect of small $u$ on $f^{\\star}$ and energy efficiency, and b) present the corresponding expression for $f^{\\star}$ in terms of $P_{\\text{max}}$, $M$, $P_{\\text{leak}}$, $P_{\\text{dyn}}$, $u$, and $P_{\\text{mem}}$.\n- The final answer is specified to be the single analytic expression for $f^{\\star}$ from the refined model.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the principles of computer architecture and CMOS electronics. The concepts of TDP, power gating, static (leakage) and dynamic power, and the impact of memory stalls (the von Neumann bottleneck) on utilization are standard and well-defined. The problem is well-posed, with sufficient information to derive the requested expressions. The language is objective and formal. The problem is a valid, formalizable physics and engineering problem. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be constructed.\n\n**Derivation and Analysis**\n\nThe analysis proceeds in two stages, corresponding to the two models presented in the problem. The core principle is that the total power dissipated by all active components cannot exceed the thermal design power budget, $P_{\\text{max}}$.\n\nFirst, we consider the simplified model. Let $N$ be the number of simultaneously active cores. Since each active core dissipates a power $P_{\\text{core}}$ and inactive cores dissipate negligible power, the total power dissipated by the CPU is $P_{\\text{total}} = N P_{\\text{core}}$. The fundamental constraint is $P_{\\text{total}} \\le P_{\\text{max}}$, which translates to $N P_{\\text{core}} \\le P_{\\text{max}}$. The maximum number of cores that can be active is therefore limited by the power budget to $N \\le P_{\\text{max}} / P_{\\text{core}}$. The number of active cores is also physically limited by the total number of cores, $M$. Thus, the maximum number of active cores, $N^{\\star}$, is the minimum of these two constraints. Treating $N$ as a continuous variable for modeling purposes, we have $N^{\\star} = \\min(P_{\\text{max}} / P_{\\text{core}}, M)$. The maximal fraction of active cores, $f^{\\star}$, is this number divided by the total number of cores $M$:\n$$f^{\\star} = \\frac{N^{\\star}}{M} = \\frac{1}{M} \\min\\left(\\frac{P_{\\text{max}}}{P_{\\text{core}}}, M\\right) = \\min\\left(\\frac{P_{\\text{max}}}{M P_{\\text{core}}}, 1\\right)$$\n\nNext, we proceed to the refined model, which provides a more detailed description of power consumption, explicitly accounting for the effects of the von Neumann memory bottleneck. In this model, the power of a single active core is decomposed into its internal components, $P_{\\text{core}} = P_{\\text{leak}} + u P_{\\text{dyn}}$, and is augmented by the power dissipated by its share of the memory subsystem, $P_{\\text{mem}}$. The total power dissipated per active core, let's denote it $P_{\\text{active}}$, is the sum of these contributions:\n$$P_{\\text{active}} = (P_{\\text{leak}} + u P_{\\text{dyn}}) + P_{\\text{mem}}$$\nHere, $P_{\\text{leak}}$ is the static leakage power, which is dissipated as long as the core is powered on, regardless of activity. The term $u P_{\\text{dyn}}$ represents the dynamic power, which is proportional to the computational activity or utilization $u$. The term $P_{\\text{mem}}$ represents the power associated with the memory hierarchy (caches, interconnects) that is active when a core is active, even during stalls.\n\nFor $N$ active cores, the total power dissipation is now $P_{\\text{total}} = N P_{\\text{active}} = N(P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}})$.\nApplying the TDP constraint, $P_{\\text{total}} \\le P_{\\text{max}}$:\n$$N (P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}}) \\le P_{\\text{max}}$$\nSolving for the power-limited maximum number of cores gives $N \\le \\frac{P_{\\text{max}}}{P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}}}$.\nAgain, combining this with the physical limit of $M$ cores, the maximum number of simultaneously active cores is:\n$$N^{\\star} = \\min\\left(\\frac{P_{\\text{max}}}{P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}}}, M\\right)$$\nThe corresponding maximal fraction of active cores, $f^{\\star}$, is $N^{\\star}/M$:\n$$f^{\\star} = \\frac{1}{M} \\min\\left(\\frac{P_{\\text{max}}}{P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}}}, M\\right) = \\min\\left(\\frac{P_{\\text{max}}}{M (P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}})}, 1\\right)$$\nThis is the required expression.\n\nNow, we analyze the impact of a memory-bound workload, which is characterized by a small utilization factor $u$ due to frequent processor stalls while waiting for data from memory.\n\n1.  **Effect on Achievable Fraction $f^{\\star}$**:\n    We examine the expression for $f^{\\star}$. The per-active-core power is $P_{\\text{active}} = P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}}$. In a memory-bound scenario, $u$ is small. As $u$ decreases, $P_{\\text{active}}$ decreases, assuming $P_{\\text{dyn}} > 0$. A lower power dissipation per core means that for a fixed total power budget $P_{\\text{max}}$, more cores can be powered on simultaneously. Mathematically, as $u$ decreases, the denominator of the fraction $\\frac{P_{\\text{max}}}{M (P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}})}$ decreases. This causes the value of the fraction to increase, and therefore $f^{\\star}$ increases. In essence, the power budget freed up by stalling cores (which are not consuming dynamic power) can be used to activate other cores. This allows for a higher degree of parallelism, albeit with each core making slow progress.\n\n2.  **Effect on Energy Efficiency**:\n    Energy efficiency can be defined as computational work performed per unit of energy consumed. Work performed by a core is directly related to its utilization, so we can consider throughput to be proportional to $u$. Power consumed is $P_{\\text{active}}$. Thus, energy efficiency, $\\eta$, can be expressed as:\n    $$\\eta \\propto \\frac{\\text{Throughput}}{\\text{Power}} = \\frac{u}{P_{\\text{active}}} = \\frac{u}{P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}}}$$\n    For a memory-bound workload, we analyze the limit as $u \\to 0$. The numerator approaches $0$. The denominator approaches a non-zero positive constant, $P_{\\text{leak}} + P_{\\text{mem}}$, which represents the power wasted while the core is idle but powered on. Therefore:\n    $$\\lim_{u \\to 0} \\eta \\propto \\lim_{u \\to 0} \\frac{u}{P_{\\text{leak}} + P_{\\text{mem}}} = 0$$\n    Energy efficiency collapses to zero. This is a critical consequence of the von Neumann bottleneck: a significant amount of energy is consumed ($P_{\\text{leak}} + P_{\\text{mem}}$) without performing any useful computation. The processor is burning power while being unproductive, waiting for data. Even though more cores can be turned on ($f^{\\star}$ increases), the overall system efficiency is extremely poor. This highlights the fundamental challenge of data movement in modern computing architectures.\n\nThe final expression for $f^{\\star}$ encapsulates these trade-offs, relating the system-level power budget to the detailed power characteristics of each core, including its workload-dependent utilization.",
            "answer": "$$\\boxed{\\min\\left(\\frac{P_{\\text{max}}}{M (P_{\\text{leak}} + u P_{\\text{dyn}} + P_{\\text{mem}})}, 1\\right)}$$"
        },
        {
            "introduction": "How well do brain-inspired algorithms run on conventional hardware? This exercise tackles that question by modeling the performance of a parallel Spiking Neural Network (SNN) simulation on a typical multicore processor. You will explore how the classic shared-bus architecture becomes a communication chokepoint, leading to diminishing returns and eventually, a performance decrease as more cores are added. This analysis will culminate in deriving an optimal number of cores, a critical insight for mapping parallel algorithms onto resource-constrained von Neumann systems .",
            "id": "4067224",
            "problem": "Consider a Spiking Neural Network (SNN) simulation mapped onto a conventional von Neumann multi-core system that uses a single shared memory bus. The system has $n$ identical cores. Each core emits inter-tile spike messages at a rate of $T_s$ messages per second. Each message contains a payload of $L$ bits and incurs a fixed propagation latency of $\\ell$ seconds. The shared bus has a BandWidth (BW) of $b$ bits per second. The baseline single-core implementation processes $N_s$ spikes per second, with a per-spike compute time of $t_c$ seconds. Assume the following:\n- The compute workload is perfectly divisible, so the compute time ideally scales as $1/n$ with no load imbalance.\n- Communication and computation do not overlap in time (i.e., communication time adds to compute time).\n- The shared bus is the only path for inter-tile messages, so all cores contend on the same bus. The total serialization time is governed by the payload size $L$ and the bus BandWidth $b$, and every message also incurs the fixed latency $\\ell$.\n- Queuing and arbitration are subsumed by the serialization term $L/b$; there is no additional contention penalty beyond the fact that $n T_s$ messages are serialized through the shared bus.\n\nStarting from fundamental definitions of speedup as baseline time divided by parallel time, and from the definition of serialization time as payload divided by BandWidth, derive:\n1. The expression for the reduction in speedup due to communication time, defined as $R(n) = S(n)/n$, where $S(n)$ is the speedup with $n$ cores under the shared-bus constraint.\n2. The expression for the optimal number of cores $n$ that maximizes speedup under the given BandWidth limit $b$, message rate $T_s$, payload $L$, latency $\\ell$, and per-spike compute parameters $N_s$ and $t_c$.\n\nExpress your final answer as a single ordered pair in analytical form containing $R(n)$ and the optimal $n$. No numerical evaluation or rounding is required. Do not include units in the final answer.",
            "solution": "The problem asks for the derivation of the speedup reduction factor $R(n)$ and the optimal number of cores $n$ for a parallel Spiking Neural Network (SNN) simulation on a multi-core system with a shared memory bus. The problem is first validated to ensure it is scientifically sound, well-posed, and self-contained.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Number of cores: $n$\n-   Message rate per core: $T_s$ messages per second\n-   Message payload size: $L$ bits\n-   Message propagation latency: $\\ell$ seconds\n-   Shared bus bandwidth: $b$ bits per second\n-   Baseline spike processing rate: $N_s$ spikes per second\n-   Baseline per-spike compute time: $t_c$ seconds\n-   Assumption: Perfect compute scalability ($1/n$).\n-   Assumption: Communication and computation are sequential (do not overlap).\n-   Assumption: All inter-core communication occurs over the single shared bus.\n-   Assumption: Communication time consists of total serialization time and total latency penalties.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the principles of parallel computing performance modeling, specifically addressing the limitations of von Neumann architectures (the shared bus bottleneck) when executing non-von Neumann style computations (SNNs). The parameters and assumptions provided are standard for creating a first-order analytical model of parallel performance. The model simplifies reality (e.g., by ignoring computation/communication overlap and more complex contention models), but it does soexplicitly and consistently.\n\nA key interpretation is required for the problem to be self-consistent. The \"baseline single-core implementation\" refers to an optimized sequential program. Its execution time, $T_1$, involves only computation, as there are no \"inter-tile\" messages. The parallel implementation, for which the communication model is described, is a separate program whose performance is analyzed for $n$ cores. The performance of this parallel code for the case $n=1$ is not expected to be identical to the baseline sequential code, as it may include overheads.\n\nThe workload is defined as processing a batch of $N_s$ spikes. The baseline time to compute this workload is $N_s t_c$. The message rate $T_s$ is interpreted as the number of messages generated by each core for this specific workload, which corresponds to one unit (e.g., one second) of simulated time.\n\nBased on this analysis, the problem is deemed valid, well-posed, and free from contradiction.\n\n### Derivation of the Solution\n\nThe solution is derived by following these steps:\n1.  Define the baseline execution time, $T_1$.\n2.  Define the parallel execution time, $T_n$, as the sum of parallel computation time and communication time.\n3.  Define the speedup, $S(n) = T_1 / T_n$.\n4.  Derive the speedup reduction factor, $R(n) = S(n) / n$.\n5.  Find the optimal number of cores, $n_{opt}$, by maximizing $S(n)$.\n\n**1. Baseline Execution Time ($T_1$)**\nThe baseline task is the processing of $N_s$ spikes. On a single core, this is purely a computational task, as there are no \"inter-tile\" messages. The time for a single core to process one spike is $t_c$.\nTherefore, the total baseline execution time is:\n$$T_1 = N_s t_c$$\n\n**2. Parallel Execution Time ($T_n$)**\nFor $n$ cores, the total execution time $T_n$ is the sum of the parallel computation time ($T_{compute, n}$) and the total communication time ($T_{comm, n}$), as they do not overlap.\n$$T_n = T_{compute, n} + T_{comm, n}$$\n\n**Parallel Computation Time ($T_{compute, n}$):**\nThe total computational workload, $N_s t_c$, is assumed to be perfectly divisible among the $n$ cores. Since the cores operate in parallel, the computation time is reduced by a factor of $n$.\n$$T_{compute, n} = \\frac{N_s t_c}{n}$$\n\n**Communication Time ($T_{comm, n}$):**\nEach of the $n$ cores emits messages at a rate of $T_s$. For the given workload, this results in a total of $n T_s$ messages that must be transmitted over the shared bus.\nThe time to transmit a single message has two components:\n-   Serialization time: The time to put the message bits onto the bus. For a message of size $L$ bits and a bus bandwidth of $b$ bits/second, this is $\\frac{L}{b}$.\n-   Latency: A fixed propagation delay of $\\ell$.\n\nThe total time to transmit one message is $\\frac{L}{b} + \\ell$.\nSince all $n T_s$ messages must be serialized on the single shared bus, the total communication time is the number of messages multiplied by the time per message.\n$$T_{comm, n} = (n T_s) \\left( \\frac{L}{b} + \\ell \\right) = n T_s \\left( \\frac{L}{b} + \\ell \\right)$$\n\nCombining the computation and communication times, the total parallel execution time is:\n$$T_n = \\frac{N_s t_c}{n} + n T_s \\left( \\frac{L}{b} + \\ell \\right)$$\n\n**3. Part 1: Derive the Reduction in Speedup, $R(n)$**\nSpeedup, $S(n)$, is defined as the ratio of baseline time to parallel time.\n$$S(n) = \\frac{T_1}{T_n} = \\frac{N_s t_c}{\\frac{N_s t_c}{n} + n T_s \\left( \\frac{L}{b} + \\ell \\right)}$$\nThe reduction in speedup, $R(n)$, is defined as $S(n)/n$.\n$$R(n) = \\frac{S(n)}{n} = \\frac{1}{n} \\left( \\frac{N_s t_c}{\\frac{N_s t_c}{n} + n T_s \\left( \\frac{L}{b} + \\ell \\right)} \\right)$$\nTo simplify, we multiply the numerator and denominator by $n$:\n$$R(n) = \\frac{N_s t_c}{n \\left( \\frac{N_s t_c}{n} + n T_s \\left( \\frac{L}{b} + \\ell \\right) \\right)} = \\frac{N_s t_c}{N_s t_c + n^2 T_s \\left( \\frac{L}{b} + \\ell \\right)}$$\nThis is the expression for the reduction in speedup.\n\n**4. Part 2: Derive the Optimal Number of Cores, $n_{opt}$**\nThe optimal number of cores, $n_{opt}$, is the value of $n$ that maximizes the speedup $S(n)$. Maximizing $S(n)$ is equivalent to minimizing its denominator, which is the parallel execution time $T_n(n)$. Let's denote the denominator as $D(n)$:\n$$D(n) = T_n(n) = \\frac{N_s t_c}{n} + n T_s \\left( \\frac{L}{b} + \\ell \\right)$$\nTo find the minimum, we treat $n$ as a continuous variable and take the derivative of $D(n)$ with respect to $n$ and set it to zero.\n$$\\frac{dD(n)}{dn} = \\frac{d}{dn} \\left( (N_s t_c) n^{-1} + n \\left( T_s \\left( \\frac{L}{b} + \\ell \\right) \\right) \\right)$$\n$$\\frac{dD(n)}{dn} = -(N_s t_c) n^{-2} + T_s \\left( \\frac{L}{b} + \\ell \\right)$$\nSet the derivative to zero to find the critical point:\n$$-(N_s t_c) n^{-2} + T_s \\left( \\frac{L}{b} + \\ell \\right) = 0$$\n$$T_s \\left( \\frac{L}{b} + \\ell \\right) = \\frac{N_s t_c}{n^2}$$\n$$n^2 = \\frac{N_s t_c}{T_s \\left( \\frac{L}{b} + \\ell \\right)}$$\n$$n = \\sqrt{\\frac{N_s t_c}{T_s \\left( \\frac{L}{b} + \\ell \\right)}}$$\nWe take the positive root since $n$ must be positive. To confirm this is a minimum for $D(n)$ (and thus a maximum for $S(n)$), we check the second derivative:\n$$\\frac{d^2D(n)}{dn^2} = \\frac{d}{dn} \\left( -(N_s t_c) n^{-2} + T_s \\left( \\frac{L}{b} + \\ell \\right) \\right) = 2(N_s t_c) n^{-3}$$\nSince all parameters $N_s$, $t_c$, and $n$ are positive, the second derivative is always positive, confirming that this value of $n$ minimizes the denominator $D(n)$ and therefore maximizes the speedup $S(n)$. This is the expression for the optimal number of cores.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{N_s t_c}{N_s t_c + n^2 T_s \\left( \\frac{L}{b} + \\ell \\right)}  \\sqrt{\\frac{N_s t_c}{T_s \\left( \\frac{L}{b} + \\ell \\right)}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}