{
    "hands_on_practices": [
        {
            "introduction": "We begin by exploring a foundational question in neural coding: what are the fundamental limits on the precision with which a population of neurons can represent a stimulus? This exercise guides you through the derivation of the asymptotic variance of an optimal decoder using the powerful concept of Fisher Information. By completing this practice, you will develop a quantitative intuition for how the number of neurons ($N$), their tuning width ($\\sigma$), and their peak firing rate ($\\alpha$) collectively constrain the fidelity of a population code .",
            "id": "4055130",
            "problem": "Consider a one-dimensional stimulus $s \\in \\mathbb{R}$ encoded by a population of $N$ neurons in a neuromorphic system, each with independent Poisson spiking and identical Gaussian tuning. The mean spike count of neuron $i$ over a fixed observation window is $\\lambda_i(s) = \\alpha \\exp\\!\\big(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\big)$, where $\\alpha  0$ is the peak mean count, $\\sigma  0$ is the tuning width, and $\\mu_i \\in \\mathbb{R}$ is the preferred stimulus of neuron $i$. Assume a dense tiling of preferred stimuli: the set $\\{\\mu_i\\}_{i=1}^{N}$ is uniformly spaced over an interval of length $L  0$ with spacing $\\Delta = \\frac{L}{N}$ and hence density $\\rho = \\frac{N}{L}$. The spike counts $n_i$ are independent with $n_i \\sim \\text{Poisson}(\\lambda_i(s))$.\n\nFocus on decoding small perturbations of $s$ around a fixed operating point $s_0 \\in \\mathbb{R}$ using an Optimal Linear Estimator (OLE), defined as a linear function of the observed counts that is locally unbiased at $s_0$ and minimizes the mean-squared error to first order in $(s - s_0)$ under the Poisson noise model. Starting from first principles of independent Poisson variability and a first-order (local) linearization of the tuning curves around $s_0$, derive the asymptotic variance of the OLE estimate of $s$ as $N \\to \\infty$ under the dense tiling assumption. Your final expression must be given in terms of $\\alpha$, $\\sigma$, $L$, and $N$.\n\nExpress the final variance as a single closed-form analytic expression. No numerical rounding is required. State your final result as the variance in the units of the stimulus variable $s$ without including units in the boxed final answer.",
            "solution": "The problem requires the derivation of the asymptotic variance of an Optimal Linear Estimator (OLE) for a stimulus $s$. The variance of an OLE that is locally unbiased and minimally variant is given by the Cram√©r-Rao Lower Bound (CRLB), which is the inverse of the Fisher Information, $J(s)$. We will compute the Fisher Information for the specified population of neurons and then invert it to find the variance.\n\nThe model consists of $N$ neurons with independent Poisson spiking. The mean spike count of neuron $i$ is given by a Gaussian tuning curve:\n$$\n\\lambda_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\nwhere $\\alpha$ is the peak mean count, $\\sigma$ is the tuning width, and $\\mu_i$ is the preferred stimulus. The spike counts $n_i$ are independent Poisson variables, $n_i \\sim \\text{Poisson}(\\lambda_i(s))$.\n\nThe Fisher Information, $J(s)$, for a population of independent Poisson neurons can be calculated as:\n$$\nJ(s) = \\sum_{i=1}^{N} \\frac{\\left(\\frac{d\\lambda_i(s)}{ds}\\right)^2}{\\lambda_i(s)}\n$$\nLet's denote the derivative of $\\lambda_i(s)$ with respect to $s$ as $\\lambda'_i(s)$. First, we compute this derivative:\n$$\n\\lambda'_i(s) = \\frac{d}{ds} \\left[ \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right) \\right]\n$$\nUsing the chain rule, we get:\n$$\n\\lambda'_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right) \\cdot \\left( -\\frac{2(s - \\mu_i)}{2\\sigma^2} \\right) = -\\frac{s - \\mu_i}{\\sigma^2} \\lambda_i(s)\n$$\nNow, we can compute the term in the sum for the Fisher Information:\n$$\n\\frac{(\\lambda'_i(s))^2}{\\lambda_i(s)} = \\frac{\\left( -\\frac{s - \\mu_i}{\\sigma^2} \\lambda_i(s) \\right)^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\frac{\\lambda_i(s)^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\lambda_i(s)\n$$\nSubstituting the expression for $\\lambda_i(s)$:\n$$\n\\frac{(\\lambda'_i(s))^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\nThe total Fisher Information at the operating point $s_0$ is the sum over all $N$ neurons:\n$$\nJ(s_0) = \\sum_{i=1}^{N} \\frac{(s_0 - \\mu_i)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s_0 - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\nThe problem specifies a dense tiling assumption as $N \\to \\infty$. The preferred stimuli $\\{\\mu_i\\}$ are uniformly spaced over an interval of length $L$, with density $\\rho = \\frac{N}{L}$. In this limit, the discrete sum can be replaced by an integral over the variable $\\mu$:\n$$\n\\sum_{i=1}^{N} f(\\mu_i) \\to \\int f(\\mu) \\rho \\, d\\mu\n$$\nApplying this to the expression for $J(s_0)$:\n$$\nJ(s_0) = \\int \\rho \\, \\frac{(s_0 - \\mu)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s_0 - \\mu)^{2}}{2 \\sigma^{2}}\\right) d\\mu\n$$\nThe integral is over the range of preferred stimuli, which is an interval of length $L$. However, since the Gaussian term decays rapidly for $|\\mu - s_0| \\gg \\sigma$, if we assume the stimulus range $L$ is large compared to the tuning width $\\sigma$, we can approximate the integral by extending the limits to $(-\\infty, \\infty)$ without significant error. This is a standard procedure in this context.\n$$\nJ(s_0) \\approx \\rho \\alpha \\int_{-\\infty}^{\\infty} \\frac{(s_0 - \\mu)^2}{\\sigma^4} \\exp\\left(-\\frac{(s_0 - \\mu)^{2}}{2 \\sigma^{2}}\\right) d\\mu\n$$\nTo solve this integral, we perform a change of variables. Let $x = \\frac{\\mu - s_0}{\\sigma}$. This implies $d\\mu = \\sigma dx$ and $(s_0 - \\mu)^2 = (-\\sigma x)^2 = \\sigma^2 x^2$. The integration limits remain unchanged.\n$$\nJ(s_0) \\approx \\rho \\alpha \\int_{-\\infty}^{\\infty} \\frac{\\sigma^2 x^2}{\\sigma^4} \\exp\\left(-\\frac{x^2}{2}\\right) (\\sigma \\, dx)\n$$\n$$\nJ(s_0) \\approx \\frac{\\rho \\alpha}{\\sigma} \\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nThe integral $\\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx$ is a standard result. It can be related to the variance of a standard normal distribution $N(0,1)$, which has a probability density function $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$. The variance is $1$, and is given by $E[X^2] - (E[X])^2 = \\int_{-\\infty}^{\\infty} x^2 \\phi(x) dx - 0^2 = 1$. Thus:\n$$\n\\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx = 1\n$$\nThis implies:\n$$\n\\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx = \\sqrt{2\\pi}\n$$\nSubstituting this back into the expression for $J(s_0)$:\n$$\nJ(s_0) \\approx \\frac{\\rho \\alpha}{\\sigma} \\sqrt{2\\pi}\n$$\nThe problem requires the final expression in terms of $N$ and $L$. Using the density $\\rho = \\frac{N}{L}$, we have:\n$$\nJ(s_0) \\approx \\frac{N}{L} \\frac{\\alpha \\sqrt{2\\pi}}{\\sigma} = \\frac{\\sqrt{2\\pi} \\alpha N}{L \\sigma}\n$$\nThe variance of the OLE, $\\text{Var}(\\hat{s})$, is the inverse of the Fisher Information:\n$$\n\\text{Var}(\\hat{s}) = J(s_0)^{-1} \\approx \\left( \\frac{\\sqrt{2\\pi} \\alpha N}{L \\sigma} \\right)^{-1}\n$$\nThis gives the final expression for the asymptotic variance:\n$$\n\\text{Var}(\\hat{s}) = \\frac{L \\sigma}{\\sqrt{2\\pi} \\alpha N}\n$$\nThis result is independent of the operating point $s_0$, which is a consequence of the uniform density of preferred stimuli and the approximation of the integral over an infinite domain.",
            "answer": "$$ \\boxed{ \\frac{L \\sigma}{\\sqrt{2\\pi} \\alpha N} } $$"
        },
        {
            "introduction": "Real neural systems are noisy, and this noise is often correlated across neurons, a complication not addressed in idealized models with independent variability. This practice confronts this complexity head-on by asking you to derive the optimal linear estimator when the noise covariance matrix ($\\Sigma$) is non-diagonal. Mastering this derivation will illuminate how an ideal decoder must \"whiten\" the population response, strategically weighting neurons not just by their signal, but also by the structure of their collective noise .",
            "id": "4055126",
            "problem": "Consider a population code with $n$ neurons whose mean response to a scalar stimulus $s \\in \\mathbb{R}$ is given by the tuning curve $\\mathbf{f}(s) \\in \\mathbb{R}^{n}$. The observed population response $\\mathbf{r} \\in \\mathbb{R}^{n}$ follows the probabilistic model $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s),\\Sigma)$, where $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a stimulus-independent, positive-definite noise covariance that may be correlated across neurons. Fix a reference stimulus $s_{0}$ and assume a locally linear regime where $\\mathbf{f}(s)$ is differentiable and can be approximated by its first-order Taylor expansion around $s_{0}$, with gradient $\\mathbf{f}'(s_{0}) = \\frac{d\\mathbf{f}}{ds}\\big|_{s_{0}} \\in \\mathbb{R}^{n}$. Consider linear estimators of the form $\\hat{s} = a + \\mathbf{w}^{\\top}\\mathbf{r}$, where $a \\in \\mathbb{R}$ and $\\mathbf{w} \\in \\mathbb{R}^{n}$. Impose the local unbiasedness constraints at $s_{0}$: $\\mathbb{E}[\\hat{s}\\mid s_{0}] = s_{0}$ and $\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s]\\big|_{s_{0}} = 1$. Among all such locally unbiased linear estimators, derive the estimator that minimizes $\\mathrm{Var}(\\hat{s}\\mid s_{0})$ under the correlated Gaussian noise $\\mathcal{N}(\\mathbf{f}(s),\\Sigma)$. Then specialize your expression to the uncorrelated case $\\Sigma = \\mathrm{diag}(\\sigma_{1}^{2},\\ldots,\\sigma_{n}^{2})$, with $\\sigma_{i}0$ for $i=1,\\ldots,n$. Express your final answer as closed-form analytic expressions for the optimal estimator at $s_{0}$ in both cases, written in terms of $\\mathbf{f}(s_{0})$, $\\mathbf{f}'(s_{0})$, $\\Sigma$, and $\\mathbf{r}$. No numerical evaluation or rounding is required. Do not introduce any additional assumptions beyond those stated. For clarity of terminology, when referring to the estimator, use the explicit expression rather than invoking the Best Linear Unbiased Estimator (BLUE) by name.",
            "solution": "The user wants to find the optimal linear estimator for a scalar stimulus $s$ based on a neural population response $\\mathbf{r}$. The problem will be solved by first formalizing the optimization problem and then solving it using the method of Lagrange multipliers.\n\nThe problem defines a linear estimator of the form $\\hat{s} = a + \\mathbf{w}^{\\top}\\mathbf{r}$, where $a \\in \\mathbb{R}$ is a scalar offset and $\\mathbf{w} \\in \\mathbb{R}^{n}$ is a vector of weights. The population response $\\mathbf{r}$ is a random vector drawn from a multivariate Gaussian distribution, $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s),\\Sigma)$, where $\\mathbf{f}(s)$ is the vector of mean neural responses (tuning curves) and $\\Sigma$ is the stimulus-independent noise covariance matrix.\n\nThe first step is to express the two local unbiasedness constraints in terms of the estimator parameters $a$ and $\\mathbf{w}$. The expected value of the estimator, conditioned on the stimulus $s$, is:\n$$\n\\mathbb{E}[\\hat{s}\\mid s] = \\mathbb{E}[a + \\mathbf{w}^{\\top}\\mathbf{r}\\mid s] = a + \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{r}\\mid s] = a + \\mathbf{w}^{\\top}\\mathbf{f}(s)\n$$\nThe first constraint, local unbiasedness at the reference stimulus $s_{0}$, is $\\mathbb{E}[\\hat{s}\\mid s_{0}] = s_{0}$. Applying this to the expression for the expectation gives:\n$$\na + \\mathbf{w}^{\\top}\\mathbf{f}(s_{0}) = s_{0}\n$$\nThis constraint allows us to express $a$ in terms of $\\mathbf{w}$:\n$$\na = s_{0} - \\mathbf{w}^{\\top}\\mathbf{f}(s_{0})\n$$\nThe second constraint involves the derivative of the expectation with respect to the stimulus, evaluated at $s_0$: $\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s]\\big|_{s_{0}} = 1$. We first compute the derivative:\n$$\n\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s] = \\frac{d}{ds} \\left( a + \\mathbf{w}^{\\top}\\mathbf{f}(s) \\right) = \\mathbf{w}^{\\top}\\frac{d\\mathbf{f}(s)}{ds} = \\mathbf{w}^{\\top}\\mathbf{f}'(s)\n$$\nEvaluating this at $s=s_{0}$ and applying the constraint yields:\n$$\n\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\nThis is a constraint on the weight vector $\\mathbf{w}$.\n\nNext, we identify the objective function to be minimized, which is the variance of the estimator at $s_{0}$, $\\mathrm{Var}(\\hat{s}\\mid s_{0})$.\n$$\n\\mathrm{Var}(\\hat{s}\\mid s_{0}) = \\mathrm{Var}(a + \\mathbf{w}^{\\top}\\mathbf{r}\\mid s_{0}) = \\mathrm{Var}(\\mathbf{w}^{\\top}\\mathbf{r}\\mid s_{0})\n$$\nThe variance of a linear transformation of a random vector is given by $\\mathbf{w}^{\\top}\\mathrm{Cov}(\\mathbf{r})\\mathbf{w}$. Since $\\mathrm{Cov}(\\mathbf{r} \\mid s_{0}) = \\Sigma$, the objective function is:\n$$\n\\mathrm{Var}(\\hat{s}\\mid s_{0}) = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w}\n$$\nThe problem is now to minimize $\\mathbf{w}^{\\top}\\Sigma\\mathbf{w}$ subject to the linear constraint $\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1$. The parameter $a$ can be determined after finding the optimal $\\mathbf{w}$. This is a constrained optimization problem that can be solved using a Lagrange multiplier $\\lambda$. We define the Lagrangian $\\mathcal{L}$:\n$$\n\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w} - \\lambda(\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) - 1)\n$$\nTo find the minimum, we take the gradient of $\\mathcal{L}$ with respect to $\\mathbf{w}$ and set it to the zero vector. The covariance matrix $\\Sigma$ is symmetric.\n$$\n\\nabla_{\\mathbf{w}}\\mathcal{L} = 2\\Sigma\\mathbf{w} - \\lambda\\mathbf{f}'(s_{0}) = \\mathbf{0}\n$$\nThis gives $2\\Sigma\\mathbf{w} = \\lambda\\mathbf{f}'(s_{0})$. Since $\\Sigma$ is given as positive-definite, it is invertible. We can solve for $\\mathbf{w}$:\n$$\n\\mathbf{w} = \\frac{\\lambda}{2}\\Sigma^{-1}\\mathbf{f}'(s_{0})\n$$\nTo determine the Lagrange multiplier $\\lambda$, we substitute this expression for $\\mathbf{w}$ back into the constraint equation $\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1$:\n$$\n\\left(\\frac{\\lambda}{2}\\Sigma^{-1}\\mathbf{f}'(s_{0})\\right)^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\n$$\n\\frac{\\lambda}{2}(\\mathbf{f}'(s_{0}))^{\\top}(\\Sigma^{-1})^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\nSince $\\Sigma$ is symmetric, its inverse $\\Sigma^{-1}$ is also symmetric, meaning $(\\Sigma^{-1})^{\\top} = \\Sigma^{-1}$. The equation simplifies to:\n$$\n\\frac{\\lambda}{2}(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0}) = 1\n$$\nSolving for $\\lambda$:\n$$\n\\lambda = \\frac{2}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nThe denominator, $(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})$, is the Fisher information for the stimulus $s$ at $s_{0}$. Since $\\Sigma^{-1}$ is positive definite and assuming $\\mathbf{f}'(s_{0}) \\neq \\mathbf{0}$, the Fisher information is positive, and $\\lambda$ is well-defined.\nNow we substitute $\\lambda$ back into the expression for $\\mathbf{w}$ to find the optimal weight vector $\\mathbf{w}_{\\text{opt}}$:\n$$\n\\mathbf{w}_{\\text{opt}} = \\frac{1}{2}\\left(\\frac{2}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right)\\Sigma^{-1}\\mathbf{f}'(s_{0}) = \\frac{\\Sigma^{-1}\\mathbf{f}'(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nWith the optimal weight vector $\\mathbf{w}_{\\text{opt}}$, we find the optimal offset $a_{\\text{opt}}$:\n$$\na_{\\text{opt}} = s_{0} - \\mathbf{w}_{\\text{opt}}^{\\top}\\mathbf{f}(s_{0}) = s_{0} - \\frac{(\\Sigma^{-1}\\mathbf{f}'(s_{0}))^{\\top}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} = s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nFinally, we construct the optimal estimator $\\hat{s}_{\\text{opt}} = a_{\\text{opt}} + \\mathbf{w}_{\\text{opt}}^{\\top}\\mathbf{r}$:\n$$\n\\hat{s}_{\\text{opt}} = \\left(s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right) + \\left(\\frac{\\Sigma^{-1}\\mathbf{f}'(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right)^{\\top}\\mathbf{r}\n$$\n$$\n\\hat{s}_{\\text{opt}} = s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{r}}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nCombining terms, the optimal estimator for the general correlated case is:\n$$\n\\hat{s}_{\\text{corr}} = s_{0} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0}))}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nNow, we specialize this result for the uncorrelated case, where $\\Sigma = \\mathrm{diag}(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2})$. The inverse covariance matrix is $\\Sigma^{-1} = \\mathrm{diag}(\\frac{1}{\\sigma_{1}^{2}}, \\ldots, \\frac{1}{\\sigma_{n}^{2}})$. Let $f_{i}(s_{0})$, $f'_{i}(s_{0})$, and $r_{i}$ be the $i$-th components of $\\mathbf{f}(s_{0})$, $\\mathbf{f}'(s_{0})$, and $\\mathbf{r}$, respectively.\nThe denominator (Fisher information) becomes a sum:\n$$\n(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0}) = \\sum_{i=1}^{n} f'_{i}(s_{0}) \\left(\\frac{1}{\\sigma_{i}^{2}}\\right) f'_{i}(s_{0}) = \\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}\n$$\nThe numerator becomes:\n$$\n(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0})) = \\sum_{i=1}^{n} f'_{i}(s_{0}) \\left(\\frac{1}{\\sigma_{i}^{2}}\\right) (r_{i} - f_{i}(s_{0})) = \\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}\n$$\nSubstituting these sums back into the general expression gives the optimal estimator for the uncorrelated case:\n$$\n\\hat{s}_{\\text{uncorr}} = s_{0} + \\frac{\\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}}\n$$\nThis completes the derivation for both requested cases.",
            "answer": "$$\n\\boxed{\\begin{matrix} s_{0} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0}))}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}  s_{0} + \\frac{\\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}} \\end{matrix}}\n$$"
        },
        {
            "introduction": "Beyond simply computing a single best estimate of a stimulus, a sophisticated brain must represent and manipulate uncertainty. This final exercise introduces the elegant framework of Probabilistic Population Codes (PPCs), where the collective activity of neurons represents an entire probability distribution over a latent variable. You will derive the update rules for a Bayesian inference process, showing how incoming spike counts can dynamically refine a probabilistic belief state, a cornerstone of the \"Bayesian brain\" hypothesis .",
            "id": "4055165",
            "problem": "Consider a two-dimensional latent variable $\\mathbf{x} \\in \\mathbb{R}^{2}$ to be represented by a probabilistic population code (PPC). A population of $M$ neurons emits spike counts $\\{n_{i}\\}_{i=1}^{M}$ over a fixed observation window, and these counts are conditionally independent given $\\mathbf{x}$. Assume that, for each neuron $i$, the per-spike log-likelihood contribution to the stimulus is approximated by a basis function of the form\n$$\n\\phi_{i}(\\mathbf{x}) \\;=\\; \\alpha_{i} \\;+\\; \\mathbf{u}_{i}^{\\top}\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x},\n$$\nwhere $\\alpha_{i} \\in \\mathbb{R}$, $\\mathbf{u}_{i} \\in \\mathbb{R}^{2}$, and $\\mathbf{U}_{i} \\in \\mathbb{R}^{2 \\times 2}$ is symmetric and positive semidefinite. The total log-likelihood is modeled as the sum of these contributions across spikes and neurons, so that (up to an additive term independent of $\\mathbf{x}$) the observation likelihood obeys\n$$\n\\ln p\\!\\left(\\{n_{i}\\}_{i=1}^{M}\\mid \\mathbf{x}\\right) \\;=\\; \\sum_{i=1}^{M} n_{i}\\,\\phi_{i}(\\mathbf{x}) \\;+\\; \\text{const}(\\{n_{i}\\}),\n$$\nwith $\\text{const}(\\{n_{i}\\})$ not depending on $\\mathbf{x}$. Let the prior over $\\mathbf{x}$ be Gaussian with mean $\\boldsymbol{\\mu}_{0} \\in \\mathbb{R}^{2}$ and covariance $\\boldsymbol{\\Sigma}_{0} \\in \\mathbb{R}^{2\\times 2}$, whose natural (canonical) parameters are defined by\n$$\n\\boldsymbol{\\Lambda}_{0} \\;=\\; \\boldsymbol{\\Sigma}_{0}^{-1}, \\qquad \\mathbf{h}_{0} \\;=\\; \\boldsymbol{\\Lambda}_{0}\\,\\boldsymbol{\\mu}_{0}.\n$$\nThe Gaussian in canonical form is\n$$\n\\ln p(\\mathbf{x}) \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}.\n$$\nStarting from these ingredients and the standard rules for combining independent likelihoods with a prior, construct the PPC that yields a Gaussian posterior over $\\mathbf{x}$ and derive the explicit mapping from the spike counts $\\{n_{i}\\}_{i=1}^{M}$ to the posterior Gaussian natural parameters $(\\mathbf{h}, \\boldsymbol{\\Lambda})$. Express your final answer in closed form as functions of $\\{n_{i}\\}$, $\\{\\mathbf{u}_{i}\\}$, $\\{\\mathbf{U}_{i}\\}$, and $(\\mathbf{h}_{0}, \\boldsymbol{\\Lambda}_{0})$. No numerical approximation is required; provide the analytic expression only.",
            "solution": "The user wants to derive the mapping from spike counts to the posterior natural parameters of a latent variable under a specific probabilistic population coding model.\n\n### Step 1: Extract Givens\n- Latent variable: $\\mathbf{x} \\in \\mathbb{R}^{2}$.\n- Number of neurons: $M$.\n- Spike counts: $\\{n_{i}\\}_{i=1}^{M}$.\n- Conditional independence of spike counts given $\\mathbf{x}$.\n- Per-spike log-likelihood contribution for neuron $i$: $\\phi_{i}(\\mathbf{x}) = \\alpha_{i} + \\mathbf{u}_{i}^{\\top}\\mathbf{x} - \\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\mathbf{x}$, where $\\alpha_{i} \\in \\mathbb{R}$, $\\mathbf{u}_{i} \\in \\mathbb{R}^{2}$, and $\\mathbf{U}_{i} \\in \\mathbb{R}^{2 \\times 2}$ is symmetric and positive semidefinite.\n- Total log-likelihood: $\\ln p(\\{n_{i}\\}_{i=1}^{M} \\mid \\mathbf{x}) = \\sum_{i=1}^{M} n_{i}\\phi_{i}(\\mathbf{x}) + \\text{const}(\\{n_{i}\\})$.\n- Prior distribution: $p(\\mathbf{x})$ is Gaussian with mean $\\boldsymbol{\\mu}_{0}$ and covariance $\\boldsymbol{\\Sigma}_{0}$.\n- Prior canonical parameters: $\\boldsymbol{\\Lambda}_{0} = \\boldsymbol{\\Sigma}_{0}^{-1}$ and $\\mathbf{h}_{0} = \\boldsymbol{\\Lambda}_{0}\\boldsymbol{\\mu}_{0}$.\n- Log-prior in canonical form: $\\ln p(\\mathbf{x}) = -\\frac{1}{2}\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\mathbf{x} + \\mathbf{h}_{0}^{\\top}\\mathbf{x} + \\text{const}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a Bayesian inference scenario common in theoretical neuroscience and machine learning. The model is a specific instance of a Probabilistic Population Code (PPC) where both the likelihood and prior are from the exponential family (specifically, they have Gaussian forms). The product of a Gaussian likelihood and a Gaussian prior yields a Gaussian posterior, a standard result involving conjugate priors. The model is scientifically coherent and well-established.\n- **Well-Posed**: The problem is clearly stated and asks for a specific derivation: finding the parameters of the posterior distribution. The given information is sufficient to perform this derivation. The assumptions (e.g., $\\mathbf{U}_i$ is positive semidefinite, $\\boldsymbol{\\Sigma}_0$ is a covariance matrix) ensure that the resulting distributions are mathematically well-behaved.\n- **Objective**: The problem is formulated in precise mathematical language, with no ambiguity or subjective components.\n\nAll other validation criteria are met. The problem is complete, consistent, realistic within a theoretical modeling context, and verifiable by direct mathematical derivation.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. Proceeding to the solution.\n\nTo find the posterior distribution over the latent variable $\\mathbf{x}$, we use Bayes' theorem. The posterior probability density is proportional to the product of the likelihood and the prior:\n$$\np(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;\\propto\\; p(\\{n_{i}\\}_{i=1}^{M} \\mid \\mathbf{x})\\,p(\\mathbf{x})\n$$\nIt is more convenient to work with the logarithm of the posterior, as the likelihood and prior are given in logarithmic form. The log-posterior is the sum of the log-likelihood and the log-prior, up to an additive constant that does not depend on $\\mathbf{x}$:\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;=\\; \\ln p(\\{n_{i}\\}_{i=1}^{M} \\mid \\mathbf{x}) \\;+\\; \\ln p(\\mathbf{x}) \\;+\\; \\text{const}\n$$\nWe are given the expressions for the log-likelihood and the log-prior. Let's substitute them into the equation above.\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}_{i=1}^{M}) \\;=\\; \\left(\\sum_{i=1}^{M} n_{i}\\,\\phi_{i}(\\mathbf{x}) + \\text{const}_1\\right) \\;+\\; \\left(-\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} + \\mathbf{h}_{0}^{\\top}\\mathbf{x} + \\text{const}_2\\right) \\;+\\; \\text{const}\n$$\nNow, substitute the definition of $\\phi_{i}(\\mathbf{x})$:\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; \\sum_{i=1}^{M} n_{i}\\left(\\alpha_{i} + \\mathbf{u}_{i}^{\\top}\\mathbf{x} - \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x}\\right) \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}'\n$$\nwhere all constants independent of $\\mathbf{x}$ are collected into a single term $\\text{const}'$. Our goal is to rearrange this expression into the canonical form of a Gaussian log-density, which is $\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) = -\\frac{1}{2}\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}\\mathbf{x} + \\mathbf{h}^{\\top}\\mathbf{x} + \\text{const}''$. To do this, we group the terms that are quadratic in $\\mathbf{x}$ and linear in $\\mathbf{x}$.\n\nFirst, let's expand the sum:\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; \\sum_{i=1}^{M} n_{i}\\alpha_{i} \\;+\\; \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}^{\\top}\\mathbf{x} \\;-\\; \\sum_{i=1}^{M} n_{i}\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;+\\; \\text{const}'\n$$\nThe term $\\sum_{i=1}^{M} n_{i}\\alpha_{i}$ does not depend on $\\mathbf{x}$ and can be absorbed into the overall constant term. Let's group the remaining terms by their dependence on $\\mathbf{x}$.\n\nThe linear terms in $\\mathbf{x}$ are:\n$$\n\\left(\\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}^{\\top}\\mathbf{x}\\right) \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;=\\; \\left(\\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x} \\;+\\; \\mathbf{h}_{0}^{\\top}\\mathbf{x} \\;=\\; \\left(\\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x}\n$$\nThe quadratic terms in $\\mathbf{x}$ are:\n$$\n-\\frac{1}{2}\\left(\\sum_{i=1}^{M} n_{i}\\,\\mathbf{x}^{\\top}\\mathbf{U}_{i}\\,\\mathbf{x}\\right) \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x} \\;-\\; \\frac{1}{2}\\,\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}_{0}\\,\\mathbf{x} \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x}\n$$\nCombining these results, the log-posterior can be written as:\n$$\n\\ln p(\\mathbf{x} \\mid \\{n_{i}\\}) \\;=\\; -\\frac{1}{2}\\,\\mathbf{x}^{\\top}\\left(\\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\\right)\\mathbf{x} \\;+\\; \\left(\\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\\right)^{\\top}\\mathbf{x} \\;+\\; \\text{const}''\n$$\nThis expression is the log-density of a Gaussian distribution in canonical form. By comparing this to the general form $\\ln p(\\mathbf{x}) = -\\frac{1}{2}\\mathbf{x}^{\\top}\\boldsymbol{\\Lambda}\\mathbf{x} + \\mathbf{h}^{\\top}\\mathbf{x} + \\text{const}$, we can directly identify the natural parameters of the posterior distribution, $(\\mathbf{h}, \\boldsymbol{\\Lambda})$.\n\nThe posterior natural parameter $\\mathbf{h}$ (the potential vector) is the coefficient of the linear term:\n$$\n\\mathbf{h} \\;=\\; \\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{u}_{i}\n$$\nThe posterior natural parameter $\\boldsymbol{\\Lambda}$ (the precision matrix) is the matrix in the quadratic term:\n$$\n\\boldsymbol{\\Lambda} \\;=\\; \\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i}\\mathbf{U}_{i}\n$$\nThese expressions provide the explicit mapping from the spike counts $\\{n_i\\}$ to the posterior natural parameters $(\\mathbf{h}, \\boldsymbol{\\Lambda})$, given the prior parameters and the neuron basis functions. This update rule is additive in the canonical parameter space, a characteristic feature of inference in exponential families with conjugate priors.",
            "answer": "$$\n\\boxed{\\begin{matrix} \\mathbf{h}_{0} + \\sum_{i=1}^{M} n_{i} \\mathbf{u}_{i}  \\boldsymbol{\\Lambda}_{0} + \\sum_{i=1}^{M} n_{i} \\mathbf{U}_{i} \\end{matrix}}\n$$"
        }
    ]
}