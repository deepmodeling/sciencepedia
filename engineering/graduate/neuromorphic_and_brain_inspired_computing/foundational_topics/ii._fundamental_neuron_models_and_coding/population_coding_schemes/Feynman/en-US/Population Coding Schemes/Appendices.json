{
    "hands_on_practices": [
        {
            "introduction": "A fundamental question in neural coding is how precisely a population of neurons can represent a stimulus. The Fisher Information provides a powerful mathematical framework for answering this, as its inverse sets a lower bound—the Cramér-Rao Lower Bound—on the variance of any unbiased estimator. This exercise  provides hands-on practice in calculating the Fisher Information for a canonical model of a neural population, revealing how the precision of the code scales with key parameters like the number of neurons and their tuning width.",
            "id": "4055130",
            "problem": "Consider a one-dimensional stimulus $s \\in \\mathbb{R}$ encoded by a population of $N$ neurons in a neuromorphic system, each with independent Poisson spiking and identical Gaussian tuning. The mean spike count of neuron $i$ over a fixed observation window is $\\lambda_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)$, where $\\alpha > 0$ is the peak mean count, $\\sigma > 0$ is the tuning width, and $\\mu_i \\in \\mathbb{R}$ is the preferred stimulus of neuron $i$. Assume a dense tiling of preferred stimuli: the set $\\{\\mu_i\\}_{i=1}^{N}$ is uniformly spaced over an interval of length $L > 0$ with spacing $\\Delta = \\frac{L}{N}$ and hence density $\\rho = \\frac{N}{L}$. The spike counts $n_i$ are independent with $n_i \\sim \\text{Poisson}(\\lambda_i(s))$.\n\nFocus on decoding small perturbations of $s$ around a fixed operating point $s_0 \\in \\mathbb{R}$ using an Optimal Linear Estimator (OLE), defined as a linear function of the observed counts that is locally unbiased at $s_0$ and minimizes the mean-squared error to first order in $(s - s_0)$ under the Poisson noise model. Starting from first principles of independent Poisson variability and a first-order (local) linearization of the tuning curves around $s_0$, derive the asymptotic variance of the OLE estimate of $s$ as $N \\to \\infty$ under the dense tiling assumption. Your final expression must be given in terms of $\\alpha$, $\\sigma$, $L$, and $N$.\n\nExpress the final variance as a single closed-form analytic expression. No numerical rounding is required. State your final result as the variance in the units of the stimulus variable $s$ without including units in the boxed final answer.",
            "solution": "The problem requires the derivation of the asymptotic variance of an Optimal Linear Estimator (OLE) for a stimulus $s$. The variance of an OLE that is locally unbiased and minimally variant is given by the Cramér-Rao Lower Bound (CRLB), which is the inverse of the Fisher Information, $J(s)$. We will compute the Fisher Information for the specified population of neurons and then invert it to find the variance.\n\nThe model consists of $N$ neurons with independent Poisson spiking. The mean spike count of neuron $i$ is given by a Gaussian tuning curve:\n$$\n\\lambda_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\nwhere $\\alpha$ is the peak mean count, $\\sigma$ is the tuning width, and $\\mu_i$ is the preferred stimulus. The spike counts $n_i$ are independent Poisson variables, $n_i \\sim \\text{Poisson}(\\lambda_i(s))$.\n\nThe Fisher Information, $J(s)$, for a population of independent Poisson neurons can be calculated as:\n$$\nJ(s) = \\sum_{i=1}^{N} \\frac{\\left(\\frac{d\\lambda_i(s)}{ds}\\right)^2}{\\lambda_i(s)}\n$$\nLet's denote the derivative of $\\lambda_i(s)$ with respect to $s$ as $\\lambda'_i(s)$. First, we compute this derivative:\n$$\n\\lambda'_i(s) = \\frac{d}{ds} \\left[ \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right) \\right]\n$$\nUsing the chain rule, we get:\n$$\n\\lambda'_i(s) = \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right) \\cdot \\left( -\\frac{2(s - \\mu_i)}{2\\sigma^2} \\right) = -\\frac{s - \\mu_i}{\\sigma^2} \\lambda_i(s)\n$$\nNow, we can compute the term in the sum for the Fisher Information:\n$$\n\\frac{(\\lambda'_i(s))^2}{\\lambda_i(s)} = \\frac{\\left( -\\frac{s - \\mu_i}{\\sigma^2} \\lambda_i(s) \\right)^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\frac{\\lambda_i(s)^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\lambda_i(s)\n$$\nSubstituting the expression for $\\lambda_i(s)$:\n$$\n\\frac{(\\lambda'_i(s))^2}{\\lambda_i(s)} = \\frac{(s - \\mu_i)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\nThe total Fisher Information at the operating point $s_0$ is the sum over all $N$ neurons:\n$$\nJ(s_0) = \\sum_{i=1}^{N} \\frac{(s_0 - \\mu_i)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s_0 - \\mu_i)^{2}}{2 \\sigma^{2}}\\right)\n$$\nThe problem specifies a dense tiling assumption as $N \\to \\infty$. The preferred stimuli $\\{\\mu_i\\}$ are uniformly spaced over an interval of length $L$, with density $\\rho = \\frac{N}{L}$. In this limit, the discrete sum can be replaced by an integral over the variable $\\mu$:\n$$\n\\sum_{i=1}^{N} f(\\mu_i) \\to \\int f(\\mu) \\rho \\, d\\mu\n$$\nApplying this to the expression for $J(s_0)$:\n$$\nJ(s_0) = \\int \\rho \\, \\frac{(s_0 - \\mu)^2}{\\sigma^4} \\alpha \\exp\\left(-\\frac{(s_0 - \\mu)^{2}}{2 \\sigma^{2}}\\right) d\\mu\n$$\nThe integral is over the range of preferred stimuli, which is an interval of length $L$. However, since the Gaussian term decays rapidly for $|\\mu - s_0| \\gg \\sigma$, if we assume the stimulus range $L$ is large compared to the tuning width $\\sigma$, we can approximate the integral by extending the limits to $(-\\infty, \\infty)$ without significant error. This is a standard procedure in this context.\n$$\nJ(s_0) \\approx \\rho \\alpha \\int_{-\\infty}^{\\infty} \\frac{(s_0 - \\mu)^2}{\\sigma^4} \\exp\\left(-\\frac{(s_0 - \\mu)^{2}}{2 \\sigma^{2}}\\right) d\\mu\n$$\nTo solve this integral, we perform a change of variables. Let $x = \\frac{\\mu - s_0}{\\sigma}$. This implies $d\\mu = \\sigma dx$ and $(s_0 - \\mu)^2 = (-\\sigma x)^2 = \\sigma^2 x^2$. The integration limits remain unchanged.\n$$\nJ(s_0) \\approx \\rho \\alpha \\int_{-\\infty}^{\\infty} \\frac{\\sigma^2 x^2}{\\sigma^4} \\exp\\left(-\\frac{x^2}{2}\\right) (\\sigma \\, dx)\n$$\n$$\nJ(s_0) \\approx \\frac{\\rho \\alpha}{\\sigma} \\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nThe integral $\\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx$ is a standard result. It can be related to the variance of a standard normal distribution $N(0,1)$, which has a probability density function $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$. The variance is $1$, and is given by $E[X^2] - (E[X])^2 = \\int_{-\\infty}^{\\infty} x^2 \\phi(x) dx - 0^2 = 1$. Thus:\n$$\n\\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx = 1\n$$\nThis implies:\n$$\n\\int_{-\\infty}^{\\infty} x^2 \\exp\\left(-\\frac{x^2}{2}\\right) dx = \\sqrt{2\\pi}\n$$\nSubstituting this back into the expression for $J(s_0)$:\n$$\nJ(s_0) \\approx \\frac{\\rho \\alpha}{\\sigma} \\sqrt{2\\pi}\n$$\nThe problem requires the final expression in terms of $N$ and $L$. Using the density $\\rho = \\frac{N}{L}$, we have:\n$$\nJ(s_0) \\approx \\frac{N}{L} \\frac{\\alpha \\sqrt{2\\pi}}{\\sigma} = \\frac{\\sqrt{2\\pi} \\alpha N}{L \\sigma}\n$$\nThe variance of the OLE, $\\text{Var}(\\hat{s})$, is the inverse of the Fisher Information:\n$$\n\\text{Var}(\\hat{s}) = J(s_0)^{-1} \\approx \\left( \\frac{\\sqrt{2\\pi} \\alpha N}{L \\sigma} \\right)^{-1}\n$$\nThis gives the final expression for the asymptotic variance:\n$$\n\\text{Var}(\\hat{s}) = \\frac{L \\sigma}{\\sqrt{2\\pi} \\alpha N}\n$$\nThis result is independent of the operating point $s_0$, which is a consequence of the uniform density of preferred stimuli and the approximation of the integral over an infinite domain.",
            "answer": "$$ \\boxed{ \\frac{L \\sigma}{\\sqrt{2\\pi} \\alpha N} } $$"
        },
        {
            "introduction": "While the Fisher Information tells us the theoretical limit of decoding precision, we also need methods to construct estimators that can achieve this performance. This practice  guides you through the derivation of an Optimal Linear Estimator (OLE), a widely used tool for reading out information from neural populations. The exercise specifically addresses the challenge of correlated noise across neurons, a common feature of biological circuits, demonstrating how to properly account for the noise covariance structure to build a truly optimal decoder.",
            "id": "4055126",
            "problem": "Consider a population code with $n$ neurons whose mean response to a scalar stimulus $s \\in \\mathbb{R}$ is given by the tuning curve $\\mathbf{f}(s) \\in \\mathbb{R}^{n}$. The observed population response $\\mathbf{r} \\in \\mathbb{R}^{n}$ follows the probabilistic model $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s),\\Sigma)$, where $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a stimulus-independent, positive-definite noise covariance that may be correlated across neurons. Fix a reference stimulus $s_{0}$ and assume a locally linear regime where $\\mathbf{f}(s)$ is differentiable and can be approximated by its first-order Taylor expansion around $s_{0}$, with gradient $\\mathbf{f}'(s_{0}) = \\frac{d\\mathbf{f}}{ds}\\big|_{s_{0}} \\in \\mathbb{R}^{n}$. Consider linear estimators of the form $\\hat{s} = a + \\mathbf{w}^{\\top}\\mathbf{r}$, where $a \\in \\mathbb{R}$ and $\\mathbf{w} \\in \\mathbb{R}^{n}$. Impose the local unbiasedness constraints at $s_{0}$: $\\mathbb{E}[\\hat{s}\\mid s_{0}] = s_{0}$ and $\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s]\\big|_{s_{0}} = 1$. Among all such locally unbiased linear estimators, derive the estimator that minimizes $\\mathrm{Var}(\\hat{s}\\mid s_{0})$ under the correlated Gaussian noise $\\mathcal{N}(\\mathbf{f}(s),\\Sigma)$. Then specialize your expression to the uncorrelated case $\\Sigma = \\mathrm{diag}(\\sigma_{1}^{2},\\ldots,\\sigma_{n}^{2})$, with $\\sigma_{i}>0$ for $i=1,\\ldots,n$. Express your final answer as closed-form analytic expressions for the optimal estimator at $s_{0}$ in both cases, written in terms of $\\mathbf{f}(s_{0})$, $\\mathbf{f}'(s_{0})$, $\\Sigma$, and $\\mathbf{r}$. No numerical evaluation or rounding is required. Do not introduce any additional assumptions beyond those stated. For clarity of terminology, when referring to the estimator, use the explicit expression rather than invoking the Best Linear Unbiased Estimator (BLUE) by name.",
            "solution": "The user wants to find the optimal linear estimator for a scalar stimulus $s$ based on a neural population response $\\mathbf{r}$. The problem will be solved by first formalizing the optimization problem and then solving it using the method of Lagrange multipliers.\n\nThe problem defines a linear estimator of the form $\\hat{s} = a + \\mathbf{w}^{\\top}\\mathbf{r}$, where $a \\in \\mathbb{R}$ is a scalar offset and $\\mathbf{w} \\in \\mathbb{R}^{n}$ is a vector of weights. The population response $\\mathbf{r}$ is a random vector drawn from a multivariate Gaussian distribution, $\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{f}(s),\\Sigma)$, where $\\mathbf{f}(s)$ is the vector of mean neural responses (tuning curves) and $\\Sigma$ is the stimulus-independent noise covariance matrix.\n\nThe first step is to express the two local unbiasedness constraints in terms of the estimator parameters $a$ and $\\mathbf{w}$. The expected value of the estimator, conditioned on the stimulus $s$, is:\n$$\n\\mathbb{E}[\\hat{s}\\mid s] = \\mathbb{E}[a + \\mathbf{w}^{\\top}\\mathbf{r}\\mid s] = a + \\mathbf{w}^{\\top}\\mathbb{E}[\\mathbf{r}\\mid s] = a + \\mathbf{w}^{\\top}\\mathbf{f}(s)\n$$\nThe first constraint, local unbiasedness at the reference stimulus $s_{0}$, is $\\mathbb{E}[\\hat{s}\\mid s_{0}] = s_{0}$. Applying this to the expression for the expectation gives:\n$$\na + \\mathbf{w}^{\\top}\\mathbf{f}(s_{0}) = s_{0}\n$$\nThis constraint allows us to express $a$ in terms of $\\mathbf{w}$:\n$$\na = s_{0} - \\mathbf{w}^{\\top}\\mathbf{f}(s_{0})\n$$\nThe second constraint involves the derivative of the expectation with respect to the stimulus, evaluated at $s_0$: $\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s]\\big|_{s_{0}} = 1$. We first compute the derivative:\n$$\n\\frac{d}{ds}\\mathbb{E}[\\hat{s}\\mid s] = \\frac{d}{ds} \\left( a + \\mathbf{w}^{\\top}\\mathbf{f}(s) \\right) = \\mathbf{w}^{\\top}\\frac{d\\mathbf{f}(s)}{ds} = \\mathbf{w}^{\\top}\\mathbf{f}'(s)\n$$\nEvaluating this at $s=s_{0}$ and applying the constraint yields:\n$$\n\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\nThis is a constraint on the weight vector $\\mathbf{w}$.\n\nNext, we identify the objective function to be minimized, which is the variance of the estimator at $s_{0}$, $\\mathrm{Var}(\\hat{s}\\mid s_{0})$.\n$$\n\\mathrm{Var}(\\hat{s}\\mid s_{0}) = \\mathrm{Var}(a + \\mathbf{w}^{\\top}\\mathbf{r}\\mid s_{0}) = \\mathrm{Var}(\\mathbf{w}^{\\top}\\mathbf{r}\\mid s_{0})\n$$\nThe variance of a linear transformation of a random vector is given by $\\mathbf{w}^{\\top}\\mathrm{Cov}(\\mathbf{r})\\mathbf{w}$. Since $\\mathrm{Cov}(\\mathbf{r} \\mid s_{0}) = \\Sigma$, the objective function is:\n$$\n\\mathrm{Var}(\\hat{s}\\mid s_{0}) = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w}\n$$\nThe problem is now to minimize $\\mathbf{w}^{\\top}\\Sigma\\mathbf{w}$ subject to the linear constraint $\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1$. The parameter $a$ can be determined after finding the optimal $\\mathbf{w}$. This is a constrained optimization problem that can be solved using a Lagrange multiplier $\\lambda$. We define the Lagrangian $\\mathcal{L}$:\n$$\n\\mathcal{L}(\\mathbf{w}, \\lambda) = \\mathbf{w}^{\\top}\\Sigma\\mathbf{w} - \\lambda(\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) - 1)\n$$\nTo find the minimum, we take the gradient of $\\mathcal{L}$ with respect to $\\mathbf{w}$ and set it to the zero vector. The covariance matrix $\\Sigma$ is symmetric.\n$$\n\\nabla_{\\mathbf{w}}\\mathcal{L} = 2\\Sigma\\mathbf{w} - \\lambda\\mathbf{f}'(s_{0}) = \\mathbf{0}\n$$\nThis gives $2\\Sigma\\mathbf{w} = \\lambda\\mathbf{f}'(s_{0})$. Since $\\Sigma$ is given as positive-definite, it is invertible. We can solve for $\\mathbf{w}$:\n$$\n\\mathbf{w} = \\frac{\\lambda}{2}\\Sigma^{-1}\\mathbf{f}'(s_{0})\n$$\nTo determine the Lagrange multiplier $\\lambda$, we substitute this expression for $\\mathbf{w}$ back into the constraint equation $\\mathbf{w}^{\\top}\\mathbf{f}'(s_{0}) = 1$:\n$$\n\\left(\\frac{\\lambda}{2}\\Sigma^{-1}\\mathbf{f}'(s_{0})\\right)^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\n$$\n\\frac{\\lambda}{2}(\\mathbf{f}'(s_{0}))^{\\top}(\\Sigma^{-1})^{\\top}\\mathbf{f}'(s_{0}) = 1\n$$\nSince $\\Sigma$ is symmetric, its inverse $\\Sigma^{-1}$ is also symmetric, meaning $(\\Sigma^{-1})^{\\top} = \\Sigma^{-1}$. The equation simplifies to:\n$$\n\\frac{\\lambda}{2}(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0}) = 1\n$$\nSolving for $\\lambda$:\n$$\n\\lambda = \\frac{2}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nThe denominator, $(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})$, is the Fisher information for the stimulus $s$ at $s_{0}$. Since $\\Sigma^{-1}$ is positive definite and assuming $\\mathbf{f}'(s_{0}) \\neq \\mathbf{0}$, the Fisher information is positive, and $\\lambda$ is well-defined.\nNow we substitute $\\lambda$ back into the expression for $\\mathbf{w}$ to find the optimal weight vector $\\mathbf{w}_{\\text{opt}}$:\n$$\n\\mathbf{w}_{\\text{opt}} = \\frac{1}{2}\\left(\\frac{2}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right)\\Sigma^{-1}\\mathbf{f}'(s_{0}) = \\frac{\\Sigma^{-1}\\mathbf{f}'(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nWith the optimal weight vector $\\mathbf{w}_{\\text{opt}}$, we find the optimal offset $a_{\\text{opt}}$:\n$$\na_{\\text{opt}} = s_{0} - \\mathbf{w}_{\\text{opt}}^{\\top}\\mathbf{f}(s_{0}) = s_{0} - \\frac{(\\Sigma^{-1}\\mathbf{f}'(s_{0}))^{\\top}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} = s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nFinally, we construct the optimal estimator $\\hat{s}_{\\text{opt}} = a_{\\text{opt}} + \\mathbf{w}_{\\text{opt}}^{\\top}\\mathbf{r}$:\n$$\n\\hat{s}_{\\text{opt}} = \\left(s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right) + \\left(\\frac{\\Sigma^{-1}\\mathbf{f}'(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\\right)^{\\top}\\mathbf{r}\n$$\n$$\n\\hat{s}_{\\text{opt}} = s_{0} - \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}(s_{0})}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{r}}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nCombining terms, the optimal estimator for the general correlated case is:\n$$\n\\hat{s}_{\\text{corr}} = s_{0} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0}))}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})}\n$$\nNow, we specialize this result for the uncorrelated case, where $\\Sigma = \\mathrm{diag}(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2})$. The inverse covariance matrix is $\\Sigma^{-1} = \\mathrm{diag}(\\frac{1}{\\sigma_{1}^{2}}, \\ldots, \\frac{1}{\\sigma_{n}^{2}})$. Let $f_{i}(s_{0})$, $f'_{i}(s_{0})$, and $r_{i}$ be the $i$-th components of $\\mathbf{f}(s_{0})$, $\\mathbf{f}'(s_{0})$, and $\\mathbf{r}$, respectively.\nThe denominator (Fisher information) becomes a sum:\n$$\n(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0}) = \\sum_{i=1}^{n} f'_{i}(s_{0}) \\left(\\frac{1}{\\sigma_{i}^{2}}\\right) f'_{i}(s_{0}) = \\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}\n$$\nThe numerator becomes:\n$$\n(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0})) = \\sum_{i=1}^{n} f'_{i}(s_{0}) \\left(\\frac{1}{\\sigma_{i}^{2}}\\right) (r_{i} - f_{i}(s_{0})) = \\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}\n$$\nSubstituting these sums back into the general expression gives the optimal estimator for the uncorrelated case:\n$$\n\\hat{s}_{\\text{uncorr}} = s_{0} + \\frac{\\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}}\n$$\nThis completes the derivation for both requested cases.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} s_{0} + \\frac{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}(\\mathbf{r} - \\mathbf{f}(s_{0}))}{(\\mathbf{f}'(s_{0}))^{\\top}\\Sigma^{-1}\\mathbf{f}'(s_{0})} & s_{0} + \\frac{\\sum_{i=1}^{n} \\frac{f'_{i}(s_{0})(r_{i} - f_{i}(s_{0}))}{\\sigma_{i}^{2}}}{\\sum_{i=1}^{n} \\frac{(f'_{i}(s_{0}))^{2}}{\\sigma_{i}^{2}}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real neurons can have complex tuning properties that go beyond simple, unimodal shapes, potentially creating ambiguity where a single neural response could correspond to multiple different stimuli. This exercise  explores the fundamental problem of decoding ambiguity that arises from bimodal tuning curves. By analyzing this scenario, you will develop a deeper appreciation for how diversity across a population—heterogeneity in tuning—is not a source of noise, but a critical feature that enables the code to be unambiguous and robust.",
            "id": "4055129",
            "problem": "Consider a neuromorphic array of $N$ neurons indexed by $i \\in \\{1,\\dots,N\\}$ that encodes a scalar stimulus $s \\in \\mathbb{R}$ via tuning curves $f_i(s)$, where each neuron emits a spike count $r_i$ in a window of duration $\\Delta t$ according to an independent Poisson process with conditional mean $f_i(s)\\Delta t$. The population likelihood is $p(\\mathbf{r} \\mid s) = \\prod_{i=1}^N \\mathrm{Poisson}(r_i; f_i(s)\\Delta t)$, where $\\mathbf{r} = (r_1,\\dots,r_N)$. Assume that for many neurons $f_i(s)$ is bimodal with two comparable peaks centered at distinct stimuli $s_a \\neq s_b$, and that for a large subset of neurons these peaks are aligned (i.e., the two maxima occur at the same $s_a$ and $s_b$ across neurons and have similar peak heights).\n\nUsing only the independence assumption, the Poisson generative model, and the definitions of Maximum Likelihood (ML) and Bayesian inference, reason from first principles about the decoding problem and address the following:\n\n$1.$ Explain the form of ambiguity this bimodality introduces for ML decoding of $s$ from $\\mathbf{r}$, and formalize a condition under which two distinct stimuli $s_a$ and $s_b$ yield identical population likelihoods $p(\\mathbf{r}\\mid s_a) = p(\\mathbf{r}\\mid s_b)$ for all observed $\\mathbf{r}$.\n\n$2.$ Propose and justify a principled disambiguation strategy that leverages population diversity in $f_i(s)$, without resorting to arbitrary external constraints, so that the joint population code can distinguish $s_a$ from $s_b$.\n\nSelect the option that most accurately articulates the ambiguity and provides a principled resolution that explicitly exploits population diversity.\n\nA. When $f_i(s)$ has two aligned peaks at $s_a$ and $s_b$ for most neurons and $f_i(s_a) = f_i(s_b)$ for all $i$, ML decoding is ambiguous because $p(\\mathbf{r}\\mid s_a) = p(\\mathbf{r}\\mid s_b)$ for any $\\mathbf{r}$. A principled fix is to induce heterogeneity such that the population response map $\\mathbf{f}(s) = (f_1(s),\\dots,f_N(s))$ is injective (e.g., by shifting peak locations or phases across neurons) and then decode using the full joint likelihood or Bayesian posterior over the diverse population.\n\nB. The ambiguity arises entirely from neural noise; increasing $\\Delta t$ sufficiently will concentrate the likelihood at a single mode, resolving bimodality even if $f_i(s_a) = f_i(s_b)$ holds for all $i$. Therefore, time-averaging alone is a principled and sufficient solution.\n\nC. The ambiguity is due to noise correlations between neurons; whitening the population response to decorrelate $\\mathbf{r}$ guarantees a unique likelihood maximum for any $f_i(s)$, eliminating the need for diversity in tuning.\n\nD. The ambiguity can be removed by collapsing the population to the scalar average rate $\\bar{r} = \\frac{1}{N}\\sum_{i=1}^N r_i$ and using a monotonic mapping from $\\bar{r}$ to $s$, avoiding reliance on heterogeneity in $f_i(s)$.",
            "solution": "The user wants me to act as a meticulous STEM professor and solve a problem on population coding in neuromorphic systems.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified protocol.\n\n**Step 1: Extract Givens**\n- A neuromorphic array of $N$ neurons, indexed by $i \\in \\{1, \\dots, N\\}$.\n- A scalar stimulus $s \\in \\mathbb{R}$.\n- Neuronal tuning curves are given by $f_i(s)$.\n- The spike count of neuron $i$ in a time window $\\Delta t$ is $r_i$.\n- Each $r_i$ is drawn from an independent Poisson process with conditional mean $f_i(s)\\Delta t$.\n- The population likelihood is $p(\\mathbf{r} \\mid s) = \\prod_{i=1}^N \\mathrm{Poisson}(r_i; f_i(s)\\Delta t)$, where $\\mathbf{r} = (r_1, \\dots, r_N)$.\n- For many neurons, the tuning curve $f_i(s)$ is bimodal with two comparable peaks at distinct stimuli $s_a \\neq s_b$.\n- For a large subset of these bimodal neurons, the peak locations ($s_a$ and $s_b$) and heights are aligned.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on a standard and well-established framework in computational neuroscience and neuromorphic engineering. The Poisson model for spike counts, the concept of tuning curves, population coding, and Maximum Likelihood (ML) decoding are all fundamental principles in the field. The scenario is scientifically sound.\n- **Well-Posed:** The problem is well-posed. It asks for a conceptual explanation of an ambiguity and a principled method for its resolution, based on the provided model. The question is clear and leads to a meaningful discussion of neural coding principles.\n- **Objective:** The language is formal and technical, free of subjective or ambiguous terminology.\n- **Completeness and Consistency:** The problem provides all necessary definitions and conditions to analyze the decoding ambiguity. The assumptions (independence, Poisson statistics, bimodal tuning curves) are explicitly stated and are not contradictory.\n- **Realism:** Bimodal and heterogeneous tuning curves are observed in biological sensory systems, so the premise is plausible.\n- **Triviality/Tautology:** The problem is not trivial. It addresses the core concept of how population heterogeneity is crucial for resolving coding ambiguities, a non-obvious and important principle in neural computation.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically grounded, well-posed, and internally consistent. I will proceed with the derivation and solution.\n\n### Derivation and Solution\n\nThe problem asks for two things: $1.$ an explanation and formalization of the decoding ambiguity, and $2.$ a principled strategy for its resolution using population diversity.\n\n**1. Ambiguity in Maximum Likelihood (ML) Decoding**\n\nThe goal of ML decoding is to find the stimulus $\\hat{s}_{ML}$ that maximizes the likelihood of observing the spike count vector $\\mathbf{r}$.\n$$ \\hat{s}_{ML} = \\arg\\max_s p(\\mathbf{r} \\mid s) $$\nThe likelihood function is given by the product of independent Poisson probabilities:\n$$ p(\\mathbf{r} \\mid s) = \\prod_{i=1}^N \\frac{(f_i(s)\\Delta t)^{r_i} e^{-f_i(s)\\Delta t}}{r_i!} $$\nIt is often more convenient to work with the log-likelihood, $\\mathcal{L}(s \\mid \\mathbf{r}) = \\log p(\\mathbf{r} \\mid s)$, since the logarithm is a monotonic function and does not change the location of the maximum.\n$$ \\mathcal{L}(s \\mid \\mathbf{r}) = \\log \\left( \\prod_{i=1}^N \\frac{(f_i(s)\\Delta t)^{r_i} e^{-f_i(s)\\Delta t}}{r_i!} \\right) = \\sum_{i=1}^N \\left( r_i \\log(f_i(s)\\Delta t) - f_i(s)\\Delta t - \\log(r_i!) \\right) $$\nTo find the maximum, we can discard terms that are constant with respect to $s$:\n$$ \\hat{s}_{ML} = \\arg\\max_s \\sum_{i=1}^N \\left( r_i \\log f_i(s) - f_i(s)\\Delta t \\right) $$\nAn ambiguity arises in ML decoding if two distinct stimuli, $s_a$ and $s_b$, produce the same likelihood value for any observed response vector $\\mathbf{r}$. This means $p(\\mathbf{r} \\mid s_a) = p(\\mathbfr \\mid s_b)$ for all possible $\\mathbf{r}$. If this condition holds, the decoder has no basis for choosing between $s_a$ and $s_b$.\n\nLet's formalize the condition under which this equality holds. For the log-likelihoods to be equal for any vector of integer spike counts $\\mathbf{r}=(r_1, ..., r_N)$:\n$$ \\mathcal{L}(s_a \\mid \\mathbf{r}) = \\mathcal{L}(s_b \\mid \\mathbf{r}) $$\n$$ \\sum_{i=1}^N \\left( r_i \\log f_i(s_a) - f_i(s_a)\\Delta t \\right) = \\sum_{i=1}^N \\left( r_i \\log f_i(s_b) - f_i(s_b)\\Delta t \\right) $$\nRearranging the terms, we get:\n$$ \\sum_{i=1}^N r_i \\left( \\log f_i(s_a) - \\log f_i(s_b) \\right) = \\Delta t \\sum_{i=1}^N \\left( f_i(s_a) - f_i(s_b) \\right) $$\nThis equation must hold for all possible non-negative integer vectors $\\mathbf{r}$. This is a strong constraint. For it to be universally true, the coefficient of each $r_i$ must be zero. If any coefficient were non-zero, we could choose an $\\mathbf{r}$ that violates the equality (e.g., set only that $r_i$ to a large value).\nThus, for each neuron $i \\in \\{1, \\dots, N\\}$, we must have:\n$$ \\log f_i(s_a) - \\log f_i(s_b) = 0 \\implies \\log f_i(s_a) = \\log f_i(s_b) \\implies f_i(s_a) = f_i(s_b) $$\nIf this condition holds for all $i$, the right-hand side of the equation also becomes zero, satisfying the equality. The formal condition for this fundamental ambiguity is that the vector of mean firing rates, which we can define as the population response map $\\mathbf{f}(s) = (f_1(s), \\dots, f_N(s))$, must be identical for the two stimuli:\n$$ \\mathbf{f}(s_a) = \\mathbf{f}(s_b) $$\nIf this condition is met, the mapping from the stimulus $s$ to the population response vector $\\mathbf{f}(s)$ is not injective.\n\n**2. Principled Disambiguation Strategy**\n\nThe ambiguity arises from the non-injective nature of the population response map. A principled way to resolve this ambiguity is to ensure that the mapping becomes injective over the stimulus range of interest, so that $\\mathbf{f}(s_a) \\neq \\mathbf{f}(s_b)$. This requires that for at least one neuron $j$, $f_j(s_a) \\neq f_j(s_b)$.\n\nThis is precisely where population diversity is essential. If the population were perfectly homogeneous, such that all neurons have bimodal tuning curves with $f_i(s_a) = f_i(s_b)$ for all $i$, the ambiguity would be unbreakable.\n\nA diverse or heterogeneous population can break this symmetry. For example:\n- Some neurons might have bimodal tuning curves where the peak at $s_a$ is higher than at $s_b$ (i.e., $f_i(s_a) > f_i(s_b)$).\n- Other neurons might have the opposite preference ($f_j(s_a) < f_j(s_b)$).\n- Yet other neurons might be unimodal, with a peak near $s_a$ but far from $s_b$, or vice versa.\n- The relative phases or locations of the bimodal peaks could be shifted across neurons.\n\nAs long as the population contains a mix of neurons such that the overall population response vector is different for $s_a$ and $s_b$, the likelihoods $p(\\mathbf{r} \\mid s_a)$ and $p(\\mathbf{r} \\mid s_b)$ will not be identical. The ML decoder can then exploit these differences to uniquely identify the stimulus. For instance, if a stimulus $s_a$ is presented, neurons that prefer $s_a$ over $s_b$ will fire more, on average, than those that prefer $s_b$ over $s_a$. This pattern in the full population response $\\mathbf{r}$ allows the decoder to distinguish $s_a$ from $s_b$.\n\nTherefore, the principled strategy is to leverage a heterogeneous population of neurons where the response map $\\mathbf{f}(s)$ is injective, and to use a decoder (like ML or a Bayesian posterior-based decoder) that considers the joint activity of this diverse population.\n\n### Option-by-Option Analysis\n\n**A. When $f_i(s)$ has two aligned peaks at $s_a$ and $s_b$ for most neurons and $f_i(s_a) = f_i(s_b)$ for all $i$, ML decoding is ambiguous because $p(\\mathbf{r}\\mid s_a) = p(\\mathbf{r}\\mid s_b)$ for any $\\mathbf{r}$. A principled fix is to induce heterogeneity such that the population response map $\\mathbf{f}(s) = (f_1(s),\\dots,f_N(s))$ is injective (e.g., by shifting peak locations or phases across neurons) and then decode using the full joint likelihood or Bayesian posterior over the diverse population.**\nThis option correctly identifies the formal condition for ambiguity, $f_i(s_a) = f_i(s_b)$ for all $i$, which leads to identical likelihoods. It correctly proposes that the solution is to make the population response map $\\mathbf{f}(s)$ injective through heterogeneity in tuning curves. This directly leverages population diversity as requested. The examples of heterogeneity (shifting peaks) and the mention of using the full joint likelihood are entirely correct. **Correct.**\n\n**B. The ambiguity arises entirely from neural noise; increasing $\\Delta t$ sufficiently will concentrate the likelihood at a single mode, resolving bimodality even if $f_i(s_a) = f_i(s_b)$ holds for all $i$. Therefore, time-averaging alone is a principled and sufficient solution.**\nThis is incorrect. The ambiguity is fundamental and arises from the tuning curves themselves, not the stochasticity (noise). If $f_i(s_a) = f_i(s_b)$ for all $i$, then the mean firing rate vectors are identical. Increasing the observation time $\\Delta t$ reduces the variance of the rate estimates, but the expected values remain identical. Even with infinite time (zero noise), one would observe the same firing rates for $s_a$ and $s_b$, making them indistinguishable. **Incorrect.**\n\n**C. The ambiguity is due to noise correlations between neurons; whitening the population response to decorrelate $\\mathbf{r}$ guarantees a unique likelihood maximum for any $f_i(s)$, eliminating the need for diversity in tuning.**\nThis is incorrect. The problem explicitly states that the neurons are independent, meaning there are no noise correlations. The likelihood function $p(\\mathbf{r} \\mid s) = \\prod_i p(r_i \\mid s)$ is a direct consequence of this independence. The ambiguity is caused by the mean firing rates, not the covariance structure of the noise. **Incorrect.**\n\n**D. The ambiguity can be removed by collapsing the population to the scalar average rate $\\bar{r} = \\frac{1}{N}\\sum_{i=1}^N r_i$ and using a monotonic mapping from $\\bar{r}$ to $s$, avoiding reliance on heterogeneity in $f_i(s)$.**\nThis is incorrect and proposes the opposite of the correct solution. If $f_i(s_a) = f_i(s_b)$ for all $i$, then the expected average rate is also identical: $E[\\bar{r} \\mid s_a] = \\frac{\\Delta t}{N}\\sum_i f_i(s_a) = \\frac{\\Delta t}{N}\\sum_i f_i(s_b) = E[\\bar{r} \\mid s_b]$. Averaging the responses throws away the very information that could be used to disambiguate the stimuli in a heterogeneous population. This strategy exacerbates, rather than solves, the problem. The resolution requires exploiting patterns across the diverse population, not collapsing it to a scalar mean. **Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}