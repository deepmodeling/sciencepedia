{
    "hands_on_practices": [
        {
            "introduction": "The McCulloch-Pitts neuron is more than just a model of biological computation; it is also one of the earliest and simplest linear classifiers. This practice bridges the historical model with the modern machine learning concept of linear separability, where finding the correct weights and threshold is equivalent to finding a hyperplane that separates data points into different classes. You will formalize this intuition by setting up a linear program to find an optimal separating hyperplane for a given dataset, thereby connecting the neuron's function to the powerful framework of convex optimization .",
            "id": "4065085",
            "problem": "Consider the McCulloch-Pitts neuron, which outputs a binary decision according to the rule that an input vector $\\mathbf{x} \\in \\{0,1\\}^{n}$ is classified as $1$ if and only if the weighted sum $\\sum_{i=1}^{n} w_{i} x_{i}$ is at least a threshold $\\theta$, and otherwise classified as $0$. Formally, the output is $f(\\mathbf{x}) = H\\!\\left(\\sum_{i=1}^{n} w_{i} x_{i} - \\theta\\right)$, where $H(\\cdot)$ is the Heaviside step function.\n\nYou are given a training set of labeled binary points $\\{(\\mathbf{x}_{i}, t_{i})\\}_{i=1}^{m}$ with $\\mathbf{x}_{i} \\in \\{0,1\\}^{n}$ and $t_{i} \\in \\{0,1\\}$. Starting only from the definition of the McCulloch-Pitts neuron and the fundamental geometric notion of linear separability, derive a linear programming formulation that searches for weights $\\mathbf{w} \\in \\mathbb{R}^{n}$ and threshold $\\theta \\in \\mathbb{R}$ which achieve strict separation with a nonnegative margin $\\gamma \\geq 0$, while controlling scale via an $\\ell_{1}$-norm bound on the weights. Your formulation must:\n- encode correct classification constraints in a way that a strictly positive margin corresponds to strict separability,\n- use variables and linear constraints to represent the $\\ell_{1}$-norm bound $\\|\\mathbf{w}\\|_{1} \\leq 1$,\n- maximize the margin $\\gamma$.\n\nThen, apply your formulation to the specific dataset in $\\mathbb{R}^{2}$:\n- positive class ($t_{i} = 1$): $\\mathbf{x}_{1} = (1,1)$, $\\mathbf{x}_{2} = (1,0)$,\n- negative class ($t_{i} = 0$): $\\mathbf{x}_{3} = (0,1)$, $\\mathbf{x}_{4} = (0,0)$.\n\nSolve for the optimal margin $\\gamma^{\\star}$ exactly under the $\\ell_{1}$-normalization constraint $\\|\\mathbf{w}\\|_{1} \\leq 1$. Provide the value of $\\gamma^{\\star}$ as your final answer.\n\nFinally, from first principles of convex separation, discuss necessary and sufficient conditions for feasibility of your linear program in the sense $\\gamma  0$ and in the weaker sense $\\gamma \\geq 0$. Your discussion should be grounded in well-tested facts about convex hulls and hyperplane separation and should not rely on any unintroduced shortcut formulas.\n\nYour final answer must be a single real-valued number. No rounding is required, but if you choose to provide an approximation, also give the exact value.",
            "solution": "The McCulloch-Pitts neuron is a threshold gate: given $\\mathbf{x} \\in \\{0,1\\}^{n}$ and parameters $(\\mathbf{w}, \\theta)$, it outputs $f(\\mathbf{x}) = 1$ if $\\sum_{i=1}^{n} w_{i} x_{i} \\geq \\theta$ and $f(\\mathbf{x}) = 0$ otherwise. Correct classification for a labeled example $(\\mathbf{x}_{i}, t_{i})$ therefore requires\n- if $t_{i} = 1$, then $\\sum_{j=1}^{n} w_{j} x_{ij} \\geq \\theta$,\n- if $t_{i} = 0$, then $\\sum_{j=1}^{n} w_{j} x_{ij}  \\theta$.\n\nTo encode strict separability within a linear optimization framework, we introduce a margin $\\gamma \\geq 0$ and rewrite the two conditions so that the inequalities become separated by at least $\\gamma$:\n- if $t_{i} = 1$, enforce $\\sum_{j=1}^{n} w_{j} x_{ij} - \\theta \\geq \\gamma$,\n- if $t_{i} = 0$, enforce $\\theta - \\sum_{j=1}^{n} w_{j} x_{ij} \\geq \\gamma$.\n\nEquivalently, we can introduce a sign-labeled target $y_{i} \\in \\{-1, +1\\}$ via $y_{i} = 2 t_{i} - 1$, and write the unified linear constraints\n$$\ny_{i}\\left(\\sum_{j=1}^{n} w_{j} x_{ij} - \\theta\\right) \\geq \\gamma \\quad \\text{for all } i \\in \\{1,\\dots,m\\}.\n$$\nA positive margin $\\gamma  0$ enforces strict separation: positives lie at least $\\gamma$ on one side, negatives at least $\\gamma$ on the other side. A margin $\\gamma = 0$ reduces to non-strict separation, allowing some points to lie exactly on the threshold.\n\nBecause the McCulloch-Pitts decision is scale-invariant under positive rescaling of $(\\mathbf{w}, \\theta)$, we must fix scale to make margin maximization well-posed. A standard way that preserves linearity is to constrain the $\\ell_{1}$-norm $\\|\\mathbf{w}\\|_{1} \\leq 1$. This is rendered linear via auxiliary variables $\\mathbf{u} \\in \\mathbb{R}^{n}$ such that $-u_{j} \\leq w_{j} \\leq u_{j}$ for all $j$ and $\\sum_{j=1}^{n} u_{j} \\leq 1$, with $u_{j} \\geq 0$.\n\nPutting this together, the Linear Programming (LP) formulation to maximize strict separation margin is:\n- variables: $\\mathbf{w} \\in \\mathbb{R}^{n}$, $\\theta \\in \\mathbb{R}$, $\\gamma \\in \\mathbb{R}$, $\\mathbf{u} \\in \\mathbb{R}^{n}$,\n- objective: maximize $\\gamma$,\n- classification constraints: for all $i$, $y_{i}\\left(\\sum_{j=1}^{n} w_{j} x_{ij} - \\theta\\right) \\geq \\gamma$,\n- norm constraints: for all $j$, $-u_{j} \\leq w_{j} \\leq u_{j}$, $u_{j} \\geq 0$, and $\\sum_{j=1}^{n} u_{j} \\leq 1$,\n- nonnegativity of margin: $\\gamma \\geq 0$.\n\nApplying this LP to the given dataset in $\\mathbb{R}^{2}$:\n- $\\mathbf{x}_{1} = (1,1)$ with $t_{1} = 1$ so $y_{1} = +1$,\n- $\\mathbf{x}_{2} = (1,0)$ with $t_{2} = 1$ so $y_{2} = +1$,\n- $\\mathbf{x}_{3} = (0,1)$ with $t_{3} = 0$ so $y_{3} = -1$,\n- $\\mathbf{x}_{4} = (0,0)$ with $t_{4} = 0$ so $y_{4} = -1$.\n\nLet $\\mathbf{w} = (a, b)$ and $\\theta = t$. The classification constraints become:\n- for $\\mathbf{x}_{1}$: $a + b - t \\geq \\gamma$,\n- for $\\mathbf{x}_{2}$: $a - t \\geq \\gamma$,\n- for $\\mathbf{x}_{3}$: $t - b \\geq \\gamma$,\n- for $\\mathbf{x}_{4}$: $t \\geq \\gamma$.\n\nThe $\\ell_{1}$-norm bounds are enforced via $u_{1}, u_{2} \\geq 0$ with $-u_{1} \\leq a \\leq u_{1}$, $-u_{2} \\leq b \\leq u_{2}$, and $u_{1} + u_{2} \\leq 1$, which implies $|a| + |b| \\leq 1$.\n\nWe now solve for the optimal $\\gamma^{\\star}$. Observe that the negative example $\\mathbf{x}_{3} = (0,1)$ imposes $t - b \\geq \\gamma$. Increasing $b$ (positively) helps the positive example $\\mathbf{x}_{1}$ via $a + b - t$, but simultaneously hurts the negative constraint $t - b \\geq \\gamma$ by decreasing the left-hand side for fixed $t$. Because the positive example $\\mathbf{x}_{2} = (1,0)$ already enforces $a - t \\geq \\gamma$, any positive $b$ is unnecessary to satisfy and maximize the margin at $\\mathbf{x}_{2}$, while it tightens the constraint at $\\mathbf{x}_{3}$. Therefore, it is optimal to set $b = 0$.\n\nWith $b = 0$, the constraints reduce to:\n$$\nt \\geq \\gamma, \\quad a - t \\geq \\gamma, \\quad a - t \\geq \\gamma \\text{ (redundant with the previous)}, \\quad t \\geq \\gamma \\text{ (redundant)}.\n$$\nThus we have\n$$\nt \\geq \\gamma \\quad \\text{and} \\quad a \\geq t + \\gamma,\n$$\nand the norm constraint becomes $|a| \\leq 1$ since $|a| + |b| = |a| \\leq 1$.\n\nTo maximize $\\gamma$, we choose $a$ as large as permitted by $\\|\\mathbf{w}\\|_{1} \\leq 1$, i.e., $a \\leq 1$ with $a \\geq 0$ optimal. The inequality $a \\geq t + \\gamma$ suggests minimizing $t$ subject to $t \\geq \\gamma$, which is achieved by setting $t = \\gamma$. Substituting $t = \\gamma$ into $a \\geq t + \\gamma$ yields $a \\geq 2 \\gamma$. Since $a \\leq 1$, we obtain\n$$\n2 \\gamma \\leq 1 \\quad \\Rightarrow \\quad \\gamma \\leq \\frac{1}{2}.\n$$\nThis upper bound is achievable by setting $a = 1$, $b = 0$, and $t = \\gamma = \\frac{1}{2}$. Check all constraints:\n- $t \\geq \\gamma$: $\\frac{1}{2} \\geq \\frac{1}{2}$ holds,\n- $a - t \\geq \\gamma$: $1 - \\frac{1}{2} = \\frac{1}{2} \\geq \\frac{1}{2}$ holds,\n- $a + b - t \\geq \\gamma$: $1 + 0 - \\frac{1}{2} = \\frac{1}{2} \\geq \\frac{1}{2}$ holds,\n- $t - b \\geq \\gamma$: $\\frac{1}{2} - 0 = \\frac{1}{2} \\geq \\frac{1}{2}$ holds,\n- $\\|\\mathbf{w}\\|_{1} = |a| + |b| = 1 + 0 = 1 \\leq 1$ holds.\n\nTherefore, the optimal margin is $\\gamma^{\\star} = \\frac{1}{2}$.\n\nWe now discuss feasibility conditions from first principles. The constraints\n$$\ny_{i}\\left(\\sum_{j=1}^{n} w_{j} x_{ij} - \\theta\\right) \\geq \\gamma\n$$\ndescribe two sets separated by a hyperplane with normal $\\mathbf{w}$ and offset $\\theta$: the set of positives lie on one side at distance at least $\\gamma$, and the set of negatives lie on the other side at distance at least $\\gamma$, measured along $\\mathbf{w}$. Because we have fixed scale via $\\|\\mathbf{w}\\|_{1} \\leq 1$, the margin $\\gamma$ compares distances consistently. For $\\gamma  0$, a necessary and sufficient condition is strict linear separability: the convex hulls of the positives and negatives are disjoint and admit a separating hyperplane with a strictly positive gap. Equivalently, by the hyperplane separation theorem, there exists $(\\mathbf{w}, \\theta)$ such that the strict inequalities hold, and by compactness of convex hulls for finite sets, a positive gap can be realized once scale is fixed. For $\\gamma \\geq 0$, the condition relaxes to non-strict linear separability: the convex hulls do not overlap in their interiors but may meet on the boundary; a separating hyperplane exists that places all positives on one closed halfspace and all negatives on the other, possibly with some points lying on the hyperplane.\n\nA dual viewpoint is provided by Farkas' lemma: for the system of linear inequalities $y_{i}(\\mathbf{w}^{\\top} \\mathbf{x}_{i} - \\theta) \\geq \\gamma$, together with $\\|\\mathbf{w}\\|_{1} \\leq 1$, either there exists a feasible $(\\mathbf{w}, \\theta, \\gamma)$ with $\\gamma  0$, or there exists a nontrivial nonnegative combination of the constraints producing an infeasibility certificate that a strictly positive margin cannot be attained. Such certificates arise precisely when the convex hulls fail to be strictly separable once the scale is fixed. In summary, feasibility with $\\gamma  0$ corresponds to strict convex separation; feasibility with $\\gamma \\geq 0$ corresponds to non-strict convex separation.\n\nFor the given dataset, the convex hull of positives $\\{(1,1), (1,0)\\}$ is the line segment $\\{(1, \\lambda) : \\lambda \\in [0,1]\\}$, and the convex hull of negatives $\\{(0,1), (0,0)\\}$ is the line segment $\\{(0, \\lambda) : \\lambda \\in [0,1]\\}$. These hulls are disjoint and parallel, so a separating hyperplane exists with a positive gap along the $x_{1}$-axis; after $\\ell_{1}$-normalization, the maximized gap is exactly $\\gamma^{\\star} = \\frac{1}{2}$ as computed above.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "A single McCulloch-Pitts neuron, as a linear classifier, is fundamentally limited to solving linearly separable problems. This practice directly confronts this limitation using the classic parity function, a simple task that is famously not linearly separable. You will learn to overcome this constraint by designing a multi-layer network, constructing it from a universal building block—the XOR gate—and analyzing how the network's size and depth scale with the number of inputs . This exercise demonstrates the power of hierarchical composition, a core principle in building complex computational systems from simple parts.",
            "id": "4065070",
            "problem": "Consider the McCulloch-Pitts neuron model, in which a neuron computes a Boolean-valued output on Boolean inputs by applying a linear threshold rule: for inputs $x_{1},\\dots,x_{m} \\in \\{0,1\\}$, weights $w_{1},\\dots,w_{m} \\in \\mathbb{R}$, and threshold $\\theta \\in \\mathbb{R}$, the neuron outputs $1$ if and only if $\\sum_{i=1}^{m} w_{i} x_{i} \\ge \\theta$, and outputs $0$ otherwise. Negative weights (representing inhibition) are allowed. The exclusive OR (XOR) of two bits, denoted $\\mathrm{XOR}(x,y)$, is defined by $\\mathrm{XOR}(x,y)=1$ if exactly one of $x$ or $y$ is $1$, and $0$ otherwise. The $n$-bit parity function is defined by $\\mathrm{PARITY}_{n}(x_{1},\\dots,x_{n})=\\mathrm{XOR}(x_{1},\\mathrm{XOR}(x_{2},\\dots,\\mathrm{XOR}(x_{n-1},x_{n})\\dots))$, equivalently the sum $\\sum_{i=1}^{n} x_{i}$ modulo $2$.\n\nDesign a realization of $\\mathrm{XOR}(x,y)$ using a two-layer McCulloch-Pitts subnetwork composed of a hidden layer and an output layer, where the hidden layer may contain multiple neurons and the output layer contains a single neuron. Then, using a hierarchical balanced binary tree of these $\\mathrm{XOR}$ subnetworks to compute $\\mathrm{PARITY}_{n}$, define the network depth as the maximum number of neuron layers along any path from an input bit to the final output neuron, and define the network size as the total number of McCulloch-Pitts neurons used (counting all hidden and output neurons across the entire tree, and excluding input nodes). From first principles, derive closed-form expressions in terms of $n$ for the depth and the size of this parity network.\n\nProvide your final answer as two expressions arranged in a single row, with the first entry equal to the depth and the second entry equal to the size. No numerical approximation or rounding is required.",
            "solution": "The problem asks for the derivation of closed-form expressions for the network depth and size of a network that computes the $n$-bit parity function, $\\mathrm{PARITY}_{n}$. The network is to be constructed as a hierarchical balanced binary tree of two-layer McCulloch-Pitts (MP) subnetworks, each implementing the two-input exclusive OR ($\\mathrm{XOR}$) function.\n\nFirst, we must design a two-layer subnetwork for $\\mathrm{XOR}(x_1, x_2)$ using MP neurons. An MP neuron with inputs $x_1, \\dots, x_m \\in \\{0, 1\\}$, real-valued weights $w_1, \\dots, w_m$, and a real-valued threshold $\\theta$ produces an output $y=1$ if the weighted sum of its inputs meets or exceeds the threshold, i.e., $\\sum_{i=1}^{m} w_i x_i \\ge \\theta$, and $y=0$ otherwise.\n\nThe $\\mathrm{XOR}$ function is not linearly separable, meaning a single MP neuron cannot implement it. Therefore, a multi-layer network is necessary. The problem specifies a two-layer network, consisting of a hidden layer and an output layer. We can construct an $\\mathrm{XOR}$ gate by decomposing the logical function. One such decomposition is $\\mathrm{XOR}(x_1, x_2) = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2)$. We can assign one hidden neuron to compute the OR clause and another to compute the NAND clause ($\\neg(x_1 \\land x_2)$). The output neuron will then compute the AND of the outputs of these two hidden neurons.\n\nLet the inputs be $x_1, x_2 \\in \\{0, 1\\}$.\nLet the hidden layer consist of two neurons, $H_1$ and $H_2$, and the output layer consist of a single neuron, $O$.\n\n**1. Hidden Neuron $H_1$: Computes $x_1 \\lor x_2$.**\nThe function $x_1 \\lor x_2$ is $1$ if at least one input is $1$, and $0$ otherwise.\nWe need to find weights $w_{11}, w_{12}$ and a threshold $\\theta_1$ such that $w_{11}x_1 + w_{12}x_2 \\ge \\theta_1$ if and only if $x_1=1$ or $x_2=1$.\nLet's choose $w_{11}=1$ and $w_{12}=1$. The weighted sum is $x_1+x_2$.\n- If $(x_1, x_2)=(0,0)$, the sum is $0$.\n- If $(x_1, x_2)=(0,1)$ or $(1,0)$, the sum is $1$.\n- If $(x_1, x_2)=(1,1)$, the sum is $2$.\nTo separate the $(0,0)$ case from the others, the threshold $\\theta_1$ must satisfy $0  \\theta_1 \\le 1$. Let's choose $\\theta_1=1$.\nThus, $H_1$ is defined by weights $w_{11}=1, w_{12}=1$ and threshold $\\theta_1=1$.\n\n**2. Hidden Neuron $H_2$: Computes $\\mathrm{NAND}(x_1, x_2) = \\neg(x_1 \\land x_2)$.**\nThe function $\\mathrm{NAND}(x_1, x_2)$ is $0$ only if both inputs are $1$, and $1$ otherwise.\nUsing inhibitory (negative) weights is convenient here. Let's choose $w_{21}=-1$ and $w_{22}=-1$. The weighted sum is $-x_1-x_2$.\n- If $(x_1, x_2)=(0,0)$, the sum is $0$.\n- If $(x_1, x_2)=(0,1)$ or $(1,0)$, the sum is $-1$.\n- If $(x_1, x_2)=(1,1)$, the sum is $-2$.\nWe want the neuron to fire for all cases except $(1,1)$. So, the condition $w_{21}x_1 + w_{22}x_2 \\ge \\theta_2$ must be true for sums of $0$ and $-1$, but false for a sum of $-2$. This requires the threshold $\\theta_2$ to satisfy $-2  \\theta_2 \\le -1$. Let's choose $\\theta_2=-1.5$.\nThus, $H_2$ is defined by weights $w_{21}=-1, w_{22}=-1$ and threshold $\\theta_2=-1.5$.\n\n**3. Output Neuron $O$: Computes $h_1 \\land h_2$.**\nThe output neuron $O$ takes the outputs of the hidden neurons, $h_1$ and $h_2$, as its inputs. It must compute $h_1 \\land h_2$ to realize the final $\\mathrm{XOR}$ function.\nLet's analyze the inputs to $O$:\n- If $(x_1, x_2)=(0,0)$, then $h_1=0$ (OR is false), $h_2=1$ (NAND is true). Final output should be $0$.\n- If $(x_1, x_2)=(0,1)$, then $h_1=1$, $h_2=1$. Final output should be $1$.\n- If $(x_1, x_2)=(1,0)$, then $h_1=1$, $h_2=1$. Final output should be $1$.\n- If $(x_1, x_2)=(1,1)$, then $h_1=1$, $h_2=0$. Final output should be $0$.\nNeuron $O$ must output $1$ only when its inputs $(h_1, h_2)$ are $(1,1)$.\nLet the weights for $O$ be $w_{o1}=1, w_{o2}=1$. The weighted sum is $h_1+h_2$.\n- If $(h_1, h_2)=(0,1)$ or $(1,0)$, the sum is $1$.\n- If $(h_1, h_2)=(1,1)$, the sum is $2$.\nTo fire only for $(1,1)$, the threshold $\\theta_o$ must satisfy $1  \\theta_o \\le 2$. Let's choose $\\theta_o=2$.\nThus, $O$ is defined by weights $w_{o1}=1, w_{o2}=1$ and threshold $\\theta_o=2$.\n\nThis completes the design of the $\\mathrm{XOR}$ subnetwork.\n- **Size of an $\\mathrm{XOR}$ subnetwork**: It contains $2$ hidden neurons and $1$ output neuron, for a total of $3$ neurons.\n- **Depth of an $\\mathrm{XOR}$ subnetwork**: The signal path flows from inputs through the hidden layer (first layer of neurons) to the output layer (second layer of neurons). Thus, the depth is $2$.\n\nNext, we analyze the entire $\\mathrm{PARITY}_{n}$ network, built as a \"hierarchical balanced binary tree\" of these $\\mathrm{XOR}$ subnetworks.\n\n**Derivation of Network Size:**\nThe function $\\mathrm{PARITY}_{n}(x_1, \\dots, x_n)$ is equivalent to $x_1 \\oplus x_2 \\oplus \\dots \\oplus x_n$, where $\\oplus$ denotes the $\\mathrm{XOR}$ operation. Computing an expression with $n-1$ binary operators requires exactly $n-1$ applications of the operator. A balanced binary tree is one way to arrange these operations. For example, for $n=4$, we compute $\\mathrm{XOR}(\\mathrm{XOR}(x_1, x_2), \\mathrm{XOR}(x_3, x_4))$. This uses $3$ (which is $4-1$) $\\mathrm{XOR}$ operations. For any $n$, such a tree structure will require $n-1$ two-input $\\mathrm{XOR}$ functions.\nSince each $\\mathrm{XOR}$ subnetwork is composed of $3$ neurons, the total number of neurons in the $\\mathrm{PARITY}_{n}$ network (its size) is:\n$$ \\text{Size} = (\\text{Number of XOR subnetworks}) \\times (\\text{Neurons per subnetwork}) = (n-1) \\times 3 = 3(n-1) $$\n\n**Derivation of Network Depth:**\nThe depth is the maximum number of neuron layers along any path from an input to the final output. The network is a tree of $\\mathrm{XOR}$ blocks. We first need to find the number of levels of these blocks in the tree.\nThe first level of $\\mathrm{XOR}$ blocks takes $n$ inputs and produces $\\lceil n/2 \\rceil$ outputs. The second level takes these $\\lceil n/2 \\rceil$ signals as inputs and produces $\\lceil (\\lceil n/2 \\rceil)/2 \\rceil$ outputs, and so on. This process continues until a single output remains.\nThe number of levels in this tree of blocks, let's call it $L$, is the number of times we must apply the function $f(k)=\\lceil k/2 \\rceil$ starting from $k=n$ until we reach $1$. This is equivalent to the definition of the ceiling of the base-$2$ logarithm.\nFor example, if $n=8$, inputs are reduced as $8 \\to 4 \\to 2 \\to 1$ ($3$ levels). $\\lceil \\log_2 8 \\rceil = 3$.\nIf $n=7$, inputs are reduced as $7 \\to 4 \\to 2 \\to 1$ ($3$ levels). $\\lceil \\log_2 7 \\rceil = 3$.\nSo, the number of levels of $\\mathrm{XOR}$ subnetworks is $L = \\lceil \\log_2 n \\rceil$.\nThe longest path from an initial input bit to the final output will pass through one $\\mathrm{XOR}$ subnetwork at each of these $L$ levels. Since each $\\mathrm{XOR}$ subnetwork has a depth of $2$ neuron layers, the total depth of the $\\mathrm{PARITY}_{n}$ network is:\n$$ \\text{Depth} = (\\text{Number of block levels}) \\times (\\text{Depth per block}) = \\lceil \\log_2 n \\rceil \\times 2 = 2 \\lceil \\log_2 n \\rceil $$\nNote that for the edge case $n=1$, $\\mathrm{PARITY}_1(x_1)=x_1$, which requires $0$ neurons and has a depth of $0$. Our formulas give Size$=3(1-1)=0$ and Depth$=2\\lceil\\log_2 1\\rceil=0$, which are consistent.\n\nThe derived closed-form expressions are $2 \\lceil \\log_2 n \\rceil$ for the depth and $3(n-1)$ for the size.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 \\lceil \\log_2 n \\rceil  3(n-1)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving beyond networks that compute static functions, we now explore the rich behavior of recurrent networks where connections form feedback loops, enabling memory and complex temporal dynamics. The emergent behavior of such a system depends not only on its architecture but also critically on its update dynamics—whether neurons update all at once (synchronously) or one by one (asynchronously). This computational exercise reveals how these two update schemes can steer the exact same network toward completely different stable states and limit cycles, a crucial insight for modeling any dynamic system from digital circuits to biological neural circuits .",
            "id": "4065112",
            "problem": "Consider the McCulloch-Pitts neuron model, where each neuron has a binary state $x_i(t) \\in \\{0,1\\}$ and updates according to a weighted sum followed by a thresholding nonlinearity. The state update for neuron $i$ is given by $x_i(t+1) = H\\left(\\sum_{j} w_{ij} x_j(t) - \\theta_i\\right)$, where $H(\\cdot)$ is the Heaviside step function returning $1$ if its argument is greater than or equal to $0$ and $0$ otherwise, $w_{ij}$ is the weight from neuron $j$ to neuron $i$, and $\\theta_i$ is the threshold of neuron $i$. Construct a recurrent network of $n = 3$ McCulloch-Pitts neurons labeled $A$, $B$, and $C$ with a ring topology and copy dynamics in the following sense: neuron $A$ copies neuron $C$, neuron $B$ copies neuron $A$, and neuron $C$ copies neuron $B$. Formally specify the weight matrix $W$ and threshold vector $\\boldsymbol{\\theta}$ as $W = \\begin{bmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 0  1  0 \\end{bmatrix}$ and $\\boldsymbol{\\theta} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$. This ensures that the update rule reduces to logical copying from the designated predecessor in the ring.\n\nDefine two update regimes:\n- Synchronous updates: all neurons update simultaneously according to the current network state $\\mathbf{x}(t) = (x_A(t), x_B(t), x_C(t))$, producing $\\mathbf{x}(t+1)$ in one step.\n- Asynchronous updates with a fixed sequential schedule: in one macro-step, perform three micro-steps in the fixed order $(A, B, C)$. At each micro-step, update the targeted neuron using the current state vector, immediately committing the new value before proceeding to the next neuron. The macro-step mapping $\\mathbf{x}(t) \\mapsto \\mathbf{x}(t+1)$ is the composition of these three micro-steps.\n\nFor each regime, consider the state graph whose vertices are all $2^3 = 8$ binary states $\\{0,1\\}^3$ and whose directed edges represent the single-step (for the synchronous regime) or single macro-step (for the asynchronous sequential regime) transitions induced by the network dynamics.\n\nStarting from the foundational definitions stated above, analyze how asynchronous updates lead to different limit cycles than synchronous updates in this network. Compute the limit cycles for both regimes and analyze their basins of attraction by constructing the full state transition function. Represent states as $3$-bit binary vectors $(x_A,x_B,x_C)$ encoded as integers in $[0,7]$ via $x_A \\cdot 4 + x_B \\cdot 2 + x_C \\cdot 1$.\n\nYour program must implement the exact specified network and update rules and produce the following outputs derived from the state graphs:\n- For each initial state in the test suite defined below, compute the length of the limit cycle reached under synchronous updates and under asynchronous sequential updates. The length of a limit cycle is the number of distinct states in the cycle. If a fixed point is reached, the length is $1$.\n- Compute the number of distinct limit cycles in the synchronous state graph and in the asynchronous sequential state graph, across all $8$ states.\n- Compute the multiset of basin sizes for the synchronous regime and for the asynchronous sequential regime. The basin size of a limit cycle is the number of states that eventually reach that cycle under repeated application of the respective update mapping.\n\nTest suite:\n- Use the initial states $\\mathbf{x}(0)$ equal to $(0,0,1)$, $(1,0,0)$, $(1,1,1)$, $(0,0,0)$, and $(0,1,1)$, encoded respectively as integers $1$, $4$, $7$, $0$, and $3$.\n- For each of these initial states, compute the synchronous and asynchronous sequential limit cycle lengths as described.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line must contain the following items in order:\n- A list of $5$ integers giving the synchronous limit cycle lengths for the initial states in the order $[1,4,7,0,3]$.\n- A list of $5$ integers giving the asynchronous sequential limit cycle lengths for the initial states in the order $[1,4,7,0,3]$.\n- A boolean indicating whether the set of limit cycles differs between the two regimes across all $8$ states.\n- An integer giving the number of distinct synchronous limit cycles across all $8$ states.\n- An integer giving the number of distinct asynchronous sequential limit cycles across all $8$ states.\n- A list of integers giving the basin sizes of all distinct synchronous limit cycles, sorted in nondecreasing order.\n- A list of integers giving the basin sizes of all distinct asynchronous sequential limit cycles, sorted in nondecreasing order.\n\nYour program must not read any input and must not print any other text beyond the single specified line.",
            "solution": "The user wants to analyze a 3-neuron McCulloch-Pitts network and compare its dynamics under synchronous and asynchronous update regimes.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **Model**: McCulloch-Pitts neuron with state $x_i(t) \\in \\{0,1\\}$ and update rule $x_i(t+1) = H\\left(\\sum_{j} w_{ij} x_j(t) - \\theta_i\\right)$. $H(\\cdot)$ is the Heaviside step function: $H(z) = 1$ for $z \\ge 0$ and $H(z) = 0$ for $z  0$.\n- **Network**: $n=3$ neurons labeled $A$, $B$, $C$.\n- **Topology**: Ring with copy dynamics: $A$ copies $C$, $B$ copies $A$, $C$ copies $B$.\n- **Parameters**: Weight matrix $W = \\begin{bmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 0  1  0 \\end{bmatrix}$ and threshold vector $\\boldsymbol{\\theta} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$.\n- **State Representation**: A state $\\mathbf{x} = (x_A, x_B, x_C)$ is encoded as an integer $k = x_A \\cdot 4 + x_B \\cdot 2 + x_C \\cdot 1$. The state space is $\\{0, 1, \\dots, 7\\}$.\n- **Update Regimes**:\n    1.  **Synchronous**: All neurons update simultaneously.\n    2.  **Asynchronous Sequential**: Neurons update in the fixed order $(A, B, C)$, with each update taking effect immediately.\n- **Tasks**:\n    1.  For each update regime, construct the state transition graph over the $8$ possible states.\n    2.  For a given test suite of initial states, compute the length of the limit cycle reached under each regime.\n    3.  Compute the number of distinct limit cycles for each regime.\n    4.  Compute the multiset of basin sizes for each regime.\n- **Test Suite**: Initial states encoded as integers $1, 4, 7, 0, 3$.\n- **Output Format**: A single line, comma-separated list enclosed in brackets, containing:\n    - List of synchronous cycle lengths for the test suite.\n    - List of asynchronous cycle lengths for the test suite.\n    - Boolean indicating if the sets of limit cycles differ.\n    - Number of synchronous limit cycles.\n    - Number of asynchronous limit cycles.\n    - Sorted list of synchronous basin sizes.\n    - Sorted list of asynchronous basin sizes.\n\n**1.2. Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is grounded in the well-established theory of artificial neural networks, specifically the McCulloch-Pitts model. The analysis of network dynamics under different update schemes (synchronous vs. asynchronous) is a fundamental topic in the study of recurrent networks and discrete dynamical systems.\n- **Well-Posedness**: The problem is fully specified. The neuron model, network parameters ($W, \\boldsymbol{\\theta}$), state space, and update rules are all defined unambiguously. The state space is finite, guaranteeing that the dynamics will eventually enter a limit cycle (which includes fixed points). All requested outputs are deterministic computations based on the specified dynamics.\n- **Objectivity**: The problem is stated in precise, mathematical terms, free of subjective or ambiguous language.\n\n**1.3. Verdict and Action**\nThe problem is scientifically sound, well-posed, and objective. It is **valid**. We proceed to the solution.\n\n### Step 2: Derivation and Solution\n\nThe state vector is $\\mathbf{x}(t) = (x_A(t), x_B(t), x_C(t))$. The update rule for a neuron $i$ is $x_i(t+1) = H(\\mathbf{w}_i \\cdot \\mathbf{x}(t) - \\theta_i)$, where $\\mathbf{w}_i$ is the $i$-th row of $W$.\n\nLet's verify the \"copy\" dynamic with $\\theta_i = 0.5$ for all $i$.\n- Neuron $A$ (index $1$): The input is $\\sum_j w_{Aj} x_j - \\theta_A = 1 \\cdot x_C - 0.5$. The output $x_A(t+1) = H(x_C(t) - 0.5)$ is $1$ if $x_C(t)=1$ and $0$ if $x_C(t)=0$. Thus, $x_A(t+1) = x_C(t)$.\n- Neuron $B$ (index $2$): The input is $\\sum_j w_{Bj} x_j - \\theta_B = 1 \\cdot x_A - 0.5$. The output $x_B(t+1) = H(x_A(t) - 0.5)$ is $1$ if $x_A(t)=1$ and $0$ if $x_A(t)=0$. Thus, $x_B(t+1) = x_A(t)$.\n- Neuron $C$ (index $3$): The input is $\\sum_j w_{Cj} x_j - \\theta_C = 1 \\cdot x_B - 0.5$. The output $x_C(t+1) = H(x_B(t) - 0.5)$ is $1$ if $x_B(t)=1$ and $0$ if $x_B(t)=0$. Thus, $x_C(t+1) = x_B(t)$.\n\nThe parameters correctly implement the described copy dynamics. We now analyze the two update regimes.\n\n**2.1. Synchronous Update Regime**\nIn this regime, all neurons update simultaneously based on the state at time $t$. The new state $\\mathbf{x}(t+1) = (x_A(t+1), x_B(t+1), x_C(t+1))$ is given by:\n$$ x_A(t+1) = x_C(t) $$\n$$ x_B(t+1) = x_A(t) $$\n$$ x_C(t+1) = x_B(t) $$\nSo, the state vector transformation is $\\mathbf{x}(t+1) = (x_C(t), x_A(t), x_B(t))$. This is a permutation of the components of $\\mathbf{x}(t)$. We construct the state transition graph for all $2^3=8$ states, using the integer encoding $k = 4x_A + 2x_B + 1x_C$.\n\n- $0=(0,0,0) \\to (0,0,0)=0$\n- $1=(0,0,1) \\to (1,0,0)=4$\n- $2=(0,1,0) \\to (0,0,1)=1$\n- $3=(0,1,1) \\to (1,0,1)=5$\n- $4=(1,0,0) \\to (0,1,0)=2$\n- $5=(1,0,1) \\to (1,1,0)=6$\n- $6=(1,1,0) \\to (0,1,1)=3$\n- $7=(1,1,1) \\to (1,1,1)=7$\n\nThe full state transition mapping is $T_{sync}: \\{0 \\to 0, 1 \\to 4, 2 \\to 1, 3 \\to 5, 4 \\to 2, 5 \\to 6, 6 \\to 3, 7 \\to 7\\}$.\nThe dynamics partition the state space into the following disjoint sets:\n- **Fixed point:** $\\{0\\}$. This is a limit cycle of length $1$. Its basin of attraction is $\\{0\\}$. Size: $1$.\n- **Fixed point:** $\\{7\\}$. This is a limit cycle of length $1$. Its basin of attraction is $\\{7\\}$. Size: $1$.\n- **Cycle:** $1 \\to 4 \\to 2 \\to 1$. This is a limit cycle of length $3$. Its basin of attraction is $\\{1, 2, 4\\}$. Size: $3$.\n- **Cycle:** $3 \\to 5 \\to 6 \\to 3$. This is a limit cycle of length $3$. Its basin of attraction is $\\{3, 5, 6\\}$. Size: $3$.\n\n**2.2. Asynchronous Sequential Update Regime**\nIn this regime, neurons are updated in the fixed order $(A, B, C)$. Let the state at time $t$ be $(x_A, x_B, x_C)$. The macro-step consists of three micro-steps:\n1.  Update $A$: $x'_A = x_C$. The state becomes $(x'_A, x_B, x_C)$.\n2.  Update $B$: $x'_B = x'_A$. The state becomes $(x'_A, x'_B, x_C)$.\n3.  Update $C$: $x'_C = x'_B$. The state becomes $(x'_A, x'_B, x'_C)$.\n\nThe final state after one macro-step is $\\mathbf{x}(t+1) = (x'_A, x'_B, x'_C)$. By substitution:\n$$ x_A(t+1) = x_C(t) $$\n$$ x_B(t+1) = x_A(t+1) = x_C(t) $$\n$$ x_C(t+1) = x_B(t+1) = x_C(t) $$\nThe transformation is $\\mathbf{x}(t+1) = (x_C(t), x_C(t), x_C(t))$. The next state is determined entirely by the current state of neuron $C$. If $x_C(t)=0$, the next state is $(0,0,0)=0$. If $x_C(t)=1$, the next state is $(1,1,1)=7$.\n\nThe state transition graph is:\n- States with $x_C=0$: $\\{0, 2, 4, 6\\}$. All transition to state $0$.\n  - $0=(0,0,0) \\to (0,0,0)=0$\n  - $2=(0,1,0) \\to (0,0,0)=0$\n  - $4=(1,0,0) \\to (0,0,0)=0$\n  - $6=(1,1,0) \\to (0,0,0)=0$\n- States with $x_C=1$: $\\{1, 3, 5, 7\\}$. All transition to state $7$.\n  - $1=(0,0,1) \\to (1,1,1)=7$\n  - $3=(0,1,1) \\to (1,1,1)=7$\n  - $5=(1,0,1) \\to (1,1,1)=7$\n  - $7=(1,1,1) \\to (1,1,1)=7$\n\nThe full state transition mapping is $T_{async}: \\{0 \\to 0, 1 \\to 7, 2 \\to 0, 3 \\to 7, 4 \\to 0, 5 \\to 7, 6 \\to 0, 7 \\to 7\\}$.\nThe dynamics partition the state space into two sets:\n- **Fixed point:** $\\{0\\}$. This is a limit cycle of length $1$. Its basin of attraction is $\\{0, 2, 4, 6\\}$. Size: $4$.\n- **Fixed point:** $\\{7\\}$. This is a limit cycle of length $1$. Its basin of attraction is $\\{1, 3, 5, 7\\}$. Size: $4$.\n\n**2.3. Calculation of Final Results**\nWe now compute the specific quantities required by the problem statement based on the analysis above.\nThe test suite consists of initial states $1, 4, 7, 0, 3$.\n\n- **Synchronous Limit Cycle Lengths**:\n  - State $1$: Is in cycle $\\{1, 4, 2\\}$. Length is $3$.\n  - State $4$: Is in cycle $\\{1, 4, 2\\}$. Length is $3$.\n  - State $7$: Is in cycle $\\{7\\}$. Length is $1$.\n  - State $0$: Is in cycle $\\{0\\}$. Length is $1$.\n  - State $3$: Is in cycle $\\{3, 5, 6\\}$. Length is $3$.\n  - Result: `[3, 3, 1, 1, 3]`\n\n- **Asynchronous Limit Cycle Lengths**:\n  - State $1$: Transitions to $7$, which is a fixed point. Length is $1$.\n  - State $4$: Transitions to $0$, which is a fixed point. Length is $1$.\n  - State $7$: Is the fixed point $\\{7\\}$. Length is $1$.\n  - State $0$: Is the fixed point $\\{0\\}$. Length is $1$.\n  - State $3$: Transitions to $7$, which is a fixed point. Length is $1$.\n  - Result: `[1, 1, 1, 1, 1]`\n\n- **Limit Cycles Differ**: The set of synchronous cycles is $\\{\\{0\\}, \\{7\\}, \\{1, 2, 4\\}, \\{3, 5, 6\\}\\}$. The set of asynchronous cycles is $\\{\\{0\\}, \\{7\\}\\}$. These sets are different. Result: `True`.\n\n- **Number of Distinct Limit Cycles**:\n  - Synchronous: $4$ cycles.\n  - Asynchronous: $2$ cycles.\n\n- **Basin Sizes (sorted)**:\n  - Synchronous: The basins have sizes $1, 1, 3, 3$. Result: `[1, 1, 3, 3]`.\n  - Asynchronous: The basins have sizes $4, 4$. Result: `[4, 4]`.\n\nThese results will be computed and formatted by the program.",
            "answer": "[[3,3,1,1,3],[1,1,1,1,1],True,4,2,[1,1,3,3],[4,4]]"
        }
    ]
}