{
    "hands_on_practices": [
        {
            "introduction": "McCulloch-Pitts (MP) 神经元是构建复杂逻辑电路的基本单元，但单个神经元的能力是有限的。这个练习将指导您克服一个著名的限制——即单个神经元无法计算异或（XOR）或奇偶校验（Parity）函数。通过分层构建一个多层网络，您将亲身体验到如何通过模块化和层次化组合来扩展计算能力，这是理解神经网络表达能力的关键一步 。",
            "id": "4065070",
            "problem": "考虑麦卡洛克-皮茨神经元模型，在该模型中，一个神经元通过应用线性阈值规则，对布尔输入计算布尔值输出：对于输入 $x_{1},\\dots,x_{m} \\in \\{0,1\\}$、权重 $w_{1},\\dots,w_{m} \\in \\mathbb{R}$ 和阈值 $\\theta \\in \\mathbb{R}$，当且仅当 $\\sum_{i=1}^{m} w_{i} x_{i} \\ge \\theta$ 时，该神经元输出 $1$，否则输出 $0$。允许使用负权重（表示抑制）。两个比特的异或（XOR）运算，记作 $\\mathrm{XOR}(x,y)$，其定义为：当 $x$ 或 $y$ 中恰好有一个为 $1$ 时，$\\mathrm{XOR}(x,y)=1$，否则为 $0$。$n$ 位奇偶校验函数定义为 $\\mathrm{PARITY}_{n}(x_{1},\\dots,x_{n})=\\mathrm{XOR}(x_{1},\\mathrm{XOR}(x_{2},\\dots,\\mathrm{XOR}(x_{n-1},x_{n})\\dots))$，等价于和 $\\sum_{i=1}^{n} x_{i}$ 模 $2$。\n\n设计一个 $\\mathrm{XOR}(x,y)$ 的实现，使用一个由隐藏层和输出层组成的两层麦卡洛克-皮茨子网络，其中隐藏层可以包含多个神经元，输出层包含一个神经元。然后，使用这些 $\\mathrm{XOR}$ 子网络构成的分层平衡二叉树来计算 $\\mathrm{PARITY}_{n}$，将网络深度定义为从任一输入位到最终输出神经元的任意路径上的最大神经元层数，将网络规模定义为所使用的麦卡洛克-皮茨神经元的总数（计算整个树中的所有隐藏神经元和输出神经元，不包括输入节点）。从第一性原理出发，推导该奇偶校验网络的深度和规模关于 $n$ 的闭式表达式。\n\n将你的最终答案以单行排列的两个表达式的形式给出，第一个条目为深度，第二个条目为规模。不需要进行数值近似或舍入。",
            "solution": "问题要求推导计算 $n$ 位奇偶校验函数 $\\mathrm{PARITY}_{n}$ 的网络的深度和规模的闭式表达式。该网络将由两层麦卡洛克-皮茨（MP）子网络构成的分层平衡二叉树来构建，每个子网络实现双输入异或（$\\mathrm{XOR}$）功能。\n\n首先，我们必须使用 MP 神经元为 $\\mathrm{XOR}(x_1, x_2)$ 设计一个两层子网络。一个 MP 神经元，其输入为 $x_1, \\dots, x_m \\in \\{0, 1\\}$，权重为实值 $w_1, \\dots, w_m$，阈值为实值 $\\theta$，当其输入的加权和达到或超过阈值时，即 $\\sum_{i=1}^{m} w_i x_i \\ge \\theta$，产生输出 $y=1$，否则 $y=0$。\n\n$\\mathrm{XOR}$ 函数不是线性可分的，这意味着单个 MP 神经元无法实现它。因此，必须使用多层网络。问题指定了一个由一个隐藏层和一个输出层组成的两层网络。我们可以通过分解逻辑函数来构造一个 $\\mathrm{XOR}$ 门。一种这样的分解是 $\\mathrm{XOR}(x_1, x_2) = (x_1 \\lor x_2) \\land \\neg(x_1 \\land x_2)$。我们可以分配一个隐藏神经元来计算“或”子句，另一个来计算“与非”子句（$\\neg(x_1 \\land x_2)$）。然后，输出神经元将计算这两个隐藏神经元输出的“与”运算。\n\n设输入为 $x_1, x_2 \\in \\{0, 1\\}$。\n设隐藏层由两个神经元 $H_1$ 和 $H_2$ 组成，输出层由单个神经元 $O$ 组成。\n\n**1. 隐藏神经元 $H_1$：计算 $x_1 \\lor x_2$。**\n函数 $x_1 \\lor x_2$ 在至少一个输入为 $1$ 时为 $1$，否则为 $0$。\n我们需要找到权重 $w_{11}, w_{12}$ 和一个阈值 $\\theta_1$，使得 $w_{11}x_1 + w_{12}x_2 \\ge \\theta_1$ 当且仅当 $x_1=1$ 或 $x_2=1$。\n我们选择 $w_{11}=1$ 和 $w_{12}=1$。加权和为 $x_1+x_2$。\n- 如果 $(x_1, x_2)=(0,0)$，和为 $0$。\n- 如果 $(x_1, x_2)=(0,1)$ 或 $(1,0)$，和为 $1$。\n- 如果 $(x_1, x_2)=(1,1)$，和为 $2$。\n为了将 $(0,0)$ 的情况与其他情况分开，阈值 $\\theta_1$ 必须满足 $0  \\theta_1 \\le 1$。我们选择 $\\theta_1=1$。\n因此，$H_1$ 由权重 $w_{11}=1, w_{12}=1$ 和阈值 $\\theta_1=1$ 定义。\n\n**2. 隐藏神经元 $H_2$：计算 $\\mathrm{NAND}(x_1, x_2) = \\neg(x_1 \\land x_2)$。**\n函数 $\\mathrm{NAND}(x_1, x_2)$ 仅在两个输入都为 $1$ 时为 $0$，否则为 $1$。\n在这里使用抑制性（负）权重很方便。我们选择 $w_{21}=-1$ 和 $w_{22}=-1$。加权和为 $-x_1-x_2$。\n- 如果 $(x_1, x_2)=(0,0)$，和为 $0$。\n- 如果 $(x_1, x_2)=(0,1)$ 或 $(1,0)$，和为 $-1$。\n- 如果 $(x_1, x_2)=(1,1)$，和为 $-2$。\n我们希望神经元在除 $(1,1)$ 之外的所有情况下都激活。所以，条件 $w_{21}x_1 + w_{22}x_2 \\ge \\theta_2$ 对于和为 $0$ 和 $-1$ 的情况必须为真，但对于和为 $-2$ 的情况必须为假。这要求阈值 $\\theta_2$ 满足 $-2  \\theta_2 \\le -1$。我们选择 $\\theta_2=-1$。\n因此，$H_2$ 由权重 $w_{21}=-1, w_{22}=-1$ 和阈值 $\\theta_2=-1$ 定义。\n\n**3. 输出神经元 $O$：计算 $h_1 \\land h_2$。**\n输出神经元 $O$ 将隐藏神经元 $h_1$ 和 $h_2$ 的输出作为其输入。它必须计算 $h_1 \\land h_2$ 以实现最终的 $\\mathrm{XOR}$ 功能。\n我们来分析一下 $O$ 的输入：\n- 如果 $(x_1, x_2)=(0,0)$，则 $h_1=0$（或为假），$h_2=1$（与非为真）。最终输出应为 $0$。\n- 如果 $(x_1, x_2)=(0,1)$，则 $h_1=1$，$h_2=1$。最终输出应为 $1$。\n- 如果 $(x_1, x_2)=(1,0)$，则 $h_1=1$，$h_2=1$。最终输出应为 $1$。\n- 如果 $(x_1, x_2)=(1,1)$，则 $h_1=1$，$h_2=0$。最终输出应为 $0$。\n神经元 $O$ 必须仅在其输入 $(h_1, h_2)$ 为 $(1,1)$ 时输出 $1$。\n设 $O$ 的权重为 $w_{o1}=1, w_{o2}=1$。加权和为 $h_1+h_2$。\n- 如果 $(h_1, h_2)=(0,1)$ 或 $(1,0)$，和为 $1$。\n- 如果 $(h_1, h_2)=(1,1)$，和为 $2$。\n为仅在 $(1,1)$ 时激活，阈值 $\\theta_o$ 必须满足 $1  \\theta_o \\le 2$。我们选择 $\\theta_o=2$。\n因此，$O$ 由权重 $w_{o1}=1, w_{o2}=1$ 和阈值 $\\theta_o=2$ 定义。\n\n这就完成了 $\\mathrm{XOR}$ 子网络的设计。\n- **$\\mathrm{XOR}$ 子网络的规模**：它包含 $2$ 个隐藏神经元和 $1$ 个输出神经元，总共 $3$ 个神经元。\n- **$\\mathrm{XOR}$ 子网络的深度**：信号路径从输入流经隐藏层（第一层神经元）到输出层（第二层神经元）。因此，深度为 $2$。\n\n接下来，我们分析整个 $\\mathrm{PARITY}_{n}$ 网络，该网络是作为这些 $\\mathrm{XOR}$ 子网络的“分层平衡二叉树”构建的。\n\n**网络规模的推导：**\n函数 $\\mathrm{PARITY}_{n}(x_1, \\dots, x_n)$ 等价于 $x_1 \\oplus x_2 \\oplus \\dots \\oplus x_n$，其中 $\\oplus$ 表示 $\\mathrm{XOR}$ 运算。计算一个包含 $n-1$ 个二元运算符的表达式需要恰好 $n-1$ 次该运算符的应用。平衡二叉树是安排这些运算的一种方式。例如，对于 $n=4$，我们计算 $\\mathrm{XOR}(\\mathrm{XOR}(x_1, x_2), \\mathrm{XOR}(x_3, x_4))$。这使用了 $3$（即 $4-1$）次 $\\mathrm{XOR}$ 运算。对于任何 $n$，这样的树形结构将需要 $n-1$ 个双输入 $\\mathrm{XOR}$ 函数。\n由于每个 $\\mathrm{XOR}$ 子网络由 $3$ 个神经元组成，$\\mathrm{PARITY}_{n}$ 网络中的神经元总数（其规模）为：\n$$ \\text{规模} = (\\text{XOR子网络数量}) \\times (\\text{每个子网络的神经元数量}) = (n-1) \\times 3 = 3(n-1) $$\n\n**网络深度的推导：**\n深度是从输入到最终输出的任何路径上的最大神经元层数。该网络是 $\\mathrm{XOR}$ 模块构成的树。我们首先需要找到树中这些模块的层数。\n第一层 $\\mathrm{XOR}$ 模块接收 $n$ 个输入并产生 $\\lceil n/2 \\rceil$ 个输出。第二层将这 $\\lceil n/2 \\rceil$ 个信号作为输入，并产生 $\\lceil (\\lceil n/2 \\rceil)/2 \\rceil$ 个输出，以此类推。这个过程一直持续到只剩下一个输出。\n在这个模块树中的层数，我们称之为 $L$，是从 $k=n$ 开始，必须应用函数 $f(k)=\\lceil k/2 \\rceil$ 直到达到 $1$ 的次数。这等价于以 2 为底的对数的上取整的定义。\n例如，如果 $n=8$，输入按 $8 \\to 4 \\to 2 \\to 1$ 减少（$3$ 层）。$\\lceil \\log_2 8 \\rceil = 3$。\n如果 $n=7$，输入按 $7 \\to 4 \\to 2 \\to 1$ 减少（$3$ 层）。$\\lceil \\log_2 7 \\rceil = 3$。\n所以，$\\mathrm{XOR}$ 子网络的层数是 $L = \\lceil \\log_2 n \\rceil$。\n从初始输入位到最终输出的最长路径将穿过这 $L$ 个层级中的每一层的一个 $\\mathrm{XOR}$ 子网络。由于每个 $\\mathrm{XOR}$ 子网络的深度为 $2$ 个神经元层，$\\mathrm{PARITY}_{n}$ 网络的总深度为：\n$$ \\text{深度} = (\\text{模块层数}) \\times (\\text{每个模块的深度}) = \\lceil \\log_2 n \\rceil \\times 2 = 2 \\lceil \\log_2 n \\rceil $$\n注意，对于边界情况 $n=1$，$\\mathrm{PARITY}_1(x_1)=x_1$，这需要 $0$ 个神经元，深度为 $0$。我们的公式给出规模$=3(1-1)=0$ 和深度$=2\\lceil\\log_2 1\\rceil=0$，两者是一致的。\n\n推导出的闭式表达式为：深度是 $2 \\lceil \\log_2 n \\rceil$，规模是 $3(n-1)$。",
            "answer": "$$\n\\boxed{2 \\lceil \\log_2 n \\rceil \\quad 3(n-1)}\n$$"
        },
        {
            "introduction": "在了解了如何构建网络之后，我们现在回过头来更深入地审视神经元本身的核心特性。MP模型中简洁的“硬阈值”激活函数，虽然简单，却对模型的稳定性有着深远的影响。本练习将引导您探索该模型对输入噪声的“脆弱性”，这是在输入信号不可避免地存在波动的现实世界应用中一个至关重要的概念 。通过精确量化这种敏感性，您将对理论模型与实际应用之间的差距有更深刻的认识。",
            "id": "4065167",
            "problem": "考虑一个具有二进制输入、固定突触权重和硬阈值激活的麦卡洛克-皮茨神经元。设输入为 $x_{1}, x_{2}, \\dots, x_{n} \\in \\{0,1\\}$，突触权重为 $w_{1}, w_{2}, \\dots, w_{n} \\in \\mathbb{R}$，阈值为 $\\theta \\in \\mathbb{R}$。将神经元的输出定义为 $y = H\\!\\left(\\sum_{i=1}^{n} w_{i} x_{i} - \\theta\\right)$，其中 $H(\\cdot)$ 是赫维赛德阶跃函数，当 $z \\ge 0$ 时 $H(z) = 1$，当 $z  0$ 时 $H(z) = 0$。在一个单比特噪声模型中，一个输入坐标 $j \\in \\{1, \\dots, n\\}$ 被均匀随机地选择并翻转 $x_{j} \\mapsto 1 - x_{j}$。\n\n仅从这些核心定义出发，从概念上解释为什么硬阈值使得神经元在阈值附近对小的输入扰动表现出脆弱性。然后，推导在何种条件下翻转单个比特 $x_{j}$ 会使输出 $y$ 翻转，这些条件需要用有符号间隔 $m = \\sum_{i=1}^{n} w_{i} x_{i} - \\theta$、权重 $w_{j}$ 和比特值 $x_{j}$ 来表示。你的推导不应假设 $n$、$\\theta$、$w_{i}$ 或 $x_{i}$ 的任何特定值。\n\n最后，将你的条件应用于 $n = 7$，权重 $(w_{1}, \\dots, w_{7}) = (0.9, 1.6, -0.7, 1.1, 0.5, -1.2, 0.8)$，阈值 $\\theta = 1.9$，以及输入 $(x_{1}, \\dots, x_{7}) = (1, 1, 0, 1, 0, 1, 0)$ 的具体实例。在单比特噪声模型下（即均匀随机翻转一个输入坐标），计算输出翻转的确切概率。将你的最终概率表示为最简分数。无需四舍五入，也不涉及单位。",
            "solution": "该问题要求对一个麦卡洛克-皮茨神经元进行三部分分析：对其脆弱性的概念性解释，在单比特噪声下输出翻转条件的通用推导，以及针对给定实例的翻转概率的具体计算。\n\n首先，我们解决概念性问题。麦卡洛克-皮茨神经元的输出由赫维赛德阶跃函数 $y = H(m)$ 决定，其中 $m = \\sum_{i=1}^{n} w_{i} x_{i} - \\theta$ 是有符号间隔。赫维赛德函数 $H(z)$ 的定义特征是其在 $z=0$ 处的不连续性，其值在此处从 $z  0$ 时的 $0$ 跳跃到 $z \\ge 0$ 时的 $1$。这意味着自变量 $m$ 的一个穿过 $0$ 值的无穷小变化（例如，从一个任意小的负值 $-\\epsilon$ 变为一个任意小的非负值 $\\delta$）会导致神经元输出的最大变化，即从 $y=0$ 变为 $y=1$。一个输入扰动，例如翻转单个比特 $x_j$，会引起加权和 $\\sum w_i x_i$ 的变化，从而引起间隔 $m$ 的变化。如果神经元的初始状态使其间隔 $m$ 非常接近阈值 $0$（即 $|m|$ 很小），那么即使一个小的输入扰动，只要它在 $m$ 中产生的变化量大于 $|m|$，就可能翻转间隔的符号。这个符号的改变迫使输出在 $0$ 和 $1$ 之间跳跃。当神经元在其阈值附近（$m \\approx 0$）工作时，其输出对小的输入扰动的这种高敏感性，正是脆弱性（brittleness）的含义。\n\n接下来，我们推导输出翻转的充要条件。设初始输入向量为 $\\mathbf{x} = (x_1, \\dots, x_n)$，初始加权和为 $S = \\sum_{i=1}^{n} w_{i} x_{i}$，初始间隔为 $m = S - \\theta$。初始输出为 $y = H(m)$。当单个输入比特 $x_j$ 被翻转为 $1-x_j$ 时，计算新的加权和 $S'$。\n如果 $x_j=0$ 被翻转为 $1$，新的和为 $S' = S + w_j$。新的间隔为 $m' = m+w_j$。\n如果 $x_j=1$ 被翻转为 $0$，新的和为 $S' = S - w_j$。新的间隔为 $m' = m-w_j$。\n当且仅当新的输出 $y' = H(m')$ 与初始输出 $y = H(m)$ 不同时，发生输出翻转。翻转有两种可能性：\n1.  输出从 $y=0$ 翻转到 $y'=1$。这要求初始间隔为负（$m  0$）且新间隔为非负（$m' \\ge 0$）。\n2.  输出从 $y=1$ 翻转到 $y'=0$。这要求初始间隔为非负（$m \\ge 0$）且新间隔为负（$m'  0$）。\n\n我们根据被翻转比特 $x_j$ 的初始值来分析这些可能性。\n\n情况1：翻转 $x_j=0$。新的间隔为 $m' = m+w_j$。\n如果（$m  0$ 且 $m+w_j \\ge 0$）或（$m \\ge 0$ 且 $m+w_j  0$），则发生翻转。\n对于第一个条件（$y=0 \\to y'=1$），我们有 $-w_j \\le m  0$。这仅在 $w_j>0$ 时才可能。\n对于第二个条件（$y=1 \\to y'=0$），我们有 $0 \\le m  -w_j$。这仅在 $w_j0$ 时才可能。\n\n情况2：翻转 $x_j=1$。新的间隔为 $m' = m-w_j$。\n如果（$m  0$ 且 $m-w_j \\ge 0$）或（$m \\ge 0$ 且 $m-w_j  0$），则发生翻转。\n对于第一个条件（$y=0 \\to y'=1$），我们有 $w_j \\le m  0$。这仅在 $w_j0$ 时才可能。\n对于第二个条件（$y=1 \\to y'=0$），我们有 $0 \\le m  w_j$。这仅在 $w_j>0$ 时才可能。\n\n这四个条件是翻转比特 $x_j$ 时输出发生翻转的充要条件。\n\n最后，我们将这些条件应用于所提供的具体实例并计算翻转的概率。\n参数为：$n=7$，权重 $(w_{1}, \\dots, w_{7}) = (0.9, 1.6, -0.7, 1.1, 0.5, -1.2, 0.8)$，阈值 $\\theta = 1.9$，输入向量 $\\mathbf{x} = (x_{1}, \\dots, x_{7}) = (1, 1, 0, 1, 0, 1, 0)$。\n\n首先，我们计算神经元的初始状态。\n加权和为 $S = \\sum_{i=1}^{7} w_{i} x_{i}$：\n$S = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5 + w_6 x_6 + w_7 x_7$\n$S = (0.9)(1) + (1.6)(1) + (-0.7)(0) + (1.1)(1) + (0.5)(0) + (-1.2)(1) + (0.8)(0)$\n$S = 0.9 + 1.6 + 0 + 1.1 + 0 - 1.2 + 0 = 2.4$\n初始间隔为 $m = S - \\theta = 2.4 - 1.9 = 0.5$。\n由于 $m = 0.5 \\ge 0$，初始输出为 $y = H(0.5) = 1$。\n\n只有当新输出为 $y'=0$ 时，输出才会发生翻转。这是 $y=1 \\to y'=0$ 的情况。我们现在检查每个输入坐标 $j \\in \\{1, \\dots, 7\\}$，以确定翻转它是否会导致翻转。\n\n对于 $x_j=0$ 的 $j$（即 $j=3, 5, 7$）：\n翻转的条件是 $0 \\le m  -w_j$，这要求 $w_j  0$。\n- 对于 $j=3$：$x_3=0$，$w_3 = -0.7$。这里 $w_3  0$。我们检查条件：$0 \\le 0.5  -(-0.7) = 0.7$。该条件成立。翻转 $x_3$ 会导致翻转。\n- 对于 $j=5$：$x_5=0$，$w_5 = 0.5$。这里 $w_5$ 不是负数，因此不满足翻转条件。不发生翻转。\n- 对于 $j=7$：$x_7=0$，$w_7 = 0.8$。这里 $w_7$ 不是负数，因此不满足翻转条件。不发生翻转。\n\n对于 $x_j=1$ 的 $j$（即 $j=1, 2, 4, 6$）：\n翻转的条件是 $0 \\le m  w_j$，这要求 $w_j > 0$。\n- 对于 $j=1$：$x_1=1$，$w_1 = 0.9$。这里 $w_1 > 0$。我们检查条件：$0 \\le 0.5  0.9$。该条件成立。翻转 $x_1$ 会导致翻转。\n- 对于 $j=2$：$x_2=1$，$w_2 = 1.6$。这里 $w_2 > 0$。我们检查条件：$0 \\le 0.5  1.6$。该条件成立。翻转 $x_2$ 会导致翻转。\n- 对于 $j=4$：$x_4=1$，$w_4 = 1.1$。这里 $w_4 > 0$。我们检查条件：$0 \\le 0.5  1.1$。该条件成立。翻转 $x_4$ 会导致翻转。\n- 对于 $j=6$：$x_6=1$，$w_6 = -1.2$。这里 $w_6$ 不是正数，因此不满足翻转条件。不发生翻转。\n\n总而言之，对于索引 $j \\in \\{1, 2, 3, 4\\}$，翻转输入比特会导致输出翻转。共有 $4$ 个这样的索引。\n单比特噪声模型会从 $j \\in \\{1, \\dots, 7\\}$ 中均匀随机地选择一个输入坐标。可能的选择总数为 $n=7$。导致输出翻转的选择数量为 $4$。\n翻转的概率是导致翻转的结果数与总结果数的比率：\n$P(\\text{flip}) = \\frac{\\text{Number of coordinates that cause a flip}}{\\text{Total number of coordinates}} = \\frac{4}{7}$。\n此分数已为最简形式。",
            "answer": "$$\\boxed{\\frac{4}{7}}$$"
        },
        {
            "introduction": "我们已经探讨了如何构建前馈网络以及如何分析单个神经元的特性。现在，让我们引入反馈，创建一个循环网络，看看会发生什么。这个练习将揭示网络动力学如何导致复杂的行为，例如振荡（极限环），以及这些行为如何对神经更新的精确时序（同步与异步）高度敏感 。这不仅是从静态计算到动态过程的飞跃，也为理解记忆、模式生成等更高级的神经计算功能奠定了基础。",
            "id": "4065112",
            "problem": "考虑McCulloch-Pitts神经元模型，其中每个神经元具有一个二元状态 $x_i(t) \\in \\{0,1\\}$，并根据加权和后跟阈值非线性进行更新。神经元 $i$ 的状态更新由 $x_i(t+1) = H\\left(\\sum_{j} w_{ij} x_j(t) - \\theta_i\\right)$ 给出，其中 $H(\\cdot)$ 是赫维赛德阶跃函数，当其参数大于或等于 $0$ 时返回 $1$，否则返回 $0$，$w_{ij}$ 是从神经元 $j$到神经元 $i$ 的权重，$\\theta_i$ 是神经元 $i$ 的阈值。构建一个由标记为 $A$、$B$ 和 $C$ 的 $n = 3$ 个McCulloch-Pitts神经元组成的循环网络，该网络具有环形拓扑和如下意义上的复制动态：神经元 $A$ 复制神经元 $C$，神经元 $B$ 复制神经元 $A$，神经元 $C$ 复制神经元 $B$。将权重矩阵 $W$ 和阈值向量 $\\boldsymbol{\\theta}$ 形式化地指定为 $W = \\begin{bmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 0  1  0 \\end{bmatrix}$ 和 $\\boldsymbol{\\theta} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$。这确保了更新规则简化为从环中指定的前驱进行逻辑复制。\n\n定义两种更新机制：\n- 同步更新：所有神经元根据当前网络状态 $\\mathbf{x}(t) = (x_A(t), x_B(t), x_C(t))$ 同时更新，一步生成 $\\mathbf{x}(t+1)$。\n- 具有固定顺序序列的异步更新：在一个宏步骤中，按固定顺序 $(A, B, C)$ 执行三个微步骤。在每个微步骤中，使用当前状态向量更新目标神经元，在进入下一个神经元之前立即提交新值。宏步骤映射 $\\mathbf{x}(t) \\mapsto \\mathbf{x}(t+1)$ 是这三个微步骤的复合。\n\n对于每种机制，考虑状态图，其顶点是所有 $2^3 = 8$ 个二元状态 $\\{0,1\\}^3$，其有向边表示由网络动态引起的单步（对于同步机制）或单个宏步骤（对于异步顺序机制）的转换。\n\n从上述基本定义出发，分析在该网络中异步更新如何导致与同步更新不同的极限环。通过构建完整的状态转移函数，计算两种机制的极限环并分析它们的吸引盆。将状态表示为3位二进制向量 $(x_A,x_B,x_C)$，通过 $x_A \\cdot 4 + x_B \\cdot 2 + x_C \\cdot 1$ 编码为 $[0,7]$ 范围内的整数。\n\n您的程序必须实现指定的精确网络和更新规则，并根据状态图生成以下输出：\n- 对于下面定义的测试套件中的每个初始状态，计算在同步更新和异步顺序更新下达到的极限环的长度。极限环的长度是环中不同状态的数量。如果达到不动点，则长度为 $1$。\n- 计算在同步状态图和异步顺序状态图中，遍历所有 $8$ 个状态时，不同极限环的数量。\n- 计算同步机制和异步顺序机制的吸引盆大小的多重集。一个极限环的吸引盆大小是，在相应更新映射的重复应用下，最终到达该环的状态数量。\n\n测试套件：\n- 使用初始状态 $\\mathbf{x}(0)$ 等于 $(0,0,1)$、$(1,0,0)$、$(1,1,1)$、$(0,0,0)$ 和 $(0,1,1)$，分别编码为整数 $1$、$4$、$7$、$0$ 和 $3$。\n- 对于这些初始状态中的每一个，计算如上所述的同步和异步顺序极限环长度。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该行必须按顺序包含以下项目：\n- 一个包含5个整数的列表，给出测试套件中初始状态按顺序 $[1,4,7,0,3]$ 的同步极限环长度。\n- 一个包含5个整数的列表，给出测试套件中初始状态按顺序 $[1,4,7,0,3]$ 的异步顺序极限环长度。\n- 一个布尔值，指示两种机制在所有 $8$ 个状态上的极限环集合是否不同。\n- 一个整数，给出遍历所有 $8$ 个状态时的不同同步极限环的数量。\n- 一个整数，给出遍历所有 $8$ 个状态时的不同异步顺序极限环的数量。\n- 一个整数列表，给出所有不同同步极限环的吸引盆大小，按非递减顺序排序。\n- 一个整数列表，给出所有不同异步顺序极限环的吸引盆大小，按非递减顺序排序。\n\n您的程序不得读取任何输入，并且除了指定的单行文本外，不得打印任何其他文本。",
            "solution": "用户希望分析一个3神经元McCulloch-Pitts网络，并比较其在同步和异步更新机制下的动态。\n\n### 步骤1：问题验证\n\n**1.1. 提取给定条件**\n- **模型**：McCulloch-Pitts神经元，状态为 $x_i(t) \\in \\{0,1\\}$，更新规则为 $x_i(t+1) = H\\left(\\sum_{j} w_{ij} x_j(t) - \\theta_i\\right)$。$H(\\cdot)$ 是赫维赛德阶跃函数：当 $z \\ge 0$ 时 $H(z) = 1$，当 $z  0$ 时 $H(z) = 0$。\n- **网络**：$n=3$ 个神经元，标记为 $A, B, C$。\n- **拓扑**：环形拓扑，具有复制动态：$A$ 复制 $C$，$B$ 复制 $A$，$C$ 复制 $B$。\n- **参数**：权重矩阵 $W = \\begin{bmatrix} 0  0  1 \\\\ 1  0  0 \\\\ 0  1  0 \\end{bmatrix}$ 和阈值向量 $\\boldsymbol{\\theta} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$。\n- **状态表示**：状态 $\\mathbf{x} = (x_A, x_B, x_C)$ 编码为整数 $k = x_A \\cdot 4 + x_B \\cdot 2 + x_C \\cdot 1$。状态空间为 $\\{0, 1, \\dots, 7\\}$。\n- **更新机制**：\n    1.  **同步**：所有神经元同时更新。\n    2.  **异步顺序**：神经元按固定顺序 $(A, B, C)$ 更新，每次更新立即生效。\n- **任务**：\n    1.  对每种更新机制，构建包含8个可能状态的状态转移图。\n    2.  对于给定的测试套件中的初始状态，计算在每种机制下达到的极限环的长度。\n    3.  计算每种机制的不同极限环的数量。\n    4.  计算每种机制的吸引盆大小的多重集。\n- **测试套件**：初始状态编码为整数 $1, 4, 7, 0, 3$。\n- **输出格式**：单行，用方括号括起来的逗号分隔列表，包含：\n    - 测试套件的同步环长度列表。\n    - 测试套件的异步环长度列表。\n    - 指示极限环集合是否不同的布尔值。\n    - 同步极限环的数量。\n    - 异步极限环的数量。\n    - 同步吸引盆大小的排序列表。\n    - 异步吸引盆大小的排序列表。\n\n**1.2. 使用提取的给定条件进行验证**\n- **科学依据**：该问题基于人工神经网络的成熟理论，特别是McCulloch-Pitts模型。分析不同更新方案（同步与异步）下的网络动态是循环网络和离散动力系统研究中的一个基本课题。\n- **良定性**：问题已完全指定。神经元模型、网络参数（$W, \\boldsymbol{\\theta}$）、状态空间和更新规则都已明确定义。状态空间是有限的，保证了动态最终会进入一个极限环（包括不动点）。所有要求的输出都是基于指定动态的确定性计算。\n- **客观性**：问题以精确的数学术语陈述，没有主观或模糊的语言。\n\n**1.3. 结论与行动**\n该问题具有科学合理性、良定性和客观性。它是**有效的**。我们继续进行求解。\n\n### 步骤2：推导与求解\n\n状态向量为 $\\mathbf{x}(t) = (x_A(t), x_B(t), x_C(t))$。神经元 $i$ 的更新规则是 $x_i(t+1) = H(\\mathbf{w}_i \\cdot \\mathbf{x}(t) - \\theta_i)$，其中 $\\mathbf{w}_i$ 是 $W$ 的第 $i$ 行。\n\n让我们验证当所有 $i$ 的 $\\theta_i = 0.5$ 时的“复制”动态。\n- 神经元 $A$（索引1）：输入为 $\\sum_j w_{Aj} x_j - \\theta_A = 1 \\cdot x_C - 0.5$。输出 $x_A(t+1) = H(x_C(t) - 0.5)$ 在 $x_C(t)=1$ 时为 $1$，在 $x_C(t)=0$ 时为 $0$。因此，$x_A(t+1) = x_C(t)$。\n- 神经元 $B$（索引2）：输入为 $\\sum_j w_{Bj} x_j - \\theta_B = 1 \\cdot x_A - 0.5$。输出 $x_B(t+1) = H(x_A(t) - 0.5)$ 在 $x_A(t)=1$ 时为 $1$，在 $x_A(t)=0$ 时为 $0$。因此，$x_B(t+1) = x_A(t)$。\n- 神经元 $C$（索引3）：输入为 $\\sum_j w_{Cj} x_j - \\theta_C = 1 \\cdot x_B - 0.5$。输出 $x_C(t+1) = H(x_B(t) - 0.5)$ 在 $x_B(t)=1$ 时为 $1$，在 $x_B(t)=0$ 时为 $0$。因此，$x_C(t+1) = x_B(t)$。\n\n参数正确地实现了所描述的复制动态。我们现在分析两种更新机制。\n\n**2.1. 同步更新机制**\n在这种机制下，所有神经元根据时间 $t$ 的状态同时更新。新状态 $\\mathbf{x}(t+1) = (x_A(t+1), x_B(t+1), x_C(t+1))$ 由以下公式给出：\n$$ x_A(t+1) = x_C(t) $$\n$$ x_B(t+1) = x_A(t) $$\n$$ x_C(t+1) = x_B(t) $$\n所以，状态向量的转换为 $\\mathbf{x}(t+1) = (x_C(t), x_A(t), x_B(t))$。这是对 $\\mathbf{x}(t)$ 分量的一个置换。我们为所有 $2^3=8$ 个状态构建状态转移图，使用整数编码 $k = 4x_A + 2x_B + 1x_C$。\n\n- $0=(0,0,0) \\to (0,0,0)=0$\n- $1=(0,0,1) \\to (1,0,0)=4$\n- $2=(0,1,0) \\to (0,0,1)=1$\n- $3=(0,1,1) \\to (1,0,1)=5$\n- $4=(1,0,0) \\to (0,1,0)=2$\n- $5=(1,0,1) \\to (1,1,0)=6$\n- $6=(1,1,0) \\to (0,1,1)=3$\n- $7=(1,1,1) \\to (1,1,1)=7$\n\n完整的状态转移映射为 $T_{sync}: \\{0 \\to 0, 1 \\to 4, 2 \\to 1, 3 \\to 5, 4 \\to 2, 5 \\to 6, 6 \\to 3, 7 \\to 7\\}$。\n该动态将状态空间划分为以下不相交的集合：\n- **不动点：** $\\{0\\}$。这是一个长度为1的极限环。其吸引盆为 $\\{0\\}$。大小：1。\n- **不动点：** $\\{7\\}$。这是一个长度为1的极限环。其吸引盆为 $\\{7\\}$。大小：1。\n- **环：** $1 \\to 4 \\to 2 \\to 1$。这是一个长度为3的极限环。其吸引盆为 $\\{1, 2, 4\\}$。大小：3。\n- **环：** $3 \\to 5 \\to 6 \\to 3$。这是一个长度为3的极限环。其吸引盆为 $\\{3, 5, 6\\}$。大小：3。\n\n**2.2. 异步顺序更新机制**\n在这种机制下，神经元按固定顺序 $(A, B, C)$ 更新。设时间 $t$ 的状态为 $(x_A, x_B, x_C)$。一个宏步骤包含三个微步骤：\n1.  更新 $A$：$x'_A = x_C$。状态变为 $(x'_A, x_B, x_C)$。\n2.  更新 $B$：$x'_B = x'_A$。状态变为 $(x'_A, x'_B, x_C)$。\n3.  更新 $C$：$x'_C = x'_B$。状态变为 $(x'_A, x'_B, x'_C)$。\n\n一个宏步骤后的最终状态是 $\\mathbf{x}(t+1) = (x'_A, x'_B, x'_C)$。通过代入：\n$$ x_A(t+1) = x_C(t) $$\n$$ x_B(t+1) = x_A(t+1) = x_C(t) $$\n$$ x_C(t+1) = x_B(t+1) = x_C(t) $$\n转换为 $\\mathbf{x}(t+1) = (x_C(t), x_C(t), x_C(t))$。下一个状态完全由神经元 $C$ 的当前状态决定。如果 $x_C(t)=0$，下一个状态是 $(0,0,0)=0$。如果 $x_C(t)=1$，下一个状态是 $(1,1,1)=7$。\n\n状态转移图为：\n- $x_C=0$ 的状态：$\\{0, 2, 4, 6\\}$。所有都转移到状态 $0$。\n  - $0=(0,0,0) \\to (0,0,0)=0$\n  - $2=(0,1,0) \\to (0,0,0)=0$\n  - $4=(1,0,0) \\to (0,0,0)=0$\n  - $6=(1,1,0) \\to (0,0,0)=0$\n- $x_C=1$ 的状态：$\\{1, 3, 5, 7\\}$。所有都转移到状态 $7$。\n  - $1=(0,0,1) \\to (1,1,1)=7$\n  - $3=(0,1,1) \\to (1,1,1)=7$\n  - $5=(1,0,1) \\to (1,1,1)=7$\n  - $7=(1,1,1) \\to (1,1,1)=7$\n\n完整的状态转移映射为 $T_{async}: \\{0 \\to 0, 1 \\to 7, 2 \\to 0, 3 \\to 7, 4 \\to 0, 5 \\to 7, 6 \\to 0, 7 \\to 7\\}$。\n该动态将状态空间划分为两个集合：\n- **不动点：** $\\{0\\}$。这是一个长度为1的极限环。其吸引盆为 $\\{0, 2, 4, 6\\}$。大小：4。\n- **不动点：** $\\{7\\}$。这是一个长度为1的极限环。其吸引盆为 $\\{1, 3, 5, 7\\}$。大小：4。\n\n**2.3. 最终结果的计算**\n我们现在根据上述分析计算问题陈述要求的具体量。\n测试套件包含初始状态 $1, 4, 7, 0, 3$。\n\n- **同步极限环长度**：\n  - 状态 $1$：位于环 $\\{1, 4, 2\\}$ 中。长度为 $3$。\n  - 状态 $4$：位于环 $\\{1, 4, 2\\}$ 中。长度为 $3$。\n  - 状态 $7$：位于环 $\\{7\\}$ 中。长度为 $1$。\n  - 状态 $0$：位于环 $\\{0\\}$ 中。长度为 $1$。\n  - 状态 $3$：位于环 $\\{3, 5, 6\\}$ 中。长度为 $3$。\n  - 结果：`[3, 3, 1, 1, 3]`\n\n- **异步极限环长度**：\n  - 状态 $1$：转移到 $7$，是一个不动点。长度为 $1$。\n  - 状态 $4$：转移到 $0$，是一个不动点。长度为 $1$。\n  - 状态 $7$：是不动点 $\\{7\\}$。长度为 $1$。\n  - 状态 $0$：是不动点 $\\{0\\}$。长度为 $1$。\n  - 状态 $3$：转移到 $7$，是一个不动点。长度为 $1$。\n  - 结果：`[1, 1, 1, 1, 1]`\n\n- **极限环是否不同**：同步环的集合是 $\\{\\{0\\}, \\{7\\}, \\{1, 2, 4\\}, \\{3, 5, 6\\}\\}$。异步环的集合是 $\\{\\{0\\}, \\{7\\}\\}$。这些集合是不同的。结果：`True`。\n\n- **不同极限环的数量**：\n  - 同步：$4$ 个环。\n  - 异步：$2$ 个环。\n\n- **吸引盆大小（排序后）**：\n  - 同步：吸引盆的大小为 $1, 1, 3, 3$。结果：`[1, 1, 3, 3]`。\n  - 异步：吸引盆的大小为 $4, 4$。结果：`[4, 4]`。\n\n这些结果将由程序计算和格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Analyzes the dynamics of a 3-neuron McCulloch-Pitts network\n    under synchronous and asynchronous update regimes.\n    \"\"\"\n    \n    num_states = 8\n    \n    # --- Compute state transition maps ---\n    \n    # Synchronous updates: x(t+1) = (xC(t), xA(t), xB(t))\n    next_state_sync = np.zeros(num_states, dtype=int)\n    for i in range(num_states):\n        x_a = (i  2)  1\n        x_b = (i  1)  1\n        x_c = i  1\n        \n        next_x_a = x_c\n        next_x_b = x_a\n        next_x_c = x_b\n        \n        next_state_sync[i] = (next_x_a  2) | (next_x_b  1) | next_x_c\n\n    # Asynchronous sequential updates (A, B, C): x(t+1) = (xC(t), xC(t), xC(t))\n    next_state_async = np.zeros(num_states, dtype=int)\n    for i in range(num_states):\n        x_c = i  1\n        \n        # The next state is (xC, xC, xC)\n        if x_c == 1:\n            next_state_async[i] = 7  # (1, 1, 1)\n        else:\n            next_state_async[i] = 0  # (0, 0, 0)\n            \n    def find_dynamics(transitions):\n        \"\"\"\n        Analyzes a state transition map to find limit cycles and basins of attraction.\n        \n        Args:\n            transitions (np.ndarray): An array where transitions[i] is the next state from i.\n\n        Returns:\n            tuple: A tuple containing:\n                - state_to_cycle (dict): Maps each state to the frozenset representing its limit cycle.\n                - basins (dict): Maps each cycle (frozenset) to its basin size.\n        \"\"\"\n        n_states = len(transitions)\n        visited = [False] * n_states\n        state_to_cycle = {}\n        basins = {}\n\n        for i in range(n_states):\n            if visited[i]:\n                continue\n\n            path = []\n            path_indices = {}\n            current_state = i\n            \n            while current_state not in path_indices:\n                if visited[current_state]: # Path has merged into an already found basin\n                    # Find the cycle this path leads to\n                    cycle_set = state_to_cycle[current_state]\n                    break\n                \n                path.append(current_state)\n                path_indices[current_state] = len(path) - 1\n                current_state = transitions[current_state]\n            else: # Loop completed without break, new cycle found\n                cycle_start_index = path_indices[current_state]\n                cycle_states = path[cycle_start_index:]\n                cycle_set = frozenset(cycle_states)\n\n            # Update basin info for all states in the traced path\n            if cycle_set not in basins:\n                basins[cycle_set] = 0\n\n            for state_in_path in path:\n                if not visited[state_in_path]:\n                    visited[state_in_path] = True\n                    state_to_cycle[state_in_path] = cycle_set\n                    basins[cycle_set] += 1\n        \n        return state_to_cycle, basins\n\n    # --- Analyze both regimes ---\n    state_to_cycle_sync, basins_sync = find_dynamics(next_state_sync)\n    state_to_cycle_async, basins_async = find_dynamics(next_state_async)\n    \n    # --- Assemble results for output ---\n    test_suite = [1, 4, 7, 0, 3]\n\n    # 1. Synchronous limit cycle lengths for test suite\n    sync_cycle_lengths = [len(state_to_cycle_sync[s]) for s in test_suite]\n\n    # 2. Asynchronous limit cycle lengths for test suite\n    async_cycle_lengths = [len(state_to_cycle_async[s]) for s in test_suite]\n\n    # 3. Boolean indicating if the set of limit cycles differs\n    sync_cycles_set = set(basins_sync.keys())\n    async_cycles_set = set(basins_async.keys())\n    cycles_differ = sync_cycles_set != async_cycles_set\n\n    # 4. Number of distinct synchronous limit cycles\n    num_sync_cycles = len(sync_cycles_set)\n\n    # 5. Number of distinct asynchronous sequential limit cycles\n    num_async_cycles = len(async_cycles_set)\n    \n    # 6. Sorted list of synchronous basin sizes\n    sync_basin_sizes = sorted(list(basins_sync.values()))\n\n    # 7. Sorted list of asynchronous sequential basin sizes\n    async_basin_sizes = sorted(list(basins_async.values()))\n\n    # --- Format final output ---\n    final_results = [\n        sync_cycle_lengths,\n        async_cycle_lengths,\n        cycles_differ,\n        num_sync_cycles,\n        num_async_cycles,\n        sync_basin_sizes,\n        async_basin_sizes\n    ]\n    \n    # Format to a comma-separated list of items enclosed in brackets, with no spaces\n    output_str = str(final_results).replace(\" \", \"\")\n    \n    print(output_str)\n\nsolve()\n```"
        }
    ]
}