{
    "hands_on_practices": [
        {
            "introduction": "单层感知器的核心功能是基于一个线性决策边界对输入进行分类。本练习将让你实践应用感知器的数学公式，$y = \\operatorname{sgn}(w^{\\top} x + b)$，来确定几个数据点的分类。通过这个练习，你将加深对感知器作为划分输入空间的超平面的几何直观理解 。",
            "id": "4065769",
            "problem": "考虑一个实现为线性阈值单元的单层感知器，这是神经形态和类脑计算中的一个经典模型，它将一个输入向量 $x \\in \\mathbb{R}^2$ 映射到一个标量预激活值 $a(x) = w^{\\top} x + b$，然后是一个符号阈值输出 $y(x) = \\operatorname{sgn}(a(x))$，其中如果 $z  0$，则 $\\operatorname{sgn}(z) = 1$；如果 $z  0$，则 $\\operatorname{sgn}(z) = -1$；如果 $z = 0$，则 $\\operatorname{sgn}(z) = 0$，以明确编码正好位于决策边界上的点。这个定义与经过充分检验的线性阈值分类的表述及其决策边界 $w^{\\top} x + b = 0$ 是一致的。\n\n设感知器的参数为 $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ 和 $b = 5$。考虑输入集 $X = \\{x_1, x_2, x_3, x_4\\}$，其中\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad\nx_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad\nx_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}, \\quad\nx_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}.\n$$\n\n仅使用上述基本定义，计算分类输出 $y(x_i)$（对于 $i \\in \\{1,2,3,4\\}$）并确定哪个 $x_i$ 正好位于决策边界上。然后计算标量\n$$\nS = \\sum_{i=1}^{4} y(x_i).\n$$\n\n仅报告 S 的值作为最终答案。无需四舍五入。",
            "solution": "在尝试任何解答之前，需对问题陈述进行验证。\n\n**第一步：提取已知条件**\n- 模型：单层感知器（线性阈值单元）。\n- 输入向量：$x \\in \\mathbb{R}^2$。\n- 预激活函数：$a(x) = w^{\\top} x + b$。\n- 输出函数：$y(x) = \\operatorname{sgn}(a(x))$。\n- 符号函数定义：如果 $z  0$，则 $\\operatorname{sgn}(z) = 1$；如果 $z  0$，则 $\\operatorname{sgn}(z) = -1$；如果 $z = 0$，则 $\\operatorname{sgn}(z) = 0$。\n- 决策边界条件：$w^{\\top} x + b = 0$。\n- 感知器权重向量：$w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$。\n- 感知器偏置：$b = 5$。\n- 输入集：$X = \\{x_1, x_2, x_3, x_4\\}$。\n- 输入向量：\n$x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, $x_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$, $x_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$, $x_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}$。\n- 目标量：$S = \\sum_{i=1}^{4} y(x_i)$。\n\n**第二步：使用提取的已知条件进行验证**\n- 该问题是**有科学依据**且**客观**的。单层感知器是机器学习和计算神经科学中的一个基本概念。所提供的预激活、输出函数和决策边界的定义是标准的且在数学上是精确的。\n- 该问题是**适定**的。它提供了所有必要的组成部分——模型参数（$w, b$）和输入数据（$x_i$）——来计算所需的输出（$y(x_i)$）和最终的和（$S$）。计算会得出一个唯一的、确定性的答案。\n- 该问题的设置是**完整且一致**的。没有缺失信息，也没有任何矛盾的约束。$\\operatorname{sgn}(0)=0$ 的明确定义是至关重要的，并且与识别决策边界上点的任务一致地使用。\n\n**第三步：结论与行动**\n- 该问题被认为是**有效的**。将推导出解决方案。\n\n任务是计算总和 $S = \\sum_{i=1}^{4} y(x_i)$。这需要为每个输入向量 $x_i$ 计算分类输出 $y(x_i)$，而分类输出又取决于预激活值 $a(x_i)$。预激活的通用公式是 $a(x) = w^{\\top} x + b$。\n\n给定的参数是 $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ 和 $b = 5$。\n\n我们通过计算每个 $i \\in \\{1, 2, 3, 4\\}$ 的预激活值 $a(x_i)$ 和相应的输出 $y(x_i) = \\operatorname{sgn}(a(x_i))$ 来进行求解。\n\n对于 $i=1$：\n输入向量是 $x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n预激活值是：\n$$\na(x_1) = w^{\\top} x_1 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + 5 = (3)(1) + (-4)(2) + 5 = 3 - 8 + 5 = 0\n$$\n由于 $a(x_1) = 0$，输出为 $y(x_1) = \\operatorname{sgn}(0) = 0$。\n\n对于 $i=2$：\n输入向量是 $x_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$。\n预激活值是：\n$$\na(x_2) = w^{\\top} x_2 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + 5 = (3)(2) + (-4)(0) + 5 = 6 + 0 + 5 = 11\n$$\n由于 $a(x_2) = 11  0$，输出为 $y(x_2) = \\operatorname{sgn}(11) = 1$。\n\n对于 $i=3$：\n输入向量是 $x_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$。\n预激活值是：\n$$\na(x_3) = w^{\\top} x_3 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} + 5 = (3)(0) + (-4)(3) + 5 = 0 - 12 + 5 = -7\n$$\n由于 $a(x_3) = -7  0$，输出为 $y(x_3) = \\operatorname{sgn}(-7) = -1$。\n\n对于 $i=4$：\n输入向量是 $x_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}$。\n预激活值是：\n$$\na(x_4) = w^{\\top} x_4 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix} + 5 = (3)\\left(\\frac{10}{3}\\right) + (-4)\\left(\\frac{5}{2}\\right) + 5 = 10 - 10 + 5 = 5\n$$\n由于 $a(x_4) = 5  0$，输出为 $y(x_4) = \\operatorname{sgn}(5) = 1$。\n\n如果一个输入点 $x_i$ 的预激活值为零，即 $a(x_i) = w^{\\top} x_i + b = 0$，那么它就正好位于决策边界上。根据我们的计算，只有 $a(x_1) = 0$。因此，$x_1$ 是唯一一个正好位于决策边界上的输入点。\n\n最后，我们计算标量和 $S$：\n$$\nS = \\sum_{i=1}^{4} y(x_i) = y(x_1) + y(x_2) + y(x_3) + y(x_4)\n$$\n代入计算出的值：\n$$\nS = 0 + 1 + (-1) + 1 = 1\n$$\n总和的值为 1。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "在理解了感知器如何分类之后，下一步是学习它如何找到正确的参数（$w$ 和 $b$）。本练习将引导你逐步体验经典的“错误驱动”学习规则，即感知器仅在分类错误时才调整其权重和偏置。通过手动追踪算法的每一步，你将对决策边界如何收敛到有效解的过程建立起直观的认识 。",
            "id": "4065810",
            "problem": "考虑一个在神经形态事件驱动流中作为硬阈值分类器运行的单层感知器。该分类器具有权重向量 $w \\in \\mathbb{R}^{2}$ 和偏置 $b \\in \\mathbb{R}$，并将输入 $x \\in \\mathbb{R}^{2}$ 的标签预测为 $\\operatorname{sign}(w \\cdot x + b)$，其中标签 $y \\in \\{ -1, +1 \\}$。学习过程是错误驱动的，学习率 $\\eta = 1$：在接收到第 $t$ 个样本 $(x^{(t)}, y^{(t)})$ 时，计算激活值 $a^{(t)} = w^{(t-1)} \\cdot x^{(t)} + b^{(t-1)}$。如果 $y^{(t)} a^{(t)} \\le 0$，则根据以下公式进行更新：\n$$\nw^{(t)} = w^{(t-1)} + y^{(t)} x^{(t)}, \n\\quad\nb^{(t)} = b^{(t-1)} + y^{(t)},\n$$\n否则参数保持不变，即 $w^{(t)} = w^{(t-1)}$ 且 $b^{(t)} = b^{(t-1)}$。初始状态为 $w^{(0)} = (0, 0)$ 和 $b^{(0)} = 0$。\n\n$\\mathbb{R}^{2}$ 中一个固定的、有限的带标签输入集由以下给出：\n- $x_{1} = (1, 0)$，标签为 $y_{1} = +1$，\n- $x_{2} = (-0.1, 0)$，标签为 $y_{2} = -1$，\n- $x_{3} = (0, 1)$，标签为 $y_{3} = +1$。\n\n这些输入以一个无限有序流的形式呈现，该流无限重复循环 $(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3})$。呈现时间 $t \\in \\mathbb{N}$ 计入已处理样本的总数，从 $t = 1$ 开始。\n\n如果在处理时间 $t$ 的样本后，当前参数 $(w^{(t)}, b^{(t)})$ 能正确分类所有三个点 $\\{x_{1}, x_{2}, x_{3}\\}$（即，对于每个 $i \\in \\{1,2,3\\}$，都有 $y_{i}\\,(w^{(t)} \\cdot x_{i} + b^{(t)})  0$），则定义分类器在时间 $t$ 与数据一致。\n\n请明确计算连续呈现时间 $t$ 的参数状态序列 $(w^{(t)}, b^{(t)})$，直到分类器与数据变得一致的最早呈现时间 $T$（如上定义）。报告最小的呈现索引 $T$。您的最终答案必须是一个没有单位的整数。不需要四舍五入。",
            "solution": "用户要求我根据给定的问题陈述来解决一个问题。\n\n问题要求找到单层感知器分类器与给定数据集达到一致的最早呈现时间 $T$。学习过程是错误驱动的，通过迭代一个包含三个数据点的固定循环来进行。\n\n首先，我将验证问题陈述。\n\n### 第1步：提取已知条件\n- **模型**：单层感知器。\n- **输入空间**：$x \\in \\mathbb{R}^{2}$。\n- **标签空间**：$y \\in \\{ -1, +1 \\}$。\n- **分类器函数**：$\\operatorname{sign}(w \\cdot x + b)$，其中 $w \\in \\mathbb{R}^{2}$ 且 $b \\in \\mathbb{R}$。\n- **学习规则**：错误驱动（感知器算法），学习率 $\\eta = 1$。\n- **更新条件**：如果在步骤 $t$ 时 $y^{(t)} a^{(t)} \\le 0$，则发生更新，其中 $a^{(t)} = w^{(t-1)} \\cdot x^{(t)} + b^{(t-1)}$。\n- **更新方程**：\n    - $w^{(t)} = w^{(t-1)} + y^{(t)} x^{(t)}$\n    - $b^{(t)} = b^{(t-1)} + y^{(t)}$\n- **不更新条件**：如果 $y^{(t)} a^{(t)}  0$，则 $w^{(t)} = w^{(t-1)}$ 且 $b^{(t)} = b^{(t-1)}$。\n- **初始状态**：$w^{(0)} = (0, 0)$ 且 $b^{(0)} = 0$。\n- **数据集**：\n    - $x_{1} = (1, 0)$，标签为 $y_{1} = +1$。\n    - $x_{2} = (-0.1, 0)$，标签为 $y_{2} = -1$。\n    - $x_{3} = (0, 1)$，标签为 $y_{3} = +1$。\n- **输入流**：一个无限重复循环 $(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3})$ 的序列。\n- **呈现时间**：$t \\in \\mathbb{N}$ 从 $t=1$ 开始。\n- **一致性定义**：分类器在时间 $t$ 是一致的，如果参数 $(w^{(t)}, b^{(t)})$ 能正确分类所有三个点 $\\{x_{1}, x_{2}, x_{3}\\}$。正确分类意味着对于 $i \\in \\{1, 2, 3\\}$，都有 $y_{i}\\,(w^{(t)} \\cdot x_{i} + b^{(t)})  0$。\n- **目标**：找到分类器达到一致的最早呈现时间 $T$。\n\n### 第2步：使用提取的已知条件进行验证\n该问题具有科学依据，提法明确，且客观。它提供了机器学习的基石——感知器学习算法的标准表述。所有初始条件、数据、更新规则和停止准则都定义得非常精确。该数据集是线性可分的，这保证了算法将在有限步数内收敛到一个解。问题要求在特定的数据显示顺序下，求解一个可验证的量（收敛所需的步数）。问题陈述中没有矛盾、歧义或事实错误。\n\n### 第3步：结论与行动\n问题是有效的。我现在将开始求解。\n\n为了找到最早的呈现时间 $T$，我们必须从初始状态开始，一步步模拟学习过程，并在每次出错时更新参数。在每个步骤 $t$ 之后，我们检查得到的分类器 $(w^{(t)}, b^{(t)})$ 是否与整个数据集一致。\n\n在 $t=0$ 时参数的初始状态是 $w^{(0)} = (0, 0)$ 和 $b^{(0)} = 0$。\n\n**步骤 $t=1$**：\n呈现的第一个样本是 $(x^{(1)}, y^{(1)}) = (x_1, y_1) = ((1, 0), +1)$。\n此步骤之前的参数是 $(w^{(0)}, b^{(0)}) = ((0, 0), 0)$。\n激活值为 $a^{(1)} = w^{(0)} \\cdot x^{(1)} + b^{(0)} = (0, 0) \\cdot (1, 0) + 0 = 0$。\n我们检查错误条件：$y^{(1)}a^{(1)} = (+1)(0) = 0$。由于 $0 \\le 0$，发生了一个错误。\n我们更新参数：\n$w^{(1)} = w^{(0)} + y^{(1)}x^{(1)} = (0, 0) + (+1)(1, 0) = (1, 0)$。\n$b^{(1)} = b^{(0)} + y^{(1)} = 0 + 1 = 1$。\n第1步之后的状态是 $(w^{(1)}, b^{(1)}) = ((1, 0), 1)$。\n我们检查一致性：\n1. 对于 $(x_1, y_1)$：$y_1(w^{(1)} \\cdot x_1 + b^{(1)}) = +1((1, 0) \\cdot (1, 0) + 1) = 2  0$。（正确）\n2. 对于 $(x_2, y_2)$：$y_2(w^{(1)} \\cdot x_2 + b^{(1)}) = -1((1, 0) \\cdot (-0.1, 0) + 1) = -1(-0.1 + 1) = -0.9 \\le 0$。（错误）\n分类器不一致。\n\n**步骤 $t=2$**：\n第二个样本是 $(x^{(2)}, y^{(2)}) = (x_2, y_2) = ((-0.1, 0), -1)$。\n参数是 $(w^{(1)}, b^{(1)}) = ((1, 0), 1)$。\n激活值为 $a^{(2)} = w^{(1)} \\cdot x^{(2)} + b^{(1)} = (1, 0) \\cdot (-0.1, 0) + 1 = -0.1 + 1 = 0.9$。\n错误检查：$y^{(2)}a^{(2)} = (-1)(0.9) = -0.9 \\le 0$。发生了一个错误。\n更新：\n$w^{(2)} = w^{(1)} + y^{(2)}x^{(2)} = (1, 0) + (-1)(-0.1, 0) = (1, 0) + (0.1, 0) = (1.1, 0)$。\n$b^{(2)} = b^{(1)} + y^{(2)} = 1 + (-1) = 0$。\n第2步之后的状态：$(w^{(2)}, b^{(2)}) = ((1.1, 0), 0)$。\n一致性检查：\n1. 对于 $(x_1, y_1)$：$y_1(w^{(2)} \\cdot x_1 + b^{(2)}) = +1((1.1, 0) \\cdot (1, 0) + 0) = 1.1  0$。（正确）\n2. 对于 $(x_2, y_2)$：$y_2(w^{(2)} \\cdot x_2 + b^{(2)}) = -1((1.1, 0) \\cdot (-0.1, 0) + 0) = -1(-0.11) = 0.11  0$。（正确）\n3. 对于 $(x_3, y_3)$：$y_3(w^{(2)} \\cdot x_3 + b^{(2)}) = +1((1.1, 0) \\cdot (0, 1) + 0) = 0 \\le 0$。（错误）\n分类器不一致。\n\n**步骤 $t=3$**：\n第三个样本是 $(x^{(3)}, y^{(3)}) = (x_3, y_3) = ((0, 1), +1)$。\n参数是 $(w^{(2)}, b^{(2)}) = ((1.1, 0), 0)$。\n激活值为 $a^{(3)} = w^{(2)} \\cdot x^{(3)} + b^{(2)} = (1.1, 0) \\cdot (0, 1) + 0 = 0$。\n错误检查：$y^{(3)}a^{(3)} = (+1)(0) = 0 \\le 0$。发生了一个错误。\n更新：\n$w^{(3)} = w^{(2)} + y^{(3)}x^{(3)} = (1.1, 0) + (+1)(0, 1) = (1.1, 1)$。\n$b^{(3)} = b^{(2)} + y^{(3)} = 0 + 1 = 1$。\n第3步之后的状态：$(w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$。\n一致性检查：\n1. 对于 $(x_1, y_1)$：$y_1(w^{(3)} \\cdot x_1 + b^{(3)}) = +1((1.1, 1) \\cdot (1, 0) + 1) = 2.1  0$。（正确）\n2. 对于 $(x_2, y_2)$：$y_2(w^{(3)} \\cdot x_2 + b^{(3)}) = -1((1.1, 1) \\cdot (-0.1, 0) + 1) = -1(-0.11 + 1) = -0.89 \\le 0$。（错误）\n分类器不一致。\n\n**步骤 $t=4$**：\n数据流重复；第四个样本是 $(x^{(4)}, y^{(4)}) = (x_1, y_1) = ((1, 0), +1)$。\n参数是 $(w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$。\n激活值为 $a^{(4)} = w^{(3)} \\cdot x^{(4)} + b^{(3)} = (1.1, 1) \\cdot (1, 0) + 1 = 1.1 + 1 = 2.1$。\n错误检查：$y^{(4)}a^{(4)} = (+1)(2.1) = 2.1  0$。没有错误。\n参数不更新：$(w^{(4)}, b^{(4)}) = (w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$。\n由于参数自步骤 $t=3$ 以来没有改变，分类器仍然不一致。\n\n**步骤 $t=5$**：\n第五个样本是 $(x^{(5)}, y^{(5)}) = (x_2, y_2) = ((-0.1, 0), -1)$。\n参数是 $(w^{(4)}, b^{(4)}) = ((1.1, 1), 1)$。\n激活值为 $a^{(5)} = w^{(4)} \\cdot x^{(5)} + b^{(4)} = (1.1, 1) \\cdot (-0.1, 0) + 1 = -0.11 + 1 = 0.89$。\n错误检查：$y^{(5)}a^{(5)} = (-1)(0.89) = -0.89 \\le 0$。发生了一个错误。\n更新：\n$w^{(5)} = w^{(4)} + y^{(5)}x^{(5)} = (1.1, 1) + (-1)(-0.1, 0) = (1.1, 1) + (0.1, 0) = (1.2, 1)$。\n$b^{(5)} = b^{(4)} + y^{(5)} = 1 + (-1) = 0$。\n第5步之后的状态：$(w^{(5)}, b^{(5)}) = ((1.2, 1), 0)$。\n一致性检查：\n1. 对于 $(x_1, y_1)$：$y_1(w^{(5)} \\cdot x_1 + b^{(5)}) = +1((1.2, 1) \\cdot (1, 0) + 0) = 1.2  0$。（正确）\n2. 对于 $(x_2, y_2)$：$y_2(w^{(5)} \\cdot x_2 + b^{(5)}) = -1((1.2, 1) \\cdot (-0.1, 0) + 0) = -1(-0.12) = 0.12  0$。（正确）\n3. 对于 $(x_3, y_3)$：$y_3(w^{(5)} \\cdot x_3 + b^{(5)}) = +1((1.2, 1) \\cdot (0, 1) + 0) = 1  0$。（正确）\n在处理完 $t=5$ 时的样本后，分类器能正确分类数据集中的所有三个点。因此，分类器已达到一致。\n\n分类器达到一致的最早呈现时间 $T$ 是 $5$。",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "感知器算法的一个显著特性是，对于线性可分的数据，它保证能在有限步内找到一个解。本练习将理论与实践相结合，要求你实现并验证著名的感知器“错误上界”定理。你将基于数据的几何特性（其半径 $R$ 和间隔 $\\gamma$）计算理论上的错误次数上限，并将其与模拟训练中观察到的实际错误次数进行比较 。",
            "id": "4065892",
            "problem": "给定一个在 $\\mathbb{R}^d$ 中有限的带标签点集 $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{-1,+1\\}$，该点集可被一个穿过原点的超平面线性分离。考虑一个零偏置（阈值在原点）、单位学习率和标准更新的单层感知机。分类器对输入 $\\mathbf{x}$ 的预测是内积的符号，且仅在出错时执行更新。形式上，从 $\\mathbf{w}_0=\\mathbf{0}$ 开始，按顺序对每个训练样本 $(\\mathbf{x}_i,y_i)$ 进行处理，预测值为 $\\operatorname{sign}(\\mathbf{w}\\cdot \\mathbf{x}_i)$。当发生错误时（定义为 $y_i(\\mathbf{w}\\cdot \\mathbf{x}_i)\\le 0$），执行更新 $\\mathbf{w}\\leftarrow \\mathbf{w}+y_i\\mathbf{x}_i$。重复遍历数据集，直到完整一轮遍历没有产生任何错误。\n\n半径界限 $R$ 定义为 $R=\\max_i\\|\\mathbf{x}_i\\|_2$。数据集相对于最佳的穿过原点的单位分离向量的间隔 $\\gamma$ 定义为\n$$\n\\gamma \\;=\\; \\max_{\\|\\mathbf{w}\\|_2=1}\\;\\min_{i}\\; y_i\\,(\\mathbf{w}\\cdot \\mathbf{x}_i).\n$$\n\n仅使用这些核心定义和线性代数中的基本不等式，推导出单层感知机收敛定理所预测的理论错误界限（用 $R$ 和 $\\gamma$ 表示），并实现一个程序来完成以下任务：\n- 精确估计 $R$ 为 $R=\\max_i\\|\\mathbf{x}_i\\|_2$。\n- 通过搜索由角度 $\\theta$ 参数化的 $\\mathbb{R}^2$ 中的单位方向来估计 $\\gamma$，取所有点中最小有符号投影的最大值。使用 $[0,2\\pi)$ 上角度的密集均匀离散化来近似最大化过程。您的搜索必须是确定性的。\n- 从 $\\mathbf{w}_0=\\mathbf{0}$ 开始，模拟感知机训练序列，按照给定的固定顺序循环遍历数据集，并计算直到完整一轮遍历产生零错误为止的总错误次数。\n- 使用推导出的表达式计算理论错误界限，并将其与模拟中观察到的错误次数进行比较。\n\n您必须将此过程应用于以下三个 $\\mathbb{R}^2$ 中的数据集测试套件（所有列出的点都按其在训练中必须出现的顺序列出）：\n\n测试用例 1（中等半径和间隔）：\n- 正样本 ($y=+1$): $(2,1)$, $(1.5,1)$, $(2,2)$。\n- 负样本 ($y=-1$): $(-2,-1)$, $(-1.5,-1)$, $(-2,-2)$。\n\n测试用例 2（大半径，小间隔）：\n- 正样本 ($y=+1$): $(0.05,100)$, $(2,100)$, $(1,110)$。\n- 负样本 ($y=-1$): $(-0.05,-100)$, $(-2,-100)$, $(-1,-110)$。\n\n测试用例 3（小半径，较大间隔，近一维）：\n- 正样本 ($y=+1$): $(1,0)$, $(0.8,0)$。\n- 负样本 ($y=-1$): $(-1,0)$, $(-0.8,0)$。\n\n所有三个数据集都可以被一个穿过原点的超平面分离。对于每个测试用例：\n- 按规定估计 $R$ 和 $\\gamma$。\n- 根据这些量推导并计算理论错误界限。\n- 模拟感知机以获得观察到的错误计数。\n- 生成一个比较结果，指明观察到的错误数是否小于或等于理论界限。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身是一个形式为 $[B,M,C]$ 的列表，其中 $B$ 是作为浮点数的理论界限，$M$ 是作为整数的观察到的错误数，而 $C$ 是一个布尔值，指示是否 $M\\le B$。例如，输出应类似于 $[[B_1,M_1,C_1],[B_2,M_2,C_2],[B_3,M_3,C_3]]$，其中的数值由您的计算填充。\n\n角度单位：在估计 $\\gamma$ 时，对任何内部角度参数化使用弧度。\n\n不涉及物理单位。所有数值输出均表示为原始数字（无百分号）。程序必须是自包含的，并且不需要用户输入。",
            "solution": "用户提供的问题经评估有效。它在科学上基于单层感知机完善的理论，问题定义明确且客观。提供了解出唯一且有意义的解所需的所有数据和定义。任务涉及推导一个经典的理论结果（感知机错误界限），并在特定数据集上进行计算验证，这是机器学习中一个标准而严谨的练习。\n\n### 第一部分：感知机错误界限的推导\n\n问题要求推导单层感知机算法所犯错误次数的理论上限。设总错误次数为 $M$。将在犯错时刻的权重向量序列表示为 $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_M$。初始权重向量为 $\\mathbf{w}_0 = \\mathbf{0}$。\n\n设第 $k$ 次错误（$k=1, \\dots, M$）发生在训练样本 $(\\mathbf{x}_k, y_k)$ 上。请注意，为简化符号，我们对发生错误的数据集序列中的点进行了重新索引。应用感知机更新规则：\n$$\n\\mathbf{w}_k = \\mathbf{w}_{k-1} + y_k \\mathbf{x}_k\n$$\n第 $k$ 次错误的条件是：\n$$\ny_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) \\le 0\n$$\n假设数据集可被一个穿过原点的超平面线性分离。这保证了存在一个单位范数 $\\|\\mathbf{w}^*\\|_2=1$ 的权重向量 $\\mathbf{w}^*$ 和一个间隔 $\\gamma  0$，使得对于数据集中的所有点 $(\\mathbf{x}_i, y_i)$：\n$$\ny_i(\\mathbf{w}^* \\cdot \\mathbf{x}_i) \\ge \\gamma\n$$\n其中 $\\gamma$ 是在所有可能的单位范数分离向量中最大的此类间隔。\n\n推导过程通过为 $M$ 次错误后的最终权重向量 $\\mathbf{w}_M$ 建立两个不等式来进行。\n\n**1. 权重向量范数的上界**\n\n我们分析第 $k$ 次更新后权重向量的 L2 范数的平方：\n$$\n\\|\\mathbf{w}_k\\|_2^2 = \\|\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k\\|_2^2\n$$\n展开点积：\n$$\n\\|\\mathbf{w}_k\\|_2^2 = (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) \\cdot (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) = \\|\\mathbf{w}_{k-1}\\|_2^2 + 2y_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) + y_k^2\\|\\mathbf{x}_k\\|_2^2\n$$\n由于 $y_k \\in \\{-1, +1\\}$，我们有 $y_k^2=1$。根据错误条件，我们知道 $y_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) \\le 0$。将这些代入方程得到：\n$$\n\\|\\mathbf{w}_k\\|_2^2 \\le \\|\\mathbf{w}_{k-1}\\|_2^2 + \\|\\mathbf{x}_k\\|_2^2\n$$\n问题定义了半径 $R = \\max_i \\|\\mathbf{x}_i\\|_2$。因此，对于任何点 $\\mathbf{x}_k$，都有 $\\|\\mathbf{x}_k\\|_2^2 \\le R^2$。这导出了递推关系：\n$$\n\\|\\mathbf{w}_k\\|_2^2 \\le \\|\\mathbf{w}_{k-1}\\|_2^2 + R^2\n$$\n从 $\\mathbf{w}_0 = \\mathbf{0}$ 开始，递归地应用这个不等式，我们得到 $M$ 次错误后权重向量范数平方的上界：\n$$\n\\|\\mathbf{w}_M\\|_2^2 \\le \\|\\mathbf{w}_{M-1}\\|_2^2 + R^2 \\le \\|\\mathbf{w}_{M-2}\\|_2^2 + 2R^2 \\le \\dots \\le \\|\\mathbf{w}_0\\|_2^2 + M R^2 = M R^2\n$$\n因此，我们得到了第一个结果：\n$$\n\\|\\mathbf{w}_M\\|_2^2 \\le M R^2\n$$\n\n**2. 与最优向量点积的下界**\n\n接下来，我们分析权重向量 $\\mathbf{w}_k$ 与最优分离向量 $\\mathbf{w}^*$ 的点积：\n$$\n\\mathbf{w}_k \\cdot \\mathbf{w}^* = (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) \\cdot \\mathbf{w}^* = \\mathbf{w}_{k-1} \\cdot \\mathbf{w}^* + y_k(\\mathbf{x}_k \\cdot \\mathbf{w}^*)\n$$\n根据间隔 $\\gamma$ 的定义，我们有 $y_k(\\mathbf{x}_k \\cdot \\mathbf{w}^*) \\ge \\gamma$。代入此式可为点积的增长提供一个下界：\n$$\n\\mathbf{w}_k \\cdot \\mathbf{w}^* \\ge \\mathbf{w}_{k-1} \\cdot \\mathbf{w}^* + \\gamma\n$$\n从 $\\mathbf{w}_0 \\cdot \\mathbf{w}^* = 0$ 开始展开这个递推关系：\n$$\n\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge \\mathbf{w}_{M-1} \\cdot \\mathbf{w}^* + \\gamma \\ge \\dots \\ge \\mathbf{w}_0 \\cdot \\mathbf{w}^* + M\\gamma = M\\gamma\n$$\n这给出了我们的第二个结果：\n$$\n\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma\n$$\n\n**3. 组合界限**\n\n我们有两个关于 $\\mathbf{w}_M$ 的不等式：\n1. $\\|\\mathbf{w}_M\\|_2^2 \\le M R^2$\n2. $\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma  0$\n\n根据柯西-施瓦茨不等式，我们有 $(\\mathbf{u} \\cdot \\mathbf{v})^2 \\le \\|\\mathbf{u}\\|_2^2 \\|\\mathbf{v}\\|_2^2$。将其应用于 $\\mathbf{w}_M$ 和 $\\mathbf{w}^*$：\n$$\n(\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2 \\|\\mathbf{w}^*\\|_2^2\n$$\n因为 $\\|\\mathbf{w}^*\\|_2 = 1$，这可以简化为 $(\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2$。\n\n现在，我们代入步骤1和2中推导出的界限。由于不等式 $\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma$ 的两边都是正数，我们可以对其进行平方而不改变不等式的方向：\n$$\n(M\\gamma)^2 \\le (\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2\n$$\n综合所有部分：\n$$\n(M\\gamma)^2 \\le (\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2 \\le M R^2\n$$\n这给出了关联 $M$、$R$ 和 $\\gamma$ 的最终不等式：\n$$\nM^2\\gamma^2 \\le M R^2\n$$\n由于算法从 $\\mathbf{w}_0 = \\mathbf{0}$ 开始收敛必须至少犯一次错误（假设 $\\mathbf{0}$ 不是一个有效的分类器），因此 $M \\ge 1$。我们可以安全地除以 $M$：\n$$\nM\\gamma^2 \\le R^2\n$$\n对 $M$ 进行整理，我们便得到了理论错误界限：\n$$\nM \\le \\left(\\frac{R}{\\gamma}\\right)^2\n$$\n推导到此完成。错误次数的界限是数据集半径与其间隔之比的平方。\n\n### 第二部分：实现与验证\n\n以下程序实现了所需的过程：\n1.  **`run_perceptron`**：模拟在给定数据集上的感知机算法，并计算收敛前的总错误次数。\n2.  **`estimate_R`**：计算半径 $R$，即数据点L2范数的最大值。\n3.  **`estimate_gamma`**：通过对由角度 $\\theta \\in [0, 2\\pi)$ 参数化的单位向量进行确定性的密集搜索，近似计算间隔 $\\gamma$。\n4.  **`solve`**：主函数，用于协调分析每个测试用例，计算理论界限 $B = (R/\\gamma)^2$，将其与观察到的错误数 $M$ 进行比较，并按规定格式化输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef run_perceptron(X, y):\n    \"\"\"\n    Simulates the single-layer perceptron algorithm with zero bias.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n        y (np.ndarray): The labels, shape (n,).\n\n    Returns:\n        int: The total number of mistakes made until convergence.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d)\n    total_mistakes = 0\n    \n    while True:\n        mistakes_in_pass = 0\n        for i in range(n):\n            x_i = X[i]\n            y_i = y[i]\n            \n            # Mistake condition\n            if y_i * (w @ x_i) = 0:\n                total_mistakes += 1\n                mistakes_in_pass += 1\n                # Update rule\n                w += y_i * x_i\n        \n        if mistakes_in_pass == 0:\n            # Convergence: an entire pass with no mistakes\n            break\n            \n    return total_mistakes\n\ndef estimate_R(X):\n    \"\"\"\n    Estimates the radius R as the maximum L2-norm of the data points.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n\n    Returns:\n        float: The radius R.\n    \"\"\"\n    return np.max(np.linalg.norm(X, axis=1))\n\ndef estimate_gamma(X, y):\n    \"\"\"\n    Estimates the margin gamma by searching over unit vectors.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n        y (np.ndarray): The labels, shape (n,).\n\n    Returns:\n        float: The estimated margin gamma.\n    \"\"\"\n    # Using a dense, deterministic grid for the search as required.\n    # 200,000 angles provide a robust approximation for these datasets.\n    num_angles = 200000 \n    angles = np.linspace(0, 2 * np.pi, num_angles, endpoint=False)\n    \n    max_min_proj = -np.inf\n\n    # Pre-compute y_i * x_i vectors to simplify projection calculation\n    Z = X * y[:, np.newaxis]\n    \n    for theta in angles:\n        w = np.array([np.cos(theta), np.sin(theta)])\n        \n        # Calculate signed projections: y_i * (w . x_i) is equivalent to w . (y_i * x_i)\n        projections = Z @ w\n        \n        # Find the minimum projection for this candidate w\n        min_proj = np.min(projections)\n        \n        # The margin gamma is the maximum of these minimums over all w\n        if min_proj > max_min_proj:\n            max_min_proj = min_proj\n            \n    return max_min_proj\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and produce the final output.\n    \"\"\"\n    test_cases_data = [\n        {\n            \"name\": \"Test case 1 (moderate radius and margin)\",\n            \"points\": [\n                (2, 1), (1.5, 1), (2, 2),  # y = +1\n                (-2, -1), (-1.5, -1), (-2, -2) # y = -1\n            ],\n            \"labels\": [1, 1, 1, -1, -1, -1]\n        },\n        {\n            \"name\": \"Test case 2 (large radius, small margin)\",\n            \"points\": [\n                (0.05, 100), (2, 100), (1, 110), # y = +1\n                (-0.05, -100), (-2, -100), (-1, -110) # y = -1\n            ],\n            \"labels\": [1, 1, 1, -1, -1, -1]\n        },\n        {\n            \"name\": \"Test case 3 (small radius, larger margin, nearly one-dimensional)\",\n            \"points\": [\n                (1, 0), (0.8, 0), # y = +1\n                (-1, 0), (-0.8, 0) # y = -1\n            ],\n            \"labels\": [1, 1, -1, -1]\n        }\n    ]\n\n    results = []\n    for case in test_cases_data:\n        X = np.array(case[\"points\"], dtype=float)\n        y = np.array(case[\"labels\"], dtype=float)\n\n        # 1. Estimate R and gamma\n        R = estimate_R(X)\n        gamma = estimate_gamma(X, y)\n\n        # 2. Simulate the perceptron to get observed mistakes\n        M = run_perceptron(X, y)\n        \n        # 3. Compute the theoretical mistake bound B\n        if gamma > 0:\n            B = (R / gamma)**2\n        else:\n            # This case should not happen for separable data, but as a safeguard.\n            B = float('inf')\n\n        # 4. Compare observed mistakes M to theoretical bound B\n        C = (M = B)\n\n        results.append(f\"[{B},{M},{str(C).lower()}]\")\n\n    # Final print statement in the exact required format.\n    # The problem asks for a boolean value. In Python, printing a bool gives `True`/`False`.\n    # Let's ensure lowercase as per standard JSON/list representation.\n    final_output_str = f\"[{','.join(results)}]\"\n    # The desired output is like a Python list of lists, which implies bools are `True`/`False`.\n    # Python `str()` on a boolean is capitalized. Let's make it lowercase for consistency with JSON.\n    print(final_output_str.replace(\"True\", \"true\").replace(\"False\", \"false\"))\n\nsolve()\n```"
        }
    ]
}