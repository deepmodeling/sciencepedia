{
    "hands_on_practices": [
        {
            "introduction": "掌握单层感知器的第一步是理解其基本工作原理：一旦权重 $w$ 和偏置 $b$ 给定，它如何对输入数据进行分类。本练习将带你完成这一核心计算过程，通过计算一系列输入点的激活值并判定其类别，从而巩固你对线性决策边界 $w^{\\top}x + b = 0$ 的理解。通过这个实践，你将清晰地看到数据点是如何根据其在决策边界的哪一侧而被赋予不同标签的。",
            "id": "4065769",
            "problem": "考虑一个实现为线性阈值单元的单层感知机，这是神经形态和受脑启发计算中的一个典型模型。它将输入向量 $x \\in \\mathbb{R}^2$ 映射到一个标量预激活值 $a(x) = w^{\\top} x + b$，然后通过一个符号阈值输出 $y(x) = \\operatorname{sgn}(a(x))$，其中如果 $z > 0$，则 $\\operatorname{sgn}(z) = 1$；如果 $z  0$，则 $\\operatorname{sgn}(z) = -1$；如果 $z = 0$，则 $\\operatorname{sgn}(z) = 0$，用以明确编码恰好位于决策边界上的点。此定义与线性阈值分类及其决策边界 $w^{\\top} x + b = 0$ 的成熟表述相一致。\n\n设感知机参数为 $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ 和 $b = 5$。考虑输入集 $X = \\{x_1, x_2, x_3, x_4\\}$，其中\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad\nx_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad\nx_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}, \\quad\nx_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}.\n$$\n\n仅使用上述基本定义，计算 $i \\in \\{1,2,3,4\\}$ 的分类输出 $y(x_i)$，并确定哪个 $x_i$ 恰好位于决策边界上。然后计算标量\n$$\nS = \\sum_{i=1}^{4} y(x_i).\n$$\n\n仅报告 $S$ 的值作为最终答案。无需四舍五入。",
            "solution": "在尝试任何解答之前，需对问题陈述进行验证。\n\n**步骤 1：提取已知条件**\n- 模型：单层感知机（线性阈值单元）。\n- 输入向量：$x \\in \\mathbb{R}^2$。\n- 预激活函数：$a(x) = w^{\\top} x + b$。\n- 输出函数：$y(x) = \\operatorname{sgn}(a(x))$。\n- 符号函数定义：如果 $z > 0$，则 $\\operatorname{sgn}(z) = 1$；如果 $z  0$，则 $\\operatorname{sgn}(z) = -1$；如果 $z = 0$，则 $\\operatorname{sgn}(z) = 0$。\n- 决策边界条件：$w^{\\top} x + b = 0$。\n- 感知机权重向量：$w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$。\n- 感知机偏置：$b = 5$。\n- 输入集：$X = \\{x_1, x_2, x_3, x_4\\}$。\n- 输入向量：\n$x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，$x_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$，$x_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$，$x_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}$。\n- 目标量：$S = \\sum_{i=1}^{4} y(x_i)$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- 该问题具有**科学依据**且**客观**。单层感知机是机器学习和计算神经科学中的一个基本概念。所提供的预激活、输出函数和决策边界的定义是标准的，并且在数学上是精确的。\n- 该问题是**适定的**。它提供了所有必需的组成部分——模型参数（$w, b$）和输入数据（$x_i$）——以计算所需的输出（$y(x_i)$）和最终总和（$S$）。计算会得出一个唯一的、确定性的答案。\n- 问题的设置是**完整且一致的**。没有缺失信息，也没有任何矛盾的约束条件。$\\operatorname{sgn}(0)=0$ 的明确定义至关重要，并与识别决策边界上点的任务保持一致使用。\n\n**步骤 3：结论与行动**\n- 该问题被认定为**有效**。将推导解答。\n\n任务是计算总和 $S = \\sum_{i=1}^{4} y(x_i)$。这需要为每个输入向量 $x_i$ 计算分类输出 $y(x_i)$，而这又取决于预激活值 $a(x_i)$。预激活的通用公式是 $a(x) = w^{\\top} x + b$。\n\n给定的参数是 $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ 和 $b = 5$。\n\n我们对每个 $i \\in \\{1, 2, 3, 4\\}$ 计算预激活值 $a(x_i)$ 和相应的输出 $y(x_i) = \\operatorname{sgn}(a(x_i))$。\n\n对于 $i=1$：\n输入向量是 $x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n预激活值为：\n$$\na(x_1) = w^{\\top} x_1 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + 5 = (3)(1) + (-4)(2) + 5 = 3 - 8 + 5 = 0\n$$\n因为 $a(x_1) = 0$，所以输出为 $y(x_1) = \\operatorname{sgn}(0) = 0$。\n\n对于 $i=2$：\n输入向量是 $x_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$。\n预激活值为：\n$$\na(x_2) = w^{\\top} x_2 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + 5 = (3)(2) + (-4)(0) + 5 = 6 + 0 + 5 = 11\n$$\n因为 $a(x_2) = 11 > 0$，所以输出为 $y(x_2) = \\operatorname{sgn}(11) = 1$。\n\n对于 $i=3$：\n输入向量是 $x_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$。\n预激活值为：\n$$\na(x_3) = w^{\\top} x_3 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} + 5 = (3)(0) + (-4)(3) + 5 = 0 - 12 + 5 = -7\n$$\n因为 $a(x_3) = -7  0$，所以输出为 $y(x_3) = \\operatorname{sgn}(-7) = -1$。\n\n对于 $i=4$：\n输入向量是 $x_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}$。\n预激活值为：\n$$\na(x_4) = w^{\\top} x_4 + b = \\begin{pmatrix} 3  -4 \\end{pmatrix} \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix} + 5 = (3)\\left(\\frac{10}{3}\\right) + (-4)\\left(\\frac{5}{2}\\right) + 5 = 10 - 10 + 5 = 5\n$$\n因为 $a(x_4) = 5 > 0$，所以输出为 $y(x_4) = \\operatorname{sgn}(5) = 1$。\n\n如果一个输入点 $x_i$ 的预激活值为零，即 $a(x_i) = w^{\\top} x_i + b = 0$，那么它就恰好位于决策边界上。根据我们的计算，只有 $a(x_1) = 0$。因此，$x_1$ 是唯一一个恰好位于决策边界上的输入点。\n\n最后，我们计算标量和 $S$：\n$$\nS = \\sum_{i=1}^{4} y(x_i) = y(x_1) + y(x_2) + y(x_3) + y(x_4)\n$$\n代入计算出的值：\n$$\nS = 0 + 1 + (-1) + 1 = 1\n$$\n总和的值是 $1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "理解了静态的分类过程后，我们来探究感知器动态的学习机制。这个练习将模拟经典的感知器学习算法，让你亲手追踪权重和偏置在处理一连串数据点时如何逐步更新。通过这种分步模拟，你将直观地感受到决策边界是如何响应错分样本而进行调整的，从而深刻理解“错误驱动”这一学习规则的本质。",
            "id": "4065810",
            "problem": "考虑一个单层感知机，它在神经形态事件驱动流中作为一个硬阈值分类器运行。该分类器具有权重向量 $w \\in \\mathbb{R}^{2}$ 和偏置 $b \\in \\mathbb{R}$，并预测输入 $x \\in \\mathbb{R}^{2}$ 的标签为 $\\operatorname{sign}(w \\cdot x + b)$，其中标签 $y \\in \\{ -1, +1 \\}$。学习是错误驱动的，学习率 $\\eta = 1$：在接收到第 $t$ 个样本 $(x^{(t)}, y^{(t)})$ 时，计算激活值 $a^{(t)} = w^{(t-1)} \\cdot x^{(t)} + b^{(t-1)}$。如果 $y^{(t)} a^{(t)} \\le 0$，则根据以下公式进行更新\n$$\nw^{(t)} = w^{(t-1)} + y^{(t)} x^{(t)}, \n\\quad\nb^{(t)} = b^{(t-1)} + y^{(t)},\n$$\n否则参数保持不变，即 $w^{(t)} = w^{(t-1)}$ 且 $b^{(t)} = b^{(t-1)}$。初始状态为 $w^{(0)} = (0, 0)$ 和 $b^{(0)} = 0$。\n\n$\\mathbb{R}^{2}$ 中一个固定的、有限的带标签输入集由以下给出：\n- $x_{1} = (1, 0)$，标签为 $y_{1} = +1$，\n- $x_{2} = (-0.1, 0)$，标签为 $y_{2} = -1$，\n- $x_{3} = (0, 1)$，标签为 $y_{3} = +1$。\n\n这些输入以一个无限有序流的形式呈现，该流无限重复循环 $(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3})$。呈现时间 $t \\in \\mathbb{N}$ 记录已处理的样本总数，从 $t = 1$ 开始。\n\n如果在处理完时间 $t$ 的样本后，当前参数 $(w^{(t)}, b^{(t)})$ 能够正确分类所有三个点 $\\{x_{1}, x_{2}, x_{3}\\}$（即，对于每个 $i \\in \\{1,2,3\\}$，都有 $y_{i}\\,(w^{(t)} \\cdot x_{i} + b^{(t)})  0$），则我们定义分类器在时间 $t$ 与数据一致。\n\n请明确计算连续呈现时间 $t$ 的参数状态序列 $(w^{(t)}, b^{(t)})$，直到分类器与数据变得一致（如上定义）的最早呈现时间 $T$。报告最小的此类呈现索引 $T$。您的最终答案必须是一个没有单位的整数。不需要四舍五入。",
            "solution": "用户希望我根据给定的问题陈述解决一个问题。\n\n问题要求找出单层感知机分类器与给定数据集变得一致的最早呈现时间 $T$。学习过程是错误驱动的，并通过迭代一个包含三个数据点的固定循环来进行。\n\n首先，我将验证问题陈述。\n\n### 步骤 1：提取给定信息\n- **模型**：单层感知机。\n- **输入空间**：$x \\in \\mathbb{R}^{2}$。\n- **标签空间**：$y \\in \\{ -1, +1 \\}$。\n- **分类器函数**：$\\operatorname{sign}(w \\cdot x + b)$，其中 $w \\in \\mathbb{R}^{2}$ 且 $b \\in \\mathbb{R}$。\n- **学习规则**：错误驱动（感知机算法），学习率 $\\eta = 1$。\n- **更新条件**：如果在步骤 $t$ 满足 $y^{(t)} a^{(t)} \\le 0$，则进行更新，其中 $a^{(t)} = w^{(t-1)} \\cdot x^{(t)} + b^{(t-1)}$。\n- **更新方程**：\n    - $w^{(t)} = w^{(t-1)} + y^{(t)} x^{(t)}$\n    - $b^{(t)} = b^{(t-1)} + y^{(t)}$\n- **不更新条件**：如果 $y^{(t)} a^{(t)} > 0$，则 $w^{(t)} = w^{(t-1)}$ 且 $b^{(t)} = b^{(t-1)}$。\n- **初始状态**：$w^{(0)} = (0, 0)$ 和 $b^{(0)} = 0$。\n- **数据集**：\n    - $x_{1} = (1, 0)$，标签为 $y_{1} = +1$。\n    - $x_{2} = (-0.1, 0)$，标签为 $y_{2} = -1$。\n    - $x_{3} = (0, 1)$，标签为 $y_{3} = +1$。\n- **输入流**：一个无限序列，重复循环 $(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3})$。\n- **呈现时间**：$t \\in \\mathbb{N}$ 从 $t=1$ 开始。\n- **一致性定义**：分类器在时间 $t$ 是一致的，如果参数 $(w^{(t)}, b^{(t)})$ 能正确分类所有三个点 $\\{x_{1}, x_{2}, x_{3}\\}$。正确分类意味着对于 $i \\in \\{1, 2, 3\\}$，都有 $y_{i}\\,(w^{(t)} \\cdot x_{i} + b^{(t)}) > 0$。\n- **目标**：找到分类器变得一致的最早呈现时间 $T$。\n\n### 步骤 2：使用提取的给定信息进行验证\n该问题具有科学依据，定义明确且客观。它提供了感知机学习算法的标准公式，这是机器学习的基石。所有的初始条件、数据、更新规则和停止准则都得到了精确定义。该数据集是线性可分的，这保证了算法将在有限步数内收敛到一个解。问题要求在一个特定的数据呈现顺序下，计算一个可验证的量（收敛所需的步数）。问题没有矛盾、歧义或事实错误。\n\n### 步骤 3：结论和行动\n问题有效。我现在开始解答。\n\n为了找到最早的呈现时间 $T$，我们必须从初始状态开始，一步一步地模拟学习过程，在每次出错时更新参数。在每一步 $t$ 之后，我们检查得到的分类器 $(w^{(t)}, b^{(t)})$ 是否与整个数据集一致。\n\n在 $t=0$ 时参数的初始状态是 $w^{(0)} = (0, 0)$ 和 $b^{(0)} = 0$。\n\n**步骤 $t=1$**：\n呈现的第一个样本是 $(x^{(1)}, y^{(1)}) = (x_1, y_1) = ((1, 0), +1)$。\n此步骤之前的参数是 $(w^{(0)}, b^{(0)}) = ((0, 0), 0)$。\n激活值为 $a^{(1)} = w^{(0)} \\cdot x^{(1)} + b^{(0)} = (0, 0) \\cdot (1, 0) + 0 = 0$。\n我们检查错误条件：$y^{(1)}a^{(1)} = (+1)(0) = 0$。由于 $0 \\le 0$，发生了一个错误。\n我们更新参数：\n$w^{(1)} = w^{(0)} + y^{(1)}x^{(1)} = (0, 0) + (+1)(1, 0) = (1, 0)$。\n$b^{(1)} = b^{(0)} + y^{(1)} = 0 + 1 = 1$。\n步骤 1 之后的状态是 $(w^{(1)}, b^{(1)}) = ((1, 0), 1)$。\n我们检查一致性：\n1. 对于 $(x_1, y_1)$：$y_1(w^{(1)} \\cdot x_1 + b^{(1)}) = +1((1, 0) \\cdot (1, 0) + 1) = 2 > 0$。（正确）\n2. 对于 $(x_2, y_2)$：$y_2(w^{(1)} \\cdot x_2 + b^{(1)}) = -1((1, 0) \\cdot (-0.1, 0) + 1) = -1(-0.1 + 1) = -0.9 \\le 0$。（不正确）\n分类器不一致。\n\n**步骤 $t=2$**：\n第二个样本是 $(x^{(2)}, y^{(2)}) = (x_2, y_2) = ((-0.1, 0), -1)$。\n参数是 $(w^{(1)}, b^{(1)}) = ((1, 0), 1)$。\n激活值为 $a^{(2)} = w^{(1)} \\cdot x^{(2)} + b^{(1)} = (1, 0) \\cdot (-0.1, 0) + 1 = -0.1 + 1 = 0.9$。\n检查错误：$y^{(2)}a^{(2)} = (-1)(0.9) = -0.9 \\le 0$。发生了一个错误。\n更新：\n$w^{(2)} = w^{(1)} + y^{(2)}x^{(2)} = (1, 0) + (-1)(-0.1, 0) = (1, 0) + (0.1, 0) = (1.1, 0)$。\n$b^{(2)} = b^{(1)} + y^{(2)} = 1 + (-1) = 0$。\n步骤 2 之后的状态：$(w^{(2)}, b^{(2)}) = ((1.1, 0), 0)$。\n检查一致性：\n1. 对于 $(x_1, y_1)$：$y_1(w^{(2)} \\cdot x_1 + b^{(2)}) = +1((1.1, 0) \\cdot (1, 0) + 0) = 1.1 > 0$。（正确）\n2. 对于 $(x_2, y_2)$：$y_2(w^{(2)} \\cdot x_2 + b^{(2)}) = -1((1.1, 0) \\cdot (-0.1, 0) + 0) = -1(-0.11) = 0.11 > 0$。（正确）\n3. 对于 $(x_3, y_3)$：$y_3(w^{(2)} \\cdot x_3 + b^{(2)}) = +1((1.1, 0) \\cdot (0, 1) + 0) = 0 \\le 0$。（不正确）\n分类器不一致。\n\n**步骤 $t=3$**：\n第三个样本是 $(x^{(3)}, y^{(3)}) = (x_3, y_3) = ((0, 1), +1)$。\n参数是 $(w^{(2)}, b^{(2)}) = ((1.1, 0), 0)$。\n激活值为 $a^{(3)} = w^{(2)} \\cdot x^{(3)} + b^{(2)} = (1.1, 0) \\cdot (0, 1) + 0 = 0$。\n检查错误：$y^{(3)}a^{(3)} = (+1)(0) = 0 \\le 0$。发生了一个错误。\n更新：\n$w^{(3)} = w^{(2)} + y^{(3)}x^{(3)} = (1.1, 0) + (+1)(0, 1) = (1.1, 1)$。\n$b^{(3)} = b^{(2)} + y^{(3)} = 0 + 1 = 1$。\n步骤 3 之后的状态：$(w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$。\n检查一致性：\n1. 对于 $(x_1, y_1)$：$y_1(w^{(3)} \\cdot x_1 + b^{(3)}) = +1((1.1, 1) \\cdot (1, 0) + 1) = 2.1 > 0$。（正确）\n2. 对于 $(x_2, y_2)$：$y_2(w^{(3)} \\cdot x_2 + b^{(3)}) = -1((1.1, 1) \\cdot (-0.1, 0) + 1) = -1(-0.11 + 1) = -0.89 \\le 0$。（不正确）\n分类器不一致。\n\n**步骤 $t=4$**：\n数据流重复；第四个样本是 $(x^{(4)}, y^{(4)}) = (x_1, y_1) = ((1, 0), +1)$。\n参数是 $(w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$。\n激活值为 $a^{(4)} = w^{(3)} \\cdot x^{(4)} + b^{(3)} = (1.1, 1) \\cdot (1, 0) + 1 = 1.1 + 1 = 2.1$。\n检查错误：$y^{(4)}a^{(4)} = (+1)(2.1) = 2.1 > 0$。没有错误。\n参数不更新：$(w^{(4)}, b^{(4)}) = (w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$。\n由于参数自步骤 $t=3$ 以来没有改变，分类器仍然不一致。\n\n**步骤 $t=5$**：\n第五个样本是 $(x^{(5)}, y^{(5)}) = (x_2, y_2) = ((-0.1, 0), -1)$。\n参数是 $(w^{(4)}, b^{(4)}) = ((1.1, 1), 1)$。\n激活值为 $a^{(5)} = w^{(4)} \\cdot x^{(5)} + b^{(4)} = (1.1, 1) \\cdot (-0.1, 0) + 1 = -0.11 + 1 = 0.89$。\n检查错误：$y^{(5)}a^{(5)} = (-1)(0.89) = -0.89 \\le 0$。发生了一个错误。\n更新：\n$w^{(5)} = w^{(4)} + y^{(5)}x^{(5)} = (1.1, 1) + (-1)(-0.1, 0) = (1.1, 1) + (0.1, 0) = (1.2, 1)$。\n$b^{(5)} = b^{(4)} + y^{(5)} = 1 + (-1) = 0$。\n步骤 5 之后的状态：$(w^{(5)}, b^{(5)}) = ((1.2, 1), 0)$。\n检查一致性：\n1. 对于 $(x_1, y_1)$：$y_1(w^{(5)} \\cdot x_1 + b^{(5)}) = +1((1.2, 1) \\cdot (1, 0) + 0) = 1.2 > 0$。（正确）\n2. 对于 $(x_2, y_2)$：$y_2(w^{(5)} \\cdot x_2 + b^{(5)}) = -1((1.2, 1) \\cdot (-0.1, 0) + 0) = -1(-0.12) = 0.12 > 0$。（正确）\n3. 对于 $(x_3, y_3)$：$y_3(w^{(5)} \\cdot x_3 + b^{(5)}) = +1((1.2, 1) \\cdot (0, 1) + 0) = 1 > 0$。（正确）\n在处理完 $t=5$ 的样本后，分类器正确分类了数据集中的所有三个点。因此，分类器变得一致了。\n\n分类器变得一致的最早呈现时间 $T$ 是 $5$。",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "理论与实践的结合是科学探究的关键。感知器收敛定理保证，对于任何线性可分的数据集，该算法都能在有限步内找到一个有效的分类超平面。本练习旨在通过编程实践来验证这一定理，你需要估算数据集的关键几何属性——半径 $R$ 和间隔 $\\gamma$，并将其与实际训练过程中观察到的错误次数进行比较。这个综合性练习不仅能让你掌握感知器算法的实现，还能让你亲眼见证理论预测在实践中的力量。",
            "id": "4065892",
            "problem": "给定一个在 $\\mathbb{R}^d$ 中的有限标记点数据集 $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{-1,+1\\}$，该数据集可被一个穿过原点的超平面线性分离。考虑一个无偏置（阈值为原点）、单位学习率并使用标准更新的单层感知机。分类器对输入 $\\mathbf{x}$ 的预测是内积的符号，并且仅在出错时执行更新。形式上，从 $\\mathbf{w}_0=\\mathbf{0}$ 开始，对于序列中的每个训练样本 $(\\mathbf{x}_i,y_i)$，预测为 $\\operatorname{sign}(\\mathbf{w}\\cdot \\mathbf{x}_i)$，当发生错误（定义为 $y_i(\\mathbf{w}\\cdot \\mathbf{x}_i)\\le 0$）时，执行更新 $\\mathbf{w}\\leftarrow \\mathbf{w}+y_i\\mathbf{x}_i$。重复遍历数据集，直到完整一轮没有产生任何错误。\n\n半径界 $R$ 定义为 $R=\\max_i\\|\\mathbf{x}_i\\|_2$。数据集关于最佳的穿过原点的分离单位向量的间隔 $\\gamma$ 定义为\n$$\n\\gamma \\;=\\; \\max_{\\|\\mathbf{w}\\|_2=1}\\;\\min_{i}\\; y_i\\,(\\mathbf{w}\\cdot \\mathbf{x}_i).\n$$\n\n仅使用这些核心定义和线性代数中的基本不等式，推导单层感知机收敛定理预测的理论错误界（用 $R$ 和 $\\gamma$ 表示），并实现一个程序，该程序：\n- 精确地将 $R$ 估计为 $R=\\max_i\\|\\mathbf{x}_i\\|_2$。\n- 通过在 $\\mathbb{R}^2$ 中搜索由角度 $\\theta$ 参数化的单位方向来估计 $\\gamma$，并取所有点中最小带符号投影的最大值。使用 $[0,2\\pi)$ 上角度的密集均匀离散化来近似最大化过程。你的搜索必须是确定性的。\n- 从 $\\mathbf{w}_0=\\mathbf{0}$ 开始模拟感知机训练序列，按照给定的固定顺序循环遍历数据集，统计直到完整一轮产生零错误为止的总错误数。\n- 使用推导出的表达式计算理论错误界，并将其与模拟中观察到的错误数进行比较。\n\n你必须在以下包含三个数据集的测试套件上应用此过程（所有点都按其在训练中必须出现的顺序列出）：\n\n测试用例 1（中等半径和间隔）：\n- 正例 ($y=+1$)：$(2,1)$、$(1.5,1)$、$(2,2)$。\n- 负例 ($y=-1$)：$(-2,-1)$、$(-1.5,-1)$、$(-2,-2)$。\n\n测试用例 2（大半径，小间隔）：\n- 正例 ($y=+1$)：$(0.05,100)$、$(2,100)$、$(1,110)$。\n- 负例 ($y=-1$)：$(-0.05,-100)$、$(-2,-100)$、$(-1,-110)$。\n\n测试用例 3（小半径，较大间隔，近一维）：\n- 正例 ($y=+1$)：$(1,0)$、$(0.8,0)$。\n- 负例 ($y=-1$)：$(-1,0)$、$(-0.8,0)$。\n\n所有三个数据集都可被一个穿过原点的超平面分离。对于每个测试用例：\n- 按规定估计 $R$ 和 $\\gamma$。\n- 根据这些量推导并计算理论错误界。\n- 模拟感知机以获得观察到的错误计数。\n- 生成一个比较，指示观察到的错误数是否小于或等于理论界。\n\n最终输出格式：你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个测试用例的结果本身是一个形如 $[B,M,C]$ 的列表，其中 $B$ 是作为浮点数的理论界， $M$ 是作为整数的观察到的错误数， $C$ 是一个表示 $M\\le B$ 是否成立的布尔值。例如，输出应类似于 $[[B_1,M_1,C_1],[B_2,M_2,C_2],[B_3,M_3,C_3]]$，其中的数值由你的计算填充。\n\n角度单位：在估计 $\\gamma$ 的任何内部角度参数化中，使用弧度。\n\n不涉及物理单位。将所有数值输出表示为原始数字（无百分号）。程序必须是自包含的，并且不需要用户输入。",
            "solution": "用户提供的问题经评估是有效的。它在科学上基于单层感知机的成熟理论，问题设定良好且客观。所有为得出唯一且有意义的解所需的数据和定义都已提供。该任务涉及推导一个经典的理论结果（感知机错误界），并在特定数据集上进行计算验证，这是机器学习中的一个标准而严谨的练习。\n\n### 第 1 部分：感知机错误界的推导\n\n该问题要求推导单层感知机算法所犯错误次数的理论上界。设总错误数为 $M$。设发生错误时的权重向量序列为 $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_M$。初始权重向量为 $\\mathbf{w}_0 = \\mathbf{0}$。\n\n设第 $k$ 次错误（对于 $k=1, \\dots, M$）发生在训练样本 $(\\mathbf{x}_k, y_k)$ 上。请注意，为简化符号，我们对数据集中发生错误的点进行了重新索引。应用感知机更新规则：\n$$\n\\mathbf{w}_k = \\mathbf{w}_{k-1} + y_k \\mathbf{x}_k\n$$\n第 $k$ 次错误的条件是：\n$$\ny_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) \\le 0\n$$\n假设数据集可被一个穿过原点的超平面线性分离。这保证了存在一个单位范数 $\\|\\mathbf{w}^*\\|_2=1$ 的权重向量 $\\mathbf{w}^*$ 和一个间隔 $\\gamma  0$，使得对于数据集中的所有点 $(\\mathbf{x}_i, y_i)$：\n$$\ny_i(\\mathbf{w}^* \\cdot \\mathbf{x}_i) \\ge \\gamma\n$$\n其中 $\\gamma$ 是在所有可能的单位范数分离向量中最大的此类间隔。\n\n推导过程通过对经过 $M$ 次错误后的最终权重向量 $\\mathbf{w}_M$ 建立两个不等式来进行。\n\n**1. 权重向量范数的上界**\n\n我们分析第 $k$ 次更新后权重向量的 L2 范数的平方：\n$$\n\\|\\mathbf{w}_k\\|_2^2 = \\|\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k\\|_2^2\n$$\n展开点积：\n$$\n\\|\\mathbf{w}_k\\|_2^2 = (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) \\cdot (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) = \\|\\mathbf{w}_{k-1}\\|_2^2 + 2y_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) + y_k^2\\|\\mathbf{x}_k\\|_2^2\n$$\n由于 $y_k \\in \\{-1, +1\\}$，我们有 $y_k^2=1$。根据错误条件，我们知道 $y_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) \\le 0$。将这些代入方程得到：\n$$\n\\|\\mathbf{w}_k\\|_2^2 \\le \\|\\mathbf{w}_{k-1}\\|_2^2 + \\|\\mathbf{x}_k\\|_2^2\n$$\n问题定义了半径 $R = \\max_i \\|\\mathbf{x}_i\\|_2$。因此，对于任何点 $\\mathbf{x}_k$，都有 $\\|\\mathbf{x}_k\\|_2^2 \\le R^2$。这导出了递推关系：\n$$\n\\|\\mathbf{w}_k\\|_2^2 \\le \\|\\mathbf{w}_{k-1}\\|_2^2 + R^2\n$$\n从 $\\mathbf{w}_0 = \\mathbf{0}$ 开始递归应用此不等式，我们得到 $M$ 次错误后权重向量范数平方的上界：\n$$\n\\|\\mathbf{w}_M\\|_2^2 \\le \\|\\mathbf{w}_{M-1}\\|_2^2 + R^2 \\le \\|\\mathbf{w}_{M-2}\\|_2^2 + 2R^2 \\le \\dots \\le \\|\\mathbf{w}_0\\|_2^2 + M R^2 = M R^2\n$$\n所以，我们得到了第一个结果：\n$$\n\\|\\mathbf{w}_M\\|_2^2 \\le M R^2\n$$\n\n**2. 与最优向量点积的下界**\n\n接下来，我们分析权重向量 $\\mathbf{w}_k$ 与最优分离向量 $\\mathbf{w}^*$ 的点积：\n$$\n\\mathbf{w}_k \\cdot \\mathbf{w}^* = (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) \\cdot \\mathbf{w}^* = \\mathbf{w}_{k-1} \\cdot \\mathbf{w}^* + y_k(\\mathbf{x}_k \\cdot \\mathbf{w}^*)\n$$\n根据间隔 $\\gamma$ 的定义，我们有 $y_k(\\mathbf{x}_k \\cdot \\mathbf{w}^*) \\ge \\gamma$。代入此式可为点积的增长提供一个下界：\n$$\n\\mathbf{w}_k \\cdot \\mathbf{w}^* \\ge \\mathbf{w}_{k-1} \\cdot \\mathbf{w}^* + \\gamma\n$$\n从 $\\mathbf{w}_0 \\cdot \\mathbf{w}^* = 0$ 开始展开此递推关系：\n$$\n\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge \\mathbf{w}_{M-1} \\cdot \\mathbf{w}^* + \\gamma \\ge \\dots \\ge \\mathbf{w}_0 \\cdot \\mathbf{w}^* + M\\gamma = M\\gamma\n$$\n这给出了我们的第二个结果：\n$$\n\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma\n$$\n\n**3. 结合边界**\n\n我们有两个关于 $\\mathbf{w}_M$ 的不等式：\n1. $\\|\\mathbf{w}_M\\|_2^2 \\le M R^2$\n2. $\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma > 0$\n\n根据柯西-施瓦茨不等式，我们有 $(\\mathbf{u} \\cdot \\mathbf{v})^2 \\le \\|\\mathbf{u}\\|_2^2 \\|\\mathbf{v}\\|_2^2$。将其应用于 $\\mathbf{w}_M$ 和 $\\mathbf{w}^*$：\n$$\n(\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2 \\|\\mathbf{w}^*\\|_2^2\n$$\n由于 $\\|\\mathbf{w}^*\\|_2 = 1$，这简化为 $(\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2$。\n\n现在，我们代入第 1 步和第 2 步中推导出的界限。由于不等式 $\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma$ 的两边都是正的，我们可以对其进行平方而不改变不等号的方向：\n$$\n(M\\gamma)^2 \\le (\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2\n$$\n结合所有部分：\n$$\n(M\\gamma)^2 \\le (\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2 \\le M R^2\n$$\n这给出了联系 $M$、$R$ 和 $\\gamma$ 的最终不等式：\n$$\nM^2\\gamma^2 \\le M R^2\n$$\n由于算法从 $\\mathbf{w}_0 = \\mathbf{0}$ 收敛至少需要犯一次错误（假设 $\\mathbf{0}$ 不是一个有效的分离器），$M \\ge 1$。我们可以安全地除以 $M$：\n$$\nM\\gamma^2 \\le R^2\n$$\n整理 $M$ 可得理论错误界：\n$$\nM \\le \\left(\\frac{R}{\\gamma}\\right)^2\n$$\n推导至此完成。错误次数受数据集半径与其间隔之比的平方所限制。\n\n### 第 2 部分：实现与验证\n\n以下程序实现了所需的过程：\n1.  **`run_perceptron`**：在给定数据集上模拟感知机算法，并统计直到收敛为止的总错误数。\n2.  **`estimate_R`**：计算半径 $R$，即输入向量 L2 范数的最大值。\n3.  **`estimate_gamma`**：通过对由角度 $\\theta \\in [0, 2\\pi)$ 参数化的单位向量进行确定性的密集搜索来近似间隔 $\\gamma$。\n4.  **`solve`**：主函数，负责为三个测试用例中的每一个组织分析，计算理论界 $B = (R/\\gamma)^2$，将其与观察到的错误数 $M$ 进行比较，并按规定格式化输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef run_perceptron(X, y):\n    \"\"\"\n    Simulates the single-layer perceptron algorithm with zero bias.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n        y (np.ndarray): The labels, shape (n,).\n\n    Returns:\n        int: The total number of mistakes made until convergence.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d)\n    total_mistakes = 0\n    \n    while True:\n        mistakes_in_pass = 0\n        for i in range(n):\n            x_i = X[i]\n            y_i = y[i]\n            \n            # Mistake condition\n            if y_i * (w @ x_i) = 0:\n                total_mistakes += 1\n                mistakes_in_pass += 1\n                # Update rule\n                w += y_i * x_i\n        \n        if mistakes_in_pass == 0:\n            # Convergence: an entire pass with no mistakes\n            break\n            \n    return total_mistakes\n\ndef estimate_R(X):\n    \"\"\"\n    Estimates the radius R as the maximum L2-norm of the data points.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n\n    Returns:\n        float: The radius R.\n    \"\"\"\n    return np.max(np.linalg.norm(X, axis=1))\n\ndef estimate_gamma(X, y):\n    \"\"\"\n    Estimates the margin gamma by searching over unit vectors.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n        y (np.ndarray): The labels, shape (n,).\n\n    Returns:\n        float: The estimated margin gamma.\n    \"\"\"\n    # Using a dense, deterministic grid for the search as required.\n    # 200,000 angles provide a robust approximation for these datasets.\n    num_angles = 200000 \n    angles = np.linspace(0, 2 * np.pi, num_angles, endpoint=False)\n    \n    max_min_proj = -np.inf\n\n    # Pre-compute y_i * x_i vectors to simplify projection calculation\n    Z = X * y[:, np.newaxis]\n    \n    for theta in angles:\n        w = np.array([np.cos(theta), np.sin(theta)])\n        \n        # Calculate signed projections: y_i * (w . x_i) is equivalent to w . (y_i * x_i)\n        projections = Z @ w\n        \n        # Find the minimum projection for this candidate w\n        min_proj = np.min(projections)\n        \n        # The margin gamma is the maximum of these minimums over all w\n        if min_proj > max_min_proj:\n            max_min_proj = min_proj\n            \n    return max_min_proj\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and produce the final output.\n    \"\"\"\n    test_cases_data = [\n        {\n            \"name\": \"Test case 1 (moderate radius and margin)\",\n            \"points\": [\n                (2, 1), (1.5, 1), (2, 2),  # y = +1\n                (-2, -1), (-1.5, -1), (-2, -2) # y = -1\n            ],\n            \"labels\": [1, 1, 1, -1, -1, -1]\n        },\n        {\n            \"name\": \"Test case 2 (large radius, small margin)\",\n            \"points\": [\n                (0.05, 100), (2, 100), (1, 110), # y = +1\n                (-0.05, -100), (-2, -100), (-1, -110) # y = -1\n            ],\n            \"labels\": [1, 1, 1, -1, -1, -1]\n        },\n        {\n            \"name\": \"Test case 3 (small radius, larger margin, nearly one-dimensional)\",\n            \"points\": [\n                (1, 0), (0.8, 0), # y = +1\n                (-1, 0), (-0.8, 0) # y = -1\n            ],\n            \"labels\": [1, 1, -1, -1]\n        }\n    ]\n\n    results = []\n    for case in test_cases_data:\n        X = np.array(case[\"points\"], dtype=float)\n        y = np.array(case[\"labels\"], dtype=float)\n\n        # 1. Estimate R and gamma\n        R = estimate_R(X)\n        gamma = estimate_gamma(X, y)\n\n        # 2. Simulate the perceptron to get observed mistakes\n        M = run_perceptron(X, y)\n        \n        # 3. Compute the theoretical mistake bound B\n        if gamma > 0:\n            B = (R / gamma)**2\n        else:\n            # This case should not happen for separable data, but as a safeguard.\n            B = float('inf')\n\n        # 4. Compare observed mistakes M to theoretical bound B\n        C = (M = B)\n\n        results.append(f\"[{B},{M},{C}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}