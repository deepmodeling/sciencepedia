{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of the single-layer perceptron, we begin with its most fundamental operation: classification. This exercise focuses on the forward pass, where a given input vector $x$ is processed by the neuron's parameters—its weight vector $w$ and bias $b$—to produce a decision. You will apply the core activation function $a(x) = w^{\\top}x + b$ and a threshold rule to determine how the perceptron partitions its input space, providing a crucial first step in visualizing the role of the decision boundary. ",
            "id": "4065769",
            "problem": "Consider a single-layer perceptron realized as a linear threshold unit, a canonical model in neuromorphic and brain-inspired computing that maps an input vector $x \\in \\mathbb{R}^2$ to a scalar pre-activation $a(x) = w^{\\top} x + b$, followed by a sign-threshold output $y(x) = \\operatorname{sgn}(a(x))$, where $\\operatorname{sgn}(z) = 1$ if $z > 0$, $\\operatorname{sgn}(z) = -1$ if $z < 0$, and $\\operatorname{sgn}(z) = 0$ if $z = 0$ to explicitly encode points lying exactly on the decision boundary. This definition is consistent with the well-tested formulation of linear threshold classification and its decision boundary $w^{\\top} x + b = 0$.\n\nLet the perceptron parameters be $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ and $b = 5$. Consider the input set $X = \\{x_1, x_2, x_3, x_4\\}$ with\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad\nx_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad\nx_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}, \\quad\nx_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}.\n$$\n\nUsing only the fundamental definitions stated above, compute the classification outputs $y(x_i)$ for $i \\in \\{1,2,3,4\\}$ and identify which $x_i$ lie exactly on the decision boundary. Then compute the scalar\n$$\nS = \\sum_{i=1}^{4} y(x_i).\n$$\n\nReport only the value of $S$ as your final answer. No rounding is required.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n- Model: Single-layer perceptron (linear threshold unit).\n- Input vector: $x \\in \\mathbb{R}^2$.\n- Pre-activation function: $a(x) = w^{\\top} x + b$.\n- Output function: $y(x) = \\operatorname{sgn}(a(x))$.\n- Sign function definition: $\\operatorname{sgn}(z) = 1$ if $z > 0$, $\\operatorname{sgn}(z) = -1$ if $z < 0$, and $\\operatorname{sgn}(z) = 0$ if $z = 0$.\n- Decision boundary condition: $w^{\\top} x + b = 0$.\n- Perceptron weight vector: $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$.\n- Perceptron bias: $b = 5$.\n- Input set: $X = \\{x_1, x_2, x_3, x_4\\}$.\n- Input vectors:\n$x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, $x_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$, $x_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$, $x_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}$.\n- Target quantity: $S = \\sum_{i=1}^{4} y(x_i)$.\n\n**Step 2: Validate Using Extracted Givens**\n- The problem is **scientifically grounded** and **objective**. The single-layer perceptron is a fundamental concept in machine learning and computational neuroscience. The definitions provided for the pre-activation, output function, and decision boundary are standard and mathematically precise.\n- The problem is **well-posed**. It provides all the necessary components—the model parameters ($w, b$) and the input data ($x_i$)—to compute the required outputs ($y(x_i)$) and the final sum ($S$). The calculation leads to a unique, deterministic answer.\n- The problem setup is **complete and consistent**. There is no missing information, nor are there any contradictory constraints. The explicit definition of $\\operatorname{sgn}(0)=0$ is crucial and is used consistently with the task of identifying points on the decision boundary.\n\n**Step 3: Verdict and Action**\n- The problem is deemed **valid**. A solution will be derived.\n\nThe task is to compute the sum $S = \\sum_{i=1}^{4} y(x_i)$. This requires calculating the classification output $y(x_i)$ for each input vector $x_i$, which in turn depends on the pre-activation $a(x_i)$. The general formula for the pre-activation is $a(x) = w^{\\top} x + b$.\n\nThe given parameters are $w = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$ and $b = 5$.\n\nWe proceed by computing the pre-activation $a(x_i)$ and the corresponding output $y(x_i) = \\operatorname{sgn}(a(x_i))$ for each $i \\in \\{1, 2, 3, 4\\}$.\n\nFor $i=1$:\nThe input vector is $x_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe pre-activation is:\n$$\na(x_1) = w^{\\top} x_1 + b = \\begin{pmatrix} 3 & -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} + 5 = (3)(1) + (-4)(2) + 5 = 3 - 8 + 5 = 0\n$$\nSince $a(x_1) = 0$, the output is $y(x_1) = \\operatorname{sgn}(0) = 0$.\n\nFor $i=2$:\nThe input vector is $x_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$.\nThe pre-activation is:\n$$\na(x_2) = w^{\\top} x_2 + b = \\begin{pmatrix} 3 & -4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + 5 = (3)(2) + (-4)(0) + 5 = 6 + 0 + 5 = 11\n$$\nSince $a(x_2) = 11 > 0$, the output is $y(x_2) = \\operatorname{sgn}(11) = 1$.\n\nFor $i=3$:\nThe input vector is $x_3 = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$.\nThe pre-activation is:\n$$\na(x_3) = w^{\\top} x_3 + b = \\begin{pmatrix} 3 & -4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} + 5 = (3)(0) + (-4)(3) + 5 = 0 - 12 + 5 = -7\n$$\nSince $a(x_3) = -7 < 0$, the output is $y(x_3) = \\operatorname{sgn}(-7) = -1$.\n\nFor $i=4$:\nThe input vector is $x_4 = \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix}$.\nThe pre-activation is:\n$$\na(x_4) = w^{\\top} x_4 + b = \\begin{pmatrix} 3 & -4 \\end{pmatrix} \\begin{pmatrix} \\frac{10}{3} \\\\ \\frac{5}{2} \\end{pmatrix} + 5 = (3)\\left(\\frac{10}{3}\\right) + (-4)\\left(\\frac{5}{2}\\right) + 5 = 10 - 10 + 5 = 5\n$$\nSince $a(x_4) = 5 > 0$, the output is $y(x_4) = \\operatorname{sgn}(5) = 1$.\n\nAn input point $x_i$ lies exactly on the decision boundary if its pre-activation is zero, i.e., $a(x_i) = w^{\\top} x_i + b = 0$. From our calculations, only $a(x_1) = 0$. Therefore, $x_1$ is the only input point that lies exactly on the decision boundary.\n\nFinally, we compute the scalar sum $S$:\n$$\nS = \\sum_{i=1}^{4} y(x_i) = y(x_1) + y(x_2) + y(x_3) + y(x_4)\n$$\nSubstituting the computed values:\n$$\nS = 0 + 1 + (-1) + 1 = 1\n$$\nThe value of the sum is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "After mastering how a perceptron classifies inputs, the next step is to understand how it learns. This practice moves from the static model to the dynamic learning process, implementing the classic mistake-driven Perceptron Learning Algorithm. By manually tracing the updates to the weight vector $w$ and bias $b$ in response to misclassifications, you will gain a deep, procedural understanding of how the decision boundary iteratively adjusts its position and orientation to correctly separate the data points. ",
            "id": "4065810",
            "problem": "Consider a single-layer perceptron operating as a hard-threshold classifier in a neuromorphic event-driven stream. The classifier has weight vector $w \\in \\mathbb{R}^{2}$ and bias $b \\in \\mathbb{R}$, and predicts the label of an input $x \\in \\mathbb{R}^{2}$ as $\\operatorname{sign}(w \\cdot x + b)$ with labels $y \\in \\{ -1, +1 \\}$. The learning is mistake-driven with learning rate $\\eta = 1$: upon receiving the $t$-th example $(x^{(t)}, y^{(t)})$, compute the activation $a^{(t)} = w^{(t-1)} \\cdot x^{(t)} + b^{(t-1)}$. If $y^{(t)} a^{(t)} \\le 0$, then update according to\n$$\nw^{(t)} = w^{(t-1)} + y^{(t)} x^{(t)}, \n\\quad\nb^{(t)} = b^{(t-1)} + y^{(t)},\n$$\notherwise leave the parameters unchanged, that is, $w^{(t)} = w^{(t-1)}$ and $b^{(t)} = b^{(t-1)}$. The initial state is $w^{(0)} = (0, 0)$ and $b^{(0)} = 0$.\n\nA fixed, finite set of labeled inputs in $\\mathbb{R}^{2}$ is given by\n- $x_{1} = (1, 0)$ with $y_{1} = +1$,\n- $x_{2} = (-0.1, 0)$ with $y_{2} = -1$,\n- $x_{3} = (0, 1)$ with $y_{3} = +1$.\n\nThese inputs are presented as an infinite ordered stream that repeats the cycle $(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3})$ ad infinitum. Presentation time $t \\in \\mathbb{N}$ counts the total number of examples processed, starting at $t = 1$.\n\nDefine the classifier to be consistent with the data at time $t$ if, immediately after processing the example at time $t$, the current parameters $(w^{(t)}, b^{(t)})$ classify all three points $\\{x_{1}, x_{2}, x_{3}\\}$ correctly (i.e., for each $i \\in \\{1,2,3\\}$, $y_{i}\\,(w^{(t)} \\cdot x_{i} + b^{(t)}) > 0$).\n\nCompute explicitly the sequence of parameter states $(w^{(t)}, b^{(t)})$ for consecutive presentation times $t$ until the earliest presentation time $T$ at which the classifier becomes consistent with the data (as defined above). Report the minimal such presentation index $T$. Your final answer must be a single integer with no units. No rounding is required.",
            "solution": "The user wants me to solve a problem for a given problem statement.\n\nThe problem asks for the earliest presentation time $T$ at which a single-layer perceptron classifier becomes consistent with a given dataset. The learning process is mistake-driven and proceeds by iterating through a fixed cycle of three data points.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- **Model**: Single-layer perceptron.\n- **Input space**: $x \\in \\mathbb{R}^{2}$.\n- **Label space**: $y \\in \\{ -1, +1 \\}$.\n- **Classifier function**: $\\operatorname{sign}(w \\cdot x + b)$, where $w \\in \\mathbb{R}^{2}$ and $b \\in \\mathbb{R}$.\n- **Learning rule**: Mistake-driven (Perceptron algorithm) with learning rate $\\eta = 1$.\n- **Update condition**: An update occurs at step $t$ if $y^{(t)} a^{(t)} \\le 0$, where $a^{(t)} = w^{(t-1)} \\cdot x^{(t)} + b^{(t-1)}$.\n- **Update equations**:\n    - $w^{(t)} = w^{(t-1)} + y^{(t)} x^{(t)}$\n    - $b^{(t)} = b^{(t-1)} + y^{(t)}$\n- **No-update condition**: If $y^{(t)} a^{(t)} > 0$, then $w^{(t)} = w^{(t-1)}$ and $b^{(t)} = b^{(t-1)}$.\n- **Initial state**: $w^{(0)} = (0, 0)$ and $b^{(0)} = 0$.\n- **Data set**:\n    - $x_{1} = (1, 0)$ with $y_{1} = +1$.\n    - $x_{2} = (-0.1, 0)$ with $y_{2} = -1$.\n    - $x_{3} = (0, 1)$ with $y_{3} = +1$.\n- **Input stream**: An infinite sequence repeating the cycle $(x_{1}, y_{1}), (x_{2}, y_{2}), (x_{3}, y_{3})$.\n- **Presentation time**: $t \\in \\mathbb{N}$ starts at $t=1$.\n- **Consistency definition**: The classifier is consistent at time $t$ if the parameters $(w^{(t)}, b^{(t)})$ correctly classify all three points $\\{x_{1}, x_{2}, x_{3}\\}$. Correct classification means $y_{i}\\,(w^{(t)} \\cdot x_{i} + b^{(t)}) > 0$ for $i \\in \\{1, 2, 3\\}$.\n- **Goal**: Find the smallest presentation time $T$ at which the classifier becomes consistent.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It provides a standard formulation of the Perceptron learning algorithm, a cornerstone of machine learning. All initial conditions, data, update rules, and the stopping criterion are precisely defined. The dataset is linearly separable, which guarantees that the algorithm will converge to a solution in a finite number of steps. The problem asks for a verifiable quantity (the number of steps to convergence) under a specific data presentation order. There are no contradictions, ambiguities, or factual errors.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution.\n\nTo find the earliest presentation time $T$, we must simulate the learning process step-by-step, starting from the initial state and updating the parameters upon each mistake. After each step $t$, we check if the resulting classifier $(w^{(t)}, b^{(t)})$ is consistent with the entire dataset.\n\nThe initial state of the parameters at $t=0$ is $w^{(0)} = (0, 0)$ and $b^{(0)} = 0$.\n\n**Step $t=1$**:\nThe first example presented is $(x^{(1)}, y^{(1)}) = (x_1, y_1) = ((1, 0), +1)$.\nThe parameters before this step are $(w^{(0)}, b^{(0)}) = ((0, 0), 0)$.\nThe activation is $a^{(1)} = w^{(0)} \\cdot x^{(1)} + b^{(0)} = (0, 0) \\cdot (1, 0) + 0 = 0$.\nWe check the mistake condition: $y^{(1)}a^{(1)} = (+1)(0) = 0$. Since $0 \\le 0$, a mistake has occurred.\nWe update the parameters:\n$w^{(1)} = w^{(0)} + y^{(1)}x^{(1)} = (0, 0) + (+1)(1, 0) = (1, 0)$.\n$b^{(1)} = b^{(0)} + y^{(1)} = 0 + 1 = 1$.\nThe state after step $1$ is $(w^{(1)}, b^{(1)}) = ((1, 0), 1)$.\nWe check for consistency:\n1. For $(x_1, y_1)$: $y_1(w^{(1)} \\cdot x_1 + b^{(1)}) = +1((1, 0) \\cdot (1, 0) + 1) = 2 > 0$. (Correct)\n2. For $(x_2, y_2)$: $y_2(w^{(1)} \\cdot x_2 + b^{(1)}) = -1((1, 0) \\cdot (-0.1, 0) + 1) = -1(-0.1 + 1) = -0.9 \\le 0$. (Incorrect)\nThe classifier is not consistent.\n\n**Step $t=2$**:\nThe second example is $(x^{(2)}, y^{(2)}) = (x_2, y_2) = ((-0.1, 0), -1)$.\nThe parameters are $(w^{(1)}, b^{(1)}) = ((1, 0), 1)$.\nThe activation is $a^{(2)} = w^{(1)} \\cdot x^{(2)} + b^{(1)} = (1, 0) \\cdot (-0.1, 0) + 1 = -0.1 + 1 = 0.9$.\nMistake check: $y^{(2)}a^{(2)} = (-1)(0.9) = -0.9 \\le 0$. A mistake has occurred.\nUpdate:\n$w^{(2)} = w^{(1)} + y^{(2)}x^{(2)} = (1, 0) + (-1)(-0.1, 0) = (1, 0) + (0.1, 0) = (1.1, 0)$.\n$b^{(2)} = b^{(1)} + y^{(2)} = 1 + (-1) = 0$.\nState after step $2$: $(w^{(2)}, b^{(2)}) = ((1.1, 0), 0)$.\nConsistency check:\n1. For $(x_1, y_1)$: $y_1(w^{(2)} \\cdot x_1 + b^{(2)}) = +1((1.1, 0) \\cdot (1, 0) + 0) = 1.1 > 0$. (Correct)\n2. For $(x_2, y_2)$: $y_2(w^{(2)} \\cdot x_2 + b^{(2)}) = -1((1.1, 0) \\cdot (-0.1, 0) + 0) = -1(-0.11) = 0.11 > 0$. (Correct)\n3. For $(x_3, y_3)$: $y_3(w^{(2)} \\cdot x_3 + b^{(2)}) = +1((1.1, 0) \\cdot (0, 1) + 0) = 0 \\le 0$. (Incorrect)\nThe classifier is not consistent.\n\n**Step $t=3$**:\nThe third example is $(x^{(3)}, y^{(3)}) = (x_3, y_3) = ((0, 1), +1)$.\nThe parameters are $(w^{(2)}, b^{(2)}) = ((1.1, 0), 0)$.\nThe activation is $a^{(3)} = w^{(2)} \\cdot x^{(3)} + b^{(2)} = (1.1, 0) \\cdot (0, 1) + 0 = 0$.\nMistake check: $y^{(3)}a^{(3)} = (+1)(0) = 0 \\le 0$. A mistake has occurred.\nUpdate:\n$w^{(3)} = w^{(2)} + y^{(3)}x^{(3)} = (1.1, 0) + (+1)(0, 1) = (1.1, 1)$.\n$b^{(3)} = b^{(2)} + y^{(3)} = 0 + 1 = 1$.\nState after step $3$: $(w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$.\nConsistency check:\n1. For $(x_1, y_1)$: $y_1(w^{(3)} \\cdot x_1 + b^{(3)}) = +1((1.1, 1) \\cdot (1, 0) + 1) = 2.1 > 0$. (Correct)\n2. For $(x_2, y_2)$: $y_2(w^{(3)} \\cdot x_2 + b^{(3)}) = -1((1.1, 1) \\cdot (-0.1, 0) + 1) = -1(-0.11 + 1) = -0.89 \\le 0$. (Incorrect)\nThe classifier is not consistent.\n\n**Step $t=4$**:\nThe stream repeats; the fourth example is $(x^{(4)}, y^{(4)}) = (x_1, y_1) = ((1, 0), +1)$.\nThe parameters are $(w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$.\nThe activation is $a^{(4)} = w^{(3)} \\cdot x^{(4)} + b^{(3)} = (1.1, 1) \\cdot (1, 0) + 1 = 1.1 + 1 = 2.1$.\nMistake check: $y^{(4)}a^{(4)} = (+1)(2.1) = 2.1 > 0$. No mistake.\nThe parameters are not updated: $(w^{(4)}, b^{(4)}) = (w^{(3)}, b^{(3)}) = ((1.1, 1), 1)$.\nSince the parameters have not changed from step $t=3$, the classifier is still not consistent.\n\n**Step $t=5$**:\nThe fifth example is $(x^{(5)}, y^{(5)}) = (x_2, y_2) = ((-0.1, 0), -1)$.\nThe parameters are $(w^{(4)}, b^{(4)}) = ((1.1, 1), 1)$.\nThe activation is $a^{(5)} = w^{(4)} \\cdot x^{(5)} + b^{(4)} = (1.1, 1) \\cdot (-0.1, 0) + 1 = -0.11 + 1 = 0.89$.\nMistake check: $y^{(5)}a^{(5)} = (-1)(0.89) = -0.89 \\le 0$. A mistake has occurred.\nUpdate:\n$w^{(5)} = w^{(4)} + y^{(5)}x^{(5)} = (1.1, 1) + (-1)(-0.1, 0) = (1.1, 1) + (0.1, 0) = (1.2, 1)$.\n$b^{(5)} = b^{(4)} + y^{(5)} = 1 + (-1) = 0$.\nState after step $5$: $(w^{(5)}, b^{(5)}) = ((1.2, 1), 0)$.\nConsistency check:\n1. For $(x_1, y_1)$: $y_1(w^{(5)} \\cdot x_1 + b^{(5)}) = +1((1.2, 1) \\cdot (1, 0) + 0) = 1.2 > 0$. (Correct)\n2. For $(x_2, y_2)$: $y_2(w^{(5)} \\cdot x_2 + b^{(5)}) = -1((1.2, 1) \\cdot (-0.1, 0) + 0) = -1(-0.12) = 0.12 > 0$. (Correct)\n3. For $(x_3, y_3)$: $y_3(w^{(5)} \\cdot x_3 + b^{(5)}) = +1((1.2, 1) \\cdot (0, 1) + 0) = 1 > 0$. (Correct)\nAfter processing the example at $t=5$, the classifier correctly classifies all three points in the dataset. Therefore, the classifier has become consistent.\n\nThe earliest presentation time $T$ at which the classifier becomes consistent is $5$.",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "A key question in machine learning is not just *how* an algorithm learns, but *why* it is guaranteed to succeed. This advanced practice bridges the gap between algorithm implementation and formal theoretical analysis by exploring the celebrated Perceptron Mistake Bound theorem. You will derive this foundational result, which proves that for any linearly separable dataset, the learning algorithm converges after a finite number of mistakes bounded by the geometric properties of the data—its radius $R$ and margin $\\gamma$. By implementing a simulation to compare the observed number of mistakes against the theoretical bound, you will gain a profound appreciation for this powerful guarantee. ",
            "id": "4065892",
            "problem": "You are given a finite dataset of labeled points $\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^n$ in $\\mathbb{R}^d$ with $y_i \\in \\{-1,+1\\}$ that is linearly separable by a hyperplane passing through the origin. Consider the single-layer perceptron with zero bias (threshold at the origin), unit learning rate, and the standard update. The classifier prediction on an input $\\mathbf{x}$ is the sign of the inner product, and an update is performed only on mistakes. Formally, starting from $\\mathbf{w}_0=\\mathbf{0}$, for each training example $(\\mathbf{x}_i,y_i)$ in sequence, the prediction is $\\operatorname{sign}(\\mathbf{w}\\cdot \\mathbf{x}_i)$, and on a mistake (defined by $y_i(\\mathbf{w}\\cdot \\mathbf{x}_i)\\le 0$), perform the update $\\mathbf{w}\\leftarrow \\mathbf{w}+y_i\\mathbf{x}_i$. Repeat passes over the dataset until an entire pass produces no mistakes.\n\nThe radius bound $R$ is defined as $R=\\max_i\\|\\mathbf{x}_i\\|_2$. The margin $\\gamma$ of the dataset with respect to the best origin-passing separating unit vector is defined as\n$$\n\\gamma \\;=\\; \\max_{\\|\\mathbf{w}\\|_2=1}\\;\\min_{i}\\; y_i\\,(\\mathbf{w}\\cdot \\mathbf{x}_i).\n$$\n\nUsing only these core definitions and fundamental inequalities from linear algebra, derive the theoretical mistake bound predicted by the single-layer perceptron convergence theorem in terms of $R$ and $\\gamma$, and implement a program that:\n- Estimates $R$ exactly as $R=\\max_i\\|\\mathbf{x}_i\\|_2$.\n- Estimates $\\gamma$ by searching over unit directions in $\\mathbb{R}^2$ parameterized by an angle $\\theta$ and taking the maximum of the minimum signed projection across all points. Use a dense uniform discretization of angles on $[0,2\\pi)$ to approximate the maximization. Your search must be deterministic.\n- Simulates the perceptron training sequence starting from $\\mathbf{w}_0=\\mathbf{0}$, cycling through the dataset in the given fixed order, counting the total number of mistakes until an entire pass yields zero mistakes.\n- Computes the theoretical mistake bound using the derived expression and compares it to the observed number of mistakes from the simulation.\n\nYou must apply this procedure on the following test suite of three datasets in $\\mathbb{R}^2$ (all points listed are in the order they must be presented during training):\n\nTest case $1$ (moderate radius and margin):\n- Positive examples ($y=+1$): $(2,1)$, $(1.5,1)$, $(2,2)$.\n- Negative examples ($y=-1$): $(-2,-1)$, $(-1.5,-1)$, $(-2,-2)$.\n\nTest case $2$ (large radius, small margin):\n- Positive examples ($y=+1$): $(0.05,100)$, $(2,100)$, $(1,110)$.\n- Negative examples ($y=-1$): $(-0.05,-100)$, $(-2,-100)$, $(-1,-110)$.\n\nTest case $3$ (small radius, larger margin, nearly one-dimensional):\n- Positive examples ($y=+1$): $(1,0)$, $(0.8,0)$.\n- Negative examples ($y=-1$): $(-1,0)$, $(-0.8,0)$.\n\nAll three datasets are separable by a hyperplane through the origin. For each test case:\n- Estimate $R$ and $\\gamma$ as specified.\n- Derive and compute the theoretical mistake bound from these quantities.\n- Simulate the perceptron to obtain the observed mistake count.\n- Produce a comparison indicating whether the observed mistakes are less than or equal to the theoretical bound.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the form $[B,M,C]$ with $B$ the theoretical bound as a floating-point number, $M$ the observed mistakes as an integer, and $C$ a boolean indicating whether $M\\le B$. For example, the output should look like $[[B_1,M_1,C_1],[B_2,M_2,C_2],[B_3,M_3,C_3]]$ with numerical values filled in by your computation.\n\nAngle unit: Use radians for any internal angle parameterization in your estimation of $\\gamma$.\n\nNo physical units are involved. Express all numerical outputs as raw numbers (no percentage signs). The program must be self-contained and require no user input.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the well-established theory of the single-layer perceptron, well-posed, and objective. All data and definitions required for a unique and meaningful solution are provided. The task involves deriving a classic theoretical result (the perceptron mistake bound) and verifying it computationally on specific datasets, which is a standard and rigorous exercise in machine learning.\n\n### Part 1: Derivation of the Perceptron Mistake Bound\n\nThe problem requires the derivation of the theoretical upper bound on the number of mistakes made by the single-layer perceptron algorithm. Let the total number of mistakes be $M$. Let the sequence of weight vectors, at the moment a mistake is made, be $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_M$. The initial weight vector is $\\mathbf{w}_0 = \\mathbf{0}$.\n\nLet the $k$-th mistake (for $k=1, \\dots, M$) occur on the training example $(\\mathbf{x}_k, y_k)$. Note that for simplicity of notation, we re-index the points from the dataset sequence on which mistakes occur. The perceptron update rule is applied:\n$$\n\\mathbf{w}_k = \\mathbf{w}_{k-1} + y_k \\mathbf{x}_k\n$$\nThe condition for the $k$-th mistake is:\n$$\ny_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) \\le 0\n$$\nThe dataset is assumed to be linearly separable by a hyperplane passing through the origin. This guarantees the existence of a weight vector $\\mathbf{w}^*$ with unit norm, $\\|\\mathbf{w}^*\\|_2=1$, and a margin $\\gamma > 0$ such that for all points $(\\mathbf{x}_i, y_i)$ in the dataset:\n$$\ny_i(\\mathbf{w}^* \\cdot \\mathbf{x}_i) \\ge \\gamma\n$$\nwhere $\\gamma$ is the maximal such margin over all possible unit-norm separating vectors.\n\nThe derivation proceeds by establishing two inequalities on the final weight vector after $M$ mistakes, $\\mathbf{w}_M$.\n\n**1. Upper Bound on the Norm of the Weight Vector**\n\nWe analyze the squared L2-norm of the weight vector after the $k$-th update:\n$$\n\\|\\mathbf{w}_k\\|_2^2 = \\|\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k\\|_2^2\n$$\nExpanding the dot product:\n$$\n\\|\\mathbf{w}_k\\|_2^2 = (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) \\cdot (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) = \\|\\mathbf{w}_{k-1}\\|_2^2 + 2y_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) + y_k^2\\|\\mathbf{x}_k\\|_2^2\n$$\nSince $y_k \\in \\{-1, +1\\}$, we have $y_k^2=1$. From the mistake condition, we know $y_k(\\mathbf{w}_{k-1} \\cdot \\mathbf{x}_k) \\le 0$. Substituting these into the equation gives:\n$$\n\\|\\mathbf{w}_k\\|_2^2 \\le \\|\\mathbf{w}_{k-1}\\|_2^2 + \\|\\mathbf{x}_k\\|_2^2\n$$\nThe problem defines the radius $R = \\max_i \\|\\mathbf{x}_i\\|_2$. Therefore, $\\|\\mathbf{x}_k\\|_2^2 \\le R^2$ for any point $\\mathbf{x}_k$. This leads to the recurrence relation:\n$$\n\\|\\mathbf{w}_k\\|_2^2 \\le \\|\\mathbf{w}_{k-1}\\|_2^2 + R^2\n$$\nBy recursively applying this inequality starting from $\\mathbf{w}_0 = \\mathbf{0}$, we obtain an upper bound on the squared norm of the weight vector after $M$ mistakes:\n$$\n\\|\\mathbf{w}_M\\|_2^2 \\le \\|\\mathbf{w}_{M-1}\\|_2^2 + R^2 \\le \\|\\mathbf{w}_{M-2}\\|_2^2 + 2R^2 \\le \\dots \\le \\|\\mathbf{w}_0\\|_2^2 + M R^2 = M R^2\n$$\nSo, we have our first result:\n$$\n\\|\\mathbf{w}_M\\|_2^2 \\le M R^2\n$$\n\n**2. Lower Bound on the Dot Product with the Optimal Vector**\n\nNext, we analyze the dot product of the weight vector $\\mathbf{w}_k$ with the optimal separating vector $\\mathbf{w}^*$:\n$$\n\\mathbf{w}_k \\cdot \\mathbf{w}^* = (\\mathbf{w}_{k-1} + y_k \\mathbf{x}_k) \\cdot \\mathbf{w}^* = \\mathbf{w}_{k-1} \\cdot \\mathbf{w}^* + y_k(\\mathbf{x}_k \\cdot \\mathbf{w}^*)\n$$\nFrom the definition of the margin $\\gamma$, we have $y_k(\\mathbf{x}_k \\cdot \\mathbf{w}^*) \\ge \\gamma$. Substituting this provides a lower bound on the growth of the dot product:\n$$\n\\mathbf{w}_k \\cdot \\mathbf{w}^* \\ge \\mathbf{w}_{k-1} \\cdot \\mathbf{w}^* + \\gamma\n$$\nUnrolling this recurrence starting from $\\mathbf{w}_0 \\cdot \\mathbf{w}^* = 0$:\n$$\n\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge \\mathbf{w}_{M-1} \\cdot \\mathbf{w}^* + \\gamma \\ge \\dots \\ge \\mathbf{w}_0 \\cdot \\mathbf{w}^* + M\\gamma = M\\gamma\n$$\nThis gives our second result:\n$$\n\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma\n$$\n\n**3. Combining the Bounds**\n\nWe have two inequalities concerning $\\mathbf{w}_M$:\n1. $\\|\\mathbf{w}_M\\|_2^2 \\le M R^2$\n2. $\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma > 0$\n\nBy the Cauchy-Schwarz inequality, we have $(\\mathbf{u} \\cdot \\mathbf{v})^2 \\le \\|\\mathbf{u}\\|_2^2 \\|\\mathbf{v}\\|_2^2$. Applying this to $\\mathbf{w}_M$ and $\\mathbf{w}^*$:\n$$\n(\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2 \\|\\mathbf{w}^*\\|_2^2\n$$\nSince $\\|\\mathbf{w}^*\\|_2 = 1$, this simplifies to $(\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2$.\n\nNow, we substitute the bounds derived in steps 1 and 2. Since both sides of the inequality $\\mathbf{w}_M \\cdot \\mathbf{w}^* \\ge M \\gamma$ are positive, we can square it without changing the direction of inequality:\n$$\n(M\\gamma)^2 \\le (\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2\n$$\nCombining everything:\n$$\n(M\\gamma)^2 \\le (\\mathbf{w}_M \\cdot \\mathbf{w}^*)^2 \\le \\|\\mathbf{w}_M\\|_2^2 \\le M R^2\n$$\nThis gives the final inequality relating $M$, $R$, and $\\gamma$:\n$$\nM^2\\gamma^2 \\le M R^2\n$$\nSince the algorithm must make at least one mistake to converge from $\\mathbf{w}_0 = \\mathbf{0}$ (assuming $\\mathbf{0}$ is not a valid separator), $M \\ge 1$. We can safely divide by $M$:\n$$\nM\\gamma^2 \\le R^2\n$$\nRearranging for $M$, we arrive at the theoretical mistake bound:\n$$\nM \\le \\left(\\frac{R}{\\gamma}\\right)^2\n$$\nThis completes the derivation. The number of mistakes is bounded by the square of the ratio of the dataset's radius to its margin.\n\n### Part 2: Implementation and Verification\n\nThe following program implements the required procedures:\n1.  **`run_perceptron`**: Simulates the perceptron algorithm on a given dataset and counts the total number of mistakes until convergence.\n2.  **`estimate_R`**: Calculates the radius $R$ as the maximum L2-norm of the input vectors.\n3.  **`estimate_gamma`**: Approximates the margin $\\gamma$ by performing a deterministic, dense search over unit vectors parameterized by an angle $\\theta \\in [0, 2\\pi)$.\n4.  **`solve`**: The main function that orchestrates the analysis for each of the three test cases, calculates the theoretical bound $B = (R/\\gamma)^2$, compares it to the observed mistakes $M$, and formats the output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef run_perceptron(X, y):\n    \"\"\"\n    Simulates the single-layer perceptron algorithm with zero bias.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n        y (np.ndarray): The labels, shape (n,).\n\n    Returns:\n        int: The total number of mistakes made until convergence.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d)\n    total_mistakes = 0\n    \n    while True:\n        mistakes_in_pass = 0\n        for i in range(n):\n            x_i = X[i]\n            y_i = y[i]\n            \n            # Mistake condition\n            if y_i * (w @ x_i) <= 0:\n                total_mistakes += 1\n                mistakes_in_pass += 1\n                # Update rule\n                w += y_i * x_i\n        \n        if mistakes_in_pass == 0:\n            # Convergence: an entire pass with no mistakes\n            break\n            \n    return total_mistakes\n\ndef estimate_R(X):\n    \"\"\"\n    Estimates the radius R as the maximum L2-norm of the data points.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n\n    Returns:\n        float: The radius R.\n    \"\"\"\n    return np.max(np.linalg.norm(X, axis=1))\n\ndef estimate_gamma(X, y):\n    \"\"\"\n    Estimates the margin gamma by searching over unit vectors.\n\n    Args:\n        X (np.ndarray): The data points, shape (n, d).\n        y (np.ndarray): The labels, shape (n,).\n\n    Returns:\n        float: The estimated margin gamma.\n    \"\"\"\n    # Using a dense, deterministic grid for the search as required.\n    # 200,000 angles provide a robust approximation for these datasets.\n    num_angles = 200000 \n    angles = np.linspace(0, 2 * np.pi, num_angles, endpoint=False)\n    \n    max_min_proj = -np.inf\n\n    # Pre-compute y_i * x_i vectors to simplify projection calculation\n    Z = X * y[:, np.newaxis]\n    \n    for theta in angles:\n        w = np.array([np.cos(theta), np.sin(theta)])\n        \n        # Calculate signed projections: y_i * (w . x_i) is equivalent to w . (y_i * x_i)\n        projections = Z @ w\n        \n        # Find the minimum projection for this candidate w\n        min_proj = np.min(projections)\n        \n        # The margin gamma is the maximum of these minimums over all w\n        if min_proj > max_min_proj:\n            max_min_proj = min_proj\n            \n    return max_min_proj\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and produce the final output.\n    \"\"\"\n    test_cases_data = [\n        {\n            \"name\": \"Test case 1 (moderate radius and margin)\",\n            \"points\": [\n                (2, 1), (1.5, 1), (2, 2),  # y = +1\n                (-2, -1), (-1.5, -1), (-2, -2) # y = -1\n            ],\n            \"labels\": [1, 1, 1, -1, -1, -1]\n        },\n        {\n            \"name\": \"Test case 2 (large radius, small margin)\",\n            \"points\": [\n                (0.05, 100), (2, 100), (1, 110), # y = +1\n                (-0.05, -100), (-2, -100), (-1, -110) # y = -1\n            ],\n            \"labels\": [1, 1, 1, -1, -1, -1]\n        },\n        {\n            \"name\": \"Test case 3 (small radius, larger margin, nearly one-dimensional)\",\n            \"points\": [\n                (1, 0), (0.8, 0), # y = +1\n                (-1, 0), (-0.8, 0) # y = -1\n            ],\n            \"labels\": [1, 1, -1, -1]\n        }\n    ]\n\n    results = []\n    for case in test_cases_data:\n        X = np.array(case[\"points\"], dtype=float)\n        y = np.array(case[\"labels\"], dtype=float)\n\n        # 1. Estimate R and gamma\n        R = estimate_R(X)\n        gamma = estimate_gamma(X, y)\n\n        # 2. Simulate the perceptron to get observed mistakes\n        M = run_perceptron(X, y)\n        \n        # 3. Compute the theoretical mistake bound B\n        if gamma > 0:\n            B = (R / gamma)**2\n        else:\n            # This case should not happen for separable data, but as a safeguard.\n            B = float('inf')\n\n        # 4. Compare observed mistakes M to theoretical bound B\n        C = (M <= B)\n\n        results.append(f\"[{B},{M},{C}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}