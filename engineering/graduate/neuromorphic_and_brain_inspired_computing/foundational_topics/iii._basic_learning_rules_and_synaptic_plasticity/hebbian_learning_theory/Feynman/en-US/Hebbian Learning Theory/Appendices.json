{
    "hands_on_practices": [
        {
            "introduction": "The core tenet of Hebbian learning, 'cells that fire together, wire together,' suggests that synaptic strength increases with correlated activity. However, this principle alone leads to unstable, runaway growth of synaptic weights. This first exercise provides a direct, hands-on calculation with a stabilized Hebbian learning rule, which incorporates a homeostatic term to counteract unchecked potentiation . By computing a single weight update, you will gain a concrete understanding of the interplay between the associative Hebbian term and the subtractive homeostatic term that ensures synaptic weights remain within a functional range.",
            "id": "4054261",
            "problem": "Consider an on-chip learning circuit in a neuromorphic synapse that implements rate-based Hebbian plasticity with a quadratic homeostatic stabilization term. The pre-synaptic activity is encoded as a firing rate $r_{\\text{pre}}$ and the post-synaptic activity is encoded as a firing rate $r_{\\text{post}}$, both represented by current-mode signals in an analog multiplier and squaring stage. The circuit computes a weight update increment $\\Delta w$ per learning cycle according to the following rate-based Hebbian rule derived from Hebb’s postulate of learning and homeostatic control: \n$$\n\\Delta w = \\eta \\left(r_{\\text{pre}}\\,r_{\\text{post}} - \\beta\\,r_{\\text{post}}^{2}\\right),\n$$\nwhere $\\eta$ is a learning rate constant and $\\beta$ is a homeostatic scaling parameter. Assume that $r_{\\text{pre}}$ and $r_{\\text{post}}$ are measured over a sufficiently long window to be treated as stationary rates and that $\\eta$ has been chosen so that $\\Delta w$ is dimensionless. For a particular learning cycle, the measured values are $r_{\\text{pre}} = 20\\ \\text{Hz}$, $r_{\\text{post}} = 15\\ \\text{Hz}$, $\\eta = 10^{-4}$, and $\\beta = 0.5$. Compute the numerical value of the dimensionless update increment $\\Delta w$ for this cycle. Provide the final answer as a single real number. No rounding is required.",
            "solution": "The problem requires the computation of the dimensionless weight update increment, $\\Delta w$, for an on-chip learning circuit. The behavior of the circuit is governed by a specified rate-based Hebbian plasticity rule that includes a quadratic homeostatic stabilization term.\n\nThe provided equation for the weight update is:\n$$\n\\Delta w = \\eta \\left(r_{\\text{pre}}\\,r_{\\text{post}} - \\beta\\,r_{\\text{post}}^{2}\\right)\n$$\nThe parameters for a specific learning cycle are given as:\n- The pre-synaptic firing rate, $r_{\\text{pre}} = 20\\ \\text{Hz}$.\n- The post-synaptic firing rate, $r_{\\text{post}} = 15\\ \\text{Hz}$.\n- The learning rate constant, $\\eta = 10^{-4}$.\n- The homeostatic scaling parameter, $\\beta = 0.5$.\n\nThe problem states that $\\Delta w$ is a dimensionless quantity, which implies that the units of $\\text{Hz}^2$ from the rate terms are absorbed by the learning rate constant $\\eta$. We proceed by substituting the given numerical values directly into the equation.\n\nSubstitution of the given values yields:\n$$\n\\Delta w = 10^{-4} \\left( (20)(15) - (0.5)(15)^{2} \\right)\n$$\nWe first evaluate the two terms inside the parentheses. The first term corresponds to the standard Hebbian product:\n$$\nr_{\\text{pre}}\\,r_{\\text{post}} = (20)(15) = 300\n$$\nThe second term corresponds to the homeostatic stabilization component:\n$$\n\\beta\\,r_{\\text{post}}^{2} = (0.5)(15)^{2}\n$$\nCalculating the square of $r_{\\text{post}}$:\n$$\n15^{2} = 225\n$$\nNow, multiplying by $\\beta$:\n$$\n\\beta\\,r_{\\text{post}}^{2} = (0.5)(225) = 112.5\n$$\nNext, we compute the difference between these two terms, as specified by the expression inside the parentheses:\n$$\n(r_{\\text{pre}}\\,r_{\\text{post}} - \\beta\\,r_{\\text{post}}^{2}) = 300 - 112.5 = 187.5\n$$\nFinally, we multiply this result by the learning rate $\\eta$ to obtain the final weight update increment $\\Delta w$:\n$$\n\\Delta w = 10^{-4} \\times 187.5\n$$\nThis calculation is equivalent to shifting the decimal point four places to the left:\n$$\n\\Delta w = 0.01875\n$$\nThus, the numerical value of the dimensionless update increment for the given learning cycle is $0.01875$.",
            "answer": "$$\n\\boxed{0.01875}\n$$"
        },
        {
            "introduction": "Beyond a single update, how does a synapse evolve over time? This practice elevates our analysis from a static snapshot to the long-term dynamics of synaptic plasticity. You will employ a mean-field approximation—a powerful technique in theoretical neuroscience—to derive a deterministic differential equation that governs the evolution of synaptic weight . By analyzing the fixed points and stability of this system, you will uncover how the balance between Hebbian drive and homeostatic decay mechanisms dictates the final, stable strength of a synapse, revealing the conditions under which a synapse can form and maintain a memory trace.",
            "id": "4046815",
            "problem": "Consider a single linear postsynaptic neuron in a neuromorphic system with one presynaptic input stream. The presynaptic activity is modeled as a zero-mean, stationary, ergodic stochastic process with finite second moment. Denote the presynaptic signal by $x(t)$ and the synaptic efficacy by $w(t)$. Assume a separation of time scales such that synaptic dynamics evolve on a slower time scale than the fluctuations of $x(t)$. The postsynaptic activity is well-approximated by a linear response model, where $y(t)$ satisfies $y(t) \\approx w(t)\\,x(t)$ for the range of $w(t)$ explored by learning. \n\nAdopt the core Hebbian learning principle that the instantaneous synaptic modification is proportional to the product of presynaptic and postsynaptic activities, and include biophysically motivated homeostatic regularization terms: a linear decay due to weight leakage and a cubic saturating term representing resource limitations and nonlinearity in synaptic consolidation. Let the proportionality constant for the Hebbian drive be $\\eta>0$, the leakage rate be $\\lambda>0$, and the saturation coefficient be $\\mu>0$. Define the input variance by $\\sigma_x^{2} = \\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T} x^{2}(t)\\,dt$, which exists by stationarity and ergodicity. \n\nStarting from these principles and assumptions, perform a mean-field analysis to derive a deterministic ordinary differential equation (ODE) governing the evolution of $w(t)$ in terms of $\\eta$, $\\lambda$, $\\mu$, and $\\sigma_x^{2}$. Identify the equilibrium states (fixed points) of this ODE and classify their local stability. Then, under the parameter regime in which a nonzero stable equilibrium exists, compute the magnitude of the nonzero stable fixed point of $w(t)$ as a closed-form expression in $\\eta$, $\\lambda$, $\\mu$, and $\\sigma_x^{2}$. Express your final answer as a single simplified analytic expression. No numerical approximation is required. Define all acronyms at first use; for example, use Ordinary Differential Equation (ODE) on first mention. The final answer must be a single closed-form analytic expression without units.",
            "solution": "The problem requires the derivation and analysis of a mean-field model for synaptic weight dynamics under a specific form of Hebbian learning. The analysis will proceed in three stages: first, constructing the full stochastic differential equation for the synaptic efficacy, $w(t)$; second, applying a mean-field approximation to derive a deterministic Ordinary Differential Equation (ODE); and third, finding the fixed points of this ODE, analyzing their stability, and determining the magnitude of the stable, nonzero equilibrium.\n\nFirst, we formulate the equation for the rate of change of the synaptic efficacy, $\\frac{dw}{dt}$. The problem states three contributing factors:\n1. A Hebbian learning term, where the synaptic modification is proportional to the product of presynaptic and postsynaptic activities, $x(t)y(t)$. The proportionality constant is $\\eta$. The postsynaptic activity is given by the linear response model $y(t) \\approx w(t)x(t)$. Thus, the Hebbian term is $\\eta x(t) y(t) = \\eta x(t) (w(t)x(t)) = \\eta w(t) x^{2}(t)$. Since $\\eta > 0$, this term drives potentiation.\n\n2. A linear decay term representing weight leakage, with a rate constant $\\lambda > 0$. This term is proportional to the weight itself and acts to decrease its magnitude. Hence, it is expressed as $-\\lambda w(t)$.\n\n3. A cubic saturating term representing resource limitations, with a coefficient $\\mu > 0$. This term prevents unbounded growth of the weight and is given as $-\\mu w^{3}(t)$. The cubic form is a common choice for modeling nonlinear saturation effects in synaptic plasticity.\n\nCombining these terms, we arrive at the stochastic differential equation governing the evolution of $w(t)$:\n$$\n\\frac{dw(t)}{dt} = \\eta w(t) x^{2}(t) - \\lambda w(t) - \\mu w^{3}(t)\n$$\nThis equation is stochastic because of the presence of the fluctuating presynaptic signal $x(t)$.\n\nNext, we perform a mean-field analysis. The problem states a separation of time scales, where the synaptic weight $w(t)$ evolves much more slowly than the presynaptic signal $x(t)$. This assumption allows us to average the stochastic equation over the fast fluctuations of $x(t)$ to obtain a deterministic Ordinary Differential Equation (ODE) for the slow evolution of $w(t)$. In this averaging process, the slow variable $w(t)$ is treated as a constant. We denote the averaging operation by $\\langle \\cdot \\rangle$. Applying this to our equation:\n$$\n\\frac{dw}{dt} = \\left\\langle \\eta w x^{2} - \\lambda w - \\mu w^{3} \\right\\rangle\n$$\nUsing the linearity of the averaging operator and treating $w$ as quasi-static:\n$$\n\\frac{dw}{dt} = \\eta w \\langle x^{2}(t) \\rangle - \\lambda w \\langle 1 \\rangle - \\mu w^{3} \\langle 1 \\rangle\n$$\n$$\n\\frac{dw}{dt} = \\eta w \\langle x^{2}(t) \\rangle - \\lambda w - \\mu w^{3}\n$$\nThe presynaptic signal $x(t)$ is a zero-mean, stationary, ergodic stochastic process. Its variance is defined as $\\sigma_x^{2} = \\lim_{T \\to \\infty} \\frac{1}{T}\\int_{0}^{T} x^{2}(t)\\,dt$. Due to ergodicity, the time average $\\langle x^{2}(t) \\rangle$ is equal to the ensemble average (or expectation) $\\mathbb{E}[x^{2}(t)]$. Since the mean is zero, the variance is equal to the second moment: $\\sigma_x^{2} = \\mathbb{E}[x^{2}(t)] - (\\mathbb{E}[x(t)])^{2} = \\mathbb{E}[x^{2}(t)] - 0^{2} = \\mathbb{E}[x^{2}(t)]$. Therefore, $\\langle x^{2}(t) \\rangle = \\sigma_x^{2}$.\n\nSubstituting this into the equation, we obtain the deterministic mean-field ODE for the evolution of $w(t)$:\n$$\n\\frac{dw}{dt} = \\eta w \\sigma_x^{2} - \\lambda w - \\mu w^{3}\n$$\n$$\n\\frac{dw}{dt} = (\\eta \\sigma_x^{2} - \\lambda) w - \\mu w^{3}\n$$\nLet us define the right-hand side as $f(w) = (\\eta \\sigma_x^{2} - \\lambda) w - \\mu w^{3}$.\n\nThe equilibrium states, or fixed points $w^*$, of this ODE are found by setting $\\frac{dw}{dt} = 0$:\n$$\nf(w^*) = (\\eta \\sigma_x^{2} - \\lambda) w^* - \\mu (w^*)^{3} = 0\n$$\nFactoring out $w^*$:\n$$\nw^* \\left[ (\\eta \\sigma_x^{2} - \\lambda) - \\mu (w^*)^{2} \\right] = 0\n$$\nThis equation yields the following solutions for $w^*$:\n1. $w_{0}^* = 0$\n2. $(\\eta \\sigma_x^{2} - \\lambda) - \\mu (w^*)^{2} = 0 \\implies (w^*)^{2} = \\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}$\n\nThe second case gives real, nonzero fixed points only if the right-hand side is positive, i.e., if $\\eta \\sigma_x^{2} - \\lambda > 0$. If this condition holds, there are two symmetric nonzero fixed points:\n$$\nw_{\\pm}^* = \\pm \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}\n$$\nTo classify the local stability of these fixed points, we analyze the sign of the derivative $f'(w) = \\frac{df}{dw}$ at each fixed point. A fixed point $w^*$ is locally stable if $f'(w^*) < 0$ and unstable if $f'(w^*) > 0$.\nThe derivative is:\n$$\nf'(w) = \\frac{d}{dw} \\left[ (\\eta \\sigma_x^{2} - \\lambda) w - \\mu w^{3} \\right] = (\\eta \\sigma_x^{2} - \\lambda) - 3\\mu w^{2}\n$$\nNow, we evaluate $f'(w)$ at the fixed points.\n- For $w_{0}^* = 0$:\n  $f'(0) = (\\eta \\sigma_x^{2} - \\lambda) - 3\\mu (0)^{2} = \\eta \\sigma_x^{2} - \\lambda$.\n  The stability of the origin depends on the sign of this term. If $\\eta \\sigma_x^{2} < \\lambda$, $f'(0) < 0$ and the origin is stable. If $\\eta \\sigma_x^{2} > \\lambda$, $f'(0) > 0$ and the origin is unstable.\n\n- For $w_{\\pm}^* = \\pm \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}$:\n  These fixed points exist only in the regime where $\\eta \\sigma_x^{2} > \\lambda$. Let's analyze their stability in this regime. Note that $f'(w)$ depends on $w^{2}$, so the stability is the same for both $w_{+}^*$ and $w_{-}^*$.\n  Substitute $(w_{\\pm}^*)^{2} = \\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}$ into the expression for $f'(w)$:\n  $$\n  f'(w_{\\pm}^*) = (\\eta \\sigma_x^{2} - \\lambda) - 3\\mu \\left( \\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu} \\right)\n  $$\n  $$\n  f'(w_{\\pm}^*) = (\\eta \\sigma_x^{2} - \\lambda) - 3(\\eta \\sigma_x^{2} - \\lambda) = -2(\\eta \\sigma_x^{2} - \\lambda)\n  $$\n  In the regime where these nonzero fixed points exist, we have $\\eta \\sigma_x^{2} - \\lambda > 0$. Therefore, $f'(w_{\\pm}^*) < 0$. This indicates that both nonzero fixed points, $w_{+}^*$ and $w_{-}^*$, are locally stable whenever they exist.\n\nThe problem asks for the magnitude of the nonzero stable fixed point in the parameter regime where it exists. This regime is $\\eta \\sigma_x^{2} > \\lambda$. The nonzero stable fixed points are $w_{\\pm}^* = \\pm \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}$.\nThe magnitude is the absolute value:\n$$\n|w_{\\pm}^*| = \\left| \\pm \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}} \\right| = \\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}\n$$\nThis is the final closed-form expression.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{\\eta \\sigma_x^{2} - \\lambda}{\\mu}}}\n$$"
        },
        {
            "introduction": "Hebbian principles are not just about strengthening individual connections; they are about extracting meaningful structure from the environment. This practice expands our scope from a single scalar weight to a neuron with a vector of weights learning from multi-dimensional input streams . You will analyze Oja's rule, a classic normalized Hebbian algorithm, and discover its remarkable emergent computational function. This exercise will guide you to prove that this simple, local rule enables a neuron to perform Principal Component Analysis (PCA), effectively learning to detect the most significant feature in its input data.",
            "id": "4046811",
            "problem": "Consider a single linear neuron in a neuromorphic system whose synaptic weight vector $w(t) \\in \\mathbb{R}^{2}$ is adapted from streaming inputs according to a local activity-dependent rule motivated by the Hebbian learning principle (“cells that fire together wire together”) and stabilized by a local normalizer. The neuron receives inputs $x(t) \\in \\mathbb{R}^{2}$ drawn as an Independent and Identically Distributed (i.i.d.) zero-mean stationary random vector with covariance matrix $C = \\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}$, where $a > 0$, $d > 0$, and $ad - b^{2} > 0$. The instantaneous output is $y(t) = w(t)^{\\top} x(t)$, and the online adaptation at time $t$ is given by $\\Delta w(t) = \\eta \\, y(t) \\big(x(t) - y(t) w(t)\\big)$ for a small learning rate $\\eta > 0$.\n\nStarting only from the Hebbian learning principle and the provided adaptation rule, treat the learning process in the mean-field continuous-time limit under the i.i.d. and stationarity assumptions. Derive the expected dynamical system for $w(t)$ in terms of the moments of $x(t)$, analyze its fixed points and their stability, and determine the limiting expected output power $\\mathbb{E}[y(t)^{2}]$ at convergence. Express your final answer as a single closed-form analytic expression in terms of $a$, $b$, and $d$. No units are required.",
            "solution": "The problem asks for the limiting expected output power of a single linear neuron whose synaptic weight vector $w(t)$ adapts according to a specific Hebbian-type rule. The solution requires analyzing the dynamics of the weight vector in the mean-field continuous-time limit.\n\nFirst, we are given the discrete-time update rule for the weight vector $w(t) \\in \\mathbb{R}^{2}$:\n$$ \\Delta w(t) = w(t+1) - w(t) = \\eta \\, y(t) \\big(x(t) - y(t) w(t)\\big) $$\nwhere $y(t) = w(t)^{\\top} x(t)$ is the neuron's output, $x(t) \\in \\mathbb{R}^{2}$ is the input vector, and $\\eta > 0$ is a small learning rate.\n\nTo derive the expected dynamical system in the continuous-time limit, we treat $w$ as a slowly varying variable (due to small $\\eta$) and average the update rule over the stationary distribution of the fast-varying input $x(t)$. The change $\\Delta w(t)$ is approximated by a differential $\\frac{dw}{dt}$, scaled by $\\eta$.\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ y(x) \\big(x - y(x) w \\big) \\right] $$\nwhere the expectation $\\mathbb{E}_{x}[\\cdot]$ is taken over the distribution of the input vector $x$. The weight vector $w$ is held constant inside the expectation. Substituting $y(x) = w^{\\top}x$:\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ (w^{\\top}x) \\left(x - (w^{\\top}x)w \\right) \\right] $$\nUsing the linearity of the expectation operator, we can separate the terms:\n$$ \\frac{dw}{dt} = \\mathbb{E}_{x} \\left[ (w^{\\top}x)x \\right] - \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 w \\right] $$\nLet's analyze each term. For the first term:\n$$ \\mathbb{E}_{x} \\left[ (w^{\\top}x)x \\right] = \\mathbb{E}_{x} \\left[ x(x^{\\top}w) \\right] = \\mathbb{E}_{x} \\left[ xx^{\\top} \\right] w $$\nThe problem states that the input $x$ is a zero-mean random vector with covariance matrix $C = \\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}$. By definition, $C = \\mathbb{E}[ (x-\\mathbb{E}[x])(x-\\mathbb{E}[x])^{\\top} ]$. Since $\\mathbb{E}[x] = 0$, the covariance matrix is $C = \\mathbb{E}[xx^{\\top}]$. Therefore, the first term is $Cw$.\n\nFor the second term:\n$$ \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 w \\right] = w \\, \\mathbb{E}_{x} \\left[ (w^{\\top}x)^2 \\right] = w \\, \\mathbb{E}_{x} \\left[ (w^{\\top}x)(x^{\\top}w) \\right] $$\n$$ = w \\, w^{\\top} \\mathbb{E}_{x} \\left[ xx^{\\top} \\right] w = w (w^{\\top}Cw) $$\nCombining the terms, we obtain the mean-field continuous-time dynamical system for $w(t)$:\n$$ \\frac{dw}{dt} = \\eta \\left( Cw - (w^{\\top}Cw)w \\right) $$\nThis is a form of Oja's rule for a single neuron.\n\nNext, we analyze the fixed points $w^*$ of this system by setting $\\frac{dw}{dt} = 0$:\n$$ Cw^* - (w^{*\\top}Cw^*)w^* = 0 $$\nThis equation implies that $Cw^*$ must be collinear with $w^*$. This means that any non-zero fixed point $w^*$ must be an eigenvector of the matrix $C$. Let $w^*$ be an eigenvector of $C$ with a corresponding eigenvalue $\\lambda$, so that $Cw^* = \\lambda w^*$. Substituting this into the fixed point equation:\n$$ \\lambda w^* - (w^{*\\top}(\\lambda w^*)) w^* = 0 $$\n$$ \\lambda w^* - \\lambda (w^{*\\top}w^*) w^* = 0 $$\n$$ \\lambda w^* (1 - \\|w^*\\|^2) = 0 $$\nThis equation admits two kinds of solutions:\n$1$. $w^* = 0$, the trivial fixed point.\n$2$. If $\\lambda \\neq 0$, then $\\|w^*\\|^2 = 1$. The non-trivial fixed points are the unit-norm eigenvectors of the covariance matrix $C$.\n\nThe stability of these fixed points determines the long-term behavior of $w(t)$.\nFor the trivial fixed point $w^*=0$, we linearize the system around $w=0$. The term $(w^{\\top}Cw)w$ is of order $\\|w\\|^3$ and can be neglected for small $\\|w\\|$. The linearized system is $\\frac{dw}{dt} \\approx \\eta Cw$. The given conditions $a>0, d>0$, and $ad-b^2>0$ ensure that $C$ is a positive definite matrix. All its eigenvalues are strictly positive. Thus, any small perturbation from $w=0$ will grow, making the fixed point $w^*=0$ unstable.\n\nFor the non-trivial fixed points $w^*=e_i$, where $e_i$ are the normalized eigenvectors of $C$, the weight vector converges to the direction of the eigenvector associated with the largest eigenvalue, $\\lambda_1$. This is a standard result for Oja's rule. The dynamics project the vector $w$ onto the principal subspace of $C$ while normalizing its length to $1$. The stable fixed points are $w_{ss} = \\pm e_1$, where $e_1$ is the normalized eigenvector corresponding to the largest eigenvalue $\\lambda_1$ of $C$. All other eigenvector fixed points are unstable (saddle points).\n\nThe problem asks for the limiting expected output power $\\mathbb{E}[y(t)^2]$ at convergence. At convergence ($t \\to \\infty$), the weight vector $w(t)$ approaches a stable fixed point, $w_{ss}$. The expected output power is calculated by averaging over the input distribution, with the weight vector fixed at its steady-state value $w_{ss}$:\n$$ \\lim_{t\\to\\infty} \\mathbb{E}_{x}[y(t)^2] = \\mathbb{E}_{x}[(w_{ss}^{\\top}x)^2] = \\mathbb{E}_{x}[w_{ss}^{\\top}xx^{\\top}w_{ss}] $$\nPulling the constant vector $w_{ss}$ out of the expectation:\n$$ = w_{ss}^{\\top} \\mathbb{E}_{x}[xx^{\\top}] w_{ss} = w_{ss}^{\\top} C w_{ss} $$\nSince $w_{ss}$ is a normalized eigenvector $e_1$ (or $-e_1$) corresponding to the largest eigenvalue $\\lambda_1$, we have $Ce_1 = \\lambda_1 e_1$ and $\\|e_1\\|^2 = 1$.\n$$ w_{ss}^{\\top} C w_{ss} = (\\pm e_1)^{\\top} C (\\pm e_1) = e_1^{\\top} C e_1 = e_1^{\\top}(\\lambda_1 e_1) = \\lambda_1(e_1^{\\top}e_1) = \\lambda_1 $$\nThus, the limiting expected output power is the largest eigenvalue of the input covariance matrix $C$.\n\nFinally, we calculate the eigenvalues of $C = \\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(C - \\lambda I) = 0$:\n$$ \\det \\begin{pmatrix} a-\\lambda & b \\\\ b & d-\\lambda \\end{pmatrix} = (a-\\lambda)(d-\\lambda) - b^2 = 0 $$\n$$ \\lambda^2 - (a+d)\\lambda + ad - b^2 = 0 $$\nUsing the quadratic formula to solve for $\\lambda$:\n$$ \\lambda = \\frac{-(-(a+d)) \\pm \\sqrt{(-(a+d))^2 - 4(1)(ad-b^2)}}{2(1)} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{a^2 + 2ad + d^2 - 4ad + 4b^2}}{2} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{a^2 - 2ad + d^2 + 4b^2}}{2} $$\n$$ \\lambda = \\frac{a+d \\pm \\sqrt{(a-d)^2 + 4b^2}}{2} $$\nThe two eigenvalues are $\\lambda_1 = \\frac{a+d + \\sqrt{(a-d)^2 + 4b^2}}{2}$ and $\\lambda_2 = \\frac{a+d - \\sqrt{(a-d)^2 + 4b^2}}{2}$.\nThe term $\\sqrt{(a-d)^2 + 4b^2}$ is always non-negative, so $\\lambda_1$ is the larger of the two eigenvalues.\nThe limiting expected output power is $\\lambda_1$.",
            "answer": "$$ \\boxed{\\frac{a+d + \\sqrt{(a-d)^2 + 4b^2}}{2}} $$"
        }
    ]
}