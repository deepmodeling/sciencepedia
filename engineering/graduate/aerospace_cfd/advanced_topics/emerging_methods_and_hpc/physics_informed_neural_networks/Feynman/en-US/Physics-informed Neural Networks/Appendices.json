{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of any Physics-Informed Neural Network is its loss function, which translates the governing principles of a physical system into a form that a neural network can learn. This first practice establishes that fundamental concept. You will construct the composite loss function for the classic Poisson's equation, a ubiquitous elliptic partial differential equation in fields from electrostatics to fluid dynamics, demonstrating how to combine residuals from the governing PDE and the boundary conditions into a single objective for optimization .",
            "id": "2126324",
            "problem": "A researcher is building a Physics-Informed Neural Network (PINN) to find an approximate solution for the electrostatic potential, $V(x,y)$, within a two-dimensional square region. The physical behavior of the potential is described by the Poisson equation:\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\nwhere $f(x,y)$ represents a given charge distribution density and $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplace operator. The potential is defined over the domain $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$. The boundary of this domain, $\\partial D$, is held at a zero potential (grounded), which imposes the boundary condition $V(x,y) = 0$ for all $(x,y) \\in \\partial D$.\n\nThe PINN model, denoted by $\\hat{V}(x,y; \\theta)$, learns to approximate $V(x,y)$ by minimizing a loss function $L(\\theta)$ that incorporates the physics of the problem. Here, $\\theta$ represents all the trainable parameters of the neural network. The loss function is calculated using two sets of discrete points:\n1.  A set of $N_{pde}$ collocation points, $S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$, located in the interior of the domain $D$.\n2.  A set of $N_{bc}$ boundary points, $S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$, located on the boundary $\\partial D$.\n\nThe total loss function, $L(\\theta)$, is the sum of two mean squared error terms: one for the governing partial differential equation ($L_{pde}$) and one for the boundary conditions ($L_{bc}$).\n\nConstruct the mathematical expression for the total loss function $L(\\theta) = L_{pde} + L_{bc}$. Your expression should be in terms of the network's output $\\hat{V}$, its second partial derivatives, the function $f$, the given point sets, and their respective sizes $N_{pde}$ and $N_{bc}$.",
            "solution": "We begin from the governing Poisson equation and boundary condition:\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\nA Physics-Informed Neural Network approximates $V$ by $\\hat{V}(x,y;\\theta)$. The PDE residual at an interior collocation point $(x_{i},y_{i})\\in S_{pde}$ is defined by imposing the Poisson equation on $\\hat{V}$:\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nUsing the definition of the Laplacian in two dimensions, this is equivalently\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nThe mean squared error enforcing the PDE over $S_{pde}$ is then\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\nThe boundary condition $V=0$ on $\\partial D$ is enforced by penalizing the deviation of $\\hat{V}$ from zero at boundary points $(x_{j},y_{j})\\in S_{bc}$:\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\nTherefore, the total loss is the sum of the two mean squared error terms:\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$"
        },
        {
            "introduction": "Building upon the static case, we now advance to dynamic systems where variables evolve in time. This exercise demonstrates how to adapt the PINN framework to a time-dependent hyperbolic PDE, the 1D acoustic wave equation. You will formulate a loss function that incorporates not only the PDE and spatial boundary conditions (including both Dirichlet and Neumann types), but also the crucial initial conditions that define the system's state at the start of its evolution . This practice illustrates the versatility of PINNs in modeling transient phenomena.",
            "id": "2126356",
            "problem": "A Physics-Informed Neural Network (PINN) is being developed to model the acoustic pressure wave, $p(x,t)$, within a narrow pipe of length $L$. The pressure fluctuation $p(x,t)$ is governed by the 1D acoustic wave equation:\n$$\n\\frac{\\partial^2 p}{\\partial t^2} = c^2 \\frac{\\partial^2 p}{\\partial x^2}\n$$\nfor $x \\in (0, L)$ and $t \\in (0, T]$, where $c$ is the constant speed of sound in the fluid.\n\nThe boundary and initial conditions for the system are as follows:\n1.  At the end $x=0$, a piston oscillates, creating a sinusoidal pressure fluctuation: $p(0, t) = P_0 \\sin(\\omega t)$.\n2.  The end at $x=L$ is closed and rigid, which imposes a zero-gradient condition on the pressure: $\\frac{\\partial p}{\\partial x}(L, t) = 0$.\n3.  The fluid in the pipe is initially at a uniform quiescent pressure: $p(x, 0) = 0$.\n4.  The fluid is also initially at rest, which means the initial rate of change of pressure is zero: $\\frac{\\partial p}{\\partial t}(x, 0) = 0$.\n\nThe PINN approximates the solution using a neural network with parameters $\\theta$, denoted as $\\hat{p}(x, t; \\theta)$. The network is trained by minimizing a loss function $\\mathcal{L}(\\theta)$ that enforces the governing PDE and the associated conditions. This is achieved by evaluating the residuals of the equations on sets of collocation points.\n\nThe training data consists of four distinct sets of collocation points:\n-   A set of $N_f$ points for the PDE residual, $\\mathcal{S}_f = \\{(x_i^f, t_i^f)\\}_{i=1}^{N_f}$, sampled from the domain interior $(0, L) \\times (0, T]$.\n-   A set of $N_{b0}$ points for the boundary at $x=0$, $\\mathcal{S}_{b0} = \\{(0, t_j^{b0})\\}_{j=1}^{N_{b0}}$, sampled over $t \\in (0, T]$.\n-   A set of $N_{bL}$ points for the boundary at $x=L$, $\\mathcal{S}_{bL} = \\{(L, t_k^{bL})\\}_{k=1}^{N_{bL}}$, sampled over $t \\in (0, T]$.\n-   A set of $N_i$ points for the initial conditions, $\\mathcal{S}_i = \\{(x_l^i, 0)\\}_{l=1}^{N_i}$, sampled over $x \\in [0, L]$.\n\nYour task is to write down the complete mathematical expression for the total loss function $\\mathcal{L}(\\theta)$ used to train this PINN. The loss should be constructed as a sum of the mean squared errors of the residuals corresponding to the governing PDE, the two boundary conditions, and the two initial conditions.",
            "solution": "We construct the physics-informed residuals for the governing PDE, boundary conditions, and initial conditions, and then define the total loss as the sum of mean squared errors over the respective collocation sets.\n\nFirst, define the PDE residual at interior points $(x_{i}^{f}, t_{i}^{f}) \\in \\mathcal{S}_{f}$ by substituting the network approximation $\\hat{p}(x,t;\\theta)$ into the wave equation:\n$$\nr_{f}(x,t;\\theta) \\equiv \\frac{\\partial^{2}\\hat{p}}{\\partial t^{2}}(x,t;\\theta) - c^{2}\\frac{\\partial^{2}\\hat{p}}{\\partial x^{2}}(x,t;\\theta).\n$$\nIts mean squared error over $\\mathcal{S}_{f}$ is\n$$\n\\frac{1}{N_{f}}\\sum_{i=1}^{N_{f}}\\left[r_{f}\\!\\left(x_{i}^{f}, t_{i}^{f}; \\theta\\right)\\right]^{2}\n= \\frac{1}{N_{f}}\\sum_{i=1}^{N_{f}}\\left(\\frac{\\partial^{2}\\hat{p}}{\\partial t^{2}}(x_{i}^{f}, t_{i}^{f}; \\theta) - c^{2}\\frac{\\partial^{2}\\hat{p}}{\\partial x^{2}}(x_{i}^{f}, t_{i}^{f}; \\theta)\\right)^{2}.\n$$\n\nNext, enforce the Dirichlet boundary condition at $x=0$ using points $(0, t_{j}^{b0}) \\in \\mathcal{S}_{b0}$:\n$$\nr_{b0}(t;\\theta) \\equiv \\hat{p}(0,t;\\theta) - P_{0}\\sin(\\omega t),\n$$\nwith mean squared error\n$$\n\\frac{1}{N_{b0}}\\sum_{j=1}^{N_{b0}}\\left[r_{b0}\\!\\left(t_{j}^{b0}; \\theta\\right)\\right]^{2}\n= \\frac{1}{N_{b0}}\\sum_{j=1}^{N_{b0}}\\left(\\hat{p}(0, t_{j}^{b0}; \\theta) - P_{0}\\sin(\\omega t_{j}^{b0})\\right)^{2}.\n$$\n\nFor the Neumann boundary condition at $x=L$ using points $(L, t_{k}^{bL}) \\in \\mathcal{S}_{bL}$, define\n$$\nr_{bL}(t;\\theta) \\equiv \\frac{\\partial \\hat{p}}{\\partial x}(L,t;\\theta),\n$$\nwith mean squared error\n$$\n\\frac{1}{N_{bL}}\\sum_{k=1}^{N_{bL}}\\left[r_{bL}\\!\\left(t_{k}^{bL}; \\theta\\right)\\right]^{2}\n= \\frac{1}{N_{bL}}\\sum_{k=1}^{N_{bL}}\\left(\\frac{\\partial \\hat{p}}{\\partial x}(L, t_{k}^{bL}; \\theta)\\right)^{2}.\n$$\n\nFor the initial conditions at $t=0$ using points $(x_{l}^{i}, 0) \\in \\mathcal{S}_{i}$, define residuals for $p(x,0)=0$ and $\\partial_{t}p(x,0)=0$:\n$$\nr_{i,p}(x;\\theta) \\equiv \\hat{p}(x,0;\\theta), \\qquad\nr_{i,t}(x;\\theta) \\equiv \\frac{\\partial \\hat{p}}{\\partial t}(x,0;\\theta),\n$$\nwith mean squared errors\n$$\n\\frac{1}{N_{i}}\\sum_{l=1}^{N_{i}}\\left[r_{i,p}\\!\\left(x_{l}^{i}; \\theta\\right)\\right]^{2}\n= \\frac{1}{N_{i}}\\sum_{l=1}^{N_{i}}\\left(\\hat{p}(x_{l}^{i}, 0; \\theta)\\right)^{2},\n$$\n$$\n\\frac{1}{N_{i}}\\sum_{l=1}^{N_{i}}\\left[r_{i,t}\\!\\left(x_{l}^{i}; \\theta\\right)\\right]^{2}\n= \\frac{1}{N_{i}}\\sum_{l=1}^{N_{i}}\\left(\\frac{\\partial \\hat{p}}{\\partial t}(x_{l}^{i}, 0; \\theta)\\right)^{2}.\n$$\n\nSumming these five mean squared error terms yields the total loss function:\n$$\n\\mathcal{L}(\\theta) =\n\\frac{1}{N_{f}}\\sum_{i=1}^{N_{f}}\\left(\\frac{\\partial^{2}\\hat{p}}{\\partial t^{2}}(x_{i}^{f}, t_{i}^{f}; \\theta) - c^{2}\\frac{\\partial^{2}\\hat{p}}{\\partial x^{2}}(x_{i}^{f}, t_{i}^{f}; \\theta)\\right)^{2}\n+ \\frac{1}{N_{b0}}\\sum_{j=1}^{N_{b0}}\\left(\\hat{p}(0, t_{j}^{b0}; \\theta) - P_{0}\\sin(\\omega t_{j}^{b0})\\right)^{2}\n+ \\frac{1}{N_{bL}}\\sum_{k=1}^{N_{bL}}\\left(\\frac{\\partial \\hat{p}}{\\partial x}(L, t_{k}^{bL}; \\theta)\\right)^{2}\n+ \\frac{1}{N_{i}}\\sum_{l=1}^{N_{i}}\\left(\\hat{p}(x_{l}^{i}, 0; \\theta)\\right)^{2}\n+ \\frac{1}{N_{i}}\\sum_{l=1}^{N_{i}}\\left(\\frac{\\partial \\hat{p}}{\\partial t}(x_{l}^{i}, 0; \\theta)\\right)^{2}.\n$$",
            "answer": "$$\\boxed{\\mathcal{L}(\\theta) =\n\\frac{1}{N_{f}}\\sum_{i=1}^{N_{f}}\\left(\\frac{\\partial^{2}\\hat{p}}{\\partial t^{2}}(x_{i}^{f}, t_{i}^{f}; \\theta) - c^{2}\\frac{\\partial^{2}\\hat{p}}{\\partial x^{2}}(x_{i}^{f}, t_{i}^{f}; \\theta)\\right)^{2}\n+ \\frac{1}{N_{b0}}\\sum_{j=1}^{N_{b0}}\\left(\\hat{p}(0, t_{j}^{b0}; \\theta) - P_{0}\\sin(\\omega t_{j}^{b0})\\right)^{2}\n+ \\frac{1}{N_{bL}}\\sum_{k=1}^{N_{bL}}\\left(\\frac{\\partial \\hat{p}}{\\partial x}(L, t_{k}^{bL}; \\theta)\\right)^{2}\n+ \\frac{1}{N_{i}}\\sum_{l=1}^{N_{i}}\\left(\\hat{p}(x_{l}^{i}, 0; \\theta)\\right)^{2}\n+ \\frac{1}{N_{i}}\\sum_{l=1}^{N_{i}}\\left(\\frac{\\partial \\hat{p}}{\\partial t}(x_{l}^{i}, 0; \\theta)\\right)^{2}}$$"
        },
        {
            "introduction": "One of the most powerful applications of PINNs is their ability to solve inverse problems, which are often ill-posed and notoriously difficult for traditional solvers. This final practice challenges you to design a PINN for the backward heat equation, a classic ill-posed problem where the goal is to infer a system's past states from final-time data. You will learn how to augment the standard loss function with a custom regularization term to counteract the explosive instabilities inherent to the problem, thereby stabilizing the learning process and enabling a physically plausible solution .",
            "id": "2126308",
            "problem": "A research team is developing a Physics-Informed Neural Network (PINN) to tackle the notoriously ill-posed one-dimensional backward heat equation. This equation models diffusion processes in reverse time and is highly sensitive to noise in the final-state data, often leading to non-physical, explosive solutions.\n\nThe governing Partial Differential Equation (PDE) is:\n$$ \\frac{\\partial u}{\\partial t} + \\alpha \\frac{\\partial^2 u}{\\partial x^2} = 0 $$\nfor a function $u(x, t)$ on the spatio-temporal domain $(x, t) \\in [-L, L] \\times [0, T]$. Here, $\\alpha > 0$ is the thermal diffusivity. The problem is \"backward\" because we are given data at the final time $T$ and must infer the solution for all $t < T$.\n\nThe PINN approximates the solution with a neural network $\\hat{u}(x, t; \\theta)$, where $\\theta$ represents the trainable network parameters. The training process minimizes a composite loss function $L(\\theta)$. This loss function is a weighted sum of four distinct terms:\n$L(\\theta) = \\lambda_d L_{data} + \\lambda_f L_{pde} + \\lambda_b L_{bc} + \\lambda_r L_{reg}$.\n\nYour task is to construct the full analytical expression for $L(\\theta)$ based on the following specifications:\n\n1.  **Data Fidelity Loss ($L_{data}$):** This term measures the mismatch with the available data. A set of $N_d$ noisy measurements of the final state, $\\{ (x_i, u_i) \\}_{i=1}^{N_d}$, is provided, where $u_i \\approx u(x_i, T)$. Use the Mean Squared Error (MSE) for this loss component.\n\n2.  **PDE Residual Loss ($L_{pde}$):** This term enforces the governing physics. It is the MSE of the PDE residual, evaluated at a set of $N_f$ collocation points $\\{ (x_j^{(f)}, t_j^{(f)}) \\}_{j=1}^{N_f}$ sampled from the interior of the domain $[-L, L] \\times [0, T]$.\n\n3.  **Boundary Condition Loss ($L_{bc}$):** The system has periodic boundary conditions: $u(-L, t) = u(L, t)$ and $\\frac{\\partial u}{\\partial x}(-L, t) = \\frac{\\partial u}{\\partial x}(L, t)$. This loss is the cumulative MSE of the residuals for both periodic conditions, evaluated at a set of $N_b$ temporal points $\\{ t_k^{(b)} \\}_{k=1}^{N_b}$ on the boundary interval $[0, T]$.\n\n4.  **Regularization Loss ($L_{reg}$):** To counteract the explosive instabilities of the backward problem, a regularization term is added. This term penalizes the overall magnitude of the solution across the entire spatio-temporal domain. It is defined as the total \"energy\" of the field, which is the integral of the energy density $\\hat{u}(x, t; \\theta)^2$ over the full domain $(x, t) \\in [-L, L] \\times [0, T]$.\n\nUsing the defined parameters and sets of points, write the complete mathematical expression for the total loss function $L(\\theta)$. Your expression should explicitly show all sums and integrals involved.",
            "solution": "We are given the backward heat equation\n$$\n\\frac{\\partial u}{\\partial t}(x,t)+\\alpha \\frac{\\partial^{2} u}{\\partial x^{2}}(x,t)=0,\n$$\non $(x,t)\\in[-L,L]\\times[0,T]$, and we approximate the solution by a neural network $\\hat{u}(x,t;\\theta)$. The total loss is a weighted sum\n$$\nL(\\theta)=\\lambda_{d}L_{data}+\\lambda_{f}L_{pde}+\\lambda_{b}L_{bc}+\\lambda_{r}L_{reg}.\n$$\nWe now derive each term explicitly.\n\nData fidelity loss at the final time $T$ with $N_{d}$ measurements $\\{(x_{i},u_{i})\\}_{i=1}^{N_{d}}$ uses the mean squared error:\n$$\nL_{data}=\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}.\n$$\n\nThe PDE residual for the heat equation is\n$$\nr(x,t;\\theta)=\\frac{\\partial \\hat{u}}{\\partial t}(x,t;\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x,t;\\theta).\n$$\nEvaluated at $N_{f}$ interior collocation points $\\{(x_{j}^{(f)},t_{j}^{(f)})\\}_{j=1}^{N_{f}}$, the residual loss is the MSE\n$$\nL_{pde}=\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(r(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}\n=\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}.\n$$\n\nThe periodic boundary conditions are $u(-L,t)=u(L,t)$ and $\\frac{\\partial u}{\\partial x}(-L,t)=\\frac{\\partial u}{\\partial x}(L,t)$. At $N_{b}$ temporal points $\\{t_{k}^{(b)}\\}_{k=1}^{N_{b}}$, we form the cumulative MSE of both residuals:\n$$\nL_{bc}=\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}\n+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right].\n$$\n\nThe regularization penalizes the total energy of the field over the full domain, defined as the space-time integral of $\\hat{u}^{2}$:\n$$\nL_{reg}=\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t.\n$$\n\nCombining the four terms yields the complete analytical expression for the total loss:\n$$\nL(\\theta)=\\lambda_{d}\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}\n+\\lambda_{f}\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}\n+\\lambda_{b}\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}\n+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right]\n+\\lambda_{r}\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t.\n$$\nThis expression explicitly shows all sums and integrals as required, with each term corresponding to the specified data fidelity, PDE residual, periodic boundary conditions, and energy regularization.",
            "answer": "$$\\boxed{\\lambda_{d}\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}+\\lambda_{f}\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}+\\lambda_{b}\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right]+\\lambda_{r}\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t}$$"
        }
    ]
}