{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on exploration with the foundational task of constructing a Physics-Informed Neural Network (PINN) loss function. This exercise  uses the classic Poisson's equation to demonstrate the core principle of physics-informed learning: embedding the governing differential equation and its boundary conditions directly into the objective function that the neural network minimizes. Mastering this formulation is the first step toward applying PINNs to more complex physical systems, such as those encountered in potential flow or electrostatics.",
            "id": "2126324",
            "problem": "A researcher is building a Physics-Informed Neural Network (PINN) to find an approximate solution for the electrostatic potential, $V(x,y)$, within a two-dimensional square region. The physical behavior of the potential is described by the Poisson equation:\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\nwhere $f(x,y)$ represents a given charge distribution density and $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplace operator. The potential is defined over the domain $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$. The boundary of this domain, $\\partial D$, is held at a zero potential (grounded), which imposes the boundary condition $V(x,y) = 0$ for all $(x,y) \\in \\partial D$.\n\nThe PINN model, denoted by $\\hat{V}(x,y; \\theta)$, learns to approximate $V(x,y)$ by minimizing a loss function $L(\\theta)$ that incorporates the physics of the problem. Here, $\\theta$ represents all the trainable parameters of the neural network. The loss function is calculated using two sets of discrete points:\n1.  A set of $N_{pde}$ collocation points, $S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$, located in the interior of the domain $D$.\n2.  A set of $N_{bc}$ boundary points, $S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$, located on the boundary $\\partial D$.\n\nThe total loss function, $L(\\theta)$, is the sum of two mean squared error terms: one for the governing partial differential equation ($L_{pde}$) and one for the boundary conditions ($L_{bc}$).\n\nConstruct the mathematical expression for the total loss function $L(\\theta) = L_{pde} + L_{bc}$. Your expression should be in terms of the network's output $\\hat{V}$, its second partial derivatives, the function $f$, the given point sets, and their respective sizes $N_{pde}$ and $N_{bc}$.",
            "solution": "We begin from the governing Poisson equation and boundary condition:\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\nA Physics-Informed Neural Network approximates $V$ by $\\hat{V}(x,y;\\theta)$. The PDE residual at an interior collocation point $(x_{i},y_{i})\\in S_{pde}$ is defined by imposing the Poisson equation on $\\hat{V}$:\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nUsing the definition of the Laplacian in two dimensions, this is equivalently\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nThe mean squared error enforcing the PDE over $S_{pde}$ is then\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\nThe boundary condition $V=0$ on $\\partial D$ is enforced by penalizing the deviation of $\\hat{V}$ from zero at boundary points $(x_{j},y_{j})\\in S_{bc}$:\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\nTherefore, the total loss is the sum of the two mean squared error terms:\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$"
        },
        {
            "introduction": "Building on the basics, we now tackle a significantly more challenging class of problems where PINNs truly shine: ill-posed inverse problems. This practice  focuses on the backward heat equation, a system notoriously unstable when solved with traditional methods due to its high sensitivity to noise in the final-state data. You will learn how to augment the standard PINN loss function with a regularization term to stabilize the solution, a powerful technique for data assimilation and system identification.",
            "id": "2126308",
            "problem": "A research team is developing a Physics-Informed Neural Network (PINN) to tackle the notoriously ill-posed one-dimensional backward heat equation. This equation models diffusion processes in reverse time and is highly sensitive to noise in the final-state data, often leading to non-physical, explosive solutions.\n\nThe governing Partial Differential Equation (PDE) is:\n$$ \\frac{\\partial u}{\\partial t} + \\alpha \\frac{\\partial^2 u}{\\partial x^2} = 0 $$\nfor a function $u(x, t)$ on the spatio-temporal domain $(x, t) \\in [-L, L] \\times [0, T]$. Here, $\\alpha > 0$ is the thermal diffusivity. The problem is \"backward\" because we are given data at the final time $T$ and must infer the solution for all $t < T$.\n\nThe PINN approximates the solution with a neural network $\\hat{u}(x, t; \\theta)$, where $\\theta$ represents the trainable network parameters. The training process minimizes a composite loss function $L(\\theta)$. This loss function is a weighted sum of four distinct terms:\n$L(\\theta) = \\lambda_d L_{data} + \\lambda_f L_{pde} + \\lambda_b L_{bc} + \\lambda_r L_{reg}$.\n\nYour task is to construct the full analytical expression for $L(\\theta)$ based on the following specifications:\n\n1.  **Data Fidelity Loss ($L_{data}$):** This term measures the mismatch with the available data. A set of $N_d$ noisy measurements of the final state, $\\{ (x_i, u_i) \\}_{i=1}^{N_d}$, is provided, where $u_i \\approx u(x_i, T)$. Use the Mean Squared Error (MSE) for this loss component.\n\n2.  **PDE Residual Loss ($L_{pde}$):** This term enforces the governing physics. It is the MSE of the PDE residual, evaluated at a set of $N_f$ collocation points $\\{ (x_j^{(f)}, t_j^{(f)}) \\}_{j=1}^{N_f}$ sampled from the interior of the domain $[-L, L] \\times [0, T]$.\n\n3.  **Boundary Condition Loss ($L_{bc}$):** The system has periodic boundary conditions: $u(-L, t) = u(L, t)$ and $\\frac{\\partial u}{\\partial x}(-L, t) = \\frac{\\partial u}{\\partial x}(L, t)$. This loss is the cumulative MSE of the residuals for both periodic conditions, evaluated at a set of $N_b$ temporal points $\\{ t_k^{(b)} \\}_{k=1}^{N_b}$ on the boundary interval $[0, T]$.\n\n4.  **Regularization Loss ($L_{reg}$):** To counteract the explosive instabilities of the backward problem, a regularization term is added. This term penalizes the overall magnitude of the solution across the entire spatio-temporal domain. It is defined as the total \"energy\" of the field, which is the integral of the energy density $\\hat{u}(x, t; \\theta)^2$ over the full domain $(x, t) \\in [-L, L] \\times [0, T]$.\n\nUsing the defined parameters and sets of points, write the complete mathematical expression for the total loss function $L(\\theta)$. Your expression should explicitly show all sums and integrals involved.",
            "solution": "We are given the backward heat equation\n$$\n\\frac{\\partial u}{\\partial t}(x,t)+\\alpha \\frac{\\partial^{2} u}{\\partial x^{2}}(x,t)=0,\n$$\non $(x,t)\\in[-L,L]\\times[0,T]$, and we approximate the solution by a neural network $\\hat{u}(x,t;\\theta)$. The total loss is a weighted sum\n$$\nL(\\theta)=\\lambda_{d}L_{data}+\\lambda_{f}L_{pde}+\\lambda_{b}L_{bc}+\\lambda_{r}L_{reg}.\n$$\nWe now derive each term explicitly.\n\nData fidelity loss at the final time $T$ with $N_{d}$ measurements $\\{(x_{i},u_{i})\\}_{i=1}^{N_{d}}$ uses the mean squared error:\n$$\nL_{data}=\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}.\n$$\n\nThe PDE residual for the heat equation is\n$$\nr(x,t;\\theta)=\\frac{\\partial \\hat{u}}{\\partial t}(x,t;\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x,t;\\theta).\n$$\nEvaluated at $N_{f}$ interior collocation points $\\{(x_{j}^{(f)},t_{j}^{(f)})\\}_{j=1}^{N_{f}}$, the residual loss is the MSE\n$$\nL_{pde}=\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(r(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}\n=\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}.\n$$\n\nThe periodic boundary conditions are $u(-L,t)=u(L,t)$ and $\\frac{\\partial u}{\\partial x}(-L,t)=\\frac{\\partial u}{\\partial x}(L,t)$. At $N_{b}$ temporal points $\\{t_{k}^{(b)}\\}_{k=1}^{N_{b}}$, we form the cumulative MSE of both residuals:\n$$\nL_{bc}=\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}\n+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right].\n$$\n\nThe regularization penalizes the total energy of the field over the full domain, defined as the space-time integral of $\\hat{u}^{2}$:\n$$\nL_{reg}=\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t.\n$$\n\nCombining the four terms yields the complete analytical expression for the total loss:\n$$\nL(\\theta)=\\lambda_{d}\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}\n+\\lambda_{f}\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}\n+\\lambda_{b}\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}\n+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right]\n+\\lambda_{r}\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t.\n$$\nThis expression explicitly shows all sums and integrals as required, with each term corresponding to the specified data fidelity, PDE residual, periodic boundary conditions, and energy regularization.",
            "answer": "$$\\boxed{\\lambda_{d}\\frac{1}{N_{d}}\\sum_{i=1}^{N_{d}}\\left(\\hat{u}(x_{i},T;\\theta)-u_{i}\\right)^{2}+\\lambda_{f}\\frac{1}{N_{f}}\\sum_{j=1}^{N_{f}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(x_{j}^{(f)},t_{j}^{(f)};\\theta)+\\alpha \\frac{\\partial^{2} \\hat{u}}{\\partial x^{2}}(x_{j}^{(f)},t_{j}^{(f)};\\theta)\\right)^{2}+\\lambda_{b}\\frac{1}{N_{b}}\\sum_{k=1}^{N_{b}}\\left[\\left(\\hat{u}(-L,t_{k}^{(b)};\\theta)-\\hat{u}(L,t_{k}^{(b)};\\theta)\\right)^{2}+\\left(\\frac{\\partial \\hat{u}}{\\partial x}(-L,t_{k}^{(b)};\\theta)-\\frac{\\partial \\hat{u}}{\\partial x}(L,t_{k}^{(b)};\\theta)\\right)^{2}\\right]+\\lambda_{r}\\int_{0}^{T}\\int_{-L}^{L}\\left(\\hat{u}(x,t;\\theta)\\right)^{2}\\,\\mathrm{d}x\\,\\mathrm{d}t}$$"
        },
        {
            "introduction": "While PINNs are powerful, it is crucial to understand their practical training dynamics and limitations. This exercise  provides a hands-on coding experience to investigate \"spectral bias,\" the tendency of standard neural networks to learn low-frequency solution components more readily than high-frequency ones. By training a network on a function with two distinct frequencies, you will quantitatively observe this phenomenon, gaining critical insight for tackling multi-scale problems common in aerospace CFD.",
            "id": "2427229",
            "problem": "You will implement a complete, runnable program that demonstrates the spectral bias of a Physics-Informed Neural Network (PINN). The central idea is to train a PINN to solve a one-dimensional boundary value problem whose known solution is the superposition of a low-frequency and a high-frequency sine, namely $u(x) = \\sin(x) + \\sin(25x)$, and to quantitatively observe which frequency component is learned first during training. Angles must be in radians throughout.\n\nStart from the following physically consistent ordinary differential equation (ODE) with periodic boundary conditions:\nGiven the domain $x \\in [0, 2\\pi]$, consider\n$$\nu''(x) + u(x) = -624 \\sin(25x),\n$$\nwith periodic boundary conditions\n$$\nu(0) = u(2\\pi), \\quad u'(0) = u'(2\\pi).\n$$\nIt is a well-tested fact that if $u(x) = \\sin(x) + \\sin(25x)$ then $u''(x) + u(x) = -624 \\sin(25x)$ and the periodic boundary conditions hold. You must not use any labeled training data for $u(x)$ except the boundary conditions; instead, use the ODE residual and boundary residuals in the loss, as is standard for a Physics-Informed Neural Network (PINN).\n\nConstruct a single-hidden-layer neural network $u_{\\theta}(x)$ with $H$ hidden units and hyperbolic tangent activation as the trial solution. Define the hidden pre-activations as $z_i(x) = w_i x + b_i$ for $i \\in \\{1,\\dots,H\\}$, the hidden activations as $h_i(x) = \\tanh(z_i(x))$, and the output as\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i h_i(x) + c.\n$$\nUse the chain rule and the product rule to compute the first and second derivatives of $u_{\\theta}(x)$ with respect to $x$ in closed form. Recall the standard identities for the hyperbolic tangent and its derivatives:\n$$\n\\tanh'(z) = \\operatorname{sech}^2(z), \\quad \\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z), \\quad \\operatorname{sech}^2(z) = 1 - \\tanh^2(z).\n$$\nDefine the pointwise physics residual for collocation points $\\{x_n\\}_{n=1}^{N}$ as\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) - \\left(-624 \\sin(25 x_n)\\right),\n$$\nand the periodic boundary residuals as\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi), \\quad r_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi).\n$$\nUse the mean-squared residual loss with a boundary weight $\\lambda_{\\text{bc}}$:\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right).\n$$\nTrain the parameters $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$ by gradient-based optimization from random initialization. To quantitatively assess spectral bias, at the end of a short training budget, project the learned function $u_{\\theta}(x)$ onto the two basis functions $\\sin(x)$ and $\\sin(25x)$ over a dense uniform grid on $[0, 2\\pi)$ by least squares. That is, find coefficients $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ minimizing\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2,\n$$\nwith $x_m$ uniformly spaced in $[0, 2\\pi)$. Define the learned amplitudes as $A_1 = |\\hat{\\alpha}_1|$ and $A_{25} = |\\hat{\\alpha}_{25}|$. Spectral bias is deemed present at early training if $A_1 > A_{25}$.\n\nImplement the program with a fully vectorized training loop and closed-form gradients with respect to all network parameters using only the ODE residual and boundary residuals. Do not use any external automatic differentiation library.\n\nTest Suite and Output Specification:\n- Use the following three test cases to exercise different regimes. Each case specifies $(H, N, K, \\eta)$ where $H$ is the number of hidden units, $N$ is the number of collocation points, $K$ is the number of gradient steps, and $\\eta$ is the learning rate. Use $\\lambda_{\\text{bc}} = 1$ in all cases. Angles are in radians.\n  1. Case $1$: $(H, N, K, \\eta) = (20, 128, 60, 0.01)$.\n  2. Case $2$: $(H, N, K, \\eta) = (10, 64, 80, 0.01)$.\n  3. Case $3$: $(H, N, K, \\eta) = (5, 128, 120, 0.01)$.\n- For each case, initialize parameters with a fixed seed so that results are deterministic. After training for $K$ steps, compute $A_1$ and $A_{25}$ by least squares projection over a dense grid of $M$ points with $M = 4096$. Record a boolean result for the case defined as\n$$\n\\text{result} = \\begin{cases}\n\\text{True}, & \\text{if } A_1 > A_{25},\\\\\n\\text{False}, & \\text{otherwise.}\n\\end{cases}\n$$\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,True,False]\").\n\nYour program must be self-contained, receive no input, and run as-is. Angles must be in radians. All numerical answers are dimensionless, and the final outputs are booleans. The training and projections must be implemented in pure linear algebra using the formulas above, without any external machine learning framework. The goal is to demonstrate, via these test cases, that the low-frequency component $\\sin(x)$ is learned earlier than the high-frequency component $\\sin(25x)$ by a Physics-Informed Neural Network (PINN), consistent with spectral bias.",
            "solution": "The objective is to train a neural network $u_{\\theta}(x)$ to approximate the solution of the one-dimensional ordinary differential equation (ODE)\n$$\nu''(x) + u(x) = -624 \\sin(25x)\n$$\non the domain $x \\in [0, 2\\pi]$ with periodic boundary conditions $u(0) = u(2\\pi)$ and $u'(0) = u'(2\\pi)$. The analytical solution, $u(x) = \\sin(x) + \\sin(25x)$, is a superposition of a low-frequency component and a high-frequency component. We will demonstrate that gradient-based optimization of the PINN loss causes the network to learn the low-frequency component, $\\sin(x)$, faster than the high-frequency component, $\\sin(25x)$.\n\nFirst, we define the neural network ansatz, a single-hidden-layer perceptron with $H$ neurons and $\\tanh$ activation function:\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i \\tanh(w_i x + b_i) + c\n$$\nThe parameters of the network are $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$. To enforce the ODE, we must compute the first and second derivatives of $u_{\\theta}(x)$ with respect to $x$. Using the chain rule and the identities $\\frac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ and $\\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z)$, we obtain:\n$$\nu'_{\\theta}(x) = \\frac{d u_{\\theta}}{dx} = \\sum_{i=1}^{H} a_i w_i \\operatorname{sech}^2(w_i x + b_i)\n$$\n$$\nu''_{\\theta}(x) = \\frac{d^2 u_{\\theta}}{dx^2} = -2 \\sum_{i=1}^{H} a_i w_i^2 \\operatorname{sech}^2(w_i x + b_i) \\tanh(w_i x + b_i)\n$$\n\nThe network is trained by minimizing a loss function composed of the mean squared error of the ODE residual and the boundary condition residuals. The physics residual at a set of $N$ collocation points $\\{x_n\\}$ is:\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n)\n$$\nThe periodic boundary condition residuals are:\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi)\n$$\n$$\nr_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi)\n$$\nThe total loss function is a weighted sum:\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{phys}} + \\lambda_{\\text{bc}} \\mathcal{L}_{\\text{bc}} = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right)\n$$\nwhere $\\lambda_{\\text{bc}}$ is a hyperparameter to balance the terms, given as $\\lambda_{\\text{bc}} = 1$.\n\nTraining is performed using gradient descent. The parameters are updated according to $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$, where $\\eta$ is the learning rate. We must derive the analytical gradients $\\nabla_{\\theta} \\mathcal{L}(\\theta)$. The gradient of the loss with respect to any parameter $p \\in \\theta$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{2}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n) \\left(\\frac{\\partial u''_{\\theta}(x_n)}{\\partial p} + \\frac{\\partial u_{\\theta}(x_n)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},1}\\left(\\frac{\\partial u_{\\theta}(0)}{\\partial p} - \\frac{\\partial u_{\\theta}(2\\pi)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},2}\\left(\\frac{\\partial u'_{\\theta}(0)}{\\partial p} - \\frac{\\partial u'_{\\theta}(2\\pi)}{\\partial p}\\right)\n$$\nThe derivatives of the network output and its spatial derivatives with respect to the parameters $\\{a_k, w_k, b_k, c\\}$ are computed via the chain rule. These derivations are tedious but systematic and are implemented in vectorized form for computational efficiency. For example, the gradient with respect to an output weight $a_k$ involves terms like $\\frac{\\partial u_{\\theta}(x)}{\\partial a_k} = \\tanh(w_k x + b_k)$. Complete expressions for all gradients are implemented in the code.\n\nAfter training for a specified number of steps, we quantify the learned frequency components. We evaluate the trained network $u_{\\theta}(x)$ over a dense grid of $M$ points $\\{x_m\\}$ in $[0, 2\\pi)$. We then project this learned function onto the basis functions $\\sin(x)$ and $\\sin(25x)$ by solving a linear least-squares problem to find coefficients $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ that minimize:\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2\n$$\nThe solution to this problem is given by $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{B}^T\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{y}$, where $\\mathbf{y}$ is the vector of network predictions $u_{\\theta}(x_m)$ and $\\mathbf{B}$ is the design matrix with columns $\\sin(x_m)$ and $\\sin(25x_m)$. The learned amplitudes are $A_1 = |\\hat{\\alpha}_1|$ and $A_{25} = |\\hat{\\alpha}_{25}|$. We conclude that spectral bias is observed if $A_1 > A_{25}$.\n\nThe implementation will follow these principles, using `numpy` for vectorized numerical computation, including a fully analytical gradient calculation and a standard gradient descent loop. Parameter initialization will use a fixed random seed and Glorot/Xavier scaling for reproducibility and stable training.",
            "answer": "```python\nimport numpy as np\n\nclass PINN:\n    \"\"\"\n    A Physics-Informed Neural Network to demonstrate spectral bias.\n    The implementation is fully vectorized and uses analytical gradients.\n    \"\"\"\n    def __init__(self, H, N, seed):\n        \"\"\"\n        Initializes the PINN.\n        H: number of hidden units\n        N: number of collocation points\n        seed: random seed for parameter initialization\n        \"\"\"\n        self.H = H\n        self.N = N\n        self.lambda_bc = 1.0\n        self.rng = np.random.default_rng(seed)\n\n        # Xavier/Glorot initialization\n        # For weights w, n_in=1, n_out=1 (conceptual). limit = sqrt(6 / (1+1)) = sqrt(3)\n        limit_w = np.sqrt(3.0)\n        self.w = self.rng.uniform(-limit_w, limit_w, size=(1, self.H))\n        \n        # For weights a, n_in=H, n_out=1. limit = sqrt(6 / (H+1))\n        limit_a = np.sqrt(6.0 / (self.H + 1.0))\n        self.a = self.rng.uniform(-limit_a, limit_a, size=(self.H, 1))\n\n        self.b = np.zeros((1, self.H))\n        self.c = np.zeros((1, 1))\n\n        self.x_colloc = np.linspace(0, 2 * np.pi, self.N, endpoint=False).reshape(-1, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Computes the network output u and its derivatives u', u'' w.r.t. x.\n        x: input points, shape (num_points, 1)\n        \"\"\"\n        z = x @ self.w + self.b\n        h = np.tanh(z)\n        s = 1.0 - h**2  # sech^2(z)\n\n        u = h @ self.a + self.c\n        u_prime = (s * self.w) @ self.a\n        u_double_prime = (-2.0 * s * h * (self.w**2)) @ self.a\n\n        return u, u_prime, u_double_prime, h, s\n\n    def _compute_gradients(self):\n        \"\"\"\n        Computes the loss and the gradients of the loss w.r.t. all parameters.\n        All calculations are vectorized.\n        \"\"\"\n        # --- Physics Loss and Gradients ---\n        u, _, u_pp, H_c, S_c = self.forward(self.x_colloc)\n        \n        f_term = -624.0 * np.sin(25.0 * self.x_colloc)\n        r_phys = u_pp + u - f_term\n        loss_phys = np.mean(r_phys**2)\n\n        # Common factor for physics gradients\n        grad_common_phys = (2.0 / self.N) * r_phys\n        \n        # Gradient w.r.t. a\n        d_u_da = H_c\n        d_u_pp_da = -2.0 * S_c * H_c * self.w**2\n        grad_a_phys = (d_u_da + d_u_pp_da).T @ grad_common_phys\n\n        # Gradient w.r.t. b\n        d_u_db = self.a.T * S_c\n        d_u_pp_db = self.a.T * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        grad_b_phys = np.sum(grad_common_phys * (d_u_db + d_u_pp_db), axis=0)\n\n        # Gradient w.r.t. w\n        d_u_dw = self.a.T * self.x_colloc * S_c\n        d_u_pp_dw = self.a.T * (\n            -4.0 * self.w * S_c * H_c + \n            self.x_colloc * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        )\n        grad_w_phys = np.sum(grad_common_phys * (d_u_dw + d_u_pp_dw), axis=0)\n        \n        # Gradient w.r.t. c\n        grad_c_phys = np.sum(grad_common_phys)\n\n        # --- Boundary Loss and Gradients ---\n        x_bc = np.array([[0.0], [2 * np.pi]])\n        u_bc, u_p_bc, _, H_bc, S_bc = self.forward(x_bc)\n        \n        u0, u2pi = u_bc[0], u_bc[1]\n        u0_p, u2pi_p = u_p_bc[0], u_p_bc[1]\n\n        r_bc1 = u0 - u2pi\n        r_bc2 = u0_p - u2pi_p\n        loss_bc = r_bc1**2 + r_bc2**2\n        \n        H0, H2pi = H_bc[0:1, :], H_bc[1:2, :]\n        S0, S2pi = S_bc[0:1, :], S_bc[1:2, :]\n        \n        # Common factors for BC gradients\n        common1 = 2.0 * self.lambda_bc * r_bc1\n        common2 = 2.0 * self.lambda_bc * r_bc2\n\n        # Gradient w.r.t. a\n        delta_u_da = (H0 - H2pi).T\n        delta_u_p_da = (self.w * (S0 - S2pi)).T\n        grad_a_bc = common1 * delta_u_da + common2 * delta_u_p_da\n        \n        # Gradient w.r.t. b\n        delta_u_db = self.a.T * (S0 - S2pi)\n        delta_u_p_db = -2.0 * self.a.T * self.w * (S0 * H0 - S2pi * H2pi)\n        grad_b_bc = common1 * delta_u_db + common2 * delta_u_p_db\n\n        # Gradient w.r.t. w\n        delta_u_dw = -2.0 * np.pi * self.a.T * S2pi\n        delta_u_p_dw = self.a.T * (S0 - S2pi) + 4.0 * np.pi * self.a.T * self.w * S2pi * H2pi\n        grad_w_bc = common1 * delta_u_dw + common2 * delta_u_p_dw\n\n        # Gradient w.r.t. c (is zero)\n        grad_c_bc = 0.0\n\n        # --- Total Loss and Gradients ---\n        loss = loss_phys + self.lambda_bc * loss_bc\n        grad_a = grad_a_phys + grad_a_bc\n        grad_w = grad_w_phys.reshape(1, -1) + grad_w_bc\n        grad_b = grad_b_phys.reshape(1, -1) + grad_b_bc\n        grad_c = grad_c_phys + grad_c_bc\n\n        return loss, grad_a, grad_w, grad_b, grad_c\n\n    def train(self, K, eta):\n        \"\"\"\n        Trains the network using gradient descent.\n        K: number of training steps\n        eta: learning rate\n        \"\"\"\n        for _ in range(K):\n            loss, grad_a, grad_w, grad_b, grad_c = self._compute_gradients()\n            \n            self.a -= eta * grad_a\n            self.w -= eta * grad_w\n            self.b -= eta * grad_b\n            self.c -= eta * grad_c\n\n    def project_and_analyze(self):\n        \"\"\"\n        Projects the learned function onto sin(x) and sin(25x) and checks for spectral bias.\n        \"\"\"\n        M = 4096\n        x_dense = np.linspace(0, 2 * np.pi, M, endpoint=False).reshape(-1, 1)\n        \n        u_pred, _, _, _, _ = self.forward(x_dense)\n        u_pred = u_pred.flatten()\n        \n        # Create design matrix for least squares\n        B = np.zeros((M, 2))\n        B[:, 0] = np.sin(x_dense.flatten())\n        B[:, 1] = np.sin(25.0 * x_dense.flatten())\n        \n        # Solve least squares problem: B * alpha = u_pred\n        alpha, _, _, _ = np.linalg.lstsq(B, u_pred, rcond=None)\n        \n        A1 = np.abs(alpha[0])\n        A25 = np.abs(alpha[1])\n        \n        return A1 > A25\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (20, 128, 60, 0.01),  # Case 1: (H, N, K, eta)\n        (10, 64, 80, 0.01),   # Case 2\n        (5, 128, 120, 0.01),  # Case 3\n    ]\n\n    results = []\n    base_seed = 42\n\n    for i, (H, N, K, eta) in enumerate(test_cases):\n        seed = base_seed + i\n        pinn = PINN(H=H, N=N, seed=seed)\n        pinn.train(K=K, eta=eta)\n        result = pinn.project_and_analyze()\n        results.append(result)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}