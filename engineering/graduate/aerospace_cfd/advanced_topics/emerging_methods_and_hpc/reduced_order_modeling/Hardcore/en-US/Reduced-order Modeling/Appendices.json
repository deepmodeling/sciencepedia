{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in reduced-order modeling is deciding the size, or rank $r$, of the model. A larger $r$ offers more accuracy but diminishes the computational savings. This exercise provides hands-on practice with the most common method for making this trade-off: the energy-based truncation criterion. By implementing this method, you will learn how to use the singular values of your data to quantify the \"energy\" captured by a given number of modes and select a model rank that meets a desired accuracy tolerance .",
            "id": "3990098",
            "problem": "Consider reduced-order modeling for aerospace computational fluid dynamics based on Proper Orthogonal Decomposition (POD). Let the snapshot matrix be denoted by $X \\in \\mathbb{R}^{m \\times n}$, and assume it admits a Singular Value Decomposition (SVD) $X = U \\Sigma V^\\top$, where the diagonal entries of $\\Sigma$ are the singular values $\\sigma_1, \\sigma_2, \\dots, \\sigma_k$ with $k = \\min(m,n)$, and $U$ and $V$ are orthonormal matrices. In POD, the energetic content associated with mode $i$ is proportional to $\\sigma_i^2$, and the total energy is proportional to $\\sum_{i=1}^{k} \\sigma_i^2$. The cumulative energy retained by a rank-$r$ approximation is defined as the fraction of total energy captured by the leading $r$ singular values. The neglected energy fraction is defined as the complement of the retained fraction.\n\nStarting from these definitions and the SVD properties, do the following for each provided test case. You must treat any list of singular values as an unordered multiset of nonnegative real numbers and first sort them in nonincreasing order before any computation.\n\n1. Given a list of singular values $\\{\\sigma_i\\}_{i=1}^k$, compute the cumulative energy retained curve $\\{E_r\\}_{r=0}^k$ defined by\n$$\nE_r = \\frac{\\sum_{i=1}^{r} \\sigma_i^2}{\\sum_{i=1}^{k} \\sigma_i^2},\n$$\nwith the convention $E_0 = 0$. If $\\sum_{i=1}^{k} \\sigma_i^2 = 0$, define $E_r = 0$ for all $r$ and, for the purpose of the tolerance criterion below, define the neglected energy fraction to be $0$.\n2. Given a tolerance $\\epsilon \\in [0,1]$ (expressed as a decimal, not a percentage), select the smallest integer $r \\in \\{0,1,\\dots,k\\}$ such that the neglected energy fraction $1 - E_r \\le \\epsilon$.\n3. For each test case, output a list of the form $[r, [E_0, E_1, \\dots, E_k], \\text{ok}]$, where $r$ is the selected rank, $[E_0, E_1, \\dots, E_k]$ is the cumulative energy retained curve, and $\\text{ok}$ is a boolean value indicating whether the inequality $1 - E_r \\le \\epsilon$ holds for the selected $r$.\n\nUse the following test suite of singular value arrays and tolerances, designed to cover typical behavior, boundary conditions, and edge cases that probe sensitivity to the tolerance:\n- Test case $1$: singular values $[4.0, 2.0, 1.0, 0.5, 0.25]$, tolerance $\\epsilon = 0.05$.\n- Test case $2$: singular values $[3.0, 0.0, 0.0]$, tolerance $\\epsilon = 0$.\n- Test case $3$: singular values $[0.5, 2.0, 1.0]$, tolerance $\\epsilon = 0.2$.\n- Test case $4$: singular values $[10, 9.9, 0.1, 0.1, 0.1]$, tolerance $\\epsilon = 0.0002$.\n- Test case $5$: singular values $[10, 9.9, 0.1, 0.1, 0.1]$, tolerance $\\epsilon = 0.00015$.\n- Test case $6$: singular values $[0.0, 0.0]$, tolerance $\\epsilon = 0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each element corresponding to one test case in order. For example, the output format must be exactly of the form\n$[[r_1,[E_{0,1},\\dots,E_{k_1,1}],\\text{ok}_1],[r_2,[E_{0,2},\\dots,E_{k_2,2}],\\text{ok}_2],\\dots]$.",
            "solution": "The problem requires the implementation of an algorithm based on Proper Orthogonal Decomposition (POD) to determine the optimal reduced-order model rank for a given energy tolerance. The process involves analyzing a set of singular values derived from a snapshot matrix. The solution is structured into a sequence of well-defined computational steps for each test case.\n\nFirst, let the given list of singular values be denoted as a multiset $\\{\\sigma_i\\}_{i=1}^k$. As per the problem statement, these must be sorted in non-increasing order to properly represent the energy hierarchy of the POD modes. Let the sorted singular values be $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$. Here, $k$ is the total number of singular values.\n\nThe energetic content, or simply \"energy\", associated with the $i$-th POD mode is proportional to the square of its corresponding singular value, $\\sigma_i^2$. The total energy of the system captured in the snapshots is proportional to the sum of the energies of all modes, which is $E_{\\text{total}} = \\sum_{i=1}^{k} \\sigma_i^2$.\n\nThe core of the problem is to compute the cumulative energy retained by a rank-$r$ approximation, defined as:\n$$\nE_r = \\frac{\\sum_{i=1}^{r} \\sigma_i^2}{\\sum_{i=1}^{k} \\sigma_i^2} \\quad \\text{for } r \\in \\{1, 2, \\dots, k\\}\n$$\nBy convention, for a rank-$0$ model (which contains no modes), the retained energy is zero, so $E_0 = 0$. The sequence $\\{E_r\\}_{r=0}^k$ constitutes the cumulative energy retained curve. This sequence is, by definition, monotonically non-decreasing, starting at $E_0 = 0$ and ending at $E_k = 1$. A special case arises if the total energy is zero ($E_{\\text{total}} = 0$), which occurs if and only if all singular values are zero. In this scenario, the problem specifies that $E_r = 0$ for all $r \\in \\{0, 1, \\dots, k\\}$.\n\nThe algorithm proceeds as follows for each test case, which provides a list of singular values and a tolerance $\\epsilon \\in [0,1]$:\n\n1.  **Initialization and Sorting**: The input list of singular values is converted into a numerical array and sorted in non-increasing order to obtain the sequence $\\sigma_1, \\sigma_2, \\dots, \\sigma_k$.\n\n2.  **Energy Calculation**: The square of each singular value, $\\sigma_i^2$, is computed. The total energy, $E_{\\text{total}} = \\sum_{i=1}^{k} \\sigma_i^2$, is then calculated.\n\n3.  **Cumulative Energy Curve Construction**:\n    - If $E_{\\text{total}}$ is zero, the cumulative energy curve $\\{E_r\\}_{r=0}^k$ is a sequence of $k+1$ zeros.\n    - If $E_{\\text{total}}$ is positive, the partial sums of the squared singular values, $S_r = \\sum_{i=1}^{r} \\sigma_i^2$, are computed for $r=1, \\dots, k$. The cumulative energy for each rank is then $E_r = S_r / E_{\\text{total}}$. The final curve is the sequence $[E_0, E_1, \\dots, E_k]$.\n\n4.  **Rank Selection**: The primary task is to find the smallest integer rank $r \\in \\{0, 1, \\dots, k\\}$ such that the neglected energy fraction is no more than the specified tolerance $\\epsilon$. This condition is expressed by the inequality:\n    $$\n    1 - E_r \\le \\epsilon\n    $$\n    This is mathematically equivalent to finding the smallest $r$ that satisfies:\n    $$\n    E_r \\ge 1 - \\epsilon\n    $$\n    Since the sequence $\\{E_r\\}$ is monotonically non-decreasing, we can efficiently find the smallest $r$ that meets this criterion by searching through the sequence, starting from $r=0$. The search terminates at the first rank where the condition is met. For the special case where $E_{\\text{total}} = 0$, the problem defines the neglected energy fraction as $0$. The condition becomes $0 \\le \\epsilon$, which is always true for $\\epsilon \\in [0,1]$. Thus, the smallest rank satisfying the condition is $r=0$.\n\n5.  **Verification**: The problem requires a boolean value, $\\text{ok}$, confirming that the selected rank $r$ indeed satisfies the inequality $1 - E_r \\le \\epsilon$. By the construction of our rank selection method, this condition will always be satisfied. This step serves as a formal verification of the result.\n\n6.  **Output Assembly**: For each test case, the final result is assembled into a list containing three elements: the selected rank $r$, the complete cumulative energy curve $[E_0, E_1, \\dots, E_k]$, and the boolean verification flag $\\text{ok}$.\n\nThis procedure provides a complete and deterministic solution to the problem, correctly handling the specified definitions, conventions, and edge cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(sigma_values, epsilon):\n    \"\"\"\n    Processes a single test case of singular values and a tolerance.\n\n    Args:\n        sigma_values (list): A list of non-negative real numbers representing singular values.\n        epsilon (float): The tolerance for neglected energy, in the range [0, 1].\n\n    Returns:\n        list: A list of the form [r, E_curve, ok], where r is the selected rank,\n              E_curve is the cumulative energy list, and ok is the verification boolean.\n    \"\"\"\n    # Use float64 for better precision in calculations.\n    sv = np.array(sigma_values, dtype=np.float64)\n\n    # 1. Sort singular values in non-increasing order.\n    sv_sorted = np.sort(sv)[::-1]\n    k = len(sv_sorted)\n\n    # 2. Calculate squared values and total energy.\n    sv_squared = np.square(sv_sorted)\n    total_energy = np.sum(sv_squared)\n\n    # 3. Handle the zero total energy edge case.\n    if np.isclose(total_energy, 0.0):\n        E_curve = [0.0] * (k + 1)\n        # As per the problem: neglected energy fraction is defined as 0.\n        # The condition is 0 <= epsilon, which is always true for valid epsilon.\n        # The smallest rank r is therefore 0.\n        r = 0\n        ok = True  # The condition 0 <= epsilon holds.\n        return [r, E_curve, ok]\n\n    # 4. Calculate the cumulative energy retained curve {E_r}.\n    cumulative_energies = np.cumsum(sv_squared) / total_energy\n    # Prepend E_0 = 0 to form the full curve {E_r}_{r=0 to k}.\n    E_curve = np.insert(cumulative_energies, 0, 0.0)\n\n    # 5. Find the smallest rank r such that 1 - E_r <= epsilon,\n    # which is equivalent to E_r >= 1 - epsilon.\n    threshold = 1.0 - epsilon\n\n    # Since E_curve is monotonically non-decreasing, we can find the\n    # first index where the condition is met. np.searchsorted is efficient for this.\n    # It finds the first index `i` such that E_curve[i] >= threshold.\n    r = np.searchsorted(E_curve, threshold, side='left')\n\n    # The rank r must be at most k. This is guaranteed because E_curve[k] = 1.0\n    # and the threshold is at most 1.0.\n\n    # 6. Verify that the selected rank r satisfies the condition.\n    # By construction of the search for r, this should always be true.\n    neglected_energy = 1.0 - E_curve[r]\n    ok = neglected_energy <= epsilon\n    \n    # Python's floating-point precision can sometimes make `ok` False if\n    # `neglected_energy` is infinitesimally larger than `epsilon`.\n    # To be robust, one might use `np.isclose` or a small tolerance.\n    # However, the problem tests sensitivity, so direct comparison is used.\n    # Re-checking with a close-to-equal check to handle precision artifacts.\n    if not ok and np.isclose(neglected_energy, epsilon):\n        ok = True\n\n    return [int(r), E_curve.tolist(), ok]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: Typical decay\n        ([4.0, 2.0, 1.0, 0.5, 0.25], 0.05),\n        # Test case 2: Rank-deficient, zero tolerance\n        ([3.0, 0.0, 0.0], 0.0),\n        # Test case 3: Unordered input\n        ([0.5, 2.0, 1.0], 0.2),\n        # Test case 4: Tolerance sensitivity 1\n        ([10.0, 9.9, 0.1, 0.1, 0.1], 0.0002),\n        # Test case 5: Tolerance sensitivity 2\n        ([10.0, 9.9, 0.1, 0.1, 0.1], 0.00015),\n        # Test case 6: Zero energy\n        ([0.0, 0.0], 0.5),\n    ]\n\n    all_results = []\n    for sv, eps in test_cases:\n        result = process_case(sv, eps)\n        all_results.append(result)\n\n    # Format the final output string to match the required format exactly.\n    # [[r1,[E...],ok1],[r2,[E...],ok2],...]\n    # Example format: [3,[0.0,0.75...,1.0],true]\n    result_strings = []\n    for res in all_results:\n        r, E_curve, ok = res\n        E_curve_str = f\"[{','.join(map(str, E_curve))}]\"\n        ok_str = str(ok).lower()\n        result_strings.append(f\"[{r},{E_curve_str},{ok_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "After learning how to select a model rank, a critical next step is to understand the scenarios where Proper Orthogonal Decomposition (POD) is effective and where it fails. This practice explores a classic case where linear POD performs poorly: a simple translating wave. By applying POD to a series of snapshots of a moving Gaussian pulse, you will discover why phenomena dominated by pure transport or advection are challenging to compress into a small number of fixed, global basis functions, a fundamental limitation that has driven much advanced ROM research .",
            "id": "3265968",
            "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_{1},E_{2},E_{5},E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like\n$[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]$,\nwith no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.",
            "solution": "The objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$ is the rank of the matrix.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$ that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^T$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Achieving a low reconstruction error is a necessary, but not sufficient, condition for a useful predictive model. This final practice explores a subtle and critical pitfall where a mathematically stable system can produce an unstable and physically meaningless reduced-order model (ROM), even when the POD basis represents the training data almost perfectly. By constructing such a system, you will gain crucial insight into the challenges posed by non-normal operators, which are common in fluid dynamics, and learn why verifying the stability of the ROM dynamics is just as important as minimizing projection error .",
            "id": "2432128",
            "problem": "You are asked to implement a complete numerical experiment in reduced-order modeling that demonstrates the following phenomenon: a Proper Orthogonal Decomposition (POD) basis can be excellent for reconstructing training snapshots of a stable full-order linear time-invariant system, yet the Galerkin-projected reduced-order model (ROM) can produce unstable dynamics that blow up when integrated in time.\n\nYour implementation must start from the full-order ordinary differential equation\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b},\n$$\nwhere $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^{n}$ are constant, and $\\mathbf{x}(t) \\in \\mathbb{R}^{n}$ is the state. All computations are over the real numbers with the standard Euclidean inner product. You will use $n=2$ throughout.\n\nFundamental definitions and requirements:\n- Proper Orthogonal Decomposition (POD) basis: Given a snapshot matrix\n$$\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}(t_1) & \\mathbf{x}(t_2) & \\cdots & \\mathbf{x}(t_m)\\end{bmatrix} \\in \\mathbb{R}^{n \\times m},\n$$\ncompute its singular value decomposition (SVD) $\\mathbf{X}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. The rank-$r$ POD basis $\\mathbf{Q} \\in \\mathbb{R}^{n \\times r}$ is taken as the first $r$ columns of $\\mathbf{U}$.\n- Galerkin projection: The reduced operator and reduced forcing are\n$$\n\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q} \\in \\mathbb{R}^{r \\times r}, \\qquad \\mathbf{b}_r=\\mathbf{Q}^\\top\\mathbf{b} \\in \\mathbb{R}^{r}.\n$$\nThe reduced state $\\mathbf{z}(t) \\in \\mathbb{R}^{r}$ evolves as\n$$\n\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r \\mathbf{z} + \\mathbf{b}_r, \\qquad \\mathbf{x}_r(t)=\\mathbf{Q}\\mathbf{z}(t).\n$$\n- Time integration: Use the classical fourth-order Runge–Kutta method with a fixed time step $h > 0$ for both the full-order model and the ROM. Set the initial condition to $\\mathbf{x}(0)=\\mathbf{0}$ and $\\mathbf{z}(0)=\\mathbf{Q}^\\top\\mathbf{x}(0)=\\mathbf{0}$.\n- Snapshot collection: Integrate the full-order model over a training horizon $[0,T_{\\text{train}}]$ with a constant time step $h$, sampling the state at every step to form $\\mathbf{X}$.\n- Reconstruction error: Measure the relative POD reconstruction error of the training snapshots as\n$$\n\\varepsilon_{\\text{rec}} = \\frac{\\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F}{\\lVert \\mathbf{X}\\rVert_F},\n$$\nwhere $\\lVert\\cdot\\rVert_F$ denotes the Frobenius norm.\n- Blow-up detection: Evolve both the full-order model and the ROM over a test horizon $[0,T_{\\text{test}}]$ with the same $h$. Declare a solution “blown up” if at any time step the Euclidean norm of the current state exceeds a threshold $M$, or if any component becomes not-a-number or infinite. Use the threshold $M=10^6$.\n\nConstructed forcing to target instability under ROM:\n- For each test, you must construct the constant forcing $\\mathbf{b}$ as follows. Compute the symmetric part $\\mathbf{S}=\\frac{1}{2}(\\mathbf{A}+\\mathbf{A}^\\top)$ and its dominant unit eigenvector $\\mathbf{q} \\in \\mathbb{R}^{n}$ associated with the largest eigenvalue of $\\mathbf{S}$ (break ties arbitrarily but deterministically). Set\n$$\n\\mathbf{b}=-\\mathbf{A}\\mathbf{q}.\n$$\nThis construction ensures that the full-order steady state is $\\mathbf{x}_\\infty = -\\mathbf{A}^{-1}\\mathbf{b}=\\mathbf{q}$. When $\\mathbf{A}$ is highly non-normal and the largest eigenvalue of $\\mathbf{S}$ is positive, the scalar ROM obtained with $r=1$ and $\\mathbf{Q}=\\mathbf{q}$ has reduced dynamics $\\frac{dz}{dt} = a_r z + b_r$ with $a_r=\\mathbf{q}^\\top\\mathbf{A}\\mathbf{q} > 0$ and $b_r=-a_r$, which is unstable and diverges from $z(0)=0$.\n\nNumerical specification common to all tests:\n- Use $n=2$.\n- Use $h=10^{-3}$.\n- Use classical fourth-order Runge–Kutta.\n- Use the Euclidean norm for all vector norms.\n- Use $\\mathbf{x}(0)=\\mathbf{0}$.\n\nTest suite:\nImplement the above for the following parameter sets. In each case, define $\\mathbf{A}$, compute $\\mathbf{q}$ and $\\mathbf{b}$ as specified, collect snapshots over $[0,T_{\\text{train}}]$ to form $\\mathbf{Q}$, then form the ROM and run both models over $[0,T_{\\text{test}}]$.\n\n- Test $1$ (highly non-normal, rank-$1$ POD):\n  - $$\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$$ with $\\alpha=50.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $2$ (highly non-normal, rank-$2$ POD):\n  - $$\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$$ with $\\alpha=50.0$,\n  - $r=2$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $3$ (symmetric negative definite, rank-$1$ POD):\n  - $$\\mathbf{A}=\\begin{bmatrix}-1.0 & 0.0 \\\\ 0.0 & -2.0\\end{bmatrix}$$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $4$ (more highly non-normal, rank-$1$ POD):\n  - $$\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$$ with $\\alpha=120.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n\nRequired outputs:\n- For each test, output a list of three entries:\n  - the scalar $\\varepsilon_{\\text{rec}}$ rounded to six decimal places,\n  - a boolean indicating whether the ROM blew up on $[0,T_{\\text{test}}]$,\n  - a boolean indicating whether the full-order model blew up on $[0,T_{\\text{test}}]$.\n- Aggregate the results from all tests into a single line as a comma-separated list enclosed in square brackets, in the same order as the tests. Example format:\n`[[\\varepsilon_rec_1, rom_blew_up_1, fom_blew_up_1], [\\varepsilon_rec_2, rom_blew_up_2, fom_blew_up_2], ...]`.",
            "solution": "The core of this problem is to demonstrate a known failure mode of reduced-order models (ROMs) where a stable full-order model (FOM) can produce an unstable ROM. This phenomenon is characteristic of systems governed by highly non-normal operators. The solution hinges on understanding the distinction between the spectrum of a matrix $\\mathbf{A}$, which determines its asymptotic stability, and its numerical range (or field of values), which provides insight into transient behavior and the stability of projected systems.\n\nFor a linear time-invariant system $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x}$, stability is determined by the eigenvalues of $\\mathbf{A}$. If all eigenvalues have negative real parts, the system is stable. However, transient growth is possible if $\\mathbf{A}$ is non-normal (i.e., $\\mathbf{A}\\mathbf{A}^\\top \\neq \\mathbf{A}^\\top\\mathbf{A}$). The real part of the numerical range is governed by the symmetric part of the matrix, $\\mathbf{S} = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)$. A positive eigenvalue of $\\mathbf{S}$ implies that the numerical range of $\\mathbf{A}$ extends into the right half-plane, indicating potential for transient energy growth.\n\nA Galerkin projection with a rank-$r$ POD basis $\\mathbf{Q}$ transforms the FOM into the ROM $\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r\\mathbf{z} + \\mathbf{b}_r$, where $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. The stability of the ROM is determined by the eigenvalues of the reduced matrix $\\mathbf{A}_r$. Crucially, the eigenvalues of $\\mathbf{A}_r$ are contained within the numerical range of $\\mathbf{A}$, but not necessarily within the convex hull of its spectrum. If the numerical range $W(\\mathbf{A})$ crosses into the right half-plane, it is possible for the reduced matrix $\\mathbf{A}_r$ to have eigenvalues with positive real parts, rendering the ROM unstable.\n\nThe problem's construction is designed to expose this pathology. The FOM is stable (eigenvalues of $\\mathbf{A}$ are $\\{-0.1, -1.0\\}$). The forcing term $\\mathbf{b} = -\\mathbf{A}\\mathbf{q}$ is chosen such that the FOM steady state is $\\mathbf{x}_{\\infty} = \\mathbf{q}$, where $\\mathbf{q}$ is the eigenvector corresponding to the largest eigenvalue of $\\mathbf{S}$. This drives the system dynamics towards the direction of maximum transient growth. The resulting snapshots will be dominated by this direction, causing the primary POD mode to align with $\\mathbf{q}$. For a rank-$1$ ROM ($r=1$), the reduced matrix $\\mathbf{A}_r$ becomes a scalar $a_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. If $\\mathbf{Q} \\approx \\mathbf{q}$, then $a_r \\approx \\mathbf{q}^\\top\\mathbf{A}\\mathbf{q} = \\mathbf{q}^\\top\\mathbf{S}\\mathbf{q} = \\lambda_{\\max}(\\mathbf{S})$. For the non-normal matrices, $\\lambda_{\\max}(\\mathbf{S}) > 0$, leading to an unstable ROM.\n\nThe computational procedure for each test case is as follows:\n1.  Define system parameters: matrix $\\mathbf{A}$, ROM rank $r$, and time horizons $T_{\\text{train}}$ and $T_{\\text{test}}$.\n2.  Construct the forcing term: Compute the symmetric part $\\mathbf{S} = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)$. Find its dominant eigenvector $\\mathbf{q}$ and set $\\mathbf{b} = -\\mathbf{A}\\mathbf{q}$.\n3.  Generate training data: Integrate the FOM from $\\mathbf{x}(0)=\\mathbf{0}$ over $[0, T_{\\text{train}}]$ and collect snapshots in matrix $\\mathbf{X}$.\n4.  Compute the POD basis: Perform an SVD on $\\mathbf{X}$ to get the rank-$r$ basis $\\mathbf{Q}$.\n5.  Calculate reconstruction error $\\varepsilon_{\\text{rec}}$ using the singular values of $\\mathbf{X}$.\n6.  Construct the ROM: Compute $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$ and $\\mathbf{b}_r = \\mathbf{Q}^\\top\\mathbf{b}$.\n7.  Perform time integration for testing: Integrate both the FOM and the ROM from zero initial conditions over $[0, T_{\\text{test}}]$, checking for blow-up against the threshold $M=10^6$.\n8.  Record results: For each test, store the reconstruction error, a boolean for ROM blow-up, and a boolean for FOM blow-up.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Implements the full numerical experiment to demonstrate ROM instability\n    for a stable FOM.\n    \"\"\"\n\n    def rk4_step(f, y, h, A, b):\n        \"\"\"A single step of the classical fourth-order Runge-Kutta method.\"\"\"\n        k1 = f(y, A, b)\n        k2 = f(y + h / 2 * k1, A, b)\n        k3 = f(y + h / 2 * k2, A, b)\n        k4 = f(y + h * k3, A, b)\n        return y + h / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n\n    def lti_rhs(y, A, b):\n        \"\"\"RHS of the LTI system dy/dt = Ay + b.\"\"\"\n        return A @ y + b\n\n    def simulate(A, b, y0, T, h, M):\n        \"\"\"\n        Simulates an LTI system and returns snapshots and blow-up status.\n        \"\"\"\n        num_steps = int(T / h)\n        y = y0.copy()\n        snapshots = [y0.copy()]\n        blew_up = False\n        \n        for _ in range(num_steps):\n            y = rk4_step(lti_rhs, y, h, A, b)\n            if np.linalg.norm(y) > M or not np.all(np.isfinite(y)):\n                blew_up = True\n                # Stop checking once blown up, but fill snapshot array to required size.\n                while len(snapshots) < num_steps + 1:\n                    snapshots.append(y.copy())\n                    # Keep integrating to get inf/nan if that's what happens\n                    y = rk4_step(lti_rhs, y, h, A, b)\n                return np.array(snapshots).T, True\n\n            snapshots.append(y.copy())\n            \n        return np.array(snapshots).T, blew_up\n\n    # General parameters\n    n = 2\n    h = 1e-3\n    M = 1e6\n    x0 = np.zeros(n)\n\n    # Test cases from the problem statement.\n    test_cases = [\n        # (A_params, r, T_train, T_test)\n        ({\"alpha\": 50.0}, 1, 4.0, 1.0),\n        ({\"alpha\": 50.0}, 2, 4.0, 1.0),\n        ({\"alpha\": None}, 1, 4.0, 1.0), # Symmetric case\n        ({\"alpha\": 120.0}, 1, 4.0, 1.0),\n    ]\n\n    results = []\n    \n    for i, (params, r, T_train, T_test) in enumerate(test_cases):\n        # 1. Define A\n        if i == 2: # Test 3: Symmetric case\n            A = np.array([[-1.0, 0.0], [0.0, -2.0]])\n        else: # Tests 1, 2, 4: Non-normal case\n            alpha = params[\"alpha\"]\n            A = np.array([[-0.1, alpha], [0.0, -1.0]])\n\n        # 2. Construct b\n        S = 0.5 * (A + A.T)\n        eigvals, eigvecs = eigh(S)\n        q = eigvecs[:, -1] # Dominant eigenvector (eigh sorts eigenvalues)\n        b = -A @ q\n\n        # 3. Generate FOM snapshots for training\n        X, _ = simulate(A, b, x0, T_train, h, M)\n\n        # 4. Compute POD basis Q\n        U, s, _ = np.linalg.svd(X, full_matrices=False)\n        Q = U[:, :r]\n\n        # 5. Calculate reconstruction error\n        # eps_rec = norm(X - Q @ Q.T @ X) / norm(X)\n        # Using singular values is more direct: sqrt(sum(s_i^2 for i>r)) / sqrt(sum(s_i^2))\n        if X.shape[1] > 1:\n            norm_X_sq = np.sum(s**2)\n            if norm_X_sq > 0:\n                norm_err_sq = np.sum(s[r:]**2)\n                eps_rec = np.sqrt(norm_err_sq / norm_X_sq)\n            else:\n                eps_rec = 0.0\n        else:\n            eps_rec = 0.0\n\n\n        # 6. Form the ROM\n        Ar = Q.T @ A @ Q\n        br = Q.T @ b\n        z0 = np.zeros(r)\n\n        # 7. Simulate FOM and ROM for testing, check blow-up\n        _, fom_blew_up = simulate(A, b, x0, T_test, h, M)\n        _, rom_blew_up = simulate(Ar, br, z0, T_test, h, M)\n\n        # 8. Record results\n        results.append([round(eps_rec, 6), rom_blew_up, fom_blew_up])\n\n    # Final print statement in the exact required format.\n    # Convert bools to lowercase 'true'/'false' for JS-like format\n    formatted_results = []\n    for res in results:\n        eps_str = f\"{res[0]:.6f}\"\n        rom_bool_str = str(res[1]).lower()\n        fom_bool_str = str(res[2]).lower()\n        formatted_results.append(f\"[{eps_str},{rom_bool_str},{fom_bool_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}