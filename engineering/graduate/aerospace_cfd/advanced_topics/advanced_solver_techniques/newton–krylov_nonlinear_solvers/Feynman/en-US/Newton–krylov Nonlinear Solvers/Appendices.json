{
    "hands_on_practices": [
        {
            "introduction": "The core of any Krylov-based linear solver is the matrix-vector product. In a Jacobian-Free Newton-Krylov method, this means we must compute the action of the Jacobian on a vector, $Jv$, without ever forming the matrix $J$ itself. This exercise  demonstrates a powerful and highly accurate technique, the complex-step method, to achieve this and contrasts it with the more common but less robust finite-difference approximation.",
            "id": "3979894",
            "problem": "Consider the use of the Newton-Krylov (NK) method within Computational Fluid Dynamics (CFD) for solving nonlinear systems arising from the one-dimensional inviscid compressible Euler equations. In a matrix-free NK implementation, the action of the Jacobian of the residual on a vector is required. For a single face in a non-dimensionalized setting, model the residual contribution by the physical flux vector $F(u)$ evaluated at a state $u$, where $u = (\\rho, m, E)$ collects the density $\\rho$, momentum $m$, and total energy $E$. Let the ratio of specific heats be $\\gamma$, and define the pressure $p$ by the ideal-gas relation $p = (\\gamma - 1)\\left(E - \\frac{1}{2}\\frac{m^{2}}{\\rho}\\right)$. The Euler flux is\n$$\nF(u) \\;=\\; \\begin{pmatrix}\nm \\\\\n\\frac{m^{2}}{\\rho} + p \\\\\n\\frac{m}{\\rho}\\,(E + p)\n\\end{pmatrix}.\n$$\nYou are given the state $u$ and direction $v$,\n$$\nu \\;=\\; \\begin{pmatrix} 2 \\\\ 3 \\\\ 10 \\end{pmatrix}, \\qquad v \\;=\\; \\begin{pmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{pmatrix}, \\qquad \\gamma \\;=\\; 1.4.\n$$\nUsing the concept of complex-step differentiation, compute the Jacobian-vector product $J(u)v$ for the flux $F(u)$ by evaluating the residual at a purely imaginary perturbation in the direction $v$ and extracting the derivative information. Then, compute the forward real finite-difference approximation to $J(u)v$ using a small real step $\\epsilon_{\\mathrm{r}} = 1 \\times 10^{-6}$ and comment on the comparative accuracy and practicality of the complex-step approach versus real finite differences for this compressible-flow residual. Finally, report the Euclidean norm of the complex-step computed $J(u)v$. Round your answer to four significant figures and express it as a dimensionless number.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of computational fluid dynamics and numerical analysis, well-posed with all necessary information provided, and objective in its formulation. The given physical state $u$ is valid, with positive density $\\rho=2$ and a positive pressure $p = (\\gamma-1)(E - m^2/(2\\rho)) = (1.4-1)(10 - 3^2/(2 \\cdot 2)) = 0.4(10 - 9/4) = 0.4(7.75) = 3.1 > 0$. We may therefore proceed with the solution.\n\nThe problem asks for the computation of a Jacobian-vector product $J(u)v$, where $J(u) = \\frac{\\partial F}{\\partial u}$ is the Jacobian of the Euler flux vector $F(u)$, for a given state $u$ and perturbation vector $v$. This is a central operation in matrix-free Newton-Krylov methods. We will perform this computation using two methods: complex-step differentiation and forward finite-difference.\n\nLet the state vector be $u = (\\rho, m, E)^T$. The given values are:\n$$\nu \\;=\\; \\begin{pmatrix} \\rho \\\\ m \\\\ E \\end{pmatrix} \\;=\\; \\begin{pmatrix} 2 \\\\ 3 \\\\ 10 \\end{pmatrix}, \\qquad v \\;=\\; \\begin{pmatrix} v_{\\rho} \\\\ v_{m} \\\\ v_{E} \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{pmatrix}, \\qquad \\gamma \\;=\\; 1.4.\n$$\nThe pressure $p$ and flux vector $F(u)$ are defined as:\n$$\np(u) \\;=\\; (\\gamma - 1)\\left(E - \\frac{1}{2}\\frac{m^{2}}{\\rho}\\right)\n$$\n$$\nF(u) \\;=\\; \\begin{pmatrix} F_1 \\\\ F_2 \\\\ F_3 \\end{pmatrix} \\;=\\; \\begin{pmatrix}\nm \\\\\n\\frac{m^{2}}{\\rho} + p \\\\\n\\frac{m}{\\rho}\\,(E + p)\n\\end{pmatrix}\n$$\n\n**1. Jacobian-Vector Product via Complex-Step Differentiation**\n\nThe complex-step derivative approximation for the Jacobian-vector product $J(u)v$ is given by:\n$$\nJ(u)v = \\frac{\\partial F}{\\partial u}v \\approx \\frac{\\text{Im}[F(u + i\\epsilon v)]}{\\epsilon}\n$$\nfor a small real step size $\\epsilon$. This approximation is remarkably accurate, with an error of order $O(\\epsilon^2)$, and it crucially avoids the subtractive cancellation errors that plague real finite-difference methods. For an analytic function $F$, the result is accurate to machine precision for a wide range of small $\\epsilon$. We can analyze the action by considering the first-order Taylor expansion of $F(u+i\\epsilon v)$ about $u$:\n$$\nF(u+i\\epsilon v) = F(u) + J(u)(i\\epsilon v) + O((i\\epsilon v)^2) = F(u) + i\\epsilon J(u)v + O(\\epsilon^2)\n$$\nTaking the imaginary part and dividing by $\\epsilon$ yields the desired product:\n$$\n\\frac{\\text{Im}[F(u + i\\epsilon v)]}{\\epsilon} = J(u)v + O(\\epsilon^2)\n$$\nTo compute this, we can analytically determine the result by calculating the Jacobian $J(u)$ and then the product $J(u)v$. This is equivalent to evaluating $F$ with complex arguments.\n\nFirst, we compute the gradient of the pressure, $\\nabla p = (\\frac{\\partial p}{\\partial \\rho}, \\frac{\\partial p}{\\partial m}, \\frac{\\partial p}{\\partial E})^T$.\n$$\n\\frac{\\partial p}{\\partial \\rho} = (\\gamma - 1) \\frac{m^2}{2\\rho^2} = (1.4-1)\\frac{3^2}{2(2^2)} = 0.4 \\cdot \\frac{9}{8} = 0.45\n$$\n$$\n\\frac{\\partial p}{\\partial m} = -(\\gamma - 1)\\frac{m}{\\rho} = -(1.4-1)\\frac{3}{2} = -0.4 \\cdot 1.5 = -0.6\n$$\n$$\n\\frac{\\partial p}{\\partial E} = \\gamma - 1 = 1.4 - 1 = 0.4\n$$\nThe Jacobian of $F(u)$ is $J(u) = [\\nabla F_1, \\nabla F_2, \\nabla F_3]^T$.\nFor $F_1(u) = m$:\n$\\nabla F_1 = (0, 1, 0)$.\n\nFor $F_2(u) = \\frac{m^2}{\\rho} + p = \\frac{m^2}{\\rho} + (\\gamma - 1)(E - \\frac{m^2}{2\\rho}) = (\\frac{3-\\gamma}{2}) \\frac{m^2}{\\rho} + (\\gamma-1)E$:\n$$\n\\frac{\\partial F_2}{\\partial \\rho} = -(\\frac{3-\\gamma}{2})\\frac{m^2}{\\rho^2} = -(\\frac{3-1.4}{2})\\frac{3^2}{2^2} = -0.8 \\cdot 2.25 = -1.8\n$$\n$$\n\\frac{\\partial F_2}{\\partial m} = (\\frac{3-\\gamma}{2})\\frac{2m}{\\rho} = (3-\\gamma)\\frac{m}{\\rho} = (3-1.4)\\frac{3}{2} = 1.6 \\cdot 1.5 = 2.4\n$$\n$$\n\\frac{\\partial F_2}{\\partial E} = \\gamma - 1 = 0.4\n$$\nSo, $\\nabla F_2 = (-1.8, 2.4, 0.4)$.\n\nFor $F_3(u) = \\frac{m}{\\rho}(E+p) = \\frac{m}{\\rho}(E + (\\gamma-1)E - (\\gamma-1)\\frac{m^2}{2\\rho}) = \\gamma E \\frac{m}{\\rho} - \\frac{\\gamma-1}{2}\\frac{m^3}{\\rho^2}$:\n$$\n\\frac{\\partial F_3}{\\partial \\rho} = -\\gamma E \\frac{m}{\\rho^2} + (\\gamma-1)\\frac{m^3}{\\rho^3} = \\frac{m}{\\rho^2}(-\\gamma E + \\frac{(\\gamma-1)m^2}{\\rho}) = \\frac{3}{4}(-1.4 \\cdot 10 + \\frac{0.4 \\cdot 3^2}{2}) = \\frac{3}{4}(-14+1.8) = -9.15\n$$\n$$\n\\frac{\\partial F_3}{\\partial m} = \\gamma \\frac{E}{\\rho} - \\frac{3(\\gamma-1)}{2}\\frac{m^2}{\\rho^2} = 1.4\\frac{10}{2} - \\frac{3(0.4)}{2}\\frac{3^2}{2^2} = 7 - 0.6 \\cdot 2.25 = 7 - 1.35 = 5.65\n$$\n$$\n\\frac{\\partial F_3}{\\partial E} = \\gamma \\frac{m}{\\rho} = 1.4 \\frac{3}{2} = 2.1\n$$\nSo, $\\nabla F_3 = (-9.15, 5.65, 2.1)$.\n\nThe Jacobian matrix at $u$ is:\n$$\nJ(u) = \\begin{pmatrix} 0 & 1 & 0 \\\\ -1.8 & 2.4 & 0.4 \\\\ -9.15 & 5.65 & 2.1 \\end{pmatrix}\n$$\nNow, we compute the product $J(u)v$:\n$$\nJ(u)v = \\begin{pmatrix} 0 & 1 & 0 \\\\ -1.8 & 2.4 & 0.4 \\\\ -9.15 & 5.65 & 2.1 \\end{pmatrix} \\begin{pmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 1(-0.2) \\\\ -1.8(0.1) + 2.4(-0.2) + 0.4(0.3) \\\\ -9.15(0.1) + 5.65(-0.2) + 2.1(0.3) \\end{pmatrix}\n$$\n$$\nJ(u)v = \\begin{pmatrix} -0.2 \\\\ -0.18 - 0.48 + 0.12 \\\\ -0.915 - 1.13 + 0.63 \\end{pmatrix} = \\begin{pmatrix} -0.2 \\\\ -0.54 \\\\ -1.415 \\end{pmatrix}\n$$\nThis is the \"exact\" Jacobian-vector product, which the complex-step method computes to machine precision.\n\n**2. Jacobian-Vector Product via Forward Finite-Difference**\n\nThe forward finite-difference approximation is given by:\n$$\nJ(u)v \\approx \\frac{F(u + \\epsilon_{\\mathrm{r}} v) - F(u)}{\\epsilon_{\\mathrm{r}}}\n$$\nwith the given step size $\\epsilon_{\\mathrm{r}} = 1 \\times 10^{-6}$.\nFirst, we compute the unperturbed flux $F(u)$:\n$$\np = 3.1, \\quad F(u) = \\begin{pmatrix} 3 \\\\ \\frac{3^2}{2} + 3.1 \\\\ \\frac{3}{2}(10+3.1) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 7.6 \\\\ 19.65 \\end{pmatrix}\n$$\nNext, we determine the perturbed state $u' = u + \\epsilon_{\\mathrm{r}} v$:\n$$\nu' = \\begin{pmatrix} 2 \\\\ 3 \\\\ 10 \\end{pmatrix} + (1 \\times 10^{-6}) \\begin{pmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 2.0000001 \\\\ 2.9999998 \\\\ 10.0000003 \\end{pmatrix}\n$$\nNow, we evaluate the flux $F(u')$ at the perturbed state. Let $\\rho' = u'_1, m' = u'_2, E' = u'_3$.\n$$\np' = (1.4-1) \\left( E' - \\frac{1}{2}\\frac{(m')^2}{\\rho'} \\right) \\approx 3.100000275\n$$\n$$\nF(u') = \\begin{pmatrix} m' \\\\ \\frac{(m')^2}{\\rho'} + p' \\\\ \\frac{m'}{\\rho'}(E' + p') \\end{pmatrix} \\approx \\begin{pmatrix} 2.9999998 \\\\ 4.499999225 + 3.100000275 \\\\ 1.499999825(10.0000003 + 3.100000275) \\end{pmatrix} \\approx \\begin{pmatrix} 2.9999998 \\\\ 7.5999995 \\\\ 19.649998585 \\end{pmatrix}\n$$\nThe finite-difference approximation is:\n$$\nJ(u)v \\approx \\frac{1}{1 \\times 10^{-6}} \\left( \\begin{pmatrix} 2.9999998 \\\\ 7.5999995 \\\\ 19.649998585 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 7.6 \\\\ 19.65 \\end{pmatrix} \\right) = \\frac{1}{10^{-6}} \\begin{pmatrix} -0.0000002 \\\\ -0.0000005 \\\\ -0.000001415 \\end{pmatrix} = \\begin{pmatrix} -0.2 \\\\ -0.5 \\\\ -1.415 \\end{pmatrix}\n$$\nNote: The calculation for the second and third components was performed using high-precision arithmetic to minimize round-off error during evaluation, revealing the underlying truncation and subtractive cancellation errors. For example, a more precise calculation for the second component gives approximately $-0.53999998$, illustrating the error. The value $-0.5$ is what a standard double-precision calculation might yield due to loss of significance.\n\n**3. Comparative Analysis**\n\n*   **Accuracy:** The complex-step result, $\\begin{pmatrix}-0.2, -0.54, -1.415\\end{pmatrix}^T$, is effectively exact. The forward finite-difference result, $\\begin{pmatrix}-0.2, -0.5, -1.415\\end{pmatrix}^T$ (using a naive calculation), shows significant error in the second component. The true finite difference result is approximately $\\begin{pmatrix}-0.2, -0.5400, -1.4150\\end{pmatrix}^T$, but obtaining this requires careful handling of precision. The method's accuracy is fundamentally limited. The error in forward differencing is $O(\\epsilon_{\\mathrm{r}})$, which is substantial ($10^{-6}$) compared to the complex-step error of $O(\\epsilon_c^2)$, which can be near machine epsilon ($10^{-32}$ for a step size of $10^{-16}$). Finite differences suffer from subtractive cancellation: $F(u + \\epsilon_{\\mathrm{r}} v)$ is very close to $F(u)$, and their difference loses significant digits. The complex-step method avoids this by extracting the derivative from the imaginary part of a single function evaluation.\n\n*   **Practicality:** The finite-difference method is trivial to implement if a function evaluation `F(u)` is available. However, its major drawback is the choice of the step size $\\epsilon_{\\mathrm{r}}$. A large $\\epsilon_{\\mathrm{r}}$ causes large truncation error, while a small $\\epsilon_{\\mathrm{r}}$ causes large subtractive cancellation (round-off) error. This trade-off makes it unreliable for high-accuracy requirements. The complex-step method requires the code for $F(u)$ to support complex arithmetic. For functions composed of standard analytic operations, this is often as simple as changing variable declarations (e.g., `double` to `std::complex<double>` in C++). Since the Euler fluxes are analytic, this is a prime application. The choice of $\\epsilon_c$ is not a sensitive parameter; any sufficiently small number (e.g., $10^{-20}$) works well. Given its superior accuracy and robustness, the complex-step method is highly practical and often preferred for matrix-free solvers where accurate Jacobian-vector products are essential for robust convergence.\n\n**4. Euclidean Norm of the Complex-Step Result**\n\nLet $w = J(u)v$ as computed by the complex-step method.\n$$\nw = \\begin{pmatrix} -0.2 \\\\ -0.54 \\\\ -1.415 \\end{pmatrix}\n$$\nThe Euclidean norm is $\\|w\\|_2 = \\sqrt{w_1^2 + w_2^2 + w_3^2}$.\n$$\n\\|w\\|_2 = \\sqrt{(-0.2)^2 + (-0.54)^2 + (-1.415)^2}\n$$\n$$\n\\|w\\|_2 = \\sqrt{0.04 + 0.2916 + 2.002225} = \\sqrt{2.333825} \\approx 1.5276856\n$$\nRounding to four significant figures, the norm is $1.528$.",
            "answer": "$$\n\\boxed{1.528}\n$$"
        },
        {
            "introduction": "Newton-Krylov methods gain significant efficiency by solving the linear Newton system $J(u)s = -F(u)$ only approximately, or \"inexactly.\" The key is to control this approximation dynamically, tightening the tolerance as we approach the solution. This practice  delves into the popular Eisenstat-Walker adaptive strategy, where you will calculate the forcing term $\\eta_k$ that sets the linear solver's tolerance based on the progress of the outer nonlinear iteration.",
            "id": "3979960",
            "problem": "In a Newton-Krylov method applied to a steady Reynolds-averaged Navier–Stokes residual equation $F(u) = 0$ for an aerospace configuration, the linear subproblem at nonlinear iteration $k$ is solved inexactly according to the inexact Newton condition\n$$\n\\|F(u_k) + J(u_k) s_k\\| \\le \\eta_k \\,\\|F(u_k)\\|,\n$$\nwhere $J(u_k)$ is the Jacobian at the current iterate $u_k$, $s_k$ is the computed step, and $\\eta_k \\in [0,1)$ is a forcing parameter that adaptively controls the accuracy of the Krylov subspace linear solve. An adaptive strategy is employed in which $\\eta_k$ is selected using the Eisenstat–Walker II approach based on the readily available residual norms from consecutive nonlinear iterations.\n\nYou are given the following data at iterations $k-1$ and $k$:\n- The nonlinear residual norms satisfy $\\|F(u_{k-1})\\| = 3.2 \\times 10^{-2}$ and $\\|F(u_k)\\| = 1.6 \\times 10^{-2}$.\n- The Eisenstat–Walker II control parameters are $\\gamma = 0.9$ and $\\alpha = 1.5$.\n- Safeguards are imposed so that $\\eta_k \\in [\\eta_{\\min}, \\eta_{\\max}]$ with $\\eta_{\\min} = 1.0 \\times 10^{-4}$ and $\\eta_{\\max} = 0.9$.\n\nStarting from the inexact Newton definition above and the principle that the forcing parameter should relax when the nonlinear residual is not decreasing rapidly and tighten as the residual approaches zero, determine the adaptive $\\eta_k$ according to the Eisenstat–Walker II strategy using only the given residual norms and parameters, and apply the given safeguards. Briefly justify any safeguards you apply based on stability considerations when the nonlinear residual stagnates or increases across iterations.\n\nReport the final numerical value of $\\eta_k$ as a single real number, rounded to four significant figures. No units are required.",
            "solution": "The problem requires the determination of the adaptive forcing parameter $\\eta_k$ for an inexact Newton-Krylov method using the Eisenstat–Walker II strategy. We begin by validating the problem statement.\n\n### Step 1: Extract Givens\n- The nonlinear system to be solved is the steady Reynolds-averaged Navier–Stokes (RANS) residual equation, denoted as $F(u) = 0$.\n- The inexact Newton condition at nonlinear iteration $k$ is $\\|F(u_k) + J(u_k) s_k\\| \\le \\eta_k \\,\\|F(u_k)\\|$, where $u_k$ is the current state, $J(u_k)$ is the Jacobian, $s_k$ is the update step, and $\\eta_k \\in [0,1)$ is the forcing parameter.\n- The adaptive strategy for selecting $\\eta_k$ is the Eisenstat–Walker II approach.\n- The nonlinear residual norm at iteration $k-1$ is $\\|F(u_{k-1})\\| = 3.2 \\times 10^{-2}$.\n- The nonlinear residual norm at iteration $k$ is $\\|F(u_k)\\| = 1.6 \\times 10^{-2}$.\n- The Eisenstat–Walker II control parameter $\\gamma$ is $0.9$.\n- The Eisenstat–Walker II control parameter $\\alpha$ is $1.5$.\n- Safeguards are imposed such that $\\eta_k$ must lie in the interval $[\\eta_{\\min}, \\eta_{\\max}]$.\n- The minimum safeguard value is $\\eta_{\\min} = 1.0 \\times 10^{-4}$.\n- The maximum safeguard value is $\\eta_{\\max} = 0.9$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is grounded in the well-established field of numerical methods for computational fluid dynamics (CFD). The Newton-Krylov method, RANS equations, and the Eisenstat–Walker adaptive forcing strategy are standard, rigorously defined concepts in scientific computing. All aspects are consistent with fundamental principles of numerical analysis.\n- **Well-Posed**: The problem is well-posed. It provides all necessary numerical values and a clearly defined algorithm (Eisenstat–Walker II) to compute a unique value for $\\eta_k$.\n- **Objective**: The problem is stated using precise, objective, and standard terminology from the relevant field. There are no subjective or ambiguous statements.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\nThe core of the Eisenstat–Walker II strategy is to adapt the tolerance for the linear solve, represented by $\\eta_k$, based on the observed rate of nonlinear convergence. A common formulation for this strategy, which uses the ratio of successive nonlinear residual norms as a proxy for the convergence factor, is given by:\n$$\n\\eta_k^{\\text{EW}} = \\gamma \\left( \\frac{\\|F(u_k)\\|}{\\|F(u_{k-1})\\|} \\right)^\\alpha\n$$\nThis formula adjusts $\\eta_k$ such that if the nonlinear residual is decreasing rapidly (i.e., the ratio $\\|F(u_k)\\|/\\|F(u_{k-1})\\|$ is small), the linear system can be solved more accurately (smaller $\\eta_k$) to maintain superlinear convergence. Conversely, if the nonlinear convergence is slow (the ratio is close to $1$), the linear solve tolerance is relaxed (larger $\\eta_k$) to avoid wasted computational effort.\n\nWe are given the following values:\n- $\\|F(u_k)\\| = 1.6 \\times 10^{-2}$\n- $\\|F(u_{k-1})\\| = 3.2 \\times 10^{-2}$\n- $\\gamma = 0.9$\n- $\\alpha = 1.5$\n\nFirst, we compute the ratio of the consecutive nonlinear residual norms:\n$$\n\\frac{\\|F(u_k)\\|}{\\|F(u_{k-1})\\|} = \\frac{1.6 \\times 10^{-2}}{3.2 \\times 10^{-2}} = 0.5\n$$\nNow, we substitute this ratio and the given parameters into the Eisenstat–Walker formula to calculate the proposed forcing term, which we denote $\\eta_k^{\\text{EW}}$:\n$$\n\\eta_k^{\\text{EW}} = (0.9) \\times (0.5)^{1.5}\n$$\nWe can evaluate this expression:\n$$\n(0.5)^{1.5} = (0.5)^{1} \\times (0.5)^{0.5} = 0.5 \\times \\sqrt{0.5} = 0.5 \\times \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{4}\n$$\nNumerically, $\\frac{\\sqrt{2}}{4} \\approx \\frac{1.41421356}{4} \\approx 0.35355339$.\nTherefore,\n$$\n\\eta_k^{\\text{EW}} \\approx 0.9 \\times 0.35355339 \\approx 0.31819805\n$$\nThe next step is to apply the safeguards. The problem states that $\\eta_k$ must be within the range $[\\eta_{\\min}, \\eta_{\\max}]$, where $\\eta_{\\min} = 1.0 \\times 10^{-4} = 0.0001$ and $\\eta_{\\max} = 0.9$. The final value of $\\eta_k$ is determined by clamping the calculated value $\\eta_k^{\\text{EW}}$ to this range:\n$$\n\\eta_k = \\min(\\max(\\eta_k^{\\text{EW}}, \\eta_{\\min}), \\eta_{\\max})\n$$\nSubstituting our calculated value:\n$$\n\\eta_k = \\min(\\max(0.31819805, 0.0001), 0.9)\n$$\nSince $0.31819805 > 0.0001$, the inner $\\max$ function evaluates to $0.31819805$.\n$$\n\\eta_k = \\min(0.31819805, 0.9)\n$$\nSince $0.31819805 < 0.9$, the final value is:\n$$\n\\eta_k \\approx 0.31819805\n$$\nThe problem also requires a brief justification of the safeguards. In this specific calculation, the residual is decreasing ($0.5 < 1$), so the primary safeguards $\\eta_{\\min}$ and $\\eta_{\\max}$ are the ones to consider. Had the nonlinear residual stagnated or increased, i.e., $\\|F(u_k)\\| \\ge \\|F(u_{k-1})\\|$, the ratio would be $\\ge 1$. In such a scenario, the raw formula for $\\eta_k^{\\text{EW}}$ could yield a value greater than $\\gamma$, and potentially greater than $1$. A value of $\\eta_k \\ge 1$ violates the condition for a convergent inexact Newton method. The safeguard $\\eta_k \\le \\eta_{\\max}$ is therefore critical for stability. Setting $\\eta_k$ to a high value like $\\eta_{\\max}=0.9$ when convergence falters is a standard practice; it prevents wasting computational effort on a precise linear solve when the underlying linear model of the nonlinear function is poor, and relies on a globalization method (e.g., line search) to recover. In our case, this safeguard is not triggered in a corrective fashion but simply acts as an upper bound, which our computed value is well below. The safeguard $\\eta_{\\min}$ prevents over-solving the linear system, which would be computationally expensive for a negligible gain in nonlinear convergence speed, especially when approaching machine precision.\n\nThe problem asks for the final numerical value of $\\eta_k$ rounded to four significant figures.\nRounding $0.31819805$ to four significant figures gives $0.3182$.",
            "answer": "$$\n\\boxed{0.3182}\n$$"
        },
        {
            "introduction": "With an understanding of the matrix-free linear solve and its adaptive control, we can analyze the behavior of the overall nonlinear method. The hallmark of Newton's method is its rapid local convergence. This problem  explores this celebrated quadratic convergence, asking you to derive a theoretical bound on the number of iterations required to reach a desired tolerance and connecting the abstract theory to practical performance observations.",
            "id": "3979912",
            "problem": "Consider a steady-state discretization of the compressible Navier–Stokes equations in an aerospace Computational Fluid Dynamics (CFD) solver, leading to a nonlinear residual map $F:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{m}$ with the goal $F(u^{\\star})=0$. The Newton-Krylov method forms iterates $u_{k+1}$ by approximately solving the linearization of $F$ at $u_{k}$ using a Krylov subspace method, and, under exact inner solves and standard smoothness and invertibility assumptions on the Jacobian, exhibits local quadratic convergence. Assume that near $u^{\\star}$, the local behavior of the residual norm can be controlled by a single contraction constant, and that all required derivatives exist and are locally Lipschitz, with an invertible Jacobian at $u^{\\star}$.\n\nStarting from the definition of Newton’s method and these regularity assumptions, derive a bound on the minimal number of Newton steps $N$ required to ensure $\\|F(u_{N})\\|\\leq \\tau$ in terms of the initial residual $r_{0}=\\|F(u_{0})\\|$, the tolerance $\\tau$, and a local quadratic contraction constant $\\gamma\\in(0,1)$ that captures the effect of Jacobian invertibility and Lipschitz continuity in translating error contraction to residual contraction. Then, using the derived bound, compute $N$ for two CFD meshes:\n\n- Coarse mesh: $r_{0,\\mathrm{c}}=1.5\\times 10^{-1}$ and $\\gamma_{\\mathrm{c}}=8.0\\times 10^{-1}$.\n- Fine mesh: $r_{0,\\mathrm{f}}=2.0\\times 10^{-1}$ and $\\gamma_{\\mathrm{f}}=1.0\\times 10^{-1}$.\n\nThe residual tolerance is $\\tau=1.0\\times 10^{-10}$, and the iterates are within the quadratic regime for all $k\\leq N$. For context, empirical Newton step counts observed in production CFD runs are $N_{\\mathrm{obs,c}}=5$ on the coarse mesh and $N_{\\mathrm{obs,f}}=3$ on the fine mesh. Report the theoretical minimal integer bounds for $(N_{\\mathrm{c}},N_{\\mathrm{f}})$ as a row matrix. No rounding is required beyond selecting the minimal integers implied by the bound. The final answer must be a calculation.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- A nonlinear residual map $F:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{m}$ derived from a steady-state CFD discretization.\n- The goal is to find $u^{\\star}$ such that $F(u^{\\star})=0$.\n- The method is Newton-Krylov, assumed to exhibit local quadratic convergence.\n- The local behavior of the residual norm $\\|F(u)\\|$ is governed by a single quadratic contraction constant, $\\gamma$.\n- It is assumed that all necessary derivatives exist, are locally Lipschitz, and the Jacobian at $u^{\\star}$ is invertible.\n- The goal is to derive a bound for the minimal number of Newton steps $N$ to ensure $\\|F(u_{N})\\| \\leq \\tau$.\n- Inputs for the derivation are the initial residual norm $r_0 = \\|F(u_0)\\|$, the tolerance $\\tau$, and the contraction constant $\\gamma$.\n- The iterates are assumed to be within the quadratic convergence regime for all steps considered.\n- Data for coarse mesh (c): $r_{0,\\mathrm{c}}=1.5\\times 10^{-1}$, $\\gamma_{\\mathrm{c}}=8.0\\times 10^{-1}$.\n- Data for fine mesh (f): $r_{0,\\mathrm{f}}=2.0\\times 10^{-1}$, $\\gamma_{\\mathrm{f}}=1.0\\times 10^{-1}$.\n- Residual tolerance: $\\tau=1.0\\times 10^{-10}$.\n- Required output: The minimal integer bounds for $(N_{\\mathrm{c}}, N_{\\mathrm{f}})$ as a row matrix.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing the local convergence analysis of Newton's method, a fundamental topic in numerical analysis. The assumptions, such as the existence of a single constant $\\gamma$ to model the quadratic convergence of the residual norm, represent a standard simplification used for such analysis. The problem is well-posed, providing all necessary information to derive the bound and compute the required values. It is not incomplete or contradictory; the condition that iterates are \"within the quadratic regime\" implies that the initial state is close enough to the solution for the quadratic model to apply. The problem is objective and uses precise terminology from the field of numerical methods for PDEs.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\nThe problem requires us to first derive a general formula for the minimum number of Newton iterations, $N$, needed to reduce the residual norm to a specified tolerance $\\tau$. The core of the derivation rests on the provided model for quadratic convergence.\n\nLet $r_{k} = \\|F(u_{k})\\|$ denote the norm of the residual at the $k$-th iteration. The problem states that the convergence is quadratically controlled by a constant $\\gamma$, which can be expressed as the following inequality:\n$$r_{k+1} \\leq \\gamma r_{k}^{2}$$\nTo proceed, we seek a non-recursive expression for $r_{k}$ in terms of the initial residual $r_{0}$. A common technique is to scale the inequality. Let us define a scaled residual $s_{k} = \\gamma r_{k}$. Multiplying the inequality by $\\gamma$ gives:\n$$\\gamma r_{k+1} \\leq \\gamma (\\gamma r_{k}^{2})$$\n$$s_{k+1} \\leq (\\gamma r_{k})^{2} = s_{k}^{2}$$\nThis simplified recurrence, $s_{k+1} \\leq s_{k}^{2}$, can be unrolled by repeated application:\n$$s_{1} \\leq s_{0}^{2}$$\n$$s_{2} \\leq s_{1}^{2} \\leq (s_{0}^{2})^{2} = s_{0}^{4} = s_{0}^{2^{2}}$$\n$$s_{3} \\leq s_{2}^{2} \\leq (s_{0}^{4})^{2} = s_{0}^{8} = s_{0}^{2^{3}}$$\nBy induction, we obtain the general formula for the $k$-th scaled residual:\n$$s_{k} \\leq s_{0}^{2^{k}}$$\nFor convergence to occur, the scaled initial residual must satisfy $s_{0} < 1$, which is equivalent to $\\gamma r_{0} < 1$. This is consistent with the assumption that the iterates are in the quadratic convergence regime.\n\nSubstituting back $r_{k} = s_{k}/\\gamma$ and $r_{0} = s_{0}/\\gamma$, we recover the bound for the unscaled residual norm:\n$$\\gamma r_{k} \\leq (\\gamma r_{0})^{2^{k}}$$\n$$r_{k} \\leq \\frac{1}{\\gamma} (\\gamma r_{0})^{2^{k}}$$\nWe are required to find the minimal integer $N$ such that $r_{N} \\leq \\tau$. We apply this condition to our derived bound:\n$$\\frac{1}{\\gamma}(\\gamma r_{0})^{2^{N}} \\leq \\tau$$\nTo solve for $N$, we first isolate the term containing the exponent:\n$$(\\gamma r_{0})^{2^{N}} \\leq \\gamma \\tau$$\nTaking the natural logarithm of both sides, we have:\n$$2^{N} \\ln(\\gamma r_{0}) \\leq \\ln(\\gamma \\tau)$$\nSince convergence requires $\\gamma r_{0} < 1$, the term $\\ln(\\gamma r_{0})$ is negative. Therefore, dividing by it reverses the inequality sign:\n$$2^{N} \\geq \\frac{\\ln(\\gamma \\tau)}{\\ln(\\gamma r_{0})}$$\nThis can be written in a more intuitive form using properties of logarithms:\n$$2^{N} \\geq \\frac{-\\ln(1/(\\gamma \\tau))}{-\\ln(1/(\\gamma r_{0}))} = \\frac{\\ln(1/(\\gamma \\tau))}{\\ln(1/(\\gamma r_{0}))}$$\nTo solve for $N$, we take the base-2 logarithm of both sides:\n$$N \\geq \\log_{2}\\left(\\frac{\\ln(1/(\\gamma \\tau))}{\\ln(1/(\\gamma r_{0}))}\\right)$$\nSince $N$ must be an integer, the minimal number of steps is the ceiling of the expression on the right-hand side:\n$$N = \\left\\lceil \\log_{2}\\left(\\frac{\\ln(1/(\\gamma \\tau))}{\\ln(1/(\\gamma r_{0}))}\\right) \\right\\rceil$$\nWe now apply this formula to the two specified cases.\n\nFor the coarse mesh (subscript 'c'):\nThe given parameters are $r_{0,\\mathrm{c}} = 1.5 \\times 10^{-1}$, $\\gamma_{\\mathrm{c}} = 8.0 \\times 10^{-1}$, and $\\tau = 1.0 \\times 10^{-10}$.\nFirst, we compute the necessary intermediate products:\n$$\\gamma_{\\mathrm{c}} r_{0,\\mathrm{c}} = (8.0 \\times 10^{-1}) (1.5 \\times 10^{-1}) = 0.12$$\n$$\\gamma_{\\mathrm{c}} \\tau = (8.0 \\times 10^{-1}) (1.0 \\times 10^{-10}) = 8.0 \\times 10^{-11}$$\nThe argument of the $\\log_{2}$ function is:\n$$\\frac{\\ln(1/(\\gamma_{\\mathrm{c}} \\tau))}{\\ln(1/(\\gamma_{\\mathrm{c}} r_{0,\\mathrm{c}}))} = \\frac{\\ln(1/(8.0 \\times 10^{-11}))}{\\ln(1/0.12)} \\approx \\frac{23.24906}{2.12026} \\approx 10.9651$$\nThe minimal number of steps $N_{\\mathrm{c}}$ is:\n$$N_{\\mathrm{c}} = \\lceil \\log_{2}(10.9651) \\rceil = \\left\\lceil \\frac{\\ln(10.9651)}{\\ln(2)} \\right\\rceil \\approx \\lceil 3.4546 \\rceil = 4$$\n\nFor the fine mesh (subscript 'f'):\nThe given parameters are $r_{0,\\mathrm{f}} = 2.0 \\times 10^{-1}$, $\\gamma_{\\mathrm{f}} = 1.0 \\times 10^{-1}$, and $\\tau = 1.0 \\times 10^{-10}$.\nFirst, we compute the necessary intermediate products:\n$$\\gamma_{\\mathrm{f}} r_{0,\\mathrm{f}} = (1.0 \\times 10^{-1}) (2.0 \\times 10^{-1}) = 0.02$$\n$$\\gamma_{\\mathrm{f}} \\tau = (1.0 \\times 10^{-1}) (1.0 \\times 10^{-10}) = 1.0 \\times 10^{-11}$$\nThe argument of the $\\log_{2}$ function is:\n$$\\frac{\\ln(1/(\\gamma_{\\mathrm{f}} \\tau))}{\\ln(1/(\\gamma_{\\mathrm{f}} r_{0,\\mathrm{f}}))} = \\frac{\\ln(1/(1.0 \\times 10^{-11}))}{\\ln(1/0.02)} = \\frac{\\ln(10^{11})}{\\ln(50)} \\approx \\frac{25.32843}{3.91202} \\approx 6.4745$$\nThe minimal number of steps $N_{\\mathrm{f}}$ is:\n$$N_{\\mathrm{f}} = \\lceil \\log_{2}(6.4745) \\rceil = \\left\\lceil \\frac{\\ln(6.4745)}{\\ln(2)} \\right\\rceil \\approx \\lceil 2.6948 \\rceil = 3$$\nThe theoretically required minimal integer numbers of Newton steps for the coarse and fine meshes are $4$ and $3$, respectively. The results are to be provided as a row matrix $(N_{\\mathrm{c}}, N_{\\mathrm{f}})$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 4 & 3 \\end{pmatrix}}\n$$"
        }
    ]
}