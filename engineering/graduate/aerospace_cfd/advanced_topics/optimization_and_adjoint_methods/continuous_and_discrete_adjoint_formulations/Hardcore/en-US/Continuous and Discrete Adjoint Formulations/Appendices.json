{
    "hands_on_practices": [
        {
            "introduction": "The discrete adjoint method can be viewed as the systematic application of the chain rule to the algorithm that computes the objective function. This process is formalized as reverse-mode Automatic Differentiation (AD). This first practice demystifies the process by guiding you through a manual AD \"by hand\" for a common numerical flux calculation . By tracing the flow of derivatives backward through the computational graph, you will build a foundational intuition for how adjoint variables are propagated and accumulated to compute the exact gradient of an output with respect to its inputs.",
            "id": "3950963",
            "problem": "Consider a one-dimensional, nondimensional scalar conservation law representative of a flux computation routine used in Computational Fluid Dynamics (CFD), where the numerical interface flux is constructed using the Rusanov (local Laxâ€“Friedrichs) method. Let the left and right states be $u_{L}$ and $u_{R}$. The primitive flux function is $f(u) = \\frac{1}{2} u^{2}$. The characteristic speeds are $a_{L} = |u_{L}|$ and $a_{R} = |u_{R}|$, and the spectral radius is $a_{\\max} = \\max(a_{L}, a_{R})$. The Rusanov flux is\n$$\nF = \\frac{1}{2}\\big(f(u_{L}) + f(u_{R})\\big) - \\frac{1}{2} a_{\\max} \\big(u_{R} - u_{L}\\big).\n$$\nDefine a scalar objective functional\n$$\nJ = w F + c F^{2},\n$$\nwhere $w$ and $c$ are fixed nondimensional coefficients.\n\nStarting from the chain rule of calculus and the definition of reverse-mode Automatic Differentiation (AD), which is the discrete adjoint of a computational graph, instrument the above flux computation routine for reverse-mode AD and derive the adjoint (reverse-mode) updates for each intermediate variable in the computational graph, including $f(u_{L})$, $f(u_{R})$, $a_{L}$, $a_{R}$, $a_{\\max}$, $F$, and the inputs $u_{L}$ and $u_{R}$. Assume standard differentiability away from kinks of $|\\cdot|$ and $\\max(\\cdot,\\cdot)$, and adopt the active-branch derivative for $\\max$ determined by the recorded primal values.\n\nFor a specific evaluation, take $u_{L} = 1.7$, $u_{R} = -0.9$, $w = 1.2$, and $c = 0.3$. Using the derived adjoint updates, compute the gradient $\\nabla J = \\big(\\frac{\\partial J}{\\partial u_{L}}, \\frac{\\partial J}{\\partial u_{R}}\\big)$ as numerical values. Round each component of the final gradient to six significant figures. The variables are nondimensional; no physical units are required in your answer. Express the final gradient as a row matrix.",
            "solution": "The user wants to find the gradient of an objective functional $J$ with respect to the input states $u_L$ and $u_R$. This will be accomplished using reverse-mode Automatic Differentiation (AD), also known as the discrete adjoint method. The process involves two stages: a forward pass to compute the value of $J$ and store all intermediate variables, followed by a reverse pass to propagate the derivatives from the output $J$ back to the inputs $u_L$ and $u_R$.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **States**: $u_{L}$ and $u_{R}$\n- **Primitive Flux Function**: $f(u) = \\frac{1}{2} u^{2}$\n- **Characteristic Speeds**: $a_{L} = |u_{L}|$, $a_{R} = |u_{R}|$\n- **Spectral Radius**: $a_{\\max} = \\max(a_{L}, a_{R})$\n- **Rusanov Flux**: $F = \\frac{1}{2}\\big(f(u_{L}) + f(u_{R})\\big) - \\frac{1}{2} a_{\\max} \\big(u_{R} - u_{L}\\big)$\n- **Objective Functional**: $J = w F + c F^{2}$\n- **Fixed Coefficients**: $w$, $c$\n- **Numerical Values**: $u_{L} = 1.7$, $u_{R} = -0.9$, $w = 1.2$, $c = 0.3$\n- **Task**: Derive adjoint updates and compute the numerical gradient $\\nabla J = \\big(\\frac{\\partial J}{\\partial u_{L}}, \\frac{\\partial J}{\\partial u_{R}}\\big)$.\n- **AD Assumption**: Use the active-branch derivative for non-differentiable functions.\n\n**1.2. Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it deals with standard concepts in computational fluid dynamics (Rusanov flux) and numerical optimization (reverse-mode AD). It is well-posed, providing all necessary equations and data for a unique solution. The language is objective and precise. The problem does not violate any of the invalidity criteria.\n\n**1.3. Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Step 2: Solution Derivation\n\nThe solution process follows the structure of reverse-mode AD.\n\n**A. Forward Pass: Computational Graph**\n\nThe computation of $J$ from inputs $u_L$ and $u_R$ can be broken down into a sequence of elementary operations, forming a computational graph.\n\n1.  $f_L = \\frac{1}{2} u_L^2$\n2.  $f_R = \\frac{1}{2} u_R^2$\n3.  $a_L = |u_L|$\n4.  $a_R = |u_R|$\n5.  $a_{\\max} = \\max(a_L, a_R)$\n6.  $F = \\frac{1}{2}(f_L + f_R) - \\frac{1}{2} a_{\\max} (u_R - u_L)$\n7.  $J = w F + c F^2$\n\n**B. Reverse Pass: Adjoint Updates Derivation**\n\nThe reverse pass computes the adjoints, denoted by a bar over the variable (e.g., $\\bar{v} = \\frac{\\partial J}{\\partial v}$), by traversing the computational graph backward. The chain rule is applied at each step. We initialize all adjoints to $0$ except for the final output, $\\bar{J} = \\frac{\\partial J}{\\partial J} = 1$.\n\n1.  **Adjoint of F**: From $J = w F + c F^2$, we compute $\\bar{F}$.\n    $$ \\bar{F} = \\bar{J} \\frac{dJ}{dF} = 1 \\cdot (w + 2cF) = w + 2cF $$\n\n2.  **Adjoints from F**: From $F = \\frac{1}{2}(f_L + f_R) - \\frac{1}{2} a_{\\max} (u_R - u_L)$, we propagate $\\bar{F}$ to its arguments. Adjoint updates are cumulative.\n    $$ \\bar{f_L} \\mathrel{+}= \\bar{F} \\frac{\\partial F}{\\partial f_L} = \\frac{1}{2} \\bar{F} $$\n    $$ \\bar{f_R} \\mathrel{+}= \\bar{F} \\frac{\\partial F}{\\partial f_R} = \\frac{1}{2} \\bar{F} $$\n    $$ \\bar{a}_{\\max} \\mathrel{+}= \\bar{F} \\frac{\\partial F}{\\partial a_{\\max}} = -\\frac{1}{2}(u_R - u_L) \\bar{F} $$\n    The function $F$ also has explicit dependencies on $u_L$ and $u_R$.\n    $$ \\bar{u}_L \\mathrel{+}= \\bar{F} \\frac{\\partial F}{\\partial u_L} = \\frac{1}{2} a_{\\max} \\bar{F} $$\n    $$ \\bar{u}_R \\mathrel{+}= \\bar{F} \\frac{\\partial F}{\\partial u_R} = -\\frac{1}{2} a_{\\max} \\bar{F} $$\n\n3.  **Adjoints from $a_{\\max}$**: From $a_{\\max} = \\max(a_L, a_R)$, we propagate $\\bar{a}_{\\max}$ based on the active branch determined during the forward pass.\n    If $a_L > a_R$:\n    $$ \\bar{a}_L \\mathrel{+}= \\bar{a}_{\\max} \\frac{\\partial a_{\\max}}{\\partial a_L} = \\bar{a}_{\\max} \\cdot 1 $$\n    $$ \\bar{a}_R \\mathrel{+}= \\bar{a}_{\\max} \\frac{\\partial a_{\\max}}{\\partial a_R} = \\bar{a}_{\\max} \\cdot 0 = 0 $$\n    If $a_R > a_L$:\n    $$ \\bar{a}_L \\mathrel{+}= \\bar{a}_{\\max} \\frac{\\partial a_{\\max}}{\\partial a_L} = \\bar{a}_{\\max} \\cdot 0 = 0 $$\n    $$ \\bar{a}_R \\mathrel{+}= \\bar{a}_{\\max} \\frac{\\partial a_{\\max}}{\\partial a_R} = \\bar{a}_{\\max} \\cdot 1 $$\n\n4.  **Adjoints from $a_L$ and $a_R$**: From $a_L = |u_L|$ and $a_R = |u_R|$, we use the derivative $\\frac{d|x|}{dx} = \\text{sgn}(x)$ for $x \\neq 0$.\n    $$ \\bar{u}_L \\mathrel{+}= \\bar{a}_L \\frac{da_L}{du_L} = \\bar{a}_L \\cdot \\text{sgn}(u_L) $$\n    $$ \\bar{u}_R \\mathrel{+}= \\bar{a}_R \\frac{da_R}{du_R} = \\bar{a}_R \\cdot \\text{sgn}(u_R) $$\n\n5.  **Adjoints from $f_L$ and $f_R$**: From $f_L = \\frac{1}{2}u_L^2$ and $f_R = \\frac{1}{2}u_R^2$.\n    $$ \\bar{u}_L \\mathrel{+}= \\bar{f}_L \\frac{df_L}{du_L} = \\bar{f}_L \\cdot u_L $$\n    $$ \\bar{u}_R \\mathrel{+}= \\bar{f}_R \\frac{df_R}{du_R} = \\bar{f}_R \\cdot u_R $$\n\nThe final values of $\\bar{u}_L$ and $\\bar{u}_R$ after all contributions have been accumulated are the desired gradient components, $\\frac{\\partial J}{\\partial u_L}$ and $\\frac{\\partial J}{\\partial u_R}$.\n\n**C. Numerical Computation**\n\nWe now apply these steps with the given numerical values: $u_{L} = 1.7$, $u_{R} = -0.9$, $w = 1.2$, $c = 0.3$.\n\n**Forward Pass Execution**:\n- $u_L = 1.7$\n- $u_R = -0.9$\n- $f_L = \\frac{1}{2}(1.7)^2 = \\frac{1}{2}(2.89) = 1.445$\n- $f_R = \\frac{1}{2}(-0.9)^2 = \\frac{1}{2}(0.81) = 0.405$\n- $a_L = |1.7| = 1.7$\n- $a_R = |-0.9| = 0.9$\n- $a_{\\max} = \\max(1.7, 0.9) = 1.7$\n- $F = \\frac{1}{2}(1.445 + 0.405) - \\frac{1}{2}(1.7)(-0.9 - 1.7) = \\frac{1}{2}(1.85) - (0.85)(-2.6) = 0.925 + 2.21 = 3.135$\n- $J = 1.2(3.135) + 0.3(3.135)^2 = 3.762 + 0.3(9.828225) = 3.762 + 2.9484675 = 6.7104675$\n\n**Reverse Pass Execution**:\nInitialize all adjoints to $0$. Let $\\bar{u}_L$ and $\\bar{u}_R$ be the accumulators for the final gradient.\n\n1.  **Start with $\\bar{J}=1$**:\n    $\\bar{F} = w + 2cF = 1.2 + 2(0.3)(3.135) = 1.2 + 0.6(3.135) = 1.2 + 1.881 = 3.081$.\n\n2.  **Propagate $\\bar{F}$**:\n    $\\bar{f_L} = \\frac{1}{2}\\bar{F} = \\frac{1}{2}(3.081) = 1.5405$.\n    $\\bar{f_R} = \\frac{1}{2}\\bar{F} = \\frac{1}{2}(3.081) = 1.5405$.\n    $\\bar{a}_{\\max} = -\\frac{1}{2}(u_R - u_L)\\bar{F} = -\\frac{1}{2}(-0.9 - 1.7)(3.081) = -\\frac{1}{2}(-2.6)(3.081) = 1.3(3.081) = 4.0053$.\n    Contribution to $\\bar{u}_L$: $\\frac{1}{2}a_{\\max}\\bar{F} = \\frac{1}{2}(1.7)(3.081) = 0.85(3.081) = 2.61885$. So, $\\bar{u}_L = 2.61885$.\n    Contribution to $\\bar{u}_R$: $-\\frac{1}{2}a_{\\max}\\bar{F} = -\\frac{1}{2}(1.7)(3.081) = -0.85(3.081) = -2.61885$. So, $\\bar{u}_R = -2.61885$.\n\n3.  **Propagate $\\bar{a}_{\\max}$**:\n    Since $a_L = 1.7 > a_R = 0.9$, the active branch is $a_L$.\n    $\\bar{a}_L = \\bar{a}_{\\max} \\cdot 1 = 4.0053$.\n    $\\bar{a}_R = \\bar{a}_{\\max} \\cdot 0 = 0$.\n\n4.  **Propagate $\\bar{a}_L$ and $\\bar{a}_R$**:\n    $\\text{sgn}(u_L) = \\text{sgn}(1.7) = 1$. Contribution to $\\bar{u}_L$: $\\bar{a}_L \\cdot \\text{sgn}(u_L) = 4.0053 \\cdot 1 = 4.0053$.\n    $\\bar{u}_L \\mathrel{+}= 4.0053 \\implies \\bar{u}_L = 2.61885 + 4.0053 = 6.62415$.\n    $\\text{sgn}(u_R) = \\text{sgn}(-0.9) = -1$. Contribution to $\\bar{u}_R$: $\\bar{a}_R \\cdot \\text{sgn}(u_R) = 0 \\cdot (-1) = 0$.\n    $\\bar{u}_R \\mathrel{+}= 0 \\implies \\bar{u}_R = -2.61885 + 0 = -2.61885$.\n\n5.  **Propagate $\\bar{f}_L$ and $\\bar{f}_R$**:\n    Contribution to $\\bar{u}_L$: $\\bar{f}_L \\cdot u_L = 1.5405 \\cdot 1.7 = 2.61885$.\n    $\\bar{u}_L \\mathrel{+}= 2.61885 \\implies \\bar{u}_L = 6.62415 + 2.61885 = 9.243$.\n    Contribution to $\\bar{u}_R$: $\\bar{f}_R \\cdot u_R = 1.5405 \\cdot (-0.9) = -1.38645$.\n    $\\bar{u}_R \\mathrel{+}= -1.38645 \\implies \\bar{u}_R = -2.61885 - 1.38645 = -4.0053$.\n\nThe final values for the gradient components are:\n$$ \\frac{\\partial J}{\\partial u_L} = 9.243 $$\n$$ \\frac{\\partial J}{\\partial u_R} = -4.0053 $$\nRounding each component to six significant figures gives $9.24300$ and $-4.00530$.\n\nThe gradient is $\\nabla J = \\begin{pmatrix} 9.24300 & -4.00530 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n9.24300 & -4.00530\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building on the mechanics of reverse-mode AD, we now apply these principles to a more complex and realistic component of a modern finite-volume solver: a high-resolution MUSCL reconstruction. Such schemes introduce non-linear dependencies through slope ratios and non-differentiability through flux limiters . This exercise will give you hands-on experience in deriving the discrete adjoint contributions for a specific flux, highlighting how to manage the intricate chain of dependencies and prompting a crucial discussion on handling the non-differentiable \"kinks\" that are ubiquitous in practical CFD codes.",
            "id": "3950940",
            "problem": "Consider the one-dimensional steady linear advection equation with constant speed $a>0$ for a scalar state $u(x)$, discretized by a uniform finite-volume mesh. At the cell face $i+\\tfrac{1}{2}$, use a Monotonic Upstream-centered Schemes for Conservation Laws (MUSCL) reconstruction with the minmod flux limiter to define the left-biased state $u_{L,i+\\frac{1}{2}}$ as follows: \n- Define the forward and backward differences $\\Delta^{+} u_{i} \\equiv u_{i+1}-u_{i}$ and $\\Delta^{-} u_{i} \\equiv u_{i}-u_{i-1}$, and the ratio $r_{i} \\equiv \\dfrac{\\Delta^{-} u_{i}}{\\Delta^{+} u_{i}}$ (assume $\\Delta^{+} u_{i} \\neq 0$).\n- Let the minmod limiter be $\\phi_{\\mathrm{mm}}(r) \\equiv \\max\\!\\big(0,\\min(1,r)\\big)$.\n- The MUSCL left state is $u_{L,i+\\frac{1}{2}} \\equiv u_{i} + \\tfrac{1}{2}\\,\\phi_{\\mathrm{mm}}(r_{i})\\,\\Delta^{+} u_{i}$.\n- The upwind numerical flux at the face is $F_{i+\\frac{1}{2}} \\equiv a\\,u_{L,i+\\frac{1}{2}}$.\n\nLet the objective be a linear functional of this face flux, $J \\equiv \\alpha\\,F_{i+\\frac{1}{2}}$, where $\\alpha$ is a given scalar weight. In the discrete adjoint sense, the contribution of this face to the local adjoint right-hand side is governed by the gradient $\\nabla_{\\mathbf{u}} J$ with respect to the triplet of conservative cell values $\\mathbf{u} \\equiv (u_{i-1},u_{i},u_{i+1})$ participating in the reconstruction stencil.\n\nTasks:\n1) Starting from the above definitions and the chain rule, and assuming $0<r_{i}<1$ so that the active branch of the minmod limiter is $\\phi_{\\mathrm{mm}}(r_{i})=r_{i}$, derive the closed-form expression for the discrete adjoint contributions $\\left(\\dfrac{\\partial J}{\\partial u_{i-1}},\\dfrac{\\partial J}{\\partial u_{i}},\\dfrac{\\partial J}{\\partial u_{i+1}}\\right)$ in terms of $a$ and $\\alpha$ only (no dependence on the local state remains in this branch).\n2) Evaluate these contributions numerically for $a=2$, $\\alpha=1$, and the consistent local state $(u_{i-1},u_{i},u_{i+1})=(1,2,5)$, which indeed satisfies $0<r_{i}<1$.\n3) Briefly discuss, in words, how the non-smooth kinks of $\\phi_{\\mathrm{mm}}(r)$ at $r=0$ and $r=1$ impact the existence and uniqueness of the discrete adjoint, and indicate a consistent choice one may adopt at these kinks to maintain an implementable discrete adjoint.\n\nProvide the final numerical contributions from part 2 as a single row matrix representing $\\left(\\dfrac{\\partial J}{\\partial u_{i-1}},\\dfrac{\\partial J}{\\partial u_{i}},\\dfrac{\\partial J}{\\partial u_{i+1}}\\right)$. No units are required. No rounding is necessary.",
            "solution": "The problem asks for the derivation and evaluation of discrete adjoint contributions for a specific objective function related to a MUSCL-reconstructed flux in a finite-volume scheme. It also requires a discussion on the implications of non-differentiable limiter functions.\n\nFirst, the problem must be validated.\n\n### Step 1: Extract Givens\n- Equation: One-dimensional steady linear advection with constant speed $a>0$.\n- Discretization: Uniform finite-volume mesh.\n- State variables: Cell-average values $u_{i-1}$, $u_{i}$, $u_{i+1}$.\n- Definitions:\n  - Forward difference: $\\Delta^{+} u_{i} \\equiv u_{i+1}-u_{i}$\n  - Backward difference: $\\Delta^{-} u_{i} \\equiv u_{i}-u_{i-1}$\n  - Ratio of gradients: $r_{i} \\equiv \\dfrac{\\Delta^{-} u_{i}}{\\Delta^{+} u_{i}}$, assuming $\\Delta^{+} u_{i} \\neq 0$.\n  - Minmod flux limiter function: $\\phi_{\\mathrm{mm}}(r) \\equiv \\max\\!\\big(0,\\min(1,r)\\big)$.\n  - Reconstructed left state: $u_{L,i+\\frac{1}{2}} \\equiv u_{i} + \\tfrac{1}{2}\\,\\phi_{\\mathrm{mm}}(r_{i})\\,\\Delta^{+} u_{i}$.\n  - Upwind numerical flux: $F_{i+\\frac{1}{2}} \\equiv a\\,u_{L,i+\\frac{1}{2}}$.\n- Objective functional: $J \\equiv \\alpha\\,F_{i+\\frac{1}{2}}$, where $\\alpha$ is a scalar weight.\n- Target gradient: $\\nabla_{\\mathbf{u}} J = \\left(\\dfrac{\\partial J}{\\partial u_{i-1}},\\dfrac{\\partial J}{\\partial u_{i}},\\dfrac{\\partial J}{\\partial u_{i+1}}\\right)$, where $\\mathbf{u} \\equiv (u_{i-1},u_{i},u_{i+1})$.\n- Condition for Task 1: Assume $0 < r_{i} < 1$, which implies $\\phi_{\\mathrm{mm}}(r_{i}) = r_{i}$.\n- Data for Task 2: $a=2$, $\\alpha=1$, and local state $(u_{i-1},u_{i},u_{i+1})=(1,2,5)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, describing a standard MUSCL scheme and the associated discrete adjoint calculation, which are core topics in advanced computational fluid dynamics (CFD) and numerical optimization. The problem is well-posed, providing all necessary definitions. The data provided for Task 2 is consistent with the assumption for Task 1:\nGiven $(u_{i-1},u_{i},u_{i+1})=(1,2,5)$:\n$\\Delta^{+} u_{i} = u_{i+1} - u_{i} = 5 - 2 = 3$.\n$\\Delta^{-} u_{i} = u_{i} - u_{i-1} = 2 - 1 = 1$.\nThe ratio is $r_{i} = \\dfrac{\\Delta^{-} u_{i}}{\\Delta^{+} u_{i}} = \\dfrac{1}{3}$.\nSince $0 < \\frac{1}{3} < 1$, the provided state correctly falls within the specified active branch of the minmod limiter. The problem is objective, complete, and mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n---\n\n### Task 1: Derivation of the Adjoint Contributions\n\nThe objective function $J$ is defined as a sequence of nested functions of the state variables $\\mathbf{u} = (u_{i-1}, u_{i}, u_{i+1})$.\n$$J = \\alpha F_{i+\\frac{1}{2}} = \\alpha \\left( a u_{L,i+\\frac{1}{2}} \\right)$$\nSubstituting the definition of the reconstructed state $u_{L,i+\\frac{1}{2}}$:\n$$J = \\alpha a \\left( u_{i} + \\frac{1}{2} \\phi_{\\mathrm{mm}}(r_i) \\Delta^{+} u_i \\right)$$\nThe problem specifies that we are in the branch where $0 < r_i < 1$. In this range, the minmod function simplifies to $\\phi_{\\mathrm{mm}}(r_i) = r_i$. The expression for $J$ becomes:\n$$J = \\alpha a \\left( u_{i} + \\frac{1}{2} r_i \\Delta^{+} u_i \\right)$$\nNow, we substitute the definitions for $r_i$ and $\\Delta^{+} u_i$:\n$$r_i = \\frac{\\Delta^{-} u_i}{\\Delta^{+} u_i} = \\frac{u_i - u_{i-1}}{u_{i+1} - u_i}$$\n$$\\Delta^{+} u_i = u_{i+1} - u_i$$\nThe term $\\frac{1}{2} r_i \\Delta^{+} u_i$ can be greatly simplified:\n$$\\frac{1}{2} r_i \\Delta^{+} u_i = \\frac{1}{2} \\left( \\frac{u_i - u_{i-1}}{u_{i+1} - u_i} \\right) (u_{i+1} - u_i) = \\frac{1}{2} (u_i - u_{i-1})$$\nIt is crucial to note that this simplification holds because the problem states $\\Delta^{+} u_i \\neq 0$.\nSubstituting this simplified term back into the expression for $J$:\n$$J = \\alpha a \\left( u_i + \\frac{1}{2} (u_i - u_{i-1}) \\right)$$\nCombining terms involving $u_i$:\n$$J = \\alpha a \\left( \\frac{3}{2} u_i - \\frac{1}{2} u_{i-1} \\right)$$\nThis is the closed-form expression for the objective function $J$ in the specified branch. As predicted, it only depends on $u_{i-1}$ and $u_i$. Now we compute the partial derivatives with respect to each component of $\\mathbf{u}$:\n\n1.  Derivative with respect to $u_{i-1}$:\n    $$\\frac{\\partial J}{\\partial u_{i-1}} = \\frac{\\partial}{\\partial u_{i-1}} \\left[ \\alpha a \\left( \\frac{3}{2} u_i - \\frac{1}{2} u_{i-1} \\right) \\right] = -\\frac{1}{2} \\alpha a$$\n\n2.  Derivative with respect to $u_{i}$:\n    $$\\frac{\\partial J}{\\partial u_{i}} = \\frac{\\partial}{\\partial u_{i}} \\left[ \\alpha a \\left( \\frac{3}{2} u_i - \\frac{1}{2} u_{i-1} \\right) \\right] = \\frac{3}{2} \\alpha a$$\n\n3.  Derivative with respect to $u_{i+1}$:\n    $$\\frac{\\partial J}{\\partial u_{i+1}} = \\frac{\\partial}{\\partial u_{i+1}} \\left[ \\alpha a \\left( \\frac{3}{2} u_i - \\frac{1}{2} u_{i-1} \\right) \\right] = 0$$\n\nThe resulting vector of adjoint contributions is $\\left(-\\frac{1}{2} \\alpha a, \\frac{3}{2} \\alpha a, 0\\right)$. These expressions depend only on the constants $a$ and $\\alpha$, as required.\n\n### Task 2: Numerical Evaluation\n\nWe evaluate the expressions derived in Task 1 using the provided numerical values: $a=2$ and $\\alpha=1$. The state values $(u_{i-1},u_{i},u_{i+1})=(1,2,5)$ were used to confirm that we are in the correct branch of the limiter, but they do not appear in the final expressions for the derivatives in this particular branch.\n\n- $\\dfrac{\\partial J}{\\partial u_{i-1}} = -\\dfrac{1}{2} \\alpha a = -\\dfrac{1}{2} (1)(2) = -1$\n- $\\dfrac{\\partial J}{\\partial u_{i}} = \\dfrac{3}{2} \\alpha a = \\dfrac{3}{2} (1)(2) = 3$\n- $\\dfrac{\\partial J}{\\partial u_{i+1}} = 0$\n\nThe numerical contributions are therefore $(-1, 3, 0)$.\n\n### Task 3: Discussion on Non-Smooth Kinks\n\nThe minmod limiter function $\\phi_{\\mathrm{mm}}(r) = \\max(0, \\min(1, r))$ is continuous but not continuously differentiable. It has non-differentiable points, or \"kinks,\" at $r=0$ and $r=1$. The derivative, $\\phi'_{\\mathrm{mm}}(r)$, is piecewise constant:\n$$ \\phi'_{\\mathrm{mm}}(r) = \\begin{cases} 0 & \\text{if } r < 0 \\\\ 1 & \\text{if } 0 < r < 1 \\\\ 0 & \\text{if } r > 1 \\end{cases} $$\nAt $r=0$ and $r=1$, the derivative is formally undefined.\n\nThe objective function $J$ depends on the state variables $\\mathbf{u}$ through $\\phi_{\\mathrm{mm}}(r_i(\\mathbf{u}))$. By the chain rule, a non-differentiability in $\\phi_{\\mathrm{mm}}$ at a specific value of $r$ translates to a non-differentiability in $J$ with respect to $\\mathbf{u}$ for any state $\\mathbf{u}$ that produces that value of $r_i$. The discrete adjoint is the gradient $\\nabla_{\\mathbf{u}} J$. If this gradient is not defined, the discrete adjoint does not exist in the classical sense, or at least is not unique.\n\nThis non-uniqueness poses a problem for gradient-based algorithms which require a single well-defined gradient vector at every point. To resolve this, one turns to the concept of subgradients from convex analysis. At a non-differentiable point, the subgradient is a set of values that characterize the local behavior of the function. For $\\phi_{\\mathrm{mm}}$, the subgradient at $r=0$ is the interval $[0,1]$, and at $r=1$ it is also the interval $[0,1]$ (as it is the convex hull of the left and right derivatives).\n\nFor a practical and implementable discrete adjoint method, a single value must be chosen from the subgradient set at each kink. This choice, while arbitrary, must be made consistently. Common choices include:\n1.  Using one of the one-sided derivatives (e.g., setting $\\phi'_{\\mathrm{mm}}(0)=0$ from the left, or $\\phi'_{\\mathrm{mm}}(0)=1$ from the right).\n2.  Using the average of the one-sided derivatives (e.g., $\\phi'_{\\mathrm{mm}}(0) = \\frac{0+1}{2} = \\frac{1}{2}$).\n\nA typical and easily implementable choice is to define the derivative to be equal to one of its one-sided limits, for example by defining $\\phi'_{\\mathrm{mm}}(r) = 1$ for $r \\in [0, 1)$ and $\\phi'_{\\mathrm{mm}}(r) = 0$ otherwise. This ensures a unique value for the generalized derivative is always available, allowing the chain rule to be applied everywhere and yielding a uniquely defined (though technically a subgradient-based) adjoint vector across the entire state space.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-1 & 3 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After mastering the calculation of local adjoint contributions, this final practice scales up the concept to a complete boundary value problem. Here, you will assemble local derivatives into global linear systems for both the primal state and the adjoint variables, and then use the adjoint solution to efficiently compute the objective function's gradient . This exercise culminates in the single most important step for any developer of adjoint solvers: the verification of the computed adjoint gradient against a finite-difference approximation. Successfully matching these two gradients provides confidence that the complex derivation and implementation are correct.",
            "id": "3950976",
            "problem": "Consider a one-dimensional, steady, dimensionless convection-diffusion model frequently used in Computational Fluid Dynamics (CFD), derived from conservation of a transported scalar. On the domain $x \\in [0,1]$ with uniform mesh of $N$ points, Dirichlet boundary conditions $U(0)=0$ and $U(1)=0$, and interior nodes indexed by $i=1,\\dots,N-2$, let the discrete residual at each interior node be\n$$\nR_i(U,p) = \\Gamma \\frac{U_{i-1} - 2 U_i + U_{i+1}}{\\Delta x^2} - a \\frac{U_i - U_{i-1}}{\\Delta x} - s_i(p),\n$$\nwhere $U_i$ denotes the discrete state at node $i$, $\\Delta x = \\frac{1}{N-1}$, $a \\ge 0$ is a dimensionless convection strength, $\\Gamma > 0$ is a dimensionless diffusion coefficient, and $s_i(p)$ is a dimensionless source term that depends on a scalar design parameter $p$. Assume $s_i(p) = p f_i$ with $f_i = \\sin(\\pi x_i)$ and $x_i = i \\Delta x$. The discrete state $U$ is defined implicitly by the steady residual equation $R(U,p)=0$, which for this linear model can be written as $A(p) U = s(p)$ with the tridiagonal matrix $A(p)$ independent of $p$ given by\n$$\nA_{i,i} = -\\frac{2\\Gamma}{\\Delta x^2} - \\frac{a}{\\Delta x}, \\quad A_{i,i-1} = \\frac{\\Gamma}{\\Delta x^2} + \\frac{a}{\\Delta x}, \\quad A_{i,i+1} = \\frac{\\Gamma}{\\Delta x^2},\n$$\nfor all valid indices where boundary values are zero and therefore contribute no additional source terms. Consider the cost functional\n$$\nJ(U,p) = \\frac{1}{2}\\,\\Delta x \\sum_{i=1}^{N-2} \\left(U_i - U_i^\\star\\right)^2 + \\beta\\, p,\n$$\nwhere $U_i^\\star = \\sin(2\\pi x_i)$ is a prescribed, dimensionless target field and $\\beta$ is a dimensionless scalar weight. Angles in all trigonometric functions are measured in radians.\n\nTask: Starting from the discrete residual definition and the chain rule applied to the implicit state $U(p)$, derive the discrete adjoint equation and the gradient formula\n$$\n\\frac{dJ}{dp} = J_p^T - \\lambda^T R_p,\n$$\nwhere $\\lambda$ is the adjoint variable defined by a linear system that you must obtain, $J_p$ is the partial derivative of $J$ with respect to $p$ at fixed $U$, and $R_p$ is the partial derivative of $R$ with respect to $p$ at fixed $U$. Then implement a program that:\n- Constructs the matrix $A$, solves for the state $U$ via $A U = s(p)$, computes the adjoint variable $\\lambda$ from the appropriate linear system, and evaluates the discrete-adjoint gradient $\\frac{dJ}{dp}$.\n- Computes a finite-difference estimate of $\\frac{dJ}{dp}$ using a symmetric difference with a small step $h$,\n$$\n\\left.\\frac{dJ}{dp}\\right|_{\\text{FD}} \\approx \\frac{J(U(p+h),p+h) - J(U(p-h),p-h)}{2h}.\n$$\n- Returns, for each test case, the absolute difference between the discrete-adjoint gradient and the finite-difference gradient, as a floating-point number.\n\nAll quantities are dimensionless. You must use angles in radians for all trigonometric evaluations. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$).\n\nUse the following test suite with parameter tuples $(N,a,\\Gamma,p,\\beta)$:\n- Case $1$: $(N,a,\\Gamma,p,\\beta) = (50, 1.0, 0.1, 0.5, 0.3)$.\n- Case $2$: $(N,a,\\Gamma,p,\\beta) = (20, 0.0, 0.5, 1.0, 0.0)$.\n- Case $3$: $(N,a,\\Gamma,p,\\beta) = (200, 5.0, 10^{-3}, 0.1, 1.0)$.\n- Case $4$: $(N,a,\\Gamma,p,\\beta) = (5, 0.2, 0.05, 0.8, 0.2)$.\n\nYour output must be a single list $[d_1,d_2,d_3,d_4]$ where $d_k$ is the absolute difference between the discrete-adjoint gradient and the finite-difference gradient for case $k$, expressed as floating-point numbers.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard problem in CFD-based optimization using the discrete adjoint method. All necessary data and definitions are provided.\n\n### Derivation of the Discrete Adjoint Gradient\n\nThe objective is to compute the total derivative of the cost functional $J(U,p)$ with respect to the design parameter $p$. The state vector $U$ is an implicit function of $p$, defined by the steady-state residual equation $R(U(p), p) = 0$.\n\nThe cost functional is given by:\n$$\nJ(U,p) = \\frac{1}{2}\\,\\Delta x \\sum_{i=1}^{N-2} \\left(U_i - U_i^\\star\\right)^2 + \\beta\\, p\n$$\nThe discrete residual equation for each interior node $i \\in \\{1, \\dots, N-2\\}$ is:\n$$\nR_i(U,p) = \\left( \\frac{\\Gamma}{\\Delta x^2} + \\frac{a}{\\Delta x} \\right) U_{i-1} + \\left( -\\frac{2\\Gamma}{\\Delta x^2} - \\frac{a}{\\Delta x} \\right) U_i + \\left( \\frac{\\Gamma}{\\Delta x^2} \\right) U_{i+1} - p f_i = 0\n$$\nwhere $f_i = \\sin(\\pi x_i)$. This can be written in matrix form as $R(U, p) = A U - s(p) = 0$, where $A$ is the system matrix, and $s(p)$ is the source vector with components $s_i(p) = p f_i$.\n\n**1. Total Derivative using the Chain Rule**\n\nApplying the chain rule to $J(U(p), p)$, we obtain the total derivative with respect to $p$:\n$$\n\\frac{dJ}{dp} = \\frac{\\partial J}{\\partial p} + \\left(\\nabla_U J\\right)^T \\frac{dU}{dp}\n$$\nHere, $\\frac{\\partial J}{\\partial p}$ is the partial derivative of $J$ with respect to $p$ holding $U$ constant, $\\nabla_U J$ is the gradient of $J$ with respect to the state vector $U$ (a column vector of partial derivatives $\\frac{\\partial J}{\\partial U_i}$), and $\\frac{dU}{dp}$ is the state sensitivity vector, whose components are $\\frac{dU_i}{dp}$.\n\n**2. State Sensitivity Derivation**\n\nThe state sensitivity vector $\\frac{dU}{dp}$ is found by differentiating the state equation $R(U(p), p) = 0$ with respect to $p$:\n$$\n\\frac{dR}{dp} = \\frac{\\partial R}{\\partial U} \\frac{dU}{dp} + \\frac{\\partial R}{\\partial p} = 0\n$$\nwhere $\\frac{\\partial R}{\\partial U}$ is the Jacobian matrix of the residual vector $R$ with respect to the state vector $U$, and $\\frac{\\partial R}{\\partial p}$ is the partial derivative of $R$ with respect to $p$.\nRearranging for the state sensitivity gives:\n$$\n\\frac{dU}{dp} = - \\left( \\frac{\\partial R}{\\partial U} \\right)^{-1} \\frac{\\partial R}{\\partial p}\n$$\nDirect computation of $\\frac{dU}{dp}$ requires solving a linear system for each component of the sensitivity vector, which is computationally expensive. The adjoint method circumvents this.\n\n**3. The Adjoint Method**\n\nSubstituting the expression for the sensitivity into the total derivative of $J$:\n$$\n\\frac{dJ}{dp} = \\frac{\\partial J}{\\partial p} - \\left(\\nabla_U J\\right)^T \\left( \\frac{\\partial R}{\\partial U} \\right)^{-1} \\frac{\\partial R}{\\partial p}\n$$\nTo avoid the matrix inverse, we define the discrete adjoint vector $\\lambda$ (a column vector) by grouping the terms that multiply $\\frac{\\partial R}{\\partial p}$:\n$$\n\\lambda^T = \\left(\\nabla_U J\\right)^T \\left( \\frac{\\partial R}{\\partial U} \\right)^{-1}\n$$\nTo solve for $\\lambda$ without computing an inverse, we right-multiply by $\\frac{\\partial R}{\\partial U}$ and take the transpose:\n$$\n\\left( \\frac{\\partial R}{\\partial U} \\right)^T \\lambda = \\nabla_U J\n$$\nThis is the **discrete adjoint equation**, which is a single linear system for the adjoint vector $\\lambda$.\n\nWith $\\lambda$ defined as such, the total derivative of the cost functional simplifies to:\n$$\n\\frac{dJ}{dp} = \\frac{\\partial J}{\\partial p} - \\lambda^T \\frac{\\partial R}{\\partial p}\n$$\nThis is the desired gradient formula. Using the notation $J_p = \\frac{\\partial J}{\\partial p}$ and $R_p = \\frac{\\partial R}{\\partial p}$, this is $\\frac{dJ}{dp} = J_p - \\lambda^T R_p$. Since $p$ is a scalar, $J_p$ is also a scalar, so $J_p^T=J_p$.\n\n**4. Application to the Specific Problem**\n\nWe now compute the necessary partial derivatives for the given problem.\n-   **Residual Jacobian:** From $R(U, p) = A U - s(p)$, the partial derivative with respect to $U$ is simply the matrix $A$.\n    $$\n    \\frac{\\partial R}{\\partial U} = A\n    $$\n-   **Residual Derivative w.r.t. Parameter:** The residual vector is $R(U,p) = AU - p f$. Its partial derivative with respect to $p$ is:\n    $$\n    \\frac{\\partial R}{\\partial p} = -f\n    $$\n    where $f$ is the vector with components $f_i = \\sin(\\pi x_i)$.\n-   **Functional Gradient w.r.t. State:** The functional is $J = \\frac{1}{2}\\,\\Delta x \\sum_{k=1}^{N-2} \\left(U_k - U_k^\\star\\right)^2 + \\beta\\, p$. Its partial derivative with respect to $U_i$ is:\n    $$\n    (\\nabla_U J)_i = \\frac{\\partial J}{\\partial U_i} = \\Delta x \\left(U_i - U_i^\\star\\right)\n    $$\n-   **Functional Derivative w.r.t. Parameter:** The partial derivative of $J$ with respect to $p$ is:\n    $$\n    \\frac{\\partial J}{\\partial p} = \\beta\n    $$\n\n**5. Final Adjoint System and Gradient**\n\nSubstituting these specific terms into the general adjoint framework yields:\n-   **Primal (State) Equation:** $A U = s(p)$, where $s_i(p) = p f_i$.\n-   **Adjoint Equation:**\n    $$\n    A^T \\lambda = \\nabla_U J, \\quad \\text{where } (\\nabla_U J)_i = \\Delta x (U_i - U_i^\\star)\n    $$\n-   **Gradient Formula:**\n    $$\n    \\frac{dJ}{dp} = \\beta - \\lambda^T (-f) = \\beta + \\lambda^T f = \\beta + \\sum_{i=1}^{N-2} \\lambda_i f_i\n    $$\n\n**6. Numerical Procedure**\n\nThe algorithm to compute the discrete-adjoint gradient is:\n1.  For given parameters $(N, a, \\Gamma, p, \\beta)$, construct the $(N-2) \\times (N-2)$ matrix $A$ and the source vector $s(p)$.\n2.  Solve the primal linear system $A U = s(p)$ for the state vector $U$.\n3.  Construct the right-hand side of the adjoint system, $\\nabla_U J$, using the computed state $U$.\n4.  Solve the adjoint linear system $A^T \\lambda = \\nabla_U J$ for the adjoint vector $\\lambda$.\n5.  Compute the gradient using the formula $\\frac{dJ}{dp} = \\beta + \\lambda^T f$.\n\nThis gradient is then compared against a finite-difference approximation for verification:\n$$\n\\left.\\frac{dJ}{dp}\\right|_{\\text{FD}} \\approx \\frac{J(U(p+h),p+h) - J(U(p-h),p-h)}{2h}\n$$\nwhere $h$ is a small perturbation. This requires two additional primal solves to find $U(p+h)$ and $U(p-h)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print results.\n    \"\"\"\n    test_cases = [\n        (50, 1.0, 0.1, 0.5, 0.3),   # Case 1\n        (20, 0.0, 0.5, 1.0, 0.0),   # Case 2\n        (200, 5.0, 1e-3, 0.1, 1.0), # Case 3\n        (5, 0.2, 0.05, 0.8, 0.2),   # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        N, a, Gamma, p, beta = case\n        diff = compute_gradient_difference(N, a, Gamma, p, beta)\n        results.append(diff)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_gradient_difference(N, a, Gamma, p, beta, h=1e-7):\n    \"\"\"\n    Computes the absolute difference between the discrete-adjoint gradient and\n    a finite-difference approximation.\n\n    Args:\n        N (int): Number of mesh points.\n        a (float): Dimensionless convection strength.\n        Gamma (float): Dimensionless diffusion coefficient.\n        p (float): Scalar design parameter.\n        beta (float): Scalar weight in the cost functional.\n        h (float): Step size for finite difference.\n\n    Returns:\n        float: The absolute difference between the two gradient calculations.\n    \"\"\"\n    # Grid and problem setup\n    num_interior = N - 2\n    if num_interior <= 0:\n        return 0.0\n    \n    dx = 1.0 / (N - 1)\n    x_interior = np.linspace(0, 1, N)[1:-1]\n\n    # Target field and source term structure\n    U_star = np.sin(2 * np.pi * x_interior)\n    f_vec = np.sin(np.pi * x_interior)\n\n    # --- Construct the primal system matrix A ---\n    A = np.zeros((num_interior, num_interior))\n    \n    # Coefficients from discretization\n    c_diff = Gamma / dx**2\n    c_conv = a / dx\n    \n    diag_val = -2.0 * c_diff - c_conv\n    sub_diag_val = c_diff + c_conv\n    sup_diag_val = c_diff\n    \n    np.fill_diagonal(A, diag_val)\n    if num_interior > 1:\n        np.fill_diagonal(A[1:, :], sub_diag_val)\n        np.fill_diagonal(A[:, 1:], sup_diag_val)\n\n    # --- Discrete-Adjoint Gradient Calculation ---\n    \n    # 1. Solve primal system for state U: A U = s(p)\n    s_p = p * f_vec\n    U = np.linalg.solve(A, s_p)\n\n    # 2. Assemble and solve adjoint system for lambda: A^T lambda = dJ/dU\n    grad_J_U = dx * (U - U_star)\n    A_T = A.T\n    lambda_vec = np.linalg.solve(A_T, grad_J_U)\n\n    # 3. Compute adjoint gradient: dJ/dp = beta + lambda^T f\n    grad_adjoint = beta + np.dot(lambda_vec, f_vec)\n    \n    # --- Finite-Difference Gradient Calculation ---\n\n    def compute_J(param_p):\n        \"\"\"Helper to compute cost functional J for a given parameter p.\"\"\"\n        s_vec = param_p * f_vec\n        U_sol = np.linalg.solve(A, s_vec)\n        cost = 0.5 * dx * np.sum((U_sol - U_star)**2) + beta * param_p\n        return cost\n\n    J_plus_h = compute_J(p + h)\n    J_minus_h = compute_J(p - h)\n    \n    grad_fd = (J_plus_h - J_minus_h) / (2.0 * h)\n\n    # Return the absolute difference\n    return abs(grad_adjoint - grad_fd)\n\nsolve()\n```"
        }
    ]
}