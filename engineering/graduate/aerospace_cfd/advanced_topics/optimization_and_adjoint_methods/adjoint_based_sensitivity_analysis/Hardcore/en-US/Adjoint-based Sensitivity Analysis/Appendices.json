{
    "hands_on_practices": [
        {
            "introduction": "The first step in leveraging adjoint methods is building trust in your implementation. This exercise guides you through the process of deriving and coding the adjoint for a simple, yet representative, physical system governed by a linear equation. You will then verify your analytical gradient using the powerful complex-step method, a technique that provides near-machine-precision derivatives and serves as an invaluable tool for debugging .",
            "id": "3942369",
            "problem": "You are given a discrete steady model representative of a computational fluid dynamics (CFD) residual and an objective functional dependent on the state and a scalar design parameter. Let $p \\in \\mathbb{R}$ be the scalar parameter, and let $u(p) \\in \\mathbb{R}^N$ be the state vector defined implicitly by the discrete residual equation\n$$\nR(u,p) = A(p) u - b = 0,\n$$\nwhere $A(p) \\in \\mathbb{R}^{N \\times N}$ is a parameter-dependent matrix, $b \\in \\mathbb{R}^N$ is a fixed right-hand side vector, and $N$ is the number of interior nodes of a one-dimensional grid on the interval $[0,1]$. Let the discrete Laplacian matrix $L \\in \\mathbb{R}^{N \\times N}$ be the standard second-order central-difference tridiagonal operator scaled by the grid spacing, and define\n$$\nA(p) = I + p L,\n$$\nwhere $I \\in \\mathbb{R}^{N \\times N}$ is the identity matrix. Let the right-hand side be the discretization of $b(x) = \\sin(\\pi x)$ on the interior grid points. Define the objective as\n$$\nJ(u,p) = \\frac{1}{2} u^\\top W u + \\gamma p^2,\n$$\nwhere $W \\in \\mathbb{R}^{N \\times N}$ is a diagonal matrix with entries $w_i = 1 + \\cos(2 \\pi x_i)$, $x_i$ are the interior grid points on $[0,1]$, and $\\gamma > 0$ is a given constant.\n\nStarting from the fundamental base of Lagrange multiplier (adjoint) methodology in constrained optimization for residual equations and the Taylor series expansion for complex analytic functions, derive a computational scheme that:\n\n1. Solves for $u(p)$ by enforcing $R(u,p)=0$.\n2. Computes the adjoint-based gradient $g_{\\mathrm{adj}}(p) = \\frac{\\partial J}{\\partial p}$ by solving the adjoint system and evaluating the appropriate sensitivity expression derived from the residual and objective definitions.\n3. Implements the complex-step method to compute $g_{\\mathrm{cs}}(p)$ by evaluating $J$ at $p + i h$ with a purely imaginary perturbation of size $h>0$ and extracting the imaginary part scaled by $h$.\n4. Produces the absolute error $\\left|g_{\\mathrm{adj}}(p) - g_{\\mathrm{cs}}(p)\\right|$ for a test suite of parameter values.\n\nYour implementation must adhere to the following specifications:\n\n- Use $N = 50$, uniform interior grid points $x_i$ on $(0,1)$ with spacing $h_x = \\frac{1}{N+1}$.\n- Construct $L$ as the standard second-order central difference matrix with entries $L_{ii} = -\\frac{2}{h_x^2}$, $L_{i,i+1} = L_{i+1,i} = \\frac{1}{h_x^2}$, and zeros elsewhere.\n- Construct $W$ as a diagonal matrix with $W_{ii} = 1 + \\cos(2 \\pi x_i)$.\n- Use $\\gamma = 0.7$.\n- Use $b_i = \\sin(\\pi x_i)$.\n- For the adjoint method, use the Lagrangian framework: solve the adjoint equation $A(p)^\\top \\lambda = \\frac{\\partial J}{\\partial u}$, and then evaluate the gradient using the sensitivity identity that follows from $R(u,p)=0$.\n- For the complex-step method, evaluate $J(u(p + i h), p + i h)$ with $h = 10^{-30}$, and approximate $\\frac{\\partial J}{\\partial p}$ as $\\operatorname{Im}(J(p + i h))/h$. Ensure that the implementation treats all algebraic operations holomorphically (do not use complex conjugation in the objective computation).\n- Your program should compute the absolute error $\\left|g_{\\mathrm{adj}}(p) - g_{\\mathrm{cs}}(p)\\right|$ for the following test suite of parameter values (expressed as real numbers): $p \\in \\{0.0, 10^{-8}, 0.1, 1.0, 10.0\\}$.\n- The final output format must be a single line containing a comma-separated list enclosed in square brackets, with each entry being the absolute error for the corresponding test case in the order given (e.g., $[e_1,e_2,e_3,e_4,e_5]$).\n- No physical units, angle units, or percentages are involved in this problem; all numerical results are dimensionless real numbers.\n\nThe goal is to verify the adjoint-based sensitivity using the complex-step method and to explain from first principles why the complex-step method avoids subtractive cancellation and what accuracy bound is expected for $\\operatorname{Im}(J(p + i h))/h$ as an approximation to $\\frac{dJ}{dp}$.",
            "solution": "The problem requires the derivation and implementation of a computational scheme to find the gradient of an objective functional with respect to a design parameter, where the system state is defined implicitly by a linear system of equations. The gradient is to be computed using two distinct methods: the adjoint method and the complex-step method. The results are then compared to verify the adjoint implementation.\n\nFirst, we formalize the derivation of the total derivative and the adjoint method. The objective functional is $J(u,p)$, where the state vector $u$ is an implicit function of the parameter $p$, defined by the residual equation $R(u(p), p) = 0$. The total derivative of $J$ with respect to $p$ is given by the chain rule:\n$$\n\\frac{dJ}{dp} = \\frac{\\partial J}{\\partial u} \\frac{du}{dp} + \\frac{\\partial J}{\\partial p}\n$$\nThis expression requires the sensitivity of the state vector, $\\frac{du}{dp}$. To find this, we differentiate the state equation $R(u(p), p) = 0$ with respect to $p$:\n$$\n\\frac{dR}{dp} = \\frac{\\partial R}{\\partial u} \\frac{du}{dp} + \\frac{\\partial R}{\\partial p} = 0\n$$\nAssuming the Jacobian of the residual with respect to the state, $\\frac{\\partial R}{\\partial u}$, is invertible, we can solve for the state sensitivity:\n$$\n\\frac{du}{dp} = - \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial p}\n$$\nSubstituting this back into the expression for $\\frac{dJ}{dp}$ gives the direct sensitivity formula:\n$$\n\\frac{dJ}{dp} = -\\frac{\\partial J}{\\partial u} \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial p} + \\frac{\\partial J}{\\partial p}\n$$\nThis approach is computationally expensive if there are many parameters, as it requires solving a linear system for each parameter's sensitivity.\n\nThe adjoint method provides a more efficient alternative. We introduce a Lagrangian functional $\\mathcal{L}$ by augmenting the objective $J$ with the constraint $R=0$ via a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^N$, also known as the adjoint variables:\n$$\n\\mathcal{L}(u, p, \\lambda) = J(u, p) - \\lambda^\\top R(u, p)\n$$\nSince $R(u(p), p) = 0$, we have $J(u(p), p) = \\mathcal{L}(u(p), p, \\lambda(p))$ for any choice of $\\lambda$. The total derivative is thus $\\frac{dJ}{dp} = \\frac{d\\mathcal{L}}{dp}$. Applying the chain rule to $\\mathcal{L}$:\n$$\n\\frac{d\\mathcal{L}}{dp} = \\frac{\\partial \\mathcal{L}}{\\partial u} \\frac{du}{dp} + \\frac{\\partial \\mathcal{L}}{\\partial p} + \\frac{\\partial \\mathcal{L}}{\\partial \\lambda}^\\top \\frac{d\\lambda}{dp}\n$$\nThe term $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -R(u,p)$ is zero by definition of the state equation. The adjoint method's key insight is to choose $\\lambda$ to eliminate the term involving the expensive state sensitivity $\\frac{du}{dp}$. We achieve this by setting its coefficient to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u} = \\frac{\\partial J}{\\partial u} - \\lambda^\\top \\frac{\\partial R}{\\partial u} = 0\n$$\nRearranging this gives the adjoint equation:\n$$\n\\left(\\frac{\\partial R}{\\partial u}\\right)^\\top \\lambda = \\left(\\frac{\\partial J}{\\partial u}\\right)^\\top\n$$\nBy solving this single linear system for $\\lambda$, we can compute the total derivative without ever forming $\\frac{du}{dp}$. With this choice of $\\lambda$, the derivative simplifies to:\n$$\n\\frac{dJ}{dp} = \\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{\\partial J}{\\partial p} - \\lambda^\\top \\frac{\\partial R}{\\partial p}\n$$\nThis expression is computationally inexpensive to evaluate once the state $u$ and the adjoint $\\lambda$ are known.\n\nNow, we apply this framework to the specific problem.\nThe givens are:\n-   State equation residual: $R(u,p) = A(p) u - b = (I + p L) u - b = 0$.\n-   Objective functional: $J(u,p) = \\frac{1}{2} u^\\top W u + \\gamma p^2$.\n\nWe compute the necessary partial derivatives:\n-   $\\frac{\\partial J}{\\partial u} = u^\\top W$ (as a row vector), or $\\left(\\frac{\\partial J}{\\partial u}\\right)^\\top = W u$ (as a column vector).\n-   $\\frac{\\partial J}{\\partial p} = 2 \\gamma p$.\n-   $\\frac{\\partial R}{\\partial u} = A(p) = I + p L$.\n-   $\\frac{\\partial R}{\\partial p} = L u$.\n\nThe computational scheme for the adjoint gradient $g_{\\mathrm{adj}}(p) = \\frac{dJ}{dp}$ is:\n1.  Solve the state equation for $u$: $(I + p L) u = b$.\n2.  Solve the adjoint equation for $\\lambda$: $(I + p L)^\\top \\lambda = W u$. Since the matrices $I$ and $L$ are symmetric, $(I + p L)^\\top = I + p L$. Thus, we solve $(I + p L) \\lambda = W u$.\n3.  Compute the gradient: $g_{\\mathrm{adj}}(p) = 2 \\gamma p - \\lambda^\\top (L u)$.\n\nNext, we describe the complex-step method. This method provides a highly accurate numerical approximation to the derivative, serving as a verification tool. It is derived from the Taylor series expansion of a holomorphic function $f(z)$ around a real point $x$ with a purely imaginary step $ih$:\n$$\nf(x+ih) = f(x) + f'(x)(ih) + \\frac{f''(x)}{2!}(ih)^2 + \\frac{f'''(x)}{3!}(ih)^3 + \\mathcal{O}(h^4)\n$$\n$$\nf(x+ih) = f(x) + ih f'(x) - \\frac{h^2}{2} f''(x) - i\\frac{h^3}{6} f'''(x) + \\mathcal{O}(h^4)\n$$\nBy taking the imaginary part of both sides, we get:\n$$\n\\operatorname{Im}[f(x+ih)] = h f'(x) - \\frac{h^3}{6} f'''(x) + \\mathcal{O}(h^5)\n$$\nDividing by $h$ yields an approximation for the derivative:\n$$\n\\frac{\\operatorname{Im}[f(x+ih)]}{h} = f'(x) - \\frac{h^2}{6} f'''(x) + \\mathcal{O}(h^4)\n$$\nThus, $\\frac{\\operatorname{Im}[f(x+ih)]}{h} = f'(x) + \\mathcal{O}(h^2)$. The approximation error is of second order in $h$.\n\nThe complex-step method avoids the subtractive cancellation error inherent in finite-difference methods. The forward-difference formula, $\\frac{f(x+h) - f(x)}{h}$, involves the subtraction of two nearly equal numbers $f(x+h)$ and $f(x)$ when $h$ is small. This leads to a loss of relative precision. The complex-step formula computes $\\operatorname{Im}[f(x+ih)]$, which is directly proportional to $h$ for small $h$. The term $f(x)$ is purely real and does not interfere with the imaginary part. Consequently, there is no subtraction of large, nearly equal quantities, and $h$ can be chosen to be very small (e.g., $h = 10^{-30}$), making the truncation error $\\mathcal{O}(h^2)$ negligible. The accuracy is limited only by machine precision.\n\nThe computational scheme for the complex-step gradient $g_{\\mathrm{cs}}(p)$ is:\n1.  Define a complex parameter $\\tilde{p} = p + ih$ for a small real number $h > 0$.\n2.  Form the complex state matrix $A(\\tilde{p}) = I + \\tilde{p} L$.\n3.  Solve the complex linear system for the complex state vector $\\tilde{u}$: $A(\\tilde{p}) \\tilde{u} = b$. The vector $b$ remains real.\n4.  Evaluate the objective functional using complex arithmetic, ensuring no conjugation operations are used: $\\tilde{J} = J(\\tilde{u}, \\tilde{p}) = \\frac{1}{2} \\tilde{u}^\\top W \\tilde{u} + \\gamma \\tilde{p}^2$.\n5.  Compute the gradient approximation: $g_{\\mathrm{cs}}(p) = \\frac{\\operatorname{Im}[\\tilde{J}]}{h}$.\n\nThe final task is to implement both schemes for the given parameters ($N=50$, $\\gamma=0.7$, etc.) and compute the absolute error $|g_{\\mathrm{adj}}(p) - g_{\\mathrm{cs}}(p)|$ for the specified test suite of $p$ values. This error is expected to be close to machine precision, confirming the correctness of the adjoint derivation and implementation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the absolute error between adjoint-based and complex-step gradients\n    for a parameter-dependent linear system.\n    \"\"\"\n    #\n    # --- Problem Specifications ---\n    #\n    N = 50  # Number of interior grid points\n    gamma = 0.7  # Regularization parameter\n    h_cs = 1e-30  # Step size for complex-step method\n    p_values = [0.0, 1e-8, 0.1, 1.0, 10.0]  # Test suite for parameter p\n\n    #\n    # --- Grid and Matrix Construction ---\n    #\n    # Grid spacing for N interior points on (0, 1)\n    h_x = 1.0 / (N + 1)\n    # Interior grid points x_i\n    x = np.linspace(h_x, 1.0 - h_x, N)\n\n    # Construct the 1D discrete Laplacian matrix L\n    diag_main = -2.0 * np.ones(N)\n    diag_off = 1.0 * np.ones(N - 1)\n    L = (np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)) / (h_x**2)\n\n    # Construct the diagonal weighting matrix W\n    W = np.diag(1.0 + np.cos(2.0 * np.pi * x))\n\n    # Construct the right-hand side vector b\n    b = np.sin(np.pi * x)\n\n    # Identity matrix\n    I = np.eye(N)\n\n    results = []\n    for p in p_values:\n        #\n        # --- 1. Adjoint-Based Gradient Calculation ---\n        #\n        # State matrix A(p) = I + p*L\n        A = I + p * L\n\n        # a. Solve the state equation for u: A(p)*u = b\n        u = np.linalg.solve(A, b)\n\n        # b. Solve the adjoint equation for lambda: A(p)^T * lambda = (dJ/du)^T\n        # dJ/du = u^T * W  => (dJ/du)^T = W * u\n        # Since A(p) is symmetric for real p, A(p)^T = A(p).\n        rhs_adjoint = W @ u\n        lam = np.linalg.solve(A, rhs_adjoint)\n\n        # c. Compute the gradient: g_adj = dJ/dp = partial_J/partial_p - lambda^T * partial_R/partial_p\n        # partial_J/partial_p = 2 * gamma * p\n        # partial_R/partial_p = L * u\n        g_adj = 2.0 * gamma * p - lam.T @ (L @ u)\n\n        #\n        # --- 2. Complex-Step Gradient Calculation ---\n        #\n        # a. Define the complex parameter p_tilde = p + i*h\n        p_cs = p + 1j * h_cs\n\n        # b. Form the complex state matrix A(p_tilde)\n        # Use complex128 for high precision\n        A_cs = np.eye(N, dtype=np.complex128) + p_cs * L\n\n        # c. Solve the complex state equation for u_tilde: A(p_tilde)*u_tilde = b\n        u_cs = np.linalg.solve(A_cs, b)\n\n        # d. Evaluate the objective functional J(u_tilde, p_tilde)\n        # J = 0.5 * u^T * W * u + gamma * p^2.\n        # np.transpose() or .T on a complex array does not conjugate. This is required.\n        J_cs = 0.5 * u_cs.T @ W @ u_cs + gamma * p_cs**2\n\n        # e. Compute the gradient approximation\n        g_cs = np.imag(J_cs) / h_cs\n\n        #\n        # --- 3. Compute Absolute Error ---\n        #\n        error = np.abs(g_adj - g_cs)\n        results.append(error)\n\n    #\n    # --- Final Output Formatting ---\n    #\n    print(f\"[{','.join(f'{err:.15e}' for err in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the complex-step method is ideal for verification, it may not always be feasible to implement. The more traditional finite-difference method remains a workhorse, but its accuracy hinges critically on the choice of step size. This practice challenges you to analyze the trade-off between truncation and round-off errors to derive the optimal step size for a central-difference gradient check, a fundamental skill for robust numerical verification .",
            "id": "3942381",
            "problem": "Consider a steady, inviscid aerodynamic analysis governed by a discrete residual equation $\\mathbf{R}(\\mathbf{u},p)=\\mathbf{0}$, where $\\mathbf{u}$ is the converged flow state and $p$ is a scalar design parameter representing the angle of attack in radians. The objective is the drag coefficient $J(\\mathbf{u}(p),p)$, and the total derivative $\\mathrm{d}J/\\mathrm{d}p$ at a baseline $p_0$ is available from an adjoint computation. To validate the adjoint using a finite-difference gradient check, you will construct a central-difference estimator and determine the optimal perturbation size that balances truncation and floating-point round-off errors.\n\nAssume the following:\n\n- Each converged objective evaluation $J(p)$ satisfies the standard floating-point model $\\mathrm{fl}(J(p))=J(p)(1+\\delta)$ with $|\\delta|\\le u$, where $u$ is the unit roundoff of double-precision arithmetic, $u=2.22\\times 10^{-16}$.\n- The function $J(p)$ is sufficiently smooth near $p_0$ so that Taylor expansions about $p_0$ are valid.\n- The baseline parameter is $p_0=2\\pi/180$ radians.\n- You have performed five high-fidelity converged flow solves yielding the following objective values near $p_0$ (angles in radians):\n  - $J(p_0)=0.015000000000000$,\n  - $J(p_0+h_1)=0.015003450000000$,\n  - $J(p_0-h_1)=0.014997350000000$,\n  - $J(p_0+2h_1)=0.015008000000000$,\n  - $J(p_0-2h_1)=0.014995200000000$,\n  where $h_1=1.0\\times 10^{-3}$.\n\nTasks:\n\n1) Starting from Taylor expansions of $J(p_0\\pm h)$ about $p_0$ and the definition of a central difference, formulate a central-difference gradient check $\\tilde{g}(h)$ for $\\mathrm{d}J/\\mathrm{d}p$ at $p_0$ and identify the leading-order truncation error term in powers of $h$.\n\n2) Using the floating-point model for $J$-evaluations and standard rules of error propagation for sums and differences, derive a leading-order bound for the round-off contribution to the error in $\\tilde{g}(h)$ in terms of $u$, $|J(p_0)|$, and $h$.\n\n3) Combine your truncation and round-off models into a single leading-order error model $E(h)$ and, by minimizing $E(h)$ with respect to $h$, derive a closed-form expression for the optimal step size $h_\\star$ in terms of $u$, a local estimate of $|J^{(3)}(p_0)|$, and $|J(p_0)|$.\n\n4) Use the supplied objective values to obtain:\n   - the central-difference estimates $\\tilde{g}(h_1)$ and $\\tilde{g}(2h_1)$,\n   - a local smoothness estimate of $|J^{(3)}(p_0)|$ based on the difference of these two central-difference estimates,\n   - and then compute the numerical value of $h_\\star$ in radians.\n\nExpress your final numerical answer for $h_\\star$ in radians, rounded to three significant figures. The final answer must be a single number.",
            "solution": "We begin from first principles: a central-difference gradient check is constructed by evaluating $J$ at symmetric perturbations about $p_0$. Let $\\Delta p=h$. A Taylor expansion of $J$ about $p_0$ gives\n$$\nJ(p_0+h)=J(p_0)+J'(p_0)h+\\frac{J''(p_0)}{2}h^2+\\frac{J^{(3)}(p_0)}{6}h^3+\\mathcal{O}(h^4),\n$$\n$$\nJ(p_0-h)=J(p_0)-J'(p_0)h+\\frac{J''(p_0)}{2}h^2-\\frac{J^{(3)}(p_0)}{6}h^3+\\mathcal{O}(h^4).\n$$\nSubtracting these two expressions yields\n$$\nJ(p_0+h)-J(p_0-h)=2J'(p_0)h+\\frac{J^{(3)}(p_0)}{3}h^3+\\mathcal{O}(h^5).\n$$\nDividing by $2h$ gives the central-difference estimator\n$$\n\\tilde{g}(h)\\equiv \\frac{J(p_0+h)-J(p_0-h)}{2h}=J'(p_0)+\\frac{J^{(3)}(p_0)}{6}h^2+\\mathcal{O}(h^4).\n$$\nTherefore, the leading-order truncation error is $\\frac{J^{(3)}(p_0)}{6}h^2$.\n\nNext, we model round-off. Each objective value is returned in floating point as $\\mathrm{fl}(J(p))=J(p)(1+\\delta)$ with $|\\delta|\\le u$. The numerator of the central difference computed in floating point is\n$$\n\\mathrm{fl}(J(p_0+h))-\\mathrm{fl}(J(p_0-h))=[J(p_0+h)-J(p_0-h)]+[J(p_0+h)\\delta_1-J(p_0-h)\\delta_2],\n$$\nwhere $|\\delta_1|,|\\delta_2|\\le u$. The worst-case absolute perturbation of the numerator due to round-off is bounded by\n$$\n|J(p_0+h)\\delta_1-J(p_0-h)\\delta_2|\\le u\\left(|J(p_0+h)|+|J(p_0-h)|\\right).\n$$\nFor sufficiently small $h$, $|J(p_0\\pm h)|\\approx |J(p_0)|$, hence\n$$\n|J(p_0+h)\\delta_1-J(p_0-h)\\delta_2|\\lesssim 2u|J(p_0)|.\n$$\nDividing by the exact denominator $2h$ to assess the effect on $\\tilde{g}(h)$ yields a leading-order round-off error bound\n$$\nE_{\\mathrm{ro}}(h)\\lesssim \\frac{u|J(p_0)|}{h}.\n$$\n\nCombining truncation and round-off contributions, a leading-order error model for the central-difference estimator is\n$$\nE(h)\\approx \\frac{|J^{(3)}(p_0)|}{6}h^2+\\frac{u|J(p_0)|}{h}.\n$$\nTo find the optimal $h_\\star$, we minimize $E(h)$ over $h>0$. Differentiating with respect to $h$ and setting to zero,\n$$\n\\frac{\\mathrm{d}E}{\\mathrm{d}h}=\\frac{|J^{(3)}(p_0)|}{3}h-\\frac{u|J(p_0)|}{h^2}=0\n\\quad\\Longrightarrow\\quad\n\\frac{|J^{(3)}(p_0)|}{3}h^3=u|J(p_0)|.\n$$\nSolving for $h$ gives the closed-form optimal step size\n$$\nh_\\star=\\left(\\frac{3u|J(p_0)|}{|J^{(3)}(p_0)|}\\right)^{1/3}.\n$$\n\nWe now estimate the required local quantities from the supplied data. First compute the central-difference estimates at $h_1$ and $2h_1$:\n$$\n\\tilde{g}(h_1)=\\frac{J(p_0+h_1)-J(p_0-h_1)}{2h_1}\n=\\frac{0.015003450000000-0.014997350000000}{2\\times 10^{-3}}\n=\\frac{6.10\\times 10^{-6}}{2\\times 10^{-3}}=0.00305,\n$$\n$$\n\\tilde{g}(2h_1)=\\frac{J(p_0+2h_1)-J(p_0-2h_1)}{2(2h_1)}\n=\\frac{0.015008000000000-0.014995200000000}{4\\times 10^{-3}}\n=\\frac{1.28\\times 10^{-5}}{4\\times 10^{-3}}=0.0032.\n$$\nFor the central-difference estimator, the leading truncation error satisfies\n$$\n\\tilde{g}(h)=J'(p_0)+\\frac{J^{(3)}(p_0)}{6}h^2+\\mathcal{O}(h^4),\\qquad\n\\tilde{g}(2h)=J'(p_0)+\\frac{J^{(3)}(p_0)}{6}(2h)^2+\\mathcal{O}(h^4).\n$$\nSubtracting these two expressions eliminates $J'(p_0)$ and yields\n$$\n\\tilde{g}(2h)-\\tilde{g}(h)\\approx \\frac{J^{(3)}(p_0)}{6}\\left(4h^2-h^2\\right)=\\frac{J^{(3)}(p_0)}{2}h^2,\n$$\nso that a local smoothness estimate is\n$$\nJ^{(3)}(p_0)\\approx \\frac{2\\left(\\tilde{g}(2h)-\\tilde{g}(h)\\right)}{h^2}.\n$$\nUsing $h=h_1=10^{-3}$ and the values above,\n$$\nJ^{(3)}(p_0)\\approx \\frac{2\\left(0.0032-0.00305\\right)}{\\left(10^{-3}\\right)^2}\n=\\frac{2\\cdot 1.5\\times 10^{-4}}{10^{-6}}=300.\n$$\nWe also have $|J(p_0)|=0.015000000000000\\approx 0.015$.\n\nFinally, substitute $u=2.22\\times 10^{-16}$, $|J(p_0)|=0.015$, and $|J^{(3)}(p_0)|=300$ into the expression for $h_\\star$:\n$$\nh_\\star=\\left(\\frac{3\\cdot 2.22\\times 10^{-16}\\cdot 0.015}{300}\\right)^{1/3}\n=\\left(\\frac{9.99\\times 10^{-18}}{300}\\right)^{1/3}\n=\\left(3.33\\times 10^{-20}\\right)^{1/3}.\n$$\nCompute the cube root:\n$$\n\\left(3.33\\times 10^{-20}\\right)^{1/3}=\\left(33.3\\times 10^{-21}\\right)^{1/3}\\approx 3.22\\times 10^{-7}.\n$$\nRounded to three significant figures and expressed in radians, the optimal step size is $3.22\\times 10^{-7}$.",
            "answer": "$$\\boxed{3.22\\times 10^{-7}}$$"
        },
        {
            "introduction": "The elegant theory of adjoints rests on the assumption that the governing equations are differentiable. In practice, many physical models used in aerospace CFD—such as turbulence closures, transition models, and wall functions—contain non-differentiable switches that violate this assumption. This exercise explores the theoretical implications of these switches and the practical techniques, like model smoothing and blending, used to ensure a robust and meaningful adjoint sensitivity analysis .",
            "id": "3942339",
            "problem": "In aerospace Computational Fluid Dynamics (CFD), adjoint-based sensitivity analysis is used to compute gradients of an objective functional with respect to design parameters for flows governed by the Reynolds-Averaged Navier–Stokes (RANS) equations with turbulence closures and wall models. Consider the abstract steady-state partial differential equation (PDE)-constrained optimization problem\n$$\n\\text{Find } u(\\alpha) \\text{ such that } R(u(\\alpha),\\alpha)=0, \\quad J(\\alpha)=\\mathcal{J}(u(\\alpha),\\alpha),\n$$\nwhere $u$ denotes the state (e.g., mean velocity and pressure fields), $\\alpha$ denotes a design parameter (e.g., shape, trip location, or inflow condition), $R$ denotes the discrete residual representing the spatial discretization of the governing equations and closures, and $\\mathcal{J}$ denotes a differentiable output functional such as drag. The adjoint method relies on the existence of the Gâteaux derivative of the mapping $\\alpha \\mapsto u(\\alpha)$ and of $\\mathcal{J}$, together with differentiation under the residual $R$.\n\nTo reason from first principles, consider the following scalar diffusion model that abstracts how turbulence model switches, transition triggers, or wall-function regime changes enter into $R$ through piecewise definitions:\n$$\n-\\nabla \\cdot \\left(\\mu(u,\\alpha)\\nabla u\\right)=f \\quad \\text{in } \\Omega,\\qquad u=0 \\quad \\text{on } \\partial \\Omega,\n$$\nwith output\n$$\n\\mathcal{J}(u,\\alpha)=\\int_{\\Omega} q \\, u \\, \\mathrm{d}\\Omega,\n$$\nwhere $f$ and $q$ are given smooth functions and the effective viscosity $\\mu$ is modeled by a regime switch\n$$\n\\mu(u,\\alpha)=\\mu_{\\ell}+\\left(\\mu_{t}-\\mu_{\\ell}\\right)\\,\\mathrm{H}\\big(T(u,\\alpha)\\big),\n$$\nwith $\\mu_{\\ell}>0$, $\\mu_{t}>\\mu_{\\ell}$ constants, $\\mathrm{H}$ the Heaviside step function, and $T(u,\\alpha)$ a smooth scalar trigger (e.g., based on a local Reynolds number, wall-distance, or turbulence sensor) such that $T(u,\\alpha)\\ge 0$ selects the turbulent/wall-log regime and $T(u,\\alpha)<0$ selects the laminar/viscous-sub-layer regime. Assume the forward problem is well-posed in an appropriate function space for each fixed $\\alpha$.\n\nBy definition, the directional Gâteaux derivative of $\\mathcal{J}$ in direction $\\dot{\\alpha}$ at $\\alpha=\\alpha_{0}$ is the limit\n$$\n\\mathrm{D}\\mathcal{J}(\\alpha_{0})[\\dot{\\alpha}]=\\lim_{\\varepsilon\\to 0}\\frac{\\mathcal{J}\\big(u(\\alpha_{0}+\\varepsilon \\dot{\\alpha}),\\alpha_{0}+\\varepsilon \\dot{\\alpha}\\big)-\\mathcal{J}\\big(u(\\alpha_{0}),\\alpha_{0}\\big)}{\\varepsilon},\n$$\nprovided the limit exists. The adjoint method seeks to evaluate $\\mathrm{D}\\mathcal{J}(\\alpha_{0})[\\dot{\\alpha}]$ without explicitly computing the state variation $\\dot{u}=\\mathrm{D}u(\\alpha_{0})[\\dot{\\alpha}]$, by differentiating $R(u,\\alpha)=0$ and solving the associated adjoint equation. This procedure relies on the differentiability of $R$ with respect to $(u,\\alpha)$.\n\nNow address the following in the context of adjoint robustness for RANS closures, transition models, and wall functions:\n\n- Explain, from the definition of the Gâteaux derivative and linearization of the residual, the implications of the non-differentiable switch embedded in $\\mu(u,\\alpha)$ for the existence and regularity of the adjoint equation, particularly when the threshold set $\\{x\\in \\Omega: T(u(\\alpha_{0}),\\alpha_{0})=0\\}$ is nontrivial or moves under perturbations $\\dot{\\alpha}$.\n\n- Consider replacing $\\mathrm{H}$ by a smooth sigmoid $\\sigma_{\\varepsilon}$, where $\\sigma_{\\varepsilon}\\in C^{\\infty}(\\mathbb{R})$, $\\sigma_{\\varepsilon}(z)\\to \\mathrm{H}(z)$ pointwise as $\\varepsilon\\to 0^{+}$, and $\\sigma_{\\varepsilon}'$ is bounded and localized. Discuss the impact of this smoothing on the well-posedness of the continuous adjoint and on the consistency of the resulting gradients as $\\varepsilon\\to 0^{+}$ under suitable measure-theoretic conditions on the threshold set.\n\n- In the realm of wall models, consider a wall function that switches regimes based on nondimensional wall distance $y^{+}$, with a discontinuous switch between a viscous sublayer law and a logarithmic law. Discuss the effect of replacing the switch by a $C^{1}$ blending function of $y^{+}$ on adjoint robustness.\n\nSelect all statements that are correct based on this analysis.\n\nA. In the model problem, if $\\mu(u,\\alpha)$ uses a Heaviside switch, the linearized operator for the adjoint exists for any perturbation because the derivative of the Heaviside is zero almost everywhere, so no singular terms appear in the differentiation of $R$.\n\nB. Replacing $\\mathrm{H}$ by a smooth sigmoid $\\sigma_{\\varepsilon}$ with small $\\varepsilon$ yields a well-defined continuous adjoint whose gradient converges to the true shape derivative as $\\varepsilon\\to 0^{+}$, provided the threshold set $\\{x\\in \\Omega: T(u(\\alpha_{0}),\\alpha_{0})=0\\}$ has measure zero and the solution map is sufficiently continuous with respect to $(u,\\alpha)$.\n\nC. Mesh refinement alone removes the non-differentiability introduced by model switches, making the continuous adjoint well-posed without any smoothing.\n\nD. Using a blended wall function that transitions continuously between viscous sublayer and log-law with a $C^{1}$ blending function in wall distance produces a residual that is differentiable with respect to design parameters, which improves adjoint robustness.\n\nE. In transitional models, replacing a hard trip based on a critical momentum-thickness Reynolds number by an intermittency transport equation eliminates all non-differentiabilities and guarantees a twice continuously differentiable discrete residual with respect to any design variable.",
            "solution": "The problem statement presents a rigorous and scientifically sound abstraction of a critical issue in modern Computational Fluid Dynamics (CFD), specifically the application of adjoint-based sensitivity analysis and optimization to flows governed by the Reynolds-Averaged Navier–Stokes (RANS) equations. The use of a scalar diffusion model with a Heaviside function to represent non-differentiable switches in turbulence models, transition models, or wall functions is a standard and effective technique for analyzing the mathematical pathologies that arise. The problem is well-posed, objective, and directly relevant to the stated topic. Therefore, the problem is valid, and we may proceed to the analysis.\n\nThe core of the issue lies in the application of variational calculus to derive the adjoint equations, a procedure which relies on the Gâteaux differentiability of the residual of the governing equations, $R(u, \\alpha)$, with respect to both the state vector $u$ and the design parameters $\\alpha$.\n\nLet the weak form of the residual for the model problem be given by the functional $R(u, \\alpha)[v]$ for a test function $v$:\n$$\nR(u, \\alpha)[v] = \\int_{\\Omega} \\mu(u,\\alpha)\\nabla u \\cdot \\nabla v \\, \\mathrm{d}\\Omega - \\int_{\\Omega} f v \\, \\mathrm{d}\\Omega = 0\n$$\nwhere $u$ and $v$ belong to a suitable function space, e.g., $H_0^1(\\Omega)$. The adjoint method requires linearizing this residual to derive the adjoint equation. The Gâteaux derivative of $R$ with respect to $u$ in the direction $\\dot{u}$ is\n$$\n\\mathrm{D}_{u} R(u,\\alpha)[\\dot{u}, v] = \\lim_{\\epsilon\\to 0} \\frac{R(u+\\epsilon\\dot{u}, \\alpha)[v] - R(u,\\alpha)[v]}{\\epsilon}.\n$$\nWith the given form of $\\mu(u,\\alpha) = \\mu_{\\ell}+\\left(\\mu_{t}-\\mu_{\\ell}\\right)\\,\\mathrm{H}\\big(T(u,\\alpha)\\big)$, the derivative of $\\mu$ with respect to $u$ must be considered. Formally, using the chain rule, the derivative of $\\mathrm{H}(T(u,\\alpha))$ involves $\\mathrm{H}'(T(u,\\alpha)) = \\delta(T(u,\\alpha))$, where $\\delta$ is the Dirac delta distribution. This leads to:\n$$\n\\mathrm{D}_{u} R(u,\\alpha)[\\dot{u}, v] = \\int_{\\Omega} \\mu(u,\\alpha)\\nabla \\dot{u} \\cdot \\nabla v \\, \\mathrm{d}\\Omega + \\int_{\\Omega} (\\mu_{t}-\\mu_{\\ell})\\delta(T(u,\\alpha)) (\\mathrm{D}_{u}T(u,\\alpha)[\\dot{u}]) (\\nabla u \\cdot \\nabla v) \\, \\mathrm{d}\\Omega\n$$\nThe second term contains a Dirac delta distribution, which is singular. It localizes the integral to the \"switching surface\" or \"threshold set\" $\\mathcal{S} = \\{x\\in \\Omega: T(u(\\alpha),\\alpha)=0\\}$. The presence of this distributional term means the linearized operator $\\mathrm{D}_{u} R$ is not a standard bounded linear operator on typical function spaces. This may render the continuous adjoint equation ill-posed or lead to an adjoint solution $\\psi$ that is itself a distribution, rather than a regular function. The entire adjoint framework, which relies on well-behaved linear operators, is thus compromised. This phenomenon is often termed \"adjoint failure\" or a lack of \"adjoint robustness.\"\n\nSimilar issues arise when differentiating with respect to $\\alpha$. A perturbation $\\dot{\\alpha}$ may cause the switching surface $\\mathcal{S}$ to move, leading to a sensitivity that is concentrated on this moving interface. The limit defining the Gâteaux derivative may not exist, or it may depend on the sign of the perturbation, yielding inconsistent gradients.\n\nNow we evaluate each option.\n\n**A. In the model problem, if $\\mu(u,\\alpha)$ uses a Heaviside switch, the linearized operator for the adjoint exists for any perturbation because the derivative of the Heaviside is zero almost everywhere, so no singular terms appear in the differentiation of $R$.**\n\nThis statement is fundamentally flawed. While it is true that the classical derivative of the Heaviside function is zero for all non-zero arguments (i.e., almost everywhere), this perspective is incorrect in the context of functional analysis and distributions. The proper derivative in the sense of distributions is the Dirac delta function, $\\mathrm{H}'(z) = \\delta(z)$. As shown in the analysis above, the linearization of the residual $R$ explicitly produces a term containing $\\delta(T(u,\\alpha))$. This is a singular term that fundamentally compromises the well-posedness of the standard adjoint formulation. The claim that \"no singular terms appear\" is false.\n\n**Verdict: Incorrect.**\n\n**B. Replacing $\\mathrm{H}$ by a smooth sigmoid $\\sigma_{\\varepsilon}$ with small $\\varepsilon$ yields a well-defined continuous adjoint whose gradient converges to the true shape derivative as $\\varepsilon\\to 0^{+}$, provided the threshold set $\\{x\\in \\Omega: T(u(\\alpha_{0}),\\alpha_{0})=0\\}$ has measure zero and the solution map is sufficiently continuous with respect to $(u,\\alpha)$.**\n\nThis describes the standard and mathematically rigorous approach to address the non-differentiability. Replacing the Heaviside function $\\mathrm{H}$ with a smooth approximation $\\sigma_{\\varepsilon}$ (a mollifier) makes the viscosity $\\mu$ and consequently the residual $R$ differentiable with respect to $u$ and $\\alpha$. The problematic Dirac delta $\\delta$ is replaced by the smooth, bounded function $\\sigma'_{\\varepsilon}$. This ensures that the linearized operator $\\mathrm{D}_u R$ is well-behaved, leading to a well-posed continuous adjoint equation. The core of the statement is about the convergence of the gradient obtained from this regularized problem to the \"true\" derivative of the original non-smooth problem. For problems where interfaces or internal boundaries move, the correct mathematical concept for the derivative is the shape derivative. It is a known result in shape optimization theory that, under certain conditions, the gradients from the regularized problem converge to the correct shape derivative as the smoothing parameter $\\varepsilon \\to 0$. The conditions mentioned—that the threshold set where the switch occurs must have measure zero (i.e., it is a surface, not a region with volume) and that the state $u$ depends continuously on the parameter $\\alpha$—are precisely the types of assumptions required for such convergence proofs.\n\n**Verdict: Correct.**\n\n**C. Mesh refinement alone removes the non-differentiability introduced by model switches, making the continuous adjoint well-posed without any smoothing.**\n\nThis is incorrect. The non-differentiability is a property of the continuous governing equations, not an artifact of discretization. Mesh refinement provides a better approximation of the solution to the continuous problem but does not alter the fundamental nature of that problem. If the continuous problem is non-differentiable, the discrete problem will inherit this characteristic. In practice, as the mesh is refined, the location of the non-differentiable switch is resolved more sharply, often leading to increasingly oscillatory and non-convergent behavior of the discrete gradients. This is a classic symptom that demonstrates the need to modify the underlying continuous model (e.g., by smoothing) rather than simply refining the mesh.\n\n**Verdict: Incorrect.**\n\n**D. Using a blended wall function that transitions continuously between viscous sublayer and log-law with a $C^{1}$ blending function in wall distance produces a residual that is differentiable with respect to design parameters, which improves adjoint robustness.**\n\nThis statement presents a concrete, practical application of the principle discussed in option B. Standard wall functions for RANS models often contain an `if-then-else` structure to switch between the viscous sublayer and the logarithmic layer based on the non-dimensional wall distance $y^+$. Since $y^+$ depends on the wall shear stress, which is part of the solution $u$, this `if` statement introduces a Heaviside-like non-differentiability into the residual $R$. Replacing this sharp switch with a blending function that is continuously differentiable ($C^1$) with respect to its arguments (like $y^+$) makes the wall boundary condition smooth. This, in turn, makes the overall residual $R(u, \\alpha)$ differentiable with respect to the state $u$ and design parameters $\\alpha$. As established, differentiability of the residual is the prerequisite for a well-posed adjoint system. Therefore, this modification directly \"improves adjoint robustness\" by eliminating the source of the mathematical pathology.\n\n**Verdict: Correct.**\n\n**E. In transitional models, replacing a hard trip based on a critical momentum-thickness Reynolds number by an intermittency transport equation eliminates all non-differentiabilities and guarantees a twice continuously differentiable discrete residual with respect to any design variable.**\n\nThis claim is too strong and therefore incorrect. Replacing a \"hard trip\" (a Heaviside switch based on $Re_{\\theta} > Re_{\\theta,c}$) with a transport equation for intermittency (e.g., the $\\gamma-Re_{\\theta}$ model) is indeed a significant step towards creating a more differentiable model. It removes the most severe non-differentiability. However, it does not \"eliminate all non-differentiabilities\". Modern transport-based transition models contain highly complex and non-linear source terms based on empirical correlations. These terms can involve functions like `max`, `min`, `abs`, or clipping/limiting functions that are not differentiable everywhere. Furthermore, the numerical discretization of the transport equations often employs flux limiters or other schemes that are, at best, piecewise smooth and certainly not $C^2$. Therefore, guaranteeing a \"twice continuously differentiable ($C^2$) discrete residual\" is an overstatement. While the model is substantially smoother, it is not guaranteed to be free of all non-differentiabilities, and $C^2$ continuity is highly unlikely.\n\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{BD}$$"
        }
    ]
}