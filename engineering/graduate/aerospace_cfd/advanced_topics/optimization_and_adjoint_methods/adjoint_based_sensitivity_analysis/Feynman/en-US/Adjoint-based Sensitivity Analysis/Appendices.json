{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of reliable adjoint-based optimization is rigorous verification of the computed gradients. This practice moves beyond a simple \"plug-and-play\" application of finite differences to explore the numerical subtleties required for a trustworthy gradient check. By analyzing the fundamental trade-off between truncation and round-off error, you will derive the optimal perturbation step size, a critical skill for ensuring that your verification is both accurate and meaningful.",
            "id": "3942381",
            "problem": "Consider a steady, inviscid aerodynamic analysis governed by a discrete residual equation $\\mathbf{R}(\\mathbf{u},p)=\\mathbf{0}$, where $\\mathbf{u}$ is the converged flow state and $p$ is a scalar design parameter representing the angle of attack in radians. The objective is the drag coefficient $J(\\mathbf{u}(p),p)$, and the total derivative $\\mathrm{d}J/\\mathrm{d}p$ at a baseline $p_0$ is available from an adjoint computation. To validate the adjoint using a finite-difference gradient check, you will construct a central-difference estimator and determine the optimal perturbation size that balances truncation and floating-point round-off errors.\n\nAssume the following:\n\n- Each converged objective evaluation $J(p)$ satisfies the standard floating-point model $\\mathrm{fl}(J(p))=J(p)(1+\\delta)$ with $|\\delta|\\le u$, where $u$ is the unit roundoff of double-precision arithmetic, $u=2.22\\times 10^{-16}$.\n- The function $J(p)$ is sufficiently smooth near $p_0$ so that Taylor expansions about $p_0$ are valid.\n- The baseline parameter is $p_0=2\\pi/180$ radians.\n- You have performed five high-fidelity converged flow solves yielding the following objective values near $p_0$ (angles in radians):\n  - $J(p_0)=0.015000000000000$,\n  - $J(p_0+h_1)=0.015003450000000$,\n  - $J(p_0-h_1)=0.014997350000000$,\n  - $J(p_0+2h_1)=0.015008000000000$,\n  - $J(p_0-2h_1)=0.014995200000000$,\n  where $h_1=1.0\\times 10^{-3}$.\n\nTasks:\n\n1) Starting from Taylor expansions of $J(p_0\\pm h)$ about $p_0$ and the definition of a central difference, formulate a central-difference gradient check $\\tilde{g}(h)$ for $\\mathrm{d}J/\\mathrm{d}p$ at $p_0$ and identify the leading-order truncation error term in powers of $h$.\n\n2) Using the floating-point model for $J$-evaluations and standard rules of error propagation for sums and differences, derive a leading-order bound for the round-off contribution to the error in $\\tilde{g}(h)$ in terms of $u$, $|J(p_0)|$, and $h$.\n\n3) Combine your truncation and round-off models into a single leading-order error model $E(h)$ and, by minimizing $E(h)$ with respect to $h$, derive a closed-form expression for the optimal step size $h_\\star$ in terms of $u$, a local estimate of $|J^{(3)}(p_0)|$, and $|J(p_0)|$.\n\n4) Use the supplied objective values to obtain:\n   - the central-difference estimates $\\tilde{g}(h_1)$ and $\\tilde{g}(2h_1)$,\n   - a local smoothness estimate of $|J^{(3)}(p_0)|$ based on the difference of these two central-difference estimates,\n   - and then compute the numerical value of $h_\\star$ in radians.\n\nExpress your final numerical answer for $h_\\star$ in radians, rounded to three significant figures. The final answer must be a single number.",
            "solution": "We begin from first principles: a central-difference gradient check is constructed by evaluating $J$ at symmetric perturbations about $p_0$. Let $\\Delta p=h$. A Taylor expansion of $J$ about $p_0$ gives\n$$\nJ(p_0+h)=J(p_0)+J'(p_0)h+\\frac{J''(p_0)}{2}h^2+\\frac{J^{(3)}(p_0)}{6}h^3+\\mathcal{O}(h^4),\n$$\n$$\nJ(p_0-h)=J(p_0)-J'(p_0)h+\\frac{J''(p_0)}{2}h^2-\\frac{J^{(3)}(p_0)}{6}h^3+\\mathcal{O}(h^4).\n$$\nSubtracting these two expressions yields\n$$\nJ(p_0+h)-J(p_0-h)=2J'(p_0)h+\\frac{J^{(3)}(p_0)}{3}h^3+\\mathcal{O}(h^5).\n$$\nDividing by $2h$ gives the central-difference estimator\n$$\n\\tilde{g}(h)\\equiv \\frac{J(p_0+h)-J(p_0-h)}{2h}=J'(p_0)+\\frac{J^{(3)}(p_0)}{6}h^2+\\mathcal{O}(h^4).\n$$\nTherefore, the leading-order truncation error is $\\frac{J^{(3)}(p_0)}{6}h^2$.\n\nNext, we model round-off. Each objective value is returned in floating point as $\\mathrm{fl}(J(p))=J(p)(1+\\delta)$ with $|\\delta|\\le u$. The numerator of the central difference computed in floating point is\n$$\n\\mathrm{fl}(J(p_0+h))-\\mathrm{fl}(J(p_0-h))=[J(p_0+h)-J(p_0-h)]+[J(p_0+h)\\delta_1-J(p_0-h)\\delta_2],\n$$\nwhere $|\\delta_1|,|\\delta_2|\\le u$. The worst-case absolute perturbation of the numerator due to round-off is bounded by\n$$\n|J(p_0+h)\\delta_1-J(p_0-h)\\delta_2|\\le u\\left(|J(p_0+h)|+|J(p_0-h)|\\right).\n$$\nFor sufficiently small $h$, $|J(p_0\\pm h)|\\approx |J(p_0)|$, hence\n$$\n|J(p_0+h)\\delta_1-J(p_0-h)\\delta_2|\\lesssim 2u|J(p_0)|.\n$$\nDividing by the exact denominator $2h$ to assess the effect on $\\tilde{g}(h)$ yields a leading-order round-off error bound\n$$\nE_{\\mathrm{ro}}(h)\\lesssim \\frac{u|J(p_0)|}{h}.\n$$\n\nCombining truncation and round-off contributions, a leading-order error model for the central-difference estimator is\n$$\nE(h)\\approx \\frac{|J^{(3)}(p_0)|}{6}h^2+\\frac{u|J(p_0)|}{h}.\n$$\nTo find the optimal $h_\\star$, we minimize $E(h)$ over $h>0$. Differentiating with respect to $h$ and setting to zero,\n$$\n\\frac{\\mathrm{d}E}{\\mathrm{d}h}=\\frac{|J^{(3)}(p_0)|}{3}h-\\frac{u|J(p_0)|}{h^2}=0\n\\quad\\Longrightarrow\\quad\n\\frac{|J^{(3)}(p_0)|}{3}h^3=u|J(p_0)|.\n$$\nSolving for $h$ gives the closed-form optimal step size\n$$\nh_\\star=\\left(\\frac{3u|J(p_0)|}{|J^{(3)}(p_0)|}\\right)^{1/3}.\n$$\n\nWe now estimate the required local quantities from the supplied data. First compute the central-difference estimates at $h_1$ and $2h_1$:\n$$\n\\tilde{g}(h_1)=\\frac{J(p_0+h_1)-J(p_0-h_1)}{2h_1}\n=\\frac{0.015003450000000-0.014997350000000}{2\\times 10^{-3}}\n=\\frac{6.10\\times 10^{-6}}{2\\times 10^{-3}}=0.00305,\n$$\n$$\n\\tilde{g}(2h_1)=\\frac{J(p_0+2h_1)-J(p_0-2h_1)}{2(2h_1)}\n=\\frac{0.015008000000000-0.014995200000000}{4\\times 10^{-3}}\n=\\frac{1.28\\times 10^{-5}}{4\\times 10^{-3}}=0.0032.\n$$\nFor the central-difference estimator, the leading truncation error satisfies\n$$\n\\tilde{g}(h)=J'(p_0)+\\frac{J^{(3)}(p_0)}{6}h^2+\\mathcal{O}(h^4),\\qquad\n\\tilde{g}(2h)=J'(p_0)+\\frac{J^{(3)}(p_0)}{6}(2h)^2+\\mathcal{O}(h^4).\n$$\nSubtracting these two expressions eliminates $J'(p_0)$ and yields\n$$\n\\tilde{g}(2h)-\\tilde{g}(h)\\approx \\frac{J^{(3)}(p_0)}{6}\\left(4h^2-h^2\\right)=\\frac{J^{(3)}(p_0)}{2}h^2,\n$$\nso that a local smoothness estimate is\n$$\nJ^{(3)}(p_0)\\approx \\frac{2\\left(\\tilde{g}(2h)-\\tilde{g}(h)\\right)}{h^2}.\n$$\nUsing $h=h_1=10^{-3}$ and the values above,\n$$\nJ^{(3)}(p_0)\\approx \\frac{2\\left(0.0032-0.00305\\right)}{\\left(10^{-3}\\right)^2}\n=\\frac{2\\cdot 1.5\\times 10^{-4}}{10^{-6}}=300.\n$$\nWe also have $|J(p_0)|=0.015000000000000\\approx 0.015$.\n\nFinally, substitute $u=2.22\\times 10^{-16}$, $|J(p_0)|=0.015$, and $|J^{(3)}(p_0)|=300$ into the expression for $h_\\star$:\n$$\nh_\\star=\\left(\\frac{3\\cdot 2.22\\times 10^{-16}\\cdot 0.015}{300}\\right)^{1/3}\n=\\left(\\frac{9.99\\times 10^{-18}}{300}\\right)^{1/3}\n=\\left(3.33\\times 10^{-20}\\right)^{1/3}.\n$$\nWe compute the cube root: $(3.33\\times 10^{-20})^{1/3} = (3.33)^{1/3} \\times 10^{-20/3} \\approx 1.493 \\times (2.154\\times 10^{-7}) \\approx 3.22\\times 10^{-7}$.\n\nRounded to three significant figures and expressed in radians, the optimal step size is $3.22\\times 10^{-7}$.",
            "answer": "$$\\boxed{3.22\\times 10^{-7}}$$"
        },
        {
            "introduction": "While checking a gradient with respect to one or two parameters is straightforward, real-world aerospace design problems often involve hundreds or thousands of variables, making an exhaustive check impossible. This exercise introduces the Randomized Directional Derivative (RDD) test, a powerful and practical method for verifying high-dimensional gradients. You will be challenged to design a statistically robust verification protocol, learning to distinguish sound methodologies from plausible but flawed alternatives.",
            "id": "3942407",
            "problem": "Consider a steady compressible flow simulation in Aerospace Computational Fluid Dynamics (CFD), modeled by a discrete residual equation $R(U,p) = 0$ for the state vector $U \\in \\mathbb{R}^{m}$ and a set of design or operating parameters $p \\in \\mathbb{R}^{n}$. Let the objective be a scalar functional $J(U,p)$ such as drag, lift, or a weighted integral of flow variables. Assume that for each $p$, a converged solution $U(p)$ is obtained satisfying $R(U(p),p) = 0$, and that differentiability conditions implied by the Implicit Function Theorem hold in a neighborhood of the nominal parameter $p$.\n\nYou have implemented an adjoint solver to compute the gradient $g_{\\mathrm{adj}} = \\frac{dJ}{dp} \\in \\mathbb{R}^{n}$ via the discrete adjoint equation associated with $R(U,p) = 0$, and you wish to validate $g_{\\mathrm{adj}}$ using a Randomized Directional Derivative (RDD) test. The idea is to compare the adjoint directional derivatives $v^{T} g_{\\mathrm{adj}}$ with finite-difference approximations of the true directional derivative of the composite mapping $\\alpha \\mapsto J(U(p+\\alpha v), p+\\alpha v)$ at $\\alpha = 0$, over random directions $v \\in \\mathbb{R}^{n}$.\n\nWhich option correctly proposes a scientifically sound RDD test and an acceptance criterion that can be justified from first principles, including how to select random directions, how to form the finite-difference approximation, how to ensure consistent state solves, and how to define a robust pass/fail condition?\n\nA. Draw $N$ independent random directions $v_{i} \\in \\mathbb{R}^{n}$ by sampling each entry from a standard normal distribution and normalizing to unit length so that $\\|v_{i}\\|_{2} = 1$, which induces a distribution uniform on the unit sphere in $\\mathbb{R}^{n}$. For each $i \\in \\{1,\\dots,N\\}$, compute the adjoint directional derivative $d_{\\mathrm{adj},i} = v_{i}^{T} g_{\\mathrm{adj}}$. Then, for a small step size $\\varepsilon > 0$ chosen so that truncation error is $\\mathcal{O}(\\varepsilon^{2})$ and roundoff/solver noise are negligible (for example, by scaling $\\varepsilon$ to the magnitude of $p$ and machine precision), form the central finite-difference approximation\n$$\nd_{\\mathrm{fd},i} = \\frac{J\\big(U(p+\\varepsilon v_{i}),\\, p+\\varepsilon v_{i}\\big) - J\\big(U(p-\\varepsilon v_{i}),\\, p-\\varepsilon v_{i}\\big)}{2\\,\\varepsilon},\n$$\nensuring that each perturbed state $U(p\\pm \\varepsilon v_{i})$ is converged to the same residual tolerance as $U(p)$. Define the symmetric relative error\n$$\n\\delta_{i} = \\frac{\\big|d_{\\mathrm{adj},i} - d_{\\mathrm{fd},i}\\big|}{\\big|d_{\\mathrm{adj},i}\\big| + \\big|d_{\\mathrm{fd},i}\\big|}.\n$$\nAccept the adjoint gradient if $\\max_{1\\le i \\le N} \\delta_{i} \\le \\tau$, where $\\tau$ is a tolerance selected to exceed the combined effects of finite-difference truncation and solver/roundoff error, and optionally corroborate with the sample mean or median being $\\le \\tau$.\n\nB. Draw $N$ independent random directions $v_{i}$ with entries sampled uniformly from $[-1,1]$ without normalization. For each $i$, compute $d_{\\mathrm{adj},i} = v_{i}^{T} g_{\\mathrm{adj}}$. Form a forward finite difference\n$$\nd_{\\mathrm{fd},i} = \\frac{J\\big(U(p),\\, p\\big) - J\\big(U(p+\\varepsilon v_{i}),\\, p+\\varepsilon v_{i}\\big)}{\\varepsilon},\n$$\nusing the previously converged state $U(p)$ and skipping re-solves for the perturbed states to save cost. Accept the adjoint gradient if the average absolute error $\\frac{1}{N}\\sum_{i=1}^{N} \\big|d_{\\mathrm{adj},i} - d_{\\mathrm{fd},i}\\big|$ is less than a tolerance $\\tau$.\n\nC. Draw $N$ random Rademacher directions $v_{i}$ with entries $v_{i,j} \\in \\{-1,+1\\}$ for each component $j$, and do not normalize. For each $i$, compute $d_{\\mathrm{adj},i} = v_{i}^{T} g_{\\mathrm{adj}}$. Approximate the finite-difference directional derivative by\n$$\nd_{\\mathrm{fd},i} = \\frac{J\\big(U(p+\\varepsilon v_{i}),\\, p+\\varepsilon v_{i}\\big) - J\\big(U(p-\\varepsilon v_{i}),\\, p-\\varepsilon v_{i}\\big)}{2\\,\\varepsilon\\, \\|v_{i}\\|_{2}^{2}},\n$$\nand accept if the Pearson correlation coefficient between the sets $\\{d_{\\mathrm{adj},i}\\}$ and $\\{d_{\\mathrm{fd},i}\\}$ exceeds $0.9$.\n\nD. Test only axis-aligned directions by setting $v_{i} = e_{i}$, the $i$-th coordinate vector in $\\mathbb{R}^{n}$, for $i = 1,\\dots,n$. For each $i$, compute $d_{\\mathrm{adj},i} = e_{i}^{T} g_{\\mathrm{adj}}$, and the forward finite difference $d_{\\mathrm{fd},i} = \\frac{J\\big(U(p+\\varepsilon e_{i}),\\, p+\\varepsilon e_{i}\\big) - J\\big(U(p),\\, p\\big)}{\\varepsilon}$. Accept if $\\max_{1\\le i \\le n} \\big|d_{\\mathrm{adj},i} - d_{\\mathrm{fd},i}\\big| \\le \\tau$; randomization is unnecessary because coordinate directions suffice.",
            "solution": "The problem statement is valid. It concerns the verification of a discrete adjoint gradient in computational fluid dynamics (CFD), a standard and critical procedure in disciplines that employ large-scale simulation and optimization. The problem is well-posed, scientifically grounded in numerical analysis and optimization theory, and uses clear, objective terminology. I will now proceed to derive the principles of gradient verification and evaluate each option.\n\n### Theoretical Foundation for Gradient Verification\n\nThe goal is to verify the computed adjoint gradient, $g_{\\mathrm{adj}} = \\frac{dJ}{dp}$, where $J$ is a scalar function of the parameters $p \\in \\mathbb{R}^{n}$ and the state $U \\in \\mathbb{R}^{m}$, which implicitly depends on $p$ through the state equation $R(U(p), p) = 0$. The total derivative $\\frac{dJ}{dp}$ provides the linear sensitivity of the composite function $\\mathcal{J}(p) = J(U(p), p)$ to perturbations in $p$.\n\nThe core principle of verification relies on the Taylor expansion of $\\mathcal{J}(p)$ around a point $p$ in a direction $v \\in \\mathbb{R}^{n}$. Let $f(\\alpha) = \\mathcal{J}(p + \\alpha v) = J(U(p+\\alpha v), p+\\alpha v)$. The Taylor expansion of $f(\\alpha)$ around $\\alpha=0$ is:\n$$ f(\\alpha) = f(0) + \\alpha f'(0) + \\frac{\\alpha^2}{2}f''(0) + \\mathcal{O}(\\alpha^3) $$\nThe directional derivative of $\\mathcal{J}$ at $p$ in the direction $v$ is, by definition, $f'(0)$. By the chain rule, this is:\n$$ f'(0) = \\left. \\frac{d\\mathcal{J}}{d\\alpha} \\right|_{\\alpha=0} = \\left( \\frac{dJ}{dp} \\right)^T v = g^T v $$\nwhere $g = \\frac{dJ}{dp}$ is the gradient vector we aim to verify. The adjoint method provides a means to compute $g_{\\mathrm{adj}}$, which should be equal to the true gradient $g$. Therefore, for any direction $v$, we must have the equality $g_{\\mathrm{adj}}^T v = f'(0)$.\n\nA verification test, such as the Randomized Directional Derivative (RDD) test, compares the analytically computed directional derivative, $d_{\\mathrm{adj}} = g_{\\mathrm{adj}}^T v$, with a numerical approximation of $f'(0)$ obtained via finite differences.\n\nA high-quality finite-difference approximation is the second-order accurate central difference:\n$$ f'(0) \\approx \\frac{f(\\varepsilon) - f(-\\varepsilon)}{2\\varepsilon} = \\frac{J(U(p+\\varepsilon v), p+\\varepsilon v) - J(U(p-\\varepsilon v), p-\\varepsilon v)}{2\\varepsilon} $$\nThis approximation has a truncation error of $\\mathcal{O}(\\varepsilon^2)$. To compute this, one must first solve the state equation for the perturbed parameters, i.e., find $U(p+\\varepsilon v)$ satisfying $R(U, p+\\varepsilon v)=0$ and $U(p-\\varepsilon v)$ satisfying $R(U, p-\\varepsilon v)=0$. The convergence tolerance for these solves must be consistent with that used for the baseline state $U(p)$ to avoid introducing significant \"solver noise\" into the comparison.\n\nA robust RDD test should incorporate:\n1.  A method for sampling directions $v$ that provides a comprehensive test of the gradient vector. Random directions are superior to axis-aligned directions as they can uncover errors in off-diagonal sensitivities that axis-aligned tests might miss.\n2.  A high-accuracy finite-difference scheme, like the central difference.\n3.  Correct computation of perturbed states by re-solving the governing equations.\n4.  A robust error metric that is scale-invariant and well-behaved, preventing false positives or negatives.\n5.  A stringent statistical acceptance criterion.\n\nWith these principles, I will now evaluate each option.\n\n### Analysis of Options\n\n**Option A**\n\nThis option proposes the following procedure:\n1.  **Directions:** Draw $N$ random directions $v_i$ by sampling from a standard normal distribution and normalizing to unit length, $\\|v_i\\|_2 = 1$. This is a standard and sound method for generating directions that are uniformly distributed on the surface of the unit sphere in $\\mathbb{R}^n$, ensuring an unbiased and thorough test.\n2.  **Finite Difference:** Use the central difference formula $d_{\\mathrm{fd},i} = \\frac{J(U(p+\\varepsilon v_{i}), p+\\varepsilon v_{i}) - J(U(p-\\varepsilon v_{i}), p-\\varepsilon v_{i})}{2\\varepsilon}$. This is the preferred, second-order accurate method, which minimizes truncation error for a given step size $\\varepsilon$.\n3.  **State Solves:** Explicitly requires that each perturbed state $U(p\\pm \\varepsilon v_{i})$ be converged to the same residual tolerance as $U(p)$. This is a critical and correctly identified requirement for an accurate finite-difference approximation.\n4.  **Error Metric:** Employs the symmetric relative error $\\delta_{i} = \\frac{|d_{\\mathrm{adj},i} - d_{\\mathrm{fd},i}|}{|d_{\\mathrm{adj},i}| + |d_{\\mathrm{fd},i}|}$. This metric is robust; it avoids division by zero, is scale-invariant, is bounded in $[0, 1]$, and treats $d_{\\mathrm{adj},i}$ and $d_{\\mathrm{fd},i}$ symmetrically.\n5.  **Acceptance Criterion:** A pass is declared if the maximum error over all samples is below a tolerance $\\tau$, i.e., $\\max_{i} \\delta_i \\le \\tau$. This is a stringent test, as a single large deviation will cause failure. Supplementing this with checks on the mean or median is good practice for statistical confidence.\n\nEvery aspect of this option aligns perfectly with best practices for numerical gradient verification.\n\n**Verdict:** **Correct**.\n\n**Option B**\n\nThis option proposes:\n1.  **Directions:** Sample entries uniformly from $[-1,1]$ without normalization. This is a valid, though less ideal, sampling method. The non-constant norm of $v_i$ means the perturbation magnitude varies with direction.\n2.  **Finite Difference:** Uses a forward difference, $d_{\\mathrm{fd},i} = \\frac{J(U(p), p) - J(U(p+\\varepsilon v_{i}), p+\\varepsilon v_{i})}{\\varepsilon}$. Aside from a likely sign error (it should be $J(p+\\dots) - J(p)$), this is a first-order accurate scheme ($\\mathcal{O}(\\varepsilon)$), making it less accurate than central differences.\n3.  **State Solves:** States to \"skip re-solves for the perturbed states to save cost.\" This is a **fatal scientific flaw**. To evaluate $J(U(p+\\varepsilon v_{i}), p+\\varepsilon v_{i})$, one *must* compute the state $U(p+\\varepsilon v_{i})$ that satisfies $R(U, p+\\varepsilon v_{i})=0$. Skipping this re-solve means the value being computed is not the true perturbed functional value, rendering the entire finite-difference approximation invalid.\n4.  **Error Metric:** Uses the average absolute error. This is not scale-invariant and can be misleading. Averages can also hide large individual errors.\n\nThe proposal to skip flow re-solves is fundamentally incorrect and violates the mathematical definition of the derivative being approximated.\n\n**Verdict:** **Incorrect**.\n\n**Option C**\n\nThis option proposes:\n1.  **Directions:** Uses Rademacher directions, where entries are either $-1$ or $+1$. This is a specific type of sampling that tests the corners of a hypercube. It is a valid, albeit specialized, sampling strategy.\n2.  **Finite Difference:** The formula given is $d_{\\mathrm{fd},i} = \\frac{J(...) - J(...)}{2\\varepsilon \\|v_{i}\\|_{2}^{2}}$. This is **scientifically incorrect**. The denominator of the central-difference approximation for the directional derivative $g^T v_i$ is $2\\varepsilon$. The inclusion of the term $\\|v_{i}\\|_{2}^{2}$ is without justification and leads to a wrong value. For Rademacher directions, $\\|v_i\\|_2^2 = n$, so this would incorrectly divide the result by the number of parameters $n$.\n3.  **Acceptance Criterion:** Uses the Pearson correlation coefficient between $\\{d_{\\mathrm{adj},i}\\}$ and $\\{d_{\\mathrm{fd},i}\\}$. This is a **grossly inadequate criterion** for gradient verification. Correlation only measures the strength of a linear relationship, not agreement. For instance, if $d_{\\mathrm{adj},i} = (2 \\cdot d_{\\mathrm{fd},i}) + 5$ for all $i$, the correlation would be $1$, but the gradient would be wrong by a factor of $2$ and an offset. A gradient check must confirm that $d_{\\mathrm{adj},i} \\approx d_{\\mathrm{fd},i}$.\n\nBoth the finite-difference formula and the acceptance criterion are fundamentally flawed.\n\n**Verdict:** **Incorrect**.\n\n**Option D**\n\nThis option proposes:\n1.  **Directions:** Uses only axis-aligned directions, $v_i = e_i$. This is not a randomized test and fails to meet a key specification of the problem. While testing along coordinate axes is a useful first step, it is not as robust as a randomized test because it may fail to detect errors in the computation of cross-sensitivities (i.e., how an output is affected by the coupling of multiple parameter changes).\n2.  **Finite Difference:** Uses the first-order accurate forward difference scheme, which is less accurate than the central difference.\n3.  **Error Metric:** Uses the maximum absolute error, $\\max_i |d_{\\mathrm{adj},i} - d_{\\mathrm{fd},i}|$. As previously noted, an absolute error metric is not scale-invariant and is less robust than a relative one. For instance, a component of the gradient that is very large in magnitude could have a large absolute error that is still small in relative terms, and vice versa.\n\nThis option describes a simplified, less robust, non-randomized check that is inferior to the procedure in Option A.\n\n**Verdict:** **Incorrect**.\n\n### Conclusion\n\nOption A describes a complete, rigorous, and scientifically sound procedure for a Randomized Directional Derivative test. It correctly specifies an unbiased direction sampling strategy, a high-order finite-difference scheme, the critical need for re-solving the state equations, a robust error metric, and a stringent acceptance criterion. The other options contain fundamental scientific errors or propose methods that are significantly less robust and accurate.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A common frustration in practice occurs when a meticulously implemented adjoint solver fails a verification test, suggesting a bug where none exists. This advanced practice addresses a more profound cause: non-differentiabilities inherent in the physical models themselves, such as switches in turbulence closures or wall functions. By working with an abstract model that captures this pathology, you will learn to diagnose these issues and appreciate why model smoothing is a prerequisite for robust adjoint sensitivity analysis in complex CFD applications.",
            "id": "3942339",
            "problem": "In aerospace Computational Fluid Dynamics (CFD), adjoint-based sensitivity analysis is used to compute gradients of an objective functional with respect to design parameters for flows governed by the Reynolds-Averaged Navier–Stokes (RANS) equations with turbulence closures and wall models. Consider the abstract steady-state partial differential equation (PDE)-constrained optimization problem\n$$\n\\text{Find } u(\\alpha) \\text{ such that } R(u(\\alpha),\\alpha)=0, \\quad J(\\alpha)=\\mathcal{J}(u(\\alpha),\\alpha),\n$$\nwhere $u$ denotes the state (e.g., mean velocity and pressure fields), $\\alpha$ denotes a design parameter (e.g., shape, trip location, or inflow condition), $R$ denotes the discrete residual representing the spatial discretization of the governing equations and closures, and $\\mathcal{J}$ denotes a differentiable output functional such as drag. The adjoint method relies on the existence of the Gâteaux derivative of the mapping $\\alpha \\mapsto u(\\alpha)$ and of $\\mathcal{J}$, together with differentiation under the residual $R$.\n\nTo reason from first principles, consider the following scalar diffusion model that abstracts how turbulence model switches, transition triggers, or wall-function regime changes enter into $R$ through piecewise definitions:\n$$\n-\\nabla \\cdot \\left(\\mu(u,\\alpha)\\nabla u\\right)=f \\quad \\text{in } \\Omega,\\qquad u=0 \\quad \\text{on } \\partial \\Omega,\n$$\nwith output\n$$\n\\mathcal{J}(u,\\alpha)=\\int_{\\Omega} q \\, u \\, \\mathrm{d}\\Omega,\n$$\nwhere $f$ and $q$ are given smooth functions and the effective viscosity $\\mu$ is modeled by a regime switch\n$$\n\\mu(u,\\alpha)=\\mu_{\\ell}+\\left(\\mu_{t}-\\mu_{\\ell}\\right)\\,\\mathrm{H}\\big(T(u,\\alpha)\\big),\n$$\nwith $\\mu_{\\ell}>0$, $\\mu_{t}>\\mu_{\\ell}$ constants, $\\mathrm{H}$ the Heaviside step function, and $T(u,\\alpha)$ a smooth scalar trigger (e.g., based on a local Reynolds number, wall-distance, or turbulence sensor) such that $T(u,\\alpha)\\ge 0$ selects the turbulent/wall-log regime and $T(u,\\alpha)<0$ selects the laminar/viscous-sub-layer regime. Assume the forward problem is well-posed in an appropriate function space for each fixed $\\alpha$.\n\nBy definition, the directional Gâteaux derivative of $\\mathcal{J}$ in direction $\\dot{\\alpha}$ at $\\alpha=\\alpha_{0}$ is the limit\n$$\n\\mathrm{D}\\mathcal{J}(\\alpha_{0})[\\dot{\\alpha}]=\\lim_{\\varepsilon\\to 0}\\frac{\\mathcal{J}\\big(u(\\alpha_{0}+\\varepsilon \\dot{\\alpha}),\\alpha_{0}+\\varepsilon \\dot{\\alpha}\\big)-\\mathcal{J}\\big(u(\\alpha_{0}),\\alpha_{0}\\big)}{\\varepsilon},\n$$\nprovided the limit exists. The adjoint method seeks to evaluate $\\mathrm{D}\\mathcal{J}(\\alpha_{0})[\\dot{\\alpha}]$ without explicitly computing the state variation $\\dot{u}=\\mathrm{D}u(\\alpha_{0})[\\dot{\\alpha}]$, by differentiating $R(u,\\alpha)=0$ and solving the associated adjoint equation. This procedure relies on the differentiability of $R$ with respect to $(u,\\alpha)$.\n\nNow address the following in the context of adjoint robustness for RANS closures, transition models, and wall functions:\n\n- Explain, from the definition of the Gâteaux derivative and linearization of the residual, the implications of the non-differentiable switch embedded in $\\mu(u,\\alpha)$ for the existence and regularity of the adjoint equation, particularly when the threshold set $\\{x\\in \\Omega: T(u(\\alpha_{0}),\\alpha_{0})=0\\}$ is nontrivial or moves under perturbations $\\dot{\\alpha}$.\n\n- Consider replacing $\\mathrm{H}$ by a smooth sigmoid $\\sigma_{\\varepsilon}$, where $\\sigma_{\\varepsilon}\\in C^{\\infty}(\\mathbb{R})$, $\\sigma_{\\varepsilon}(z)\\to \\mathrm{H}(z)$ pointwise as $\\varepsilon\\to 0^{+}$, and $\\sigma_{\\varepsilon}'$ is bounded and localized. Discuss the impact of this smoothing on the well-posedness of the continuous adjoint and on the consistency of the resulting gradients as $\\varepsilon\\to 0^{+}$ under suitable measure-theoretic conditions on the threshold set.\n\n- In the realm of wall models, consider a wall function that switches regimes based on nondimensional wall distance $y^{+}$, with a discontinuous switch between a viscous sublayer law and a logarithmic law. Discuss the effect of replacing the switch by a $C^{1}$ blending function of $y^{+}$ on adjoint robustness.\n\nSelect all statements that are correct based on this analysis.\n\nA. In the model problem, if $\\mu(u,\\alpha)$ uses a Heaviside switch, the linearized operator for the adjoint exists for any perturbation because the derivative of the Heaviside is zero almost everywhere, so no singular terms appear in the differentiation of $R$.\n\nB. Replacing $\\mathrm{H}$ by a smooth sigmoid $\\sigma_{\\varepsilon}$ with small $\\varepsilon$ yields a well-defined continuous adjoint whose gradient converges to the true shape derivative as $\\varepsilon\\to 0^{+}$, provided the threshold set $\\{x\\in \\Omega: T(u(\\alpha_{0}),\\alpha_{0})=0\\}$ has measure zero and the solution map is sufficiently continuous with respect to $(u,\\alpha)$.\n\nC. Mesh refinement alone removes the non-differentiability introduced by model switches, making the continuous adjoint well-posed without any smoothing.\n\nD. Using a blended wall function that transitions continuously between viscous sublayer and log-law with a $C^{1}$ blending function in wall distance produces a residual that is differentiable with respect to design parameters, which improves adjoint robustness.\n\nE. In transitional models, replacing a hard trip based on a critical momentum-thickness Reynolds number by an intermittency transport equation eliminates all non-differentiabilities and guarantees a twice continuously differentiable discrete residual with respect to any design variable.",
            "solution": "The problem statement presents a rigorous and scientifically sound abstraction of a critical issue in modern Computational Fluid Dynamics (CFD), specifically the application of adjoint-based sensitivity analysis and optimization to flows governed by the Reynolds-Averaged Navier–Stokes (RANS) equations. The use of a scalar diffusion model with a Heaviside function to represent non-differentiable switches in turbulence models, transition models, or wall functions is a standard and effective technique for analyzing the mathematical pathologies that arise. The problem is well-posed, objective, and directly relevant to the stated topic. Therefore, the problem is valid, and we may proceed to the analysis.\n\nThe core of the issue lies in the application of variational calculus to derive the adjoint equations, a procedure which relies on the Gâteaux differentiability of the residual of the governing equations, $R(u, \\alpha)$, with respect to both the state vector $u$ and the design parameters $\\alpha$.\n\nLet the weak form of the residual for the model problem be given by the functional $R(u, \\alpha)[v]$ for a test function $v$:\n$$\nR(u, \\alpha)[v] = \\int_{\\Omega} \\mu(u,\\alpha)\\nabla u \\cdot \\nabla v \\, \\mathrm{d}\\Omega - \\int_{\\Omega} f v \\, \\mathrm{d}\\Omega = 0\n$$\nwhere $u$ and $v$ belong to a suitable function space, e.g., $H_0^1(\\Omega)$. The adjoint method requires linearizing this residual to derive the adjoint equation. The Gâteaux derivative of $R$ with respect to $u$ in the direction $\\dot{u}$ is\n$$\n\\mathrm{D}_{u} R(u,\\alpha)[\\dot{u}, v] = \\lim_{\\epsilon\\to 0} \\frac{R(u+\\epsilon\\dot{u}, \\alpha)[v] - R(u,\\alpha)[v]}{\\epsilon}.\n$$\nWith the given form of $\\mu(u,\\alpha) = \\mu_{\\ell}+\\left(\\mu_{t}-\\mu_{\\ell}\\right)\\,\\mathrm{H}\\big(T(u,\\alpha)\\big)$, the derivative of $\\mu$ with respect to $u$ must be considered. Formally, using the chain rule, the derivative of $\\mathrm{H}(T(u,\\alpha))$ involves $\\mathrm{H}'(T(u,\\alpha)) = \\delta(T(u,\\alpha))$, where $\\delta$ is the Dirac delta distribution. This leads to:\n$$\n\\mathrm{D}_{u} R(u,\\alpha)[\\dot{u}, v] = \\int_{\\Omega} \\mu(u,\\alpha)\\nabla \\dot{u} \\cdot \\nabla v \\, \\mathrm{d}\\Omega + \\int_{\\Omega} (\\mu_{t}-\\mu_{\\ell})\\delta(T(u,\\alpha)) (\\mathrm{D}_{u}T(u,\\alpha)[\\dot{u}]) (\\nabla u \\cdot \\nabla v) \\, \\mathrm{d}\\Omega\n$$\nThe second term contains a Dirac delta distribution, which is singular. It localizes the integral to the \"switching surface\" or \"threshold set\" $\\mathcal{S} = \\{x\\in \\Omega: T(u(\\alpha),\\alpha)=0\\}$. The presence of this distributional term means the linearized operator $\\mathrm{D}_{u} R$ is not a standard bounded linear operator on typical function spaces. This may render the continuous adjoint equation ill-posed or lead to an adjoint solution $\\psi$ that is itself a distribution, rather than a regular function. The entire adjoint framework, which relies on well-behaved linear operators, is thus compromised. This phenomenon is often termed \"adjoint failure\" or a lack of \"adjoint robustness.\"\n\nSimilar issues arise when differentiating with respect to $\\alpha$. A perturbation $\\dot{\\alpha}$ may cause the switching surface $\\mathcal{S}$ to move, leading to a sensitivity that is concentrated on this moving interface. The limit defining the Gâteaux derivative may not exist, or it may depend on the sign of the perturbation, yielding inconsistent gradients.\n\nNow we evaluate each option.\n\n**A. In the model problem, if $\\mu(u,\\alpha)$ uses a Heaviside switch, the linearized operator for the adjoint exists for any perturbation because the derivative of the Heaviside is zero almost everywhere, so no singular terms appear in the differentiation of $R$.**\n\nThis statement is fundamentally flawed. While it is true that the classical derivative of the Heaviside function is zero for all non-zero arguments (i.e., almost everywhere), this perspective is incorrect in the context of functional analysis and distributions. The proper derivative in the sense of distributions is the Dirac delta function, $\\mathrm{H}'(z) = \\delta(z)$. As shown in the analysis above, the linearization of the residual $R$ explicitly produces a term containing $\\delta(T(u,\\alpha))$. This is a singular term that fundamentally compromises the well-posedness of the standard adjoint formulation. The claim that \"no singular terms appear\" is false.\n\n**Verdict: Incorrect.**\n\n**B. Replacing $\\mathrm{H}$ by a smooth sigmoid $\\sigma_{\\varepsilon}$ with small $\\varepsilon$ yields a well-defined continuous adjoint whose gradient converges to the true shape derivative as $\\varepsilon\\to 0^{+}$, provided the threshold set $\\{x\\in \\Omega: T(u(\\alpha_{0}),\\alpha_{0})=0\\}$ has measure zero and the solution map is sufficiently continuous with respect to $(u,\\alpha)$.**\n\nThis describes the standard and mathematically rigorous approach to address the non-differentiability. Replacing the Heaviside function $\\mathrm{H}$ with a smooth approximation $\\sigma_{\\varepsilon}$ (a mollifier) makes the viscosity $\\mu$ and consequently the residual $R$ differentiable with respect to $u$ and $\\alpha$. The problematic Dirac delta $\\delta$ is replaced by the smooth, bounded function $\\sigma'_{\\varepsilon}$. This ensures that the linearized operator $\\mathrm{D}_u R$ is well-behaved, leading to a well-posed continuous adjoint equation. The core of the statement is about the convergence of the gradient obtained from this regularized problem to the \"true\" derivative of the original non-smooth problem. For problems where interfaces or internal boundaries move, the correct mathematical concept for the derivative is the shape derivative. It is a known result in shape optimization theory that, under certain conditions, the gradients from the regularized problem converge to the correct shape derivative as the smoothing parameter $\\varepsilon \\to 0$. The conditions mentioned—that the threshold set where the switch occurs must have measure zero (i.e., it is a surface, not a region with volume) and that the state $u$ depends continuously on the parameter $\\alpha$—are precisely the types of assumptions required for such convergence proofs.\n\n**Verdict: Correct.**\n\n**C. Mesh refinement alone removes the non-differentiability introduced by model switches, making the continuous adjoint well-posed without any smoothing.**\n\nThis is incorrect. The non-differentiability is a property of the continuous governing equations, not an artifact of discretization. Mesh refinement provides a better approximation of the solution to the continuous problem but does not alter the fundamental nature of that problem. If the continuous problem is non-differentiable, the discrete problem will inherit this characteristic. In practice, as the mesh is refined, the location of the non-differentiable switch is resolved more sharply, often leading to increasingly oscillatory and non-convergent behavior of the discrete gradients. This is a classic symptom that demonstrates the need to modify the underlying continuous model (e.g., by smoothing) rather than simply refining the mesh.\n\n**Verdict: Incorrect.**\n\n**D. Using a blended wall function that transitions continuously between viscous sublayer and log-law with a $C^{1}$ blending function in wall distance produces a residual that is differentiable with respect to design parameters, which improves adjoint robustness.**\n\nThis statement presents a concrete, practical application of the principle discussed in option B. Standard wall functions for RANS models often contain an `if-then-else` structure to switch between the viscous sublayer and the logarithmic layer based on the non-dimensional wall distance $y^+$. Since $y^+$ depends on the wall shear stress, which is part of the solution $u$, this `if` statement introduces a Heaviside-like non-differentiability into the residual $R$. Replacing this sharp switch with a blending function that is continuously differentiable ($C^1$) with respect to its arguments (like $y^+$) makes the wall boundary condition smooth. This, in turn, makes the overall residual $R(u, \\alpha)$ differentiable with respect to the state $u$ and design parameters $\\alpha$. As established, differentiability of the residual is the prerequisite for a well-posed adjoint system. Therefore, this modification directly \"improves adjoint robustness\" by eliminating the source of the mathematical pathology.\n\n**Verdict: Correct.**\n\n**E. In transitional models, replacing a hard trip based on a critical momentum-thickness Reynolds number by an intermittency transport equation eliminates all non-differentiabilities and guarantees a twice continuously differentiable discrete residual with respect to any design variable.**\n\nThis claim is too strong and therefore incorrect. Replacing a \"hard trip\" (a Heaviside switch based on $Re_{\\theta} > Re_{\\theta,c}$) with a transport equation for intermittency (e.g., the $\\gamma-Re_{\\theta}$ model) is indeed a significant step towards creating a more differentiable model. It removes the most severe non-differentiability. However, it does not \"eliminate all non-differentiabilities\". Modern transport-based transition models contain highly complex and non-linear source terms based on empirical correlations. These terms can involve functions like `max`, `min`, `abs`, or clipping/limiting functions that are not differentiable everywhere. Furthermore, the numerical discretization of the transport equations often employs flux limiters or other schemes that are, at best, piecewise smooth and certainly not $C^2$. Therefore, guaranteeing a \"twice continuously differentiable ($C^2$) discrete residual\" is an overstatement. While the model is substantially smoother, it is not guaranteed to be free of all non-differentiabilities, and $C^2$ continuity is highly unlikely.\n\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{BD}$$"
        }
    ]
}