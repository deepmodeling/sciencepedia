{
    "hands_on_practices": [
        {
            "introduction": "To effectively apply and analyze the Successive Over-Relaxation (SOR) method, one must first master its fundamental formulation. This exercise provides practice in the essential skill of deriving the SOR iteration matrix, $T_{\\omega}$, directly from the matrix splitting of a given system. By calculating the spectral radius of $T_{\\omega}$, you will be determining the theoretical convergence rate of the method for a specific relaxation parameter $\\omega$, a crucial first step in understanding the behavior of any iterative solver .",
            "id": "3581656",
            "problem": "Consider the linear system $A x = b$ with \n$$\nA=\\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix}, \n\\qquad \nb=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix},\n\\qquad \n\\omega=\\frac{3}{2}.\n$$\nStarting from the decomposition of $A$ into its diagonal part, strictly lower triangular part, and strictly upper triangular part, and from the definition of the Successive Over-Relaxation (SOR) method as a stationary iteration obtained by applying a relaxation parameter to the Gauss–Seidel update, derive the iteration in the affine form $x^{k+1}=T_{\\omega}\\,x^{k}+c_{\\omega}$, identify the SOR iteration matrix $T_{\\omega}$ for the given $A$ and $\\omega$, and determine its spectral radius (the maximum absolute value of its eigenvalues). Your final reported answer must be the spectral radius as a single exact number. Do not round.",
            "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- The linear system is $A x = b$.\n- The matrix $A$ is given as $A=\\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix}$.\n- The vector $b$ is given as $b=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$.\n- The relaxation parameter $\\omega$ is given as $\\omega=\\frac{3}{2}$.\n- The task is to derive the Successive Over-Relaxation (SOR) iteration matrix $T_{\\omega}$ and compute its spectral radius, $\\rho(T_{\\omega})$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard exercise in numerical linear algebra concerning stationary iterative methods, specifically the SOR method. It is based on well-established mathematical principles.\n- **Well-Posed**: The problem is well-posed. The matrix $A$ and the parameter $\\omega$ are explicitly defined. The task of finding the SOR iteration matrix and its spectral radius is a uniquely defined mathematical procedure. The matrix $A$ is symmetric and positive definite (its eigenvalues are $1$ and $3$), which guarantees that the SOR method converges for any $\\omega \\in (0, 2)$. The given value $\\omega = \\frac{3}{2}$ lies within this interval.\n- **Objective**: The problem is stated using precise, objective mathematical language.\n- **Completeness**: All necessary information to determine the iteration matrix $T_{\\omega}$ and its spectral radius is provided. The vector $b$ is not required for this specific task, but its presence does not create a contradiction.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and complete. A full solution will be provided.\n\nThe Successive Over-Relaxation (SOR) method is a stationary iterative method for solving a linear system $A x = b$. The matrix $A$ is first decomposed into its diagonal component $D$, its strictly lower triangular component $L$, and its strictly upper triangular component $U$, such that $A = D + L + U$.\n\nFor the given matrix $A = \\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix}$, the decomposition is:\n$$D = \\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix}, \\quad L = \\begin{pmatrix}0  0 \\\\ -1  0\\end{pmatrix}, \\quad U = \\begin{pmatrix}0  -1 \\\\ 0  0\\end{pmatrix}$$\n\nThe SOR iteration is defined by the equation:\n$$(D + \\omega L) x^{k+1} = \\big((1-\\omega)D - \\omega U\\big) x^{k} + \\omega b$$\nThis can be written in the affine form $x^{k+1} = T_{\\omega} x^{k} + c_{\\omega}$, where the iteration matrix $T_{\\omega}$ is given by:\n$$T_{\\omega} = (D + \\omega L)^{-1} \\big((1-\\omega)D - \\omega U\\big)$$\nand the constant vector $c_{\\omega}$ is given by $c_{\\omega} = \\omega (D + \\omega L)^{-1} b$. Our goal is to find the spectral radius of $T_{\\omega}$.\n\nWe are given $\\omega = \\frac{3}{2}$. First, we compute the matrix $(D + \\omega L)$:\n$$D + \\omega L = \\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix} + \\frac{3}{2}\\begin{pmatrix}0  0 \\\\ -1  0\\end{pmatrix} = \\begin{pmatrix}2  0 \\\\ -\\frac{3}{2}  2\\end{pmatrix}$$\n\nNext, we find the inverse of this matrix, $(D + \\omega L)^{-1}$:\n$$(D + \\omega L)^{-1} = \\frac{1}{(2)(2) - (0)(-\\frac{3}{2})} \\begin{pmatrix}2  0 \\\\ \\frac{3}{2}  2\\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix}2  0 \\\\ \\frac{3}{2}  2\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2}  0 \\\\ \\frac{3}{8}  \\frac{1}{2}\\end{pmatrix}$$\n\nNow, we compute the matrix $\\big((1-\\omega)D - \\omega U\\big)$. Since $\\omega = \\frac{3}{2}$, the term $(1-\\omega)$ is $1 - \\frac{3}{2} = -\\frac{1}{2}$.\n$$(1-\\omega)D - \\omega U = -\\frac{1}{2}\\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix} - \\frac{3}{2}\\begin{pmatrix}0  -1 \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}-1  0 \\\\ 0  -1\\end{pmatrix} + \\begin{pmatrix}0  \\frac{3}{2} \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}-1  \\frac{3}{2} \\\\ 0  -1\\end{pmatrix}$$\n\nFinally, we compute the SOR iteration matrix $T_{\\omega}$ by multiplying the two resulting matrices:\n$$T_{\\omega} = (D + \\omega L)^{-1} \\big((1-\\omega)D - \\omega U\\big) = \\begin{pmatrix}\\frac{1}{2}  0 \\\\ \\frac{3}{8}  \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}-1  \\frac{3}{2} \\\\ 0  -1\\end{pmatrix}$$\n$$T_{\\omega} = \\begin{pmatrix} (\\frac{1}{2})(-1) + (0)(0)  (\\frac{1}{2})(\\frac{3}{2}) + (0)(-1) \\\\ (\\frac{3}{8})(-1) + (\\frac{1}{2})(0)  (\\frac{3}{8})(\\frac{3}{2}) + (\\frac{1}{2})(-1) \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2}  \\frac{3}{4} \\\\ -\\frac{3}{8}  \\frac{9}{16} - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2}  \\frac{3}{4} \\\\ -\\frac{3}{8}  \\frac{1}{16} \\end{pmatrix}$$\n\nTo find the spectral radius $\\rho(T_{\\omega})$, we need to find the eigenvalues of $T_{\\omega}$ by solving the characteristic equation $\\det(T_{\\omega} - \\lambda I) = 0$.\n$$\\det\\begin{pmatrix} -\\frac{1}{2} - \\lambda  \\frac{3}{4} \\\\ -\\frac{3}{8}  \\frac{1}{16} - \\lambda \\end{pmatrix} = 0$$\n$$\\left(-\\frac{1}{2} - \\lambda\\right)\\left(\\frac{1}{16} - \\lambda\\right) - \\left(\\frac{3}{4}\\right)\\left(-\\frac{3}{8}\\right) = 0$$\n$$-\\frac{1}{32} + \\frac{1}{2}\\lambda - \\frac{1}{16}\\lambda + \\lambda^2 + \\frac{9}{32} = 0$$\n$$\\lambda^2 + \\left(\\frac{8}{16} - \\frac{1}{16}\\right)\\lambda + \\left(\\frac{9}{32} - \\frac{1}{32}\\right) = 0$$\n$$\\lambda^2 + \\frac{7}{16}\\lambda + \\frac{8}{32} = 0$$\n$$\\lambda^2 + \\frac{7}{16}\\lambda + \\frac{1}{4} = 0$$\n\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\\lambda = \\frac{-\\frac{7}{16} \\pm \\sqrt{\\left(\\frac{7}{16}\\right)^2 - 4(1)\\left(\\frac{1}{4}\\right)}}{2(1)} = \\frac{-\\frac{7}{16} \\pm \\sqrt{\\frac{49}{256} - 1}}{2}$$\n$$\\lambda = \\frac{-\\frac{7}{16} \\pm \\sqrt{\\frac{49 - 256}{256}}}{2} = \\frac{-\\frac{7}{16} \\pm i\\frac{\\sqrt{207}}{16}}{2}$$\nThe two eigenvalues are a complex conjugate pair:\n$$\\lambda_{1,2} = -\\frac{7}{32} \\pm i\\frac{\\sqrt{207}}{32}$$\n\nThe spectral radius $\\rho(T_{\\omega})$ is the maximum absolute value (modulus) of the eigenvalues. Since the eigenvalues are a complex conjugate pair, their moduli are equal. We calculate the modulus squared of one of the eigenvalues:\n$$|\\lambda|^2 = \\left(-\\frac{7}{32}\\right)^2 + \\left(\\frac{\\sqrt{207}}{32}\\right)^2 = \\frac{49}{1024} + \\frac{207}{1024} = \\frac{49 + 207}{1024} = \\frac{256}{1024} = \\frac{1}{4}$$\nThe modulus is the square root of this value:\n$$|\\lambda| = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}$$\nBoth eigenvalues have a modulus of $\\frac{1}{2}$. Therefore, the spectral radius of $T_{\\omega}$ is:\n$$\\rho(T_{\\omega}) = \\max(|\\lambda_1|, |\\lambda_2|) = \\frac{1}{2}$$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "The convergence rate of the SOR method is highly sensitive to the choice of the relaxation parameter, $\\omega$. While theoretical formulas for the optimal parameter, $\\omega_{\\text{opt}}$, exist for specific matrix classes, a powerful and general approach is to determine it through numerical experimentation. This hands-on coding practice guides you through the process of mapping the spectral radius $\\rho(T_{\\text{SOR}}(\\omega))$ across a range of $\\omega$ values, allowing you to empirically discover the value that minimizes the spectral radius and thus maximizes the convergence speed for the discrete Poisson equation .",
            "id": "3198998",
            "problem": "You are asked to implement a complete, runnable program that performs a numerical experiment to study the Successive Over-Relaxation (SOR) method for the one-dimensional Poisson linear system with homogeneous Dirichlet boundary conditions. The experiment will map the spectral radius of the SOR iteration matrix as a function of the relaxation parameter and infer an approximately optimal relaxation parameter from the minimizer of this spectral radius, using only fundamental definitions.\n\nThe one-dimensional Poisson boundary value problem on the unit interval with homogeneous Dirichlet boundary conditions is discretized on $N$ interior points with grid spacing $h = \\frac{1}{N+1}$. This produces a linear system $A \\mathbf{u} = \\mathbf{f}$ where $A \\in \\mathbb{R}^{N \\times N}$ is the tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals, corresponding to the standard second-order centered finite-difference approximation of the second derivative subject to $u(0)=u(1)=0$. Note that for constructing the SOR iteration matrix, the right-hand side $\\mathbf{f}$ is not needed.\n\nDefine the classical matrix splitting $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower-triangular part of $A$, and $U$ is the strictly upper-triangular part of $A$. For a relaxation parameter $\\omega$ with $0  \\omega  2$, the SOR fixed-point iteration $x^{(k+1)} = T_{\\mathrm{SOR}}(\\omega) x^{(k)} + c(\\omega)$ is defined via\n$$\nT_{\\mathrm{SOR}}(\\omega) = \\left(D + \\omega L\\right)^{-1} \\left((1-\\omega) D - \\omega U\\right).\n$$\nThe spectral radius $\\rho\\!\\left(T_{\\mathrm{SOR}}(\\omega)\\right)$ is the largest magnitude of the eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$.\n\nYour task is to:\n- For each specified test case, construct $A$, build $T_{\\mathrm{SOR}}(\\omega)$ for a dense set of $\\omega$ values in $(0,2)$, numerically compute $\\rho\\!\\left(T_{\\mathrm{SOR}}(\\omega)\\right)$ for each $\\omega$ by computing the eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$, and determine the value of $\\omega$ on the grid that minimizes $\\rho\\!\\left(T_{\\mathrm{SOR}}(\\omega)\\right)$. Call this minimizer $\\omega_{\\mathrm{opt}}$.\n- Ensure that matrix inverses are not formed explicitly; use linear solves to apply $\\left(D + \\omega L\\right)^{-1}$ to a matrix on the right.\n\nUse the following test suite of parameter values:\n- Test case $1$: $N = 1$, $\\omega_{\\min} = 0.05$, $\\omega_{\\max} = 1.95$, $\\Delta \\omega = 0.005$.\n- Test case $2$: $N = 10$, $\\omega_{\\min} = 0.05$, $\\omega_{\\max} = 1.95$, $\\Delta \\omega = 0.01$.\n- Test case $3$: $N = 50$, $\\omega_{\\min} = 0.05$, $\\omega_{\\max} = 1.95$, $\\Delta \\omega = 0.02$.\n\nConstraints and requirements:\n- All computations are purely numerical and must be done with $\\omega$ sampled uniformly from the specified interval, inclusive of endpoints that lie strictly within $(0,2)$.\n- For each test case, report a single floating-point number equal to the minimizing $\\omega_{\\mathrm{opt}}$ found on the grid, rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases. For example: $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It presents a standard numerical experiment in computational science that is free of contradictions or ambiguities.\n\nThe objective is to numerically determine the optimal relaxation parameter, $\\omega_{\\mathrm{opt}}$, for the Successive Over-Relaxation (SOR) method applied to the linear system arising from the finite-difference discretization of the one-dimensional Poisson equation, $u''(x) = g(x)$, on the unit interval $x \\in [0, 1]$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$.\n\nA second-order centered finite-difference scheme on a uniform grid with $N$ interior points and spacing $h = \\frac{1}{N+1}$ leads to the linear system $A \\mathbf{u} = \\mathbf{f}$. The matrix $A \\in \\mathbb{R}^{N \\times N}$ is a symmetric, positive-definite, tridiagonal matrix given by:\n$$\nA = \n\\begin{pmatrix}\n 2  -1                     \\\\\n-1   2  -1                 \\\\\n    \\ddots  \\ddots  \\ddots     \\\\\n            -1      2       -1 \\\\\n                    -1      2\n\\end{pmatrix}\n$$\nThe convergence rate of the SOR method is determined by the spectral radius of its iteration matrix. To construct this matrix, we perform a matrix splitting of $A$ into its diagonal ($D$), strictly lower-triangular ($L$), and strictly upper-triangular ($U$) parts. The standard decomposition is $A = D + L + U$. Given the structure of $A$, these matrices are:\n- $D$ is a diagonal matrix with all entries equal to $2$. $D = \\mathrm{diag}(2, 2, \\dots, 2)$.\n- Since $A_{i, i-1} = -1$, the matrix $L$ contains these entries. Thus, $L$ is a strictly lower-bidiagonal matrix with entries of $-1$ on its first subdiagonal.\n- Similarly, since $A_{i, i+1} = -1$, the matrix $U$ contains these entries. Thus, $U$ is a strictly upper-bidiagonal matrix with entries of $-1$ on its first superdiagonal.\n\nThe SOR iteration is defined by the fixed-point relation $\\mathbf{x}^{(k+1)} = T_{\\mathrm{SOR}}(\\omega) \\mathbf{x}^{(k)} + \\mathbf{c}$, where $\\omega \\in (0, 2)$ is the relaxation parameter and $T_{\\mathrm{SOR}}(\\omega)$ is the iteration matrix. The formula for the iteration matrix is given as:\n$$\nT_{\\mathrm{SOR}}(\\omega) = (D + \\omega L)^{-1}((1-\\omega)D - \\omega U)\n$$\nThe asymptotic rate of convergence of the SOR iteration is governed by the spectral radius of $T_{\\mathrm{SOR}}(\\omega)$, denoted $\\rho(T_{\\mathrm{SOR}}(\\omega))$, which is the maximum absolute value of the eigenvalues of $T_{\\mathrm{SOR}}(\\omega)$. The optimal relaxation parameter, $\\omega_{\\mathrm{opt}}$, is the value of $\\omega$ that minimizes this spectral radius:\n$$\n\\omega_{\\mathrm{opt}} = \\arg\\min_{\\omega \\in (0,2)} \\rho(T_{\\mathrm{SOR}}(\\omega))\n$$\nThis problem requires a numerical search for an approximation of $\\omega_{\\mathrm{opt}}$ by sampling $\\omega$ on a specified discrete grid within the interval $(0, 2)$ for different matrix sizes $N$.\n\nThe numerical algorithm for each test case $(N, \\omega_{\\min}, \\omega_{\\max}, \\Delta \\omega)$ is as follows:\n1.  Construct the matrices $D$, $L$, and $U$ for the given dimension $N$.\n2.  Generate a uniform grid of $\\omega$ values from $\\omega_{\\min}$ to $\\omega_{\\max}$ with an increment of $\\Delta \\omega$.\n3.  Initialize a variable $\\rho_{\\min}$ to a very large number (e.g., infinity) and $\\omega_{\\mathrm{found}}$ to an invalid value.\n4.  Iterate through each value of $\\omega$ in the generated grid:\n    a.  Form the matrices $M(\\omega) = D + \\omega L$ and $N(\\omega) = (1-\\omega)D - \\omega U$.\n    b.  Calculate the SOR iteration matrix $T_{\\mathrm{SOR}}(\\omega) = M(\\omega)^{-1}N(\\omega)$. Critically, the inverse $M(\\omega)^{-1}$ is not computed explicitly. Instead, we solve the matrix equation $M(\\omega) T = N(\\omega)$ for $T$. Since $M(\\omega)$ is a lower-triangular matrix (as $D$ is diagonal and $L$ is strictly lower-triangular), this system can be solved efficiently using forward substitution for each column of $N(\\omega)$.\n    c.  Compute all eigenvalues $\\lambda_i$ of the resulting matrix $T_{\\mathrm{SOR}}(\\omega)$.\n    d.  Calculate the spectral radius $\\rho(\\omega) = \\max_{i} |\\lambda_i|$.\n    e.  If $\\rho(\\omega)  \\rho_{\\min}$, update $\\rho_{\\min} = \\rho(\\omega)$ and set $\\omega_{\\mathrm{found}} = \\omega$.\n5.  After iterating through all $\\omega$ values, the final $\\omega_{\\mathrm{found}}$ is the approximation of $\\omega_{\\mathrm{opt}}$ for the given grid. This value is then reported.\n\nThis entire procedure is implemented for each of the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Performs a numerical experiment to find the optimal SOR relaxation parameter omega\n    by minimizing the spectral radius of the SOR iteration matrix.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, omega_min, omega_max, delta_omega)\n        (1, 0.05, 1.95, 0.005),\n        (10, 0.05, 1.95, 0.01),\n        (50, 0.05, 1.95, 0.02),\n    ]\n\n    results = []\n    for N, w_min, w_max, delta_w in test_cases:\n        # Construct the matrices D, L, and U based on the problem description.\n        # A is tridiagonal with 2 on the diagonal and -1 on the off-diagonals.\n        # The splitting is A = D + L + U.\n        \n        # D is a diagonal matrix with entries 2.\n        D = np.diag(np.full(N, 2.0))\n        \n        if N > 1:\n            # L has -1 on the first sub-diagonal.\n            L = np.diag(np.full(N - 1, -1.0), k=-1)\n            # U has -1 on the first super-diagonal.\n            U = np.diag(np.full(N - 1, -1.0), k=1)\n        else: # N=1 case, L and U are zero matrices.\n            L = np.array([[0.0]])\n            U = np.array([[0.0]])\n\n        # Generate the grid of omega values.\n        # Using np.linspace is more robust for floating-point ranges.\n        num_points = int(round((w_max - w_min) / delta_w)) + 1\n        omegas = np.linspace(w_min, w_max, num_points)\n\n        min_rho = float('inf')\n        opt_omega = -1.0\n\n        for omega in omegas:\n            # Construct the matrices for the SOR operator T_sor = M^-1 * N\n            # T_sor = (D + omega*L)^-1 * ((1-omega)*D - omega*U)\n            M_omega = D + omega * L\n            N_omega = (1.0 - omega) * D - omega * U\n\n            # Compute T_sor by solving the system M * T = N.\n            # This avoids explicit inversion of M.\n            # Since M_omega is lower triangular, we use solve_triangular.\n            T_sor = linalg.solve_triangular(M_omega, N_omega, lower=True)\n\n            # Compute the eigenvalues of the iteration matrix.\n            eigenvalues = linalg.eigvals(T_sor)\n\n            # The spectral radius is the maximum magnitude of the eigenvalues.\n            rho = np.max(np.abs(eigenvalues))\n\n            # Check if this omega gives a smaller spectral radius.\n            if rho  min_rho:\n                min_rho = rho\n                opt_omega = omega\n        \n        # Append the found optimal omega, formatted to 6 decimal places.\n        results.append(f\"{opt_omega:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A core theorem in numerical analysis guarantees SOR convergence for Symmetric Positive-Definite (SPD) matrices with $\\omega \\in (0, 2)$. However, many important physical models, such as the Helmholtz equation, produce matrices that are indefinite. This advanced computational practice challenges you to investigate what happens when this condition is violated, exploring how the spectral radius $\\rho(T_{\\text{SOR}})$ can exceed 1, leading to divergence. This exercise underscores the critical importance of understanding a method's theoretical limitations before applying it to a problem .",
            "id": "3198971",
            "problem": "Consider the one-dimensional Helmholtz-type linear system on a uniform grid with Dirichlet boundary conditions. The continuous operator is the Helmholtz operator, and the discretization yields a matrix of the form $A = -\\Delta - k^2 I$, where $-\\Delta$ denotes the discrete negative Laplacian and $k^2$ is a nonnegative scalar shift. Using a second-order central difference approximation on the interval $[0,1]$ with $n$ interior points and uniform spacing $h = \\frac{1}{n+1}$, the matrix $A \\in \\mathbb{R}^{n \\times n}$ is tridiagonal with diagonal entries $a_{ii} = \\frac{2}{h^2} - k^2$ and off-diagonal entries $a_{i,i\\pm 1} = -\\frac{1}{h^2}$.\n\nYour tasks are:\n1. Starting from the standard splitting of a matrix for stationary iterations, write the Successive Over-Relaxation (SOR) iteration in matrix form for solving $A x = b$. Define the splitting $A = D + L + U$, where $D$ is the diagonal part of $A$, $L$ is the strictly lower-triangular part, and $U$ is the strictly upper-triangular part. From this, derive the SOR iteration as a linear iteration $x^{(m+1)} = T_{\\mathrm{SOR}} x^{(m)} + c$ and identify the SOR iteration matrix $T_{\\mathrm{SOR}}$ as a function of $D$, $L$, $U$, and the relaxation parameter $\\omega$.\n2. Explain, using the eigenstructure of $A$, why increasing $k$ can lead to indefiniteness of $A$ (i.e., $A$ has both positive and negative eigenvalues). Connect this indefiniteness to the behavior of the spectral radius $\\rho(T_{\\mathrm{SOR}})$ and the convergence or divergence of the SOR iteration.\n3. Implement a program that, for each given test case, constructs $A$ as described, forms $T_{\\mathrm{SOR}}$, computes $\\rho(T_{\\mathrm{SOR}})$, and returns it as a floating-point number rounded to six decimal places. Use the definition $\\rho(M) = \\max_i |\\lambda_i(M)|$ for the spectral radius of a matrix $M$.\n\nIn your program, the smallest eigenvalue of $-\\Delta$ under Dirichlet boundary conditions on the uniform grid should be computed using the well-tested discrete formula\n$$\n\\lambda_{\\min}(-\\Delta) = \\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right),\n$$\nand then set $k = \\sqrt{r \\, \\lambda_{\\min}(-\\Delta)}$ so that $k^2 = r \\, \\lambda_{\\min}(-\\Delta)$ for a prescribed ratio $r$. This choice provides controlled proximity to the indefiniteness threshold of $A$.\n\nDesign decisions:\n- Define the Successive Over-Relaxation (SOR) method (with acronym given on first use) precisely and construct the SOR iteration matrix directly from the splitting, without using shortcut formulas not justified from the splitting definition.\n- The program must compute the spectral radius numerically.\n- No physical units or angles are involved; all outputs are pure numbers.\n- Express all outputs as floats rounded to six decimal places.\n\nTest suite:\nUse the following test cases, each given as a triple $(n, \\omega, r)$:\n- Case $1$: $(n, \\omega, r) = (50, 1.2, 0.0)$.\n- Case $2$: $(n, \\omega, r) = (50, 1.2, 0.5)$.\n- Case $3$: $(n, \\omega, r) = (50, 1.2, 0.99)$.\n- Case $4$: $(n, \\omega, r) = (50, 1.2, 1.01)$.\n- Case $5$: $(n, \\omega, r) = (50, 1.2, 2.0)$.\n- Case $6$: $(n, \\omega, r) = (50, 1.8, 0.5)$.\n- Case $7$: $(n, \\omega, r) = (10, 1.2, 1.01)$.\n- Case $8$: $(n, \\omega, r) = (100, 1.2, 1.01)$.\n- Case $9$: $(n, \\omega, r) = (100, 1.0, 1.01)$.\n\nCoverage rationale:\n- Cases $1$–$3$ are the \"happy path\" where $A$ is symmetric positive definite and SOR should converge, with spectral radius less than $1$.\n- Case $4$ is just beyond the indefiniteness threshold and is expected to show $\\rho(T_{\\mathrm{SOR}})$ at or above $1$.\n- Case $5$ tests a stronger indefinite regime.\n- Case $6$ varies the relaxation parameter $\\omega$ near the upper end but still within the theoretically acceptable range for positive definite matrices.\n- Cases $7$–$9$ test size sensitivity and different $\\omega$ values under indefiniteness.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, with each entry equal to the computed spectral radius $\\rho(T_{\\mathrm{SOR}})$ rounded to six decimal places. For example, the format must be exactly like $[x_1,x_2,\\dots,x_9]$ with each $x_i$ a decimal number to six places.",
            "solution": "The problem is valid as it is scientifically grounded in numerical linear algebra, well-posed with all necessary parameters defined, and objectively stated. It presents a standard but non-trivial exercise in computational science.\n\n### 1. Derivation of the Successive Over-Relaxation (SOR) Iteration Matrix\n\nThe goal is to solve the linear system $Ax = b$ using an iterative method. The Successive Over-Relaxation (SOR) method is a stationary iterative method derived from the splitting of the matrix $A$.\n\nThe matrix $A$ is decomposed into its diagonal, strictly lower triangular, and strictly upper triangular parts:\n$$\nA = D + L + U\n$$\nwhere $D$ is a diagonal matrix containing the diagonal entries of $A$, $L$ is the strictly lower-triangular part of $A$, and $U$ is the strictly upper-triangular part of $A$.\n\nThe system $Ax = b$ can be written as $(D + L + U)x = b$, or rearranged as $Dx = -(L+U)x + b$.\n\nThe Gauss-Seidel iteration is defined by using the most recently computed components of the vector $x^{(m+1)}$ within the same iteration step $m+1$:\n$$\nD x^{(m+1)} = -L x^{(m+1)} - U x^{(m)} + b\n$$\n\nThe SOR method introduces a relaxation parameter, $\\omega$, to accelerate convergence. The new iterate $x^{(m+1)}$ is a weighted average of the previous iterate $x^{(m)}$ and the current Gauss-Seidel update. For each component $x_i$, the update is:\n$$\nx_i^{(m+1)} = (1-\\omega) x_i^{(m)} + \\frac{\\omega}{a_{ii}} \\left( b_i - \\sum_{ji} a_{ij} x_j^{(m+1)} - \\sum_{ji} a_{ij} x_j^{(m)} \\right)\n$$\nTo write this in matrix form, we can rearrange the equation for the $i$-th component:\n$$\na_{ii} x_i^{(m+1)} = (1-\\omega) a_{ii} x_i^{(m)} + \\omega b_i - \\omega \\sum_{ji} a_{ij} x_j^{(m+1)} - \\omega \\sum_{ji} a_{ij} x_j^{(m)}\n$$\nMoving all terms with $x^{(m+1)}$ to the left-hand side:\n$$\na_{ii} x_i^{(m+1)} + \\omega \\sum_{ji} a_{ij} x_j^{(m+1)} = (1-\\omega) a_{ii} x_i^{(m)} - \\omega \\sum_{ji} a_{ij} x_j^{(m)} + \\omega b_i\n$$\nRecognizing the matrix components in this expression:\n- The left-hand side corresponds to the $i$-th component of $(D + \\omega L)x^{(m+1)}$.\n- The terms on the right-hand side involving $x^{(m)}$ correspond to the $i$-th component of $((1-\\omega)D - \\omega U)x^{(m)}$.\n\nThus, the full system in matrix form is:\n$$\n(D + \\omega L) x^{(m+1)} = ((1-\\omega)D - \\omega U) x^{(m)} + \\omega b\n$$\nSolving for $x^{(m+1)}$ yields the SOR iteration:\n$$\nx^{(m+1)} = (D + \\omega L)^{-1} ((1-\\omega)D - \\omega U) x^{(m)} + \\omega (D + \\omega L)^{-1} b\n$$\nThis is a linear iteration of the form $x^{(m+1)} = T_{\\mathrm{SOR}} x^{(m)} + c$, where the SOR iteration matrix $T_{\\mathrm{SOR}}$ is:\n$$\nT_{\\mathrm{SOR}} = (D + \\omega L)^{-1} ((1-\\omega)D - \\omega U)\n$$\nand the constant vector is $c = \\omega (D + \\omega L)^{-1} b$.\n\n### 2. Indefiniteness and SOR Convergence\n\nThe convergence of the SOR method is determined by the spectral radius, $\\rho(T_{\\mathrm{SOR}})$, of the iteration matrix. The method converges if and only if $\\rho(T_{\\mathrm{SOR}})  1$.\n\nThe matrix in question is $A = -\\Delta - k^2 I$, where $-\\Delta$ is the discrete negative Laplacian on a uniform grid with Dirichlet boundary conditions. The eigenvalues of $A$, denoted $\\mu_j$, are related to the eigenvalues of $-\\Delta$, denoted $\\lambda_j(-\\Delta)$, by a simple shift:\n$$\n\\mu_j = \\lambda_j(-\\Delta) - k^2\n$$\nThe eigenvalues of $-\\Delta$ are all real and positive. The smallest of these is given by the formula:\n$$\n\\lambda_{\\min}(-\\Delta) = \\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)\n$$\nA matrix is symmetric positive definite (SPD) if and only if all its eigenvalues are positive. For matrix $A$, this requires $\\mu_j  0$ for all $j$. This condition is satisfied if and only if the smallest eigenvalue is positive:\n$$\n\\mu_{\\min} = \\lambda_{\\min}(-\\Delta) - k^2  0 \\implies k^2  \\lambda_{\\min}(-\\Delta)\n$$\nThe problem defines $k^2$ in terms of a ratio $r$: $k^2 = r \\cdot \\lambda_{\\min}(-\\Delta)$. Substituting this into the inequality gives:\n$$\nr \\cdot \\lambda_{\\min}(-\\Delta)  \\lambda_{\\min}(-\\Delta) \\implies r  1\n$$\nTherefore, the matrix $A$ is SPD for $r  1$. For an SPD matrix, it is a well-established theorem that the SOR method converges (i.e., $\\rho(T_{\\mathrm{SOR}})  1$) for any relaxation parameter $\\omega$ in the range $0  \\omega  2$.\n\nWhen $r \\ge 1$, the condition for positive definiteness is violated.\n- If $r = 1$, then $\\mu_{\\min} = 0$, and the matrix $A$ is singular and positive semi-definite.\n- If $r  1$, then $\\mu_{\\min}  0$. Since the largest eigenvalue of $-\\Delta$, $\\lambda_{\\max}(-\\Delta)$, is significantly larger than $\\lambda_{\\min}(-\\Delta)$, the largest eigenvalue of $A$, $\\mu_{\\max} = \\lambda_{\\max}(-\\Delta) - k^2$, will remain positive for moderate values of $r$. Thus, for $r1$, $A$ possesses both positive and negative eigenvalues, making it an indefinite matrix.\n\nWhen $A$ is not SPD, the convergence guarantee for SOR with $\\omega \\in (0, 2)$ is lost. The theory of SOR for indefinite matrices is substantially more complex, but in this common physical setting, transitioning from a positive definite to an indefinite matrix typically leads to the divergence of the SOR iteration. We, therefore, expect to find $\\rho(T_{\\mathrm{SOR}}) \\ge 1$ for the test cases where $r > 1$. The test suite is designed to explore this exact behavior by varying $r$ across the threshold $r=1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing the SOR iteration matrix for several\n    test cases and computing its spectral radius.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, omega, r)\n        (50, 1.2, 0.0),\n        (50, 1.2, 0.5),\n        (50, 1.2, 0.99),\n        (50, 1.2, 1.01),\n        (50, 1.2, 2.0),\n        (50, 1.8, 0.5),\n        (10, 1.2, 1.01),\n        (100, 1.2, 1.01),\n        (100, 1.0, 1.01),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, omega, r = case\n\n        # 1. Calculate grid parameters\n        h = 1.0 / (n + 1)\n\n        # 2. Calculate k^2 based on the smallest eigenvalue of the discrete Laplacian\n        lambda_min_laplacian = (4.0 / h**2) * np.sin(np.pi / (2.0 * (n + 1)))**2\n        k_squared = r * lambda_min_laplacian\n\n        # 3. Construct the matrix A for the Helmholtz-type system\n        # A = -Delta - k^2 * I\n        diag_val = (2.0 / h**2) - k_squared\n        offdiag_val = -1.0 / h**2\n        \n        A = np.diag(np.full(n, diag_val))\n        A += np.diag(np.full(n - 1, offdiag_val), k=1)\n        A += np.diag(np.full(n - 1, offdiag_val), k=-1)\n\n        # 4. Decompose A into D, L, and U for the splitting A = D + L + U\n        # D is the diagonal of A\n        D = np.diag(np.diag(A))\n        # L is the strictly lower triangular part of A\n        L = np.tril(A, k=-1)\n        # U is the strictly upper triangular part of A\n        U = np.triu(A, k=1)\n\n        # 5. Construct the SOR iteration matrix T_SOR\n        # T_SOR = inv(D + omega*L) * ((1-omega)*D - omega*U)\n        P = D + omega * L\n        Q = (1.0 - omega) * D - omega * U\n        \n        try:\n            P_inv = np.linalg.inv(P)\n            T_SOR = P_inv @ Q\n        except np.linalg.LinAlgError:\n            # This should not happen for the given test cases as D+omega*L is invertible.\n            # The determinant of a triangular matrix is the product of its diagonal elements.\n            # det(P) = det(D) != 0 for the test cases.\n            results.append(float('nan'))\n            continue\n\n        # 6. Compute the spectral radius of T_SOR\n        # rho(M) = max(|lambda_i(M)|)\n        eigenvalues = np.linalg.eigvals(T_SOR)\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        results.append(spectral_radius)\n\n    # Final print statement in the exact required format.\n    # Each value must be represented as a float with 6 decimal places.\n    formatted_results = [\"{:.6f}\".format(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}