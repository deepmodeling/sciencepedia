{
    "hands_on_practices": [
        {
            "introduction": "Before deploying an algorithm, it is crucial to understand its resource requirements. This first exercise provides a fundamental analysis of the computational and memory costs of the unrestarted Generalized Minimal Residual (GMRES) algorithm. By deriving these costs as a function of problem size $n$ and iteration number $k$, you will gain a clear, quantitative appreciation for why the algorithm's demands grow so rapidly and why restarted variants like GMRES($m$) are indispensable for large-scale applications .",
            "id": "3962703",
            "problem": "In computational fluid dynamics for aerospace applications, the semi-discrete compressible Navier–Stokes equations yield a large, sparse, non-symmetric linear system of the form $A x = b$ at each Newton step or pseudo-time step, where $A \\in \\mathbb{R}^{n \\times n}$ encodes flux Jacobians and stabilization terms, and $x \\in \\mathbb{R}^{n}$ collects the unknowns (e.g., density, momentum components, and energy over all control volumes). Consider solving this linear system using the Generalized Minimal Residual (GMRES) method. Assume an unrestarted implementation with the Arnoldi process and modified Gram–Schmidt orthogonalization, together with incremental least-squares updates via Givens rotations.\n\nAdopt the standard scalar flop counting model in which each scalar multiply or add counts as $1$ flop, a scalar divide counts as $1$ flop, a dot product of two vectors of length $n$ costs $2 n$ flops, an “axpy” update of the form $y \\leftarrow y + \\alpha x$ with $x, y \\in \\mathbb{R}^{n}$ costs $2 n$ flops, computing a $2$-norm of a vector of length $n$ costs $2 n$ flops, and a scaling $y \\leftarrow y / \\alpha$ costs $n$ flops. Let the cost of one sparse matrix–vector product $v \\leftarrow A u$ be denoted $c_{\\mathrm{mv}}(n)$ flops, where $c_{\\mathrm{mv}}(n)$ depends only on the number of unknowns $n$ and the sparsity pattern induced by the aerospace discretization. Ignore any preconditioner applications and exclude the storage for $A$ itself from the storage budget.\n\nAt iteration $k$ of GMRES, the algorithm stores the current Krylov basis $V_{k+1} \\in \\mathbb{R}^{n \\times (k+1)}$, the upper Hessenberg matrix $H_{k+1,k} \\in \\mathbb{R}^{(k+1) \\times k}$, the working vectors of length $n$ needed by Arnoldi (e.g., the residual, the candidate vector before normalization, and the current approximate solution), and the least-squares workspace of length proportional to $k$ (e.g., the right-hand side in the reduced problem, the current coefficients, and the Givens parameters). Within iteration $k$, the arithmetic consists of a single matrix–vector product, $k$ inner products and $k$ axpy updates within modified Gram–Schmidt, a single normalization (one $2$-norm and one scaling), and an incremental Givens update of the reduced least-squares problem that costs a number of flops proportional to $k$.\n\nCompute, in closed form, the total scalar storage required at iteration $k$ (excluding storage of $A$ itself) and the arithmetic cost in flops for iteration $k$ as functions of the number of unknowns $n$ and the iteration count $k$. Express your final answer as analytic expressions in terms of $n$ and $k$. Do not introduce any additional parameters other than $c_{\\mathrm{mv}}(n)$, which must depend only on $n$. Provide the single final answer as two expressions arranged in a row matrix, where the first entry is the storage count in scalars and the second entry is the flop count per iteration. No units are required in the final answer.",
            "solution": "The problem is evaluated as scientifically sound, well-posed, objective, and self-contained, and therefore is deemed valid. We proceed with a detailed derivation of the requested quantities.\n\nThe task is to compute the total scalar storage required at the conclusion of iteration $k$ of unrestarted GMRES, denoted $S_k$, and the arithmetic cost in floating-point operations (flops) for executing iteration $k$, denoted $C_k$. The variables are the number of unknowns $n$ and the iteration count $k$, where $k \\ge 1$. The cost of a sparse matrix-vector product is given as $c_{\\mathrm{mv}}(n)$.\n\n**1. Total Scalar Storage ($S_k$)**\n\nThe total storage at the end of iteration $k$ is the sum of the storage for the components generated and maintained by the algorithm. We tally these based on the problem description.\n\n- **Krylov Basis Storage**: At the end of iteration $k$, the algorithm has computed $k+1$ orthonormal basis vectors, $v_1, v_2, \\dots, v_{k+1}$, which form the columns of the matrix $V_{k+1} \\in \\mathbb{R}^{n \\times (k+1)}$. Each vector is of length $n$. The total storage for the basis is:\n$$ S_{\\text{basis}} = (k+1)n $$\n\n- **Hessenberg Matrix Storage**: The Arnoldi process generates an upper Hessenberg matrix $H_{k+1,k} \\in \\mathbb{R}^{(k+1) \\times k}$. This matrix is typically stored densely. The storage required is:\n$$ S_{H} = (k+1)k = k^2 + k $$\n\n- **Working Vector Storage**: The problem states that the algorithm stores \"working vectors of length $n$ needed by Arnoldi (e.g., the residual, the candidate vector before normalization, and the current approximate solution)\". Interpreting this list of examples as the required set of vectors, we account for $3$ working vectors of length $n$. The storage is:\n$$ S_{\\text{work}} = 3n $$\n\n- **Least-Squares Workspace Storage**: The problem specifies storage for \"the right-hand side in the reduced problem, the current coefficients, and the Givens parameters\". At the end of iteration $k$:\n    - The right-hand side of the reduced least-squares problem, after application of $k$ Givens rotations, is a vector of length $k+1$. Storage: $k+1$.\n    - The coefficients of the solution in the Krylov basis, $y_k$, form a vector of length $k$. Storage: $k$.\n    - The $k$ Givens rotations require storing one cosine and one sine each. Storage: $2k$.\nThe total storage for this workspace is:\n$$ S_{\\text{LS}} = (k+1) + k + 2k = 4k + 1 $$\n\nThe total scalar storage $S_k$ is the sum of these components:\n$$ S_k = S_{\\text{basis}} + S_{H} + S_{\\text{work}} + S_{\\text{LS}} $$\n$$ S_k = (k+1)n + (k^2+k) + 3n + (4k+1) $$\n$$ S_k = kn + n + k^2 + k + 3n + 4k + 1 $$\n$$ S_k = (k+4)n + k^2 + 5k + 1 $$\n\n**2. Arithmetic Cost for Iteration $k$ ($C_k$)**\n\nThe cost for iteration $k$ is the sum of flops for the operations performed within that single iteration. Iteration $k$ generates the basis vector $v_{k+1}$ and the $k$-th column of the Hessenberg matrix.\n\n- **Sparse Matrix-Vector Product**: The Arnoldi process begins by computing $w \\leftarrow A v_k$. The cost is given as:\n$$ C_{\\text{mv}} = c_{\\mathrm{mv}}(n) $$\n\n- **Modified Gram-Schmidt Orthogonalization**: The vector $w$ is orthogonalized against the existing basis vectors $v_1, \\dots, v_k$. The problem specifies this involves \"$k$ inner products and $k$ axpy updates\".\n    - The cost of $k$ inner products between vectors of length $n$ is $k \\times (2n) = 2kn$ flops.\n    - The cost of $k$ \"axpy\" updates ($w \\leftarrow w - h_{i,k}v_i$) on vectors of length $n$ is $k \\times (2n) = 2kn$ flops.\nThe total MGS cost is:\n$$ C_{\\text{MGS}} = 2kn + 2kn = 4kn $$\n\n- **Normalization**: The resulting orthogonal vector $w$ is normalized to produce $v_{k+1}$. This involves one norm calculation and one vector scaling.\n    - The cost of a $2$-norm of a vector of length $n$ is $2n$ flops.\n    - The cost of scaling a vector of length $n$ is $n$ flops.\nThe total normalization cost is:\n$$ C_{\\text{norm}} = 2n + n = 3n $$\n\n- **Givens Rotation Update**: An incremental update to the least-squares problem is performed. This involves:\n    1. Applying the $k-1$ previous Givens rotations to the newly computed $k$-th column of $H$. Each rotation application costs $4$ multiplications and $2$ additions, for $6$ flops. Total cost: $6(k-1)$ flops.\n    2. Computing the parameters ($c_k, s_k$) for the new $k$-th Givens rotation. This requires approximately $2$ squares, $1$ add, $1$ square root, and $2$ divisions, which we count as $6$ flops.\n    3. Applying the new rotation to the right-hand-side vector. This also costs $6$ flops.\nThe total cost for the Givens update is:\n$$ C_{\\text{Givens}} = 6(k-1) + 6 + 6 = 6k - 6 + 12 = 6k + 6 $$\n\nThe total arithmetic cost for iteration $k$, $C_k$, is the sum of these costs:\n$$ C_k = C_{\\text{mv}} + C_{\\text{MGS}} + C_{\\text{norm}} + C_{\\text{Givens}} $$\n$$ C_k = c_{\\mathrm{mv}}(n) + 4kn + 3n + (6k+6) $$\n$$ C_k = c_{\\mathrm{mv}}(n) + (4k+3)n + 6k + 6 $$\n\nThus, the storage and flop counts are derived as functions of $n$ and $k$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(k+4)n + k^2 + 5k + 1 & c_{\\mathrm{mv}}(n) + (4k+3)n + 6k + 6\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A frequent and often puzzling observation when using GMRES for CFD problems is the stagnation of the residual norm, where convergence stalls despite further iterations. This exercise moves beyond simple convergence analysis based on eigenvalues to explore the reasons for this behavior in the non-normal systems typical of fluid dynamics. You will analyze how properties like the pseudospectrum and the field of values, which are more descriptive than eigenvalues for non-normal operators, govern GMRES convergence and can explain the onset of a residual plateau .",
            "id": "3962730",
            "problem": "Consider a linear system $A x = b$ arising in Computational Fluid Dynamics (CFD) from a fully implicit Newton linearization of the compressible Navier–Stokes equations on a curvilinear mesh using upwind fluxes. The discrete Jacobian $A$ is large, sparse, nonsymmetric, and strongly non-normal because of convection-dominated transport and source-term linearization. A right preconditioner $M$ is applied, so the solved system is $A M^{-1} y = b$ with $x = M^{-1} y$. The Generalized Minimal Residual (GMRES) method without restart is used, and the residual $\\|r_k\\|_2$ is observed to plateau for $k$ increasing from $80$ to $200$, i.e., despite increasing the Krylov subspace dimension $k$, the residual norm does not decrease further.\n\nStarting from the core definitions of the residual $r_k = b - A x_k$, the Krylov subspace $\\mathcal{K}_k(A M^{-1}, r_0)$, the spectrum (the set of eigenvalues), and the $\\varepsilon$-pseudospectrum (the set of points $z$ for which $\\|(A M^{-1} - z I)^{-1}\\|$ exceeds $1/\\varepsilon$), reason about mechanisms that can explain residual stagnation in GMRES for such CFD matrices. Ignore any restart effects and assume double precision arithmetic is used with appropriate scaling of unknowns.\n\nWhich of the following statements are most consistent with scientifically sound explanations of stagnation tied to spectral and pseudospectral features of $A M^{-1}$ in this aerospace CFD setting? Select all that apply.\n\nA. If the $\\varepsilon$-pseudospectrum of $A M^{-1}$ intersects $0$ for small $\\varepsilon$, then residual polynomials constrained by $p(0) = 1$ cannot be uniformly small on the relevant operator region, and GMRES can exhibit stagnation even as $k$ grows.\n\nB. Eigenvalues of $A M^{-1}$ tightly clustered near a point $z_0$ with $|z_0|$ bounded away from $0$ necessarily cause GMRES stagnation, because polynomials cannot reduce the residual when eigenvalues are clustered away from $0$.\n\nC. Strong non-normality, reflected in an ill-conditioned eigenvector matrix of $A M^{-1}$, implies that smallness of the residual-minimizing polynomial on the spectrum does not control $\\|p_k(A M^{-1})\\|$, so transient amplification can lead to a residual plateau despite increasing $k$.\n\nD. In exact arithmetic, $\\|r_k\\|_2$ strictly decreases with $k$ for GMRES without restart, so a residual plateau at nonzero level must be due solely to loss of Arnoldi orthogonality from roundoff errors.\n\nE. If the field of values of $A M^{-1}$ contains $0$, known GMRES bounds provide no guarantee of reduction of $\\|r_k\\|_2$, and stagnation can occur even as $k$ increases.",
            "solution": "The problem statement is scientifically sound, well-posed, and provides a clear scenario for analysis. It describes a common and challenging situation in computational science and engineering: the iterative solution of large, sparse, non-normal linear systems arising from the discretization of partial differential equations. The observed phenomenon of GMRES residual stagnation is a well-documented issue for which several theoretical explanations exist, centered on the properties of the system matrix. I will now proceed with the analysis.\n\nLet the preconditioned matrix be denoted by $B = A M^{-1}$. The GMRES algorithm at iteration $k$ finds an approximate solution $y_k$ from the affine Krylov subspace $y_0 + \\mathcal{K}_k(B, r_0)$, where $r_0 = b - B y_0$ is the initial residual, that minimizes the $L_2$-norm of the residual $\\|r_k\\|_2 = \\|b - B y_k\\|_2$. The residual can be expressed as $r_k = p_k(B) r_0$, where $p_k$ is a polynomial of degree at most $k$ satisfying the constraint $p_k(0) = 1$. GMRES finds the optimal such polynomial. The convergence behavior is thus governed by how small $\\|p_k(B) r_0\\|_2$ can be made as $k$ increases. This leads to the upper bound on the relative residual:\n$$ \\frac{\\|r_k\\|_2}{\\|r_0\\|_2} \\le \\min_{p_k \\in \\mathcal{P}_k, p_k(0)=1} \\|p_k(B)\\|_2 $$\nwhere $\\mathcal{P}_k$ is the set of polynomials of degree at most $k$.\n\nFor a normal matrix $B$ (i.e., $B B^H = B^H B$), the $2$-norm of a matrix polynomial is determined by the matrix's spectrum, $\\sigma(B)$: $\\|p_k(B)\\|_2 = \\max_{\\lambda \\in \\sigma(B)} |p_k(\\lambda)|$. Convergence is then a matter of finding a polynomial $p_k$ with $p_k(0)=1$ that is small on the set of eigenvalues.\n\nThe problem states that $A$, and thus likely $B=AM^{-1}$, is strongly non-normal. For a non-normal matrix, the spectrum alone does not govern the norm of matrix functions. Instead, quantities like the pseudospectrum ($\\Lambda_\\varepsilon(B)$), the field of values ($W(B)$), and the condition number of the eigenvector matrix are required for a complete picture. The observed stagnation, i.e., a plateau in $\\|r_k\\|_2$ for $k \\in [80, 200]$, must be explained through these concepts.\n\nLet's evaluate each statement.\n\n**A. If the $\\varepsilon$-pseudospectrum of $A M^{-1}$ intersects $0$ for small $\\varepsilon$, then residual polynomials constrained by $p(0) = 1$ cannot be uniformly small on the relevant operator region, and GMRES can exhibit stagnation even as $k$ grows.**\n\nLet $B = A M^{-1}$. The convergence of GMRES is not governed by the spectrum of $B$, $\\sigma(B)$, but by its pseudospectrum, $\\Lambda_{\\varepsilon}(B)$. A common bound on the norm of the polynomial is $\\|p_k(B)\\|_2 \\le C \\sup_{z \\in \\Omega} |p_k(z)|$, where $\\Omega$ is a set containing $\\Lambda_{\\varepsilon}(B)$ for some $\\varepsilon$. The GMRES residual polynomial $p_k(z)$ must satisfy $p_k(0)=1$. If for a small $\\varepsilon$, the $\\varepsilon$-pseudospectrum $\\Lambda_\\varepsilon(B)$ contains the origin or a region around it, then the domain on which $|p_k(z)|$ must be made small includes points near $z=0$. By the Maximum Modulus Principle or simply continuity, a polynomial that is $1$ at $z=0$ cannot be small throughout a region that contains $z=0$. Therefore, $\\sup_{z \\in \\Lambda_\\varepsilon(B)} |p_k(z)|$ will be bounded below by a value close to $1$, irrespective of the degree $k$. This implies that $\\|p_k(B)\\|_2$ cannot be made arbitrarily small, leading to a poor upper bound on the residual norm and explaining the observed stagnation. This statement is a cornerstone of the modern understanding of GMRES for non-normal matrices.\n\nVerdict: **Correct**.\n\n**B. Eigenvalues of $A M^{-1}$ tightly clustered near a point $z_0$ with $|z_0|$ bounded away from $0$ necessarily cause GMRES stagnation, because polynomials cannot reduce the residual when eigenvalues are clustered away from $0$.**\n\nThis statement is fundamentally incorrect. If the matrix were normal, having its eigenvalues clustered around a point $z_0 \\neq 0$ would lead to very rapid, often superlinear, convergence. This is because one can construct a polynomial of degree $k$, such as $p_k(z) = (1 - z/z_0)^k$, which satisfies $p_k(0)=1$ and is extremely small in the neighborhood of $z_0$. GMRES would find an even better polynomial. The premise \"polynomials cannot reduce the residual when eigenvalues are clustered away from $0$\" is false. While non-normality complicates this picture (the pseudospectrum may be much larger than the cluster of eigenvalues), the clustering of eigenvalues itself is a favorable condition for GMRES, not a cause of stagnation. Stagnation is caused by other features that are dominant despite the favorable eigenvalue locations. The word \"necessarily\" makes the statement definitively false.\n\nVerdict: **Incorrect**.\n\n**C. Strong non-normality, reflected in an ill-conditioned eigenvector matrix of $A M^{-1}$, implies that smallness of the residual-minimizing polynomial on the spectrum does not control $\\|p_k(A M^{-1})\\|$, so transient amplification can lead to a residual plateau despite increasing $k$.**\n\nThis statement provides a correct and insightful explanation. Strong non-normality of $B=AM^{-1}$ is equivalent to its matrix of eigenvectors, $V$, being ill-conditioned (i.e., having a large condition number $\\kappa_2(V) = \\|V\\|_2 \\|V^{-1}\\|_2 \\gg 1$). The relationship between the norm of $p_k(B)$ and its values on the spectrum $\\sigma(B)$ is given by the bound $\\|p_k(B)\\|_2 \\le \\kappa_2(V) \\max_{\\lambda \\in \\sigma(B)} |p_k(\\lambda)|$. If $\\kappa_2(V)$ is large, even if the polynomial is very small on the eigenvalues, $\\|p_k(B)\\|_2$ can be large. This is the essence of why eigenvalue analysis is insufficient for non-normal matrices. The term \"transient amplification\" refers to the behavior of $\\|f(B)\\|$ for certain functions $f$, like matrix exponentials or polynomials, where the norm can initially grow significantly before decaying. In the context of GMRES, this translates to a long initial phase where the residual barely decreases (a plateau), because the norm of the GMRES polynomial $\\|p_k(B)\\|_2$ remains large for small to moderate $k$. Only when $k$ becomes large enough to control the polynomial's behavior over the much larger pseudospectrum does convergence accelerate.\n\nVerdict: **Correct**.\n\n**D. In exact arithmetic, $\\|r_k\\|_2$ strictly decreases with $k$ for GMRES without restart, so a residual plateau at nonzero level must be due solely to loss of Arnoldi orthogonality from roundoff errors.**\n\nThe first part of the statement is correct: GMRES minimizes the residual norm over a sequence of nested subspaces, $\\mathcal{K}_k \\subset \\mathcal{K}_{k+1}$. Therefore, in exact arithmetic, $\\|r_{k+1}\\|_2 \\le \\|r_k\\|_2$. The decrease is strict unless the exact solution is found. However, the conclusion that a plateau *must be due solely* to roundoff errors is incorrect. A plateau, in the sense of an extremely slow rate of decrease ($\\|r_k\\|_2 \\approx \\|r_{k-1}\\|_2$), is a well-known behavior for highly non-normal matrices even in exact arithmetic. The theoretical convergence rate can be arbitrarily slow. While loss of orthogonality in the Arnoldi process due to floating-point arithmetic is a real and important practical issue that exacerbates stagnation, it is not the *sole* cause. The fundamental difficulty is a mathematical property of the operator $A M^{-1}$, as described in options A and C. The question specifically asks for explanations tied to spectral and pseudospectral features, which are properties of the matrix itself, not the floating-point system.\n\nVerdict: **Incorrect**.\n\n**E. If the field of values of $A M^{-1}$ contains $0$, known GMRES bounds provide no guarantee of reduction of $\\|r_k\\|_2$, and stagnation can occur even as $k$ increases.**\n\nThe field of values (or numerical range) of $B=AM^{-1}$ is the set $W(B) = \\{v^H B v : \\|v\\|_2 = 1\\}$. A significant body of GMRES convergence theory (e.g., a theorem by Eisenstat, Elman, and Schultz) provides convergence rate bounds that depend on $W(B)$. Specifically, if $W(B)$ is contained in a half-plane not containing the origin, or more generally, if the origin is well separated from $W(B)$, then a bound on the convergence rate can be established. If the field of values contains the origin, $0 \\in W(B)$, these bounds are no longer applicable and provide no guarantee of convergence. The condition $0 \\in W(B)$ is a well-established indicator that GMRES may converge very slowly or stagnate. It is a more general condition than the matrix having a zero eigenvalue; for a non-normal matrix, $0$ can be in $W(B)$ even if the matrix is invertible. This statement correctly identifies a key theoretical condition linked to GMRES stagnation.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "Theoretical knowledge finds its true value when applied to practical design choices. This final practice places you in the role of a CFD practitioner tasked with configuring the restarted GMRES($m$) algorithm for a large-scale simulation. You will synthesize your understanding of GMRES convergence theory for non-normal operators with practical hardware constraints, such as available memory, to determine an optimal restart parameter $m$ that balances performance and resource limitations .",
            "id": "3962743",
            "problem": "A compressible Reynolds-Averaged Navier–Stokes (RANS) solver with implicit time integration yields at each nonlinear update a large sparse linear system $A x = b$. A left preconditioner $M$ is applied to form $M^{-1} A x = M^{-1} b$, and the linear system is solved using restarted Generalized Minimal Residual (GMRES) with restart parameter $m$, denoted GMRES($m$). This question concerns selecting $m$ based on three constraints that arise in computational fluid dynamics (CFD): available memory, the level of non-normality of the preconditioned operator, and a desired per-restart convergence rate.\n\nAssume the following scientifically consistent scenario:\n- The number of degrees of freedom is $n = 3 \\times 10^{6}$.\n- The memory reserved for Krylov subspace storage (basis vectors and small dense structures) is $M_{\\mathrm{bytes}} = 8 \\times 10^{9}$ bytes.\n- The preconditioning and orthogonalization pipelines require $s = 5$ additional auxiliary vectors of length $n$ resident during iteration (for the current iterate, residual, preconditioned residual, and two work vectors).\n- Each Krylov vector is stored in double precision (each entry uses $8$ bytes), and GMRES($m$) stores approximately $(m+1)$ basis vectors, an upper Hessenberg matrix of size $(m+1) \\times m$, and a small number of scalar coefficients.\n- The preconditioned operator $M^{-1} A$ has a moderate level of non-normality characterized by the departure-from-normality index $\\delta \\approx 0.3$ and an a priori estimate of the $2$-norm $\\|M^{-1}A\\|_{2} \\approx 1.4$. The distance from the origin to the field of values (numerical range) of $M^{-1}A$ is estimated as $\\nu \\approx 0.5$.\n- The desired per-restart residual reduction factor is $q = 10^{-4}$, meaning after $m$ inner iterations the residual norm should be reduced by at least a factor $10^{-4}$ before restarting.\n\nWhich option best specifies a scientifically sound, practically implementable rule to select the GMRES restart parameter $m$ in this setting and gives the corresponding numerical choice of $m$ for the given data?\n\nA. Enforce the memory constraint by bounding the number of stored length-$n$ vectors, including $(m+1)$ GMRES basis vectors and $s$ auxiliary vectors, with the Hessenberg storage negligible for the given $n$. That yields $m_{\\mathrm{mem}} = \\left\\lfloor \\dfrac{M_{\\mathrm{bytes}}}{8 n} \\right\\rfloor - (s+1)$. Impose the convergence target via the field-of-values bound on GMRES residuals, giving the minimum $m$ needed to achieve $q$; choose $m_{\\mathrm{rate}}$ as the smallest integer such that the bound is $\\le q$. Set $m$ to the smallest $m$ that satisfies the rate but does not violate memory; if $m_{\\mathrm{rate}} \\le m_{\\mathrm{mem}}$ use $m = m_{\\mathrm{rate}}$, otherwise use $m = m_{\\mathrm{mem}}$ and strengthen $M$ or consider Biconjugate Gradient Stabilized (BiCGSTAB) if needed. For the given data, $m_{\\mathrm{mem}} = \\left\\lfloor \\dfrac{8 \\times 10^{9}}{8 \\cdot 3 \\times 10^{6}} \\right\\rfloor - 6 = \\left\\lfloor \\dfrac{8 \\times 10^{9}}{24 \\times 10^{6}} \\right\\rfloor - 6 = \\left\\lfloor 333.\\overline{3} \\right\\rfloor - 6 = 327$, and the field-of-values estimate with $\\|M^{-1}A\\|_{2} \\approx 1.4$ and $\\nu \\approx 0.5$ yields $m_{\\mathrm{rate}} \\approx 135$, so choose $m = 135$.\n\nB. Base $m$ solely on the spectral radius $\\rho(M^{-1}A)$ assuming a normal operator. Target $q$ using a geometric convergence model $q \\approx \\rho^{m}$, so $m \\approx \\left\\lceil \\dfrac{\\ln(q^{-1})}{\\ln(\\rho^{-1})} \\right\\rceil$. With $\\rho \\approx 0.95$, this gives $m \\approx \\left\\lceil \\dfrac{\\ln(10^{4})}{\\ln(1/0.95)} \\right\\rceil \\approx 180$, and since the rough memory limit for $(m+1)$ vectors is $\\left\\lfloor \\dfrac{M_{\\mathrm{bytes}}}{8n} \\right\\rfloor - 1 \\approx 332$, the choice $m = 180$ fits memory.\n\nC. Maximize $m$ subject only to memory by using all available basis-vector storage, i.e., set $m = \\left\\lfloor \\dfrac{M_{\\mathrm{bytes}}}{8 n} \\right\\rfloor - (s+1)$, which yields $m = 327$. This ensures the largest subspace per restart, which is generally optimal irrespective of operator non-normality and desired rate.\n\nD. Non-normality causes transient growth, so to prevent stagnation one must restart very frequently. Cap $m$ at a small fixed value, e.g., $m = 50$, regardless of the desired $q$, because frequent restarting suppresses non-normal amplification and stabilizes convergence under fixed memory.",
            "solution": "The task is to determine a scientifically and practically sound rule for selecting the restart parameter $m$ for the GMRES($m$) algorithm in the context of a compressible RANS simulation. The selection must balance three constraints: available memory, the non-normal nature of the preconditioned linear system, and a target for convergence speed.\n\nFirst, we validate the problem statement. The givens are:\n- System size (degrees of freedom): $n = 3 \\times 10^{6}$.\n- Available memory for Krylov-related data: $M_{\\mathrm{bytes}} = 8 \\times 10^{9}$ bytes.\n- Number of auxiliary vectors: $s = 5$.\n- Data precision: double precision, requiring $8$ bytes per entry.\n- Storage model for GMRES($m$): $(m+1)$ basis vectors plus auxiliary structures.\n- Properties of the preconditioned operator $B = M^{-1} A$:\n    - Non-normality index: $\\delta \\approx 0.3$.\n    - 2-norm: $\\|B\\|_{2} \\approx 1.4$.\n    - Distance from the origin to the field of values $W(B)$: $\\nu = \\text{dist}(0, W(B)) \\approx 0.5$.\n- Desired per-restart residual reduction factor: $q = 10^{-4}$.\n\nThe problem is scientifically grounded, well-posed, objective, and provides consistent and realistic data for a large-scale CFD problem. The characterization of the operator using its norm and field of values is standard in the numerical analysis of iterative methods for non-normal matrices. The problem is therefore valid, and we may proceed with the solution.\n\nThe selection of $m$ is governed by two main constraints: an upper bound from memory limitations, $m_{\\mathrm{mem}}$, and a lower bound from the desired convergence rate, $m_{\\mathrm{rate}}$.\n\n**1. Memory Constraint ($m \\le m_{\\mathrm{mem}}$)**\n\nThe total available memory $M_{\\mathrm{bytes}}$ must accommodate all simultaneously resident, full-length vectors. These include the $(m+1)$ orthonormal basis vectors of the Krylov subspace generated by the Arnoldi process, and the $s=5$ auxiliary vectors required for the solver's operation. Storage for the Hessenberg matrix and other small arrays is stated to be negligible, which is a valid approximation for large $n$.\n\nThe size of a single vector of length $n$ in double precision is:\n$$S_{\\mathrm{vec}} = n \\times 8 = (3 \\times 10^{6}) \\times 8 = 24 \\times 10^{6} \\text{ bytes}.$$\n\nThe total number of vectors to be stored is $(m+1) + s$. With $s=5$, this is $m+6$ vectors. The memory constraint is thus:\n$$(m+6) \\times S_{\\mathrm{vec}} \\le M_{\\mathrm{bytes}}$$\n$$m+6 \\le \\frac{M_{\\mathrm{bytes}}}{S_{\\mathrm{vec}}} = \\frac{8 \\times 10^{9}}{24 \\times 10^{6}} = \\frac{8000}{24} = \\frac{1000}{3} \\approx 333.33.$$\n\nSince $m$ must be an integer, we have $m+6 \\le 333$, which implies:\n$$m \\le 333 - 6 = 327.$$\nThus, the maximum restart parameter allowed by memory is $m_{\\mathrm{mem}} = 327$.\n\n**2. Convergence Rate Constraint ($m \\ge m_{\\mathrm{rate}}$)**\n\nThe problem states that the operator $B=M^{-1}A$ is non-normal. For such operators, convergence estimates based on the spectral radius $\\rho(B)$ are unreliable. A more robust approach uses information about the field of values (or numerical range), $W(B)$.\n\nA standard convergence bound for GMRES, applicable when the field of values $W(B)$ does not contain the origin, is given in terms of $\\nu = \\text{dist}(0, W(B))$ and $\\|B\\|_2$. A commonly used estimate for the residual reduction after $m$ steps is:\n$$ \\frac{\\|r_m\\|_2}{\\|r_0\\|_2} \\le C \\cdot \\left(\\sqrt{1 - \\frac{\\nu^2}{\\|B\\|_2^2}}\\right)^m $$\nwhere the pre-asymptotic factor $C$ can be greater than $1$ (e.g., $C=\\|B\\|_2/\\nu$) and accounts for possible initial transient growth of the residual. For a-priori estimation of the required iteration count, it is common to use the simplified asymptotic bound by setting $C=1$:\n$$ \\frac{\\|r_m\\|_2}{\\|r_0\\|_2} \\approx \\left(\\sqrt{1 - \\frac{\\nu^2}{\\|B\\|_2^2}}\\right)^m \\le q. $$\nWe require the residual reduction to be at least $q=10^{-4}$. Let's calculate the convergence factor from the provided data: $\\nu \\approx 0.5$ and $\\|B\\|_2 \\approx 1.4$.\n$$ \\rho_{\\mathrm{fov}} = \\sqrt{1 - \\frac{\\nu^2}{\\|B\\|_2^2}} = \\sqrt{1 - \\frac{(0.5)^2}{(1.4)^2}} = \\sqrt{1 - \\frac{0.25}{1.96}} = \\sqrt{1 - 0.12755...} = \\sqrt{0.87244...} \\approx 0.93405. $$\nNow, we solve for the minimum $m$ that satisfies the rate requirement:\n$$ (\\rho_{\\mathrm{fov}})^m \\le q $$\n$$ (0.93405)^m \\le 10^{-4} $$\nTaking the natural logarithm of both sides:\n$$ m \\ln(0.93405) \\le \\ln(10^{-4}) $$\nSince $\\ln(0.93405) \\approx -0.06824$ is negative, we must reverse the inequality when dividing:\n$$ m \\ge \\frac{\\ln(10^{-4})}{\\ln(0.93405)} \\approx \\frac{-9.21034}{-0.06824} \\approx 134.96. $$\nSince $m$ must be an integer, the minimum value required to meet the convergence target is $m_{\\mathrm{rate}} = \\lceil 134.96 \\rceil = 135$.\n\n**3. Selection of the\nOptimal Restart Parameter $m$**\n\nWe have determined the memory limit $m_{\\mathrm{mem}} = 327$ and the convergence requirement $m_{\\mathrm{rate}} = 135$. The scientifically sound and practically efficient strategy is to choose the smallest restart parameter that achieves the desired convergence rate, provided it fits within the memory budget. This minimizes the work per restart cycle while meeting the performance goal.\n\nHere, $m_{\\mathrm{rate}} = 135 \\le m_{\\mathrm{mem}} = 327$. The condition is met. Therefore, the recommended choice is $m = m_{\\mathrm{rate}} = 135$. If it were the case that $m_{\\mathrm{rate}} > m_{\\mathrm{mem}}$, it would be impossible to satisfy both constraints simultaneously. In that scenario, one would be forced to use $m = m_{\\mathrm{mem}}$ and accept a slower convergence rate (i.e., not achieving the reduction $q$ in one cycle), seek to improve the preconditioner $M$, or switch to a different iterative method like BiCGSTAB.\n\n**Evaluation of Options**\n\n**A. Enforce the memory constraint... Impose the convergence target via the field-of-values bound... For the given data, $m_{\\mathrm{mem}} = ... = 327$, and the field-of-values estimate ... yields $m_{\\mathrm{rate}} \\approx 135$, so choose $m = 135$.**\n- The formula for the memory constraint is given as $m_{\\mathrm{mem}} = \\left\\lfloor \\dfrac{M_{\\mathrm{bytes}}}{8 n} \\right\\rfloor - (s+1)$. With $s=5$, this is $\\left\\lfloor \\dfrac{M_{\\mathrm{bytes}}}{8 n} \\right\\rfloor - 6$. Our derivation yielded $m \\le \\left\\lfloor \\dfrac{M_{\\mathrm{bytes}}}{8 n} \\right\\rfloor - 6$, which is identical. The calculation $m_{\\mathrm{mem}} = 327$ is correct.\n- The use of the field-of-values bound is the correct approach for a non-normal operator. The calculation yielding $m_{\\mathrm{rate}} \\approx 135$ matches our derivation.\n- The decision logic—choosing $m = m_{\\mathrm{rate}}$ because $m_{\\mathrm{rate}} \\le m_{\\mathrm{mem}}$, and outlining the alternative path if this were not the case—is sound and reflects best practices.\n- The final choice $m=135$ is correct based on the provided data and sound principles.\n**Verdict: Correct.**\n\n**B. Base $m$ solely on the spectral radius $\\rho(M^{-1}A)$ assuming a normal operator.**\n- This option's fundamental premise is flawed. The problem explicitly states the operator is non-normal. For such operators, convergence behavior is dictated by the pseudospectra, not just the spectra (eigenvalues). Relying on a spectral radius-based estimate, which is valid only for normal operators, is scientifically incorrect in this context and can lead to grossly optimistic predictions and solver failure. The option also invents a value for the spectral radius ($\\rho \\approx 0.95$) that is not provided in the problem statement.\n**Verdict: Incorrect.**\n\n**C. Maximize $m$ subject only to memory by using all available basis-vector storage, i.e., set $m = 327$.**\n- This is a greedy strategy that is not necessarily optimal. The computational work of a GMRES($m$) cycle is proportional to $m$. If a smaller value $m=135$ is sufficient to meet the convergence target per cycle, using a larger value $m=327$ performs more than twice the necessary work (matrix-vector products, orthogonalizations) per cycle. The goal is to minimize total time to solution, not just maximize the subspace size. This strategy is inefficient.\n**Verdict: Incorrect.**\n\n**D. Non-normality causes transient growth, so to prevent stagnation one must restart very frequently. Cap $m$ at a small fixed value, e.g., $m = 50$.**\n- This option correctly identifies a potential issue with non-normal operators (transient residual growth) but proposes the wrong solution. If GMRES is restarted too early (i.e., with an $m$ that is too small), it may never get past the transient growth phase, leading to stagnation. The correct approach is to choose $m$ large enough to \"get over the hump\" and into the region of asymptotic convergence. The field-of-values analysis suggests this requires about $m=135$ iterations. Choosing $m=50$ would almost certainly cause the solver to stagnate, as it would restart repeatedly while the residual is still in its transient, non-decreasing phase.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}