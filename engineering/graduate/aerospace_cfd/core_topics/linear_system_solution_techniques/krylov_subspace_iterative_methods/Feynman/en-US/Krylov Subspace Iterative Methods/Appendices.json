{
    "hands_on_practices": [
        {
            "introduction": "In computational fluid dynamics, the choice of discretization scheme profoundly impacts the properties of the resulting linear system. This first practice explores how numerical techniques used to stabilize convection-dominated flows, such as first-order upwinding, directly influence the algebraic structure of the system matrix. You will construct a matrix for a 1D convection-diffusion problem and compute its departure from normality, providing a concrete link between a common CFD practice and the emergence of non-normal matrices that necessitate robust solvers like GMRES. ",
            "id": "3970343",
            "problem": "Consider the steady one-dimensional convection–diffusion operator $L u = a \\, \\frac{d u}{d x} - \\nu \\, \\frac{d^{2} u}{d x^{2}}$ on the domain $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. In computational fluid dynamics for aerospace applications, the discrete linear system $A \\mathbf{u} = \\mathbf{f}$ is obtained from $L$ by finite differences on a uniform grid. To model typical stabilization used in practice, augment the physical diffusion coefficient $\\nu$ by an artificial diffusion $\\nu_{\\text{art}}=\\gamma \\, a \\, h$, where $a>0$ is the convection speed, $h$ is the uniform grid spacing, and $\\gamma \\geq 0$ is a stabilization parameter.\n\nUse $N=4$ interior grid points, so that $h = \\frac{1}{N+1}$. Discretize the convective term $a \\, \\frac{d u}{d x}$ at interior node $i$ with first-order upwind differencing valid for $a>0$, namely $\\frac{u_{i} - u_{i-1}}{h}$, and discretize the diffusive term $-\\nu \\, \\frac{d^{2} u}{d x^{2}}$ by the centered second difference $\\frac{u_{i+1}-2 u_i + u_{i-1}}{h^{2}}$. Incorporate the stabilization by replacing $\\nu$ with the total diffusion $\\nu_{\\text{tot}} = \\nu + \\nu_{\\text{art}}$ in the discrete diffusion term. Assume homogeneous Dirichlet boundary conditions so that any stencil entries involving $u_{0}$ or $u_{N+1}$ drop out of the matrix $A$ and do not contribute unknowns.\n\nAdvanced iterative solvers for large-scale systems in aerospace computational fluid dynamics often employ Krylov subspace methods such as the Generalized Minimal Residual (GMRES) method, whose convergence behavior is sensitive to the non-normality of the operator. One way to quantify non-normality is the Frobenius-norm departure from normality given by $\\delta(A) = \\|A^{\\top} A - A A^{\\top}\\|_{F}$, where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm and $A^{\\top}$ denotes the transpose.\n\nUsing $a=3$, $\\nu=0.01$, and $\\gamma=0.5$, construct the matrix $A$ and compute the value of $\\delta(A)$. Round your final answer to four significant figures and express it as a dimensionless number.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Operator**: $L u = a \\, \\frac{d u}{d x} - \\nu \\, \\frac{d^{2} u}{d x^{2}}$\n- **Domain**: $[0,1]$\n- **Boundary Conditions**: Homogeneous Dirichlet, $u(0)=0$ and $u(1)=0$.\n- **System**: Discrete linear system $A \\mathbf{u} = \\mathbf{f}$.\n- **Artificial Diffusion**: $\\nu_{\\text{art}}=\\gamma \\, a \\, h$, where $a>0$.\n- **Total Diffusion**: $\\nu_{\\text{tot}} = \\nu + \\nu_{\\text{art}}$.\n- **Grid**: $N=4$ interior grid points, uniform grid spacing $h = \\frac{1}{N+1}$.\n- **Discretization (Convection)**: First-order upwind for $a>0$, $a \\, \\frac{d u}{d x} \\approx a \\frac{u_{i} - u_{i-1}}{h}$ at node $i$.\n- **Discretization (Diffusion)**: Centered second difference, $-\\nu \\, \\frac{d^{2} u}{d x^{2}} \\approx -(\\nu_{\\text{tot}}) \\frac{u_{i+1}-2 u_i + u_{i-1}}{h^{2}}$.\n- **Boundary Handling**: Stencil entries involving $u_{0}$ or $u_{N+1}$ are zero.\n- **Quantity to Compute**: Frobenius-norm departure from normality, $\\delta(A) = \\|A^{\\top} A - A A^{\\top}\\|_{F}$.\n- **Parameters**: $a=3$, $\\nu=0.01$, $\\gamma=0.5$.\n- **Output Requirement**: Round the final answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes the one-dimensional steady convection-diffusion equation, a fundamental model in fluid dynamics. The discretization methods (first-order upwind for convection, second-order central for diffusion) are standard in computational fluid dynamics (CFD). The concept of artificial diffusion is a common technique for stabilizing numerical schemes, particularly for convection-dominated problems. The matrix non-normality and its quantification using the Frobenius norm are standard topics in numerical linear algebra, directly relevant to the convergence of Krylov subspace methods like GMRES used in CFD. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem provides all necessary data and a clear, unambiguous procedure to construct the matrix $A$ and compute $\\delta(A)$. With a finite number of grid points ($N=4$), the matrix is small and the calculation is well-defined and leads to a unique solution.\n- **Objective**: The problem is stated in precise, objective mathematical and technical language, free from any subjective or opinion-based claims.\n- **Completeness**: All required numerical values ($a, \\nu, \\gamma, N$), boundary conditions, and discretization schemes are specified. The problem is self-contained.\n- **Realism**: The context is realistic, and the formulation, while simplified, reflects genuine issues encountered in aerospace CFD. The parameters are for an academic exercise, which is appropriate.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically grounded, well-posed, objective, and complete. A detailed solution will be provided.\n\n## Solution Derivation\n\nThe problem requires the construction of a matrix $A$ representing the discretization of the one-dimensional convection-diffusion operator and the computation of its departure from normality.\n\nFirst, we calculate the necessary parameters.\nThe number of interior grid points is $N=4$. The uniform grid spacing $h$ is:\n$$h = \\frac{1}{N+1} = \\frac{1}{4+1} = \\frac{1}{5} = 0.2$$\nThe given parameters are the convection speed $a=3$, the physical diffusion coefficient $\\nu=0.01$, and the stabilization parameter $\\gamma=0.5$.\nThe artificial diffusion $\\nu_{\\text{art}}$ is:\n$$\\nu_{\\text{art}} = \\gamma \\, a \\, h = (0.5)(3)(0.2) = 0.3$$\nThe total diffusion coefficient $\\nu_{\\text{tot}}$ is the sum of the physical and artificial diffusion:\n$$\\nu_{\\text{tot}} = \\nu + \\nu_{\\text{art}} = 0.01 + 0.3 = 0.31$$\n\nNext, we establish the discrete equation at an interior grid point $i$, for $i=1, \\dots, N$. The discrete operator $L_h$ applied to the grid function $u$ at point $i$ is given by:\n$$ (A\\mathbf{u})_i = a \\frac{u_i - u_{i-1}}{h} - \\nu_{\\text{tot}} \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} $$\nTo find the entries of the matrix $A$, we regroup the terms by the indices of $u$:\n$$ (A\\mathbf{u})_i = \\left(-\\frac{a}{h} - \\frac{\\nu_{\\text{tot}}}{h^2}\\right) u_{i-1} + \\left(\\frac{a}{h} + \\frac{2\\nu_{\\text{tot}}}{h^2}\\right) u_i + \\left(-\\frac{\\nu_{\\text{tot}}}{h^2}\\right) u_{i+1} $$\nThis shows that the matrix $A$ is tridiagonal. Let's calculate the values of the coefficients for the diagonals:\nLower diagonal element, $d_l$:\n$$d_l = -\\frac{a}{h} - \\frac{\\nu_{\\text{tot}}}{h^2} = -\\frac{3}{0.2} - \\frac{0.31}{(0.2)^2} = -15 - \\frac{0.31}{0.04} = -15 - 7.75 = -22.75$$\nMain diagonal element, $d_m$:\n$$d_m = \\frac{a}{h} + \\frac{2\\nu_{\\text{tot}}}{h^2} = \\frac{3}{0.2} + 2\\frac{0.31}{(0.2)^2} = 15 + 2(7.75) = 15 + 15.5 = 30.5$$\nUpper diagonal element, $d_u$:\n$$d_u = -\\frac{\\nu_{\\text{tot}}}{h^2} = -\\frac{0.31}{(0.2)^2} = -7.75$$\n\nThe matrix $A$ is a $4 \\times 4$ matrix for the $N=4$ interior points. The boundary conditions $u_0=0$ and $u_{N+1}=u_5=0$ mean that the stencil for $i=1$ does not involve $u_0$ and the stencil for $i=4$ does not involve $u_5$ in the matrix structure. The resulting matrix $A$ is:\n$$\nA = \\begin{pmatrix}\nd_m & d_u & 0 & 0 \\\\\nd_l & d_m & d_u & 0 \\\\\n0 & d_l & d_m & d_u \\\\\n0 & 0 & d_l & d_m\n\\end{pmatrix} = \\begin{pmatrix}\n30.5 & -7.75 & 0 & 0 \\\\\n-22.75 & 30.5 & -7.75 & 0 \\\\\n0 & -22.75 & 30.5 & -7.75 \\\\\n0 & 0 & -22.75 & 30.5\n\\end{pmatrix}\n$$\nThe departure from normality is $\\delta(A) = \\|A^{\\top} A - A A^{\\top}\\|_{F}$. Let $C = A^{\\top} A - A A^{\\top}$. Since the off-diagonal elements of the commutator matrix $C$ are zero for a tridiagonal matrix, the commutator matrix $C$ is a diagonal matrix:\n$$ C = \\begin{pmatrix} d_l^2 - d_u^2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & d_u^2 - d_l^2 \\end{pmatrix} $$\nLet's compute the value of the non-zero element:\n$$ d_l^2 - d_u^2 = (-22.75)^2 - (-7.75)^2 = 517.5625 - 60.0625 = 457.5 $$\nSo, the matrix $C$ is:\n$$ C = \\begin{pmatrix} 457.5 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -457.5 \\end{pmatrix} $$\nThe Frobenius norm of $C$ is the square root of the sum of the squares of its elements:\n$$ \\delta(A) = \\|C\\|_F = \\sqrt{\\sum_{i,j=1}^{4} |C_{ij}|^2} = \\sqrt{(457.5)^2 + 0^2 + \\dots + (-457.5)^2} $$\n$$ \\delta(A) = \\sqrt{(457.5)^2 + (-457.5)^2} = \\sqrt{2 \\times (457.5)^2} = 457.5\\sqrt{2} $$\nNow, we compute the numerical value:\n$$ \\delta(A) \\approx 457.5 \\times 1.41421356 \\approx 647.00966 $$\nThe problem asks for the answer to be rounded to four significant figures.\n$$ \\delta(A) \\approx 647.0 $$\nThe problem is posed with dimensionless parameters over a dimensionless domain $[0,1]$, so the matrix entries and the resulting departure from normality are also dimensionless.",
            "answer": "$$\\boxed{647.0}$$"
        },
        {
            "introduction": "After identifying challenging spectral properties in a system matrix, the next step is to accelerate the convergence of the iterative solver. This exercise demonstrates the power of polynomial preconditioning, a technique that reshapes the spectrum of an operator without the cost of forming and inverting a separate preconditioner matrix. Focusing on a symmetric positive definite system, a common case in pressure-correction steps, you will derive and apply an optimal preconditioner based on the minimax properties of Chebyshev polynomials, providing insight into how spectral manipulation can dramatically improve solver performance. ",
            "id": "3970309",
            "problem": "In a steady, incompressible Computational Fluid Dynamics (CFD) simulation for an aerospace configuration, a segregated pressure-correction step produces a symmetric positive definite linear system $A u = f$, where $A \\in \\mathbb{R}^{n \\times n}$ arises from a second-order finite-volume discretization of an elliptic operator on a hexahedral mesh. Spectral analysis and mesh-independent estimates yield that the spectrum of $A$ lies in an interval $[\\alpha,\\beta]$ with $0 < \\alpha < \\beta$, and there are no eigenvalues outside this interval.\n\nConsider left-preconditioning with a polynomial preconditioner $p(A)$ of degree at most $m-1$, so that the preconditioned operator is $B = p(A) A$. The Generalized Minimal Residual (GMRES) method applied to $B u = p(A) f$ chooses a residual polynomial $r_{k}$ of degree $k$ with $r_{k}(0)=1$ to minimize the residual $\\|r_{k}(B) r_{0}\\|$ at each iteration, where $r_{0}$ is the initial residual. The goal of the polynomial preconditioner is to cluster the spectrum of $B$ near $1$ by minimizing the worst-case deviation $|1 - \\lambda p(\\lambda)|$ over $\\lambda \\in [\\alpha,\\beta]$.\n\nStarting from the following foundational facts:\n- The GMRES residual after $k$ steps has the form $r_{k}(B) r_{0}$ with $r_{k}$ a degree-$k$ polynomial satisfying $r_{k}(0)=1$.\n- The Chebyshev polynomials of the first kind, $T_{m}$, defined by $T_{m}(\\cos\\theta)=\\cos(m\\theta)$, have the minimax property on $[-1,1]$: among all monic degree-$m$ polynomials scaled to match a fixed value at a point $x_{0} \\notin [-1,1]$, the magnitude of $T_{m}$ is minimized in the uniform norm on $[-1,1]$.\n\nTasks:\n1. Derive, from the minimax property, the optimal residual polynomial $r_{m}(\\lambda)$ of degree $m$ with $r_{m}(0)=1$ that minimizes $\\max_{\\lambda \\in [\\alpha,\\beta]} |r_{m}(\\lambda)|$. Use an affine map from $[\\alpha,\\beta]$ to $[-1,1]$ to express $r_{m}$ in terms of $T_{m}$, and then derive the associated polynomial preconditioner $p_{m-1}(\\lambda)$ defined by $1 - \\lambda p_{m-1}(\\lambda) = r_{m}(\\lambda)$.\n2. Using your derived formulas, take $\\alpha = \\tfrac{1}{5}$, $\\beta = 5$, and $m=3$. Compute explicitly the resulting quadratic preconditioner $p_{2}(\\lambda)$ as a closed-form analytic expression in $\\lambda$ with rational coefficients. Express the final answer as a simplified polynomial in $\\lambda$. Do not round.\n\nAdditionally, in your derivation, explain qualitatively how the constructed $p(A)$ damps undesirable spectral components and how this clustering of the spectrum of $B$ affects the convergence behavior of GMRES in this CFD context.\n\nYour final answer must be the explicit expression for $p_{2}(\\lambda)$.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It poses a standard, solvable problem in numerical linear algebra concerning polynomial preconditioning, using established principles and clear, formal language.\n\nThe overall task is to derive an optimal polynomial preconditioner based on Chebyshev polynomials and then compute a specific instance of it. The preconditioning strategy aims to improve the convergence of the GMRES method by clustering the spectrum of the preconditioned operator.\n\n### Part 1: Derivation of the Optimal Preconditioner\n\nThe problem is to find a polynomial $p_{m-1}(\\lambda)$ of degree at most $m-1$ that minimizes the quantity $\\max_{\\lambda \\in [\\alpha,\\beta]} |1 - \\lambda p_{m-1}(\\lambda)|$. Let $r_m(\\lambda) = 1 - \\lambda p_{m-1}(\\lambda)$. This $r_m(\\lambda)$ is a polynomial of degree at most $m$. The condition $r_m(0) = 1 - 0 \\cdot p_{m-1}(0) = 1$ is automatically satisfied. Thus, the problem is equivalent to finding the polynomial $r_m(\\lambda)$ of degree $m$ that satisfies $r_m(0)=1$ and minimizes the uniform norm $\\|r_m\\|_{L_{\\infty}([\\alpha,\\beta])} = \\max_{\\lambda \\in [\\alpha,\\beta]} |r_m(\\lambda)|$.\n\nThis is a classic minimax problem. The solution is known to be a scaled and shifted Chebyshev polynomial of the first kind, $T_m(x)$. The key property of $T_m(x)$ is that, of all polynomials of degree $m$ with a leading coefficient of $2^{m-1}$, it has the minimal $L_{\\infty}$ norm on $[-1,1]$, which is $1$. Equivalently, for a point $x_0$ outside $[-1,1]$, the polynomial $r(x) = T_m(x) / T_m(x_0)$ minimizes $\\max_{x\\in[-1,1]} |r(x)|$ among all polynomials of degree $m$ satisfying $r(x_0)=1$.\n\nTo apply this property, we must first map the spectral interval $[\\alpha,\\beta]$ to the interval $[-1,1]$ using an affine transformation. Let this map be $x = f(\\lambda) = c\\lambda+d$. We require $f(\\alpha)=-1$ and $f(\\beta)=1$:\n$$\n\\begin{cases}\nc\\alpha + d = -1 \\\\\nc\\beta + d = 1\n\\end{cases}\n$$\nSolving this system for $c$ and $d$ yields:\n$$c(\\beta-\\alpha) = 2 \\implies c = \\frac{2}{\\beta-\\alpha}$$\n$$d = 1 - c\\beta = 1 - \\frac{2\\beta}{\\beta-\\alpha} = \\frac{\\beta-\\alpha-2\\beta}{\\beta-\\alpha} = -\\frac{\\alpha+\\beta}{\\beta-\\alpha}$$\nSo, the affine map is:\n$$x(\\lambda) = \\frac{2\\lambda - (\\alpha+\\beta)}{\\beta-\\alpha}$$\nOur constraint is at $\\lambda=0$, which is outside the interval $[\\alpha,\\beta]$ since $0 < \\alpha$. The point $\\lambda=0$ maps to:\n$$x_0 = x(0) = \\frac{-(\\alpha+\\beta)}{\\beta-\\alpha}$$\nThe optimal residual polynomial $r_m(\\lambda)$ that minimizes its maximum magnitude on $[\\alpha,\\beta]$ subject to $r_m(0)=1$ is given by scaling the Chebyshev polynomial $T_m(x)$ such that its value is $1$ at the mapped point $x_0$.\n$$r_m(\\lambda) = \\frac{T_m(x(\\lambda))}{T_m(x_0)} = \\frac{T_m\\left(\\frac{2\\lambda - (\\alpha+\\beta)}{\\beta-\\alpha}\\right)}{T_m\\left(\\frac{-(\\alpha+\\beta)}{\\beta-\\alpha}\\right)}$$\nSince $T_m(-x) = (-1)^m T_m(x)$, the denominator can also be written as $(-1)^m T_m\\left(\\frac{\\alpha+\\beta}{\\beta-\\alpha}\\right)$.\n\nFrom the definition $r_m(\\lambda) = 1 - \\lambda p_{m-1}(\\lambda)$, we can solve for the polynomial preconditioner $p_{m-1}(\\lambda)$:\n$$p_{m-1}(\\lambda) = \\frac{1 - r_m(\\lambda)}{\\lambda} = \\frac{1}{\\lambda}\\left(1 - \\frac{T_m\\left(\\frac{2\\lambda - (\\alpha+\\beta)}{\\beta-\\alpha}\\right)}{T_m\\left(\\frac{-(\\alpha+\\beta)}{\\beta-\\alpha}\\right)}\\right)$$\nThe numerator, when evaluated at $\\lambda=0$, is $1 - T_m(x(0))/T_m(x(0)) = 0$. This confirms that the expression is divisible by $\\lambda$ and $p_{m-1}(\\lambda)$ is indeed a polynomial of degree $m-1$.\n\n### Qualitative Effect on GMRES Convergence\n\nThe polynomial preconditioner $p(A)$ transforms the original linear system $Au=f$ into $p(A)Au = p(A)f$. The new system matrix is $B=p(A)A$. If $\\lambda_i$ is an eigenvalue of $A$, then the corresponding eigenvalue of $B$ is $\\mu_i = \\lambda_i p(\\lambda_i)$.\nOur construction aims to make $\\lambda p(\\lambda)$ as close to $1$ as possible for all $\\lambda$ in the spectrum of $A$, i.e., for $\\lambda \\in [\\alpha, \\beta]$.\nThe polynomial $q(\\lambda) = \\lambda p_{m-1}(\\lambda)$ is constructed as $q(\\lambda) = 1 - r_m(\\lambda)$. The magnitude of $r_m(\\lambda)$ on $[\\alpha, \\beta]$ is bounded by:\n$$\\rho_m = \\max_{\\lambda \\in [\\alpha,\\beta]} |r_m(\\lambda)| = \\frac{\\max_{x \\in [-1,1]} |T_m(x)|}{\\left|T_m\\left(\\frac{-(\\alpha+\\beta)}{\\beta-\\alpha}\\right)\\right|} = \\frac{1}{\\left|T_m\\left(\\frac{\\alpha+\\beta}{\\beta-\\alpha}\\right)\\right|}$$\nThis means that for any eigenvalue $\\lambda_i$ of $A$, the corresponding eigenvalue $\\mu_i = \\lambda_i p_{m-1}(\\lambda_i)$ of $B$ lies in the interval $[1-\\rho_m, 1+\\rho_m]$. The preconditioner thus maps the original, possibly wide, spectral interval $[\\alpha, \\beta]$ to a much smaller interval centered at $1$.\n\nThe convergence rate of GMRES is highly dependent on the distribution of eigenvalues. By clustering the eigenvalues of the preconditioned matrix $B$ tightly around $1$, the effective condition number of the system is greatly reduced from $\\kappa(A) \\approx \\beta/\\alpha$ to $\\kappa(B) \\approx \\frac{1+\\rho_m}{1-\\rho_m}$. A smaller condition number and a more clustered spectrum allow the GMRES algorithm to find a low-degree residual polynomial that is small across the entire spectrum, leading to a much faster reduction in the residual norm and, consequently, faster convergence. The polynomial preconditioner acts as a spectral filter, damping the components of the residual corresponding to eigenvalues that slow down convergence.\n\n### Part 2: Explicit Computation for $p_2(\\lambda)$\n\nWe are given $\\alpha = \\frac{1}{5}$, $\\beta = 5$, and $m=3$. We seek the quadratic preconditioner $p_2(\\lambda)$.\n\nFirst, we compute the parameters for the affine map and its evaluation at $\\lambda=0$:\n$$\\alpha+\\beta = \\frac{1}{5} + 5 = \\frac{26}{5}$$\n$$\\beta-\\alpha = 5 - \\frac{1}{5} = \\frac{24}{5}$$\nThe map from $\\lambda$ to $x$ is:\n$$x(\\lambda) = \\frac{2\\lambda - 26/5}{24/5} = \\frac{10\\lambda - 26}{24} = \\frac{5\\lambda - 13}{12}$$\nThe point $x_0$ corresponding to $\\lambda=0$ is:\n$$x_0 = x(0) = -\\frac{13}{12}$$\nWe need the Chebyshev polynomial of degree $m=3$, which is $T_3(x) = 4x^3 - 3x$.\nNext, we evaluate $T_3(x_0)$:\n$$T_3\\left(-\\frac{13}{12}\\right) = 4\\left(-\\frac{13}{12}\\right)^3 - 3\\left(-\\frac{13}{12}\\right) = 4\\left(-\\frac{2197}{1728}\\right) + \\frac{39}{12}$$\n$$= -\\frac{2197}{432} + \\frac{39 \\times 36}{12 \\times 36} = \\frac{-2197 + 1404}{432} = -\\frac{793}{432}$$\nThe optimal residual polynomial of degree $3$ is:\n$$r_3(\\lambda) = \\frac{T_3\\left(\\frac{5\\lambda-13}{12}\\right)}{T_3\\left(-\\frac{13}{12}\\right)} = -\\frac{432}{793} T_3\\left(\\frac{5\\lambda-13}{12}\\right)$$\nWe now find the preconditioner $p_2(\\lambda) = \\frac{1-r_3(\\lambda)}{\\lambda}$:\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(1 + \\frac{432}{793} T_3\\left(\\frac{5\\lambda-13}{12}\\right)\\right)$$\nLet's expand $T_3\\left(\\frac{5\\lambda-13}{12}\\right)$:\n$$T_3\\left(\\frac{5\\lambda-13}{12}\\right) = 4\\left(\\frac{5\\lambda-13}{12}\\right)^3 - 3\\left(\\frac{5\\lambda-13}{12}\\right)$$\n$$= \\frac{4(5\\lambda-13)^3}{1728} - \\frac{3(5\\lambda-13)}{12} = \\frac{(5\\lambda-13)^3}{432} - \\frac{108(5\\lambda-13)}{432}$$\nThe numerator is $(5\\lambda-13)^3 - 108(5\\lambda-13)$. Expanding the cubic term:\n$$(5\\lambda-13)^3 = (5\\lambda)^3 - 3(5\\lambda)^2(13) + 3(5\\lambda)(13^2) - 13^3 = 125\\lambda^3 - 975\\lambda^2 + 2535\\lambda - 2197$$\nSo the numerator becomes:\n$$(125\\lambda^3 - 975\\lambda^2 + 2535\\lambda - 2197) - (540\\lambda - 1404) = 125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793$$\nThus,\n$$T_3\\left(\\frac{5\\lambda-13}{12}\\right) = \\frac{125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793}{432}$$\nSubstituting this into the expression for $p_2(\\lambda)$:\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(1 + \\frac{432}{793} \\left(\\frac{125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793}{432}\\right)\\right)$$\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(1 + \\frac{125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793}{793}\\right)$$\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(\\frac{793 + 125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793}{793}\\right)$$\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(\\frac{125\\lambda^3 - 975\\lambda^2 + 1995\\lambda}{793}\\right)$$\nDividing by $\\lambda$, we obtain the final quadratic polynomial:\n$$p_2(\\lambda) = \\frac{125\\lambda^2 - 975\\lambda + 1995}{793}$$\nThe coefficients can be simplified. The prime factorization of the denominator is $793 = 13 \\times 61$. We check the numerator terms for these factors.\n$125 = 5^3$.\n$975 = 75 \\times 13$.\n$1995 = 5 \\times 399 = 5 \\times 3 \\times 133 = 5 \\times 3 \\times 7 \\times 19$.\nOnly the middle term has a factor of $13$.\n$$p_2(\\lambda) = \\frac{125}{793}\\lambda^2 - \\frac{975}{793}\\lambda + \\frac{1995}{793} = \\frac{125}{793}\\lambda^2 - \\frac{75 \\times 13}{61 \\times 13}\\lambda + \\frac{1995}{793}$$\nThe simplified polynomial is:\n$$p_2(\\lambda) = \\frac{125}{793}\\lambda^2 - \\frac{75}{61}\\lambda + \\frac{1995}{793}$$\nThis is the desired closed-form analytic expression for the preconditioner.",
            "answer": "$$ \\boxed{ \\frac{125}{793}\\lambda^2 - \\frac{75}{61}\\lambda + \\frac{1995}{793} } $$"
        },
        {
            "introduction": "A robust Krylov subspace solver is not just about the theory, but also about the stability of its implementation. At the heart of the GMRES method lies the Arnoldi process, which relies on orthogonalizing a sequence of vectors to build the Krylov basis. This conceptual practice challenges you to analyze the numerical stability of two algebraically equivalent algorithms for this task: Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS). By reasoning from first principles of floating-point arithmetic, you will uncover why MGS is essential for a reliable GMRES implementation when dealing with the nearly linearly dependent vectors that often arise in CFD. ",
            "id": "3970321",
            "problem": "Consider solving the sparse, non-symmetric linear system $A x = b$ arising from a Jacobian linearization of the compressible Navier–Stokes equations discretized by a finite-volume scheme with an upwind numerical flux in Computational Fluid Dynamics (CFD). To solve $A x = b$, you employ restarted Generalized Minimal Residual (GMRES) with restart parameter $m$, which builds a Krylov basis via the Arnoldi process. The Arnoldi relation in exact arithmetic reads\n$$\nA V_m = V_{m+1} \\,\\bar{H}_m,\n$$\nwhere $V_{m+1} \\in \\mathbb{R}^{n \\times (m+1)}$ has orthonormal columns and $\\bar{H}_m \\in \\mathbb{R}^{(m+1)\\times m}$ is upper Hessenberg. The orthonormalization step within Arnoldi can be performed by either Classical Gram–Schmidt (CGS) or Modified Gram–Schmidt (MGS).\n\nAssume standard floating point arithmetic with unit roundoff $u$, and the basic model that for any scalar operation or inner product involving $p$ terms, the computed result satisfies a relative error bounded by $c\\,p\\,u$ for a modest constant $c$, and that each saxpy update incurs a relative error bounded by a modest constant multiple of $u$. Assume also that $A$ is strongly non-normal, so the Krylov basis vectors can become nearly linearly dependent as $m$ grows.\n\nUsing only the projection definition of Gram–Schmidt, namely that orthogonalization of a vector $w$ against the current basis $Q_{k-1}$ applies the projector $I - Q_{k-1}Q_{k-1}^\\top$, and the above floating point model, reason about the effect of using CGS versus MGS inside Arnoldi on the computed orthogonality of $V_m$ and the convergence behavior of restarted GMRES$(m)$ for such an $A$. Select all statements that are correct.\n\n- A. In exact arithmetic, CGS and MGS yield the same Arnoldi decomposition, that is, they both produce $V_{m+1}$ with orthonormal columns and the same $\\bar{H}_m$, because both implement the same projection $I - Q_{k-1}Q_{k-1}^\\top$ at each step.\n\n- B. In finite precision, CGS without reorthogonalization can suffer a loss of orthogonality in $V_m$ that grows with the conditioning of the evolving basis, and for a strongly non-normal $A$ this can lead to premature stagnation of GMRES residual norms.\n\n- C. MGS, even without reorthogonalization, guarantees exact orthogonality of $V_m$ up to machine precision regardless of the non-normality of $A$.\n\n- D. Performing a single reorthogonalization pass (that is, applying MGS twice) typically reduces the loss of orthogonality in $V_m$ to the level of unit roundoff $u$, and is usually sufficient in practice for restarted GMRES on non-normal matrices arising in CFD.\n\n- E. In Arnoldi, any loss of orthogonality in $V_m$ only rotates the basis within the same subspace, so Ritz value approximations and GMRES convergence are unaffected by such loss of orthogonality.\n\nChoose all that apply and justify your selection based on first principles of orthogonal projection and the floating point error model described above, without appealing to any unproved shortcut formulas.",
            "solution": "The user wants a critical validation of the problem statement, followed by a detailed solution deriving the correct answer and evaluating each option.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- The problem is to solve a sparse, non-symmetric linear system $A x = b$.\n- The system arises from a Jacobian linearization of the compressible Navier–Stokes equations discretized by a finite-volume scheme with an upwind numerical flux.\n- The solution method is restarted GMRES$(m)$.\n- The Krylov basis is built using the Arnoldi process.\n- The Arnoldi relation in exact arithmetic is $A V_m = V_{m+1} \\,\\bar{H}_m$.\n- $V_{m+1} \\in \\mathbb{R}^{n \\times (m+1)}$ has orthonormal columns.\n- $\\bar{H}_m \\in \\mathbb{R}^{(m+1)\\times m}$ is upper Hessenberg.\n- Orthogonalization in Arnoldi can be done by Classical Gram–Schmidt (CGS) or Modified Gram–Schmidt (MGS).\n- A floating point model is assumed with unit roundoff $u$.\n- The computed result of a scalar operation or an inner product of $p$ terms has a relative error bounded by $c\\,p\\,u$ for a modest constant $c$.\n- Each `saxpy` update incurs a relative error bounded by a modest constant multiple of $u$.\n- The matrix $A$ is strongly non-normal.\n- Krylov basis vectors can become nearly linearly dependent as $m$ grows.\n- The Gram-Schmidt process is defined as applying the projector $I - Q_{k-1}Q_{k-1}^\\top$ to orthogonalize a vector $w$ against a basis $Q_{k-1}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the field of numerical linear algebra and its application to Computational Fluid Dynamics (CFD). The scenario described—solving a non-symmetric system from a CFD discretization using GMRES with the Arnoldi process—is a standard and fundamentally important problem in scientific computing. The distinction between CGS and MGS and their stability properties are classical topics in numerical analysis. The matrix properties (strong non-normality) are characteristic of systems arising from advection-dominated fluid dynamics problems.\n- **Well-Posed:** The problem provides a clear context, specifies the algorithms (CGS, MGS), defines the mathematical relationship (Arnoldi relation), provides a simplified but standard floating point error model, and asks for a qualitative analysis of the numerical behavior. The information is sufficient and self-contained to allow a rigorous line of reasoning based on established principles of numerical analysis.\n- **Objective:** The problem is phrased in precise, technical language. It asks for reasoning based on a given model, avoiding any subjective or ambiguous terminology.\n\nThe problem does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, ambiguity). It represents a standard, non-trivial question in advanced numerical methods.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. The solution process will now proceed.\n\n**Solution Derivation**\n\nThe core of the problem lies in the numerical stability of two different implementations of the Gram-Schmidt orthogonalization process within Arnoldi, when applied to a set of vectors that are nearly linearly dependent. Let a set of orthonormal vectors $\\{v_1, \\dots, v_{k-1}\\}$ forming the columns of $V_{k-1}$ be available. The next step of the Arnoldi process is to compute a vector $w = A v_{k-1}$ and then orthogonalize it against $V_{k-1}$. Let this vector to be orthogonalized be denoted $z$. In exact arithmetic, the goal is to compute $z_{\\perp} = (I - V_{k-1}V_{k-1}^\\top)z$.\n\n**Classical Gram-Schmidt (CGS)** implements this projection as follows:\n$1.$ Compute all inner products: $h_{j, k-1} = v_j^\\top z$ for $j = 1, \\dots, k-1$.\n$2.$ Subtract the projection in one step: $z_{\\perp} = z - \\sum_{j=1}^{k-1} h_{j, k-1} v_j$.\nThis can be written compactly as $z_{\\perp} = z - V_{k-1}(V_{k-1}^\\top z)$.\n\n**Modified Gram-Schmidt (MGS)** implements the same projection sequentially:\nInitialize $z^{(0)} = z$.\nFor $j=1, \\dots, k-1$:\n$1.$ Compute inner product: $h_{j, k-1} = v_j^\\top z^{(j-1)}$.\n$2.$ Subtract one component: $z^{(j)} = z^{(j-1)} - h_{j, k-1} v_j$.\nThe final result is $z_{\\perp} = z^{(k-1)}$.\n\nIn finite-precision arithmetic, with unit roundoff $u$, these two algorithms behave very differently. The problem states that the raw Krylov basis vectors can become nearly linearly dependent for a strongly non-normal matrix $A$. This means that the vector $z = A v_{k-1}$ will lie almost entirely in the subspace spanned by $\\{v_1, \\dots, v_{k-1}\\}$. Therefore, its orthogonal component $z_{\\perp}$ will be very small in magnitude compared to $z$.\n\nWith CGS, we compute $\\hat{z}_{\\perp} = \\text{fl}(z - \\sum_{j=1}^{k-1} \\text{fl}(v_j^\\top z) v_j)$. The term $\\sum \\text{fl}(v_j^\\top z) v_j$ is the computed projection of $z$ onto $\\text{span}(V_{k-1})$, and its magnitude is very close to the magnitude of $z$. The subtraction is of the form \"large number - nearly equal large number\", which is a classic cause of catastrophic cancellation. The roundoff errors in computing the inner products and the final subtraction, which are small relative to $\\|z\\|$, can become very large relative to the true result $\\|z_{\\perp}\\|$. The computed vector $\\hat{z}_{\\perp}$ can retain significant components along the previous $v_j$ vectors, leading to a severe loss of orthogonality. The loss of orthogonality $\\|I - \\hat{V}_k^\\top \\hat{V}_k\\|$ can be shown to be proportional to $u \\cdot \\kappa(\\mathcal{K}_k)$, where $\\kappa(\\mathcal{K}_k)$ is the condition number of the matrix whose columns are the raw Krylov vectors, which can be very large.\n\nWith MGS, at each step $j$, we compute $\\hat{z}^{(j)} = \\text{fl}(\\hat{z}^{(j-1)} - \\text{fl}(v_j^\\top \\hat{z}^{(j-1)}) v_j)$. The vector $\\hat{z}^{(j-1)}$ has already been made nearly orthogonal to $\\{v_1, \\dots, v_{j-1}\\}$. The sequential removal of components is more numerically stable than the single-step removal in CGS. While MGS also suffers from some loss of orthogonality, it is far less severe. The resulting computed vectors are nearly orthogonal in the sense that $|v_i^\\top \\hat{v}_j| = O(u)$ for $i < j$, which is a much better property than what CGS provides.\n\nThis loss of orthogonality directly impacts GMRES. The GMRES algorithm relies on the Arnoldi relation $AV_m = V_{m+1}\\bar{H}_m$ to build a small-size least squares problem $\\| \\beta e_1 - \\bar{H}_m y \\|_2$, whose solution gives the next iterate. If the columns of the computed basis $\\hat{V}_m$ are not orthogonal, this relation holds only with a large error term: $A\\hat{V}_m = \\hat{V}_{m+1}\\hat{\\bar{H}}_m + F_m$. Consequently, the residual of the small problem no longer accurately reflects the norm of the true residual, $\\|b-Ax_m\\|_2$. This can cause the algorithm to falsely detect stagnation and terminate or restart, a phenomenon known as premature stagnation.\n\n**Option-by-Option Analysis**\n\n- **A. In exact arithmetic, CGS and MGS yield the same Arnoldi decomposition, that is, they both produce $V_{m+1}$ with orthonormal columns and the same $\\bar{H}_m$, because both implement the same projection $I - Q_{k-1}Q_{k-1}^\\top$ at each step.**\n  - **Justification:** In exact arithmetic, CGS and MGS are algebraically equivalent. Both processes compute the component of a vector orthogonal to a given subspace. CGS computes the projection $Pz = \\sum (v_j^\\top z) v_j$ and subtracts it, $z_{\\perp} = z - Pz$. MGS applies a sequence of projectors $z_{\\perp} = (I - v_{k-1}v_{k-1}^\\top) \\cdots (I - v_1v_1^\\top)z$. Since the $v_j$ are orthonormal, these two procedures produce the identical result. Therefore, starting from the same initial vector, they will produce the same orthonormal basis $V_{m+1}$ and the same Hessenberg matrix $\\bar{H}_m$. The reasoning provided in the option is correct.\n  - **Verdict:** Correct.\n\n- **B. In finite precision, CGS without reorthogonalization can suffer a loss of orthogonality in $V_m$ that grows with the conditioning of the evolving basis, and for a strongly non-normal $A$ this can lead to premature stagnation of GMRES residual norms.**\n  - **Justification:** As derived above, CGS is numerically unstable when the vectors to be orthogonalized are nearly linearly dependent. This is precisely the case for a strongly non-normal matrix $A$. The loss of orthogonality in the computed basis $\\hat{V}_m$ grows with the condition number of the raw Krylov basis. This loss of orthogonality invalidates the assumptions underlying the GMRES residual update, causing the computed residual norm to diverge from the true residual norm, which can lead to premature stagnation of the algorithm. This statement accurately describes the well-known failure mode of GMRES with CGS.\n  - **Verdict:** Correct.\n\n- **C. MGS, even without reorthogonalization, guarantees exact orthogonality of $V_m$ up to machine precision regardless of the non-normality of $A$.**\n  - **Justification:** This statement is too strong and therefore false. While MGS is significantly more stable than CGS, it does not guarantee orthogonality to machine precision (i.e., $\\|I - \\hat{V}_m^\\top \\hat{V}_m\\| = O(u)$) under all circumstances. The loss of orthogonality in MGS is also dependent on the conditioning of the basis being generated. For problems with very strong non-normality leading to extremely ill-conditioned Krylov bases, MGS alone may not be sufficient to maintain orthogonality to working precision. The claim that it works \"regardless of the non-normality of A\" is incorrect.\n  - **Verdict:** Incorrect.\n\n- **D. Performing a single reorthogonalization pass (that is, applying MGS twice) typically reduces the loss of orthogonality in $V_m$ to the level of unit roundoff $u$, and is usually sufficient in practice for restarted GMRES on non-normal matrices arising in CFD.**\n  - **Justification:** Reorthogonalization is the process of applying the orthogonalization procedure a second time to the computed vector. Let $\\hat{v}_{k}$ be the vector produced by one pass of MGS. It will be nearly, but not perfectly, orthogonal to the preceding basis vectors. Applying MGS again to $\\hat{v}_{k}$ computes and subtracts the small remaining components along the basis vectors. This process can be shown to drive the loss of orthogonality down to the level of machine precision, $O(u)$, as long as the initial loss was not catastrophic. This technique, often denoted MGS2, is a standard and robust practice for ensuring the stability of GMRES for challenging problems, such as those arising in CFD with strongly non-normal matrices. The statement is an accurate reflection of both theory and practice in numerical linear algebra.\n  - **Verdict:** Correct.\n\n- **E. In Arnoldi, any loss of orthogonality in $V_m$ only rotates the basis within the same subspace, so Ritz value approximations and GMRES convergence are unaffected by such loss of orthogonality.**\n  - **Justification:** This statement is fundamentally false. A rotation is an orthogonal transformation and would preserve orthonormality, which is the opposite of what is being described. Loss of orthogonality means the computed basis vectors $\\hat{v}_j$ are no longer orthogonal to each other. This has profound negative consequences. The computed subspace may not even correctly represent the true Krylov subspace. The Arnoldi relation breaks down, causing the Ritz values (eigenvalues of $\\hat{H}_m$) calculated from the corrupted Hessenberg matrix to become spurious approximations of the true eigenvalues of $A$. As explained for option B, the convergence of GMRES is severely affected, often leading to stagnation. The idea that convergence is \"unaffected\" is incorrect.\n  - **Verdict:** Incorrect.",
            "answer": "$$\\boxed{ABD}$$"
        }
    ]
}