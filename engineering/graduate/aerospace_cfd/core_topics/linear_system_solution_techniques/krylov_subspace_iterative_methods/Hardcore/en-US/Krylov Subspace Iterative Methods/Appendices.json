{
    "hands_on_practices": [
        {
            "introduction": "The convergence behavior of Krylov methods like the Generalized Minimal Residual (GMRES) method is not governed by the condition number alone, but is highly sensitive to the non-normality of the system matrix. This first practice provides a direct, hands-on link between the discretization schemes used in Computational Fluid Dynamics (CFD) and the resulting matrix properties. By constructing the matrix for a model convection-diffusion problem with an upwind scheme, you will quantify its departure from normality and gain a tangible understanding of why matrices arising from advection-dominated flows pose a challenge for iterative solvers .",
            "id": "3970343",
            "problem": "Consider the steady one-dimensional convection–diffusion operator $L u = a \\, \\frac{d u}{d x} - \\nu \\, \\frac{d^{2} u}{d x^{2}}$ on the domain $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. In computational fluid dynamics for aerospace applications, the discrete linear system $A \\mathbf{u} = \\mathbf{f}$ is obtained from $L$ by finite differences on a uniform grid. To model typical stabilization used in practice, augment the physical diffusion coefficient $\\nu$ by an artificial diffusion $\\nu_{\\text{art}}=\\gamma \\, a \\, h$, where $a>0$ is the convection speed, $h$ is the uniform grid spacing, and $\\gamma \\geq 0$ is a stabilization parameter.\n\nUse $N=4$ interior grid points, so that $h = \\frac{1}{N+1}$. Discretize the convective term $a \\, \\frac{d u}{d x}$ at interior node $i$ with first-order upwind differencing valid for $a>0$, namely $\\frac{u_{i} - u_{i-1}}{h}$, and discretize the diffusive term $-\\nu \\, \\frac{d^{2} u}{d x^{2}}$ by the centered second difference $\\frac{u_{i+1}-2 u_i + u_{i-1}}{h^{2}}$. Incorporate the stabilization by replacing $\\nu$ with the total diffusion $\\nu_{\\text{tot}} = \\nu + \\nu_{\\text{art}}$ in the discrete diffusion term. Assume homogeneous Dirichlet boundary conditions so that any stencil entries involving $u_{0}$ or $u_{N+1}$ drop out of the matrix $A$ and do not contribute unknowns.\n\nAdvanced iterative solvers for large-scale systems in aerospace computational fluid dynamics often employ Krylov subspace methods such as the Generalized Minimal Residual (GMRES) method, whose convergence behavior is sensitive to the non-normality of the operator. One way to quantify non-normality is the Frobenius-norm departure from normality given by $\\delta(A) = \\|A^{\\top} A - A A^{\\top}\\|_{F}$, where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm and $A^{\\top}$ denotes the transpose.\n\nUsing $a=3$, $\\nu=0.01$, and $\\gamma=0.5$, construct the matrix $A$ and compute the value of $\\delta(A)$. Round your final answer to four significant figures and express it as a dimensionless number.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Operator**: $L u = a \\, \\frac{d u}{d x} - \\nu \\, \\frac{d^{2} u}{d x^{2}}$\n- **Domain**: $[0,1]$\n- **Boundary Conditions**: Homogeneous Dirichlet, $u(0)=0$ and $u(1)=0$.\n- **System**: Discrete linear system $A \\mathbf{u} = \\mathbf{f}$.\n- **Artificial Diffusion**: $\\nu_{\\text{art}}=\\gamma \\, a \\, h$, where $a>0$.\n- **Total Diffusion**: $\\nu_{\\text{tot}} = \\nu + \\nu_{\\text{art}}$.\n- **Grid**: $N=4$ interior grid points, uniform grid spacing $h = \\frac{1}{N+1}$.\n- **Discretization (Convection)**: First-order upwind for $a>0$, $a \\, \\frac{d u}{d x} \\approx a \\frac{u_{i} - u_{i-1}}{h}$ at node $i$.\n- **Discretization (Diffusion)**: Centered second difference, $-\\nu \\, \\frac{d^{2} u}{d x^{2}} \\approx -(\\nu_{\\text{tot}}) \\frac{u_{i+1}-2 u_i + u_{i-1}}{h^{2}}$.\n- **Boundary Handling**: Stencil entries involving $u_{0}$ or $u_{N+1}$ are zero.\n- **Quantity to Compute**: Frobenius-norm departure from normality, $\\delta(A) = \\|A^{\\top} A - A A^{\\top}\\|_{F}$.\n- **Parameters**: $a=3$, $\\nu=0.01$, $\\gamma=0.5$.\n- **Output Requirement**: Round the final answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes the one-dimensional steady convection-diffusion equation, a fundamental model in fluid dynamics. The discretization methods (first-order upwind for convection, second-order central for diffusion) are standard in computational fluid dynamics (CFD). The concept of artificial diffusion is a common technique for stabilizing numerical schemes, particularly for convection-dominated problems. The matrix non-normality and its quantification using the Frobenius norm are standard topics in numerical linear algebra, directly relevant to the convergence of Krylov subspace methods like GMRES used in CFD. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem provides all necessary data and a clear, unambiguous procedure to construct the matrix $A$ and compute $\\delta(A)$. With a finite number of grid points ($N=4$), the matrix is small and the calculation is well-defined and leads to a unique solution.\n- **Objective**: The problem is stated in precise, objective mathematical and technical language, free from any subjective or opinion-based claims.\n- **Completeness**: All required numerical values ($a, \\nu, \\gamma, N$), boundary conditions, and discretization schemes are specified. The problem is self-contained.\n- **Realism**: The context is realistic, and the formulation, while simplified, reflects genuine issues encountered in aerospace CFD. The parameters are for an academic exercise, which is appropriate.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically grounded, well-posed, objective, and complete. A detailed solution will be provided.\n\n## Solution Derivation\n\nThe problem requires the construction of a matrix $A$ representing the discretization of the one-dimensional convection-diffusion operator and the computation of its departure from normality.\n\nFirst, we calculate the necessary parameters.\nThe number of interior grid points is $N=4$. The uniform grid spacing $h$ is:\n$$h = \\frac{1}{N+1} = \\frac{1}{4+1} = \\frac{1}{5} = 0.2$$\nThe given parameters are the convection speed $a=3$, the physical diffusion coefficient $\\nu=0.01$, and the stabilization parameter $\\gamma=0.5$.\nThe artificial diffusion $\\nu_{\\text{art}}$ is:\n$$\\nu_{\\text{art}} = \\gamma \\, a \\, h = (0.5)(3)(0.2) = 0.3$$\nThe total diffusion coefficient $\\nu_{\\text{tot}}$ is the sum of the physical and artificial diffusion:\n$$\\nu_{\\text{tot}} = \\nu + \\nu_{\\text{art}} = 0.01 + 0.3 = 0.31$$\n\nNext, we establish the discrete equation at an interior grid point $i$, for $i=1, \\dots, N$. The discrete operator $L_h$ applied to the grid function $u$ at point $i$ is given by:\n$$ (A\\mathbf{u})_i = a \\frac{u_i - u_{i-1}}{h} - \\nu_{\\text{tot}} \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} $$\nTo find the entries of the matrix $A$, we regroup the terms by the indices of $u$:\n$$ (A\\mathbf{u})_i = \\left(-\\frac{a}{h} - \\frac{\\nu_{\\text{tot}}}{h^2}\\right) u_{i-1} + \\left(\\frac{a}{h} + \\frac{2\\nu_{\\text{tot}}}{h^2}\\right) u_i + \\left(-\\frac{\\nu_{\\text{tot}}}{h^2}\\right) u_{i+1} $$\nThis shows that the matrix $A$ is tridiagonal. Let's calculate the values of the coefficients for the diagonals:\nLower diagonal element, $d_l$:\n$$d_l = -\\frac{a}{h} - \\frac{\\nu_{\\text{tot}}}{h^2} = -\\frac{3}{0.2} - \\frac{0.31}{(0.2)^2} = -15 - \\frac{0.31}{0.04} = -15 - 7.75 = -22.75$$\nMain diagonal element, $d_m$:\n$$d_m = \\frac{a}{h} + \\frac{2\\nu_{\\text{tot}}}{h^2} = \\frac{3}{0.2} + 2\\frac{0.31}{(0.2)^2} = 15 + 2(7.75) = 15 + 15.5 = 30.5$$\nUpper diagonal element, $d_u$:\n$$d_u = -\\frac{\\nu_{\\text{tot}}}{h^2} = -\\frac{0.31}{(0.2)^2} = -7.75$$\n\nThe matrix $A$ is a $4 \\times 4$ matrix for the $N=4$ interior points. The boundary conditions $u_0=0$ and $u_{N+1}=u_5=0$ mean that the stencil for $i=1$ does not involve $u_0$ and the stencil for $i=4$ does not involve $u_5$ in the matrix structure. The resulting matrix $A$ is:\n$$\nA = \\begin{pmatrix}\nd_m & d_u & 0 & 0 \\\\\nd_l & d_m & d_u & 0 \\\\\n0 & d_l & d_m & d_u \\\\\n0 & 0 & d_l & d_m\n\\end{pmatrix} = \\begin{pmatrix}\n30.5 & -7.75 & 0 & 0 \\\\\n-22.75 & 30.5 & -7.75 & 0 \\\\\n0 & -22.75 & 30.5 & -7.75 \\\\\n0 & 0 & -22.75 & 30.5\n\\end{pmatrix}\n$$\nThe departure from normality is $\\delta(A) = \\|A^{\\top} A - A A^{\\top}\\|_{F}$. Let $C = A^{\\top} A - A A^{\\top}$.\n$$\nA^{\\top} = \\begin{pmatrix}\nd_m & d_l & 0 & 0 \\\\\nd_u & d_m & d_l & 0 \\\\\n0 & d_u & d_m & d_l \\\\\n0 & 0 & d_u & d_m\n\\end{pmatrix} = \\begin{pmatrix}\n30.5 & -22.75 & 0 & 0 \\\\\n-7.75 & 30.5 & -22.75 & 0 \\\\\n0 & -7.75 & 30.5 & -22.75 \\\\\n0 & 0 & -7.75 & 30.5\n\\end{pmatrix}\n$$\nThe non-zero elements of the commutator matrix $C$ for a tridiagonal Toeplitz-like matrix arise from the boundary rows.\n$C_{11} = (A^{\\top}A)_{11} - (AA^{\\top})_{11} = (d_m^2+d_l^2) - (d_m^2+d_u^2) = d_l^2 - d_u^2$.\n$C_{44} = (A^{\\top}A)_{44} - (AA^{\\top})_{44} = (d_m^2+d_u^2) - (d_m^2+d_l^2) = d_u^2 - d_l^2 = -C_{11}$.\nThe interior diagonal elements are zero: e.g., $C_{22} = (d_u^2+d_m^2+d_l^2) - (d_l^2+d_m^2+d_u^2) = 0$.\nThe off-diagonal elements are also zero. For example, $C_{12} = (d_m d_u + d_l d_m) - (d_m d_l + d_u d_m) = 0$. Similarly, all other off-diagonal elements are zero.\nThus, the commutator matrix $C$ is a diagonal matrix:\n$$ C = \\begin{pmatrix} d_l^2 - d_u^2 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & d_u^2 - d_l^2 \\end{pmatrix} $$\nLet's compute the value of the non-zero element:\n$$ d_l^2 - d_u^2 = (-22.75)^2 - (-7.75)^2 = 517.5625 - 60.0625 = 457.5 $$\nSo, the matrix $C$ is:\n$$ C = \\begin{pmatrix} 457.5 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -457.5 \\end{pmatrix} $$\nThe Frobenius norm of $C$ is the square root of the sum of the squares of its elements:\n$$ \\delta(A) = \\|C\\|_F = \\sqrt{\\sum_{i,j=1}^{4} |C_{ij}|^2} = \\sqrt{(457.5)^2 + 0^2 + \\dots + (-457.5)^2} $$\n$$ \\delta(A) = \\sqrt{(457.5)^2 + (-457.5)^2} = \\sqrt{2 \\times (457.5)^2} = 457.5\\sqrt{2} $$\nNow, we compute the numerical value:\n$$ \\delta(A) \\approx 457.5 \\times 1.41421356 \\approx 647.00966 $$\nThe problem asks for the answer to be rounded to four significant figures.\n$$ \\delta(A) \\approx 647.0 $$\nThe problem is posed with dimensionless parameters over a dimensionless domain $[0,1]$, so the matrix entries and the resulting departure from normality are also dimensionless.",
            "answer": "$$\\boxed{647.0}$$"
        },
        {
            "introduction": "Preconditioning is the cornerstone of efficient Krylov subspace methods, yet it introduces its own set of potential challenges. This case study focuses on a critical pitfall associated with left preconditioning, a common strategy in CFD solvers. You will analyze a scenario where monitoring the preconditioned residual, which is what left-preconditioned GMRES minimizes, can give a false sense of convergence. This practice will sharpen your ability to critically assess solver behavior and understand the crucial difference between the true residual and the preconditioned residual .",
            "id": "3970361",
            "problem": "In an implicit Newton–Krylov solve of the steady compressible Navier–Stokes equations for a transonic wing in Computational Fluid Dynamics (CFD), the linear subproblem at iteration $k$ has the form $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is the Jacobian of the residual with respect to the state variables, $x \\in \\mathbb{R}^{n}$ is the Newton correction, and $b \\in \\mathbb{R}^{n}$ is the nonlinear residual. A nonsingular left preconditioner $M \\in \\mathbb{R}^{n \\times n}$ is built from a physics-based block factorization with row scaling, intended to approximate $A$ and accelerate Krylov convergence. The Generalized Minimal Residual (GMRES) method is applied to the left-preconditioned system. Denote the true residual by $r_k = b - A x_k$ and the preconditioned residual by $\\tilde{r}_k = M^{-1} r_k$. The Euclidean norm is used, and both $M$ and $A$ are real, with $M$ possibly highly nonsymmetric and ill-conditioned due to mixed-unit scaling across velocity, pressure, and temperature.\n\nAssume that the operator $2$-norm of the preconditioner is $\\|M\\|_2 = 10^{5}$ and its condition number is $\\kappa_2(M) = 10^{6}$. A practitioner monitors $\\|\\tilde{r}_k\\|_2$ and stops when $\\|\\tilde{r}_k\\|_2 \\le \\tau$ with $\\tau = 10^{-8}$, expecting this to imply a small true residual $\\|r_k\\|_2$ as well.\n\nWhich of the following statements about the difference between the preconditioned residual $\\tilde{r}_k$ and the true residual $r_k$, and the risk of misleading stopping when monitoring $\\|\\tilde{r}_k\\|_2$, are correct?\n\nA. In left-preconditioned Generalized Minimal Residual (GMRES), the algorithm minimizes $\\|\\tilde{r}_k\\|_2$ rather than $\\|r_k\\|_2$, so a stopping test based solely on $\\|\\tilde{r}_k\\|_2$ can be misleading if $\\|M\\|_2$ is large.\n\nB. For any nonsingular preconditioner $M$, one always has $\\|\\tilde{r}_k\\|_2 \\le \\|r_k\\|_2$, hence monitoring $\\|\\tilde{r}_k\\|_2$ is conservative.\n\nC. The equality $\\tilde{r}_k = r_k$ for all iterations $k$ holds if and only if $M$ is the identity matrix.\n\nD. In right-preconditioned Krylov methods, the residual naturally reported and minimized is $r_k$, and a stopping criterion based on $\\|r_k\\|_2$ is not directly affected by the magnitude of $\\|M\\|_2$.\n\nE. If $M$ is nonsingular but extremely ill-conditioned, there can be iterations with $\\|\\tilde{r}_k\\|_2 \\le \\tau$ while $\\|r_k\\|_2$ exceeds $\\tau$ by a factor on the order of $\\|M\\|_2$, posing a risk of premature stopping.",
            "solution": "The problem statement has been validated and is deemed sound. It describes a standard and important scenario in the application of preconditioned Krylov subspace methods to problems in computational fluid dynamics. We may proceed with the analysis.\n\nThe core of the problem lies in the relationship between the true residual, $r_k = b - A x_k$, and the left-preconditioned residual, $\\tilde{r}_k = M^{-1} r_k$. Here, $x_k$ is the $k$-th iterate for the solution of the linear system $Ax=b$. The Generalized Minimal Residual (GMRES) method, when applied to the left-preconditioned system $M^{-1} A x = M^{-1} b$, generates a sequence of iterates $x_k$ that minimize the Euclidean norm of the preconditioned residual, $\\|\\tilde{r}_k\\|_2$, over the affine Krylov subspace.\n\nFrom the definition $\\tilde{r}_k = M^{-1} r_k$, we can write the true residual as $r_k = M \\tilde{r}_k$. We can establish bounds on the norm of the true residual, $\\|r_k\\|_2$, in terms of the norm of the preconditioned residual, $\\|\\tilde{r}_k\\|_2$, using the properties of induced matrix norms.\n\nThe relationship $r_k = M \\tilde{r}_k$ immediately implies the inequality:\n$$ \\|r_k\\|_2 = \\|M \\tilde{r}_k\\|_2 \\le \\|M\\|_2 \\|\\tilde{r}_k\\|_2 $$\nSimilarly, from $\\tilde{r}_k = M^{-1} r_k$, we have:\n$$ \\|\\tilde{r}_k\\|_2 = \\|M^{-1} r_k\\|_2 \\le \\|M^{-1}\\|_2 \\|r_k\\|_2 $$\nThis can be rearranged to give a lower bound on $\\|r_k\\|_2$:\n$$ \\|r_k\\|_2 \\ge \\frac{\\|\\tilde{r}_k\\|_2}{\\|M^{-1}\\|_2} $$\nThe condition number of the preconditioner is given by $\\kappa_2(M) = \\|M\\|_2 \\|M^{-1}\\|_2$. Thus, we can express $\\|M^{-1}\\|_2$ as $\\|M^{-1}\\|_2 = \\kappa_2(M) / \\|M\\|_2$. Combining these results, we obtain the two-sided inequality:\n$$ \\frac{\\|M\\|_2}{\\kappa_2(M)} \\|\\tilde{r}_k\\|_2 \\le \\|r_k\\|_2 \\le \\|M\\|_2 \\|\\tilde{r}_k\\|_2 $$\nUsing the provided numerical values, $\\|M\\|_2 = 10^5$ and $\\kappa_2(M) = 10^6$, the inequality becomes:\n$$ \\frac{10^5}{10^6} \\|\\tilde{r}_k\\|_2 \\le \\|r_k\\|_2 \\le 10^5 \\|\\tilde{r}_k\\|_2 $$\n$$ 0.1 \\|\\tilde{r}_k\\|_2 \\le \\|r_k\\|_2 \\le 10^5 \\|\\tilde{r}_k\\|_2 $$\nThe stopping criterion is $\\|\\tilde{r}_k\\|_2 \\le \\tau$, where $\\tau = 10^{-8}$. At the point of termination, where $\\|\\tilde{r}_k\\|_2$ is on the order of $\\tau$, the true residual norm $\\|r_k\\|_2$ is bounded by:\n$$ 0.1 \\times 10^{-8} \\le \\|r_k\\|_2 \\le 10^5 \\times 10^{-8} $$\n$$ 10^{-9} \\le \\|r_k\\|_2 \\le 10^{-3} $$\nThis demonstrates that while the preconditioned residual is small ($10^{-8}$), the true residual could be as large as $10^{-3}$. This discrepancy of five orders of magnitude highlights the potential danger of monitoring only the preconditioned residual.\n\nNow, we evaluate each option.\n\n**A. In left-preconditioned Generalized Minimal Residual (GMRES), the algorithm minimizes $\\|\\tilde{r}_k\\|_2$ rather than $\\|r_k\\|_2$, so a stopping test based solely on $\\|\\tilde{r}_k\\|_2$ can be misleading if $\\|M\\|_2$ is large.**\nGMRES applied to the system $M^{-1}Ax = M^{-1}b$ finds the iterate $x_k$ in the corresponding Krylov subspace that minimizes the norm of the system's residual, which is precisely $\\|\\tilde{r}_k\\|_2 = \\|M^{-1}(b-Ax_k)\\|_2$. This is distinct from minimizing the true residual norm, $\\|r_k\\|_2 = \\|b-Ax_k\\|_2$. As our derived inequality $\\|r_k\\|_2 \\le \\|M\\|_2 \\|\\tilde{r}_k\\|_2$ shows, if $\\|M\\|_2$ is large (here, $10^5$), a small $\\|\\tilde{r}_k\\|_2$ does not guarantee a small $\\|r_k\\|_2$. This potential for a large discrepancy makes the stopping test misleading.\n**Verdict: Correct.**\n\n**B. For any nonsingular preconditioner $M$, one always has $\\|\\tilde{r}_k\\|_2 \\le \\|r_k\\|_2$, hence monitoring $\\|\\tilde{r}_k\\|_2$ is conservative.**\nThe statement claims $\\|\\tilde{r}_k\\|_2 \\le \\|r_k\\|_2$ is always true. This is equivalent to $\\|M^{-1} r_k\\|_2 \\le \\|r_k\\|_2$ for any vector $r_k$. This inequality only holds if the operator norm $\\|M^{-1}\\|_2 \\le 1$. From the given data, $\\kappa_2(M) = 10^6$ and $\\|M\\|_2 = 10^5$, which gives $\\|M^{-1}\\|_2 = \\kappa_2(M) / \\|M\\|_2 = 10^6 / 10^5 = 10$. Since $\\|M^{-1}\\|_2 = 10 > 1$, it is possible for $\\|\\tilde{r}_k\\|_2$ to be significantly larger than $\\|r_k\\|_2$ (by up to a factor of $10$). Therefore, the statement is false, and monitoring the preconditioned residual is not necessarily conservative.\n**Verdict: Incorrect.**\n\n**C. The equality $\\tilde{r}_k = r_k$ for all iterations $k$ holds if and only if $M$ is the identity matrix.**\nThis is an \"if and only if\" statement.\n($\\Leftarrow$) If $M$ is the identity matrix ($M=I$), then its inverse $M^{-1}$ is also the identity matrix. The definition $\\tilde{r}_k = M^{-1} r_k$ becomes $\\tilde{r}_k = I r_k = r_k$. This holds for all $k$.\n($\\Rightarrow$) If $\\tilde{r}_k = r_k$ for all iterations $k$, then from the definition, $M^{-1} r_k = r_k$. This can be written as $(M^{-1} - I)r_k = 0$. This means that for every iteration $k$, the residual vector $r_k$ must be an eigenvector of $M^{-1}$ corresponding to the eigenvalue $1$. The sequence of residual vectors $r_k$ generated during a Krylov iteration for a general problem is not constrained to a single direction (unless the problem is trivial or converges in one step). For the equality to hold regardless of the specific matrix $A$ and starting vector $x_0$ (i.e., for any possible sequence of non-zero residuals $r_k$), the matrix operator $(M^{-1} - I)$ must be the zero matrix. This implies $M^{-1} = I$, and therefore $M=I$.\n**Verdict: Correct.**\n\n**D. In right-preconditioned Krylov methods, the residual naturally reported and minimized is $r_k$, and a stopping criterion based on $\\|r_k\\|_2$ is not directly affected by the magnitude of $\\|M\\|_2$.**\nIn right preconditioning, one solves the system $A M^{-1} y = b$ for the variable $y$, and then recovers the solution as $x = M^{-1} y$. A Krylov method like GMRES applied to this system will minimize the norm of its residual, which is $\\|b - (A M^{-1}) y_k\\|_2$. Substituting $x_k = M^{-1} y_k$, this becomes $\\|b - A x_k\\|_2$, which is exactly the norm of the true residual, $\\|r_k\\|_2$. Thus, the quantity being minimized is the true residual norm. A stopping criterion based on $\\|r_k\\|_2$ is therefore a direct measure of convergence. The magnitude of $\\|M\\|_2$ affects the operator $A M^{-1}$ and thus the rate of convergence, but it does not appear as a scaling factor between the minimized residual and the true residual, unlike in the left-preconditioned case. In this sense, the stopping criterion is not \"directly affected\".\n**Verdict: Correct.**\n\n**E. If $M$ is nonsingular but extremely ill-conditioned, there can be iterations with $\\|\\tilde{r}_k\\|_2 \\le \\tau$ while $\\|r_k\\|_2$ exceeds $\\tau$ by a factor on the order of $\\|M\\|_2$, posing a risk of premature stopping.**\nThis statement directly follows from our initial analysis. The upper bound on the true residual norm is $\\|r_k\\|_2 \\le \\|M\\|_2 \\|\\tilde{r}_k\\|_2$. If the stopping criterion $\\|\\tilde{r}_k\\|_2 \\le \\tau$ is met (e.g., $\\|\\tilde{r}_k\\|_2 = \\tau$), then the true residual norm $\\|r_k\\|_2$ can be as large as $\\|M\\|_2 \\tau$. This value exceeds the tolerance $\\tau$ by a factor of $\\|M\\|_2$. Given $\\|M\\|_2=10^5$, this is a factor of $10^5$. This is precisely what \"on the order of $\\|M\\|_2$\" means. This possibility constitutes a severe risk of premature termination, where the solver stops even though the true residual is far from converged. The high condition number $\\kappa_2(M)$ expands the range of uncertainty for $\\|r_k\\|_2$, but the risk of a large true residual is determined by the upper bound, which depends on $\\|M\\|_2$.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACDE}$$"
        },
        {
            "introduction": "Having explored the challenges of non-normality and preconditioning pitfalls, we now turn to a constructive and elegant solution: polynomial preconditioning. This practice guides you through the process of building a preconditioner $p(A)$ explicitly as a polynomial in the system matrix $A$. You will use the powerful minimax properties of Chebyshev polynomials to derive an optimal preconditioner that clusters the eigenvalues of the system, a key strategy for accelerating GMRES convergence .",
            "id": "3970309",
            "problem": "In a steady, incompressible Computational Fluid Dynamics (CFD) simulation for an aerospace configuration, a segregated pressure-correction step produces a symmetric positive definite linear system $A u = f$, where $A \\in \\mathbb{R}^{n \\times n}$ arises from a second-order finite-volume discretization of an elliptic operator on a hexahedral mesh. Spectral analysis and mesh-independent estimates yield that the spectrum of $A$ lies in an interval $[\\alpha,\\beta]$ with $0 < \\alpha < \\beta$, and there are no eigenvalues outside this interval.\n\nConsider left-preconditioning with a polynomial preconditioner $p(A)$ of degree at most $m-1$, so that the preconditioned operator is $B = p(A) A$. The Generalized Minimal Residual (GMRES) method applied to $B u = p(A) f$ chooses a residual polynomial $r_{k}$ of degree $k$ with $r_{k}(0)=1$ to minimize the residual $\\|r_{k}(B) r_{0}\\|$ at each iteration, where $r_{0}$ is the initial residual. The goal of the polynomial preconditioner is to cluster the spectrum of $B$ near $1$ by minimizing the worst-case deviation $|1 - \\lambda p(\\lambda)|$ over $\\lambda \\in [\\alpha,\\beta]$.\n\nStarting from the following foundational facts:\n- The GMRES residual after $k$ steps has the form $r_{k}(B) r_{0}$ with $r_{k}$ a degree-$k$ polynomial satisfying $r_{k}(0)=1$.\n- The Chebyshev polynomials of the first kind, $T_{m}$, defined by $T_{m}(\\cos\\theta)=\\cos(m\\theta)$, have the minimax property on $[-1,1]$: among all monic degree-$m$ polynomials scaled to match a fixed value at a point $x_{0} \\notin [-1,1]$, the magnitude of $T_{m}$ is minimized in the uniform norm on $[-1,1]$.\n\nTasks:\n1. Derive, from the minimax property, the optimal residual polynomial $r_{m}(\\lambda)$ of degree $m$ with $r_{m}(0)=1$ that minimizes $\\max_{\\lambda \\in [\\alpha,\\beta]} |r_{m}(\\lambda)|$. Use an affine map from $[\\alpha,\\beta]$ to $[-1,1]$ to express $r_{m}$ in terms of $T_{m}$, and then derive the associated polynomial preconditioner $p_{m-1}(\\lambda)$ defined by $1 - \\lambda p_{m-1}(\\lambda) = r_{m}(\\lambda)$.\n2. Using your derived formulas, take $\\alpha = \\tfrac{1}{5}$, $\\beta = 5$, and $m=3$. Compute explicitly the resulting quadratic preconditioner $p_{2}(\\lambda)$ as a closed-form analytic expression in $\\lambda$ with rational coefficients. Express the final answer as a simplified polynomial in $\\lambda$. Do not round.\n\nAdditionally, in your derivation, explain qualitatively how the constructed $p(A)$ damps undesirable spectral components and how this clustering of the spectrum of $B$ affects the convergence behavior of GMRES in this CFD context.\n\nYour final answer must be the explicit expression for $p_{2}(\\lambda)$.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It poses a standard, solvable problem in numerical linear algebra concerning polynomial preconditioning, using established principles and clear, formal language.\n\nThe overall task is to derive an optimal polynomial preconditioner based on Chebyshev polynomials and then compute a specific instance of it. The preconditioning strategy aims to improve the convergence of the GMRES method by clustering the spectrum of the preconditioned operator.\n\n### Part 1: Derivation of the Optimal Preconditioner\n\nThe problem is to find a polynomial $p_{m-1}(\\lambda)$ of degree at most $m-1$ that minimizes the quantity $\\max_{\\lambda \\in [\\alpha,\\beta]} |1 - \\lambda p_{m-1}(\\lambda)|$. Let $r_m(\\lambda) = 1 - \\lambda p_{m-1}(\\lambda)$. This $r_m(\\lambda)$ is a polynomial of degree at most $m$. The condition $r_m(0) = 1 - 0 \\cdot p_{m-1}(0) = 1$ is automatically satisfied. Thus, the problem is equivalent to finding the polynomial $r_m(\\lambda)$ of degree $m$ that satisfies $r_m(0)=1$ and minimizes the uniform norm $\\|r_m\\|_{L_{\\infty}([\\alpha,\\beta])} = \\max_{\\lambda \\in [\\alpha,\\beta]} |r_m(\\lambda)|$.\n\nThis is a classic minimax problem. The solution is known to be a scaled and shifted Chebyshev polynomial of the first kind, $T_m(x)$. The key property of $T_m(x)$ is that, of all polynomials of degree $m$ with a leading coefficient of $2^{m-1}$, it has the minimal $L_{\\infty}$ norm on $[-1,1]$, which is $1$. Equivalently, for a point $x_0$ outside $[-1,1]$, the polynomial $r(x) = T_m(x) / T_m(x_0)$ minimizes $\\max_{x\\in[-1,1]} |r(x)|$ among all polynomials of degree $m$ satisfying $r(x_0)=1$.\n\nTo apply this property, we must first map the spectral interval $[\\alpha,\\beta]$ to the interval $[-1,1]$ using an affine transformation. Let this map be $x = f(\\lambda) = c\\lambda+d$. We require $f(\\alpha)=-1$ and $f(\\beta)=1$:\n$$\n\\begin{cases}\nc\\alpha + d = -1 \\\\\nc\\beta + d = 1\n\\end{cases}\n$$\nSolving this system for $c$ and $d$ yields:\n$$c(\\beta-\\alpha) = 2 \\implies c = \\frac{2}{\\beta-\\alpha}$$\n$$d = 1 - c\\beta = 1 - \\frac{2\\beta}{\\beta-\\alpha} = \\frac{\\beta-\\alpha-2\\beta}{\\beta-\\alpha} = -\\frac{\\alpha+\\beta}{\\beta-\\alpha}$$\nSo, the affine map is:\n$$x(\\lambda) = \\frac{2\\lambda - (\\alpha+\\beta)}{\\beta-\\alpha}$$\nOur constraint is at $\\lambda=0$, which is outside the interval $[\\alpha,\\beta]$ since $0 < \\alpha$. The point $\\lambda=0$ maps to:\n$$x_0 = x(0) = \\frac{-(\\alpha+\\beta)}{\\beta-\\alpha}$$\nThe optimal residual polynomial $r_m(\\lambda)$ that minimizes its maximum magnitude on $[\\alpha,\\beta]$ subject to $r_m(0)=1$ is given by scaling the Chebyshev polynomial $T_m(x)$ such that its value is $1$ at the mapped point $x_0$.\n$$r_m(\\lambda) = \\frac{T_m(x(\\lambda))}{T_m(x_0)} = \\frac{T_m\\left(\\frac{2\\lambda - (\\alpha+\\beta)}{\\beta-\\alpha}\\right)}{T_m\\left(\\frac{-(\\alpha+\\beta)}{\\beta-\\alpha}\\right)}$$\nSince $T_m(-x) = (-1)^m T_m(x)$, the denominator can also be written as $(-1)^m T_m\\left(\\frac{\\alpha+\\beta}{\\beta-\\alpha}\\right)$.\n\nFrom the definition $r_m(\\lambda) = 1 - \\lambda p_{m-1}(\\lambda)$, we can solve for the polynomial preconditioner $p_{m-1}(\\lambda)$:\n$$p_{m-1}(\\lambda) = \\frac{1 - r_m(\\lambda)}{\\lambda} = \\frac{1}{\\lambda}\\left(1 - \\frac{T_m\\left(\\frac{2\\lambda - (\\alpha+\\beta)}{\\beta-\\alpha}\\right)}{T_m\\left(\\frac{-(\\alpha+\\beta)}{\\beta-\\alpha}\\right)}\\right)$$\nThe numerator, when evaluated at $\\lambda=0$, is $1 - T_m(x(0))/T_m(x(0)) = 0$. This confirms that the expression is divisible by $\\lambda$ and $p_{m-1}(\\lambda)$ is indeed a polynomial of degree $m-1$.\n\n### Qualitative Effect on GMRES Convergence\n\nThe polynomial preconditioner $p(A)$ transforms the original linear system $Au=f$ into $p(A)Au = p(A)f$. The new system matrix is $B=p(A)A$. If $\\lambda_i$ is an eigenvalue of $A$, then the corresponding eigenvalue of $B$ is $\\mu_i = \\lambda_i p(\\lambda_i)$.\nOur construction aims to make $\\lambda p(\\lambda)$ as close to $1$ as possible for all $\\lambda$ in the spectrum of $A$, i.e., for $\\lambda \\in [\\alpha, \\beta]$.\nThe polynomial $q(\\lambda) = \\lambda p_{m-1}(\\lambda)$ is constructed as $q(\\lambda) = 1 - r_m(\\lambda)$. The magnitude of $r_m(\\lambda)$ on $[\\alpha, \\beta]$ is bounded by:\n$$\\rho_m = \\max_{\\lambda \\in [\\alpha,\\beta]} |r_m(\\lambda)| = \\frac{\\max_{x \\in [-1,1]} |T_m(x)|}{\\left|T_m\\left(\\frac{-(\\alpha+\\beta)}{\\beta-\\alpha}\\right)\\right|} = \\frac{1}{\\left|T_m\\left(\\frac{\\alpha+\\beta}{\\beta-\\alpha}\\right)\\right|}$$\nThis means that for any eigenvalue $\\lambda_i$ of $A$, the corresponding eigenvalue $\\mu_i = \\lambda_i p_{m-1}(\\lambda_i)$ of $B$ lies in the interval $[1-\\rho_m, 1+\\rho_m]$. The preconditioner thus maps the original, possibly wide, spectral interval $[\\alpha, \\beta]$ to a much smaller interval centered at $1$.\n\nThe convergence rate of GMRES is highly dependent on the distribution of eigenvalues. By clustering the eigenvalues of the preconditioned matrix $B$ tightly around $1$, the effective condition number of the system is greatly reduced from $\\kappa(A) \\approx \\beta/\\alpha$ to $\\kappa(B) \\approx \\frac{1+\\rho_m}{1-\\rho_m}$. A smaller condition number and a more clustered spectrum allow the GMRES algorithm to find a low-degree residual polynomial that is small across the entire spectrum, leading to a much faster reduction in the residual norm and, consequently, faster convergence. The polynomial preconditioner acts as a spectral filter, damping the components of the residual corresponding to eigenvalues that slow down convergence.\n\n### Part 2: Explicit Computation for $p_2(\\lambda)$\n\nWe are given $\\alpha = \\frac{1}{5}$, $\\beta = 5$, and $m=3$. We seek the quadratic preconditioner $p_2(\\lambda)$.\n\nFirst, we compute the parameters for the affine map and its evaluation at $\\lambda=0$:\n$$\\alpha+\\beta = \\frac{1}{5} + 5 = \\frac{26}{5}$$\n$$\\beta-\\alpha = 5 - \\frac{1}{5} = \\frac{24}{5}$$\nThe map from $\\lambda$ to $x$ is:\n$$x(\\lambda) = \\frac{2\\lambda - 26/5}{24/5} = \\frac{10\\lambda - 26}{24} = \\frac{5\\lambda - 13}{12}$$\nThe point $x_0$ corresponding to $\\lambda=0$ is:\n$$x_0 = x(0) = -\\frac{13}{12}$$\nWe need the Chebyshev polynomial of degree $m=3$, which is $T_3(x) = 4x^3 - 3x$.\nNext, we evaluate $T_3(x_0)$:\n$$T_3\\left(-\\frac{13}{12}\\right) = 4\\left(-\\frac{13}{12}\\right)^3 - 3\\left(-\\frac{13}{12}\\right) = 4\\left(-\\frac{2197}{1728}\\right) + \\frac{39}{12}$$\n$$= -\\frac{2197}{432} + \\frac{39 \\times 36}{12 \\times 36} = \\frac{-2197 + 1404}{432} = -\\frac{793}{432}$$\nThe optimal residual polynomial of degree $3$ is:\n$$r_3(\\lambda) = \\frac{T_3\\left(\\frac{5\\lambda-13}{12}\\right)}{T_3\\left(-\\frac{13}{12}\\right)} = -\\frac{432}{793} T_3\\left(\\frac{5\\lambda-13}{12}\\right)$$\nWe now find the preconditioner $p_2(\\lambda) = \\frac{1-r_3(\\lambda)}{\\lambda}$:\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(1 + \\frac{432}{793} T_3\\left(\\frac{5\\lambda-13}{12}\\right)\\right)$$\nLet's expand $T_3\\left(\\frac{5\\lambda-13}{12}\\right)$:\n$$T_3\\left(\\frac{5\\lambda-13}{12}\\right) = 4\\left(\\frac{5\\lambda-13}{12}\\right)^3 - 3\\left(\\frac{5\\lambda-13}{12}\\right)$$\n$$= \\frac{4(5\\lambda-13)^3}{1728} - \\frac{3(5\\lambda-13)}{12} = \\frac{(5\\lambda-13)^3}{432} - \\frac{108(5\\lambda-13)}{432}$$\nThe numerator is $(5\\lambda-13)^3 - 108(5\\lambda-13)$. Expanding the cubic term:\n$$(5\\lambda-13)^3 = (5\\lambda)^3 - 3(5\\lambda)^2(13) + 3(5\\lambda)(13^2) - 13^3 = 125\\lambda^3 - 975\\lambda^2 + 2535\\lambda - 2197$$\nSo the numerator becomes:\n$$(125\\lambda^3 - 975\\lambda^2 + 2535\\lambda - 2197) - (540\\lambda - 1404) = 125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793$$\nThus,\n$$T_3\\left(\\frac{5\\lambda-13}{12}\\right) = \\frac{125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793}{432}$$\nSubstituting this into the expression for $p_2(\\lambda)$:\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(1 + \\frac{432}{793} \\left(\\frac{125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793}{432}\\right)\\right)$$\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(1 + \\frac{125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793}{793}\\right)$$\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(\\frac{793 + 125\\lambda^3 - 975\\lambda^2 + 1995\\lambda - 793}{793}\\right)$$\n$$p_2(\\lambda) = \\frac{1}{\\lambda} \\left(\\frac{125\\lambda^3 - 975\\lambda^2 + 1995\\lambda}{793}\\right)$$\nDividing by $\\lambda$, we obtain the final quadratic polynomial:\n$$p_2(\\lambda) = \\frac{125\\lambda^2 - 975\\lambda + 1995}{793}$$\nThe coefficients can be simplified. The prime factorization of the denominator is $793 = 13 \\times 61$. We check the numerator terms for these factors.\n$125 = 5^3$.\n$975 = 75 \\times 13$.\n$1995 = 5 \\times 399 = 5 \\times 3 \\times 133 = 5 \\times 3 \\times 7 \\times 19$.\nOnly the middle term has a factor of $13$.\n$$p_2(\\lambda) = \\frac{125}{793}\\lambda^2 - \\frac{975}{793}\\lambda + \\frac{1995}{793} = \\frac{125}{793}\\lambda^2 - \\frac{75 \\times 13}{61 \\times 13}\\lambda + \\frac{1995}{793}$$\nThe simplified polynomial is:\n$$p_2(\\lambda) = \\frac{125}{793}\\lambda^2 - \\frac{75}{61}\\lambda + \\frac{1995}{793}$$\nThis is the desired closed-form analytic expression for the preconditioner.",
            "answer": "$$ \\boxed{ \\frac{125}{793}\\lambda^2 - \\frac{75}{61}\\lambda + \\frac{1995}{793} } $$"
        }
    ]
}