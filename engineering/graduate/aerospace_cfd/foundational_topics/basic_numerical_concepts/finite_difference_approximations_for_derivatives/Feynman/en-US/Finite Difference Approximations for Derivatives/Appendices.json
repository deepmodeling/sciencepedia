{
    "hands_on_practices": [
        {
            "introduction": "Before deploying any numerical method in a complex simulation, it is crucial to verify that its implementation behaves as theoretically expected. This foundational practice in computational science is known as code verification. This exercise guides you through a systematic grid refinement study, a standard procedure to measure the \"observed order of accuracy\" and confirm that the error in your numerical derivative decreases at the predicted rate as the grid becomes finer . Mastering this technique is the first step toward building trust in any simulation code.",
            "id": "3958504",
            "problem": "In Computational Fluid Dynamics (CFD), spatial derivative operators used in discretizing the compressible Navierâ€“Stokes equations must be assessed for accuracy on smooth benchmark fields before use in production simulations. Consider a one-dimensional, periodic test field $u(x)$ on the interval $x \\in [0,1]$, representing a non-dimensionalized smooth wall-tangential velocity distribution along a wing section. You use a three-point centered finite difference derivative operator to approximate $u^{\\prime}(x)$ on three successively refined, uniform grids with $N_{1}$, $N_{2}$, and $N_{3}$ points, where the refinement ratio is fixed as $r = \\frac{h_{1}}{h_{2}} = \\frac{h_{2}}{h_{3}} = 2$ with $h_{k}$ denoting the grid spacing on level $k$. The discrete error at grid point $x_{i}$ is defined as $e_{i}(h) = D_{h}u(x_{i}) - u^{\\prime}(x_{i})$, where $D_{h}$ is the discrete derivative operator on grid spacing $h$.\n\nDesign a systematic grid refinement study that estimates the observed order of accuracy of the derivative operator from first principles, beginning from the Taylor-series-based definition of truncation error for consistent finite difference approximations of smooth functions. From this basis, derive the forms of the discrete error norms $\\|e\\|_{L_{1}}$, $\\|e\\|_{L_{2}}$, and $\\|e\\|_{L_{\\infty}}$ appropriate for a uniform periodic grid on $[0,1]$, and then derive an expression for the observed order of accuracy $p_{\\text{obs}}$ in terms of a refinement ratio $r$ and an error norm measured on successive grids.\n\nFinally, apply your derived expression to the following measured $L_{2}$-norm errors obtained with the same derivative operator on three grids with refinement ratio $r = 2$:\n$$\nE_{L_{2}}(h_{1}) = 2.985 \\times 10^{-3}, \\quad\nE_{L_{2}}(h_{2}) = 7.500 \\times 10^{-4}, \\quad\nE_{L_{2}}(h_{3}) = 1.885 \\times 10^{-4}.\n$$\nCompute the observed order of accuracy $p_{\\text{obs}}$ using the pair $(h_{1}, h_{2})$ and express your final numerical value rounded to four significant figures. No physical units are required for $p_{\\text{obs}}$ because it is dimensionless.",
            "solution": "A systematic grid refinement study for a derivative operator in Computational Fluid Dynamics (CFD) must be built on the consistency and smoothness assumptions required by finite difference analysis. For a smooth function $u(x)$ and a consistent discrete derivative operator $D_{h}$, a Taylor series expansion about a grid point $x_{i}$ yields the local truncation error. For a three-point centered finite difference approximation of the first derivative, one writes\n$$\nD_{h}u(x_{i}) = \\frac{u(x_{i+1}) - u(x_{i-1})}{2h}.\n$$\nExpanding $u(x_{i \\pm 1})$ about $x_{i}$ using Taylor series and subtracting the exact derivative $u^{\\prime}(x_{i})$ gives the leading-order truncation error proportional to $h^{2}$ for smooth $u$, i.e.,\n$$\nD_{h}u(x_{i}) - u^{\\prime}(x_{i}) = \\frac{h^{2}}{6}u^{(3)}(\\xi_{i}) + \\mathcal{O}(h^{4}),\n$$\nfor some $\\xi_{i}$ in a neighborhood of $x_{i}$. This implies an asymptotic error model in a chosen norm,\n$$\nE(h) = C h^{p} + \\mathcal{O}(h^{p+1}),\n$$\nwhere $C$ is a constant depending on $u$ and the operator, and $p$ is the formal order of accuracy (here expected to be $p=2$ for the centered difference on smooth $u$). The observed order of accuracy $p_{\\text{obs}}$ is estimated by measuring the error on successively refined grids and exploiting the power-law scaling in $h$ once the solution is in the asymptotic regime.\n\nOn a uniform periodic grid $x_{i} = ih$ for $i = 0,1,\\dots,N-1$ with spacing $h = 1/N$, the discrete error samples are $e_{i}(h) = D_{h}u(x_{i}) - u^{\\prime}(x_{i})$. The norm definitions that mimic the continuous $L_{1}$, $L_{2}$, and $L_{\\infty}$ norms over $[0,1]$ are:\n$$\n\\|e\\|_{L_{1}} = \\int_{0}^{1} |e(x)| \\, dx \\approx h \\sum_{i=0}^{N-1} |e_{i}(h)|,\n$$\n$$\n\\|e\\|_{L_{2}} = \\left( \\int_{0}^{1} e(x)^{2} \\, dx \\right)^{1/2} \\approx \\left( h \\sum_{i=0}^{N-1} e_{i}(h)^{2} \\right)^{1/2},\n$$\n$$\n\\|e\\|_{L_{\\infty}} = \\sup_{x \\in [0,1]} |e(x)| \\approx \\max_{0 \\le i \\le N-1} |e_{i}(h)|.\n$$\nThese discrete formulas preserve the interpretation of the norms as measures of error over the domain and are standard in assessing discretization errors on uniform grids.\n\nAssuming the asymptotic model $E(h) \\approx C h^{p}$ dominates, consider two successive grids with spacings $h$ and $h/r$, where $r > 1$ is the refinement ratio. Then\n$$\n\\frac{E(h)}{E(h/r)} \\approx \\frac{C h^{p}}{C (h/r)^{p}} = r^{p}.\n$$\nTaking natural logarithms gives\n$$\n\\ln\\!\\left( \\frac{E(h)}{E(h/r)} \\right) \\approx p \\ln(r),\n$$\nwhich yields the observed order of accuracy estimator\n$$\np_{\\text{obs}} \\approx \\frac{\\ln\\!\\left( E(h) / E(h/r) \\right)}{\\ln(r)}.\n$$\nThis estimator is valid for any of the norms provided the errors are measured in the asymptotic range where the leading-order term dominates.\n\nWe now apply this to the provided $L_{2}$ errors with refinement ratio $r = 2$ for the pair $(h_{1}, h_{2})$:\n$$\nE_{L_{2}}(h_{1}) = 2.985 \\times 10^{-3}, \\quad E_{L_{2}}(h_{2}) = 7.500 \\times 10^{-4}.\n$$\nCompute the ratio\n$$\n\\frac{E_{L_{2}}(h_{1})}{E_{L_{2}}(h_{2})} = \\frac{2.985 \\times 10^{-3}}{7.500 \\times 10^{-4}} = 3.98.\n$$\nTherefore,\n$$\np_{\\text{obs}} \\approx \\frac{\\ln(3.98)}{\\ln(2)}.\n$$\nUsing $\\ln(4) = 2 \\ln(2)$ and $3.98 = 4 \\times 0.995$, we have\n$$\n\\ln(3.98) = \\ln(4) + \\ln(0.995) \\approx 2 \\ln(2) - \\left(0.005 + \\frac{0.005^{2}}{2} + \\frac{0.005^{3}}{3} + \\cdots \\right),\n$$\nwhich numerically gives\n$$\n\\ln(3.98) \\approx 1.381281819, \\quad \\ln(2) \\approx 0.693147181,\n$$\nso\n$$\np_{\\text{obs}} \\approx \\frac{1.381281819}{0.693147181} \\approx 1.992773\\ldots\n$$\nRounded to four significant figures, $p_{\\text{obs}} \\approx 1.993$.\nThis result is consistent with the expected second-order behavior of the centered finite difference derivative operator applied to a smooth field.",
            "answer": "$$\\boxed{1.993}$$"
        },
        {
            "introduction": "While numerical methods are often tested on clean, analytic functions, real-world data from experiments or even other simulations is rarely perfect. This practice moves from the idealized world to the practical challenge of differentiating noisy data, a task where finite difference methods must be applied with great care. You will investigate the critical trade-off between truncation error, which decreases with smaller step sizes, and noise amplification, which dramatically increases . This exercise provides essential insight into the inherent instability of numerical differentiation and how to analyze its impact.",
            "id": "2392343",
            "problem": "You are given a time series model for one-dimensional position as a function of time with additive measurement noise. The position is a smooth, known function of time with superimposed zero-mean, independent noise at each sample. Your task is to design and implement a program that, for a set of test cases, constructs the noisy position data, estimates the velocity and acceleration using finite differences, and quantitatively analyzes the amplification of measurement noise by the differentiation operators.\n\nBase your reasoning on the following fundamental definitions and facts only: the derivative of a function is the limit of the difference quotient, linear operators acting on independent random variables produce outputs whose variances add according to the squares of the operator weights, and a Taylor series expansion can be used to derive the local truncation error order of a finite difference stencil. Do not use any other pre-packaged formulas beyond these principles.\n\nUse the following signal model. The noiseless position is\n$$\nx(t) = A \\sin(2\\pi f_1 t) + C \\sin(2\\pi f_2 t) + D t^2,\n$$\nwith parameters\n$$\nA = 1.0\\ \\text{m},\\quad C = 0.5\\ \\text{m},\\quad f_1 = 1.0\\ \\text{Hz},\\quad f_2 = 3.0\\ \\text{Hz},\\quad D = 0.05\\ \\text{m/s}^2.\n$$\nAngles in trigonometric functions are in radians. The exact velocity and acceleration are, respectively,\n$$\nv(t) = \\frac{dx}{dt} = 2\\pi f_1 A \\cos(2\\pi f_1 t) + 2\\pi f_2 C \\cos(2\\pi f_2 t) + 2 D t,\n$$\n$$\na(t) = \\frac{d^2 x}{dt^2} = - (2\\pi f_1)^2 A \\sin(2\\pi f_1 t) - (2\\pi f_2)^2 C \\sin(2\\pi f_2 t) + 2 D.\n$$\n\nSampling and noise model. For each test case, sample uniformly at times\n$$\nt_n = n \\,\\Delta t,\\quad n=0,1,\\dots,N-1,\\quad N = \\left\\lfloor \\frac{T}{\\Delta t}\\right\\rfloor + 1,\\quad T = 10\\ \\text{s},\n$$\nand form noisy measurements\n$$\nx_n^{\\text{noisy}} = x(t_n) + \\eta_n,\\quad \\eta_n \\sim \\mathcal{N}(0,\\sigma_x^2)\\ \\text{i.i.d.},\n$$\nwith a fixed random seed equal to $12345$ for reproducibility. All distances are in meters and time in seconds.\n\nFinite difference requirements. From first principles, derive and implement second-order accurate finite difference schemes for the first and second derivatives as follows:\n- For interior points, use a centered, second-order accurate stencil.\n- At the two boundaries, use one-sided, second-order accurate stencils.\nYour implementation must produce arrays $v_n^{\\text{FD}}$ and $a_n^{\\text{FD}}$ of the same length as the input $x_n$.\n\nNoise amplification analysis. Let a finite difference derivative at index $i$ be the linear combination\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\nwhere $y_i$ represents either the first or second derivative estimate, and $w_{i,j}$ are the finite difference weights divided by the appropriate power of $\\Delta t$. Using only linearity of expectation and independence of the noise samples, derive and compute:\n- The empirical Root Mean Square (RMS) noise amplification for the first derivative,\n$$\ng_v^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 },\n$$\nand for the second derivative,\n$$\ng_a^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 }.\n$$\n- The theoretical RMS noise amplification predicted by the weights,\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j} w_{i,j}^2 \\right) }.\n$$\nCompute $g_v^{\\text{theory}}$ and $g_a^{\\text{theory}}$ using the actual weights used at each index, including boundary stencils.\n\nPerformance metrics. For each test case, compute:\n- The RMS error of the velocity estimate relative to the exact velocity,\n$$\nE_v = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v(t_i) \\right)^2 } \\ \\text{in m/s}.\n$$\n- The RMS error of the acceleration estimate relative to the exact acceleration,\n$$\nE_a = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a(t_i) \\right)^2 } \\ \\text{in m/s}^2.\n$$\n- The ratios\n$$\nR_v = \\frac{ g_v^{\\text{emp}} }{ g_v^{\\text{theory}} },\\qquad R_a = \\frac{ g_a^{\\text{emp}} }{ g_a^{\\text{theory}} },\n$$\nwhich indicate how well the empirical noise amplification matches the theoretical prediction.\n\nTest suite. Run your program on the following four test cases, which vary sampling interval and noise level:\n- Case $1$: $\\Delta t = 0.01\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n- Case $2$: $\\Delta t = 0.1\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n- Case $3$: $\\Delta t = 0.01\\ \\text{s}$, $\\sigma_x = 0.01\\ \\text{m}$.\n- Case $4$: $\\Delta t = 0.001\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n\nFinal output format. Your program should produce a single line of output containing the results aggregated as a comma-separated list enclosed in square brackets. For each case in the order $1$ through $4$, append the four floating-point numbers in the following order: $E_v$ (in m/s), $E_a$ (in m/s$^2$), $R_v$ (dimensionless), $R_a$ (dimensionless). Each number must be printed in scientific notation with exactly six significant figures. For example, the overall output format is\n$$\n[\\ E_{v,1},\\ E_{a,1},\\ R_{v,1},\\ R_{a,1},\\ E_{v,2},\\ E_{a,2},\\ R_{v,2},\\ R_{a,2},\\ E_{v,3},\\ E_{a,3},\\ R_{v,3},\\ R_{a,3},\\ E_{v,4},\\ E_{a,4},\\ R_{v,4},\\ R_{a,4}\\ ].\n$$",
            "solution": "We begin from first principles. The first derivative of a function $x(t)$ at time $t$ is defined by the limit\n$$\n\\frac{dx}{dt}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - x(t-\\Delta t)}{2\\Delta t},\n$$\nwhich suggests symmetric (centered) difference quotients for finite but small $\\Delta t$. The second derivative is defined by\n$$\n\\frac{d^2x}{dt^2}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - 2x(t) + x(t-\\Delta t)}{\\Delta t^2}.\n$$\nUsing Taylor series expansions around $t_i = i \\Delta t$,\n$$\nx(t_{i\\pm 1}) = x(t_i) \\pm \\Delta t\\, x'(t_i) + \\frac{\\Delta t^2}{2} x''(t_i) \\pm \\frac{\\Delta t^3}{6} x^{(3)}(t_i) + \\frac{\\Delta t^4}{24} x^{(4)}(t_i) + \\mathcal{O}(\\Delta t^5),\n$$\nwe can derive second-order accurate centered stencils for interior points:\n$$\nx'(t_i) = \\frac{x_{i+1} - x_{i-1}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_i) = \\frac{x_{i+1} - 2 x_i + x_{i-1}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2),\n$$\nwhere $x_i = x(t_i)$. At the boundaries, one cannot form symmetric differences; instead, a Taylor expansion using forward points yields one-sided, second-order accurate stencils. For the first derivative at the left boundary $i=0$,\n$$\nx'(t_0) = \\frac{-3 x_0 + 4 x_1 - x_2}{2\\Delta t} + \\mathcal{O}(\\Delta t^2),\n$$\nand analogously for the right boundary $i=N-1$,\n$$\nx'(t_{N-1}) = \\frac{3 x_{N-1} - 4 x_{N-2} + x_{N-3}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n$$\nFor the second derivative, forward and backward second-order accurate one-sided stencils are\n$$\nx''(t_0) = \\frac{2 x_0 - 5 x_1 + 4 x_2 - x_3}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_{N-1}) = \\frac{2 x_{N-1} - 5 x_{N-2} + 4 x_{N-3} - x_{N-4}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2).\n$$\nThese formulas are obtained by solving systems of equations that match the Taylor expansions term-by-term to eliminate lower-order error terms, ensuring second-order accuracy.\n\nNoise amplification analysis rests on linearity. Let the finite difference operator mapping $\\{x_j\\}$ to a derivative estimate $\\{y_i\\}$ be linear:\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\nwhere $w_{i,j}$ are the operator weights, including normalization by the appropriate power of $\\Delta t$ dictated by the derivative order. Suppose the measurements include additive zero-mean, independent noise $\\eta_j$ with variance $\\mathbb{V}[\\eta_j] = \\sigma_x^2$. Due to linearity and independence,\n$$\n\\mathbb{E}[y_i] = \\sum_{j} w_{i,j} \\mathbb{E}[x_j]\n$$\n$$\n\\mathbb{V}[y_i] = \\sum_{j} w_{i,j}^2 \\,\\mathbb{V}[\\eta_j] = \\sigma_x^2 \\sum_{j} w_{i,j}^2.\n$$\nTherefore, the Root Mean Square (RMS) of the noise component at index $i$ equals\n$$\n\\sqrt{\\mathbb{V}[y_i]} = \\sigma_x \\sqrt{\\sum_{j} w_{i,j}^2}.\n$$\nA global RMS across all indices, consistent with the empirical RMS used in practice, is\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left(\\sum_{j} w_{i,j}^2\\right)}.\n$$\nThis expression includes boundary effects naturally via index-dependent weights.\n\nTwo consequences follow:\n- For the first derivative, the weights scale like $1/\\Delta t$, so $g_v^{\\text{theory}} \\propto \\sigma_x/\\Delta t$; that is, reducing $\\Delta t$ increases the noise amplification in the velocity estimate if the position noise level per sample is fixed.\n- For the second derivative, the weights scale like $1/\\Delta t^2$, so $g_a^{\\text{theory}} \\propto \\sigma_x/\\Delta t^2$, implying even stronger amplification of noise.\n\nThe empirical Root Mean Square (RMS) noise amplification can be isolated by comparing the derivative operator applied to noisy data and to clean data, which cancels deterministic truncation error:\n$$\ng^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( y_i[x^{\\text{noisy}}] - y_i[x^{\\text{clean}}] \\right)^2 }.\n$$\nBecause $y_i[\\cdot]$ is linear and $x^{\\text{noisy}} = x^{\\text{clean}} + \\eta$, the difference equals $y_i[\\eta]$, whose RMS matches the theoretical expression derived above in the limit of many samples. Hence, the ratios\n$$\nR_v = \\frac{g_v^{\\text{emp}}}{g_v^{\\text{theory}}},\\qquad R_a = \\frac{g_a^{\\text{emp}}}{g_a^{\\text{theory}}},\n$$\nshould be close to $1$ when the empirical averages are representative.\n\nAlgorithmic design:\n- Construct time samples $t_i = i \\Delta t$ for $i=0,\\dots,N-1$ with $T=10$ s.\n- Compute the clean position $x(t_i)$, and the exact velocity $v(t_i)$ and acceleration $a(t_i)$ from the given analytic expressions.\n- Generate additive noise $\\eta_i \\sim \\mathcal{N}(0,\\sigma_x^2)$ using a fixed seed to ensure reproducibility, and form $x^{\\text{noisy}}_i = x(t_i) + \\eta_i$.\n- Implement second-order accurate finite difference estimators for the first and second derivatives that apply centered stencils in the interior and one-sided stencils at the boundaries, yielding $v^{\\text{FD}}$ and $a^{\\text{FD}}$ for both clean and noisy inputs.\n- Compute performance metrics: $E_v$ and $E_a$ as RMS errors against the exact derivatives. Compute $g_v^{\\text{emp}}$ and $g_a^{\\text{emp}}$ as RMS differences between the finite difference outputs on noisy and clean data. Compute $g_v^{\\text{theory}}$ and $g_a^{\\text{theory}}$ via the weights by summing squared weights at each index and averaging, then multiply by $\\sigma_x$ and take a square root. Finally, compute $R_v$ and $R_a$ as ratios of empirical to theoretical amplification.\n- Repeat for the four specified test cases. Print the consolidated results in the required single-line, bracketed, comma-separated list, with scientific notation and six significant figures.\n\nExpected trends:\n- Increasing $\\sigma_x$ by a factor of $10$ should increase $g^{\\text{emp}}$ and the noise-dominated components of $E_v$ and $E_a$ by a factor of $10$.\n- Increasing $\\Delta t$ should reduce $g_v^{\\text{theory}}$ roughly like $1/\\Delta t$ and $g_a^{\\text{theory}}$ roughly like $1/\\Delta t^2$; however, truncation error grows like $\\mathcal{O}(\\Delta t^2)$, so $E_v$ and $E_a$ may not decrease monotonically with $\\Delta t$ because of the trade-off between truncation error and noise amplification.\n- The ratios $R_v$ and $R_a$ should be close to $1$, validating the linear noise amplification analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef position(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # x(t) in meters\n    return A * np.sin(2*np.pi*f1*t) + C * np.sin(2*np.pi*f2*t) + D * t**2\n\ndef velocity_true(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # v(t) in m/s\n    return 2*np.pi*f1*A * np.cos(2*np.pi*f1*t) + 2*np.pi*f2*C * np.cos(2*np.pi*f2*t) + 2*D*t\n\ndef acceleration_true(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # a(t) in m/s^2\n    return -(2*np.pi*f1)**2 * A * np.sin(2*np.pi*f1*t) - (2*np.pi*f2)**2 * C * np.sin(2*np.pi*f2*t) + 2*D\n\ndef fd_first_derivative(x, dt):\n    \"\"\"\n    Second-order accurate finite difference for first derivative.\n    - One-sided 3-point at boundaries (order 2).\n    - Centered 3-point in interior (order 2).\n    Returns y of same length as x.\n    \"\"\"\n    n = x.size\n    y = np.empty_like(x, dtype=float)\n    if n  3:\n        raise ValueError(\"Need at least 3 points for second-order first derivative\")\n    # Left boundary: (-3 x0 + 4 x1 - x2)/(2 dt)\n    y[0] = (-3.0*x[0] + 4.0*x[1] - 1.0*x[2]) / (2.0*dt)\n    # Interior\n    y[1:-1] = (x[2:] - x[:-2]) / (2.0*dt)\n    # Right boundary: (3 xN-1 - 4 xN-2 + xN-3)/(2 dt)\n    y[-1] = (3.0*x[-1] - 4.0*x[-2] + 1.0*x[-3]) / (2.0*dt)\n    return y\n\ndef fd_second_derivative(x, dt):\n    \"\"\"\n    Second-order accurate finite difference for second derivative.\n    - One-sided 4-point at boundaries (order 2).\n    - Centered 3-point in interior (order 2).\n    Returns y of same length as x.\n    \"\"\"\n    n = x.size\n    y = np.empty_like(x, dtype=float)\n    if n  4:\n        raise ValueError(\"Need at least 4 points for second derivative with second-order accuracy\")\n    # Left boundary: (2 x0 - 5 x1 + 4 x2 - x3)/dt^2\n    y[0] = (2.0*x[0] - 5.0*x[1] + 4.0*x[2] - 1.0*x[3]) / (dt*dt)\n    # Interior\n    y[1:-1] = (x[2:] - 2.0*x[1:-1] + x[:-2]) / (dt*dt)\n    # Right boundary: (2 xN-1 - 5 xN-2 + 4 xN-3 - xN-4)/dt^2\n    y[-1] = (2.0*x[-1] - 5.0*x[-2] + 4.0*x[-3] - 1.0*x[-4]) / (dt*dt)\n    return y\n\ndef weights_sqsum_first(n, dt):\n    \"\"\"\n    For each index i, compute sum_j w_{i,j}^2 for the first derivative operator.\n    Returns an array s of length n with s[i] = sum_j w_{i,j}^2.\n    \"\"\"\n    s = np.zeros(n, dtype=float)\n    inv = 1.0 / (2.0*dt)\n    # Left boundary: (-3, 4, -1)/(2 dt)\n    coeffs = np.array([-3.0, 4.0, -1.0]) * inv\n    s[0] = np.sum(coeffs**2)\n    # Interior: [-1, +1] at i-1 and i+1\n    c = np.array([-1.0, 1.0]) * inv\n    val = np.sum(c**2)\n    s[1:-1] = val\n    # Right boundary: (1, -4, 3)/(2 dt) applied to (i-2, i-1, i)\n    coeffs = np.array([1.0, -4.0, 3.0]) * inv\n    s[-1] = np.sum(coeffs**2)\n    return s\n\ndef weights_sqsum_second(n, dt):\n    \"\"\"\n    For each index i, compute sum_j w_{i,j}^2 for the second derivative operator.\n    Returns an array s of length n with s[i] = sum_j w_{i,j}^2.\n    \"\"\"\n    s = np.zeros(n, dtype=float)\n    inv2 = 1.0 / (dt*dt)\n    # Left boundary: (2, -5, 4, -1)/dt^2\n    coeffs = np.array([2.0, -5.0, 4.0, -1.0]) * inv2\n    s[0] = np.sum(coeffs**2)\n    # Interior: (1, -2, 1)/dt^2\n    c = np.array([1.0, -2.0, 1.0]) * inv2\n    val = np.sum(c**2)\n    s[1:-1] = val\n    # Right boundary: (-1, 4, -5, 2)/dt^2 applied to (i-3, i-2, i-1, i)\n    coeffs = np.array([-1.0, 4.0, -5.0, 2.0]) * inv2\n    s[-1] = np.sum(coeffs**2)\n    return s\n\ndef rms(x):\n    return np.sqrt(np.mean(np.square(x)))\n\ndef format_float(x):\n    # Scientific notation with exactly six significant figures\n    return f\"{x:.6e}\"\n\ndef run_case(dt, sigma_x, rng):\n    T = 10.0\n    N = int(np.floor(T/dt)) + 1\n    t = np.linspace(0.0, dt*(N-1), N)\n    x_clean = position(t)\n    # Generate noise with given sigma\n    noise = rng.normal(loc=0.0, scale=sigma_x, size=N)\n    x_noisy = x_clean + noise\n\n    # True derivatives\n    v_true = velocity_true(t)\n    a_true = acceleration_true(t)\n\n    # Finite difference estimates\n    v_fd_clean = fd_first_derivative(x_clean, dt)\n    a_fd_clean = fd_second_derivative(x_clean, dt)\n    v_fd_noisy = fd_first_derivative(x_noisy, dt)\n    a_fd_noisy = fd_second_derivative(x_noisy, dt)\n\n    # RMS errors against exact\n    E_v = rms(v_fd_noisy - v_true)\n    E_a = rms(a_fd_noisy - a_true)\n\n    # Empirical noise amplification (difference noisy-clean)\n    g_v_emp = rms(v_fd_noisy - v_fd_clean)\n    g_a_emp = rms(a_fd_noisy - a_fd_clean)\n\n    # Theoretical noise amplification from weights\n    s1 = weights_sqsum_first(N, dt)\n    s2 = weights_sqsum_second(N, dt)\n    g_v_theory = sigma_x * np.sqrt(np.mean(s1))\n    g_a_theory = sigma_x * np.sqrt(np.mean(s2))\n\n    # Ratios (avoid division by zero, though here not zero)\n    R_v = g_v_emp / g_v_theory if g_v_theory > 0 else np.nan\n    R_a = g_a_emp / g_a_theory if g_a_theory > 0 else np.nan\n\n    return E_v, E_a, R_v, R_a\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (dt [s], sigma_x [m])\n    test_cases = [\n        (0.01, 0.001),   # Case 1\n        (0.1, 0.001),    # Case 2\n        (0.01, 0.01),    # Case 3\n        (0.001, 0.001),  # Case 4\n    ]\n\n    rng = np.random.default_rng(12345)\n\n    results = []\n    for dt, sigma_x in test_cases:\n        E_v, E_a, R_v, R_a = run_case(dt, sigma_x, rng)\n        results.extend([E_v, E_a, R_v, R_a])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(format_float(x) for x in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Most physical phenomena occur in multiple dimensions, requiring us to extend our numerical toolkit from one-dimensional lines to two- or three-dimensional domains. This exercise demonstrates a powerful and modular technique for this extension: constructing multi-dimensional operators by composing simpler 1D operators. You will derive and implement a second-order accurate approximation for the mixed partial derivative $\\frac{\\partial^2 u}{\\partial x \\partial y}$ by sequentially applying 1D derivative stencils, a method that elegantly handles complex derivatives and boundary conditions in higher dimensions .",
            "id": "2392349",
            "problem": "You are asked to derive, analyze, and implement a finite difference method for the mixed partial derivative $\\frac{\\partial^2 u}{\\partial x \\partial y}$ of a sufficiently smooth scalar field $u(x,y)$ defined on a rectangular domain. Your final answer must be a complete, runnable program that evaluates the requested approximations and aggregates numerical results for a provided test suite.\n\nStarting point and scope:\n- Begin from fundamental definitions: partial derivatives as limits of difference quotients and the Taylor series expansion of a sufficiently smooth function about a point. Use these to derive a consistent, second-order accurate in space finite difference approximation for $\\frac{\\partial^2 u}{\\partial x \\partial y}$ on a uniform, rectangular grid with spacings $h_x$ and $h_y$ in the $x$ and $y$ directions, respectively.\n- For interior grid points where central differences are applicable in both directions, derive a central approximation that is second-order accurate in both $h_x$ and $h_y$.\n- For grid points on boundaries where a centered stencil in one or both directions is not available, derive a consistent construction by composing one-sided, second-order accurate approximations for first derivatives in each direction to obtain an overall approximation of the mixed derivative that remains second-order accurate in space.\n- Provide a brief leading-order truncation error characterization in terms of $h_x$ and $h_y$.\n\nGrid model and notation:\n- Consider a uniform grid on $\\Omega = [0,1]\\times[0,1]$ with $N_x$ points along $x$ and $N_y$ points along $y$. Define $h_x = 1/(N_x-1)$ and $h_y = 1/(N_y-1)$. Grid point indices are $i=0,1,\\dots,N_x-1$ and $j=0,1,\\dots,N_y-1$. Coordinates are $x_i = i h_x$ and $y_j = j h_y$.\n- Your implementation should compute an approximation to $u_{xy}(x_i,y_j) = \\frac{\\partial^2 u}{\\partial x \\partial y}$ at any grid node $\\{i,j\\}$ by composing first-derivative finite differences in $x$ and $y$, using central schemes at interior indices $i\\in\\{1,\\dots,N_x-2\\}$ or $j\\in\\{1,\\dots,N_y-2\\}$, and second-order one-sided schemes at boundary indices $i\\in\\{0,N_x-1\\}$ or $j\\in\\{0,N_y-1\\}$.\n\nImplementation constraints:\n- Implement the approximation by sequentially applying a finite difference for $\\partial/\\partial x$ to $u$ and then a finite difference for $\\partial/\\partial y$ to the result (or vice versa). Each one-dimensional operator must be second-order accurate (central in the interior, one-sided of order $2$ at boundaries).\n- Your program must use the above definitions to evaluate the mixed derivative at specified grid nodes, compare to the analytic exact value $u_{xy}(x,y)$ where required, and report absolute errors or error ratios as specified below.\n\nTest suite:\n- Use the following analytic test fields and grids. Angles are in radians. There are no physical units to report in the final answers.\n- Function A: $u(x,y) = \\sin(a x)\\mathrm{e}^{b y}$ with parameters $a=1.3$ and $b=0.7$. The exact mixed derivative is $u_{xy}(x,y) = a b \\cos(a x)\\mathrm{e}^{b y}$.\n- Function B (polynomial): $u(x,y) = x^2 y^2 + 2xy + 3x^2 + 5y^2 + 7$. The exact mixed derivative is $u_{xy}(x,y) = 4xy + 2$.\n\nDefine the following $5$ test cases. For each, compute the specified numerical quantity:\n- Case $1$ (interior absolute error): Function A on $N_x=41$, $N_y=37$. Compute the absolute error $|u_{xy}^{\\text{FD}} - u_{xy}^{\\text{exact}}|$ at the interior node with indices $i=15$ and $j=17$.\n- Case $2$ (interior convergence ratio): Function A on two grids: coarse with $N_x=41$, $N_y=41$; fine with $N_x=81$, $N_y=81$. In both cases, evaluate at the physical location $x=0.5$ and $y=0.5$ (which correspond to indices $i=20$, $j=20$ on the coarse grid and $i=40$, $j=40$ on the fine grid). Compute the ratio $\\varepsilon_{\\text{coarse}} / \\varepsilon_{\\text{fine}}$ of absolute errors, where $\\varepsilon$ denotes $|u_{xy}^{\\text{FD}} - u_{xy}^{\\text{exact}}|$ at that node.\n- Case $3$ (corner convergence ratio): Function A on the same two grids as in Case $2$. Evaluate at the corner node $i=0$, $j=0$. Compute the ratio $\\varepsilon_{\\text{coarse}} / \\varepsilon_{\\text{fine}}$ of absolute errors.\n- Case $4$ (polynomial exactness check): Function B on $N_x=50$, $N_y=70$. Compute the maximum absolute error over all grid nodes, i.e., $\\max_{0\\le i\\le N_x-1,\\,0\\le j\\le N_y-1} |u_{xy}^{\\text{FD}}(x_i,y_j) - u_{xy}^{\\text{exact}}(x_i,y_j)|$.\n- Case $5$ (anisotropic grid interior error): Function A on an anisotropic grid with $N_x=33$, $N_y=5$. Evaluate the absolute error at the interior node $i=16$, $j=2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the five results in order for Cases $1$ through $5$, as a comma-separated list enclosed in square brackets, for example: $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$.",
            "solution": "The problem presented is to derive, analyze, and implement a second-order accurate finite difference approximation for the mixed partial derivative $\\frac{\\partial^2 u}{\\partial x \\partial y}$ of a scalar field $u(x,y)$. The methodology must be valid for a uniform rectangular grid, including at the boundaries. The problem is well-posed, scientifically sound, and contains all necessary information for a unique solution. We shall proceed.\n\nThe fundamental approach, as specified, is to compose two one-dimensional, second-order accurate finite difference operators. We can express the mixed derivative as a sequential application of first-order derivatives:\n$$\n\\frac{\\partial^2 u}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x}\\left(\\frac{\\partial u}{\\partial y}\\right)\n$$\nWe will construct a finite difference operator $D_x$ that approximates $\\frac{\\partial}{\\partial x}$ and an operator $D_y$ that approximates $\\frac{\\partial}{\\partial y}$. The final approximation is then $D_x D_y u$. To ensure second-order accuracy over the entire domain, the operators $D_x$ and $D_y$ must themselves be second-order accurate at every grid point, including boundaries.\n\nFirst, we derive the necessary one-dimensional, second-order accurate finite difference formulas for a function $f(z)$ on a uniform grid with spacing $h$. These are derived from the Taylor series expansion of $f(z)$ around a point $z$.\n\nThe Taylor series expansions around $z$ are:\n$$\nf(z+h) = f(z) + h f'(z) + \\frac{h^2}{2} f''(z) + \\frac{h^3}{6} f'''(z) + \\mathcal{O}(h^4)\n$$\n$$\nf(z-h) = f(z) - h f'(z) + \\frac{h^2}{2} f''(z) - \\frac{h^3}{6} f'''(z) + \\mathcal{O}(h^4)\n$$\n$$\nf(z+2h) = f(z) + 2h f'(z) + 2h^2 f''(z) + \\frac{4h^3}{3} f'''(z) + \\mathcal{O}(h^4)\n$$\n\n1.  **Central Difference for Interior Points:** For a point $z_k$ not on a boundary, we can use grid points on either side. Subtracting the expansion for $f(z_k-h)$ from that of $f(z_k+h)$ yields:\n    $$f(z_k+h) - f(z_k-h) = 2h f'(z_k) + \\frac{h^3}{3} f'''(z_k) + \\mathcal{O}(h^5)$$\n    Solving for $f'(z_k)$, we obtain the second-order accurate central difference formula:\n    $$f'(z_k) = \\frac{f(z_k+h) - f(z_k-h)}{2h} - \\frac{h^2}{6} f'''(z_k) + \\mathcal{O}(h^4)$$\n    The approximation is $D_{central}f(z_k) = \\frac{f(z_k+h) - f(z_k-h)}{2h}$, with a leading-order truncation error of $\\mathcal{O}(h^2)$.\n\n2.  **One-Sided Differences for Boundary Points:** At a boundary point, for instance $z_0$, we can only use points to one side (e.g., $z_0, z_1, z_2, \\dots$). To achieve second-order accuracy, we must use a stencil with at least three points. We construct a linear combination of $f(z_0)$, $f(z_0+h)$, and $f(z_0+2h)$ to approximate $f'(z_0)$. A linear algebra approach on the Taylor coefficients shows that the unique combination is:\n    $$f'(z_0) = \\frac{-3f(z_0) + 4f(z_0+h) - f(z_0+2h)}{2h} + \\frac{h^2}{3}f'''(z_0) + \\mathcal{O}(h^3)$$\n    This is the second-order accurate forward difference formula, $D_{fwd}f(z_0)$, with a truncation error of $\\mathcal{O}(h^2)$. By symmetry, the second-order backward difference at the other end of the domain, $z_{N-1}$, is:\n    $$f'(z_{N-1}) = \\frac{3f(z_{N-1}) - 4f(z_{N-1}-h) + f(z_{N-1}-2h)}{2h} - \\frac{h^2}{3}f'''(z_{N-1}) + \\mathcal{O}(h^3)$$\n    This is $D_{bwd}f(z_{N-1})$, also with a truncation error of $\\mathcal{O}(h^2)$.\n\nNow, we compose these operators to approximate $\\frac{\\partial^2 u}{\\partial x \\partial y}$ on the grid $u_{ij} = u(x_i, y_j)$. We first apply the operator $D_y$ to the grid data $U=\\{u_{ij}\\}$, which involves differentiating each column of $U$ to obtain an intermediate grid $V \\approx \\frac{\\partial u}{\\partial y}$. Then, we apply the operator $D_x$ to $V$, differentiating each row of $V$ to obtain the final approximation $W \\approx \\frac{\\partial^2 u}{\\partial x \\partial y}$.\n\nLet's inspect an interior point $(x_i, y_j)$, where $1 \\le i \\le N_x-2$ and $1 \\le j \\le N_y-2$. We use central differences for both steps.\nFirst, apply $D_y^{central}$:\n$$v(x_k, y_j) \\approx \\frac{u(x_k, y_j+h_y) - u(x_k, y_j-h_y)}{2h_y}$$\nThis is computed for $k \\in \\{i-1, i, i+1\\}$.\nNext, apply $D_x^{central}$ to the intermediate function $v$:\n$$\\left.\\frac{\\partial^2 u}{\\partial x \\partial y}\\right|_{i,j} \\approx \\frac{v(x_i+h_x, y_j) - v(x_i-h_x, y_j)}{2h_x}$$\nSubstituting the expression for $v$:\n$$\\left.\\frac{\\partial^2 u}{\\partial x \\partial y}\\right|_{i,j} \\approx \\frac{1}{2h_x} \\left( \\frac{u_{i+1,j+1} - u_{i+1,j-1}}{2h_y} - \\frac{u_{i-1,j+1} - u_{i-1,j-1}}{2h_y} \\right)$$\n$$\\left.\\frac{\\partial^2 u}{\\partial x \\partial y}\\right|_{i,j} \\approx \\frac{u_{i+1,j+1} - u_{i+1,j-1} - u_{i-1,j+1} + u_{i-1,j-1}}{4h_x h_y}$$\nThis is the standard nine-point stencil for the mixed derivative.\n\nThe truncation error $(\\tau)$ of this composed operator $D_x D_y$ can be analyzed. The operators are:\n$$\nD_x = \\frac{\\partial}{\\partial x} + C_x h_x^2 \\frac{\\partial^3}{\\partial x^3} + \\dots\n$$\n$$\nD_y = \\frac{\\partial}{\\partial y} + C_y h_y^2 \\frac{\\partial^3}{\\partial y^3} + \\dots\n$$\nwhere the constant $C$ is $-\\frac{1}{6}$ for central differences and $\\frac{1}{3}$ for one-sided differences. The composed operator is:\n$$D_x D_y = \\left(\\frac{\\partial}{\\partial x} + \\mathcal{O}(h_x^2)\\right) \\left(\\frac{\\partial}{\\partial y} + \\mathcal{O}(h_y^2)\\right) = \\frac{\\partial^2}{\\partial x \\partial y} + \\mathcal{O}(h_x^2, h_y^2)$$\nThe leading-order truncation error is $\\tau = C_y h_y^2 \\frac{\\partial^4 u}{\\partial x \\partial y^3} + C_x h_x^2 \\frac{\\partial^4 u}{\\partial x^3 \\partial y}$. As both individual operators are second-order accurate, their composition is also second-order accurate, both in the interior and at the boundaries.\n\nFor a polynomial function like Function B, $u(x,y) = x^2 y^2 + \\dots$, the relevant higher-order partial derivatives (e.g., $\\frac{\\partial^4 u}{\\partial x \\partial y^3}$ and $\\frac{\\partial^4 u}{\\partial x^3 \\partial y}$) are zero, since the total degree of the polynomial is four. One would expect high accuracy. In fact, our chosen one-dimensional, three-point finite difference operators are exact for polynomials of degree up to two. Let us examine the process for Function B. The function $u(x,y)$ is quadratic in $x$ for fixed $y$, and quadratic in $y$ for fixed $x$. The first differentiation, say $V=D_y u$, will be exact because $u$ is a quadratic polynomial in $y$. The result $V = \\frac{\\partial u}{\\partial y} = 2x^2 y + 2x + 10y$ is also a quadratic polynomial in $x$. Thus, the second differentiation, $D_x V$, will also be exact. The final computed value will equal the analytical derivative, and the error will be zero up to machine precision.\n\nThe implementation will follow this logic of composition. A function will compute the second-order 1D derivative along a given axis of a NumPy array. This function is then called sequentially for the y-axis and then the x-axis to produce the final result matrix. Test cases are then evaluated by generating the appropriate grids, computing the finite difference and exact derivatives, and comparing them as specified.",
            "answer": "```python\nimport numpy as np\n\ndef u_func_A(x, y, a, b):\n    \"\"\"\n    Computes the value of Function A: u(x,y) = sin(a*x) * exp(b*y).\n    \"\"\"\n    return np.sin(a * x) * np.exp(b * y)\n\ndef u_xy_exact_A(x, y, a, b):\n    \"\"\"\n    Computes the exact mixed partial derivative of Function A.\n    u_xy(x,y) = a*b*cos(a*x)*exp(b*y).\n    \"\"\"\n    return a * b * np.cos(a * x) * np.exp(b * y)\n\ndef u_func_B(x, y):\n    \"\"\"\n    Computes the value of Function B (polynomial).\n    u(x,y) = x^2*y^2 + 2*x*y + 3*x^2 + 5*y^2 + 7.\n    \"\"\"\n    return x**2 * y**2 + 2 * x * y + 3 * x**2 + 5 * y**2 + 7\n\ndef u_xy_exact_B(x, y):\n    \"\"\"\n    Computes the exact mixed partial derivative of Function B.\n    u_xy(x,y) = 4*x*y + 2.\n    \"\"\"\n    return 4 * x * y + 2\n\ndef compute_1d_derivative_array(f: np.ndarray, h: float) -> np.ndarray:\n    \"\"\"\n    Computes the second-order accurate derivative of a 1D array 'f'.\n    Uses central differences for interior and one-sided for boundaries.\n    \"\"\"\n    n = len(f)\n    if n  3:\n        raise ValueError(\"Input array must have at least 3 points for second-order boundary stencils.\")\n    \n    df = np.zeros_like(f, dtype=float)\n\n    # Second-order forward difference at the start (i=0)\n    df[0] = (-3.0 * f[0] + 4.0 * f[1] - f[2]) / (2.0 * h)\n\n    # Second-order central difference for the interior\n    if n > 2:\n        df[1:-1] = (f[2:] - f[:-2]) / (2.0 * h)\n\n    # Second-order backward difference at the end (i=N-1)\n    df[-1] = (3.0 * f[-1] - 4.0 * f[-2] + f[-3]) / (2.0 * h)\n\n    return df\n\ndef compute_mixed_derivative(U: np.ndarray, hx: float, hy: float) -> np.ndarray:\n    \"\"\"\n    Computes the mixed partial derivative d2u/dxdy using composition.\n    The input array U is assumed to have shape (Nx, Ny) where U[i, j] = u(x_i, y_j).\n    \"\"\"\n    # Step 1: Compute dU/dy by applying the 1D derivative along axis 1 (y-axis)\n    dU_dy = np.apply_along_axis(compute_1d_derivative_array, 1, U, hy)\n    \n    # Step 2: Compute d/dx(dU/dy) by applying the 1D derivative along axis 0 (x-axis)\n    d2U_dxdy = np.apply_along_axis(compute_1d_derivative_array, 0, dU_dy, hx)\n    \n    return d2U_dxdy\n\ndef solve():\n    \"\"\"\n    Main function to execute all test cases and print results.\n    \"\"\"\n    results = []\n    \n    a_param = 1.3\n    b_param = 0.7\n\n    # Case 1: Interior absolute error\n    Nx1, Ny1 = 41, 37\n    hx1, hy1 = 1.0 / (Nx1 - 1), 1.0 / (Ny1 - 1)\n    i1, j1 = 15, 17\n    x_coords1 = np.linspace(0, 1, Nx1)\n    y_coords1 = np.linspace(0, 1, Ny1)\n    X1, Y1 = np.meshgrid(x_coords1, y_coords1, indexing='ij')\n    U1 = u_func_A(X1, Y1, a_param, b_param)\n    \n    U1_xy_fd = compute_mixed_derivative(U1, hx1, hy1)\n    u1_xy_exact_val = u_xy_exact_A(X1[i1, j1], Y1[i1, j1], a_param, b_param)\n    \n    error1 = abs(U1_xy_fd[i1, j1] - u1_xy_exact_val)\n    results.append(error1)\n\n    # Case 2: Interior convergence ratio\n    # Coarse grid\n    N_coarse = 41\n    h_coarse = 1.0 / (N_coarse - 1)\n    i_coarse, j_coarse = 20, 20\n    x_coords_c = np.linspace(0, 1, N_coarse)\n    y_coords_c = np.linspace(0, 1, N_coarse)\n    Xc, Yc = np.meshgrid(x_coords_c, y_coords_c, indexing='ij')\n    Uc = u_func_A(Xc, Yc, a_param, b_param)\n    Uc_xy_fd = compute_mixed_derivative(Uc, h_coarse, h_coarse)\n    uc_xy_exact_val = u_xy_exact_A(Xc[i_coarse, j_coarse], Yc[i_coarse, j_coarse], a_param, b_param)\n    error_coarse = abs(Uc_xy_fd[i_coarse, j_coarse] - uc_xy_exact_val)\n\n    # Fine grid\n    N_fine = 81\n    h_fine = 1.0 / (N_fine - 1)\n    i_fine, j_fine = 40, 40\n    x_coords_f = np.linspace(0, 1, N_fine)\n    y_coords_f = np.linspace(0, 1, N_fine)\n    Xf, Yf = np.meshgrid(x_coords_f, y_coords_f, indexing='ij')\n    Uf = u_func_A(Xf, Yf, a_param, b_param)\n    Uf_xy_fd = compute_mixed_derivative(Uf, h_fine, h_fine)\n    uf_xy_exact_val = u_xy_exact_A(Xf[i_fine, j_fine], Yf[i_fine, j_fine], a_param, b_param)\n    error_fine = abs(Uf_xy_fd[i_fine, j_fine] - uf_xy_exact_val)\n    \n    ratio2 = error_coarse / error_fine if error_fine != 0 else np.nan\n    results.append(ratio2)\n\n    # Case 3: Corner convergence ratio\n    # Same grids as Case 2, but evaluate at i=0, j=0\n    i_corner, j_corner = 0, 0\n    error_coarse_corner = abs(Uc_xy_fd[i_corner, j_corner] - u_xy_exact_A(Xc[i_corner, j_corner], Yc[i_corner, j_corner], a_param, b_param))\n    error_fine_corner = abs(Uf_xy_fd[i_corner, j_corner] - u_xy_exact_A(Xf[i_corner, j_corner], Yf[i_corner, j_corner], a_param, b_param))\n    \n    ratio3 = error_coarse_corner / error_fine_corner if error_fine_corner != 0 else np.nan\n    results.append(ratio3)\n\n    # Case 4: Polynomial exactness check\n    Nx4, Ny4 = 50, 70\n    hx4, hy4 = 1.0 / (Nx4 - 1), 1.0 / (Ny4 - 1)\n    x_coords4 = np.linspace(0, 1, Nx4)\n    y_coords4 = np.linspace(0, 1, Ny4)\n    X4, Y4 = np.meshgrid(x_coords4, y_coords4, indexing='ij')\n    \n    U4 = u_func_B(X4, Y4)\n    U4_xy_fd = compute_mixed_derivative(U4, hx4, hy4)\n    U4_xy_exact = u_xy_exact_B(X4, Y4)\n    \n    max_error4 = np.max(np.abs(U4_xy_fd - U4_xy_exact))\n    results.append(max_error4)\n\n    # Case 5: Anisotropic grid interior error\n    Nx5, Ny5 = 33, 5\n    hx5, hy5 = 1.0 / (Nx5 - 1), 1.0 / (Ny5 - 1)\n    i5, j5 = 16, 2\n    x_coords5 = np.linspace(0, 1, Nx5)\n    y_coords5 = np.linspace(0, 1, Ny5)\n    X5, Y5 = np.meshgrid(x_coords5, y_coords5, indexing='ij')\n    U5 = u_func_A(X5, Y5, a_param, b_param)\n    \n    U5_xy_fd = compute_mixed_derivative(U5, hx5, hy5)\n    u5_xy_exact_val = u_xy_exact_A(X5[i5, j5], Y5[i5, j5], a_param, b_param)\n    \n    error5 = abs(U5_xy_fd[i5, j5] - u5_xy_exact_val)\n    results.append(error5)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}