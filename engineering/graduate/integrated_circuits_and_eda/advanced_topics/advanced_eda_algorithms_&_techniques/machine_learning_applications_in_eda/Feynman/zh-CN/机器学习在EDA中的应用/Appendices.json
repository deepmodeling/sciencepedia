{
    "hands_on_practices": [
        {
            "introduction": "机器学习模型的成功在很大程度上取决于其输入特征的质量。在电子设计自动化（EDA）领域，这意味着需要将复杂的物理或逻辑信息转化为模型可以理解的数值表示。本练习将指导您完成一项基础的特征工程任务：根据给定的单元布局计算不同信号网的半周长线长（HPWL），并将其进行归一化处理，为模型输入做好准备。",
            "id": "4281028",
            "problem": "考虑在电子设计自动化 (EDA) 中，从一个合法的单元布局中为机器学习 (ML) 构建输入特征。一组五个方向为 $N$（无旋转或镜像）的实例放置如下：实例 $U_1$ 位于 $(10,10)\\,\\mathrm{\\mu m}$，实例 $U_2$ 位于 $(16,13)\\,\\mathrm{\\mu m}$，实例 $U_3$ 位于 $(22,9)\\,\\mathrm{\\mu m}$，实例 $U_4$ 位于 $(15,18)\\,\\mathrm{\\mu m}$，实例 $U_5$ 位于 $(20,13)\\,\\mathrm{\\mu m}$。每个实例都有相对于实例原点的局部偏移量的引脚：在 $U_1$ 上，引脚 $A$ 的偏移量为 $(1,0)\\,\\mathrm{\\mu m}$，引脚 $B$ 的偏移量为 $(0,2)\\,\\mathrm{\\mu m}$；在 $U_2$ 上，引脚 $A$ 的偏移量为 $(-1,1)\\,\\mathrm{\\mu m}$，引脚 $B$ 的偏移量为 $(2,-1)\\,\\mathrm{\\mu m}$；在 $U_3$ 上，引脚 $A$ 的偏移量为 $(0,-2)\\,\\mathrm{\\mu m}$，引脚 $C$ 的偏移量为 $(3,1)\\,\\mathrm{\\mu m}$；在 $U_4$ 上，引脚 $C$ 的偏移量为 $(-2,2)\\,\\mathrm{\\mu m}$，引脚 $D$ 的偏移量为 $(1,-3)\\,\\mathrm{\\mu m}$；在 $U_5$ 上，引脚 $A$ 的偏移量为 $(0,0)\\,\\mathrm{\\mu m}$。由于方向为 $N$，引脚的绝对坐标是通过实例坐标和引脚局部偏移量的向量加法得到的。\n\n连接表如下：\n- 网络 $e_1$：引脚 $U_1/A$、$U_2/B$、$U_3/A$。\n- 网络 $e_2$：引脚 $U_1/B$、$U_4/C$。\n- 网络 $e_3$：引脚 $U_2/A$、$U_3/C$、$U_4/D$、$U_5/A$。\n- 网络 $e_4$：引脚 $U_1/A$、$U_2/A$、$U_5/A$。\n\n使用半周长线长 (HPWL) 的标准定义：对于一个引脚坐标为 $\\{(x_i,y_i)\\}_{i \\in e}$ 的网络 $e$，HPWL 特征 $w_e$ 是包含其所有引脚的最小轴对齐边界矩形的水平跨度和垂直跨度之和，即 $w_e$ 等于水平跨度 $\\max_{i \\in e} x_i - \\min_{i \\in e} x_i$ 加上垂直跨度 $\\max_{i \\in e} y_i - \\min_{i \\in e} y_i$。根据给定的布局和连接表，计算 HPWL 特征 $w_{e_1}$、$w_{e_2}$、$w_{e_3}$ 和 $w_{e_4}$。然后，使用样本均值和无偏样本标准差（即，对于 $n=4$，除以分母为 $n-1$ 的样本方差的平方根），对集合 $\\{w_{e_1},w_{e_2},w_{e_3},w_{e_4}\\}$ 中的 HPWL 特征进行 $z$-score 标准化。请以单一的封闭形式精确代数表达式给出网络 $e_3$ 的标准化 HPWL。标准化的 HPWL 是无单位的。不要对答案进行四舍五入。",
            "solution": "所述问题是有效的。它在科学上基于电子设计自动化 (EDA) 的原理和标准统计方法。半周长线长 (HPWL) 和 z-score 标准化的定义清晰且标准。所有必要的数据，包括实例布局、引脚偏移量和连接表，均已提供，且没有内部矛盾。该问题是适定的，并存在一个唯一的、可验证的解。\n\n解法通过以下步骤获得：\n$1$. 通过将局部引脚偏移量与其各自的实例坐标相加来确定所有引脚的绝对坐标。\n$2$. 计算四个网络中每一个的 HPWL 特征，记为 $w_{e_1}$、$w_{e_2}$、$w_{e_3}$ 和 $w_{e_4}$。\n$3$. 计算得到的 HPWL 值集合 $\\{w_{e_1}, w_{e_2}, w_{e_3}, w_{e_4}\\}$ 的样本均值 $\\bar{w}$ 和无偏样本标准差 $s$。\n$4$. 使用公式 $z_{e_3} = (w_{e_3} - \\bar{w}) / s$ 计算 $w_{e_3}$ 的 z-score。\n\n步骤 1：绝对引脚坐标\n所有坐标的单位均为 $\\mathrm{\\mu m}$。一个引脚的绝对坐标由实例坐标和引脚的局部偏移量的向量和给出。\n\n对于网络 $e_1$ (引脚 $U_1/A, U_2/B, U_3/A$):\n- 引脚 $U_1/A$: $(10,10) + (1,0) = (11,10)$\n- 引脚 $U_2/B$: $(16,13) + (2,-1) = (18,12)$\n- 引脚 $U_3/A$: $(22,9) + (0,-2) = (22,7)$\n\n对于网络 $e_2$ (引脚 $U_1/B, U_4/C$):\n- 引脚 $U_1/B$: $(10,10) + (0,2) = (10,12)$\n- 引脚 $U_4/C$: $(15,18) + (-2,2) = (13,20)$\n\n对于网络 $e_3$ (引脚 $U_2/A, U_3/C, U_4/D, U_5/A$):\n- 引脚 $U_2/A$: $(16,13) + (-1,1) = (15,14)$\n- 引脚 $U_3/C$: $(22,9) + (3,1) = (25,10)$\n- 引脚 $U_4/D$: $(15,18) + (1,-3) = (16,15)$\n- 引脚 $U_5/A$: $(20,13) + (0,0) = (20,13)$\n\n对于网络 $e_4$ (引脚 $U_1/A, U_2/A, U_5/A$):\n- 引脚 $U_1/A$: $(11,10)$\n- 引脚 $U_2/A$: $(15,14)$\n- 引脚 $U_5/A$: $(20,13)$\n\n步骤 2：各网络 HPWL 计算\nHPWL $w_e$ 定义为 $(\\max_{i \\in e} x_i - \\min_{i \\in e} x_i) + (\\max_{i \\in e} y_i - \\min_{i \\in e} y_i)$。\n\n对于网络 $e_1$，引脚坐标为 $\\{(11,10), (18,12), (22,7)\\}$:\n- 水平跨度: $\\max(11, 18, 22) - \\min(11, 18, 22) = 22 - 11 = 11$\n- 垂直跨度: $\\max(10, 12, 7) - \\min(10, 12, 7) = 12 - 7 = 5$\n- $w_{e_1} = 11 + 5 = 16$\n\n对于网络 $e_2$，引脚坐标为 $\\{(10,12), (13,20)\\}$:\n- 水平跨度: $\\max(10, 13) - \\min(10, 13) = 13 - 10 = 3$\n- 垂直跨度: $\\max(12, 20) - \\min(12, 20) = 20 - 12 = 8$\n- $w_{e_2} = 3 + 8 = 11$\n\n对于网络 $e_3$，引脚坐标为 $\\{(15,14), (25,10), (16,15), (20,13)\\}$:\n- 水平跨度: $\\max(15, 25, 16, 20) - \\min(15, 25, 16, 20) = 25 - 15 = 10$\n- 垂直跨度: $\\max(14, 10, 15, 13) - \\min(14, 10, 15, 13) = 15 - 10 = 5$\n- $w_{e_3} = 10 + 5 = 15$\n\n对于网络 $e_4$，引脚坐标为 $\\{(11,10), (15,14), (20,13)\\}$:\n- 水平跨度: $\\max(11, 15, 20) - \\min(11, 15, 20) = 20 - 11 = 9$\n- 垂直跨度: $\\max(10, 14, 13) - \\min(10, 14, 13) = 14 - 10 = 4$\n- $w_{e_4} = 9 + 4 = 13$\n\nHPWL 特征集合为 $W = \\{16, 11, 15, 13\\}$。\n\n步骤 3：统计计算\n我们有 $n=4$ 个 HPWL 值的样本。\n样本均值 $\\bar{w}$ 为：\n$$ \\bar{w} = \\frac{1}{4} (16 + 11 + 15 + 13) = \\frac{55}{4} $$\n无偏样本方差 $s^2$ 的计算分母为 $n-1 = 3$：\n$$ s^2 = \\frac{1}{3} \\sum_{w \\in W} (w - \\bar{w})^2 $$\n$$ s^2 = \\frac{1}{3} \\left[ \\left(16 - \\frac{55}{4}\\right)^2 + \\left(11 - \\frac{55}{4}\\right)^2 + \\left(15 - \\frac{55}{4}\\right)^2 + \\left(13 - \\frac{55}{4}\\right)^2 \\right] $$\n$$ s^2 = \\frac{1}{3} \\left[ \\left(\\frac{9}{4}\\right)^2 + \\left(-\\frac{11}{4}\\right)^2 + \\left(\\frac{5}{4}\\right)^2 + \\left(-\\frac{3}{4}\\right)^2 \\right] $$\n$$ s^2 = \\frac{1}{3} \\left[ \\frac{81}{16} + \\frac{121}{16} + \\frac{25}{16} + \\frac{9}{16} \\right] = \\frac{1}{3} \\left( \\frac{236}{16} \\right) = \\frac{1}{3} \\left( \\frac{59}{4} \\right) = \\frac{59}{12} $$\n无偏样本标准差 $s$ 是方差的平方根：\n$$ s = \\sqrt{\\frac{59}{12}} $$\n\n步骤 4：$w_{e_3}$ 的 Z-score 标准化\n$w_{e_3}=15$ 的 z-score 为：\n$$ z_{e_3} = \\frac{w_{e_3} - \\bar{w}}{s} = \\frac{15 - \\frac{55}{4}}{\\sqrt{\\frac{59}{12}}} = \\frac{\\frac{60-55}{4}}{\\sqrt{\\frac{59}{12}}} = \\frac{\\frac{5}{4}}{\\frac{\\sqrt{59}}{\\sqrt{12}}} $$\n简化表达式：\n$$ z_{e_3} = \\frac{5}{4} \\frac{\\sqrt{12}}{\\sqrt{59}} = \\frac{5}{4} \\frac{\\sqrt{4 \\cdot 3}}{\\sqrt{59}} = \\frac{5}{4} \\frac{2\\sqrt{3}}{\\sqrt{59}} = \\frac{5\\sqrt{3}}{2\\sqrt{59}} $$\n为了提供一个具有有理分母的封闭形式表达式，我们将分子和分母同乘以 $\\sqrt{59}$：\n$$ z_{e_3} = \\frac{5\\sqrt{3}}{2\\sqrt{59}} \\cdot \\frac{\\sqrt{59}}{\\sqrt{59}} = \\frac{5\\sqrt{3 \\cdot 59}}{2 \\cdot 59} = \\frac{5\\sqrt{177}}{118} $$\n这是最终的、精确的、无单位的值。",
            "answer": "$$\\boxed{\\frac{5\\sqrt{177}}{118}}$$"
        },
        {
            "introduction": "在完成特征工程之后，下一步便是利用这些特征来构建和评估预测模型。在EDA领域，一个常见的目标是加速耗时较长的黄金标准分析流程，例如静态时序分析（STA）。本练习中，您将使用一个预训练的线性模型，根据工程特征来预测时序路径的裕量，并使用平均绝对误差（Mean Absolute Error）来衡量其与静态时序分析（STA）基准值之间的准确性。",
            "id": "4281022",
            "problem": "一个电子设计自动化（EDA）的研究小组正在训练一个机器学习模型来预测路径时序裕度，以减少进行完整静态时序分析（STA）的频率。对于一个给定的设计，训练好的预测器是一个线性模型，它将一个工程特征向量映射到一个预测的裕度。该模型接受一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^{5}$、一个学习到的权重向量 $\\mathbf{w} \\in \\mathbb{R}^{5}$ 和一个学习到的偏置 $b \\in \\mathbb{R}$，并产生一个预测的裕度。这五个特征是根据路径特性设计的无量纲指数，并且在训练和推断过程中使用一致的内部尺度；例子包括驱动强度指数、扇出负载指数、转换率倒数指数、互连长度指数和高阈值单元比例指数。目标是通过将其预测结果与参考 STA 裕度进行比较，来评估该模型在三条未见过的路径上的表现。\n\n给定学习到的参数 $\\mathbf{w}$ 和 $b$：\n- $\\mathbf{w} = [\\,0.12,\\,-0.08,\\,0.50,\\,-0.02,\\,-0.40\\,]$，\n- $b = 0.05$。\n\n同时给定三个路径特征向量及其对应的 STA 裕度（参考值），所有裕度均以纳秒为单位：\n- 路径 $1$：$\\mathbf{x}^{(1)} = [\\,10,\\,5,\\,0.3,\\,20,\\,0.1\\,]$，STA 裕度 $s^{\\mathrm{STA}}_{1} = 0.60$。\n- 路径 $2$：$\\mathbf{x}^{(2)} = [\\,8,\\,7,\\,0.5,\\,15,\\,0.2\\,]$，STA 裕度 $s^{\\mathrm{STA}}_{2} = 0.28$。\n- 路径 $3$：$\\mathbf{x}^{(3)} = [\\,12,\\,3,\\,0.2,\\,25,\\,0.05\\,]$，STA 裕度 $s^{\\mathrm{STA}}_{3} = 0.70$。\n\n使用线性模型计算每条路径的预测裕度，然后计算三个预测值与三个 STA 裕度之间的平均绝对误差。以纳秒表示最终的平均绝对误差。将最终答案四舍五入到四位有效数字。",
            "solution": "问题陈述经过验证。\n**第1步：提取给定信息**\n- 机器学习模型：用于预测裕度 $s^{\\mathrm{pred}}$ 的线性模型。\n- 模型形式：$s^{\\mathrm{pred}} = \\mathbf{w}^T \\mathbf{x} + b$。\n- 特征向量：$\\mathbf{x} \\in \\mathbb{R}^{5}$。\n- 权重向量：$\\mathbf{w} \\in \\mathbb{R}^{5}$。\n- 偏置：$b \\in \\mathbb{R}$。\n- 学习到的权重向量：$\\mathbf{w} = [\\,0.12,\\,-0.08,\\,0.50,\\,-0.02,\\,-0.40\\,]$。\n- 学习到的偏置：$b = 0.05$。\n- 路径 $1$ 特征向量：$\\mathbf{x}^{(1)} = [\\,10,\\,5,\\,0.3,\\,20,\\,0.1\\,]$。\n- 路径 $1$ STA 裕度：$s^{\\mathrm{STA}}_{1} = 0.60$ ns。\n- 路径 $2$ 特征向量：$\\mathbf{x}^{(2)} = [\\,8,\\,7,\\,0.5,\\,15,\\,0.2\\,]$。\n- 路径 $2$ STA 裕度：$s^{\\mathrm{STA}}_{2} = 0.28$ ns。\n- 路径 $3$ 特征向量：$\\mathbf{x}^{(3)} = [\\,12,\\,3,\\,0.2,\\,25,\\,0.05\\,]$。\n- 路径 $3$ STA 裕度：$s^{\\mathrm{STA}}_{3} = 0.70$ ns。\n- 任务：计算每条路径的预测裕度，以及预测值与 STA 裕度之间的平均绝对误差（MAE）。\n- 最终答案要求：四舍五入到四位有效数字。\n\n**第2步：使用提取的给定信息进行验证**\n根据验证标准对问题进行评估。\n- **科学依据**：该问题基于线性回归这一基础机器学习技术在电子设计自动化（EDA）中预测时序裕度的标准应用。这是该领域一个成熟且有效的方法。\n- **适定性**：该问题是适定的。它提供了一个清晰的数学模型，所有必要的参数（$\\mathbf{w}$、$b$）和输入（$\\mathbf{x}^{(i)}$）的数值，以及待计算量（平均绝对误差）的精确定义。存在唯一解。\n- **客观性**：该问题使用客观、技术性的语言表述，没有歧义或主观论断。\n- **完整性与一致性**：该问题是自洽且一致的。所有必要的数据均已提供，给定信息中没有矛盾之处。向量的维度与指定运算兼容。\n\n**第3步：结论与行动**\n该问题被认为是**有效的**，因为它满足可解科学问题的所有标准。现在开始解题过程。\n\n预测裕度 $s^{\\mathrm{pred}}$ 使用线性模型计算：\n$$ s^{\\mathrm{pred}} = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{j=1}^{5} w_j x_j + b $$\n给定的参数是权重向量 $\\mathbf{w} = [\\,0.12,\\, -0.08,\\, 0.50,\\, -0.02,\\, -0.40\\,]^T$ 和偏置 $b = 0.05$。\n\n我们计算三条路径中每一条的预测裕度。\n\n对于路径 $1$，其特征向量为 $\\mathbf{x}^{(1)} = [\\,10,\\, 5,\\, 0.3,\\, 20,\\, 0.1\\,]^T$：\n$$ s^{\\mathrm{pred}}_{1} = \\mathbf{w}^T \\mathbf{x}^{(1)} + b $$\n$$ s^{\\mathrm{pred}}_{1} = (0.12)(10) + (-0.08)(5) + (0.50)(0.3) + (-0.02)(20) + (-0.40)(0.1) + 0.05 $$\n$$ s^{\\mathrm{pred}}_{1} = 1.20 - 0.40 + 0.15 - 0.40 - 0.04 + 0.05 $$\n$$ s^{\\mathrm{pred}}_{1} = 1.40 - 0.84 = 0.56 $$\n\n对于路径 $2$，其特征向量为 $\\mathbf{x}^{(2)} = [\\,8,\\, 7,\\, 0.5,\\, 15,\\, 0.2\\,]^T$：\n$$ s^{\\mathrm{pred}}_{2} = \\mathbf{w}^T \\mathbf{x}^{(2)} + b $$\n$$ s^{\\mathrm{pred}}_{2} = (0.12)(8) + (-0.08)(7) + (0.50)(0.5) + (-0.02)(15) + (-0.40)(0.2) + 0.05 $$\n$$ s^{\\mathrm{pred}}_{2} = 0.96 - 0.56 + 0.25 - 0.30 - 0.08 + 0.05 $$\n$$ s^{\\mathrm{pred}}_{2} = 1.26 - 0.94 = 0.32 $$\n\n对于路径 $3$，其特征向量为 $\\mathbf{x}^{(3)} = [\\,12,\\, 3,\\, 0.2,\\, 25,\\, 0.05\\,]^T$：\n$$ s^{\\mathrm{pred}}_{3} = \\mathbf{w}^T \\mathbf{x}^{(3)} + b $$\n$$ s^{\\mathrm{pred}}_{3} = (0.12)(12) + (-0.08)(3) + (0.50)(0.2) + (-0.02)(25) + (-0.40)(0.05) + 0.05 $$\n$$ s^{\\mathrm{pred}}_{3} = 1.44 - 0.24 + 0.10 - 0.50 - 0.02 + 0.05 $$\n$$ s^{\\mathrm{pred}}_{3} = 1.59 - 0.76 = 0.83 $$\n\n接下来，我们计算预测裕度（$s^{\\mathrm{pred}}_{i}$）和参考 STA 裕度（$s^{\\mathrm{STA}}_{i}$）之间的平均绝对误差（MAE）。对于 $N=3$ 个样本，MAE 的公式为：\n$$ \\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |s^{\\mathrm{pred}}_{i} - s^{\\mathrm{STA}}_{i}| $$\n给定的参考 STA 裕度为 $s^{\\mathrm{STA}}_{1} = 0.60$，$s^{\\mathrm{STA}}_{2} = 0.28$ 和 $s^{\\mathrm{STA}}_{3} = 0.70$。\n\n首先，我们计算每条路径的绝对误差：\n- 路径 $1$ 误差：$|s^{\\mathrm{pred}}_{1} - s^{\\mathrm{STA}}_{1}| = |0.56 - 0.60| = |-0.04| = 0.04$。\n- 路径 $2$ 误差：$|s^{\\mathrm{pred}}_{2} - s^{\\mathrm{STA}}_{2}| = |0.32 - 0.28| = |0.04| = 0.04$。\n- 路径 $3$ 误差：$|s^{\\mathrm{pred}}_{3} - s^{\\mathrm{STA}}_{3}| = |0.83 - 0.70| = |0.13| = 0.13$。\n\n现在，我们计算 MAE：\n$$ \\text{MAE} = \\frac{1}{3} (0.04 + 0.04 + 0.13) $$\n$$ \\text{MAE} = \\frac{1}{3} (0.21) $$\n$$ \\text{MAE} = 0.07 $$\n\n问题要求最终答案四舍五入到四位有效数字。计算值为 $0.07$。为了用四位有效数字表示，我们添加后缀零，得到 $0.07000$。所有裕度和误差的单位都是纳秒。",
            "answer": "$$\\boxed{0.07000}$$"
        },
        {
            "introduction": "尽管简单的特征向量在某些任务中很有效，但许多EDA问题本质上是由电路复杂的连接性定义的，而这种连接性最好用图（Graph）来表示。图神经网络（GNNs）已成为处理此类结构化数据的最先进工具。这个高级练习将带您实现一个双层GNN的前向传播过程，以预测信号网的关键度，让您亲身体验其核心的消息传递机制，从而揭开GNNs的神秘面纱。",
            "id": "4281004",
            "problem": "你的任务是在电子设计自动化 (EDA) 的背景下，实现一个两层图神经网络 (GNN) 来预测网络关键度。网络被表示为图中的有向边，该网络在有向图上执行消息传递以生成节点嵌入，然后进行边级别的读出，为每个网络产生一个标量分数。你必须针对下方指定的小型测试图，计算一个完整的前向传播，包括聚合和更新步骤。\n\n从图论和线性代数的基础开始，使用以下定义：\n\n- 设一个有向图表示为 $G = (V, E)$，其中 $V$ 是节点集合，$E \\subseteq V \\times V$ 是有向边集合。对于一个节点 $i \\in V$，其入邻居集合为 $\\mathcal{N}_{\\text{in}}(i) = \\{ j \\in V \\mid (j, i) \\in E \\}$。\n- 设 $X \\in \\mathbb{R}^{|V| \\times F}$ 为节点特征矩阵，其中 $F$ 是输入特征维度。\n- 定义一个对传入消息进行操作的聚合器：可以是求和聚合器或均值聚合器。对于均值聚合器，当 $\\mathcal{N}_{\\text{in}}(i)$ 为空时，定义其均值为 $\\mathbb{R}^{H}$ 中相应维度的零向量。\n- 第一层通过仿射自更新和聚合的邻居消息生成隐藏节点嵌入 $H^{(1)} \\in \\mathbb{R}^{|V| \\times H}$，然后经过一个修正线性单元 (ReLU) 非线性变换。第二层类似地生成 $H^{(2)} \\in \\mathbb{R}^{|V| \\times H_{2}}$。\n- 对于任意实数标量 $x$，ReLU 非线性变换按元素定义为 $\\sigma(x) = \\max(x, 0)$。\n- 对于每一层 $\\ell \\in \\{1, 2\\}$，使用一个自权重矩阵 $W_{s}^{(\\ell)}$、一个消息权重矩阵 $W_{m}^{(\\ell)}$ 和一个适当维度的偏置向量 $b^{(\\ell)}$。节点 $i$ 的更新规则是通过对节点的当前表示应用仿射自变换，并加上由消息权重矩阵变换后的入邻居的聚合消息，然后加上偏置向量并应用 ReLU 非线性变换来计算的。\n- 对于边 $(u,v) \\in E$，其网络（边）级别的读出分数 $s_{(u,v)}$ 是通过最终节点嵌入使用双线性形式和线性校正计算得出的：\n$$\ns_{(u,v)} = \\left(H^{(2)}_u\\right)^{\\top} R \\, H^{(2)}_v + c^{\\top} \\left(H^{(2)}_u + H^{(2)}_v\\right) + b,\n$$\n其中 $R \\in \\mathbb{R}^{H_{2} \\times H_{2}}$ 是一个读出矩阵，$c \\in \\mathbb{R}^{H_{2}}$ 是一个读出向量，$b \\in \\mathbb{R}$ 是一个标量偏置。\n\n使用上述定义，实现一个两层 GNN 的前向传播，该 GNN 计算 $H^{(1)}$，然后是 $H^{(2)}$，最后是边级别分数。你必须支持求和与均值两种聚合器，具体根据每个测试用例的指定。\n\n你的程序应该为下面指定的每个测试用例中的指定“目标网络”计算输出分数。程序的最终输出必须是单行，包含所有测试用例中目标网络分数的逗号分隔列表，并用方括号括起来。每个分数必须四舍五入到 $6$ 位小数。\n\n测试套件和参数：\n\n- 所有用例的通用维度：\n    - 输入特征维度 $F = 2$。\n    - 第一个隐藏层维度 $H = 3$。\n    - 第二个隐藏层维度 $H_{2} = 2$。\n\n- 所有测试用例使用的通用权重和偏置：\n    - 第一层消息权重：\n    $$\n    W_{m}^{(1)} = \\begin{bmatrix}\n    0.2  -0.1  0.0 \\\\\n    0.0  0.3  0.1\n    \\end{bmatrix}\n    $$\n    - 第一层自权重：\n    $$\n    W_{s}^{(1)} = \\begin{bmatrix}\n    -0.1  0.2  0.4 \\\\\n    0.5  -0.3  0.2\n    \\end{bmatrix}\n    $$\n    - 第一层偏置：\n    $$\n    b^{(1)} = \\begin{bmatrix}\n    0.0  0.1  -0.2\n    \\end{bmatrix}\n    $$\n    - 第二层消息权重：\n    $$\n    W_{m}^{(2)} = \\begin{bmatrix}\n    0.1  -0.2 \\\\\n    0.0  0.3 \\\\\n    -0.1  0.0\n    \\end{bmatrix}\n    $$\n    - 第二层自权重：\n    $$\n    W_{s}^{(2)} = \\begin{bmatrix}\n    0.2  0.1 \\\\\n    0.1  -0.1 \\\\\n    0.0  0.2\n    \\end{bmatrix}\n    $$\n    - 第二层偏置：\n    $$\n    b^{(2)} = \\begin{bmatrix}\n    0.05  -0.05\n    \\end{bmatrix}\n    $$\n    - 边读出矩阵：\n    $$\n    R = \\begin{bmatrix}\n    0.3  0.0 \\\\\n    0.0  -0.2\n    \\end{bmatrix}\n    $$\n    - 边读出向量：\n    $$\n    c = \\begin{bmatrix}\n    0.1  0.05\n    \\end{bmatrix}\n    $$\n    - 边读出偏置：\n    $$\n    b = -0.01\n    $$\n\n- 测试用例 1（正常路径，有向链）：\n    - 节点 $|V| = 3$，索引为 $0, 1, 2$。\n    - 边 $E = \\{ (0,1), (1,2) \\}$。\n    - 聚合器：求和 (sum)。\n    - 节点特征矩阵：\n    $$\n    X = \\begin{bmatrix}\n    0.5  -1.0 \\\\\n    1.0  0.0 \\\\\n    0.5  0.5\n    \\end{bmatrix}\n    $$\n    - 目标网络索引：$0$（边 $(0,1)$）。\n\n- 测试用例 2（自环和孤立节点，均值聚合）：\n    - 节点 $|V| = 4$，索引为 $0, 1, 2, 3$。\n    - 边 $E = \\{ (0,0), (0,1), (2,1) \\}$。\n    - 聚合器：均值 (mean)。\n    - 节点特征矩阵：\n    $$\n    X = \\begin{bmatrix}\n    0.0  1.0 \\\\\n    -0.5  0.5 \\\\\n    1.0  -1.0 \\\\\n    0.2  0.2\n    \\end{bmatrix}\n    $$\n    - 目标网络索引：$2$（边 $(2,1)$）。\n\n- 测试用例 3（用双向边建模的无向网络）：\n    - 节点 $|V| = 3$，索引为 $0, 1, 2$。\n    - 边 $E = \\{ (0,1), (1,0), (1,2), (2,1) \\}$。\n    - 聚合器：求和 (sum)。\n    - 节点特征矩阵：\n    $$\n    X = \\begin{bmatrix}\n    1.0  1.0 \\\\\n    0.0  1.0 \\\\\n    -1.0  0.5\n    \\end{bmatrix}\n    $$\n    - 目标网络索引：$1$（边 $(1,0)$）。\n\n- 测试用例 4（零特征的边界情况）：\n    - 节点 $|V| = 2$，索引为 $0, 1$。\n    - 边 $E = \\{ (0,1) \\}$。\n    - 聚合器：求和 (sum)。\n    - 节点特征矩阵：\n    $$\n    X = \\begin{bmatrix}\n    0.0  0.0 \\\\\n    0.0  0.0\n    \\end{bmatrix}\n    $$\n    - 目标网络索引：$0$（边 $(0,1)$）。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[r_1, r_2, r_3, r_4]$），其中 $r_k$ 是测试用例 $k$ 中目标网络的分数。每个 $r_k$ 必须四舍五入到 $6$ 位小数。不应打印任何额外文本。",
            "solution": "用户提供了一个定义明确的问题，即计算一个两层图神经网络 (GNN) 的前向传播和一个边级别分数。该问题经验证具有科学依据、定义良好且内部一致，包含了所有必要的参数和规范。解决方案通过逐步实现定义的数学运算来进行。\n\n设图为 $G=(V, E)$，节点特征为 $X \\in \\mathbb{R}^{|V| \\times F}$。GNN 架构由两个消息传递层和一个边级别的读出函数组成。\n\n**GNN 层更新规则**\n\n对于每一层 $\\ell \\in \\{1, 2\\}$，节点 $i \\in V$ 的隐藏表示 $H_i^{(\\ell)}$ 是根据其上一层的表示 $H_i^{(\\ell-1)}$ 和其入邻居 $\\mathcal{N}_{\\text{in}}(i) = \\{j \\in V \\mid (j,i) \\in E\\}$ 的表示来计算的。第一层的输入是节点特征矩阵，因此 $H^{(0)} = X$。\n\n在层 $\\ell$ 处节点 $i$ 的更新方程为：\n$$\nH_i^{(\\ell)} = \\sigma \\left( \\left( H_i^{(\\ell-1)} \\right)^{\\top} W_{s}^{(\\ell)} + A_i^{(\\ell)} + b^{(\\ell)} \\right)\n$$\n这种形式略显非传统。更标准的表示方法假设 $H_i$ 是行向量。让我们使用行向量表示法重述这些运算，这在深度学习库中是标准做法，并能简化矩阵代数表示法。设 $h_i^{(\\ell-1)}$ 是来自层 $\\ell-1$ 的节点 $i$ 的行向量。更新过程为：\n$$\nh_i^{(\\ell)} = \\sigma \\left( h_i^{(\\ell-1)} W_{s}^{(\\ell)} + A_i^{(\\ell)} + b^{(\\ell)} \\right)\n$$\n其中 $\\sigma(x) = \\max(x, 0)$ 是逐元素的 ReLU 激活函数，$W_s^{(\\ell)}$ 和 $W_m^{(\\ell)}$ 分别是层 $\\ell$ 的自权重和消息权重矩阵，$b^{(\\ell)}$ 是偏置向量。项 $A_i^{(\\ell)}$ 表示来自入邻居的聚合消息：\n$$\nA_i^{(\\ell)} = \\text{AGG}_{j \\in \\mathcal{N}_{\\text{in}}(i)} \\left( h_j^{(\\ell-1)} W_{m}^{(\\ell)} \\right)\n$$\n\n**聚合函数**\n\n问题指定了两种聚合器：`sum` (求和) 和 `mean` (均值)。\n1.  **求和聚合器 (Sum Aggregator)**：对传入的消息求和。\n    $$\n    A_i^{(\\ell)} = \\sum_{j \\in \\mathcal{N}_{\\text{in}}(i)} h_j^{(\\ell-1)} W_{m}^{(\\ell)}\n    $$\n2.  **均值聚合器 (Mean Aggregator)**：对传入的消息求平均值。\n    $$\n    A_i^{(\\ell)} = \\frac{1}{|\\mathcal{N}_{\\text{in}}(i)|} \\sum_{j \\in \\mathcal{N}_{\\text{in}}(i)} h_j^{(\\ell-1)} W_{m}^{(\\ell)}\n    $$\n在两种情况下，如果入邻居集合 $\\mathcal{N}_{\\text{in}}(i)$ 为空，则聚合消息 $A_i^{(\\ell)}$ 是一个相应维度（$\\ell=1$ 时为 $H$，$\\ell=2$ 时为 $H_2$）的零向量。\n\n**边读出函数**\n\n在计算出第二层的最终节点嵌入 $H^{(2)}$ 后，计算从节点 $u$ 到节点 $v$ 的网络（边）的关键度分数 $s_{(u,v)}$。设 $h_u^{(2)}$ 和 $h_v^{(2)}$ 分别是节点 $u$ 和 $v$ 的最终行向量嵌入。该分数由一个双线性和线性形式给出：\n$$\ns_{(u,v)} = h_u^{(2)} R \\left(h_v^{(2)}\\right)^{\\top} + \\left( h_u^{(2)} + h_v^{(2)} \\right) c^{\\top} + b\n$$\n其中 $R$ 是读出矩阵，$c$ 是读出向量（此处为与问题定义保持一致而表示为行向量，需要转置），$b$ 是标量偏置。原始问题陈述在读出部分对嵌入使用了列向量表示法，$s_{(u,v)} = (H^{(2)}_u)^{\\top} R H^{(2)}_v + c^{\\top}(H^{(2)}_u+H^{(2)}_v)+b$。当使用 `numpy` 中的一维数组实现时，两种表述在数值上是等价的。\n\n**计算步骤**\n\n对每个测试用例执行前向传播，步骤如下：\n\n1.  **初始化**：给定图结构 $(V, E)$ 和节点特征 $X$，为每个节点 $i$ 确定其入邻居 $\\mathcal{N}_{\\text{in}}(i)$。设置 $H^{(0)} = X$。\n2.  **第一层计算**：对于每个节点 $i \\in V$，使用层更新规则（$\\ell=1$），权重 $W_s^{(1)}$、$W_m^{(1)}$、偏置 $b^{(1)}$ 以及指定的聚合器，计算隐藏状态 $h_i^{(1)}$。这将生成矩阵 $H^{(1)} \\in \\mathbb{R}^{|V| \\times H}$。\n3.  **第二层计算**：对于每个节点 $i \\in V$，使用层更新规则（$\\ell=2$），以 $H^{(1)}$ 为输入，权重为 $W_s^{(2)}$、$W_m^{(2)}$、偏置为 $b^{(2)}$，计算最终嵌入 $h_i^{(2)}$。这将生成最终嵌入矩阵 $H^{(2)} \\in \\mathbb{R}^{|V| \\times H_2}$。\n4.  **分数计算**：确定当前测试用例的目标网络 $(u,v)$。检索其最终节点嵌入 $h_u^{(2)}$ 和 $h_v^{(2)}$。使用带有参数 $R$、$c$ 和 $b$ 的读出公式计算最终分数 $s_{(u,v)}$。\n5.  **最终输出**：收集所有测试用例的分数，四舍五入到 $6$ 位小数，并格式化为所需的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # --- Common Dimensions ---\n    # F = 2, H = 3, H2 = 2\n\n    # --- Common Weights and Biases ---\n    W_m1 = np.array([\n        [0.2, -0.1, 0.0],\n        [0.0, 0.3, 0.1]\n    ])\n    W_s1 = np.array([\n        [-0.1, 0.2, 0.4],\n        [0.5, -0.3, 0.2]\n    ])\n    b1 = np.array([0.0, 0.1, -0.2])\n\n    W_m2 = np.array([\n        [0.1, -0.2],\n        [0.0, 0.3],\n        [-0.1, 0.0]\n    ])\n    W_s2 = np.array([\n        [0.2, 0.1],\n        [0.1, -0.1],\n        [0.0, 0.2]\n    ])\n    b2 = np.array([0.05, -0.05])\n\n    R = np.array([\n        [0.3, 0.0],\n        [0.0, -0.2]\n    ])\n    c = np.array([0.1, 0.05])\n    b_readout = -0.01\n\n    def relu(x):\n        return np.maximum(x, 0)\n\n    def compute_gnn_pass(num_nodes, edges, X, aggregator, target_net_index):\n        # --- Pre-compute in-neighbors for all nodes ---\n        in_neighbors = {i: [] for i in range(num_nodes)}\n        for u, v in edges:\n            in_neighbors[v].append(u)\n\n        # --- Layer 1 ---\n        H0 = X\n        H1 = np.zeros((num_nodes, 3)) # H = 3\n        for i in range(num_nodes):\n            # Self-transformation\n            self_term = H0[i] @ W_s1\n\n            # Message aggregation\n            if not in_neighbors[i]:\n                agg_messages = np.zeros(3)\n            else:\n                messages = np.array([H0[j] @ W_m1 for j in in_neighbors[i]])\n                if aggregator == 'sum':\n                    agg_messages = np.sum(messages, axis=0)\n                elif aggregator == 'mean':\n                    agg_messages = np.mean(messages, axis=0)\n            \n            pre_activation = self_term + agg_messages + b1\n            H1[i] = relu(pre_activation)\n        \n        # --- Layer 2 ---\n        H2 = np.zeros((num_nodes, 2)) # H2 = 2\n        for i in range(num_nodes):\n            # Self-transformation\n            self_term = H1[i] @ W_s2\n\n            # Message aggregation\n            if not in_neighbors[i]:\n                agg_messages = np.zeros(2)\n            else:\n                messages = np.array([H1[j] @ W_m2 for j in in_neighbors[i]])\n                if aggregator == 'sum':\n                    agg_messages = np.sum(messages, axis=0)\n                elif aggregator == 'mean':\n                    agg_messages = np.mean(messages, axis=0)\n\n            pre_activation = self_term + agg_messages + b2\n            H2[i] = relu(pre_activation)\n            \n        # --- Edge Readout ---\n        u, v = edges[target_net_index]\n        h_u = H2[u]\n        h_v = H2[v]\n\n        bilinear_term = h_u @ R @ h_v\n        linear_term = (h_u + h_v) @ c\n        score = bilinear_term + linear_term + b_readout\n\n        return score\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"num_nodes\": 3,\n            \"edges\": [(0, 1), (1, 2)],\n            \"aggregator\": \"sum\",\n            \"X\": np.array([[0.5, -1.0], [1.0, 0.0], [0.5, 0.5]]),\n            \"target_net_index\": 0\n        },\n        {\n            \"num_nodes\": 4,\n            \"edges\": [(0, 0), (0, 1), (2, 1)],\n            \"aggregator\": \"mean\",\n            \"X\": np.array([[0.0, 1.0], [-0.5, 0.5], [1.0, -1.0], [0.2, 0.2]]),\n            \"target_net_index\": 2\n        },\n        {\n            \"num_nodes\": 3,\n            \"edges\": [(0, 1), (1, 0), (1, 2), (2, 1)],\n            \"aggregator\": \"sum\",\n            \"X\": np.array([[1.0, 1.0], [0.0, 1.0], [-1.0, 0.5]]),\n            \"target_net_index\": 1\n        },\n        {\n            \"num_nodes\": 2,\n            \"edges\": [(0, 1)],\n            \"aggregator\": \"sum\",\n            \"X\": np.array([[0.0, 0.0], [0.0, 0.0]]),\n            \"target_net_index\": 0\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        score = compute_gnn_pass(\n            case[\"num_nodes\"],\n            case[\"edges\"],\n            case[\"X\"],\n            case[\"aggregator\"],\n            case[\"target_net_index\"]\n        )\n        results.append(round(score, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}