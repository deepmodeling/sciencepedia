{
    "hands_on_practices": [
        {
            "introduction": "A core task in TI-ADC calibration is to design digital FIR filters that compensate for per-channel gain and bandwidth mismatches. This practice guides you through a fundamental, measurement-based approach where you will use the least-squares method to compute optimal filter coefficients. By minimizing the error between each channel's response and a common reference, you will gain hands-on experience in the direct synthesis of equalization filters from system identification data .",
            "id": "4306021",
            "problem": "A time-interleaved Analog-to-Digital Converter (ADC) comprises multiple parallel channels whose individual linear time-invariant responses introduce gain and bandwidth mismatches that degrade overall performance. Let there be $M$ channels indexed by $k \\in \\{0,1,\\dots,M-1\\}$. Each channel is modeled as a discrete-time, causal, linear time-invariant system with impulse response $h_k[n]$ measured over a finite window $n \\in \\{0,1,\\dots,L_h-1\\}$. The goal is to design, for each channel, a Finite Impulse Response (FIR) equalizer $F_k(z)$ of length $L_f$ (coefficients $f_k[n]$, $n \\in \\{0,1,\\dots,L_f-1\\}$) that minimizes the mismatch across channels within a specified passband by solving a least-squares problem using the measured impulse responses. Then, quantify the expected residual mismatch across the band after equalization.\n\nStart from the following fundamental definitions:\n\n1. Discrete-time convolution: the output of a linear time-invariant system with impulse response $h[n]$ to input $x[n]$ is $y[n] = \\sum_{m=-\\infty}^{\\infty} h[m] x[n-m]$.\n2. Discrete-time Fourier transform over frequency sample $ \\omega $: the frequency response is $H(\\mathrm{e}^{j\\omega}) = \\sum_{n=-\\infty}^{\\infty} h[n] \\mathrm{e}^{-j\\omega n}$, specialized here to finite $n$ support.\n3. Least-squares optimality: Given complex-valued linear model $A \\mathbf{f} \\approx \\mathbf{b}$ with $A \\in \\mathbb{C}^{P \\times Q}$, the minimizer is given by the solution to $\\min_{\\mathbf{f} \\in \\mathbb{R}^{Q}} \\|A \\mathbf{f} - \\mathbf{b}\\|_2^2$, implementable via real-valued stacking of real and imaginary parts.\n\nDefine the per-channel measured impulse response model as a first-order low-pass with gain variation and a discrete pole, plus deterministic measurement noise:\n$$\nh_k[n] = g_k (1 - p_k) p_k^{n} + v_k[n], \\quad n \\in \\{0,1,\\dots,L_h-1\\},\n$$\nwhere $g_k > 0$ is a dimensionless gain, $p_k \\in (0,1)$ is the discrete-time pole, and $v_k[n]$ is a deterministic noise sequence\n$$\nv_k[n] = \\sigma \\,\\sin\\!\\left(\\frac{2\\pi (n+1)}{L_h+1}\\right) \\cos\\!\\left(\\frac{2\\pi (k+1)}{M+1}\\right),\n$$\nwith $\\sigma \\ge 0$ a dimensionless noise amplitude. All angles are in radians. The finite-length measured frequency response at frequency samples $\\{\\omega_m\\}_{m=0}^{M_f-1}$ is computed by\n$$\nH_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_h-1} h_k[n]\\, \\mathrm{e}^{-j\\omega_m n}.\n$$\nLet the common reference be the per-frequency average across channels\n$$\nH_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) = \\frac{1}{M} \\sum_{k=0}^{M-1} H_k(\\mathrm{e}^{j\\omega_m}).\n$$\nFor each channel $k$, design the FIR equalizer $F_k(z)$ of length $L_f$ with coefficients $\\{f_k[n]\\}_{n=0}^{L_f-1}$, whose frequency response is\n$$\nF_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_f-1} f_k[n]\\, \\mathrm{e}^{-j\\omega_m n},\n$$\nby solving the least-squares problem over the passband samples:\n$$\n\\min_{\\{f_k[n]\\}} \\sum_{m=0}^{M_f-1} \\left| F_k(\\mathrm{e}^{j\\omega_m})\\, H_k(\\mathrm{e}^{j\\omega_m}) - H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right|^2.\n$$\nThis can be expressed as a linear system in the real domain by stacking real and imaginary parts.\n\nAfter computing the equalizers, define the equalized per-channel response\n$$\nG_k(\\mathrm{e}^{j\\omega_m}) = F_k(\\mathrm{e}^{j\\omega_m})\\, H_k(\\mathrm{e}^{j\\omega_m}),\n$$\nand quantify the residual mismatch across the band by the root-mean-square normalized error\n$$\nr = \\sqrt{ \\frac{1}{M M_f} \\sum_{k=0}^{M-1} \\sum_{m=0}^{M_f-1} \\left( \\frac{ \\left| G_k(\\mathrm{e}^{j\\omega_m}) - H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right| }{ \\max\\left( \\left| H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right|, \\varepsilon \\right) } \\right)^2 },\n$$\nwhere $\\varepsilon$ is a small positive number to avoid division by zero. Angles must be in radians and all amplitudes are dimensionless.\n\nYour task is to implement this computation for the following test suite of parameter sets. For each case, generate $\\{h_k[n]\\}$ using the specified parameters, compute $\\{H_k(\\mathrm{e}^{j\\omega_m})\\}$ at uniformly spaced samples in the passband, design $\\{F_k\\}$ via least squares, form $\\{G_k\\}$, and return the scalar $r$.\n\nTest Suite:\n- Case $1$ (happy path): $M=4$, $L_h=64$, $L_f=7$, $M_f=257$, $\\Omega_b=0.45\\pi$, $g_k \\in \\{1.00, 1.02, 0.98, 1.01\\}$, $p_k \\in \\{0.90, 0.88, 0.92, 0.91\\}$, $\\sigma=0.0001$.\n- Case $2$ (higher mismatch, shorter equalizer): $M=8$, $L_h=64$, $L_f=3$, $M_f=257$, $\\Omega_b=0.45\\pi$, $g_k \\in \\{1.00, 1.05, 0.95, 1.02, 0.97, 1.03, 0.94, 1.01\\}$, $p_k \\in \\{0.85, 0.90, 0.93, 0.88, 0.92, 0.86, 0.94, 0.89\\}$, $\\sigma=0.0005$.\n- Case $3$ (boundary, scalar gain equalization): $M=4$, $L_h=64$, $L_f=1$, $M_f=257$, $\\Omega_b=0.40\\pi$, $g_k \\in \\{1.04, 0.96, 1.00, 1.02\\}$, $p_k \\in \\{0.93, 0.89, 0.91, 0.92\\}$, $\\sigma=0.0001$.\n\nFrequency sampling must be uniform over $[0, \\Omega_b]$ with $\\omega_m = \\frac{m}{M_f-1} \\Omega_b$ for $m \\in \\{0,1,\\dots,M_f-1\\}$. Angles are in radians. Express the final output as a single line containing the three residual mismatch values for the cases above, as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3]$, where each $r_i$ is a floating-point number.",
            "solution": "The problem is addressed by combining linear time-invariant system theory, discrete-time Fourier analysis, and least-squares optimization.\n\nFirst principles and modeling:\n- Each channel is modeled as a discrete-time linear time-invariant system with impulse response $h_k[n]$ whose measured samples are finite-length and contaminated by deterministic measurement noise $v_k[n]$. The discrete-time frequency response at a finite set of angles $\\omega_m$ is $H_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_h-1} h_k[n] \\mathrm{e}^{-j\\omega_m n}$.\n- The equalizer is a finite impulse response filter $F_k(z)$ of length $L_f$ with coefficients $f_k[n]$, having frequency response $F_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_f-1} f_k[n] \\mathrm{e}^{-j\\omega_m n}$. The total equalized response per channel is $G_k(\\mathrm{e}^{j\\omega_m}) = F_k(\\mathrm{e}^{j\\omega_m}) H_k(\\mathrm{e}^{j\\omega_m})$.\n- To align channels, we define a reference $H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m})$ as the average of the measured responses across channels at each frequency, which is a common choice to avoid excessive amplification or deamplification relative to the ensemble.\n\nLeast-squares formulation:\n- The design objective is to minimize the sum of squared frequency-domain errors across the passband samples:\n$$\nJ_k(\\mathbf{f}_k) = \\sum_{m=0}^{M_f-1} \\left| F_k(\\mathrm{e}^{j\\omega_m}) H_k(\\mathrm{e}^{j\\omega_m}) - H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right|^2,\n$$\nwhere $\\mathbf{f}_k = [f_k[0], f_k[1], \\dots, f_k[L_f-1]]^{\\top}$.\n- Introduce the complex-valued design matrix $A_k \\in \\mathbb{C}^{M_f \\times L_f}$ with entries\n$$\n[A_k]_{m,n} = H_k(\\mathrm{e}^{j\\omega_m}) \\mathrm{e}^{-j \\omega_m n}.\n$$\nThen the model reads, per frequency $m$,\n$$\n\\sum_{n=0}^{L_f-1} [A_k]_{m,n} f_k[n] \\approx H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}),\n$$\nwhich is $A_k \\mathbf{f}_k \\approx \\mathbf{b}$ with $\\mathbf{b} = [H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_0}), \\dots, H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_{M_f-1}})]^{\\top}$.\n- Since the coefficients $f_k[n]$ are real, we recast the complex least-squares problem in the real domain by stacking real and imaginary parts:\n$$\n\\min_{\\mathbf{f}_k \\in \\mathbb{R}^{L_f}} \\left\\| \n\\begin{bmatrix}\n\\Re\\{A_k\\}\\\\\n\\Im\\{A_k\\}\n\\end{bmatrix} \\mathbf{f}_k - \n\\begin{bmatrix}\n\\Re\\{\\mathbf{b}\\}\\\\\n\\Im\\{\\mathbf{b}\\}\n\\end{bmatrix}\n\\right\\|_2^2.\n$$\nThis ensures computational tractability using standard real-valued least-squares solvers.\n\nAlgorithmic steps:\n1. For each test case, construct $\\omega_m$ as $M_f$ uniformly spaced samples over $[0, \\Omega_b]$ using $\\omega_m = \\frac{m}{M_f-1} \\Omega_b$.\n2. Generate measured impulse responses $h_k[n] = g_k (1-p_k) p_k^n + v_k[n]$, with $v_k[n]$ as specified, for $n \\in \\{0,\\dots,L_h-1\\}$ and $k \\in \\{0,\\dots,M-1\\}$.\n3. Compute frequency responses $H_k(\\mathrm{e}^{j\\omega_m})$ directly from the measured $h_k[n]$ by $H_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_h-1} h_k[n] \\mathrm{e}^{-j\\omega_m n}$. Implement as a matrix-vector multiplication where the matrix entries are $\\mathrm{e}^{-j\\omega_m n}$.\n4. Form $H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) = \\frac{1}{M} \\sum_{k=0}^{M-1} H_k(\\mathrm{e}^{j\\omega_m})$.\n5. For each channel $k$, build $A_k$ with $[A_k]_{m,n} = H_k(\\mathrm{e}^{j\\omega_m}) \\mathrm{e}^{-j \\omega_m n}$, and solve the real-valued least-squares system\n$$\n\\begin{bmatrix}\n\\Re\\{A_k\\}\\\\\n\\Im\\{A_k\\}\n\\end{bmatrix} \\mathbf{f}_k \\approx \n\\begin{bmatrix}\n\\Re\\{H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m})\\}\\\\\n\\Im\\{H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m})\\}\n\\end{bmatrix}\n$$\nto obtain the FIR coefficients $\\mathbf{f}_k$.\n6. Evaluate $F_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_f-1} f_k[n] \\mathrm{e}^{-j\\omega_m n}$, and then compute $G_k(\\mathrm{e}^{j\\omega_m}) = F_k(\\mathrm{e}^{j\\omega_m}) H_k(\\mathrm{e}^{j\\omega_m})$.\n7. Compute the residual mismatch metric\n$$\nr = \\sqrt{ \\frac{1}{M M_f} \\sum_{k=0}^{M-1} \\sum_{m=0}^{M_f-1} \\left( \\frac{ \\left| G_k(\\mathrm{e}^{j\\omega_m}) - H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right| }{ \\max\\left( \\left| H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right|, \\varepsilon \\right) } \\right)^2 },\n$$\nwith a small $\\varepsilon$ (for example, $\\varepsilon = 10^{-12}$) to avoid division by zero.\n\nInterpretation:\n- If $L_f$ is sufficiently large relative to the variation in $H_k(\\mathrm{e}^{j\\omega})$ across frequency, the least-squares solution closely approximates $F_k(\\mathrm{e}^{j\\omega}) \\approx \\frac{H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega})}{H_k(\\mathrm{e}^{j\\omega})}$ and the residual $r$ is small.\n- In the boundary case $L_f=1$, the equalizer reduces to a scalar gain per channel, so bandwidth mismatches cannot be compensated, increasing $r$.\n\nThe program constructs the three cases, executes these steps, and outputs $[r_1,r_2,r_3]$ as specified, with angles in radians and amplitudes dimensionless.",
            "answer": "```python\nimport numpy as np\n\ndef generate_impulse_responses(M, Lh, g_list, p_list, sigma):\n    \"\"\"\n    Generate measured impulse responses h_k[n] = g_k*(1-p_k)*p_k^n + v_k[n],\n    where v_k[n] is a deterministic \"noise\" sequence per the problem statement.\n    \"\"\"\n    h = np.zeros((M, Lh), dtype=float)\n    n = np.arange(Lh)\n    for k in range(M):\n        gk = g_list[k]\n        pk = p_list[k]\n        ideal = gk * (1.0 - pk) * (pk ** n)\n        # Deterministic noise as specified\n        vk = sigma * np.sin(2.0 * np.pi * (n + 1) / (Lh + 1)) * np.cos(2.0 * np.pi * (k + 1) / (M + 1))\n        h[k, :] = ideal + vk\n    return h\n\ndef frequency_samples(Mf, Omega_b):\n    \"\"\"\n    Uniform frequency sampling over [0, Omega_b] in radians.\n    \"\"\"\n    m = np.arange(Mf)\n    return (m / (Mf - 1)) * Omega_b\n\ndef compute_frequency_response(h, omega):\n    \"\"\"\n    Compute H_k(e^{j omega_m}) = sum_{n=0}^{Lh-1} h_k[n] e^{-j omega_m n}\n    for each channel k and frequency sample omega_m.\n    Returns H of shape (M, Mf) complex.\n    \"\"\"\n    M, Lh = h.shape\n    Mf = omega.shape[0]\n    # Precompute the exponential matrix E[m, n] = e^{-j omega[m] n}\n    n = np.arange(Lh)\n    # Broadcast: omega[:, None] * n[None, :]\n    E = np.exp(-1j * (omega[:, None] * n[None, :]))  # shape (Mf, Lh)\n    # For each channel k, H_k = E @ h_k\n    H = np.zeros((M, Mf), dtype=complex)\n    for k in range(M):\n        H[k, :] = E @ h[k, :]\n    return H\n\ndef design_equalizer_ls(Hk, Href, omega, Lf):\n    \"\"\"\n    For a single channel k, design FIR f_k of length Lf minimizing\n    ||A f - b||^2 where A[m, n] = Hk[m] * e^{-j omega[m] n}, b = Href[m].\n    Returns the real-valued FIR coefficients f_k[n].\n    \"\"\"\n    Mf = omega.shape[0]\n    n = np.arange(Lf)\n    # Build A complex matrix of shape (Mf, Lf)\n    A = (Hk[:, None]) * np.exp(-1j * (omega[:, None] * n[None, :]))\n    b = Href\n    # Stack real and imaginary parts to get real-valued least squares\n    Ar = np.vstack([A.real, A.imag])\n    br = np.hstack([b.real, b.imag])\n    # Solve least squares\n    f, *_ = np.linalg.lstsq(Ar, br, rcond=None)\n    return f  # real-valued coefficients\n\ndef evaluate_equalized_response(f, Hk, omega):\n    \"\"\"\n    Compute F_k(e^{j omega}) from FIR taps f, then G_k = F_k * Hk.\n    \"\"\"\n    Lf = f.shape[0]\n    n = np.arange(Lf)\n    Mf = omega.shape[0]\n    # Compute F_k(e^{j omega}) at samples omega\n    Fw = np.exp(-1j * (omega[:, None] * n[None, :])) @ f\n    Gk = Fw * Hk\n    return Gk, Fw\n\ndef residual_mismatch_metric(G_all, Href, eps=1e-12):\n    \"\"\"\n    Compute r = sqrt( mean_{k,m} ( |G_k - Href| / max(|Href|, eps) )^2 ).\n    G_all: array shape (M, Mf), complex\n    Href: array shape (Mf,), complex\n    \"\"\"\n    M, Mf = G_all.shape\n    denom = np.maximum(np.abs(Href), eps)\n    err_sq = ((np.abs(G_all - Href[None, :]) / denom[None, :]) ** 2)\n    r = np.sqrt(err_sq.mean())\n    return r\n\ndef run_case(M, Lh, Lf, Mf, Omega_b, g_list, p_list, sigma):\n    # Generate measured impulses\n    h = generate_impulse_responses(M, Lh, g_list, p_list, sigma)\n    # Frequency samples\n    omega = frequency_samples(Mf, Omega_b)\n    # Frequency responses per channel\n    H = compute_frequency_response(h, omega)  # shape (M, Mf)\n    # Reference frequency response\n    Href = H.mean(axis=0)\n    # Design equalizers per channel and evaluate equalized responses\n    G_all = np.zeros_like(H)\n    for k in range(M):\n        f_k = design_equalizer_ls(H[k, :], Href, omega, Lf)\n        Gk, _ = evaluate_equalized_response(f_k, H[k, :], omega)\n        G_all[k, :] = Gk\n    # Residual mismatch metric\n    r = residual_mismatch_metric(G_all, Href, eps=1e-12)\n    return float(r)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"M\": 4,\n            \"Lh\": 64,\n            \"Lf\": 7,\n            \"Mf\": 257,\n            \"Omega_b\": np.pi * 0.45,\n            \"g_list\": [1.00, 1.02, 0.98, 1.01],\n            \"p_list\": [0.90, 0.88, 0.92, 0.91],\n            \"sigma\": 1e-4,\n        },\n        # Case 2\n        {\n            \"M\": 8,\n            \"Lh\": 64,\n            \"Lf\": 3,\n            \"Mf\": 257,\n            \"Omega_b\": np.pi * 0.45,\n            \"g_list\": [1.00, 1.05, 0.95, 1.02, 0.97, 1.03, 0.94, 1.01],\n            \"p_list\": [0.85, 0.90, 0.93, 0.88, 0.92, 0.86, 0.94, 0.89],\n            \"sigma\": 5e-4,\n        },\n        # Case 3\n        {\n            \"M\": 4,\n            \"Lh\": 64,\n            \"Lf\": 1,\n            \"Mf\": 257,\n            \"Omega_b\": np.pi * 0.40,\n            \"g_list\": [1.04, 0.96, 1.00, 1.02],\n            \"p_list\": [0.93, 0.89, 0.91, 0.92],\n            \"sigma\": 1e-4,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        r = run_case(\n            M=case[\"M\"],\n            Lh=case[\"Lh\"],\n            Lf=case[\"Lf\"],\n            Mf=case[\"Mf\"],\n            Omega_b=case[\"Omega_b\"],\n            g_list=case[\"g_list\"],\n            p_list=case[\"p_list\"],\n            sigma=case[\"sigma\"],\n        )\n        results.append(r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While one-shot filter design is effective, many calibration systems employ adaptive algorithms like Least Mean Squares (LMS) to track variations over time. The performance of these algorithms, particularly their convergence speed, is critically dependent on the statistical properties of the input signal. This exercise challenges you to derive the convergence rate for an LMS-based gain calibrator, revealing the fundamental link between the input signal's power spectrum and the stability and speed of adaptation .",
            "id": "4306014",
            "problem": "Consider a two-channel time-interleaved Analog-to-Digital Converter (ADC), with master sampling period $T$ and channel sampling instants $t_{1}[n] = nT$ and $t_{2}[n] = nT + T/2$. The digital calibration aims to equalize per-channel gain by adapting a two-parameter weight vector $w[n] = \\begin{pmatrix} w_{1}[n] \\\\ w_{2}[n] \\end{pmatrix}$ using the Least Mean Squares (LMS) algorithm to minimize the instantaneous difference signal $e[n] = w_{1}[n]\\,x(t_{1}[n]) - w_{2}[n]\\,x(t_{2}[n])$, where $x(t)$ is a zero-mean, wide-sense stationary input process. Assume the standard independence assumption for LMS analysis holds at the advanced-graduate level. Let the input process have a two-sided Power Spectral Density (PSD) given by $S_{x}(f) = \\dfrac{S_{0}}{1 + \\left(f/f_{c}\\right)^{2}}$, with $S_{0} > 0$ and $f_{c} > 0$ finite. The Electronic Design Automation (EDA) calibration observes the regressors $u[n] = \\begin{pmatrix} x(t_{1}[n]) \\\\ -x(t_{2}[n]) \\end{pmatrix}$.\n\nUsing first-principles definitions for wide-sense stationarity and the Fourier-transform relation between the autocorrelation function $R_{x}(\\tau)$ and the PSD, derive the linearized mean recursion for the LMS weight error and define the per-iteration mean convergence rate as the spectral radius of the linearized mean update matrix. Compute this convergence rate in closed form, as an analytic expression in terms of the step size $\\,\\mu\\,$, the PSD parameters $\\,S_{0}\\,$ and $\\,f_{c}\\,$, and the sampling period $\\,T\\,$. Assume the step size $\\,\\mu\\,$ is chosen to satisfy the usual small-step stability condition.\n\nIn addition, explain within your derivation how an ideal prewhitening filter would modify the eigenvalue spread and accelerate adaptation in this time-interleaved ADC calibration, and why this reduces the risk of stagnation near undesirable fixed points. Your final computed convergence rate must be a single closed-form analytic expression. No numerical evaluation is required.",
            "solution": "The problem asks for the derivation of the mean convergence rate of a Least Mean Squares (LMS) algorithm used for gain equalization in a two-channel time-interleaved ADC. The solution proceeds by first establishing the linearized mean recursion for the weight vector, then calculating the required autocorrelation matrix from the given power spectral density (PSD), and finally finding the eigenvalues of this matrix to determine the convergence rate.\n\nThe LMS algorithm adapts the weight vector $w[n]$ to minimize the instantaneous squared error $e^2[n]$. The weight update equation is given by:\n$$w[n+1] = w[n] - \\mu \\nabla_{w} \\left(\\frac{1}{2} e^2[n]\\right)$$\nwhere $\\mu$ is the step-size parameter. The error signal is $e[n] = w_{1}[n]\\,x(t_{1}[n]) - w_{2}[n]\\,x(t_{2}[n])$. We can express this in vector form. The problem defines the regressor vector as $u[n] = \\begin{pmatrix} x(t_{1}[n]) \\\\ -x(t_{2}[n]) \\end{pmatrix}$. Therefore, the error signal can be written as $e[n] = w[n]^T u[n]$.\n\nThe gradient of the instantaneous cost $\\frac{1}{2}e^2[n]$ with respect to $w[n]$ is:\n$$\\nabla_{w} \\left(\\frac{1}{2} (w[n]^T u[n])^2\\right) = (w[n]^T u[n]) u[n] = e[n] u[n]$$\nSubstituting this into the update equation gives the standard LMS update rule:\n$$w[n+1] = w[n] - \\mu e[n] u[n] = w[n] - \\mu (w[n]^T u[n]) u[n] = (I - \\mu u[n]u[n]^T) w[n]$$\nTo find the linearized mean recursion, we take the expectation of both sides. Based on the standard independence assumption for LMS analysis, the weight vector $w[n]$ is assumed to be independent of the current regressor vector $u[n]$.\n$$E[w[n+1]] = E[w[n]] - \\mu E[u[n]u[n]^T] E[w[n]]$$\nLet $\\bar{w}[n] = E[w[n]]$ be the mean weight vector. The equation becomes:\n$$\\bar{w}[n+1] = (I - \\mu R) \\bar{w}[n]$$\nwhere $R = E[u[n]u[n]^T]$ is the autocorrelation matrix of the regressor vector. This is the linearized mean recursion for the weights. The cost function being minimized is the mean squared error $J(w) = E[e^2[n]] = E[(w^T u[n])^2] = w^T E[u[n]u[n]^T] w = w^T R w$. Since $R$ is an autocorrelation matrix, it is positive semi-definite. If it's positive definite, the unique minimum of $J(w)$ occurs at $w_{opt} = 0$. The weight error vector is $v[n] = w[n] - w_{opt} = w[n]$. Thus, the derived recursion for the mean weight vector is also the recursion for the mean weight error vector.\n\nNext, we must compute the matrix $R$. Given $u[n] = \\begin{pmatrix} x(nT) \\\\ -x(nT + T/2) \\end{pmatrix}$:\n$$R = E\\left[ \\begin{pmatrix} x(nT) \\\\ -x(nT + T/2) \\end{pmatrix} \\begin{pmatrix} x(nT) & -x(nT + T/2) \\end{pmatrix} \\right] = E\\left[ \\begin{pmatrix} x^2(nT) & -x(nT)x(nT+T/2) \\\\ -x(nT+T/2)x(nT) & x^2(nT+T/2) \\end{pmatrix} \\right]$$\nSince the input process $x(t)$ is wide-sense stationary (WSS), its autocorrelation function $R_x(\\tau) = E[x(t)x(t+\\tau)]$ depends only on the time lag $\\tau$. The elements of $R$ are:\n$$R = \\begin{pmatrix} R_x(0) & -R_x(T/2) \\\\ -R_x(T/2) & R_x(0) \\end{pmatrix}$$\nTo find $R_x(\\tau)$, we compute the inverse Fourier transform of the given two-sided Power Spectral Density (PSD), $S_x(f) = \\frac{S_0}{1 + (f/f_c)^2} = \\frac{S_0 f_c^2}{f^2 + f_c^2}$.\n$$R_x(\\tau) = \\int_{-\\infty}^{\\infty} S_x(f) e^{j 2\\pi f \\tau} df = S_0 f_c^2 \\int_{-\\infty}^{\\infty} \\frac{e^{j 2\\pi f \\tau}}{f^2 + f_c^2} df$$\nThis is a standard Fourier transform pair. Using the identity $\\mathcal{F}^{-1}\\left\\{\\frac{1}{a^2+f^2}\\right\\} = \\frac{\\pi}{a}e^{-2\\pi a|\\tau|}$ with $a=f_c$:\n$$R_x(\\tau) = S_0 f_c^2 \\left( \\frac{\\pi}{f_c} e^{-2\\pi f_c |\\tau|} \\right) = \\pi S_0 f_c e^{-2\\pi f_c |\\tau|}$$\nWe need the values at $\\tau=0$ and $\\tau=T/2$:\n$$R_x(0) = \\pi S_0 f_c$$\n$$R_x(T/2) = \\pi S_0 f_c \\exp(-\\pi f_c T)$$\nSubstituting these into the matrix $R$:\n$$R = \\pi S_0 f_c \\begin{pmatrix} 1 & -\\exp(-\\pi f_c T) \\\\ -\\exp(-\\pi f_c T) & 1 \\end{pmatrix}$$\nThe convergence behavior of the mean recursion $\\bar{w}[n+1] = (I - \\mu R) \\bar{w}[n]$ is determined by the eigenvalues of the matrix $R$. The characteristic equation is $\\det(R-\\lambda I)=0$. For a $2 \\times 2$ matrix of the form $\\begin{pmatrix} A & B \\\\ B & A \\end{pmatrix}$, the eigenvalues are $A \\pm B$. Here, $A=\\pi S_0 f_c$ and $B=-\\pi S_0 f_c \\exp(-\\pi f_c T)$.\nThe eigenvalues of $R$ are:\n$$\\lambda_{max} = A - B = \\pi S_0 f_c (1 + \\exp(-\\pi f_c T))$$\n$$\\lambda_{min} = A + B = \\pi S_0 f_c (1 - \\exp(-\\pi f_c T))$$\nSince $S_0, f_c, T > 0$, we have $0 < \\exp(-\\pi f_c T) < 1$, which ensures both eigenvalues are positive and distinct. Thus, $R$ is positive definite and the optimal weight is indeed $w_{opt}=0$.\n\nThe speed of convergence of the LMS algorithm is limited by the eigenvalue spread (condition number) of $R$, $\\chi(R) = \\lambda_{max}/\\lambda_{min}$. A large spread implies that different components of the weight error vector (in the eigenbasis of $R$) decay at vastly different rates. The mode associated with $\\lambda_{min}$ converges very slowly, leading to stagnation. The surfaces of constant mean-squared error, $w^T R w = C$, are hyperellipsoids whose principal axes are the eigenvectors of $R$. A large $\\chi(R)$ corresponds to highly eccentric ellipsoids. The negative gradient, which the LMS algorithm follows stochastically, is not always aligned with the direction to the minimum, causing slow convergence in the \"valleys\" of the error surface.\n\nAn ideal prewhitening filter, applied to the regressor vector $u[n]$, transforms the autocorrelation matrix $R$ to be proportional to the identity matrix, $R' = \\sigma^2 I$. In this case, all eigenvalues are equal, so the eigenvalue spread $\\chi(R')$ is $1$. The error surfaces become spherical, and the gradient points directly to the minimum. This equalizes the convergence rates of all modes, eliminating the problem of stagnation and significantly accelerating adaptation. This is particularly important in scenarios with non-quadratic cost surfaces where shallow valleys could harbor undesirable local minima (fixed points); whitening the data reduces the risk of the algorithm becoming trapped.\n\nThe problem defines the mean convergence rate as the spectral radius of the linearized mean update matrix, $M = I - \\mu R$. The eigenvalues of $M$ are $1-\\mu\\lambda_{max}$ and $1-\\mu\\lambda_{min}$. The spectral radius is $\\rho(M) = \\max(|1-\\mu\\lambda_{max}|, |1-\\mu\\lambda_{min}|)$. The problem specifies that the step size $\\mu$ is small and satisfies the stability condition $0 < \\mu < 2/\\lambda_{max}$. This ensures that both $1-\\mu\\lambda_{max}$ and $1-\\mu\\lambda_{min}$ are positive and less than $1$. Therefore, the spectral radius is:\n$$\\rho(M) = \\max(1-\\mu\\lambda_{max}, 1-\\mu\\lambda_{min}) = 1 - \\mu \\lambda_{min}$$\nSubstituting the expression for $\\lambda_{min}$:\n$$\\text{Convergence Rate} = 1 - \\mu \\pi S_0 f_c (1 - \\exp(-\\pi f_c T))$$\nThis is the final closed-form expression for the mean convergence rate as defined.",
            "answer": "$$\n\\boxed{1 - \\mu \\pi S_0 f_c \\left(1 - \\exp(-\\pi f_c T)\\right)}\n$$"
        },
        {
            "introduction": "Background calibration algorithms often rely on the assumption that the input signal is statistically stationary, allowing mismatches to be estimated from long-term averages. However, real-world signals are often non-stationary, which can introduce significant bias into mismatch estimates and destabilize the calibration loop. This practice explores these failure modes and requires you to evaluate system-level strategies for detecting non-stationarity and implementing robust mitigation techniques to ensure reliable ADC performance .",
            "id": "4306002",
            "problem": "Consider a time-interleaved Analog-to-Digital Converter (TI-ADC) composed of $M$ sub-ADCs sampling in a round-robin fashion with a per-channel sampling period $T_s$. The $i$-th sub-ADC samples at times $t_k^{(i)} = \\left(k M + i\\right)\\frac{T_s}{M} + \\Delta_i$, where $\\Delta_i$ is the channel timing skew, and exhibits gain and offset mismatches modeled by $g_i = 1 + \\delta g_i$ and $o_i$, respectively. The input is modeled as $x(t) = s(t) + m(t)$, where $s(t)$ is a zero-mean wide-sense stationary (WSS) random process with autocovariance $R_s(\\tau)$ and $m(t)$ is a slowly varying deterministic trend. Background calibration relies on ergodicity and stationarity to estimate channel mismatches using sample statistics computed from blocks of $N$ samples per channel.\n\nTwo background calibration estimators are applied:\n- Offset estimator: per-channel sample mean $\\bar{y}_i = \\frac{1}{N}\\sum_{k=0}^{N-1} y_i[k]$ and offset correction $\\hat{o}_i = \\bar{y}_i - \\frac{1}{M}\\sum_{j=0}^{M-1} \\bar{y}_j$.\n- Timing-skew estimator: a slope-based least-squares estimate that treats interleaved differences as a discrete derivative. Let $d[n]$ denote a discrete-time proxy for $x'(t)$ built from adjacent interleaved samples, and let the per-channel error be $e_i[n] = y_i[n] - y_{\\text{ref}}[n]$, where $y_{\\text{ref}}[n]$ is a synthesized reference stream aligned to the ideal sampling grid. The estimator updates $\\hat{\\Delta}_i$ by minimizing $\\sum_n \\left(e_i[n] - \\Delta_i d[n]\\right)^2$, yielding $\\hat{\\Delta}_i = \\frac{\\sum_n e_i[n] d[n]}{\\sum_n d[n]^2}$.\n\nAssume the input trend $m(t)$ is differentiable over the block and satisfies the approximation $m(t_k^{(i)}) \\approx m(\\bar{t}) + m'(\\bar{t})\\left(t_k^{(i)} - \\bar{t}\\right)$, where $\\bar{t}$ is the block midpoint time. Under these conditions:\n- Derive the leading-order bias of the offset estimator $\\hat{o}_i$ due to $m(t)$, in terms of $m'(\\bar{t})$, $i$, $M$, and $\\Delta_i$.\n- Explain qualitatively why the slope-based timing-skew estimator $\\hat{\\Delta}_i$ can exhibit transiently biased updates in finite samples when $m(t)$ contains steps or ramps, even if its infinite-sample expectation remains tied to $\\Delta_i$ under ideal $d[n] \\approx x'(t_n)$, and why such biases degrade calibration stability.\n\nIn practice, nonstationarity can arise from changes in mean, variance, or spectrum of $x(t)$ (e.g., amplitude ramps, bursts, or modulation changes). Designers wish to detect such episodes and mitigate calibration bias without halting conversion. Consider the following statistical tests and mitigation tools:\n- The Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test for level stationarity on blockwise residuals, with test statistic $$\\eta = \\frac{1}{T^2 \\hat{\\sigma}^2}\\sum_{t=1}^{T} S_t^2,$$ where $S_t = \\sum_{j=1}^{t} \\hat{u}_j$ is the partial sum of residuals $\\hat{u}_j$ from an ordinary least squares (OLS) level model and $\\hat{\\sigma}^2$ is a consistent long-run variance estimate.\n- A sequential cumulative sum (CUSUM) change-point detector applied to first differences to rapidly flag abrupt changes.\n- A pilot tone injection of known amplitude and frequency, placed outside the user signal band but within the TI-ADC bandwidth, enabling mismatch estimation from tone-induced spurs that are independent of $x(t)$.\n- Robust aggregation using a median-of-means estimator over sub-blocks to reduce the influence of transient heavy-tailed fluctuations on parameter updates.\n\nWhich of the following detection and mitigation strategies most directly addresses the failure modes above by detecting nonstationarity that biases background calibration and yielding unbiased or stabilized mismatch estimates in the presence of $m(t)$, while preserving conversion?\n\nA. Apply the KPSS test to blockwise per-channel residuals to detect violations of level stationarity; gate (freeze) background calibration updates when the KPSS statistic exceeds its critical value. In parallel, run an online CUSUM detector on first differences to catch abrupt changes early. When either detector flags nonstationarity, switch to pilot-tone-based calibration for gains, offsets, and timing skews, estimating mismatch parameters from known tone spurs using a median-of-means aggregator; resume background calibration only when KPSS no longer rejects stationarity.\n\nB. Run the Augmented Dickey–Fuller (ADF) test on the raw interleaved stream, treating its null hypothesis as “stationarity” and increasing the adaptation gain when the test rejects the null; continue slope-based updates without gating or auxiliary signals because increasing gain accelerates convergence under nonstationary inputs.\n\nC. Increase the block length $N$ for computing $\\bar{y}_i$ and $\\hat{\\Delta}_i$ until the mean drift averages out; do not perform any statistical testing or gating, because larger $N$ guarantees that nonstationarity has negligible impact on estimators by the law of large numbers.\n\nD. High-pass filter the input to remove low-frequency drifts before calibration; run slope-based updates continuously with the filtered derivative proxy and do not inject pilot tones or perform stationarity testing, because filtering makes the process stationary for calibration purposes.\n\nSelect the single best option.",
            "solution": "The problem asks for an analysis of background calibration techniques for a time-interleaved ADC (TI-ADC) in the presence of a non-stationary input signal, specifically a deterministic trend $m(t)$. The first part requires deriving the bias of an offset estimator and explaining the bias in a timing-skew estimator. The second part requires evaluating several strategies for detecting and mitigating these biases.\n\n### Problem Validation\n\nThe problem statement describes a standard model for a TI-ADC, including timing, gain, and offset mismatches. The input signal model, $x(t) = s(t) + m(t)$, where $s(t)$ is a wide-sense stationary (WSS) process and $m(t)$ is a deterministic trend, is a common and valid approach to analyzing the impact of non-stationarity. The calibration estimators described are standard in the literature. The proposed statistical tools (KPSS, CUSUM) and mitigation techniques (pilot tone, robust estimation) are all scientifically sound and relevant to the problem. The problem is well-posed, objective, and internally consistent. It is a valid, non-trivial problem in the field of mixed-signal integrated circuit design.\n\n### Preliminary Analysis\n\nBefore evaluating the options, we must perform the two preliminary tasks requested.\n\n#### 1. Bias of the Offset Estimator $\\hat{o}_i$\n\nThe output of the $i$-th sub-ADC at sample time $t_k^{(i)}$ is given by:\n$$y_i[k] = g_i x(t_k^{(i)}) + o_i = (1 + \\delta g_i)(s(t_k^{(i)}) + m(t_k^{(i)})) + o_i$$\nWe use the expectation operator $E[\\cdot]$ to represent averaging over the random process $s(t)$. Since $s(t)$ is zero-mean, $E[s(t)] = 0$. The expectation of the per-channel sample mean $\\bar{y}_i = \\frac{1}{N}\\sum_{k=0}^{N-1} y_i[k]$ is:\n$$E[\\bar{y}_i] = \\frac{1}{N}\\sum_{k=0}^{N-1} E[y_i[k]] = \\frac{1}{N}\\sum_{k=0}^{N-1} \\left[ (1 + \\delta g_i) m(t_k^{(i)}) + o_i \\right] = (1 + \\delta g_i) \\left( \\frac{1}{N}\\sum_{k=0}^{N-1} m(t_k^{(i)}) \\right) + o_i$$\nUsing the given linear approximation for the trend, $m(t) \\approx m(\\bar{t}) + m'(\\bar{t})(t - \\bar{t})$, we get:\n$$E[\\bar{y}_i] \\approx (1 + \\delta g_i) \\left( \\frac{1}{N}\\sum_{k=0}^{N-1} \\left[ m(\\bar{t}) + m'(\\bar{t})(t_k^{(i)} - \\bar{t}) \\right] \\right) + o_i$$\n$$E[\\bar{y}_i] \\approx (1 + \\delta g_i) \\left( m(\\bar{t}) + m'(\\bar{t}) \\left[ \\left(\\frac{1}{N}\\sum_{k=0}^{N-1} t_k^{(i)}\\right) - \\bar{t} \\right] \\right) + o_i$$\nThe average sampling time for channel $i$ is $\\frac{1}{N}\\sum_{k=0}^{N-1} t_k^{(i)} = \\frac{1}{N}\\sum_{k=0}^{N-1} \\left( k T_s + i \\frac{T_s}{M} + \\Delta_i \\right) = \\frac{(N-1)}{2} T_s + i \\frac{T_s}{M} + \\Delta_i$.\nLetting the block midpoint time be $\\bar{t} = \\frac{(N-1)}{2} T_s$, the term in the square brackets simplifies to $i \\frac{T_s}{M} + \\Delta_i$.\n$$E[\\bar{y}_i] \\approx (1 + \\delta g_i) \\left( m(\\bar{t}) + m'(\\bar{t}) \\left( i \\frac{T_s}{M} + \\Delta_i \\right) \\right) + o_i$$\nThe offset estimator is $\\hat{o}_i = \\bar{y}_i - \\frac{1}{M}\\sum_{j=0}^{M-1} \\bar{y}_j$. Its expected value is $E[\\hat{o}_i] = E[\\bar{y}_i] - \\frac{1}{M}\\sum_{j=0}^{M-1} E[\\bar{y}_j]$. The ideal value that the estimator should converge to is $o_i - \\bar{o}$, where $\\bar{o} = \\frac{1}{M}\\sum o_j$. The bias is the difference $E[\\hat{o}_i] - (o_i - \\bar{o})$.\nTo find this, we subtract the all-channel average from the channel $i$ value. The terms $o_i$ and $o_j$ will combine to form $o_i - \\bar{o}$. The remaining terms, which depend on $m(t)$, constitute the bias. Neglecting products of small quantities (e.g., $\\delta g_i \\cdot m'$), the leading-order bias is:\n$$ \\text{Bias}(\\hat{o}_i) \\approx (\\delta g_i - \\bar{\\delta g})m(\\bar{t}) + m'(\\bar{t}) \\left[ \\left( i \\frac{T_s}{M} + \\Delta_i \\right) - \\frac{1}{M}\\sum_{j=0}^{M-1} \\left( j \\frac{T_s}{M} + \\Delta_j \\right) \\right] $$\nwhere $\\bar{\\delta g} = \\frac{1}{M}\\sum \\delta g_j$. Let $\\bar{\\Delta} = \\frac{1}{M}\\sum \\Delta_j$ and using $\\sum_{j=0}^{M-1} j = \\frac{M(M-1)}{2}$, the expression becomes:\n$$ \\text{Bias}(\\hat{o}_i) \\approx (\\delta g_i - \\bar{\\delta g})m(\\bar{t}) + m'(\\bar{t}) \\left[ i\\frac{T_s}{M} - \\frac{M-1}{2M}T_s + \\Delta_i - \\bar{\\Delta} \\right] $$\nThe question asks for the bias due to $m(t)$ in terms of $m'(\\bar{t})$, $i$, $M$, and $\\Delta_i$. Assuming gain errors are negligible or have been corrected ($\\delta g_i \\approx 0$), and that the timing skews are constrained such that their sum is zero ($\\sum \\Delta_j = 0$, so $\\bar{\\Delta}=0$), the bias simplifies to:\n$$ \\text{Bias}(\\hat{o}_i) \\approx m'(\\bar{t}) \\left[ \\left( i - \\frac{M-1}{2} \\right) \\frac{T_s}{M} + \\Delta_i \\right] $$\nThis shows that a linear trend $m(t)$ with slope $m'(\\bar{t})$ creates a bias in the offset estimate that is proportional to the deviation of the channel's average sampling time from the overall average sampling time.\n\n#### 2. Transient Bias of the Timing-Skew Estimator $\\hat{\\Delta}_i$\n\nThe estimator $\\hat{\\Delta}_i = \\frac{\\sum_n e_i[n] d[n]}{\\sum_n d[n]^2}$ is derived from the linear model $e_i[n] \\approx \\Delta_i d[n]$. This model is based on a first-order Taylor expansion of the signal, $x(t+\\Delta_i) - x(t) \\approx x'(t)\\Delta_i$. Here, $e_i[n]$ represents the sample error (e.g., $y_i[n] - y_{ref}[n]$) and $d[n]$ is a proxy for the signal derivative $x'(t)$. This estimation relies on statistical averaging over the WSS process $s(t)$ to ensure that unwanted correlations (e.g., between signal derivative and offset errors) average to zero.\n\nWhen the input contains a deterministic trend $m(t)$, the stationarity assumption is violated. A trend like a ramp ($m(t) = ct$) or a step introduces a deterministic, non-zero-mean component into the true derivative $x'(t) = s'(t) + m'(t)$. For a ramp, $m'(t) = c$ is a constant offset in the derivative. For a step, $m'(t)$ is an impulse.\n\nThis has several biasing effects over a finite sample block:\n- **Correlation with Other Mismatches:** The error signal $e_i[n]$ is not just a function of timing skew. It also contains gain and offset errors: $e_i[n] \\approx \\delta g_i x(t_n) + o_i + x'(t_n)\\Delta_i$. The presence of the trend $m(t)$ means that $x(t_n)$ has a non-zero mean and $x'(t_n)$ has a non-zero mean component ($m'(t_n)$) over the block. The estimator will then form correlations like $\\sum_n o_i d[n]$ and $\\sum_n \\delta g_i x(t_n) d[n]$. These quantities no longer average to zero over a finite block due to the deterministic components from $m(t)$ and $m'(t)$, leading to a bias in $\\hat{\\Delta}_i$ that is coupled with $o_i$ and $\\delta g_i$.\n- **Finite-Sample Statistics:** The estimators rely on ergodic properties, where time averages converge to ensemble averages. A trend is a non-ergodic feature. Over a finite block, the sample mean of $m'(t)$ is not zero, and its sample correlation with other signals will be non-zero, creating a transient bias.\n- **Unstable Feedback:** This transient bias is problematic for calibration stability. The calibration loop will interpret this bias as a genuine timing skew and apply a \"correction.\" Since the bias depends on the specific input trend within the current block, the \"correction\" will be wrong in the next block if the trend changes or disappears. This leads to the estimated skew parameters oscillating or diverging, degrading overall ADC performance, a classic sign of calibration instability.\n\n### Evaluation of Detection and Mitigation Strategies\n\nThe main task is to select the best strategy among the options provided to handle the non-stationarity issues analyzed above.\n\n**A. Apply the KPSS test... gate... switch to pilot-tone-based calibration...**\nThis option proposes a sophisticated, multi-faceted strategy.\n- **Detection**: It uses two complementary tests. The KPSS test is designed to detect slow drifts (level/trend non-stationarity), which directly addresses the problem of a slowly varying $m(t)$ biasing the offset estimator. The CUSUM detector on first differences is excellent for rapidly detecting abrupt changes like steps and ramps in $m(t)$, which strongly affect the timing-skew estimator. This two-pronged detection is comprehensive.\n- **Mitigation**: The proposed actions are sound engineering practice. First, **gating** (freezing) updates when non-stationarity is detected is a critical safety measure. It prevents the calibration loop from running on biased data and becoming unstable. Second, switching to a **pilot-tone-based** method provides a robust backup. Pilot-tone calibration derives mismatch information from deterministic spurs generated by a known, injected tone. Its estimates are independent of the statistics of the unknown input signal $x(t)$, making it immune to the non-stationarity caused by $m(t)$. Using a **median-of-means** aggregator adds further robustness against outliers. Finally, it defines a clear condition for resuming background calibration.\n- **Overall**: This strategy directly addresses the identified failure modes by detecting the specific types of non-stationarity that cause bias, providing a safe-shutdown mechanism (gating), and employing a robust, signal-independent alternative (pilot tone) to maintain calibration during non-stationary episodes, all while preserving conversion.\n**Verdict: Correct.**\n\n**B. Run the Augmented Dickey–Fuller (ADF) test... increase the adaptation gain...**\nThis option is flawed in multiple ways.\n- The ADF test's null hypothesis is the presence of a unit root (non-stationarity), while the alternative is stationarity. The option incorrectly states the null is \"stationarity\".\n- The proposed mitigation is to *increase* adaptation gain during non-stationarity. This would make the calibration loop react *more* strongly to the biased statistical estimates, exacerbating instability rather than mitigating it. The correct action would be to decrease gain or gate the updates.\n**Verdict: Incorrect.**\n\n**C. Increase the block length N...**\nThis option misunderstands the nature of the problem and the Law of Large Numbers (LLN).\n- The LLN applies to stationary (or at least asymptotically mean-stationary) processes. It does not state that averaging a deterministic trend will make it disappear. As shown in our bias derivation, the bias from a linear trend does not depend on the block length $N$. Increasing $N$ will not remove the bias caused by the trend $m(t)$.\n**Verdict: Incorrect.**\n\n**D. High-pass filter the input to remove low-frequency drifts...**\nThis option suggests a plausible pre-processing step but is an incomplete and less robust solution than A.\n- Filtering the *analog* input is generally not acceptable, as it would alter the signal the user wants to convert. Assuming the filtering is done in the *digital* domain on the signal fed to the calibrator, it can remove slow drifts.\n- However, this single-shot solution is not adaptive. It doesn't *detect* when the signal is problematic. It lacks a safety mechanism like gating. If a form of non-stationarity occurs that is not removed by the high-pass filter (e.g., a burst of high-frequency energy), the calibration will still be fed bad data, potentially leading to instability. It also does not explicitly handle abrupt changes like steps, which have significant high-frequency content.\n- Compared to option A, this is a much simpler and less robust approach. Option A's combination of detection, gating, and switching to a signal-independent method is a superior systems-level solution.\n**Verdict: Incorrect.**\n\n**Conclusion**\n\nOption A presents the most comprehensive and correct strategy. It accurately identifies the problem using appropriate statistical tests for different types of non-stationarity and implements a robust, multi-stage mitigation plan that ensures calibration stability and accuracy without interrupting the ADC's primary conversion task.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}