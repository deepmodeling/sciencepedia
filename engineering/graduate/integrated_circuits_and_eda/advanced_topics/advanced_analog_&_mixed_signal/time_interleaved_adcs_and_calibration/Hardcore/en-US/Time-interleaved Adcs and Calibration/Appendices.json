{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in time-interleaved ADCs is compensating for gain and bandwidth mismatches between the parallel channels. This exercise provides hands-on experience with a fundamental and powerful calibration technique: designing per-channel Finite Impulse Response (FIR) filters. By formulating the problem in the frequency domain, you will use a least-squares approach to compute optimal filter coefficients that force each channel's response to match a common reference, thereby restoring the converter's overall performance .",
            "id": "4306021",
            "problem": "A time-interleaved Analog-to-Digital Converter (ADC) comprises multiple parallel channels whose individual linear time-invariant responses introduce gain and bandwidth mismatches that degrade overall performance. Let there be $M$ channels indexed by $k \\in \\{0,1,\\dots,M-1\\}$. Each channel is modeled as a discrete-time, causal, linear time-invariant system with impulse response $h_k[n]$ measured over a finite window $n \\in \\{0,1,\\dots,L_h-1\\}$. The goal is to design, for each channel, a Finite Impulse Response (FIR) equalizer $F_k(z)$ of length $L_f$ (coefficients $f_k[n]$, $n \\in \\{0,1,\\dots,L_f-1\\}$) that minimizes the mismatch across channels within a specified passband by solving a least-squares problem using the measured impulse responses. Then, quantify the expected residual mismatch across the band after equalization.\n\nStart from the following fundamental definitions:\n\n1. Discrete-time convolution: the output of a linear time-invariant system with impulse response $h[n]$ to input $x[n]$ is $y[n] = \\sum_{m=-\\infty}^{\\infty} h[m] x[n-m]$.\n2. Discrete-time Fourier transform over frequency sample $ \\omega $: the frequency response is $H(\\mathrm{e}^{j\\omega}) = \\sum_{n=-\\infty}^{\\infty} h[n] \\mathrm{e}^{-j\\omega n}$, specialized here to finite $n$ support.\n3. Least-squares optimality: Given complex-valued linear model $A \\mathbf{f} \\approx \\mathbf{b}$ with $A \\in \\mathbb{C}^{P \\times Q}$, the minimizer is given by the solution to $\\min_{\\mathbf{f} \\in \\mathbb{R}^{Q}} \\|A \\mathbf{f} - \\mathbf{b}\\|_2^2$, implementable via real-valued stacking of real and imaginary parts.\n\nDefine the per-channel measured impulse response model as a first-order low-pass with gain variation and a discrete pole, plus deterministic measurement noise:\n$$\nh_k[n] = g_k (1 - p_k) p_k^{n} + v_k[n], \\quad n \\in \\{0,1,\\dots,L_h-1\\},\n$$\nwhere $g_k  0$ is a dimensionless gain, $p_k \\in (0,1)$ is the discrete-time pole, and $v_k[n]$ is a deterministic noise sequence\n$$\nv_k[n] = \\sigma \\,\\sin\\!\\left(\\frac{2\\pi (n+1)}{L_h+1}\\right) \\cos\\!\\left(\\frac{2\\pi (k+1)}{M+1}\\right),\n$$\nwith $\\sigma \\ge 0$ a dimensionless noise amplitude. All angles are in radians. The finite-length measured frequency response at frequency samples $\\{\\omega_m\\}_{m=0}^{M_f-1}$ is computed by\n$$\nH_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_h-1} h_k[n]\\, \\mathrm{e}^{-j\\omega_m n}.\n$$\nLet the common reference be the per-frequency average across channels\n$$\nH_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) = \\frac{1}{M} \\sum_{k=0}^{M-1} H_k(\\mathrm{e}^{j\\omega_m}).\n$$\nFor each channel $k$, design the FIR equalizer $F_k(z)$ of length $L_f$ with coefficients $\\{f_k[n]\\}_{n=0}^{L_f-1}$, whose frequency response is\n$$\nF_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_f-1} f_k[n]\\, \\mathrm{e}^{-j\\omega_m n},\n$$\nby solving the least-squares problem over the passband samples:\n$$\n\\min_{\\{f_k[n]\\}} \\sum_{m=0}^{M_f-1} \\left| F_k(\\mathrm{e}^{j\\omega_m})\\, H_k(\\mathrm{e}^{j\\omega_m}) - H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right|^2.\n$$\nThis can be expressed as a linear system in the real domain by stacking real and imaginary parts.\n\nAfter computing the equalizers, define the equalized per-channel response\n$$\nG_k(\\mathrm{e}^{j\\omega_m}) = F_k(\\mathrm{e}^{j\\omega_m})\\, H_k(\\mathrm{e}^{j\\omega_m}),\n$$\nand quantify the residual mismatch across the band by the root-mean-square normalized error\n$$\nr = \\sqrt{ \\frac{1}{M M_f} \\sum_{k=0}^{M-1} \\sum_{m=0}^{M_f-1} \\left( \\frac{ \\left| G_k(\\mathrm{e}^{j\\omega_m}) - H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right| }{ \\max\\left( \\left| H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right|, \\varepsilon \\right) } \\right)^2 },\n$$\nwhere $\\varepsilon$ is a small positive number to avoid division by zero. Angles must be in radians and all amplitudes are dimensionless.\n\nYour task is to implement this computation for the following test suite of parameter sets. For each case, generate $\\{h_k[n]\\}$ using the specified parameters, compute $\\{H_k(\\mathrm{e}^{j\\omega_m})\\}$ at uniformly spaced samples in the passband, design $\\{F_k\\}$ via least squares, form $\\{G_k\\}$, and return the scalar $r$.\n\nTest Suite:\n- Case $1$ (happy path): $M=4$, $L_h=64$, $L_f=7$, $M_f=257$, $\\Omega_b=0.45\\pi$, $g_k \\in \\{1.00, 1.02, 0.98, 1.01\\}$, $p_k \\in \\{0.90, 0.88, 0.92, 0.91\\}$, $\\sigma=0.0001$.\n- Case $2$ (higher mismatch, shorter equalizer): $M=8$, $L_h=64$, $L_f=3$, $M_f=257$, $\\Omega_b=0.45\\pi$, $g_k \\in \\{1.00, 1.05, 0.95, 1.02, 0.97, 1.03, 0.94, 1.01\\}$, $p_k \\in \\{0.85, 0.90, 0.93, 0.88, 0.92, 0.86, 0.94, 0.89\\}$, $\\sigma=0.0005$.\n- Case $3$ (boundary, scalar gain equalization): $M=4$, $L_h=64$, $L_f=1$, $M_f=257$, $\\Omega_b=0.40\\pi$, $g_k \\in \\{1.04, 0.96, 1.00, 1.02\\}$, $p_k \\in \\{0.93, 0.89, 0.91, 0.92\\}$, $\\sigma=0.0001$.\n\nFrequency sampling must be uniform over $[0, \\Omega_b]$ with $\\omega_m = \\frac{m}{M_f-1} \\Omega_b$ for $m \\in \\{0,1,\\dots,M_f-1\\}$. Angles are in radians. Express the final output as a single line containing the three residual mismatch values for the cases above, as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3]$, where each $r_i$ is a floating-point number.",
            "solution": "The problem is addressed by combining linear time-invariant system theory, discrete-time Fourier analysis, and least-squares optimization.\n\nFirst principles and modeling:\n- Each channel is modeled as a discrete-time linear time-invariant system with impulse response $h_k[n]$ whose measured samples are finite-length and contaminated by deterministic measurement noise $v_k[n]$. The discrete-time frequency response at a finite set of angles $\\omega_m$ is $H_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_h-1} h_k[n] \\mathrm{e}^{-j\\omega_m n}$.\n- The equalizer is a finite impulse response filter $F_k(z)$ of length $L_f$ with coefficients $f_k[n]$, having frequency response $F_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_f-1} f_k[n] \\mathrm{e}^{-j\\omega_m n}$. The total equalized response per channel is $G_k(\\mathrm{e}^{j\\omega_m}) = F_k(\\mathrm{e}^{j\\omega_m}) H_k(\\mathrm{e}^{j\\omega_m})$.\n- To align channels, we define a reference $H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m})$ as the average of the measured responses across channels at each frequency, which is a common choice to avoid excessive amplification or deamplification relative to the ensemble.\n\nLeast-squares formulation:\n- The design objective is to minimize the sum of squared frequency-domain errors across the passband samples:\n$$\nJ_k(\\mathbf{f}_k) = \\sum_{m=0}^{M_f-1} \\left| F_k(\\mathrm{e}^{j\\omega_m}) H_k(\\mathrm{e}^{j\\omega_m}) - H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right|^2,\n$$\nwhere $\\mathbf{f}_k = [f_k[0], f_k[1], \\dots, f_k[L_f-1]]^{\\top}$.\n- Introduce the complex-valued design matrix $A_k \\in \\mathbb{C}^{M_f \\times L_f}$ with entries\n$$\n[A_k]_{m,n} = H_k(\\mathrm{e}^{j\\omega_m}) \\mathrm{e}^{-j \\omega_m n}.\n$$\nThen the model reads, per frequency $m$,\n$$\n\\sum_{n=0}^{L_f-1} [A_k]_{m,n} f_k[n] \\approx H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}),\n$$\nwhich is $A_k \\mathbf{f}_k \\approx \\mathbf{b}$ with $\\mathbf{b} = [H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_0}), \\dots, H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_{M_f-1}})]^{\\top}$.\n- Since the coefficients $f_k[n]$ are real, we recast the complex least-squares problem in the real domain by stacking real and imaginary parts:\n$$\n\\min_{\\mathbf{f}_k \\in \\mathbb{R}^{L_f}} \\left\\| \n\\begin{bmatrix}\n\\Re\\{A_k\\}\\\\\n\\Im\\{A_k\\}\n\\end{bmatrix} \\mathbf{f}_k - \n\\begin{bmatrix}\n\\Re\\{\\mathbf{b}\\}\\\\\n\\Im\\{\\mathbf{b}\\}\n\\end{bmatrix}\n\\right\\|_2^2.\n$$\nThis ensures computational tractability using standard real-valued least-squares solvers.\n\nAlgorithmic steps:\n1. For each test case, construct $\\omega_m$ as $M_f$ uniformly spaced samples over $[0, \\Omega_b]$ using $\\omega_m = \\frac{m}{M_f-1} \\Omega_b$.\n2. Generate measured impulse responses $h_k[n] = g_k (1-p_k) p_k^n + v_k[n]$, with $v_k[n]$ as specified, for $n \\in \\{0,\\dots,L_h-1\\}$ and $k \\in \\{0,\\dots,M-1\\}$.\n3. Compute frequency responses $H_k(\\mathrm{e}^{j\\omega_m})$ directly from the measured $h_k[n]$ by $H_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_h-1} h_k[n] \\mathrm{e}^{-j\\omega_m n}$. Implement as a matrix-vector multiplication where the matrix entries are $\\mathrm{e}^{-j\\omega_m n}$.\n4. Form $H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) = \\frac{1}{M} \\sum_{k=0}^{M-1} H_k(\\mathrm{e}^{j\\omega_m})$.\n5. For each channel $k$, build $A_k$ with $[A_k]_{m,n} = H_k(\\mathrm{e}^{j\\omega_m}) \\mathrm{e}^{-j \\omega_m n}$, and solve the real-valued least-squares system\n$$\n\\begin{bmatrix}\n\\Re\\{A_k\\}\\\\\n\\Im\\{A_k\\}\n\\end{bmatrix} \\mathbf{f}_k \\approx \n\\begin{bmatrix}\n\\Re\\{H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m})\\}\\\\\n\\Im\\{H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m})\\}\n\\end{bmatrix}\n$$\nto obtain the FIR coefficients $\\mathbf{f}_k$.\n6. Evaluate $F_k(\\mathrm{e}^{j\\omega_m}) = \\sum_{n=0}^{L_f-1} f_k[n] \\mathrm{e}^{-j\\omega_m n}$, and then compute $G_k(\\mathrm{e}^{j\\omega_m}) = F_k(\\mathrm{e}^{j\\omega_m}) H_k(\\mathrm{e}^{j\\omega_m})$.\n7. Compute the residual mismatch metric\n$$\nr = \\sqrt{ \\frac{1}{M M_f} \\sum_{k=0}^{M-1} \\sum_{m=0}^{M_f-1} \\left( \\frac{ \\left| G_k(\\mathrm{e}^{j\\omega_m}) - H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right| }{ \\max\\left( \\left| H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega_m}) \\right|, \\varepsilon \\right) } \\right)^2 },\n$$\nwith a small $\\varepsilon$ (for example, $\\varepsilon = 10^{-12}$) to avoid division by zero.\n\nInterpretation:\n- If $L_f$ is sufficiently large relative to the variation in $H_k(\\mathrm{e}^{j\\omega})$ across frequency, the least-squares solution closely approximates $F_k(\\mathrm{e}^{j\\omega}) \\approx \\frac{H_{\\mathrm{ref}}(\\mathrm{e}^{j\\omega})}{H_k(\\mathrm{e}^{j\\omega})}$ and the residual $r$ is small.\n- In the boundary case $L_f=1$, the equalizer reduces to a scalar gain per channel, so bandwidth mismatches cannot be compensated, increasing $r$.\n\nThe program constructs the three cases, executes these steps, and outputs $[r_1,r_2,r_3]$ as specified, with angles in radians and amplitudes dimensionless.",
            "answer": "```python\nimport numpy as np\n\ndef generate_impulse_responses(M, Lh, g_list, p_list, sigma):\n    \"\"\"\n    Generate measured impulse responses h_k[n] = g_k*(1-p_k)*p_k^n + v_k[n],\n    where v_k[n] is a deterministic \"noise\" sequence per the problem statement.\n    \"\"\"\n    h = np.zeros((M, Lh), dtype=float)\n    n = np.arange(Lh)\n    for k in range(M):\n        gk = g_list[k]\n        pk = p_list[k]\n        ideal = gk * (1.0 - pk) * (pk ** n)\n        # Deterministic noise as specified\n        vk = sigma * np.sin(2.0 * np.pi * (n + 1) / (Lh + 1)) * np.cos(2.0 * np.pi * (k + 1) / (M + 1))\n        h[k, :] = ideal + vk\n    return h\n\ndef frequency_samples(Mf, Omega_b):\n    \"\"\"\n    Uniform frequency sampling over [0, Omega_b] in radians.\n    \"\"\"\n    m = np.arange(Mf)\n    return (m / (Mf - 1)) * Omega_b\n\ndef compute_frequency_response(h, omega):\n    \"\"\"\n    Compute H_k(e^{j omega_m}) = sum_{n=0}^{Lh-1} h_k[n] e^{-j omega_m n}\n    for each channel k and frequency sample omega_m.\n    Returns H of shape (M, Mf) complex.\n    \"\"\"\n    M, Lh = h.shape\n    Mf = omega.shape[0]\n    # Precompute the exponential matrix E[m, n] = e^{-j omega[m] n}\n    n = np.arange(Lh)\n    # Broadcast: omega[:, None] * n[None, :]\n    E = np.exp(-1j * (omega[:, None] * n[None, :]))  # shape (Mf, Lh)\n    # For each channel k, H_k = E @ h_k\n    H = np.zeros((M, Mf), dtype=complex)\n    for k in range(M):\n        H[k, :] = E @ h[k, :]\n    return H\n\ndef design_equalizer_ls(Hk, Href, omega, Lf):\n    \"\"\"\n    For a single channel k, design FIR f_k of length Lf minimizing\n    ||A f - b||^2 where A[m, n] = Hk[m] * e^{-j omega[m] n}, b = Href[m].\n    Returns the real-valued FIR coefficients f_k[n].\n    \"\"\"\n    Mf = omega.shape[0]\n    n = np.arange(Lf)\n    # Build A complex matrix of shape (Mf, Lf)\n    A = (Hk[:, None]) * np.exp(-1j * (omega[:, None] * n[None, :]))\n    b = Href\n    # Stack real and imaginary parts to get real-valued least squares\n    Ar = np.vstack([A.real, A.imag])\n    br = np.hstack([b.real, b.imag])\n    # Solve least squares\n    f, *_ = np.linalg.lstsq(Ar, br, rcond=None)\n    return f  # real-valued coefficients\n\ndef evaluate_equalized_response(f, Hk, omega):\n    \"\"\"\n    Compute F_k(e^{j omega}) from FIR taps f, then G_k = F_k * Hk.\n    \"\"\"\n    Lf = f.shape[0]\n    n = np.arange(Lf)\n    Mf = omega.shape[0]\n    # Compute F_k(e^{j omega}) at samples omega\n    Fw = np.exp(-1j * (omega[:, None] * n[None, :])) @ f\n    Gk = Fw * Hk\n    return Gk, Fw\n\ndef residual_mismatch_metric(G_all, Href, eps=1e-12):\n    \"\"\"\n    Compute r = sqrt( mean_{k,m} ( |G_k - Href| / max(|Href|, eps) )^2 ).\n    G_all: array shape (M, Mf), complex\n    Href: array shape (Mf,), complex\n    \"\"\"\n    M, Mf = G_all.shape\n    denom = np.maximum(np.abs(Href), eps)\n    err_sq = ((np.abs(G_all - Href[None, :]) / denom[None, :]) ** 2)\n    r = np.sqrt(err_sq.mean())\n    return r\n\ndef run_case(M, Lh, Lf, Mf, Omega_b, g_list, p_list, sigma):\n    # Generate measured impulses\n    h = generate_impulse_responses(M, Lh, g_list, p_list, sigma)\n    # Frequency samples\n    omega = frequency_samples(Mf, Omega_b)\n    # Frequency responses per channel\n    H = compute_frequency_response(h, omega)  # shape (M, Mf)\n    # Reference frequency response\n    Href = H.mean(axis=0)\n    # Design equalizers per channel and evaluate equalized responses\n    G_all = np.zeros_like(H)\n    for k in range(M):\n        f_k = design_equalizer_ls(H[k, :], Href, omega, Lf)\n        Gk, _ = evaluate_equalized_response(f_k, H[k, :], omega)\n        G_all[k, :] = Gk\n    # Residual mismatch metric\n    r = residual_mismatch_metric(G_all, Href, eps=1e-12)\n    return float(r)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"M\": 4,\n            \"Lh\": 64,\n            \"Lf\": 7,\n            \"Mf\": 257,\n            \"Omega_b\": np.pi * 0.45,\n            \"g_list\": [1.00, 1.02, 0.98, 1.01],\n            \"p_list\": [0.90, 0.88, 0.92, 0.91],\n            \"sigma\": 1e-4,\n        },\n        # Case 2\n        {\n            \"M\": 8,\n            \"Lh\": 64,\n            \"Lf\": 3,\n            \"Mf\": 257,\n            \"Omega_b\": np.pi * 0.45,\n            \"g_list\": [1.00, 1.05, 0.95, 1.02, 0.97, 1.03, 0.94, 1.01],\n            \"p_list\": [0.85, 0.90, 0.93, 0.88, 0.92, 0.86, 0.94, 0.89],\n            \"sigma\": 5e-4,\n        },\n        # Case 3\n        {\n            \"M\": 4,\n            \"Lh\": 64,\n            \"Lf\": 1,\n            \"Mf\": 257,\n            \"Omega_b\": np.pi * 0.40,\n            \"g_list\": [1.04, 0.96, 1.00, 1.02],\n            \"p_list\": [0.93, 0.89, 0.91, 0.92],\n            \"sigma\": 1e-4,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        r = run_case(\n            M=case[\"M\"],\n            Lh=case[\"Lh\"],\n            Lf=case[\"Lf\"],\n            Mf=case[\"Mf\"],\n            Omega_b=case[\"Omega_b\"],\n            g_list=case[\"g_list\"],\n            p_list=case[\"p_list\"],\n            sigma=case[\"sigma\"],\n        )\n        results.append(r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Beyond gain and bandwidth, timing skew between channel sampling instants is a critical source of error that generates distortion. This practice explores a widely used technique for estimating this skew by exploiting the cross-correlation between adjacent channel data streams. You will not only derive and implement a skew estimator but also investigate a crucial real-world challenge: how the estimator's accuracy is biased by the spectral properties of the input signal, demonstrating the subtle interplay between signal statistics and calibration robustness .",
            "id": "4306058",
            "problem": "Consider a two-channel time-interleaved Analog-to-Digital Converter (ADC) sampling a real-valued, wide-sense stationary (WSS) stochastic process $x(t)$. The total sampling period is $T$ seconds, so the interleaved composite output occurs at instants $t_n = nT$ with $n \\in \\mathbb{Z}$. Channel $\\mathcal{A}$ samples nominally at times $t = 2kT$ and channel $\\mathcal{B}$ samples nominally at times $t = (2k+1)T$ for integer $k \\ge 0$. Due to timing skew, channel $\\mathcal{A}$ samples at $t = 2kT + \\epsilon_{\\mathcal{A}}$ and channel $\\mathcal{B}$ samples at $t = (2k+1)T + \\epsilon_{\\mathcal{B}}$. Define the relative skew as $\\delta = \\epsilon_{\\mathcal{B}} - \\epsilon_{\\mathcal{A}}$, which is assumed small compared to $T$.\n\nLet $A[k] = x(2kT + \\epsilon_{\\mathcal{A}})$ and $B[k] = x((2k+1)T + \\epsilon_{\\mathcal{B}})$ be the sampled sequences for $k = 0,1,2,\\dots$. Define the empirical cross-correlations between adjacent channels\n$$\n\\widehat{C}_{+} = \\frac{1}{K-1} \\sum_{k=1}^{K-1} A[k]\\,B[k], \\quad\n\\widehat{C}_{-} = \\frac{1}{K-1} \\sum_{k=1}^{K-1} A[k]\\,B[k-1],\n$$\nfor some finite $K \\in \\mathbb{N}$, and the composite sequence $S[n]$ formed by interleaving $A[k]$ and $B[k]$ as $S[2k] = A[k]$ and $S[2k+1] = B[k]$ for $k = 0,1,2,\\dots,K-1$. Also define the empirical energy\n$$\n\\widehat{R}_0 = \\frac{1}{2K} \\sum_{n=0}^{2K-1} S[n]^2.\n$$\n\nStarting from the following bases:\n- The definition of cross-correlation for WSS processes and the property that the autocorrelation function $R_x(\\tau) = \\mathbb{E}[x(t)\\,x(t+\\tau)]$ is an even function of $\\tau$ for real-valued processes.\n- The first-order Taylor linearization $x(t+\\delta) \\approx x(t) + \\delta\\,x'(t)$ valid for sufficiently small $\\delta$.\n- The discrete-time approximation of the derivative of an autocorrelation function at lag $T$ by a symmetric difference at lags $0$ and $2T$.\n\nYou must derive and implement a cross-correlation-based skew estimator that uses $\\widehat{C}_{+}$ and $\\widehat{C}_{-}$ and a gain normalization based on a discrete-time white input assumption to produce an estimate $\\widehat{\\delta}$ in seconds. Then, under the assumption that the estimator normalizes its gain using the discrete-time white-noise model for the input (flat power spectral density), derive the estimatorâ€™s expected bias when the actual input spectrum is colored (not white). Quantify the bias using an Autoregressive (AR) first-order model $x[n] = a\\,x[n-1] + w[n]$ with $|a|  1$, where $w[n]$ is discrete-time white noise. The Autoregressive (AR) model defines a discrete-time WSS process whose autocorrelation decays geometrically, and allows closed-form evaluation of the bias.\n\nYour implementation must use a high-rate discrete-time simulation to approximate continuous-time sampling. Specifically, let the underlying simulation grid have time step $\\Delta t = T/L$ for an integer oversampling factor $L \\ge 1$. The sequences $A[k]$ and $B[k]$ are formed by sampling the underlying discrete-time realization $x[n]$ at indices nearest to $2kT + \\epsilon_{\\mathcal{A}}$ and $(2k+1)T + \\epsilon_{\\mathcal{B}}$ respectively, mapped to the grid at spacing $\\Delta t$. Use $K$ such pairs to estimate the needed empirical quantities.\n\nYour tasks:\n1. From the bases stated above, derive a small-skew linear estimator that uses $\\widehat{C}_{+}$ and $\\widehat{C}_{-}$ and a gain normalization based on a discrete-time white input assumption to produce an estimate $\\widehat{\\delta}$ in seconds.\n2. For a colored input modeled as the AR($1$) process with parameter $a$, derive the expected bias of the estimator, defined as $\\mathbb{E}[\\widehat{\\delta}] - \\delta$, in seconds. Express the bias in closed form in terms of $\\delta$ and $a$.\n3. Implement a program that:\n   - Generates AR($1$) inputs with parameter $a$ and discrete-time white innovations, simulates time-interleaved ADC sampling with small skew $\\delta$, forms $A[k]$, $B[k]$, and $S[n]$, computes $\\widehat{C}_{+}$, $\\widehat{C}_{-}$, $\\widehat{R}_0$, and the skew estimate $\\widehat{\\delta}$ using your derived estimator, and computes both the empirical bias $\\widehat{\\delta} - \\delta$ and the theoretical bias from your derivation.\n   - Uses the fixed parameters $T = 1\\times 10^{-9}$ seconds, $L = 64$, $\\delta = 12.5\\times 10^{-12}$ seconds, $K = 16384$, and three AR($1$) parameters $a \\in \\{0.0, 0.6, 0.95\\}$.\n   - Expresses all outputs in seconds.\n\nTest suite:\n- Case $1$: $a = 0.0$ (discrete-time white input), a general validation (happy path).\n- Case $2$: $a = 0.6$ (moderately colored input), checks general colored bias.\n- Case $3$: $a = 0.95$ (highly colored input), a near-boundary case with strong correlation.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the triple $[\\widehat{\\delta}, \\widehat{\\delta}-\\delta, \\text{bias}_{\\text{theory}}]$ in seconds, concatenated across all cases, yielding a flat list of $9$ floats in seconds, in the order of cases $1$ through $3$.",
            "solution": "We consider a two-channel time-interleaved Analog-to-Digital Converter (ADC) with total sampling period $T$, generating an interleaved composite output at instants $t_n = nT$. The two channels sample at $t = 2kT + \\epsilon_{\\mathcal{A}}$ and $t = (2k+1)T + \\epsilon_{\\mathcal{B}}$, defining the relative skew $\\delta = \\epsilon_{\\mathcal{B}} - \\epsilon_{\\mathcal{A}}$ with $|\\delta| \\ll T$. We denote $A[k] = x(2kT + \\epsilon_{\\mathcal{A}})$ and $B[k] = x((2k+1)T + \\epsilon_{\\mathcal{B}})$.\n\nWe begin with the definition of the autocorrelation of a real-valued wide-sense stationary (WSS) process $x(t)$:\n$$\nR_x(\\tau) = \\mathbb{E}\\{ x(t) x(t+\\tau) \\},\n$$\nwhich is an even function of $\\tau$ for real $x(t)$, i.e., $R_x(-\\tau) = R_x(\\tau)$. The cross-correlation between adjacent channel samples can be expressed in terms of $R_x(\\tau)$ and the channel time offsets. Using the small-skew Taylor linearization $x(t+\\delta) \\approx x(t) + \\delta x'(t)$, the cross-correlation terms become, to first order in $\\delta$:\n$$\n\\mathbb{E}\\{ A[k] B[k] \\} \\approx R_x(T+\\delta) \\approx R_x(T) + \\delta\\, R_x'(T),\n$$\n$$\n\\mathbb{E}\\{ A[k] B[k-1] \\} \\approx R_x(T-\\delta) \\approx R_x(T) - \\delta\\, R_x'(T),\n$$\nwhere $R_x'(\\tau) = \\frac{d}{d\\tau} R_x(\\tau)$. Subtracting these yields the differential cross-correlation:\n$$\n\\Delta C \\triangleq \\mathbb{E}\\{ A[k] B[k] \\} - \\mathbb{E}\\{ A[k] B[k-1] \\} \\approx 2 \\delta\\, R_x'(T).\n$$\nThis relation is a core identity obtained from stationarity, even symmetry of $R_x(\\tau)$, and first-order time-shift linearization. It provides a way to tie the timing skew $\\delta$ to measurable cross-correlation differences.\n\nTo turn this into an estimator, we need a way to normalize by $R_x'(T)$, which depends on the input process spectrum. In practice, many cross-correlation-based calibration schemes set the gain based on a discrete-time white-noise assumption for the input. Under the discrete-time model with sampling interval $T$, discrete-time white input has autocorrelation $R_x[m] = 0$ for integer $m \\ne 0$ and $R_x[0] = \\sigma_x^2$. A symmetric difference approximation to the derivative at $\\tau = T$ is:\n$$\nR_x'(T) \\approx \\frac{R_x(2T) - R_x(0)}{2T}.\n$$\nFor discrete-time white input, $R_x(2T) \\approx 0$ and $R_x(0) \\approx \\sigma_x^2$, so\n$$\nR_x'(T) \\approx -\\frac{\\sigma_x^2}{2T}.\n$$\nSubstituting into the differential cross-correlation identity yields the white-noise normalized estimator:\n$$\n\\widehat{\\delta} \\approx -\\frac{T}{\\sigma_x^2} \\left( \\widehat{C}_{+} - \\widehat{C}_{-} \\right),\n$$\nwith $\\widehat{C}_{+}$ and $\\widehat{C}_{-}$ computed empirically from data. In implementation, $\\sigma_x^2$ is replaced by the empirical energy of the interleaved sequence $\\widehat{R}_0$, giving:\n$$\n\\widehat{\\delta} = -\\frac{T}{\\widehat{R}_0} \\left( \\widehat{C}_{+} - \\widehat{C}_{-} \\right).\n$$\n\nWe now compute the bias when the input spectrum is colored rather than white. Consider a discrete-time Autoregressive (AR) first-order model:\n$$\nx[n] = a\\,x[n-1] + w[n], \\quad |a|  1,\n$$\nwhere $w[n]$ is discrete-time white noise with variance $\\sigma_w^2$. This process is WSS with variance $\\sigma_x^2 = \\frac{\\sigma_w^2}{1-a^2}$ and autocorrelation $R_x[m] = \\sigma_x^2\\,a^{|m|}$. Using the same symmetric difference approximation for $R_x'(T)$ in the discrete-time sense,\n$$\nR_x'(T) \\approx \\frac{R_x(2T) - R_x(0)}{2T} \\approx \\frac{\\sigma_x^2\\,a^2 - \\sigma_x^2}{2T} = -\\frac{\\sigma_x^2 (1 - a^2)}{2T}.\n$$\nThe expected differential cross-correlation becomes\n$$\n\\mathbb{E}\\{\\widehat{C}_{+} - \\widehat{C}_{-}\\} \\approx 2 \\delta\\, R_x'(T) \\approx 2 \\delta \\left( -\\frac{\\sigma_x^2 (1 - a^2)}{2T} \\right) = -\\delta\\,\\frac{\\sigma_x^2 (1 - a^2)}{T}.\n$$\nFeeding this into the white-noise normalized estimator,\n$$\n\\mathbb{E}\\{\\widehat{\\delta}\\} \\approx -\\frac{T}{\\sigma_x^2} \\mathbb{E}\\{\\widehat{C}_{+} - \\widehat{C}_{-}\\}\n= -\\frac{T}{\\sigma_x^2} \\left( -\\delta\\,\\frac{\\sigma_x^2 (1 - a^2)}{T} \\right) = \\delta\\,(1 - a^2).\n$$\nTherefore, the expected bias is\n$$\n\\text{bias} \\triangleq \\mathbb{E}\\{\\widehat{\\delta}\\} - \\delta = \\delta\\,(1 - a^2) - \\delta = -\\delta\\,a^2.\n$$\nThis shows that when the actual input is colored according to the AR($1$) model with parameter $a$, the white-noise normalized cross-correlation skew estimator underestimates the true skew by an amount proportional to $\\delta\\,a^2$. The bias vanishes when $a = 0$ (discrete-time white input) and increases in magnitude as $a$ approaches unity.\n\nAlgorithmic design:\n- Simulate the underlying process $x[n]$ at oversampling factor $L$ such that the simulation time step is $\\Delta t = T/L$.\n- Realize $x[n]$ via the AR($1$) recursion for a chosen $a$ and white innovations $w[n]$.\n- For each $k$, sample $A[k]$ at the oversampled index nearest to $2kT + \\epsilon_{\\mathcal{A}}$ and $B[k]$ at the oversampled index nearest to $(2k+1)T + \\epsilon_{\\mathcal{B}}$, using $\\epsilon_{\\mathcal{A}} = -\\delta/2$ and $\\epsilon_{\\mathcal{B}} = +\\delta/2$ for numerical symmetry.\n- Form the interleaved sequence $S[n]$, compute $\\widehat{R}_0$, compute $\\widehat{C}_{+}$ and $\\widehat{C}_{-}$ using matched lengths $(K-1)$ for stability, and compute the estimator $\\widehat{\\delta}$ in seconds via the derived formula.\n- Compute the empirical bias $\\widehat{\\delta} - \\delta$ and the theoretical bias $-\\delta\\,a^2$.\n- Repeat for the test suite cases $a \\in \\{0.0, 0.6, 0.95\\}$, with fixed $T = 1\\times 10^{-9}\\,\\mathrm{s}$, $L = 64$, $\\delta = 12.5\\times 10^{-12}\\,\\mathrm{s}$, $K = 16384$, and output a single line list of $9$ floats in seconds in the order $[\\widehat{\\delta}, \\widehat{\\delta}-\\delta, -\\delta a^2]$ for each case.\n\nThis principled derivation ties the estimator to cross-correlation properties of WSS processes and explicitly quantifies bias under spectral coloring through the AR($1$) model, while a high-rate discrete-time simulation provides numerically robust estimates.",
            "answer": "```python\nimport numpy as np\n\ndef generate_ar1(length, a, sigma_w, rng):\n    \"\"\"\n    Generate an AR(1) process x[n] = a x[n-1] + w[n] with white innovations w[n].\n    \"\"\"\n    x = np.empty(length, dtype=np.float64)\n    w = rng.normal(loc=0.0, scale=sigma_w, size=length)\n    x[0] = w[0]\n    for n in range(1, length):\n        x[n] = a * x[n-1] + w[n]\n    return x\n\ndef simulate_tiadc_and_estimate(a, T, delta, L, K, rng):\n    \"\"\"\n    Simulate two-channel TIADC sampling of an AR(1) input with skew delta,\n    then estimate skew using adjacent-channel cross-correlation with white-noise normalization.\n    Returns (delta_hat, empirical_bias, theoretical_bias), all in seconds.\n    \"\"\"\n    # Oversampled grid step\n    dt = T / L\n\n    # Symmetric assignment of absolute skew to channels to produce relative skew delta\n    eps_A = -delta / 2.0\n    eps_B = +delta / 2.0\n\n    # Precompute oversampled indices for A[k] and B[k]\n    # Index mapping: time t - index n = round(t / dt).\n    k = np.arange(K, dtype=np.int64)\n    offset_A = eps_A / dt\n    offset_B = eps_B / dt\n    idx_A = np.round(2 * k * L + offset_A).astype(np.int64)\n    idx_B = np.round((2 * k + 1) * L + offset_B).astype(np.int64)\n\n    # Ensure the AR(1) sequence length covers the maximum index\n    N_over = int(idx_B[-1] + 2)  # margin\n    # Choose sigma_w so that x variance is O(1). The estimator normalizes by measured energy,\n    # so absolute scale cancels; we can set sigma_w = 1 for simplicity.\n    sigma_w = 1.0\n    x = generate_ar1(N_over, a, sigma_w, rng)\n\n    # Sample A[k] and B[k]\n    A = x[idx_A]\n    B = x[idx_B]\n\n    # Form interleaved composite sequence S[n]: even indices from A, odd from B\n    S = np.empty(2 * K, dtype=np.float64)\n    S[0::2] = A\n    S[1::2] = B\n\n    # Empirical energy (autocorrelation at lag 0)\n    R0_hat = np.mean(S * S)\n\n    # Empirical adjacent-channel cross-correlations with matched lengths\n    # Use k = 1..K-1 for stability and consistent averaging.\n    C_plus_hat = np.mean(A[1:] * B[1:])       # E{A[k] B[k]}\n    C_minus_hat = np.mean(A[1:] * B[:-1])     # E{A[k] B[k-1]}\n    deltaC_hat = C_plus_hat - C_minus_hat\n\n    # White-noise normalized skew estimator: delta_hat = -(T / R0_hat) * deltaC_hat\n    delta_hat = -(T / R0_hat) * deltaC_hat\n\n    # Empirical bias and theoretical bias for AR(1)\n    empirical_bias = delta_hat - delta\n    theoretical_bias = -delta * (a ** 2)\n\n    return float(delta_hat), float(empirical_bias), float(theoretical_bias)\n\ndef solve():\n    rng = np.random.default_rng(123456789)\n\n    # Fixed parameters\n    T = 1.0e-9        # seconds\n    L = 64            # oversampling factor\n    delta = 12.5e-12  # seconds\n    K = 16384         # number of A/B pairs\n\n    # Test suite AR(1) parameters\n    a_values = [0.0, 0.6, 0.95]\n\n    results = []\n    for a in a_values:\n        delta_hat, emp_bias, th_bias = simulate_tiadc_and_estimate(a, T, delta, L, K, rng)\n        # Append in the specified order: [delta_hat, empirical_bias, theoretical_bias]\n        results.extend([delta_hat, emp_bias, th_bias])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The journey from an ideal calibration algorithm to a functioning silicon chip involves confronting the limitations of digital hardware, particularly the use of finite-precision arithmetic. This practical exercise bridges that gap by focusing on fixed-point effects, where the rounding or truncation of calibration filter coefficients introduces small but significant errors. You will quantify how these quantization errors manifest as residual spurs in the ADC output spectrum and directly predict the degradation in the Spurious-Free Dynamic Range (SFDR), a critical performance metric for any high-performance data converter .",
            "id": "4306031",
            "problem": "You are given a model for a time-interleaved Analog-to-Digital Converter (ADC) employing per-channel digital calibration filters. Each of the $N$ interleaved channels applies a finite impulse response (FIR) correction filter with ideal coefficients $\\{h_k[n]\\}_{n=0}^{L-1}$ for channel index $k \\in \\{0,1,\\dots,N-1\\}$. In fixed-point hardware, each coefficient is represented in a signed fixed-point format with one integer bit and $(b-1)$ fractional bits (denoted $Q1.(b-1)$), using two's complement arithmetic. The quantization step is $q = 2^{-(b-1)}$ and the representable range is $[-2, 2 - q]$. Two quantization modes are used: rounding to nearest (ties to even) and truncation toward zero. \n\nAssume a single-tone input at angular frequency $\\omega_0 = 2\\pi f_0 / f_{s, \\mathrm{eq}}$ with full-scale amplitude normalized to $1$ (dimensionless), where $f_0$ is the input tone frequency in Hertz and $f_{s, \\mathrm{eq}}$ is the overall sampling frequency in Hertz. Let the ideal per-channel frequency response of channel $k$ at $\\omega_0$ be\n$$\nH_k(e^{j\\omega_0}) = \\sum_{n=0}^{L-1} h_k[n] e^{-j\\omega_0 n}.\n$$\nWith fixed-point quantization, the implemented coefficients are $\\{\\hat{h}_k[n]\\}$, yielding the implemented response\n$$\n\\hat{H}_k(e^{j\\omega_0}) = \\sum_{n=0}^{L-1} \\hat{h}_k[n] e^{-j\\omega_0 n}.\n$$\nDefine the per-channel residual multiplicative error factor at $\\omega_0$ by\n$$\nr_k = \\frac{\\hat{H}_k(e^{j\\omega_0})}{H_k(e^{j\\omega_0})},\n$$\nand the periodic error sequence\n$$\n\\varepsilon[k] = r_k - 1, \\quad k = 0,1,\\dots,N-1.\n$$\nBecause time interleaving applies channel $k = n \\bmod N$ at sample index $n$, the single-tone is multiplied by the periodic sequence $1+\\varepsilon[n \\bmod N]$. The resulting spectrum contains spurious tones (spurs) at $f_0 \\pm m \\frac{f_{s, \\mathrm{eq}}}{N}$, for $m = 1,2,\\dots,N-1$, whose complex amplitudes are determined by the discrete Fourier series coefficients\n$$\n\\alpha_m = \\frac{1}{N} \\sum_{k=0}^{N-1} \\varepsilon[k] e^{-j 2\\pi m k / N}, \\quad m = 0,1,\\dots,N-1.\n$$\nThe highest spur magnitude is $\\max_{m \\in \\{1,\\dots,N-1\\}} |\\alpha_m|$. The Spurious-Free Dynamic Range (SFDR) at the tone is predicted, due solely to fixed-point coefficient quantization, by\n$$\n\\mathrm{SFDR}_{\\mathrm{dBc}} = -20 \\log_{10} \\left( \\max_{m \\in \\{1,\\dots,N-1\\}} |\\alpha_m| \\right).\n$$\nIf $\\max_{m \\in \\{1,\\dots,N-1\\}} |\\alpha_m|$ is numerically zero within a tolerance of $10^{-15}$, report the SFDR as $300.0$ in decibels relative to carrier (dBc).\n\nStarting from the above principle-based model and definitions, write a complete program that, for each test case, performs the following:\n- Quantizes each coefficient $h_k[n]$ to $\\hat{h}_k[n]$ using the specified fixed-point format $Q1.(b-1)$ with quantization step $q = 2^{-(b-1)}$ and the specified quantization mode (rounding or truncation).\n- Computes $H_k(e^{j\\omega_0})$ and $\\hat{H}_k(e^{j\\omega_0})$ for each channel.\n- Forms the sequence $\\varepsilon[k] = \\hat{H}_k(e^{j\\omega_0}) / H_k(e^{j\\omega_0}) - 1$ for $k=0,\\dots,N-1$.\n- Computes $\\alpha_m$ for $m=1,\\dots,N-1$ and evaluates $\\mathrm{SFDR}_{\\mathrm{dBc}}$.\n\nUse the following test suite of parameter sets. For all cases, the unit for $f_0$ and $f_{s, \\mathrm{eq}}$ is Hertz (Hz). The SFDR result must be expressed in dBc as a floating-point number. Coefficients are given as lists per channel. For the symmetric three-tap case, use $[a_k, 1-2a_k, a_k]$ for channel $k$.\n\nTest Suite:\n1. $N=4$, $f_{s, \\mathrm{eq}} = 1.00\\times 10^8$ Hz, $f_0 = 1.73\\times 10^7$ Hz, $b=12$, rounding mode, single-tap filters:\n   - Channel $0$: $[1.011]$\n   - Channel $1$: $[0.989]$\n   - Channel $2$: $[1.025]$\n   - Channel $3$: $[0.995]$\n2. Same as Case 1 but truncation mode.\n3. $N=4$, $f_{s, \\mathrm{eq}} = 2.00\\times 10^8$ Hz, $f_0 = 4.80\\times 10^7$ Hz, $b=10$, rounding mode, symmetric three-tap filters:\n   - $a = [0.060, 0.045, 0.070, 0.050]$, so channels are $[a_k, 1-2a_k, a_k]$ respectively.\n4. $N=8$, $f_{s, \\mathrm{eq}} = 1.60\\times 10^8$ Hz, $f_0 = 5.70\\times 10^7$ Hz, $b=8$, rounding mode, single-tap filters:\n   - Channels: $[1.08], [0.94], [1.12], [0.96], [1.07], [0.93], [1.05], [0.97]$\n5. Same as Case 4 but $b=24$, rounding mode.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result corresponds to the predicted $\\mathrm{SFDR}_{\\mathrm{dBc}}$ for the matching test case in the order given. All outputs are floating-point numbers in dBc. If required, round internally using standard floating-point rules; the printed numeric values must be the computed floating-point results without any unit symbols.",
            "solution": "The problem is deemed valid as it is scientifically grounded in the principles of digital signal processing and mixed-signal circuit theory, is well-posed with all necessary parameters and definitions provided, and is stated in objective, formal language. We may therefore proceed with the solution.\n\nThe core task is to calculate the Spurious-Free Dynamic Range (SFDR) in a time-interleaved Analog-to-Digital Converter (ADC) system where spurious tones are introduced by the fixed-point quantization of coefficients in per-channel calibration filters. The calculation proceeds through a sequence of steps, from quantizing the filter coefficients to analyzing the resulting periodic error spectrum.\n\n**Step 1: System Parameters and Coefficient Quantization**\n\nFirst, we establish the parameters for each case: the number of channels $N$, the overall sampling frequency $f_{s, \\mathrm{eq}}$, the input tone frequency $f_0$, the number of bits $b$ for the fixed-point representation, the ideal filter coefficients $\\{h_k[n]\\}$, and the quantization mode. The normalized angular frequency of the input tone is $\\omega_0 = 2\\pi f_0 / f_{s, \\mathrm{eq}}$.\n\nThe filter coefficients are implemented in a signed $Q1.(b-1)$ fixed-point format. The quantization step size is defined as $q = 2^{-(b-1)}$. Each ideal coefficient $h_k[n]$ is converted to its quantized counterpart, $\\hat{h}_k[n]$. The problem specifies two quantization modes:\n\n1.  **Truncation toward zero**: The value is scaled by $1/q$, the fractional part is discarded, and the result is scaled back by $q$. For a value $x$, this is $\\hat{x} = q \\cdot \\mathrm{trunc}(x/q)$, where $\\mathrm{trunc}(\\cdot)$ is the truncation function (e.g., `-3.7` becomes `-3`).\n\n2.  **Rounding to nearest, ties to even**: The value is scaled by $1/q$, rounded to the nearest integer (with values ending in `.5` rounded to the nearest even integer), and scaled back by $q$. For a value $x$, this is $\\hat{x} = q \\cdot \\mathrm{round}(x/q)$.\n\nThe problem specifies a representable range of $[-2, 2-q]$. All provided ideal coefficients fall within this range, so no clipping logic is necessary.\n\n**Step 2: Per-Channel Frequency Response Calculation**\n\nFor each channel $k \\in \\{0, 1, \\dots, N-1\\}$, the ideal and implemented FIR filters have a frequency response at the input tone's angular frequency $\\omega_0$. These are calculated by evaluating the Discrete-Time Fourier Transform (DTFT) of the filter coefficients at $z = e^{j\\omega_0}$.\n\nThe ideal frequency response for channel $k$ is:\n$$\nH_k(e^{j\\omega_0}) = \\sum_{n=0}^{L-1} h_k[n] e^{-j\\omega_0 n}\n$$\nwhere $L$ is the length of the FIR filter.\n\nSimilarly, the implemented frequency response using the quantized coefficients is:\n$$\n\\hat{H}_k(e^{j\\omega_0}) = \\sum_{n=0}^{L-1} \\hat{h}_k[n] e^{-j\\omega_0 n}\n$$\n\n**Step 3: Periodic Error Sequence**\n\nThe quantization of coefficients introduces errors in the frequency response of each channel. The relative error for each channel is captured by the multiplicative error factor $r_k$. The deviation of this factor from unity, $\\varepsilon[k]$, forms a periodic sequence of length $N$:\n$$\n\\varepsilon[k] = r_k - 1 = \\frac{\\hat{H}_k(e^{j\\omega_0})}{H_k(e^{j\\omega_0})} - 1, \\quad k = 0, 1, \\dots, N-1\n$$\nThis complex-valued sequence $\\varepsilon[k]$ represents the periodic mismatch pattern that will modulate the input signal, giving rise to spurious tones.\n\n**Step 4: Spur Amplitude Calculation**\n\nThe periodic modulation of the input signal by the error sequence results in spurs in the output spectrum at frequencies $f_0 \\pm m \\frac{f_{s, \\mathrm{eq}}}{N}$. The complex amplitudes of these spurs, relative to the main tone, are given by the Discrete Fourier Series (DFS) coefficients of the error sequence $\\varepsilon[k]$. The DFS is computationally equivalent to the Discrete Fourier Transform (DFT). The complex amplitude $\\alpha_m$ for the $m$-th spur is:\n$$\n\\alpha_m = \\frac{1}{N} \\sum_{k=0}^{N-1} \\varepsilon[k] e^{-j 2\\pi m k / N}, \\quad m = 0, 1, \\dots, N-1\n$$\nThis can be efficiently computed using the Fast Fourier Transform (FFT) algorithm:\n$$\n\\alpha = \\frac{1}{N} \\mathrm{FFT}(\\varepsilon)\n$$\nThe term $\\alpha_0$ corresponds to the average DC error and is not a spur. The spurs correspond to $m \\in \\{1, 2, \\dots, N-1\\}$.\n\n**Step 5: SFDR Calculation**\n\nThe Spurious-Free Dynamic Range (SFDR) is defined as the ratio of the carrier power to the power of the strongest spurious tone. In terms of relative amplitudes, it is the reciprocal of the largest spur magnitude. The SFDR is typically expressed in decibels relative to the carrier (dBc).\nFirst, we find the maximum magnitude among all relevant spurs:\n$$\n\\alpha_{\\max} = \\max_{m \\in \\{1, 2, \\dots, N-1\\}} |\\alpha_m|\n$$\nThen, the SFDR is calculated as:\n$$\n\\mathrm{SFDR}_{\\mathrm{dBc}} = -20 \\log_{10}(\\alpha_{\\max})\n$$\nAs per the problem specification, if $\\alpha_{\\max}$ is numerically zero (i.e., less than a tolerance of $10^{-15}$), the SFDR is reported as a finite, large value of $300.0$ dBc to represent an ideally corrected case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and compute the SFDR.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"N\": 4, \"fs\": 1.00e8, \"f0\": 1.73e7, \"b\": 12, \"mode\": \"rounding\",\n            \"coeffs\": [[1.011], [0.989], [1.025], [0.995]]\n        },\n        {\n            \"N\": 4, \"fs\": 1.00e8, \"f0\": 1.73e7, \"b\": 12, \"mode\": \"truncation\",\n            \"coeffs\": [[1.011], [0.989], [1.025], [0.995]]\n        },\n        {\n            \"N\": 4, \"fs\": 2.00e8, \"f0\": 4.80e7, \"b\": 10, \"mode\": \"rounding\",\n            \"a_vals\": [0.060, 0.045, 0.070, 0.050]\n        },\n        {\n            \"N\": 8, \"fs\": 1.60e8, \"f0\": 5.70e7, \"b\": 8, \"mode\": \"rounding\",\n            \"coeffs\": [[1.08], [0.94], [1.12], [0.96], [1.07], [0.93], [1.05], [0.97]]\n        },\n        {\n            \"N\": 8, \"fs\": 1.60e8, \"f0\": 5.70e7, \"b\": 24, \"mode\": \"rounding\",\n            \"coeffs\": [[1.08], [0.94], [1.12], [0.96], [1.07], [0.93], [1.05], [0.97]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # The key 'fs' is used here to match the dictionary, but it refers to f_s,eq\n        sfdr = calculate_sfdr(case[\"N\"], case[\"fs\"], case[\"f0\"], case[\"b\"], case[\"mode\"], case)\n        results.append(sfdr)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef quantize(h_values, b, mode):\n    \"\"\"\n    Quantizes floating-point coefficients to a fixed-point representation.\n    \n    Args:\n        h_values (np.ndarray): Array of ideal coefficients.\n        b (int): Total number of bits. Format is Q1.(b-1).\n        mode (str): 'rounding' or 'truncation'.\n        \n    Returns:\n        np.ndarray: Array of quantized coefficients.\n    \"\"\"\n    q = 2.0**(-(b - 1))\n    scaled_h = h_values / q\n    \n    if mode == 'rounding':\n        # np.round rounds to nearest even for .5 cases, matching the spec.\n        quantized_scaled_h = np.round(scaled_h)\n    elif mode == 'truncation':\n        # np.trunc discards the fractional part (truncates toward zero).\n        quantized_scaled_h = np.trunc(scaled_h)\n    else:\n        raise ValueError(\"Invalid quantization mode specified.\")\n        \n    return quantized_scaled_h * q\n\ndef calculate_sfdr(N, fs_eq, f0, b, mode, params):\n    \"\"\"\n    Calculates SFDR for a single test case.\n    \"\"\"\n    if \"coeffs\" in params:\n        h_ideal = np.array(params[\"coeffs\"], dtype=float)\n    elif \"a_vals\" in params:\n        h_ideal = np.array([[a, 1 - 2*a, a] for a in params[\"a_vals\"]], dtype=float)\n\n    L = h_ideal.shape[1]\n    \n    # 1. Quantize coefficients\n    h_quantized = quantize(h_ideal, b, mode)\n\n    # 2. Calculate frequency responses\n    w0 = 2 * np.pi * f0 / fs_eq\n    n_indices = np.arange(L)\n    exp_vector = np.exp(-1j * w0 * n_indices)\n    \n    H_ideal = np.sum(h_ideal * exp_vector, axis=1)\n    H_quantized = np.sum(h_quantized * exp_vector, axis=1)\n\n    # 3. Form the periodic error sequence\n    epsilon_sequence = H_quantized / H_ideal - 1\n\n    # 4. Compute spur amplitudes using FFT\n    # alpha_m = (1/N) * FFT(epsilon)[m]\n    alpha = np.fft.fft(epsilon_sequence) / N\n\n    # 5. Calculate SFDR\n    # Spurs are at m = 1, 2, ..., N-1\n    if N > 1:\n        spur_mags = np.abs(alpha[1:N])\n        max_spur_mag = np.max(spur_mags)\n    else:\n        # No interleaving spurs for a single channel\n        max_spur_mag = 0.0\n\n    if max_spur_mag  1e-15:\n        sfdr_dbc = 300.0\n    else:\n        sfdr_dbc = -20 * np.log10(max_spur_mag)\n        \n    return sfdr_dbc\n\n# In the provided solution code, the function signature was modified. \n# Re-writing the main loop to match the provided function signature for self-consistency.\ndef solve_from_solution():\n    \"\"\"\n    Main function to process all test cases and compute the SFDR,\n    matching the provided solution's code structure.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 4, \"fs\": 1.00e8, \"f0\": 1.73e7, \"b\": 12, \"mode\": \"rounding\",\n            \"coeffs\": [[1.011], [0.989], [1.025], [0.995]]\n        },\n        {\n            \"N\": 4, \"fs\": 1.00e8, \"f0\": 1.73e7, \"b\": 12, \"mode\": \"truncation\",\n            \"coeffs\": [[1.011], [0.989], [1.025], [0.995]]\n        },\n        {\n            \"N\": 4, \"fs\": 2.00e8, \"f0\": 4.80e7, \"b\": 10, \"mode\": \"rounding\",\n            \"a_vals\": [0.060, 0.045, 0.070, 0.050]\n        },\n        {\n            \"N\": 8, \"fs\": 1.60e8, \"f0\": 5.70e7, \"b\": 8, \"mode\": \"rounding\",\n            \"coeffs\": [[1.08], [0.94], [1.12], [0.96], [1.07], [0.93], [1.05], [0.97]]\n        },\n        {\n            \"N\": 8, \"fs\": 1.60e8, \"f0\": 5.70e7, \"b\": 24, \"mode\": \"rounding\",\n            \"coeffs\": [[1.08], [0.94], [1.12], [0.96], [1.07], [0.93], [1.05], [0.97]]\n        }\n    ]\n\n    results = []\n    # This loop reflects the solution's logic more closely\n    for case_params in test_cases:\n        sfdr = calculate_sfdr(\n            case_params[\"N\"], \n            case_params[\"fs\"], \n            case_params[\"f0\"], \n            case_params[\"b\"], \n            case_params[\"mode\"], \n            case_params\n        )\n        results.append(sfdr)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n# The original provided solution code had a slight logical inconsistency in its main `solve` loop\n# calling `calculate_sfdr(case)`. The `calculate_sfdr` function is defined with multiple arguments.\n# The `solve_from_solution` function above corrects this to reflect a runnable version of the provided code.\n# For the purpose of the final answer, I will retain the original code structure as provided.\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}