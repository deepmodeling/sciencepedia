{
    "hands_on_practices": [
        {
            "introduction": "The foundation of manufacturing yield modeling is understanding how random defects are distributed across a wafer. While the simplest model assumes defects are uniformly random, reality is often more complex, with defects tending to cluster. This practice explores the critical difference between the simple Poisson model for homogeneous defects and the more realistic Negative Binomial model, which accounts for this clustering. By deriving both yield formulas and quantifying their divergence for a large die, you will gain a crucial insight into why larger chips can be more economically viable than predicted by simplistic models and understand the importance of spatial statistics in accurate yield prediction. ",
            "id": "4281279",
            "problem": "A fabrication line for integrated circuits operates in a regime where random fatal defects can be treated using point processes. Assume the following modeling base:\n\n- Under spatially homogeneous defects, the number of fatal defects on a die of area $A$ follows a Poisson distribution with mean $\\mu = D_0 A$, where $D_0$ is the defect density in units of defects per square centimeter. The die yield is the probability that a die has zero fatal defects.\n\n- Under spatially clustered defects, the local defectivity is heterogeneous across the wafer: conditional on a local defectivity $\\Lambda$, the defects on a die are Poisson with mean $\\Lambda A$, and the heterogeneity in $\\Lambda$ across dies follows a Gamma distribution with a shape parameter $\\alpha$ that quantifies clustering. The die yield is the probability of zero fatal defects marginalized over this heterogeneity.\n\nStarting strictly from these definitions, derive closed-form expressions for the die yield under the homogeneous (Poisson) and clustered (Gamma-Poisson mixture) cases, then determine the die area $A$ (in $\\mathrm{cm}^2$) at which the clustered-model yield exceeds the homogeneous-model yield by a factor of $1.05$ for a process characterized by $D_0 = 0.3$ defects per $\\mathrm{cm}^2$ and clustering parameter $\\alpha = 5$. Round your final numerical result for $A$ to three significant figures and express it in $\\mathrm{cm}^2$. In addition, discuss the practical implications of your result for predicting manufacturing yield and for cost sensitivity to die size under clustering. Your final answer must be only the computed value of $A$.",
            "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   For a homogeneous defect distribution, the number of fatal defects on a die of area $A$ follows a Poisson distribution with mean $\\mu = D_0 A$.\n-   $D_0$ is the defect density.\n-   The yield is the probability of zero fatal defects.\n-   For a clustered defect distribution, the number of fatal defects on a die of area $A$ follows a Poisson distribution with mean $\\Lambda A$, conditional on a local defectivity $\\Lambda$.\n-   The local defectivity $\\Lambda$ follows a Gamma distribution with a shape parameter $\\alpha$.\n-   The goal is to find the die area $A$ (in $\\mathrm{cm}^2$) where the clustered-model yield is $1.05$ times the homogeneous-model yield.\n-   The given parameters are $D_0 = 0.3 \\text{ defects}/\\mathrm{cm}^2$ and $\\alpha = 5$.\n-   The final numerical result for $A$ must be rounded to three significant figures.\n-   The solution must include a discussion of practical implications.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective. It is based on standard and widely accepted statistical models for manufacturing yield in integrated circuits: the Poisson model for purely random defects and the Gamma-Poisson (Negative Binomial) model for clustered defects. The provided parameters, $D_0 = 0.3 \\text{ defects}/\\mathrm{cm}^2$ and $\\alpha = 5$, are realistic for a semiconductor manufacturing process. The problem implicitly assumes that the mean of the Gamma distribution for the local defectivity, $E[\\Lambda]$, is equal to the global average defect density $D_0$. This is a standard and necessary condition for a meaningful comparison between the two models and does not render the problem invalid; rather, it is a standard convention in the field. The problem is self-contained, mathematically formalizable, and has a unique, meaningful solution.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Derivation of Yield Models\n\nFirst, we derive the closed-form expressions for the die yield under both models as defined in the problem. The yield, $Y$, is the probability that a die has zero fatal defects.\n\n**1. Homogeneous (Poisson) Model**\n\nLet $N$ be the random variable representing the number of fatal defects on a die of area $A$. According to the problem, $N$ follows a Poisson distribution with mean $\\mu = D_0 A$. The probability mass function (PMF) of a Poisson distribution is given by:\n$$\nP(N=k) = \\frac{\\mu^{k} e^{-\\mu}}{k!} = \\frac{(D_0 A)^{k} \\exp(-D_0 A)}{k!}\n$$\nThe yield, which we denote as $Y_{\\text{hom}}$, is the probability of having zero defects ($k=0$):\n$$\nY_{\\text{hom}}(A) = P(N=0) = \\frac{(D_0 A)^{0} \\exp(-D_0 A)}{0!}\n$$\nSince $x^{0}=1$ and $0!=1$, this simplifies to the well-known Poisson yield model:\n$$\nY_{\\text{hom}}(A) = \\exp(-D_0 A)\n$$\n\n**2. Clustered (Gamma-Poisson) Model**\n\nIn this model, the defect density is not uniform. The local defect density $\\Lambda$ is itself a random variable following a Gamma distribution. The probability density function (PDF) of a Gamma distribution with shape parameter $\\alpha$ and scale parameter $\\theta$ is:\n$$\nf(\\lambda; \\alpha, \\theta) = \\frac{\\lambda^{\\alpha-1} \\exp(-\\lambda/\\theta)}{\\theta^{\\alpha} \\Gamma(\\alpha)} \\quad \\text{for } \\lambda \\ge 0\n$$\nwhere $\\Gamma(\\alpha)$ is the Gamma function. For the two models to be comparable, they must share the same overall average defect density. We set the expected value of $\\Lambda$ equal to $D_0$:\n$$\nE[\\Lambda] = \\alpha\\theta = D_0\n$$\nFrom this, we determine the scale parameter $\\theta = D_0/\\alpha$. Substituting this back into the PDF:\n$$\nf(\\lambda) = \\frac{\\lambda^{\\alpha-1} \\exp(-\\lambda\\alpha/D_0)}{(D_0/\\alpha)^{\\alpha} \\Gamma(\\alpha)}\n$$\nConditional on a specific value $\\Lambda = \\lambda$, the number of defects $N$ follows a Poisson distribution with mean $\\lambda A$. The conditional probability of zero defects is:\n$$\nP(N=0 | \\Lambda = \\lambda) = \\exp(-\\lambda A)\n$$\nTo find the overall yield for the clustered model, $Y_{\\text{clust}}$, we must marginalize over all possible values of $\\lambda$ by integrating the conditional probability weighted by the PDF of $\\Lambda$:\n$$\nY_{\\text{clust}}(A) = \\int_{0}^{\\infty} P(N=0 | \\Lambda=\\lambda) f(\\lambda) \\,d\\lambda = \\int_{0}^{\\infty} \\exp(-\\lambda A) \\frac{\\lambda^{\\alpha-1} \\exp(-\\lambda\\alpha/D_0)}{(D_0/\\alpha)^{\\alpha} \\Gamma(\\alpha)} \\,d\\lambda\n$$\nCombining the exponential terms and factoring out constants:\n$$\nY_{\\text{clust}}(A) = \\frac{1}{(D_0/\\alpha)^{\\alpha} \\Gamma(\\alpha)} \\int_{0}^{\\infty} \\lambda^{\\alpha-1} \\exp\\left(-\\lambda \\left(A + \\frac{\\alpha}{D_0}\\right)\\right) \\,d\\lambda\n$$\nThe integral is of the form of a Gamma function. Using the identity $\\int_{0}^{\\infty} x^{z-1} \\exp(-cx) \\,dx = \\frac{\\Gamma(z)}{c^{z}}$ with $z=\\alpha$ and $c = A + \\frac{\\alpha}{D_0}$, the integral evaluates to:\n$$\n\\int_{0}^{\\infty} \\lambda^{\\alpha-1} \\exp\\left(-\\lambda \\left(\\frac{AD_0+\\alpha}{D_0}\\right)\\right) \\,d\\lambda = \\frac{\\Gamma(\\alpha)}{\\left(\\frac{AD_0+\\alpha}{D_0}\\right)^{\\alpha}}\n$$\nSubstituting this result back into the expression for $Y_{\\text{clust}}(A)$:\n$$\nY_{\\text{clust}}(A) = \\frac{1}{(D_0/\\alpha)^{\\alpha} \\Gamma(\\alpha)} \\cdot \\frac{\\Gamma(\\alpha)}{\\left(\\frac{AD_0+\\alpha}{D_0}\\right)^{\\alpha}} = \\left(\\frac{\\alpha}{D_0}\\right)^{\\alpha} \\left(\\frac{D_0}{AD_0+\\alpha}\\right)^{\\alpha}\n$$\nThis simplifies to the Negative Binomial yield model:\n$$\nY_{\\text{clust}}(A) = \\left(\\frac{\\alpha}{AD_0+\\alpha}\\right)^{\\alpha} = \\left(1 + \\frac{D_0 A}{\\alpha}\\right)^{-\\alpha}\n$$\n\n### Solving for the Die Area A\n\nThe problem requires finding the area $A$ at which the clustered-model yield exceeds the homogeneous-model yield by a factor of $1.05$. This condition is expressed as:\n$$\nY_{\\text{clust}}(A) = 1.05 \\cdot Y_{\\text{hom}}(A)\n$$\nSubstituting the derived yield formulas:\n$$\n\\left(1 + \\frac{D_0 A}{\\alpha}\\right)^{-\\alpha} = 1.05 \\cdot \\exp(-D_0 A)\n$$\nNow, we substitute the given numerical values: $D_0 = 0.3$ and $\\alpha = 5$.\n$$\n\\left(1 + \\frac{0.3 A}{5}\\right)^{-5} = 1.05 \\cdot \\exp(-0.3 A)\n$$\n$$\n(1 + 0.06 A)^{-5} = 1.05 \\cdot \\exp(-0.3 A)\n$$\nThis is a transcendental equation that must be solved for $A$. To facilitate a numerical solution, we can take the natural logarithm of both sides:\n$$\n\\ln\\left((1 + 0.06 A)^{-5}\\right) = \\ln(1.05 \\cdot \\exp(-0.3 A))\n$$\n$$\n-5 \\ln(1 + 0.06 A) = \\ln(1.05) - 0.3 A\n$$\nRearranging the terms, we seek the root of the function $g(A)$:\n$$\ng(A) = 0.3 A - 5 \\ln(1 + 0.06 A) - \\ln(1.05) = 0\n$$\nWe can solve this equation numerically. Using a numerical solver (such as the Newton-Raphson method or a built-in solver function in a computational tool), we find the value of $A$. Let's perform an iterative search for the root.\nThe value of $\\ln(1.05)$ is approximately $0.048790$.\nLet's test $A = 2.4$:\n$g(2.4) = 0.3(2.4) - 5\\ln(1 + 0.06 \\times 2.4) - 0.04879 = 0.72 - 5\\ln(1.144) - 0.04879 \\approx 0.72 - 5(0.13456) - 0.04879 = 0.72 - 0.6728 - 0.04879 = -0.00159$.\nLet's test $A = 2.44$:\n$g(2.44) = 0.3(2.44) - 5\\ln(1 + 0.06 \\times 2.44) - 0.04879 = 0.732 - 5\\ln(1.1464) - 0.04879 \\approx 0.732 - 5(0.13661) - 0.04879 = 0.732 - 0.68305 - 0.04879 = 0.00016$.\nThe root is between $A=2.4$ and $A=2.44$. A more precise calculation yields $A \\approx 2.4384$.\nRounding the result to three significant figures, we get $A = 2.44 \\, \\mathrm{cm}^{2}$.\n\n### Discussion of Practical Implications\n\nThe result shows that for a die area of $A = 2.44 \\, \\mathrm{cm}^2$, a relatively large but common size for modern processors, the more realistic clustered defect model predicts a yield that is $5\\%$ higher than the prediction from the simplistic homogeneous Poisson model. This has two major practical implications:\n\n1.  **For predicting manufacturing yield:** The simple Poisson model is consistently pessimistic. It assumes defects are uniformly distributed, whereas in reality they tend to cluster. This clustering \"protects\" some parts of the wafer, leading to a higher number of defect-free dies than would be expected under a uniform distribution. For a given average defect density $D_0$, the actual yield will be higher than $\\exp(-D_0 A)$. Accurately predicting yield is crucial for financial forecasting and process planning. Relying on the Poisson model could lead to underestimating profitability and making incorrect decisions, such as prematurely discontinuing a promising but seemingly low-yield manufacturing process. Capturing spatial statistics, such as the clustering parameter $\\alpha$, is essential for accurate yield modeling.\n\n2.  **For cost sensitivity to die size under clustering:** The yield penalty for increasing die size is significantly lower in the clustered model compared to the homogeneous model. The Poisson yield, $Y_{\\text{hom}} = \\exp(-D_0 A)$, decays exponentially with area, making very large dies appear economically unviable. In contrast, the Negative Binomial yield, $Y_{\\text{clust}} = (1 + D_0 A/\\alpha)^{-\\alpha}$, decays as a power law for large $A$. This slower rate of decay makes larger dies (e.g., for high-performance computing, GPUs, and FPGAs) more feasible from a cost perspective. The analysis shows that the ratio $Y_{\\text{clust}}/Y_{\\text{hom}}$ grows with $A$, meaning the economic advantage of accounting for clustering becomes more pronounced for larger and more complex chips. This understanding alters the economic trade-offs involved in chip design, potentially justifying larger die sizes than would be considered under a simple Poisson yield assumption.",
            "answer": "$$\n\\boxed{2.44}\n$$"
        },
        {
            "introduction": "Raw electrical measurements from a wafer contain a mix of systematic, spatially-dependent trends and purely random local variations. To accurately predict parametric yield, which depends on circuit parameters falling within a specification window, these two components must be carefully separated. This exercise guides you through a standard data analysis workflow: using polynomial regression via Ordinary Least Squares (OLS) to model the systematic radial trend and then analyzing the residuals to characterize the random component as a Gaussian distribution. This hands-on coding practice will equip you with a fundamental technique for process characterization, allowing you to build location-aware parametric yield models directly from wafer test data. ",
            "id": "4281302",
            "problem": "You are given sets of across-wafer measurements of a single scalar electrical parameter collected at multiple die locations. The dominant systematic variation is assumed to be radially symmetric and smoothly varying with the normalized radius. The task is to decompose the measurements into a systematic radial component and a random component suitable for parametric yield prediction.\n\nStarting from the following fundamental base:\n- Ordinary Least Squares (OLS) minimizes the sum of squared residuals to estimate the parameters of a linear model.\n- For independent and identically distributed Gaussian noise with zero mean and variance $\\sigma^{2}$, the unbiased residual variance estimator divides the residual sum of squares by the degrees of freedom.\n- Under a Gaussian model, parametric yield within a two-sided specification window is computed by integrating the Gaussian probability density function between the lower and upper limits.\n\nFor each test case, you are given:\n- A list of normalized radii $\\{r_i\\}_{i=1}^N$, with each $r_i \\in [0, 1]$.\n- A corresponding list of measured scalar values $\\{x_i\\}_{i=1}^N$.\n- A nonnegative integer polynomial degree $d$ specifying the degree of a radial polynomial model for the systematic component.\n- A two-sided specification interval $[L, U]$ with $L < U$, in the same dimensionless units as the measurements.\n\nYour program must, for each test case:\n1. Fit the polynomial model $m(r) = \\sum_{k=0}^{d} a_k r^k$ to the data $\\{(r_i, x_i)\\}$ by Ordinary Least Squares.\n2. Compute the residuals $e_i = x_i - m(r_i)$ and estimate the random standard deviation $\\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^{N} e_i^2}{N - (d + 1)}}$ using the unbiased estimator, assuming $N > d + 1$.\n3. For each die location $i$, compute the probability that a new draw $X_i$ from the random component added to the fitted mean $m(r_i)$ lies within the specification window, assuming $X_i \\sim \\mathcal{N}(m(r_i), \\hat{\\sigma}^2)$. This die-level acceptance probability is the integral of the Gaussian density between $L$ and $U$. If $\\hat{\\sigma} = 0$, define the die-level acceptance probability to be $1$ if $L \\le m(r_i) \\le U$ and $0$ otherwise.\n4. Report the wafer-level predicted yield as the arithmetic mean of the die-level acceptance probabilities across the provided locations, expressed as a decimal in $[0, 1]$, rounded to exactly $6$ decimal places.\n\nAll quantities are dimensionless. Angles are not used. Percentages must be expressed as decimals.\n\nTest Suite:\n- Case $1$ (general quadratic trend, moderate noise):\n  - Radii $[r_{i}]$: [$0.00$, $0.20$, $0.35$, $0.50$, $0.65$, $0.80$, $0.90$, $1.00$, $0.10$, $0.30$, $0.55$, $0.85$]\n  - Measurements $[x_{i}]$: [$0.0100$, $-0.0290$, $-0.0636$, $-0.0475$, $-0.0746$, $-0.0600$, $-0.0565$, $-0.0510$, $-0.0125$, $-0.0505$, $-0.0526$, $-0.0686$]\n  - Polynomial degree $d = 2$\n  - Specification $[L, U] = [-0.08, 0.05]$\n- Case $2$ (no trend model, small random variation):\n  - Radii $[r_{i}]$: [$0.00$, $0.20$, $0.40$, $0.60$, $0.80$, $1.00$, $0.30$, $0.70$]\n  - Measurements $[x_{i}]$: [$0.0050$, $-0.0030$, $0.0020$, $-0.0100$, $0.0060$, $-0.0040$, $0.0010$, $-0.0090$]\n  - Polynomial degree $d = 0$\n  - Specification $[L, U] = [-0.02, 0.02]$\n- Case $3$ (near-linear trend, near-zero random variation):\n  - Radii $[r_{i}]$: [$0.00$, $0.25$, $0.50$, $0.75$, $1.00$]\n  - Measurements $[x_{i}]$: [$0.20010$, $0.17480$, $0.15000$, $0.12515$, $0.09995$]\n  - Polynomial degree $d = 1$\n  - Specification $[L, U] = [0.09, 0.21]$\n- Case $4$ (quadratic trend with few points, small noise):\n  - Radii $[r_{i}]$: [$0.00$, $0.50$, $0.80$, $1.00$]\n  - Measurements $[x_{i}]$: [$-0.0440$, $-0.0380$, $-0.0268$, $-0.0320$]\n  - Polynomial degree $d = 2$\n  - Specification $[L, U] = [-0.06, -0.02]$\n\nYour program should produce a single line of output containing the four wafer-level predicted yields, in the order of the cases above, as a comma-separated list enclosed in square brackets, with each yield rounded to exactly six decimal places (for example, $[0.987650,0.934210,0.500000,0.123456]$).",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. It is therefore deemed valid. The solution proceeds by implementing the specified statistical modeling and yield prediction workflow.\n\nThe core of the problem is to decompose a set of across-wafer measurements $\\{x_i\\}_{i=1}^{N}$ at given normalized radial locations $\\{r_i\\}_{i=1}^{N}$ into a systematic radial component $m(r)$ and a random component $e$.\n\n**1. Systematic Component Modeling and Parameter Estimation**\n\nThe systematic radial variation is modeled by a polynomial of degree $d$, as specified for each case:\n$$\nm(r) = \\sum_{k=0}^{d} a_{k} r^{k} = a_0 + a_1 r + a_2 r^2 + \\dots + a_d r^d\n$$\nThe unknown coefficients $\\mathbf{a} = [a_0, a_1, \\dots, a_d]^T$ are determined by fitting this model to the measurement data $\\{(r_i, x_i)\\}_{i=1}^{N}$ using the method of Ordinary Least Squares (OLS). The OLS procedure minimizes the sum of squared differences between the measured values $x_i$ and the model's predictions $m(r_i)$.\n\nThis can be formulated as a linear algebra problem. Let $\\mathbf{x}$ be the $N \\times 1$ vector of measurements, $\\mathbf{a}$ be the $(d+1) \\times 1$ vector of coefficients, and $\\mathbf{R}$ be the $N \\times (d+1)$ design matrix, often called a Vandermonde matrix in this context:\n$$\n\\mathbf{x} = \n\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_N\n\\end{pmatrix},\n\\quad\n\\mathbf{R} = \n\\begin{pmatrix}\n1 & r_1 & r_1^2 & \\dots & r_1^d \\\\\n1 & r_2 & r_2^2 & \\dots & r_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & r_N & r_N^2 & \\dots & r_N^d\n\\end{pmatrix},\n\\quad\n\\mathbf{a} = \n\\begin{pmatrix}\na_0 \\\\\na_1 \\\\\n\\vdots \\\\\na_d\n\\end{pmatrix}\n$$\nThe OLS problem seeks to find the coefficient vector $\\hat{\\mathbf{a}}$ that minimizes the squared Euclidean norm of the residual vector $\\mathbf{e} = \\mathbf{x} - \\mathbf{R}\\mathbf{a}$:\n$$\n\\hat{\\mathbf{a}} = \\arg\\min_{\\mathbf{a}} \\|\\mathbf{x} - \\mathbf{R}\\mathbf{a}\\|^2\n$$\nThe solution is given by the normal equations: $(\\mathbf{R}^T \\mathbf{R})\\hat{\\mathbf{a}} = \\mathbf{R}^T \\mathbf{x}$. Assuming the matrix $\\mathbf{R}^T \\mathbf{R}$ is invertible, the solution is $\\hat{\\mathbf{a}} = (\\mathbf{R}^T \\mathbf{R})^{-1} \\mathbf{R}^T \\mathbf{x}$. For numerical stability, especially with higher-degree polynomials, the solution is best computed using methods like QR decomposition or Singular Value Decomposition (SVD), which are employed by standard numerical linear algebra libraries.\n\n**2. Random Component Characterization**\n\nOnce the coefficients $\\hat{\\mathbf{a}}$ are found, we compute the fitted systematic component at each location, $m(r_i) = \\sum_{k=0}^{d} \\hat{a}_k r_i^k$. The residuals are then calculated as:\n$$\ne_i = x_i - m(r_i)\n$$\nThe random component is assumed to be a Gaussian random variable with zero mean and a constant variance $\\sigma^2$. We estimate this variance from the residuals. The problem specifies using the unbiased estimator for the variance, which accounts for the degrees of freedom consumed by fitting the $p = d+1$ parameters of the polynomial model:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{N - (d+1)} \\sum_{i=1}^{N} e_i^2\n$$\nThe estimated standard deviation is simply $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$. This estimator is valid provided the number of data points $N$ is greater than the number of fitted parameters, i.e., $N > d+1$, a condition met by all test cases.\n\n**3. Parametric Yield Prediction**\n\nWith the model fully specified, the value of the electrical parameter at a given location $i$ is modeled as a random variable $X_i$ drawn from a Gaussian distribution:\n$$\nX_i \\sim \\mathcal{N}(m(r_i), \\hat{\\sigma}^2)\n$$\nThe parametric yield for a die at location $i$ is the probability that $X_i$ falls within the two-sided specification interval $[L, U]$. This probability, $P_i$, is calculated by integrating the probability density function (PDF) of $X_i$ from $L$ to $U$. This is most conveniently computed using the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z) = P(Z \\le z)$ where $Z \\sim \\mathcal{N}(0, 1)$:\n$$\nP_i = P(L \\le X_i \\le U) = \\Phi\\left(\\frac{U - m(r_i)}{\\hat{\\sigma}}\\right) - \\Phi\\left(\\frac{L - m(r_i)}{\\hat{\\sigma}}\\right)\n$$\nIn the special case where the random variation is zero ($\\hat{\\sigma} = 0$), the distribution becomes a Dirac delta function centered at $m(r_i)$. The probability $P_i$ is then deterministic: it is $1$ if the fitted value $m(r_i)$ falls within the specification window, $[L, U]$, and $0$ otherwise.\n$$\nP_i = \n\\begin{cases}\n1 & \\text{if } L \\le m(r_i) \\le U \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\quad (\\text{for } \\hat{\\sigma} = 0)\n$$\n\n**4. Wafer-Level Yield Calculation**\n\nFinally, the overall predicted wafer-level yield, $Y$, is the arithmetic mean of the individual die-level acceptance probabilities over the $N$ sampled locations:\n$$\nY = \\frac{1}{N} \\sum_{i=1}^{N} P_i\n$$\nThe final result for each test case is this value $Y$, rounded to exactly $6$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the parametric yield prediction problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"radii\": [0.00, 0.20, 0.35, 0.50, 0.65, 0.80, 0.90, 1.00, 0.10, 0.30, 0.55, 0.85],\n            \"measurements\": [0.0100, -0.0290, -0.0636, -0.0475, -0.0746, -0.0600, -0.0565, -0.0510, -0.0125, -0.0505, -0.0526, -0.0686],\n            \"degree\": 2,\n            \"spec\": [-0.08, 0.05]\n        },\n        # Case 2\n        {\n            \"radii\": [0.00, 0.20, 0.40, 0.60, 0.80, 1.00, 0.30, 0.70],\n            \"measurements\": [0.0050, -0.0030, 0.0020, -0.0100, 0.0060, -0.0040, 0.0010, -0.0090],\n            \"degree\": 0,\n            \"spec\": [-0.02, 0.02]\n        },\n        # Case 3\n        {\n            \"radii\": [0.00, 0.25, 0.50, 0.75, 1.00],\n            \"measurements\": [0.20010, 0.17480, 0.15000, 0.12515, 0.09995],\n            \"degree\": 1,\n            \"spec\": [0.09, 0.21]\n        },\n        # Case 4\n        {\n            \"radii\": [0.00, 0.50, 0.80, 1.00],\n            \"measurements\": [-0.0440, -0.0380, -0.0268, -0.0320],\n            \"degree\": 2,\n            \"spec\": [-0.06, -0.02]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        r = np.array(case[\"radii\"])\n        x = np.array(case[\"measurements\"])\n        d = case[\"degree\"]\n        L, U = case[\"spec\"]\n        N = len(r)\n\n        # Step 1: Fit the polynomial model using Ordinary Least Squares.\n        # The design matrix R (Vandermonde matrix) has columns for r^0, r^1, ..., r^d.\n        R = np.vander(r, d + 1, increasing=True)\n        \n        # Use np.linalg.lstsq for a numerically stable OLS solution.\n        # It returns coefficients, sum of squared residuals (RSS), rank, and singular values.\n        coeffs, rss, _, _ = np.linalg.lstsq(R, x, rcond=None)\n        \n        # The fitted systematic component m(r_i) at each location.\n        m_r = R @ coeffs\n\n        # Step 2: Estimate the random standard deviation.\n        # Degrees of freedom for the residuals.\n        dof = N - (d + 1)\n        if dof <= 0:\n            # This case is precluded by the problem statement (N > d + 1).\n            # If it happened, variance would be undefined or infinite.\n            # Handle defensively.\n            sigma_hat = 0.0 \n        else:\n            # Unbiased variance estimator.\n            variance_hat = rss[0] / dof\n            sigma_hat = np.sqrt(variance_hat)\n\n        # Step 3: Compute die-level acceptance probabilities.\n        # This is the probability that a new draw from N(m(r_i), sigma_hat^2) is in [L, U].\n        if sigma_hat < 1e-12:  # Handle the case of zero or near-zero standard deviation.\n            # Probability is 1 if the mean is in spec, 0 otherwise.\n            # The inequalities handle the L <= m(r_i) <= U condition.\n            p_accept = (m_r >= L) & (m_r <= U)\n            p_accept = p_accept.astype(float)\n        else:\n            # Use the CDF of the standard normal distribution.\n            z_upper = (U - m_r) / sigma_hat\n            z_lower = (L - m_r) / sigma_hat\n            p_accept = norm.cdf(z_upper) - norm.cdf(z_lower)\n\n        # Step 4: Compute wafer-level predicted yield.\n        # The yield is the arithmetic mean of the die-level probabilities.\n        wafer_yield = np.mean(p_accept)\n        \n        # Format the result to exactly 6 decimal places.\n        results.append(f\"{wafer_yield:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Yield models are not merely predictive tools; they are essential for making informed, quantitative design decisions. A primary strategy in design-for-manufacturability (DFM) is the addition of redundancy to modules that are susceptible to failure, but this comes at the cost of increased chip area. This practice challenges you to design and implement a greedy algorithm that uses a yield model to intelligently insert redundancy where it is most effective. By deriving and implementing a marginal-yield-gain-per-unit-area decision rule, you will bridge the gap between abstract yield theory and practical electronic design automation (EDA), solving a core optimization problem in modern chip design. ",
            "id": "4281256",
            "problem": "Consider a chip-level manufacturing yield planning problem in the domain of Electronic Design Automation (EDA). A chip consists of $n$ functionally independent modules. Each module $i$ occupies area $a_i$ expressed in $\\mathrm{cm}^2$. The defect density is uniform and equal to $D_0$ defects per $\\mathrm{cm}^2$, and defects are assumed to cause catastrophic failure of a module if at least one defect lands in its area. Assume independence of defect events across disjoint areas.\n\nFundamental basis to use:\n- Under the classical Poisson defect model, the yield of a single module of area $a_i$ is $Y_i = \\exp(-D_0 a_i)$.\n- Under the standard series composition rule from probability theory for independent events, the chip operates correctly if each required function operates correctly. If a function is implemented with $k_i$ parallel redundant identical modules (each of area $a_i$ and independent in the defect sense), the function operates correctly if at least one of the $k_i$ modules operates correctly.\n\nYou may assume independence of failure events across module copies and across different modules. Redundancy is inserted by duplicating modules so that function $i$ is implemented by $k_i \\ge 1$ identical copies. The physical area consumption of function $i$ scales as $k_i a_i$, and the total added area due to redundancy is $\\sum_{i=1}^n (k_i - 1) a_i$, expressed in $\\mathrm{cm}^2$.\n\nTask:\n1. Starting from the fundamental basis above, derive the expression for the success probability of a function implemented with $k_i$ redundant copies and the overall chip yield as a function of the vector $k = (k_1, \\dots, k_n)$.\n2. Derive, from first principles using probability and the Poisson model, the marginal gain in overall chip yield when increasing $k_i$ by one, and then derive the marginal yield gain per unit area decision rule for selecting the next module to duplicate. The decision rule must be expressed as a quantity to be maximized at each step of a greedy heuristic that inserts one duplicate at a time.\n3. Design and implement a greedy redundancy insertion heuristic that:\n   - Initializes with no redundancy, i.e., $k_i = 1$ for all $i$.\n   - At each step, selects the module index $i$ that maximizes the derived marginal yield gain per unit area decision rule, increases $k_i$ by one, and repeats until the chip yield meets or exceeds a given target yield $Y_\\mathrm{target}$ (expressed as a decimal, not a percentage).\n   - If multiple modules have equal decision rule values, break ties by selecting the module with smaller $a_i$, and if still tied, by the smallest index $i$.\n   - Outputs, for each test case, the final redundancy vector $[k_1, \\dots, k_n]$ as integers, the total added area in $\\mathrm{cm}^2$ as a float, and the achieved chip yield as a decimal float.\n\nAngle units are not relevant in this problem. Physical units are strictly required for area: all area values must be handled in $\\mathrm{cm}^2$. All yields must be decimals in the unitless range $(0,1)$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,\\dots]$), where each $resultj$ is a list of the form $[k_1,\\dots,k_n,\\text{added\\_area\\_cm2},\\text{achieved\\_yield}]$.\n\nTest suite:\nProvide results for the following parameter sets $(D_0, [a_1,\\dots,a_n], Y_\\mathrm{target})$:\n- Case A: $D_0 = 0.5$, $[a_1,a_2,a_3] = [0.2, 0.05, 0.1]$, $Y_\\mathrm{target} = 0.85$.\n- Case B: $D_0 = 0.1$, $[a_1,a_2,a_3] = [0.3, 0.3, 0.4]$, $Y_\\mathrm{target} = 0.90$.\n- Case C: $D_0 = 2.0$, $[a_1,a_2,a_3,a_4,a_5] = [0.02, 0.02, 0.02, 0.02, 0.02]$, $Y_\\mathrm{target} = 0.95$.\n- Case D: $D_0 = 0.8$, $[a_1,a_2,a_3] = [0.5, 0.01, 0.01]$, $Y_\\mathrm{target} = 0.75$.\n- Case E: $D_0 = 0.3$, $[a_1,a_2] = [0.1, 0.1]$, $Y_\\mathrm{target} = 0.995$.\n\nYour program must compute the redundancy vector and outputs in the specified format for these cases, and no other input should be read.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically sound, well-posed, objective, and contains all necessary information to derive a unique solution based on the provided methodology. The problem is grounded in the established Poisson yield model and fundamental probability theory as applied to system reliability in integrated circuit manufacturing.\n\nHerein, a step-by-step derivation and algorithmic design are presented, as requested.\n\n### Task 1: Derivation of Yield Expressions\n\nThe objective is to derive the success probability for a function with redundancy and the overall chip yield.\n\nLet $D_0$ be the defect density and $a_i$ be the area of module $i$.\nThe yield of a single, non-redundant copy of module $i$, denoted as $Y_{\\text{copy},i}$, is given by the Poisson model:\n$$Y_{\\text{copy},i} = \\exp(-D_0 a_i)$$\nThe probability of failure for a single copy is the complement of its yield:\n$$F_{\\text{copy},i} = 1 - Y_{\\text{copy},i} = 1 - \\exp(-D_0 a_i)$$\nA function $i$ is implemented with $k_i$ identical, parallel, and independent copies. The function operates correctly if at least one of these $k_i$ copies is defect-free. It is more direct to first calculate the probability that the function fails. The function fails only if all $k_i$ copies fail. Due to the independence of failure events among the copies, the probability of function $i$ failing, $F_{\\text{func},i}$, is the product of the individual copy failure probabilities:\n$$F_{\\text{func},i}(k_i) = (F_{\\text{copy},i})^{k_i} = (1 - \\exp(-D_0 a_i))^{k_i}$$\nThe success probability, or yield, of function $i$, denoted as $Y_{\\text{func},i}(k_i)$, is the complement of its failure probability:\n$$Y_{\\text{func},i}(k_i) = 1 - F_{\\text{func},i}(k_i) = 1 - (1 - \\exp(-D_0 a_i))^{k_i}$$\nThe chip consists of $n$ functionally independent modules. The chip operates correctly only if all $n$ functions operate correctly. This represents a series system of functions. The total chip yield, $Y_{\\text{chip}}$, is the product of the yields of all individual functions. Given a redundancy configuration vector $k = (k_1, k_2, \\dots, k_n)$, the chip yield is:\n$$Y_{\\text{chip}}(k) = \\prod_{i=1}^n Y_{\\text{func},i}(k_i)$$\nSubstituting the expression for $Y_{\\text{func},i}(k_i)$:\n$$Y_{\\text{chip}}(k) = \\prod_{i=1}^n \\left[1 - (1 - \\exp(-D_0 a_i))^{k_i}\\right]$$\nThis completes the derivation for the first task.\n\n### Task 2: Derivation of the Marginal Gain Decision Rule\n\nThe second task is to derive the marginal gain in chip yield from adding one redundant copy to a module $j$ and to formulate a decision rule for a greedy heuristic based on yield gain per unit area.\n\nLet the current redundancy vector be $k = (k_1, \\dots, k_j, \\dots, k_n)$. The current chip yield is $Y_{\\text{chip}}(k)$. If we increase the redundancy of module $j$ from $k_j$ to $k_j+1$, the new vector is $k' = (k_1, \\dots, k_j+1, \\dots, k_n)$. The marginal gain in chip yield, $\\Delta Y_{\\text{chip}, j}$, is the difference $Y_{\\text{chip}}(k') - Y_{\\text{chip}}(k)$.\n\nThe chip yield can be written as the product of the yield of function $j$ and the yield of all other functions:\n$$Y_{\\text{chip}}(k) = Y_{\\text{func},j}(k_j) \\cdot \\prod_{i \\ne j} Y_{\\text{func},i}(k_i)$$\nThe new yield is:\n$$Y_{\\text{chip}}(k') = Y_{\\text{func},j}(k_j+1) \\cdot \\prod_{i \\ne j} Y_{\\text{func},i}(k_i)$$\nThe marginal gain is therefore:\n$$\\Delta Y_{\\text{chip}, j} = \\left[ Y_{\\text{func},j}(k_j+1) - Y_{\\text{func},j}(k_j) \\right] \\cdot \\prod_{i \\ne j} Y_{\\text{func},i}(k_i)$$\nLet's analyze the term in the square brackets representing the change in yield for function $j$:\n$$\\Delta Y_{\\text{func},j} = Y_{\\text{func},j}(k_j+1) - Y_{\\text{func},j}(k_j)$$\n$$= \\left[1 - (1 - e^{-D_0 a_j})^{k_j+1}\\right] - \\left[1 - (1 - e^{-D_0 a_j})^{k_j}\\right]$$\n$$= (1 - e^{-D_0 a_j})^{k_j} - (1 - e^{-D_0 a_j})^{k_j+1}$$\nFactoring out the common term $(1 - e^{-D_0 a_j})^{k_j}$:\n$$= (1 - e^{-D_0 a_j})^{k_j} \\left[1 - (1 - e^{-D_0 a_j})\\right]$$\n$$= (1 - e^{-D_0 a_j})^{k_j} \\cdot e^{-D_0 a_j}$$\nThe product term $\\prod_{i \\ne j} Y_{\\text{func},i}(k_i)$ can be expressed as $\\frac{Y_{\\text{chip}}(k)}{Y_{\\text{func},j}(k_j)}$. Substituting these components back into the expression for $\\Delta Y_{\\text{chip}, j}$:\n$$\\Delta Y_{\\text{chip}, j} = \\left[ (1 - e^{-D_0 a_j})^{k_j} e^{-D_0 a_j} \\right] \\cdot \\frac{Y_{\\text{chip}}(k)}{Y_{\\text{func},j}(k_j)}$$\n$$\\Delta Y_{\\text{chip}, j} = Y_{\\text{chip}}(k) \\cdot \\frac{e^{-D_0 a_j} (1 - e^{-D_0 a_j})^{k_j}}{1 - (1 - e^{-D_0 a_j})^{k_j}}$$\nThis is the marginal gain in overall chip yield.\n\nThe cost of this action is the area of the added module copy, which is $a_j$. The greedy decision rule should select the module that provides the maximum marginal yield gain per unit of added area. This metric, which we will call $M_j$, is:\n$$M_j = \\frac{\\Delta Y_{\\text{chip}, j}}{a_j} = \\frac{Y_{\\text{chip}}(k)}{a_j} \\cdot \\frac{e^{-D_0 a_j} (1 - e^{-D_0 a_j})^{k_j}}{1 - (1 - e^{-D_0 a_j})^{k_j}}$$\nAt any given step of the greedy algorithm, the current chip yield $Y_{\\text{chip}}(k)$ is a positive constant factor for all modules $j$. Since we are only interested in finding the index $j$ that maximizes $M_j$, we can simplify the decision by maximizing a metric $D_j$ where this common factor is removed:\n$$D_j = \\frac{1}{a_j} \\cdot \\frac{e^{-D_0 a_j} (1 - e^{-D_0 a_j})^{k_j}}{1 - (1 - e^{-D_0 a_j})^{k_j}}$$\nAt each step, the greedy heuristic will choose to increment $k_j$ for the module $j$ that has the largest value of $D_j$.\n\n### Task 3: Greedy Redundancy Insertion Heuristic Design\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: Given a defect density $D_0$, a vector of module areas $a = [a_1, \\dots, a_n]$, and a target yield $Y_{\\text{target}}$.\n    -   Initialize the redundancy vector $k$ to $[1, 1, \\dots, 1]$, representing the baseline design with no redundancy.\n    -   Calculate the initial chip yield $Y_{\\text{chip}}$ using the formula derived in Task 1 with the initial $k$.\n\n2.  **Greedy Iteration**: Enter a loop that continues as long as the current $Y_{\\text{chip}}$ is less than $Y_{\\text{target}}$.\n    a.  **Evaluate Candidates**: For each module $j \\in \\{1, \\dots, n\\}$, calculate the decision metric $D_j$ using the formula from Task 2 with the current redundancy level $k_j$.\n    b.  **Select Best Module**: Identify the module index $j^*$ that maximizes the decision metric $D_j$. The selection must adhere to the specified tie-breaking rules:\n        i.  If multiple modules share the same maximum $D_j$ value, select the one with the smallest area $a_j$.\n        ii. If a tie persists (i.e., same $D_j$ and same $a_j$), select the one with the smallest index $j$.\n    c.  **Update State**: Increment the redundancy count for the selected module: $k_{j^*} \\leftarrow k_{j^*} + 1$.\n    d.  **Recalculate Yield**: Update the total chip yield $Y_{\\text{chip}}$ a using the new redundancy vector $k$.\n\n3.  **Termination**: The loop terminates when $Y_{\\text{chip}} \\ge Y_{\\text{target}}$. Since adding redundancy always increases yield, termination is guaranteed for any $Y_{\\text{target}} < 1$.\n    -   Upon termination, calculate the total added area: $\\text{Added Area} = \\sum_{i=1}^n (k_i - 1) a_i$.\n    -   The final outputs are the final redundancy vector $k$, the total added area, and the achieved chip yield $Y_{\\text{chip}}$.\n\nThis algorithm systematically improves the chip yield by making locally optimal choices at each step, aiming to reach the target yield with an efficient use of area.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run the greedy redundancy insertion heuristic\n    for all specified test cases.\n    \"\"\"\n    # Test suite: (D0, [a1, ..., an], Y_target)\n    test_cases = [\n        # Case A\n        (0.5, [0.2, 0.05, 0.1], 0.85),\n        # Case B\n        (0.1, [0.3, 0.3, 0.4], 0.90),\n        # Case C\n        (2.0, [0.02, 0.02, 0.02, 0.02, 0.02], 0.95),\n        # Case D\n        (0.8, [0.5, 0.01, 0.01], 0.75),\n        # Case E\n        (0.3, [0.1, 0.1], 0.995),\n    ]\n\n    results = []\n    for D0, a_list, Y_target in test_cases:\n        areas = np.array(a_list, dtype=float)\n        n_modules = len(areas)\n        k = np.ones(n_modules, dtype=int)\n        \n        # Pre-calculate constants for each module to speed up iterations\n        # Y_copy_i = exp(-D0 * a_i)\n        # F_copy_i = 1 - Y_copy_i\n        Y_copy = np.exp(-D0 * areas)\n        F_copy = 1.0 - Y_copy\n\n        def calculate_chip_yield(current_k):\n            \"\"\"Calculates total chip yield for a given redundancy vector k.\"\"\"\n            # Y_func_i = 1 - (F_copy_i)^k_i\n            Y_func = 1.0 - np.power(F_copy, current_k)\n            # Y_chip = product of all Y_func_i\n            return np.prod(Y_func)\n\n        # Initial state\n        current_yield = calculate_chip_yield(k)\n\n        # Greedy iteration loop\n        while current_yield < Y_target:\n            decision_metrics = []\n            \n            for i in range(n_modules):\n                # Decision metric D_j = (1/a_j) * (Y_copy_j * F_copy_j^k_j) / (1 - F_copy_j^k_j)\n                # Y_func_j = 1 - F_copy_j^k_j\n                f_copy_i_k = np.power(F_copy[i], k[i])\n                y_func_i = 1.0 - f_copy_i_k\n                \n                # Handle potential numerical instability if y_func_i is near 0.\n                if y_func_i < 1e-12: # This would mean chip yield is already extremely low\n                    # In this regime, even a small improvement is huge relatively.\n                    # This metric would be very large, prioritizing this module.\n                    # The formula remains valid.\n                    pass\n\n                numerator = Y_copy[i] * f_copy_i_k\n                denominator = areas[i] * y_func_i\n                \n                metric = numerator / denominator\n                \n                # Store metric with tie-breaking info: (-metric, area, index)\n                # Sorting will maximize metric, then minimize area, then minimize index.\n                decision_metrics.append((-metric, areas[i], i))\n\n            # Sort to find the best candidate according to the rules\n            decision_metrics.sort()\n            \n            # The best module is the first one in the sorted list\n            best_module_index = decision_metrics[0][2]\n            \n            # Update redundancy for the best module\n            k[best_module_index] += 1\n            \n            # Recalculate chip yield\n            current_yield = calculate_chip_yield(k)\n\n        # Calculate final results\n        initial_areas = np.array(a_list, dtype=float)\n        added_area = float(np.sum((k - 1) * initial_areas))\n        achieved_yield = float(current_yield)\n\n        # Format result for the current case: [k1, ..., kn, added_area, yield]\n        final_k_list = k.tolist()\n        result_case = final_k_list + [added_area, achieved_yield]\n        results.append(str(result_case))\n    \n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}