{
    "hands_on_practices": [
        {
            "introduction": "The fundamental property of any storage element is its ability to reliably hold a state—a '0' or a '1'—despite the presence of noise. This practice delves into the heart of latch design, exploring how the physical dimensions of transistors determine the static noise margin (SNM), a key metric for robustness. By deriving the optimal sizing ratio for the pull-up and pull-down networks using a modern device model, you will gain a first-principles understanding of the link between device physics and circuit-level reliability, a cornerstone of memory and logic design.",
            "id": "4296461",
            "problem": "Consider a bistable latch formed by two identical Complementary Metal-Oxide-Semiconductor (CMOS) inverters cross-coupled, powered by a direct-current supply of magnitude $V_{DD}$. Each inverter consists of a single-channel $n$-type Metal-Oxide-Semiconductor (NMOS) pull-down and $p$-type Metal-Oxide-Semiconductor (PMOS) pull-up device characterized in the strong-inversion region by the widely used $\\alpha$-power law, which captures velocity saturation and short-channel effects. Specifically, the drain current of the NMOS and PMOS when they are in saturation at a gate-source overdrive is given by\n$$\ni_{n} = k_{n} \\left(v - V_{Tn}\\right)^{\\alpha}, \\quad v \\geq V_{Tn},\n$$\n$$\ni_{p} = k_{p} \\left(V_{DD} - v - |V_{Tp}|\\right)^{\\alpha}, \\quad V_{DD} - v \\geq |V_{Tp}|,\n$$\nwhere $k_{n} > 0$ and $k_{p} > 0$ are technology- and size-dependent conduction coefficients proportional to $\\mu C_{\\text{ox}} W/L$ for the respective devices, $V_{Tn} > 0$ and $V_{Tp}  0$ are threshold voltages, and $\\alpha \\in (1,2]$ is the velocity-saturation exponent. Assume quasi-static direct-current operation and that, near the inverter’s switching point, both devices are in saturation so that the above expressions are valid.\n\nDefine the static noise margin (SNM) as the side length of the largest axis-aligned square that can be inscribed between the direct-current voltage transfer characteristic $v_{o} = g(v_{i})$ of one inverter and the reflection of the other inverter’s characteristic through the line $v_{o} = v_{i}$ (the so-called butterfly plot). Starting from the definitions of fixed points and their stability for the cross-coupled composition $v = g(g(v))$, and from the $\\alpha$-power law currents and direct-current equilibrium condition $i_{n} = i_{p}$ at the inverter’s switching point, derive the sizing condition on the ratio of pull-up to pull-down strengths that maximizes the SNM while preserving bistability. Your derivation must explicitly justify why the symmetry of the two inverter characteristics around the point $(V_{DD}/2, V_{DD}/2)$ is preferred for maximizing SNM, and why the resulting latch remains bistable under that condition.\n\nExpress your final answer as a single closed-form analytic expression for the optimal ratio $k_{n}/k_{p}$ in terms of $V_{DD}$, $V_{Tn}$, $|V_{Tp}|$, and $\\alpha$. No numerical evaluation is required.",
            "solution": "The cross-coupled CMOS latch implements positive feedback between two identical inverters. Let the direct-current voltage transfer characteristic (VTC) of a single inverter be denoted $v_{o} = g(v_{i})$, where $g$ is a strictly decreasing, continuous function mapping $[0,V_{DD}]$ to $[V_{DD},0]$. The fixed points of the cross-coupled system satisfy $v = g(g(v))$. Stability at a fixed point $v^{\\star}$ is determined by the magnitude of the derivative of the composed mapping:\n$$\n\\left|\\frac{d}{dv} \\left[g\\!\\left(g(v)\\right)\\right]\\bigg|_{v=v^{\\star}}\\right| = \\left|g'\\!\\left(g(v^{\\star})\\right)\\right| \\cdot \\left|g'(v^{\\star})\\right|.\n$$\nFor identical inverters, $g(v^{\\star}) = v^{\\star}$ at a symmetric fixed point and thus the stability condition simplifies to $\\left|g'(v^{\\star})\\right|^{2}  1$ for stability and $\\left|g'(v^{\\star})\\right|^{2}  1$ for instability. The latch exhibits bistability when there exist two stable fixed points near the rails and one unstable fixed point near the center; the unstable central fixed point ensures the existence of two distinct basins of attraction.\n\nThe static noise margin (SNM) is geometrically defined on the butterfly plot formed by the VTC $v_{o}=g(v_{i})$ and its reflection $v_{i}=g(v_{o})$. The largest axis-aligned square that can be inscribed between these two curves has its sides parallel to the axes, and its side length is constrained by the minimum separation between the two curves measured along either axis. For two identical inverters, the butterfly plot is symmetric with respect to the line $v_{o}=v_{i}$ and the two lobes correspond to the low and high stable states. Let $V_{M}$ denote the inverter’s switching point, defined as the solution to $g(V_{M}) = V_{M}$, equivalently the point where input equals output.\n\nA robust way to see why symmetry about $(V_{DD}/2,V_{DD}/2)$ maximizes SNM is the following minimax argument. Consider the distances from $V_{M}$ to the supply rails along the input axis: the low-side headroom $d_{L} = V_{M}$ and high-side headroom $d_{H} = V_{DD} - V_{M}$. For an axis-aligned inscribed square, the limiting side length is determined by the smaller of the two lobes, which in turn is limited by the smaller headroom between the switching point and the respective rail after accounting for the local gain. In the first-order approximation near $V_{M}$, model the VTC locally as a straight line with slope $-m$, $m = -g'(V_{M})  0$, crossing the point $(V_{M},V_{M})$:\n$$\nv_{o} \\approx V_{M} - m\\,(v_{i} - V_{M}).\n$$\nThe reflected characteristic is $v_{i} \\approx V_{M} - m\\,(v_{o} - V_{M})$. Under this linear approximation, the limiting separation that sets the square side is proportional to the smaller of $d_{L}$ and $d_{H}$, scaled by a function of $m$ that is monotone increasing in $m$ for $m  1$ (the regime that yields an unstable central fixed point, since $\\left|g'(V_{M})\\right|^{2} = m^{2}  1$). Therefore, for fixed $m$ the SNM is maximized when $d_{L} = d_{H}$, i.e., when $V_{M} = V_{DD}/2$, because this balances the two lobes and increases the minimum separation in the minimax sense. This symmetry condition about $(V_{DD}/2,V_{DD}/2)$ ensures that the butterfly plot is mirror-symmetric with equal lobe sizes, which maximizes the largest inscribed square. This is the geometric reason symmetry is preferred. From the stability viewpoint, choosing device sizes to center $V_{M}$ does not, by itself, reduce $m$; typical inverter designs yield $m  1$ near the switching point. Thus bistability is preserved because the central fixed point remains unstable ($m^{2}  1$), while the two near-rail fixed points have composite slope magnitude less than $1$.\n\nWe now determine the sizing condition that places $V_{M}$ at $V_{DD}/2$. At the inverter switching point, the NMOS and PMOS carry equal current under direct-current equilibrium:\n$$\ni_{n}(V_{M}) = i_{p}(V_{M}).\n$$\nUnder the $\\alpha$-power law saturation model, this becomes\n$$\nk_{n} \\left(V_{M} - V_{Tn}\\right)^{\\alpha} = k_{p} \\left(V_{DD} - V_{M} - |V_{Tp}|\\right)^{\\alpha}.\n$$\nSolving for the ratio of strengths yields\n$$\n\\frac{k_{n}}{k_{p}} = \\left(\\frac{V_{DD} - V_{M} - |V_{Tp}|}{V_{M} - V_{Tn}}\\right)^{\\alpha}.\n$$\nImposing the symmetry condition $V_{M} = V_{DD}/2$ that maximizes the static noise margin gives the optimal ratio\n$$\n\\frac{k_{n}}{k_{p}} = \\left(\\frac{\\frac{V_{DD}}{2} - |V_{Tp}|}{\\frac{V_{DD}}{2} - V_{Tn}}\\right)^{\\alpha}.\n$$\n\nFinally, we comment on bistability under this condition. The local direct-current gain magnitude $m = -g'(V_{M})$ depends on the transconductances of the devices and the output resistance at $V_{M}$; for typical short-channel CMOS inverters sized per the above condition, $m  1$ holds, so the central fixed point is unstable because $\\left|g'(V_{M})\\right|^{2} = m^{2}  1$, and the two near-rail fixed points are stable due to the much smaller local gain magnitudes there. Thus, the latch remains bistable while the SNM is maximized. Symmetry is preferred not only because it maximizes SNM in the minimax sense but also because it equalizes margins for the two logic states, reduces sensitivity to systematic mismatches, and simplifies robust sizing across process, voltage, and temperature variations.\n\nTherefore, the condition on device sizing that maximizes static noise margin while preserving bistability is to choose the pull-up to pull-down strength ratio so that the inverter switching point is centered, yielding the closed-form expression for the optimal ratio $k_{n}/k_{p}$ as above.",
            "answer": "$$\\boxed{\\left(\\frac{\\frac{V_{DD}}{2}-|V_{Tp}|}{\\frac{V_{DD}}{2}-V_{Tn}}\\right)^{\\alpha}}$$"
        },
        {
            "introduction": "Beyond static stability, a sequential element's correct operation depends on strict adherence to timing contracts with external signals like the clock and asynchronous resets. This exercise addresses the critical scenario of a reset signal's deassertion near an active clock edge, a common source of failure in complex digital systems. This practice will guide you through a complete timing analysis—accounting for component delays, clock uncertainty, and signal skew—to define a safe operational window, which is essential for ensuring the temporal integrity of state machines and data paths in high-performance integrated circuits.",
            "id": "4296439",
            "problem": "Consider a positive-edge-triggered D flip-flop (DFF) implemented as a Complementary Metal-Oxide Semiconductor (CMOS) master-slave configuration. The master latch is transparent when the clock is low and the slave latch is transparent when the clock is high. The DFF includes an asynchronous active-low reset that clamps the slave latch’s internal storage node to logic low when asserted. When the reset is deasserted (released), the clamp is disengaged and the slave latch’s feedback is re-enabled after a finite time.\n\nAssume the following device-level timing characteristics measured at the slave latch:\n- Setup time of the DFF with respect to the rising clock edge: $t_{su} = 32\\ \\text{ps}$.\n- Hold time of the DFF with respect to the rising clock edge: $t_{h} = 16\\ \\text{ps}$.\n- Effective propagation delay from reset deassertion at the pad to removal of the slave clamp: $t_{pd}^{R\\rightarrow \\text{clamp}} = 10\\ \\text{ps}$.\n- Feedback re-establishment time in the slave latch (time from clamp removal until loop gain exceeds unity and the latch is strongly regenerative): $t_{\\text{settle}}^{\\text{fb}} = 20\\ \\text{ps}$.\n- Worst-case clock arrival time uncertainty (integrated jitter and local skew at the slave latch): $\\sigma_{\\text{clk}} = 6\\ \\text{ps}$.\n- Worst-case reset-to-clock spatial skew at the slave latch (reset leads clock): $t_{\\text{skew}}^{R,C} = 8\\ \\text{ps}$.\n\nLet the rising clock edge arrival at the slave latch be the reference time $t=0$. Define the DFF’s sampling aperture at the data input as the interval $[-t_{su},\\ t_{h}]$ and assume clock uncertainty expands this interval to $[-(t_{su}+\\sigma_{\\text{clk}}),\\ t_{h}+\\sigma_{\\text{clk}}]$. The reset deassertion causes a disturbance in the slave latch from the effective arrival of the reset edge at the latch input until the slave’s feedback is fully re-established, producing a hazard interval whose duration equals the sum of the clamp removal delay and the feedback re-establishment time.\n\nUsing only these definitions, derive the minimum safe lead and lag times, measured at the pad relative to the rising clock edge, such that reset release does not produce partial transparency in the slave latch nor interact with setup or hold constraints. The safe lead time is the minimum magnitude by which reset must be released before $t=0$, and the safe lag time is the minimum magnitude by which reset must be released after $t=0$, to ensure no overlap between the reset-induced hazard interval and the expanded sampling aperture.\n\nExpress your final answer as two real numbers in picoseconds, in the order “safe lead time” and “safe lag time.” No rounding is required.",
            "solution": "The problem requires the calculation of minimum safe lead and lag times for the deassertion of an asynchronous reset signal relative to a clock edge in a D flip-flop, such that the reset recovery process does not interfere with the data sampling window.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions:\n- DFF setup time: $t_{su} = 32\\ \\text{ps}$.\n- DFF hold time: $t_{h} = 16\\ \\text{ps}$.\n- Propagation delay from reset deassertion at pad to clamp removal: $t_{pd}^{R\\rightarrow \\text{clamp}} = 10\\ \\text{ps}$.\n- Feedback re-establishment time in slave latch: $t_{\\text{settle}}^{\\text{fb}} = 20\\ \\text{ps}$.\n- Worst-case clock arrival time uncertainty: $\\sigma_{\\text{clk}} = 6\\ \\text{ps}$.\n- Worst-case reset-to-clock spatial skew (reset leads clock): $t_{\\text{skew}}^{R,C} = 8\\ \\text{ps}$.\n- The reference time $t=0$ is the rising clock edge arrival at the slave latch.\n- The DFF's sampling aperture is defined as $[-t_{su},\\ t_{h}]$.\n- The expanded sampling aperture is defined as $[-(t_{su}+\\sigma_{\\text{clk}}),\\ t_{h}+\\sigma_{\\text{clk}}]$.\n- The reset-induced hazard interval starts at the effective arrival of the reset edge at the latch and its duration is the sum of the clamp removal delay and the feedback re-establishment time.\n- The lead and lag times are measured at the pad relative to the rising clock edge at the pad.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, concerning a standard and critical timing analysis scenario in digital integrated circuit design known as Reset Recovery and Removal (RRR) timing. The parameters and their values are realistic for modern CMOS technologies. The problem is well-posed, as it provides a clear objective and a set of formal definitions and numerical values to reach a unique solution. The language is objective and technical.\n\nThere is a minor ambiguity in the problem statement. The parameter $t_{pd}^{R\\rightarrow \\text{clamp}}$ is defined as the delay \"from reset deassertion at the pad\", while the hazard interval duration is described as the \"sum of the clamp removal delay and the feedback re-establishment time\". This suggests $t_{pd}^{R\\rightarrow \\text{clamp}}$ is a component of the hazard *duration*, which is a local property of the latch recovery process. The most consistent interpretation, which preserves the problem's solvability, is to treat $t_{pd}^{R\\rightarrow \\text{clamp}}$ as the \"clamp removal delay\" component of the hazard duration, and that this duration starts upon the reset signal's arrival at the latch. The explicit reference to \"at the pad\" in its definition is thus interpreted as a slight imprecision in wording, a common occurrence, rather than a fundamental contradiction that would render the problem invalid. With this clarification, the problem is deemed complete and consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid under the specified interpretation. A detailed solution follows.\n\n### Derivation of the Solution\n\nThe core of the problem is to ensure two time intervals do not overlap. The first interval is the DFF's data sampling aperture, expanded by clock uncertainty. The second is the hazard interval during which the slave latch is unstable due to reset deassertion. All calculations will be performed relative to the clock edge arrival at the slave latch, which is our reference time $t=0$. The final answer will be converted back to the pad reference frame.\n\nLet $T_R$ be the time of reset deassertion at the pad, measured relative to the rising clock edge at the pad ($T_{C,pad}=0$). The problem asks for the minimum $|T_R|$ for $T_R  0$ (lead time) and the minimum $T_R$ for $T_R  0$ (lag time).\n\nThe arrival time of the reset deassertion signal at the latch, $t_{R,latch}$, relative to the clock arrival at the latch ($t=0$), is affected by the skew. Since the reset path leads the clock path by $t_{\\text{skew}}^{R,C}$, the reset signal arrives earlier. Therefore, the relationship between the pad timing $T_R$ and the latch timing $t_{R,latch}$ is:\n$$t_{R,latch} = T_R - t_{\\text{skew}}^{R,C}$$\n\n**1. Define the Expanded Sampling Aperture ($I_{S/H}$)**\n\nThis is the time window around the clock edge at the latch ($t=0$) during which the data input must be stable. The problem defines this interval, including clock uncertainty, as:\n$$I_{S/H} = [-(t_{su} + \\sigma_{\\text{clk}}), \\ t_{h} + \\sigma_{\\text{clk}}]$$\nSubstituting the given values:\n$$t_{start,S/H} = -(32\\ \\text{ps} + 6\\ \\text{ps}) = -38\\ \\text{ps}$$\n$$t_{end,S/H} = (16\\ \\text{ps} + 6\\ \\text{ps}) = 22\\ \\text{ps}$$\nSo, the expanded sampling aperture is $I_{S/H} = [-38\\ \\text{ps}, 22\\ \\text{ps}]$.\n\n**2. Define the Reset-Induced Hazard Interval ($I_{HAZARD}$)**\n\nThis interval begins at the arrival of the reset deassertion at the latch, $t_{R,latch}$, and has a duration equal to the sum of the clamp removal delay and the feedback settling time.\nLet the duration be $T_{HAZARD}$:\n$$T_{HAZARD} = t_{pd}^{R\\rightarrow \\text{clamp}} + t_{\\text{settle}}^{\\text{fb}} = 10\\ \\text{ps} + 20\\ \\text{ps} = 30\\ \\text{ps}$$\nThe start and end times of the hazard interval are:\n$$t_{start,HAZARD} = t_{R,latch} = T_R - t_{\\text{skew}}^{R,C} = T_R - 8\\ \\text{ps}$$\n$$t_{end,HAZARD} = t_{start,HAZARD} + T_{HAZARD} = (T_R - 8\\ \\text{ps}) + 30\\ \\text{ps} = T_R + 22\\ \\text{ps}$$\nSo, the hazard interval is $I_{HAZARD} = [T_R - 8\\ \\text{ps}, T_R + 22\\ \\text{ps}]$.\n\n**3. Calculate the Safe Lead Time**\n\nThe safe lead time corresponds to deasserting the reset *before* the clock edge. In this case, the hazard interval must end before the sampling aperture begins to ensure no overlap.\nThe condition is:\n$$t_{end,HAZARD} \\le t_{start,S/H}$$\nSubstituting the expressions for the interval boundaries:\n$$T_R + 22\\ \\text{ps} \\le -38\\ \\text{ps}$$\n$$T_R \\le -38\\ \\text{ps} - 22\\ \\text{ps}$$\n$$T_R \\le -60\\ \\text{ps}$$\nThis means the reset must be deasserted at the pad no later than $60\\ \\text{ps}$ before the clock edge at the pad. The minimum safe lead time is the magnitude of this value.\n$$\\text{Safe Lead Time} = |-60\\ \\text{ps}| = 60\\ \\text{ps}$$\n\n**4. Calculate the Safe Lag Time**\n\nThe safe lag time corresponds to deasserting the reset *after* the clock edge. In this case, the hazard interval must begin after the sampling aperture has ended.\nThe condition is:\n$$t_{start,HAZARD} \\ge t_{end,S/H}$$\nSubstituting the expressions for the interval boundaries:\n$$T_R - 8\\ \\text{ps} \\ge 22\\ \\text{ps}$$\n$$T_R \\ge 22\\ \\text{ps} + 8\\ \\text{ps}$$\n$$T_R \\ge 30\\ \\text{ps}$$\nThis means the reset must be deasserted at the pad no earlier than $30\\ \\text{ps}$ after the clock edge at the pad. The minimum safe lag time is this value.\n$$\\text{Safe Lag Time} = 30\\ \\text{ps}$$\n\nThe required values are a safe lead time of $60\\ \\text{ps}$ and a safe lag time of $30\\ \\text{ps}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n60  30\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "When data must cross between asynchronous clock domains, timing violations are inevitable, leading to a probabilistic phenomenon known as metastability. This practice confronts this challenge head-on by analyzing the most common mitigation structure: the two-flip-flop synchronizer. You will derive the Mean Time Between Failures (MTBF) from the ground up, connecting device-level properties and signal frequencies to the system's overall reliability, providing a quantitative framework for designing robust synchronizers and appreciating the exponential relationship between timing margin and system stability.",
            "id": "4296488",
            "problem": "In the context of Electronic Design Automation (EDA) for integrated circuits, consider a two-flip-flop synchronizer used to capture an asynchronous data signal into a synchronous clock domain. A synchronous storage element (e.g., a flip-flop) exhibits metastable behavior when its input violates the setup or hold constraints relative to the clock edge. The metastable resolution time $t$ of such an element is modeled as an exponentially distributed random variable with survival function $P\\{T_{\\text{resolve}}  t\\} = \\exp(-t/\\tau)$, where $\\tau$ is the metastability time constant. A capturing attempt that can cause metastability occurs when a data transition falls into the aperture window of the first flip-flop; the expected number of such near-coincident events per unit time is proportional to the product of the data transition frequency and the clock frequency, scaled by a device-dependent window parameter $T_0$ with units of time.\n\nAssume the following parameters for a specific technology and synchronizer implementation: $\\tau = 20$ ps, $T_0 = 50$ ps, $f_{\\text{data}} = 100$ MHz, $f_{\\text{clk}} = 500$ MHz, and an available metastability resolution time $T_{\\text{res}} = 200$ ps between the sampling edge of the first stage and the sampling edge of the second stage of the synchronizer. Using the exponential resolution model and the definition of event rate based on independent asynchronous crossings, derive from first principles the analytical expression for the Mean Time Between Failures (MTBF) of the synchronizer in terms of $\\tau$, $T_0$, $f_{\\text{data}}$, $f_{\\text{clk}}$, and $T_{\\text{res}}$. Then compute the MTBF using the given values and express the result in hours.\n\nRound your final numerical answer to $4$ significant figures and express the MTBF in hours. Provide, in addition to the computation, a brief assessment of whether the computed MTBF is adequate for a high-reliability system where the mission duration is significantly larger than typical operational spans (e.g., many years), based on order-of-magnitude reasoning.",
            "solution": "The problem asks for the derivation of the analytical expression for the Mean Time Between Failures (MTBF) of a two-flip-flop synchronizer, a numerical calculation of its value given specific parameters, and an assessment of its reliability.\n\nFirst, we validate the problem statement.\n**Step 1: Extracted Givens**\n- Synchronizer structure: two-flip-flop synchronizer.\n- Metastable resolution time survival function: $P\\{T_{\\text{resolve}}  t\\} = \\exp(-t/\\tau)$.\n- Metastability time constant: $\\tau = 20$ ps.\n- Aperture window parameter: $T_0 = 50$ ps.\n- Asynchronous data transition frequency: $f_{\\text{data}} = 100$ MHz.\n- Synchronous clock frequency: $f_{\\text{clk}} = 500$ MHz.\n- Available metastability resolution time: $T_{\\text{res}} = 200$ ps.\n- The expected number of near-coincident events per unit time is proportional to the product $f_{\\text{data}} f_{\\text{clk}} T_0$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically grounded, using standard models for metastability analysis in digital circuits (e.g., the exponential decay model for resolution and the event rate formulation). The concepts are fundamental to digital integrated circuit design. The problem is well-posed, providing all necessary parameters to derive an expression and compute a value. The language is objective and precise. The parameter values, while leading to a poor design outcome, are not physically impossible or contradictory, but rather serve to illustrate the sensitivity of MTBF to said parameters. The problem is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\nThe MTBF is the reciprocal of the failure rate, $R_{\\text{failure}}$.\n$$\n\\text{MTBF} = \\frac{1}{R_{\\text{failure}}}\n$$\nThe failure rate is the product of the rate at which metastability-inducing events occur, $R_{\\text{event}}$, and the probability that such an event results in a synchronizer failure, $P_{\\text{failure}}$.\n$$\nR_{\\text{failure}} = R_{\\text{event}} \\cdot P_{\\text{failure}}\n$$\n\nFirst, we derive the expression for $R_{\\text{event}}$. A metastability-inducing event occurs when the asynchronous data signal changes its value within the critical timing window (aperture window) of the first flip-flop. This window has a duration $T_0$. The problem states that the rate of these events is proportional to $f_{\\text{data}} f_{\\text{clk}} T_0$. The standard model, which we adopt here, sets the proportionality constant to $1$. This model arises from considering that data transitions occur at a rate $f_{\\text{data}}$ and the fraction of time the circuit is vulnerable around each clock edge is $T_0/T_{clk} = T_0 f_{clk}$. However, a more direct physical argument is that data transitions occur at a rate of $f_{\\text{data}}$, and each transition has a probability $T_0 f_{\\text{clk}}$ of landing in a vulnerable window. Thus, the rate of coincidence is $R_{\\text{event}} = f_{\\text{data}} f_{\\text{clk}} T_0$. The units are $[s^{-1}] \\cdot [s^{-1}] \\cdot [s] = [s^{-1}]$, which is a rate, as expected.\n$$\nR_{\\text{event}} = f_{\\text{data}} f_{\\text{clk}} T_0\n$$\n\nSecond, we determine the probability of failure, $P_{\\text{failure}}$. When a metastable event occurs at the first flip-flop, a failure of the two-stage synchronizer happens if the output of this first flip-flop has not settled to a stable logic state ($0$ or $1$) by the time it is sampled by the second flip-flop. The time available for resolution is given as $T_{\\text{res}}$. The probability that the resolution time, $T_{\\text{resolve}}$, exceeds this available time is given by the survival function evaluated at $t = T_{\\text{res}}$.\n$$\nP_{\\text{failure}} = P\\{T_{\\text{resolve}}  T_{\\text{res}}\\} = \\exp\\left(-\\frac{T_{\\text{res}}}{\\tau}\\right)\n$$\n\nCombining these results, the failure rate is:\n$$\nR_{\\text{failure}} = (f_{\\text{data}} f_{\\text{clk}} T_0) \\exp\\left(-\\frac{T_{\\text{res}}}{\\tau}\\right)\n$$\n\nThe analytical expression for the MTBF is the reciprocal of this rate:\n$$\n\\text{MTBF} = \\frac{1}{f_{\\text{data}} f_{\\text{clk}} T_0 \\exp(-T_{\\text{res}}/\\tau)} = \\frac{\\exp(T_{\\text{res}}/\\tau)}{f_{\\text{data}} f_{\\text{clk}} T_0}\n$$\nThis is the required analytical expression.\n\nNext, we compute the numerical value of the MTBF using the given parameters:\n- $\\tau = 20 \\text{ ps} = 20 \\times 10^{-12} \\text{ s}$\n- $T_0 = 50 \\text{ ps} = 50 \\times 10^{-12} \\text{ s}$\n- $f_{\\text{data}} = 100 \\text{ MHz} = 100 \\times 10^{6} \\text{ s}^{-1} = 1 \\times 10^{8} \\text{ s}^{-1}$\n- $f_{\\text{clk}} = 500 \\text{ MHz} = 500 \\times 10^{6} \\text{ s}^{-1} = 5 \\times 10^{8} \\text{ s}^{-1}$\n- $T_{\\text{res}} = 200 \\text{ ps} = 200 \\times 10^{-12} \\text{ s}$\n\nFirst, we calculate the dimensionless exponent in the exponential term:\n$$\n\\frac{T_{\\text{res}}}{\\tau} = \\frac{200 \\times 10^{-12} \\text{ s}}{20 \\times 10^{-12} \\text{ s}} = 10\n$$\n\nNext, we calculate the denominator term, which represents the rate of events entering the metastable window:\n$$\nf_{\\text{data}} f_{\\text{clk}} T_0 = (1 \\times 10^{8} \\text{ s}^{-1}) \\cdot (5 \\times 10^{8} \\text{ s}^{-1}) \\cdot (50 \\times 10^{-12} \\text{ s})\n$$\n$$\nf_{\\text{data}} f_{\\text{clk}} T_0 = 250 \\times 10^{8+8-12} \\text{ s}^{-1} = 250 \\times 10^{4} \\text{ s}^{-1} = 2.5 \\times 10^{6} \\text{ s}^{-1}\n$$\n\nNow, we can compute the MTBF in seconds:\n$$\n\\text{MTBF} = \\frac{\\exp(10)}{2.5 \\times 10^{6}} \\text{ s} \\approx \\frac{22026.4658}{2.5 \\times 10^{6}} \\text{ s} \\approx 8.810586 \\times 10^{-3} \\text{ s}\n$$\n\nThe problem requires the result in hours. There are $60 \\times 60 = 3600$ seconds in an hour.\n$$\n\\text{MTBF (hours)} = \\frac{8.810586 \\times 10^{-3} \\text{ s}}{3600 \\text{ s/h}} \\approx 2.447385 \\times 10^{-6} \\text{ h}\n$$\n\nRounding to $4$ significant figures, the MTBF is $2.447 \\times 10^{-6}$ hours.\n\nFinally, we provide a brief assessment of this result.\nAn MTBF of $2.447 \\times 10^{-6}$ hours corresponds to approximately $8.81$ milliseconds. This means the synchronizer is expected to fail on average more than $100$ times per second. For any practical application, this is a catastrophic failure rate. A high-reliability system, with a mission duration of many years, must have an extremely low probability of failure. This requires an MTBF that is many orders of magnitude larger than the mission duration, typically measured in millions or billions of years.\nThe computed MTBF is therefore completely inadequate. The primary cause for this poor performance is the small available resolution time, $T_{\\text{res}} = 200$ ps. The ratio $T_{\\text{res}}/\\tau = 10$ is insufficient to suppress the probability of unresolved metastability to an acceptable level. A robust synchronizer design would ensure $T_{\\text{res}}$ is a much larger multiple of $\\tau$. Typically, $T_{\\text{res}}$ is close to one full clock period (which is $1/(500 \\text{ MHz}) = 2000$ ps in this case), which would yield a ratio $T_{\\text{res}}/\\tau \\approx 100$ and a resulting MTBF that is astronomically large and acceptable for high-reliability applications.",
            "answer": "$$\\boxed{2.447 \\times 10^{-6}}$$"
        }
    ]
}