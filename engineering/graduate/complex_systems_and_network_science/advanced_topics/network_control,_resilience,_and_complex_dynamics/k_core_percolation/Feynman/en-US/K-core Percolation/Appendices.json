{
    "hands_on_practices": [
        {
            "introduction": "This exercise is foundational for understanding k-core percolation, focusing on the simplest non-trivial case: the emergence of the $2$-core in an Erdős–Rényi random graph. By deriving the self-consistency equations from first principles and analyzing the behavior near the critical point, you will develop a deep intuition for continuous phase transitions in networks. This practice illustrates the power of generating functions and asymptotic analysis in predicting the macroscopic properties of random structures .",
            "id": "4284889",
            "problem": "Consider the Erdős–Rényi random graph $G(n,p)$ with $p=c/n$ and mean degree $c>0$. The $2$-core of a graph is the maximal subgraph in which every vertex has degree at least $2$. In the sparse regime with fixed $c$ as $n \\to \\infty$, the degree distribution is asymptotically Poisson with mean $c$, and the graph is locally tree-like.\n\nDefine $S(c)$ to be the limiting fraction of vertices in the $2$-core. Using only the following foundational elements:\n- the definition of the $2$-core via iterative leaf-removal (repeatedly delete vertices of degree $1$ and their incident edges until no such vertices remain),\n- the locally tree-like property of $G(n,c/n)$,\n- the Poisson degree distribution and its probability generating function $G_{0}(x)=\\exp(c(x-1))$,\nderive from first principles a self-consistency description for the probability that a neighbor reached by a uniformly random edge remains in the $2$-core after leaf-removal, and from it an expression for $S(c)$.\n\nThen analyze the emergence of the $2$-core near the critical point $c=1$ by setting $c=1+\\epsilon$ with small $\\epsilon>0$ and performing a systematic small-$\\epsilon$ expansion of your expression for $S(c)$. Show that the transition is continuous and that\n$$\nS(c)\\sim B\\,(c-1)^{2}\n$$\nas $\\epsilon=c-1\\downarrow 0^{+}$, and determine the exact value of the coefficient $B$ from the Poisson-based equations you derived.\n\nYour final answer must be the exact value of $B$. No rounding is required.",
            "solution": "We begin from the definition of the $2$-core via iterative leaf-removal and exploit the locally tree-like property of $G(n,c/n)$ to set up a self-consistency framework. Let $p_{d}$ denote the probability that a vertex has degree $d$; in the Erdős–Rényi model $G(n,c/n)$ in the sparse limit, we have the Poisson distribution $p_{d}=\\exp(-c)\\,c^{d}/d!$. The probability generating function is $G_{0}(x)=\\sum_{d\\ge 0}p_{d}x^{d}=\\exp(c(x-1))$.\n\nIntroduce the quantity $y(c)$ as the probability that a vertex reached by following a uniformly random edge remains in the $2$-core after leaf-removal. Because we follow an edge, the degree of the reached vertex is distributed according to the size-biased distribution $\\frac{d\\,p_{d}}{c}$, and conditional on degree $d$ the number of its other neighbors is $d-1$. In the $2$-core, a vertex must have at least two surviving neighbors. When conditioning on having arrived along an edge at a neighbor, that neighbor can count the incoming edge among its surviving neighbors, so it requires at least one additional surviving neighbor among its $d-1$ others to remain in the $2$-core. Under the locally tree-like independence assumption, each of those $d-1$ neighbors independently remains with probability $y(c)$.\n\nTherefore, conditional on degree $d$, the probability the reached neighbor remains in the $2$-core is $1-(1-y)^{d-1}$, since $(1-y)^{d-1}$ is the probability that none of the $d-1$ other neighbors remains. Averaging over the size-biased degree distribution yields the self-consistency equation\n$$\ny \\;=\\; \\sum_{d\\ge 1}\\frac{d\\,p_{d}}{c}\\,\\bigl[\\,1-(1-y)^{d-1}\\,\\bigr]\n\\;=\\; 1-\\sum_{d\\ge 1}\\frac{d\\,p_{d}}{c}\\,(1-y)^{d-1}.\n$$\nFor the Poisson distribution, $\\frac{d\\,p_{d}}{c}=p_{d-1}$, so the sum simplifies:\n$$\ny \\;=\\; 1 - \\sum_{m\\ge 0}p_{m}(1-y)^{m} \\;=\\; 1 - G_{0}(1-y).\n$$\nUsing $G_{0}(x)=\\exp(c(x-1))$, we obtain the explicit fixed-point equation\n$$\ny \\;=\\; 1 - \\exp\\!\\bigl(c((1-y)-1)\\bigr) \\;=\\; 1 - \\exp(-c\\,y).\n$$\n\nNext, we express the fraction $S(c)$ of vertices in the $2$-core. A vertex of degree $d$ remains in the $2$-core if at least two of its $d$ neighbors remain. With the independence approximation, the number of surviving neighbors is ${\\rm Binomial}(d,y)$, so the survival probability is $1-\\bigl[(1-y)^{d}+d\\,y\\,(1-y)^{d-1}\\bigr]$, where the subtracted terms are the cases of $0$ or $1$ surviving neighbors. Averaging over $p_{d}$ yields\n$$\nS(c)\n\\;=\\; \\sum_{d\\ge 0}p_{d}\\Bigl[\\,1-(1-y)^{d}-d\\,y\\,(1-y)^{d-1}\\Bigr]\n\\;=\\; 1 - \\sum_{d\\ge 0}p_{d}(1-y)^{d} - y\\sum_{d\\ge 0}d\\,p_{d}(1-y)^{d-1}.\n$$\nRecognizing generating-function sums,\n$$\n\\sum_{d\\ge 0}p_{d}(1-y)^{d} \\;=\\; G_{0}(1-y) \\;=\\; \\exp(-c\\,y),\n$$\nand\n$$\n\\sum_{d\\ge 0}d\\,p_{d}(1-y)^{d-1}\n\\;=\\; G_{0}'(1-y) \\;=\\; c\\,\\exp\\!\\bigl(c((1-y)-1)\\bigr)\n\\;=\\; c\\,\\exp(-c\\,y),\n$$\nwe obtain\n$$\nS(c) \\;=\\; 1 - \\exp(-c\\,y) - y\\,c\\,\\exp(-c\\,y).\n$$\nUsing the fixed-point relation $y=1-\\exp(-c\\,y)$, we simplify $\\exp(-c\\,y)=1-y$ and thus\n$$\nS(c) \\;=\\; 1 - (1-y) - y\\,c\\,(1-y) \\;=\\; y\\bigl[1 - c + c\\,y\\bigr]\n\\;=\\; y\\bigl[c\\,y - (c-1)\\bigr].\n$$\n\nTo analyze the behavior near the critical point $c=1$, set $c=1+\\epsilon$ with $\\epsilon>0$ small. The fixed-point equation for $y$ becomes\n$$\ny \\;=\\; 1 - \\exp\\!\\bigl(-(1+\\epsilon)\\,y\\bigr).\n$$\nExpanding the exponential for small $y$ and $\\epsilon$,\n$$\n\\exp\\!\\bigl(-(1+\\epsilon)\\,y\\bigr)\n\\;=\\; 1 - (1+\\epsilon)\\,y + \\tfrac{1}{2}(1+\\epsilon)^{2}y^{2} - \\tfrac{1}{6}(1+\\epsilon)^{3}y^{3} + \\cdots,\n$$\nso\n$$\ny \\;=\\; (1+\\epsilon)\\,y - \\tfrac{1}{2}(1+\\epsilon)^{2}y^{2} + \\tfrac{1}{6}(1+\\epsilon)^{3}y^{3} - \\cdots.\n$$\nRearranging,\n$$\n-\\epsilon\\,y + \\tfrac{1}{2}(1+\\epsilon)^{2}y^{2} - \\tfrac{1}{6}(1+\\epsilon)^{3}y^{3} + \\cdots \\;=\\; 0.\n$$\nFor small $\\epsilon$, the nontrivial solution has $y=O(\\epsilon)$. Keeping the leading balance between the linear and quadratic terms gives\n$$\n\\epsilon\\,y \\;\\approx\\; \\tfrac{1}{2}(1+\\epsilon)^{2}y^{2},\n$$\nhence\n$$\ny \\;\\approx\\; \\frac{2\\,\\epsilon}{(1+\\epsilon)^{2}}\n\\;=\\; 2\\,\\epsilon - 4\\,\\epsilon^{2} + O(\\epsilon^{3}).\n$$\nSubstituting $c=1+\\epsilon$ and this expansion into $S(c)=y\\bigl[c\\,y-(c-1)\\bigr]$:\n$$\nS(c) \\;=\\; y\\Bigl((1+\\epsilon)\\,y - \\epsilon\\Bigr) \\;=\\; y\\bigl(y + \\epsilon\\,y - \\epsilon\\bigr).\n$$\nUsing $y=2\\epsilon + O(\\epsilon^2)$, we have $y + \\epsilon y - \\epsilon = (2\\epsilon+O(\\epsilon^2)) + \\epsilon(2\\epsilon+O(\\epsilon^2)) - \\epsilon = \\epsilon + O(\\epsilon^2)$. Substituting this into the expression for $S(c)$:\n$$\nS(c) \\;=\\; (2\\epsilon + O(\\epsilon^2))(\\epsilon + O(\\epsilon^2)) \\;=\\; 2\\epsilon^2 + O(\\epsilon^3).\n$$\nTherefore, as $\\epsilon=c-1\\downarrow 0^{+}$,\n$$\nS(c) \\sim B\\,(c-1)^{2}\n$$\nwith\n$$\nB = 2.\n$$\nThis confirms that the $2$-core emerges continuously at $c=1$ with quadratic scaling and identifies the coefficient from the Poisson self-consistency equations.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "While the degree distribution is a primary determinant of network properties, it does not tell the whole story. This practice delves into the crucial role of degree-degree correlations, exploring how the mixing patterns between high- and low-degree nodes affect core stability. By calculating the $3$-core size for two networks with identical degree distributions but opposite assortativity, you will quantify how network organization can either drastically reinforce or completely dismantle a network's core .",
            "id": "4284879",
            "problem": "Consider an undirected random network generated by the configuration model with degree distribution $P(q)=\\frac{1}{2}\\delta_{q,2}+\\frac{1}{2}\\delta_{q,4}$, so that one half of the nodes have degree $2$ and one half have degree $4$. Two ensembles are defined that share this degree distribution but differ in their degree-degree correlations through the conditional neighbor-degree distribution $P(q' \\mid q)$:\n- Assortative ensemble $\\mathrm{A}$: $P(2 \\mid 2)=1$, $P(4 \\mid 4)=1$, and $P(2 \\mid 4)=P(4 \\mid 2)=0$.\n- Disassortative ensemble $\\mathrm{D}$: $P(4 \\mid 2)=1$, $P(2 \\mid 2)=0$, $P(2 \\mid 4)=\\frac{1}{2}$, and $P(4 \\mid 4)=\\frac{1}{2}$.\n\nThese conditional probabilities satisfy the undirected consistency constraint $q P(q) P(q' \\mid q)=q' P(q') P(q \\mid q')$ for all $q,q' \\in \\{2,4\\}$.\n\nStarting only from the definition of the $k$-core as the maximal induced subgraph in which every node has degree at least $k$, and from the locally tree-like nature of the configuration model (so that messages arriving along different edges can be treated as independent events conditioned on node degrees), analyze the $3$-core ($k=3$) in each ensemble. Define a cavity message along an edge to be positive if the node at the receiving end would remain in the $3$-core when that edge is removed; let $y_q$ denote the probability that such a message sent from a node of degree $q$ is positive in the infinite-size limit, and let $t_q$ denote the probability that a node of degree $q$ belongs to the $3$-core.\n\nDerive the appropriate self-consistency relations for $y_q$ and use them to compute $t_q$ in each ensemble. Then compute the total fraction of nodes in the $3$-core, $S_{\\mathrm{A}}$ and $S_{\\mathrm{D}}$, for ensembles $\\mathrm{A}$ and $\\mathrm{D}$, respectively. Finally, report the reduction\n$$\\Delta S \\equiv S_{\\mathrm{A}}-S_{\\mathrm{D}}.$$\nExpress your final answer as an exact number. No rounding is required.",
            "solution": "We analyze the $3$-core ($k=3$) in two random network ensembles, $\\mathrm{A}$ and $\\mathrm{D}$, which share the same degree distribution $P(q)=\\frac{1}{2}\\delta_{q,2}+\\frac{1}{2}\\delta_{q,4}$ but differ in their degree-degree correlations. The analysis uses a cavity method suitable for locally tree-like networks.\n\nLet $y_q$ be the probability that a message sent from a node of degree $q$ is 'positive', indicating the sender would remain in the $3$-core in a 'cavity' graph (where the message edge is removed). A node of original degree $q$ has $q-1$ neighbors in the cavity graph. It sends a positive message if it receives at least $k=3$ positive messages from these $q-1$ neighbors.\nLet $Y_q$ be the probability that a message received by our degree-$q$ node is positive. This is given by $Y_q = \\sum_{q'} P(q'|q) y_{q'}$.\nThe self-consistency equation for $y_q$ is:\n$$y_q = \\sum_{m=k}^{q-1} \\binom{q-1}{m} (Y_q)^m (1-Y_q)^{q-1-m}$$\nFor $k=3$ and degrees $q \\in \\{2, 4\\}$:\n- For a node of degree $q=2$, it has $q-1=1$ neighbor in the cavity graph. It's impossible to receive $k=3$ positive messages. Thus, $y_2 = 0$ for both ensembles.\n- For a node of degree $q=4$, it has $q-1=3$ neighbors. It must receive $3$ positive messages from these $3$ neighbors, so $y_4 = \\binom{3}{3} (Y_4)^3 (1-Y_4)^{0} = (Y_4)^3$.\n\nLet $t_q$ be the probability that a randomly chosen node of degree $q$ is in the $3$-core. This node (in the full graph) must receive at least $k=3$ positive messages from its $q$ neighbors.\n$$t_q = \\sum_{m=k}^{q} \\binom{q}{m} (Y_q)^m (1-Y_q)^{q-m}$$\nFor $q=2$, $t_2=0$. For $q=4$, $t_4 = \\binom{4}{3} (Y_4)^3 (1-Y_4) + \\binom{4}{4} (Y_4)^4$.\nThe total fraction of nodes in the $3$-core is $S = \\sum_q P(q)t_q$.\n\n**Analysis of Ensemble A (Assortative)**\nThe conditional probabilities are $P(2|2)=1$, $P(4|4)=1$, and $P(2|4)=P(4|2)=0$.\nAs established, $y_2=0$.\nFor $y_4$, we first find $Y_4 = P(2|4)y_2 + P(4|4)y_4 = (0)(0) + (1)y_4 = y_4$.\nThe self-consistency equation is $y_4 = (y_4)^3$, which implies $y_4(y_4^2 - 1) = 0$. The solutions in $[0,1]$ are $y_4=0$ and $y_4=1$. We take the non-trivial solution, $y_4=1$, corresponding to a non-empty core.\nWith $y_2=0$ and $y_4=1$, we have $Y_4 = y_4=1$. We compute $t_4$:\n$$t_4 = \\binom{4}{3}(1)^3(0)^1 + \\binom{4}{4}(1)^4 = 1$$\nSo, $t_2=0$ and $t_4=1$. All degree-$4$ nodes are in the $3$-core, and no degree-$2$ nodes are.\nThe total fraction of nodes in the $3$-core for ensemble A is:\n$$S_{\\mathrm{A}} = P(2)t_2 + P(4)t_4 = \\frac{1}{2}(0) + \\frac{1}{2}(1) = \\frac{1}{2}$$\n\n**Analysis of Ensemble D (Disassortative)**\nThe conditional probabilities are $P(4|2)=1$, $P(2|2)=0$, $P(2|4)=\\frac{1}{2}$, and $P(4|4)=\\frac{1}{2}$.\nAgain, $y_2=0$.\nFor $y_4$, we find $Y_4 = P(2|4)y_2 + P(4|4)y_4 = \\left(\\frac{1}{2}\\right)(0) + \\left(\\frac{1}{2}\\right)y_4 = \\frac{1}{2}y_4$.\nThe self-consistency equation is:\n$$y_4 = \\left(\\frac{1}{2}y_4\\right)^3 = \\frac{1}{8}y_4^3 \\implies y_4\\left(1 - \\frac{y_4^2}{8}\\right) = 0$$\nThe solutions are $y_4=0$ and $y_4 = \\sqrt{8} = 2\\sqrt{2}$. Since $y_4$ is a probability, the only valid solution is $y_4=0$.\nThis implies that no non-trivial $3$-core exists. With $y_4=0$, we have $Y_4 = 0$, which gives $t_4=0$.\nThe total fraction of nodes in the $3$-core for ensemble D is:\n$$S_{\\mathrm{D}} = P(2)t_2 + P(4)t_4 = \\frac{1}{2}(0) + \\frac{1}{2}(0) = 0$$\n\n**Final Calculation**\nThe reduction is $\\Delta S = S_{\\mathrm{A}} - S_{\\mathrm{D}}$.\n$$\\Delta S = \\frac{1}{2} - 0 = \\frac{1}{2}$$\nThis result highlights the strong impact of degree correlations on network robustness. The assortative network maintains a large $3$-core composed of all its degree-$4$ nodes, while the disassortative network's $3$-core collapses completely.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Moving from theory to application, a key task in network science is to determine if an observed structural feature is statistically significant. This coding exercise guides you through the process of testing for $k$-core reinforcement in an empirical network using a null model approach. You will implement degree-preserving randomization to create an ensemble of random graphs, providing a baseline against which the observed core size can be rigorously compared, a fundamental skill for any data-driven network analysis .",
            "id": "4284882",
            "problem": "Consider a simple undirected network represented by a graph $G = (V, E)$ with $|V| = n$ nodes and $|E| = m$ edges. The degree sequence is the list of degrees $\\{d_1, d_2, \\dots, d_n\\}$, where $d_i$ is the degree of node $i$. The $k$-core of a graph is the maximal induced subgraph in which every node has degree at least $k$, obtained by recursively pruning nodes with degree strictly less than $k$ until no further pruning is possible. The size of the $k$-core, denoted $|V_k(G)|$, is the number of nodes remaining after this pruning process.\n\nYou are tasked with designing a program that performs empirical inference under a degree-preserving null model to test for structural reinforcement of $k$-cores. The null model must preserve the empirical degree sequence exactly and otherwise randomize the network by repeatedly applying degree-preserving double-edge swaps on simple graphs (no self-loops and no multiple edges). A double-edge swap selects two distinct edges $(a,b)$ and $(c,d)$ with four distinct endpoints $\\{a,b,c,d\\}$ and replaces them by either $(a,c)$ and $(b,d)$ or $(a,d)$ and $(b,c)$, provided that the resulting graph remains simple. The double-edge swap must be repeated a fixed number of successful times to induce sufficient randomization. For each randomized sample, compute the $k$-core size $|V_k(\\cdot)|$.\n\nThe statistical test for structural reinforcement should proceed as follows. Let $X_{\\text{obs}}$ denote the observed $k$-core size of the empirical graph $G$. Generate a null ensemble by performing $R$ independent randomizations, each starting from $G$ and applying $S$ successful double-edge swaps. Let $\\{X_1, X_2, \\dots, X_R\\}$ be the resulting $k$-core sizes from the randomized ensemble. Use these samples to compute a standardized test statistic that compares $X_{\\text{obs}}$ against the null expectation. Use a fixed threshold $z^{\\ast} = 2$ for decision-making. If the standardized test statistic indicates that the observed $k$-core size is substantially larger than the null expectation (exceeding the threshold), classify the case as structurally reinforced; otherwise, do not classify it as reinforced. In cases where the empirical variance of the null ensemble is zero, define the standardized test statistic to be $0$ and do not classify as reinforced.\n\nYour program must implement:\n- A procedure to compute the $k$-core size $|V_k(G)|$ using the recursive pruning definition.\n- A procedure to generate degree-preserving randomized samples of $G$ via double-edge swaps, ensuring the graph remains simple at all times.\n- A procedure to compute the null expectation and the standardized test statistic, and to make the reinforcement decision using the threshold $z^{\\ast} = 2$.\n\nUse the following test suite. Each test case specifies the graph by the node count $n$, the edge set $E$, the core parameter $k$, the number of randomizations $R$, the number of successful swaps per randomization $S$, and the random seed $s$ for reproducibility. Nodes are labeled by integers from $0$ to $n-1$. All numbers must be treated as integers.\n\nTest case $1$ (dense core with periphery):\n- $n = 12$\n- $E$ consists of all pairs among nodes $\\{0,1,2,3,4,5\\}$ forming a clique, plus two edges from each periphery node $\\{6,7,8,9,10,11\\}$ to the clique:\n  - Clique edges: $\\{(0,1),(0,2),(0,3),(0,4),(0,5),(1,2),(1,3),(1,4),(1,5),(2,3),(2,4),(2,5),(3,4),(3,5),(4,5)\\}$\n  - Periphery attachments: $(6,0),(6,1),(7,2),(7,3),(8,4),(8,5),(9,0),(9,2),(10,1),(10,4),(11,3),(11,5)$\n- $k = 5$\n- $R = 200$\n- $S = 270$\n- $s = 12345$\n\nTest case $2$ (near-regular random-like graph):\n- $n = 12$\n- $E = \\{(0,1),(0,2),(0,3),(0,4),(1,2),(1,5),(1,6),(2,3),(2,7),(3,4),(3,8),(4,5),(4,9),(5,6),(5,10),(6,7),(6,11),(7,8),(7,9),(8,9),(8,10),(9,10),(10,11),(11,0),(11,3)\\}$\n- $k = 4$\n- $R = 200$\n- $S = 250$\n- $s = 2021$\n\nTest case $3$ (star graph boundary case):\n- $n = 12$\n- $E = \\{(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7),(0,8),(0,9),(0,10),(0,11)\\}$\n- $k = 5$\n- $R = 100$\n- $S = 100$\n- $s = 7$\n\nYour program must produce, for each test case, a list of the form $[\\mu, X_{\\text{obs}}, Z, \\text{reinforced}]$ where:\n- $\\mu$ is the sample mean of $\\{X_1,\\dots,X_R\\}$ expressed as a floating-point number,\n- $X_{\\text{obs}}$ is the empirical $k$-core size expressed as an integer,\n- $Z$ is the standardized test statistic expressed as a floating-point number,\n- $\\text{reinforced}$ is a boolean indicating whether the standardized test statistic exceeds the threshold $z^{\\ast} = 2$.\n\nFinal output format: Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each item is itself a list in the order above. For example, the output should look like $[[\\mu_1,X_{\\text{obs},1},Z_1,\\text{reinforced}_1],[\\mu_2,X_{\\text{obs},2},Z_2,\\text{reinforced}_2],[\\mu_3,X_{\\text{obs},3},Z_3,\\text{reinforced}_3]]$. No physical units, angle units, or percentages are involved in this problem, so none should be provided.",
            "solution": "The solution implements the specified statistical test for $k$-core reinforcement. The overall approach consists of three main algorithmic components.\n\n1.  **$k$-Core Size Calculation**: A function calculates the size of the $k$-core for a given graph and core parameter $k$. It implements the iterative pruning algorithm. First, the degrees of all nodes are computed. Then, a queue is initialized with all nodes having a degree less than $k$. The algorithm processes this queue: for each node removed, the degrees of its neighbors are decremented. If a neighbor's degree falls below $k$, it is added to the queue. This process continues until the queue is empty. The final $k$-core size is the total number of nodes minus the count of pruned nodes.\n\n2.  **Degree-Preserving Randomization**: A function generates a randomized version of the input graph while preserving its degree sequence exactly. This is achieved through a series of double-edge swaps. The function repeatedly selects two distinct edges $(a, b)$ and $(c, d)$ at random, ensuring the four endpoints are unique. It then attempts to rewire them to $(a, c)$ and $(b, d)$ or $(a, d)$ and $(b, c)$, but only if the new edges do not already exist in the graph (to maintain simplicity). A swap is considered successful once the graph is rewired. This process is repeated for a specified number of successful swaps to ensure sufficient randomization.\n\n3.  **Statistical Inference Procedure**: The main procedure orchestrates the test. For each test case, it first computes the $k$-core size of the original, empirical graph, denoted $X_{\\text{obs}}$. It then generates a null ensemble by running the randomization procedure $R$ times, calculating the $k$-core size for each of the $R$ randomized graphs. The sample mean $(\\mu)$ and standard deviation $(\\sigma)$ of these null-model core sizes are computed. The standardized test statistic (Z-score) is then calculated as $Z = (X_{\\text{obs}} - \\mu) / \\sigma$. A special case handles a null distribution with zero variance by setting $Z=0$. Finally, the observed $k$-core is classified as \"structurally reinforced\" if its Z-score exceeds the predefined threshold of $z^*=2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef k_core_size(adj, k):\n    \"\"\"\n    Computes the size of the k-core of a graph using recursive pruning.\n    \n    Args:\n        adj (list of set): The adjacency list of the graph.\n        k (int): The core parameter.\n        \n    Returns:\n        int: The number of nodes in the k-core.\n    \"\"\"\n    n = len(adj)\n    degrees = np.array([len(neighbors) for neighbors in adj])\n    \n    # Queue for nodes to be pruned\n    prune_queue = [i for i, d in enumerate(degrees) if d  k]\n    head = 0\n    \n    is_pruned = np.zeros(n, dtype=bool)\n    num_pruned = 0\n    \n    while head  len(prune_queue):\n        u = prune_queue[head]\n        head += 1\n        \n        if is_pruned[u]:\n            continue\n            \n        is_pruned[u] = True\n        num_pruned += 1\n        \n        for v in adj[u]:\n            if not is_pruned[v]:\n                degrees[v] -= 1\n                if degrees[v]  k:\n                    prune_queue.append(v)\n                    \n    return n - num_pruned\n\ndef perform_randomization(adj, num_swaps, rng):\n    \"\"\"\n    Generates a degree-preserving randomized graph via double-edge swaps.\n    \n    Args:\n        adj (list of set): The adjacency list of the original graph.\n        num_swaps (int): The number of successful swaps to perform.\n        rng (np.random.Generator): The random number generator.\n        \n    Returns:\n        list of set: The adjacency list of the randomized graph.\n    \"\"\"\n    n = len(adj)\n    rand_adj = [s.copy() for s in adj]\n    \n    edges = []\n    for u in range(n):\n        for v in rand_adj[u]:\n            if u  v:\n                edges.append((u, v))\n                \n    num_edges = len(edges)\n    if num_edges  2:\n        return rand_adj\n\n    successful_swaps = 0\n    # Add a safety break for non-randomizable graphs\n    max_attempts = 100 * num_swaps if num_swaps > 0 else 1\n    attempts = 0\n\n    while successful_swaps  num_swaps and attempts  max_attempts:\n        attempts += 1\n        \n        if num_edges  2:\n            break\n\n        idx1, idx2 = rng.choice(num_edges, 2, replace=False)\n        e1, e2 = edges[idx1], edges[idx2]\n        a, b = e1\n        c, d = e2\n        \n        if len({a, b, c, d}) != 4:\n            continue\n            \n        # Rewiring option 1: (a,d) and (b,c)\n        valid1 = (d not in rand_adj[a]) and (c not in rand_adj[b])\n        # Rewiring option 2: (a,c) and (b,d)\n        valid2 = (c not in rand_adj[a]) and (d not in rand_adj[b])\n        \n        chosen_rewiring = None\n        if valid1 and valid2:\n            chosen_rewiring = 1 if rng.random()  0.5 else 2\n        elif valid1:\n            chosen_rewiring = 1\n        elif valid2:\n            chosen_rewiring = 2\n        \n        if chosen_rewiring:\n            # Remove old edges\n            rand_adj[a].remove(b); rand_adj[b].remove(a)\n            rand_adj[c].remove(d); rand_adj[d].remove(c)\n            \n            if chosen_rewiring == 1: # (a,d) and (b,c)\n                rand_adj[a].add(d); rand_adj[d].add(a)\n                rand_adj[b].add(c); rand_adj[c].add(b)\n                edges[idx1] = tuple(sorted((a,d)))\n                edges[idx2] = tuple(sorted((b,c)))\n            else: # chosen_rewiring == 2, (a,c) and (b,d)\n                rand_adj[a].add(c); rand_adj[c].add(a)\n                rand_adj[b].add(d); rand_adj[d].add(b)\n                edges[idx1] = tuple(sorted((a,c)))\n                edges[idx2] = tuple(sorted((b,d)))\n\n            successful_swaps += 1\n            \n    return rand_adj\n\ndef solve():\n    test_cases = [\n        {\n            \"n\": 12,\n            \"E\": [\n                (0,1),(0,2),(0,3),(0,4),(0,5),(1,2),(1,3),(1,4),(1,5),\n                (2,3),(2,4),(2,5),(3,4),(3,5),(4,5),\n                (6,0),(6,1),(7,2),(7,3),(8,4),(8,5),(9,0),(9,2),\n                (10,1),(10,4),(11,3),(11,5)\n            ],\n            \"k\": 5, \"R\": 200, \"S\": 270, \"s\": 12345\n        },\n        {\n            \"n\": 12,\n            \"E\": [\n                (0,1),(0,2),(0,3),(0,4),(1,2),(1,5),(1,6),(2,3),(2,7),\n                (3,4),(3,8),(4,5),(4,9),(5,6),(5,10),(6,7),(6,11),\n                (7,8),(7,9),(8,9),(8,10),(9,10),(10,11),(11,0),(11,3)\n            ],\n            \"k\": 4, \"R\": 200, \"S\": 250, \"s\": 2021\n        },\n        {\n            \"n\": 12,\n            \"E\": [(0,i) for i in range(1, 12)],\n            \"k\": 5, \"R\": 100, \"S\": 100, \"s\": 7\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n, E, k, R, S, s = case[\"n\"], case[\"E\"], case[\"k\"], case[\"R\"], case[\"S\"], case[\"s\"]\n        \n        rng = np.random.default_rng(s)\n        \n        adj_obs = [set() for _ in range(n)]\n        for u, v in E:\n            adj_obs[u].add(v)\n            adj_obs[v].add(u)\n            \n        X_obs = k_core_size(adj_obs, k)\n        \n        null_core_sizes = []\n        for _ in range(R):\n            adj_random = perform_randomization(adj_obs, S, rng)\n            X_rand = k_core_size(adj_random, k)\n            null_core_sizes.append(X_rand)\n\n        null_dist = np.array(null_core_sizes, dtype=float)\n        mu = np.mean(null_dist)\n        sigma = np.std(null_dist)\n        \n        if sigma == 0.0:\n            Z = 0.0\n        else:\n            Z = (X_obs - mu) / sigma\n            \n        z_star = 2.0\n        reinforced = Z > z_star\n        \n        all_results.append([float(mu), int(X_obs), float(Z), bool(reinforced)])\n\n    # Format the final output string precisely\n    result_strings = []\n    for res in all_results:\n        # Convert boolean to lowercase as per common data exchange formats\n        bool_str = str(res[3]).lower()\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{bool_str}]\"\n        result_strings.append(res_str)\n        \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}