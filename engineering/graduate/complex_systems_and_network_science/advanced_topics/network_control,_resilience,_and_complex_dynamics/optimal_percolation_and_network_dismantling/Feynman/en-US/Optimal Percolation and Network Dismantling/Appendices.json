{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation in network dismantling, we begin with simple, analyzable cases. The star graph provides a perfect example of a network with a clear structural vulnerability, where a single central hub maintains the entire network's connectivity. This exercise  challenges you to formalize this intuition, proving analytically that removing the center node is the most efficient strategy to shatter the network into isolated components.",
            "id": "4295177",
            "problem": "Consider the star graph $K_{1,m}$, defined as the graph with one central node $c$ connected to $m$ leaf nodes and no other edges. Let $G=(V,E)$ denote this graph, where $|V|=m+1$. For any node-removal set $R \\subseteq V$, define $S(G \\setminus R)$ to be the number of nodes in the largest connected component of the induced subgraph on $V \\setminus R$. In the context of optimal percolation for network dismantling, the goal is to minimize $S(G \\setminus R)$ by removing nodes.\n\nStarting from the fundamental definition of connected components in graphs and the effect of node removal on component structure, determine the minimal fraction $q^{\\star}$ of nodes that must be removed from $K_{1,m}$ to guarantee that $S(G \\setminus R) \\le 1$, and verify analytically that an optimal removal set achieving this bound includes the center node $c$. Express $q^{\\star}$ as a function of $m$ in exact form. Your final answer must be a single exact mathematical expression with no units. No rounding is required.",
            "solution": "The goal is to find the minimal fraction of nodes to remove from a star graph $K_{1,m}$ such that the largest remaining connected component has size at most 1, i.e., $S(G \\setminus R) \\le 1$. The graph has $N=m+1$ nodes: one central node $c$ and $m$ leaf nodes $l_1, \\dots, l_m$. A component of size at most 1 means all remaining nodes must be isolated.\n\n**Case 1: The central node $c$ is removed.**\nThe removal set is $R=\\{c\\}$. The number of removed nodes is $|R|=1$. The remaining graph $G \\setminus R$ consists of the $m$ leaf nodes, which are now all isolated since their only connection (to $c$) has been removed. The largest connected component is a single node, so its size is $S(G \\setminus R) = 1$. This satisfies the condition. The fraction of nodes removed is $q = \\frac{|R|}{N} = \\frac{1}{m+1}$.\n\n**Case 2: The central node $c$ is not removed.**\nTo isolate all nodes, we must sever all connections. Since node $c$ is connected to all $m$ leaf nodes, we must remove all nodes connected to $c$ to isolate it. This means we must remove all $m$ leaf nodes. The removal set would be $R=\\{l_1, \\dots, l_m\\}$. The number of removed nodes is $|R|=m$. The remaining graph consists of only the isolated node $c$, so $S(G \\setminus R)=1$. This strategy works, but requires removing $m$ nodes. The fraction is $\\frac{m}{m+1}$.\n\nIf we remove fewer than $m$ leaves (say, $k < m$ leaves), the central node $c$ remains connected to $m-k$ leaves. This forms a connected component of size $1 + (m-k)$. Since $m \\ge 1$ and $k < m$, we have $m-k \\ge 1$, so the component size is at least 2. This does not satisfy the condition $S \\le 1$.\n\n**Comparison:**\n- Strategy 1 (removing the center) requires removing 1 node.\n- Strategy 2 (keeping the center) requires removing $m$ nodes to satisfy the condition.\n\nSince $m \\ge 1$ for any non-trivial star graph, $1 \\le m$. Thus, removing the single central node is the optimal strategy. The minimal number of nodes to remove is 1.\n\nThe minimal fraction of nodes to be removed, $q^{\\star}$, is the fraction corresponding to this optimal strategy:\n$$\nq^{\\star} = \\frac{\\text{minimal number of removed nodes}}{\\text{total number of nodes}} = \\frac{1}{m+1}\n$$\nThis analytically verifies that the optimal strategy is to remove the center node, and the minimal fraction is $\\frac{1}{m+1}$.",
            "answer": "$$\\boxed{\\frac{1}{m+1}}$$"
        },
        {
            "introduction": "While intuition works for simple graphs, dismantling complex networks requires quantitative heuristics. The Collective Influence (CI) metric, derived from the stability analysis of the network, offers a principled way to identify nodes whose removal is most detrimental to connectivity. This hands-on practice  will guide you through the manual calculation of the CI score, solidifying your understanding of its components, such as excess degree ($k-1$) and the influence of nodes at a specific distance.",
            "id": "4295214",
            "problem": "Consider an undirected, simple graph $G$ with vertex set $V=\\{1,2,\\dots,10\\}$ and edge set\n$$\nE=\\{(1,2),(1,3),(1,4),(2,5),(2,6),(3,7),(4,8),(4,9),(5,10),(9,10)\\}.\n$$\nLet $k_{i}$ denote the degree of node $i$. In the optimal percolation framework under the locally tree-like approximation, the onset of the disappearance of the largest connected component can be analyzed through the spectral properties of the non-backtracking matrix (NB), and the node-removal problem can be cast into an influence-maximization objective over finite-radius neighborhoods. Starting from these foundations—percolation on sparse networks, the largest eigenvalue of the non-backtracking matrix as the stability criterion, and the restriction to finite-radius neighborhoods—derive a node-level influence measure at radius $\\ell=2$ and compute it for every node of the graph $G$.\n\nThen, based on the collective influence dismantling strategy that removes nodes in descending order of the derived radius-$2$ influence, and breaking ties by selecting the node with the smallest label first, produce the complete ordered list of node labels for the first ten selections (i.e., an ordering of all nodes according to their initial radius-$2$ influence). Express your final ordered list as a row matrix. No rounding is required.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   The graph is an undirected, simple graph $G$ with vertex set $V=\\{1,2,\\dots,10\\}$.\n-   The edge set is $E=\\{(1,2),(1,3),(1,4),(2,5),(2,6),(3,7),(4,8),(4,9),(5,10),(9,10)\\}$.\n-   $k_i$ denotes the degree of node $i$.\n-   The theoretical context is optimal percolation on sparse networks, using a locally tree-like approximation.\n-   The stability criterion for the largest connected component is the largest eigenvalue of the non-backtracking matrix (NB).\n-   The analysis is restricted to finite-radius neighborhoods.\n-   The task is to derive a node-level influence measure at radius $\\ell=2$.\n-   This measure must be computed for every node of the graph $G$.\n-   A collective influence dismantling strategy is to be followed, removing nodes in descending order of their initial radius-$2$ influence.\n-   The tie-breaking rule is to select the node with the smallest label first.\n-   The final output is the complete ordered list of all $10$ node labels.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on the \"Collective Influence\" (CI) algorithm, a well-established method in network science for identifying influential spreaders or nodes critical for network integrity. The concepts of optimal percolation, the non-backtracking matrix, and tree-like approximations are fundamental to modern network theory. The problem is scientifically sound.\n-   **Well-Posed**: The graph is explicitly defined. The task is unambiguous: calculate a specific metric ($CI_2$) for all nodes and then rank them according to clear rules. The tie-breaking rule ensures a unique solution.\n-   **Objective**: The problem is stated in precise, formal language without subjective or opinion-based elements.\n-   **Completeness and Consistency**: All information required to solve the problem (graph structure, radius, ranking rule, tie-breaking rule) is provided. There are no contradictions.\n-   **Feasibility**: The graph is small, making the manual calculation of degrees, distances, and influence scores feasible.\n-   **Structure**: The problem is well-structured and leads to a unique, verifiable answer.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, objective, and complete. I will now proceed with the solution.\n\nThe problem requires the derivation and application of a node-level influence measure based on the principles of optimal percolation. The Collective Influence (CI) of a node $i$ is derived from a perturbation analysis of the largest eigenvalue, $\\lambda_B$, of the non-backtracking matrix of the graph. Under the locally tree-like approximation, the stability of the giant component is dictated by $\\lambda_B$, with the percolation threshold occurring at $\\lambda_B=1$. The influence of a node is its contribution to $\\lambda_B$. Removing a node $i$ prunes all non-backtracking paths that pass through it. The CI score approximates this impact by considering paths of a specific length $\\ell$ originating from $i$.\n\nThe Collective Influence $\\mathrm{CI}_{\\ell}(i)$ of a node $i$ at radius $\\ell$ is defined as the product of the excess degree of node $i$ and the sum of the excess degrees of all nodes $j$ located at a distance $\\ell$ from $i$. The excess degree $k_i-1$ represents the number of forward-propagating paths from node $i$ in a tree-like structure. The sum over the boundary of the ball of radius $\\ell$ captures the aggregated branching potential at the influence frontier. The formula is:\n$$\n\\mathrm{CI}_{\\ell}(i) = (k_i - 1) \\sum_{j \\in \\partial B(i, \\ell)} (k_j - 1)\n$$\nwhere $\\partial B(i, \\ell)$ is the set of vertices at a shortest path distance of exactly $\\ell$ from vertex $i$. For this problem, we are required to use $\\ell=2$.\n\nFirst, we compute the degree $k_i$ for each node $i \\in V = \\{1, 2, \\dots, 10\\}$.\n-   $k_1 = |\\{2,3,4\\}| = 3$\n-   $k_2 = |\\{1,5,6\\}| = 3$\n-   $k_3 = |\\{1,7\\}| = 2$\n-   $k_4 = |\\{1,8,9\\}| = 3$\n-   $k_5 = |\\{2,10\\}| = 2$\n-   $k_6 = |\\{2\\}| = 1$\n-   $k_7 = |\\{3\\}| = 1$\n-   $k_8 = |\\{4\\}| = 1$\n-   $k_9 = |\\{4,10\\}| = 2$\n-   $k_{10} = |\\{5,9\\}| = 2$\n\nNext, we compute the excess degree, $k_i-1$, for each node.\n-   $k_1-1 = 2$\n-   $k_2-1 = 2$\n-   $k_3-1 = 1$\n-   $k_4-1 = 2$\n-   $k_5-1 = 1$\n-   $k_6-1 = 0$\n-   $k_7-1 = 0$\n-   $k_8-1 = 0$\n-   $k_9-1 = 1$\n-   $k_{10}-1 = 1$\n\nNow, for each node $i$, we find the set of nodes $\\partial B(i, 2)$ at distance $2$ and compute $\\mathrm{CI}_2(i)$.\n\n-   **Node $1$**: Neighbors at distance $1$ are $\\{2, 3, 4\\}$. Neighbors of these (excluding those already visited) give nodes at distance $2$: $\\partial B(1, 2) = \\{5, 6, 7, 8, 9\\}$.\n    $\\mathrm{CI}_2(1) = (k_1-1) \\left( (k_5-1)+(k_6-1)+(k_7-1)+(k_8-1)+(k_9-1) \\right) = 2(1+0+0+0+1) = 4$.\n\n-   **Node $2$**: Neighbors at distance $1$ are $\\{1, 5, 6\\}$. Neighbors of these give nodes at distance $2$: $\\partial B(2, 2) = \\{3, 4, 10\\}$.\n    $\\mathrm{CI}_2(2) = (k_2-1) \\left( (k_3-1)+(k_4-1)+(k_{10}-1) \\right) = 2(1+2+1) = 8$.\n\n-   **Node $3$**: Neighbors at distance $1$ are $\\{1, 7\\}$. Neighbors of these give nodes at distance $2$: $\\partial B(3, 2) = \\{2, 4\\}$.\n    $\\mathrm{CI}_2(3) = (k_3-1) \\left( (k_2-1)+(k_4-1) \\right) = 1(2+2) = 4$.\n\n-   **Node $4$**: Neighbors at distance $1$ are $\\{1, 8, 9\\}$. Neighbors of these give nodes at distance $2$: $\\partial B(4, 2) = \\{2, 3, 10\\}$.\n    $\\mathrm{CI}_2(4) = (k_4-1) \\left( (k_2-1)+(k_3-1)+(k_{10}-1) \\right) = 2(2+1+1) = 8$.\n\n-   **Node $5$**: Neighbors at distance $1$ are $\\{2, 10\\}$. Neighbors of these give nodes at distance $2$: $\\partial B(5, 2) = \\{1, 6, 9\\}$.\n    $\\mathrm{CI}_2(5) = (k_5-1) \\left( (k_1-1)+(k_6-1)+(k_9-1) \\right) = 1(2+0+1) = 3$.\n\n-   **Node $6$**: $k_6-1=0$, so $\\mathrm{CI}_2(6) = 0$.\n\n-   **Node $7$**: $k_7-1=0$, so $\\mathrm{CI}_2(7) = 0$.\n\n-   **Node $8$**: $k_8-1=0$, so $\\mathrm{CI}_2(8) = 0$.\n\n-   **Node $9$**: Neighbors at distance $1$ are $\\{4, 10\\}$. Neighbors of these give nodes at distance $2$: $\\partial B(9, 2) = \\{1, 8, 5\\}$.\n    $\\mathrm{CI}_2(9) = (k_9-1) \\left( (k_1-1)+(k_8-1)+(k_5-1) \\right) = 1(2+0+1) = 3$.\n\n-   **Node $10$**: Neighbors at distance $1$ are $\\{5, 9\\}$. Neighbors of these give nodes at distance $2$: $\\partial B(10, 2) = \\{2, 4\\}$.\n    $\\mathrm{CI}_2(10) = (k_{10}-1) \\left( (k_2-1)+(k_4-1) \\right) = 1(2+2) = 4$.\n\nThe computed $\\mathrm{CI}_2$ values are:\n-   $\\mathrm{CI}_2(1) = 4$\n-   $\\mathrm{CI}_2(2) = 8$\n-   $\\mathrm{CI}_2(3) = 4$\n-   $\\mathrm{CI}_2(4) = 8$\n-   $\\mathrm{CI}_2(5) = 3$\n-   $\\mathrm{CI}_2(6) = 0$\n-   $\\mathrm{CI}_2(7) = 0$\n-   $\\mathrm{CI}_2(8) = 0$\n-   $\\mathrm{CI}_2(9) = 3$\n-   $\\mathrm{CI}_2(10) = 4$\n\nFinally, we produce the ordered list of nodes for removal. The rule is to remove nodes in descending order of their initial $\\mathrm{CI}_2$ value, breaking ties by selecting the node with the smallest label first.\n\n1.  Highest score is $\\mathrm{CI}_2 = 8$: Nodes $\\{2, 4\\}$. Ordered by label: $2, 4$.\n2.  Next score is $\\mathrm{CI}_2 = 4$: Nodes $\\{1, 3, 10\\}$. Ordered by label: $1, 3, 10$.\n3.  Next score is $\\mathrm{CI}_2 = 3$: Nodes $\\{5, 9\\}$. Ordered by label: $5, 9$.\n4.  Lowest score is $\\mathrm{CI}_2 = 0$: Nodes $\\{6, 7, 8\\}$. Ordered by label: $6, 7, 8$.\n\nCombining these groups, the complete dismantling order is: $2, 4, 1, 3, 10, 5, 9, 6, 7, 8$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 4 & 1 & 3 & 10 & 5 & 9 & 6 & 7 & 8\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from theory to application, we now explore how a metric like Collective Influence is deployed in an algorithmic strategy. A critical choice in network dismantling is whether to use a static ranking, where scores are computed once, or an adaptive approach that re-evaluates the network after each node removal. This advanced computational exercise  asks you to implement and compare both strategies, revealing the practical trade-offs between computational cost and dismantling efficiency across different network structures.",
            "id": "4295239",
            "problem": "You are given an undirected, simple graph $G = (V,E)$ with $|V| = N$ and a target threshold $\\theta \\in [0,1]$ that bounds the acceptable size of the residual largest connected component after node removals. The goal is to compare two dismantling strategies: a greedy adaptive strategy that recalculates a node score after each removal, and a static ranking strategy that computes scores once and removes nodes in that fixed order. You must implement both strategies, stopping when the fraction of the size of the largest connected component with respect to the original number of nodes $N$ drops to at most $\\theta$, and report how many nodes each strategy removes to achieve this target.\n\nStart from the following foundational definitions. Percolation on a network is modeled by removing nodes and examining connectivity properties of the residual graph $G \\setminus R$, where $R \\subseteq V$ is the set of removed nodes. Define the largest connected component fraction under removals $R$ by\n$$\ng(R) = \\frac{|C_{\\max}(G \\setminus R)|}{N},\n$$\nwhere $C_{\\max}(\\cdot)$ denotes the largest connected component in the residual graph induced by $V \\setminus R$. The dismantling task is to find a removal set $R$ such that $g(R) \\le \\theta$, while minimizing $|R|$.\n\nAs a node scoring function, use Collective Influence (CI) at radius $\\ell$, defined for a node $i \\in V \\setminus R$ by\n$$\n\\mathrm{CI}_\\ell(i) = \\big(k_i - 1\\big) \\sum_{j \\in \\partial \\mathcal{B}(i,\\ell)} \\big(k_j - 1\\big),\n$$\nwhere $k_i$ is the degree of node $i$ in the current residual graph and $\\partial \\mathcal{B}(i,\\ell)$ is the boundary of the ball of radius $\\ell$ around $i$, i.e., the set of nodes at graph distance exactly $\\ell$ from $i$ in $G \\setminus R$. Intuitively, $\\mathrm{CI}_\\ell(i)$ estimates the multiplicative impact of removing node $i$ on weakening long-range connectivity, capturing non-backtracking paths that sustain the largest connected component.\n\nImplement and compare the following strategies:\n- Greedy adaptive strategy: Starting from $R = \\varnothing$, while $g(R) > \\theta$, compute $\\mathrm{CI}_\\ell(i)$ for all $i \\in V \\setminus R$, select a node $i^\\star$ that maximizes $\\mathrm{CI}_\\ell(i)$, apply the tie-breaker that prefers larger $k_i$ and then smaller index $i$ for equal scores, remove $i^\\star$ by setting $R \\leftarrow R \\cup \\{i^\\star\\}$, and repeat.\n- Static ranking strategy: Compute $\\mathrm{CI}_\\ell(i)$ once on the original graph ($R = \\varnothing$), sort nodes in descending order by $\\mathrm{CI}_\\ell(i)$ with the same tie-breaker rule, and remove nodes in that fixed order, updating $g(R)$ after each removal, until $g(R) \\le \\theta$.\n\nYour program must implement both strategies and return, for each test case, the integer triple $[r_{\\text{static}}, r_{\\text{adaptive}}, r_{\\text{static}} - r_{\\text{adaptive}}]$, where $r_{\\text{static}}$ and $r_{\\text{adaptive}}$ are the numbers of removed nodes required by the static and adaptive strategies, respectively, to meet $g(R) \\le \\theta$.\n\nUse $\\ell=2$ for the Collective Influence radius in all cases. The graphs and targets in the test suite are:\n\n- Case A (Erdős–Rényi graph): $G$ is $G(N,p)$ with $N=50$, $p=0.08$, and random seed $42$. Target $\\theta = 0.2$.\n- Case B (two cliques with a bridge): $G$ is the disjoint union of two cliques of size $20$ each, with an additional single edge connecting node $0$ in the first clique to node $20$ in the second clique. Target $\\theta = 0.4$.\n- Case C (Barabási–Albert preferential attachment graph): $G$ has $N=60$, attachment parameter $m=2$, and random seed $1337$. Target $\\theta = 0.15$.\n- Case D (path graph): $G$ is a path on $N=30$ nodes. Target $\\theta = 0.5$.\n\nScientific realism and algorithmic constraints:\n- Use an undirected, simple graph model. There are no weights, self-loops, or parallel edges.\n- For the Erdős–Rényi and Barabási–Albert graphs, ensure reproducibility by using the specified seeds and the standard degree-proportional attachment rule in the Barabási–Albert case.\n- For tie-breaking when multiple nodes have equal $\\mathrm{CI}_\\ell$, choose the node with larger current degree $k_i$, and if still tied, the node with smaller index $i$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[r_{\\text{static}}, r_{\\text{adaptive}}, r_{\\text{static}} - r_{\\text{adaptive}}]$ for the corresponding test case, in the order A, B, C, D. For example, the printed line should look like\n$$\n\\big[\\,[r_{\\text{static}}^{(A)}, r_{\\text{adaptive}}^{(A)}, r_{\\text{static}}^{(A)} - r_{\\text{adaptive}}^{(A)}],\\,[r_{\\text{static}}^{(B)}, r_{\\text{adaptive}}^{(B)}, r_{\\text{static}}^{(B)} - r_{\\text{adaptive}}^{(B)}],\\,\\ldots\\,\\big].\n$$\nAll reported quantities must be integers with no physical units.\n\nTest suite coverage rationale:\n- Case A probes a typical random graph above the percolation threshold.\n- Case B stresses modular structure with a minimal inter-community bridge.\n- Case C evaluates heterogeneity characteristic of scale-free networks.\n- Case D examines a boundary condition where a single removal at the center can meet the target $\\theta$.",
            "solution": "The user has provided a well-defined computational problem from the field of network science, specifically focusing on network dismantling via optimal percolation. The problem asks for a comparison of two node removal strategies—a static ranking approach and a greedy adaptive approach—based on the Collective Influence (CI) metric. The problem statement is complete, scientifically sound, and algorithmically specified, including all necessary parameters, graph generation procedures, and tie-breaking rules. Therefore, the problem is valid and a solution can be constructed.\n\nThe solution will be implemented by breaking down the problem into several logical components:\n1.  Graph Generation: Procedures to construct the four specified network topologies.\n2.  Core Algorithm Components: Functions to compute graph properties essential for the dismantling process, such as connected components and the Collective Influence (CI) score.\n3.  Dismantling Strategies: Implementations of the static and adaptive node removal procedures.\n4.  Main Execution Loop: A main function to iterate through the test cases, run both strategies, and format the results.\n\n**1. Graph Generation**\n\nWe will create four distinct graphs as per the problem specification. An adjacency matrix, represented by a NumPy array, is a suitable data structure for these graphs given their modest size ($N \\le 60$).\n\n-   **Case A (Erdős–Rényi Graph)**: An instance of the $G(N,p)$ model is generated with $N=50$ and $p=0.08$. Using a seeded random number generator (`seed=42`) ensures reproducibility. An edge is created between each pair of distinct nodes $(i, j)$ with probability $p$.\n-   **Case B (Two Cliques with a Bridge)**: This graph consists of two complete graphs (cliques) of size $20$. We construct this by creating a block-diagonal adjacency matrix where each block is the adjacency matrix of a clique. A single bridging edge is then added between node $0$ (in the first clique) and node $20$ (in the second clique), for a total of $N=40$ nodes.\n-   **Case C (Barabási–Albert Graph)**: A scale-free network is generated using the preferential attachment model with $N=60$ nodes and attachment parameter $m=2$. Starting with a small initial connected graph ($m$ nodes), new nodes are added one by one, each forming $m$ edges to existing nodes. The probability of connecting to an existing node $i$ is proportional to its current degree $k_i$. A seeded random number generator (`seed=1337`) to ensures a deterministic outcome.\n-   **Case D (Path Graph)**: A simple linear chain of $N=30$ nodes is created, where each node $i$ is connected to nodes $i-1$ and $i+1$, for $i \\in \\{1, \\ldots, N-2\\}$.\n\n**2. Core Algorithm Components**\n\nTo implement the dismantling strategies, we require several algorithmic helper functions.\n\n-   **Largest Connected Component (LCC) Size**: The stopping condition for both strategies, $g(R) = |C_{\\max}(G \\setminus R)|/N \\le \\theta$, requires calculating the size of the largest connected component in the residual graph. This can be efficiently accomplished using a Breadth-First Search (BFS) or Depth-First Search (DFS). Alternatively, we can use the `scipy.sparse.csgraph.connected_components` function, which identifies the components and assigns a label to each node. The sizes of the components can then be found by counting the occurrences of each label. The maximum of these counts is $|C_{\\max}|$.\n\n-   **Single-Source Shortest Paths (Distances)**: The Collective Influence metric, $\\mathrm{CI}_\\ell(i)$, depends on identifying nodes at an exact graph distance $\\ell$ from a given node $i$. For an unweighted graph, a Breadth-First Search (BFS) starting from node $i$ is the standard and most efficient algorithm to compute the shortest path distances to all other nodes. This will be implemented as a helper function.\n\n-   **Collective Influence (CI) Calculation**: The CI score for a node $i$ with radius $\\ell=2$ is given by:\n    $$\n    \\mathrm{CI}_2(i) = \\big(k_i - 1\\big) \\sum_{j \\in \\partial \\mathcal{B}(i,2)} \\big(k_j - 1\\big)\n    $$\n    To compute this for a node $i$ in a given (residual) graph:\n    1.  Determine the current degrees $k_v$ for all nodes $v$ in the graph by summing the rows of the adjacency matrix.\n    2.  Perform a BFS starting from node $i$ to find the set of nodes $\\partial \\mathcal{B}(i,2)$ at exactly distance $\\ell=2$.\n    3.  Calculate the term $(k_i - 1)$.\n    4.  Calculate the sum $\\sum_{j \\in \\partial \\mathcal{B}(i,2)} (k_j - 1)$. A node with degree $1$ contributes $0$ to this sum, and a node with degree $0$ contributes $-1$.\n    5.  The product of these two values is $\\mathrm{CI}_2(i)$. If $\\partial \\mathcal{B}(i,2)$ is empty, the sum is $0$, and thus $\\mathrm{CI}_2(i) = 0$. This correctly handles cases like nodes in a clique, where no nodes are at distance $2$.\n\n**3. Dismantling Strategies**\n\nWith the core components in place, we can implement the two strategies. Both will operate on a copy of the original graph's adjacency matrix and iteratively \"remove\" nodes by zeroing out the corresponding rows and columns.\n\n-   **Static Ranking Strategy**:\n    1.  On the original, intact graph, calculate the initial $\\mathrm{CI}_2$ score and degree for every node.\n    2.  Create a ranked list of all nodes, sorted in descending order based on a tuple: $(\\mathrm{CI}_2, k, -i)$. The negative index handles the final tie-breaker correctly (smaller index is \"greater\").\n    3.  Iterate through this fixed ranked list. In each step, remove the next node from the graph.\n    4.  After each removal, calculate the LCC size of the residual graph.\n    5.  Stop when the LCC fraction drops to or below the threshold $\\theta$. The number of nodes removed is $r_{\\text{static}}$.\n\n-   **Greedy Adaptive Strategy**:\n    1.  Start with the original graph and an empty set of removed nodes.\n    2.  Begin a loop that continues as long as the LCC fraction is greater than $\\theta$.\n    3.  **Inside the loop (at each step)**:\n        a. Identify the set of currently active (not yet removed) nodes.\n        b. For each active node, re-calculate its $\\mathrm{CI}_2$ score and degree based on the *current* state of the residual graph.\n        c. Select the node $i^\\star$ that maximizes the score, using the same $(\\mathrm{CI}_2, k, -i)$ tuple for sorting and tie-breaking.\n        d. Add $i^\\star$ to the set of removed nodes and update the graph.\n    4.  The loop terminates when the condition is met. The total number of nodes removed is $r_{\\text{adaptive}}$.\n\n**4. Main Execution and Output**\n\nThe main program will define the parameters for each of the four test cases. It will then loop through them, performing the following for each case:\n1.  Generate the specified graph.\n2.  Run the static strategy to find $r_{\\text{static}}$.\n3.  Run the adaptive strategy to find $r_{\\text{adaptive}}$.\n4.  Compute the difference $r_{\\text{static}} - r_{\\text{adaptive}}$.\n5.  Store the resulting triple $[r_{\\text{static}}, r_{\\text{adaptive}}, r_{\\text{static}} - r_{\\text{adaptive}}]$.\n\nFinally, the list of results for all cases will be printed to standard output in the specified format `[[...],[...],...]`. This structured approach ensures correctness, reproducibility, and adherence to the problem's requirements.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.csgraph import connected_components\nfrom collections import deque\n\ndef generate_graph(case_params):\n    \"\"\"Generates a graph based on the specified case parameters.\"\"\"\n    case, N, params, seed = case_params\n    adj = np.zeros((N, N), dtype=int)\n    rng = np.random.default_rng(seed)\n\n    if case == 'ER':\n        p = params['p']\n        for i in range(N):\n            for j in range(i + 1, N):\n                if rng.random() < p:\n                    adj[i, j] = adj[j, i] = 1\n    \n    elif case == 'two_cliques':\n        size = params['size']\n        # First clique\n        adj[0:size, 0:size] = 1\n        # Second clique\n        adj[size:2*size, size:2*size] = 1\n        np.fill_diagonal(adj, 0)\n        # Bridge\n        adj[0, size] = adj[size, 0] = 1\n\n    elif case == 'BA':\n        m = params['m']\n        # Start with a complete graph of m nodes\n        adj[:m, :m] = 1\n        np.fill_diagonal(adj[:m, :m], 0)\n        \n        degrees = np.sum(adj, axis=1)\n        \n        for i in range(m, N):\n            # Flatten degrees of existing nodes and their indices\n            existing_nodes = np.arange(i)\n            existing_degrees = degrees[:i]\n            \n            # Prevent division by zero if all degrees are zero (unlikely)\n            total_degree = np.sum(existing_degrees)\n            if total_degree == 0:\n                # Connect to m random nodes if no edges exist\n                targets = rng.choice(existing_nodes, size=m, replace=False)\n            else:\n                probs = existing_degrees / total_degree\n                targets = rng.choice(existing_nodes, size=m, replace=False, p=probs)\n            \n            for target in targets:\n                adj[i, target] = adj[target, i] = 1\n            degrees = np.sum(adj, axis=1) # Update degrees for next iteration\n\n    elif case == 'path':\n        for i in range(N - 1):\n            adj[i, i + 1] = adj[i + 1, i] = 1\n\n    return adj\n\ndef get_lcc_size(adj_matrix, active_nodes_mask):\n    \"\"\"Calculates the size of the Largest Connected Component.\"\"\"\n    num_nodes = adj_matrix.shape[0]\n    if not np.any(active_nodes_mask):\n        return 0\n    \n    # Create a subgraph view with only active nodes\n    active_indices = np.where(active_nodes_mask)[0]\n    subgraph_adj = adj_matrix[np.ix_(active_indices, active_indices)]\n    \n    if subgraph_adj.size == 0:\n        return 0\n\n    n_components, labels = connected_components(subgraph_adj, directed=False, return_labels=True)\n    \n    if n_components == 0:\n        return 0\n        \n    component_sizes = np.bincount(labels)\n    return np.max(component_sizes)\n\ndef get_distances(adj_matrix, start_node_idx):\n    \"\"\"Calculates shortest path distances from a start node using BFS.\"\"\"\n    num_nodes = adj_matrix.shape[0]\n    distances = -np.ones(num_nodes, dtype=int)\n    \n    if adj_matrix[start_node_idx].sum() == 0: # Node is isolated\n        distances[start_node_idx] = 0\n        return distances\n\n    q = deque([(start_node_idx, 0)])\n    distances[start_node_idx] = 0\n    \n    while q:\n        curr_node, dist = q.popleft()\n        \n        for neighbor in np.where(adj_matrix[curr_node] == 1)[0]:\n            if distances[neighbor] == -1:\n                distances[neighbor] = dist + 1\n                q.append((neighbor, dist + 1))\n                \n    return distances\n\ndef calculate_ci_l2(adj, degrees, node_idx):\n    \"\"\"Calculates Collective Influence for a single node with l=2.\"\"\"\n    k_i = degrees[node_idx]\n    if k_i <= 1:\n        return 0.0\n\n    distances = get_distances(adj, node_idx)\n    ball_boundary = np.where(distances == 2)[0]\n    \n    if ball_boundary.size == 0:\n        return 0.0\n        \n    sum_term = np.sum(degrees[ball_boundary] - 1)\n    ci = (k_i - 1) * sum_term\n    return float(ci)\n\ndef solve_strategy(adj_orig, N, theta, l, is_adaptive):\n    \"\"\"Solves for the number of removals for a given strategy.\"\"\"\n    adj = adj_orig.copy()\n    removed_nodes = set()\n    active_nodes_mask = np.ones(N, dtype=bool)\n    num_removals = 0\n\n    # Initial LCC check\n    lcc_size = get_lcc_size(adj, active_nodes_mask)\n    if lcc_size / N <= theta:\n        return 0\n    \n    # Static strategy: pre-compute ranking\n    removal_order = []\n    if not is_adaptive:\n        degrees = np.sum(adj, axis=1)\n        scores = []\n        for i in range(N):\n            ci = calculate_ci_l2(adj, degrees, i)\n            scores.append((ci, degrees[i], -i))\n        # Sort nodes by (CI, degree, -index)\n        sorted_nodes = sorted(range(N), key=lambda i: scores[i], reverse=True)\n        removal_order = deque(sorted_nodes)\n\n    while True:\n        node_to_remove = -1\n        \n        if is_adaptive:\n            # Re-calculate scores for all active nodes in each step\n            active_indices = np.where(active_nodes_mask)[0]\n            degrees = np.sum(adj, axis=1)\n            \n            best_score = (-1, -1, N)\n            \n            for i in active_indices:\n                ci = calculate_ci_l2(adj, degrees, i)\n                score = (ci, degrees[i], -i)\n                if score > best_score:\n                    best_score = score\n                    node_to_remove = i\n        else: # Static strategy\n            node_to_remove = removal_order.popleft()\n\n        # Remove the node\n        adj[node_to_remove, :] = 0\n        adj[:, node_to_remove] = 0\n        removed_nodes.add(node_to_remove)\n        active_nodes_mask[node_to_remove] = False\n        num_removals += 1\n        \n        # Check stopping condition\n        lcc_size = get_lcc_size(adj, active_nodes_mask)\n        if lcc_size == 0 or (lcc_size / N) <= theta:\n            return num_removals\n\ndef solve():\n    test_cases = [\n        # (Case Name, N, params, seed), theta\n        (('ER', 50, {'p': 0.08}, 42), 0.2),\n        (('two_cliques', 40, {'size': 20}, None), 0.4),\n        (('BA', 60, {'m': 2}, 1337), 0.15),\n        (('path', 30, {}, None), 0.5),\n    ]\n\n    all_results = []\n    l = 2\n\n    for case_params, theta in test_cases:\n        N = case_params[1]\n        adj_orig = generate_graph(case_params)\n\n        r_static = solve_strategy(adj_orig, N, theta, l, is_adaptive=False)\n        r_adaptive = solve_strategy(adj_orig, N, theta, l, is_adaptive=True)\n        \n        diff = r_static - r_adaptive\n        all_results.append([r_static, r_adaptive, diff])\n\n    # Format the final output string\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}