{
    "hands_on_practices": [
        {
            "introduction": "Before delving into complex algorithms, it is crucial to build intuition with a foundational case. This first practice explores the star graph, a simple yet powerful model for networks with a central hub. By analytically determining the optimal way to dismantle this structure, you will solidify your understanding of the core objective of network dismantling: to fragment the network into the smallest possible components with minimal effort .",
            "id": "4295177",
            "problem": "Consider the star graph $K_{1,m}$, defined as the graph with one central node $c$ connected to $m$ leaf nodes and no other edges. Let $G=(V,E)$ denote this graph, where $|V|=m+1$. For any node-removal set $R \\subseteq V$, define $S(G \\setminus R)$ to be the number of nodes in the largest connected component of the induced subgraph on $V \\setminus R$. In the context of optimal percolation for network dismantling, the goal is to minimize $S(G \\setminus R)$ by removing nodes.\n\nStarting from the fundamental definition of connected components in graphs and the effect of node removal on component structure, determine the minimal fraction $q^{\\star}$ of nodes that must be removed from $K_{1,m}$ to guarantee that $S(G \\setminus R) \\leq 1$, and verify analytically that an optimal removal set achieving this bound includes the center node $c$. Express $q^{\\star}$ as a function of $m$ in exact form. Your final answer must be a single exact mathematical expression with no units. No rounding is required.",
            "solution": "The star graph $K_{1,m}$ has one central node $c$ and $m$ leaf nodes. The total number of nodes is $N = m+1$. The goal is to achieve a largest connected component (LCC) size of $S \\le 1$.\n\nThere are two primary strategies for node removal:\n1.  **Remove the central node, $c$**: The set of removed nodes is $R = \\{c\\}$, so $|R|=1$. The remaining graph consists of $m$ isolated leaf nodes. The LCC in the remaining graph is a single node, so its size is $S(G \\setminus R) = 1$. This satisfies the condition. The fraction of removed nodes is $q = \\frac{|R|}{N} = \\frac{1}{m+1}$.\n\n2.  **Remove leaf nodes**: To ensure $S \\le 1$, we must disconnect the center node from all leaves. This requires removing all $m$ leaf nodes. The set of removed nodes is $R = \\{\\text{all leaves}\\}$, so $|R|=m$. The remaining graph has only the isolated central node, so $S(G \\setminus R) = 1$. This strategy also satisfies the condition, but the fraction of removed nodes is $q = \\frac{m}{m+1}$.\n\nComparing the two strategies, removing the central node is far more efficient. The minimal fraction of nodes required is achieved by removing only the center node.\nTherefore, the minimal fraction is $q^{\\star} = \\frac{1}{m+1}$.",
            "answer": "$$\\boxed{\\frac{1}{m+1}}$$"
        },
        {
            "introduction": "In most real-world networks, identifying the most critical node is not as obvious as in a star graph. We need a quantitative measure of influence, and the Collective Influence (CI) score provides just that. This exercise provides direct, hands-on practice in calculating the $\\mathrm{CI}_{\\ell}$ metric, a key tool in optimal percolation that estimates a node's role in maintaining long-range connectivity .",
            "id": "4295214",
            "problem": "Consider an undirected, simple graph $G$ with vertex set $V=\\{1,2,\\dots,10\\}$ and edge set\n$$\nE=\\{(1,2),(1,3),(1,4),(2,5),(2,6),(3,7),(4,8),(4,9),(5,10),(9,10)\\}.\n$$\nLet $k_{i}$ denote the degree of node $i$. In the optimal percolation framework under the locally tree-like approximation, the onset of the disappearance of the largest connected component can be analyzed through the spectral properties of the non-backtracking matrix (NB), and the node-removal problem can be cast into an influence-maximization objective over finite-radius neighborhoods. Starting from these foundations—percolation on sparse networks, the largest eigenvalue of the non-backtracking matrix as the stability criterion, and the restriction to finite-radius neighborhoods—derive a node-level influence measure at radius $\\ell=2$ and compute it for every node of the graph $G$.\n\nThen, based on the collective influence dismantling strategy that removes nodes in descending order of the derived radius-$2$ influence, and breaking ties by selecting the node with the smallest label first, produce the complete ordered list of node labels for the first ten selections (i.e., an ordering of all nodes according to their initial radius-$2$ influence). Express your final ordered list as a row matrix. No rounding is required.",
            "solution": "The Collective Influence $CI_{\\ell}(i)$ of a node $i$ at radius $\\ell$ is defined as:\n$$\nCI_{\\ell}(i) = (k_i - 1) \\sum_{j \\in \\partial B(i, \\ell)} (k_j - 1)\n$$\nwhere $\\partial B(i, \\ell)$ is the set of vertices at a shortest path distance of exactly $\\ell$ from vertex $i$. For this problem, we use $\\ell=2$.\n\nFirst, we compute the degree $k_i$ for each node:\n-   $k_1 = 3$, $k_2 = 3$, $k_3 = 2$, $k_4 = 3$, $k_5 = 2$, $k_6 = 1$, $k_7 = 1$, $k_8 = 1$, $k_9 = 2$, $k_{10} = 2$.\n\nThe excess degrees $(k_i-1)$ are:\n-   $k_1-1 = 2$, $k_2-1 = 2$, $k_3-1 = 1$, $k_4-1 = 2$, $k_5-1 = 1$, $k_6-1 = 0$, $k_7-1 = 0$, $k_8-1 = 0$, $k_9-1 = 1$, $k_{10}-1 = 1$.\n\nNext, we compute $CI_2(i)$ for each node by identifying nodes at distance 2.\n-   $\\partial B(1, 2) = \\{5, 6, 7, 8, 9\\}$. $CI_2(1) = (k_1-1) \\left( (k_5-1)+(k_6-1)+(k_7-1)+(k_8-1)+(k_9-1) \\right) = 2(1+0+0+0+1) = 4$.\n-   $\\partial B(2, 2) = \\{3, 4, 10\\}$. $CI_2(2) = (k_2-1) \\left( (k_3-1)+(k_4-1)+(k_{10}-1) \\right) = 2(1+2+1) = 8$.\n-   $\\partial B(3, 2) = \\{2, 4\\}$. $CI_2(3) = (k_3-1) \\left( (k_2-1)+(k_4-1) \\right) = 1(2+2) = 4$.\n-   $\\partial B(4, 2) = \\{2, 3, 10\\}$. $CI_2(4) = (k_4-1) \\left( (k_2-1)+(k_3-1)+(k_{10}-1) \\right) = 2(2+1+1) = 8$.\n-   $\\partial B(5, 2) = \\{1, 6, 9\\}$. $CI_2(5) = (k_5-1) \\left( (k_1-1)+(k_6-1)+(k_9-1) \\right) = 1(2+0+1) = 3$.\n-   $CI_2(6) = 0$ since $k_6-1=0$.\n-   $CI_2(7) = 0$ since $k_7-1=0$.\n-   $CI_2(8) = 0$ since $k_8-1=0$.\n-   $\\partial B(9, 2) = \\{1, 8, 5\\}$. $CI_2(9) = (k_9-1) \\left( (k_1-1)+(k_8-1)+(k_5-1) \\right) = 1(2+0+1) = 3$.\n-   $\\partial B(10, 2) = \\{2, 4\\}$. $CI_2(10) = (k_{10}-1) \\left( (k_2-1)+(k_4-1) \\right) = 1(2+2) = 4$.\n\nThe computed $CI_2$ values are:\n-   $CI_2(1) = 4$\n-   $CI_2(2) = 8$\n-   $CI_2(3) = 4$\n-   $CI_2(4) = 8$\n-   $CI_2(5) = 3$\n-   $CI_2(6) = 0$\n-   $CI_2(7) = 0$\n-   $CI_2(8) = 0$\n-   $CI_2(9) = 3$\n-   $CI_2(10) = 4$\n\nFinally, we produce the ordered list of nodes for removal. The rule is to remove nodes in descending order of their initial $CI_2$ value, breaking ties by selecting the node with the smallest label first.\n\n1.  Score 8: Nodes $\\{2, 4\\} \\rightarrow$ Order: $2, 4$.\n2.  Score 4: Nodes $\\{1, 3, 10\\} \\rightarrow$ Order: $1, 3, 10$.\n3.  Score 3: Nodes $\\{5, 9\\} \\rightarrow$ Order: $5, 9$.\n4.  Score 0: Nodes $\\{6, 7, 8\\} \\rightarrow$ Order: $6, 7, 8$.\n\nThe complete dismantling order is: $2, 4, 1, 3, 10, 5, 9, 6, 7, 8$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  4  1  3  10  5  9  6  7  8\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With a method to score nodes, a practical question arises: should we recalculate scores after each removal, or is an initial ranking sufficient? This advanced computational exercise tackles the vital comparison between adaptive and static dismantling strategies, exploring the trade-off between algorithmic efficiency and performance across different network topologies. By implementing and comparing these approaches, you will gain insight into the dynamic nature of network collapse and the practical application of dismantling theories .",
            "id": "4295239",
            "problem": "You are given an undirected, simple graph $G = (V,E)$ with $|V| = N$ and a target threshold $\\theta \\in [0,1]$ that bounds the acceptable size of the residual largest connected component after node removals. The goal is to compare two dismantling strategies: a greedy adaptive strategy that recalculates a node score after each removal, and a static ranking strategy that computes scores once and removes nodes in that fixed order. You must implement both strategies, stopping when the fraction of the size of the largest connected component with respect to the original number of nodes $N$ drops to at most $\\theta$, and report how many nodes each strategy removes to achieve this target.\n\nStart from the following foundational definitions. Percolation on a network is modeled by removing nodes and examining connectivity properties of the residual graph $G \\setminus R$, where $R \\subseteq V$ is the set of removed nodes. Define the largest connected component fraction under removals $R$ by\n$$\ng(R) = \\frac{|C_{\\max}(G \\setminus R)|}{N},\n$$\nwhere $C_{\\max}(\\cdot)$ denotes the largest connected component in the residual graph induced by $V \\setminus R$. The dismantling task is to find a removal set $R$ such that $g(R) \\le \\theta$, while minimizing $|R|$.\n\nAs a node scoring function, use Collective Influence (CI) at radius $l$ (Collective Influence (CI)), defined for a node $i \\in V \\setminus R$ by\n$$\n\\mathrm{CI}_l(i) = \\big(k_i - 1\\big) \\sum_{j \\in \\partial \\mathcal{B}(i,l)} \\big(k_j - 1\\big),\n$$\nwhere $k_i$ is the degree of node $i$ in the current residual graph and $\\partial \\mathcal{B}(i,l)$ is the boundary of the ball of radius $l$ around $i$, i.e., the set of nodes at graph distance exactly $l$ from $i$ in $G \\setminus R$. Intuitively, $\\mathrm{CI}_l(i)$ estimates the multiplicative impact of removing node $i$ on weakening long-range connectivity, capturing non-backtracking paths that sustain the largest connected component.\n\nImplement and compare the following strategies:\n- Greedy adaptive strategy: Starting from $R = \\varnothing$, while $g(R)  \\theta$, compute $\\mathrm{CI}_l(i)$ for all $i \\in V \\setminus R$, select a node $i^\\star$ that maximizes $\\mathrm{CI}_l(i)$, apply the tie-breaker that prefers larger $k_i$ and then smaller index $i$ for equal scores, remove $i^\\star$ by setting $R \\leftarrow R \\cup \\{i^\\star\\}$, and repeat.\n- Static ranking strategy: Compute $\\mathrm{CI}_l(i)$ once on the original graph ($R = \\varnothing$), sort nodes in descending order by $\\mathrm{CI}_l(i)$ with the same tie-breaker rule, and remove nodes in that fixed order, updating $g(R)$ after each removal, until $g(R) \\le \\theta$.\n\nYour program must implement both strategies and return, for each test case, the integer triple $[r_{\\text{static}}, r_{\\text{adaptive}}, r_{\\text{static}} - r_{\\text{adaptive}}]$, where $r_{\\text{static}}$ and $r_{\\text{adaptive}}$ are the numbers of removed nodes required by the static and adaptive strategies, respectively, to meet $g(R) \\le \\theta$.\n\nUse $l=2$ for the Collective Influence radius in all cases. The graphs and targets in the test suite are:\n\n- Case A (Erdős–Rényi graph): $G$ is $G(N,p)$ with $N=50$, $p=0.08$, and random seed $42$. Target $\\theta = 0.2$.\n- Case B (two cliques with a bridge): $G$ is the disjoint union of two cliques of size $20$ each, with an additional single edge connecting node $0$ in the first clique to node $20$ in the second clique. Target $\\theta = 0.4$.\n- Case C (Barabási–Albert preferential attachment graph): $G$ has $N=60$, attachment parameter $m=2$, and random seed $1337$. Target $\\theta = 0.15$.\n- Case D (path graph): $G$ is a path on $N=30$ nodes. Target $\\theta = 0.5$.\n\nScientific realism and algorithmic constraints:\n- Use an undirected, simple graph model. There are no weights, self-loops, or parallel edges.\n- For the Erdős–Rényi and Barabási–Albert graphs, ensure reproducibility by using the specified seeds and the standard degree-proportional attachment rule in the Barabási–Albert case.\n- For tie-breaking when multiple nodes have equal $\\mathrm{CI}_l$, choose the node with larger current degree $k_i$, and if still tied, the node with smaller index $i$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[r_{\\text{static}}, r_{\\text{adaptive}}, r_{\\text{static}} - r_{\\text{adaptive}}]$ for the corresponding test case, in the order A, B, C, D. For example, the printed line should look like\n$$\n\\big[\\,[r_{\\text{static}}^{(A)}, r_{\\text{adaptive}}^{(A)}, r_{\\text{static}}^{(A)} - r_{\\text{adaptive}}^{(A)}],\\,[r_{\\text{static}}^{(B)}, r_{\\text{adaptive}}^{(B)}, r_{\\text{static}}^{(B)} - r_{\\text{adaptive}}^{(B)}],\\,\\ldots\\,\\big].\n$$\nAll reported quantities must be integers with no physical units.\n\nTest suite coverage rationale:\n- Case A probes a typical random graph above the percolation threshold.\n- Case B stresses modular structure with a minimal inter-community bridge.\n- Case C evaluates heterogeneity characteristic of scale-free networks.\n- Case D examines a boundary condition where a single removal at the center can meet the target $\\theta$.",
            "solution": "The problem requires implementing and comparing static and adaptive network dismantling strategies using the Collective Influence (CI) metric. The solution involves several computational steps.\n\n**1. Graph Generation**\nFour graphs are constructed according to the specifications:\n-   **Case A (Erdős–Rényi Graph)**: A $G(N,p)$ graph with $N=50, p=0.08$, using `seed=42`.\n-   **Case B (Two Cliques with a Bridge)**: Two 20-node cliques joined by a single edge, resulting in $N=40$.\n-   **Case C (Barabási–Albert Graph)**: A scale-free graph with $N=60, m=2$, using the preferential attachment model with `seed=1337`.\n-   **Case D (Path Graph)**: A linear chain of $N=30$ nodes.\n\n**2. Core Algorithm Components**\nHelper functions are needed to support the dismantling process:\n-   **Largest Connected Component (LCC) Size**: This function takes an adjacency matrix and a mask of active nodes, and returns the size of the largest component. It can be implemented using Breadth-First Search (BFS), Depth-First Search (DFS), or a library function like `scipy.sparse.csgraph.connected_components`.\n-   **Shortest Path Distances**: A BFS starting from a source node is used to find the shortest path distances required for the CI calculation.\n-   **Collective Influence (CI) Calculation**: A function is created to compute the CI score for a given node in the current graph state. For $\\ell=2$, the formula is:\n    $$ \\mathrm{CI}_2(i) = \\big(k_i - 1\\big) \\sum_{j \\in \\partial \\mathcal{B}(i,2)} \\big(k_j - 1\\big) $$\n    This requires calculating current degrees and distances within the residual graph.\n\n**3. Dismantling Strategies**\nBoth strategies are implemented to run until the LCC fraction drops to or below the target $\\theta$.\n-   **Static Ranking Strategy**:\n    1.  Calculate initial CI scores and degrees for all nodes on the original graph.\n    2.  Create a fixed ranked list of nodes, sorted in descending order by $(\\mathrm{CI}_2, k, -\\text{index})$ to handle tie-breaking.\n    3.  Remove nodes sequentially from this list until the stopping condition is met.\n-   **Greedy Adaptive Strategy**:\n    1.  In a loop, as long as the LCC fraction is above $\\theta$:\n        a. Re-calculate CI scores and degrees for all *currently active* nodes.\n        b. Select the node with the highest score according to the tie-breaking rule.\n        c. Remove the selected node and update the graph.\n\n**4. Main Execution and Output**\nThe main routine iterates through the four test cases. For each case, it generates the graph, runs both the static and adaptive strategies to find the number of removals ($r_{\\text{static}}$ and $r_{\\text{adaptive}}$), computes the difference, and stores the resulting triple. The final output is a list of these triples, formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.csgraph import connected_components\nfrom collections import deque\n\ndef generate_graph(case_params):\n    \"\"\"Generates a graph based on the specified case parameters.\"\"\"\n    case, N, params, seed = case_params\n    adj = np.zeros((N, N), dtype=int)\n    rng = np.random.default_rng(seed)\n\n    if case == 'ER':\n        p = params['p']\n        for i in range(N):\n            for j in range(i + 1, N):\n                if rng.random()  p:\n                    adj[i, j] = adj[j, i] = 1\n    \n    elif case == 'two_cliques':\n        size = params['size']\n        # First clique\n        adj[0:size, 0:size] = 1\n        # Second clique\n        adj[size:2*size, size:2*size] = 1\n        np.fill_diagonal(adj, 0)\n        # Bridge\n        adj[0, size] = adj[size, 0] = 1\n\n    elif case == 'BA':\n        m = params['m']\n        # Start with a complete graph of m nodes\n        adj[:m, :m] = 1\n        np.fill_diagonal(adj[:m, :m], 0)\n        \n        degrees = np.sum(adj, axis=1)\n        \n        for i in range(m, N):\n            # Flatten degrees of existing nodes and their indices\n            existing_nodes = np.arange(i)\n            existing_degrees = degrees[:i]\n            \n            # Prevent division by zero if all degrees are zero (unlikely)\n            total_degree = np.sum(existing_degrees)\n            if total_degree == 0:\n                # Connect to m random nodes if no edges exist\n                targets = rng.choice(existing_nodes, size=m, replace=False)\n            else:\n                probs = existing_degrees / total_degree\n                targets = rng.choice(existing_nodes, size=m, replace=False, p=probs)\n            \n            for target in targets:\n                adj[i, target] = adj[target, i] = 1\n            degrees = np.sum(adj, axis=1) # Update degrees for next iteration\n\n    elif case == 'path':\n        for i in range(N - 1):\n            adj[i, i + 1] = adj[i + 1, i] = 1\n\n    return adj\n\ndef get_lcc_size(adj_matrix, active_nodes_mask):\n    \"\"\"Calculates the size of the Largest Connected Component.\"\"\"\n    num_nodes = adj_matrix.shape[0]\n    if not np.any(active_nodes_mask):\n        return 0\n    \n    # Create a subgraph view with only active nodes\n    active_indices = np.where(active_nodes_mask)[0]\n    subgraph_adj = adj_matrix[np.ix_(active_indices, active_indices)]\n    \n    if subgraph_adj.size == 0:\n        return 0\n\n    n_components, labels = connected_components(subgraph_adj, directed=False, return_labels=True)\n    \n    if n_components == 0:\n        return 0\n        \n    component_sizes = np.bincount(labels)\n    return np.max(component_sizes)\n\ndef get_distances(adj_matrix, start_node_idx):\n    \"\"\"Calculates shortest path distances from a start node using BFS.\"\"\"\n    num_nodes = adj_matrix.shape[0]\n    distances = -np.ones(num_nodes, dtype=int)\n    \n    if adj_matrix[start_node_idx].sum() == 0: # Node is isolated\n        distances[start_node_idx] = 0\n        return distances\n\n    q = deque([(start_node_idx, 0)])\n    distances[start_node_idx] = 0\n    \n    while q:\n        curr_node, dist = q.popleft()\n        \n        for neighbor in np.where(adj_matrix[curr_node] == 1)[0]:\n            if distances[neighbor] == -1:\n                distances[neighbor] = dist + 1\n                q.append((neighbor, dist + 1))\n                \n    return distances\n\ndef calculate_ci_l2(adj, degrees, node_idx):\n    \"\"\"Calculates Collective Influence for a single node with l=2.\"\"\"\n    k_i = degrees[node_idx]\n    if k_i = 1:\n        return 0.0\n\n    distances = get_distances(adj, node_idx)\n    ball_boundary = np.where(distances == 2)[0]\n    \n    if ball_boundary.size == 0:\n        return 0.0\n        \n    sum_term = np.sum(degrees[ball_boundary] - 1)\n    ci = (k_i - 1) * sum_term\n    return float(ci)\n\ndef solve_strategy(adj_orig, N, theta, l, is_adaptive):\n    \"\"\"Solves for the number of removals for a given strategy.\"\"\"\n    adj = adj_orig.copy()\n    removed_nodes = set()\n    active_nodes_mask = np.ones(N, dtype=bool)\n    num_removals = 0\n\n    # Initial LCC check\n    lcc_size = get_lcc_size(adj, active_nodes_mask)\n    if lcc_size / N = theta:\n        return 0\n    \n    # Static strategy: pre-compute ranking\n    removal_order = []\n    if not is_adaptive:\n        degrees = np.sum(adj, axis=1)\n        scores = []\n        for i in range(N):\n            ci = calculate_ci_l2(adj, degrees, i)\n            scores.append((ci, degrees[i], -i))\n        # Sort nodes by (CI, degree, -index)\n        sorted_nodes = sorted(range(N), key=lambda i: scores[i], reverse=True)\n        removal_order = deque(sorted_nodes)\n\n    while True:\n        node_to_remove = -1\n        \n        if is_adaptive:\n            # Re-calculate scores for all active nodes in each step\n            active_indices = np.where(active_nodes_mask)[0]\n            degrees = np.sum(adj, axis=1)\n            \n            best_score = (-1, -1, N)\n            \n            for i in active_indices:\n                ci = calculate_ci_l2(adj, degrees, i)\n                score = (ci, degrees[i], -i)\n                if score  best_score:\n                    best_score = score\n                    node_to_remove = i\n        else: # Static strategy\n            node_to_remove = removal_order.popleft()\n\n        # Remove the node\n        adj[node_to_remove, :] = 0\n        adj[:, node_to_remove] = 0\n        removed_nodes.add(node_to_remove)\n        active_nodes_mask[node_to_remove] = False\n        num_removals += 1\n        \n        # Check stopping condition\n        lcc_size = get_lcc_size(adj, active_nodes_mask)\n        if lcc_size == 0 or (lcc_size / N) = theta:\n            return num_removals\n\ndef solve():\n    test_cases = [\n        # (Case Name, N, params, seed), theta\n        (('ER', 50, {'p': 0.08}, 42), 0.2),\n        (('two_cliques', 40, {'size': 20}, None), 0.4),\n        (('BA', 60, {'m': 2}, 1337), 0.15),\n        (('path', 30, {}, None), 0.5),\n    ]\n\n    all_results = []\n    l = 2\n\n    for case_params, theta in test_cases:\n        N = case_params[1]\n        adj_orig = generate_graph(case_params)\n\n        r_static = solve_strategy(adj_orig, N, theta, l, is_adaptive=False)\n        r_adaptive = solve_strategy(adj_orig, N, theta, l, is_adaptive=True)\n        \n        diff = r_static - r_adaptive\n        all_results.append([r_static, r_adaptive, diff])\n\n    # Format the final output string\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}