{
    "hands_on_practices": [
        {
            "introduction": "Before analyzing the specific dynamics of a network, we can ask a more fundamental question: does the network's wiring diagram even permit full state observation, regardless of the interaction strengths? This concept, known as *structural observability*, is crucial for designing effective sensor placement strategies from the ground up. This exercise introduces a powerful graph-theoretic technique that connects the abstract structure of a system to the minimum number of sensors required to ensure it can be observed .",
            "id": "4293873",
            "problem": "Consider a linear time-invariant (LTI) networked dynamical system with state vector $x \\in \\mathbb{R}^{6}$ and dynamics $\\dot{x} = A x$, where the structure of the system matrix $A$ is constrained by a directed acyclic graph (DAG) on the vertex set $\\{1,2,3,4,5,6\\}$. The graph has directed edges $1 \\to 2$, $1 \\to 3$, $2 \\to 4$, $3 \\to 4$, $4 \\to 5$, and $4 \\to 6$. A dedicated sensor is defined as an output that directly measures a single state component, i.e., $y = C x$ where each nonzero row of $C$ selects one coordinate of $x$. \n\nStarting from fundamental definitions of structural observability and the graph-theoretic construction of the system bipartite graph associated with the transpose matrix $A^{\\top}$, explain why unmatched vertices in a maximum matching determine the number of required dedicated sensors to render the system structurally observable. Then, construct the bipartite graph induced by the given DAG and use maximum matching to compute the minimal number of dedicated sensors needed to ensure structural observability of the system. Provide your final answer as a single integer.",
            "solution": "The problem requires an explanation of the relationship between structural observability and bipartite graph matching, followed by a computation of the minimal number of dedicated sensors for a given networked system.\n\nFirst, we address the theoretical foundation. For a *structured* system, the entries of the matrices $A$ and $C$ are either fixed zeros or independent free parameters. Structural observability requires that the system is observable for almost all numerical values of these free parameters. This is equivalent to ensuring that a certain structured matrix has a generic rank of $n$, the dimension of the state space.\n\nA powerful way to determine the minimum number of dedicated sensors is by analyzing the bipartite graph of the dynamics matrix $A$. We construct a bipartite graph $G_A = (V_x, V_{\\dot{x}}, E_A)$ for the matrix $A$, where $V_x = \\{x_1, \\dots, x_n\\}$ represents the state variables (columns of A) and $V_{\\dot{x}} = \\{\\dot{x}_1, \\dots, \\dot{x}_n\\}$ represents the state derivatives (rows of A). An edge $(x_j, \\dot{x}_i)$ exists if the entry $A_{ij}$ is non-zero (i.e., state $j$ influences the derivative of state $i$). Let the size of a maximum matching in this graph be $\\mu(G_A)$. By definition, $\\mu(G_A) \\le n$. The number of vertices in $V_x$ that are left unmatched by a maximum matching is $n - \\mu(G_A)$. These unmatched vertices correspond to structural deficiencies in the system's ability to propagate information.\n\nTo achieve structural observability, we must add sensors. A dedicated sensor on state $x_k$ provides a direct measurement of that state. In the graph-theoretic framework, this is equivalent to providing a \"self-loop\" or a dedicated output for $x_k$, which guarantees that $x_k$ can be matched. If we place a sensor on a state $x_k$ corresponding to an unmatched vertex from $V_x$, we can use this new \"measurement edge\" to augment the matching, increasing its size by one. By placing one dedicated sensor on each of the $n - \\mu(G_A)$ unmatched input vertices, we can ensure a full matching of size $n$. Since each sensor can resolve at most one unmatched vertex, this is the minimal number of sensors required. Thus, the minimal number of dedicated sensors is $n - \\mu(G_A)$.\n\nThis result connects to the dual problem, as mentioned in the prompt. Structural observability of the pair $(A, C)$ is equivalent to structural controllability of the dual pair $(A^T, C^T)$. The minimum number of driver nodes required for structural controllability is given by the number of unmatched state nodes in the bipartite graph of the dynamics matrix. The graph for $A^T$ has an edge from vertex $j$ to vertex $i$ if $(A^T)_{ij} \\neq 0$ (i.e., $A_{ji} \\neq 0$), which has the same edge set and thus the same maximum matching size as the graph for $A$. Therefore, the reasoning based on the matching of $G_A$ is sound.\n\nNow we apply this methodology to the given problem.\nThe system has $n=6$ states. The dynamics are constrained by a directed acyclic graph (DAG) with edges $1 \\to 2$, $1 \\to 3$, $2 \\to 4$, $3 \\to 4$, $4 \\to 5$, and $4 \\to 6$. An edge $j \\to i$ in the DAG implies that the entry $A_{ij}$ of the system matrix $A$ is non-zero. The term DAG implies no directed cycles, which includes self-loops, so the diagonal entries $A_{ii}$ are all zero. The structured matrix $A$ is:\n$$\nA = \\begin{pmatrix}\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n* & 0 & 0 & 0 & 0 & 0 \\\\\n* & 0 & 0 & 0 & 0 & 0 \\\\\n0 & * & * & 0 & 0 & 0 \\\\\n0 & 0 & 0 & * & 0 & 0 \\\\\n0 & 0 & 0 & * & 0 & 0\n\\end{pmatrix}\n$$\nwhere $*$ represents a non-zero entry.\n\nWe construct the associated bipartite graph $G_A = (V_x, V_{\\dot{x}}, E_A)$, where $V_x=\\{x_1, \\dots, x_6\\}$ and $V_{\\dot{x}}=\\{\\dot{x}_1, \\dots, \\dot{x}_6\\}$. The edges $(x_j, \\dot{x}_i)$ correspond to the non-zero entries $A_{ij}$:\n$E_A = \\{(x_1, \\dot{x}_2), (x_1, \\dot{x}_3), (x_2, \\dot{x}_4), (x_3, \\dot{x}_4), (x_4, \\dot{x}_5), (x_4, \\dot{x}_6)\\}$.\n\nOur goal is to find the size of the maximum matching, $\\mu(G_A)$. We examine the graph:\nThe input vertices $x_5$ and $x_6$ have no outgoing edges, so they cannot be part of any matching. This immediately implies that they will be among the unmatched vertices in $V_x$.\nThe input vertices $x_2$ and $x_3$ both have only one possible edge target: the output vertex $\\dot{x}_4$. In any matching, a vertex can be used at most once. Therefore, only one of the edges $(x_2, \\dot{x}_4)$ or $(x_3, \\dot{x}_4)$ can be in a matching. This means that at least one of the input vertices $\\{x_2, x_3\\}$ must be left unmatched.\n\nBased on this analysis, we have at least three unmatched vertices in $V_x$: two from $\\{x_5, x_6\\}$, and at least one from $\\{x_2, x_3\\}$.\nThe total number of unmatched vertices is at least $3$. This implies the maximum matching size is at most $n - 3 = 6 - 3 = 3$.\n\nLet's construct a matching of size $3$ to confirm this is the maximum size.\nConsider the set of edges $M = \\{(x_1, \\dot{x}_2), (x_2, \\dot{x}_4), (x_4, \\dot{x}_5)\\}$.\nThe vertices involved are $\\{x_1, x_2, x_4\\}$ from $V_x$ and $\\{\\dot{x}_2, \\dot{x}_4, \\dot{x}_5\\}$ from $V_{\\dot{x}}$. No vertex is used more than once, so this is a valid matching. Its size is $3$.\nAnother possible maximum matching is $M' = \\{(x_1, \\dot{x}_3), (x_3, \\dot{x}_4), (x_4, \\dot{x}_6)\\}$, also of size $3$.\nSince we have found a matching of size $3$, and we have argued that no matching of size $4$ or greater can exist, the size of the maximum matching is $\\mu(G_A) = 3$.\n\nThe minimal number of dedicated sensors required to achieve structural observability is given by:\n$$\nN_{sensors} = n - \\mu(G_A)\n$$\nSubstituting the values $n=6$ and $\\mu(G_A)=3$:\n$$\nN_{sensors} = 6 - 3 = 3\n$$\nTherefore, $3$ dedicated sensors are needed. For instance, placing sensors on states $x_3$, $x_5$, and $x_6$ would allow for a matching of size $6$: $\\{(x_1, \\dot{x}_2), (x_2, \\dot{x}_4), (x_4, \\dot{x}_5), (x_3, y_3), (x_5, y_5), (x_6, y_6)\\}$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Network topology is a primary determinant of observability, and a common feature in complex systems is modularity or the presence of disconnected components. This practice explores a canonical example of how such a structure directly impacts our ability to observe the full system state, even for simple linear dynamics like diffusion . By analyzing a system with a sensor placed on only one of two disconnected components, you will see precisely how the unmeasured part of the network constitutes a fundamental unobservable subspace.",
            "id": "4293879",
            "problem": "Consider a linear diffusion process on an undirected, unweighted network with two disconnected components. Let the state vector be $x(t) \\in \\mathbb{R}^{5}$ and the dynamics be governed by the linear time-invariant (LTI) system $\\dot{x}(t) = A x(t)$ with $A = -L$, where $L$ is the graph Laplacian. The first component is a $2$-node path graph with Laplacian\n$$\nL_{1} = \\begin{pmatrix}\n1 & -1 \\\\\n-1 & 1\n\\end{pmatrix},\n$$\nand the second component is a $3$-node complete graph (triangle) with Laplacian\n$$\nL_{2} = \\begin{pmatrix}\n2 & -1 & -1 \\\\\n-1 & 2 & -1 \\\\\n-1 & -1 & 2\n\\end{pmatrix}.\n$$\nThe overall Laplacian is block diagonal,\n$$\nL = \\begin{pmatrix}\nL_{1} & 0 \\\\\n0 & L_{2}\n\\end{pmatrix},\n$$\nso that $A = -L$ is also block diagonal with blocks $A_{1} = -L_{1}$ and $A_{2} = -L_{2}$. Suppose the output is a single sensor measuring only the state of the first node in the $2$-node component, i.e.,\n$$\ny(t) = C x(t), \\quad C = \\begin{pmatrix}1 & 0 & 0 & 0 & 0\\end{pmatrix}.\n$$\nStarting from the definitions of diffusion on graphs and observability in linear systems, and without invoking any pre-specified shortcut formulas, determine the dimension of the unobservable subspace of the pair $(A, C)$. Explain how the block-diagonal structure of the Laplacian determines this dimension by isolating the influence of each component on the measured output. Provide your final answer as a single integer. No rounding is required.",
            "solution": "The unobservable subspace of a linear time-invariant system pair $(A, C)$ is the set of initial states $x(0)$ that produce a zero output $y(t) = 0$ for all $t \\geq 0$. An equivalent characterization is that the unobservable subspace is the null space of the observability matrix $\\mathcal{O}$. The dimension of this subspace is what we seek.\n\nThe state space has dimension $n=5$. The observability matrix $\\mathcal{O}$ for the pair $(A, C)$ is constructed as:\n$$\n\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ CA^2 \\\\ CA^3 \\\\ CA^4 \\end{pmatrix}\n$$\nThe dimension of the unobservable subspace is given by $n - \\text{rank}(\\mathcal{O})$.\n\nThe problem's structure, with a block-diagonal system matrix $A$ and a corresponding partitioned output matrix $C$, allows for a more insightful analysis. Let the state vector $x(t)$ be partitioned according to the network components:\n$$\nx(t) = \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix}\n$$\nwhere $x_1(t) \\in \\mathbb{R}^2$ corresponds to the states of the $2$-node path graph and $x_2(t) \\in \\mathbb{R}^3$ corresponds to the states of the $3$-node complete graph.\n\nThe system dynamics are decoupled:\n$$\n\\dot{x}_1(t) = A_1 x_1(t) \\quad \\text{where} \\quad A_1 = -L_1 = \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}\n$$\n$$\n\\dot{x}_2(t) = A_2 x_2(t) \\quad \\text{where} \\quad A_2 = -L_2 = \\begin{pmatrix} -2 & 1 & 1 \\\\ 1 & -2 & 1 \\\\ 1 & 1 & -2 \\end{pmatrix}\n$$\nThe output equation can also be written in terms of this partition. Let $C = \\begin{pmatrix} C_1 & C_2 \\end{pmatrix}$, where $C_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and $C_2 = \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}$. The output is:\n$$\ny(t) = C x(t) = \\begin{pmatrix} C_1 & C_2 \\end{pmatrix} \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix} = C_1 x_1(t) + C_2 x_2(t) = C_1 x_1(t)\n$$\nThe output $y(t)$ depends exclusively on the state of the first component, $x_1(t)$. The state of the second component, $x_2(t)$, has no direct or indirect influence on the measurement.\n\nLet us formally analyze the unobservable states $x(0) = \\begin{pmatrix} x_1(0) \\\\ x_2(0) \\end{pmatrix}$. A state is unobservable if the output $y(t) = C e^{At} x(0)$ is identically zero for all $t \\ge 0$.\nThe matrix exponential $e^{At}$ is also block-diagonal due to the block structure of $A$:\n$$\ne^{At} = \\begin{pmatrix} e^{A_1 t} & 0 \\\\ 0 & e^{A_2 t} \\end{pmatrix}\n$$\nThe condition for unobservability is:\n$$\ny(t) = \\begin{pmatrix} C_1 & C_2 \\end{pmatrix} \\begin{pmatrix} e^{A_1 t} & 0 \\\\ 0 & e^{A_2 t} \\end{pmatrix} \\begin{pmatrix} x_1(0) \\\\ x_2(0) \\end{pmatrix} = C_1 e^{A_1 t} x_1(0) + C_2 e^{A_2 t} x_2(0) = 0\n$$\nSince $C_2$ is the zero matrix, this simplifies to:\n$$\nC_1 e^{A_1 t} x_1(0) = 0 \\quad \\text{for all } t \\ge 0\n$$\nThis is precisely the condition for $x_1(0)$ to be an unobservable state for the subsystem $(A_1, C_1)$. The condition places no constraint on $x_2(0)$. Any initial state $x_2(0) \\in \\mathbb{R}^3$ for the second component, when paired with an unobservable state $x_1(0)$ for the first, will result in an unobservable state for the overall system.\n\nThe unobservable subspace of the full system, $\\mathcal{U}$, is therefore the direct sum of the unobservable subspace of the first component, $\\mathcal{U}_1$, and the entire state space of the second component, $\\mathbb{R}^3$:\n$$\n\\mathcal{U} = \\mathcal{U}_1 \\oplus \\mathbb{R}^3\n$$\nThe dimension is $\\text{dim}(\\mathcal{U}) = \\text{dim}(\\mathcal{U}_1) + \\text{dim}(\\mathbb{R}^3) = \\text{dim}(\\mathcal{U}_1) + 3$.\n\nWe now determine $\\text{dim}(\\mathcal{U}_1)$ by analyzing the subsystem $(A_1, C_1)$. The state dimension is $n_1=2$. The observability matrix is:\n$$\n\\mathcal{O}_1 = \\begin{pmatrix} C_1 \\\\ C_1 A_1 \\end{pmatrix}\n$$\nWe have $C_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and $A_1 = \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}$.\nThe second row is:\n$$\nC_1 A_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} -1 & 1 \\end{pmatrix}\n$$\nSo, the observability matrix for the first subsystem is:\n$$\n\\mathcal{O}_1 = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\end{pmatrix}\n$$\nThe rank of $\\mathcal{O}_1$ can be found by computing its determinant: $\\det(\\mathcal{O}_1) = (1)(1) - (0)(-1) = 1 \\ne 0$. Since the determinant is non-zero, the matrix has full rank, $\\text{rank}(\\mathcal{O}_1) = 2$.\nThe dimension of the unobservable subspace for the first component is:\n$$\n\\text{dim}(\\mathcal{U}_1) = n_1 - \\text{rank}(\\mathcal{O}_1) = 2 - 2 = 0\n$$\nThis means that the first subsystem is fully observable, and its unobservable subspace is the trivial subspace $\\{\\mathbf{0}\\}$.\n\nSubstituting this result back into the expression for the dimension of the total unobservable subspace:\n$$\n\\text{dim}(\\mathcal{U}) = \\text{dim}(\\mathcal{U}_1) + 3 = 0 + 3 = 3\n$$\n\nThis result can be verified by constructing the full observability matrix $\\mathcal{O}$ for the $5 \\times 5$ system. The rows of $\\mathcal{O}$ are $CA^k$ for $k=0, \\dots, 4$.\nSince $A = \\begin{pmatrix} A_1 & 0 \\\\ 0 & A_2 \\end{pmatrix}$ and $C = \\begin{pmatrix} C_1 & 0 \\end{pmatrix}$, we have $CA^k = \\begin{pmatrix} C_1 A_1^k & 0 \\end{pmatrix}$.\nThe rows of $\\mathcal{O}$ are therefore:\n$C = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix}$\n$CA = \\begin{pmatrix} -1 & 1 & 0 & 0 & 0 \\end{pmatrix}$\n$A_1^2 = \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}^2 = \\begin{pmatrix} 2 & -2 \\\\ -2 & 2 \\end{pmatrix} = -2 A_1$. Thus, $CA^2 = -2(CA) = \\begin{pmatrix} 2 & -2 & 0 & 0 & 0 \\end{pmatrix}$. All subsequent rows $CA^k$ will be scalar multiples of $CA$ and thus linearly dependent.\nThe observability matrix is:\n$$\n\\mathcal{O} = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ -1 & 1 & 0 & 0 & 0 \\\\ 2 & -2 & 0 & 0 & 0 \\\\ -4 & 4 & 0 & 0 & 0 \\\\ 8 & -8 & 0 & 0 & 0 \\end{pmatrix}\n$$\nThe first two rows are linearly independent. All other rows are scalar multiples of the second row. Thus, the rank of $\\mathcal{O}$ is $2$.\nThe dimension of the unobservable subspace is $n - \\text{rank}(\\mathcal{O}) = 5 - 2 = 3$. The subspace consists of vectors of the form $(0, 0, v_3, v_4, v_5)^T$, which corresponds exactly to the state space of the unmeasured second component.\nThe dimension of the unobservable subspace is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "In the modern era of data science, a key challenge is to infer the governing laws of a system purely from measurement data, a task for which methods like Dynamic Mode Decomposition (DMD) are powerful tools. These methods, however, have fundamental limits imposed by the principles of observability. This practice challenges you to build a data-driven discovery algorithm and demonstrate through simulation that it can only recover the dynamic modes that are \"visible\" to the sensors, providing a concrete illustration of why observability is a critical prerequisite for system identification .",
            "id": "4293880",
            "problem": "Consider a discrete-time, linear, time-invariant networked dynamical system with partial measurements. The state dynamics are given by $x_{t+1} = A x_t$, where $A \\in \\mathbb{R}^{n \\times n}$, and the outputs are $y_t = C x_t$, where $C \\in \\mathbb{R}^{p \\times n}$. You will design a data-driven experiment that uses outputs $y_t$ to estimate the leading eigenvalues (by magnitude) of the unknown matrix $A$, and then demonstrate, by construction and proof, that consistent estimation of these eigenvalues from data requires the pair $(A,C)$ to be observable. The experiment must follow a principled subspace identification approach compatible with the Koopman operator perspective and Dynamic Mode Decomposition (DMD), specifically: construct block-Hankel matrices of outputs, perform a low-rank factorization to approximate a minimal realization, and use time-shift invariance of the stacked outputs to recover an estimator for $A$. Your solution must not assume access to the state $x_t$ nor to any inputs; only outputs $y_t$ are available. The algorithm should estimate an $A$-like operator whose eigenvalues approximate those of the true $A$ on the observable subspace.\n\nStart from the following fundamental bases: definitions of discrete-time linear systems, the Kalman observability concept for $(A,C)$, the notion that the Koopman operator on linear systems coincides with the state-transition operator, and the fact that, under observability, finite output sequences encode the state evolution. Avoid introducing any shortcut formulas that oversimplify the derivation.\n\nDefine the leading eigenvalues as the eigenvalues of $A$ with the largest absolute value. Given a tolerance parameter $\\tau$, declare success for a test case if the $k$ largest-by-magnitude eigenvalues of $A$ can be matched one-to-one to the eigenvalues estimated by your subspace method with absolute difference below $\\tau$. If the estimated model order is less than $k$, or any of the top $k$ true eigenvalues are not matched to within $\\tau$, declare failure.\n\nYou must implement a program that for each test case:\n- Generates outputs $y_t$ of length $T$ from $x_{t+1} = A x_t$, $y_t = C x_t$, with a specified initial condition $x_0$.\n- Builds a past block-Hankel matrix of outputs with a specified window length $L$.\n- Performs a low-rank factorization consistent with a minimal realization using the past block-Hankel.\n- Uses time-shift invariance of stacked outputs to estimate an $A$-like operator and extracts its eigenvalues.\n- Compares the estimated eigenvalues to the $k$ leading eigenvalues of the ground-truth $A$ using tolerance $\\tau$, yielding a boolean success value.\n\nYou must also include a proof that data-driven consistent recovery of the leading eigenvalues of $A$ requires $(A,C)$ to be observable in the sense of Kalman, and connect this requirement to the Koopman/Dynamic Mode Decomposition perspective for linear systems.\n\nUse the following test suite with fixed parameters. Each test case is specified by $(A,C,x_0,T,L,k,\\tau,\\sigma)$, where $\\sigma$ is the standard deviation of additive output noise and $k$ is the number of leading eigenvalues to be tested. All numbers are dimensionless.\n\n- Test Case $1$ (observable, partial measurements, noise-free):\n  - $n = 3$, $p = 2$,\n  - $\n    A = \\begin{bmatrix}\n    0.92 & 0.10 & 0.00 \\\\\n    0.00 & 0.85 & 0.07 \\\\\n    0.00 & 0.00 & 0.70\n    \\end{bmatrix},\\quad\n    C = \\begin{bmatrix}\n    1.00 & 0.00 & 1.00 \\\\\n    0.00 & 1.00 & 0.50\n    \\end{bmatrix}\n  $,\n  - $x_0 = \\begin{bmatrix} 1.00 \\\\ -0.50 \\\\ 0.70 \\end{bmatrix}$,\n  - $T = 200$, $L = 3$, $k = 2$, $\\tau = 10^{-2}$, $\\sigma = 0.00$.\n\n- Test Case $2$ (unobservable, single-output, noise-free):\n  - $n = 3$, $p = 1$,\n  - $\n    A = \\begin{bmatrix}\n    0.92 & 0.10 & 0.00 \\\\\n    0.00 & 0.85 & 0.07 \\\\\n    0.00 & 0.00 & 0.70\n    \\end{bmatrix},\\quad\n    C = \\begin{bmatrix}\n    0.00 & 1.00 & 0.00\n    \\end{bmatrix}\n  $,\n  - $x_0 = \\begin{bmatrix} 0.90 \\\\ 1.20 \\\\ -0.60 \\end{bmatrix}$,\n  - $T = 200$, $L = 3$, $k = 2$, $\\tau = 10^{-2}$, $\\sigma = 0.00$.\n\n- Test Case $3$ (observable, partial measurements, small noise):\n  - $n = 4$, $p = 2$,\n  - $\n    A = \\begin{bmatrix}\n    0.95 & 0.02 & 0.00 & 0.00 \\\\\n    0.00 & 0.90 & 0.01 & 0.00 \\\\\n    0.00 & 0.00 & 0.80 & 0.03 \\\\\n    0.00 & 0.00 & 0.00 & 0.60\n    \\end{bmatrix},\\quad\n    C = \\begin{bmatrix}\n    1.00 & 0.00 & 1.00 & 0.00 \\\\\n    0.00 & 1.00 & 0.00 & 1.00\n    \\end{bmatrix}\n  $,\n  - $x_0 = \\begin{bmatrix} 1.00 \\\\ -0.25 \\\\ 0.50 \\\\ 0.10 \\end{bmatrix}$,\n  - $T = 400$, $L = 4$, $k = 2$, $\\tau = 10^{-2}$, $\\sigma = 10^{-4}$.\n\n- Test Case $4$ (observable, repeated leading eigenvalue, noise-free):\n  - $n = 3$, $p = 2$,\n  - $\n    A = \\begin{bmatrix}\n    0.90 & 0.02 & 0.00 \\\\\n    0.00 & 0.90 & 0.00 \\\\\n    0.00 & 0.00 & 0.50\n    \\end{bmatrix},\\quad\n    C = \\begin{bmatrix}\n    1.00 & 0.00 & 0.00 \\\\\n    0.00 & 1.00 & 1.00\n    \\end{bmatrix}\n  $,\n  - $x_0 = \\begin{bmatrix} 1.00 \\\\ 0.10 \\\\ -0.30 \\end{bmatrix}$,\n  - $T = 300$, $L = 4$, $k = 2$, $\\tau = 10^{-2}$, $\\sigma = 0.00$.\n\nYour program must:\n- Use only the specified matrices and parameters to generate outputs $y_t$ for $t = 0,1,\\dots,T-1$.\n- Construct the past block-Hankel matrix of outputs with window length $L$.\n- Estimate an $A$-like operator from outputs alone using a subspace method consistent with the above description.\n- Compute its eigenvalues and test recovery of the $k$ leading eigenvalues of the ground-truth $A$ within tolerance $\\tau$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is a boolean indicating success ($\\mathrm{True}$) or failure ($\\mathrm{False}$) for Test Case $i$.\n\nNo physical units or angles are involved; all quantities are dimensionless. The algorithm should be robust to small measurement noise in the specified test case and handle the case where $(A,C)$ is not observable by returning failure for recovery of the leading eigenvalues.",
            "solution": "The problem requires a two-part response: a formal proof concerning the necessity of observability for eigenvalue estimation and an algorithmic implementation of a subspace identification method to perform this estimation.\n\n### Part 1: Proof of Necessity of Observability\n\nWe begin by establishing the theoretical foundation. The system is a discrete-time, linear time-invariant (LTI) system described by:\n$$\nx_{t+1} = A x_t\n$$\n$$\ny_t = C x_t\n$$\nwhere $x_t \\in \\mathbb{R}^{n}$ is the state, $y_t \\in \\mathbb{R}^{p}$ is the output, $A \\in \\mathbb{R}^{n \\times n}$ is the state-transition matrix, and $C \\in \\mathbb{R}^{p \\times n}$ is the output matrix. The core task is to estimate the leading eigenvalues of $A$ using only the sequence of outputs $\\{y_0, y_1, y_2, \\dots, y_{T-1}\\}$.\n\nThe subspace identification method proposed relies on constructing block-Hankel matrices from the output data. Let us define a vector of stacked outputs over a window of length $L$:\n$$\nY_t = \\begin{bmatrix} y_t \\\\ y_{t+1} \\\\ \\vdots \\\\ y_{t+L-1} \\end{bmatrix} \\in \\mathbb{R}^{pL}\n$$\nUsing the system equations, we can relate this stacked vector to the state $x_t$:\n$$\nY_t = \\begin{bmatrix} C x_t \\\\ C x_{t+1} \\\\ \\vdots \\\\ C x_{t+L-1} \\end{bmatrix} = \\begin{bmatrix} C \\\\ CA \\\\ \\vdots \\\\ CA^{L-1} \\end{bmatrix} x_t\n$$\nLet us define the extended observability matrix of order $L$ as $\\mathcal{O}_L = \\begin{bmatrix} C^T & (CA)^T & \\dots & (CA^{L-1})^T \\end{bmatrix}^T \\in \\mathbb{R}^{pL \\times n}$. Then, the relationship is compactly written as:\n$$\nY_t = \\mathcal{O}_L x_t\n$$\nThe data-driven experiment uses a sequence of these stacked vectors to form a block-Hankel data matrix. Let us define two such matrices, one representing the \"past\" and one the \"future\":\n$$\n\\mathcal{H}_p = \\begin{bmatrix} Y_0 & Y_1 & \\dots & Y_{N-1} \\end{bmatrix} = \\begin{bmatrix} y_0 & y_1 & \\dots & y_{N-1} \\\\ y_1 & y_2 & \\dots & y_N \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_{L-1} & y_L & \\dots & y_{L+N-2} \\end{bmatrix}\n$$\n$$\n\\mathcal{H}_f = \\begin{bmatrix} Y_1 & Y_2 & \\dots & Y_N \\end{bmatrix} = \\begin{bmatrix} y_1 & y_2 & \\dots & y_N \\\\ y_2 & y_3 & \\dots & y_{N+1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_L & y_{L+1} & \\dots & y_{L+N-1} \\end{bmatrix}\n$$\nwhere $N$ is the number of columns, limited by the total data length $T$. Using the relation $Y_t = \\mathcal{O}_L x_t$, we can factor these Hankel matrices:\n$$\n\\mathcal{H}_p = \\mathcal{O}_L \\begin{bmatrix} x_0 & x_1 & \\dots & x_{N-1} \\end{bmatrix} = \\mathcal{O}_L X_p\n$$\n$$\n\\mathcal{H}_f = \\mathcal{O}_L \\begin{bmatrix} x_1 & x_2 & \\dots & x_N \\end{bmatrix} = \\mathcal{O}_L X_f\n$$\nThe dynamics $x_{t+1} = A x_t$ imply $X_f = A X_p$. Substituting this into the equation for $\\mathcal{H}_f$ yields:\n$$\n\\mathcal{H}_f = \\mathcal{O}_L A X_p\n$$\nThe key insight of subspace methods is to find an operator that maps $\\mathcal{H}_p$ to $\\mathcal{H}_f$. All information available to the algorithm is contained within $\\mathcal{H}_p$ and $\\mathcal{H}_f$. The column space of these matrices is a subset of the range of the extended observability matrix, $\\mathrm{range}(\\mathcal{O}_L)$. The dimension of this space is crucial. Assuming the state sequence $X_p$ is sufficiently rich (i.e., it has full row rank $n$), the rank of the data matrix $\\mathcal{H}_p$ is given by:\n$$\n\\mathrm{rank}(\\mathcal{H}_p) = \\mathrm{rank}(\\mathcal{O}_L X_p) = \\mathrm{rank}(\\mathcal{O}_L)\n$$\nThe Kalman observability condition states that the pair $(A, C)$ is observable if the standard observability matrix $\\mathcal{O} = \\mathcal{O}_n = \\begin{bmatrix} C^T & (CA)^T & \\dots & (CA^{n-1})^T \\end{bmatrix}^T$ has full column rank, i.e., $\\mathrm{rank}(\\mathcal{O}) = n$. By the Cayley-Hamilton theorem, any power $A^k$ for $k \\ge n$ is a linear combination of lower powers of $A$. This implies that $\\mathrm{rank}(\\mathcal{O}_L) = \\mathrm{rank}(\\mathcal{O})$ for any $L \\ge n$.\n\nLet's consider the two cases:\nCase 1: The system $(A, C)$ is observable.\nIn this case, $\\mathrm{rank}(\\mathcal{O}_L) = n$ (for $L \\ge n$). The matrix $\\mathcal{O}_L$ has a left inverse, $\\mathcal{O}_L^\\dagger = (\\mathcal{O}_L^T \\mathcal{O}_L)^{-1}\\mathcal{O}_L^T$. From $\\mathcal{H}_p = \\mathcal{O}_L X_p$, one can (in principle) recover the state sequence: $X_p = \\mathcal{O}_L^\\dagger \\mathcal{H}_p$. Substituting this into the expression for $\\mathcal{H}_f$:\n$$\n\\mathcal{H}_f = (\\mathcal{O}_L A \\mathcal{O}_L^\\dagger) \\mathcal{H}_p\n$$\nThe operator $\\mathcal{K} = \\mathcal{O}_L A \\mathcal{O}_L^\\dagger$ is a representation of the dynamics in the space of stacked outputs. A subspace identification algorithm, like Dynamic Mode Decomposition (DMD), finds an approximation of a low-rank version of this operator. Specifically, it computes a reduced-order model operator $\\hat{A}_r$ via SVD of $\\mathcal{H}_p = U S V^T$. For an observable system, the effective rank is $r=n$. The estimated operator is $\\hat{A}_n = U_n^T \\mathcal{H}_f V_n S_n^{-1}$, which is an $n \\times n$ matrix. It can be shown that this operator is related to $A$ by a similarity transformation, $\\hat{A}_n \\sim A$. Therefore, the eigenvalues of $\\hat{A}_n$ are identical to the eigenvalues of $A$. Data-driven estimation is successful.\n\nCase 2: The system $(A, C)$ is unobservable.\nIn this case, $\\mathrm{rank}(\\mathcal{O}) < n$. The observability matrix has a non-trivial null space, $\\mathrm{ker}(\\mathcal{O}) \\neq \\{0\\}$. This is the unobservable subspace. Let $v$ be any vector in this subspace. By definition, $\\mathcal{O}v = 0$, which implies $CA^i v = 0$ for all $i \\ge 0$.\nNow, consider an initial state $x_0$ that has a component in the unobservable subspace. We can decompose $x_0$ as $x_0 = x_{obs} + x_{unobs}$, where $x_{obs}$ is in the range of $\\mathcal{O}^T$ (the observable subspace) and $x_{unobs} \\in \\mathrm{ker}(\\mathcal{O})$. The state at time $t$ is $x_t = A^t x_0 = A^t x_{obs} + A^t x_{unobs}$. The output is:\n$$\ny_t = C x_t = C A^t x_{obs} + C A^t x_{unobs}\n$$\nSince the unobservable subspace is $A$-invariant, $A^t x_{unobs}$ is also in the unobservable subspace. Therefore, $C(A^t x_{unobs}) = 0$. The output becomes:\n$$\ny_t = C A^t x_{obs}\n$$\nThe output sequence $\\{y_t\\}$ is completely independent of the unobservable component of the initial state, $x_{unobs}$, and its subsequent evolution. The data matrices $\\mathcal{H}_p$ and $\\mathcal{H}_f$ contain no information about the dynamics within the unobservable subspace. The rank of the data matrix will be $r = \\mathrm{rank}(\\mathcal{O}_L) < n$. The subspace identification algorithm will thus identify a reduced-order model of dimension $r < n$. The operator $\\hat{A}_r$ will be an $r \\times r$ matrix whose eigenvalues can only approximate the eigenvalues of $A$ corresponding to the dynamics projected onto the observable subspace. Any eigenvalues of $A$ associated with eigenvectors or generalized eigenvectors lying within the unobservable subspace are \"invisible\" to the output and cannot be recovered.\n\nThis corresponds directly to the Koopman operator perspective. The Koopman operator for this linear system has the same spectrum as $A$. A data-driven method like DMD attempts to approximate the Koopman operator's spectral properties from data. The data consists of measurements of a set of \"observable functions.\" In this problem, the observable functions are the linear projections defined by the rows of $C$. If the system is unobservable, then the span of these observable functions and their time-evolutions under the Koopman operator does not cover the full state space. The resulting approximation of the Koopman operator is restricted to the observable subspace, and its spectrum will only contain the observable eigenvalues of the system.\n\nTherefore, we conclude that the observability of the pair $(A, C)$ is a necessary condition for the consistent estimation of all eigenvalues of $A$ from output data. If a leading eigenvalue corresponds to an unobservable mode, it cannot be estimated.\n\n### Part 2: Algorithmic Design and Implementation\n\nThe algorithm implements the subspace identification procedure described in the proof.\n\n1.  **Data Generation**: For each test case, generate a time series of outputs $y_t$ for $t \\in [0, T-1]$ by simulating $x_{t+1} = Ax_t$ starting from $x_0$, computing $y_t = Cx_t$, and adding Gaussian noise with standard deviation $\\sigma$ if specified.\n\n2.  **Block-Hankel Matrix Construction**: From the output time series $\\{y_t\\}_{t=0}^{T-1}$, construct two block-Hankel matrices, $\\mathcal{H}_p$ and $\\mathcal{H}_f$, each with a block-window size of $L$. The number of columns in each matrix is $N = T-L$.\n    $$\n    \\mathcal{H}_p = \\begin{bmatrix} y_0 & y_1 & \\dots & y_{T-L-1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_{L-1} & y_{L} & \\dots & y_{T-2} \\end{bmatrix}, \\quad\n    \\mathcal{H}_f = \\begin{bmatrix} y_1 & y_2 & \\dots & y_{T-L} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_{L} & y_{L+1} & \\dots & y_{T-1} \\end{bmatrix}\n    $$\n    These matrices have dimensions $(p \\cdot L) \\times (T-L)$.\n\n3.  **Low-Rank Factorization via SVD**: Perform a Singular Value Decomposition (SVD) on the \"past\" data matrix $\\mathcal{H}_p$:\n    $$\n    \\mathcal{H}_p = U S V^T\n    $$\n    The rank $r$ of the underlying observable system is determined by the number of significant singular values in the diagonal matrix $S$. For noise-free cases, this is the number of non-zero singular values. For the noisy case, we truncate at the rank of the true underlying system, which corresponds to finding the \"knee\" in the singular value plot. In this implementation, a practical threshold is used to determine $r$. Let the truncated matrices be $U_r \\in \\mathbb{R}^{pL \\times r}$, $S_r \\in \\mathbb{R}^{r \\times r}$, and $V_r \\in \\mathbb{R}^{N \\times r}$.\n\n4.  **Estimation of the Reduced-Order Operator**: The low-dimensional system dynamics are captured by an $r \\times r$ matrix $\\hat{A}_r$, which approximates the projection of $A$ onto the observable subspace. This is computed using the standard DMD formula:\n    $$\n    \\hat{A}_r = U_r^T \\mathcal{H}_f V_r S_r^{-1}\n    $$\n\n5.  **Eigenvalue Comparison**:\n    -   Compute the eigenvalues of $\\hat{A}_r$, let this set be $\\{\\hat{\\lambda}_j\\}_{j=1}^r$.\n    -   Compute the eigenvalues of the true matrix $A$, let this set be $\\{\\lambda_i\\}_{i=1}^n$.\n    -   Sort the true eigenvalues by magnitude in descending order and select the top $k$: $\\{\\lambda_1^*, \\dots, \\lambda_k^*\\}$.\n    -   The test is declared a success if a one-to-one matching can be found between the $k$ leading true eigenvalues and $k$ of the estimated eigenvalues such that the absolute difference for each pair is less than the tolerance $\\tau$. This is performed using the Hungarian algorithm (`linear_sum_assignment`) on a cost matrix of absolute differences. The test fails if the estimated model order $r$ is less than $k$, or if such a matching is not found.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: observable, partial measurements, noise-free\n        {\n            \"A\": np.array([[0.92, 0.10, 0.00], [0.00, 0.85, 0.07], [0.00, 0.00, 0.70]]),\n            \"C\": np.array([[1.00, 0.00, 1.00], [0.00, 1.00, 0.50]]),\n            \"x0\": np.array([1.00, -0.50, 0.70]),\n            \"T\": 200, \"L\": 3, \"k\": 2, \"tau\": 1e-2, \"sigma\": 0.00\n        },\n        # Test Case 2: unobservable, single-output, noise-free\n        {\n            \"A\": np.array([[0.92, 0.10, 0.00], [0.00, 0.85, 0.07], [0.00, 0.00, 0.70]]),\n            \"C\": np.array([[0.00, 1.00, 0.00]]),\n            \"x0\": np.array([0.90, 1.20, -0.60]),\n            \"T\": 200, \"L\": 3, \"k\": 2, \"tau\": 1e-2, \"sigma\": 0.00\n        },\n        # Test Case 3: observable, partial measurements, small noise\n        {\n            \"A\": np.array([[0.95, 0.02, 0.00, 0.00], [0.00, 0.90, 0.01, 0.00], [0.00, 0.00, 0.80, 0.03], [0.00, 0.00, 0.00, 0.60]]),\n            \"C\": np.array([[1.00, 0.00, 1.00, 0.00], [0.00, 1.00, 0.00, 1.00]]),\n            \"x0\": np.array([1.00, -0.25, 0.50, 0.10]),\n            \"T\": 400, \"L\": 4, \"k\": 2, \"tau\": 1e-2, \"sigma\": 1e-4\n        },\n        # Test Case 4: observable, repeated leading eigenvalue, noise-free\n        {\n            \"A\": np.array([[0.90, 0.02, 0.00], [0.00, 0.90, 0.00], [0.00, 0.00, 0.50]]),\n            \"C\": np.array([[1.00, 0.00, 0.00], [0.00, 1.00, 1.00]]),\n            \"x0\": np.array([1.00, 0.10, -0.30]),\n            \"T\": 300, \"L\": 4, \"k\": 2, \"tau\": 1e-2, \"sigma\": 0.00\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_test_case(**params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_test_case(A, C, x0, T, L, k, tau, sigma):\n    \"\"\"\n    Executes the subspace identification experiment for a single test case.\n    \"\"\"\n    n, p = A.shape[0], C.shape[0]\n\n    # 1. Generate output data\n    x_data = np.zeros((n, T))\n    y_data = np.zeros((p, T))\n    x_data[:, 0] = x0\n    y_data[:, 0] = C @ x0\n\n    # Initialize a random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n\n    for t in range(T - 1):\n        x_data[:, t+1] = A @ x_data[:, t]\n        y_data[:, t+1] = C @ x_data[:, t+1]\n\n    if sigma > 0.0:\n        noise = rng.normal(0, sigma, size=y_data.shape)\n        y_data += noise\n\n    # 2. Construct block-Hankel matrices\n    num_cols_hankel = T - L\n    H_p = np.zeros((p * L, num_cols_hankel))\n    H_f = np.zeros((p * L, num_cols_hankel))\n\n    for j in range(num_cols_hankel):\n        H_p[:, j] = y_data[:, j:j+L].flatten()\n        H_f[:, j] = y_data[:, j+1:j+L+1].flatten()\n\n    # 3. Low-rank factorization via SVD\n    try:\n        U, s, Vh = np.linalg.svd(H_p, full_matrices=False)\n    except np.linalg.LinAlgError:\n        return False # SVD failed, cannot proceed\n\n    # Determine model order r\n    # For noise-free cases, this is the number of singular values > tolerance\n    # For noisy cases, a relative threshold is used for robustness.\n    rank_tol = 1e-8 if sigma == 0 else s[0] * 1e-3\n    r = np.sum(s > rank_tol)\n\n    # Truncate according to rank r\n    U_r = U[:, :r]\n    S_r = np.diag(s[:r])\n    V_r = Vh[:r, :].T\n\n    # 4. Estimate reduced-order operator\n    # A_hat = U_r^T @ H_f @ V_r @ S_r^{-1}\n    S_r_inv = np.linalg.inv(S_r)\n    A_hat_r = U_r.T @ H_f @ V_r @ S_r_inv\n\n    # 5. Eigenvalue comparison\n    # Get ground-truth leading eigenvalues\n    true_eigs = np.linalg.eigvals(A)\n    sorted_true_eigs = sorted(true_eigs, key=np.abs, reverse=True)\n    top_k_true_eigs = sorted_true_eigs[:k]\n\n    # Check for failure condition: model order less than k\n    if r  k:\n        return False\n\n    # Get estimated eigenvalues\n    est_eigs = np.linalg.eigvals(A_hat_r)\n\n    # Find best one-to-one matching using the Hungarian algorithm\n    cost_matrix = np.abs(np.array([top_k_true_eigs]).T - np.array([est_eigs]))\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    \n    # Check if all k leading eigenvalues are matched within tolerance\n    if len(row_ind)  k:\n        # Not enough matched pairs to satisfy the condition\n        return False\n        \n    num_matched = 0\n    for i in range(len(row_ind)):\n        if cost_matrix[row_ind[i], col_ind[i]]  tau:\n            num_matched += 1\n\n    return num_matched == k\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}