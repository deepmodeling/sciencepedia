## 引言
[图神经网络](@entry_id:136853)（GNNs）已经成为处理和分析[结构化数据](@entry_id:914605)的革命性工具，从社交[网络分析](@entry_id:139553)到药物发现，其应用无处不在。然而，要真正驾驭GNN的强大能力，必须深入理解其核心工作机制——消息传递（Message Passing）范式。这一优雅的框架统一了绝大多数现代GNN架构，但其理论基础、[表达能力](@entry_id:149863)的边界以及在实践中遇到的挑战，往往是初学者和研究人员需要攻克的难点。本文旨在系统性地剖析[消息传递范式](@entry_id:635682)，填补从理论到实践的认知鸿沟。

在接下来的内容中，我们将分三个章节展开一次全面的探索之旅。首先，在“原理与机制”章节中，我们将建立[消息传递](@entry_id:751915)的通用框架，探讨其必须遵循的置换[等变性](@entry_id:636671)原则，并揭示其与[Weisfeiler-Lehman测试](@entry_id:1134117)的深刻联系，从而理解其表达能力的理论上限以及如过平滑、过挤压等内在挑战。接着，在“应用与跨学科连接”章节中，我们将展示[消息传递](@entry_id:751915)如何作为一种[通用计算](@entry_id:275847)工具，模拟物理和概率过程，并在[计算化学](@entry_id:143039)、生物信息学和神经科学等前沿领域解决实际问题。最后，在“动手实践”部分，你将通过一系列精心设计的计算问题，亲手实现和分析[消息传递](@entry_id:751915)的关键环节，将理论知识转化为实践技能。

## 原理与机制

在“引言”章节中，我们初步探讨了[图神经网络](@entry_id:136853)（GNNs）在处理[结构化数据](@entry_id:914605)方面的重要性。本章将深入其核心，系统地阐述驱动大多数现代GNN架构的“消息传递”范式的基本原理与核心机制。我们将从一个通用的形式化框架出发，探讨其必须满足的[基本对称性](@entry_id:161256)原则，分析其[表达能力](@entry_id:149863)的理论边界，并剖析在构建深度GNN时可能遇到的关键挑战。

### 消息传递的通用框架

[图神经网络](@entry_id:136853)的核心思想是迭代地聚合邻域信息来更新节点的表示（representation）或状态（state）。这个过程可以被抽象为一个统一的**[消息传递](@entry_id:751915)（message passing）**框架。给定一个图 $G=(V,E)$，其中每个节点 $v \in V$ 在第 $t$ 层（或时间步）有一个[特征向量](@entry_id:151813) $h_v^{(t)}$，每条边 $(u,v) \in E$ 可能有关联的特征 $e_{uv}$。从第 $t$ 层到第 $t+1$ 层的更新过程可以分解为三个核心组件：**消息函数（message function）** $\psi$、**聚合函数（aggregation function）** $\square$ 和**[更新函数](@entry_id:275392)（update function）** $\phi$。

该过程的通用范式可以表述为：
$$
h_v^{(t+1)} = \phi \! \left( h_v^{(t)}, \; \square_{u \in N(v)} \; \psi \! \left( h_v^{(t)}, h_u^{(t)}, e_{uv} \right) \right)
$$
其中 $N(v)$ 代表节点 $v$ 的邻居节点集合。

- **消息函数 $\psi$**：对于每个从邻居节点 $u$ 到中心节点 $v$ 的信息流，$\psi$ 负责生成一条“消息”。这条消息是基于发送方节点 $u$ 的状态 $h_u^{(t)}$、接收方节点 $v$ 的状态 $h_v^{(t)}$ 以及它们之间边的特征 $e_{uv}$ 计算得出的。这体现了交互的**局部性（locality）**原则，即信息的产生仅依赖于直接相连的元素。

- **聚合函数 $\square$**：中心节点 $v$ 会从其所有邻居接收到一个消息的多重集（multiset）。聚合函数 $\square$ 的作用是将这个可变大小的消息集合规约为一个固定大小的向量。这个聚合后的向量概括了来自节点 $v$ 邻域的全部信息。

- **[更新函数](@entry_id:275392) $\phi$**：最后，[更新函数](@entry_id:275392) $\phi$ 结合节点 $v$ 在上一层的自身状态 $h_v^{(t)}$ 和聚合后的邻域信息，计算出节点 $v$ 在新一层的新状态 $h_v^{(t+1)}$。这一步通常允许模型在保留部分自身信息的同时，融入来自邻居的新知识。

这个三段式框架极具普适性，涵盖了从[图卷积网络](@entry_id:194500)（GCN）到[图注意力网络](@entry_id:1125735)（GAT）等众多知名的GNN架构。理解这三个组件各自的角色及其必须遵循的原则，是掌握GNN设计的关键 。

### 基石原则：置换等变性

图数据的一个根本特性是节点的任意性：节点的索引或标签（如 $v_1, v_2, \dots$）通常是任意分配的，不包含任何内在信息。一个健壮的图学习模型，其输出应该与这种任意的节点标签无关。换言之，如果我们对图中的节点进行重新标记（即置换），模型的输出也应该相应地以同样的方式置换，而模型的行为本质不应改变。这个属性被称为**置换[等变性](@entry_id:636671)（permutation equivariance）**。

为了在消息传递框架中实现置换等变性，必须满足两个关键条件  ：

1.  **共享参数（Shared Parameters）**：消息函数 $\psi$ 和[更新函数](@entry_id:275392) $\phi$ 必须在图的所有节点和边上共享。也就是说，我们不为每个节点或每条边学习一个单独的函数，而是学习一个通用的 $\psi$ 和一个通用的 $\phi$。如果参数是节点特定的（例如，每个节点 $v$ 有一个自己的权重矩阵 $W_v$），那么在节点重排后，模型将无法保持一致性，因为一个节点的新标签将对应于一个完全不同的、未经训练的参数。[参数共享](@entry_id:634285)确保了GNN以一种“结构感知”而非“标签感知”的方式运作 。

2.  **置换不变的聚合（Permutation-Invariant Aggregation）**：一个节点的邻居集合 $N(v)$ 是一个**集合**，其元素没有固有的顺序。因此，聚合函数 $\square$ 必须对其输入的消息集合具有**[置换不变性](@entry_id:753356)（permutation invariance）**。这意味着，无论消息以何种顺序输入，聚合结果都应保持不变。常见的置换不变[聚合算子](@entry_id:746335)包括**求和（sum）**、**均值（mean）**和**最大值（max）**。与此相反，像拼接（concatenation）这样的操作依赖于顺序，因此通常会破坏等变性，除非施加一个不依赖于任意标签的规范排序（这在通用图上通常是不可能的）。

以**[图注意力网络](@entry_id:1125735)（Graph Attention Network, GAT）**为例，它巧妙地实现了置换[等变性](@entry_id:636671)。GAT通过一个共享的[注意力机制](@entry_id:917648)计算每条边的权重 $\alpha_{vu}$，然后进行加权求和。其更新规则可抽象为：
$$
h_v' = \sigma \! \left( \sum_{u \in N(v)} \alpha_{vu} \, W h_u \right)
$$
其中注意力系数 $\alpha_{vu}$ 通过对邻居集合 $N(v)$ 进行 softmax 归一化得到：
$$
\alpha_{vu} = \mathrm{softmax}_{u \in N(v)}(s_{vu}) = \frac{\exp(s_{vu})}{\sum_{w \in N(v)} \exp(s_{vw})}
$$
这里的求和与 softmax 操作都是对邻居集合进行的，天然地忽略了邻居的排列顺序，从而保证了聚合步骤的[置换不变性](@entry_id:753356)。只要计算注意力分数 $s_{vu}$ 的函数（通常是一个小型神经网络）和[变换矩阵](@entry_id:151616) $W$ 是在所有节点间共享的，整个GAT层就满足置换[等变性](@entry_id:636671) 。

### 从原理到实践：一个计算视角

为了更具体地理解[消息传递](@entry_id:751915)的运作方式，让我们通过一个简单的例子来追踪其前向传播和[反向传播](@entry_id:199535)（梯度计算）的过程。这个过程是所有基于梯度优化的GNN模型训练的基础 。

考虑一个包含节点 $\{1, 2, 3\}$ 的图，其中节点1的邻居是 $\{2, 3\}$。我们定义一个带有[注意力机制](@entry_id:917648)的[消息传递](@entry_id:751915)层，其目标是根据节点1的更新后的状态 $h_1'$ 计算一个损失 $L$。

- **消息函数 $\psi$**: 对每个邻居 $j \in \{2, 3\}$，我们计算一个“注意力分数” $s_{1j} = \alpha h_1 h_j$ 和一个“消息值” $v_{1j} = u h_j$。这里的 $\alpha$ 和 $u$ 是可学习的标量参数。

- **聚合函数 $\square$**: 我们使用 softmax 对注意力分数进行归一化，得到注意力权重 $w_{1j} = \frac{\exp(s_{1j})}{\sum_{k \in \{2,3\}} \exp(s_{1k})}$。聚合后的消息 $m_1$ 是消息值的加权和：$m_1 = \sum_{j \in \{2,3\}} w_{1j} v_{1j}$。

- **[更新函数](@entry_id:275392) $\phi$**: 节点1的新状态为 $h_1' = a h_1 + b m_1$，其中 $a$ 和 $b$ 也是可学习参数。

- **[损失函数](@entry_id:634569) $L$**: 假设我们有一个目标值 $y$，损失为 $L = \frac{1}{2}(h_1' - y)^2$。

**[前向传播](@entry_id:193086)**的[计算图](@entry_id:636350)如下：
1.  输入：$h_1, h_2, h_3$ 以及参数 $\alpha, u, a, b$。
2.  计算分数：$s_{12} = \alpha h_1 h_2$ 和 $s_{13} = \alpha h_1 h_3$。
3.  计算权重：$w_{12}$ 和 $w_{13}$（通过对 $s_{12}, s_{13}$ 进行 softmax）。
4.  计算消息值：$v_{12} = u h_2$ 和 $v_{13} = u h_3$。
5.  聚合消息：$m_1 = w_{12}v_{12} + w_{13}v_{13}$。
6.  更新状态：$h_1' = a h_1 + b m_1$。
7.  计算损失：$L = \frac{1}{2}(h_1' - y)^2$。

**反向传播**则利用[链式法则](@entry_id:190743)，将损失 $L$ 的梯度[反向传播](@entry_id:199535)给每个参数。例如，我们想计算损失对参数 $\alpha$ 的梯度 $\frac{\partial L}{\partial \alpha}$。根据链式法则，这可以分解为：
$$
\frac{\partial L}{\partial \alpha} = \frac{\partial L}{\partial h_1'} \frac{\partial h_1'}{\partial m_1} \frac{\partial m_1}{\partial \alpha}
$$
其中，$\frac{\partial L}{\partial h_1'} = h_1' - y$ 和 $\frac{\partial h_1'}{\partial m_1} = b$ 是直接可得的。核心在于计算 $\frac{\partial m_1}{\partial \alpha}$，这需要梯度穿过注意力权重的计算。由于 $v_{1j}$ 不依赖于 $\alpha$，我们有：
$$
\frac{\partial m_1}{\partial \alpha} = \sum_{j \in \{2,3\}} v_{1j} \frac{\partial w_{1j}}{\partial \alpha}
$$
而权重的梯度 $\frac{\partial w_{1j}}{\partial \alpha}$ 又可以通过对 softmax 函数和[分数函数](@entry_id:164520) $s_{1j}$ 应用链式法则来得到。这个过程虽然繁琐，但清晰地展示了消息传递的每个部分都是可微的，因此整个GNN模型可以通过标准的[自动微分](@entry_id:144512)工具（如PyTorch或TensorFlow）进行端到端的训练 。

### 表达能力及其上限：与[Weisfeiler-Lehman测试](@entry_id:1134117)的联系

一个自然的问题是：消息传递[GNN的表达能力](@entry_id:637052)有多强？它能区分哪些不同的图结构？为了回答这个问题，研究者们将其与经典的**[图同构](@entry_id:143072)测试（graph isomorphism test）**——**Weisfeiler-Lehman（WL）算法**——联系起来。

**1维WL算法（1-WL）**，也称为“颜色精化”（color refinement），通过一个迭代过程为图中的每个节点分配“颜色”（即标签）。在每一轮迭代中，每个节点的新颜色由其当前颜色和其邻居颜色的**多重集**唯一确定。具体来说，节点 $v$ 在第 $t+1$ 轮的颜色 $c^{(t+1)}(v)$ 通过一个[哈希函数](@entry_id:636237)（或注入映射）生成：
$$
c^{(t+1)}(v) = \text{HASH} \! \left( c^{(t)}(v), \; \{\!\{ c^{(t)}(u) : u \in N(v) \}\!\} \right)
$$
其中 $\{\!\{ \cdot \}\!\}$ 表示多重集。这个过程不断重复，直到所有节点的颜色不再改变。如果两个图在算法稳定后具有不同的颜色[直方图](@entry_id:178776)（即每种颜色的节点数量不同），那么它们一定是非同构的。

1-WL算法的更新规则与[MPN](@entry_id:910658)N的[消息传递范式](@entry_id:635682)惊人地相似。事实上，一个重要的理论结果表明：**任何标准[MPN](@entry_id:910658)N的[表达能力](@entry_id:149863)上限由1-[WL测试](@entry_id:1134117)界定** 。这意味着，如果1-WL无法区分两个图，那么任何基于[消息传递](@entry_id:751915)的GNN也无法区分它们。

一个[MPN](@entry_id:910658)N要想达到与1-WL相同的[表达能力](@entry_id:149863)，其聚合函数 $\mathcal{A}$ 和[更新函数](@entry_id:275392) $\Psi$（这里用 $\Psi$ 泛指整个更新步骤）必须是**[单射](@entry_id:183792)的（injective）**。[单射性](@entry_id:147722)意味着不同的输入必然产生不同的输出。具体来说：
- 聚合函数 $\mathcal{A}$ 必须能将不同的邻居特征多重集映射到不同的聚合表示。
- [更新函数](@entry_id:275392) $\Psi$ 必须能将不同的 (自身特征, 聚合邻域特征) 对映射到不同的新特征。

如果这两个条件都满足，[MPN](@entry_id:910658)N就能在每一步[完美模拟](@entry_id:753337)1-WL的颜色精化过程。然而，许多常用的聚合函数，如均值（mean）和最大值（max），并非[单射](@entry_id:183792)。例如，多重集 $\{1, 5\}$ 和 $\{2, 4\}$ 的均值都是3，使用均值聚合会丢失这种结构差异。求和（sum）聚合在某些条件下是[单射](@entry_id:183792)的，这使得基于求和的GNN（如Graph Isomorphism Network, GIN）具有与1-WL同等的最大表达能力 。

这个理论联系也揭示了[MPN](@entry_id:910658)N的固有局限。1-WL无法区分所有[非同构图](@entry_id:274028)。一个经典的失败案例是**[正则图](@entry_id:265877)（regular graphs）**。对于任意两个具有相同节点数和相同度数的非同构[正则图](@entry_id:265877)（例如，6个节点的环图 $C_6$ vs. 两个分离的三角形 $C_3 \cup C_3$），1-WL无法区分它们。原因在于，如果所有节点初始颜色相同，那么在每一轮迭代中，每个节点看到的邻居颜色多重集都是完全一样的（例如，在 $d$-[正则图](@entry_id:265877)中，每个节点都看到 $d$ 个相同颜色的邻居）。因此，所有节点的颜色将永远保持一致，无法产生区分性的结构信息。由于[MPN](@entry_id:910658)N的表达能力受限于1-WL，标准的[MPN](@entry_id:910658)N同样无法区分这些图结构 。

### 超越局部性：增强模型表达能力

标准[消息传递](@entry_id:751915)模型的表达能力受到1-WL的限制，并且其感知范围在每一层仅扩展一跳，这限制了其捕捉[长程依赖](@entry_id:181727)和更复杂图结构的能力。幸运的是，我们可以通过多种方式增强其表达能力。

#### 利用边特征

1-[WL测试](@entry_id:1134117)的一个直接扩展是**边标记的1-WL**，它在更新颜色时同时考虑邻居的颜色和连接边的标签。类似地，[MPN](@entry_id:910658)N也可以通过在消息函数 $\psi$ 中有效地利用边特征 $e_{uv}$ 来提升其分辨能力。

考虑一个简单的例子：两个图 $G_1$ 和 $G_2$ 都有4个节点，构成一个环。在 $G_1$ 中，所有边的特征都是 'r'；而在 $G_2$ 中，边的特征交替出现 'r' 和 'b'。如果一个[MPN](@entry_id:910658)N忽略边特征，那么它将无法区分 $G_1$ 和 $G_2$，因为从每个节点的视角看，它们都有两个邻居，局部结构完全相同。然而，一个“边感知”的[MPN](@entry_id:910658)N，其消息函数为 $\psi(h_v^{(t)}, h_u^{(t)}, e_{uv})$，将能够区分它们。在第一轮[消息传递](@entry_id:751915)后， $G_1$ 中的每个节点收到的消息多重集是 $\{\text{msg}(r), \text{msg}(r)\}$，而 $G_2$ 中的每个节点收到的是 $\{\text{msg}(r), \text{msg}(b)\}$。由于这两个多重集不同，一个具有[单射](@entry_id:183792)聚合器的[MPN](@entry_id:910658)N将为两个图中的节点生成不同的表示，从而成功区分它们 。这说明，利用边特征可以使[MPN](@entry_id:910658)N的[表达能力](@entry_id:149863)超越标准的1-WL，达到边标记1-WL的水平。

#### 融入长程信息

为了克服[消息传递](@entry_id:751915)的纯粹局部性，我们可以将非局部的、长程的结构信息直接注入到消息传递框架中，同时注意保持置换[等变性](@entry_id:636671)。以下是一些有效的设计蓝图 ：

- **多跳[消息传递](@entry_id:751915)**：我们可以将图的 $k$ 次幂[邻接矩阵](@entry_id:151010)视为定义了“$k$-跳邻居”关系。模型可以为不同跳数 $\ell \in \{1, \dots, L\}$ 的邻居定义不同的消息函数 $\psi_\ell$，并聚合来自所有这些“虚拟”邻居的信息。这相当于在每一层都直接建立了与多跳邻居的连接。

- **基于距离的虚拟边**：我们可以预先[计算图](@entry_id:636350)中所有节点对之间的[最短路径距离](@entry_id:754797) $d(u,v)$。然后，为所有距离小于等于某个阈值 $L$ 的节点对 $(u,v)$ 添加“虚拟边”，并将距离本身（例如，通过one-hot编码）作为这些虚拟边的特征。模型接着在包含这些虚拟边的新图上执行消息传递。

- **全局锚[点特征](@entry_id:155984)**：我们可以选择一些“锚点”节点（或者所有节点都作为锚点），然后计算每个节点到这些锚点的距离。这些距离信息可以被编码成一个[特征向量](@entry_id:151813)，附加到节点的原始特征上，或者直接在消息函数中使用。这个锚点距离向量为每个节点提供了一个关于其在整个图中“位置”的全局上下文。

这些方法的共同点是，它们都利用了如图的[最短路径距离](@entry_id:754797)这类**置换不变的结构属性**。只要这些长程特征是以一种不依赖于节点任意标签的方式计算和引入的，整个模型的置换[等变性](@entry_id:636671)就不会被破坏。相比之下，一些看似简单的方法，如根据节点的ID（例如，它们在邻接矩阵中的行号）对邻居进行排序，会破坏等变性，因为ID是任意的，与图的结构无关 。

### 深度GNN的[病理学](@entry_id:193640)：过平滑、过挤压与[异质性](@entry_id:275678)挑战

理论上，通过堆叠更多的消息传递层，GNN可以扩大其[感受野](@entry_id:636171)，从而捕捉更大范围的图结构。然而，在实践中，深度GNNs面临着一些独特的挑战，这些挑战限制了其性能。

#### 过平滑（Over-smoothing）

**过平滑**是指随着GNN层数的增加，所有节点的表示趋于收敛到同一个向量的现象。当这种情况发生时，节点失去了其独特性，模型也因此失去了分辨不同节点的能力。

这一现象的根源在于[消息传递](@entry_id:751915)的**邻域平均**本质。每一层[消息传递](@entry_id:751915)都可以看作是在图上进行的一次“信号扩散”或“平滑”操作。经过多轮迭代，初始的节[点特征](@entry_id:155984)信号会在整个图（或其连通分量）上混合，最终达到一个平衡状态，即所有节点的表示都变得非常相似。这个过程的收敛速度与图的谱特性（特别是[谱隙](@entry_id:144877)）密切相关。在混合速度快的图（如[扩展图](@entry_id:141813)）上，过平滑现象会迅速出现 。

诊断过平滑可以通过直接度量节点表示的区分度，例如计算节[点特征](@entry_id:155984)在所有节点间的方差，或者[计算图](@entry_id:636350)的[狄利克雷能量](@entry_id:276589) $\sum_{(u,v) \in E} \|h_u - h_v\|^2$。当这些指标趋于零时，就表明发生了过平滑。

#### 过挤压（Over-squashing）

**过挤压**是另一个限制深度GNN捕捉[长程依赖](@entry_id:181727)的因素，但其机制与过平滑不同。它指的是，当大量来自远方节点的信息需要通过图中的一个狭窄“瓶颈”传递到一个目标节点时，这些信息会被“挤压”到一个固定大小的表示向量中，从而导致指数级的信息损失 。

想象一个树状结构，根节点需要整合来自所有叶子节点的信息。叶子节点的数量随树的深度[指数增长](@entry_id:141869)，但所有这些信息最终都必须通过连接根节点的少数几条边传递。这迫使模型将指数级增长的信息量压缩到有限的[信道容量](@entry_id:143699)中，导致来自远处叶子节点的影响（或梯度）在到达根节点时几乎消失。

过挤压是一个**结构性问题**，而过平滑是一个**迭代过程**的后果。即使在浅层GNN中，如果图结构存在瓶颈，过挤压也可能发生。与此相对，过平滑通常在深层网络中才成为严重问题。

我们可以通过两种方式来量化和诊断过挤压：
- **有效电阻（Effective Resistance）**：源自[电路理论](@entry_id:189041)，两个节点间的[有效电阻](@entry_id:272328)可以衡量它们之间连接的“宽度”或“冗余度”。高有效电阻意味着连接稀疏，存在瓶颈，是过挤压的强指示器。
- **离散曲率（Discrete Curvature）**：如[奥利维尔-里奇曲率](@entry_id:1129104)（Ollivier-Ricci curvature），可以衡量图的局部几何。[负曲率](@entry_id:159335)的边通常扮演着“桥梁”的角色，连接着两个相对疏远的社群，是构成瓶颈的关键部分 。

为了在实验中区分过平滑和过挤压，我们可以设计特定的图结构。例如，在没有瓶颈的[扩展图](@entry_id:141813)上，我们可以主要观察到过平滑；而在带有明显瓶颈的[树状图](@entry_id:266792)或杠铃图上，过挤压将成为主导问题。通过测量不同的指标——如用节[点特征](@entry_id:155984)方差来衡量过平滑，用雅可比范数（衡量远端节点输入对目标节点输出的影响）来衡量过挤压——我们可以独立地诊断这两种现象 。

#### 异质性挑战（The Challenge of Heterophily）

标准的[消息传递](@entry_id:751915)机制在设计上隐含了一个假设，即“同质性”（**homophily**），或称“物以类聚”。这个假设认为，相互连接的节点倾向于具有相似的特征或属于同一类别。因此，通过邻域平均来平滑节[点特征](@entry_id:155984)，可以有效去噪并强化类别信号。

然而，在许多现实世界的网络中，情况恰恰相反。连接的节点可能倾向于具有不同的标签，这种属性被称为“[异质性](@entry_id:275678)”（**heterophily**）。例如，在蛋白质相互作用网络中，不同功能的蛋白质可能需要相互作用才能完成生物任务；在欺诈检测中，欺诈者需要与正常用户进行交易。

在高[度异质性](@entry_id:1123508)的图上，标准的邻域平均会产生灾难性的后果。从[图信号处理](@entry_id:183351)的角度看，同质性标签信号是图上的“低频”信号，而[异质性](@entry_id:275678)标签信号是“高频”信号。邻域平均本质上是一个**低通滤波器**，它会保留低频信号，但抑制高频信号。

考虑一个完美的[异质性](@entry_id:275678)图——二部图，其中所有边都连接着分属不同类别的节点。在这种情况下，一个节点的标签与其所有邻居的标签都相反。对其邻居的标签进行平均，得到的结果恰好是其自身标签的负值。因此，应用一次邻域平均，不仅不会强化节点的类别信息，反而会削弱甚至完全翻转它，从而严重损害模型的分类性能 。

应对[异质性](@entry_id:275678)挑战需要对[消息传递范式](@entry_id:635682)本身进行修改，例如设计能够区分不同类型邻居（如相同类别 vs. 不同类别）的消息函数，或者引入能产生“高通”滤波效果的更新机制。