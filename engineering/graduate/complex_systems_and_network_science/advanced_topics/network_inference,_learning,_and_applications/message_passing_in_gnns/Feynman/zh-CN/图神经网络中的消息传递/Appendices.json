{
    "hands_on_practices": [
        {
            "introduction": "为了真正掌握消息传递机制，我们必须从其基本构成入手。这个练习将该过程分解为核心部分——在简单的路径图上进行消息创建、聚合和更新。通过手动计算一个节点的更新状态，你将为信息如何在图神经网络（GNN）层中局部传播建立起具体的直觉 。",
            "id": "4287364",
            "problem": "考虑一个包含 $3$ 个节点的路径图，其顶点集为 $\\{1,2,3\\}$，无向边为 $\\{(1,2),(2,3)\\}$，每条边的权重均为1。图中没有自环。每个节点 $v$ 携带一个标量特征 $h_{v}^{(0)} \\in \\mathbb{R}$。应用图神经网络 (GNN) 的单个消息传递层，其过程如下：消息从邻居节点 $u \\in \\mathcal{N}(v)$ 沿边发送到节点 $v$，由一个置换不变算子聚合，然后用于更新节点 $v$ 的状态。邻居集合 $\\mathcal{N}(v)$ 是 $v$ 的开邻域。消息函数指定为 $\\psi(h_{v}^{(t)},h_{u}^{(t)},e_{uv}) = h_{u}^{(t)}$，聚合算子为求和 $\\square = \\sum$，节点更新函数为 $\\phi(h_{v}^{(t)},m_{v}) = \\sigma(\\alpha h_{v}^{(t)} + \\beta m_{v})$，其中 $\\alpha \\in \\mathbb{R}$ 和 $\\beta \\in \\mathbb{R}$ 是固定参数，$\\sigma:\\mathbb{R}\\to\\mathbb{R}$ 是一个固定的非线性激活函数。仅使用这些规范以及图上消息传递的核心定义（即对邻居进行聚合后进行局部更新），推导节点 $2$ 的更新后状态 $h_{2}^{(1)}$ 关于 $\\alpha$、$\\beta$、$\\sigma$、$h_{1}^{(0)}$、$h_{2}^{(0)}$ 和 $h_{3}^{(0)}$ 的闭式表达式。将您的最终答案表示为单个简化的解析表达式。无需四舍五入，也不涉及物理单位。",
            "solution": "问题陈述在形式上是有效的，具有科学依据且适定。唯一解所需的所有组成部分均已提供。我们开始进行推导。\n\n在图神经网络 (GNN) 层中，更新节点特征向量的过程由消息传递机制定义。对于任何节点 $v$，其在第 $t$ 层的特征向量 $h_v^{(t)}$ 通过一个三步过程更新为第 $t+1$ 层的 $h_v^{(t+1)}$：消息创建、消息聚合和节点更新。\n\n此更新的一般形式可以写成：\n$$h_{v}^{(t+1)} = \\phi\\left(h_{v}^{(t)}, m_{v}^{(t+1)}\\right)$$\n其中 $m_{v}^{(t+1)}$ 是从节点 $v$ 的邻域聚合而来的消息，计算方式如下：\n$$m_{v}^{(t+1)} = \\square_{u \\in \\mathcal{N}(v)} \\psi\\left(h_{v}^{(t)}, h_{u}^{(t)}, e_{uv}\\right)$$\n\n问题为此框架的各组成部分提供了具体定义：\n1.  消息函数 $\\psi$ 由 $\\psi(h_{v}^{(t)}, h_{u}^{(t)}, e_{uv}) = h_{u}^{(t)}$ 给出。这意味着从邻居节点 $u$ 发送到节点 $v$ 的消息就是邻居节点 $u$ 在第 $t$ 层的特征向量。\n2.  聚合算子 $\\square$ 是求和算子 $\\sum$。\n3.  节点更新函数 $\\phi$ 由 $\\phi(h_{v}^{(t)}, m_{v}) = \\sigma(\\alpha h_{v}^{(t)} + \\beta m_{v})$ 给出，其中 $\\alpha$ 和 $\\beta$ 是标量参数，$\\sigma$ 是一个非线性激活函数。\n\n首先，我们构造聚合消息 $m_{v}^{(t+1)}$ 的具体公式。将给定的消息函数和聚合算子代入通用公式，我们得到：\n$$m_{v}^{(t+1)} = \\sum_{u \\in \\mathcal{N}(v)} h_{u}^{(t)}$$\n这意味着节点 $v$ 的聚合消息是其邻居特征向量的总和。\n\n接下来，我们将此聚合消息代入节点更新函数，以获得适用于任何节点 $v$ 的完整更新规则：\n$$h_{v}^{(t+1)} = \\sigma\\left(\\alpha h_{v}^{(t)} + \\beta \\left(\\sum_{u \\in \\mathcal{N}(v)} h_{u}^{(t)}\\right)\\right)$$\n\n问题要求在经过一层消息传递后，节点 $v=2$ 的更新后状态 $h_{2}^{(1)}$。这对应于从第 $t=0$ 层的初始特征更新到第 $t+1=1$ 层的新特征。对 $v=2$ 和 $t=0$ 应用更新规则：\n$$h_{2}^{(1)} = \\sigma\\left(\\alpha h_{2}^{(0)} + \\beta \\left(\\sum_{u \\in \\mathcal{N}(2)} h_{u}^{(0)}\\right)\\right)$$\n\n最后一步是确定节点 $2$ 的邻域 $\\mathcal{N}(2)$。该图是一个路径图，其顶点集为 $\\{1, 2, 3\\}$，边集为 $\\{(1,2), (2,3)\\}$。一个节点的邻居是指通过边与其直接相连的节点。对于节点 $2$，其边为 $(1,2)$ 和 $(2,3)$。因此，节点 $2$ 的开邻域是 $\\mathcal{N}(2) = \\{1, 3\\}$。\n\n现在我们可以计算节点 $2$ 的邻居上的总和：\n$$\\sum_{u \\in \\mathcal{N}(2)} h_{u}^{(0)} = h_{1}^{(0)} + h_{3}^{(0)}$$\n\n将此总和代回 $h_{2}^{(1)}$ 的表达式，我们得到最终的闭式表达式：\n$$h_{2}^{(1)} = \\sigma\\left(\\alpha h_{2}^{(0)} + \\beta (h_{1}^{(0)} + h_{3}^{(0)})\\right)$$\n该表达式按照要求，给出了节点 $2$ 的更新后特征，用其自身和邻居的初始特征以及 GNN 层的参数表示。",
            "answer": "$$\\boxed{\\sigma(\\alpha h_{2}^{(0)} + \\beta (h_{1}^{(0)} + h_{3}^{(0)}))}$$"
        },
        {
            "introduction": "深度图神经网络的一个关键挑战是确保信息能够在整个图中有效流动。本练习通过一个动手计算，探讨了信息瓶颈（information bottlenecks）和过度挤压（over-squashing）的概念。通过观察移除一条关键的“桥”边后节点表示的变化，你将对图结构如何阻碍远距离消息传递获得定量的理解 。",
            "id": "4287359",
            "problem": "考虑一个连通无向图，其节点标记为 $1,2,3,4,5,6$，边集为 $\\{(1,2),(2,3),(3,4),(4,5),(5,6)\\}$。边 $(3,4)$ 是一个桥，如果移除它，图会断开成两个连通分量 $\\{1,2,3\\}$ 和 $\\{4,5,6\\}$。令 $\\mathbf{A}$ 为该图的邻接矩阵，令 $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ 为带自环的邻接矩阵。令 $\\tilde{\\mathbf{D}}$ 为 $\\tilde{\\mathbf{A}}$ 的对角度矩阵，并定义对称归一化算子 $\\mathbf{S} = \\tilde{\\mathbf{D}}^{-1/2} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-1/2}$。\n\n一个双层图卷积网络 (GCN) 由逐层传播规则 $\\mathbf{H}^{(l+1)} = \\sigma(\\mathbf{S}\\,\\mathbf{H}^{(l)}\\,\\mathbf{W}^{(l)})$ 定义，其中 $\\sigma$ 是激活函数，$\\mathbf{W}^{(l)}$ 是可训练的权重矩阵，$\\mathbf{H}^{(0)} = \\mathbf{X}$ 是输入特征矩阵。考虑标量节点特征，其中 $\\mathbf{X} \\in \\mathbb{R}^{6 \\times 1}$，并设 $\\sigma$ 为恒等函数，$\\mathbf{W}^{(0)} = [1]$，且 $\\mathbf{W}^{(1)} = [1]$。输入特征指定为 $x_1 = 1, x_2 = 1, x_3 = 1$ 以及 $x_4 = x_5 = x_6 = 0$。\n\n在这些条件下，在存在桥边 $(3,4)$ 的原始图上计算得到的节点 5 的双层输出为 $h^{(2)}_5$。现在考虑移除桥边 $(3,4)$ 后得到的修改图；将相应的算子表示为 $\\mathbf{S}'$，并将节点 5 的双层输出表示为 $h'^{(2)}_5$。\n\n从消息传递和归一化邻接的核心定义出发，推导双层输出并计算因移除桥边而引起的变化，定义为 $\\Delta = h^{(2)}_5 - h'^{(2)}_5$。提供 $\\Delta$ 的精确解析值。然后，解释该计算如何定量地反映图上消息传递中的“过度挤压” (over-squashing) 和“瓶颈敏感性” (bottleneck sensitivity) 的概念。\n\n你的最终答案必须是 $\\Delta$ 的单一精确值，不带单位。无需四舍五入。使用数学符号表示所有中间推导过程。",
            "solution": "问题要求计算当从路径图中移除一条桥边时，节点 5 的双层图卷积网络 (GCN) 输出的变化。这个变化量记为 $\\Delta$，将通过首先确定原始图的输出 $h^{(2)}_5$，然后确定修改后图的输出 $h'^{(2)}_5$ 来计算。最后，我们将在图神经网络 (GNN) 中的消息传递、瓶颈和过度挤压的背景下分析此结果。\n\nGCN 的传播规则为 $\\mathbf{H}^{(l+1)} = \\sigma\\!\\left(\\mathbf{S}\\,\\mathbf{H}^{(l)}\\,\\mathbf{W}^{(l)}\\right)$。当激活函数 $\\sigma$ 为恒等函数，权重 $\\mathbf{W}^{(0)} = [1]$ 和 $\\mathbf{W}^{(1)} = [1]$ 时，该规则简化为 $\\mathbf{H}^{(l+1)} = \\mathbf{S}\\,\\mathbf{H}^{(l)}$。对于一个双层网络，最终输出为 $\\mathbf{H}^{(2)} = \\mathbf{S}\\,\\mathbf{H}^{(1)} = \\mathbf{S}(\\mathbf{S}\\,\\mathbf{H}^{(0)}) = \\mathbf{S}^2\\,\\mathbf{H}^{(0)}$，其中 $\\mathbf{H}^{(0)} = \\mathbf{X}$ 是输入特征矩阵。\n\n**第一部分：原始图分析**\n\n首先，我们定义原始图的矩阵，其节点为 $V=\\{1,2,3,4,5,6\\}$，边为 $E = \\{(1,2),(2,3),(3,4),(4,5),(5,6)\\}$。\n\n邻接矩阵 $\\mathbf{A}$ 是：\n$$ \\mathbf{A} = \\begin{pmatrix} 0  1  0  0  0  0 \\\\ 1  0  1  0  0  0 \\\\ 0  1  0  1  0  0 \\\\ 0  0  1  0  1  0 \\\\ 0  0  0  1  0  1 \\\\ 0  0  0  0  1  0 \\end{pmatrix} $$\n带自环的邻接矩阵 $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ 是：\n$$ \\tilde{\\mathbf{A}} = \\begin{pmatrix} 1  1  0  0  0  0 \\\\ 1  1  1  0  0  0 \\\\ 0  1  1  1  0  0 \\\\ 0  0  1  1  1  0 \\\\ 0  0  0  1  1  1 \\\\ 0  0  0  0  1  1 \\end{pmatrix} $$\n度矩阵 $\\tilde{\\mathbf{D}}$ 包含由 $\\tilde{\\mathbf{A}}$ 表示的图中节点的度（即 $\\tilde{d}_i = 1 + \\deg(i)$）。对角线元素为 $\\tilde{d}_1=2, \\tilde{d}_2=3, \\tilde{d}_3=3, \\tilde{d}_4=3, \\tilde{d}_5=3, \\tilde{d}_6=2$。\n$$ \\tilde{\\mathbf{D}} = \\mathrm{diag}(2, 3, 3, 3, 3, 2) $$\n对称归一化算子 $\\mathbf{S} = \\tilde{\\mathbf{D}}^{-1/2} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-1/2}$ 的元素为 $S_{ij} = \\frac{\\tilde{A}_{ij}}{\\sqrt{\\tilde{d}_i \\tilde{d}_j}}$。\n$$ \\mathbf{S} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{\\sqrt{6}}  0  0  0  0 \\\\ \\frac{1}{\\sqrt{6}}  \\frac{1}{3}  \\frac{1}{3}  0  0  0 \\\\ 0  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3}  0  0 \\\\ 0  0  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3}  0 \\\\ 0  0  0  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{\\sqrt{6}} \\\\ 0  0  0  0  \\frac{1}{\\sqrt{6}}  \\frac{1}{2} \\end{pmatrix} $$\n输入特征向量为 $\\mathbf{H}^{(0)} = [1, 1, 1, 0, 0, 0]^T$。\n\n我们需要计算 $h^{(2)}_5$，它是 $\\mathbf{H}^{(2)} = \\mathbf{S}^2 \\mathbf{H}^{(0)}$ 的第五个元素。让我们逐步计算。\n首先，$\\mathbf{H}^{(1)} = \\mathbf{S} \\mathbf{H}^{(0)}$。第 $i$ 个元素是 $h^{(1)}_i = \\sum_{j} S_{ij} h^{(0)}_j$。\n$$ h^{(1)}_4 = S_{4,1}x_1 + S_{4,2}x_2 + S_{4,3}x_3 + S_{4,4}x_4 + S_{4,5}x_5 + S_{4,6}x_6 = 0 \\cdot 1 + 0 \\cdot 1 + \\frac{1}{3} \\cdot 1 + \\frac{1}{3} \\cdot 0 + \\frac{1}{3} \\cdot 0 + 0 \\cdot 0 = \\frac{1}{3} $$\n$$ h^{(1)}_5 = S_{5,3}x_3 + S_{5,4}x_4 + S_{5,5}x_5 + S_{5,6}x_6 = 0 \\cdot 1 + \\frac{1}{3} \\cdot 0 + \\frac{1}{3} \\cdot 0 + \\frac{1}{\\sqrt{6}} \\cdot 0 = 0 $$\n$$ h^{(1)}_6 = S_{6,5}x_5 + S_{6,6}x_6 = \\frac{1}{\\sqrt{6}} \\cdot 0 + \\frac{1}{2} \\cdot 0 = 0 $$\n现在，我们计算 $\\mathbf{H}^{(2)} = \\mathbf{S} \\mathbf{H}^{(1)}$。我们只需要第五个元素 $h^{(2)}_5$。\n$$ h^{(2)}_5 = S_{5,4}h^{(1)}_4 + S_{5,5}h^{(1)}_5 + S_{5,6}h^{(1)}_6 = \\frac{1}{3} \\cdot h^{(1)}_4 + \\frac{1}{3} \\cdot h^{(1)}_5 + \\frac{1}{\\sqrt{6}} \\cdot h^{(1)}_6 $$\n代入 $\\mathbf{H}^{(1)}$ 的值：\n$$ h^{(2)}_5 = \\frac{1}{3} \\cdot \\frac{1}{3} + \\frac{1}{3} \\cdot 0 + \\frac{1}{\\sqrt{6}} \\cdot 0 = \\frac{1}{9} $$\n\n**第二部分：修改后图的分析**\n\n接下来，我们考虑移除了桥边 $(3,4)$ 的图。该图现在断开成两个连通分量：$\\{1,2,3\\}$ 和 $\\{4,5,6\\}$。\n\n新的邻接矩阵 $\\mathbf{A}'$ 及其带自环的版本 $\\tilde{\\mathbf{A}}'$ 是块对角矩阵：\n$$ \\tilde{\\mathbf{A}}' = \\begin{pmatrix} 1  1  0  0  0  0 \\\\ 1  1  1  0  0  0 \\\\ 0  1  1  0  0  0 \\\\ 0  0  0  1  1  0 \\\\ 0  0  0  1  1  1 \\\\ 0  0  0  0  1  1 \\end{pmatrix} $$\n新的度为 $\\tilde{d}'_1=2, \\tilde{d}'_2=3, \\tilde{d}'_3=2$ 和 $\\tilde{d}'_4=2, \\tilde{d}'_5=3, \\tilde{d}'_6=2$。\n新的度矩阵为 $\\tilde{\\mathbf{D}}' = \\mathrm{diag}(2, 3, 2, 2, 3, 2)$。\n新的归一化算子 $\\mathbf{S}'$ 也是块对角的，反映了这两个连通分量。当 $i \\in \\{1,2,3\\}$ 且 $j \\in \\{4,5,6\\}$ 或反之亦然时，不存在非零项 $S'_{ij}$。\n\n输出为 $\\mathbf{H}'^{(2)} = (\\mathbf{S}')^2 \\mathbf{H}^{(0)}$。由于 $\\mathbf{S}'$ 的块对角结构，消息传递被限制在每个连通分量内部。$\\{4,5,6\\}$ 中的节点只能从 $\\{4,5,6\\}$ 中的其他节点聚合信息。\n这个连通分量的输入特征是 $x_4=0, x_5=0, x_6=0$。\n在第一层，对于任何节点 $i \\in \\{4,5,6\\}$，输出是：\n$$ h'^{(1)}_i = \\sum_{j \\in \\{4,5,6\\}} S'_{ij} x_j = \\sum_{j \\in \\{4,5,6\\}} S'_{ij} \\cdot 0 = 0 $$\n由于该连通分量中所有第一层的输出都为零，因此第二层的输出也必须为零：\n$$ h'^{(2)}_i = \\sum_{j \\in \\{4,5,6\\}} S'_{ij} h'^{(1)}_j = \\sum_{j \\in \\{4,5,6\\}} S'_{ij} \\cdot 0 = 0 $$\n因此，修改后图上节点 5 的输出为 $h'^{(2)}_5 = 0$。\n\n**第三部分：变化计算**\n\n变化量 $\\Delta$ 是两个输出之差：\n$$ \\Delta = h^{(2)}_5 - h'^{(2)}_5 = \\frac{1}{9} - 0 = \\frac{1}{9} $$\n\n**第四部分：概念解释**\n\n这个计算为 GNN 如何处理跨图瓶颈的信息流提供了定量的见解。\n1.  **消息传递与感受野**：GCN 通过聚合其局部邻域的消息来更新节点的特征向量。经过 $L$ 层后，一个节点的表示会受到距离 $L$ 以内所有节点的初始特征的影响，即其 $L$-hop 感受野。在我们 $L=2$ 的问题中，输出 $h^{(2)}_5$ 是距离节点 5 最多 2 跳远的节点的初始特征的函数。\n\n2.  **瓶颈敏感性**：在原始图中，从节点 3 到节点 5 的路径是 $3-4-5$，长度为 2。因此，初始特征 $x_3$ 可以影响最终表示 $h^{(2)}_5$。边 $(3,4)$ 作为一个结构性**瓶颈**；它是信息从子图 $\\{1,2,3\\}$ 传递到子图 $\\{4,5,6\\}$ 的唯一路径。我们的计算表明，$x_3=1$ 的影响穿过这个瓶颈传播到节点 5，导致 $h^{(2)}_5 = 1/9$。这个值 $\\Delta = 1/9$ 精确地量化了来自第一个连通分量的信号在经过两次聚合步骤后到达节点 5 的强度。\n\n3.  **过度挤压 (Over-squashing)**：术语**过度挤压**描述了这样一个问题：当信息经过多层（长距离）传递时，它可能会被指数级地稀释或“挤压”，尤其是在被迫通过狭窄瓶颈时。从节点 $u$ 传递到 $v$ 的消息被一个归一化常数缩放，这里是 $1/\\sqrt{\\tilde{d}_u \\tilde{d}_v}$。在我们的案例中，来自 $x_3$ 的信号首先被 $1/\\sqrt{\\tilde{d}_4 \\tilde{d}_3} = 1/3$ 缩放，以贡献于 $h^{(1)}_4$。然后，节点 4 的这个中间信号被传递到节点 5，并被 $1/\\sqrt{\\tilde{d}_5 \\tilde{d}_4} = 1/3$ 缩放。这两跳的总衰减给出了 $1/9$ 的因子，这是信号在穿过路径 $3-4-5$ 时被“挤压”的直接度量。\n\n通过移除桥 $(3,4)$，我们切断了唯一的通信链接。因此，节点 5 的感受野缩小到仅包含其自身连通分量 $\\{4,5,6\\}$ 内的节点。由于该分量中的初始特征均为零，$h'^{(2)}_5$ 变为零。变化量 $\\Delta=1/9$ 代表了来自图另一侧信息流的完全丧失，凸显了 GCN 对图连通性的关键依赖以及瓶颈的严重性。这是过度挤压的最极端形式，即由于路径断开，消息被挤压为零。",
            "answer": "$$\\boxed{\\frac{1}{9}}$$"
        }
    ]
}