{
    "hands_on_practices": [
        {
            "introduction": "Before we can claim a link prediction algorithm is successful, we must first understand what constitutes a baseline level of performance. This exercise guides you through the process of deriving the expected performance of a completely random predictor, which serves as this crucial benchmark . By applying first principles of probability, you will determine the expected Area Under the ROC Curve (AUC) and Precision@k, gaining fundamental insights into how these metrics behave in the absence of any predictive signal.",
            "id": "4309921",
            "problem": "Consider a link prediction evaluation scenario in a simple, unweighted, undirected network. Let the candidate set of node pairs to be ranked be denoted by $\\mathcal{C}$ with $|\\mathcal{C}|=M$. A held-out set of true links (positives) is denoted by $E^{+}\\subset \\mathcal{C}$ with $|E^{+}|=P$, and the remaining pairs are treated as negatives $E^{-}=\\mathcal{C}\\setminus E^{+}$ with $|E^{-}|=N=M-P$. Quantify class imbalance by the positive rate $\\pi=P/M$. Consider a baseline that assigns a uniformly random ranking to all pairs in $\\mathcal{C}$, which is equivalent to assigning independent and identically distributed continuous random scores or a uniformly random permutation with no ties.\n\nYou will evaluate two standard ranking metrics on this random ranking baseline:\n\n- The Area Under the Receiver Operating Characteristic Curve (AUC), defined by the probability that a randomly chosen positive is ranked above a randomly chosen negative, with ties broken uniformly at random when they occur.\n\n- The Precision at $k$ (denoted $\\mathrm{Prec}@k$), defined as the fraction of positives among the top $k$ ranked pairs, where $k$ is an integer satisfying $1\\leq k\\leq M$.\n\nStarting only from these definitions and basic probability (such as symmetry arguments, indicator random variables, and linearity of expectation), derive closed-form expressions for the expected AUC and the expected $\\mathrm{Prec}@k$ under the random ranking baseline, as functions of $\\pi$ and $k$.\n\nExpress your final answer as a single row vector $\\left(\\mathbb{E}[\\mathrm{AUC}],\\,\\mathbb{E}[\\mathrm{Prec}@k]\\right)$. No rounding is required and no units are involved. Assume $M\\geq 2$ and $P\\geq 1$, $N\\geq 1$ so that both classes are present.",
            "solution": "The user wants me to solve the following problem:\n\"Consider a link prediction evaluation scenario in a simple, unweighted, undirected network. Let the candidate set of node pairs to be ranked be denoted by $\\mathcal{C}$ with $|\\mathcal{C}|=M$. A held-out set of true links (positives) is denoted by $E^{+}\\subset \\mathcal{C}$ with $|E^{+}|=P$, and the remaining pairs are treated as negatives $E^{-}=\\mathcal{C}\\setminus E^{+}$ with $|E^{-}|=N=M-P$. Quantify class imbalance by the positive rate $\\pi=P/M$. Consider a baseline that assigns a uniformly random ranking to all pairs in $\\mathcal{C}$, which is equivalent to assigning independent and identically distributed continuous random scores or a uniformly random permutation with no ties.\n\nYou will evaluate two standard ranking metrics on this random ranking baseline:\n\n- The Area Under the Receiver Operating Characteristic Curve (AUC), defined by the probability that a randomly chosen positive is ranked above a randomly chosen negative, with ties broken uniformly at random when they occur.\n\n- The Precision at $k$ (denoted $\\mathrm{Prec}@k$), defined as the fraction of positives among the top $k$ ranked pairs, where $k$ is an integer satisfying $1\\leq k\\leq M$.\n\nStarting only from these definitions and basic probability (such as symmetry arguments, indicator random variables, and linearity of expectation), derive closed-form expressions for the expected AUC and the expected $\\mathrm{Prec}@k$ under the random ranking baseline, as functions of $\\pi$ and $k$.\n\nExpress your final answer as a single row vector $\\left(\\mathbb{E}[\\mathrm{AUC}],\\,\\mathbb{E}[\\mathrm{Prec}@k]\\right)$. No rounding is required and no units are involved. Assume $M\\geq 2$ and $P\\geq 1$, $N\\geq 1$ so that both classes are present.\"\n\n### Step 1: Extract Givens\n- Candidate set of pairs: $\\mathcal{C}$, with size $|\\mathcal{C}|=M$.\n- Set of positive pairs (true links): $E^{+}$, with size $|E^{+}|=P$.\n- Set of negative pairs: $E^{-}=\\mathcal{C}\\setminus E^{+}$, with size $|E^{-}|=N=M-P$.\n- Positive rate: $\\pi=P/M$.\n- Ranking method: Uniformly random ranking of pairs in $\\mathcal{C}$, with no ties.\n- Metric 1: AUC, the probability that a randomly chosen positive is ranked above a randomly chosen negative.\n- Metric 2: $\\mathrm{Prec}@k$, the fraction of positives among the top $k$ ranked pairs, for $1\\leq k\\leq M$.\n- Objective: Derive the expected values $\\mathbb{E}[\\mathrm{AUC}]$ and $\\mathbb{E}[\\mathrm{Prec}@k]$.\n- Conditions: $M\\geq 2$, $P\\geq 1$, $N\\geq 1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within the standard framework of machine learning and network science for evaluating classifiers and ranking algorithms. The definitions of AUC and Precision@k are standard. The random ranking baseline is a fundamental benchmark. The problem is well-posed, providing all necessary definitions and variables ($\\mathcal{C}, M, E^{+}, P, E^{-}, N, \\pi, k$) to derive the requested expected values. Its language is objective and formal. The assumptions ($P\\geq 1, N\\geq 1$) ensure that both positive and negative classes are non-empty, which is necessary for the AUC metric to be well-defined. The problem is a straightforward but non-trivial exercise in probability theory and does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Expected Values\n\nLet the random ranking be modeled by assigning a unique score $S(e)$ to each pair $e \\in \\mathcal{C}$, where the scores are drawn independently from a continuous distribution. This is equivalent to choosing a permutation of $\\mathcal{C}$ uniformly at random. The \"no ties\" condition simplifies the analysis. The expectation $\\mathbb{E}[\\cdot]$ is taken over this random process of generating rankings.\n\n**1. Expected Area Under the ROC Curve ($\\mathbb{E}[\\mathrm{AUC}]$)**\n\nThe AUC for a specific ranking can be calculated as the fraction of all positive-negative pairs where the positive pair is ranked higher than the negative pair. Let $S(e)$ be the score assigned to a pair $e \\in \\mathcal{C}$. A higher score implies a better rank. Given that the assigned scores are continuous and i.i.d., there are no ties. The AUC for a given ranking (i.e., a given realization of scores) is:\n$$\n\\mathrm{AUC} = \\frac{1}{P \\cdot N} \\sum_{e^{+} \\in E^{+}} \\sum_{e^{-} \\in E^{-}} I(S(e^{+})  S(e^{-}))\n$$\nwhere $I(\\cdot)$ is the indicator function. The problem asks for the expected value of the AUC over all possible random rankings.\n$$\n\\mathbb{E}[\\mathrm{AUC}] = \\mathbb{E}\\left[ \\frac{1}{PN} \\sum_{e^{+} \\in E^{+}} \\sum_{e^{-} \\in E^{-}} I(S(e^{+})  S(e^{-})) \\right]\n$$\nBy the linearity of expectation, we can move the expectation operator inside the summations:\n$$\n\\mathbb{E}[\\mathrm{AUC}] = \\frac{1}{PN} \\sum_{e^{+} \\in E^{+}} \\sum_{e^{-} \\in E^{-}} \\mathbb{E}[I(S(e^{+})  S(e^{-}))]\n$$\nThe expectation of an indicator function is the probability of the event it indicates:\n$$\n\\mathbb{E}[I(S(e^{+})  S(e^{-}))] = \\mathrm{P}(S(e^{+})  S(e^{-}))\n$$\nNow, consider any specific positive pair $e^{+} \\in E^{+}$ and any specific negative pair $e^{-} \\in E^{-}$. The baseline model assigns scores to $e^{+}$ and $e^{-}$ randomly and independently from the same continuous distribution. By symmetry, the event $S(e^{+})  S(e^{-})$ is as likely as the event $S(e^{-})  S(e^{+})$. Since the scores are drawn from a continuous distribution, the probability of a tie, $\\mathrm{P}(S(e^{+}) = S(e^{-}))$, is $0$. Therefore:\n$$\n\\mathrm{P}(S(e^{+})  S(e^{-})) + \\mathrm{P}(S(e^{-})  S(e^{+})) = 1\n$$\n$$\n2 \\cdot \\mathrm{P}(S(e^{+})  S(e^{-})) = 1 \\implies \\mathrm{P}(S(e^{+})  S(e^{-})) = \\frac{1}{2}\n$$\nThis probability is the same for every pair of $(e^{+}, e^{-})$. The double summation consists of $PN$ identical terms. Substituting this result back into the expression for $\\mathbb{E}[\\mathrm{AUC}]$:\n$$\n\\mathbb{E}[\\mathrm{AUC}] = \\frac{1}{PN} \\sum_{e^{+} \\in E^{+}} \\sum_{e^{-} \\in E^{-}} \\frac{1}{2} = \\frac{1}{PN} \\left( PN \\cdot \\frac{1}{2} \\right) = \\frac{1}{2}\n$$\nThe expected AUC of a random ranking baseline is $1/2$, irrespective of the class imbalance $\\pi$. This corresponds to the performance of a classifier that guesses randomly.\n\n**2. Expected Precision at $k$ ($\\mathbb{E}[\\mathrm{Prec}@k]$)**\n\nThe Precision at $k$, denoted $\\mathrm{Prec}@k$, is the fraction of positive pairs among the top $k$ ranked pairs. Let $\\mathcal{T}_k$ be the set of the $k$ pairs with the highest scores. The number of positives in this set is $|\\mathcal{T}_k \\cap E^{+}|$.\n$$\n\\mathrm{Prec}@k = \\frac{|\\mathcal{T}_k \\cap E^{+}|}{k}\n$$\nWe need to compute the expected value of this quantity over the random rankings.\n$$\n\\mathbb{E}[\\mathrm{Prec}@k] = \\mathbb{E}\\left[ \\frac{|\\mathcal{T}_k \\cap E^{+}|}{k} \\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[\\mathrm{Prec}@k] = \\frac{1}{k} \\mathbb{E}[|\\mathcal{T}_k \\cap E^{+}|]\n$$\nThe number of positives in the top $k$, $|\\mathcal{T}_k \\cap E^{+}|$, can be expressed as a sum of indicator variables over all positive pairs:\n$$\n|\\mathcalT}_k \\cap E^{+}| = \\sum_{e \\in E^{+}} I(e \\in \\mathcal{T}_k)\n$$\nwhere $I(e \\in \\mathcal{T}_k)$ is $1$ if the pair $e$ is in the top $k$ and $0$ otherwise.\nTaking the expectation:\n$$\n\\mathbb{E}[|\\mathcal{T}_k \\cap E^{+}|] = \\mathbb{E}\\left[ \\sum_{e \\in E^{+}} I(e \\in \\mathcal{T}_k) \\right] = \\sum_{e \\in E^{+}} \\mathbb{E}[I(e \\in \\mathcal{T}_k)]\n$$\nThe expectation of the indicator function is the probability that the event occurs:\n$$\n\\mathbb{E}[I(e \\in \\mathcal{T}_k)] = \\mathrm{P}(e \\in \\mathcal{T}_k)\n$$\nThis is the probability that a specific pair $e$ is ranked among the top $k$. Since the ranking is uniformly random over all $M$ pairs, any given pair $e$ is equally likely to be at any of the $M$ possible rank positions. There are $k$ positions in the top $k$ set. Thus, the probability of a specific pair $e$ being in one of these $k$ positions is:\n$$\n\\mathrm{P}(e \\in \\mathcal{T}_k) = \\frac{k}{M}\n$$\nThis probability is the same for any pair $e \\in \\mathcal{C}$, regardless of whether it is positive or negative.\nSubstituting this probability back into the sum for the expected number of positives:\n$$\n\\mathbb{E}[|\\mathcal{T}_k \\cap E^{+}|] = \\sum_{e \\in E^{+}} \\frac{k}{M}\n$$\nThe sum is over all $P$ positive pairs.\n$$\n\\mathbb{E}[|\\mathcal{T}_k \\cap E^{+}|] = P \\cdot \\frac{k}{M}\n$$\nFinally, we can find the expected precision at $k$:\n$$\n\\mathbb{E}[\\mathrm{Prec}@k] = \\frac{1}{k} \\mathbb{E}[|\\mathcal{T}_k \\cap E^{+}|] = \\frac{1}{k} \\left( P \\frac{k}{M} \\right) = \\frac{P}{M}\n$$\nBy definition, the positive rate is $\\pi = P/M$. Therefore:\n$$\n\\mathbb{E}[\\mathrm{Prec}@k] = \\pi\n$$\nThe expected precision at any rank $k$ for a random baseline is simply the overall fraction of positives in the candidate set.\n\nThe final answer is the row vector of the two derived expected values, $(\\mathbb{E}[\\mathrm{AUC}], \\mathbb{E}[\\mathrm{Prec}@k])$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\pi \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving beyond evaluation, this practice delves into the mechanics of a powerful class of link prediction models: latent factor models. Logistic matrix factorization learns vector representations for nodes, where the relationship between vectors determines link probability . This exercise challenges you to derive the stochastic gradient descent updates from the model's likelihood function, an essential skill for understanding how these models learn from positive and negative examples and for customizing machine learning algorithms.",
            "id": "4309978",
            "problem": "Consider a simple undirected network with node set $V$ and observed edges $E \\subseteq V \\times V$. In the link prediction problem, let the binary random variable $Y_{uv} \\in \\{0,1\\}$ denote the presence or absence of a link between nodes $u$ and $v$. Suppose we model the probability of a link by logistic matrix factorization: for each node $u \\in V$, there is a latent vector $z_u \\in \\mathbb{R}^{d}$, and the link probability is given by $p_{uv} = \\sigma(s_{uv})$, where $s_{uv} = \\langle z_u, z_v \\rangle + b$, $\\langle \\cdot, \\cdot \\rangle$ is the Euclidean inner product, $b \\in \\mathbb{R}$ is a global bias, and $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ is the logistic function. Assume the following generative model for $Y_{uv}$: $Y_{uv} \\sim \\mathrm{Bernoulli}(p_{uv})$ independently across pairs $(u,v)$.\n\nIn practice, the number of non-edges is large; a common approximation of the full regularized negative log-likelihood uses negative sampling. For a single observed positive pair $(u, v^{+}) \\in E$, draw $K \\in \\mathbb{N}$ negative samples $\\{v^{-}_{i}\\}_{i=1}^{K}$ independently from a fixed distribution $q(v \\mid u)$ on $V$, and define the per-sample regularized negative log-likelihood\n$$\n\\ell(u, v^{+}, \\{v^{-}_{i}\\}_{i=1}^{K}) = - \\ln \\sigma\\!\\big(s_{u v^{+}}\\big) \\;-\\; \\sum_{i=1}^{K} \\ln \\sigma\\!\\big(-s_{u v^{-}_{i}}\\big) \\;+\\; \\frac{\\lambda}{2}\\!\\left(\\|z_u\\|^{2} + \\|z_{v^{+}}\\|^{2} + \\sum_{i=1}^{K} \\|z_{v^{-}_{i}}\\|^{2}\\right),\n$$\nwhere $\\lambda  0$ is an $\\ell_{2}$ regularization coefficient. Starting from the Bernoulli likelihood and the logistic function definitions, derive the stochastic gradient descent (SGD) update increments for the latent vectors $z_u$, $z_{v^{+}}$, and, for an arbitrary index $i \\in \\{1,\\dots,K\\}$, $z_{v^{-}_{i}}$, with learning rate $\\eta  0$.\n\nYour derivation must begin from first principles of the Bernoulli model and the logistic function, and must not assume or invoke any pre-derived gradient identities. Express your final answer in closed form in terms of $\\sigma$, $z_u$, $z_{v^{+}}$, $z_{v^{-}_{i}}$, $b$, $\\lambda$, and $\\eta$, using the shorthand $s_{u v^{+}} = \\langle z_u, z_{v^{+}} \\rangle + b$ and $s_{u v^{-}_{i}} = \\langle z_u, z_{v^{-}_{i}} \\rangle + b$. Present the final answer as the three update increments in the order $\\big(\\Delta z_u, \\Delta z_{v^{+}}, \\Delta z_{v^{-}_{i}}\\big)$ in a single row matrix. No numerical approximation or rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of network science and machine learning, is well-posed with a clear objective, and is defined with mathematical precision and without ambiguity. We proceed with the derivation.\n\nThe objective is to derive the stochastic gradient descent (SGD) update increments for the latent vectors $z_u$, $z_{v^{+}}$, and $z_{v^{-}_{i}}$. The SGD update for a parameter vector $\\theta$ is given by $\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} \\ell$, where $\\eta  0$ is the learning rate and $\\ell$ is the objective function to be minimized. The update increment is therefore $\\Delta \\theta = -\\eta \\nabla_{\\theta} \\ell$. We must compute the gradients of the per-sample regularized negative log-likelihood $\\ell$ with respect to $z_u$, $z_{v^{+}}$, and $z_{v^{-}_{i}}$.\n\nThe objective function is:\n$$\n\\ell(u, v^{+}, \\{v^{-}_{i}\\}_{i=1}^{K}) = - \\ln \\sigma\\!\\big(s_{u v^{+}}\\big) \\;-\\; \\sum_{i=1}^{K} \\ln \\sigma\\!\\big(-s_{u v^{-}_{i}}\\big) \\;+\\; \\frac{\\lambda}{2}\\!\\left(\\|z_u\\|^{2} + \\|z_{v^{+}}\\|^{2} + \\sum_{i=1}^{K} \\|z_{v^{-}_{i}}\\|^{2}\\right)\n$$\n\nFirst, we establish the connection of the objective function to the generative model. The model assumes $Y_{uv} \\sim \\mathrm{Bernoulli}(p_{uv})$, where $p_{uv} = \\sigma(s_{uv})$. The likelihood for a single observation $(u,v)$ with outcome $Y_{uv}$ is $P(Y_{uv}) = p_{uv}^{Y_{uv}} (1-p_{uv})^{1-Y_{uv}}$. The log-likelihood is $\\mathcal{L}_{uv} = Y_{uv} \\ln p_{uv} + (1-Y_{uv}) \\ln(1-p_{uv})$.\nFor a positive sample $(u, v^{+})$, we have $Y_{uv^{+}}=1$, so the negative log-likelihood is $-\\mathcal{L}_{uv^{+}} = -\\ln p_{uv^{+}} = -\\ln \\sigma(s_{uv^{+}})$. This matches the first term of $\\ell$.\nFor a negative sample $(u, v^{-})$, we have $Y_{uv^{-}}=0$, so the negative log-likelihood is $-\\mathcal{L}_{uv^{-}} = -\\ln(1-p_{uv^{-}})$. Using the identity $1 - \\sigma(x) = \\sigma(-x)$, this becomes $-\\ln \\sigma(-s_{uv^{-}})$. The summation term in $\\ell$ is thus the sum of negative log-likelihoods for the $K$ negative samples. The objective function $\\ell$ is the regularized negative log-likelihood of the observed positive sample and the $K$ drawn negative samples.\n\nAs required, we derive the derivative of the logistic function $\\sigma(x) = \\frac{1}{1 + \\exp(-x)} = (1 + \\exp(-x))^{-1}$ from first principles. Using the chain rule:\n$$\n\\frac{d\\sigma}{dx} = -1 \\cdot (1 + \\exp(-x))^{-2} \\cdot (-\\exp(-x)) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2}\n$$\nWe can show this is equivalent to $\\sigma(x)(1 - \\sigma(x))$:\n$$\n\\sigma(x)(1 - \\sigma(x)) = \\frac{1}{1 + \\exp(-x)} \\left(1 - \\frac{1}{1 + \\exp(-x)}\\right) = \\frac{1}{1 + \\exp(-x)} \\left(\\frac{1+\\exp(-x)-1}{1 + \\exp(-x)}\\right) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2}\n$$\nThus, $\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))$.\n\nNow we compute the gradients of the components of $\\ell$. The gradient of a scalar function $f(s)$ with respect to a vector $\\theta$ via the chain rule is $\\nabla_{\\theta} f(s( \\theta )) = \\frac{df}{ds} \\nabla_{\\theta} s$.\n\nLet's first compute the derivatives of the log-likelihood terms with respect to the scores.\nFor the positive sample term, $\\ell_{pos} = -\\ln \\sigma(s_{uv^{+}})$:\n$$\n\\frac{\\partial \\ell_{pos}}{\\partial s_{uv^{+}}} = -\\frac{1}{\\sigma(s_{uv^{+}})} \\frac{d\\sigma(s_{uv^{+}})}{ds_{uv^{+}}} = -\\frac{\\sigma(s_{uv^{+}})(1 - \\sigma(s_{uv^{+}}))}{\\sigma(s_{uv^{+}})} = -(1 - \\sigma(s_{uv^{+}})) = \\sigma(s_{uv^{+}}) - 1\n$$\nFor a single negative sample term, $\\ell_{neg,i} = -\\ln \\sigma(-s_{uv^{-}_{i}})$:\n$$\n\\frac{\\partial \\ell_{neg,i}}{\\partial s_{uv^{-}_{i}}} = -\\frac{1}{\\sigma(-s_{uv^{-}_{i}})} \\frac{d\\sigma(-s_{uv^{-}_{i}})}{ds_{uv^{-}_{i}}} = -\\frac{1}{\\sigma(-s_{uv^{-}_{i}})} \\cdot \\sigma'(-s_{uv^{-}_{i}}) \\cdot (-1) = \\frac{\\sigma(-s_{uv^{-}_{i}})(1 - \\sigma(-s_{uv^{-}_{i}}))}{\\sigma(-s_{uv^{-}_{i}})} = 1 - \\sigma(-s_{uv^{-}_{i}})\n$$\nUsing the identity $\\sigma(x) = 1 - \\sigma(-x)$, this simplifies to:\n$$\n\\frac{\\partial \\ell_{neg,i}}{\\partial s_{uv^{-}_{i}}} = \\sigma(s_{uv^{-}_{i}})\n$$\nNext, we compute the gradients of the scores and regularization terms. The score is $s_{uv} = \\langle z_u, z_v \\rangle + b = z_u^T z_v + b$. The gradients with respect to the latent vectors are:\n$$\n\\nabla_{z_u} s_{uv} = z_v \\quad \\text{and} \\quad \\nabla_{z_v} s_{uv} = z_u\n$$\nThe gradient of an $\\ell_2$ regularization term $\\frac{\\lambda}{2} \\|z\\|^2$ with respect to the vector $z$ is:\n$$\n\\nabla_z \\left( \\frac{\\lambda}{2} \\|z\\|^2 \\right) = \\nabla_z \\left( \\frac{\\lambda}{2} z^T z \\right) = \\lambda z\n$$\n\nWe can now assemble the full gradients of $\\ell$:\n\n1.  **Gradient with respect to $z_u$**:\n    The vector $z_u$ appears in the positive term, all $K$ negative terms, and its own regularization term.\n    $$\n    \\nabla_{z_u} \\ell = \\frac{\\partial \\ell}{\\partial s_{uv^{+}}} \\nabla_{z_u} s_{uv^{+}} + \\sum_{j=1}^{K} \\left( \\frac{\\partial \\ell}{\\partial s_{uv^{-}_{j}}} \\nabla_{z_u} s_{uv^{-}_{j}} \\right) + \\nabla_{z_u} \\left(\\frac{\\lambda}{2}\\|z_u\\|^2\\right)\n    $$\n    Substituting the derived components:\n    $$\n    \\nabla_{z_u} \\ell = (\\sigma(s_{uv^{+}}) - 1) z_{v^{+}} + \\sum_{j=1}^{K} \\sigma(s_{uv^{-}_{j}}) z_{v^{-}_{j}} + \\lambda z_u\n    $$\n\n2.  **Gradient with respect to $z_{v^{+}}$**:\n    The vector $z_{v^{+}}$ appears only in the positive term and its own regularization term.\n    $$\n    \\nabla_{z_{v^{+}}} \\ell = \\frac{\\partial \\ell}{\\partial s_{uv^{+}}} \\nabla_{z_{v^{+}}} s_{uv^{+}} + \\nabla_{z_{v^{+}}} \\left(\\frac{\\lambda}{2}\\|z_{v^{+}}\\|^2\\right)\n    $$\n    Substituting the components:\n    $$\n    \\nabla_{z_{v^{+}}} \\ell = (\\sigma(s_{uv^{+}}) - 1) z_u + \\lambda z_{v^{+}}\n    $$\n\n3.  **Gradient with respect to $z_{v^{-}_{i}}$** for an arbitrary index $i \\in \\{1,\\dots,K\\}$:\n    The vector $z_{v^{-}_{i}}$ appears only in the $i$-th negative term and its own regularization term.\n    $$\n    \\nabla_{z_{v^{-}_{i}}} \\ell = \\frac{\\partial \\ell}{\\partial s_{uv^{-}_{i}}} \\nabla_{z_{v^{-}_{i}}} s_{uv^{-}_{i}} + \\nabla_{z_{v^{-}_{i}}} \\left(\\frac{\\lambda}{2}\\|z_{v^{-}_{i}}\\|^2\\right)\n    $$\n    Substituting the components:\n    $$\n    \\nabla_{z_{v^{-}_{i}}} \\ell = \\sigma(s_{uv^{-}_{i}}) z_u + \\lambda z_{v^{-}_{i}}\n    $$\n\nFinally, the SGD update increments are $\\Delta \\theta = -\\eta \\nabla_{\\theta} \\ell$.\n\nThe update increment for $z_u$ is:\n$$\n\\Delta z_u = -\\eta \\left( (\\sigma(s_{uv^{+}}) - 1) z_{v^{+}} + \\sum_{j=1}^{K} \\sigma(s_{uv^{-}_{j}}) z_{v^{-}_{j}} + \\lambda z_u \\right) = \\eta \\left( (1 - \\sigma(s_{uv^{+}})) z_{v^{+}} - \\sum_{j=1}^{K} \\sigma(s_{uv^{-}_{j}}) z_{v^{-}_{j}} - \\lambda z_u \\right)\n$$\nThe update increment for $z_{v^{+}}$ is:\n$$\n\\Delta z_{v^{+}} = -\\eta \\left( (\\sigma(s_{uv^{+}}) - 1) z_u + \\lambda z_{v^{+}} \\right) = \\eta \\left( (1 - \\sigma(s_{uv^{+}})) z_u - \\lambda z_{v^{+}} \\right)\n$$\nThe update increment for $z_{v^{-}_{i}}$ is:\n$$\n\\Delta z_{v^{-}_{i}} = -\\eta \\left( \\sigma(s_{uv^{-}_{i}}) z_u + \\lambda z_{v^{-}_{i}} \\right)\n$$\nThese are the requested expressions for the update increments. The shorthand notations $s_{u v^{+}} = \\langle z_u, z_{v^{+}} \\rangle + b$ and $s_{u v^{-}_{i}} = \\langle z_u, z_{v^{-}_{i}} \\rangle + b$ are implicitly used.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\eta \\left( (\\sigma(s_{uv^{+}}) - 1) z_{v^{+}} + \\sum_{j=1}^{K} \\sigma(s_{uv^{-}_{j}}) z_{v^{-}_{j}} + \\lambda z_u \\right)  -\\eta \\left( (\\sigma(s_{uv^{+}}) - 1) z_u + \\lambda z_{v^{+}} \\right)  -\\eta \\left( \\sigma(s_{uv^{-}_{i}}) z_u + \\lambda z_{v^{-}_{i}} \\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A single performance score provides an incomplete picture; rigorous scientific practice demands that we also quantify the uncertainty in our measurements. This hands-on coding challenge addresses the complex statistical dependencies inherent in network data, where pairs of nodes are not independent observations . You will implement and compare two different bootstrap resampling schemes to construct confidence intervals for the AUC, learning a principled approach to assess the statistical reliability of your link prediction models.",
            "id": "4309977",
            "problem": "Consider the link prediction evaluation problem in complex systems and network science, where an undirected, simple graph on $N$ nodes is partially observed for training, and a set of candidate node pairs is evaluated with a real-valued score computed solely from the training graph. The quality of ranking is summarized by the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC), defined as the probability that a randomly chosen positive candidate has a strictly greater score than a randomly chosen negative candidate, with ties contributing half. Formally, if $X=\\{x_i\\}_{i=1}^{n_+}$ are scores for positive pairs and $Y=\\{y_j\\}_{j=1}^{n_-}$ are scores for negative pairs, then the empirical estimator is the two-sample $U$-statistic\n$$\n\\widehat{\\mathrm{AUC}} \\;=\\; \\frac{1}{n_+\\,n_-}\\sum_{i=1}^{n_+}\\sum_{j=1}^{n_-}\\big(\\mathbf{1}\\{x_iy_j\\}+\\tfrac{1}{2}\\mathbf{1}\\{x_i=y_j\\}\\big).\n$$\nIn graphs, candidate pair scores are statistically dependent because multiple pairs share endpoints (nodes), which induces a nontrivial dependence structure among $X$ and $Y$. Two bootstrap resampling schemes are considered:\n\n- Pair bootstrap: resample the positive pairs with replacement to form a bootstrap sample of size $n_+$ and independently resample the negative pairs with replacement to form a bootstrap sample of size $n_-$. Recompute $\\widehat{\\mathrm{AUC}}$ for each bootstrap replicate.\n- Node-induced cluster bootstrap: resample nodes with replacement to obtain a multiset of size $N$; assign each node $u$ a multiplicity $m_u$. Assign each pair $(u,v)$ a weight $w_{uv}=m_u m_v$. Form a weighted $U$-statistic,\n$$\n\\widehat{\\mathrm{AUC}}_{\\mathrm{w}} \\;=\\; \\frac{\\sum_{i=1}^{n_+}\\sum_{j=1}^{n_-}w_{u_i v_i}\\,w_{p_j q_j}\\,\\big(\\mathbf{1}\\{x_iy_j\\}+\\tfrac{1}{2}\\mathbf{1}\\{x_i=y_j\\}\\big)}{\\left(\\sum_{i=1}^{n_+}w_{u_i v_i}\\right)\\left(\\sum_{j=1}^{n_-}w_{p_j q_j}\\right)},\n$$\nwhere $(u_i,v_i)$ are the endpoints of positive pair $i$ and $(p_j,q_j)$ are the endpoints of negative pair $j$. This scheme preserves the node-level clustering that induces dependence among pair scores.\n\nYour task is to implement both schemes to obtain percentile bootstrap confidence intervals for $\\widehat{\\mathrm{AUC}}$ for the following test suite. In all cases, the score function for a candidate pair $(u,v)$ is the number of common neighbors in the training graph,\n$$\ns(u,v) \\;=\\; \\sum_{k=0}^{N-1} A_{uk}\\,A_{vk},\n$$\nwhere $A\\in\\{0,1\\}^{N\\times N}$ is the symmetric adjacency matrix of the training graph with zero diagonal, and edges in the training graph are excluded from the candidate set. You must compute the point estimate $\\widehat{\\mathrm{AUC}}$ using the exact $U$-statistic (no parametric approximation), then perform bootstrap with $B$ replicates and report a two-sided confidence interval with nominal level $1-\\alpha$ using the percentile method.\n\nTest Suite:\n\n- Case $1$ (happy path, pair bootstrap): $N=6$ training nodes labeled $0,1,2,3,4,5$. Training edges $E_{\\mathrm{train}}=\\{(0,1),(1,2),(2,3),(3,4),(4,5),(0,5),(1,4)\\}$. Positive candidates $T_+=\\{(0,2),(1,3),(2,4)\\}$. Negative candidates $T_-=\\{(0,3),(2,5),(3,5)\\}$. Bootstrap replicates $B=1000$, significance level $\\alpha=0.05$, random seed $42$. Resampling scheme: pair bootstrap.\n- Case $2$ (same graph, node-induced cluster bootstrap): $N=6$, the same $E_{\\mathrm{train}}$, $T_+$, $T_-$ as in Case $1$. Bootstrap replicates $B=1000$, significance level $\\alpha=0.05$, random seed $777$. Resampling scheme: node-induced cluster bootstrap.\n- Case $3$ (class imbalance, pair bootstrap): $N=7$ training nodes labeled $0,1,2,3,4,5,6$. Training edges $E_{\\mathrm{train}}=\\{(0,1),(1,2),(2,3),(3,4),(2,5),(3,6)\\}$. Positive candidates $T_+=\\{(1,3),(2,4)\\}$. Negative candidates $T_-=\\{(0,4),(0,5),(0,6),(1,5),(1,6),(3,5),(4,6),(5,6)\\}$. Bootstrap replicates $B=1000$, significance level $\\alpha=0.05$, random seed $123$. Resampling scheme: pair bootstrap.\n- Case $4$ (complete tie scenario, node-induced cluster bootstrap): $N=6$ training nodes labeled $0,1,2,3,4,5$. Training edges $E_{\\mathrm{train}}=\\{(0,1),(0,2),(0,3),(0,4),(0,5)\\}$ (a star centered at $0$). Positive candidates $T_+=\\{(1,2),(3,4)\\}$. Negative candidates $T_-=\\{(1,3),(2,4)\\}$. In this case, all candidate pairs have identical scores $s(u,v)=1$, so $\\widehat{\\mathrm{AUC}}$ equals $0.5$. Bootstrap replicates $B=1000$, significance level $\\alpha=0.05$, random seed $2024$. Resampling scheme: node-induced cluster bootstrap.\n\nAlgorithmic requirements:\n\n- Implement computation of $s(u,v)$ from $A$.\n- Implement computation of $\\widehat{\\mathrm{AUC}}$ from $X$ and $Y$ via the $U$-statistic.\n- Implement both bootstrap schemes to generate $\\{\\widehat{\\mathrm{AUC}}^{(b)}\\}_{b=1}^B$ and the percentile confidence interval at levels $\\alpha/2$ and $1-\\alpha/2$.\n- If in the node-induced cluster bootstrap a replicate yields zero total weight for all positive or all negative pairs (i.e., the denominator in $\\widehat{\\mathrm{AUC}}_{\\mathrm{w}}$ is zero), discard the replicate and resample until $B$ valid replicates are obtained, with a reasonable cap on attempts.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be the triple $[\\widehat{\\mathrm{AUC}},L,U]$ where $L$ and $U$ are the lower and upper percentile bounds. The overall output must therefore be a list of lists, ordered as Case $1$, Case $2$, Case $3$, Case $4$. For example, the output should look like $[[0.73,0.65,0.80],[0.71,0.62,0.79],[0.85,0.80,0.89],[0.50,0.48,0.52]]$ with decimal values (no percentage sign).",
            "solution": "We begin from the fundamental probabilistic definition of Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) for a ranking function. Let $(X,Y)$ denote independent random scores for a positive and a negative item, respectively. The population quantity is\n$$\n\\mathrm{AUC}\\;=\\;\\mathbb{P}(XY)\\;+\\;\\tfrac{1}{2}\\,\\mathbb{P}(X=Y).\n$$\nGiven samples $X=\\{x_i\\}_{i=1}^{n_+}$ from the positive-score distribution and $Y=\\{y_j\\}_{j=1}^{n_-}$ from the negative-score distribution, a well-tested nonparametric estimator is the two-sample $U$-statistic\n$$\n\\widehat{\\mathrm{AUC}}=\\frac{1}{n_+n_-}\\sum_{i=1}^{n_+}\\sum_{j=1}^{n_-}\\left[\\mathbf{1}\\{x_iy_j\\}+\\tfrac{1}{2}\\mathbf{1}\\{x_i=y_j\\}\\right].\n$$\nThis estimator is equivalent to the normalized Mann–Whitney statistic and is unbiased for $\\mathrm{AUC}$ under independent sampling. In link prediction, the scores arise from a deterministic function of the training graph applied to each candidate pair $(u,v)$, and we explicitly use the number of common neighbors:\n$$\ns(u,v)=\\sum_{k=0}^{N-1}A_{uk}A_{vk}.\n$$\nThis function depends only on the training adjacency matrix $A$ and sums over neighbors $k$. For undirected simple graphs, $A$ is symmetric with zero diagonal, and $A_{uk}\\in\\{0,1\\}$.\n\nDependence structure and bootstrap design: Candidate pair scores in graphs exhibit dependence because multiple pairs share nodes, so their scores are not independent. For example, if pairs $(u,v)$ and $(u,w)$ share node $u$, then both $s(u,v)$ and $s(u,w)$ depend on $\\{A_{uk}\\}_{k}$ and are positively correlated when $u$ has high degree or clustered neighborhood. Standard bootstrap techniques must acknowledge this to produce valid uncertainty quantification.\n\nTwo resampling schemes are considered:\n\n- Pair bootstrap: Resample pairs with replacement independently within positives and negatives. Formally, draw bootstrap indices $\\{I_i\\}_{i=1}^{n_+}$ uniformly from $\\{1,\\dots,n_+\\}$ and $\\{J_j\\}_{j=1}^{n_-}$ uniformly from $\\{1,\\dots,n_-\\}$, then compute\n$$\n\\widehat{\\mathrm{AUC}}^{\\ast}=\\frac{1}{n_+n_-}\\sum_{i=1}^{n_+}\\sum_{j=1}^{n_-}\\left[\\mathbf{1}\\{x_{I_i}y_{J_j}\\}+\\tfrac{1}{2}\\mathbf{1}\\{x_{I_i}=y_{J_j}\\}\\right].\n$$\nThis treats each pair as an independent observation, which may under-estimate variance when there is clustering by nodes because the effective sample size is closer to the number of distinct nodes than the number of pairs.\n- Node-induced cluster bootstrap: Resample nodes with replacement to preserve shared-node dependence. Draw a multiset of nodes $(U_1,\\dots,U_N)$ uniformly from $\\{0,\\dots,N-1\\}$, and define multiplicities $m_u=\\sum_{\\ell=1}^{N}\\mathbf{1}\\{U_\\ell=u\\}$. A candidate pair $(u,v)$ is then assigned weight $w_{uv}=m_u m_v$, interpretable as the number of times it would appear if one expanded the bootstrap multiset into all pairs formed by resampled nodes. For positives with endpoints $(u_i,v_i)$ and negatives with endpoints $(p_j,q_j)$, define weights $W_i=w_{u_i v_i}$ and $V_j=w_{p_j q_j}$. The bootstrap estimator is the weighted $U$-statistic\n$$\n\\widehat{\\mathrm{AUC}}_{\\mathrm{w}}^{\\ast}=\\frac{\\sum_{i=1}^{n_+}\\sum_{j=1}^{n_-}W_iV_j\\left[\\mathbf{1}\\{x_iy_j\\}+\\tfrac{1}{2}\\mathbf{1}\\{x_i=y_j\\}\\right]}{\\left(\\sum_{i=1}^{n_+}W_i\\right)\\left(\\sum_{j=1}^{n_-}V_j\\right)}.\n$$\nThis is a principled cluster bootstrap because it resamples at the node (cluster) level, thus preserving the dependence induced by shared nodes. The denominator ensures normalization by the total positive and negative weights. Edge cases arise when all positive or all negative weights are zero, in which case the replicate provides no information about cross-class comparisons and should be discarded.\n\nConfidence intervals via bootstrap: For either scheme, let $\\widehat{\\mathrm{AUC}}^{(1)},\\dots,\\widehat{\\mathrm{AUC}}^{(B)}$ be the bootstrap replicates. The percentile interval at confidence level $1-\\alpha$ is\n$$\n\\left[\\mathrm{Quantile}_{\\alpha/2}\\left(\\{\\widehat{\\mathrm{AUC}}^{(b)}\\}_{b=1}^{B}\\right),\\;\\mathrm{Quantile}_{1-\\alpha/2}\\left(\\{\\widehat{\\mathrm{AUC}}^{(b)}\\}_{b=1}^{B}\\right)\\right].\n$$\n\nJustification of resampling scheme to preserve dependence: In the presence of node-level clustering, pairs are not exchangeable as independent units; they are conditionally dependent given shared node neighborhoods. The pair bootstrap assumes exchangeability of pairs and thus treats all pair observations equally and independently, which can lead to optimistic variance estimates and narrower confidence intervals. In contrast, the node-induced cluster bootstrap treats nodes as primary sampling units (clusters). Because each pair’s score is a function of its endpoints, resampling nodes with multiplicities naturally induces weights on pairs that reflect how often those endpoints are included in the bootstrap sample. This approach maintains the intra-cluster correlation structure as per classical cluster bootstrap theory: if $\\{X_i\\}$ and $\\{Y_j\\}$ are grouped by clusters with within-cluster dependence, resampling clusters yields a valid approximation to the sampling distribution of $U$-statistics under weak dependence conditions, capturing the reduction in effective sample size. For the link prediction setting with common-neighbor scores, node resampling preserves the shared-neighbor dependence via $A$ and thus produces more reliable uncertainty quantification.\n\nAlgorithmic steps implemented in code:\n\n- Build the adjacency matrix $A$ from the training edge set $E_{\\mathrm{train}}$.\n- Compute candidate scores $X$ and $Y$ via $s(u,v)$ for all $(u,v)\\in T_+$ and $(u,v)\\in T_-$.\n- Compute the exact $U$-statistic $\\widehat{\\mathrm{AUC}}$ using indicator comparisons across all positive–negative pairs, including $\\tfrac{1}{2}$ contribution for ties.\n- For the pair bootstrap, resample $X$ and $Y$ with replacement to obtain $B$ replicates $\\widehat{\\mathrm{AUC}}^{(b)}$.\n- For the node-induced cluster bootstrap, resample nodes with replacement, compute multiplicities $m_u$, form pair weights $w_{uv}=m_u m_v$, compute the weighted $U$-statistic for each valid replicate, and repeat until $B$ replicates are obtained (discarding replicates with zero total positive or negative weight).\n- Compute percentile confidence intervals with quantiles at $\\alpha/2$ and $1-\\alpha/2$.\n- Aggregate results for the four cases in the specified order and print a single-line list of triples $[\\widehat{\\mathrm{AUC}},L,U]$.\n\nCoverage of the test suite:\n\n- Case $1$ exercises the standard scenario with moderate dependence and balanced classes, using pair bootstrap.\n- Case $2$ uses the same data but applies node-induced cluster bootstrap to illustrate the effect of dependence preservation.\n- Case $3$ explores class imbalance with few positives and many negatives, challenging the variance estimation for pair bootstrap.\n- Case $4$ enforces complete ties with identical scores for all candidates; the $U$-statistic yields $\\widehat{\\mathrm{AUC}}=0.5$ exactly, and bootstrap replicates should concentrate around $0.5$, validating tie handling and weighted resampling.\n\nAll outputs are decimals in $[0,1]$ and are presented as a list of lists in a single line, as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_adjacency(num_nodes, edges):\n    \"\"\"Build symmetric adjacency matrix with zero diagonal.\"\"\"\n    A = np.zeros((num_nodes, num_nodes), dtype=int)\n    for u, v in edges:\n        if u == v:\n            continue\n        A[u, v] = 1\n        A[v, u] = 1\n    np.fill_diagonal(A, 0)\n    return A\n\ndef common_neighbors_score(A, u, v):\n    \"\"\"Number of common neighbors between u and v in adjacency A.\"\"\"\n    # Since A is binary, dot product of rows gives count of shared neighbors.\n    return int(np.dot(A[u], A[v]))\n\ndef compute_scores(A, pairs):\n    \"\"\"Compute common neighbors scores for a list of node pairs.\"\"\"\n    return np.array([common_neighbors_score(A, u, v) for (u, v) in pairs], dtype=int)\n\ndef auc_from_scores(pos_scores, neg_scores):\n    \"\"\"Compute exact U-statistic AUC with 0.5 for ties.\"\"\"\n    pos = pos_scores.astype(float)\n    neg = neg_scores.astype(float)\n    # Broadcast comparisons\n    gt = (pos[:, None]  neg[None, :]).sum()\n    eq = (pos[:, None] == neg[None, :]).sum()\n    npos = pos.size\n    nneg = neg.size\n    return (gt + 0.5 * eq) / (npos * nneg)\n\ndef bootstrap_pairs_auc(pos_scores, neg_scores, B, rng):\n    \"\"\"Pair bootstrap: resample pos and neg with replacement, recompute AUC.\"\"\"\n    npos = pos_scores.size\n    nneg = neg_scores.size\n    aucs = np.empty(B, dtype=float)\n    for b in range(B):\n        idx_pos = rng.integers(0, npos, size=npos)\n        idx_neg = rng.integers(0, nneg, size=nneg)\n        aucs[b] = auc_from_scores(pos_scores[idx_pos], neg_scores[idx_neg])\n    return aucs\n\ndef bootstrap_nodes_auc(num_nodes, pos_pairs, neg_pairs, pos_scores, neg_scores, B, rng, max_attempts_factor=10):\n    \"\"\"Node-induced cluster bootstrap: resample nodes, weight pairs by multiplicities.\"\"\"\n    pos_u = np.array([u for (u, v) in pos_pairs], dtype=int)\n    pos_v = np.array([v for (u, v) in pos_pairs], dtype=int)\n    neg_u = np.array([u for (u, v) in neg_pairs], dtype=int)\n    neg_v = np.array([v for (u, v) in neg_pairs], dtype=int)\n    aucs = []\n    attempts = 0\n    max_attempts = max_attempts_factor * B\n    # Precompute comparison matrices (scores are fixed across replicates)\n    gt_mat = (pos_scores[:, None]  neg_scores[None, :]).astype(float)\n    eq_mat = (pos_scores[:, None] == neg_scores[None, :]).astype(float)\n    while len(aucs)  B and attempts  max_attempts:\n        attempts += 1\n        # Resample nodes with replacement: multiplicity vector length num_nodes\n        sample = rng.integers(0, num_nodes, size=num_nodes)\n        mult = np.bincount(sample, minlength=num_nodes)\n        # Pair weights are product of endpoint multiplicities\n        Wp = (mult[pos_u] * mult[pos_v]).astype(float)\n        Wn = (mult[neg_u] * mult[neg_v]).astype(float)\n        sumWp = Wp.sum()\n        sumWn = Wn.sum()\n        if sumWp == 0.0 or sumWn == 0.0:\n            continue  # discard and retry\n        weight_matrix = Wp[:, None] * Wn[None, :]\n        numerator = (gt_mat * weight_matrix).sum() + 0.5 * (eq_mat * weight_matrix).sum()\n        denom = sumWp * sumWn\n        aucs.append(numerator / denom)\n    if len(aucs)  B:\n        # If not enough valid replicates, pad by repeating point estimate behavior (fallback)\n        # This ensures output is produced; in practice, with small graphs this should be rare.\n        needed = B - len(aucs)\n        aucs.extend([np.nan] * needed)\n    return np.array(aucs, dtype=float)\n\ndef percentile_ci(values, alpha):\n    \"\"\"Compute percentile CI handling NaNs by ignoring them.\"\"\"\n    vals = np.asarray(values, dtype=float)\n    vals = vals[~np.isnan(vals)]\n    if vals.size == 0:\n        return (np.nan, np.nan)\n    lo = float(np.quantile(vals, alpha / 2.0))\n    hi = float(np.quantile(vals, 1.0 - alpha / 2.0))\n    return (lo, hi)\n\ndef format_results(triples):\n    \"\"\"Format results as a single-line JSON-like list with no spaces.\"\"\"\n    def fmt(x):\n        if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n            return \"null\"\n        return f\"{x:.6f}\".rstrip('0').rstrip('.')\n    inner = \",\".join(\"[\" + \",\".join(fmt(v) for v in trip) + \"]\" for trip in triples)\n    return \"[\" + inner + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"num_nodes\": 6,\n            \"train_edges\": [(0,1),(1,2),(2,3),(3,4),(4,5),(0,5),(1,4)],\n            \"pos_pairs\": [(0,2),(1,3),(2,4)],\n            \"neg_pairs\": [(0,3),(2,5),(3,5)],\n            \"scheme\": \"pairs\",\n            \"B\": 1000,\n            \"alpha\": 0.05,\n            \"seed\": 42,\n        },\n        {\n            \"num_nodes\": 6,\n            \"train_edges\": [(0,1),(1,2),(2,3),(3,4),(4,5),(0,5),(1,4)],\n            \"pos_pairs\": [(0,2),(1,3),(2,4)],\n            \"neg_pairs\": [(0,3),(2,5),(3,5)],\n            \"scheme\": \"nodes\",\n            \"B\": 1000,\n            \"alpha\": 0.05,\n            \"seed\": 777,\n        },\n        {\n            \"num_nodes\": 7,\n            \"train_edges\": [(0,1),(1,2),(2,3),(3,4),(2,5),(3,6)],\n            \"pos_pairs\": [(1,3),(2,4)],\n            \"neg_pairs\": [(0,4),(0,5),(0,6),(1,5),(1,6),(3,5),(4,6),(5,6)],\n            \"scheme\": \"pairs\",\n            \"B\": 1000,\n            \"alpha\": 0.05,\n            \"seed\": 123,\n        },\n        {\n            \"num_nodes\": 6,\n            \"train_edges\": [(0,1),(0,2),(0,3),(0,4),(0,5)],\n            \"pos_pairs\": [(1,2),(3,4)],\n            \"neg_pairs\": [(1,3),(2,4)],\n            \"scheme\": \"nodes\",\n            \"B\": 1000,\n            \"alpha\": 0.05,\n            \"seed\": 2024,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"num_nodes\"]\n        A = build_adjacency(n, case[\"train_edges\"])\n        pos_pairs = case[\"pos_pairs\"]\n        neg_pairs = case[\"neg_pairs\"]\n        pos_scores = compute_scores(A, pos_pairs)\n        neg_scores = compute_scores(A, neg_pairs)\n        point_auc = auc_from_scores(pos_scores, neg_scores)\n        rng = np.random.default_rng(case[\"seed\"])\n        if case[\"scheme\"] == \"pairs\":\n            boot_vals = bootstrap_pairs_auc(pos_scores, neg_scores, case[\"B\"], rng)\n        elif case[\"scheme\"] == \"nodes\":\n            boot_vals = bootstrap_nodes_auc(n, pos_pairs, neg_pairs, pos_scores, neg_scores, case[\"B\"], rng)\n        else:\n            boot_vals = np.array([np.nan] * case[\"B\"], dtype=float)\n        lo, hi = percentile_ci(boot_vals, case[\"alpha\"])\n        results.append((point_auc, lo, hi))\n\n    # Final print statement in the exact required format.\n    print(format_results(results))\n\nsolve()\n```"
        }
    ]
}