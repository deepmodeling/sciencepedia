{
    "hands_on_practices": [
        {
            "introduction": "在网络重构中，一个核心挑战是区分直接因果关系和由中介变量引起的伪相关。本练习将通过一个模拟的三变量系统，直观地展示成对格兰杰因果关系如何错误地推断出一个直接连接，而条件格兰杰因果关系通过引入中介变量，能够揭示真实的间接影响路径。掌握这种区分对于避免在网络推断中得出虚假结论至关重要。",
            "id": "4291711",
            "problem": "您需要设计并实现一个完整且可运行的程序，用于构建和分析线性时间序列，以便在复杂系统和网络科学的框架内，论证成对格兰杰因果关系与条件格兰杰因果关系之间的差异。从以下基本原理开始：格兰杰因果关系（Granger causality, GC）指出，如果使用过程 $Z$ 自身历史预测 $Z$ 的预测误差方差，严格大于使用 $Z$ 自身历史加上过程 $X$ 的历史预测 $Z$ 的预测误差方差，那么过程 $X$ 格兰杰导致（Granger-causes）过程 $Z$。在线性模型、高斯新息和平稳性的假设下，该检验可简化为通过标准统计假设检验来比较嵌套的线性自回归模型。使用阶为1的向量自回归（Vector Autoregressive, VAR）模型作为过程的生成模型。\n\n构建一个包含三个标量过程 $X$、$Y$ 和 $Z$ 的模拟，并根据具有线性关系和高斯噪声的阶为1的向量自回归（VAR）模型生成样本。使用一个仅出现滞后1项的模型，其形式如下\n$$\nX_t = a_x X_{t-1} + \\eta^x_t,\\quad\nY_t = b_{xy} X_{t-1} + a_y Y_{t-1} + \\eta^y_t,\\quad\nZ_t = b_{xz} X_{t-1} + b_{zy} Y_{t-1} + a_z Z_{t-1} + \\eta^z_t,\n$$\n其中，每个新息 $\\eta^x_t$、$\\eta^y_t$ 和 $\\eta^z_t$ 是独立的、服从均值为零、方差为 $\\sigma^2$ 的同分布高斯噪声。在此设置中，从 $X$到 $Z$ 的格兰杰因果关系将通过两种方式进行评估：\n- 成对格兰杰因果关系：比较一个仅使用 $Z_{t-1}$ 的受限自回归模型与一个使用 $Z_{t-1}$ 和 $X_{t-1}$ 的完整模型。\n- 条件格兰杰因果关系：比较一个使用 $Z_{t-1}$ 和 $Y_{t-1}$ 的受限自回归模型与一个使用 $Z_{t-1}$、$Y_{t-1}$ 和 $X_{t-1}$ 的完整模型。\n\n在这两种情况下，都在显著性水平 $\\alpha$ 下执行标准的嵌套线性模型显著性检验，以判断添加 $X$ 的滞后预测项是否显著减少了 $Z$ 的预测误差方差。使用基于高斯线性模型假设推导出的传统 $F$ 检验的双边决策规则（不要实现标准嵌套模型 $F$ 检验之外的任何简化公式）。为每个检验返回一个布尔决策，指明成对格兰杰因果关系是否检测到边 $X \\to Z$，以及在控制变量 $Y$ 的条件下，该边在条件格兰杰因果关系下是否仍然存在。\n\n您的程序必须使用固定的随机种子模拟独立的实现，并且必须使用以下测试套件。每个测试用例指定 $(T, a_x, a_y, a_z, b_{xy}, b_{xz}, b_{zy}, \\sigma, \\text{seed})$，其中 $T$ 是预烧期后的样本大小，$a_x$、$a_y$ 和 $a_z$ 是自回归系数，$b_{xy}$、$b_{xz}$ 和 $b_{zy}$ 是交叉滞后系数，$\\sigma$ 是新息标准差，$\\text{seed}$ 是用于可复现性的随机种子。在分析前，使用额外的 $B$ 个样本作为预烧期并丢弃，以减少瞬态效应。设置 $\\alpha = 10^{-6}$ 和 $B = 100$。测试套件如下：\n- 案例1（无直接边的链式结构，预期成对检测到，但在条件下被移除）：$(T,a_x,a_y,a_z,b_{xy},b_{xz},b_{zy},\\sigma,\\text{seed}) = (\\,$ $5000$, $0.3$, $0.5$, $0.4$, $0.8$, $0.0$, $0.8$, $1.0$, $42$ $\\,)$.\n- 案例2（链式结构加直接边，预期在成对和条件下均能检测到）：$(T,a_x,a_y,a_z,b_{xy},b_{xz},b_{zy},\\sigma,\\text{seed}) = (\\,$ $5000$, $0.3$, $0.5$, $0.4$, $0.8$, $0.3$, $0.7$, $1.0$, $43$ $\\,)$.\n- 案例3（弱中介效应和高噪声，检测可能性不大的边界情况）：$(T,a_x,a_y,a_z,b_{xy},b_{xz},b_{zy},\\sigma,\\text{seed}) = (\\,$ $800$, $0.3$, $0.2$, $0.2$, $0.05$, $0.0$, $0.05$, $3.0$, $44$ $\\,)$.\n\n对于每个案例，根据指定的参数生成时间序列，使用普通最小二乘法和嵌套模型 $F$ 检验，在显著性水平 $\\alpha$ 下估计从 $X$到 $Z$ 的滞后1的成对和条件格兰杰因果关系，并输出一个包含两个布尔值的列表 $[b_{\\text{pairwise}}, b_{\\text{conditional}}]$，其中如果成对格兰杰表明存在边 $X \\to Z$，则 $b_{\\text{pairwise}}$ 为真，如果在以 $Y$ 为条件时该边仍然存在，则 $b_{\\text{conditional}}$ 为真。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[[\\text{True},\\text{False}],[\\text{True},\\text{True}],[\\text{False},\\text{False}]]$）。不涉及任何物理单位或角度；所有量均为无量纲实数。每个测试用例的输出必须是基于所述统计决策规则的布尔值。",
            "solution": "该问题被评估为有效。它在科学上基于时间序列分析理论，特别是向量自回归（VAR）模型和格兰杰因果关系。该问题定义明确，所有必要的参数、模型和统计程序都有明确规定。它是客观、完整的，并构成了一个计算科学中标准的、可验证的任务。\n\n任务是使用模拟的时间序列来区分成对格兰杰因果关系和条件格兰杰因果关系。我们将构建一个程序，首先从一个指定的三变量VAR($1$)模型生成数据，然后应用嵌套模型$F$检验，以确定在有和没有以第三个变量 $Y$ 为条件的情况下，从变量 $X$ 到变量 $Z$ 是否存在因果联系。\n\n### 1. 生成模型\n\n三个标量过程 $X_t, Y_t, Z_t$ 的时间序列由一个VAR($1$)模型生成：\n$$\n\\begin{pmatrix} X_t \\\\ Y_t \\\\ Z_t \\end{pmatrix}\n=\n\\begin{pmatrix} a_x  0  0 \\\\ b_{xy}  a_y  0 \\\\ b_{xz}  b_{zy}  a_z \\end{pmatrix}\n\\begin{pmatrix} X_{t-1} \\\\ Y_{t-1} \\\\ Z_{t-1} \\end{pmatrix}\n+\n\\begin{pmatrix} \\eta^x_t \\\\ \\eta^y_t \\\\ \\eta^z_t \\end{pmatrix}\n$$\n其中 $\\eta^x_t, \\eta^y_t, \\eta^z_t$ 是从高斯分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的独立同分布（i.i.d.）随机变量。系数 $a_x, a_y, a_z$ 是自回归项，$b_{xy}, b_{xz}, b_{zy}$ 是交叉滞后耦合项。系统在 $X_0 = Y_0 = Z_0 = 0$ 处初始化。使用 $B=100$ 个样本的预烧期，以使系统接近其平稳分布，之后保留 $T$ 个样本用于分析。\n\n### 2. 格兰杰因果关系检验\n\n格兰杰因果关系是一个基于预测的统计因果概念。如果变量 $X$ 的过去值包含了有助于预测 $Z$ 的信息，并且这些信息超出了仅使用 $Z$ 的过去值所包含的信息，那么就称变量 $X$ 格兰杰导致变量 $Z$。对于线性模型，这可以通过比较嵌套的线性回归模型来检验。预测改进的显著性通过 $F$ 检验进行评估。\n\n用于比较受限模型（$R$）和非受限模型（$U$）的 $F$ 统计量由下式给出：\n$$\nF = \\frac{(\\text{RSS}_R - \\text{RSS}_U) / q}{\\text{RSS}_U / (n - k)}\n$$\n其中：\n- $\\text{RSS}_R$ 是受限模型的残差平方和。\n- $\\text{RSS}_U$ 是非受限模型的残差平方和。\n- $n$ 是用于回归的观测数量。对于滞后1模型，长度为 $T$ 的时间序列提供 $n=T-1$ 个观测值。\n- $k$ 是非受限模型中的参数数量（包括截距在内的系数）。\n- $q$ 是为从非受限模型得到受限模型而施加的约束数量。在我们的案例中，$q=1$，因为我们检验单个系数的显著性。\n\n计算出的 $F$ 统计量与自由度为 $(q, n-k)$ 的 $F$ 分布进行比较。如果与 $F$ 统计量相关的p值小于指定的显著性水平 $\\alpha = 10^{-6}$，则拒绝无格兰杰因果关系的原假设（即滞后的 $X$ 项的系数为零）。\n\n### 3. 成对格兰杰因果关系：$X \\to Z$\n\n对于成对检验，我们检验在仅使用 $Z_{t-1}$ 作为预测变量时，$X_{t-1}$ 是否能改善对 $Z_t$ 的预测。\n- **非受限模型 ($U_p$):** $Z_t = \\beta_0 + \\beta_1 Z_{t-1} + \\beta_2 X_{t-1} + \\epsilon_U(t)$。该模型有 $k_p=3$ 个参数（$\\beta_0, \\beta_1, \\beta_2$）。\n- **受限模型 ($R_p$):** $Z_t = \\gamma_0 + \\gamma_1 Z_{t-1} + \\epsilon_R(t)$。该模型通过对 $U_p$ 施加约束 $\\beta_2 = 0$ 而得到。\n\n$F$ 检验使用 $n = T-1$ 个观测值，完整模型中有 $k = k_p = 3$ 个参数，以及 $q = 1$ 个约束。自由度为 $(1, T-1-3)$。\n\n### 4. 条件格兰杰因果关系：$X \\to Z | Y$\n\n对于条件检验，我们检验在 $Z_{t-1}$ 和中介变量 $Y_{t-1}$ 都已作为预测变量包含在内时，$X_{t-1}$ 是否能改善对 $Z_t$ 的预测。\n- **非受限模型 ($U_c$):** $Z_t = \\delta_0 + \\delta_1 Z_{t-1} + \\delta_2 Y_{t-1} + \\delta_3 X_{t-1} + \\epsilon_U(t)$。该模型有 $k_c=4$ 个参数（$\\delta_0, \\delta_1, \\delta_2, \\delta_3$）。\n- **受限模型 ($R_c$):** $Z_t = \\zeta_0 + \\zeta_1 Z_{t-1} + \\zeta_2 Y_{t-1} + \\epsilon_R(t)$。该模型通过对 $U_c$ 施加约束 $\\delta_3 = 0$ 而得到。\n\n$F$ 检验使用 $n = T-1$ 个观测值，完整模型中有 $k = k_c = 4$ 个参数，以及 $q = 1$ 个约束。自由度为 $(1, T-1-4)$。\n\n所有模型的系数（$\\beta_i, \\gamma_i, \\delta_i, \\zeta_i$）均使用普通最小二乘法（OLS）进行估计。对每个检验（成对和条件）的决策是一个布尔值，指示原假设是否在显著性水平 $\\alpha$ 下被拒绝。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Simulates a 3-variable VAR(1) process and performs pairwise and conditional\n    Granger causality tests to demonstrate their differences.\n    \"\"\"\n    # Define problem parameters and test cases\n    B = 100  # Burn-in period\n    alpha = 1e-6  # Significance level\n\n    test_cases = [\n        # Case 1: Chain X -> Y -> Z (b_xz = 0). Expect [True, False].\n        (5000, 0.3, 0.5, 0.4, 0.8, 0.0, 0.8, 1.0, 42),\n        # Case 2: Chain plus direct link X -> Z. Expect [True, True].\n        (5000, 0.3, 0.5, 0.4, 0.8, 0.3, 0.7, 1.0, 43),\n        # Case 3: Weak chain, high noise. Expect [False, False].\n        (800, 0.3, 0.2, 0.2, 0.05, 0.0, 0.05, 3.0, 44),\n    ]\n\n    def _calculate_f_test_p_value(y, X_full, X_reduced):\n        \"\"\"\n        Performs a nested model F-test and returns the p-value.\n        \n        Args:\n            y (np.ndarray): Target variable vector.\n            X_full (np.ndarray): Design matrix for the unrestricted model.\n            X_reduced (np.ndarray): Design matrix for the restricted model.\n            \n        Returns:\n            float: The p-value from the F-test.\n        \"\"\"\n        n = y.shape[0]\n        k_full = X_full.shape[1]\n        k_reduced = X_reduced.shape[1]\n        q = k_full - k_reduced\n\n        # Fit models using Ordinary Least Squares\n        try:\n            # lstsq returns: coefficients, residuals, rank, singular values\n            rss_full = np.linalg.lstsq(X_full, y, rcond=None)[1][0]\n            rss_reduced = np.linalg.lstsq(X_reduced, y, rcond=None)[1][0]\n        except IndexError:\n            # This can happen if the time series is too short, leading to empty RSS\n            return 1.0\n\n        # Handle cases where the full model provides no improvement\n        if rss_reduced == rss_full:\n            return 1.0\n\n        # Calculate F-statistic\n        df_num = q\n        df_den = n - k_full\n        \n        # Avoid division by zero if residual sum of squares is (close to) zero\n        if df_den == 0 or rss_full  1e-9:\n            return 1.0\n            \n        f_stat = ((rss_reduced - rss_full) / df_num) / (rss_full / df_den)\n        \n        # Calculate p-value using the survival function (1 - CDF)\n        p_value = f.sf(f_stat, df_num, df_den)\n        \n        return p_value\n\n    all_results = []\n    for case in test_cases:\n        T, a_x, a_y, a_z, b_xy, b_xz, b_zy, sigma, seed = case\n        \n        # Set seed for reproducibility\n        np.random.seed(seed)\n        \n        # Generate time series data\n        total_len = T + B\n        X = np.zeros(total_len)\n        Y = np.zeros(total_len)\n        Z = np.zeros(total_len)\n        \n        # Generate noise terms\n        noise = np.random.normal(loc=0, scale=sigma, size=(3, total_len - 1))\n        \n        for t in range(1, total_len):\n            X[t] = a_x * X[t-1] + noise[0, t-1]\n            Y[t] = b_xy * X[t-1] + a_y * Y[t-1] + noise[1, t-1]\n            Z[t] = b_xz * X[t-1] + b_zy * Y[t-1] + a_z * Z[t-1] + noise[2, t-1]\n            \n        # Discard burn-in samples\n        X_main = X[B:]\n        Y_main = Y[B:]\n        Z_main = Z[B:]\n\n        # Prepare data for regression (n = T-1 observations)\n        y_target = Z_main[1:]\n        lag_Z = Z_main[:-1]\n        lag_X = X_main[:-1]\n        lag_Y = Y_main[:-1]\n        intercept = np.ones_like(y_target)\n        \n        # --- Pairwise Granger Causality Test (X -> Z) ---\n        X_full_p = np.column_stack([lag_Z, lag_X, intercept])\n        X_reduced_p = np.column_stack([lag_Z, intercept])\n        p_value_p = _calculate_f_test_p_value(y_target, X_full_p, X_reduced_p)\n        b_pairwise = p_value_p  alpha\n\n        # --- Conditional Granger Causality Test (X -> Z | Y) ---\n        X_full_c = np.column_stack([lag_Z, lag_Y, lag_X, intercept])\n        X_reduced_c = np.column_stack([lag_Z, lag_Y, intercept])\n        p_value_c = _calculate_f_test_p_value(y_target, X_full_c, X_reduced_c)\n        b_conditional = p_value_c  alpha\n\n        all_results.append([b_pairwise, b_conditional])\n\n    # Format the output string exactly as required\n    output_str = \"[\" + \",\".join([f\"[{str(p)},{str(c)}]\" for p, c in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在处理混杂效应时，当共同驱动因素本身是隐藏或无法直接测量时，问题会变得更加复杂。本练习将探讨为何对一个不完美的“代理”变量进行条件化不足以消除偏差，并引导您推导和应用工具变量（IV）方法来准确估计因果效应。这个实践介绍了一种在存在隐藏变量时进行稳健因果推断的强大技术。",
            "id": "4291668",
            "problem": "考虑一个包含三个节点的有向网络：一个观测到的驱动节点 $X$，一个观测到的目标节点 $Y$，以及一个隐藏的共同驱动节点 $H$。您观测到时间序列 $\\{X_{t}\\}_{t=1}^{T}$ 和 $\\{Y_{t}\\}_{t=1}^{T}$，并可以向节点 $X$ 注入一个受控的外生输入 $\\{I_{t}\\}_{t=1}^{T}$。隐藏节点 $H$ 无法直接观测，但您同时收集了一个带噪声的代理变量 $\\{W_{t}\\}_{t=1}^{T}$，它不完美地测量了 $H$。该系统在一个线性结构模型下按离散时间演化：\n$$\nX_{t} \\;=\\; \\gamma I_{t} \\;+\\; c H_{t} \\;+\\; \\epsilon^{X}_{t}, \\qquad\nY_{t+1} \\;=\\; b X_{t} \\;+\\; d H_{t} \\;+\\; \\epsilon^{Y}_{t+1}, \\qquad\nH_{t} \\;=\\; r H_{t-1} \\;+\\; \\epsilon^{H}_{t}, \\qquad\nW_{t} \\;=\\; \\alpha H_{t} \\;+\\; \\eta_{t},\n$$\n其中所有序列均为零均值，$\\epsilon^{X}_{t}$、$\\epsilon^{Y}_{t}$、$\\epsilon^{H}_{t}$ 和 $\\eta_{t}$ 是相互独立的白噪声过程，而 $I_{t}$ 是一个独立的白噪声序列，与 $H_{t}$ 及所有噪声项在统计上是独立的。参数 $b$、$c$、$d$、$\\gamma$、$r$ 和 $\\alpha$ 是常数。注入 $I_{t}$ 由实验者设计，不直接作用于 $Y$ 或 $H$。\n\n您的目标是仅基于观测到的时间序列和已知的注入 $I_{t}$，重建从 $X$ 到 $Y$ 的有向边强度，即耦合系数 $b$。假设时间序列足够长，使得经验协方差收敛于其总体协方差。您被给予了在指定滞后下的以下交叉协方差的经验极限：\n$$\n\\operatorname{Cov}(I_{t}, X_{t}) \\;=\\; 1.6, \\qquad \\operatorname{Cov}(I_{t}, Y_{t+1}) \\;=\\; 2.4,\n$$\n并且您可以假设 $\\operatorname{Cov}(I_{t}, H_{t}) \\;=\\; 0$, $\\operatorname{Cov}(I_{t}, \\epsilon^{X}_{t}) \\;=\\; 0$, $\\operatorname{Cov}(I_{t}, \\epsilon^{Y}_{t+1}) \\;=\\; 0$, and $\\operatorname{Cov}(I_{t}, \\eta_{t}) \\;=\\; 0$。\n\n从时间序列分析和回归的核心定义出发，即线性投影、协方差，以及普通最小二乘法 (OLS) 和工具变量 (IV) 背后的假设，完成以下任务：\n\n1. 解释为什么以观测到的代理变量 $W_{t}$ 为条件不足以消除由隐藏共同驱动因素 $H_{t}$ 引起的混淆，以及为什么在 $Y_{t+1}$ 对 $X_{t}$ 和 $W_{t}$ 的回归中残余内生性仍然存在。您的解释应明确使用上述线性模型以及内生性（即回归变量与误差项之间的非零协方差）的定义。\n\n2. 使用外生输入 $I_{t}$ 和时滞结构，提出并论证一种工具变量补救方法。在模型假设和 $I_{t}$ 满足的排他性限制条件下，推导出一个以时间序列的可观测（总体）协方差为函数的 $b$ 的识别公式。您的推导必须从期望和协方差定律以及上面给出的线性结构方程开始，不得引用任何未经证明的快捷公式。\n\n3. 使用提供的经验协方差极限，计算 $b$ 的数值。除非必要，否则不要对最终数值进行四舍五入。\n\n您的最终答案必须是一个实数。无需单位。",
            "solution": "该问题已经过验证，被认为是系统辨识和时间序列因果推断领域中一个适定的、有科学依据的问题。我们可以进行完整的解答。解答按要求分为三个部分。\n\n该系统由以下线性结构模型描述：\n$$\n\\begin{aligned}\nX_{t}  \\;=\\; \\gamma I_{t} \\;+\\; c H_{t} \\;+\\; \\epsilon^{X}_{t} \\\\\nY_{t+1}  \\;=\\; b X_{t} \\;+\\; d H_{t} \\;+\\; \\epsilon^{Y}_{t+1} \\\\\nH_{t}  \\;=\\; r H_{t-1} \\;+\\; \\epsilon^{H}_{t} \\\\\nW_{t}  \\;=\\; \\alpha H_{t} \\;+\\; \\eta_{t}\n\\end{aligned}\n$$\n所有随机过程均为零均值，噪声项 $\\epsilon^{X}_{t}$、$\\epsilon^{Y}_{t}$、$\\epsilon^{H}_{t}$ 和 $\\eta_{t}$ 是相互独立的白噪声。外生输入 $I_{t}$ 是一个与所有其他过程独立的白噪声序列。我们的目标是确定耦合系数 $b$。\n\n1. 以代理变量 $W_{t}$ 为条件的不足之处\n\n对 $Y_{t+1}$ 和 $X_{t}$ 进行简单回归会因隐藏共同驱动因素 $H_{t}$ 的混淆而得到一个有偏的 $b$ 的估计值。底层结构方程为 $Y_{t+1} = b X_{t} + d H_{t} + \\epsilon^{Y}_{t+1}$。在仅对 $Y_{t+1}$ 和 $X_{t}$ 进行的回归中，误差项为 $v_{t} = d H_{t} + \\epsilon^{Y}_{t+1}$。回归变量为 $X_{t} = \\gamma I_{t} + c H_{t} + \\epsilon^{X}_{t}$。由于回归变量 $X_{t}$ 与误差项 $v_{t}$ 相关，因此存在内生性：\n$$\n\\operatorname{Cov}(X_{t}, v_{t}) = \\operatorname{Cov}(\\gamma I_{t} + c H_{t} + \\epsilon^{X}_{t}, d H_{t} + \\epsilon^{Y}_{t+1}) = cd \\operatorname{Var}(H_{t})\n$$\n如果 $c \\neq 0$ 且 $d \\neq 0$，该协方差非零，这是经典的遗漏变量偏误情景。\n\n问题提出通过以可观测的代理变量 $W_{t}$ 为条件来补救此问题，即对 $Y_{t+1}$ 进行关于 $X_{t}$ 和 $W_{t}$ 的多元回归。回归模型被指定为 $Y_{t+1} = \\beta_{1} X_{t} + \\beta_{2} W_{t} + u_{t}$。我们必须解释为什么 $\\beta_{1}$ 的普通最小二乘 (OLS) 估计量不是真实参数 $b$ 的一致估计量。\n\n为了分析这一点，我们可以用可用的回归变量 $X_{t}$ 和 $W_{t}$ 来表示 $Y_{t+1}$ 的真实数据生成过程。根据代理变量的定义 $W_{t} = \\alpha H_{t} + \\eta_{t}$，假设 $\\alpha \\neq 0$，我们可以将隐藏变量 $H_{t}$ 写为 $H_{t} = \\frac{1}{\\alpha}(W_{t} - \\eta_{t})$。将此代入 $Y_{t+1}$ 的结构方程：\n$$\nY_{t+1} = b X_{t} + d \\left(\\frac{W_{t} - \\eta_{t}}{\\alpha}\\right) + \\epsilon^{Y}_{t+1}\n$$\n$$\nY_{t+1} = b X_{t} + \\frac{d}{\\alpha} W_{t} + \\left(\\epsilon^{Y}_{t+1} - \\frac{d}{\\alpha} \\eta_{t}\\right)\n$$\n该方程具有多元回归模型的形式，其中真实参数为 $\\beta_{1} = b$ 和 $\\beta_{2} = \\frac{d}{\\alpha}$，结构误差项为 $u_{t} = \\epsilon^{Y}_{t+1} - \\frac{d}{\\alpha} \\eta_{t}$。\n\n为了使 $\\beta_{1}$ 和 $\\beta_{2}$ 的 OLS 估计量具有一致性，所有回归变量必须与误差项 $u_{t}$ 不相关。我们来检验这个条件。我们检查每个回归变量 $X_{t}$ 和 $W_{t}$ 与误差项 $u_t$ 的协方差。\n\n首先，对于回归变量 $W_{t}$：\n$$\n\\operatorname{Cov}(W_{t}, u_{t}) = \\operatorname{Cov}\\left(\\alpha H_{t} + \\eta_{t}, \\epsilon^{Y}_{t+1} - \\frac{d}{\\alpha} \\eta_{t}\\right)\n$$\n利用协方差的双线性以及 $H_{t}$、$\\eta_{t}$ 和 $\\epsilon^{Y}_{t+1}$ 的相互独立性：\n$$\n\\operatorname{Cov}(W_{t}, u_{t}) = \\alpha \\operatorname{Cov}(H_{t}, \\epsilon^{Y}_{t+1}) - d \\operatorname{Cov}(H_{t}, \\eta_{t}) + \\operatorname{Cov}(\\eta_{t}, \\epsilon^{Y}_{t+1}) - \\frac{d}{\\alpha} \\operatorname{Cov}(\\eta_{t}, \\eta_{t})\n$$\n由于独立性，前三项为零。最后一项非零。\n$$\n\\operatorname{Cov}(W_{t}, u_{t}) = -\\frac{d}{\\alpha} \\operatorname{Var}(\\eta_{t})\n$$\n由于 $W_{t}$ 与误差项 $u_{t}$ 相关（假设 $d \\neq 0$ 且测量有噪声，即 $\\operatorname{Var}(\\eta_{t})  0$），$W_{t}$ 是一个内生回归变量。这是一种测量误差问题；代理变量 $W_{t}$ 含有误差，不是作为回归变量的估计量，而是作为控制变量。\n\n在多元回归中，如果一个内生回归变量与其他回归变量相关，偏误将会“涂抹”或“溢出”，导致所有相关回归变量的估计量都不一致。我们来检查 $X_{t}$ 和 $W_{t}$ 之间的相关性：\n$$\n\\operatorname{Cov}(X_{t}, W_{t}) = \\operatorname{Cov}(\\gamma I_{t} + c H_{t} + \\epsilon^{X}_{t}, \\alpha H_{t} + \\eta_{t})\n$$\n利用双线性和独立性假设：\n$$\n\\operatorname{Cov}(X_{t}, W_{t}) = c\\alpha \\operatorname{Cov}(H_{t}, H_{t}) = c\\alpha \\operatorname{Var}(H_{t})\n$$\n这个协方差通常非零。因为回归变量 $W_{t}$ 是内生的，并且与回归变量 $X_{t}$ 相关，所以 $X_{t}$ 系数的 OLS 估计量 $\\hat{\\beta}_{1}$ 将是真实因果参数 $b$ 的一个有偏且不一致的估计量。残余内生性持续存在，因为带噪声的代理变量 $W_{t}$ 不能完全解释 $H_{t}$ 的混淆影响。测量噪声 $\\eta_{t}$ 阻止了干净的分离，从而创造了一个新的内生性来源。\n\n2. 工具变量 (IV) 补救方法与推导\n\n为了获得 $b$ 的一致估计，我们可以使用工具变量 (IV) 方法。外生输入 $I_{t}$ 是内生回归变量 $X_{t}$ 的一个合适的工具变量候选。为了使工具变量 $Z$ 在方程 $Y = bX + \\text{error}$ 中能有效估计 $X$ 对 $Y$ 的影响，它必须满足两个条件：\n\ni. **相关性**：工具变量必须与内生回归变量相关。我们必须验证 $\\operatorname{Cov}(I_{t}, X_{t}) \\neq 0$。\n$$\n\\operatorname{Cov}(I_{t}, X_{t}) = \\operatorname{Cov}(I_{t}, \\gamma I_{t} + c H_{t} + \\epsilon^{X}_{t})\n$$\n根据双线性和 $I_{t}$ 与 $H_{t}$ 及 $\\epsilon^{X}_{t}$ 的独立性：\n$$\n\\operatorname{Cov}(I_{t}, X_{t}) = \\gamma \\operatorname{Cov}(I_{t}, I_{t}) + c \\operatorname{Cov}(I_{t}, H_{t}) + \\operatorname{Cov}(I_{t}, \\epsilon^{X}_{t}) = \\gamma \\operatorname{Var}(I_{t})\n$$\n问题陈述 $I_{t}$ 是一个白噪声序列，因此 $\\operatorname{Var}(I_{t})  0$。假设 $\\gamma \\neq 0$（即输入对 $X$ 有影响），则相关性条件得到满足。所提供的数据 $\\operatorname{Cov}(I_{t}, X_{t}) = 1.6$ 从经验上证实了这种非零协方差。\n\nii. **排他性限制**：工具变量必须与结构方程的误差项不相关。结构方程是 $Y_{t+1} = b X_{t} + d H_{t} + \\epsilon^{Y}_{t+1}$，所以包含混淆变量的误差项是 $v_{t} = d H_{t} + \\epsilon^{Y}_{t+1}$。我们必须验证 $\\operatorname{Cov}(I_{t}, v_{t}) = 0$。\n$$\n\\operatorname{Cov}(I_{t}, v_{t}) = \\operatorname{Cov}(I_{t}, d H_{t} + \\epsilon^{Y}_{t+1})\n$$\n根据双线性和 $I_{t}$ 与 $H_{t}$ 及 $\\epsilon^{Y}_{t+1}$ 的独立性：\n$$\n\\operatorname{Cov}(I_{t}, v_{t}) = d \\operatorname{Cov}(I_{t}, H_{t}) + \\operatorname{Cov}(I_{t}, \\epsilon^{Y}_{t+1})\n$$\n问题陈述明确给出 $\\operatorname{Cov}(I_{t}, H_{t}) = 0$ 和 $\\operatorname{Cov}(I_{t}, \\epsilon^{Y}_{t+1}) = 0$。因此，$\\operatorname{Cov}(I_{t}, v_{t}) = 0$，排他性限制得到满足。工具变量 $I_{t}$ 仅通过其对 $X_{t}$ 的影响来影响 $Y_{t+1}$。\n\n有了一个有效的工具变量 $I_{t}$，我们可以推导出 $b$ 的一个识别公式。排他性限制意味着矩条件 $E[I_{t} v_{t}] = 0$，对于零均值变量，这等同于 $\\operatorname{Cov}(I_{t}, v_{t}) = 0$。我们从 $Y_{t+1}$ 的结构方程开始：\n$$\nY_{t+1} = b X_{t} + d H_{t} + \\epsilon^{Y}_{t+1}\n$$\n计算方程两边与工具变量 $I_{t}$ 的协方差：\n$$\n\\operatorname{Cov}(I_{t}, Y_{t+1}) = \\operatorname{Cov}(I_{t}, b X_{t} + d H_{t} + \\epsilon^{Y}_{t+1})\n$$\n利用协方差算子的线性性质：\n$$\n\\operatorname{Cov}(I_{t}, Y_{t+1}) = b \\operatorname{Cov}(I_{t}, X_{t}) + d \\operatorname{Cov}(I_{t}, H_{t}) + \\operatorname{Cov}(I_{t}, \\epsilon^{Y}_{t+1})\n$$\n应用排他性限制条件，其中 $\\operatorname{Cov}(I_{t}, H_{t}) = 0$ 且 $\\operatorname{Cov}(I_{t}, \\epsilon^{Y}_{t+1}) = 0$：\n$$\n\\operatorname{Cov}(I_{t}, Y_{t+1}) = b \\operatorname{Cov}(I_{t}, X_{t})\n$$\n由于相关性条件保证了 $\\operatorname{Cov}(I_{t}, X_{t}) \\neq 0$，我们可以解出 $b$。这就得到了 $b$ 的识别公式，它是一个可观测协方差的比率：\n$$\nb = \\frac{\\operatorname{Cov}(I_{t}, Y_{t+1})}{\\operatorname{Cov}(I_{t}, X_{t})}\n$$\n\n3. $b$ 的数值计算\n\n问题提供了以下总体协方差的经验极限：\n$$\n\\operatorname{Cov}(I_{t}, X_{t}) = 1.6\n$$\n$$\n\\operatorname{Cov}(I_{t}, Y_{t+1}) = 2.4\n$$\n将这些值代入我们为 $b$ 推导出的公式：\n$$\nb = \\frac{2.4}{1.6}\n$$\n$$\nb = \\frac{24}{16} = \\frac{3 \\times 8}{2 \\times 8} = \\frac{3}{2}\n$$\n$$\nb = 1.5\n$$\n从 $X$到 $Y$ 的耦合系数的数值是 $1.5$。",
            "answer": "$$\\boxed{1.5}$$"
        },
        {
            "introduction": "现实世界中的网络，从基因调控到金融市场，通常是稀疏的，即节点仅与少数其他节点直接相互作用。本练习将介绍组套索（group Lasso）正则化方法，这是一种专为从高维时间序列中恢复稀疏网络结构而设计的先进技术。您将通过实现一个近端梯度求解器，体验该方法如何自动选择相关的时间滞后，从而在组级别上进行变量选择，揭示潜在的稀疏连接模式。",
            "id": "4291666",
            "problem": "给定一个在 $N$ 个变量上定义的 $L$ 阶向量自回归模型，其基本框架如下：向量自回归 (VAR) 动态为 $$\\mathbf{x}_t = \\sum_{\\ell=1}^{L} \\mathbf{A}^{(\\ell)} \\mathbf{x}_{t-\\ell} + \\mathbf{e}_t,$$ 其中 $\\mathbf{x}_t \\in \\mathbb{R}^N$ 是离散时间 $t$ 的状态，$\\mathbf{A}^{(\\ell)} \\in \\mathbb{R}^{N \\times N}$ 是滞后-$\\ell$ 系数矩阵，其编码了来自滞后 $\\ell$ 的网络影响，而 $\\mathbf{e}_t \\in \\mathbb{R}^N$ 是一个均值为零且具有有限协方差的噪声项。重构问题将时间序列转化为一个多元回归问题：设 $T$ 为总序列长度，$L$ 为最大滞后，定义响应矩阵 $$\\mathbf{Y} \\in \\mathbb{R}^{(T-L) \\times N},$$ 其第 $t$ 行为 $\\mathbf{x}_{t+L}^\\top$，以及设计矩阵 $$\\mathbf{X} \\in \\mathbb{R}^{(T-L) \\times (N L)},$$ 其第 $t$ 行为 $\\left[\\mathbf{x}_{t+L-1}^\\top, \\mathbf{x}_{t+L-2}^\\top, \\ldots, \\mathbf{x}_{t}^\\top\\right]$ 的拼接。系数矩阵 $$\\mathbf{B} \\in \\mathbb{R}^{(N L) \\times N}$$ 将所有滞后矩阵按行分块堆叠，其中第 $\\ell$ 组（行 $((\\ell-1)N + 1)$ 到 $(\\ell N)$）对应于 $\\mathbf{A}^{(\\ell)}$。\n\n为了在滞后矩阵上引入联合（分组）稀疏性，从而实现稀疏网络重构，考虑以下组套索（group-Lasso）正则化目标函数：$$\\min_{\\mathbf{B} \\in \\mathbb{R}^{(N L) \\times N}} \\; \\frac{1}{2}\\left\\|\\mathbf{Y} - \\mathbf{X}\\mathbf{B}\\right\\|_F^2 + \\lambda \\sum_{\\ell=1}^{L} w_\\ell \\left\\|\\mathbf{B}_{[\\ell]}\\right\\|_F,$$ 其中 $\\lambda \\ge 0$ 是一个正则化参数，$w_\\ell  0$ 是组权重，$\\mathbf{B}_{[\\ell]}$ 表示 $\\mathbf{B}$ 中对应于滞后 $\\ell$ 的行块。弗罗贝尼乌斯范数（Frobenius norm）可以引导分组选择：整个滞后组 $\\ell$ 要么被选中（$\\left\\|\\mathbf{B}_{[\\ell]}\\right\\|_F  0$），要么不被选中（$\\mathbf{B}_{[\\ell]} = \\mathbf{0}$），从而能够在滞后层面上进行网络重构。\n\n你的任务是实现一个完整、可运行的程序，该程序：\n- 对于给定的 $(N, L, T)$、一组指定的滞后矩阵 $\\{\\mathbf{A}^{(\\ell)}\\}_{\\ell=1}^L$ 以及各向同性方差的高斯噪声 $\\mathbf{e}_t$，在稳定的 VAR 动态下模拟多元时间序列数据。\n- 构造如上定义的回归设计矩阵 $\\mathbf{X}$ 和响应矩阵 $\\mathbf{Y}$。\n- 使用基于原则的近端梯度法（proximal-gradient method）求解组套索正则化问题，该方法对弗罗贝尼乌斯范数进行块（组）软阈值化，并使用由平滑项梯度的李普希茨（Lipschitz）常数导出的步长。\n- 通过将 $\\left\\|\\mathbf{B}_{[\\ell]}\\right\\|_F$ 与一个小的数值容差进行比较，来确定哪些滞后组被选中，为每个滞后产生布尔值。\n\n此外，你必须从凸优化基础和卡罗需-库恩-塔克（Karush–Kuhn–Tucker, KKT）条件出发，推导滞后组在解中被联合选中（活跃）或排除（不活跃）的充分必要条件。\n\n实现程序以运行以下测试套件（所有情况都必须使用固定的随机种子 $42$ 进行模拟以确保可复现性），每个情况由 $(N, L, T, \\lambda, \\sigma_{\\text{noise}}, \\text{construction of }\\{\\mathbf{A}^{(\\ell)}\\})$ 指定：\n- 测试用例 $1$ (正常路径): $(N=3, L=2, T=800, \\lambda=0.12, \\sigma_{\\text{noise}}=0.05)$，其中 $\\mathbf{A}^{(1)}$ 生成为随机矩阵，其元素被缩放到 $[-0.2, 0.2]$ 范围内且谱半径严格小于 $1$，$\\mathbf{A}^{(2)} = \\mathbf{0}$。\n- 测试用例 $2$ (大正则化边缘情况): $(N=3, L=3, T=600, \\lambda=5.0, \\sigma_{\\text{noise}}=0.05)$，其中 $\\mathbf{A}^{(1)}$ 和 $\\mathbf{A}^{(3)}$ 生成为独立的随机矩阵，其元素分别位于 $[-0.05, 0.05]$ 和 $[-0.03, 0.03]$ 范围内，两者都被联合缩放以确保稳定性（伴随矩阵的谱半径严格小于 $1$），且 $\\mathbf{A}^{(2)} = \\mathbf{0}$。\n- 测试用例 $3$ (多个活跃滞后): $(N=5, L=3, T=1000, \\lambda=0.15, \\sigma_{\\text{noise}}=0.05)$，其中 $\\mathbf{A}^{(1)}$ 和 $\\mathbf{A}^{(3)}$ 为随机矩阵，其元素分别被缩放到 $[-0.18, 0.18]$ 和 $[-0.14, 0.14]$ 范围内，并被联合缩放以确保稳定性，且 $\\mathbf{A}^{(2)} = \\mathbf{0}$。\n- 测试用例 $4$ (边界情况 $\\lambda=0$): $(N=4, L=2, T=800, \\lambda=0.0, \\sigma_{\\text{noise}}=0.05)$，其中 $\\mathbf{A}^{(1)}$ 和 $\\mathbf{A}^{(2)}$ 为随机矩阵，其元素分别被缩放到 $[-0.10, 0.10]$ 和 $[-0.08, 0.08]$ 范围内，并被联合缩放以确保稳定性。\n\n在每种情况下，你必须：\n- 在收集最终的 $T$ 个时间步之前，使用 $L + 200$ 步的预烧期，以减轻瞬态效应。\n- 对所有 $\\ell \\in \\{1, \\ldots, L\\}$，使用组权重 $w_\\ell = 1$。\n- 当且仅当 $\\left\\|\\mathbf{B}_{[\\ell]}\\right\\|_F  10^{-4}$ 时，选择滞后组 $\\ell$。\n\n你的程序应该生成单行输出，按顺序包含每个测试用例的布尔值列表，该列表指示从 $\\ell=1$ 到 $\\ell=L$ 的滞后组的选择情况。格式必须是一个顶层的、逗号分隔的列表，其中包含每个案例的列表，不含空格，例如：$$[[\\text{True},\\text{False}],[\\text{False},\\text{True},\\text{False}],\\ldots].$$\n\n不涉及物理单位、角度单位或百分比；所有输出均为布尔值。最终输出的精确格式必须与指定的单行、带括号、逗号分隔的列表之列表格式相匹配，且不含空格。",
            "solution": "该问题要求在向量自回归 (VAR) 模型背景下，推导组套索估计量的选择条件，并实现一个数值解，以从模拟的时间序列数据中识别出相关的滞后。\n\n该问题在科学上基础扎实、内容自洽、算法明确。它代表了网络科学和高维统计学中一个标准且重要的任务。因此，该问题被认为是有效的，并将在下面提供完整的解决方案。\n\n### I. 理论基础：组套索的最优性条件\n\n目标是解决以下优化问题：\n$$\n\\min_{\\mathbf{B} \\in \\mathbb{R}^{(N L) \\times N}} \\; f(\\mathbf{B})\n$$\n其中目标函数 $f(\\mathbf{B})$ 定义为：\n$$\nf(\\mathbf{B}) = \\underbrace{\\frac{1}{2}\\left\\|\\mathbf{Y} - \\mathbf{X}\\mathbf{B}\\right\\|_F^2}_{g(\\mathbf{B})} + \\underbrace{\\lambda \\sum_{\\ell=1}^{L} w_\\ell \\left\\|\\mathbf{B}_{[\\ell]}\\right\\|_F}_{h(\\mathbf{B})}\n$$\n这里，$g(\\mathbf{B})$ 是一个平滑、凸、可微的函数（数据保真项），而 $h(\\mathbf{B})$ 是一个非平滑、凸、不可微的正则化项。矩阵 $\\mathbf{B}_{[\\ell]} \\in \\mathbb{R}^{N \\times N}$ 表示 $\\mathbf{B}$ 中对应于滞后 $\\ell$ 的行块。\n\n根据凸分析，一个点 $\\mathbf{B}^*$ 是 $f(\\mathbf{B})$ 的一个最小化点，当且仅当它满足一阶最优性条件。由于 $f$ 并非处处可微，该条件使用次微分表示：\n$$\n\\mathbf{0} \\in \\partial f(\\mathbf{B}^*)\n$$\n其中 $\\partial f(\\mathbf{B}^*)$ 是 $f$ 在 $\\mathbf{B}^*$ 处的次微分。由于 $g$ 是可微的，和的次微分是 $g$ 的梯度与 $h$ 的次微分之和：\n$$\n\\mathbf{0} \\in \\nabla g(\\mathbf{B}^*) + \\partial h(\\mathbf{B}^*)\n$$\n这等价于：\n$$\n-\\nabla g(\\mathbf{B}^*) \\in \\partial h(\\mathbf{B}^*)\n$$\n平滑项 $g(\\mathbf{B})$ 的梯度为：\n$$\n\\nabla g(\\mathbf{B}) = \\frac{\\partial}{\\partial \\mathbf{B}} \\frac{1}{2} \\text{tr}\\left((\\mathbf{Y} - \\mathbf{X}\\mathbf{B})^\\top(\\mathbf{Y} - \\mathbf{X}\\mathbf{B})\\right) = -\\mathbf{X}^\\top(\\mathbf{Y} - \\mathbf{X}\\mathbf{B})\n$$\n正则化项 $h(\\mathbf{B})$ 在各个滞后组 $\\mathbf{B}_{[\\ell]}$ 之间是可分的。因此，其次微分可以逐块分析。令 $h_\\ell(\\mathbf{B}_{[\\ell]}) = \\lambda w_\\ell \\left\\|\\mathbf{B}_{[\\ell]}\\right\\|_F$。$h_\\ell$ 在点 $\\mathbf{B}_{[\\ell]}$ 处的次微分为：\n$$\n\\partial h_\\ell(\\mathbf{B}_{[\\ell]}) = \\begin{cases}\n    \\left\\{ \\lambda w_\\ell \\frac{\\mathbf{B}_{[\\ell]}}{\\left\\|\\mathbf{B}_{[\\ell]}\\right\\|_F} \\right\\}  \\text{若 } \\mathbf{B}_{[\\ell]} \\neq \\mathbf{0} \\\\\n    \\left\\{ \\mathbf{U} \\in \\mathbb{R}^{N \\times N} : \\left\\|\\mathbf{U}\\right\\|_F \\le \\lambda w_\\ell \\right\\}  \\text{若 } \\mathbf{B}_{[\\ell]} = \\mathbf{0}\n\\end{cases}\n$$\n最优性条件 $-\\nabla g(\\mathbf{B}^*) \\in \\partial h(\\mathbf{B}^*)$ 现在可以对每个块 $\\ell$ 表示为：\n$$\n-(\\nabla g(\\mathbf{B}^*))_{[\\ell]} \\in \\partial h_\\ell(\\mathbf{B}_{[\\ell]}^*)\n$$\n其中 $(\\nabla g(\\mathbf{B}^*))_{[\\ell]}$ 是梯度矩阵中对应于变量块 $\\mathbf{B}_{[\\ell]}$ 的块。\n\n这导致了两种不同的条件，取决于在最优解 $\\mathbf{B}^*$ 中一个组是活跃的（非零）还是不活跃的（零）。\n\n**情况1：活跃组 ($\\mathbf{B}_{[\\ell]}^* \\neq \\mathbf{0}$)**\n在这种情况下，次微分是一个单点集。块 $\\ell$ 的最优性条件变为一个等式：\n$$\n-(\\nabla g(\\mathbf{B}^*))_{[\\ell]} = \\lambda w_\\ell \\frac{\\mathbf{B}_{[\\ell]}^*}{\\left\\|\\mathbf{B}_{[\\ell]}^*\\right\\|_F}\n$$\n对两边取弗罗贝尼乌斯范数，我们得到了一个组为活跃的必要条件：\n$$\n\\left\\|(\\nabla g(\\mathbf{B}^*))_{[\\ell]}\\right\\|_F = \\lambda w_\\ell\n$$\n\n**情况2：不活跃组 ($\\mathbf{B}_{[\\ell]}^* = \\mathbf{0}$)**\n在这种情况下，块 $\\ell$ 的最优性条件变为一个包含关系：\n$$\n-(\\nabla g(\\mathbf{B}^*))_{[\\ell]} \\in \\left\\{ \\mathbf{U} \\in \\mathbb{R}^{N \\times N} : \\left\\|\\mathbf{U}\\right\\|_F \\le \\lambda w_\\ell \\right\\}\n$$\n这等价于不等式：\n$$\n\\left\\|(\\nabla g(\\mathbf{B}^*))_{[\\ell]}\\right\\|_F \\le \\lambda w_\\ell\n$$\n在解 $\\mathbf{B}^*$ 处，一个不活跃块的梯度项为 $(\\nabla g(\\mathbf{B}^*))_{[\\ell]} = -(\\mathbf{X}^\\top\\mathbf{Y})_{[\\ell]} + (\\mathbf{X}^\\top\\mathbf{X}\\mathbf{B}^*)_{[\\ell]}$。\n\n**卡罗需-库恩-塔克（KKT）条件总结**\n结合这些情况，可以得到组选择的充分必要条件：\n1.  一个滞后组 $\\ell$ 是 **不活跃的** ($\\mathbf{B}_{[\\ell]}^* = \\mathbf{0}$)，当且仅当其在解处的对应偏导数块的弗罗贝尼乌斯范数不超过正则化阈值：$\\left\\|(\\nabla g(\\mathbf{B}^*))_{[\\ell]}\\right\\|_F \\le \\lambda w_\\ell$。\n2.  一个滞后组 $\\ell$ 是 **活跃的** ($\\mathbf{B}_{[\\ell]}^* \\neq \\mathbf{0}$)，当且仅当其偏导数块的弗罗贝尼乌斯范数恰好等于阈值：$\\left\\|(\\nabla g(\\mathbf{B}^*))_{[\\ell]}\\right\\|_F = \\lambda w_\\ell$。\n\n### II. 算法解：近端梯度法\n\n组套索问题可以通过迭代的近端梯度算法高效求解。该方法通过在目标函数的光滑部分上执行标准的梯度下降步骤后，应用一个“近端”（proximal）算子来处理不可微的正则化项。在第 $k$ 次迭代的更新规则是：\n$$\n\\mathbf{B}^{k+1} = \\text{prox}_{\\alpha h}(\\mathbf{B}^k - \\alpha \\nabla g(\\mathbf{B}^k))\n$$\n其中 $\\alpha  0$ 是步长。\n\n近端算子 $\\text{prox}_{\\alpha h}(\\mathbf{Z})$ 定义为：\n$$\n\\text{prox}_{\\alpha h}(\\mathbf{Z}) = \\arg\\min_{\\mathbf{U}} \\left( \\frac{1}{2}\\left\\|\\mathbf{U} - \\mathbf{Z}\\right\\|_F^2 + \\alpha h(\\mathbf{U}) \\right)\n$$\n由于 $h(\\mathbf{B})$ 的块可分性，近端算子可以独立地应用于每个滞后组：\n$$\n(\\text{prox}_{\\alpha h}(\\mathbf{Z}))_{[\\ell]} = \\text{prox}_{\\alpha h_\\ell}(\\mathbf{Z}_{[\\ell]})\n$$\n块级近端问题 $\\arg\\min_{\\mathbf{U}_\\ell} \\left( \\frac{1}{2}\\left\\|\\mathbf{U}_\\ell - \\mathbf{Z}_{[\\ell]}\\right\\|_F^2 + \\alpha \\lambda w_\\ell \\left\\|\\mathbf{U}_\\ell\\right\\|_F \\right)$ 的解由 **组软阈值** (group soft-thresholding) 算子给出：\n$$\n\\text{prox}_{\\alpha h_\\ell}(\\mathbf{Z}_{[\\ell]}) = \\left(1 - \\frac{\\alpha \\lambda w_\\ell}{\\left\\|\\mathbf{Z}_{[\\ell]}\\right\\|_F}\\right)_+ \\mathbf{Z}_{[\\ell]}\n$$\n其中 $(c)_+ = \\max(0, c)$。该算子将整个块 $\\mathbf{Z}_{[\\ell]}$ 向零收缩，如果收缩因子为负，则将其精确地设置为零。\n\n为保证收敛，步长 $\\alpha$ 必须满足 $0  \\alpha  2/L_g$，其中 $L_g$ 是 $\\nabla g(\\mathbf{B})$ 的李普希茨常数。常数 $L_g$ 是 $g(\\mathbf{B})$ 的海森矩阵（Hessian） $\\mathbf{X}^\\top\\mathbf{X}$ 的最大特征值。因此，$L_g = \\|\\mathbf{X}^\\top\\mathbf{X}\\|_2 = \\sigma_{\\max}(\\mathbf{X})^2$，其中 $\\sigma_{\\max}(\\mathbf{X})$ 是 $\\mathbf{X}$ 的最大奇异值。一个安全有效的步长选择是 $\\alpha = 1/L_g$。\n\n### III. VAR 模拟与稳定性\n如果一个 VAR($L$) 过程其关联的伴随矩阵 $\\mathbf{F}$ 的谱半径严格小于1，则该过程是稳定的。伴随矩阵定义为：\n$$\n\\mathbf{F} = \\begin{pmatrix}\n\\mathbf{A}^{(1)}  \\mathbf{A}^{(2)}  \\cdots  \\mathbf{A}^{(L-1)}  \\mathbf{A}^{(L)} \\\\\n\\mathbf{I}_N  \\mathbf{0}  \\cdots  \\mathbf{0}  \\mathbf{0} \\\\\n\\mathbf{0}  \\mathbf{I}_N  \\cdots  \\mathbf{0}  \\mathbf{0} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\\n\\mathbf{0}  \\mathbf{0}  \\cdots  \\mathbf{I}_N  \\mathbf{0}\n\\end{pmatrix} \\in \\mathbb{R}^{(NL) \\times (NL)}\n$$\n稳定性要求 $\\rho(\\mathbf{F}) = \\max_i |\\lambda_i(\\mathbf{F})|  1$。为了确保随机生成的系数矩阵 $\\mathbf{A}^{(\\ell)}$ 满足此条件，如果不满足，它们的范数将被按比例缩小。时间序列模拟将包含一个预烧期，以使过程在数据收集前达到其平稳分布。\n\n### IV. 实现概要\n对于每个测试用例，程序将按以下步骤进行：\n1.  **设置**：初始化参数 $(N, L, T, \\lambda, \\sigma_{\\text{noise}})$ 和一个使用固定种子的随机数生成器以保证可复现性。\n2.  **VAR系数生成**：按规定构建 $\\mathbf{A}^{(\\ell)}$ 矩阵。通过检查 $\\rho(\\mathbf{F})$ 并在 $\\rho(\\mathbf{F}) \\ge 1$ 时缩减所有 $\\mathbf{A}^{(\\ell)}$ 矩阵来确保稳定性。\n3.  **时间序列模拟**：生成一个长度为 $T + \\text{burn\\_in}$ 的时间序列，并丢弃初始的预烧期样本。\n4.  **矩阵构建**：构建响应矩阵 $\\mathbf{Y} \\in \\mathbb{R}^{(T-L) \\times N}$ 和设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{(T-L) \\times (NL)}$。\n5.  **组套索求解器**：使用理论推导的步长 $\\alpha = 1 / \\sigma_{\\max}(\\mathbf{X})^2$ 执行近端梯度算法，以找到最优的系数矩阵 $\\mathbf{B}$。\n6.  **滞后选择**：对于每个滞后 $\\ell \\in \\{1, \\ldots, L\\}$，计算弗罗贝尼乌斯范数 $\\|\\mathbf{B}_{[\\ell]}\\|_F$。如果该范数超过给定的容差 $10^{-4}$，则认为该组被选中。\n7.  **输出**：收集所有测试用例的布尔选择结果，并将其格式化为要求的单行字符串。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for VAR network reconstruction.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Happy path\n        {'N': 3, 'L': 2, 'T': 800, 'lambda': 0.12, 'sigma_noise': 0.05,\n         'A_specs': [{'lag': 1, 'scale': 0.2}]},\n        # Case 2: Large regularization\n        {'N': 3, 'L': 3, 'T': 600, 'lambda': 5.0, 'sigma_noise': 0.05,\n         'A_specs': [{'lag': 1, 'scale': 0.05}, {'lag': 3, 'scale': 0.03}]},\n        # Case 3: Multiple active lags\n        {'N': 5, 'L': 3, 'T': 1000, 'lambda': 0.15, 'sigma_noise': 0.05,\n         'A_specs': [{'lag': 1, 'scale': 0.18}, {'lag': 3, 'scale': 0.14}]},\n        # Case 4: No regularization (lambda=0)\n        {'N': 4, 'L': 2, 'T': 800, 'lambda': 0.0, 'sigma_noise': 0.05,\n         'A_specs': [{'lag': 1, 'scale': 0.10}, {'lag': 2, 'scale': 0.08}]},\n    ]\n\n    overall_results = []\n\n    for case in test_cases:\n        # Use a new generator for each case, seeded with 42, for independent reproducibility.\n        rng = np.random.default_rng(42)\n\n        N, L, T = case['N'], case['L'], case['T']\n        lambda_reg, sigma_noise = case['lambda'], case['sigma_noise']\n        A_specs = case['A_specs']\n\n        # 1. Generate VAR coefficient matrices and ensure stability\n        A_matrices = [np.zeros((N, N)) for _ in range(L)]\n        for spec in A_specs:\n            lag_idx = spec['lag'] - 1\n            scale = spec['scale']\n            A_matrices[lag_idx] = (rng.uniform(-1, 1, (N, N))) * scale\n\n        A_matrices = _ensure_stability(A_matrices, N, L)\n\n        # 2. Simulate VAR time series data\n        burn_in = L + 200\n        time_series = _simulate_var(A_matrices, N, L, T, sigma_noise, burn_in, rng)\n\n        # 3. Construct design (X) and response (Y) matrices\n        Y = time_series[L:]\n        T_eff = T - L\n        X = np.zeros((T_eff, N * L))\n        for t in range(T_eff):\n            # Concatenate [x_{t+L-1}, x_{t+L-2}, ..., x_{t}]\n            history_segment = time_series[t : t + L]\n            X[t, :] = history_segment[::-1].ravel()\n\n        # 4. Solve the group-Lasso problem\n        B_estimated = _solve_group_lasso(X, Y, N, L, lambda_reg)\n\n        # 5. Determine which lag groups are selected\n        lag_selection = []\n        for l in range(L):\n            row_start = l * N\n            row_end = (l + 1) * N\n            B_ell = B_estimated[row_start:row_end, :]\n            norm_B_ell = np.linalg.norm(B_ell, 'fro')\n            lag_selection.append(norm_B_ell > 1e-4)\n        \n        overall_results.append(lag_selection)\n\n    # Final print statement in the exact required format\n    # Example: [[True,False],[False,False,False],...]\n    print(str(overall_results).replace(\" \", \"\").replace(\"'\", \"\"))\n\n\ndef _ensure_stability(A_matrices, N, L):\n    \"\"\"\n    Ensures the VAR process is stable by scaling coefficients if needed.\n    \"\"\"\n    F = np.zeros((N * L, N * L))\n    for i in range(L):\n        F[0:N, i*N:(i+1)*N] = A_matrices[i]\n    if L > 1:\n        for i in range(L - 1):\n            F[(i+1)*N:(i+2)*N, i*N:(i+1)*N] = np.eye(N)\n    \n    max_eig = np.linalg.eigvals(F)\n    spectral_radius = np.max(np.abs(max_eig))\n    \n    if spectral_radius >= 1.0:\n        # Scale all matrices to ensure stability\n        scale_factor = 0.99 / spectral_radius\n        stable_A_matrices = [A * scale_factor for A in A_matrices]\n        return stable_A_matrices\n    return A_matrices\n\n\ndef _simulate_var(A_matrices, N, L, T, sigma_noise, burn_in, rng):\n    \"\"\"\n    Simulates a VAR(L) process.\n    \"\"\"\n    total_len = T + burn_in\n    series = np.zeros((total_len, N))\n    \n    for t in range(L, total_len):\n        x_t = np.zeros(N)\n        for l_idx in range(L):\n            x_t += A_matrices[l_idx] @ series[t - (l_idx + 1)]\n        noise = rng.normal(0, sigma_noise, N)\n        series[t] = x_t + noise\n        \n    return series[burn_in:]\n\n\ndef _solve_group_lasso(X, Y, N, L, lambda_reg, max_iter=1000, tol=1e-5):\n    \"\"\"\n    Solves the group-Lasso problem using proximal gradient descent.\n    \"\"\"\n    n_samples, n_features_flat = X.shape\n    \n    # Step size based on Lipschitz constant\n    s = np.linalg.svd(X, compute_uv=False)\n    lip_const = s[0] ** 2\n    alpha = 1.0 / lip_const\n\n    # Initialize coefficient matrix B\n    B = np.zeros((n_features_flat, Y.shape[1]))\n    w_ell = 1.0\n\n    for i in range(max_iter):\n        B_old = B.copy()\n        \n        grad = -X.T @ (Y - X @ B)\n        B_intermediate = B - alpha * grad\n        \n        for l in range(L):\n            row_start = l * N\n            row_end = (l + 1) * N\n            \n            Z_ell = B_intermediate[row_start:row_end, :]\n            norm_Z_ell = np.linalg.norm(Z_ell, 'fro')\n            \n            if norm_Z_ell > 0:\n                shrinkage = max(0, 1 - (alpha * lambda_reg * w_ell) / norm_Z_ell)\n                B[row_start:row_end, :] = shrinkage * Z_ell\n            else:\n                B[row_start:row_end, :] = 0\n                \n        rel_change = np.linalg.norm(B - B_old, 'fro') / (np.linalg.norm(B_old, 'fro') + 1e-10)\n        if rel_change  tol:\n            break\n            \n    return B\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}