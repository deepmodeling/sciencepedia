{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握图卷积网络（GCN），亲手计算是无可替代的方法。这个练习将引导你完成 GCN 单层前向传播的核心步骤。通过在一个简单的双节点图上进行计算 ，你将具体地理解对称归一化和特征聚合是如何共同作用，将邻居节点的信息融合到中心节点的表示中。",
            "id": "4278919",
            "problem": "考虑一个包含两个节点的无向、无权玩具图，其邻接矩阵为 $A=\\begin{pmatrix}0  1 \\\\ 1  0\\end{pmatrix}$。设节点特征矩阵为 $X=\\begin{pmatrix}1  -2 \\\\ 1  -2\\end{pmatrix}$，层权重矩阵为 $W=\\begin{pmatrix}2  1 \\\\ 1  3\\end{pmatrix}$。采用标准的图卷积网络 (GCN) 约定，即为图添加自环，并对得到的邻接矩阵使用对称归一化。定义单层传播规则为 $H'=\\hat{A}XW$，其中激活函数 $\\sigma$ 为恒等函数。\n\n仅使用谱图理论和复杂网络的核心定义，完成以下任务：\n- 为图添加自环，并计算相应的增广度矩阵 $\\tilde{D}$。\n- 计算对称归一化的、增广自环的邻接矩阵 $\\hat{A}$。\n- 计算单层输出 $H'$。\n\n最后，将节点表示的“尺度”定义为其欧几里得范数。对于在原始图中（添加自环前）度为 1 的节点 1，计算比率\n$$r=\\frac{\\|H'_{1,\\cdot}\\|_{2}}{\\|X_{1,\\cdot}W\\|_{2}}$$\n并将 $r$ 作为最终答案报告。无需四舍五入。将最终答案表示为精确的实数。",
            "solution": "用户提供了一个关于单层图卷积网络 (GCN) 的问题。我将首先验证问题陈述的有效性，如果有效，则继续给出完整解答。\n\n### 步骤 1：提取已知条件\n-   邻接矩阵: $A=\\begin{pmatrix}0  1 \\\\ 1  0\\end{pmatrix}$\n-   节点特征矩阵: $X=\\begin{pmatrix}1  -2 \\\\ 1  -2\\end{pmatrix}$\n-   层权重矩阵: $W=\\begin{pmatrix}2  1 \\\\ 1  3\\end{pmatrix}$\n-   GCN 结构: 为图添加自环，并对邻接矩阵使用对称归一化。\n-   传播规则: $H'=\\hat{A}XW$，其中激活函数 $\\sigma$ 为恒等函数。\n-   任务：\n    1.  添加自环后，计算增广度矩阵 $\\tilde{D}$。\n    2.  计算对称归一化的、增广自环的邻接矩阵 $\\hat{A}$。\n    3.  计算单层输出 $H'$。\n    4.  对于节点 1，计算比率 $r=\\frac{\\|H'_{1,\\cdot}\\|_{2}}{\\|X_{1,\\cdot}W\\|_{2}}$。\n-   最终答案要求: 将 $r$ 报告为精确的实数。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学或事实合理性**：该问题基于图卷积网络的既定理论，这是机器学习和网络科学中的一个标准课题。关于添加自环、对称归一化和层传播规则的定义都是常规且正确的。没有违反任何科学原理。\n2.  **不可形式化或不相关**：该问题是一个具体的数学练习，与 GCN 直接相关。它完全可以形式化。\n3.  **不完整或矛盾的设置**：问题提供了所有必要的矩阵（$A$、$X$、$W$）和操作定义。设置是完整且自洽的。\n4.  **不切实际或不可行**：矩阵和运算都很简单，并且在计算上是可行的。维度对于矩阵乘法是一致的。\n5.  **不适定或结构不良**：该问题是适定的，有一套清晰的指令，可以导出一个唯一、稳定且有意义的解。所使用的术语是标准且明确的。\n6.  **伪深刻、琐碎或同义反复**：虽然问题很简单，但它是一个有效的教学练习，用于测试 GCN 的基本机制。初始节点特征相同这一事实导致了简化（$H' = XW$），但计算这个结果仍然需要正确应用 GCN 传播规则。它不是一个同义反复。\n7.  **超出科学可验证范围**：结果可以通过直接计算进行数学验证。\n\n### 步骤 3：结论与行动\n问题有效。我现在将进行分步解答。\n\n该问题要求我们执行一系列与单层图卷积网络相关的计算。我们将遵循问题陈述中列出的任务。\n\n首先，我们为原始图添加自环。在数学上，这表示为将单位矩阵 $I$ 加到邻接矩阵 $A$ 上。得到的增广邻接矩阵记为 $\\tilde{A}$。\n$$ \\tilde{A} = A + I = \\begin{pmatrix}0  1 \\\\ 1  0\\end{pmatrix} + \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} = \\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix} $$\n\n其次，我们计算增广度矩阵 $\\tilde{D}$。这是一个对角矩阵，其中每个对角元素 $\\tilde{D}_{ii}$ 是节点 $i$ 在带自环的图中的度，它对应于 $\\tilde{A}$ 的第 $i$ 行元素之和。\n对于节点 1，度为 $\\tilde{d}_1 = 1 + 1 = 2$。\n对于节点 2，度为 $\\tilde{d}_2 = 1 + 1 = 2$。\n因此，增广度矩阵为：\n$$ \\tilde{D} = \\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix} $$\n\n第三，我们使用公式 $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$ 计算对称归一化的邻接矩阵 $\\hat{A}$。我们首先需要求出 $\\tilde{D}^{-1/2}$。\n$$ \\tilde{D}^{-1/2} = \\begin{pmatrix}2^{-1/2}  0 \\\\ 0  2^{-1/2}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{2}}\\end{pmatrix} $$\n现在我们可以计算 $\\hat{A}$：\n$$ \\hat{A} = \\begin{pmatrix}\\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{2}}\\end{pmatrix} \\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{2}}\\end{pmatrix} $$\n$$ \\hat{A} = \\begin{pmatrix}\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}\\end{pmatrix} \\begin{pmatrix}\\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{2}}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{2}\\end{pmatrix} $$\n\n第四，我们使用传播规则 $H' = \\hat{A}XW$ 计算单层输出 $H'$。为了计算方便，可以先计算乘积 $XW$。\n$$ XW = \\begin{pmatrix}1  -2 \\\\ 1  -2\\end{pmatrix} \\begin{pmatrix}2  1 \\\\ 1  3\\end{pmatrix} = \\begin{pmatrix}(1)(2) + (-2)(1)  (1)(1) + (-2)(3) \\\\ (1)(2) + (-2)(1)  (1)(1) + (-2)(3) \\end{pmatrix} $$\n$$ XW = \\begin{pmatrix}2 - 2  1 - 6 \\\\ 2 - 2  1 - 6\\end{pmatrix} = \\begin{pmatrix}0  -5 \\\\ 0  -5\\end{pmatrix} $$\n现在我们计算 $H' = \\hat{A}(XW)$：\n$$ H' = \\begin{pmatrix}\\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}0  -5 \\\\ 0  -5\\end{pmatrix} = \\begin{pmatrix}(\\frac{1}{2})(0) + (\\frac{1}{2})(0)  (\\frac{1}{2})(-5) + (\\frac{1}{2})(-5) \\\\ (\\frac{1}{2})(0) + (\\frac{1}{2})(0)  (\\frac{1}{2})(-5) + (\\frac{1}{2})(-5)\\end{pmatrix} $$\n$$ H' = \\begin{pmatrix}0  -10/2 \\\\ 0  -10/2\\end{pmatrix} = \\begin{pmatrix}0  -5 \\\\ 0  -5\\end{pmatrix} $$\n\n最后，我们计算比率 $r=\\frac{\\|H'_{1,\\cdot}\\|_{2}}{\\|X_{1,\\cdot}W\\|_{2}}$。\n分母涉及 $XW$ 第一行的欧几里得范数，记为 $X_{1,\\cdot}W$。根据我们的计算，$X_{1,\\cdot}W = \\begin{pmatrix}0  -5\\end{pmatrix}$。\n$$ \\|X_{1,\\cdot}W\\|_{2} = \\|\\begin{pmatrix}0  -5\\end{pmatrix}\\|_{2} = \\sqrt{0^2 + (-5)^2} = \\sqrt{25} = 5 $$\n分子涉及 $H'$ 第一行的欧几里得范数，记为 $H'_{1,\\cdot}$。根据我们的计算，$H'_{1,\\cdot} = \\begin{pmatrix}0  -5\\end{pmatrix}$。\n$$ \\|H'_{1,\\cdot}\\|_{2} = \\|\\begin{pmatrix}0  -5\\end{pmatrix}\\|_{2} = \\sqrt{0^2 + (-5)^2} = \\sqrt{25} = 5 $$\n比率 $r$ 为：\n$$ r = \\frac{5}{5} = 1 $$\n这个结果可以从概念上理解。由于两个节点的初始特征向量相同（$X_{1,\\cdot} = X_{2,\\cdot}$），并且图是连通的，对称归一化操作会对邻居的特征进行平均。$\\hat{A}$ 的每一行之和为 1。因此，将 $\\hat{A}$ 乘以一个所有行都相同的矩阵 $X$ 将得到相同的矩阵 $X$。也就是说，$\\hat{A}X = X$。因此，$H' = \\hat{A}XW = XW$。这意味着 $H'_{1,\\cdot} = (XW)_{1,\\cdot} = X_{1,\\cdot}W$。只要范数非零（本例中即是如此），两个相同向量的范数之比就是 $1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "在理解了 GCN 如何生成节点表示之后，下一步自然是探究它如何学习。本练习将通过手动执行一步梯度下降来揭示 GCN 的训练过程 。你将计算损失、推导梯度，并更新模型权重，从而深入理解模型是如何根据标注数据进行自我优化的。",
            "id": "4278966",
            "problem": "考虑一个包含 $3$ 个节点（标记为 $1$、$2$ 和 $3$）的简单无向图，其邻接矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$ 如下所示\n$$\nA \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}.\n$$\n定义添加了自环的邻接矩阵 $\\tilde{A} \\;=\\; A \\,+\\, I$，其中 $I$ 是 $3 \\times 3$ 的单位矩阵。令 $D$ 为对角度矩阵，其元素为 $D_{ii} \\;=\\; \\sum_{j=1}^{3} \\tilde{A}_{ij}$。考虑行归一化的传播算子 $\\hat{A} \\;=\\; D^{-1} \\tilde{A}$。每个节点 $i$ 具有单个标量特征，节点特征矩阵为 $X \\in \\mathbb{R}^{3 \\times 1}$，\n$$\nX \\;=\\; \\begin{pmatrix}\n2 \\\\\n0 \\\\\n-1\n\\end{pmatrix}.\n$$\n我们研究一个使用恒等激活函数的单层图卷积网络（GCN, Graph Convolutional Network），因此隐藏表示为 $H \\;=\\; \\hat{A} X$，logits 为 $Z \\;=\\; H W$（权重矩阵 $W \\in \\mathbb{R}^{1 \\times 2}$），并通过 softmax 函数获得节点级别的类别概率，\n$$\np_{i,c} \\;=\\; \\frac{\\exp\\!\\left(Z_{i,c}\\right)}{\\sum_{k=1}^{2} \\exp\\!\\left(Z_{i,k}\\right)} \\quad \\text{for} \\quad c \\in \\{1,2\\}.\n$$\n假设标记集为 $S \\;=\\; \\{1,\\,3\\}$，其独热（one-hot）标签为 $y_{1} \\;=\\; (1,\\,0)$ 和 $y_{3} \\;=\\; (0,\\,1)$，训练损失是标记节点上的平均交叉熵，\n$$\n\\mathcal{L}(W) \\;=\\; -\\,\\frac{1}{|S|} \\sum_{i \\in S} \\sum_{c=1}^{2} y_{i,c} \\,\\ln\\!\\big(p_{i,c}\\big).\n$$\n将权重初始化为 $W^{(0)} \\;=\\; \\begin{pmatrix} 0  0 \\end{pmatrix}$ 并使用学习率 $\\alpha \\;=\\; \\frac{2}{3}$。从上述定义出发，且不假设任何预先推导的训练公式，对 $W$ 执行单步梯度下降，使用标记节点来获得 $W^{(1)} \\;=\\; W^{(0)} \\,-\\, \\alpha \\,\\nabla_{W}\\mathcal{L}(W^{(0)})$。\n\n请明确计算：\n- 归一化的传播算子 $\\hat{A}$，\n- 隐藏表示 $H \\;=\\; \\hat{A} X$，\n- 节点 $1$ 和 $3$ 处的 logits 和 softmax 概率，\n- 梯度 $\\nabla_{W}\\mathcal{L}(W^{(0)})$，\n- 更新后的权重 $W^{(1)}$。\n\n您的最终答案必须是更新后的权重 $W^{(1)}$，以单行矩阵的形式给出。无需四舍五入，必须提供精确值。",
            "solution": "用户提供了一个问题陈述，在尝试解答之前需要进行验证。\n\n### 第一步：提取已知条件\n- **图结构**：一个包含 $3$ 个节点的简单无向图。\n- **邻接矩阵**：\n$$\nA = \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}\n$$\n- **传播算子定义**：\n  - 添加自环的邻接矩阵：$\\tilde{A} = A + I$，其中 $I$ 是 $3 \\times 3$ 的单位矩阵。\n  - 度矩阵：$D$ 是一个对角矩阵，其元素为 $D_{ii} = \\sum_{j=1}^{3} \\tilde{A}_{ij}$。\n  - 行归一化的传播算子：$\\hat{A} = D^{-1} \\tilde{A}$。\n- **节点特征**：节点特征矩阵为 $X \\in \\mathbb{R}^{3 \\times 1}$，如下所示\n$$\nX = \\begin{pmatrix}\n2 \\\\\n0 \\\\\n-1\n\\end{pmatrix}\n$$\n- **GCN 模型架构**：\n  - 一个使用恒等激活函数的单层 GCN。\n  - 隐藏表示：$H = \\hat{A} X$。\n  - Logits：$Z = H W$，权重矩阵为 $W \\in \\mathbb{R}^{1 \\times 2}$。\n  - 节点级别的类别概率 (softmax)：$p_{i,c} = \\frac{\\exp(Z_{i,c})}{\\sum_{k=1}^{2} \\exp(Z_{i,k})}$，其中 $c \\in \\{1,2\\}$。\n- **训练设置**：\n  - 标记集：$S = \\{1, 3\\}$。\n  - 独热（One-hot）标签：$y_1 = (1, 0)$ 和 $y_3 = (0, 1)$。\n  - 损失函数（平均交叉熵）：$\\mathcal{L}(W) = -\\frac{1}{|S|} \\sum_{i \\in S} \\sum_{c=1}^{2} y_{i,c} \\ln(p_{i,c})$。\n  - 初始权重：$W^{(0)} = \\begin{pmatrix} 0  0 \\end{pmatrix}$。\n  - 学习率：$\\alpha = \\frac{2}{3}$。\n- **任务**：执行单步梯度下降来计算 $W^{(1)} = W^{(0)} - \\alpha \\nabla_{W}\\mathcal{L}(W^{(0)})$。问题要求明确计算 $\\hat{A}$、$H$、标记集的 logits 和概率、梯度 $\\nabla_{W}\\mathcal{L}(W^{(0)})$ 以及更新后的权重 $W^{(1)}$。\n\n### 第二步：使用提取的已知条件进行验证\n- **科学依据**：该问题描述了一个标准的单层图卷积网络，这是机器学习中一个成熟的模型。所有的数学定义（softmax、交叉熵、梯度下降）都是标准且正确的。该问题在科学上是合理的。\n- **适定性**：所有必要的数据、矩阵（$A$、$X$）、标签（$y_1$、$y_3$）和参数（$W^{(0)}$、$\\alpha$）均已提供。模型架构和损失函数有明确定义。任务是一个特定的计算，存在唯一解。该问题是适定的。\n- **客观性**：问题使用精确的数学语言和符号陈述，没有主观或模糊的术语。\n\n### 第三步：结论与行动\n该问题有效。可以开始解答过程。\n\n### 解法\n\n我们将按顺序计算所要求的量。\n\n**1. 归一化的传播算子 $\\hat{A}$**\n\n首先，我们通过加上单位矩阵 $I$ 来为邻接矩阵 $A$ 增加自环。\n$$\n\\tilde{A} = A + I = \\begin{pmatrix} 0  1  0 \\\\ 1  0  1 \\\\ 0  1  0 \\end{pmatrix} + \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1  0 \\\\ 1  1  1 \\\\ 0  1  1 \\end{pmatrix}\n$$\n接下来，我们通过对 $\\tilde{A}$ 的行求和来计算对角度矩阵 $D$。\n$D_{11} = 1 + 1 + 0 = 2$\n$D_{22} = 1 + 1 + 1 = 3$\n$D_{33} = 0 + 1 + 1 = 2$\n$$\nD = \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  2 \\end{pmatrix}\n$$\n度矩阵的逆矩阵为：\n$$\nD^{-1} = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{1}{3}  0 \\\\ 0  0  \\frac{1}{2} \\end{pmatrix}\n$$\n最后，我们计算行归一化的传播算子 $\\hat{A} = D^{-1} \\tilde{A}$。\n$$\n\\hat{A} = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{1}{3}  0 \\\\ 0  0  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1  1  0 \\\\ 1  1  1 \\\\ 0  1  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  0 \\\\ \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3} \\\\ 0  \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}\n$$\n\n**2. 隐藏表示 $H$**\n\n隐藏表示 $H$ 是通过将传播算子 $\\hat{A}$ 应用于特征矩阵 $X$ 来计算的。\n$$\nH = \\hat{A} X = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  0 \\\\ \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3} \\\\ 0  \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}(2) + \\frac{1}{2}(0) + 0(-1) \\\\ \\frac{1}{3}(2) + \\frac{1}{3}(0) + \\frac{1}{3}(-1) \\\\ 0(2) + \\frac{1}{2}(0) + \\frac{1}{2}(-1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{3} \\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\n\n**3. 在 $W^{(0)}$ 处的 Logits 和 Softmax 概率**\n\nLogits $Z$ 按 $Z = H W$ 计算。使用初始权重 $W^{(0)} = \\begin{pmatrix} 0  0 \\end{pmatrix}$，logits 为：\n$$\nZ^{(0)} = H W^{(0)} = \\begin{pmatrix} 1 \\\\ \\frac{1}{3} \\\\ -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}\n$$\n标记节点 $1$ 和 $3$ 的 logits 为 $Z_{1}^{(0)} = \\begin{pmatrix} 0  0 \\end{pmatrix}$ 和 $Z_{3}^{(0)} = \\begin{pmatrix} 0  0 \\end{pmatrix}$。\n\n为节点 $i \\in \\{1, 3\\}$ 计算 softmax 概率 $p_{i,c}$。\n对于节点 1：\n$$\np_{1,c} = \\frac{\\exp(Z_{1,c}^{(0)})}{\\sum_{k=1}^{2} \\exp(Z_{1,k}^{(0)})} = \\frac{\\exp(0)}{\\exp(0)+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}\n$$\n因此，节点 $1$ 的概率向量为 $p_1 = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}$。\n同理，对于节点 3：\n$$\np_{3,c} = \\frac{\\exp(Z_{3,c}^{(0)})}{\\sum_{k=1}^{2} \\exp(Z_{3,k}^{(0)})} = \\frac{\\exp(0)}{\\exp(0)+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}\n$$\n节点 $3$ 的概率向量为 $p_3 = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}$。\n\n**4. 梯度 $\\nabla_{W}\\mathcal{L}(W^{(0)})$**\n\n损失函数为 $\\mathcal{L}(W) = -\\frac{1}{|S|} \\sum_{i \\in S} \\sum_{c=1}^{2} y_{i,c} \\ln(p_{i,c})$。\n对于单个样本 $i$，交叉熵损失关于 logits $Z_{ic}$ 的梯度是 $p_{ic} - y_{ic}$。对标记集 $S$ 进行平均，总损失 $\\mathcal{L}$ 的导数为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial Z_{ic}} = \\frac{1}{|S|} (p_{ic} - y_{ic}) \\quad \\text{for } i \\in S, \\text{ and } 0 \\text{ otherwise.}\n$$\n使用链式法则，关于权重 $W$ 的梯度为 $\\nabla_W \\mathcal{L} = H^T (\\nabla_Z \\mathcal{L})$，其中 $\\nabla_Z \\mathcal{L}$ 是由偏导数 $\\frac{\\partial \\mathcal{L}}{\\partial Z_{ic}}$ 构成的 $3 \\times 2$ 矩阵。\n我们定义一个误差矩阵 $E \\in \\mathbb{R}^{3 \\times 2}$，其中如果 $i \\in S$ 则 $E_{ic} = p_{ic} - y_{ic}$，否则 $E_{ic}=0$。那么 $\\nabla_W \\mathcal{L} = \\frac{1}{|S|} H^T E$。\n标记集为 $S = \\{1, 3\\}$，其中 $|S|=2$。\n对于节点 $1 \\in S$，$y_1 = \\begin{pmatrix} 1  0 \\end{pmatrix}$ 且 $p_1 = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}$。误差为 $E_1 = p_1 - y_1 = \\begin{pmatrix} -\\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}$。\n对于节点 $3 \\in S$，$y_3 = \\begin{pmatrix} 0  1 \\end{pmatrix}$ 且 $p_3 = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}$。误差为 $E_3 = p_3 - y_3 = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix}$。\n节点 $2 \\notin S$，因此其误差贡献为零。\n在 $W^{(0)}$ 处的误差矩阵 $E$ 为：\n$$\nE^{(0)} = \\begin{pmatrix} -\\frac{1}{2}  \\frac{1}{2} \\\\ 0  0 \\\\ \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix}\n$$\n转置后的隐藏表示矩阵为 $H^T = \\begin{pmatrix} 1  \\frac{1}{3}  -\\frac{1}{2} \\end{pmatrix}$。\n现在我们计算梯度：\n$$\n\\nabla_{W}\\mathcal{L}(W^{(0)}) = \\frac{1}{|S|} H^T E^{(0)} = \\frac{1}{2} \\begin{pmatrix} 1  \\frac{1}{3}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2}  \\frac{1}{2} \\\\ 0  0 \\\\ \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix}\n$$\n$$\n\\nabla_{W}\\mathcal{L}(W^{(0)}) = \\frac{1}{2} \\begin{pmatrix} 1(-\\frac{1}{2}) + \\frac{1}{3}(0) - \\frac{1}{2}(\\frac{1}{2})  & 1(\\frac{1}{2}) + \\frac{1}{3}(0) - \\frac{1}{2}(-\\frac{1}{2}) \\end{pmatrix}\n$$\n$$\n\\nabla_{W}\\mathcal{L}(W^{(0)}) = \\frac{1}{2} \\begin{pmatrix} -\\frac{1}{2} - \\frac{1}{4}  & \\frac{1}{2} + \\frac{1}{4} \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} -\\frac{3}{4}  & \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{8}  & \\frac{3}{8} \\end{pmatrix}\n$$\n\n**5. 更新后的权重 $W^{(1)}$**\n\n我们执行单步梯度下降：$W^{(1)} = W^{(0)} - \\alpha \\nabla_{W}\\mathcal{L}(W^{(0)})$。\n给定 $W^{(0)} = \\begin{pmatrix} 0  0 \\end{pmatrix}$ 和 $\\alpha = \\frac{2}{3}$：\n$$\nW^{(1)} = \\begin{pmatrix} 0  0 \\end{pmatrix} - \\frac{2}{3} \\begin{pmatrix} -\\frac{3}{8}  \\frac{3}{8} \\end{pmatrix}\n$$\n$$\nW^{(1)} = \\begin{pmatrix} -(\\frac{2}{3})(-\\frac{3}{8})  & -(\\frac{2}{3})(\\frac{3}{8}) \\end{pmatrix}\n$$\n$$\nW^{(1)} = \\begin{pmatrix} \\frac{6}{24}  & -\\frac{6}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  & -\\frac{1}{4} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{4}  & -\\frac{1}{4} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "标准的 GCN 模型在“同质性”假设下表现出色，即相连的节点倾向于相似。但如果这个假设不成立呢？这个高级练习旨在通过一个精心设计的“异质性”图来挑战你的理解，在这种图中，相连的节点具有不同的标签 。通过计算，你将亲眼见证 GCN 的平滑操作为何在这种情况下可能反而降低分类性能，从而对 GCN 的适用范围和局限性有更批判性的认识。",
            "id": "4278969",
            "problem": "考虑一个包含 $n=4$ 个节点的玩具网络，旨在凸显异质性。该网络由一个无向简单环图 $G$ 定义，其邻接矩阵 $A \\in \\mathbb{R}^{4 \\times 4}$ 在节点 $(1,2)$、$(2,3)$、$(3,4)$ 和 $(4,1)$ 之间有边。设节点特征矩阵为 $X \\in \\mathbb{R}^{4 \\times 2}$，其行向量为 $x_{1}=(2,\\,0.5)$、$x_{2}=(-2,\\,0.5)$、$x_{3}=(1.5,\\,-0.5)$ 和 $x_{4}=(-1.5,\\,-0.5)$。用向量 $u=(1,\\,0)^{\\top}$ 定义目标线性决策边界，用 $y_{i}=\\mathrm{sign}(u^{\\top}x_{i}) \\in \\{-1,\\,1\\}$（其中 $i \\in \\{1,2,3,4\\}$）定义真实标签。\n\n使用图卷积网络（GCNs）的标准重归一化技巧：$\\tilde{A}=A+I$、$\\tilde{D}=\\mathrm{diag}(\\tilde{A}\\mathbf{1})$，以及对称归一化传播算子 $\\hat{A}=\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$。考虑一个具有恒等非线性和单位权重矩阵的单个 GCN 层，因此变换后的表示由 $H^{(1)}=\\hat{A}X$ 给出。从邻接、度和对称归一化的核心定义出发，计算节点表示 $h_{i}^{(1)}$（其中 $i \\in \\{1,2,3,4\\}$），然后评估相对于目标决策边界 $u$ 的平均带符号间隔的变化，该变化定义为\n$$\n\\Delta m \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}h_{i}^{(1)} \\;-\\; \\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}x_{i}.\n$$\n解释为什么异质性会导致由 $\\hat{A}$ 实现的平滑与目标决策边界 $u$ 之间的错位，并以最简分数形式给出 $\\Delta m$ 的精确值。将您的最终答案表示为单个精确的有理数，无需四舍五入。",
            "solution": "该问题是适定的、有科学依据且自洽的。所有必要信息都已提供，可得到唯一解。我们遵循给定的定义来求解。\n\n首先，我们确定图结构和相关矩阵。该网络有 $n=4$ 个节点。它是一个无向简单环图，节点 $(1,2)$、$(2,3)$、$(3,4)$ 和 $(4,1)$ 之间有边。这意味着边集为 $E = \\{\\{1,2\\}, \\{2,3\\}, \\{3,4\\}, \\{4,1\\}\\}$。每个节点的度为 $2$。邻接矩阵 $A \\in \\mathbb{R}^{4 \\times 4}$ 的构造如下：如果节点 $i$ 和节点 $j$ 之间存在边，则 $A_{ij}=1$，否则 $A_{ij}=0$。\n\n$$\nA = \\begin{pmatrix}\n0  1  0  1 \\\\\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n1  0  1  0\n\\end{pmatrix}\n$$\n\n接下来，我们应用标准的 GCN 重归一化技巧。我们定义 $\\tilde{A} = A + I$，其中 $I$ 是 $4 \\times 4$ 的单位矩阵。这为每个节点添加了一个自环。\n\n$$\n\\tilde{A} = A + I = \\begin{pmatrix}\n0  1  0  1 \\\\\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n1  0  1  0\n\\end{pmatrix} + \\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n1  1  0  1 \\\\\n1  1  1  0 \\\\\n0  1  1  1 \\\\\n1  0  1  1\n\\end{pmatrix}\n$$\n\n矩阵 $\\tilde{D}$ 是与 $\\tilde{A}$ 对应的对角度矩阵。第 $i$ 个对角元素是 $\\tilde{d}_i = \\sum_j \\tilde{A}_{ij}$，即原始度 $d_i$ 加上自环的 $1$。由于每个节点的度都是 $d_i=2$，所有修改后的度都是 $\\tilde{d}_i = 2+1=3$。\n\n$$\n\\tilde{D} = \\mathrm{diag}(3, 3, 3, 3) = \\begin{pmatrix}\n3  0  0  0 \\\\\n0  3  0  0 \\\\\n0  0  3  0 \\\\\n0  0  0  3\n\\end{pmatrix} = 3I\n$$\n\n对称归一化传播算子为 $\\hat{A} = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$。由于 $\\tilde{D}=3I$，我们有 $\\tilde{D}^{-1/2} = (3I)^{-1/2} = \\frac{1}{\\sqrt{3}}I$。\n\n$$\n\\hat{A} = \\left(\\frac{1}{\\sqrt{3}}I\\right) \\tilde{A} \\left(\\frac{1}{\\sqrt{3}}I\\right) = \\frac{1}{3}\\tilde{A} = \\frac{1}{3}(A+I) = \\frac{1}{3}\\begin{pmatrix}\n1  1  0  1 \\\\\n1  1  1  0 \\\\\n0  1  1  1 \\\\\n1  0  1  1\n\\end{pmatrix}\n$$\n\n现在，我们确定真实标签 $y_i = \\mathrm{sign}(u^{\\top}x_{i})$。给定的特征向量为行向量 $x_1=(2,\\,0.5)$、$x_2=(-2,\\,0.5)$、$x_3=(1.5,\\,-0.5)$ 和 $x_4=(-1.5,\\,-0.5)$。决策边界向量为 $u=(1,\\,0)^{\\top}$。操作 $u^{\\top}x_i$ 提取特征向量 $x_i$ 的第一个分量。\n\n$y_1 = \\mathrm{sign}(u^{\\top}x_1) = \\mathrm{sign}(2) = 1$\n$y_2 = \\mathrm{sign}(u^{\\top}x_2) = \\mathrm{sign}(-2) = -1$\n$y_3 = \\mathrm{sign}(u^{\\top}x_3) = \\mathrm{sign}(1.5) = 1$\n$y_4 = \\mathrm{sign}(u^{\\top}x_4) = \\mathrm{sign}(-1.5) = -1$\n\n标签向量为 $y = (1, -1, 1, -1)^{\\top}$。我们可以观察到，图中的每条边都连接着具有相反标签的节点。例如，节点 $1$（标签 $1$）连接到节点 $2$（标签 $-1$）和节点 $4$（标签 $-1$）。这是一个完全异质性的情况。\n\n我们首先计算初始平均带符号间隔，$\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}x_{i}$。\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}x_{i} = \\frac{1}{4} \\left( y_1 u^{\\top}x_1 + y_2 u^{\\top}x_2 + y_3 u^{\\top}x_3 + y_4 u^{\\top}x_4 \\right) \\\\\n= \\frac{1}{4} \\left( (1)(2) + (-1)(-2) + (1)(1.5) + (-1)(-1.5) \\right) \\\\\n= \\frac{1}{4} \\left( 2 + 2 + 1.5 + 1.5 \\right) = \\frac{7}{4}\n$$\n\n接下来，我们计算变换后的节点表示 $H^{(1)} = \\hat{A}X$。节点 $i$ 的新表示 $h_i^{(1)}$ 是其自身特征及其邻居特征的加权平均值：$h_i^{(1)} = \\frac{1}{\\tilde{d}_i} (x_i + \\sum_{j \\in \\mathcal{N}(i)} x_j)$。由于对所有 $i$ 都有 $\\tilde{d}_i=3$：\n\n$h_1^{(1)} = \\frac{1}{3}(x_1 + x_2 + x_4) = \\frac{1}{3}\\left((2, 0.5) + (-2, 0.5) + (-1.5, -0.5)\\right) = \\frac{1}{3}(-1.5, 0.5) = (-\\frac{1}{2}, \\frac{1}{6})$\n$h_2^{(1)} = \\frac{1}{3}(x_2 + x_1 + x_3) = \\frac{1}{3}\\left((-2, 0.5) + (2, 0.5) + (1.5, -0.5)\\right) = \\frac{1}{3}(1.5, 0.5) = (\\frac{1}{2}, \\frac{1}{6})$\n$h_3^{(1)} = \\frac{1}{3}(x_3 + x_2 + x_4) = \\frac{1}{3}\\left((1.5, -0.5) + (-2, 0.5) + (-1.5, -0.5)\\right) = \\frac{1}{3}(-2, -0.5) = (-\\frac{2}{3}, -\\frac{1}{6})$\n$h_4^{(1)} = \\frac{1}{3}(x_4 + x_1 + x_3) = \\frac{1}{3}\\left((-1.5, -0.5) + (2, 0.5) + (1.5, -0.5)\\right) = \\frac{1}{3}(2, -0.5) = (\\frac{2}{3}, -\\frac{1}{6})$\n\n现在我们计算新的平均带符号间隔，$\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}h_{i}^{(1)}$。\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}h_{i}^{(1)} = \\frac{1}{4} \\left( y_1 u^{\\top}h_1^{(1)} + y_2 u^{\\top}h_2^{(1)} + y_3 u^{\\top}h_3^{(1)} + y_4 u^{\\top}h_4^{(1)} \\right) \\\\\n= \\frac{1}{4} \\left( (1)(-\\frac{1}{2}) + (-1)(\\frac{1}{2}) + (1)(-\\frac{2}{3}) + (-1)(\\frac{2}{3}) \\right) \\\\\n= \\frac{1}{4} \\left( -\\frac{1}{2} - \\frac{1}{2} - \\frac{2}{3} - \\frac{2}{3} \\right) = \\frac{1}{4} \\left( -1 - \\frac{4}{3} \\right) = \\frac{1}{4} \\left( -\\frac{7}{3} \\right) = -\\frac{7}{12}\n$$\n\n最后，我们计算平均带符号间隔的变化量 $\\Delta m$。\n$$\n\\Delta m = \\left(\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}h_{i}^{(1)}\\right) - \\left(\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}x_{i}\\right) \\\\\n= -\\frac{7}{12} - \\frac{7}{4} = -\\frac{7}{12} - \\frac{21}{12} = -\\frac{28}{12} = -\\frac{7}{3}\n$$\n\n$\\Delta m$ 的负值表明 GCN 层降低了分类性能。原因在于 GCN 的平滑机制与图的异质性结构之间的冲突。GCN 层在图上充当一个低通滤波器，将一个节点的特征与其邻居的特征进行平均。在同质性假设（即相连节点具有相似的特征和标签）下，此操作是有效的。在这种情况下，平均化会增强特定类别的特征。\n\n在本问题中，图是异质的：每个节点只与相反类别的节点相连。例如，节点 $1$ 的标签为 $1$，其特征 $x_1$ 的第一个分量是正数（$2$）。它的邻居节点 $2$ 和 $4$ 的标签为 $-1$，其特征的第一个分量是负数（$-2$ 和 $-1.5$）。GCN 层对这些特征进行平均，生成 $h_1^{(1)}$，其第一个分量为 $\\frac{1}{3}(2-2-1.5) = -0.5$。特征向量被拉到了由 $u$ 定义的决策边界的另一侧。这种在图上是局部的平滑操作是有害的，因为图中的邻居在特征空间中位于决策边界的两侧。因此，GCN 的特征聚合混合了来自不同类别的特征，降低了可分性，并且在本例中，甚至反转了每个节点的分类结果。图拓扑结构与特征分布之间的这种错位是性能下降的核心原因，由负值 $\\Delta m$ 量化体现。",
            "answer": "$$\\boxed{-\\frac{7}{3}}$$"
        }
    ]
}