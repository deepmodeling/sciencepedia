{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Graph Convolutional Networks (GCNs), we must begin with their fundamental building block: the propagation layer. This exercise guides you through a manual calculation of a single GCN layer's output, focusing on the crucial steps of adding self-loops and applying symmetric normalization. By working through this simple two-node graph, you will gain a concrete understanding of how the normalized adjacency matrix, $\\hat{A}$, acts as a neighborhood aggregator that updates a node's representation based on itself and its neighbors .",
            "id": "4278919",
            "problem": "Consider an undirected, unweighted toy graph with two nodes whose adjacency matrix is given by $A=\\begin{pmatrix}0 & 1 \\\\ 1 & 0\\end{pmatrix}$. Let the node feature matrix be $X=\\begin{pmatrix}1 & -2 \\\\ 1 & -2\\end{pmatrix}$ and the layer weight matrix be $W=\\begin{pmatrix}2 & 1 \\\\ 1 & 3\\end{pmatrix}$. Adopt the standard Graph Convolutional Network (GCN) convention in which self-loops are added to the graph and a symmetric normalization of the resulting adjacency is used. Define the one-layer propagation with the activation function $\\sigma$ equal to the identity as $H'=\\hat{A}XW$. \n\nUsing only core definitions from spectral graph theory and complex networks, do the following:\n- Add self-loops to the graph and compute the corresponding augmented degree matrix $\\tilde{D}$.\n- Compute the symmetrically normalized, self-loop–augmented adjacency $\\hat{A}$.\n- Compute the one-layer output $H'$.\n\nFinally, define the “scale” of a node’s representation as its Euclidean norm. For node $1$, which has degree one in the original graph (before adding self-loops), compute the ratio\n$$r=\\frac{\\|H'_{1,\\cdot}\\|_{2}}{\\|X_{1,\\cdot}W\\|_{2}}$$\nand report $r$ as your final answer. No rounding is required. Express the final answer as an exact real number.",
            "solution": "The problem requires us to perform a series of calculations related to a one-layer Graph Convolutional Network. We will follow the tasks outlined in the problem statement.\n\nFirst, we add self-loops to the original graph. This is mathematically represented by adding the identity matrix $I$ to the adjacency matrix $A$. The resulting augmented adjacency matrix is denoted as $\\tilde{A}$.\n$$ \\tilde{A} = A + I = \\begin{pmatrix}0 & 1 \\\\ 1 & 0\\end{pmatrix} + \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} = \\begin{pmatrix}1 & 1 \\\\ 1 & 1\\end{pmatrix} $$\n\nSecond, we compute the augmented degree matrix $\\tilde{D}$. This is a diagonal matrix where each diagonal element $\\tilde{D}_{ii}$ is the degree of node $i$ in the graph with self-loops, which corresponds to the sum of the elements in the $i$-th row of $\\tilde{A}$.\nFor node $1$, the degree is $\\tilde{d}_1 = 1 + 1 = 2$.\nFor node $2$, the degree is $\\tilde{d}_2 = 1 + 1 = 2$.\nTherefore, the augmented degree matrix is:\n$$ \\tilde{D} = \\begin{pmatrix}2 & 0 \\\\ 0 & 2\\end{pmatrix} $$\n\nThird, we compute the symmetrically normalized adjacency matrix $\\hat{A}$ using the formula $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$. We first need to find $\\tilde{D}^{-1/2}$.\n$$ \\tilde{D}^{-1/2} = \\begin{pmatrix}2^{-1/2} & 0 \\\\ 0 & 2^{-1/2}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}}\\end{pmatrix} $$\nNow we can compute $\\hat{A}$:\n$$ \\hat{A} = \\begin{pmatrix}\\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}}\\end{pmatrix} \\begin{pmatrix}1 & 1 \\\\ 1 & 1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}}\\end{pmatrix} $$\n$$ \\hat{A} = \\begin{pmatrix}\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\end{pmatrix} \\begin{pmatrix}\\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & \\frac{1}{\\sqrt{2}}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} $$\n\nFourth, we compute the one-layer output $H'$ using the propagation rule $H' = \\hat{A}XW$. It is computationally convenient to first compute the product $XW$.\n$$ XW = \\begin{pmatrix}1 & -2 \\\\ 1 & -2\\end{pmatrix} \\begin{pmatrix}2 & 1 \\\\ 1 & 3\\end{pmatrix} = \\begin{pmatrix}(1)(2) + (-2)(1) & (1)(1) + (-2)(3) \\\\ (1)(2) + (-2)(1) & (1)(1) + (-2)(3) \\end{pmatrix} $$\n$$ XW = \\begin{pmatrix}2 - 2 & 1 - 6 \\\\ 2 - 2 & 1 - 6\\end{pmatrix} = \\begin{pmatrix}0 & -5 \\\\ 0 & -5\\end{pmatrix} $$\nNow we compute $H' = \\hat{A}(XW)$:\n$$ H' = \\begin{pmatrix}\\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}0 & -5 \\\\ 0 & -5\\end{pmatrix} = \\begin{pmatrix}(\\frac{1}{2})(0) + (\\frac{1}{2})(0) & (\\frac{1}{2})(-5) + (\\frac{1}{2})(-5) \\\\ (\\frac{1}{2})(0) + (\\frac{1}{2})(0) & (\\frac{1}{2})(-5) + (\\frac{1}{2})(-5)\\end{pmatrix} $$\n$$ H' = \\begin{pmatrix}0 & -5 \\\\ 0 & -5\\end{pmatrix} $$\n\nFinally, we compute the ratio $r=\\frac{\\|H'_{1,\\cdot}\\|_{2}}{\\|X_{1,\\cdot}W\\|_{2}}$.\nThe denominator involves the Euclidean norm of the first row of $XW$, denoted $X_{1,\\cdot}W$. From our calculation, $X_{1,\\cdot}W = \\begin{pmatrix}0 & -5\\end{pmatrix}$.\n$$ \\|X_{1,\\cdot}W\\|_{2} = \\|\\begin{pmatrix}0 & -5\\end{pmatrix}\\|_{2} = \\sqrt{0^2 + (-5)^2} = \\sqrt{25} = 5 $$\nThe numerator involves the Euclidean norm of the first row of $H'$, denoted $H'_{1,\\cdot}$. From our calculation, $H'_{1,\\cdot} = \\begin{pmatrix}0 & -5\\end{pmatrix}$.\n$$ \\|H'_{1,\\cdot}\\|_{2} = \\|\\begin{pmatrix}0 & -5\\end{pmatrix}\\|_{2} = \\sqrt{0^2 + (-5)^2} = \\sqrt{25} = 5 $$\nThe ratio $r$ is:\n$$ r = \\frac{5}{5} = 1 $$\nThis result can be understood conceptually. Since the initial feature vectors for both nodes are identical ($X_{1,\\cdot} = X_{2,\\cdot}$), and the graph is connected, the symmetric normalization operation averages the features of neighbors. Each row of $\\hat{A}$ sums to $1$. Therefore, multiplying $\\hat{A}$ by a matrix $X$ with identical rows will result in the same matrix $X$. That is, $\\hat{A}X = X$. Consequently, $H' = \\hat{A}XW = XW$. This implies $H'_{1,\\cdot} = (XW)_{1,\\cdot} = X_{1,\\cdot}W$. The ratio of the norms of two identical vectors is $1$, provided the norm is non-zero, which is the case here.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "A GCN's ability to perform tasks stems from its capacity to learn from data. This practice moves beyond the forward pass to demystify the training process by guiding you through a single step of gradient descent by hand. You will calculate the cross-entropy loss for a partially labeled graph and compute the gradient with respect to the model's weights, ultimately updating them to improve performance . This foundational exercise reveals the mechanics of how a GCN adjusts its parameters to minimize prediction error.",
            "id": "4278966",
            "problem": "Consider a simple undirected graph with $3$ nodes labeled $1$, $2$, and $3$ and adjacency matrix $A \\in \\mathbb{R}^{3 \\times 3}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\nDefine the self-loop augmented adjacency $\\tilde{A} \\;=\\; A \\,+\\, I$, where $I$ is the $3 \\times 3$ identity matrix. Let $D$ be the diagonal degree matrix with entries $D_{ii} \\;=\\; \\sum_{j=1}^{3} \\tilde{A}_{ij}$. Consider the row-normalized propagation operator $\\hat{A} \\;=\\; D^{-1} \\tilde{A}$. Each node $i$ has a single scalar feature, and the node-feature matrix is $X \\in \\mathbb{R}^{3 \\times 1}$,\n$$\nX \\;=\\; \\begin{pmatrix}\n2 \\\\\n0 \\\\\n-1\n\\end{pmatrix}.\n$$\nWe study a one-layer Graph Convolutional Network (GCN) with identity activation, so the hidden representation is $H \\;=\\; \\hat{A} X$, the logits are $Z \\;=\\; H W$ with weight matrix $W \\in \\mathbb{R}^{1 \\times 2}$, and the node-wise class probabilities are obtained with the softmax,\n$$\np_{i,c} \\;=\\; \\frac{\\exp\\!\\left(Z_{i,c}\\right)}{\\sum_{k=1}^{2} \\exp\\!\\left(Z_{i,k}\\right)} \\quad \\text{for} \\quad c \\in \\{1,2\\}.\n$$\nAssume the labeled set is $S \\;=\\; \\{1,\\,3\\}$ with one-hot labels $y_{1} \\;=\\; (1,\\,0)$ and $y_{3} \\;=\\; (0,\\,1)$, and the training loss is the average cross-entropy over labeled nodes,\n$$\n\\mathcal{L}(W) \\;=\\; -\\,\\frac{1}{|S|} \\sum_{i \\in S} \\sum_{c=1}^{2} y_{i,c} \\,\\ln\\!\\big(p_{i,c}\\big).\n$$\nInitialize the weights at $W^{(0)} \\;=\\; \\begin{pmatrix} 0 & 0 \\end{pmatrix}$ and use a learning rate $\\alpha \\;=\\; \\frac{2}{3}$. Starting from the definitions above and without assuming any pre-derived training formulas, perform a single gradient descent step on $W$ using the labeled nodes to obtain $W^{(1)} \\;=\\; W^{(0)} \\,-\\, \\alpha \\,\\nabla_{W}\\mathcal{L}(W^{(0)})$. \n\nCompute explicitly:\n- the normalized propagation operator $\\hat{A}$,\n- the hidden representation $H \\;=\\; \\hat{A} X$,\n- the logits and softmax probabilities at nodes $1$ and $3$,\n- the gradient $\\nabla_{W}\\mathcal{L}(W^{(0)})$,\n- the updated weights $W^{(1)}$.\n\nYour final answer must be the updated weights $W^{(1)}$ given as a single row matrix. No rounding is required, and you must provide exact values.",
            "solution": "We will compute the requested quantities in sequence.\n\n**1. Normalized Propagation Operator $\\hat{A}$**\n\nFirst, we augment the adjacency matrix $A$ with self-loops by adding the identity matrix $I$.\n$$\n\\tilde{A} = A + I = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nNext, we compute the diagonal degree matrix $D$ by summing the rows of $\\tilde{A}$.\n$D_{11} = 1 + 1 + 0 = 2$\n$D_{22} = 1 + 1 + 1 = 3$\n$D_{33} = 0 + 1 + 1 = 2$\n$$\nD = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}\n$$\nThe inverse of the degree matrix is:\n$$\nD^{-1} = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{3} & 0 \\\\ 0 & 0 & \\frac{1}{2} \\end{pmatrix}\n$$\nFinally, we compute the row-normalized propagation operator $\\hat{A} = D^{-1} \\tilde{A}$.\n$$\n\\hat{A} = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{3} & 0 \\\\ 0 & 0 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}\n$$\n\n**2. Hidden Representation $H$**\n\nThe hidden representation $H$ is computed by applying the propagation operator $\\hat{A}$ to the feature matrix $X$.\n$$\nH = \\hat{A} X = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}(2) + \\frac{1}{2}(0) + 0(-1) \\\\ \\frac{1}{3}(2) + \\frac{1}{3}(0) + \\frac{1}{3}(-1) \\\\ 0(2) + \\frac{1}{2}(0) + \\frac{1}{2}(-1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{3} \\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\n\n**3. Logits and Softmax Probabilities at $W^{(0)}$**\n\nThe logits $Z$ are calculated as $Z = H W$. With the initial weights $W^{(0)} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}$, the logits are:\n$$\nZ^{(0)} = H W^{(0)} = \\begin{pmatrix} 1 \\\\ \\frac{1}{3} \\\\ -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe logits for the labeled nodes $1$ and $3$ are $Z_{1}^{(0)} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}$ and $Z_{3}^{(0)} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}$.\n\nThe softmax probabilities $p_{i,c}$ are computed for nodes $i \\in \\{1, 3\\}$.\nFor node $1$:\n$$\np_{1,c} = \\frac{\\exp(Z_{1,c}^{(0)})}{\\sum_{k=1}^{2} \\exp(Z_{1,k}^{(0)})} = \\frac{\\exp(0)}{\\exp(0)+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}\n$$\nSo, the probability vector for node $1$ is $p_1 = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$.\nSimilarly, for node $3$:\n$$\np_{3,c} = \\frac{\\exp(Z_{3,c}^{(0)})}{\\sum_{k=1}^{2} \\exp(Z_{3,k}^{(0)})} = \\frac{\\exp(0)}{\\exp(0)+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}\n$$\nThe probability vector for node $3$ is $p_3 = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$.\n\n**4. Gradient $\\nabla_{W}\\mathcal{L}(W^{(0)})$}\n\nThe loss is $\\mathcal{L}(W) = -\\frac{1}{|S|} \\sum_{i \\in S} \\sum_{c=1}^{2} y_{i,c} \\ln(p_{i,c})$.\nThe gradient of the cross-entropy loss with respect to the logits $Z_{ic}$ for a single sample $i$ is $p_{ic} - y_{ic}$. Averaging over the labeled set $S$, the derivative of the total loss $\\mathcal{L}$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial Z_{ic}} = \\frac{1}{|S|} (p_{ic} - y_{ic}) \\quad \\text{for } i \\in S, \\text{ and } 0 \\text{ otherwise.}\n$$\nUsing the chain rule, the gradient with respect to the weights $W$ is $\\nabla_W \\mathcal{L} = H^T (\\nabla_Z \\mathcal{L})$, where $\\nabla_Z \\mathcal{L}$ is the $3 \\times 2$ matrix of partial derivatives $\\frac{\\partial \\mathcal{L}}{\\partial Z_{ic}}$.\nLet's define an error matrix $E \\in \\mathbb{R}^{3 \\times 2}$ where $E_{ic} = p_{ic} - y_{ic}$ if $i \\in S$ and $E_{ic}=0$ otherwise. Then $\\nabla_W \\mathcal{L} = \\frac{1}{|S|} H^T E$.\nThe labeled set is $S = \\{1, 3\\}$ with $|S|=2$.\nFor node $1 \\in S$, $y_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and $p_1 = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$. The error is $E_1 = p_1 - y_1 = \\begin{pmatrix} -\\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$.\nFor node $3 \\in S$, $y_3 = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$ and $p_3 = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$. The error is $E_3 = p_3 - y_3 = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix}$.\nNode $2 \\notin S$, so its error contribution is zero.\nThe error matrix $E$ at $W^{(0)}$ is:\n$$\nE^{(0)} = \\begin{pmatrix} -\\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\\\ \\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix}\n$$\nThe transposed hidden representation matrix is $H^T = \\begin{pmatrix} 1 & \\frac{1}{3} & -\\frac{1}{2} \\end{pmatrix}$.\nNow we compute the gradient:\n$$\n\\nabla_{W}\\mathcal{L}(W^{(0)}) = \\frac{1}{|S|} H^T E^{(0)} = \\frac{1}{2} \\begin{pmatrix} 1 & \\frac{1}{3} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\\\ \\frac{1}{2} & -\\frac{1}{2} \\end{pmatrix}\n$$\n$$\n\\nabla_{W}\\mathcal{L}(W^{(0)}) = \\frac{1}{2} \\begin{pmatrix} 1(-\\frac{1}{2}) + \\frac{1}{3}(0) - \\frac{1}{2}(\\frac{1}{2}) & 1(\\frac{1}{2}) + \\frac{1}{3}(0) - \\frac{1}{2}(-\\frac{1}{2}) \\end{pmatrix}\n$$\n$$\n\\nabla_{W}\\mathcal{L}(W^{(0)}) = \\frac{1}{2} \\begin{pmatrix} -\\frac{1}{2} - \\frac{1}{4} & \\frac{1}{2} + \\frac{1}{4} \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} -\\frac{3}{4} & \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{8} & \\frac{3}{8} \\end{pmatrix}\n$$\n\n**5. Updated Weights $W^{(1)}$**\n\nWe perform a single gradient descent step: $W^{(1)} = W^{(0)} - \\alpha \\nabla_{W}\\mathcal{L}(W^{(0)})$.\nGiven $W^{(0)} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}$ and $\\alpha = \\frac{2}{3}$:\n$$\nW^{(1)} = \\begin{pmatrix} 0 & 0 \\end{pmatrix} - \\frac{2}{3} \\begin{pmatrix} -\\frac{3}{8} & \\frac{3}{8} \\end{pmatrix}\n$$\n$$\nW^{(1)} = \\begin{pmatrix} -(\\frac{2}{3})(-\\frac{3}{8}) & -(\\frac{2}{3})(\\frac{3}{8}) \\end{pmatrix}\n$$\n$$\nW^{(1)} = \\begin{pmatrix} \\frac{6}{24} & -\\frac{6}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{4} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{4} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The standard GCN architecture excels when connected nodes are similar, a principle known as homophily. But what happens when connected nodes are fundamentally different? This exercise explores the important concept of heterophily, a scenario where the GCN's inherent smoothing mechanism can be counterproductive . By computing the change in classification margin on a carefully constructed heterophilous graph, you will see firsthand why the GCN's feature averaging can degrade performance, encouraging a critical perspective on the model's underlying assumptions.",
            "id": "4278969",
            "problem": "Consider a toy network with $n=4$ nodes intended to highlight heterophily, defined by the undirected simple cycle graph $G$ whose adjacency matrix $A \\in \\mathbb{R}^{4 \\times 4}$ has edges between nodes $(1,2)$, $(2,3)$, $(3,4)$, and $(4,1)$. Let the node feature matrix be $X \\in \\mathbb{R}^{4 \\times 2}$ with rows $x_{1}=(2,\\,0.5)$, $x_{2}=(-2,\\,0.5)$, $x_{3}=(1.5,\\,-0.5)$, and $x_{4}=(-1.5,\\,-0.5)$. Define the target linear decision boundary by the vector $u=(1,\\,0)^{\\top}$ and ground-truth labels by $y_{i}=\\mathrm{sign}(u^{\\top}x_{i}) \\in \\{-1,\\,1\\}$ for $i \\in \\{1,2,3,4\\}$.\n\nUse the standard renormalization trick for Graph Convolutional Networks (GCNs): $\\tilde{A}=A+I$, $\\tilde{D}=\\mathrm{diag}(\\tilde{A}\\mathbf{1})$, and the symmetrically normalized propagation operator $\\hat{A}=\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$. Consider a single GCN layer with identity nonlinearity and identity weight matrix, so the transformed representations are given by $H^{(1)}=\\hat{A}X$. Starting from the core definitions of adjacency, degree, and symmetric normalization, compute the node representations $h_{i}^{(1)}$ for $i \\in \\{1,2,3,4\\}$, and then evaluate the change in the average signed margin with respect to the target decision boundary $u$, defined by\n$$\n\\Delta m \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}h_{i}^{(1)} \\;-\\; \\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}x_{i}.\n$$\nExplain why heterophily can induce a misalignment between the smoothing effected by $\\hat{A}$ and the target decision boundary $u$, and provide the exact value of $\\Delta m$ in simplest fractional form. Express your final answer as a single exact rational number. No rounding is required.",
            "solution": "First, we establish the graph structure and the associated matrices. The network has $n=4$ nodes. It is an undirected simple cycle graph with edges between nodes $(1,2)$, $(2,3)$, $(3,4)$, and $(4,1)$. This means the set of edges is $E = \\{\\{1,2\\}, \\{2,3\\}, \\{3,4\\}, \\{4,1\\}\\}$. Each node has a degree of $2$. The adjacency matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is constructed as follows: $A_{ij}=1$ if an edge exists between node $i$ and node $j$, and $A_{ij}=0$ otherwise.\n\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0\n\\end{pmatrix}\n$$\n\nNext, we apply the standard GCN renormalization trick. We define $\\tilde{A} = A + I$, where $I$ is the $4 \\times 4$ identity matrix. This adds a self-loop to each node.\n\n$$\n\\tilde{A} = A + I = \\begin{pmatrix}\n0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0\n\\end{pmatrix} + \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 1 & 0 & 1 \\\\\n1 & 1 & 1 & 0 \\\\\n0 & 1 & 1 & 1 \\\\\n1 & 0 & 1 & 1\n\\end{pmatrix}\n$$\n\nThe matrix $\\tilde{D}$ is the diagonal degree matrix corresponding to $\\tilde{A}$. The $i$-th diagonal element is $\\tilde{d}_i = \\sum_j \\tilde{A}_{ij}$, which is the original degree $d_i$ plus $1$ for the self-loop. Since every node has a degree of $d_i=2$, all modified degrees are $\\tilde{d}_i = 2+1=3$.\n\n$$\n\\tilde{D} = \\mathrm{diag}(3, 3, 3, 3) = \\begin{pmatrix}\n3 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 3\n\\end{pmatrix} = 3I\n$$\n\nThe symmetrically normalized propagation operator is $\\hat{A} = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$. Since $\\tilde{D}=3I$, we have $\\tilde{D}^{-1/2} = (3I)^{-1/2} = \\frac{1}{\\sqrt{3}}I$.\n\n$$\n\\hat{A} = \\left(\\frac{1}{\\sqrt{3}}I\\right) \\tilde{A} \\left(\\frac{1}{\\sqrt{3}}I\\right) = \\frac{1}{3}\\tilde{A} = \\frac{1}{3}(A+I) = \\frac{1}{3}\\begin{pmatrix}\n1 & 1 & 0 & 1 \\\\\n1 & 1 & 1 & 0 \\\\\n0 & 1 & 1 & 1 \\\\\n1 & 0 & 1 & 1\n\\end{pmatrix}\n$$\n\nNow, we determine the ground-truth labels $y_i = \\mathrm{sign}(u^{\\top}x_{i})$. The feature vectors are given as rows $x_1=(2,\\,0.5)$, $x_2=(-2,\\,0.5)$, $x_3=(1.5,\\,-0.5)$, and $x_4=(-1.5,\\,-0.5)$. The decision boundary vector is $u=(1,\\,0)^{\\top}$. The operation $u^{\\top}x_i$ extracts the first component of the feature vector $x_i$.\n\n$y_1 = \\mathrm{sign}(u^{\\top}x_1) = \\mathrm{sign}(2) = 1$\n$y_2 = \\mathrm{sign}(u^{\\top}x_2) = \\mathrm{sign}(-2) = -1$\n$y_3 = \\mathrm{sign}(u^{\\top}x_3) = \\mathrm{sign}(1.5) = 1$\n$y_4 = \\mathrm{sign}(u^{\\top}x_4) = \\mathrm{sign}(-1.5) = -1$\n\nThe label vector is $y = (1, -1, 1, -1)^{\\top}$. We can observe that every edge in the graph connects nodes with opposite labels. For instance, node $1$ (label $1$) is connected to node $2$ (label $-1$) and node $4$ (label $-1$). This is a case of perfect heterophily.\n\nWe first compute the initial average signed margin, $\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}x_{i}$.\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}x_{i} = \\frac{1}{4} \\left( y_1 u^{\\top}x_1 + y_2 u^{\\top}x_2 + y_3 u^{\\top}x_3 + y_4 u^{\\top}x_4 \\right) \\\\\n= \\frac{1}{4} \\left( (1)(2) + (-1)(-2) + (1)(1.5) + (-1)(-1.5) \\right) \\\\\n= \\frac{1}{4} \\left( 2 + 2 + 1.5 + 1.5 \\right) = \\frac{7}{4}\n$$\n\nNext, we compute the transformed node representations $H^{(1)} = \\hat{A}X$. The new representation for node $i$, $h_i^{(1)}$, is a weighted average of its own features and its neighbors' features: $h_i^{(1)} = \\frac{1}{\\tilde{d}_i} (x_i + \\sum_{j \\in \\mathcal{N}(i)} x_j)$. Since $\\tilde{d}_i=3$ for all $i$:\n\n$h_1^{(1)} = \\frac{1}{3}(x_1 + x_2 + x_4) = \\frac{1}{3}\\left((2, 0.5) + (-2, 0.5) + (-1.5, -0.5)\\right) = \\frac{1}{3}(-1.5, 0.5) = (-\\frac{1}{2}, \\frac{1}{6})$\n$h_2^{(1)} = \\frac{1}{3}(x_2 + x_1 + x_3) = \\frac{1}{3}\\left((-2, 0.5) + (2, 0.5) + (1.5, -0.5)\\right) = \\frac{1}{3}(1.5, 0.5) = (\\frac{1}{2}, \\frac{1}{6})$\n$h_3^{(1)} = \\frac{1}{3}(x_3 + x_2 + x_4) = \\frac{1}{3}\\left((1.5, -0.5) + (-2, 0.5) + (-1.5, -0.5)\\right) = \\frac{1}{3}(-2, -0.5) = (-\\frac{2}{3}, -\\frac{1}{6})$\n$h_4^{(1)} = \\frac{1}{3}(x_4 + x_1 + x_3) = \\frac{1}{3}\\left((-1.5, -0.5) + (2, 0.5) + (1.5, -0.5)\\right) = \\frac{1}{3}(2, -0.5) = (\\frac{2}{3}, -\\frac{1}{6})$\n\nNow we compute the new average signed margin, $\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}h_{i}^{(1)}$.\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}h_{i}^{(1)} = \\frac{1}{4} \\left( y_1 u^{\\top}h_1^{(1)} + y_2 u^{\\top}h_2^{(1)} + y_3 u^{\\top}h_3^{(1)} + y_4 u^{\\top}h_4^{(1)} \\right) \\\\\n= \\frac{1}{4} \\left( (1)(-\\frac{1}{2}) + (-1)(\\frac{1}{2}) + (1)(-\\frac{2}{3}) + (-1)(\\frac{2}{3}) \\right) \\\\\n= \\frac{1}{4} \\left( -\\frac{1}{2} - \\frac{1}{2} - \\frac{2}{3} - \\frac{2}{3} \\right) = \\frac{1}{4} \\left( -1 - \\frac{4}{3} \\right) = \\frac{1}{4} \\left( -\\frac{7}{3} \\right) = -\\frac{7}{12}\n$$\n\nFinally, we calculate the change in the average signed margin, $\\Delta m$.\n$$\n\\Delta m = \\left(\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}h_{i}^{(1)}\\right) - \\left(\\frac{1}{n}\\sum_{i=1}^{n} y_{i}\\,u^{\\top}x_{i}\\right) \\\\\n= -\\frac{7}{12} - \\frac{7}{4} = -\\frac{7}{12} - \\frac{21}{12} = -\\frac{28}{12} = -\\frac{7}{3}\n$$\n\nThe negative value of $\\Delta m$ indicates that the GCN layer has degraded the classification performance. The reason lies in the conflict between the GCN's smoothing mechanism and the graph's heterophilic structure. A GCN layer acts as a low-pass filter on the graph, averaging a node's features with those of its neighbors. This operation is effective under the assumption of homophily, where connected nodes have similar features and labels. In such cases, averaging reinforces class-specific features.\n\nIn this problem, the graph is heterophilic: every node is connected only to nodes of the opposite class. For example, node $1$ has label $1$ and its feature $x_1$'s first component is positive ($2$). Its neighbors, nodes $2$ and $4$, have label $-1$ and their features' first components are negative ($-2$ and $-1.5$). The GCN layer averages these, producing $h_1^{(1)}$ whose first component is $\\frac{1}{3}(2-2-1.5) = -0.5$. The feature vector is pulled across the decision boundary defined by $u$. This smoothing operation, which is local on the graph, is detrimental because neighbors in the graph are on opposite sides of the decision boundary in the feature space. Consequently, the GCN's feature aggregation mixes features from different classes, reducing the separability and, in this case, even inverting the classification for every node. This misalignment between the graph topology and the feature distribution is the core reason for the performance degradation, quantified by the negative $\\Delta m$.",
            "answer": "$$\\boxed{-\\frac{7}{3}}$$"
        }
    ]
}