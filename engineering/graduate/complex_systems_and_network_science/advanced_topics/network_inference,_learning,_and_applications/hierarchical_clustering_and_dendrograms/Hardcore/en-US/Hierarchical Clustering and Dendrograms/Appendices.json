{
    "hands_on_practices": [
        {
            "introduction": "To master hierarchical clustering, one must first grasp the mechanics of the agglomerative process. This exercise provides a step-by-step walkthrough of Ward's method, a widely used technique that minimizes the total within-cluster variance at each merge. By manually calculating the increase in the Sum of Squared Errors ($SSE$) for a small dataset, you will gain a concrete understanding of how dendrograms are built and how cluster structures emerge. ",
            "id": "4280752",
            "problem": "Consider an agglomerative hierarchical clustering problem on a dataset in $\\mathbb{R}^{p}$ with $p=2$, consisting of $n=5$ points with Euclidean geometry. The points are\n$$x_{1}=(0,0),\\quad x_{2}=(0,2),\\quad x_{3}=(9,0),\\quad x_{4}=(9,2),\\quad x_{5}=(4,1).$$\nUse Ward’s agglomeration rule, defined as follows: at each merge, choose the pair of clusters whose union produces the smallest increase in the total within-cluster Sum of Squared Errors (SSE) of the current partition. The within-cluster Sum of Squared Errors (SSE) of a cluster is defined as the sum of squared Euclidean distances from the points in the cluster to their cluster centroid. The dendrogram height for a merge is defined to be the increase in the total SSE caused by that merge. Begin from the partition of $n=5$ singleton clusters and perform merges step-by-step until all points form a single cluster. Track the cumulative total SSE after each merge and the dendrogram height at each merge.\n\nProvide the final cumulative total SSE after the last merge as a single exact value. Do not round. Express the final answer as a single real number or a single simplified rational expression without units.",
            "solution": "The user has provided a valid problem statement. All givens are clearly defined, and the task falls within the established principles of hierarchical clustering in statistical analysis. The problem is well-posed and objective. We will proceed with the solution.\n\nThe problem requires performing agglomerative hierarchical clustering on a set of $n=5$ points in $\\mathbb{R}^2$ using Ward’s agglomeration rule. The points are $x_{1}=(0,0)$, $x_{2}=(0,2)$, $x_{3}=(9,0)$, $x_{4}=(9,2)$, and $x_{5}=(4,1)$.\n\nWard's method seeks to merge the pair of clusters $(C_i, C_j)$ that results in the minimum increase in the total within-cluster Sum of Squared Errors (SSE), denoted as $\\Delta SSE(C_i, C_j)$. The increase in SSE upon merging two clusters $C_i$ and $C_j$ is given by the formula:\n$$ \\Delta SSE(C_i, C_j) = \\frac{n_i n_j}{n_i + n_j} \\|\\mu_i - \\mu_j\\|^2_2 $$\nwhere $n_i$ and $n_j$ are the number of points in clusters $C_i$ and $C_j$ respectively, $\\mu_i$ and $\\mu_j$ are their centroids, and $\\|\\cdot\\|_2$ is the Euclidean norm. The dendrogram height of a merge is this $\\Delta SSE$ value. The total cumulative SSE of a partition is the sum of the SSEs of its constituent clusters.\n\nWe begin with $n=5$ singleton clusters, where each point is its own cluster: $C_1=\\{x_1\\}$, $C_2=\\{x_2\\}$, $C_3=\\{x_3\\}$, $C_4=\\{x_4\\}$, $C_5=\\{x_5\\}$. For any singleton cluster $C_k = \\{x_k\\}$, its centroid is $\\mu_k = x_k$ and its SSE is $0$. Therefore, the initial total SSE is $0$.\n\n**Step 1: Merge 1 (from 5 clusters to 4 clusters)**\nThe current clusters are all singletons, so $n_i=n_j=1$ for any pair. The formula for $\\Delta SSE$ simplifies to:\n$$ \\Delta SSE(\\{x_i\\}, \\{x_j\\}) = \\frac{1 \\cdot 1}{1 + 1} \\|x_i - x_j\\|^2_2 = \\frac{1}{2} d(x_i, x_j)^2 $$\nWe calculate this for all $\\binom{5}{2}=10$ pairs of points:\n\\begin{itemize}\n    \\item $d(x_1, x_2)^2 = (0-0)^2 + (0-2)^2 = 4 \\implies \\Delta SSE = \\frac{1}{2}(4) = 2$\n    \\item $d(x_3, x_4)^2 = (9-9)^2 + (0-2)^2 = 4 \\implies \\Delta SSE = \\frac{1}{2}(4) = 2$\n    \\item $d(x_1, x_5)^2 = (0-4)^2 + (0-1)^2 = 17 \\implies \\Delta SSE = \\frac{1}{2}(17) = 8.5$\n    \\item $d(x_2, x_5)^2 = (0-4)^2 + (2-1)^2 = 17 \\implies \\Delta SSE = \\frac{1}{2}(17) = 8.5$\n    \\item $d(x_3, x_5)^2 = (9-4)^2 + (0-1)^2 = 26 \\implies \\Delta SSE = \\frac{1}{2}(26) = 13$\n    \\item $d(x_4, x_5)^2 = (9-4)^2 + (2-1)^2 = 26 \\implies \\Delta SSE = \\frac{1}{2}(26) = 13$\n    \\item $d(x_1, x_3)^2 = (0-9)^2 + (0-0)^2 = 81 \\implies \\Delta SSE = \\frac{1}{2}(81) = 40.5$\n    \\item $d(x_2, x_4)^2 = (0-9)^2 + (2-2)^2 = 81 \\implies \\Delta SSE = \\frac{1}{2}(81) = 40.5$\n    \\item $d(x_1, x_4)^2 = (0-9)^2 + (0-2)^2 = 85 \\implies \\Delta SSE = \\frac{1}{2}(85) = 42.5$\n    \\item $d(x_2, x_3)^2 = (0-9)^2 + (2-0)^2 = 85 \\implies \\Delta SSE = \\frac{1}{2}(85) = 42.5$\n\\end{itemize}\nThe minimum $\\Delta SSE$ is $2$, which occurs for two pairs: $(x_1, x_2)$ and $(x_3, x_4)$. We can choose to merge either pair. Due to the symmetry of the point configuration, the final result will be identical regardless of the choice. Let us merge $C_1$ and $C_2$ into a new cluster $C_A$.\n\\begin{itemize}\n    \\item Dendrogram height: $2$.\n    \\item New cluster: $C_A = \\{x_1, x_2\\}$. Its centroid is $\\mu_A = (\\frac{0+0}{2}, \\frac{0+2}{2}) = (0, 1)$.\n    \\item New partition: $\\{C_A, C_3, C_4, C_5\\}$.\n    \\item The total cumulative SSE is the initial SSE plus the increase: $0 + 2 = 2$.\n\\end{itemize}\n\n**Step 2: Merge 2 (from 4 clusters to 3 clusters)**\nThe current partition is $\\{C_A, C_3, C_4, C_5\\}$, with centroids $\\mu_A=(0,1)$, $\\mu_3=(9,0)$, $\\mu_4=(9,2)$, $\\mu_5=(4,1)$, and sizes $n_A=2, n_3=1, n_4=1, n_5=1$. We calculate $\\Delta SSE$ for all possible merges:\n\\begin{itemize}\n    \\item $C_3$ and $C_4$: $\\Delta SSE = \\frac{1 \\cdot 1}{1+1} \\|(9,0) - (9,2)\\|^2_2 = \\frac{1}{2}(4) = 2$.\n    \\item $C_A$ and $C_3$: $\\Delta SSE = \\frac{2 \\cdot 1}{2+1} \\|(0,1) - (9,0)\\|^2_2 = \\frac{2}{3}(81+1) = \\frac{164}{3} \\approx 54.67$.\n    \\item $C_A$ and $C_4$: $\\Delta SSE = \\frac{2 \\cdot 1}{2+1} \\|(0,1) - (9,2)\\|^2_2 = \\frac{2}{3}(81+1) = \\frac{164}{3} \\approx 54.67$.\n    \\item $C_A$ and $C_5$: $\\Delta SSE = \\frac{2 \\cdot 1}{2+1} \\|(0,1) - (4,1)\\|^2_2 = \\frac{2}{3}(16) = \\frac{32}{3} \\approx 10.67$.\n    \\item $C_3$ and $C_5$: $\\Delta SSE = \\frac{1 \\cdot 1}{1+1} \\|(9,0) - (4,1)\\|^2_2 = \\frac{1}{2}(25+1) = 13$.\n    \\item $C_4$ and $C_5$: $\\Delta SSE = \\frac{1 \\cdot 1}{1+1} \\|(9,2) - (4,1)\\|^2_2 = \\frac{1}{2}(25+1) = 13$.\n\\end{itemize}\nThe minimum $\\Delta SSE$ is $2$. We merge $C_3$ and $C_4$ to form a new cluster $C_B$.\n\\begin{itemize}\n    \\item Dendrogram height: $2$.\n    \\item New cluster: $C_B = \\{x_3, x_4\\}$. Its centroid is $\\mu_B = (\\frac{9+9}{2}, \\frac{0+2}{2}) = (9,1)$.\n    \\item New partition: $\\{C_A, C_B, C_5\\}$.\n    \\item The total cumulative SSE is the previous total SSE plus the increase: $2 + 2 = 4$.\n\\end{itemize}\n\n**Step 3: Merge 3 (from 3 clusters to 2 clusters)**\nThe current partition is $\\{C_A, C_B, C_5\\}$, with centroids $\\mu_A=(0,1)$, $\\mu_B=(9,1)$, $\\mu_5=(4,1)$, and sizes $n_A=2, n_B=2, n_5=1$. We calculate $\\Delta SSE$ for all possible merges:\n\\begin{itemize}\n    \\item $C_A$ and $C_B$: $\\Delta SSE = \\frac{2 \\cdot 2}{2+2} \\|(0,1) - (9,1)\\|^2_2 = \\frac{4}{4}(81) = 81$.\n    \\item $C_A$ and $C_5$: $\\Delta SSE = \\frac{2 \\cdot 1}{2+1} \\|(0,1) - (4,1)\\|^2_2 = \\frac{2}{3}(16) = \\frac{32}{3} \\approx 10.67$.\n    \\item $C_B$ and $C_5$: $\\Delta SSE = \\frac{2 \\cdot 1}{2+1} \\|(9,1) - (4,1)\\|^2_2 = \\frac{2}{3}(25) = \\frac{50}{3} \\approx 16.67$.\n\\end{itemize}\nThe minimum $\\Delta SSE$ is $\\frac{32}{3}$. We merge $C_A$ and $C_5$ to form a new cluster $C_C$.\n\\begin{itemize}\n    \\item Dendrogram height: $\\frac{32}{3}$.\n    \\item New cluster: $C_C = \\{x_1, x_2, x_5\\}$. Its centroid is $\\mu_C = (\\frac{0+0+4}{3}, \\frac{0+2+1}{3}) = (\\frac{4}{3}, 1)$.\n    \\item New partition: $\\{C_C, C_B\\}$.\n    \\item The total cumulative SSE is: $4 + \\frac{32}{3} = \\frac{12}{3} + \\frac{32}{3} = \\frac{44}{3}$.\n\\end{itemize}\n\n**Step 4: Merge 4 (from 2 clusters to 1 cluster)**\nThis is the final merge, between clusters $C_C$ and $C_B$.\nThe clusters are $C_C=\\{x_1, x_2, x_5\\}$ and $C_B=\\{x_3, x_4\\}$, with centroids $\\mu_C=(\\frac{4}{3},1)$ and $\\mu_B=(9,1)$, and sizes $n_C=3, n_B=2$.\n\\begin{itemize}\n    \\item $\\Delta SSE = \\frac{3 \\cdot 2}{3+2} \\|(\\frac{4}{3},1) - (9,1)\\|^2_2 = \\frac{6}{5} \\left( (\\frac{4}{3} - \\frac{27}{3})^2 + (1-1)^2 \\right) = \\frac{6}{5} \\left( (-\\frac{23}{3})^2 \\right) = \\frac{6}{5} \\cdot \\frac{529}{9} = \\frac{2 \\cdot 3}{5} \\cdot \\frac{529}{3^2} = \\frac{2 \\cdot 529}{5 \\cdot 3} = \\frac{1058}{15}$.\n    \\item Dendrogram height: $\\frac{1058}{15}$.\n    \\item New cluster: $C_{final} = \\{x_1, x_2, x_3, x_4, x_5\\}$.\n    \\item The final cumulative total SSE is: $\\frac{44}{3} + \\frac{1058}{15} = \\frac{44 \\cdot 5}{15} + \\frac{1058}{15} = \\frac{220 + 1058}{15} = \\frac{1278}{15}$.\n\\end{itemize}\nSimplifying the fraction by dividing the numerator and denominator by their greatest common divisor, which is $3$:\n$$ \\frac{1278}{15} = \\frac{1278 \\div 3}{15 \\div 3} = \\frac{426}{5} $$\nThe final cumulative total SSE is the SSE of the single cluster containing all points. We can verify this result by direct calculation. The centroid of all 5 points is $\\mu_{final} = (\\frac{0+0+9+9+4}{5}, \\frac{0+2+0+2+1}{5}) = (\\frac{22}{5}, 1)$.\nThe SSE is:\n$$ SSE_{final} = \\sum_{i=1}^5 \\|x_i - \\mu_{final}\\|^2_2 $$\n$$ SSE_{final} = \\|(0,0)-(\\frac{22}{5},1)\\|^2 + \\|(0,2)-(\\frac{22}{5},1)\\|^2 + \\|(9,0)-(\\frac{22}{5},1)\\|^2 + \\|(9,2)-(\\frac{22}{5},1)\\|^2 + \\|(4,1)-(\\frac{22}{5},1)\\|^2 $$\n$$ SSE_{final} = \\left( (\\frac{22}{5})^2 + 1^2 \\right) + \\left( (\\frac{22}{5})^2 + 1^2 \\right) + \\left( (\\frac{23}{5})^2 + 1^2 \\right) + \\left( (\\frac{23}{5})^2 + 1^2 \\right) + \\left( (-\\frac{2}{5})^2 + 0^2 \\right) $$\n$$ SSE_{final} = \\left( \\frac{484}{25} + 1 \\right) + \\left( \\frac{484}{25} + 1 \\right) + \\left( \\frac{529}{25} + 1 \\right) + \\left( \\frac{529}{25} + 1 \\right) + \\frac{4}{25} $$\n$$ SSE_{final} = 2\\left(\\frac{509}{25}\\right) + 2\\left(\\frac{554}{25}\\right) + \\frac{4}{25} = \\frac{1018 + 1108 + 4}{25} = \\frac{2130}{25} = \\frac{426}{5} $$\nThe direct calculation confirms the result from the incremental agglomeration process. The final cumulative total SSE is $\\frac{426}{5}$.",
            "answer": "$$\\boxed{\\frac{426}{5}}$$"
        },
        {
            "introduction": "The choice of linkage criterion dramatically influences the outcome of hierarchical clustering. This practice is designed to illustrate the famous \"chaining effect\" inherent to single-linkage clustering, where disparate clusters can be merged due to a single close pair of points. By analyzing a dataset with two dense groups connected by a sparse bridge, you will explore the deep connection between single-linkage clustering and the graph-theoretic concept of a Minimum Spanning Tree (MST). ",
            "id": "4280700",
            "problem": "Consider the metric space $(X,d)$ where $X \\subset \\mathbb{R}$ and $d(x,y) = |x-y|$. Define hierarchical agglomerative clustering with single linkage as follows: for each threshold $\\tau \\geq 0$, construct a graph $G_{\\tau}$ on vertex set $X$ with an undirected edge between $x,y \\in X$ if and only if $d(x,y) \\leq \\tau$. The clusters at level $\\tau$ are the connected components of $G_{\\tau}$. The single-linkage ultrametric $u(x,y)$ between points $x,y \\in X$ is the minimal $\\tau$ at which $x$ and $y$ lie in the same connected component of $G_{\\tau}$.\n\nTo analyze the chaining effect of single linkage, consider a dataset composed of two dense groups connected by a sparse bridge:\n- Dense group $A = \\{0, 0.2, 0.4, 0.6, 0.8\\}$,\n- Sparse bridge $S = \\{2, 5.5, 9\\}$,\n- Dense group $B = \\{12, 12.2, 12.4, 12.6, 12.8\\}$.\n\nLet $a^{\\star} = 0.4 \\in A$ and $b^{\\star} = 12.4 \\in B$. Starting from the fundamental definitions above and the graph-theoretic notion of a Minimum Spanning Tree (MST) (Minimum Spanning Tree (MST) is a spanning tree of a weighted graph that minimizes the total edge weight), determine the exact value of the single-linkage ultrametric $u(a^{\\star}, b^{\\star})$ for this dataset, thereby quantifying the level at which the two dense groups merge through the sparse bridge in the single-linkage dendrogram.\n\nExpress your final answer as a single real number. No rounding is required.",
            "solution": "The problem as stated is formally and scientifically valid. It is a well-posed question within the domain of hierarchical clustering and network science, with all necessary data and definitions provided. We may therefore proceed with a solution.\n\nThe single-linkage ultrametric, $u(x,y)$, between two points $x$ and $y$ is defined as the minimum threshold $\\tau \\ge 0$ at which $x$ and $y$ reside in the same connected component of the graph $G_{\\tau}$. An edge exists between any two points $x_i, x_j \\in X$ in $G_{\\tau}$ if and only if their distance $d(x_i, x_j) \\le \\tau$. As $\\tau$ increases from $0$, edges are added to the graph in non-decreasing order of their corresponding distances. This process is identical to the construction of a Minimum Spanning Tree (MST) of the complete graph on $X$ using Kruskal's algorithm, where edge weights are given by the metric $d(x,y) = |x-y|$.\n\nA fundamental theorem of single-linkage clustering states that the ultrametric distance $u(x,y)$ is equal to the weight of the heaviest edge on the unique path between $x$ and $y$ in the MST of the dataset. Our task is thus transformed into finding this maximum edge weight for the specific points $a^{\\star} = 0.4$ and $b^{\\star} = 12.4$.\n\nThe dataset is the set of points $X = A \\cup S \\cup B$, where:\n- $A = \\{0, 0.2, 0.4, 0.6, 0.8\\}$\n- $S = \\{2, 5.5, 9\\}$\n- $B = \\{12, 12.2, 12.4, 12.6, 12.8\\}$\n\nWe will determine the edges of the MST by considering edges in increasing order of weight, analogous to Kruskal's algorithm.\n\n1.  **Intra-group edges:** The smallest distances in the entire dataset are between adjacent points within the dense groups $A$ and $B$.\n    - Within group $A$, the edges connecting adjacent points, such as $(0, 0.2)$, $(0.2, 0.4)$, $(0.4, 0.6)$, and $(0.6, 0.8)$, all have a weight of $0.2$. These edges will be part of the MST and will connect all points in $A$ into a single component.\n    - Similarly, within group $B$, the edges $(12, 12.2)$, $(12.2, 12.4)$, $(12.4, 12.6)$, and $(12.6, 12.8)$ all have a weight of $0.2$. These edges will connect all points in $B$ into a separate component.\n    After this step, we have three main components: the cluster of points in $A$, the cluster of points in $B$, and the three isolated points of $S$.\n\n2.  **Inter-group (bridging) edges:** Now we must find the lowest-weight edges that connect these components.\n    - The shortest connection between a point in group $A$ and a point in the bridge $S$ is $d(0.8, 2) = |0.8 - 2| = 1.2$. This edge, $(0.8, 2)$, is added to the MST.\n    - The shortest connection between a point in group $B$ and a point in the bridge $S$ is $d(9, 12) = |9 - 12| = 3$. This edge, $(9, 12)$, is added to the MST.\n    - We must connect the points within the sparse bridge $S = \\{2, 5.5, 9\\}$. The distances are:\n        - $d(2, 5.5) = |2 - 5.5| = 3.5$\n        - $d(5.5, 9) = |5.5 - 9| = 3.5$\n        - $d(2, 9) = |2 - 9| = 7$\n    The MST will include the edges $(2, 5.5)$ and $(5.5, 9)$ to connect these three points, as using these two edges (both of weight $3.5$) is preferable to using the single, heavier edge $(2, 9)$ of weight $7$.\n\nWith the structure of the MST established, we can identify the unique path from $a^{\\star} = 0.4$ to $b^{\\star} = 12.4$. This path must traverse the bridging edges we found. The sequence of vertices in the path is:\n$$0.4 \\leftrightarrow 0.6 \\leftrightarrow 0.8 \\leftrightarrow 2 \\leftrightarrow 5.5 \\leftrightarrow 9 \\leftrightarrow 12 \\leftrightarrow 12.2 \\leftrightarrow 12.4$$\nThe path from $0.4$ to $0.8$ is through its neighbor(s) in group A, and similarly for the path from $12$ to $12.4$ in group B.\n\nLet's list the weights of the edges on this unique path:\n- $d(0.4, 0.6) = 0.2$\n- $d(0.6, 0.8) = 0.2$\n- $d(0.8, 2) = 1.2$\n- $d(2, 5.5) = 3.5$\n- $d(5.5, 9) = 3.5$\n- $d(9, 12) = 3$\n- $d(12, 12.2) = 0.2$\n- $d(12.2, 12.4) = 0.2$\n\nThe set of edge weights along the path is $\\{0.2, 0.2, 1.2, 3.5, 3.5, 3.0, 0.2, 0.2\\}$.\n\nThe ultrametric distance $u(a^{\\star}, b^{\\star})$ is the maximum of these weights:\n$$ u(a^{\\star}, b^{\\star}) = \\max\\{0.2, 1.2, 3.0, 3.5\\} $$\n$$ u(a^{\\star}, b^{\\star}) = 3.5 $$\nThis value, $3.5$, is the minimal threshold $\\tau$ at which $a^{\\star}$ and $b^{\\star}$ become members of the same cluster. This occurs when the path between them is completed by adding the last required bridging edge of weight $3.5$. This quantifies the level at which the two dense groups merge via the sparse bridge.",
            "answer": "$$\\boxed{3.5}$$"
        },
        {
            "introduction": "This final practice transitions from abstract examples to a real-world application in network science. You will apply hierarchical clustering to the classic Zachary’s Karate Club graph, a benchmark for community detection algorithms. The exercise requires using the effective resistance distance, a sophisticated metric for network analysis, and evaluating your results against the known social split, thereby completing the full workflow from theoretical model to empirical validation. ",
            "id": "4280745",
            "problem": "You are given the undirected, unweighted Zachary’s Karate Club graph with nodes labeled from $0$ to $33$ and the following edge set expressed as unordered pairs of node indices: $\\{(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7),(0,8),(0,10),(0,11),(0,12),(0,13),(0,17),(0,19),(0,21),(0,31),(1,2),(1,3),(1,7),(1,13),(1,17),(1,19),(1,21),(1,30),(2,3),(2,7),(2,27),(2,28),(2,32),(3,7),(3,12),(3,13),(4,6),(4,10),(5,6),(5,10),(5,16),(6,16),(8,30),(8,32),(8,33),(9,33),(13,33),(14,32),(14,33),(15,32),(15,33),(18,32),(18,33),(19,33),(20,32),(20,33),(22,32),(22,33),(23,25),(23,27),(23,29),(23,32),(23,33),(24,25),(24,27),(24,31),(25,26),(25,28),(25,32),(25,33),(26,27),(26,28),(26,32),(26,33),(27,28),(27,29),(27,32),(27,33),(28,32),(28,33),(29,32),(29,33),(30,32),(30,33),(31,32),(31,33),(32,33)\\}$. The ground-truth split of the club’s members is given by the two sets $S_{\\mathrm{MrHi}} = \\{0,1,2,3,4,5,6,7,8,10,11,12,13,17,19,21\\}$ and $S_{\\mathrm{Officer}} = \\{9,14,15,16,18,20,22,23,24,25,26,27,28,29,30,31,32,33\\}$, which together partition the $34$ nodes.\n\nStarting from first principles in complex systems and network science, you must compute the pairwise effective resistance distance between all node pairs and then perform agglomerative hierarchical clustering based on these distances. The resistance distance metric arises from modeling the graph as an electrical network with each edge as a unit resistor. Let $A \\in \\mathbb{R}^{n \\times n}$ be the adjacency matrix with $n=34$, let $D$ be the diagonal degree matrix, and let $L = D - A$ be the combinatorial Laplacian. The Moore–Penrose pseudoinverse of the Laplacian, denoted $L^{+}$, induces the effective resistance distance via $r_{ij} = L^{+}_{ii} + L^{+}_{jj} - 2 L^{+}_{ij}$ for nodes $i$ and $j$. Use this $r_{ij}$ as the metric for hierarchical clustering. Construct dendrograms using the standard agglomerative scheme, where clusters merge at increasing linkage distances. Cutting a dendrogram at a height threshold $h$ yields a partition whose merges have linkage distances less than or equal to $h$.\n\nFor each dendrogram cut, interpret the resulting communities by comparing the clustering to the given ground-truth split. Use the Adjusted Rand Index (ARI) to quantify agreement between the clustering and the ground truth. The Adjusted Rand Index is defined from the contingency counts of pairs of nodes placed in the same or different clusters in the two partitions, with adjustment for chance. For a partition of $n$ items into $J$ classes and $K$ clusters, let $n_{jk}$ be the number of items simultaneously in class $j$ and cluster $k$, let $a_{j} = \\sum_{k} n_{jk}$, and let $b_{k} = \\sum_{j} n_{jk}$. Define $C_{2}(x) = x(x-1)/2$. Then the index is $I = \\sum_{j,k} C_{2}(n_{jk})$, the expected index under randomness is $E = \\frac{(\\sum_{j} C_{2}(a_{j}))(\\sum_{k} C_{2}(b_{k}))}{C_{2}(n)}$, and the maximum index is $M = \\frac{1}{2}(\\sum_{j} C_{2}(a_{j}) + \\sum_{k} C_{2}(b_{k}))$. The Adjusted Rand Index is $\\mathrm{ARI} = \\frac{I - E}{M - E}$.\n\nImplement the following steps in a complete, runnable program:\n- Construct $A$, $D$, and $L$, compute $L^{+}$, and build the full symmetric matrix $R = [r_{ij}]$ with $r_{ii} = 0$ and $r_{ij} \\geq 0$.\n- For each specified test case, run hierarchical clustering on $R$ using the indicated linkage method and cut the dendrogram at the specified height threshold $h$ to obtain a partition.\n- For each partition, compute: (i) the total number of communities, (ii) the size of the largest community, and (iii) the Adjusted Rand Index between the partition and the ground-truth split $S_{\\mathrm{MrHi}}$ versus $S_{\\mathrm{Officer}}$.\n\nTest Suite:\n- Case $1$: linkage method $=$ “average”, cut height $h = 0.0$.\n- Case $2$: linkage method $=$ “average”, cut height $h = 0.6$.\n- Case $3$: linkage method $=$ “complete”, cut height $h = 0.6$.\n- Case $4$: linkage method $=$ “single”, cut height $h = 2.0$.\n- Case $5$: linkage method $=$ “average”, cut height $h = 10.0$.\n\nAnswer Specification and Output Format:\n- For each test case, output a list $[c, m, a]$ where $c$ is the integer number of communities, $m$ is the integer size of the largest community, and $a$ is the Adjusted Rand Index rounded to $6$ decimal digits as a decimal number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[[c_{1},m_{1},a_{1}],[c_{2},m_{2},a_{2}],\\dots]$.\n\nThere are no physical units involved and no angles; all results are pure numbers in the specified format.",
            "solution": "The user has provided a well-defined computational problem in the domain of network science. The task is to perform community detection on the Zachary's Karate Club graph using a specific methodology and evaluate the results. The problem statement is scientifically sound, internally consistent, and contains all necessary information for a unique solution.\n\n### Step 1: Graph Representation and Laplacian Matrix\n\nThe first step is to represent the undirected, unweighted Zachary's Karate Club graph mathematically. The graph consists of $n=34$ nodes, indexed from $0$ to $33$. We construct the adjacency matrix $A \\in \\mathbb{R}^{34 \\times 34}$, where $A_{ij} = 1$ if an edge exists between nodes $i$ and $j$, and $A_{ij} = 0$ otherwise. Since the graph is undirected, $A$ is a symmetric matrix.\n\nFrom the adjacency matrix $A$, we compute the degree matrix $D$, which is a diagonal matrix where the element $D_{ii}$ is the degree of node $i$ (i.e., the number of edges connected to it). The degree of node $i$ is calculated as $d_i = \\sum_{j=0}^{n-1} A_{ij}$.\n\nThe combinatorial graph Laplacian, $L$, is then defined as $L = D - A$. The Laplacian is a fundamental matrix in graph theory that encodes many structural properties of the graph. For a connected graph like the Karate Club graph, $L$ is a symmetric, positive semi-definite matrix with exactly one eigenvalue equal to $0$.\n\n### Step 2: Effective Resistance Distance Matrix\n\nThe problem specifies using the effective resistance distance as the metric for clustering. This metric is derived by modeling the graph as an electrical circuit where each edge represents a resistor with unit resistance ($1\\,\\Omega$). The effective resistance distance $r_{ij}$ between two nodes $i$ and $j$ is the potential difference measured between them when a unit current is injected at node $i$ and withdrawn at node $j$. This distance captures not just the shortest path but all paths between the nodes, providing a more robust measure of their connectivity and separation.\n\nThe Laplacian matrix $L$ is singular (non-invertible) because its rows and columns sum to $0$. To compute the resistance distances, we must use its Moore-Penrose pseudoinverse, denoted $L^{+}$. The effective resistance distance $r_{ij}$ is then given by the explicit formula:\n$$r_{ij} = L^{+}_{ii} + L^{+}_{jj} - 2L^{+}_{ij}$$\nWe compute $L^{+}$ and use this formula to construct the full symmetric distance matrix $R \\in \\mathbb{R}^{34 \\times 34}$, where the entry $R_{ij} = r_{ij}$. By definition, $r_{ii} = L^{+}_{ii} + L^{+}_{ii} - 2L^{+}_{ii} = 0$.\n\n### Step 3: Hierarchical Agglomerative Clustering\n\nWith the distance matrix $R$ established, we perform hierarchical agglomerative clustering. This bottom-up approach starts by treating each of the $34$ nodes as its own singleton cluster. Then, in each step, the two \"closest\" clusters are merged into a new, larger cluster. This process continues until all nodes belong to a single, all-encompassing cluster. The result is a tree-like structure called a dendrogram, which records the sequence of merges and the distance (linkage distance) at which each merge occurred.\n\nThe definition of \"closest\" depends on the chosen linkage method. The problem specifies three standard methods:\n- **Single Linkage**: The distance between two clusters is the minimum distance between any pair of nodes, one from each cluster. $d(C_1, C_2) = \\min_{u \\in C_1, v \\in C_2} r_{uv}$.\n- **Complete Linkage**: The distance is the maximum distance between any pair of nodes. $d(C_1, C_2) = \\max_{u \\in C_1, v \\in C_2} r_{uv}$.\n- **Average Linkage**: The distance is the average of all pairwise distances between nodes in the two clusters. $d(C_1, C_2) = \\frac{1}{|C_1||C_2|} \\sum_{u \\in C_1, v \\in C_2} r_{uv}$.\n\n### Step 4: Dendrogram Cutting and Partitioning\n\nA partition of the nodes into a set of communities can be obtained by cutting the dendrogram at a specific height $h$. This operation severs all branches of the dendrogram corresponding to merges that occurred at a linkage distance greater than $h$. The connected components of the resulting forest form the clusters of the partition. The problem provides specific cut heights for each test case. For a given cut height $h$, we will obtain a partition of the $34$ nodes.\n\n### Step 5: Evaluation with Adjusted Rand Index (ARI)\n\nTo evaluate the quality of the obtained partitions, we compare them against the provided ground-truth split of the club members into two factions: $S_{\\mathrm{MrHi}}$ and $S_{\\mathrm{Officer}}$. The Adjusted Rand Index (ARI) is a measure of similarity between two clusterings that accounts for chance agreement. Its value ranges from $-1$ (less agreement than expected by chance) to $1$ (perfect agreement), with $0$ indicating the level of agreement expected by random chance.\n\nThe computation of ARI is based on a contingency table, which tabulates the overlap between the classes of the two partitions. Let the ground-truth partition have $J$ classes and the computed partition have $K$ clusters. The contingency table entry $n_{jk}$ is the number of nodes that are in class $j$ of the ground truth and in cluster $k$ of the computed partition. From this table, we compute the following quantities as specified in the problem statement:\n- $a_j = \\sum_k n_{jk}$ (row sums) and $b_k = \\sum_j n_{jk}$ (column sums).\n- Using $C_2(x) = x(x-1)/2$, we calculate the Index $I = \\sum_{j,k} C_2(n_{jk})$, the Expected Index $E = \\frac{(\\sum_j C_2(a_j))(\\sum_k C_2(b_k))}{C_2(n)}$, and the Maximum Index $M = \\frac{1}{2}(\\sum_j C_2(a_j) + \\sum_k C_2(b_k))$.\n- The ARI is then $\\mathrm{ARI} = \\frac{I - E}{M - E}$.\n\n### Step 6: Application to Test Cases\n\nThe above procedure is executed for each of the five test cases provided. For each resulting partition, we calculate:\n1.  $c$: The total number of communities.\n2.  $m$: The size of the largest community.\n3.  $a$: The Adjusted Rand Index, rounded to $6$ decimal places.\n\nThe results are then compiled into the specified output format. For a cut height of $h=0.0$, every node is expected to be in its own cluster, as the resistance distance $r_{ij}$ is $0$ only if $i=j$. For a very large cut height, all nodes are expected to merge into a single cluster. The intermediate cut heights will reveal the community structure detected by the algorithm.",
            "answer": "```python\nimport numpy as np\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import squareform\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical clustering problem for the Zachary's Karate Club graph.\n    \"\"\"\n    N = 34\n    edges = [\n        (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 10),\n        (0, 11), (0, 12), (0, 13), (0, 17), (0, 19), (0, 21), (0, 31), (1, 2),\n        (1, 3), (1, 7), (1, 13), (1, 17), (1, 19), (1, 21), (1, 30), (2, 3),\n        (2, 7), (2, 27), (2, 28), (2, 32), (3, 7), (3, 12), (3, 13), (4, 6),\n        (4, 10), (5, 6), (5, 10), (5, 16), (6, 16), (8, 30), (8, 32), (8, 33),\n        (9, 33), (13, 33), (14, 32), (14, 33), (15, 32), (15, 33), (18, 32),\n        (18, 33), (19, 33), (20, 32), (20, 33), (22, 32), (22, 33), (23, 25),\n        (23, 27), (23, 29), (23, 32), (23, 33), (24, 25), (24, 27), (24, 31),\n        (25, 26), (25, 28), (25, 32), (25, 33), (26, 27), (26, 28), (26, 32),\n        (26, 33), (27, 28), (27, 29), (27, 32), (27, 33), (28, 32), (28, 33),\n        (29, 32), (29, 33), (30, 32), (30, 33), (31, 32), (31, 33), (32, 33)\n    ]\n    \n    S_officer = {9, 14, 15, 16, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}\n    ground_truth_labels = np.zeros(N, dtype=int)\n    for i in S_officer:\n        ground_truth_labels[i] = 1\n\n    # 1. Construct Adjacency, Degree, and Laplacian matrices\n    A = np.zeros((N, N))\n    for i, j in edges:\n        A[i, j] = 1\n        A[j, i] = 1\n    \n    D = np.diag(np.sum(A, axis=1))\n    L = D - A\n\n    # 2. Compute Moore-Penrose pseudoinverse and Resistance Distance Matrix\n    L_plus = np.linalg.pinv(L)\n    diag_L_plus = np.diag(L_plus).reshape(-1, 1)\n    R = diag_L_plus + diag_L_plus.T - 2 * L_plus\n    np.fill_diagonal(R, 0) # Ensure diagonal is exactly zero\n\n    # Convert to condensed distance matrix for scipy\n    dist_condensed = squareform(R, checks=False)\n\n    test_cases = [\n        {\"method\": \"average\", \"h\": 0.0},\n        {\"method\": \"average\", \"h\": 0.6},\n        {\"method\": \"complete\", \"h\": 0.6},\n        {\"method\": \"single\", \"h\": 2.0},\n        {\"method\": \"average\", \"h\": 10.0},\n    ]\n\n    results = []\n\n    def C2(x):\n        return x * (x - 1) // 2\n\n    def compute_ari(labels_true, labels_pred):\n        n = len(labels_true)\n        classes, class_counts = np.unique(labels_true, return_counts=True)\n        clusters, cluster_counts = np.unique(labels_pred, return_counts=True)\n        \n        # Build contingency table\n        contingency_table = np.zeros((len(classes), len(clusters)), dtype=int)\n        for i in range(n):\n            true_idx = np.where(classes == labels_true[i])[0][0]\n            pred_idx = np.where(clusters == labels_pred[i])[0][0]\n            contingency_table[true_idx, pred_idx] += 1\n\n        sum_C2_nij = np.sum(C2(contingency_table))\n        sum_C2_a = np.sum(C2(class_counts))\n        sum_C2_b = np.sum(C2(cluster_counts))\n\n        I = sum_C2_nij\n        E = (sum_C2_a * sum_C2_b) / C2(n)\n        M = 0.5 * (sum_C2_a + sum_C2_b)\n        \n        denominator = M - E\n        if denominator == 0:\n            return 0.0\n        \n        ari = (I - E) / denominator\n        return ari\n\n    for case in test_cases:\n        method = case[\"method\"]\n        h = case[\"h\"]\n        \n        # 3. Hierarchical Clustering\n        Z = hierarchy.linkage(dist_condensed, method=method)\n        \n        # 4. Dendrogram Cutting\n        labels = hierarchy.fcluster(Z, t=h, criterion='distance')\n        \n        # 5. Compute metrics\n        unique_labels = np.unique(labels)\n        c = len(unique_labels)\n        \n        if c > 0:\n            m = np.max(np.bincount(labels)[1:]) # fcluster labels start from 1\n        else: # Should not happen with fcluster\n            m = 0\n\n        a = compute_ari(ground_truth_labels, labels)\n\n        results.append([c, int(m), round(a, 6)])\n\n    print(f\"{results}\")\n\nsolve()\n```"
        }
    ]
}