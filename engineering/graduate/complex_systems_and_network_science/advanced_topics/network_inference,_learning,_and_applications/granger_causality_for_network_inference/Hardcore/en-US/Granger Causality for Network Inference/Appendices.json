{
    "hands_on_practices": [
        {
            "introduction": "Transitioning from theoretical understanding to practical application is a crucial step in mastering any quantitative method. This first exercise guides you through building a complete Granger causality inference pipeline from scratch . By generating synthetic data from a known Vector Autoregression (VAR) model and then applying nested model comparison, you will directly implement the logic of testing whether one time series improves the prediction of another, solidifying your understanding of the entire workflow.",
            "id": "4278726",
            "problem": "You are asked to implement a complete, runnable program that infers a directed network from multivariate time series using the definition of Granger causality. The program must generate synthetic data from a Vector Autoregression (VAR) process and apply hypothesis testing to detect directed edges among variables based on whether the past of one variable improves the linear prediction of another, controlling for the past of all variables in the system. All mathematical and algorithmic steps must be derived from first principles of linear stochastic processes and statistical hypothesis testing.\n\nFundamental base:\n- A multivariate discrete-time stochastic process is modeled as a Vector Autoregression (VAR), where, for lag order $p$, the state vector $\\mathbf{x}_t \\in \\mathbb{R}^k$ evolves according to $\\mathbf{x}_t = \\sum_{\\ell=1}^{p} A^{(\\ell)} \\mathbf{x}_{t-\\ell} + \\boldsymbol{\\varepsilon}_t$, with coefficient matrices $A^{(\\ell)} \\in \\mathbb{R}^{k \\times k}$ and innovation noise $\\boldsymbol{\\varepsilon}_t$ that is zero-mean Gaussian with covariance matrix $\\Sigma \\in \\mathbb{R}^{k \\times k}$.\n- Granger causality states that variable $x_j$ Granger-causes variable $x_i$ if, conditional on the past of all variables except $x_j$, adding the past of $x_j$ yields a statistically significant reduction in the linear prediction error variance of $x_i$.\n- Ordinary Least Squares (OLS) solves linear regression by minimizing the sum of squared residuals, yielding estimates that are maximum likelihood under Gaussian noise.\n- Nested model hypothesis testing compares a restricted model (without certain predictors) against an unrestricted model (with those predictors), using the increase or decrease of the sum of squared residuals under Gaussian assumptions to form a test statistic. The residuals are the differences between observed and predicted values; the sum of squared residuals is the prediction error energy.\n\nYour tasks:\n1. Simulation. For each test case below, simulate a stationary VAR process of lag order $p=1$ in $k=3$ dimensions:\n   - The recursion is $\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\varepsilon}_t$, with $A \\in \\mathbb{R}^{3 \\times 3}$ and $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$.\n   - Use burn-in of $200$ time steps, then keep the next $T$ samples. Initialize $\\mathbf{x}_0 = \\mathbf{0}$.\n   - Ensure stability by using the provided $A$ and $\\Sigma$.\n\n2. Inference. For each ordered pair (j -> i) with $j \\neq i$, perform a Granger causality test at significance level $\\alpha = 0.01$:\n   - Form two linear models predicting $x_i(t)$ from lagged variables at $t-1$:\n     - Unrestricted model: include all $k$ variables at lag $1$ as predictors.\n     - Restricted model: include all variables at lag $1$ except $x_j$.\n   - Fit both models by Ordinary Least Squares (OLS) and compute the sum of squared residuals of each model.\n   - Derive a suitable test statistic for the nested model comparison under Gaussian noise and compute a $p$-value from its sampling distribution. Decide edge presence as $1$ if the test rejects the null hypothesis (no Granger causality from $x_j$ to $x_i$) at level $\\alpha$ and the unrestricted model reduces prediction error relative to the restricted model; otherwise decide edge absence as $0$.\n   - Also compute the magnitude of Granger causality as the natural logarithm of the ratio of estimated residual variances between the restricted and unrestricted models; this measure is for internal validation only and must not be printed.\n\n3. Output specification. For each test case, output a list of six integers corresponding to directed edges in the fixed order $(1 \\to 2),(1 \\to 3),(2 \\to 1),(2 \\to 3),(3 \\to 1),(3 \\to 2)$, where variables are indexed by $1,2,3$. A present edge is encoded as the integer $1$ and an absent edge as the integer $0$.\n\n4. Final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the top-level list corresponds to one test case and must itself be the six-element list described above. For example, a valid output form is $[[a_1,a_2,a_3,a_4,a_5,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6],[d_1,\\dots,d_6]]$ where each $a_\\ell,b_\\ell,c_\\ell,d_\\ell$ is an integer $0$ or $1$.\n\nThere are no physical quantities in this problem; no unit specification is required.\n\nTest suite:\n- Case $1$ (chain):\n  - Dimension $k=3$, lag order $p=1$, samples $T=500$.\n  - Coefficient matrix\n    $$A = \\begin{bmatrix}\n    0  0  0 \\\\\n    0.4  0  0 \\\\\n    0  0.4  0\n    \\end{bmatrix}.$$\n  - Noise covariance\n    $$\\Sigma = \\begin{bmatrix}\n    1  0  0 \\\\\n    0  1  0 \\\\\n    0  0  1\n    \\end{bmatrix}.$$\n  - Expected generative edges: $(1 \\to 2)$ and $(2 \\to 3)$.\n\n- Case $2$ (bidirectional and fan-out):\n  - Dimension $k=3$, lag order $p=1$, samples $T=800$.\n  - Coefficient matrix\n    $$A = \\begin{bmatrix}\n    0  0.5  0 \\\\\n    0.5  0  0 \\\\\n    0.3  0  0\n    \\end{bmatrix}.$$\n  - Noise covariance\n    $$\\Sigma = \\begin{bmatrix}\n    0.5  0  0 \\\\\n    0  0.5  0 \\\\\n    0  0  0.5\n    \\end{bmatrix}.$$\n  - Expected generative edges: $(1 \\leftrightarrow 2)$ and $(1 \\to 3)$.\n\n- Case $3$ (null model):\n  - Dimension $k=3$, lag order $p=1$, samples $T=600$.\n  - Coefficient matrix $A = \\mathbf{0}_{3 \\times 3}$.\n  - Noise covariance\n    $$\\Sigma = \\begin{bmatrix}\n    1.5  0  0 \\\\\n    0  1.5  0 \\\\\n    0  0  1.5\n    \\end{bmatrix}.$$\n  - Expected generative edges: none.\n\n- Case $4$ (confounding contemporaneous noise correlation):\n  - Dimension $k=3$, lag order $p=1$, samples $T=700$.\n  - Coefficient matrix\n    $$A = \\begin{bmatrix}\n    0  0  0 \\\\\n    0.35  0  0 \\\\\n    0  0.2  0\n    \\end{bmatrix}.$$\n  - Noise covariance\n    $$\\Sigma = \\begin{bmatrix}\n    1  0  0.6 \\\\\n    0  1  0 \\\\\n    0.6  0  1\n    \\end{bmatrix}.$$\n  - Expected generative edges: $(1 \\to 2)$ and $(2 \\to 3)$; note the contemporaneous correlation between $x_1$ and $x_3$ should not induce Granger causality when properly conditioning on all lagged variables.\n\nImplementation constraints:\n- Use only Python version $3.12$, NumPy version $1.23.5$, and SciPy version $1.11.4$.\n- No external input or files; fixed seeds must be used for reproducibility.\n- Follow the final output format exactly; print only the specified single-line result.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded task from the field of complex systems and network science, requiring the implementation of a standard statistical inference algorithm. All necessary parameters and conditions are provided.\n\nThe objective is to infer the directed network structure underlying a multivariate time series by applying the principle of Granger causality. The data is generated from a known Vector Autoregressive (VAR) model, and the task is to recover the connectivity encoded in the model's coefficient matrix. The solution involves three primary stages: data simulation, statistical inference via nested model comparison, and hypothesis testing.\n\n**1. Data Simulation from a Vector Autoregressive (VAR) Process**\n\nThe problem specifies that the time series data is generated by a first-order VAR process, or VAR($p=1$), in $k=3$ dimensions. The process is defined by the stochastic difference equation:\n$$ \\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\varepsilon}_t $$\nwhere:\n- $\\mathbf{x}_t \\in \\mathbb{R}^k$ is the state vector of the system at a discrete time step $t$.\n- $A \\in \\mathbb{R}^{k \\times k}$ is the coefficient matrix. A non-zero entry $A_{ij}$ indicates a direct causal influence from variable $x_j$ at time $t-1$ to variable $x_i$ at time $t$.\n- $\\boldsymbol{\\varepsilon}_t \\in \\mathbb{R}^k$ is a vector of random innovations (noise), drawn from a multivariate normal distribution with mean zero and covariance matrix $\\Sigma$, denoted as $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$.\n\nThe simulation procedure is as follows:\n1. Initialize the state vector at time $t=0$ to $\\mathbf{x}_0 = \\mathbf{0}$.\n2. Generate a sequence of innovation vectors $\\boldsymbol{\\varepsilon}_1, \\boldsymbol{\\varepsilon}_2, \\dots, \\boldsymbol{\\varepsilon}_{N_{sim}}$ from $\\mathcal{N}(\\mathbf{0}, \\Sigma)$, where $N_{sim}$ is the total number of simulation steps.\n3. Iteratively compute the state vector $\\mathbf{x}_t$ for $t=1, \\dots, N_{sim}$ using the VAR equation.\n4. To ensure the process has reached its stationary distribution, a burn-in period of $200$ steps is used. The first $200$ samples are discarded.\n5. The subsequent $T$ samples constitute the time series data used for inference. The final data is an observation matrix $X \\in \\mathbb{R}^{T \\times k}$.\n\n**2. Granger Causality Inference via Nested Model Comparison**\n\nGranger causality provides a statistical definition of causality based on prediction. A time series $x_j$ is said to Granger-cause another time series $x_i$ if past values of $x_j$ contain information that helps predict $x_i$ above and beyond the information contained in the past values of all other variables in the system, including $x_i$ itself.\n\nFor a VAR($1$) process, this translates to testing the significance of the coefficient $A_{ij}$ in the row equation for $x_i(t)$:\n$$ x_i(t) = \\sum_{m=1}^{k} A_{im} x_m(t-1) + \\varepsilon_i(t) $$\nThe null hypothesis for the absence of Granger causality from $x_j$ to $x_i$ is $H_0: A_{ij} = 0$.\n\nTo test this hypothesis, we compare two linear regression models:\n- **Unrestricted Model:** This model incorporates all possible lagged predictors for $x_i(t)$. Its equation is:\n$$ x_i(t) = \\sum_{m=1}^{k} \\beta_{im} x_m(t-1) + u_t $$\nThe predictors are $\\{x_1(t-1), x_2(t-1), \\dots, x_k(t-1)\\}$.\n- **Restricted Model:** This model is formulated under the null hypothesis $H_0$, where the influence from $x_j(t-1)$ is excluded. Its equation is:\n$$ x_i(t) = \\sum_{m=1, m \\neq j}^{k} \\beta_{im} x_m(t-1) + v_t $$\nThe predictors are $\\{x_1(t-1), \\dots, x_{j-1}(t-1), x_{j+1}(t-1), \\dots, x_k(t-1)\\}$.\n\nBoth models are fitted to the data using Ordinary Least Squares (OLS), which minimizes the sum of squared residuals (RSS). Let $RSS_U$ be the RSS for the unrestricted model and $RSS_R$ be the RSS for the restricted model. By construction, $RSS_U \\le RSS_R$. A significant drop in RSS (i.e., $RSS_U \\ll RSS_R$) suggests that the excluded predictor, $x_j(t-1)$, has significant predictive power.\n\n**3. The F-Test for Statistical Significance**\n\nThe statistical significance of the improvement in prediction is quantified using an F-test for nested models. The F-statistic is formulated as the ratio of the explained variance per new predictor to the unexplained variance of the full model:\n$$ F = \\frac{(RSS_R - RSS_U) / q}{RSS_U / (N - k_U)} $$\nThe parameters of this statistic are defined as:\n- $N$: The number of observations used in the regression. For a time series of length $T$ and a lag of $p=1$, we have $N = T-1$ data points for regression.\n- $k_U$: The number of predictors in the unrestricted model. For a VAR($1$) in $k$ dimensions, $k_U = k$.\n- $q$: The number of restrictions imposed on the unrestricted model to obtain the restricted model. Here, we test a single coefficient, so $q=1$.\n\nUnder the null hypothesis $H_0$, this F-statistic follows an F-distribution with $(q, N - k_U)$ degrees of freedom. In this specific problem, the distribution is $F(1, T-1-k)$.\n\nFor each potential directed edge $(j \\to i)$, we perform the following steps:\n1. Construct the response vector $\\mathbf{y} = [x_i(2), \\dots, x_i(T)]^T$ and the design matrices for the unrestricted ($X_U$) and restricted ($X_R$) models from the simulated data.\n2. Use OLS (e.g., `numpy.linalg.lstsq`) to fit both models and obtain $RSS_U$ and $RSS_R$.\n3. Compute the F-statistic $F_{obs} = ((RSS_R - RSS_U) / 1) / (RSS_U / (T - 1 - k))$.\n4. Calculate the p-value, which is the probability $P(F_{1, T-1-k}  F_{obs})$. This is computed using the survival function of the F-distribution.\n5. Compare the p-value to the significance level $\\alpha = 0.01$. If the p-value is less than $\\alpha$, we reject the null hypothesis and conclude that $x_j$ Granger-causes $x_i$. An edge is drawn from node $j$ to node $i$. Otherwise, we fail to reject $H_0$ and no edge is drawn.\n\nThe condition that the unrestricted model must reduce prediction error is equivalent to $RSS_U  RSS_R$. This is inherently satisfied if the F-statistic is positive, which is a necessary condition for rejecting the null hypothesis at any non-trivial significance level.\n\nThis procedure is repeated for all $k(k-1)$ ordered pairs of distinct variables $(j, i)$ to construct the full inferred adjacency matrix. The final output is a list of binary values for the specific edges requested.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for Granger causality inference.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of the entire process.\n    np.random.seed(42)\n\n    test_cases = [\n        # Case 1 (chain)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 500, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0.4, 0, 0], [0, 0.4, 0]]),\n            \"Sigma\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        },\n        # Case 2 (bidirectional and fan-out)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 800, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0.5, 0], [0.5, 0, 0], [0.3, 0, 0]]),\n            \"Sigma\": np.array([[0.5, 0, 0], [0, 0.5, 0], [0, 0, 0.5]])\n        },\n        # Case 3 (null model)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 600, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            \"Sigma\": np.array([[1.5, 0, 0], [0, 1.5, 0], [0, 0, 1.5]])\n        },\n        # Case 4 (confounding contemporaneous noise correlation)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 700, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0.35, 0, 0], [0, 0.2, 0]]),\n            \"Sigma\": np.array([[1, 0, 0.6], [0, 1, 0], [0.6, 0, 1]])\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = infer_network_granger(params)\n        all_results.append(result)\n\n    # Format the final output as a single string.\n    # e.g., [[0,1,0,0,1,0],[...]]\n    output_str = \"[\" + \",\".join([str(res) for res in all_results]) + \"]\"\n    print(output_str)\n\ndef infer_network_granger(params):\n    \"\"\"\n    Simulates VAR data and performs Granger causality inference for one test case.\n    \"\"\"\n    k = params[\"k\"]\n    p = params[\"p\"]\n    T = params[\"T\"]\n    burn_in = params[\"burn_in\"]\n    A = params[\"A\"]\n    Sigma = params[\"Sigma\"]\n    alpha = params[\"alpha\"]\n    \n    # 1. Simulate the VAR(p) process\n    total_samples = T + burn_in\n    x = np.zeros((total_samples + p, k)) # Store history including initial zeros\n\n    # Generate noise for all time steps at once\n    mean_noise = np.zeros(k)\n    # Correct cholesky decomposition for noise generation\n    L = np.linalg.cholesky(Sigma)\n    innovations = (L @ np.random.randn(k, total_samples)).T\n    \n    for t in range(p, total_samples + p):\n        # The VAR model formulation is x_t = A x_{t-1} + eps\n        # Since p=1 is given for all test cases, this simplification is fine.\n        lagged_x = x[t-1, :]\n        x[t, :] = A @ lagged_x + innovations[t-p, :]\n\n    # Discard burn-in period to get the final time series data\n    data = x[p + burn_in:, :]\n\n    # 2. Inference: Perform Granger causality test for each pair (j -> i)\n    adj_matrix = np.zeros((k, k), dtype=int)\n    \n    # The number of observations for regression is T-p\n    # For a lag of p=1, we predict x(t) from x(t-1) for t=1..T-1\n    # Thus, we have T-1 observations.\n    num_obs = T - p\n    \n    for i in range(k):  # Target variable index\n        for j in range(k):  # Source variable index\n            if i == j:\n                continue\n            \n            # Prepare data for regression\n            # Response variable y is the i-th time series from t=p to T-1\n            y = data[p:, i]\n            \n            # --- Unrestricted Model ---\n            # Predictors are all k variables at lag p\n            # For p=1, this is data from t=0 to T-2\n            X_unrestricted = data[p-1:-1, :]\n            k_unrestricted = X_unrestricted.shape[1]\n            \n            # Fit using OLS\n            _, rss_u_array, _, _ = np.linalg.lstsq(X_unrestricted, y, rcond=None)\n            rss_u = rss_u_array[0] if rss_u_array.size > 0 else 0.0\n\n            # --- Restricted Model ---\n            # Predictors are all variables at lag p except j\n            predictor_indices = list(range(k))\n            predictor_indices.remove(j)\n            X_restricted = data[p-1:-1, predictor_indices]\n\n            # Fit using OLS\n            _, rss_r_array, _, _ = np.linalg.lstsq(X_restricted, y, rcond=None)\n            rss_r = rss_r_array[0] if rss_r_array.size > 0 else 0.0\n            \n            # --- F-Test ---\n            # q = number of restrictions (parameters removed)\n            q = k_unrestricted - X_restricted.shape[1]\n            \n            # Degrees of freedom for F-distribution\n            df1 = q\n            df2 = num_obs - k_unrestricted\n\n            if rss_u > 1e-10: # Avoid division by zero\n                f_stat = ((rss_r - rss_u) / df1) / (rss_u / df2)\n                \n                # p-value from survival function (1 - cdf)\n                p_value = f_dist.sf(f_stat, df1, df2)\n                \n                if p_value  alpha:\n                    adj_matrix[j, i] = 1 # Edge from j to i\n    \n    # 3. Format output according to problem specification\n    # Order: (1->2), (1->3), (2->1), (2->3), (3->1), (3->2)\n    # Corresponds to (0-based): adj[0,1], adj[0,2], adj[1,0], adj[1,2], adj[2,0], adj[2,1]\n    output_list = [\n        adj_matrix[0, 1],\n        adj_matrix[0, 2],\n        adj_matrix[1, 0],\n        adj_matrix[1, 2],\n        adj_matrix[2, 0],\n        adj_matrix[2, 1],\n    ]\n    \n    return output_list\n\nsolve()\n```"
        },
        {
            "introduction": "While Granger causality reveals predictive power from the past, it does not explain correlations observed at the exact same time step, which are captured in the innovation covariance matrix. This practice moves from lagged to instantaneous causality by introducing the Structural Vector Autoregression (SVAR) model . You will derive how to untangle the correlated errors of a standard VAR model to identify the underlying, uncorrelated structural shocks and their direct, simultaneous influence on each other.",
            "id": "4278735",
            "problem": "Consider a two-node complex dynamical system represented by the structural vector autoregression of order one (SVAR($1$))\n$$\nA_{0}\\, x_{t} \\;=\\; A_{1}\\, x_{t-1} \\;+\\; e_{t},\n$$\nwhere $x_{t} = \\begin{pmatrix} x_{1,t} \\\\ x_{2,t} \\end{pmatrix}$ collects the two node states at time $t$, $A_{0}$ is an invertible contemporaneous coefficient matrix encoding instantaneous causality, $A_{1}$ is a lagged coefficient matrix encoding lagged (Granger) causality, and $e_{t}$ are structural shocks satisfying $\\mathbb{E}[e_{t}] = 0$ and $\\mathbb{E}[e_{t} e_{t}^{\\top}] = \\Lambda$, with $\\Lambda$ diagonal and positive definite. Assume an acyclic instantaneous network consistent with the ordering $(x_{1,t}, x_{2,t})$, so that $A_{0}$ is lower triangular with ones on the diagonal:\n$$\nA_{0} \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n-\\theta  1\n\\end{pmatrix},\n$$\nwhere the scalar $\\theta$ is the instantaneous causal effect from node $1$ to node $2$. Let the corresponding reduced form be\n$$\nx_{t} \\;=\\; \\Phi\\, x_{t-1} \\;+\\; u_{t}, \\quad \\text{with} \\quad u_{t} \\;=\\; A_{0}^{-1} e_{t},\n$$\nand denote the reduced-form innovation covariance by\n$$\n\\Sigma_{u} \\;=\\; \\mathbb{E}[u_{t} u_{t}^{\\top}] \\;=\\; \\begin{pmatrix}\ns_{11}  s_{12} \\\\\ns_{12}  s_{22}\n\\end{pmatrix},\n$$\nwhere $s_{11}  0$, $s_{22}  0$, and $s_{11} s_{22} - s_{12}^{2}  0$. You may assume weak stationarity so that these second moments are well defined. Using only the stated structural assumptions and the definitions above, derive the instantaneous effect $\\theta$ as a closed-form symbolic expression in terms of $s_{11}$ and $s_{12}$ only. Provide your final answer as a single simplified analytic expression. No numerical rounding is required.",
            "solution": "The problem requires the derivation of the instantaneous causal effect $\\theta$ from the parameters of the reduced-form innovation covariance matrix $\\Sigma_{u}$. The derivation hinges on the relationship between the structural shocks $e_{t}$ and the reduced-form innovations $u_{t}$, and their respective covariance matrices, $\\Lambda$ and $\\Sigma_{u}$.\n\nThe relationship between the shocks is given by $u_{t} = A_{0}^{-1} e_{t}$, which can be rewritten as $e_{t} = A_{0} u_{t}$. We can use this to establish a connection between the covariance matrices. The covariance matrix of the structural shocks, $\\Lambda$, is defined as $\\Lambda = \\mathbb{E}[e_{t} e_{t}^{\\top}]$. Substituting the expression for $e_{t}$ in terms of $u_{t}$ yields:\n$$\n\\Lambda = \\mathbb{E}[ (A_{0} u_{t}) (A_{0} u_{t})^{\\top} ]\n$$\nUsing the properties of the transpose operator, $(AB)^{\\top} = B^{\\top}A^{\\top}$, we have:\n$$\n\\Lambda = \\mathbb{E}[ A_{0} u_{t} u_{t}^{\\top} A_{0}^{\\top} ]\n$$\nSince $A_{0}$ is a matrix of constant coefficients, we can move it outside the expectation:\n$$\n\\Lambda = A_{0} \\mathbb{E}[u_{t} u_{t}^{\\top}] A_{0}^{\\top}\n$$\nBy definition, the covariance matrix of the reduced-form innovations is $\\Sigma_{u} = \\mathbb{E}[u_{t} u_{t}^{\\top}]$. Therefore, we arrive at the fundamental identification equation:\n$$\nA_{0} \\Sigma_{u} A_{0}^{\\top} = \\Lambda\n$$\nThis equation links the structural parameters encoded in $A_{0}$ and $\\Lambda$ to the observable reduced-form covariance matrix $\\Sigma_{u}$.\n\nThe problem provides the specific forms for these matrices. The contemporaneous coefficient matrix $A_{0}$ is given by:\n$$\nA_{0} = \\begin{pmatrix} 1  0 \\\\ -\\theta  1 \\end{pmatrix}\n$$\nIts transpose is:\n$$\nA_{0}^{\\top} = \\begin{pmatrix} 1  -\\theta \\\\ 0  1 \\end{pmatrix}\n$$\nThe reduced-form innovation covariance matrix $\\Sigma_{u}$ is:\n$$\n\\Sigma_{u} = \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12}  s_{22} \\end{pmatrix}\n$$\nThe structural shock covariance matrix $\\Lambda$ is diagonal and positive definite. For a $2 \\times 2$ system, it has the form:\n$$\n\\Lambda = \\begin{pmatrix} \\lambda_{1}  0 \\\\ 0  \\lambda_{2} \\end{pmatrix}\n$$\nwhere $\\lambda_{1}  0$ and $\\lambda_{2}  0$ are the variances of the structural shocks $e_{1,t}$ and $e_{2,t}$, respectively.\n\nWe now substitute these matrices into the identification equation $A_{0} \\Sigma_{u} A_{0}^{\\top} = \\Lambda$. First, we compute the product $A_{0} \\Sigma_{u}$:\n$$\nA_{0} \\Sigma_{u} = \\begin{pmatrix} 1  0 \\\\ -\\theta  1 \\end{pmatrix} \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12}  s_{22} \\end{pmatrix} = \\begin{pmatrix} (1)(s_{11}) + (0)(s_{12})  (1)(s_{12}) + (0)(s_{22}) \\\\ (-\\theta)(s_{11}) + (1)(s_{12})  (-\\theta)(s_{12}) + (1)(s_{22}) \\end{pmatrix} = \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12} - \\theta s_{11}  s_{22} - \\theta s_{12} \\end{pmatrix}\n$$\nNext, we multiply this result by $A_{0}^{\\top}$:\n$$\n(A_{0} \\Sigma_{u}) A_{0}^{\\top} = \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12} - \\theta s_{11}  s_{22} - \\theta s_{12} \\end{pmatrix} \\begin{pmatrix} 1  -\\theta \\\\ 0  1 \\end{pmatrix}\n$$\nPerforming this final matrix multiplication gives:\n$$\n\\begin{pmatrix} (s_{11})(1) + (s_{12})(0)  (s_{11})(-\\theta) + (s_{12})(1) \\\\ (s_{12} - \\theta s_{11})(1) + (s_{22} - \\theta s_{12})(0)  (s_{12} - \\theta s_{11})(-\\theta) + (s_{22} - \\theta s_{12})(1) \\end{pmatrix} = \\begin{pmatrix} s_{11}  s_{12} - \\theta s_{11} \\\\ s_{12} - \\theta s_{11}  -\\theta s_{12} + \\theta^2 s_{11} + s_{22} - \\theta s_{12} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} s_{11}  s_{12} - \\theta s_{11} \\\\ s_{12} - \\theta s_{11}  s_{22} - 2\\theta s_{12} + \\theta^2 s_{11} \\end{pmatrix}\n$$\nNow, we equate this resulting matrix to $\\Lambda$:\n$$\n\\begin{pmatrix} s_{11}  s_{12} - \\theta s_{11} \\\\ s_{12} - \\theta s_{11}  s_{22} - 2\\theta s_{12} + \\theta^2 s_{11} \\end{pmatrix} = \\begin{pmatrix} \\lambda_{1}  0 \\\\ 0  \\lambda_{2} \\end{pmatrix}\n$$\nThe structural identification comes from the assumption that the structural shocks are uncorrelated, which implies that $\\Lambda$ is a diagonal matrix. Therefore, the off-diagonal elements of the matrix on the left-hand side must be zero. This provides an equation for $\\theta$:\n$$\ns_{12} - \\theta s_{11} = 0\n$$\nThe problem asks for an expression for $\\theta$ in terms of $s_{11}$ and $s_{12}$. We can solve the equation above for $\\theta$.\n$$\n\\theta s_{11} = s_{12}\n$$\nThe problem statement specifies that $s_{11}  0$, ensuring we can divide by $s_{11}$ without issue.\n$$\n\\theta = \\frac{s_{12}}{s_{11}}\n$$\nThis expression gives the instantaneous causal effect $\\theta$ purely in terms of the elements of the reduced-form innovation covariance matrix $\\Sigma_{u}$, as required.",
            "answer": "$$\\boxed{\\frac{s_{12}}{s_{11}}}$$"
        },
        {
            "introduction": "A central challenge in any causal analysis is the potential for unobserved confounding variables to create misleading results. This hands-on calculation demonstrates a critical pitfall of applying Granger causality: the detection of spurious causal links . You will analytically compute the non-zero Granger causality that arises between two processes that share a latent common driver, even when no direct causal path exists between them, reinforcing the need for cautious interpretation of statistical results.",
            "id": "4278730",
            "problem": "Consider a pair of observed time series $\\{X_{t}^{o}\\}$ and $\\{Y_{t}\\}$ generated by a latent confounder process $\\{L_{t}\\}$ that is not observed. The latent driver $\\{L_{t}\\}$ follows a stable first-order autoregressive model, defined by $L_{t} = \\alpha L_{t-1} + \\epsilon_{t}^{L}$, where $\\epsilon_{t}^{L}$ is zero-mean, independent and identically distributed Gaussian noise with variance $\\sigma_{L}^{2}$, and $|\\alpha|  1$ for stationarity. The observed variables are linearly driven by the lagged latent process with independent Gaussian innovations: $X_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X}$ and $Y_{t} = \\gamma L_{t-1} + \\epsilon_{t}^{Y}$, where $\\epsilon_{t}^{X}$ and $\\epsilon_{t}^{Y}$ are zero-mean, independent and identically distributed Gaussian noises with variances $\\sigma_{X}^{2}$ and $\\sigma_{Y}^{2}$, respectively. The measurement of $X_{t}$ is corrupted by independent Gaussian noise $\\nu_{t}$ with variance $\\sigma_{\\nu}^{2}$, yielding $X_{t}^{o} = X_{t} + \\nu_{t}$. All innovation and measurement noise terms are mutually independent across time and across processes, and are also independent of $\\{L_{t}\\}$.\n\nA researcher, unaware of the latent confounder and the measurement noise, fits a bivariate Vector Autoregression (VAR) of order one to the observed pair $(Y_{t}, X_{t}^{o})$ and evaluates the one-step-ahead linear prediction of $Y_{t}$ from past lags $(Y_{t-1}, X_{t-1}^{o})$. In large samples under Gaussian assumptions, the Ordinary Least Squares (OLS) predictor equals the linear least-squares projection of $Y_{t}$ onto the regressors. Let the restricted predictor of $Y_{t}$ use only $Y_{t-1}$, and the unrestricted predictor use both $Y_{t-1}$ and $X_{t-1}^{o}$. Define the Granger causality (GC) from $X$ to $Y$ as the natural logarithm of the ratio of the restricted one-step-ahead prediction error variance to the unrestricted one-step-ahead prediction error variance for $Y_{t}$.\n\nTake the specific parameter values $\\alpha = \\tfrac{1}{2}$, $\\sigma_{L}^{2} = 1$, $\\beta = 1$, $\\gamma = 1$, $\\sigma_{X}^{2} = 1$, $\\sigma_{Y}^{2} = 1$, and $\\sigma_{\\nu}^{2} = 1$. Under these conditions, compute the exact asymptotic Granger causality from $X$ to $Y$ implied by the bivariate VAR($1$) fit to $(Y_{t}, X_{t}^{o})$, expressed as a single closed-form analytical expression. The quantity is dimensionless. No rounding is required; provide the exact expression.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the theory of time series analysis and Granger causality, well-posed with all necessary parameters and definitions provided, and objective in its language. It presents a standard, albeit non-trivial, calculation within its field.\n\nThe objective is to compute the asymptotic Granger causality (GC) from the observed process $X^o$ to the process $Y$, defined as:\n$$\nGC_{X \\to Y} = \\ln\\left( \\frac{\\sigma_{R}^{2}}{\\sigma_{U}^{2}} \\right)\n$$\nwhere $\\sigma_{R}^{2}$ is the variance of the one-step-ahead prediction error for $Y_t$ using only its own past, and $\\sigma_{U}^{2}$ is the variance of the prediction error using the past of both $Y_t$ and $X_t^o$. Asymptotically, for Gaussian processes, these errors correspond to a linear least-squares projection.\n\nThe model is defined by the following equations:\n1.  Latent process: $L_{t} = \\alpha L_{t-1} + \\epsilon_{t}^{L}$, with $\\epsilon_{t}^{L} \\sim N(0, \\sigma_{L}^{2})$.\n2.  Observed processes (ideal): $Y_{t} = \\gamma L_{t-1} + \\epsilon_{t}^{Y}$ and $X_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X}$, with $\\epsilon_{t}^{X} \\sim N(0, \\sigma_{X}^{2})$ and $\\epsilon_{t}^{Y} \\sim N(0, \\sigma_{Y}^{2})$.\n3.  Noisy observation: $X_{t}^{o} = X_{t} + \\nu_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X} + \\nu_{t}$, with $\\nu_{t} \\sim N(0, \\sigma_{\\nu}^{2})$.\n\nAll processes are assumed to be stationary and have zero mean. We first compute the necessary second-order moments (variances and covariances).\n\nFirst, the variance of the stationary latent process $L_t$ is:\n$$\n\\text{Var}(L_t) = \\Sigma_{LL}(0) = \\frac{\\sigma_{L}^{2}}{1 - \\alpha^2}\n$$\nThe autocovariance of $L_t$ at lag $k$ is $\\Sigma_{LL}(k) = \\text{Cov}(L_t, L_{t-k}) = \\alpha^{k} \\Sigma_{LL}(0)$.\n\nWe are given the parameter values: $\\alpha = \\frac{1}{2}$, $\\sigma_{L}^{2} = 1$, $\\beta = 1$, $\\gamma = 1$, $\\sigma_{X}^{2} = 1$, $\\sigma_{Y}^{2} = 1$, and $\\sigma_{\\nu}^{2} = 1$.\nSubstituting these values, we find the variance of $L_t$:\n$$\n\\Sigma_{LL}(0) = \\frac{1}{1 - (\\frac{1}{2})^2} = \\frac{1}{1 - \\frac{1}{4}} = \\frac{1}{\\frac{3}{4}} = \\frac{4}{3}\n$$\n\nNext, we compute the variances and covariances of the observed processes $Y_t$ and $X_t^o$. Due to stationarity, $\\text{Var}(Y_t) = \\text{Var}(Y_{t-1})$ and $\\text{Var}(X_t^o) = \\text{Var}(X_{t-1}^o)$.\n$$\n\\text{Var}(Y_t) = \\text{Var}(\\gamma L_{t-1} + \\epsilon_{t}^{Y}) = \\gamma^2 \\text{Var}(L_{t-1}) + \\text{Var}(\\epsilon_{t}^{Y}) = \\gamma^2 \\Sigma_{LL}(0) + \\sigma_{Y}^{2}\n$$\n$$\n\\text{Var}(Y_t) = (1)^2 \\left(\\frac{4}{3}\\right) + 1 = \\frac{7}{3}\n$$\n$$\n\\text{Var}(X_t^o) = \\text{Var}(\\beta L_{t-1} + \\epsilon_{t}^{X} + \\nu_{t}) = \\beta^2 \\text{Var}(L_{t-1}) + \\text{Var}(\\epsilon_{t}^{X}) + \\text{Var}(\\nu_{t}) = \\beta^2 \\Sigma_{LL}(0) + \\sigma_{X}^{2} + \\sigma_{\\nu}^{2}\n$$\n$$\n\\text{Var}(X_t^o) = (1)^2 \\left(\\frac{4}{3}\\right) + 1 + 1 = \\frac{4}{3} + 2 = \\frac{10}{3}\n$$\nThe cross-covariance between $Y_{t-1}$ and $X_{t-1}^o$ is:\n$$\n\\text{Cov}(Y_{t-1}, X_{t-1}^o) = \\text{E}[(\\gamma L_{t-2} + \\epsilon_{t-1}^{Y})(\\beta L_{t-2} + \\epsilon_{t-1}^{X} + \\nu_{t-1})]\n$$\nSince all noise terms are independent of each other and of $L_t$, this simplifies to:\n$$\n\\text{Cov}(Y_{t-1}, X_{t-1}^o) = \\gamma \\beta \\text{E}[L_{t-2}^2] = \\gamma \\beta \\Sigma_{LL}(0) = (1)(1)\\left(\\frac{4}{3}\\right) = \\frac{4}{3}\n$$\n\nNow we compute the covariances between $Y_t$ and its predictors, $Y_{t-1}$ and $X_{t-1}^o$.\n$$\n\\text{Cov}(Y_t, Y_{t-1}) = \\text{E}[(\\gamma L_{t-1} + \\epsilon_{t}^{Y})(\\gamma L_{t-2} + \\epsilon_{t-1}^{Y})] = \\gamma^2 \\text{E}[L_{t-1}L_{t-2}]\n$$\nSince $L_{t-1} = \\alpha L_{t-2} + \\epsilon_{t-1}^L$, we have $\\text{E}[L_{t-1}L_{t-2}] = \\alpha \\text{E}[L_{t-2}^2] = \\alpha \\Sigma_{LL}(0)$.\n$$\n\\text{Cov}(Y_t, Y_{t-1}) = \\gamma^2 \\alpha \\Sigma_{LL}(0) = (1)^2 \\left(\\frac{1}{2}\\right) \\left(\\frac{4}{3}\\right) = \\frac{2}{3}\n$$\n$$\n\\text{Cov}(Y_t, X_{t-1}^o) = \\text{E}[(\\gamma L_{t-1} + \\epsilon_{t}^{Y})(\\beta L_{t-2} + \\epsilon_{t-1}^{X} + \\nu_{t-1})] = \\gamma \\beta \\text{E}[L_{t-1}L_{t-2}] = \\gamma \\beta \\alpha \\Sigma_{LL}(0)\n$$\n$$\n\\text{Cov}(Y_t, X_{t-1}^o) = (1)(1)\\left(\\frac{1}{2}\\right)\\left(\\frac{4}{3}\\right) = \\frac{2}{3}\n$$\n\nThe restricted prediction error variance $\\sigma_R^2$ is for predicting $Y_t$ from $Y_{t-1}$:\n$$\n\\sigma_R^2 = \\text{Var}(Y_t) - \\frac{\\text{Cov}(Y_t, Y_{t-1})^2}{\\text{Var}(Y_{t-1})}\n$$\n$$\n\\sigma_R^2 = \\frac{7}{3} - \\frac{(\\frac{2}{3})^2}{\\frac{7}{3}} = \\frac{7}{3} - \\frac{\\frac{4}{9}}{\\frac{7}{3}} = \\frac{7}{3} - \\frac{4}{9} \\cdot \\frac{3}{7} = \\frac{7}{3} - \\frac{4}{21} = \\frac{49 - 4}{21} = \\frac{45}{21} = \\frac{15}{7}\n$$\n\nThe unrestricted prediction error variance $\\sigma_U^2$ is for predicting $Y_t$ from the vector of regressors $Z_{t-1} = \\begin{pmatrix} Y_{t-1} \\\\ X_{t-1}^o \\end{pmatrix}$. The variance is given by $\\sigma_U^2 = \\text{Var}(Y_t) - c^T \\Sigma_Z^{-1} c$, where $\\Sigma_Z$ is the covariance matrix of $Z_{t-1}$ and $c$ is the vector of covariances between $Y_t$ and $Z_{t-1}$.\n$$\n\\Sigma_Z = \\begin{pmatrix} \\text{Var}(Y_{t-1})  \\text{Cov}(Y_{t-1}, X_{t-1}^o) \\\\ \\text{Cov}(Y_{t-1}, X_{t-1}^o)  \\text{Var}(X_{t-1}^o) \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3}  \\frac{4}{3} \\\\ \\frac{4}{3}  \\frac{10}{3} \\end{pmatrix}\n$$\n$$\nc = \\begin{pmatrix} \\text{Cov}(Y_t, Y_{t-1}) \\\\ \\text{Cov}(Y_t, X_{t-1}^o) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\nThe determinant of $\\Sigma_Z$ is:\n$$\n\\det(\\Sigma_Z) = \\left(\\frac{7}{3}\\right)\\left(\\frac{10}{3}\\right) - \\left(\\frac{4}{3}\\right)^2 = \\frac{70}{9} - \\frac{16}{9} = \\frac{54}{9} = 6\n$$\nThe inverse of $\\Sigma_Z$ is:\n$$\n\\Sigma_Z^{-1} = \\frac{1}{6} \\begin{pmatrix} \\frac{10}{3}  -\\frac{4}{3} \\\\ -\\frac{4}{3}  \\frac{7}{3} \\end{pmatrix}\n$$\nNow we compute the term $c^T \\Sigma_Z^{-1} c$:\n$$\nc^T \\Sigma_Z^{-1} c = \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\frac{1}{6} \\begin{pmatrix} \\frac{10}{3}  -\\frac{4}{3} \\\\ -\\frac{4}{3}  \\frac{7}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\n$$\nc^T \\Sigma_Z^{-1} c = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{10}{3} \\cdot \\frac{2}{3} - \\frac{4}{3} \\cdot \\frac{2}{3} \\\\ -\\frac{4}{3} \\cdot \\frac{2}{3} + \\frac{7}{3} \\cdot \\frac{2}{3} \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{20-8}{9} \\\\ \\frac{-8+14}{9} \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{12}{9} \\\\ \\frac{6}{9} \\end{pmatrix}\n$$\n$$\nc^T \\Sigma_Z^{-1} c = \\frac{1}{6} \\left( \\frac{2}{3} \\cdot \\frac{12}{9} + \\frac{2}{3} \\cdot \\frac{6}{9} \\right) = \\frac{1}{6} \\left( \\frac{24}{27} + \\frac{12}{27} \\right) = \\frac{1}{6} \\left( \\frac{36}{27} \\right) = \\frac{1}{6} \\left( \\frac{4}{3} \\right) = \\frac{4}{18} = \\frac{2}{9}\n$$\nThe unrestricted error variance is:\n$$\n\\sigma_U^2 = \\text{Var}(Y_t) - c^T \\Sigma_Z^{-1} c = \\frac{7}{3} - \\frac{2}{9} = \\frac{21 - 2}{9} = \\frac{19}{9}\n$$\nFinally, we compute the Granger causality measure:\n$$\nGC_{X \\to Y} = \\ln\\left( \\frac{\\sigma_{R}^{2}}{\\sigma_{U}^{2}} \\right) = \\ln\\left( \\frac{\\frac{15}{7}}{\\frac{19}{9}} \\right) = \\ln\\left( \\frac{15}{7} \\cdot \\frac{9}{19} \\right) = \\ln\\left( \\frac{135}{133} \\right)\n$$\nThis positive value indicates spurious causality detected from $X^o$ to $Y$ due to the combination of the latent confounder and measurement noise, despite no direct causal link in the data generating process.",
            "answer": "$$\\boxed{\\ln\\left(\\frac{135}{133}\\right)}$$"
        }
    ]
}