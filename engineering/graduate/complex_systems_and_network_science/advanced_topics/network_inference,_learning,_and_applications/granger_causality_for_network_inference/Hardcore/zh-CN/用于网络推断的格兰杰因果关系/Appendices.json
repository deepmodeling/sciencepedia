{
    "hands_on_practices": [
        {
            "introduction": "第一个实践是基础性的，它将指导你构建一个完整的流程，从合成数据中推断网络结构。通过从一个已知的向量自回归（VAR）模型生成数据，然后使用嵌套回归来恢复其结构，你将对格兰杰因果关系在最基本层面上的工作原理获得亲身、实践性的理解 。",
            "id": "4278726",
            "problem": "您需要实现一个完整的、可运行的程序，该程序使用格兰杰因果关系的定义，从多元时间序列中推断有向网络。该程序必须从向量自回归（VAR）过程中生成合成数据，并应用假设检验来检测变量之间的有向边，其依据是在控制系统中所有变量的过去值的情况下，一个变量的过去值是否能改善另一个变量的线性预测。所有数学和算法步骤都必须从线性随机过程和统计假设检验的第一性原理推导得出。\n\n基本原理：\n- 一个多元离散时间随机过程被建模为向量自回归（VAR），其中，对于滞后阶数 $p$，状态向量 $\\mathbf{x}_t \\in \\mathbb{R}^k$ 遵循 $\\mathbf{x}_t = \\sum_{\\ell=1}^{p} A^{(\\ell)} \\mathbf{x}_{t-\\ell} + \\boldsymbol{\\varepsilon}_t$ 演化，其中 $A^{(\\ell)} \\in \\mathbb{R}^{k \\times k}$ 为系数矩阵，$\\boldsymbol{\\varepsilon}_t$ 为新息噪声，是均值为零、协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{k \\times k}$ 的高斯噪声。\n- 格兰杰因果关系指出，如果以除 $x_j$ 外所有变量的过去值为条件，加入 $x_j$ 的过去值能够导致 $x_i$ 的线性预测误差方差出现统计上显著的降低，那么变量 $x_j$ 格兰杰导致（Granger-causes）变量 $x_i$。\n- 普通最小二乘法（OLS）通过最小化残差平方和来求解线性回归，在高斯噪声假设下产生最大似然估计。\n- 嵌套模型假设检验将一个受限模型（不包含某些预测变量）与一个非受限模型（包含那些预测变量）进行比较，利用高斯假设下残差平方和的增减来构建检验统计量。残差是观测值与预测值之间的差异；残差平方和是预测误差能量。\n\n您的任务：\n1. 模拟。对于下述每个测试用例，模拟一个滞后阶数 $p=1$、维度 $k=3$ 的平稳VAR过程：\n   - 递推关系为 $\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\varepsilon}_t$，其中 $A \\in \\mathbb{R}^{3 \\times 3}$，$\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$。\n   - 使用 $200$ 个时间步的预烧期，然后保留接下来的 $T$ 个样本。初始化 $\\mathbf{x}_0 = \\mathbf{0}$。\n   - 使用提供的 $A$ 和 $\\Sigma$ 以确保稳定性。\n\n2. 推断。对于每个有序对 $(j \\to i)$ 且 $j \\neq i$，在显著性水平 $\\alpha = 0.01$ 下执行格兰杰因果关系检验：\n   - 构建两个线性模型，从 $t-1$ 时刻的滞后变量预测 $x_i(t)$：\n     - 非受限模型：包含滞后1的所有 $k$ 个变量作为预测变量。\n     - 受限模型：包含滞后1的所有变量，但不包括 $x_j$。\n   - 通过普通最小二乘法（OLS）拟合两个模型，并计算每个模型的残差平方和。\n   - 为嵌套模型比较推导一个在高斯噪声假设下合适的检验统计量，并从其抽样分布中计算出一个 $p$ 值。如果检验在水平 $\\alpha$ 下拒绝原假设（即从 $x_j$ 到 $x_i$ 不存在格兰杰因果关系），并且非受限模型相对于受限模型降低了预测误差，则判定边存在为 $1$；否则判定边不存在为 $0$。\n   - 同时计算格兰杰因果关系的大小，即受限模型与非受限模型估计的残差方差之比的自然对数；此度量仅用于内部验证，不得打印。\n\n3. 输出规范。对于每个测试用例，输出一个包含六个整数的列表，按固定顺序 $(1 \\to 2),(1 \\to 3),(2 \\to 1),(2 \\to 3),(3 \\to 1),(3 \\to 2)$ 对应于有向边，其中变量索引为 $1,2,3$。存在的边编码为整数 $1$，不存在的边编码为整数 $0$。\n\n4. 最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。顶层列表的每个元素对应一个测试用例，并且其本身必须是上述的六元素列表。例如，一个有效的输出形式是 $[[a_1,a_2,a_3,a_4,a_5,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6],[d_1,\\dots,d_6]]$，其中每个 $a_\\ell,b_\\ell,c_\\ell,d_\\ell$ 是整数 $0$ 或 $1$。\n\n此问题不涉及物理量；无需指定单位。\n\n测试套件：\n- 用例1（链式结构）：\n  - 维度 $k=3$，滞后阶数 $p=1$，样本数 $T=500$。\n  - 系数矩阵\n    $$A = \\begin{bmatrix}\n    0  0  0 \\\\\n    0.4  0  0 \\\\\n    0  0.4  0\n    \\end{bmatrix}.$$\n  - 噪声协方差\n    $$\\Sigma = \\begin{bmatrix}\n    1  0  0 \\\\\n    0  1  0 \\\\\n    0  0  1\n    \\end{bmatrix}.$$\n  - 预期的生成边：$(1 \\to 2)$ 和 $(2 \\to 3)$。\n\n- 用例2（双向和扇出结构）：\n  - 维度 $k=3$，滞后阶数 $p=1$，样本数 $T=800$。\n  - 系数矩阵\n    $$A = \\begin{bmatrix}\n    0  0.5  0 \\\\\n    0.5  0  0 \\\\\n    0.3  0  0\n    \\end{bmatrix}.$$\n  - 噪声协方差\n    $$\\Sigma = \\begin{bmatrix}\n    0.5  0  0 \\\\\n    0  0.5  0 \\\\\n    0  0  0.5\n    \\end{bmatrix}.$$\n  - 预期的生成边：$(1 \\leftrightarrow 2)$ 和 $(1 \\to 3)$。\n\n- 用例3（空模型）：\n  - 维度 $k=3$，滞后阶数 $p=1$，样本数 $T=600$。\n  - 系数矩阵 $A = \\mathbf{0}_{3 \\times 3}$。\n  - 噪声协方差\n    $$\\Sigma = \\begin{bmatrix}\n    1.5  0  0 \\\\\n    0  1.5  0 \\\\\n    0  0  1.5\n    \\end{bmatrix}.$$\n  - 预期的生成边：无。\n\n- 用例4（混杂的同期噪声相关性）：\n  - 维度 $k=3$，滞后阶数 $p=1$，样本数 $T=700$。\n  - 系数矩阵\n    $$A = \\begin{bmatrix}\n    0  0  0 \\\\\n    0.35  0  0 \\\\\n    0  0.2  0\n    \\end{bmatrix}.$$\n  - 噪声协方差\n    $$\\Sigma = \\begin{bmatrix}\n    1  0  0.6 \\\\\n    0  1  0 \\\\\n    0.6  0  1\n    \\end{bmatrix}.$$\n  - 预期的生成边：$(1 \\to 2)$ 和 $(2 \\to 3)$；请注意，在正确地以所有滞后变量为条件时，$x_1$ 和 $x_3$ 之间的同期相关性不应引发格兰杰因果关系。\n\n实现约束：\n- 仅使用 Python 版本 $3.12$、NumPy 版本 $1.23.5$ 和 SciPy 版本 $1.11.4$。\n- 无外部输入或文件；必须使用固定的种子以保证可复现性。\n- 严格遵循最终输出格式；仅打印指定的单行结果。",
            "solution": "该问题是有效的。这是一个来自复杂系统和网络科学领域的、具有科学依据的明确任务，要求实现一个标准的统计推断算法。所有必要的参数和条件都已提供。\n\n目标是通过应用格兰杰因果关系原理，推断多元时间序列底层的有向网络结构。数据由一个已知的向量自回归（VAR）模型生成，任务是恢复模型系数矩阵中编码的连接性。解决方案涉及三个主要阶段：数据模拟、通过嵌套模型比较进行统计推断以及假设检验。\n\n**1. 从向量自回归（VAR）过程模拟数据**\n\n问题指定时间序列数据由一个一阶VAR过程，或VAR($p=1$)，在 $k=3$ 维度上生成。该过程由以下随机差分方程定义：\n$$ \\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\varepsilon}_t $$\n其中：\n- $\\mathbf{x}_t \\in \\mathbb{R}^k$ 是系统在离散时间步 $t$ 的状态向量。\n- $A \\in \\mathbb{R}^{k \\times k}$ 是系数矩阵。一个非零项 $A_{ij}$ 表示从变量 $x_j$ 在时间 $t-1$ 到变量 $x_i$ 在时间 $t$ 的直接因果影响。\n- $\\boldsymbol{\\varepsilon}_t \\in \\mathbb{R}^k$ 是一个随机新息（噪声）向量，从均值为零、协方差矩阵为 $\\Sigma$ 的多元正态分布中抽取，记作 $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$。\n\n模拟过程如下：\n1. 将时间 $t=0$ 时的状态向量初始化为 $\\mathbf{x}_0 = \\mathbf{0}$。\n2. 从 $\\mathcal{N}(\\mathbf{0}, \\Sigma)$ 生成一个新息向量序列 $\\boldsymbol{\\varepsilon}_1, \\boldsymbol{\\varepsilon}_2, \\dots, \\boldsymbol{\\varepsilon}_{N_{sim}}$，其中 $N_{sim}$ 是总模拟步数。\n3. 对 $t=1, \\dots, N_{sim}$，使用VAR方程迭代计算状态向量 $\\mathbf{x}_t$。\n4. 为确保过程达到其平稳分布，使用一个200步的预烧期。前200个样本被丢弃。\n5. 随后的 $T$ 个样本构成了用于推断的时间序列数据。最终数据是一个观测矩阵 $X \\in \\mathbb{R}^{T \\times k}$。\n\n**2. 通过嵌套模型比较进行格兰杰因果关系推断**\n\n格兰杰因果关系提供了一种基于预测的因果关系统计定义。如果一个时间序列 $x_j$ 的过去值包含了有助于预测另一个时间序列 $x_i$ 的信息，且这些信息超出了系统中所有其他变量（包括 $x_i$ 自身）的过去值所包含的信息，那么就称 $x_j$ 格兰杰导致 $x_i$。\n\n对于一个VAR(1)过程，这转化为检验 $x_i(t)$ 的行方程中系数 $A_{ij}$ 的显著性：\n$$ x_i(t) = \\sum_{m=1}^{k} A_{im} x_m(t-1) + \\varepsilon_i(t) $$\n从 $x_j$到 $x_i$ 不存在格兰杰因果关系的原假设是 $H_0: A_{ij} = 0$。\n\n为了检验这个假设，我们比较两个线性回归模型：\n- **非受限模型：** 该模型包含 $x_i(t)$ 的所有可能的滞后预测变量。其方程为：\n$$ x_i(t) = \\sum_{m=1}^{k} \\beta_{im} x_m(t-1) + u_t $$\n预测变量为 $\\{x_1(t-1), x_2(t-1), \\dots, x_k(t-1)\\}$。\n- **受限模型：** 该模型在原假设 $H_0$ 下构建，排除了来自 $x_j(t-1)$ 的影响。其方程为：\n$$ x_i(t) = \\sum_{m=1, m \\neq j}^{k} \\beta_{im} x_m(t-1) + v_t $$\n预测变量为 $\\{x_1(t-1), \\dots, x_{j-1}(t-1), x_{j+1}(t-1), \\dots, x_k(t-1)\\}$。\n\n两个模型都使用普通最小二乘法（OLS）进行拟合，该方法最小化残差平方和（RSS）。设 $RSS_U$ 为非受限模型的RSS，$RSS_R$ 为受限模型的RSS。根据构造，$RSS_U \\le RSS_R$。RSS的显著下降（即 $RSS_U \\ll RSS_R$）表明被排除的预测变量 $x_j(t-1)$ 具有显著的预测能力。\n\n**3. 用于统计显著性的F检验**\n\n预测改进的统计显著性通过嵌套模型的F检验来量化。F统计量的构造是每个新预测变量解释的方差与完整模型未解释的方差之比：\n$$ F = \\frac{(RSS_R - RSS_U) / q}{RSS_U / (N - k_U)} $$\n该统计量的参数定义如下：\n- $N$：回归中使用的观测数量。对于长度为 $T$ 且滞后为 $p=1$ 的时间序列，我们有 $N = T-1$ 个数据点用于回归。\n- $k_U$：非受限模型中的预测变量数量。对于 $k$ 维的VAR(1)模型，$k_U = k$。\n- $q$：为得到受限模型而对非受限模型施加的约束数量。这里，我们检验单个系数，所以 $q=1$。\n\n在原假设 $H_0$ 下，该F统计量服从自由度为 $(q, N - k_U)$ 的F分布。在这个具体问题中，分布是 $F(1, T-1-k)$。\n\n对于每个潜在的有向边 $(j \\to i)$，我们执行以下步骤：\n1. 从模拟数据中构建响应向量 $\\mathbf{y} = [x_i(2), \\dots, x_i(T)]^T$ 以及非受限模型（$X_U$）和受限模型（$X_R$）的设计矩阵。\n2. 使用OLS（例如`numpy.linalg.lstsq`）拟合两个模型，并获得 $RSS_U$ 和 $RSS_R$。\n3. 计算F统计量 $F_{obs} = ((RSS_R - RSS_U) / 1) / (RSS_U / (T - 1 - k))$。\n4. 计算p值，即概率 $P(F_{1, T-1-k} > F_{obs})$。这通过F分布的生存函数计算。\n5. 将p值与显著性水平 $\\alpha = 0.01$ 进行比较。如果p值小于 $\\alpha$，我们拒绝原假设并得出结论：$x_j$ 格兰杰导致 $x_i$。从节点 $j$ 到节点 $i$ 画一条边。否则，我们未能拒绝 $H_0$，不画边。\n\n非受限模型必须减少预测误差的条件等价于 $RSS_U  RSS_R$。如果F统计量为正，则该条件自然得到满足，而F统计量为正是以任何非平凡的显著性水平拒绝原假设的必要条件。\n\n对所有 $k(k-1)$ 个不同的变量有序对 $(j, i)$ 重复此过程，以构建完整的推断邻接矩阵。最终输出是针对所要求的特定边的二进制值列表。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for Granger causality inference.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of the entire process.\n    np.random.seed(42)\n\n    test_cases = [\n        # Case 1 (chain)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 500, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0.4, 0, 0], [0, 0.4, 0]]),\n            \"Sigma\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        },\n        # Case 2 (bidirectional and fan-out)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 800, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0.5, 0], [0.5, 0, 0], [0.3, 0, 0]]),\n            \"Sigma\": np.array([[0.5, 0, 0], [0, 0.5, 0], [0, 0, 0.5]])\n        },\n        # Case 3 (null model)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 600, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            \"Sigma\": np.array([[1.5, 0, 0], [0, 1.5, 0], [0, 0, 1.5]])\n        },\n        # Case 4 (confounding contemporaneous noise correlation)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 700, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0.35, 0, 0], [0, 0.2, 0]]),\n            \"Sigma\": np.array([[1, 0, 0.6], [0, 1, 0], [0.6, 0, 1]])\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = infer_network_granger(params)\n        all_results.append(result)\n\n    # Format the final output as a single string.\n    # e.g., [[0,1,0,0,1,0],[...]]\n    output_str = \"[\" + \",\".join([str(res) for res in all_results]) + \"]\"\n    print(output_str)\n\ndef infer_network_granger(params):\n    \"\"\"\n    Simulates VAR data and performs Granger causality inference for one test case.\n    \"\"\"\n    k = params[\"k\"]\n    p = params[\"p\"]\n    T = params[\"T\"]\n    burn_in = params[\"burn_in\"]\n    A = params[\"A\"]\n    Sigma = params[\"Sigma\"]\n    alpha = params[\"alpha\"]\n    \n    # 1. Simulate the VAR(p) process\n    total_samples = T + burn_in\n    x = np.zeros((total_samples + p, k)) # Store history including initial zeros\n\n    # Generate noise for all time steps at once\n    mean_noise = np.zeros(k)\n    # Correct cholesky decomposition for noise generation\n    L = np.linalg.cholesky(Sigma)\n    innovations = (L @ np.random.randn(k, total_samples)).T\n    \n    for t in range(p, total_samples + p):\n        # The VAR model formulation is x_t = A x_{t-1} + eps\n        # Since p=1 is given for all test cases, this simplification is fine.\n        lagged_x = x[t-1, :]\n        x[t, :] = A @ lagged_x + innovations[t-p, :]\n\n    # Discard burn-in period to get the final time series data\n    data = x[p + burn_in:, :]\n\n    # 2. Inference: Perform Granger causality test for each pair (j -> i)\n    adj_matrix = np.zeros((k, k), dtype=int)\n    \n    # The number of observations for regression is T-p\n    # For a lag of p=1, we predict x(t) from x(t-1) for t=1..T-1\n    # Thus, we have T-1 observations.\n    num_obs = T - p\n    \n    for i in range(k):  # Target variable index\n        for j in range(k):  # Source variable index\n            if i == j:\n                continue\n            \n            # Prepare data for regression\n            # Response variable y is the i-th time series from t=p to T-1\n            y = data[p:, i]\n            \n            # --- Unrestricted Model ---\n            # Predictors are all k variables at lag p\n            # For p=1, this is data from t=0 to T-2\n            X_unrestricted = data[p-1:-1, :]\n            k_unrestricted = X_unrestricted.shape[1]\n            \n            # Fit using OLS\n            _, rss_u_array, _, _ = np.linalg.lstsq(X_unrestricted, y, rcond=None)\n            rss_u = rss_u_array[0] if rss_u_array.size > 0 else 0.0\n\n            # --- Restricted Model ---\n            # Predictors are all variables at lag p except j\n            predictor_indices = list(range(k))\n            predictor_indices.remove(j)\n            X_restricted = data[p-1:-1, predictor_indices]\n\n            # Fit using OLS\n            _, rss_r_array, _, _ = np.linalg.lstsq(X_restricted, y, rcond=None)\n            rss_r = rss_r_array[0] if rss_r_array.size > 0 else 0.0\n            \n            # --- F-Test ---\n            # q = number of restrictions (parameters removed)\n            q = k_unrestricted - X_restricted.shape[1]\n            \n            # Degrees of freedom for F-distribution\n            df1 = q\n            df2 = num_obs - k_unrestricted\n\n            if rss_u > 1e-10: # Avoid division by zero\n                f_stat = ((rss_r - rss_u) / df1) / (rss_u / df2)\n                \n                # p-value from survival function (1 - cdf)\n                p_value = f_dist.sf(f_stat, df1, df2)\n                \n                if p_value  alpha:\n                    adj_matrix[j, i] = 1 # Edge from j to i\n    \n    # 3. Format output according to problem specification\n    # Order: (1->2), (1->3), (2->1), (2->3), (3->1), (3->2)\n    # Corresponds to (0-based): adj[0,1], adj[0,2], adj[1,0], adj[1,2], adj[2,0], adj[2,1]\n    output_list = [\n        adj_matrix[0, 1],\n        adj_matrix[0, 2],\n        adj_matrix[1, 0],\n        adj_matrix[1, 2],\n        adj_matrix[2, 0],\n        adj_matrix[2, 1],\n    ]\n    \n    return output_list\n\nsolve()\n```"
        },
        {
            "introduction": "在实现了基本方法之后，我们现在面临一个在实际应用中的关键挑战：虚假因果关系。这个理论练习  从数学上展示了一个未观测到的共同驱动因素（即潜在混杂因素）与测量噪声相结合，如何在没有直接因果联系的情况下，制造出存在因果关系的假象。理解这个陷阱对于正确解释格兰杰因果关系的结果和避免错误结论至关重要。",
            "id": "4278730",
            "problem": "考虑一对观测时间序列 $\\{X_{t}^{o}\\}$ 和 $\\{Y_{t}\\}$，它们由一个未被观测到的潜混杂因子过程 $\\{L_{t}\\}$ 生成。潜驱动因子 $\\{L_{t}\\}$ 遵循一个稳定的一阶自回归模型，定义为 $L_{t} = \\alpha L_{t-1} + \\epsilon_{t}^{L}$，其中 $\\epsilon_{t}^{L}$ 是均值为零、独立同分布的高斯噪声，其方差为 $\\sigma_{L}^{2}$，且为保证平稳性有 $|\\alpha|  1$。观测变量由滞后的潜过程线性驱动，并带有独立的高斯新息：$X_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X}$ 和 $Y_{t} = \\gamma L_{t-1} + \\epsilon_{t}^{Y}$，其中 $\\epsilon_{t}^{X}$ 和 $\\epsilon_{t}^{Y}$ 分别是均值为零、独立同分布的高斯噪声，其方差分别为 $\\sigma_{X}^{2}$ 和 $\\sigma_{Y}^{2}$。$X_{t}$ 的测量受到方差为 $\\sigma_{\\nu}^{2}$ 的独立高斯噪声 $\\nu_{t}$ 的干扰，得到 $X_{t}^{o} = X_{t} + \\nu_{t}$。所有新息项和测量噪声项在时间上和过程间都是相互独立的，并且也独立于 $\\{L_{t}\\}$。\n\n一位研究人员在不知道潜混杂因子和测量噪声的情况下，对观测对 $(Y_{t}, X_{t}^{o})$ 拟合一个一阶二元向量自回归 (VAR) 模型，并从过去的滞后项 $(Y_{t-1}, X_{t-1}^{o})$ 评估 $Y_{t}$ 的单步线性预测。在高斯假设下的大样本中，普通最小二乘 (OLS) 预测器等于 $Y_{t}$ 在回归量上的线性最小二乘投影。设 $Y_{t}$ 的受限预测器仅使用 $Y_{t-1}$，而非受限预测器同时使用 $Y_{t-1}$ 和 $X_{t-1}^{o}$。将从 $X$ 到 $Y$ 的格兰杰因果关系 (GC) 定义为 $Y_{t}$ 的受限单步预测误差方差与非受限单步预测误差方差之比的自然对数。\n\n取具体参数值 $\\alpha = \\tfrac{1}{2}$，$\\sigma_{L}^{2} = 1$，$\\beta = 1$，$\\gamma = 1$，$\\sigma_{X}^{2} = 1$，$\\sigma_{Y}^{2} = 1$ 以及 $\\sigma_{\\nu}^{2} = 1$。在这些条件下，计算由拟合到 $(Y_{t}, X_{t}^{o})$ 的二元 VAR($1$) 模型所隐含的从 $X$ 到 $Y$ 的精确渐近格兰杰因果关系，并将其表示为单一的闭式解析表达式。该量是无量纲的。无需四舍五入；请提供精确表达式。",
            "solution": "问题陈述被评估为有效。它在科学上基于时间序列分析和格兰杰因果关系的理论，所有必要的参数和定义都已提供，问题阐述清晰，语言客观。它提出了其领域内一个标准但并非无足轻重的计算。\n\n目标是计算从观测过程 $X^o$ 到过程 $Y$ 的渐近格兰杰因果关系 (GC)，其定义为：\n$$\nGC_{X \\to Y} = \\ln\\left( \\frac{\\sigma_{R}^{2}}{\\sigma_{U}^{2}} \\right)\n$$\n其中 $\\sigma_{R}^{2}$ 是仅使用 $Y_t$ 自身过去信息进行单步预测的误差方差，而 $\\sigma_{U}^{2}$ 是使用 $Y_t$ 和 $X_t^o$ 两者过去信息进行预测的误差方差。对于高斯过程，在渐近意义下，这些误差对应于线性最小二乘投影。\n\n该模型由以下方程定义：\n1.  潜过程：$L_{t} = \\alpha L_{t-1} + \\epsilon_{t}^{L}$，其中 $\\epsilon_{t}^{L} \\sim N(0, \\sigma_{L}^{2})$。\n2.  观测过程（理想情况）：$Y_{t} = \\gamma L_{t-1} + \\epsilon_{t}^{Y}$ 和 $X_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X}$，其中 $\\epsilon_{t}^{X} \\sim N(0, \\sigma_{X}^{2})$ 且 $\\epsilon_{t}^{Y} \\sim N(0, \\sigma_{Y}^{2})$。\n3.  带噪声的观测：$X_{t}^{o} = X_{t} + \\nu_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X} + \\nu_{t}$，其中 $\\nu_{t} \\sim N(0, \\sigma_{\\nu}^{2})$。\n\n假设所有过程都是平稳的且均值为零。我们首先计算必要的二阶矩（方差和协方差）。\n\n首先，平稳潜过程 $L_t$ 的方差为：\n$$\n\\text{Var}(L_t) = \\Sigma_{LL}(0) = \\frac{\\sigma_{L}^{2}}{1 - \\alpha^2}\n$$\n$L_t$ 在滞后 $k$ 阶的自协方差为 $\\Sigma_{LL}(k) = \\text{Cov}(L_t, L_{t-k}) = \\alpha^{k} \\Sigma_{LL}(0)$。\n\n给定的参数值为：$\\alpha = \\frac{1}{2}$，$\\sigma_{L}^{2} = 1$，$\\beta = 1$，$\\gamma = 1$，$\\sigma_{X}^{2} = 1$，$\\sigma_{Y}^{2} = 1$ 以及 $\\sigma_{\\nu}^{2} = 1$。\n代入这些值，我们求得 $L_t$ 的方差：\n$$\n\\Sigma_{LL}(0) = \\frac{1}{1 - (\\frac{1}{2})^2} = \\frac{1}{1 - \\frac{1}{4}} = \\frac{1}{\\frac{3}{4}} = \\frac{4}{3}\n$$\n\n接下来，我们计算观测过程 $Y_t$ 和 $X_t^o$ 的方差和协方差。由于平稳性，$\\text{Var}(Y_t) = \\text{Var}(Y_{t-1})$ 且 $\\text{Var}(X_t^o) = \\text{Var}(X_{t-1}^o)$。\n$$\n\\text{Var}(Y_t) = \\text{Var}(\\gamma L_{t-1} + \\epsilon_{t}^{Y}) = \\gamma^2 \\text{Var}(L_{t-1}) + \\text{Var}(\\epsilon_{t}^{Y}) = \\gamma^2 \\Sigma_{LL}(0) + \\sigma_{Y}^{2}\n$$\n$$\n\\text{Var}(Y_t) = (1)^2 \\left(\\frac{4}{3}\\right) + 1 = \\frac{7}{3}\n$$\n$$\n\\text{Var}(X_t^o) = \\text{Var}(\\beta L_{t-1} + \\epsilon_{t}^{X} + \\nu_{t}) = \\beta^2 \\text{Var}(L_{t-1}) + \\text{Var}(\\epsilon_{t}^{X}) + \\text{Var}(\\nu_{t}) = \\beta^2 \\Sigma_{LL}(0) + \\sigma_{X}^{2} + \\sigma_{\\nu}^{2}\n$$\n$$\n\\text{Var}(X_t^o) = (1)^2 \\left(\\frac{4}{3}\\right) + 1 + 1 = \\frac{4}{3} + 2 = \\frac{10}{3}\n$$\n$Y_{t-1}$ 和 $X_{t-1}^o$ 之间的互协方差为：\n$$\n\\text{Cov}(Y_{t-1}, X_{t-1}^o) = \\text{E}[(\\gamma L_{t-2} + \\epsilon_{t-1}^{Y})(\\beta L_{t-2} + \\epsilon_{t-1}^{X} + \\nu_{t-1})]\n$$\n由于所有噪声项相互独立且独立于 $L_t$，这可以简化为：\n$$\n\\text{Cov}(Y_{t-1}, X_{t-1}^o) = \\gamma \\beta \\text{E}[L_{t-2}^2] = \\gamma \\beta \\Sigma_{LL}(0) = (1)(1)\\left(\\frac{4}{3}\\right) = \\frac{4}{3}\n$$\n\n现在我们计算 $Y_t$ 与其预测变量 $Y_{t-1}$ 和 $X_{t-1}^o$ 之间的协方差。\n$$\n\\text{Cov}(Y_t, Y_{t-1}) = \\text{E}[(\\gamma L_{t-1} + \\epsilon_{t}^{Y})(\\gamma L_{t-2} + \\epsilon_{t-1}^{Y})] = \\gamma^2 \\text{E}[L_{t-1}L_{t-2}]\n$$\n由于 $L_{t-1} = \\alpha L_{t-2} + \\epsilon_{t-1}^L$，我们有 $\\text{E}[L_{t-1}L_{t-2}] = \\alpha \\text{E}[L_{t-2}^2] = \\alpha \\Sigma_{LL}(0)$。\n$$\n\\text{Cov}(Y_t, Y_{t-1}) = \\gamma^2 \\alpha \\Sigma_{LL}(0) = (1)^2 \\left(\\frac{1}{2}\\right) \\left(\\frac{4}{3}\\right) = \\frac{2}{3}\n$$\n$$\n\\text{Cov}(Y_t, X_{t-1}^o) = \\text{E}[(\\gamma L_{t-1} + \\epsilon_{t}^{Y})(\\beta L_{t-2} + \\epsilon_{t-1}^{X} + \\nu_{t-1})] = \\gamma \\beta \\text{E}[L_{t-1}L_{t-2}] = \\gamma \\beta \\alpha \\Sigma_{LL}(0)\n$$\n$$\n\\text{Cov}(Y_t, X_{t-1}^o) = (1)(1)\\left(\\frac{1}{2}\\right)\\left(\\frac{4}{3}\\right) = \\frac{2}{3}\n$$\n\n受限预测误差方差 $\\sigma_R^2$ 是用于从 $Y_{t-1}$ 预测 $Y_t$：\n$$\n\\sigma_R^2 = \\text{Var}(Y_t) - \\frac{\\text{Cov}(Y_t, Y_{t-1})^2}{\\text{Var}(Y_{t-1})}\n$$\n$$\n\\sigma_R^2 = \\frac{7}{3} - \\frac{(\\frac{2}{3})^2}{\\frac{7}{3}} = \\frac{7}{3} - \\frac{\\frac{4}{9}}{\\frac{7}{3}} = \\frac{7}{3} - \\frac{4}{9} \\cdot \\frac{3}{7} = \\frac{7}{3} - \\frac{4}{21} = \\frac{49 - 4}{21} = \\frac{45}{21} = \\frac{15}{7}\n$$\n\n非受限预测误差方差 $\\sigma_U^2$ 是用于从回归量向量 $Z_{t-1} = \\begin{pmatrix} Y_{t-1} \\\\ X_{t-1}^o \\end{pmatrix}$ 预测 $Y_t$。该方差由 $\\sigma_U^2 = \\text{Var}(Y_t) - c^T \\Sigma_Z^{-1} c$ 给出，其中 $\\Sigma_Z$ 是 $Z_{t-1}$ 的协方差矩阵，$c$ 是 $Y_t$ 和 $Z_{t-1}$ 之间的协方差向量。\n$$\n\\Sigma_Z = \\begin{pmatrix} \\text{Var}(Y_{t-1})  \\text{Cov}(Y_{t-1}, X_{t-1}^o) \\\\ \\text{Cov}(Y_{t-1}, X_{t-1}^o)  \\text{Var}(X_{t-1}^o) \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3}  \\frac{4}{3} \\\\ \\frac{4}{3}  \\frac{10}{3} \\end{pmatrix}\n$$\n$$\nc = \\begin{pmatrix} \\text{Cov}(Y_t, Y_{t-1}) \\\\ \\text{Cov}(Y_t, X_{t-1}^o) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\n$\\Sigma_Z$ 的行列式是：\n$$\n\\det(\\Sigma_Z) = \\left(\\frac{7}{3}\\right)\\left(\\frac{10}{3}\\right) - \\left(\\frac{4}{3}\\right)^2 = \\frac{70}{9} - \\frac{16}{9} = \\frac{54}{9} = 6\n$$\n$\\Sigma_Z$ 的逆矩阵是：\n$$\n\\Sigma_Z^{-1} = \\frac{1}{6} \\begin{pmatrix} \\frac{10}{3}  -\\frac{4}{3} \\\\ -\\frac{4}{3}  \\frac{7}{3} \\end{pmatrix}\n$$\n现在我们计算 $c^T \\Sigma_Z^{-1} c$ 这一项：\n$$\nc^T \\Sigma_Z^{-1} c = \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\frac{1}{6} \\begin{pmatrix} \\frac{10}{3}  -\\frac{4}{3} \\\\ -\\frac{4}{3}  \\frac{7}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\n$$\nc^T \\Sigma_Z^{-1} c = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{10}{3} \\cdot \\frac{2}{3} - \\frac{4}{3} \\cdot \\frac{2}{3} \\\\ -\\frac{4}{3} \\cdot \\frac{2}{3} + \\frac{7}{3} \\cdot \\frac{2}{3} \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{20-8}{9} \\\\ \\frac{-8+14}{9} \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{12}{9} \\\\ \\frac{6}{9} \\end{pmatrix}\n$$\n$$\nc^T \\Sigma_Z^{-1} c = \\frac{1}{6} \\left( \\frac{2}{3} \\cdot \\frac{12}{9} + \\frac{2}{3} \\cdot \\frac{6}{9} \\right) = \\frac{1}{6} \\left( \\frac{24}{27} + \\frac{12}{27} \\right) = \\frac{1}{6} \\left( \\frac{36}{27} \\right) = \\frac{1}{6} \\left( \\frac{4}{3} \\right) = \\frac{4}{18} = \\frac{2}{9}\n$$\n非受限误差方差为：\n$$\n\\sigma_U^2 = \\text{Var}(Y_t) - c^T \\Sigma_Z^{-1} c = \\frac{7}{3} - \\frac{2}{9} = \\frac{21 - 2}{9} = \\frac{19}{9}\n$$\n最后，我们计算格兰杰因果关系度量：\n$$\nGC_{X \\to Y} = \\ln\\left( \\frac{\\sigma_{R}^{2}}{\\sigma_{U}^{2}} \\right) = \\ln\\left( \\frac{\\frac{15}{7}}{\\frac{19}{9}} \\right) = \\ln\\left( \\frac{15}{7} \\cdot \\frac{9}{19} \\right) = \\ln\\left( \\frac{135}{133} \\right)\n$$\n这个正值表明，尽管在数据生成过程中没有直接的因果联系，但由于潜混杂因子和测量噪声的共同作用，检测到了从 $X^o$ 到 $Y$ 的伪因果关系。",
            "answer": "$$\\boxed{\\ln\\left(\\frac{135}{133}\\right)}$$"
        },
        {
            "introduction": "为了应对噪声和统计不确定性带来的挑战，我们必须构建一个更严谨的推断流程。这个高级实践  通过整合必要的验证技术，提升了我们最初的实现。你将实现非参数置换检验以补充参数检验，应用错误发现率（FDR）校正来处理多重比较问题，并评估你研究结果的可复现性，这反映了现代计算科学的标准。",
            "id": "4278729",
            "problem": "您的任务是设计并实现一个独立的程序，对由滞后阶数 $1$ 的稳定向量自回归（VAR）过程生成的合成多元时间序列，执行经统计验证的格兰杰因果网络推断，并进行可复现性评估。此问题必须从第一性原理出发，基于格兰杰因果关系和线性模型的核心定义来解决，不依赖任何预先提供的专用公式。\n\n此任务的基础是以下这组定义和概念：\n- 格兰杰因果关系：如果时间序列 $X_j$ 的过去信息在所有其他变量的过去信息之外，对 $X_i$ 提供了统计上显著的预测信息，则称 $X_j$ 格兰杰导致 $X_i$。在具有高斯新息的线性模型下，这被操作化为对预测 $X_i(t)$ 的方程进行嵌套模型比较，比较的对象是包含所有变量滞后值的完整模型与排除了 $X_j$ 滞后值的简化模型。\n- 普通最小二乘法（OLS）：对于线性模型 $Y = X\\beta + \\varepsilon$，OLS 估计量通过最小化残差平方和（由 $Y - X\\hat{\\beta}$ 给出）来求解。模型比较基于残差平方和与自由度。\n- 错误发现率（FDR）：错误发现率（FDR）是在所有阳性发现中假阳性发现的期望比例。对于一组假设检验，Benjamini–Hochberg 程序通过对排序后的 $p$ 值选择一个阈值，将 FDR 控制在用户指定的水平 $q$。\n- 通过置换进行经验验证：对于一个特定的假定因果联系 $(j \\to i)$，可以通过随机置换预测变量 $X_j(t-1)$ 的时间索引，同时保持响应变量 $X_i(t)$ 和其他预测变量不变，来获得一个非参数零分布；经验 $p$ 值通过计算零假设统计量超过观测统计量的频率来得出。\n\n您必须从一个具有 $N$ 个变量、滞后阶数为 $1$ 的稳定 VAR 过程中生成合成数据，该过程由以下线性递归定义：\n$$\n\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\epsilon}_t,\n$$\n其中 $\\boldsymbol{\\epsilon}_t$ 是独立同分布的高斯新息，协方差为 $\\sigma^2 I$，$A$ 是 $N \\times N$ 的系数矩阵。通过缩放 $A$ 使其谱半径严格小于 $1$ 来强制实现稳定性。\n\n非零假设情况下的真实加权邻接模板 $W$ 由以下条目指定（所有未指定的条目均为零）。权重矩阵 $W$ 的维度为 $N=5$，具体如下：\n$$\nW = \\begin{bmatrix}\n0.4  0.2  0.0  0.0  0.0 \\\\\n0.0  0.4  0.25  0.0  0.0 \\\\\n0.1  0.0  0.4  0.3  0.0 \\\\\n0.0  0.0  0.0  0.4  0.2 \\\\\n0.0  0.15  0.0  0.0  0.4\n\\end{bmatrix}.\n$$\n对于每个非零假设测试用例，设置 $A = s W$，其中 $s = 0.9 / \\rho(W)$，$\\rho(W)$ 是 $W$ 的谱半径。这确保了 VAR 过程的稳定性。对于零假设测试用例，令 $W$ 为对角矩阵：\n$$\nW_{\\text{null}} = \\begin{bmatrix}\n0.4  0  0  0  0 \\\\\n0  0.4  0  0  0 \\\\\n0  0  0.4  0  0 \\\\\n0  0  0  0.4  0 \\\\\n0  0  0  0  0.4\n\\end{bmatrix},\n$$\n并再次使用相同的缩放规则设置 $A = s W_{\\text{null}}$。\n\n对于每个测试用例，按以下步骤进行：\n1. 数据生成：对于每个测试用例，使用相同的 $A$ 但为新息使用不同的随机种子 $s_1$ 和 $s_2$，生成 VAR 过程的两个独立实现（复本）。使用 $x_0 = \\mathbf{0}$ 进行初始化。每个用例的种子必须是确定性的：设置 $s_1 = b$ 和 $s_2 = b+1$，其中 $b$ 是测试套件中指定的基础种子。新息必须从方差为 $\\sigma^2$ 的高斯分布中抽取，即 $\\boldsymbol{\\epsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$。\n2. 模型拟合与嵌套比较：对于每个 $j \\neq i$ 的有序对 $(j,i)$，使用所有滞后变量 $X_k(t-1)$（其中 $k \\in \\{1,\\dots,N\\}$）为 $X_i(t)$ 拟合完整的 OLS 回归模型，并拟合排除 $X_j(t-1)$ 的简化回归模型。从嵌套线性模型比较的第一性原理出发，推导一个合适的比较统计量，并在其适当的参考分布下计算一个参数 $p$ 值，同时考虑样本大小和自由度。\n3. 经验验证：对于每个假定的因果边 $(j \\to i)$，通过对预测变量 $X_j(t-1)$ 的时间索引进行 $B$ 次独立的随机置换（同时保持其他所有变量固定）来构建一个经验零分布。对于每个置换后的数据集，重新计算相同的比较统计量。计算经验 $p$ 值，其值为统计量至少与观测统计量一样大的置换比例，并使用标准的有限样本校正（分子和分母各加 $1$）。\n4. 多重检验控制：对所有 $j \\neq i$ 的有序对 $(j,i)$ 的参数 $p$ 值集和经验 $p$ 值集分别应用 Benjamini–Hochberg FDR 程序。将一个复本的“已验证边集”定义为在水平 $q$ 下，通过参数和经验 FDR 程序共同选择的边的交集。\n5. 可复现性评估：将测试用例的可复现边集定义为两个独立复本的已验证边集的交集。令真实边集为所有满足 $W_{i j} > 0$ 的有序对 $(j,i)$（$j \\neq i$）（在非零假设情况下），在零假设情况下为空集。计算可复现边集相对于真实边集的精确率和召回率；如果分母为零，在此任务中将相应的比率定义为 $0$。\n\n您的程序必须实现上述工作流程，并为以下测试套件生成结果。每个元组列出 $(T, \\sigma, q, B, \\text{is\\_null}, b)$:\n- 用例 1: $(T = 1000, \\sigma = 0.1, q = 0.05, B = 200, \\text{is\\_null} = 0, b = 11)$。\n- 用例 2: $(T = 200, \\sigma = 0.1, q = 0.05, B = 200, \\text{is\\_null} = 0, b = 13)$。\n- 用例 3: $(T = 1000, \\sigma = 0.1, q = 0.01, B = 200, \\text{is\\_null} = 0, b = 17)$。\n- 用例 4: $(T = 1000, \\sigma = 0.1, q = 0.05, B = 200, \\text{is\\_null} = 1, b = 19)$。\n\n对于每个测试用例，您的程序必须输出一个包含三个条目的列表：可复现边集的整数大小、作为浮点数的精确率，以及作为浮点数的召回率。您的程序应生成单行输出，其中包含这些按用例排列的列表的逗号分隔列表，并用方括号括起来（例如，`[[a_1,p_1,r_1],[a_2,p_2,r_2],[a_3,p_3,r_3],[a_4,p_4,r_4]]`）。此问题不涉及物理单位，也不需要角度单位。",
            "solution": "此问题的目标是设计并实现一个计算流程，用于从多元时间序列数据中推断因果网络。这是通过格兰杰因果关系原理实现的，并通过参数和非参数（基于置换的）假设检验进行统计验证，同时对多重比较进行校正。最后一步涉及评估推断网络在独立数据实现间的可复现性。整个过程是基于线性代数和统计学的第一性原理构建的。\n\n### 1. 理论框架\n\n#### 1.1. 数据生成：向量自回归（VAR）过程\n\n合成的时间序列数据由一个稳定的一阶向量自回归（或称 $\\text{VAR}(1)$）过程生成。该模型将 $N$ 个时间序列变量的向量 $\\mathbf{x}_t \\in \\mathbb{R}^N$ 的演化描述为其前一时间步状态的线性函数，外加一个随机新息项。其控制方程为：\n$$\n\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\epsilon}_t\n$$\n其中：\n- $\\mathbf{x}_t = [X_1(t), X_2(t), \\dots, X_N(t)]^T$ 是时间 $t$ 的状态向量。\n- $A$ 是 $N \\times N$ 的系数矩阵，其中元素 $A_{ij}$ 量化了变量 $X_j$ 在时间 $t-1$ 对变量 $X_i$ 在时间 $t$ 的线性影响。\n- $\\boldsymbol{\\epsilon}_t$ 是一个独立同分布（i.i.d.）的高斯新息向量，从多元正态分布 $\\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$ 中抽取，其中 $I$ 是 $N \\times N$ 的单位矩阵，$\\sigma^2$ 是噪声的方差。\n\n为使过程稳定（即不发散），系数矩阵 $A$ 的谱半径（记为 $\\rho(A)$）必须严格小于 $1$。谱半径是 $A$ 的特征值的最大绝对值。为确保稳定性，我们通过缩放一个真实加权邻接矩阵 $W$ 来构造 $A$：$A = sW$，缩放因子 $s = 0.9 / \\rho(W)$。这保证了 $\\rho(A) = 0.9  1$。\n\n模拟从初始条件 $\\mathbf{x}_0 = \\mathbf{0}$ 开始，进行 $T$ 个时间步，生成一个大小为 $N \\times T$ 的数据矩阵。\n\n#### 1.2. 作为嵌套模型检验的格兰杰因果关系\n\n格兰杰因果关系提供了一个基于预测来确定因果关系的统计框架。如果变量 $X_j$ 的过去值包含了有助于预测 $X_i$ 的信息，且这些信息超出了 $X_i$ 自身及所有其他变量过去值所包含的信息，则称 $X_j$ 格兰杰导致 $X_i$。\n\n对于 $\\text{VAR}(1)$ 模型，此假设可以通过比较两个用于预测 $X_i(t)$ 的嵌套线性回归模型来检验，使用的数据为 $t=1, \\dots, T-1$。用于回归的观测数量为 $n = T-1$。\n\n- **完整模型**：该模型包含所有 $N$ 个变量的滞后值作为预测变量。\n$$\nX_i(t) = \\sum_{k=1}^{N} \\beta_k X_k(t-1) + \\varepsilon_{\\text{full}}(t)\n$$\n- **简化模型**：该模型排除了假定的因果变量 $X_j(t-1)$ 的滞后值。\n$$\nX_i(t) = \\sum_{k=1, k \\neq j}^{N} \\beta_k X_k(t-1) + \\varepsilon_{\\text{red}}(t)\n$$\n\n零假设 $H_0$ 是 $X_j$ 不格兰杰导致 $X_i$，这对应于完整模型中 $X_j(t-1)$ 的回归系数为零。如果完整模型对数据的拟合在统计上显著优于简化模型，我们则拒绝 $H_0$。\n\n#### 1.3. 用于模型比较的 F 统计量\n\n完整模型和简化模型之间的比较通过 F 检验进行。两个模型都使用普通最小二乘法（OLS）进行拟合，该方法最小化残差平方和（RSS）。对于一个通用模型 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其 RSS 由 $\\text{RSS} = ||\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}||^2$ 给出，其中 $\\hat{\\boldsymbol{\\beta}}$ 是 OLS 估计值。\n\n令 $\\text{RSS}_{\\text{full}}$ 为完整模型的 RSS，$\\text{RSS}_{\\text{red}}$ 为简化模型的 RSS。F 统计量的公式为：\n$$\nF = \\frac{(\\text{RSS}_{\\text{red}} - \\text{RSS}_{\\text{full}}) / (p_{\\text{full}} - p_{\\text{red}})}{\\text{RSS}_{\\text{full}} / (n - p_{\\text{full}})}\n$$\n其中：\n- $n = T-1$ 是用于回归的观测数量。\n- $p_{\\text{full}} = N$ 是完整模型中的预测变量数量。\n- $p_{\\text{red}} = N-1$ 是简化模型中的预测变量数量。\n\n代入这些值，分子的自由度为 $df_1 = p_{\\text{full}} - p_{\\text{red}} = 1$，分母的自由度为 $df_2 = n - p_{\\text{full}} = T - 1 - N$。F 统计量简化为：\n$$\nF = \\frac{\\text{RSS}_{\\text{red}} - \\text{RSS}_{\\text{full}}}{\\text{RSS}_{\\text{full}} / (T - 1 - N)}\n$$\n在零假设下，该统计量服从 F 分布，$F \\sim \\mathcal{F}(df_1, df_2)$。**参数 p 值**是观测到至少与从数据中计算出的 F 统计量一样大的概率，$p_{\\text{param}} = P(Y \\ge F_{\\text{obs}} | Y \\sim \\mathcal{F}(df_1, df_2))$。\n\n#### 1.4. 通过置换检验计算经验 p 值\n\n为了构建一个非参数检验，我们为 F 统计量生成一个经验零分布。这是通过重复地打破预测变量 $X_j(t-1)$ 和响应变量 $X_i(t)$ 之间的时间关系，同时保持其他关系不变来实现的。对于一个特定的假定联系 $j \\to i$，其过程如下：\n1.  如上所述，计算观测到的 F 统计量 $F_{\\text{obs}}$。\n2.  重复以下步骤 $B$ 次：\n    a. 通过随机打乱其时间索引，为预测变量 $X_j$ 创建一个置换后的时间序列。这得到 $X_{j, \\text{perm}}(t-1)$。\n    b. 构建一个新的“完整模型”设计矩阵，其中对应于 $X_j(t-1)$ 的列被置换后的版本替换。简化模型保持不变。\n    c. 使用这个置换后的数据重新计算 F 统计量，从而得到一个来自零分布的样本 $F_{\\text{null}}$。\n3.  **经验 p 值**是零假设统计量大于或等于观测统计量的比例，并对有限次置换进行校正：\n$$\np_{\\text{emp}} = \\frac{1 + |\\{k \\in \\{1, \\dots, B\\} : F_{\\text{null},k} \\geq F_{\\text{obs}}\\}|}{1 + B}\n$$\n\n#### 1.5. 控制多重比较\n\n我们进行了 $m = N(N-1)$ 次假设检验，即对每个不同变量之间可能的有向边都进行一次检验。这需要进行多重检验校正，以控制错误发现的数量。Benjamini-Hochberg (BH) 程序用于将错误发现率（FDR）控制在指定的水平 $q$。该程序如下：\n1.  收集所有被检验边的 $m$ 个 p 值，$\\{p_1, p_2, \\dots, p_m\\}$。\n2.  将 p 值按升序排序：$p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$。\n3.  找到满足 $p_{(k)} \\le \\frac{k}{m} q$ 的最大整数 $k$。\n4.  对所有对应于 p 值 $p_{(1)}, \\dots, p_{(k)}$ 的检验拒绝零假设。\n\n此程序独立地应用于参数 p 值集和经验 p 值集。\n\n#### 1.6. 验证与可复现性\n\n为了增加对推断连接的置信度，我们结合了来自两种统计方法和独立实验的证据。\n- **已验证边集**：对于单个数据复本，已验证边集是通过 BH 校正的参数检验和 BH 校正的经验检验均发现显著的边的交集。\n- **可复现边集**：对于给定的测试用例，最终推断的连接集是从两个独立的数据复本（使用相同的 $A$ 矩阵但不同的新息随机种子生成）获得的已验证边集的交集。\n\n#### 1.7. 性能指标\n\n推断流程的性能是根据由矩阵 $W$ 定义的真实网络结构进行评估的。如果 $W_{ij} > 0$（对于 $j \\neq i$），则存在一条边 $j \\to i$。令 $E_{\\text{repro}}$ 为可复现边集，$E_{\\text{GT}}$ 为真实边集。\n- 真阳性（$TP$）：$|E_{\\text{repro}} \\cap E_{\\text{GT}}|$\n- 假阳性（$FP$）：$|E_{\\text{repro}} \\setminus E_{\\text{GT}}|$\n- 假阴性（$FN$）：$|E_{\\text{GT}} \\setminus E_{\\text{repro}}|$\n\n然后，精确率和召回率计算如下：\n- **精确率**：$P = \\frac{TP}{TP + FP}$。如果 $TP + FP = 0$，则 $P=0$。\n- **召回率**：$R = \\frac{TP}{TP + FN}$。如果 $TP + FN = 0$，则 $R=0$。\n\n每个测试用例的最终输出是可复现边集的大小（$|E_{\\text{repro}}|$）、精确率和召回率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the entire test suite and print the results.\n    \"\"\"\n    \n    # Ground truth matrix for non-null cases\n    W_non_null = np.array([\n        [0.4, 0.2, 0.0, 0.0, 0.0],\n        [0.0, 0.4, 0.25, 0.0, 0.0],\n        [0.1, 0.0, 0.4, 0.3, 0.0],\n        [0.0, 0.0, 0.0, 0.4, 0.2],\n        [0.0, 0.15, 0.0, 0.0, 0.4]\n    ])\n\n    # Ground truth matrix for null cases\n    W_null = np.diag([0.4] * 5)\n    \n    # Test suite parameters: (T, sigma, q, B, is_null, b)\n    test_cases = [\n        (1000, 0.1, 0.05, 200, 0, 11),\n        (200, 0.1, 0.05, 200, 0, 13),\n        (1000, 0.1, 0.01, 200, 0, 17),\n        (1000, 0.1, 0.05, 200, 1, 19),\n    ]\n\n    all_results = []\n    \n    for T, sigma, q, B, is_null, b in test_cases:\n        W = W_null if is_null else W_non_null\n        result = run_single_case(W, T, sigma, q, B, b)\n        all_results.append(result)\n\n    formatted_results = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_single_case(W, T, sigma, q, B, base_seed):\n    \"\"\"\n    Runs a single test case for Granger causality inference.\n    \"\"\"\n    N = W.shape[0]\n\n    # Stabilize the VAR process\n    spec_rad = np.max(np.abs(np.linalg.eigvals(W)))\n    s = 0.9 / spec_rad\n    A = s * W\n\n    # Determine ground truth edge set\n    ground_truth_edges = set()\n    if not np.all(W == np.diag(np.diag(W))): # Check if non-null\n        rows, cols = np.where(W != 0)\n        for i, j in zip(rows, cols):\n            if i != j:\n                # W_ij corresponds to edge j -> i in Granger causality terms\n                # (past of column j affects present of row i).\n                # But problem says A_ij is for j to i, and W_ij is for j to i.\n                # W[i,j] -> edge j to i.\n                ground_truth_edges.add((j, i))\n\n    # Run two independent replicates\n    seed1, seed2 = base_seed, base_seed + 1\n    validated_edges1 = infer_replicate(A, T, sigma, q, B, seed1)\n    validated_edges2 = infer_replicate(A, T, sigma, q, B, seed2)\n    \n    # Assess reproducibility\n    reproducible_edges = validated_edges1.intersection(validated_edges2)\n\n    # Compute performance metrics\n    tp = len(reproducible_edges.intersection(ground_truth_edges))\n    fp = len(reproducible_edges - ground_truth_edges)\n    fn = len(ground_truth_edges - reproducible_edges)\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    \n    return [len(reproducible_edges), precision, recall]\n\ndef infer_replicate(A, T, sigma, q, B, seed):\n    \"\"\"\n    Performs inference on a single replicate of the time series data.\n    \"\"\"\n    np.random.seed(seed)\n    N = A.shape[0]\n\n    # 1. Data Generation\n    X = np.zeros((N, T))\n    for t in range(1, T):\n        noise = np.random.normal(0, sigma, N)\n        X[:, t] = A @ X[:, t - 1] + noise\n\n    # 2. Model Fitting and Testing\n    edges = []\n    param_p_values = []\n    emp_p_values = []\n    \n    for i in range(N):  # Response variable\n        for j in range(N):  # Predictor variable\n            if i == j:\n                continue\n            \n            p_param, p_emp = compute_p_values(X, i, j, B)\n            edges.append((j, i))\n            param_p_values.append(p_param)\n            emp_p_values.append(p_emp)\n    \n    # 4. Multiple Testing Control and Validation\n    param_sig_edges = apply_fdr(edges, param_p_values, q)\n    emp_sig_edges = apply_fdr(edges, emp_p_values, q)\n    \n    validated_edges = param_sig_edges.intersection(emp_sig_edges)\n    return validated_edges\n\ndef compute_p_values(X, i, j, B):\n    \"\"\"\n    Computes parametric and empirical p-values for a single putative edge (j -> i).\n    \"\"\"\n    N, T = X.shape\n    n_obs = T - 1\n\n    y = X[i, 1:]\n    X_lagged = X[:, :-1].T\n    \n    # Reduced model design matrix (excluding predictor j)\n    X_red = np.delete(X_lagged, j, axis=1)\n\n    # Compute observed F-statistic\n    f_obs, _, _ = calculate_f_statistic(y, X_lagged, X_red)\n    \n    # Parametric p-value\n    df1 = 1\n    df2 = n_obs - N\n    p_param = f_dist.sf(f_obs, df1, df2)\n\n    # Empirical p-value from permutation test\n    f_perms = np.zeros(B)\n    rng_perm = np.random.default_rng(seed=i*N+j) # a deterministic seed for permutation\n    for k in range(B):\n        X_lagged_perm = X_lagged.copy()\n        X_lagged_perm[:, j] = rng_perm.permutation(X_lagged_perm[:, j])\n        f_perms[k], _, _ = calculate_f_statistic(y, X_lagged_perm, X_red)\n    \n    count_exceed = np.sum(f_perms >= f_obs)\n    p_emp = (1 + count_exceed) / (1 + B)\n    \n    return p_param, p_emp\n\ndef calculate_f_statistic(y, X_full, X_red):\n    \"\"\"\n    Calculates the F-statistic for nested OLS models.\n    \"\"\"\n    n_obs, p_full = X_full.shape\n    \n    # Fit full model\n    _, rss_full_arr, _, _ = np.linalg.lstsq(X_full, y, rcond=None)\n    rss_full = rss_full_arr[0] if len(rss_full_arr) > 0 else 0.0\n\n    # Fit reduced model\n    _, rss_red_arr, _, _ = np.linalg.lstsq(X_red, y, rcond=None)\n    rss_red = rss_red_arr[0] if len(rss_red_arr) > 0 else 0.0\n\n    # Handle cases of perfect fit, though unlikely with noise\n    if rss_full  np.finfo(float).eps:\n        return np.inf, rss_full, rss_red\n\n    df_num = 1\n    df_den = n_obs - p_full\n    if df_den = 0:\n        return 0.0, rss_full, rss_red\n    \n    f_stat = ((rss_red - rss_full) / df_num) / (rss_full / df_den)\n    return f_stat, rss_full, rss_red\n\ndef apply_fdr(edges, p_values, q):\n    \"\"\"\n    Applies the Benjamini-Hochberg FDR correction.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return set()\n\n    # Sort p-values and corresponding edges\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = np.array(p_values)[sorted_indices]\n    \n    # Find threshold\n    thresholds = (np.arange(1, m + 1) / m) * q\n    significant = sorted_p_values = thresholds\n    \n    if np.any(significant):\n        # Find the last significant p-value\n        k = np.where(significant)[0].max()\n        # All edges up to this index are significant\n        significant_indices = sorted_indices[:k+1]\n        return {edges[i] for i in significant_indices}\n    else:\n        return set()\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}