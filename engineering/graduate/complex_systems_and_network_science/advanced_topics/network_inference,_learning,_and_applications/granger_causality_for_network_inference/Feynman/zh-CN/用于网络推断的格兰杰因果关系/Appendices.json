{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的巩固方式莫过于亲手实践。这个练习将指导你从零开始，编写一个完整的程序，使用格兰杰因果关系的基本定义从多变量时间序列中推断有向网络。通过这个项目，你将把向量自回归（VAR）模型、普通最小二乘法（OLS）和假设检验等核心概念融会贯通，将抽象理论转化为可运行的代码，并直观地看到网络结构如何从数据中浮现。",
            "id": "4278726",
            "problem": "要求您实现一个完整的、可运行的程序，该程序使用格兰杰因果关系的定义从多变量时间序列中推断有向网络。该程序必须从向量自回归（VAR）过程中生成合成数据，并应用假设检验来检测变量之间的有向边，其依据是：在控制系统中所有变量的过去信息的条件下，一个变量的过去信息是否能改善对另一个变量的线性预测。所有的数学和算法步骤都必须从线性随机过程和统计假设检验的基本原理推导出来。\n\n基础理论：\n- 一个多变量离散时间随机过程被建模为向量自回归（VAR）模型，其中，对于滞后阶数 $p$，状态向量 $\\mathbf{x}_t \\in \\mathbb{R}^k$ 根据 $\\mathbf{x}_t = \\sum_{\\ell=1}^{p} A^{(\\ell)} \\mathbf{x}_{t-\\ell} + \\boldsymbol{\\varepsilon}_t$ 演化，其中系数矩阵为 $A^{(\\ell)} \\in \\mathbb{R}^{k \\times k}$，创新噪声 $\\boldsymbol{\\varepsilon}_t$ 为零均值高斯分布，其协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{k \\times k}$。\n- 格兰杰因果关系指出，如果以除 $x_j$ 之外所有变量的过去信息为条件，加入 $x_j$ 的过去信息能够显著降低 $x_i$ 的线性预测误差方差，则变量 $x_j$ 格兰杰导致变量 $x_i$。\n- 普通最小二乘法（OLS）通过最小化残差平方和来求解线性回归，在高斯噪声假设下，其估计量是最大似然估计。\n- 嵌套模型假设检验通过比较一个受限模型（不包含某些预测变量）和一个非受限模型（包含那些预测变量），利用高斯假设下残差平方和的增减来构建检验统计量。残差是观测值与预测值之间的差异；残差平方和是预测误差能量。\n\n您的任务：\n1. 模拟。对于下述每个测试用例，模拟一个 $k=3$ 维的滞后阶数 $p=1$ 的平稳VAR过程：\n   - 递归关系为 $\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\varepsilon}_t$，其中 $A \\in \\mathbb{R}^{3 \\times 3}$ 且 $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$。\n   - 使用 $200$ 个时间步的预烧期（burn-in），然后保留接下来的 $T$ 个样本。初始化 $\\mathbf{x}_0 = \\mathbf{0}$。\n   - 使用提供的 $A$ 和 $\\Sigma$ 以确保稳定性。\n\n2. 推断。对于每个 $j \\neq i$ 的有序对 $(j \\to i)$，在显著性水平 $\\alpha = 0.01$ 下执行格兰杰因果检验：\n   - 构建两个线性模型，用于根据 $t-1$ 时的滞后变量预测 $x_i(t)$：\n     - 非受限模型：包含滞后1的所有 $k$ 个变量作为预测变量。\n     - 受限模型：包含除 $x_j$ 之外的所有滞后1的变量。\n   - 通过普通最小二乘法（OLS）拟合两个模型，并计算每个模型的残差平方和。\n   - 推导一个适用于高斯噪声下嵌套模型比较的检验统计量，并从其抽样分布中计算 $p$ 值。如果检验在水平 $\\alpha$ 下拒绝原假设（即 $x_j$ 对 $x_i$ 没有格兰杰因果关系），并且非受限模型相对于受限模型减小了预测误差，则判定边存在为 $1$；否则判定边不存在为 $0$。\n   - 同时，计算格兰杰因果关系的大小，其值为受限模型与非受限模型估计的残差方差之比的自然对数；此度量仅用于内部验证，不得打印。\n\n3. 输出规范。对于每个测试用例，按固定顺序 $(1 \\to 2),(1 \\to 3),(2 \\to 1),(2 \\to 3),(3 \\to 1),(3 \\to 2)$ 输出一个包含六个整数的列表，其中变量索引为 $1,2,3$。存在的边编码为整数 $1$，不存在的边编码为整数 $0$。\n\n4. 最终输出格式。您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。顶级列表的每个元素对应一个测试用例，并且其本身必须是上述的六元素列表。例如，一个有效的输出形式是 $[[a_1,a_2,a_3,a_4,a_5,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6],[d_1,\\dots,d_6]]$，其中每个 $a_\\ell,b_\\ell,c_\\ell,d_\\ell$ 是整数 $0$ 或 $1$。\n\n此问题不涉及物理量；无需单位说明。\n\n测试套件：\n- 用例 1 (链式结构):\n  - 维度 $k=3$，滞后阶数 $p=1$，样本数 $T=500$。\n  - 系数矩阵\n    $$A = \\begin{bmatrix}\n    0  & 0  & 0 \\\\\n    0.4  & 0  & 0 \\\\\n    0  & 0.4  & 0\n    \\end{bmatrix}.$$\n  - 噪声协方差\n    $$\\Sigma = \\begin{bmatrix}\n    1  & 0  & 0 \\\\\n    0  & 1  & 0 \\\\\n    0  & 0  & 1\n    \\end{bmatrix}.$$\n  - 预期的生成边： $(1 \\to 2)$ 和 $(2 \\to 3)$。\n\n- 用例 2 (双向和扇出结构):\n  - 维度 $k=3$，滞后阶数 $p=1$，样本数 $T=800$。\n  - 系数矩阵\n    $$A = \\begin{bmatrix}\n    0  & 0.5  & 0 \\\\\n    0.5  & 0  & 0 \\\\\n    0.3  & 0  & 0\n    \\end{bmatrix}.$$\n  - 噪声协方差\n    $$\\Sigma = \\begin{bmatrix}\n    0.5  & 0  & 0 \\\\\n    0  & 0.5  & 0 \\\\\n    0  & 0  & 0.5\n    \\end{bmatrix}.$$\n  - 预期的生成边： $(1 \\leftrightarrow 2)$ 和 $(1 \\to 3)$。\n\n- 用例 3 (零模型):\n  - 维度 $k=3$，滞后阶数 $p=1$，样本数 $T=600$。\n  - 系数矩阵 $A = \\mathbf{0}_{3 \\times 3}$。\n  - 噪声协方差\n    $$\\Sigma = \\begin{bmatrix}\n    1.5  & 0  & 0 \\\\\n    0  & 1.5  & 0 \\\\\n    0  & 0  & 1.5\n    \\end{bmatrix}.$$\n  - 预期的生成边：无。\n\n- 用例 4 (混淆的同期噪声相关性):\n  - 维度 $k=3$，滞后阶数 $p=1$，样本数 $T=700$。\n  - 系数矩阵\n    $$A = \\begin{bmatrix}\n    0  & 0  & 0 \\\\\n    0.35  & 0  & 0 \\\\\n    0  & 0.2  & 0\n    \\end{bmatrix}.$$\n  - 噪声协方差\n    $$\\Sigma = \\begin{bmatrix}\n    1  & 0  & 0.6 \\\\\n    0  & 1  & 0 \\\\\n    0.6  & 0  & 1\n    \\end{bmatrix}.$$\n  - 预期的生成边： $(1 \\to 2)$ 和 $(2 \\to 3)$；注意，在对所有滞后变量进行适当条件化时，$x_1$ 和 $x_3$ 之间的同期相关性不应导致格兰杰因果关系。\n\n实现约束：\n- 仅使用 Python 版本 $3.12$、NumPy 版本 $1.23.5$ 和 SciPy 版本 $1.11.4$。\n- 无外部输入或文件；必须使用固定的种子以保证可复现性。\n- 严格遵循最终输出格式；仅打印指定的单行结果。",
            "solution": "该问题是有效的。这是一个来自复杂系统和网络科学领域的、适定的、有科学依据的任务，要求实现一个标准的统计推断算法。所有必要的参数和条件都已提供。\n\n目标是通过应用格兰杰因果关系原理，推断多变量时间序列底层的有向网络结构。数据由一个已知的向量自回归（VAR）模型生成，任务是恢复编码在模型系数矩阵中的连接性。解决方案涉及三个主要阶段：数据模拟、通过嵌套模型比较进行统计推断以及假设检验。\n\n**1. 从向量自回归（VAR）过程模拟数据**\n\n问题指定时间序列数据由一个一阶VAR过程，即VAR($p=1$)，在$k=3$维度上生成。该过程由随机差分方程定义：\n$$ \\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\varepsilon}_t $$\n其中：\n- $\\mathbf{x}_t \\in \\mathbb{R}^k$ 是系统在离散时间步 $t$ 的状态向量。\n- $A \\in \\mathbb{R}^{k \\times k}$ 是系数矩阵。一个非零项 $A_{ij}$ 表示变量 $x_j$ 在时间 $t-1$ 对变量 $x_i$ 在时间 $t$ 的直接因果影响。\n- $\\boldsymbol{\\varepsilon}_t \\in \\mathbb{R}^k$ 是一个随机创新（噪声）向量，从均值为零、协方差矩阵为 $\\Sigma$ 的多元正态分布中抽取，记为 $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$。\n\n模拟过程如下：\n1. 将时间 $t=0$ 时的状态向量初始化为 $\\mathbf{x}_0 = \\mathbf{0}$。\n2. 从 $\\mathcal{N}(\\mathbf{0}, \\Sigma)$ 生成一系列创新向量 $\\boldsymbol{\\varepsilon}_1, \\boldsymbol{\\varepsilon}_2, \\dots, \\boldsymbol{\\varepsilon}_{N_{sim}}$，其中 $N_{sim}$ 是总模拟步数。\n3. 对于 $t=1, \\dots, N_{sim}$，使用VAR方程迭代计算状态向量 $\\mathbf{x}_t$。\n4. 为确保过程达到其平稳分布，使用了一个 200 步的预烧期。前 200 个样本被丢弃。\n5. 随后的 $T$ 个样本构成了用于推断的时间序列数据。最终数据是一个观测矩阵 $X \\in \\mathbb{R}^{T \\times k}$。\n\n**2. 通过嵌套模型比较进行格兰杰因果推断**\n\n格兰杰因果关系提供了一种基于预测的因果关系统计定义。如果 $x_j$ 的过去值包含了有助于预测 $x_i$ 的信息，并且这些信息超出了系统中所有其他变量（包括 $x_i$ 本身）的过去值所包含的信息，那么就称时间序列 $x_j$ 格兰杰导致了另一个时间序列 $x_i$。\n\n对于一个VAR($1$)过程，这等同于检验 $x_i(t)$ 的行方程中系数 $A_{ij}$ 的显著性：\n$$ x_i(t) = \\sum_{m=1}^{k} A_{im} x_m(t-1) + \\varepsilon_i(t) $$\n从 $x_j$到 $x_i$ 不存在格兰杰因果关系的原假设是 $H_0: A_{ij} = 0$。\n\n为了检验这个假设，我们比较两个线性回归模型：\n- **非受限模型：** 该模型包含了对 $x_i(t)$ 的所有可能的滞后预测变量。其方程为：\n$$ x_i(t) = \\sum_{m=1}^{k} \\beta_{im} x_m(t-1) + u_t $$\n预测变量是 $\\{x_1(t-1), x_2(t-1), \\dots, x_k(t-1)\\}$。\n- **受限模型：** 该模型在原假设 $H_0$ 下构建，排除了来自 $x_j(t-1)$ 的影响。其方程为：\n$$ x_i(t) = \\sum_{m=1, m \\neq j}^{k} \\beta_{im} x_m(t-1) + v_t $$\n预测变量是 $\\{x_1(t-1), \\dots, x_{j-1}(t-1), x_{j+1}(t-1), \\dots, x_k(t-1)\\}$。\n\n两个模型都使用普通最小二乘法（OLS）进行拟合，该方法最小化残差平方和（RSS）。设 $RSS_U$ 为非受限模型的RSS，$RSS_R$ 为受限模型的RSS。根据定义，$RSS_U \\le RSS_R$。RSS 的显著下降（即 $RSS_U \\ll RSS_R$）表明被排除的预测变量 $x_j(t-1)$ 具有显著的预测能力。\n\n**3. 用于统计显著性检验的 F 检验**\n\n预测改进的统计显著性通过嵌套模型的 F 检验来量化。F 统计量的公式为每个新预测变量解释的方差与完整模型未解释方差之比：\n$$ F = \\frac{(RSS_R - RSS_U) / q}{RSS_U / (N - k_U)} $$\n该统计量的参数定义如下：\n- $N$: 回归中使用的观测数量。对于长度为 $T$ 且滞后为 $p=1$ 的时间序列，我们有 $N = T-1$ 个数据点用于回归。\n- $k_U$: 非受限模型中的预测变量数量。对于 $k$ 维的VAR($1$)模型，$k_U = k$。\n- $q$: 为获得受限模型而对非受限模型施加的约束数量。在这里，我们检验单个系数，所以 $q=1$。\n\n在原假设 $H_0$ 下，此 F 统计量服从自由度为 $(q, N - k_U)$ 的 F 分布。在这个具体问题中，该分布为 $F(1, T-1-k)$。\n\n对于每个可能的有向边 $(j \\to i)$，我们执行以下步骤：\n1. 从模拟数据中构建响应向量 $\\mathbf{y} = [x_i(2), \\dots, x_i(T)]^T$ 以及非受限模型（$X_U$）和受限模型（$X_R$）的设计矩阵。\n2. 使用 OLS（例如，`numpy.linalg.lstsq`）拟合两个模型，得到 $RSS_U$ 和 $RSS_R$。\n3. 计算 F 统计量 $F_{obs} = ((RSS_R - RSS_U) / 1) / (RSS_U / (T - 1 - k))$。\n4. 计算 p 值，即概率 $P(F_{1, T-1-k} > F_{obs})$。这通过 F 分布的生存函数计算。\n5. 将 p 值与显著性水平 $\\alpha = 0.01$ 进行比较。如果 p 值小于 $\\alpha$，我们拒绝原假设，并断定 $x_j$ 格兰杰导致 $x_i$。从节点 $j$ 到节点 $i$ 画一条边。否则，我们不拒绝 $H_0$，不画边。\n\n非受限模型必须减少预测误差这一条件等价于 $RSS_U  RSS_R$。如果 F 统计量为正，则该条件自然满足，而 F 统计量为正是以任何非平凡显著性水平拒绝原假设的必要条件。\n\n对所有 $k(k-1)$ 个不同的变量有序对 $(j, i)$ 重复此过程，以构建完整的推断邻接矩阵。最终输出是针对所要求的特定边的二进制值列表。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for Granger causality inference.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of the entire process.\n    np.random.seed(42)\n\n    test_cases = [\n        # Case 1 (chain)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 500, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0.4, 0, 0], [0, 0.4, 0]]),\n            \"Sigma\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        },\n        # Case 2 (bidirectional and fan-out)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 800, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0.5, 0], [0.5, 0, 0], [0.3, 0, 0]]),\n            \"Sigma\": np.array([[0.5, 0, 0], [0, 0.5, 0], [0, 0, 0.5]])\n        },\n        # Case 3 (null model)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 600, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            \"Sigma\": np.array([[1.5, 0, 0], [0, 1.5, 0], [0, 0, 1.5]])\n        },\n        # Case 4 (confounding contemporaneous noise correlation)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 700, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0.35, 0, 0], [0, 0.2, 0]]),\n            \"Sigma\": np.array([[1, 0, 0.6], [0, 1, 0], [0.6, 0, 1]])\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = infer_network_granger(params)\n        all_results.append(result)\n\n    # Format the final output as a single string.\n    # e.g., [[0,1,0,0,1,0],[...]]\n    output_str = \"[\" + \",\".join([str(res) for res in all_results]) + \"]\"\n    print(output_str)\n\ndef infer_network_granger(params):\n    \"\"\"\n    Simulates VAR data and performs Granger causality inference for one test case.\n    \"\"\"\n    k = params[\"k\"]\n    p = params[\"p\"]\n    T = params[\"T\"]\n    burn_in = params[\"burn_in\"]\n    A = params[\"A\"]\n    Sigma = params[\"Sigma\"]\n    alpha = params[\"alpha\"]\n    \n    # 1. Simulate the VAR(p) process\n    total_samples = T + burn_in\n    x = np.zeros((total_samples + p, k)) # Store history including initial zeros\n\n    # Generate noise for all time steps at once\n    mean_noise = np.zeros(k)\n    # Correct cholesky decomposition for noise generation\n    L = np.linalg.cholesky(Sigma)\n    innovations = (L @ np.random.randn(k, total_samples)).T\n    \n    for t in range(p, total_samples + p):\n        # The VAR model formulation is x_t = A x_{t-1} + eps\n        # Since p=1 is given for all test cases, this simplification is fine.\n        lagged_x = x[t-1, :]\n        x[t, :] = A @ lagged_x + innovations[t-p, :]\n\n    # Discard burn-in period to get the final time series data\n    data = x[p + burn_in:, :]\n\n    # 2. Inference: Perform Granger causality test for each pair (j -> i)\n    adj_matrix = np.zeros((k, k), dtype=int)\n    \n    # The number of observations for regression is T-p\n    # For a lag of p=1, we predict x(t) from x(t-1) for t=1..T-1\n    # Thus, we have T-1 observations.\n    num_obs = T - p\n    \n    for i in range(k):  # Target variable index\n        for j in range(k):  # Source variable index\n            if i == j:\n                continue\n            \n            # Prepare data for regression\n            # Response variable y is the i-th time series from t=p to T-1\n            y = data[p:, i]\n            \n            # --- Unrestricted Model ---\n            # Predictors are all k variables at lag p\n            # For p=1, this is data from t=0 to T-2\n            X_unrestricted = data[p-1:-1, :]\n            k_unrestricted = X_unrestricted.shape[1]\n            \n            # Fit using OLS\n            _, rss_u_array, _, _ = np.linalg.lstsq(X_unrestricted, y, rcond=None)\n            rss_u = rss_u_array[0] if rss_u_array.size > 0 else 0.0\n\n            # --- Restricted Model ---\n            # Predictors are all variables at lag p except j\n            predictor_indices = list(range(k))\n            predictor_indices.remove(j)\n            X_restricted = data[p-1:-1, predictor_indices]\n\n            # Fit using OLS\n            _, rss_r_array, _, _ = np.linalg.lstsq(X_restricted, y, rcond=None)\n            rss_r = rss_r_array[0] if rss_r_array.size > 0 else 0.0\n            \n            # --- F-Test ---\n            # q = number of restrictions (parameters removed)\n            q = k_unrestricted - X_restricted.shape[1]\n            \n            # Degrees of freedom for F-distribution\n            df1 = q\n            df2 = num_obs - k_unrestricted\n\n            if rss_u > 1e-10: # Avoid division by zero\n                f_stat = ((rss_r - rss_u) / df1) / (rss_u / df2)\n                \n                # p-value from survival function (1 - cdf)\n                p_value = f_dist.sf(f_stat, df1, df2)\n                \n                if p_value  alpha:\n                    adj_matrix[j, i] = 1 # Edge from j to i\n    \n    # 3. Format output according to problem specification\n    # Order: (1->2), (1->3), (2->1), (2->3), (3->1), (3->2)\n    # Corresponds to (0-based): adj[0,1], adj[0,2], adj[1,0], adj[1,2], adj[2,0], adj[2,1]\n    output_list = [\n        adj_matrix[0, 1],\n        adj_matrix[0, 2],\n        adj_matrix[1, 0],\n        adj_matrix[1, 2],\n        adj_matrix[2, 0],\n        adj_matrix[2, 1],\n    ]\n    \n    return output_list\n\nsolve()\n```"
        },
        {
            "introduction": "在实现了格兰杰因果关系的计算流程后，让我们深入其数学核心。这个练习将引导你从第一性原理出发，对一个具体的多变量系统进行精确的理论计算。你将不再依赖于现成的程序库，而是直接运用格兰杰(Granger)因果关系的定义——即一个变量的过去是否能显著减少对另一个变量的预测误差——来推导条件格兰杰因果关系的大小。这项练习旨在加深你对多变量系统中预测误差协方差矩阵及其行列式（即广义方差）在量化因果效应中核心作用的理解。",
            "id": "4278727",
            "problem": "考虑一个针对四维过程 $(x_{1,t}, x_{2,t}, y_{t}, z_{t})^{\\top}$ 的稳定高斯一阶向量自回归 (VAR(1)) 模型，其定义如下\n$$\n\\begin{pmatrix}\nx_{1,t} \\\\\nx_{2,t} \\\\\ny_{t} \\\\\nz_{t}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0   0   b   0 \\\\\n0   0   0   d \\\\\n0   0   \\alpha   0 \\\\\n0   0   0   \\gamma\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1,t-1} \\\\\nx_{2,t-1} \\\\\ny_{t-1} \\\\\nz_{t-1}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n\\varepsilon_{1,t} \\\\\n\\varepsilon_{2,t} \\\\\n\\varepsilon_{y,t} \\\\\n\\varepsilon_{z,t}\n\\end{pmatrix},\n$$\n其中新息向量 $(\\varepsilon_{1,t}, \\varepsilon_{2,t}, \\varepsilon_{y,t}, \\varepsilon_{z,t})^{\\top}$ 是零均值高斯向量，跨时间独立，并具有对角协方差矩阵\n$$\n\\Sigma_{\\varepsilon} = \\mathrm{diag}\\!\\left(s_{1}^{2},\\, s_{2}^{2},\\, s_{y}^{2},\\, s_{z}^{2}\\right).\n$$\n假设 $b = 2$，$d = \\frac{1}{2}$，$\\alpha = 0$，$\\gamma = \\frac{1}{3}$，$s_{1}^{2} = 1$，$s_{2}^{2} = 2$，$s_{y}^{2} = 3$ 以及 $s_{z}^{2} = 4$。在这些参数值下，该过程是稳定且平稳的。\n\n仅使用 Granger 因果关系的基本定义（即，如果在仅使用过程 x 和 z 的过去值所能达到的预测效果之外，加入过程 y 的过去值能够进一步减小 x 的均方预测误差，则在 z 为条件下，过程 y 是过程 x 的 Granger 原因），从第一性原理出发，推导并计算从单变量 y 到给定 z 条件下的双变量 x = (x_{1}, x_{2})^{\\top} 的条件多元 Granger 因果关系（时域）。将最终值表示为一个实数，并四舍五入至四位有效数字。",
            "solution": "该问题要求计算从 $y$ 到 $x = (x_1, x_2)^\\top$ 在给定 $z$ 条件下的多元格兰杰因果关系。其定义为：\n$$\nF_{y \\to x|z} = \\ln \\frac{\\det(\\Sigma_{x|x,z})}{\\det(\\Sigma_{x|x,y,z})}\n$$\n其中 $\\Sigma_{x|x,y,z}$ 是使用所有变量 $(x, y, z)$ 的过去信息预测 $x_t$ 时的一步预测误差协方差矩阵，而 $\\Sigma_{x|x,z}$ 是仅使用 $(x, z)$ 的过去信息进行预测时的误差协方差矩阵。\n\n**1. 计算非受限模型的预测误差协方差 $\\Sigma_{x|x,y,z}$**\n\nVAR(1) 模型的方程为：\n$x_{1,t} = b y_{t-1} + \\varepsilon_{1,t}$\n$x_{2,t} = d z_{t-1} + \\varepsilon_{2,t}$\n当所有变量的过去信息都可用时，对 $x_t$ 的最优线性预测是：\n$\\hat{x}_{1,t} = E[x_{1,t} | \\mathcal{F}_{t-1}] = b y_{t-1}$\n$\\hat{x}_{2,t} = E[x_{2,t} | \\mathcal{F}_{t-1}] = d z_{t-1}$\n预测误差向量为 $e_t^{unres} = (x_{1,t} - \\hat{x}_{1,t}, x_{2,t} - \\hat{x}_{2,t})^\\top = (\\varepsilon_{1,t}, \\varepsilon_{2,t})^\\top$。\n由于噪声项不相关，误差协方差矩阵为：\n$$\n\\Sigma_{x|x,y,z} = \\begin{pmatrix} s_{1}^{2}  0 \\\\ 0  s_{2}^{2} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}\n$$\n其行列式为 $\\det(\\Sigma_{x|x,y,z}) = 1 \\times 2 = 2$。\n\n**2. 计算受限模型的预测误差协方差 $\\Sigma_{x|x,z}$**\n\n在受限模型中，我们只能使用 $x$ 和 $z$ 的过去信息来预测 $x_t$。\n对 $x_{2,t}$ 的预测不变，因为其驱动项 $z_{t-1}$ 是已知的：\n$\\tilde{x}_{2,t} = E[x_{2,t} | \\{x_\\tau, z_\\tau\\}_{\\tau  t}] = d z_{t-1}$\n预测误差为 $e_{2,t}^{res} = x_{2,t} - \\tilde{x}_{2,t} = \\varepsilon_{2,t}$。\n\n对 $x_{1,t}$ 的预测则不同，因为其驱动项 $y_{t-1}$ 是未知的。我们需要用其在由 $x$ 和 $z$ 的历史构成的空间上的投影来代替它，即 $E[y_{t-1} | \\{x_\\tau, z_\\tau\\}_{\\tau  t}]$。\n在给定的模型中，$y_t = \\alpha y_{t-1} + \\varepsilon_{y,t}$，且 $\\alpha=0$，所以 $y_t = \\varepsilon_{y,t}$。这意味着 $\\{y_t\\}$ 是一个白噪声过程。\n此外，由于所有新息项 $(\\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{y}, \\varepsilon_{z})$ 相互独立，并且 $z_t$ 的演化不依赖于 $y$，所以 $y_t$ 与 $z_t$ 的整个历史是独立的。\n同样，$x_{1,t} = b y_{t-1} + \\varepsilon_{1,t}$ 和 $x_{2,t} = d z_{t-1} + \\varepsilon_{2,t}$ 的定义表明，$y_{t-1}$ 也与 $x_t$ 和 $z_t$ 的历史无关。\n因此，$y_{t-1}$ 与 $x$ 和 $z$ 的整个历史都无关。\n所以，$E[y_{t-1} | \\{x_\\tau, z_\\tau\\}_{\\tau  t}] = E[y_{t-1}] = 0$。\n那么，对 $x_{1,t}$ 的受限预测为：\n$\\tilde{x}_{1,t} = E[x_{1,t} | \\{x_\\tau, z_\\tau\\}_{\\tau  t}] = E[b y_{t-1} + \\varepsilon_{1,t} | \\{x_\\tau, z_\\tau\\}_{\\tau  t}] = b \\cdot 0 + 0 = 0$\n预测误差为 $e_{1,t}^{res} = x_{1,t} - \\tilde{x}_{1,t} = b y_{t-1} + \\varepsilon_{1,t}$。\n\n受限预测误差向量为 $e_t^{res} = (b y_{t-1} + \\varepsilon_{1,t}, \\varepsilon_{2,t})^\\top$。其协方差矩阵 $\\Sigma_{x|x,z}$ 的元素为：\n- $\\text{Var}(e_{1,t}^{res}) = \\text{Var}(b y_{t-1} + \\varepsilon_{1,t}) = b^2 \\text{Var}(y_{t-1}) + \\text{Var}(\\varepsilon_{1,t}) = b^2 s_y^2 + s_1^2 = (2)^2(3) + 1 = 13$。\n- $\\text{Var}(e_{2,t}^{res}) = \\text{Var}(\\varepsilon_{2,t}) = s_2^2 = 2$。\n- $\\text{Cov}(e_{1,t}^{res}, e_{2,t}^{res}) = \\text{Cov}(b y_{t-1} + \\varepsilon_{1,t}, \\varepsilon_{2,t}) = 0$ (由于独立性)。\n所以，\n$$\n\\Sigma_{x|x,z} = \\begin{pmatrix} 13  0 \\\\ 0  2 \\end{pmatrix}\n$$\n其行列式为 $\\det(\\Sigma_{x|x,z}) = 13 \\times 2 = 26$。\n\n**3. 计算格兰杰因果关系**\n\n$$\nF_{y \\to x|z} = \\ln \\frac{\\det(\\Sigma_{x|x,z})}{\\det(\\Sigma_{x|x,y,z})} = \\ln\\left(\\frac{26}{2}\\right) = \\ln(13)\n$$\n将 $\\ln(13)$ 四舍五入到四位有效数字，得到 $2.565$。",
            "answer": "$$\n\\boxed{2.565}\n$$"
        },
        {
            "introduction": "格兰杰因果关系是一个强大的工具，但它的应用并非没有前提。一个至关重要的假设是系统中没有未被观测到的混杂变量。这个练习通过一个精心设计的思想实验，揭示了当存在一个驱动两个观测变量的“潜伏”过程时，会发生什么。你将通过精确的数学推导，计算出即使在两个观测变量之间没有直接因果联系的情况下，由于共同的潜在驱动因素和测量噪声，格兰杰因果检验仍会错误地检测到“伪”因果关系。这项实践对于培养批判性思维至关重要，它提醒我们在解释因果推断结果时必须保持谨慎。",
            "id": "4278730",
            "problem": "考虑一对由未被观测到的潜混杂过程 $\\{L_{t}\\}$ 生成的观测时间序列 $\\{X_{t}^{o}\\}$ 和 $\\{Y_{t}\\}$。潜驱动过程 $\\{L_{t}\\}$ 服从一个稳定的一阶自回归模型，定义为 $L_{t} = \\alpha L_{t-1} + \\epsilon_{t}^{L}$，其中 $\\epsilon_{t}^{L}$ 是方差为 $\\sigma_{L}^{2}$ 的零均值、独立同分布的高斯噪声，且为了满足平稳性，有 $|\\alpha|  1$。观测变量由滞后的潜过程线性驱动，并带有独立的高斯新息：$X_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X}$ 和 $Y_{t} = \\gamma L_{t-1} + \\epsilon_{t}^{Y}$，其中 $\\epsilon_{t}^{X}$ 和 $\\epsilon_{t}^{Y}$ 分别是方差为 $\\sigma_{X}^{2}$ 和 $\\sigma_{Y}^{2}$ 的零均值、独立同分布的高斯噪声。对 $X_{t}$ 的测量受到方差为 $\\sigma_{\\nu}^{2}$ 的独立高斯噪声 $\\nu_{t}$ 的干扰，得到 $X_{t}^{o} = X_{t} + \\nu_{t}$。所有新息项和测量噪声项在时间和过程上都相互独立，并且也与 $\\{L_{t}\\}$ 独立。\n\n一位研究人员，在不知道存在潜混杂和测量噪声的情况下，对观测对 $(Y_{t}, X_{t}^{o})$ 拟合了一个二元一阶向量自回归 (VAR) 模型，并评估了从过去的滞后项 $(Y_{t-1}, X_{t-1}^{o})$ 对 $Y_{t}$ 的一步向前线性预测。在大样本和高斯假设下，普通最小二乘 (OLS) 预测器等于 $Y_{t}$ 在回归量上的线性最小二乘投影。设 $Y_{t}$ 的受限预测器仅使用 $Y_{t-1}$，而非受限预测器同时使用 $Y_{t-1}$ 和 $X_{t-1}^{o}$。将从 $X$ 到 $Y$ 的Granger因果关系 (GC) 定义为 $Y_{t}$ 的受限一步向前预测误差方差与非受限一步向前预测误差方差之比的自然对数。\n\n取具体参数值为 $\\alpha = \\tfrac{1}{2}$，$\\sigma_{L}^{2} = 1$，$\\beta = 1$，$\\gamma = 1$，$\\sigma_{X}^{2} = 1$，$\\sigma_{Y}^{2} = 1$ 以及 $\\sigma_{\\nu}^{2} = 1$。在这些条件下，计算由对 $(Y_{t}, X_{t}^{o})$ 拟合的二元VAR($1$)模型所蕴含的从 $X$到 $Y$ 的精确渐近Granger因果关系，并以单一的闭式解析表达式表示。该值为无量纲。无需四舍五入；请提供精确表达式。",
            "solution": "问题陈述被评估为有效。它在科学上基于时间序列分析和Granger因果关系理论，提法恰当，提供了所有必要的参数和定义，且语言客观。它提出了其领域内一个标准的、尽管不平凡的计算。\n\n目标是计算从观测过程 $X^o$ 到过程 $Y$ 的渐近Granger因果关系 (GC)，定义为：\n$$\nGC_{X \\to Y} = \\ln\\left( \\frac{\\sigma_{R}^{2}}{\\sigma_{U}^{2}} \\right)\n$$\n其中 $\\sigma_{R}^{2}$ 是仅使用 $Y_t$ 自身历史信息对其进行一步向前预测的误差方差，而 $\\sigma_{U}^{2}$ 是同时使用 $Y_t$ 和 $X_t^o$ 的历史信息进行预测的误差方差。对于高斯过程，渐近地，这些误差对应于线性最小二乘投影。\n\n模型由以下方程定义：\n1.  潜过程：$L_{t} = \\alpha L_{t-1} + \\epsilon_{t}^{L}$，其中 $\\epsilon_{t}^{L} \\sim N(0, \\sigma_{L}^{2})$。\n2.  观测过程（理想）：$Y_{t} = \\gamma L_{t-1} + \\epsilon_{t}^{Y}$ 和 $X_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X}$，其中 $\\epsilon_{t}^{X} \\sim N(0, \\sigma_{X}^{2})$ 且 $\\epsilon_{t}^{Y} \\sim N(0, \\sigma_{Y}^{2})$。\n3.  带噪声的观测：$X_{t}^{o} = X_{t} + \\nu_{t} = \\beta L_{t-1} + \\epsilon_{t}^{X} + \\nu_{t}$，其中 $\\nu_{t} \\sim N(0, \\sigma_{\\nu}^{2})$。\n\n假设所有过程都是平稳的且均值为零。我们首先计算必要的二阶矩（方差和协方差）。\n\n首先，平稳潜过程 $L_t$ 的方差是：\n$$\n\\text{Var}(L_t) = \\Sigma_{LL}(0) = \\frac{\\sigma_{L}^{2}}{1 - \\alpha^2}\n$$\n$L_t$ 在滞后 $k$ 处的自协方差为 $\\Sigma_{LL}(k) = \\text{Cov}(L_t, L_{t-k}) = \\alpha^{k} \\Sigma_{LL}(0)$。\n\n给定的参数值为：$\\alpha = \\frac{1}{2}$，$\\sigma_{L}^{2} = 1$，$\\beta = 1$，$\\gamma = 1$，$\\sigma_{X}^{2} = 1$，$\\sigma_{Y}^{2} = 1$ 和 $\\sigma_{\\nu}^{2} = 1$。\n代入这些值，我们求得 $L_t$ 的方差：\n$$\n\\Sigma_{LL}(0) = \\frac{1}{1 - (\\frac{1}{2})^2} = \\frac{1}{1 - \\frac{1}{4}} = \\frac{1}{\\frac{3}{4}} = \\frac{4}{3}\n$$\n\n接下来，我们计算观测过程 $Y_t$ 和 $X_t^o$ 的方差和协方差。由于平稳性，$\\text{Var}(Y_t) = \\text{Var}(Y_{t-1})$ 且 $\\text{Var}(X_t^o) = \\text{Var}(X_{t-1}^o)$。\n$$\n\\text{Var}(Y_t) = \\text{Var}(\\gamma L_{t-1} + \\epsilon_{t}^{Y}) = \\gamma^2 \\text{Var}(L_{t-1}) + \\text{Var}(\\epsilon_{t}^{Y}) = \\gamma^2 \\Sigma_{LL}(0) + \\sigma_{Y}^{2}\n$$\n$$\n\\text{Var}(Y_t) = (1)^2 \\left(\\frac{4}{3}\\right) + 1 = \\frac{7}{3}\n$$\n$$\n\\text{Var}(X_t^o) = \\text{Var}(\\beta L_{t-1} + \\epsilon_{t}^{X} + \\nu_{t}) = \\beta^2 \\text{Var}(L_{t-1}) + \\text{Var}(\\epsilon_{t}^{X}) + \\text{Var}(\\nu_{t}) = \\beta^2 \\Sigma_{LL}(0) + \\sigma_{X}^{2} + \\sigma_{\\nu}^{2}\n$$\n$$\n\\text{Var}(X_t^o) = (1)^2 \\left(\\frac{4}{3}\\right) + 1 + 1 = \\frac{4}{3} + 2 = \\frac{10}{3}\n$$\n$Y_{t-1}$ 和 $X_{t-1}^o$ 之间的互协方差为：\n$$\n\\text{Cov}(Y_{t-1}, X_{t-1}^o) = \\text{E}[(\\gamma L_{t-2} + \\epsilon_{t-1}^{Y})(\\beta L_{t-2} + \\epsilon_{t-1}^{X} + \\nu_{t-1})]\n$$\n由于所有噪声项相互独立且与 $L_t$ 独立，上式简化为：\n$$\n\\text{Cov}(Y_{t-1}, X_{t-1}^o) = \\gamma \\beta \\text{E}[L_{t-2}^2] = \\gamma \\beta \\Sigma_{LL}(0) = (1)(1)\\left(\\frac{4}{3}\\right) = \\frac{4}{3}\n$$\n\n现在我们计算 $Y_t$ 与其预测变量 $Y_{t-1}$ 和 $X_{t-1}^o$ 之间的协方差。\n$$\n\\text{Cov}(Y_t, Y_{t-1}) = \\text{E}[(\\gamma L_{t-1} + \\epsilon_{t}^{Y})(\\gamma L_{t-2} + \\epsilon_{t-1}^{Y})] = \\gamma^2 \\text{E}[L_{t-1}L_{t-2}]\n$$\n由于 $L_{t-1} = \\alpha L_{t-2} + \\epsilon_{t-1}^L$，我们有 $\\text{E}[L_{t-1}L_{t-2}] = \\alpha \\text{E}[L_{t-2}^2] = \\alpha \\Sigma_{LL}(0)$。\n$$\n\\text{Cov}(Y_t, Y_{t-1}) = \\gamma^2 \\alpha \\Sigma_{LL}(0) = (1)^2 \\left(\\frac{1}{2}\\right) \\left(\\frac{4}{3}\\right) = \\frac{2}{3}\n$$\n$$\n\\text{Cov}(Y_t, X_{t-1}^o) = \\text{E}[(\\gamma L_{t-1} + \\epsilon_{t}^{Y})(\\beta L_{t-2} + \\epsilon_{t-1}^{X} + \\nu_{t-1})] = \\gamma \\beta \\text{E}[L_{t-1}L_{t-2}] = \\gamma \\beta \\alpha \\Sigma_{LL}(0)\n$$\n$$\n\\text{Cov}(Y_t, X_{t-1}^o) = (1)(1)\\left(\\frac{1}{2}\\right)\\left(\\frac{4}{3}\\right) = \\frac{2}{3}\n$$\n\n受限预测误差方差 $\\sigma_R^2$ 是用于从 $Y_{t-1}$ 预测 $Y_t$ 的：\n$$\n\\sigma_R^2 = \\text{Var}(Y_t) - \\frac{\\text{Cov}(Y_t, Y_{t-1})^2}{\\text{Var}(Y_{t-1})}\n$$\n$$\n\\sigma_R^2 = \\frac{7}{3} - \\frac{(\\frac{2}{3})^2}{\\frac{7}{3}} = \\frac{7}{3} - \\frac{\\frac{4}{9}}{\\frac{7}{3}} = \\frac{7}{3} - \\frac{4}{9} \\cdot \\frac{3}{7} = \\frac{7}{3} - \\frac{4}{21} = \\frac{49 - 4}{21} = \\frac{45}{21} = \\frac{15}{7}\n$$\n\n非受限预测误差方差 $\\sigma_U^2$ 是用于从回归量向量 $Z_{t-1} = \\begin{pmatrix} Y_{t-1} \\\\ X_{t-1}^o \\end{pmatrix}$ 预测 $Y_t$ 的。该方差由 $\\sigma_U^2 = \\text{Var}(Y_t) - c^T \\Sigma_Z^{-1} c$ 给出，其中 $\\Sigma_Z$ 是 $Z_{t-1}$ 的协方差矩阵，$c$ 是 $Y_t$ 和 $Z_{t-1}$ 之间的协方差向量。\n$$\n\\Sigma_Z = \\begin{pmatrix} \\text{Var}(Y_{t-1})  \\text{Cov}(Y_{t-1}, X_{t-1}^o) \\\\ \\text{Cov}(Y_{t-1}, X_{t-1}^o)  \\text{Var}(X_{t-1}^o) \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3}  \\frac{4}{3} \\\\ \\frac{4}{3}  \\frac{10}{3} \\end{pmatrix}\n$$\n$$\nc = \\begin{pmatrix} \\text{Cov}(Y_t, Y_{t-1}) \\\\ \\text{Cov}(Y_t, X_{t-1}^o) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\n$\\Sigma_Z$ 的行列式为：\n$$\n\\det(\\Sigma_Z) = \\left(\\frac{7}{3}\\right)\\left(\\frac{10}{3}\\right) - \\left(\\frac{4}{3}\\right)^2 = \\frac{70}{9} - \\frac{16}{9} = \\frac{54}{9} = 6\n$$\n$\\Sigma_Z$ 的逆矩阵为：\n$$\n\\Sigma_Z^{-1} = \\frac{1}{6} \\begin{pmatrix} \\frac{10}{3}  -\\frac{4}{3} \\\\ -\\frac{4}{3}  \\frac{7}{3} \\end{pmatrix}\n$$\n现在我们计算 $c^T \\Sigma_Z^{-1} c$ 项：\n$$\nc^T \\Sigma_Z^{-1} c = \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\frac{1}{6} \\begin{pmatrix} \\frac{10}{3}  -\\frac{4}{3} \\\\ -\\frac{4}{3}  \\frac{7}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\n$$\nc^T \\Sigma_Z^{-1} c = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{10}{3} \\cdot \\frac{2}{3} - \\frac{4}{3} \\cdot \\frac{2}{3} \\\\ -\\frac{4}{3} \\cdot \\frac{2}{3} + \\frac{7}{3} \\cdot \\frac{2}{3} \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{20-8}{9} \\\\ \\frac{-8+14}{9} \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} \\frac{2}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{12}{9} \\\\ \\frac{6}{9} \\end{pmatrix}\n$$\n$$\nc^T \\Sigma_Z^{-1} c = \\frac{1}{6} \\left( \\frac{2}{3} \\cdot \\frac{12}{9} + \\frac{2}{3} \\cdot \\frac{6}{9} \\right) = \\frac{1}{6} \\left( \\frac{24}{27} + \\frac{12}{27} \\right) = \\frac{1}{6} \\left( \\frac{36}{27} \\right) = \\frac{1}{6} \\left( \\frac{4}{3} \\right) = \\frac{4}{18} = \\frac{2}{9}\n$$\n非受限误差方差为：\n$$\n\\sigma_U^2 = \\text{Var}(Y_t) - c^T \\Sigma_Z^{-1} c = \\frac{7}{3} - \\frac{2}{9} = \\frac{21 - 2}{9} = \\frac{19}{9}\n$$\n最后，我们计算Granger因果关系度量：\n$$\nGC_{X \\to Y} = \\ln\\left( \\frac{\\sigma_{R}^{2}}{\\sigma_{U}^{2}} \\right) = \\ln\\left( \\frac{\\frac{15}{7}}{\\frac{19}{9}} \\right) = \\ln\\left( \\frac{15}{7} \\cdot \\frac{9}{19} \\right) = \\ln\\left( \\frac{135}{133} \\right)\n$$\n这个正值表明，尽管在数据生成过程中没有直接的因果联系，但由于潜混杂和测量噪声的共同作用，从 $X^o$ 到 $Y$ 检测到了伪因果关系。",
            "answer": "$$\\boxed{\\ln\\left(\\frac{135}{133}\\right)}$$"
        }
    ]
}