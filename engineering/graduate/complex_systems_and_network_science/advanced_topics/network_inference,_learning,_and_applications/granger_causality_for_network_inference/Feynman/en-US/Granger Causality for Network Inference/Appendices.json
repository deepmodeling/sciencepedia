{
    "hands_on_practices": [
        {
            "introduction": "The most effective way to grasp the mechanics of Granger causality is to build an inference engine from the ground up. This first exercise  guides you through the complete process of implementing a Granger causality test in code. You will start by generating synthetic data from a Vector Autoregression (VAR) model with a known network structure, and then apply the principle of nested model comparison using Ordinary Least Squares (OLS) to see if you can recover the original connections. This foundational practice solidifies the link between statistical theory and practical application, providing direct experience with the core algorithm.",
            "id": "4278726",
            "problem": "You are asked to implement a complete, runnable program that infers a directed network from multivariate time series using the definition of Granger causality. The program must generate synthetic data from a Vector Autoregression (VAR) process and apply hypothesis testing to detect directed edges among variables based on whether the past of one variable improves the linear prediction of another, controlling for the past of all variables in the system. All mathematical and algorithmic steps must be derived from first principles of linear stochastic processes and statistical hypothesis testing.\n\nFundamental base:\n- A multivariate discrete-time stochastic process is modeled as a Vector Autoregression (VAR), where, for lag order $p$, the state vector $\\mathbf{x}_t \\in \\mathbb{R}^k$ evolves according to $\\mathbf{x}_t = \\sum_{\\ell=1}^{p} A^{(\\ell)} \\mathbf{x}_{t-\\ell} + \\boldsymbol{\\varepsilon}_t$, with coefficient matrices $A^{(\\ell)} \\in \\mathbb{R}^{k \\times k}$ and innovation noise $\\boldsymbol{\\varepsilon}_t$ that is zero-mean Gaussian with covariance matrix $\\Sigma \\in \\mathbb{R}^{k \\times k}$.\n- Granger causality states that variable $x_j$ Granger-causes variable $x_i$ if, conditional on the past of all variables except $x_j$, adding the past of $x_j$ yields a statistically significant reduction in the linear prediction error variance of $x_i$.\n- Ordinary Least Squares (OLS) solves linear regression by minimizing the sum of squared residuals, yielding estimates that are maximum likelihood under Gaussian noise.\n- Nested model hypothesis testing compares a restricted model (without certain predictors) against an unrestricted model (with those predictors), using the increase or decrease of the sum of squared residuals under Gaussian assumptions to form a test statistic. The residuals are the differences between observed and predicted values; the sum of squared residuals is the prediction error energy.\n\nYour tasks:\n1. Simulation. For each test case below, simulate a stationary VAR process of lag order $p=1$ in $k=3$ dimensions:\n   - The recursion is $\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\varepsilon}_t$, with $A \\in \\mathbb{R}^{3 \\times 3}$ and $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$.\n   - Use burn-in of $200$ time steps, then keep the next $T$ samples. Initialize $\\mathbf{x}_0 = \\mathbf{0}$.\n   - Ensure stability by using the provided $A$ and $\\Sigma$.\n\n2. Inference. For each ordered pair $(j \\to i)$ with $j \\neq i$, perform a Granger causality test at significance level $\\alpha = 0.01$:\n   - Form two linear models predicting $x_i(t)$ from lagged variables at $t-1$:\n     - Unrestricted model: include all $k$ variables at lag $1$ as predictors.\n     - Restricted model: include all variables at lag $1$ except $x_j$.\n   - Fit both models by Ordinary Least Squares (OLS) and compute the sum of squared residuals of each model.\n   - Derive a suitable test statistic for the nested model comparison under Gaussian noise and compute a $p$-value from its sampling distribution. Decide edge presence as $1$ if the test rejects the null hypothesis (no Granger causality from $x_j$ to $x_i$) at level $\\alpha$ and the unrestricted model reduces prediction error relative to the restricted model; otherwise decide edge absence as $0$.\n   - Also compute the magnitude of Granger causality as the natural logarithm of the ratio of estimated residual variances between the restricted and unrestricted models; this measure is for internal validation only and must not be printed.\n\n3. Output specification. For each test case, output a list of six integers corresponding to directed edges in the fixed order $(1 \\to 2),(1 \\to 3),(2 \\to 1),(2 \\to 3),(3 \\to 1),(3 \\to 2)$, where variables are indexed by $1,2,3$. A present edge is encoded as the integer $1$ and an absent edge as the integer $0$.\n\n4. Final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the top-level list corresponds to one test case and must itself be the six-element list described above. For example, a valid output form is $[[a_1,a_2,a_3,a_4,a_5,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6],[d_1,\\dots,d_6]]$ where each $a_\\ell,b_\\ell,c_\\ell,d_\\ell$ is an integer $0$ or $1$.\n\nThere are no physical quantities in this problem; no unit specification is required.\n\nTest suite:\n- Case $1$ (chain):\n  - Dimension $k=3$, lag order $p=1$, samples $T=500$.\n  - Coefficient matrix\n    $$A = \\begin{bmatrix}\n    0  0  0 \\\\\n    0.4  0  0 \\\\\n    0  0.4  0\n    \\end{bmatrix}.$$\n  - Noise covariance\n    $$\\Sigma = \\begin{bmatrix}\n    1  0  0 \\\\\n    0  1  0 \\\\\n    0  0  1\n    \\end{bmatrix}.$$\n  - Expected generative edges: $(1 \\to 2)$ and $(2 \\to 3)$.\n\n- Case $2$ (bidirectional and fan-out):\n  - Dimension $k=3$, lag order $p=1$, samples $T=800$.\n  - Coefficient matrix\n    $$A = \\begin{bmatrix}\n    0  0.5  0 \\\\\n    0.5  0  0 \\\\\n    0.3  0  0\n    \\end{bmatrix}.$$\n  - Noise covariance\n    $$\\Sigma = \\begin{bmatrix}\n    0.5  0  0 \\\\\n    0  0.5  0 \\\\\n    0  0  0.5\n    \\end{bmatrix}.$$\n  - Expected generative edges: $(1 \\leftrightarrow 2)$ and $(1 \\to 3)$.\n\n- Case $3$ (null model):\n  - Dimension $k=3$, lag order $p=1$, samples $T=600$.\n  - Coefficient matrix $A = \\mathbf{0}_{3 \\times 3}$.\n  - Noise covariance\n    $$\\Sigma = \\begin{bmatrix}\n    1.5  0  0 \\\\\n    0  1.5  0 \\\\\n    0  0  1.5\n    \\end{bmatrix}.$$\n  - Expected generative edges: none.\n\n- Case $4$ (confounding contemporaneous noise correlation):\n  - Dimension $k=3$, lag order $p=1$, samples $T=700$.\n  - Coefficient matrix\n    $$A = \\begin{bmatrix}\n    0  0  0 \\\\\n    0.35  0  0 \\\\\n    0  0.2  0\n    \\end{bmatrix}.$$\n  - Noise covariance\n    $$\\Sigma = \\begin{bmatrix}\n    1  0  0.6 \\\\\n    0  1  0 \\\\\n    0.6  0  1\n    \\end{bmatrix}.$$\n  - Expected generative edges: $(1 \\to 2)$ and $(2 \\to 3)$; note the contemporaneous correlation between $x_1$ and $x_3$ should not induce Granger causality when properly conditioning on all lagged variables.\n\nImplementation constraints:\n- Use only Python version $3.12$, NumPy version $1.23.5$, and SciPy version $1.11.4$.\n- No external input or files; fixed seeds must be used for reproducibility.\n- Follow the final output format exactly; print only the specified single-line result.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded task from the field of complex systems and network science, requiring the implementation of a standard statistical inference algorithm. All necessary parameters and conditions are provided.\n\nThe objective is to infer the directed network structure underlying a multivariate time series by applying the principle of Granger causality. The data is generated from a known Vector Autoregressive (VAR) model, and the task is to recover the connectivity encoded in the model's coefficient matrix. The solution involves three primary stages: data simulation, statistical inference via nested model comparison, and hypothesis testing.\n\n**1. Data Simulation from a Vector Autoregressive (VAR) Process**\n\nThe problem specifies that the time series data is generated by a first-order VAR process, or VAR($p=1$), in $k=3$ dimensions. The process is defined by the stochastic difference equation:\n$$ \\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\varepsilon}_t $$\nwhere:\n- $\\mathbf{x}_t \\in \\mathbb{R}^k$ is the state vector of the system at a discrete time step $t$.\n- $A \\in \\mathbb{R}^{k \\times k}$ is the coefficient matrix. A non-zero entry $A_{ij}$ indicates a direct causal influence from variable $x_j$ at time $t-1$ to variable $x_i$ at time $t$.\n- $\\boldsymbol{\\varepsilon}_t \\in \\mathbb{R}^k$ is a vector of random innovations (noise), drawn from a multivariate normal distribution with mean zero and covariance matrix $\\Sigma$, denoted as $\\boldsymbol{\\varepsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$.\n\nThe simulation procedure is as follows:\n1. Initialize the state vector at time $t=0$ to $\\mathbf{x}_0 = \\mathbf{0}$.\n2. Generate a sequence of innovation vectors $\\boldsymbol{\\varepsilon}_1, \\boldsymbol{\\varepsilon}_2, \\dots, \\boldsymbol{\\varepsilon}_{N_{sim}}$ from $\\mathcal{N}(\\mathbf{0}, \\Sigma)$, where $N_{sim}$ is the total number of simulation steps.\n3. Iteratively compute the state vector $\\mathbf{x}_t$ for $t=1, \\dots, N_{sim}$ using the VAR equation.\n4. To ensure the process has reached its stationary distribution, a burn-in period of $200$ steps is used. The first $200$ samples are discarded.\n5. The subsequent $T$ samples constitute the time series data used for inference. The final data is an observation matrix $X \\in \\mathbb{R}^{T \\times k}$.\n\n**2. Granger Causality Inference via Nested Model Comparison**\n\nGranger causality provides a statistical definition of causality based on prediction. A time series $x_j$ is said to Granger-cause another time series $x_i$ if past values of $x_j$ contain information that helps predict $x_i$ above and beyond the information contained in the past values of all other variables in the system, including $x_i$ itself.\n\nFor a VAR($1$) process, this translates to testing the significance of the coefficient $A_{ij}$ in the row equation for $x_i(t)$:\n$$ x_i(t) = \\sum_{m=1}^{k} A_{im} x_m(t-1) + \\varepsilon_i(t) $$\nThe null hypothesis for the absence of Granger causality from $x_j$ to $x_i$ is $H_0: A_{ij} = 0$.\n\nTo test this hypothesis, we compare two linear regression models:\n- **Unrestricted Model:** This model incorporates all possible lagged predictors for $x_i(t)$. Its equation is:\n$$ x_i(t) = \\sum_{m=1}^{k} \\beta_{im} x_m(t-1) + u_t $$\nThe predictors are $\\{x_1(t-1), x_2(t-1), \\dots, x_k(t-1)\\}$.\n- **Restricted Model:** This model is formulated under the null hypothesis $H_0$, where the influence from $x_j(t-1)$ is excluded. Its equation is:\n$$ x_i(t) = \\sum_{m=1, m \\neq j}^{k} \\beta_{im} x_m(t-1) + v_t $$\nThe predictors are $\\{x_1(t-1), \\dots, x_{j-1}(t-1), x_{j+1}(t-1), \\dots, x_k(t-1)\\}$.\n\nBoth models are fitted to the data using Ordinary Least Squares (OLS), which minimizes the sum of squared residuals (RSS). Let $RSS_U$ be the RSS for the unrestricted model and $RSS_R$ be the RSS for the restricted model. By construction, $RSS_U \\le RSS_R$. A significant drop in RSS (i.e., $RSS_U \\ll RSS_R$) suggests that the excluded predictor, $x_j(t-1)$, has significant predictive power.\n\n**3. The F-Test for Statistical Significance**\n\nThe statistical significance of the improvement in prediction is quantified using an F-test for nested models. The F-statistic is formulated as the ratio of the explained variance per new predictor to the unexplained variance of the full model:\n$$ F = \\frac{(RSS_R - RSS_U) / q}{RSS_U / (N - k_U)} $$\nThe parameters of this statistic are defined as:\n- $N$: The number of observations used in the regression. For a time series of length $T$ and a lag of $p=1$, we have $N = T-1$ data points for regression.\n- $k_U$: The number of predictors in the unrestricted model. For a VAR($1$) in $k$ dimensions, $k_U = k$.\n- $q$: The number of restrictions imposed on the unrestricted model to obtain the restricted model. Here, we test a single coefficient, so $q=1$.\n\nUnder the null hypothesis $H_0$, this F-statistic follows an F-distribution with $(q, N - k_U)$ degrees of freedom. In this specific problem, the distribution is $F(1, T-1-k)$.\n\nFor each potential directed edge $(j \\to i)$, we perform the following steps:\n1. Construct the response vector $\\mathbf{y} = [x_i(2), \\dots, x_i(T)]^T$ and the design matrices for the unrestricted ($X_U$) and restricted ($X_R$) models from the simulated data.\n2. Use OLS (e.g., `numpy.linalg.lstsq`) to fit both models and obtain $RSS_U$ and $RSS_R$.\n3. Compute the F-statistic $F_{obs} = ((RSS_R - RSS_U) / 1) / (RSS_U / (T - 1 - k))$.\n4. Calculate the p-value, which is the probability $P(F_{1, T-1-k} > F_{obs})$. This is computed using the survival function of the F-distribution.\n5. Compare the p-value to the significance level $\\alpha = 0.01$. If the p-value is less than $\\alpha$, we reject the null hypothesis and conclude that $x_j$ Granger-causes $x_i$. An edge is drawn from node $j$ to node $i$. Otherwise, we fail to reject $H_0$ and no edge is drawn.\n\nThe condition that the unrestricted model must reduce prediction error is equivalent to $RSS_U  RSS_R$. This is inherently satisfied if the F-statistic is positive, which is a necessary condition for rejecting the null hypothesis at any non-trivial significance level.\n\nThis procedure is repeated for all $k(k-1)$ ordered pairs of distinct variables $(j, i)$ to construct the full inferred adjacency matrix. The final output is a list of binary values for the specific edges requested.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for Granger causality inference.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of the entire process.\n    np.random.seed(42)\n\n    test_cases = [\n        # Case 1 (chain)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 500, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0.4, 0, 0], [0, 0.4, 0]]),\n            \"Sigma\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        },\n        # Case 2 (bidirectional and fan-out)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 800, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0.5, 0], [0.5, 0, 0], [0.3, 0, 0]]),\n            \"Sigma\": np.array([[0.5, 0, 0], [0, 0.5, 0], [0, 0, 0.5]])\n        },\n        # Case 3 (null model)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 600, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]]),\n            \"Sigma\": np.array([[1.5, 0, 0], [0, 1.5, 0], [0, 0, 1.5]])\n        },\n        # Case 4 (confounding contemporaneous noise correlation)\n        {\n            \"k\": 3, \"p\": 1, \"T\": 700, \"burn_in\": 200, \"alpha\": 0.01,\n            \"A\": np.array([[0, 0, 0], [0.35, 0, 0], [0, 0.2, 0]]),\n            \"Sigma\": np.array([[1, 0, 0.6], [0, 1, 0], [0.6, 0, 1]])\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = infer_network_granger(params)\n        all_results.append(result)\n\n    # Format the final output as a single string.\n    # e.g., [[0,1,0,0,1,0],[...]]\n    output_str = \"[\" + \",\".join([str(res) for res in all_results]) + \"]\"\n    print(output_str)\n\ndef infer_network_granger(params):\n    \"\"\"\n    Simulates VAR data and performs Granger causality inference for one test case.\n    \"\"\"\n    k = params[\"k\"]\n    p = params[\"p\"]\n    T = params[\"T\"]\n    burn_in = params[\"burn_in\"]\n    A = params[\"A\"]\n    Sigma = params[\"Sigma\"]\n    alpha = params[\"alpha\"]\n    \n    # 1. Simulate the VAR(p) process\n    total_samples = T + burn_in\n    x = np.zeros((total_samples + p, k)) # Store history including initial zeros\n\n    # Generate noise for all time steps at once\n    mean_noise = np.zeros(k)\n    # Correct cholesky decomposition for noise generation\n    L = np.linalg.cholesky(Sigma)\n    innovations = (L @ np.random.randn(k, total_samples)).T\n    \n    for t in range(p, total_samples + p):\n        # The VAR model formulation is x_t = A x_{t-1} + eps\n        # Since p=1 is given for all test cases, this simplification is fine.\n        lagged_x = x[t-1, :]\n        x[t, :] = A @ lagged_x + innovations[t-p, :]\n\n    # Discard burn-in period to get the final time series data\n    data = x[p + burn_in:, :]\n\n    # 2. Inference: Perform Granger causality test for each pair (j - i)\n    adj_matrix = np.zeros((k, k), dtype=int)\n    \n    # The number of observations for regression is T-p\n    # For a lag of p=1, we predict x(t) from x(t-1) for t=1..T-1\n    # Thus, we have T-1 observations.\n    num_obs = T - p\n    \n    for i in range(k):  # Target variable index\n        for j in range(k):  # Source variable index\n            if i == j:\n                continue\n            \n            # Prepare data for regression\n            # Response variable y is the i-th time series from t=p to T-1\n            y = data[p:, i]\n            \n            # --- Unrestricted Model ---\n            # Predictors are all k variables at lag p\n            # For p=1, this is data from t=0 to T-2\n            X_unrestricted = data[p-1:-1, :]\n            k_unrestricted = X_unrestricted.shape[1]\n            \n            # Fit using OLS\n            _, rss_u_array, _, _ = np.linalg.lstsq(X_unrestricted, y, rcond=None)\n            rss_u = rss_u_array[0] if rss_u_array.size > 0 else 0.0\n\n            # --- Restricted Model ---\n            # Predictors are all variables at lag p except j\n            predictor_indices = list(range(k))\n            predictor_indices.remove(j)\n            X_restricted = data[p-1:-1, predictor_indices]\n\n            # Fit using OLS\n            _, rss_r_array, _, _ = np.linalg.lstsq(X_restricted, y, rcond=None)\n            rss_r = rss_r_array[0] if rss_r_array.size > 0 else 0.0\n            \n            # --- F-Test ---\n            # q = number of restrictions (parameters removed)\n            q = k_unrestricted - X_restricted.shape[1]\n            \n            # Degrees of freedom for F-distribution\n            df1 = q\n            df2 = num_obs - k_unrestricted\n\n            if rss_u > 1e-10: # Avoid division by zero\n                f_stat = ((rss_r - rss_u) / df1) / (rss_u / df2)\n                \n                # p-value from survival function (1 - cdf)\n                p_value = f_dist.sf(f_stat, df1, df2)\n                \n                if p_value  alpha:\n                    adj_matrix[j, i] = 1 # Edge from j to i\n    \n    # 3. Format output according to problem specification\n    # Order: (1-2), (1-3), (2-1), (2-3), (3-1), (3-2)\n    # Corresponds to (0-based): adj[0,1], adj[0,2], adj[1,0], adj[1,2], adj[2,0], adj[2,1]\n    output_list = [\n        adj_matrix[0, 1],\n        adj_matrix[0, 2],\n        adj_matrix[1, 0],\n        adj_matrix[1, 2],\n        adj_matrix[2, 0],\n        adj_matrix[2, 1],\n    ]\n    \n    return output_list\n\nsolve()\n```"
        },
        {
            "introduction": "Standard Granger causality is defined by lagged effects, but what happens when interactions occur faster than our measurement timescale? This practice  explores the concept of 'instantaneous causality' through the lens of a Structural Vector Autoregression (SVAR) model. By working through the derivation, you will discover how the correlations in the reduced-form model's noise can be decomposed to reveal underlying contemporaneous causal links, a critical aspect of system dynamics that standard Granger causality alone cannot identify. This exercise sharpens your theoretical understanding of the assumptions and limitations inherent in different VAR formulations.",
            "id": "4278735",
            "problem": "Consider a two-node complex dynamical system represented by the structural vector autoregression of order one (SVAR($1$))\n$$\nA_{0}\\, x_{t} \\;=\\; A_{1}\\, x_{t-1} \\;+\\; e_{t},\n$$\nwhere $x_{t} = \\begin{pmatrix} x_{1,t} \\\\ x_{2,t} \\end{pmatrix}$ collects the two node states at time $t$, $A_{0}$ is an invertible contemporaneous coefficient matrix encoding instantaneous causality, $A_{1}$ is a lagged coefficient matrix encoding lagged (Granger) causality, and $e_{t}$ are structural shocks satisfying $\\mathbb{E}[e_{t}] = 0$ and $\\mathbb{E}[e_{t} e_{t}^{\\top}] = \\Lambda$, with $\\Lambda$ diagonal and positive definite. Assume an acyclic instantaneous network consistent with the ordering $(x_{1,t}, x_{2,t})$, so that $A_{0}$ is lower triangular with ones on the diagonal:\n$$\nA_{0} \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n-\\theta  1\n\\end{pmatrix},\n$$\nwhere the scalar $\\theta$ is the instantaneous causal effect from node $1$ to node $2$. Let the corresponding reduced form be\n$$\nx_{t} \\;=\\; \\Phi\\, x_{t-1} \\;+\\; u_{t}, \\quad \\text{with} \\quad u_{t} \\;=\\; A_{0}^{-1} e_{t},\n$$\nand denote the reduced-form innovation covariance by\n$$\n\\Sigma_{u} \\;=\\; \\mathbb{E}[u_{t} u_{t}^{\\top}] \\;=\\; \\begin{pmatrix}\ns_{11}  s_{12} \\\\\ns_{12}  s_{22}\n\\end{pmatrix},\n$$\nwhere $s_{11} > 0$, $s_{22} > 0$, and $s_{11} s_{22} - s_{12}^{2} > 0$. You may assume weak stationarity so that these second moments are well defined. Using only the stated structural assumptions and the definitions above, derive the instantaneous effect $\\theta$ as a closed-form symbolic expression in terms of $s_{11}$ and $s_{12}$ only. Provide your final answer as a single simplified analytic expression. No numerical rounding is required.",
            "solution": "The problem requires the derivation of the instantaneous causal effect $\\theta$ from the parameters of the reduced-form innovation covariance matrix $\\Sigma_{u}$. The derivation hinges on the relationship between the structural shocks $e_{t}$ and the reduced-form innovations $u_{t}$, and their respective covariance matrices, $\\Lambda$ and $\\Sigma_{u}$.\n\nThe relationship between the shocks is given by $u_{t} = A_{0}^{-1} e_{t}$, which can be rewritten as $e_{t} = A_{0} u_{t}$. We can use this to establish a connection between the covariance matrices. The covariance matrix of the structural shocks, $\\Lambda$, is defined as $\\Lambda = \\mathbb{E}[e_{t} e_{t}^{\\top}]$. Substituting the expression for $e_{t}$ in terms of $u_{t}$ yields:\n$$\n\\Lambda = \\mathbb{E}[ (A_{0} u_{t}) (A_{0} u_{t})^{\\top} ]\n$$\nUsing the properties of the transpose operator, $(AB)^{\\top} = B^{\\top}A^{\\top}$, we have:\n$$\n\\Lambda = \\mathbb{E}[ A_{0} u_{t} u_{t}^{\\top} A_{0}^{\\top} ]\n$$\nSince $A_{0}$ is a matrix of constant coefficients, we can move it outside the expectation:\n$$\n\\Lambda = A_{0} \\mathbb{E}[u_{t} u_{t}^{\\top}] A_{0}^{\\top}\n$$\nBy definition, the covariance matrix of the reduced-form innovations is $\\Sigma_{u} = \\mathbb{E}[u_{t} u_{t}^{\\top}]$. Therefore, we arrive at the fundamental identification equation:\n$$\nA_{0} \\Sigma_{u} A_{0}^{\\top} = \\Lambda\n$$\nThis equation links the structural parameters encoded in $A_{0}$ and $\\Lambda$ to the observable reduced-form covariance matrix $\\Sigma_{u}$.\n\nThe problem provides the specific forms for these matrices. The contemporaneous coefficient matrix $A_{0}$ is given by:\n$$\nA_{0} = \\begin{pmatrix} 1  0 \\\\ -\\theta  1 \\end{pmatrix}\n$$\nIts transpose is:\n$$\nA_{0}^{\\top} = \\begin{pmatrix} 1  -\\theta \\\\ 0  1 \\end{pmatrix}\n$$\nThe reduced-form innovation covariance matrix $\\Sigma_{u}$ is:\n$$\n\\Sigma_{u} = \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12}  s_{22} \\end{pmatrix}\n$$\nThe structural shock covariance matrix $\\Lambda$ is diagonal and positive definite. For a $2 \\times 2$ system, it has the form:\n$$\n\\Lambda = \\begin{pmatrix} \\lambda_{1}  0 \\\\ 0  \\lambda_{2} \\end{pmatrix}\n$$\nwhere $\\lambda_{1} > 0$ and $\\lambda_{2} > 0$ are the variances of the structural shocks $e_{1,t}$ and $e_{2,t}$, respectively.\n\nWe now substitute these matrices into the identification equation $A_{0} \\Sigma_{u} A_{0}^{\\top} = \\Lambda$. First, we compute the product $A_{0} \\Sigma_{u}$:\n$$\nA_{0} \\Sigma_{u} = \\begin{pmatrix} 1  0 \\\\ -\\theta  1 \\end{pmatrix} \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12}  s_{22} \\end{pmatrix} = \\begin{pmatrix} (1)(s_{11}) + (0)(s_{12})  (1)(s_{12}) + (0)(s_{22}) \\\\ (-\\theta)(s_{11}) + (1)(s_{12})  (-\\theta)(s_{12}) + (1)(s_{22}) \\end{pmatrix} = \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12} - \\theta s_{11}  s_{22} - \\theta s_{12} \\end{pmatrix}\n$$\nNext, we multiply this result by $A_{0}^{\\top}$:\n$$\n(A_{0} \\Sigma_{u}) A_{0}^{\\top} = \\begin{pmatrix} s_{11}  s_{12} \\\\ s_{12} - \\theta s_{11}  s_{22} - \\theta s_{12} \\end{pmatrix} \\begin{pmatrix} 1  -\\theta \\\\ 0  1 \\end{pmatrix}\n$$\nPerforming this final matrix multiplication gives:\n$$\n\\begin{pmatrix} (s_{11})(1) + (s_{12})(0)  (s_{11})(-\\theta) + (s_{12})(1) \\\\ (s_{12} - \\theta s_{11})(1) + (s_{22} - \\theta s_{12})(0)  (s_{12} - \\theta s_{11})(-\\theta) + (s_{22} - \\theta s_{12})(1) \\end{pmatrix} = \\begin{pmatrix} s_{11}  s_{12} - \\theta s_{11} \\\\ s_{12} - \\theta s_{11}  -\\theta s_{12} + \\theta^2 s_{11} + s_{22} - \\theta s_{12} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} s_{11}  s_{12} - \\theta s_{11} \\\\ s_{12} - \\theta s_{11}  s_{22} - 2\\theta s_{12} + \\theta^2 s_{11} \\end{pmatrix}\n$$\nNow, we equate this resulting matrix to $\\Lambda$:\n$$\n\\begin{pmatrix} s_{11}  s_{12} - \\theta s_{11} \\\\ s_{12} - \\theta s_{11}  s_{22} - 2\\theta s_{12} + \\theta^2 s_{11} \\end{pmatrix} = \\begin{pmatrix} \\lambda_{1}  0 \\\\ 0  \\lambda_{2} \\end{pmatrix}\n$$\nThe structural identification comes from the assumption that the structural shocks are uncorrelated, which implies that $\\Lambda$ is a diagonal matrix. Therefore, the off-diagonal elements of the matrix on the left-hand side must be zero. This provides an equation for $\\theta$:\n$$\ns_{12} - \\theta s_{11} = 0\n$$\nThe problem asks for an expression for $\\theta$ in terms of $s_{11}$ and $s_{12}$. We can solve the equation above for $\\theta$.\n$$\n\\theta s_{11} = s_{12}\n$$\nThe problem statement specifies that $s_{11} > 0$, ensuring we can divide by $s_{11}$ without issue.\n$$\n\\theta = \\frac{s_{12}}{s_{11}}\n$$\nThis expression gives the instantaneous causal effect $\\theta$ purely in terms of the elements of the reduced-form innovation covariance matrix $\\Sigma_{u}$, as required.",
            "answer": "$$\\boxed{\\frac{s_{12}}{s_{11}}}$$"
        },
        {
            "introduction": "Moving from a basic test to a robust scientific conclusion requires careful statistical validation and an emphasis on reproducibility. This capstone exercise  challenges you to build a research-grade inference pipeline that incorporates modern best practices. You will augment the basic Granger causality test with non-parametric validation using permutation tests, rigorously control for false positives using the Benjamini-Hochberg procedure, and ultimately assess the reproducibility of your findings across independent datasets. This practice demonstrates how to build confidence in network inference results and deliver more reliable scientific insights.",
            "id": "4278729",
            "problem": "You are given the task of designing and implementing a self-contained program to perform statistically validated Granger causality network inference with reproducibility assessment on synthetic multivariate time series generated by a stable Vector Autoregression (VAR) process of lag $1$. The problem must be approached from first principles grounded in core definitions of Granger causality and linear models, without relying on any pre-supplied specialized formulas.\n\nThe fundamental base for this task is the following set of definitions and concepts:\n- Granger causality: A time series $X_j$ is said to Granger-cause $X_i$ if the past of $X_j$ provides statistically significant predictive information about $X_i$ beyond the information provided by the past of all other variables. Under a linear model with Gaussian innovations, this is operationalized as a nested model comparison for the equation predicting $X_i(t)$, between a full model that includes all variables’ lagged values and a reduced model that excludes the lagged value of $X_j$.\n- Ordinary Least Squares (OLS): For a linear model $Y = X\\beta + \\varepsilon$, the OLS estimator minimizes the sum of squared residuals, which are given by $Y - X\\hat{\\beta}$. Model comparison is based on residual sum of squares and degrees of freedom.\n- False Discovery Rate (FDR): The False Discovery Rate (FDR) is the expected proportion of false positives among all positives. The Benjamini–Hochberg procedure controls FDR at a user-specified level $q$ for a set of hypothesis tests by selecting a threshold on sorted $p$-values.\n- Empirical validation by permutation: A nonparametric null distribution for a specific putative causal link $(j \\to i)$ can be obtained by randomly permuting the time indices of the predictor $X_j(t-1)$ while keeping the response $X_i(t)$ and the other predictors unchanged; an empirical $p$-value is computed by counting how often the null statistic exceeds the observed statistic.\n\nYou must generate synthetic data from a stable VAR process of lag $1$ with $N$ variables, defined by the linear recursion\n$$\n\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\epsilon}_t,\n$$\nwhere $\\boldsymbol{\\epsilon}_t$ are independent and identically distributed Gaussian innovations with covariance $\\sigma^2 I$, and $A$ is the $N \\times N$ coefficient matrix. Stability is enforced by scaling $A$ to have spectral radius strictly less than $1$.\n\nThe ground-truth weighted adjacency template $W$ for the non-null cases is specified by the following entries (all unspecified entries are zero). The weight matrix $W$ has $N=5$ and is given by\n$$\nW = \\begin{bmatrix}\n0.4  0.2  0.0  0.0  0.0 \\\\\n0.0  0.4  0.25  0.0  0.0 \\\\\n0.1  0.0  0.4  0.3  0.0 \\\\\n0.0  0.0  0.0  0.4  0.2 \\\\\n0.0  0.15  0.0  0.0  0.4\n\\end{bmatrix}.\n$$\nFor each non-null test case, set $A = s W$, where $s = 0.9 / \\rho(W)$ and $\\rho(W)$ is the spectral radius of $W$. This ensures a stable VAR process. For the null test case, let $W$ be diagonal:\n$$\nW_{\\text{null}} = \\begin{bmatrix}\n0.4  0  0  0  0 \\\\\n0  0.4  0  0  0 \\\\\n0  0  0.4  0  0 \\\\\n0  0  0  0.4  0 \\\\\n0  0  0  0  0.4\n\\end{bmatrix},\n$$\nand again set $A = s W_{\\text{null}}$ with the same scaling rule.\n\nFor each test case, proceed as follows:\n1. Data generation: For each test case, generate two independent realizations (replicates) of the VAR process with identical $A$ but different random seeds, $s_1$ and $s_2$, for the innovations. Use $x_0 = \\mathbf{0}$ for initialization. The seeds must be deterministic per case: set $s_1 = b$ and $s_2 = b+1$, where $b$ is the base seed specified in the test suite. The innovations must be drawn from a Gaussian with variance $\\sigma^2$, that is, $\\boldsymbol{\\epsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$.\n2. Model fitting and nested comparison: For each ordered pair $(j,i)$ with $j \\neq i$, fit the full OLS regression for $X_i(t)$ using all lagged variables $X_k(t-1)$ for $k \\in \\{1,\\dots,N\\}$, and fit the reduced regression that excludes $X_j(t-1)$. Derive a suitable comparison statistic from first principles of nested linear model comparison and compute a parametric $p$-value under its appropriate reference distribution, taking into account sample size and degrees of freedom.\n3. Empirical validation: For each putative causal edge $(j \\to i)$, construct an empirical null distribution by performing $B$ independent random permutations of the time index of the predictor $X_j(t-1)$ while keeping all else fixed. For each permuted dataset, recompute the same comparison statistic. Compute an empirical $p$-value as the fraction of permutations whose statistic is at least as large as the observed statistic, with the standard finite-sample correction by adding $1$ to numerator and denominator.\n4. Multiple testing control: Apply the Benjamini–Hochberg FDR procedure separately to the sets of parametric $p$-values and empirical $p$-values across all ordered pairs $(j,i)$ with $j \\neq i$. Define the \"validated edge set\" for a replicate as the intersection of edges selected by both the parametric and empirical FDR procedures at level $q$.\n5. Reproducibility assessment: Define the reproducible edge set for the test case as the intersection of the validated edge sets across the two independent replicates. Let the ground truth edge set be all ordered pairs $(j,i)$, $j \\neq i$, with $W_{i j} > 0$ in the non-null cases, and empty in the null case. Compute the precision and recall of the reproducible edge set relative to the ground truth; if a denominator is zero, define the corresponding ratio as $0$ for this task.\n\nYour program must implement the above workflow and produce results for the following test suite. Each tuple lists $(T, \\sigma, q, B, \\text{is\\_null}, b)$:\n- Case $1$: $(T = 1000, \\sigma = 0.1, q = 0.05, B = 200, \\text{is\\_null} = 0, b = 11)$.\n- Case $2$: $(T = 200, \\sigma = 0.1, q = 0.05, B = 200, \\text{is\\_null} = 0, b = 13)$.\n- Case $3$: $(T = 1000, \\sigma = 0.1, q = 0.01, B = 200, \\text{is\\_null} = 0, b = 17)$.\n- Case $4$: $(T = 1000, \\sigma = 0.1, q = 0.05, B = 200, \\text{is\\_null} = 1, b = 19)$.\n\nFor each test case, your program must output a list with three entries: the integer size of the reproducible edge set, the precision as a floating-point number, and the recall as a floating-point number. Your program should produce a single line of output containing the results as a comma-separated list of these per-case lists, enclosed in square brackets (for example, $[[a_1,p_1,r_1],[a_2,p_2,r_2],[a_3,p_3,r_3],[a_4,p_4,r_4]]$). No physical units are involved in this problem, and no angle units are required.",
            "solution": "The objective of this problem is to design and implement a computational pipeline for inferring a causal network from multivariate time series data. This is accomplished using the principle of Granger causality, statistically validated through both parametric and non-parametric (permutation-based) hypothesis testing, with corrections for multiple comparisons. The final step involves assessing the reproducibility of the inferred network across independent data realizations. The entire process is built from first principles of linear algebra and statistics.\n\n### 1. Theoretical Framework\n\n#### 1.1. Data Generation: Vector Autoregressive (VAR) Process\n\nThe synthetic time series data are generated from a stable, first-order Vector Autoregressive, or $\\text{VAR}(1)$, process. This model describes the evolution of a vector of $N$ time series variables, $\\mathbf{x}_t \\in \\mathbb{R}^N$, as a linear function of its state at the previous time step, plus a random innovation term. The governing equation is:\n$$\n\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\boldsymbol{\\epsilon}_t\n$$\nwhere:\n- $\\mathbf{x}_t = [X_1(t), X_2(t), \\dots, X_N(t)]^T$ is the state vector at time $t$.\n- $A$ is the $N \\times N$ coefficient matrix, where the element $A_{ij}$ quantifies the linear influence of variable $X_j$ at time $t-1$ on variable $X_i$ at time $t$.\n- $\\boldsymbol{\\epsilon}_t$ is a vector of independent and identically distributed (i.i.d.) Gaussian innovations, drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$, where $I$ is the $N \\times N$ identity matrix and $\\sigma^2$ is the variance of the noise.\n\nFor the process to be stable (i.e., non-divergent), the spectral radius of the coefficient matrix $A$, denoted $\\rho(A)$, must be strictly less than $1$. The spectral radius is the maximum absolute value of the eigenvalues of $A$. To ensure stability, we construct $A$ from a ground-truth weighted adjacency matrix $W$ by scaling it: $A = sW$, with the scaling factor $s = 0.9 / \\rho(W)$. This guarantees that $\\rho(A) = 0.9  1$.\n\nThe simulation starts with an initial condition $\\mathbf{x}_0 = \\mathbf{0}$ and proceeds for $T$ time steps, generating a data matrix of size $N \\times T$.\n\n#### 1.2. Granger Causality as a Nested Model Test\n\nGranger causality provides a statistical framework for determining causality based on prediction. A variable $X_j$ is said to Granger-cause another variable $X_i$ if past values of $X_j$ contain information that helps predict $X_i$ above and beyond the information contained in the past values of $X_i$ and all other variables.\n\nFor a $\\text{VAR}(1)$ model, this hypothesis can be tested by comparing two nested linear regression models for predicting $X_i(t)$ using data from $t=1, \\dots, T-1$. The number of observations available for regression is $n = T-1$.\n\n- **Full Model:** This model includes the lagged values of all $N$ variables as predictors.\n$$\nX_i(t) = \\sum_{k=1}^{N} \\beta_k X_k(t-1) + \\varepsilon_{\\text{full}}(t)\n$$\n- **Reduced Model:** This model excludes the lagged value of the putative causal variable, $X_j(t-1)$.\n$$\nX_i(t) = \\sum_{k=1, k \\neq j}^{N} \\beta_k X_k(t-1) + \\varepsilon_{\\text{red}}(t)\n$$\n\nThe null hypothesis, $H_0$, is that $X_j$ does not Granger-cause $X_i$, which corresponds to the regression coefficient for $X_j(t-1)$ being zero in the full model. If the full model provides a statistically significant better fit to the data than the reduced model, we reject $H_0$.\n\n#### 1.3. The F-statistic for Model Comparison\n\nThe comparison between the full and reduced models is performed using an F-test. Both models are fitted using Ordinary Least Squares (OLS), which minimizes the Residual Sum of Squares (RSS). The RSS for a generic model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ is given by $\\text{RSS} = ||\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}||^2$, where $\\hat{\\boldsymbol{\\beta}}$ is the OLS estimate.\n\nLet $\\text{RSS}_{\\text{full}}$ be the RSS of the full model and $\\text{RSS}_{\\text{red}}$ be the RSS of the reduced model. The F-statistic is formulated as:\n$$\nF = \\frac{(\\text{RSS}_{\\text{red}} - \\text{RSS}_{\\text{full}}) / (p_{\\text{full}} - p_{\\text{red}})}{\\text{RSS}_{\\text{full}} / (n - p_{\\text{full}})}\n$$\nwhere:\n- $n = T-1$ is the number of observations used for regression.\n- $p_{\\text{full}} = N$ is the number of predictors in the full model.\n- $p_{\\text{red}} = N-1$ is the number of predictors in the reduced model.\n\nSubstituting these values, the number of degrees of freedom for the numerator is $df_1 = p_{\\text{full}} - p_{\\text{red}} = 1$, and for the denominator is $df_2 = n - p_{\\text{full}} = T - 1 - N$. The F-statistic simplifies to:\n$$\nF = \\frac{\\text{RSS}_{\\text{red}} - \\text{RSS}_{\\text{full}}}{\\text{RSS}_{\\text{full}} / (T - 1 - N)}\n$$\nUnder the null hypothesis, this statistic follows an F-distribution, $F \\sim \\mathcal{F}(df_1, df_2)$. The **parametric p-value** is the probability of observing an F-statistic at least as large as the one computed from the data, $p_{\\text{param}} = P(Y \\ge F_{\\text{obs}} | Y \\sim \\mathcal{F}(df_1, df_2))$.\n\n#### 1.4. Empirical p-value via Permutation Testing\n\nTo construct a non-parametric test, we generate an empirical null distribution for the F-statistic. This is done by repeatedly breaking the temporal relationship between the predictor $X_j(t-1)$ and the response $X_i(t)$, while preserving other relationships. For a specific putative link $j \\to i$, the procedure is:\n1.  Compute the observed F-statistic, $F_{\\text{obs}}$, as described above.\n2.  Perform $B$ repetitions of the following:\n    a. Create a permuted time series for the predictor $X_j$ by randomly shuffling its time indices. This yields $X_{j, \\text{perm}}(t-1)$.\n    b. Construct a new \"full model\" design matrix where the column corresponding to $X_j(t-1)$ is replaced by the permuted version. The reduced model remains unchanged.\n    c. Recompute the F-statistic using this permuted data, yielding a sample from the null distribution, $F_{\\text{null}}$.\n3.  The **empirical p-value** is the fraction of null statistics that are greater than or equal to the observed statistic, with a correction for a finite number of permutations:\n$$\np_{\\text{emp}} = \\frac{1 + |\\{k \\in \\{1, \\dots, B\\} : F_{\\text{null},k} \\geq F_{\\text{obs}}\\}|}{1 + B}\n$$\n\n#### 1.5. Controlling for Multiple Comparisons\n\nWe perform $m = N(N-1)$ hypothesis tests, one for each potential directed edge between distinct variables. This necessitates a multiple testing correction to control the number of false discoveries. The Benjamini-Hochberg (BH) procedure is used to control the False Discovery Rate (FDR) at a specified level $q$. The procedure is:\n1.  Collect the $m$ p-values for all tested edges, $\\{p_1, p_2, \\dots, p_m\\}$.\n2.  Sort the p-values in ascending order: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$.\n3.  Find the largest integer $k$ such that $p_{(k)} \\le \\frac{k}{m} q$.\n4.  Reject the null hypothesis for all tests corresponding to the p-values $p_{(1)}, \\dots, p_{(k)}$.\n\nThis procedure is applied independently to the set of parametric p-values and the set of empirical p-values.\n\n#### 1.6. Validation and Reproducibility\n\nTo increase confidence in the inferred connections, we combine evidence from both statistical approaches and from independent experiments.\n- **Validated Edge Set**: For a single data replicate, the set of validated edges is the intersection of the edges found significant by the BH-corrected parametric tests and the BH-corrected empirical tests.\n- **Reproducible Edge Set**: The final set of inferred connections for a given test case is the intersection of the validated edge sets obtained from two independent data replicates (generated with the same $A$ matrix but different random seeds for the innovations).\n\n#### 1.7. Performance Metrics\n\nThe performance of the inference pipeline is evaluated against the ground-truth network structure defined by the matrix $W$. An edge $j \\to i$ exists if $W_{ij} > 0$ (for $j \\neq i$). Let $E_{\\text{repro}}$ be the reproducible edge set and $E_{\\text{GT}}$ be the ground-truth edge set.\n- True Positives ($TP$): $|E_{\\text{repro}} \\cap E_{\\text{GT}}|$\n- False Positives ($FP$): $|E_{\\text{repro}} \\setminus E_{\\text{GT}}|$\n- False Negatives ($FN$): $|E_{\\text{GT}} \\setminus E_{\\text{repro}}|$\n\nPrecision and Recall are then calculated as:\n- **Precision**: $P = \\frac{TP}{TP + FP}$. If $TP + FP = 0$, $P=0$.\n- **Recall**: $R = \\frac{TP}{TP + FN}$. If $TP + FN = 0$, $R=0$.\n\nThe final output for each test case is the size of the reproducible edge set ($|E_{\\text{repro}}|$), the precision, and the recall.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the entire test suite and print the results.\n    \"\"\"\n    \n    # Ground truth matrix for non-null cases\n    W_non_null = np.array([\n        [0.4, 0.2, 0.0, 0.0, 0.0],\n        [0.0, 0.4, 0.25, 0.0, 0.0],\n        [0.1, 0.0, 0.4, 0.3, 0.0],\n        [0.0, 0.0, 0.0, 0.4, 0.2],\n        [0.0, 0.15, 0.0, 0.0, 0.4]\n    ])\n\n    # Ground truth matrix for null cases\n    W_null = np.diag([0.4] * 5)\n    \n    # Test suite parameters: (T, sigma, q, B, is_null, b)\n    test_cases = [\n        (1000, 0.1, 0.05, 200, 0, 11),\n        (200, 0.1, 0.05, 200, 0, 13),\n        (1000, 0.1, 0.01, 200, 0, 17),\n        (1000, 0.1, 0.05, 200, 1, 19),\n    ]\n\n    all_results = []\n    \n    for T, sigma, q, B, is_null, b in test_cases:\n        W = W_null if is_null else W_non_null\n        result = run_single_case(W, T, sigma, q, B, b)\n        all_results.append(result)\n\n    formatted_results = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_single_case(W, T, sigma, q, B, base_seed):\n    \"\"\"\n    Runs a single test case for Granger causality inference.\n    \"\"\"\n    N = W.shape[0]\n\n    # Stabilize the VAR process\n    spec_rad = np.max(np.abs(np.linalg.eigvals(W)))\n    s = 0.9 / spec_rad\n    A = s * W\n\n    # Determine ground truth edge set\n    ground_truth_edges = set()\n    if not np.all(W == np.diag(np.diag(W))): # Check if non-null\n        rows, cols = np.where(W.T != 0) # Adjacency matrix is transpose of coefficient matrix\n        for i, j in zip(rows, cols):\n            if i != j:\n                # W_ji corresponds to edge i -> j\n                ground_truth_edges.add((i, j))\n\n    # Run two independent replicates\n    seed1, seed2 = base_seed, base_seed + 1\n    validated_edges1 = infer_replicate(A, T, sigma, q, B, seed1)\n    validated_edges2 = infer_replicate(A, T, sigma, q, B, seed2)\n    \n    # Assess reproducibility\n    reproducible_edges = validated_edges1.intersection(validated_edges2)\n\n    # Compute performance metrics\n    tp = len(reproducible_edges.intersection(ground_truth_edges))\n    fp = len(reproducible_edges - ground_truth_edges)\n    fn = len(ground_truth_edges - reproducible_edges)\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    \n    return [len(reproducible_edges), precision, recall]\n\ndef infer_replicate(A, T, sigma, q, B, seed):\n    \"\"\"\n    Performs inference on a single replicate of the time series data.\n    \"\"\"\n    np.random.seed(seed)\n    N = A.shape[0]\n\n    # 1. Data Generation\n    X = np.zeros((N, T))\n    for t in range(1, T):\n        noise = np.random.normal(0, sigma, N)\n        X[:, t] = A @ X[:, t - 1] + noise\n\n    # 2. Model Fitting and Testing\n    edges = []\n    param_p_values = []\n    emp_p_values = []\n    \n    for i in range(N):  # Response variable\n        for j in range(N):  # Predictor variable\n            if i == j:\n                continue\n            \n            p_param, p_emp = compute_p_values(X, i, j, B)\n            edges.append((j, i)) # edge j -> i\n            param_p_values.append(p_param)\n            emp_p_values.append(p_emp)\n    \n    # 4. Multiple Testing Control and Validation\n    param_sig_edges = apply_fdr(edges, param_p_values, q)\n    emp_sig_edges = apply_fdr(edges, emp_p_values, q)\n    \n    validated_edges = param_sig_edges.intersection(emp_sig_edges)\n    return validated_edges\n\ndef compute_p_values(X, i, j, B):\n    \"\"\"\n    Computes parametric and empirical p-values for a single putative edge (j -> i).\n    \"\"\"\n    N, T = X.shape\n    n_obs = T - 1\n\n    y = X[i, 1:]\n    X_lagged = X[:, :-1].T\n    \n    # Reduced model design matrix (excluding predictor j)\n    X_red = np.delete(X_lagged, j, axis=1)\n\n    # Compute observed F-statistic\n    f_obs, _, _ = calculate_f_statistic(y, X_lagged, X_red)\n    \n    # Parametric p-value\n    df1 = 1\n    df2 = n_obs - N\n    p_param = f_dist.sf(f_obs, df1, df2) if df2 > 0 else 1.0\n\n    # Empirical p-value from permutation test\n    f_perms = np.zeros(B)\n    rng = np.random.default_rng(seed=i*N+j) # create a local RNG for this permutation test\n    for k in range(B):\n        X_lagged_perm = X_lagged.copy()\n        X_lagged_perm[:, j] = rng.permutation(X_lagged_perm[:, j])\n        f_perms[k], _, _ = calculate_f_statistic(y, X_lagged_perm, X_red)\n    \n    count_exceed = np.sum(f_perms >= f_obs)\n    p_emp = (1 + count_exceed) / (1 + B)\n    \n    return p_param, p_emp\n\ndef calculate_f_statistic(y, X_full, X_red):\n    \"\"\"\n    Calculates the F-statistic for nested OLS models.\n    \"\"\"\n    n_obs, p_full = X_full.shape\n    \n    # Fit full model\n    _, rss_full_arr, _, _ = np.linalg.lstsq(X_full, y, rcond=None)\n    rss_full = rss_full_arr[0] if len(rss_full_arr) > 0 else 0.0\n\n    # Fit reduced model\n    _, rss_red_arr, _, _ = np.linalg.lstsq(X_red, y, rcond=None)\n    rss_red = rss_red_arr[0] if len(rss_red_arr) > 0 else 0.0\n\n    # Handle cases of perfect fit, though unlikely with noise\n    if rss_full  np.finfo(float).eps:\n        return np.inf if rss_red > rss_full else 0.0, rss_full, rss_red\n\n    df_num = 1\n    df_den = n_obs - p_full\n    if df_den = 0:\n        return 0.0, rss_full, rss_red\n    \n    f_stat = ((rss_red - rss_full) / df_num) / (rss_full / df_den)\n    return max(0, f_stat), rss_full, rss_red # ensure non-negative due to float precision\n\ndef apply_fdr(edges, p_values, q):\n    \"\"\"\n    Applies the Benjamini-Hochberg FDR correction.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return set()\n\n    # Sort p-values and corresponding edges\n    sorted_indices = np.argsort(p_values)\n    sorted_p_values = np.array(p_values)[sorted_indices]\n    \n    # Find threshold\n    thresholds = (np.arange(1, m + 1) / m) * q\n    significant_mask = sorted_p_values = thresholds\n    \n    if np.any(significant_mask):\n        # Find the last significant p-value\n        k = np.where(significant_mask)[0].max()\n        significant_indices = sorted_indices[:k+1]\n        return {edges[i] for i in significant_indices}\n    else:\n        return set()\n\n# Corrected call to main execution block\nsolve()\n\n```"
        }
    ]
}