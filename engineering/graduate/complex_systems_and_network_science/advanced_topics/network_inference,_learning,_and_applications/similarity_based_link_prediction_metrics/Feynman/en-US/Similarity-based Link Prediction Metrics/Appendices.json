{
    "hands_on_practices": [
        {
            "introduction": "To understand how a link prediction metric performs on a real network, we first need a baseline. This practice explores the behavior of the Common Neighbors score in an Erdős–Rényi random graph, the simplest null model for networks. By deriving the expected value and variance of the score, you will build a foundational understanding of what to expect in a network devoid of any specific structural patterns .",
            "id": "4302811",
            "problem": "Consider a simple undirected Erdős–Rényi random graph $G(n,p)$ on vertex set $V=\\{1,2,\\dots,n\\}$, where each unordered pair of distinct vertices forms an edge independently with probability $p \\in (0,1)$. For two distinct vertices $u,v \\in V$, the common neighbors similarity score $s_{\\mathrm{CN}}(u,v)$ is defined as the number of vertices in $V \\setminus \\{u,v\\}$ that are adjacent to both $u$ and $v$, i.e., \n$$\ns_{\\mathrm{CN}}(u,v) \\equiv \\sum_{w \\in V \\setminus \\{u,v\\}} \\mathbf{1}\\{(u,w) \\in E \\text{ and } (v,w) \\in E\\},\n$$\nwhere $E$ is the edge set and $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function.\n\nFix two distinct vertices $u$ and $v$ and condition on the event that $(u,v)$ is not an edge. Starting only from the independence structure of the Erdős–Rényi model and the above definition of $s_{\\mathrm{CN}}(u,v)$, derive closed-form expressions for the conditional expectation $\\mathbb{E}[s_{\\mathrm{CN}}(u,v) \\mid (u,v) \\notin E]$ and the conditional variance $\\operatorname{Var}(s_{\\mathrm{CN}}(u,v) \\mid (u,v) \\notin E)$ in terms of $n$ and $p$.\n\nExpress your final answers as analytic expressions in terms of $n$ and $p$. No numerical approximation or rounding is required.",
            "solution": "The problem asks for the conditional expectation $\\mathbb{E}[s_{\\mathrm{CN}}(u,v) \\mid (u,v) \\notin E]$ and conditional variance $\\operatorname{Var}(s_{\\mathrm{CN}}(u,v) \\mid (u,v) \\notin E]$ of the common neighbors similarity score between two distinct vertices $u$ and $v$ in an Erdős–Rényi random graph $G(n,p)$.\n\nThe graph $G(n,p)$ has a vertex set $V = \\{1, 2, \\dots, n\\}$. The edge set $E$ is formed by including each of the $\\binom{n}{2}$ possible edges independently with probability $p$. Let us denote the indicator random variable for the presence of an edge between vertices $i$ and $j$ as $X_{ij}$. That is, $X_{ij} = 1$ if $(i,j) \\in E$ and $X_{ij}=0$ otherwise. The set of variables $\\{X_{ij} \\mid 1 \\le i  j \\le n\\}$ is a collection of independent and identically distributed Bernoulli random variables with success probability $p$.\n\nThe common neighbors score $s_{\\mathrm{CN}}(u,v)$ for two distinct vertices $u,v \\in V$ is defined as the number of other vertices adjacent to both $u$ and $v$. Formally,\n$$\ns_{\\mathrm{CN}}(u,v) = \\sum_{w \\in V \\setminus \\{u,v\\}} \\mathbf{1}\\{(u,w) \\in E \\text{ and } (v,w) \\in E\\}\n$$\nFor each vertex $w \\in V \\setminus \\{u,v\\}$, let us define an indicator variable $Z_w$:\n$$\nZ_w = \\mathbf{1}\\{(u,w) \\in E \\text{ and } (v,w) \\in E\\} = X_{uw} X_{vw}\n$$\nThe common neighbors score can then be written as a sum of these $n-2$ indicator variables:\n$$\ns_{\\mathrm{CN}}(u,v) = \\sum_{w \\in V \\setminus \\{u,v\\}} Z_w\n$$\nWe are asked to compute the expectation and variance of $s_{\\mathrm{CN}}(u,v)$ conditioned on the event $A \\equiv \\{(u,v) \\notin E\\}$. This event corresponds to $X_{uv} = 0$.\n\nThe crucial insight lies in analyzing the stochastic dependence structure. The random variable $s_{\\mathrm{CN}}(u,v)$ is a function of the set of random variables $\\{Z_w \\mid w \\in V \\setminus \\{u,v\\}\\}$. Each $Z_w$ is, in turn, a function of the edge indicator variables $X_{uw}$ and $X_{vw}$. Thus, $s_{\\mathrm{CN}}(u,v)$ is determined entirely by the set of random variables $\\{X_{uw}, X_{vw} \\mid w \\in V \\setminus \\{u,v\\}\\}$.\n\nThe conditioning event $A$ is determined by the state of a single, different random variable, $X_{uv}$.\nIn the Erdős–Rényi model, all edge indicator variables $X_{ij}$ are mutually independent. The set of edge indices for the variables determining $s_{\\mathrm{CN}}(u,v)$ is $\\{\\{u,w\\}, \\{v,w\\} \\mid w \\in V \\setminus \\{u,v\\}\\}$. The edge index for the conditioning event is $\\{u,v\\}$. Since $w$ is distinct from $u$ and $v$, the pair $\\{u,v\\}$ is distinct from any pair in the form $\\{u,w\\}$ or $\\{v,w\\}$. Therefore, the random variable $X_{uv}$ is independent of the entire set of random variables $\\{X_{uw}, X_{vw} \\mid w \\in V \\setminus \\{u,v\\}\\}$.\n\nBecause $s_{\\mathrm{CN}}(u,v)$ is a function of a set of random variables that are stochastically independent of $X_{uv}$, the random variable $s_{\\mathrm{CN}}(u,v)$ is itself independent of $X_{uv}$. This implies that $s_{\\mathrm{CN}}(u,v)$ is also independent of the event $A = \\{X_{uv}=0\\}$.\n\nFor any random variable $Y$ and an event $B$ that are stochastically independent, the conditional expectation and variance of $Y$ given $B$ are equal to their unconditional counterparts: $\\mathbb{E}[Y \\mid B] = \\mathbb{E}[Y]$ and $\\operatorname{Var}(Y \\mid B) = \\operatorname{Var}(Y)$.\nApplying this principle, we conclude:\n$$\n\\mathbb{E}[s_{\\mathrm{CN}}(u,v) \\mid (u,v) \\notin E] = \\mathbb{E}[s_{\\mathrm{CN}}(u,v)]\n$$\n$$\n\\operatorname{Var}(s_{\\mathrm{CN}}(u,v) \\mid (u,v) \\notin E) = \\operatorname{Var}(s_{\\mathrm{CN}}(u,v))\n$$\nThe problem is now reduced to calculating the unconditional expectation and variance.\n\n**1. Expectation of $s_{\\mathrm{CN}}(u,v)$**\nBy the linearity of expectation,\n$$\n\\mathbb{E}[s_{\\mathrm{CN}}(u,v)] = \\mathbb{E}\\left[\\sum_{w \\in V \\setminus \\{u,v\\}} Z_w\\right] = \\sum_{w \\in V \\setminus \\{u,v\\}} \\mathbb{E}[Z_w]\n$$\nThe expectation of the indicator variable $Z_w$ is the probability that $Z_w = 1$:\n$$\n\\mathbb{E}[Z_w] = P(Z_w = 1) = P((u,w) \\in E \\text{ and } (v,w) \\in E)\n$$\nSince the existence of edges $(u,w)$ and $(v,w)$ are independent events in the $G(n,p)$ model, we have:\n$$\n\\mathbb{E}[Z_w] = P((u,w) \\in E) \\cdot P((v,w) \\in E) = p \\cdot p = p^2\n$$\nEach $Z_w$ is thus a Bernoulli random variable with parameter $p^2$. The sum for the expectation contains $n-2$ identical terms:\n$$\n\\mathbb{E}[s_{\\mathrm{CN}}(u,v)] = \\sum_{w \\in V \\setminus \\{u,v\\}} p^2 = (n-2)p^2\n$$\n\n**2. Variance of $s_{\\mathrm{CN}}(u,v)$**\nThe variance of a sum of random variables is the sum of their covariances:\n$$\n\\operatorname{Var}(s_{\\mathrm{CN}}(u,v)) = \\operatorname{Var}\\left(\\sum_{w \\in V \\setminus \\{u,v\\}} Z_w\\right) = \\sum_{w_1, w_2 \\in V \\setminus \\{u,v\\}} \\operatorname{Cov}(Z_{w_1}, Z_{w_2})\n$$\nLet's analyze the covariance term $\\operatorname{Cov}(Z_{w_1}, Z_{w_2})$ for two distinct vertices $w_1, w_2 \\in V \\setminus \\{u,v\\}$. The random variable $Z_{w_1}$ depends on the edges $(u,w_1)$ and $(v,w_1)$. The random variable $Z_{w_2}$ depends on the edges $(u,w_2)$ and $(v,w_2)$. As $u,v,w_1,w_2$ are four distinct vertices, the set of edges $\\{(u,w_1), (v,w_1)\\}$ is disjoint from $\\{(u,w_2), (v,w_2)\\}$. Since all edges in $G(n,p)$ are formed independently, the random variables $Z_{w_1}$ and $Z_{w_2}$ are independent for $w_1 \\neq w_2$.\nThe covariance of independent random variables is zero, so $\\operatorname{Cov}(Z_{w_1}, Z_{w_2})=0$ for $w_1 \\neq w_2$.\nThis simplifies the variance calculation to the sum of the individual variances:\n$$\n\\operatorname{Var}(s_{\\mathrm{CN}}(u,v)) = \\sum_{w \\in V \\setminus \\{u,v\\}} \\operatorname{Var}(Z_w)\n$$\nSince $Z_w \\sim \\operatorname{Bernoulli}(p^2)$, its variance is:\n$$\n\\operatorname{Var}(Z_w) = p^2(1-p^2)\n$$\nThe total variance is the sum of $n-2$ identical variance terms:\n$$\n\\operatorname{Var}(s_{\\mathrm{CN}}(u,v)) = \\sum_{w \\in V \\setminus \\{u,v\\}} p^2(1-p^2) = (n-2)p^2(1-p^2)\n$$\nCombining these results gives the final conditional expectation and variance.\nThe conditional expectation is $\\mathbb{E}[s_{\\mathrm{CN}}(u,v) \\mid (u,v) \\notin E] = (n-2)p^2$.\nThe conditional variance is $\\operatorname{Var}(s_{\\mathrm{CN}}(u,v) \\mid (u,v) \\notin E) = (n-2)p^2(1-p^2)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} (n-2)p^2  (n-2)p^2(1-p^2) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The simple act of counting common neighbors treats all neighbors equally, but this can be misleading. This exercise demonstrates the vulnerability of the basic Common Neighbors metric to manipulation through a thought experiment involving an artificially inflated hub. By comparing its flawed recommendations to those from degree-penalizing metrics like Adamic-Adar and Resource Allocation, you will discover the importance of weighing neighbors to create more robust predictions .",
            "id": "4302824",
            "problem": "Consider an undirected simple graph $G=(V,E)$ representing a recommendation network. A target vertex $x \\in V$ has neighbors $\\{h,a,b\\}$, and we aim to score potential links from $x$ to non-neighbor vertices using similarity-based link prediction metrics. An adversary constructs a hub $h$ with artificially inflated degree by attaching $m=50$ fake vertices $\\{f_1,\\dots,f_{50}\\}$, each connected only to $h$. The neighborhood sets are:\n- $\\Gamma(x)=\\{h,a,b\\}$,\n- $\\Gamma(h)=\\{x\\}\\cup\\{f_1,\\dots,f_{50}\\}$,\n- $\\Gamma(a)=\\{x,u,v\\}$,\n- $\\Gamma(b)=\\{x,u,w\\}$,\n- $\\Gamma(u)=\\{a,b\\}$,\n- $\\Gamma(v)=\\{a\\}$,\n- $\\Gamma(w)=\\{b\\}$,\n- $\\Gamma(f_i)=\\{h\\}$ for all $i\\in\\{1,\\dots,50\\}$.\n\nThe candidate non-edges to be scored are $(x,u)$, $(x,v)$, $(x,w)$, and $(x,f_i)$ for any $i\\in\\{1,\\dots,50\\}$. Starting from the core definitions of degree and common neighbors in undirected graphs, and the standard similarity-based link prediction metrics Common Neighbors (CN), Adamic-Adar (AA), and Resource Allocation (RA), determine which statement best characterizes the relative ordering of scores across these candidate non-edges and explains the susceptibility of $CN$ to the adversarial hub and the mitigation provided by $AA$ and $RA$. Assume logarithms are natural and ties are permitted.\n\nA. Under $CN$, one has $s_{CN}(x,u)s_{CN}(x,v)=s_{CN}(x,w)=s_{CN}(x,f_i)$ for any $i$. Under Adamic-Adar (AA) and Resource Allocation (RA), one has $s_{AA}(x,u)s_{AA}(x,v)=s_{AA}(x,w)s_{AA}(x,f_i)$ and $s_{RA}(x,u)s_{RA}(x,v)=s_{RA}(x,w)s_{RA}(x,f_i)$, because the hub’s inflated degree down-weights its contribution. Thus, the adversarial hub floods $CN$ with many ties via $\\{f_i\\}$, but $AA$ and $RA$ reduce susceptibility. This qualitative ordering under $AA$ and $RA$ remains unchanged even if $10\\%$ of $h$’s incident edges are missing.\n\nB. Under $CN$, $s_{CN}(x,f_i)s_{CN}(x,u)$ because the hub’s degree is higher. Under $AA$, inflating $k_h$ further increases $s_{AA}(x,f_i)$, and under $RA$ the hub contributes zero, so $s_{RA}(x,f_i)=0$.\n\nC. Under $CN$, $s_{CN}(x,u)=s_{CN}(x,v)=s_{CN}(x,w)=s_{CN}(x,f_i)$. Under $AA$, $s_{AA}(x,f_i)s_{AA}(x,v)$ because $1/\\log k_h$ grows with $k_h$. Under $RA$, the scores are invariant to the degree of common neighbors.\n\nD. Under $CN$, $s_{CN}(x,u)=s_{CN}(x,v)s_{CN}(x,w)=s_{CN}(x,f_i)$. Under $AA$ and $RA$, one has $s_{AA}(x,f_i)=s_{AA}(x,v)$ and $s_{RA}(x,f_i)=s_{RA}(x,v)$ due to fairness considerations and missing data, so the adversarial hub does not affect relative ordering.",
            "solution": "The problem statement is critically validated as follows.\n\n### Step 1: Extract Givens\n-   The graph is an undirected simple graph $G=(V,E)$.\n-   A target vertex $x \\in V$ is considered.\n-   An adversarial hub $h$ has its degree inflated by adding $m=50$ fake vertices $\\{f_1,\\dots,f_{50}\\}$, each connected only to $h$.\n-   Neighborhood sets are provided:\n    -   $\\Gamma(x)=\\{h,a,b\\}$\n    -   $\\Gamma(h)=\\{x\\}\\cup\\{f_1,\\dots,f_{50}\\}$\n    -   $\\Gamma(a)=\\{x,u,v\\}$\n    -   $\\Gamma(b)=\\{x,u,w\\}$\n    -   $\\Gamma(u)=\\{a,b\\}$\n    -   $\\Gamma(v)=\\{a\\}$\n    -   $\\Gamma(w)=\\{b\\}$\n    -   $\\Gamma(f_i)=\\{h\\}$ for all $i\\in\\{1,\\dots,50\\}$\n-   Candidate non-edges to be scored: $(x,u)$, $(x,v)$, $(x,w)$, and $(x,f_i)$ for any $i\\in\\{1,\\dots,50\\}$.\n-   Metrics to be used: Common Neighbors (CN), Adamic-Adar (AA), and Resource Allocation (RA).\n-   Assumptions: Logarithms are natural, and ties are permitted.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem uses standard, well-defined concepts from graph theory and network science, including neighborhoods, degrees, and the link prediction metrics CN, AA, and RA. The adversarial scenario is a recognized research topic in the robustness of network algorithms. The problem is scientifically sound.\n2.  **Well-Posedness**: The graph structure is explicitly and fully defined through the neighborhood sets. The set of candidate links is clearly specified. The scoring functions are standard. This allows for a unique set of scores to be computed, leading to a determinable ordering. The problem is well-posed.\n3.  **Objectivity**: The problem is stated using precise mathematical notation and objective language, free of ambiguity or subjective claims.\n4.  **Completeness and Consistency**: The neighborhood definitions are consistent for an undirected graph. For every edge $(y,z)$, $y$ is in $\\Gamma(z)$ and $z$ is in $\\Gamma(y)$. For instance, $a \\in \\Gamma(x)$ and $x \\in \\Gamma(a)$; $h \\in \\Gamma(f_i)$ and $f_i \\in \\Gamma(h)$. All provided definitions are mutually consistent. The problem is complete and self-contained.\n5.  **Realism and Feasibility**: The scenario, while a simplified model, represents a realistic type of adversarial attack on recommendation systems. All calculations are feasible.\n6.  **Other Flaws**: The problem is not trivial, tautological, or ill-posed. It requires a direct application and comparison of standard metrics.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A rigorous solution can be derived.\n\n### Derivation of Solution\n\nFirst, we define the three similarity-based link prediction metrics for a pair of non-connected vertices $(X,Y)$:\n1.  **Common Neighbors (CN)**: The score is the number of neighbors shared by $X$ and $Y$.\n    $$s_{CN}(X,Y) = |\\Gamma(X) \\cap \\Gamma(Y)|$$\n2.  **Adamic-Adar (AA)**: This metric refines CN by assigning higher weights to common neighbors with lower degrees.\n    $$s_{AA}(X,Y) = \\sum_{z \\in \\Gamma(X) \\cap \\Gamma(Y)} \\frac{1}{\\log k_z}$$\n    where $k_z = |\\Gamma(z)|$ is the degree of vertex $z$.\n3.  **Resource Allocation (RA)**: Similar to AA, this metric penalizes high-degree common neighbors.\n    $$s_{RA}(X,Y) = \\sum_{z \\in \\Gamma(X) \\cap \\Gamma(Y)} \\frac{1}{k_z}$$\n\nNext, we calculate the degrees ($k_z$) of all relevant vertices from their given neighborhood sets:\n-   $k_x = |\\Gamma(x)| = |\\{h,a,b\\}| = 3$\n-   $k_h = |\\Gamma(h)| = |\\{x\\} \\cup \\{f_1, \\dots, f_{50}\\}| = 1 + 50 = 51$\n-   $k_a = |\\Gamma(a)| = |\\{x,u,v\\}| = 3$\n-   $k_b = |\\Gamma(b)| = |\\{x,u,w\\}| = 3$\n-   $k_u = |\\Gamma(u)| = |\\{a,b\\}| = 2$\n-   $k_v = |\\Gamma(v)| = |\\{a\\}| = 1$\n-   $k_w = |\\Gamma(w)| = |\\{b\\}| = 1$\n-   $k_{f_i} = |\\Gamma(f_i)| = |\\{h\\}| = 1$\n\nNow, for each candidate non-edge, we identify the set of common neighbors:\n-   For $(x,u)$: $\\Gamma(x) \\cap \\Gamma(u) = \\{h,a,b\\} \\cap \\{a,b\\} = \\{a,b\\}$\n-   For $(x,v)$: $\\Gamma(x) \\cap \\Gamma(v) = \\{h,a,b\\} \\cap \\{a\\} = \\{a\\}$\n-   For $(x,w)$: $\\Gamma(x) \\cap \\Gamma(w) = \\{h,a,b\\} \\cap \\{b\\} = \\{b\\}$\n-   For $(x,f_i)$: $\\Gamma(x) \\cap \\Gamma(f_i) = \\{h,a,b\\} \\cap \\{h\\} = \\{h\\}$\n\nWith these, we calculate the scores.\n\n**Common Neighbors (CN) Scores:**\n-   $s_{CN}(x,u) = |\\{a,b\\}| = 2$\n-   $s_{CN}(x,v) = |\\{a\\}| = 1$\n-   $s_{CN}(x,w) = |\\{b\\}| = 1$\n-   $s_{CN}(x,f_i) = |\\{h\\}| = 1$\nThe ordering is $s_{CN}(x,u)  s_{CN}(x,v) = s_{CN}(x,w) = s_{CN}(x,f_i)$.\nThis result demonstrates the susceptibility of the CN metric. The link to any of the $50$ fake leaf nodes, $(x, f_i)$, is scored as high as the more \"natural\" potential links $(x,v)$ and $(x,w)$, simply because they each have one common neighbor with $x$. The metric fails to distinguish the quality of the common neighbor.\n\n**Adamic-Adar (AA) Scores:**\n-   $s_{AA}(x,u) = \\frac{1}{\\log k_a} + \\frac{1}{\\log k_b} = \\frac{1}{\\log 3} + \\frac{1}{\\log 3} = \\frac{2}{\\log 3}$\n-   $s_{AA}(x,v) = \\frac{1}{\\log k_a} = \\frac{1}{\\log 3}$\n-   $s_{AA}(x,w) = \\frac{1}{\\log k_b} = \\frac{1}{\\log 3}$\n-   $s_{AA}(x,f_i) = \\frac{1}{\\log k_h} = \\frac{1}{\\log 51}$\nTo compare these values, we note that the natural logarithm function $\\log(z)$ is monotonically increasing for $z1$. Thus, $51  3 \\implies \\log 51  \\log 3$. This in turn implies $\\frac{1}{\\log 51}  \\frac{1}{\\log 3}$.\nThe ordering is $\\frac{2}{\\log 3}  \\frac{1}{\\log 3}  \\frac{1}{\\log 51}$, which gives:\n$s_{AA}(x,u)  s_{AA}(x,v) = s_{AA}(x,w)  s_{AA}(x,f_i)$.\n\n**Resource Allocation (RA) Scores:**\n-   $s_{RA}(x,u) = \\frac{1}{k_a} + \\frac{1}{k_b} = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}$\n-   $s_{RA}(x,v) = \\frac{1}{k_a} = \\frac{1}{3}$\n-   $s_{RA}(x,w) = \\frac{1}{k_b} = \\frac{1}{3}$\n-   $s_{RA}(x,f_i) = \\frac{1}{k_h} = \\frac{1}{51}$\nSince $51  3$, it follows that $\\frac{1}{51}  \\frac{1}{3}$.\nThe ordering is $\\frac{2}{3}  \\frac{1}{3}  \\frac{1}{51}$, which gives:\n$s_{RA}(x,u)  s_{RA}(x,v) = s_{RA}(x,w)  s_{RA}(x,f_i)$.\n\nBoth AA and RA successfully mitigate the adversarial attack by heavily down-weighting the contribution of the high-degree hub $h$, resulting in a very low score for the links $(x,f_i)$ involving the fake nodes.\n\n### Option-by-Option Analysis\n\n**A. Under $CN$, one has $s_{CN}(x,u)s_{CN}(x,v)=s_{CN}(x,w)=s_{CN}(x,f_i)$ for any $i$. Under Adamic-Adar (AA) and Resource Allocation (RA), one has $s_{AA}(x,u)s_{AA}(x,v)=s_{AA}(x,w)s_{AA}(x,f_i)$ and $s_{RA}(x,u)s_{RA}(x,v)=s_{RA}(x,w)s_{RA}(x,f_i)$, because the hub’s inflated degree down-weights its contribution. Thus, the adversarial hub floods $CN$ with many ties via $\\{f_i\\}$, but $AA$ and $RA$ reduce susceptibility. This qualitative ordering under $AA$ and $RA$ remains unchanged even if $10\\%$ of $h$’s incident edges are missing.**\n\n-   The statement for CN is $s_{CN}(x,u)  s_{CN}(x,v)=s_{CN}(x,w)=s_{CN}(x,f_i)$. Our calculation gives $2  1=1=1$, which is correct.\n-   The statement for AA is $s_{AA}(x,u)s_{AA}(x,v)=s_{AA}(x,w)s_{AA}(x,f_i)$. Our calculation gives $\\frac{2}{\\log 3}  \\frac{1}{\\log 3}  \\frac{1}{\\log 51}$, which is correct.\n-   The statement for RA is $s_{RA}(x,u)s_{RA}(x,v)=s_{RA}(x,w)s_{RA}(x,f_i)$. Our calculation gives $\\frac{2}{3}  \\frac{1}{3}  \\frac{1}{51}$, which is correct.\n-   The reasoning that the hub's inflated degree down-weights its contribution is correct for AA and RA.\n-   The interpretation about CN's susceptibility and AA/RA's mitigation is also correct.\n-   The final claim about stability: If $10\\%$ of $h$'s edges are missing, its degree becomes $k'_h$. For example, removing $5$ edges to fake nodes results in $k'_h = 51 - 5 = 46$. As long as $k'_h  k_a = 3$ and $k'_h  k_b = 3$, the qualitative ordering for AA ($s(x,v)  s(x,f_i)$ because $\\frac{1}{\\log 3}  \\frac{1}{\\log 46}$) and RA ($s(x,v)  s(x,f_i)$ because $\\frac{1}{3}  \\frac{1}{46}$) will be preserved. Reducing $k_h$ by $10\\%$ still leaves it much larger than $3$. Thus, this claim is also correct.\n-   **Verdict: Correct.**\n\n**B. Under $CN$, $s_{CN}(x,f_i)s_{CN}(x,u)$ because the hub’s degree is higher. Under $AA$, inflating $k_h$ further increases $s_{AA}(x,f_i)$, and under $RA$ the hub contributes zero, so $s_{RA}(x,f_i)=0$.**\n\n-   The first claim is $s_{CN}(x,f_i)s_{CN}(x,u)$. Our calculation is $1  2$, which is false. The reasoning \"because the hub's degree is higher\" is irrelevant to the CN score.\n-   The second claim is that inflating $k_h$ increases $s_{AA}(x,f_i) = \\frac{1}{\\log k_h}$. As $k_h$ increases, $\\log k_h$ increases, and its reciprocal $\\frac{1}{\\log k_h}$ *decreases*. This claim is false.\n-   The third claim is that $s_{RA}(x,f_i)=0$. Our calculation is $s_{RA}(x,f_i) = \\frac{1}{k_h} = \\frac{1}{51} \\neq 0$. This claim is false.\n-   **Verdict: Incorrect.**\n\n**C. Under $CN$, $s_{CN}(x,u)=s_{CN}(x,v)=s_{CN}(x,w)=s_{CN}(x,f_i)$. Under $AA$, $s_{AA}(x,f_i)s_{AA}(x,v)$ because $1/\\log k_h$ grows with $k_h$. Under $RA$, the scores are invariant to the degree of common neighbors.**\n\n-   The first claim is $s_{CN}(x,u)=s_{CN}(x,v)=s_{CN}(x,w)=s_{CN}(x,f_i)$. Our calculation gives $2=1=1=1$, which is false.\n-   The second claim is $s_{AA}(x,f_i)s_{AA}(x,v)$. Our calculation gives $\\frac{1}{\\log 51}  \\frac{1}{\\log 3}$, which is false. The reasoning \"$1/\\log k_h$ grows with $k_h$\" is also false.\n-   The third claim is that RA scores are invariant to the degree of common neighbors. The RA formula is $\\sum \\frac{1}{k_z}$, which explicitly depends on the degree $k_z$ of common neighbors. This claim is false.\n-   **Verdict: Incorrect.**\n\n**D. Under $CN$, $s_{CN}(x,u)=s_{CN}(x,v)s_{CN}(x,w)=s_{CN}(x,f_i)$. Under $AA$ and $RA$, one has $s_{AA}(x,f_i)=s_{AA}(x,v)$ and $s_{RA}(x,f_i)=s_{RA}(x,v)$ due to fairness considerations and missing data, so the adversarial hub does not affect relative ordering.**\n\n-   The first claim is $s_{CN}(x,u)=s_{CN}(x,v)s_{CN}(x,w)=s_{CN}(x,f_i)$. Our calculation shows $s_{CN}(x,u)=2$ and $s_{CN}(x,v)=1$. The equality $2=1$ is false.\n-   The second claim is $s_{AA}(x,f_i)=s_{AA}(x,v)$. Our calculation gives $\\frac{1}{\\log 51} = \\frac{1}{\\log 3}$, which is false.\n-   The third claim is $s_{RA}(x,f_i)=s_{RA}(x,v)$. Our calculation gives $\\frac{1}{51} = \\frac{1}{3}$, which is false. The reasoning \"due to fairness considerations and missing data\" is unfounded and irrelevant to the metric definitions.\n-   **Verdict: Incorrect.**\n\nBased on the rigorous derivation and option-by-option analysis, only option A is correct.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While understanding the theory of similarity metrics is essential, applying them to massive, real-world networks requires careful consideration of computational efficiency. This practice shifts the focus to implementation and scalability, challenging you to analyze the algorithmic complexity of two distinct methods for computing all-pairs Common Neighbors scores. You will learn how network structure itself dictates the most efficient computational strategy, bridging the gap between theoretical concepts and practical application .",
            "id": "4302829",
            "problem": "Consider an undirected, simple graph with $n$ nodes and $m$ edges, represented both as adjacency lists and as a sparse adjacency matrix. The Common Neighbors (CN) similarity for a node pair $(i,j)$ is defined as the cardinality of the set of nodes that are adjacent to both $i$ and $j$. Assume all adjacency lists are stored sorted and each list intersection is performed using a two-pointer scan that compares elements until both lists are exhausted. Consider the task of computing CN for all unordered node pairs $(i,j)$ with $1 \\leq i  j \\leq n$ by intersecting the adjacency lists of $i$ and $j$.\n\nSeparately, consider computing the full all-pairs CN by forming the square of the sparse adjacency matrix, that is, computing the product of the adjacency matrix with itself, using a standard Sparse General Matrix-Matrix multiplication (SpGEMM) method implemented over Compressed Sparse Row (CSR) and Compressed Sparse Column (CSC) data structures. In this method, each row of the product matrix is formed by summing the adjacency rows of its neighbors, merging their sparse index lists to accumulate counts; assume the unit work model in which each adjacency entry visited and merged contributes one primitive operation, and hashing or duplicate suppression costs are negligible compared to list merging.\n\nLet the degree sequence of the graph be $(d_1, d_2, \\dots, d_n)$, with $\\sum_{i=1}^{n} d_i = 2m$. Under the above assumptions, and starting from first principles, derive expressions for the total primitive-operation counts of the two approaches. Then, provide the leading-order analytic expression for the ratio of the all-pairs CN computation cost via adjacency list intersections to the cost via SpGEMM, expressed solely in terms of $n$, $m$, and $\\sum_{i=1}^{n} d_i^2$. Your final answer must be a single closed-form analytic expression with no units. No numerical approximation or rounding is required.",
            "solution": "We begin by restating the definitions and modeling assumptions precisely. For an undirected, simple graph with $n$ nodes and $m$ edges, the Common Neighbors (CN) similarity between nodes $i$ and $j$ is the number of nodes that are adjacent to both $i$ and $j$. Equivalently, it is the cardinality of the intersection of the adjacency lists of $i$ and $j$. The adjacency lists are sorted, and an intersection is computed using a two-pointer scan, which advances pointers in both lists and compares entries until one list is exhausted; the number of comparisons performed in such a scan is proportional to the sum of the lengths of the two lists.\n\nWe analyze two approaches to compute CN for all unordered node pairs $(i,j)$ with $1 \\leq i  j \\leq n$.\n\nApproach 1: All-pairs adjacency list intersections. For each unordered pair $(i,j)$, we intersect the adjacency lists of $i$ and $j$. If the degree of node $i$ is $d_i$ and that of node $j$ is $d_j$, a two-pointer intersection performs on the order of $d_i + d_j$ comparisons. Thus, the total primitive-operation count $T_{\\text{intersect}}$ across all pairs is\n$$\nT_{\\text{intersect}} = \\sum_{1 \\leq i  j \\leq n} \\left(d_i + d_j\\right).\n$$\nWe simplify this sum. Observe that in the double sum, each $d_i$ appears once for every pair that includes node $i$, and each node participates in exactly $n-1$ unordered pairs. Therefore,\n$$\n\\sum_{1 \\leq i  j \\leq n} \\left(d_i + d_j\\right)\n= \\sum_{i=1}^{n} d_i \\cdot (n-1).\n$$\nSince $\\sum_{i=1}^{n} d_i = 2m$, we obtain\n$$\nT_{\\text{intersect}} = (n-1) \\sum_{i=1}^{n} d_i = 2m(n-1).\n$$\n\nApproach 2: Sparse General Matrix-Matrix multiplication (SpGEMM) to compute the square of the adjacency matrix. Let $A$ be the $n \\times n$ sparse adjacency matrix. The CN value for $(i,j)$ equals the $(i,j)$ entry of $A^{2}$, because\n$$\n\\left(A^{2}\\right)_{ij} = \\sum_{k=1}^{n} A_{ik} A_{kj},\n$$\nand $A_{ik} A_{kj} = 1$ precisely when node $k$ is adjacent to both $i$ and $j$, contributing one common neighbor. In a standard CSR/CSC-based SpGEMM, the $i$-th row of $A^{2}$ is formed by summing the adjacency rows of all neighbors of $i$. For each neighbor $k \\in \\Gamma(i)$, we merge the sparse row corresponding to $k$ (whose length is $d_k$) into the accumulator for row $i$. Under the unit work model in which each adjacency entry visited in a merge contributes one primitive operation, the cost of forming row $i$ is\n$$\n\\sum_{k \\in \\Gamma(i)} d_k.\n$$\nTherefore, the total primitive-operation count $T_{\\text{spgemm}}$ across all rows is\n$$\nT_{\\text{spgemm}} = \\sum_{i=1}^{n} \\sum_{k \\in \\Gamma(i)} d_k.\n$$\nWe can rewrite this double sum by recognizing that each neighbor relationship $(i,k)$ contributes $d_k$ once for each $i \\in \\Gamma(k)$, and the number of neighbors of $k$ is $d_k$. Hence,\n$$\nT_{\\text{spgemm}} = \\sum_{k=1}^{n} d_k \\cdot d_k = \\sum_{i=1}^{n} d_i^2.\n$$\n\nWe now compare the two approaches by forming the ratio of their total primitive-operation counts. Using the expressions derived above,\n$$\n\\frac{T_{\\text{intersect}}}{T_{\\text{spgemm}}}\n= \\frac{2m(n-1)}{\\sum_{i=1}^{n} d_i^2}.\n$$\nThis ratio is the leading-order analytic expression under the stated assumptions. It is expressed solely in terms of $n$, $m$, and $\\sum_{i=1}^{n} d_i^2$, as required. For large $n$, one may treat $(n-1)$ as asymptotically equivalent to $n$, but the exact expression remains $\\frac{2m(n-1)}{\\sum_{i=1}^{n} d_i^2}$.\n\nThus, the requested ratio is\n$$\n\\frac{2 m (n-1)}{\\sum_{i=1}^{n} d_i^2}.\n$$",
            "answer": "$$\\boxed{\\frac{2 m (n-1)}{\\sum_{i=1}^{n} d_i^2}}$$"
        }
    ]
}