{
    "hands_on_practices": [
        {
            "introduction": "A foundational approach to network embedding views the task as a low-rank approximation of the network's adjacency matrix. This exercise provides direct, hands-on experience with this principle by using Singular Value Decomposition (SVD), a cornerstone of linear algebra, to create node embeddings. By factorizing the adjacency matrix into source and target vectors, you will build a low-dimensional model of the network and evaluate its ability to reconstruct the original connections, offering a clear illustration of how abstract vector representations relate to concrete network topology. ",
            "id": "4290602",
            "problem": "Consider a directed network with $n$ nodes represented by a real adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij} \\geq 0$ encodes the presence and strength of a directed edge from node $i$ to node $j$. A network embedding seeks to represent each node $i$ by a source vector $x_i \\in \\mathbb{R}^d$ and a target vector $y_i \\in \\mathbb{R}^d$ such that the interaction score for a directed edge $\\{i \\rightarrow j\\}$ is approximated by the inner product $x_i^\\top y_j$. For a given dimension $d$, the reconstructed adjacency is $\\hat{A} = X Y^\\top$, where $X \\in \\mathbb{R}^{n \\times d}$ stacks source vectors row-wise and $Y \\in \\mathbb{R}^{n \\times d}$ stacks target vectors row-wise.\n\nStart from fundamental definitions of linear algebra and matrix decompositions. The Singular Value Decomposition (SVD) of a real matrix $A$ exists and is written $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with nonnegative singular values in nonincreasing order on its diagonal. The best rank-$k$ approximation (in the sense of minimizing the Frobenius norm of the error) is obtained by truncating the SVD to the top $k$ singular values and associated singular vectors.\n\nYour task is to:\n- Construct $d = 2$ dimensional source and target embeddings for each node by using a principled factorization derived from the truncated SVD of $A$. Use this construction to form $X \\in \\mathbb{R}^{n \\times 2}$ and $Y \\in \\mathbb{R}^{n \\times 2}$, and then compute the rank-$2$ reconstruction $\\hat{A} = X Y^\\top$.\n- Define the observed edge set as $\\mathcal{E}_{\\mathrm{obs}} = \\{(i,j) \\mid A_{ij}  0\\}$. Compute the mean squared reconstruction error on observed edges, given by\n$$\\mathrm{MSE}_{\\mathrm{obs}} = \\begin{cases}\n\\frac{1}{|\\mathcal{E}_{\\mathrm{obs}}|} \\sum_{(i,j) \\in \\mathcal{E}_{\\mathrm{obs}}} \\left(A_{ij} - \\hat{A}_{ij}\\right)^2,  \\text{if } |\\mathcal{E}_{\\mathrm{obs}}|  0, \\\\\n0,  \\text{if } |\\mathcal{E}_{\\mathrm{obs}}| = 0.\n\\end{cases}$$\nNo clipping or post-processing should be applied to $\\hat{A}$; calculate the error directly from the reconstructed values.\n\nImplement a program that performs this computation for the following test suite of adjacency matrices (all entries are real and nonnegative), each specified as a list of lists. In each case, let $n$ equal the number of rows (and columns) in the matrix:\n1. Directed $4$-cycle ($n = 4$): edges $\\{1 \\rightarrow 2, 2 \\rightarrow 3, 3 \\rightarrow 4, 4 \\rightarrow 1\\}$ with unit weights, represented by\n$A^{(1)} = \\begin{bmatrix}\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n1  0  0  0\n\\end{bmatrix}.$\n2. Small bidirectional core ($n = 3$): edges $\\{1 \\leftrightarrow 2\\}$ and $\\{2 \\leftrightarrow 3\\}$ with unit weights, represented by\n$A^{(2)} = \\begin{bmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{bmatrix}.$\n3. Empty graph ($n = 3$): no edges, represented by\n$A^{(3)} = \\begin{bmatrix}\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{bmatrix}.$\n4. Complete directed graph without self-loops ($n = 3$): all off-diagonal entries are $1$, represented by\n$A^{(4)} = \\begin{bmatrix}\n0  1  1 \\\\\n1  0  1 \\\\\n1  1  0\n\\end{bmatrix}.$\n5. Weighted directed network ($n = 4$): asymmetric weights, represented by\n$A^{(5)} = \\begin{bmatrix}\n0  2.0  0  1.5 \\\\\n0  0  0  0 \\\\\n0.5  0  0  1.0 \\\\\n0  0.25  1.0  0\n\\end{bmatrix}.$\n\nYour program must:\n- For each $A^{(t)}$ in the test suite, compute $X$ and $Y$ using the truncated SVD to dimension $d = 2$ and reconstruct $\\hat{A}$ as described.\n- Compute $\\mathrm{MSE}_{\\mathrm{obs}}$ for each test case.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite: $[r^{(1)}, r^{(2)}, r^{(3)}, r^{(4)}, r^{(5)}]$, where each $r^{(t)}$ is the computed $\\mathrm{MSE}_{\\mathrm{obs}}$ for test case $t$ as a floating-point number.\n\nNo physical units are involved. Angles are not involved. Percentages are not involved. The output format must follow the specification exactly. The program must be entirely self-contained and must not read input or write anything other than the final line.",
            "solution": "The problem requires us to compute network embeddings and evaluate their reconstruction error using a method based on the Singular Value Decomposition (SVD). The core task is to find a low-dimensional representation for the nodes of a directed network, which can be framed as a low-rank matrix factorization problem.\n\n### 1. Principle: Low-Rank Approximation via SVD\n\nA directed network with $n$ nodes is given by its adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij} \\geq 0$ represents the weight of the edge from node $i$ to node $j$. The goal is to find two sets of low-dimensional vectors, source vectors $\\{x_i\\}_{i=1}^n$ and target vectors $\\{y_i\\}_{i=1}^n$, with $x_i, y_i \\in \\mathbb{R}^d$, such that their inner product $x_i^\\top y_j$ approximates the original edge weight $A_{ij}$.\n\nThis can be expressed in matrix form. Let $X \\in \\mathbb{R}^{n \\times d}$ be the matrix whose rows are the source vectors $x_i^\\top$, and let $Y \\in \\mathbb{R}^{n \\times d}$ be the matrix whose rows are the target vectors $y_i^\\top$. The reconstructed adjacency matrix $\\hat{A}$ is then given by:\n$$ \\hat{A} = X Y^\\top $$\nBy construction, $\\hat{A}$ has a rank of at most $d$. The problem is thus to find the best rank-$d$ approximation of $A$.\n\nThe Singular Value Decomposition (SVD) of the matrix $A$ provides the theoretical foundation for this task. Any real matrix $A \\in \\mathbb{R}^{n \\times n}$ has an SVD of the form:\n$$ A = U \\Sigma V^\\top $$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices ($U^\\top U = I$, $V^\\top V = I$), and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix containing the singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_n \\geq 0$. The columns of $U$ are the left-singular vectors, and the columns of $V$ are the right-singular vectors.\n\nAccording to the Eckart-Young-Mirsky theorem, the best rank-$d$ approximation of $A$ (in the sense of minimizing the Frobenius norm of the difference $A - \\hat{A}$) is obtained by truncating the SVD. This is achieved by keeping only the top $d$ singular values and their corresponding singular vectors:\n$$ \\hat{A}_d = U_d \\Sigma_d V_d^\\top $$\nHere, $U_d \\in \\mathbb{R}^{n \\times d}$ contains the first $d$ columns of $U$, $\\Sigma_d \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix with the top $d$ singular values $\\{\\sigma_1, \\dots, \\sigma_d\\}$, and $V_d \\in \\mathbb{R}^{n \\times d}$ contains the first $d$ columns of $V$.\n\n### 2. Constructing Embeddings from Truncated SVD\n\nThe problem specifies that the dimension of the embedding is $d=2$, so we need to construct the rank-$2$ approximation $\\hat{A}_2 = U_2 \\Sigma_2 V_2^\\top$. To satisfy the embedding structure $\\hat{A} = XY^\\top$, we must define $X$ and $Y$ in terms of the SVD components. A \"principled factorization,\" as requested, is one that is well-motivated. A standard and balanced choice is to distribute the \"energy\" of the singular values between the source and target embeddings. We define $\\Sigma_d^{1/2}$ as the diagonal matrix with entries $\\sqrt{\\sigma_i}$ for $i=1, \\dots, d$. We can then set:\n$$ X = U_d \\Sigma_d^{1/2} $$\n$$ Y = V_d \\Sigma_d^{1/2} $$\nWith this choice, the reconstruction is indeed the optimal rank-$d$ approximation:\n$$ XY^\\top = (U_d \\Sigma_d^{1/2}) (V_d \\Sigma_d^{1/2})^\\top = U_d \\Sigma_d^{1/2} (\\Sigma_d^{1/2})^\\top V_d^\\top = U_d \\Sigma_d V_d^\\top = \\hat{A}_d $$\nFor this problem, with $d=2$, we will use $X = U_2 \\Sigma_2^{1/2}$ and $Y = V_2 \\Sigma_2^{1/2}$.\n\nIt is important to note that if there are repeated singular values (e.g., $\\sigma_d = \\sigma_{d+1}$), the singular vectors are not unique, and thus the best rank-$d$ approximation is not unique. Several test cases ($A^{(1)}$, $A^{(2)}$, $A^{(4)}$) exhibit this property. In such scenarios, a deterministic numerical algorithm for SVD will select a specific valid basis for the singular vectors, leading to a specific, computationally reproducible result. Our solution relies on this deterministic outcome.\n\n### 3. Error Calculation\n\nThe quality of the reconstruction is measured by the Mean Squared Error over the set of observed edges, $\\mathcal{E}_{\\mathrm{obs}} = \\{(i,j) \\mid A_{ij}  0\\}$. The formula is:\n$$ \\mathrm{MSE}_{\\mathrm{obs}} = \\begin{cases} \\frac{1}{|\\mathcal{E}_{\\mathrm{obs}}|} \\sum_{(i,j) \\in \\mathcal{E}_{\\mathrm{obs}}} \\left(A_{ij} - \\hat{A}_{ij}\\right)^2,  \\text{if } |\\mathcal{E}_{\\mathrm{obs}}|  0 \\\\ 0,  \\text{if } |\\mathcal{E}_{\\mathrm{obs}}| = 0 \\end{cases} $$\nThis metric focuses the evaluation on how well the embedding model reproduces the connections that are actually present in the network.\n\n### 4. Algorithm for Implementation\n\nFor each given adjacency matrix $A^{(t)}$:\n1.  Convert the input list of lists into a NumPy array $A$.\n2.  Identify the observed edges by creating a boolean mask where $A  0$. Count the number of observed edges, $|\\mathcal{E}_{\\mathrm{obs}}|$.\n3.  If $|\\mathcal{E}_{\\mathrm{obs}}| = 0$ (as in the empty graph case $A^{(3)}$), the result is $0.0$.\n4.  Otherwise, compute the SVD of $A$: $u, s, vh = \\text{svd}(A)$, where $s$ contains the singular values and $vh$ is $V^\\top$.\n5.  Set the embedding dimension $d=2$.\n6.  Form the truncated SVD components:\n    -   $U_2$ consists of the first $2$ columns of $u$.\n    -   $\\Sigma_2^{1/2}$ is formed from the square roots of the first $2$ singular values in $s$.\n    -   $V_2$ is formed from the first $2$ columns of $vh^\\top$.\n7.  Construct the embedding matrices $X \\in \\mathbb{R}^{n \\times 2}$ and $Y \\in \\mathbb{R}^{n \\times 2}$:\n    -   $X = U_2 \\Sigma_2^{1/2}$\n    -   $Y = V_2 \\Sigma_2^{1/2}$\n8.  Compute the rank-$2$ reconstructed matrix $\\hat{A} = XY^\\top$.\n9.  Calculate the squared errors $(A_{ij} - \\hat{A}_{ij})^2$ only for the observed edges $(i,j) \\in \\mathcal{E}_{\\mathrm{obs}}$.\n10. Compute the mean of these squared errors to obtain $\\mathrm{MSE}_{\\mathrm{obs}}$.\n11. Store this value and repeat for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes network embeddings using truncated SVD and calculates the \n    mean squared reconstruction error on observed edges.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Directed 4-cycle\n        [[0, 1, 0, 0],\n         [0, 0, 1, 0],\n         [0, 0, 0, 1],\n         [1, 0, 0, 0]],\n        \n        # 2. Small bidirectional core\n        [[0, 1, 0],\n         [1, 0, 1],\n         [0, 1, 0]],\n        \n        # 3. Empty graph\n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n        \n        # 4. Complete directed graph without self-loops\n        [[0, 1, 1],\n         [1, 0, 1],\n         [1, 1, 0]],\n        \n        # 5. Weighted directed network\n        [[0, 2.0, 0, 1.5],\n         [0, 0, 0, 0],\n         [0.5, 0, 0, 1.0],\n         [0, 0.25, 1.0, 0]]\n    ]\n\n    results = []\n    d = 2 # Embedding dimension\n\n    for case_matrix in test_cases:\n        A = np.array(case_matrix, dtype=float)\n        \n        # Define the observed edge set as a boolean mask.\n        obs_mask = A  0\n        num_obs_edges = np.sum(obs_mask)\n\n        # Handle the case of no observed edges.\n        if num_obs_edges == 0:\n            results.append(0.0)\n            continue\n            \n        # SVD is only needed if there are edges to reconstruct.\n        # Compute the SVD of the adjacency matrix.\n        # u: left-singular vectors, s: singular values, vh: right-singular vectors (transposed)\n        u, s, vh = np.linalg.svd(A)\n\n        # Truncate to dimension d.\n        # Pad singular values with zeros if rank(A)  d\n        s_d = np.zeros(d)\n        num_singular_values = len(s)\n        if num_singular_values  0:\n            len_to_copy = min(d, num_singular_values)\n            s_d[:len_to_copy] = s[:len_to_copy]\n        \n        U_d = u[:, :d]\n        Vh_d = vh[:d, :]\n        V_d = Vh_d.T\n\n        # Construct embeddings X and Y based on the principled factorization.\n        s_d_sqrt = np.sqrt(s_d)\n        \n        # Broadcasting s_d_sqrt to the columns of U_d and V_d\n        X = U_d * s_d_sqrt \n        Y = V_d * s_d_sqrt\n        \n        # Compute the rank-d reconstructed adjacency matrix.\n        A_hat = X @ Y.T\n\n        # Calculate the mean squared error on observed edges.\n        squared_errors = (A[obs_mask] - A_hat[obs_mask])**2\n        mse_obs = np.mean(squared_errors)\n        \n        results.append(mse_obs)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While basic embeddings capture direct connections, real-world networks possess rich higher-order structures, such as triangles, which are crucial for community formation. This practice moves beyond simple adjacency to explore Laplacian Eigenmaps, an embedding technique that emphasizes the local neighborhood structure. You will learn to explicitly enhance the preservation of triangles by modifying the weights used in the embedding process, and then evaluate how this structural reinforcement improves the recovery of the graph's clustering properties. ",
            "id": "4290676",
            "problem": "Consider an undirected, unweighted, connected graph with $n$ nodes and adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ defined as follows. Let the node set be $\\{0,1,2,3,4,5,6\\}$ and the edge set be $\\{(0,1),(1,2),(2,0),(2,3),(3,4),(4,5),(5,3),(4,6),(5,6)\\}$. The adjacency matrix $A$ (with symmetric entries and zeros on the diagonal) is\n$$\nA = \\begin{pmatrix}\n0  1  1  0  0  0  0 \\\\\n1  0  1  0  0  0  0 \\\\\n1  1  0  1  0  0  0 \\\\\n0  0  1  0  1  1  0 \\\\\n0  0  0  1  0  1  1 \\\\\n0  0  0  1  1  0  1 \\\\\n0  0  0  0  1  1  0\n\\end{pmatrix}.\n$$\nThis graph has triangles at nodes $\\{0,1,2\\}$, $\\{3,4,5\\}$, and $\\{4,5,6\\}$, connected through the bridge edge $(2,3)$, ensuring the graph is connected.\n\nYou will compute two embeddings in $\\mathbb{R}^2$ based on the following principles and definitions, and then reconstruct graphs from the embeddings to evaluate the recovery of clustering properties.\n\nFundamental base:\n- Graph Laplacian (GL): For a symmetric weight matrix $W \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries and zero diagonal, define the degree matrix $D = \\mathrm{diag}(W\\mathbf{1})$, and the combinatorial Laplacian $L = D - W$.\n- Dirichlet energy of an embedding $X \\in \\mathbb{R}^{n \\times d}$: $E_W(X) = \\sum_{i,j} W_{ij} \\lVert x_i - x_j \\rVert_2^2$, where $x_i \\in \\mathbb{R}^d$ is the row of $X$ for node $i$.\n- Laplacian Eigenmaps (LE): Laplacian Eigenmaps (LE) find low-dimensional coordinates by minimizing $E_W(X)$ subject to orthogonality and centering constraints relative to $D$.\n- Triangle-preserving edge weights: For an undirected simple graph with adjacency $A$, the number of triangles supported by edge $(i,j)$ equals the number of common neighbors of $i$ and $j$, which is $\\sum_k A_{ik}A_{kj}$. Define the triangle edge-count matrix $T^{\\mathrm{edge}} = A \\circ (A^2)$, where $\\circ$ denotes Hadamard (element-wise) product and $A^2$ is the square of $A$ under matrix multiplication.\n\nDerived constructions to use:\n- Baseline embedding: Use $W^{(0)} = A$.\n- Triangle-preserving embedding: For a given $\\lambda \\ge 0$, use $W^{(\\lambda)} = A + \\lambda \\, T^{\\mathrm{edge}}$, with zero diagonal and symmetry enforced.\n- Compute $L^{(0)} = D^{(0)} - W^{(0)}$ and $L^{(\\lambda)} = D^{(\\lambda)} - W^{(\\lambda)}$. Obtain $\\mathbb{R}^2$ embeddings $X^{(0)}$ and $X^{(\\lambda)}$ by solving a constrained quadratic energy minimization consistent with Laplacian Eigenmaps (LE), taking the two nontrivial eigenvectors associated with the smallest generalized eigenvalues of $L v = \\mu D v$.\n\nGraph reconstruction from embedding:\n- Convert pairwise Euclidean distances $\\delta_{ij} = \\lVert x_i - x_j \\rVert_2$ into affinities using a Gaussian kernel $K_{ij} = \\exp\\left(-\\frac{\\delta_{ij}^2}{2\\sigma^2}\\right)$, with $\\sigma$ chosen as the median of $\\{\\delta_{ij}: ij\\}$.\n- Given a threshold parameter $\\tau \\in [0,1]$, reconstruct a symmetric adjacency $\\widehat{A}$ by setting $\\widehat{A}_{ij} = 1$ if $i \\ne j$ and $K_{ij} \\ge \\tau$, and $\\widehat{A}_{ij} = 0$ otherwise. Ensure zero diagonal.\n\nClustering metrics to quantify recovery:\n- Global transitivity (also called global clustering coefficient): Let the number of triangles be $N_\\triangle = \\frac{1}{6}\\mathrm{trace}(A^3)$ and the number of connected triplets be $N_{\\wedge} = \\sum_{i} \\binom{d_i}{2}$ where $d_i$ is the degree of node $i$. Define global transitivity as $C_{\\mathrm{glob}} = \\frac{3N_\\triangle}{N_{\\wedge}}$ whenever $N_{\\wedge}  0$, and $C_{\\mathrm{glob}} = 0$ when $N_{\\wedge} = 0$.\n- Average local clustering coefficient (local transitivity): For node $i$ with $d_i \\ge 2$, let $N_{\\triangle}(i)$ be the number of edges among its neighbors, equivalently the number of triangles incident to $i$. The local clustering coefficient is $C_i = \\frac{N_{\\triangle}(i)}{\\binom{d_i}{2}}$. Define $C_{\\mathrm{loc}} = \\frac{1}{|\\{i: d_i \\ge 2\\}|}\\sum_{i: d_i \\ge 2} C_i$.\n\nFor each parameter pair $(\\lambda,\\tau)$, you must:\n- Compute $X^{(0)}$ and $X^{(\\lambda)}$ as described.\n- Reconstruct $\\widehat{A}^{(0)}$ and $\\widehat{A}^{(\\lambda)}$ using $\\tau$ and the Gaussian kernel with $\\sigma$ equal to the median pairwise distance computed from the corresponding embedding.\n- Compute $C_{\\mathrm{glob}}$ and $C_{\\mathrm{loc}}$ for the original $A$, and for each reconstructed adjacency $\\widehat{A}^{(0)}$ and $\\widehat{A}^{(\\lambda)}$.\n- Report the absolute errors $|C_{\\mathrm{glob}}(\\widehat{A}^{(0)}) - C_{\\mathrm{glob}}(A)|$, $|C_{\\mathrm{glob}}(\\widehat{A}^{(\\lambda)}) - C_{\\mathrm{glob}}(A)|$, and their improvement defined as the baseline error minus the triangle-preserving error. Similarly report $|C_{\\mathrm{loc}}(\\widehat{A}^{(0)}) - C_{\\mathrm{loc}}(A)|$, $|C_{\\mathrm{loc}}(\\widehat{A}^{(\\lambda)}) - C_{\\mathrm{loc}}(A)|$, and the corresponding improvement.\n\nAngle units are not applicable and there are no physical units. All outputs should be real numbers.\n\nTest suite:\n- Case $1$: $(\\lambda,\\tau) = (0.0, 0.6)$, a boundary case where triangle-preserving reduces to baseline.\n- Case $2$: $(\\lambda,\\tau) = (0.8, 0.6)$, a typical case with moderate triangle reinforcement.\n- Case $3$: $(\\lambda,\\tau) = (2.0, 0.75)$, a stronger triangle preservation with a stricter threshold.\n- Case $4$: $(\\lambda,\\tau) = (1.0, 0.9)$, a high threshold testing sparsity in reconstruction.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result for each test case must be a list of $6$ floats in the order\n$$\n[\\;e^{(0)}_{\\mathrm{glob}},\\; e^{(\\lambda)}_{\\mathrm{glob}},\\; \\Delta e_{\\mathrm{glob}},\\; e^{(0)}_{\\mathrm{loc}},\\; e^{(\\lambda)}_{\\mathrm{loc}},\\; \\Delta e_{\\mathrm{loc}}\\;],\n$$\nwhere $e^{(0)}_{\\mathrm{glob}}$ is the baseline absolute error in global transitivity, $e^{(\\lambda)}_{\\mathrm{glob}}$ is the triangle-preserving absolute error in global transitivity, $\\Delta e_{\\mathrm{glob}} = e^{(0)}_{\\mathrm{glob}} - e^{(\\lambda)}_{\\mathrm{glob}}$ is its improvement, and similarly for the local average. Aggregate the four case results into a single list, for example\n$$\n[[a_1,a_2,a_3,a_4,a_5,a_6],[b_1,b_2,b_3,b_4,b_5,b_6],[c_1,c_2,c_3,c_4,c_5,c_6],[d_1,d_2,d_3,d_4,d_5,d_6]].\n$$",
            "solution": "The problem requires an evaluation of two network embedding techniques based on Laplacian Eigenmaps (LE) concerning their ability to preserve and recover the clustering structure of a given graph. The evaluation is performed by reconstructing graphs from the embeddings and comparing their clustering coefficients to those of the original graph.\n\nFirst, we define the subject of our analysis: an undirected, unweighted, connected graph $G$ with $n=7$ nodes. The node set is $\\{0, 1, 2, 3, 4, 5, 6\\}$. Its structure is defined by the adjacency matrix $A \\in \\{0, 1\\}^{n \\times n}$:\n$$\nA = \\begin{pmatrix}\n0  1  1  0  0  0  0 \\\\\n1  0  1  0  0  0  0 \\\\\n1  1  0  1  0  0  0 \\\\\n0  0  1  0  1  1  0 \\\\\n0  0  0  1  0  1  1 \\\\\n0  0  0  1  1  0  1 \\\\\n0  0  0  0  1  1  0\n\\end{pmatrix}\n$$\nThe graph consists of three triangles, $\\{0,1,2\\}$, $\\{3,4,5\\}$, and $\\{4,5,6\\}$, connected in a way that makes the overall graph connected.\n\nTo quantify the clustering structure, we use two standard metrics: the global transitivity ($C_{\\mathrm{glob}}$) and the average local clustering coefficient ($C_{\\mathrm{loc}}$). These values for the original graph $A$ serve as the ground truth.\n\nThe number of triangles, $N_\\triangle$, is given by $N_\\triangle = \\frac{1}{6}\\mathrm{trace}(A^3)$. The matrix $A^3$ is computed, and its trace is $\\mathrm{trace}(A^3) = 18$. Thus, $N_\\triangle = \\frac{18}{6} = 3$.\nThe number of connected triplets (or wedges), $N_\\wedge$, is $\\sum_i \\binom{d_i}{2}$, where $d_i$ is the degree of node $i$. The degree vector is $d = (2, 2, 3, 3, 4, 4, 2)$. This gives $N_\\wedge = \\binom{2}{2}+\\binom{2}{2}+\\binom{3}{2}+\\binom{3}{2}+\\binom{4}{2}+\\binom{4}{2}+\\binom{2}{2} = 1+1+3+3+6+6+1=21$.\nThe global transitivity is $C_{\\mathrm{glob}}(A) = \\frac{3N_\\triangle}{N_\\wedge} = \\frac{3 \\times 3}{21} = \\frac{9}{21} = \\frac{3}{7}$.\n\nThe local clustering coefficient for a node $i$ is $C_i = \\frac{N_\\triangle(i)}{\\binom{d_i}{2}}$, where $N_\\triangle(i)$ is the number of triangles incident to node $i$. $N_\\triangle(i)$ can be computed as $\\frac{1}{2}(A^3)_{ii}$. The diagonal of $A^3$ is $(2, 2, 2, 2, 4, 4, 2)$, which gives the vector of triangle counts per node as $N_\\triangle(i) = (1, 1, 1, 1, 2, 2, 1)$. The local coefficients are $C_i(A) = (1/1, 1/1, 1/3, 1/3, 2/6, 2/6, 1/1) = (1, 1, 1/3, 1/3, 1/3, 1/3, 1)$.\nThe average local clustering coefficient is $C_{\\mathrm{loc}}(A) = \\frac{1}{7}\\sum_i C_i(A) = \\frac{1}{7}(1+1+4 \\times \\frac{1}{3} + 1) = \\frac{1}{7}(3 + \\frac{4}{3}) = \\frac{13}{21}$.\n\nThe core of the problem involves generating two distinct $\\mathbb{R}^2$ embeddings, $X^{(0)}$ and $X^{(\\lambda)}$, using the Laplacian Eigenmaps framework. This framework finds a low-dimensional representation of the graph's nodes that minimizes the Dirichlet energy, $E_W(X) = \\sum_{i,j} W_{ij} \\lVert x_i - x_j \\rVert_2^2$, where $W$ is a matrix of non-negative edge weights. This minimization is equivalent to solving the generalized eigenvalue problem $L v = \\mu D v$, where $L = D - W$ is the combinatorial graph Laplacian and $D = \\mathrm{diag}(W\\mathbf{1})$ is the weighted degree matrix. The embedding coordinates are given by the eigenvectors corresponding to the smallest non-zero eigenvalues. For an embedding in $\\mathbb{R}^2$, we use the eigenvectors associated with the second and third smallest eigenvalues.\n\nThe two embeddings differ in their choice of weight matrix $W$:\n1.  **Baseline Embedding ($X^{(0)}$)**: This uses the graph's adjacency matrix as weights, so $W^{(0)} = A$.\n2.  **Triangle-Preserving Embedding ($X^{(\\lambda)}$)**: This uses a modified weight matrix $W^{(\\lambda)} = A + \\lambda T^{\\mathrm{edge}}$, where $\\lambda \\ge 0$. The matrix $T^{\\mathrm{edge}} = A \\circ (A^2)$ (element-wise product) quantifies the number of triangles each edge participates in. This weighting scheme reinforces edges that are part of clustered structures.\n\nFor each test case pair $(\\lambda, \\tau)$, we compute both embeddings, $X^{(0)}$ and $X^{(\\lambda)}$.\n\nNext, we reconstruct graphs from these embeddings. This involves reversing the mapping from nodes to points in $\\mathbb{R}^2$.\n1.  For each embedding $X$, calculate all pairwise Euclidean distances $\\delta_{ij} = \\lVert x_i - x_j \\rVert_2$.\n2.  Compute a scale parameter $\\sigma$ as the median of these pairwise distances (for $i  j$).\n3.  Convert distances to affinities using a Gaussian kernel: $K_{ij} = \\exp\\left(-\\frac{\\delta_{ij}^2}{2\\sigma^2}\\right)$. This maps small distances to high affinities (close to $1$) and large distances to low affinities (close to $0$).\n4.  Apply a threshold $\\tau$ to the affinity matrix $K$ to obtain a reconstructed adjacency matrix $\\widehat{A}$. An edge $(\\widehat{A}_{ij}=1)$ is formed if $i \\ne j$ and $K_{ij} \\ge \\tau$. The diagonal is set to $0$.\n\nThis process yields two reconstructed graphs, $\\widehat{A}^{(0)}$ and $\\widehat{A}^{(\\lambda)}$, for each $(\\lambda, \\tau)$.\n\nFinally, we evaluate the quality of the reconstruction by computing the clustering coefficients $C_{\\mathrm{glob}}$ and $C_{\\mathrm{loc}}$ for $\\widehat{A}^{(0)}$ and $\\widehat{A}^{(\\lambda)}$ and comparing them to the original values for $A$. The performance is measured by the absolute errors:\n$e_{\\mathrm{glob}} = |C_{\\mathrm{glob}}(\\widehat{A}) - C_{\\mathrm{glob}}(A)|$\n$e_{\\mathrm{loc}} = |C_{\\mathrm{loc}}(\\widehat{A}) - C_{\\mathrm{loc}}(A)|$\n\nThe improvement offered by the triangle-preserving method over the baseline is the difference in their respective errors:\n$\\Delta e_{\\mathrm{glob}} = e^{(0)}_{\\mathrm{glob}} - e^{(\\lambda)}_{\\mathrm{glob}}$\n$\\Delta e_{\\mathrm{loc}} = e^{(0)}_{\\mathrm{loc}} - e^{(\\lambda)}_{\\mathrm{loc}}$\n\nThis entire procedure is executed for each of the four test cases provided. The case $(\\lambda, \\tau) = (0.0, 0.6)$ is a control, as $W^{(\\lambda=0)} = W^{(0)}$, implying that the two methods are identical and the improvements must be $0$. The other cases test the hypothesis that for appropriate $\\lambda  0$, the triangle-preserving embedding will better capture the graph's clustering structure, resulting in smaller reconstruction errors and positive improvement values. The implementation will follow these steps precisely.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Solves the network embedding and reconstruction problem for the given test cases.\n    \"\"\"\n    # Define the original adjacency matrix A\n    A = np.array([\n        [0, 1, 1, 0, 0, 0, 0],\n        [1, 0, 1, 0, 0, 0, 0],\n        [1, 1, 0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 1, 1, 0],\n        [0, 0, 0, 1, 0, 1, 1],\n        [0, 0, 0, 1, 1, 0, 1],\n        [0, 0, 0, 0, 1, 1, 0]\n    ], dtype=float)\n\n    # Test cases from the problem statement\n    test_cases = [\n        (0.0, 0.6),\n        (0.8, 0.6),\n        (2.0, 0.75),\n        (1.0, 0.9)\n    ]\n\n    def compute_clustering_metrics(adj_matrix):\n        \"\"\"\n        Computes global transitivity and average local clustering coefficient for a graph.\n        \"\"\"\n        n = adj_matrix.shape[0]\n        if n == 0:\n            return 0.0, 0.0\n\n        degrees = adj_matrix.sum(axis=1)\n\n        # Global transitivity (C_glob)\n        num_wedges = np.sum(degrees * (degrees - 1) / 2)\n        if num_wedges == 0:\n            c_glob = 0.0\n        else:\n            # Using trace(A^3) for triangles\n            A_cubed = np.linalg.matrix_power(adj_matrix, 3)\n            num_triangles = np.trace(A_cubed) / 6.0\n            c_glob = 3.0 * num_triangles / num_wedges\n\n        # Average local clustering coefficient (C_loc)\n        nodes_with_degree_ge_2 = np.where(degrees = 2)[0]\n        if len(nodes_with_degree_ge_2) == 0:\n            c_loc = 0.0\n        else:\n            local_coeffs = []\n            # Diag(A^3) gives 2 * num_triangles_per_node for unweighted graphs\n            if 'A_cubed' not in locals():\n                A_cubed = np.linalg.matrix_power(adj_matrix, 3)\n            \n            triangles_per_node = np.diag(A_cubed) / 2.0\n            \n            for i in nodes_with_degree_ge_2:\n                d_i = degrees[i]\n                # denominator is n_choose_2 for neighbors\n                denom = d_i * (d_i - 1) / 2.0\n                if denom > 0:\n                    local_coeffs.append(triangles_per_node[i] / denom)\n            \n            if not local_coeffs:\n                c_loc = 0.0\n            else:\n                c_loc = np.mean(local_coeffs)\n        \n        return c_glob, c_loc\n\n    # Compute ground truth clustering coefficients for the original graph A\n    c_glob_A, c_loc_A = compute_clustering_metrics(A)\n\n    # Pre-compute the triangle edge-count matrix T_edge\n    A_squared = A @ A\n    T_edge = A * A_squared # Hadamard product\n\n    results = []\n\n    for lambda_val, tau in test_cases:\n        # 1. Baseline Embedding (lambda = 0)\n        W0 = A\n        \n        # 2. Triangle-Preserving Embedding\n        W_lambda = A + lambda_val * T_edge\n\n        embeddings_data = {}\n        for key, W in [('0', W0), ('lambda', W_lambda)]:\n            # Compute Laplacian L and Degree matrix D\n            D = np.diag(W.sum(axis=1))\n            L = D - W\n            \n            # Solve generalized eigenvalue problem: Lv = mu * Dv\n            # Need to handle potential singularity in D if an isolated node has 0 weight to all others\n            # In this problem, graph is connected and weights are non-negative, so D is pos-def.\n            eigvals, eigvecs = eigh(L, D)\n            \n            # The embedding is formed by the 2nd and 3rd eigenvectors\n            X = eigvecs[:, 1:3]\n            \n            # Reconstruct graph\n            # Pairwise distances\n            dists_condensed = pdist(X, 'euclidean')\n            \n            if len(dists_condensed) == 0: # Graph with 0 or 1 node\n                A_hat = np.zeros_like(A)\n            else:\n                dists_matrix = squareform(dists_condensed)\n                \n                # Sigma is the median of pairwise distances\n                sigma = np.median(dists_condensed)\n                \n                if sigma == 0:\n                    # All points are coincident, create an empty graph\n                    K = np.zeros_like(dists_matrix)\n                else:\n                    # Gaussian kernel\n                    K = np.exp(-dists_matrix**2 / (2 * sigma**2))\n\n                # Thresholding\n                A_hat = (K = tau).astype(float)\n                np.fill_diagonal(A_hat, 0)\n            \n            # Compute clustering for reconstructed graph\n            c_glob_hat, c_loc_hat = compute_clustering_metrics(A_hat)\n            \n            # Compute absolute errors\n            e_glob = abs(c_glob_hat - c_glob_A)\n            e_loc = abs(c_loc_hat - c_loc_A)\n            \n            embeddings_data[key] = {'e_glob': e_glob, 'e_loc': e_loc}\n\n        # Calculate improvements\n        e0_glob = embeddings_data['0']['e_glob']\n        elambda_glob = embeddings_data['lambda']['e_glob']\n        delta_e_glob = e0_glob - elambda_glob\n\n        e0_loc = embeddings_data['0']['e_loc']\n        elambda_loc = embeddings_data['lambda']['e_loc']\n        delta_e_loc = e0_loc - elambda_loc\n\n        results.append(\n            [e0_glob, elambda_glob, delta_e_glob, e0_loc, elambda_loc, delta_e_loc]\n        )\n\n    # Format the final output string\n    def format_list(lst):\n        return f\"[{','.join(f'{x:.10f}' for x in lst)}]\"\n        \n    final_output = f\"[{','.join(format_list(res) for res in results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Creating an embedding is only half the challenge; a crucial skill is knowing how to evaluate its quality and understand its limitations. This practice introduces a versatile toolkit of evaluation metrics designed to quantify the fidelity of an embedding from multiple perspectives. By calculating Stress, Relative Distortion, and Link Prediction AUC, you will learn to assess how well an embedding preserves global structure, maintains local distances, and predicts unobserved links, revealing the fundamental trade-offs inherent in any low-dimensional network representation. ",
            "id": "4290685",
            "problem": "You are given two low-dimensional embeddings for the same undirected, unweighted graphs. Your task is to compute three metrics that evaluate how well each embedding preserves the graph’s structure and then aggregate the results across a provided test suite. The three metrics are defined as follows for a graph $G = (V, E)$ with $|V| = n$ nodes and embedding coordinates $\\{x_i \\in \\mathbb{R}^2\\}_{i=0}^{n-1}$:\n\n1. Graph-theoretic shortest path distance: For nodes $i, j \\in V$, let $d_G(i,j)$ denote the length (number of hops) of a shortest path between $i$ and $j$ in $G$. For $i = j$, define $d_G(i,i) = 0$.\n2. Euclidean distance in the embedding: For nodes $i, j \\in V$, let $d_E(i,j) = \\lVert x_i - x_j \\rVert_2$.\n3. Stress of the embedding (global distortion): \n   $$S = \\sum_{0 \\le i  j \\le n-1} \\left( d_E(i,j) - d_G(i,j) \\right)^2.$$\n4. Relative distortion (normalized absolute error over all pairs with nonzero graph distance):\n   $$R = \\frac{1}{|\\{(i,j) \\mid 0 \\le i  j \\le n-1, d_G(i,j)  0\\}|} \\sum_{\\substack{0 \\le i  j \\le n-1 \\\\ d_G(i,j)  0}} \\frac{\\left| d_E(i,j) - d_G(i,j) \\right|}{d_G(i,j)}.$$\n5. Link prediction area under the receiver operating characteristic curve (AUC): Treat existing edges as positives and non-edges as negatives. Use the score $s(i,j) = -d_E(i,j)$ so that smaller Euclidean distances imply higher likelihood of an edge. The AUC is defined as the probability that a randomly selected positive pair has a higher score than a randomly selected negative pair, with ties contributing half:\n   $$\\mathrm{AUC} = \\Pr\\left( s(i,j)  s(u,v) \\right) + \\frac{1}{2} \\Pr\\left( s(i,j) = s(u,v) \\right),$$\n   where $(i,j) \\in E$, $i  j$, and $(u,v)$ is any unordered pair with $u  v$ and $(u,v) \\notin E$.\n\nYou must implement a program that computes $S$, $R$, and $\\mathrm{AUC}$ for each of two embeddings of the same graph, across a test suite of three graphs. All distances are dimensionless. Angles used in coordinate construction must be in radians. Final outputs must be floats. Ties in AUC must be handled using exact equality with a small tolerance in floating point comparisons.\n\nFundamental base and definitions to be used:\n- Graph $G = (V,E)$ is undirected, unweighted, and finite. Shortest path distances $d_G(i,j)$ are defined via breadth-first search and count discrete hops.\n- Euclidean distances $d_E(i,j)$ are computed in $\\mathbb{R}^2$ using the standard two-norm.\n- Stress $S$ and relative distortion $R$ are aggregations over all unordered node pairs, forbidding division by zero via the condition $d_G(i,j)  0$ for $R$.\n- Link prediction AUC is computed using the ranking induced by $s(i,j) = -d_E(i,j)$.\n\nTest suite:\n- Test Case $1$ (cycle graph with $6$ nodes):\n  - Nodes: $\\{0,1,2,3,4,5\\}$.\n  - Edges: $(0,1)$, $(1,2)$, $(2,3)$, $(3,4)$, $(4,5)$, $(5,0)$.\n  - Embedding A (regular hexagon, radius $1$): for each node index $k \\in \\{0,1,2,3,4,5\\}$,\n    $$x_k = \\left( \\cos\\left( \\frac{2\\pi k}{6} \\right), \\sin\\left( \\frac{2\\pi k}{6} \\right) \\right).$$\n  - Embedding B (equally spaced on a line): for each node index $k \\in \\{0,1,2,3,4,5\\}$,\n    $$x_k = (k, 0).$$\n- Test Case $2$ (star graph with $5$ nodes):\n  - Nodes: $\\{0,1,2,3,4\\}$.\n  - Edges: $(0,1)$, $(0,2)$, $(0,3)$, $(0,4)$.\n  - Embedding A (center at origin, leaves at unit circle axis points):\n    $$x_0 = (0,0), \\quad x_1 = (1,0), \\quad x_2 = (0,1), \\quad x_3 = (-1,0), \\quad x_4 = (0,-1).$$\n  - Embedding B (same directions, radius $0.3$):\n    $$x_0 = (0,0), \\quad x_1 = (0.3,0), \\quad x_2 = (0,0.3), \\quad x_3 = (-0.3,0), \\quad x_4 = (0,-0.3).$$\n- Test Case $3$ (path graph with $4$ nodes):\n  - Nodes: $\\{0,1,2,3\\}$.\n  - Edges: $(0,1)$, $(1,2)$, $(2,3)$.\n  - Embedding A (unit spacing on a line):\n    $$x_k = (k,0), \\quad k \\in \\{0,1,2,3\\}.$$\n  - Embedding B (double spacing on a line):\n    $$x_k = (2k,0), \\quad k \\in \\{0,1,2,3\\}.$$\n\nProgram requirements:\n- Compute $d_G(i,j)$ via breadth-first search for each source node.\n- Compute $d_E(i,j)$ via Euclidean distances from the provided embeddings.\n- Compute $S$, $R$, and $\\mathrm{AUC}$ as defined above.\n- For AUC, treat ties where $|s(i,j) - s(u,v)| \\le \\varepsilon$ as exact ties, with $\\varepsilon = 10^{-12}$.\n- Output formatting: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output six floats in the order $[S_A, R_A, \\mathrm{AUC}_A, S_B, R_B, \\mathrm{AUC}_B]$, where A and B denote the two embeddings. Concatenate the results for all three test cases into one flat list. Each float must be rounded to $6$ decimal places.\n\nScientific realism and interpretation requirement:\n- The cycle graph tests trade-offs between preserving geodesic ring distances globally versus linear ordering. The star graph tests local fidelity of hub-leaf distances versus relative placement of leaves. The path graph tests scale invariance of AUC under monotonic transformations and its distinction from stress and relative distortion.\n- No physical units are involved; all distances are dimensionless. Angles are in radians.\n\nYour program must implement the computations and produce the final output line in the exact format described.",
            "solution": "The problem statement has been rigorously validated and is determined to be valid. It is scientifically grounded, well-posed, and free of contradictions or ambiguities. The problem is a well-defined computational task in the field of network science, specifically concerning the evaluation of network embeddings. We may therefore proceed with a solution.\n\nThe task is to compute three quality metrics—Stress ($S$), Relative Distortion ($R$), and link prediction Area Under the Curve ($\\mathrm{AUC}$)—for two distinct embeddings of three different graphs. The solution will be structured as an algorithm that systematically processes each test case. For each graph and each of its two embeddings, the algorithm will execute a sequence of steps to compute the required metrics.\n\nFirst, we must compute the two distance matrices that form the basis of all metrics: the graph-theoretic shortest path distance matrix, $d_G$, and the Euclidean distance matrix in the embedding space, $d_E$.\n\n**1. Computation of Graph-Theoretic Distances ($d_G$)**\n\nFor a given graph $G=(V,E)$ with $n=|V|$ nodes, the distance $d_G(i,j)$ is the number of edges in a shortest path between nodes $i$ and $j$. Since the graphs are unweighted, this can be efficiently computed for all pairs of nodes by executing a Breadth-First Search (BFS) algorithm starting from each node. A more robust and efficient approach is to construct an $n \\times n$ adjacency matrix $A$ where $A_{ij}=1$ if $(i,j) \\in E$ and $A_{ij}=0$ otherwise. The `scipy.sparse.csgraph.shortest_path` function can then be applied to this matrix with the `unweighted=True` and `directed=False` options to yield the complete $n \\times n$ matrix of shortest path distances, which we denote as $d_G$. For any node $i$, $d_G(i, i) = 0$.\n\n**2. Computation of Euclidean Distances ($d_E$)**\n\nFor a given embedding, a set of coordinates $\\{x_k \\in \\mathbb{R}^2\\}_{k=0}^{n-1}$ is provided. The Euclidean distance $d_E(i,j)$ between the embeddings of nodes $i$ and $j$ is defined by the $L_2$-norm:\n$$d_E(i,j) = \\lVert x_i - x_j \\rVert_2 = \\sqrt{(x_{i,1} - x_{j,1})^2 + (x_{i,2} - x_{j,2})^2}.$$\nThis computation is performed for all pairs $(i,j)$ to generate an $n \\times n$ Euclidean distance matrix, $d_E$. This can be efficiently implemented using vectorized operations in `numpy`, by computing the pairwise differences between all coordinate vectors.\n\n**3. Computation of Evaluation Metrics**\n\nWith these two matrices $d_G$ and $d_E$ available, we can compute the three metrics.\n\n**Stress ($S$)**: The stress is the sum of squared differences between the Euclidean distances and the graph-theoretic distances over all unique pairs of nodes.\n$$S = \\sum_{0 \\le i  j \\le n-1} \\left( d_E(i,j) - d_G(i,j) \\right)^2.$$\nThis is computed by taking the upper triangle (excluding the diagonal) of the matrix $(d_E - d_G)$, squaring each element, and summing the results.\n\n**Relative Distortion ($R$)**: The relative distortion is the average of the absolute difference between the two distances, normalized by the graph-theoretic distance. The sum is taken only over pairs with a non-zero graph distance to avoid division by zero.\n$$R = \\frac{1}{|\\{(i,j) \\mid 0 \\le i  j \\le n-1, d_G(i,j)  0\\}|} \\sum_{\\substack{0 \\le i  j \\le n-1 \\\\ d_G(i,j)  0}} \\frac{\\left| d_E(i,j) - d_G(i,j) \\right|}{d_G(i,j)}.$$\nThis is implemented by iterating over the unique pairs $(i,j)$, and for each pair where $d_G(i,j)  0$, we calculate the term and add it to a running sum. The final sum is then divided by the count of such pairs.\n\n**Link Prediction AUC ($\\mathrm{AUC}$)**: This metric evaluates the embedding's ability to distinguish between pairs of nodes that are connected by an edge (positives) and those that are not (negatives). A score $s(i,j) = -d_E(i,j)$ is assigned to each pair, reflecting the assumption that smaller Euclidean distance implies a higher likelihood of an edge. The $\\mathrm{AUC}$ is the probability that a randomly chosen positive pair has a higher score than a randomly chosen negative pair.\nTo compute this, we first partition all unique pairs of nodes $(i,j)$ with $ij$ into two sets: positive pairs $P = \\{(i,j) \\mid (i,j) \\in E, ij\\}$ and negative pairs $N = \\{(i,j) \\mid (i,j) \\notin E, ij\\}$. We then compute the scores $s(i,j)$ for all pairs in both sets. The $\\mathrm{AUC}$ is calculated as:\n$$\\mathrm{AUC} = \\frac{N_{}+\\frac{1}{2}N_{=}}{|P| \\cdot |N|},$$\nwhere $N_{}$ is the number of times a positive pair's score is greater than a negative pair's score, and $N_{=}$ is the number of times the scores are equal. The comparison for equality must account for floating-point inaccuracies, so we treat scores $s_p$ and $s_n$ as equal if $|s_p - s_n| \\le \\varepsilon$, where $\\varepsilon$ is a small tolerance, given as $10^{-12}$. This is achieved by iterating through all pairs of (positive_score, negative_score) and counting the number of greater, equal, and lesser outcomes.\n\nThe final program will encapsulate this logic, applying it to each of the three test cases and their two embeddings, and will format the resulting $18$ floating-point numbers as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import csgraph\n\ndef solve():\n    \"\"\"\n    Computes and prints the embedding evaluation metrics for the given test suite.\n    \"\"\"\n    \n    # Test cases defined as (num_nodes, edges, embedding_A_def, embedding_B_def)\n    # where embedding_def is a lambda function that returns coordinates.\n    test_cases = [\n        (\n            6, # n_nodes\n            [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)], # edges\n            lambda: np.array([(np.cos(2 * np.pi * k / 6), np.sin(2 * np.pi * k / 6)) for k in range(6)]), # emb_A\n            lambda: np.array([(float(k), 0.0) for k in range(6)]) # emb_B\n        ),\n        (\n            5, # n_nodes\n            [(0, 1), (0, 2), (0, 3), (0, 4)], # edges\n            lambda: np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [-1.0, 0.0], [0.0, -1.0]]), # emb_A\n            lambda: np.array([[0.0, 0.0], [0.3, 0.0], [0.0, 0.3], [-0.3, 0.0], [0.0, -0.3]]) # emb_B\n        ),\n        (\n            4, # n_nodes\n            [(0, 1), (1, 2), (2, 3)], # edges\n            lambda: np.array([(float(k), 0.0) for k in range(4)]), # emb_A\n            lambda: np.array([(2.0 * k, 0.0) for k in range(4)]) # emb_B\n        )\n    ]\n\n    all_results = []\n    \n    # Epsilon for AUC tie handling\n    epsilon = 1e-12\n\n    for n_nodes, edges, emb_A_func, emb_B_func in test_cases:\n        # 1. Compute graph-theoretic distance matrix d_G\n        adj_matrix = np.zeros((n_nodes, n_nodes))\n        for i, j in edges:\n            adj_matrix[i, j] = adj_matrix[j, i] = 1\n        \n        d_G = csgraph.shortest_path(csgraph=adj_matrix, directed=False, unweighted=True)\n        # Ensure diagonal is zero, although shortest_path should handle this\n        np.fill_diagonal(d_G, 0)\n        \n        # Get upper triangle indices for pair-wise calculations\n        triu_indices = np.triu_indices(n_nodes, k=1)\n\n        for emb_func in [emb_A_func, emb_B_func]:\n            coords = emb_func()\n            \n            # 2. Compute Euclidean distance matrix d_E\n            # Using broadcasting for efficiency: diffs[i, j, k] = coords[i, k] - coords[j, k]\n            diffs = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]\n            d_E = np.sqrt(np.sum(diffs**2, axis=-1))\n\n            d_G_flat = d_G[triu_indices]\n            d_E_flat = d_E[triu_indices]\n\n            # 3. Compute Stress (S)\n            stress = np.sum((d_E_flat - d_G_flat)**2)\n\n            # 4. Compute Relative Distortion (R)\n            nonzero_graph_dist_mask = d_G_flat  0\n            if np.any(nonzero_graph_dist_mask):\n                relative_errors = np.abs(d_E_flat[nonzero_graph_dist_mask] - d_G_flat[nonzero_graph_dist_mask]) / d_G_flat[nonzero_graph_dist_mask]\n                relative_distortion = np.mean(relative_errors)\n            else:\n                relative_distortion = 0.0\n\n            # 5. Compute Link Prediction AUC\n            pos_scores = []\n            neg_scores = []\n            for i in range(n_nodes):\n                for j in range(i + 1, n_nodes):\n                    score = -d_E[i, j]\n                    if adj_matrix[i, j] == 1:\n                        pos_scores.append(score)\n                    else:\n                        neg_scores.append(score)\n\n            if not pos_scores or not neg_scores:\n                auc = 0.5  # Default if no positives or no negatives\n            else:\n                n_greater = 0\n                n_equal = 0\n                for p_score in pos_scores:\n                    for n_score in neg_scores:\n                        if p_score  n_score + epsilon:\n                            n_greater += 1\n                        elif abs(p_score - n_score) = epsilon:\n                            n_equal += 1\n                \n                total_comparisons = len(pos_scores) * len(neg_scores)\n                auc = (n_greater + 0.5 * n_equal) / total_comparisons\n\n            all_results.extend([stress, relative_distortion, auc])\n\n    # Final print statement in the exact required format.\n    # Each float is rounded to 6 decimal places.\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}