{
    "hands_on_practices": [
        {
            "introduction": "A foundational approach to network embedding views the task as a dimensionality reduction problem for the adjacency matrix. This exercise grounds you in this perspective by using a fundamental tool from linear algebra, the Singular Value Decomposition (SVD), to create low-dimensional node representations. By approximating the adjacency matrix with a low-rank reconstruction, you will derive source and target embeddings for a directed graph and quantify the reconstruction error, providing direct insight into the trade-off between dimensionality and fidelity .",
            "id": "4290602",
            "problem": "Consider a directed network with $n$ nodes represented by a real adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij} \\geq 0$ encodes the presence and strength of a directed edge from node $i$ to node $j$. A network embedding seeks to represent each node $i$ by a source vector $x_i \\in \\mathbb{R}^d$ and a target vector $y_i \\in \\mathbb{R}^d$ such that the interaction score for a directed edge $\\{i \\rightarrow j\\}$ is approximated by the inner product $x_i^\\top y_j$. For a given dimension $d$, the reconstructed adjacency is $\\hat{A} = X Y^\\top$, where $X \\in \\mathbb{R}^{n \\times d}$ stacks source vectors row-wise and $Y \\in \\mathbb{R}^{n \\times d}$ stacks target vectors row-wise.\n\nStart from fundamental definitions of linear algebra and matrix decompositions. The Singular Value Decomposition (SVD) of a real matrix $A$ exists and is written $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with nonnegative singular values in nonincreasing order on its diagonal. The best rank-$k$ approximation (in the sense of minimizing the Frobenius norm of the error) is obtained by truncating the SVD to the top $k$ singular values and associated singular vectors.\n\nYour task is to:\n- Construct $d = 2$ dimensional source and target embeddings for each node by using a principled factorization derived from the truncated SVD of $A$. Use this construction to form $X \\in \\mathbb{R}^{n \\times 2}$ and $Y \\in \\mathbb{R}^{n \\times 2}$, and then compute the rank-$2$ reconstruction $\\hat{A} = X Y^\\top$.\n- Define the observed edge set as $\\mathcal{E}_{\\mathrm{obs}} = \\{(i,j) \\mid A_{ij} > 0\\}$. Compute the mean squared reconstruction error on observed edges, given by\n$$\\mathrm{MSE}_{\\mathrm{obs}} = \\begin{cases}\n\\frac{1}{|\\mathcal{E}_{\\mathrm{obs}}|} \\sum_{(i,j) \\in \\mathcal{E}_{\\mathrm{obs}}} \\left(A_{ij} - \\hat{A}_{ij}\\right)^2, & \\text{if } |\\mathcal{E}_{\\mathrm{obs}}| > 0, \\\\\n0, & \\text{if } |\\mathcal{E}_{\\mathrm{obs}}| = 0.\n\\end{cases}$$\nNo clipping or post-processing should be applied to $\\hat{A}$; calculate the error directly from the reconstructed values.\n\nImplement a program that performs this computation for the following test suite of adjacency matrices (all entries are real and nonnegative), each specified as a list of lists. In each case, let $n$ equal the number of rows (and columns) in the matrix:\n1. Directed $4$-cycle ($n = 4$): edges $\\{1 \\rightarrow 2, 2 \\rightarrow 3, 3 \\rightarrow 4, 4 \\rightarrow 1\\}$ with unit weights, represented by\n$A^{(1)} = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0\n\\end{bmatrix}.$\n2. Small bidirectional core ($n = 3$): edges $\\{1 \\leftrightarrow 2\\}$ and $\\{2 \\leftrightarrow 3\\}$ with unit weights, represented by\n$A^{(2)} = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}.$\n3. Empty graph ($n = 3$): no edges, represented by\n$A^{(3)} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}.$\n4. Complete directed graph without self-loops ($n = 3$): all off-diagonal entries are $1$, represented by\n$A^{(4)} = \\begin{bmatrix}\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{bmatrix}.$\n5. Weighted directed network ($n = 4$): asymmetric weights, represented by\n$A^{(5)} = \\begin{bmatrix}\n0 & 2.0 & 0 & 1.5 \\\\\n0 & 0 & 0 & 0 \\\\\n0.5 & 0 & 0 & 1.0 \\\\\n0 & 0.25 & 1.0 & 0\n\\end{bmatrix}.$\n\nYour program must:\n- For each $A^{(t)}$ in the test suite, compute $X$ and $Y$ using the truncated SVD to dimension $d = 2$ and reconstruct $\\hat{A}$ as described.\n- Compute $\\mathrm{MSE}_{\\mathrm{obs}}$ for each test case.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite: $[r^{(1)}, r^{(2)}, r^{(3)}, r^{(4)}, r^{(5)}]$, where each $r^{(t)}$ is the computed $\\mathrm{MSE}_{\\mathrm{obs}}$ for test case $t$ as a floating-point number.\n\nNo physical units are involved. Angles are not involved. Percentages are not involved. The output format must follow the specification exactly. The program must be entirely self-contained and must not read input or write anything other than the final line.",
            "solution": "### 1. Principle: Low-Rank Approximation via SVD\n\nA directed network with $n$ nodes is given by its adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij} \\geq 0$ represents the weight of the edge from node $i$ to node $j$. The goal is to find two sets of low-dimensional vectors, source vectors $\\{x_i\\}_{i=1}^n$ and target vectors $\\{y_i\\}_{i=1}^n$, with $x_i, y_i \\in \\mathbb{R}^d$, such that their inner product $x_i^\\top y_j$ approximates the original edge weight $A_{ij}$.\n\nThis can be expressed in matrix form. Let $X \\in \\mathbb{R}^{n \\times d}$ be the matrix whose rows are the source vectors $x_i^\\top$, and let $Y \\in \\mathbb{R}^{n \\times d}$ be the matrix whose rows are the target vectors $y_i^\\top$. The reconstructed adjacency matrix $\\hat{A}$ is then given by:\n$$ \\hat{A} = X Y^\\top $$\nBy construction, $\\hat{A}$ has a rank of at most $d$. The problem is thus to find the best rank-$d$ approximation of $A$.\n\nThe Singular Value Decomposition (SVD) of the matrix $A$ provides the theoretical foundation for this task. Any real matrix $A \\in \\mathbb{R}^{n \\times n}$ has an SVD of the form:\n$$ A = U \\Sigma V^\\top $$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices ($U^\\top U = I$, $V^\\top V = I$), and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix containing the singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_n \\geq 0$. The columns of $U$ are the left-singular vectors, and the columns of $V$ are the right-singular vectors.\n\nAccording to the Eckart-Young-Mirsky theorem, the best rank-$d$ approximation of $A$ (in the sense of minimizing the Frobenius norm of the difference $A - \\hat{A}$) is obtained by truncating the SVD. This is achieved by keeping only the top $d$ singular values and their corresponding singular vectors:\n$$ \\hat{A}_d = U_d \\Sigma_d V_d^\\top $$\nHere, $U_d \\in \\mathbb{R}^{n \\times d}$ contains the first $d$ columns of $U$, $\\Sigma_d \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix with the top $d$ singular values $\\{\\sigma_1, \\dots, \\sigma_d\\}$, and $V_d \\in \\mathbb{R}^{n \\times d}$ contains the first $d$ columns of $V$.\n\n### 2. Constructing Embeddings from Truncated SVD\n\nThe problem specifies that the dimension of the embedding is $d=2$, so we need to construct the rank-$2$ approximation $\\hat{A}_2 = U_2 \\Sigma_2 V_2^\\top$. To satisfy the embedding structure $\\hat{A} = XY^\\top$, we must define $X$ and $Y$ in terms of the SVD components. A \"principled factorization,\" as requested, is one that is well-motivated. A standard and balanced choice is to distribute the \"energy\" of the singular values between the source and target embeddings. We define $\\Sigma_d^{1/2}$ as the diagonal matrix with entries $\\sqrt{\\sigma_i}$ for $i=1, \\dots, d$. We can then set:\n$$ X = U_d \\Sigma_d^{1/2} $$\n$$ Y = V_d \\Sigma_d^{1/2} $$\nWith this choice, the reconstruction is indeed the optimal rank-$d$ approximation:\n$$ XY^\\top = (U_d \\Sigma_d^{1/2}) (V_d \\Sigma_d^{1/2})^\\top = U_d \\Sigma_d^{1/2} (\\Sigma_d^{1/2})^\\top V_d^\\top = U_d \\Sigma_d V_d^\\top = \\hat{A}_d $$\nFor this problem, with $d=2$, we will use $X = U_2 \\Sigma_2^{1/2}$ and $Y = V_2 \\Sigma_2^{1/2}$.\n\nIt is important to note that if there are repeated singular values (e.g., $\\sigma_d = \\sigma_{d+1}$), the singular vectors are not unique, and thus the best rank-$d$ approximation is not unique. Several test cases ($A^{(1)}$, $A^{(2)}$, $A^{(4)}$) exhibit this property. In such scenarios, a deterministic numerical algorithm for SVD will select a specific valid basis for the singular vectors, leading to a specific, computationally reproducible result. Our solution relies on this deterministic outcome.\n\n### 3. Error Calculation\n\nThe quality of the reconstruction is measured by the Mean Squared Error over the set of observed edges, $\\mathcal{E}_{\\mathrm{obs}} = \\{(i,j) \\mid A_{ij} > 0\\}$. The formula is:\n$$ \\mathrm{MSE}_{\\mathrm{obs}} = \\begin{cases} \\frac{1}{|\\mathcal{E}_{\\mathrm{obs}}|} \\sum_{(i,j) \\in \\mathcal{E}_{\\mathrm{obs}}} \\left(A_{ij} - \\hat{A}_{ij}\\right)^2, & \\text{if } |\\mathcal{E}_{\\mathrm{obs}}| > 0 \\\\ 0, & \\text{if } |\\mathcal{E}_{\\mathrm{obs}}| = 0 \\end{cases} $$\nThis metric focuses the evaluation on how well the embedding model reproduces the connections that are actually present in the network.\n\n### 4. Algorithm for Implementation\n\nFor each given adjacency matrix $A^{(t)}$:\n1.  Convert the input list of lists into a NumPy array $A$.\n2.  Identify the observed edges by creating a boolean mask where $A > 0$. Count the number of observed edges, $|\\mathcal{E}_{\\mathrm{obs}}|$.\n3.  If $|\\mathcal{E}_{\\mathrm{obs}}| = 0$ (as in the empty graph case $A^{(3)}$), the result is $0.0$.\n4.  Otherwise, compute the SVD of $A$: $u, s, vh = \\text{svd}(A)$, where $s$ contains the singular values and $vh$ is $V^\\top$.\n5.  Set the embedding dimension $d=2$.\n6.  Form the truncated SVD components:\n    -   $U_2$ consists of the first $2$ columns of $u$.\n    -   $\\Sigma_2^{1/2}$ is formed from the square roots of the first $2$ singular values in $s$.\n    -   $V_2$ is formed from the first $2$ columns of $vh^\\top$.\n7.  Construct the embedding matrices $X \\in \\mathbb{R}^{n \\times 2}$ and $Y \\in \\mathbb{R}^{n \\times 2}$:\n    -   $X = U_2 \\Sigma_2^{1/2}$\n    -   $Y = V_2 \\Sigma_2^{1/2}$\n8.  Compute the rank-$2$ reconstructed matrix $\\hat{A} = XY^\\top$.\n9.  Calculate the squared errors $(A_{ij} - \\hat{A}_{ij})^2$ only for the observed edges $(i,j) \\in \\mathcal{E}_{\\mathrm{obs}}$.\n10. Compute the mean of these squared errors to obtain $\\mathrm{MSE}_{\\mathrm{obs}}$.\n11. Store this value and repeat for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes network embeddings using truncated SVD and calculates the \n    mean squared reconstruction error on observed edges.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Directed 4-cycle\n        [[0, 1, 0, 0],\n         [0, 0, 1, 0],\n         [0, 0, 0, 1],\n         [1, 0, 0, 0]],\n        \n        # 2. Small bidirectional core\n        [[0, 1, 0],\n         [1, 0, 1],\n         [0, 1, 0]],\n        \n        # 3. Empty graph\n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n        \n        # 4. Complete directed graph without self-loops\n        [[0, 1, 1],\n         [1, 0, 1],\n         [1, 1, 0]],\n        \n        # 5. Weighted directed network\n        [[0, 2.0, 0, 1.5],\n         [0, 0, 0, 0],\n         [0.5, 0, 0, 1.0],\n         [0, 0.25, 1.0, 0]]\n    ]\n\n    results = []\n    d = 2 # Embedding dimension\n\n    for case_matrix in test_cases:\n        A = np.array(case_matrix, dtype=float)\n        \n        # Define the observed edge set as a boolean mask.\n        obs_mask = A > 0\n        num_obs_edges = np.sum(obs_mask)\n\n        # Handle the case of no observed edges.\n        if num_obs_edges == 0:\n            results.append(0.0)\n            continue\n            \n        # SVD is only needed if there are edges to reconstruct.\n        # Compute the SVD of the adjacency matrix.\n        # u: left-singular vectors, s: singular values, vh: right-singular vectors (transposed)\n        u, s, vh = np.linalg.svd(A)\n\n        # Truncate to dimension d.\n        # Pad singular values with zeros if rank(A) < d\n        s_d = np.zeros(d)\n        num_singular_values = len(s)\n        if num_singular_values > 0:\n            len_to_copy = min(d, num_singular_values)\n            s_d[:len_to_copy] = s[:len_to_copy]\n        \n        U_d = u[:, :d]\n        Vh_d = vh[:d, :]\n        V_d = Vh_d.T\n\n        # Construct embeddings X and Y based on the principled factorization.\n        s_d_sqrt = np.sqrt(s_d)\n        \n        # Broadcasting s_d_sqrt to the columns of U_d and V_d\n        X = U_d * s_d_sqrt \n        Y = V_d * s_d_sqrt\n        \n        # Compute the rank-d reconstructed adjacency matrix.\n        A_hat = X @ Y.T\n\n        # Calculate the mean squared error on observed edges.\n        squared_errors = (A[obs_mask] - A_hat[obs_mask])**2\n        mse_obs = np.mean(squared_errors)\n        \n        results.append(mse_obs)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simple matrix factorization, spectral methods offer a powerful alternative for network embedding by analyzing the eigenvectors of the graph Laplacian. These \"spectral coordinates\" reveal the underlying connectivity and community structure of a network in a way that is not always apparent from the adjacency matrix alone. In this practice, you will implement a classic spectral clustering algorithm, applying it to the fundamental task of community detection and comparing its performance against the theoretically optimal (but computationally intensive) solution, highlighting the power and practicality of embedding-based heuristics .",
            "id": "4290674",
            "problem": "Consider an undirected, unweighted graph with $n$ nodes represented by an $n \\times n$ symmetric adjacency matrix $A$ with entries $A_{ij} \\in \\{0,1\\}$, where $A_{ij} = 1$ if there is an edge between node $i$ and node $j$, and $A_{ij} = 0$ otherwise. Let the degree of node $i$ be $k_i = \\sum_{j=1}^{n} A_{ij}$, the diagonal degree matrix be $D = \\mathrm{diag}(k_1, k_2, \\ldots, k_n)$, and the total edge count be $m = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij}$. Define the symmetric normalized Laplacian as\n$$\nL_{\\mathrm{sym}} = I - D^{-1/2} A D^{-1/2},\n$$\nwhere $I$ is the $n \\times n$ identity matrix and $D^{-1/2}$ denotes the diagonal matrix with entries $D_{ii}^{-1/2}$, using the convention $D_{ii}^{-1/2} = 0$ when $k_i = 0$. Spectral embedding maps each node $i$ to a vector in $\\mathbb{R}^k$ using the eigenvectors associated with the $k$ smallest eigenvalues of $L_{\\mathrm{sym}}$, followed by row-wise normalization. A clustering of these embedded points into $k$ groups can then be produced by the $k$-means algorithm.\n\nCommunity quality is quantified by the modularity\n$$\nQ(\\mathbf{g}) = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left(A_{ij} - \\frac{k_i k_j}{2m}\\right) \\delta(g_i, g_j),\n$$\nwhere $\\mathbf{g} = (g_1, g_2, \\ldots, g_n)$ are community labels with $g_i \\in \\{0,1,\\ldots,k-1\\}$, and $\\delta(g_i,g_j)$ is the Kronecker delta that equals $1$ if $g_i = g_j$ and $0$ otherwise.\n\nYour task is to implement a program that, for each provided test graph, performs the following:\n- For each candidate number of communities $k \\in \\{1,2,3\\}$, compute the maximal modularity $Q_{\\mathrm{opt}}$ by exhaustive search over all assignments of nodes to $k$ communities (for $k > 1$, restrict to assignments that use all $k$ community labels at least once).\n- Let $k^\\star$ be the value of $k \\in \\{1,2,3\\}$ that achieves the maximal modularity $Q_{\\mathrm{opt}}$. Construct the spectral embedding using $k^\\star$ eigenvectors associated with the smallest eigenvalues of $L_{\\mathrm{sym}}$, normalize rows to unit $\\ell_2$ norm, and apply $k^\\star$-means clustering to obtain labels $\\mathbf{g}^{\\mathrm{spec}}$. Compute $Q_{\\mathrm{spec}} = Q(\\mathbf{g}^{\\mathrm{spec}})$.\n- Report the modularity gap $\\Delta = Q_{\\mathrm{opt}} - Q_{\\mathrm{spec}}$ for each test case as a real number.\n\nThe test suite consists of the following graphs, each specified by its node count and edge list:\n- Test case $1$ (clear two-community structure): $n = 6$, edges $\\{(0,1),(1,2),(2,0),(3,4),(4,5),(5,3),(2,3)\\}$.\n- Test case $2$ (chain graph): $n = 5$, edges $\\{(0,1),(1,2),(2,3),(3,4)\\}$.\n- Test case $3$ (dense, no community structure): $n = 5$, edges $\\{(i,j) \\mid 0 \\le i < j \\le 4\\}$, the complete graph on $5$ nodes.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where each $r_i$ is the modularity gap $\\Delta$ for test case $i$ expressed as a decimal number.",
            "solution": "The task is to compute the modularity gap, $\\Delta = Q_{\\mathrm{opt}} - Q_{\\mathrm{spec}}$, for three given graphs. This requires finding the true maximum modularity, $Q_{\\mathrm{opt}}$, via an exhaustive search, and comparing it to the modularity, $Q_{\\mathrm{spec}}$, obtained from a standard spectral clustering heuristic. The procedure is as follows.\n\nFor each graph, defined by its number of nodes $n$ and its edge list:\n\n**1. Fundamental Graph Properties and Modularity**\n\nFirst, we establish the fundamental mathematical objects. The graph is represented by its $n \\times n$ adjacency matrix $A$, where $A_{ij} = 1$ if an edge exists between nodes $i$ and $j$, and $A_{ij} = 0$ otherwise. Since the graph is undirected, $A$ is symmetric.\n\nThe degree of each node $i$ is $k_i = \\sum_{j=1}^{n} A_{ij}$. The total number of edges is $m = \\frac{1}{2}\\sum_i k_i = \\frac{1}{2}\\sum_{i,j} A_{ij}$. The degree matrix $D$ is a diagonal matrix with $D_{ii} = k_i$.\n\nThe modularity $Q$ for a given partition of nodes into communities, described by a label vector $\\mathbf{g} = (g_1, \\ldots, g_n)$, is given by the formula:\n$$\nQ(\\mathbf{g}) = \\frac{1}{2m} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left(A_{ij} - \\frac{k_i k_j}{2m}\\right) \\delta(g_i, g_j)\n$$\nwhere $\\delta(g_i, g_j)$ is the Kronecker delta. This function quantifies the quality of a partition by comparing the fraction of edges within communities to the expected fraction if edges were distributed randomly, preserving node degrees.\n\n**2. Determination of Optimal Modularity ($Q_{\\mathrm{opt}}$) and Optimal Community Count ($k^\\star$)**\n\nThe problem requires finding the maximum possible modularity, $Q_{\\mathrm{opt}}$, by searching over all possible community assignments for a number of communities $k \\in \\{1, 2, 3\\}$.\n\nFor $k=1$, all nodes belong to a single community. In this case, $\\delta(g_i, g_j) = 1$ for all $i,j$. The sum becomes $\\sum_{i,j} (A_{ij} - \\frac{k_i k_j}{2m})$. Since $\\sum_{i,j} A_{ij} = 2m$ and $\\sum_i k_i = 2m$, the sum evaluates to $2m - \\frac{(\\sum_i k_i)(\\sum_j k_j)}{2m} = 2m - \\frac{(2m)(2m)}{2m} = 0$. Thus, for $k=1$, the modularity is always $0$.\n\nFor $k=2$ and $k=3$, we must perform an exhaustive search. We generate all possible assignments of the $n$ nodes to $k$ communities, with the constraint that each of the $k$ communities must be non-empty. This is achieved by generating all $k^n$ possible label vectors $\\mathbf{g}$ and retaining only those where the set of unique labels has size $k$. For each valid partition, we compute $Q(\\mathbf{g})$ and identify the maximum value for that $k$.\n\nFinally, $Q_{\\mathrm{opt}}$ is the maximum modularity found across $k \\in \\{1, 2, 3\\}$, and $k^\\star$ is the number of communities that yields this optimal value.\n$$\nQ_{\\mathrm{opt}} = \\max_{k \\in \\{1,2,3\\}} \\left( \\max_{\\mathbf{g} \\in \\mathcal{P}_k} Q(\\mathbf{g}) \\right)\n$$\nwhere $\\mathcal{P}_k$ is the set of all valid partitions into $k$ communities.\n\n**3. Spectral Clustering and Approximate Modularity ($Q_{\\mathrm{spec}}$)**\n\nUsing the optimal community count $k^\\star$ determined above, we apply the spectral clustering algorithm.\n\nFirst, we construct the symmetric normalized Laplacian matrix:\n$$\nL_{\\mathrm{sym}} = I - D^{-1/2} A D^{-1/2}\n$$\nHere, $I$ is the $n \\times n$ identity matrix. $D^{-1/2}$ is the diagonal matrix with entries $(D^{-1/2})_{ii} = 1/\\sqrt{k_i}$. For any isolated node with $k_i=0$, we use the convention $(D^{-1/2})_{ii}=0$.\n\nNext, we compute the eigenvalues and eigenvectors of $L_{\\mathrm{sym}}$. Since $L_{\\mathrm{sym}}$ is a real symmetric matrix, its eigenvalues are real and its eigenvectors are orthogonal. We select the $k^\\star$ eigenvectors corresponding to the $k^\\star$ smallest eigenvalues. Let these eigenvectors be $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_{k^\\star}$.\n\nAn embedding matrix $U \\in \\mathbb{R}^{n \\times k^\\star}$ is formed, where the columns are these eigenvectors. The $i$-th row of $U$ is the new vector representation for node $i$. These row vectors are then normalized to have a unit $\\ell_2$ norm. Let this new matrix be $U_{\\mathrm{norm}}$.\n$$ (U_{\\mathrm{norm}})_{i,:} = \\frac{U_{i,:}}{\\|U_{i,:}\\|_2} $$\nIf a row $U_{i,:}$ is a zero vector, its normalized form remains a zero vector.\n\nFinally, the $k^\\star$-means clustering algorithm is applied to the $n$ row vectors of $U_{\\mathrm{norm}}$. This algorithm partitions the embedded points into $k^\\star$ clusters, yielding the community assignment $\\mathbf{g}^{\\mathrm{spec}}$. To ensure reproducibility, the random initialization of the $k$-means algorithm is controlled by a fixed seed. The modularity of this resulting partition, $Q_{\\mathrm{spec}} = Q(\\mathbf{g}^{\\mathrm{spec}})$, is then calculated.\n\n**4. Computation of the Modularity Gap ($\\Delta$)**\n\nFor each test case, the final result is the modularity gap, which measures how close the spectral clustering heuristic came to achieving the theoretical maximum modularity for $k \\le 3$.\n$$\n\\Delta = Q_{\\mathrm{opt}} - Q_{\\mathrm{spec}}\n$$\nThis procedure is systematically applied to each of the three test graphs provided.",
            "answer": "```python\nimport numpy as np\nimport itertools\nfrom scipy.linalg import eigh\nfrom scipy.cluster.vq import kmeans, vq\n\ndef solve():\n    \"\"\"\n    Solves the graph clustering problem for the specified test cases.\n    \"\"\"\n    test_cases = [\n        (6, [(0, 1), (1, 2), (2, 0), (3, 4), (4, 5), (5, 3), (2, 3)]),\n        (5, [(0, 1), (1, 2), (2, 3), (3, 4)]),\n        (5, [(i, j) for i in range(5) for j in range(i + 1, 5)])\n    ]\n\n    results = []\n    # A fixed seed for k-means initialization to ensure reproducibility.\n    KMEANS_SEED = 42\n\n    for n, edges in test_cases:\n        # Step 1: Construct graph properties\n        A = np.zeros((n, n), dtype=int)\n        for u, v in edges:\n            A[u, v] = 1\n            A[v, u] = 1\n        \n        k_vec = A.sum(axis=1)\n        m = k_vec.sum() / 2.0\n\n        # Step 2: Find optimal modularity Q_opt and k_star by exhaustive search\n        if m == 0:\n            # For graphs with no edges, modularity is ill-defined or 0.\n            # Q_opt is 0 (for k=1), k_star is 1.\n            Q_opt, k_star = 0.0, 1\n        else:\n            Q_opt, k_star = find_optimal_modularity(A, k_vec, m, n)\n\n        # Step 3: Perform spectral clustering\n        if k_star == 1 or m == 0:\n            # If optimal k is 1, spectral result is also one cluster.\n            g_spec = np.zeros(n, dtype=int)\n        else:\n            # Construct symmetric normalized Laplacian L_sym\n            with np.errstate(divide='ignore', invalid='ignore'):\n                D_inv_sqrt_diag = 1.0 / np.sqrt(k_vec)\n            D_inv_sqrt_diag[k_vec == 0] = 0\n            D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n            \n            L_sym = np.identity(n) - D_inv_sqrt @ A @ D_inv_sqrt\n            \n            # Eigen-decomposition\n            eigenvalues, eigenvectors = eigh(L_sym)\n            \n            # Form embedding matrix U\n            U = eigenvectors[:, :k_star]\n            \n            # Normalize rows of U\n            norms = np.linalg.norm(U, axis=1, keepdims=True)\n            U_norm = np.divide(U, norms, out=np.zeros_like(U), where=norms!=0)\n            \n            # Apply k-means. Seeding NumPy's global random state for scipy.cluster.vq.kmeans\n            np.random.seed(KMEANS_SEED)\n            centroids, _ = kmeans(U_norm, k_star)\n            g_spec, _ = vq(U_norm, centroids)\n\n        # Step 4: Calculate modularity of the spectral partition\n        if m == 0:\n            Q_spec = 0.0\n        else:\n            Q_spec = calculate_modularity(A, k_vec, m, n, g_spec)\n\n        # Step 5: Compute the modularity gap\n        delta = Q_opt - Q_spec\n        results.append(delta)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef calculate_modularity(A, k_vec, m, n, g):\n    \"\"\"\n    Calculates the modularity for a given partition.\n    \"\"\"\n    mod_sum = 0.0\n    g = np.asarray(g)\n    unique_labels = np.unique(g)\n    \n    for community_id in unique_labels:\n        nodes_in_community = np.where(g == community_id)[0]\n        for i in nodes_in_community:\n            for j in nodes_in_community:\n                mod_sum += (A[i, j] - (k_vec[i] * k_vec[j]) / (2 * m))\n                \n    return mod_sum / (2 * m)\n\ndef find_optimal_modularity(A, k_vec, m, n):\n    \"\"\"\n    Finds the maximum modularity Q_opt and corresponding k_star by exhaustive search.\n    \"\"\"\n    # k=1 gives Q=0\n    overall_max_q = 0.0\n    k_star = 1\n\n    for k in range(2, 4):\n        # Generate all possible assignments of n nodes to k clusters\n        partitions = itertools.product(range(k), repeat=n)\n        \n        current_k_max_q = -np.inf\n\n        for p in partitions:\n            # A valid partition must use all k community labels\n            if len(set(p)) != k:\n                continue\n            \n            q = calculate_modularity(A, k_vec, m, n, p)\n            \n            if q > current_k_max_q:\n                current_k_max_q = q\n        \n        if current_k_max_q > overall_max_q:\n            overall_max_q = current_k_max_q\n            k_star = k\n            \n    return overall_max_q, k_star\n\nsolve()\n```"
        },
        {
            "introduction": "Creating an embedding is only half the battle; knowing whether it is a \"good\" representation of the original network is equally critical. Since embeddings are inherently lossy approximations, we need robust metrics to evaluate their quality. This final practice focuses on this crucial evaluation step, asking you to implement and compare three distinct metrics that capture different aspects of an embedding's fidelity—from preserving global shortest-path distances to retaining local neighborhood information for link prediction . Through this, you will develop a nuanced understanding of the unavoidable trade-offs between capturing local and global network structure.",
            "id": "4290685",
            "problem": "You are given two low-dimensional embeddings for the same undirected, unweighted graphs. Your task is to compute three metrics that evaluate how well each embedding preserves the graph’s structure and then aggregate the results across a provided test suite. The three metrics are defined as follows for a graph $G = (V, E)$ with $|V| = n$ nodes and embedding coordinates $\\{x_i \\in \\mathbb{R}^2\\}_{i=0}^{n-1}$:\n\n1. Graph-theoretic shortest path distance: For nodes $i, j \\in V$, let $d_G(i,j)$ denote the length (number of hops) of a shortest path between $i$ and $j$ in $G$. For $i = j$, define $d_G(i,i) = 0$.\n2. Euclidean distance in the embedding: For nodes $i, j \\in V$, let $d_E(i,j) = \\lVert x_i - x_j \\rVert_2$.\n3. Stress of the embedding (global distortion): \n   $$S = \\sum_{0 \\le i < j \\le n-1} \\left( d_E(i,j) - d_G(i,j) \\right)^2.$$\n4. Relative distortion (normalized absolute error over all pairs with nonzero graph distance):\n   $$R = \\frac{1}{|\\{(i,j) \\mid 0 \\le i < j \\le n-1, d_G(i,j) > 0\\}|} \\sum_{\\substack{0 \\le i < j \\le n-1 \\\\ d_G(i,j) > 0}} \\frac{\\left| d_E(i,j) - d_G(i,j) \\right|}{d_G(i,j)}.$$\n5. Link prediction area under the receiver operating characteristic curve (AUC): Treat existing edges as positives and non-edges as negatives. Use the score $s(i,j) = -d_E(i,j)$ so that smaller Euclidean distances imply higher likelihood of an edge. The AUC is defined as the probability that a randomly selected positive pair has a higher score than a randomly selected negative pair, with ties contributing half:\n   $$\\mathrm{AUC} = \\Pr\\left( s(i,j) > s(u,v) \\right) + \\frac{1}{2} \\Pr\\left( s(i,j) = s(u,v) \\right),$$\n   where $(i,j) \\in E$, $i < j$, and $(u,v)$ is any unordered pair with $u < v$ and $(u,v) \\notin E$.\n\nYou must implement a program that computes $S$, $R$, and $\\mathrm{AUC}$ for each of two embeddings of the same graph, across a test suite of three graphs. All distances are dimensionless. Angles used in coordinate construction must be in radians. Final outputs must be floats. Ties in AUC must be handled using exact equality with a small tolerance in floating point comparisons.\n\nFundamental base and definitions to be used:\n- Graph $G = (V,E)$ is undirected, unweighted, and finite. Shortest path distances $d_G(i,j)$ are defined via breadth-first search and count discrete hops.\n- Euclidean distances $d_E(i,j)$ are computed in $\\mathbb{R}^2$ using the standard two-norm.\n- Stress $S$ and relative distortion $R$ are aggregations over all unordered node pairs, forbidding division by zero via the condition $d_G(i,j) > 0$ for $R$.\n- Link prediction AUC is computed using the ranking induced by $s(i,j) = -d_E(i,j)$.\n\nTest suite:\n- Test Case $1$ (cycle graph with $6$ nodes):\n  - Nodes: $\\{0,1,2,3,4,5\\}$.\n  - Edges: $(0,1)$, $(1,2)$, $(2,3)$, $(3,4)$, $(4,5)$, $(5,0)$.\n  - Embedding A (regular hexagon, radius $1$): for each node index $k \\in \\{0,1,2,3,4,5\\}$,\n    $$x_k = \\left( \\cos\\left( \\frac{2\\pi k}{6} \\right), \\sin\\left( \\frac{2\\pi k}{6} \\right) \\right).$$\n  - Embedding B (equally spaced on a line): for each node index $k \\in \\{0,1,2,3,4,5\\}$,\n    $$x_k = (k, 0).$$\n- Test Case $2$ (star graph with $5$ nodes):\n  - Nodes: $\\{0,1,2,3,4\\}$.\n  - Edges: $(0,1)$, $(0,2)$, $(0,3)$, $(0,4)$.\n  - Embedding A (center at origin, leaves at unit circle axis points):\n    $$x_0 = (0,0), \\quad x_1 = (1,0), \\quad x_2 = (0,1), \\quad x_3 = (-1,0), \\quad x_4 = (0,-1).$$\n  - Embedding B (same directions, radius $0.3$):\n    $$x_0 = (0,0), \\quad x_1 = (0.3,0), \\quad x_2 = (0,0.3), \\quad x_3 = (-0.3,0), \\quad x_4 = (0,-0.3).$$\n- Test Case $3$ (path graph with $4$ nodes):\n  - Nodes: $\\{0,1,2,3\\}$.\n  - Edges: $(0,1)$, $(1,2)$, $(2,3)$.\n  - Embedding A (unit spacing on a line):\n    $$x_k = (k,0), \\quad k \\in \\{0,1,2,3\\}.$$\n  - Embedding B (double spacing on a line):\n    $$x_k = (2k,0), \\quad k \\in \\{0,1,2,3\\}.$$\n\nProgram requirements:\n- Compute $d_G(i,j)$ via breadth-first search for each source node.\n- Compute $d_E(i,j)$ via Euclidean distances from the provided embeddings.\n- Compute $S$, $R$, and $\\mathrm{AUC}$ as defined above.\n- For AUC, treat ties where $|s(i,j) - s(u,v)| \\le \\varepsilon$ as exact ties, with $\\varepsilon = 10^{-12}$.\n- Output formatting: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output six floats in the order $[S_A, R_A, \\mathrm{AUC}_A, S_B, R_B, \\mathrm{AUC}_B]$, where A and B denote the two embeddings. Concatenate the results for all three test cases into one flat list. Each float must be rounded to $6$ decimal places.\n\nScientific realism and interpretation requirement:\n- The cycle graph tests trade-offs between preserving geodesic ring distances globally versus linear ordering. The star graph tests local fidelity of hub-leaf distances versus relative placement of leaves. The path graph tests scale invariance of AUC under monotonic transformations and its distinction from stress and relative distortion.\n- No physical units are involved; all distances are dimensionless. Angles are in radians.\n\nYour program must implement the computations and produce the final output line in the exact format described.",
            "solution": "The task is to compute three quality metrics—Stress ($S$), Relative Distortion ($R$), and link prediction Area Under the Curve ($\\mathrm{AUC}$)—for two distinct embeddings of three different graphs. The solution will be structured as an algorithm that systematically processes each test case. For each graph and each of its two embeddings, the algorithm will execute a sequence of steps to compute the required metrics.\n\nFirst, we must compute the two distance matrices that form the basis of all metrics: the graph-theoretic shortest path distance matrix, $d_G$, and the Euclidean distance matrix in the embedding space, $d_E$.\n\n**1. Computation of Graph-Theoretic Distances ($d_G$)**\n\nFor a given graph $G=(V,E)$ with $n=|V|$ nodes, the distance $d_G(i,j)$ is the number of edges in a shortest path between nodes $i$ and $j$. Since the graphs are unweighted, this can be efficiently computed for all pairs of nodes by executing a Breadth-First Search (BFS) algorithm starting from each node. An efficient approach is to construct an $n \\times n$ adjacency matrix $A$ where $A_{ij}=1$ if $(i,j) \\in E$ and $A_{ij}=0$ otherwise. A standard library function for all-pairs shortest-paths can then be applied to this matrix to yield the complete $n \\times n$ matrix of shortest path distances, which we denote as $d_G$. For any node $i$, $d_G(i, i) = 0$.\n\n**2. Computation of Euclidean Distances ($d_E$)**\n\nFor a given embedding, a set of coordinates $\\{x_k \\in \\mathbb{R}^2\\}_{k=0}^{n-1}$ is provided. The Euclidean distance $d_E(i,j)$ between the embeddings of nodes $i$ and $j$ is defined by the $L_2$-norm:\n$$d_E(i,j) = \\lVert x_i - x_j \\rVert_2 = \\sqrt{(x_{i,1} - x_{j,1})^2 + (x_{i,2} - x_{j,2})^2}.$$\nThis computation is performed for all pairs $(i,j)$ to generate an $n \\times n$ Euclidean distance matrix, $d_E$. This can be efficiently implemented using vectorized operations by computing the pairwise differences between all coordinate vectors.\n\n**3. Computation of Evaluation Metrics**\n\nWith the distance matrices $d_G$ and $d_E$ available, we can compute the three metrics.\n\n**Stress ($S$)**: The stress is the sum of squared differences between the Euclidean distances and the graph-theoretic distances over all unique pairs of nodes.\n$$S = \\sum_{0 \\le i < j \\le n-1} \\left( d_E(i,j) - d_G(i,j) \\right)^2.$$\nThis is computed by taking the upper triangle (excluding the diagonal) of the matrix $(d_E - d_G)$, squaring each element, and summing the results.\n\n**Relative Distortion ($R$)**: The relative distortion is the average of the absolute difference between the two distances, normalized by the graph-theoretic distance. The sum is taken only over pairs with a non-zero graph distance to avoid division by zero.\n$$R = \\frac{1}{|\\{(i,j) \\mid 0 \\le i < j \\le n-1, d_G(i,j) > 0\\}|} \\sum_{\\substack{0 \\le i < j \\le n-1 \\\\ d_G(i,j) > 0}} \\frac{\\left| d_E(i,j) - d_G(i,j) \\right|}{d_G(i,j)}.$$\nThis is implemented by iterating over the unique pairs $(i,j)$, and for each pair where $d_G(i,j) > 0$, we calculate the term and add it to a running sum. The final sum is then divided by the count of such pairs.\n\n**Link Prediction AUC ($\\mathrm{AUC}$)**: This metric evaluates the embedding's ability to distinguish between pairs of nodes that are connected by an edge (positives) and those that are not (negatives). A score $s(i,j) = -d_E(i,j)$ is assigned to each pair, reflecting the assumption that smaller Euclidean distance implies a higher likelihood of an edge. The $\\mathrm{AUC}$ is the probability that a randomly chosen positive pair has a higher score than a randomly chosen negative pair.\nTo compute this, we first partition all unique pairs of nodes $(i,j)$ with $i<j$ into two sets: positive pairs $P = \\{(i,j) \\mid (i,j) \\in E, i<j\\}$ and negative pairs $N = \\{(i,j) \\mid (i,j) \\notin E, i<j\\}$. We then compute the scores $s(i,j)$ for all pairs in both sets. The $\\mathrm{AUC}$ is calculated as:\n$$\\mathrm{AUC} = \\frac{N_{>}+\\frac{1}{2}N_{=}}{|P| \\cdot |N|},$$\nwhere $N_{>}$ is the number of times a positive pair's score is greater than a negative pair's score, and $N_{=}$ is the number of times the scores are equal. The comparison for equality must account for floating-point inaccuracies, so we treat scores $s_p$ and $s_n$ as equal if $|s_p - s_n| \\le \\varepsilon$, where $\\varepsilon$ is a small tolerance, given as $10^{-12}$. This is achieved by iterating through all pairs of (positive_score, negative_score) and counting the number of greater, equal, and lesser outcomes.\n\nThe final program will encapsulate this logic, applying it to each of the three test cases and their two embeddings, and will format the resulting $18$ floating-point numbers as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import csgraph\n\ndef solve():\n    \"\"\"\n    Computes and prints the embedding evaluation metrics for the given test suite.\n    \"\"\"\n    \n    # Test cases defined as (num_nodes, edges, embedding_A_def, embedding_B_def)\n    # where embedding_def is a lambda function that returns coordinates.\n    test_cases = [\n        (\n            6, # n_nodes\n            [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)], # edges\n            lambda: np.array([(np.cos(2 * np.pi * k / 6), np.sin(2 * np.pi * k / 6)) for k in range(6)]), # emb_A\n            lambda: np.array([(float(k), 0.0) for k in range(6)]) # emb_B\n        ),\n        (\n            5, # n_nodes\n            [(0, 1), (0, 2), (0, 3), (0, 4)], # edges\n            lambda: np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [-1.0, 0.0], [0.0, -1.0]]), # emb_A\n            lambda: np.array([[0.0, 0.0], [0.3, 0.0], [0.0, 0.3], [-0.3, 0.0], [0.0, -0.3]]) # emb_B\n        ),\n        (\n            4, # n_nodes\n            [(0, 1), (1, 2), (2, 3)], # edges\n            lambda: np.array([(float(k), 0.0) for k in range(4)]), # emb_A\n            lambda: np.array([(2.0 * k, 0.0) for k in range(4)]) # emb_B\n        )\n    ]\n\n    all_results = []\n    \n    # Epsilon for AUC tie handling\n    epsilon = 1e-12\n\n    for n_nodes, edges, emb_A_func, emb_B_func in test_cases:\n        # 1. Compute graph-theoretic distance matrix d_G\n        adj_matrix = np.zeros((n_nodes, n_nodes))\n        for i, j in edges:\n            adj_matrix[i, j] = adj_matrix[j, i] = 1\n        \n        d_G = csgraph.shortest_path(csgraph=adj_matrix, directed=False, unweighted=True)\n        # Ensure diagonal is zero, although shortest_path should handle this\n        np.fill_diagonal(d_G, 0)\n        \n        # Get upper triangle indices for pair-wise calculations\n        triu_indices = np.triu_indices(n_nodes, k=1)\n\n        for emb_func in [emb_A_func, emb_B_func]:\n            coords = emb_func()\n            \n            # 2. Compute Euclidean distance matrix d_E\n            # Using broadcasting for efficiency: diffs[i, j, k] = coords[i, k] - coords[j, k]\n            diffs = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]\n            d_E = np.sqrt(np.sum(diffs**2, axis=-1))\n\n            d_G_flat = d_G[triu_indices]\n            d_E_flat = d_E[triu_indices]\n\n            # 3. Compute Stress (S)\n            stress = np.sum((d_E_flat - d_G_flat)**2)\n\n            # 4. Compute Relative Distortion (R)\n            nonzero_graph_dist_mask = d_G_flat > 0\n            if np.any(nonzero_graph_dist_mask):\n                relative_errors = np.abs(d_E_flat[nonzero_graph_dist_mask] - d_G_flat[nonzero_graph_dist_mask]) / d_G_flat[nonzero_graph_dist_mask]\n                relative_distortion = np.mean(relative_errors)\n            else:\n                relative_distortion = 0.0\n\n            # 5. Compute Link Prediction AUC\n            pos_scores = []\n            neg_scores = []\n            for i in range(n_nodes):\n                for j in range(i + 1, n_nodes):\n                    score = -d_E[i, j]\n                    if adj_matrix[i, j] == 1:\n                        pos_scores.append(score)\n                    else:\n                        neg_scores.append(score)\n\n            if not pos_scores or not neg_scores:\n                auc = 0.5  # Default if no positives or no negatives\n            else:\n                n_greater = 0\n                n_equal = 0\n                for p_score in pos_scores:\n                    for n_score in neg_scores:\n                        if p_score > n_score + epsilon:\n                            n_greater += 1\n                        elif abs(p_score - n_score) <= epsilon:\n                            n_equal += 1\n                \n                total_comparisons = len(pos_scores) * len(neg_scores)\n                auc = (n_greater + 0.5 * n_equal) / total_comparisons\n\n            all_results.extend([stress, relative_distortion, auc])\n\n    # Final print statement in the exact required format.\n    # Each float is rounded to 6 decimal places.\n    formatted_results = [f\"{x:.6f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}