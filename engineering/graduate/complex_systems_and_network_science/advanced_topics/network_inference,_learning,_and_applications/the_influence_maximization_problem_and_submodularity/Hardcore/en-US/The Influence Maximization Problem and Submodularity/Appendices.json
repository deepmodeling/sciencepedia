{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds our understanding of influence spread in the fundamental mechanics of the Independent Cascade (IC) model. By manually calculating the expected influence for a single seed node on a small network, you will build intuition for how probabilities combine and how influence propagates. This hands-on calculation, using the live-edge graph representation, is a crucial first step before moving to more complex models and algorithms. ",
            "id": "4309725",
            "problem": "Consider the Independent Cascade (IC) diffusion model on a directed graph with node set $V=\\{1,2,3,4\\}$. Each directed edge $(u,v)$ is associated with a transmission probability $p_{uv}\\in[0,1]$, and all edge transmission events are independent. Under the IC dynamics, a node that becomes active at time $t$ gets exactly one chance at time $t+1$ to activate each currently inactive out-neighbor $v$, which succeeds with probability $p_{uv}$; once a node has made its activation attempts, it plays no further role in propagating influence. The process starts from the seed set $S=\\{1\\}$ at time $t=0$ and runs until no further activations occur. It is known that the expected size of the final active set under the IC model equals the expected number of nodes reachable from the seed set in the corresponding live-edge representation, where each directed edge $(u,v)$ is independently declared live with probability $p_{uv}$ and blocked otherwise.\n\nThe directed edges and their transmission probabilities are:\n- $(1,2)$ with $p_{12}=0.5$,\n- $(2,3)$ with $p_{23}=0.4$,\n- $(1,3)$ with $p_{13}=0.2$,\n- $(3,4)$ with $p_{34}=0.3$.\n\nUsing the live-edge interpretation and exact enumeration of the outcomes and reachable sets implied by the independent edge realizations, compute the expected influence spread $\\sigma_{\\mathrm{IC}}(\\{1\\})$, defined as the expected number of nodes active at termination, including the seed node. Provide your final answer as an exact rational number; do not round.",
            "solution": "The problem asks for the expected influence spread, $\\sigma_{\\mathrm{IC}}(\\{1\\})$, for the Independent Cascade (IC) model on a given directed graph. The seed set is $S=\\{1\\}$. The problem specifies using the live-edge graph interpretation.\n\nAccording to the provided definition, the expected influence spread $\\sigma_{\\mathrm{IC}}(S)$ is equal to the expected number of nodes reachable from the seed set $S$ in the live-edge graph realization. In a live-edge graph $G'$, each edge $(u,v)$ of the original graph is present (i.e., \"live\") with probability $p_{uv}$, independently of all other edges.\n\nThe expected number of reachable nodes can be calculated using the linearity of expectation. Let $R(S, G')$ be the set of nodes reachable from $S$ in a specific live-edge graph realization $G'$. The expected spread is the expectation of the size of this set over all possible realizations:\n$$ \\sigma_{\\mathrm{IC}}(S) = E\\left[|R(S, G')|\\right] $$\nBy linearity of expectation, this can be expressed as the sum of the probabilities that each node $v \\in V$ is reachable from $S$:\n$$ \\sigma_{\\mathrm{IC}}(S) = E\\left[\\sum_{v \\in V} \\mathbf{1}_{v \\in R(S, G')}\\right] = \\sum_{v \\in V} E\\left[\\mathbf{1}_{v \\in R(S, G')}\\right] = \\sum_{v \\in V} P(v \\text{ is reachable from } S) $$\nwhere $\\mathbf{1}_{v \\in R(S, G')}$ is the indicator function that is $1$ if node $v$ is reachable from $S$ and $0$ otherwise.\n\nThe node set is $V=\\{1,2,3,4\\}$ and the seed set is $S=\\{1\\}$. We will compute the probability of each node being reachable from node $1$. The given edge probabilities are:\n$p_{12}=0.5 = \\frac{1}{2}$\n$p_{23}=0.4 = \\frac{4}{10} = \\frac{2}{5}$\n$p_{13}=0.2 = \\frac{2}{10} = \\frac{1}{5}$\n$p_{34}=0.3 = \\frac{3}{10}$\n\nWe now compute the reachability probability for each node $v \\in V$.\n\n1.  **Node 1:** The seed node $1$ is always considered reachable from itself.\n    $$ P(1 \\text{ is reachable from } \\{1\\}) = 1 $$\n\n2.  **Node 2:** Node $2$ is reachable from $1$ if and only if the direct edge $(1,2)$ is live. There is only one path from $1$ to $2$.\n    $$ P(2 \\text{ is reachable from } \\{1\\}) = p_{12} = \\frac{1}{2} $$\n\n3.  **Node 3:** Node $3$ is reachable from $1$ if there is at least one live path from $1$ to $3$. The possible paths are the direct edge $(1,3)$ and the path $1 \\to 2 \\to 3$ via edges $(1,2)$ and $(2,3)$. Let $P_1$ be the event that the direct path $1 \\to 3$ is live, and $P_2$ be the event that the path $1 \\to 2 \\to 3$ is live.\n    -   $P(P_1) = p_{13}$\n    -   $P(P_2) = p_{12} \\times p_{23}$ (due to independence of edges)\n    The reachability of node $3$ is the event $P_1 \\cup P_2$. The probability is most easily calculated using the complement rule: the probability of being reachable is $1$ minus the probability of not being reachable. Node $3$ is not reachable if and only if both path options are blocked. That is, the edge $(1,3)$ must be blocked, AND the path $1 \\to 2 \\to 3$ must be blocked. The path $1 \\to 2 \\to 3$ is blocked if edge $(1,2)$ is blocked OR edge $(2,3)$ is blocked.\n    Let $B_{uv}$ be the event that edge $(u,v)$ is blocked, so $P(B_{uv}) = 1-p_{uv}$.\n    $$ P(3 \\text{ is not reachable from } \\{1\\}) = P(B_{13} \\text{ and } (B_{12} \\text{ or } B_{23})) $$\n    Since the liveness of edge $(1,3)$ is independent of the other edges, we have:\n    $$ P(3 \\text{ is not reachable}) = P(B_{13}) \\times P(B_{12} \\text{ or } B_{23}) $$\n    $$ P(B_{12} \\text{ or } B_{23}) = 1 - P(\\text{not}(B_{12} \\text{ or } B_{23})) = 1 - P(\\text{not } B_{12} \\text{ and not } B_{23}) = 1 - p_{12}p_{23} $$\n    Therefore,\n    $$ P(3 \\text{ is not reachable}) = (1 - p_{13})(1 - p_{12}p_{23}) $$\n    And the probability that node $3$ is reachable is:\n    $$ P(3 \\text{ is reachable}) = 1 - (1 - p_{13})(1 - p_{12}p_{23}) $$\n    Substituting the values:\n    $p_{12}p_{23} = \\frac{1}{2} \\times \\frac{2}{5} = \\frac{1}{5}$.\n    $$ P(3 \\text{ is reachable}) = 1 - \\left(1 - \\frac{1}{5}\\right)\\left(1 - \\frac{1}{5}\\right) = 1 - \\left(\\frac{4}{5}\\right)\\left(\\frac{4}{5}\\right) = 1 - \\frac{16}{25} = \\frac{9}{25} $$\n\n4.  **Node 4:** Node $4$ is reachable from $1$ if and only if there's a live path from $1$ to $4$. The only edge leading to node $4$ is $(3,4)$. Thus, for node $4$ to be reachable from $1$, node $3$ must be reachable from $1$, AND the edge $(3,4)$ must be live.\n    Let $R_{13}$ be the event that node $3$ is reachable from $1$, and $L_{34}$ be the event that edge $(3,4)$ is live.\n    $$ P(4 \\text{ is reachable from } \\{1\\}) = P(R_{13} \\cap L_{34}) $$\n    The event $R_{13}$ depends on the states of edges $(1,2)$, $(2,3)$, and $(1,3)$. The event $L_{34}$ depends only on the state of edge $(3,4)$. Since these edge sets are disjoint, the events $R_{13}$ and $L_{34}$ are independent.\n    $$ P(4 \\text{ is reachable}) = P(R_{13}) \\times P(L_{34}) = P(3 \\text{ is reachable}) \\times p_{34} $$\n    Substituting the previously calculated probability for node $3$:\n    $$ P(4 \\text{ is reachable}) = \\frac{9}{25} \\times \\frac{3}{10} = \\frac{27}{250} $$\n\nFinally, we sum the probabilities for each node to find the expected total number of active nodes:\n$$ \\sigma_{\\mathrm{IC}}(\\{1\\}) = \\sum_{v \\in \\{1,2,3,4\\}} P(v \\text{ is reachable from } \\{1\\}) $$\n$$ \\sigma_{\\mathrm{IC}}(\\{1\\}) = P(1 \\text{ reachable}) + P(2 \\text{ reachable}) + P(3 \\text{ reachable}) + P(4 \\text{ reachable}) $$\n$$ \\sigma_{\\mathrm{IC}}(\\{1\\}) = 1 + \\frac{1}{2} + \\frac{9}{25} + \\frac{27}{250} $$\nTo sum these fractions, we find a common denominator, which is $250$.\n$$ \\sigma_{\\mathrm{IC}}(\\{1\\}) = \\frac{250}{250} + \\frac{125}{250} + \\frac{90}{250} + \\frac{27}{250} $$\n$$ \\sigma_{\\mathrm{IC}}(\\{1\\}) = \\frac{250 + 125 + 90 + 27}{250} = \\frac{492}{250} $$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor, which is $2$.\n$$ \\sigma_{\\mathrm{IC}}(\\{1\\}) = \\frac{246}{125} $$\nThe fraction is now in its simplest form, as the prime factorization of the numerator is $2 \\times 3 \\times 41$ and the denominator is $5^3$.",
            "answer": "$$\\boxed{\\frac{246}{125}}$$"
        },
        {
            "introduction": "The greedy algorithm offers a surprisingly effective solution to the influence maximization problem, which is known to be computationally difficult (NP-hard). This effectiveness, however, is not a given; it relies on a crucial property of the influence function called submodularity. This exercise explores the profound consequences of this property by analyzing a scenario where it is violated, allowing you to discover firsthand why the greedy algorithm's performance guarantee can fail. ",
            "id": "4309730",
            "problem": "You are studying influence maximization in a networked system under a deterministic aggregate measure of spread. Let the ground set be $V$, and let the influence spread function be a set function $\\sigma: 2^{V} \\to \\mathbb{R}_{\\ge 0}$ that maps a seed set $S \\subseteq V$ to its expected total number of activated nodes under a diffusion model. In the standard Independent Cascade (IC) model and Linear Threshold (LT) model, the function $\\sigma$ is known to satisfy two properties: monotonicity (adding seeds never decreases expected spread) and submodularity (the marginal gain of any seed weakly decreases as the context set grows).\n\nAssume a cardinality constraint $|S| \\le k$, where $k \\in \\mathbb{N}$ is fixed. Consider the greedy algorithm that starts with $S_{0} = \\varnothing$ and, for $t = 1, 2, \\dots, k$, adds an element $e_{t} \\in V \\setminus S_{t-1}$ that maximizes the marginal gain $\\sigma(S_{t-1} \\cup \\{e\\}) - \\sigma(S_{t-1})$. Let $S^{\\star}$ be an optimal seed set of size at most $k$ that maximizes $\\sigma(S)$.\n\nTasks:\n1. Starting only from the definitions of monotonicity and submodularity of $\\sigma$, and the above description of the greedy algorithm, derive a lower bound on $\\sigma(S_{k})$ in terms of $\\sigma(S^{\\star})$ and $k$. Your derivation must proceed from first principles of set function monotonicity and submodularity, and may not use any pre-stated approximation guarantees. Express the bound you derive as an exact symbolic expression in $k$.\n2. Construct a scientifically plausible modified influence measure $F: 2^{V} \\to \\mathbb{R}_{\\ge 0}$ that is monotone but not submodular. Use the following scenario: $V = \\{x_{1}, x_{2}, y_{1}, y_{2}\\}$ represents four potential seed accounts in a co-marketing campaign. Individual seeds have baseline reach, but the pair $\\{y_{1}, y_{2}\\}$ exhibits strong positive synergy when co-seeded (e.g., complementary audiences that amplify each otherâ€™s activation). Define $F$ by assigning the following values consistent with that story:\n- $F(\\varnothing) = 0$.\n- For singletons: $F(\\{x_{1}\\}) = 5$, $F(\\{x_{2}\\}) = 5$, $F(\\{y_{1}\\}) = 1$, $F(\\{y_{2}\\}) = 1$.\n- For the synergistic pair: $F(\\{y_{1}, y_{2}\\}) = F(\\{y_{1}\\}) + F(\\{y_{2}\\}) + 20$.\n- For any other set $S$, define $F(S)$ as the sum of singleton values of elements of $S$ plus the synergy bonus of $20$ if and only if both $y_{1}$ and $y_{2}$ are in $S$.\nProve that $F$ is monotone but not submodular by exhibiting a specific violation of the diminishing returns condition.\n3. Under the cardinality constraint $k = 2$, run the greedy algorithm on $F$ as defined above. Compute the greedy solution value $G = F(S_{2})$ and the optimal value $O = \\max_{|S| \\le 2} F(S)$. Finally, compute the exact approximation ratio $G/O$ as a single reduced fraction. Provide this ratio as your final answer.\n\nNo rounding is required. The final answer must be a single number or a single closed-form analytic expression without units or symbols beyond standard mathematical notation.",
            "solution": "This problem consists of three tasks related to influence maximization, submodularity, and the performance of a greedy algorithm. The problem is validated as scientifically grounded, well-posed, and objective. I will address each task in order.\n\nThe first task is to derive the approximation guarantee for the greedy algorithm for maximizing a monotone, submodular function $\\sigma: 2^{V} \\to \\mathbb{R}_{\\ge 0}$ subject to a cardinality constraint $|S| \\le k$. Let $S^{\\star}$ be an optimal seed set of size at most $k$, so $\\sigma(S^{\\star})$ is the maximum possible value. Let $S_i$ be the set of $i$ seeds selected by the greedy algorithm after $i$ iterations, for $i \\in \\{0, 1, \\dots, k\\}$. The greedy algorithm starts with $S_0 = \\varnothing$ and for each step $i$ from $1$ to $k$, it selects an element $e_i$ that maximizes the marginal gain, i.e., $e_i = \\arg\\max_{e \\in V \\setminus S_{i-1}} [\\sigma(S_{i-1} \\cup \\{e\\}) - \\sigma(S_{i-1})]$. Then $S_i = S_{i-1} \\cup \\{e_i\\}$. We also assume $\\sigma(\\varnothing) = 0$.\n\nLet us consider the value of the optimal solution, $\\sigma(S^{\\star})$. By the monotonicity of $\\sigma$, for any iteration $i$, we know that $\\sigma(S^{\\star}) \\le \\sigma(S_i \\cup S^{\\star})$. This implies:\n$$\n\\sigma(S^{\\star}) - \\sigma(S_i) \\le \\sigma(S_i \\cup S^{\\star}) - \\sigma(S_i)\n$$\nThe term on the right is the marginal gain of adding the entire set $S^{\\star}$ to the current greedy set $S_i$. By the property of submodularity, the marginal gain of a set is less than or equal to the sum of the marginal gains of its individual elements. Let $S^{\\star} \\setminus S_i$ be the elements of the optimal set not yet chosen by the greedy algorithm. We can write:\n$$\n\\sigma(S_i \\cup S^{\\star}) - \\sigma(S_i) = \\sigma(S_i \\cup (S^{\\star} \\setminus S_i)) - \\sigma(S_i) \\le \\sum_{u \\in S^{\\star} \\setminus S_i} [\\sigma(S_i \\cup \\{u\\}) - \\sigma(S_i)]\n$$\nAt step $i+1$, the greedy algorithm chooses the element $e_{i+1}$ that provides the maximum possible marginal gain with respect to $S_i$. Therefore, for any element $u \\in V \\setminus S_i$, and specifically for any $u \\in S^{\\star} \\setminus S_i$, the following holds:\n$$\n\\sigma(S_{i+1}) - \\sigma(S_i) \\ge \\sigma(S_i \\cup \\{u\\}) - \\sigma(S_i)\n$$\nSumming this inequality over all $u \\in S^{\\star} \\setminus S_i$:\n$$\n\\sum_{u \\in S^{\\star} \\setminus S_i} [\\sigma(S_i \\cup \\{u\\}) - \\sigma(S_i)] \\le \\sum_{u \\in S^{\\star} \\setminus S_i} [\\sigma(S_{i+1}) - \\sigma(S_i)] = |S^{\\star} \\setminus S_i| \\cdot (\\sigma(S_{i+1}) - \\sigma(S_i))\n$$\nThe size of the set $S^{\\star}$ is at most $k$, so $|S^{\\star} \\setminus S_i| \\le |S^{\\star}| \\le k$. Combining these inequalities, we get:\n$$\n\\sigma(S^{\\star}) - \\sigma(S_i) \\le k \\cdot (\\sigma(S_{i+1}) - \\sigma(S_i))\n$$\nLet us define the gap between the optimal value and the greedy value at step $i$ as $g_i = \\sigma(S^{\\star}) - \\sigma(S_i)$. Then $\\sigma(S_i) = \\sigma(S^{\\star}) - g_i$ and $\\sigma(S_{i+1}) = \\sigma(S^{\\star}) - g_{i+1}$. Substituting these into the inequality:\n$$\ng_i \\le k \\cdot ((\\sigma(S^{\\star}) - g_{i+1}) - (\\sigma(S^{\\star}) - g_i)) = k \\cdot (g_i - g_{i+1})\n$$\nRearranging this inequality to find a bound on $g_{i+1}$:\n$$\ng_i \\le k \\cdot g_i - k \\cdot g_{i+1} \\implies k \\cdot g_{i+1} \\le (k-1) \\cdot g_i \\implies g_{i+1} \\le \\left(1 - \\frac{1}{k}\\right) g_i\n$$\nThis is a recursive relationship. Starting from $i=0$, where $S_0 = \\varnothing$ and $\\sigma(S_0)=0$, we have $g_0 = \\sigma(S^{\\star}) - 0 = \\sigma(S^{\\star})$. Applying the recursion $k$ times:\n$$\ng_k \\le \\left(1 - \\frac{1}{k}\\right) g_{k-1} \\le \\left(1 - \\frac{1}{k}\\right)^2 g_{k-2} \\le \\dots \\le \\left(1 - \\frac{1}{k}\\right)^k g_0 = \\left(1 - \\frac{1}{k}\\right)^k \\sigma(S^{\\star})\n$$\nSubstituting back $g_k = \\sigma(S^{\\star}) - \\sigma(S_k)$:\n$$\n\\sigma(S^{\\star}) - \\sigma(S_k) \\le \\left(1 - \\frac{1}{k}\\right)^k \\sigma(S^{\\star})\n$$\nFinally, solving for $\\sigma(S_k)$ provides the lower bound:\n$$\n\\sigma(S_k) \\ge \\sigma(S^{\\star}) - \\left(1 - \\frac{1}{k}\\right)^k \\sigma(S^{\\star}) = \\sigma(S^{\\star}) \\left[1 - \\left(1 - \\frac{1}{k}\\right)^k\\right]\n$$\nThis is the desired lower bound on the performance of the greedy algorithm.\n\nThe second task is to construct and analyze a monotone but not submodular function $F$. The ground set is $V = \\{x_1, x_2, y_1, y_2\\}$. The function $F: 2^V \\to \\mathbb{R}_{\\ge 0}$ is defined by $F(\\varnothing) = 0$; singleton values $F(\\{x_1\\}) = 5$, $F(\\{x_2\\}) = 5$, $F(\\{y_1\\}) = 1$, $F(\\{y_2\\}) = 1$; and for any general set $S \\subseteq V$, $F(S) = \\sum_{v \\in S} F(\\{v\\}) + 20 \\cdot \\mathbb{I}(\\{y_1, y_2\\} \\subseteq S)$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n\nTo prove that $F$ is monotone, we must show that for any $A \\subseteq B \\subseteq V$, $F(A) \\le F(B)$. It is sufficient to show this for $B = A \\cup \\{e\\}$ where $e \\in V \\setminus A$. The marginal gain of adding $e$ to $A$ is:\n$$\nF(A \\cup \\{e\\}) - F(A) = F(\\{e\\}) + 20 \\cdot [\\mathbb{I}(\\{y_1, y_2\\} \\subseteq A \\cup \\{e\\}) - \\mathbb{I}(\\{y_1, y_2\\} \\subseteq A)]\n$$\nAll singleton values $F(\\{e\\})$ are positive (at least $1$). The term involving the indicator function is also non-negative, because if $\\{y_1, y_2\\} \\subseteq A$, then it must also be that $\\{y_1, y_2\\} \\subseteq A \\cup \\{e\\}$. Thus, the difference of the indicators is either $0$ or $1$. The marginal gain is therefore always greater than $0$, so $F(A \\cup \\{e\\}) > F(A)$. By induction, $F$ is strictly monotone.\n\nTo prove that $F$ is not submodular, we must find a counterexample to the submodularity condition, which states that for any $A \\subseteq B \\subseteq V$ and $e \\in V \\setminus B$, it must be that $F(A \\cup \\{e\\}) - F(A) \\ge F(B \\cup \\{e\\}) - F(B)$. Consider the synergistic elements $y_1$ and $y_2$. Let $e = y_2$. Let $A = \\varnothing$ and $B = \\{y_1\\}$. Clearly $A \\subseteq B$ and $e \\notin B$.\nLet's compute the marginal gain of $e$ with respect to $A$:\n$$\nF(A \\cup \\{e\\}) - F(A) = F(\\{y_2\\}) - F(\\varnothing) = 1 - 0 = 1\n$$\nNow, let's compute the marginal gain of $e$ with respect to $B$:\n$$\nF(B \\cup \\{e\\}) - F(B) = F(\\{y_1, y_2\\}) - F(\\{y_1\\})\n$$\nUsing the definition of $F$, we have $F(\\{y_1\\}) = 1$ and $F(\\{y_1, y_2\\}) = F(\\{y_1\\}) + F(\\{y_2\\}) + 20 = 1 + 1 + 20 = 22$.\nThe marginal gain is $22 - 1 = 21$.\nComparing the two marginal gains, we find that $1 < 21$. This means $F(A \\cup \\{e\\}) - F(A) < F(B \\cup \\{e\\}) - F(B)$, which is a violation of the submodularity condition. Therefore, $F$ is not submodular.\n\nThe third task is to run the greedy algorithm on $F$ with a cardinality constraint $k=2$, find the greedy solution value $G$, find the optimal solution value $O$, and compute the ratio $G/O$.\n\nGreedy Algorithm Run ($k=2$):\nStep 1: Start with $S_0 = \\varnothing$. We need to find the element with the highest marginal gain relative to $\\varnothing$. The gains are simply the singleton values:\n- Gain of $x_1$: $F(\\{x_1\\}) - F(\\varnothing) = 5 - 0 = 5$.\n- Gain of $x_2$: $F(\\{x_2\\}) - F(\\varnothing) = 5 - 0 = 5$.\n- Gain of $y_1$: $F(\\{y_1\\}) - F(\\varnothing) = 1 - 0 = 1$.\n- Gain of $y_2$: $F(\\{y_2\\}) - F(\\varnothing) = 1 - 0 = 1$.\nThe maximum gain is $5$, corresponding to either $x_1$ or $x_2$. Let's select $e_1=x_1$. The greedy set is now $S_1 = \\{x_1\\}$.\n\nStep 2: We need to find the element in $V \\setminus S_1 = \\{x_2, y_1, y_2\\}$ with the highest marginal gain relative to $S_1 = \\{x_1\\}$.\n- Gain of $x_2$: $F(\\{x_1, x_2\\}) - F(\\{x_1\\}) = (F(\\{x_1\\}) + F(\\{x_2\\})) - F(\\{x_1\\}) = (5+5)-5 = 5$.\n- Gain of $y_1$: $F(\\{x_1, y_1\\}) - F(\\{x_1\\}) = (F(\\{x_1\\}) + F(\\{y_1\\})) - F(\\{x_1\\}) = (5+1)-5 = 1$.\n- Gain of $y_2$: $F(\\{x_1, y_2\\}) - F(\\{x_1\\}) = (F(\\{x_1\\}) + F(\\{y_2\\})) - F(\\{x_1\\}) = (5+1)-5 = 1$.\nThe maximum marginal gain is $5$, corresponding to $x_2$. So we select $e_2=x_2$. The final greedy set is $S_2 = \\{x_1, x_2\\}$.\nThe value of the greedy solution is $G = F(S_2) = F(\\{x_1, x_2\\}) = 5 + 5 = 10$.\n\nOptimal Solution ($k=2$):\nWe need to find $O = \\max_{|S|=2} F(S)$. We evaluate $F(S)$ for all $\\binom{4}{2}=6$ possible pairs:\n- $F(\\{x_1, x_2\\}) = 5 + 5 = 10$.\n- $F(\\{x_1, y_1\\}) = 5 + 1 = 6$.\n- $F(\\{x_1, y_2\\}) = 5 + 1 = 6$.\n- $F(\\{x_2, y_1\\}) = 5 + 1 = 6$.\n- $F(\\{x_2, y_2\\}) = 5 + 1 = 6$.\n- $F(\\{y_1, y_2\\}) = 1 + 1 + 20 = 22$.\nThe maximum value among all pairs is $22$. So the optimal value is $O=22$, achieved by the set $S^{\\star} = \\{y_1, y_2\\}$.\n\nApproximation Ratio:\nThe greedy algorithm found a solution with value $G=10$, while the optimal solution has a value of $O=22$. The approximation ratio is:\n$$\n\\frac{G}{O} = \\frac{10}{22} = \\frac{5}{11}\n$$",
            "answer": "$$\\boxed{\\frac{5}{11}}$$"
        },
        {
            "introduction": "While exact influence calculation provides theoretical clarity, it is computationally intractable for most real-world networks. This practice bridges the gap between theory and application by introducing the Monte Carlo method, the standard approach for estimating influence in practice. You will implement the first step of the greedy algorithm by simulating the Independent Cascade process, a core skill for applying influence maximization to problems of a realistic scale. ",
            "id": "4309704",
            "problem": "You are given a directed network defined on the node set $V=\\{a,b,c,d\\}$ with independent activation probabilities on edges under the Independent Cascade model. The Independent Cascade model evolves in discrete steps: when a node $u$ becomes newly active, it receives exactly one chance to activate each currently inactive neighbor $v$ along a directed edge $(u,v)$, succeeding with probability equal to the edge weight $p_{uv}$, independently across edges and times. The activation attempts do not repeat for the same ordered pair $(u,v)$. The process continues until no further activations occur.\n\nThe network has the following directed edges and probabilities: $p_{ab}=0.4$, $p_{ac}=0.3$, $p_{bd}=0.5$, and $p_{cd}=0.2$, with no other edges present. Let $\\sigma(S)$ denote the expected number of activated nodes (the expected influence) resulting from a seed set $S\\subseteq V$ under the Independent Cascade model.\n\nThe Greedy algorithm for influence maximization chooses seed nodes sequentially by maximizing the marginal gain $\\Delta(v\\mid S)=\\sigma(S\\cup\\{v\\})-\\sigma(S)$ at each iteration. In its first iteration, when $S=\\emptyset$, this reduces to choosing $v\\in V$ that maximizes $\\sigma(\\{v\\})$.\n\nTask: Write a complete, runnable program that, for a given integer $m$, estimates $\\sigma(\\{v\\})$ for each $v\\in V$ using Monte Carlo simulation with exactly $m$ independent samples of the Independent Cascade process and then selects the node with the largest estimated marginal gain. If multiple nodes attain the same estimated marginal gain, break ties by choosing the node with the smallest index under the fixed mapping $a\\mapsto 0$, $b\\mapsto 1$, $c\\mapsto 2$, $d\\mapsto 3$. For reproducibility, for each test case use a pseudorandom number generator initialized to the seed $314159+m$ and ensure all randomness in the Monte Carlo simulation is generated from that seeded generator.\n\nFundamental base to use: the Independent Cascade model definition and the fact that the expected influence function $\\sigma(\\cdot)$ is monotone and submodular under this model. Monotonicity means $\\sigma(S)\\le \\sigma(T)$ whenever $S\\subseteq T$; submodularity means diminishing returns, i.e., for all $S\\subseteq T\\subseteq V$ and $v\\in V\\setminus T$, $\\sigma(S\\cup\\{v\\})-\\sigma(S)\\ge \\sigma(T\\cup\\{v\\})-\\sigma(T)$.\n\nYour program must implement the Independent Cascade process precisely as described above. Estimation for a given seed $\\{v\\}$ equals the arithmetic mean of $m$ independent cascade sizes started from $\\{v\\}$. If $m=0$, define all estimates to be $0$ and apply the specified tie-breaking rule.\n\nTest Suite:\n- Case 1: $m=0$\n- Case 2: $m=1$\n- Case 3: $m=10$\n- Case 4: $m=100$\n- Case 5: $m=5000$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets. Each result must be an integer corresponding to the selected node index under the fixed mapping stated above, in the same order as the test cases. For example, the output should look like $[r_1,r_2,r_3,r_4,r_5]$ where each $r_i$ is an integer in $\\{0,1,2,3\\}$. All quantities in this problem are dimensionless; no physical units or angle units are involved. Percentages must not be used anywhere; all probabilities are to be treated as real numbers in $[0,1]$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of influence maximization in networks, specifically using the Independent Cascade (IC) model. The problem is well-posed, providing a complete and consistent set of definitions, constraints, and objectives, including a clear tie-breaking rule and a specification for reproducibility, which guarantees a unique and verifiable solution. All terms are defined unambiguously, and the task is a standard computational problem within the specified domain.\n\nThe task is to determine the optimal initial seed node for influence maximization by implementing the first step of the Greedy algorithm. The Greedy algorithm is a foundational method for the influence maximization problem, which is known to be NP-hard. The algorithm works by iteratively selecting the node that provides the largest marginal gain in expected influence. The expected influence of a seed set $S$, denoted $\\sigma(S)$, is the average number of nodes that will be active at the end of the cascade process, starting from the initial activation of nodes in $S$.\n\nFor the first step of the Greedy algorithm, the initial seed set is empty, $S = \\emptyset$. We seek to find the node $v \\in V$ that maximizes the marginal gain:\n$$ \\Delta(v \\mid \\emptyset) = \\sigma(\\{v\\}) - \\sigma(\\emptyset) $$\nSince no nodes are active if the seed set is empty, $\\sigma(\\emptyset) = 0$. The task simplifies to finding the node $v$ that maximizes its own expected influence, $\\sigma(\\{v\\})$.\n$$ v^* = \\arg\\max_{v \\in V} \\sigma(\\{v\\}) $$\n\nDirectly calculating the expected influence $\\sigma(S)$ is computationally intractable for general graphs (#P-hard). Therefore, the problem specifies the use of Monte Carlo simulation for its estimation. The estimate of $\\sigma(\\{v\\})$, denoted $\\hat{\\sigma}_m(\\{v\\})$, is obtained by running the Independent Cascade process $m$ times with $\\{v\\}$ as the seed set and averaging the total number of activated nodes over these $m$ runs.\nLet $N_i(\\{v\\})$ be the final number of activated nodes in the $i$-th simulation run starting from seed $v$. The estimate is:\n$$ \\hat{\\sigma}_m(\\{v\\}) = \\frac{1}{m} \\sum_{i=1}^{m} N_i(\\{v\\}) $$\nFor the special case where $m=0$, the problem defines $\\hat{\\sigma}_0(\\{v\\}) = 0$ for all $v \\in V$.\n\nThe Independent Cascade model itself is a stochastic process evolving in discrete steps. Given a set of active nodes, the process unfolds as follows:\n1.  Initialize with a seed set $S_0$. All nodes in $S_0$ are active.\n2.  In each step, every newly activated node $u$ gets a single chance to activate each of its currently inactive neighbors $v$ through the directed edge $(u,v)$.\n3.  Each activation attempt is an independent Bernoulli trial which succeeds with probability $p_{uv}$, the weight of the edge.\n4.  The process continues until no new nodes can be activated in a step. The final set of active nodes is the union of all nodes activated throughout the process.\n\nOur algorithm will proceed as follows for each value of $m$ in the test suite:\n1.  Handle the base case $m=0$. In this case, all estimated influences are $0$. By the specified tie-breaking rule (smallest index), we select node $a$, which corresponds to index $0$.\n2.  For any $m > 0$, initialize a pseudorandom number generator with the specified seed, $314159+m$, to ensure reproducibility.\n3.  Define the network structure, $V=\\{a,b,c,d\\}$ and the given edges with their probabilities. We use the mapping $a \\mapsto 0, b \\mapsto 1, c \\mapsto 2, d \\mapsto 3$.\n4.  For each node $v \\in V$:\n    a. We will estimate $\\sigma(\\{v\\})$ by performing $m$ simulation runs.\n    b. Each run simulates one cascade starting from seed $\\{v\\}$. We can implement the cascade using a breadth-first traversal. A queue will store newly activated nodes whose neighbors are yet to be processed. A set will track all nodes that have become active in the current cascade to prevent re-activation.\n    c. After $m$ runs, we compute the average cascade size, which is our estimate $\\hat{\\sigma}_m(\\{v\\})$.\n5.  After computing the estimated influence for all four nodes, we identify the node with the maximum estimated influence. If there is a tie, we select the node with the lowest index.\n6.  The index of the selected node is recorded as the result for the given $m$.\n\nThis procedure is repeated for all specified test cases, and the final results are collected into a single list. The properties of monotonicity and submodularity of $\\sigma(\\cdot)$ are the theoretical underpinnings that guarantee the Greedy algorithm provides a $(1 - 1/e)$-approximation for the full influence maximization problem, but they are not directly used in the computation for this specific task, which only concerns the first step of the algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the influence maximization problem for the first seed node\n    using Monte Carlo simulation of the Independent Cascade model.\n    \"\"\"\n\n    # Define the graph structure and probabilities.\n    # Node mapping: a->0, b->1, c->2, d->3\n    graph = {\n        0: [(1, 0.4), (2, 0.3)],  # a -> b, a -> c\n        1: [(3, 0.5)],             # b -> d\n        2: [(3, 0.2)],             # c -> d\n        3: []                      # d has no outgoing edges\n    }\n    num_nodes = 4\n\n    def run_ic_cascade(start_node_idx, rng):\n        \"\"\"\n        Runs a single instance of the Independent Cascade model.\n\n        Args:\n            start_node_idx (int): The index of the seed node.\n            rng (numpy.random.Generator): The random number generator to use.\n\n        Returns:\n            int: The total number of activated nodes in the cascade.\n        \"\"\"\n        active_nodes = {start_node_idx}\n        # Use a list as a queue for nodes whose neighbors need to be processed.\n        # This simulates the discrete steps of the cascade in a breadth-first manner.\n        queue = [start_node_idx]\n        head = 0  # Use an index to simulate dequeuing for efficiency.\n\n        while head < len(queue):\n            current_node = queue[head]\n            head += 1\n\n            for neighbor, prob in graph.get(current_node, []):\n                if neighbor not in active_nodes:\n                    # Perform the activation attempt.\n                    if rng.random() < prob:\n                        active_nodes.add(neighbor)\n                        queue.append(neighbor)\n        \n        return len(active_nodes)\n\n    def estimate_influence(m):\n        \"\"\"\n        Estimates the influence for each node and selects the best one.\n\n        Args:\n            m (int): The number of Monte Carlo simulations.\n\n        Returns:\n            int: The index of the node with the highest estimated influence.\n        \"\"\"\n        if m == 0:\n            # All estimates are 0, tie-break picks the smallest index.\n            return 0\n        \n        # Initialize the pseudorandom number generator with the specified seed.\n        rng = np.random.default_rng(seed=314159 + m)\n        \n        estimated_influences = []\n        \n        for node_idx in range(num_nodes):\n            total_activated_count = 0\n            for _ in range(m):\n                total_activated_count += run_ic_cascade(node_idx, rng)\n            \n            # The estimate is the arithmetic mean.\n            influence = total_activated_count / m\n            estimated_influences.append(influence)\n            \n        # Find the index of the node with the maximum estimated influence.\n        # np.argmax() breaks ties by returning the index of the first maximum,\n        # which complies with the problem's tie-breaking rule.\n        best_node_idx = np.argmax(estimated_influences)\n        return best_node_idx\n\n    # Define the test cases from the problem statement.\n    test_cases = [0, 1, 10, 100, 5000]\n    \n    results = []\n    for m in test_cases:\n        result = estimate_influence(m)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}