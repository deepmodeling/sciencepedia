{
    "hands_on_practices": [
        {
            "introduction": "基于随机游走的嵌入方法，其核心思想是将图的拓扑结构转化为线性节点序列，然后利用语言模型从中学习节点表示。本练习将引导你手动完成这一转化过程。通过在一个简单的路径图上模拟一次随机游走，并应用 Skip-Gram 模型的上下文窗口，你将亲手生成中心词与上下文的配对，从而直观地理解结构性邻近是如何被编码为序列共现信息的。",
            "id": "4300089",
            "problem": "考虑路径图 $P_{5}$，其顶点集为 $\\{v_{1}, v_{2}, v_{3}, v_{4}, v_{5}\\}$，无向边集为 $\\{(v_{1}, v_{2}), (v_{2}, v_{3}), (v_{3}, v_{4}), (v_{4}, v_{5})\\}$。在 $P_{5}$ 上的一个简单随机游走是一个马尔可夫链，在每个内部顶点 $v_{i}$（其中 $i \\in \\{2,3,4\\}$），它以相等的概率移动到任一邻居；在每个端点 $v_{1}$ 或 $v_{5}$，它确定性地移动到唯一的邻居。在 DeepWalk 采样方案中，一个长度为 $L$ 的游走是由此随机游走生成的一个包含 $L$ 个顶点的序列，从一个初始顶点开始，并在记录了 $L$ 个顶点后截断。\n\n你将模拟一次从 $v_{3}$ 开始的长度为 $L=6$ 的截断游走，其内部移动遵循以下确定的结果：在 $v_{3}$ 处，移动到索引更高的邻居 $v_{4}$；在 $v_{4}$ 处，移动到索引更高的邻居 $v_{5}$；在 $v_{5}$ 处，移动被强制到 $v_{4}$；在 $v_{4}$ 处，移动到索引更低的邻居 $v_{3}$；在 $v_{3}$ 处，移动到索引更低的邻居 $v_{2}$。这将产生一个包含 $L=6$ 个顶点的序列。使用对称窗口大小 $w=2$ 的 Skip-Gram 风格上下文，定义有序对的经验共现多重集如下：对于序列中每个位置 $i \\in \\{1,\\dots,L\\}$（中心顶点为 $s_{i}$），包括所有满足 $1 \\leq |j-i| \\leq w$ 和 $1 \\leq j \\leq L$ 的索引 $j$ 对应的有序对 $(s_{i}, s_{j})$。经验共现计数 $C(u,v)$ 是有序对 $(u,v)$ 在此多重集中出现的次数。\n\n任务：\n- 写下生成的长度为 $L=6$ 的游走序列。\n- 列出由大小为 $w=2$ 的对称窗口生成的所有有序上下文对。\n- 计算出现的所有有序对 $(u,v)$ 的完整经验共现计数 $C(u,v)$。\n- 最后，报告单个经验共现计数 $C(v_{4}, v_{4})$。\n\n将最终答案表示为一个实数。无需四舍五入，也不涉及任何物理单位。",
            "solution": "我们从图上简单随机游走的定义开始，它是一个时齐马尔可夫链，其转移概率由邻接结构决定。在路径图 $P_{5}$ 上，转移规则是：在内部顶点 $v_{i}$（其中 $i \\in \\{2,3,4\\}$），以各 $\\frac{1}{2}$ 的概率移动到 $v_{i-1}$ 或 $v_{i+1}$；在端点 $v_{1}$（或 $v_{5}$），以概率 $1$ 移动到 $v_{2}$（或 $v_{4}$）。长度为 $L$ 的截断游走是一个有限序列 $(s_{1}, s_{2}, \\dots, s_{L})$，其中 $s_{1}$ 是起始顶点，$s_{i+1}$ 是根据转移规则得到的下一个顶点，直到记录了 $L$ 个顶点为止。\n\n问题指定了确定性移动以避免随机性歧义：从 $v_{3}$ 开始，然后移动到 $v_{4}$，再移动到 $v_{5}$，然后（在端点处强制）移动到 $v_{4}$，再移动到 $v_{3}$，最后移动到 $v_{2}$。因此，长度为 $L=6$ 的游走序列是\n$$\n(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}, s_{6}) = (v_{3}, v_{4}, v_{5}, v_{4}, v_{3}, v_{2}).\n$$\n\n接下来，我们使用大小为 $w=2$ 的对称窗口构建 Skip-Gram 风格的上下文对。对于每个位置 $i \\in \\{1,\\dots,6\\}$，我们包括所有满足 $1 \\leq |j-i| \\leq 2$ 和 $1 \\leq j \\leq 6$ 的索引 $j$ 对应的有序对 $(s_{i}, s_{j})$。我们按位置枚举所有这些对。\n\n- 对于 $i=1$，$s_{1}=v_{3}$。有效的 $j$ 是 $j=2$ 和 $j=3$（因为 $|j-1| \\in \\{1,2\\}$ 且在界限内）。有序对：\n  $$\n  (v_{3}, v_{4}),\\quad (v_{3}, v_{5}).\n  $$\n\n- 对于 $i=2$，$s_{2}=v_{4}$。有效的 $j$ 是 $j=1,3,4$。有序对：\n  $$ \n  (v_{4}, v_{3}),\\quad (v_{4}, v_{5}),\\quad (v_{4}, v_{4}).\n  $$\n\n- 对于 $i=3$，$s_{3}=v_{5}$。有效的 $j$ 是 $j=1,2,4,5$。有序对：\n  $$\n  (v_{5}, v_{3}),\\quad (v_{5}, v_{4}),\\quad (v_{5}, v_{4}),\\quad (v_{5}, v_{3}).\n  $$\n\n- 对于 $i=4$，$s_{4}=v_{4}$。有效的 $j$ 是 $j=2,3,5,6$。有序对：\n  $$\n  (v_{4}, v_{4}),\\quad (v_{4}, v_{5}),\\quad (v_{4}, v_{3}),\\quad (v_{4}, v_{2}).\n  $$\n\n- 对于 $i=5$，$s_{5}=v_{3}$。有效的 $j$ 是 $j=3,4,6$。有序对：\n  $$\n  (v_{3}, v_{5}),\\quad (v_{3}, v_{4}),\\quad (v_{3}, v_{2}).\n  $$\n\n- 对于 $i=6$，$s_{6}=v_{2}$。有效的 $j$ 是 $j=4,5$。有序对：\n  $$\n  (v_{2}, v_{4}),\\quad (v_{2}, v_{3}).\n  $$\n\n收集从 $i=1$ 到 $i=6$ 的所有有序对，上下文对的多重集为\n$$\n\\{(v_{3}, v_{4}), (v_{3}, v_{5}), (v_{4}, v_{3}), (v_{4}, v_{5}), (v_{4}, v_{4}), (v_{5}, v_{3}), (v_{5}, v_{4}), (v_{5}, v_{4}), (v_{5}, v_{3}), (v_{4}, v_{4}), (v_{4}, v_{5}), (v_{4}, v_{3}), (v_{4}, v_{2}), (v_{3}, v_{5}), (v_{3}, v_{4}), (v_{3}, v_{2}), (v_{2}, v_{4}), (v_{2}, v_{3})\\}.\n$$\n\n现在我们为每个出现的有序对 $(u,v)$ 计算经验共现计数 $C(u,v)$。将相同的对分组并计数其重数：\n\n- 中心为 $v_{2}$：\n  $$\n  C(v_{2}, v_{4}) = 1,\\quad C(v_{2}, v_{3}) = 1.\n  $$\n\n- 中心为 $v_{3}$：\n  $$\n  C(v_{3}, v_{4}) = 2,\\quad C(v_{3}, v_{5}) = 2,\\quad C(v_{3}, v_{2}) = 1.\n  $$\n\n- 中心为 $v_{4}$：\n  $$\n  C(v_{4}, v_{3}) = 2,\\quad C(v_{4}, v_{5}) = 2,\\quad C(v_{4}, v_{4}) = 2,\\quad C(v_{4}, v_{2}) = 1.\n  $$\n\n- 中心为 $v_{5}$：\n  $$\n  C(v_{5}, v_{3}) = 2,\\quad C(v_{5}, v_{4}) = 2.\n  $$\n\n在这次单次游走中，所有其他未列出的有序对 $(u,v)$ 的经验计数均为 $0$。\n\n最终答案所要求的量是 $C(v_{4}, v_{4})$，即有序对 $(v_{4}, v_{4})$ 的经验共现计数。根据上面的统计，我们得到\n$$\nC(v_{4}, v_{4}) = 2.\n$$",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "虽然 DeepWalk 中的无偏随机游走能捕捉基本的邻近关系，但 node2vec 通过引入返回参数 $p$ 和进出参数 $q$ 提供了更精细的控制。这种二阶随机游走策略能够在“同质性”（homophily）和“结构对等性”（structural equivalence）之间取得平衡。本练习将让你亲手计算 node2vec 在给定参数下的转移概率，从而深入理解算法是如何通过偏置游走来灵活地探索网络结构，并捕捉不同尺度的节点关系的。",
            "id": "4300077",
            "problem": "考虑一个嵌入在更大网络中的无向加权图，其节点集为 $V=\\{t,v,a,b,c,d\\}$。在一个二阶随机游走中，您当前位于节点 $v$，前一个节点是 $t$。$v$ 的邻域是 $N(v)=\\{a,b,c,d\\}$。从 $t$到这些邻居节点的最短路径距离分别为 $d(t,a)=1$、$d(t,b)=1$、$d(t,c)=2$ 和 $d(t,d)=2$。假设该图使用对称的度归一化边权重，定义为 $w_{uv}=1/\\sqrt{\\deg(u)\\deg(v)}$，其中 $u$ 和 $v$ 是任意两个相邻节点，$\\deg(\\cdot)$ 表示节点度。节点度为 $\\deg(v)=6$、$\\deg(a)=3$、$\\deg(b)=4$、$\\deg(c)=2$ 和 $\\deg(d)=5$。`node2vec` 的超参数为 $(p,q)=(3,\\tfrac{1}{2})$。使用标准的 `node2vec` 二阶随机游走规则，该规则根据从 $t$ 到候选下一步的最短路径距离来偏置转移，计算：\n- 从 $v$到每个邻居（按 $(a,b,c,d)$ 的顺序）的未归一化转移权重，以及\n- 从 $v$到 $(a,b,c,d)$ 的相应归一化转移概率。\n所有结果均需精确表示（不进行四舍五入）。以单行矩阵的形式提供最终答案，顺序为 $[w(a),w(b),w(c),w(d),P(a),P(b),P(c),P(d)]$，其中 $w(\\cdot)$ 是未归一化的权重，$P(\\cdot)$ 是归一化的概率。",
            "solution": "问题陈述已经过验证，被认为是自洽的、有科学依据且表述清晰的。计算所需量所需的所有必要参数和定义均已提供。\n\n该问题要求计算 `node2vec` 二阶随机游走的未归一化转移权重和归一化转移概率。游走当前位于节点 $v$，其前一个节点是 $t$。潜在的下一个节点是 $v$ 的邻居，即 $N(v) = \\{a, b, c, d\\}$。\n\n对于从 $v$ 到邻居 $x$ 的转移，我们将其未归一化转移权重表示为 $\\pi_{vx}$，它是 `node2vec` 搜索偏置 $\\alpha_{pq}(t,x)$ 与基础边权重 $w_{vx}$ 的乘积。\n$$ \\pi_{vx} = \\alpha_{pq}(t,x) \\cdot w_{vx} $$\n\n搜索偏置 $\\alpha_{pq}(t,x)$ 由前一个节点 $t$ 和候选下一个节点 $x$ 之间的最短路径距离 $d(t,x)$ 决定。给定超参数 $p=3$ 和 $q=\\frac{1}{2}$，规则如下：\n$$\n\\alpha_{pq}(t,x) =\n\\begin{cases}\n\\frac{1}{p} = \\frac{1}{3}  \\text{if } d(t,x) = 0 \\\\\n1  \\text{if } d(t,x) = 1 \\\\\n\\frac{1}{q} = 2  \\text{if } d(t,x) = 2\n\\end{cases}\n$$\n\n任意两个相邻节点 $u$ 和 $v$ 之间的基础边权重 $w_{uv}$ 被定义为对称的度归一化权重：\n$$ w_{uv} = \\frac{1}{\\sqrt{\\deg(u)\\deg(v)}} $$\n\n给定的数据包括：\n- 相关节点的度：$\\deg(v)=6$、$\\deg(a)=3$、$\\deg(b)=4$、$\\deg(c)=2$、$\\deg(d)=5$。\n- 从 $t$ 出发的最短路径距离：$d(t,a)=1$、$d(t,b)=1$、$d(t,c)=2$、$d(t,d)=2$。\n\n我们现在将为每个邻居 $x \\in \\{a, b, c, d\\}$ 计算未归一化的转移权重 $\\pi_{vx}$。\n\n1.  **对于邻居 $a$**：\n    距离为 $d(t,a)=1$，因此偏置因子为 $\\alpha_{pq}(t,a)=1$。\n    基础边权重为 $w_{va} = \\frac{1}{\\sqrt{\\deg(v)\\deg(a)}} = \\frac{1}{\\sqrt{6 \\cdot 3}} = \\frac{1}{\\sqrt{18}} = \\frac{1}{3\\sqrt{2}}$。\n    分母有理化后得到 $w_{va} = \\frac{\\sqrt{2}}{6}$。\n    未归一化的转移权重为 $\\pi_{va} = 1 \\cdot \\frac{\\sqrt{2}}{6} = \\frac{\\sqrt{2}}{6}$。\n\n2.  **对于邻居 $b$**：\n    距离为 $d(t,b)=1$，因此偏置因子为 $\\alpha_{pq}(t,b)=1$。\n    基础边权重为 $w_{vb} = \\frac{1}{\\sqrt{\\deg(v)\\deg(b)}} = \\frac{1}{\\sqrt{6 \\cdot 4}} = \\frac{1}{\\sqrt{24}} = \\frac{1}{2\\sqrt{6}}$。\n    分母有理化后得到 $w_{vb} = \\frac{\\sqrt{6}}{12}$。\n    未归一化的转移权重为 $\\pi_{vb} = 1 \\cdot \\frac{\\sqrt{6}}{12} = \\frac{\\sqrt{6}}{12}$。\n\n3.  **对于邻居 $c$**：\n    距离为 $d(t,c)=2$，因此偏置因子为 $\\alpha_{pq}(t,c)=\\frac{1}{q} = 2$。\n    基础边权重为 $w_{vc} = \\frac{1}{\\sqrt{\\deg(v)\\deg(c)}} = \\frac{1}{\\sqrt{6 \\cdot 2}} = \\frac{1}{\\sqrt{12}} = \\frac{1}{2\\sqrt{3}}$。\n    分母有理化后得到 $w_{vc} = \\frac{\\sqrt{3}}{6}$。\n    未归一化的转移权重为 $\\pi_{vc} = 2 \\cdot \\frac{\\sqrt{3}}{6} = \\frac{\\sqrt{3}}{3}$。\n\n4.  **对于邻居 $d$**：\n    距离为 $d(t,d)=2$，因此偏置因子为 $\\alpha_{pq}(t,d)=\\frac{1}{q} = 2$。\n    基础边权重为 $w_{vd} = \\frac{1}{\\sqrt{\\deg(v)\\deg(d)}} = \\frac{1}{\\sqrt{6 \\cdot 5}} = \\frac{1}{\\sqrt{30}}$。\n    分母有理化后得到 $w_{vd} = \\frac{\\sqrt{30}}{30}$。\n    未归一化的转移权重为 $\\pi_{vd} = 2 \\cdot \\frac{\\sqrt{30}}{30} = \\frac{\\sqrt{30}}{15}$。\n\n未归一化的转移权重为：\n$w(a) = \\frac{\\sqrt{2}}{6}$, $w(b) = \\frac{\\sqrt{6}}{12}$, $w(c) = \\frac{\\sqrt{3}}{3}$, $w(d) = \\frac{\\sqrt{30}}{15}$。\n\n接下来，我们计算归一化的转移概率。转移到邻居 $x$ 的概率由 $P(v,x) = \\frac{\\pi_{vx}}{Z}$ 给出，其中 $Z$ 是归一化常数，即从 $v$ 出发的所有未归一化转移权重的总和。\n$$ Z = \\sum_{y \\in N(v)} \\pi_{vy} = \\pi_{va} + \\pi_{vb} + \\pi_{vc} + \\pi_{vd} $$\n$$ Z = \\frac{\\sqrt{2}}{6} + \\frac{\\sqrt{6}}{12} + \\frac{\\sqrt{3}}{3} + \\frac{\\sqrt{30}}{15} $$\n为了对这些分数求和，我们找到一个公分母，即 $\\text{lcm}(6, 12, 3, 15) = 60$。\n$$ Z = \\frac{10\\sqrt{2}}{60} + \\frac{5\\sqrt{6}}{60} + \\frac{20\\sqrt{3}}{60} + \\frac{4\\sqrt{30}}{60} = \\frac{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}{60} $$\n\n现在，我们计算归一化的概率：\n1.  **到 $a$ 的概率**：\n    $P(a) = \\frac{\\pi_{va}}{Z} = \\frac{\\sqrt{2}/6}{Z} = \\frac{\\sqrt{2}}{6} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{10\\sqrt{2}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$。\n\n2.  **到 $b$ 的概率**：\n    $P(b) = \\frac{\\pi_{vb}}{Z} = \\frac{\\sqrt{6}/12}{Z} = \\frac{\\sqrt{6}}{12} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{5\\sqrt{6}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$。\n\n3.  **到 $c$ 的概率**：\n    $P(c) = \\frac{\\pi_{vc}}{Z} = \\frac{\\sqrt{3}/3}{Z} = \\frac{\\sqrt{3}}{3} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{20\\sqrt{3}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$。\n\n4.  **到 $d$ 的概率**：\n    $P(d) = \\frac{\\pi_{vd}}{Z} = \\frac{\\sqrt{30}/15}{Z} = \\frac{\\sqrt{30}}{15} \\cdot \\frac{60}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} = \\frac{4\\sqrt{30}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}$。\n\n最终结果是未归一化权重和归一化概率的有序集合。",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{\\sqrt{2}}{6}  \\frac{\\sqrt{6}}{12}  \\frac{\\sqrt{3}}{3}  \\frac{\\sqrt{30}}{15}  \\frac{10\\sqrt{2}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{5\\sqrt{6}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{20\\sqrt{3}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}}  \\frac{4\\sqrt{30}}{10\\sqrt{2} + 5\\sqrt{6} + 20\\sqrt{3} + 4\\sqrt{30}} \\end{pmatrix}} $$"
        },
        {
            "introduction": "生成随机游走序列只是第一步，真正的学习发生在嵌入模型的训练过程中。负采样 Skip-Gram (SGNS) 模型是这些方法的核心引擎，它通过优化一个目标函数来调整节点向量。本练习将带你深入该模型的数学核心，通过推导 SGNS 损失函数相对于上下文向量的梯度，你将清晰地看到嵌入向量是如何被更新的——即在向量空间中拉近（吸引）共现节点，同时推远（排斥）非共现节点。",
            "id": "4300053",
            "problem": "考虑一个连通无向图 $G = (V, E)$，其节点数为 $|V| = n$。通过固定长度为 $L$ 的截断随机游走和大小为 $w$ 的对称上下文窗口，生成一个节点序列语料库，这会产生训练事件，其中中心节点 $u \\in V$ 与在游走过程中于该窗口内观察到的上下文节点 $c \\in V$ 配对。假设使用带负采样的 Skip-Gram (SGNS) 目标来学习两个嵌入函数：中心节点的源嵌入 $\\mathbf{u} \\in \\mathbb{R}^{d}$ 和上下文节点的上下文嵌入 $\\mathbf{c} \\in \\mathbb{R}^{d}$。对于每个观察到的正样本对 $(u, c)$，从一个固定的噪声分布 $Q$（在 $V$ 上）中抽取 $k$ 个独立的负上下文节点 $\\{n_{i}\\}_{i=1}^{k}$，并将其上下文嵌入表示为 $\\{\\mathbf{n}_{i}\\}_{i=1}^{k} \\subset \\mathbb{R}^{d}$。此单个训练事件的 SGNS 损失为\n$$\n\\ell(\\mathbf{u}, \\mathbf{c}, \\{\\mathbf{n}_{i}\\}_{i=1}^{k}) \\;=\\; -\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\;-\\;\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big),\n$$\n其中 $\\sigma(x) = \\frac{1}{1+\\exp(-x)}$ 是 logistic sigmoid 函数，$\\exp(\\cdot)$ 表示指数函数。从二元逻辑交叉熵的第一性原理出发，仅使用基本微积分法则，推导损失函数关于上下文嵌入 $\\mathbf{c}$ 的梯度。用 $\\mathbf{u}$、$\\mathbf{c}$ 和 $\\sigma(\\cdot)$ 将最终结果表示为一个闭式符号表达式。然后，基于推导出的表达式和 SGNS 损失的结构，解释 sigmoid 项如何调节正样本对嵌入之间的引力以及相对于负样本的斥力。\n\n你的最终答案必须是 $\\nabla_{\\mathbf{c}}\\ell$ 的解析表达式。不需要进行数值计算。",
            "solution": "我们首先回顾单个训练事件的 SGNS 损失，该事件包含一个正样本对 $(u, c)$ 和 $k$ 个负样本 $\\{n_{i}\\}_{i=1}^{k}$：\n$$\n\\ell(\\mathbf{u}, \\mathbf{c}, \\{\\mathbf{n}_{i}\\}_{i=1}^{k}) \\;=\\; -\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\;-\\;\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big).\n$$\n我们将从第一性原理出发，使用链式法则和 logistic sigmoid 函数的导数来推导 $\\nabla_{\\mathbf{c}} \\ell$。设 $s = \\mathbf{u}^{\\top}\\mathbf{c}$。第一项通过 $s$ 依赖于 $\\mathbf{c}$，而第二项仅依赖于 $\\mathbf{u}$ 和负上下文嵌入 $\\{\\mathbf{n}_{i}\\}_{i=1}^{k}$；因此，它关于 $\\mathbf{c}$ 的梯度为零。\n\n我们回顾 logistic sigmoid 函数及其导数：\n$$\n\\sigma(x) \\;=\\; \\frac{1}{1+\\exp(-x)},\\qquad \\frac{d}{dx}\\sigma(x) \\;=\\; \\sigma(x)\\big(1 - \\sigma(x)\\big).\n$$\nsigmoid 函数的对数的导数可通过链式法则得到：\n$$\n\\frac{d}{dx}\\ln\\!\\big(\\sigma(x)\\big) \\;=\\; \\frac{1}{\\sigma(x)}\\,\\sigma'(x) \\;=\\; \\frac{1}{\\sigma(x)}\\,\\sigma(x)\\big(1 - \\sigma(x)\\big) \\;=\\; 1 - \\sigma(x).\n$$\n现在我们可以计算第一项关于 $\\mathbf{c}$ 的梯度：\n$$\n\\nabla_{\\mathbf{c}}\\Big(-\\,\\ln\\!\\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\big)\\Big)\n\\;=\\;\n-\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\nabla_{\\mathbf{c}}\\!\\big(\\mathbf{u}^{\\top}\\mathbf{c}\\big)\n\\;=\\;\n-\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\mathbf{u}.\n$$\n因为负样本项不依赖于 $\\mathbf{c}$，所以它关于 $\\mathbf{c}$ 的梯度为零：\n$$\n\\nabla_{\\mathbf{c}}\\Big(-\\,\\sum_{i=1}^{k}\\ln\\!\\big(\\sigma(-\\,\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\big)\\Big) \\;=\\; \\mathbf{0}.\n$$\n因此，关于 $\\mathbf{c}$ 的总梯度为\n$$\n\\nabla_{\\mathbf{c}}\\ell \\;=\\; -\\,\\Big(1 - \\sigma(\\mathbf{u}^{\\top}\\mathbf{c})\\Big)\\,\\mathbf{u}\n\\;=\\;\n\\Big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1\\Big)\\,\\mathbf{u}.\n$$\n\n关于 sigmoid 项调节引力和斥力的解释：系数 $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ 乘以方向 $\\mathbf{u}$。当 $\\mathbf{u}^{\\top}\\mathbf{c}$ 很小时，$\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})$ 接近于 $0$，因此 $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ 为负，梯度下降更新会使 $\\mathbf{c}$ 朝着 $+\\mathbf{u}$ 的方向移动（因为更新规则为 $\\mathbf{c} \\leftarrow \\mathbf{c} - \\eta \\nabla_{\\mathbf{c}}\\ell$，其中 $\\eta  0$），从而增加 $\\mathbf{u}^{\\top}\\mathbf{c}$ 的值，进而在正样本对 $(u, c)$ 之间产生引力。随着 $\\mathbf{u}^{\\top}\\mathbf{c}$ 的增大，$\\sigma(\\mathbf{u}^{\\top}\\mathbf{c})$ 趋近于 $1$，系数 $\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1$ 趋于 $0$，使得引力饱和。\n\n相对于负样本的斥力出现在与负样本直接相互作用的参数的梯度中。例如，对于评分为 $t = \\mathbf{u}^{\\top}\\mathbf{n}$ 的单个负样本 $\\mathbf{n}$，其导数\n$$\n\\nabla_{\\mathbf{n}}\\Big(-\\ln\\big(\\sigma(-t)\\big)\\Big) \\;=\\; \\Big(1 - \\sigma(-t)\\Big)\\,\\mathbf{u}\n\\;=\\; \\sigma(t)\\,\\mathbf{u}\n$$\n在 $\\mathbf{u}$ 上有一个正系数 $\\sigma(t)$，因此对 $\\mathbf{n}$ 的梯度下降更新会使其朝 $-\\mathbf{u}$ 方向移动，从而减小 $t$ 并产生与 $\\mathbf{u}$ 的斥力。同样地，关于 $\\mathbf{u}$ 的梯度既包含引力项 $(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1)\\mathbf{c}$，也包含斥力项 $\\sum_{i=1}^{k}\\sigma(\\mathbf{u}^{\\top}\\mathbf{n}_{i})\\mathbf{n}_{i}$，从而在对正样本的引力和对负样本的斥力之间取得平衡。相比之下，对于此处考虑的正样本对的上下文嵌入 $\\mathbf{c}$，只有由 sigmoid 调节的引力项有贡献。",
            "answer": "$$\\boxed{\\nabla_{\\mathbf{c}}\\ell \\;=\\; \\big(\\sigma(\\mathbf{u}^{\\top}\\mathbf{c}) - 1\\big)\\,\\mathbf{u}}$$"
        }
    ]
}