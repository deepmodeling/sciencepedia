## 引言
在当今数据驱动的世界中，从社交网络到生物分子相互作用，网络作为一种描述复杂系统实体间关系的通用语言无处不在。然而，如何将这些复杂的、非欧几里得的图结构数据转化为现代机器学习算法能够理解和处理的格式，是一个长期存在的挑战。直接在图上应用传统算法十分困难，这构成了一个显著的知识鸿沟，限制了我们从网络数据中挖掘深层见解的能力。

基于随机游走的嵌入（Random-walk Based Embeddings）方法为解决这一问题提供了优雅而强大的方案。其核心思想借鉴了自然语言处理领域的智慧，通过在图上模拟“行走”过程来捕捉节点的局部邻域结构，并将其“翻译”成低维、密集的[向量表示](@entry_id:166424)（即“嵌入”）。这些嵌入向量如同节点在[向量空间](@entry_id:151108)中的“坐标”，不仅保留了关键的拓扑信息，还使得我们可以直接应用分类、聚类、相似性计算等成熟的机器学习工具来分析网络。

本文旨在系统性地介绍这一关键技术。我们将在接下来的章节中：
- **原理与机制**：深入剖析随机游走如何生成节点序列，探讨Skip-Gram和[负采样](@entry_id:634675)如何从序列中学习嵌入，并揭示其背后的数学理论，最后介绍[node2vec](@entry_id:752530)如何通过有偏游走捕捉更多样的结构信息。
- **应用与跨学科连接**：展示这些嵌入方法在[节点分类](@entry_id:752531)、[链接预测](@entry_id:262538)、[社区发现](@entry_id:143791)等核心网络任务中的实际应用，并探索其在生物信息学等前沿领域的跨学科价值，同时讨论其与[谱方法](@entry_id:141737)及图神经网络的深刻联系。
- **动手实践**：通过一系列具体的编程练习，引导你亲手实现随机游走序列的生成、转移概率的计算以及学习梯度的推导，将理论知识转化为实践技能。

通过阅读本文，你将构建起对基于随机游走的嵌入方法的全面理解，从 foundational 的理论到前沿的应用，为你驾驭和分析[复杂网络](@entry_id:261695)数据打下坚实的基础。

## 原理与机制

本章旨在深入剖析基于随机游走的嵌入方法的内部工作原理与核心机制。在“引言”章节的基础上，我们将不再赘述背景，而是直接从构成这些方法的基本组件出发，系统性地构建一幅完整的技术图景。我们将从随机游走如何从图结构中采样上下文信息开始，继而探讨学习算法如何将这些信息转化为低维[向量表示](@entry_id:166424)，然后深入分析这些过程背后深刻的数学理论，最后介绍对基本随机游走的扩展，以捕获网络中更多样的结构信息。

### 从图游走到序列：上下文的生成

基于随机游走的嵌入方法，其核心思想在于将图的结构信息转化为一种[线性序](@entry_id:146781)列，这种序列可以被借鉴自自然语言处理（NLP）的强大模型所理解。这个转化的第一步，也是最关键的一步，就是定义并执行在[图上的随机游走](@entry_id:273358)过程。

#### 随机游走的基本数学原理

让我们从一个无向、连通的图 $G=(V,E)$ 开始，其中 $V$ 是节点集合，$E$ 是[边集](@entry_id:267160)合。一个**简单随机游走 (simple random walk)** 是一个离散时间的[马尔可夫过程](@entry_id:1127634)，其[状态空间](@entry_id:160914)就是图的节点集合 $V$。在每一步，游走从当前节点移动到一个随机选择的邻居节点。

为了将这个过程形式化，我们需要定义**[一步转移概率](@entry_id:272678)矩阵 (one-step transition probability matrix)** $P$。矩阵的第 $(i,j)$ 个元素 $P_{ij}$ 表示从节点 $i$ 一步转移到节点 $j$ 的概率。根据简单随机游走的定义，转移必须沿着存在的边进行，且对于任意一个节点 $i$，其所有邻居被选为下一步的概率是均等的。

设节点 $i$ 的度为 $d_i$，即其邻居的数量。那么，从节点 $i$ 转移到其任意一个邻居 $j$ 的概率就是 $1/d_i$。如果节点 $j$ 不是 $i$ 的邻居，或者 $j=i$（在[简单图](@entry_id:274882)中），则转移概率为 $0$。我们可以用图的**[邻接矩阵](@entry_id:151010) (adjacency matrix)** $A$ 来统一地表达这个规则，其中如果 $(i,j) \in E$，则 $A_{ij}=1$，否则 $A_{ij}=0$。因此，转移概率可以写作：
$$
P_{ij} = \frac{A_{ij}}{d_i}
$$
这个表达式精确地编码了游走的行为。我们可以验证，对于任意节点 $i$，其转移概率之和为 $1$，满足[马尔可夫链](@entry_id:150828)的基本公理：
$$
\sum_{j \in V} P_{ij} = \sum_{j \in V} \frac{A_{ij}}{d_i} = \frac{1}{d_i} \sum_{j \in V} A_{ij} = \frac{d_i}{d_i} = 1
$$
每一行的元素之和都为 $1$ 的矩阵被称为**[行随机矩阵](@entry_id:1131129) (row-stochastic matrix)**。

在[矩阵表示法](@entry_id:190318)中，这个[转移矩阵](@entry_id:145510)可以通过邻接矩阵 $A$ 和一个对角矩阵——**度矩阵 (degree matrix)** $D$——来方便地计算。度矩阵 $D$ 的对角[线元](@entry_id:196833)素 $D_{ii} = d_i$。其[逆矩阵](@entry_id:140380) $D^{-1}$ 也是一个对角矩阵，对角[线元](@entry_id:196833)素为 $1/d_i$。于是，转移矩阵 $P$ 可以简洁地表示为：
$$
P = D^{-1}A
$$
这个等式是所有基于简单随机游走方法的基础。例如，[DeepWalk算法](@entry_id:1123471)就利用这个转移矩阵来生成其核心的节点序列 。给定一个当前节点 $i$，下一个节点 $j$ 就是根据 $P$ 矩阵第 $i$ 行的概率分布进行采样的。

#### 从游走中提取上下文：DeepWalk 的类比

有了生成无限长随机游走的机制，我们如何将其转化为可供学习算法使用的有限训练数据呢？DeepWalk 算法的开创性贡献在于它巧妙地将图中的随机游走序列类比于自然语言中的句子。

具体过程如下：对于图中的每一个节点，我们都以它为起点，生成若干条固定长度的**截断随机游走 (truncated random walks)**。例如，一条长度为 $L$ 的游走是一个包含 $L+1$ 个节点的序列 $(X_0, X_1, \dots, X_L)$。这样，我们就从图中生成了一个由节点序列组成的“语料库”。

接下来，借鉴 NLP 中 Skip-Gram 模型的思想，我们使用一个**滑动窗口 (sliding window)** 来从这些“句子”中提取共现信息。对于序列中的每一个节点 $X_t$（称为**中心节点 (center node)**），我们在其前后一个固定大小的窗口内（例如，半径为 $w$）的所有节点都视为其**上下文节点 (context nodes)**。

例如，给定一个半径为 $w$ 的对称窗口，对于中心节点 $X_t$，其上下文集合是 $\{X_{t-w}, \dots, X_{t-1}, X_{t+1}, \dots, X_{t+w}\}$，只要这些节点在序列的边界之内。通过在整个语料库的所有序列上滑动这个窗口，我们就生成了大量的（中心节点，上下文节点）配对。这些配对构成了训练嵌入模型的原始数据。这个过程精确地定义了如何将图的邻近关系转化为可用于[预测学](@entry_id:1130218)习的共现对 。

### 从共现到嵌入：学习机制

生成了大量的（中心节点，上下文节点）配对后，下一步就是设计一个学习目标，使得在随机游走中频繁共现的节点在最终的[嵌入空间](@entry_id:637157)中也彼此靠近。这个过程同样深受 NLP 领域中 [Word2Vec](@entry_id:634267) 模型的启发。

#### Skip-Gram 与[负采样](@entry_id:634675)

**Skip-Gram** 模型的目标是，给定一个中心节点 $u$，最大化其在语料库中真实出现的上下文节点 $c$ 的条件概率 $P(c|u)$。这个概率通常通过嵌入向量的点积和 [Softmax](@entry_id:636766) 函数来[参数化](@entry_id:265163)：
$$
P(c|u) = \frac{\exp(\mathbf{v}_c^\top \mathbf{v}_u)}{\sum_{v' \in V} \exp(\mathbf{v}_{v'}^\top \mathbf{v}_u)}
$$
其中 $\mathbf{v}_u$ 和 $\mathbf{v}_c$ 分别是节点 $u$ 和 $c$ 的嵌入向量。然而，这个公式的计算成本非常高，因为分母需要对图中所有节点进行求和，这在大型网络中是不可行的。

为了解决这个问题，**[负采样](@entry_id:634675) (Negative Sampling, NS)** 被提出作为一种高效的近似方法。[负采样](@entry_id:634675)的核心思想是将多[分类问题](@entry_id:637153)（预测哪个是上下文节点）转化为一个二[分类问题](@entry_id:637153)：区分真实的上下文配对 $(u,c)$ 和随机生成的“负”配对 $(u,n)$。

具体来说，对于每一个真实的（中心，上下文）配对 $(u,c)$，我们称之为“正样本”。然后，我们从一个噪声分布 $Q$ 中随机抽取 $k$ 个节点 $n_1, \dots, n_k$，与中心节点 $u$ 组成 $k$ 个“负样本” $(u,n_i)$。学习的目标是训练一个逻辑回归分类器，使其能够将正样本的预测概率最大化，同时将负样本的预测概率最小化。

使用[逻辑斯谛函数](@entry_id:634233) $\sigma(x) = (1 + \exp(-x))^{-1}$，我们将一个配对 $(u,c)$ 的得分（通常是向量点积 $\mathbf{v}_u^\top \tilde{\mathbf{v}}_c$，这里使用不同的中心向量 $\mathbf{v}$ 和上下文向量 $\tilde{\mathbf{v}}$ 是一种常见的有效技巧）映射到概率。对于一个正样本 $(u,c)$ 和 $k$ 个负样本 $\{n_i\}_{i=1}^k$，需要最大化的单对[目标函数](@entry_id:267263)（对数似然）是：
$$
\mathcal{L} = \log \sigma(\mathbf{v}_u^\top \tilde{\mathbf{v}}_c) + \sum_{i=1}^{k} \log \sigma(-\mathbf{v}_u^\top \tilde{\mathbf{v}}_{n_i})
$$
第一项最大化了模型正确识别真实配对的概率，而第二项（求和项）最大化了模型正确识别所有负样本为假的概率。通过对语料库中所有的共现配对优化这个目标函数，算法就能学习到捕捉了共现统计规律的[节点嵌入](@entry_id:1128746) 。

#### SGNS 的理论内涵：隐式[矩阵分解](@entry_id:139760)

Skip-Gram [负采样](@entry_id:634675)（SGNS）模型看似一个基于神经网络的预测任务，但其背后有着深刻的理论联系，即**隐式[矩阵分解](@entry_id:139760) (implicit matrix factorization)**。2014年，Levy 和 Goldberg 的研究揭示，SGNS 实际上是在分解一个特定的**逐点[互信息](@entry_id:138718) (Pointwise Mutual Information, PMI)** 矩阵。

PMI 是信息论中的一个概念，用于衡量两个事件同时发生的概率与它们各自独立发生概率的乘积之间的差异。在我们的情境下，节点 $u$ 和 $c$ 的 PMI 定义为：
$$
\text{PMI}(u,c) = \log \left( \frac{P(u,c)}{P(u)P(c)} \right)
$$
其中 $P(u,c)$ 是 $u$ 和 $c$ 作为一个（中心，上下文）对出现的[联合概率](@entry_id:266356)，$P(u)$ 和 $P(c)$ 是它们的边缘概率。这些概率可以从语料库的共现计数中估计：
$$
\text{PMI}(u,c) = \log \left( \frac{X(u,c) \cdot X(\cdot,\cdot)}{X(u,\cdot) \cdot X(\cdot,c)} \right)
$$
其中 $X(u,c)$ 是配对计数，$X(u,\cdot)$ 和 $X(\cdot,c)$ 是边缘计数，$X(\cdot,\cdot)$ 是总计数。

通过对 SGNS 的目标函数求导并设为零，可以找到其最优解满足的条件。这个条件揭示了模型学习到的最优向量点积 $\mathbf{v}_u^\top \tilde{\mathbf{v}}_c$ 实际上等于该配对的 PMI 值减去一个与[负采样](@entry_id:634675)数量 $k$ 相关的偏移量：
$$
\mathbf{v}_u^\top \tilde{\mathbf{v}}_c = \text{PMI}(u,c) - \log(k)
$$
这意味着，SGNS 算法通过学习中心向量 $\mathbf{v}$ 和上下文向量 $\tilde{\mathbf{v}}$，隐式地对一个“[移位](@entry_id:145848)的”PMI 矩阵进行低秩分解。例如，在一个包含节点 $\{a,b,c\}$ 的小型网络中，如果我们有共现计数 $X(b,a)=30$，边缘计数 $X(b,\cdot)=50$ 和 $X(\cdot,a)=40$，总计数 $X(\cdot,\cdot)=120$，并且使用 $k=5$ 个负样本，那么 SGNS 算法会调整嵌入向量，使其点积 $\mathbf{v}_b^\top \tilde{\mathbf{v}}_a$ 趋近于 $\text{PMI}(b,a) - \ln(k) = \ln\left(\frac{30 \cdot 120}{50 \cdot 40}\right) - \ln(5) = \ln(1.8) - \ln(5) = \ln(0.36)$ (即 $\ln(9/25)$) 。这一发现为我们理解随机游走嵌入提供了一个强大的理论视角：它不仅仅是在预测邻居，更是在学习一种能够重构节点间[互信息](@entry_id:138718)关系的[向量表示](@entry_id:166424)。

### 随机游走的深层理论

虽然随机游走和 Skip-Gram 的组合在实践中非常成功，但更深层次的理论分析能够揭示其成功的数学基础，以及其内在的局限性。

#### 游走统计与转移矩阵的关系

DeepWalk 等方法生成的共现统计数据并非随意，它们与图的基本属性——特别是转移矩阵 $P$ 的幂——有着直接的数学联系。考虑一个理想化的场景：我们在一个已经达到**[平稳分布](@entry_id:194199) (stationary distribution)** $\pi$ 的极长随机游走中采样（对于无向[连通图](@entry_id:264785)，$\pi_i = d_i / (2m)$，其中 $m$ 是边的总数）。

如果我们从这个平稳的游走中随机选择一个中心节点 $i$，然后在半径为 $w$ 的窗口内随机选择一个偏移量 $t \in \{1, \dots, w\}$ 来确定上下文节点 $j$，那么我们采到配对 $(i,j)$ 的概率是多少？

这个概率可以被分解为：(1) 在平稳状态下采到节点 $i$ 的概率，即 $\pi_i$；(2) 给定从 $i$ 出发，在 $t$ 步后到达 $j$ 的概率，即 $(P^t)_{ij}$。将所有可能的偏移量 $t$ 的贡献加总，我们发现，期望的共现计数矩阵与下式成正比：
$$
\mathbb{E}[C] \propto D_\pi \sum_{t=1}^{w} P^t
$$
其中 $D_\pi$ 是以[平稳分布](@entry_id:194199) $\pi$ 为对角线元素的[对角矩阵](@entry_id:637782) 。这个优雅的结论表明，DeepWalk 算法通过在一个大小为 $w$ 的窗口内收集共现信息，实际上是在隐式地聚合关于图中长度从 $1$ 到 $w$ 的所有路径的信息。每当窗口大小 $w$ 增加 $1$，模型就在其知识库中加入了关于长度为 $w+1$ 的[路径信息](@entry_id:169683)，从而获得了对图结构更长程的视野。

#### [谱域](@entry_id:755169)视角：窗口大小作为滤波器

我们可以从**谱图理论 (spectral graph theory)** 的角度进一步深化这一理解。对于一个对称的转移矩阵 $P$（例如，在 $d$-[正则图](@entry_id:265877)上），它可以被[谱分解](@entry_id:173707)为 $P = U \Lambda U^\top$，其中 $U$ 是由[特征向量](@entry_id:151813)构成的[正交矩阵](@entry_id:169220)，$\Lambda$ 是由特征值 $\lambda_i$ 构成的[对角矩阵](@entry_id:637782)。

利用这个分解，前面得到的算子 $\sum_{t=1}^w P^t$ 可以被重写为：
$$
\sum_{t=1}^w P^t = U \left( \sum_{t=1}^w \Lambda^t \right) U^\top
$$
这表明，该算子对图的每一个**[特征模式](@entry_id:747279) (eigenmode)**（由[特征向量](@entry_id:151813)代表）的贡献，被一个与对应特征值 $\lambda_i$ 相关的因子 $f_w(\lambda_i) = \sum_{t=1}^w \lambda_i^t$ 所缩放。

这个缩放因子 $f_w(\lambda_i)$ 的行为揭示了窗口大小 $w$ 的深刻作用。
*   对于图的平稳模式（对应特征值 $\lambda_1 = 1$），其贡献因子为 $w$，随窗口大小线性增长。
*   对于其他模式（$|\lambda_i|  1$），贡献因子是一个收敛的[几何级数](@entry_id:158490)，当 $w \to \infty$ 时，它会饱和到一个有限值 $\lambda_i / (1-\lambda_i)$。

因此，窗口 $w$ 扮演了一个**低通滤波器 (low-pass filter)** 的角色 。它极大地放大了与[平稳分布](@entry_id:194199)相关的“慢”模式（特征值接近 $1$），而相对抑制了变化迅速的“快”模式（特征值接近 $0$）。随着 $w$ 的增加，平稳模式在聚合的共现信息中的主导地位也随之增强。

#### 随机游走的内在偏见：度偏差问题

尽管简单随机游走是一种强大的采样工具，但它并非没有偏见。其[平稳分布](@entry_id:194199) $\pi_i \propto d_i$ 表明，游走访问一个节点的频率与其度成正比。这意味着高度节点（“枢纽”节点）在随机游走序列中出现的频率远高于低度节点。

这种采样偏见会直接传递到共现统计中。在已经混合的游走中，一个（中心，上下文）配对 $(i,j)$ 被观察到的期望次数，其[主导项](@entry_id:167418)与 $i$ 和 $j$ 的平稳概率的乘积成正比，即：
$$
\mathbb{E}[C_{ij}] \propto \pi_i \pi_j \propto d_i d_j
$$
这个**度偏差 (degree bias)** 意味着，两个节点仅仅因为它们都是高度节点，就可能在我们的语料库中频繁共现，而这种共现可能并不反映任何更深层次的结构相似性。

为了纠正这种偏差，一种可行的方法是在学习过程中对每个观察到的共现配对 $(i,j)$ 进行加权，权重与偏差因子成反比。例如，我们可以应用一个权重 $w_{ij} = 1/(d_i d_j)$。这样，校正后的共现统计量在期望上将不再依赖于度的乘积，从而使得学习算法能够更专注于那些不能被节点度简单解释的、更有意义的结构关系 。

### 超越简单游走：[node2vec](@entry_id:752530) 与结构多样性

简单随机游走倾向于在图上进行局部探索，这使得它非常适合捕捉基于社群的相似性。然而，网络中的相似性概念是多样的。为了捕获更丰富的结构信息，我们需要一种更灵活的游走策略。

#### [同质性](@entry_id:636502)与结构对等性

在网络科学中，节点相似性通常被区分为两种主要类型：

1.  **[同质性](@entry_id:636502) (Homophily)**：指的是“物以类聚，人以群分”的原则。具有[同质性](@entry_id:636502)相似的节点通常[紧密连接](@entry_id:170497)，属于同一个密集的社群或集群。例如，社交网络中同一个朋友圈里的人。捕捉同质性需要一种能够深入探索局部邻域的策略。

2.  **结构对等性 (Structural Equivalence)**：指的是节点在网络中扮演相似的角色。具有结构对等性的节点拥有相似的连接模式，即使它们彼此相距很远，甚至不属于同一个社群。例如，不同公司中担任“部门经理”角色的人，他们都连接着一批“下属”和一个“高管”。捕捉结构对等性需要一种能够看到宏观连接模式的、更具探索性的策略。

DeepWalk 的简单随机游走本质上是一种局部探索，因此它在捕捉[同质性](@entry_id:636502)方面表现出色，但在识别结构对等性方面能力有限。

#### [node2vec](@entry_id:752530) 的二阶有偏游走

为了解决这个问题，**[node2vec](@entry_id:752530)** 算法引入了一种巧妙的**二阶有偏随机游走 (second-order biased random walk)**。与简单随机游走（一阶，只依赖当前节点）不同，[node2vec](@entry_id:752530) 的下一步决策同时取决于当前节点 $v$ 和上一步的节点 $t$。

这种偏置通过两个参数来控制：**返回参数 $p$ (return parameter)** 和**进出参数 $q$ (in-out parameter)**。假设游走刚从节点 $t$ 到达节点 $v$，现在要决定下一个节点 $x$（$x$ 是 $v$ 的邻居）。[node2vec](@entry_id:752530) 会根据 $x$ 与 $t$ 的距离来调整转移概率。

具体来说，从 $v$ 到 $x$ 的未归一化转移概率 $\pi_{vx}$ 与一个偏置因子 $\alpha_{pq}(t,x)$ 成正比：
$$
\alpha_{pq}(t, x) =
\begin{cases}
1/p   \text{if } d_{tx} = 0 \text{ (回到 } t) \\
1   \text{if } d_{tx} = 1 \text{ (访问 } t \text{ 的邻居)} \\
1/q   \text{if } d_{tx} = 2 \text{ (访问 } t \text{ 之外的新节点)}
\end{cases}
$$
其中 $d_{tx}$ 是节点 $t$ 和 $x$ 之间的[最短路径距离](@entry_id:754797)。归一化的转移概率通过对当前节点 $v$ 的所有邻居的未归一化概率求和来实现。

例如，设当前节点为 $v$，前一节点为 $t$，$v$ 的邻居为 $\{t, a, b, c\}$，边权分别为 $w_{vt}=2, w_{va}=1, w_{vb}=3, w_{vc}=4$。假设 $a,c$ 也是 $t$ 的邻居，而 $b$ 不是。取 $p=0.5, q=2$。
*   转移回 $t$ 的偏置为 $1/p = 2$。
*   转移到 $a,c$ 的偏置为 $1$。
*   转移到 $b$ 的偏置为 $1/q = 0.5$。
结合边权，可以计算出归一化的转移概率，例如到节点 $c$ 的概率为 $\frac{1 \cdot w_{vc}}{2 \cdot w_{vt} + 1 \cdot w_{va} + 0.5 \cdot w_{vb} + 1 \cdot w_{vc}} = \frac{4}{4+1+1.5+4} = \frac{4}{10.5} \approx 0.380952$ 。

#### 控制游走以捕获不同结构

参数 $p$ 和 $q$ 的引入为我们提供了控制游走行为的强大杠杆，使其能够在两种主要的探索策略之间插值：

*   **[广度优先搜索](@entry_id:156630) (Breadth-First Search, BFS)**：当**进出参数 $q$ 较大时**（例如 $q1$），游走会倾向于停留在起始节点的局部邻域内。这是因为 $1/q$ 很小，使得游走向外探索的概率降低。这种类似 BFS 的局部探索策略非常适合生成反映**[同质性](@entry_id:636502)**的上下文，因为它主要在密集的社群内部进行采样。

*   **[深度优先搜索](@entry_id:270983) (Depth-First Search, DFS)**：当**进出参数 $q$ 较小时**（例如 $q  1$），$1/q$ 很大，游走被鼓励去探索更远的、未知的区域。这种类似 DFS 的全局探索策略对于发现**结构对等性**至关重要。通过跳出局部邻域，游走能够采样到反映节点角色的宏观连接模式。

**返回参数 $p$** 主要控制了游走“回头看”的可能性。较大的 $p$ 会降低立即返回的概率，鼓励游走进行更广泛的探索。

在一个包含紧密社群和具有相似角色的“星形”结构（例如，多个中心节点连接着一批叶子节点）的合成网络中，我们可以清晰地看到这种权衡 。
*   要学习社群成员的相似性（[同质性](@entry_id:636502)），应使用 [node2vec](@entry_id:752530) 并设置 $q1$，其性能会优于 DeepWalk，因为它能更有效地将游走限制在社群内部。
*   要学习不同“星形”结构中叶子节点的角色相似性（结构对等性），应使用 [node2vec](@entry_id:752530) 并设置 $q  1$。这种策略能促使游走从一个叶子节点跳到其中心，再向外探索，从而发现所有叶子节点共有的“连接到中心”这一结构模式，这是 DeepWalk 或 BFS 策略难以捕捉的 。

总之，[node2vec](@entry_id:752530) 通过引入两个简单的参数，极大地增强了随机游走作为网络结构采样器的灵活性和[表达能力](@entry_id:149863)，使得我们可以根据具体的分析任务和网络特性，定制化地生成最能反映目标相似性类型的[节点嵌入](@entry_id:1128746)。这标志着从“一刀切”的采样到“量身定制”的采样的重要进步。