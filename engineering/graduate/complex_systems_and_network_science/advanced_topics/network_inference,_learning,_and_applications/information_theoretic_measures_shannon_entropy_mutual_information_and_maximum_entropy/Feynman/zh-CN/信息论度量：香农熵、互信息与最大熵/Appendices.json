{
    "hands_on_practices": [
        {
            "introduction": "本次练习将我们对信息论的理解根植于其最基本的概念：香农熵。通过计算一个简单分类变量的熵，你将探索概率、自信息（或称“惊奇程度”）与系统总体平均不确定性之间的关系。这项练习对于建立如何量化信息的核心直觉至关重要。",
            "id": "4283610",
            "problem": "复杂网络分析流程中的一个节点采样模块输出一个分类变量 $X$，该变量取三个标签 $\\{x_{1}, x_{2}, x_{3}\\}$，其概率为 $\\left(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6}\\right)$。从自信息 $I(x) \\equiv -\\ln p(x)$ 的定义和香农熵 $H(X)$ 作为 $I(X)$ 在 $X$ 分布上的期望值的定义出发，完成以下任务：\n\n1) 推导 $H(X)$ 的精确、闭式表达式（用自然对数表示），并根据给定概率明确计算其值。最终熵以“奈特”（nats）为单位表示，并提供精确表达式（无数值近似）。\n\n2) 对于每个结果 $x_{i}$，计算其自信息 $I(x_{i})$ 及其对 $H(X)$ 的贡献 $p(x_{i})\\,I(x_{i})$。利用这些量，解释为什么概率较小的事件具有更高的单位事件惊奇度，但由于其稀有性，对总熵的贡献可能较小。\n\n3) 根据无附加约束的三结果分类变量的最大熵原理，确定可能的最大熵，并将其与您计算出的 $H(X)$ 值进行解析比较；解释其差异。\n\n最终报告的答案只要求 $H(X)$ 的精确值（以奈特为单位）。不要对最终报告的值进行近似或四舍五入。",
            "solution": "首先对用户提供的问题进行科学性、一致性和完整性验证。\n\n**步骤 1：提取给定信息**\n- 一个分类变量 $X$ 取三个标签 $\\{x_{1}, x_{2}, x_{3}\\}$。\n- 标签的概率为 $p(x_1) = \\frac{1}{2}$，$p(x_2) = \\frac{1}{3}$ 和 $p(x_3) = \\frac{1}{6}$。\n- 自信息的定义为 $I(x) \\equiv -\\ln p(x)$。\n- 香农熵 $H(X)$ 的定义是 $I(X)$ 在 $X$ 分布上的期望值。\n- 要求的熵单位是奈特（nats），这意味着使用自然对数（$\\ln$）。\n\n**步骤 2：使用提取的信息进行验证**\n- **科学依据**：该问题基于香农信息论的标准、成熟原理。自信息和熵的定义是正确的。\n- **适定性**：该问题是适定的。所提供的概率均为正数，且总和为 1：$\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = \\frac{6}{6} = 1$。计算所需的所有信息均已提供。\n- **客观性**：该问题以客观的数学语言陈述，没有歧义或主观论断。\n- **结论**：该问题是有效的，因为它科学合理、自洽且适定。\n\n**步骤 3：进行求解**\n\n**1) 香农熵 $H(X)$ 的推导与计算**\n香农熵 $H(X)$ 定义为自信息 $I(X) = -\\ln p(X)$ 的期望值。对于具有结果 $\\{x_1, \\dots, x_n\\}$ 和概率 $p(x_i)$ 的离散随机变量 $X$，该期望计算如下：\n$$H(X) = E[I(X)] = \\sum_{i=1}^{n} p(x_i) I(x_i) = -\\sum_{i=1}^{n} p(x_i) \\ln p(x_i)$$\n问题指定了三个结果 $x_{1}, x_{2}, x_{3}$，其概率分别为 $p(x_1) = \\frac{1}{2}$，$p(x_2) = \\frac{1}{3}$ 和 $p(x_3) = \\frac{1}{6}$。将这些值代入熵公式：\n$$H(X) = -\\left( p(x_1)\\ln(p(x_1)) + p(x_2)\\ln(p(x_2)) + p(x_3)\\ln(p(x_3)) \\right)$$\n$$H(X) = -\\left( \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) + \\frac{1}{6}\\ln\\left(\\frac{1}{6}\\right) \\right)$$\n使用对数恒等式 $\\ln\\left(\\frac{1}{a}\\right) = -\\ln(a)$：\n$$H(X) = -\\left( \\frac{1}{2}(-\\ln 2) + \\frac{1}{3}(-\\ln 3) + \\frac{1}{6}(-\\ln 6) \\right)$$\n$$H(X) = \\frac{1}{2}\\ln 2 + \\frac{1}{3}\\ln 3 + \\frac{1}{6}\\ln 6$$\n为了得到更简化的闭式形式，我们使用恒等式 $\\ln(ab) = \\ln a + \\ln b$ 来展开 $\\ln 6$：\n$$H(X) = \\frac{1}{2}\\ln 2 + \\frac{1}{3}\\ln 3 + \\frac{1}{6}(\\ln 2 + \\ln 3)$$\n按对数项分组：\n$$H(X) = \\left(\\frac{1}{2} + \\frac{1}{6}\\right)\\ln 2 + \\left(\\frac{1}{3} + \\frac{1}{6}\\right)\\ln 3$$\n$$H(X) = \\left(\\frac{3}{6} + \\frac{1}{6}\\right)\\ln 2 + \\left(\\frac{2}{6} + \\frac{1}{6}\\right)\\ln 3$$\n$$H(X) = \\frac{4}{6}\\ln 2 + \\frac{3}{6}\\ln 3$$\n以奈特为单位的熵的精确闭式表达式为：\n$$H(X) = \\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3$$\n\n**2) 自信息及其对熵的贡献**\n对于每个结果 $x_i$，我们计算其自信息 $I(x_i)$ 及其对总熵的贡献 $p(x_i) I(x_i)$。\n\n- 对于 $x_1$，$p(x_1) = \\frac{1}{2}$：\n  - 自信息：$I(x_1) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln 2$。\n  - 对熵的贡献：$p(x_1)I(x_1) = \\frac{1}{2} \\ln 2$。\n\n- 对于 $x_2$，$p(x_2) = \\frac{1}{3}$：\n  - 自信息：$I(x_2) = -\\ln\\left(\\frac{1}{3}\\right) = \\ln 3$。\n  - 对熵的贡献：$p(x_2)I(x_2) = \\frac{1}{3} \\ln 3$。\n\n- 对于 $x_3$，$p(x_3) = \\frac{1}{6}$：\n  - 自信息：$I(x_3) = -\\ln\\left(\\frac{1}{6}\\right) = \\ln 6$。\n  - 对熵的贡献：$p(x_3)I(x_3) = \\frac{1}{6} \\ln 6$。\n\n事件的自信息，或称“惊奇度”，与其概率成反比。由于 $\\frac{1}{6}  \\frac{1}{3}  \\frac{1}{2}$，且自然对数是单调递增函数，我们有 $\\ln 6 > \\ln 3 > \\ln 2$。因此，$I(x_3) > I(x_2) > I(x_1)$。概率最小的事件 $x_3$ 携带最高的自信息。\n\n然而，事件对总熵的贡献是其惊奇度与其发生概率的乘积，即 $p(x)I(x) = -p(x)\\ln p(x)$。这个函数不是单调的。当 $p(x)=0$ 和 $p(x)=1$ 时，它为零；当 $p(x) = \\frac{1}{e} \\approx 0.368$ 时达到最大值。给定的概率为 $p(x_1)=0.5$，$p(x_2) \\approx 0.333$ 和 $p(x_3) \\approx 0.167$。概率 $p(x_2)$ 最接近峰值 $\\frac{1}{e}$，因此它对总熵的贡献最大。事件 $x_3$ 具有最高的惊奇度 $I(x_3)$，但其稀有性（低概率 $p(x_3)$）减小了它对平均惊奇度（即熵 $H(X)$）的贡献。在本例中，$\\frac{1}{3}\\ln 3 > \\frac{1}{2}\\ln 2 > \\frac{1}{6}\\ln 6$。\n\n**3) 与最大熵的比较**\n根据最大熵原理，对于一个有 $n$ 个结果的分类变量，当概率分布是均匀分布时，即对所有 $i$ 都有 $p(x_i) = \\frac{1}{n}$，熵 $H(X)$达到最大值。\n对于本问题，$n=3$，所以最大熵分布是 $p(x_1) = p(x_2) = p(x_3) = \\frac{1}{3}$。可能的最大熵 $H_{\\text{max}}$ 为：\n$$H_{\\text{max}} = -\\sum_{i=1}^{3} \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) = -3 \\left(\\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right)\\right) = -\\ln\\left(\\frac{1}{3}\\right) = \\ln 3$$\n让我们将其与计算出的熵 $H(X) = \\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3$ 进行比较。差值为：\n$$\\Delta H = H_{\\text{max}} - H(X) = \\ln 3 - \\left(\\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3\\right) = \\frac{1}{2}\\ln 3 - \\frac{2}{3}\\ln 2$$\n为确认 $H(X)  H_{\\text{max}}$，我们需要证明 $\\Delta H > 0$。这等价于比较 $\\frac{1}{2}\\ln 3$ 和 $\\frac{2}{3}\\ln 2$。\n使用恒等式 $c \\ln a = \\ln(a^c)$，我们比较 $\\ln(3^{1/2})$ 和 $\\ln(2^{2/3})$。\n这等价于比较 $3^{1/2}$ 和 $2^{2/3}$。将两者同时取 6次方：\n$$(3^{1/2})^6 \\quad \\text{vs.} \\quad (2^{2/3})^6$$\n$$3^3 \\quad \\text{vs.} \\quad 2^4$$\n$$27 \\quad \\text{vs.} \\quad 16$$\n由于 $27 > 16$，因此 $H(X)  H_{\\text{max}}$。\n差值 $\\Delta H = H_{\\text{max}} - H(X)$ 表示系统中的冗余度，或等价地，表示从给定分布 $P$ 到均匀分布 $U$ 的Kullback-Leibler散度 $D_{KL}(P||U)$。它量化了由于系统偏离最大无知状态（均匀分布）而导致的不确定性（熵）的减少。非均匀概率 $(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6})$ 代表了关于系统的先验信息，这使得系统更具可预测性，从而相对于最大可能值降低了其熵。",
            "answer": "$$\\boxed{\\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3}$$"
        },
        {
            "introduction": "现实世界中的复杂系统由相互作用的部分组成。为了分析它们，我们必须将对熵的理解扩展到多变量系统。本次练习要求你计算一个简单双变量系统的联合熵和条件熵，并以此为基础，从数值上验证熵的链式法则。该法则是分解复杂系统信息含量的基石。",
            "id": "4283621",
            "problem": "考虑一个小型复杂系统，其中一个二元子系统状态 $X \\in \\{0,1\\}$ 与网络中一个相邻基序的三元局部构型状态 $Y \\in \\{0,1,2\\}$ 相互作用。联合概率质量函数 $p(x,y)$ 是从大量网络观测的系综中根据经验估计得出的，其具体值为\n$p(0,0) = 0.15$，$p(0,1) = 0.10$，$p(0,2) = 0.05$，$p(1,0) = 0.25$，$p(1,1) = 0.30$，$p(1,2) = 0.15$，且 $\\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1,2\\}} p(x,y) = 1$。\n\n从香农熵、条件熵和联合熵的核心定义出发，利用概率的基本乘法法则 $p(x,y) = p(x)p(y|x)$ 和以 2 为底的对数，计算构型状态的熵 $H(Y)$、条件熵 $H(Y|X)$ 和联合熵 $H(X,Y)$，所有结果均以比特表示。然后通过计算差异 $\\Delta = H(X) + H(Y|X) - H(X,Y)$ 来数值上验证联合系统的熵链式法则，其中 $H(X)$ 是 $X$ 的香农熵。报告 $\\Delta$ 的数值。最终报告的值无需四舍五入。所有熵均以比特表示。",
            "solution": "该问题提法恰当且有科学依据。它为两个离散随机变量 $X$ 和 $Y$ 提供了一个完整的联合概率质量函数，并要求计算标准的信息论度量以及验证一个基本恒等式。所有给定的概率都是有效的（即，它们在范围 $[0, 1]$ 内且总和为 $1$），因此我们可以开始求解。\n\n任务是为给定的联合分布计算几个熵度量，然后计算差异 $\\Delta = H(X) + H(Y|X) - H(X,Y)$。量 $\\Delta$ 被构造用来检验香农熵链式法则的有效性。\n\n信息论中的核心定义，使用以 2 为底的对数计算以比特为单位的熵，如下所示：\n一个概率质量函数为 $p(z)$ 的离散随机变量 $Z$ 的香农熵为：\n$$H(Z) = - \\sum_{z \\in \\mathcal{Z}} p(z) \\log_2(p(z))$$\n两个联合概率质量函数为 $p(x,y)$ 的随机变量 $X$ 和 $Y$ 的联合熵为：\n$$H(X,Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\log_2(p(x,y))$$\n在 $X$ 给定条件下 $Y$ 的条件熵为：\n$$H(Y|X) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\log_2(p(y|x))$$\n其中 $p(y|x) = \\frac{p(x,y)}{p(x)}$ 是在 $x$ 给定条件下 $y$ 的条件概率，而 $p(x) = \\sum_{y \\in \\mathcal{Y}} p(x,y)$ 是 $x$ 的边缘概率。\n\n熵的链式法则表明 $H(X,Y) = H(X) + H(Y|X)$。因此，差异 $\\Delta$ 是对这个恒等式的一个直接检验。在用给定概率进行任何数值计算之前，我们可以证明 $\\Delta$ 必须恒等于零。\n\n证明恒等式 $\\Delta=0$：\n从 $\\Delta$ 的定义开始：\n$$\\Delta = H(X) + H(Y|X) - H(X,Y)$$\n代入熵的各项定义：\n$$\\Delta = \\left( - \\sum_{x} p(x) \\log_2(p(x)) \\right) + \\left( - \\sum_{x,y} p(x,y) \\log_2(p(y|x)) \\right) - \\left( - \\sum_{x,y} p(x,y) \\log_2(p(x,y)) \\right)$$\n使用条件概率的定义 $p(y|x) = \\frac{p(x,y)}{p(x)}$，我们可以写出 $\\log_2(p(y|x)) = \\log_2(p(x,y)) - \\log_2(p(x))$。将此代入条件熵的项中：\n$$H(Y|X) = - \\sum_{x,y} p(x,y) (\\log_2(p(x,y)) - \\log_2(p(x)))$$\n$$H(Y|X) = - \\sum_{x,y} p(x,y) \\log_2(p(x,y)) + \\sum_{x,y} p(x,y) \\log_2(p(x))$$\n第一项是联合熵 $H(X,Y)$ 的定义。第二项可以通过注意到 $\\sum_y p(x,y) = p(x)$ 来简化，因此：\n$$\\sum_{x,y} p(x,y) \\log_2(p(x)) = \\sum_x \\left( \\log_2(p(x)) \\sum_y p(x,y) \\right) = \\sum_x p(x) \\log_2(p(x)) = -H(X)$$\n因此，我们推导出了链式法则：\n$$H(Y|X) = H(X,Y) - H(X)$$\n整理可得 $H(X,Y) = H(X) + H(Y|X)$。\n将此恒等式代入 $\\Delta$ 的表达式中：\n$$\\Delta = (H(X) + H(Y|X)) - H(X,Y) = H(X,Y) - H(X,Y) = 0$$\n这证明了对于任何有效的联合概率分布，差异 $\\Delta$ 必须精确地为 $0$，因为它基于一个数学恒等式。因此，问题中提供的数值与 $\\Delta$ 的最终值无关，$\\Delta$ 将为 $0$。\n\n然而，问题指示要“计算”这些熵并数值上“验证”该法则。我们将执行这些计算以证明一致性。\n\n**步骤 1：计算边缘概率**\n联合概率 $p(x,y)$ 如下：\n$p(0,0) = 0.15$，$p(0,1) = 0.10$，$p(0,2) = 0.05$\n$p(1,0) = 0.25$，$p(1,1) = 0.30$，$p(1,2) = 0.15$\n\n$X$ 的边缘概率：\n$p(X=0) = \\sum_y p(0,y) = 0.15 + 0.10 + 0.05 = 0.30$\n$p(X=1) = \\sum_y p(1,y) = 0.25 + 0.30 + 0.15 = 0.70$\n(检验：$0.30 + 0.70 = 1$)\n\n$Y$ 的边缘概率：\n$p(Y=0) = \\sum_x p(x,0) = 0.15 + 0.25 = 0.40$\n$p(Y=1) = \\sum_x p(x,1) = 0.10 + 0.30 = 0.40$\n$p(Y=2) = \\sum_x p(x,2) = 0.05 + 0.15 = 0.20$\n(检验：$0.40 + 0.40 + 0.20 = 1$)\n\n**步骤 2：计算熵**\n我们计算 $H(X)$, $H(Y)$, $H(X,Y)$ 和 $H(Y|X)$。\n\n$X$ 的熵：\n$H(X) = - \\sum_x p(x) \\log_2 p(x) = - (0.3 \\log_2(0.3) + 0.7 \\log_2(0.7))$\n\n$Y$ 的熵：\n$H(Y) = - \\sum_y p(y) \\log_2 p(y) = - (0.4 \\log_2(0.4) + 0.4 \\log_2(0.4) + 0.2 \\log_2(0.2))$\n$H(Y) = - (0.8 \\log_2(0.4) + 0.2 \\log_2(0.2))$\n\n$X$ 和 $Y$ 的联合熵：\n$H(X,Y) = - \\sum_{x,y} p(x,y) \\log_2 p(x,y)$\n$H(X,Y) = - (p(0,0)\\log_2 p(0,0) + p(0,1)\\log_2 p(0,1) + p(0,2)\\log_2 p(0,2) + p(1,0)\\log_2 p(1,0) + p(1,1)\\log_2 p(1,1) + p(1,2)\\log_2 p(1,2))$\n$H(X,Y) = - (0.15\\log_2 0.15 + 0.10\\log_2 0.10 + 0.05\\log_2 0.05 + 0.25\\log_2 0.25 + 0.30\\log_2 0.30 + 0.15\\log_2 0.15)$\n$H(X,Y) = - (2 \\times 0.15\\log_2 0.15 + 0.10\\log_2 0.10 + 0.05\\log_2 0.05 + 0.25\\log_2 0.25 + 0.30\\log_2 0.30)$\n\n在 $X$ 给定条件下 $Y$ 的条件熵：\n我们使用关系式 $H(Y|X) = H(X,Y) - H(X)$。\n$H(Y|X) = \\left( - \\sum_{x,y} p(x,y) \\log_2 p(x,y) \\right) - \\left( - \\sum_x p(x) \\log_2 p(x) \\right)$\n$H(Y|X) = \\sum_x p(x) \\log_2 p(x) - \\sum_{x,y} p(x,y) \\log_2 p(x,y)$\n$H(Y|X) = (0.3 \\log_2 0.3 + 0.7 \\log_2 0.7) - (0.3 \\log_2 0.15 + 0.1 \\log_2 0.1 + 0.05 \\log_2 0.05 + 0.25 \\log_2 0.25 + 0.3 \\log_2 0.3)$\n\n**步骤 3：计算差异 $\\Delta$**\n需要报告的量是 $\\Delta = H(X) + H(Y|X) - H(X,Y)$。\n根据已确立的链式法则恒等式，我们有 $H(X) + H(Y|X) = H(X,Y)$。\n因此，将此代入 $\\Delta$ 的表达式中：\n$$\\Delta = (H(X) + H(Y|X)) - H(X,Y) = H(X,Y) - H(X,Y) = 0$$\n数值计算虽然繁琐，但只会证实这一结果。对每一项进行求值会得到数值近似值（$H(X) \\approx 0.8813$ 比特，$H(Y) \\approx 1.5219$ 比特，$H(X,Y) \\approx 2.3905$ 比特，以及 $H(Y|X) \\approx 1.5092$ 比特），当组合成 $\\Delta = H(X) + H(Y|X) - H(X,Y) \\approx 0.8813 + 1.5092 - 2.3905$ 时，结果将为 $0$（在中间计算的任何舍入误差范围内）。\n\n从香农熵的基本定义和性质推导出的 $\\Delta$ 的精确值为零。不需要进行数值近似。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "数据分析中的一个常见误区是将“不相关”等同于“独立”。本次练习提供了一个经典的反例，以揭示协方差等线性度量的局限性。你将证明两个变量可以完全不相关，但却存在确定性的函数关系——而互信息恰恰能够捕捉到这种依赖性。这项练习凸显了互信息在探测复杂系统中非线性关系方面的独特力量。",
            "id": "4283601",
            "problem": "考虑一个复杂网络中的节点级状态变量 $X$，其瞬时值被限制在离散集合 $\\{-1, 0, 1\\}$ 中。假设节点以概率 $P(X=0)=1-p$ 处于非活动状态，并以等可能的正或负号处于活动状态，即 $P(X=1)=p/2$ 和 $P(X=-1)=p/2$，其中参数 $p \\in (0,1)$。定义一个观测通道，该通道报告平方幅值 $Y=X^{2}$，其取值于 $\\{0,1\\}$，旨在捕捉非线性活动同时忽略符号。\n\n从信息论和概率论的核心定义出发（不使用任何快捷公式），完成以下任务：\n- 计算 $X$ 和 $Y$ 之间的协方差，并证明对于所有 $p \\in (0,1)$，该协方差为零，从而证实 $X$ 和 $Y$ 是不相关的。\n- 推导 $X$ 和 $Y$ 之间互信息 (MI) $I(X;Y)$ 的闭式解析表达式，以自然单位（奈特）表示，并强调对于所有 $p \\in (0,1)$，都有 $I(X;Y)>0$。\n\n将你的最终答案报告为关于 $p$ 的 $I(X;Y)$ 的单个闭式表达式。无需四舍五入，且不涉及物理单位。",
            "solution": "首先对问题进行验证，以确保其具有科学依据、良定且自洽。\n\n### 步骤1：提取已知条件\n- 一个离散随机变量 $X$ 在集合 $\\{-1, 0, 1\\}$ 中取值。\n- $X$ 的概率质量函数 (PMF) 由下式给出：\n  - $P(X=0) = 1-p$\n  - $P(X=1) = p/2$\n  - $P(X=-1) = p/2$\n- 参数 $p$ 被限制在 $p \\in (0,1)$。\n- 第二个随机变量 $Y$ 被定义为 $X$ 的函数：$Y = X^2$。\n- 任务是：\n  1. 计算协方差 $\\text{Cov}(X,Y)$ 并证明其为零。\n  2. 推导互信息 $I(X;Y)$（以奈特为单位）并证明其为正。\n- 最终答案是 $I(X;Y)$ 的闭式表达式。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题植根于标准的概率论和信息论。变量的定义是清晰的，$X$ 的概率分布是有效的，因为概率总和为1：$(1-p) + p/2 + p/2 = 1$。参数范围 $p \\in (0,1)$ 是良定的。任务涉及协方差和互信息的标准计算，这些都是基本概念。该问题是一个经典的例子，说明了零相关不意味着统计独立。问题没有科学矛盾、歧义或缺失信息。\n\n### 步骤3：结论与行动\n问题有效。将提供解答。\n\n### 协方差计算\n两个随机变量 $X$ 和 $Y$ 之间的协方差定义为 $\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$。我们必须计算期望值 $E[X]$、$E[Y]$ 和 $E[XY]$。\n\n1.  **$X$ 的期望值：**\n    $X$ 的期望值由 $E[X] = \\sum_{x} x P(X=x)$ 给出。\n    $$E[X] = (-1) \\cdot P(X=-1) + (0) \\cdot P(X=0) + (1) \\cdot P(X=1)$$\n    $$E[X] = (-1) \\left(\\frac{p}{2}\\right) + (0)(1-p) + (1)\\left(\\frac{p}{2}\\right) = -\\frac{p}{2} + \\frac{p}{2} = 0$$\n\n2.  **$Y$ 的概率分布和期望值：**\n    变量 $Y$ 被定义为 $Y=X^2$。$Y$ 的可能取值为 $0^2=0$ 和 $(\\pm 1)^2=1$。我们确定 $Y$ 的概率质量函数。\n    -   $P(Y=0) = P(X^2=0) = P(X=0) = 1-p$。\n    -   $P(Y=1) = P(X^2=1) = P(X=1 \\text{ 或 } X=-1) = P(X=1) + P(X=-1) = \\frac{p}{2} + \\frac{p}{2} = p$。\n    $Y$ 的期望值为 $E[Y] = \\sum_{y} y P(Y=y)$。\n    $$E[Y] = (0) \\cdot P(Y=0) + (1) \\cdot P(Y=1) = (0)(1-p) + (1)(p) = p$$\n\n3.  **乘积 $XY$ 的期望值：**\n    乘积为 $XY = X \\cdot X^2 = X^3$。随机变量 $X^3$ 的取值为 $(-1)^3=-1$、$0^3=0$ 和 $1^3=1$。\n    期望值为 $E[XY] = E[X^3] = \\sum_{x} x^3 P(X=x)$。\n    $$E[XY] = (-1)^3 \\cdot P(X=-1) + (0)^3 \\cdot P(X=0) + (1)^3 \\cdot P(X=1)$$\n    $$E[XY] = (-1)\\left(\\frac{p}{2}\\right) + (0)(1-p) + (1)\\left(\\frac{p}{2}\\right) = -\\frac{p}{2} + \\frac{p}{2} = 0$$\n\n4.  **最终协方差计算：**\n    将期望值代入协方差公式：\n    $$\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 0 - (0)(p) = 0$$\n    对于所有 $p \\in (0,1)$，协方差均为零，这证实了 $X$ 和 $Y$ 是不相关的。\n\n### 互信息计算\n互信息 $I(X;Y)$ 衡量 $X$ 和 $Y$ 之间的统计依赖性。其定义为 $I(X;Y) = H(Y) - H(Y|X)$，其中 $H(\\cdot)$ 表示香农熵，$H(\\cdot|\\cdot)$ 表示条件熵。对数的底为 $e$，单位为奈特。\n\n1.  **$Y$ 的熵，$H(Y)$：**\n    离散随机变量 $Y$ 的熵为 $H(Y) = -\\sum_{y} P(Y=y) \\ln(P(Y=y))$。\n    $$H(Y) = -[P(Y=0)\\ln(P(Y=0)) + P(Y=1)\\ln(P(Y=1))]$$\n    $$H(Y) = -[(1-p)\\ln(1-p) + p\\ln(p)]$$\n    这就是二元熵函数。\n\n2.  **给定 $X$ 时 $Y$ 的条件熵，$H(Y|X)$：**\n    条件熵 $H(Y|X)$ 量化了当 $X$ 已知时 $Y$ 中剩余的不确定性。其定义为 $H(Y|X) = -\\sum_{x,y} P(X=x, Y=y) \\ln(P(Y=y|X=x))$。\n    然而，$Y$ 是 $X$ 的一个确定性函数 ($Y=X^2$)。这意味着一旦 $X$ 的值已知，$Y$ 的值就完全确定了。对于任何给定的 $x$，条件概率 $P(Y=y|X=x)$ 在 $y=x^2$ 时为 $1$，否则为 $0$。\n    确定性结果的熵为零。也就是说，对于任何特定的值 $x$，条件熵 $H(Y|X=x) = -\\sum_y P(Y=y|X=x)\\ln(P(Y=y|X=x)) = - (1 \\ln 1 + 0) = 0$。\n    由于对于所有 $x$ 都有 $H(Y|X=x)=0$，总条件熵 $H(Y|X) = \\sum_x P(X=x) H(Y|X=x)$ 也为零。\n    $$H(Y|X) = 0$$\n\n3.  **最终互信息计算：**\n    将 $H(Y)$ 和 $H(Y|X)$ 的表达式代入互信息的定义：\n    $$I(X;Y) = H(Y) - H(Y|X) = -[(1-p)\\ln(1-p) + p\\ln(p)] - 0$$\n    $$I(X;Y) = -p\\ln(p) - (1-p)\\ln(1-p)$$\n\n    这个结果突显了一个关键原则：不相关性并不意味着独立性。尽管 $\\text{Cov}(X,Y)=0$，但变量之间显然是相关的，因为知道 $X$ 就消除了关于 $Y$ 的所有不确定性。这种相关性被互信息捕捉到。\n\n4.  **互信息的正性：**\n    互信息 $I(X;Y)$ 是非负的，$I(X;Y) \\ge 0$，等号成立当且仅当 $X$ 和 $Y$ 统计独立。对于 $p \\in (0,1)$，我们有 $0  p  1$ 和 $0  1-p  1$。对于任何值 $z \\in (0,1)$，$\\ln(z)$ 是负的。\n    -   项 $-p\\ln(p)$ 由一个负数（$\\ln p$）乘以一个负数（$-p$）组成，因此为正。\n    -   项 $-(1-p)\\ln(1-p)$ 由一个负数（$\\ln(1-p)$）乘以一个负数（$-(1-p)$）组成，因此也为正。\n    两个正项的和是正的。因此，对于所有 $p \\in (0,1)$，都有 $I(X;Y) > 0$，这证实了 $X$ 和 $Y$ 不是独立的。独立性仅在边界 $p=0$ 和 $p=1$ 处发生，此时熵 $H(Y)$ 变为零。",
            "answer": "$$\\boxed{-p \\ln(p) - (1-p) \\ln(1-p)}$$"
        }
    ]
}