{
    "hands_on_practices": [
        {
            "introduction": "This first exercise takes us back to the fundamental building blocks of information theory. By calculating the Shannon entropy for a simple system, we will explore the concepts of \"surprise\" (self-information) and how an event's probability influences its contribution to the overall uncertainty of the system . This practice helps solidify the intuition that entropy is the *average* surprise, not just the surprise of the least likely event.",
            "id": "4283610",
            "problem": "A node-sampling module in a complex network analysis pipeline outputs a categorical variable $X$ taking three labels $\\{x_{1}, x_{2}, x_{3}\\}$ with probabilities $\\left(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6}\\right)$. Starting from the definition of self-information $I(x) \\equiv -\\ln p(x)$ and the definition of Shannon entropy $H(X)$ as the expectation of $I(X)$ over the distribution of $X$, do the following:\n\n1) Derive an exact, closed-form expression for $H(X)$ in terms of natural logarithms and compute it explicitly for the given probabilities. Express the final entropy in nats and provide the exact expression (no numerical approximation).\n\n2) For each outcome $x_{i}$, compute its self-information $I(x_{i})$ and its contribution $p(x_{i})\\,I(x_{i})$ to $H(X)$. Using these quantities, explain why smaller-probability events carry higher per-event surprise yet can contribute less to the total entropy due to their rarity.\n\n3) In light of the maximum entropy principle for a three-outcome categorical variable with no additional constraints, identify the maximum possible entropy and compare it analytically to the value you obtained for $H(X)$; interpret the difference.\n\nOnly the exact value of $H(X)$ in nats is required as the final reported answer. Do not approximate or round the final reported value.",
            "solution": "The user-provided problem is first validated for scientific soundness, consistency, and completeness.\n\n**Step 1: Extract Givens**\n- A categorical variable $X$ takes three labels $\\{x_{1}, x_{2}, x_{3}\\}$.\n- The probabilities of the labels are $p(x_1) = \\frac{1}{2}$, $p(x_2) = \\frac{1}{3}$, and $p(x_3) = \\frac{1}{6}$.\n- The definition of self-information is given as $I(x) \\equiv -\\ln p(x)$.\n- The definition of Shannon entropy $H(X)$ is the expectation of $I(X)$ over the distribution of $X$.\n- The required unit for entropy is nats, implying the use of the natural logarithm ($\\ln$).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is based on the standard, well-established principles of Shannon's information theory. The definitions of self-information and entropy are correct.\n- **Well-Posed**: The problem is well-posed. The provided probabilities are positive and sum to unity: $\\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3}{6} + \\frac{2}{6} + \\frac{1}{6} = \\frac{6}{6} = 1$. All necessary information for the calculations is provided.\n- **Objective**: The problem is stated in objective, mathematical language without ambiguity or subjective claims.\n- **Conclusion**: The problem is valid as it is scientifically sound, self-contained, and well-posed.\n\n**Step 3: Proceed with Solution**\n\n**1) Derivation and Computation of Shannon Entropy $H(X)$**\nThe Shannon entropy $H(X)$ is defined as the expected value of the self-information $I(X) = -\\ln p(X)$. For a discrete random variable $X$ with outcomes $\\{x_1, \\dots, x_n\\}$ and probabilities $p(x_i)$, this expectation is calculated as:\n$$H(X) = E[I(X)] = \\sum_{i=1}^{n} p(x_i) I(x_i) = -\\sum_{i=1}^{n} p(x_i) \\ln p(x_i)$$\nThe problem specifies three outcomes $x_{1}, x_{2}, x_{3}$ with probabilities $p(x_1) = \\frac{1}{2}$, $p(x_2) = \\frac{1}{3}$, and $p(x_3) = \\frac{1}{6}$. Substituting these values into the entropy formula:\n$$H(X) = -\\left( p(x_1)\\ln(p(x_1)) + p(x_2)\\ln(p(x_2)) + p(x_3)\\ln(p(x_3)) \\right)$$\n$$H(X) = -\\left( \\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) + \\frac{1}{6}\\ln\\left(\\frac{1}{6}\\right) \\right)$$\nUsing the logarithmic identity $\\ln\\left(\\frac{1}{a}\\right) = -\\ln(a)$:\n$$H(X) = -\\left( \\frac{1}{2}(-\\ln 2) + \\frac{1}{3}(-\\ln 3) + \\frac{1}{6}(-\\ln 6) \\right)$$\n$$H(X) = \\frac{1}{2}\\ln 2 + \\frac{1}{3}\\ln 3 + \\frac{1}{6}\\ln 6$$\nTo obtain a more simplified closed form, we use the identity $\\ln(ab) = \\ln a + \\ln b$ to expand $\\ln 6$:\n$$H(X) = \\frac{1}{2}\\ln 2 + \\frac{1}{3}\\ln 3 + \\frac{1}{6}(\\ln 2 + \\ln 3)$$\nGrouping terms by the logarithm:\n$$H(X) = \\left(\\frac{1}{2} + \\frac{1}{6}\\right)\\ln 2 + \\left(\\frac{1}{3} + \\frac{1}{6}\\right)\\ln 3$$\n$$H(X) = \\left(\\frac{3}{6} + \\frac{1}{6}\\right)\\ln 2 + \\left(\\frac{2}{6} + \\frac{1}{6}\\right)\\ln 3$$\n$$H(X) = \\frac{4}{6}\\ln 2 + \\frac{3}{6}\\ln 3$$\nThe exact, closed-form expression for the entropy in nats is:\n$$H(X) = \\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3$$\n\n**2) Self-Information and Contribution to Entropy**\nFor each outcome $x_i$, we compute its self-information $I(x_i)$ and its contribution to the total entropy, $p(x_i) I(x_i)$.\n\n- For $x_1$ with $p(x_1) = \\frac{1}{2}$:\n  - Self-information: $I(x_1) = -\\ln\\left(\\frac{1}{2}\\right) = \\ln 2$.\n  - Contribution to entropy: $p(x_1)I(x_1) = \\frac{1}{2} \\ln 2$.\n\n- For $x_2$ with $p(x_2) = \\frac{1}{3}$:\n  - Self-information: $I(x_2) = -\\ln\\left(\\frac{1}{3}\\right) = \\ln 3$.\n  - Contribution to entropy: $p(x_2)I(x_2) = \\frac{1}{3} \\ln 3$.\n\n- For $x_3$ with $p(x_3) = \\frac{1}{6}$:\n  - Self-information: $I(x_3) = -\\ln\\left(\\frac{1}{6}\\right) = \\ln 6$.\n  - Contribution to entropy: $p(x_3)I(x_3) = \\frac{1}{6} \\ln 6$.\n\nAn event's self-information, or \"surprise,\" is inversely related to its probability. Since $\\frac{1}{6}  \\frac{1}{3}  \\frac{1}{2}$, and the natural logarithm is a monotonically increasing function, we have $\\ln 6  \\ln 3  \\ln 2$. Thus, $I(x_3)  I(x_2)  I(x_1)$. The least probable event, $x_3$, carries the highest self-information.\n\nHowever, an event's contribution to the total entropy is the product of its surprise and its probability of occurrence, $p(x)I(x) = -p(x)\\ln p(x)$. This function is not monotonic. It is zero for $p(x)=0$ and $p(x)=1$, and it reaches its maximum value at $p(x) = \\frac{1}{e} \\approx 0.368$. The probabilities are $p(x_1)=0.5$, $p(x_2) \\approx 0.333$, and $p(x_3) \\approx 0.167$. The probability $p(x_2)$ is closest to the peak at $\\frac{1}{e}$, so it provides the largest contribution to the total entropy. The event $x_3$ has the highest surprise $I(x_3)$, but its rarity (low probability $p(x_3)$) diminishes its contribution to the average surprise, which is the entropy $H(X)$. In this case, $\\frac{1}{3}\\ln 3  \\frac{1}{2}\\ln 2  \\frac{1}{6}\\ln 6$.\n\n**3) Comparison with Maximum Entropy**\nAccording to the maximum entropy principle, for a categorical variable with $n$ outcomes, the entropy $H(X)$ is maximized when the probability distribution is uniform, i.e., $p(x_i) = \\frac{1}{n}$ for all $i$.\nFor this problem, $n=3$, so the maximum entropy distribution is $p(x_1) = p(x_2) = p(x_3) = \\frac{1}{3}$. The maximum possible entropy, $H_{\\text{max}}$, is:\n$$H_{\\text{max}} = -\\sum_{i=1}^{3} \\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) = -3 \\left(\\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right)\\right) = -\\ln\\left(\\frac{1}{3}\\right) = \\ln 3$$\nLet's compare this with the calculated entropy $H(X) = \\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3$. The difference is:\n$$\\Delta H = H_{\\text{max}} - H(X) = \\ln 3 - \\left(\\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3\\right) = \\frac{1}{2}\\ln 3 - \\frac{2}{3}\\ln 2$$\nTo confirm that $H(X)  H_{\\text{max}}$, we need to show that $\\Delta H  0$. This is equivalent to comparing $\\frac{1}{2}\\ln 3$ with $\\frac{2}{3}\\ln 2$.\nUsing the identity $c \\ln a = \\ln(a^c)$, we compare $\\ln(3^{1/2})$ and $\\ln(2^{2/3})$.\nThis is equivalent to comparing $3^{1/2}$ and $2^{2/3}$. Raising both to the power of $6$:\n$$(3^{1/2})^6 \\quad \\text{vs.} \\quad (2^{2/3})^6$$\n$$3^3 \\quad \\text{vs.} \\quad 2^4$$\n$$27 \\quad \\text{vs.} \\quad 16$$\nSince $27  16$, it follows that $H(X)  H_{\\text{max}}$.\nThe difference $\\Delta H = H_{\\text{max}} - H(X)$ represents the redundancy in the system or, equivalently, the Kullback-Leibler divergence from the given distribution $P$ to the uniform distribution $U$, $D_{KL}(P||U)$. It quantifies the reduction in uncertainty (entropy) due to the system's deviation from a state of maximum ignorance (the uniform distribution). The non-uniform probabilities $(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6})$ represent prior information about the system, which makes it more predictable and thus lowers its entropy relative to the maximum possible value.",
            "answer": "$$\\boxed{\\frac{2}{3}\\ln 2 + \\frac{1}{2}\\ln 3}$$"
        },
        {
            "introduction": "A common pitfall in data analysis is equating zero correlation with statistical independence. This practice provides a clear, hands-on demonstration of why this is incorrect, using a simple non-linear relationship between two variables . By computing both the covariance and the mutual information, you will see firsthand how mutual information captures any kind of statistical dependence, whereas correlation is blind to non-linear relationships.",
            "id": "4283601",
            "problem": "Consider a node-level state variable $X$ in a complex network whose instantaneous value is constrained to the discrete set $\\{-1, 0, 1\\}$. Suppose the node is inactive with probability $P(X=0)=1-p$ and active with equally likely positive or negative sign $P(X=1)=p/2$ and $P(X=-1)=p/2$, for a parameter $p \\in (0,1)$. Define an observation channel that reports the squared magnitude $Y=X^{2}$, which takes values in $\\{0,1\\}$ and is intended to capture nonlinear activity while ignoring sign.\n\nStarting from core definitions in information theory and probability (without using any shortcut formulas), do the following:\n- Compute the covariance between $X$ and $Y$ and show it vanishes for all $p \\in (0,1)$, thereby establishing that $X$ and $Y$ are uncorrelated.\n- Derive a closed-form analytic expression for the mutual information (MI) $I(X;Y)$ between $X$ and $Y$, expressed in natural units (nats), highlighting that $I(X;Y)0$ for all $p \\in (0,1)$.\n\nReport your final answer as a single closed-form expression for $I(X;Y)$ in terms of $p$. No rounding is required, and no physical units are involved.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and self-contained.\n\n### Step 1: Extract Givens\n- A discrete random variable $X$ takes values in the set $\\{-1, 0, 1\\}$.\n- The probability mass function (PMF) for $X$ is given by:\n  - $P(X=0) = 1-p$\n  - $P(X=1) = p/2$\n  - $P(X=-1) = p/2$\n- The parameter $p$ is constrained to $p \\in (0,1)$.\n- A second random variable $Y$ is defined as a function of $X$: $Y = X^2$.\n- The tasks are:\n  1. Compute the covariance $\\text{Cov}(X,Y)$ and show it is zero.\n  2. Derive the mutual information $I(X;Y)$ in nats and show it is positive.\n- The final answer is to be the closed-form expression for $I(X;Y)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is rooted in standard probability and information theory. The definitions of the variables are clear, and the probability distribution for $X$ is valid, as the probabilities sum to unity: $(1-p) + p/2 + p/2 = 1$. The parameter range $p \\in (0,1)$ is well-defined. The tasks involve standard calculations of covariance and mutual information, which are fundamental concepts. The problem is a classic illustration of how zero correlation does not imply statistical independence. There are no scientific contradictions, ambiguities, or missing information.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Covariance Calculation\nThe covariance between two random variables $X$ and $Y$ is defined as $\\text{Cov}(X,Y) = E[XY] - E[X]E[Y]$. We must compute the expected values $E[X]$, $E[Y]$, and $E[XY]$.\n\n1.  **Expected Value of $X$:**\n    The expected value of $X$ is given by $E[X] = \\sum_{x} x P(X=x)$.\n    $$E[X] = (-1) \\cdot P(X=-1) + (0) \\cdot P(X=0) + (1) \\cdot P(X=1)$$\n    $$E[X] = (-1) \\left(\\frac{p}{2}\\right) + (0)(1-p) + (1)\\left(\\frac{p}{2}\\right) = -\\frac{p}{2} + \\frac{p}{2} = 0$$\n\n2.  **Probability Distribution and Expected Value of $Y$:**\n    The variable $Y$ is defined as $Y=X^2$. The possible values for $Y$ are $0^2=0$ and $(\\pm 1)^2=1$. We determine the PMF for $Y$.\n    -   $P(Y=0) = P(X^2=0) = P(X=0) = 1-p$.\n    -   $P(Y=1) = P(X^2=1) = P(X=1 \\text{ or } X=-1) = P(X=1) + P(X=-1) = \\frac{p}{2} + \\frac{p}{2} = p$.\n    The expected value of $Y$ is $E[Y] = \\sum_{y} y P(Y=y)$.\n    $$E[Y] = (0) \\cdot P(Y=0) + (1) \\cdot P(Y=1) = (0)(1-p) + (1)(p) = p$$\n\n3.  **Expected Value of the Product $XY$:**\n    The product is $XY = X \\cdot X^2 = X^3$. The random variable $X^3$ takes values $(-1)^3=-1$, $0^3=0$, and $1^3=1$.\n    The expected value is $E[XY] = E[X^3] = \\sum_{x} x^3 P(X=x)$.\n    $$E[XY] = (-1)^3 \\cdot P(X=-1) + (0)^3 \\cdot P(X=0) + (1)^3 \\cdot P(X=1)$$\n    $$E[XY] = (-1)\\left(\\frac{p}{2}\\right) + (0)(1-p) + (1)\\left(\\frac{p}{2}\\right) = -\\frac{p}{2} + \\frac{p}{2} = 0$$\n\n4.  **Final Covariance Calculation:**\n    Substituting the expected values into the covariance formula:\n    $$\\text{Cov}(X,Y) = E[XY] - E[X]E[Y] = 0 - (0)(p) = 0$$\n    The covariance is zero for all $p \\in (0,1)$, which establishes that $X$ and $Y$ are uncorrelated.\n\n### Mutual Information Calculation\nThe mutual information $I(X;Y)$ measures the statistical dependence between $X$ and $Y$. It is defined as $I(X;Y) = H(Y) - H(Y|X)$, where $H(\\cdot)$ denotes Shannon entropy and $H(\\cdot|\\cdot)$ denotes conditional entropy. The base of the logarithm is $e$ for units of nats.\n\n1.  **Entropy of $Y$, $H(Y)$:**\n    The entropy of a discrete random variable $Y$ is $H(Y) = -\\sum_{y} P(Y=y) \\ln(P(Y=y))$.\n    $$H(Y) = -[P(Y=0)\\ln(P(Y=0)) + P(Y=1)\\ln(P(Y=1))]$$\n    $$H(Y) = -[(1-p)\\ln(1-p) + p\\ln(p)]$$\n    This is the binary entropy function.\n\n2.  **Conditional Entropy of $Y$ given $X$, $H(Y|X)$:**\n    The conditional entropy $H(Y|X)$ quantifies the remaining uncertainty in $Y$ when $X$ is known. It is defined as $H(Y|X) = -\\sum_{x,y} P(X=x, Y=y) \\ln(P(Y=y|X=x))$.\n    However, $Y$ is a deterministic function of $X$ ($Y=X^2$). This means that once the value of $X$ is known, the value of $Y$ is fully determined. For any given $x$, the conditional probability $P(Y=y|X=x)$ is $1$ for $y=x^2$ and $0$ otherwise.\n    The entropy of a deterministic outcome is zero. That is, for any specific value $x$, the conditional entropy $H(Y|X=x) = -\\sum_y P(Y=y|X=x)\\ln(P(Y=y|X=x)) = - (1 \\ln 1 + 0) = 0$.\n    Since $H(Y|X=x)=0$ for all $x$, the total conditional entropy $H(Y|X) = \\sum_x P(X=x) H(Y|X=x)$ is also zero.\n    $$H(Y|X) = 0$$\n\n3.  **Final Mutual Information Calculation:**\n    Substituting the expressions for $H(Y)$ and $H(Y|X)$ into the definition of mutual information:\n    $$I(X;Y) = H(Y) - H(Y|X) = -[(1-p)\\ln(1-p) + p\\ln(p)] - 0$$\n    $$I(X;Y) = -p\\ln(p) - (1-p)\\ln(1-p)$$\n\n    This result highlights a key principle: uncorrelatedness does not imply independence. While $\\text{Cov}(X,Y)=0$, the variables are clearly dependent, as knowing $X$ removes all uncertainty about $Y$. This dependence is captured by the mutual information.\n\n4.  **Positivity of Mutual Information:**\n    The mutual information $I(X;Y)$ is non-negative, $I(X;Y) \\ge 0$, with equality if and only if $X$ and $Y$ are statistically independent. For $p \\in (0,1)$, we have $0  p  1$ and $0  1-p  1$. For any value $z \\in (0,1)$, $\\ln(z)$ is negative.\n    -   The term $-p\\ln(p)$ consists of a negative number ($\\ln p$) multiplied by a negative number ($-p$), so it is positive.\n    -   The term $-(1-p)\\ln(1-p)$ consists of a negative number ($\\ln(1-p)$) multiplied by a negative number ($-(1-p)$), so it is also positive.\n    The sum of two positive terms is positive. Thus, $I(X;Y)  0$ for all $p \\in (0,1)$, confirming that $X$ and $Y$ are not independent. Independence only occurs at the boundaries $p=0$ and $p=1$, where the entropy $H(Y)$ becomes zero.",
            "answer": "$$\\boxed{-p \\ln(p) - (1-p) \\ln(1-p)}$$"
        },
        {
            "introduction": "Moving to systems with more than two variables reveals fascinating and sometimes counter-intuitive phenomena. This exercise explores how conditioning on a third variable can induce a statistical dependence between two previously independent variables, a concept known as collider bias or Berkson's paradox . Calculating the conditional mutual information provides a rigorous way to quantify this effect and highlights the importance of careful model specification in complex systems.",
            "id": "4283620",
            "problem": "In a social-ecological network, consider three binary attributes associated with each node: an attribute $X \\in \\{0,1\\}$ indicating a local resource use behavior, an attribute $Y \\in \\{0,1\\}$ indicating engagement with an external market, and a selection indicator $Z \\in \\{0,1\\}$ that records whether the node is included in a study sample. Suppose the latent drivers $X$ and $Y$ are independent across the population and each equals $1$ with probability $\\tfrac{1}{2}$. The sampling mechanism is such that a node is included in the study if and only if at least one of the attributes is present, that is, $Z = \\mathbf{1}\\{X=1 \\,\\text{or}\\, Y=1\\}$.\n\nStarting from the core definitions of Shannon entropy and mutual information, and without invoking any context-specific shortcut formulas, do the following:\n\n1. Demonstrate that $X$ and $Y$ are independent in the population, and compute the mutual information $I(X;Y)$.\n2. Show that conditioning on the collider variable $Z$ induces a statistical dependence between $X$ and $Y$, and compute the conditional mutual information $I(X;Y \\mid Z)$.\n\nAll information-theoretic quantities must be expressed using the natural logarithm and in natural units (nats). Report your final answer as the ordered pair $\\big(I(X;Y),\\, I(X;Y \\mid Z)\\big)$ in exact, closed form. Do not provide any numerical approximations or rounding.",
            "solution": "The problem requires the calculation of the mutual information $I(X;Y)$ and the conditional mutual information $I(X;Y \\mid Z)$ for a system of three binary random variables $X$, $Y$, and $Z$. We will proceed by first establishing the full probability distribution of the system from the given information and then applying the fundamental definitions of Shannon entropy and mutual information. All logarithms will be natural logarithms ($\\ln$), and information will be measured in nats.\n\n**1. Population-level analysis and Mutual Information $I(X;Y)$**\n\nThe problem states that the attributes $X$ and $Y$ are binary variables, $X, Y \\in \\{0, 1\\}$, and are independent in the population. The probability of each attribute being present is given as $\\frac{1}{2}$.\nThe marginal probability distributions for $X$ and $Y$ are:\n$$P(X=1) = \\frac{1}{2}, \\quad P(X=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$$\n$$P(Y=1) = \\frac{1}{2}, \\quad P(Y=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$$\nThe independence of $X$ and $Y$ means that their joint probability distribution is the product of their marginal distributions: $P(X=x, Y=y) = P(X=x)P(Y=y)$.\nThis yields a uniform joint distribution over the four possible outcomes for the pair $(X, Y)$:\n$$P(X=x, Y=y) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4} \\quad \\text{for all } (x,y) \\in \\{0,1\\} \\times \\{0,1\\}$$\n\nThe mutual information $I(X;Y)$ quantifies the reduction in uncertainty about one variable from knowing the other. Its definition is:\n$$I(X;Y) = \\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} P(X=x, Y=y) \\ln\\left(\\frac{P(X=x, Y=y)}{P(X=x)P(Y=y)}\\right)$$\nAs demonstrated, the independence of $X$ and $Y$ implies $P(X=x, Y=y) = P(X=x)P(Y=y)$. Substituting this into the definition:\n$$I(X;Y) = \\sum_{x,y} P(x,y) \\ln\\left(\\frac{P(x)P(y)}{P(x)P(y)}\\right) = \\sum_{x,y} P(x,y) \\ln(1)$$\nSince $\\ln(1) = 0$, every term in the summation is zero. Therefore, the mutual information is:\n$$I(X;Y) = 0$$\nThis result formally confirms that, at the population level, $X$ and $Y$ are independent and share no information.\n\n**2. Conditional Analysis and Conditional Mutual Information $I(X;Y \\mid Z)$**\n\nThe variable $Z$ is a selection indicator defined as $Z = \\mathbf{1}\\{X=1 \\,\\text{or}\\, Y=1\\}$. This means $Z$ is a deterministic function of $X$ and $Y$. Specifically, $Z=0$ if and only if $X=0$ and $Y=0$. Otherwise, $Z=1$. This structure makes $Z$ a \"collider\" variable in the causal graph $X \\rightarrow Z \\leftarrow Y$. We will now show that conditioning on this collider induces a statistical dependence between $X$ and $Y$.\n\nFirst, we determine the probability distribution of $Z$:\n$$P(Z=0) = P(X=0, Y=0) = \\frac{1}{4}$$\n$$P(Z=1) = 1 - P(Z=0) = 1 - \\frac{1}{4} = \\frac{3}{4}$$\n\nThe conditional mutual information $I(X;Y \\mid Z)$ is defined as the expectation of the mutual information between $X$ and $Y$ over the distribution of $Z$:\n$$I(X;Y \\mid Z) = \\sum_{z \\in \\{0,1\\}} P(Z=z) I(X;Y \\mid Z=z)$$\nWe compute this by analyzing the cases $z=0$ and $z=1$ separately.\n\n*Case 1: $Z=0$*\nIf we know $Z=0$, it must be that $X=0$ and $Y=0$. The conditional probability distribution $P(X,Y \\mid Z=0)$ is a point mass at $(0,0)$:\n$$P(X=0, Y=0 \\mid Z=0) = \\frac{P(X=0, Y=0, Z=0)}{P(Z=0)} = \\frac{P(X=0, Y=0)}{P(Z=0)} = \\frac{1/4}{1/4} = 1$$\nGiven $Z=0$, there is no uncertainty about $X$ or $Y$. The conditional entropies $H(X|Z=0)$, $H(Y|Z=0)$, and $H(X,Y|Z=0)$ are all zero. The mutual information conditioned on this specific outcome is:\n$$I(X;Y \\mid Z=0) = H(X \\mid Z=0) + H(Y \\mid Z=0) - H(X,Y \\mid Z=0) = 0 + 0 - 0 = 0$$\n\n*Case 2: $Z=1$*\nIf we know $Z=1$, the outcome $(X=0, Y=0)$ is excluded. The three remaining outcomes are $(0,1)$, $(1,0)$, and $(1,1)$. The conditional joint distribution is:\n$$P(X=x, Y=y \\mid Z=1) = \\frac{P(X=x, Y=y, Z=1)}{P(Z=1)} = \\frac{P(X=x, Y=y)}{P(Z=1)} = \\frac{1/4}{3/4} = \\frac{1}{3}$$\nfor each of the three pairs $(x,y) \\in \\{(0,1), (1,0), (1,1)\\}$.\n\nTo show that conditioning on $Z=1$ induces dependence, we compare the conditional joint probability to the product of conditional marginal probabilities.\nThe conditional marginals for $X$ and $Y$ given $Z=1$:\n$$P(X=0 \\mid Z=1) = P(X=0, Y=1 \\mid Z=1) = \\frac{1}{3}$$\n$$P(X=1 \\mid Z=1) = P(X=1, Y=0 \\mid Z=1) + P(X=1, Y=1 \\mid Z=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}$$\nBy symmetry, the distribution for $Y$ is the same:\n$$P(Y=0 \\mid Z=1) = \\frac{1}{3}, \\quad P(Y=1 \\mid Z=1) = \\frac{2}{3}$$\nNow we check for independence:\n$$P(X=0 \\mid Z=1)P(Y=1 \\mid Z=1) = \\frac{1}{3} \\times \\frac{2}{3} = \\frac{2}{9}$$\nHowever, we found that $P(X=0, Y=1 \\mid Z=1) = \\frac{1}{3}$. Since $\\frac{1}{3} \\neq \\frac{2}{9}$, we have formally demonstrated that $X$ and $Y$ are statistically dependent when conditioned on $Z=1$.\n\nTo quantify this dependence, we compute $I(X;Y \\mid Z=1)$ using the identity $I(X;Y \\mid C) = H(X \\mid C) + H(Y \\mid C) - H(X,Y \\mid C)$.\nThe conditional entropy of the joint distribution $P(X,Y \\mid Z=1)$ is the entropy of a uniform distribution over $3$ states:\n$$H(X,Y \\mid Z=1) = -\\sum_{(x,y)} P(x,y \\mid Z=1) \\ln(P(x,y \\mid Z=1)) = -3 \\times \\left(\\frac{1}{3} \\ln\\left(\\frac{1}{3}\\right)\\right) = \\ln(3)$$\nThe conditional marginal entropies are:\n$$H(X \\mid Z=1) = -\\left(P(X=0 \\mid Z=1)\\ln(P(X=0 \\mid Z=1)) + P(X=1 \\mid Z=1)\\ln(P(X=1 \\mid Z=1))\\right)$$\n$$H(X \\mid Z=1) = -\\left(\\frac{1}{3}\\ln\\left(\\frac{1}{3}\\right) + \\frac{2}{3}\\ln\\left(\\frac{2}{3}\\right)\\right) = \\frac{1}{3}\\ln(3) + \\frac{2}{3}(\\ln(3) - \\ln(2)) = \\ln(3) - \\frac{2}{3}\\ln(2)$$\nBy symmetry, $H(Y \\mid Z=1) = H(X \\mid Z=1) = \\ln(3) - \\frac{2}{3}\\ln(2)$.\nNow, we find the mutual information given $Z=1$:\n$$I(X;Y \\mid Z=1) = H(X \\mid Z=1) + H(Y \\mid Z=1) - H(X,Y \\mid Z=1)$$\n$$I(X;Y \\mid Z=1) = \\left(\\ln(3) - \\frac{2}{3}\\ln(2)\\right) + \\left(\\ln(3) - \\frac{2}{3}\\ln(2)\\right) - \\ln(3) = \\ln(3) - \\frac{4}{3}\\ln(2)$$\n\n*Final calculation of $I(X;Y \\mid Z)$*\nWe now combine the results for $Z=0$ and $Z=1$:\n$$I(X;Y \\mid Z) = P(Z=0)I(X;Y \\mid Z=0) + P(Z=1)I(X;Y \\mid Z=1)$$\n$$I(X;Y \\mid Z) = \\left(\\frac{1}{4} \\times 0\\right) + \\left(\\frac{3}{4} \\times \\left(\\ln(3) - \\frac{4}{3}\\ln(2)\\right)\\right)$$\n$$I(X;Y \\mid Z) = \\frac{3}{4}\\ln(3) - \\left(\\frac{3}{4} \\times \\frac{4}{3}\\right)\\ln(2) = \\frac{3}{4}\\ln(3) - \\ln(2)$$\nThe positive value of $I(X;Y \\mid Z)$ confirms that, on average, conditioning on $Z$ creates a statistical dependency between $X$ and $Y$.\n\nThe final result is the ordered pair $\\big(I(X;Y),\\, I(X;Y \\mid Z)\\big)$.\n$$ \\left(0, \\frac{3}{4}\\ln(3) - \\ln(2)\\right) $$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  \\frac{3}{4}\\ln(3) - \\ln(2)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}