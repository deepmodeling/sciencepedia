{
    "hands_on_practices": [
        {
            "introduction": "Before attempting to align two networks, a fundamental question is whether they are structurally identical, or isomorphic. Answering this question directly can be computationally hard. This practice introduces the concept of graph invariants—properties that are preserved under any relabeling of nodes—as a powerful and efficient tool to prove that two graphs are *not* isomorphic, providing a crucial first step in any network comparison task .",
            "id": "4279396",
            "problem": "Consider two undirected, simple, labeled graphs on $6$ nodes, represented by their adjacency matrices $A$ and $B$ in the standard, label-induced basis $\\{1,2,3,4,5,6\\}$. In the context of Network Alignment (NA) and graph matching, two graphs are isomorphic if and only if there exists a permutation matrix $P$ such that $B = P^{\\top} A P$. You are asked to determine whether such a permutation matrix exists for the specific matrices given below by reasoning from core definitions and invariants preserved under graph isomorphism (e.g., degree sequences, spectra, and counts of closed walks), and to report a single number indicating the existence of an isomorphism.\n\nThe adjacency matrices $A$ and $B$ are\n$$\nA = \\begin{pmatrix}\n0 & 0 & 1 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n0 & 1 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0\n\\end{pmatrix}.\n$$\n\nFrom the foundational definition of graph isomorphism, if $B = P^{\\top} A P$ for some permutation matrix $P$, then $A$ and $B$ must share all invariants preserved by such relabelings, including the sorted degree sequence and the eigenvalue spectrum of the adjacency matrix, as well as any quantity expressible as a trace of a polynomial in the adjacency matrix. Work from these core definitions to decide whether there exists a permutation matrix $P$ satisfying $B = P^{\\top} A P$.\n\nDefine the indicator $I$ by\n$$\nI = \\begin{cases}\n1 & \\text{if there exists a permutation matrix } P \\text{ with } B = P^{\\top} A P, \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$\n\nYour task:\n- Use only core definitions and invariants preserved by permutation similarity to argue for the existence or non-existence of such a permutation matrix $P$.\n- Compute and report the single scalar $I$ as the final answer. No rounding is required.",
            "solution": "The problem requires determining whether two graphs, represented by their adjacency matrices $A$ and $B$, are isomorphic. Two simple graphs are isomorphic if and only if there exists a permutation of the vertex labels of one graph that makes it identical to the other graph. In algebraic terms, this is equivalent to the existence of a permutation matrix $P$ such that the adjacency matrices $A$ and $B$ are related by a similarity transformation $B = P^{\\top} A P$. Note that for a permutation matrix, its inverse is its transpose, $P^{-1} = P^{\\top}$.\n\nA necessary condition for two graphs to be isomorphic is that all graph invariants must be identical. Graph invariants are properties of graphs that depend only on the abstract structure, not on the specific labeling of vertices. If we can find a single invariant that differs between the two graphs, we can conclude they are not isomorphic. The problem suggests considering invariants such as degree sequences, eigenvalue spectra, and counts of closed walks.\n\nThe adjacency matrices are given as:\n$$\nA = \\begin{pmatrix}\n0 & 0 & 1 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n0 & 1 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0\n\\end{pmatrix}.\n$$\nBoth matrices are symmetric and have zero diagonals, correctly representing undirected simple graphs on $N=6$ vertices.\n\nFirst, we examine the most basic invariants.\nThe number of vertices is $N=6$ for both graphs.\nThe number of edges is $m = \\frac{1}{2} \\sum_{i,j} M_{ij}$.\nFor graph A, the sum of all entries is $12$, so $m_A = \\frac{12}{2} = 6$.\nFor graph B, the sum of all entries is $12$, so $m_B = \\frac{12}{2} = 6$.\n\nNext, we compute the degree sequence of each graph. The degree of vertex $i$ is the sum of the entries in row $i$ of the adjacency matrix.\nFor graph A, the degrees of vertices $1$ through $6$ are:\n$d_A(1) = 3$, $d_A(2) = 3$, $d_A(3) = 2$, $d_A(4) = 2$, $d_A(5) = 1$, $d_A(6) = 1$.\nThe sorted degree sequence for A is $(3, 3, 2, 2, 1, 1)$.\n\nFor graph B, the degrees are:\n$d_B(1) = 3$, $d_B(2) = 3$, $d_B(3) = 2$, $d_B(4) = 2$, $d_B(5) = 1$, $d_B(6) = 1$.\nThe sorted degree sequence for B is $(3, 3, 2, 2, 1, 1)$.\n\nSince the degree sequences are identical, this invariant is insufficient to distinguish the graphs.\n\nWe now proceed to a more powerful set of invariants: the number of closed walks of length $k$. This is given by the trace of the $k$-th power of the adjacency matrix, $\\mathrm{tr}(M^k)$. If $B = P^{\\top} A P$, then $B^k = P^{\\top} A^k P$, and by the cyclic property of the trace, $\\mathrm{tr}(B^k) = \\mathrm{tr}(P^{\\top} A^k P) = \\mathrm{tr}(A^k P P^{\\top}) = \\mathrm{tr}(A^k I) = \\mathrm{tr}(A^k)$. Thus, the traces of all powers of the adjacency matrices must be equal for isomorphic graphs.\n\nFor $k=0$, $\\mathrm{tr}(A^0) = \\mathrm{tr}(I_6) = 6$ and $\\mathrm{tr}(B^0) = \\mathrm{tr}(I_6) = 6$.\nFor $k=1$, $\\mathrm{tr}(A) = 0$ and $\\mathrm{tr}(B) = 0$, as expected for simple graphs.\nFor $k=2$, $\\mathrm{tr}(M^2) = \\sum_{i} d_i$. Since the degree sequences are the same, $\\mathrm{tr}(A^2) = \\sum d_A(i) = 12$ and $\\mathrm{tr}(B^2) = \\sum d_B(i) = 12$.\n\nLet's examine the case $k=3$. The value of $\\mathrm{tr}(M^3)$ is the total number of closed walks of length $3$. In a simple, undirected graph, the only closed walks of length $3$ are triangles (3-cycles). Each triangle $\\{i,j,k\\}$ corresponds to $6$ directed closed walks ($i \\to j \\to k \\to i$, $i \\to k \\to j \\to i$, and their cyclic permutations), so $\\mathrm{tr}(M^3) = 6 \\times (\\text{number of triangles})$.\n\nConsider graph A. We can check for triangles or, more efficiently, analyze its structure. Let's attempt to partition the vertices into two sets $U$ and $V$ such that all edges connect a vertex in $U$ to one in $V$. Let $U = \\{1, 2\\}$ and $V = \\{3, 4, 5, 6\\}$.\nThe edges in graph A are: $(1,3), (1,4), (1,5), (2,3), (2,4), (2,6)$.\nWe observe that there are no edges connecting two vertices within $U$ (e.g., $(1,2)$) and no edges connecting any two vertices within $V$. All edges connect a vertex from $U$ to a vertex in $V$. Thus, graph A is a bipartite graph. A fundamental property of bipartite graphs is that they contain no odd-length cycles. Therefore, graph A contains no triangles. This implies that the number of triangles in A is $0$, and consequently, $\\mathrm{tr}(A^3) = 0$.\n\nNow consider graph B. We inspect its adjacency matrix for triangles. A triangle exists among vertices $\\{i,j,k\\}$ if $B_{ij}=B_{jk}=B_{ki}=1$. Let's test the set $\\{1,2,3\\}$.\n$B_{12}=1$, $B_{23}=1$, and $B_{31}=1$.\nThus, vertices $\\{1,2,3\\}$ form a triangle in graph B. The existence of at least one triangle means the number of triangles in B is at least $1$, and therefore $\\mathrm{tr}(B^3) \\ge 6$.\n\nSince we have found an invariant that differs, $\\mathrm{tr}(A^3) \\neq \\mathrm{tr}(B^3)$, we can definitively conclude that the graphs are not isomorphic.\n\nFor completeness, we can compute $\\mathrm{tr}(B^3)$ explicitly.\n$$\nB^2 = \\begin{pmatrix}\n0 & 1 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0\n\\end{pmatrix}^2 = \\begin{pmatrix}\n3 & 1 & 1 & 1 & 0 & 0 \\\\\n1 & 3 & 1 & 0 & 1 & 1 \\\\\n1 & 1 & 2 & 1 & 1 & 0 \\\\\n1 & 0 & 1 & 2 & 0 & 0 \\\\\n0 & 1 & 1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThe trace of $B^3$ can be calculated as $\\mathrm{tr}(B^3) = \\sum_{i,j,k} B_{ij}B_{jk}B_{ki}$.\nThis computation yields:\n$(B^3)_{11} = \\sum_j B_{1j}(B^2)_{j1} = B_{12}(B^2)_{21} + B_{13}(B^2)_{31} + B_{15}(B^2)_{51} = 1(1) + 1(1) + 1(0) = 2$.\n$(B^3)_{22} = \\sum_j B_{2j}(B^2)_{j2} = B_{21}(B^2)_{12} + B_{23}(B^2)_{32} + B_{24}(B^2)_{42} = 1(1) + 1(1) + 1(0) = 2$.\n$(B^3)_{33} = \\sum_j B_{3j}(B^2)_{j3} = B_{31}(B^2)_{13} + B_{32}(B^2)_{23} = 1(1) + 1(1) = 2$.\n$(B^3)_{44} = \\sum_j B_{4j}(B^2)_{j4} = B_{42}(B^2)_{24} + B_{46}(B^2)_{64} = 1(0) + 1(0) = 0$.\n$(B^3)_{55} = \\sum_j B_{5j}(B^2)_{j5} = B_{51}(B^2)_{15} = 1(0) = 0$.\n$(B^3)_{66} = \\sum_j B_{6j}(B^2)_{j6} = B_{64}(B^2)_{46} = 1(0) = 0$.\nSo, $\\mathrm{tr}(B^3) = 2+2+2+0+0+0 = 6$.\n\nComparing the invariants:\n$\\mathrm{tr}(A^3) = 0$\n$\\mathrm{tr}(B^3) = 6$\nSince $\\mathrm{tr}(A^3) \\neq \\mathrm{tr}(B^3)$, the eigenvalue spectra of $A$ and $B$ must also be different, as $\\mathrm{tr}(M^k) = \\sum_i \\lambda_i^k$. The dissimilarity of the spectra is another confirmation of non-isomorphism.\n\nBecause a graph invariant differs, the graphs represented by $A$ and $B$ are not isomorphic. Therefore, there exists no permutation matrix $P$ such that $B = P^{\\top} A P$.\nThe indicator variable $I$ is defined as $1$ if such a matrix exists and $0$ otherwise. We conclude that $I=0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "When graphs are not perfectly isomorphic but share large-scale structural similarities, spectral methods offer a robust framework for discovering alignments. This exercise provides a step-by-step guide through a complete spectral alignment pipeline, from creating low-dimensional node embeddings using the graph Laplacian to finding the optimal node-to-node mapping . Mastering this process is key to applying modern alignment techniques in fields like neuroscience and computational biology.",
            "id": "4279408",
            "problem": "Consider two simple undirected graphs $X$ and $Y$ on $n=3$ vertices with adjacency matrices\n$$\nA_X=\\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix},\\quad\nA_Y=\\begin{pmatrix}\n0 & 0 & 1 \\\\\n0 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix}.\n$$\nLet $D_X$ and $D_Y$ be the corresponding degree matrices, and let $L_X=D_X-A_X$ and $L_Y=D_Y-A_Y$ be the combinatorial Laplacians. Define the Laplacian Eigenmap embedding in dimension $k=2$ as follows: for each graph, compute the orthonormal eigenvectors of the Laplacian associated to the two smallest nonzero eigenvalues, arrange them as columns in an $n\\times k$ matrix, and use this matrix as the node embedding $U\\in\\mathbb{R}^{n\\times k}$ where the $i$-th row gives the $k$-dimensional coordinates of node $i$. Denote these embeddings by $U_X$ for graph $X$ and $U_Y$ for graph $Y$.\n\nUsing these embeddings, solve the orthogonal Procrustes problem: find the orthogonal matrix $R^\\star\\in\\mathbb{R}^{k\\times k}$ that minimizes the Frobenius norm objective $\\|U_X R-U_Y\\|_F$ subject to $R^\\top R=I$. Then, define the assignment cost matrix $C\\in\\mathbb{R}^{n\\times n}$ by $C_{ij}=\\|(U_X R^\\star)_{i,:}-(U_Y)_{j,:}\\|_2^2$, and compute the minimum total assignment cost over all permutations $\\pi$ of $\\{1,2,3\\}$, that is, $\\min_{\\pi}\\sum_{i=1}^{n} C_{i,\\pi(i)}$, using the Hungarian algorithm (also known as the Kuhn–Munkres algorithm).\n\nReport the exact value of this minimum total assignment cost as your final answer. No rounding is required, and no units are involved.",
            "solution": "### Step 1: Compute the Graph Laplacians\nFor an undirected graph with adjacency matrix $A$ and degree matrix $D$, the combinatorial Laplacian is $L = D - A$.\n\nFor graph $X$:\nThe adjacency matrix is $A_X = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}$.\nThe degrees of the vertices are $d_1 = 1$, $d_2 = 2$, and $d_3 = 1$. The degree matrix is $D_X = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\nThe Laplacian for graph $X$ is:\n$$L_X = D_X - A_X = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix}$$\n\nFor graph $Y$:\nThe adjacency matrix is $A_Y = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix}$.\nThe degrees of the vertices are $d_1 = 1$, $d_2 = 1$, and $d_3 = 2$. The degree matrix is $D_Y = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}$.\nThe Laplacian for graph $Y$ is:\n$$L_Y = D_Y - A_Y = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & -1 \\\\ 0 & 1 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix}$$\n\n### Step 2: Find Eigenvalues and Eigenvectors of the Laplacians\nWe need the eigenvectors associated with the two smallest non-zero eigenvalues.\n\nFor $L_X$:\nThe characteristic equation is $\\det(L_X - \\lambda I) = 0$, which gives $\\lambda(1-\\lambda)(\\lambda-3) = 0$. The eigenvalues are $\\lambda_1 = 0$, $\\lambda_2 = 1$, $\\lambda_3 = 3$. The two smallest non-zero eigenvalues are $1$ and $3$.\n\n- For $\\lambda=1$: The eigenvector $v$ satisfies $(L_X - I)v = 0$. This gives $v_2=0$ and $v_1=-v_3$. An orthonormal eigenvector is $u_X^{(1)} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$.\n- For $\\lambda=3$: The eigenvector $v$ satisfies $(L_X - 3I)v = 0$. This gives $v_2=-2v_1$ and $v_1=v_3$. An orthonormal eigenvector is $u_X^{(2)} = \\frac{1}{\\sqrt{6}}\\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n\nFor $L_Y$:\nThe characteristic equation is $\\det(L_Y - \\lambda I) = 0$, which gives $\\lambda(1-\\lambda)(\\lambda-3) = 0$. The eigenvalues are the same: $\\lambda_1 = 0$, $\\lambda_2 = 1$, $\\lambda_3 = 3$. The two smallest non-zero eigenvalues are $1$ and $3$.\n\n- For $\\lambda=1$: The eigenvector $v$ satisfies $(L_Y - I)v = 0$. This gives $v_3=0$ and $v_1=-v_2$. An orthonormal eigenvector is $u_Y^{(1)} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\n- For $\\lambda=3$: The eigenvector $v$ satisfies $(L_Y - 3I)v = 0$. This gives $v_3=-2v_1$ and $v_1=v_2$. An orthonormal eigenvector is $u_Y^{(2)} = \\frac{1}{\\sqrt{6}}\\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix}$.\n\n### Step 3: Construct the Embedding Matrices\nThe embedding matrices $U_X$ and $U_Y$ are constructed by arranging the orthonormal eigenvectors as columns, ordered by their corresponding eigenvalues. The dimension is $n=3, k=2$.\n\n$$ U_X = \\begin{pmatrix} u_X^{(1)} & u_X^{(2)} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & -\\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\end{pmatrix} $$\n$$ U_Y = \\begin{pmatrix} u_Y^{(1)} & u_Y^{(2)} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & -\\frac{2}{\\sqrt{6}} \\end{pmatrix} $$\n\n### Step 4: Solve the Orthogonal Procrustes Problem\nWe need to find an orthogonal matrix $R^\\star \\in \\mathbb{R}^{2\\times 2}$ that minimizes $\\|U_X R - U_Y\\|_F$. The solution is found via the Singular Value Decomposition (SVD) of the matrix $M = U_X^\\top U_Y$. If $M = U_S \\Sigma V_S^\\top$ is the SVD of $M$, then $R^\\star = U_S V_S^\\top$.\n\n$$ M = U_X^\\top U_Y = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{6}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & -\\frac{2}{\\sqrt{6}} \\end{pmatrix} $$\n$$ M = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{\\sqrt{12}} + \\frac{2}{\\sqrt{12}} \\\\ \\frac{1}{\\sqrt{12}} + \\frac{2}{\\sqrt{12}} & \\frac{1}{6} - \\frac{2}{6} - \\frac{2}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{3}{\\sqrt{12}} \\\\ \\frac{3}{\\sqrt{12}} & -\\frac{3}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\end{pmatrix} $$\nTo find the SVD of $M$, we note that $M$ is symmetric. Also, $M^\\top M = M^2 = I$, so $M$ is orthogonal.\nFor an orthogonal matrix $M$, we can write its SVD as $M = M I I^\\top$. This gives SVD components $U_S = M$, $\\Sigma = I$, and $V_S = I$.\nThe optimal orthogonal matrix is $R^\\star = U_S V_S^\\top = M I^\\top = M$.\n$$ R^\\star = \\begin{pmatrix} \\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\end{pmatrix} $$\n\n### Step 5: Compute the Assignment Cost Matrix\nThe cost matrix $C \\in \\mathbb{R}^{3\\times 3}$ is given by $C_{ij} = \\|(U_X R^\\star)_{i,:} - (U_Y)_{j,:}\\|_2^2$.\nFirst, let's compute the aligned embedding $U_X' = U_X R^\\star$.\n$$ U_X' = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & -\\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\end{pmatrix} $$\nAfter matrix multiplication, we find that $U_X R^\\star = U_Y$. The alignment is perfect, meaning the minimum value of $\\|U_X R - U_Y\\|_F$ is $0$.\n\nNow we build the cost matrix $C$:\n$$ C_{ij} = \\|(U_Y)_{i,:} - (U_Y)_{j,:}\\|_2^2 $$\nThis is the matrix of squared Euclidean distances between the embedded points of graph $Y$. Let $p_j = (U_Y)_{j,:}$.\n$p_1 = (\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{6}})$, $p_2 = (-\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{6}})$, $p_3 = (0, -\\frac{2}{\\sqrt{6}})$.\n$C_{11}=C_{22}=C_{33}=0$.\n$C_{12} = \\|p_1-p_2\\|^2 = \\|(\\frac{2}{\\sqrt{2}}, 0)\\|^2 = (\\sqrt{2})^2 = 2$.\n$C_{13} = \\|p_1-p_3\\|^2 = \\|(\\frac{1}{\\sqrt{2}}, \\frac{3}{\\sqrt{6}})\\|^2 = (\\frac{1}{\\sqrt{2}})^2 + (\\frac{3}{\\sqrt{6}})^2 = \\frac{1}{2} + \\frac{9}{6} = \\frac{1}{2} + \\frac{3}{2} = 2$.\n$C_{23} = \\|p_2-p_3\\|^2 = \\|(-\\frac{1}{\\sqrt{2}}, \\frac{3}{\\sqrt{6}})\\|^2 = (-\\frac{1}{\\sqrt{2}})^2 + (\\frac{3}{\\sqrt{6}})^2 = \\frac{1}{2} + \\frac{9}{6} = 2$.\nSince $C$ is symmetric, $C_{ji}=C_{ij}$.\n$$ C = \\begin{pmatrix} 0 & 2 & 2 \\\\ 2 & 0 & 2 \\\\ 2 & 2 & 0 \\end{pmatrix} $$\n\n### Step 6: Compute the Minimum Total Assignment Cost\nWe must find $\\min_{\\pi} \\sum_{i=1}^{3} C_{i,\\pi(i)}$, where $\\pi$ is a permutation of $\\{1,2,3\\}$. This is a linear assignment problem. The Hungarian algorithm would proceed as follows:\n\n1.  **Row and Column Reduction**: Subtracting the minimum element from each row and then each column. Since the minimum in every row and column is $0$, the matrix $C$ remains unchanged.\n2.  **Cover Zeros**: We need to find the minimum number of lines (rows or columns) to cover all zeros in the matrix. The zeros are at $(1,1)$, $(2,2)$, and $(3,3)$. These cannot be covered by fewer than $3$ lines.\n3.  **Optimal Assignment**: Since the minimum number of lines is $3$, which equals $n$, an optimal assignment exists among the current zero entries. The only assignment that selects one zero from each row and column is $(1,1), (2,2), (3,3)$. This corresponds to the identity permutation $\\pi(i)=i$.\n\nThe minimum total cost is the sum of the costs for this assignment from the original cost matrix $C$:\n$$ \\text{Cost} = C_{1,1} + C_{2,2} + C_{3,3} = 0 + 0 + 0 = 0 $$\nThe minimum total assignment cost is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The success of a network alignment algorithm hinges on the objective function it optimizes. This practice explores a subtle but critical pitfall in aligning sparse networks: the statistical dominance of non-edge agreements can obscure the true structural signal from the edges . By analyzing the problem under the Erdős–Rényi model, you will derive a principled reweighting scheme to counteract this bias, a vital skill for designing effective alignment scores in real-world scenarios.",
            "id": "4279336",
            "problem": "Consider two simple undirected graphs with no self-loops, $G_{1}$ and $G_{2}$, each on the same vertex set of size $n$, with adjacency matrices $A \\in \\{0,1\\}^{n \\times n}$ and $B \\in \\{0,1\\}^{n \\times n}$. Assume that $G_{1}$ is a realization of an Erdős–Rényi (ER) graph with edge probability $p_{1} \\in (0,1)$, and $G_{2}$ is a realization of an independent Erdős–Rényi graph with edge probability $p_{2} \\in (0,1)$. For any permutation matrix $P \\in \\{0,1\\}^{n \\times n}$, define the naive alignment agreement score that counts both edge agreements and non-edge agreements (i.e., matching ones and matching zeros) by\n$$\nS_{\\mathrm{naive}}(P) \\;=\\; \\sum_{1 \\le i < j \\le n} \\Big( \\mathbf{1}\\{A_{ij}=1,\\, B_{P(i)P(j)}=1\\} \\;+\\; \\mathbf{1}\\{A_{ij}=0,\\, B_{P(i)P(j)}=0\\} \\Big).\n$$\nBecause $(1-p_{1})(1-p_{2}) \\gg p_{1}p_{2}$ in sparse regimes, maximizing $S_{\\mathrm{naive}}(P)$ can be biased toward aligning non-edges, thereby obscuring the structural signal carried by edges.\n\nTo mitigate this bias, consider the reweighted agreement objective\n$$\nS_{w}(P) \\;=\\; \\sum_{1 \\le i < j \\le n} \\Big( w_{1}\\,\\mathbf{1}\\{A_{ij}=1,\\, B_{P(i)P(j)}=1\\} \\;+\\; w_{0}\\,\\mathbf{1}\\{A_{ij}=0,\\, B_{P(i)P(j)}=0\\} \\Big),\n$$\nwith weights $w_{1} > 0$ and $w_{0} > 0$. Under the null model in which $P$ is independent of $(A,B)$ and $G_{1},G_{2}$ are independent ER graphs with parameters $p_{1}$ and $p_{2}$ as above, derive the exact ratio of weights that equalizes the expected total contribution of edge agreements and non-edge agreements to $S_{w}(P)$, thereby removing the class-imbalance bias at the expectation level.\n\nProvide your final answer as a single closed-form expression for $w_{0}^{\\star}/w_{1}^{\\star}$ in terms of $p_{1}$ and $p_{2}$. No rounding is required. Express your answer without units.",
            "solution": "The objective is to find the ratio of weights $w_{0}/w_{1}$ that equalizes the expected contributions of edge agreements and non-edge agreements to the reweighted score $S_{w}(P)$. Let the total contribution from edge agreements be denoted by $C_{1}(P)$ and the total contribution from non-edge agreements be denoted by $C_{0}(P)$. These are defined as:\n$$\nC_{1}(P) = \\sum_{1 \\le i < j \\le n} w_{1}\\,\\mathbf{1}\\{A_{ij}=1,\\, B_{P(i)P(j)}=1\\}\n$$\n$$\nC_{0}(P) = \\sum_{1 \\le i < j \\le n} w_{0}\\,\\mathbf{1}\\{A_{ij}=0,\\, B_{P(i)P(j)}=0\\}\n$$\nThe problem requires us to find the specific weights, which we denote as $w_{1}^{\\star}$ and $w_{0}^{\\star}$, such that the expectations of these two quantities are equal under the null model. The condition is therefore:\n$$\n\\mathbb{E}[C_{1}(P)] = \\mathbb{E}[C_{0}(P)]\n$$\nWe will now calculate the expectation of each term separately.\n\nFirst, let us compute the expected total contribution from edge agreements, $\\mathbb{E}[C_{1}(P)]$. By linearity of expectation, we can move the expectation operator inside the summation:\n$$\n\\mathbb{E}[C_{1}(P)] = \\mathbb{E}\\left[ \\sum_{1 \\le i < j \\le n} w_{1}\\,\\mathbf{1}\\{A_{ij}=1,\\, B_{P(i)P(j)}=1\\} \\right] = w_{1} \\sum_{1 \\le i < j \\le n} \\mathbb{E}\\left[ \\mathbf{1}\\{A_{ij}=1,\\, B_{P(i)P(j)}=1\\} \\right]\n$$\nThe expectation of an indicator function is the probability of the event it indicates. Thus, we have:\n$$\n\\mathbb{E}\\left[ \\mathbf{1}\\{A_{ij}=1,\\, B_{P(i)P(j)}=1\\} \\right] = \\mathbb{P}\\left(A_{ij}=1 \\text{ and } B_{P(i)P(j)}=1\\right)\n$$\nUnder the specified null model, the graphs $G_{1}$ (with adjacency matrix $A$) and $G_{2}$ (with adjacency matrix $B$) are independent realizations of Erdős–Rényi graphs, and the permutation $P$ is independent of both $A$ and $B$. This independence allows us to factor the joint probability:\n$$\n\\mathbb{P}\\left(A_{ij}=1 \\text{ and } B_{P(i)P(j)}=1\\right) = \\mathbb{P}(A_{ij}=1) \\times \\mathbb{P}(B_{P(i)P(j)}=1)\n$$\nFor an Erdős–Rényi graph with edge probability $p_{1}$, the probability of an edge existing between any two distinct vertices $i$ and $j$ is $\\mathbb{P}(A_{ij}=1) = p_{1}$.\nSimilarly, for an Erdős–Rényi graph with edge probability $p_{2}$, the probability of an edge existing between any two distinct vertices $u$ and $v$ is $\\mathbb{P}(B_{uv}=1) = p_{2}$. Since $P$ is a permutation matrix and $i \\neq j$, the mapped vertices $P(i)$ and $P(j)$ are also distinct. Therefore, $\\mathbb{P}(B_{P(i)P(j)}=1) = p_{2}$.\nSubstituting these probabilities back, we get:\n$$\n\\mathbb{P}\\left(A_{ij}=1 \\text{ and } B_{P(i)P(j)}=1\\right) = p_{1} p_{2}\n$$\nThe summation is over all distinct pairs of vertices $(i, j)$ with $1 \\le i < j \\le n$. The total number of such pairs is $\\binom{n}{2}$. Therefore, the expected total contribution from edge agreements is:\n$$\n\\mathbb{E}[C_{1}(P)] = w_{1} \\sum_{1 \\le i < j \\le n} p_{1}p_{2} = w_{1} \\binom{n}{2} p_{1}p_{2}\n$$\n\nNext, we compute the expected total contribution from non-edge agreements, $\\mathbb{E}[C_{0}(P)]$, in an analogous manner.\n$$\n\\mathbb{E}[C_{0}(P)] = \\mathbb{E}\\left[ \\sum_{1 \\le i < j \\le n} w_{0}\\,\\mathbf{1}\\{A_{ij}=0,\\, B_{P(i)P(j)}=0\\} \\right] = w_{0} \\sum_{1 \\le i < j \\le n} \\mathbb{E}\\left[ \\mathbf{1}\\{A_{ij}=0,\\, B_{P(i)P(j)}=0\\} \\right]\n$$\nThis requires computing the probability $\\mathbb{P}\\left(A_{ij}=0 \\text{ and } B_{P(i)P(j)}=0\\right)$. Again, due to independence:\n$$\n\\mathbb{P}\\left(A_{ij}=0 \\text{ and } B_{P(i)P(j)}=0\\right) = \\mathbb{P}(A_{ij}=0) \\times \\mathbb{P}(B_{P(i)P(j)}=0)\n$$\nThe probability of a non-edge in an ER graph is $\\mathbb{P}(A_{ij}=0) = 1 - \\mathbb{P}(A_{ij}=1) = 1 - p_{1}$.\nSimilarly, $\\mathbb{P}(B_{P(i)P(j)}=0) = 1 - \\mathbb{P}(B_{P(i)P(j)}=1) = 1 - p_{2}$.\nSo, the probability for a non-edge agreement is:\n$$\n\\mathbb{P}\\left(A_{ij}=0 \\text{ and } B_{P(i)P(j)}=0\\right) = (1 - p_{1})(1 - p_{2})\n$$\nThe expected total contribution from non-edge agreements is the sum over all $\\binom{n}{2}$ pairs:\n$$\n\\mathbb{E}[C_{0}(P)] = w_{0} \\sum_{1 \\le i < j \\le n} (1 - p_{1})(1 - p_{2}) = w_{0} \\binom{n}{2} (1 - p_{1})(1 - p_{2})\n$$\n\nFinally, we enforce the condition that the expected contributions are equal, using the weights $w_{1}^{\\star}$ and $w_{0}^{\\star}$:\n$$\nw_{1}^{\\star} \\binom{n}{2} p_{1}p_{2} = w_{0}^{\\star} \\binom{n}{2} (1 - p_{1})(1 - p_{2})\n$$\nAssuming the number of vertices $n \\ge 2$, the term $\\binom{n}{2} = \\frac{n(n-1)}{2}$ is non-zero and can be canceled from both sides:\n$$\nw_{1}^{\\star} p_{1}p_{2} = w_{0}^{\\star} (1 - p_{1})(1 - p_{2})\n$$\nThe problem asks for the ratio $w_{0}^{\\star}/w_{1}^{\\star}$. We can solve for this ratio by rearranging the terms. Since $p_{1}, p_{2} \\in (0,1)$, the quantities $p_{1}p_{2}$ and $(1-p_{1})(1-p_{2})$ are non-zero, and $w_{1}^{\\star}$ is given as positive, so the division is well-defined.\n$$\n\\frac{w_{0}^{\\star}}{w_{1}^{\\star}} = \\frac{p_{1}p_{2}}{(1 - p_{1})(1 - p_{2})}\n$$\nThis expression gives the exact ratio of weights that balances the expected contributions of edge and non-edge agreements under the null model, thereby removing the specified bias.",
            "answer": "$$\n\\boxed{\\frac{p_{1}p_{2}}{(1 - p_{1})(1 - p_{2})}}\n$$"
        }
    ]
}