{
    "hands_on_practices": [
        {
            "introduction": "Before we can navigate a network using its hidden geometry, we must first uncover this latent structure from the observable network topology. This exercise guides you through a cornerstone of this process: using Maximum Likelihood Estimation (MLE) to connect a node's observed degree to its unobserved \"popularity\" parameter, $\\kappa_{i}$, which in turn defines its radial coordinate in the hidden space. By working through this derivation , you will gain a practical understanding of how the geometric map for navigation can be inferred from real-world network data.",
            "id": "4289367",
            "problem": "Consider a network generation mechanism in the class of hidden metric space models in a circle-based formulation (the so-called $\\mathbb{S}^{1}$ model), where each node $i$ is assigned an angular coordinate $\\theta_{i} \\in [0,2\\pi)$ and a popularity (hidden degree) parameter $\\kappa_{i}  0$. Conditional on $\\{\\theta_{i}\\}$ and $\\{\\kappa_{i}\\}$, edges are independent and the connection probability between nodes $i$ and $j$ is\n$$\np_{ij} \\equiv \\frac{1}{1 + \\chi_{ij}^{\\beta}}, \\quad \\chi_{ij} \\equiv \\frac{N \\Delta\\theta_{ij}}{2\\pi \\mu \\kappa_{i} \\kappa_{j}},\n$$\nwhere $N$ is the number of nodes, $\\mu  0$ is a global scale parameter controlling expected degree, $\\beta  1$ is a clustering control parameter, and $\\Delta\\theta_{ij} \\equiv \\pi - |\\pi - |\\theta_{i} - \\theta_{j}||$ is the geodesic angular distance in $[0,\\pi]$. The popularity parameters $\\{\\kappa_{i}\\}$ are mapped to hyperbolic radial coordinates $\\{r_{i}\\}$ via\n$$\nr_{i} = R - 2 \\ln \\kappa_{i},\n$$\nwhere $R$ is a fixed constant setting the disk radius. Let $A$ be the adjacency matrix with entries $a_{ij} \\in \\{0,1\\}$, and denote by $k_{i} \\equiv \\sum_{j \\neq i} a_{ij}$ the observed degree of node $i$.\n\nStarting from the independence of edges and the definition of the network likelihood, answer the following:\n\n1. Derive the derivative of the log-likelihood with respect to the radial coordinate $r_{i}$, expressing your result in terms of $k_{i}$, $\\kappa_{i}$, and $\\{p_{ij}\\}_{j \\neq i}$, without introducing any shortcut formulas beyond those explicitly defined above.\n\n2. Assume the angles $\\{\\theta_{j}\\}_{j \\neq i}$ are independent and identically distributed as uniform on $[0,2\\pi)$, and consider the sparse large-$N$ regime in which the integral approximation applies. Show that the mean-field approximation of the expected degree of node $i$ can be written as\n$$\n\\sum_{j \\neq i} p_{ij} \\approx C_{\\beta} \\, \\kappa_{i},\n$$\nand obtain the explicit expression of $C_{\\beta}$ in terms of $\\mu$, $\\beta$, and the average popularity $\\bar{\\kappa} \\equiv \\frac{1}{N} \\sum_{j=1}^{N} \\kappa_{j}$.\n\n3. Using the result of part $1$ and the mean-field approximation of part $2$, determine the maximum likelihood estimator for $\\kappa_{i}$ in closed form when the global scale $\\mu$ is calibrated so that the model reproduces the observed average degree, that is, so that $\\mathbb{E}[k_{i} \\mid \\kappa_{i}] = \\kappa_{i}$ holds on average over the network.\n\nYour final answer must be a single closed-form analytical expression for the calibrated estimator of $\\kappa_{i}$ in terms of the observed degree $k_{i}$. No rounding is required. Express angles in radians if they appear.",
            "solution": "The problem is first validated to be self-contained, scientifically grounded, and well-posed. All definitions and parameters for the $\\mathbb{S}^{1}$ hidden metric space model are explicitly provided. The questions are standard derivations and applications of statistical estimation theory within the context of network science, specifically maximum likelihood estimation and mean-field approximations. The problem is therefore valid.\n\nThe solution proceeds in three parts as requested.\n\nPart 1: Derivative of the log-likelihood\n\nThe likelihood of observing a network with adjacency matrix $A$ with entries $a_{ij} \\in \\{0,1\\}$, given the parameters $\\{\\theta_k, \\kappa_k\\}$, is based on the independence of edges. It is given by the product over all distinct pairs of nodes $(j,k)$ with $jk$:\n$$\n\\mathcal{L} = \\prod_{jk} p_{jk}^{a_{jk}} (1-p_{jk})^{1-a_{jk}}\n$$\nThe log-likelihood, $\\ln \\mathcal{L}$, is the sum of the logarithms of these terms:\n$$\n\\ln \\mathcal{L} = \\sum_{jk} \\left[ a_{jk} \\ln(p_{jk}) + (1-a_{jk}) \\ln(1-p_{jk}) \\right]\n$$\nTo find the derivative with respect to the radial coordinate $r_i$ of a specific node $i$, we only need to consider terms in the sum that involve node $i$. The sum can be rewritten isolating terms with index $i$:\n$$\n\\ln \\mathcal{L} = \\sum_{j \\neq i} \\left[ a_{ij} \\ln(p_{ij}) + (1-a_{ij}) \\ln(1-p_{ij}) \\right] + \\text{terms not dependent on } r_i\n$$\nWe take the partial derivative of $\\ln \\mathcal{L}$ with respect to $r_i$:\n$$\n\\frac{\\partial \\ln \\mathcal{L}}{\\partial r_i} = \\sum_{j \\neq i} \\left[ \\frac{a_{ij}}{p_{ij}} \\frac{\\partial p_{ij}}{\\partial r_i} - \\frac{1-a_{ij}}{1-p_{ij}} \\frac{\\partial p_{ij}}{\\partial r_i} \\right] = \\sum_{j \\neq i} \\left( \\frac{a_{ij} - a_{ij} p_{ij} - p_{ij} + a_{ij} p_{ij}}{p_{ij}(1-p_{ij})} \\right) \\frac{\\partial p_{ij}}{\\partial r_i} = \\sum_{j \\neq i} \\frac{a_{ij} - p_{ij}}{p_{ij}(1-p_{ij})} \\frac{\\partial p_{ij}}{\\partial r_i}\n$$\nNext, we must calculate the derivative $\\frac{\\partial p_{ij}}{\\partial r_i}$. The connection probability $p_{ij}$ depends on $r_i$ through the popularity parameter $\\kappa_i$. The given mapping is $r_i = R - 2 \\ln \\kappa_i$, which implies $\\ln \\kappa_i = \\frac{R-r_i}{2}$, and thus $\\kappa_i = \\exp\\left(\\frac{R-r_i}{2}\\right)$.\nThe connection probability is $p_{ij} = (1 + \\chi_{ij}^{\\beta})^{-1}$, where $\\chi_{ij} = \\frac{N \\Delta\\theta_{ij}}{2\\pi \\mu \\kappa_i \\kappa_j}$. We can write $\\chi_{ij}$ as a function of $r_i$:\n$$\n\\chi_{ij} = \\frac{N \\Delta\\theta_{ij}}{2\\pi \\mu \\kappa_j} \\frac{1}{\\kappa_i} = \\frac{N \\Delta\\theta_{ij}}{2\\pi \\mu \\kappa_j} \\exp\\left(-\\frac{R-r_i}{2}\\right) = C_{ij} \\exp\\left(\\frac{r_i-R}{2}\\right)\n$$\nwhere $C_{ij}$ contains all terms not depending on $r_i$.\nUsing the chain rule, $\\frac{\\partial p_{ij}}{\\partial r_i} = \\frac{\\partial p_{ij}}{\\partial \\chi_{ij}} \\frac{\\partial \\chi_{ij}}{\\partial r_i}$.\nFirst, $\\frac{\\partial \\chi_{ij}}{\\partial r_i} = C_{ij} \\exp\\left(\\frac{r_i-R}{2}\\right) \\cdot \\frac{1}{2} = \\frac{1}{2} \\chi_{ij}$.\nSecond, $\\frac{\\partial p_{ij}}{\\partial \\chi_{ij}} = -(1+\\chi_{ij}^{\\beta})^{-2} \\cdot (\\beta \\chi_{ij}^{\\beta-1}) = -p_{ij}^2 \\beta \\chi_{ij}^{\\beta-1}$.\nCombining these gives:\n$$\n\\frac{\\partial p_{ij}}{\\partial r_i} = (-p_{ij}^2 \\beta \\chi_{ij}^{\\beta-1}) \\left(\\frac{1}{2} \\chi_{ij}\\right) = -\\frac{\\beta}{2} p_{ij}^2 \\chi_{ij}^{\\beta}\n$$\nWe use the identity $1-p_{ij} = 1 - \\frac{1}{1+\\chi_{ij}^{\\beta}} = \\frac{\\chi_{ij}^{\\beta}}{1+\\chi_{ij}^{\\beta}} = p_{ij} \\chi_{ij}^{\\beta}$.\nSubstituting this into the expression for the derivative:\n$$\n\\frac{\\partial p_{ij}}{\\partial r_i} = -\\frac{\\beta}{2} p_{ij} (p_{ij} \\chi_{ij}^{\\beta}) = -\\frac{\\beta}{2} p_{ij} (1-p_{ij})\n$$\nNow, we substitute this result back into the expression for $\\frac{\\partial \\ln \\mathcal{L}}{\\partial r_i}$:\n$$\n\\frac{\\partial \\ln \\mathcal{L}}{\\partial r_i} = \\sum_{j \\neq i} \\frac{a_{ij} - p_{ij}}{p_{ij}(1-p_{ij})} \\left( -\\frac{\\beta}{2} p_{ij} (1-p_{ij}) \\right) = -\\frac{\\beta}{2} \\sum_{j \\neq i} (a_{ij} - p_{ij})\n$$\nThe sum $\\sum_{j \\neq i} a_{ij}$ is the definition of the observed degree $k_i$ of node $i$. Therefore, the derivative is:\n$$\n\\frac{\\partial \\ln \\mathcal{L}}{\\partial r_i} = -\\frac{\\beta}{2} \\left( k_i - \\sum_{j \\neq i} p_{ij} \\right)\n$$\nThis expression is in terms of $k_i$ and the set $\\{p_{ij}\\}_{j \\neq i}$. Note that $p_{ij}$ is an implicit function of $\\kappa_i$ (and thus $r_i$), as required.\n\nPart 2: Mean-field approximation of the expected degree\n\nThe expected degree of node $i$, conditional on its parameters, is $\\mathbb{E}[k_i \\mid \\kappa_i, \\theta_i] = \\sum_{j \\neq i} p_{ij}$. We need to approximate this sum in the large-$N$ sparse regime, assuming node angles $\\{\\theta_j\\}_{j \\neq i}$ are i.i.d. $U[0, 2\\pi)$. This suggests replacing the sum with an integral over the distribution of node properties. In a mean-field spirit, we replace the specific popularity $\\kappa_j$ of each node $j$ with the network average popularity $\\bar{\\kappa}$. The expected degree is then approximated by averaging over the uniform distribution of angles.\nFor large $N$, we can replace the sum over $N-1$ nodes with an integral over $N$ nodes.\n$$\n\\mathbb{E}[k_i] \\approx N \\int_0^{2\\pi} p_{ij}(\\theta_i, \\theta_j; \\kappa_i, \\bar{\\kappa}) \\frac{d\\theta_j}{2\\pi}\n$$\nThe probability $p_{ij}$ depends on $\\theta_j$ only through the angular distance $\\Delta\\theta_{ij} = \\pi - |\\pi - |\\theta_i - \\theta_j||$. Because the integration is over the full circle $[0, 2\\pi)$, the result is independent of the specific value of $\\theta_i$. The probability density function of the geodesic distance $\\Delta\\theta$ between two uniformly random points on a circle is $f(\\Delta\\theta) = 1/\\pi$ for $\\Delta\\theta \\in [0, \\pi]$. Thus, we can integrate over $\\Delta\\theta$ directly:\n$$\n\\mathbb{E}[k_i] \\approx N \\int_0^\\pi \\frac{1}{1 + \\left(\\frac{N \\Delta\\theta}{2\\pi \\mu \\kappa_i \\bar{\\kappa}}\\right)^\\beta} \\frac{d(\\Delta\\theta)}{\\pi}\n$$\nLet's perform a change of variables. Let $u = \\frac{N \\Delta\\theta}{2\\pi \\mu \\kappa_i \\bar{\\kappa}}$. Then $\\Delta\\theta = \\frac{2\\pi \\mu \\kappa_i \\bar{\\kappa}}{N} u$, and $d(\\Delta\\theta) = \\frac{2\\pi \\mu \\kappa_i \\bar{\\kappa}}{N} du$. The integration limits for $u$ are from $0$ to $\\frac{N\\pi}{2\\pi \\mu \\kappa_i \\bar{\\kappa}} = \\frac{N}{2 \\mu \\kappa_i \\bar{\\kappa}}$.\n$$\n\\mathbb{E}[k_i] \\approx N \\cdot \\frac{1}{\\pi} \\int_0^{\\frac{N}{2\\mu\\kappa_i\\bar{\\kappa}}} \\frac{1}{1+u^\\beta} \\left( \\frac{2\\pi \\mu \\kappa_i \\bar{\\kappa}}{N} \\right) du = 2\\mu\\kappa_i\\bar{\\kappa} \\int_0^{\\frac{N}{2\\mu\\kappa_i\\bar{\\kappa}}} \\frac{du}{1+u^\\beta}\n$$\nIn the large-$N$ limit for a sparse network, the upper limit of the integral tends to infinity. This allows us to approximate the integral by its value from $0$ to $\\infty$:\n$$\n\\int_0^{\\infty} \\frac{du}{1+u^\\beta} = \\frac{\\pi/\\beta}{\\sin(\\pi/\\beta)}\n$$\nThis integral converges for $\\beta  1$, which is a condition given in the problem statement.\nSubstituting this result back, we get the mean-field approximation for the expected degree:\n$$\n\\mathbb{E}[k_i] \\approx \\left( 2\\mu\\bar{\\kappa} \\frac{\\pi/\\beta}{\\sin(\\pi/\\beta)} \\right) \\kappa_i = \\left( \\frac{2\\pi\\mu\\bar{\\kappa}}{\\beta \\sin(\\pi/\\beta)}\\right) \\kappa_i\n$$\nThis is of the form $\\mathbb{E}[k_i] \\approx C_{\\beta} \\kappa_i$, where the constant $C_{\\beta}$ is:\n$$\nC_{\\beta} = \\frac{2\\pi \\mu \\bar{\\kappa}}{\\beta \\sin(\\pi/\\beta)}\n$$\n\nPart 3: Maximum likelihood estimator for $\\kappa_i$\n\nThe maximum likelihood estimator (MLE) for the parameters is found by setting the derivatives of the log-likelihood to zero. From Part 1, the condition for the MLE of $r_i$ (and equivalently for $\\kappa_i$) is $\\frac{\\partial \\ln \\mathcal{L}}{\\partial r_i} = 0$, which gives:\n$$\n-\\frac{\\beta}{2} \\left( k_i - \\sum_{j \\neq i} p_{ij} \\right) = 0 \\implies k_i = \\sum_{j \\neq i} \\hat{p}_{ij}\n$$\nHere, $\\hat{p}_{ij}$ denotes the connection probability calculated using the estimated parameters, e.g., $\\hat{\\kappa}_i, \\hat{\\kappa}_j$. This is a system of $N$ coupled equations for all $\\{\\hat{\\kappa}_i\\}$.\nThe problem instructs us to use the mean-field approximation from Part 2 to simplify this. We replace the sum $\\sum_{j \\neq i} \\hat{p}_{ij}$ with its approximate expected value, $C_\\beta \\hat{\\kappa}_i$. The MLE equation for node $i$ then becomes decoupled from the other nodes:\n$$\nk_i \\approx C_{\\beta} \\hat{\\kappa}_i\n$$\nThis gives an estimator for $\\kappa_i$ as $\\hat{\\kappa}_i \\approx k_i / C_{\\beta}$.\nFinally, we apply the calibration condition. The problem states that the parameter $\\mu$ is calibrated such that the model reproduces the observed average degree, which is expressed as the condition that $\\mathbb{E}[k_{i} \\mid \\kappa_{i}] = \\kappa_{i}$ should hold on average. Using our mean-field result, this condition is:\n$$\n\\mathbb{E}[k_i \\mid \\kappa_i] \\approx C_\\beta \\kappa_i = \\kappa_i\n$$\nThis equality must hold for the model to be calibrated as specified, which directly implies that the constant $C_\\beta$ must be equal to $1$:\n$$\nC_{\\beta} = 1\n$$\nBy applying this calibration, the MLE equation simplifies significantly:\n$$\nk_i \\approx 1 \\cdot \\hat{\\kappa}_i\n$$\nThus, the maximum likelihood estimator for the popularity parameter $\\kappa_i$ under this calibration is simply the observed degree $k_i$.\n$$\n\\hat{\\kappa}_i = k_i\n$$\nThis result is a closed-form expression for the estimator $\\hat{\\kappa}_i$ in terms of the observable $k_i$.",
            "answer": "$$\\boxed{k_{i}}$$"
        },
        {
            "introduction": "In any realistic scenario, the inferred hidden coordinates used for navigation will be imperfect. This practice problem  explores the crucial question of how such estimation errors affect the performance of greedy routing. You will derive both a deterministic upper bound on the one-step navigation error and a probabilistic guarantee for selecting the correct next hop, providing a rigorous framework for understanding the robustness of routing algorithms in the face of noisy information.",
            "id": "4289366",
            "problem": "Consider a network whose nodes are embedded in a hidden Euclidean metric space with coordinates in $\\mathbb{R}^{d}$, where $d \\in \\mathbb{N}$ is fixed. A source node $i$ must forward a message toward a target node $t$ by greedy neighbor selection, i.e., at node $i$ it selects the neighbor $j \\in \\mathcal{N}(i)$ that minimizes the estimated distance to $t$. Let the true coordinates of node $j$ and the target $t$ be $x_{j}$ and $x_{t}$, with true distances $D_{j} = \\|x_{j} - x_{t}\\|_{2}$. Let the available estimates be $\\hat{x}_{j}$ and $\\hat{x}_{t}$ with coordinate estimation errors uniformly bounded as $\\|\\hat{x}_{j} - x_{j}\\|_{2} \\leq \\delta$ and $\\|\\hat{x}_{t} - x_{t}\\|_{2} \\leq \\delta$ for a known $\\delta  0$. The greedy rule uses estimated distances $\\hat{D}_{j} = \\|\\hat{x}_{j} - \\hat{x}_{t}\\|_{2}$ to select the forward neighbor.\n\nDefine the one-step navigation error as $E_{\\text{step}} = D_{\\text{sel}} - \\min_{j \\in \\mathcal{N}(i)} D_{j}$, where $D_{\\text{sel}}$ is the true distance of the neighbor selected based on the estimated distances.\n\nTask 1: Using only core properties of the Euclidean metric and the triangle inequality, derive a deterministic upper bound on $E_{\\text{step}}$ in terms of $\\delta$ only.\n\nTask 2: Now suppose instead that coordinate estimates are obtained by averaging $m$ independent and identically distributed (i.i.d.) bounded perturbations. Specifically, for each neighbor $j$, the estimated distance has the form $\\hat{D}_{j} = D_{j} + \\bar{\\xi}_{j}$ with $\\bar{\\xi}_{j} = \\frac{1}{m}\\sum_{\\ell=1}^{m}\\xi_{j\\ell}$, where the $\\xi_{j\\ell}$ are i.i.d. with mean $0$ and support in the interval $[-b, b]$ for a known $b  0$. Let the true margin between the closest and second closest neighbor distances be $\\gamma = D_{(2)} - D_{(1)}  0$, where $D_{(1)} = \\min_{j \\in \\mathcal{N}(i)} D_{j}$ and $D_{(2)}$ is the second-smallest distance. Using an appropriate concentration inequality for bounded variables, together with a union bound over all competitors of the closest neighbor, derive a sufficient condition on $m$ that guarantees the probability that the selected neighbor (based on $\\hat{D}_{j}$) is truly the closest is at least $1 - \\alpha$, for a given $\\alpha \\in (0,1)$.\n\nFinally, evaluate your sufficient condition for the case $b = 0.5$, $\\gamma = 0.8$, $|\\mathcal{N}(i)| = 20$, and $\\alpha = 0.01$, and report the minimal integer $m$ satisfying it as your final answer.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It presents two standard, formalizable tasks in the analysis of network navigation algorithms under uncertainty. All necessary parameters and definitions are provided, and there are no contradictions or ambiguities. The problem is a valid exercise in applying fundamental mathematical principles (triangle inequality, concentration inequalities) to a problem in network science.\n\nTask 1: Derivation of the deterministic upper bound on $E_{\\text{step}}$.\n\nLet $x_j, x_t \\in \\mathbb{R}^{d}$ be the true coordinates of a neighbor node $j$ and the target node $t$, respectively. The true distance is $D_j = \\|x_j - x_t\\|_2$. The estimated coordinates are $\\hat{x}_j$ and $\\hat{x}_t$, with the estimation error bounded by $\\|\\hat{x}_j - x_j\\|_2 \\leq \\delta$ and $\\|\\hat{x}_t - x_t\\|_2 \\leq \\delta$. The estimated distance is $\\hat{D}_j = \\|\\hat{x}_j - \\hat{x}_t\\|_2$.\n\nWe first establish a relationship between the true distance $D_j$ and the estimated distance $\\hat{D}_j$ for any node $j$. By the triangle inequality on the Euclidean norm $\\|\\cdot\\|_2$:\n$$ \\hat{D}_j = \\|\\hat{x}_j - \\hat{x}_t\\|_2 = \\|(\\hat{x}_j - x_j) + (x_j - x_t) + (x_t - \\hat{x}_t)\\|_2 $$\n$$ \\leq \\|\\hat{x}_j - x_j\\|_2 + \\|x_j - x_t\\|_2 + \\|x_t - \\hat{x}_t\\|_2 $$\nUsing the given error bounds, we have:\n$$ \\hat{D}_j \\leq \\delta + D_j + \\delta = D_j + 2\\delta $$\nSimilarly, we can express $D_j$ in terms of $\\hat{D}_j$:\n$$ D_j = \\|x_j - x_t\\|_2 = \\|(x_j - \\hat{x}_j) + (\\hat{x}_j - \\hat{x}_t) + (\\hat{x}_t - x_t)\\|_2 $$\n$$ \\leq \\|x_j - \\hat{x}_j\\|_2 + \\|\\hat{x}_j - \\hat{x}_t\\|_2 + \\|\\hat{x}_t - x_t\\|_2 $$\n$$ D_j \\leq \\delta + \\hat{D}_j + \\delta = \\hat{D}_j + 2\\delta $$\nCombining these two inequalities, $D_j - 2\\delta \\leq \\hat{D}_j \\leq D_j + 2\\delta$, which is equivalent to $|\\hat{D}_j - D_j| \\leq 2\\delta$.\n\nThe one-step navigation error is defined as $E_{\\text{step}} = D_{\\text{sel}} - \\min_{j \\in \\mathcal{N}(i)} D_{j}$. Let $j^* \\in \\mathcal{N}(i)$ be the truly closest neighbor to the target $t$, such that $D_{j^*} = \\min_{j \\in \\mathcal{N}(i)} D_j$. Let $j_{\\text{sel}} \\in \\mathcal{N}(i)$ be the neighbor selected by the greedy algorithm, meaning it minimizes the estimated distance: $\\hat{D}_{j_{\\text{sel}}} = \\min_{j \\in \\mathcal{N}(i)} \\hat{D}_j$. By definition, $D_{\\text{sel}} = D_{j_{\\text{sel}}}$.\n\nTo find an upper bound for $E_{\\text{step}} = D_{j_{\\text{sel}}} - D_{j^*}$, we proceed as follows:\n1.  From the inequality $D_j \\leq \\hat{D}_j + 2\\delta$, we can state for the selected neighbor $j_{\\text{sel}}$:\n    $$ D_{j_{\\text{sel}}} \\leq \\hat{D}_{j_{\\text{sel}}} + 2\\delta $$\n2.  By the definition of the greedy selection, $\\hat{D}_{j_{\\text{sel}}}$ is the minimum of all estimated distances. Thus, it must be less than or equal to the estimated distance of the truly closest neighbor, $j^*$:\n    $$ \\hat{D}_{j_{\\text{sel}}} \\leq \\hat{D}_{j^*} $$\n3.  From the inequality $\\hat{D}_j \\leq D_j + 2\\delta$, we can state for the neighbor $j^*$:\n    $$ \\hat{D}_{j^*} \\leq D_{j^*} + 2\\delta $$\nCombining these steps:\n$$ D_{j_{\\text{sel}}} \\leq \\hat{D}_{j_{\\text{sel}}} + 2\\delta \\leq \\hat{D}_{j^*} + 2\\delta \\leq (D_{j^*} + 2\\delta) + 2\\delta = D_{j^*} + 4\\delta $$\nTherefore, the one-step navigation error is bounded by:\n$$ E_{\\text{step}} = D_{j_{\\text{sel}}} - D_{j^*} \\leq 4\\delta $$\nThe deterministic upper bound on $E_{\\text{step}}$ is $4\\delta$.\n\nTask 2: Derivation of the sufficient condition on $m$.\n\nLet $j_{(1)}$ be the neighbor with the minimum true distance $D_{(1)} = \\min_{j \\in \\mathcal{N}(i)} D_j$. Any other neighbor $j \\neq j_{(1)}$ has a true distance $D_j \\geq D_{(2)}$. The problem states a margin $\\gamma = D_{(2)} - D_{(1)}  0$.\nThe correct neighbor $j_{(1)}$ is selected if $\\hat{D}_{j_{(1)}}  \\hat{D}_j$ for all other neighbors $j \\in \\mathcal{N}(i) \\setminus \\{j_{(1)}\\}$.\nThe navigation fails if the selected neighbor is not $j_{(1)}$. This occurs if there exists at least one neighbor $j \\neq j_{(1)}$ such that $\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}$.\nLet $N = |\\mathcal{N}(i)|$. Using the union bound, the probability of failure can be bounded by the sum of probabilities of individual error events:\n$$ P(\\text{failure}) = P\\left(\\bigcup_{j \\neq j_{(1)}} \\{\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}\\}\\right) \\leq \\sum_{j \\neq j_{(1)}} P(\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}) $$\nLet's analyze the condition $\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}$. Substituting the model $\\hat{D}_k = D_k + \\bar{\\xi}_k$:\n$$ D_j + \\bar{\\xi}_j \\leq D_{j_{(1)}} + \\bar{\\xi}_{j_{(1)}} $$\n$$ \\bar{\\xi}_{j_{(1)}} - \\bar{\\xi}_j \\geq D_j - D_{j_{(1)}} $$\nFor any $j \\neq j_{(1)}$, we have $D_j - D_{j_{(1)}} \\geq D_{(2)} - D_{(1)} = \\gamma$.\nTherefore, a failure event $\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}$ implies $\\bar{\\xi}_{j_{(1)}} - \\bar{\\xi}_j \\geq \\gamma$. This gives us an upper bound on the probability:\n$$ P(\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}) \\leq P(\\bar{\\xi}_{j_{(1)}} - \\bar{\\xi}_j \\geq \\gamma) $$\nLet $Z_j = \\bar{\\xi}_{j_{(1)}} - \\bar{\\xi}_j = \\frac{1}{m}\\sum_{\\ell=1}^{m}(\\xi_{j_{(1)}\\ell} - \\xi_{j\\ell})$.\nThe random variables $\\xi_{k\\ell}$ are i.i.d with mean $0$ and support in $[-b, b]$. The term $Y_\\ell = \\xi_{j_{(1)}\\ell} - \\xi_{j\\ell}$ is also a random variable with mean $E[Y_\\ell] = E[\\xi_{j_{(1)}\\ell}] - E[\\xi_{j\\ell}] = 0 - 0 = 0$. The support of $Y_\\ell$ is $[-2b, 2b]$, which has a length of $4b$. The variables $Y_\\ell$ are i.i.d. for $\\ell=1,\\dots,m$.\nWe can apply Hoeffding's inequality to the sum $S_m = \\sum_{\\ell=1}^m Y_\\ell = mZ_j$. Hoeffding's inequality states that for a sum $S_m$ of $m$ independent variables $Y_\\ell$ with range lengths $c_\\ell$, $P(S_m - E[S_m] \\ge t) \\le \\exp\\left(\\frac{-2t^2}{\\sum c_\\ell^2}\\right)$.\nHere, $E[S_m]=0$, $t = m\\gamma$ (since we test for $Z_j \\geq \\gamma$), and $c_\\ell = 4b$ for all $\\ell$.\n$$ P(Z_j \\geq \\gamma) = P(S_m \\geq m\\gamma) \\leq \\exp\\left( - \\frac{2(m\\gamma)^2}{\\sum_{\\ell=1}^m (4b)^2} \\right) = \\exp\\left( - \\frac{2m^2\\gamma^2}{m(16b^2)} \\right) = \\exp\\left( - \\frac{m\\gamma^2}{8b^2} \\right) $$\nApplying the union bound over the $N-1$ competitors to $j_{(1)}$:\n$$ P(\\text{failure}) \\leq \\sum_{j \\neq j_{(1)}} P(\\hat{D}_j \\leq \\hat{D}_{j_{(1)}}) \\leq \\sum_{j \\neq j_{(1)}} \\exp\\left( - \\frac{m\\gamma^2}{8b^2} \\right) = (N-1) \\exp\\left( - \\frac{m\\gamma^2}{8b^2} \\right) $$\nWe require the probability of success to be at least $1-\\alpha$, which means $P(\\text{failure}) \\leq \\alpha$. A sufficient condition is:\n$$ (N-1) \\exp\\left( - \\frac{m\\gamma^2}{8b^2} \\right) \\leq \\alpha $$\nSolving for $m$:\n$$ \\exp\\left( - \\frac{m\\gamma^2}{8b^2} \\right) \\leq \\frac{\\alpha}{N-1} $$\n$$ - \\frac{m\\gamma^2}{8b^2} \\leq \\ln\\left(\\frac{\\alpha}{N-1}\\right) $$\n$$ \\frac{m\\gamma^2}{8b^2} \\geq -\\ln\\left(\\frac{\\alpha}{N-1}\\right) = \\ln\\left(\\frac{N-1}{\\alpha}\\right) $$\n$$ m \\geq \\frac{8b^2}{\\gamma^2} \\ln\\left(\\frac{N-1}{\\alpha}\\right) $$\nThis is the sufficient condition on $m$.\n\nFinally, we evaluate this condition for $b = 0.5$, $\\gamma = 0.8$, $N=|\\mathcal{N}(i)| = 20$, and $\\alpha = 0.01$.\n$$ m \\geq \\frac{8(0.5)^2}{(0.8)^2} \\ln\\left(\\frac{20-1}{0.01}\\right) $$\n$$ m \\geq \\frac{8(0.25)}{0.64} \\ln\\left(\\frac{19}{0.01}\\right) $$\n$$ m \\geq \\frac{2}{0.64} \\ln(1900) $$\n$$ m \\geq 3.125 \\ln(1900) $$\nUsing the value $\\ln(1900) \\approx 7.5496$:\n$$ m \\geq 3.125 \\times 7.5496 \\approx 23.5925 $$\nSince $m$ must be an integer, the minimal integer value of $m$ that satisfies this condition is the smallest integer greater than or equal to $23.5925$.\n$$ m_{\\min} = \\lceil 23.5925 \\rceil = 24 $$",
            "answer": "$$\\boxed{24}$$"
        },
        {
            "introduction": "Beyond specific error models, we can ask a more fundamental question: what are the ultimate information-theoretic limits to successful navigation? This advanced problem  reframes the routing challenge in terms of information and uncertainty, using Fano's inequality to connect routing success to the mutual information between the noisy observations and the correct path. This exercise provides a powerful lens for quantifying the minimum informational cost required to achieve a desired level of navigation performance.",
            "id": "4289393",
            "problem": "Consider a navigation process in a complex network embedded in a hidden metric space, where greedy routing selects the next hop that is closest to the destination in the latent geometry. Let the hidden positions of nodes be corrupted by a noisy embedding observed as $Y_{0}$, and suppose that at any routing decision the correct next hop $X$ is one among $M$ candidates with uniform prior over the candidate set. Assume the routing decision is made from $Y_{0}$ alone unless additional side information $S$ (transmitted per decision) is provided. Let the mutual information (MI) between the correct next hop $X$ and the noisy embedding $Y_{0}$ be $I(X;Y_{0}) = I_{0}$, measured in bits, and let the MI with side information be $I(X;Y_{0}, S) = I_{0} + \\Delta I$. Assume that the side information has entropy $H(S)$ in bits and that the increase in MI is bounded by fundamental information-theoretic limits.\n\nStarting from fundamental definitions of entropy and mutual information, and using well-tested bounds connecting error probability to conditional entropy for $M$-ary decisions, derive a lower bound on the mutual information required to guarantee a target single-step success ratio $S^{\\ast}$ of greedy routing under the noisy embedding. Use this bound to determine the minimal number of additional bits $\\Delta I$ that must be provided by $S$ per decision to achieve $S^{\\ast}$, given that $I_{0}$ is fixed.\n\nFor a concrete instance, take $M = 32$, $I_{0} = 3.2$ bits, and a target single-step success ratio $S^{\\ast} = 0.9$. Express your final result for the minimal additional information $\\Delta I$ in bits, and round your answer to four significant figures.",
            "solution": "The problem asks for the minimal additional mutual information, $\\Delta I$, required to achieve a target single-step success ratio, $S^{\\ast}$, in a greedy routing process on a network with noisy positional information. The problem is well-posed and scientifically grounded in the principles of information theory and network science.\n\nWe begin by establishing the relationship between the probability of a decision error and the information available to make that decision. Let $X$ be a random variable representing the correct next hop. The problem states that $X$ is chosen from a set of $M$ candidates with a uniform prior probability. Therefore, the probability of any specific candidate being the correct one is $P(X=x_i) = \\frac{1}{M}$ for $i=1, 2, \\dots, M$. The entropy of $X$, which measures the inherent uncertainty about the correct hop before any information is observed, is given by:\n$$ H(X) = -\\sum_{i=1}^{M} P(X=x_i) \\log_2(P(X=x_i)) = -\\sum_{i=1}^{M} \\frac{1}{M} \\log_2\\left(\\frac{1}{M}\\right) = -M \\left(\\frac{1}{M}\\right) (-\\log_2(M)) = \\log_2(M) $$\nAll information quantities are measured in bits, hence the use of base-$2$ logarithms.\n\nThe routing decision is based on an observation, which consists of the noisy embedding $Y_0$ and potentially some side information $S$. Let the complete observation be denoted by $Y = (Y_0, S)$. An estimator, $\\hat{X} = g(Y)$, is used to select a next hop based on the observation $Y$. The single-step success ratio, $S^{\\ast}$, is the probability that this estimate is correct, i.e., $S^{\\ast} = P(\\hat{X}=X)$. The probability of error is $P_e = P(\\hat{X} \\neq X) = 1 - S^{\\ast}$.\n\nTo connect the error probability $P_e$ to the information content of the observation $Y$, we use Fano's inequality. Fano's inequality provides an upper bound on the conditional entropy $H(X|Y)$, which is the remaining uncertainty about $X$ after observing $Y$. For a decision among $M$ choices, the inequality states:\n$$ H(X|Y) \\le H(P_e) + P_e \\log_2(M-1) $$\nwhere $H(P_e)$ is the binary entropy function, defined as:\n$$ H(P_e) = -P_e \\log_2(P_e) - (1-P_e) \\log_2(1-P_e) $$\n\nThe mutual information between the correct hop $X$ and the observation $Y$ is defined as $I(X;Y) = H(X) - H(X|Y)$. It quantifies the reduction in uncertainty about $X$ gained from observing $Y$. By rearranging this definition to $H(X|Y) = H(X) - I(X;Y)$ and substituting it into Fano's inequality, we can derive a lower bound on the mutual information required to achieve a certain error probability $P_e$.\n$$ H(X) - I(X;Y) \\le H(P_e) + P_e \\log_2(M-1) $$\nRearranging for $I(X;Y)$:\n$$ I(X;Y) \\ge H(X) - H(P_e) - P_e \\log_2(M-1) $$\n\nThis inequality gives the minimum mutual information, $I_{min}$, required to guarantee that the error probability does not exceed $P_e$. Substituting $H(X) = \\log_2(M)$ and $P_e = 1 - S^{\\ast}$, we get the lower bound for the mutual information required to achieve a success ratio of at least $S^{\\ast}$:\n$$ I_{min}(S^{\\ast}) = \\log_2(M) - H(1-S^{\\ast}) - (1-S^{\\ast}) \\log_2(M-1) $$\nThe total mutual information available is given as $I(X;Y_0, S) = I_0 + \\Delta I$. To meet the target success ratio $S^{\\ast}$, this total information must be at least as large as the required minimum:\n$$ I_0 + \\Delta I \\ge I_{min}(S^{\\ast}) $$\nTherefore, the minimal additional information $\\Delta I$ needed is:\n$$ \\Delta I_{min} = I_{min}(S^{\\ast}) - I_0 = \\left[ \\log_2(M) - H(1-S^{\\ast}) - (1-S^{\\ast}) \\log_2(M-1) \\right] - I_0 $$\n\nWe can now substitute the given numerical values: $M=32$, $I_0=3.2$ bits, and $S^{\\ast}=0.9$.\nFirst, we calculate the components of the expression for $I_{min}(S^{\\ast})$.\nThe error probability is $P_e = 1 - S^{\\ast} = 1 - 0.9 = 0.1$.\nThe entropy of the source is $H(X) = \\log_2(32) = 5$ bits.\nThe term $\\log_2(M-1)$ is $\\log_2(31)$.\nThe binary entropy of the error probability is:\n$$ H(0.1) = -0.1 \\log_2(0.1) - (1-0.1) \\log_2(1-0.1) = -0.1 \\log_2(0.1) - 0.9 \\log_2(0.9) $$\nUsing the property $\\log_2(x) = \\frac{\\ln(x)}{\\ln(2)}$:\n$$ \\log_2(0.1) \\approx \\frac{-2.302585}{0.693147} \\approx -3.321928 $$\n$$ \\log_2(0.9) \\approx \\frac{-0.105361}{0.693147} \\approx -0.152003 $$\nSo, the binary entropy is:\n$$ H(0.1) \\approx -0.1(-3.321928) - 0.9(-0.152003) = 0.3321928 + 0.1368027 \\approx 0.4689955 \\text{ bits} $$\nNext, we calculate $\\log_2(31)$:\n$$ \\log_2(31) \\approx \\frac{\\ln(31)}{0.693147} \\approx \\frac{3.433987}{0.693147} \\approx 4.954196 $$\nNow, we compute the minimum required total mutual information, $I_{min}(0.9)$:\n$$ I_{min}(0.9) = 5 - H(0.1) - 0.1 \\times \\log_2(31) $$\n$$ I_{min}(0.9) \\approx 5 - 0.4689955 - 0.1 \\times 4.954196 $$\n$$ I_{min}(0.9) \\approx 5 - 0.4689955 - 0.4954196 $$\n$$ I_{min}(0.9) \\approx 4.0355849 \\text{ bits} $$\nThis is the total amount of mutual information required to achieve a $90\\%$ success rate. The problem states that the noisy embedding provides $I_0 = 3.2$ bits of information. The minimal additional information $\\Delta I$ that must be supplied by the side channel $S$ is the difference between the required information and the information already present.\n$$ \\Delta I_{min} = I_{min}(0.9) - I_0 \\approx 4.0355849 - 3.2 = 0.8355849 \\text{ bits} $$\nThe problem requires rounding the result to four significant figures.\n$$ \\Delta I_{min} \\approx 0.8356 \\text{ bits} $$\nThis result represents the minimum number of additional bits of information about the correct next hop that must be provided per routing decision, on average, to elevate the greedy choice success probability from whatever is achievable with $I_0$ to the target of $S^{\\ast}=0.9$.",
            "answer": "$$ \\boxed{0.8356} $$"
        }
    ]
}