{
    "hands_on_practices": [
        {
            "introduction": "图神经网络（GNN）的表达能力，即其区分不同图结构的能力，是一个核心的理论问题。这个能力通常以一维Weisfeiler-Lehman（1-WL）图同构测试为基准。本练习旨在通过一个具体的计算实例，揭示标准消息传递神经网络（MPNN）的内在局限性，并阐明为何它们无法区分某些非同构图。",
            "id": "4298398",
            "problem": "考虑两个节点集为 $\\{1,2,3,4,5,6\\}$ 的无向简单图：\n- 图 $G_{1}$ 是一个 6-环 $C_{6}$，其边集为 $\\{(1,2),(2,3),(3,4),(4,5),(5,6),(6,1)\\}$。\n- 图 $G_{2}$ 是两个 3-环的不相交并集 $C_{3} \\cup C_{3}$，其边集为 $\\{(1,2),(2,3),(3,1),(4,5),(5,6),(6,4)\\}$。\n\n两个图中每个节点 $i$ 的初始标量特征均为 $h_{i}^{(0)} = 1$。考虑一个图神经网络（GNN），具体来说是一个消息传递神经网络（MPNN），它使用和聚合，无边特征，并且每一层的线性更新规则如下：\n$$\nh_{i}^{(t+1)} \\;=\\; \\alpha \\, h_{i}^{(t)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(t)},\n$$\n其中 $\\alpha, \\beta \\in \\mathbb{R}$ 是固定标量，$\\mathcal{N}(i)$ 表示节点 $i$ 的邻居节点集。\n\n仅使用关于排列不变性消息传递和一维Weisfeiler–Lehman (WL) 颜色细化测试的基本定义，计算 $G_{1}$ 和 $G_{2}$ 在两层之后的节点表示，并严格证明它们是否相同。你的最终答案必须是 $h_{i}^{(2)}$ 的共同值（如果所有节点的值都相同，则该值与 $i$ 无关），并表示为关于 $\\alpha$ 和 $\\beta$ 的单个闭式解析表达式。无需四舍五入，也不涉及单位。不要报告中间值；仅提供 $h_{i}^{(2)}$ 的最终表达式。",
            "solution": "该问题是良构的，具有科学依据，并提供了推导唯一解所需的所有信息。图中、初始节点特征以及消息传递更新规则的定义是精确且数学上一致的。因此，可以推导出完整的解。\n\n问题的核心在于，将给定的消息传递神经网络（MPNN）更新规则迭代应用于两种不同的图结构 $G_1$ 和 $G_2$。在第 $t+1$ 层（或时间步），节点 $i$ 的特征向量 $h_i$ 的更新规则如下：\n$$\nh_{i}^{(t+1)} \\;=\\; \\alpha \\, h_{i}^{(t)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(t)}\n$$\n其中 $\\alpha$ 和 $\\beta$ 是标量参数，$\\mathcal{N}(i)$ 是节点 $i$ 的邻居节点集。\n\n我们已知在初始层 $t=0$ 时，两个图中所有节点的标量特征均为 $h_{i}^{(0)} = 1$。任务是计算两层之后的节点特征，即 $h_{i}^{(2)}$。\n\n首先，我们必须分析两个图 $G_1$ 和 $G_2$ 的结构。\n图 $G_1$ 是 6-环 $C_6$。在环图中，每个节点都恰好与另外两个节点相连。因此，对于 $G_1$ 中的任何节点 $i$，其度为 $d(i) = |\\mathcal{N}(i)| = 2$。\n图 $G_2$ 是两个 3-环的不相交并集 $C_3 \\cup C_3$。在 3-环中，每个节点也恰好与另外两个节点相连。因此，对于 $G_2$ 中的任何节点 $i$，其度也为 $d(i) = |\\mathcal{N}(i)| = 2$。\n一个关键的观察是，$G_1$ 和 $G_2$ 都是 2-正则图。这个结构特性——即每个节点都具有相同的度——将是决定结果的关键。\n\n让我们逐层进行计算。\n\n**第1层计算 ($t=0 \\to t=1$)：**\n\n我们使用所有节点 $i \\in \\{1, 2, 3, 4, 5, 6\\}$ 的初始特征 $h_{i}^{(0)} = 1$ 来计算 $h_{i}^{(1)}$。更新规则为：\n$$\nh_{i}^{(1)} \\;=\\; \\alpha \\, h_{i}^{(0)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(0)}\n$$\n对于 $G_1$ 或 $G_2$ 中的任何节点 $i$，我们有 $d(i)=2$。设 $i$ 的邻居为 $j_1$ 和 $j_2$。求和项变为：\n$$\n\\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(0)} \\;=\\; h_{j_1}^{(0)} + h_{j_2}^{(0)} \\;=\\; 1 + 1 \\;=\\; 2\n$$\n这个和对于两个图中的所有节点都是恒定的，因为每个节点都恰好有两个邻居，并且所有邻居节点的初始特征值都为 1。\n\n将此代入 $h_{i}^{(1)}$ 的更新规则中：\n$$\nh_{i}^{(1)} \\;=\\; \\alpha (1) \\;+\\; \\beta (2) \\;=\\; \\alpha + 2\\beta\n$$\n这个结果与节点索引 $i$ 和图（无论是 $G_1$ 还是 $G_2$）无关。经过一层消息传递后，两个图中的所有六个节点都具有相同的特征值 $h_{i}^{(1)} = \\alpha + 2\\beta$。\n\n**第2层计算 ($t=1 \\to t=2$)：**\n\n接下来，我们使用第1层的特征 $h_{i}^{(1)} = \\alpha + 2\\beta$ 来计算 $h_{i}^{(2)}$。更新规则为：\n$$\nh_{i}^{(2)} \\;=\\; \\alpha \\, h_{i}^{(1)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(1)}\n$$\n同样，对于 $G_1$ 或 $G_2$ 中的任何节点 $i$，其度为 $d(i)=2$。设其邻居为 $j_1$ 和 $j_2$。由于所有节点在上一步中获得了相同的特征值，我们有 $h_{j_1}^{(1)} = h_{j_2}^{(1)} = \\alpha + 2\\beta$。求和项为：\n$$\n\\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(1)} \\;=\\; h_{j_1}^{(1)} + h_{j_2}^{(1)} \\;=\\; (\\alpha + 2\\beta) + (\\alpha + 2\\beta) \\;=\\; 2(\\alpha + 2\\beta)\n$$\n这个和对于两个图中的所有节点再次是恒定的。\n\n现在，我们将这个和以及 $h_{i}^{(1)}$ 的值代入 $h_{i}^{(2)}$ 的更新规则中：\n$$\nh_{i}^{(2)} \\;=\\; \\alpha (\\alpha + 2\\beta) \\;+\\; \\beta [2(\\alpha + 2\\beta)]\n$$\n我们可以提出公因式 $(\\alpha + 2\\beta)$：\n$$\nh_{i}^{(2)} \\;=\\; (\\alpha + 2\\beta)(\\alpha + 2\\beta)\n$$\n这可以简化为一个最终的闭式表达式：\n$$\nh_{i}^{(2)} \\;=\\; (\\alpha + 2\\beta)^2\n$$\n或者，展开各项可得 $h_{i}^{(2)} = \\alpha^2 + 2\\alpha\\beta + 2\\alpha\\beta + 4\\beta^2 = \\alpha^2 + 4\\alpha\\beta + 4\\beta^2$，结果相同。\n\n计算表明，对于所有节点 $i$ 以及两个图 $G_1$ 和 $G_2$，节点表示 $h_{i}^{(2)}$ 都是相同的。其共同值为 $(\\alpha + 2\\beta)^2$。\n\n这个结果是 MPNN 架构表达能力的直接体现，已知其表达能力最多与一维 Weisfeiler-Lehman (1-WL) 图同构测试相当。1-WL 测试根据邻居颜色的多重集迭代更新节点的“颜色”。对于图 $G_1$（一个 6-环）和 $G_2$（两个不相交的 3-环），1-WL 测试无法区分它们。两者都是 2-正则图，因此在 WL 测试的每次迭代（以及 MPNN 的每一层）中，从聚合的角度来看，每个节点都具有相同的局部邻域结构。从相同的节点特征（颜色）开始，每一步中对每个节点的更新都是相同的，这使得 GNN 无法学习到这两个非同构图之间的任何结构差异。我们的显式计算证实了这一理论局限性。因此，节点表示是相同的。",
            "answer": "$$\n\\boxed{(\\alpha + 2\\beta)^{2}}\n$$"
        },
        {
            "introduction": "为了突破标准GNN的表达能力限制，研究者们提出了更复杂的聚合机制，其中图注意力网络（GAT）是最具代表性的模型之一。与对所有邻居一视同仁的平均或求和聚合不同，GAT通过注意力机制为邻居节点动态分配不同的重要性权重。本练习将引导你逐步计算这些注意力系数，并完成节点嵌入的更新，从而深入理解GAT层的工作原理。",
            "id": "4298405",
            "problem": "考虑一个单头图注意力网络（GAT）层，作用于一个简单的无向三角形图，节点为 $\\{1,2,3\\}$，包含自环。因此，对于节点 $i$ 的聚合，其邻域为 $N(i)=\\{1,2,3\\}$。每个节点都有一个 $\\mathbb{R}^{2}$ 中的特征向量。设初始节点特征为\n$$\n\\mathbf{x}_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\n\\mathbf{x}_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\quad\n\\mathbf{x}_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n$$\n该 GAT 层使用一个共享的线性映射 $\\mathbf{W}\\in\\mathbb{R}^{2\\times 2}$ 和一个由向量 $\\mathbf{a}\\in\\mathbb{R}^{4}$ 指定的加性注意力机制，该机制作用于变换后特征的拼接。具体来说，设\n$$\n\\mathbf{W}=\\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix},\\qquad\n\\mathbf{a}=\\begin{pmatrix}1\\\\ -1\\\\ \\tfrac{1}{2}\\\\ 0\\end{pmatrix}.\n$$\n对于目标节点 $i$ 和其邻居 $j\\in N(i)$，定义预激活分数\n$$\ns_{ij}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{W}\\mathbf{x}_{i}\\,\\|\\,\\mathbf{W}\\mathbf{x}_{j}\\bigr],\n$$\n其中 $\\|$ 表示拼接，激活函数是负斜率为 $\\lambda$ 的 Leaky Rectified Linear Unit (LeakyReLU)，即\n$$\n\\operatorname{LReLU}_{\\lambda}(z)=\\begin{cases}\nz,  z\\geq 0,\\\\\n\\lambda z,  z0,\n\\end{cases}\n$$\n其中 $\\lambda=0.2$。然后通过对邻域进行 softmax 归一化来获得注意力系数，\n$$\n\\alpha_{ij}=\\frac{\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})\\bigr)}{\\sum\\limits_{k\\in N(i)}\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ik}^{(\\mathrm{pre})})\\bigr)}.\n$$\n节点 $i$ 更新后的嵌入是置换不变的加权和（输出非线性函数为恒等函数）\n$$\n\\mathbf{h}_{i}'=\\sum_{j\\in N(i)}\\alpha_{ij}\\,\\mathbf{W}\\mathbf{x}_{j}.\n$$\n\n仅根据上述定义，对节点 $i=1$ 执行以下操作：\n1. 计算 $j\\in\\{1,2,3\\}$ 的三个注意力系数 $\\alpha_{1j}$，并通过明确展示 $\\sum_{j\\in\\{1,2,3\\}}\\alpha_{1j}=1$ 来验证它们已被归一化。\n2. 计算更新后的嵌入 $\\mathbf{h}_{1}'$。\n3. 作为最终答案，提供 $\\mathbf{h}_{1}'$ 的第一个坐标，形式为单个指数函数的封闭解析表达式。不要进行近似计算。\n\n将最终答案表示为没有数值近似的封闭形式表达式。无需四舍五入。不带单位。最终答案必须是单个解析表达式。",
            "solution": "首先验证问题，以确保其自洽、科学上合理且定义明确。\n\n**步骤 1：提取给定信息**\n- 图：包含节点 $\\{1,2,3\\}$ 的无向三角形图。\n- 带自环的邻域：对于任意节点 $i$， $N(i)=\\{1,2,3\\}$。\n- 初始节点特征：$\\mathbf{x}_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $\\mathbf{x}_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix}$, $\\mathbf{x}_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$。\n- 共享线性映射：$\\mathbf{W}=\\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}$。\n- 注意力机制向量：$\\mathbf{a}=\\begin{pmatrix}1\\\\ -1\\\\ \\frac{1}{2}\\\\ 0\\end{pmatrix}$。\n- 预激活分数：$s_{ij}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{W}\\mathbf{x}_{i}\\,\\|\\,\\mathbf{W}\\mathbf{x}_{j}\\bigr]$。\n- 激活函数：负斜率为 $\\lambda=0.2$ 的 LeakyReLU。\n- 注意力系数：$\\alpha_{ij}=\\frac{\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})\\bigr)}{\\sum\\limits_{k\\in N(i)}\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ik}^{(\\mathrm{pre})})\\bigr)}$。\n- 更新后的嵌入：$\\mathbf{h}_{i}'=\\sum_{j\\in N(i)}\\alpha_{ij}\\,\\mathbf{W}\\mathbf{x}_{j}$。\n- 任务是为目标节点 $i=1$ 进行计算。\n\n**步骤 2：使用提取的给定信息进行验证**\n- **科学依据**：问题描述了一个标准的单头图注意力网络（GAT）层，这是机器学习中一个有效且成熟的模型。其表述与开创性文献一致。\n- **定义明确**：所有必要的数据（节点特征、权重矩阵、图结构）和函数形式都已提供。问题是确定性的，并导向唯一的解。\n- **客观性**：问题使用精确的数学语言陈述，没有任何主观或模棱两可的术语。\n\n**步骤 3：结论与行动**\n问题有效。将推导出一个完整的解。\n\n总体目标是计算节点 1 的更新后嵌入 $\\mathbf{h}_{1}'$ 的第一个坐标。该过程分几个连续步骤执行。\n\n首先，我们将共享线性变换 $\\mathbf{W}$ 应用于每个节点的特征向量 $\\mathbf{x}_j$，以获得变换后的特征 $\\mathbf{h}_j = \\mathbf{W}\\mathbf{x}_j$。\n对于 $j=1$：\n$$\n\\mathbf{h}_{1} = \\mathbf{W}\\mathbf{x}_{1} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}2(1) - 1(0)\\\\ 1(1) + 1(0)\\end{pmatrix} = \\begin{pmatrix}2\\\\1\\end{pmatrix}.\n$$\n对于 $j=2$：\n$$\n\\mathbf{h}_{2} = \\mathbf{W}\\mathbf{x}_{2} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}0\\\\1\\end{pmatrix} = \\begin{pmatrix}2(0) - 1(1)\\\\ 1(0) + 1(1)\\end{pmatrix} = \\begin{pmatrix}-1\\\\1\\end{pmatrix}.\n$$\n对于 $j=3$：\n$$\n\\mathbf{h}_{3} = \\mathbf{W}\\mathbf{x}_{3} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\end{pmatrix} = \\begin{pmatrix}2(1) - 1(1)\\\\ 1(1) + 1(1)\\end{pmatrix} = \\begin{pmatrix}1\\\\2\\end{pmatrix}.\n$$\n接下来，我们计算目标节点 $i=1$ 与其邻居 $j \\in N(1) = \\{1,2,3\\}$ 的预激活注意力分数 $s_{1j}^{(\\mathrm{pre})}$。公式为 $s_{1j}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{h}_{1}\\,\\|\\,\\mathbf{h}_{j}\\bigr]$。\n对于 $j=1$：\n$$\ns_{11}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\2\\\\1\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(2) + 0(1) = 2 - 1 + 1 = 2.\n$$\n对于 $j=2$：\n$$\ns_{12}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\-1\\\\1\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(-1) + 0(1) = 2 - 1 - \\frac{1}{2} = \\frac{1}{2}.\n$$\n对于 $j=3$：\n$$\ns_{13}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\1\\\\2\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(1) + 0(2) = 2 - 1 + \\frac{1}{2} = \\frac{3}{2}.\n$$\n然后，这些分数通过负斜率为 $\\lambda=0.2$ 的 LeakyReLU 激活函数。令 $e_{ij} = \\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})$。由于所有的预激活分数（$2$, $\\frac{1}{2}$, $\\frac{3}{2}$）都是非负的，LeakyReLU 函数的作用等同于恒等函数。\n$$\ne_{11} = \\operatorname{LReLU}_{0.2}(2) = 2.\n$$\n$$\ne_{12} = \\operatorname{LReLU}_{0.2}\\left(\\frac{1}{2}\\right) = \\frac{1}{2}.\n$$\n$$\ne_{13} = \\operatorname{LReLU}_{0.2}\\left(\\frac{3}{2}\\right) = \\frac{3}{2}.\n$$\n现在我们使用 softmax 函数在邻域 $j \\in \\{1,2,3\\}$ 上计算归一化的注意力系数 $\\alpha_{1j}$。softmax 的分母是激活分数指数的总和：\n$$\nD = \\sum_{k=1}^{3} \\exp(e_{1k}) = \\exp(e_{11}) + \\exp(e_{12}) + \\exp(e_{13}) = \\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right).\n$$\n注意力系数为：\n$$\n\\alpha_{11} = \\frac{\\exp(e_{11})}{D} = \\frac{\\exp(2)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n$$\n\\alpha_{12} = \\frac{\\exp(e_{12})}{D} = \\frac{\\exp\\left(\\frac{1}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n$$\n\\alpha_{13} = \\frac{\\exp(e_{13})}{D} = \\frac{\\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n这就完成了第一个任务。为了验证归一化：\n$$\n\\sum_{j=1}^{3} \\alpha_{1j} = \\alpha_{11} + \\alpha_{12} + \\alpha_{13} = \\frac{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)} = 1.\n$$\n系数确实被归一化了。\n\n对于第二个任务，我们计算更新后的嵌入 $\\mathbf{h}_{1}'$ 作为变换后特征的加权和：\n$$\n\\mathbf{h}_{1}' = \\sum_{j=1}^{3} \\alpha_{1j}\\mathbf{h}_j = \\alpha_{11}\\mathbf{h}_1 + \\alpha_{12}\\mathbf{h}_2 + \\alpha_{13}\\mathbf{h}_3.\n$$\n代入 $\\alpha_{1j}$ 和向量 $\\mathbf{h}_j$ 的表达式：\n$$\n\\mathbf{h}_{1}' = \\frac{1}{D} \\left( \\exp(2)\\begin{pmatrix}2\\\\1\\end{pmatrix} + \\exp\\left(\\frac{1}{2}\\right)\\begin{pmatrix}-1\\\\1\\end{pmatrix} + \\exp\\left(\\frac{3}{2}\\right)\\begin{pmatrix}1\\\\2\\end{pmatrix} \\right).\n$$\n合并向量：\n$$\n\\mathbf{h}_{1}' = \\frac{1}{D} \\begin{pmatrix} 2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right) \\\\ 1\\exp(2) + 1\\exp\\left(\\frac{1}{2}\\right) + 2\\exp\\left(\\frac{3}{2}\\right) \\end{pmatrix}.\n$$\n代入 $D$ 的表达式：\n$$\n\\mathbf{h}_{1}' = \\frac{1}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)} \\begin{pmatrix} 2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right) \\\\ \\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + 2\\exp\\left(\\frac{3}{2}\\right) \\end{pmatrix}.\n$$\n这就完成了第二个任务。\n\n对于第三个任务和最终答案，我们提取 $\\mathbf{h}_{1}'$ 的第一个坐标。记作 $(\\mathbf{h}_{1}')_1$。\n$$\n(\\mathbf{h}_{1}')_1 = \\frac{2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n这就是所要求的最终封闭形式解析表达式。",
            "answer": "$$\n\\boxed{\\frac{2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}}\n$$"
        },
        {
            "introduction": "现实世界中的网络，如知识图谱或社交网络，通常包含多种类型的关系，这超出了简单图的范畴。本练习将GNN的原理扩展到这种异构的、带有多关系类型的图中，这在处理复杂系统时至关重要。你将从基本原则出发，推导出一个关系图神经网络（R-GNN）的更新规则，并在一个具体实例上应用它，从而掌握处理多关系数据的核心方法。",
            "id": "4298431",
            "problem": "考虑在图神经网络 (GNN) 意义下的一个具有类型化关系的有向知识图谱上的单层消息传递更新。目标是确定一个唯一的、线性的、置换不变的、按关系相加的更新规则，该规则遵循按每个关系的入邻居数量进行归一化，然后在一个具体的图实例上对其进行数值评估。请在以下基本原则下进行操作：\n\n- 该图是一个用于建模知识图谱的有向、类型化多重图：边是 $(j,r,i)$ 形式的三元组，表示一条从节点 $j$ 到节点 $i$ 的、关系类型为 $r$ 的有向边。\n- 在节点 $i$ 处的消息传递更新必须对每个关系类型 $r$ 下的邻居是置换不变的。\n- 对于任何固定的关系 $r$，关系内的消息映射对发送者的特征 $h_{j}$ 是线性的，并且不同关系类型的贡献是可加的。\n- 为确保对于固定关系下入邻居数量的尺度不变性，每个关系的聚合消息需要通过节点 $i$ 在该关系下的入邻居数量进行归一化。\n- 该层中没有自环或偏置项，也不应用非线性激活函数。\n\n给定以下具体的图和参数。节点集为 $V=\\{1,2,3\\}$，关系集为 $R=\\{a,b,c\\}$，有向、类型化边集为\n$$\nE=\\{(2,a,1),(3,a,1),(1,b,2),(1,c,3),(2,c,3)\\}.\n$$\n每个节点有一个二维特征向量。初始特征为\n$$\nh_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad h_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\quad h_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n$$\n对于每个关系 $r\\in R$，特定于关系的线性映射 $W_{r}\\in\\mathbb{R}^{2\\times 2}$ 指定为\n$$\nW_{a}=\\begin{pmatrix}1  2\\\\0  1\\end{pmatrix},\\quad\nW_{b}=\\begin{pmatrix}2  0\\\\1  1\\end{pmatrix},\\quad\nW_{c}=\\begin{pmatrix}0  1\\\\1  0\\end{pmatrix}.\n$$\n对于每个节点 $i$ 和关系 $r$，定义 $N_{i}^{r}=\\{\\,j\\in V:\\,(j,r,i)\\in E\\,\\}$，令 $d_{i,r}^{\\mathrm{in}}=|N_{i}^{r}|$ 为关系 $r$ 下的入邻居计数，并设置归一化系数 $c_{i,r}=1/d_{i,r}^{\\mathrm{in}}$（如果 $d_{i,r}^{\\mathrm{in}}0$），否则关系 $r$ 对节点 $i$ 的贡献为零。对于 $d_{i,r}^{\\mathrm{in}}=0$ 的关系，其贡献为零。\n\n任务：\n- 仅使用上述原则，推导每个节点 $i$ 的节点更新 $h_{i}'$ 的唯一形式，用 $\\{W_{r}\\}$、$\\{c_{i,r}\\}$ 和 $\\{h_{j}\\}$ 表示。\n- 应用你推导的更新规则，在给定的图和参数上计算所有 $i\\in\\{1,2,3\\}$ 的 $h_{i}'$。\n- 最后，计算标量\n$$\nS=\\sum_{i\\in V}\\mathbf{1}^{\\top}h_{i}',\n$$\n其中 $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$。将 $S$ 表示为一个精确的有理数。不要进行四舍五入。",
            "solution": "首先对问题进行验证，以确保其具有科学依据、是良定的且客观的。\n\n**步骤 1：提取已知条件**\n- **图模型：** 一个有向、类型化多重图，其边具有 $(j,r,i)$ 的形式，其中 $j$ 是源节点， $i$ 是目标节点， $r$ 是关系类型。\n- **更新规则原则：**\n    1.  在节点 $i$ 处的更新对于每种关系类型 $r$ 的邻居是置换不变的。\n    2.  对于给定的关系 $r$，消息映射对发送者的特征向量 $h_j$ 是线性的。\n    3.  来自不同关系类型的贡献是可加的。\n    4.  每种关系 $r$ 的聚合消息通过入度 $d_{i,r}^{\\mathrm{in}} = |N_{i}^{r}|$ 进行归一化，其中 $N_{i}^{r}=\\{\\,j\\in V:\\,(j,r,i)\\in E\\,\\}$。对于 $d_{i,r}^{\\mathrm{in}}0$，归一化系数为 $c_{i,r}=1/d_{i,r}^{\\mathrm{in}}$。\n    5.  如果 $d_{i,r}^{\\mathrm{in}}=0$，关系 $r$ 对节点 $i$ 的贡献为零。\n    6.  不使用自环、偏置项或非线性激活函数。\n- **图实例：**\n    - 节点集：$V=\\{1,2,3\\}$。\n    - 关系集：$R=\\{a,b,c\\}$。\n    - 边集：$E=\\{(2,a,1),(3,a,1),(1,b,2),(1,c,3),(2,c,3)\\}$。\n- **初始特征向量 ($h_i \\in \\mathbb{R}^2$):**\n    - $h_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$\n    - $h_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix}$\n    - $h_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$\n- **特定于关系的线性映射 ($W_r \\in \\mathbb{R}^{2\\times 2}$):**\n    - $W_{a}=\\begin{pmatrix}1  2\\\\0  1\\end{pmatrix}$\n    - $W_{b}=\\begin{pmatrix}2  0\\\\1  1\\end{pmatrix}$\n    - $W_{c}=\\begin{pmatrix}0  1\\\\1  0\\end{pmatrix}$\n- **最终计算：** 计算 $S=\\sum_{i\\in V}\\mathbf{1}^{\\top}h_{i}'$，其中 $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题陈述是关系图神经网络 (GNN) 中消息传递层的标准表述，具体来说是关系图卷积网络 (R-GCN) 的一个变体。\n- 它**有科学依据**：线性、置换不变性和聚合的原则是 GNN 的基础。\n- 它**是良定的**：给定的原则唯一地确定了更新规则，并且提供了计算唯一数值结果所需的所有数据（图结构、特征、权重矩阵）。\n- 它**是客观的**：问题以精确的数学语言陈述，没有歧义或主观声明。\n所有必需的条件都已满足。该问题没有任何科学缺陷、矛盾或信息缺失。\n\n**步骤 3：结论与行动**\n该问题是**有效的**。将提供完整的解决方案。\n\n**唯一更新规则的推导**\n\n令 $h_i'$ 为节点 $i \\in V$ 的更新后特征向量。推导过程通过应用给定的原则进行：\n1.  **按关系相加：** 总更新是来自每种关系类型 $r \\in R$ 贡献的总和。\n    $$h_i' = \\sum_{r \\in R} m_{r \\to i}$$\n    其中 $m_{r \\to i}$ 是关系 $r$ 到达节点 $i$ 的聚合消息。\n\n2.  **线性与置换不变性：** 对于一个固定的关系 $r$，来自邻居 $j \\in N_i^r$ 的消息必须是其特征向量 $h_j$ 的线性函数。最一般的形式是 $W_r h_j$，其中 $W_r$ 是某个矩阵。这些消息的聚合必须对 $N_i^r$ 中节点的排序是置换不变的。求和操作满足此属性。因此，归一化前的聚合消息是 $\\sum_{j \\in N_i^r} W_r h_j$。\n\n3.  **归一化：** 这个聚合消息由入度 $d_{i,r}^{\\mathrm{in}}$ 进行归一化。归一化常数为 $c_{i,r} = 1/d_{i,r}^{\\mathrm{in}}$。因此，消息为：\n    $$m_{r \\to i} = \\frac{1}{d_{i,r}^{\\mathrm{in}}} \\sum_{j \\in N_i^r} W_r h_j$$\n    原则还规定，如果 $d_{i,r}^{\\mathrm{in}}=0$，则贡献为零。这通过对空集 $N_i^r$ 进行求和自然处理，其结果为零向量。\n\n结合这些原则，得到了在指定约束下的唯一更新规则。由于权重矩阵 $W_r$ 和归一化常数 $c_{i,r}$ 不依赖于求和索引 $j$，它们可以被提取出来：\n$$ h_i' = \\sum_{r \\in R, d_{i,r}^{\\mathrm{in}}  0} \\frac{1}{d_{i,r}^{\\mathrm{in}}} W_r \\left( \\sum_{j \\in N_i^r} h_j \\right) $$\n这是更新规则的一般形式。\n\n**在给定图上的应用**\n\n首先，我们根据边集 $E=\\{(2,a,1),(3,a,1),(1,b,2),(1,c,3),(2,c,3)\\}$ 确定非空邻居集 $N_i^r$ 和相应的入度 $d_{i,r}^{\\mathrm{in}}$。\n\n-   **对于节点 $i=1$**：\n    -   关系为 $a$ 的入边：$(2,a,1), (3,a,1)$。 因此，$N_1^a = \\{2,3\\}$ 且 $d_{1,a}^{\\mathrm{in}}=2$。\n    -   没有关系为 $b$ 或 $c$ 的入边。\n    -   $h_1$ 的更新为：\n        $$ h_1' = \\frac{1}{d_{1,a}^{\\mathrm{in}}} W_a \\left( \\sum_{j \\in N_1^a} h_j \\right) = \\frac{1}{2} W_a (h_2 + h_3) $$\n        $$ h_2 + h_3 = \\begin{pmatrix}0\\\\1\\end{pmatrix} + \\begin{pmatrix}1\\\\1\\end{pmatrix} = \\begin{pmatrix}1\\\\2\\end{pmatrix} $$\n        $$ h_1' = \\frac{1}{2} \\begin{pmatrix}1  2\\\\0  1\\end{pmatrix} \\begin{pmatrix}1\\\\2\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}1(1)+2(2)\\\\0(1)+1(2)\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}5\\\\2\\end{pmatrix} = \\begin{pmatrix}5/2\\\\1\\end{pmatrix} $$\n\n-   **对于节点 $i=2$**：\n    -   关系为 $b$ 的入边：$(1,b,2)$。 因此，$N_2^b = \\{1\\}$ 且 $d_{2,b}^{\\mathrm{in}}=1$。\n    -   没有关系为 $a$ 或 $c$ 的入边。\n    -   $h_2$ 的更新为：\n        $$ h_2' = \\frac{1}{d_{2,b}^{\\mathrm{in}}} W_b \\left( \\sum_{j \\in N_2^b} h_j \\right) = \\frac{1}{1} W_b (h_1) $$\n        $$ h_2' = \\begin{pmatrix}2  0\\\\1  1\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}2(1)+0(0)\\\\1(1)+1(0)\\end{pmatrix} = \\begin{pmatrix}2\\\\1\\end{pmatrix} $$\n\n-   **对于节点 $i=3$**：\n    -   关系为 $c$ 的入边：$(1,c,3), (2,c,3)$。 因此，$N_3^c = \\{1,2\\}$ 且 $d_{3,c}^{\\mathrm{in}}=2$。\n    -   没有关系为 $a$ 或 $b$ 的入边。\n    -   $h_3$ 的更新为：\n        $$ h_3' = \\frac{1}{d_{3,c}^{\\mathrm{in}}} W_c \\left( \\sum_{j \\in N_3^c} h_j \\right) = \\frac{1}{2} W_c (h_1 + h_2) $$\n        $$ h_1 + h_2 = \\begin{pmatrix}1\\\\0\\end{pmatrix} + \\begin{pmatrix}0\\\\1\\end{pmatrix} = \\begin{pmatrix}1\\\\1\\end{pmatrix} $$\n        $$ h_3' = \\frac{1}{2} \\begin{pmatrix}0  1\\\\1  0\\end{pmatrix} \\begin{pmatrix}1\\\\1\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}0(1)+1(1)\\\\1(1)+0(1)\\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix}1\\\\1\\end{pmatrix} = \\begin{pmatrix}1/2\\\\1/2\\end{pmatrix} $$\n\n**标量 S 的计算**\n\n最后的任务是计算 $S = \\sum_{i \\in V} \\mathbf{1}^{\\top} h_i'$，其中 $\\mathbf{1} = \\begin{pmatrix}1\\\\1\\end{pmatrix}$。这是每个更新后特征向量各分量之和。\n\n-   对于 $h_1' = \\begin{pmatrix}5/2\\\\1\\end{pmatrix}$：\n    $$ \\mathbf{1}^{\\top} h_1' = \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}5/2\\\\1\\end{pmatrix} = \\frac{5}{2} + 1 = \\frac{7}{2} $$\n-   对于 $h_2' = \\begin{pmatrix}2\\\\1\\end{pmatrix}$：\n    $$ \\mathbf{1}^{\\top} h_2' = \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}2\\\\1\\end{pmatrix} = 2 + 1 = 3 $$\n-   对于 $h_3' = \\begin{pmatrix}1/2\\\\1/2\\end{pmatrix}$：\n    $$ \\mathbf{1}^{\\top} h_3' = \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}1/2\\\\1/2\\end{pmatrix} = \\frac{1}{2} + \\frac{1}{2} = 1 $$\n\n将这些值相加得到最终的标量 $S$：\n$$ S = \\frac{7}{2} + 3 + 1 = \\frac{7}{2} + 4 = \\frac{7}{2} + \\frac{8}{2} = \\frac{15}{2} $$\n该值作为一个精确的有理数是 $15/2$。",
            "answer": "$$\\boxed{\\frac{15}{2}}$$"
        }
    ]
}