{
    "hands_on_practices": [
        {
            "introduction": "The power of Graph Neural Networks comes from their ability to learn representations that encode graph structure. However, the expressive power of standard Message Passing Neural Networks (MPNNs) is fundamentally limited, being at most as powerful as the 1-dimensional Weisfeiler-Lehman (1-WL) graph isomorphism test. This practice provides a hands-on demonstration of this theoretical boundary, showing how a simple MPNN computes identical embeddings for two non-isomorphic graphs that the 1-WL test cannot distinguish .",
            "id": "4298398",
            "problem": "Consider two undirected, simple graphs with node set $\\{1,2,3,4,5,6\\}$:\n- Graph $G_{1}$ is the $6$-cycle $C_{6}$ with edge set $\\{(1,2),(2,3),(3,4),(4,5),(5,6),(6,1)\\}$.\n- Graph $G_{2}$ is the disjoint union of two $3$-cycles $C_{3} \\cup C_{3}$ with edge set $\\{(1,2),(2,3),(3,1),(4,5),(5,6),(6,4)\\}$.\n\nEach node $i$ in both graphs is initialized with the same scalar feature $h_{i}^{(0)} = 1$. Consider a Graph Neural Network (GNN), specifically a Message Passing Neural Network (MPNN), that uses sum aggregation, no edge features, and a linear update rule at each layer given by\n$$\nh_{i}^{(t+1)} \\;=\\; \\alpha \\, h_{i}^{(t)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(t)},\n$$\nwhere $\\alpha, \\beta \\in \\mathbb{R}$ are fixed scalars, and $\\mathcal{N}(i)$ denotes the set of neighbors of node $i$.\n\nUsing only fundamental definitions about permutation-invariant message passing and the one-dimensional Weisfeiler–Lehman (WL) color refinement test, compute the node representations after two layers for both $G_{1}$ and $G_{2}$, and rigorously justify whether they are identical. Your final answer must be the common value of $h_{i}^{(2)}$ (which is independent of $i$ if it is the same across nodes), expressed as a single closed-form analytical expression in terms of $\\alpha$ and $\\beta$. No rounding is required, and no units are involved. Do not report intermediate values; provide only the final expression for $h_{i}^{(2)}$.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to derive a unique solution. The definitions of the graphs, initial node features, and the message-passing update rule are precise and mathematically consistent. Therefore, a full solution can be derived.\n\nThe core of the problem lies in the iterative application of the given update rule for a Message Passing Neural Network (MPNN) on two different graph structures, $G_1$ and $G_2$. The update rule for the feature vector $h_i$ of a node $i$ at layer (or time step) $t+1$ is given by:\n$$\nh_{i}^{(t+1)} \\;=\\; \\alpha \\, h_{i}^{(t)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(t)}\n$$\nwhere $\\alpha$ and $\\beta$ are scalar parameters, and $\\mathcal{N}(i)$ is the set of neighbors of node $i$.\n\nWe are given that initially, at layer $t=0$, all nodes in both graphs have the same scalar feature $h_{i}^{(0)} = 1$. The task is to compute the node features after two layers, i.e., $h_{i}^{(2)}$.\n\nFirst, we must analyze the structure of the two graphs, $G_1$ and $G_2$.\nGraph $G_1$ is the $6$-cycle, $C_6$. In a cycle graph, every node is connected to exactly two other nodes. Therefore, for any node $i$ in $G_1$, its degree is $d(i) = |\\mathcal{N}(i)| = 2$.\nGraph $G_2$ is the disjoint union of two $3$-cycles, $C_3 \\cup C_3$. In a $3$-cycle, every node is also connected to exactly two other nodes. Therefore, for any node $i$ in $G_2$, its degree is also $d(i) = |\\mathcal{N}(i)| = 2$.\nA critical observation is that both $G_1$ and $G_2$ are $2$-regular graphs. This structural property—that every node has the same degree—will be central to the outcome.\n\nLet us proceed with the calculation layer by layer.\n\n**Layer 1 Calculation ($t=0 \\to t=1$):**\n\nWe compute $h_{i}^{(1)}$ using the initial features $h_{i}^{(0)} = 1$ for all $i \\in \\{1, 2, 3, 4, 5, 6\\}$. The update rule is:\n$$\nh_{i}^{(1)} \\;=\\; \\alpha \\, h_{i}^{(0)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(0)}\n$$\nFor any node $i$ in either $G_1$ or $G_2$, we have $d(i)=2$. Let the neighbors of $i$ be $j_1$ and $j_2$. The summation term becomes:\n$$\n\\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(0)} \\;=\\; h_{j_1}^{(0)} + h_{j_2}^{(0)} \\;=\\; 1 + 1 \\;=\\; 2\n$$\nThis sum is constant for all nodes in both graphs because every node has exactly two neighbors, and all neighboring nodes have the initial feature value of $1$.\n\nSubstituting this into the update rule for $h_{i}^{(1)}$:\n$$\nh_{i}^{(1)} \\;=\\; \\alpha (1) \\;+\\; \\beta (2) \\;=\\; \\alpha + 2\\beta\n$$\nThis result is independent of the node index $i$ and the graph (either $G_1$ or $G_2$). After one layer of message passing, all six nodes in both graphs have the identical feature value, $h_{i}^{(1)} = \\alpha + 2\\beta$.\n\n**Layer 2 Calculation ($t=1 \\to t=2$):**\n\nNext, we compute $h_{i}^{(2)}$ using the features from layer 1, $h_{i}^{(1)} = \\alpha + 2\\beta$. The update rule is:\n$$\nh_{i}^{(2)} \\;=\\; \\alpha \\, h_{i}^{(1)} \\;+\\; \\beta \\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(1)}\n$$\nAgain, for any node $i$ in either $G_1$ or $G_2$, the degree is $d(i)=2$. Let its neighbors be $j_1$ and $j_2$. Since all nodes acquired the same feature value at the previous step, we have $h_{j_1}^{(1)} = h_{j_2}^{(1)} = \\alpha + 2\\beta$. The summation term is:\n$$\n\\sum_{j \\in \\mathcal{N}(i)} h_{j}^{(1)} \\;=\\; h_{j_1}^{(1)} + h_{j_2}^{(1)} \\;=\\; (\\alpha + 2\\beta) + (\\alpha + 2\\beta) \\;=\\; 2(\\alpha + 2\\beta)\n$$\nThis sum is again constant for all nodes in both graphs.\n\nNow, we substitute this sum and the value of $h_{i}^{(1)}$ into the update rule for $h_{i}^{(2)}$:\n$$\nh_{i}^{(2)} \\;=\\; \\alpha (\\alpha + 2\\beta) \\;+\\; \\beta [2(\\alpha + 2\\beta)]\n$$\nWe can factor out the common term $(\\alpha + 2\\beta)$:\n$$\nh_{i}^{(2)} \\;=\\; (\\alpha + 2\\beta)(\\alpha + 2\\beta)\n$$\nThis simplifies to a final closed-form expression:\n$$\nh_{i}^{(2)} \\;=\\; (\\alpha + 2\\beta)^2\n$$\nAlternatively, expanding the terms gives $h_{i}^{(2)} = \\alpha^2 + 2\\alpha\\beta + 2\\alpha\\beta + 4\\beta^2 = \\alpha^2 + 4\\alpha\\beta + 4\\beta^2$, which is the same result.\n\nThe calculation demonstrates that the node representations $h_{i}^{(2)}$ are identical for all nodes $i$ and for both graphs $G_1$ and $G_2$. The common value is $(\\alpha + 2\\beta)^2$.\n\nThis result is a direct consequence of the expressive power of the MPNN architecture, which is known to be at most as powerful as the one-dimensional Weisfeiler–Lehman (1-WL) graph isomorphism test. The 1-WL test iteratively updates node \"colors\" based on the multiset of neighbor colors. For the graphs $G_1$ (a $6$-cycle) and $G_2$ (two disjoint $3$-cycles), the 1-WL test fails to distinguish them. Both are $2$-regular graphs, so at each iteration of the WL test (and each layer of the MPNN), every node has an identical local neighborhood structure from the perspective of aggregation. Starting with identical node features (colors), the update for every node is identical at every step, preventing the GNN from learning any structural differences between the two non-isomorphic graphs. Our explicit calculation confirms this theoretical limitation. The node representations are therefore identical.",
            "answer": "$$\n\\boxed{(\\alpha + 2\\beta)^{2}}\n$$"
        },
        {
            "introduction": "Simple aggregators like sum or mean treat all neighbors equally, which limits the model's expressivity as seen previously. Graph Attention Networks (GATs) overcome this by introducing an attention mechanism, allowing a node to assign different importance to its neighbors' features. This exercise demystifies the GAT architecture by guiding you through the step-by-step computation of attention coefficients and the resulting updated node embedding for a single GAT layer .",
            "id": "4298405",
            "problem": "Consider a single-head Graph Attention Network (GAT) layer on a simple undirected triangle graph with nodes $\\{1,2,3\\}$ and self-loops included, so that for aggregation at a node $i$, the neighborhood is $N(i)=\\{1,2,3\\}$. Each node has a feature vector in $\\mathbb{R}^{2}$. Let the initial node features be\n$$\n\\mathbf{x}_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\n\\mathbf{x}_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\quad\n\\mathbf{x}_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n$$\nThe GAT layer uses a shared linear map $\\mathbf{W}\\in\\mathbb{R}^{2\\times 2}$ and an additive attention mechanism specified by a vector $\\mathbf{a}\\in\\mathbb{R}^{4}$ acting on the concatenation of transformed features. Specifically, let\n$$\n\\mathbf{W}=\\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix},\\qquad\n\\mathbf{a}=\\begin{pmatrix}1\\\\ -1\\\\ \\tfrac{1}{2}\\\\ 0\\end{pmatrix}.\n$$\nFor a target node $i$ and a neighbor $j\\in N(i)$, define the pre-activation score\n$$\ns_{ij}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{W}\\mathbf{x}_{i}\\,\\|\\,\\mathbf{W}\\mathbf{x}_{j}\\bigr],\n$$\nwhere $\\|$ denotes concatenation, and the activation is the Leaky Rectified Linear Unit (LeakyReLU) with negative slope $\\lambda$, that is,\n$$\n\\operatorname{LReLU}_{\\lambda}(z)=\\begin{cases}\nz,  z\\geq 0,\\\\\n\\lambda z,  z0,\n\\end{cases}\n$$\nwith $\\lambda=0.2$. The attention coefficients are then obtained by softmax normalization over the neighborhood,\n$$\n\\alpha_{ij}=\\frac{\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})\\bigr)}{\\sum\\limits_{k\\in N(i)}\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ik}^{(\\mathrm{pre})})\\bigr)}.\n$$\nThe updated embedding at node $i$ is the permutation-invariant weighted sum (with identity output nonlinearity) \n$$\n\\mathbf{h}_{i}'=\\sum_{j\\in N(i)}\\alpha_{ij}\\,\\mathbf{W}\\mathbf{x}_{j}.\n$$\n\nStarting only from the definitions above, do the following for node $i=1$:\n1. Compute the three attention coefficients $\\alpha_{1j}$ for $j\\in\\{1,2,3\\}$ and verify that they are normalized by explicitly showing that $\\sum_{j\\in\\{1,2,3\\}}\\alpha_{1j}=1$.\n2. Compute the updated embedding $\\mathbf{h}_{1}'$.\n3. Provide, as your final answer, the first coordinate of $\\mathbf{h}_{1}'$ as a single closed-form analytic expression in terms of exponentials. Do not approximate.\n\nExpress the final answer as a closed-form expression without numerical approximation. No rounding is required. State no units. The final answer must be a single analytic expression.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically sound, and well-posed.\n\n**Step 1: Extract Givens**\n- Graph: Undirected triangle with nodes $\\{1,2,3\\}$.\n- Neighborhood with self-loops: $N(i)=\\{1,2,3\\}$ for any node $i$.\n- Initial node features: $\\mathbf{x}_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $\\mathbf{x}_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix}$, $\\mathbf{x}_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$.\n- Shared linear map: $\\mathbf{W}=\\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}$.\n- Attention mechanism vector: $\\mathbf{a}=\\begin{pmatrix}1\\\\ -1\\\\ \\frac{1}{2}\\\\ 0\\end{pmatrix}$.\n- Pre-activation score: $s_{ij}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{W}\\mathbf{x}_{i}\\,\\|\\,\\mathbf{W}\\mathbf{x}_{j}\\bigr]$.\n- Activation function: LeakyReLU with negative slope $\\lambda=0.2$.\n- Attention coefficients: $\\alpha_{ij}=\\frac{\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})\\bigr)}{\\sum\\limits_{k\\in N(i)}\\exp\\bigl(\\operatorname{LReLU}_{\\lambda}(s_{ik}^{(\\mathrm{pre})})\\bigr)}$.\n- Updated embedding: $\\mathbf{h}_{i}'=\\sum_{j\\in N(i)}\\alpha_{ij}\\,\\mathbf{W}\\mathbf{x}_{j}$.\n- The task is to perform calculations for the target node $i=1$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem describes a standard single-head Graph Attention Network (GAT) layer, a valid and well-established model in machine learning. Its formulation is consistent with the seminal literature.\n- **Well-Posed**: All necessary data (node features, weight matrices, graph structure) and functional forms are provided. The problem is deterministic and leads to a unique solution.\n- **Objective**: The problem is stated using precise mathematical language, free from any subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be derived.\n\nThe overall goal is to compute the first coordinate of the updated embedding for node $1$, denoted as $\\mathbf{h}_{1}'$. The process is executed in several sequential steps.\n\nFirst, we apply the shared linear transformation $\\mathbf{W}$ to each node's feature vector $\\mathbf{x}_j$ to obtain the transformed features $\\mathbf{h}_j = \\mathbf{W}\\mathbf{x}_j$.\nFor $j=1$:\n$$\n\\mathbf{h}_{1} = \\mathbf{W}\\mathbf{x}_{1} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}2(1) - 1(0)\\\\ 1(1) + 1(0)\\end{pmatrix} = \\begin{pmatrix}2\\\\1\\end{pmatrix}.\n$$\nFor $j=2$:\n$$\n\\mathbf{h}_{2} = \\mathbf{W}\\mathbf{x}_{2} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}0\\\\1\\end{pmatrix} = \\begin{pmatrix}2(0) - 1(1)\\\\ 1(0) + 1(1)\\end{pmatrix} = \\begin{pmatrix}-1\\\\1\\end{pmatrix}.\n$$\nFor $j=3$:\n$$\n\\mathbf{h}_{3} = \\mathbf{W}\\mathbf{x}_{3} = \\begin{pmatrix}2  -1\\\\ 1  1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\end{pmatrix} = \\begin{pmatrix}2(1) - 1(1)\\\\ 1(1) + 1(1)\\end{pmatrix} = \\begin{pmatrix}1\\\\2\\end{pmatrix}.\n$$\nNext, we compute the pre-activation attention scores $s_{1j}^{(\\mathrm{pre})}$ for the target node $i=1$ with its neighbors $j \\in N(1) = \\{1,2,3\\}$. The formula is $s_{1j}^{(\\mathrm{pre})}=\\mathbf{a}^{\\top}\\bigl[\\mathbf{h}_{1}\\,\\|\\,\\mathbf{h}_{j}\\bigr]$.\nFor $j=1$:\n$$\ns_{11}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\2\\\\1\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(2) + 0(1) = 2 - 1 + 1 = 2.\n$$\nFor $j=2$:\n$$\ns_{12}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\-1\\\\1\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(-1) + 0(1) = 2 - 1 - \\frac{1}{2} = \\frac{1}{2}.\n$$\nFor $j=3$:\n$$\ns_{13}^{(\\mathrm{pre})} = \\begin{pmatrix}1  -1  \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\1\\\\2\\end{pmatrix} = 1(2) - 1(1) + \\frac{1}{2}(1) + 0(2) = 2 - 1 + \\frac{1}{2} = \\frac{3}{2}.\n$$\nThese scores are then passed through the LeakyReLU activation function with $\\lambda=0.2$. Let $e_{ij} = \\operatorname{LReLU}_{\\lambda}(s_{ij}^{(\\mathrm{pre})})$. Since all pre-activation scores ($2$, $\\frac{1}{2}$, $\\frac{3}{2}$) are non-negative, the LeakyReLU function acts as an identity function.\n$$\ne_{11} = \\operatorname{LReLU}_{0.2}(2) = 2.\n$$\n$$\ne_{12} = \\operatorname{LReLU}_{0.2}\\left(\\frac{1}{2}\\right) = \\frac{1}{2}.\n$$\n$$\ne_{13} = \\operatorname{LReLU}_{0.2}\\left(\\frac{3}{2}\\right) = \\frac{3}{2}.\n$$\nNow we compute the normalized attention coefficients $\\alpha_{1j}$ using the softmax function over the neighborhood $j \\in \\{1,2,3\\}$. The denominator for the softmax is the sum of the exponentials of the activated scores:\n$$\nD = \\sum_{k=1}^{3} \\exp(e_{1k}) = \\exp(e_{11}) + \\exp(e_{12}) + \\exp(e_{13}) = \\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right).\n$$\nThe attention coefficients are:\n$$\n\\alpha_{11} = \\frac{\\exp(e_{11})}{D} = \\frac{\\exp(2)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n$$\n\\alpha_{12} = \\frac{\\exp(e_{12})}{D} = \\frac{\\exp\\left(\\frac{1}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\n$$\n\\alpha_{13} = \\frac{\\exp(e_{13})}{D} = \\frac{\\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\nThis completes the first task. To verify normalization:\n$$\n\\sum_{j=1}^{3} \\alpha_{1j} = \\alpha_{11} + \\alpha_{12} + \\alpha_{13} = \\frac{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)} = 1.\n$$\nThe coefficients are indeed normalized.\n\nFor the second task, we compute the updated embedding $\\mathbf{h}_{1}'$ as the weighted sum of the transformed features:\n$$\n\\mathbf{h}_{1}' = \\sum_{j=1}^{3} \\alpha_{1j}\\mathbf{h}_j = \\alpha_{11}\\mathbf{h}_1 + \\alpha_{12}\\mathbf{h}_2 + \\alpha_{13}\\mathbf{h}_3.\n$$\nSubstituting the expressions for $\\alpha_{1j}$ and vectors $\\mathbf{h}_j$:\n$$\n\\mathbf{h}_{1}' = \\frac{1}{D} \\left( \\exp(2)\\begin{pmatrix}2\\\\1\\end{pmatrix} + \\exp\\left(\\frac{1}{2}\\right)\\begin{pmatrix}-1\\\\1\\end{pmatrix} + \\exp\\left(\\frac{3}{2}\\right)\\begin{pmatrix}1\\\\2\\end{pmatrix} \\right).\n$$\nCombining the vectors:\n$$\n\\mathbf{h}_{1}' = \\frac{1}{D} \\begin{pmatrix} 2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right) \\\\ 1\\exp(2) + 1\\exp\\left(\\frac{1}{2}\\right) + 2\\exp\\left(\\frac{3}{2}\\right) \\end{pmatrix}.\n$$\nSubstituting the expression for $D$:\n$$\n\\mathbf{h}_{1}' = \\frac{1}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)} \\begin{pmatrix} 2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right) \\\\ \\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + 2\\exp\\left(\\frac{3}{2}\\right) \\end{pmatrix}.\n$$\nThis completes the second task.\n\nFor the third task and the final answer, we extract the first coordinate of $\\mathbf{h}_{1}'$. Let this be denoted by $(\\mathbf{h}_{1}')_1$.\n$$\n(\\mathbf{h}_{1}')_1 = \\frac{2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}.\n$$\nThis is the final closed-form analytic expression required.",
            "answer": "$$\n\\boxed{\\frac{2\\exp(2) - \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}{\\exp(2) + \\exp\\left(\\frac{1}{2}\\right) + \\exp\\left(\\frac{3}{2}\\right)}}\n$$"
        },
        {
            "introduction": "Standard GNNs are built on the principle of homophily, assuming connected nodes are similar. In heterophilous graphs, where adjacent nodes are often dissimilar, this assumption fails. This practice challenges you to design a GNN that moves beyond adjacency-based aggregation, instead grouping nodes by structural identity—a concept rooted in network motifs and roles—to learn effective representations in non-homophilous settings .",
            "id": "4298462",
            "problem": "Consider a simple, undirected graph neural network (GNN) and a small heterophilous graph. The heterophilous property means that adjacent nodes tend to have dissimilar attributes; to exploit structural identity instead of feature similarity, the architecture must aggregate information across nodes that share the same structural role rather than across adjacent nodes.\n\nStart from the following foundational definitions and constraints, and design a single-layer structural-identity GNN, then compute the scalar updated feature at a specific node.\n\nDefinitions and graph specification:\n- Let the graph be $G=(V,E)$ with $V=\\{1,2,3,4\\}$ and edges $E=\\{(1,2),(2,3),(3,4),(4,1),(2,4)\\}$.\n- For each node $v \\in V$, define the role vector $r(v)$ by $r(v) = [d(v), t(v)]$, where $d(v)$ is the degree of $v$ and $t(v)$ is the number of triangle motifs that include $v$. A triangle motif is a set of three nodes $\\{i,j,k\\}$ that are mutually adjacent, that is, $\\{(i,j),(j,k),(k,i)\\} \\subseteq E$.\n- Define the structural equivalence class $C(v) = \\{u \\in V \\mid r(u) = r(v)\\}$.\n- Let the initial scalar node features be $h^{(0)}(1) = -2$, $h^{(0)}(2) = 5$, $h^{(0)}(3) = 4$, and $h^{(0)}(4) = -3$.\n\nArchitectural design requirements:\n- The one-layer update $h^{(1)}(v)$ must aggregate information only from the structural equivalence class $C(v)$, not from adjacency $E$ nor from feature similarity.\n- The update must be permutation-invariant with respect to relabeling nodes within $C(v)$.\n- The update must be linear in the inputs $\\{h^{(0)}(u) : u \\in C(v)\\}$.\n- The update must be scale-stable under replication of structurally identical nodes: if $C(v)$ is enlarged by duplicating nodes with identical role vector and identical feature values, the magnitude of $h^{(1)}(v)$ should not blow up and should remain within the convex hull of the inputs.\n\nTask:\n1. Using only the above definitions and requirements, derive the unique linear, permutation-invariant, scale-stable aggregator over $C(v)$.\n2. Apply the resulting single-layer structural-identity GNN to compute the updated scalar feature $h^{(1)}(3)$ for node $3$ on the given graph using the provided initial features.\n\nExpress the final answer as a single real number. No rounding is required and no units are associated with the answer. The final answer must be a single real-valued number.",
            "solution": "The problem requires the derivation of a single-layer structural-identity Graph Neural Network (GNN) aggregator and its application to compute an updated node feature. The solution proceeds in four steps: first, calculating the structural role vector for each node in the graph; second, partitioning the nodes into structural equivalence classes based on these vectors; third, deriving the unique aggregation function from the given architectural constraints; and fourth, applying this function to compute the updated feature for node $3$.\n\nStep 1: Compute node-level structural properties.\nThe graph is given by $G=(V,E)$ with the set of vertices $V=\\{1,2,3,4\\}$ and the set of edges $E=\\{(1,2),(2,3),(3,4),(4,1),(2,4)\\}$.\nThe structural role vector for each node $v \\in V$ is defined as $r(v) = [d(v), t(v)]$, where $d(v)$ is the degree of node $v$ and $t(v)$ is the number of triangle motifs containing $v$.\n\nFirst, we compute the degree $d(v)$ for each node, which is the number of edges connected to it.\n- Node $1$ is connected to nodes $2$ and $4$, so $d(1)=2$.\n- Node $2$ is connected to nodes $1$, $3$, and $4$, so $d(2)=3$.\n- Node $3$ is connected to nodes $2$ and $4$, so $d(3)=2$.\n- Node $4$ is connected to nodes $1$, $2$, and $3$, so $d(4)=3$.\n\nNext, we identify all triangle motifs. A triangle is a set of three nodes that are all mutually connected. We examine all subsets of $V$ of size $3$.\n- $\\{1,2,3\\}$: Edges are $(1,2)$ and $(2,3)$. The edge $(1,3)$ is missing. This is not a triangle.\n- $\\{1,2,4\\}$: Edges are $(1,2)$, $(1,4)$, and $(2,4)$. All three nodes are mutually connected. This is a triangle.\n- $\\{1,3,4\\}$: Edges are $(1,4)$ and $(3,4)$. The edge $(1,3)$ is missing. This is not a triangle.\n- $\\{2,3,4\\}$: Edges are $(2,3)$, $(2,4)$, and $(3,4)$. All three nodes are mutually connected. This is a triangle.\nThere are two triangles in the graph: $\\{1,2,4\\}$ and $\\{2,3,4\\}$.\n\nNow, we compute the triangle count $t(v)$ for each node, which is the number of triangles it belongs to.\n- Node $1$ is a member of the triangle $\\{1,2,4\\}$. Thus, $t(1)=1$.\n- Node $2$ is a member of both triangles, $\\{1,2,4\\}$ and $\\{2,3,4\\}$. Thus, $t(2)=2$.\n- Node $3$ is a member of the triangle $\\{2,3,4\\}$. Thus, $t(3)=1$.\n- Node $4$ is a member of both triangles, $\\{1,2,4\\}$ and $\\{2,3,4\\}$. Thus, $t(4)=2$.\n\nUsing these values, we can write the role vector $r(v)=[d(v), t(v)]$ for each node:\n- $r(1) = [2, 1]$\n- $r(2) = [3, 2]$\n- $r(3) = [2, 1]$\n- $r(4) = [3, 2]$\n\nStep 2: Determine the structural equivalence classes.\nThe structural equivalence class for a node $v$ is defined as $C(v) = \\{u \\in V \\mid r(u) = r(v)\\}$. We group nodes that have identical role vectors.\n- For node $1$, $r(1) = [2, 1]$. Node $3$ has the same role vector, $r(3) = [2, 1]$. So, the equivalence class is $C(1) = C(3) = \\{1, 3\\}$.\n- For node $2$, $r(2) = [3, 2]$. Node $4$ has the same role vector, $r(4) = [3, 2]$. So, the equivalence class is $C(2) = C(4) = \\{2, 4\\}$.\nThe vertices of the graph are partitioned into two structural equivalence classes: $\\{1, 3\\}$ and $\\{2, 4\\}$.\n\nStep 3: Derive the unique aggregator function.\nThe problem specifies four requirements for the single-layer update $h^{(1)}(v)$:\n$1$. The update aggregates information only from the structural equivalence class $C(v)$.\n$2$. The update is linear in the input features $\\{h^{(0)}(u) : u \\in C(v)\\}$.\n$3$. The update is permutation-invariant with respect to relabeling nodes within $C(v)$.\n$4$. The update is scale-stable and its output remains within the convex hull of the inputs.\n\nLet the aggregator be denoted by a function $f$. From requirements $1$ and $2$, $h^{(1)}(v)$ must be a linear combination of the features of the nodes in $C(v)$:\n$$h^{(1)}(v) = f(\\{h^{(0)}(u)\\}_{u \\in C(v)}) = \\sum_{u \\in C(v)} w_u h^{(0)}(u)$$\nwhere $w_u$ are weights. Requirement $3$, permutation invariance, implies that the weights cannot depend on the specific node $u$ but only on the class $C(v)$. Therefore, all weights $w_u$ for $u \\in C(v)$ must be equal. Let's call this common weight $w_{C(v)}$. The update rule simplifies to:\n$$h^{(1)}(v) = w_{C(v)} \\sum_{u \\in C(v)} h^{(0)}(u)$$\nRequirement $4$ states that the output must remain within the convex hull of the input features. For a set of scalar inputs, this means the output must be between the minimum and maximum input values. A sufficient condition to guarantee this for a linear combination is that the combination is a convex combination. This requires the coefficients to be non-negative and to sum to $1$.\nIn our case, the effective coefficients for the input features $\\{h^{(0)}(u)\\}_{u \\in C(v)}$ are all equal to $w_{C(v)}$. The sum of these coefficients is:\n$$\\sum_{u \\in C(v)} w_{C(v)} = |C(v)| \\cdot w_{C(v)}$$\nFor this sum to be equal to $1$, we must have:\n$$|C(v)| \\cdot w_{C(v)} = 1 \\implies w_{C(v)} = \\frac{1}{|C(v)|}$$\nSince the size of a class $|C(v)|$ is always at least $1$, the weight $w_{C(v)}$ is positive, satisfying the non-negativity condition. Thus, the unique aggregator that meets all specified requirements is the mean aggregator over the structural equivalence class:\n$$h^{(1)}(v) = \\frac{1}{|C(v)|} \\sum_{u \\in C(v)} h^{(0)}(u)$$\n\nStep 4: Compute the updated feature $h^{(1)}(3)$.\nThe task is to compute the updated feature for node $v=3$. From Step 2, the structural equivalence class for node $3$ is $C(3) = \\{1, 3\\}$. The size of this class is $|C(3)|=2$.\nThe initial features for the nodes in this class are given as $h^{(0)}(1) = -2$ and $h^{(0)}(3) = 4$.\nApplying the derived mean aggregator formula:\n$$h^{(1)}(3) = \\frac{1}{|C(3)|} \\sum_{u \\in C(3)} h^{(0)}(u) = \\frac{1}{2} (h^{(0)}(1) + h^{(0)}(3))$$\nSubstituting the given feature values:\n$$h^{(1)}(3) = \\frac{1}{2} (-2 + 4) = \\frac{1}{2}(2) = 1$$\nThe updated scalar feature for node $3$ is $1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}