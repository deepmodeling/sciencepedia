{
    "hands_on_practices": [
        {
            "introduction": "To ground our theoretical understanding of maximum likelihood estimation, we begin with a complete, end-to-end link prediction workflow. This exercise  uses the foundational Stochastic Block Model (SBM) on a small, hypothetical network. You will walk through the entire process: estimating model parameters from a training dataset, using the fitted model to score and rank unobserved potential links, and finally, evaluating the quality of your predictions with standard performance metrics like precision and recall.",
            "id": "4286645",
            "problem": "Consider an undirected, unweighted network with node set $V=\\{1,2,3,4,5,6\\}$ and a known two-block partition $g:V\\to\\{A,B\\}$ given by $g(1)=g(2)=g(3)=A$ and $g(4)=g(5)=g(6)=B$. A training graph $G_{\\text{train}}=(V,E_{\\text{train}})$ is observed, with training edge set\n$$\nE_{\\text{train}}=\\{(1,2),(1,3),(4,5),(1,4),(2,5)\\}.\n$$\nA held-out probe set of true but removed edges is\n$$\nE_{\\text{probe}}=\\{(2,3),(4,6),(3,6)\\}.\n$$\nAll other unordered pairs in $\\binom{V}{2}$ not in $E_{\\text{train}}$ are treated as non-edges in the training data and constitute the candidate set $U$ for link prediction.\n\nAssume a Stochastic Block Model (SBM) with independent Bernoulli edges conditioned on the block memberships, with within-block probabilities $p_{AA}$ and $p_{BB}$ and between-block probability $p_{AB}$. Use the Maximum Likelihood Estimator (MLE) to estimate $(\\hat{p}_{AA},\\hat{p}_{BB},\\hat{p}_{AB})$ from the training graph $G_{\\text{train}}$. Use the resulting estimated edge probabilities to produce a link-prediction ranking over $U$ by sorting candidate pairs in descending order of their estimated probability under the fitted SBM. In the case of ties, break ties deterministically by lexicographic order on node labels, i.e., $(i,j)$ precedes $(i',j')$ if and only if $ii'$ or ($i=i'$ and $jj'$).\n\nDefine precision at top-$k$ as the fraction of the first $k$ ranked pairs that lie in $E_{\\text{probe}}$, and recall at top-$k$ as the fraction of $E_{\\text{probe}}$ that appear among the first $k$ ranked pairs. Let $T=|U|$. Define the global base-rate estimate $\\hat{p}$ under the Erdős–Rényi $G(n,p)$ model as the MLE computed from $G_{\\text{train}}$. Select\n$$\nk^{\\star}=\\left\\lceil \\hat{p} \\, T \\right\\rceil.\n$$\n\nStarting only from the standard independence assumptions of the SBM, the Bernoulli likelihood for edges within and between blocks, and the definitions of MLE, precision, and recall, do the following:\n- Derive $(\\hat{p}_{AA},\\hat{p}_{BB},\\hat{p}_{AB})$ from $G_{\\text{train}}$.\n- Produce the ranked list over $U$ implied by these MLEs and the tie-breaking rule.\n- Compute the precision and recall at top-$k$ for $k\\in\\{1,2,3,4\\}$ explicitly.\n- Derive the base-rate estimate $\\hat{p}$ and compute $k^{\\star}$.\n- Using $k^{\\star}$, compute the harmonic mean of precision and recall (the $F_1$ score) at top-$k^{\\star}$.\n\nAnswer Specification:\n- Report as your final answer the exact value of the $F_1$ score at top-$k^{\\star}$ as a single simplified fraction, with no units.",
            "solution": "The solution proceeds in five steps: estimating the SBM parameters, ranking candidate links, computing precision and recall, determining the threshold $k^{\\star}$, and calculating the final $F_1$ score.\n\n**Step 1: Estimate SBM parameters using MLE**\n\nFor a Stochastic Block Model, the Maximum Likelihood Estimate (MLE) for the probability of an edge between any two blocks, $r$ and $s$, is the ratio of the observed number of edges ($E_{rs}$) to the total number of possible edges ($N_{rs}$) between those blocks in the training data.\n$$ \\hat{p}_{rs} = \\frac{E_{rs}}{N_{rs}} $$\nThe node partitions are $V_A=\\{1,2,3\\}$ and $V_B=\\{4,5,6\\}$, so $n_A=3$ and $n_B=3$. The number of possible edges for each block-pair type are:\n- Within A (AA): $N_{AA} = \\binom{3}{2} = 3$.\n- Within B (BB): $N_{BB} = \\binom{3}{2} = 3$.\n- Between A and B (AB): $N_{AB} = n_A \\times n_B = 9$.\n\nWe count the edges in $E_{\\text{train}}=\\{(1,2),(1,3),(4,5),(1,4),(2,5)\\}$ for each type:\n- AA edges: $(1,2), (1,3)$. Thus, $E_{AA} = 2$.\n- BB edges: $(4,5)$. Thus, $E_{BB} = 1$.\n- AB edges: $(1,4), (2,5)$. Thus, $E_{AB} = 2$.\n\nThe MLEs are:\n- $\\hat{p}_{AA} = E_{AA} / N_{AA} = 2/3$.\n- $\\hat{p}_{BB} = E_{BB} / N_{BB} = 1/3$.\n- $\\hat{p}_{AB} = E_{AB} / N_{AB} = 2/9$.\n\n**Step 2: Produce the ranked list over the candidate set $U$**\n\nThe set $U$ contains all pairs not in $E_{\\text{train}}$. Total possible pairs are $\\binom{6}{2}=15$. $|E_{\\text{train}}|=5$, so $|U|=T=10$.\nWe score each candidate pair in $U$ with its estimated probability and sort them. The probabilities are $\\hat{p}_{AA} \\approx 0.667$, $\\hat{p}_{BB} \\approx 0.333$, and $\\hat{p}_{AB} \\approx 0.222$.\nThe candidate pairs and their types are:\n- Type AA (score $2/3$): $(2,3)$.\n- Type BB (score $1/3$): $(4,6), (5,6)$.\n- Type AB (score $2/9$): $(1,5), (1,6), (2,4), (2,6), (3,4), (3,5), (3,6)$.\n\nSorting by score (descending) and then lexicographically (ascending) gives the ranked list:\n1. $(2,3)$\n2. $(4,6)$\n3. $(5,6)$\n4. $(1,5)$\n5. $(1,6)$\n6. $(2,4)$\n7. $(2,6)$\n8. $(3,4)$\n9. $(3,5)$\n10. $(3,6)$\n\n**Step 3: Compute precision and recall at $k \\in \\{1,2,3,4\\}$**\n\nThe probe set is $E_{\\text{probe}}=\\{(2,3), (4,6), (3,6)\\}$, so $|E_{\\text{probe}}|=3$. The true links appear at ranks 1, 2, and 10.\n- For $k=1$: Top-1 is $\\{(2,3)\\}$. Hits=1. Precision@1 = $1/1=1$. Recall@1 = $1/3$.\n- For $k=2$: Top-2 is $\\{(2,3), (4,6)\\}$. Hits=2. Precision@2 = $2/2=1$. Recall@2 = $2/3$.\n- For $k=3$: Top-3 is $\\{(2,3), (4,6), (5,6)\\}$. Hits=2. Precision@3 = $2/3$. Recall@3 = $2/3$.\n- For $k=4$: Top-4 is $\\{(2,3), (4,6), (5,6), (1,5)\\}$. Hits=2. Precision@4 = $2/4 = 1/2$. Recall@4 = $2/3$.\n\n**Step 4: Compute the base-rate estimate $\\hat{p}$ and $k^{\\star}$**\n\nThe MLE for the Erdős–Rényi model parameter $p$ is the overall edge density in the training graph.\n$$ \\hat{p} = \\frac{|E_{\\text{train}}|}{\\binom{|V|}{2}} = \\frac{5}{15} = \\frac{1}{3} $$\nWith $T=|U|=10$, we find $k^{\\star}$:\n$$ k^{\\star} = \\lceil \\hat{p} \\, T \\rceil = \\left\\lceil \\frac{1}{3} \\times 10 \\right\\rceil = \\lceil 3.33... \\rceil = 4 $$\n\n**Step 5: Compute the $F_1$ score at $k^{\\star}=4$**\n\nThe $F_1$ score is the harmonic mean of precision and recall. Using the values for $k=4$:\n- Precision@4 = $1/2$\n- Recall@4 = $2/3$\n$$ F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{\\frac{1}{2} \\times \\frac{2}{3}}{\\frac{1}{2} + \\frac{2}{3}} = 2 \\times \\frac{\\frac{1}{3}}{\\frac{3+4}{6}} = 2 \\times \\frac{\\frac{1}{3}}{\\frac{7}{6}} = 2 \\times \\frac{1}{3} \\times \\frac{6}{7} = \\frac{12}{21} = \\frac{4}{7} $$",
            "answer": "$$ \\boxed{\\frac{4}{7}} $$"
        },
        {
            "introduction": "Building on the SBM, the Mixed-Membership Stochastic Block Model (MMSBM) offers a more expressive framework by allowing nodes to have partial memberships in multiple communities. Before we can apply such a model, we must first understand how to construct its likelihood function, which is the objective for maximum likelihood estimation. This practice  challenges you to derive the observed-data likelihood by correctly marginalizing over the model's latent variables, providing a crucial bridge from the model's generative story to its statistical inference.",
            "id": "4286590",
            "problem": "Consider an undirected, simple network with $N$ nodes and no self-loops, represented by a symmetric adjacency matrix $A$ with entries $A_{ij} \\in \\{0,1\\}$ for $ij$. Assume a mixed-membership stochastic block model (MMSBM) with $K$ latent groups and the following generative structure based on core definitions:\n- For each node $i$, a membership vector $\\pi_{i} = (\\pi_{i1},\\ldots,\\pi_{iK})$ is a probability vector over $K$ groups, encoding soft memberships.\n- For each unordered node pair $ij$, independent latent one-hot assignments (hard assignments) $z_{i \\to j}$ and $z_{j \\to i}$ are drawn, where $z_{i \\to j}$ is a $K$-dimensional standard basis vector with $\\Pr(z_{i \\to j} = e_{k}) = \\pi_{ik}$ and $z_{j \\to i}$ is a $K$-dimensional standard basis vector with $\\Pr(z_{j \\to i} = e_{\\ell}) = \\pi_{j\\ell}$, independently of $z_{i \\to j}$. These encode the dyad-specific hard community roles sampled from the soft memberships.\n- Conditioned on these dyad-specific hard assignments and a symmetric $K \\times K$ matrix of link probabilities $B = (B_{k\\ell})$ with $B_{k\\ell} \\in (0,1)$ and $B_{k\\ell} = B_{\\ell k}$, the edge $A_{ij}$ is drawn independently as a Bernoulli random variable with parameter $B_{k\\ell}$ when $z_{i \\to j} = e_{k}$ and $z_{j \\to i} = e_{\\ell}$.\n\nTask:\n- Starting from these assumptions and the independence structure, distinguish the role of dyad-specific hard assignments $z_{i \\to j}, z_{j \\to i}$ from node-level soft memberships $\\pi_{i}$, and derive the observed-data likelihood $p(A \\mid \\{\\pi_{i}\\}_{i=1}^{N}, B)$ by marginalizing out all latent $z_{i \\to j}, z_{j \\to i}$.\n- Then, specialize to the following concrete instance with $N=3$ nodes and $K=2$ groups. The membership vectors are\n$\\pi_{1} = (0.8, 0.2)$, $\\pi_{2} = (0.3, 0.7)$, $\\pi_{3} = (0.6, 0.4)$,\nthe symmetric link-probability matrix is\n$B = \\begin{pmatrix} 0.85  0.25 \\\\ 0.25  0.50 \\end{pmatrix}$,\nand the observed adjacency entries for the unordered pairs are $A_{12} = 1$, $A_{13} = 0$, $A_{23} = 1$.\nCompute the numerical value of the observed-data likelihood $p(A \\mid \\{\\pi_{i}\\}_{i=1}^{3}, B)$ implied by your derived expression. Round your final numerical answer to $4$ significant figures. No units are required.",
            "solution": "The problem consists of three parts: distinguishing the roles of different model components, deriving the observed-data likelihood, and computing its value for a specific instance.\n\n### Role of Soft Memberships vs. Dyad-Specific Hard Assignments\n\nIn the Mixed-Membership Stochastic Block Model, the parameters $\\pi_i$ and the latent variables $z_{i \\to j}, z_{j \\to i}$ represent different levels of abstraction in how nodes relate to latent groups.\n\n- **Node-level Soft Memberships $\\pi_i$**: The vector $\\pi_i = (\\pi_{i1}, \\dots, \\pi_{iK})$ is a property of node $i$ itself. It represents a static, long-term probabilistic profile of the node's affiliations across the $K$ latent groups. It is a \"soft\" or \"mixed\" membership because a node is not forced into a single group but can simultaneously belong to multiple groups to varying degrees. For instance, in a social network of scientists, a researcher $i$ might have $\\pi_i = (0.7, 0.3)$ for groups corresponding to \"Physics\" and \"Computer Science,\" indicating their interdisciplinary nature. This vector is fixed for the node across all its potential interactions.\n\n- **Dyad-specific Hard Assignments $z_{i \\to j}, z_{j \\to i}$**: These are unobserved latent variables specific to a particular dyad, or pair of nodes $(i, j)$. For each potential interaction (the formation of an edge $A_{ij}$), node $i$ \"expresses\" a single group identity, and so does node $j$. The variable $z_{i \\to j}$ is a one-hot vector indicating which of the $K$ groups node $i$ assumes *in the context of its interaction with node $j$*. Similarly, $z_{j \\to i}$ is the group identity expressed by node $j$ in its interaction with node $i$. These are \"hard\" assignments because for any given interaction, the nodes take on definite, singular roles (e.g., node $i$ acts as a member of group $k$, and node $j$ acts as a member of group $\\ell$). These hard roles are sampled independently for each dyad from the nodes' soft membership profiles: $\\Pr(z_{i \\to j} = e_k) = \\pi_{ik}$ and $\\Pr(z_{j \\to i} = e_\\ell) = \\pi_{j\\ell}$, where $e_k$ is the standard basis vector for group $k$. This mechanism allows a node to exhibit different facets of its mixed identity when interacting with different partners.\n\nIn summary, $\\pi_i$ is the underlying, node-specific potential, while $z_{i \\to j}$ and $z_{j \\to i}$ are the realized, interaction-specific roles actualized from that potential.\n\n### Derivation of the Observed-Data Likelihood\n\nThe goal is to find the likelihood of the observed adjacency matrix $A$, given the model parameters $\\{\\pi_{i}\\}_{i=1}^{N}$ and $B$. This requires marginalizing out the unobserved latent variables $\\{z_{i \\to j}, z_{j \\to i}\\}$ for all pairs $ij$.\n\nThe model assumes that, conditional on the parameters, the presence or absence of each edge $A_{ij}$ (for $i  j$) is an independent event. Therefore, the total likelihood is the product of the likelihoods for each pair:\n$$\np(A \\mid \\{\\pi_{i}\\}, B) = \\prod_{ij} p(A_{ij} \\mid \\pi_i, \\pi_j, B)\n$$\nWe focus on deriving the likelihood for a single edge $A_{ij}$. Using the law of total probability, we sum over all possible latent hard assignments for the pair $(i, j)$:\n$$\np(A_{ij} \\mid \\pi_i, \\pi_j, B) = \\sum_{k=1}^{K} \\sum_{\\ell=1}^{K} p(A_{ij}, z_{i \\to j}=e_k, z_{j \\to i}=e_\\ell \\mid \\pi_i, \\pi_j, B)\n$$\nBy the chain rule of probability, the joint probability inside the summation can be decomposed:\n$$\np(A_{ij}, z_{i \\to j}=e_k, z_{j \\to i}=e_\\ell \\mid \\pi_i, \\pi_j, B) = p(A_{ij} \\mid z_{i \\to j}=e_k, z_{j \\to i}=e_\\ell, B) \\cdot p(z_{i \\to j}=e_k, z_{j \\to i}=e_\\ell \\mid \\pi_i, \\pi_j)\n$$\nFrom the model definition:\n1. The edge probability $p(A_{ij} \\mid \\dots)$ depends only on the latent assignments $z_{i \\to j}, z_{j \\to i}$ and the matrix $B$. Specifically, it is a Bernoulli trial with parameter $B_{k\\ell}$:\n$$\np(A_{ij} \\mid z_{i \\to j}=e_k, z_{j \\to i}=e_\\ell, B) = B_{k\\ell}^{A_{ij}} (1 - B_{k\\ell})^{1-A_{ij}}\n$$\n2. The latent assignments are drawn independently from the node membership vectors:\n$$\np(z_{i \\to j}=e_k, z_{j \\to i}=e_\\ell \\mid \\pi_i, \\pi_j) = p(z_{i \\to j}=e_k \\mid \\pi_i) \\cdot p(z_{j \\to i}=e_\\ell \\mid \\pi_j) = \\pi_{ik} \\pi_{j\\ell}\n$$\nSubstituting these back into the summation gives the likelihood for a single edge:\n$$\np(A_{ij} \\mid \\pi_i, \\pi_j, B) = \\sum_{k=1}^{K} \\sum_{\\ell=1}^{K} \\pi_{ik} \\pi_{j\\ell} B_{k\\ell}^{A_{ij}} (1 - B_{k\\ell})^{1-A_{ij}}\n$$\nThis expression can be simplified. Let $P_{ij}$ be the marginal probability of a link between $i$ and $j$, i.e., $P_{ij} = p(A_{ij}=1 \\mid \\pi_i, \\pi_j, B)$.\nIf $A_{ij}=1$, the likelihood is:\n$$\np(A_{ij}=1 \\mid \\pi_i, \\pi_j, B) = \\sum_{k=1}^{K} \\sum_{\\ell=1}^{K} \\pi_{ik} \\pi_{j\\ell} B_{k\\ell} = \\pi_i^T B \\pi_j\n$$\nIf $A_{ij}=0$, the likelihood is:\n$$\np(A_{ij}=0 \\mid \\pi_i, \\pi_j, B) = \\sum_{k=1}^{K} \\sum_{\\ell=1}^{K} \\pi_{ik} \\pi_{j\\ell} (1 - B_{k\\ell}) = 1 - \\pi_i^T B \\pi_j\n$$\nCombining both cases, the likelihood for a single edge $A_{ij}$ is given by a Bernoulli distribution with parameter $P_{ij} = \\pi_i^T B \\pi_j$:\n$$\np(A_{ij} \\mid \\pi_i, \\pi_j, B) = (\\pi_i^T B \\pi_j)^{A_{ij}} (1 - \\pi_i^T B \\pi_j)^{1 - A_{ij}}\n$$\nThe full observed-data likelihood is the product over all unique pairs:\n$$\np(A \\mid \\{\\pi_{i}\\}, B) = \\prod_{ij} \\left( \\pi_i^T B \\pi_j \\right)^{A_{ij}} \\left( 1 - \\pi_i^T B \\pi_j \\right)^{1 - A_{ij}}\n$$\n\n### Numerical Computation\n\nWe are given $N=3$ nodes, $K=2$ groups, and the following data:\n- $\\pi_{1} = \\begin{pmatrix} 0.8 \\\\ 0.2 \\end{pmatrix}$, $\\pi_{2} = \\begin{pmatrix} 0.3 \\\\ 0.7 \\end{pmatrix}$, $\\pi_{3} = \\begin{pmatrix} 0.6 \\\\ 0.4 \\end{pmatrix}$\n- $B = \\begin{pmatrix} 0.85  0.25 \\\\ 0.25  0.50 \\end{pmatrix}$\n- $A_{12} = 1$, $A_{13} = 0$, $A_{23} = 1$\n\nThe total likelihood $L$ is the product of the probabilities for the three unique pairs $(1,2)$, $(1,3)$, and $(2,3)$:\n$$\nL = p(A_{12}=1 \\mid \\pi_1, \\pi_2, B) \\cdot p(A_{13}=0 \\mid \\pi_1, \\pi_3, B) \\cdot p(A_{23}=1 \\mid \\pi_2, \\pi_3, B)\n$$\nUsing the derived formula, this is:\n$$\nL = (\\pi_1^T B \\pi_2) \\cdot (1 - \\pi_1^T B \\pi_3) \\cdot (\\pi_2^T B \\pi_3)\n$$\nWe compute each term separately.\n\n1.  **Term 1: $p(A_{12}=1) = \\pi_1^T B \\pi_2$**\n    $$\n    P_{12} = \\begin{pmatrix} 0.8  0.2 \\end{pmatrix} \\begin{pmatrix} 0.85  0.25 \\\\ 0.25  0.50 \\end{pmatrix} \\begin{pmatrix} 0.3 \\\\ 0.7 \\end{pmatrix} = (0.73)(0.3) + (0.30)(0.7) = 0.219 + 0.210 = 0.429\n    $$\n\n2.  **Term 2: $p(A_{13}=0) = 1 - \\pi_1^T B \\pi_3$**\n    First, compute $P_{13} = \\pi_1^T B \\pi_3$.\n    $$\n    P_{13} = \\begin{pmatrix} 0.8  0.2 \\end{pmatrix} \\begin{pmatrix} 0.85  0.25 \\\\ 0.25  0.50 \\end{pmatrix} \\begin{pmatrix} 0.6 \\\\ 0.4 \\end{pmatrix} = (0.73)(0.6) + (0.30)(0.4) = 0.438 + 0.120 = 0.558\n    $$\n    The probability of a non-edge is $1 - P_{13} = 1 - 0.558 = 0.442$.\n\n3.  **Term 3: $p(A_{23}=1) = \\pi_2^T B \\pi_3$**\n    $$\n    P_{23} = \\begin{pmatrix} 0.3  0.7 \\end{pmatrix} \\begin{pmatrix} 0.85  0.25 \\\\ 0.25  0.50 \\end{pmatrix} \\begin{pmatrix} 0.6 \\\\ 0.4 \\end{pmatrix} = (0.43)(0.6) + (0.425)(0.4) = 0.258 + 0.170 = 0.428\n    $$\n\nFinally, we multiply the three terms to find the total likelihood $L$:\n$$\nL = 0.429 \\times 0.442 \\times 0.428 = 0.081156504\n$$\nRounding this value to $4$ significant figures, we get $0.08116$.",
            "answer": "$$\n\\boxed{0.08116}\n$$"
        },
        {
            "introduction": "Having established the likelihood for the MMSBM, we now turn to its primary application: prediction. This exercise  demonstrates the practical power of the mixed-membership formulation. By working with a pre-fitted model, you will calculate the predicted probabilities of new links and see firsthand how the model's ability to capture nuanced, \"soft\" community affiliations results in a more refined and discriminative ranking of candidates than a simpler hard-assignment approach would yield.",
            "id": "4286556",
            "problem": "Consider an undirected, simple network modeled by the Mixed-Membership Stochastic Block Model (MMSBM) with $K=2$ communities. In this model, each node $i$ has a membership vector $\\pi_{i} \\in \\Delta^{K-1}$ that specifies the probabilities of node $i$ participating in each of the $K$ communities, and a $K \\times K$ block-probability matrix $B$ specifies the probability of an edge conditioned on a pair of communities. Under maximum likelihood estimation (MLE) from observed data, you obtain parameter estimates $\\hat{\\pi}_{i}$ and $\\hat{B}$. For prediction on an unobserved pair $\\{i,j\\}$, the edge $A_{ij}$ is modeled as a Bernoulli random variable whose parameter is the expectation, under the endpoint community memberships, of the corresponding block probability.\n\nYou are given the following MLEs for a toy case with $K=2$:\n- $\\hat{B}=\\begin{pmatrix}0.6  0.05 \\\\ 0.05  0.4\\end{pmatrix}$,\n- $\\hat{\\pi}_{1}=(0.8,\\,0.2)$, $\\hat{\\pi}_{2}=(0.15,\\,0.85)$, $\\hat{\\pi}_{3}=(0.55,\\,0.45)$, $\\hat{\\pi}_{4}=(0.95,\\,0.05)$.\n\nAssume that the present snapshot of the observed network has no edges between node $1$ and nodes $2$, $3$, and $4$, and you wish to rank the candidate links $\\{1,2\\}$, $\\{1,3\\}$, and $\\{1,4\\}$ by their predicted probabilities under the MMSBM. Starting from the generative definition of the MMSBM (community indicators drawn for each endpoint, followed by a Bernoulli edge with a community-pair-specific probability), derive the predictive Bernoulli parameter for a generic pair $\\{i,j\\}$ in terms of $\\hat{\\pi}_{i}$, $\\hat{\\pi}_{j}$, and $\\hat{B}$, and then use it to compute the specific value for $\\hat{p}_{13}$. Briefly interpret, within your working, how allowing mixed memberships (as opposed to assigning each node to the single community with maximum membership probability) can change the ranking among the three candidate links.\n\nReport only the numerical value of $\\hat{p}_{13}$, rounded to four significant figures.",
            "solution": "The task is to derive the predictive probability of an edge in an MMSBM, use it to rank candidate links, compute a specific value, and interpret the model's behavior.\n\nThe MMSBM's generative process involves drawing community indicators for each node in a pair and then drawing an edge based on those indicators. The predictive probability $p_{ij}$ for an edge between nodes $i$ and $j$ is found by marginalizing over all possible community pairings:\n$$p_{ij} = P(A_{ij} = 1) = \\sum_{k=1}^{K} \\sum_{l=1}^{K} P(A_{ij}=1 | z_{i \\to j}=k, z_{j \\to i}=l) P(z_{i \\to j}=k) P(z_{j \\to i}=l)$$\nGiven the model definitions, this becomes:\n$$p_{ij} = \\sum_{k=1}^{K} \\sum_{l=1}^{K} \\pi_{ik} \\pi_{jl} B_{kl}$$\nIn matrix notation, this is the quadratic form $p_{ij} = \\pi_{i} B \\pi_{j}^{T}$. Using the given maximum likelihood estimates (MLEs), the predictive probability is $\\hat{p}_{ij} = \\hat{\\pi}_{i} \\hat{B} \\hat{\\pi}_{j}^{T}$.\n\nWe are given $K=2$, $\\hat{B}=\\begin{pmatrix}0.6  0.05 \\\\ 0.05  0.4\\end{pmatrix}$, and membership vectors for four nodes. To rank the candidate links $\\{1,2\\}$, $\\{1,3\\}$, and $\\{1,4\\}$, we first compute the common factor $\\hat{\\pi}_1 \\hat{B}$:\n$$\\hat{\\pi}_1 \\hat{B} = \\begin{pmatrix} 0.8  0.2 \\end{pmatrix} \\begin{pmatrix} 0.6  0.05 \\\\ 0.05  0.4 \\end{pmatrix} = \\begin{pmatrix} (0.8)(0.6) + (0.2)(0.05)  (0.8)(0.05) + (0.2)(0.4) \\end{pmatrix} = \\begin{pmatrix} 0.49  0.12 \\end{pmatrix}$$\nNow we compute the specific probabilities:\n- For link $\\{1,2\\}$ with $\\hat{\\pi}_{2}=(0.15,\\,0.85)$:\n  $$\\hat{p}_{12} = \\begin{pmatrix} 0.49  0.12 \\end{pmatrix} \\begin{pmatrix} 0.15 \\\\ 0.85 \\end{pmatrix} = (0.49)(0.15) + (0.12)(0.85) = 0.0735 + 0.102 = 0.1755$$\n- For link $\\{1,3\\}$ with $\\hat{\\pi}_{3}=(0.55,\\,0.45)$:\n  $$\\hat{p}_{13} = \\begin{pmatrix} 0.49  0.12 \\end{pmatrix} \\begin{pmatrix} 0.55 \\\\ 0.45 \\end{pmatrix} = (0.49)(0.55) + (0.12)(0.45) = 0.2695 + 0.054 = 0.3235$$\n- For link $\\{1,4\\}$ with $\\hat{\\pi}_{4}=(0.95,\\,0.05)$:\n  $$\\hat{p}_{14} = \\begin{pmatrix} 0.49  0.12 \\end{pmatrix} \\begin{pmatrix} 0.95 \\\\ 0.05 \\end{pmatrix} = (0.49)(0.95) + (0.12)(0.05) = 0.4655 + 0.006 = 0.4715$$\nThe ranking of candidate links by their predicted probabilities is $\\{1,4\\} > \\{1,3\\} > \\{1,2\\}$, since $0.4715 > 0.3235 > 0.1755$. The numerical value for $\\hat{p}_{13}$ is $0.3235$.\n\nTo interpret how mixed memberships change the ranking, we compare this to a \"hard\" assignment model where each node is assigned to its most probable community.\nThe hard memberships ($\\pi'$) would be:\n- Node 1: $\\hat{\\pi}_1=(0.8, 0.2) \\implies \\pi'_1=(1,0)$ (Community 1)\n- Node 2: $\\hat{\\pi}_2=(0.15, 0.85) \\implies \\pi'_2=(0,1)$ (Community 2)\n- Node 3: $\\hat{\\pi}_3=(0.55, 0.45) \\implies \\pi'_3=(1,0)$ (Community 1)\n- Node 4: $\\hat{\\pi}_4=(0.95, 0.05) \\implies \\pi'_4=(1,0)$ (Community 1)\n\nThe predicted probabilities ($p'$) under this hard model are simply the entries of $\\hat{B}$:\n- $p'_{12} = \\hat{B}_{12} = 0.05$\n- $p'_{13} = \\hat{B}_{11} = 0.6$\n- $p'_{14} = \\hat{B}_{11} = 0.6$\n\nThe hard assignment model ranks $\\{1,3\\}$ and $\\{1,4\\}$ equally, as it treats nodes 3 and 4 as identical members of community 1. The MMSBM, by contrast, uses the full membership vectors. Node 4 is a much \"purer\" member of community 1 than node 3, which has a substantial affiliation with community 2. The MMSBM's probability calculation reflects this nuance: $\\hat{p}_{14}$ is higher than $\\hat{p}_{13}$ because the calculation for $\\{1,3\\}$ incorporates more weight from the lower-probability off-diagonal and community-2 entries of $\\hat{B}$, due to node 3's mixed profile. This breaks the tie and provides a more refined ranking.\n\nThe problem requires reporting the numerical value of $\\hat{p}_{13}$, which is $0.3235$.",
            "answer": "$$\\boxed{0.3235}$$"
        }
    ]
}