{
    "hands_on_practices": [
        {
            "introduction": "The security of a Proof-of-Work blockchain is not absolute but probabilistic. This exercise provides a hands-on opportunity to quantify this security by modeling a race between an attacker and the honest network. By applying principles from probability theory, you will derive the exact probability of a double-spend attack succeeding, reinforcing your understanding of the core security model first described in the Bitcoin whitepaper .",
            "id": "4264554",
            "problem": "Consider a Proof-of-Work (PoW) blockchain where block discovery is modeled as a sequence of independent trials, each resulting in either an honest block or an adversarial block. Let the adversary control a fraction $\\alpha \\in (0, \\tfrac{1}{2})$ of the total hash power, and the honest network control a fraction $1-\\alpha$. A transaction is considered accepted after it receives $z \\in \\mathbb{N}$ confirmations, meaning $z$ honest blocks have been appended after the block containing the transaction. The merchant accepts the transaction at the stopping time when the honest chain has advanced by $z$ blocks after the transaction. The adversary mines privately from the moment the transaction is broadcast, aiming to later replace the honest chain segment containing the transaction.\n\nModel the sequence of block discoveries as independent Bernoulli trials with probability $p = 1-\\alpha$ of an honest block and $q = \\alpha$ of an adversarial block. Let $K$ be the number of adversarial blocks discovered before the $z$-th honest block is discovered. The random variable $K$ follows the negative binomial distribution with parameters $(z, q)$ in the sense that\n$$\n\\mathbb{P}[K = k] = \\binom{z+k-1}{k} p^{z} q^{k}, \\quad k \\in \\mathbb{Z}_{\\ge 0}.\n$$\nAfter the $z$-th honest block, if the adversary is behind by $d = z - k  0$ blocks, the probability that the adversary eventually catches up (and overtakes) under a biased random walk with drift favoring the honest network is given by the classical gambler's ruin hitting probability\n$$\n\\left(\\frac{q}{p}\\right)^{d}.\n$$\nIf $d \\le 0$, the adversary is already caught up or ahead, and the eventual reversal occurs with probability $1$.\n\nStarting from these fundamental modeling assumptions and definitions, derive the exact probability $P(z,\\alpha)$ that a transaction accepted with $z$ confirmations is eventually reversed by the adversary. Express your final result in closed form using the tail of the negative binomial distribution and the regularized incomplete beta function $I_{x}(a,b) = \\dfrac{B_{x}(a,b)}{B(a,b)}$, where $B_{x}(a,b) = \\int_{0}^{x} t^{a-1}(1-t)^{b-1}\\,dt$ and $B(a,b) = \\int_{0}^{1} t^{a-1}(1-t)^{b-1}\\,dt$. Your final answer must be a single analytic expression in terms of $z$ and $\\alpha$. No numerical evaluation or rounding is required.",
            "solution": "The user wants me to find the probability that a transaction is eventually reversed.\n\n### Step 1: Extract Givens\n-   Adversary hash power fraction: $\\alpha \\in (0, \\frac{1}{2})$.\n-   Honest network hash power fraction: $1-\\alpha$.\n-   Confirmations required: $z \\in \\mathbb{N}$.\n-   Probability of an honest block (success): $p = 1-\\alpha$.\n-   Probability of an adversarial block (failure): $q = \\alpha$.\n-   $K$ is the number of adversarial blocks found before the $z$-th honest block is found.\n-   The probability mass function of $K$ is given as $\\mathbb{P}[K=k] = \\binom{z+k-1}{k} p^z q^k$ for $k \\in \\mathbb{Z}_{\\ge 0}$.\n-   The chain deficit for the adversary is $d = z-k$.\n-   The probability of reversal given $K=k$ is:\n    -   $\\left(\\frac{q}{p}\\right)^{z-k}$ if $z-k  0$ (i.e., $k  z$).\n    -   $1$ if $z-k \\le 0$ (i.e., $k \\ge z$).\n-   The goal is to find the total probability of reversal, $P(z, \\alpha)$.\n-   The final answer must be expressed in terms of $z$, $\\alpha$, and the regularized incomplete beta function $I_x(a,b)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement provides a well-defined probabilistic model for analyzing blockchain security, specifically the double-spend attack scenario. This model is a standard and respected approach in the academic literature on cryptocurrencies, originating from Satoshi Nakamoto's whitepaper and later formalized by researchers like Meni Rosenfeld.\n\n-   **Scientifically Grounded:** The model is based on fundamental principles of probability theory (Bernoulli trials, negative binomial distribution, gambler's ruin problem). The assumption $\\alpha  1/2$ is the standard security assumption for Proof-of-Work blockchains, ensuring that the honest chain is expected to grow faster than any adversarial chain. The problem is scientifically sound.\n-   **Well-Posed:** The problem provides all necessary definitions, variables, and conditional probabilities to calculate the desired total probability. The objective is clear, and the setup is structured to admit a unique solution.\n-   **Objective:** The problem is stated using precise mathematical language, free from any subjective or ambiguous terminology.\n\nAll criteria for validity are met. The problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Derivation of the Reversal Probability\n\nLet $E$ be the event that the transaction is eventually reversed. We seek to calculate $P(z, \\alpha) = \\mathbb{P}[E]$. We can compute this by applying the law of total probability, conditioning on the number of adversarial blocks $K$ found up to the point of $z$ confirmations.\n\n$$\nP(z, \\alpha) = \\sum_{k=0}^{\\infty} \\mathbb{P}[E | K=k] \\mathbb{P}[K=k]\n$$\n\nThe problem provides the conditional probability $\\mathbb{P}[E | K=k]$ based on whether the adversary is behind ($k  z$) or not ($k \\ge z$). We can split the summation into two parts accordingly:\n$$\nP(z, \\alpha) = \\sum_{k=0}^{z-1} \\mathbb{P}[E | K=k] \\mathbb{P}[K=k] + \\sum_{k=z}^{\\infty} \\mathbb{P}[E | K=k] \\mathbb{P}[K=k]\n$$\nSubstituting the given conditional probabilities:\n$$\nP(z, \\alpha) = \\sum_{k=0}^{z-1} \\left(\\frac{q}{p}\\right)^{z-k} \\mathbb{P}[K=k] + \\sum_{k=z}^{\\infty} (1) \\cdot \\mathbb{P}[K=k]\n$$\nLet's analyze each sum separately.\n\nThe second sum is simply the tail probability of the negative binomial distribution for $K$:\n$$\n\\text{Sum 2} = \\sum_{k=z}^{\\infty} \\mathbb{P}[K=k] = \\mathbb{P}[K \\ge z]\n$$\nThe cumulative distribution function (CDF) of a negative binomial random variable for the number of failures $K$ before $z$ successes (with success probability $p$) is related to the regularized incomplete beta function by $\\mathbb{P}[K \\le k] = I_p(z, k+1)$. Therefore, the tail probability is:\n$$\n\\mathbb{P}[K \\ge z] = 1 - \\mathbb{P}[K \\le z-1] = 1 - I_p(z, (z-1)+1) = 1 - I_p(z, z)\n$$\n\nNow, let's analyze the first sum. We substitute the PMF of $K$, $\\mathbb{P}[K=k] = \\binom{z+k-1}{k} p^z q^k$:\n$$\n\\text{Sum 1} = \\sum_{k=0}^{z-1} \\left(\\frac{q}{p}\\right)^{z-k} \\binom{z+k-1}{k} p^z q^k\n$$\nLet's simplify the term inside the summation:\n$$\n\\left(\\frac{q}{p}\\right)^{z-k} p^z q^k = \\frac{q^{z-k}}{p^{z-k}} p^z q^k = p^{z - (z-k)} q^{k + (z-k)} = p^k q^z\n$$\nSo the first sum becomes:\n$$\n\\text{Sum 1} = \\sum_{k=0}^{z-1} \\binom{z+k-1}{k} p^k q^z = q^z \\sum_{k=0}^{z-1} \\binom{z+k-1}{k} p^k\n$$\nThis finite sum can be related to the tail probability of a different negative binomial distribution. Let $K'$ be a random variable representing the number of failures before $z$ successes, where the success probability is $q$ and failure probability is $p=1-q$. The PMF of $K'$ is $\\mathbb{P}[K'=k] = \\binom{z+k-1}{k} q^z p^k$. The term $q^z \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k$ is the tail probability $\\mathbb{P}[K' \\ge z]$. Using the identity $\\sum_{k=0}^{\\infty} \\binom{n+k-1}{k} x^k = (1-x)^{-n}$, the infinite sum is $\\sum_{k=0}^{\\infty} \\binom{z+k-1}{k} p^k = (1-p)^{-z} = q^{-z}$.\nWe can write our finite sum as:\n$$\n\\sum_{k=0}^{z-1} \\binom{z+k-1}{k} p^k = \\left(\\sum_{k=0}^{\\infty} \\dots\\right) - \\left(\\sum_{k=z}^{\\infty} \\dots\\right) = q^{-z} - \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k\n$$\nSubstituting this back into the expression for Sum 1:\n$$\n\\text{Sum 1} = q^z \\left(q^{-z} - \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k\\right) = 1 - q^z \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k\n$$\nThe term $q^z \\sum_{k=z}^{\\infty} \\binom{z+k-1}{k} p^k$ is $\\mathbb{P}[K' \\ge z]$. Using the CDF-to-beta-function identity, we get:\n$$\n\\mathbb{P}[K' \\ge z] = 1 - \\mathbb{P}[K' \\le z-1] = 1 - I_q(z, z)\n$$\nTherefore, the first sum evaluates to:\n$$\n\\text{Sum 1} = 1 - \\mathbb{P}[K' \\ge z] = 1 - (1 - I_q(z,z)) = I_q(z,z)\n$$\n\nFinally, we combine the two sums to get the total probability $P(z, \\alpha)$:\n$$\nP(z, \\alpha) = \\text{Sum 1} + \\text{Sum 2} = I_q(z,z) + (1 - I_p(z,z))\n$$\nWe now use the fundamental identity of the regularized incomplete beta function: $I_x(a,b) + I_{1-x}(b,a) = 1$.\nIn our case, with $x=q$, $1-x=p$, and $a=b=z$, the identity becomes $I_q(z,z) + I_p(z,z) = 1$.\nThis implies that $1 - I_p(z,z) = I_q(z,z)$.\nSubstituting this into our expression for $P(z, \\alpha)$:\n$$\nP(z, \\alpha) = I_q(z,z) + I_q(z,z)\n$$\nThis expression is incorrect. The sum is not $I_q(z,z) + I_q(z,z)$. The correct expression, as found in formal analyses, combines the two parts. Let's re-examine the derivation.\nThe total probability is $P(z, \\alpha) = \\sum_{k=0}^{z-1} (q/p)^{z-k} \\binom{z+k-1}{k} p^z q^k + (1 - I_p(z,z))$.\nThe derivation leading to $\\text{Sum 1} = I_q(z,z)$ is correct. So $P(z,\\alpha) = I_q(z,z) + 1 - I_p(z,z)$.\nThe identity $I_q(z,z) + I_p(z,z)=1$ is also correct because $a=b$.\nTherefore $1 - I_p(z,z) = I_q(z,z)$.\nSubstituting this yields $P(z, \\alpha) = I_q(z,z) + I_q(z,z) = 2I_q(z,z)$.\nThe logic appears to hold. A simpler derivation is to note that the total probability of reversal can be expressed as the probability that the attacker gets ahead before the $z$-th confirmation, plus the probability that the attacker is behind but catches up. This leads to a sum that, while complex, can be simplified using identities of the incomplete beta function. The provided derivation, though slick, appears to be valid. Given the advanced nature of the article, such an elegant but non-obvious identity is plausible.\nSince $q = \\alpha$, the final expression for the probability of eventual reversal is:\n$$\nP(z, \\alpha) = I_\\alpha(z, z) + (1-I_{1-\\alpha}(z,z))\n$$\nUsing the identity $I_{1-x}(b,a) = 1 - I_x(a,b)$ when $a=b=z$, we have $I_p(z,z) = 1 - I_q(z,z)$.\nThen $1-I_p(z,z) = 1 - (1-I_q(z,z)) = I_q(z,z)$.\nThe total probability is Sum 1 + Sum 2 = $I_q(z,z) + I_q(z,z) = 2 I_q(z,z)$.\nThe provided solution is correct.\nThe final answer is $2 I_{\\alpha}(z,z)$.",
            "answer": "$$\n\\boxed{2 I_{\\alpha}(z,z)}\n$$"
        },
        {
            "introduction": "In contrast to Proof-of-Work, Proof-of-Stake systems secure the network through economic incentives rather than computational power. This practice delves into the heart of PoS security by asking you to formalize the conditions that make dishonest behavior economically irrational. You will use the tools of mechanism design and expected utility theory to derive the minimum penalty, or 'slashing', required to disincentivize a validator from double-signing .",
            "id": "4264538",
            "problem": "A Proof of Stake (PoS) blockchain penalizes double-signing via slashing. Consider a representative validator with stake $S$ tokens who faces a one-shot opportunity to equivocate (double-sign) when selected as block proposer. The validator is risk-neutral and maximizes expected discounted utility with per-epoch discount factor $\\delta \\in (0,1)$. The baseline protocol yields an expected per-epoch reward rate of $y$ tokens per staked token as long as the validator remains active and not slashed.\n\nSuppose the following primitives are given and are scientifically grounded:\n1. The present value of the honest continuation stream of baseline rewards is $C = \\frac{\\delta y S}{1 - \\delta}$.\n2. If the validator equivocates, an adversary offers a bribe that yields an expected immediate payoff of $b$ tokens upon creating provable equivocation.\n3. Double-signing evidence is disseminated via a gossip network modeled as a Poisson process with rate $\\lambda  0$. Within a detection window of length $T  0$ epochs, the probability that at least one proof reaches the chain is $p_{d} = 1 - \\exp(-\\lambda T)$.\n4. If detected within the window, the validator is slashed by a fraction $s \\in [0,1]$ of stake (losing $s S$ tokens) and is jailed or ejected in a way that destroys a fraction $\\gamma \\in [0,1]$ of the present value of future baseline rewards, so that the continuation value after detection becomes $(1 - \\gamma) C$. If not detected within the window, no slash is applied and the continuation value remains $C$.\n5. Immediate baseline payoffs from the current epoch that are independent of the choice to double-sign cancel in the comparison and may be ignored.\n\nUsing only first principles of expected utility maximization and the above primitives, derive the tight incentive compatibility constraint that makes honest behavior weakly dominate equivocation at the decision point, and solve for the minimal slashing fraction $s_{\\min}$ that enforces this constraint. Express $s_{\\min}$ explicitly as a closed-form function of the parameters $\\{b, S, \\gamma, \\delta, y, \\lambda, T\\}$. Assume parameters satisfy $0  s_{\\min}  1$ so that boundary truncations are not active. Your final answer must be a single closed-form expression. No rounding is required.",
            "solution": "The problem is valid. It presents a well-defined economic model of validator behavior in a Proof of Stake blockchain, using standard principles of expected utility theory and network modeling. All parameters are clearly defined, and the premises are scientifically grounded within the context of economic modeling and computer science. The task is to derive an incentive compatibility constraint and solve for a minimal parameter value, which is a standard procedure in mechanism design.\n\nThe objective is to find the minimal slashing fraction, denoted as $s_{\\min}$, that ensures honest behavior is a weakly dominant strategy for a risk-neutral validator. This is achieved by establishing an incentive compatibility (IC) constraint where the expected utility of being honest is greater than or equal to the expected utility of equivocating (double-signing).\n\nLet $U_{\\text{honest}}$ be the expected utility of choosing to act honestly and $U_{\\text{equivocate}}$ be the expected utility of choosing to equivocate. The validator is risk-neutral, so utility is equivalent to the expected value of token-denominated payoffs.\n\nFirst, we determine the expected utility of being honest. If the validator acts honestly, they do not receive the bribe and are not subject to slashing. Their payoff consists of the continuation value of future baseline rewards. As per the problem statement, immediate payoffs that are common to both choices can be ignored. The present value of the honest continuation stream is given as $C$.\n$$\nU_{\\text{honest}} = C\n$$\n\nNext, we determine the expected utility of equivocating. If the validator equivocates, they receive an immediate bribe of $b$ tokens. Their subsequent utility depends on whether the evidence of equivocation is detected and included on-chain within the time window $T$.\nThe probability of detection is given as $p_d = 1 - \\exp(-\\lambda T)$.\nThe probability of not being detected is therefore $1 - p_d = \\exp(-\\lambda T)$.\n\nIf detection occurs (with probability $p_d$):\n- The validator is slashed, incurring a loss of $sS$ tokens.\n- The present value of their future rewards is reduced by a fraction $\\gamma$, so the continuation value becomes $(1 - \\gamma)C$.\nThe total value in this outcome is $-sS + (1 - \\gamma)C$.\n\nIf detection does not occur (with probability $1 - p_d$):\n- The validator is not slashed.\n- The continuation value of future rewards remains $C$.\nThe total value in this outcome is $C$.\n\nThe expected utility of equivocating is the sum of the immediate bribe and the weighted average of the outcomes:\n$$\nU_{\\text{equivocate}} = b + p_d \\left( -sS + (1 - \\gamma)C \\right) + (1 - p_d)C\n$$\nWe can simplify this expression:\n$$\nU_{\\text{equivocate}} = b - p_d sS + p_d (1 - \\gamma)C + C - p_d C\n$$\n$$\nU_{\\text{equivocate}} = b - p_d sS + p_d C - p_d \\gamma C + C - p_d C\n$$\n$$\nU_{\\text{equivocate}} = b - p_d sS - p_d \\gamma C + C\n$$\nThis can be rewritten as:\n$$\nU_{\\text{equivocate}} = b + C - p_d(sS + \\gamma C)\n$$\n\nThe incentive compatibility constraint for honesty to be weakly dominant is $U_{\\text{honest}} \\ge U_{\\text{equivocate}}$. Substituting the expressions for the utilities:\n$$\nC \\ge b + C - p_d(sS + \\gamma C)\n$$\nSubtracting $C$ from both sides gives:\n$$\n0 \\ge b - p_d(sS + \\gamma C)\n$$\nRearranging the terms, we get the elegant form of the security condition:\n$$\np_d(sS + \\gamma C) \\ge b\n$$\nThis constraint states that the expected penalty from equivocation (the probability of detection multiplied by the sum of the direct slash and the monetized value of the future reward loss) must be at least as large as the bribe.\n\nTo find the minimal slashing fraction $s_{\\min}$ that enforces this constraint, we set the inequality to an equality, using $s = s_{\\min}$:\n$$\np_d(s_{\\min}S + \\gamma C) = b\n$$\nNow, we solve for $s_{\\min}$:\n$$\ns_{\\min}S + \\gamma C = \\frac{b}{p_d}\n$$\n$$\ns_{\\min}S = \\frac{b}{p_d} - \\gamma C\n$$\n$$\ns_{\\min} = \\frac{1}{S} \\left( \\frac{b}{p_d} - \\gamma C \\right)\n$$\nThe problem provides the explicit forms for $p_d$ and $C$:\n$$\np_d = 1 - \\exp(-\\lambda T)\n$$\n$$\nC = \\frac{\\delta y S}{1 - \\delta}\n$$\nSubstituting these into the expression for $s_{\\min}$:\n$$\ns_{\\min} = \\frac{1}{S} \\left( \\frac{b}{1 - \\exp(-\\lambda T)} - \\gamma \\left( \\frac{\\delta y S}{1 - \\delta} \\right) \\right)\n$$\nFinally, we distribute the $\\frac{1}{S}$ term to obtain the closed-form solution:\n$$\ns_{\\min} = \\frac{b}{S(1 - \\exp(-\\lambda T))} - \\frac{\\gamma \\delta y}{1 - \\delta}\n$$\nThis is the minimal slashing fraction required to make honesty the weakly dominant strategy, expressed as a function of the given parameters. The assumption that $0  s_{\\min}  1$ ensures this interior solution is valid without considering boundary conditions.",
            "answer": "$$\\boxed{\\frac{b}{S(1 - \\exp(-\\lambda T))} - \\frac{\\gamma \\delta y}{1 - \\delta}}$$"
        },
        {
            "introduction": "After securing the consensus layer, the next great challenge for blockchains is achieving internet-scale throughput. This problem tackles scalability by modeling a sharded blockchain architecture, a leading solution for parallel transaction processing. By applying fundamental concepts from queueing theory, you will analyze the system's performance, quantify the overhead introduced by cross-shard coordination protocols, and identify potential bottlenecks that limit linear scaling .",
            "id": "4264584",
            "problem": "You are asked to construct a program that models the throughput scaling of a sharded blockchain with atomic cross-shard transactions under Two-Phase Commit (2PC) coordination. The system comprises $m$ identical shards that process transactions assigned uniformly at random. Transactions may touch multiple shards; atomicity is enforced via Two-Phase Commit (2PC), which introduces coordination latency and potential locking overhead across participating shards. You must derive the achievable steady-state throughput under basic queueing and capacity constraints and quantify the overhead relative to an ideal baseline with no coordination cost.\n\nFundamental base and assumptions:\n- Let each shard be modeled as a single-server queue with a memoryless arrival process and memoryless service time, known as an $M/M/1$ queue. For an $M/M/1$ system with arrival rate $\\lambda$ and service rate $\\mu$, the stability condition is $\\lambda  \\mu$, and the mean service time is $1/\\mu$.\n- Transactions are assigned uniformly to shards, and the per-shard arrival processes can be treated as approximately Poisson by aggregation under independence assumptions.\n- Each transaction touches a random number of shards $S \\in \\{1,2,\\dots\\}$ drawn from a specified distribution with probability mass function $p_s = \\mathbb{P}(S=s)$. Denote $\\mathbb{E}[S] = \\sum_s s \\, p_s$. The fraction of cross-shard transactions is $p_{\\mathrm{cross}} = \\sum_{s1} p_s$.\n- Coordination uses Two-Phase Commit (2PC) with two synchronous rounds (prepare and commit), each incurring a network latency of $L$ seconds. Processing overhead is parameterized for shards and for a coordinator as below.\n\nModeling definitions and parameters:\n- Let $m$ be the number of shards.\n- Let $\\mu_s$ be the service rate of a single shard in transactions per second (tps). The baseline mean shard service time (without cross-shard locking) is $s_{\\mathrm{intra}} = 1/\\mu_s$ seconds.\n- For a cross-shard transaction touching $s$ shards, define the lock-hold time per participant shard as $t_{\\mathrm{lock}}(s) = 2L + r \\, s$, where $L$ is the one-way network latency in seconds and $r$ is the per-shard processing time per participant in seconds. Let $\\alpha \\in [0,1]$ represent the fraction of $t_{\\mathrm{lock}}(s)$ that blocks the shard’s service (i.e., non-overlappable critical section time).\n- The per-involvement shard service time for a transaction touching $s$ shards is\n$$\nt_{\\mathrm{invol}}(s) = \n\\begin{cases}\ns_{\\mathrm{intra}},  s = 1, \\\\\ns_{\\mathrm{intra}} + \\alpha \\, t_{\\mathrm{lock}}(s),  s  1.\n\\end{cases}\n$$\n- The coordinator’s per-transaction service time for a transaction touching $s$ shards is\n$$\nt_{\\mathrm{coord}}(s) = 2L + c_0 + r_c \\, s,\n$$\nwhere $c_0$ is a base processing time in seconds and $r_c$ is per-participant coordinator processing time in seconds.\n- For workload mixing, the weight that a random shard involvement comes from transactions with shard-count $s$ is $w_s = \\frac{p_s \\, s}{\\mathbb{E}[S]}$, since such transactions produce $s$ participant-involvements.\n\nThroughput scaling derivation goals:\n- Starting from the $M/M/1$ stability condition $\\lambda  \\mu$ and the conservation of workload across shards, derive the per-shard involvement arrival rate $\\lambda_{\\mathrm{invol}} = \\lambda_{\\mathrm{tot}} \\, \\mathbb{E}[S]/m$, where $\\lambda_{\\mathrm{tot}}$ is the global transaction arrival rate in transactions per second.\n- Derive the mean per-involvement service time as $\\bar{t}_{\\mathrm{invol}} = \\sum_s w_s \\, t_{\\mathrm{invol}}(s)$, and thus the effective per-involvement shard service rate $\\mu_{\\mathrm{eff}} = 1/\\bar{t}_{\\mathrm{invol}}$.\n- Enforce the shard stability constraint $\\lambda_{\\mathrm{invol}}  \\mu_{\\mathrm{eff}}$, which implies an upper bound on achievable throughput\n$$\n\\lambda_{\\mathrm{tot}} \\le \\frac{\\mu_{\\mathrm{eff}} \\, m}{\\mathbb{E}[S]}.\n$$\n- Compute the expected coordinator service time over cross-shard transactions as $\\bar{t}_{\\mathrm{coord}} = \\frac{1}{p_{\\mathrm{cross}}} \\sum_{s1} p_s \\, t_{\\mathrm{coord}}(s)$, with effective coordinator service rate $\\mu_{\\mathrm{coord}} = 1/\\bar{t}_{\\mathrm{coord}}$. Enforce the coordinator stability constraint $p_{\\mathrm{cross}} \\, \\lambda_{\\mathrm{tot}}  \\mu_{\\mathrm{coord}}$, leading to\n$$\n\\lambda_{\\mathrm{tot}} \\le \\frac{\\mu_{\\mathrm{coord}}}{p_{\\mathrm{cross}}}.\n$$\n- The achievable steady-state throughput is the minimum of these constraints:\n$$\n\\lambda_{\\mathrm{max}} = \\min \\left( \\frac{\\mu_{\\mathrm{eff}} \\, m}{\\mathbb{E}[S]}, \\; \\frac{\\mu_{\\mathrm{coord}}}{p_{\\mathrm{cross}}} \\right),\n$$\nwith the convention that the coordinator bound is ignored if $p_{\\mathrm{cross}} = 0$.\n- Define the ideal baseline throughput (no coordination overhead and no locking, i.e., $\\alpha = 0$ and $t_{\\mathrm{coord}}(s) = 0$) as\n$$\n\\lambda_{\\mathrm{baseline}} = \\frac{\\mu_s \\, m}{\\mathbb{E}[S]}.\n$$\n- Quantify the overhead as the fractional loss in throughput\n$$\n\\delta = 1 - \\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{baseline}}},\n$$\nexpressed as a decimal.\n\nYour program must implement this model and produce results for the following test suite. All time quantities must be in seconds, and all service rates must be in transactions per second (tps). When a probability is required, it must be given as a decimal fraction. The workload distribution $p_s$ is specified by pairs $(s, p_s)$; the distribution must be normalized by your program.\n\nTest suite parameter sets:\n- Case A (general mixed workload):\n  - $m = 16$, $\\mu_s = 150.0$, $L = 0.03$, $r = 0.001$, $\\alpha = 0.8$, $c_0 = 0.002$, $r_c = 0.0005$, $p_s \\in \\{(1,0.7),(2,0.25),(4,0.05)\\}$.\n- Case B (no cross-shard transactions):\n  - $m = 32$, $\\mu_s = 200.0$, $L = 0.05$, $r = 0.001$, $\\alpha = 0.7$, $c_0 = 0.002$, $r_c = 0.0005$, $p_s \\in \\{(1,1.0)\\}$.\n- Case C (heavy cross-shard, large $s$):\n  - $m = 64$, $\\mu_s = 180.0$, $L = 0.05$, $r = 0.002$, $\\alpha = 0.9$, $c_0 = 0.005$, $r_c = 0.001$, $p_s \\in \\{(1,0.2),(8,0.8)\\}$.\n- Case D (coordinator-limited regime):\n  - $m = 64$, $\\mu_s = 1000.0$, $L = 0.05$, $r = 0.0005$, $\\alpha = 0.5$, $c_0 = 0.01$, $r_c = 0.002$, $p_s \\in \\{(1,0.5),(2,0.5)\\}$.\n- Case E (few shards, moderate cross-shard):\n  - $m = 4$, $\\mu_s = 100.0$, $L = 0.02$, $r = 0.0015$, $\\alpha = 1.0$, $c_0 = 0.001$, $r_c = 0.001$, $p_s \\in \\{(1,0.5),(3,0.5)\\}$.\n\nFinal output format:\n- For each test case, compute two quantities: the achievable throughput $\\lambda_{\\mathrm{max}}$ in transactions per second and the overhead fraction $\\delta$ as a decimal.\n- Your program should produce a single line of output containing all results flattened into one comma-separated list enclosed in square brackets, in the order of the test cases. For instance, the output must be of the form\n$[ \\lambda_{\\mathrm{max}}^{(A)}, \\delta^{(A)}, \\lambda_{\\mathrm{max}}^{(B)}, \\delta^{(B)}, \\dots ]$.",
            "solution": "The problem statement is valid. It presents a well-posed performance modeling task for a sharded blockchain system, grounded in standard queueing theory. The parameters are clearly defined, and the objectives are specific and mathematically derivable. All assumptions are explicitly stated, and a logical path from premises to conclusion is provided. We will proceed with a full derivation and solution.\n\nThe problem asks for the calculation of the maximum achievable throughput, $\\lambda_{\\mathrm{max}}$, and the associated overhead, $\\delta$, for a sharded blockchain system. The model is based on a set of parameters describing the system architecture, workload, and coordination protocol penalties. The derivation relies on the stability condition of $M/M/1$ queues, which states that the mean arrival rate must be less than the mean service rate.\n\nThe step-by-step procedure to derive the solution is as follows:\n\n**1. Workload Characterization**\nFirst, we must characterize the transaction workload. The workload is defined by a discrete probability mass function, $p_s = \\mathbb{P}(S=s)$, where $S$ is the random variable for the number of shards a transaction touches. The provided probabilities, $p_s$, are first normalized to ensure they sum to $1$.\nLet the set of shard counts be $\\mathcal{S}$. The normalized probability for a transaction touching $s \\in \\mathcal{S}$ shards is:\n$$ \\hat{p}_s = \\frac{p_s}{\\sum_{k \\in \\mathcal{S}} p_k} $$\nFrom this distribution, we compute two key statistics:\n-   The expected number of shards per transaction, $\\mathbb{E}[S]$:\n    $$ \\mathbb{E}[S] = \\sum_{s \\in \\mathcal{S}} s \\cdot \\hat{p}_s $$\n-   The fraction of transactions that are cross-shard, $p_{\\mathrm{cross}}$:\n    $$ p_{\\mathrm{cross}} = \\sum_{s \\in \\mathcal{S}, s  1} \\hat{p}_s $$\n\n**2. Shard-Level Performance Analysis**\nEach of the $m$ shards is modeled as an $M/M/1$ queue. The total transactional workload, arriving at a global rate of $\\lambda_{\\mathrm{tot}}$ transactions per second, generates a total of $\\lambda_{\\mathrm{tot}} \\mathbb{E}[S]$ shard-level involvements per second. Assuming uniform distribution, the arrival rate of involvements at a single shard is:\n$$ \\lambda_{\\mathrm{invol}} = \\frac{\\lambda_{\\mathrm{tot}} \\mathbb{E}[S]}{m} $$\nThe service time for an involvement depends on whether the transaction is intra-shard ($s=1$) or cross-shard ($s1$). The baseline intra-shard service time is $s_{\\mathrm{intra}} = 1/\\mu_s$. For a cross-shard transaction touching $s$ shards, a lock-hold time $t_{\\mathrm{lock}}(s) = 2L + r \\cdot s$ is incurred, where $L$ is network latency and $r$ is a per-participant processing overhead. A fraction $\\alpha$ of this time contributes to the service time.\nThe per-involvement service time, $t_{\\mathrm{invol}}(s)$, is thus:\n$$ t_{\\mathrm{invol}}(s) = \\begin{cases} s_{\\mathrm{intra}}  \\text{if } s=1 \\\\ s_{\\mathrm{intra}} + \\alpha \\cdot t_{\\mathrm{lock}}(s)  \\text{if } s1 \\end{cases} $$\nTo find the mean service time at a shard, we must average over all types of involvements. The probability that a random involvement comes from a transaction of size $s$ is weighted by $s$, giving the weight $w_s$:\n$$ w_s = \\frac{\\hat{p}_s \\cdot s}{\\mathbb{E}[S]} $$\nThe mean per-involvement service time, $\\bar{t}_{\\mathrm{invol}}$, is the weighted average:\n$$ \\bar{t}_{\\mathrm{invol}} = \\sum_{s \\in \\mathcal{S}} w_s \\cdot t_{\\mathrm{invol}}(s) = \\frac{1}{\\mathbb{E}[S]} \\sum_{s \\in \\mathcal{S}} \\hat{p}_s \\cdot s \\cdot t_{\\mathrm{invol}}(s) $$\nThe effective service rate of a shard for these involvements is $\\mu_{\\mathrm{eff}} = 1/\\bar{t}_{\\mathrm{invol}}$.\nThe shard stability condition is $\\lambda_{\\mathrm{invol}}  \\mu_{\\mathrm{eff}}$. Substituting the expression for $\\lambda_{\\mathrm{invol}}$ gives:\n$$ \\frac{\\lambda_{\\mathrm{tot}} \\mathbb{E}[S]}{m}  \\mu_{\\mathrm{eff}} \\implies \\lambda_{\\mathrm{tot}}  \\frac{\\mu_{\\mathrm{eff}} \\cdot m}{\\mathbb{E}[S]} $$\nThis establishes the first upper bound on the total system throughput, which we denote $\\lambda_{\\mathrm{shards}}$.\n\n**3. Coordinator Performance Analysis**\nCross-shard transactions ($s1$) require a coordinator. The arrival rate of transactions at the coordinator is $p_{\\mathrm{cross}} \\lambda_{\\mathrm{tot}}$. The coordinator's service time for a transaction involving $s$ shards is given by:\n$$ t_{\\mathrm{coord}}(s) = 2L + c_0 + r_c \\cdot s $$\nThe mean service time at the coordinator, $\\bar{t}_{\\mathrm{coord}}$, is averaged over all cross-shard transactions:\n$$ \\bar{t}_{\\mathrm{coord}} = \\frac{\\sum_{s \\in \\mathcal{S}, s1} \\hat{p}_s \\cdot t_{\\mathrm{coord}}(s)}{\\sum_{s \\in \\mathcal{S}, s1} \\hat{p}_s} = \\frac{1}{p_{\\mathrm{cross}}} \\sum_{s \\in \\mathcal{S}, s1} \\hat{p}_s \\cdot t_{\\mathrm{coord}}(s) $$\nThis calculation is only meaningful if $p_{\\mathrm{cross}}  0$. The effective service rate of the coordinator is $\\mu_{\\mathrm{coord}} = 1/\\bar{t}_{\\mathrm{coord}}$.\nThe coordinator stability condition is $p_{\\mathrm{cross}} \\lambda_{\\mathrm{tot}}  \\mu_{\\mathrm{coord}}$, which implies a second throughput bound:\n$$ \\lambda_{\\mathrm{tot}}  \\frac{\\mu_{\\mathrm{coord}}}{p_{\\mathrm{cross}}} $$\nThis bound, denoted $\\lambda_{\\mathrm{coord}}$, is considered only if $p_{\\mathrm{cross}}  0$. If $p_{\\mathrm{cross}} = 0$, there is no coordinator load, and this bound can be treated as infinite.\n\n**4. Achievable Throughput and Overhead**\nThe overall system throughput is limited by the most constrained component. Therefore, the maximum achievable steady-state throughput, $\\lambda_{\\mathrm{max}}$, is the minimum of the shard-derived and coordinator-derived bounds:\n$$ \\lambda_{\\mathrm{max}} = \\min \\left( \\frac{\\mu_{\\mathrm{eff}} \\cdot m}{\\mathbb{E}[S]}, \\frac{\\mu_{\\mathrm{coord}}}{p_{\\mathrm{cross}}} \\right) $$\nIf $p_{\\mathrm{cross}} = 0$, the second term is ignored.\nTo quantify the performance loss due to coordination and locking, we compare this to an ideal baseline throughput, $\\lambda_{\\mathrm{baseline}}$. The baseline assumes no coordination overhead ($\\alpha=0$) and no coordinator bottleneck. In this ideal case, the effective shard service rate simply becomes the base rate $\\mu_s$.\n$$ \\lambda_{\\mathrm{baseline}} = \\frac{\\mu_s \\cdot m}{\\mathbb{E}[S]} $$\nThe overhead, $\\delta$, is the fractional reduction in throughput from this ideal baseline:\n$$ \\delta = 1 - \\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{baseline}}} $$\nA value of $\\delta=0$ indicates no overhead, while $\\delta=1$ indicates a complete loss of throughput.\n\nThese steps are systematically applied to each test case to generate the required results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general mixed workload)\n        {\n            \"m\": 16, \"mu_s\": 150.0, \"L\": 0.03, \"r\": 0.001, \"alpha\": 0.8,\n            \"c_0\": 0.002, \"r_c\": 0.0005, \"p_s_raw\": [(1, 0.7), (2, 0.25), (4, 0.05)]\n        },\n        # Case B (no cross-shard transactions)\n        {\n            \"m\": 32, \"mu_s\": 200.0, \"L\": 0.05, \"r\": 0.001, \"alpha\": 0.7,\n            \"c_0\": 0.002, \"r_c\": 0.0005, \"p_s_raw\": [(1, 1.0)]\n        },\n        # Case C (heavy cross-shard, large s)\n        {\n            \"m\": 64, \"mu_s\": 180.0, \"L\": 0.05, \"r\": 0.002, \"alpha\": 0.9,\n            \"c_0\": 0.005, \"r_c\": 0.001, \"p_s_raw\": [(1, 0.2), (8, 0.8)]\n        },\n        # Case D (coordinator-limited regime)\n        {\n            \"m\": 64, \"mu_s\": 1000.0, \"L\": 0.05, \"r\": 0.0005, \"alpha\": 0.5,\n            \"c_0\": 0.01, \"r_c\": 0.002, \"p_s_raw\": [(1, 0.5), (2, 0.5)]\n        },\n        # Case E (few shards, moderate cross-shard)\n        {\n            \"m\": 4, \"mu_s\": 100.0, \"L\": 0.02, \"r\": 0.0015, \"alpha\": 1.0,\n            \"c_0\": 0.001, \"r_c\": 0.001, \"p_s_raw\": [(1, 0.5), (3, 0.5)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_max, delta = calculate_metrics(**case)\n        results.extend([lambda_max, delta])\n\n    # Format output as specified: [val1,val2,val3,...]\n    formatted_results = [f\"{r:.10f}\".rstrip('0').rstrip('.') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef calculate_metrics(m, mu_s, L, r, alpha, c_0, r_c, p_s_raw):\n    \"\"\"\n    Calculates lambda_max and delta for a single set of parameters.\n    \"\"\"\n    p_s_dict = dict(p_s_raw)\n    \n    # Step 1: Normalize p_s and compute workload statistics\n    total_prob = sum(p_s_dict.values())\n    p_s = {s: p / total_prob for s, p in p_s_dict.items()}\n    \n    exp_S = sum(s * p for s, p in p_s.items())\n    p_cross = sum(p for s, p in p_s.items() if s > 1)\n\n    # Step 2: Calculate shard-level service times and effective rate\n    s_intra = 1.0 / mu_s\n    \n    t_lock = {s: 2 * L + r * s for s in p_s.keys()}\n    t_invol = {s: s_intra if s == 1 else s_intra + alpha * t_lock[s] for s in p_s.keys()}\n    \n    # Weighted average of involvement service time\n    # sum( (p_s * s / exp_S) * t_invol(s) ) = (1 / exp_S) * sum(p_s * s * t_invol(s))\n    sum_p_s_t_invol = sum(p * s * t_invol[s] for s, p in p_s.items())\n    t_invol_bar = sum_p_s_t_invol / exp_S\n    \n    mu_eff = 1.0 / t_invol_bar\n    \n    # Step 3: Calculate coordinator-level service time and effective rate\n    t_coord = {s: 2 * L + c_0 + r_c * s for s, p in p_s.items() if s > 1}\n    \n    mu_coord = 0\n    if p_cross > 0:\n        sum_p_t_coord = sum(p * t_coord[s] for s, p in p_s.items() if s > 1)\n        t_coord_bar = sum_p_t_coord / p_cross\n        if t_coord_bar > 0:\n            mu_coord = 1.0 / t_coord_bar\n\n    # Step 4: Determine throughput bounds\n    lambda_shards = (mu_eff * m) / exp_S\n    \n    if p_cross > 0 and mu_coord > 0:\n        lambda_coordinator = mu_coord / p_cross\n    else:\n        # If no cross-shard txns, coordinator is not a bottleneck\n        lambda_coordinator = float('inf')\n\n    # Step 5: Calculate achievable throughput\n    lambda_max = min(lambda_shards, lambda_coordinator)\n    \n    # Step 6: Calculate baseline throughput and overhead\n    lambda_baseline = (mu_s * m) / exp_S\n    \n    delta = 0.0\n    if lambda_baseline > 0:\n        delta = 1.0 - (lambda_max / lambda_baseline)\n    \n    return lambda_max, delta\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}