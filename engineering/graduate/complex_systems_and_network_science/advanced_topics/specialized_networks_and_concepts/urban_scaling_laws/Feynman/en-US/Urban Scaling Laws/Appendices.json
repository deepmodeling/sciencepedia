{
    "hands_on_practices": [
        {
            "introduction": "Urban scaling laws are powerful descriptive tools, but their true scientific value is unlocked when they can be derived from first principles. This exercise challenges you to build a scaling model from the ground up, starting with well-established concepts in transportation engineering and human mobility . By synthesizing principles of infrastructure scaling, network capacity, and congestion, you will derive the emergent scaling exponent for total vehicle-kilometers traveled, revealing how complex urban dynamics can arise from simpler, interacting components.",
            "id": "4312858",
            "problem": "Consider an idealized metropolitan region with population $N$ in which daily personal mobility arises from individual trip-making on a road network. Assume the following empirically grounded principles and definitions drawn from complex systems and network science and transportation engineering:\n- Each resident generates trips at a constant rate $\\lambda$ (trips per person per day), so the aggregate demand is $Q(N) = \\lambda N$.\n- The total lane length of the road network scales with population as $L(N) = L_{0} N^{\\delta}$, where $L_{0} > 0$ and $0 < \\delta < 1$ are constants consistent with sublinear infrastructure scaling.\n- The network’s aggregate capacity is $C(N) = \\mu L(N)$, where $\\mu > 0$ is the capacity per unit lane length (vehicles per unit time).\n- Average travel time per unit distance (that is, the reciprocal of average speed) obeys a Bureau of Public Roads (BPR) form under congestion, namely $\\tau(N) = \\frac{1}{v_{f}}\\left[1 + \\alpha \\left(\\frac{Q(N)}{C(N)}\\right)^{\\kappa}\\right]$, where $v_{f} > 0$ is free-flow speed, $\\alpha > 0$ and $\\kappa > 0$ are congestion parameters calibrated from empirical observations, and the congested regime is characterized by $\\alpha \\left(\\frac{Q(N)}{C(N)}\\right)^{\\kappa} \\gg 1$.\n- Individuals allocate a constant daily time budget $T$ to travel (Marchetti’s constant), independent of $N$, so the average per-capita daily travel distance is $\\ell(N) = v(N) T$, where $v(N) = 1/\\tau(N)$.\n\nDefine the total daily vehicle-kilometers traveled (VKT) as $Y(N) = N \\ell(N)$. In the asymptotic congested regime described above, $Y(N)$ follows a power-law in $N$. Derive the scaling exponent $\\beta_{Y}$ such that $Y(N)$ scales as a power of $N$ with exponent $\\beta_{Y}$, and express $\\beta_{Y}$ as a closed-form analytic expression in terms of $\\delta$ and $\\kappa$ only. Do not substitute numerical values; provide the exact symbolic expression for the exponent. No rounding is required, and no units are needed for the exponent.",
            "solution": "The problem requires the derivation of the scaling exponent $\\beta_{Y}$ for the total daily vehicle-kilometers traveled, $Y(N)$, in a metropolitan region, under the assumption of a congested traffic regime. The relationship is given by $Y(N) \\propto N^{\\beta_{Y}}$. The exponent $\\beta_{Y}$ must be expressed in terms of the infrastructure scaling exponent $\\delta$ and the congestion parameter $\\kappa$.\n\nThe derivation proceeds by combining the given definitions and principles step-by-step.\n\nFirst, the total daily vehicle-kilometers traveled, $Y(N)$, is defined as the product of the population $N$ and the average per-capita daily travel distance $\\ell(N)$:\n$$Y(N) = N \\ell(N)$$\n\nThe average per-capita daily travel distance, $\\ell(N)$, is determined by the constant daily travel time budget, $T$, and the average travel speed, $v(N)$:\n$$\\ell(N) = v(N) T$$\nThe average travel speed $v(N)$ is the reciprocal of the average travel time per unit distance, $\\tau(N)$, so $v(N) = 1/\\tau(N)$. Substituting this into the expression for $\\ell(N)$ gives:\n$$\\ell(N) = \\frac{T}{\\tau(N)}$$\nSubstituting this back into the equation for $Y(N)$, we obtain:\n$$Y(N) = \\frac{N T}{\\tau(N)}$$\nSince $T$ is a constant independent of $N$, the scaling behavior of $Y(N)$ is determined by the scaling of $N / \\tau(N)$:\n$$Y(N) \\propto \\frac{N}{\\tau(N)}$$\n\nNext, we must determine the scaling of $\\tau(N)$ with respect to the population $N$. The average travel time per unit distance is given by the Bureau of Public Roads (BPR) function:\n$$\\tau(N) = \\frac{1}{v_{f}}\\left[1 + \\alpha \\left(\\frac{Q(N)}{C(N)}\\right)^{\\kappa}\\right]$$\nThe problem specifies that the system is in the congested regime, which is characterized by the condition $\\alpha \\left(\\frac{Q(N)}{C(N)}\\right)^{\\kappa} \\gg 1$. In this asymptotic limit, the constant term $1$ inside the brackets becomes negligible compared to the congestion term. Therefore, $\\tau(N)$ can be approximated as:\n$$\\tau(N) \\approx \\frac{1}{v_{f}} \\alpha \\left(\\frac{Q(N)}{C(N)}\\right)^{\\kappa}$$\nSince we are interested in the scaling exponent, we can use proportionality, where the constants $\\alpha$ and $v_{f}$ do not affect the exponent:\n$$\\tau(N) \\propto \\left(\\frac{Q(N)}{C(N)}\\right)^{\\kappa}$$\n\nNow, we express the volume-to-capacity ratio, $Q(N)/C(N)$, as a function of $N$. The aggregate demand $Q(N)$ is given by:\n$$Q(N) = \\lambda N$$\nThe aggregate capacity $C(N)$ is given by:\n$$C(N) = \\mu L(N)$$\nThe total lane length $L(N)$ scales with population as:\n$$L(N) = L_{0} N^{\\delta}$$\nSubstituting this into the expression for $C(N)$:\n$$C(N) = \\mu L_{0} N^{\\delta}$$\nThe volume-to-capacity ratio is therefore:\n$$\\frac{Q(N)}{C(N)} = \\frac{\\lambda N}{\\mu L_{0} N^{\\delta}} = \\left(\\frac{\\lambda}{\\mu L_{0}}\\right) N^{1-\\delta}$$\nThis shows that the volume-to-capacity ratio scales with $N$ as $N^{1-\\delta}$:\n$$\\frac{Q(N)}{C(N)} \\propto N^{1-\\delta}$$\n\nWe substitute this scaling relationship back into the expression for $\\tau(N)$:\n$$\\tau(N) \\propto \\left(N^{1-\\delta}\\right)^{\\kappa} = N^{\\kappa(1-\\delta)}$$\n\nFinally, we substitute the scaling for $\\tau(N)$ into the scaling relationship for $Y(N)$:\n$$Y(N) \\propto \\frac{N}{\\tau(N)} \\propto \\frac{N^{1}}{N^{\\kappa(1-\\delta)}}$$\nUsing the laws of exponents, we find the scaling for $Y(N)$:\n$$Y(N) \\propto N^{1 - \\kappa(1-\\delta)}$$\n\nThe problem defines the scaling exponent as $\\beta_{Y}$, where $Y(N) \\propto N^{\\beta_{Y}}$. By comparing this with our derived expression, we identify the exponent $\\beta_{Y}$ as:\n$$\\beta_{Y} = 1 - \\kappa(1-\\delta)$$\nThis expression for $\\beta_{Y}$ depends only on the parameters $\\delta$ and $\\kappa$, as required. It can also be written as $\\beta_{Y} = 1 - \\kappa + \\kappa\\delta$. This result provides the scaling exponent for total vehicle-kilometers traveled in the congested regime based on the provided a priori assumptions of urban scaling.",
            "answer": "$$\\boxed{1 - \\kappa(1-\\delta)}$$"
        },
        {
            "introduction": "Identifying a scaling relationship in empirical data is only the first step; rigorously selecting the best model and validating its assumptions is crucial for sound scientific inference. This computational practice guides you through the essential workflow of fitting urban scaling models to synthetic data, where the true underlying process is known . You will use standard statistical tools like the Akaike Information Criterion (AIC) and the Likelihood Ratio Test (LRT) to compare a simple power-law model with a more flexible alternative, and then assess the goodness-of-fit of your chosen model.",
            "id": "4312862",
            "problem": "Consider a set of cities with population sizes denoted by $N_i$ and an urban quantity denoted by $Y_i$, where $i$ indexes cities. Urban scaling laws posit that, on average, the quantity $Y$ scales with population $N$ according to a power-law, while exhibiting multiplicative fluctuations around that mean. In order to perform principled model selection and goodness-of-fit assessment at an advanced-graduate level in complex systems and network science, you must formalize the following:\n\n1. Base assumptions grounded in definitions and well-tested observations:\n   - The canonical urban scaling hypothesis assumes a mean relationship $Y(N)$ that is a power law. In logarithmic form this is linear, and multiplicative log-normal fluctuations become additive Gaussian noise. Specifically, let $y_i = \\log Y_i$ and $x_i = \\log N_i$. Then the single-scaling (power-law) model posits $y_i = a + \\beta x_i + \\varepsilon_i$, where $\\varepsilon_i$ are independent and identically distributed Gaussian residuals with zero mean and variance $\\sigma^2$.\n   - To allow for curvature in scaling that may arise from network-structural or socio-economic transitions, consider an extended log-quadratic model $y_i = a + \\beta x_i + \\gamma x_i^2 + \\varepsilon_i$, with the same Gaussian residual assumption.\n   - Parameters in either model are to be estimated by Maximum Likelihood Estimation (MLE) under the Gaussian assumption.\n\n2. From first principles, select the most appropriate model and assess goodness-of-fit using:\n   - Akaike Information Criterion (AIC), defined for a model with $k$ free parameters and maximized log-likelihood $\\ell$.\n   - Bayesian Information Criterion (BIC), defined for a model with $k$ free parameters, sample size $n$, and maximized log-likelihood $\\ell$.\n   - The Likelihood Ratio Test (LRT) for nested models, comparing the single-scaling model to the log-quadratic model and reporting the $p$-value under the appropriate asymptotic distribution.\n   - A residual-based goodness-of-fit diagnostic: standardize residuals under the fitted model by the MLE of $\\sigma$, and apply the one-sample Kolmogorov-Smirnov (KS) test against the standard normal distribution to obtain a $p$-value. This tests the adequacy of the Gaussian residual assumption implied by multiplicative log-normal fluctuations.\n\n3. Implementation requirements:\n   - Perform MLE for both the single-scaling model and the log-quadratic model by solving the corresponding linear regression in $x_i = \\log N_i$ space with Gaussian residuals.\n   - Compute the maximized log-likelihood $\\ell$ for each model under the Gaussian assumption with the MLE residual variance and derive $k$ for AIC and BIC as follows: include all regression coefficients and the residual variance parameter $\\sigma^2$ in $k$. For the single-scaling model, $k=3$. For the log-quadratic model, $k=4$.\n   - Use the LRT between the single-scaling and log-quadratic models. The log-quadratic model nests the single-scaling model when $\\gamma = 0$. Report the LRT $p$-value using the asymptotic chi-square distribution for the difference in the number of regression coefficients, which is $1$ in this case.\n   - For goodness-of-fit, compute standardized residuals $z_i$ by dividing residuals by the MLE standard deviation and apply the Kolmogorov-Smirnov test against the standard normal distribution to obtain a $p$-value.\n\n4. Test suite and data generation:\n   - You must generate synthetic datasets deterministically without any random number generators, to ensure reproducibility. For each dataset, do the following:\n     - Let $n$ be the number of cities.\n     - Let $N_{\\min}$ and $N_{\\max}$ define the range of population sizes.\n     - Let $a$, $\\beta$, $\\gamma$ define the log-mean relationship $y_i = a + \\beta x_i + \\gamma x_i^2 + \\varepsilon_i$ with $x_i = \\log N_i$.\n     - Let $\\sigma$ define the scale of Gaussian residuals.\n     - Construct $x_i$ as $x_i$ linearly spaced between $\\log N_{\\min}$ and $\\log N_{\\max}$ for $i=1,\\dots,n$. Explicitly, $x_i$ are $n$ evenly spaced points on $[\\log N_{\\min}, \\log N_{\\max}]$.\n     - Construct the deterministic Gaussian residuals by choosing quantiles $q_i = \\frac{i - 0.5}{n}$ and setting $\\varepsilon_i = \\sigma \\Phi^{-1}(q_i)$, where $\\Phi^{-1}$ is the inverse cumulative distribution function of the standard normal distribution.\n     - Set $y_i = a + \\beta x_i + \\gamma x_i^2 + \\varepsilon_i$ and $Y_i = \\exp(y_i)$.\n   - Define four datasets (test cases), each specified by $(n, N_{\\min}, N_{\\max}, a, \\beta, \\gamma, \\sigma)$:\n     1. Case $1$: $n = 20$, $N_{\\min} = 10000$, $N_{\\max} = 10000000$, $a = 1.0$, $\\beta = 1.15$, $\\gamma = 0.0$, $\\sigma = 0.2$.\n     2. Case $2$: $n = 25$, $N_{\\min} = 5000$, $N_{\\max} = 20000000$, $a = 0.5$, $\\beta = 0.9$, $\\gamma = 0.06$, $\\sigma = 0.15$.\n     3. Case $3$: $n = 8$, $N_{\\min} = 20000$, $N_{\\max} = 2000000$, $a = 2.0$, $\\beta = 1.0$, $\\gamma = 0.0$, $\\sigma = 0.3$.\n     4. Case $4$: $n = 30$, $N_{\\min} = 8000$, $N_{\\max} = 30000000$, $a = 1.0$, $\\beta = 1.05$, $\\gamma = -0.02$, $\\sigma = 0.25$.\n\n5. Required outputs:\n   - For each dataset, compute and return four values in this order:\n     1. The AIC-based model selection indicator, with $1$ denoting the single-scaling (power-law) model and $2$ denoting the log-quadratic model.\n     2. The BIC-based selection indicator, with $1$ denoting the single-scaling model and $2$ denoting the log-quadratic model.\n     3. The LRT $p$-value comparing the single-scaling model against the log-quadratic model.\n     4. The KS $p$-value for the AIC-selected model’s standardized residuals against the standard normal distribution.\n   - The final output must aggregate the results for all four datasets into a single line. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$. Specifically, the output must contain $16$ entries corresponding to the four values for each of the $4$ test cases, ordered by test case. Integer indicators must be printed as integers, and each $p$-value must be printed as a decimal rounded to six places.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in computational statistics, specifically focused on model selection and goodness-of-fit for urban scaling laws. The premises are consistent with established literature in complex systems, and the tasks are specified with mathematical precision, including the models, estimation methods, test statistics, and data generation procedures. All terms are clearly defined, and the problem is self-contained.\n\nThe solution proceeds by first generating the required synthetic datasets and then applying a sequence of statistical analyses to each dataset as specified.\n\n### 1. Data Generation\nFor each of the four test cases, a synthetic dataset is constructed deterministically. The parameters for each case are given by the tuple $(n, N_{\\min}, N_{\\max}, a, \\beta, \\gamma, \\sigma)$, where $n$ is the number of cities, $N_{\\min}$ and $N_{\\max}$ define the population range, and $a, \\beta, \\gamma, \\sigma$ are the parameters of the generative model.\n\nThe independent variable, log-population $x_i = \\log N_i$, is constructed by creating $n$ linearly spaced points on the interval $[\\log N_{\\min}, \\log N_{\\max}]$.\n$$ x_i = \\log N_{\\min} + (i-1) \\frac{\\log N_{\\max} - \\log N_{\\min}}{n-1} \\quad \\text{for } i = 1, \\dots, n $$\nThe residuals $\\varepsilon_i$ are generated deterministically to follow a Gaussian distribution with mean $0$ and standard deviation $\\sigma$. This is achieved by using the inverse of the standard normal cumulative distribution function, $\\Phi^{-1}(q)$, also known as the quantile function. We define quantiles $q_i = \\frac{i-0.5}{n}$ for $i=1, \\dots, n$, and set the residuals as:\n$$ \\varepsilon_i = \\sigma \\Phi^{-1}(q_i) $$\nFinally, the dependent variable, log-quantity $y_i = \\log Y_i$, is generated using the specified log-quadratic model form:\n$$ y_i = a + \\beta x_i + \\gamma x_i^2 + \\varepsilon_i $$\n\n### 2. Model Fitting and Log-Likelihood\nTwo models are fitted to each synthetic dataset $(x_i, y_i)$:\n- **Model 1 (Single-Scaling):** $y_i = a_1 + \\beta_1 x_i + \\varepsilon_i$\n- **Model 2 (Log-Quadratic):** $y_i = a_2 + \\beta_2 x_i + \\gamma_2 x_i^2 + \\varepsilon_i$\n\nUnder the assumption of i.i.d. Gaussian residuals, Maximum Likelihood Estimation (MLE) for the regression coefficients $(a, \\beta, \\gamma)$ is equivalent to Ordinary Least Squares (OLS). For each model, we solve the linear system to find the coefficients that minimize the sum of squared residuals.\n\nLet the number of data points be $n$. For a given model, let the predicted values be $\\hat{y}_i$. The residuals are $\\hat{\\varepsilon}_i = y_i - \\hat{y}_i$. The MLE for the variance of the residuals is:\n$$ \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\varepsilon}_i^2 $$\nThe maximized log-likelihood, $\\ell_{\\max}$, for a model with $n$ data points and an MLE variance estimate of $\\hat{\\sigma}^2_{\\text{MLE}}$ is given by:\n$$ \\ell_{\\max} = - \\frac{n}{2} \\log(2\\pi) - \\frac{n}{2} \\log(\\hat{\\sigma}^2_{\\text{MLE}}) - \\frac{1}{2\\hat{\\sigma}^2_{\\text{MLE}}} \\sum_{i=1}^n \\hat{\\varepsilon}_i^2 = - \\frac{n}{2} \\left( \\log(2\\pi\\hat{\\sigma}^2_{\\text{MLE}}) + 1 \\right) $$\nThis calculation is performed for both Model 1 and Model 2, yielding $\\ell_{\\max,1}$ and $\\ell_{\\max,2}$, respectively.\n\n### 3. Model Selection\nWe use two information criteria, AIC and BIC, to select the better model. Both criteria penalize models for having more parameters. The model with the lower criterion value is preferred.\n\n- **Akaike Information Criterion (AIC):** $AIC = 2k - 2\\ell_{\\max}$\n- **Bayesian Information Criterion (BIC):** $BIC = k \\log(n) - 2\\ell_{\\max}$\n\nHere, $k$ is the total number of estimated parameters. As specified, $k$ includes the regression coefficients and the variance.\n- For Model 1, the parameters are $(a_1, \\beta_1, \\sigma_1^2)$, so $k_1 = 3$.\n- For Model 2, the parameters are $(a_2, \\beta_2, \\gamma_2, \\sigma_2^2)$, so $k_2 = 4$.\n\nAn indicator of $1$ is assigned if Model 1 is selected, and $2$ if Model 2 is selected. A tie is resolved in favor of the simpler model (Model 1).\n\n### 4. Likelihood Ratio Test (LRT)\nThe LRT is used to compare nested models. Model 1 is nested within Model 2, as Model 1 is a special case of Model 2 where $\\gamma=0$. The null hypothesis is $H_0: \\gamma=0$. The test statistic is:\n$$ \\lambda_{LR} = 2(\\ell_{\\max,2} - \\ell_{\\max,1}) $$\nUnder $H_0$, $\\lambda_{LR}$ asymptotically follows a chi-squared distribution, $\\chi^2(df)$, where the degrees of freedom $df$ is the difference in the number of free regression coefficients between the two models. Here, Model 2 has one additional coefficient ($\\gamma$), so $df=1$. The $p$-value is the probability of observing a test statistic as extreme or more extreme than the one calculated, assuming $H_0$ is true:\n$$ p_{\\text{LRT}} = P(\\chi^2_1 \\ge \\lambda_{LR}) $$\n\n### 5. Goodness-of-Fit: Kolmogorov-Smirnov (KS) Test\nThis test assesses whether the residuals from the fitted model are consistent with the assumed Gaussian distribution.\nFirst, the model selected by AIC is identified. Then, its residuals, $\\hat{\\varepsilon}_i$, are standardized using the MLE of the standard deviation, $\\hat{\\sigma}_{\\text{MLE}} = \\sqrt{\\hat{\\sigma}^2_{\\text{MLE}}}$:\n$$ z_i = \\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}_{\\text{MLE}}} $$\nThe one-sample Kolmogorov-Smirnov test is then applied to these standardized residuals $\\{z_i\\}$. It compares the empirical cumulative distribution function (ECDF) of the standardized residuals to the cumulative distribution function (CDF) of the standard normal distribution, $\\Phi(z)$. The test yields a $p$-value, $p_{\\text{KS}}$. A high $p$-value (e.g., $>0.05$) suggests that the data are not inconsistent with the null hypothesis that the residuals are drawn from a standard normal distribution, indicating a good fit.\n\nThe implementation of these steps for each test case will produce the four required outputs.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, chi2, kstest\n\ndef solve():\n    \"\"\"\n    Performs model selection and goodness-of-fit analysis for urban scaling laws\n    on four deterministically generated synthetic datasets.\n    \"\"\"\n\n    test_cases = [\n        # (n, N_min, N_max, a, beta, gamma, sigma)\n        (20, 10000, 10000000, 1.0, 1.15, 0.0, 0.2),\n        (25, 5000, 20000000, 0.5, 0.9, 0.06, 0.15),\n        (8, 20000, 2000000, 2.0, 1.0, 0.0, 0.3),\n        (30, 8000, 30000000, 1.0, 1.05, -0.02, 0.25),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        n, N_min, N_max, a_true, beta_true, gamma_true, sigma_true = params\n\n        # === 1. Data Generation ===\n        x_min = np.log(N_min)\n        x_max = np.log(N_max)\n        x = np.linspace(x_min, x_max, n)\n        \n        quantiles = (np.arange(1, n + 1) - 0.5) / n\n        eps = sigma_true * norm.ppf(quantiles)\n        \n        y = a_true + beta_true * x + gamma_true * x**2 + eps\n\n        # === 2. Model Fitting and Log-Likelihood ===\n\n        # Model 1: Single-Scaling (linear) y = a + beta*x\n        X1 = np.c_[np.ones(n), x]\n        k1 = 3 # Parameters: a, beta, sigma^2\n        coeffs1, sum_sq_res1, _, _ = np.linalg.lstsq(X1, y, rcond=None)\n        sigma2_1_hat = sum_sq_res1[0] / n\n        log_L1 = -n / 2 * (np.log(2 * np.pi * sigma2_1_hat) + 1)\n\n        # Model 2: Log-Quadratic y = a + beta*x + gamma*x^2\n        X2 = np.c_[np.ones(n), x, x**2]\n        k2 = 4 # Parameters: a, beta, gamma, sigma^2\n        coeffs2, sum_sq_res2, _, _ = np.linalg.lstsq(X2, y, rcond=None)\n        sigma2_2_hat = sum_sq_res2[0] / n\n        log_L2 = -n / 2 * (np.log(2 * np.pi * sigma2_2_hat) + 1)\n\n        # === 3. Model Selection (AIC/BIC) ===\n        aic1 = 2 * k1 - 2 * log_L1\n        aic2 = 2 * k2 - 2 * log_L2\n        aic_indicator = 1 if aic1 <= aic2 else 2\n\n        bic1 = k1 * np.log(n) - 2 * log_L1\n        bic2 = k2 * np.log(n) - 2 * log_L2\n        bic_indicator = 1 if bic1 <= bic2 else 2\n\n        # === 4. Likelihood Ratio Test (LRT) ===\n        # H0: single-scaling (gamma=0), H1: log-quadratic\n        lrt_stat = 2 * (log_L2 - log_L1)\n        # The test statistic must be non-negative\n        lrt_stat = max(0, lrt_stat)\n        lrt_p_value = chi2.sf(lrt_stat, df=1) # df = k2_coeffs - k1_coeffs = 3 - 2 = 1\n\n        # === 5. Goodness-of-Fit (KS Test) ===\n        if aic_indicator == 1:\n            y_pred = X1 @ coeffs1\n            residuals = y - y_pred\n            sigma_mle = np.sqrt(sigma2_1_hat)\n        else: # aic_indicator == 2\n            y_pred = X2 @ coeffs2\n            residuals = y - y_pred\n            sigma_mle = np.sqrt(sigma2_2_hat)\n\n        standardized_residuals = residuals / sigma_mle\n        ks_stat, ks_p_value = kstest(standardized_residuals, 'norm')\n\n        # === Aggregate and format results for the current case ===\n        all_results.extend([\n            aic_indicator,\n            bic_indicator,\n            f\"{lrt_p_value:.6f}\",\n            f\"{ks_p_value:.6f}\"\n        ])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Cities are not isolated points; they exist in a geographic space, and nearby cities often share economic, cultural, or environmental characteristics. This spatial dependence can introduce autocorrelation into the residuals of a scaling model, potentially biasing estimates of the scaling exponent $\\beta$ if ignored. This simulation-based practice demonstrates how to quantify the impact of spatial autocorrelation using Moran's $I$ statistic, a fundamental tool in spatial econometrics , providing you with the skills to diagnose and understand this critical feature of urban datasets.",
            "id": "4312852",
            "problem": "Consider the canonical urban scaling formulation in which a nonnegative urban indicator $Y_{i}$ for city $i \\in \\{1,\\dots,n\\}$ obeys a multiplicative law relative to its population $N_{i}$, namely $Y_{i} = a \\, N_{i}^{\\beta} \\, \\varepsilon_{i}$, where $a&gt;0$ is a constant, $\\beta$ is the scaling exponent of interest, and $\\varepsilon_{i}$ is a positive multiplicative disturbance. Taking logarithms yields $\\log Y_{i} = \\log a + \\beta \\log N_{i} + \\eta_{i}$, where $\\eta_{i} = \\log \\varepsilon_{i}$ is an additive disturbance. In many urban systems, disturbances may exhibit spatial dependence arising from unobserved regional factors or interaction processes. A standard representation for spatial dependence in the disturbances is the Spatial Error Model (SEM), in which $\\boldsymbol{\\eta}$ satisfies $(\\mathbf{I} - \\rho \\mathbf{W}) \\boldsymbol{\\eta} = \\mathbf{u}$, where $\\rho \\in (-1,1)$ controls the strength and sign of spatial autocorrelation, $\\mathbf{W}$ is a row-standardized spatial weight matrix, $\\mathbf{I}$ is the identity matrix, and $\\mathbf{u}$ is an independent and identically distributed zero-mean Gaussian vector with variance $\\sigma^{2}$ per component.\n\nYour task is to implement a program that, for a specified set of test cases, simulates synthetic urban data under the assumptions above, estimates the scaling exponent by ordinary least squares (OLS) from the log-log relation, quantifies spatial autocorrelation in the OLS residuals using Moran’s $I$, and reports the estimated exponent and diagnostics.\n\nUse the following fundamental base:\n- Urban scaling definition: $\\log Y_{i} = \\log a + \\beta \\log N_{i} + \\eta_{i}$.\n- Ordinary Least Squares (OLS): the estimator $\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$ for $\\mathbf{y} = \\log \\mathbf{Y}$ and $\\mathbf{X} = [\\mathbf{1}, \\log \\mathbf{N}]$, where $\\hat{\\boldsymbol{\\theta}} = [\\widehat{\\log a}, \\hat{\\beta}]^{\\top}$.\n- Moran’s $I$: $I = \\dfrac{n}{S_{0}} \\cdot \\dfrac{\\mathbf{e}^{\\top} \\mathbf{W} \\mathbf{e}}{\\mathbf{e}^{\\top} \\mathbf{e}}$, where $\\mathbf{e}$ are the OLS residuals and $S_{0} = \\sum_{i}\\sum_{j} w_{ij}$, with $w_{ij}$ the $(i,j)$ entry of $\\mathbf{W}$. Under the randomization null hypothesis of no spatial autocorrelation, the expected value is $\\mathbb{E}[I] = -\\dfrac{1}{n-1}$.\n- Spatial Error Model (SEM): $(\\mathbf{I} - \\rho \\mathbf{W}) \\boldsymbol{\\eta} = \\mathbf{u}$ with $\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} \\mathbf{I})$ and $|\\rho| &lt; 1$ to ensure that $(\\mathbf{I} - \\rho \\mathbf{W})$ is invertible for row-standardized $\\mathbf{W}$.\n\nYour program must implement the following procedure for each test case:\n1. Generate $n$ city locations $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ as independent samples from the unit square, i.e., $(x_{i}, y_{i}) \\sim \\text{Uniform}([0,1]^{2})$. Use the specified integer random seed per test case for all random draws in that case.\n2. Construct the $k$-nearest-neighbor spatial weight matrix $\\mathbf{W}$ over these locations as follows. For each city $i$, find the $k$ distinct cities $j \\neq i$ with the smallest Euclidean distance to $i$, set $w_{ij} = 1$ if $j$ is a $k$-nearest neighbor of $i$ and $w_{ij} = 0$ otherwise, and then row-standardize so that each nonzero row sums to $1$ (i.e., $w_{ij} \\leftarrow w_{ij} / \\sum_{j} w_{ij}$). This yields a row-stochastic $\\mathbf{W}$ with $S_{0} = \\sum_{i}\\sum_{j} w_{ij} = n$.\n3. Draw $\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I})$ and compute the spatially autocorrelated disturbance $\\boldsymbol{\\eta}$ by solving $(\\mathbf{I} - \\rho \\mathbf{W}) \\boldsymbol{\\eta} = \\mathbf{u}$ for $\\boldsymbol{\\eta}$.\n4. Generate independent city populations by first drawing $\\log N_{i} \\sim \\mathcal{N}(\\mu, \\tau^{2})$ with $\\mu = 11.0$ and $\\tau = 1.0$, and then setting $N_{i} = \\exp(\\log N_{i})$. This ensures a broad, realistic city size distribution. Assume independence between $\\log N_{i}$ and $\\boldsymbol{\\eta}$.\n5. Set $a = 1$ and form $Y_{i} = a \\, N_{i}^{\\beta} \\, \\exp(\\eta_{i})$. Then compute $\\mathbf{y} = \\log \\mathbf{Y}$ and $\\mathbf{X} = [\\mathbf{1}, \\log \\mathbf{N}]$. Estimate $\\hat{\\beta}$ by OLS. Compute the OLS residual vector $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\theta}}$.\n6. Compute Moran’s $I$ using the formula above and the expected value under the null $\\mathbb{E}[I] = -\\dfrac{1}{n-1}$. Report the estimated exponent $\\hat{\\beta}$, the bias $\\hat{\\beta} - \\beta$, the Moran’s $I$ of residuals, and the null expectation.\n\nDesign a test suite covering no autocorrelation, moderate positive autocorrelation, strong positive autocorrelation, negative autocorrelation, and a small-sample condition:\n- Test case $1$: $(n, k, \\beta, \\rho, \\sigma, \\text{seed}) = (64, 4, 1.15, 0.0, 0.2, 1)$.\n- Test case $2$: $(n, k, \\beta, \\rho, \\sigma, \\text{seed}) = (64, 4, 1.15, 0.6, 0.2, 2)$.\n- Test case $3$: $(n, k, \\beta, \\rho, \\sigma, \\text{seed}) = (64, 4, 0.85, 0.9, 0.3, 3)$.\n- Test case $4$: $(n, k, \\beta, \\rho, \\sigma, \\text{seed}) = (64, 4, 1.00, -0.5, 0.2, 4)$.\n- Test case $5$: $(n, k, \\beta, \\rho, \\sigma, \\text{seed}) = (16, 3, 1.20, 0.7, 0.5, 5)$.\n\nYour program must produce, for the five test cases in the order given, a single line of output containing a list of length $5$, where each element is a list of four floats: $[\\hat{\\beta}, \\hat{\\beta}-\\beta, I, \\mathbb{E}[I]]$, each rounded to $6$ decimal places. The final printed line must be exactly a single Python-style list of lists, for example $[[b_{1},\\Delta_{1},I_{1},E_{1}],[b_{2},\\Delta_{2},I_{2},E_{2}],\\dots]$, with no additional text.",
            "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, unambiguous, and all necessary parameters and procedures are explicitly defined. The models and methods proposed—urban scaling laws, the Spatial Error Model (SEM), Ordinary Least Squares (OLS), and Moran's $I$—are standard and appropriate for the task.\n\nThe objective is to conduct a simulation study to assess the OLS estimator of an urban scaling exponent in the presence of spatially autocorrelated disturbances. For each test case, we will generate synthetic data according to a specified process, estimate the model parameters, and compute a diagnostic for spatial autocorrelation.\n\nThe theoretical foundation is the urban scaling law, which posits a power-law relationship between an urban indicator $Y_i$ and the city population $N_i$:\n$$Y_{i} = a \\, N_{i}^{\\beta} \\, \\varepsilon_{i}$$\nHere, $a > 0$ is a normalization constant, $\\beta$ is the scaling exponent, and $\\varepsilon_{i}$ is a multiplicative disturbance term. To facilitate linear estimation, this equation is logarithmically transformed:\n$$\\log Y_{i} = \\log a + \\beta \\log N_{i} + \\eta_{i}$$\nwhere $\\eta_{i} = \\log \\varepsilon_{i}$ is now an additive disturbance. In vector notation, this is expressed as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\eta}$, where $\\mathbf{y} = \\log\\mathbf{Y}$, the design matrix is $\\mathbf{X} = [\\mathbf{1}, \\log\\mathbf{N}]$, and the parameter vector is $\\boldsymbol{\\theta} = [\\log a, \\beta]^{\\top}$.\n\nA key assumption of the problem is that the disturbances $\\eta_i$ are not independent but exhibit spatial autocorrelation. This is modeled using a Spatial Error Model (SEM), a cornerstone of spatial econometrics. The SEM defines the disturbance vector $\\boldsymbol{\\eta}$ through the relation:\n$$(\\mathbf{I} - \\rho \\mathbf{W}) \\boldsymbol{\\eta} = \\mathbf{u}$$\nwhere $\\mathbf{I}$ is the $n \\times n$ identity matrix, $\\rho$ is a scalar parameter controlling the strength and sign of the spatial autocorrelation, $\\mathbf{W}$ is an $n \\times n$ spatial weights matrix encoding the neighborhood structure of the cities, and $\\mathbf{u}$ is a vector of independent and identically distributed errors, specifically $\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I})$. The disturbance vector can thus be generated as $\\boldsymbol{\\eta} = (\\mathbf{I} - \\rho \\mathbf{W})^{-1} \\mathbf{u}$. The constraint $|\\rho| < 1$ is required to ensure the matrix $(\\mathbf{I} - \\rho \\mathbf{W})$ is invertible for a row-standardized $\\mathbf{W}$.\n\nThe spatial weights matrix $\\mathbf{W}$ is constructed based on the geographic locations of the cities. The procedural steps are:\n1.  Generate $n$ city locations $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ as independent random samples from a uniform distribution over the unit square $[0,1]^2$.\n2.  For each city $i$, identify its $k$ nearest neighbors based on Euclidean distance.\n3.  Construct an initial binary matrix where the entry $w_{ij}$ is $1$ if city $j$ is one of the $k$ nearest neighbors of city $i$, and $0$ otherwise. The diagonal elements $w_{ii}$ are always $0$.\n4.  Row-standardize this matrix by dividing each element $w_{ij}$ by its row sum $\\sum_j w_{ij}$. The resulting matrix $\\mathbf{W}$ is row-stochastic, meaning each row sums to $1$. Consequently, the sum of all its elements, $S_0 = \\sum_{i}\\sum_{j} w_{ij}$, is equal to $n$.\n\nThe full simulation and estimation procedure for each test case is as follows:\n1.  A specific seed is used to initialize a pseudo-random number generator for reproducibility.\n2.  City locations are generated and the corresponding row-standardized $k$-nearest neighbor matrix $\\mathbf{W}$ is constructed.\n3.  The i.i.d. error vector $\\mathbf{u}$ is drawn from $\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$. The spatially correlated error vector $\\boldsymbol{\\eta}$ is then computed by solving the linear system $(\\mathbf{I} - \\rho \\mathbf{W})\\boldsymbol{\\eta} = \\mathbf{u}$.\n4.  City populations $N_i$ are generated from a log-normal distribution, specified by drawing $\\log N_i$ from $\\mathcal{N}(\\mu=11.0, \\tau^2=1.0^2)$.\n5.  The dependent variable $\\mathbf{y} = \\log\\mathbf{Y}$ is created using the log-linear model, with parameters $a=1$ (so $\\log a=0$) and the given $\\beta$: $\\mathbf{y} = \\beta\\log\\mathbf{N} + \\boldsymbol{\\eta}$.\n6.  The parameters are estimated using Ordinary Least Squares (OLS). The OLS estimator for $\\boldsymbol{\\theta}$ is given by the formula $\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$. The estimated scaling exponent, $\\hat{\\beta}$, is the second element of $\\hat{\\boldsymbol{\\theta}}$.\n7.  The OLS residuals, $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\theta}}$, are calculated. These residuals are then tested for spatial autocorrelation using Moran’s $I$. The formula for Moran's $I$ is $I = \\dfrac{n}{S_{0}} \\cdot \\dfrac{\\mathbf{e}^{\\top} \\mathbf{W} \\mathbf{e}}{\\mathbf{e}^{\\top} \\mathbf{e}}$. Since $\\mathbf{W}$ is constructed to be row-stochastic, $S_0 = n$, and the formula simplifies to $I = \\dfrac{\\mathbf{e}^{\\top} \\mathbf{W} \\mathbf{e}}{\\mathbf{e}^{\\top} \\mathbf{e}}$.\n8.  The calculated value of $I$ is compared to its expected value under the null hypothesis of no spatial autocorrelation, which is $\\mathbb{E}[I] = -1/(n-1)$.\n9.  Finally, four values are reported: the estimated exponent $\\hat{\\beta}$, its bias $\\hat{\\beta} - \\beta$, the computed Moran's $I$ for the residuals, and the null expectation $\\mathbb{E}[I]$.\n\nThis entire process is systematically implemented in code to run the specified test cases and produce the required outputs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef calculate_for_case(params):\n    \"\"\"\n    Runs a single simulation case for the urban scaling model with spatial errors.\n    \"\"\"\n    n, k, beta, rho, sigma, seed = params\n    mu_log_N, tau_log_N = 11.0, 1.0\n    a = 1.0  # As per problem, log(a) = 0\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate n city locations from Uniform([0,1]^2)\n    locations = rng.uniform(0.0, 1.0, size=(n, 2))\n\n    # 2. Construct k-nearest-neighbor spatial weight matrix W\n    dist_matrix = cdist(locations, locations, 'euclidean')\n    np.fill_diagonal(dist_matrix, np.inf)\n\n    # Get indices of k nearest neighbors for each city\n    neighbor_indices = np.argpartition(dist_matrix, k, axis=1)[:, :k]\n\n    # Create binary weight matrix\n    W_unstd = np.zeros((n, n))\n    np.put_along_axis(W_unstd, neighbor_indices, 1, axis=1)\n\n    # Row-standardize the matrix\n    row_sums = W_unstd.sum(axis=1, keepdims=True)\n    W = np.divide(W_unstd, row_sums, out=np.zeros_like(W_unstd), where=row_sums != 0)\n\n    # 3. Generate spatially autocorrelated disturbance eta\n    u = rng.normal(loc=0.0, scale=sigma, size=n)\n    A = np.identity(n) - rho * W\n    eta = np.linalg.solve(A, u)\n\n    # 4. Generate city populations N\n    log_N = rng.normal(loc=mu_log_N, scale=tau_log_N, size=n)\n\n    # 5. Formulate log-linear model and estimate by OLS\n    # log(Y) = log(a) + beta * log(N) + eta. Since a=1, log(a)=0.\n    y = beta * log_N + eta\n    \n    # Construct design matrix X = [1, log(N)]\n    X = np.vstack([np.ones(n), log_N]).T\n\n    # Compute OLS estimator: theta_hat = (X'X)^-1 X'y\n    try:\n        XTX_inv = np.linalg.inv(X.T @ X)\n        theta_hat = XTX_inv @ X.T @ y\n    except np.linalg.LinAlgError:\n        # Fallback for singular matrix, although not expected here\n        theta_hat, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n\n    beta_hat = theta_hat[1]\n\n    # Compute OLS residuals\n    e = y - X @ theta_hat\n\n    # 6. Compute Moran’s I\n    # For row-standardized W, S0=n, so the formula is I = (e'We) / (e'e)\n    numerator_I = e.T @ W @ e\n    denominator_I = e.T @ e\n    \n    # Handle case where residuals are all zero\n    if denominator_I == 0.0:\n        moran_I = 0.0\n    else:\n        moran_I = numerator_I / denominator_I\n\n    # Expected value of Moran's I under the null hypothesis\n    E_I = -1.0 / (n - 1)\n    \n    # Bias of the exponent estimate\n    bias = beta_hat - beta\n\n    return [beta_hat, bias, moran_I, E_I]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, k, beta, rho, sigma, seed)\n        (64, 4, 1.15, 0.0, 0.2, 1),\n        (64, 4, 1.15, 0.6, 0.2, 2),\n        (64, 4, 0.85, 0.9, 0.3, 3),\n        (64, 4, 1.00, -0.5, 0.2, 4),\n        (16, 3, 1.20, 0.7, 0.5, 5),\n    ]\n\n    results = []\n    for case in test_cases:\n        result_metrics = calculate_for_case(case)\n        results.append(result_metrics)\n\n    # Format the results to 6 decimal places per the problem specification.\n    # The final print statement must match the required format exactly.\n    formatted_results = [[round(val, 6) for val in res] for res in results]\n    print(f\"{formatted_results}\")\n\nsolve()\n```"
        }
    ]
}