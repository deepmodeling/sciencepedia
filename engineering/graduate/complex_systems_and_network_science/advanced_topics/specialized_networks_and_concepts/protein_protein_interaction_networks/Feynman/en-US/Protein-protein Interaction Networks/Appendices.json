{
    "hands_on_practices": [
        {
            "introduction": "The analysis of any network begins with its most fundamental properties. This exercise grounds the abstract concept of a protein-protein interaction network in its concrete matrix representation. By calculating node degree, the simplest measure of connectivity, and relating it to hypothetical experimental data, you will confront a critical real-world challenge: distinguishing true biological properties from artifacts like sampling bias. This practice  develops both computational skill and the critical mindset required to interpret high-throughput biological data.",
            "id": "4381240",
            "problem": "Consider a curated, undirected, unweighted Protein-Protein Interaction (PPI) network of $8$ proteins labeled $P_1,\\dots,P_8$, represented by the symmetric adjacency matrix $A$ with entries $A_{ij} \\in \\{0,1\\}$ and $A_{ii}=0$:\n$$\nA=\\begin{pmatrix}\n0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\\\\n1 & 1 & 0 & 0 & 1 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 0 & 1 & 1 \\\\\n0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 1 & 0 & 0 & 0\n\\end{pmatrix}.\n$$\nStarting from standard graph-theoretic definitions appropriate for undirected simple graphs in systems biomedicine, define the node degree $k_i$ in terms of the entries of $A$ and compute the degree sequence $\\big(k_1,\\dots,k_8\\big)$ for this network.\n\nTo study experimental coverage bias in PPI mapping (for example, Affinity Purification followed by Mass Spectrometry (AP-MS) assays), suppose each protein $i$ has a measured coverage factor $c_i$ (dimensionless), reflecting the relative number of independent bait experiments performed for that protein, given by the vector\n$$\n\\mathbf{c}=\\big(1.2,\\ 1.8,\\ 2.2,\\ 1.3,\\ 1.7,\\ 1.4,\\ 0.9,\\ 0.8\\big).\n$$\nUsing the base definition of the Pearson correlation coefficient between two real-valued variables indexed by nodes, compute the correlation between the degree sequence $\\big(k_1,\\dots,k_8\\big)$ and the coverage vector $\\mathbf{c}$. Round the correlation to four significant figures. Explain, from first principles about detection probability and sampling, why higher $c_i$ tends to inflate observed degree in PPI networks and under what experimental conditions the degree–coverage correlation could reflect measurement bias rather than true biological connectivity.\n\nExpress the final answer as a single row matrix containing the degree sequence entries $k_1,\\dots,k_8$ followed by the rounded Pearson correlation coefficient (dimensionless). No units are required in the final answer.",
            "solution": "The problem is scientifically grounded, well-posed, and objective. All provided data and definitions are self-contained and consistent. The adjacency matrix $A$ is properly defined for an undirected, unweighted simple graph, being symmetric with a zero diagonal. The tasks involve standard, well-defined calculations (node degree, Pearson correlation) and a conceptual explanation based on established principles of experimental biology and statistics. Therefore, the problem is deemed valid and a solution will be provided.\n\nFirst, we define and compute the degree sequence for the given Protein-Protein Interaction (PPI) network. The network has $N=8$ proteins, and its topology is described by the $8 \\times 8$ adjacency matrix $A$. For an undirected, unweighted graph, the degree $k_i$ of a node $i$ is the number of edges connected to it. In terms of the adjacency matrix $A$, where $A_{ij}=1$ if an edge exists between nodes $i$ and $j$ and $A_{ij}=0$ otherwise, the degree $k_i$ is the sum of the entries in the $i$-th row (or, due to symmetry, the $i$-th column).\n$$k_i = \\sum_{j=1}^{N} A_{ij}$$\nUsing this definition, we compute the degree for each protein $P_1, \\dots, P_8$ by summing the rows of the given matrix $A$:\n$k_1 = 0+1+1+0+1+0+0+0 = 3$\n$k_2 = 1+0+1+1+0+1+0+0 = 4$\n$k_3 = 1+1+0+0+1+1+1+0 = 5$\n$k_4 = 0+1+0+0+0+1+0+1 = 3$\n$k_5 = 1+0+1+0+0+0+1+1 = 4$\n$k_6 = 0+1+1+1+0+0+0+0 = 3$\n$k_7 = 0+0+1+0+1+0+0+0 = 2$\n$k_8 = 0+0+0+1+1+0+0+0 = 2$\nThe resulting degree sequence is the vector $\\mathbf{k} = \\big(k_1, k_2, k_3, k_4, k_5, k_6, k_7, k_8\\big) = \\big(3, 4, 5, 3, 4, 3, 2, 2\\big)$.\n\nNext, we compute the Pearson correlation coefficient between the degree sequence $\\mathbf{k}$ and the given coverage vector $\\mathbf{c} = \\big(1.2, 1.8, 2.2, 1.3, 1.7, 1.4, 0.9, 0.8\\big)$. The Pearson correlation coefficient $r$ for two sets of $n$ measurements $X = (x_1, \\dots, x_n)$ and $Y = (y_1, \\dots, y_n)$ is defined as:\n$$r_{XY} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means, $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$ and $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$.\nIn our case, $n=8$, $X = \\mathbf{k}$, and $Y = \\mathbf{c}$. We first calculate the means:\n$$\\bar{k} = \\frac{1}{8} \\sum_{i=1}^{8} k_i = \\frac{3+4+5+3+4+3+2+2}{8} = \\frac{26}{8} = 3.25$$\n$$\\bar{c} = \\frac{1}{8} \\sum_{i=1}^{8} c_i = \\frac{1.2+1.8+2.2+1.3+1.7+1.4+0.9+0.8}{8} = \\frac{11.3}{8} = 1.4125$$\nNow we calculate the three required summations:\nThe sum of products of deviations (the numerator's covariance term):\n$$ \\sum_{i=1}^{8} (k_i - \\bar{k})(c_i - \\bar{c}) = (3 - 3.25)(1.2 - 1.4125) + (4 - 3.25)(1.8 - 1.4125) + \\dots + (2 - 3.25)(0.8 - 1.4125) $$\n$$ = (-0.25)(-0.2125) + (0.75)(0.3875) + (1.75)(0.7875) + (-0.25)(-0.1125) + (0.75)(0.2875) + (-0.25)(-0.0125) + (-1.25)(-0.5125) + (-1.25)(-0.6125) $$\n$$ = 0.053125 + 0.290625 + 1.378125 + 0.028125 + 0.215625 + 0.003125 + 0.640625 + 0.765625 = 3.375 $$\nThe sum of squared deviations for $k$:\n$$ \\sum_{i=1}^{8} (k_i - \\bar{k})^2 = (-0.25)^2 + (0.75)^2 + (1.75)^2 + (-0.25)^2 + (0.75)^2 + (-0.25)^2 + (-1.25)^2 + (-1.25)^2 $$\n$$ = 0.0625 + 0.5625 + 3.0625 + 0.0625 + 0.5625 + 0.0625 + 1.5625 + 1.5625 = 7.5 $$\nThe sum of squared deviations for $c$:\n$$ \\sum_{i=1}^{8} (c_i - \\bar{c})^2 = (-0.2125)^2 + (0.3875)^2 + (0.7875)^2 + (-0.1125)^2 + (0.2875)^2 + (-0.0125)^2 + (-0.5125)^2 + (-0.6125)^2 $$\n$$ = 0.04515625 + 0.15015625 + 0.62015625 + 0.01265625 + 0.08265625 + 0.00015625 + 0.26265625 + 0.37515625 = 1.54875 $$\nSubstituting these values into the correlation formula:\n$$r = \\frac{3.375}{\\sqrt{7.5} \\sqrt{1.54875}} = \\frac{3.375}{\\sqrt{11.615625}} \\approx \\frac{3.375}{3.4081703} \\approx 0.990267$$\nRounding to four significant figures, the correlation is $r \\approx 0.9903$.\n\nFinally, we explain why a higher coverage factor $c_i$ tends to inflate the observed degree $k_i$ in PPI networks.\nIn techniques like Affinity Purification-Mass Spectrometry (AP-MS), a specific protein (the \"bait\", $P_i$) is used to pull down its interaction partners (the \"prey\"), which are then identified. The coverage factor $c_i$ is proportional to the number of times $P_i$ has been used as a bait in experiments.\nThe detection of any given protein-protein interaction is a stochastic process. In a single experiment, a true interaction may not be detected due to various factors (e.g., low-affinity binding, low protein expression, technical limitations of mass spectrometry). The probability of detecting a specific true interaction in a single experiment is less than $1$, let's call it $p_{\\text{detection}}$.\nIf a protein $P_i$ is used as a bait in $M_i$ independent experiments (where $M_i$ is proportional to $c_i$), the probability of *failing* to detect a particular true partner in all $M_i$ experiments is $(1 - p_{\\text{detection}})^{M_i}$. The probability of successfully detecting that partner at least once is therefore $1 - (1 - p_{\\text{detection}})^{M_i}$. This detection probability is a monotonically increasing function of $M_i$.\nConsequently, proteins that are used as baits more frequently (higher $c_i$) have a higher chance of their true interaction partners being detected. This systematically inflates their *observed* degree $k_i$ compared to proteins that are studied less frequently. A protein with a low $c_i$ may have a high true degree, but many of its interactions might remain undiscovered, leading to a low observed degree.\nThis correlation between degree and coverage reflects *measurement bias* under the condition that the experimental effort (represented by $\\mathbf{c}$) is not uniformly distributed across all proteins and is not a perfect representation of the proteins' true biological connectivity. If researchers, for instance, choose to study \"popular\" proteins (e.g., those already linked to a disease) more heavily, the vector $\\mathbf{c}$ becomes skewed. The resulting high correlation between observed degree and coverage is then an artifact of this non-uniform sampling. It conflates the true biological signal (if any) with a strong systematic bias, making it impossible to conclude from the observed data alone that proteins with higher coverage are truly more-connected hubs in the biological system. The bias is the portion of the observed correlation induced by the sampling process itself, independent of the underlying biology.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 3 & 4 & 5 & 3 & 4 & 3 & 2 & 2 & 0.9903 \\end{pmatrix} } $$"
        },
        {
            "introduction": "While a protein's degree describes its local connectivity, its importance often stems from its global position within the network. This practice moves beyond local measures to explore a protein's role in mediating communication across the entire system. By calculating the betweenness centrality in a weighted network, you will learn a quantitative method to identify \"bottleneck\" proteins that may bridge distinct functional modules or control information flow. This exercise  provides deep insight into network robustness, redundancy, and the structural basis of a protein's systemic function.",
            "id": "4298718",
            "problem": "Consider a Protein-Protein Interaction Network (PPIN), defined as an undirected, weighted graph where proteins are nodes and interactions are edges with positive weights representing effective path costs inversely related to interaction affinity (lower weight indicates higher affinity). Let the network have nodes $A$, $B$, $C$, $D$, $E$, and $F$. The set of undirected weighted edges is\n$\\{(A,B,1), (A,C,1), (B,C,1), (C,D,1), (D,E,1), (D,F,1), (E,F,1), (A,D,2), (B,D,2), (C,E,2)\\}$,\nwhere each tuple denotes $(\\text{protein}_1,\\ \\text{protein}_2,\\ \\text{weight})$ and all weights are strictly positive. Shortest paths are defined with respect to the sum of weights along a path.\n\nUsing the standard, unnormalized definition of betweenness centrality employed in network science, compute the betweenness centrality of protein $C$, taking shortest paths over unordered pairs of distinct proteins that exclude $C$ as endpoints. In addition, explain—based on first principles of shortest-path structure—how removing protein $C$ affects the flow along shortest paths (i.e., the redundancy and routing of geodesic paths) between the remaining proteins.\n\nReport only the betweenness centrality of protein $C$ as a single reduced fraction with no units. No rounding is required.",
            "solution": "The problem is evaluated as valid. It is scientifically grounded in network theory, well-posed with a complete and consistent definition, and stated objectively. The solution process may proceed.\n\nThe task is twofold: first, to compute the unnormalized betweenness centrality of protein $C$, denoted $B_C$; and second, to explain the structural impact of removing protein $C$ on the shortest paths within the network.\n\n**Part 1: Computation of Betweenness Centrality**\n\nThe betweenness centrality of a node $v$, $B(v)$, is defined as the sum of the fractions of all-pairs shortest paths that pass through $v$. For an undirected graph, the standard formula is:\n$$\nB(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n$$\nwhere the sum is over all unordered pairs of distinct nodes $\\{s, t\\}$ in the network, not including $v$. Here, $\\sigma_{st}$ is the total number of shortest paths (geodesics) between node $s$ and node $t$, and $\\sigma_{st}(v)$ is the number of those shortest paths that pass through node $v$.\n\nThe set of nodes is $V = \\{A, B, C, D, E, F\\}$. We must calculate $B_C$ by considering all unordered pairs of nodes from the set $V \\setminus \\{C\\} = \\{A, B, D, E, F\\}$. The number of such pairs is $\\binom{5}{2} = 10$. Let us analyze the shortest paths for each pair.\n\nThe network edges and weights are:\n$w(A,B) = 1$, $w(A,C) = 1$, $w(B,C) = 1$, $w(C,D) = 1$, $w(D,E) = 1$, $w(D,F) = 1$, $w(E,F) = 1$, $w(A,D) = 2$, $w(B,D) = 2$, $w(C,E) = 2$.\n\n1.  **Pair $\\{A, B\\}$**: The shortest path is the direct edge $A-B$ with weight $1$. The path $A-C-B$ has weight $1+1=2$. Thus, $\\sigma_{AB} = 1$, and this path does not pass through $C$. The contribution to $B_C$ is $\\frac{0}{1} = 0$.\n\n2.  **Pair $\\{D, E\\}$**: The shortest path is the direct edge $D-E$ with weight $1$. The path $D-F-E$ has weight $1+1=2$. The path $D-C-E$ has weight $1+2=3$. Thus, $\\sigma_{DE} = 1$, and it does not pass through $C$. Contribution is $0$.\n\n3.  **Pair $\\{D, F\\}$**: The shortest path is the direct edge $D-F$ with weight $1$. The path $D-E-F$ has weight $1+1=2$. Thus, $\\sigma_{DF} = 1$, and it does not pass through $C$. Contribution is $0$.\n\n4.  **Pair $\\{E, F\\}$**: The shortest path is the direct edge $E-F$ with weight $1$. The path $E-D-F$ has weight $1+1=2$. Thus, $\\sigma_{EF} = 1$, and it does not pass through $C$. Contribution is $0$.\n\n5.  **Pair $\\{A, D\\}$**:\n    *   Path $A-C-D$: weight $w(A,C) + w(C,D) = 1+1=2$.\n    *   Path $A-D$: weight $w(A,D) = 2$.\n    *   Path $A-B-D$: weight $w(A,B) + w(B,D) = 1+2=3$.\n    There are two shortest paths of length $2$: $A-C-D$ and $A-D$. Thus, $\\sigma_{AD} = 2$. One of these paths, $A-C-D$, passes through $C$, so $\\sigma_{AD}(C) = 1$. The contribution is $\\frac{1}{2}$.\n\n6.  **Pair $\\{B, D\\}$**: By symmetry with pair $\\{A,D\\}$ (nodes $A$ and $B$ are structurally equivalent with respect to the rest of the network), we find two shortest paths of length $2$: $B-C-D$ and $B-D$. Thus, $\\sigma_{BD} = 2$ and $\\sigma_{BD}(C) = 1$. The contribution is $\\frac{1}{2}$.\n\n7.  **Pair $\\{A, E\\}$**:\n    *   Path $A-D-E$: weight $w(A,D) + w(D,E) = 2+1=3$.\n    *   Path $A-C-D-E$: weight $w(A,C) + w(C,D) + w(D,E) = 1+1+1=3$.\n    *   Path $A-C-E$: weight $w(A,C) + w(C,E) = 1+2=3$.\n    *   Path $A-B-D-E$: weight $1+2+1=4$.\n    There are three shortest paths of length $3$. Thus, $\\sigma_{AE} = 3$. Two of these, $A-C-D-E$ and $A-C-E$, pass through $C$. Thus, $\\sigma_{AE}(C) = 2$. The contribution is $\\frac{2}{3}$.\n\n8.  **Pair $\\{B, E\\}$**: By symmetry with pair $\\{A,E\\}$, there are three shortest paths of length $3$: $B-D-E$, $B-C-D-E$, and $B-C-E$. Thus, $\\sigma_{BE} = 3$. Two of these pass through $C$, so $\\sigma_{BE}(C) = 2$. The contribution is $\\frac{2}{3}$.\n\n9.  **Pair $\\{A, F\\}$**:\n    *   Path $A-D-F$: weight $w(A,D) + w(D,F) = 2+1=3$.\n    *   Path $A-C-D-F$: weight $w(A,C) + w(C,D) + w(D,F) = 1+1+1=3$.\n    *   Path $A-C-E-F$: weight $1+2+1=4$.\n    There are two shortest paths of length $3$: $A-D-F$ and $A-C-D-F$. Thus, $\\sigma_{AF} = 2$. One path passes through $C$, so $\\sigma_{AF}(C) = 1$. The contribution is $\\frac{1}{2}$.\n\n10. **Pair $\\{B, F\\}$**: By symmetry with pair $\\{A,F\\}$, there are two shortest paths of length $3$: $B-D-F$ and $B-C-D-F$. Thus, $\\sigma_{BF} = 2$. One path passes through $C$, so $\\sigma_{BF}(C) = 1$. The contribution is $\\frac{1}{2}$.\n\nNow, we sum all non-zero contributions to find $B_C$:\n$$\nB_C = \\sum_{\\{s,t\\} \\subset V \\setminus \\{C\\}} \\frac{\\sigma_{st}(C)}{\\sigma_{st}} = 0 + 0 + 0 + 0 + \\frac{1}{2} + \\frac{1}{2} + \\frac{2}{3} + \\frac{2}{3} + \\frac{1}{2} + \\frac{1}{2}\n$$\n$$\nB_C = 4 \\times \\frac{1}{2} + 2 \\times \\frac{2}{3} = 2 + \\frac{4}{3} = \\frac{6}{3} + \\frac{4}{3} = \\frac{10}{3}\n$$\n\n**Part 2: Effect of Removing Protein C**\n\nRemoving protein $C$ also removes all edges incident to it: $(A,C)$, $(B,C)$, $(C,D)$, and $(C,E)$. This fundamentally alters the network's topology and the available paths for information or signal flow. The analysis must be based on the first principles of shortest-path structure: the set of shortest paths for any given pair of nodes is determined by finding all paths that achieve the minimum possible sum of edge weights. Removing a node eliminates any path that includes it, forcing a re-evaluation of shortest paths among the remaining alternatives.\n\nThe primary effect of removing $C$ is on paths connecting the sub-network involving nodes $\\{A,B\\}$ to the sub-network involving nodes $\\{D,E,F\\}$. For pairs within these sub-networks (e.g., $\\{A,B\\}$ or $\\{D,E\\}$), the shortest paths are unaffected as they do not involve $C$. Let us examine the pairs whose geodesic paths were mediated by $C$:\n\n*   **Pairs $\\{A,D\\}$ and $\\{B,D\\}$**: Before removal, the shortest path length was $2$, and there were two redundant paths (e.g., $A-D$ and $A-C-D$). After removing $C$, the paths $A-C-D$ and $B-C-D$ are eliminated. For both pairs, only a single shortest path remains ($A-D$ and $B-D$, respectively), both still of length $2$. The effect is a **complete loss of redundancy** in the shortest connections between $\\{A,B\\}$ and $D$, without an increase in path length.\n\n*   **Pairs $\\{A,E\\}$, $\\{B,E\\}$, $\\{A,F\\}$, and $\\{B,F\\}$**:\n    *   For $\\{A,E\\}$, removing $C$ eliminates two of the three shortest paths ($A-C-D-E$ and $A-C-E$). The sole remaining shortest path is $A-D-E$, with its length of $3$ becoming the new unique geodesic distance.\n    *   For $\\{A,F\\}$, removing $C$ eliminates one of the two shortest paths ($A-C-D-F$). The sole remaining shortest path is $A-D-F$, with its length of $3$ becoming the new unique geodesic distance.\n    *   The same logic applies symmetrically to pairs $\\{B,E\\}$ and $\\{B,F\\}$.\n\nIn summary, the removal of protein $C$ has the following consequences for shortest-path routing:\n\n1.  **Loss of Redundancy**: For every pair of proteins $(s,t)$ where $s \\in \\{A,B\\}$ and $t \\in \\{D,E,F\\}$, the number of distinct shortest paths, $\\sigma_{st}$, is strictly reduced. This signifies a decrease in the network's robustness. The flow, which was previously distributed across multiple geodesic paths, is now concentrated onto fewer, or in these cases, single routes.\n\n2.  **Increased Centrality of Other Nodes**: The rerouting of flow makes other nodes and edges more critical. Specifically, all shortest paths from $\\{A,B\\}$ to $\\{D,E,F\\}$ must now pass through node $D$ and utilize the edges $(A,D)$ and $(B,D)$. The betweenness centrality of node $D$ and the edge betweenness of $(A,D)$ and $(B,D)$ would increase significantly. Protein $D$ becomes a critical bottleneck or articulation point for communication between these two parts of the network.\n\n3.  **No Increase in Path Distance**: For this specific network configuration, the removal of $C$ does not increase the shortest path *length* for any pair of remaining nodes. This is a crucial observation: protein $C$ provided alternative routes that were exactly as efficient (in terms of path weight) as the best non-$C$ paths. It functioned as a source of parallel, equally optimal pathways rather than as a \"shortcut.\" If the weights had been different (e.g., if $w(C,D)$ were less than $1$), its removal could have increased path distances.\n\nFrom the first principle of shortest-path computation, removing node $C$ prunes the set of candidate paths. For the affected pairs, this pruning removed paths that were members of the set of minimum-weight paths, thereby reducing the size of this set ($\\sigma_{st}$). Since at least one minimum-weight path remained that did not involve $C$, the shortest path distance itself did not change.",
            "answer": "$$\\boxed{\\frac{10}{3}}$$"
        },
        {
            "introduction": "The networks we analyze are models of reality, and their predictive power depends on how well they are constructed. The edges in a PPI network are not all equal; their biological significance varies. This exercise demonstrates how to build a more realistic and informative network model by integrating diverse biophysical data. You will implement a scoring function  that translates structural and energetic properties into a quantitative confidence weight, transforming a simple binary graph into a nuanced, weighted network that better reflects the underlying molecular biology.",
            "id": "2423170",
            "problem": "You are given a graph where each node represents a protein and each edge represents a putative interaction between two proteins in a Protein-Protein Interaction (PPI) network. For each interaction edge, you are provided with five physical and structural descriptors derived from Three-Dimensional (3D) structural data: interface area, binding free energy, number of interfacial hydrogen bonds, fraction of hydrophobic residues at the interface, and an average atomic displacement parameter (B-factor). Using these descriptors, define a dimensionless confidence weight for each interaction that increases with larger and more stable interfaces.\n\nDefine the following dimensionless sub-scores for an interaction with interface area $A$ (in $\\text{Å}^2$), binding free energy $\\Delta G$ (in kcal/mol), number of hydrogen bonds $h$ (unitless), hydrophobic fraction $f_h$ (a fraction in $\\left[0,1\\right]$), and average B-factor $B$ (in $\\text{Å}^2$):\n- Area sub-score: $S_A = \\min\\!\\left(1, \\dfrac{A}{A_0}\\right)$ with $A_0 = 2000$ $\\text{Å}^2$.\n- Free energy sub-score: $S_G = \\dfrac{1}{1 + \\exp\\!\\left(\\beta \\left(\\Delta G - \\Delta G_0\\right)\\right)}$ with $\\Delta G_0 = -6$ kcal/mol and $\\beta = 0.5$ (per kcal/mol).\n- Hydrogen bond sub-score: $S_H = \\tanh\\!\\left(\\dfrac{h}{h_0}\\right)$ with $h_0 = 10$.\n- Hydrophobicity sub-score: $S_F = f_h$.\n- Flexibility sub-score: $S_B = \\dfrac{1}{1 + \\dfrac{B}{B_0}}$ with $B_0 = 30$ $\\text{Å}^2$.\n\nDefine the final confidence weight as the geometric mean of the five sub-scores:\n$$\nw \\;=\\; \\left(S_A \\cdot S_G \\cdot S_H \\cdot S_F \\cdot S_B\\right)^{1/5}.\n$$\n\nAll inputs must use the units specified above. The output is unitless.\n\nImplement a program that computes $w$ for each of the following test cases, each given as a $5$-tuple $(A,\\ \\Delta G,\\ h,\\ f_h,\\ B)$:\n1. $(1800,\\ -10,\\ 12,\\ 0.6,\\ 15)$\n2. $(400,\\ -2,\\ 2,\\ 0.3,\\ 50)$\n3. $(0,\\ -5,\\ 5,\\ 0.5,\\ 20)$\n4. $(2500,\\ -5,\\ 5,\\ 0.8,\\ 60)$\n5. $(700,\\ -7,\\ 20,\\ 0.4,\\ 10)$\n6. $(800,\\ -15,\\ 8,\\ 1.0,\\ 25)$\n\nThese cases are chosen to cover a typical strong interaction, a weak interaction, a zero-area boundary case, a saturation boundary for area with high flexibility, a case with many hydrogen bonds and low flexibility but small area, and a case with extremely favorable free energy and maximal hydrophobic fraction.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each weight rounded to exactly six digits after the decimal point (for example, \"[0.123456,0.654321,0.500000,0.000000,0.999999,0.314159]\").",
            "solution": "The problem statement is valid. It presents a computationally tractable and biophysically sound model for assessing the confidence of protein-protein interaction evidence. The use of multiple, orthogonal descriptors derived from $3$D structural data is a standard and robust practice in structural bioinformatics. The provided scoring functions and constants are consistent with established principles. We will proceed with a systematic analysis of the confidence weight $w$.\n\nThe model defines a composite confidence weight, $w$, as the geometric mean of $5$ dimensionless sub-scores, each corresponding to a key biophysical property of the protein-protein interface.\n\n$1$. **Area Sub-score ($S_A$)**: $S_A = \\min\\!\\left(1, \\dfrac{A}{A_0}\\right)$ with reference area $A_0 = 2000 \\, \\text{Å}^2$.\nThe stability of a protein complex is strongly correlated with the size of the buried surface area, $A$, upon association. This function models a linear increase in confidence with area, up to a saturation point. An interface area greater than or equal to $A_0$ is considered sufficiently large to form a stable complex, thus receiving the maximum score of $1$. This prevents disproportionately large interfaces from dominating the final score. An area of $A=0 \\, \\text{Å}^2$ correctly yields $S_A=0$, signifying no physical contact.\n\n$2$. **Free Energy Sub-score ($S_G$)**: $S_G = \\dfrac{1}{1 + \\exp\\!\\left(\\beta \\left(\\Delta G - \\Delta G_0\\right)\\right)}$ with $\\Delta G_0 = -6 \\, \\text{kcal/mol}$ and $\\beta = 0.5 \\, (\\text{kcal/mol})^{-1}$.\nThis is a standard logistic function that maps the binding free energy, $\\Delta G$, to a score between $0$ and $1$. More negative values of $\\Delta G$ indicate more favorable binding, which correctly corresponds to higher values of $S_G$. The parameter $\\Delta G_0$ is the midpoint of the sigmoid, representing a typical threshold for a specific interaction. The parameter $\\beta$ controls the steepness of the transition, determining how sensitively the score responds to changes in $\\Delta G$ around $\\Delta G_0$.\n\n$3$. **Hydrogen Bond Sub-score ($S_H$)**: $S_H = \\tanh\\!\\left(\\dfrac{h}{h_0}\\right)$ with reference count $h_0 = 10$.\nHydrogen bonds, $h$, are crucial for the specificity and stability of protein interfaces. The hyperbolic tangent function, $\\tanh$, provides a smooth, saturating score. As the number of hydrogen bonds increases, the score asymptotically approaches $1$. The reference value $h_0$ normalizes the count, with $h=h_0$ corresponding to a score of $\\tanh(1) \\approx 0.76$.\n\n$4$. **Hydrophobicity Sub-score ($S_F$)**: $S_F = f_h$.\nThe hydrophobic effect is a primary driving force for protein folding and association. This sub-score is simply the fraction of hydrophobic residues at the interface, $f_h$, which is a direct measure of this contribution. The value $f_h$ is already a dimensionless fraction between $0$ and $1$, requiring no further transformation.\n\n$5$. **Flexibility Sub-score ($S_B$)**: $S_B = \\dfrac{1}{1 + \\dfrac{B}{B_0}}$ with reference B-factor $B_0 = 30 \\, \\text{Å}^2$.\nThe crystallographic B-factor, $B$, is a measure of atomic displacement, reflecting local structural flexibility. A well-ordered, stable interface is characterized by low B-factors. This scoring function correctly represents an inverse relationship: as flexibility $B$ increases, the score $S_B$ decreases, asymptotically approaching $0$. The reference value $B_0$ serves as a threshold to distinguish between rigid (low $B$) and flexible (high $B$) regions.\n\n**Final Confidence Weight ($w$)**: $w = \\left(S_A \\cdot S_G \\cdot S_H \\cdot S_F \\cdot S_B\\right)^{1/5}$.\nThe final weight is the geometric mean of the $5$ sub-scores. This is a mathematically sound choice for combining different metrics. A key property of the geometric mean is its sensitivity to low values; if any single sub-score is very low (or zero), it will severely penalize the overall confidence weight. This ensures that a putative interaction must be favorable across all criteria to be considered high-confidence. For instance, if $A=0$, then $S_A=0$, which correctly forces the final weight $w$ to be $0$.\n\nThe computational procedure is as follows:\nFor each of the $6$ test cases, represented by a tuple $(A, \\Delta G, h, f_h, B)$, we will:\na. Calculate each of the $5$ sub-scores $S_A, S_G, S_H, S_F, S_B$ using the provided formulas and constants.\nb. Compute the product of these $5$ sub-scores.\nc. Calculate the $5$-th root of the product to obtain the final weight $w$.\nd. Collate the results from all test cases and format them according to the specified output requirements.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a dimensionless confidence weight for protein-protein interactions\n    based on five physical and structural descriptors.\n    \"\"\"\n\n    # Define the constants from the problem statement.\n    A0 = 2000.0  # Reference interface area in Å^2\n    dG0 = -6.0   # Reference binding free energy in kcal/mol\n    beta = 0.5   # Steepness parameter for the free energy score\n    h0 = 10.0    # Reference number of hydrogen bonds\n    B0 = 30.0    # Reference B-factor in Å^2\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (A, ΔG, h, f_h, B)\n    test_cases = [\n        (1800.0, -10.0, 12.0, 0.6, 15.0),\n        (400.0, -2.0, 2.0, 0.3, 50.0),\n        (0.0, -5.0, 5.0, 0.5, 20.0),\n        (2500.0, -5.0, 5.0, 0.8, 60.0),\n        (700.0, -7.0, 20.0, 0.4, 10.0),\n        (800.0, -15.0, 8.0, 1.0, 25.0)\n    ]\n\n    results = []\n    for case in test_cases:\n        A, dG, h, fh, B = case\n\n        # Step 1: Calculate the five dimensionless sub-scores.\n\n        # Area sub-score\n        S_A = min(1.0, A / A0)\n\n        # Free energy sub-score (logistic function)\n        S_G = 1.0 / (1.0 + np.exp(beta * (dG - dG0)))\n\n        # Hydrogen bond sub-score (hyperbolic tangent)\n        S_H = np.tanh(h / h0)\n\n        # Hydrophobicity sub-score\n        S_F = fh\n\n        # Flexibility sub-score\n        S_B = 1.0 / (1.0 + B / B0)\n\n        # Step 2: Calculate the product of the sub-scores.\n        product = S_A * S_G * S_H * S_F * S_B\n        \n        # Step 3: Compute the final confidence weight as the geometric mean.\n        # The 5th root is equivalent to raising to the power of 1/5.\n        # If the product is 0 (e.g., if A=0 or fh=0), the weight is correctly 0.\n        weight = product**(1.0 / 5.0)\n        \n        results.append(weight)\n\n    # Final print statement in the exact required format.\n    # The output is a single line containing a comma-separated list of weights\n    # enclosed in square brackets, with each weight rounded to six decimal places.\n    print(f\"[{','.join([f'{res:.6f}' for res in results])}]\")\n\nsolve()\n```"
        }
    ]
}