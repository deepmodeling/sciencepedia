{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in functional neuroimaging is ensuring that observed correlations reflect true neural communication rather than measurement artifacts. This exercise addresses a fundamental confound in fMRI studies, where non-neural physiological signals and preprocessing steps can create spatial autocorrelation that inflates connectivity estimates between nearby brain regions. By working through this problem , you will mathematically model this spatial proximity confound and derive a principled method for its correction, a critical skill for producing valid and interpretable functional connectomes.",
            "id": "4277694",
            "problem": "Consider an advanced graduate-level investigation of the confounding effect of spatial proximity on estimates of functional connectivity in brain networks derived from Blood Oxygen Level Dependent (BOLD) signals measured by functional Magnetic Resonance Imaging (fMRI). Let brain activity be summarized at the parcel level, with parcel $i$ located at spatial coordinate $\\mathbf{x}_i \\in \\mathbb{R}^3$ and parcel $j$ at $\\mathbf{x}_j \\in \\mathbb{R}^3$. Define the Euclidean distance $d_{ij} = \\|\\mathbf{x}_i - \\mathbf{x}_j\\|$.\n\nAssume each observed parcel time series $y_i(t)$ is the sum of an unobserved neural signal $s_i(t)$ and a spatially autocorrelated confound $n(\\mathbf{x}_i, t)$ arising from non-neural sources (for example, motion, vasculature, or spatially smooth preprocessing artifacts). Formally,\n$$\ny_i(t) = s_i(t) + n(\\mathbf{x}_i, t).\n$$\nAssume $s_i(t)$ and $n(\\mathbf{x}_i, t)$ are zero-mean, wide-sense stationary, mutually independent processes. Let $\\mathrm{Var}(s_i) = \\sigma_s^2$ for all parcels, and let the cross-covariance of neural signals satisfy $\\mathrm{Cov}(s_i, s_j) = \\rho_{ij} \\sigma_s^2$, where $\\rho_{ij} \\in [-1,1]$ encodes the true functional coupling between parcels $i$ and $j$ independent of measurement confounds.\n\nModel the confound $n(\\mathbf{x}, t)$ as a zero-mean Gaussian Random Field (GRF) indexed by spatial location $\\mathbf{x}$ with covariance\n$$\n\\mathrm{Cov}\\!\\big(n(\\mathbf{x}_i, t), n(\\mathbf{x}_j, t)\\big) = \\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right),\n$$\nwhere $\\sigma_n^2 > 0$ is the confound variance and $\\ell > 0$ is the spatial correlation length scale. Assume time series are sufficiently long that sample correlations equal their ensemble expectations.\n\nUsing only the core definitions of covariance, variance, and Pearson correlation, and the GRF covariance model above:\n\n1. Derive the expected Pearson correlation between $y_i(t)$ and $y_j(t)$ as a function of $d_{ij}$, $\\rho_{ij}$, $\\sigma_s^2$, $\\sigma_n^2$, and $\\ell$. Explain how spatial proximity inflates this observed correlation for small $d_{ij}$.\n\n2. Propose a spatial statistical control that removes the GRF-induced inflation. Specifically, construct a debiased estimator of the expected correlation by regressing out the known GRF kernel contribution or, equivalently, subtracting the GRF expectation at distance $d_{ij}$. Provide the closed-form expression of this debiased expected correlation in terms of $\\rho_{ij}$, $\\sigma_s^2$, and $\\sigma_n^2$.\n\n3. For a concrete pair of parcels with parameters $\\sigma_s^2 = 2$, $\\sigma_n^2 = 1$, $\\ell = 20$ mm, $d_{ij} = 10$ mm, and $\\rho_{ij} = 0.3$, compute the debiased expected correlation. Round your answer to four significant figures. Express the final answer as a single real number without units.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of signal processing and spatial statistics as applied to neuroimaging data, a common topic in complex systems and network science. The problem is well-posed, with all necessary variables, assumptions, and definitions provided to derive a unique solution. The language is objective and formal.\n\n### Part 1: Derivation of the Observed Correlation\n\nThe Pearson correlation coefficient between the observed time series $y_i(t)$ and $y_j(t)$ is defined as:\n$$\n\\mathrm{Corr}(y_i, y_j) = \\frac{\\mathrm{Cov}(y_i, y_j)}{\\sqrt{\\mathrm{Var}(y_i) \\mathrm{Var}(y_j)}}\n$$\nWe must first derive the covariance and variance terms in the numerator and denominator.\n\nThe observed time series is given by the model $y_k(t) = s_k(t) + n(\\mathbf{x}_k, t)$ for any parcel $k$. Let us denote $n_k(t) = n(\\mathbf{x}_k, t)$ for simplicity.\n\nThe covariance between $y_i(t)$ and $y_j(t)$ is:\n$$\n\\mathrm{Cov}(y_i, y_j) = \\mathrm{Cov}(s_i + n_i, s_j + n_j)\n$$\nUsing the bilinearity of the covariance operator:\n$$\n\\mathrm{Cov}(y_i, y_j) = \\mathrm{Cov}(s_i, s_j) + \\mathrm{Cov}(s_i, n_j) + \\mathrm{Cov}(n_i, s_j) + \\mathrm{Cov}(n_i, n_j)\n$$\nThe problem states that the neural signals $s_k(t)$ and the confound processes $n_k(t)$ are mutually independent. This implies that their covariance is zero, i.e., $\\mathrm{Cov}(s_i, n_j) = 0$ and $\\mathrm{Cov}(n_i, s_j) = 0$ for all $i$ and $j$.\nTherefore, the expression simplifies to:\n$$\n\\mathrm{Cov}(y_i, y_j) = \\mathrm{Cov}(s_i, s_j) + \\mathrm{Cov}(n_i, n_j)\n$$\nSubstituting the given expressions for the covariances of the signal and confound:\n$$\n\\mathrm{Cov}(s_i, s_j) = \\rho_{ij} \\sigma_s^2\n$$\n$$\n\\mathrm{Cov}(n_i, n_j) = \\mathrm{Cov}\\!\\big(n(\\mathbf{x}_i, t), n(\\mathbf{x}_j, t)\\big) = \\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)\n$$\nThus, the total observed covariance is:\n$$\n\\mathrm{Cov}(y_i, y_j) = \\rho_{ij} \\sigma_s^2 + \\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)\n$$\nNext, we calculate the variance of an observed time series $y_i(t)$. The variance is the covariance of a variable with itself.\n$$\n\\mathrm{Var}(y_i) = \\mathrm{Cov}(y_i, y_i) = \\mathrm{Cov}(s_i + n_i, s_i + n_i)\n$$\n$$\n\\mathrm{Var}(y_i) = \\mathrm{Var}(s_i) + \\mathrm{Var}(n_i) + 2\\mathrm{Cov}(s_i, n_i)\n$$\nDue to independence, $\\mathrm{Cov}(s_i, n_i) = 0$. The variance of the signal is given as $\\mathrm{Var}(s_i) = \\sigma_s^2$. The variance of the confound at parcel $i$, $\\mathrm{Var}(n_i)$, can be found by setting $j=i$ (and thus $d_{ii} = 0$) in its covariance function:\n$$\n\\mathrm{Var}(n_i) = \\mathrm{Cov}(n_i, n_i) = \\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ii}^2}{2 \\ell^2}\\right) = \\sigma_n^2 \\exp(0) = \\sigma_n^2\n$$\nSo, the total variance of the observed signal is:\n$$\n\\mathrm{Var}(y_i) = \\sigma_s^2 + \\sigma_n^2\n$$\nSince this holds for any parcel $i$, we also have $\\mathrm{Var}(y_j) = \\sigma_s^2 + \\sigma_n^2$.\n\nNow we can assemble the Pearson correlation:\n$$\n\\mathrm{Corr}(y_i, y_j) = \\frac{\\rho_{ij} \\sigma_s^2 + \\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)}{\\sqrt{(\\sigma_s^2 + \\sigma_n^2)(\\sigma_s^2 + \\sigma_n^2)}} = \\frac{\\rho_{ij} \\sigma_s^2 + \\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)}{\\sigma_s^2 + \\sigma_n^2}\n$$\nThis can be written as:\n$$\n\\mathrm{Corr}(y_i, y_j) = \\frac{\\rho_{ij} \\sigma_s^2}{\\sigma_s^2 + \\sigma_n^2} + \\frac{\\sigma_n^2}{\\sigma_s^2 + \\sigma_n^2} \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)\n$$\nThe second term, $\\frac{\\sigma_n^2}{\\sigma_s^2 + \\sigma_n^2} \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)$, represents the inflation of the observed correlation due to the spatially autocorrelated confound. This term is always non-negative. Its magnitude depends on the Euclidean distance $d_{ij}$. For small distances ($d_{ij} \\ll \\ell$), the exponential term is close to $1$, and the inflation is maximal. As the distance $d_{ij}$ increases, the exponential term decays to $0$, and the inflation vanishes. Therefore, spatial proximity (small $d_{ij}$) artificially inflates the observed functional connectivity estimate.\n\n### Part 2: Debiased Expected Correlation\n\nThe problem asks to construct a debiased estimator of the expected correlation by subtracting the contribution from the GRF confound. From the expression derived in Part 1, the observed correlation is the sum of two components: one related to the true neural correlation $\\rho_{ij}$ and one arising purely from the confound.\n\nThe contribution of the confound to the observed correlation is the value the correlation would take if there were no neural coupling, i.e., if $\\rho_{ij} = 0$. This is the second term derived above:\n$$\nR_{ij}^{\\text{confound}} = \\frac{\\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)}{\\sigma_s^2 + \\sigma_n^2}\n$$\nA debiased estimator of the expected correlation, which we denote $\\rho_{ij}^{\\text{debiased}}$, is obtained by subtracting this confounding contribution from the total observed correlation, $\\mathrm{Corr}(y_i, y_j)$:\n$$\n\\rho_{ij}^{\\text{debiased}} = \\mathrm{Corr}(y_i, y_j) - R_{ij}^{\\text{confound}}\n$$\n$$\n\\rho_{ij}^{\\text{debiased}} = \\left( \\frac{\\rho_{ij} \\sigma_s^2 + \\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)}{\\sigma_s^2 + \\sigma_n^2} \\right) - \\left( \\frac{\\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)}{\\sigma_s^2 + \\sigma_n^2} \\right)\n$$\n$$\n\\rho_{ij}^{\\text{debiased}} = \\frac{\\rho_{ij} \\sigma_s^2 + \\sigma_n^2 \\exp\\!\\left(-\\fracd_{ij}^2}{2 \\ell^2}\\right) - \\sigma_n^2 \\exp\\!\\left(-\\frac{d_{ij}^2}{2 \\ell^2}\\right)}{\\sigma_s^2 + \\sigma_n^2}\n$$\nThe terms involving the confound cancel, yielding the closed-form expression for the debiased expected correlation:\n$$\n\\rho_{ij}^{\\text{debiased}} = \\frac{\\rho_{ij} \\sigma_s^2}{\\sigma_s^2 + \\sigma_n^2}\n$$\nThis expression represents the true neural correlation $\\rho_{ij}$ scaled by the proportion of the total variance that is attributable to the signal, $\\frac{\\sigma_s^2}{\\sigma_s^2 + \\sigma_n^2}$, often known as the signal-to-total-variance ratio. Note that this corrected value is independent of the distance $d_{ij}$ and the length scale $\\ell$, as the spatial effect has been properly removed.\n\n### Part 3: Numerical Computation\n\nWe are asked to compute the debiased expected correlation using the derived formula and the given parameters:\n- $\\sigma_s^2 = 2$\n- $\\sigma_n^2 = 1$\n- $\\ell = 20$ mm\n- $d_{ij} = 10$ mm\n- $\\rho_{ij} = 0.3$\n\nThe debiased expected correlation is given by the expression from Part 2:\n$$\n\\rho_{ij}^{\\text{debiased}} = \\frac{\\rho_{ij} \\sigma_s^2}{\\sigma_s^2 + \\sigma_n^2}\n$$\nSubstituting the given numerical values:\n$$\n\\rho_{ij}^{\\text{debiased}} = \\frac{(0.3)(2)}{2 + 1} = \\frac{0.6}{3} = 0.2\n$$\nThe problem requires the answer to be rounded to four significant figures. The exact value is $0.2$, which is written as $0.2000$ to four significant figures.",
            "answer": "$$\n\\boxed{0.2000}\n$$"
        },
        {
            "introduction": "Beyond static measurement artifacts, functional networks are susceptible to dynamic confounds where observed correlations do not imply direct connections. This problem explores the classic \"common driver\" scenario, where two brain regions appear connected only because they are both influenced by a third, unobserved source. This practice  challenges you to first derive the spurious correlation induced by a common input and then design a statistical procedure using partial correlation to disambiguate this confounding effect from a true, direct interaction.",
            "id": "4277727",
            "problem": "Consider the distinction between structural brain networks (physical axonal connections) and functional brain networks (statistical dependencies among neural signals). Suppose an experiment records three zero-mean discrete-time stochastic processes: a putative common driver $S_t$, and two regional signals $X_t$ and $Y_t$. The common driver $S_t$ is first-order autoregressive, that is, $S_t$ satisfies $S_t = \\phi S_{t-1} + \\eta_t$ with $|\\phi| < 1$, where $\\eta_t$ are independent and identically distributed Gaussian innovations with variance $\\sigma_\\eta^2$. Let $X_t$ and $Y_t$ be generated by\n$$\nX_t = a\\, S_{t-\\ell_x} + u_t,\n\\qquad\nY_t = b\\, S_{t-\\ell_y} + c\\, X_{t-\\tau} + v_t,\n$$\nwhere $a$, $b$, $c$ are real constants, $\\ell_x$, $\\ell_y$, $\\tau$ are nonnegative integers, and $u_t$, $v_t$ are independent Gaussian white noise processes with variances $\\sigma_u^2$ and $\\sigma_v^2$, respectively. All noise terms are independent of $S_t$. You do not know whether the direct coupling parameter $c$ is zero. You observe a positive sample cross-correlation between $X_{t-\\tau}$ and $Y_t$ at lag $\\tau$.\n\nPart 1. Using only the linearity of covariance, the definition of the autoregressive model, and stationarity of $S_t$, explain mechanistically how time-lagged correlations between $X_{t-\\tau}$ and $Y_t$ can arise even when $c = 0$ due to the shared input $S_t$. Derive the theoretical cross-covariance $\\operatorname{Cov}(X_{t-\\tau}, Y_t)$ in terms of $a$, $b$, $\\phi$, $\\sigma_\\eta^2$, $\\ell_x$, $\\ell_y$, and $\\tau$ under the null hypothesis $c = 0$.\n\nPart 2. You are tasked with designing a statistical test that differentiates true interactions ($c \\neq 0$) from common-driver confounds using partial correlation or residualization. Assume $N$ samples are available and Gaussian assumptions hold. Which procedure below correctly targets the null hypothesis $H_0: c = 0$ by removing the confounding pathways from $S_t$ while preserving a test for the direct interaction from $X_{t-\\tau}$ to $Y_t$?\n\nA. Declare a direct interaction if the sample cross-correlation between $X_{t-\\tau}$ and $Y_t$ is significantly greater than zero, using a threshold derived from the asymptotic variance of a correlation coefficient under independence.\n\nB. Form the control vector $Z_t$ consisting of lags of $S_t$ that enter $X_{t-\\tau}$ and $Y_t$, for example $Z_t = \\big(S_{t-\\ell_y}, S_{t-\\tau-\\ell_x}, S_{t-\\ell_y-1}, \\dots, S_{t-\\tau-\\ell_x-1}, \\dots\\big)$ up to a finite order that captures the autocorrelation of $S_t$, then test $H_0: c = 0$ by either (i) computing the partial correlation $\\rho_{X_{t-\\tau}, Y_t \\cdot Z_t}$ and using its $t$-statistic with $N - q - 2$ degrees of freedom, where $q$ is the dimension of $Z_t$, or (ii) fitting the ordinary least squares (OLS) regression $Y_t = \\beta_0 + \\beta_X X_{t-\\tau} + \\beta^\\top Z_t + \\varepsilon_t$ and testing $H_0: \\beta_X = 0$; both are equivalent under linear Gaussian assumptions.\n\nC. Compute the partial correlation between $X_{t-\\tau}$ and $Y_t$ controlling only for the instantaneous driver $S_t$, that is, $\\rho_{X_{t-\\tau}, Y_t \\cdot S_t}$, and use a $t$-test with $N - 3$ degrees of freedom.\n\nD. Residualize $Y_t$ by first regressing out $X_{t-\\tau}$ to obtain $Y_t^{\\perp X} = Y_t - \\hat{\\gamma} X_{t-\\tau}$, then test whether $Y_t^{\\perp X}$ is uncorrelated with any lags of $S_t$ using a joint significance test; if the residual is unrelated to $S_t$, declare a direct interaction from $X_{t-\\tau}$ to $Y_t$.",
            "solution": "The problem statement is scientifically sound, well-posed, and unambiguous. It presents a classic scenario in time series analysis for distinguishing direct from indirect (confounded) interactions, a fundamental topic in network neuroscience. The models are standard, and all necessary parameters and assumptions are clearly defined. We can therefore proceed with the derivation and analysis.\n\n**Part 1. Spurious Correlation and Cross-Covariance Derivation**\n\nThe problem asks for a mechanistic explanation of how correlations between $X_{t-\\tau}$ and $Y_t$ can arise even when the direct coupling constant $c$ is zero, and to derive the theoretical cross-covariance.\n\n**Mechanistic Explanation:**\nUnder the null hypothesis $H_0: c=0$, the signals $X_t$ and $Y_t$ are generated as:\n$$\nX_t = a\\, S_{t-\\ell_x} + u_t\n$$\n$$\nY_t = b\\, S_{t-\\ell_y} + v_t\n$$\nBoth $X_t$ and $Y_t$ are influenced by the same underlying process, $S_t$, which is referred to as a \"common driver\". The process $S_t$ is autoregressive, meaning its value at any time $t$ is correlated with its past values. Specifically, $S_t = \\phi S_{t-1} + \\eta_t$. This property is known as autocorrelation.\n\nWhen we examine the relationship between $X_{t-\\tau}$ and $Y_t$, we are comparing the signal $X$ at time $t-\\tau$ with the signal $Y$ at time $t$. Their expressions are:\n$$\nX_{t-\\tau} = a\\, S_{t-\\tau-\\ell_x} + u_{t-\\tau}\n$$\n$$\nY_t = b\\, S_{t-\\ell_y} + v_t\n$$\nThe correlation between $X_{t-\\tau}$ and $Y_t$ arises because they share a common cause, the process $S_t$. The signal $X_{t-\\tau}$ is influenced by $S_t$ at time $t-\\tau-\\ell_x$, while $Y_t$ is influenced by $S_t$ at time $t-\\ell_y$. Due to the autocorrelation of $S_t$, the values $S_{t-\\tau-\\ell_x}$ and $S_{t-\\ell_y}$ are themselves correlated. This correlation, propagated through the constants $a$ and $b$, induces a statistical dependency (correlation) between $X_{t-\\tau}$ and $Y_t$, even in the complete absence of a direct causal link from $X$ to $Y$ (i.e., when $c=0$). This phenomenon is a classic example of confounding.\n\n**Derivation of Cross-Covariance:**\nThe cross-covariance between $X_{t-\\tau}$ and $Y_t$ is defined as $\\operatorname{Cov}(X_{t-\\tau}, Y_t) = \\operatorname{E}[X_{t-\\tau} Y_t] - \\operatorname{E}[X_{t-\\tau}]\\operatorname{E}[Y_t]$. Since all processes are given as zero-mean, this simplifies to $\\operatorname{Cov}(X_{t-\\tau}, Y_t) = \\operatorname{E}[X_{t-\\tau} Y_t]$.\n\nUnder the null hypothesis $c=0$, we have:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = \\operatorname{E}[(a\\, S_{t-\\tau-\\ell_x} + u_{t-\\tau})(b\\, S_{t-\\ell_y} + v_t)]\n$$\nExpanding the product gives:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = \\operatorname{E}[ab\\, S_{t-\\tau-\\ell_x} S_{t-\\ell_y} + a\\, S_{t-\\tau-\\ell_x} v_t + b\\, S_{t-\\ell_y} u_{t-\\tau} + u_{t-\\tau} v_t]\n$$\nBy linearity of expectation, we can separate the terms:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = ab\\, \\operatorname{E}[S_{t-\\tau-\\ell_x} S_{t-\\ell_y}] + a\\, \\operatorname{E}[S_{t-\\tau-\\ell_x} v_t] + b\\, \\operatorname{E}[S_{t-\\ell_y} u_{t-\\tau}] + \\operatorname{E}[u_{t-\\tau} v_t]\n$$\nAccording to the problem statement, all noise terms ($u_t, v_t, \\eta_t$) are mutually independent and independent of $S_t$. Since the processes are zero-mean, the expectation of products of independent variables is zero.\n- $\\operatorname{E}[S_{t-\\tau-\\ell_x} v_t] = \\operatorname{E}[S_{t-\\tau-\\ell_x}] \\operatorname{E}[v_t] = 0 \\cdot 0 = 0$.\n- $\\operatorname{E}[S_{t-\\ell_y} u_{t-\\tau}] = \\operatorname{E}[S_{t-\\ell_y}] \\operatorname{E}[u_{t-\\tau}] = 0 \\cdot 0 = 0$.\n- $\\operatorname{E}[u_{t-\\tau} v_t] = \\operatorname{E}[u_{t-\\tau}] \\operatorname{E}[v_t] = 0 \\cdot 0 = 0$ (as $u_t$ and $v_t$ are independent processes).\n\nThis leaves only the first term:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = ab\\, \\operatorname{E}[S_{t-\\tau-\\ell_x} S_{t-\\ell_y}]\n$$\nThe term $\\operatorname{E}[S_{t-\\tau-\\ell_x} S_{t-\\ell_y}]$ is the autocovariance of the stationary process $S_t$, denoted $\\gamma_S(k)$, where the lag $k$ is the difference in the time indices: $k = (t-\\ell_y) - (t-\\tau-\\ell_x) = \\tau + \\ell_x - \\ell_y$.\nSo,\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = ab\\, \\gamma_S(\\tau + \\ell_x - \\ell_y)\n$$\nFor the stationary AR(1) process $S_t = \\phi S_{t-1} + \\eta_t$ with $|\\phi|<1$, the variance is $\\gamma_S(0) = \\operatorname{Var}(S_t) = \\frac{\\sigma_\\eta^2}{1-\\phi^2}$. The autocovariance function is $\\gamma_S(k) = \\phi^{|k|} \\gamma_S(0)$. Substituting this in, we get the final expression:\n$$\n\\operatorname{Cov}(X_{t-\\tau}, Y_t) = ab\\, \\frac{\\sigma_\\eta^2}{1-\\phi^2} \\phi^{|\\tau + \\ell_x - \\ell_y|}\n$$\nThis quantity is generally non-zero if $a \\neq 0$, $b \\neq 0$, and $\\phi \\neq 0$, which explicitly shows how a non-zero cross-covariance arises from the common driver even when $c=0$.\n\n**Part 2. Design of the Statistical Test**\n\nThe task is to find a procedure that correctly tests the null hypothesis $H_0: c=0$. This requires isolating the direct influence of $X_{t-\\tau}$ on $Y_t$ from the indirect influence mediated by the common driver $S_t$. The correct approach is to statistically control for the confounding variables.\n\nThe full model is $Y_t = b\\, S_{t-\\ell_y} + c\\, X_{t-\\tau} + v_t$. The variable $X_{t-\\tau}$ is itself dependent on $S_{t-\\tau-\\ell_x}$. The variables that are common causes to both $Y_t$ and the predictor of interest $X_{t-\\tau}$ are the various lags of $S_t$. To test for the unique contribution of $X_{t-\\tau}$ to $Y_t$, one must account for the explanatory power of the $S_t$ process.\n\n**Analysis of Options:**\n\n**A.** Declaring an interaction based on the simple sample cross-correlation between $X_{t-\\tau}$ and $Y_t$ is precisely the flawed approach that fails to distinguish direct from confounded effects. As derived in Part 1, this correlation can be significant even when $c=0$. This method tests for *functional connectivity* (any statistical association) rather than *effective connectivity* (direct causal influence).\n**Verdict: Incorrect.**\n\n**B.** This option proposes forming a control vector $Z_t$ from the lags of the common driver $S_t$ and then testing for the relationship between $X_{t-\\tau}$ and $Y_t$ *after* controlling for $Z_t$. This can be done by calculating the partial correlation $\\rho_{X_{t-\\tau}, Y_t \\cdot Z_t}$ or by testing the significance of the coefficient $\\beta_X$ in the multiple regression model $Y_t = \\beta_0 + \\beta_X X_{t-\\tau} + \\beta^\\top Z_t + \\varepsilon_t$. This is the standard, correct procedure for dealing with confounders in a linear system. By including the relevant dynamics of $S_t$ in the model, we can isolate the additional explanatory power provided by $X_{t-\\tau}$, which, under this model, corresponds to the direct path parameterized by $c$. The equivalence of the partial correlation test and the t-test for a regression coefficient is a standard result for linear Gaussian models. The description of $Z_t$ correctly identifies that it should contain the lags of $S_t$ responsible for the confounding.\n**Verdict: Correct.**\n\n**C.** This option proposes to control only for the instantaneous driver $S_t$. The confounding effect, however, is mediated by specific lags $S_{t-\\ell_y}$ and $S_{t-\\tau-\\ell_x}$. Controlling for $S_t$ is arbitrary and generally incorrect; it does not properly remove the influence of the specific lags that cause the confounding. While $S_t$ is correlated with all its lags, controlling for it will typically only partially, not fully, remove the confounding effect, leading to an invalid test.\n**Verdict: Incorrect.**\n\n**D.** This option describes an incorrect logical sequence. It first regresses $Y_t$ on $X_{t-\\tau}$ and then tests if the residual is correlated with $S_t$. If one performs the regression $Y_t = \\hat{\\gamma} X_{t-\\tau} + Y_t^{\\perp X}$, the estimated coefficient $\\hat{\\gamma}$ will be biased due to the omission of the confounder $S_t$. Finding that the residual $Y_t^{\\perp X}$ is correlated with $S_t$ merely confirms that the initial simple regression model was misspecified because it omitted a relevant variable ($S_t$). This procedure does not provide a valid test for the null hypothesis $H_0: c=0$. The correct logic is to test for the influence of $X_{t-\\tau}$ *after* controlling for $S_t$, not the other way around.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "This final exercise confronts a fundamental limit in network neuroscience: the challenge of inferring directed structural connections from purely functional (observational) data. Using a simple two-node model, you will demonstrate that two completely different underlying structural architectures can produce statistically indistinguishable functional connectivity patterns. This hands-on coding problem  provides a powerful and concrete illustration of observational equivalence, highlighting why integrating structural data or using experimental interventions is often necessary to resolve ambiguities in brain circuitry.",
            "id": "4277686",
            "problem": "Consider a pair of two-node brain regions modeled as a linear Gaussian Structural Equation Model (SEM), which provides a mathematical representation of a structural brain network. The structural brain network is captured by a directed adjacency matrix $B$ and an independent noise covariance matrix $\\Omega$, while the functional brain network is captured by the second-order (covariance) statistics of the observed variables. For a linear acyclic SEM, the observable covariance matrix $\\Sigma$ is related to the structure by the well-tested formula $\\Sigma = (I - B)^{-1} \\Omega (I - B)^{-T}$, where $I$ is the identity matrix, $B$ encodes directed edges, and $\\Omega$ encodes independent Gaussian noise at each node. This relation is the foundational base for the problem.\n\nYou will construct a synthetic dataset to demonstrate that two distinct directed architectures can yield indistinguishable second-order statistics at zero lag, thereby showing that purely observational second-order statistics are insufficient to identify directionality without interventions or richer observables.\n\nWork with two scalar variables $x$ and $y$. Consider two alternative directed architectures:\n- Model $A$: a directed edge from $y$ to $x$, corresponding to the equations $x = \\alpha y + \\epsilon_x$ and $y = \\epsilon_y$, where $\\epsilon_x$ and $\\epsilon_y$ are independent zero-mean Gaussian noises.\n- Model $B$: a directed edge from $x$ to $y$, corresponding to the equations $y = \\beta x + \\epsilon_y'$ and $x = \\epsilon_x'$, where $\\epsilon_x'$ and $\\epsilon_y'$ are independent zero-mean Gaussian noises.\n\nYour task is to:\n- Use the relation between the structural parameters and the functional covariance to choose parameters $(\\alpha, \\operatorname{Var}(\\epsilon_x), \\operatorname{Var}(\\epsilon_y))$ for Model $A$ and $(\\beta, \\operatorname{Var}(\\epsilon_x'), \\operatorname{Var}(\\epsilon_y'))$ for Model $B$ so that both models produce the same target observable covariance matrix $\\Sigma$ with marginal variances and correlation specified below.\n- Generate synthetic datasets for both models of size $n$ using independent Gaussian draws for the noise terms, and construct the observed variables according to the model definitions.\n- Compute the sample covariance matrices for both datasets and evaluate the maximum absolute entrywise difference between these matrices. Declare the models indistinguishable in second-order statistics if this maximum difference is smaller than a specified tolerance.\n\nScientific realism constraints:\n- Ensure that all noise variances are nonnegative and that the implied covariance matrices are positive semidefinite.\n- All variables are dimensionless, so no physical units are required.\n- Angles are not used, so no angle unit is required.\n\nTest suite:\nProvide a program that executes the following test cases, each with a target covariance defined by marginal variances $\\sigma_x^2$, $\\sigma_y^2$ and correlation coefficient $\\rho$, along with a sample size $n$, a random seed $s$, and a numerical tolerance $\\tau$ for indistinguishability.\n\n- Test case $1$ (general case): $\\sigma_x^2 = 1.0$, $\\sigma_y^2 = 1.0$, $\\rho = 0.4$, $n = 100000$, $s = 0$, $\\tau = 2 \\times 10^{-3}$.\n- Test case $2$ (boundary case): $\\sigma_x^2 = 1.0$, $\\sigma_y^2 = 2.0$, $\\rho = 0.0$, $n = 100000$, $s = 1$, $\\tau = 2 \\times 10^{-3}$.\n- Test case $3$ (edge case with strong correlation): $\\sigma_x^2 = 1.0$, $\\sigma_y^2 = 1.0$, $\\rho = 0.95$, $n = 150000$, $s = 2$, $\\tau = 5 \\times 10^{-3}$.\n\nFor each test case:\n- Construct parameters for Model $A$ and Model $B$ such that both produce the same target covariance matrix $\\Sigma$ with entries $\\Sigma_{xx} = \\sigma_x^2$, $\\Sigma_{yy} = \\sigma_y^2$, and $\\Sigma_{xy} = \\rho \\sigma_x \\sigma_y$.\n- Simulate $n$ samples for both models with the given seed.\n- Compute the sample covariance matrices and return a boolean indicating whether the maximum absolute difference between the two sample covariance matrices is smaller than $\\tau$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean corresponding to the indistinguishability decision for the respective test case.",
            "solution": "The fundamental problem is to demonstrate that two distinct directed causal structures, termed Model $A$ and Model $B$, can be observationally equivalent with respect to second-order statistics. This means both models can generate data with the exact same theoretical covariance matrix $\\Sigma$, rendering them indistinguishable from purely observational data at zero lag. We will first derive the parameters for each model that achieve this equivalence and then outline the numerical simulation to verify it.\n\nThe relationship between the structural parameters—the directed adjacency matrix $B$ and the independent noise covariance matrix $\\Omega$—and the observable covariance matrix $\\Sigma$ for a linear acyclic Structural Equation Model (SEM) is given by:\n$$\n\\Sigma = (I - B)^{-1} \\Omega (I - B)^{-T}\n$$\nHere, $I$ is the $2 \\times 2$ identity matrix, and we consider the variables to be ordered in a vector as $\\begin{pmatrix} x \\\\ y \\end{pmatrix}$. The target observable covariance matrix $\\Sigma$ is expressed in terms of the marginal variances $\\sigma_x^2$ and $\\sigma_y^2$, and the correlation coefficient $\\rho$:\n$$\n\\Sigma = \\begin{pmatrix} \\Sigma_{xx} & \\Sigma_{xy} \\\\ \\Sigma_{yx} & \\Sigma_{yy} \\end{pmatrix} = \\begin{pmatrix} \\sigma_x^2 & \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y & \\sigma_y^2 \\end{pmatrix}\n$$\n\n**Derivation of Parameters for Model A ($y \\to x$)**\n\nModel $A$ represents a directed edge from $y$ to $x$. The structural equations are:\n$$\nx = \\alpha y + \\epsilon_x \\\\\ny = \\epsilon_y\n$$\nwhere $\\epsilon_x$ and $\\epsilon_y$ are independent, zero-mean Gaussian noises with variances $\\operatorname{Var}(\\epsilon_x)$ and $\\operatorname{Var}(\\epsilon_y)$, respectively. The matrix representation of this system is $\\vec{v} = B \\vec{v} + \\vec{\\epsilon}$, which gives the structural matrices:\n$$\nB_A = \\begin{pmatrix} 0 & \\alpha \\\\ 0 & 0 \\end{pmatrix}, \\quad \\Omega_A = \\begin{pmatrix} \\operatorname{Var}(\\epsilon_x) & 0 \\\\ 0 & \\operatorname{Var}(\\epsilon_y) \\end{pmatrix}\n$$\nTo find the resulting covariance matrix $\\Sigma_A$, we first compute the matrix $(I - B_A)^{-1}$:\n$$\nI - B_A = \\begin{pmatrix} 1 & -\\alpha \\\\ 0 & 1 \\end{pmatrix} \\implies (I - B_A)^{-1} = \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix}\n$$\nIts transpose is $(I - B_A)^{-T} = \\begin{pmatrix} 1 & 0 \\\\ \\alpha & 1 \\end{pmatrix}$. Substituting these into the main covariance formula:\n$$\n\\Sigma_A = (I - B_A)^{-1} \\Omega_A (I - B_A)^{-T} = \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\operatorname{Var}(\\epsilon_x) & 0 \\\\ 0 & \\operatorname{Var}(\\epsilon_y) \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\alpha & 1 \\end{pmatrix}\n$$\nPerforming the matrix multiplication yields:\n$$\n\\Sigma_A = \\begin{pmatrix} \\operatorname{Var}(\\epsilon_x) + \\alpha^2 \\operatorname{Var}(\\epsilon_y) & \\alpha \\operatorname{Var}(\\epsilon_y) \\\\ \\alpha \\operatorname{Var}(\\epsilon_y) & \\operatorname{Var}(\\epsilon_y) \\end{pmatrix}\n$$\nWe equate the elements of $\\Sigma_A$ to the corresponding elements of the target matrix $\\Sigma$ to solve for the model parameters $(\\alpha, \\operatorname{Var}(\\epsilon_x), \\operatorname{Var}(\\epsilon_y))$:\n1.  From the element $(2,2)$: $\\Sigma_{yy} = \\operatorname{Var}(\\epsilon_y) \\implies \\operatorname{Var}(\\epsilon_y) = \\sigma_y^2$.\n2.  From the element $(1,2)$: $\\Sigma_{xy} = \\alpha \\operatorname{Var}(\\epsilon_y) \\implies \\rho \\sigma_x \\sigma_y = \\alpha \\sigma_y^2$. Assuming $\\sigma_y \\neq 0$, we find $\\alpha = \\frac{\\rho \\sigma_x}{\\sigma_y}$.\n3.  From the element $(1,1)$: $\\Sigma_{xx} = \\operatorname{Var}(\\epsilon_x) + \\alpha^2 \\operatorname{Var}(\\epsilon_y) \\implies \\sigma_x^2 = \\operatorname{Var}(\\epsilon_x) + \\left(\\frac{\\rho \\sigma_x}{\\sigma_y}\\right)^2 \\sigma_y^2$. This simplifies to $\\operatorname{Var}(\\epsilon_x) = \\sigma_x^2 - \\rho^2 \\sigma_x^2 = \\sigma_x^2(1 - \\rho^2)$.\n\n**Derivation of Parameters for Model B ($x \\to y$)**\n\nModel $B$ represents a directed edge from $x$ to $y$. The structural equations are:\n$$\ny = \\beta x + \\epsilon_y' \\\\\nx = \\epsilon_x'\n$$\nwhere $\\epsilon_x'$ and $\\epsilon_y'$ are independent, zero-mean Gaussian noises with variances $\\operatorname{Var}(\\epsilon_x')$ and $\\operatorname{Var}(\\epsilon_y')$. The structural matrices are:\n$$\nB_B = \\begin{pmatrix} 0 & 0 \\\\ \\beta & 0 \\end{pmatrix}, \\quad \\Omega_B = \\begin{pmatrix} \\operatorname{Var}(\\epsilon_x') & 0 \\\\ 0 & \\operatorname{Var}(\\epsilon_y') \\end{pmatrix}\n$$\nWe compute $(I - B_B)^{-1}$:\n$$\nI - B_B = \\begin{pmatrix} 1 & 0 \\\\ -\\beta & 1 \\end{pmatrix} \\implies (I - B_B)^{-1} = \\begin{pmatrix} 1 & 0 \\\\ \\beta & 1 \\end{pmatrix}\n$$\nIts transpose is $(I - B_B)^{-T} = \\begin{pmatrix} 1 & \\beta \\\\ 0 & 1 \\end{pmatrix}$. Substituting into the covariance formula:\n$$\n\\Sigma_B = (I - B_B)^{-1} \\Omega_B (I - B_B)^{-T} = \\begin{pmatrix} 1 & 0 \\\\ \\beta & 1 \\end{pmatrix} \\begin{pmatrix} \\operatorname{Var}(\\epsilon_x') & 0 \\\\ 0 & \\operatorname{Var}(\\epsilon_y') \\end{pmatrix} \\begin{pmatrix} 1 & \\beta \\\\ 0 & 1 \\end{pmatrix}\n$$\nPerforming the matrix multiplication gives:\n$$\n\\Sigma_B = \\begin{pmatrix} \\operatorname{Var}(\\epsilon_x') & \\beta \\operatorname{Var}(\\epsilon_x') \\\\ \\beta \\operatorname{Var}(\\epsilon_x') & \\beta^2 \\operatorname{Var}(\\epsilon_x') + \\operatorname{Var}(\\epsilon_y') \\end{pmatrix}\n$$\nEquating the elements of $\\Sigma_B$ to those of $\\Sigma$ to solve for $(\\beta, \\operatorname{Var}(\\epsilon_x'), \\operatorname{Var}(\\epsilon_y'))$:\n1.  From the element $(1,1)$: $\\Sigma_{xx} = \\operatorname{Var}(\\epsilon_x') \\implies \\operatorname{Var}(\\epsilon_x') = \\sigma_x^2$.\n2.  From the element $(1,2)$: $\\Sigma_{xy} = \\beta \\operatorname{Var}(\\epsilon_x') \\implies \\rho \\sigma_x \\sigma_y = \\beta \\sigma_x^2$. Assuming $\\sigma_x \\neq 0$, we find $\\beta = \\frac{\\rho \\sigma_y}{\\sigma_x}$.\n3.  From the element $(2,2)$: $\\Sigma_{yy} = \\beta^2 \\operatorname{Var}(\\epsilon_x') + \\operatorname{Var}(\\epsilon_y') \\implies \\sigma_y^2 = \\left(\\frac{\\rho \\sigma_y}{\\sigma_x}\\right)^2 \\sigma_x^2 + \\operatorname{Var}(\\epsilon_y')$. This simplifies to $\\operatorname{Var}(\\epsilon_y') = \\sigma_y^2 - \\rho^2 \\sigma_y^2 = \\sigma_y^2(1 - \\rho^2)$.\n\n**Simulation and Verification Algorithm**\n\nFor each test case with given values $(\\sigma_x^2, \\sigma_y^2, \\rho, n, s, \\tau)$, the following procedure is executed:\n1.  Initialize a pseudo-random number generator with the seed $s$.\n2.  Calculate the standard deviations $\\sigma_x = \\sqrt{\\sigma_x^2}$ and $\\sigma_y = \\sqrt{\\sigma_y^2}$.\n3.  **Simulate Model A:**\n    a. Determine the model parameters: $\\alpha = \\frac{\\rho \\sigma_x}{\\sigma_y}$, $\\operatorname{std}(\\epsilon_y) = \\sigma_y$, and $\\operatorname{std}(\\epsilon_x) = \\sigma_x \\sqrt{1 - \\rho^2}$.\n    b. Generate $n$ samples of noise: $\\epsilon_y \\sim \\mathcal{N}(0, \\sigma_y^2)$ and $\\epsilon_x \\sim \\mathcal{N}(0, \\sigma_x^2(1 - \\rho^2))$.\n    c. Synthesize the observable data according to the model: $y_A = \\epsilon_y$ and $x_A = \\alpha y_A + \\epsilon_x$.\n4.  **Simulate Model B:**\n    a. Determine the model parameters: $\\beta = \\frac{\\rho \\sigma_y}{\\sigma_x}$, $\\operatorname{std}(\\epsilon_x') = \\sigma_x$, and $\\operatorname{std}(\\epsilon_y') = \\sigma_y \\sqrt{1 - \\rho^2}$.\n    b. Generate $n$ new samples of noise: $\\epsilon_x' \\sim \\mathcal{N}(0, \\sigma_x^2)$ and $\\epsilon_y' \\sim \\mathcal{N}(0, \\sigma_y^2(1 - \\rho^2))$.\n    c. Synthesize the observable data: $x_B = \\epsilon_x'$ and $y_B = \\beta x_B + \\epsilon_y'$.\n5.  **Verification:**\n    a. Construct two $2 \\times n$ data matrices, $D_A = \\begin{pmatrix} x_A \\\\ y_A \\end{pmatrix}$ and $D_B = \\begin{pmatrix} x_B \\\\ y_B \\end{pmatrix}$.\n    b. Compute the sample covariance matrices $\\hat{\\Sigma}_A = \\operatorname{Cov}(D_A)$ and $\\hat{\\Sigma}_B = \\operatorname{Cov}(D_B)$.\n    c. Calculate the maximum absolute difference between the elements of the two sample matrices: $\\Delta = \\max_{i,j} \\left| (\\hat{\\Sigma}_A)_{ij} - (\\hat{\\Sigma}_B)_{ij} \\right|$.\n    d. The a boolean result is returned, which is `True` if $\\Delta < \\tau$ and `False` otherwise.\nThis structured approach allows us to numerically validate the theoretical finding that distinct causal structures can be indistinguishable from their second-order statistics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_test_case(var_x, var_y, rho, n, seed, tau):\n    \"\"\"\n    Executes a single test case for comparing Model A and Model B.\n\n    Args:\n        var_x (float): Target variance of variable x.\n        var_y (float): Target variance of variable y.\n        rho (float): Target correlation coefficient between x and y.\n        n (int): Number of samples to generate.\n        seed (int): Seed for the random number generator.\n        tau (float): Tolerance for indistinguishability.\n\n    Returns:\n        bool: True if the models are indistinguishable within tolerance, False otherwise.\n    \"\"\"\n    \n    # Initialize a random number generator with the specified seed.\n    rng = np.random.default_rng(seed)\n\n    # Calculate standard deviations from variances.\n    sigma_x = np.sqrt(var_x)\n    sigma_y = np.sqrt(var_y)\n    \n    # --- Model A (y -> x) ---\n    # Equations:\n    # x = alpha * y + eps_x\n    # y = eps_y\n    # Derived parameters:\n    # alpha = rho * sigma_x / sigma_y\n    # Var(eps_y) = var_y\n    # Var(eps_x) = var_x * (1 - rho^2)\n\n    # Handle the case sigma_y = 0, though problem constraints prevent it.\n    if sigma_y == 0:\n        alpha = 0.0\n    else:\n        alpha = rho * sigma_x / sigma_y\n    \n    std_eps_y_A = sigma_y\n    std_eps_x_A = sigma_x * np.sqrt(1 - rho**2)\n\n    # Generate noise terms for Model A.\n    eps_y_A = rng.normal(loc=0.0, scale=std_eps_y_A, size=n)\n    eps_x_A = rng.normal(loc=0.0, scale=std_eps_x_A, size=n)\n\n    # Synthesize data for Model A.\n    y_A = eps_y_A\n    x_A = alpha * y_A + eps_x_A\n\n    # --- Model B (x -> y) ---\n    # Equations:\n    # y = beta * x + eps_y_prime\n    # x = eps_x_prime\n    # Derived parameters:\n    # beta = rho * sigma_y / sigma_x\n    # Var(eps_x_prime) = var_x\n    # Var(eps_y_prime) = var_y * (1 - rho^2)\n\n    # Handle the case sigma_x = 0, though problem constraints prevent it.\n    if sigma_x == 0:\n        beta = 0.0\n    else:\n        beta = rho * sigma_y / sigma_x\n\n    std_eps_x_B = sigma_x\n    std_eps_y_B = sigma_y * np.sqrt(1 - rho**2)\n\n    # Generate noise terms for Model B.\n    # Note: These are independent draws from the Model A noises.\n    eps_x_B = rng.normal(loc=0.0, scale=std_eps_x_B, size=n)\n    eps_y_B = rng.normal(loc=0.0, scale=std_eps_y_B, size=n)\n\n    # Synthesize data for Model B.\n    x_B = eps_x_B\n    y_B = beta * x_B + eps_y_B\n\n    # --- Verification ---\n\n    # Arrange data for covariance calculation (rows are variables, columns are observations).\n    data_A = np.vstack((x_A, y_A))\n    data_B = np.vstack((x_B, y_B))\n\n    # Compute the sample covariance matrices.\n    cov_A = np.cov(data_A)\n    cov_B = np.cov(data_B)\n\n    # Calculate the maximum absolute entrywise difference.\n    max_abs_diff = np.max(np.abs(cov_A - cov_B))\n    \n    # Check if the difference is within the specified tolerance.\n    is_indistinguishable = max_abs_diff < tau\n    \n    return is_indistinguishable\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple contains: (var_x, var_y, rho, n, seed, tau)\n    test_cases = [\n        (1.0, 1.0, 0.4, 100000, 0, 2e-3),\n        (1.0, 2.0, 0.0, 100000, 1, 2e-3),\n        (1.0, 1.0, 0.95, 150000, 2, 5e-3),\n    ]\n\n    results = []\n    for case in test_cases:\n        var_x, var_y, rho, n, s, tau = case\n        result = run_test_case(var_x, var_y, rho, n, s, tau)\n        results.append(str(result).lower()) # Convert boolean to lowercase string\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}