{
    "hands_on_practices": [
        {
            "introduction": "This practice dives into the fundamental metric of hyperbolic latent space models: the geodesic distance. Before we can model networks, we must understand how distance behaves in this negatively curved geometry. This exercise will guide you through deriving a crucial approximation for the distance between two nearby points, revealing how radial and angular separations contribute differently to their proximity. ",
            "id": "4290911",
            "problem": "Consider a latent space network model embedded in the two-dimensional hyperbolic plane of constant curvature $-1$, where each node $i$ has polar coordinates $(r_i,\\theta_i)$, with $r_i \\ge 0$ and $\\theta_i \\in [0,2\\pi)$. The hyperbolic geodesic distance $x_{ij}$ between nodes $i$ and $j$ is defined by the hyperbolic law of cosines. Starting from the fundamental identity of hyperbolic geometry,\n$$\\cosh x_{ij}=\\cosh r_i\\,\\cosh r_j-\\sinh r_i\\,\\sinh r_j\\,\\cos\\big(\\Delta\\theta\\big),$$\nwhere $\\Delta\\theta=|\\theta_i-\\theta_j|$ is the angular separation measured in radians, derive an explicit expression for $x_{ij}$ in terms of $r_i$, $r_j$, and $\\Delta\\theta$. Then, assuming $r_i\\neq r_j$, obtain the small-angle approximation of $x_{ij}$ up to and including terms of order $(\\Delta\\theta)^2$.\n\nYour final answer must be a single closed-form analytic expression giving the small-angle approximation of $x_{ij}$ through order $(\\Delta\\theta)^2$, expressed in terms of $r_i$, $r_j$, and $\\Delta\\theta$. Angles must be in radians. Distances are dimensionless in these curvature units.",
            "solution": "The setting is the hyperbolic plane with curvature $-1$, in which geodesic distances in polar coordinates obey the hyperbolic law of cosines. The goal is to invert the given identity to obtain $x_{ij}$ and then to expand for small angular separation $\\Delta\\theta$.\n\nWe begin with the hyperbolic law of cosines,\n$$\\cosh x_{ij}=\\cosh r_i\\,\\cosh r_j-\\sinh r_i\\,\\sinh r_j\\,\\cos(\\Delta\\theta).$$\nThe explicit expression for $x_{ij}$ follows from applying the inverse hyperbolic cosine:\n$$x_{ij}=\\arccosh\\!\\Big(\\cosh r_i\\,\\cosh r_j-\\sinh r_i\\,\\sinh r_j\\,\\cos(\\Delta\\theta)\\Big).$$\n\nTo obtain the small-angle approximation, we consider $\\Delta\\theta\\to 0$ and expand $\\cos(\\Delta\\theta)$ to second order:\n$$\\cos(\\Delta\\theta)=1-\\frac{(\\Delta\\theta)^2}{2}+O\\big((\\Delta\\theta)^4\\big).$$\nSubstituting this into the expression inside the $\\arccosh$, we get\n\\begin{align*}\n\\cosh r_i\\,\\cosh r_j-\\sinh r_i\\,\\sinh r_j\\,\\cos(\\Delta\\theta)\n=\\cosh r_i\\,\\cosh r_j-\\sinh r_i\\,\\sinh r_j\\left(1-\\frac{(\\Delta\\theta)^2}{2}+O\\big((\\Delta\\theta)^4\\big)\\right)\\\\\n=\\Big(\\cosh r_i\\,\\cosh r_j-\\sinh r_i\\,\\sinh r_j\\Big)+\\frac{1}{2}\\,\\sinh r_i\\,\\sinh r_j\\,(\\Delta\\theta)^2+O\\big((\\Delta\\theta)^4\\big).\n\\end{align*}\nUsing the hyperbolic identity $\\cosh a\\,\\cosh b-\\sinh a\\,\\sinh b=\\cosh(a-b)$, the leading term simplifies to\n$$\\cosh(r_i-r_j)+\\frac{1}{2}\\,\\sinh r_i\\,\\sinh r_j\\,(\\Delta\\theta)^2+O\\big((\\Delta\\theta)^4\\big).$$\nTherefore,\n$$x_{ij}=\\arccosh\\!\\Big(\\cosh(r_i-r_j)+\\frac{1}{2}\\,\\sinh r_i\\,\\sinh r_j\\,(\\Delta\\theta)^2+O\\big((\\Delta\\theta)^4\\big)\\Big).$$\nLet us denote $x_0=|r_i-r_j|$, so that $\\cosh x_0=\\cosh(r_i-r_j)$ because $\\cosh$ is even. For $r_i\\neq r_j$, we have $x_00$, and we can use a first-order expansion of $\\arccosh$ around $\\cosh x_0$:\n$$\\arccosh(z)\\approx \\arccosh(\\cosh x_0)+\\left.\\frac{d}{dz}\\arccosh(z)\\right|_{z=\\cosh x_0}\\,(z-\\cosh x_0).$$\nThe derivative of $\\arccosh z$ at $z=\\cosh x_0$ is\n$$\\left.\\frac{d}{dz}\\arccosh(z)\\right|_{z=\\cosh x_0}=\\frac{1}{\\sinh x_0},$$\nsince $\\frac{d}{dz}\\arccosh(z)=\\frac{1}{\\sqrt{z-1}\\sqrt{z+1}}$ and $\\sqrt{(\\cosh^2 x_0-1)}=\\sinh x_0$. With $z=\\cosh(r_i-r_j)+\\frac{1}{2}\\,\\sinh r_i\\,\\sinh r_j\\,(\\Delta\\theta)^2+O((\\Delta\\theta)^4)$, we obtain\n\\begin{align*}\nx_{ij}\n\\approx x_0+\\frac{1}{\\sinh x_0}\\left(\\frac{1}{2}\\,\\sinh r_i\\,\\sinh r_j\\,(\\Delta\\theta)^2\\right)+O\\big((\\Delta\\theta)^4\\big)\\\\\n=|r_i-r_j|+\\frac{\\sinh r_i\\,\\sinh r_j}{2\\,\\sinh|r_i-r_j|}\\,(\\Delta\\theta)^2+O\\big((\\Delta\\theta)^4\\big).\n\\end{align*}\nThus, to second order in the small angle $\\Delta\\theta$, the geodesic distance admits the approximation\n$$x_{ij}\\approx |r_i-r_j|+\\frac{\\sinh r_i\\,\\sinh r_j}{2\\,\\sinh|r_i-r_j|}\\,(\\Delta\\theta)^2.$$\n\nWe note the domain caveat: the above expansion holds for $r_i\\neq r_j$. In the degenerate case $r_i=r_j$, the baseline $x_0=0$ and the correct small-angle behavior is $x_{ij}\\approx \\sqrt{\\sinh^2 r_i\\,(\\Delta\\theta)^2/2}=\\frac{1}{\\sqrt{2}}\\,\\sinh r_i\\,|\\Delta\\theta|$, which follows from $\\arccosh(1+y)\\approx \\sqrt{2y}$ for small $y0$. However, under the stated assumption $r_i\\neq r_j$, the provided second-order expansion is the appropriate small-angle approximation.\n\nFor completeness and connection to latent-space network models, in the asymptotic regime of large radii $r_i,r_j\\gg 1$ with small angle $\\Delta\\theta$, one may use $\\cosh r\\approx \\frac{1}{2}\\exp(r)$ and $\\sinh r\\approx \\frac{1}{2}\\exp(r)$, together with $\\cosh x\\approx \\frac{1}{2}\\exp(x)$ and $1-\\cos(\\Delta\\theta)\\approx \\frac{(\\Delta\\theta)^2}{2}$, to obtain the well-known approximation\n$$x_{ij}\\approx r_i+r_j+2\\ln\\!\\left(\\frac{\\Delta\\theta}{2}\\right),$$\nwhich is widely used in the analysis of hyperbolic latent-space networks. This asymptotic form is distinct from the general small-angle quadratic expansion derived above and applies in the specific large-radius limit.\n\nThe requested final expression is the small-angle approximation through order $(\\Delta\\theta)^2$, valid for $r_i\\neq r_j$.",
            "answer": "$$\\boxed{|r_i-r_j|+\\frac{\\sinh r_i\\,\\sinh r_j}{2\\,\\sinh|r_i-r_j|}\\,(\\Delta\\theta)^2}$$"
        },
        {
            "introduction": "With an understanding of hyperbolic distance, we can now build a network from its geometric embedding. This exercise provides a concrete example of the generative process, where you will calculate the probabilities of connections for a small set of nodes with given hyperbolic coordinates. This practice solidifies the link between a node's position in the latent space and its emergent connectivity patterns in the network. ",
            "id": "4290874",
            "problem": "Consider a latent-space network model in the two-dimensional hyperbolic plane $\\mathbb{H}^2$ with curvature $-1$, where each node $i$ has polar hyperbolic coordinates $(r_i,\\theta_i)$ and the metric is $ds^2 = dr^2 + \\sinh^2(r)\\,d\\theta^2$. The geodesic distance $d_{ij}$ between two nodes $i$ and $j$ at $(r_i,\\theta_i)$ and $(r_j,\\theta_j)$ is defined by the hyperbolic law of cosines:\n$$\n\\cosh(d_{ij}) = \\cosh(r_i)\\cosh(r_j) - \\sinh(r_i)\\sinh(r_j)\\cos(\\Delta\\theta_{ij}),\n$$\nwhere $\\Delta\\theta_{ij} = \\theta_i - \\theta_j$ is measured in radians. Edges are independent and undirected, and the probability of an edge between nodes $i$ and $j$ is given by the logistic kernel\n$$\np(d_{ij}) = \\frac{1}{1 + \\exp\\!\\left(\\frac{d_{ij} - \\mu}{\\sigma}\\right)},\n$$\nwith parameters $\\mu = 2$ and $\\sigma = 1$. There are $5$ nodes with coordinates:\n$$\n(r_1,\\theta_1) = (1,0),\\quad (r_2,\\theta_2) = (3,0),\\quad (r_3,\\theta_3) = (2,\\pi),\\quad (r_4,\\theta_4) = (1,\\pi),\\quad (r_5,\\theta_5) = (2,0).\n$$\nBy convention, self-loops are not allowed, so $p_{ii} = 0$ for all $i$. \n\nCompute the full $5 \\times 5$ matrix $P = (p_{ij})$ of edge probabilities. Then, from this matrix, compute the arithmetic mean of the upper-triangular off-diagonal entries (that is, over all unordered pairs $\\{i,j\\}$ with $ij$). Express your final answer as a single real number rounded to four significant figures.",
            "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n-   **Model**: Latent-space network model in the two-dimensional hyperbolic plane $\\mathbb{H}^2$.\n-   **Curvature**: $-1$.\n-   **Metric**: $ds^2 = dr^2 + \\sinh^2(r)\\,d\\theta^2$.\n-   **Geodesic distance formula**: $\\cosh(d_{ij}) = \\cosh(r_i)\\cosh(r_j) - \\sinh(r_i)\\sinh(r_j)\\cos(\\Delta\\theta_{ij})$, where $\\Delta\\theta_{ij} = \\theta_i - \\theta_j$.\n-   **Edge probability function**: $p(d_{ij}) = \\frac{1}{1 + \\exp\\!\\left(\\frac{d_{ij} - \\mu}{\\sigma}\\right)}$.\n-   **Parameters**: $\\mu = 2$ and $\\sigma = 1$.\n-   **Number of nodes**: $5$.\n-   **Node coordinates**:\n    -   Node 1: $(r_1,\\theta_1) = (1,0)$\n    -   Node 2: $(r_2,\\theta_2) = (3,0)$\n    -   Node 3: $(r_3,\\theta_3) = (2,\\pi)$\n    -   Node 4: $(r_4,\\theta_4) = (1,\\pi)$\n    -   Node 5: $(r_5,\\theta_5) = (2,0)$\n-   **Self-loops**: $p_{ii} = 0$.\n-   **Task**: Compute the $5 \\times 5$ matrix of edge probabilities $P=(p_{ij})$, then compute the arithmetic mean of its upper-triangular off-diagonal entries, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, employing standard definitions and formulas from network science and hyperbolic geometry, specifically the popularity-similarity model. It is well-posed, providing all necessary data and functions to compute a unique solution. The problem is stated objectively with no ambiguity. All provided data are consistent. The problem is therefore deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe solution proceeds in four steps:\n1.  Calculate the geodesic distances $d_{ij}$ between all unique pairs of nodes $\\{i,j\\}$.\n2.  Use the distances to calculate the edge probabilities $p_{ij} = p(d_{ij})$.\n3.  Construct the full probability matrix $P=(p_{ij})$ as requested.\n4.  Compute the arithmetic mean of the upper-triangular off-diagonal entries of $P$.\n\n**1. Geodesic Distance Calculation**\n\nThe distance $d_{ij}$ between two nodes $i$ and $j$ is given by the hyperbolic law of cosines:\n$$\n\\cosh(d_{ij}) = \\cosh(r_i)\\cosh(r_j) - \\sinh(r_i)\\sinh(r_j)\\cos(\\Delta\\theta_{ij})\n$$\nWe can simplify this for two special cases present in the problem:\n-   If $\\Delta\\theta_{ij} = 0$ (or any multiple of $2\\pi$), then $\\cos(\\Delta\\theta_{ij}) = 1$. The formula becomes $\\cosh(d_{ij}) = \\cosh(r_i)\\cosh(r_j) - \\sinh(r_i)\\sinh(r_j) = \\cosh(r_i - r_j)$. Since distance is non-negative, $d_{ij} = |r_i - r_j|$.\n-   If $\\Delta\\theta_{ij} = \\pi$ (or any odd multiple of $\\pi$), then $\\cos(\\Delta\\theta_{ij}) = -1$. The formula becomes $\\cosh(d_{ij}) = \\cosh(r_i)\\cosh(r_j) + \\sinh(r_i)\\sinh(r_j) = \\cosh(r_i + r_j)$. Since radial coordinates $r_i$ are non-negative, $d_{ij} = r_i + r_j$.\n\nThere are $\\binom{5}{2} = 10$ unique pairs of nodes. Let's calculate $d_{ij}$ for each pair with $ij$:\n-   Pair $(1,2)$: $(r_1,\\theta_1)=(1,0), (r_2,\\theta_2)=(3,0)$. $\\Delta\\theta_{12} = 0$. $d_{12} = |r_1 - r_2| = |1 - 3| = 2$.\n-   Pair $(1,3)$: $(r_1,\\theta_1)=(1,0), (r_3,\\theta_3)=(2,\\pi)$. $\\Delta\\theta_{13} = -\\pi$. $d_{13} = r_1 + r_3 = 1 + 2 = 3$.\n-   Pair $(1,4)$: $(r_1,\\theta_1)=(1,0), (r_4,\\theta_4)=(1,\\pi)$. $\\Delta\\theta_{14} = -\\pi$. $d_{14} = r_1 + r_4 = 1 + 1 = 2$.\n-   Pair $(1,5)$: $(r_1,\\theta_1)=(1,0), (r_5,\\theta_5)=(2,0)$. $\\Delta\\theta_{15} = 0$. $d_{15} = |r_1 - r_5| = |1 - 2| = 1$.\n-   Pair $(2,3)$: $(r_2,\\theta_2)=(3,0), (r_3,\\theta_3)=(2,\\pi)$. $\\Delta\\theta_{23} = -\\pi$. $d_{23} = r_2 + r_3 = 3 + 2 = 5$.\n-   Pair $(2,4)$: $(r_2,\\theta_2)=(3,0), (r_4,\\theta_4)=(1,\\pi)$. $\\Delta\\theta_{24} = -\\pi$. $d_{24} = r_2 + r_4 = 3 + 1 = 4$.\n-   Pair $(2,5)$: $(r_2,\\theta_2)=(3,0), (r_5,\\theta_5)=(2,0)$. $\\Delta\\theta_{25} = 0$. $d_{25} = |r_2 - r_5| = |3 - 2| = 1$.\n-   Pair $(3,4)$: $(r_3,\\theta_3)=(2,\\pi), (r_4,\\theta_4)=(1,\\pi)$. $\\Delta\\theta_{34} = 0$. $d_{34} = |r_3 - r_4| = |2 - 1| = 1$.\n-   Pair $(3,5)$: $(r_3,\\theta_3)=(2,\\pi), (r_5,\\theta_5)=(2,0)$. $\\Delta\\theta_{35} = \\pi$. $d_{35} = r_3 + r_5 = 2 + 2 = 4$.\n-   Pair $(4,5)$: $(r_4,\\theta_4)=(1,\\pi), (r_5,\\theta_5)=(2,0)$. $\\Delta\\theta_{45} = \\pi$. $d_{45} = r_4 + r_5 = 1 + 2 = 3$.\n\nThe set of calculated distances is $\\{1, 1, 1, 2, 2, 3, 3, 4, 4, 5\\}$.\n\n**2. Edge Probability Calculation**\n\nThe probability of an edge is given by the logistic function with $\\mu=2$ and $\\sigma=1$:\n$$\np(d) = \\frac{1}{1 + \\exp(d - 2)}\n$$\nWe calculate the probabilities for the unique distance values we found: $1, 2, 3, 4, 5$.\n-   $p(d=1) = \\frac{1}{1 + \\exp(1-2)} = \\frac{1}{1+e^{-1}} = \\frac{e}{e+1}$.\n-   $p(d=2) = \\frac{1}{1 + \\exp(2-2)} = \\frac{1}{1+e^0} = \\frac{1}{2}$.\n-   $p(d=3) = \\frac{1}{1 + \\exp(3-2)} = \\frac{1}{1+e^1} = \\frac{1}{1+e}$.\n-   $p(d=4) = \\frac{1}{1 + \\exp(4-2)} = \\frac{1}{1+e^2}$.\n-   $p(d=5) = \\frac{1}{1 + \\exp(5-2)} = \\frac{1}{1+e^3}$.\n\n**3. Probability Matrix Construction**\n\nThe matrix $P = (p_{ij})$ is symmetric ($p_{ij}=p_{ji}$) with $p_{ii}=0$. We assemble the matrix using the distances calculated in step 1 and the probability functions from step 2.\n- $d_{12}=2 \\implies p_{12} = 1/2$\n- $d_{13}=3 \\implies p_{13} = 1/(1+e)$\n- $d_{14}=2 \\implies p_{14} = 1/2$\n- $d_{15}=1 \\implies p_{15} = e/(e+1)$\n- $d_{23}=5 \\implies p_{23} = 1/(1+e^3)$\n- $d_{24}=4 \\implies p_{24} = 1/(1+e^2)$\n- $d_{25}=1 \\implies p_{25} = e/(e+1)$\n- $d_{34}=1 \\implies p_{34} = e/(e+1)$\n- $d_{35}=4 \\implies p_{35} = 1/(1+e^2)$\n- $d_{45}=3 \\implies p_{45} = 1/(1+e)$\n\nThe full probability matrix $P$ is:\n$$\nP = \n\\begin{pmatrix}\n0  \\frac{1}{2}  \\frac{1}{1+e}  \\frac{1}{2}  \\frac{e}{e+1} \\\\\n\\frac{1}{2}  0  \\frac{1}{1+e^3}  \\frac{1}{1+e^2}  \\frac{e}{e+1} \\\\\n\\frac{1}{1+e}  \\frac{1}{1+e^3}  0  \\frac{e}{e+1}  \\frac{1}{1+e^2} \\\\\n\\frac{1}{2}  \\frac{1}{1+e^2}  \\frac{e}{e+1}  0  \\frac{1}{1+e} \\\\\n\\frac{e}{e+1}  \\frac{e}{e+1}  \\frac{1}{1+e^2}  \\frac{1}{1+e}  0\n\\end{pmatrix}\n$$\n\n**4. Mean of Upper-Triangular Entries**\n\nWe need to compute the arithmetic mean of the $10$ upper-triangular off-diagonal entries ($p_{ij}$ for $ij$).\nThe sum $S$ of these probabilities is:\n$$\nS = p_{12} + p_{13} + p_{14} + p_{15} + p_{23} + p_{24} + p_{25} + p_{34} + p_{35} + p_{45}\n$$\nWe can group the terms by the distance value:\n-   Distance $d=1$ occurs $3$ times ($p_{15}, p_{25}, p_{34}$). Contribution: $3 \\times p(1)$.\n-   Distance $d=2$ occurs $2$ times ($p_{12}, p_{14}$). Contribution: $2 \\times p(2)$.\n-   Distance $d=3$ occurs $2$ times ($p_{13}, p_{45}$). Contribution: $2 \\times p(3)$.\n-   Distance $d=4$ occurs $2$ times ($p_{24}, p_{35}$). Contribution: $2 \\times p(4)$.\n-   Distance $d=5$ occurs $1$ time ($p_{23}$). Contribution: $1 \\times p(5)$.\n\nThe total sum is:\n$$\nS = 3 \\cdot p(1) + 2 \\cdot p(2) + 2 \\cdot p(3) + 2 \\cdot p(4) + 1 \\cdot p(5)\n$$\nSubstituting the probability expressions:\n$$\nS = 3\\left(\\frac{e}{e+1}\\right) + 2\\left(\\frac{1}{2}\\right) + 2\\left(\\frac{1}{1+e}\\right) + 2\\left(\\frac{1}{1+e^2}\\right) + \\frac{1}{1+e^3}\n$$\nA useful identity for the logistic function is $p(d) + p(2\\mu-d)=1$. Here $\\mu=2$, so $p(d)+p(4-d)=1$. Specifically, $p(1)+p(3) = \\frac{e}{e+1} + \\frac{1}{1+e} = \\frac{e+1}{1+e} = 1$. We can use this to simplify the sum:\n$$\nS = 2(p(1) + p(3)) + p(1) + 2p(2) + 2p(4) + p(5)\n$$\n$$\nS = 2(1) + p(1) + 2\\left(\\frac{1}{2}\\right) + 2p(4) + p(5) = 2 + p(1) + 1 + 2p(4) + p(5)\n$$\n$$\nS = 3 + \\frac{e}{1+e} + \\frac{2}{1+e^2} + \\frac{1}{1+e^3}\n$$\nNow, we compute the numerical value:\n$e \\approx 2.71828$\n$ S \\approx 3 + \\frac{2.71828}{3.71828} + \\frac{2}{1 + (2.71828)^2} + \\frac{1}{1 + (2.71828)^3} $\n$ S \\approx 3 + 0.731059 + \\frac{2}{1 + 7.389056} + \\frac{1}{1 + 20.085537} $\n$ S \\approx 3 + 0.731059 + \\frac{2}{8.389056} + \\frac{1}{21.085537} $\n$ S \\approx 3 + 0.731059 + 0.238406 + 0.047426 $\n$ S \\approx 4.016891 $\n\nThe arithmetic mean $\\bar{p}$ is the sum $S$ divided by the number of entries, which is $10$:\n$$\n\\bar{p} = \\frac{S}{10} \\approx \\frac{4.016891}{10} = 0.4016891\n$$\nRounding to four significant figures, we get $0.4017$.",
            "answer": "$$\\boxed{0.4017}$$"
        },
        {
            "introduction": "While generating networks from coordinates is instructive, the real power of latent space models lies in the inverse problem: inferring latent positions from an observed network. This practice introduces the core task of statistical inference by having you implement a gradient ascent algorithm to find the latent positions that best explain a given network's structure. This exercise provides a first-hand look at the optimization techniques that bring these geometric models to life. ",
            "id": "4290846",
            "problem": "Consider a binary undirected network with $N=50$ nodes and no self-loops, modeled by a Euclidean latent space of dimension $d$. Each node $i$ has a latent position $x_i \\in \\mathbb{R}^d$, collected in a matrix $X \\in \\mathbb{R}^{N \\times d}$. The probability of an edge between nodes $i$ and $j$ is a logistic function of their Euclidean distance $r_{ij}$, defined by\n$$\np_{ij} = \\sigma(z_{ij}), \\quad z_{ij} = \\theta - \\alpha \\, r_{ij}, \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}},\n$$\nwhere $\\theta \\in \\mathbb{R}$ is an intercept controlling the baseline connectivity and $\\alpha  0$ is the distance sensitivity. The Euclidean distance is\n$$\nr_{ij} = \\lVert x_i - x_j \\rVert_2.\n$$\nGiven an observed symmetric adjacency matrix $A \\in \\{0,1\\}^{N \\times N}$ with $A_{ij} = A_{ji}$ and $A_{ii} = 0$, the log-likelihood of the latent positions $X$ under the independent edge model is\n$$\n\\mathcal{L}(X; A, \\theta, \\alpha) = \\sum_{1 \\le i  j \\le N} \\left[ A_{ij} \\, z_{ij} - \\log\\!\\bigl(1 + e^{z_{ij}}\\bigr) \\right].\n$$\nYou must perform one iteration of gradient ascent on $\\mathcal{L}$ starting from a random initialization $X^{(0)}$ and compute:\n- the change in log-likelihood $\\Delta \\mathcal{L} = \\mathcal{L}(X^{(1)}) - \\mathcal{L}(X^{(0)})$,\n- the step size $t$ chosen by a backtracking line search satisfying the Armijo condition for ascent:\n$$\n\\mathcal{L}\\!\\left(X^{(0)} + t \\, G^{(0)}\\right) \\ge \\mathcal{L}\\!\\left(X^{(0)}\\right) + c \\, t \\, \\left\\langle G^{(0)}, G^{(0)} \\right\\rangle,\n$$\nwhere $G^{(0)}$ is the gradient of $\\mathcal{L}$ with respect to $X$ evaluated at $X^{(0)}$, $c \\in (0,1)$ is a fixed constant, and $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product on $\\mathbb{R}^{N \\times d}$, that is the sum of element-wise products across all coordinates. Use initial step size $t_0 = 1$, reduction factor $\\rho = 0.5$, Armijo constant $c = 10^{-4}$, and at most $50$ backtracking reductions. The updated positions after one iteration are $X^{(1)} = X^{(0)} + t \\, G^{(0)}$.\n\nTo ensure numerical stability, when computing gradients that involve division by $r_{ij}$, regularize distances with a small constant $\\varepsilon = 10^{-12}$, replacing $r_{ij}$ by $r_{ij} + \\varepsilon$ in denominators. When computing $\\log(1+e^{z_{ij}})$, use a numerically stable form of the softplus function.\n\nYou must generate the observed network $A$ in each test case from ground-truth latent positions $X^\\star$ drawn independently from a standard normal distribution in $\\mathbb{R}^d$. The adjacency is sampled independently for each pair $ij$ with probability $p_{ij} = \\sigma(\\theta - \\alpha \\, r^\\star_{ij})$ and then symmetrized with zero diagonal. The initial positions $X^{(0)}$ are independently drawn from a standard normal distribution in $\\mathbb{R}^d$.\n\nImplement the above for the following test suite of parameter sets, each specified by $(d,\\theta,\\alpha,\\text{net\\_seed},\\text{init\\_seed})$:\n1. $(2, 0.0, 2.0, 42, 123)$,\n2. $(1, -2.0, 1.5, 7, 99)$,\n3. $(3, 2.0, 0.8, 202, 303)$,\n4. $(2, 0.0, 8.0, 314, 2718)$,\n5. $(2, 0.0, 0.1, 1234, 9876)$.\n\nFor each test case, you must compute and output the two quantities $\\Delta \\mathcal{L}$ and $t$ for the single gradient ascent iteration. The final program output must be a single line containing a comma-separated list enclosed in square brackets, aggregating the results for all test cases in order and flattening the pairs. In other words, the output format is\n$$\n[\\Delta \\mathcal{L}_1, t_1, \\Delta \\mathcal{L}_2, t_2, \\Delta \\mathcal{L}_3, t_3, \\Delta \\mathcal{L}_4, t_4, \\Delta \\mathcal{L}_5, t_5].\n$$\nNo physical units, angles, or percentages are involved in this problem, and all computations are in dimensionless quantities. Your implementation must be self-contained and deterministic given the specified seeds, and must not require any external input or files.",
            "solution": "The user-provided problem is valid. It describes a well-posed computational task within the established scientific framework of latent space models for networks. All parameters, variables, and procedures are clearly and objectively defined, allowing for a unique and verifiable solution.\n\n### 1. Model and Log-Likelihood\nThe problem describes a Euclidean latent space model for a binary undirected network of $N=50$ nodes. Each node $i$ is assigned a position $x_i \\in \\mathbb{R}^d$ in a $d$-dimensional latent space. The collection of these positions is denoted by the matrix $X \\in \\mathbb{R}^{N \\times d}$.\n\nThe probability of an edge between two distinct nodes $i$ and $j$ is given by a logistic function of their Euclidean distance $r_{ij} = \\lVert x_i - x_j \\rVert_2$:\n$$\np_{ij} = \\sigma(z_{ij}) = \\frac{1}{1 + e^{-z_{ij}}}\n$$\nwhere the log-odds $z_{ij}$ is a linear function of the distance:\n$$\nz_{ij} = \\theta - \\alpha r_{ij}\n$$\nHere, $\\theta$ is a global intercept parameter that controls the overall density of the network, and $\\alpha  0$ is a scale parameter that dictates how rapidly the connection probability decays with increasing distance.\n\nGiven an observed adjacency matrix $A \\in \\{0,1\\}^{N \\times N}$ (symmetric with zero diagonal), the log-likelihood of the latent positions $X$ is derived from the assumption that all potential edges are independent Bernoulli trials. The log-likelihood is given by:\n$$\n\\mathcal{L}(X; A, \\theta, \\alpha) = \\sum_{1 \\le i  j \\le N} \\log P(\\text{edge } (i,j) | x_i, x_j)\n$$\nUsing the edge probabilities $p_{ij}$, this becomes:\n$$\n\\mathcal{L}(X) = \\sum_{1 \\le i  j \\le N} \\left[ A_{ij} \\log(p_{ij}) + (1-A_{ij}) \\log(1-p_{ij}) \\right]\n$$\nSubstituting $p_{ij} = \\frac{e^{z_{ij}}}{1+e^{z_{ij}}}$ and $1-p_{ij} = \\frac{1}{1+e^{z_{ij}}}$, a standard algebraic simplification yields the form provided in the problem statement:\n$$\n\\mathcal{L}(X) = \\sum_{1 \\le i  j \\le N} \\left[ A_{ij} z_{ij} - \\log(1 + e^{z_{ij}}) \\right]\n$$\nThe term $\\log(1 + e^{z})$ is also known as the softplus function.\n\n### 2. Gradient of the Log-Likelihood\nTo perform gradient ascent, we must compute the gradient of the log-likelihood $\\mathcal{L}$ with respect to the latent positions $X$. The gradient is an $N \\times d$ matrix $G = \\nabla_X \\mathcal{L}$, where each row $G_k$ is the gradient with respect to the coordinates of node $k$, $x_k \\in \\mathbb{R}^d$.\n\nThe position $x_k$ influences all terms $\\mathcal{L}_{kj}$ in the summation where $j \\ne k$. Using the chain rule, we compute the partial derivative of a single term $\\mathcal{L}_{kj} = A_{kj} z_{kj} - \\log(1+e^{z_{kj}})$ with respect to $x_k$:\n$$\n\\nabla_{x_k} \\mathcal{L}_{kj} = \\frac{\\partial \\mathcal{L}_{kj}}{\\partial z_{kj}} \\frac{\\partial z_{kj}}{\\partial r_{kj}} \\nabla_{x_k} r_{kj}\n$$\nThe individual components are:\n1.  $\\frac{\\partial \\mathcal{L}_{kj}}{\\partial z_{kj}} = A_{kj} - \\frac{e^{z_{kj}}}{1 + e^{z_{kj}}} = A_{kj} - \\sigma(z_{kj}) = A_{kj} - p_{kj}$\n2.  $\\frac{\\partial z_{kj}}{\\partial r_{kj}} = \\frac{\\partial}{\\partial r_{kj}}(\\theta - \\alpha r_{kj}) = -\\alpha$\n3.  $\\nabla_{x_k} r_{kj} = \\nabla_{x_k} \\lVert x_k - x_j \\rVert_2 = \\frac{x_k - x_j}{\\lVert x_k - x_j \\rVert_2} = \\frac{x_k - x_j}{r_{kj}}$\n\nCombining these results, the gradient contribution from the pair $(k,j)$ is:\n$$\n\\nabla_{x_k} \\mathcal{L}_{kj} = (A_{kj} - p_{kj}) (-\\alpha) \\frac{x_k - x_j}{r_{kj}} = \\alpha (p_{kj} - A_{kj}) \\frac{x_k - x_j}{r_{kj}}\n$$\nThe full gradient for node $k$ is the sum over all other nodes $j$:\n$$\nG_k = \\nabla_{x_k} \\mathcal{L} = \\sum_{j \\ne k} \\alpha (p_{kj} - A_{kj}) \\frac{x_k - x_j}{r_{kj}}\n$$\nTo prevent division by zero for nodes at the same position, we regularize the denominator as specified, replacing $r_{kj}$ with $r_{kj} + \\varepsilon$ for a small constant $\\varepsilon = 10^{-12}$:\n$$\nG_k = \\alpha \\sum_{j \\ne k} (p_{kj} - A_{kj}) \\frac{x_k - x_j}{r_{kj} + \\varepsilon}\n$$\nFor efficient computation, this can be vectorized. Let $S$ be an $N \\times N$ matrix with elements $S_{ij} = \\alpha (p_{ij} - A_{ij}) / (r_{ij} + \\varepsilon)$ for $i \\ne j$ and $S_{ii} = 0$. The gradient matrix $G \\in \\mathbb{R}^{N \\times d}$ can be expressed as:\n$$\nG = \\text{diag}(S \\mathbf{1}) X - S X\n$$\nwhere $\\mathbf{1}$ is a column vector of ones, so $\\text{diag}(S \\mathbf{1})$ is a diagonal matrix whose $k$-th diagonal entry is the sum of the $k$-th row of $S$.\n\n### 3. Gradient Ascent with Backtracking Line Search\nThe gradient ascent algorithm updates the latent positions to increase the log-likelihood. A single step is given by:\n$$\nX^{(1)} = X^{(0)} + t \\, G^{(0)}\n$$\nwhere $X^{(0)}$ are the initial positions, $G^{(0)} = \\nabla_X \\mathcal{L}(X^{(0)})$ is the gradient at $X^{(0)}$, and $t0$ is the step size.\n\nThe step size $t$ is determined using a backtracking line search to ensure sufficient progress. Starting with an initial guess $t=t_0=1$, we iteratively reduce $t$ by a factor $\\rho=0.5$ until the Armijo condition for ascent is met:\n$$\n\\mathcal{L}(X^{(0)} + t G^{(0)}) \\ge \\mathcal{L}(X^{(0)}) + c t \\langle G^{(0)}, G^{(0)} \\rangle\n$$\nHere, $c=10^{-4}$ is a small constant, and $\\langle G^{(0)}, G^{(0)} \\rangle = \\sum_{i,k} (G^{(0)}_{ik})^2$ is the squared Frobenius norm of the gradient matrix. This condition prevents the step from being too large, which could decrease the likelihood, while still ensuring a non-trivial increase proportional to the step size and squared gradient magnitude. The search terminates after at most $50$ reductions.\n\n### 4. Implementation Strategy\nThe overall algorithm for each test case is as follows:\n1.  **Generate Data**: Set the random seed to `net_seed`. Generate ground-truth positions $X^\\star \\in \\mathbb{R}^{N \\times d}$ from a standard normal distribution. Compute pairwise distances $r^\\star_{ij}$ and probabilities $p_{ij} = \\sigma(\\theta - \\alpha r^\\star_{ij})$. Sample the upper triangle of the adjacency matrix $A$ from Bernoulli distributions with these probabilities, then symmetrize $A$ and set its diagonal to zero.\n2.  **Initialize Positions**: Set the random seed to `init_seed`. Generate the initial positions for the optimization, $X^{(0)} \\in \\mathbb{R}^{N \\times d}$, from a standard normal distribution.\n3.  **Perform One Ascent Step**:\n    a. Calculate the initial log-likelihood $\\mathcal{L}^{(0)} = \\mathcal{L}(X^{(0)})$.\n    b. Compute the gradient matrix $G^{(0)} = \\nabla_X \\mathcal{L}(X^{(0)})$ using the vectorized formula.\n    c. Execute the backtracking line search to find the step size $t$.\n    d. Update the positions: $X^{(1)} = X^{(0)} + t G^{(0)}$.\n    e. Calculate the new log-likelihood $\\mathcal{L}^{(1)} = \\mathcal{L}(X^{(1)})$.\n4.  **Store Results**: Compute the change in log-likelihood $\\Delta \\mathcal{L} = \\mathcal{L}^{(1)} - \\mathcal{L}^{(0)}$ and store the pair $(\\Delta \\mathcal{L}, t)$.\n\nFor numerical stability, the log-likelihood calculation for $\\log(1+e^z)$ will use the `numpy.logaddexp(0, z)` function, which is a numerically robust implementation of the softplus function. The gradient calculation will use the regularization constant $\\varepsilon=10^{-12}$ in denominators as specified. The `scipy.spatial.distance.pdist` and `squareform` functions will be used for efficient distance matrix computation, and `scipy.special.expit` will be used for the sigmoid function.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient ascent simulation for the given test cases.\n    \"\"\"\n    # Problem constants\n    N = 50\n    EPSILON = 1e-12\n    # Backtracking line search parameters\n    T_0 = 1.0\n    RHO = 0.5\n    C_ARMIJO = 1e-4\n    MAX_REDUCTIONS = 50\n\n    test_cases = [\n        (2, 0.0, 2.0, 42, 123),\n        (1, -2.0, 1.5, 7, 99),\n        (3, 2.0, 0.8, 202, 303),\n        (2, 0.0, 8.0, 314, 2718),\n        (2, 0.0, 0.1, 1234, 9876),\n    ]\n\n    results = []\n\n    def compute_log_likelihood(X, A, theta, alpha):\n        \"\"\"Computes the log-likelihood of the latent space model.\"\"\"\n        # Pairwise distances\n        # For d=1, pdist needs reshaping\n        if X.shape[1] == 1:\n            r_vec = pdist(X.reshape(-1, 1), 'euclidean')\n        else:\n            r_vec = pdist(X, 'euclidean')\n        R = squareform(r_vec)\n        \n        # Linear predictor z_ij\n        Z = theta - alpha * R\n        \n        # Extract upper triangle indices to avoid double counting\n        iu = np.triu_indices(N, k=1)\n        \n        # Log-likelihood calculation using numerically stable softplus\n        A_upper = A[iu]\n        Z_upper = Z[iu]\n        \n        log_L = np.sum(A_upper * Z_upper - np.logaddexp(0, Z_upper))\n        return log_L\n\n    def compute_gradient(X, A, theta, alpha):\n        \"\"\"Computes the gradient of the log-likelihood w.r.t. X.\"\"\"\n        d = X.shape[1]\n\n        # Pairwise distances\n        if d == 1:\n            r_vec = pdist(X.reshape(-1, 1), 'euclidean')\n        else:\n            r_vec = pdist(X, 'euclidean')\n        R = squareform(r_vec)\n        \n        # Linear predictor and probabilities\n        Z = theta - alpha * R\n        P = expit(Z)\n        \n        # Regularized denominator for the gradient\n        R_reg = R + EPSILON\n        \n        # S matrix for vectorized gradient calculation\n        S = -alpha * (A - P) / R_reg\n        np.fill_diagonal(S, 0)\n        \n        # Vectorized gradient calculation: G = diag(S*1)X - SX\n        row_sums = np.sum(S, axis=1)\n        diag_S1_X = (row_sums * X.T).T\n        SX = S @ X\n        \n        G = diag_S1_X - SX\n        return G\n\n    for d, theta, alpha, net_seed, init_seed in test_cases:\n        # 1. Generate network A from ground-truth positions\n        rng_net = np.random.default_rng(net_seed)\n        X_star = rng_net.standard_normal(size=(N, d))\n        \n        if d == 1:\n            r_star_vec = pdist(X_star.reshape(-1, 1), 'euclidean')\n        else:\n            r_star_vec = pdist(X_star, 'euclidean')\n        \n        p_star_vec = expit(theta - alpha * r_star_vec)\n        \n        A = np.zeros((N, N), dtype=np.int32)\n        iu = np.triu_indices(N, k=1)\n        edge_samples = rng_net.random(size=len(p_star_vec))  p_star_vec\n        A[iu] = edge_samples.astype(np.int32)\n        A = A + A.T\n        \n        # 2. Initialize positions for gradient ascent\n        rng_init = np.random.default_rng(init_seed)\n        X0 = rng_init.standard_normal(size=(N, d))\n        \n        # 3. Perform one iteration of gradient ascent\n        \n        # a. Calculate initial log-likelihood\n        L0 = compute_log_likelihood(X0, A, theta, alpha)\n        \n        # b. Compute gradient\n        G0 = compute_gradient(X0, A, theta, alpha)\n        \n        # c. Backtracking line search for step size t\n        t = T_0\n        G0_norm_sq = np.sum(G0**2)\n        \n        for _ in range(MAX_REDUCTIONS + 1):\n            X_trial = X0 + t * G0\n            L_trial = compute_log_likelihood(X_trial, A, theta, alpha)\n            \n            armijo_rhs = L0 + C_ARMIJO * t * G0_norm_sq\n            \n            if L_trial >= armijo_rhs:\n                break\n            \n            t *= RHO\n        \n        # d. Update positions and get final log-likelihood\n        X1 = X0 + t * G0\n        L1 = compute_log_likelihood(X1, A, theta, alpha)\n        \n        # e. Calculate change in log-likelihood\n        delta_L = L1 - L0\n        \n        results.extend([delta_L, t])\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{x:.8f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}