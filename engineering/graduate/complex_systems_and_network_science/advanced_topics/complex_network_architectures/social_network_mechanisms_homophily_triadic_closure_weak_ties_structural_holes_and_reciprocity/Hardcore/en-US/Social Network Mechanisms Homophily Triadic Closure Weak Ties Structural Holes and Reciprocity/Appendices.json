{
    "hands_on_practices": [
        {
            "introduction": "Homophily, the principle that individuals tend to associate with similar others, is a primary driver of social structure. While often measured as a static property of a network, it is crucial to understand it as a dynamic outcome of individual choices. This practice  provides a hands-on way to explore this dynamic aspect by asking you to calculate the minimum number of edge rewirings needed to reduce a network's overall homophily. By connecting a macro-level pattern to micro-level changes, this exercise builds intuition about the constraints and possibilities of network evolution.",
            "id": "4303436",
            "problem": "Consider an undirected, simple graph with node attribute types indexed by $i \\in \\{1,\\dots,K\\}$. Let the degree sequence be given as pairs $(d_v,g_v)$ for each node $v$, where $d_v \\in \\mathbb{Z}_{\\ge 0}$ is the degree and $g_v \\in \\{1,\\dots,K\\}$ is the attribute type. Define the total number of edges as $m = \\frac{1}{2}\\sum_v d_v$. Let $S_i = \\sum_{v:g_v=i} d_v$ be the total number of stubs in group $i$, and define the group stub share $a_i = \\frac{S_i}{2m}$. Define a symmetric mixing matrix $M = (m_{ij})_{i,j=1}^K$ whose entries count edges in the following standard undirected convention: each same-type edge contributes $1$ to $m_{ii}$, and each cross-type edge between $i \\ne j$ contributes $\\frac{1}{2}$ to both $m_{ij}$ and $m_{ji}$. Thus the normalization $\\sum_{i,j} m_{ij} = m$ holds, and the row sums satisfy $\\sum_j m_{ij} = a_i m$.\n\nThe global inbreeding homophily (equivalently, the attribute assortativity coefficient of Newman) is defined by\n$$\nH \\equiv r = \\frac{\\operatorname{Tr}(E) - \\sum_{i=1}^K a_i^2}{1 - \\sum_{i=1}^K a_i^2},\n$$\nwhere $E = M/m$ is the normalized mixing matrix and $\\operatorname{Tr}(E) = \\sum_{i=1}^K E_{ii}$ is the fraction of same-type edges.\n\nA degree-preserving rewiring via a double-edge swap selects two edges $(u,v)$ and $(x,y)$ and rewires them to $(u,y)$ and $(x,v)$, provided no self-loops or multi-edges are created. Each such swap preserves all node degrees. Under this operation, the number of same-type edges can change by at most $2$ per swap.\n\nStarting from a given consistent $(M,\\{(d_v,g_v)\\})$, your task is to compute the minimal number of double-edge swaps required to reduce the inbreeding homophily by at least a specified amount $\\Delta H \\ge 0$, while preserving the degree sequence. If the requested reduction is infeasible given the group stub shares, return $-1$.\n\nYou must base your derivation on the following well-tested facts:\n- The configuration model preserves degree sequences under double-edge swaps.\n- The Newman assortativity coefficient $r$ for discrete attributes in undirected networks is given by $r = \\frac{\\operatorname{Tr}(E) - \\sum_i a_i^2}{1 - \\sum_i a_i^2}$, with $a_i$ equal to the row sums of $E$.\n- Each double-edge swap can decrease the number of same-type edges by at most $2$.\n- For fixed $a_i$, the minimum achievable diagonal mass of $E$ is bounded below by\n$$\n\\operatorname{Tr}(E)_{\\min} \\ge \\sum_{i=1}^K \\max\\left(0,\\,2a_i - 1\\right),\n$$\nwhich follows from row-sum capacity constraints in the symmetric transportation polytope.\n\nFrom first principles, derive a sound approach to decide feasibility and compute the minimal swap count. Your program must implement this approach exactly.\n\nInput to your program is embedded as a test suite within the code. For each test case, you are given:\n- The mixing matrix $M$ as a list of lists specifying $m_{ij}$ for $i,j \\in \\{1,\\dots,K\\}$, with $\\sum_{i,j} m_{ij} = m$.\n- The degree sequence with group labels as a list of pairs $(d_v,g_v)$.\n- The requested homophily reduction $\\Delta H$ (a nonnegative real number).\n\nYour program must output, for each test case, a single integer equal to the minimal number of swaps required, or $-1$ if infeasible. The final output must be a single line with a comma-separated Python list of the results for all test cases, in the specified order.\n\nTest suite:\n- Test $1$ (happy path, feasible): $K=2$, $M = \\begin{bmatrix}6  1 \\\\ 1  2\\end{bmatrix}$ with total edges $m = 10$, degrees and groups $\\{(2,1),(3,1),(3,1),(3,1),(3,1),(2,2),(1,2),(1,2),(2,2)\\}$ so that $S_1 = 14$, $S_2 = 6$, hence $a_1 = 0.7$, $a_2 = 0.3$. Requested $\\Delta H = 0.2$.\n- Test $2$ (boundary, zero reduction): Same $M$ and degrees as Test $1$, with $\\Delta H = 0$.\n- Test $3$ (infeasible request): Same $M$ and degrees as Test $1$, with $\\Delta H = 1.0$.\n- Test $4$ (three groups, dominant group, feasible): $K=3$, $M = \\begin{bmatrix}5  2  1 \\\\ 2  1  0 \\\\ 1  0  0\\end{bmatrix}$ with total edges $m = 12$, degrees and groups $\\{(3,1),(3,1),(3,1),(3,1),(4,1),(2,2),(2,2),(1,2),(1,2),(2,3)\\}$ so that $S_1 = 16$, $S_2 = 6$, $S_3 = 2$, hence $a_1 = \\frac{16}{24}$, $a_2 = \\frac{6}{24}$, $a_3 = \\frac{2}{24}$. Requested $\\Delta H = 0.1$.\n- Test $5$ (all edges same-type initially, feasible): $K=2$, $M = \\begin{bmatrix}5  0 \\\\ 0  5\\end{bmatrix}$ with total edges $m = 10$, degrees and groups $\\{(2,1),(2,1),(2,1),(2,1),(2,1),(2,2),(2,2),(2,2),(2,2),(2,2)\\}$ so that $S_1 = 10$, $S_2 = 10$, hence $a_1 = 0.5$, $a_2 = 0.5$. Requested $\\Delta H = 0.1$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_t$ is the integer minimal swap count for test $t$ or $-1$ if infeasible. No other text should be printed.",
            "solution": "The problem asks for the minimum number of double-edge swaps required to reduce the inbreeding homophily coefficient, $H$, by at least a given amount $\\Delta H$. The solution must be derived from first principles using the provided facts. The core of the solution is to relate the change in homophily to the change in the number of same-type edges, and then determine the number of swaps based on the maximum possible change per swap.\n\nFirst, we analyze the relationship between the homophily coefficient $H$ and the network's structural properties. The homophily $H$ (or Newman's assortativity coefficient $r$) is given by:\n$$\nH = \\frac{\\operatorname{Tr}(E) - \\sum_{i=1}^K a_i^2}{1 - \\sum_{i=1}^K a_i^2}\n$$\nwhere $E = M/m$ is the normalized mixing matrix, $m$ is the total number of edges, and $a_i$ is the fraction of stubs belonging to nodes of type $i$. Double-edge swaps are degree-preserving operations. Consequently, the degree sequence $\\{(d_v, g_v)\\}$, the total number of stubs for each group $S_i = \\sum_{v:g_v=i} d_v$, the total number of edges $m=\\frac{1}{2}\\sum_v d_v$, and the group stub shares $a_i = S_i/(2m)$ are all invariant under such rewiring.\n\nThe denominator of the homophily expression, $C = 1 - \\sum_{i=1}^K a_i^2$, is therefore a constant throughout the rewiring process. The homophily $H$ becomes a linear function of the trace of the normalized mixing matrix, $\\operatorname{Tr}(E)$:\n$$\nH(\\operatorname{Tr}(E)) = \\frac{1}{C} \\operatorname{Tr}(E) - \\frac{\\sum_{i=1}^K a_i^2}{C}\n$$\nA desired reduction in homophily from an initial value $H_0$ to a final value $H_f$ such that $H_f \\le H_0 - \\Delta H$ corresponds to a change in the trace. Let $\\operatorname{Tr}(E)_0$ and $\\operatorname{Tr}(E)_f$ be the initial and final traces, respectively. The required reduction in trace is given by:\n$$\n\\Delta\\operatorname{Tr}(E) = \\operatorname{Tr}(E)_0 - \\operatorname{Tr}(E)_f \\ge \\Delta H \\cdot \\left(1 - \\sum_{i=1}^K a_i^2\\right)\n$$\nThe trace $\\operatorname{Tr}(E) = \\sum_i E_{ii} = \\sum_i (m_{ii}/m)$ is the fraction of edges that are same-type. Let $W = \\sum_i m_{ii}$ be the total number of same-type edges. Then $\\operatorname{Tr}(E) = W/m$. The required reduction in the number of same-type edges, $\\Delta W = W_0 - W_f$, must satisfy:\n$$\n\\Delta W \\ge m \\cdot \\Delta H \\cdot \\left(1 - \\sum_{i=1}^K a_i^2\\right)\n$$\nLet $T = m \\cdot \\Delta H \\cdot (1 - \\sum_{i=1}^K a_i^2)$ be the target reduction value. Since the number of edges $W$ must be an integer, the actual reduction $\\Delta W$ must also be an integer. The smallest integer reduction that satisfies the condition is $R = \\lceil T \\rceil$.\n\nNext, we establish the feasibility of the requested reduction. The problem states a lower bound on the achievable trace: $\\operatorname{Tr}(E)_{\\min} \\ge \\sum_{i=1}^K \\max(0, 2a_i - 1)$. Any valid rewiring must result in a state where the trace $\\operatorname{Tr}(E)_f$ is not less than this minimum. The target final trace is $\\operatorname{Tr}(E)_f^{\\text{target}} = \\operatorname{Tr}(E)_0 - \\Delta H (1 - \\sum a_i^2)$. The reduction is feasible if and only if:\n$$\n\\operatorname{Tr}(E)_f^{\\text{target}} \\ge \\operatorname{Tr}(E)_{\\min}\n$$\nIf this condition is violated, the requested reduction $\\Delta H$ is unattainable, and the result is defined as $-1$.\n\nFinally, if the reduction is feasible, we compute the minimal number of swaps. The problem states that \"each double-edge swap can decrease the number of same-type edges by at most $2$\". This maximal reduction occurs when two same-type edges from different groups are rewired into two cross-type edges. To achieve the required integer reduction $R$ in the minimal number of swaps, we must assume each swap is maximally efficient. Let $N$ be the number of swaps. The total reduction in same-type edges is at most $2N$. We require this maximum possible reduction to be at least the minimally required integer reduction $R$:\n$$\n2N \\ge R \\implies N \\ge \\frac{R}{2}\n$$\nSince the number of swaps $N$ must be an integer, the minimal number of swaps is $N_{\\min} = \\lceil R/2 \\rceil$.\n\nThis leads to the following algorithm:\n1.  From the input degree sequence $\\{(d_v, g_v)\\}$, calculate the total number of edges $m$, the number of groups $K$, and the stub share $a_i$ for each group $i \\in \\{1,\\dots,K\\}$.\n2.  From the input mixing matrix $M$, calculate the initial number of same-type edges $W_0 = \\sum_{i=1}^K m_{ii}$ and the initial trace $\\operatorname{Tr}(E)_0 = W_0/m$.\n3.  Calculate the theoretical minimum trace $\\operatorname{Tr}(E)_{\\min} = \\sum_{i=1}^K \\max(0, 2a_i - 1)$.\n4.  Calculate the target final trace $\\operatorname{Tr}(E)_f^{\\text{target}} = \\operatorname{Tr}(E)_0 - \\Delta H (1 - \\sum_{i=1}^K a_i^2)$.\n5.  If $\\operatorname{Tr}(E)_f^{\\text{target}}  \\operatorname{Tr}(E)_{\\min}$, the reduction is infeasible. Return $-1$.\n6.  If $\\Delta H \\le 0$, no reduction is needed. Return $0$.\n7.  Otherwise, calculate the target reduction in same-type edge count: $T = m \\cdot \\Delta H \\cdot (1 - \\sum_{i=1}^K a_i^2)$.\n8.  Determine the minimal integer reduction required: $R = \\lceil T \\rceil$.\n9.  Compute the minimum number of swaps: $N = \\lceil R/2 \\rceil$. Return $N$.\nThis approach directly implements the logic derived from the provided principles.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves for the minimal number of swaps to reduce homophily for a series of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"M\": np.array([[6, 1], [1, 2]]),\n            \"degree_groups\": [(2, 1), (3, 1), (3, 1), (3, 1), (3, 1), (2, 2), (1, 2), (1, 2), (2, 2)],\n            \"delta_H\": 0.2\n        },\n        {\n            \"M\": np.array([[6, 1], [1, 2]]),\n            \"degree_groups\": [(2, 1), (3, 1), (3, 1), (3, 1), (3, 1), (2, 2), (1, 2), (1, 2), (2, 2)],\n            \"delta_H\": 0.0\n        },\n        {\n            \"M\": np.array([[6, 1], [1, 2]]),\n            \"degree_groups\": [(2, 1), (3, 1), (3, 1), (3, 1), (3, 1), (2, 2), (1, 2), (1, 2), (2, 2)],\n            \"delta_H\": 1.0\n        },\n        {\n            \"M\": np.array([[5, 2, 1], [2, 1, 0], [1, 0, 0]]),\n            \"degree_groups\": [(3, 1), (3, 1), (3, 1), (3, 1), (4, 1), (2, 2), (2, 2), (1, 2), (1, 2), (2, 3)],\n            \"delta_H\": 0.1\n        },\n        {\n            \"M\": np.array([[5, 0], [0, 5]]),\n            \"degree_groups\": [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)],\n            \"delta_H\": 0.1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        M = case[\"M\"]\n        degree_groups = case[\"degree_groups\"]\n        delta_H = case[\"delta_H\"]\n        \n        result = calculate_min_swaps(M, degree_groups, delta_H)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_min_swaps(M, degree_groups, delta_H):\n    \"\"\"\n    Calculates the minimum number of swaps for a single test case.\n\n    Args:\n        M (np.ndarray): The mixing matrix.\n        degree_groups (list): A list of (degree, group) tuples for each node.\n        delta_H (float): The requested homophily reduction.\n\n    Returns:\n        int: The minimal number of swaps, or -1 if infeasible.\n    \"\"\"\n    # Step 1: Calculate m, S_i, a_i, and K from the degree sequence.\n    # These are invariants under degree-preserving rewiring.\n    stubs_per_group = {}\n    total_degree = 0\n    max_group_id = 0\n    for degree, group in degree_groups:\n        stubs_per_group[group] = stubs_per_group.get(group, 0) + degree\n        total_degree += degree\n        if group > max_group_id:\n            max_group_id = group\n            \n    m = total_degree / 2.0\n    \n    # If there are no edges, no swaps are possible or needed.\n    if m == 0:\n        return 0\n        \n    K = max_group_id\n    a = np.zeros(K)\n    for group_id, stubs_count in stubs_per_group.items():\n        # Group IDs are 1-based, array indices are 0-based\n        a[group_id - 1] = stubs_count / (2.0 * m)\n\n    # Step 2: Calculate auxiliary quantities for the homophily formula.\n    sum_a_sq = np.sum(a**2)\n    # The denominator C. If C = 0, homophily is ill-defined or cannot change.\n    # The problem structure implies C > 0.\n    C = 1.0 - sum_a_sq\n    if C = 1e-9: # Handle case of perfect structural balance or single group\n      return 0 if delta_H = 1e-9 else -1\n    \n    # Step 3: Feasibility Check. The target state must be reachable.\n    # Initial trace of the normalized mixing matrix E.\n    initial_same_type_edges = np.trace(M)\n    Tr_E_initial = initial_same_type_edges / m\n\n    # Theoretical minimum trace based on group stub shares.\n    Tr_E_min = np.sum(np.maximum(0, 2 * a - 1))\n\n    # The target trace after reduction.\n    target_Tr_E = Tr_E_initial - delta_H * C\n\n    # If the target trace is below the theoretical minimum, the reduction is infeasible.\n    if target_Tr_E  Tr_E_min - 1e-9: # Using a small tolerance for float comparison\n        return -1\n\n    # Step 4: Calculate the minimal number of swaps.\n    # A non-positive reduction requires no action.\n    if delta_H = 0:\n        return 0\n    \n    # Calculate the required reduction in the raw count of same-type edges.\n    target_edge_reduction = m * delta_H * C\n    \n    # The number of edges is an integer, so the reduction must be an integer.\n    # We need to reduce the count by at least target_edge_reduction.\n    required_integer_reduction = np.ceil(target_edge_reduction)\n    \n    # Each swap can reduce the same-type edge count by at most 2.\n    # The minimal number of swaps is thus ceil(required_reduction / 2).\n    min_swaps = np.ceil(required_integer_reduction / 2.0)\n    \n    return int(min_swaps)\n\nsolve()\n```"
        },
        {
            "introduction": "From the global tendency of homophily, we now turn to the local environment surrounding an individual. The structure of an ego's personal network critically determines their access to information and resources. This exercise  asks you to perform a concrete calculation of Burt's structural constraint, a key concept that quantifies the redundancy in an ego's relationships. By working through this numerical example, you will gain a tangible understanding of how the abstract ideas of structural holes and triadic closure are operationalized and measured.",
            "id": "4303500",
            "problem": "Consider an ego-centric, weighted, directed social network that models information or resource allocation among actors. Let the ego be actor $i$ with alters $A$, $B$, and $C$. For any ordered pair of actors $(a,b)$, let $w_{ab}$ denote the fraction of $a$’s total outgoing investment directed to $b$. Assume there are no self-ties, so $w_{aa}=0$ for all actors $a$, and that all outgoing weights are nonnegative and sum to one for each actor. Define the normalized exposure (proportion) $p_{ab}$ as the actor’s relative investment, $p_{ab} = w_{ab} / \\sum_{k} w_{ak}$, so that $p_{ab}$ equals $w_{ab}$ whenever the outgoing weights of actor $a$ already sum to one.\n\nThe ego’s outgoing weights are given by $w_{iA} = 0.5$, $w_{iB} = 0.3$, and $w_{iC} = 0.2$, and satisfy $\\sum_{j \\in \\{A,B,C\\}} w_{ij} = 1$. Each alter’s outgoing weights toward the other alters satisfy the same normalization and are given by\n- $w_{A B} = 0.6$ and $w_{A C} = 0.4$,\n- $w_{B A} = 0.2$ and $w_{B C} = 0.8$,\n- $w_{C A} = 0.7$ and $w_{C B} = 0.3$.\n\nUsing the fundamental definition that the ego’s exposure to alter $j$ consists of direct exposure via $i \\to j$ and indirect exposure via two-step paths $i \\to q \\to j$ through neighbors $q \\in \\{A,B,C\\}$, compute the detailed constraint components $c_{ij}$ for $j \\in \\{A,B,C\\}$ and the ego’s total network constraint $C_{i}$ as the sum of these components over all alters. Based on these values, interpret qualitatively how each alter contributes to ego $i$’s constraint in terms of triadic closure, weak ties, and structural holes.\n\nExpress your final answer for the total network constraint $C_{i}$ as a unitless decimal and round your final answer to four significant figures.",
            "solution": "The problem is first validated for scientific soundness, well-posedness, and objectivity.\n\n### Step 1: Extract Givens\n- Network is ego-centric, weighted, and directed.\n- Ego: actor $i$. Alters: $j \\in \\{A, B, C\\}$.\n- Weight $w_{ab}$: fraction of actor $a$'s total outgoing investment to $b$.\n- No self-ties: $w_{aa}=0$.\n- Normalization: $\\sum_{b} w_{ab} = 1$ for any actor $a$.\n- Normalized exposure: $p_{ab} = w_{ab} / \\sum_{k} w_{ak}$, which simplifies to $p_{ab}=w_{ab}$.\n- Ego's outgoing weights: $w_{iA} = 0.5$, $w_{iB} = 0.3$, $w_{iC} = 0.2$.\n- Alters' outgoing weights:\n    - Actor $A$: $w_{A B} = 0.6$, $w_{A C} = 0.4$.\n    - Actor $B$: $w_{B A} = 0.2$, $w_{B C} = 0.8$.\n    - Actor $C$: $w_{C A} = 0.7$, $w_{C B} = 0.3$.\n- Task: Compute constraint components $c_{ij}$ and total constraint $C_i = \\sum_j c_{ij}$.\n- Definition of exposure: \"ego’s exposure to alter $j$ consists of direct exposure via $i \\to j$ and indirect exposure via two-step paths $i \\to q \\to j$ through neighbors $q \\in \\{A,B,C\\}$\".\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the field of network science, specifically invoking the concept of network constraint developed by Ronald Burt. The provided data is internally consistent, with all outgoing weights summing to $1$ as stated. The problem is well-posed, as it provides all necessary values to compute the requested quantities. The only ambiguity lies in the term \"constraint components $c_{ij}$,\" which is not explicitly defined. However, the description of exposure (direct plus indirect paths) perfectly matches the term inside the parenthesis of Burt's formal definition of network constraint. It is standard in this field to define the constraint component $c_{ij}$ that ego $i$ experiences from alter $j$ as the square of the total exposure of $i$ to $j$. The problem phrasing \"Using the fundamental definition...\" implies that this standard definition is to be used. Thus, the problem is deemed valid under the assumption of this standard definition.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed by applying the standard definition of network constraint.\n\n### Solution\nThe network consists of an ego actor, $i$, and a set of three alters, $\\{A, B, C\\}$. The interactions are described by weights $w_{ab}$ representing the fraction of actor $a$'s investment directed to actor $b$. Since the outgoing weights for each actor are normalized to sum to $1$, the normalized exposure $p_{ab}$ is equal to the weight $w_{ab}$.\n\nThe network constraint on ego $i$ from an alter $j$ is a measure of the redundancy of the $i \\to j$ tie. It is fundamentally defined based on the total exposure of $i$ to $j$, which includes the direct tie $p_{ij}$ and all indirect ties through intermediary actors $q$. The constraint component, $c_{ij}$, is the square of this total exposure. The formula, as established by R. Burt, is:\n$$\nc_{ij} = \\left( p_{ij} + \\sum_{q \\neq i, j} p_{iq} p_{qj} \\right)^2\n$$\nHere, $p_{ij}$ represents the direct exposure, and the summation term $\\sum_{q \\neq i, j} p_{iq} p_{qj}$ represents the indirect exposure through all other alters $q$. The total network constraint on ego $i$ is the sum of the components from all alters: $C_i = \\sum_j c_{ij}$.\n\nWe compute the constraint component for each alter $j \\in \\{A, B, C\\}$.\n\n1.  **Constraint from alter A ($c_{iA}$)**\n    The alters that can act as intermediaries are $q \\in \\{B, C\\}$. The formula is:\n    $$\n    c_{iA} = \\left( p_{iA} + p_{iB}p_{BA} + p_{iC}p_{CA} \\right)^2\n    $$\n    Using the given weights where $p_{ab} = w_{ab}$:\n    - Direct exposure: $p_{iA} = 0.5$.\n    - Indirect exposure through $B$: $p_{iB}p_{BA} = (0.3)(0.2) = 0.06$.\n    - Indirect exposure through $C$: $p_{iC}p_{CA} = (0.2)(0.7) = 0.14$.\n    - Total indirect exposure to $A$: $0.06 + 0.14 = 0.20$.\n    The total exposure of $i$ to $A$ is $0.5 + 0.20 = 0.70$.\n    The constraint component is:\n    $$\n    c_{iA} = (0.70)^2 = 0.49\n    $$\n\n2.  **Constraint from alter B ($c_{iB}$)**\n    The alters that can act as intermediaries are $q \\in \\{A, C\\}$. The formula is:\n    $$\n    c_{iB} = \\left( p_{iB} + p_{iA}p_{AB} + p_{iC}p_{CB} \\right)^2\n    $$\n    Using the given weights:\n    - Direct exposure: $p_{iB} = 0.3$.\n    - Indirect exposure through $A$: $p_{iA}p_{AB} = (0.5)(0.6) = 0.30$.\n    - Indirect exposure through $C$: $p_{iC}p_{CB} = (0.2)(0.3) = 0.06$.\n    - Total indirect exposure to $B$: $0.30 + 0.06 = 0.36$.\n    The total exposure of $i$ to $B$ is $0.3 + 0.36 = 0.66$.\n    The constraint component is:\n    $$\n    c_{iB} = (0.66)^2 = 0.4356\n    $$\n\n3.  **Constraint from alter C ($c_{iC}$)**\n    The alters that can act as intermediaries are $q \\in \\{A, B\\}$. The formula is:\n    $$\n    c_{iC} = \\left( p_{iC} + p_{iA}p_{AC} + p_{iB}p_{BC} \\right)^2\n    $$\n    Using the given weights:\n    - Direct exposure: $p_{iC} = 0.2$.\n    - Indirect exposure through $A$: $p_{iA}p_{AC} = (0.5)(0.4) = 0.20$.\n    - Indirect exposure through $B$: $p_{iB}p_{BC} = (0.3)(0.8) = 0.24$.\n    - Total indirect exposure to $C$: $0.20 + 0.24 = 0.44$.\n    The total exposure of $i$ to $C$ is $0.2 + 0.44 = 0.64$.\n    The constraint component is:\n    $$\n    c_{iC} = (0.64)^2 = 0.4096\n    $$\n\nFinally, we compute the total network constraint $C_i$ on the ego by summing these components:\n$$\nC_i = c_{iA} + c_{iB} + c_{iC} = 0.49 + 0.4356 + 0.4096 = 1.3352\n$$\n\n### Qualitative Interpretation\nThe calculated constraint components are $c_{iA} = 0.49$, $c_{iB} \\approx 0.436$, and $c_{iC} \\approx 0.410$. The total constraint is $C_i \\approx 1.335$. A high constraint value indicates that the ego's relationships are redundant and his or her social capital is concentrated within a dense, cohesive group.\n\n-   **Triadic Closure**: All three alters impose significant constraint on ego $i$. This is indicative of strong triadic closure. For each of ego's ties (e.g., $i \\to A$), there are strong indirect paths through other alters (e.g., $i \\to B \\to A$ and $i \\to C \\to A$). This means the alters $A$, $B$, and $C$ are themselves highly interconnected, forming a tight cluster around the ego. The high indirect exposure values ($0.20$ for $A$, $0.36$ for $B$, $0.44$ for $C$) quantify this redundancy. For instance, even the weakest direct tie, $i \\to C$ ($w_{iC}=0.2$), is made highly constraining ($c_{iC}=0.4096$) because of the very strong indirect connections to $C$ via $A$ and $B$.\n\n-   **Structural Holes**: A high total constraint like $C_i = 1.3352$ signifies a lack of structural holes in ego's network. Structural holes are gaps in a social structure, where an ego connects to actors who are not connected to each other. By bridging such a hole, the ego can broker information and control resources. In this network, all of ego's alters are connected to each other, meaning there are no holes for ego to span. Ego $i$ is embedded within a single cohesive component, not bridging disparate groups.\n\n-   **Weak Ties**: According to Granovetter's theory, weak ties are valuable because they often act as bridges to novel information and diverse social circles. In this network, all of ego's ties are \"strong\" in the structural sense that they are heavily embedded and redundant. Even the tie with the lowest weight ($w_{iC}=0.2$) is not a structurally \"weak\" tie in the Granovetterian sense, as it is reinforced by strong triadic relationships. Consequently, ego's network is likely to provide redundant information and limited opportunities for social or economic advantage that arise from brokerage. Alter $A$ contributes the most to this constraint ($c_{iA}=0.49$), largely because it is ego's primary investment and is also well-connected to other alters.\n\nRounding the final answer for $C_i$ to four significant figures gives $1.335$.",
            "answer": "$$\n\\boxed{1.335}\n$$"
        },
        {
            "introduction": "The concepts of tie strength, triadic closure, and structural holes converge in one of social network analysis's most celebrated theories: the 'strength of weak ties'. This theory posits that weak ties, by bridging otherwise disconnected social circles, are invaluable for accessing novel information and opportunities. This final practice  challenges you to emulate the work of a computational social scientist by testing this hypothesis. You will implement a full data analysis pipeline to investigate the relationship between tie strength and a node's potential for brokerage, learning to control for confounding variables and interpret the results of a regression model.",
            "id": "4303487",
            "problem": "You are given weighted, undirected graphs that represent call detail records where each edge weight captures the total number of calls between two individuals over a fixed observation window. The objective is to quantify the association between tie strength and bridging betweenness centrality while controlling for degree, formulated as a regression model, and to implement this pipeline for a specified test suite of graphs. The program to be produced must be self-contained and produce the required output without user input.\n\nFundamental base and definitions:\n- Let a network be represented by a weighted, undirected graph with adjacency matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ where $A_{ij} = A_{ji} \\ge 0$ and $A_{ii} = 0$ for all $i$. An edge exists between nodes $i$ and $j$ if and only if $A_{ij}  0$.\n- The unweighted structural degree of node $i$ is $k_i = \\left|\\{j \\in \\{1,\\dots,n\\} : A_{ij}  0\\}\\right|$.\n- The tie strength of node $i$ is defined as the average weight of its incident edges, $s_i = \\frac{1}{k_i} \\sum_{j : A_{ij}  0} A_{ij}$.\n- Consider weighted shortest paths where each edge $(i,j)$ has length $\\ell_{ij} = \\frac{1}{A_{ij}}$ for $A_{ij}  0$. The distance between nodes $s$ and $t$ is the minimal sum of $\\ell_{uv}$ along any path from $s$ to $t$.\n- The node betweenness centrality $C_B(i)$ for node $i$ is defined by $C_B(i) = \\sum_{s \\ne i \\ne t, s \\ne t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}$, where $\\sigma_{st}$ is the total number of shortest paths between $s$ and $t$, and $\\sigma_{st}(i)$ is the number of those shortest paths that pass through node $i$. For weighted graphs, shortest paths are with respect to lengths $\\ell_{ij}$.\n- The bridging betweenness centrality for node $i$ is defined as $B(i) = \\frac{C_B(i)}{k_i}$, which emphasizes nodes that broker between groups (structural holes) while penalizing hubs with large $k_i$.\n- To estimate the association between tie strength and bridging betweenness centrality while controlling for degree, we employ Ordinary Least Squares (OLS) regression: for each node $i$, set the response $y_i = B(i)$ and predictors $x_{i1} = s_i$ and $x_{i2} = k_i$, with an intercept. The model is $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i$. The OLS estimator is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$, where $\\mathbf{X}$ is the design matrix whose rows are $(1, x_{i1}, x_{i2})$ and $\\mathbf{y}$ is the vector of responses. The coefficient $\\hat{\\beta}_1$ quantifies the association between tie strength and bridging betweenness centrality, controlling for degree.\n\nTask:\n- For each graph in the test suite, compute $k_i$, $s_i$, $C_B(i)$, and $B(i)$ for all nodes using the definitions above, then fit the OLS regression $y_i = \\beta_0 + \\beta_1 s_i + \\beta_2 k_i + \\varepsilon_i$, and extract $\\hat{\\beta}_1$ (the coefficient on $s_i$).\n- All shortest path calculations must be done on weighted graphs with edge lengths $\\ell_{ij} = 1/A_{ij}$.\n- All graphs are undirected and connected, so $k_i \\ge 1$ for all $i$.\n\nTest suite:\nProvide three adjacency matrices, each specified as $\\mathbf{A}^{(m)}$ for test case $m$.\n\n1. Test case $1$ (two cohesive groups bridged by a weak tie), with $n=6$:\n$$\n\\mathbf{A}^{(1)} =\n\\begin{bmatrix}\n0  5  4  0  0  0 \\\\\n5  0  5  0  0  0 \\\\\n4  5  0  1  0  0 \\\\\n0  0  1  0  5  4 \\\\\n0  0  0  5  0  5 \\\\\n0  0  0  4  5  0\n\\end{bmatrix}.\n$$\n\n2. Test case $2$ (star topology with heterogeneous tie strengths), with $n=5$:\n$$\n\\mathbf{A}^{(2)} =\n\\begin{bmatrix}\n0  10  1  2  3 \\\\\n10  0  0  0  0 \\\\\n1  0  0  0  0 \\\\\n2  0  0  0  0 \\\\\n3  0  0  0  0\n\\end{bmatrix}.\n$$\n\n3. Test case $3$ (a tightly closed triad connected to a linear chain by a weak tie), with $n=5$:\n$$\n\\mathbf{A}^{(3)} =\n\\begin{bmatrix}\n0  6  6  0  0 \\\\\n6  0  6  0  0 \\\\\n6  6  0  1  0 \\\\\n0  0  1  0  2 \\\\\n0  0  0  2  0\n\\end{bmatrix}.\n$$\n\nOutput specification:\n- For each test case $m \\in \\{1,2,3\\}$, compute the OLS coefficient $\\hat{\\beta}_1^{(m)}$ on tie strength $s_i$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\texttt{[result1,result2,result3]}$). Each result must be a floating-point number. No additional text should be printed.",
            "solution": "The posed problem requires the computation of a specific regression coefficient that quantifies the relationship between node-level tie strength and bridging betweenness centrality, while controlling for node degree, across a suite of specified graphs. The solution is executed by implementing a computational pipeline based on the provided definitions. This involves a multi-step process for each graph: calculation of basic node properties, determination of all-pairs shortest paths to compute betweenness centrality, and finally, Ordinary Least Squares (OLS) regression.\n\nLet the given weighted, undirected graph be represented by its adjacency matrix $\\mathbf{A}$, where $A_{ij}$ is the weight of the edge between nodes $i$ and $j$.\n\n**Step 1: Computation of Node-Level Structural Properties**\n\nFor each node $i$ in the graph with $n$ nodes, we first compute its fundamental properties: structural degree ($k_i$) and tie strength ($s_i$).\n\n1.  **Structural Degree ($k_i$)**: The degree $k_i$ is the number of direct connections node $i$ has. It is calculated by counting the non-zero entries in the $i$-th row (or column) of the adjacency matrix $\\mathbf{A}$:\n    $$k_i = \\left|\\{j \\in \\{1,\\dots,n\\} : A_{ij}  0\\}\\right|$$\n    The problem states that all graphs are connected, which implies $k_i \\ge 1$ for all $i$.\n\n2.  **Tie Strength ($s_i$)**: The tie strength $s_i$ is defined as the average weight of all edges incident to node $i$. It is computed by summing the weights of these edges and dividing by the node's degree $k_i$:\n    $$s_i = \\frac{1}{k_i} \\sum_{j : A_{ij}  0} A_{ij}$$\n\n**Step 2: Computation of Bridging Betweenness Centrality**\n\nThis step involves two sub-procedures: calculating the node betweenness centrality ($C_B(i)$) and then normalizing it by the degree to obtain the bridging betweenness centrality ($B(i)$).\n\n1.  **Node Betweenness Centrality ($C_B(i)$)**: This metric quantifies how often a node $i$ lies on the shortest paths between other pairs of nodes $(s, t)$. The problem defines it as:\n    $$C_B(i) = \\sum_{s \\ne i \\ne t, s \\ne t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}$$\n    where $\\sigma_{st}$ is the total count of shortest paths between nodes $s$ and $t$, and $\\sigma_{st}(i)$ is the number of such paths that pass through node $i$.\n\n    The shortest path calculations are performed on a graph where edge lengths are inversely proportional to the tie strengths (weights), $\\ell_{ij} = 1/A_{ij}$ for $A_{ij}  0$. This correctly models stronger ties as shorter \"social distances\".\n\n    The computation of $C_B(i)$ for all nodes is most efficiently performed using Brandes' algorithm. This algorithm avoids the computationally expensive enumeration of all paths. It operates by iterating through each node $s$ as a source and performing the following:\n    a.  **Single-Source Shortest Paths (SSSP)**: A modified Dijkstra's algorithm is run from source $s$ to compute the shortest distance $d(s, t)$ and the number of shortest paths $\\sigma_{st}$ to all other nodes $t$. A priority queue is used to explore nodes in increasing order of distance. During this traversal, for each node $v$, we maintain a list of its predecessors $P(v)$ on shortest paths from $s$.\n    b.  **Dependency Accumulation**: After the SSSP phase, nodes are processed in decreasing order of their distance from $s$. For each node $w$, its dependency on the source $s$, denoted $\\delta_s(w)$, is calculated. This represents the sum of fractions of shortest paths from $s$ to other nodes $t$ that pass through $w$. The dependency of a node $v$ is updated based on the dependencies of its successors $w$ on the shortest paths from s:\n    $$\\delta_s(v) = \\sum_{w : v \\in P(w)} \\frac{\\sigma_{sv}}{\\sigma_{sw}}(1 + \\delta_s(w))$$\n    The betweenness centrality $C_B(v)$ is the sum of these dependencies over all possible sources $s$: $C_B(v) = \\sum_{s \\ne v} \\delta_s(v)$. The summation in the definition is over ordered pairs $(s,t)$, which is precisely what Brandes' algorithm computes, so no additional normalization factor (e.g., $1/2$) is required for undirected graphs under this definition.\n\n2.  **Bridging Betweenness Centrality ($B(i)$)**: This metric is derived by normalizing the standard betweenness centrality $C_B(i)$ by the node's degree $k_i$.\n    $$B(i) = \\frac{C_B(i)}{k_i}$$\n    This normalization penalizes high-degree nodes (hubs) and emphasizes nodes that act as bridges between distinct network regions, which often have a lower degree but high centrality.\n\n**Step 3: Ordinary Least Squares (OLS) Regression**\n\nThe final step is to quantify the association between tie strength ($s_i$) and bridging betweenness centrality ($B(i)$) while controlling for degree ($k_i$). This is achieved by fitting a linear regression model. For each node $i$, we define a response variable $y_i = B(i)$ and a set of predictors: $x_{i1} = s_i$ and $x_{i2} = k_i$. The model includes an intercept term $\\beta_0$:\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i$$\nThis can be expressed in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$. The vector of responses is $\\mathbf{y} = [B(0), B(1), \\dots, B(n-1)]^\\top$. The design matrix $\\mathbf{X}$ is an $n \\times 3$ matrix where each row $i$ is $[1, s_i, k_i]$.\n\nThe OLS estimator $\\hat{\\boldsymbol{\\beta}}$ for the coefficient vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^\\top$ is given by the normal equations:\n$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\nThe coefficient of interest is $\\hat{\\beta}_1$, which represents the partial effect of tie strength on bridging betweenness, holding degree constant. This is obtained by constructing the matrices $\\mathbf{X}$ and $\\mathbf{y}$ from the previously computed metrics and solving this system of linear equations, for instance, using `numpy.linalg.solve`.\n\nThis entire pipeline is applied to each of the three test case adjacency matrices provided, yielding three distinct values for the coefficient $\\hat{\\beta}_1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport heapq\n\ndef solve():\n    \"\"\"\n    Main solver function that processes each test case and prints the results.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        np.array([\n            [0, 5, 4, 0, 0, 0],\n            [5, 0, 5, 0, 0, 0],\n            [4, 5, 0, 1, 0, 0],\n            [0, 0, 1, 0, 5, 4],\n            [0, 0, 0, 5, 0, 5],\n            [0, 0, 0, 4, 5, 0]\n        ]),\n        # Test case 2\n        np.array([\n            [0, 10, 1, 2, 3],\n            [10, 0, 0, 0, 0],\n            [1, 0, 0, 0, 0],\n            [2, 0, 0, 0, 0],\n            [3, 0, 0, 0, 0]\n        ]),\n        # Test case 3\n        np.array([\n            [0, 6, 6, 0, 0],\n            [6, 0, 6, 0, 0],\n            [6, 6, 0, 1, 0],\n            [0, 0, 1, 0, 2],\n            [0, 0, 0, 2, 0]\n        ])\n    ]\n\n    results = []\n    for A in test_cases:\n        beta_1 = _process_graph(A)\n        results.append(beta_1)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _process_graph(A):\n    \"\"\"\n    Computes node metrics and performs OLS regression for a single graph.\n    \"\"\"\n    n = A.shape[0]\n\n    # Step 1: Compute k_i and s_i\n    k = np.zeros(n)\n    s = np.zeros(n)\n    for i in range(n):\n        neighbors = np.where(A[i] > 0)[0]\n        k[i] = len(neighbors)\n        if k[i] > 0:\n            s[i] = np.sum(A[i, neighbors]) / k[i]\n\n    # Step 2: Compute C_B(i) and B(i)\n    # Create length matrix L where L_ij = 1/A_ij\n    with np.errstate(divide='ignore'):\n        L = 1.0 / A\n    L[A == 0] = np.inf\n    np.fill_diagonal(L, 0)\n    \n    C_B = _compute_betweenness_centrality(n, A, L)\n    \n    # an explicit check to avoid division by zero although problem statement\n    # ensures k_i >= 1\n    B = np.zeros(n)\n    non_zero_k_indices = np.where(k > 0)[0]\n    B[non_zero_k_indices] = C_B[non_zero_k_indices] / k[non_zero_k_indices]\n\n    # Step 3: Perform OLS Regression\n    # Construct design matrix X and response vector y\n    X = np.ones((n, 3))\n    X[:, 1] = s\n    X[:, 2] = k\n    y = B\n\n    # Solve for beta using np.linalg.solve for numerical stability\n    XTX = X.T @ X\n    XTy = X.T @ y\n    \n    # Handle cases of perfect multicollinearity resulting in a singular matrix\n    if np.linalg.det(XTX) == 0:\n        # Use pseudo-inverse for non-unique solutions.\n        # This is a robust way to handle the linear algebra.\n        beta_hat = np.linalg.pinv(XTX) @ XTy\n    else:\n        beta_hat = np.linalg.solve(XTX, XTy)\n\n    beta_1 = beta_hat[1]\n    return beta_1\n\ndef _compute_betweenness_centrality(n, adj, length_matrix):\n    \"\"\"\n    Computes node betweenness centrality for a weighted graph using Brandes' algorithm.\n    \"\"\"\n    betweenness = np.zeros(n, dtype=np.float64)\n    for s_node in range(n):\n        # SSSP phase using Dijkstra's algorithm\n        stack = []\n        predecessors = [[] for _ in range(n)]\n        sigma = np.zeros(n, dtype=np.float64)\n        sigma[s_node] = 1.0\n        dist = np.full(n, np.inf, dtype=np.float64)\n        dist[s_node] = 0.0\n        \n        pq = [(0.0, s_node)]\n        \n        while pq:\n            d, u = heapq.heappop(pq)\n            \n            if d > dist[u]:\n                continue\n            \n            stack.append(u)\n            \n            # Iterate through neighbors\n            neighbors = np.where(adj[u] > 0)[0]\n            for v in neighbors:\n                l_uv = length_matrix[u, v]\n                # Path relaxation\n                if dist[v] > dist[u] + l_uv:\n                    dist[v] = dist[u] + l_uv\n                    heapq.heappush(pq, (dist[v], v))\n                    sigma[v] = 0.0\n                    predecessors[v] = []\n                \n                # Path counting on shortest path\n                # Use a small tolerance for floating point comparisons\n                if np.isclose(dist[v], dist[u] + l_uv):\n                    sigma[v] += sigma[u]\n                    predecessors[v].append(u)\n\n        # Accumulation phase\n        delta = np.zeros(n, dtype=np.float64)\n        while stack:\n            w = stack.pop()\n            for v_pred in predecessors[w]:\n                if sigma[w] != 0:\n                    delta[v_pred] += (sigma[v_pred] / sigma[w]) * (1.0 + delta[w])\n            if w != s_node:\n                betweenness[w] += delta[w]\n                \n    return betweenness\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}