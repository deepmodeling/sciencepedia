{
    "hands_on_practices": [
        {
            "introduction": "Understanding one-mode projection begins with translating the conceptual idea of shared neighbors into the language of linear algebra. This first exercise  provides a concrete, hands-on calculation to demonstrate how the matrix product $B B^\\top$ elegantly encodes the count of common neighbors for every pair of nodes. By working through this fundamental example, you will build a solid intuition for why this matrix operation is the cornerstone of projection analysis.",
            "id": "4294500",
            "problem": "Consider a bipartite network with node sets $\\mathcal{U}$ and $\\mathcal{V}$, represented by a binary incidence matrix $B \\in \\{0,1\\}^{|\\mathcal{U}| \\times |\\mathcal{V}|}$, where $B_{uv} = 1$ if and only if node $u \\in \\mathcal{U}$ is connected to node $v \\in \\mathcal{V}$, and $B_{uv} = 0$ otherwise. The standard one-mode projection onto $\\mathcal{U}$ is defined so that the weight between nodes $u,u' \\in \\mathcal{U}$ equals the number of common neighbors they share in $\\mathcal{V}$; similarly, the projection onto $\\mathcal{V}$ assigns weights between nodes $v,v' \\in \\mathcal{V}$ equal to the number of common neighbors in $\\mathcal{U}$. These weights can be constructed by the Gram matrices of the row vectors and column vectors of $B$, respectively.\n\nLet\n$$\nB=\\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix},\n$$\nwith $|\\mathcal{U}|=2$ rows and $|\\mathcal{V}|=3$ columns.\n\nUsing only the definitions above and standard linear-algebraic facts, complete the following:\n\n1. Derive the weighted adjacency matrix $W$ of the one-mode projection onto $\\mathcal{U}$ from first principles, and compute it explicitly for the given $B$.\n\n2. Derive the weighted adjacency matrix $W'$ of the one-mode projection onto $\\mathcal{V}$ from first principles, and compute it explicitly for the given $B$.\n\n3. Let $k_{\\mathcal{U}} \\in \\mathbb{R}^{|\\mathcal{U}|}$ and $k_{\\mathcal{V}} \\in \\mathbb{R}^{|\\mathcal{V}|}$ denote the bipartite degree vectors obtained by summing rows and columns of $B$, respectively. Express $k_{\\mathcal{U}}$ and $k_{\\mathcal{V}}$ using matrix-vector products with the all-ones vectors $\\mathbf{1}_{\\mathcal{V}} \\in \\mathbb{R}^{|\\mathcal{V}|}$ and $\\mathbf{1}_{\\mathcal{U}} \\in \\mathbb{R}^{|\\mathcal{U}|}$, and compute them for the given $B$. Prove that the diagonal entries of $W$ equal the entries of $k_{\\mathcal{U}}$, and that the diagonal entries of $W'$ equal the entries of $k_{\\mathcal{V}}$.\n\n4. Define the weighted degree (also called strength) vectors in the projections by $s_{\\mathcal{U}} = W \\,\\mathbf{1}_{\\mathcal{U}}$ and $s_{\\mathcal{V}} = W' \\,\\mathbf{1}_{\\mathcal{V}}$. Derive identities that link $s_{\\mathcal{U}}$ to $B$ and $k_{\\mathcal{V}}$, and $s_{\\mathcal{V}}$ to $B$ and $k_{\\mathcal{U}}$, and compute these vectors explicitly for the given $B$.\n\nAs your final numeric answer, report the scalar\n$$\nD \\equiv \\det(W) + \\operatorname{rank}(W').\n$$\nNo rounding is required. Provide only the value of $D$ as your final answer.",
            "solution": "The problem statement has been validated and is found to be scientifically grounded, well-posed, objective, and internally consistent. It presents a standard exercise in network science, specifically concerning the one-mode projection of bipartite networks, and requires the application of fundamental concepts from linear algebra. All definitions are clear and all necessary data are provided. We may therefore proceed with a complete solution.\n\nThe problem asks for derivations and computations related to the one-mode projections of a bipartite network defined by the incidence matrix\n$$\nB=\\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n$$\nwhere the rows correspond to the node set $\\mathcal{U}$ (with $|\\mathcal{U}|=2$) and the columns to the node set $\\mathcal{V}$ (with $|\\mathcal{V}|=3$).\n\n1. Weighted adjacency matrix $W$ of the projection onto $\\mathcal{U}$.\n\nBy definition, the weight $W_{u,u'}$ between two nodes $u, u' \\in \\mathcal{U}$ is the number of common neighbors they share in $\\mathcal{V}$. A node $v \\in \\mathcal{V}$ is a common neighbor of $u$ and $u'$ if and only if there are edges connecting $u$ to $v$ and $u'$ to $v$. In terms of the incidence matrix $B$, this means $B_{uv}=1$ and $B_{u'v}=1$. Since the entries of $B$ are either $0$ or $1$, the product $B_{uv}B_{u'v}$ is $1$ if $v$ is a common neighbor and $0$ otherwise. The total number of common neighbors is the sum over all possible neighbors $v \\in \\mathcal{V}$:\n$$\nW_{u,u'} = \\sum_{v \\in \\mathcal{V}} B_{uv}B_{u'v}\n$$\nThis expression is the dot product of the $u$-th row vector of $B$ with the $u'$-th row vector of $B$. Let the matrix $B$ have dimensions $m \\times n$, where $m = |\\mathcal{U}|$ and $n = |\\mathcal{V}|$. The product of $B$ with its transpose, $B^T$, is an $m \\times m$ matrix. The $(u,u')$-th element of the product $B B^T$ is given by:\n$$\n(B B^T)_{u,u'} = \\sum_{v=1}^{n} B_{uv} (B^T)_{vu'} = \\sum_{v=1}^{n} B_{uv} B_{u'v}\n$$\nThis matches the definition of $W_{u,u'}$. Therefore, the weighted adjacency matrix for the projection onto $\\mathcal{U}$ is given by $W = B B^T$.\n\nFor the given matrix $B$, we have:\n$$\nB^T = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThus,\n$$\nW = B B^T = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(1)(1)+(0)(0) & (1)(0)+(1)(1)+(0)(1) \\\\ (0)(1)+(1)(1)+(1)(0) & (0)(0)+(1)(1)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\n\n2. Weighted adjacency matrix $W'$ of the projection onto $\\mathcal{V}$.\n\nSimilarly, the weight $W'_{v,v'}$ between two nodes $v, v' \\in \\mathcal{V}$ is the number of their common neighbors in $\\mathcal{U}$. Following the same logic, a node $u \\in \\mathcal{U}$ is a common neighbor if $B_{uv}=1$ and $B_{uv'}=1$. The total number of common neighbors is:\n$$\nW'_{v,v'} = \\sum_{u \\in \\mathcal{U}} B_{uv}B_{uv'}\n$$\nThis expression can be recognized as an element of the matrix product $B^T B$. The dimensions of $B^T$ are $n \\times m$ and $B$ are $m \\times n$, so $B^T B$ is an $n \\times n$ matrix. The $(v,v')$-th element of $B^T B$ is:\n$$\n(B^T B)_{v,v'} = \\sum_{u=1}^{m} (B^T)_{vu} B_{uv'} = \\sum_{u=1}^{m} B_{uv} B_{uv'}\n$$\nThis matches the definition of $W'_{v,v'}$. Therefore, the weighted adjacency matrix for the projection onto $\\mathcal{V}$ is given by $W' = B^T B$.\n\nFor the given matrix $B$:\n$$\nW' = B^T B = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(0)(0) & (1)(1)+(0)(1) & (1)(0)+(0)(1) \\\\ (1)(1)+(1)(0) & (1)(1)+(1)(1) & (1)(0)+(1)(1) \\\\ (0)(1)+(1)(0) & (0)(1)+(1)(1) & (0)(0)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\n\n3. Bipartite degree vectors and their relation to the diagonals of $W$ and $W'$.\n\nThe bipartite degree of a node $u \\in \\mathcal{U}$, denoted $k_u$, is the sum of the entries in the $u$-th row of $B$. The degree vector $k_{\\mathcal{U}}$ is the column vector of these row sums. Using the all-ones vector $\\mathbf{1}_{\\mathcal{V}} \\in \\mathbb{R}^{|\\mathcal{V}|}$, the $u$-th component of the product $B \\mathbf{1}_{\\mathcal{V}}$ is $\\sum_{v \\in \\mathcal{V}} B_{uv} \\cdot 1 = k_u$. Thus, $k_{\\mathcal{U}} = B \\mathbf{1}_{\\mathcal{V}}$.\nThe bipartite degree of a node $v \\in \\mathcal{V}$, denoted $k_v$, is the sum of the entries in the $v$-th column of $B$. The degree vector $k_{\\mathcal{V}}$ is the column vector of these column sums. This is equivalent to summing the rows of $B^T$. Using the all-ones vector $\\mathbf{1}_{\\mathcal{U}} \\in \\mathbb{R}^{|\\mathcal{U}|}$, the $v$-th component of the product $B^T \\mathbf{1}_{\\mathcal{U}}$ is $\\sum_{u \\in \\mathcal{U}} (B^T)_{vu} \\cdot 1 = \\sum_{u \\in \\mathcal{U}} B_{uv} = k_v$. Thus, $k_{\\mathcal{V}} = B^T \\mathbf{1}_{\\mathcal{U}}$.\n\nComputing these vectors for the given $B$:\n$$\n\\mathbf{1}_{\\mathcal{U}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{1}_{\\mathcal{V}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\nk_{\\mathcal{U}} = B \\mathbf{1}_{\\mathcal{V}} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+1+0 \\\\ 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n$$\nk_{\\mathcal{V}} = B^T \\mathbf{1}_{\\mathcal{U}} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+0 \\\\ 1+1 \\\\ 0+1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\n\nTo prove the relationship with the diagonals, consider the diagonal entry $W_{u,u}$:\n$$\nW_{u,u} = \\sum_{v \\in \\mathcal{V}} B_{uv}B_{uv} = \\sum_{v \\in \\mathcal{V}} (B_{uv})^2\n$$\nSince $B_{uv} \\in \\{0,1\\}$, we have $(B_{uv})^2 = B_{uv}$. Therefore,\n$$\nW_{u,u} = \\sum_{v \\in \\mathcal{V}} B_{uv} = k_u\n$$\nThis shows the diagonal entries of $W$ are the elements of the degree vector $k_{\\mathcal{U}}$.\nFor our computed $W$, the diagonal is $(2, 2)$, which matches $k_{\\mathcal{U}}$.\n\nSimilarly, for the diagonal entry $W'_{v,v}$:\n$$\nW'_{v,v} = \\sum_{u \\in \\mathcal{U}} B_{uv}B_{uv} = \\sum_{u \\in \\mathcal{U}} (B_{uv})^2 = \\sum_{u \\in \\mathcal{U}} B_{uv} = k_v\n$$\nThis shows the diagonal entries of $W'$ are the elements of the degree vector $k_{\\mathcal{V}}$.\nFor our computed $W'$, the diagonal is $(1, 2, 1)$, which matches $k_{\\mathcal{V}}$.\n\n4. Strength vectors in the projections.\n\nThe strength vector $s_{\\mathcal{U}}$ is defined as $s_{\\mathcal{U}} = W \\mathbf{1}_{\\mathcal{U}}$. Substituting $W = B B^T$:\n$$\ns_{\\mathcal{U}} = (B B^T) \\mathbf{1}_{\\mathcal{U}} = B (B^T \\mathbf{1}_{\\mathcal{U}})\n$$\nFrom part 3, we know $k_{\\mathcal{V}} = B^T \\mathbf{1}_{\\mathcal{U}}$. Thus, we obtain the identity $s_{\\mathcal{U}} = B k_{\\mathcal{V}}$.\n\nThe strength vector $s_{\\mathcal{V}}$ is defined as $s_{\\mathcal{V}} = W' \\mathbf{1}_{\\mathcal{V}}$. Substituting $W' = B^T B$:\n$$\ns_{\\mathcal{V}} = (B^T B) \\mathbf{1}_{\\mathcal{V}} = B^T (B \\mathbf{1}_{\\mathcal{V}})\n$$\nFrom part 3, we know $k_{\\mathcal{U}} = B \\mathbf{1}_{\\mathcal{V}}$. Thus, we obtain the identity $s_{\\mathcal{V}} = B^T k_{\\mathcal{U}}$.\n\nComputing these vectors for the given $B$:\n$$\ns_{\\mathcal{U}} = W \\mathbf{1}_{\\mathcal{U}} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+1 \\\\ 1+2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n$$\nVerifying with the identity:\n$s_{\\mathcal{U}} = B k_{\\mathcal{V}} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+2+0 \\\\ 0+2+1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$. The result is consistent.\n\n$$\ns_{\\mathcal{V}} = W' \\mathbf{1}_{\\mathcal{V}} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+1+0 \\\\ 1+2+1 \\\\ 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nVerifying with the identity:\n$s_{\\mathcal{V}} = B^T k_{\\mathcal{U}} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2+0 \\\\ 2+2 \\\\ 0+2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}$. The result is consistent.\n\nFinal numeric answer.\nThe scalar to be computed is $D \\equiv \\det(W) + \\operatorname{rank}(W')$.\nFirst, we compute the determinant of $W$:\n$$\nW = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\n$$\n\\det(W) = (2)(2) - (1)(1) = 4 - 1 = 3\n$$\nNext, we compute the rank of $W'$:\n$$\nW' = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nA fundamental result in linear algebra states that $\\operatorname{rank}(A^T A) = \\operatorname{rank}(A)$. Therefore, $\\operatorname{rank}(W') = \\operatorname{rank}(B^T B) = \\operatorname{rank}(B)$. The matrix $B$ is:\n$$\nB = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nThis matrix is already in row echelon form. It has two non-zero rows, which are linearly independent. Thus, the rank of $B$ is $2$.\n$$\n\\operatorname{rank}(W') = \\operatorname{rank}(B) = 2\n$$\nAlternatively, one could compute the determinant of $W'$:\n$$\n\\det(W') = 1 \\begin{vmatrix} 2 & 1 \\\\ 1 & 1 \\end{vmatrix} - 1 \\begin{vmatrix} 1 & 1 \\\\ 0 & 1 \\end{vmatrix} + 0 = 1(2-1) - 1(1-0) = 1 - 1 = 0\n$$\nSince $\\det(W')=0$, its rank must be less than $3$. The top-left $2 \\times 2$ submatrix $\\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$ has a determinant of $2-1=1 \\neq 0$, so the rank is at least $2$. Thus, $\\operatorname{rank}(W')=2$.\n\nFinally, we compute $D$:\n$$\nD = \\det(W) + \\operatorname{rank}(W') = 3 + 2 = 5\n$$",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "While counting common neighbors is a starting point, it can often be misleading; a high overlap between two very active users might be less significant than a smaller overlap between two selective users. This practice  delves into the crucial step of normalization by deriving and comparing two of the most common metrics: the Cosine similarity and the Jaccard index. By calculating both for a specific scenario, you will gain a deeper understanding of their distinct assumptions and how to choose the appropriate measure for your research question.",
            "id": "4294525",
            "problem": "Consider a userâ€“item bipartite network represented by an incidence matrix $B$ with entries $B_{i\\alpha} \\in \\{0,1\\}$, where $B_{i\\alpha} = 1$ if user $i$ has purchased item $\\alpha$, and $B_{i\\alpha} = 0$ otherwise. The degree of a user $i$ is $d_i = \\sum_{\\alpha} B_{i\\alpha}$, and the number of common items between users $i$ and $j$ is $c = \\sum_{\\alpha} B_{i\\alpha} B_{j\\alpha}$. In the one-mode projection onto users, a common approach assigns a similarity weight between users $i$ and $j$ based on their item-neighborhood overlap.\n\nStarting only from these definitions, derive expressions for the cosine similarity and the Jaccard index between users $i$ and $j$ in terms of $d_i$, $d_j$, and $c$, and then evaluate both for the case $d_i = 8$, $d_j = 10$, and $c = 4$. As a quantitative comparison between these two similarity notions, compute the ratio of the cosine similarity to the Jaccard index for this case. Give your final answer as a single, simplified exact expression with no units. Do not round.",
            "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, and provides all necessary information to derive and compute the required quantities, assuming standard definitions for cosine similarity and the Jaccard index, which is a reasonable expectation in this context.\n\nThe problem requires the derivation of two common similarity metrics, cosine similarity and the Jaccard index, for users in a one-mode projection of a bipartite user-item network. These derivations will be expressed in terms of the given quantities: the degree of user $i$, $d_i$; the degree of user $j$, $d_j$; and the number of common items between them, $c$.\n\nLet the set of all items be indexed by $\\alpha$. A user $i$ can be represented by a vector $\\mathbf{v}_i$ in the item space, where the component corresponding to item $\\alpha$ is given by the incidence matrix entry $B_{i\\alpha}$. Thus, $\\mathbf{v}_i = (B_{i1}, B_{i2}, \\dots, B_{iM})$, where $M$ is the total number of items. The entries $B_{i\\alpha}$ are either $1$ or $0$.\n\nFirst, we derive the expression for cosine similarity. The cosine similarity between two vectors $\\mathbf{v}_i$ and $\\mathbf{v}_j$ is defined as the cosine of the angle between them, which is computed as the dot product of the vectors divided by the product of their magnitudes (Euclidean or L2-norms).\n$$ \\cos(\\theta)_{ij} = \\frac{\\mathbf{v}_i \\cdot \\mathbf{v}_j}{\\|\\mathbf{v}_i\\| \\|\\mathbf{v}_j\\|} $$\nThe dot product $\\mathbf{v}_i \\cdot \\mathbf{v}_j$ is calculated as:\n$$ \\mathbf{v}_i \\cdot \\mathbf{v}_j = \\sum_{\\alpha} B_{i\\alpha} B_{j\\alpha} $$\nThe product $B_{i\\alpha} B_{j\\alpha}$ is $1$ only if both $B_{i\\alpha}=1$ and $B_{j\\alpha}=1$, meaning both users $i$ and $j$ have purchased item $\\alpha$. Otherwise, the product is $0$. The sum over all $\\alpha$ therefore counts the number of items common to both users. The problem statement defines this quantity as $c$. Thus,\n$$ \\mathbf{v}_i \\cdot \\mathbf{v}_j = c $$\nThe magnitude of a user vector $\\mathbf{v}_i$ is given by $\\|\\mathbf{v}_i\\| = \\sqrt{\\sum_{\\alpha} B_{i\\alpha}^2}$. Since the entries $B_{i\\alpha}$ are binary ($0$ or $1$), we have $B_{i\\alpha}^2 = B_{i\\alpha}$. Therefore, the squared magnitude is:\n$$ \\|\\mathbf{v}_i\\|^2 = \\sum_{\\alpha} B_{i\\alpha}^2 = \\sum_{\\alpha} B_{i\\alpha} $$\nThis sum is the total number of items purchased by user $i$, which is defined as the user's degree, $d_i$. So, $\\|\\mathbf{v}_i\\|^2 = d_i$, which implies $\\|\\mathbf{v}_i\\| = \\sqrt{d_i}$.\nSimilarly, for user $j$, we have $\\|\\mathbf{v}_j\\| = \\sqrt{d_j}$.\nSubstituting these expressions back into the formula for cosine similarity gives:\n$$ \\cos(\\theta)_{ij} = \\frac{c}{\\sqrt{d_i d_j}} $$\n\nNext, we derive the expression for the Jaccard index. The Jaccard index between two sets is defined as the size of their intersection divided by the size of their union. Let $S_i$ be the set of items purchased by user $i$, and $S_j$ be the set of items for user $j$.\n$$ J_{ij} = \\frac{|S_i \\cap S_j|}{|S_i \\cup S_j|} $$\nThe size of the intersection, $|S_i \\cap S_j|$, is the number of items belonging to both sets. This is precisely the number of common items, which is defined as $c$.\n$$ |S_i \\cap S_j| = c $$\nThe size of the union, $|S_i \\cup S_j|$, can be found using the principle of inclusion-exclusion:\n$$ |S_i \\cup S_j| = |S_i| + |S_j| - |S_i \\cap S_j| $$\nThe size of set $S_i$, $|S_i|$, is the number of items purchased by user $i$, which is the degree $d_i$. Similarly, $|S_j| = d_j$. Substituting these into the union formula:\n$$ |S_i \\cup S_j| = d_i + d_j - c $$\nTherefore, the Jaccard index is:\n$$ J_{ij} = \\frac{c}{d_i + d_j - c} $$\n\nNow, we evaluate these two similarity metrics for the given case: $d_i = 8$, $d_j = 10$, and $c = 4$.\nThe cosine similarity is:\n$$ \\cos(\\theta)_{ij} = \\frac{4}{\\sqrt{8 \\times 10}} = \\frac{4}{\\sqrt{80}} = \\frac{4}{\\sqrt{16 \\times 5}} = \\frac{4}{4\\sqrt{5}} = \\frac{1}{\\sqrt{5}} $$\nThe Jaccard index is:\n$$ J_{ij} = \\frac{4}{8 + 10 - 4} = \\frac{4}{14} = \\frac{2}{7} $$\n\nFinally, we compute the ratio of the cosine similarity to the Jaccard index for this case.\n$$ \\text{Ratio} = \\frac{\\cos(\\theta)_{ij}}{J_{ij}} = \\frac{1/\\sqrt{5}}{2/7} = \\frac{1}{\\sqrt{5}} \\times \\frac{7}{2} = \\frac{7}{2\\sqrt{5}} $$\nTo simplify this exact expression, we rationalize the denominator:\n$$ \\text{Ratio} = \\frac{7}{2\\sqrt{5}} \\times \\frac{\\sqrt{5}}{\\sqrt{5}} = \\frac{7\\sqrt{5}}{2 \\times 5} = \\frac{7\\sqrt{5}}{10} $$\nThis is the final, simplified exact expression for the ratio.",
            "answer": "$$\\boxed{\\frac{7\\sqrt{5}}{10}}$$"
        },
        {
            "introduction": "Real-world interactions are rarely just about simple connections; they often involve positive and negative sentiment, such as likes and dislikes. This practice  extends the projection framework to handle such signed data, where the resulting edge weight reflects net agreement between users. You will see how the same matrix formulation can be used and, importantly, discover the phenomenon of cancellation, where a mix of agreements and disagreements can lead to a low or even zero similarity score.",
            "id": "4294513",
            "problem": "Consider a bipartite network with two disjoint node sets: four users $U_1, U_2, U_3, U_4$ and five items $I_1, I_2, I_3, I_4, I_5$. Each user-item pair $(U_u, I_i)$ carries a signed interaction $r_{u i} \\in \\{-1, 0, +1\\}$, where $+1$ denotes a like, $-1$ denotes a dislike, and $0$ denotes no evaluation. The data are given by the $4 \\times 5$ rating matrix $R = \\big(r_{u i}\\big)$:\n$$\nR \\;=\\;\n\\begin{pmatrix}\n+1 & -1 & +1 & 0 & -1 \\\\\n+1 & +1 & -1 & +1 & 0 \\\\\n-1 & +1 & +1 & -1 & -1 \\\\\n+1 & 0 & +1 & -1 & +1\n\\end{pmatrix}.\n$$\nThe one-mode projection onto the user set produces a weighted user-user network. Adopt the following principles for constructing user-user edge weights from first principles:\n- In the unweighted case, the weight between two users equals the count of shared items they both connect to.\n- For signed evaluations $r_{u i} \\in \\{-1, 0, +1\\}$, the contribution of an item $I_i$ to the weight between two users $U_u$ and $U_v$ must be positive when they agree in sign on $I_i$ (both $+1$ or both $-1$), negative when they disagree in sign ($+1$ versus $-1$), and zero when at least one of them has $r_{u i} = 0$ or $r_{v i} = 0$.\n\nStarting only from these principles and core definitions of bipartite adjacency and one-mode projection, derive an analytic expression for the user-user weight $w_{u v}$ in terms of $\\{r_{u i}\\}$ and use it to compute the full $4 \\times 4$ weight matrix $W = \\big(w_{u v}\\big)$ for the user one-mode projection. Include diagonal entries $w_{u u}$ consistent with the same contribution rule.\n\nExplain, using the computed $W$, how cancellation arises due to mixed signs across co-rated items, and identify at least two user pairs for which cancellations lead to exactly zero edge weight.\n\nFinally, report the numerical value of the entry $w_{2 3}$ of $W$. No rounding is required for the final number.",
            "solution": "The user-user weight $w_{u v}$ for the one-mode projection is derived by summing the contributions from each item $I_i$. Let the contribution of item $I_i$ to the weight between user $U_u$ and user $U_v$ be denoted by $c_i(u, v)$. The problem states principles for determining this contribution based on the signed ratings $r_{ui}$ and $r_{vi}$.\n\nThe principles are:\n1. The contribution $c_i(u, v)$ is positive if users $U_u$ and $U_v$ agree in their assessment of item $I_i$. This occurs when both rate it positively ($r_{ui}=+1$, $r_{vi}=+1$) or both rate it negatively ($r_{ui}=-1$, $r_{vi}=-1$).\n2. The contribution $c_i(u, v)$ is negative if users $U_u$ and $U_v$ disagree in their assessment of item $I_i$. This occurs when one rates it positively and the other negatively ($r_{ui}=+1$, $r_{vi}=-1$ or $r_{ui}=-1$, $r_{vi}=+1$).\n3. The contribution $c_i(u, v)$ is zero if at least one of the users has not rated the item, i.e., $r_{ui}=0$ or $r_{vi}=0$.\n\nLet us find a simple mathematical expression for $c_i(u, v)$ that satisfies these conditions. The product of the ratings, $r_{ui} r_{vi}$, fulfills these requirements directly:\n- If $r_{ui}=+1$ and $r_{vi}=+1$, then $r_{ui} r_{vi} = (+1)(+1) = +1$ (positive contribution).\n- If $r_{ui}=-1$ and $r_{vi}=-1$, then $r_{ui} r_{vi} = (-1)(-1) = +1$ (positive contribution).\n- If $r_{ui}=+1$ and $r_{vi}=-1$, then $r_{ui} r_{vi} = (+1)(-1) = -1$ (negative contribution).\n- If $r_{ui}=-1$ and $r_{vi}=+1$, then $r_{ui} r_{vi} = (-1)(+1) = -1$ (negative contribution).\n- If $r_{ui}=0$ or $r_{vi}=0$, then $r_{ui} r_{vi} = 0$ (zero contribution).\n\nThe simplest formalization of the contribution for item $I_i$ is thus $c_i(u,v) = r_{ui} r_{vi}$. The total weight $w_{uv}$ between users $U_u$ and $U_v$ is the sum of these contributions over all items $i=1, \\dots, 5$:\n$$\nw_{uv} = \\sum_{i=1}^{5} c_i(u,v) = \\sum_{i=1}^{5} r_{ui} r_{vi}\n$$\nThis expression is the dot product of the $u$-th row vector of the rating matrix $R$ with the $v$-th row vector of $R$. If $R_u$ denotes the row vector for user $U_u$, then $w_{uv} = R_u \\cdot R_v$. The entire weight matrix $W = (w_{uv})$ can therefore be computed as the product of the matrix $R$ and its transpose, $R^T$:\n$$\nW = R R^T\n$$\nThe given rating matrix is:\n$$\nR \\;=\\;\n\\begin{pmatrix}\n1 & -1 & 1 & 0 & -1 \\\\\n1 & 1 & -1 & 1 & 0 \\\\\n-1 & 1 & 1 & -1 & -1 \\\\\n1 & 0 & 1 & -1 & 1\n\\end{pmatrix}\n$$\nIts transpose, $R^T$, is:\n$$\nR^T \\;=\\;\n\\begin{pmatrix}\n1 & 1 & -1 & 1 \\\\\n-1 & 1 & 1 & 0 \\\\\n1 & -1 & 1 & 1 \\\\\n0 & 1 & -1 & -1 \\\\\n-1 & 0 & -1 & 1\n\\end{pmatrix}\n$$\nWe now compute the product $W = R R^T$. The entry $w_{uv}$ is the dot product of the $u$-th row of $R$ and the $v$-th column of $R^T$ (which is the $v$-th row of $R$).\n\n$w_{11} = (1)(1) + (-1)(-1) + (1)(1) + (0)(0) + (-1)(-1) = 1 + 1 + 1 + 0 + 1 = 4$\n$w_{12} = (1)(1) + (-1)(1) + (1)(-1) + (0)(1) + (-1)(0) = 1 - 1 - 1 + 0 + 0 = -1$\n$w_{13} = (1)(-1) + (-1)(1) + (1)(1) + (0)(-1) + (-1)(-1) = -1 - 1 + 1 + 0 + 1 = 0$\n$w_{14} = (1)(1) + (-1)(0) + (1)(1) + (0)(-1) + (-1)(1) = 1 + 0 + 1 + 0 - 1 = 1$\n\n$w_{21} = w_{12} = -1$\n$w_{22} = (1)(1) + (1)(1) + (-1)(-1) + (1)(1) + (0)(0) = 1 + 1 + 1 + 1 + 0 = 4$\n$w_{23} = (1)(-1) + (1)(1) + (-1)(1) + (1)(-1) + (0)(-1) = -1 + 1 - 1 - 1 + 0 = -2$\n$w_{24} = (1)(1) + (1)(0) + (-1)(1) + (1)(-1) + (0)(1) = 1 + 0 - 1 - 1 + 0 = -1$\n\n$w_{31} = w_{13} = 0$\n$w_{32} = w_{23} = -2$\n$w_{33} = (-1)(-1) + (1)(1) + (1)(1) + (-1)(-1) + (-1)(-1) = 1 + 1 + 1 + 1 + 1 = 5$\n$w_{34} = (-1)(1) + (1)(0) + (1)(1) + (-1)(-1) + (-1)(1) = -1 + 0 + 1 + 1 - 1 = 0$\n\n$w_{41} = w_{14} = 1$\n$w_{42} = w_{24} = -1$\n$w_{43} = w_{34} = 0$\n$w_{44} = (1)(1) + (0)(0) + (1)(1) + (-1)(-1) + (1)(1) = 1 + 0 + 1 + 1 + 1 = 4$\n\nThe full user-user weight matrix $W$ is:\n$$\nW \\;=\\;\n\\begin{pmatrix}\n4 & -1 & 0 & 1 \\\\\n-1 & 4 & -2 & -1 \\\\\n0 & -2 & 5 & 0 \\\\\n1 & -1 & 0 & 4\n\\end{pmatrix}\n$$\nCancellation arises when users have co-rated multiple items, and their ratings exhibit a mix of agreement and disagreement. The positive contributions (from agreement, $r_{ui}r_{vi}=+1$) and negative contributions (from disagreement, $r_{ui}r_{vi}=-1$) can sum to a small value, or even exactly to zero.\n\nFrom the matrix $W$, we identify two user pairs with a zero edge weight: $(U_1, U_3)$ for which $w_{13} = 0$, and $(U_3, U_4)$ for which $w_{34} = 0$.\n\nLet's examine the cancellation for pair $(U_1, U_3)$:\nThe rating vectors are $R_1 = (1, -1, 1, 0, -1)$ and $R_3 = (-1, 1, 1, -1, -1)$.\nThe item-wise contributions are:\n- Item $I_1$: $r_{11}r_{31} = (1)(-1) = -1$ (Disagreement)\n- Item $I_2$: $r_{12}r_{32} = (-1)(1) = -1$ (Disagreement)\n- Item $I_3$: $r_{13}r_{33} = (1)(1) = +1$ (Agreement)\n- Item $I_4$: $r_{14}r_{34} = (0)(-1) = 0$ (No co-rating)\n- Item $I_5$: $r_{15}r_{35} = (-1)(-1) = +1$ (Agreement)\nThe total weight is $w_{13} = -1 - 1 + 1 + 0 + 1 = 0$. The two agreements and two disagreements cancel perfectly.\n\nNext, let's examine the cancellation for pair $(U_3, U_4)$:\nThe rating vectors are $R_3 = (-1, 1, 1, -1, -1)$ and $R_4 = (1, 0, 1, -1, 1)$.\nThe item-wise contributions are:\n- Item $I_1$: $r_{31}r_{41} = (-1)(1) = -1$ (Disagreement)\n- Item $I_2$: $r_{32}r_{42} = (1)(0) = 0$ (No co-rating)\n- Item $I_3$: $r_{33}r_{43} = (1)(1) = +1$ (Agreement)\n- Item $I_4$: $r_{34}r_{44} = (-1)(-1) = +1$ (Agreement)\n- Item $I_5$: $r_{35}r_{45} = (-1)(1) = -1$ (Disagreement)\nThe total weight is $w_{34} = -1 + 0 + 1 + 1 - 1 = 0$. Again, two agreements and two disagreements cancel out.\n\nThe diagonal entries $w_{uu}$ represent the number of items rated by user $U_u$, as $r_{ui}r_{ui} = r_{ui}^2$, which is $1$ if $r_{ui} \\in \\{-1,+1\\}$ and $0$ if $r_{ui}=0$. For example, $w_{33}=5$ because user $U_3$ rated all $5$ items.\n\nFinally, the problem asks for the numerical value of the entry $w_{23}$. From our calculation of the matrix $W$:\n$w_{23} = (1)(-1) + (1)(1) + (-1)(1) + (1)(-1) + (0)(-1) = -1 + 1 - 1 - 1 + 0 = -2$.\nThis negative weight indicates a net disagreement between users $U_2$ and $U_3$ across the items they co-rated.",
            "answer": "$$\\boxed{-2}$$"
        }
    ]
}