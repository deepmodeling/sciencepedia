{
    "hands_on_practices": [
        {
            "introduction": "The journey into generative network models begins with the foundational Erdős–Rényi (ER) random graph, which serves as a crucial baseline for comparison. By calculating fundamental properties such as the expected number of triangles and the global clustering coefficient, we can quantitatively grasp the structure that emerges from pure randomness. This exercise  sharpens skills in probabilistic reasoning and reveals the model's limitations, particularly its failure to produce the high clustering seen in most real-world networks, thereby motivating the need for more sophisticated models.",
            "id": "4278501",
            "problem": "A widely used null generative model for networks is the Erdős–Rényi (ER) random graph $G(n,p)$, where each unordered pair of distinct nodes is independently connected by an edge with probability $p$. Let $T$ denote the number of triangles (unordered triples of distinct nodes for which all three possible edges among them are present). Let the number of connected triples of nodes (also known as wedges) be $W$, defined as $W=\\sum_{v} \\binom{D_v}{2}$, where $D_v$ is the degree of node $v$. The global transitivity (also called the global clustering coefficient) is defined by $C = \\frac{3T}{W}$.\n\nStarting from these definitions and the independence of edges in $G(n,p)$, derive the expected value of $T$ and an asymptotically accurate expression for $C$ in terms of $n$ and $p$ that relies only on the definitions above and standard probabilistic identities. Then evaluate both for $n=10^{5}$ and $p=10^{-3}$. Round your final numerical results to four significant figures. No units are required in the final answer.",
            "solution": "The problem requires the derivation of the expected number of triangles, $\\mathbb{E}[T]$, and an asymptotically accurate expression for the global transitivity, $C$, in an Erdős–Rényi random graph $G(n,p)$. Subsequently, these quantities are to be evaluated for specific values of $n$ and $p$.\n\nFirst, we derive the expected number of triangles, $\\mathbb{E}[T]$. A triangle is formed by a set of three distinct nodes, say $\\{i, j, k\\}$, if the edges $(i,j)$, $(j,k)$, and $(k,i)$ all exist in the graph.\nThe total number of ways to choose an unordered set of three distinct nodes from a total of $n$ nodes is given by the binomial coefficient $\\binom{n}{3}$.\nLet us consider one such set of three nodes. In the $G(n,p)$ model, the existence of any edge is an independent event with probability $p$. For these three nodes to form a triangle, all three possible edges among them must be present. The probability of this occurring is $p \\times p \\times p = p^3$, due to the independence of edges.\n\nTo find the expected total number of triangles, $T$, we can use the method of indicator variables. Let $I_{ijk}$ be an indicator random variable for the event that the specific triple of nodes $\\{i, j, k\\}$ forms a triangle.\n$$\nI_{ijk} = \\begin{cases} 1  \\text{if nodes } i,j,k \\text{ form a triangle} \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe total number of triangles is the sum of these indicators over all unique triples of nodes:\n$$\nT = \\sum_{1 \\le i  j  k \\le n} I_{ijk}\n$$\nBy the linearity of expectation, the expected number of triangles is the sum of the expected values of these indicator variables:\n$$\n\\mathbb{E}[T] = \\mathbb{E}\\left[\\sum_{1 \\le i  j  k \\le n} I_{ijk}\\right] = \\sum_{1 \\le i  j  k \\le n} \\mathbb{E}[I_{ijk}]\n$$\nThe expectation of an indicator variable is the probability of the event it indicates. Thus, $\\mathbb{E}[I_{ijk}] = P(\\text{nodes } i,j,k \\text{ form a triangle}) = p^3$.\nThe number of terms in the sum is the number of ways to choose $3$ nodes from $n$, which is $\\binom{n}{3}$. Therefore, the expected number of triangles is:\n$$\n\\mathbb{E}[T] = \\binom{n}{3} p^3 = \\frac{n(n-1)(n-2)}{6} p^3\n$$\n\nNext, we derive an asymptotically accurate expression for the global transitivity, $C$. The definition provided is $C = \\frac{3T}{W}$, where $W$ is the number of wedges, given by $W = \\sum_{v} \\binom{D_v}{2}$. Here, $D_v$ is the degree of node $v$. The quantity $W$ counts the number of paths of length two in the graph.\nFor large networks, a random variable representing a global graph property is often highly concentrated around its mean. This allows us to approximate a function of random variables, such as the ratio $C = \\frac{3T}{W}$, by the function of their expectations. This approximation becomes increasingly accurate as $n \\to \\infty$.\n$$\nC \\approx \\frac{3\\mathbb{E}[T]}{\\mathbb{E}[W]}\n$$\nWe have already found $\\mathbb{E}[T]$. We now need to compute $\\mathbb{E}[W]$.\nUsing the definition of $W$ and the linearity of expectation:\n$$\n\\mathbb{E}[W] = \\mathbb{E}\\left[\\sum_{v=1}^{n} \\binom{D_v}{2}\\right] = \\sum_{v=1}^{n} \\mathbb{E}\\left[\\binom{D_v}{2}\\right]\n$$\nIn an Erdős–Rényi graph, all nodes are statistically equivalent by symmetry. Therefore, $\\mathbb{E}\\left[\\binom{D_v}{2}\\right]$ is the same for all nodes $v$. We can write:\n$$\n\\mathbb{E}[W] = n \\cdot \\mathbb{E}\\left[\\binom{D_1}{2}\\right]\n$$\nwhere $D_1$ is the degree of an arbitrary node, say node $1$. The degree $D_1$ is the number of edges connected to node $1$. Since there are $n-1$ other nodes, and an edge to each exists independently with probability $p$, the degree $D_1$ follows a binomial distribution: $D_1 \\sim \\text{Binomial}(n-1, p)$.\nWe need to calculate $\\mathbb{E}\\left[\\binom{D_1}{2}\\right]$:\n$$\n\\mathbb{E}\\left[\\binom{D_1}{2}\\right] = \\mathbb{E}\\left[\\frac{D_1(D_1-1)}{2}\\right] = \\frac{1}{2} \\left( \\mathbb{E}[D_1^2] - \\mathbb{E}[D_1] \\right)\n$$\nFor a binomial distribution $\\text{Bin}(k, p)$, the mean is $kp$ and the variance is $kp(1-p)$. For $D_1$, we have $k=n-1$.\nThe mean degree is $\\mathbb{E}[D_1] = (n-1)p$.\nThe variance is $\\text{Var}(D_1) = (n-1)p(1-p)$.\nUsing the relation $\\text{Var}(D_1) = \\mathbb{E}[D_1^2] - (\\mathbb{E}[D_1])^2$, we find the second moment of $D_1$:\n$$\n\\mathbb{E}[D_1^2] = \\text{Var}(D_1) + (\\mathbb{E}[D_1])^2 = (n-1)p(1-p) + ((n-1)p)^2\n$$\nSubstituting this into the expression for $\\mathbb{E}\\left[\\binom{D_1}{2}\\right]$:\n$$\n\\mathbb{E}\\left[\\binom{D_1}{2}\\right] = \\frac{1}{2} \\left( (n-1)p(1-p) + ((n-1)p)^2 - (n-1)p \\right)\n$$\n$$\n= \\frac{1}{2} \\left( (n-1)p - (n-1)p^2 + (n-1)^2 p^2 - (n-1)p \\right)\n$$\n$$\n= \\frac{1}{2} \\left( (n-1)^2 p^2 - (n-1)p^2 \\right) = \\frac{p^2}{2} (n-1)(n-1-1) = \\frac{(n-1)(n-2)}{2} p^2\n$$\nThis is equivalent to $\\binom{n-1}{2}p^2$.\nNow we can express $\\mathbb{E}[W]$:\n$$\n\\mathbb{E}[W] = n \\cdot \\mathbb{E}\\left[\\binom{D_1}{2}\\right] = n \\binom{n-1}{2} p^2 = \\frac{n(n-1)(n-2)}{2} p^2\n$$\nWe can now assemble the asymptotic expression for $C$:\n$$\nC \\approx \\frac{3\\mathbb{E}[T]}{\\mathbb{E}[W]} = \\frac{3 \\cdot \\frac{n(n-1)(n-2)}{6} p^3}{\\frac{n(n-1)(n-2)}{2} p^2} = \\frac{\\frac{n(n-1)(n-2)}{2} p^3}{\\frac{n(n-1)(n-2)}{2} p^2} = p\n$$\nThus, the asymptotically accurate expression for $C$ is simply $p$. This reflects the fact that in a $G(n,p)$ graph, the probability of two neighbors of a given node being connected is itself $p$, due to edge independence.\n\nFinally, we evaluate the derived expressions for $n=10^5$ and $p=10^{-3}$.\nFor the expected number of triangles:\n$$\n\\mathbb{E}[T] = \\binom{10^5}{3} (10^{-3})^3 = \\frac{10^5 \\cdot (10^5-1) \\cdot (10^5-2)}{6} \\cdot (10^{-3})^3\n$$\n$$\n\\mathbb{E}[T] = \\frac{10^5 \\cdot 99999 \\cdot 99998}{6} \\cdot 10^{-9}\n$$\n$$\n\\mathbb{E}[T] = \\frac{9999700002 \\times 10^5}{6} \\cdot 10^{-9} = \\frac{9.999700002 \\times 10^{14}}{6} \\cdot 10^{-9}\n$$\n$$\n\\mathbb{E}[T] = 1.666616667 \\times 10^{14} \\cdot 10^{-9} = 1.666616667 \\times 10^5\n$$\nRounding to four significant figures, we get $\\mathbb{E}[T] \\approx 1.667 \\times 10^5$.\n\nFor the global transitivity, the asymptotic expression is $C \\approx p$.\nGiven $p=10^{-3}$, the value is $0.001$. Expressed in scientific notation and rounded to four significant figures, this is $1.000 \\times 10^{-3}$.\n\nThe required numerical results are for $\\mathbb{E}[T]$ and $C$.\n$\\mathbb{E}[T] \\approx 1.667 \\times 10^5$\n$C \\approx 1.000 \\times 10^{-3}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.667 \\times 10^{5}  1.000 \\times 10^{-3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Many real-world networks, from the internet to biological systems, display \"scale-free\" properties, meaning their degree distributions follow a heavy-tailed power law. The Configuration Model (CM) is a cornerstone technique for generating networks with precisely this feature. This practice  delves into the mechanics of the CM, using a truncated power-law distribution to derive the expected average degree and, crucially, to assess the model's feasibility through the concept of a \"structural cutoff,\" which highlights a fundamental constraint in generating simple graphs with highly heterogeneous degrees.",
            "id": "4278534",
            "problem": "Consider a generative network model in which degrees are independently sampled from a truncated power-law degree distribution specified by $P(k)\\propto k^{-\\gamma}$ for integer $k$ in the range $k\\in[k_{\\min},k_{\\max}]$, where $k_{\\min}=1$, $k_{\\max}=10^{3}$, and $\\gamma=2.5$. Assume the network is constructed using the Configuration Model (CM), defined as follows: given a degree sequence $\\{k_{i}\\}_{i=1}^{n}$, attach $k_{i}$ half-edges (stubs) to node $i$ and match stubs uniformly at random to form edges, rejecting self-loops and parallel edges to target simple graphs when feasible.\n\nStarting from foundational definitions for discrete distributions and asymptotic continuous approximations, do the following:\n\n1. Using normalization of $P(k)$ and the definition of the expected degree $\\langle k \\rangle$, derive an analytic expression for $\\langle k \\rangle$ under the continuous approximation to the truncated power-law on $[k_{\\min},k_{\\max}]$. Then evaluate this expression numerically for the given parameters.\n\n2. Using first principles of stub matching probabilities in the Configuration Model, derive the structural cutoff $k_s$ as a scaling threshold that ensures the probability of multi-edges and self-loops vanishes in the large-$n$ limit. Express $k_s$ in terms of $n$ and $\\langle k \\rangle$, and use it to assess the feasibility of obtaining a simple graph with the specified cutoff $k_{\\max}$ for $n=10^{5}$.\n\nReturn your final answer as a two-entry row matrix $\\begin{pmatrix}a  b\\end{pmatrix}$ where:\n- The first entry $a$ is the expected average degree $\\langle k \\rangle$ rounded to four significant figures.\n- The second entry $b$ is a feasibility indicator, with $b=1$ if $k_{\\max} \\le k_s$ and $b=0$ otherwise.\n\nNo units are required in the final answer. Round any required numerical value to four significant figures.",
            "solution": "We begin from the definitions for a truncated power-law degree distribution. Let the continuous approximation to the discrete distribution on $[k_{\\min},k_{\\max}]$ be\n$$\np(k) = C\\,k^{-\\gamma},\\quad k\\in[k_{\\min},k_{\\max}],\n$$\nwhere $C$ is the normalization constant determined by\n$$\n\\int_{k_{\\min}}^{k_{\\max}} C\\,k^{-\\gamma}\\,dk = 1.\n$$\nFor $\\gamma\\neq 1$, we have\n$$\n\\int k^{-\\gamma}\\,dk = \\frac{k^{1-\\gamma}}{1-\\gamma}.\n$$\nTherefore,\n$$\nC\\,\\frac{k_{\\max}^{1-\\gamma}-k_{\\min}^{1-\\gamma}}{1-\\gamma} = 1\n\\quad\\Rightarrow\\quad\nC = \\frac{1-\\gamma}{k_{\\max}^{1-\\gamma}-k_{\\min}^{1-\\gamma}}\n= \\frac{\\gamma-1}{k_{\\min}^{1-\\gamma}-k_{\\max}^{1-\\gamma}}.\n$$\nWith $k_{\\min}=1$ and $\\gamma=2.5$, this becomes\n$$\nC = \\frac{\\gamma-1}{1 - k_{\\max}^{1-\\gamma}}\n= \\frac{2.5-1}{1 - k_{\\max}^{-1.5}}\n= \\frac{1.5}{1 - k_{\\max}^{-1.5}}.\n$$\n\nThe expected degree under the continuous approximation is\n$$\n\\langle k\\rangle = \\int_{k_{\\min}}^{k_{\\max}} k\\,p(k)\\,dk\n= \\int_{k_{\\min}}^{k_{\\max}} C\\,k^{1-\\gamma}\\,dk.\n$$\nFor $\\gamma=2.5$, we have $1-\\gamma=-1.5$ and thus\n$$\n\\int k^{-1.5}\\,dk = \\frac{k^{2-\\gamma}}{2-\\gamma} = \\frac{k^{-0.5}}{-0.5} = -2\\,k^{-0.5}.\n$$\nHence,\n$$\n\\langle k\\rangle\n= C\\left[-2\\,k^{-0.5}\\right]_{k_{\\min}}^{k_{\\max}}\n= C\\cdot 2\\left(k_{\\min}^{-0.5}-k_{\\max}^{-0.5}\\right).\n$$\nWith $k_{\\min}=1$, this simplifies to\n$$\n\\langle k\\rangle = 2C\\left(1 - k_{\\max}^{-0.5}\\right).\n$$\nSubstituting the expression for $C$,\n$$\n\\langle k\\rangle\n= 2\\cdot \\frac{1.5}{1 - k_{\\max}^{-1.5}}\\left(1 - k_{\\max}^{-0.5}\\right)\n= 3\\,\\frac{1 - k_{\\max}^{-0.5}}{1 - k_{\\max}^{-1.5}}.\n$$\nFor $k_{\\max}=10^{3}$, observe that $10^{3}$ raised to $-0.5$ is $10^{-1.5}$ and to $-1.5$ is $10^{-4.5}$. Therefore,\n$$\n\\langle k\\rangle\n= 3\\,\\frac{1 - 10^{-1.5}}{1 - 10^{-4.5}}.\n$$\nNumerically, $10^{-1.5}=0.03162277660168379$ and $10^{-4.5}=0.00003162277660168379$, yielding\n$$\n\\langle k\\rangle\n= 3\\,\\frac{1 - 0.03162277660168379}{1 - 0.00003162277660168379}\n= 3\\,\\frac{0.9683772233983162}{0.9999683772233983}\n\\approx 3\\times 0.9684078471396242\n\\approx 2.9052235414188726.\n$$\nRounded to four significant figures, this is\n$$\n\\langle k\\rangle \\approx 2.905.\n$$\n\nNext, we assess feasibility for producing a simple graph via the Configuration Model (CM). In CM, stubs are paired uniformly at random. The probability that two stubs from nodes $i$ and $j$ connect is approximately\n$$\np_{ij} \\approx \\frac{k_i k_j}{2m},\n$$\nwhere $m$ is the number of edges and $2m=\\sum_{i}k_{i}=n\\,\\langle k\\rangle$. To keep the expected number of multi-edges and self-loops negligible in the large-$n$ limit, it is sufficient that the maximum degree $k_{\\max}$ satisfies the structural cutoff condition\n$$\nk_{\\max} \\lesssim k_s,\\quad\\text{where}\\quad k_s = \\sqrt{2m}=\\sqrt{n\\langle k \\rangle}.\n$$\nThis follows from bounding $p_{ij}$ by substituting $k_i,k_j \\le k_{\\max}$, which implies $p_{ij} \\lesssim k_{\\max}^2 / (2m)$; requiring $p_{ij} \\ll 1$ yields $k_{\\max} \\ll \\sqrt{2m}$ and motivates the threshold $k_s = \\sqrt{n \\langle k \\rangle}$.\n\nFor $n=10^{5}$ and the computed $\\langle k\\rangle\\approx 2.9052235414188726$,\n$$\nk_s = \\sqrt{n \\langle k \\rangle}\n= \\sqrt{10^{5}\\times 2.9052235414188726}\n= \\sqrt{290522.35414188726}\n\\approx 539.001256.\n$$\nComparing with $k_{\\max}=10^{3}$,\n$$\nk_{\\max}=1000 > k_s \\approx 539.001256,\n$$\nso the specified cutoff exceeds the structural cutoff. Under the CM without additional constraints, this implies that generating a simple graph (without self-loops or parallel edges) is not feasible at scale $n=10^{5}$ with the given $k_{\\max}$; we therefore set the feasibility indicator to $0$.\n\nThe requested final answer is the two-entry row matrix with the rounded expected degree and the feasibility indicator.",
            "answer": "$$\\boxed{\\begin{pmatrix}2.905  0\\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving to the forefront of network science, deep learning methods like Generative Adversarial Networks (GANs) offer a powerful, data-driven approach to creating realistic graphs. Instead of prescribing rules, these models learn to mimic complex structural patterns directly from examples. This problem  shifts the focus from calculation to conceptual design, asking how to construct a GAN's training objective to explicitly enforce the matching of both degree and clustering distributions, two key signatures of network organization. It provides insight into the modern techniques used to translate abstract structural goals into differentiable loss functions for training advanced generative models.",
            "id": "4278492",
            "problem": "You are designing a Generative Adversarial Network (GAN) to synthesize undirected, simple graphs with a fixed number of nodes $n$. The data distribution over real graphs is denoted by $\\mathcal{P}_{\\text{data}}$, where each real graph is represented by an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ that is symmetric with zero diagonal. The generator $G_{\\theta}$ produces a real-valued matrix $S \\in \\mathbb{R}^{n \\times n}$ which is mapped to a symmetric matrix of edge probabilities $P \\in [0,1]^{n \\times n}$ with zero diagonal through a smooth transformation (for example, $P = \\sigma(S)$ elementwise, followed by $P \\leftarrow \\frac{1}{2}(P + P^{\\top})$ and setting $P_{ii} = 0$). A sample adjacency from the generator can be obtained by independent Bernoulli draws $A^{(G)}_{ij} \\sim \\mathrm{Bernoulli}(P_{ij})$ for $i  j$, then symmetrized with zero diagonal.\n\nThe discriminator $D_{\\phi}$ maps an adjacency matrix to a scalar in $[0,1]$, yielding an estimate of the probability that its input is from $\\mathcal{P}_{\\text{data}}$.\n\nYou are tasked with specifying a training objective such that, at convergence, the generated graphs match the data not only in a global adversarial sense but also specifically in their degree distribution and clustering characteristics. Use the following fundamental definitions.\n\n- The degree of node $i$ in a graph with adjacency $A$ is $k_i(A) = \\sum_{j=1}^{n} A_{ij}$. The degree distribution is the empirical distribution over $\\{k_i(A)\\}_{i=1}^{n}$.\n- The number of triangles incident to node $i$ is $t_i(A) = \\frac{(A^3)_{ii}}{2}$ for an undirected simple graph. The local clustering coefficient of node $i$ is $c_i(A) = \\frac{2 t_i(A)}{k_i(A)\\big(k_i(A)-1\\big)}$ when $k_i(A) \\ge 2$, and $c_i(A) = 0$ otherwise. The clustering distribution is the empirical distribution over $\\{c_i(A)\\}_{i=1}^{n}$.\n- For a probabilistic adjacency $P \\in [0,1]^{n \\times n}$, the expected degree is $d_i(P) = \\sum_{j=1}^{n} P_{ij}$, and a smooth surrogate for the expected local clustering is $c_i(P) = \\frac{(P^3)_{ii}}{d_i(P)\\big(d_i(P)-1\\big) + \\epsilon}$ with a small $\\epsilon > 0$ to avoid division by zero.\n\nYour goal is to select a training objective and constraint mechanism that, in principle, drive the generator to match both the degree distribution and the clustering distribution of $\\mathcal{P}_{\\text{data}}$, while remaining differentiable with respect to $\\theta$ and amenable to stochastic optimization.\n\nWhich of the following proposals are correct?\n\nA. Use the standard non-saturating GAN objective for $G_{\\theta}$ and $D_{\\phi}$ without any additional terms:\n$$\n\\min_{\\theta} \\max_{\\phi} \\ \\mathbb{E}_{A \\sim \\mathcal{P}_{\\text{data}}}\\big[\\log D_{\\phi}(A)\\big] + \\mathbb{E}_{z \\sim p(z)}\\big[\\log\\big(1 - D_{\\phi}(\\mathrm{Bernoulli}(G_{\\theta}(z)))\\big)\\big].\n$$\nRely on the discriminator to implicitly enforce matching of degree and clustering distributions.\n\nB. Add differentiable, distribution-matching penalties on degrees and clustering to the GAN objective, using a kernel Maximum Mean Discrepancy (MMD) with a characteristic kernel $k(\\cdot,\\cdot)$. For a real graph $A$ and generator probabilities $P$, define\n$$\n\\mathcal{L}_{\\text{deg}}(P, A) = \\mathrm{MMD}^{2}\\big(\\{d_i(P)\\}_{i=1}^{n}, \\{k_i(A)\\}_{i=1}^{n}\\big),\n$$\n$$\n\\mathcal{L}_{\\text{clust}}(P, A) = \\mathrm{MMD}^{2}\\big(\\{c_i(P)\\}_{i=1}^{n}, \\{c_i(A)\\}_{i=1}^{n}\\big),\n$$\nwhere for samples $\\{x_i\\}_{i=1}^{m}$ and $\\{y_j\\}_{j=1}^{n}$,\n$$\n\\mathrm{MMD}^{2} = \\frac{1}{m^{2}} \\sum_{i,i'} k(x_i, x_{i'}) + \\frac{1}{n^{2}} \\sum_{j,j'} k(y_j, y_{j'}) - \\frac{2}{mn} \\sum_{i,j} k(x_i, y_j).\n$$\nTrain with an augmented Lagrangian objective\n$$\n\\min_{\\theta} \\max_{\\phi} \\ \\mathcal{L}_{\\text{GAN}}(\\theta,\\phi) + \\lambda_{\\text{deg}} \\, \\mathcal{L}_{\\text{deg}} + \\lambda_{\\text{clust}} \\, \\mathcal{L}_{\\text{clust}} + \\frac{\\rho}{2}\\big(\\mathcal{L}_{\\text{deg}}^{2} + \\mathcal{L}_{\\text{clust}}^{2}\\big),\n$$\nwith dual updates $\\lambda_{\\ell} \\leftarrow \\lambda_{\\ell} + \\rho \\, \\mathcal{L}_{\\ell}$ for $\\ell \\in \\{\\text{deg}, \\text{clust}\\}$.\n\nC. Replace the standard GAN with a Wasserstein GAN with gradient penalty, and enforce only the average degree by a scalar penalty. After each generator update, project the sampled graphs onto the set with the target degree sequence using the Havel–Hakimi algorithm. The objective is\n$$\n\\min_{\\theta} \\max_{\\phi \\in \\mathcal{L}\\text{-}\\mathrm{Lip}} \\ \\mathbb{E}_{A \\sim \\mathcal{P}_{\\text{data}}}\\big[D_{\\phi}(A)\\big] - \\mathbb{E}_{z \\sim p(z)}\\big[D_{\\phi}(\\mathrm{Bernoulli}(G_{\\theta}(z)))\\big] + \\lambda \\big(\\bar{d}(P) - \\bar{k}(A)\\big)^{2},\n$$\nwhere $\\bar{d}(P) = \\frac{1}{n} \\sum_{i=1}^{n} d_i(P)$ and $\\bar{k}(A) = \\frac{1}{n} \\sum_{i=1}^{n} k_i(A)$, followed by a hard projection step to match the empirical degree sequence exactly.\n\nD. Use multiple discriminators: a base discriminator on adjacency matrices, an auxiliary discriminator on degree sequences, and an auxiliary discriminator on local clustering vectors. Let $D^{(A)}_{\\phi}$ distinguish real versus generated adjacencies, $D^{(k)}_{\\psi}$ distinguish real versus generated degree vectors, and $D^{(c)}_{\\xi}$ distinguish real versus generated clustering vectors. Train with\n$$\n\\min_{\\theta} \\max_{\\phi,\\psi,\\xi} \\ \\mathbb{E}_{A}\\big[\\log D^{(A)}_{\\phi}(A)\\big] + \\mathbb{E}_{P}\\big[\\log\\big(1 - D^{(A)}_{\\phi}(\\mathrm{Bernoulli}(P))\\big)\\big]\n$$\n$$\n+ \\ \\mathbb{E}_{A}\\big[\\log D^{(k)}_{\\psi}(k(A))\\big] + \\mathbb{E}_{P}\\big[\\log\\big(1 - D^{(k)}_{\\psi}(d(P))\\big)\\big]\n$$\n$$\n+ \\ \\mathbb{E}_{A}\\big[\\log D^{(c)}_{\\xi}(c(A))\\big] + \\mathbb{E}_{P}\\big[\\log\\big(1 - D^{(c)}_{\\xi}(c(P))\\big)\\big],\n$$\nwhere $d(P) = (d_1(P),\\dots,d_n(P))$ and $c(P) = (c_1(P),\\dots,c_n(P))$, possibly with gradient penalties or spectral normalization to stabilize training.\n\nE. Adopt a mutual-information regularized objective that maximizes the mutual information between a subset of latent variables and the sorted degree sequence of the generated graph, without any explicit matching to real degree or clustering distributions. Use an InfoGAN-style term\n$$\n\\min_{\\theta} \\max_{\\phi} \\ \\mathcal{L}_{\\text{GAN}}(\\theta,\\phi) - \\beta \\, I\\big(c; \\mathrm{sort}(d(P))\\big),\n$$\nwith an auxiliary network to lower bound the mutual information $I(\\cdot;\\cdot)$, and no other structure-matching constraints.\n\nSelect all options that correctly specify a training objective that preserves the degree distribution and clustering and propose a valid constraint mechanism to achieve this, under the differentiability and stochastic optimization requirements described above.",
            "solution": "The problem requires the selection of a training objective for a Generative Adversarial Network (GAN) designed to synthesize undirected simple graphs. The objective must ensure that the generated graphs match a real data distribution, $\\mathcal{P}_{\\text{data}}$, not only in a general adversarial sense but also specifically in their degree and clustering distributions. Key constraints are that the method must be differentiable with respect to the generator's parameters, $\\theta$, and suitable for stochastic optimization. The generator $G_{\\theta}$ outputs a probability matrix $P$, from which a graph $A^{(G)}$ is sampled. This sampling operation, $\\mathrm{Bernoulli}(P)$, is non-differentiable. A valid approach must therefore either use a policy gradient method (like REINFORCE), a straight-through estimator, or, as is common, define auxiliary loss terms on the differentiable probabilistic adjacency matrix $P$ itself, using differentiable surrogates for the graph properties.\n\nLet's evaluate each proposal against these requirements.\n\nA. Use the standard non-saturating GAN objective for $G_{\\theta}$ and $D_{\\phi}$ without any additional terms:\n$$\n\\min_{\\theta} \\max_{\\phi} \\ \\mathbb{E}_{A \\sim \\mathcal{P}_{\\text{data}}}\\big[\\log D_{\\phi}(A)\\big] + \\mathbb{E}_{z \\sim p(z)}\\big[\\log\\big(1 - D_{\\phi}(\\mathrm{Bernoulli}(G_{\\theta}(z)))\\big)\\big].\n$$\nRely on the discriminator to implicitly enforce matching of degree and clustering distributions.\n\nThis proposal relies on the assumption that a sufficiently powerful discriminator, $D_{\\phi}$, will learn to identify discrepancies in any statistical property, including degree and clustering distributions, thereby forcing the generator, $G_{\\theta}$, to match them. While theoretically, at the Nash equilibrium with infinite model capacity and data, the generated distribution $\\mathcal{P}_G$ would equal the data distribution $\\mathcal{P}_{\\text{data}}$, this is rarely achieved in practice. The discriminator has finite capacity and may find it easier to distinguish real from fake graphs based on simpler, more local features. It is not guaranteed to learn the specific, and often subtle, high-order correlations that define the degree and clustering distributions. The problem asks for a mechanism that *drives* the generator to match these properties, which implies an explicit directive, not a reliance on implicit and often unreliable learning dynamics. Therefore, this approach is insufficient to meet the specific requirements of the task.\n\nVerdict on A: **Incorrect**.\n\nB. Add differentiable, distribution-matching penalties on degrees and clustering to the GAN objective, using a kernel Maximum Mean Discrepancy (MMD) with a characteristic kernel $k(\\cdot,\\cdot)$. For a real graph $A$ and generator probabilities $P$, define\n$$\n\\mathcal{L}_{\\text{deg}}(P, A) = \\mathrm{MMD}^{2}\\big(\\{d_i(P)\\}_{i=1}^{n}, \\{k_i(A)\\}_{i=1}^{n}\\big),\n$$\n$$\n\\mathcal{L}_{\\text{clust}}(P, A) = \\mathrm{MMD}^{2}\\big(\\{c_i(P)\\}_{i=1}^{n}, \\{c_i(A)\\}_{i=1}^{n}\\big),\n$$\n...and train with an augmented objective.\n\nThis proposal directly addresses the problem by introducing explicit penalty terms to minimize the distance between the distributions of graph properties.\n1.  **Enforcing Properties**: The MMD is a principled metric for comparing distributions. Using a characteristic kernel, $\\mathrm{MMD}^{2} = 0$ if and only if the distributions are identical. By penalizing the MMD between the target property distribution from a real graph $A$ (i.e., $\\{k_i(A)\\}$ and $\\{c_i(A)\\}$) and the surrogate property distribution from the generated probability matrix $P$ (i.e., $\\{d_i(P)\\}$ and $\\{c_i(P)\\}$), the generator is explicitly guided to match these distributions.\n2.  **Differentiability**: The loss terms $\\mathcal{L}_{\\text{deg}}$ and $\\mathcal{L}_{\\text{clust}}$ are calculated using the differentiable surrogates for degree, $d_i(P) = \\sum_j P_{ij}$, and clustering, $c_i(P) = \\frac{(P^3)_{ii}}{d_i(P)\\big(d_i(P)-1\\big) + \\epsilon}$. Both $d_i(P)$ and $c_i(P)$ are differentiable functions of the elements of $P$, which in turn is a smooth function of the generator's parameters $\\theta$. Assuming a differentiable kernel $k(\\cdot,\\cdot)$ (e.g., Gaussian RBF), the entire MMD expression is differentiable with respect to its inputs, and thus the penalty terms are differentiable with respect to $\\theta$.\n3.  **Optimization**: The proposed objective, including the Augmented Lagrangian-style terms, is amenable to stochastic gradient descent. The additional terms can be computed for each mini-batch and added to the generator's loss.\nThis approach provides an explicit, differentiable, and stochastically optimizable mechanism to enforce the desired structural properties.\n\nVerdict on B: **Correct**.\n\nC. Replace the standard GAN with a Wasserstein GAN with gradient penalty, and enforce only the average degree by a scalar penalty. After each generator update, project the sampled graphs onto the set with the target degree sequence using the Havel–Hakimi algorithm.\n\nThis proposal is fundamentally flawed for several reasons.\n1.  **Enforcing Properties**:\n    *   **Degree**: The penalty term $\\lambda \\big(\\bar{d}(P) - \\bar{k}(A)\\big)^{2}$ only matches the *average* degree of the graph, not the entire degree *distribution*. A power-law distribution and a Poisson distribution could have the same mean but are structurally very different.\n    *   **Clustering**: The proposal includes no mechanism whatsoever to match the clustering distribution. It fails to meet a primary requirement.\n2.  **Constraint Mechanism**: The use of the Havel–Hakimi algorithm as a \"projection\" is incorrect. First, it is a discrete, combinatorial algorithm and is therefore non-differentiable. Placing it as a post-processing step *after* the generator update means no gradient signal from this step can flow back to the generator. The generator does not learn from this correction. It effectively learns to produce graphs that are then discarded and replaced by a different graph that has the desired degree sequence. This breaks the end-to-end training principle of GANs.\n\nVerdict on C: **Incorrect**.\n\nD. Use multiple discriminators: a base discriminator on adjacency matrices, an auxiliary discriminator on degree sequences, and an auxiliary discriminator on local clustering vectors.\n\nThis proposal employs a sophisticated and valid adversarial technique.\n1.  **Enforcing Properties**: Instead of using a fixed statistical metric like MMD, this approach uses auxiliary discriminators, $D^{(k)}_{\\psi}$ and $D^{(c)}_{\\xi}$, to learn a distance function between the distributions of graph properties. $D^{(k)}_{\\psi}$ is trained to distinguish between degree vectors from real graphs ($k(A)$) and expected degree vectors from the generator ($d(P)$). To minimize its loss, the generator must produce $d(P)$ vectors whose distribution is indistinguishable from that of $k(A)$. The same logic applies to the clustering discriminator $D^{(c)}_{\\xi}$ and the vectors $c(A)$ and $c(P)$. This is a powerful application of the adversarial principle to enforce matching of specific feature distributions.\n2.  **Differentiability**: As in option B, the generator's loss depends on the output of discriminators applied to the differentiable surrogates $d(P)$ and $c(P)$. The gradients from the auxiliary discriminators' decisions can be backpropagated through the differentiable surrogate calculations and the generator network. The entire mechanism is differentiable with respect to $\\theta$.\n3.  **Optimization**: The objective describes a multi-discriminator GAN setup, which is a known, albeit complex, training paradigm. The suggestion to use gradient penalties or spectral normalization indicates an awareness of the stability issues and their standard solutions, making the proposal practical and amenable to stochastic optimization.\n\nVerdict on D: **Correct**.\n\nE. Adopt a mutual-information regularized objective that maximizes the mutual information between a subset of latent variables and the sorted degree sequence of the generated graph, without any explicit matching to real degree or clustering distributions. Use an InfoGAN-style term.\n\nThis proposal misinterprets the goal of the problem.\n1.  **Enforcing Properties**: The objective of InfoGAN is to learn disentangled and interpretable latent representations. Maximizing mutual information $I\\big(c; \\mathrm{sort}(d(P))\\big)$ gives the user *control* over the generated graph's degree sequence by manipulating the latent code $c$. However, it does not enforce that the *distribution* of degree sequences produced by the generator matches the one found in the real data $\\mathcal{P}_{\\text{data}}$. The generator might learn to produce a variety of valid degree sequences, but they might not be the ones characteristic of the target dataset. Furthermore, the proposal completely ignores the requirement to match the clustering distribution.\n2.  **Differentiability**: The InfoGAN objective is amenable to stochastic optimization via a variational lower bound, so it is technically sound in that regard. However, its purpose is fundamentally different from the stated goal.\n\nVerdict on E: **Incorrect**.\n\nIn summary, proposals B and D both present valid, state-of-the-art methods for explicitly enforcing distributional constraints on generated graphs within a differentiable, end-to-end learning framework. They correctly use differentiable surrogates for the non-differentiable graph properties to allow for gradient-based optimization of the generator.",
            "answer": "$$\\boxed{BD}$$"
        }
    ]
}