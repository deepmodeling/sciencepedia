{
    "hands_on_practices": [
        {
            "introduction": "Before we can estimate spillover effects, we must first understand the nature of network exposure itself. This fundamental exercise  explores the statistical properties of a common exposure measure—the fraction of treated neighbors—under a simple random treatment assignment. By deriving its expectation and variance, you will build intuition for how a node's structural position, captured by its degree $d_i$, shapes its experience of treatment spillovers from its environment.",
            "id": "4266879",
            "problem": "Consider a simple undirected network with $n$ units represented by a binary adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ that is symmetric and has no self-loops, so $A_{ij} = A_{ji}$ and $A_{ii} = 0$ for all $i$. Let the degree of unit $i$ be $d_i = \\sum_{j=1}^{n} A_{ij}$, and restrict attention to units with $d_i \\geq 1$. Suppose treatments are assigned independently to each unit according to a Bernoulli($p$) scheme: for each unit $j$, the treatment indicator $Z_j \\in \\{0,1\\}$ is drawn independent and identically distributed (IID) with $\\mathbb{P}(Z_j = 1) = p$ and $\\mathbb{P}(Z_j = 0) = 1 - p$, independently across all $j$.\n\nIn the presence of network interference, consider the neighborhood exposure mapping for unit $i$ defined by\n$$\nE_i \\equiv \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j,\n$$\nwhich is the fraction of treated neighbors of unit $i$. Starting from the fundamental properties of expectation and variance for independent random variables and the known facts for the Bernoulli distribution, derive a closed-form expression for the expected exposure $\\mathbb{E}[E_i]$ and its variance $\\operatorname{Var}(E_i)$ as functions of $p$ and $d_i$.\n\nExpress your final answer as a single closed-form analytic expression containing both quantities in a single row matrix. No rounding is required.",
            "solution": "The problem requires the derivation of the expectation, $\\mathbb{E}[E_i]$, and variance, $\\operatorname{Var}(E_i)$, of the neighborhood exposure for a unit $i$ in a network. The neighborhood exposure, $E_i$, is defined as:\n$$\nE_i \\equiv \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\n$$\nwhere $A$ is the adjacency matrix, $d_i = \\sum_{j=1}^{n} A_{ij}$ is the degree of unit $i$ (with the constraint $d_i \\geq 1$), and $Z_j$ are independent and identically distributed (IID) Bernoulli random variables with parameter $p$. Specifically, $\\mathbb{P}(Z_j = 1) = p$ and $\\mathbb{P}(Z_j = 0) = 1 - p$. The known properties of a Bernoulli($p$) random variable $Z$ are its expectation, $\\mathbb{E}[Z] = p$, and its variance, $\\operatorname{Var}(Z) = p(1-p)$.\n\nFirst, we derive the expected value of $E_i$. We apply the property of linearity of expectation, which states that for random variables $X_k$ and constants $c_k$, $\\mathbb{E}\\left[\\sum_k c_k X_k\\right] = \\sum_k c_k \\mathbb{E}[X_k]$. In our case, the random variables are $Z_j$, and the constants are the coefficients $\\frac{A_{ij}}{d_i}$. The adjacency matrix $A$ and the degree $d_i$ are considered fixed, non-random quantities.\n\n$$\n\\mathbb{E}[E_i] = \\mathbb{E}\\left[\\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\\right]\n$$\n\nSince $d_i$ and $A_{ij}$ are constants, we can write:\n$$\n\\mathbb{E}[E_i] = \\frac{1}{d_i} \\sum_{j=1}^{n} \\mathbb{E}[A_{ij} Z_j] = \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} \\mathbb{E}[Z_j]\n$$\n\nEach $Z_j$ is a Bernoulli random variable with parameter $p$, so its expectation is $\\mathbb{E}[Z_j] = p$. Substituting this into the equation gives:\n$$\n\\mathbb{E}[E_i] = \\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} p\n$$\n\nThe parameter $p$ is a constant with respect to the summation index $j$, so it can be factored out:\n$$\n\\mathbb{E}[E_i] = \\frac{p}{d_i} \\sum_{j=1}^{n} A_{ij}\n$$\n\nBy the definition of the degree of unit $i$, we have $d_i = \\sum_{j=1}^{n} A_{ij}$. Substituting this definition into the expression for the expectation yields:\n$$\n\\mathbb{E}[E_i] = \\frac{p}{d_i} (d_i) = p\n$$\nThe constraint $d_i \\ge 1$ ensures that this expression is well-defined. Thus, the expected neighborhood exposure for any unit $i$ is simply the probability of treatment, $p$.\n\nNext, we derive the variance of $E_i$. We start with the definition of variance and use the property that for a constant $c$ and a random variable $X$, $\\operatorname{Var}(cX) = c^2 \\operatorname{Var}(X)$.\n$$\n\\operatorname{Var}(E_i) = \\operatorname{Var}\\left(\\frac{1}{d_i} \\sum_{j=1}^{n} A_{ij} Z_j\\right) = \\left(\\frac{1}{d_i}\\right)^2 \\operatorname{Var}\\left(\\sum_{j=1}^{n} A_{ij} Z_j\\right) = \\frac{1}{d_i^2} \\operatorname{Var}\\left(\\sum_{j=1}^{n} A_{ij} Z_j\\right)\n$$\n\nThe sum $\\sum_{j=1}^{n} A_{ij} Z_j$ can be simplified. Since $A_{ij}$ is either $1$ if $j$ is a neighbor of $i$ or $0$ otherwise, this sum is equivalent to summing $Z_j$ over the set of neighbors of $i$, which we can denote as $\\mathcal{N}_i = \\{j \\mid A_{ij}=1\\}$. The number of neighbors is $|\\mathcal{N}_i| = d_i$.\n$$\n\\sum_{j=1}^{n} A_{ij} Z_j = \\sum_{j \\in \\mathcal{N}_i} Z_j\n$$\n\nThe problem states that the variables $Z_j$ are drawn IID for all $j \\in \\{1, \\dots, n\\}$. Consequently, the variables $\\{Z_j \\mid j \\in \\mathcal{N}_i\\}$ are a set of mutually independent random variables. A fundamental property of variance is that for a sum of independent random variables $X_k$, the variance of the sum is the sum of the variances: $\\operatorname{Var}\\left(\\sum_k X_k\\right) = \\sum_k \\operatorname{Var}(X_k)$.\n\nApplying this property, we get:\n$$\n\\operatorname{Var}\\left(\\sum_{j \\in \\mathcal{N}_i} Z_j\\right) = \\sum_{j \\in \\mathcal{N}_i} \\operatorname{Var}(Z_j)\n$$\n\nSince each $Z_j$ is an IID Bernoulli($p$) variable, its variance is $\\operatorname{Var}(Z_j) = p(1-p)$. This variance is the same for all $j$. The sum is over the $d_i$ neighbors of unit $i$.\n$$\n\\sum_{j \\in \\mathcal{N}_i} \\operatorname{Var}(Z_j) = \\sum_{j \\in \\mathcal{N}_i} p(1-p) = d_i \\cdot p(1-p)\n$$\n\nFinally, we substitute this result back into the expression for $\\operatorname{Var}(E_i)$:\n$$\n\\operatorname{Var}(E_i) = \\frac{1}{d_i^2} \\left[d_i \\cdot p(1-p)\\right]\n$$\n\nGiven the problem constraint that $d_i \\ge 1$, we can simplify the expression by canceling one factor of $d_i$:\n$$\n\\operatorname{Var}(E_i) = \\frac{p(1-p)}{d_i}\n$$\nThis result shows that the variance of the neighborhood exposure is inversely proportional to the degree of the unit.\n\nThe derived quantities are the expectation $\\mathbb{E}[E_i] = p$ and the variance $\\operatorname{Var}(E_i) = \\frac{p(1-p)}{d_i}$. We are asked to present these in a single row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\np & \\frac{p(1-p)}{d_i}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The randomized controlled trial is the ideal for causal inference, but network interference challenges its standard application. This practice  places you in the role of an experimental designer, tasked with partitioning a network for a cluster-randomized trial. Your objective is to leverage the network's structure to create clusters that minimize treatment spillover, thereby strengthening the validity and interpretability of the experimental results.",
            "id": "4266836",
            "problem": "You are given an undirected simple graph $G=(V,E)$ and a set of cluster size constraints. The task is to design a cluster-based experimental assignment that minimizes cross-cluster connectivity, thereby reducing interference between clusters in a causal inference setting under network interference. The fundamental base for this derivation is the definition of potential outcomes under interference, graph partitioning, and cluster-randomized designs. You must construct a program that, for each provided test case, finds a vertex partition into clusters of specified sizes that minimizes the cross-cluster edge density and then computes a leakage metric under cluster-randomized assignment.\n\nDefinitions and objectives are as follows.\n\n- Let $V=\\{0,1,\\dots,n-1\\}$ denote the set of $n$ vertices and $E\\subseteq\\{\\{i,j\\}\\mid i,j\\in V, i<j\\}$ denote the set of undirected edges. Let $\\Pi=\\{C_1,\\dots,C_K\\}$ be a partition of $V$ into $K$ disjoint clusters with exact sizes $\\lvert C_k \\rvert=s_k$ for all $k\\in\\{1,\\dots,K\\}$, where $\\sum_{k=1}^{K} s_k = n$.\n- Define the set of cross-cluster edges under partition $\\Pi$ as $E_{\\times}(\\Pi)=\\big\\{\\{i,j\\}\\in E \\mid i\\in C_a, j\\in C_b, a\\neq b\\big\\}$.\n- Define the cross-cluster edge density as $d_{\\times}(\\Pi)=\\lvert E_{\\times}(\\Pi) \\rvert/\\lvert E\\rvert$, which is the fraction of all edges that span different clusters.\n- Consider cluster-randomized treatment assignment where each cluster $C_k$ is independently assigned treatment with probability $p\\in(0,1)$; here, take $p=0.5$.\n- Define the leakage index under partition $\\Pi$ as $L(\\Pi)=p\\cdot d_{\\times}(\\Pi)$, which represents the expected fraction of edges that both cross clusters and connect to treated vertices in external clusters under independent cluster-level randomization with probability $p$.\n\nYour program must, for each test case below, compute the partition $\\Pi^\\star$ that minimizes $d_{\\times}(\\Pi)$ subject to the exact cluster sizes, and then report the pair $\\big(d_{\\times}(\\Pi^\\star), L(\\Pi^\\star)\\big)$.\n\nTest suite:\n\n- Test case $1$ (community-like graph):\n  - Vertex set $V=\\{0,1,2,3,4,5,6,7,8,9\\}$.\n  - Edge set $E$ consists of the following undirected edges:\n    - Within $\\{0,1,2,3,4\\}$: $\\{(0,1),(1,2),(2,3),(3,4),(4,0),(0,2),(1,3),(2,4)\\}$.\n    - Within $\\{5,6,7,8,9\\}$: $\\{(5,6),(6,7),(7,8),(8,9),(9,5),(5,7),(6,8),(7,9)\\}$.\n    - Across the two groups: $\\{(4,5),(3,6),(1,7),(0,8)\\}$.\n  - Cluster size constraints: $[5,5]$ (that is, $K=2$ clusters, sizes $s_1=5$, $s_2=5$).\n  - Use $p=0.5$.\n\n- Test case $2$ (complete graph):\n  - Vertex set $V=\\{0,1,2,3,4,5\\}$.\n  - Edge set $E$ consists of all $\\binom{6}{2}$ undirected edges of the complete graph $K_6$.\n  - Cluster size constraints: $[3,3]$ (that is, $K=2$ clusters, sizes $s_1=3$, $s_2=3$).\n  - Use $p=0.5$.\n\n- Test case $3$ (path graph):\n  - Vertex set $V=\\{0,1,2,3,4,5,6,7\\}$.\n  - Edge set $E$ consists of the path edges $\\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7)\\}$.\n  - Cluster size constraints: $[3,3,2]$ (that is, $K=3$ clusters, sizes $s_1=3$, $s_2=3$, $s_3=2$).\n  - Use $p=0.5$.\n\nScientific realism and justification: You must justify, in your solution, using first principles of causal inference under interference, why minimizing $d_{\\times}(\\Pi)$ reduces cross-cluster interference under cluster randomization and how $L(\\Pi)$ quantifies expected external exposure under independent cluster-level assignment.\n\nFinal output specification: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element list of floats in the order $\\big[d_{\\times}(\\Pi^\\star),L(\\Pi^\\star)\\big]$. For example, the format must be `[[x1,y1],[x2,y2],[x3,y3]]` with no spaces. No physical units are involved, and floats must be expressed as decimals (not percentages). Your program must be self-contained and require no input.",
            "solution": "The problem is to find a vertex partition of a given graph that minimizes cross-cluster edge density, subject to specified cluster sizes, and then to compute a leakage metric. This task is situated within the framework of designing experiments for causal inference in the presence of network interference.\n\n### Scientific Justification\n\nCausal inference on networks grapples with the breakdown of the Stable Unit Treatment Value Assumption (SUTVA), which posits that a unit's outcome is only affected by its own treatment status. In a network context, interference arises when the treatment of one unit affects the outcomes of its neighbors, a phenomenon mediated by the edges of the graph.\n\nLet $V$ be the set of units, and let $Z_i \\in \\{0, 1\\}$ be the treatment assigned to unit $i \\in V$. The vector of all treatment assignments is $\\mathbf{Z} \\in \\{0,1\\}^{\\lvert V \\rvert}$. Under interference, the potential outcome for unit $i$ is denoted $Y_i(\\mathbf{Z})$, explicitly depending on the entire treatment vector. This contrasts with the no-interference case where $Y_i(\\mathbf{Z}) = Y_i(Z_i)$.\n\nA common strategy to mitigate (though not eliminate) interference and enable the estimation of causal effects is cluster-randomized assignment. The set of vertices $V$ is partitioned into $K$ disjoint clusters, $\\Pi = \\{C_1, \\dots, C_K\\}$. Treatment is then assigned at the cluster level, meaning all units within a cluster $C_k$ receive the same treatment status, $Z_k$. This design ensures no interference *within* clusters (as all units are co-treated or co-controlled), localizing interference to a between-cluster phenomenon.\n\nThe objective is to create a partition $\\Pi$ that minimizes this inter-cluster interference. Interference between two clusters, $C_a$ and $C_b$, is transmitted through the set of edges connecting them, i.e., edges $\\{i,j\\}$ where $i \\in C_a$ and $j \\in C_b$. The set of all such edges in the graph is the cross-cluster edge set, $E_{\\times}(\\Pi) = \\big\\{\\{i,j\\}\\in E \\mid i\\in C_a, j\\in C_b, a\\neq b\\big\\}$. Minimizing the cardinality of this set, $\\lvert E_{\\times}(\\Pi) \\rvert$, directly reduces the number of pathways for treatment spillage between clusters. Since the total number of edges, $\\lvert E \\rvert$, is a constant for a given graph, minimizing $\\lvert E_{\\times}(\\Pi) \\rvert$ is equivalent to minimizing the cross-cluster edge density, $d_{\\times}(\\Pi) = \\lvert E_{\\times}(\\Pi) \\rvert / \\lvert E \\rvert$. Therefore, finding the partition $\\Pi^\\star$ that minimizes $d_{\\times}(\\Pi)$ is a principled approach to designing an experiment that maximally curtails network interference.\n\nThe leakage index, $L(\\Pi) = p \\cdot d_{\\times}(\\Pi)$, quantifies the expected exposure to treatment from external clusters. Given a random edge $\\{i,j\\} \\in E$, the probability that it crosses clusters is $d_{\\times}(\\Pi)$. The problem defines $L(\\Pi)$ to represent \"the expected fraction of edges that both cross clusters and connect to treated vertices in external clusters\". A more precise interpretation is the expected fraction of *edge-endpoints* that are part of a cross-cluster edge and belong to a treated cluster. The total number of edge-endpoints is $2\\lvert E \\rvert$. The number of cross-cluster edge-endpoints is $2\\lvert E_{\\times}(\\Pi) \\rvert$. Since each cluster is treated with probability $p$, the expected number of cross-cluster endpoints belonging to a treated cluster is $p \\cdot 2\\lvert E_{\\times}(\\Pi) \\rvert$. The expected fraction is thus $\\frac{p \\cdot 2\\lvert E_{\\times}(\\Pi) \\rvert}{2\\lvert E \\rvert} = p \\cdot \\frac{\\lvert E_{\\times}(\\Pi) \\rvert}{\\lvert E \\rvert} = p \\cdot d_{\\times}(\\Pi)$. This metric serves as a simple, aggregate measure of the potential for treatment to \"leak\" across cluster boundaries.\n\n### Algorithmic Approach\n\nThe problem of partitioning a graph to minimize cuts is a well-known NP-hard problem. However, the vertex sets in the provided test cases are small ($n \\le 10$), making a brute-force search computationally feasible. The algorithm proceeds as follows:\n$1$. For each test case, represent the graph $G=(V,E)$ and the cluster size constraints $\\{s_k\\}$.\n$2$. Systematically generate all possible partitions of the vertex set $V$ that satisfy the specific size constraints. This is achieved via a recursive algorithm that, for a list of sizes, iteratively selects combinations of available nodes for the first cluster size, then recurses on the remaining nodes and sizes. To handle symmetries (i.e., partitions $\\{C_1, C_2\\}$ and $\\{C_2, C_1\\}$ are identical if $|C_1|=|C_2|$), we canonicalize each generated partition (e.g., by sorting the clusters) and store them in a set to avoid redundant computations.\n$3$. For each unique partition $\\Pi = \\{C_1, \\dots, C_K\\}$, calculate the number of cross-cluster edges, $\\lvert E_{\\times}(\\Pi) \\rvert$. This is done by first calculating the number of internal edges, $\\sum_k \\lvert E(C_k) \\rvert$, and subtracting this from the total number of edges, $\\lvert E \\rvert$.\n$4$. Track the minimum number of cross-cluster edges, $\\lvert E_{\\times}(\\Pi^\\star) \\rvert$, found across all partitions.\n$5$. After checking all unique partitions, compute the minimized cross-cluster density: $d_{\\times}(\\Pi^\\star) = \\lvert E_{\\times}(\\Pi^\\star) \\rvert / \\lvert E \\rvert$.\n$6$. Compute the corresponding leakage index: $L(\\Pi^\\star) = p \\cdot d_{\\times}(\\Pi^\\star)$, using the given probability $p=0.5$.\n\n### Step-by-Step Derivations\n\n**Test Case 1**\n- Graph: $n=10$ vertices. The edges form two dense subgraphs, $\\{0,1,2,3,4\\}$ and $\\{5,6,7,8,9\\}$, with a few edges between them. Total edges $\\lvert E \\rvert = 8+8+4=20$.\n- Constraints: $K=2$ clusters of sizes $s_1=5$, $s_2=5$.\n- The number of ways to choose $5$ vertices out of $10$ for the first cluster is $\\binom{10}{5} = 252$. Since the clusters have equal size, swapping them yields the same partition, so there are $\\frac{1}{2}\\binom{10}{5} = 126$ unique partitions.\n- By inspection, the optimal partition $\\Pi^\\star$ aligns with the graph's community structure: $\\Pi^\\star = \\big\\{\\{0,1,2,3,4\\}, \\{5,6,7,8,9\\}\\big\\}$.\n- For this partition, the cross-cluster edges are precisely the $4$ edges connecting the two communities: $\\{(4,5),(3,6),(1,7),(0,8)\\}$. Thus, $\\lvert E_{\\times}(\\Pi^\\star) \\rvert = 4$. Any other partition would sever one of the dense internal structures, increasing the cross-edge count.\n- Cross-cluster density: $d_{\\times}(\\Pi^\\star) = \\lvert E_{\\times}(\\Pi^\\star) \\rvert / \\lvert E \\rvert = 4 / 20 = 0.2$.\n- Leakage index: $L(\\Pi^\\star) = p \\cdot d_{\\times}(\\Pi^\\star) = 0.5 \\cdot 0.2 = 0.1$.\n- Result: $\\big[0.2, 0.1\\big]$.\n\n**Test Case 2**\n- Graph: A complete graph $K_6$ on $n=6$ vertices. The number of edges is $\\lvert E \\rvert = \\binom{6}{2} = 15$.\n- Constraints: $K=2$ clusters of sizes $s_1=3$, $s_2=3$.\n- Let a partition be $\\Pi = \\{C_1, C_2\\}$ with $\\lvert C_1 \\rvert = 3$ and $\\lvert C_2 \\rvert = 3$.\n- In a complete graph, the number of edges within a cluster of size $s$ is $\\binom{s}{2}$.\n- Number of edges within $C_1$: $\\binom{3}{2} = 3$.\n- Number of edges within $C_2$: $\\binom{3}{2} = 3$.\n- Total internal edges: $3+3=6$. This is constant for any partition of this size structure.\n- Number of cross-cluster edges: $\\lvert E_{\\times}(\\Pi) \\rvert = \\lvert E \\rvert - (\\text{internal edges}) = 15 - 6 = 9$.\n- Since this value is the same for all valid partitions, $\\lvert E_{\\times}(\\Pi^\\star) \\rvert = 9$.\n- Cross-cluster density: $d_{\\times}(\\Pi^\\star) = 9 / 15 = 0.6$.\n- Leakage index: $L(\\Pi^\\star) = p \\cdot d_{\\times}(\\Pi^\\star) = 0.5 \\cdot 0.6 = 0.3$.\n- Result: $\\big[0.6, 0.3\\big]$.\n\n**Test Case 3**\n- Graph: A path graph on $n=8$ vertices, $0-1-2-3-4-5-6-7$. The number of edges is $\\lvert E \\rvert = 7$.\n- Constraints: $K=3$ clusters of sizes $s_1=3$, $s_2=3$, $s_3=2$.\n- To minimize cross-cluster edges on a path graph, we should make cuts that sever the path. Partitioning into $K=3$ clusters requires at least $K-1=2$ cuts (edges removed).\n- Consider the partition of contiguous blocks: $\\Pi^\\star = \\big\\{\\{0,1,2\\}, \\{3,4,5\\}, \\{6,7\\}\\big\\}$.\n- This partition is valid as the sizes are $3$, $3$, and $2$.\n- The edges cut by this partition are $\\{2,3\\}$ and $\\{5,6\\}$. There are exactly $2$ such edges.\n- Thus, $\\lvert E_{\\times}(\\Pi^\\star) \\rvert = 2$. This is the minimum possible.\n- Cross-cluster density: $d_{\\times}(\\Pi^\\star) = 2 / 7$.\n- Leakage index: $L(\\Pi^\\star) = p \\cdot d_{\\times}(\\Pi^\\star) = 0.5 \\cdot (2 / 7) = 1 / 7$.\n- Numerically, $2/7 \\approx 0.2857142857$ and $1/7 \\approx 0.1428571428$.\n- Result: $\\big[0.2857142857142857, 0.14285714285714285\\big]$.",
            "answer": "[[0.2,0.1],[0.6,0.3],[0.2857142857142857,0.14285714285714285]]"
        },
        {
            "introduction": "While well-designed experiments are powerful, researchers often work with observational data where treatments are not randomly assigned. This advanced exercise  demonstrates how to identify causal effects in such settings by deriving an inverse probability weighting (IPW) formula. You will see how this classic causal inference technique can be extended to account for confounding in both an individual's treatment $Z_i$ and their network exposure $E_i$, a crucial skill for real-world network analysis.",
            "id": "4266843",
            "problem": "Consider a population of $n$ units indexed by $i \\in \\{1,\\dots,n\\}$ embedded in a network. Each unit has a binary treatment $Z_i \\in \\{0,1\\}$, a network exposure $E_i$ summarizing the treatments of its neighbors through a known exposure mapping, baseline covariates $W_i$, and an observed outcome $Y_i$. Assume the exposure $E_i$ takes values in a discrete support $\\mathcal{E}$ and that potential outcomes under interference are well-defined as $Y_i(z,e)$ for $z \\in \\{0,1\\}$ and $e \\in \\mathcal{E}$. Let the observed data obey the consistency relation $Y_i = Y_i(Z_i,E_i)$.\n\nDefine the causal direct effect at exposure level $E \\in \\mathcal{E}$ by\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) - Y_i(0,E) \\big],\n$$\nwhere the expectation is with respect to the population distribution of units.\n\nYou are given the joint propensity score\n$$\ne_i(z,E) = \\Pr\\big( Z_i = z,\\; E_i = E \\mid W_i \\big),\n$$\nwhich is strictly between $0$ and $1$ whenever $\\Pr(W_i \\in \\mathcal{W}) > 0$ and $(z,E)$ is in the support of $(Z_i,E_i)$ given $W_i$. Assume the following identification conditions:\n- Consistency: $Y_i = Y_i(Z_i,E_i)$ for all $i$.\n- Conditional ignorability: $\\{Y_i(1,E), Y_i(0,E)\\} \\perp (Z_i, E_i) \\mid W_i$ for all $E \\in \\mathcal{E}$.\n- Positivity: $0  e_i(z,E)  1$ almost surely for all $z \\in \\{0,1\\}$ and $E \\in \\mathcal{E}$ in the support.\n\nStarting from these assumptions and the definitions of conditional expectation and the law of iterated expectations, derive an identification formula for $\\tau^{\\text{dir}}(E)$ using inverse probability weighting based on the joint propensity score $e_i(z,E)$. Your final answer must be a single closed-form analytic expression. No rounding is required, and no units apply to the final expression.",
            "solution": "The objective is to derive an identification formula for the causal direct effect, $\\tau^{\\text{dir}}(E)$, using the provided assumptions and inverse probability weighting (IPW). The direct effect is defined as:\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) - Y_i(0,E) \\big]\n$$\nBy the linearity of the expectation operator, we can separate this into two terms:\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\big[ Y_i(1,E) \\big] - \\mathbb{E}\\big[ Y_i(0,E) \\big]\n$$\nWe will derive an identification formula for a generic mean potential outcome, $\\mu(z,E) = \\mathbb{E}\\big[ Y_i(z,E) \\big]$, for a given treatment level $z \\in \\{0,1\\}$ and exposure level $E \\in \\mathcal{E}$.\n\nLet $\\mathbb{I}(\\cdot)$ be the indicator function. The IPW estimator relies on weighting observed outcomes by the inverse of the probability of receiving the observed treatment. Consider the following expectation:\n$$\n\\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right]\n$$\nWe can analyze this expression using the law of iterated expectations by conditioning on the covariates $W_i$:\n$$\n\\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right] = \\mathbb{E}_{W_i}\\left[ \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\;\\middle|\\; W_i \\right] \\right]\n$$\nSince the propensity score $e_i(z,E) = \\Pr(Z_i=z, E_i=E \\mid W_i)$ is a function of $W_i$, it can be treated as a constant inside the inner expectation:\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i \\;\\middle|\\; W_i \\right] \\right]\n$$\nBy the **Consistency** assumption, $Y_i = Y_i(Z_i, E_i)$, we can replace the observed outcome $Y_i$ with the potential outcome corresponding to the observed treatment and exposure. The term $\\mathbb{I}(Z_i=z, E_i=E) Y_i$ is equivalent to $\\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E)$.\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E) \\;\\middle|\\; W_i \\right] \\right]\n$$\nNext, by the **Conditional Ignorability** assumption, $Y_i(z,E)$ is independent of $(Z_i, E_i)$ conditional on $W_i$. This allows us to factor the inner expectation:\n$$\n\\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) Y_i(z,E) \\;\\middle|\\; W_i \\right] = \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] \\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) \\mid W_i \\right]\n$$\nThe second term in this product is simply the definition of the joint propensity score, $e_i(z,E)$:\n$$\n\\mathbb{E}\\left[ \\mathbb{I}(Z_i=z, E_i=E) \\mid W_i \\right] = \\Pr(Z_i=z, E_i=E \\mid W_i) = e_i(z,E)\n$$\nSubstituting this back, we get:\n$$\n= \\mathbb{E}_{W_i}\\left[ \\frac{1}{e_i(z,E)} \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] e_i(z,E) \\right]\n$$\nThe **Positivity** assumption ($e_i(z,E)  0$) ensures the fraction is well-defined and allows the propensity score terms to cancel:\n$$\n= \\mathbb{E}_{W_i}\\left[ \\mathbb{E}\\left[ Y_i(z,E) \\mid W_i \\right] \\right]\n$$\nBy the law of iterated expectations, this simplifies to the target quantity:\n$$\n= \\mathbb{E}\\big[ Y_i(z,E) \\big] = \\mu(z,E)\n$$\nThus, we have identified the mean potential outcome:\n$$\n\\mathbb{E}\\big[ Y_i(z,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=z, E_i=E)}{e_i(z,E)} Y_i \\right]\n$$\nApplying this formula to both terms of the direct effect:\n$$\n\\mathbb{E}\\big[ Y_i(1,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} Y_i \\right]\n$$\n$$\n\\mathbb{E}\\big[ Y_i(0,E) \\big] = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} Y_i \\right]\n$$\nSubtracting the second from the first and using the linearity of expectation gives the final identification formula for the direct effect:\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\left[ \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} Y_i - \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} Y_i \\right]\n$$\nThis can be combined into a single expression:\n$$\n\\tau^{\\text{dir}}(E) = \\mathbb{E}\\left[ \\left( \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} - \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} \\right) Y_i \\right]\n$$\nThis is the required closed-form analytic expression, identified from observed data.",
            "answer": "$$\n\\boxed{\\mathbb{E}\\left[ \\left( \\frac{\\mathbb{I}(Z_i=1, E_i=E)}{e_i(1,E)} - \\frac{\\mathbb{I}(Z_i=0, E_i=E)}{e_i(0,E)} \\right) Y_i \\right]}\n$$"
        }
    ]
}