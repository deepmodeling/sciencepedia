{
    "hands_on_practices": [
        {
            "introduction": "One of the most fundamental, and often consequential, decisions in designing an agent-based model is the update scheme. This choice dictates the 'flow of time' within the simulation and how agents perceive and react to their environment. This exercise  provides a minimal, powerful demonstration of how different update schemes—synchronous versus sequential—can lead to qualitatively different emergent dynamics, even when the individual agent's decision rule is identical. By working through this simple two-agent system, you will gain a crucial, firsthand understanding of how implementation details can produce artifacts like spurious oscillations or lock the system into specific states.",
            "id": "4113495",
            "problem": "Consider an Agent-Based Model (ABM) in the domain of complex adaptive systems modeling with two agents labeled $i \\in \\{1,2\\}$ on an undirected network consisting of a single edge connecting the two agents. Each agent has a binary state $x_i(t) \\in \\{0,1\\}$ at discrete time $t \\in \\mathbb{Z}_{\\ge 0}$. The ABM employs a deterministic threshold decision rule: an agent becomes active at the next time step if and only if the fraction of its neighbors that are active meets or exceeds its threshold. Formally, for any agent $i$, let $N_i$ denote the set of neighbors of $i$, and let $m_i(\\tau) = \\frac{1}{|N_i|} \\sum_{j \\in N_i} x_j(\\tau)$ be the fraction of neighbors active at an update reference time $\\tau$. The threshold parameter for agent $i$ is $\\theta_i \\in [0,1]$, and the individual update rule is\n$$\nx_i(\\text{next}) = \n\\begin{cases}\n1, & \\text{if } m_i(\\tau) \\ge \\theta_i, \\\\\n0, & \\text{if } m_i(\\tau) < \\theta_i.\n\\end{cases}\n$$\nTwo update regimes are considered:\n\n- Synchronous update: Both agents compute $m_i(t)$ using the states at time $t$ and simultaneously set $x_i(t+1)$ according to the threshold rule.\n- Sequential update with fixed order $(1,2)$: Agent $1$ updates first using $m_1(t)$ to set $x_1(t+1)$, and then agent $2$ updates using $m_2(t+1)$, where $m_2(t+1)$ is computed from the latest available neighbor state after agent $1$ has updated.\n\nSuppose the initial condition is $x_1(0) = 0$ and $x_2(0) = 1$, and the thresholds are $\\theta_1 = \\frac{1}{2}$ and $\\theta_2 = 1$. Because the network has a single edge, each agent’s $m_i(\\tau)$ equals the neighbor’s state at the specified reference time. Define the trajectory difference over a finite horizon $T$ as the cumulative Hamming distance between the two regime trajectories,\n$$\nD(T) = \\sum_{t=0}^{T} \\left( \\left| x_1^{\\text{sync}}(t) - x_1^{\\text{seq}}(t) \\right| + \\left| x_2^{\\text{sync}}(t) - x_2^{\\text{seq}}(t) \\right| \\right),\n$$\nwhere $x_i^{\\text{sync}}(t)$ and $x_i^{\\text{seq}}(t)$ denote the state of agent $i$ at time $t$ under synchronous and sequential updating, respectively.\n\nCompute $D(T)$ for the horizon $T = 10$. Provide your final answer as a single real number. No rounding instruction is necessary for this problem.",
            "solution": "The problem is well-posed, scientifically grounded in the theory of Agent-Based Models (ABMs) and Complex Adaptive Systems, and contains all necessary information for a unique solution. The validation criteria are met.\n\nThe task is to compute the cumulative Hamming distance, $D(T)$, between the state trajectories of a two-agent system under synchronous and sequential update regimes for a time horizon of $T=10$. We begin by analyzing the agent update rules, then simulate the trajectories for both regimes, and finally compute the required sum.\n\nFirst, we formalize the update rules for agent $i \\in \\{1, 2\\}$ with thresholds $\\theta_1 = \\frac{1}{2}$ and $\\theta_2 = 1$. The network consists of a single edge connecting the two agents, so the set of neighbors for agent $1$ is $N_1 = \\{2\\}$ and for agent $2$ is $N_2 = \\{1\\}$. The fraction of active neighbors for agent $i$ is $m_i(\\tau) = \\frac{1}{|N_i|} \\sum_{j \\in N_i} x_j(\\tau)$. This simplifies to $m_1(\\tau) = x_2(\\tau)$ and $m_2(\\tau) = x_1(\\tau)$.\n\nThe individual update rule is $x_i(\\text{next}) = 1$ if $m_i(\\tau) \\ge \\theta_i$ and $x_i(\\text{next}) = 0$ otherwise. Given the binary state space $x_i \\in \\{0, 1\\}$, the rules for each agent become:\n- For agent $1$: $x_1(\\text{next}) = 1$ if and only if $x_2(\\tau) \\ge \\theta_1 = \\frac{1}{2}$. This means $x_1(\\text{next}) = 1$ only if $x_2(\\tau)=1$. Thus, the rule is $x_1(\\text{next}) = x_2(\\tau)$.\n- For agent $2$: $x_2(\\text{next}) = 1$ if and only if $x_1(\\tau) \\ge \\theta_2 = 1$. This means $x_2(\\text{next}) = 1$ only if $x_1(\\tau)=1$. Thus, the rule is $x_2(\\text{next}) = x_1(\\tau)$.\n\nNow we generate the trajectories for both update regimes starting from the initial condition $(x_1(0), x_2(0)) = (0, 1)$.\n\n**1. Synchronous Update Trajectory**\nUnder synchronous updating, both agents' next states are computed based on the current state of the system at time $t$. The rules are:\n$$\n\\begin{cases}\nx_1^{\\text{sync}}(t+1) = x_2^{\\text{sync}}(t) \\\\\nx_2^{\\text{sync}}(t+1) = x_1^{\\text{sync}}(t)\n\\end{cases}\n$$\nWe compute the state vector $(x_1^{\\text{sync}}(t), x_2^{\\text{sync}}(t))$ for $t \\in \\{0, 1, \\dots, 10\\}$:\n- $t=0$: $(0, 1)$ (Initial condition)\n- $t=1$: $(x_1(1), x_2(1)) = (x_2(0), x_1(0)) = (1, 0)$\n- $t=2$: $(x_1(2), x_2(2)) = (x_2(1), x_1(1)) = (0, 1)$\n- $t=3$: $(x_1(3), x_2(3)) = (x_2(2), x_1(2)) = (1, 0)$\nThe system oscillates with a period of $2$. The state is $(0, 1)$ for even values of $t$ and $(1, 0)$ for odd values of $t$.\n\n**2. Sequential Update Trajectory (Order 1, 2)**\nUnder sequential updating with order $(1, 2)$, agent $1$ first updates its state for time $t+1$ based on states at time $t$. Then, agent $2$ updates its state for time $t+1$ using the most recent states, which includes the newly updated state of agent $1$.\nThe rules for one time step from $t$ to $t+1$ are:\n$$\n\\begin{cases}\nx_1^{\\text{seq}}(t+1) = x_2^{\\text{seq}}(t) \\\\\nx_2^{\\text{seq}}(t+1) = x_1^{\\text{seq}}(t+1)\n\\end{cases}\n$$\nWe compute the state vector $(x_1^{\\text{seq}}(t), x_2^{\\text{seq}}(t))$ for $t \\in \\{0, 1, \\dots, 10\\}$:\n- $t=0$: $(0, 1)$ (Initial condition)\n- $t=1$:\n  - Agent $1$ updates: $x_1^{\\text{seq}}(1) = x_2^{\\text{seq}}(0) = 1$.\n  - Agent $2$ updates: $x_2^{\\text{seq}}(1) = x_1^{\\text{seq}}(1) = 1$.\n  - The state at $t=1$ is $(1, 1)$.\n- $t=2$:\n  - Agent $1$ updates: $x_1^{\\text{seq}}(2) = x_2^{\\text{seq}}(1) = 1$.\n  - Agent $2$ updates: $x_2^{\\text{seq}}(2) = x_1^{\\text{seq}}(2) = 1$.\n  - The state at $t=2$ is $(1, 1)$.\nThe system reaches a fixed point $(1, 1)$ at $t=1$. Therefore, for all $t \\ge 1$, the state is $(x_1^{\\text{seq}}(t), x_2^{\\text{seq}}(t)) = (1, 1)$.\n\n**3. Compute the Trajectory Difference $D(10)$**\nThe trajectory difference is defined as $D(T) = \\sum_{t=0}^{T} d(t)$, where the per-step Hamming distance is $d(t) = |x_1^{\\text{sync}}(t) - x_1^{\\text{seq}}(t)| + |x_2^{\\text{sync}}(t) - x_2^{\\text{seq}}(t)|$. We calculate $d(t)$ for each $t$ from $0$ to $10$.\n\n- For $t=0$:\n  - $x^{\\text{sync}}(0) = (0, 1)$ and $x^{\\text{seq}}(0) = (0, 1)$.\n  - $d(0) = |0-0| + |1-1| = 0$.\n\n- For $t \\ge 1$:\n  - The sequential trajectory is fixed: $x^{\\text{seq}}(t) = (1, 1)$.\n  - The synchronous trajectory alternates.\n  - For odd $t \\in \\{1, 3, 5, 7, 9\\}$:\n    - $x^{\\text{sync}}(t) = (1, 0)$.\n    - $d(t) = |1-1| + |0-1| = 0 + 1 = 1$.\n  - For even $t \\in \\{2, 4, 6, 8, 10\\}$:\n    - $x^{\\text{sync}}(t) = (0, 1)$.\n    - $d(t) = |0-1| + |1-1| = 1 + 0 = 1$.\n\nSo, we have $d(0)=0$ and $d(t)=1$ for all $t \\in \\{1, 2, \\dots, 10\\}$.\n\nFinally, we sum these values to find $D(10)$:\n$$\nD(10) = \\sum_{t=0}^{10} d(t) = d(0) + \\sum_{t=1}^{10} d(t)\n$$\n$$\nD(10) = 0 + (d(1) + d(2) + \\dots + d(10))\n$$\n$$\nD(10) = 0 + \\sum_{t=1}^{10} 1 = 10 \\times 1 = 10\n$$\nThe cumulative Hamming distance over the horizon $T=10$ is $10$.",
            "answer": "$$\n\\boxed{10}\n$$"
        },
        {
            "introduction": "While simulation is the primary tool for exploring agent-based models, some simplified systems are amenable to direct mathematical analysis. This practice  challenges you to step away from coding and instead use the tools of probability theory to derive a key macroscopic observable: the expected rate of agent encounters. By applying the principle of linearity of expectation to agents performing a random walk on a lattice, you will see how to connect micro-level agent behaviors to a precise, closed-form expression for a system-level outcome. This skill is invaluable for building intuition, validating simulation results, and understanding the core dynamics of spatial ABMs.",
            "id": "4284371",
            "problem": "Consider an agent-based modeling (ABM) setup on a finite two-dimensional toroidal square lattice of side length $L$, so there are $L^{2}$ sites. There are $N$ indistinguishable agents, with density $\\rho$ defined by $\\rho = N/L^{2}$. Time is discrete, and at each time step every agent updates its position synchronously by choosing uniformly at random one of the $9$ sites in its Moore neighborhood (the current site plus its $8$ nearest neighbors) and moving there; choices are independent across agents and time. Multiple occupancy is allowed. Define an encounter during a time step as an unordered pair of distinct agents that occupy the same lattice site after the movement of that time step. Assume the system is in the spatially homogeneous regime in which the marginal distribution of an agent’s position is uniform over sites (which is the stationary distribution for this symmetric random walk on a torus).\n\nStarting only from the core definitions of probability, independence of agent moves, and linearity of expectation, derive the exact closed-form expression for the expected number of encounters per time step across the entire system, expressed as a function of $\\rho$ and $L$. Provide your final result in its simplest analytic form. Do not provide an inequality or an equation to be solved; your final answer must be a single closed-form expression without units.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution without any internal contradictions.\n\nLet $C$ be the random variable representing the total number of encounters per time step across the entire system. An encounter is defined as an unordered pair of distinct agents occupying the same lattice site after the synchronous movement phase. Our goal is to compute the expected value of $C$, denoted as $\\mathbb{E}[C]$.\n\nThe system consists of $N$ agents on a toroidal lattice with $L^2$ sites. The agents are labeled by an index $i \\in \\{1, 2, \\ldots, N\\}$. To calculate the total expected number of encounters, we can use the principle of linearity of expectation. The total number of encounters $C$ is the sum of encounters over all possible distinct pairs of agents. The number of distinct unordered pairs of agents is given by the binomial coefficient $\\binom{N}{2}$.\n\nLet us define an indicator random variable $I_{ij}$ for each unordered pair of distinct agents $(i, j)$, where $1 \\le i < j \\le N$. The variable $I_{ij}$ is defined as:\n$$\nI_{ij} = \\begin{cases} 1 & \\text{if agents } i \\text{ and } j \\text{ occupy the same site} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe total number of encounters $C$ can then be expressed as the sum of these indicator variables over all distinct pairs:\n$$\nC = \\sum_{1 \\le i < j \\le N} I_{ij}\n$$\nBy the linearity of expectation, the expected total number of encounters is:\n$$\n\\mathbb{E}[C] = \\mathbb{E}\\left[\\sum_{1 \\le i < j \\le N} I_{ij}\\right] = \\sum_{1 \\le i < j \\le N} \\mathbb{E}[I_{ij}]\n$$\nThe expectation of an indicator variable is simply the probability of the event it indicates. Therefore, $\\mathbb{E}[I_{ij}] = P(I_{ij} = 1)$, which is the probability that agents $i$ and $j$ encounter each other.\n\nBecause the agents are indistinguishable and the system dynamics are symmetric for all agents, the probability of an encounter is the same for any pair of distinct agents $(i, j)$. Let $p_{\\text{enc}}$ denote this common probability.\n$$\np_{\\text{enc}} = P(I_{ij} = 1) \\quad \\text{for any } 1 \\le i < j \\le N\n$$\nThe total number of pairs is $\\binom{N}{2} = \\frac{N(N-1)}{2}$. Thus, the expected total number of encounters becomes:\n$$\n\\mathbb{E}[C] = \\binom{N}{2} p_{\\text{enc}} = \\frac{N(N-1)}{2} p_{\\text{enc}}\n$$\nOur next step is to calculate $p_{\\text{enc}}$. Let $X'_k$ denote the random variable for the position of agent $k$ after the movement step. The lattice has $S = L^2$ sites. The problem states that the system is in a spatially homogeneous regime, where the marginal distribution of an agent's position is uniform over the sites. This is the stationary distribution of the random walk. This means that for any agent $k$ and any site $s$, the probability that the agent is at site $s$ after the move is:\n$$\nP(X'_k = s) = \\frac{1}{L^2}\n$$\nAn encounter between agents $i$ and $j$ occurs if $X'_i = X'_j$. To find the probability of this event, we can sum over all possible sites $s$ where they could meet:\n$$\np_{\\text{enc}} = P(X'_i = X'_j) = \\sum_{s=1}^{L^2} P(X'_i = s \\text{ and } X'_j = s)\n$$\nThe problem states that the agents' choices of movement are independent. Consequently, their final positions $X'_i$ and $X'_j$ are independent random variables. Therefore, we can write:\n$$\nP(X'_i = s \\text{ and } X'_j = s) = P(X'_i = s) \\times P(X'_j = s)\n$$\nSubstituting the uniform probability for each agent:\n$$\nP(X'_i = s \\text{ and } X'_j = s) = \\left(\\frac{1}{L^2}\\right) \\times \\left(\\frac{1}{L^2}\\right) = \\frac{1}{L^4}\n$$\nThis probability is the same for any site $s$. Now, we sum over all $L^2$ possible sites:\n$$\np_{\\text{enc}} = \\sum_{s=1}^{L^2} \\frac{1}{L^4} = L^2 \\times \\frac{1}{L^4} = \\frac{1}{L^2}\n$$\nThe specific update rule (Moore neighborhood) is irrelevant for this calculation, as the stationary uniform distribution assumption is given.\n\nNow we substitute this probability back into our expression for the expected number of encounters:\n$$\n\\mathbb{E}[C] = \\frac{N(N-1)}{2} \\times p_{\\text{enc}} = \\frac{N(N-1)}{2L^2}\n$$\nThe final step is to express this result in terms of the agent density $\\rho$ and the lattice side length $L$. The density is defined as $\\rho = N/L^2$. From this, we have $N = \\rho L^2$. Substituting this expression for $N$ into our result for $\\mathbb{E}[C]$:\n$$\n\\mathbb{E}[C] = \\frac{(\\rho L^2)(\\rho L^2 - 1)}{2L^2}\n$$\nThe factor $L^2$ in the numerator and denominator cancels out, leading to the simplified final expression:\n$$\n\\mathbb{E}[C] = \\frac{\\rho(\\rho L^2 - 1)}{2}\n$$\nThis is the exact closed-form expression for the expected number of encounters per time step.",
            "answer": "$$\n\\boxed{\\frac{\\rho(\\rho L^2 - 1)}{2}}\n$$"
        },
        {
            "introduction": "A model's credibility rests on the correctness of its implementation. This hands-on practice  moves into the critical domain of Verification and Validation (V), focusing on how to ensure your computational model accurately reflects its conceptual design. You will implement update rules and then design a suite of unit tests to verify their behavior, check for the preservation of mathematical invariants like conservation laws, and measure code coverage. Engaging with these software engineering principles is essential for building robust, reliable, and trustworthy agent-based simulations.",
            "id": "4284335",
            "problem": "Consider an agent-based model on a ring network where each agent is indexed by $i \\in \\{0,1,\\dots,N-1\\}$ and has a state $s_i(t) = (w_i(t), x_i(t))$ at discrete time $t \\in \\mathbb{N}$. The network has undirected edges between agent $i$ and agent $(i+1) \\bmod N$. Each time step consists of a full sweep of pairwise interactions over all $N$ edges in the order $i=0$ through $i=N-1$, with the pair $(i, (i+1) \\bmod N)$ interacting once per sweep. The state update uses the following fundamental definitions for pairwise interactions between agents $i$ and $j$ with $j = (i+1) \\bmod N$:\n\n- Wealth exchange rule with parameter $\\gamma \\in [0,1]$:\n  - If $\\gamma = 0$, then $(w_i', w_j') = (w_i, w_j)$.\n  - Else if $w_i = w_j$, then $(w_i', w_j') = (w_i, w_j)$.\n  - Else if $\\gamma = 1$, then $(w_i', w_j') = (w_j, w_i)$.\n  - Else, $(w_i', w_j') = \\left(w_i + \\gamma (w_j - w_i),\\ w_j - \\gamma (w_j - w_i)\\right)$.\n\n- Opinion update rule with parameter $\\mu \\in [0,1]$ and threshold $\\theta \\ge 0$:\n  - If $\\mu = 0$, then $(x_i', x_j') = (x_i, x_j)$.\n  - Else if $|x_i - x_j| \\le \\theta$, then $(x_i', x_j') = (x_i, x_j)$.\n  - Else, $(x_i', x_j') = \\left(x_i + \\mu (x_j - x_i),\\ x_j + \\mu (x_i - x_j)\\right)$.\n\nYou must design unit tests for these agent state updates and interaction rules that achieve specified branch coverage and invariance properties. Instrument your update implementation to record which of the following distinct branches are executed at least once across the test suite:\n\n- Wealth branches: \"gamma zero no-op\", \"equal wealth no-op\", \"gamma one full swap\", \"regular exchange\".\n- Opinion branches: \"mu zero no-op\", \"threshold no-op\", \"regular update\".\n\nFrom the definitions above, derive and then verify the following invariance properties for any single full sweep ($T=1$):\n- Total wealth invariance: $\\sum_{i=0}^{N-1} w_i(1) = \\sum_{i=0}^{N-1} w_i(0)$.\n- Wealth nonnegativity invariance: $w_i(1) \\ge 0$ for all $i$ given $w_i(0) \\ge 0$.\n- Opinion bounds invariance: $x_i(1) \\in [-1,1]$ for all $i$ given $x_i(0) \\in [-1,1]$.\n\nYour program must implement the update rules exactly as stated and instrument branch execution to compute the branch coverage ratio defined as\n$$\n\\text{coverage} = \\frac{\\text{number of distinct branches hit across the suite}}{\\text{total number of distinct branches enumerated}}.\n$$\n\nTest Suite:\nUse the following parameter sets. For each test case $k$, perform exactly one full sweep ($T=1$) on the ring with the given initial states and parameters. All arrays are listed in the order of agent indices $i=0,1,\\dots,N-1$.\n\n- Case $\\mathrm{A}$ (happy path): $N=4$, $\\gamma=0.5$, $\\mu=0.5$, $\\theta=0.0$, $w(0)=[1,0,2,3]$, $x(0)=[-0.5,0.5,-1.0,1.0]$.\n- Case $\\mathrm{B}$ (wealth gamma zero, opinion threshold no-op): $N=4$, $\\gamma=0.0$, $\\mu=0.4$, $\\theta=0.5$, $w(0)=[1,2,3,4]$, $x(0)=[0.1,0.1,0.1,0.1]$.\n- Case $\\mathrm{C}$ (equal wealth branch, mu zero no-op): $N=4$, $\\gamma=0.5$, $\\mu=0.0$, $\\theta=0.0$, $w(0)=[2,2,3,4]$, $x(0)=[-0.2,0.6,-0.6,0.2]$.\n- Case $\\mathrm{D}$ (gamma one full swap, opinion update extreme): $N=4$, $\\gamma=1.0$, $\\mu=1.0$, $\\theta=0.0$, $w(0)=[5,1,5,1]$, $x(0)=[-1.0,1.0,-1.0,1.0]$.\n- Case $\\mathrm{E}$ (network size boundary): $N=1$, $\\gamma=0.7$, $\\mu=0.3$, $\\theta=0.1$, $w(0)=[3.0]$, $x(0)=[0.0]$.\n\nRequired boolean result per test case: The invariance property is satisfied if and only if all three invariances (total wealth invariance, wealth nonnegativity invariance, opinion bounds invariance) hold simultaneously for that case. Return a single boolean per case indicating this.\n\nRequired coverage target: For the entire test suite, compute the coverage ratio as defined above and round it to $3$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain five boolean values (one for each test case) followed by one float equal to the achieved branch coverage ratio for the entire suite, rounded to three decimal places. For example: $[b_A,b_B,b_C,b_D,b_E,c]$ where each $b_\\cdot$ is a boolean and $c$ is a float rounded to $3$ decimal places. No additional text should be printed.",
            "solution": "The problem requires the implementation and testing of a specific agent-based model on a ring network. The core of the task is to write a program that simulates the model for a given test suite, verifies certain invariance properties, and calculates the a posteriori branch coverage of the implementation.\n\nFirst, I will provide a formal analysis of the specified invariance properties to confirm their mathematical validity under the given update rules. This serves as a theoretical foundation for the subsequent computational verification. The state of the system at time $t$ is given by the wealth vector $w(t) = [w_0(t), \\dots, w_{N-1}(t)]$ and the opinion vector $x(t) = [x_0(t), \\dots, x_{N-1}(t)]$. A single time step $t=1$ consists of a sequence of $N$ pairwise interactions between agents $(i, (i+1) \\bmod N)$ for $i=0, 1, \\dots, N-1$.\n\n**Analysis of Invariance Properties**\n\n1.  **Total Wealth Invariance:** $\\sum_{k=0}^{N-1} w_k(1) = \\sum_{k=0}^{N-1} w_k(0)$.\n    Let's consider a single pairwise interaction between agents $i$ and $j$. Let their wealths before the interaction be $w_i$ and $w_j$, and after be $w_i'$ and $w_j'$.\n    - If $\\gamma = 0$ or $w_i = w_j$, then $w_i' = w_i$ and $w_j' = w_j$. Thus, $w_i' + w_j' = w_i + w_j$.\n    - If $\\gamma = 1$, then $w_i' = w_j$ and $w_j' = w_i$. Thus, $w_i' + w_j' = w_j + w_i = w_i + w_j$.\n    - For the regular exchange case, $w_i' = w_i + \\gamma (w_j - w_i)$ and $w_j' = w_j - \\gamma (w_j - w_i)$. Summing these yields:\n    $$w_i' + w_j' = \\left(w_i + \\gamma (w_j - w_i)\\right) + \\left(w_j - \\gamma (w_j - w_i)\\right) = w_i + w_j.$$\n    In all cases, the sum of wealth for the interacting pair is conserved. A full sweep from $t=0$ to $t=1$ is a sequence of such pairwise interactions. Since each interaction conserves the local sum, the total sum of wealth across all agents must be conserved for the entire sweep. Therefore, the total wealth invariance property holds.\n\n2.  **Wealth Nonnegativity Invariance:** given $w_k(0) \\ge 0$ for all $k$, then $w_k(1) \\ge 0$ for all $k$.\n    The initial conditions for all test cases provide $w_k(0) \\ge 0$. We need to show that wealth remains non-negative after any interaction.\n    - The no-op and swap cases trivially preserve non-negativity.\n    - For the regular exchange, the new wealths are:\n    $$w_i' = w_i + \\gamma (w_j - w_i) = (1-\\gamma)w_i + \\gamma w_j$$\n    $$w_j' = w_j - \\gamma (w_j - w_i) = w_j + \\gamma (w_i - w_j) = (1-\\gamma)w_j + \\gamma w_i$$\n    Since $\\gamma \\in [0,1]$, it follows that $(1-\\gamma) \\in [0,1]$. Thus, $w_i'$ and $w_j'$ are convex combinations of $w_i$ and $w_j$. If $w_i, w_j \\ge 0$, any convex combination of them must be non-negative. Therefore, $w_i', w_j' \\ge 0$. Since this holds for every interaction in the sweep, wealth non-negativity is an invariant of the system.\n\n3.  **Opinion Bounds Invariance:** given $x_k(0) \\in [-1,1]$ for all $k$, then $x_k(1) \\in [-1,1]$ for all $k$.\n    The initial conditions provide $x_k(0) \\in [-1,1]$. We analyze a single opinion update between agents $i$ and $j$.\n    - The no-op cases trivially preserve the bounds.\n    - For the regular update, the new opinions are:\n    $$x_i' = x_i + \\mu (x_j - x_i) = (1-\\mu)x_i + \\mu x_j$$\n    $$x_j' = x_j + \\mu (x_i - x_j) = (1-\\mu)x_j + \\mu x_i$$\n    Since $\\mu \\in [0,1]$, $(1-\\mu) \\in [0,1]$. Similar to wealth, $x_i'$ and $x_j'$ are convex combinations of $x_i$ and $x_j$. A well-known property of convex combinations is that they are bounded by the minimum and maximum of their components. Let $m = \\min(x_i, x_j)$ and $M = \\max(x_i, x_j)$. Then $m \\le x_i', x_j' \\le M$. Given $x_i, x_j \\in [-1,1]$, we have $-1 \\le m \\le M \\le 1$. Therefore, $x_i', x_j' \\in [m, M] \\subseteq [-1,1]$. The opinion bounds are preserved in each interaction and thus throughout the entire sweep.\n\nAll three invariance properties are mathematically sound. The implementation must now computationally verify them for the given test cases.\n\n**Implementation Design**\n\nThe core of the program will be a function that simulates one full time step for a given test case. This function will take the model parameters ($N, \\gamma, \\mu, \\theta$) and initial states ($w(0), x(0)$) as input.\n\nA global set, `hit_branches`, will be used to record the execution of each distinct logical branch across the entire test suite. The wealth update rule has $4$ branches (\"gamma zero no-op\", \"equal wealth no-op\", \"gamma one full swap\", \"regular exchange\"), and the opinion update rule has $3$ branches (\"mu zero no-op\", \"threshold no-op\", \"regular update\"), for a total of $7$ distinct branches to monitor.\n\nThe simulation of one full sweep proceeds by iterating $i$ from $0$ to $N-1$. For each $i$, the pair of interacting agents is $(i, j)$ where $j = (i+1) \\bmod N$. The states of agents $i$ and $j$ are updated sequentially. The state update of agent $i$ from the $(i-1, i)$ interaction (modulo $N$) becomes the input for the $(i, i+1)$ interaction. This sequential update must be handled correctly by modifying the state arrays in place during the sweep.\n\nAfter the full sweep for a given test case, the final states $w(1)$ and $x(1)$ are compared against the initial states $w(0)$ and $x(0)$ to verify the three invariance properties.\n- **Total Wealth:** Verified using `np.isclose` to account for potential floating-point inaccuracies, i.e., `np.isclose(np.sum(w(1)), np.sum(w(0)))`.\n- **Wealth Nonnegativity:** Verified by checking if all elements of $w(1)$ are greater than or equal to $0$.\n- **Opinion Bounds:** Verified by checking if all elements of $x(1)$ lie in the closed interval $[-1,1]$.\n\nA boolean result is generated for each test case, being `True` if and only if all three properties hold. After processing all test cases, the branch coverage ratio is calculated as the number of unique branches in `hit_branches` divided by the total number of branches ($7$). This ratio is rounded to $3$ decimal places. The final output is a list containing the five boolean results and the computed coverage ratio.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the agent-based model,\n    verify invariants, and compute branch coverage.\n    \"\"\"\n    test_cases = [\n        # Case A (happy path)\n        {'N': 4, 'gamma': 0.5, 'mu': 0.5, 'theta': 0.0, 'w0': [1, 0, 2, 3], 'x0': [-0.5, 0.5, -1.0, 1.0]},\n        # Case B (wealth gamma zero, opinion threshold no-op)\n        {'N': 4, 'gamma': 0.0, 'mu': 0.4, 'theta': 0.5, 'w0': [1, 2, 3, 4], 'x0': [0.1, 0.1, 0.1, 0.1]},\n        # Case C (equal wealth branch, mu zero no-op)\n        {'N': 4, 'gamma': 0.5, 'mu': 0.0, 'theta': 0.0, 'w0': [2, 2, 3, 4], 'x0': [-0.2, 0.6, -0.6, 0.2]},\n        # Case D (gamma one full swap, opinion update extreme)\n        {'N': 4, 'gamma': 1.0, 'mu': 1.0, 'theta': 0.0, 'w0': [5, 1, 5, 1], 'x0': [-1.0, 1.0, -1.0, 1.0]},\n        # Case E (network size boundary)\n        {'N': 1, 'gamma': 0.7, 'mu': 0.3, 'theta': 0.1, 'w0': [3.0], 'x0': [0.0]},\n    ]\n\n    total_branches = {\n        \"gamma zero no-op\", \"equal wealth no-op\", \"gamma one full swap\", \"regular exchange\",\n        \"mu zero no-op\", \"threshold no-op\", \"regular update\"\n    }\n    hit_branches = set()\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        gamma = case['gamma']\n        mu = case['mu']\n        theta = case['theta']\n        \n        w0 = np.array(case['w0'], dtype=float)\n        x0 = np.array(case['x0'], dtype=float)\n        \n        w = w0.copy()\n        x = x0.copy()\n\n        if N > 0:\n            for i in range(N):\n                j = (i + 1) % N\n                \n                # Wealth update logic\n                wi, wj = w[i], w[j]\n                if gamma == 0:\n                    hit_branches.add(\"gamma zero no-op\")\n                    # No change\n                elif wi == wj:\n                    hit_branches.add(\"equal wealth no-op\")\n                    # No change\n                elif gamma == 1:\n                    hit_branches.add(\"gamma one full swap\")\n                    w[i], w[j] = wj, wi\n                else:\n                    hit_branches.add(\"regular exchange\")\n                    delta_w = gamma * (wj - wi)\n                    w[i] += delta_w\n                    w[j] -= delta_w\n\n                # Opinion update logic\n                xi, xj = x[i], x[j]\n                if mu == 0:\n                    hit_branches.add(\"mu zero no-op\")\n                    # No change\n                elif abs(xi - xj) = theta:\n                    hit_branches.add(\"threshold no-op\")\n                    # No change\n                else:\n                    hit_branches.add(\"regular update\")\n                    delta_x_i = mu * (xj - xi)\n                    delta_x_j = mu * (xi - xj)\n                    x[i] += delta_x_i\n                    x[j] += delta_x_j\n\n        # Verify invariants\n        total_wealth_inv = np.isclose(np.sum(w0), np.sum(w))\n        \n        # Check non-negativity only if initial wealth is non-negative\n        wealth_nonneg_inv = np.all(w >= 0) if np.all(w0 >= 0) else True\n\n        # Check bounds only if initial opinions are bounded\n        opinion_bounds_inv = (np.all(x >= -1) and np.all(x = 1)) if (np.all(x0 >= -1) and np.all(x0 = 1)) else True\n\n        invariants_hold = all([total_wealth_inv, wealth_nonneg_inv, opinion_bounds_inv])\n        results.append(invariants_hold)\n\n    coverage_ratio = len(hit_branches) / len(total_branches)\n    results.append(round(coverage_ratio, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}