{
    "hands_on_practices": [
        {
            "introduction": "The maximal Lyapunov exponent, $\\lambda$, is the definitive indicator of chaos, measuring the average rate of exponential separation of nearby trajectories. This exercise provides a rare opportunity to move beyond numerical estimation and calculate $\\lambda$ analytically for the logistic map at full chaoticity ($r=4$). By leveraging the Birkhoff Ergodic Theorem to equate a time average with a space average over the system's invariant measure, you will gain a deep, first-principles understanding of how this crucial diagnostic is rooted in the foundations of ergodic theory .",
            "id": "4267640",
            "problem": "Consider the one-dimensional logistic map on the unit interval $[0,1]$ given by $f(x)=r\\,x(1-x)$ at parameter $r=4$. The map $f$ preserves an absolutely continuous invariant probability measure $\\mu$ on $[0,1]$ with density $\\rho(x)=\\frac{1}{\\pi\\sqrt{x(1-x)}}$ with respect to Lebesgue measure. The maximal Lyapunov exponent is defined, for $\\mu$-almost every initial condition $x_{0}\\in[0,1]$, by\n$$\n\\lambda(x_{0})=\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{k=0}^{n-1}\\ln\\big|f'(x_{k})\\big|\\quad\\text{where }x_{k}=f^{k}(x_{0}).\n$$\nStarting from the core definitions of invariant measure and the Birkhoff Ergodic Theorem (BET), and assuming ergodicity of $\\mu$ for $f$ at $r=4$, derive the space-average representation of the maximal Lyapunov exponent as an integral with respect to $\\rho$. Then, using only the given invariant density and fundamental calculus, compute the exact closed-form value of the maximal Lyapunov exponent for this map at $r=4$. Express your final answer as a single exact expression involving the natural logarithm, and do not round. No physical units are required for the final answer.",
            "solution": "The problem requires a two-part response: first, to derive the space-average representation of the maximal Lyapunov exponent using the provided definitions and the Birkhoff Ergodic Theorem (BET); and second, to compute the exact value of this exponent for the logistic map at the specified parameter value.\n\nThe problem statement has been validated and is deemed valid. It is scientifically grounded in the established principles of ergodic theory and nonlinear dynamics, well-posed, and presented objectively. All necessary information is provided.\n\n**Part 1: Derivation of the Space-Average Representation**\n\nThe maximal Lyapunov exponent, $\\lambda(x_0)$, is given as a time average along an orbit starting at $x_0$:\n$$\n\\lambda(x_0) = \\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{k=0}^{n-1}\\ln\\big|f'(x_k)\\big|\n$$\nwhere $x_k = f^k(x_0) = f(f(...f(x_0)...))$ is the $k$-th iterate of the map $f(x)$.\n\nThis expression has the form of a time average of a function $g(x) = \\ln|f'(x)|$ along the trajectory $\\{x_k\\}_{k=0}^\\infty$. The Birkhoff Ergodic Theorem (BET) establishes a fundamental connection between time averages and space averages. The theorem states that for a measure-preserving transformation $f$ on a probability space $(X, \\mathcal{B}, \\mu)$ that is ergodic with respect to the measure $\\mu$, and for any integrable function $g \\in L^1(\\mu)$, the time average of $g$ along an orbit equals the space average of $g$ for $\\mu$-almost every initial point $x_0 \\in X$.\nMathematically, for $\\mu$-almost every $x_0$:\n$$\n\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{k=0}^{n-1}g(f^k(x_0)) = \\int_X g(x) \\,d\\mu(x)\n$$\nIn the context of this problem:\n- The space is the unit interval, $X = [0, 1]$.\n- The transformation is the logistic map, $f(x) = 4x(1-x)$.\n- The system is assumed to be ergodic for the invariant probability measure $\\mu$.\n- The function to be averaged is $g(x) = \\ln|f'(x)|$. We must ensure $g(x)$ is integrable with respect to $\\mu$, which we will confirm during the calculation.\n\nBy applying the BET, we can equate the time-average definition of the Lyapunov exponent to its space-average counterpart. Since the theorem holds for $\\mu$-almost every $x_0$, the Lyapunov exponent $\\lambda$ is independent of the initial condition for this set of points.\n$$\n\\lambda = \\int_0^1 \\ln|f'(x)| \\,d\\mu(x)\n$$\nThe problem states that the invariant measure $\\mu$ is absolutely continuous with respect to the Lebesgue measure $dx$ and provides its probability density function $\\rho(x)$. This means that the differential measure element $d\\mu(x)$ can be written as $\\rho(x)dx$. Substituting this into the integral yields the desired space-average representation:\n$$\n\\lambda = \\int_0^1 \\ln|f'(x)| \\rho(x) \\,dx\n$$\nThis completes the derivation.\n\n**Part 2: Computation of the Lyapunov Exponent**\n\nWe now compute the value of the integral derived above. The given components are:\n- The map: $f(x) = 4x(1-x) = 4x - 4x^2$.\n- The derivative: $f'(x) = 4 - 8x = 4(1-2x)$.\n- The invariant density: $\\rho(x) = \\frac{1}{\\pi\\sqrt{x(1-x)}}$.\n\nSubstituting these into the space-average formula for $\\lambda$:\n$$\n\\lambda = \\int_0^1 \\ln|4(1-2x)| \\frac{1}{\\pi\\sqrt{x(1-x)}} \\,dx\n$$\nWe can separate the logarithm term $\\ln|4(1-2x)| = \\ln(4) + \\ln|1-2x|$. The integral can then be split into two parts:\n$$\n\\lambda = \\frac{1}{\\pi}\\int_0^1 \\frac{\\ln(4)}{\\sqrt{x(1-x)}} \\,dx + \\frac{1}{\\pi}\\int_0^1 \\frac{\\ln|1-2x|}{\\sqrt{x(1-x)}} \\,dx\n$$\nLet's evaluate each integral separately.\n\nThe first integral, $I_1$:\n$$\nI_1 = \\frac{\\ln(4)}{\\pi} \\int_0^1 \\frac{1}{\\sqrt{x(1-x)}} \\,dx\n$$\nThis integral is related to the normalization of the probability density function $\\rho(x)$. By definition, a probability density must integrate to $1$:\n$$\n\\int_0^1 \\rho(x) \\,dx = \\int_0^1 \\frac{1}{\\pi\\sqrt{x(1-x)}} \\,dx = 1\n$$\nThis implies that $\\int_0^1 \\frac{1}{\\sqrt{x(1-x)}} \\,dx = \\pi$. Therefore, the first term is:\n$I_1 = \\frac{\\ln(4)}{\\pi} \\cdot \\pi = \\ln(4)$.\n\nThe second integral, $I_2$:\n$$\nI_2 = \\frac{1}{\\pi}\\int_0^1 \\frac{\\ln|1-2x|}{\\sqrt{x(1-x)}} \\,dx\n$$\nTo evaluate this integral, we employ a trigonometric substitution. Let $x = \\sin^2(\\theta)$.\nThen $dx = 2\\sin(\\theta)\\cos(\\theta) \\,d\\theta$.\nThe limits of integration change as follows: when $x=0$, $\\theta=0$; when $x=1$, $\\theta=\\pi/2$.\nThe terms in the integrand become:\n- $\\sqrt{x(1-x)} = \\sqrt{\\sin^2(\\theta)(1-\\sin^2(\\theta))} = \\sqrt{\\sin^2(\\theta)\\cos^2(\\theta)} = \\sin(\\theta)\\cos(\\theta)$ for $\\theta \\in [0, \\pi/2]$.\n- $1-2x = 1 - 2\\sin^2(\\theta) = \\cos(2\\theta)$.\nSubstituting these into the expression for $\\lambda$ directly:\n$$\n\\lambda = \\frac{1}{\\pi} \\int_0^{\\pi/2} \\frac{\\ln|4\\cos(2\\theta)|}{\\sin(\\theta)\\cos(\\theta)} \\left(2\\sin(\\theta)\\cos(\\theta)\\right) \\,d\\theta = \\frac{2}{\\pi} \\int_0^{\\pi/2} \\ln|4\\cos(2\\theta)| \\,d\\theta\n$$\nWe can split the logarithm again:\n$$\n\\lambda = \\frac{2}{\\pi} \\int_0^{\\pi/2} \\left(\\ln(4) + \\ln|\\cos(2\\theta)|\\right) \\,d\\theta = \\frac{2}{\\pi} \\left( \\int_0^{\\pi/2} \\ln(4) \\,d\\theta + \\int_0^{\\pi/2} \\ln|\\cos(2\\theta)| \\,d\\theta \\right)\n$$\nThe first part is $\\frac{2}{\\pi} \\left( \\ln(4) \\cdot \\frac{\\pi}{2} \\right) = \\ln(4)$. This matches $I_1$.\nThe second part is exactly $I_2$ after the change of variables:\n$$\nI_2 = \\frac{2}{\\pi}\\int_0^{\\pi/2} \\ln|\\cos(2\\theta)| \\,d\\theta\n$$\nThe argument of the logarithm, $\\cos(2\\theta)$, is positive for $\\theta \\in [0, \\pi/4)$ and negative for $\\theta \\in (\\pi/4, \\pi/2]$. We split the integral at $\\theta = \\pi/4$:\n$$\nI_2 = \\frac{2}{\\pi} \\left[ \\int_0^{\\pi/4} \\ln(\\cos(2\\theta)) \\,d\\theta + \\int_{\\pi/4}^{\\pi/2} \\ln(-\\cos(2\\theta)) \\,d\\theta \\right]\n$$\nFor the first sub-integral, let $u=2\\theta$, so $d\\theta = du/2$. The limits become $0$ to $\\pi/2$.\n$\\int_0^{\\pi/4} \\ln(\\cos(2\\theta)) \\,d\\theta = \\frac{1}{2}\\int_0^{\\pi/2} \\ln(\\cos(u)) \\,du$.\nFor the second sub-integral, let $v=\\pi - 2\\theta$, so $d\\theta = -dv/2$. The limits become $\\pi/2$ to $0$. $-\\cos(2\\theta) = -\\cos(\\pi-v) = -(-\\cos(v)) = \\cos(v)$.\n$\\int_{\\pi/4}^{\\pi/2} \\ln(-\\cos(2\\theta)) \\,d\\theta = \\int_{\\pi/2}^0 \\ln(\\cos(v)) \\left(-\\frac{dv}{2}\\right) = \\frac{1}{2}\\int_0^{\\pi/2} \\ln(\\cos(v)) \\,dv$.\nThus, the two sub-integrals are identical.\n$$\nI_2 = \\frac{2}{\\pi} \\left[ \\frac{1}{2}\\int_0^{\\pi/2} \\ln(\\cos(u)) \\,du + \\frac{1}{2}\\int_0^{\\pi/2} \\ln(\\cos(v)) \\,dv \\right] = \\frac{2}{\\pi} \\int_0^{\\pi/2} \\ln(\\cos(u)) \\,du\n$$\nThis is a standard definite integral result. Let $J = \\int_0^{\\pi/2} \\ln(\\cos u) \\,du$. Using the property $\\int_a^b f(x)dx = \\int_a^b f(a+b-x)dx$, we also have $J = \\int_0^{\\pi/2} \\ln(\\sin u) \\,du$.\nAdding these two forms gives:\n$$\n2J = \\int_0^{\\pi/2} (\\ln(\\sin u) + \\ln(\\cos u)) \\,du = \\int_0^{\\pi/2} \\ln(\\sin u \\cos u) \\,du = \\int_0^{\\pi/2} \\ln\\left(\\frac{\\sin(2u)}{2}\\right) \\,du\n$$\n$$\n2J = \\int_0^{\\pi/2} \\ln(\\sin(2u)) \\,du - \\int_0^{\\pi/2} \\ln(2) \\,du\n$$\nThe second term is $\\frac{\\pi}{2}\\ln(2)$. For the first term, let $w=2u$, so $du = dw/2$. Limits are $0$ to $\\pi$.\n$\\int_0^{\\pi/2} \\ln(\\sin(2u)) \\,du = \\frac{1}{2}\\int_0^{\\pi} \\ln(\\sin w) \\,dw$. By symmetry of $\\sin(w)$ about $w=\\pi/2$, this is $\\frac{1}{2} \\left( 2 \\int_0^{\\pi/2} \\ln(\\sin w) \\,dw \\right) = \\int_0^{\\pi/2} \\ln(\\sin w) \\,dw = J$.\nSubstituting back:\n$2J = J - \\frac{\\pi}{2}\\ln(2) \\implies J = -\\frac{\\pi}{2}\\ln(2)$.\nFinally, we can compute $I_2$:\n$I_2 = \\frac{2}{\\pi} J = \\frac{2}{\\pi} \\left(-\\frac{\\pi}{2}\\ln(2)\\right) = -\\ln(2)$.\nCombining the results for $I_1$ and $I_2$:\n$\\lambda = I_1 + I_2 = \\ln(4) - \\ln(2) = \\ln(2^2) - \\ln(2) = 2\\ln(2) - \\ln(2) = \\ln(2)$.\nThe maximal Lyapunov exponent for the logistic map at $r=4$ is $\\ln(2)$.",
            "answer": "$$\\boxed{\\ln(2)}$$"
        },
        {
            "introduction": "While analytical solutions are invaluable, most investigations in chaos theory rely on numerical simulation, with the bifurcation diagram being a primary tool. However, every simulation is an approximation, and our methodological choices can shape—or distort—our results. This computational practice challenges you to investigate how fundamental parameters, such as the number of discarded transient iterations and the density of sampling, influence the empirical measure of an attractor and the estimated Lyapunov exponent, building crucial skills for critically assessing numerical evidence in nonlinear dynamics .",
            "id": "4267582",
            "problem": "You are tasked with analyzing how finite-time numerical procedures used to construct a bifurcation diagram of the one-dimensional logistic map affect the empirical representation of its long-term behavior. The logistic map is the discrete-time dynamical system defined by the function $f_r:[0,1]\\to[0,1]$ with the update rule $x_{n+1} = r\\,x_n(1-x_n)$, where $r \\in [0,4]$ is a real parameter and $x_n \\in [0,1]$ is the state at discrete time $n \\in \\mathbb{N}$. For each fixed parameter $r$, the orbit $(x_n)_{n\\ge 0}$ is determined by an initial condition $x_0 \\in (0,1)$. The bifurcation diagram visualizes the asymptotic set of orbit values as $r$ varies, and its empirical construction relies on two numerical design choices: discarding a finite transient and sampling a finite subsequence of the orbit.\n\nFundamental definitions and well-tested facts to use as the base of your reasoning:\n- A map $f_r$ admits an invariant probability measure $\\mu_r$ on $[0,1]$ whenever the long-time statistics of almost every orbit converge to the same distribution; for chaotic attractors this is typically the Sinai–Ruelle–Bowen (SRB) measure. For piecewise smooth one-dimensional maps like $f_r$, the empirical distribution of a long orbit is commonly used as an approximation to $\\mu_r$.\n- The largest Lyapunov exponent is defined (when the limit exists) by $\\lambda(r) = \\lim_{N\\to\\infty} \\frac{1}{N} \\sum_{n=0}^{N-1} \\log |f_r'(x_n)|$, where $f_r'(x) = r(1-2x)$. For ergodic measures, time averages along typical orbits equal space averages with respect to $\\mu_r$ by the Birkhoff ergodic theorem.\n\nYour program must do the following for each test case parameter $r$:\n1. Generate a single orbit of length $N = 10^5$ iterates of the logistic map $x_{n+1} = r\\,x_n(1-x_n)$, starting from the same fixed initial condition $x_0 = 0.123456789$.\n2. Consider two transient discard lengths, $T_{\\text{short}}$ and $T_{\\text{long}}$, with $0  T_{\\text{short}}  T_{\\text{long}}  N$, and two sampling schemes: dense sampling with stride $s_{\\text{dense}} = 1$ and thin sampling with stride $s_{\\text{thin}} \\in \\mathbb{N}$, $s_{\\text{thin}} \\ge 2$.\n3. To ensure a fair comparison across schemes under the fixed budget of $N$ total iterations, define the common sample size $M_0$ by\n$$\nM_0 = \\min\\left\\{N - T_{\\text{short}},\\; N - T_{\\text{long}},\\; \\left\\lfloor \\frac{N - T_{\\text{long}}}{s_{\\text{thin}}} \\right\\rfloor \\right\\}.\n$$\nThen extract exactly $M_0$ samples using each of the following three configurations:\n- Short-transient dense: discard $T_{\\text{short}}$ then take $M_0$ consecutive points with stride $s_{\\text{dense}} = 1$.\n- Long-transient dense: discard $T_{\\text{long}}$ then take $M_0$ consecutive points with stride $s_{\\text{dense}} = 1$.\n- Long-transient thin: discard $T_{\\text{long}}$ then take $M_0$ points with stride $s_{\\text{thin}}$.\n4. For each sample set, compute a normalized histogram over $B = 200$ equal-width bins on the interval $[0,1]$ to approximate the empirical invariant measure $\\mu_r$ induced by that sampling scheme. Let the resulting probability vectors be $p_{\\text{short,dense}}$, $p_{\\text{long,dense}}$, and $p_{\\text{long,thin}}$.\n5. Quantify how the empirical visual measure of the attractor is affected by transient discarding and sampling by computing the following two distances:\n- Transient sensitivity as the total variation distance between dense histograms with short and long transients:\n$$\nD_{\\text{transient}} = \\frac{1}{2} \\sum_{i=1}^{B} \\left| \\left(p_{\\text{short,dense}}\\right)_i - \\left(p_{\\text{long,dense}}\\right)_i \\right|.\n$$\n- Sampling sensitivity as the total variation distance between long-transient dense and long-transient thin histograms:\n$$\nD_{\\text{sampling}} = \\frac{1}{2} \\sum_{i=1}^{B} \\left| \\left(p_{\\text{long,dense}}\\right)_i - \\left(p_{\\text{long,thin}}\\right)_i \\right|.\n$$\n6. For each sample set, estimate the largest Lyapunov exponent by the finite-time average\n$$\n\\hat{\\lambda} = \\frac{1}{M_0} \\sum_{m=1}^{M_0} \\log \\left| r \\left( 1 - 2 x_m \\right) \\right|,\n$$\nwhere $x_m$ ranges over the sampled subsequence. Using these, compute:\n- $\\Delta_{\\text{transient}} = \\left| \\hat{\\lambda}_{\\text{short,dense}} - \\hat{\\lambda}_{\\text{long,dense}} \\right|$,\n- $\\Delta_{\\text{sampling}} = \\left| \\hat{\\lambda}_{\\text{long,dense}} - \\hat{\\lambda}_{\\text{long,thin}} \\right|$.\n\nTest suite and parameter coverage:\n- Case $1$ (period-$2$ regime and aliasing edge case): $r = 3.0$, $T_{\\text{short}} = 100$, $T_{\\text{long}} = 5000$, $s_{\\text{thin}} = 2$.\n- Case $2$ (periodic window beyond period-doubling): $r = 3.5$, $T_{\\text{short}} = 100$, $T_{\\text{long}} = 5000$, $s_{\\text{thin}} = 2$.\n- Case $3$ (fully chaotic regime): $r = 3.7$, $T_{\\text{short}} = 100$, $T_{\\text{long}} = 5000$, $s_{\\text{thin}} = 5$.\n- Case $4$ (strongly chaotic regime, near the upper end): $r = 3.9$, $T_{\\text{short}} = 100$, $T_{\\text{long}} = 5000$, $s_{\\text{thin}} = 7$.\n\nRequired output:\n- For each case, output a list with four floats $[D_{\\text{transient}}, D_{\\text{sampling}}, \\Delta_{\\text{transient}}, \\Delta_{\\text{sampling}}]$, where each float is rounded to exactly six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of these lists, enclosed in square brackets. For example, a valid shape is $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],\\ldots]$ with no spaces.\n\nNotes on scientific realism:\n- Keep all iterations within $N = 10^5$ total steps per parameter, as specified.\n- Use $x_0 = 0.123456789$ and ensure all histogram bins cover $[0,1]$ uniformly.\n- When computing $\\log |f_r'(x)|$, if $|f_r'(x)|$ underflows numerically, use a standard small positive threshold to avoid taking the logarithm of zero, and state this choice explicitly in your solution reasoning.",
            "solution": "The user-provided problem is assessed as valid. It is scientifically sound, well-posed, and all variables and conditions are specified unambiguously. The problem constitutes a standard numerical investigation in the field of nonlinear dynamics and chaos theory, focusing on computational artifacts in the analysis of the logistic map.\n\nThe solution proceeds by implementing the specified numerical experiment. For each parameter set, the analysis involves orbit generation, subsampling according to three different schemes, and subsequent calculation of statistical and dynamical quantities to quantify the differences induced by these schemes.\n\n### Principle-Based Design\n\nThe fundamental principle being investigated is the correspondence between the long-term behavior of a dynamical system and its finite-time numerical approximation. For an ergodic system, the Birkhoff ergodic theorem guarantees that for almost all initial conditions, the time average of an observable along an orbit converges to the space average with respect to the system's natural invariant measure, $\\mu_r$. Numerically simulating a bifurcation diagram involves two key approximations:\n1.  **Discarding Transients**: Orbits are iterated for a finite time $T$ to allow them to settle onto the attractor, discarding the initial $T$ points.\n2.  **Sampling the Attractor**: The state of the system is recorded for a finite number of subsequent iterations to build a picture of the attractor's geometry and the invariant measure supported on it.\n\nThis problem formalizes the investigation of how these choices affect two primary characterizations of the dynamics: the invariant measure itself, approximated by a histogram, and the largest Lyapunov exponent, $\\hat{\\lambda}$, which measures the system's sensitivity to initial conditions.\n\n### Algorithmic Implementation\n\nFor each test case, defined by a parameter set $(r, T_{\\text{short}}, T_{\\text{long}}, s_{\\text{thin}})$, the following sequence of steps is executed.\n\n1.  **Orbit Generation**: A single time series $(x_n)_{n=0}^{N-1}$ of length $N = 10^5$ is generated using the logistic map recurrence relation, $x_{n+1} = r\\,x_n(1-x_n)$. The initial condition is fixed at $x_0 = 0.123456789$. This long orbit serves as the base data for all subsequent analyses.\n\n2.  **Common Sample Size Calculation**: To ensure a fair comparison, all statistical estimates are based on the same number of data points, $M_0$. This size is determined by the most restrictive sampling scheme:\n    $$\n    M_0 = \\min\\left\\{N - T_{\\text{short}},\\; N - T_{\\text{long}},\\; \\left\\lfloor \\frac{N - T_{\\text{long}}}{s_{\\text{thin}}} \\right\\rfloor \\right\\}\n    $$\n    This ensures that any observed differences are due to the sampling strategy itself (i.e., which points are chosen) and not due to the statistical uncertainty associated with different sample sizes.\n\n3.  **Data Extraction**: Three distinct samples, each of size $M_0$, are extracted from the master orbit:\n    *   **Short-transient, dense sample**: The subsequence starts after $T_{\\text{short}}$ iterations and consists of $M_0$ consecutive points. This corresponds to `orbit[T_short : T_short + M_0]`.\n    *   **Long-transient, dense sample**: The subsequence starts after $T_{\\text{long}}$ iterations and consists of $M_0$ consecutive points. This corresponds to `orbit[T_long : T_long + M_0]`.\n    *   **Long-transient, thin sample**: The subsequence starts after $T_{\\text{long}}$ iterations but takes points with a stride of $s_{\\text{thin}}$. This corresponds to `orbit[T_long : T_long + M_0 * s_thin : s_thin]`.\n\n4.  **Empirical Invariant Measure Estimation**: For each of the three samples, we approximate the invariant measure $\\mu_r$. This is done by constructing a normalized histogram over the interval $[0,1]$ using $B = 200$ equal-width bins. If $c_i$ is the count of sample points in bin $i$, the probability for that bin is $p_i = c_i / M_0$. This process yields three probability vectors: $p_{\\text{short,dense}}$, $p_{\\text{long,dense}}$, and $p_{\\text{long,thin}}$.\n\n5.  **Quantifying Measure Discrepancy**: The difference between the empirical measures is quantified using the total variation distance, a standard metric for probability distributions. The two specific quantities computed are:\n    *   Transient Sensitivity: $D_{\\text{transient}} = \\frac{1}{2} \\sum_{i=1}^{B} | (p_{\\text{short,dense}})_i - (p_{\\text{long,dense}})_i |$. This measures the effect of extending the transient discard period.\n    *   Sampling Sensitivity: $D_{\\text{sampling}} = \\frac{1}{2} \\sum_{i=1}^{B} | (p_{\\text{long,dense}})_i - (p_{\\text{long,thin}})_i |$. This measures the effect of sparse sampling versus dense sampling, after a long transient.\n\n6.  **Finite-Time Lyapunov Exponent Estimation**: The largest Lyapunov exponent is a key indicator of chaos. For a one-dimensional map, it is estimated by averaging the logarithm of the absolute value of the map's derivative along the orbit. For each sample, the estimate $\\hat{\\lambda}$ is computed as:\n    $$\n    \\hat{\\lambda} = \\frac{1}{M_0} \\sum_{m=1}^{M_0} \\log \\left| r \\left( 1 - 2 x_m \\right) \\right|\n    $$\n    where $\\{x_m\\}_{m=1}^{M_0}$ are the points in the sample. A critical implementation detail is handling the case where the argument of the logarithm is zero, which occurs if an orbit point $x_m$ hits the critical point $x=0.5$. To prevent a fatal mathematical error ($\\log(0)$) and to maintain numerical stability, a small positive constant, `numpy.finfo(float).tiny`, is added to the absolute value $|f_r'(x_m)|$ before taking the logarithm.\n\n7.  **Quantifying Lyapunov Exponent Discrepancy**: The sensitivity of the Lyapunov exponent estimate to the sampling scheme is measured by the absolute differences:\n    *   Transient Sensitivity: $\\Delta_{\\text{transient}} = | \\hat{\\lambda}_{\\text{short,dense}} - \\hat{\\lambda}_{\\text{long,dense}} |$.\n    *   Sampling Sensitivity: $\\Delta_{\\text{sampling}} = | \\hat{\\lambda}_{\\text{long,dense}} - \\hat{\\lambda}_{\\text{long,thin}} |$.\n\nThese calculations are performed for each of the four specified test cases, and the results are compiled and formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries outside the Python standard library are permitted.\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (r, T_short, T_long, s_thin)\n        (3.0, 100, 5000, 2),\n        (3.5, 100, 5000, 2),\n        (3.7, 100, 5000, 5),\n        (3.9, 100, 5000, 7),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = analyze_logistic_map(*params)\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # Format: [[a,b,c,d],[e,f,g,h],...]\n    output_parts = []\n    for res_list in all_results:\n        formatted_list = ','.join([f'{x:.6f}' for x in res_list])\n        output_parts.append(f'[{formatted_list}]')\n    \n    print(f\"[{','.join(output_parts)}]\")\n\ndef analyze_logistic_map(r, T_short, T_long, s_thin):\n    \"\"\"\n    Performs the analysis for a single set of parameters for the logistic map.\n    \n    Args:\n        r (float): The parameter of the logistic map.\n        T_short (int): Short transient discard length.\n        T_long (int): Long transient discard length.\n        s_thin (int): Stride for thin sampling.\n\n    Returns:\n        list: A list containing [D_transient, D_sampling, Delta_transient, Delta_sampling].\n    \"\"\"\n    N = 100000\n    x0 = 0.123456789\n    B = 200\n\n    # 1. Generate the orbit\n    orbit = np.zeros(N)\n    orbit[0] = x0\n    for n in range(N - 1):\n        orbit[n + 1] = r * orbit[n] * (1 - orbit[n])\n        \n    # 2. Define the common sample size M0\n    M0 = min(N - T_short, N - T_long, (N - T_long) // s_thin)\n\n    # 3. Extract the three sample sets\n    sample_short_dense = orbit[T_short : T_short + M0]\n    sample_long_dense = orbit[T_long : T_long + M0]\n    # The stop index for thin sampling ensures exactly M0 points are taken\n    stop_index_thin = T_long + M0 * s_thin\n    sample_long_thin = orbit[T_long : stop_index_thin : s_thin]\n\n    # 4. Compute normalized histograms\n    hist_short_dense, _ = np.histogram(sample_short_dense, bins=B, range=(0, 1))\n    hist_long_dense, _ = np.histogram(sample_long_dense, bins=B, range=(0, 1))\n    hist_long_thin, _ = np.histogram(sample_long_thin, bins=B, range=(0, 1))\n\n    # Normalize to get probability vectors\n    p_short_dense = hist_short_dense / M0\n    p_long_dense = hist_long_dense / M0\n    p_long_thin = hist_long_thin / M0\n    \n    # 5. Compute total variation distances\n    D_transient = 0.5 * np.sum(np.abs(p_short_dense - p_long_dense))\n    D_sampling = 0.5 * np.sum(np.abs(p_long_dense - p_long_thin))\n\n    # 6. Estimate largest Lyapunov exponents\n    # Define a helper function for clarity\n    def estimate_lyapunov(sample, r_param):\n        # Add a small epsilon to avoid log(0)\n        # np.finfo(float).tiny is the smallest positive representable number.\n        log_derivs = np.log(np.abs(r_param * (1 - 2 * sample)) + np.finfo(float).tiny)\n        return np.mean(log_derivs)\n\n    lambda_short_dense = estimate_lyapunov(sample_short_dense, r)\n    lambda_long_dense = estimate_lyapunov(sample_long_dense, r)\n    lambda_long_thin = estimate_lyapunov(sample_long_thin, r)\n    \n    # Compute differences in Lyapunov estimates\n    Delta_transient = np.abs(lambda_short_dense - lambda_long_dense)\n    Delta_sampling = np.abs(lambda_long_dense - lambda_long_thin)\n    \n    return [D_transient, D_sampling, Delta_transient, Delta_sampling]\n\nsolve()\n```"
        },
        {
            "introduction": "One of the most profound discoveries in chaos theory is universality: seemingly different systems can exhibit identical quantitative behavior during their transition to chaos. The Feigenbaum constant, $\\delta$, epitomizes this, describing a universal scaling ratio for the period-doubling cascade. In this exercise, you will implement a robust numerical algorithm to find the superstable parameter values of the logistic map and compute an estimate of $\\delta$, thereby engaging directly with a fundamental constant of nature and the elegant regularities that govern the onset of chaos .",
            "id": "4267656",
            "problem": "Consider the one-dimensional iterative dynamical system known as the logistic map defined by $f_{r}(x) = r x (1 - x)$, where the parameter $r$ is real and $x \\in [0,1]$. The logistic map exhibits a period-doubling cascade as $r$ increases, accumulating at a finite parameter value where chaos emerges. The Feigenbaum constant $\\delta$ characterizes the geometric convergence of the lengths of successive parameter intervals associated with this period-doubling process. The constant $\\delta$ is defined by the limiting ratio of successive differences $\\frac{r_{n} - r_{n-1}}{s_{n+1} - s_{n}}$ as $n \\to \\infty$, where $r_{n}$ denotes the parameter value at which the system bifurcates from a stable orbit of period $2^{n-1}$ to a stable orbit of period $2^{n}$. In practice, one can estimate these $r_{n}$ values by identifying the superstable parameter values $s_{n}$ for which the critical point $x_{c} = \\frac{1}{2}$ belongs to the attracting cycle of period $2^{n}$. Equivalently, $s_{n}$ is a root of the equation $f_{s_{n}}^{(2^{n})}\\!\\left(\\frac{1}{2}\\right) - \\frac{1}{2} = 0$, where $f_{r}^{(k)}$ denotes $k$-fold composition.\n\nStarting from the fundamental base consisting of definitions of iterative maps, continuity of $f_{r}(x)$ in both $r$ and $x$, and the Intermediate Value Theorem for continuous functions, you must:\n\n- Derive and implement a robust numerical method to locate the superstable parameters $s_{n}$ for the logistic map by solving $F_{n}(r) = f_{r}^{(2^{n})}\\!\\left(\\frac{1}{2}\\right) - \\frac{1}{2} = 0$ with $n \\in \\mathbb{N}$. Begin with $s_{0} = 2$, corresponding to a fixed point, and for each successive $n$, search for the smallest $r  s_{n-1}$ that solves $F_{n}(r) = 0$.\n- Use a bracketing strategy to guarantee existence of a root interval for $F_{n}(r)$ and then apply bisection on that interval to compute $s_{n}$ to high precision. You must ensure all steps maintain numerical stability.\n- Compute the sequence of Feigenbaum ratio estimates $\\delta_{n} = \\frac{s_{n} - s_{n-1}}{s_{n+1} - s_{n}}$ for specified $n$ values and analyze their convergence behavior.\n\nYour program must produce outputs for the following test suite, which is designed to assess correctness, stability, and convergence:\n\n- Compute the Feigenbaum ratio estimates $\\delta_{n}$ for $n \\in \\{1,2,3,4,5,6,7\\}$ using the superstable sequence $\\{s_{n}\\}$ obtained by your method.\n- Additionally compute two boolean diagnostics:\n  1. $M_{s}$: whether the sequence $\\{s_{n}\\}$ is strictly increasing for the computed range, that is, $s_{n+1}  s_{n}$ for all consecutive pairs considered.\n  2. $D_{\\delta}$: whether the absolute differences $|\\delta_{n+1} - \\delta_{n}|$ are strictly decreasing for the last three consecutive indices available in your computed set, which probes empirical convergence behavior.\n\nAll quantities are purely numerical and unitless. Angles do not appear. The final output must be presented as a single line containing a comma-separated list enclosed in square brackets, structured as:\n$[\\delta_{1},\\delta_{2},\\delta_{3},\\delta_{4},\\delta_{5},\\delta_{6},\\delta_{7},M_{s},D_{\\delta}]$,\nwhere the first seven entries are floating-point numbers and the last two entries are booleans. Your program must compute these values without any external inputs or files and must be runnable as-is in a standard environment.\n\nYour algorithm must be derived from the above fundamental base and must not use any shortcut formulas or pre-tabulated bifurcation points. It must produce all values algorithmically from the definitions and fundamental continuity properties given.",
            "solution": "The problem requires the numerical computation of the Feigenbaum constant, $\\delta$, which characterizes the universal scaling behavior in the period-doubling route to chaos. We are tasked to achieve this for the specific case of the one-dimensional logistic map, defined by the iterative function $f_{r}(x) = r x (1 - x)$, where the state variable $x$ is confined to the interval $[0, 1]$ and $r$ is a real parameter. The solution must be derived from fundamental principles, namely the definition of an iterative map, the continuity of its iterates, and the Intermediate Value Theorem.\n\nFirst, we establish the theoretical and algorithmic foundation. The $k$-th iterate of the map is defined by the composition $f_{r}^{(k)}(x) = f_r(f_r(\\dots f_r(x)\\dots))$, where the function $f_r$ is applied $k$ times. For any fixed integer $k$ and initial point $x$, the function $g(r) = f_{r}^{(k)}(x)$ is a polynomial in $r$. A direct consequence of this is that $g(r)$ is a continuous function of $r$ for all $r \\in \\mathbb{R}$. This continuity is the central pillar upon which our numerical method rests, as it permits the application of the Intermediate Value Theorem.\n\nThe problem directs us to use superstable cycles to approximate the period-doubling bifurcation points. An attractor is a superstable cycle of period $p$ if the critical point of the map, $x_c$, is an element of the cycle. For the logistic map $f_r(x)$, the critical point is found by solving $f_r'(x_c) = r(1-2x_c) = 0$, which yields $x_c = 1/2$. A superstable cycle of period $2^n$ is one for which the orbit of $x_c=1/2$ has a period of $2^n$. The parameter value $s_n$ that gives rise to such a cycle is found by solving the equation $f_{s_n}^{(2^n)}(1/2) = 1/2$. This leads us to define a sequence of functions $F_n(r) = f_{r}^{(2^n)}(1/2) - 1/2$. The superstable parameters $s_n$ are the roots of the equation $F_n(r)=0$. Since $f_r^{(2^n)}(1/2)$ is continuous in $r$, so too is $F_n(r)$.\n\nOur objective is to find the sequence of roots $\\{s_n\\}$ for $n=0, 1, 2, \\dots$. The problem specifies that for each $n \\geq 1$, we must find the smallest root of $F_n(r)=0$ that is strictly greater than $s_{n-1}$. The base case is provided as $s_0=2$, which corresponds to the superstable fixed point ($n=0$, period $2^0=1$). We can verify this: $F_0(r) = f_r(1/2) - 1/2 = r(1/2)(1-1/2) - 1/2 = r/4 - 1/2$. Setting $F_0(r)=0$ gives $r/4=1/2$, so $s_0=2$.\n\nTo find each subsequent root $s_n$, we employ a two-stage numerical algorithm grounded in the continuity of $F_n(r)$:\n1.  **Root Bracketing**: The Intermediate Value Theorem states that for a continuous function $F_n(r)$ on an interval $[a, b]$, if $F_n(a)$ and $F_n(b)$ have opposite signs, then at least one root must exist in $(a, b)$. Our algorithm must first find such a bracket $[a, b]$ for the desired root $s_n$. We initiate a search starting from a point slightly greater than the previously found root, $s_{n-1}$, and step forward in $r$ with a sufficiently small step size, evaluating the sign of $F_n(r)$ at each point. The first interval over which a sign change is detected becomes our bracket. This procedure reliably isolates the \"smallest root greater than $s_{n-1}$\".\n\n2.  **Root Refinement via Bisection**: Once a root is bracketed in an interval $[a, b]$, the bisection method provides a robust and guaranteed-convergent algorithm for refining its value. The method proceeds as follows:\n    a. Compute the midpoint $c = (a+b)/2$ and evaluate $F_n(c)$.\n    b. If $F_n(a)$ and $F_n(c)$ have opposite signs, the root lies in $[a, c]$, so we set $b=c$.\n    c. Otherwise, the root must lie in $[c, b]$, and we set $a=c$.\n    d. This process is repeated, halving the interval width at each step, until the width $(b-a)$ is smaller than a predetermined numerical tolerance, $\\epsilon=10^{-15}$. The midpoint of the final interval is taken as the high-precision estimate of $s_n$. The computational cost is dominated by the evaluation of $F_n(r)$, which requires $2^n$ iterations of the logistic map.\n\nThis procedure is applied sequentially to compute the sequence $s_0, s_1, s_2, \\dots, s_8$. A high tolerance (e.g., $\\epsilon=10^{-15}$) is necessary because the subsequent calculation of the Feigenbaum ratio is sensitive to the precision of the $s_n$ values.\n\nWith the sequence $\\{s_n\\}_{n=0}^8$ computed, we can find the estimates for the Feigenbaum constant, $\\delta_n$. These are given by the ratio of the lengths of successive parameter intervals between superstable points:\n$$ \\delta_n = \\frac{s_n - s_{n-1}}{s_{n+1} - s_n} $$\nWe will compute these values for $n \\in \\{1, 2, 3, 4, 5, 6, 7\\}$.\n\nFinally, two diagnostic booleans are computed:\n-   $M_s$: This checks for the strict monotonic increase of the computed sequence, i.e., $s_{k+1}  s_k$ for all relevant $k$. This is an expected theoretical property.\n-   $D_\\delta$: This probes the convergence of the $\\delta_n$ sequence by checking if the absolute differences $|\\delta_{n+1} - \\delta_n|$ are strictly decreasing for the last three available consecutive indices of our computed set. This corresponds to testing if $|\\delta_5 - \\delta_4|  |\\delta_6 - \\delta_5|  |\\delta_7 - \\delta_6|$.\n\nThis robust, first-principles-based algorithm allows for a precise determination of the required quantities without resorting to pre-tabulated values or advanced library functions, adhering strictly to the problem's constraints.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is available but not used to adhere to the problem's\n# requirement of deriving and implementing the numerical method.\n\ndef solve():\n    \"\"\"\n    Computes Feigenbaum constant estimates for the logistic map by finding\n    superstable parameter values using a custom bisection root-finder.\n    \"\"\"\n    # Configuration\n    N_MAX = 8  # We need s_0 through s_8 to compute delta_1 through delta_7\n    BISECTION_TOL = 1e-15\n\n    # A dictionary of search step sizes for the bracketing algorithm.\n    # The step size must decrease as n increases because the distance\n    # between successive s_n values shrinks geometrically.\n    SEARCH_STEPS = {\n        1: 1e-1, 2: 1e-2, 3: 5e-3, 4: 1e-3,\n        5: 1e-4, 6: 1e-5, 7: 1e-6, 8: 1e-7\n    }\n\n    def iterate_map(r, x0, k):\n        \"\"\"Computes the k-th iterate of the logistic map f_r(x) starting from x0.\"\"\"\n        x = x0\n        for _ in range(k):\n            # Using np.float64 for operations to maintain precision\n            x = np.float64(r) * x * (np.float64(1.0) - x)\n        return x\n\n    def F_n(r, n):\n        \"\"\"The target function whose root is the superstable parameter s_n.\n        F_n(r) = f_r^(2^n)(1/2) - 1/2\n        \"\"\"\n        k = 1  n  # 2**n using bitwise shift\n        x_final = iterate_map(r, np.float64(0.5), k)\n        return x_final - np.float64(0.5)\n\n    def bisection(func, a, b, tol):\n        \"\"\"\n        Finds a root of func in the bracket [a, b] to a given tolerance.\n        Assumes func(a) and func(b) have opposite signs.\n        \"\"\"\n        fa = func(a)\n        if fa * func(b)  0:\n            raise ValueError(f\"Root is not bracketed in [{a}, {b}].\")\n        \n        while (b - a)  tol:\n            c = a + (b - a) / 2\n            fc = func(c)\n            if fc == 0:\n                return c\n            if fa * fc  0:\n                b = c\n            else:\n                a = c\n                fa = fc\n        return a + (b - a) / 2\n\n    s = [np.float64(2.0)]  # Start with s_0 = 2\n\n    # Sequentially compute s_1, s_2, ..., s_N_MAX\n    for n in range(1, N_MAX + 1):\n        target_func = lambda r: F_n(r, n)\n        \n        # --- Root Bracketing ---\n        # Start search for s_n just after s_{n-1}\n        # Add a small epsilon to avoid starting exactly at the previous root\n        r_a = s[n-1] + 1e-9\n        \n        search_step = SEARCH_STEPS.get(n, 1e-8)\n        val_a = target_func(r_a)\n        \n        while True:\n            r_b = r_a + search_step\n            val_b = target_func(r_b)\n            if val_a * val_b  0:\n                # Bracket found\n                break\n            r_a = r_b\n            val_a = val_b\n            if r_a  4.0: # Safety break; chaos is confined to r = 4\n                raise RuntimeError(f\"Bracket search failed for s_{n}\")\n\n        # --- Root Refinement ---\n        s_n = bisection(target_func, r_a, r_b, BISECTION_TOL)\n        s.append(s_n)\n\n    # Compute Feigenbaum ratio estimates delta_n\n    deltas = []\n    for n in range(1, 8):  # n from 1 to 7\n        numerator = s[n] - s[n-1]\n        denominator = s[n+1] - s[n]\n        delta_n = numerator / denominator\n        deltas.append(delta_n)\n    \n    # Compute diagnostic booleans\n    # M_s: Check if the s_n sequence is strictly increasing\n    M_s = all(s[i]  s[i+1] for i in range(len(s) - 1))\n    \n    # D_delta: Check for convergence behavior in the delta_n sequence.\n    # We check if the absolute differences |delta_{n+1} - delta_n|\n    # are strictly decreasing for the last three available indices.\n    delta_diffs_abs = [abs(deltas[i+1] - deltas[i]) for i in range(len(deltas) - 1)]\n    # This checks |d5-d4|  |d6-d5| AND |d6-d5|  |d7-d6|\n    D_delta = (delta_diffs_abs[3]  delta_diffs_abs[4]) and \\\n              (delta_diffs_abs[4]  delta_diffs_abs[5])\n              \n    # Format and print the final result\n    final_results = deltas + [M_s, D_delta]\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}