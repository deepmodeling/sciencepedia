{
    "hands_on_practices": [
        {
            "introduction": "Before delving into the full PageRank algorithm, it's essential to grasp the underlying principle of a stationary distribution for a random walk. This exercise  simplifies the model by setting the teleportation parameter $\\alpha=1$, effectively modeling a random surfer who only follows network links. By deriving the stationary distribution for a simple cyclic network, you will practice finding the unique equilibrium state where the probability of being at any given node becomes constant over time.",
            "id": "4296056",
            "problem": "Consider a directed network with $3$ nodes labeled $1$, $2$, and $3$, whose adjacency matrix is\n$$\nA \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n1  0  0\n\\end{pmatrix}.\n$$\nA random surfer moves along outgoing links with equal probability from its current node, producing a finite-state first-order Markov chain (MC). Let $P$ denote the corresponding column-stochastic transition matrix derived from $A$ by normalizing each column by the out-degree of its source node. In the PageRank (PR) random surfer model without teleportation (i.e., teleportation parameter $\\alpha = 1$), the stationary distribution $r$ is defined as the nonnegative vector that is invariant under the dynamics of the MC, with entries summing to $1$.\n\nStarting only from core definitions of a column-stochastic transition matrix on a directed graph, the definition of a stationary distribution for a finite MC, and conservation of probability, derive the stationary distribution $r$ for this network. Express your final answer as a single row vector using the LaTeX `pmatrix` environment. No rounding is required.",
            "solution": "The problem requires the derivation of the stationary distribution $r$ for the given network. This involves two main steps: first, constructing the column-stochastic transition matrix $P$ from the adjacency matrix $A$, and second, solving for the vector $r$ that satisfies the properties of a stationary distribution.\n\n**Step 1: Constructing the Transition Matrix $P$**\n\nThe problem states that the transition matrix $P$ is derived from the adjacency matrix $A$ by normalizing each column by the out-degree of its source node. This implies that the columns of $A$ correspond to source nodes, and an entry $A_{ij}$ being $1$ represents a directed link from node $j$ to node $i$.\n\nThe out-degree of a node $j$, denoted $k_j^{\\text{out}}$, is the number of outgoing links from it. Given the convention for $A$, $k_j^{\\text{out}}$ is the sum of the entries in column $j$ of $A$.\n$$\nk_j^{\\text{out}} = \\sum_{i=1}^{3} A_{ij}\n$$\nLet's calculate the out-degrees for each node:\n-   For node $1$: $k_1^{\\text{out}} = A_{11} + A_{21} + A_{31} = 0 + 0 + 1 = 1$.\n-   For node $2$: $k_2^{\\text{out}} = A_{12} + A_{22} + A_{32} = 1 + 0 + 0 = 1$.\n-   For node $3$: $k_3^{\\text{out}} = A_{13} + A_{23} + A_{33} = 0 + 1 + 0 = 1$.\n\nThe probability of transitioning from node $j$ to node $i$, denoted $P_{ij}$, is given by $A_{ij}$ divided by the out-degree of the source node $j$.\n$$\nP_{ij} = \\frac{A_{ij}}{k_j^{\\text{out}}}\n$$\nSince $k_j^{\\text{out}} = 1$ for all $j \\in \\{1, 2, 3\\}$, the transition matrix $P$ is identical to the adjacency matrix $A$.\n$$\nP \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n1  0  0\n\\end{pmatrix}\n$$\nThis matrix is column-stochastic, as the sum of entries in each column is $1$.\n\n**Step 2: Finding the Stationary Distribution $r$**\n\nThe stationary distribution $r = \\begin{pmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{pmatrix}$ is defined by two conditions:\n1.  Invariance: $P r = r$.\n2.  Conservation of Probability: $\\sum_{i=1}^{3} r_i = 1$, with $r_i \\ge 0$.\n\nThe invariance condition, $P r = r$, is an eigenvector equation for the eigenvalue $\\lambda = 1$. We can write it as $(P - I)r = 0$, where $I$ is the $3 \\times 3$ identity matrix.\n$$\n\\left( \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n1  0  0\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} \\right)\n\\begin{pmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix}\n-1  1  0 \\\\\n0  -1  1 \\\\\n1  0  -1\n\\end{pmatrix}\n\\begin{pmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis matrix equation corresponds to the following system of linear equations:\n1.  $-r_1 + r_2 = 0 \\implies r_1 = r_2$\n2.  $-r_2 + r_3 = 0 \\implies r_2 = r_3$\n3.  $r_1 - r_3 = 0 \\implies r_1 = r_3$\n\nFrom this system, we deduce that all components of the stationary distribution vector are equal:\n$$\nr_1 = r_2 = r_3\n$$\nNow, we apply the conservation of probability condition:\n$$\nr_1 + r_2 + r_3 = 1\n$$\nSubstituting $r_2=r_1$ and $r_3=r_1$ into the equation gives:\n$$\nr_1 + r_1 + r_1 = 1\n$$\n$$\n3 r_1 = 1\n$$\n$$\nr_1 = \\frac{1}{3}\n$$\nTherefore, the components of the stationary distribution are:\n$$\nr_1 = r_2 = r_3 = \\frac{1}{3}\n$$\nThe stationary distribution vector is $r = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}$. The problem requests the final answer as a single row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Building on the concept of a stationary state, this practice  introduces the complete PageRank update rule, incorporating both the damping factor $\\alpha$ and a non-uniform teleportation vector. You will tackle a key practical challenge: the \"dangling node,\" a node with no outgoing links, which requires a specific adjustment to the transition matrix to ensure probability is conserved. This pen-and-paper calculation will give you a concrete feel for how the power method iteratively refines the PageRank scores.",
            "id": "4296061",
            "problem": "Consider a directed network with $n=4$ nodes labeled $\\{1,2,3,4\\}$. Nodes $2$ and $4$ are dangling nodes (they have zero out-degree). The observed hyperlinks are: node $1$ links to nodes $2$ and $3$, and node $3$ links to node $1$. Adopt the random-surfer model of a discrete-time Markov chain on the nodes, where at each step a surfer follows an outgoing hyperlink with probability $\\alpha \\in (0,1)$ and teleports according to a personalization distribution $\\mathbf{v} \\in \\mathbb{R}^{4}$ with probability $1-\\alpha$. The personalization vector is non-uniform and given by $\\mathbf{v} = \\big(\\frac{1}{2}, \\frac{1}{6}, \\frac{1}{3}, 0\\big)^{\\top}$, and the damping factor is $\\alpha = \\frac{17}{20}$. The Markov chain transition mechanism must be constructed from first principles as follows:\n- For any non-dangling node $j$, the probability of transitioning from node $j$ to any node $i$ is proportional to the observed out-links of node $j$, normalized to form a column that sums to $1$.\n- For any dangling node $j$, the column of the stochastic transition kernel is replaced by the personalization distribution $\\mathbf{v}$.\n\nLet $S \\in \\mathbb{R}^{4 \\times 4}$ denote the resulting column-stochastic hyperlink matrix built under the above rules. Starting from the personalization distribution as the initial state, $x^{(0)} = \\mathbf{v}$, consider the linear iteration for the PageRank vector given by $x^{(k+1)} = \\alpha S x^{(k)} + (1-\\alpha)\\mathbf{v}$. Explicitly construct the matrix $S$ for this network, verify it is column-stochastic, and compute the first two iterates $x^{(1)}$ and $x^{(2)}$ exactly as rational numbers. Provide the exact value of the third component of $x^{(2)}$, expressed as a reduced fraction. No rounding is required and no units are involved.",
            "solution": "The problem asks for the construction of the hyperlink matrix $S$, verification of its properties, and the calculation of the first two iterates of the PageRank vector, culminating in the third component of the second iterate.\n\nFirst, we construct the column-stochastic hyperlink matrix $S \\in \\mathbb{R}^{4 \\times 4}$. The network has $n=4$ nodes. Nodes are labeled $\\{1,2,3,4\\}$.\n\n- **Node 1** is non-dangling, with out-links to nodes $2$ and $3$. Its out-degree is $k_1=2$. The first column of $S$ represents transitions from node $1$. The probabilities are distributed equally among its out-links. Thus, the entries are $S_{21} = \\frac{1}{2}$, $S_{31} = \\frac{1}{2}$, and all other entries in this column are $0$.\n\n- **Node 2** is a dangling node. As per the rule, the second column of $S$ is replaced by the personalization vector $\\mathbf{v} = \\left(\\frac{1}{2}, \\frac{1}{6}, \\frac{1}{3}, 0\\right)^{\\top}$.\n\n- **Node 3** is non-dangling, with an out-link to node $1$. Its out-degree is $k_3=1$. The third column of $S$ has a single non-zero entry, $S_{13} = 1$.\n\n- **Node 4** is a dangling node. The fourth column of $S$ is also replaced by the personalization vector $\\mathbf{v}$.\n\nCombining these columns, the matrix $S$ is constructed as:\n$$S = \\begin{pmatrix} 0  \\frac{1}{2}  1  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{6}  0  \\frac{1}{6} \\\\ \\frac{1}{2}  \\frac{1}{3}  0  \\frac{1}{3} \\\\ 0  0  0  0 \\end{pmatrix}$$\n\nNext, we verify that $S$ is column-stochastic by checking if the sum of entries in each column is equal to $1$.\n- Column 1: $0 + \\frac{1}{2} + \\frac{1}{2} + 0 = 1$.\n- Column 2: $\\frac{1}{2} + \\frac{1}{6} + \\frac{1}{3} + 0 = \\frac{3}{6} + \\frac{1}{6} + \\frac{2}{6} = \\frac{6}{6} = 1$.\n- Column 3: $1 + 0 + 0 + 0 = 1$.\n- Column 4: $\\frac{1}{2} + \\frac{1}{6} + \\frac{1}{3} + 0 = 1$.\nThe matrix $S$ is confirmed to be column-stochastic.\n\nThe PageRank iteration is given by $x^{(k+1)} = \\alpha S x^{(k)} + (1-\\alpha)\\mathbf{v}$.\nWe are given the damping factor $\\alpha = \\frac{17}{20}$, hence $1-\\alpha = \\frac{3}{20}$. The initial state is $x^{(0)} = \\mathbf{v} = \\left(\\frac{1}{2}, \\frac{1}{6}, \\frac{1}{3}, 0\\right)^{\\top}$.\n\nWe compute the first iterate $x^{(1)}$. First, we calculate the product $S x^{(0)} = S\\mathbf{v}$:\n$$S\\mathbf{v} = \\begin{pmatrix} 0  \\frac{1}{2}  1  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{6}  0  \\frac{1}{6} \\\\ \\frac{1}{2}  \\frac{1}{3}  0  \\frac{1}{3} \\\\ 0  0  0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{6} \\\\ \\frac{1}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0(\\frac{1}{2}) + \\frac{1}{2}(\\frac{1}{6}) + 1(\\frac{1}{3}) + \\frac{1}{2}(0) \\\\ \\frac{1}{2}(\\frac{1}{2}) + \\frac{1}{6}(\\frac{1}{6}) + 0(\\frac{1}{3}) + \\frac{1}{6}(0) \\\\ \\frac{1}{2}(\\frac{1}{2}) + \\frac{1}{3}(\\frac{1}{6}) + 0(\\frac{1}{3}) + \\frac{1}{3}(0) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{12} + \\frac{1}{3} \\\\ \\frac{1}{4} + \\frac{1}{36} \\\\ \\frac{1}{4} + \\frac{1}{18} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{12} \\\\ \\frac{10}{36} \\\\ \\frac{11}{36} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{12} \\\\ \\frac{5}{18} \\\\ \\frac{11}{36} \\\\ 0 \\end{pmatrix}$$\nNow, we find $x^{(1)}$:\n$$x^{(1)} = \\alpha S x^{(0)} + (1-\\alpha)\\mathbf{v} = \\frac{17}{20} \\begin{pmatrix} \\frac{5}{12} \\\\ \\frac{5}{18} \\\\ \\frac{11}{36} \\\\ 0 \\end{pmatrix} + \\frac{3}{20} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{6} \\\\ \\frac{1}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{85}{240} + \\frac{3}{40} \\\\ \\frac{85}{360} + \\frac{3}{120} \\\\ \\frac{187}{720} + \\frac{3}{60} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{85}{240} + \\frac{18}{240} \\\\ \\frac{85}{360} + \\frac{9}{360} \\\\ \\frac{187}{720} + \\frac{36}{720} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{103}{240} \\\\ \\frac{94}{360} \\\\ \\frac{223}{720} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{103}{240} \\\\ \\frac{47}{180} \\\\ \\frac{223}{720} \\\\ 0 \\end{pmatrix}$$\n\nFinally, we compute the second iterate $x^{(2)}$ to find its third component.\n$x^{(2)} = \\alpha S x^{(1)} + (1-\\alpha)\\mathbf{v}$.\nThe third component is $x^{(2)}_3 = \\alpha (S x^{(1)})_3 + (1-\\alpha)v_3$.\nFirst, we find the third component of the product $S x^{(1)}$:\n$$(S x^{(1)})_3 = S_{3,:} \\cdot x^{(1)} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{3}  0  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} \\frac{103}{240} \\\\ \\frac{47}{180} \\\\ \\frac{223}{720} \\\\ 0 \\end{pmatrix} = \\frac{1}{2}\\left(\\frac{103}{240}\\right) + \\frac{1}{3}\\left(\\frac{47}{180}\\right) = \\frac{103}{480} + \\frac{47}{540}$$\nTo sum these fractions, we find a common denominator, $\\text{lcm}(480, 540) = 4320$.\n$$(S x^{(1)})_3 = \\frac{103 \\cdot 9}{4320} + \\frac{47 \\cdot 8}{4320} = \\frac{927 + 376}{4320} = \\frac{1303}{4320}$$\nNow we can compute $x^{(2)}_3$:\n$$x^{(2)}_3 = \\frac{17}{20} \\left( \\frac{1303}{4320} \\right) + \\frac{3}{20} \\left( \\frac{1}{3} \\right) = \\frac{17 \\cdot 1303}{20 \\cdot 4320} + \\frac{1}{20} = \\frac{22151}{86400} + \\frac{1}{20}$$\nTo add these, we use the common denominator $86400$:\n$$x^{(2)}_3 = \\frac{22151}{86400} + \\frac{4320}{86400} = \\frac{22151 + 4320}{86400} = \\frac{26471}{86400}$$\nThe denominator is $86400 = 2^7 \\cdot 3^3 \\cdot 5^2$. The numerator $26471$ is not divisible by $2$, $3$ (sum of digits is $20$), or $5$. Thus, the fraction is in its reduced form.\nThe exact value of the third component of $x^{(2)}$ is $\\frac{26471}{86400}$.",
            "answer": "$$\\boxed{\\frac{26471}{86400}}$$"
        },
        {
            "introduction": "The true power and scalability of PageRank are realized through computational implementation. This hands-on coding exercise  challenges you to build the Google matrix and apply the power method to compute PageRank vectors for several distinct network structures. By tracking the change between successive iterations, you will empirically estimate the algorithm's convergence rate, gaining practical insight into how network topology influences the speed at which the stationary distribution is reached.",
            "id": "4296002",
            "problem": "Consider a directed network of $5$ nodes and the PageRank process modeled as a discrete-time Markov chain. Use the following fundamental base: a Markov chain is defined by a column-stochastic transition matrix, where the entry $P_{ij}$ represents the probability of a transition from node $j$ to node $i$, and a probability distribution vector $x^{(k)}$ evolves by multiplication $x^{(k)} = P \\, x^{(k-1)}$. In the random surfer model, at each step, with probability $\\alpha$ the surfer follows an outgoing link from the current node uniformly at random, and with probability $1 - \\alpha$ the surfer teleports uniformly to any node. If a node has no outgoing links (a dangling node), then when attempting to follow a link, the surfer chooses uniformly among all nodes. Initialize the process at the uniform distribution. The task is to compute several iterations of the Power Method (PM) for this process and empirically estimate the convergence rate from the decay of the $L^1$ norm of successive differences.\n\nConstruct the transition matrix by the following procedure, derived from the random surfer model:\n- Let $A$ be the adjacency matrix with entries $A_{ij} = 1$ if there is a directed edge from node $j$ to node $i$, and $A_{ij} = 0$ otherwise.\n- Form a column-stochastic matrix $S$ by normalizing each column of $A$; if a column $j$ has zero sum (node $j$ is dangling), replace that column by the uniform distribution over all $5$ nodes.\n- Form the random surfer transition matrix $G$ by combining $S$ with uniform teleportation: each column of $G$ is a convex combination of the corresponding column of $S$ with the uniform distribution over all $5$ nodes, using weights $\\alpha$ and $1 - \\alpha$ respectively.\n\nFor each graph below, set $\\alpha = 0.85$, initialize $x^{(0)}$ as the uniform distribution over $5$ nodes, and iterate $x^{(k)} = G \\, x^{(k-1)}$ for $k = 1,2,\\dots,K$ with $K = 40$. Let $d^{(k)} = \\lVert x^{(k)} - x^{(k-1)} \\rVert_{1}$ for $k \\geq 1$. For $k \\geq 2$, define the stepwise decay ratio $r^{(k)} = d^{(k)} / d^{(k-1)}$. Empirically estimate the convergence rate as the median of $r^{(k)}$ over the tail window $k \\in \\{20,21,\\dots,40\\}$.\n\nThe three test graphs (each with $5$ nodes) are:\n- Test case $1$ (no dangling nodes, moderately dense):\n  - Directed edges: $1 \\to 2$, $1 \\to 3$; $2 \\to 3$, $2 \\to 4$; $3 \\to 1$, $3 \\to 5$; $4 \\to 3$, $4 \\to 5$; $5 \\to 1$, $5 \\to 2$.\n- Test case $2$ (one dangling node):\n  - Directed edges: $1 \\to 2$; $2 \\to 3$, $2 \\to 5$; $3 \\to 1$; $4 \\to 3$, $4 \\to 5$; $5$ has no outgoing edges.\n- Test case $3$ (two communities with sparse interconnection):\n  - Directed edges: $1 \\to 2$, $1 \\to 3$; $2 \\to 1$; $3 \\to 2$, $3 \\to 4$; $4 \\to 5$; $5 \\to 4$.\n\nYour program must:\n- Construct $S$ and $G$ according to the described rules for each test case.\n- Compute $x^{(k)}$ for $k = 1,\\dots,40$ with $x^{(0)}$ uniform over $5$ nodes.\n- Compute $d^{(k)}$ and $r^{(k)}$ as specified.\n- For each test case, output the empirical convergence rate estimate defined as the median of $r^{(k)}$ over $k \\in \\{20,\\dots,40\\}$.\n\nFinal output format:\n- Produce a single line containing the three estimated convergence rates for test cases $1$, $2$, and $3$, in that order, as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places, for example, `[0.731234,0.812345,0.843210]`.",
            "solution": "The core of the problem is to simulate the PageRank process, which is modeled as a discrete-time Markov chain on a network of $N=5$ nodes. The state of the system at step $k$ is a probability distribution vector $x^{(k)} \\in \\mathbb{R}^N$, where the $i$-th component $x_i^{(k)}$ is the probability that a \"random surfer\" is at node $i$. The evolution of this state is governed by the equation $x^{(k)} = G x^{(k-1)}$, where $G$ is the Google matrix, a column-stochastic transition matrix. The stationary distribution, $x^* = \\lim_{k\\to\\infty} x^{(k)}$, is the PageRank vector.\n\nThe procedure involves several steps:\n1.  Constructing the adjacency matrix $A$ for each graph.\n2.  From $A$, constructing the link-following stochastic matrix $S$, accounting for dangling nodes.\n3.  Forming the Google matrix $G$ using the damping factor $\\alpha$.\n4.  Applying the Power Method iteratively to find the PageRank vector.\n5.  Empirically estimating the convergence rate from the sequence of iterates.\n\nFor computational purposes, we will index the nodes from $0$ to $4$ instead of the problem's $1$ to $5$.\n\n**1. Adjacency Matrix $A$**\nThe adjacency matrix $A$ encodes the network's link structure. Following the problem's definition, an entry $A_{ij}$ is $1$ if there is a directed edge from node $j$ to node $i$, and $0$ otherwise. Each column $j$ of $A$ thus represents the outgoing links from node $j$.\n\n**2. Stochastic Matrix $S$**\nThe matrix $S$ models the transitions of a surfer who only follows links. It is derived from $A$. For each node $j$, we calculate its out-degree, $d_j^{out} = \\sum_{i=1}^{N} A_{ij}$. This is the sum of the elements in column $j$ of $A$.\n- If $d_j^{out} > 0$, the surfer at node $j$ chooses one of the $d_j^{out}$ outgoing links uniformly at random. The $j$-th column of $S$, denoted $S_j$, is obtained by normalizing the corresponding column of $A$: $S_{ij} = A_{ij} / d_j^{out}$.\n- If $d_j^{out} = 0$, node $j$ is a \"dangling node\". The problem specifies that in this case, the surfer jumps to any node in the network with uniform probability. Therefore, the $j$-th column of $S$ is set to a uniform probability vector, i.e., $S_{ij} = 1/N$ for all $i$.\n\n**3. Google Matrix $G$**\nThe Google matrix $G$ incorporates the \"random teleportation\" mechanism, which ensures the underlying Markov chain is ergodic and thus has a unique stationary distribution. It is constructed as a convex combination:\n$$G = \\alpha S + (1 - \\alpha) \\frac{1}{N} J$$\nwhere $\\alpha = 0.85$ is the damping factor, $S$ is the matrix from the previous step, $N=5$ is the number of nodes, and $J$ is the $N \\times N$ matrix of all ones. The term $\\frac{1}{N}J$ represents uniform teleportation to any node. Each column of $G$ is a valid probability distribution, making $G$ column-stochastic.\n\n**4. Power Method Iteration**\nThe PageRank vector is the principal eigenvector of $G$ (corresponding to the eigenvalue $\\lambda_1 = 1$). The Power Method is an iterative algorithm to find this eigenvector. We start with an initial probability distribution $x^{(0)}$ and repeatedly apply the transition matrix:\n$$x^{(k)} = G x^{(k-1)}$$\nThe problem specifies the initial state to be the uniform distribution: $x_i^{(0)} = 1/N = 1/5$ for all $i \\in \\{0, 1, 2, 3, 4\\}$. We are to perform $K=40$ iterations.\n\n**5. Empirical Convergence Rate Estimation**\nThe theoretical convergence rate of the Power Method is determined by the magnitude of the second-largest eigenvalue of $G$, denoted $|\\lambda_2|$. As $k$ becomes large, the error term is dominated by the component associated with $\\lambda_2$, and the distance between successive iterates decays geometrically:\n$$\\lVert x^{(k)} - x^{(k-1)} \\rVert \\propto |\\lambda_2|^k$$\nThe problem asks for an empirical estimate of this rate. We calculate the $L^1$-norm of the difference between successive iterates:\n$$d^{(k)} = \\lVert x^{(k)} - x^{(k-1)} \\rVert_{1} = \\sum_{i=0}^{N-1} |x_i^{(k)} - x_i^{(k-1)}| \\quad \\text{for } k \\geq 1$$\nFrom this sequence of differences, we compute the ratio of successive terms:\n$$r^{(k)} = \\frac{d^{(k)}}{d^{(k-1)}} \\quad \\text{for } k \\geq 2$$\nFor large $k$, this ratio $r^{(k)}$ should approach $|\\lambda_2|$. To obtain a stable estimate, we compute the median of these ratios over a tail window of the iterations, specifically for $k \\in \\{20, 21, \\dots, 40\\}$. This median serves as our empirical estimate of the convergence rate.\n\nThis procedure will be applied to each of the three test cases provided.\n- **Test case 1:** A well-connected graph with no dangling nodes.\n- **Test case 2:** A graph with one dangling node (node $5$, or index $4$), requiring special handling during the construction of $S$.\n- **Test case 3:** A graph with a community structure, which is known to potentially slow down convergence, leading to an empirical rate closer to $\\alpha$.\n\nThe implementation will follow these steps precisely to calculate the estimated convergence rate for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the empirical convergence rate for the PageRank algorithm on three test graphs.\n    \"\"\"\n\n    # Node count and algorithm parameters\n    N = 5\n    ALPHA = 0.85\n    K = 40\n    \n    # Define the test cases from the problem statement.\n    # Edges are (source, destination) using 1-based indexing as in the problem.\n    test_cases_edges = [\n        # Test case 1 (no dangling nodes, moderately dense)\n        [(1, 2), (1, 3), (2, 3), (2, 4), (3, 1), (3, 5), (4, 3), (4, 5), (5, 1), (5, 2)],\n        # Test case 2 (one dangling node)\n        [(1, 2), (2, 3), (2, 5), (3, 1), (4, 3), (4, 5)], # Node 5 is dangling\n        # Test case 3 (two communities with sparse interconnection)\n        [(1, 2), (1, 3), (2, 1), (3, 2), (3, 4), (4, 5), (5, 4)],\n    ]\n\n    results = []\n    \n    for edges in test_cases_edges:\n        # Step 1: Construct Adjacency Matrix A (using 0-based indexing)\n        # A[i, j] = 1 if there is an edge from j to i\n        A = np.zeros((N, N), dtype=float)\n        for src, dest in edges:\n            A[dest - 1, src - 1] = 1.0\n\n        # Step 2: Construct Stochastic Matrix S\n        S = A.copy()\n        out_degrees = S.sum(axis=0)\n        \n        for j in range(N):\n            if out_degrees[j] > 0:\n                S[:, j] /= out_degrees[j]\n            else: # Dangling node\n                S[:, j] = 1.0 / N\n\n        # Step 3: Construct Google Matrix G\n        J = np.ones((N, N), dtype=float)\n        G = ALPHA * S + (1 - ALPHA) * (1.0 / N) * J\n\n        # Step 4: Power Method Iteration\n        # Initialize x^(0) as the uniform distribution\n        x_prev = np.ones(N) / N\n        \n        # Store L1 norms of differences\n        d_k = []\n        for _ in range(K):\n            x_curr = G @ x_prev\n            # Calculate d^(k) = ||x^(k) - x^(k-1)||_1\n            diff_norm = np.linalg.norm(x_curr - x_prev, ord=1)\n            d_k.append(diff_norm)\n            x_prev = x_curr\n\n        # Step 5: Empirical Convergence Rate Estimation\n        # Calculate ratios r^(k) = d^(k) / d^(k-1)\n        r_k = []\n        # Loop for k from 2 to K (which is index 1 to K-1 of d_k)\n        for i in range(1, len(d_k)):\n            # Avoid division by zero, although not expected for this problem\n            if d_k[i-1] > 1e-15:\n                ratio = d_k[i] / d_k[i-1]\n                r_k.append(ratio)\n            else:\n                # If a difference becomes zero, convergence is perfect (rate 0)\n                r_k.append(0.0)\n\n        # The problem asks for the median of r^(k) for k in {20, ..., 40}.\n        # r_k[i] corresponds to r^(i+2). So k=20 corresponds to index 18.\n        # The tail window is from index 18 (for k=20) to 38 (for k=40).\n        tail_window_start_index = 18\n        tail_ratios = r_k[tail_window_start_index:]\n        \n        convergence_rate = np.median(tail_ratios)\n        results.append(convergence_rate)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}