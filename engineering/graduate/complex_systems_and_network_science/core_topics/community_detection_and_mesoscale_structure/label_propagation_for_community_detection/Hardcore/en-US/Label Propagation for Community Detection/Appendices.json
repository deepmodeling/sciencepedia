{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the Label Propagation Algorithm, we must start with its fundamental building block: the single-node update. This first exercise provides a hands-on walk-through of one synchronous iteration on a small, weighted network. By manually calculating the new label for each node based on the weighted votes of its neighbors and a specific tie-breaking rule, you will solidify your grasp of the core LPA mechanism before moving on to more complex dynamics .",
            "id": "4285516",
            "problem": "Consider an undirected, weighted network with node set $\\{1,2,3,4,5,6\\}$ and weighted adjacency matrix $W \\in \\mathbb{R}^{6 \\times 6}$ given by\n$$\nW \\;=\\;\n\\begin{pmatrix}\n0 & 1 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 2 & 0 \\\\\n1 & 1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 1 & 1 \\\\\n0 & 2 & 0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 0\n\\end{pmatrix}.\n$$\nWe will apply one synchronous iteration of the Label Propagation Algorithm (LPA), defined as follows: at iteration $t$, each node $i$ updates its label to the label that maximizes the sum of the weights $W_{ij}$ of edges from $i$ to its neighbors $j$ currently carrying that label. If there is a tie among two or more labels (equal maximizing sums), break the tie by choosing the label with the smallest index. Let the label set be $\\mathcal{L} = \\{\\ell_1, \\ell_2\\}$, with the index order $\\ell_1 \\prec \\ell_2$. The initial labeling is\n$$\nL^{(0)} \\;=\\; (\\ell_1,\\; \\ell_2,\\; \\ell_2,\\; \\ell_1,\\; \\ell_2,\\; \\ell_1),\n$$\nassigned to nodes $(1,2,3,4,5,6)$ respectively.\n\nPerform exactly one synchronous iteration of LPA to obtain $L^{(1)}$. Then, treating $L^{(1)}$ as a partition of the nodes, compute the modularity $Q$ of this partition under the standard weighted, undirected configuration-model null baseline. Express the final modularity as an exact rational number. No rounding is required. The final answer must be a single real-valued number.",
            "solution": "We begin from the definition of the Label Propagation Algorithm (LPA): each node selects the label maximizing the sum of incident edge weights to neighbors carrying that label. The update is synchronous, so all nodes compute their new labels from $L^{(0)}$ and update simultaneously to form $L^{(1)}$. The tie-breaking rule selects the smallest index label when maxima are equal, here $\\ell_1 \\prec \\ell_2$.\n\nStep $1$: compute $L^{(1)}$ by aggregating neighbor weights by label for each node under $L^{(0)}$.\n\n- Node $1$ has neighbors $\\{2,3\\}$ with weights $W_{12} = 1$ and $W_{13} = 1$. Under $L^{(0)}$, node $2$ carries $\\ell_2$ and node $3$ carries $\\ell_2$. Aggregated weights: $\\ell_1$: $0$, $\\ell_2$: $1+1 = 2$. Node $1$ updates to $\\ell_2$.\n\n- Node $2$ has neighbors $\\{1,3,5\\}$ with weights $W_{21} = 1$, $W_{23} = 1$, $W_{25} = 2$. Under $L^{(0)}$, labels are $L^{(0)}(1) = \\ell_1$, $L^{(0)}(3) = \\ell_2$, $L^{(0)}(5) = \\ell_2$. Aggregated weights: $\\ell_1$: $1$, $\\ell_2$: $1+2 = 3$. Node $2$ updates to $\\ell_2$ (unchanged).\n\n- Node $3$ has neighbors $\\{1,2,4\\}$ with weights $W_{31} = 1$, $W_{32} = 1$, $W_{34} = 1$. Under $L^{(0)}$, labels are $L^{(0)}(1) = \\ell_1$, $L^{(0)}(2) = \\ell_2$, $L^{(0)}(4) = \\ell_1$. Aggregated weights: $\\ell_1$: $1+1 = 2$, $\\ell_2$: $1$. Node $3$ updates to $\\ell_1$.\n\n- Node $4$ has neighbors $\\{3,5,6\\}$ with weights $W_{43} = 1$, $W_{45} = 1$, $W_{46} = 1$. Under $L^{(0)}$, labels are $L^{(0)}(3) = \\ell_2$, $L^{(0)}(5) = \\ell_2$, $L^{(0)}(6) = \\ell_1$. Aggregated weights: $\\ell_1$: $1$, $\\ell_2$: $1+1 = 2$. Node $4$ updates to $\\ell_2$.\n\n- Node $5$ has neighbors $\\{2,4,6\\}$ with weights $W_{52} = 2$, $W_{54} = 1$, $W_{56} = 1$. Under $L^{(0)}$, labels are $L^{(0)}(2) = \\ell_2$, $L^{(0)}(4) = \\ell_1$, $L^{(0)}(6) = \\ell_1$. Aggregated weights: $\\ell_1$: $1+1 = 2$, $\\ell_2$: $2$. This is a tie between $\\ell_1$ and $\\ell_2$ at total weight $2$. By the tie-breaking rule, node $5$ updates to $\\ell_1$ (the smallest index).\n\n- Node $6$ has neighbors $\\{4,5\\}$ with weights $W_{64} = 1$, $W_{65} = 1$. Under $L^{(0)}$, labels are $L^{(0)}(4) = \\ell_1$, $L^{(0)}(5) = \\ell_2$. Aggregated weights: $\\ell_1$: $1$, $\\ell_2$: $1$. This is a tie; by tie-breaking, node $6$ updates to $\\ell_1$ (unchanged).\n\nCollecting the updates, the synchronous iteration yields\n$$\nL^{(1)} \\;=\\; (\\ell_2,\\; \\ell_2,\\; \\ell_1,\\; \\ell_2,\\; \\ell_1,\\; \\ell_1).\n$$\n\nStep $2$: compute the modularity $Q$ of the partition induced by $L^{(1)}$ under the standard weighted, undirected configuration-model baseline. The modularity is defined by comparing the observed intra-community edge weight to the expected weight under the null model. For an undirected weighted graph, let $m$ be the sum of all edge weights, and let $k_i = \\sum_{j} W_{ij}$ be the strength (weighted degree) of node $i$. An equivalent and standard expression for modularity is\n$$\nQ \\;=\\; \\sum_{s} \\left( \\frac{l_s}{m} \\;-\\; \\left( \\frac{d_s}{2m} \\right)^{2} \\right),\n$$\nwhere the sum is over communities $s$, $l_s$ is the total weight of edges with both endpoints in community $s$, and $d_s$ is the sum of strengths $k_i$ over nodes in community $s$. We derive this identity from the baseline $\\left(W_{ij} - \\frac{k_i k_j}{2m}\\right)$ formulation by summing over node pairs within each community and recognizing that $\\sum_{i,j \\in s} W_{ij} = 2 l_s$ and $\\sum_{i \\in s} k_i = d_s$.\n\nCompute $m$, the total edge weight:\n$$\nm \\;=\\; W_{12} + W_{13} + W_{23} + W_{45} + W_{56} + W_{46} + W_{34} + W_{25}\n\\;=\\; 1+1+1+1+1+1+1+2 \\;=\\; 9.\n$$\nThus $2m = 18$.\n\nCompute strengths $k_i$ for $i \\in \\{1,\\dots,6\\}$:\n\\begin{align*}\nk_1 &= W_{12} + W_{13} \\;=\\; 1 + 1 \\;=\\; 2, \\\\\nk_2 &= W_{21} + W_{23} + W_{25} \\;=\\; 1 + 1 + 2 \\;=\\; 4, \\\\\nk_3 &= W_{31} + W_{32} + W_{34} \\;=\\; 1 + 1 + 1 \\;=\\; 3, \\\\\nk_4 &= W_{43} + W_{45} + W_{46} \\;=\\; 1 + 1 + 1 \\;=\\; 3, \\\\\nk_5 &= W_{52} + W_{54} + W_{56} \\;=\\; 2 + 1 + 1 \\;=\\; 4, \\\\\nk_6 &= W_{64} + W_{65} \\;=\\; 1 + 1 \\;=\\; 2.\n\\end{align*}\nCheck consistency: $\\sum_{i=1}^{6} k_i = 2+4+3+3+4+2 = 18 = 2m$.\n\nUnder $L^{(1)}$, community $\\ell_1$ contains nodes $\\{3,5,6\\}$ and community $\\ell_2$ contains nodes $\\{1,2,4\\}$.\n\nCompute intra-community weights:\n- For community $\\ell_1$ (nodes $\\{3,5,6\\}$), the edges with both endpoints inside are $(5,6)$ with weight $W_{56} = 1$. There are no edges $(3,5)$ or $(3,6)$ in $W$. Hence $l_{\\ell_1} = 1$.\n\n- For community $\\ell_2$ (nodes $\\{1,2,4\\}$), the edges with both endpoints inside are $(1,2)$ with weight $W_{12} = 1$. There are no edges $(1,4)$ or $(2,4)$. Hence $l_{\\ell_2} = 1$.\n\nCompute community strengths:\n\\begin{align*}\nd_{\\ell_1} &= k_3 + k_5 + k_6 \\;=\\; 3 + 4 + 2 \\;=\\; 9, \\\\\nd_{\\ell_2} &= k_1 + k_2 + k_4 \\;=\\; 2 + 4 + 3 \\;=\\; 9.\n\\end{align*}\n\nNow evaluate $Q$:\n\\begin{align*}\nQ &= \\left( \\frac{l_{\\ell_1}}{m} - \\left( \\frac{d_{\\ell_1}}{2m} \\right)^{2} \\right)\n   + \\left( \\frac{l_{\\ell_2}}{m} - \\left( \\frac{d_{\\ell_2}}{2m} \\right)^{2} \\right) \\\\\n  &= \\left( \\frac{1}{9} - \\left( \\frac{9}{18} \\right)^{2} \\right)\n   + \\left( \\frac{1}{9} - \\left( \\frac{9}{18} \\right)^{2} \\right) \\\\\n  &= 2 \\left( \\frac{1}{9} - \\frac{1}{4} \\right)\n   \\;=\\; 2 \\left( \\frac{4 - 9}{36} \\right)\n   \\;=\\; 2 \\left( -\\frac{5}{36} \\right)\n   \\;=\\; -\\frac{10}{36}\n   \\;=\\; -\\frac{5}{18}.\n\\end{align*}\n\nTherefore, the modularity of the partition produced by one synchronous LPA iteration with the specified tie-breaking is $-\\frac{5}{18}$.",
            "answer": "$$\\boxed{-\\frac{5}{18}}$$"
        },
        {
            "introduction": "The behavior of an algorithm often reveals itself not in a single step, but over multiple iterations. This practice explores the emergent dynamics of synchronous LPA, demonstrating how simple local rules can lead to non-trivial global behavior. By analyzing the label updates on a complete bipartite graph, you will uncover a classic limitation of the synchronous approach—the potential for persistent oscillations—which highlights why asynchronous updates are often preferred in practice .",
            "id": "4285556",
            "problem": "Consider the Label Propagation Algorithm (LPA), where each vertex in a network carries a label in the set $\\{+1,-1\\}$ and, under synchronous updates, each vertex simultaneously adopts the label that is the strict majority among its neighbors’ labels. In the case of an exact tie, the vertex retains its current label. Let $G$ be the complete bipartite graph $K_{m,n}$ with bipartition $(U,V)$, where $|U|=m$ and $|V|=n$, and every vertex in $U$ is connected to every vertex in $V$ with no edges inside $U$ or $V$. Initialize labels so that all vertices in $U$ have label $+1$ and all vertices in $V$ have label $-1$.\n\nUsing only core definitions of synchronous label updates and the structure of a bipartite graph, derive the resulting synchronous label dynamics starting from this initial condition. Then, determine the minimal period of the orbit generated by these dynamics (i.e., the smallest positive integer $p$ such that the system returns to its initial labeling after exactly $p$ synchronous updates). Your final answer must be a single real number. No rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and self-contained. It describes a deterministic process on a well-defined mathematical object. We may therefore proceed with the solution.\n\nLet the state of the network at a discrete time step $t$ be denoted by the collection of all vertex labels. Let $L_t(x)$ be the label of vertex $x$ at time $t$. The set of labels is $\\{+1, -1\\}$. The network is a complete bipartite graph $G = K_{m,n}$ with bipartition $(U, V)$, where $|U|=m$ and $|V|=n$.\n\nThe initial condition at $t=0$ is given as:\nFor any vertex $u \\in U$, $L_0(u) = +1$.\nFor any vertex $v \\in V$, $L_0(v) = -1$.\n\nThe update rule is synchronous. At each time step $t+1$, every vertex $x$ simultaneously updates its label, $L_{t+1}(x)$, to be the label held by the strict majority of its neighbors at time $t$. If there is an exact tie among neighbor labels, the vertex retains its current label, i.e., $L_{t+1}(x) = L_t(x)$.\n\nLet us analyze the evolution of the system from $t=0$ to $t=1$.\n\nFirst, consider an arbitrary vertex $u \\in U$. By the definition of a complete bipartite graph, the set of neighbors of $u$, denoted $N(u)$, is the entire set $V$. The size of the neighborhood is $|N(u)| = |V| = n$. At time $t=0$, all vertices in $V$ have the label $-1$. Therefore, for any vertex $u \\in U$, all of its $n$ neighbors have the label $-1$. The collection of neighbor labels for $u$ is $\\{L_0(y) \\text{ for } y \\in N(u)\\} = \\{-1, -1, \\dots, -1\\}$. Provided $n > 0$, the strict majority label is unambiguously $-1$. The tie-breaking condition is not met. Thus, at time $t=1$, every vertex in $U$ adopts the label $-1$.\n$$ \\forall u \\in U, \\quad L_1(u) = -1 $$\n\nNext, consider an arbitrary vertex $v \\in V$. The set of its neighbors, $N(v)$, is the entire set $U$. The size of the neighborhood is $|N(v)| = |U| = m$. At time $t=0$, all vertices in $U$ have the label $+1$. Therefore, for any vertex $v \\in V$, all of its $m$ neighbors have the label $+1$. The collection of neighbor labels for $v$ is $\\{L_0(y) \\text{ for } y \\in N(v)\\} = \\{+1, +1, \\dots, +1\\}$. Provided $m > 0$, the strict majority label is unambiguously $+1$. Again, the tie-breaking condition is irrelevant. Thus, at time $t=1$, every vertex in $V$ adopts the label $+1$.\n$$ \\forall v \\in V, \\quad L_1(v) = +1 $$\n\nSo, at time $t=1$, the state of the system is the complete opposite of the initial state: all vertices in $U$ have label $-1$ and all vertices in $V$ have label $+1$.\n\nNow, let us analyze the evolution of the system from $t=1$ to $t=2$, using the state at $t=1$ as the new initial condition.\n\nConsider an arbitrary vertex $u \\in U$. Its neighbors are the $n$ vertices in $V$. At time $t=1$, all these neighbors have the label $+1$. The strict majority label among the neighbors of $u$ is therefore $+1$. So, at time $t=2$, every vertex in $U$ adopts the label $+1$.\n$$ \\forall u \\in U, \\quad L_2(u) = +1 $$\n\nConsider an arbitrary vertex $v \\in V$. Its neighbors are the $m$ vertices in $U$. At time $t=1$, all these neighbors have the label $-1$. The strict majority label among the neighbors of $v$ is therefore $-1$. So, at time $t=2$, every vertex in $V$ adopts the label $-1$.\n$$ \\forall v \\in V, \\quad L_2(v) = -1 $$\n\nThe state of the system at $t=2$ is: $L_2(u)=+1$ for all $u \\in U$ and $L_2(v)=-1$ for all $v \\in V$. This is identical to the initial state at $t=0$.\n\nLet $S_t$ be the state of the system (the full configuration of labels) at time $t$. We have shown that $S_2 = S_0$. Since the update rule is deterministic, the system's evolution is periodic. The sequence of states is $S_0, S_1, S_2, S_3, \\dots$, which is equivalent to $S_0, S_1, S_0, S_1, \\dots$.\n\nThe problem asks for the minimal period of the orbit, which is the smallest positive integer $p$ such that $S_p = S_0$.\nWe have found that for $p=2$, $S_2 = S_0$. We must check if a smaller positive integer satisfies this condition. The only smaller positive integer is $p=1$.\n\nLet's compare the state at $t=1$ with the state at $t=0$.\nState $S_0$: $L_0(u)=+1$ for $u \\in U$ and $L_0(v)=-1$ for $v \\in V$.\nState $S_1$: $L_1(u)=-1$ for $u \\in U$ and $L_1(v)=+1$ for $v \\in V$.\n\nAssuming the graph is non-trivial (i.e., $m \\ge 1$ and $n \\ge 1$), the sets $U$ and $V$ are non-empty, and the labels are different. Therefore, $S_1 \\neq S_0$.\n\nThe smallest positive integer $p$ for which $S_p = S_0$ is $p=2$. This result holds for any $m, n \\ge 1$. The system enters a limit cycle of length $2$ immediately. The minimal period of the orbit generated by the dynamics is therefore $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Moving from theoretical dynamics to practical application, this exercise addresses the implementation of asynchronous LPA and one of its most critical properties: sensitivity to initial conditions. You will not only implement the algorithm but also learn to quantify the difference between partitions using the Variation of Information (VI), a standard metric derived from information theory. This practice is essential for developing a robust understanding of how to use and evaluate LPA in real-world scenarios where the final community structure is not known in advance .",
            "id": "4285521",
            "problem": "Consider an undirected, simple graph $G = (V,E)$ with $|V| = n$ nodes labeled $0,1,\\ldots,n-1$. A community assignment is represented by a partition of $V$ encoded as an integer label vector $\\ell \\in \\{0,1,2,\\ldots\\}^{n}$, where $\\ell_i$ is the community label of node $i$. The Label Propagation Algorithm (LPA) assigns labels by local majority voting and proceeds as follows. Start from an initial labeling $\\ell^{(0)}$. At each iteration $t$, sweep nodes in the fixed order $0,1,\\ldots,n-1$, and update node $i$ with the rule: set $\\ell^{(t+)}_i$ to the label that appears most frequently among the neighbors of $i$ in $G$; if there is a tie, choose the smallest label value among the tied labels; if node $i$ has no neighbors, leave $\\ell^{(t+)}_i$ unchanged. Use the updated $\\ell^{(t+)}$ in-place as you progress through the sweep. Repeat sweeps until a full sweep produces no changes or a maximum of $1000$ sweeps is reached.\n\nTo quantify the sensitivity of LPA to initialization, consider two final partitions produced from different initial labels on the same graph. Let $X$ be the discrete random variable equal to the community label of a node drawn uniformly at random under the first partition, and let $Y$ be the corresponding variable under the second partition. Use the standard information-theoretic framework based on Shannon entropy and mutual information to derive and implement a principled distance between the two partitions. The derivation must start from the fundamental definitions: for a discrete random variable $Z$ with probability mass function $p_Z$, the Shannon entropy is $H(Z) = -\\sum_{z} p_Z(z)\\,\\log p_Z(z)$, and for a pair $(X,Y)$ with joint mass function $p_{X,Y}$ and marginals $p_X$ and $p_Y$, the mutual information is $I(X;Y) = \\sum_{x,y} p_{X,Y}(x,y)\\,\\log\\left(\\frac{p_{X,Y}(x,y)}{p_X(x)\\,p_Y(y)}\\right)$. The logarithm must be the natural logarithm.\n\nImplement the above asynchronous LPA with deterministic tie-breaking as specified. For each test case, run LPA to convergence from each provided initial labeling, collect the set of final partitions, and compute the average of the pairwise distances across all distinct pairs of final partitions. Express each test case’s answer as a real number rounded to six decimal places.\n\nThe test suite consists of four graphs and their corresponding initialization sets:\n\n- Test case $1$ (complete graph $K_5$):\n    - Nodes: $n = 5$.\n    - Edges: all unordered pairs $(i,j)$ with $0 \\le i < j \\le 4$, that is $\\{(0,1),(0,2),(0,3),(0,4),(1,2),(1,3),(1,4),(2,3),(2,4),(3,4)\\}$.\n    - Initial labelings:\n        - $\\ell^{(0,1)} = [0,1,2,3,4]$,\n        - $\\ell^{(0,2)} = [0,0,1,1,1]$,\n        - $\\ell^{(0,3)} = [2,2,2,2,2]$.\n\n- Test case $2$ (cycle graph $C_{10}$):\n    - Nodes: $n = 10$.\n    - Edges: $\\{(i,(i+1)\\bmod 10) \\mid i \\in \\{0,1,2,3,4,5,6,7,8,9\\}\\}$, explicitly $\\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,0)\\}$.\n    - Initial labelings:\n        - $\\ell^{(0,1)} = [0,1,2,3,4,5,6,7,8,9]$,\n        - $\\ell^{(0,2)} = [0,1,0,1,0,1,0,1,0,1]$,\n        - $\\ell^{(0,3)} = [0,0,0,0,0,1,1,1,1,1]$.\n\n- Test case $3$ (two cliques of size $4$ connected by a bridge):\n    - Nodes: $n = 8$.\n    - Clique edges on nodes $\\{0,1,2,3\\}$: all unordered pairs $(i,j)$ with $0 \\le i < j \\le 3$, that is $\\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)\\}$.\n    - Clique edges on nodes $\\{4,5,6,7\\}$: all unordered pairs $(i,j)$ with $4 \\le i < j \\le 7$, that is $\\{(4,5),(4,6),(4,7),(5,6),(5,7),(6,7)\\}$.\n    - Bridge edge: $(3,4)$.\n    - Initial labelings:\n        - $\\ell^{(0,1)} = [0,1,2,3,4,5,6,7]$,\n        - $\\ell^{(0,2)} = [0,0,0,0,1,1,1,1]$,\n        - $\\ell^{(0,3)} = [1,0,1,0,2,2,3,3]$.\n\n- Test case $4$ (single isolated node):\n    - Nodes: $n = 1$.\n    - Edges: $\\varnothing$.\n    - Initial labelings:\n        - $\\ell^{(0,1)} = [0]$,\n        - $\\ell^{(0,2)} = [1]$,\n        - $\\ell^{(0,3)} = [5]$.\n\nYour program must compute, for each test case, the average of the pairwise partition distances across all distinct pairs of final partitions obtained from the listed initial labelings. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1,2,3,4$, with each value rounded to six decimal places. For example, the output format should be exactly like: $[x_1,x_2,x_3,x_4]$ where each $x_i$ is a float as specified.",
            "solution": "The problem requires the implementation of an asynchronous Label Propagation Algorithm (LPA) and the derivation and implementation of an information-theoretic distance metric to compare the resulting community partitions. The solution is structured into three parts: first, the derivation of the distance metric; second, a description of the algorithmic implementation of LPA; and third, the application of these methods to the specific test cases.\n\n### Part 1: Derivation of the Partition Distance Metric\n\nWe are tasked with deriving a principled distance between two partitions, $\\mathcal{C}_1$ and $\\mathcal{C}_2$, of the same set of $n$ nodes. These partitions are represented by label vectors $\\ell^{(1)}$ and $\\ell^{(2)}$. Let $X$ be the discrete random variable for the community label of a node chosen uniformly at random from $V$ according to partition $\\mathcal{C}_1$, and $Y$ be the corresponding variable for partition $\\mathcal{C}_2$.\n\nThe probability mass functions are given by:\n- Marginal probability of a label $c_x$ in $\\mathcal{C}_1$: $p_X(c_x) = \\frac{|\\{i \\mid \\ell^{(1)}_i = c_x\\}|}{n}$\n- Marginal probability of a label $c_y$ in $\\mathcal{C}_2$: $p_Y(c_y) = \\frac{|\\{i \\mid \\ell^{(2)}_i = c_y\\}|}{n}$\n- Joint probability of observing labels $c_x$ and $c_y$: $p_{X,Y}(c_x, c_y) = \\frac{|\\{i \\mid \\ell^{(1)}_i = c_x \\text{ and } \\ell^{(2)}_i = c_y\\}|}{n}$\n\nThe Shannon entropy of a partition measures its uncertainty. For partition $\\mathcal{C}_1$, this is:\n$$ H(X) = -\\sum_{c_x} p_X(c_x) \\log p_X(c_x) $$\nwhere the logarithm is the natural logarithm as specified. Similarly, $H(Y)$ is the entropy of $\\mathcal{C}_2$.\n\nThe mutual information, $I(X;Y)$, quantifies the information shared between the two partitions:\n$$ I(X;Y) = \\sum_{c_x, c_y} p_{X,Y}(c_x, c_y) \\log \\left( \\frac{p_{X,Y}(c_x, c_y)}{p_X(c_x) p_Y(c_y)} \\right) $$\n\nA principled distance metric should measure the difference in information between the two partitions. A standard metric that fits this description is the **Variation of Information (VI)**. It is defined as the sum of the conditional entropies:\n$$ D(\\mathcal{C}_1, \\mathcal{C}_2) \\equiv VI(X,Y) = H(X|Y) + H(Y|X) $$\n$H(X|Y)$ is the uncertainty remaining in partition $\\mathcal{C}_1$ given partition $\\mathcal{C}_2$, and $H(Y|X)$ is the converse. This sum represents the total information we gain about one partition when the other is revealed. $VI$ is a true metric, satisfying non-negativity, identity of indiscernibles, symmetry, and the triangle inequality.\n\nUsing the chain rule for entropy, $H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$, and the definition of mutual information, $I(X;Y) = H(X) + H(Y) - H(X,Y)$, we can express the conditional entropies in terms of marginal entropies and mutual information:\n- $H(X|Y) = H(X) - I(X;Y)$\n- $H(Y|X) = H(Y) - I(X;Y)$\n\nSubstituting these into the definition of $VI$ gives the form we will use for computation:\n$$ D(\\mathcal{C}_1, \\mathcal{C}_2) = H(X) + H(Y) - 2I(X;Y) $$\nThis expression is derived from fundamental information-theoretic principles and provides a robust, symmetric measure of distance between partitions. It is independent of the specific integer values of the labels, depending only on the grouping of nodes. If two partitions are identical (up to a relabeling), their distance is $0$.\n\n### Part 2: Algorithmic Implementation\n\n**Label Propagation Algorithm (LPA):**\nThe asynchronous LPA is implemented as described. The graph is represented by an adjacency list. The algorithm proceeds in sweeps over the nodes in the fixed order $i=0, 1, \\ldots, n-1$. For each node $i$, we identify the labels of its neighbors. The new label for node $i$, $\\ell_i'$, is the one that occurs most frequently among its neighbors' current labels. The update is performed in-place, meaning that if we update node $i$, any subsequent node $j > i$ in the same sweep will see the new label $\\ell_i'$ when considering its neighbors. This makes the update order significant. Ties are broken by choosing the smallest integer label value. If a node has no neighbors, its label remains unchanged. The process iterates until a full sweep results in no label changes, or a maximum of $1000$ sweeps is completed.\n\n**Distance Calculation:**\nTo compute $D(\\mathcal{C}_1, \\mathcal{C}_2)$, we first construct a contingency table (or confusion matrix) $M$, where $M_{ij}$ counts the number of nodes assigned to community $i$ in $\\mathcal{C}_1$ and community $j$ in $\\mathcal{C}_2$. Dividing this matrix by $n$ gives the joint probability distribution $p_{X,Y}$. The marginal distributions $p_X$ and $p_Y$ are obtained by summing the rows and columns of the $p_{X,Y}$ matrix, respectively. We then compute $H(X)$, $H(Y)$, and $I(X;Y)$ using these distributions, taking care to handle terms where probabilities are zero to avoid numerical errors (e.g., $p \\log p$ is $0$ if $p=0$).\n\n### Part 3: Application to Test Cases\n\nFor each test case, we are given a graph and a set of three initial labelings.\n1.  For each of the three initial labelings, we run the LPA to convergence to obtain a final, stable partition. This results in a set of three final partitions, $\\{\\mathcal{P}_1, \\mathcal{P}_2, \\mathcal{P}_3\\}$.\n2.  We then compute the pairwise distances for all distinct pairs of these final partitions: $d_{12} = D(\\mathcal{P}_1, \\mathcal{P}_2)$, $d_{13} = D(\\mathcal{P}_1, \\mathcal{P}_3)$, and $d_{23} = D(\\mathcal{P}_2, \\mathcal{P}_3)$.\n3.  The final result for the test case is the average of these three distances: $\\frac{1}{3}(d_{12} + d_{13} + d_{23})$.\n\nThis procedure is followed for each of the four test cases provided. For example, in Test Case 3 (two cliques with a bridge), the initial labeling $[0,0,0,0,1,1,1,1]$ is already a stable configuration corresponding to the ground-truth communities, and LPA makes no changes. Other initializations, like $[0,1,2,3,4,5,6,7]$, converge to a single large community. The third initialization $[1,0,1,0,2,2,3,3]$ converges to a two-community structure identical to the stable one but with different label values. The distance between these two-community partitions is $0$, while the distance between either of them and the single-community partition is $\\log(2) \\approx 0.693147$. The average distance for this test case is therefore $\\frac{1}{3}(\\log(2) + \\log(2) + 0) = \\frac{2}{3}\\log(2) \\approx 0.462098$. The remaining cases are computed analogously.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef compute_distance(labels1, labels2):\n    \"\"\"\n    Computes the Variation of Information (VI) distance between two partitions.\n    VI(X,Y) = H(X) + H(Y) - 2*I(X,Y)\n    \"\"\"\n    n = len(labels1)\n    if n == 0:\n        return 0.0\n\n    # Map labels to 0-indexed integers for matrix construction\n    unique_l1, inv_l1 = np.unique(labels1, return_inverse=True)\n    unique_l2, inv_l2 = np.unique(labels2, return_inverse=True)\n    \n    num_c1 = len(unique_l1)\n    num_c2 = len(unique_l2)\n    \n    # Build contingency table (joint counts)\n    contingency = np.zeros((num_c1, num_c2), dtype=float)\n    np.add.at(contingency, (inv_l1, inv_l2), 1)\n\n    # Convert to joint probability distribution\n    p_xy = contingency / n\n    \n    # Calculate marginal probabilities\n    p_x = np.sum(p_xy, axis=1)\n    p_y = np.sum(p_xy, axis=0)\n    \n    # Calculate H(X)\n    nz_x = p_x > 0\n    h_x = -np.sum(p_x[nz_x] * np.log(p_x[nz_x]))\n    \n    # Calculate H(Y)\n    nz_y = p_y > 0\n    h_y = -np.sum(p_y[nz_y] * np.log(p_y[nz_y]))\n    \n    # Calculate I(X;Y)\n    nz_xy = p_xy > 0\n    p_x_p_y = np.outer(p_x, p_y)\n    \n    mi = np.sum(p_xy[nz_xy] * np.log(p_xy[nz_xy] / p_x_p_y[nz_xy]))\n    \n    # VI distance\n    vi = h_x + h_y - 2 * mi\n    \n    # Clamp small negative values that can arise from floating point errors\n    return max(0.0, vi)\n\n\ndef run_lpa(n, edges, initial_labels):\n    \"\"\"\n    Runs the asynchronous Label Propagation Algorithm.\n    \"\"\"\n    adj = [[] for _ in range(n)]\n    for u, v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n\n    labels = np.array(initial_labels, dtype=int)\n    max_sweeps = 1000\n\n    for _ in range(max_sweeps):\n        num_changes = 0\n        for i in range(n):\n            if not adj[i]:\n                continue\n            \n            neighbor_labels = labels[adj[i]]\n            \n            unique_labs, counts = np.unique(neighbor_labels, return_counts=True)\n            max_count = np.max(counts)\n            tied_labels = unique_labs[counts == max_count]\n            new_label = np.min(tied_labels)\n            \n            if labels[i] != new_label:\n                labels[i] = new_label\n                num_changes += 1\n        \n        if num_changes == 0:\n            break\n            \n    return labels.tolist()\n\n\ndef solve():\n    test_cases = [\n        # Test case 1: K_5\n        {\n            \"n\": 5,\n            \"edges\": [(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)],\n            \"initial_labelings\": [\n                [0, 1, 2, 3, 4],\n                [0, 0, 1, 1, 1],\n                [2, 2, 2, 2, 2]\n            ]\n        },\n        # Test case 2: C_10\n        {\n            \"n\": 10,\n            \"edges\": [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 0)],\n            \"initial_labelings\": [\n                [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n                [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n                [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n            ]\n        },\n        # Test case 3: two cliques + bridge\n        {\n            \"n\": 8,\n            \"edges\": [\n                (0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3), # Clique 1\n                (4, 5), (4, 6), (4, 7), (5, 6), (5, 7), (6, 7), # Clique 2\n                (3, 4)                                          # Bridge\n            ],\n            \"initial_labelings\": [\n                [0, 1, 2, 3, 4, 5, 6, 7],\n                [0, 0, 0, 0, 1, 1, 1, 1],\n                [1, 0, 1, 0, 2, 2, 3, 3]\n            ]\n        },\n        # Test case 4: single isolated node\n        {\n            \"n\": 1,\n            \"edges\": [],\n            \"initial_labelings\": [\n                [0],\n                [1],\n                [5]\n            ]\n        }\n    ]\n\n    final_results = []\n    for case in test_cases:\n        final_partitions = []\n        for init_labels in case[\"initial_labelings\"]:\n            final_p = run_lpa(case[\"n\"], case[\"edges\"], init_labels)\n            final_partitions.append(final_p)\n            \n        pairwise_distances = []\n        for p1_idx, p2_idx in combinations(range(len(final_partitions)), 2):\n            dist = compute_distance(final_partitions[p1_idx], final_partitions[p2_idx])\n            pairwise_distances.append(dist)\n            \n        if not pairwise_distances:\n            avg_dist = 0.0\n        else:\n            avg_dist = sum(pairwise_distances) / len(pairwise_distances)\n            \n        final_results.append(f\"{avg_dist:.6f}\")\n        \n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}