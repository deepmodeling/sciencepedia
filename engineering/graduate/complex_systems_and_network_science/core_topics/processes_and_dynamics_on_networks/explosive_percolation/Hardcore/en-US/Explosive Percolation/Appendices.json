{
    "hands_on_practices": [
        {
            "introduction": "To truly appreciate what makes a percolation transition \"explosive,\" it is essential to first master the classical case. This exercise guides you through deriving the critical properties of the standard Erdős–Rényi random graph, a cornerstone of network theory, using the elegant framework of branching processes. By first calculating the universal tail exponent $\\tau$ for this continuous transition, you will build a theoretical baseline to understand how competitive selection rules fundamentally alter cluster growth and lead to the abrupt phenomena seen in explosive percolation .",
            "id": "4275747",
            "problem": "Consider the Erdős–Rényi (ER) random graph $G(n, c/n)$ with $0  c \\leq 1$, where $n$ is the number of vertices and $c$ is a fixed control parameter. In the subcritical and critical regimes, finite connected components are tree-like and can be represented by a Galton–Watson branching process with Poisson($c$) offspring. Let $n_s$ denote the expected number of clusters of size $s$ per vertex, and define the susceptibility as $\\chi = \\sum_{s \\geq 1} s^{2} n_s$. The precritical tail of the cluster-size distribution is often written in the scaling form $n_s \\sim s^{-\\tau} \\exp(-s/s_c)$, where $s_c$ is a cutoff that diverges at criticality.\n\nStarting from the foundational connection between $G(n, c/n)$ and Galton–Watson branching processes with finite offspring variance, derive the asymptotic form of the per-vertex cluster-size number density $n_s$ at the critical point $c = 1$, and compute the tail exponent $\\tau$ in the scaling $n_s \\sim s^{-\\tau}$. Use only well-tested facts from branching processes, generating functions, and asymptotics, and demonstrate the consistency of your result with the divergence of the susceptibility $\\chi$ near criticality via standard scaling arguments for mean-field percolation.\n\nThen, analyze how Achlioptas edge-selection rules (for example, selecting one edge from two candidates by minimizing the product of the sizes of the clusters the endpoints belong to) bias the formation of large clusters. Based on first principles of near-critical mean-field growth with finite offspring variance and tree-like local structure, reason whether such selection is expected to change the leading power-law exponent $\\tau$ in the complete-graph limit, or instead to introduce subleading, slowly varying corrections to scaling (for example, multiplicative logarithmic factors) in the precritical tail and the susceptibility. Your analysis must clearly separate universal leading behavior from non-universal metric factors and possible logarithmic corrections.\n\nReport the value of the tail exponent $\\tau$ as a single exact number. No rounding is required. The final answer must be unitless.",
            "solution": "The Erdős–Rényi (ER) random graph $G(n, c/n)$ with $0  c \\leq 1$ has a well-known correspondence with Galton–Watson branching processes: the connected component containing a uniformly random vertex is distributed as the total progeny of a Galton–Watson process with Poisson($c$) offspring, provided the component is finite and cycles are negligible, which is true in the subcritical and critical regimes when $n$ is large.\n\nLet $S$ denote the total progeny of the Galton–Watson process starting from a single ancestor. The probability generating function $f(z)$ of the offspring distribution is $f(z) = \\exp(c(z - 1))$, and the generating function $H(z)$ for $S$ satisfies the classical functional equation\n$$\nH(z) = z f(H(z)) = z \\exp\\!\\big(c(H(z) - 1)\\big).\n$$\nThe coefficient $p_s = \\Pr(S = s)$ is given by the Lagrange inversion formula,\n$$\np_s = [z^s] H(z) = \\frac{1}{s} [u^{s-1}] f(u)^{s} = \\frac{1}{s} [u^{s-1}] \\exp\\!\\big(c s (u - 1)\\big).\n$$\nEvaluating the coefficient of $u^{s-1}$ in $\\exp\\!\\big(c s u\\big)$ yields\n$$\np_s = \\frac{1}{s} \\exp(-c s) \\frac{(c s)^{s-1}}{(s-1)!} = \\exp(-c s) \\frac{(c s)^{s-1}}{s!}.\n$$\nAt criticality, $c = 1$, so\n$$\np_s = \\exp(-s) \\frac{s^{s-1}}{s!}.\n$$\nThis is the Borel distribution for the total progeny at criticality. To obtain the asymptotic tail, we apply Stirling's approximation to $s!$,\n$$\ns! \\sim \\sqrt{2 \\pi} \\, s^{s + 1/2} \\exp(-s),\n$$\nwhich yields\n$$\np_s \\sim \\frac{\\exp(-s) s^{s-1}}{\\sqrt{2 \\pi} \\, s^{s + 1/2} \\exp(-s)} = \\frac{1}{\\sqrt{2 \\pi}} s^{-3/2}.\n$$\nThe quantity $n_s$, the expected number of clusters of size $s$ per vertex, relates to $p_s$ by the identity $s n_s = \\Pr(\\text{a random vertex is in a cluster of size } s) = p_s$, because $s n_s$ is the fraction of vertices belonging to clusters of size $s$. Therefore,\n$$\nn_s = \\frac{p_s}{s} \\sim \\frac{1}{\\sqrt{2 \\pi}} s^{-5/2}.\n$$\nHence, the tail exponent in the scaling $n_s \\sim s^{-\\tau}$ at criticality is\n$$\n\\tau = \\frac{5}{2}.\n$$\n\nTo check consistency with susceptibility scaling, define the susceptibility $\\chi = \\sum_{s \\geq 1} s^{2} n_s$. At criticality, substituting $n_s \\sim s^{-5/2}$ gives\n$$\n\\chi \\sim \\sum_{s \\geq 1} s^{2} \\cdot s^{-5/2} = \\sum_{s \\geq 1} s^{-1/2},\n$$\nwhich diverges, as expected. Away from criticality in the subcritical regime $c  1$, the cluster-size distribution acquires an exponential cutoff $s_c$,\n$$\nn_s \\sim s^{-\\tau} \\exp(-s/s_c),\n$$\nand the susceptibility scales as\n$$\n\\chi \\sim \\int_{1}^{s_c} s^{2 - \\tau} \\, ds \\sim s_c^{3 - \\tau}.\n$$\nIn mean-field percolation (including ER), the cutoff obeys $s_c \\sim |t|^{-1/\\sigma}$ with $t$ a measure of distance to criticality and $\\sigma = \\frac{1}{2}$, whence\n$$\n\\chi \\sim |t|^{-(3 - \\tau)/\\sigma}.\n$$\nWith $\\tau = \\frac{5}{2}$ and $\\sigma = \\frac{1}{2}$, this gives\n$$\n\\chi \\sim |t|^{-1},\n$$\nwhich matches the mean-field susceptibility exponent $\\gamma = 1$, confirming the internal consistency of the derived $\\tau$ with standard scaling.\n\nWe now analyze Achlioptas edge-selection rules (for instance, picking between two candidate edges by minimizing the product of the sizes of the clusters that their endpoints belong to) in the complete-graph limit. Such rules suppress the early coalescence of large clusters, thereby delaying the emergence of a giant component and narrowing the apparent transition window. However, the local structure of finite clusters remains tree-like, and the offspring distribution retains finite variance. In the near-critical regime, this implies that the universal features dictated by the Galton–Watson fixed point with finite variance persist. Consequently, the leading power-law tail of $n_s$ at criticality is governed by the same branching-process asymptotics, which yields $\\tau = \\frac{5}{2}$ as the universal mean-field value.\n\nWhat the selection bias can change are non-universal metric factors and the approach to criticality, often introducing slowly varying corrections encapsulated by multiplicative functions that vary logarithmically with size or control parameter. A consistent way to capture such effects is to write\n$$\nn_s \\sim s^{-\\tau} L(s), \\quad L(s) \\text{ slowly varying, e.g., } L(s) \\sim (\\ln s)^{\\hat{\\alpha}},\n$$\nand similarly, in the subcritical regime,\n$$\n\\chi \\sim |t|^{-1} \\, (\\ln |t|^{-1})^{\\hat{\\gamma}},\n$$\nfor some correction exponents $\\hat{\\alpha}$ and $\\hat{\\gamma}$ determined by the detailed selection dynamics. These logarithmic corrections arise because the bias can cause the effective control parameter to approach criticality with a drift that depends on system size in a slowly varying manner (for example, proportional to $1/\\ln n$), without changing the underlying branching fixed point that enforces the universal $\\tau = \\frac{5}{2}$ in the complete-graph limit. Therefore, Achlioptas selection biases are expected to modify subleading corrections while leaving the leading mean-field tail exponent $\\tau$ unchanged.\n\nIn summary, the computation from first principles via critical branching-process asymptotics gives $\\tau = \\frac{5}{2}$ for ER at criticality, and Achlioptas selection rules in the complete-graph limit are expected to preserve this leading exponent while potentially introducing logarithmic corrections in the slowly varying part of the distribution and susceptibility.",
            "answer": "$$\\boxed{\\frac{5}{2}}$$"
        },
        {
            "introduction": "Theory provides the \"why,\" but simulation provides the \"wow.\" This practice moves from analytical derivation to direct computational experiment, challenging you to implement an online version of an Achlioptas-type process. By coding the simulation, you will gain firsthand experience with the core mechanism of competitive selection and directly observe how parameters like lookahead horizon influence the sharpness of the transition, quantifying explosiveness through metrics like the maximum jump size .",
            "id": "4275752",
            "problem": "Consider a discrete-time random graph process on a complete graph of $n$ labeled vertices, where the evolving network begins with no edges. Let the set of all possible undirected edges be uniformly randomly permuted once at the start and then processed in order without replacement. At each time step $t$, an online controller with lookahead horizon $h$ is given a contiguous window of the next $h$ candidate edges in this fixed global permutation and must irrevocably choose exactly one edge to add to the network, discarding the remaining $h-1$ candidate edges. The controller’s choice is constrained to be purely online: it can only use the current connected-component partition of the network and the identities of the $h$ candidate edges. Define the controlled rule as follows: among the $h$ candidates, choose the edge that minimizes the product of the sizes of the connected components containing its endpoints; if an edge’s endpoints are already in the same component, define its product to be $+\\infty$ to deprioritize redundant edges when other options exist. This selection rule is an online Achlioptas-type minimization, with $h$ controlling the lookahead.\n\nFundamental base and core definitions:\n- A connected component of the current graph at time $t$ is a maximal set of vertices such that each pair is connected by a path of edges present at time $t$.\n- Let $L(t)$ denote the size of the largest connected component at time $t$, with $L(0)=1$ since all vertices start isolated.\n- Define the normalized largest component size $S(t) = L(t)/n$.\n- Explosiveness will be quantified by the maximum single-step jump in $S(t)$, namely $J_{\\max} = \\max_{1 \\le t \\le T} \\left(S(t) - S(t-1)\\right)$, with $S(0)=1/n$.\n- Define the critical crossing index for half-giant emergence as the smallest time $t$ such that $S(t) \\ge 1/2$; the associated edge density proxy is $p_{1/2} = t/n$, and if no such $t$ exists within the horizon of added edges, return $-1$.\n- Computational cost is defined as the total number of edge evaluations the controller performs; under the $h$-lookahead rule this is $C = h \\cdot T$, where $T$ is the number of edges actually added.\n\nTask:\n- Implement a complete, runnable program that simulates the above online controlled percolation for multiple parameter settings, using Disjoint Set Union (DSU) with path compression and union by size to maintain connected components efficiently. The program must compute, for each test case, the tuple $(J_{\\max}, p_{1/2}, C, T)$, where $J_{\\max}$ and $p_{1/2}$ are floats and $C$ and $T$ are integers.\n\nScientific realism and constraints:\n- The random permutation of edges must be generated once per test case using a fixed pseudorandom seed to ensure reproducibility.\n- Each time step consumes $h$ candidate edges from the permutation and adds exactly one selected edge to the graph, discarding the other $h-1$ candidates. This implies $h \\cdot T$ must not exceed the total number of distinct edges $\\frac{n(n-1)}{2}$ in the complete graph (to avoid running out of candidates).\n\nTest suite:\nProvide results for the following five parameter settings, each specified by $(n,h,T,\\text{seed})$:\n1. $(n,h,T,\\text{seed}) = (200,1,200,7)$\n2. $(n,h,T,\\text{seed}) = (200,4,200,7)$\n3. $(n,h,T,\\text{seed}) = (200,8,200,7)$\n4. $(n,h,T,\\text{seed}) = (30,4,80,3)$\n5. $(n,h,T,\\text{seed}) = (50,20,50,11)$\n\nAnswer specification:\n- For each test case, compute $(J_{\\max}, p_{1/2}, C, T)$ using the definitions above. Express $J_{\\max}$ and $p_{1/2}$ as decimal floats and $C$ and $T$ as integers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test-case result itself formatted as a four-element list. For example, a valid output format is $\\left[ [J_{\\max}^{(1)},p_{1/2}^{(1)},C^{(1)},T^{(1)}], [J_{\\max}^{(2)},p_{1/2}^{(2)},C^{(2)},T^{(2)}], \\dots \\right]$.",
            "solution": "The problem is well-posed, scientifically grounded in the field of network science, and provides a clear, formalizable set of rules for a computational simulation. All parameters and definitions are precise, and the constraints are consistent. Therefore, we proceed with the solution.\n\nThe core of this problem is to efficiently track the evolution of connected components in a graph under a specific growth rule. For this task, the Disjoint Set Union (DSU) data structure, also known as a union-find data structure, is exceptionally well-suited. As specified, our implementation utilizes two key optimizations: path compression and union by size.\n\n- **Path Compression**: During a `find` operation, which determines the representative or root of a component, we flatten the structure of the tree by making every node on the find path point directly to the root. This dramatically speeds up future `find` operations for nodes in that component.\n- **Union by Size**: When merging two components in a `union` operation, we always attach the root of the smaller component to the root of the larger one. This heuristic helps in keeping the component trees shallow, which in turn keeps `find` operations fast. The amortized time complexity for a sequence of $m$ operations on $n$ elements is nearly constant, specifically $O(m \\alpha(n))$, where $\\alpha(n)$ is the extremely slow-growing inverse Ackermann function.\n\nThe simulation proceeds for each test case, given by parameters $(n,h,T,\\text{seed})$, as follows:\n\n1.  **Initialization**:\n    - A pseudorandom number generator is seeded with the given `seed` value to ensure reproducibility of the edge permutation.\n    - We generate the complete set of $\\frac{n(n-1)}{2}$ possible undirected edges for a graph of $n$ vertices. This set is then shuffled to create a single, fixed, random permutation of candidate edges that will be processed sequentially.\n    - The DSU structure is initialized for $n$ vertices, with each vertex $i \\in \\{0, 1, \\dots, n-1\\}$ placed in its own component of size $1$.\n    - The initial largest component size is $L(0)=1$ (for $n>0$), and its normalized counterpart is $S(0)=L(0)/n = 1/n$. We initialize a history of these normalized sizes, which we can denote `S_history`, with this first value $S(0)$.\n    - We initialize the metrics to be computed: the maximum jump $J_{\\max}$ is set to $0.0$, and the half-giant-crossing density proxy $p_{1/2}$ is set to $-1.0$ to indicate the threshold has not yet been crossed.\n\n2.  **Iterative Process**: The simulation runs for a total of $T$ time steps, from $t=1$ to $t=T$. In each step $t$:\n    - A window of $h$ candidate edges is drawn sequentially from the globally permuted list. This consumes $h$ edges from the permutation.\n    - The controller evaluates each of the $h$ candidate edges $(u, v)$ in the window. For each edge, it uses the DSU's `find` operation to identify the components of its endpoints, $u$ and $v$.\n    - The selection rule is applied:\n        - If the endpoints $u$ and $v$ are already in the same component (i.e., `find(u) == find(v)`), the edge is considered redundant. Its cost product is treated as $+\\infty$.\n        - If the endpoints are in different components, the cost is the product of their respective component sizes. These sizes are readily available from our DSU structure, which maintains them during `union` operations.\n    - The controller selects the candidate edge that has the minimum finite cost product. If multiple edges share the same minimum product, the one that appeared first in the $h$-edge window is chosen, providing a deterministic tie-breaker.\n    - If a non-redundant edge is selected, a `union` operation is performed on its endpoints, merging their two components. The size of the largest component, $L(t)$, is updated if the newly formed component is larger than the previous maximum. If all $h$ candidates are redundant, no `union` operation that changes the component structure is performed, and thus $L(t) = L(t-1)$.\n    - The normalized size $S(t) = L(t)/n$ is calculated and recorded.\n    - The maximum jump $J_{\\max}$ is updated by taking the maximum of its current value and the jump at the current step, $S(t) - S(t-1)$.\n    - If the half-giant threshold $S(t) \\ge 0.5$ is crossed for the first time (i.e., $p_{1/2}$ is still $-1.0$), the critical edge density proxy is recorded as $p_{1/2} = t/n$.\n\n3.  **Finalization**:\n    - After $T$ steps, the total computational cost, defined as the number of edge evaluations, is computed as $C = h \\cdot T$.\n    - The final tuple of results, $(J_{\\max}, p_{1/2}, C, T)$, is assembled and stored.\n\nThis entire procedure is encapsulated in a Python program. The program iterates through the provided test suite, executes the simulation for each parameter set, and formats the collected results into a single line of output as specified.",
            "answer": "```python\nimport numpy as np\n\nclass DSU:\n    \"\"\"\n    A Disjoint Set Union (DSU) data structure with path compression and\n    union by size. This is used to efficiently track the connected\n    components of the graph.\n    \"\"\"\n    def __init__(self, n):\n        # parent[i] is the parent of element i\n        self.parent = list(range(n))\n        # comp_size[i] is the size of the component if i is a root\n        self.comp_size = [1] * n\n\n    def find(self, i):\n        \"\"\"Finds the representative (root) of the set containing element i.\"\"\"\n        if self.parent[i] == i:\n            return i\n        # Path compression: make the node point directly to the root\n        self.parent[i] = self.find(self.parent[i])\n        return self.parent[i]\n\n    def union(self, i, j):\n        \"\"\"Merges the sets containing elements i and j.\"\"\"\n        root_i = self.find(i)\n        root_j = self.find(j)\n        if root_i != root_j:\n            # Union by size: attach the smaller tree to the root of the larger tree\n            if self.comp_size[root_i]  self.comp_size[root_j]:\n                root_i, root_j = root_j, root_i\n            self.parent[root_j] = root_i\n            self.comp_size[root_i] += self.comp_size[root_j]\n            return self.comp_size[root_i]\n        return self.comp_size[root_i]\n\ndef simulate(n, h, T, seed):\n    \"\"\"\n    Runs a single simulation of the controlled percolation process.\n\n    Args:\n        n (int): Number of vertices.\n        h (int): Lookahead horizon.\n        T (int): Number of edges to add.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        list: A list containing [J_max, p_1/2, C, T].\n    \"\"\"\n    # 1. Initialization\n    rng = np.random.default_rng(seed)\n    dsu = DSU(n)\n    \n    # Generate and permute all possible edges\n    all_edges = []\n    if n > 0:\n        for i in range(n):\n            for j in range(i + 1, n):\n                all_edges.append((i, j))\n    rng.shuffle(all_edges)\n    permuted_edges = all_edges\n    \n    L_current = 1 if n > 0 else 0\n    # S_history stores S(0), S(1), ..., S(T)\n    S_history = [L_current / n if n > 0 else 0.0]\n    \n    J_max = 0.0\n    p_half = -1.0\n    \n    edge_idx = 0\n\n    # 2. Simulation Loop for T time steps\n    for t in range(1, T + 1):\n        candidate_edges = permuted_edges[edge_idx : edge_idx + h]\n        edge_idx += h\n        \n        best_edge = None\n        min_product = float('inf')\n        \n        # Controller's selection logic\n        for u, v in candidate_edges:\n            root_u = dsu.find(u)\n            root_v = dsu.find(v)\n            \n            if root_u != root_v:\n                product = dsu.comp_size[root_u] * dsu.comp_size[root_v]\n                if product  min_product:\n                    min_product = product\n                    best_edge = (u, v)\n\n        S_previous = S_history[-1]\n        \n        # Add the selected edge and update component sizes\n        if best_edge:\n            u_best, v_best = best_edge\n            new_comp_size = dsu.union(u_best, v_best)\n            L_current = max(L_current, new_comp_size)\n\n        # If best_edge is None, L_current does not change\n        \n        S_current = L_current / n\n        S_history.append(S_current)\n        \n        # Update metrics\n        if p_half == -1.0 and S_current >= 0.5:\n            p_half = t / n\n            \n        J_max = max(J_max, S_current - S_previous)\n\n    # 3. Finalization\n    C = h * T\n    \n    return [J_max, p_half, C, T]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, h, T, seed)\n        (200, 1, 200, 7),\n        (200, 4, 200, 7),\n        (200, 8, 200, 7),\n        (30, 4, 80, 3),\n        (50, 20, 50, 11),\n    ]\n\n    results = []\n    for params in test_cases:\n        n, h, T, seed = params\n        result = simulate(n, h, T, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Format: [[J_max1,p_1/2_1,C1,T1],[J_max2,p_1/2_2,C2,T2],...]\n    formatted_results = []\n    for res in results:\n        # res is [J_max, p_half, C, T]\n        formatted_res = f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\"\n        formatted_results.append(formatted_res)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A working simulation is the first step; an efficient one is the goal of a computational scientist. This problem focuses on the practical performance of the Achlioptas process, asking you to analyze its time complexity on a step-by-step basis. By dissecting the algorithm and the role of the Disjoint Set Union (DSU) data structure, you will learn to identify computational bottlenecks and appreciate the optimizations necessary to make large-scale simulations of explosive percolation feasible .",
            "id": "4275775",
            "problem": "Consider simulating explosive percolation on an initially empty, simple undirected graph with $N$ labeled vertices using the Achlioptas best-of-$m$ product rule. At each discrete step, you sample $m$ candidate edges uniformly without replacement from the set of currently absent edges. For each candidate edge $\\{u,v\\}$, you compute the product rule score as the product $s_{u} s_{v}$, where $s_{u}$ and $s_{v}$ are the sizes of the connected components containing $u$ and $v$, respectively. The selected edge is the candidate with minimal score among those that connect two different components; ties are broken arbitrarily, and the selected edge is then added. Connected components and their sizes are maintained via Disjoint Set Union (DSU), also known as union–find, implemented with union by size (or union by rank) and path compression. The size of a component is maintained as an integer stored at its root.\n\nAssume the following standard facts: each DSU find operation with union by size (or union by rank) and path compression runs in amortized time proportional to the inverse Ackermann function $\\alpha(N)$, DSU union given two root identifiers runs in constant time, and reading the size stored at a root is constant time. Arithmetic operations on integers and comparisons are constant time. You may assume that the $m$ candidate edges are independently processed by first computing both endpoint roots via DSU find, then reading component sizes, and finally computing the product rule score in constant time.\n\nDerive, from these primitives and definitions, the exact count of DSU find operations per simulation step in the described implementation, and use it to obtain the leading-order amortized time per step as a closed-form expression in terms of $m$ and $\\alpha(N)$, ignoring lower-order additive contributions from unions and constant-time arithmetic or comparisons. Additionally, briefly justify two implementation-level optimizations that ensure no redundant DSU calls are performed during selection and merging so that the derived bound is achieved.\n\nExpress your final answer as a single closed-form analytic expression in terms of $m$ and $\\alpha(N)$. No rounding is required. No physical units are involved.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the established fields of network science and algorithm analysis, specifically concerning the explosive percolation model and the Disjoint Set Union (DSU) data structure. The problem is well-posed, objective, and self-contained, providing a clear set of definitions and computational cost assumptions from which a unique and meaningful solution can be derived. There are no contradictions, factual errors, or ambiguities that would preclude a rigorous analysis.\n\nThe task is to determine the leading-order amortized time per step of an explosive percolation simulation. A single step involves selecting one edge from $m$ candidates and adding it to the graph. The analysis hinges on counting the number of DSU `find` operations, as these are the dominant cost factor according to the problem statement. A step can be divided into two phases: the selection phase and the merging phase.\n\n**1. Analysis of DSU `find` Operations**\n\n**Selection Phase:**\nAt each step, $m$ candidate edges are sampled. For each candidate edge, denoted $\\{u, v\\}$, the algorithm must compute a score equal to the product of the sizes of the components containing $u$ and $v$, written as $s_u s_v$. Crucially, the selection rule only considers edges that connect two different components. To satisfy these requirements for a single edge $\\{u, v\\}$, the following sub-steps are necessary:\n1.  Determine the root of the component containing vertex $u$. This is achieved by one DSU operation: `find(u)`.\n2.  Determine the root of the component containing vertex $v$. This is achieved by a second DSU operation: `find(v)`.\n3.  Compare the two roots. If they are identical, the edge connects vertices within the same component and is not considered for selection.\n4.  If the roots are different, their respective component sizes, $s_u$ and $s_v$, are read from the data stored at each root. The problem states this is a constant-time operation. The score $s_u s_v$ is then computed.\n\nThe problem specifies that \"the $m$ candidate edges are independently processed by first computing both endpoint roots via DSU find\". This instruction implies a direct loop over the $m$ candidates, where for each candidate, two `find` operations are executed. Therefore, the total number of `find` operations performed during the selection phase is exactly $2m$.\n\n**Merging Phase:**\nAfter iterating through all $m$ candidates, the edge $\\{u^*, v^*\\}$ with the minimal score that connects two distinct components is selected. This edge must then be added to the graph, which corresponds to merging the two components in the DSU data structure. A naive implementation of a `union(u, v)` function would internally call `find(u)` and `find(v)` before performing the merge, resulting in two additional `find` operations.\n\nHowever, the problem requires deriving the complexity for an implementation where \"no redundant DSU calls are performed\" and explicitly states that \"DSU union given two root identifiers runs in constant time\". This points towards an optimized implementation where the root identifiers for the selected edge $\\{u^*, v^*\\}$, which were already computed and are known from the selection phase, are passed directly to the union function. Such a `union_by_root` operation does not require any `find` calls. Consequently, the merging phase contributes $0$ DSU `find` operations.\n\n**Total `find` Operations:**\nCombining the two phases, the total exact count of DSU `find` operations per simulation step in an optimized implementation is the sum of operations from the selection and merging phases: $2m + 0 = 2m$.\n\n**2. Leading-Order Amortized Time**\n\nThe problem provides that a DSU `find` operation with the specified optimizations (union by size/rank and path compression) runs in an amortized time proportional to the inverse Ackermann function, $\\alpha(N)$. Let the time for a single `find` operation be $T_{\\text{find}} = c \\cdot \\alpha(N)$, where $c$ is a constant of proportionality.\n\nThe total amortized time per step is dominated by the $2m$ `find` operations. Other operations, such as the constant-time union operation, arithmetic operations, and comparisons, contribute lower-order additive terms. Ignoring these per the problem's instruction, the leading-order amortized time per step, $T_{\\text{step}}$, is:\n$$T_{\\text{step}} \\propto 2m \\cdot T_{\\text{find}}$$\n$$T_{\\text{step}} \\propto 2m \\cdot \\alpha(N)$$\nThe final expression should be in terms of $m$ and $\\alpha(N)$, so we can write the leading-order time as $2m \\alpha(N)$.\n\n**3. Justification of Optimizations**\n\nThe derived count of $2m$ `find` operations and the corresponding time complexity are achieved through specific implementation choices that eliminate redundant computations. A completely naive implementation would perform $2m+2$ `find` operations. The two key optimizations to reduce this to $2m$ are:\n\n1.  **Caching Roots for the Winning Edge:** During the selection loop, as each candidate edge $\\{u,v\\}$ is evaluated, its roots `root_u` and `root_v` are computed. When an edge is identified as having the minimum score so far, its root identifiers (`root_u`, `root_v`) must be cached along with the vertex identifiers. Simply storing $\\{u, v\\}$ would necessitate re-computing the roots later.\n2.  **Using a Root-Based Union Function:** After the selection loop finishes, the merge operation must be performed using a function variant, say `union_by_root(root_u, root_v)`, that accepts the pre-computed and cached root identifiers of the winning edge. This directly leverages the problem's stated primitive that \"union given two root identifiers runs in constant time\" and avoids the two redundant `find` calls a generic `union(u, v)` function would implicitly make.\n\nThese two optimizations ensure that the merging phase adds no `find` operations to the $2m$ operations required by the selection phase, thereby achieving the derived bound.",
            "answer": "$$\\boxed{2m\\alpha(N)}$$"
        }
    ]
}