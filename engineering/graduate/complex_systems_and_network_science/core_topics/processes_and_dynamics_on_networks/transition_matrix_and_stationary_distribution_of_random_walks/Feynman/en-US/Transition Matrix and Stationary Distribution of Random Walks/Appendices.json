{
    "hands_on_practices": [
        {
            "introduction": "This practice grounds you in a fundamental case: a simple random walk on an undirected graph. For this class of problems, a beautiful and powerful relationship exists between the graph's local structure, specifically the vertex degrees $d(i)$, and the walker's long-term behavior. You will derive the transition matrix $P$ from first principles and then use the concept of reversibility to find the stationary distribution $\\pi$ elegantly, bypassing the need to solve a full system of linear equations.",
            "id": "4312660",
            "problem": "Consider the undirected path graph on $4$ nodes with vertex set $\\{1,2,3,4\\}$ and edge set $\\{(1,2),(2,3),(3,4)\\}$. A discrete-time simple random walk on this graph is defined by the rule that from any node $i$, the walker moves to one of its adjacent neighbors chosen uniformly at random in the next time step. Let $P$ denote the transition probability matrix of this random walk, with entries $P_{ij}$ equal to the probability of moving from node $i$ to node $j$ in one step.\n\nUsing only foundational definitions for Markov chains and random walks on undirected graphs, do the following:\n- Derive the transition probability matrix $P$ from first principles based on the graph topology and the simple random walk rule.\n- Using the definition of a stationary distribution for a finite Markov chain, and by invoking reversibility considerations grounded in the symmetry of undirected edges, compute the stationary distribution $\\pi$ explicitly.\n\nReport only the stationary distribution vector $\\pi$ in exact fractional form. Do not round; no units are required.",
            "solution": "The problem asks for the derivation of the transition matrix $P$ and the stationary distribution $\\pi$ for a simple random walk on a specified path graph. The derivation must be based on first principles and reversibility considerations.\n\nThe graph is an undirected path graph with vertex set $V = \\{1, 2, 3, 4\\}$ and edge set $E = \\{(1,2), (2,3), (3,4)\\}$. A simple random walk on this graph proceeds by moving from a current node $i$ to one of its adjacent neighbors with uniform probability.\n\nFirst, we derive the transition probability matrix $P$. The entries $P_{ij}$ represent the probability of moving from node $i$ to node $j$ in one step. For a simple random walk on an undirected graph, if node $j$ is a neighbor of node $i$, this probability is $P_{ij} = \\frac{1}{d(i)}$, where $d(i)$ is the degree of node $i$ (the number of its neighbors). If $j$ is not a neighbor of $i$ and $i \\neq j$, then $P_{ij} = 0$. The problem implies that the walker always moves to a *neighbor*, so there are no self-loops in the walk dynamics, meaning $P_{ii}=0$ for all $i \\in V$.\n\nLet's calculate the degrees of the vertices:\n- Node $1$: one neighbor ($2$), so $d(1) = 1$.\n- Node $2$: two neighbors ($1$ and $3$), so $d(2) = 2$.\n- Node $3$: two neighbors ($2$ and $4$), so $d(3) = 2$.\n- Node $4$: one neighbor ($3$), so $d(4) = 1$.\n\nUsing these degrees, we construct the transition matrix $P$ row by row:\n- From node $1$: The only neighbor is $2$. So, $P_{12} = \\frac{1}{d(1)} = \\frac{1}{1} = 1$. The other probabilities in this row are $0$.\n- From node $2$: The neighbors are $1$ and $3$. So, $P_{21} = \\frac{1}{d(2)} = \\frac{1}{2}$ and $P_{23} = \\frac{1}{d(2)} = \\frac{1}{2}$.\n- From node $3$: The neighbors are $2$ and $4$. So, $P_{32} = \\frac{1}{d(3)} = \\frac{1}{2}$ and $P_{34} = \\frac{1}{d(3)} = \\frac{1}{2}$.\n- From node $4$: The only neighbor is $3$. So, $P_{43} = \\frac{1}{d(4)} = \\frac{1}{1} = 1$.\n\nThe resulting transition matrix is:\n$$\nP = \\begin{pmatrix}\n0  1  0  0 \\\\\n\\frac{1}{2}  0  \\frac{1}{2}  0 \\\\\n0  \\frac{1}{2}  0  \\frac{1}{2} \\\\\n0  0  1  0\n\\end{pmatrix}\n$$\n\nNext, we compute the stationary distribution $\\pi = \\begin{pmatrix} \\pi_1  \\pi_2  \\pi_3  \\pi_4 \\end{pmatrix}$. A distribution is stationary if it satisfies the equation $\\pi P = \\pi$, subject to the normalization condition $\\sum_{i \\in V} \\pi_i = 1$. For a finite, connected, undirected graph, a unique stationary distribution is guaranteed to exist.\n\nThe problem requires using reversibility arguments. A Markov chain is reversible with respect to a distribution $\\pi$ if the detailed balance condition holds:\n$$ \\pi_i P_{ij} = \\pi_j P_{ji} \\quad \\text{for all } i, j \\in V $$\nA key theorem states that any distribution satisfying detailed balance for an irreducible Markov chain is a stationary distribution. We can prove this by summing the detailed balance equation over all $j$:\n$$ \\sum_{j} \\pi_i P_{ij} = \\sum_{j} \\pi_j P_{ji} $$\nSince $P$ is a stochastic matrix, each row sums to 1, so $\\sum_{j} P_{ij} = 1$. The left-hand side therefore becomes $\\pi_i \\sum_{j} P_{ij} = \\pi_i$. The right-hand side, $\\sum_{j} P_{ji} \\pi_j$, is the definition of the $i$-th component of the vector product $\\pi P$. Thus, the equation simplifies to:\n$$ \\pi_i = (\\pi P)_i $$\nThis holds for all $i$, which is the definition of a stationary distribution, $\\pi = \\pi P$. Thus, if we find a distribution that satisfies detailed balance and is properly normalized, it is the unique stationary distribution.\n\nFor a simple random walk on an undirected graph, there is a fundamental connection between the stationary distribution and the vertex degrees, grounded in the symmetry of the edges. Let $A$ be the adjacency matrix of the graph, where $A_{ij}=1$ if $(i,j)$ is an edge and $0$ otherwise. Since the graph is undirected, $A_{ij}=A_{ji}$. The transition probability is $P_{ij} = \\frac{A_{ij}}{d(i)}$.\n\nLet's test if a distribution proportional to the vertex degrees, $\\pi_i = c \\cdot d(i)$ for some constant $c$, satisfies detailed balance.\n- Left-hand side: $\\pi_i P_{ij} = (c \\cdot d(i)) \\left( \\frac{A_{ij}}{d(i)} \\right) = c \\cdot A_{ij}$.\n- Right-hand side: $\\pi_j P_{ji} = (c \\cdot d(j)) \\left( \\frac{A_{ji}}{d(j)} \\right) = c \\cdot A_{ji}$.\nSince $A_{ij} = A_{ji}$, the two sides are equal: $c \\cdot A_{ij} = c \\cdot A_{ji}$. This holds if there is an edge between $i$ and $j$ ($A_{ij}=A_{ji}=1$) and also if there is no edge ($A_{ij}=A_{ji}=0$). Thus, any distribution where $\\pi_i$ is proportional to $d(i)$ satisfies the detailed balance condition.\n\nTo find the specific stationary distribution, we enforce the normalization condition $\\sum_{i} \\pi_i = 1$.\n$$ \\sum_{i=1}^4 c \\cdot d(i) = 1 \\implies c \\left( \\sum_{i=1}^4 d(i) \\right) = 1 $$\nThe sum of the degrees is $\\sum d(i) = d(1) + d(2) + d(3) + d(4) = 1 + 2 + 2 + 1 = 6$.\nSo, $c \\cdot 6 = 1$, which gives $c = \\frac{1}{6}$.\n\nThe stationary distribution is therefore given by $\\pi_i = \\frac{d(i)}{6}$. We can compute each component:\n- $\\pi_1 = \\frac{d(1)}{6} = \\frac{1}{6}$\n- $\\pi_2 = \\frac{d(2)}{6} = \\frac{2}{6} = \\frac{1}{3}$\n- $\\pi_3 = \\frac{d(3)}{6} = \\frac{2}{6} = \\frac{1}{3}$\n- $\\pi_4 = \\frac{d(4)}{6} = \\frac{1}{6}$\n\nThe stationary distribution vector is $\\pi = \\begin{pmatrix} \\frac{1}{6}  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{6} \\end{pmatrix}$. This result is exact and derived from first principles as requested.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{6}  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{6} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After learning to find the stationary distribution , a deeper question arises: how does a system evolve towards this equilibrium? This practice provides a complete spectral analysis of a transition matrix $P$, revealing the dynamics of convergence. By computing eigenvalues and eigenvectors, you will not only see that the stationary distribution is the left eigenvector for eigenvalue $\\lambda=1$, but also derive the exact form of $P^t$ for large $t$, making the approach to the steady state mathematically explicit.",
            "id": "4312686",
            "problem": "Consider a discrete-time Random Walk (RW) on a directed network with two nodes labeled $1$ and $2$. At each time step, a walker at node $1$ deterministically moves to node $2$, while a walker at node $2$ moves to node $1$ with probability $1/2$ and remains at node $2$ with probability $1/2$. The evolution of the RW is governed by a row-stochastic transition matrix $P$ such that a probability row vector $x(t)$ updates as $x(t+1) = x(t) P$. The transition matrix is\n$$\nP = \\begin{pmatrix}\n0  1 \\\\\n\\frac{1}{2}  \\frac{1}{2}\n\\end{pmatrix}.\n$$\nStarting from foundational definitions in complex systems and network science—specifically, the definition of eigenvalues and eigenvectors of a linear operator, the characterization of a stationary distribution $\\pi$ as a nonnegative row vector satisfying $\\pi P = \\pi$ and $\\pi \\mathbf{1} = 1$ where $\\mathbf{1}$ is the column vector of all ones, and the asymptotic behavior of powers of a primitive (irreducible and aperiodic) stochastic matrix—derive the following for the given $P$:\n\n- The eigenvalues of $P$.\n- A basis of right eigenvectors of $P$ corresponding to those eigenvalues (choose any convenient nonzero representatives).\n- The unique stationary distribution $\\pi$ of the RW.\n- The asymptotic form of $P^{t}$ as $t \\to \\infty$ expressed in closed form, exhibiting the dominant rank-one term and the leading decaying correction.\n\nProvide exact results (no rounding). Express your final answer as a single row matrix with four entries, in the following order: the pair of eigenvalues; a $2 \\times 2$ matrix whose columns are the corresponding right eigenvectors; the stationary distribution as a $1 \\times 2$ row vector; and the asymptotic form of $P^{t}$ as a $2 \\times 2$ matrix written as the sum of a rank-one limit and a term proportional to $(-\\frac{1}{2})^{t}$.",
            "solution": "The problem is well-defined and requires the application of standard methods from the theory of Markov chains and linear algebra. The first step is to analyze the spectral properties of the given transition matrix $P$.\n\nThe eigenvalues $\\lambda$ of $P$ are the roots of the characteristic equation $\\det(P - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\n\\det \\begin{pmatrix} -\\lambda  1 \\\\ \\frac{1}{2}  \\frac{1}{2} - \\lambda \\end{pmatrix} = (-\\lambda)\\left(\\frac{1}{2} - \\lambda\\right) - (1)\\left(\\frac{1}{2}\\right) = 0\n$$\nThis simplifies to the quadratic equation:\n$$\n\\lambda^2 - \\frac{1}{2}\\lambda - \\frac{1}{2} = 0\n$$\nMultiplying by $2$ to clear the fractions gives $2\\lambda^2 - \\lambda - 1 = 0$. Factoring the quadratic polynomial yields:\n$$\n(2\\lambda + 1)(\\lambda - 1) = 0\n$$\nThe eigenvalues are therefore $\\lambda_1 = 1$ and $\\lambda_2 = -\\frac{1}{2}$. As dictated by the Perron-Frobenius theorem for stochastic matrices, the eigenvalue with the largest magnitude is $1$.\n\nNext, we find a basis of right eigenvectors. A right eigenvector $\\mathbf{v}$ corresponding to an eigenvalue $\\lambda$ satisfies the equation $(P - \\lambda I)\\mathbf{v} = \\mathbf{0}$.\nFor the eigenvalue $\\lambda_1 = 1$:\n$$\n(P - 1 \\cdot I)\\mathbf{v}_1 = \\begin{pmatrix} -1  1 \\\\ \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the equation $-x + y = 0$, which implies $x=y$. A convenient non-zero choice for the eigenvector is $\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nFor the eigenvalue $\\lambda_2 = -\\frac{1}{2}$:\n$$\n\\left(P - \\left(-\\frac{1}{2}\\right)I\\right)\\mathbf{v}_2 = \\left(P + \\frac{1}{2}I\\right)\\mathbf{v}_2 = \\begin{pmatrix} \\frac{1}{2}  1 \\\\ \\frac{1}{2}  1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the equation $\\frac{1}{2}x + y = 0$, or $x = -2y$. A convenient non-zero choice for the eigenvector is $\\mathbf{v}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$.\nA basis of right eigenvectors is thus given by the columns of the matrix $\\begin{pmatrix} \\mathbf{v}_1  \\mathbf{v}_2 \\end{pmatrix} = \\begin{pmatrix} 1  2 \\\\ 1  -1 \\end{pmatrix}$.\n\nThe unique stationary distribution $\\pi = \\begin{pmatrix} \\pi_1  \\pi_2 \\end{pmatrix}$ is the unique non-negative left eigenvector of $P$ corresponding to the eigenvalue $\\lambda_1 = 1$, normalized such that its components sum to $1$. It satisfies $\\pi P = \\pi$ and $\\pi_1 + \\pi_2 = 1$. The equation $\\pi P = \\pi$ is:\n$$\n\\begin{pmatrix} \\pi_1  \\pi_2 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\pi_1  \\pi_2 \\end{pmatrix}\n$$\nThis expands to the system of equations $\\frac{1}{2}\\pi_2 = \\pi_1$ and $\\pi_1 + \\frac{1}{2}\\pi_2 = \\pi_2$. Both equations are equivalent to $\\pi_2 = 2\\pi_1$. Using the normalization condition $\\pi_1 + \\pi_2 = 1$, we substitute for $\\pi_2$:\n$$\n\\pi_1 + 2\\pi_1 = 1 \\implies 3\\pi_1 = 1 \\implies \\pi_1 = \\frac{1}{3}\n$$\nConsequently, $\\pi_2 = 2(\\frac{1}{3}) = \\frac{2}{3}$. The stationary distribution is $\\pi = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$.\n\nFinally, to find the asymptotic form of $P^t$, we use the spectral decomposition of $P$. Since the given matrix $P$ is diagonalizable, we can write its powers as $P^t = \\sum_{i=1}^{2} \\lambda_i^t \\mathbf{v}_i \\mathbf{u}_i$, where $\\mathbf{v}_i$ are the right eigenvectors (as column vectors) and $\\mathbf{u}_i$ are the corresponding left eigenvectors (as row vectors), normalized such that $\\mathbf{u}_i \\mathbf{v}_j = \\delta_{ij}$. The left eigenvectors can be found as the rows of the inverse of the right eigenvector matrix $V = \\begin{pmatrix} \\mathbf{v}_1  \\mathbf{v}_2 \\end{pmatrix}$.\n$$\nV = \\begin{pmatrix} 1  2 \\\\ 1  -1 \\end{pmatrix} \\implies V^{-1} = \\frac{1}{(1)(-1)-(2)(1)} \\begin{pmatrix} -1  -2 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\\\ \\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix}\n$$\nThe left eigenvectors are $\\mathbf{u}_1 = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$ (which is the stationary distribution $\\pi$) and $\\mathbf{u}_2 = \\begin{pmatrix} \\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix}$.\n\nThe first term in the spectral expansion corresponds to $\\lambda_1 = 1$:\n$$\n\\lambda_1^t \\mathbf{v}_1 \\mathbf{u}_1 = 1^t \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\\\ \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}\n$$\nThis is the dominant, rank-one term, which is the limit of $P^t$ as $t \\to \\infty$. Each row of this matrix is the stationary distribution $\\pi$.\n\nThe second term corresponds to the sub-dominant eigenvalue $\\lambda_2 = -\\frac{1}{2}$:\n$$\n\\lambda_2^t \\mathbf{v}_2 \\mathbf{u}_2 = \\left(-\\frac{1}{2}\\right)^t \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix} = \\left(-\\frac{1}{2}\\right)^t \\begin{pmatrix} \\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{1}{3}  \\frac{1}{3} \\end{pmatrix}\n$$\nThis is the leading decaying correction term, which governs the transient behavior.\nThe complete closed-form expression for $P^t$ is the sum of these two terms:\n$$\nP^t = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\\\ \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} + \\left(-\\frac{1}{2}\\right)^t \\begin{pmatrix} \\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{1}{3}  \\frac{1}{3} \\end{pmatrix}\n$$\nThis expression exhibits the required asymptotic form, converging to the rank-one limit as the correction term decays to zero for $t \\to \\infty$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} 1  -\\frac{1}{2} \\end{pmatrix}  \\begin{pmatrix} 1  2 \\\\ 1  -1 \\end{pmatrix}  \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}  \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\\\ \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} + \\left(-\\frac{1}{2}\\right)^{t} \\begin{pmatrix} \\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{1}{3}  \\frac{1}{3} \\end{pmatrix} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real-world networks are often directed and may contain structural traps or nodes with no way out. This exercise introduces a \"teleportation\" mechanism, a core idea behind Google's PageRank algorithm, which ensures a well-behaved random walk on any graph structure. You will construct a transition matrix for this modified process on a directed graph and compute its unique stationary distribution $\\pi$ by setting up and solving the defining system of linear equations, $\\pi = \\pi P_{\\alpha}$. This practice bridges fundamental theory with the robust methods needed for complex, practical applications.",
            "id": "4312640",
            "problem": "Consider the directed graph on three nodes with adjacency matrix $A=\\begin{pmatrix}010\\\\001\\\\100\\end{pmatrix}$. A discrete-time random walk on a directed graph is defined by the rule that, at each time step, the walker located at a node chooses uniformly at random among that node’s outgoing neighbors and moves along the chosen edge. The corresponding transition matrix $P$ is row-stochastic, meaning each row sums to $1$.\n\n(a) Using only the foundational definition above, construct the row-stochastic transition matrix $P$ for this graph and verify that each row of $P$ sums to $1$.\n\n(b) Now consider a modified process: with probability $\\alpha=\\frac{3}{5}$ the walker follows one step of the random walk with transition matrix $P$, and with probability $1-\\alpha=\\frac{2}{5}$ the walker “teleports” to a node sampled independently from a fixed preference distribution $q=\\begin{pmatrix}\\frac{1}{2}\\frac{1}{3}\\frac{1}{6}\\end{pmatrix}$. The resulting transition matrix is row-stochastic. Using only the core definitions of a Markov chain and stationary distribution (that a stationary distribution $\\pi$ satisfies $\\pi=\\pi P_{\\alpha}$ and $\\sum_{i}\\pi_{i}=1$), derive and compute the stationary probability of node $1$ for this modified chain.\n\nProvide your final answer as the exact value of $\\pi_{1}$ in lowest terms. Do not round.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and self-contained. It presents a standard, solvable problem in the study of Markov chains on graphs.\n\nPart (a): Construction and Verification of the Transition Matrix $P$\n\nThe problem specifies a discrete-time random walk on a directed graph defined by the adjacency matrix $A=\\begin{pmatrix}010\\\\001\\\\100\\end{pmatrix}$. An entry $A_{ij}=1$ indicates a directed edge from node $i$ to node $j$. The rule for the random walk is that a walker at a node $i$ moves to one of its outgoing neighbors, chosen uniformly at random.\n\nThe out-degree of a node $i$, denoted $k_i^{\\text{out}}$, is the number of outgoing edges from that node. It can be computed by summing the entries of the $i$-th row of the adjacency matrix: $k_i^{\\text{out}} = \\sum_{j} A_{ij}$.\nFor the given matrix $A$:\n-   Out-degree of node $1$: $k_1^{\\text{out}} = A_{11} + A_{12} + A_{13} = 0 + 1 + 0 = 1$.\n-   Out-degree of node $2$: $k_2^{\\text{out}} = A_{21} + A_{22} + A_{23} = 0 + 0 + 1 = 1$.\n-   Out-degree of node $3$: $k_3^{\\text{out}} = A_{31} + A_{32} + A_{33} = 1 + 0 + 0 = 1$.\n\nThe transition probability $P_{ij}$ from node $i$ to node $j$ is the probability of moving to neighbor $j$ from node $i$. Since the choice is uniform among the $k_i^{\\text{out}}$ outgoing neighbors, $P_{ij} = \\frac{1}{k_i^{\\text{out}}}$ if an edge from $i$ to $j$ exists (i.e., $A_{ij}=1$), and $P_{ij}=0$ otherwise. This can be written compactly as $P_{ij} = \\frac{A_{ij}}{k_i^{\\text{out}}}$.\n\nUsing this definition, we construct the transition matrix $P$:\n-   Row $1$ (from node $1$): $P_{11}=0$, $P_{12}=\\frac{1}{1}=1$, $P_{13}=0$.\n-   Row $2$ (from node $2$): $P_{21}=0$, $P_{22}=0$, $P_{23}=\\frac{1}{1}=1$.\n-   Row $3$ (from node $3$): $P_{31}=\\frac{1}{1}=1$, $P_{32}=0$, $P_{33}=0$.\n\nThus, the transition matrix is $P = \\begin{pmatrix}010\\\\001\\\\100\\end{pmatrix}$.\n\nTo verify that $P$ is row-stochastic, we confirm that each row sums to $1$:\n-   Sum of row $1$: $0+1+0=1$.\n-   Sum of row $2$: $0+0+1=1$.\n-   Sum of row $3$: $1+0+0=1$.\nThe condition is satisfied for all rows.\n\nPart (b): Stationary Distribution of the Modified Chain\n\nThe modified Markov chain has a transition matrix $P_{\\alpha}$ defined by a two-part process. With probability $\\alpha = \\frac{3}{5}$, the walker follows the random walk dynamics of $P$. With probability $1-\\alpha = \\frac{2}{5}$, the walker teleports to a node chosen from the distribution $q=\\begin{pmatrix}\\frac{1}{2}\\frac{1}{3}\\frac{1}{6}\\end{pmatrix}$.\n\nThe transition probability from node $i$ to node $j$ in the modified process, $(P_{\\alpha})_{ij}$, is given by the law of total probability:\n$(P_{\\alpha})_{ij} = \\alpha \\cdot P_{ij} + (1-\\alpha) \\cdot q_j$.\n\nLet $J$ be the teleportation matrix, where each row is the vector $q$:\n$J = \\begin{pmatrix} 1/2  1/3  1/6 \\\\ 1/2  1/3  1/6 \\\\ 1/2  1/3  1/6 \\end{pmatrix}$.\nThe modified transition matrix is $P_{\\alpha} = \\alpha P + (1-\\alpha) J$.\nSubstituting the values:\n$P_{\\alpha} = \\frac{3}{5} \\begin{pmatrix}010\\\\001\\\\100\\end{pmatrix} + \\frac{2}{5} \\begin{pmatrix} 1/2  1/3  1/6 \\\\ 1/2  1/3  1/6 \\\\ 1/2  1/3  1/6 \\end{pmatrix}$\n$P_{\\alpha} = \\begin{pmatrix}0  3/5  0 \\\\ 0  0  3/5 \\\\ 3/5  0  0 \\end{pmatrix} + \\begin{pmatrix} 1/5  2/15  1/15 \\\\ 1/5  2/15  1/15 \\\\ 1/5  2/15  1/15 \\end{pmatrix}$\n$P_{\\alpha} = \\begin{pmatrix}0+\\frac{1}{5}  \\frac{3}{5}+\\frac{2}{15}  0+\\frac{1}{15} \\\\ 0+\\frac{1}{5}  0+\\frac{2}{15}  \\frac{3}{5}+\\frac{1}{15} \\\\ \\frac{3}{5}+\\frac{1}{5}  0+\\frac{2}{15}  0+\\frac{1}{15} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5}  \\frac{11}{15}  \\frac{1}{15} \\\\ \\frac{1}{5}  \\frac{2}{15}  \\frac{10}{15} \\\\ \\frac{4}{5}  \\frac{2}{15}  \\frac{1}{15} \\end{pmatrix}$\nFor calculational convenience, we write all entries with a common denominator of $15$:\n$P_{\\alpha} = \\frac{1}{15} \\begin{pmatrix} 3  11  1 \\\\ 3  2  10 \\\\ 12  2  1 \\end{pmatrix}$.\n\nA stationary distribution $\\pi = \\begin{pmatrix}\\pi_1  \\pi_2  \\pi_3\\end{pmatrix}$ is a row vector that satisfies the core definitions $\\pi = \\pi P_{\\alpha}$ and $\\sum_{i=1}^3 \\pi_i = 1$. The equation $\\pi = \\pi P_{\\alpha}$ yields the following system of linear equations:\n1. $\\pi_1 = \\frac{3}{15}\\pi_1 + \\frac{3}{15}\\pi_2 + \\frac{12}{15}\\pi_3$\n2. $\\pi_2 = \\frac{11}{15}\\pi_1 + \\frac{2}{15}\\pi_2 + \\frac{2}{15}\\pi_3$\n3. $\\pi_3 = \\frac{1}{15}\\pi_1 + \\frac{10}{15}\\pi_2 + \\frac{1}{15}\\pi_3$\n\nMultiplying by $15$ and simplifying gives:\n1. $15\\pi_1 = 3\\pi_1 + 3\\pi_2 + 12\\pi_3 \\implies 12\\pi_1 = 3\\pi_2 + 12\\pi_3 \\implies 4\\pi_1 = \\pi_2 + 4\\pi_3$\n2. $15\\pi_2 = 11\\pi_1 + 2\\pi_2 + 2\\pi_3 \\implies 13\\pi_2 = 11\\pi_1 + 2\\pi_3$\n3. $15\\pi_3 = \\pi_1 + 10\\pi_2 + \\pi_3 \\implies 14\\pi_3 = \\pi_1 + 10\\pi_2$\n\nThis system is linearly dependent. We will use equations (1) and (3) together with the normalization condition $\\pi_1 + \\pi_2 + \\pi_3 = 1$.\nFrom equation (1), we express $\\pi_2$ in terms of $\\pi_1$ and $\\pi_3$:\n$\\pi_2 = 4\\pi_1 - 4\\pi_3$.\n\nSubstitute this expression for $\\pi_2$ into equation (3):\n$14\\pi_3 = \\pi_1 + 10(4\\pi_1 - 4\\pi_3)$\n$14\\pi_3 = \\pi_1 + 40\\pi_1 - 40\\pi_3$\n$14\\pi_3 = 41\\pi_1 - 40\\pi_3$\n$54\\pi_3 = 41\\pi_1 \\implies \\pi_3 = \\frac{41}{54}\\pi_1$.\n\nNow, substitute this result for $\\pi_3$ back into the expression for $\\pi_2$:\n$\\pi_2 = 4\\pi_1 - 4\\left(\\frac{41}{54}\\pi_1\\right) = 4\\pi_1 - \\frac{164}{54}\\pi_1 = \\left(\\frac{216 - 164}{54}\\right)\\pi_1 = \\frac{52}{54}\\pi_1 = \\frac{26}{27}\\pi_1$.\n\nWe have now expressed both $\\pi_2$ and $\\pi_3$ as multiples of $\\pi_1$. We use the normalization condition to find $\\pi_1$:\n$\\pi_1 + \\pi_2 + \\pi_3 = 1$\n$\\pi_1 + \\frac{26}{27}\\pi_1 + \\frac{41}{54}\\pi_1 = 1$\nTo combine the terms, we find a common denominator, which is $54$:\n$\\frac{54}{54}\\pi_1 + \\frac{52}{54}\\pi_1 + \\frac{41}{54}\\pi_1 = 1$\n$\\left(\\frac{54 + 52 + 41}{54}\\right)\\pi_1 = 1$\n$\\frac{147}{54}\\pi_1 = 1$\n$\\pi_1 = \\frac{54}{147}$.\n\nThe problem requires the answer in lowest terms. We find the greatest common divisor of the numerator and the denominator. The prime factorization of $54$ is $2 \\times 3^3$. The sum of the digits of $147$ is $1+4+7=12$, which is divisible by $3$. $147 = 3 \\times 49 = 3 \\times 7^2$. The greatest common divisor is $3$.\n$\\pi_1 = \\frac{54 \\div 3}{147 \\div 3} = \\frac{18}{49}$.\nThe stationary probability of node $1$ is $\\frac{18}{49}$.",
            "answer": "$$\\boxed{\\frac{18}{49}}$$"
        }
    ]
}