{
    "hands_on_practices": [
        {
            "introduction": "Understanding the stationary distribution of a random walk often begins with the most intuitive case: undirected graphs. For these symmetric systems, there is a direct and elegant relationship between a node's structural importance, as measured by its degree, and the long-term probability of finding a walker there. This practice challenges you to derive this fundamental result from first principles, solidifying the connection between local topology and global system behavior. ",
            "id": "4312660",
            "problem": "Consider the undirected path graph on $4$ nodes with vertex set $\\{1,2,3,4\\}$ and edge set $\\{(1,2),(2,3),(3,4)\\}$. A discrete-time simple random walk on this graph is defined by the rule that from any node $i$, the walker moves to one of its adjacent neighbors chosen uniformly at random in the next time step. Let $P$ denote the transition probability matrix of this random walk, with entries $P_{ij}$ equal to the probability of moving from node $i$ to node $j$ in one step.\n\nUsing only foundational definitions for Markov chains and random walks on undirected graphs, do the following:\n- Derive the transition probability matrix $P$ from first principles based on the graph topology and the simple random walk rule.\n- Using the definition of a stationary distribution for a finite Markov chain, and by invoking reversibility considerations grounded in the symmetry of undirected edges, compute the stationary distribution $\\pi$ explicitly.\n\nReport only the stationary distribution vector $\\pi$ in exact fractional form. Do not round; no units are required.",
            "solution": "The problem asks for the derivation of the transition matrix $P$ and the stationary distribution $\\pi$ for a simple random walk on a specified path graph. The derivation must be based on first principles and reversibility considerations.\n\nThe graph is an undirected path graph with vertex set $V = \\{1, 2, 3, 4\\}$ and edge set $E = \\{(1,2), (2,3), (3,4)\\}$. A simple random walk on this graph proceeds by moving from a current node $i$ to one of its adjacent neighbors with uniform probability.\n\nFirst, we derive the transition probability matrix $P$. The entries $P_{ij}$ represent the probability of moving from node $i$ to node $j$ in one step. For a simple random walk on an undirected graph, if node $j$ is a neighbor of node $i$, this probability is $P_{ij} = \\frac{1}{d(i)}$, where $d(i)$ is the degree of node $i$ (the number of its neighbors). If $j$ is not a neighbor of $i$ and $i \\neq j$, then $P_{ij} = 0$. The problem implies that the walker always moves to a *neighbor*, so there are no self-loops in the walk dynamics, meaning $P_{ii}=0$ for all $i \\in V$.\n\nLet's calculate the degrees of the vertices:\n- Node $1$: one neighbor ($2$), so $d(1) = 1$.\n- Node $2$: two neighbors ($1$ and $3$), so $d(2) = 2$.\n- Node $3$: two neighbors ($2$ and $4$), so $d(3) = 2$.\n- Node $4$: one neighbor ($3$), so $d(4) = 1$.\n\nUsing these degrees, we construct the transition matrix $P$ row by row:\n- From node $1$: The only neighbor is $2$. So, $P_{12} = \\frac{1}{d(1)} = \\frac{1}{1} = 1$. The other probabilities in this row are $0$.\n- From node $2$: The neighbors are $1$ and $3$. So, $P_{21} = \\frac{1}{d(2)} = \\frac{1}{2}$ and $P_{23} = \\frac{1}{d(2)} = \\frac{1}{2}$.\n- From node $3$: The neighbors are $2$ and $4$. So, $P_{32} = \\frac{1}{d(3)} = \\frac{1}{2}$ and $P_{34} = \\frac{1}{d(3)} = \\frac{1}{2}$.\n- From node $4$: The only neighbor is $3$. So, $P_{43} = \\frac{1}{d(4)} = \\frac{1}{1} = 1$.\n\nThe resulting transition matrix is:\n$$\nP = \\begin{pmatrix}\n0  1  0  0 \\\\\n\\frac{1}{2}  0  \\frac{1}{2}  0 \\\\\n0  \\frac{1}{2}  0  \\frac{1}{2} \\\\\n0  0  1  0\n\\end{pmatrix}\n$$\n\nNext, we compute the stationary distribution $\\pi = \\begin{pmatrix} \\pi_1  \\pi_2  \\pi_3  \\pi_4 \\end{pmatrix}$. A distribution is stationary if it satisfies the equation $\\pi P = \\pi$, subject to the normalization condition $\\sum_{i \\in V} \\pi_i = 1$. For a finite, connected, undirected graph, a unique stationary distribution is guaranteed to exist.\n\nThe problem requires using reversibility arguments. A Markov chain is reversible with respect to a distribution $\\pi$ if the detailed balance condition holds:\n$$ \\pi_i P_{ij} = \\pi_j P_{ji} \\quad \\text{for all } i, j \\in V $$\nA key theorem states that any distribution satisfying detailed balance for an irreducible Markov chain is a stationary distribution. We can prove this by summing the detailed balance equation over $i$:\n$$ \\sum_{i} \\pi_i P_{ij} = \\sum_{i} \\pi_j P_{ji} = \\pi_j \\sum_{i} P_{ji} $$\nThe left side is the $j$-th component of the vector product $\\pi P$. Let's re-examine the proof. Summing over $j$ is more direct:\n$$ \\sum_{j} \\pi_i P_{ij} = \\sum_{j} \\pi_j P_{ji} $$\n$$ \\pi_i \\sum_{j} P_{ij} = \\sum_{j} \\pi_j P_{ji} $$\nSince $P$ is a stochastic matrix, each row sums to $1$, so $\\sum_{j} P_{ij} = 1$. The equation becomes:\n$$ \\pi_i = \\sum_{j} \\pi_j P_{ji} = (\\pi P)_i $$\nThis proves that if $\\pi$ satisfies detailed balance, then $\\pi = \\pi P$. Thus, if we find a distribution that satisfies detailed balance and is properly normalized, it is the unique stationary distribution.\n\nFor a simple random walk on an undirected graph, there is a fundamental connection between the stationary distribution and the vertex degrees, grounded in the symmetry of the edges. Let $A$ be the adjacency matrix of the graph, where $A_{ij}=1$ if $(i,j)$ is an edge and $0$ otherwise. Since the graph is undirected, $A_{ij}=A_{ji}$. The transition probability is $P_{ij} = \\frac{A_{ij}}{d(i)}$.\n\nLet's test if a distribution proportional to the vertex degrees, $\\pi_i = c \\cdot d(i)$ for some constant $c$, satisfies detailed balance.\n- Left-hand side: $\\pi_i P_{ij} = (c \\cdot d(i)) \\left( \\frac{A_{ij}}{d(i)} \\right) = c \\cdot A_{ij}$.\n- Right-hand side: $\\pi_j P_{ji} = (c \\cdot d(j)) \\left( \\frac{A_{ji}}{d(j)} \\right) = c \\cdot A_{ji}$.\nSince $A_{ij} = A_{ji}$, the two sides are equal: $c \\cdot A_{ij} = c \\cdot A_{ji}$. This holds if there is an edge between $i$ and $j$ ($A_{ij}=A_{ji}=1$) and also if there is no edge ($A_{ij}=A_{ji}=0$). Thus, any distribution where $\\pi_i$ is proportional to $d(i)$ satisfies the detailed balance condition.\n\nTo find the specific stationary distribution, we enforce the normalization condition $\\sum_{i} \\pi_i = 1$.\n$$ \\sum_{i=1}^4 c \\cdot d(i) = 1 \\implies c \\left( \\sum_{i=1}^4 d(i) \\right) = 1 $$\nThe sum of the degrees is $\\sum d(i) = d(1) + d(2) + d(3) + d(4) = 1 + 2 + 2 + 1 = 6$.\nSo, $c \\cdot 6 = 1$, which gives $c = \\frac{1}{6}$.\n\nThe stationary distribution is therefore given by $\\pi_i = \\frac{d(i)}{6}$. We can compute each component:\n- $\\pi_1 = \\frac{d(1)}{6} = \\frac{1}{6}$\n- $\\pi_2 = \\frac{d(2)}{6} = \\frac{2}{6} = \\frac{1}{3}$\n- $\\pi_3 = \\frac{d(3)}{6} = \\frac{2}{6} = \\frac{1}{3}$\n- $\\pi_4 = \\frac{d(4)}{6} = \\frac{1}{6}$\n\nThe stationary distribution vector is $\\pi = \\begin{pmatrix} \\frac{1}{6}  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{6} \\end{pmatrix}$. This result is exact and derived from first principles as requested.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{6}  \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{6} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond simply calculating the stationary distribution, a deeper understanding comes from analyzing the dynamics of convergence. How does a system approach its long-term equilibrium, and at what rate? This exercise delves into the spectral properties of the transition matrix, showing how its eigenvalues and eigenvectors govern the system's evolution over time and dictate the approach to the stationary state. ",
            "id": "4312686",
            "problem": "Consider a discrete-time Random Walk (RW) on a directed network with two nodes labeled $1$ and $2$. At each time step, a walker at node $1$ deterministically moves to node $2$, while a walker at node $2$ moves to node $1$ with probability $1/2$ and remains at node $2$ with probability $1/2$. The evolution of the RW is governed by a row-stochastic transition matrix $P$ such that a probability row vector $x(t)$ updates as $x(t+1) = x(t) P$. The transition matrix is\n$$\nP = \\begin{pmatrix}\n0  1 \\\\\n\\frac{1}{2}  \\frac{1}{2}\n\\end{pmatrix}.\n$$\nStarting from foundational definitions in complex systems and network science—specifically, the definition of eigenvalues and eigenvectors of a linear operator, the characterization of a stationary distribution $\\pi$ as a nonnegative row vector satisfying $\\pi P = \\pi$ and $\\pi \\mathbf{1} = 1$ where $\\mathbf{1}$ is the column vector of all ones, and the asymptotic behavior of powers of a primitive (irreducible and aperiodic) stochastic matrix—derive the following for the given $P$:\n\n- The eigenvalues of $P$.\n- A basis of right eigenvectors of $P$ corresponding to those eigenvalues (choose any convenient nonzero representatives).\n- The unique stationary distribution $\\pi$ of the RW.\n- The asymptotic form of $P^{t}$ as $t \\to \\infty$ expressed in closed form, exhibiting the dominant rank-one term and the leading decaying correction.\n\nProvide exact results (no rounding). Express your final answer as a single row matrix with four entries, in the following order: the pair of eigenvalues; a $2 \\times 2$ matrix whose columns are the corresponding right eigenvectors; the stationary distribution as a $1 \\times 2$ row vector; and the asymptotic form of $P^{t}$ as a $2 \\times 2$ matrix written as the sum of a rank-one limit and a term proportional to $(-\\frac{1}{2})^{t}$.",
            "solution": "The problem is well-defined and requires the application of standard methods from the theory of Markov chains and linear algebra. The first step is to analyze the spectral properties of the given transition matrix $P$.\n\nThe eigenvalues $\\lambda$ of $P$ are the roots of the characteristic equation $\\det(P - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\n\\det \\begin{pmatrix} -\\lambda  1 \\\\ \\frac{1}{2}  \\frac{1}{2} - \\lambda \\end{pmatrix} = (-\\lambda)\\left(\\frac{1}{2} - \\lambda\\right) - (1)\\left(\\frac{1}{2}\\right) = 0\n$$\nThis simplifies to the quadratic equation:\n$$\n\\lambda^2 - \\frac{1}{2}\\lambda - \\frac{1}{2} = 0\n$$\nMultiplying by $2$ to clear the fractions gives $2\\lambda^2 - \\lambda - 1 = 0$. Factoring the quadratic polynomial yields:\n$$\n(2\\lambda + 1)(\\lambda - 1) = 0\n$$\nThe eigenvalues are therefore $\\lambda_1 = 1$ and $\\lambda_2 = -\\frac{1}{2}$. As dictated by the Perron-Frobenius theorem for stochastic matrices, the eigenvalue with the largest magnitude is $1$.\n\nNext, we find a basis of right eigenvectors. A right eigenvector $\\mathbf{v}$ corresponding to an eigenvalue $\\lambda$ satisfies the equation $(P - \\lambda I)\\mathbf{v} = \\mathbf{0}$.\nFor the eigenvalue $\\lambda_1 = 1$:\n$$\n(P - 1 \\cdot I)\\mathbf{v}_1 = \\begin{pmatrix} -1  1 \\\\ \\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the equation $-x + y = 0$, which implies $x=y$. A convenient non-zero choice for the eigenvector is $\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nFor the eigenvalue $\\lambda_2 = -\\frac{1}{2}$:\n$$\n\\left(P - \\left(-\\frac{1}{2}\\right)I\\right)\\mathbf{v}_2 = \\left(P + \\frac{1}{2}I\\right)\\mathbf{v}_2 = \\begin{pmatrix} \\frac{1}{2}  1 \\\\ \\frac{1}{2}  1 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the equation $\\frac{1}{2}x + y = 0$, or $x = -2y$. A convenient non-zero choice for the eigenvector is $\\mathbf{v}_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$.\nA basis of right eigenvectors is thus given by the columns of the matrix $\\begin{pmatrix} \\mathbf{v}_1  \\mathbf{v}_2 \\end{pmatrix} = \\begin{pmatrix} 1  2 \\\\ 1  -1 \\end{pmatrix}$.\n\nThe unique stationary distribution $\\pi = \\begin{pmatrix} \\pi_1  \\pi_2 \\end{pmatrix}$ is the unique non-negative left eigenvector of $P$ corresponding to the eigenvalue $\\lambda_1 = 1$, normalized such that its components sum to $1$. It satisfies $\\pi P = \\pi$ and $\\pi_1 + \\pi_2 = 1$. The equation $\\pi P = \\pi$ is:\n$$\n\\begin{pmatrix} \\pi_1  \\pi_2 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\pi_1  \\pi_2 \\end{pmatrix}\n$$\nThis expands to the system of equations $\\frac{1}{2}\\pi_2 = \\pi_1$ and $\\pi_1 + \\frac{1}{2}\\pi_2 = \\pi_2$. Both equations are equivalent to $\\pi_2 = 2\\pi_1$. Using the normalization condition $\\pi_1 + \\pi_2 = 1$, we substitute for $\\pi_2$:\n$$\n\\pi_1 + 2\\pi_1 = 1 \\implies 3\\pi_1 = 1 \\implies \\pi_1 = \\frac{1}{3}\n$$\nConsequently, $\\pi_2 = 2(\\frac{1}{3}) = \\frac{2}{3}$. The stationary distribution is $\\pi = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$.\n\nFinally, to find the asymptotic form of $P^t$, we use the spectral decomposition of $P$. Since the given matrix $P$ is diagonalizable, we can write its powers as $P^t = \\sum_{i=1}^{2} \\lambda_i^t \\mathbf{v}_i \\mathbf{u}_i$, where $\\mathbf{v}_i$ are the right eigenvectors (as column vectors) and $\\mathbf{u}_i$ are the corresponding left eigenvectors (as row vectors), normalized such that $\\mathbf{u}_i \\mathbf{v}_j = \\delta_{ij}$. The left eigenvectors can be found as the rows of the inverse of the right eigenvector matrix $V = \\begin{pmatrix} \\mathbf{v}_1  \\mathbf{v}_2 \\end{pmatrix}$.\n$$\nV = \\begin{pmatrix} 1  2 \\\\ 1  -1 \\end{pmatrix} \\implies V^{-1} = \\frac{1}{(1)(-1)-(2)(1)} \\begin{pmatrix} -1  -2 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\\\ \\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix}\n$$\nThe left eigenvectors are $\\mathbf{u}_1 = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$ (which is the stationary distribution $\\pi$) and $\\mathbf{u}_2 = \\begin{pmatrix} \\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix}$.\n\nThe first term in the spectral expansion corresponds to $\\lambda_1 = 1$:\n$$\n\\lambda_1^t \\mathbf{v}_1 \\mathbf{u}_1 = 1^t \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\\\ \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}\n$$\nThis is the dominant, rank-one term, which is the limit of $P^t$ as $t \\to \\infty$. Each row of this matrix is the stationary distribution $\\pi$.\n\nThe second term corresponds to the sub-dominant eigenvalue $\\lambda_2 = -\\frac{1}{2}$:\n$$\n\\lambda_2^t \\mathbf{v}_2 \\mathbf{u}_2 = \\left(-\\frac{1}{2}\\right)^{t} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix} = \\left(-\\frac{1}{2}\\right)^{t} \\begin{pmatrix} \\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{1}{3}  \\frac{1}{3} \\end{pmatrix}\n$$\nThis is the leading decaying correction term, which governs the transient behavior.\nThe complete closed-form expression for $P^t$ is the sum of these two terms:\n$$\nP^t = \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\\\ \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} + \\left(-\\frac{1}{2}\\right)^{t} \\begin{pmatrix} \\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{1}{3}  \\frac{1}{3} \\end{pmatrix}\n$$\nThis expression exhibits the required asymptotic form, converging to the rank-one limit as the correction term decays to zero for $t \\to \\infty$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} 1  -\\frac{1}{2} \\end{pmatrix}  \\begin{pmatrix} 1  2 \\\\ 1  -1 \\end{pmatrix}  \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}  \\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\\\ \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} + \\left(-\\frac{1}{2}\\right)^{t} \\begin{pmatrix} \\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{1}{3}  \\frac{1}{3} \\end{pmatrix} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The convergence of a random walk to its stationary distribution hinges on more than just the ability to move between any two states (irreducibility). This practice explores the crucial property of periodicity, where a system can become trapped in an oscillatory pattern, preventing the matrix of transition probabilities $P^t$ from converging to a single limit. By analyzing a deterministic cyclic network, you will see why aperiodicity is a necessary condition for simple convergence and learn how concepts like the Cesàro average relate to the stationary distribution in such systems. ",
            "id": "4312630",
            "problem": "Consider a discrete-time random walk on a directed network with state space $\\{1,2,3\\}$, where from each state the walker moves deterministically to the unique outgoing neighbor: from $1$ to $2$, from $2$ to $3$, and from $3$ to $1$. Let $P$ denote the transition matrix of this Markov chain. This chain is a simple model of flow on a directed $3$-cycle in complex networks, and it exemplifies an irreducible but periodic stochastic dynamics.\n\nUsing only the core definitions of a Markov chain, transition matrix, irreducibility, period, and stationary distribution, address the following:\n\n1. Write the transition matrix $P$ explicitly and justify that the chain is irreducible. Determine the period of each state through its definition as the greatest common divisor of return times with positive probability.\n\n2. Compute the stationary distribution $\\pi$ of the chain using the defining equations $\\pi = \\pi P$ and $\\sum_{i=1}^{3} \\pi_i = 1$.\n\n3. Compute $P^t$ for general integer time $t \\geq 1$ by characterizing its nonzero entries in terms of congruence classes modulo $3$, and use this explicit form to demonstrate that the sequence $\\{P^t\\}_{t \\geq 1}$ does not converge as $t \\to \\infty$ but instead oscillates among a finite set of matrices.\n\n4. Define the Cesàro average $\\overline{P}^{(n)} := \\frac{1}{n} \\sum_{t=1}^{n} P^t$ and compute the single entry $\\overline{P}^{(\\infty)}_{1,1} := \\lim_{n \\to \\infty} \\overline{P}^{(n)}_{1,1}$ directly from the explicit form of $P^t$.\n\nYour final reported quantity must be the exact value of $\\overline{P}^{(\\infty)}_{1,1}$, expressed as a simplified rational number. No rounding is required and no units apply. Provide only this final value as your answer.",
            "solution": "We begin from the fundamental definitions. A discrete-time Markov chain on a finite state space has a transition matrix $P$ with entries $P_{i,j} = \\mathbb{P}(X_{t+1} = j \\mid X_t = i)$ that are nonnegative and row-stochastic, meaning $\\sum_{j} P_{i,j} = 1$ for each $i$. The chain is irreducible if for any pair of states $i$ and $j$, there exists an integer $t \\geq 1$ such that $(P^t)_{i,j}  0$. The period of a state $i$ is defined as $d(i) := \\gcd\\{ t \\geq 1 : (P^t)_{i,i}  0 \\}$. A stationary distribution $\\pi$ is a probability vector satisfying $\\pi = \\pi P$ and $\\sum_{i} \\pi_i = 1$ with $\\pi_i \\geq 0$.\n\nFor this chain, the dynamics are deterministic: $1 \\to 2$, $2 \\to 3$, $3 \\to 1$.\n\n1. The transition matrix $P$ is\n$$\nP \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n1  0  0\n\\end{pmatrix}.\n$$\nThis $P$ is a permutation matrix corresponding to the $3$-cycle $(1\\,2\\,3)$. To verify irreducibility, observe that from any state $i$, the chain reaches any state $j$ in exactly the unique time $t \\in \\{1,2\\}$ or $t=3$ such that $j \\equiv i+t \\pmod{3}$, thus $(P^t)_{i,j} = 1$ for that $t$. Hence for any $i,j$ there exists $t \\geq 1$ with $(P^t)_{i,j}  0$, establishing irreducibility.\n\nTo determine the period, we compute return times to a state. Because the dynamics are a $3$-cycle, the only return times to any state $i$ are multiples of $3$. In particular, $(P^t)_{i,i} = 1$ if and only if $t \\equiv 0 \\pmod{3}$, and $(P^t)_{i,i} = 0$ otherwise. Therefore, the set $\\{ t \\geq 1 : (P^t)_{i,i}  0 \\}$ equals $\\{ 3, 6, 9, \\dots \\}$, whose greatest common divisor is $3$. Hence each state has period $3$, and the chain is periodic with period $3$.\n\n2. The stationary distribution $\\pi = (\\pi_1,\\pi_2,\\pi_3)$ satisfies $\\pi = \\pi P$ and $\\pi_1 + \\pi_2 + \\pi_3 = 1$. From $\\pi = \\pi P$,\n$$\n\\begin{aligned}\n\\pi_1 = \\pi_3, \\\\\n\\pi_2 = \\pi_1, \\\\\n\\pi_3 = \\pi_2.\n\\end{aligned}\n$$\nThese relations imply $\\pi_1 = \\pi_2 = \\pi_3$. With the normalization, we obtain $\\pi_1 = \\pi_2 = \\pi_3 = \\frac{1}{3}$. Thus the unique stationary distribution is the uniform distribution on the three states.\n\n3. To compute $P^t$ for general $t \\geq 1$, note that $P$ is the permutation matrix of the cyclic shift by $+1$ modulo $3$. Therefore, $P^t$ is the permutation matrix corresponding to the cyclic shift by $+t$ modulo $3$. Explicitly,\n$$\n(P^t)_{i,j} \\;=\\; \\begin{cases}\n1,  \\text{if } j \\equiv i + t \\pmod{3}, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nIn particular, $(P^t)_{1,1} = 1$ if and only if $t \\equiv 0 \\pmod{3}$, and $(P^t)_{1,1} = 0$ otherwise. Hence the sequence $\\{P^t\\}_{t \\geq 1}$ is periodic with period $3$ and oscillates among the three distinct permutation matrices $I$, $P$, and $P^2$. Because the sequence does not settle to a single matrix, it does not converge as $t \\to \\infty$. A direct contradiction argument using the $(1,1)$ entry makes this precise: if $P^t \\to Q$ entrywise, then the $(1,1)$ entry would satisfy $Q_{1,1} = \\lim_{t \\to \\infty} (P^t)_{1,1}$, but along the subsequences $t \\equiv 0 \\pmod{3}$ and $t \\equiv 1 \\pmod{3}$, the $(1,1)$ entry takes values $1$ and $0$ respectively, which cannot converge to a single limit.\n\n4. Consider the Cesàro average $\\overline{P}^{(n)} := \\frac{1}{n} \\sum_{t=1}^{n} P^t$. We are asked to compute\n$$\n\\overline{P}^{(\\infty)}_{1,1} \\;:=\\; \\lim_{n \\to \\infty} \\overline{P}^{(n)}_{1,1}\n\\;=\\; \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{t=1}^n (P^t)_{1,1}.\n$$\nFrom the explicit form above, $(P^t)_{1,1} = 1$ if and only if $t$ is divisible by $3$, and $(P^t)_{1,1} = 0$ otherwise. Therefore,\n$$\n\\sum_{t=1}^n (P^t)_{1,1} \\;=\\; \\left\\lfloor \\frac{n}{3} \\right\\rfloor,\n$$\nand\n$$\n\\overline{P}^{(n)}_{1,1} \\;=\\; \\frac{1}{n} \\left\\lfloor \\frac{n}{3} \\right\\rfloor.\n$$\nTaking the limit as $n \\to \\infty$ yields\n$$\n\\overline{P}^{(\\infty)}_{1,1} \\;=\\; \\lim_{n \\to \\infty} \\frac{1}{n} \\left\\lfloor \\frac{n}{3} \\right\\rfloor \\;=\\; \\frac{1}{3}.\n$$\nThis value matches the first component of the stationary distribution $\\pi$, consistent with the general result that for an irreducible periodic chain, the Cesàro averages $\\overline{P}^{(n)}$ converge to the rank-one matrix $\\mathbf{1}\\,\\pi^{\\top}$.\n\nTherefore, the required final quantity is $\\frac{1}{3}$.",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        }
    ]
}