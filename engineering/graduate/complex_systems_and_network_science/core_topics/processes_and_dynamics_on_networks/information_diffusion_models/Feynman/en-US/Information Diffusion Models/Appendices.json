{
    "hands_on_practices": [
        {
            "introduction": "Understanding the fundamental mechanics of different diffusion models is the first step towards applying them correctly. This exercise explores a crucial concept: the most influential actors in a network can change depending on the assumed diffusion dynamics. By analyzing a purpose-built network under both the Linear Threshold (LT) and Independent Cascade (IC) models, you will see firsthand how their distinct rules—deterministic thresholds versus probabilistic activations—lead to different optimal seeding strategies .",
            "id": "4283370",
            "problem": "Consider a directed graph $G=(V,E)$ designed to contrast influence regions under the Linear Threshold (LT) model and the Independent Cascade (IC) model. The LT model (Linear Threshold, LT) is defined by node-specific thresholds and weighted incoming edges: a node becomes active when the sum of weights from its active in-neighbors meets or exceeds its threshold. The IC model (Independent Cascade, IC) is defined by edge-specific activation probabilities: each newly activated node gets one chance to activate each inactive out-neighbor, independently, with the given edge probability. In both models, a chosen seed set is active at time step $t=0$ and remains active thereafter. The influence region of a seed set is the set of nodes that eventually become active; in the IC model, this region is random, so we consider its expected size.\n\nConstruct $G$ as follows. Let $V=\\{c,h,t_1,\\dots,t_{10},l_1,\\dots,l_8\\}$, and let $E$ consist of edges $c\\to t_i$ for each $i\\in\\{1,\\dots,10\\}$, and edges $h\\to l_j$ for each $j\\in\\{1,\\dots,8\\}$. There are no other edges. Under the LT model, set thresholds $\\theta_{t_i}=0.4$ and $\\theta_{l_j}=0.4$, and set edge weights $w_{c\\to t_i}=1$ and $w_{h\\to l_j}=1$; the thresholds of $c$ and $h$ are irrelevant because seeds are activated initially. Under the IC model, set activation probabilities $p_{c\\to t_i}=0.3$ and $p_{h\\to l_j}=0.8$ for all relevant edges.\n\nAssume a budget of $k=1$ seed. Your task is to compare the influence regions and identify which single-node seed maximizes influence under each model, and to explain the structural reason that the most influential seed differs across models (if it does).\n\nWhich option correctly identifies the most influential single-node seed under LT and IC for the graph $G$ and provides the correct structural reason?\n\nA. Under LT, seeding $c$ maximizes the influence region; under IC, seeding $h$ maximizes expected influence. The structural reason is that LT deterministically activates neighbors when edge weights suffice to clear thresholds (here, $1\\ge 0.4$), so more out-neighbors deterministically implies more activations, whereas IC relies on independent one-shot trials, so fewer neighbors with higher per-edge probabilities (here, $0.8$) can yield higher expected spread than more neighbors with lower per-edge probabilities (here, $0.3$).\n\nB. Under both LT and IC, seeding $h$ is optimal because high out-degree always dominates edge weights and probabilities, so degree centrality alone determines the most influential seed regardless of model-specific dynamics.\n\nC. Under LT, seeding $h$ is optimal; under IC, seeding $c$ is optimal because IC accumulates activation probability from multiple time steps for the same edge, whereas LT does not, so IC favors many weak edges while LT favors few strong edges.\n\nD. Under both LT and IC, any single-node seed has the same influence due to submodularity and monotonicity of the spread functions, which imply identical optimal seeds across models for any graph with the same node set and budget $k=1$.",
            "solution": "The user wants me to solve a problem about information diffusion models (LT and IC) on a specific graph.\n\nHere's my plan:\n1.  **Validate the Problem Statement**:\n    *   **Step 1: Extract Givens**: I'll list all the definitions, graph structure, model parameters, and the task.\n        *   Graph $G=(V, E)$.\n        *   Nodes $V = \\{c, h, t_1, \\dots, t_{10}, l_1, \\dots, l_8\\}$. Total nodes: $1+1+10+8 = 20$.\n        *   Edges $E$: $c \\to t_i$ for $i=1, \\dots, 10$. $h \\to l_j$ for $j=1, \\dots, 8$.\n        *   **LT Model Parameters**:\n            *   Thresholds: $\\theta_{t_i} = 0.4$ for all $i$.\n            *   Thresholds: $\\theta_{l_j} = 0.4$ for all $j$.\n            *   Thresholds for $c, h$ are irrelevant.\n            *   Edge weights: $w_{c \\to t_i} = 1$ for all $i$.\n            *   Edge weights: $w_{h \\to l_j} = 1$ for all $j$.\n        *   **IC Model Parameters**:\n            *   Probabilities: $p_{c \\to t_i} = 0.3$ for all $i$.\n            *   Probabilities: $p_{h \\to l_j} = 0.8$ for all $j$.\n        *   **Task**:\n            *   Budget $k=1$ seed.\n            *   Compare influence regions.\n            *   Identify the single-node seed that maximizes influence under each model.\n            *   Explain the structural reason for any difference.\n            *   Evaluate the given options.\n\n    *   **Step 2: Validate Using Extracted Givens**:\n        *   **Scientifically Grounded?**: Yes, LT and IC are standard models in network science and social network analysis. The problem is a textbook-style application of these models.\n        *   **Well-Posed?**: Yes, the graph structure and model parameters are fully specified. The task is to find the optimal single-node seed set, which is a well-defined problem for both models. A unique solution should exist for each model.\n        *   **Objective?**: Yes, the language is precise and quantitative. No subjective claims.\n        *   **Flaws Check**:\n            1.  **Scientific/Factual Unsoundness**: No. The models are defined correctly. The parameter choices are valid.\n            2.  **Non-Formalizable/Irrelevant**: No. It's a standard problem in information diffusion models.\n            3.  **Incomplete/Contradictory**: No. All necessary information is provided. The graph structure is simple and unambiguous.\n            4.  **Unrealistic/Infeasible**: No. It is a theoretical construct, but the parameters are mathematically and physically consistent.\n            5.  **Ill-Posed/Poorly Structured**: No. The solution is unique and meaningful. The question is clear.\n            6.  **Pseudo-Profound/Trivial**: No. It requires understanding the fundamental differences between the LT and IC models, which is a key concept in the field. The structure is designed specifically to highlight this difference.\n            7.  **Outside Scientific Verifiability**: No. The results can be directly calculated.\n\n    *   **Step 3: Verdict and Action**: The problem is **valid**. I will proceed to the solution.\n\n2.  **Solve the Problem**:\n    *   The goal is to find the optimal seed for $k=1$. The possible seeds are any node in $V$. However, only nodes $c$ and $h$ have out-going edges. Seeding any of the $t_i$ or $l_j$ nodes will result in an influence spread of only 1 (the node itself), as they have no out-neighbors. Therefore, the only non-trivial choices for a seed are $c$ and $h$. I need to compare the influence of seeding $c$ vs. seeding $h$ under both models.\n    *   Let $\\sigma_{LT}(S)$ be the size of the influence region for seed set $S$ in the LT model.\n    *   Let $\\mathbb{E}[\\sigma_{IC}(S)]$ be the expected size of the influence region for seed set $S$ in the IC model.\n\n    *   **Analysis under LT Model**:\n        *   **Seed set $S = \\{c\\}$**:\n            *   At $t=0$, $c$ is active.\n            *   At $t=1$, we check its neighbors, the nodes $t_i$ for $i=1, \\dots, 10$.\n            *   For each node $t_i$, its only active in-neighbor is $c$. The weight of the edge $c \\to t_i$ is $w_{c \\to t_i} = 1$.\n            *   The threshold for $t_i$ is $\\theta_{t_i} = 0.4$.\n            *   Since $w_{c \\to t_i} = 1 \\ge \\theta_{t_i} = 0.4$, the node $t_i$ will become active.\n            *   This is true for all $i=1, \\dots, 10$. So, all $10$ neighbors of $c$ become active.\n            *   The nodes $t_i$ have no out-neighbors, so the cascade stops.\n            *   The final set of active nodes is $\\{c, t_1, \\dots, t_{10}\\}$.\n            *   The total influence is $\\sigma_{LT}(\\{c\\}) = 1 (\\text{seed}) + 10 (\\text{neighbors}) = 11$.\n        *   **Seed set $S = \\{h\\}$**:\n            *   At $t=0$, $h$ is active.\n            *   At $t=1$, we check its neighbors, the nodes $l_j$ for $j=1, \\dots, 8$.\n            *   For each node $l_j$, its only active in-neighbor is $h$. The weight of the edge $h \\to l_j$ is $w_{h \\to l_j} = 1$.\n            *   The threshold for $l_j$ is $\\theta_{l_j} = 0.4$.\n            *   Since $w_{h \\to l_j} = 1 \\ge \\theta_{l_j} = 0.4$, the node $l_j$ will become active.\n            *   This is true for all $j=1, \\dots, 8$. So, all $8$ neighbors of $h$ become active.\n            *   The nodes $l_j$ have no out-neighbors, so the cascade stops.\n            *   The final set of active nodes is $\\{h, l_1, \\dots, l_8\\}$.\n            *   The total influence is $\\sigma_{LT}(\\{h\\}) = 1 (\\text{seed}) + 8 (\\text{neighbors}) = 9$.\n        *   **Comparison for LT**:\n            *   $\\sigma_{LT}(\\{c\\}) = 11$.\n            *   $\\sigma_{LT}(\\{h\\}) = 9$.\n            *   Since $11 > 9$, the optimal seed for the LT model is $c$.\n\n    *   **Analysis under IC Model**:\n        *   The process is probabilistic. We need to calculate the expected influence size.\n        *   In the IC model, the expected number of active nodes, given a seed set $S$, is $|S| + \\sum_{v \\in V \\setminus S} P(\\text{v is activated}|S)$.\n        *   In this simple graph structure, a node $u$ can only be activated by its single in-neighbor.\n        *   **Seed set $S = \\{c\\}$**:\n            *   $c$ is active.\n            *   For each neighbor $t_i$, it has one chance to be activated by $c$. This happens with probability $p_{c \\to t_i} = 0.3$.\n            *   The activation of each $t_i$ is an independent Bernoulli trial with success probability $0.3$.\n            *   The expected number of activated neighbors is $\\sum_{i=1}^{10} p_{c \\to t_i} = 10 \\times 0.3 = 3$.\n            *   The expected size of the influence region, $\\mathbb{E}[\\sigma_{IC}(\\{c\\})]$, is the size of the seed set plus the expected number of newly activated nodes.\n            *   $\\mathbb{E}[\\sigma_{IC}(\\{c\\})] = 1 (\\text{seed}) + \\text{Expected number of activated neighbors} = 1 + 10 \\times 0.3 = 1 + 3 = 4$.\n        *   **Seed set $S = \\{h\\}$**:\n            *   $h$ is active.\n            *   For each neighbor $l_j$, it has one chance to be activated by $h$. This happens with probability $p_{h \\to l_j} = 0.8$.\n            *   The activation of each $l_j$ is an independent Bernoulli trial with success probability $0.8$.\n            *   The expected number of activated neighbors is $\\sum_{j=1}^{8} p_{h \\to l_j} = 8 \\times 0.8 = 6.4$.\n            *   The expected size of the influence region, $\\mathbb{E}[\\sigma_{IC}(\\{h\\})]$, is the size of the seed set plus the expected number of newly activated nodes.\n            *   $\\mathbb{E}[\\sigma_{IC}(\\{h\\})] = 1 (\\text{seed}) + \\text{Expected number of activated neighbors} = 1 + 8 \\times 0.8 = 1 + 6.4 = 7.4$.\n        *   **Comparison for IC**:\n            *   $\\mathbb{E}[\\sigma_{IC}(\\{c\\})] = 4$.\n            *   $\\mathbb{E}[\\sigma_{IC}(\\{h\\})] = 7.4$.\n            *   Since $7.4 > 4$, the optimal seed for the IC model is $h$.\n\n    *   **Summary of Results**:\n        *   LT Model: Optimal seed is $c$.\n        *   IC Model: Optimal seed is $h$.\n        *   The optimal seed differs between the models.\n\n    *   **Structural Reason**:\n        *   In the LT model, activation is deterministic once a threshold is met. Since the weights $w=1$ are much larger than the thresholds $\\theta=0.4$, activation is guaranteed for any neighbor of an active node. Therefore, influence is directly proportional to the number of out-neighbors (out-degree). Node $c$ has out-degree $10$, while node $h$ has out-degree $8$. Thus, $c$ is better.\n        *   In the IC model, activation is probabilistic. The expected number of activations is the sum of probabilities, which is (out-degree) $\\times$ (probability per edge). For node $c$, the expected number of secondary activations is $10 \\times 0.3 = 3$. For node $h$, it is $8 \\times 0.8 = 6.4$. Here, the higher probability per edge for $h$ outweighs the larger number of neighbors for $c$. The focus is on the product of degree and probability, not just degree. Thus, $h$ is better.\n        *   The problem is designed to show that LT is sensitive to the number of connections (degree) when weights are high enough, while IC is sensitive to the product of degree and probability, allowing a smaller-degree node with high-probability edges to be more influential.\n\n3.  **Evaluate the Options**:\n    *   **A. Under LT, seeding $c$ maximizes the influence region; under IC, seeding $h$ maximizes expected influence. The structural reason is that LT deterministically activates neighbors when edge weights suffice to clear thresholds (here, $1\\ge 0.4$), so more out-neighbors deterministically implies more activations, whereas IC relies on independent one-shot trials, so fewer neighbors with higher per-edge probabilities (here, $0.8$) can yield higher expected spread than more neighbors with lower per-edge probabilities (here, $0.3$).**\n        *   My analysis confirms: LT optimal is $c$, IC optimal is $h$.\n        *   The reason given for LT is correct. Since $w > \\theta$, every neighbor is activated deterministically. Thus, the node with more neighbors ($c$) wins.\n        *   The reason given for IC is correct. It's about the expected number of activations, which depends on the product of degree and probability. $8 \\times 0.8 > 10 \\times 0.3$, so $h$ wins.\n        *   This option seems entirely correct.\n\n    *   **B. Under both LT and IC, seeding $h$ is optimal because high out-degree always dominates edge weights and probabilities, so degree centrality alone determines the most influential seed regardless of model-specific dynamics.**\n        *   This is incorrect. My analysis shows LT optimal is $c$ and IC optimal is $h$. They are not the same.\n        *   The reasoning is also flawed. It claims seeding $h$ is optimal, but $c$ has the higher out-degree ($10$ vs $8$). So the reasoning \"high out-degree always dominates\" would imply $c$ is better, contradicting the first part of the statement. The claim that degree centrality alone determines influence is precisely what this problem is designed to disprove.\n\n    *   **C. Under LT, seeding $h$ is optimal; under IC, seeding $c$ is optimal because IC accumulates activation probability from multiple time steps for the same edge, whereas LT does not, so IC favors many weak edges while LT favors few strong edges.**\n        *   The predictions are the exact opposite of my calculations. I found LT optimal is $c$ and IC optimal is $h$.\n        *   The reasoning about IC is factually incorrect. In the standard IC model, an active node gets **one chance** to activate its inactive neighbors. There is no accumulation of probability over multiple time steps for the same edge from the same parent. (If a node has multiple active parents, then it can get multiple activation attempts, but that's not what is described).\n        *   The reasoning about LT is also simplistic and misleading (\"LT favors few strong edges\"). In this case, LT favors many edges because all are \"strong\" enough.\n\n    *   **D. Under both LT and IC, any single-node seed has the same influence due to submodularity and monotonicity of the spread functions, which imply identical optimal seeds across models for any graph with the same node set and budget $k=1$.**\n        *   This is incorrect. My calculations show different influence values and different optimal seeds for the two models.\n        *   The reasoning is flawed. While influence functions for LT and IC are indeed monotone and submodular, this does not imply that the optimal seeds are the same across different models. Submodularity is a property of a single influence function $\\sigma(\\cdot)$, it does not say anything about the relationship between two different functions $\\sigma_{LT}(\\cdot)$ and $\\sigma_{IC}(\\cdot)$. It's a well-known result that optimal seeds for LT and IC can differ, and this problem is a standard example of that.\n\n4.  **Final Conclusion**:\n    *   My analysis showed that seeding $c$ is optimal under the LT model ($\\sigma_{LT}(\\{c\\})=11$ vs $\\sigma_{LT}(\\{h\\})=9$) and seeding $h$ is optimal under the IC model ($\\mathbb{E}[\\sigma_{IC}(\\{h\\})]=7.4$ vs $\\mathbb{E}[\\sigma_{IC}(\\{c\\})]=4$).\n    *   The reason is that in this setup, LT boils down to a comparison of out-degrees, while IC boils down to a comparison of expected secondary activations, which is (out-degree) $\\times$ (probability).\n    *   Option A correctly identifies the optimal seeds for both models and provides the correct underlying reasoning.\n    *   All other options are incorrect either in their predictions or their reasoning, or both.\n\n5.  **Final Polish**:\n    *   I will write out the solution clearly, defining all symbols in LaTeX.\n    *   I will start with the validation section as required.\n    *   Then, I'll provide the detailed derivation for both LT and IC models.\n    *   Finally, I will analyze each option (A, B, C, D) and state why it is correct or incorrect.\n    *   The final answer will be enclosed in `$$\\boxed{A}$$`.\n\nDouble check the math.\nLT:\nSeed c: 1(c) + 10(t_i) = 11. Because for each t_i, sum of weights from active neighbors is w_{c-t_i} = 1, and threshold is theta_{t_i} = 0.4. Since 1 = 0.4, t_i activates.\nSeed h: 1(h) + 8(l_j) = 9. Because for each l_j, sum of weights is w_{h-l_j} = 1, and threshold is theta_{l_j} = 0.4. Since 1 = 0.4, l_j activates.\nSo for LT, c is better.\n\nIC:\nSeed c: Expected influence = 1(c) + sum_{i=1 to 10} P(t_i activates). P(t_i activates) = p_{c-t_i} = 0.3. So expected influence = 1 + 10 * 0.3 = 1 + 3 = 4.\nSeed h: Expected influence = 1(h) + sum_{j=1 to 8} P(l_j activates). P(l_j activates) = p_{h-l_j} = 0.8. So expected influence = 1 + 8 * 0.8 = 1 + 6.4 = 7.4.\nSo for IC, h is better.\n\nThe results are consistent. The reasoning in Option A is also correct.\nLT: activation is deterministic when weight exceeds threshold. The problem is set up so $w > \\theta$ for all relevant edges. So influence spread becomes just a count of out-neighbors. Node $c$ has more out-neighbors ($10$) than $h$ ($8$).\nIC: activation is probabilistic. Expected spread depends on the sum of activation probabilities over the out-edges. For $c$, this sum is $10 \\times 0.3 = 3$. For $h$, this sum is $8 \\times 0.8 = 6.4$. So $h$ has a larger expected spread.\n\nThe logic is sound. I will proceed with writing the solution.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once a model is chosen, a central task is to identify a small set of seeds to maximize the spread of information, a problem known as influence maximization. While intuitively one might pick nodes with the most connections, this exercise demonstrates the pitfalls of such simple heuristics. Through concrete calculation , you will discover the principle of submodularity, or diminishing returns, which explains why a more nuanced, greedy approach that considers the overlap of influence pathways yields superior results.",
            "id": "4283432",
            "problem": "Consider the following directed graph designed to compare naive high-degree seeding with the standard greedy algorithm for influence maximization under the Independent Cascade (IC) model. The node set is $V=\\{a,b,h,o,t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$. Only the nodes $\\{a,b,h,o\\}$ have outgoing edges (the candidate seed set), and all edges point to the target nodes $\\{t_{1},\\dots,t_{6}\\}$. There are no edges among the target nodes and no edges into any of $\\{a,b,h,o\\}$. The directed edges with their IC activation probabilities are:\n- From $a$ to $t_{1},t_{2},t_{3}$ with probability $0.9$ each.\n- From $b$ to $t_{4},t_{5},t_{6}$ with probability $0.9$ each.\n- From $o$ to $t_{1},t_{2},t_{3},t_{4}$ with probability $0.2$ each.\n- From $h$ to $t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}$ with probability $0.1$ each.\n\nWe adopt the Independent Cascade (IC) model: initially seeded nodes are active at time $t=0$; each active node gets a single chance to activate each currently inactive out-neighbor once in the next time step, independently, using the edge’s activation probability; the process then terminates because there are no edges beyond the first step in this graph. The total influence (spread) is defined as the expected number of active nodes at termination, counting seed nodes as active.\n\nLet the seed budget be $k=2$. Define two seeding strategies:\n- Naive high-degree seeding: choose the $k$ nodes in $\\{a,b,h,o\\}$ with largest out-degree, ignoring edge probabilities.\n- Greedy submodular seeding: iteratively add one seed at a time, each time selecting the node in $\\{a,b,h,o\\}$ that maximizes the marginal increase in expected spread under the IC model.\n\nUsing only the definitions and rules of the IC model and standard probability laws, compute the difference between the expected total influence of the greedy submodular seeding strategy and that of the naive high-degree strategy for this graph. Express your final answer as a single exact decimal number with no rounding.",
            "solution": "The problem is validated as follows:\n### Step 1: Extract Givens\n- Node set: $V=\\{a,b,h,o,t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$\n- Candidate seed set: $S_{cand}=\\{a,b,h,o\\}$\n- Target node set: $T=\\{t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}\\}$\n- Edge structure and probabilities:\n  - Edges from $a$ to $t_{1},t_{2},t_{3}$ with probability $p=0.9$ each.\n  - Edges from $b$ to $t_{4},t_{5},t_{6}$ with probability $p=0.9$ each.\n  - Edges from $o$ to $t_{1},t_{2},t_{3},t_{4}$ with probability $p=0.2$ each.\n  - Edges from $h$ to $t_{1},t_{2},t_{3},t_{4},t_{5},t_{6}$ with probability $p=0.1$ each.\n- Model: Independent Cascade (IC) model with one-step activation.\n- Influence (spread): Expected number of total active nodes (seeds + newly activated).\n- Seed budget: $k=2$.\n- Strategy 1 (Naive): Choose the $k=2$ nodes from $\\{a,b,h,o\\}$ with the largest out-degree.\n- Strategy 2 (Greedy): Iteratively choose $k=2$ seeds, where each choice maximizes the marginal increase in expected spread.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded:** The problem is based on the well-established Independent Cascade (IC) model and standard algorithms (greedy algorithm for submodular maximization) from the field of network science and information diffusion. The concepts are sound.\n- **Well-Posed:** All necessary information (graph structure, probabilities, budget, algorithm definitions) is provided. The objective—to compute a specific numerical difference—is clear and unambiguous. A unique solution exists.\n- **Objective:** The problem is stated in precise, formal language without subjective or opinion-based content.\n- **Completeness and Consistency:** The problem is self-contained and free of contradictions. The graph structure is simple and fully described.\n- **Realism:** The problem is a constructed example to illustrate a concept, which is standard academic practice. The probabilities and graph structure are not physically impossible.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe expected total influence (spread), denoted $\\sigma(S)$, for a given seed set $S$ is the sum of the number of seeds and the expected number of newly activated nodes.\n$$ \\sigma(S) = |S| + \\sum_{v \\in V \\setminus S} P(v \\text{ is activated} | S) $$\nIn this specific graph structure, only nodes in $T = \\{t_1, \\dots, t_6\\}$ can be newly activated. The probability that a target node $t_i$ is activated by the seed set $S$ is given by $1$ minus the probability that it is not activated by any seed in $S$. Let $N_{in}(t_i)$ be the set of in-neighbors of $t_i$.\n$$ P(t_i \\text{ is activated} | S) = 1 - \\prod_{s \\in S \\cap N_{in}(t_i)} (1 - p_{s,t_i}) $$\nwhere $p_{s,t_i}$ is the activation probability on the edge from $s$ to $t_i$. The total expected influence is:\n$$ \\sigma(S) = |S| + \\sum_{i=1}^{6} \\left( 1 - \\prod_{s \\in S \\cap N_{in}(t_i)} (1 - p_{s,t_i}) \\right) $$\n\n**1. Naive High-Degree Seeding Strategy**\nFirst, we determine the out-degrees for the candidate seed nodes $\\{a,b,h,o\\}$:\n- Out-degree of $a$: $\\text{deg}^+(a) = 3$ (to $t_1, t_2, t_3$).\n- Out-degree of $b$: $\\text{deg}^+(b) = 3$ (to $t_4, t_5, t_6$).\n- Out-degree of $h$: $\\text{deg}^+(h) = 6$ (to $t_1, \\dots, t_6$).\n- Out-degree of $o$: $\\text{deg}^+(o) = 4$ (to $t_1, \\dots, t_4$).\n\nThe two nodes with the highest out-degrees are $h$ (degree $6$) and $o$ (degree $4$). Therefore, the naive seed set is $S_{naive} = \\{h, o\\}$. The budget is $k=2$.\nThe expected influence $\\sigma(S_{naive})$ is calculated as:\n$$ \\sigma(S_{naive}) = |S_{naive}| + \\sum_{i=1}^{6} P(t_i \\text{ is activated} | \\{h,o\\}) $$\n$|S_{naive}|=2$. Let's calculate the activation probability for each target node:\n- For $t_1, t_2, t_3$: These are neighbors of both $h$ (with $p=0.1$) and $o$ (with $p=0.2$).\n  $P(\\text{activated}) = 1 - (1 - p_{h,t_i})(1 - p_{o,t_i}) = 1 - (1 - 0.1)(1 - 0.2) = 1 - (0.9)(0.8) = 1 - 0.72 = 0.28$.\n- For $t_4$: This node is also a neighbor of both $h$ ($p=0.1$) and $o$ ($p=0.2$).\n  $P(\\text{activated}) = 1 - (0.9)(0.8) = 0.28$.\n- For $t_5, t_6$: These are neighbors of only $h$ (with $p=0.1$).\n  $P(\\text{activated}) = p_{h,t_i} = 0.1$.\n\nSumming the expected new activations:\n$$ \\sum_{i=1}^{6} P(t_i \\text{ is activated}) = 3 \\times (0.28) + 1 \\times (0.28) + 2 \\times (0.1) = 4 \\times 0.28 + 0.2 = 1.12 + 0.2 = 1.32 $$\nThe total expected influence for the naive strategy is:\n$$ \\sigma(S_{naive}) = 2 + 1.32 = 3.32 $$\n\n**2. Greedy Submodular Seeding Strategy**\nThe greedy algorithm selects seeds one by one. With a budget of $k=2$, we perform two selections.\n\n**Selection 1:** Find the single seed with the highest expected influence.\n$\\sigma(\\{s\\}) = 1 + \\sum_{t_i \\in T} p_{s,t_i}$.\n- $\\sigma(\\{a\\}) = 1 + 3 \\times 0.9 = 1 + 2.7 = 3.7$.\n- $\\sigma(\\{b\\}) = 1 + 3 \\times 0.9 = 1 + 2.7 = 3.7$.\n- $\\sigma(\\{h\\}) = 1 + 6 \\times 0.1 = 1 + 0.6 = 1.6$.\n- $\\sigma(\\{o\\}) = 1 + 4 \\times 0.2 = 1 + 0.8 = 1.8$.\nThe maximum influence is $3.7$, achieved by both $a$ and $b$. We break the tie by choosing $a$. The first seed is $s_1 = a$. Let the current seed set be $S_1 = \\{a\\}$.\n\n**Selection 2:** Find the node that provides the maximum marginal gain in influence when added to $S_1$.\nThe marginal gain of adding node $s$ to $S_1$ is $\\sigma(S_1 \\cup \\{s\\}) - \\sigma(S_1)$. We evaluate this for $s \\in \\{b,h,o\\}$.\n- **Add $b$ to $\\{a\\}$**: $S=\\{a,b\\}$. The neighbors of $a$ and $b$ are disjoint.\n  $\\sigma(\\{a,b\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,b\\})$.\n  For $t_1,t_2,t_3$: $P(\\text{act.}) = p_{a,t_i} = 0.9$.\n  For $t_4,t_5,t_6$: $P(\\text{act.}) = p_{b,t_i} = 0.9$.\n  $\\sum P = 3 \\times 0.9 + 3 \\times 0.9 = 5.4$.\n  $\\sigma(\\{a,b\\}) = 2 + 5.4 = 7.4$.\n  Marginal gain from $b$: $\\sigma(\\{a,b\\}) - \\sigma(\\{a\\}) = 7.4 - 3.7 = 3.7$.\n\n- **Add $h$ to $\\{a\\}$**: $S=\\{a,h\\}$.\n  $\\sigma(\\{a,h\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,h\\})$.\n  For $t_1,t_2,t_3$: neighbors of $a$ ($p=0.9$) and $h$ ($p=0.1$).\n  $P(\\text{act.}) = 1 - (1-0.9)(1-0.1) = 1 - (0.1)(0.9) = 1 - 0.09 = 0.91$.\n  For $t_4,t_5,t_6$: neighbors of only $h$ ($p=0.1$). $P(\\text{act.}) = 0.1$.\n  $\\sum P = 3 \\times 0.91 + 3 \\times 0.1 = 2.73 + 0.3 = 3.03$.\n  $\\sigma(\\{a,h\\}) = 2 + 3.03 = 5.03$.\n  Marginal gain from $h$: $\\sigma(\\{a,h\\}) - \\sigma(\\{a\\}) = 5.03 - 3.7 = 1.33$.\n\n- **Add $o$ to $\\{a\\}$**: $S=\\{a,o\\}$.\n  $\\sigma(\\{a,o\\}) = 2 + \\sum P(t_i \\text{ act.} | \\{a,o\\})$.\n  For $t_1,t_2,t_3$: neighbors of $a$ ($p=0.9$) and $o$ ($p=0.2$).\n  $P(\\text{act.}) = 1 - (1-0.9)(1-0.2) = 1 - (0.1)(0.8) = 1 - 0.08 = 0.92$.\n  For $t_4$: neighbor of only $o$ ($p=0.2$). $P(\\text{act.}) = 0.2$.\n  For $t_5,t_6$: no incoming edges from $\\{a,o\\}$. $P(\\text{act.}) = 0$.\n  $\\sum P = 3 \\times 0.92 + 1 \\times 0.2 = 2.76 + 0.2 = 2.96$.\n  $\\sigma(\\{a,o\\}) = 2 + 2.96 = 4.96$.\n  Marginal gain from $o$: $\\sigma(\\{a,o\\}) - \\sigma(\\{a\\}) = 4.96 - 3.7 = 1.26$.\n\nComparing the marginal gains: $3.7$ (from $b$) $ 1.33$ (from $h$) $ 1.26$ (from $o$).\nThe greedy algorithm selects $b$ as the second seed. The greedy seed set is $S_{greedy} = \\{a, b\\}$.\nThe total expected influence for the greedy strategy is $\\sigma(S_{greedy}) = \\sigma(\\{a,b\\}) = 7.4$.\n\n**3. Difference in Expected Influence**\nThe difference is the expected influence of the greedy strategy minus the expected influence of the naive strategy.\n$$ \\text{Difference} = \\sigma(S_{greedy}) - \\sigma(S_{naive}) = 7.4 - 3.32 = 4.08 $$\nThe final answer is an exact decimal.",
            "answer": "$$ \\boxed{4.08} $$"
        },
        {
            "introduction": "Calculating exact influence is computationally infeasible for large, real-world networks, necessitating efficient algorithms and approximation methods. This practice moves from theory to implementation by tasking you with building the lazy-greedy algorithm, a widely-used, scalable solution for influence maximization . You will combine Monte Carlo simulation to estimate influence with a priority-queue-based optimization to develop a practical tool for finding influential nodes in complex networks.",
            "id": "4283386",
            "problem": "You are given a directive to implement a program that constructs a lazy-greedy selection sequence of seed nodes for information diffusion under the Independent Cascade model. The program must use a sample-based oracle to estimate marginal gains. The task is framed in purely mathematical and algorithmic terms, grounded in the theory of submodular maximization and diffusion processes on networks, and should adhere to the following specifications.\n\nA directed network is represented as a finite directed graph $G = (V, E)$, where $V$ is a finite set of nodes labeled by $0, 1, 2, \\dots, n-1$, and $E \\subseteq V \\times V$ is the set of directed edges. Each edge $(u, v) \\in E$ is assigned an activation probability $p_{uv} \\in [0, 1]$. Under the Independent Cascade model, diffusion unfolds in discrete time steps; if a node $u$ is newly active at time $t$, it attempts to activate each out-neighbor $v$ exactly once with independent success probability $p_{uv}$ at time $t+1$. An initially active set of seed nodes $S \\subseteq V$ initiates the process at time $t=0$. The expected spread $f(S)$ is defined as the expected number of distinct nodes that become active by termination of the process. The goal is to construct a sequence of $k$ seed nodes using a lazy-greedy algorithm that relies on sample-based estimates of marginal gains to select, at each step, the node $u \\in V \\setminus S$ that maximizes the marginal gain $f(S \\cup \\{u\\}) - f(S)$.\n\nFundamental base and requirements:\n- You must use the Independent Cascade model and the definition of expected spread $f(S)$ as a well-tested foundation.\n- The expected spread $f(S)$ must be estimated by a sample-based oracle using Monte Carlo simulation with exactly $R$ independent trials per spread estimation. In each trial, simulate the cascade process until no new activations occur and record the total activated count; then take the arithmetic mean over $R$ trials to estimate $f(S)$.\n- Implement lazy-greedy selection: maintain a priority queue of candidate nodes with their last estimated marginal gains and a stamp indicating for which seed set size they were computed. In each iteration, pop the highest-priority candidate; if its estimate is stale (computed for a smaller seed set size), recompute its marginal gain for the current seed set and push it back; if up-to-date, accept it, add it to $S$, and proceed to the next iteration. This procedure must continue until $|S| = k$ or until no candidates remain.\n- In case multiple nodes have equal estimated marginal gains at selection time, break ties deterministically by selecting the node with the smallest identifier.\n- For $k = 0$, the output must be the empty seed set.\n- All nodes are labeled with zero-based integers; all outputs must use these labels.\n\nYour program must implement the above logic and produce final seed sets for the following test suite. For all test cases, you must use a fixed pseudorandom seed as specified, to ensure deterministic behavior of the sample-based oracle.\n\nTest Suite:\n1. Happy-path case:\n   - Graph $G_1$ on $V_1 = \\{0,1,2,3,4,5,6\\}$ with directed edges and activation probabilities:\n     - $(0,1)$ with $p_{01} = 0.4$, $(0,2)$ with $p_{02} = 0.2$,\n     - $(1,2)$ with $p_{12} = 0.4$,\n     - $(2,0)$ with $p_{20} = 0.1$, $(2,3)$ with $p_{23} = 0.2$,\n     - $(3,4)$ with $p_{34} = 0.3$,\n     - $(4,5)$ with $p_{45} = 0.3$, $(4,6)$ with $p_{46} = 0.2$,\n     - $(5,3)$ with $p_{53} = 0.3$.\n   - Budget $k = 2$, samples $R = 800$, pseudorandom seed $s = 20251$.\n\n2. Boundary condition $k=0$ on the same graph $G_1$:\n   - Budget $k = 0$, samples $R = 800$, pseudorandom seed $s = 20252$.\n\n3. All-zero probabilities (edge-case of no propagation):\n   - Graph $G_2$ on $V_2 = \\{0,1,2,3,4\\}$ with directed edges and activation probabilities:\n     - $(0,1)$ with $p_{01} = 0.0$, $(0,2)$ with $p_{02} = 0.0$,\n     - $(1,2)$ with $p_{12} = 0.0$,\n     - $(2,3)$ with $p_{23} = 0.0$,\n     - $(3,4)$ with $p_{34} = 0.0$.\n   - Budget $k = 3$, samples $R = 200$, pseudorandom seed $s = 20253$.\n\n4. Deterministic star with certain activation:\n   - Graph $G_3$ on $V_3 = \\{0,1,2,3,4,5\\}$ with directed edges and activation probabilities:\n     - $(0,1)$ with $p_{01} = 1.0$, $(0,2)$ with $p_{02} = 1.0$, $(0,3)$ with $p_{03} = 1.0$, $(0,4)$ with $p_{04} = 1.0$, $(0,5)$ with $p_{05} = 1.0$.\n   - Budget $k = 1$, samples $R = 50$, pseudorandom seed $s = 20254$.\n\n5. Disconnected components:\n   - Graph $G_4$ on $V_4 = \\{0,1,2,3,4,5,6,7\\}$ with directed edges and activation probabilities:\n     - Component A: $(0,1)$ with $p_{01} = 0.5$, $(1,2)$ with $p_{12} = 0.5$, $(2,3)$ with $p_{23} = 0.5$.\n     - Component B: $(4,5)$ with $p_{45} = 0.5$, $(5,6)$ with $p_{56} = 0.5$, $(6,7)$ with $p_{67} = 0.5$.\n   - Budget $k = 2$, samples $R = 400$, pseudorandom seed $s = 20255$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[result_1, result_2, result_3]$), where each $result_i$ is itself a list of integers representing the selected seed nodes for the $i$-th test case. The output must therefore be a single line of the form $[[s_{1,1},\\dots,s_{1,k_1}],[s_{2,1},\\dots],[\\dots]]$ with no additional text. There are no physical units or angles in this problem; all values are purely discrete and dimensionless.",
            "solution": "The problem requires the implementation of a lazy-greedy algorithm to find a near-optimal set of $k$ seed nodes for information diffusion in a directed network, governed by the Independent Cascade (IC) model. The objective is to maximize the expected spread, $f(S)$, which is the expected number of nodes activated starting from a seed set $S$.\n\nThe problem is well-defined and grounded in the established theory of submodular function maximization. The expected spread function $f(S)$ for the IC model is known to be monotone ($f(A) \\le f(B)$ for any $A \\subseteq B \\subseteq V$) and submodular (for any $A \\subseteq B \\subseteq V$ and $u \\in V \\setminus B$, the marginal gain satisfies $f(A \\cup \\{u\\}) - f(A) \\ge f(B \\cup \\{u\\}) - f(B)$). Submodularity formalizes the notion of diminishing returns and is the key property that guarantees a simple greedy algorithm provides a solution with an expected spread of at least $(1 - 1/e)$ times the optimal solution's spread.\n\nThe lazy-greedy algorithm is a well-known computational optimization of the standard greedy algorithm. The standard greedy approach would, at each of the $k$ steps, re-evaluate the marginal gain of every non-seed node, which is computationally expensive. The lazy-greedy approach leverages the submodularity property: the marginal gain of a node can only decrease as more seeds are added to the set $S$. Thus, a node that had a high marginal gain for a small seed set is likely to remain a top contender. The algorithm maintains a priority queue of nodes ordered by their last computed marginal gains. At each step, it inspects only the top node. If its gain was computed for the current seed set, it is provably the best choice. If not (i.e., it is stale), its gain is re-evaluated, and it is placed back into the queue. This process repeats until an up-to-date node is found at the top of the queue. This significantly reduces the number of spread estimations compared to the standard greedy method.\n\nThe implementation consists of two primary components: a sample-based oracle to estimate $f(S)$ and the main lazy-greedy selection logic.\n\n1.  **Sample-Based Oracle for Expected Spread Estimation**:\n    The analytical calculation of $f(S)$ is $\\#P$-hard. Therefore, we use a Monte Carlo simulation to estimate it. The oracle function, let's call it $\\text{estimate\\_spread}(S, G, R)$, performs $R$ independent simulations of the IC process on the graph $G$ starting from the seed set $S$. Each simulation proceeds as follows:\n    -   Initialize the set of active nodes, $\\mathcal{A}$, with the seed set $S$. A queue of newly activated nodes is also initialized with $S$.\n    -   The process unfolds in discrete steps. In each step, every newly activated node attempts to activate its inactive out-neighbors. For a newly active node $u$ and its out-neighbor $v$, a successful activation occurs with the specified probability $p_{uv}$. This is simulated by drawing a random number from a uniform distribution on $[0, 1)$ and checking if it is less than $p_{uv}$.\n    -   All newly activated nodes in a step are collected. The process continues until a step occurs where no new nodes are activated.\n    -   The total number of nodes in $\\mathcal{A}$ at the end of the simulation is the spread for that single trial.\n    -   The oracle returns the arithmetic mean of the spread values obtained from all $R$ trials. The use of a fixed pseudorandom seed ensures that the outcomes of these probabilistic simulations are deterministic and reproducible.\n\n2.  **Lazy-Greedy Selection Algorithm**:\n    The algorithm proceeds as follows to select a seed set $S$ of size $k$:\n    -   **Base Case**: If $k=0$, an empty set is returned.\n    -   **Initialization**: The seed set $S$ is initialized as empty. A priority queue, $\\mathcal{Q}$, is created to store tuples of the form $(\\delta, u, i)$, where $\\delta$ is the estimated marginal gain of node $u$, and $i$ is the \"stamp\" indicating that this gain was calculated when $|S|=i$. The queue is ordered to prioritize the largest gain $\\delta$. Ties are broken by choosing the node $u$ with the smallest integer label. For the initial state where $S = \\emptyset$, we compute the initial marginal gain for each node $u \\in V$, which is simply $f(\\{u\\}) - f(\\emptyset) = f(\\{u\\})$, as $f(\\emptyset)=0$. We push $(f(\\{u\\}), u, 0)$ into $\\mathcal{Q}$ for all $u \\in V$.\n    -   **Iterative Selection**: The algorithm iterates from $i=0$ to $k-1$ to select $k$ seeds. In each iteration $i$:\n        1.  A loop is initiated to find the best node for the current seed set $S$ (of size $i$).\n        2.  The top element $(\\delta, u, \\text{stamp})$ is extracted from $\\mathcal{Q}$.\n        3.  If node $u$ is already in $S$, it is discarded, and the loop continues.\n        4.  The stamp is checked. If $\\text{stamp} = i$, the gain estimate is up-to-date. Node $u$ is the best choice. It is added to $S$, the inner loop terminates, and the algorithm proceeds to the next main iteration (for seed $i+1$).\n        5.  If $\\text{stamp}  i$, the gain estimate is stale. It was calculated for a smaller seed set. The marginal gain of $u$ with respect to the current set $S$ must be re-evaluated: $\\delta_{\\text{new}} = f(S \\cup \\{u\\}) - f(S)$. This requires two calls to the oracle. The node is then pushed back into $\\mathcal{Q}$ with the updated information: $(\\delta_{\\text{new}}, u, i)$. The inner loop continues, extracting the new top element from $\\mathcal{Q}$.\n    -   **Termination**: After $k$ seeds have been selected, the final set $S$ is returned.\n\nThe implementation will precisely follow this logic, using the specified parameters for each test case to generate the required seed sets. The graph will be represented as an adjacency list. The priority queue will be implemented using Python's `heapq` module, storing tuples of `(-gain, node_id, stamp)` to simulate a max-heap and handle tie-breaking correctly.",
            "answer": "```python\nimport heapq\nimport numpy as np\nfrom collections import deque\n\ndef estimate_spread(seeds, graph, n_nodes, R, rng):\n    \"\"\"\n    Estimates the expected spread f(S) for a given seed set S using Monte Carlo simulation.\n    Args:\n        seeds (list or set): The initial set of active nodes.\n        graph (dict): Adjacency list representation of the graph.\n                      {u: [(v1, p1), (v2, p2), ...]}\n        n_nodes (int): The total number of nodes in the graph.\n        R (int): The number of Monte Carlo simulations to run.\n        rng (numpy.random.Generator): The random number generator.\n    Returns:\n        float: The estimated expected spread.\n    \"\"\"\n    if not seeds:\n        return 0.0\n\n    total_spread = 0\n    for _ in range(R):\n        active = set(seeds)\n        \n        # Use a deque for efficient BFS-like propagation\n        q = deque(seeds)\n        \n        while q:\n            u = q.popleft()\n            \n            if u not in graph:\n                continue\n\n            for v, p_uv in graph[u]:\n                if v not in active:\n                    if rng.random()  p_uv:\n                        active.add(v)\n                        q.append(v)\n        \n        total_spread += len(active)\n    \n    return total_spread / R\n\ndef lazy_greedy_selector(graph_edges, n_nodes, k, R, seed):\n    \"\"\"\n    Constructs a seed set of size k using the lazy-greedy algorithm.\n    Args:\n        graph_edges (list): List of tuples (u, v, p) representing directed edges.\n        n_nodes (int): The number of nodes in the graph.\n        k (int): The desired size of the seed set (budget).\n        R (int): The number of samples for spread estimation.\n        seed (int): The seed for the pseudorandom number generator.\n    Returns:\n        list: The selected seed set of size k.\n    \"\"\"\n    if k == 0:\n        return []\n\n    # Build graph as adjacency list\n    graph = {i: [] for i in range(n_nodes)}\n    for u, v, p in graph_edges:\n        graph[u].append((v, p))\n\n    rng = np.random.default_rng(seed)\n    \n    S = []\n    pq = [] # Max-heap simulated with negative gains in a min-heap\n    \n    # 1. Initial population of the priority queue\n    # The stamp '0' indicates the gain is computed w.r.t. S of size 0 (empty set)\n    for u in range(n_nodes):\n        # Marginal gain for an empty S is just f({u})\n        gain = estimate_spread({u}, graph, n_nodes, R, rng)\n        # Tie-breaking by smaller node_id is implicitly handled by tuple comparison\n        heapq.heappush(pq, (-gain, u, 0))\n\n    # 2. Iteratively select k seeds\n    for i in range(k):\n        # We need the spread of the current set S to calculate future marginal gains.\n        # This is computed only once per main iteration i if needed.\n        spread_S = -1.0 \n\n        while pq:\n            neg_gain, u_top, stamp = heapq.heappop(pq)\n            \n            if u_top in S:\n                continue\n\n            # If the stamp is up-to-date, this is our best node\n            if stamp == i:\n                S.append(u_top)\n                break\n            else: # stamp  i, so the gain estimate is stale\n                if spread_S  0: # Lazy computation of spread_S\n                    spread_S = estimate_spread(S, graph, n_nodes, R, rng)\n\n                # Recompute marginal gain w.r.t. the current S\n                spread_S_union_u = estimate_spread(S + [u_top], graph, n_nodes, R, rng)\n                new_gain = spread_S_union_u - spread_S\n                \n                # Push back with updated gain and stamp\n                new_stamp = i\n                heapq.heappush(pq, (-new_gain, u_top, new_stamp))\n    \n    return S\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        # 1. Happy-path case\n        {\n            \"n_nodes\": 7,\n            \"edges\": [\n                (0, 1, 0.4), (0, 2, 0.2), (1, 2, 0.4), (2, 0, 0.1), \n                (2, 3, 0.2), (3, 4, 0.3), (4, 5, 0.3), (4, 6, 0.2), \n                (5, 3, 0.3)\n            ],\n            \"k\": 2, \"R\": 800, \"seed\": 20251\n        },\n        # 2. Boundary condition k=0\n        {\n            \"n_nodes\": 7,\n            \"edges\": [\n                (0, 1, 0.4), (0, 2, 0.2), (1, 2, 0.4), (2, 0, 0.1), \n                (2, 3, 0.2), (3, 4, 0.3), (4, 5, 0.3), (4, 6, 0.2), \n                (5, 3, 0.3)\n            ],\n            \"k\": 0, \"R\": 800, \"seed\": 20252\n        },\n        # 3. All-zero probabilities\n        {\n            \"n_nodes\": 5,\n            \"edges\": [\n                (0, 1, 0.0), (0, 2, 0.0), (1, 2, 0.0), (2, 3, 0.0), (3, 4, 0.0)\n            ],\n            \"k\": 3, \"R\": 200, \"seed\": 20253\n        },\n        # 4. Deterministic star\n        {\n            \"n_nodes\": 6,\n            \"edges\": [\n                (0, 1, 1.0), (0, 2, 1.0), (0, 3, 1.0), (0, 4, 1.0), (0, 5, 1.0)\n            ],\n            \"k\": 1, \"R\": 50, \"seed\": 20254\n        },\n        # 5. Disconnected components\n        {\n            \"n_nodes\": 8,\n            \"edges\": [\n                (0, 1, 0.5), (1, 2, 0.5), (2, 3, 0.5),\n                (4, 5, 0.5), (5, 6, 0.5), (6, 7, 0.5)\n            ],\n            \"k\": 2, \"R\": 400, \"seed\": 20255\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = lazy_greedy_selector(\n            case[\"edges\"],\n            case[\"n_nodes\"],\n            case[\"k\"],\n            case[\"R\"],\n            case[\"seed\"]\n        )\n        results.append(result)\n\n    # Format the final output string\n    # e.g., [[0, 4], [], [0, 1, 2], [0], [0, 4]]\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    output_str = output_str.replace(\" \", \"\") # Remove spaces for exact formatting\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}