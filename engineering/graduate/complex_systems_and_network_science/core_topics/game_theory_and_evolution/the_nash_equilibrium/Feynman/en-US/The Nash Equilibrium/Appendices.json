{
    "hands_on_practices": [
        {
            "introduction": "A foundational concept in game theory is the tension between individual self-interest and collective well-being. This exercise provides a concrete, analytical exploration of this tension by comparing the outcome of non-cooperative behavior (the Nash Equilibrium) with the result of full cooperation (the social optimum). By working through a linear-quadratic game, you will practice deriving these distinct solutions from first-order optimality conditions and quantify the efficiency loss, often termed the \"price of anarchy,\" that arises from decentralized decision-making .",
            "id": "3154673",
            "problem": "Consider a two-player linear-quadratic game in which player $i \\in \\{1,2\\}$ chooses a scalar decision variable $x_i \\in \\mathbb{R}$. The cost functions are given by\n$$\nf_i(x_1,x_2) \\;=\\; \\frac{1}{2} \\, q_i \\, x_i^{2} \\;+\\; b_i \\, x_i \\, x_{-i} \\;+\\; c_i \\, x_i,\n$$\nwhere $x_{-i}$ denotes the decision of the other player. Let $q_1 = 2$, $q_2 = 4$, $b_1 = 1$, $b_2 = 1$, $c_1 = -5$, and $c_2 = 1$. Assume $q_i > 0$ for each player so that each player’s optimization problem is strictly convex in their own decision variable.\n\nStarting from the definition of Nash equilibrium (NE), which states that no player can reduce their own cost by a unilateral deviation, and using first-order optimality conditions for differentiable convex functions, derive the block linear system that characterizes the NE and solve for the equilibrium $(x_1^{\\ast}, x_2^{\\ast})$. Then form the centralized optimization problem that minimizes the social cost\n$$\nF(x_1,x_2) \\;=\\; f_1(x_1,x_2) \\;+\\; f_2(x_1,x_2),\n$$\nderive its first-order optimality conditions, and solve for the centralized optimizer $(x_1^{c}, x_2^{c})$.\n\nProvide your final result as the two solutions concatenated into a single row vector $\\big(x_1^{\\ast}, \\, x_2^{\\ast}, \\, x_1^{c}, \\, x_2^{c}\\big)$. No rounding is required.",
            "solution": "The problem requires finding two distinct solutions for a two-player linear-quadratic game: the Nash equilibrium (NE) and the centralized (socially optimal) solution.\n\nFirst, we determine the Nash Equilibrium, denoted by $(x_1^{\\ast}, x_2^{\\ast})$. A Nash equilibrium is a state where neither player can improve their outcome (i.e., lower their cost) by unilaterally changing their own decision, assuming the other player's decision remains fixed. For player $i \\in \\{1,2\\}$ with a cost function $f_i(x_1, x_2)$ that is differentiable and convex in their own decision variable $x_i$, this condition is mathematically expressed by the first-order optimality condition, $\\frac{\\partial f_i}{\\partial x_i} = 0$.\n\nThe cost function for player $i$ is given as:\n$$\nf_i(x_1,x_2) = \\frac{1}{2} q_i x_i^{2} + b_i x_i x_{-i} + c_i x_i\n$$\nwhere $x_{-i}$ is the decision of the other player. The problem states that $q_i > 0$, ensuring that $f_i$ is strictly convex with respect to $x_i$, which guarantees that the first-order condition identifies a unique minimum for player $i$'s optimization problem.\n\nLet's compute the partial derivatives for each player.\nFor player $1$:\n$$\nf_1(x_1,x_2) = \\frac{1}{2} q_1 x_1^{2} + b_1 x_1 x_2 + c_1 x_1\n$$\nThe first-order condition is:\n$$\n\\frac{\\partial f_1}{\\partial x_1} = q_1 x_1 + b_1 x_2 + c_1 = 0\n$$\n\nFor player $2$:\n$$\nf_2(x_1,x_2) = \\frac{1}{2} q_2 x_2^{2} + b_2 x_2 x_1 + c_2 x_2\n$$\nThe first-order condition is:\n$$\n\\frac{\\partial f_2}{\\partial x_2} = q_2 x_2 + b_2 x_1 + c_2 = 0\n$$\n\nThese two equations form a linear system for the Nash equilibrium $(x_1^{\\ast}, x_2^{\\ast})$:\n\\begin{align*}\nq_1 x_1^{\\ast} + b_1 x_2^{\\ast} &= -c_1 \\\\\nb_2 x_1^{\\ast} + q_2 x_2^{\\ast} &= -c_2\n\\end{align*}\nIn matrix form, this is:\n$$\n\\begin{pmatrix} q_1 & b_1 \\\\ b_2 & q_2 \\end{pmatrix} \\begin{pmatrix} x_1^{\\ast} \\\\ x_2^{\\ast} \\end{pmatrix} = \\begin{pmatrix} -c_1 \\\\ -c_2 \\end{pmatrix}\n$$\nWe are given the parameter values: $q_1 = 2$, $q_2 = 4$, $b_1 = 1$, $b_2 = 1$, $c_1 = -5$, and $c_2 = 1$. Substituting these values into the system gives:\n\\begin{align*}\n2 x_1^{\\ast} + 1 x_2^{\\ast} &= -(-5) = 5 \\\\\n1 x_1^{\\ast} + 4 x_2^{\\ast} &= -1\n\\end{align*}\nFrom the first equation, we can express $x_2^{\\ast}$ in terms of $x_1^{\\ast}$:\n$$\nx_2^{\\ast} = 5 - 2 x_1^{\\ast}\n$$\nSubstituting this into the second equation:\n$$\nx_1^{\\ast} + 4(5 - 2 x_1^{\\ast}) = -1\n$$\n$$\nx_1^{\\ast} + 20 - 8 x_1^{\\ast} = -1\n$$\n$$\n-7 x_1^{\\ast} = -21\n$$\n$$\nx_1^{\\ast} = 3\n$$\nNow, we find $x_2^{\\ast}$ by substituting the value of $x_1^{\\ast}$ back:\n$$\nx_2^{\\ast} = 5 - 2(3) = 5 - 6 = -1\n$$\nThus, the Nash equilibrium is $(x_1^{\\ast}, x_2^{\\ast}) = (3, -1)$.\n\nNext, we determine the centralized optimizer, denoted by $(x_1^{c}, x_2^{c})$. This is the pair of decisions that minimizes the total or social cost, $F(x_1,x_2) = f_1(x_1,x_2) + f_2(x_1,x_2)$.\nLet's write out the social cost function $F(x_1,x_2)$:\n$$\nF(x_1,x_2) = \\left(\\frac{1}{2} q_1 x_1^{2} + b_1 x_1 x_2 + c_1 x_1\\right) + \\left(\\frac{1}{2} q_2 x_2^{2} + b_2 x_1 x_2 + c_2 x_2\\right)\n$$\n$$\nF(x_1,x_2) = \\frac{1}{2} q_1 x_1^{2} + \\frac{1}{2} q_2 x_2^{2} + (b_1 + b_2) x_1 x_2 + c_1 x_1 + c_2 x_2\n$$\nThis is a joint optimization problem in the variables $(x_1, x_2)$. To find the minimum, we compute the gradient of $F$ and set it to zero. The first-order conditions are $\\frac{\\partial F}{\\partial x_1} = 0$ and $\\frac{\\partial F}{\\partial x_2} = 0$.\n$$\n\\frac{\\partial F}{\\partial x_1} = q_1 x_1 + (b_1 + b_2) x_2 + c_1 = 0\n$$\n$$\n\\frac{\\partial F}{\\partial x_2} = q_2 x_2 + (b_1 + b_2) x_1 + c_2 = 0\n$$\nThis gives us a system of linear equations for the centralized optimizer $(x_1^{c}, x_2^{c})$:\n\\begin{align*}\nq_1 x_1^{c} + (b_1 + b_2) x_2^{c} &= -c_1 \\\\\n(b_1 + b_2) x_1^{c} + q_2 x_2^{c} &= -c_2\n\\end{align*}\nTo ensure this is a minimum, we can check the Hessian matrix of $F$, which is $H_F = \\begin{pmatrix} q_1 & b_1+b_2 \\\\ b_1+b_2 & q_2 \\end{pmatrix}$. With the given values, $H_F = \\begin{pmatrix} 2 & 2 \\\\ 2 & 4 \\end{pmatrix}$. The principal minors are $2 > 0$ and $\\det(H_F) = 2(4) - 2(2) = 4 > 0$. Since the Hessian is positive definite, $F$ is strictly convex, and the solution to the first-order conditions is the unique global minimum.\n\nSubstituting the parameter values into the system for the centralized solution:\n\\begin{align*}\n2 x_1^{c} + (1+1) x_2^{c} &= -(-5) \\implies 2 x_1^{c} + 2 x_2^{c} = 5 \\\\\n(1+1) x_1^{c} + 4 x_2^{c} &= -1 \\implies 2 x_1^{c} + 4 x_2^{c} = -1\n\\end{align*}\nWe now solve this system. Subtracting the first equation from the second gives:\n$$\n(2 x_1^{c} + 4 x_2^{c}) - (2 x_1^{c} + 2 x_2^{c}) = -1 - 5\n$$\n$$\n2 x_2^{c} = -6\n$$\n$$\nx_2^{c} = -3\n$$\nSubstituting $x_2^{c} = -3$ into the first equation:\n$$\n2 x_1^{c} + 2(-3) = 5\n$$\n$$\n2 x_1^{c} - 6 = 5\n$$\n$$\n2 x_1^{c} = 11\n$$\n$$\nx_1^{c} = \\frac{11}{2}\n$$\nThus, the centralized optimal solution is $(x_1^{c}, x_2^{c}) = (\\frac{11}{2}, -3)$.\n\nThe problem asks for the final result as the concatenated row vector $\\big(x_1^{\\ast}, x_2^{\\ast}, x_1^{c}, x_2^{c}\\big)$.\nThe calculated values are $x_1^{\\ast} = 3$, $x_2^{\\ast} = -1$, $x_1^{c} = \\frac{11}{2}$, and $x_2^{c} = -3$.\nThe final vector is $\\big(3, -1, \\frac{11}{2}, -3\\big)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3 & -1 & \\frac{11}{2} & -3 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond calculating equilibria, it is crucial to understand the logical foundations of different solution concepts. This practice explores the relationship between finding a Nash Equilibrium and the process of Iterated Elimination of Strictly Dominated Strategies (IESDS), a method based on step-by-step rational deductions. Using a classic zero-sum game, you will see that while every Nash Equilibrium strategy survives IESDS, the reverse is not always true, and that a pure-strategy equilibrium is not guaranteed to exist . This highlights the subtle yet important distinctions between solution concepts and motivates the need for more general tools like mixed strategies.",
            "id": "2381522",
            "problem": "Consider the following simultaneous-move static game between two algorithmic traders competing on a limit order book. Each trader chooses one of two execution protocols: $H$ (high-speed execution) or $T$ (time-delay execution). Payoffs capture a zero-sum prediction contest: the row player gains when both traders match and loses when they mismatch, while the column player gains when they mismatch and loses when they match. The normal-form payoff matrix (row player’s payoff first, column player’s payoff second) is:\n$$\n\\begin{array}{c|cc}\n & H & T \\\\\n\\hline\nH & (1,-1) & (-1,1) \\\\\nT & (-1,1) & (1,-1)\n\\end{array}\n$$\nUsing only core definitions from static game theory:\n- A strategy is strictly dominated if there exists another strategy that yields a strictly higher payoff for all possible strategies of the opponent.\n- Iterated elimination of strictly dominated strategies (IESDS) means repeatedly removing strictly dominated strategies until no more such strategies exist.\n- A pure-strategy Nash equilibrium is a strategy profile where each player’s strategy is a best response to the other’s.\n\nTasks:\n1) Determine the set of strategies that survive iterated elimination of strictly dominated strategies (IESDS).\n2) Determine the set of pure-strategy Nash equilibria.\n3) Let $S$ be the number of pure-strategy profiles formed by strategies that survive IESDS, and let $E$ be the number of pure-strategy Nash equilibria. Compute the value of $S - E$.\n\nProvide your final answer as a single real number. No rounding is required.",
            "solution": "The problem statement is validated as being scientifically grounded, well-posed, and objective. It presents a standard problem in elementary game theory based on a classic zero-sum game structure, commonly known as Matching Pennies. The definitions provided are standard, and the required tasks are unambiguous. All information necessary for a solution is present and self-consistent. Therefore, proceeding with a formal solution is appropriate.\n\nThe problem requires a three-part analysis of a given $2 \\times 2$ static game. Let the row player be Player $1$ and the column player be Player $2$. Their strategy sets are $S_1 = \\{H, T\\}$ and $S_2 = \\{H, T\\}$, respectively. The payoff matrix is given as:\n$$\n\\begin{array}{c|cc}\n & H & T \\\\\n\\hline\nH & (1,-1) & (-1,1) \\\\\nT & (-1,1) & (1,-1)\n\\end{array}\n$$\nwhere payoffs are of the form $(u_1, u_2)$.\n\n**Part 1: Iterated Elimination of Strictly Dominated Strategies (IESDS)**\n\nWe must determine if any strategy for either player is strictly dominated. A strategy $s_i'$ for player $i$ is strictly dominated by another strategy $s_i''$ if for every possible strategy profile $s_{-i}$ of the other players, the payoff to player $i$ from playing $s_i'$ is strictly less than the payoff from playing $s_i''$.\n$$ u_i(s_i', s_{-i}) < u_i(s_i'', s_{-i}) \\quad \\forall s_{-i} \\in S_{-i} $$\n\nLet us analyze the Row Player (Player $1$):\n-  Compare strategy $H$ to strategy $T$.\n   - If Player $2$ chooses $H$, Player $1$'s payoff from $H$ is $1$ and from $T$ is $-1$. Here, $u_1(H, H) = 1 > u_1(T, H) = -1$. So, $T$ does not strictly dominate $H$.\n   - If Player $2$ chooses $T$, Player $1$'s payoff from $H$ is $-1$ and from $T$ is $1$. Here, $u_1(H, T) = -1 < u_1(T, T) = 1$. So, $H$ does not strictly dominate $T$.\n- Conclusion for Player $1$: Neither strategy $H$ nor $T$ is strictly dominated.\n\nLet us analyze the Column Player (Player $2$):\n- Compare strategy $H$ to strategy $T$.\n   - If Player $1$ chooses $H$, Player $2$'s payoff from $H$ is $-1$ and from $T$ is $1$. Here, $u_2(H, H) = -1 < u_2(H, T) = 1$. So, $H$ does not strictly dominate $T$.\n   - If Player $1$ chooses $T$, Player $2$'s payoff from $H$ is $1$ and from $T$ is $-1$. Here, $u_2(T, H) = 1 > u_2(T, T) = -1$. So, $T$ does not strictly dominate $H$.\n- Conclusion for Player $2$: Neither strategy $H$ nor $T$ is strictly dominated.\n\nSince no strategies are eliminated in the first round of elimination, the process of IESDS terminates. The set of strategies that survive IESDS is $\\{H, T\\}$ for the Row Player and $\\{H, T\\}$ for the Column Player. The set of pure-strategy profiles formed by these surviving strategies is $\\{(H, H), (H, T), (T, H), (T, T)\\}$. The number of such profiles, denoted by $S$, is $2 \\times 2 = 4$.\n\n**Part 2: Pure-Strategy Nash Equilibria (PSNE)**\n\nA pure-strategy profile is a Nash equilibrium if no player can improve their payoff by unilaterally changing their own strategy. We must examine each of the four possible pure-strategy profiles.\n\n1. Profile $(H, H)$: Payoffs are $(1, -1)$.\n   - Player $1$'s best response to Player $2$ playing $H$ is $H$, since $u_1(H, H) = 1 > u_1(T, H) = -1$. Player $1$ has no incentive to deviate.\n   - Player $2$'s best response to Player $1$ playing $H$ is $T$, since $u_2(H, T) = 1 > u_2(H, H) = -1$. Player $2$ has an incentive to deviate from $H$ to $T$.\n   - Since Player $2$ has a profitable unilateral deviation, $(H, H)$ is not a PSNE.\n\n2. Profile $(H, T)$: Payoffs are $(-1, 1)$.\n   - Player $1$'s best response to Player $2$ playing $T$ is $T$, since $u_1(T, T) = 1 > u_1(H, T) = -1$. Player $1$ has an incentive to deviate from $H$ to $T$.\n   - Since Player $1$ has a profitable unilateral deviation, $(H, T)$ is not a PSNE.\n\n3. Profile $(T, H)$: Payoffs are $(-1, 1)$.\n   - Player $1$'s best response to Player $2$ playing $H$ is $H$, since $u_1(H, H) = 1 > u_1(T, H) = -1$. Player $1$ has an incentive to deviate from $T$ to $H$.\n   - Since Player $1$ has a profitable unilateral deviation, $(T, H)$ is not a PSNE.\n\n4. Profile $(T, T)$: Payoffs are $(1, -1)$.\n   - Player $1$'s best response to Player $2$ playing $T$ is $T$, since $u_1(T, T) = 1 > u_1(H, T) = -1$. Player $1$ has no incentive to deviate.\n   - Player $2$'s best response to Player $1$ playing $T$ is $H$, since $u_2(T, H) = 1 > u_2(T, T) = -1$. Player $2$ has an incentive to deviate from $T$ to $H$.\n   - Since Player $2$ has a profitable unilateral deviation, $(T, T)$ is not a PSNE.\n\nThere are no pure-strategy profiles where neither player has an incentive to unilaterally deviate. Therefore, the set of pure-strategy Nash equilibria is empty. The number of PSNE, denoted by $E$, is $0$.\n\n**Part 3: Calculation of $S - E$**\n\nThe problem asks for the value of $S - E$. From the preceding analysis:\n- The number of pure-strategy profiles formed by strategies that survive IESDS is $S = 4$.\n- The number of pure-strategy Nash equilibria is $E = 0$.\n\nTherefore, the value is computed as:\n$$ S - E = 4 - 0 = 4 $$",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Static equilibrium concepts describe a state of rest, but how do systems arrive at such a state? This computational practice bridges the gap between static theory and dynamic reality by modeling how a population of agents might \"learn\" their way to an equilibrium over time. You will implement a replicator dynamic—a model of imitative learning from evolutionary game theory—within the context of a nonatomic congestion game, a classic model for traffic flow and resource allocation in networks . This exercise provides powerful, hands-on intuition for how stable equilibria can emerge from simple, decentralized adaptive behaviors in complex systems.",
            "id": "4310494",
            "problem": "Consider a nonatomic congestion game on a network consisting of two parallel links connecting a single origin-destination pair. A continuum of agents with total mass normalized to $1$ chooses between link $1$ and link $2$. Let $p \\in [0,1]$ denote the population share using link $1$, so that the link flows are $x_1 = p$ and $x_2 = 1 - p$. Each link $k \\in \\{1,2\\}$ has an affine cost function $c_k(x_k) = a_k + b_k x_k$ with parameters $a_k \\in \\mathbb{R}$ and $b_k \\ge 0$. The payoff to agents choosing link $k$ is $u_k = -c_k(x_k)$ since they minimize cost.\n\nFundamental definitions and base:\n- A Nash equilibrium (Wardrop equilibrium in nonatomic routing games) is a population profile where all used strategies have minimal cost; specifically, when both links are used, the equilibrium condition is $c_1(x_1) = c_2(x_2)$, and when only one link is used, it has weakly lower cost than the other link.\n- In potential games, including congestion games, learning dynamics such as imitative replicator dynamics possess a Lyapunov structure. Consider the discrete-time imitative replicator dynamic defined by the update rule\n$$\np_{t+1} = p_t + \\eta \\, p_t \\, (1 - p_t) \\, \\big(u_1(p_t) - u_2(p_t)\\big),\n$$\nwhere $\\eta > 0$ is a learning-rate parameter and $u_k(p) = -c_k(x_k)$ for $x_1 = p$ and $x_2 = 1 - p$. Stationary points of this dynamic satisfy either $p \\in \\{0,1\\}$ or $u_1(p) = u_2(p)$, which translates to $c_1(p) = c_2(1 - p)$, i.e., the Wardrop equilibrium condition.\n\nTask:\n- Derive the analytical Wardrop equilibrium share $p^\\star$ by equating the costs of the two links when both are used. For $b_1 + b_2 > 0$, the interior equilibrium is obtained by solving\n$$\na_1 + b_1 p^\\star = a_2 + b_2 (1 - p^\\star),\n$$\nwhich yields\n$$\np^\\star = \\frac{a_2 + b_2 - a_1}{b_1 + b_2}.\n$$\nIf the computed $p^\\star$ lies outside $[0,1]$, the equilibrium is at the boundary $p^\\star = 0$ or $p^\\star = 1$, depending on which link has lower cost at the extreme. If $b_1 + b_2 = 0$ and $a_1 = a_2$, then $c_1(x_1) = c_2(x_2)$ for all $p$, and the equilibrium set is the entire interval $[0,1]$; in that degenerate case, any $p$ is a Nash equilibrium.\n- Implement the discrete-time imitative replicator dynamic with the update rule given above. Use $u_1(p) = -\\big(a_1 + b_1 p\\big)$ and $u_2(p) = -\\big(a_2 + b_2 (1 - p)\\big)$, starting from an initial condition $p_0 \\in [0,1]$, and iterate for a specified number of steps $T \\in \\mathbb{N}$. The algorithm should clip iterates to stay within the interval $[0,1]$.\n\nFor each test case listed below, your program must:\n$1.$ Compute the analytical equilibrium $p^\\star$ based on the parameters.\n$2.$ Simulate the replicator dynamic for $T$ steps with learning rate $\\eta$ starting from $p_0$.\n$3.$ Compute the final absolute deviation $|p_T - p^\\star|$. In the degenerate case where $b_1 + b_2 = 0$ and $a_1 = a_2$, treat the equilibrium set as $[0,1]$ and report $0$ for the deviation.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$), where each $r_i$ is the floating-point deviation for the corresponding test case.\n\nTest suite:\nUse the following five test cases, each specified by $(a_1, b_1, a_2, b_2, \\eta, T, p_0)$:\n$1.$ $(0.2, 1.0, 0.8, 0.4, 0.05, 20000, 0.1)$\n$2.$ $(0.0, 1.0, 2.0, 1.0, 0.05, 20000, 0.5)$\n$3.$ $(1.5, 1.0, 0.0, 1.0, 0.05, 20000, 0.8)$\n$4.$ $(1.0, 0.0, 1.0, 0.0, 0.2, 1000, 0.3)$\n$5.$ $(0.0, 1.0, 0.1, 3.0, 0.03, 30000, 0.9)$\n\nFinal output format requirement:\nYour program must print a single line containing a list of five floating-point numbers $[r_1,r_2,r_3,r_4,r_5]$, where $r_i$ is the deviation for test case $i$. No additional text should be printed.",
            "solution": "We begin from core definitions in game theory and network science. In a nonatomic routing game on two parallel links with affine cost functions $c_k(x_k) = a_k + b_k x_k$, agents minimize cost, and the Wardrop equilibrium coincides with a Nash equilibrium: used strategies (links with positive flow) must have equal and minimal cost.\n\nLet $p \\in [0,1]$ denote the population share choosing link $1$, so that $x_1 = p$ and $x_2 = 1 - p$. The costs are $c_1(p) = a_1 + b_1 p$ and $c_2(1 - p) = a_2 + b_2 (1 - p)$. The Nash equilibrium conditions are:\n$1.$ Interior equilibrium: if both links are used, then $c_1(p^\\star) = c_2(1 - p^\\star)$.\n$2.$ Boundary equilibrium: if the interior solution lies outside $[0,1]$, the equilibrium must be at $p^\\star = 0$ or $p^\\star = 1$, depending on which boundary yields lower cost for used strategies.\n\nSolving the interior equation $a_1 + b_1 p^\\star = a_2 + b_2 (1 - p^\\star)$ yields\n$$\np^\\star = \\frac{a_2 + b_2 - a_1}{b_1 + b_2},\n$$\nprovided $b_1 + b_2 > 0$. If $p^\\star \\notin [0,1]$, the equilibrium is at the boundary: $p^\\star = 0$ if link $2$ is strictly cheaper at $p = 0$, and $p^\\star = 1$ if link $1$ is strictly cheaper at $p = 1$. In the degenerate case $b_1 + b_2 = 0$, both costs are constant. If $a_1 = a_2$, then $c_1 = c_2$ for all $p$, and every $p \\in [0,1]$ is a Nash equilibrium; otherwise, the equilibrium is at the boundary with all mass on the cheaper link.\n\nWe now connect learning and adaptive dynamics to equilibrium. The discrete-time imitative replicator dynamic for a two-strategy population game is\n$$\np_{t+1} = p_t + \\eta \\, p_t \\, (1 - p_t) \\, \\big(u_1(p_t) - u_2(p_t)\\big),\n$$\nwhere $\\eta > 0$ is the learning rate, and $u_k(p) = -c_k(x_k)$ are the payoffs equal to the negative costs. Substituting $u_1(p) = -\\big(a_1 + b_1 p\\big)$ and $u_2(p) = -\\big(a_2 + b_2 (1 - p)\\big)$, we obtain the payoff difference:\n$$\nu_1(p) - u_2(p) = c_2(1-p) - c_1(p) = \\big(a_2 + b_2 (1 - p)\\big) - \\big(a_1 + b_1 p\\big) = (a_2+b_2-a_1) - (b_1+b_2)p.\n$$\nThe sign of $u_1(p) - u_2(p)$ mirrors the sign of $c_2(1 - p) - c_1(p)$, and the update moves $p$ in the direction of higher payoff (lower cost). The stationary points of the dynamic satisfy either $p \\in \\{0,1\\}$ or $u_1(p) = u_2(p)$, which is precisely $c_1(p) = c_2(1 - p)$. In potential games such as congestion games, imitative dynamics admit a Lyapunov function related to the potential, and under suitable step sizes, iterates approach the equilibrium set.\n\nAlgorithmic design:\n$1.$ For each test case $(a_1, b_1, a_2, b_2, \\eta, T, p_0)$, compute the analytical equilibrium $p^\\star$:\n$1.1.$ If $b_1 + b_2 > 0$, compute $p^\\star = \\dfrac{a_2 + b_2 - a_1}{b_1 + b_2}$ and project it onto $[0,1]$; if the projection changes the value, that indicates a boundary equilibrium.\n$1.2.$ If $b_1 + b_2 = 0$, check if $a_1 = a_2$. If yes, the equilibrium set is the entire interval $[0,1]$; if not, set $p^\\star = 1$ when $a_1 < a_2$ and $p^\\star = 0$ when $a_2 < a_1$.\n$2.$ Simulate the discrete-time replicator dynamic for $T$ iterations with learning rate $\\eta$ starting from $p_0$:\n$$\np_{t+1} = \\mathrm{clip}\\Big(p_t + \\eta \\, p_t \\, (1 - p_t) \\, \\big(u_1(p_t) - u_2(p_t)\\big), \\, 0, \\, 1\\Big),\n$$\nwhere $\\mathrm{clip}$ ensures $p_{t+1} \\in [0,1]$. This clipping is consistent with the simplex constraint and counteracts numerical drift.\n$3.$ Compute the final deviation $|p_T - p^\\star|$. In the degenerate case with constant and equal costs ($b_1 + b_2 = 0$ and $a_1 = a_2$), the equilibrium set is $[0,1]$; since any $p_T \\in [0,1]$ is an equilibrium, report $0$.\n\nCoverage rationale for the test suite:\n$1.$ $(0.2, 1.0, 0.8, 0.4, 0.05, 20000, 0.1)$ represents a typical interior equilibrium. The analytical solution is $p^\\star = \\dfrac{0.8 + 0.4 - 0.2}{1.0 + 0.4} = \\dfrac{1.0}{1.4} \\approx 0.7142857$, and replicator dynamics should approach this interior solution.\n$2.$ $(0.0, 1.0, 2.0, 1.0, 0.05, 20000, 0.5)$ yields $p^\\star = \\dfrac{2.0 + 1.0 - 0.0}{1.0 + 1.0} = \\dfrac{3.0}{2.0} = 1.5$, which lies outside $[0,1]$, hence the boundary equilibrium $p^\\star = 1$; dynamics should converge to $1$.\n$3.$ $(1.5, 1.0, 0.0, 1.0, 0.05, 20000, 0.8)$ yields $p^\\star = \\dfrac{0.0 + 1.0 - 1.5}{1.0 + 1.0} = \\dfrac{-0.5}{2.0} = -0.25$, hence the boundary equilibrium $p^\\star = 0$; dynamics should converge to $0$.\n$4.$ $(1.0, 0.0, 1.0, 0.0, 0.2, 1000, 0.3)$ has $b_1 + b_2 = 0$ and $a_1 = a_2$, making costs constant and equal for both links; the equilibrium set is the entire interval $[0,1]$, and the dynamic has $u_1(p) - u_2(p) = 0$, so $p_t$ is constant. The deviation reported is $0$.\n$5.$ $(0.0, 1.0, 0.1, 3.0, 0.03, 30000, 0.9)$ has $p^\\star = \\dfrac{0.1 + 3.0 - 0.0}{1.0 + 3.0} = \\dfrac{3.1}{4.0} = 0.775$, an interior solution near the boundary where link slopes are asymmetric; dynamics should converge close to this value.\n\nThe final program carries out these computations deterministically and outputs the deviations as a single comma-separated list enclosed in brackets, ensuring reproducibility and precise numerical answers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_equilibrium(a1, b1, a2, b2):\n    \"\"\"\n    Compute the Wardrop (Nash) equilibrium share p* for a two-link affine congestion game.\n    Returns (p_star, continuum_flag).\n    continuum_flag is True only when b1 + b2 == 0 and a1 == a2 (continuum of equilibria).\n    \"\"\"\n    denom = b1 + b2\n    if denom == 0.0:\n        # Costs are constant. If equal constants, continuum of equilibria.\n        if np.isclose(a1, a2):\n            return None, True  # Any p in [0,1] is equilibrium\n        else:\n            # All mass on cheaper link\n            return (1.0 if a1 < a2 else 0.0), False\n    else:\n        p_star = (a2 + b2 - a1) / denom\n        # Project onto [0,1] due to boundary equilibria\n        if p_star < 0.0:\n            p_star = 0.0\n        elif p_star > 1.0:\n            p_star = 1.0\n        return p_star, False\n\ndef simulate_replicator(a1, b1, a2, b2, eta, T, p0):\n    \"\"\"\n    Simulate discrete-time imitative replicator dynamics for T steps:\n        p_{t+1} = p_t + eta * p_t * (1 - p_t) * (u1(p_t) - u2(p_t)),\n    with u1 = -(a1 + b1 * p), u2 = -(a2 + b2 * (1 - p)).\n    Returns p_T.\n    \"\"\"\n    p = float(p0)\n    for _ in range(T):\n        # Payoffs are negative costs\n        u1 = -(a1 + b1 * p)\n        u2 = -(a2 + b2 * (1.0 - p))\n        delta = eta * p * (1.0 - p) * (u1 - u2)\n        p = p + delta\n        # Clip to [0,1] to respect the simplex constraint\n        if p < 0.0:\n            p = 0.0\n        elif p > 1.0:\n            p = 1.0\n    return p\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a1, b1, a2, b2, eta, T, p0)\n    test_cases = [\n        (0.2, 1.0, 0.8, 0.4, 0.05, 20000, 0.1),\n        (0.0, 1.0, 2.0, 1.0, 0.05, 20000, 0.5),\n        (1.5, 1.0, 0.0, 1.0, 0.05, 20000, 0.8),\n        (1.0, 0.0, 1.0, 0.0, 0.2, 1000, 0.3),\n        (0.0, 1.0, 0.1, 3.0, 0.03, 30000, 0.9),\n    ]\n\n    results = []\n    for a1, b1, a2, b2, eta, T, p0 in test_cases:\n        p_star, continuum = compute_equilibrium(a1, b1, a2, b2)\n        p_T = simulate_replicator(a1, b1, a2, b2, eta, T, p0)\n        if continuum:\n            deviation = 0.0  # Any p in [0,1] is an equilibrium\n        else:\n            deviation = abs(p_T - p_star)\n        # Format with fixed precision for deterministic output\n        results.append(f\"{deviation:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}