{
    "hands_on_practices": [
        {
            "introduction": "A rigorous analysis of system boundaries begins with a formal definition. In probabilistic systems, the Markov blanket provides this foundation, defining the minimal set of variables that shield a target node from the rest of the network. This exercise () challenges you to derive the core implications of this boundary, demonstrating how it induces a factorization of the system's joint probability distribution and enables the construction of an optimal predictive estimator using only local information.",
            "id": "4306340",
            "problem": "Consider a complex system represented by a set of random variables $\\{X_{v}\\}_{v \\in V}$ indexed by the node set $V$ of an undirected network $G=(V,E)$. Fix a node $i \\in V$ and let its Markov blanket $B$ be the minimal subset of $V \\setminus \\{i\\}$ such that $X_{i}$ is conditionally independent of the remaining variables $R := V \\setminus (\\{i\\} \\cup B)$ given $X_{B}$. The Markov blanket encodes a principled system boundary: $X_{i}$ interacts with the environment only through $X_{B}$.\n\nAssume the following foundational properties:\n- By the chain rule of probability, for any partition of variables, joint distributions factor into products of conditionals.\n- By the global Markov property and the Hammersley–Clifford theorem, for strictly positive distributions consistent with $G$, the Markov blanket $B$ separates $i$ from $R$, that is, $X_{i} \\perp X_{R} \\mid X_{B}$.\n\nPart A (boundary-induced factorization). Using only these foundational properties and the definition of the Markov blanket, derive a boundary factorization of the joint distribution of the form\n$$\nP(x) = P(x_{B}) \\, P(x_{i} \\mid x_{B}) \\, P(x_{R} \\mid x_{B}),\n$$\nwhere $x$ denotes a realization of $X_{V}$ and $x_{S}$ the restriction to indices in $S \\subseteq V$.\n\nPart B (blanket-only estimator). Now suppose $X := (X_{v})_{v \\in V}$ follows a zero-mean Gaussian Markov random field consistent with $G$, with density\n$$\nP(x) \\propto \\exp\\!\\left( -\\tfrac{1}{2} \\, x^{\\top} \\Lambda \\, x \\right),\n$$\nwhere $\\Lambda \\in \\mathbb{R}^{|V| \\times |V|}$ is a symmetric positive definite precision matrix whose sparsity pattern matches $G$, meaning $\\Lambda_{ij} = 0$ whenever $(i,j) \\notin E$ and $i \\neq j$. Partition the variables into $x = (x_{i}, x_{B}, x_{R})$ and conformally partition $\\Lambda$ into blocks $(\\Lambda_{ii}, \\Lambda_{iB}, \\Lambda_{iR}, \\Lambda_{BB}, \\Lambda_{BR}, \\Lambda_{RR})$.\n\nDerive the Minimum Mean Squared Error (MMSE) estimator of $X_{i}$ given the blanket variables, $\\widehat{x}_{i}(x_{B}) := \\mathbb{E}[X_{i} \\mid X_{B} = x_{B}]$, and express it as a closed-form analytic expression that depends only on $\\Lambda_{ii}$, $\\Lambda_{iB}$, and $x_{B}$.\n\nYour final answer must be a single closed-form expression for $\\widehat{x}_{i}(x_{B})$ with no units. No numerical rounding is required.",
            "solution": "The problem is concerned with system boundaries and the environment in an undirected networked system. The Markov blanket forms the boundary that shields node $i$ from the rest of the environment $R$ by rendering $X_{i}$ independent of $X_{R}$ given the boundary variables $X_{B}$. We proceed in two parts.\n\nPart A. The boundary-induced factorization follows from foundational rules. By the chain rule of probability, for any partition of $V$ into three disjoint subsets $\\{i\\}$, $B$, and $R$, we have\n$$\nP(x) = P(x_{B}) \\, P(x_{i}, x_{R} \\mid x_{B}) = P(x_{B}) \\, P(x_{i} \\mid x_{B}, x_{R}) \\, P(x_{R} \\mid x_{B}).\n$$\nBy the defining property of the Markov blanket, $X_{i} \\perp X_{R} \\mid X_{B}$, which implies\n$$\nP(x_{i} \\mid x_{B}, x_{R}) = P(x_{i} \\mid x_{B}).\n$$\nSubstituting this equality into the chain rule factorization yields\n$$\nP(x) = P(x_{B}) \\, P(x_{i} \\mid x_{B}) \\, P(x_{R} \\mid x_{B}),\n$$\nwhich is the desired boundary factorization. This exhibits the Markov blanket as a system boundary: the interior variable $X_{i}$ interacts probabilistically with the environment $X_{R}$ only through the boundary $X_{B}$.\n\nPart B. We now derive a blanket-only estimator in the Gaussian Markov random field setting. The random vector $X$ is zero-mean Gaussian with density\n$$\nP(x) \\propto \\exp\\!\\left( -\\tfrac{1}{2} \\, x^{\\top} \\Lambda \\, x \\right),\n$$\nwhere $\\Lambda$ is a symmetric positive definite precision matrix whose sparsity matches the undirected graph $G$. Partition $x$ as $x = (x_{i}, x_{B}, x_{R})$ and write the quadratic form in block coordinates:\n$$\nx^{\\top} \\Lambda x\n= \n\\begin{pmatrix}\nx_{i} & x_{B}^{\\top} & x_{R}^{\\top}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Lambda_{ii} & \\Lambda_{iB} & \\Lambda_{iR} \\\\\n\\Lambda_{Bi} & \\Lambda_{BB} & \\Lambda_{BR} \\\\\n\\Lambda_{Ri} & \\Lambda_{RB} & \\Lambda_{RR} \n\\end{pmatrix}\n\\begin{pmatrix}\nx_{i} \\\\ x_{B} \\\\ x_{R}\n\\end{pmatrix}.\n$$\nExpanding terms that involve $x_{i}$,\n$$\nx^{\\top} \\Lambda x\n=\n\\Lambda_{ii} x_{i}^{2} + 2 x_{i} \\left( \\Lambda_{iB} x_{B} + \\Lambda_{iR} x_{R} \\right) + \\text{terms independent of } x_{i}.\n$$\nThe conditional density $P(x_{i} \\mid x_{B}, x_{R})$ is proportional to the exponentiated negative one-half of this quadratic in $x_{i}$, which is a univariate Gaussian with precision (inverse variance) $\\Lambda_{ii}$ and linear coefficient $\\Lambda_{iB} x_{B} + \\Lambda_{iR} x_{R}$. Completing the square, the conditional mean is\n$$\n\\mathbb{E}[X_{i} \\mid X_{B} = x_{B}, X_{R} = x_{R}]\n= - \\Lambda_{ii}^{-1} \\left( \\Lambda_{iB} x_{B} + \\Lambda_{iR} x_{R} \\right).\n$$\nBy the sparsity property of $\\Lambda$ aligned with $G$, we have $\\Lambda_{iR} = 0$ whenever $(i,r) \\notin E$ for $r \\in R$. Since $R$ contains only non-neighbors of $i$ by construction of $B$ as the neighbor set (Markov blanket) in an undirected graph, it follows that $\\Lambda_{iR} = 0$. Therefore,\n$$\n\\mathbb{E}[X_{i} \\mid X_{B} = x_{B}, X_{R} = x_{R}]\n= - \\Lambda_{ii}^{-1} \\Lambda_{iB} x_{B}.\n$$\nMoreover, the boundary-induced conditional independence $X_{i} \\perp X_{R} \\mid X_{B}$ implies\n$$\n\\mathbb{E}[X_{i} \\mid X_{B} = x_{B}] = \\mathbb{E}[X_{i} \\mid X_{B} = x_{B}, X_{R} = x_{R}]\n$$\nfor any $x_{R}$, so the Minimum Mean Squared Error (MMSE) estimator that uses only the blanket variables is the same expression:\n$$\n\\widehat{x}_{i}(x_{B}) := \\mathbb{E}[X_{i} \\mid X_{B} = x_{B}]\n= - \\Lambda_{ii}^{-1} \\Lambda_{iB} x_{B}.\n$$\nThis estimator depends only on the precision sub-blocks $\\Lambda_{ii}$ and $\\Lambda_{iB}$ and the observed blanket values $x_{B}$, thus operationalizing the Markov blanket as the predictive boundary between the system variable $X_{i}$ and its environment.",
            "answer": "$$\\boxed{-\\Lambda_{ii}^{-1}\\,\\Lambda_{iB}\\,x_{B}}$$"
        },
        {
            "introduction": "Defining a system boundary is not merely a descriptive act; it has profound causal implications. Failing to include critical environmental factors can lead to paradoxical conclusions, where observed associations are the opposite of the true causal effects. This practice () uses the classic case of Simpson's paradox to provide a hands-on demonstration of this pitfall, challenging you to use the principles of structural causal models to expand the system boundary, resolve the paradox, and correctly identify the underlying causal relationship.",
            "id": "4306410",
            "problem": "A researcher studies a binary treatment $T \\in \\{0,1\\}$ and a binary outcome $Y \\in \\{0,1\\}$ in a population composed of two environmental strata $S \\in \\{A,B\\}$. Initially, the system boundary excludes $S$, and only the aggregated association between $T$ and $Y$ is considered. Later, the boundary is expanded to explicitly include $S$ and its causal relationships. The relevant structural relations are represented by a Directed Acyclic Graph (DAG) with arrows $S \\rightarrow T$, $S \\rightarrow Y$, and $T \\rightarrow Y$, consistent with the presence of confounding by $S$.\n\nThe following observational quantities are empirically well estimated:\n- Within-stratum conditional outcome rates:\n  - $P(Y=1 \\mid T=1,S=A) = 0.95$, $P(Y=1 \\mid T=0,S=A) = 0.90$.\n  - $P(Y=1 \\mid T=1,S=B) = 0.35$, $P(Y=1 \\mid T=0,S=B) = 0.30$.\n- Stratum distribution in the target population:\n  - $P(S=A) = 0.5$, $P(S=B) = 0.5$.\n- Observed selection into treatment by stratum:\n  - $P(S=A \\mid T=1) = 0.1$, $P(S=B \\mid T=1) = 0.9$.\n  - $P(S=A \\mid T=0) = 0.9$, $P(S=B \\mid T=0) = 0.1$.\n\nYou may assume a Structural Causal Model (SCM) in which the intervention $\\mathrm{do}(T=t)$ sets $T$ to $t$ and severs incoming causal arrows into $T$, while leaving the distribution of $S$ unchanged. Counterfactual outcomes are denoted by $Y_t$, and causal effects are defined in terms of interventional distributions $P(Y \\mid \\mathrm{do}(T=t))$. The law of total probability and the graphical implications of the DAG apply.\n\nGiven the above, the aggregated association $P(Y=1 \\mid T=1)$ versus $P(Y=1 \\mid T=0)$ exhibits Simpson’s paradox, with a reversal relative to the within-stratum comparisons. Under the expanded system boundary that includes $S$, select the option that correctly constructs the counterfactual demonstrating how the paradox is resolved, by providing both the correct formal expression and the correct numerical value for the causal risk difference $P(Y=1 \\mid \\mathrm{do}(T=1)) - P(Y=1 \\mid \\mathrm{do}(T=0))$ computed from the supplied quantities.\n\nOptions:\n- A. $P(Y=1 \\mid T=1) - P(Y=1 \\mid T=0) = \\left(0.1 \\cdot 0.95 + 0.9 \\cdot 0.35\\right) - \\left(0.9 \\cdot 0.90 + 0.1 \\cdot 0.30\\right) = 0.41 - 0.84 = -0.43$.\n\n- B. $P(Y=1 \\mid \\mathrm{do}(T=1)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=1,S=s)\\,P(S=s \\mid T=1)$ and $P(Y=1 \\mid \\mathrm{do}(T=0)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=0,S=s)\\,P(S=s \\mid T=0)$, yielding $0.41 - 0.84 = -0.43$.\n\n- C. $P(Y=1 \\mid \\mathrm{do}(T=1)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=1,S=s)\\,P(S=s) = 0.5 \\cdot 0.95 + 0.5 \\cdot 0.35 = 0.65$, and $P(Y=1 \\mid \\mathrm{do}(T=0)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=0,S=s)\\,P(S=s) = 0.5 \\cdot 0.90 + 0.5 \\cdot 0.30 = 0.60$, so the risk difference is $0.65 - 0.60 = 0.05$.\n\n- D. $P(Y=1 \\mid \\mathrm{do}(T=1)) - P(Y=1 \\mid \\mathrm{do}(T=0)) = \\left(0.95 - 0.90\\right) + \\left(0.35 - 0.30\\right) = 0.10$.",
            "solution": "The user wants me to solve a problem about Simpson's paradox in the context of causality and structural causal models.\n\nMy role is to be a meticulous STEM professor. This means I must first rigorously validate the problem statement.\n\n**1. Problem Validation**\n\nLet's break down the problem statement and extract all givens.\n\n**Step 1: Extract Givens**\n\n- **System Variables**:\n    - Binary treatment: $T \\in \\{0,1\\}$\n    - Binary outcome: $Y \\in \\{0,1\\}$\n    - Binary environmental stratum: $S \\in \\{A,B\\}$\n\n- **Causal Structure**:\n    - Represented by a Directed Acyclic Graph (DAG) with arrows:\n        - $S \\rightarrow T$ (S is a cause of T, i.e., confounding)\n        - $S \\rightarrow Y$ (S is a cause of Y, i.e., confounding)\n        - $T \\rightarrow Y$ (T is a cause of Y, the direct effect of interest)\n    - This structure implies that $S$ is a common cause of $T$ and $Y$, making it a confounder.\n\n- **Observational Quantities (well estimated)**:\n    - **Within-stratum conditional outcome rates**:\n        - $P(Y=1 \\mid T=1,S=A) = 0.95$\n        - $P(Y=1 \\mid T=0,S=A) = 0.90$\n        - $P(Y=1 \\mid T=1,S=B) = 0.35$\n        - $P(Y=1 \\mid T=0,S=B) = 0.30$\n    - **Stratum distribution in the target population**:\n        - $P(S=A) = 0.5$\n        - $P(S=B) = 0.5$\n    - **Observed selection into treatment by stratum**:\n        - $P(S=A \\mid T=1) = 0.1$\n        - $P(S=B \\mid T=1) = 0.9$\n        - $P(S=A \\mid T=0) = 0.9$\n        - $P(S=B \\mid T=0) = 0.1$\n\n- **Assumptions & Definitions**:\n    - A Structural Causal Model (SCM) is assumed.\n    - The intervention $\\mathrm{do}(T=t)$ sets $T$ to $t$ and severs incoming causal arrows into $T$ (i.e., cuts the $S \\rightarrow T$ arrow).\n    - The intervention leaves the distribution of $S$ unchanged. This is consistent with the DAG, as $T$ is not a cause of $S$.\n    - Counterfactual outcomes are denoted by $Y_t$.\n    - Causal effects are defined by interventional distributions $P(Y \\mid \\mathrm{do}(T=t))$.\n    - The law of total probability and the graphical implications of the DAG apply.\n\n- **Core Problem Task**:\n    - The problem states that the aggregated association $P(Y=1 \\mid T=1)$ vs $P(Y=1 \\mid T=0)$ exhibits Simpson's paradox.\n    - I need to select the option that correctly constructs the counterfactual demonstrating how the paradox is resolved.\n    - This involves finding the correct formal expression and numerical value for the causal risk difference $P(Y=1 \\mid \\mathrm{do}(T=1)) - P(Y=1 \\mid \\mathrm{do}(T=0))$.\n\n**Step 2: Validate Using Extracted Givens**\n\nNow, I'll check for validity based on the criteria.\n\n- **Scientifically Grounded**: The problem is grounded in the well-established fields of causal inference, statistics, and network science (specifically, complex systems). The concepts of DAGs, SCMs, do-calculus, Simpson's paradox, and confounding are standard and rigorous. The problem is a classic example used to illustrate these concepts. No pseudoscience or controversial claims are present.\n\n- **Well-Posed**: The problem asks for a specific quantity, the causal risk difference, and provides sufficient data to calculate it. The causal structure is clearly defined by the DAG. The assumptions of the SCM and the definition of the $\\mathrm{do}$-operator are standard. The goal is clear. A unique solution should exist.\n    - Let me double-check if the provided observational quantities are consistent.\n    - We have $P(S=A \\mid T=t)$ and $P(S=B \\mid T=t)$. Check: $P(S=A \\mid T=1) + P(S=B \\mid T=1) = 0.1 + 0.9 = 1$. This is consistent. $P(S=A \\mid T=0) + P(S=B \\mid T=0) = 0.9 + 0.1 = 1$. This is also consistent.\n    - We are given $P(S=A)=0.5$ and $P(S=B)=0.5$.\n    - Can we derive $P(T=1)$ and $P(T=0)$ from the given information?\n    - Using the law of total probability:\n        - $P(S=A) = P(S=A \\mid T=1)P(T=1) + P(S=A \\mid T=0)P(T=0)$\n        - Let $p_T = P(T=1)$. Then $P(T=0) = 1-p_T$.\n        - $0.5 = (0.1) \\cdot p_T + (0.9) \\cdot (1-p_T)$\n        - $0.5 = 0.1 p_T + 0.9 - 0.9 p_T$\n        - $0.5 = 0.9 - 0.8 p_T$\n        - $0.8 p_T = 0.4$\n        - $p_T = 0.4 / 0.8 = 0.5$.\n    - So, $P(T=1) = 0.5$ and $P(T=0) = 0.5$.\n    - Let's check this with $P(S=B)$:\n        - $P(S=B) = P(S=B \\mid T=1)P(T=1) + P(S=B \\mid T=0)P(T=0)$\n        - $0.5 = (0.9) \\cdot (0.5) + (0.1) \\cdot (0.5)$\n        - $0.5 = 0.45 + 0.05$\n        - $0.5 = 0.5$.\n    - The provided probabilities are internally consistent. This is a very good sign. The problem is well-constructed.\n\n- **Objective**: The language is clear, precise, and objective. It defines all terms ($T$, $Y$, $S$, DAG, do-operator) and provides numerical data. There are no subjective statements.\n\nLet's check for the specific invalidity flaws:\n1. **Scientific Unsoundness**: No, the problem is based on standard causal inference theory.\n2. **Non-Formalizable**: No, it is a formal mathematical problem. It is directly related to the topic of system boundaries and environment in complex systems, as it discusses expanding the boundary to include the confounder $S$.\n3. **Incomplete/Contradictory**: No, as shown above, the data is complete and consistent. We have enough information to calculate both the associational and causal effects.\n4. **Unrealistic/Infeasible**: The probabilities are all between $0$ and $1$. They represent a plausible, though stylized, scenario.\n5. **Ill-Posed**: The question is very specific and asks for a unique quantity. The existence of a confounder $S$ makes the problem meaningful.\n6. **Pseudo-Profound/Trivial**: The problem is a classic, non-trivial illustration of confounding and the difference between association and causation. The reversal of the effect (Simpson's paradox) is a key feature, making it conceptually important. It is not contrived or oversimplified in a way that trivializes the core issue.\n7. **Outside Verifiability**: The problem is a mathematical and logical one, entirely verifiable within its framework.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is a well-formed, consistent, and standard problem in causal inference. I will proceed to the solution phase.\n\n---\n\n**Solution Phase**\n\nThe task is to calculate the causal risk difference, $P(Y=1 \\mid \\mathrm{do}(T=1)) - P(Y=1 \\mid \\mathrm{do}(T=0))$, and identify the correct option.\n\nFirst, let's verify the claim of Simpson's paradox. This requires calculating the aggregated observational association, $P(Y=1 \\mid T=1) - P(Y=1 \\mid T=0)$.\n\n**1. Calculate the Aggregated Observational Association**\n\nThe problem states that the aggregated association exhibits a reversal relative to the within-stratum comparisons. Let's check this.\n\nFirst, the within-stratum risk differences:\n- Stratum A: $P(Y=1 \\mid T=1, S=A) - P(Y=1 \\mid T=0, S=A) = 0.95 - 0.90 = 0.05$. (Positive association)\n- Stratum B: $P(Y=1 \\mid T=1, S=B) - P(Y=1 \\mid T=0, S=B) = 0.35 - 0.30 = 0.05$. (Positive association)\nIn both strata, the treatment is associated with a $0.05$ increase in the probability of the outcome.\n\nNow, calculate the aggregated (marginal) associations, $P(Y=1 \\mid T=1)$ and $P(Y=1 \\mid T=0)$. We use the law of total probability, conditioning on $S$:\n\n$P(Y=1 \\mid T=1) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=1, S=s)P(S=s \\mid T=1)$\nUsing the given values:\n$P(Y=1 \\mid T=1) = P(Y=1 \\mid T=1, S=A)P(S=A \\mid T=1) + P(Y=1 \\mid T=1, S=B)P(S=B \\mid T=1)$\n$P(Y=1 \\mid T=1) = (0.95)(0.1) + (0.35)(0.9)$\n$P(Y=1 \\mid T=1) = 0.095 + 0.315 = 0.41$\n\n$P(Y=1 \\mid T=0) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=0, S=s)P(S=s \\mid T=0)$\nUsing the given values:\n$P(Y=1 \\mid T=0) = P(Y=1 \\mid T=0, S=A)P(S=A \\mid T=0) + P(Y=1 \\mid T=0, S=B)P(S=B \\mid T=0)$\n$P(Y=1 \\mid T=0) = (0.90)(0.9) + (0.30)(0.1)$\n$P(Y=1 \\mid T=0) = 0.81 + 0.03 = 0.84$\n\nThe aggregated risk difference (observational) is:\n$P(Y=1 \\mid T=1) - P(Y=1 \\mid T=0) = 0.41 - 0.84 = -0.43$.\n\nThis is a negative association. Since the within-stratum associations are both positive ($+0.05$), we have confirmed the presence of Simpson's paradox. The aggregated association is reversed compared to the stratum-specific associations.\n\nThe reason for the paradox is clear from the data:\n- The treatment $T=1$ is strongly associated with stratum $B$ ($P(S=B|T=1)=0.9$), where the outcome rates are low (around $0.3$).\n- The control $T=0$ is strongly associated with stratum $A$ ($P(S=A|T=0)=0.9$), where the outcome rates are high (around $0.9$).\nSo, the comparison $P(Y=1|T=1)$ vs $P(Y=1|T=0)$ is not comparing like with like. It's largely comparing outcomes for individuals in stratum B (who are treated) with outcomes for individuals in stratum A (who are not treated). Since stratum A has a much higher baseline risk, this confounding by $S$ creates the illusion that the treatment is harmful.\n\n**2. Calculate the Causal Risk Difference**\n\nTo resolve the paradox and find the true causal effect, we must \"adjust\" or \"control for\" the confounder $S$. The formal way to do this in the SCM framework is to calculate the interventional distributions $P(Y=1 \\mid \\mathrm{do}(T=1))$ and $P(Y=1 \\mid \\mathrm{do}(T=0))$.\n\nThe DAG is $S \\rightarrow T$ and $S \\rightarrow Y$ and $T \\rightarrow Y$. Here, $S$ is a confounder that satisfies the backdoor criterion for adjusting for the effect of $T$ on $Y$. The formula for adjustment (also known as the backdoor adjustment formula or standardization) is:\n$$P(Y=1 \\mid \\mathrm{do}(T=t)) = \\sum_{s} P(Y=1 \\mid T=t, S=s)P(S=s)$$\n\nThis formula constructs a counterfactual scenario. It asks: what would the probability of $Y=1$ be if we forced everyone to have treatment $T=t$, but the distribution of the confounder $S$ remained as it is naturally in the population, i.e., $P(S=s)$?\n\nLet's calculate this for $t=1$ and $t=0$.\n\nFor $t=1$:\n$P(Y=1 \\mid \\mathrm{do}(T=1)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=1, S=s)P(S=s)$\n$P(Y=1 \\mid \\mathrm{do}(T=1)) = P(Y=1 \\mid T=1, S=A)P(S=A) + P(Y=1 \\mid T=1, S=B)P(S=B)$\nUsing the given values:\n$P(Y=1 \\mid \\mathrm{do}(T=1)) = (0.95)(0.5) + (0.35)(0.5)$\n$P(Y=1 \\mid \\mathrm{do}(T=1)) = 0.475 + 0.175 = 0.65$\n\nFor $t=0$:\n$P(Y=1 \\mid \\mathrm{do}(T=0)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=0, S=s)P(S=s)$\n$P(Y=1 \\mid \\mathrm{do}(T=0)) = P(Y=1 \\mid T=0, S=A)P(S=A) + P(Y=1 \\mid T=0, S=B)P(S=B)$\nUsing the given values:\n$P(Y=1 \\mid \\mathrm{do}(T=0)) = (0.90)(0.5) + (0.30)(0.5)$\n$P(Y=1 \\mid \\mathrm{do}(T=0)) = 0.45 + 0.15 = 0.60$\n\nNow, the causal risk difference (also known as the Average Causal Effect, ACE, or Average Treatment Effect, ATE) is:\n$P(Y=1 \\mid \\mathrm{do}(T=1)) - P(Y=1 \\mid \\mathrm{do}(T=0)) = 0.65 - 0.60 = 0.05$.\n\nThis causal effect is positive, matching the sign of the within-stratum associations. This is the resolution of the paradox. The crude observational association was $-0.43$, but the causal effect is $+0.05$. The reversal was due to confounding by $S$.\n\nThis result can also be seen as the weighted average of the stratum-specific risk differences:\nCausal RD $ = \\sum_s [P(Y=1|T=1,S=s) - P(Y=1|T=0,S=s)] P(S=s)$\nCausal RD $ = (0.95 - 0.90)P(S=A) + (0.35 - 0.30)P(S=B)$\nCausal RD $ = (0.05)(0.5) + (0.05)(0.5)$\nCausal RD $ = 0.025 + 0.025 = 0.05$.\nThis confirms the result.\n\n**3. Evaluate the Options**\n\nNow, I will evaluate each option against my derived solution.\n\n- **Option A**: $P(Y=1 \\mid T=1) - P(Y=1 \\mid T=0) = \\left(0.1 \\cdot 0.95 + 0.9 \\cdot 0.35\\right) - \\left(0.9 \\cdot 0.90 + 0.1 \\cdot 0.30\\right) = 0.41 - 0.84 = -0.43$.\n    - The formula used is $P(Y=1 \\mid T=t) = \\sum_s P(Y=1|T=t,S=s)P(S=s|T=t)$. This correctly calculates the *observational* or *associational* risk difference, but not the *causal* risk difference. The problem asks for the counterfactual that resolves the paradox. This option presents the paradox itself, not its resolution.\n    - **Verdict: Incorrect.**\n\n- **Option B**: $P(Y=1 \\mid \\mathrm{do}(T=1)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=1,S=s)\\,P(S=s \\mid T=1)$ and $P(Y=1 \\mid \\mathrm{do}(T=0)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=0,S=s)\\,P(S=s \\mid T=0)$, yielding $0.41 - 0.84 = -0.43$.\n    - This option presents an incorrect formal expression for the interventional probability. It equates the causal quantity $P(Y=1 \\mid \\mathrm{do}(T=t))$ with the observational quantity $P(Y=1 \\mid T=t)$. The correct backdoor adjustment formula requires weighting by the marginal probability of the confounder, $P(S=s)$, not the conditional probability $P(S=s \\mid T=t)$. The use of $P(S=s \\mid T=t)$ is precisely what introduces the confounding bias.\n    - **Verdict: Incorrect.**\n\n- **Option C**: $P(Y=1 \\mid \\mathrm{do}(T=1)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=1,S=s)\\,P(S=s) = 0.5 \\cdot 0.95 + 0.5 \\cdot 0.35 = 0.65$, and $P(Y=1 \\mid \\mathrm{do}(T=0)) = \\sum_{s \\in \\{A,B\\}} P(Y=1 \\mid T=0,S=s)\\,P(S=s) = 0.5 \\cdot 0.90 + 0.5 \\cdot 0.30 = 0.60$, so the risk difference is $0.65 - 0.60 = 0.05$.\n    - This option presents the correct formal expression for the causal effect, which is the backdoor adjustment formula (or standardization). It correctly computes the interventional probabilities by weighting the stratum-specific outcomes by the marginal prevalence of the strata, $P(S=s)$. The numerical calculations are accurate, and the resulting causal risk difference of $0.05$ is correct. This calculation resolves the paradox by demonstrating the true, unconfounded positive effect of the treatment.\n    - **Verdict: Correct.**\n\n- **Option D**: $P(Y=1 \\mid \\mathrm{do}(T=1)) - P(Y=1 \\mid \\mathrm{do}(T=0)) = \\left(0.95 - 0.90\\right) + \\left(0.35 - 0.30\\right) = 0.10$.\n    - This expression incorrectly sums the stratum-specific risk differences. The Average Causal Effect is the *weighted average* of these differences, not their simple sum. The weights must be the probabilities of the strata, $P(S=s)$. The formula $\\sum_s [P(Y=1|T=1,S=s) - P(Y=1|T=0,S=s)]$ has no basis in causal theory and is dimensionally suspect, as it would grow with the number of strata.\n    - **Verdict: Incorrect.**",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "In many real-world complex systems, a complete model is intractable, forcing us to define a simplified boundary that retains the most relevant dynamics. This practice () moves from conceptual definitions to a computational challenge in model reduction. You will implement an algorithm to identify a minimal sufficient boundary—the smallest sub-system that can approximate a key observable within a given error tolerance—by systematically exploring the trade-off between model complexity and predictive fidelity.",
            "id": "4306421",
            "problem": "You are given a finite, weighted, undirected graph with node set $\\{1,2,\\dots,n\\}$ and nonnegative symmetric weights $W_{ij}=W_{ji}\\ge 0$. Consider the linear time-invariant dynamics on $\\mathbb{R}^n$ defined by the ordinary differential equation (ODE) $d\\mathbf{x}/dt = A \\mathbf{x}$, where $A = W - D$ and $D$ is a diagonal matrix with entries $D_{ii} = \\sum_{j=1}^n W_{ij} + \\delta_i$. The quantities $\\delta_i>0$ are per-node decay rates. Let the initial condition be $\\mathbf{x}(0) = \\mathbf{e}_{i^\\star}$, the canonical basis vector centered at a chosen target node $i^\\star \\in \\{1,\\dots,n\\}$. Define the target observable at time $T>0$ as $Y(T) = \\mathbf{e}_{i^\\star}^\\top e^{A T} \\mathbf{e}_{i^\\star}$, where $e^{A T}$ denotes the matrix exponential.\n\nFor a nonnegative integer radius $r$, define the boundary set $B(r)$ as the set of nodes with unweighted shortest-path distance at most $r$ from $i^\\star$. Let $E(r)$ denote the environment, that is, the complement of $B(r)$ in $\\{1,\\dots,n\\}$. Partition $A$ conformably as\n$$\nA = \\begin{pmatrix}\nA_{BB} & A_{BE} \\\\\nA_{EB} & A_{EE}\n\\end{pmatrix}\n$$\nwhere the blocks correspond to the boundary $B(r)$ and the environment $E(r)$. Under a standard adiabatic elimination (fast environment) approximation, the environment is slaved to the boundary according to $\\mathbf{x}_E \\approx -A_{EE}^{-1} A_{EB} \\mathbf{x}_B$, which induces the effective generator on the boundary\n$$\nA_{\\text{eff}}(r) = A_{BB} - A_{BE} A_{EE}^{-1} A_{EB}.\n$$\nDefine the approximation to the observable by\n$$\n\\widehat{Y}(T;r) = \\mathbf{e}_{i^\\star,B}^\\top e^{A_{\\text{eff}}(r) T} \\mathbf{e}_{i^\\star,B},\n$$\nwhere $\\mathbf{e}_{i^\\star,B}$ denotes the restriction of the canonical basis vector to the coordinates indexed by $B(r)$. For a given error tolerance $\\epsilon>0$, a boundary $B(r)$ is sufficient if $|Y(T) - \\widehat{Y}(T;r)| \\le \\epsilon$, and minimal if its cardinality $|B(r^\\star)|$ is smallest among sufficient boundaries.\n\nYour task is to write a complete program that, for each test case below, computes the minimal sufficient boundary cardinality $|B(r^\\star)|$, where\n$$\nr^\\star = \\min\\{r \\in \\{0,1,2,\\dots,n-1\\}: |Y(T) - \\widehat{Y}(T;r)| \\le \\epsilon\\}.\n$$\nIf no radius $r$ in $\\{0,1,2,\\dots,n-1\\}$ satisfies the inequality, define $r^\\star = n-1$.\n\nAll graphs below are unweighted for shortest-path purposes, regardless of $W$, and use $1$-based indexing for node labels. In all cases, distances for decay-rate assignment and boundary shells are computed from $i^\\star$ via unweighted shortest paths. The program should implement the following test suite:\n\n- Test Case $1$ (single chain with three-scale decay):\n    - Graph: a chain of length $n=12$, edges between consecutive nodes, with symmetric weight $W_{i,i+1} = W_{i+1,i} = g$ for $i=1,\\dots,11$, and $W_{ij}=0$ otherwise.\n    - Parameters: $g=0.25$, target node $i^\\star=6$, final time $T=1.0$, tolerance $\\epsilon=10^{-4}$.\n    - Decay rates: Let $d(i)$ be the unweighted shortest-path distance from $i^\\star$ to $i$. Set\n      $\\delta_i = \\delta_{\\text{slow}}$ if $d(i) \\le 1$,\n      $\\delta_i = \\delta_{\\text{medium}}$ if $2 \\le d(i) \\le 3$,\n      and $\\delta_i = \\delta_{\\text{fast}}$ if $d(i) \\ge 4$,\n      with $\\delta_{\\text{slow}}=0.05$, $\\delta_{\\text{medium}}=0.20$, $\\delta_{\\text{fast}}=1.50$.\n\n- Test Case $2$ (longer chain, tighter tolerance):\n    - Graph: a chain of length $n=20$, edges between consecutive nodes, with symmetric weight $W_{i,i+1} = W_{i+1,i} = g$ for $i=1,\\dots,19$, and $W_{ij}=0$ otherwise.\n    - Parameters: $g=0.40$, target node $i^\\star=10$, final time $T=3.0$, tolerance $\\epsilon=10^{-8}$.\n    - Decay rates: Let $d(i)$ be the unweighted shortest-path distance from $i^\\star$ to $i$. Set\n      $\\delta_i = \\delta_{\\text{slow}}$ if $d(i) \\le 1$,\n      $\\delta_i = \\delta_{\\text{medium}}$ if $2 \\le d(i) \\le 4$,\n      and $\\delta_i = \\delta_{\\text{fast}}$ if $d(i) \\ge 5$,\n      with $\\delta_{\\text{slow}}=0.02$, $\\delta_{\\text{medium}}=0.10$, $\\delta_{\\text{fast}}=2.00$.\n\n- Test Case $3$ (two weakly connected chains):\n    - Graph: two disjoint chains connected by a single weak link. Chain A of length $8$ on nodes $1,\\dots,8$ with $W_{i,i+1} = W_{i+1,i} = g_A$ for $i=1,\\dots,7$. Chain B of length $5$ on nodes $9,\\dots,13$ with $W_{j,j+1} = W_{j+1,j} = g_B$ for $j=9,\\dots,12$. A single cross-edge of weight $\\gamma$ connects node $8$ to node $9$. All other $W_{ij}=0$.\n    - Parameters: $g_A=0.30$, $g_B=0.30$, $\\gamma=0.001$, target node $i^\\star=4$, final time $T=1.0$, tolerance $\\epsilon=10^{-3}$.\n    - Decay rates: Let $d(i)$ be the unweighted shortest-path distance from $i^\\star$ to $i$ in the full graph (including the weak link). Set\n      $\\delta_i = \\delta_{\\text{slow}}$ if $d(i) \\le 1$,\n      $\\delta_i = \\delta_{\\text{medium}}$ if $2 \\le d(i) \\le 4$,\n      and $\\delta_i = \\delta_{\\text{fast}}$ if $d(i) \\ge 5$,\n      with $\\delta_{\\text{slow}}=0.05$, $\\delta_{\\text{medium}}=0.20$, $\\delta_{\\text{fast}}=1.20$.\n\nImplementation requirements:\n- Compute $Y(T)$ exactly via the matrix exponential $e^{A T}$.\n- For each radius $r=0,1,\\dots,n-1$, compute $A_{\\text{eff}}(r)$ via $A_{\\text{eff}}(r) = A_{BB} - A_{BE} A_{EE}^{-1} A_{EB}$. If $A_{EE}$ is singular or ill-conditioned, use the Moore–Penrose pseudoinverse in place of $A_{EE}^{-1}$.\n- Compute $\\widehat{Y}(T;r)$ and the error $|Y(T) - \\widehat{Y}(T;r)|$; select the minimal $r$ for which the error is at most $\\epsilon$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$), where each entry is the minimal sufficient boundary cardinality $|B(r^\\star)|$ for the corresponding test case.\n\nAll quantities are dimensionless; angles do not appear; there are no percentages. The final output should be a list of integers.",
            "solution": "The problem requires us to determine the minimal sufficient boundary size for a linear dynamical system on a graph. This is a model reduction problem where we approximate a large system by a smaller, effective model defined on a sub-region (the \"boundary\"). The sufficiency of this approximation is measured by the error in a specific observable, and we seek the smallest boundary that meets a given error tolerance.\n\nThe overall approach involves the following steps for each test case:\n1.  **System Definition**: Construct the mathematical objects representing the system: the weight matrix $W$, the per-node decay rates $\\delta_i$, and the full system dynamics matrix $A = W - D$, where $D_{ii} = (\\sum_j W_{ij}) + \\delta_i$.\n2.  **Shortest-Path Calculation**: Determine the unweighted shortest-path distance $d(i)$ from the target node $i^\\star$ to every other node $i$ in the graph. This is accomplished using a Breadth-First Search (BFS) algorithm, which is optimal for unweighted graphs. These distances are used to define both the decay rates $\\delta_i$ and the concentric boundary sets $B(r)$.\n3.  **Exact Observable Calculation**: Compute the \"exact\" value of the target observable $Y(T) = \\mathbf{e}_{i^\\star}^\\top e^{A T} \\mathbf{e}_{i^\\star}$. This requires computing the matrix exponential $e^{A T}$ of the full system matrix $A$ scaled by the time $T$. The result $Y(T)$ is the diagonal element of this matrix exponential corresponding to the target node $i^\\star$.\n4.  **Iterative Search for Minimal Radius**: Iterate through possible boundary radii $r = 0, 1, 2, \\dots, n-1$. For each radius $r$:\n    a.  **System Partitioning**: The set of all nodes $\\{1, \\dots, n\\}$ is partitioned into a boundary set $B(r) = \\{i \\mid d(i) \\le r\\}$ and an environment set $E(r) = \\{i \\mid d(i) > r\\}$. The system matrix $A$ is partitioned conformably into blocks $A_{BB}$, $A_{BE}$, $A_{EB}$, and $A_{EE}$.\n    b.  **Effective Model Construction**: An effective generator for the boundary dynamics, $A_{\\text{eff}}(r)$, is computed using the formula for adiabatic elimination: $A_{\\text{eff}}(r) = A_{BB} - A_{BE} A_{EE}^{-1} A_{EB}$. To ensure numerical stability and handle cases where the environment block $A_{EE}$ might be singular, its Moore-Penrose pseudoinverse is used in place of the standard inverse $A_{EE}^{-1}$. If the environment set $E(r)$ is empty, the effective model is simply the full system restricted to the boundary, i.e., $A_{\\text{eff}}(r) = A_{BB} = A$, and the approximation becomes exact.\n    c.  **Approximate Observable Calculation**: The approximate observable $\\widehat{Y}(T;r) = \\mathbf{e}_{i^\\star,B}^\\top e^{A_{\\text{eff}}(r) T} \\mathbf{e}_{i^\\star,B}$ is computed. This involves the matrix exponentiation of the smaller effective matrix $A_{\\text{eff}}(r)$ and extracting the diagonal element corresponding to the target node $i^\\star$ within the boundary's local indexing.\n    d.  **Error Evaluation**: The absolute error $|Y(T) - \\widehat{Y}(T;r)|$ is calculated and compared against the specified tolerance $\\epsilon$.\n5.  **Result Determination**: The first radius $r^\\star$ for which the error is less than or equal to $\\epsilon$ is the minimal sufficient radius. Because the boundary set size $|B(r)|$ is a monotonically non-decreasing function of the radius $r$, minimizing $r$ is equivalent to minimizing the boundary cardinality. The final result for the test case is the cardinality of this minimal sufficient boundary, $|B(r^\\star)|$.\n\nThis procedure is systematically applied to each of the three test cases specified. All computations involving matrices and vectors are performed using the `numpy` library. The matrix exponential is computed via `scipy.linalg.expm`, and the pseudoinverse via `numpy.linalg.pinv`. Node indexing, given as $1$-based in the problem, is converted to $0$-based for internal computations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\nfrom numpy.linalg import pinv\nfrom collections import deque\n\ndef _get_shortest_paths_bfs(n, adj_list, start_node_0):\n    \"\"\"Computes unweighted shortest-path distances from a start node using BFS.\"\"\"\n    distances = np.full(n, -1, dtype=int)\n    queue = deque([(start_node_0, 0)])\n    distances[start_node_0] = 0\n\n    while queue:\n        u, d = queue.popleft()\n        for v in adj_list[u]:\n            if distances[v] == -1:\n                distances[v] = d + 1\n                queue.append((v, d + 1))\n    \n    return distances\n\ndef _process_case(n, i_star, T, epsilon, adj_list, W, delta_rules):\n    \"\"\"\n    Solves for the minimal sufficient boundary cardinality for a single test case.\n    \"\"\"\n    # 1. Use 0-based indexing for all internal calculations\n    i_star_0 = i_star - 1\n\n    # 2. Get shortest path distances from the target node\n    distances = _get_shortest_paths_bfs(n, adj_list, i_star_0)\n\n    # 3. Construct the delta vector based on distance rules\n    deltas = np.zeros(n)\n    max_dist = np.max(distances)\n    for min_d, max_d, val in delta_rules:\n        # Handle maximum distance for open-ended rules\n        effective_max_d = max_dist if max_d >= n else max_d\n        indices = np.where((distances >= min_d) & (distances <= effective_max_d))[0]\n        deltas[indices] = val\n\n    # 4. Construct the full system matrix A\n    D = np.diag(np.sum(W, axis=1) + deltas)\n    A = W - D\n\n    # 5. Compute the exact observable Y(T)\n    expm_AT = expm(A * T)\n    Y_T = expm_AT[i_star_0, i_star_0]\n\n    # 6. Iterate through radii r = 0, 1, ..., n-1 to find the minimal sufficient radius\n    for r in range(n):\n        # 7. Define boundary B(r) and environment E(r) index sets\n        B_indices = np.where(distances <= r)[0]\n        E_indices = np.where(distances > r)[0]\n        \n        # Sort for consistent matrix slicing\n        B_indices.sort()\n        E_indices.sort()\n\n        Y_hat_T_r = 0.0 # Initialize approximated observable\n        \n        if E_indices.size == 0:\n            # If the environment is empty, the approximation is exact.\n            Y_hat_T_r = Y_T\n        else:\n            # 8. Partition matrix A and compute the effective generator A_eff\n            A_BB = A[np.ix_(B_indices, B_indices)]\n            A_BE = A[np.ix_(B_indices, E_indices)]\n            A_EB = A[np.ix_(E_indices, B_indices)]\n            A_EE = A[np.ix_(E_indices, E_indices)]\n\n            A_eff = A_BB - A_BE @ pinv(A_EE) @ A_EB\n            \n            # Find the index of the target node within the boundary set\n            i_star_B_idx = np.where(B_indices == i_star_0)[0][0]\n            \n            # 9. Compute the approximate observable Y_hat(T;r)\n            expm_A_eff_T = expm(A_eff * T)\n            Y_hat_T_r = expm_A_eff_T[i_star_B_idx, i_star_B_idx]\n\n        # 10. Check the error condition\n        error = abs(Y_T - Y_hat_T_r)\n        if error <= epsilon:\n            # Found the minimal sufficient radius r*; return its cardinality\n            return len(B_indices)\n\n    # This part should not be reached because for r large enough, E is empty,\n    # error is 0, and the condition is met.\n    return n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = []\n\n    # Test Case 1\n    n1 = 12\n    W1 = np.zeros((n1, n1))\n    adj1 = [[] for _ in range(n1)]\n    for i in range(n1 - 1):\n        W1[i, i + 1] = W1[i + 1, i] = 0.25\n        adj1[i].append(i + 1)\n        adj1[i+1].append(i)\n    case1_params = {\n        \"n\": n1, \"i_star\": 6, \"T\": 1.0, \"epsilon\": 1e-4,\n        \"adj_list\": adj1, \"W\": W1,\n        \"delta_rules\": [(0, 1, 0.05), (2, 3, 0.20), (4, n1, 1.50)]\n    }\n    test_cases.append(case1_params)\n\n    # Test Case 2\n    n2 = 20\n    W2 = np.zeros((n2, n2))\n    adj2 = [[] for _ in range(n2)]\n    for i in range(n2 - 1):\n        W2[i, i + 1] = W2[i + 1, i] = 0.40\n        adj2[i].append(i + 1)\n        adj2[i+1].append(i)\n    case2_params = {\n        \"n\": n2, \"i_star\": 10, \"T\": 3.0, \"epsilon\": 1e-8,\n        \"adj_list\": adj2, \"W\": W2,\n        \"delta_rules\": [(0, 1, 0.02), (2, 4, 0.10), (5, n2, 2.00)]\n    }\n    test_cases.append(case2_params)\n\n    # Test Case 3\n    n3 = 13\n    gA, gB, gamma = 0.30, 0.30, 0.001\n    W3 = np.zeros((n3, n3))\n    adj3 = [[] for _ in range(n3)]\n    # Chain A: nodes 1..8 (indices 0..7)\n    for i in range(7):\n        W3[i, i + 1] = W3[i + 1, i] = gA\n        adj3[i].append(i + 1)\n        adj3[i+1].append(i)\n    # Chain B: nodes 9..13 (indices 8..12)\n    for i in range(8, 12):\n        W3[i, i + 1] = W3[i + 1, i] = gB\n        adj3[i].append(i + 1)\n        adj3[i+1].append(i)\n    # Link: node 8 to 9 (indices 7 to 8)\n    W3[7, 8] = W3[8, 7] = gamma\n    adj3[7].append(8)\n    adj3[8].append(7)\n    case3_params = {\n        \"n\": n3, \"i_star\": 4, \"T\": 1.0, \"epsilon\": 1e-3,\n        \"adj_list\": adj3, \"W\": W3,\n        \"delta_rules\": [(0, 1, 0.05), (2, 4, 0.20), (5, n3, 1.20)]\n    }\n    test_cases.append(case3_params)\n\n    results = []\n    for case_params in test_cases:\n        result = _process_case(**case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}