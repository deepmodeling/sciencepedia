{
    "hands_on_practices": [
        {
            "introduction": "Our first exercise provides a foundational comparison between a simple deterministic evolution and its stochastic counterpart in discrete time. By analyzing the autoregressive AR(1) process, you will see how adding an unobserved noise term transforms a predictable trajectory into a fluctuating stationary process. This practice challenges you to derive the conditions for this stability and quantify the magnitude of the resulting random fluctuations, a core skill in modeling time-series data. ",
            "id": "4304936",
            "problem": "Consider a single node in a large directed network whose coarse-grained state $X_t$ at discrete time $t \\in \\mathbb{Z}$ evolves under the influence of its neighbors and exogenous shocks. Two modeling choices are proposed:\n\n- A stochastic autoregressive process of order $1$ (AR($1$)) with constant drift, defined by a linear recurrence driven by independent and identically distributed (i.i.d.) white noise.\n- A deterministic linear recurrence with the same drift and feedback coefficient but no noise.\n\nYou are asked to formalize both, contrast their qualitative behaviors as models of complex networked dynamics, and then derive the conditions under which the stochastic model is stationary and compute its stationary variance from first principles.\n\nAssume the stochastic model is\n$$\nX_t = \\mu + \\phi X_{t-1} + \\varepsilon_t,\n$$\nwhere $\\mu \\in \\mathbb{R}$ is a constant drift, $\\phi \\in \\mathbb{R}$ is a feedback parameter, and $\\{\\varepsilon_t\\}_{t \\in \\mathbb{Z}}$ is a sequence of independent and identically distributed (i.i.d.) white noise with $\\mathbb{E}[\\varepsilon_t] = 0$ and $\\operatorname{Var}(\\varepsilon_t) = \\sigma_{\\varepsilon}^{2} \\in (0,\\infty)$. The deterministic comparator is\n$$\nY_t = \\mu + \\phi Y_{t-1}.\n$$\nUse core definitions of stationarity (constant mean and time-invariant autocovariance under time shifts for wide-sense stationarity; distributional invariance under time shifts for strict-sense stationarity) and the properties of i.i.d. white noise to derive necessary and sufficient conditions for the existence of a stationary solution to the AR($1$) model and to compute its stationary variance.\n\nYour final reported result must be the closed-form expression for the stationary variance $\\operatorname{Var}(X_t)$ of the AR($1$) model in terms of $\\phi$ and $\\sigma_{\\varepsilon}^{2}$. No rounding is required, and no physical units apply. As part of your derivation in the solution, explicitly state the condition under which stationarity holds and briefly contrast the stochastic AR($1$) recurrence with the deterministic recurrence.",
            "solution": "We begin by formalizing the two recurrences. The stochastic autoregressive process of order $1$ (AR($1$)) is\n$$\nX_t = \\mu + \\phi X_{t-1} + \\varepsilon_t,\n$$\nwith $\\{\\varepsilon_t\\}$ independent and identically distributed (i.i.d.) white noise satisfying $\\mathbb{E}[\\varepsilon_t] = 0$ and $\\operatorname{Var}(\\varepsilon_t) = \\sigma_{\\varepsilon}^{2}$. The deterministic comparator is\n$$\nY_t = \\mu + \\phi Y_{t-1},\n$$\nwhich contains no randomness.\n\nTo contrast the two, observe that $Y_t$ is an ordinary linear difference equation. Iterating gives\n$$\nY_t = \\phi^t Y_0 + \\mu \\sum_{k=0}^{t-1} \\phi^k = \\phi^t Y_0 + \\mu \\frac{1 - \\phi^t}{1 - \\phi}, \\quad \\text{for } \\phi \\neq 1.\n$$\nIf $|\\phi| < 1$, $\\phi^t \\to 0$ as $t \\to \\infty$, so $Y_t$ converges to the fixed point $Y^{\\star} = \\frac{\\mu}{1 - \\phi}$. If $|\\phi| \\geq 1$, $Y_t$ either diverges or grows without bound unless $\\mu = 0$ and specific initial conditions produce trivial behavior. In contrast, the stochastic AR($1$) process is driven by random innovations $\\varepsilon_t$ at each step. Even when it admits a stationary solution, its realizations fluctuate around a constant mean with time-invariant variance and autocovariance determined by $\\phi$ and $\\sigma_{\\varepsilon}^{2}$, rather than converging to a fixed point deterministically.\n\nWe now derive the stationarity conditions and stationary variance for the AR($1$) process. We first consider the mean. Taking expectations and using $\\mathbb{E}[\\varepsilon_t] = 0$:\n$$\n\\mathbb{E}[X_t] = \\mu + \\phi \\mathbb{E}[X_{t-1}].\n$$\nIn a stationary regime, $\\mathbb{E}[X_t] = \\mathbb{E}[X_{t-1}] = m$ for some constant $m$. Solving $m = \\mu + \\phi m$ gives\n$$\nm = \\frac{\\mu}{1 - \\phi}, \\quad \\text{provided } \\phi \\neq 1.\n$$\nFor strict-sense stationarity to exist, the process must be representable by a convergent infinite series of past shocks. Define the backshift operator $B$ so that $B X_t = X_{t-1}$. Then $(1 - \\phi B) X_t = \\mu + \\varepsilon_t$. If $|\\phi| < 1$, the inverse exists as a convergent power series:\n$$\n(1 - \\phi B)^{-1} = \\sum_{k=0}^{\\infty} \\phi^k B^k,\n$$\nyielding the moving-average representation\n$$\nX_t = \\frac{\\mu}{1 - \\phi} + \\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k}.\n$$\nUnder $|\\phi| < 1$ and with $\\{\\varepsilon_t\\}$ i.i.d. possessing finite second moment $\\sigma_{\\varepsilon}^{2}$, the infinite sum converges in mean square, and the distribution of $X_t$ does not depend on $t$ (for example, if $\\varepsilon_t$ is Gaussian, $X_t$ is strictly stationary Gaussian; with general i.i.d. noise of finite variance, $X_t$ is at least wide-sense stationary). Thus, $|\\phi| < 1$ is the necessary and sufficient condition for the existence of a unique stationary solution with finite variance.\n\nWe now compute the stationary variance. Define $v_t = \\operatorname{Var}(X_t)$. Using the recurrence, independence of $\\varepsilon_t$ from $X_{t-1}$, and $\\mathbb{E}[\\varepsilon_t] = 0$:\n$$\n\\operatorname{Var}(X_t) = \\operatorname{Var}(\\mu + \\phi X_{t-1} + \\varepsilon_t) = \\operatorname{Var}(\\phi X_{t-1}) + \\operatorname{Var}(\\varepsilon_t) + 2 \\operatorname{Cov}(\\phi X_{t-1}, \\varepsilon_t).\n$$\nBecause $\\varepsilon_t$ is independent of $X_{t-1}$, $\\operatorname{Cov}(\\phi X_{t-1}, \\varepsilon_t) = 0$, and $\\operatorname{Var}(\\phi X_{t-1}) = \\phi^2 \\operatorname{Var}(X_{t-1})$. Therefore,\n$$\nv_t = \\phi^2 v_{t-1} + \\sigma_{\\varepsilon}^{2}.\n$$\nIn stationarity, $v_t = v_{t-1} = v$ is constant, and we obtain the fixed-point equation\n$$\nv = \\phi^2 v + \\sigma_{\\varepsilon}^{2}.\n$$\nSolving for $v$ yields\n$$\nv = \\frac{\\sigma_{\\varepsilon}^{2}}{1 - \\phi^2},\n$$\nprovided $|\\phi| < 1$. If $|\\phi| \\geq 1$, the variance recursion does not admit a finite fixed point and $v_t$ diverges, consistent with nonstationarity.\n\nThus, under the necessary and sufficient stationarity condition $|\\phi| < 1$, the stationary variance of the AR($1$) process is $\\operatorname{Var}(X_t) = \\frac{\\sigma_{\\varepsilon}^{2}}{1 - \\phi^2}$, which is the requested closed-form expression.",
            "answer": "$$\\boxed{\\frac{\\sigma_{\\varepsilon}^{2}}{1 - \\phi^{2}}}$$"
        },
        {
            "introduction": "Having explored a classic stochastic model, we now investigate a fascinating paradox: a purely deterministic system that exhibits unpredictable, seemingly random behavior. This practice focuses on the logistic map, a famous example of deterministic chaos, and asks you to quantify its unpredictability by deriving its Lyapunov exponent. This exercise demonstrates that long-term unpredictability is not solely the domain of stochastic processes, but can emerge from the nonlinear dynamics of a system itself. ",
            "id": "4305047",
            "problem": "Consider the fully deterministic logistic map on the unit interval defined by the iterated one-dimensional map $f(x)=r\\,x(1-x)$ with $r=4$, so that $x_{n+1}=f(x_n)$ for $n \\in \\mathbb{N}$ and $x_0 \\in (0,1)$. In the study of complex systems and network science, the sign and magnitude of the maximal Lyapunov exponent for such a discrete-time dynamical system quantify the average exponential rate at which nearby trajectories separate and thus provide a principled measure of predictability versus determinism.\n\nStarting from first principles appropriate to one-dimensional differentiable maps—namely, the definition of the maximal Lyapunov exponent as the asymptotic time-average growth rate of infinitesimal perturbations along typical trajectories, together with standard notions of invariant measures and ergodicity—derive an exact, closed-form expression for the maximal Lyapunov exponent of the logistic map at $r=4$. Then, based on the sign and value you obtain, explain concisely how a deterministic update rule can exhibit limited predictability, and contrast this with genuinely stochastic evolution.\n\nYour final numerical result must be a single exact analytic expression (do not approximate and do not round). No units are required for the final answer.",
            "solution": "We are given the logistic map $f(x)=r\\,x(1-x)$ with $r=4$, so $f(x)=4x(1-x)$ on the interval $x \\in (0,1)$. For a one-dimensional differentiable map $f$, the maximal Lyapunov exponent $\\lambda$ for a typical initial condition is defined from first principles as the asymptotic average exponential growth rate of an infinitesimal perturbation under the linearization along the trajectory. Concretely, if $\\delta x_0$ is an infinitesimal perturbation to $x_0$, then after $n$ iterations the linearized perturbation magnitude is approximately\n$$\n|\\delta x_n| \\approx \\left|\\prod_{k=0}^{n-1} f'(x_k)\\right|\\,|\\delta x_0| = \\exp\\!\\left(\\sum_{k=0}^{n-1} \\ln|f'(x_k)|\\right)\\,|\\delta x_0| .\n$$\nThe maximal Lyapunov exponent is defined as\n$$\n\\lambda \\equiv \\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{k=0}^{n-1} \\ln |f'(x_k)| ,\n$$\nfor almost every initial condition $x_0$ with respect to the invariant measure on $(0,1)$. This definition is a standard core concept in dynamical systems and complex systems theory.\n\nFor $f(x)=4x(1-x)$, we have\n$$\nf'(x) = 4(1-2x) .\n$$\nThus, for a trajectory $\\{x_k\\}$,\n$$\n\\ln |f'(x_k)| = \\ln\\!\\big(4|1-2x_k|\\big) = \\ln 4 + \\ln|1-2x_k| .\n$$\n\nTo evaluate the time average, we invoke the ergodic theorem for chaotic one-dimensional maps at $r=4$, which is a well-tested fact: the logistic map at $r=4$ is ergodic on $(0,1)$ with respect to its absolutely continuous invariant measure $\\rho(x)\\,dx$, where the invariant density is\n$$\n\\rho(x) = \\frac{1}{\\pi \\sqrt{x(1-x)}} \\quad \\text{for } x \\in (0,1).\n$$\nBy Birkhoff’s ergodic theorem, for almost every initial condition, the time average equals the space average with respect to $\\rho(x)$. Therefore,\n$$\n\\lambda = \\int_{0}^{1} \\ln|f'(x)|\\, \\rho(x)\\,dx\n= \\int_{0}^{1} \\ln\\!\\big(4|1-2x|\\big)\\, \\frac{dx}{\\pi \\sqrt{x(1-x)}} .\n$$\nWe compute this integral exactly. Write\n$$\n\\lambda = \\int_{0}^{1} \\left(\\ln 4 + \\ln|1-2x|\\right) \\frac{dx}{\\pi \\sqrt{x(1-x)}}\n= \\ln 4 \\int_{0}^{1} \\frac{dx}{\\pi \\sqrt{x(1-x)}} + \\int_{0}^{1} \\frac{\\ln|1-2x|}{\\pi \\sqrt{x(1-x)}}\\,dx .\n$$\nThe first integral is\n$$\n\\int_{0}^{1} \\frac{dx}{\\pi \\sqrt{x(1-x)}} = \\frac{1}{\\pi} \\int_{0}^{1} \\frac{dx}{\\sqrt{x(1-x)}} .\n$$\nUse the substitution $x=\\sin^{2}\\theta$ with $\\theta \\in (0,\\frac{\\pi}{2})$. Then $dx=2\\sin\\theta\\cos\\theta\\, d\\theta$ and $\\sqrt{x(1-x)}=\\sin\\theta\\cos\\theta$, so\n$$\n\\int_{0}^{1} \\frac{dx}{\\sqrt{x(1-x)}} = \\int_{0}^{\\pi/2} \\frac{2\\sin\\theta\\cos\\theta}{\\sin\\theta\\cos\\theta}\\, d\\theta = \\int_{0}^{\\pi/2} 2\\, d\\theta = \\pi .\n$$\nHence the first term contributes\n$$\n\\ln 4 \\cdot \\frac{1}{\\pi} \\cdot \\pi = \\ln 4.\n$$\n\nFor the second term,\n$$\nI \\equiv \\int_{0}^{1} \\frac{\\ln|1-2x|}{\\pi \\sqrt{x(1-x)}}\\,dx .\n$$\nAgain use $x=\\sin^{2}\\theta$, $dx=2\\sin\\theta\\cos\\theta\\, d\\theta$, $\\sqrt{x(1-x)}=\\sin\\theta\\cos\\theta$, $\\theta \\in (0,\\frac{\\pi}{2})$. Note that $1-2x=1-2\\sin^{2}\\theta=\\cos 2\\theta$. Thus,\n$$\nI = \\frac{1}{\\pi} \\int_{0}^{\\pi/2} \\ln|\\cos 2\\theta| \\cdot \\frac{2\\sin\\theta\\cos\\theta}{\\sin\\theta\\cos\\theta}\\, d\\theta\n= \\frac{2}{\\pi} \\int_{0}^{\\pi/2} \\ln|\\cos 2\\theta| \\, d\\theta .\n$$\nLet $u=2\\theta$, so $du=2\\, d\\theta$ and as $\\theta$ goes from $0$ to $\\pi/2$, $u$ goes from $0$ to $\\pi$. Then\n$$\nI = \\frac{2}{\\pi} \\cdot \\frac{1}{2} \\int_{0}^{\\pi} \\ln|\\cos u|\\, du\n= \\frac{1}{\\pi} \\int_{0}^{\\pi} \\ln|\\cos u|\\, du .\n$$\nUsing the evenness and periodicity of $\\cos u$, and the standard integral $\\int_{0}^{\\pi/2} \\ln(\\cos u)\\, du = -\\frac{\\pi}{2}\\ln 2$, we have\n$$\n\\int_{0}^{\\pi} \\ln|\\cos u|\\, du = 2 \\int_{0}^{\\pi/2} \\ln(\\cos u)\\, du = -\\pi \\ln 2 .\n$$\nTherefore,\n$$\nI = \\frac{1}{\\pi} \\cdot \\left(-\\pi \\ln 2\\right) = -\\ln 2 .\n$$\n\nCombining both contributions,\n$$\n\\lambda = \\ln 4 + (-\\ln 2) = \\ln 2 .\n$$\n\nInterpretation in terms of predictability versus determinism: The positive Lyapunov exponent $\\lambda=\\ln 2>0$ means that typical infinitesimal perturbations grow on average like $\\exp(\\lambda n)=2^{n}$ after $n$ iterations. Thus, even though the logistic map is strictly deterministic (the next state is a fixed function of the current state with no stochasticity), the exponential separation of nearby trajectories implies rapidly shrinking predictability horizons under finite precision of initial conditions or measurements. This is the hallmark of deterministic chaos: limited long-term predictability arising from sensitivity to initial conditions and mixing, in contrast to genuinely stochastic processes where randomness is intrinsic to the update rule rather than emergent from deterministic dynamics.",
            "answer": "$$\\boxed{\\ln 2}$$"
        },
        {
            "introduction": "This final practice transitions from discrete-time dynamics to the continuous-time domain, focusing on the Ornstein-Uhlenbeck process, a cornerstone model for systems exhibiting mean-reverting behavior. You will apply the tools of Itô calculus to derive the exact mean and covariance functions directly from the process's defining stochastic differential equation (SDE). Completing this exercise will provide a concrete understanding of how a process's fundamental properties, such as being a Gaussian and Markov process, are encoded within these statistical moments. ",
            "id": "4305116",
            "problem": "Consider the Ornstein–Uhlenbeck (OU) process $\\{X_{t}\\}_{t \\geq 0}$ defined as the unique strong solution to the linear stochastic differential equation (SDE)\n$$\n\\mathrm{d}X_{t} \\;=\\; -\\theta\\bigl(X_{t}-\\mu\\bigr)\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t}, \\qquad t \\geq 0,\n$$\nwhere $\\theta>0$ and $\\sigma>0$ are constants, $\\mu \\in \\mathbb{R}$ is a constant level, and $\\{W_{t}\\}_{t \\geq 0}$ is a standard Brownian motion. Assume $X_{0}$ is independent of $\\{W_{t}\\}_{t \\geq 0}$ and is Gaussian with $X_{0} \\sim \\mathcal{N}(m_{0}, s_{0}^{2})$, where $m_{0} \\in \\mathbb{R}$ and $s_{0}^{2} > 0$.\n\nUsing only foundational facts about linear SDEs driven by Brownian motion, properties of Itô integrals, and the definitions of Gaussian processes and Markov processes, derive closed-form formulas for the mean function $m(t) = \\mathbb{E}[X_{t}]$ and the covariance function $K(s,t) = \\mathrm{Cov}(X_{s}, X_{t})$ for all $s,t \\geq 0$. Then, explain how these functions characterize the Gaussian Markov property of the OU process through the structure of its finite-dimensional distributions and conditional laws.\n\nYour final answer must be a single analytic expression containing the pair $\\bigl(m(t), K(s,t)\\bigr)$ written as a single row matrix using the LaTeX $\\mathrm{pmatrix}$ environment. No numerical approximation or rounding is required.",
            "solution": "The Ornstein–Uhlenbeck (OU) process is defined by the stochastic differential equation (SDE):\n$$\n\\mathrm{d}X_{t} \\;=\\; -\\theta\\bigl(X_{t}-\\mu\\bigr)\\,\\mathrm{d}t \\;+\\; \\sigma\\,\\mathrm{d}W_{t}, \\qquad t \\geq 0\n$$\nwhere $\\theta > 0$, $\\sigma > 0$, $\\mu \\in \\mathbb{R}$ are constants, and $\\{W_{t}\\}_{t \\geq 0}$ is a standard Brownian motion. The initial condition is $X_{0} \\sim \\mathcal{N}(m_{0}, s_{0}^{2})$, independent of $\\{W_{t}\\}_{t \\geq 0}$.\n\nWe first find the explicit solution for $X_t$. The SDE can be rewritten as:\n$$\n\\mathrm{d}X_{t} + \\theta X_{t} \\mathrm{d}t = \\theta\\mu\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}\n$$\nThis is a linear SDE. We use the integrating factor method with the integrating factor $I_t = \\exp(\\theta t)$. Multiplying the SDE by $I_t$ gives:\n$$\n\\exp(\\theta t)\\mathrm{d}X_{t} + \\theta \\exp(\\theta t) X_{t} \\mathrm{d}t = \\theta\\mu \\exp(\\theta t)\\,\\mathrm{d}t + \\sigma \\exp(\\theta t)\\,\\mathrm{d}W_{t}\n$$\nThe left-hand side is the differential of the product $\\exp(\\theta t)X_t$, according to Itô's product rule. So, we have:\n$$\n\\mathrm{d}\\bigl(\\exp(\\theta t)X_t\\bigr) = \\theta\\mu \\exp(\\theta t)\\,\\mathrm{d}t + \\sigma \\exp(\\theta t)\\,\\mathrm{d}W_{t}\n$$\nIntegrating both sides from $0$ to $t$:\n$$\n\\int_{0}^{t} \\mathrm{d}\\bigl(\\exp(\\theta u)X_u\\bigr) = \\int_{0}^{t} \\theta\\mu \\exp(\\theta u)\\,\\mathrm{d}u + \\int_{0}^{t} \\sigma \\exp(\\theta u)\\,\\mathrm{d}W_{u}\n$$\nThis yields:\n$$\n\\exp(\\theta t)X_t - \\exp(0)X_0 = \\theta\\mu \\left[\\frac{1}{\\theta}\\exp(\\theta u)\\right]_{0}^{t} + \\sigma \\int_{0}^{t} \\exp(\\theta u)\\,\\mathrm{d}W_{u}\n$$\n$$\n\\exp(\\theta t)X_t - X_0 = \\mu\\bigl(\\exp(\\theta t) - 1\\bigr) + \\sigma \\int_{0}^{t} \\exp(\\theta u)\\,\\mathrm{d}W_{u}\n$$\nSolving for $X_t$, we get the explicit solution:\n$$\nX_t = X_0\\exp(-\\theta t) + \\mu\\bigl(1 - \\exp(-\\theta t)\\bigr) + \\sigma \\exp(-\\theta t)\\int_{0}^{t} \\exp(\\theta u)\\,\\mathrm{d}W_{u}\n$$\nThe integral term can be rewritten as $\\sigma \\int_{0}^{t} \\exp(-\\theta(t-u))\\,\\mathrm{d}W_{u}$. Thus:\n$$\nX_t = X_0\\exp(-\\theta t) + \\mu\\bigl(1 - \\exp(-\\theta t)\\bigr) + \\sigma \\int_{0}^{t} \\exp(-\\theta(t-u))\\,\\mathrm{d}W_{u}\n$$\n\n**1. Derivation of the Mean Function $m(t)$**\nThe mean function is $m(t) = \\mathbb{E}[X_t]$. Taking the expectation of the solution for $X_t$:\n$$\nm(t) = \\mathbb{E}\\left[X_0\\exp(-\\theta t) + \\mu\\bigl(1 - \\exp(-\\theta t)\\bigr) + \\sigma \\int_{0}^{t} \\exp(-\\theta(t-u))\\,\\mathrm{d}W_{u}\\right]\n$$\nBy linearity of expectation:\n$$\nm(t) = \\mathbb{E}[X_0]\\exp(-\\theta t) + \\mu\\bigl(1 - \\exp(-\\theta t)\\bigr) + \\sigma \\mathbb{E}\\left[\\int_{0}^{t} \\exp(-\\theta(t-u))\\,\\mathrm{d}W_{u}\\right]\n$$\nWe use two key facts:\n- The initial mean is given as $\\mathbb{E}[X_0] = m_0$.\n- The expectation of an Itô integral with a deterministic integrand is zero: $\\mathbb{E}[\\int_{0}^{t} f(u)\\,\\mathrm{d}W_{u}] = 0$. Here, $f(u) = \\exp(-\\theta(t-u))$ is deterministic.\nSubstituting these into the equation for $m(t)$:\n$$\nm(t) = m_0\\exp(-\\theta t) + \\mu\\bigl(1 - \\exp(-\\theta t)\\bigr)\n$$\n\n**2. Derivation of the Covariance Function $K(s,t)$**\nThe covariance function is $K(s,t) = \\mathrm{Cov}(X_s, X_t) = \\mathbb{E}[(X_s - m(s))(X_t - m(t))]$. First, let's find the centered process $X_t - m(t)$:\n$$\nX_t - m(t) = (X_0 - m_0)\\exp(-\\theta t) + \\sigma \\int_{0}^{t} \\exp(-\\theta(t-u))\\,\\mathrm{d}W_{u}\n$$\nWithout loss of generality, assume $s \\leq t$. The covariance is then:\n$$\nK(s,t) = \\mathbb{E}\\Biggl[\\left((X_0 - m_0)\\exp(-\\theta s) + \\sigma \\int_{0}^{s} \\exp(-\\theta(s-u))\\,\\mathrm{d}W_{u}\\right) \\times \\left((X_0 - m_0)\\exp(-\\theta t) + \\sigma \\int_{0}^{t} \\exp(-\\theta(t-u))\\,\\mathrm{d}W_{u}\\right)\\Biggr]\n$$\nWe expand the product. Since $X_0$ is independent of $\\{W_u\\}_{u \\geq 0}$, any term involving a product of $(X_0-m_0)$ and an Itô integral will have an expectation of zero. For example, $\\mathbb{E}[(X_0-m_0) \\int_{0}^{t} \\dots \\mathrm{d}W_u] = \\mathbb{E}[X_0-m_0]\\mathbb{E}[\\int_{0}^{t} \\dots \\mathrm{d}W_u] = 0 \\times 0 = 0$. Thus, only two terms survive:\n$$\nK(s,t) = \\mathbb{E}\\left[(X_0 - m_0)^2\\right]\\exp(-\\theta(s+t)) + \\sigma^2 \\mathbb{E}\\left[\\left(\\int_{0}^{s} e^{-\\theta(s-u)}\\,\\mathrm{d}W_{u}\\right)\\left(\\int_{0}^{t} e^{-\\theta(t-v)}\\,\\mathrm{d}W_{v}\\right)\\right]\n$$\nThe first term is $\\mathrm{Var}(X_0)\\exp(-\\theta(s+t)) = s_0^2\\exp(-\\theta(s+t))$.\nFor the second term, we use the Itô isometry property, $\\mathbb{E}[(\\int_0^T f(u)\\,\\mathrm{d}W_u)(\\int_0^T g(u)\\,\\mathrm{d}W_u)] = \\int_0^T f(u)g(u)\\,\\mathrm{d}u$. Since $s \\leq t$, the common interval of integration for the stochastic integrals is $[0, s]$.\n$$\n\\mathbb{E}\\left[\\left(\\int_{0}^{s} \\dots \\right)\\left(\\int_{0}^{t} \\dots \\right)\\right] = \\mathbb{E}\\left[\\left(\\int_{0}^{s} e^{-\\theta(s-u)}\\,\\mathrm{d}W_{u}\\right)\\left(\\int_{0}^{s} e^{-\\theta(t-u)}\\,\\mathrm{d}W_{u}\\right)\\right] = \\int_{0}^{s} e^{-\\theta(s-u)}e^{-\\theta(t-u)}\\,\\mathrm{d}u\n$$\nWe evaluate this integral:\n$$\n\\int_{0}^{s} e^{-\\theta(s+t-2u)}\\,\\mathrm{d}u = e^{-\\theta(s+t)}\\int_{0}^{s} e^{2\\theta u}\\,\\mathrm{d}u = e^{-\\theta(s+t)}\\left[\\frac{1}{2\\theta}e^{2\\theta u}\\right]_{0}^{s} = \\frac{e^{-\\theta(s+t)}}{2\\theta}\\left(e^{2\\theta s}-1\\right) = \\frac{1}{2\\theta}\\left(e^{-\\theta(t-s)} - e^{-\\theta(s+t)}\\right)\n$$\nCombining the terms, for $s \\leq t$:\n$$\nK(s,t) = s_0^2\\exp(-\\theta(s+t)) + \\frac{\\sigma^2}{2\\theta}\\left(\\exp(-\\theta(t-s)) - \\exp(-\\theta(s+t))\\right)\n$$\n$$\nK(s,t) = \\frac{\\sigma^2}{2\\theta}\\exp(-\\theta(t-s)) + \\left(s_0^2 - \\frac{\\sigma^2}{2\\theta}\\right)\\exp(-\\theta(s+t))\n$$\nSince the covariance function must be symmetric, i.e., $K(s,t) = K(t,s)$, we can write the general expression using the absolute value $|t-s|$:\n$$\nK(s,t) = \\frac{\\sigma^2}{2\\theta}\\exp(-\\theta|t-s|) + \\left(s_0^2 - \\frac{\\sigma^2}{2\\theta}\\right)\\exp(-\\theta(s+t))\n$$\n\n**3. Characterization of the Gaussian Markov Property**\nThe derived mean function $m(t)$ and covariance function $K(s,t)$ fully characterize the OU process and reveal its fundamental properties.\n\nFirst, the process $\\{X_t\\}$ is a **Gaussian process**. A process is Gaussian if, for any finite set of times $t_1, \\dots, t_n$, the random vector $(X_{t_1}, \\dots, X_{t_n})$ has a multivariate normal distribution. From the explicit solution, $X_t$ is the sum of a deterministic function of time, the Gaussian random variable $X_0\\exp(-\\theta t)$, and the Itô integral term. The Itô integral with a deterministic integrand is known to be a Gaussian random variable. Since $X_0$ is independent of the Brownian motion, $X_t$ is a sum of two independent Gaussian variables (plus a deterministic shift), which is itself Gaussian. Any linear combination of $X_{t_i}$'s will similarly be a Gaussian random variable. The distribution of any such vector is therefore uniquely determined by its mean vector, with elements $m(t_i)$, and its covariance matrix, with elements $K(t_i, t_j)$.\n\nSecond, the process $\\{X_t\\}$ is a **Markov process**. For a Gaussian process, the Markov property is equivalent to a specific condition on its covariance function. A zero-mean continuous-time Gaussian process $\\{Y_t\\}$ is Markov if and only if its covariance function $K(s,t) = \\mathbb{E}[Y_s Y_t]$ satisfies the relation $K(s,u) = K(s,t)K(t,u)/K(t,t)$ for any ordered times $s < t < u$. Our process $X_t$ is not zero-mean, but the centered process $Y_t = X_t - m(t)$ is a zero-mean Gaussian process with the covariance function $K(s,t)$ we derived. We verify the Markov condition: $K(s,u)K(t,t) = K(s,t)K(t,u)$. Let $A = \\frac{\\sigma^2}{2\\theta}$ and $C = s_0^2 - \\frac{\\sigma^2}{2\\theta}$. For $s<t<u$:\n$K(s,t) = A\\exp(-\\theta(t-s)) + C\\exp(-\\theta(s+t))$.\n$K(t,u) = A\\exp(-\\theta(u-t)) + C\\exp(-\\theta(t+u))$.\n$K(s,u) = A\\exp(-\\theta(u-s)) + C\\exp(-\\theta(s+u))$.\n$K(t,t) = A + C\\exp(-2\\theta t)$.\n\nLet's compute the product $K(s,t)K(t,u)$:\n$$\nK(s,t)K(t,u) = \\left(A e^{-\\theta(t-s)} + C e^{-\\theta(s+t)}\\right)\\left(A e^{-\\theta(u-t)} + C e^{-\\theta(t+u)}\\right)\n$$\n$$\n= A^2 e^{-\\theta(u-s)} + AC e^{-\\theta(t-s)}e^{-\\theta(t+u)} + AC e^{-\\theta(s+t)}e^{-\\theta(u-t)} + C^2 e^{-\\theta(s+t)}e^{-\\theta(t+u)}\n$$\n$$\n= A^2 e^{-\\theta(u-s)} + AC e^{-\\theta(u-s+2t)} + AC e^{-\\theta(s+u)} + C^2 e^{-\\theta(s+u+2t)}\n$$\nNow, let's compute the product $K(s,u)K(t,t)$:\n$$\nK(s,u)K(t,t) = \\left(A e^{-\\theta(u-s)} + C e^{-\\theta(s+u)}\\right)\\left(A + C e^{-2\\theta t}\\right)\n$$\n$$\n= A^2 e^{-\\theta(u-s)} + AC e^{-\\theta(u-s)}e^{-2\\theta t} + AC e^{-\\theta(s+u)} + C^2 e^{-\\theta(s+u)}e^{-2\\theta t}\n$$\n$$\n= A^2 e^{-\\theta(u-s)} + AC e^{-\\theta(u-s+2t)} + AC e^{-\\theta(s+u)} + C^2 e^{-\\theta(s+u+2t)}\n$$\nThe two expressions are identical. This confirms that $K(s,u)K(t,t) = K(s,t)K(t,u)$, which is the condition for a Gaussian process to be Markov. Thus, the specific functional form of $K(s,t)$ derived from the SDE is precisely what ensures the Markov property of the OU process. The functions $m(t)$ and $K(s,t)$ together provide a complete statistical description of the OU process and confirm it is a Gaussian Markov process.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} m_0 \\exp(-\\theta t) + \\mu(1 - \\exp(-\\theta t)) & \\frac{\\sigma^2}{2\\theta} \\exp(-\\theta|t-s|) + \\left(s_0^2 - \\frac{\\sigma^2}{2\\theta}\\right) \\exp(-\\theta(s+t)) \\end{pmatrix}}\n$$"
        }
    ]
}