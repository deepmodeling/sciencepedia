{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration, we start with the foundational building blocks of discrete-time dynamics. This exercise directly contrasts a simple deterministic linear recurrence with its stochastic counterpart, the first-order autoregressive AR(1) process. By deriving the conditions under which the stochastic model achieves stationarity and computing its characteristic variance, you will gain a first-principles understanding of how random shocks transform the system's behavior, leading to sustained fluctuations rather than simple convergence or divergence .",
            "id": "4304936",
            "problem": "Consider a single node in a large directed network whose coarse-grained state $X_t$ at discrete time $t \\in \\mathbb{Z}$ evolves under the influence of its neighbors and exogenous shocks. Two modeling choices are proposed:\n\n- A stochastic autoregressive process of order $1$ (AR($1$)) with constant drift, defined by a linear recurrence driven by independent and identically distributed (i.i.d.) white noise.\n- A deterministic linear recurrence with the same drift and feedback coefficient but no noise.\n\nYou are asked to formalize both, contrast their qualitative behaviors as models of complex networked dynamics, and then derive the conditions under which the stochastic model is stationary and compute its stationary variance from first principles.\n\nAssume the stochastic model is\n$$\nX_t = \\mu + \\phi X_{t-1} + \\varepsilon_t,\n$$\nwhere $\\mu \\in \\mathbb{R}$ is a constant drift, $\\phi \\in \\mathbb{R}$ is a feedback parameter, and $\\{\\varepsilon_t\\}_{t \\in \\mathbb{Z}}$ is a sequence of independent and identically distributed (i.i.d.) white noise with $\\mathbb{E}[\\varepsilon_t] = 0$ and $\\operatorname{Var}(\\varepsilon_t) = \\sigma_{\\varepsilon}^{2} \\in (0,\\infty)$. The deterministic comparator is\n$$\nY_t = \\mu + \\phi Y_{t-1}.\n$$\nUse core definitions of stationarity (constant mean and time-invariant autocovariance under time shifts for wide-sense stationarity; distributional invariance under time shifts for strict-sense stationarity) and the properties of i.i.d. white noise to derive necessary and sufficient conditions for the existence of a stationary solution to the AR($1$) model and to compute its stationary variance.\n\nYour final reported result must be the closed-form expression for the stationary variance $\\operatorname{Var}(X_t)$ of the AR($1$) model in terms of $\\phi$ and $\\sigma_{\\varepsilon}^{2}$. No rounding is required, and no physical units apply. As part of your derivation in the solution, explicitly state the condition under which stationarity holds and briefly contrast the stochastic AR($1$) recurrence with the deterministic recurrence.",
            "solution": "We begin by formalizing the two recurrences. The stochastic autoregressive process of order $1$ (AR($1$)) is\n$$\nX_t = \\mu + \\phi X_{t-1} + \\varepsilon_t,\n$$\nwith $\\{\\varepsilon_t\\}$ independent and identically distributed (i.i.d.) white noise satisfying $\\mathbb{E}[\\varepsilon_t] = 0$ and $\\operatorname{Var}(\\varepsilon_t) = \\sigma_{\\varepsilon}^{2}$. The deterministic comparator is\n$$\nY_t = \\mu + \\phi Y_{t-1},\n$$\nwhich contains no randomness.\n\nTo contrast the two, observe that $Y_t$ is an ordinary linear difference equation. Iterating gives\n$$\nY_t = \\phi^t Y_0 + \\mu \\sum_{k=0}^{t-1} \\phi^k = \\phi^t Y_0 + \\mu \\frac{1 - \\phi^t}{1 - \\phi}, \\quad \\text{for } \\phi \\neq 1.\n$$\nIf $|\\phi| < 1$, $\\phi^t \\to 0$ as $t \\to \\infty$, so $Y_t$ converges to the fixed point $Y^{\\star} = \\frac{\\mu}{1 - \\phi}$. If $|\\phi| \\geq 1$, $Y_t$ either diverges or grows without bound unless $\\mu = 0$ and specific initial conditions produce trivial behavior. In contrast, the stochastic AR($1$) process is driven by random innovations $\\varepsilon_t$ at each step. Even when it admits a stationary solution, its realizations fluctuate around a constant mean with time-invariant variance and autocovariance determined by $\\phi$ and $\\sigma_{\\varepsilon}^{2}$, rather than converging to a fixed point deterministically.\n\nWe now derive the stationarity conditions and stationary variance for the AR($1$) process. We first consider the mean. Taking expectations and using $\\mathbb{E}[\\varepsilon_t] = 0$:\n$$\n\\mathbb{E}[X_t] = \\mu + \\phi \\mathbb{E}[X_{t-1}].\n$$\nIn a stationary regime, $\\mathbb{E}[X_t] = \\mathbb{E}[X_{t-1}] = m$ for some constant $m$. Solving $m = \\mu + \\phi m$ gives\n$$\nm = \\frac{\\mu}{1 - \\phi}, \\quad \\text{provided } \\phi \\neq 1.\n$$\nFor strict-sense stationarity to exist, the process must be representable by a convergent infinite series of past shocks. Define the backshift operator $B$ so that $B X_t = X_{t-1}$. Then $(1 - \\phi B) X_t = \\mu + \\varepsilon_t$. If $|\\phi| < 1$, the inverse exists as a convergent power series:\n$$\n(1 - \\phi B)^{-1} = \\sum_{k=0}^{\\infty} \\phi^k B^k,\n$$\nyielding the moving-average representation\n$$\nX_t = \\frac{\\mu}{1 - \\phi} + \\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k}.\n$$\nUnder $|\\phi| < 1$ and with $\\{\\varepsilon_t\\}$ i.i.d. possessing finite second moment $\\sigma_{\\varepsilon}^{2}$, the infinite sum converges in mean square, and the distribution of $X_t$ does not depend on $t$ (for example, if $\\varepsilon_t$ is Gaussian, $X_t$ is strictly stationary Gaussian; with general i.i.d. noise of finite variance, $X_t$ is at least wide-sense stationary). Thus, $|\\phi| < 1$ is the necessary and sufficient condition for the existence of a unique stationary solution with finite variance.\n\nWe now compute the stationary variance. Define $v_t = \\operatorname{Var}(X_t)$. Using the recurrence, independence of $\\varepsilon_t$ from $X_{t-1}$, and $\\mathbb{E}[\\varepsilon_t] = 0$:\n$$\n\\operatorname{Var}(X_t) = \\operatorname{Var}(\\mu + \\phi X_{t-1} + \\varepsilon_t) = \\operatorname{Var}(\\phi X_{t-1}) + \\operatorname{Var}(\\varepsilon_t) + 2 \\operatorname{Cov}(\\phi X_{t-1}, \\varepsilon_t).\n$$\nBecause $\\varepsilon_t$ is independent of $X_{t-1}$, $\\operatorname{Cov}(\\phi X_{t-1}, \\varepsilon_t) = 0$, and $\\operatorname{Var}(\\phi X_{t-1}) = \\phi^2 \\operatorname{Var}(X_{t-1})$. Therefore,\n$$\nv_t = \\phi^2 v_{t-1} + \\sigma_{\\varepsilon}^{2}.\n$$\nIn stationarity, $v_t = v_{t-1} = v$ is constant, and we obtain the fixed-point equation\n$$\nv = \\phi^2 v + \\sigma_{\\varepsilon}^{2}.\n$$\nSolving for $v$ yields\n$$\nv = \\frac{\\sigma_{\\varepsilon}^{2}}{1 - \\phi^2},\n$$\nprovided $|\\phi| < 1$. If $|\\phi| \\geq 1$, the variance recursion does not admit a finite fixed point and $v_t$ diverges, consistent with nonstationarity.\n\nThus, under the necessary and sufficient stationarity condition $|\\phi| < 1$, the stationary variance of the AR($1$) process is $\\operatorname{Var}(X_t) = \\frac{\\sigma_{\\varepsilon}^{2}}{1 - \\phi^2}$, which is the requested closed-form expression.",
            "answer": "$$\\boxed{\\frac{\\sigma_{\\varepsilon}^{2}}{1 - \\phi^{2}}}$$"
        },
        {
            "introduction": "After establishing the fundamental differences between simple deterministic and stochastic models, we now investigate a more subtle phenomenon: the emergence of unpredictable behavior from purely deterministic rules. This practice challenges you to analyze the iconic logistic map, a classic example of deterministic chaos, and calculate its Lyapunov exponent, $\\lambda$, for a fully chaotic parameter regime. Completing this derivation will demonstrate that a lack of long-term predictability is not exclusively a feature of stochastic processes, introducing the critical concept of sensitive dependence on initial conditions .",
            "id": "4305047",
            "problem": "Consider the fully deterministic logistic map on the unit interval defined by the iterated one-dimensional map $f(x)=r\\,x(1-x)$ with $r=4$, so that $x_{n+1}=f(x_n)$ for $n \\in \\mathbb{N}$ and $x_0 \\in (0,1)$. In the study of complex systems and network science, the sign and magnitude of the maximal Lyapunov exponent for such a discrete-time dynamical system quantify the average exponential rate at which nearby trajectories separate and thus provide a principled measure of predictability versus determinism.\n\nStarting from first principles appropriate to one-dimensional differentiable maps—namely, the definition of the maximal Lyapunov exponent as the asymptotic time-average growth rate of infinitesimal perturbations along typical trajectories, together with standard notions of invariant measures and ergodicity—derive an exact, closed-form expression for the maximal Lyapunov exponent of the logistic map at $r=4$. Then, based on the sign and value you obtain, explain concisely how a deterministic update rule can exhibit limited predictability, and contrast this with genuinely stochastic evolution.\n\nYour final numerical result must be a single exact analytic expression (do not approximate and do not round). No units are required for the final answer.",
            "solution": "We are given the logistic map $f(x)=r\\,x(1-x)$ with $r=4$, so $f(x)=4x(1-x)$ on the interval $x \\in (0,1)$. For a one-dimensional differentiable map $f$, the maximal Lyapunov exponent $\\lambda$ for a typical initial condition is defined from first principles as the asymptotic average exponential growth rate of an infinitesimal perturbation under the linearization along the trajectory. Concretely, if $\\delta x_0$ is an infinitesimal perturbation to $x_0$, then after $n$ iterations the linearized perturbation magnitude is approximately\n$$\n|\\delta x_n| \\approx \\left|\\prod_{k=0}^{n-1} f'(x_k)\\right|\\,|\\delta x_0| = \\exp\\!\\left(\\sum_{k=0}^{n-1} \\ln|f'(x_k)|\\right)\\,|\\delta x_0| .\n$$\nThe maximal Lyapunov exponent is defined as\n$$\n\\lambda \\equiv \\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{k=0}^{n-1} \\ln |f'(x_k)| ,\n$$\nfor almost every initial condition $x_0$ with respect to the invariant measure on $(0,1)$. This definition is a standard core concept in dynamical systems and complex systems theory.\n\nFor $f(x)=4x(1-x)$, we have\n$$\nf'(x) = 4(1-2x) .\n$$\nThus, for a trajectory $\\{x_k\\}$,\n$$\n\\ln |f'(x_k)| = \\ln\\!\\big(4|1-2x_k|\\big) = \\ln 4 + \\ln|1-2x_k| .\n$$\n\nTo evaluate the time average, we invoke the ergodic theorem for chaotic one-dimensional maps at $r=4$, which is a well-tested fact: the logistic map at $r=4$ is ergodic on $(0,1)$ with respect to its absolutely continuous invariant measure $\\rho(x)\\,dx$, where the invariant density is\n$$\n\\rho(x) = \\frac{1}{\\pi \\sqrt{x(1-x)}} \\quad \\text{for } x \\in (0,1).\n$$\nBy Birkhoff’s ergodic theorem, for almost every initial condition, the time average equals the space average with respect to $\\rho(x)$. Therefore,\n$$\n\\lambda = \\int_{0}^{1} \\ln|f'(x)|\\, \\rho(x)\\,dx\n= \\int_{0}^{1} \\ln\\!\\big(4|1-2x|\\big)\\, \\frac{dx}{\\pi \\sqrt{x(1-x)}} .\n$$\nWe compute this integral exactly. Write\n$$\n\\lambda = \\int_{0}^{1} \\left(\\ln 4 + \\ln|1-2x|\\right) \\frac{dx}{\\pi \\sqrt{x(1-x)}}\n= \\ln 4 \\int_{0}^{1} \\frac{dx}{\\pi \\sqrt{x(1-x)}} + \\int_{0}^{1} \\frac{\\ln|1-2x|}{\\pi \\sqrt{x(1-x)}}\\,dx .\n$$\nThe first integral is\n$$\n\\int_{0}^{1} \\frac{dx}{\\pi \\sqrt{x(1-x)}} = \\frac{1}{\\pi} \\int_{0}^{1} \\frac{dx}{\\sqrt{x(1-x)}} .\n$$\nUse the substitution $x=\\sin^{2}\\theta$ with $\\theta \\in (0,\\frac{\\pi}{2})$. Then $dx=2\\sin\\theta\\cos\\theta\\, d\\theta$ and $\\sqrt{x(1-x)}=\\sin\\theta\\cos\\theta$, so\n$$\n\\int_{0}^{1} \\frac{dx}{\\sqrt{x(1-x)}} = \\int_{0}^{\\pi/2} \\frac{2\\sin\\theta\\cos\\theta}{\\sin\\theta\\cos\\theta}\\, d\\theta = \\int_{0}^{\\pi/2} 2\\, d\\theta = \\pi .\n$$\nHence the first term contributes\n$$\n\\ln 4 \\cdot \\frac{1}{\\pi} \\cdot \\pi = \\ln 4.\n$$\n\nFor the second term,\n$$\nI \\equiv \\int_{0}^{1} \\frac{\\ln|1-2x|}{\\pi \\sqrt{x(1-x)}}\\,dx .\n$$\nAgain use $x=\\sin^{2}\\theta$, $dx=2\\sin\\theta\\cos\\theta\\, d\\theta$, $\\sqrt{x(1-x)}=\\sin\\theta\\cos\\theta$, $\\theta \\in (0,\\frac{\\pi}{2})$. Note that $1-2x=1-2\\sin^{2}\\theta=\\cos 2\\theta$. Thus,\n$$\nI = \\frac{1}{\\pi} \\int_{0}^{\\pi/2} \\ln|\\cos 2\\theta| \\cdot \\frac{2\\sin\\theta\\cos\\theta}{\\sin\\theta\\cos\\theta}\\, d\\theta\n= \\frac{2}{\\pi} \\int_{0}^{\\pi/2} \\ln|\\cos 2\\theta| \\, d\\theta .\n$$\nLet $u=2\\theta$, so $du=2\\, d\\theta$ and as $\\theta$ goes from $0$ to $\\pi/2$, $u$ goes from $0$ to $\\pi$. Then\n$$\nI = \\frac{2}{\\pi} \\cdot \\frac{1}{2} \\int_{0}^{\\pi} \\ln|\\cos u|\\, du\n= \\frac{1}{\\pi} \\int_{0}^{\\pi} \\ln|\\cos u|\\, du .\n$$\nUsing the evenness and periodicity of $\\cos u$, and the standard integral $\\int_{0}^{\\pi/2} \\ln(\\cos u)\\, du = -\\frac{\\pi}{2}\\ln 2$, we have\n$$\n\\int_{0}^{\\pi} \\ln|\\cos u|\\, du = 2 \\int_{0}^{\\pi/2} \\ln(\\cos u)\\, du = -\\pi \\ln 2 .\n$$\nTherefore,\n$$\nI = \\frac{1}{\\pi} \\cdot \\left(-\\pi \\ln 2\\right) = -\\ln 2 .\n$$\n\nCombining both contributions,\n$$\n\\lambda = \\ln 4 + (-\\ln 2) = \\ln 2 .\n$$\n\nInterpretation in terms of predictability versus determinism: The positive Lyapunov exponent $\\lambda=\\ln 2>0$ means that typical infinitesimal perturbations grow on average like $\\exp(\\lambda n)=2^{n}$ after $n$ iterations. Thus, even though the logistic map is strictly deterministic (the next state is a fixed function of the current state with no stochasticity), the exponential separation of nearby trajectories implies rapidly shrinking predictability horizons under finite precision of initial conditions or measurements. This is the hallmark of deterministic chaos: limited long-term predictability arising from sensitivity to initial conditions and mixing, in contrast to genuinely stochastic processes where randomness is intrinsic to the update rule rather than emergent from deterministic dynamics.",
            "answer": "$$\\boxed{\\ln 2}$$"
        },
        {
            "introduction": "We have seen that both intrinsic randomness and deterministic chaos can generate complex and unpredictable time series. This reality presents a core challenge in modeling: given a set of observations, which class of model provides a better explanation for the data? This final practice moves from theory to a powerful computational application, guiding you to implement a Bayesian model selection framework to formally compare a deterministic linear model against a stochastic one. By using a Kalman filter to compute the model evidence and the resulting Bayes factor, you will learn a principled and quantitative method for distinguishing between these competing hypotheses about a system's underlying dynamics .",
            "id": "4304932",
            "problem": "You are given two competing models for a scalar discrete-time dynamical process observed through Gaussian noise. The first model is a deterministic linear map with an unknown parameter and observation noise. The second model is a stochastic linear map with the same unknown parameter but with intrinsic Gaussian process noise in addition to observation noise. Your task is to compute the model evidence for each model under a realistic prior on the parameter, and then compute the Bayes factor comparing the deterministic versus stochastic formulation for a specified test suite.\n\nLet the observed data be a sequence $\\{y_t\\}_{t=0}^{T}$ of real numbers. The latent state is $\\{x_t\\}_{t=0}^{T}$.\n\nDeterministic model $M_D$:\n- Dynamics: $x_{t+1} = \\theta x_t$ for $t = 0,1,\\dots,T-1$.\n- Observation: $y_t = x_t + v_t$, with $v_t \\sim \\mathcal{N}(0,\\tau^2)$ independently across $t$.\n- Prior on initial state: $x_0 \\sim \\mathcal{N}(\\mu_0, s_0^2)$.\n- Prior on parameter: stability-constrained prior on $\\theta \\in (-1,1)$ induced by a Beta distribution on $u \\in [0,1]$ via the transform $\\theta = 2u - 1$, with $u \\sim \\mathrm{Beta}(\\alpha,\\beta)$. The density over $\\theta$ is $p(\\theta) = \\frac{1}{2} \\mathrm{Beta}\\left(\\frac{\\theta+1}{2};\\alpha,\\beta\\right)$, where $\\mathrm{Beta}(\\cdot;\\alpha,\\beta)$ denotes the Beta density.\n\nStochastic model $M_S$:\n- Dynamics: $x_{t+1} = \\theta x_t + w_t$, with $w_t \\sim \\mathcal{N}(0,\\sigma_w^2)$ independently across $t$.\n- Observation: $y_t = x_t + v_t$, with $v_t \\sim \\mathcal{N}(0,\\tau^2)$ independently across $t$.\n- Prior on initial state: $x_0 \\sim \\mathcal{N}(\\mu_0, s_0^2)$.\n- Prior on parameter: the same stability-constrained prior on $\\theta \\in (-1,1)$ as above.\n\nYou must compute, for each model $M \\in \\{M_D, M_S\\}$, the model evidence (marginal likelihood)\n$$\np(\\mathbf{y} \\mid M) = \\int_{-1}^{1} p(\\mathbf{y} \\mid \\theta, M) \\, p(\\theta) \\, d\\theta,\n$$\nwhere $\\mathbf{y} = (y_0,\\dots,y_T)$, $p(\\theta)$ is the prior density on $\\theta$ induced by the $\\mathrm{Beta}(\\alpha,\\beta)$ prior on $u$ and the transform $\\theta = 2u - 1$, and $p(\\mathbf{y} \\mid \\theta, M)$ is the likelihood under model $M$ after marginalizing out any latent states and hyperparameters specified in the model definition. Then compute the Bayes factor\n$$\nBF = \\frac{p(\\mathbf{y} \\mid M_D)}{p(\\mathbf{y} \\mid M_S)}.\n$$\n\nFoundational starting points you may use:\n- Bayes' theorem and the definition of model evidence.\n- Properties of the Gaussian distribution and independence assumptions stated above.\n- The definition of the Beta distribution and change-of-variables for probability densities.\n- Linear Gaussian state-space modeling and the derivation of predictive densities, using fundamental principles of conditional Gaussians and Bayesian updating. If invoking an algorithm, it must be derived from these principles.\n\nYour program must implement the computation of $p(\\mathbf{y} \\mid \\theta, M_D)$ and $p(\\mathbf{y} \\mid \\theta, M_S)$ exactly for given $\\theta$, and then numerically integrate over $\\theta \\in (-1,1)$ using the change-of-variables $u = (\\theta+1)/2 \\in [0,1]$ with the $\\mathrm{Beta}(\\alpha,\\beta)$ density on $u$. You must ensure scientific realism by using the given parameter ranges and priors. No physical units or angle units are involved in this problem.\n\nTest Suite:\nFor each test case below, compute the Bayes factor as a float. The hyperparameters $\\tau^2$, $\\sigma_w^2$, $\\mu_0$, $s_0^2$, $\\alpha$, and $\\beta$ are specified per case. The unknown parameter $\\theta$ is integrated out as described.\n\n- Case 1 (general, moderately stable dynamics):\n  - $\\mathbf{y} = [1.0, 0.62, 0.41, 0.27, 0.18, 0.12, 0.08, 0.05, 0.03]$\n  - $\\tau^2 = 0.01$, $\\sigma_w^2 = 0.04$, $\\mu_0 = 1.0$, $s_0^2 = 0.10$, $\\alpha = 2.5$, $\\beta = 2.5$\n\n- Case 2 (near-boundary stability):\n  - $\\mathbf{y} = [1.0, 0.95, 0.93, 0.91, 0.89, 0.87, 0.86, 0.85, 0.84]$\n  - $\\tau^2 = 0.005$, $\\sigma_w^2 = 0.002$, $\\mu_0 = 1.0$, $s_0^2 = 0.05$, $\\alpha = 2.5$, $\\beta = 2.5$\n\n- Case 3 (short, nearly constant sequence):\n  - $\\mathbf{y} = [0.5, 0.51, 0.50, 0.49, 0.50, 0.49]$\n  - $\\tau^2 = 0.002$, $\\sigma_w^2 = 0.02$, $\\mu_0 = 0.5$, $s_0^2 = 0.20$, $\\alpha = 2.5$, $\\beta = 2.5$\n\n- Case 4 (irregular variations suggesting process noise):\n  - $\\mathbf{y} = [0.2, 0.18, 0.27, 0.20, 0.30, 0.22, 0.33, 0.24, 0.36, 0.25, 0.37]$\n  - $\\tau^2 = 0.005$, $\\sigma_w^2 = 0.03$, $\\mu_0 = 0.2$, $s_0^2 = 0.10$, $\\alpha = 2.5$, $\\beta = 2.5$\n\n- Case 5 (edge case, almost deterministic process noise):\n  - $\\mathbf{y} = [0.8, 0.52, 0.34, 0.22]$\n  - $\\tau^2 = 0.008$, $\\sigma_w^2 = 10^{-6}$, $\\mu_0 = 0.8$, $s_0^2 = 0.05$, $\\alpha = 2.5$, $\\beta = 2.5$\n\nFinal Output Format:\nYour program should produce a single line of output containing the Bayes factors for the five cases as a comma-separated list enclosed in square brackets, for example $[\\text{bf}_1,\\text{bf}_2,\\text{bf}_3,\\text{bf}_4,\\text{bf}_5]$, where each $\\text{bf}_i$ is a float.",
            "solution": "The problem requires the computation of a Bayes factor, $BF$, to compare a deterministic model ($M_D$) and a stochastic model ($M_S$) for a given time series $\\mathbf{y} = \\{y_t\\}_{t=0}^{T}$. The Bayes factor is defined as the ratio of the model evidences (marginal likelihoods):\n$$\nBF = \\frac{p(\\mathbf{y} \\mid M_D)}{p(\\mathbf{y} \\mid M_S)}\n$$\nThe model evidence for a model $M$ is obtained by integrating the likelihood of the data over the prior distribution of the model parameters. In this case, the only unknown parameter is $\\theta$.\n$$\np(\\mathbf{y} \\mid M) = \\int_{-1}^{1} p(\\mathbf{y} \\mid \\theta, M) \\, p(\\theta) \\, d\\theta\n$$\nHere, $p(\\mathbf{y} \\mid \\theta, M)$ is the likelihood of the data given a specific value of $\\theta$, and $p(\\theta)$ is the prior probability density of $\\theta$.\n\nThe overall computational strategy involves three main steps for each model:\n1.  Derive an analytical expression for the likelihood $p(\\mathbf{y} \\mid \\theta, M)$ for a fixed $\\theta$.\n2.  Numerically compute the definite integral for the model evidence $p(\\mathbf{y} \\mid M)$.\n3.  Compute the ratio of the evidences to find the Bayes factor.\n\n**1. Likelihood Computation via Kalman Filtering**\n\nBoth models, $M_D$ and $M_S$, are linear Gaussian state-space models. For such models, the likelihood $p(\\mathbf{y} \\mid \\theta, M)$ can be computed efficiently and exactly using the prediction-update cycle of the Kalman filter. The likelihood is decomposed by the chain rule of probability:\n$$\np(\\mathbf{y} \\mid \\theta, M) = p(y_0 \\mid \\theta, M) \\prod_{t=1}^{T} p(y_t \\mid y_{0:t-1}, \\theta, M)\n$$\nIn a linear Gaussian system, each term $p(y_t \\mid y_{0:t-1}, \\theta, M)$ is a Gaussian distribution, whose parameters are determined by the Kalman filter recursions. It is numerically more stable to work with the log-likelihood:\n$$\n\\log p(\\mathbf{y} \\mid \\theta, M) = \\sum_{t=0}^{T} \\log p(y_t \\mid y_{0:t-1}, \\theta, M)\n$$\nLet the distribution of the latent state $x_t$ given observations up to time $t-1$ be the *predictive distribution* $p(x_t \\mid y_{0:t-1}) = \\mathcal{N}(x_t \\mid \\mu_{t|t-1}, \\Sigma_{t|t-1})$.\nLet the distribution of the latent state $x_t$ given observations up to time $t$ be the *filtered distribution* $p(x_t \\mid y_{0:t}) = \\mathcal{N}(x_t \\mid \\mu_{t|t}, \\Sigma_{t|t})$.\n\nThe Kalman filter algorithm proceeds as follows:\n- **Initialization ($t=0$):** The process starts with the prior on the initial state $x_0 \\sim \\mathcal{N}(\\mu_0, s_0^2)$. This is the predictive distribution for $x_0$ before any observations, so we set $\\mu_{0|-1} = \\mu_0$ and $\\Sigma_{0|-1} = s_0^2$.\n\n- **For $t = 0, 1, \\dots, T$:**\n    1.  **Likelihood Contribution:** The predictive distribution for the observation $y_t$ is derived from the state prediction and the observation model $y_t = x_t + v_t$. Since $x_t \\mid y_{0:t-1} \\sim \\mathcal{N}(\\mu_{t|t-1}, \\Sigma_{t|t-1})$ and $v_t \\sim \\mathcal{N}(0, \\tau^2)$,\n        $$y_t \\mid y_{0:t-1} \\sim \\mathcal{N}(\\mu_{t|t-1}, \\Sigma_{t|t-1} + \\tau^2)$$\n        The log-likelihood contribution from observation $y_t$ is the log-PDF of this Gaussian evaluated at the observed value:\n        $$\\log p(y_t \\mid y_{0:t-1}) = -\\frac{1}{2}\\log(2\\pi(\\Sigma_{t|t-1} + \\tau^2)) - \\frac{(y_t - \\mu_{t|t-1})^2}{2(\\Sigma_{t|t-1} + \\tau^2)}$$\n\n    2.  **State Update (Filtering):** Upon observing $y_t$, the state distribution is updated using Bayes' rule. For a joint Gaussian $(x_t, y_t)$, the conditional distribution $p(x_t \\mid y_t, y_{0:t-1})$ is also Gaussian with mean $\\mu_{t|t}$ and variance $\\Sigma_{t|t}$:\n        $$\\mu_{t|t} = \\mu_{t|t-1} + K_t (y_t - \\mu_{t|t-1})$$\n        $$\\Sigma_{t|t} = (1 - K_t) \\Sigma_{t|t-1}$$\n        where the Kalman gain $K_t$ is given by\n        $$K_t = \\frac{\\Sigma_{t|t-1}}{\\Sigma_{t|t-1} + \\tau^2}$$\n\n    3.  **State Prediction:** The distribution for the next state $x_{t+1}$ is predicted using the dynamics model $x_{t+1} = \\theta x_t + w_t$ and the filtered distribution of $x_t$.\n        $$\\mu_{t+1|t} = E[\\theta x_t + w_t \\mid y_{0:t}] = \\theta \\mu_{t|t}$$\n        $$\\Sigma_{t+1|t} = \\mathrm{Var}[\\theta x_t + w_t \\mid y_{0:t}] = \\theta^2 \\Sigma_{t|t} + Q$$\n        Here, the process noise variance $Q$ is $0$ for model $M_D$ and $\\sigma_w^2$ for model $M_S$. These new values, $\\mu_{t+1|t}$ and $\\Sigma_{t+1|t}$, are then used for the next iteration of the loop at time $t+1$.\n\nThe total log-likelihood $\\log p(\\mathbf{y} \\mid \\theta, M)$ is the sum of the log-likelihood contributions over all $t$.\n\n**2. Numerical Integration for Model Evidence**\n\nThe prior on $\\theta$ is induced by the transformation $\\theta = 2u - 1$, where $u \\sim \\mathrm{Beta}(\\alpha, \\beta)$ on the interval $[0,1]$. By applying the change of variables formula for probabilities, the model evidence integral becomes:\n$$\np(\\mathbf{y} \\mid M) = \\int_0^1 p(\\mathbf{y} \\mid \\theta=2u-1, M) \\, \\mathrm{Beta}(u; \\alpha, \\beta) \\, du\n$$\nwhere $\\mathrm{Beta}(u; \\alpha, \\beta)$ is the probability density function of the Beta distribution. This integral is not generally tractable analytically and must be computed numerically.\n\nDue to the wide range of values the likelihood can take, direct computation of this integral is prone to numerical underflow. A more robust method involves working with logarithms. Let the log of the integrand be $f(u)$:\n$$\nf(u) = \\log p(\\mathbf{y} \\mid \\theta=2u-1, M) + \\log \\mathrm{Beta}(u; \\alpha, \\beta)\n$$\nThe evidence is $p(\\mathbf{y} \\mid M) = \\int_0^1 e^{f(u)} du$. To stabilize the computation, we find the maximum value of the log-integrand, $f_{max} = \\max_{u \\in [0,1]} f(u)$, using a numerical optimization routine. The integral is then rewritten as:\n$$\np(\\mathbf{y} \\mid M) = e^{f_{max}} \\int_0^1 e^{f(u) - f_{max}} du\n$$\nThe new integrand, $g(u) = e^{f(u) - f_{max}}$, is well-behaved, with a maximum value of $1$. This integral can be computed reliably using a standard numerical quadrature method. The log-evidence is then:\n$$\n\\log p(\\mathbf{y} \\mid M) = f_{max} + \\log \\left( \\int_0^1 g(u) du \\right)\n$$\n\n**3. Bayes Factor Calculation**\n\nAfter computing the log-evidence for both models, $\\log p(\\mathbf{y} \\mid M_D)$ and $\\log p(\\mathbf{y} \\mid M_S)$, the Bayes factor is found by:\n$$\nBF = \\frac{p(\\mathbf{y} \\mid M_D)}{p(\\mathbf{y} \\mid M_S)} = \\exp\\left( \\log p(\\mathbf{y} \\mid M_D) - \\log p(\\mathbf{y} \\mid M_S) \\right)\n$$\nThis procedure is applied to each test case to yield the final results.",
            "answer": "```python\nimport numpy as np\nfrom scipy import stats, integrate, optimize\n\ndef solve():\n    \"\"\"\n    Main function to compute Bayes factors for the specified test suite.\n    \"\"\"\n\n    def log_likelihood_kalman(theta, y, mu0, s0_sq, tau_sq, Q):\n        \"\"\"\n        Computes the log-likelihood of observations y for a linear Gaussian\n        state-space model using the Kalman filter.\n\n        Args:\n            theta (float): The dynamics parameter.\n            y (np.ndarray): The sequence of observations.\n            mu0 (float): Mean of the initial state prior.\n            s0_sq (float): Variance of the initial state prior.\n            tau_sq (float): Observation noise variance.\n            Q (float): Process noise variance.\n\n        Returns:\n            float: The total log-likelihood, log p(y|theta, M).\n        \"\"\"\n        T = y.shape[0]\n        log_L = 0.0\n\n        # Initialize with prior at t=0 before any observation\n        mu_predict = float(mu0)\n        Sigma_predict = float(s0_sq)\n\n        for t in range(T):\n            # Predictive distribution for observation y_t\n            mu_y = mu_predict\n            Sigma_y = Sigma_predict + tau_sq\n            \n            # Numerical stability check for predictive variance\n            if Sigma_y <= 1e-12:\n                return -np.inf\n\n            # Log-likelihood of y_t\n            log_p = -0.5 * np.log(2 * np.pi * Sigma_y) - 0.5 * ((y[t] - mu_y)**2 / Sigma_y)\n            log_L += log_p\n            \n            # Kalman Gain\n            K = Sigma_predict / Sigma_y\n            \n            # Posterior of state x_t after observing y_t (filtering)\n            mu_update = mu_predict + K * (y[t] - mu_y)\n            Sigma_update = (1 - K) * Sigma_predict\n            \n            # Prediction for next state x_{t+1}\n            if t < T - 1:\n                mu_predict = theta * mu_update\n                Sigma_predict = theta**2 * Sigma_update + Q\n                \n        return log_L\n\n    def calculate_log_evidence(y, mu0, s0_sq, tau_sq, Q, alpha, beta):\n        \"\"\"\n        Computes the log model evidence log p(y|M) by numerically integrating\n        the likelihood over the prior on theta.\n\n        Args:\n            y (np.ndarray): The sequence of observations.\n            mu0 (float): Prior mean for x_0.\n            s0_sq (float): Prior variance for x_0.\n            tau_sq (float): Observation noise variance.\n            Q (float): Process noise variance.\n            alpha (float): Alpha parameter of the Beta prior.\n            beta (float): Beta parameter of the Beta prior.\n\n        Returns:\n            float: The log model evidence.\n        \"\"\"\n        \n        def log_integrand(u):\n            # Function to compute the log of the term inside the evidence integral\n            if not (0 < u < 1):\n                return -np.inf\n            theta = 2.0 * u - 1.0\n            log_lik = log_likelihood_kalman(theta, y, mu0, s0_sq, tau_sq, Q)\n            \n            if np.isneginf(log_lik):\n                return -np.inf\n            \n            log_prior = stats.beta.logpdf(u, alpha, beta)\n            return log_lik + log_prior\n\n        # Find the maximum of the log-integrand for numerical stability\n        res = optimize.minimize_scalar(\n            lambda u: -log_integrand(u),\n            bounds=(0, 1),\n            method='bounded'\n        )\n        \n        if not np.isfinite(res.fun):\n            return -np.inf\n            \n        log_max = -res.fun\n\n        def integrand(u):\n            # Scaled integrand for quadrature to prevent underflow\n            val = log_integrand(u)\n            if np.isneginf(val):\n                return 0.0\n            return np.exp(val - log_max)\n\n        # Perform numerical integration\n        integral_val, _ = integrate.quad(integrand, 0, 1)\n\n        if integral_val <= 0:\n            return -np.inf\n            \n        log_evidence = log_max + np.log(integral_val)\n        return log_evidence\n\n    def compute_bayes_factor(case_data):\n        \"\"\"\n        Computes the Bayes Factor for a given test case.\n        \"\"\"\n        y, tau_sq, sigma_w_sq, mu0, s0_sq, alpha, beta = case_data\n        y_arr = np.array(y)\n\n        # Log evidence for the deterministic model (M_D), where Q=0\n        log_evidence_D = calculate_log_evidence(y_arr, mu0, s0_sq, tau_sq, 0.0, alpha, beta)\n\n        # Log evidence for the stochastic model (M_S)\n        log_evidence_S = calculate_log_evidence(y_arr, mu0, s0_sq, tau_sq, sigma_w_sq, alpha, beta)\n        \n        # Bayes Factor BF = p(y|M_D) / p(y|M_S)\n        # log(BF) = log_evidence_D - log_evidence_S\n        log_bf = log_evidence_D - log_evidence_S\n        bf = np.exp(log_bf)\n        return bf\n\n    test_cases = [\n        # Case 1\n        ([1.0, 0.62, 0.41, 0.27, 0.18, 0.12, 0.08, 0.05, 0.03], 0.01, 0.04, 1.0, 0.10, 2.5, 2.5),\n        # Case 2\n        ([1.0, 0.95, 0.93, 0.91, 0.89, 0.87, 0.86, 0.85, 0.84], 0.005, 0.002, 1.0, 0.05, 2.5, 2.5),\n        # Case 3\n        ([0.5, 0.51, 0.50, 0.49, 0.50, 0.49], 0.002, 0.02, 0.5, 0.20, 2.5, 2.5),\n        # Case 4\n        ([0.2, 0.18, 0.27, 0.20, 0.30, 0.22, 0.33, 0.24, 0.36, 0.25, 0.37], 0.005, 0.03, 0.2, 0.10, 2.5, 2.5),\n        # Case 5\n        ([0.8, 0.52, 0.34, 0.22], 0.008, 1e-6, 0.8, 0.05, 2.5, 2.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        bf = compute_bayes_factor(case)\n        results.append(bf)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}