{
    "hands_on_practices": [
        {
            "introduction": "The concept of homeostasis, or self-regulation, is central to cybernetics. This exercise provides a concrete application of this idea using the tools of classical control theory. By analyzing a simple negative feedback system, you will mathematically demonstrate how goal-directed behavior—in this case, driving the error between a desired setpoint and the actual output to zero—emerges from the structure of the feedback loop itself .",
            "id": "4281598",
            "problem": "In the historical context of Cybernetics (as developed by Norbert Wiener) and General Systems Theory (as developed by Ludwig von Bertalanffy and W. Ross Ashby), negative feedback is the foundational mechanism by which a system regulates its outputs to maintain stability and achieve goals, embodying homeostatic regulation. Consider a canonical single-input single-output linear time-invariant regulator with unity negative feedback. The forward (plant) dynamics are specified in the Laplace domain by the transfer function $G(s)=\\frac{2}{s(s+1)}$ and the feedback path by $H(s)=1$. The reference input is a unit step, $r(t)=u(t)$ with Laplace transform $R(s)=\\frac{1}{s}$.\n\nStarting from the fundamental definitions of negative feedback regulation in Cybernetics—namely, the error $e(t)$ is the difference between the reference $r(t)$ and the measured output $y(t)$ passed through the feedback $h(t)$, and the output $y(t)$ results from driving the plant by the error—perform the following:\n\n1. Derive the closed-loop transfer function $T(s)$ from reference $R(s)$ to output $Y(s)$.\n2. Using the Final Value Theorem for Laplace transforms, compute the steady-state error $e_{\\infty}$ to the unit step input.\n3. Based on these results and the structure of the loop, discuss why negative feedback embodies homeostatic regulation in this case, making clear reference to Cybernetics and General Systems Theory concepts.\n\nYour final answer must present the pair consisting of the closed-loop transfer function $T(s)$ and the steady-state error $e_{\\infty}$ together as a two-entry row vector. No rounding is required, and no units are necessary. Express your final results exactly.",
            "solution": "The problem presents a valid and well-posed question rooted in the fundamentals of classical control theory, which provides the mathematical language for the principles of Cybernetics and General Systems Theory. All provided information is sufficient, consistent, and scientifically sound.\n\nThe analysis proceeds in three parts as requested: derivation of the closed-loop transfer function, computation of the steady-state error, and a discussion contextualizing the results within Cybernetics and General Systems Theory.\n\n**1. Derivation of the Closed-Loop Transfer Function $T(s)$**\n\nThe system is a negative feedback loop. The relationships between the signals in the Laplace domain are defined as follows:\nThe error signal at the summing junction, which drives the plant, is the difference between the reference input $R(s)$ and the signal fed back from the output. Let's call this driving error signal $E_{drive}(s)$.\n$$E_{drive}(s) = R(s) - B(s)$$\nwhere $B(s)$ is the output of the feedback element $H(s)$:\n$$B(s) = H(s)Y(s)$$\nThe plant output $Y(s)$ is the result of the plant $G(s)$ being driven by $E_{drive}(s)$:\n$$Y(s) = G(s)E_{drive}(s)$$\nSubstituting the first two equations into the third gives:\n$$Y(s) = G(s)(R(s) - H(s)Y(s))$$\nWe can now solve for the output $Y(s)$ in terms of the input $R(s)$:\n$$Y(s) = G(s)R(s) - G(s)H(s)Y(s)$$\n$$Y(s) + G(s)H(s)Y(s) = G(s)R(s)$$\n$$Y(s)(1 + G(s)H(s)) = G(s)R(s)$$\nThe closed-loop transfer function, $T(s)$, is the ratio of the output $Y(s)$ to the input $R(s)$:\n$$T(s) = \\frac{Y(s)}{R(s)} = \\frac{G(s)}{1 + G(s)H(s)}$$\nNow, we substitute the given transfer functions $G(s) = \\frac{2}{s(s+1)}$ and $H(s) = 1$:\n$$T(s) = \\frac{\\frac{2}{s(s+1)}}{1 + \\left(\\frac{2}{s(s+1)}\\right)(1)}$$\nTo simplify, we multiply the numerator and denominator by $s(s+1)$:\n$$T(s) = \\frac{2}{s(s+1) + 2}$$\n$$T(s) = \\frac{2}{s^2 + s + 2}$$\nThis is the closed-loop transfer function from the reference input $R(s)$ to the system output $Y(s)$.\n\n**2. Computation of the Steady-State Error $e_{\\infty}$**\n\nThe problem defines the error $e(t)$ as the difference between the reference $r(t)$ and the output $y(t)$. Since the feedback is unity ($H(s)=1$), this error $e(t)$ is identical to the signal driving the plant. In the Laplace domain:\n$$E(s) = R(s) - Y(s)$$\nWe can express $E(s)$ in terms of $R(s)$ using the closed-loop transfer function $T(s) = Y(s)/R(s)$:\n$$E(s) = R(s) - T(s)R(s) = (1 - T(s))R(s)$$\nSubstituting the expression for $T(s)$:\n$$E(s) = \\left(1 - \\frac{G(s)}{1+G(s)H(s)}\\right)R(s) = \\left(\\frac{1+G(s)H(s)-G(s)}{1+G(s)H(s)}\\right)R(s)$$\nSince $H(s)=1$, this simplifies to:\n$$E(s) = \\frac{1}{1+G(s)}R(s)$$\nThe steady-state error $e_{\\infty}$ can be found using the Final Value Theorem, which states $e_{\\infty} = \\lim_{t \\to \\infty} e(t) = \\lim_{s \\to 0} sE(s)$. This theorem is applicable only if the closed-loop system is stable. The poles of $T(s)$ are the roots of the characteristic equation $s^2 + s + 2 = 0$. The roots are $s = \\frac{-1 \\pm \\sqrt{1-8}}{2} = -\\frac{1}{2} \\pm i\\frac{\\sqrt{7}}{2}$. Since the real parts of the poles are negative ($-1/2$), the system is stable, and the Final Value Theorem can be applied.\n\nThe input is a unit step, so $r(t) = u(t)$ and its Laplace transform is $R(s) = \\frac{1}{s}$.\nSubstituting $R(s)$ into the expression for $E(s)$:\n$$E(s) = \\frac{1}{1+G(s)} \\cdot \\frac{1}{s}$$\nNow we apply the Final Value Theorem:\n$$e_{\\infty} = \\lim_{s \\to 0} sE(s) = \\lim_{s \\to 0} s \\left(\\frac{1}{1+G(s)} \\cdot \\frac{1}{s}\\right) = \\lim_{s \\to 0} \\frac{1}{1+G(s)}$$\nSubstitute the plant transfer function $G(s) = \\frac{2}{s(s+1)}$:\n$$e_{\\infty} = \\lim_{s \\to 0} \\frac{1}{1 + \\frac{2}{s(s+1)}}$$\nTo evaluate this limit, we can first analyze the behavior of the term $G(s)$ as $s \\to 0$. The term $s$ in the denominator causes the magnitude of $G(s)$ to approach infinity.\n$$\\lim_{s \\to 0} G(s) = \\lim_{s \\to 0} \\frac{2}{s(s+1)} = \\infty$$\nTherefore, the denominator of the expression for $e_{\\infty}$ also approaches infinity:\n$$\\lim_{s \\to 0} \\left(1 + \\frac{2}{s(s+1)}\\right) = \\infty$$\nThis means the steady-state error is zero:\n$$e_{\\infty} = 0$$\nAlternatively, we can use algebraic manipulation before taking the limit:\n$$e_{\\infty} = \\lim_{s \\to 0} \\frac{1}{\\frac{s(s+1)+2}{s(s+1)}} = \\lim_{s \\to 0} \\frac{s(s+1)}{s^2+s+2}$$\nSubstituting $s=0$ into this continuous function yields:\n$$e_{\\infty} = \\frac{0(0+1)}{0^2+0+2} = \\frac{0}{2} = 0$$\n\n**3. Discussion on Homeostatic Regulation, Cybernetics, and General Systems Theory**\n\nThe result that the steady-state error $e_{\\infty}$ is zero is profound in the context of Cybernetics and General Systems Theory. It provides a concrete mathematical demonstration of goal-seeking behavior and homeostatic regulation.\n\n*   **Homeostatic Regulation**: Homeostasis is the ability of a system to maintain a stable internal state by adjusting its physiological processes. In this problem, the system's \"state\" is its output $y(t)$, and the desired \"stable state\" or setpoint is the reference input $r(t)=1$. The calculation shows that $\\lim_{t \\to \\infty} y(t) = \\lim_{t \\to \\infty} (r(t) - e(t)) = 1 - 0 = 1$. The system successfully regulates its output to match the desired setpoint, perfectly counteracting its own internal dynamics to achieve the goal. This is the essence of homeostasis.\n\n*   **Negative Feedback as the Mechanism**: The mechanism that achieves this regulation is negative feedback. The system continuously measures its output $y(t)$, compares it to the reference $r(t)$, and uses the resulting error $e(t) = r(t) - y(t)$ to drive the plant. If the output is below the setpoint ($y(t) < 1$), the error is positive, causing the plant to increase its output. If the output overshoots the setpoint ($y(t) > 1$), the error becomes negative, causing the plant to decrease its output. This perpetual process of comparison and correction is what drives the error to zero.\n\n*   **Cybernetics**: Norbert Wiener defined Cybernetics as the science of \"control and communication in the animal and the machine.\" This problem is a textbook example. Information about the system's performance (the output $y(t)$) is \"communicated\" via the feedback loop back to the input, where it is used to \"control\" the system's future behavior. The zero steady-state error demonstrates that this structure endows the system with purpose, or teleology, a hallmark of cybernetic thinking. The system's \"goal\" (to make $y(t)=r(t)$) is an emergent property of the feedback loop's structure, not an intrinsic property of the components themselves.\n\n*   **General Systems Theory**: Ludwig von Bertalanffy and W. Ross Ashby sought to identify principles that are universal across different types of systems. Negative feedback is arguably the most important of these principles. The mathematical model analyzed here is an archetype for regulation in countless systems, from the control of body temperature in mammals and the regulation of glucose by insulin (biological systems) to thermostats and cruise control (engineered systems). The crucial feature of the plant, $G(s)$, that enables perfect regulation for a step input is the presence of an integrator (the $1/s$ term). In physical systems, an integrator represents accumulation. By accumulating the error over time, the controller can generate an output large enough to overcome any persistent drifts or loads, ensuring the error is ultimately eliminated. This demonstrates how a general structural property (the presence of an integrator in the loop) leads to a specific, highly desirable function (perfect steady-state regulation), a key theme in General Systems Theory.\n\nIn summary, the calculations for $T(s)$ and $e_{\\infty}$ are not merely a mathematical exercise; they formalize the foundational concepts of Cybernetics and GST, demonstrating how a simple, local mechanism—negative feedback—can give rise to complex, goal-directed, and self-regulating global behavior.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{s^{2}+s+2} & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While negative feedback explains *how* a system can regulate itself, W. Ross Ashby's Law of Requisite Variety addresses *how much* regulatory capacity is needed. This practice formalizes this profound cybernetic principle using concepts from information theory. You will determine the minimum variety a regulator must possess to successfully counteract a set of environmental disturbances, revealing the fundamental link between information and control .",
            "id": "4281601",
            "problem": "In the historical framework of cybernetics and general systems theory, consider a regulator tasked with countering disturbances to maintain a system at a single acceptable outcome state. Let the environmental disturbance be modeled as a random variable $D$ taking $n$ equiprobable and distinguishable values, each of which, in the absence of control, leads to a distinct deviation from the acceptable outcome. The regulator chooses a control action by selecting one of its internal states, modeled as a random variable $R$, via a deterministic mapping $R = h(D)$ based on perfect observation of $D$. The closed-loop outcome is given by a deterministic mapping $O = g(D,R)$, and the objective is to ensure that the outcome is a fixed target value $o^{\\star}$ for all realizations of $D$.\n\nUse only the following foundational elements:\n- The variety of an equiprobable set of $N$ distinguishable states is measured in bits by $\\log_{2} N$ (the Hartley measure), which coincides with the Shannon entropy of a uniform distribution.\n- Deterministic mappings cannot increase Shannon entropy, and composition of deterministic mappings remains deterministic.\n\nAssume the plant’s counteraction requirements are such that any two distinct disturbances $d_{1} \\neq d_{2}$ require distinct regulator control actions to achieve the same target outcome $o^{\\star}$. Under these assumptions and using the stated foundational elements, determine the minimum regulator capacity, expressed as an analytic expression in $n$, measured in bits. Provide your final answer as a single closed-form expression in terms of $n$. No rounding is required, and no units should be included in the final boxed answer; the quantity is in bits by construction.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It presents a formal scenario based on the foundational principles of cybernetics, specifically relating to W. Ross Ashby's Law of Requisite Variety. The givens are complete, consistent, and allow for a unique solution.\n\nLet $\\mathcal{D}$ be the set of distinguishable values for the environmental disturbance $D$. According to the problem, this set contains $n$ values, which are equiprobable. Let's denote these values as $\\mathcal{D} = \\{d_1, d_2, \\ldots, d_n\\}$. The size of this set is $|\\mathcal{D}| = n$.\n\nThe variety of a set of $N$ distinguishable states is given by the Hartley measure, $V = \\log_{2} N$. The variety of the disturbance is therefore:\n$$V(D) = \\log_{2} |\\mathcal{D}| = \\log_{2} n$$\nThis quantity, measured in bits, represents the amount of information or uncertainty associated with the disturbance.\n\nThe regulator's function is to maintain the system at a single acceptable outcome state, $o^{\\star}$. Let $\\mathcal{O}$ be the set of possible outcomes. The objective dictates that this set must contain only one element, $\\mathcal{O} = \\{o^{\\star}\\}$. The variety of the outcome is therefore:\n$$V(O) = \\log_{2} |\\mathcal{O}| = \\log_{2} 1 = 0$$\nThe regulator's action reduces the variety from $\\log_{2} n$ at the input (disturbance) to $0$ at the output (outcome).\n\nLet $\\mathcal{R}$ be the set of the regulator's internal states, from which it chooses a control action. The regulator's capacity is defined as the variety of this set, $V(R) = \\log_{2} |\\mathcal{R}|$. We are asked to find the minimum possible value for this capacity.\n\nThe regulator's action is determined by a deterministic mapping $R = h(D)$, where $h: \\mathcal{D} \\to \\mathcal{R}$. The final outcome is given by another deterministic mapping $O = g(D, R)$. The control objective is to ensure that for any disturbance $d \\in \\mathcal{D}$, the outcome is $o^{\\star}$. By substituting the control law into the outcome function, we get:\n$$g(d, h(d)) = o^{\\star} \\quad \\forall d \\in \\mathcal{D}$$\n\nThe critical constraint is that \"any two distinct disturbances $d_1 \\neq d_2$ require distinct regulator control actions to achieve the same target outcome $o^{\\star}$.\" Let's formalize this. Consider any two distinct disturbances, $d_i \\in \\mathcal{D}$ and $d_j \\in \\mathcal{D}$ with $d_i \\neq d_j$. The regulator selects actions $r_i = h(d_i)$ and $r_j = h(d_j)$ respectively. The resulting outcomes are $g(d_i, r_i) = o^{\\star}$ and $g(d_j, r_j) = o^{\\star}$. The constraint states that because the disturbances are different ($d_i \\neq d_j$) but the outcome is the same ($o^{\\star}$), the actions taken must be different. That is:\n$$r_i \\neq r_j$$\nSubstituting the definition of the control law $h$, we have:\n$$h(d_i) \\neq h(d_j)$$\nThis condition, which must hold for any pair of distinct disturbances $d_i, d_j$, is the definition of an injective (one-to-one) function. The function $h: \\mathcal{D} \\to \\mathcal{R}$ must map every distinct disturbance to a unique and distinct regulator state.\n\nFor an injective mapping to exist from a set $\\mathcal{D}$ to a set $\\mathcal{R}$, the cardinality of the codomain $\\mathcal{R}$ must be at least as great as the cardinality of the domain $\\mathcal{D}$.\n$$|\\mathcal{R}| \\ge |\\mathcal{D}|$$\nSince $|\\mathcal{D}| = n$, we must have:\n$$|\\mathcal{R}| \\ge n$$\nThis establishes that the minimum number of distinguishable states the regulator must possess is $n$.\n\nThe regulator capacity is the variety of its state space $\\mathcal{R}$. To find the minimum capacity, we use the minimum required number of states, which is $n$.\n$$V(R)_{\\min} = \\log_{2} (|\\mathcal{R}|_{\\min})$$\n$$V(R)_{\\min} = \\log_{2} n$$\n\nThis result is a direct expression of Ashby's Law of Requisite Variety, which states that \"only variety can destroy variety.\" To reduce the variety of the outcome to zero in the face of $n$ possible disturbances (with variety $\\log_2 n$), the regulator must be capable of generating at least as much variety, which requires it to have at least $n$ distinct states, corresponding to a capacity of $\\log_2 n$ bits.",
            "answer": "$$\\boxed{\\log_{2}(n)}$$"
        },
        {
            "introduction": "Not all feedback leads to stable, single-goal regulation. This practice explores the consequences of positive feedback, which can create path dependence and multiple stable states. By modeling and simulating a nonlinear dynamical system, you will investigate the phenomenon of bistability and the failure of equifinality, where a system's final state depends critically on its initial conditions .",
            "id": "4281557",
            "problem": "Consider a one-dimensional autonomous dynamical system motivated by Cybernetics and General Systems Theory (GST), where positive feedback and threshold effects can lead to multiple attractors and path dependence. Let the state variable $x \\in [0,1]$ evolve according to the ordinary differential equation (ODE)\n$$\n\\frac{dx}{dt} = f(x; r, a) = r \\, x \\, (1 - x) \\, (x - a),\n$$\nwhere $r > 0$ is a rate parameter and $a \\in \\mathbb{R}$ is a threshold parameter that encodes the strength and position of positive feedback. This system has equilibria at $x=0$, $x=a$, and $x=1$, and it exhibits path dependence when multiple stable equilibria exist and the final outcome depends on the initial condition. General Systems Theory (GST) defines equifinality as the phenomenon in which different initial conditions lead to the same final state; equifinality fails when path dependence causes distinct final states under different initial conditions in the same system.\n\nYour task is to construct and analyze cases in which equifinality fails due to positive feedback and path dependence, and to perform a basic bifurcation analysis with respect to the parameter $a$. Use only mathematical and computational reasoning grounded in the following fundamental bases:\n- Definitions of autonomous dynamical systems and equilibria: an equilibrium $x^\\ast$ satisfies $f(x^\\ast; r, a)=0$.\n- Linear stability in one dimension: an equilibrium $x^\\ast$ is strictly stable if $f'(x^\\ast; r, a) < 0$, unstable if $f'(x^\\ast; r, a) > 0$, and neutrally stable (nonhyperbolic) if $f'(x^\\ast; r, a) = 0$.\n- Bifurcation analysis concept: qualitative changes in the number and stability of equilibria as a parameter varies.\n\nImplement a program that, for each given parameter set $(r, a, x_0, \\varepsilon)$, performs the following steps:\n1. Compute the set of equilibria in the domain $[0,1]$, which are the intersections of $f(x; r, a)=0$ with $[0,1]$. In this model, the candidate equilibria are $x=0$, $x=a$, and $x=1$. Retain only those within $[0,1]$.\n2. Determine the strict stability of each retained equilibrium using $f'(x; r, a)$ evaluated at the equilibrium. Count the number of strictly stable equilibria in $[0,1]$ (exclude neutrally stable cases where $f'(x^\\ast; r, a)=0$).\n3. Numerically integrate the ODE from initial condition $x(0)=x_0$ for a fixed, sufficiently long time using a fixed stepsize method to approximate the long-time limit $x(\\infty)$, clamping $x(t)$ to $[0,1]$ after each update to preserve the domain. Repeat this integration starting from a perturbed initial condition $x(0)=\\min\\{\\max\\{x_0+\\varepsilon, 0\\},1\\}$ to approximate $x_\\varepsilon(\\infty)$.\n4. Quantify the sensitivity to initial conditions by comparing $x(\\infty)$ and $x_\\varepsilon(\\infty)$. Define equifinality failure as a boolean value that is true when $|x(\\infty) - x_\\varepsilon(\\infty)|$ exceeds a numerical tolerance, indicating path dependence under positive feedback.\n5. Round $x(\\infty)$ and $x_\\varepsilon(\\infty)$ to six decimal places for reporting.\n\nFor the derivative needed in step 2, compute\n$$\nf'(x; r, a) = r \\, \\frac{d}{dx}\\left[-x^3 + (1+a) x^2 - a x\\right] = r \\left(-3x^2 + 2(1+a)x - a\\right).\n$$\n\nTest Suite:\nUse the following parameter sets $(r, a, x_0, \\varepsilon)$ to exercise different dynamical regimes, including a general path-dependent case, threshold boundary conditions, and edge cases:\n- Case 1 (general path dependence with two stable equilibria): $(1.0, 0.4, 0.35, 0.06)$.\n- Case 2 (initial condition exactly at the unstable threshold): $(1.0, 0.4, 0.4, 0.000001)$.\n- Case 3 (boundary case with a neutrally stable equilibrium at $x=0$ and a single strict attractor at $x=1$): $(1.0, 0.0, 0.001, 0.1)$.\n- Case 4 (boundary case with a neutrally stable equilibrium at $x=1$ and a single strict attractor at $x=0$): $(1.0, 1.0, 0.5, 0.1)$.\n\nAnswer Specification:\nFor each case, your program should produce a list with four entries:\n- The integer number of strictly stable equilibria in $[0,1]$.\n- The float $x(\\infty)$ from $x_0$, rounded to six decimals.\n- The float $x_\\varepsilon(\\infty)$ from $x_0+\\varepsilon$ clamped to $[0,1]$, rounded to six decimals.\n- A boolean indicating equifinality failure (true if $|x(\\infty) - x_\\varepsilon(\\infty)|$ exceeds $10^{-6}$, false otherwise).\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each case’s result represented as a list in the same order as above. For example: \"[[n1,x1,y1,b1],[n2,x2,y2,b2],...]\" with no additional whitespace. All reported quantities are unitless real numbers; report booleans as true or false in their standard programming language boolean representation.",
            "solution": "The problem statement is valid. It is scientifically grounded in the theory of autonomous dynamical systems, well-posed, and objective. It provides a clear, verifiable task rooted in fundamental principles of stability and bifurcation analysis.\n\nThe analysis proceeds in two stages. First, a general analysis of the provided dynamical system is performed to understand its behavior as a function of the parameter $a$. Second, this general analysis is applied to each of the specific test cases.\n\nThe one-dimensional autonomous dynamical system is described by the ordinary differential equation (ODE):\n$$\n\\frac{dx}{dt} = f(x; r, a) = r \\, x \\, (1 - x) \\, (x - a)\n$$\nThe state variable $x$ is constrained to the interval $[0, 1]$. The parameter $r$ is a positive rate constant, $r > 0$, and $a \\in \\mathbb{R}$ is a threshold parameter.\n\n**1. General Analysis: Equilibria and Stability**\n\nEquilibria, or fixed points $x^\\ast$, of the system are the states where the rate of change is zero, i.e., $f(x^\\ast; r, a) = 0$. For the given function, the equilibria are the roots of $r \\, x \\, (1 - x) \\, (x - a) = 0$, which are $x^\\ast=0$, $x^\\ast=1$, and $x^\\ast=a$. We only consider equilibria within the domain of interest, $[0, 1]$.\n\nThe stability of an equilibrium $x^\\ast$ is determined by the sign of the derivative of $f(x; r, a)$ with respect to $x$, evaluated at $x^\\ast$. The derivative is given as:\n$$\nf'(x; r, a) = r \\left(-3x^2 + 2(1+a)x - a\\right)\n$$\nAn equilibrium $x^\\ast$ is strictly stable if $f'(x^\\ast) < 0$ and unstable if $f'(x^\\ast) > 0$. If $f'(x^\\ast) = 0$, the equilibrium is nonhyperbolic, and linear stability analysis is inconclusive, requiring further investigation. Since $r > 0$, the sign of $f'(x^\\ast)$ is determined by the polynomial term.\n\n-   **Stability of $x^\\ast=0$**:\n    $f'(0; r, a) = r(-a)$.\n    Thus, $x^\\ast=0$ is strictly stable if $a>0$, unstable if $a<0$, and nonhyperbolic if $a=0$.\n\n-   **Stability of $x^\\ast=1$**:\n    $f'(1; r, a) = r(-3 + 2(1+a) - a) = r(-3 + 2 + 2a - a) = r(a-1)$.\n    Thus, $x^\\ast=1$ is strictly stable if $a<1$, unstable if $a>1$, and nonhyperbolic if $a=1$.\n\n-   **Stability of $x^\\ast=a$**:\n    This equilibrium is only in the domain $[0, 1]$ if $0 \\le a \\le 1$.\n    $f'(a; r, a) = r(-3a^2 + 2(1+a)a - a) = r(-3a^2 + 2a + 2a^2 - a) = r(a - a^2) = r \\cdot a(1-a)$.\n    If $0 < a < 1$, then $a(1-a) > 0$, making $f'(a) > 0$. Thus, $x^\\ast=a$ is an unstable equilibrium. If $a=0$ or $a=1$, it merges with the other equilibria and is nonhyperbolic.\n\n**2. Bifurcation Analysis with respect to $a$**\n\nThe qualitative behavior of the system changes as the parameter $a$ crosses the values $0$ and $1$.\n\n-   **For $a < 0$**: The equilibria in $[0, 1]$ are $x^\\ast=0$ (unstable, since $f'(0) = -ra > 0$) and $x^\\ast=1$ (stable, since $f'(1) = r(a-1) < 0$). There is one stable equilibrium at $x=1$.\n-   **For $a = 0$**: The equilibria are $x^\\ast=0$ (nonhyperbolic, $f'(0)=0$) and $x^\\ast=1$ (stable, $f'(1)=-r < 0$). The number of strictly stable equilibria is one.\n-   **For $0 < a < 1$**: The equilibria are $x^\\ast=0$ (stable, $f'(0)=-ra < 0$), $x^\\ast=a$ (unstable, $f'(a)=ra(1-a) > 0$), and $x^\\ast=1$ (stable, $f'(1)=r(a-1) < 0$). This is the bistable regime with two stable equilibria. Path dependence is expected, and equifinality fails. The unstable equilibrium $x^\\ast=a$ acts as a threshold, separating the basins of attraction of $x^\\ast=0$ and $x^\\ast=1$.\n-   **For $a = 1$**: The equilibria are $x^\\ast=0$ (stable, $f'(0)=-r < 0$) and $x^\\ast=1$ (nonhyperbolic, $f'(1)=0$). The number of strictly stable equilibria is one.\n-   **For $a > 1$**: The equilibria in $[0, 1]$ are $x^\\ast=0$ (stable, since $f'(0)=-ra < 0$) and $x^\\ast=1$ (unstable, since $f'(1)=r(a-1) > 0$). There is one stable equilibrium at $x=0$.\n\nIn summary, the system exhibits bistability and path dependence only when $0 < a < 1$. The points $a=0$ and $a=1$ are bifurcation points where the equilibrium $x^\\ast=a$ collides with $x^\\ast=0$ and $x^\\ast=1$, respectively, changing their stability.\n\n**3. Analysis of Test Cases**\n\nNumerical integration is performed using the Forward Euler method, $x_{k+1} = x_k + \\Delta t \\cdot f(x_k)$, with a step size $\\Delta t = 0.01$ and a total of $20000$ steps. The state $x_k$ is clamped to $[0, 1]$ after each step.\n\n-   **Case 1**: $(r=1.0, a=0.4, x_0=0.35, \\varepsilon=0.06)$\n    Since $0 < a=0.4 < 1$, the system is bistable.\n    1.  **Equilibria and Stability**: The equilibria in $[0, 1]$ are {$0, 0.4, 1$}. Based on our general analysis, $x^\\ast=0$ and $x^\\ast=1$ are strictly stable, while $x^\\ast=0.4$ is unstable. The number of strictly stable equilibria is $2$.\n    2.  **Integration**: The initial condition is $x_0=0.35$. Since $x_0 < a$, the trajectory will converge to the lower stable equilibrium, $x^\\ast=0$. So, $x(\\infty) \\approx 0.0$. The perturbed initial condition is $x_0+\\varepsilon = 0.35+0.06 = 0.41$. Since this is greater than $a$, the trajectory will converge to the upper stable equilibrium, $x^\\ast=1$. So, $x_\\varepsilon(\\infty) \\approx 1.0$.\n    3.  **Equifinality**: $|x(\\infty) - x_\\varepsilon(\\infty)| \\approx |0.0 - 1.0| = 1.0$, which is greater than the tolerance $10^{-6}$. Thus, equifinality fails (true).\n\n-   **Case 2**: $(r=1.0, a=0.4, x_0=0.4, \\varepsilon=0.000001)$\n    Again, $0 < a=0.4 < 1$, so the system is bistable.\n    1.  **Equilibria and Stability**: Same as Case 1; there are $2$ strictly stable equilibria.\n    2.  **Integration**: The initial condition is $x_0=0.4$, which is exactly the unstable equilibrium. In an ideal simulation, $f(0.4)=0$, so the state remains at $0.4$. Thus, $x(\\infty) \\approx 0.4$. The perturbed initial condition is $x_0+\\varepsilon = 0.400001$. As this is greater than $a$, the trajectory converges to $x=1$. So, $x_\\varepsilon(\\infty) \\approx 1.0$.\n    3.  **Equifinality**: $|x(\\infty) - x_\\varepsilon(\\infty)| \\approx |0.4 - 1.0| = 0.6 > 10^{-6}$. Equifinality fails (true).\n\n-   **Case 3**: $(r=1.0, a=0.0, x_0=0.001, \\varepsilon=0.1)$\n    This is a bifurcation case with $a=0$.\n    1.  **Equilibria and Stability**: The equilibria in $[0, 1]$ are {$0, 1$}. $x^\\ast=1$ is strictly stable ($f'(1)=-1 < 0$). $x^\\ast=0$ is nonhyperbolic ($f'(0)=0$). The number of strictly stable equilibria is $1$.\n    2.  **Integration**: The ODE is $\\frac{dx}{dt} = x^2(1-x)$. For any $x \\in (0, 1)$, $\\frac{dx}{dt} > 0$. Therefore, any trajectory starting in $(0, 1)$ will converge to $x=1$. Both $x_0=0.001$ and $x_0+\\varepsilon = 0.101$ are in this interval. Thus, both $x(\\infty)$ and $x_\\varepsilon(\\infty)$ will be approximately $1.0$.\n    3.  **Equifinality**: $|x(\\infty) - x_\\varepsilon(\\infty)| \\approx |1.0 - 1.0| = 0.0 \\le 10^{-6}$. Equifinality holds (false).\n\n-   **Case 4**: $(r=1.0, a=1.0, x_0=0.5, \\varepsilon=0.1)$\n    This is the other bifurcation case with $a=1$.\n    1.  **Equilibria and Stability**: The equilibria in $[0, 1]$ are {$0, 1$}. $x^\\ast=0$ is strictly stable ($f'(0)=-1 < 0$). $x^\\ast=1$ is nonhyperbolic ($f'(1)=0$). The number of strictly stable equilibria is $1$.\n    2.  **Integration**: The ODE is $\\frac{dx}{dt} = x(1-x)(x-1) = -x(1-x)^2$. For any $x \\in (0, 1)$, $\\frac{dx}{dt} < 0$. Therefore, any trajectory starting in $(0, 1)$ will converge to $x=0$. Both $x_0=0.5$ and $x_0+\\varepsilon = 0.6$ are in this interval. Thus, both $x(\\infty)$ and $x_\\varepsilon(\\infty)$ will be approximately $0.0$.\n    3.  **Equifinality**: $|x(\\infty) - x_\\varepsilon(\\infty)| \\approx |0.0 - 0.0| = 0.0 \\le 10^{-6}$. Equifinality holds (false).",
            "answer": "[[2,0.000000,1.000000,true],[2,0.400000,1.000000,true],[1,1.000000,1.000000,false],[1,0.000000,0.000000,false]]"
        }
    ]
}