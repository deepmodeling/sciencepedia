{
    "hands_on_practices": [
        {
            "introduction": "泊松近似的核心思想源于在大型稀疏图中各类事件的近似独立性。本练习将通过一个最简单的非平凡案例——孤立节点（度为0的节点）——来深入探讨这一概念。通过直接计算两个节点同时被孤立的联合概率，并将其与各自独立被孤立的概率乘积进行比较，我们可以精确地量化它们之间存在的微弱依赖关系，并观察到这种依赖性是如何随着网络规模的增长而消失的 。这项练习有助于巩固对“渐近独立性”这一核心概念的理解，而这正是将泊松分布应用于网络科学的理论基石。",
            "id": "4297656",
            "problem": "考虑一个 Erdős–Rényi (ER) 随机图 $G(n,p)$，它有 $n$ 个已标记的顶点，其中每条可能的边都以概率 $p$ 独立存在。固定两个不同的顶点，标记为 $i$ 和 $j$。如果一个顶点的度为 $0$，则称该顶点是孤立的。\n\n仅从 $G(n,p)$ 中各边相互独立以及孤立的定义出发，推导给定顶点被孤立的概率以及顶点 $i$ 和 $j$ 都被孤立的概率的表达式。然后，对于稀疏情况 $p=p_{n}=\\frac{c}{n}$（其中 $c0$ 为固定值），使用您的表达式讨论当 $n \\to \\infty$ 时，$i$ 和 $j$ 的孤立事件的近似独立性。\n\n为了量化与独立性的主阶偏差，定义\n$$\nL(c)\\;=\\;\\lim_{n\\to\\infty} n\\left[\\mathbb{P}\\big(\\text{$i$ and $j$ are isolated in }G(n,p_{n})\\big)\\;-\\;\\mathbb{P}\\big(\\text{$i$ is isolated in }G(n,p_{n})\\big)^{2}\\right].\n$$\n以 $c$ 的函数形式，计算 $L(c)$ 的闭式解。您的最终答案必须是仅含 $c$ 的单个解析表达式。",
            "solution": "我们从 Erdős–Rényi (ER) 随机图 $G(n,p)$ 模型开始，其中 $\\binom{n}{2}$ 条可能的边中的每一条都以概率 $p$ 独立地被包含，因此以概率 $1-p$ 独立地被排除。如果一个顶点的所有关联边都不存在，则该顶点是孤立的。\n\n首先，我们计算固定顶点 $i$ 是孤立的概率。有 $n-1$ 条可能的边与 $i$ 相关联。根据各边的独立性以及每条边以概率 $1-p$ 不存在的事实，所有 $n-1$ 条关联边都不存在的概率是\n$$\n\\mathbb{P}(\\text{$i$ isolated}) \\;=\\; (1-p)^{n-1}.\n$$\n\n接下来，我们计算两个不同顶点 $i$ 和 $j$ 都是孤立的概率。要使 $i$ 孤立，所有与 $i$ 相邻的 $n-1$ 条边都必须不存在；同样，要使 $j$ 孤立，所有与 $j$ 相邻的 $n-1$ 条边都必须不存在。然而，连接 $i$ 和 $j$ 的边在两组中都被计算了，因此我们需要计算要使 $i$ 和 $j$ 都孤立，必须不存在的边的具体数量。与 $i$ 或 $j$ 相关联的边的集合包括从 $i$ 到除 $j$ 以外的顶点的 $(n-2)$ 条边，从 $j$ 到除 $i$ 以外的顶点的 $(n-2)$ 条边，以及单条边 $\\{i,j\\}$。因此，总共有 $2(n-2)+1 = 2n-3$ 条不同的边必须不存在。根据独立性，\n$$\n\\mathbb{P}(\\text{$i$ and $j$ isolated}) \\;=\\; (1-p)^{2n-3}.\n$$\n\n为了讨论近似独立性，我们将联合概率与边际概率的乘积进行比较：\n$$\n\\frac{\\mathbb{P}(\\text{$i$ and $j$ isolated})}{\\mathbb{P}(\\text{$i$ isolated})\\,\\mathbb{P}(\\text{$j$ isolated})}\n\\;=\\;\n\\frac{(1-p)^{2n-3}}{(1-p)^{n-1}\\,(1-p)^{n-1}}\n\\;=\\;\n(1-p)^{-1}.\n$$\n对于 $p=p_{n}=\\frac{c}{n}$（其中 $c0$ 为固定值），我们有\n$$\n(1-p_{n})^{-1} \\;=\\; \\frac{1}{1-\\frac{c}{n}} \\;=\\; 1 \\;+\\; \\frac{c}{n} \\;+\\; \\mathcal{O}\\!\\left(\\frac{1}{n^{2}}\\right),\n$$\n当 $n\\to\\infty$ 时，该式趋向于 $1$。这表明在稀疏情况下，$i$ 和 $j$ 的孤立事件是渐近独立的，尽管对于有限的 $n$ 它们并非完全独立。\n\n为了量化与独立性的主阶偏差，我们考虑孤立指示变量的协方差，并将其乘以 $n$。定义指示变量 $X_{i}=\\mathbf{1}\\{\\text{$i$ isolated}\\}$ 和 $X_{j}=\\mathbf{1}\\{\\text{$j$ isolated}\\}$。那么\n$$\n\\operatorname{Cov}(X_{i},X_{j})\n\\;=\\;\n\\mathbb{P}(\\text{$i$ and $j$ isolated}) - \\mathbb{P}(\\text{$i$ isolated})^{2}\n\\;=\\;\n(1-p)^{2n-3} - (1-p)^{2n-2}\n\\;=\\;\np\\,(1-p)^{2n-3}.\n$$\n特别地，与独立性的主阶偏差恰好是 $p\\,(1-p)^{2n-3}$。因此，\n$$\nL(c)\n\\;=\\;\n\\lim_{n\\to\\infty} n\\left[(1-p_{n})^{2n-3} - (1-p_{n})^{2n-2}\\right]\n\\;=\\;\n\\lim_{n\\to\\infty} n\\,p_{n}\\,(1-p_{n})^{2n-3}.\n$$\n在 $p_{n}=\\frac{c}{n}$ 的条件下，我们有 $n\\,p_{n}=c$。剩下需要计算极限 $(1-p_{n})^{2n-3}$：\n$$\n\\lim_{n\\to\\infty} (1-\\tfrac{c}{n})^{2n-3}\n\\;=\\;\n\\exp\\!\\left(\\lim_{n\\to\\infty} (2n-3)\\,\\ln\\!\\big(1-\\tfrac{c}{n}\\big)\\right).\n$$\n当 $x\\to 0$ 时，使用展开式 $\\ln(1-x) = -x + \\mathcal{O}(x^{2})$，我们得到\n$$\n(2n-3)\\,\\ln\\!\\big(1-\\tfrac{c}{n}\\big)\n\\;=\\;\n(2n-3)\\left(-\\tfrac{c}{n} + \\mathcal{O}\\!\\left(\\tfrac{1}{n^{2}}\\right)\\right)\n\\;=\\;\n-2c + \\mathcal{O}\\!\\left(\\tfrac{1}{n}\\right),\n$$\n这得出\n$$\n\\lim_{n\\to\\infty} (1-\\tfrac{c}{n})^{2n-3} \\;=\\; \\exp(-2c).\n$$\n将这些结合起来，\n$$\nL(c) \\;=\\; \\left(\\lim_{n\\to\\infty} n\\,p_{n}\\right)\\left(\\lim_{n\\to\\infty} (1-p_{n})^{2n-3}\\right) \\;=\\; c\\,\\exp(-2c).\n$$\n这个闭式表达式表明，孤立事件之间的协方差是正的，但以 $\\frac{1}{n}$ 的速率消失，其主项系数为 $c\\,\\exp(-2c)$，从而量化了稀疏 ER 情况下的近似独立性。",
            "answer": "$$\\boxed{c\\,\\exp(-2c)}$$"
        },
        {
            "introduction": "网络科学中的一个关键任务是从观测数据中推断模型参数。假设一个稀疏随机图的度分布遵循泊松分布，我们应如何估计其关键参数 $c$ 呢？本练习将介绍一种基础的统计技术——矩方法（method of moments）——来解决这个问题 。你将通过此练习，把泊松分布的理论均值（即参数 $c$）与网络中观测到的样本平均度联系起来，并进一步探索该估计量的统计性质。这项实践提供了一种直接的参数估计方法，并展示了泊松分布的性质如何在实际统计情境中得到应用。",
            "id": "4297665",
            "problem": "考虑一个由 Erdős–Rényi (ER) 随机图模型生成的大型稀疏网络，其中每对节点以概率 $p$ 独立地连接。在稀疏情况下，当节点数为 $n$ 且 $n \\to \\infty$ 时，连接概率为 $p = c/(n-1)$（其中 $c$ 为常数），随机选择一个节点的度 $k$ 可以很好地用参数为 $c$ 的泊松分布来近似。假设您从该网络中无放回地均匀采样 $m$ 个节点，并记录它们的度为 $k_{1}, k_{2}, \\dots, k_{m}$。假设这些度是独立的，并且每个度都服从 $\\mathrm{Poi}(c)$ 分布。使用矩估计法，基于样本平均度构建 $c$ 的估计量 $\\hat{c}$。然后，在所述假设下，计算方差 $\\mathrm{Var}(\\hat{c})$。以精确的符号形式提供您的最终答案，形式为序对 $\\left(\\hat{c}, \\mathrm{Var}(\\hat{c})\\right)$。不需要进行数值舍入。",
            "solution": "问题要求两个量：泊松分布参数 $c$ 的矩估计量 $\\hat{c}$，以及它的方差 $\\mathrm{Var}(\\hat{c})$。\n\n**1. 构造估计量 $\\hat{c}$**\n\n矩估计法通过将分布的理论矩与从数据计算出的相应样本矩相等来工作。我们将使用一阶矩，即均值。\n\n参数为 $c$ 的泊松分布（记为 $\\mathrm{Poi}(c)$）的理论一阶矩是其期望值 $E[k]$。对于泊松分布，期望值等于其参数。\n$$\nE[k] = c\n$$\n一阶样本矩是观测到的度 $k_{1}, k_{2}, \\dots, k_{m}$ 的样本均值。样本均值记为 $\\bar{k}$，计算如下：\n$$\n\\bar{k} = \\frac{1}{m} \\sum_{i=1}^{m} k_i\n$$\n$c$ 的矩估计量，记为 $\\hat{c}$，通过令理论一阶矩等于样本一阶矩得到：\n$$\n\\hat{c} = \\bar{k} = \\frac{1}{m} \\sum_{i=1}^{m} k_i\n$$\n这就是 $c$ 的估计量。\n\n**2. 计算方差 $\\mathrm{Var}(\\hat{c})$**\n\n接下来，我们计算该估计量的方差。$\\hat{c}$ 的方差就是样本均值的方差。\n$$\n\\mathrm{Var}(\\hat{c}) = \\mathrm{Var}(\\bar{k}) = \\mathrm{Var}\\left(\\frac{1}{m} \\sum_{i=1}^{m} k_i\\right)\n$$\n根据方差的性质，对于常数 $a$，有 $\\mathrm{Var}(aX) = a^2 \\mathrm{Var}(X)$。这里，$a = 1/m$。\n$$\n\\mathrm{Var}(\\hat{c}) = \\left(\\frac{1}{m}\\right)^2 \\mathrm{Var}\\left(\\sum_{i=1}^{m} k_i\\right) = \\frac{1}{m^2} \\mathrm{Var}\\left(\\sum_{i=1}^{m} k_i\\right)\n$$\n问题明确指出，抽样得到的度 $k_1, k_2, \\dots, k_m$ 应被视为独立的随机变量。对于独立随机变量的和，其方差等于各方差之和：\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{m} k_i\\right) = \\sum_{i=1}^{m} \\mathrm{Var}(k_i)\n$$\n假设每个度 $k_i$ 服从 $\\mathrm{Poi}(c)$ 分布。泊松分布的一个关键性质是其方差等于其均值（也等于其参数）。\n$$\n\\mathrm{Var}(k_i) = c \\quad \\text{for all } i = 1, 2, \\dots, m\n$$\n因此，方差之和为：\n$$\n\\sum_{i=1}^{m} \\mathrm{Var}(k_i) = \\sum_{i=1}^{m} c = mc\n$$\n将此结果代回 $\\mathrm{Var}(\\hat{c})$ 的表达式中：\n$$\n\\mathrm{Var}(\\hat{c}) = \\frac{1}{m^2} (mc) = \\frac{c}{m}\n$$\n因此，估计量 $\\hat{c}$ 的方差为 $\\frac{c}{m}$。\n\n所要求的序对是 $(\\hat{c}, \\mathrm{Var}(\\hat{c}))$。\n- 估计量为 $\\hat{c} = \\frac{1}{m} \\sum_{i=1}^{m} k_i$。\n- 估计量的方差为 $\\mathrm{Var}(\\hat{c}) = \\frac{c}{m}$。\n\n最终答案是这两个表达式组成的序对。",
            "answer": "$$\n\\boxed{\\left(\\frac{1}{m}\\sum_{i=1}^{m} k_i, \\frac{c}{m}\\right)}\n$$"
        },
        {
            "introduction": "虽然矩方法提供了参数估计的一种途径，但最大似然估计（Maximum Likelihood Estimation, MLE）因其优良的理论性质而通常更受青睐。本练习首先要求你推导参数 $c$ 的最大似然估计量，其结果与矩方法估计量恰好相同。然而，本练习的核心在于一项更严谨的挑战：证明这个估计量在真实的 Erdős–Rényi 模型中是*一致*的，尽管在该模型中节点的度并非严格独立 。这要求我们超越简化的泊松假设，运用如图论基本恒等式等更根本的性质来完成证明。这项高级练习将帮助你弥合简化建模假设与真实数据生成过程之间的理论鸿沟，从而更深刻地理解统计估计量在网络科学中的稳健性。",
            "id": "4297652",
            "problem": "考虑一个在 $n$ 个顶点上观察到的无向、简单、有标签的图序列，其观察到的度为 $\\{k_i\\}_{i=1}^n$。假设为了对网络稀疏性进行参数推断，这些度被建模为来自未知均值为 $c$ 的泊松分布的独立同分布实现。从泊松概率质量函数的定义和最大似然原理出发，推导 $c$ 的最大似然估计量（Maximum Likelihood Estimator (MLE)）关于 $\\{k_i\\}_{i=1}^n$ 的显式表达式。然后，在数据实际上来自 $p = c/n$ 的 Erdős–Rényi (ER) $G(n,p)$ 随机图的情况下，评估该估计量的大$n$一致性，评估时仅使用 $G(n,p)$ 的基础性质，如边的独立性、度数之和等于边数两倍的恒等式，以及标准的大数定律。你的评估应确定该估计量是否在 $n \\to \\infty$ 时依概率收敛于真实的 $c$，并在不假设度独立的情况下证明此收敛。报告该估计量的表达式作为你的最终答案。",
            "solution": "**第一部分：最大似然估计量（MLE）的推导**\n\n问题首先将观察到的度 $\\{k_i\\}_{i=1}^n$ 建模为来自未知均值参数为 $c$ 的泊松分布的独立同分布（i.i.d.）随机变量。单个观测值 $k_i$ 来自泊松($c$)分布的概率质量函数（PMF）由下式给出：\n$$\nP(K=k_i | c) = \\frac{\\exp(-c) c^{k_i}}{k_i!}\n$$\n似然函数 $L(c; \\{k_i\\})$ 是观察到整个数据集 $\\{k_i\\}_{i=1}^n$ 的联合概率。由于i.i.d.假设，似然函数是各个概率的乘积：\n$$\nL(c; \\{k_i\\}) = \\prod_{i=1}^{n} P(K=k_i | c) = \\prod_{i=1}^{n} \\frac{\\exp(-c) c^{k_i}}{k_i!}\n$$\n最大化似然函数等价于最大化其自然对数，即对数似然函数 $\\ell(c) = \\ln(L(c))$，这在代数上通常更简单。\n$$\n\\ell(c) = \\ln \\left( \\prod_{i=1}^{n} \\frac{\\exp(-c) c^{k_i}}{k_i!} \\right) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\exp(-c) c^{k_i}}{k_i!} \\right)\n$$\n利用对数的性质，我们可以展开此表达式：\n$$\n\\ell(c) = \\sum_{i=1}^{n} (\\ln(\\exp(-c)) + \\ln(c^{k_i}) - \\ln(k_i!))\n$$\n$$\n\\ell(c) = \\sum_{i=1}^{n} (-c + k_i \\ln(c) - \\ln(k_i!))\n$$\n$$\n\\ell(c) = -nc + \\ln(c) \\sum_{i=1}^{n} k_i - \\sum_{i=1}^{n} \\ln(k_i!)\n$$\n为了找到使该函数最大化的 $c$ 值，我们对 $\\ell(c)$ 关于 $c$ 求一阶导数并令其为零。\n$$\n\\frac{d\\ell}{dc} = \\frac{d}{dc} \\left( -nc + \\ln(c) \\sum_{i=1}^{n} k_i - \\sum_{i=1}^{n} \\ln(k_i!) \\right) = -n + \\frac{1}{c} \\sum_{i=1}^{n} k_i\n$$\n将导数设为零，得到MLE的方程，记为 $\\hat{c}_{\\text{MLE}}$：\n$$\n-n + \\frac{1}{\\hat{c}_{\\text{MLE}}} \\sum_{i=1}^{n} k_i = 0\n$$\n解出 $\\hat{c}_{\\text{MLE}}$ 得：\n$$\n\\hat{c}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} k_i\n$$\n该表达式是观察到的度的样本均值。为确认这是一个最大值，我们检查二阶导数：\n$$\n\\frac{d^2\\ell}{dc^2} = -\\frac{1}{c^2} \\sum_{i=1}^{n} k_i\n$$\n由于度 $k_i$ 是非负的且 $c > 0$，二阶导数为负（假设并非所有度都为零），这证实了 $\\hat{c}_{\\text{MLE}}$ 确实是一个最大似然估计量。\n\n**第二部分：Erdős–Rényi模型的一致性评估**\n\n现在我们评估当数据实际上来自一个 Erdős–Rényi $G(n,p)$ 随机图时，该估计量是否是一致的，其中连接概率为 $p = c/n$。一致性意味着当顶点数 $n$ 趋于无穷大时，估计量依概率收敛于真实参数 $c$。我们必须证明：\n$$\n\\hat{c}_n = \\frac{1}{n} \\sum_{i=1}^{n} K_i \\xrightarrow{p} c \\quad \\text{as } n \\to \\infty\n$$\n其中 $\\{K_i\\}_{i=1}^n$ 是 $G(n,p)$ 图中度的随机变量。关键的约束条件是我们不能假设度 $K_i$ 是独立的。\n\n我们使用基本的图论恒等式：所有顶点的度之和等于图中边数的两倍。令 $M$ 为表示 $G(n,p)$ 中边数的随机变量。则：\n$$\n\\sum_{i=1}^{n} K_i = 2M\n$$\n将此代入我们的估计量 $\\hat{c}_n$ 的表达式中：\n$$\n\\hat{c}_n = \\frac{1}{n} (2M) = \\frac{2M}{n}\n$$\n在 $G(n,p)$ 图中，有 $\\binom{n}{2}$ 条潜在的边。模型指定每条边都以概率 $p$ 独立存在。因此，总边数 $M$ 是 $\\binom{n}{2}$ 个独立同分布的伯努利($p$)随机变量之和。这意味着 $M$ 服从二项分布：\n$$\nM \\sim \\text{Binomial}\\left(\\binom{n}{2}, p\\right)\n$$\n如果一个估计量是渐近无偏的，并且其方差收敛于零，那么它就是一致的。\n首先，我们计算估计量 $\\hat{c}_n$ 的期望值：\n$$\nE[\\hat{c}_n] = E\\left[\\frac{2M}{n}\\right] = \\frac{2}{n} E[M]\n$$\n二项随机变量的期望是试验次数乘以成功概率：\n$$\nE[M] = \\binom{n}{2} p = \\frac{n(n-1)}{2} p\n$$\n将此代入 $E[\\hat{c}_n]$ 的表达式中，并使用 $p = c/n$：\n$$\nE[\\hat{c}_n] = \\frac{2}{n} \\left(\\frac{n(n-1)}{2} \\frac{c}{n}\\right) = \\frac{c(n-1)}{n} = c - \\frac{c}{n}\n$$\n估计量的偏差是 $E[\\hat{c}_n] - c = -c/n$。当 $n \\to \\infty$ 时，偏差趋近于 $0$。该估计量是渐近无偏的。\n\n接下来，我们计算 $\\hat{c}_n$ 的方差：\n$$\n\\text{Var}(\\hat{c}_n) = \\text{Var}\\left(\\frac{2M}{n}\\right) = \\frac{4}{n^2} \\text{Var}(M)\n$$\n二项随机变量的方差是试验次数乘以成功概率再乘以失败概率：\n$$\n\\text{Var}(M) = \\binom{n}{2} p(1-p) = \\frac{n(n-1)}{2} p(1-p)\n$$\n将此代入 $\\text{Var}(\\hat{c}_n)$ 的表达式中，并使用 $p=c/n$：\n$$\n\\text{Var}(\\hat{c}_n) = \\frac{4}{n^2} \\left(\\frac{n(n-1)}{2} \\frac{c}{n} \\left(1-\\frac{c}{n}\\right)\\right) = \\frac{2c(n-1)}{n^2} \\left(1-\\frac{c}{n}\\right)\n$$\n现在我们取 $n \\to \\infty$ 时的极限：\n$$\n\\lim_{n \\to \\infty} \\text{Var}(\\hat{c}_n) = \\lim_{n \\to \\infty} \\frac{2c(n-1)}{n^2} \\left(1-\\frac{c}{n}\\right) = \\lim_{n \\to \\infty} \\left( \\frac{2c}{n} - \\frac{2c}{n^2} \\right) \\left(1-\\frac{c}{n}\\right)\n$$\n由于 $\\lim_{n \\to \\infty} \\frac{1}{n} = 0$ 和 $\\lim_{n \\to \\infty} \\frac{1}{n^2} = 0$，第一项趋于 $0$，第二项趋于 $1$。因此：\n$$\n\\lim_{n \\to \\infty} \\text{Var}(\\hat{c}_n) = 0 \\times 1 = 0\n$$\n由于估计量的偏差和方差都随着 $n \\to \\infty$ 收敛到零，根据一致性的均方误差准则（通过切比雪夫不等式意味着依概率收敛），该估计量是一致的。在简化的i.i.d.泊松模型下推导出的估计量，对于更现实的Erdős–Rényi图模型中的参数 $c$ 确实是一个一致的估计量，而且我们是在没有假设度独立的情况下证明了这一点。标准大数定律的使用隐含在二项变量 $M$（经过适当缩放后）收敛到其期望值的过程中，我们在分析 $E[\\hat{c}_n]$ 和 $\\text{Var}(\\hat{c}_n)$ 时利用了这一点。\n\n估计量的显式表达式是最终答案。",
            "answer": "$$\n\\boxed{\\frac{1}{n} \\sum_{i=1}^{n} k_i}\n$$"
        }
    ]
}