{
    "hands_on_practices": [
        {
            "introduction": "The Poisson approximation for degree distributions in sparse random graphs is built on the foundational idea that the states of any two vertices are nearly independent. This exercise makes this abstract concept concrete by guiding you to calculate the probability of isolation (degree zero) for one and then two vertices from first principles. By analyzing how the joint probability deviates from the product of the marginal probabilities, you will quantify the subtle, vanishing correlation between vertices, providing a solid, intuitive grasp of why the Poisson model is so effective in this regime .",
            "id": "4297656",
            "problem": "Consider an Erdős–Rényi (ER) random graph $G(n,p)$ on $n$ labeled vertices, where each potential edge is present independently with probability $p$. Fix two distinct vertices labeled $i$ and $j$. A vertex is said to be isolated if it has degree $0$.\n\nStarting only from the independence of edges in $G(n,p)$ and the definition of isolation, derive expressions for the probability that a given vertex is isolated and the probability that both $i$ and $j$ are isolated. Then, for the sparse regime $p=p_{n}=\\frac{c}{n}$ with fixed $c>0$, use your expressions to discuss the approximate independence (as $n \\to \\infty$) of the isolation events for $i$ and $j$.\n\nTo quantify the leading-order deviation from independence, define\n$$\nL(c)\\;=\\;\\lim_{n\\to\\infty} n\\left[\\mathbb{P}\\big(\\text{$i$ and $j$ are isolated in }G(n,p_{n})\\big)\\;-\\;\\mathbb{P}\\big(\\text{$i$ is isolated in }G(n,p_{n})\\big)^{2}\\right].\n$$\nCompute $L(c)$ in closed form as a function of $c$. Your final answer must be a single analytic expression in terms of $c$ only.",
            "solution": "We begin with the Erdős–Rényi (ER) random graph $G(n,p)$ model in which each of the $\\binom{n}{2}$ potential edges is included independently with probability $p$, and thus excluded independently with probability $1-p$. A vertex is isolated if all edges incident to it are absent.\n\nFirst, we compute the probability that a fixed vertex $i$ is isolated. There are $n-1$ potential edges incident to $i$. By independence of edges and the fact that each edge is absent with probability $1-p$, the probability that all $n-1$ incident edges are absent is\n$$\n\\mathbb{P}(\\text{$i$ isolated}) \\;=\\; (1-p)^{n-1}.\n$$\n\nNext, we compute the probability that both distinct vertices $i$ and $j$ are isolated. For $i$ to be isolated, all $n-1$ edges adjacent to $i$ must be absent; similarly, for $j$ to be isolated, all $n-1$ edges adjacent to $j$ must be absent. However, the edge between $i$ and $j$ is counted in both sets, so we need to count the number of distinct edges that must be absent for both $i$ and $j$ to be isolated. The set of edges incident to $i$ or $j$ consists of the $(n-2)$ edges from $i$ to vertices other than $j$, the $(n-2)$ edges from $j$ to vertices other than $i$, and the single edge $\\{i,j\\}$. Hence, a total of $2(n-2)+1 = 2n-3$ distinct edges must be absent. By independence,\n$$\n\\mathbb{P}(\\text{$i$ and $j$ isolated}) \\;=\\; (1-p)^{2n-3}.\n$$\n\nTo discuss approximate independence, compare the joint probability to the product of the marginals:\n$$\n\\frac{\\mathbb{P}(\\text{$i$ and $j$ isolated})}{\\mathbb{P}(\\text{$i$ isolated})\\,\\mathbb{P}(\\text{$j$ isolated})}\n\\;=\\;\n\\frac{(1-p)^{2n-3}}{(1-p)^{n-1}\\,(1-p)^{n-1}}\n\\;=\\;\n(1-p)^{-1}.\n$$\nFor $p=p_{n}=\\frac{c}{n}$ with fixed $c>0$, we have\n$$\n(1-p_{n})^{-1} \\;=\\; \\frac{1}{1-\\frac{c}{n}} \\;=\\; 1 \\;+\\; \\frac{c}{n} \\;+\\; \\mathcal{O}\\!\\left(\\frac{1}{n^{2}}\\right),\n$$\nwhich tends to $1$ as $n\\to\\infty$. This indicates that the isolation events of $i$ and $j$ are asymptotically independent in the sparse regime, although they are not exactly independent for finite $n$.\n\nTo quantify the leading-order deviation from independence, we consider the covariance of the isolation indicators and scale it by $n$. Define indicators $X_{i}=\\mathbf{1}\\{\\text{$i$ isolated}\\}$ and $X_{j}=\\mathbf{1}\\{\\text{$j$ isolated}\\}$. Then\n$$\n\\operatorname{Cov}(X_{i},X_{j})\n\\;=\\;\n\\mathbb{P}(\\text{$i$ and $j$ isolated}) - \\mathbb{P}(\\text{$i$ isolated})^{2}\n\\;=\\;\n(1-p)^{2n-3} - (1-p)^{2n-2}\n\\;=\\;\np\\,(1-p)^{2n-3}.\n$$\nIn particular, the leading deviation from independence is exactly $p\\,(1-p)^{2n-3}$. Therefore,\n$$\nL(c)\n\\;=\\;\n\\lim_{n\\to\\infty} n\\left[(1-p_{n})^{2n-3} - (1-p_{n})^{2n-2}\\right]\n\\;=\\;\n\\lim_{n\\to\\infty} n\\,p_{n}\\,(1-p_{n})^{2n-3}.\n$$\nUnder $p_{n}=\\frac{c}{n}$, we have $n\\,p_{n}=c$. It remains to evaluate the limit $(1-p_{n})^{2n-3}$:\n$$\n\\lim_{n\\to\\infty} (1-\\tfrac{c}{n})^{2n-3}\n\\;=\\;\n\\exp\\!\\left(\\lim_{n\\to\\infty} (2n-3)\\,\\ln\\!\\big(1-\\tfrac{c}{n}\\big)\\right).\n$$\nUsing the expansion $\\ln(1-x) = -x + \\mathcal{O}(x^{2})$ as $x\\to 0$, we get\n$$\n(2n-3)\\,\\ln\\!\\big(1-\\tfrac{c}{n}\\big)\n\\;=\\;\n(2n-3)\\left(-\\tfrac{c}{n} + \\mathcal{O}\\!\\left(\\tfrac{1}{n^{2}}\\right)\\right)\n\\;=\\;\n-2c + \\mathcal{O}\\!\\left(\\tfrac{1}{n}\\right),\n$$\nwhich yields\n$$\n\\lim_{n\\to\\infty} (1-\\tfrac{c}{n})^{2n-3} \\;=\\; \\exp(-2c).\n$$\nCombining these,\n$$\nL(c) \\;=\\; \\left(\\lim_{n\\to\\infty} n\\,p_{n}\\right)\\left(\\lim_{n\\to\\infty} (1-p_{n})^{2n-3}\\right) \\;=\\; c\\,\\exp(-2c).\n$$\nThis closed-form expression shows that the covariance between isolation events is positive but vanishes at rate $\\frac{1}{n}$, with leading coefficient $c\\,\\exp(-2c)$, thereby quantifying the approximate independence in the sparse ER regime.",
            "answer": "$$\\boxed{c\\,\\exp(-2c)}$$"
        },
        {
            "introduction": "Once we establish a model, the next step is to fit it to data. This practice bridges theory and application by asking you to estimate the network's average degree $c$ using the powerful method of maximum likelihood. Crucially, this exercise distinguishes between the simplified assumption used for derivation (that degrees are independent) and the reality of the Erdős–Rényi graph. By proving that the estimator is consistent even when degrees are not strictly independent, you will develop a deeper appreciation for the robustness of statistical models .",
            "id": "4297652",
            "problem": "Consider a sequence of undirected, simple, labeled graphs observed on $n$ vertices, with observed degrees $\\{k_i\\}_{i=1}^n$. Suppose that, for the purpose of parametric inference on network sparsity, the degrees are modeled as independent and identically distributed realizations from a Poisson distribution with unknown mean $c$. Starting from the definition of the Poisson probability mass function and the principle of maximum likelihood, derive the explicit expression for the maximum likelihood estimator (Maximum Likelihood Estimator (MLE)) of $c$ in terms of $\\{k_i\\}_{i=1}^n$. Then, assess the large-$n$ consistency of this estimator when the data actually come from an Erdős–Rényi (ER) $G(n,p)$ random graph with $p = c/n$, using only foundational properties of $G(n,p)$ such as independence of edges, the identity that the sum of degrees equals twice the number of edges, and standard laws of large numbers. Your assessment should establish whether the estimator converges in probability to the true $c$ as $n \\to \\infty$ and justify this convergence without assuming independence of degrees. Report the estimator expression as your final answer.",
            "solution": "**Part 1: Derivation of the Maximum Likelihood Estimator (MLE)**\n\nWe model the observed degrees $\\{k_i\\}_{i=1}^n$ as independent and identically distributed (i.i.d.) realizations from a Poisson distribution with an unknown mean $c$. The probability mass function (PMF) for a single observation $k_i$ is $P(K=k_i | c) = \\frac{\\exp(-c) c^{k_i}}{k_i!}$.\n\nThe likelihood function, $L(c; \\{k_i\\})$, is the joint probability of observing the entire dataset. Under the i.i.d. assumption, it is the product of the individual probabilities:\n$$\nL(c; \\{k_i\\}) = \\prod_{i=1}^{n} \\frac{\\exp(-c) c^{k_i}}{k_i!}\n$$\nTo simplify maximization, we work with the log-likelihood function, $\\ell(c) = \\ln(L(c))$:\n$$\n\\ell(c) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\exp(-c) c^{k_i}}{k_i!} \\right) = \\sum_{i=1}^{n} (-c + k_i \\ln(c) - \\ln(k_i!))\n$$\n$$\n\\ell(c) = -nc + (\\ln c) \\sum_{i=1}^{n} k_i - \\sum_{i=1}^{n} \\ln(k_i!)\n$$\nTo find the maximum, we take the derivative with respect to $c$ and set it to zero:\n$$\n\\frac{d\\ell}{dc} = -n + \\frac{1}{c} \\sum_{i=1}^{n} k_i = 0\n$$\nSolving for the MLE, $\\hat{c}_{\\text{MLE}}$, yields:\n$$\n\\hat{c}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} k_i\n$$\nThis is the sample mean of the observed degrees. The second derivative, $\\frac{d^2\\ell}{dc^2} = -\\frac{1}{c^2} \\sum_{i=1}^{n} k_i$, is negative (for non-empty graphs), confirming this is a maximum.\n\n**Part 2: Consistency Assessment for the Erdős–Rényi Model**\n\nNow we assess if this estimator is consistent when the data comes from an Erdős–Rényi $G(n,p)$ graph with $p = c/n$. Consistency requires that the estimator converges in probability to the true parameter $c$ as $n \\to \\infty$:\n$$\n\\hat{c}_n = \\frac{1}{n} \\sum_{i=1}^{n} K_i \\xrightarrow{p} c\n$$\nWe proceed without assuming the degrees $\\{K_i\\}$ are independent. We use the identity that the sum of degrees equals twice the number of edges, $M$:\n$$\n\\sum_{i=1}^{n} K_i = 2M\n$$\nThe estimator can thus be written as $\\hat{c}_n = \\frac{2M}{n}$. In a $G(n,p)$ graph, the total number of edges $M$ follows a binomial distribution, $M \\sim \\text{Binomial}\\left(\\binom{n}{2}, p\\right)$, since each of the $\\binom{n}{2}$ potential edges is an independent Bernoulli trial.\n\nWe check for consistency by examining the bias and variance of the estimator. The expectation of $\\hat{c}_n$ is:\n$$\nE[\\hat{c}_n] = E\\left[\\frac{2M}{n}\\right] = \\frac{2}{n} E[M] = \\frac{2}{n} \\left(\\binom{n}{2} p\\right) = \\frac{2}{n} \\frac{n(n-1)}{2} \\frac{c}{n} = \\frac{c(n-1)}{n} = c - \\frac{c}{n}\n$$\nThe bias, $E[\\hat{c}_n] - c = -c/n$, approaches 0 as $n \\to \\infty$. The estimator is asymptotically unbiased.\n\nThe variance of $\\hat{c}_n$ is:\n$$\n\\text{Var}(\\hat{c}_n) = \\text{Var}\\left(\\frac{2M}{n}\\right) = \\frac{4}{n^2} \\text{Var}(M) = \\frac{4}{n^2} \\left(\\binom{n}{2} p(1-p)\\right)\n$$\n$$\n\\text{Var}(\\hat{c}_n) = \\frac{4}{n^2} \\frac{n(n-1)}{2} \\frac{c}{n} \\left(1-\\frac{c}{n}\\right) = \\frac{2c(n-1)}{n^2} \\left(1-\\frac{c}{n}\\right)\n$$\nAs $n \\to \\infty$, this variance approaches 0. Since both the bias and variance go to zero, the estimator is consistent. Thus, the sample mean of degrees is a consistent estimator for the average degree parameter $c$ in the sparse Erdős–Rényi model.",
            "answer": "$$\n\\boxed{\\frac{1}{n} \\sum_{i=1}^{n} k_i}\n$$"
        },
        {
            "introduction": "This advanced exercise challenges you to look beyond the properties of a single vertex and investigate a global feature of the network: the total count of vertices with a specific degree $k$. While a single vertex's degree is well-approximated by a Poisson distribution with mean $c$, it does not automatically follow that the number of such vertices, $X_k$, is a Poisson random variable with mean $n\\mathbb{P}(D=k)$. By calculating the variance of $X_k$ and comparing it to its mean, you will uncover the effect of subtle correlations between vertex degrees and gain a more refined understanding of the network's global structure .",
            "id": "4297639",
            "problem": "Consider the independent-edge random graph model $G(n,p)$ on $n$ labeled vertices, where each of the $\\binom{n}{2}$ possible edges is present independently with probability $p$. Fix an integer $k \\ge 0$. Let $X_k$ denote the number of vertices of degree exactly $k$ in $G(n,p)$. Using only the core definitions of $G(n,p)$ and independence of edges, proceed as follows:\n\n- Express $X_k$ as a sum of vertex-level indicator random variables and derive an exact, closed-form expression for $\\mathrm{Var}(X_k)$ in terms of $n$, $p$, and binomial point probabilities that arise from the degree distribution of a single vertex and joint degree events for vertex pairs. Your derivation must be from first principles and should not invoke any pre-packaged variance formulas beyond the definition $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$ and bilinearity of covariance.\n\n- Specialize to the sparse connectivity regime $p = c/(n-1)$ with fixed $c>0$ and fixed $k$. Using only classical large-$n$ limits for binomial distributions, obtain the leading-order asymptotic form of $\\mathrm{Var}(X_k)$ as $n \\to \\infty$, expressed as $n$ times an explicit function of $c$ and $k$.\n\n- Compare this to the Poisson-mean benchmark $\\lambda_n = n \\exp(-c)\\, c^{k}/k!$ suggested by the Poisson approximation for the degree distribution of a single vertex. Compute the leading-order ratio $\\mathrm{Var}(X_k)/\\lambda_n$ as $n \\to \\infty$, simplified to a closed-form expression in $c$ and $k$.\n\nProvide the leading-order ratio $\\mathrm{Var}(X_k)/\\lambda_n$ as your final answer in a single closed-form analytic expression in terms of $c$ and $k$. No rounding is required.",
            "solution": "**Part 1: Exact Expression for $\\mathrm{Var}(X_k)$**\n\nLet $I_i$ be the indicator variable that vertex $v_i$ has degree $k$. Then $X_k = \\sum_{i=1}^{n} I_i$. The variance is given by $\\mathrm{Var}(X_k) = \\sum_{i,j} \\mathrm{Cov}(I_i, I_j) = n \\mathrm{Var}(I_1) + n(n-1) \\mathrm{Cov}(I_1, I_2)$.\n\nThe degree of a single vertex follows a binomial distribution $B(n-1, p)$. Let $P_k = \\mathbb{P}(I_1=1) = \\binom{n-1}{k} p^k (1-p)^{n-1-k}$.\nThe variance term for a single indicator is $\\mathrm{Var}(I_1) = \\mathbb{E}[I_1^2] - (\\mathbb{E}[I_1])^2 = P_k - P_k^2 = P_k(1-P_k)$.\n\nThe covariance term is $\\mathrm{Cov}(I_1, I_2) = \\mathbb{E}[I_1 I_2] - \\mathbb{E}[I_1]\\mathbb{E}[I_2] = P_{kk} - P_k^2$, where $P_{kk} = \\mathbb{P}(d(v_1)=k, d(v_2)=k)$.\nCombining these gives the exact variance:\n$$\n\\mathrm{Var}(X_k) = n P_k(1-P_k) + n(n-1)(P_{kk} - P_k^2)\n$$\nA detailed calculation shows that the covariance can be expressed as $P_{kk} - P_k^2 = p(1-p) \\left[ b(n-2, k-1, p) - b(n-2, k, p) \\right]^2$, where $b(N, j, p) = \\binom{N}{j}p^j(1-p)^{N-j}$.\n\n**Part 2: Asymptotic Form of $\\mathrm{Var}(X_k)$**\n\nIn the regime $p = c/(n-1)$, the binomial probability $P_k$ converges to the Poisson probability $\\pi_k(c) = \\frac{c^k \\exp(-c)}{k!}$.\nThe term $n P_k(1-P_k)$ has the leading-order behavior $n \\pi_k(c)(1-\\pi_k(c))$.\nThe covariance term $n(n-1)(P_{kk}-P_k^2)$ is of order $n$. Asymptotically, $p(1-p) \\approx p \\approx c/n$, and $b(n-2, j, p) \\to \\pi_j(c)$. The covariance term becomes:\n$$\nn(n-1)(P_{kk} - P_k^2) \\approx n(n-1) \\frac{c}{n-1} \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2 = nc \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2\n$$\nSo, the leading-order variance is:\n$$\n\\mathrm{Var}(X_k) \\approx n\\pi_k(c)(1-\\pi_k(c)) + nc \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2\n$$\n\n**Part 3: Ratio $\\mathrm{Var}(X_k)/\\lambda_n$**\n\nThe benchmark is $\\lambda_n = n \\pi_k(c)$. We compute the ratio using the asymptotic variance:\n$$\n\\lim_{n\\to\\infty} \\frac{\\mathrm{Var}(X_k)}{\\lambda_n} = \\frac{n\\pi_k(c)(1-\\pi_k(c)) + nc \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2}{n \\pi_k(c)} = (1-\\pi_k(c)) + \\frac{c}{\\pi_k(c)} \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2\n$$\nUsing the Poisson recurrence relation $\\pi_{k-1}(c) = \\frac{k}{c}\\pi_k(c)$:\n\\begin{align*}\n\\text{Ratio} &= 1 - \\pi_k(c) + \\frac{c}{\\pi_k(c)} \\left[ \\frac{k}{c}\\pi_k(c) - \\pi_k(c) \\right]^2 \\\\\n&= 1 - \\pi_k(c) + \\frac{c}{\\pi_k(c)} \\left( \\pi_k(c) \\left( \\frac{k-c}{c} \\right) \\right)^2 \\\\\n&= 1 - \\pi_k(c) + c \\pi_k(c) \\frac{(k-c)^2}{c^2} \\\\\n&= 1 - \\pi_k(c) + \\pi_k(c) \\frac{(k-c)^2}{c}\n\\end{align*}\nSubstituting the explicit form of $\\pi_k(c) = \\frac{c^k \\exp(-c)}{k!}$ gives:\n$$\n\\text{Ratio} = 1 - \\frac{c^k \\exp(-c)}{k!} + \\frac{(k-c)^2}{c} \\frac{c^k \\exp(-c)}{k!} = 1 - \\frac{c^k \\exp(-c)}{k!} + (k-c)^2 \\frac{c^{k-1} \\exp(-c)}{k!}\n$$",
            "answer": "$$\\boxed{1 - \\frac{c^k \\exp(-c)}{k!} + (k-c)^2 \\frac{c^{k-1} \\exp(-c)}{k!}}$$"
        }
    ]
}