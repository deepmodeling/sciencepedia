{
    "hands_on_practices": [
        {
            "introduction": "Understanding the Poisson degree distribution begins with hands-on probabilistic calculation. This exercise tasks you with deriving the probability of finding isolated vertices in an Erdős–Rényi graph directly from the first principle of edge independence. By comparing the joint probability of two vertices being isolated to the product of their individual probabilities, you will rigorously explore the concept of asymptotic independence and quantify how the small correlation between these events vanishes in large, sparse networks. ",
            "id": "4297656",
            "problem": "Consider an Erdős–Rényi (ER) random graph $G(n,p)$ on $n$ labeled vertices, where each potential edge is present independently with probability $p$. Fix two distinct vertices labeled $i$ and $j$. A vertex is said to be isolated if it has degree $0$.\n\nStarting only from the independence of edges in $G(n,p)$ and the definition of isolation, derive expressions for the probability that a given vertex is isolated and the probability that both $i$ and $j$ are isolated. Then, for the sparse regime $p=p_{n}=\\frac{c}{n}$ with fixed $c0$, use your expressions to discuss the approximate independence (as $n \\to \\infty$) of the isolation events for $i$ and $j$.\n\nTo quantify the leading-order deviation from independence, define\n$$\nL(c)\\;=\\;\\lim_{n\\to\\infty} n\\left[\\mathbb{P}\\big(\\text{$i$ and $j$ are isolated in }G(n,p_{n})\\big)\\;-\\;\\mathbb{P}\\big(\\text{$i$ is isolated in }G(n,p_{n})\\big)^{2}\\right].\n$$\nCompute $L(c)$ in closed form as a function of $c$. Your final answer must be a single analytic expression in terms of $c$ only.",
            "solution": "We begin with the Erdős–Rényi (ER) random graph $G(n,p)$ model in which each of the $\\binom{n}{2}$ potential edges is included independently with probability $p$, and thus excluded independently with probability $1-p$. A vertex is isolated if all edges incident to it are absent.\n\nFirst, we compute the probability that a fixed vertex $i$ is isolated. There are $n-1$ potential edges incident to $i$. By independence of edges and the fact that each edge is absent with probability $1-p$, the probability that all $n-1$ incident edges are absent is\n$$\n\\mathbb{P}(\\text{$i$ isolated}) \\;=\\; (1-p)^{n-1}.\n$$\n\nNext, we compute the probability that both distinct vertices $i$ and $j$ are isolated. For $i$ to be isolated, all $n-1$ edges adjacent to $i$ must be absent; similarly, for $j$ to be isolated, all $n-1$ edges adjacent to $j$ must be absent. However, the edge between $i$ and $j$ is counted in both sets, so we need to count the number of distinct edges that must be absent for both $i$ and $j$ to be isolated. The set of edges incident to $i$ or $j$ consists of the $(n-2)$ edges from $i$ to vertices other than $j$, the $(n-2)$ edges from $j$ to vertices other than $i$, and the single edge $\\{i,j\\}$. Hence, a total of $2(n-2)+1 = 2n-3$ distinct edges must be absent. By independence,\n$$\n\\mathbb{P}(\\text{$i$ and $j$ isolated}) \\;=\\; (1-p)^{2n-3}.\n$$\n\nTo discuss approximate independence, compare the joint probability to the product of the marginals:\n$$\n\\frac{\\mathbb{P}(\\text{$i$ and $j$ isolated})}{\\mathbb{P}(\\text{$i$ isolated})\\,\\mathbb{P}(\\text{$j$ isolated})}\n\\;=\\;\n\\frac{(1-p)^{2n-3}}{(1-p)^{n-1}\\,(1-p)^{n-1}}\n\\;=\\;\n(1-p)^{-1}.\n$$\nFor $p=p_{n}=\\frac{c}{n}$ with fixed $c0$, we have\n$$\n(1-p_{n})^{-1} \\;=\\; \\frac{1}{1-\\frac{c}{n}} \\;=\\; 1 \\;+\\; \\frac{c}{n} \\;+\\; \\mathcal{O}\\!\\left(\\frac{1}{n^{2}}\\right),\n$$\nwhich tends to $1$ as $n\\to\\infty$. This indicates that the isolation events of $i$ and $j$ are asymptotically independent in the sparse regime, although they are not exactly independent for finite $n$.\n\nTo quantify the leading-order deviation from independence, we consider the covariance of the isolation indicators and scale it by $n$. Define indicators $X_{i}=\\mathbf{1}\\{\\text{$i$ isolated}\\}$ and $X_{j}=\\mathbf{1}\\{\\text{$j$ isolated}\\}$. Then\n$$\n\\operatorname{Cov}(X_{i},X_{j})\n\\;=\\;\n\\mathbb{P}(\\text{$i$ and $j$ isolated}) - \\mathbb{P}(\\text{$i$ isolated})^{2}\n\\;=\\;\n(1-p)^{2n-3} - (1-p)^{2n-2}\n\\;=\\;\np\\,(1-p)^{2n-3}.\n$$\nIn particular, the leading deviation from independence is exactly $p\\,(1-p)^{2n-3}$. Therefore,\n$$\nL(c)\n\\;=\\;\n\\lim_{n\\to\\infty} n\\left[(1-p_{n})^{2n-3} - (1-p_{n})^{2n-2}\\right]\n\\;=\\;\n\\lim_{n\\to\\infty} n\\,p_{n}\\,(1-p_{n})^{2n-3}.\n$$\nUnder $p_{n}=\\frac{c}{n}$, we have $n\\,p_{n}=c$. It remains to evaluate the limit $(1-p_{n})^{2n-3}$:\n$$\n\\lim_{n\\to\\infty} (1-\\tfrac{c}{n})^{2n-3}\n\\;=\\;\n\\exp\\!\\left(\\lim_{n\\to\\infty} (2n-3)\\,\\ln\\!\\big(1-\\tfrac{c}{n}\\big)\\right).\n$$\nUsing the expansion $\\ln(1-x) = -x + \\mathcal{O}(x^{2})$ as $x\\to 0$, we get\n$$\n(2n-3)\\,\\ln\\!\\big(1-\\tfrac{c}{n}\\big)\n\\;=\\;\n(2n-3)\\left(-\\tfrac{c}{n} + \\mathcal{O}\\!\\left(\\tfrac{1}{n^{2}}\\right)\\right)\n\\;=\\;\n-2c + \\mathcal{O}\\!\\left(\\tfrac{1}{n}\\right),\n$$\nwhich yields\n$$\n\\lim_{n\\to\\infty} (1-\\tfrac{c}{n})^{2n-3} \\;=\\; \\exp(-2c).\n$$\nCombining these,\n$$\nL(c) \\;=\\; \\left(\\lim_{n\\to\\infty} n\\,p_{n}\\right)\\left(\\lim_{n\\to\\infty} (1-p_{n})^{2n-3}\\right) \\;=\\; c\\,\\exp(-2c).\n$$\nThis closed-form expression shows that the covariance between isolation events is positive but vanishes at rate $\\frac{1}{n}$, with leading coefficient $c\\,\\exp(-2c)$, thereby quantifying the approximate independence in the sparse ER regime.",
            "answer": "$$\\boxed{c\\,\\exp(-2c)}$$"
        },
        {
            "introduction": "Moving from theory to application, a key skill is to infer the parameters of a network model from observed data. This practice guides you through deriving the Maximum Likelihood Estimator (MLE) for the average degree $c$ by initially modeling the degrees as independent Poisson variables. The crucial part of the exercise is then to prove that this estimator is consistent, converging to the true value of $c$, even under the more realistic Erdős–Rényi model where degrees are not strictly independent, a powerful lesson in statistical modeling. ",
            "id": "4297652",
            "problem": "Consider a sequence of undirected, simple, labeled graphs observed on $n$ vertices, with observed degrees $\\{k_i\\}_{i=1}^n$. Suppose that, for the purpose of parametric inference on network sparsity, the degrees are modeled as independent and identically distributed realizations from a Poisson distribution with unknown mean $c$. Starting from the definition of the Poisson probability mass function and the principle of maximum likelihood, derive the explicit expression for the maximum likelihood estimator (Maximum Likelihood Estimator (MLE)) of $c$ in terms of $\\{k_i\\}_{i=1}^n$. Then, assess the large-$n$ consistency of this estimator when the data actually come from an Erdős–Rényi (ER) $G(n,p)$ random graph with $p = c/n$, using only foundational properties of $G(n,p)$ such as independence of edges, the identity that the sum of degrees equals twice the number of edges, and standard laws of large numbers. Your assessment should establish whether the estimator converges in probability to the true $c$ as $n \\to \\infty$ and justify this convergence without assuming independence of degrees. Report the estimator expression as your final answer.",
            "solution": "**Part 1: Derivation of the Maximum Likelihood Estimator (MLE)**\n\nThe problem begins by modeling the observed degrees $\\{k_i\\}_{i=1}^n$ as independent and identically distributed (i.i.d.) random variables from a Poisson distribution with an unknown mean parameter $c$. The probability mass function (PMF) for a single observation $k_i$ from a Poisson($c$) distribution is given by:\n$$\nP(K=k_i | c) = \\frac{\\exp(-c) c^{k_i}}{k_i!}\n$$\nThe likelihood function, $L(c; \\{k_i\\})$, is the joint probability of observing the entire dataset $\\{k_i\\}_{i=1}^n$. Due to the i.i.d. assumption, the likelihood is the product of the individual probabilities:\n$$\nL(c; \\{k_i\\}) = \\prod_{i=1}^{n} P(K=k_i | c) = \\prod_{i=1}^{n} \\frac{\\exp(-c) c^{k_i}}{k_i!}\n$$\nMaximizing the likelihood function is equivalent to maximizing its natural logarithm, the log-likelihood function $\\ell(c) = \\ln(L(c))$, which is often algebraically simpler.\n$$\n\\ell(c) = \\ln \\left( \\prod_{i=1}^{n} \\frac{\\exp(-c) c^{k_i}}{k_i!} \\right) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\exp(-c) c^{k_i}}{k_i!} \\right)\n$$\nUsing the properties of logarithms, we can expand this expression:\n$$\n\\ell(c) = \\sum_{i=1}^{n} (\\ln(\\exp(-c)) + \\ln(c^{k_i}) - \\ln(k_i!))\n$$\n$$\n\\ell(c) = \\sum_{i=1}^{n} (-c + k_i \\ln(c) - \\ln(k_i!))\n$$\n$$\n\\ell(c) = -nc + \\ln(c) \\sum_{i=1}^{n} k_i - \\sum_{i=1}^{n} \\ln(k_i!)\n$$\nTo find the value of $c$ that maximizes this function, we take the first derivative of $\\ell(c)$ with respect to $c$ and set it to zero.\n$$\n\\frac{d\\ell}{dc} = \\frac{d}{dc} \\left( -nc + \\ln(c) \\sum_{i=1}^{n} k_i - \\sum_{i=1}^{n} \\ln(k_i!) \\right) = -n + \\frac{1}{c} \\sum_{i=1}^{n} k_i\n$$\nSetting the derivative to zero gives the equation for the MLE, denoted $\\hat{c}_{\\text{MLE}}$:\n$$\n-n + \\frac{1}{\\hat{c}_{\\text{MLE}}} \\sum_{i=1}^{n} k_i = 0\n$$\nSolving for $\\hat{c}_{\\text{MLE}}$ yields:\n$$\n\\hat{c}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} k_i\n$$\nThis expression is the sample mean of the observed degrees. To confirm this is a maximum, we check the second derivative:\n$$\n\\frac{d^2\\ell}{dc^2} = -\\frac{1}{c^2} \\sum_{i=1}^{n} k_i\n$$\nSince degrees $k_i$ are non-negative and $c > 0$, the second derivative is negative (assuming not all degrees are zero), which confirms that $\\hat{c}_{\\text{MLE}}$ is indeed a maximum likelihood estimator.\n\n**Part 2: Consistency Assessment for the Erdős–Rényi Model**\n\nWe now assess if this estimator is consistent when the data actually arises from an Erdős–Rényi $G(n,p)$ random graph, where the connection probability is $p = c/n$. Consistency means that the estimator converges in probability to the true parameter $c$ as the number of vertices $n$ approaches infinity. We must show:\n$$\n\\hat{c}_n = \\frac{1}{n} \\sum_{i=1}^{n} K_i \\xrightarrow{p} c \\quad \\text{as } n \\to \\infty\n$$\nwhere $\\{K_i\\}_{i=1}^n$ are the random variables for the degrees in the $G(n,p)$ graph. The crucial constraint is that we cannot assume the degrees $K_i$ are independent.\n\nWe use the fundamental graph-theoretic identity that the sum of the degrees of all vertices is equal to twice the number of edges in the graph. Let $M$ be the random variable representing the number of edges in $G(n,p)$. Then:\n$$\n\\sum_{i=1}^{n} K_i = 2M\n$$\nSubstituting this into the expression for our estimator $\\hat{c}_n$:\n$$\n\\hat{c}_n = \\frac{1}{n} (2M) = \\frac{2M}{n}\n$$\nIn a $G(n,p)$ graph, there are $\\binom{n}{2}$ potential edges. The model specifies that each of these edges exists independently with probability $p$. Therefore, the total number of edges $M$ is the sum of $\\binom{n}{2}$ independent and identically distributed Bernoulli($p$) random variables. This means $M$ follows a binomial distribution:\n$$\nM \\sim \\text{Binomial}\\left(\\binom{n}{2}, p\\right)\n$$\nAn estimator is consistent if it is asymptotically unbiased and its variance converges to zero.\nFirst, we compute the expected value of our estimator $\\hat{c}_n$:\n$$\nE[\\hat{c}_n] = E\\left[\\frac{2M}{n}\\right] = \\frac{2}{n} E[M]\n$$\nThe expectation of a binomial random variable is the number of trials times the success probability:\n$$\nE[M] = \\binom{n}{2} p = \\frac{n(n-1)}{2} p\n$$\nSubstituting this into the expression for $E[\\hat{c}_n]$ and using $p = c/n$:\n$$\nE[\\hat{c}_n] = \\frac{2}{n} \\left(\\frac{n(n-1)}{2} \\frac{c}{n}\\right) = \\frac{c(n-1)}{n} = c - \\frac{c}{n}\n$$\nThe bias of the estimator is $E[\\hat{c}_n] - c = -c/n$. As $n \\to \\infty$, the bias approaches $0$. The estimator is asymptotically unbiased.\n\nNext, we compute the variance of $\\hat{c}_n$:\n$$\n\\text{Var}(\\hat{c}_n) = \\text{Var}\\left(\\frac{2M}{n}\\right) = \\frac{4}{n^2} \\text{Var}(M)\n$$\nThe variance of a binomial random variable is the number of trials times the success probability times the failure probability:\n$$\n\\text{Var}(M) = \\binom{n}{2} p(1-p) = \\frac{n(n-1)}{2} p(1-p)\n$$\nSubstituting this into the expression for $\\text{Var}(\\hat{c}_n)$ and using $p=c/n$:\n$$\n\\text{Var}(\\hat{c}_n) = \\frac{4}{n^2} \\left(\\frac{n(n-1)}{2} \\frac{c}{n} \\left(1-\\frac{c}{n}\\right)\\right) = \\frac{2c(n-1)}{n^2} \\left(1-\\frac{c}{n}\\right)\n$$\nWe now take the limit as $n \\to \\infty$:\n$$\n\\lim_{n \\to \\infty} \\text{Var}(\\hat{c}_n) = \\lim_{n \\to \\infty} \\frac{2c(n-1)}{n^2} \\left(1-\\frac{c}{n}\\right) = \\lim_{n \\to \\infty} \\left( \\frac{2c}{n} - \\frac{2c}{n^2} \\right) \\left(1-\\frac{c}{n}\\right)\n$$\nSince $\\lim_{n \\to \\infty} \\frac{1}{n} = 0$ and $\\lim_{n \\to \\infty} \\frac{1}{n^2} = 0$, the first term goes to $0$, and the second term goes to $1$. Thus:\n$$\n\\lim_{n \\to \\infty} \\text{Var}(\\hat{c}_n) = 0 \\times 1 = 0\n$$\nSince the estimator's bias and variance both converge to zero as $n \\to \\infty$, the estimator is consistent by the mean-squared error criterion for consistency (which implies convergence in probability via Chebyshev's inequality). The estimator derived under the simplified i.i.d. Poisson model is indeed a consistent estimator for the parameter $c$ in the more realistic Erdős–Rényi graph model, and we have established this without assuming independence of the degrees. The use of a standard law of large numbers is implicit in the convergence of the binomial variable $M$ (properly scaled) to its expectation, which was leveraged in our analysis of $E[\\hat{c}_n]$ and $\\text{Var}(\\hat{c}_n)$.\n\nThe explicit expression for the estimator is the final answer.",
            "answer": "$$\n\\boxed{\\frac{1}{n} \\sum_{i=1}^{n} k_i}\n$$"
        },
        {
            "introduction": "To achieve a deeper understanding of random graphs, we must analyze not just the expected properties but also their fluctuations. This advanced problem challenges you to go beyond the mean and compute the variance of $X_k$, the number of vertices with degree $k$, using the second-moment method. This calculation reveals a crucial insight: while the expected value of $X_k$ aligns with the Poisson approximation, its variance does not, highlighting the subtle but important correlations present in the graph's global structure. ",
            "id": "4297639",
            "problem": "Consider the independent-edge random graph model $G(n,p)$ on $n$ labeled vertices, where each of the $\\binom{n}{2}$ possible edges is present independently with probability $p$. Fix an integer $k \\ge 0$. Let $X_k$ denote the number of vertices of degree exactly $k$ in $G(n,p)$. Using only the core definitions of $G(n,p)$ and independence of edges, proceed as follows:\n\n- Express $X_k$ as a sum of vertex-level indicator random variables and derive an exact, closed-form expression for $\\mathrm{Var}(X_k)$ in terms of $n$, $p$, and binomial point probabilities that arise from the degree distribution of a single vertex and joint degree events for vertex pairs. Your derivation must be from first principles and should not invoke any pre-packaged variance formulas beyond the definition $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$ and bilinearity of covariance.\n\n- Specialize to the sparse connectivity regime $p = c/(n-1)$ with fixed $c0$ and fixed $k$. Using only classical large-$n$ limits for binomial distributions, obtain the leading-order asymptotic form of $\\mathrm{Var}(X_k)$ as $n \\to \\infty$, expressed as $n$ times an explicit function of $c$ and $k$.\n\n- Compare this to the Poisson-mean benchmark $\\lambda_n = n \\exp(-c)\\, c^{k}/k!$ suggested by the Poisson approximation for the degree distribution of a single vertex. Compute the leading-order ratio $\\mathrm{Var}(X_k)/\\lambda_n$ as $n \\to \\infty$, simplified to a closed-form expression in $c$ and $k$.\n\nProvide the leading-order ratio $\\mathrm{Var}(X_k)/\\lambda_n$ as your final answer in a single closed-form analytic expression in terms of $c$ and $k$. No rounding is required.",
            "solution": "### Part 1: Exact Expression for $\\mathrm{Var}(X_k)$\n\nLet $V = \\{v_1, v_2, \\dots, v_n\\}$ be the set of vertices. For each vertex $v_i \\in V$, let $I_i$ be the indicator random variable such that $I_i = 1$ if the degree of $v_i$, denoted $d(v_i)$, is equal to $k$, and $I_i = 0$ otherwise. The total number of vertices with degree $k$ is the sum of these indicators:\n$$X_k = \\sum_{i=1}^{n} I_i$$\nWe compute the variance of $X_k$ using the property $\\mathrm{Var}(X_k) = \\mathbb{E}[X_k^2] - (\\mathbb{E}[X_k])^2$.\nThe expectation of $X_k$ is, by linearity:\n$$\\mathbb{E}[X_k] = \\mathbb{E}\\left[\\sum_{i=1}^{n} I_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[I_i] = n \\mathbb{E}[I_1]$$\nThe expectation of an indicator is the probability of the event it indicates. The degree of any vertex $v_i$ follows a binomial distribution $B(n-1, p)$, as there are $n-1$ other vertices it could connect to, each with probability $p$. Let $P_k$ be this probability:\n$$P_k = \\mathbb{P}(d(v_i) = k) = \\binom{n-1}{k} p^k (1-p)^{n-1-k}$$\nThus, $\\mathbb{E}[I_i] = P_k$ and $\\mathbb{E}[X_k] = n P_k$.\n\nNow, we compute $\\mathbb{E}[X_k^2]$:\n$$\\mathbb{E}[X_k^2] = \\mathbb{E}\\left[ \\left(\\sum_{i=1}^{n} I_i\\right)^2 \\right] = \\mathbb{E}\\left[ \\sum_{i,j} I_i I_j \\right] = \\sum_i \\mathbb{E}[I_i^2] + \\sum_{i \\neq j} \\mathbb{E}[I_i I_j]$$\nSince $I_i$ is an indicator, $I_i^2 = I_i$, so $\\mathbb{E}[I_i^2] = \\mathbb{E}[I_i] = P_k$. There are $n$ such terms.\nFor the cross-terms, $\\mathbb{E}[I_i I_j] = \\mathbb{P}(I_i=1 \\text{ and } I_j=1) = \\mathbb{P}(d(v_i)=k, d(v_j)=k)$. Let this joint probability be $P_{kk}$. By symmetry, this is the same for any distinct pair $(i,j)$. There are $n(n-1)$ such pairs.\nSo, $\\mathbb{E}[X_k^2] = n P_k + n(n-1) P_{kk}$.\nThe variance is then:\n$$\\mathrm{Var}(X_k) = (n P_k + n(n-1) P_{kk}) - (n P_k)^2 = n P_k - n P_k^2 + n(n-1) (P_{kk} - P_k^2)$$\n$$\\mathrm{Var}(X_k) = n P_k(1-P_k) + n(n-1)(P_{kk} - P_k^2)$$\nTo find $P_{kk}$, we condition on the state of the edge between $v_i$ and $v_j$. Let $E_{ij}$ be the event that this edge exists.\n\\begin{align*} P_{kk} = \\mathbb{P}(d(v_i)=k, d(v_j)=k | E_{ij})\\mathbb{P}(E_{ij}) + \\mathbb{P}(d(v_i)=k, d(v_j)=k | E_{ij}^c)\\mathbb{P}(E_{ij}^c) \\\\ = p \\cdot \\mathbb{P}(d(v_i)=k, d(v_j)=k | E_{ij}) + (1-p) \\cdot \\mathbb{P}(d(v_i)=k, d(v_j)=k | E_{ij}^c)\\end{align*}\nIf $E_{ij}$ occurs, both $v_i$ and $v_j$ must form $k-1$ additional edges with the remaining $n-2$ vertices. These events are independent. The probability for each is $b(n-2, k-1, p) = \\binom{n-2}{k-1}p^{k-1}(1-p)^{n-2-(k-1)}$.\nIf $E_{ij}^c$ occurs, both $v_i$ and $v_j$ must form $k$ edges with the remaining $n-2$ vertices. The probability for each is $b(n-2, k, p) = \\binom{n-2}{k}p^k(1-p)^{n-2-k}$.\nSo, the joint probability is:\n$$P_{kk} = p\\left[\\binom{n-2}{k-1}p^{k-1}(1-p)^{n-k-1}\\right]^2 + (1-p)\\left[\\binom{n-2}{k}p^k(1-p)^{n-k-2}\\right]^2$$\nThis provides the exact closed-form expression for $\\mathrm{Var}(X_k)$.\n\n### Part 2: Asymptotic Form of $\\mathrm{Var}(X_k)$\n\nWe specialize to the regime $p = c/(n-1)$ for fixed $c>0, k$ and $n \\to \\infty$.\nFirst, we find the limit of $P_k$. This is the classic Poisson limit of a binomial distribution. The mean is $(n-1)p = c$.\n$$P_k = \\binom{n-1}{k} p^k (1-p)^{n-1-k} \\to \\frac{c^k \\exp(-c)}{k!} \\quad \\text{as } n \\to \\infty$$\nLet's denote this limit by $\\pi_k(c)$. So, $P_k = \\pi_k(c) + O(1/n)$.\nThe first term of the variance is $n P_k(1-P_k) \\approx n \\pi_k(c)(1-\\pi_k(c))$.\n\nFor the covariance term, $P_{kk} - P_k^2$, we can use the identity $P_k = p \\cdot b(n-2, k-1, p) + (1-p) \\cdot b(n-2, k, p)$.\nSquaring this gives $P_k^2 = p^2 b(n-2, k-1, p)^2 + (1-p)^2 b(n-2, k, p)^2 + 2p(1-p)b(n-2, k-1, p)b(n-2, k, p)$.\nSubtracting this from the expression for $P_{kk}$:\n\\begin{align*} P_{kk} - P_k^2 = (p-p^2)b(n-2, k-1,p)^2 + ((1-p)-(1-p)^2)b(n-2, k,p)^2 - 2p(1-p)b(n-2, k-1,p)b(n-2, k,p) \\\\ = p(1-p)b(n-2, k-1,p)^2 + p(1-p)b(n-2, k,p)^2 - 2p(1-p)b(n-2, k-1,p)b(n-2, k,p) \\\\ = p(1-p) \\left[ b(n-2, k-1, p) - b(n-2, k, p) \\right]^2 \\end{align*}\nThe mean of the binomials $b(n-2, \\cdot, p)$ is $(n-2)p = c(n-2)/(n-1) \\to c$.\nThus, $b(n-2, j, p) \\to \\pi_j(c)$ as $n \\to \\infty$. Also, $p(1-p) \\approx p =c/(n-1) \\approx c/n$.\nThe covariance term is asymptotically:\n$$P_{kk} - P_k^2 \\approx \\frac{c}{n-1} \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2$$\nThe full variance is $\\mathrm{Var}(X_k) = nP_k(1-P_k) + n(n-1)(P_{kk}-P_k^2)$.\nThe leading-order term is:\n\\begin{align*} \\mathrm{Var}(X_k) \\approx n \\pi_k(c)(1-\\pi_k(c)) + n(n-1) \\frac{c}{n-1} \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2 \\\\ = n \\pi_k(c)(1-\\pi_k(c)) + nc \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2 \\end{align*}\nThis is the leading-order asymptotic form, which is $n$ times a function of $c$ and $k$.\n\n### Part 3: Ratio $\\mathrm{Var}(X_k)/\\lambda_n$\n\nThe benchmark is $\\lambda_n = n \\exp(-c) c^k/k! = n \\pi_k(c)$. We compute the ratio of the asymptotic variance to this benchmark:\n$$ \\frac{\\mathrm{Var}(X_k)}{\\lambda_n} \\approx \\frac{n \\pi_k(c)(1-\\pi_k(c)) + nc \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2}{n \\pi_k(c)} $$\nThe $n$ factors cancel.\n$$ \\lim_{n\\to\\infty} \\frac{\\mathrm{Var}(X_k)}{\\lambda_n} = (1-\\pi_k(c)) + \\frac{c}{\\pi_k(c)} \\left[ \\pi_{k-1}(c) - \\pi_k(c) \\right]^2 $$\nWe use the property of Poisson probabilities: $\\pi_k(c) = \\frac{c}{k} \\pi_{k-1}(c)$, which implies $\\pi_{k-1}(c) = \\frac{k}{c} \\pi_k(c)$ for $k \\ge 1$.\nThe case $k=0$ must be handled separately. For $k=0$, $\\pi_{-1}(c)=0$. The formula still holds.\nSubstitute this into the expression:\n\\begin{align*} \\text{Ratio} = 1 - \\pi_k(c) + \\frac{c}{\\pi_k(c)} \\left[ \\frac{k}{c}\\pi_k(c) - \\pi_k(c) \\right]^2 \\\\ = 1 - \\pi_k(c) + \\frac{c}{\\pi_k(c)} \\left( \\pi_k(c) \\left( \\frac{k}{c} - 1 \\right) \\right)^2 \\\\ = 1 - \\pi_k(c) + \\frac{c}{\\pi_k(c)} \\pi_k(c)^2 \\left( \\frac{k-c}{c} \\right)^2 \\\\ = 1 - \\pi_k(c) + c \\pi_k(c) \\frac{(k-c)^2}{c^2} \\\\ = 1 - \\pi_k(c) + \\frac{(k-c)^2}{c} \\pi_k(c) \\end{align*}\nWriting this out explicitly in terms of $c$ and $k$:\n$$ \\text{Ratio} = 1 - \\frac{c^k \\exp(-c)}{k!} + \\frac{(k-c)^2}{c} \\frac{c^k \\exp(-c)}{k!} $$\n$$ \\text{Ratio} = 1 - \\frac{c^k \\exp(-c)}{k!} + (k-c)^2 \\frac{c^{k-1} \\exp(-c)}{k!} $$\nThis is the final closed-form expression for the leading-order ratio.",
            "answer": "$$\\boxed{1 - \\frac{c^k \\exp(-c)}{k!} + (k-c)^2 \\frac{c^{k-1} \\exp(-c)}{k!}}$$"
        }
    ]
}