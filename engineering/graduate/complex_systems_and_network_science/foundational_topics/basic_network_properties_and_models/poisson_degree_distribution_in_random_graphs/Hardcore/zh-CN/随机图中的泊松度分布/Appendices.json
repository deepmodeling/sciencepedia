{
    "hands_on_practices": [
        {
            "introduction": "在网络科学中，一个核心任务是从观测数据中推断模型参数。在稀疏 Erdős–Rényi 模型中，代表平均度的参数 $c$ 是最基本的。本练习将指导你在一个简化的泊松假设下，推导出 $c$ 的最大似然估计量。更重要的是，它将挑战你在真实的、更复杂的 ER 模型中证明其一致性，从而突显统计估计量的稳健性。",
            "id": "4297652",
            "problem": "考虑一个在 $n$ 个顶点上观测到的无向、简单、有标签的图序列，其观测度为 $\\{k_i\\}_{i=1}^n$。假设为了对网络稀疏性进行参数推断，将度建模为来自未知均值为 $c$ 的泊松分布的独立同分布实现。从泊松概率质量函数的定义和最大似然原理出发，推导 $c$ 的最大似然估计量（Maximum Likelihood Estimator, MLE）关于 $\\{k_i\\}_{i=1}^n$ 的显式表达式。然后，在数据实际来自 $p = c/n$ 的 Erdős–Rényi (ER) $G(n,p)$ 随机图的情况下，评估该估计量的大样本 $n$ 一致性，仅使用 $G(n,p)$ 的基本性质，如边的独立性、度之和等于边数的两倍这一恒等式，以及标准的大数定律。您的评估应确定当 $n \\to \\infty$ 时，该估计量是否依概率收敛于真实的 $c$，并在不假设度独立性的情况下证明此收敛。将估计量的表达式作为最终答案报告。",
            "solution": "用户需要一个问题的解答。我现在开始验证阶段。\n\n### 第一步：提取已知条件\n- 一个在 $n$ 个顶点上的无向、简单、有标签的图序列。\n- 观测度 $\\{k_i\\}_{i=1}^n$。\n- 度被建模为来自未知均值为 $c$ 的泊松分布的独立同分布（i.i.d.）实现。\n- 任务是推导 $c$ 关于 $\\{k_i\\}_{i=1}^n$ 的最大似然估计量（MLE）。\n- 第二个任务是在数据实际来自 $p = c/n$ 的 Erdős–Rényi (ER) $G(n,p)$ 随机图的情况下，评估该估计量的大样本 $n$ 一致性。\n- 对于一致性评估，约束条件是只能使用：\n    1. $G(n,p)$ 的基本性质，如边的独立性。\n    2. 度之和等于边数的两倍这一恒等式。\n    3. 标准的大数定律。\n- 一致性评估必须在不假设度独立性的情况下进行证明。\n- 最终报告的答案应该是估计量的表达式。\n\n### 第二步：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n\n- **科学依据**：该问题在统计推断和网络科学领域有坚实的理论基础。最大似然估计是参数估计的基石。对于大 $n$ 和小 $p$ 且 $np$ 为常数的情况下，泊松分布是 Erdős–Rényi 随机图公认的极限度分布。问题的结构——在一个简单模型（i.i.d. 泊松度）下推导估计量，然后在一个更现实的模型（ER 图，其中度不严格独立）下测试其一致性——是统计建模中一个标准而严谨的程序。所有前提在事实上和科学上都是合理的。\n\n- **良态的（Well-Posed）**：该问题是良态的。它要求在明确定义的条件下进行特定的推导（MLE）和证明特定的统计性质（一致性）。所提供的约束条件指导了证明方法，确保了通往解的唯一路径。泊松分布参数的最大似然估计的存在性和唯一性是标准结论。\n\n- **客观性**：该问题以精确、客观、无歧义的数学语言陈述。没有主观或基于意见的成分。\n\n该问题不存在任何列出的缺陷：\n1.  **科学或事实上的不健全**：有限 ER 图中的度不是独立的（例如，它们的和必须是偶数），这是事实。该问题正确地认识到这一点，通过在一致性证明中禁止假设度的独立性。这种在 MLE 推导的建模假设（i.i.d.）与一致性检验的真实数据生成过程（ER 图）之间的区别，是该问题一个复杂而有效的设计，并非缺陷。\n2.  **无法形式化或不相关**：该问题完全可以在概率论和图论的框架内形式化。\n3.  **不完整或矛盾的设置**：设置是完整的，没有矛盾。使用简化模型进行推导，然后用更现实的模型进行验证是一种常见且强大的技术。\n4.  **不现实或不可行**：该情景是网络科学中一个标准的理论设置。\n5.  **良态性差或结构不良**：问题结构清晰，分为两个不同部分，并导向一个唯一的、有意义的答案。\n6.  **伪深刻、琐碎或同义反复**：该问题并不琐碎。虽然泊松分布的 MLE 是标准的教科书结果，但在不假设度独立性的情况下，为 ER 图模型证明其一致性需要仔细应用图论性质和极限定理，这考验了对基础模型的更深层次理解。\n7.  **超出科学可验证性**：需要证明的命题完全在数学证明的范畴之内。\n\n### 第三步：结论与行动\n该问题是**有效的**。将提供完整的解答。\n\n**第一部分：最大似然估计量（MLE）的推导**\n\n问题首先将观测到的度 $\\{k_i\\}_{i=1}^n$ 建模为来自未知均值参数为 $c$ 的泊松分布的独立同分布（i.i.d.）随机变量。对于来自泊松($c$)分布的单个观测值 $k_i$，其概率质量函数（PMF）为：\n$$\nP(K=k_i | c) = \\frac{\\exp(-c) c^{k_i}}{k_i!}\n$$\n似然函数 $L(c; \\{k_i\\})$ 是观测到整个数据集 $\\{k_i\\}_{i=1}^n$ 的联合概率。由于 i.i.d. 假设，似然函数是各个概率的乘积：\n$$\nL(c; \\{k_i\\}) = \\prod_{i=1}^{n} P(K=k_i | c) = \\prod_{i=1}^{n} \\frac{\\exp(-c) c^{k_i}}{k_i!}\n$$\n最大化似然函数等价于最大化其自然对数，即对数似然函数 $\\ell(c) = \\ln(L(c))$，后者在代数上通常更简单。\n$$\n\\ell(c) = \\ln \\left( \\prod_{i=1}^{n} \\frac{\\exp(-c) c^{k_i}}{k_i!} \\right) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\exp(-c) c^{k_i}}{k_i!} \\right)\n$$\n利用对数的性质，我们可以展开这个表达式：\n$$\n\\ell(c) = \\sum_{i=1}^{n} (\\ln(\\exp(-c)) + \\ln(c^{k_i}) - \\ln(k_i!))\n$$\n$$\n\\ell(c) = \\sum_{i=1}^{n} (-c + k_i \\ln(c) - \\ln(k_i!))\n$$\n$$\n\\ell(c) = -nc + \\ln(c) \\sum_{i=1}^{n} k_i - \\sum_{i=1}^{n} \\ln(k_i!)\n$$\n为了找到使该函数最大化的 $c$ 值，我们对 $\\ell(c)$ 关于 $c$ 求一阶导数，并将其设为零。\n$$\n\\frac{d\\ell}{dc} = \\frac{d}{dc} \\left( -nc + \\ln(c) \\sum_{i=1}^{n} k_i - \\sum_{i=1}^{n} \\ln(k_i!) \\right) = -n + \\frac{1}{c} \\sum_{i=1}^{n} k_i\n$$\n将导数设为零，得到 MLE（记为 $\\hat{c}_{\\text{MLE}}$）的方程：\n$$\n-n + \\frac{1}{\\hat{c}_{\\text{MLE}}} \\sum_{i=1}^{n} k_i = 0\n$$\n解出 $\\hat{c}_{\\text{MLE}}$：\n$$\n\\hat{c}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^{n} k_i\n$$\n这个表达式是观测度的样本均值。为了确认这是一个最大值，我们检查二阶导数：\n$$\n\\frac{d^2\\ell}{dc^2} = -\\frac{1}{c^2} \\sum_{i=1}^{n} k_i\n$$\n由于度 $k_i$ 是非负的且 $c  0$，二阶导数为负（假设并非所有度都为零），这证实了 $\\hat{c}_{\\text{MLE}}$ 确实是一个最大似然估计量。\n\n**第二部分：Erdős–Rényi 模型的一致性评估**\n\n现在我们评估当数据实际来自 Erdős–Rényi $G(n,p)$ 随机图（其中连接概率为 $p = c/n$）时，该估计量是否是一致的。一致性意味着当顶点数 $n$ 趋于无穷大时，估计量依概率收敛于真实参数 $c$。我们必须证明：\n$$\n\\hat{c}_n = \\frac{1}{n} \\sum_{i=1}^{n} K_i \\xrightarrow{p} c \\quad \\text{as } n \\to \\infty\n$$\n其中 $\\{K_i\\}_{i=1}^n$ 是 $G(n,p)$ 图中度的随机变量。关键的约束条件是我们不能假设度 $K_i$ 是独立的。\n\n我们利用基本的图论恒等式：所有顶点的度之和等于图中边数的两倍。令 $M$ 为表示 $G(n,p)$ 中边数的随机变量。那么：\n$$\n\\sum_{i=1}^{n} K_i = 2M\n$$\n将此代入我们的估计量 $\\hat{c}_n$ 的表达式中：\n$$\n\\hat{c}_n = \\frac{1}{n} (2M) = \\frac{2M}{n}\n$$\n在 $G(n,p)$ 图中，有 $\\binom{n}{2}$ 条可能的边。模型规定每条边以概率 $p$ 独立存在。因此，总边数 $M$ 是 $\\binom{n}{2}$ 个独立同分布的伯努利($p$)随机变量之和。这意味着 $M$ 服从二项分布：\n$$\nM \\sim \\text{Binomial}\\left(\\binom{n}{2}, p\\right)\n$$\n如果一个估计量是渐进无偏的且其方差收敛于零，则它是一致的。\n首先，我们计算估计量 $\\hat{c}_n$ 的期望值：\n$$\nE[\\hat{c}_n] = E\\left[\\frac{2M}{n}\\right] = \\frac{2}{n} E[M]\n$$\n二项随机变量的期望是试验次数乘以成功概率：\n$$\nE[M] = \\binom{n}{2} p = \\frac{n(n-1)}{2} p\n$$\n将此代入 $E[\\hat{c}_n]$ 的表达式并使用 $p = c/n$：\n$$\nE[\\hat{c}_n] = \\frac{2}{n} \\left(\\frac{n(n-1)}{2} \\frac{c}{n}\\right) = \\frac{c(n-1)}{n} = c - \\frac{c}{n}\n$$\n估计量的偏差为 $E[\\hat{c}_n] - c = -c/n$。当 $n \\to \\infty$ 时，偏差趋于 $0$。该估计量是渐进无偏的。\n\n接下来，我们计算 $\\hat{c}_n$ 的方差：\n$$\n\\text{Var}(\\hat{c}_n) = \\text{Var}\\left(\\frac{2M}{n}\\right) = \\frac{4}{n^2} \\text{Var}(M)\n$$\n二项随机变量的方差是试验次数乘以成功概率再乘以失败概率：\n$$\n\\text{Var}(M) = \\binom{n}{2} p(1-p) = \\frac{n(n-1)}{2} p(1-p)\n$$\n将此代入 $\\text{Var}(\\hat{c}_n)$ 的表达式并使用 $p=c/n$：\n$$\n\\text{Var}(\\hat{c}_n) = \\frac{4}{n^2} \\left(\\frac{n(n-1)}{2} \\frac{c}{n} \\left(1-\\frac{c}{n}\\right)\\right) = \\frac{2c(n-1)}{n^2} \\left(1-\\frac{c}{n}\\right)\n$$\n现在我们取 $n \\to \\infty$ 的极限：\n$$\n\\lim_{n \\to \\infty} \\text{Var}(\\hat{c}_n) = \\lim_{n \\to \\infty} \\frac{2c(n-1)}{n^2} \\left(1-\\frac{c}{n}\\right) = \\lim_{n \\to \\infty} \\left( \\frac{2c}{n} - \\frac{2c}{n^2} \\right) \\left(1-\\frac{c}{n}\\right)\n$$\n由于 $\\lim_{n \\to \\infty} \\frac{1}{n} = 0$ 且 $\\lim_{n \\to \\infty} \\frac{1}{n^2} = 0$，第一项趋于 $0$，第二项趋于 $1$。因此：\n$$\n\\lim_{n \\to \\infty} \\text{Var}(\\hat{c}_n) = 0 \\times 1 = 0\n$$\n由于估计量的偏差和方差在 $n \\to \\infty$ 时都收敛于零，根据均方误差一致性准则（通过切比雪夫不等式意味着依概率收敛），该估计量是一致的。在简化的 i.i.d. 泊松模型下推导出的估计量，对于更现实的 Erdős–Rényi 图模型中的参数 $c$ 确实是一个一致的估计量，并且我们在没有假设度独立性的情况下建立了这一点。标准大数定律的使用隐含在二项变量 $M$（经过适当缩放后）收敛于其期望值的过程中，我们在对 $E[\\hat{c}_n]$ 和 $\\text{Var}(\\hat{c}_n)$ 的分析中利用了这一点。\n\n估计量的显式表达式是最终答案。",
            "answer": "$$\n\\boxed{\\frac{1}{n} \\sum_{i=1}^{n} k_i}\n$$"
        },
        {
            "introduction": "度的泊松近似依赖于节点间的连接“基本”独立的思想，但它们的独立性究竟如何？本练习将通过关注一个节点被孤立（度为 0）这一简单事件，来锻炼你的概率推理能力。通过计算两个顶点同时被孤立的精确联合概率，并将其与独立情况下的概率进行比较，你将能够量化图中存在的微弱相关性，并理解渐近独立的概念。",
            "id": "4297656",
            "problem": "考虑一个有 $n$ 个标记顶点的 Erdős–Rényi (ER) 随机图 $G(n,p)$，其中每条可能的边都以概率 $p$ 独立存在。固定两个标记为 $i$ 和 $j$ 的不同顶点。如果一个顶点的度为 $0$，则称该顶点是孤立的。\n\n仅从 $G(n,p)$ 中各边相互独立以及孤立的定义出发，推导给定顶点被孤立的概率以及 $i$ 和 $j$ 都被孤立的概率的表达式。然后，对于固定 $c0$ 的稀疏情况 $p=p_{n}=\\frac{c}{n}$，使用您的表达式讨论当 $n \\to \\infty$ 时，$i$ 和 $j$ 的孤立事件的近似独立性。\n\n为了量化与独立性的主阶偏差，定义\n$$\nL(c)\\;=\\;\\lim_{n\\to\\infty} n\\left[\\mathbb{P}\\big(\\text{$i$ 和 $j$ 在 }G(n,p_{n})\\text{ 中是孤立的}\\big)\\;-\\;\\mathbb{P}\\big(\\text{$i$ 在 }G(n,p_{n})\\text{ 中是孤立的}\\big)^{2}\\right].\n$$\n以 $c$ 的函数形式计算 $L(c)$ 的闭式解。您的最终答案必须是仅含 $c$ 的单个解析表达式。",
            "solution": "我们从 Erdős–Rényi (ER) 随机图 $G(n,p)$ 模型开始，其中 $\\binom{n}{2}$ 条可能的边中的每一条都以概率 $p$ 独立地被包含，因此以概率 $1-p$ 独立地被排除。如果一个顶点的所有邻接边都不存在，则该顶点是孤立的。\n\n首先，我们计算固定顶点 $i$ 被孤立的概率。有 $n-1$ 条可能与 $i$ 邻接的边。根据各边的独立性以及每条边都以概率 $1-p$ 不存在的事实，所有 $n-1$ 条邻接边都不存在的概率是\n$$\n\\mathbb{P}(\\text{$i$ 是孤立的}) \\;=\\; (1-p)^{n-1}.\n$$\n\n接下来，我们计算两个不同顶点 $i$ 和 $j$ 都被孤立的概率。要使 $i$ 孤立，所有与 $i$ 相邻的 $n-1$ 条边都必须不存在；同样，要使 $j$ 孤立，所有与 $j$ 相邻的 $n-1$ 条边都必须不存在。然而，连接 $i$ 和 $j$ 的边在两组中都被计数了，所以我们需要计算要使 $i$ 和 $j$ 都被孤立，必须不存在的边的数量。与 $i$ 或 $j$ 邻接的边集包括从 $i$ 到除 $j$ 以外的顶点的 $(n-2)$ 条边，从 $j$ 到除 $i$ 以外的顶点的 $(n-2)$ 条边，以及单条边 $\\{i,j\\}$。因此，总共有 $2(n-2)+1 = 2n-3$ 条不同的边必须不存在。根据独立性，\n$$\n\\mathbb{P}(\\text{$i$ 和 $j$ 都是孤立的}) \\;=\\; (1-p)^{2n-3}.\n$$\n\n为了讨论近似独立性，我们将联合概率与边际概率的乘积进行比较：\n$$\n\\frac{\\mathbb{P}(\\text{$i$ 和 $j$ 都是孤立的})}{\\mathbb{P}(\\text{$i$ 是孤立的})\\,\\mathbb{P}(\\text{$j$ 是孤立的})}\n\\;=\\;\n\\frac{(1-p)^{2n-3}}{(1-p)^{n-1}\\,(1-p)^{n-1}}\n\\;=\\;\n(1-p)^{-1}.\n$$\n对于固定 $c0$ 的 $p=p_{n}=\\frac{c}{n}$，我们有\n$$\n(1-p_{n})^{-1} \\;=\\; \\frac{1}{1-\\frac{c}{n}} \\;=\\; 1 \\;+\\; \\frac{c}{n} \\;+\\; \\mathcal{O}\\!\\left(\\frac{1}{n^{2}}\\right),\n$$\n当 $n\\to\\infty$ 时，该式趋向于 $1$。这表明在稀疏情况下，$i$ 和 $j$ 的孤立事件是渐近独立的，尽管对于有限的 $n$ 它们并非完全独立。\n\n为了量化与独立性的主阶偏差，我们考虑孤立指示变量的协方差，并用 $n$ 对其进行缩放。定义指示变量 $X_{i}=\\mathbf{1}\\{\\text{$i$ 是孤立的}\\}$ 和 $X_{j}=\\mathbf{1}\\{\\text{$j$ 是孤立的}\\}$。那么\n$$\n\\operatorname{Cov}(X_{i},X_{j})\n\\;=\\;\n\\mathbb{P}(\\text{$i$ 和 $j$ 都是孤立的}) - \\mathbb{P}(\\text{$i$ 是孤立的})^{2}\n\\;=\\;\n(1-p)^{2n-3} - (1-p)^{2n-2}\n\\;=\\;\np\\,(1-p)^{2n-3}.\n$$\n特别地，与独立性的主阶偏差恰好是 $p\\,(1-p)^{2n-3}$。因此，\n$$\nL(c)\n\\;=\\;\n\\lim_{n\\to\\infty} n\\left[(1-p_{n})^{2n-3} - (1-p_{n})^{2n-2}\\right]\n\\;=\\;\n\\lim_{n\\to\\infty} n\\,p_{n}\\,(1-p_{n})^{2n-3}.\n$$\n在 $p_{n}=\\frac{c}{n}$ 的条件下，我们有 $n\\,p_{n}=c$。剩下的就是评估极限 $(1-p_{n})^{2n-3}$：\n$$\n\\lim_{n\\to\\infty} (1-\\tfrac{c}{n})^{2n-3}\n\\;=\\;\n\\exp\\!\\left(\\lim_{n\\to\\infty} (2n-3)\\,\\ln\\!\\big(1-\\tfrac{c}{n}\\big)\\right).\n$$\n使用当 $x\\to 0$ 时的展开式 $\\ln(1-x) = -x + \\mathcal{O}(x^{2})$，我们得到\n$$\n(2n-3)\\,\\ln\\!\\big(1-\\tfrac{c}{n}\\big)\n\\;=\\;\n(2n-3)\\left(-\\tfrac{c}{n} + \\mathcal{O}\\!\\left(\\tfrac{1}{n^{2}}\\right)\\right)\n\\;=\\;\n-2c + \\mathcal{O}\\!\\left(\\tfrac{1}{n}\\right),\n$$\n这得出\n$$\n\\lim_{n\\to\\infty} (1-\\tfrac{c}{n})^{2n-3} \\;=\\; \\exp(-2c).\n$$\n综合这些结果，\n$$\nL(c) \\;=\\; \\left(\\lim_{n\\to\\infty} n\\,p_{n}\\right)\\left(\\lim_{n\\to\\infty} (1-p_{n})^{2n-3}\\right) \\;=\\; c\\,\\exp(-2c).\n$$\n这个闭式表达式表明，孤立事件之间的协方差是正的，但以 $\\frac{1}{n}$ 的速率趋于零，其主系数为 $c\\,\\exp(-2c)$，从而量化了稀疏ER情景下的近似独立性。",
            "answer": "$$\\boxed{c\\,\\exp(-2c)}$$"
        },
        {
            "introduction": "理论收敛性证明显示，二项式度分布会趋近于泊松分布。但是，我们如何衡量这两个分布对于一个有限大小网络的“接近程度”呢？这个计算练习将带你从纯理论走向实践。通过计算并比较两种分布的香农熵——一个来自信息论的测度不确定性的量——你将对泊松近似的质量获得一个具体的、定量的感受。",
            "id": "4297671",
            "problem": "考虑一个 Erdős–Rényi 随机图 $G(n,p)$，其中一个均匀随机选择的顶点的度 $D_n$ 服从二项随机变量 $\\mathrm{Bin}(n-1,p)$。在 $p=c/n$（固定 $c0$）的稀疏情况下，当 $n\\to\\infty$ 时，度分布在律上收敛于泊松分布 $\\mathrm{Poi}(c)$。具有概率质量函数 $k\\mapsto \\mathbb{P}(D=k)$ 的离散随机变量 $D$ 的香农熵（以自然单位“奈特” (nats) 计量）定义为\n$$\nH(D) \\;=\\; -\\sum_{k\\in \\mathbb{Z}_{\\ge 0}} \\mathbb{P}(D=k)\\,\\log\\big(\\mathbb{P}(D=k)\\big),\n$$\n其中 $\\log$ 是以 $e$ 为底的自然对数。\n\n您的任务是编写一个完整的、可运行的程序，对于给定的参数对 $(c,n)$，数值计算：\n- $D\\sim \\mathrm{Poi}(c)$ 的香农熵 $H(D)$，以及\n- $D_n\\sim \\mathrm{Bin}(n-1,p)$（其中 $p=c/n$）的香农熵 $H(D_n)$，\n\n然后报告这两个熵之间的绝对差。计算必须是数值稳定且科学合理的。具体来说：\n- 对于泊松熵的计算，由于其支撑集是无限的，您必须在一个有限的上限 $K$ 处截断求和，该上限 $K$ 的选择应使被忽略的尾部质量满足 $\\mathbb{P}(DK)\\le \\varepsilon$，其中 $\\varepsilon=10^{-12}$。请根据熵的定义以及在此界限下被忽略尾部的贡献可以忽略不计来证明此截断的合理性。\n- 对于二项熵的计算，精确地在其有限支撑集 $\\{0,1,\\dots,n-1\\}$ 上求和。\n- 始终将 $0\\cdot \\log(0)$ 这一项解释为 $0$。\n- 使用以 $e$ 为底的自然对数；以奈特 (nats) 为单位报告熵值。\n- 将报告的每个浮点熵值和差值四舍五入到 $12$ 位小数。\n\n测试套件：\n计算并报告以下参数集的结果，这些参数集旨在探究一般行为、边界条件和有限尺寸效应：\n- 情况 1：$c=3$, $n=1000$。\n- 情况 2：$c=0$, $n=10$。\n- 情况 3：$c=0.1$, $n=1000$。\n- 情况 4：$c=10$, $n=2000$。\n- 情况 5：$c=3$, $n=10$。\n\n输出规格：\n您的程序应生成单行输出，其中包含一个逗号分隔的列表的列表，每个测试用例对应一个。每个内部列表按顺序包含三个四舍五入到 $12$ 位小数的浮点数：$[H(\\mathrm{Poi}(c)), H(\\mathrm{Bin}(n-1,c/n)), |H(\\mathrm{Poi}(c)) - H(\\mathrm{Bin}(n-1,c/n))|]$。外部列表应用方括号括起来。例如，输出格式必须是：\n$$\n\\text{[}\\,[h\\_1,h\\_2,\\Delta\\_1],\\,[h\\_1',h\\_2',\\Delta\\_2],\\,\\dots\\,\\text{]},\n$$\n其中每个 $h\\_i$、$h\\_i'$、$\\Delta\\_i$ 是以奈特 (nats) 为单位的四舍五入浮点数。\n\n约束条件：\n- 程序必须是自包含的，不得从文件或标准输入中读取任何输入，也不得访问任何网络资源。\n- 程序必须使用双精度浮点运算和泊松尾部质量的截断标准来实现上述计算。",
            "solution": "问题被评估为 **有效**。它在科学上基于概率论和随机图的研究，特别是已确立的泊松分布对二项分布的近似。问题提法清晰，提供了所有必要的定义、参数和数值约束（截断标准、四舍五入精度），以得出一个唯一的、可验证的解。语言客观、正式。测试用例（包括边界情况 $c=0$）定义明确，用于测试实现的鲁棒性。\n\n任务是计算并比较泊松分布 $D \\sim \\mathrm{Poi}(c)$ 和二项分布 $D_n \\sim \\mathrm{Bin}(n-1, p)$（其中 $p=c/n$）的香农熵。对于具有概率质量函数 (PMF) $p_k = \\mathbb{P}(\\text{variable}=k)$ 的离散随机变量，其香农熵 $H$（以奈特为单位）由下式给出：\n$$\nH \\;=\\; -\\sum_{k} p_k\\,\\log(p_k)\n$$\n其中，如果 $p_k=0$，则 $p_k \\log(p_k)$ 项取为 $0$。此计算将针对几对 $(c, n)$ 执行。\n\n**1. 泊松分布的熵，$H(\\mathrm{Poi}(c))$**\n\n均值为 $c$ 的泊松分布的 PMF 为：\n$$\n\\mathbb{P}(D=k) \\;=\\; p_k \\;=\\; \\frac{e^{-c} c^k}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\}\n$$\n该分布的支撑集是无限的，因此在数值计算中必须截断熵的求和。问题指定了一个截断标准：求和从 $k=0$ 到一个上限 $K$，使得被忽略的尾部概率可以忽略不计，具体为 $\\mathbb{P}(DK) \\le \\varepsilon = 10^{-12}$。因此，熵近似为：\n$$\nH(\\mathrm{Poi}(c)) \\;\\approx\\; -\\sum_{k=0}^{K} p_k\\,\\log(p_k)\n$$\n这种截断是合理的，因为尾部项对熵和的贡献极小。由该截断引入的误差大小可以被界定。设 $P_{\\text{tail}} = \\mathbb{P}(D  K) = \\sum_{k=K+1}^{\\infty} p_k$。误差为 $\\Delta H = |-\\sum_{k=K+1}^{\\infty} p_k \\log p_k|$。这可以被界定为 $\\Delta H \\le -P_{\\text{tail}} \\log P_{\\text{tail}} + P_{\\text{tail}} \\log(|S_{\\text{tail}}|)$，其中 $|S_{\\text{tail}}|$ 是尾部支撑集的大小。当 $P_{\\text{tail}} \\leq \\varepsilon = 10^{-12}$ 时，$-P_{\\text{tail}} \\log P_{\\text{tail}}$ 项约等于 $-10^{-12} \\log(10^{-12}) \\approx 2.76 \\times 10^{-11}$。第二项也是 $\\varepsilon$ 阶的，这不改变总误差远小于所要求的 $12$ 位小数的数值精度的结论。\n\n在实现上，截断点 $K$ 将使用泊松分布的逆生存函数（或百分点函数）来确定。然后计算 $k \\in \\{0, 1, \\dots, K\\}$ 的 PMF 值 $p_k$。熵和使用数值稳定的方法计算，通过识别 $\\lim_{x\\to 0^+} x \\log x = 0$ 来正确处理 $p_k=0$ 的情况。对于特殊情况 $c=0$，分布为 $\\mathrm{Poi}(0)$，其所有质量都集中在 $k=0$ 处（即 $\\mathbb{P}(D=0)=1$）。其熵恰好为 $H = -1\\log(1) = 0$。\n\n**2. 二项分布的熵，$H(\\mathrm{Bin}(n-1, c/n))$**\n\n具有 $N=n-1$ 次试验和成功概率 $p=c/n$ 的二项分布的 PMF 为：\n$$\n\\mathbb{P}(D_n=k) \\;=\\; p_k \\;=\\; \\binom{N}{k} p^k (1-p)^{N-k} \\quad \\text{for } k \\in \\{0, 1, \\dots, N\\}\n$$\n该分布的支撑集是有限的，因此熵的计算涉及对所有可能的 $k$ 值（从 $0$ 到 $N = n-1$）进行精确求和：\n$$\nH(D_n) \\;=\\; -\\sum_{k=0}^{n-1} p_k\\,\\log(p_k)\n$$\n与泊松计算一样，该和是数值计算的，确保了稳定性并正确处理了 $p_k=0$ 的项。对于特殊情况 $c=0$，我们有 $p=0$。分布 $\\mathrm{Bin}(n-1, 0)$ 也将其所有质量集中在 $k=0$ 处，因此其熵也为 $0$。\n\n**3. 实现计划**\n整体算法将遍历所提供的测试用例 $(c, n)$。对于每个用例：\n- 对于 $H(\\mathrm{Poi}(c))$，如果 $c=0$，则熵为 $0$。否则，使用 `scipy.stats.poisson.isf` 找到截断点 $K$。使用 `scipy.stats.poisson.pmf` 为 $k \\in \\{0, \\dots, K\\}$ 生成一个概率数组 $p_k$。熵通过 $-\\sum_k p_k \\log p_k$ 计算，其中使用 `scipy.special.xlogy(p_k, p_k)` 来确保 $0 \\log 0 = 0$。\n- 对于 $H(\\mathrm{Bin}(n-1, c/n))$，设置参数 $N=n-1$ 和 $p=c/n$。使用 `scipy.stats.binom.pmf` 为 $k \\in \\{0, \\dots, N\\}$ 生成一个概率数组 $p_k$。熵的计算方法类似。\n- 计算两个熵之间的绝对差 $|H(\\mathrm{Poi}(c)) - H(\\mathrm{Bin}(n-1, c/n))|$。\n- 将所有三个得到的浮点数四舍五入到 $12$ 位小数。\n- 收集结果并将其格式化为指定的字符串格式作为最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson, binom\nfrom scipy.special import xlogy\n\ndef solve():\n    \"\"\"\n    Computes and compares the Shannon entropy of Poisson and Binomial distributions\n    for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (3.0, 1000),  # Case 1\n        (0.0, 10),    # Case 2\n        (0.1, 1000),  # Case 3\n        (10.0, 2000), # Case 4\n        (3.0, 10),    # Case 5\n    ]\n\n    all_results = []\n    \n    # Epsilon for Poisson tail truncation\n    eps = 1e-12\n\n    for c, n in test_cases:\n        # 1. Calculate Shannon entropy for Poi(c)\n        h_poi = 0.0\n        # The isf function in scipy.stats.poisson requires mu  0.\n        # For c=0, Poi(0) has p(0)=1, so entropy is -1*log(1)=0.\n        if c  0:\n            # Find truncation limit K such that P(D  K) = eps\n            # poisson.isf(q, mu) returns k such that P(X  k) = q\n            # So, we sum up to and including K.\n            K = int(poisson.isf(eps, c))\n            k_vals_poi = np.arange(0, K + 1)\n            \n            # Calculate PMF for the range [0, K]\n            pk_poi = poisson.pmf(k_vals_poi, c)\n            \n            # Calculate entropy summing -p*log(p).\n            # xlogy(p, p) computes p*log(p) and handles p=0 correctly.\n            h_poi = -np.sum(xlogy(pk_poi, pk_poi))\n\n        # 2. Calculate Shannon entropy for Bin(n-1, c/n)\n        p = c / n\n        N = n - 1\n        h_bin = 0.0\n        \n        # For p=0 or p=1, the distribution is deterministic, so entropy is 0.\n        # The general formula also works thanks to xlogy.\n        if N = 0:\n            k_vals_bin = np.arange(0, N + 1)\n            \n            # Calculate PMF for the full support [0, N]\n            pk_bin = binom.pmf(k_vals_bin, N, p)\n            \n            # Calculate entropy\n            h_bin = -np.sum(xlogy(pk_bin, pk_bin))\n        \n        # 3. Calculate absolute difference\n        diff = abs(h_poi - h_bin)\n\n        # 4. Round results to 12 decimal places and store\n        # The problem asks for rounding, not just formatting.\n        h_poi_rounded = round(h_poi, 12)\n        h_bin_rounded = round(h_bin, 12)\n        diff_rounded = round(diff, 12)\n\n        all_results.append([h_poi_rounded, h_bin_rounded, diff_rounded])\n\n    # Final print statement in the exact required format.\n    # We construct the string manually to ensure it matches the specification,\n    # including the number of decimal places for each float.\n    inner_results_str = []\n    for res_list in all_results:\n        formatted_floats = [f\"{x:.12f}\" for x in res_list]\n        inner_str = f\"[{','.join(formatted_floats)}]\"\n        inner_results_str.append(inner_str)\n        \n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}