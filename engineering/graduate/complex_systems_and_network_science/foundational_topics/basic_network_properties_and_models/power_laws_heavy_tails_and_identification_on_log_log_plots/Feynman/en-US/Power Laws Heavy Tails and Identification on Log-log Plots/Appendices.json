{
    "hands_on_practices": [
        {
            "introduction": "Before we can confidently identify and model power laws in data, it is essential to grasp their fundamental mathematical properties. This exercise guides you through a first-principles derivation of the mean and median of the Pareto distribution. By comparing these two measures of central tendency, you will gain a concrete understanding of the profound right-skewness that is a hallmark of heavy-tailed systems.",
            "id": "4297927",
            "problem": "A researcher studying a large technological network finds that the complementary cumulative distribution function (CCDF) of node degrees is approximately linear on a log-log plot over several orders of magnitude beyond a cutoff degree $x_{\\min}$. Motivated by this, assume the degree distribution above $x_{\\min}$ follows a Pareto (Type I) distribution with scale parameter $x_{\\min} > 0$ and shape parameter $\\alpha > 0$, having probability density function $f(x)$ supported on $x \\ge x_{\\min}$. Starting only from the definitions of the probability density function $f(x)$, the cumulative distribution function $F(x)$, and the definitions of the median and the mean, perform the following:\n\n- Using $f(x) = \\alpha x_{\\min}^{\\alpha} x^{-(\\alpha+1)}$ for $x \\ge x_{\\min}$ and $f(x) = 0$ otherwise, derive the cumulative distribution function $F(x)$.\n- Using the definition that the median $m$ satisfies $F(m) = \\tfrac{1}{2}$, solve for $m$ in terms of $x_{\\min}$ and $\\alpha$.\n- Assuming $\\alpha > 1$ so that the mean exists, compute the mean $\\mu$ from first principles and then provide the exact expression for the ratio $\\mu/m$ as a function of $\\alpha$.\n\nYour final answer must be a single closed-form analytic expression for $\\mu/m$ in terms of $\\alpha$. Do not approximate or round. Do not include units. Explain each derivation step using only the fundamental definitions stated above. Conclude by briefly stating, in one sentence, how the value of $\\mu/m$ reflects right-skewness for $\\alpha > 1$ in heavy-tailed data observed on log-log plots.",
            "solution": "The problem requires the derivation of the ratio of the mean to the median for a Pareto (Type I) distribution, starting from first principles. The problem is well-posed, scientifically sound, and contains all necessary information for a unique solution.\n\nFirst, we derive the cumulative distribution function (CDF), $F(x)$, from the given probability density function (PDF), $f(x) = \\alpha x_{\\min}^{\\alpha} x^{-(\\alpha+1)}$ for $x \\ge x_{\\min}$. The CDF is defined as $F(x) = \\int_{-\\infty}^{x} f(t) dt$. Since the support of the distribution is $[x_{\\min}, \\infty)$, the PDF is zero for $t < x_{\\min}$. Therefore, for any $x \\ge x_{\\min}$, the CDF is given by the integral:\n$$F(x) = \\int_{x_{\\min}}^{x} f(t) dt = \\int_{x_{\\min}}^{x} \\alpha x_{\\min}^{\\alpha} t^{-(\\alpha+1)} dt$$\nWe can factor out the constants with respect to the integration variable $t$:\n$$F(x) = \\alpha x_{\\min}^{\\alpha} \\int_{x_{\\min}}^{x} t^{-(\\alpha+1)} dt$$\nThe integral of $t^n$ is $\\frac{t^{n+1}}{n+1}$. Here, the exponent is $-(\\alpha+1)$, so the integral evaluates to:\n$$F(x) = \\alpha x_{\\min}^{\\alpha} \\left[ \\frac{t^{-(\\alpha+1)+1}}{-(\\alpha+1)+1} \\right]_{x_{\\min}}^{x} = \\alpha x_{\\min}^{\\alpha} \\left[ \\frac{t^{-\\alpha}}{-\\alpha} \\right]_{x_{\\min}}^{x}$$\nEvaluating the expression at the limits of integration:\n$$F(x) = \\alpha x_{\\min}^{\\alpha} \\left( \\frac{x^{-\\alpha}}{-\\alpha} - \\frac{x_{\\min}^{-\\alpha}}{-\\alpha} \\right) = -x_{\\min}^{\\alpha} (x^{-\\alpha} - x_{\\min}^{-\\alpha})$$\n$$F(x) = -x_{\\min}^{\\alpha} x^{-\\alpha} + x_{\\min}^{\\alpha} x_{\\min}^{-\\alpha} = - \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha} + 1$$\nSo, the cumulative distribution function for $x \\ge x_{\\min}$ is:\n$$F(x) = 1 - \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$$\nFor $x < x_{\\min}$, $F(x) = 0$.\n\nNext, we find the median, $m$. By definition, the median is the value of $x$ for which $F(x) = \\frac{1}{2}$. Since $\\alpha > 0$ and $x_{\\min} > 0$, the CDF is strictly increasing from $F(x_{\\min}) = 0$ to $\\lim_{x \\to \\infty} F(x) = 1$, so a unique median exists and is greater than $x_{\\min}$. We set $F(m) = \\frac{1}{2}$:\n$$1 - \\left(\\frac{x_{\\min}}{m}\\right)^{\\alpha} = \\frac{1}{2}$$\nRearranging the terms to solve for $m$:\n$$\\left(\\frac{x_{\\min}}{m}\\right)^{\\alpha} = 1 - \\frac{1}{2} = \\frac{1}{2}$$\nTaking the $\\alpha$-th root of both sides:\n$$\\frac{x_{\\min}}{m} = \\left(\\frac{1}{2}\\right)^{1/\\alpha} = 2^{-1/\\alpha}$$\nSolving for $m$:\n$$m = \\frac{x_{\\min}}{2^{-1/\\alpha}} = x_{\\min} 2^{1/\\alpha}$$\n\nNow, we compute the mean, $\\mu$, of the distribution. The mean is the expected value of the random variable $X$, defined by $\\mu = E[X] = \\int_{-\\infty}^{\\infty} x f(x) dx$. The problem states that we assume $\\alpha > 1$, which is the condition required for the mean to be finite.\n$$\\mu = \\int_{x_{\\min}}^{\\infty} x \\left( \\alpha x_{\\min}^{\\alpha} x^{-(\\alpha+1)} \\right) dx$$\nCombining the terms involving $x$:\n$$\\mu = \\alpha x_{\\min}^{\\alpha} \\int_{x_{\\min}}^{\\infty} x \\cdot x^{-(\\alpha+1)} dx = \\alpha x_{\\min}^{\\alpha} \\int_{x_{\\min}}^{\\infty} x^{-\\alpha} dx$$\nWe integrate $x^{-\\alpha}$:\n$$\\mu = \\alpha x_{\\min}^{\\alpha} \\left[ \\frac{x^{-\\alpha+1}}{-\\alpha+1} \\right]_{x_{\\min}}^{\\infty}$$\nThe upper limit requires evaluating $\\lim_{x \\to \\infty} x^{1-\\alpha}$. Since we assumed $\\alpha > 1$, the exponent $(1-\\alpha)$ is negative, so this limit is $0$.\n$$\\mu = \\frac{\\alpha x_{\\min}^{\\alpha}}{1-\\alpha} \\left( 0 - x_{\\min}^{1-\\alpha} \\right) = \\frac{\\alpha x_{\\min}^{\\alpha}}{-(\\alpha-1)} (-x_{\\min}^{1-\\alpha})$$\n$$\\mu = \\frac{\\alpha}{\\alpha-1} x_{\\min}^{\\alpha} x_{\\min}^{1-\\alpha} = \\frac{\\alpha}{\\alpha-1} x_{\\min}^{\\alpha+1-\\alpha}$$\nThis simplifies to the expression for the mean:\n$$\\mu = \\frac{\\alpha}{\\alpha-1} x_{\\min}$$\n\nFinally, we find the ratio of the mean to the median, $\\mu/m$.\n$$\\frac{\\mu}{m} = \\frac{\\frac{\\alpha}{\\alpha-1} x_{\\min}}{x_{\\min} 2^{1/\\alpha}}$$\nThe parameter $x_{\\min}$ cancels out, yielding the final expression as a function of $\\alpha$ only:\n$$\\frac{\\mu}{m} = \\frac{\\alpha}{(\\alpha-1) 2^{1/\\alpha}}$$\n\nFor $\\alpha > 1$, the ratio $\\mu/m$ is always greater than $1$, confirming the right-skewness of the distribution where the mean is pulled above the median by the heavy right tail.",
            "answer": "$$\\boxed{\\frac{\\alpha}{(\\alpha-1) 2^{1/\\alpha}}}$$"
        },
        {
            "introduction": "While visual inspection of log-log plots can suggest a power-law relationship, a more rigorous statistical approach is required for robust parameter estimation. This practice introduces the principle of Maximum Likelihood Estimation (MLE), the gold standard for fitting the Pareto distribution to data. Deriving the estimator for the tail index $\\alpha$ will provide you with a powerful and widely applicable tool for analyzing heavy-tailed phenomena.",
            "id": "4297975",
            "problem": "A researcher studies the heavy-tailed distribution of node strengths in a communication network and models the upper tail using a continuous Pareto (Type I) law. Let $x_{\\min} > 0$ be a known lower cutoff and assume the tail is described by the survival function $P(X \\geq x) = \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$ for $x \\geq x_{\\min}$, where $\\alpha > 0$ is the unknown tail index. The researcher collects an independent and identically distributed sample $\\{x_{i}\\}_{i=1}^{n}$ from the tail, where each observation satisfies $x_{i} \\geq x_{\\min}$. \n\nStarting only from the survival function definition and the principle of maximum likelihood estimation, derive the closed-form maximum likelihood estimator for the tail index $\\alpha$ and establish the asymptotic variance of this estimator under standard regularity conditions. Express your final results as analytic expressions involving $n$, $x_{\\min}$, and the sample $\\{x_{i}\\}_{i=1}^{n}$. No numerical evaluation is required. If you provide more than one expression, list them in a single row as a matrix. Do not include any units in your final expressions.",
            "solution": "The problem statement is subjected to validation before proceeding with a solution.\n\n### Step 1: Extract Givens\n- The random variable $X$ represents node strengths in a communication network.\n- The upper tail of the distribution of $X$ follows a continuous Pareto (Type I) law for $x \\geq x_{\\min}$.\n- $x_{\\min} > 0$ is a known lower cutoff value.\n- The survival function is given by $S(x) = P(X \\geq x) = \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$ for $x \\geq x_{\\min}$.\n- The tail index $\\alpha > 0$ is an unknown parameter to be estimated.\n- The data consist of an independent and identically distributed (i.i.d.) sample $\\{x_{i}\\}_{i=1}^{n}$ from this tail distribution, where each $x_i \\geq x_{\\min}$.\n- The goal is to derive the closed-form maximum likelihood estimator (MLE) for $\\alpha$ and to establish the asymptotic variance of this estimator.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem is scientifically sound. The Pareto distribution is a canonical model for heavy-tailed phenomena, and maximum likelihood estimation is a fundamental principle of statistical inference. The entire problem is a standard exercise in mathematical statistics.\n- **Well-Posed:** The problem is well-posed. It provides the distributional form, the parameter of interest, the nature of the data, and the estimation method to be used. It asks for well-defined statistical quantities: the estimator and its asymptotic variance.\n- **Objective:** The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is self-contained, scientifically grounded, and well-posed. A rigorous derivation can be performed.\n\n### Derivation of the Maximum Likelihood Estimator (MLE)\n\nThe derivation proceeds by first finding the probability density function (PDF) from the given survival function, then constructing the log-likelihood function for the sample, and finally maximizing this function with respect to the parameter $\\alpha$.\n\n1.  **Probability Density Function (PDF):**\n    The cumulative distribution function (CDF) is $F(x) = P(X \\leq x) = 1 - P(X > x)$. Since the distribution is continuous, $P(X>x) = P(X\\geq x)$. Thus, for $x \\geq x_{\\min}$, the CDF is:\n    $$F(x; \\alpha) = 1 - S(x) = 1 - \\left(\\frac{x_{\\min}}{x}\\right)^{\\alpha}$$\n    The PDF, $p(x; \\alpha)$, is the derivative of the CDF with respect to $x$:\n    $$p(x; \\alpha) = \\frac{d}{dx} F(x; \\alpha) = \\frac{d}{dx} \\left(1 - x_{\\min}^{\\alpha}x^{-\\alpha}\\right) = -x_{\\min}^{\\alpha} (-\\alpha) x^{-\\alpha-1}$$\n    This simplifies to the PDF for the Pareto Type I distribution:\n    $$p(x; \\alpha) = \\frac{\\alpha x_{\\min}^{\\alpha}}{x^{\\alpha+1}} \\quad \\text{for } x \\geq x_{\\min}$$\n\n2.  **Likelihood and Log-Likelihood Functions:**\n    Given an i.i.d. sample $\\{x_{i}\\}_{i=1}^{n}$, the likelihood function $L(\\alpha; \\{x_i\\})$ is the joint probability of observing the data:\n    $$L(\\alpha; \\{x_i\\}) = \\prod_{i=1}^{n} p(x_i; \\alpha) = \\prod_{i=1}^{n} \\frac{\\alpha x_{\\min}^{\\alpha}}{x_i^{\\alpha+1}}$$\n    To simplify maximization, we work with the log-likelihood function, $\\ell(\\alpha) = \\ln(L(\\alpha))$:\n    $$\\ell(\\alpha; \\{x_i\\}) = \\ln\\left( \\prod_{i=1}^{n} \\frac{\\alpha x_{\\min}^{\\alpha}}{x_i^{\\alpha+1}} \\right) = \\sum_{i=1}^{n} \\ln\\left( \\frac{\\alpha x_{\\min}^{\\alpha}}{x_i^{\\alpha+1}} \\right)$$\n    $$\\ell(\\alpha) = \\sum_{i=1}^{n} \\left[ \\ln(\\alpha) + \\alpha \\ln(x_{\\min}) - (\\alpha+1) \\ln(x_i) \\right]$$\n    $$\\ell(\\alpha) = n \\ln(\\alpha) + n\\alpha \\ln(x_{\\min}) - (\\alpha+1) \\sum_{i=1}^{n} \\ln(x_i)$$\n\n3.  **Maximization:**\n    To find the MLE, $\\hat{\\alpha}_{ML}$, we differentiate $\\ell(\\alpha)$ with respect to $\\alpha$ and set the result to zero (the first-order condition):\n    $$\\frac{d\\ell}{d\\alpha} = \\frac{n}{\\alpha} + n \\ln(x_{\\min}) - \\sum_{i=1}^{n} \\ln(x_i) = 0$$\n    Solving for $\\alpha$ gives the estimator $\\hat{\\alpha}_{ML}$:\n    $$\\frac{n}{\\hat{\\alpha}_{ML}} = \\sum_{i=1}^{n} \\ln(x_i) - n \\ln(x_{\\min}) = \\sum_{i=1}^{n} \\left( \\ln(x_i) - \\ln(x_{\\min}) \\right)$$\n    $$\\frac{n}{\\hat{\\alpha}_{ML}} = \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)$$\n    Thus, the closed-form MLE for $\\alpha$ is:\n    $$\\hat{\\alpha}_{ML} = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)}$$\n    To confirm this is a maximum, we check the second derivative:\n    $$\\frac{d^2\\ell}{d\\alpha^2} = -\\frac{n}{\\alpha^2}$$\n    Since $n>0$ and $\\alpha^2>0$, the second derivative is always negative, confirming the log-likelihood function is concave and the stationary point is a global maximum.\n\n### Derivation of the Asymptotic Variance\n\nThe asymptotic variance of an MLE is given by the inverse of the Fisher information.\n\n1.  **Fisher Information:**\n    The Fisher information $I(\\alpha)$ for a sample of size $n$ is $n$ times the Fisher information for a single observation, $I_1(\\alpha)$. For a single parameter, $I_1(\\alpha)$ can be calculated as:\n    $$I_1(\\alpha) = -E\\left[ \\frac{d^2}{d\\alpha^2} \\ln(p(x; \\alpha)) \\right]$$\n    From our earlier work, the log-PDF for a single observation is $\\ln(p(x; \\alpha)) = \\ln(\\alpha) + \\alpha \\ln(x_{\\min}) - (\\alpha+1) \\ln(x)$.\n    The second derivative with respect to $\\alpha$ is:\n    $$\\frac{d^2}{d\\alpha^2} \\ln(p(x; \\alpha)) = \\frac{d}{d\\alpha} \\left(\\frac{1}{\\alpha} + \\ln(x_{\\min}) - \\ln(x)\\right) = -\\frac{1}{\\alpha^2}$$\n    Since this expression does not depend on the random variable $x$, its expectation is the expression itself:\n    $$E\\left[ \\frac{d^2}{d\\alpha^2} \\ln(p(x; \\alpha)) \\right] = -\\frac{1}{\\alpha^2}$$\n    Therefore, the Fisher information for a single observation is:\n    $$I_1(\\alpha) = - \\left(-\\frac{1}{\\alpha^2}\\right) = \\frac{1}{\\alpha^2}$$\n    The total Fisher information for the sample of size $n$ is:\n    $$I(\\alpha) = n I_1(\\alpha) = \\frac{n}{\\alpha^2}$$\n\n2.  **Asymptotic Variance:**\n    Under standard regularity conditions, the MLE $\\hat{\\alpha}_{ML}$ is asymptotically normally distributed with a variance equal to the CramÃ©r-Rao Lower Bound, which is the inverse of the Fisher information.\n    $$\\text{Var}(\\hat{\\alpha}_{ML}) \\approx [I(\\alpha)]^{-1} = \\frac{\\alpha^2}{n}$$\n    This is the theoretical asymptotic variance of the estimator, which depends on the true unknown parameter $\\alpha$. The problem requires the final result to be expressed in terms of the sample data. A consistent estimator for the asymptotic variance is obtained by substituting the MLE $\\hat{\\alpha}_{ML}$ for the true parameter $\\alpha$:\n    $$\\widehat{\\text{Var}}(\\hat{\\alpha}_{ML}) = \\frac{\\hat{\\alpha}_{ML}^2}{n} = \\frac{1}{n} \\left( \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)} \\right)^2 = \\frac{n}{\\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right) \\right)^2}$$\n    This expression provides the estimated asymptotic variance based on the sample data.\n\nThe two required expressions are the estimator $\\hat{\\alpha}_{ML}$ and its estimated asymptotic variance.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)} & \\frac{n}{\\left(\\sum_{i=1}^{n} \\ln\\left(\\frac{x_i}{x_{\\min}}\\right)\\right)^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world data collection is often imperfect, and our estimation methods must account for these imperfections. Building upon the standard MLE framework (), this practice tackles a common challenge in network science: estimating a true degree distribution from an incomplete, subsampled graph. By deriving a corrected estimator, you will learn how to systematically account for sampling bias and recover a more accurate picture of the underlying system's structure.",
            "id": "4297905",
            "problem": "Consider a large undirected network in which node degrees are independent and identically distributed. Let the true degree of a randomly chosen node be the integer-valued random variable $K$ with Complementary Cumulative Distribution Function (CCDF) $\\bar{F}(k) = \\mathbb{P}(K \\ge k)$. Assume that the tail of $\\bar{F}(k)$ is regularly varying in the sense that there exists $\\alpha > 1$ and a slowly varying function $L(\\cdot)$ such that for large $k$, $\\bar{F}(k) \\sim L(k)\\,k^{1-\\alpha}$. As a special case used for explicit calculation, assume that for $k \\ge k_{\\min}$ the CCDF is exactly Pareto Type I: $\\bar{F}(k) = \\left(\\frac{k}{k_{\\min}}\\right)^{1-\\alpha}$, where $k_{\\min} \\ge 1$ is a known lower-tail cutoff.\n\nSuppose the network is observed via either independent edge subsampling or node-induced subsampling:\n\n- Under edge subsampling, each edge is independently retained with probability $f \\in (0,1)$, and removed otherwise. Conditional on $K=k$, the observed degree $D$ of the node in the subsampled graph satisfies $D \\sim \\mathrm{Binomial}(k,f)$.\n\n- Under node-induced subsampling, each node is independently retained with probability $f \\in (0,1)$, and only edges between retained nodes are kept. Conditional on $K=k$, the observed degree $D$ of a retained node also satisfies $D \\sim \\mathrm{Binomial}(k,f)$.\n\nThus for either subsampling mechanism, conditional on $K$, $D \\sim \\mathrm{Binomial}(K,f)$.\n\nStarting from the definitions above and well-tested concentration inequalities for the binomial distribution, derive, for large $k$, how subsampling biases the observed degree tail $\\bar{F}_f(k) = \\mathbb{P}(D \\ge k)$ in terms of $\\bar{F}(k)$ and $f$. Then, under the exact Pareto tail assumption for $k \\ge k_{\\min}$, determine the asymptotic scaling of $\\bar{F}_f(k)$ on a log-log plot, explicitly quantifying the slope and the vertical shift caused by $f$.\n\nFinally, using the continuous Pareto Type I likelihood for the tail (treated as a high-degree continuous approximation), derive the maximum likelihood estimator that corrects for the sampling fraction $f$ to estimate the tail exponent $\\alpha$ from an observed sample of subsampled degrees $d_1, d_2, \\dots, d_n$ that satisfy $d_i \\ge f\\,k_{\\min}$. Your estimator should be expressed in closed form using $d_i$, $f$, and $k_{\\min}$ only. Report, as your final answer, the explicit analytic expression for this corrected estimator of $\\alpha$. No rounding is required. Express the final answer as a single closed-form expression with standard mathematical notation.",
            "solution": "The problem requires an analysis of the effects of network subsampling on a heavy-tailed degree distribution. The analysis is divided into three parts: first, deriving the asymptotic relationship between the true degree Complementary Cumulative Distribution Function (CCDF), $\\bar{F}(k)$, and the observed degree CCDF, $\\bar{F}_f(k)$; second, quantifying the effect of this relationship on a log-log plot; and third, deriving a maximum likelihood estimator (MLE) for the tail exponent $\\alpha$ that corrects for the subsampling fraction $f$.\n\nLet $K$ be the true degree of a node, a random variable with CCDF $\\bar{F}(k) = \\mathbb{P}(K \\ge k)$. For large $k$, the tail is regularly varying: $\\bar{F}(k) \\sim L(k)\\,k^{1-\\alpha}$ for some $\\alpha > 1$ and a slowly varying function $L(\\cdot)$. Let $D$ be the observed degree after subsampling. Conditional on $K=k$, the observed degree $D$ follows a binomial distribution, $D \\sim \\mathrm{Binomial}(k,f)$, where $f \\in (0,1)$ is the sampling probability.\n\nThe CCDF of the observed degree $D$, denoted $\\bar{F}_f(k) = \\mathbb{P}(D \\ge k)$, can be expressed using the law of total probability, by averaging over all possible values of the true degree $K$:\n$$ \\bar{F}_f(k) = \\mathbb{P}(D \\ge k) = \\mathbb{E}_K\\left[ \\mathbb{P}(D \\ge k | K) \\right] = \\sum_{j=0}^{\\infty} \\mathbb{P}(D \\ge k | K=j) \\mathbb{P}(K=j) $$\nSince a node with observed degree $D \\ge k$ must have a true degree of at least $k$, the sum starts at $j=k$:\n$$ \\bar{F}_f(k) = \\sum_{j=k}^{\\infty} \\mathbb{P}(\\mathrm{Binomial}(j,f) \\ge k) \\mathbb{P}(K=j) $$\n\nThe first part of the problem asks to derive the asymptotic behavior of $\\bar{F}_f(k)$ for large $k$. The key insight comes from the concentration of the binomial distribution. For a large number of trials $j$, the binomial random variable $\\mathrm{Binomial}(j,f)$ is sharply concentrated around its mean $jf$. Concentration inequalities, such as the Chernoff bound, formally state that the probability of significant deviation from the mean is exponentially small in $j$.\nThis implies that the function $g(j) = \\mathbb{P}(\\mathrm{Binomial}(j,f) \\ge k)$ behaves essentially as a step function for large $k$.\n\\begin{itemize}\n    \\item If $j$ is substantially smaller than $k/f$, then $jf \\ll k$. The probability of observing $k$ or more successes is negligible, so $g(j) \\approx 0$.\n    \\item If $j$ is substantially larger than $k/f$, then $jf \\gg k$. The probability of observing at least $k$ successes is very close to $1$, so $g(j) \\approx 1$.\n\\end{itemize}\nThe transition between these two regimes occurs in a narrow region around $j \\approx k/f$. Because the true degree distribution $\\mathbb{P}(K=j)$ is heavy-tailed and decays slowly (as a power law), the sum for $\\bar{F}_f(k)$ is dominated by the behavior of the terms where $\\mathbb{P}(K=j)$ is largest and $g(j)$ is non-negligible. This occurs for the smallest values of $j$ where $g(j) \\approx 1$, i.e., for $j \\ge k/f$.\nTherefore, we can approximate the sum by taking $g(j)$ to be a step function that is $0$ for $j < k/f$ and $1$ for $j \\ge k/f$:\n$$ \\bar{F}_f(k) \\approx \\sum_{j=\\lceil k/f \\rceil}^{\\infty} \\mathbb{P}(K=j) = \\mathbb{P}(K \\ge \\lceil k/f \\rceil) $$\nFor large $k$, we can ignore the ceiling function and treat $k/f$ as continuous. This gives the asymptotic relationship:\n$$ \\bar{F}_f(k) \\sim \\bar{F}(k/f) $$\nThis expression reveals how subsampling biases the tail: the probability of observing a degree of at least $k$ is approximately equal to the probability that the true degree was at least $k/f$.\n\nFor the second part, we use the specific Pareto Type I form for the true degree tail, $\\bar{F}(k) = \\left(\\frac{k}{k_{\\min}}\\right)^{1-\\alpha}$ for $k \\ge k_{\\min}$. We apply the derived asymptotic relationship. This is valid for observed degrees $k$ such that the argument of $\\bar{F}$ is above its threshold, i.e., $k/f \\ge k_{\\min}$, or $k \\ge f k_{\\min}$.\n$$ \\bar{F}_f(k) \\sim \\bar{F}(k/f) = \\left(\\frac{k/f}{k_{\\min}}\\right)^{1-\\alpha} = (f^{-1})^{1-\\alpha} \\left(\\frac{k}{k_{\\min}}\\right)^{1-\\alpha} = f^{\\alpha-1} \\left(\\frac{k}{k_{\\min}}\\right)^{1-\\alpha} $$\nTo analyze the scaling on a log-log plot, we take the natural logarithm of both the true and observed CCDFs.\nFor the true distribution:\n$$ \\ln(\\bar{F}(k)) = (1-\\alpha) \\ln\\left(\\frac{k}{k_{\\min}}\\right) = (1-\\alpha)\\ln(k) - (1-\\alpha)\\ln(k_{\\min}) $$\nFor the observed distribution:\n$$ \\ln(\\bar{F}_f(k)) \\approx \\ln\\left(f^{\\alpha-1} \\left(\\frac{k}{k_{\\min}}\\right)^{1-\\alpha}\\right) = (\\alpha-1)\\ln(f) + (1-\\alpha)\\ln(k) - (1-\\alpha)\\ln(k_{\\min}) $$\nComparing the two expressions, we can identify the slope and the vertical shift on a log-log plot (where the y-axis is $\\ln(\\bar{F})$ and the x-axis is $\\ln(k)$).\n\\begin{itemize}\n    \\item **Slope**: The coefficient of $\\ln(k)$ is $(1-\\alpha)$ for both distributions. Thus, the power-law exponent is preserved under subsampling, and the slope of the tail on a log-log plot remains unchanged.\n    \\item **Vertical Shift**: The difference in the vertical position of the two curves is $\\ln(\\bar{F}_f(k)) - \\ln(\\bar{F}(k)) = (\\alpha-1)\\ln(f)$. Since $f \\in (0,1)$, $\\ln(f)$ is negative. Since $\\alpha > 1$, the vertical shift $(\\alpha-1)\\ln(f)$ is negative. This means the observed tail is shifted downwards on the log-log plot relative to the true tail.\n\\end{itemize}\n\nFor the final part, we derive the MLE for $\\alpha$. The problem specifies using a continuous Pareto Type I approximation for the observed tail, based on the data $d_1, d_2, \\dots, d_n$, where each $d_i \\ge f k_{\\min}$. From our previous analysis, the CCDF of the observed degrees $D$ for $d \\ge f k_{\\min}$ is modeled by:\n$$ \\mathbb{P}(D \\ge d) = \\left(\\frac{d}{f k_{\\min}}\\right)^{1-\\alpha} $$\nThis is the CCDF of a continuous Pareto Type I distribution with tail exponent $\\alpha$ and a lower cutoff (minimum value) of $d_{\\min}' = f k_{\\min}$. The corresponding probability density function (PDF) $p_f(d)$ for $d \\ge f k_{\\min}$ is:\n$$ p_f(d) = -\\frac{d}{dd} \\left(\\frac{d}{f k_{\\min}}\\right)^{1-\\alpha} = - (f k_{\\min})^{\\alpha-1} (1-\\alpha) d^{-\\alpha} = (\\alpha-1) (f k_{\\min})^{\\alpha-1} d^{-\\alpha} $$\nThe likelihood function $L(\\alpha)$ for an i.i.d. sample $d_1, \\dots, d_n$ is the product of the individual probabilities:\n$$ L(\\alpha | \\{d_i\\}, f, k_{\\min}) = \\prod_{i=1}^{n} p_f(d_i) = \\prod_{i=1}^{n} \\left[ (\\alpha-1) (f k_{\\min})^{\\alpha-1} d_i^{-\\alpha} \\right] $$\n$$ L(\\alpha) = \\left( (\\alpha-1) (f k_{\\min})^{\\alpha-1} \\right)^n \\left( \\prod_{i=1}^{n} d_i \\right)^{-\\alpha} $$\nTo simplify maximization, we use the log-likelihood function, $\\ell(\\alpha) = \\ln(L(\\alpha))$:\n$$ \\ell(\\alpha) = n \\ln(\\alpha-1) + n(\\alpha-1)\\ln(f k_{\\min}) - \\alpha \\sum_{i=1}^{n} \\ln(d_i) $$\nWe find the MLE, $\\hat{\\alpha}$, by differentiating $\\ell(\\alpha)$ with respect to $\\alpha$ and setting the result to zero:\n$$ \\frac{d\\ell}{d\\alpha} = \\frac{n}{\\alpha-1} + n\\ln(f k_{\\min}) - \\sum_{i=1}^{n} \\ln(d_i) = 0 $$\nNow, we solve for $\\alpha$:\n$$ \\frac{n}{\\alpha-1} = \\sum_{i=1}^{n} \\ln(d_i) - n\\ln(f k_{\\min}) $$\n$$ \\frac{n}{\\alpha-1} = \\sum_{i=1}^{n} (\\ln(d_i) - \\ln(f k_{\\min})) $$\n$$ \\frac{n}{\\alpha-1} = \\sum_{i=1}^{n} \\ln\\left(\\frac{d_i}{f k_{\\min}}\\right) $$\nInverting this expression gives $\\alpha-1$:\n$$ \\alpha-1 = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{d_i}{f k_{\\min}}\\right)} $$\nFinally, the corrected estimator for the tail exponent $\\alpha$ is:\n$$ \\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{d_i}{f k_{\\min}}\\right)} $$\nThis estimator correctly accounts for the subsampling fraction $f$ by effectively treating the observed data as being drawn from a Pareto distribution with a shifted minimum value of $f k_{\\min}$.",
            "answer": "$$\n\\boxed{1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{d_i}{f k_{\\min}}\\right)}}\n$$"
        }
    ]
}