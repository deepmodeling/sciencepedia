{
    "hands_on_practices": [
        {
            "introduction": "计算混合模式是网络科学中的一项基本技能。虽然许多讨论都集中在单分网络上，但二分网络（如作者-论文网络或演员-电影网络）在现实世界中同样普遍且重要。本练习将指导您在一个具体的二分网络上计算度相关性，从而巩固您对相关性基本统计定义的理解。通过为连接两个不同节点集的边构建度混合矩阵并计算相关系数 $r_{AB}$，您将学会如何将核心概念应用于更复杂的网络结构中。",
            "id": "4287880",
            "problem": "考虑一个简单的二分网络 $G=(A \\cup B, E)$，其中 $A=\\{A_1, A_2, A_3, A_4, A_5\\}$ 且 $B=\\{B_1, B_2, B_3, B_4, B_5, B_6\\}$，边仅存在于划分的两部分之间，即 $E \\subseteq A \\times B$。跨划分的边集为\n$$(A_1,B_1),\\ (A_1,B_2),\\ (A_1,B_3),\\ (A_2,B_2),\\ (A_2,B_4),\\ (A_3,B_3),\\ (A_3,B_5),\\ (A_3,B_6),\\ (A_3,B_2),\\ (A_4,B_1),\\ (A_5,B_4),\\ (A_5,B_5)。$$\n令 $k_A(i)$ 表示节点 $i \\in A$ 的度（邻接边的数量），令 $k_B(j)$ 表示节点 $j \\in B$ 的度。度混合矩阵 $M_{k_A,k_B}$ 是在度类上定义的数组，其条目记录了在 $E$ 中连接一个度为 $k_A$ 的 A 节点和一个度为 $k_B$ 的 B 节点的边的数量。仅从二分网络中度的基本定义以及统计期望、协方差和皮尔逊相关系数（PCC）出发，为该网络构建度混合矩阵 $M_{k_A, k_B}$，并推导出跨划分边的端点度相关性 $r_{AB}$ 作为联合边-端点度分布函数的解析表达式。将 $r_{AB}$ 的最终结果表示为单个精确的封闭形式表达式。无需四舍五入，无单位适用。",
            "solution": "所述问题具有科学依据，提法恰当，客观且内部一致。它代表了网络科学中的一个标准练习，特别是关于二分图中的度分类匹配性的分析。所有必要的数据和定义均已提供，不存在逻辑矛盾或含糊不清之处。因此，该问题被认为是有效的，可以构建一个完整的解答。\n\n主要任务是计算给定二分网络的端点度相关性，记为 $r_{AB}$。该量正式定义为随机选择一条边的两个端点节点度之间的皮尔逊相关系数（PCC）。样本空间是网络中所有边 $E$ 的集合，我们假设每条边被选中的概率相等。\n\n令 $K_A$ 和 $K_B$ 为随机变量，分别表示随机选择一条边的端点（分属划分 A 和 B）的度。PCC，$r_{AB}$，定义为：\n$$r_{AB} = \\frac{\\text{Cov}(K_A, K_B)}{\\sigma_{K_A} \\sigma_{K_B}}$$\n其中 $\\text{Cov}(K_A, K_B)$ 是 $K_A$ 和 $K_B$ 的协方差，$\\sigma_{K_A}$ 和 $\\sigma_{K_B}$ 是它们各自的标准差。这些由基本的统计矩定义：\n$$\\text{Cov}(K_A, K_B) = E[K_A K_B] - E[K_A]E[K_B]$$\n$$\\sigma_{K_A}^2 = \\text{Var}(K_A) = E[K_A^2] - (E[K_A])^2$$\n$$\\sigma_{K_B}^2 = \\text{Var}(K_B) = E[K_B^2] - (E[K_B])^2$$\n因此，$r_{AB}$ 的表达式为：\n$$r_{AB} = \\frac{E[K_A K_B] - E[K_A]E[K_B]}{\\sqrt{\\left(E[K_A^2] - (E[K_A])^2\\right) \\left(E[K_B^2] - (E[K_B])^2\\right)}}$$\n期望是在端点度的联合概率分布 $p(k_A, k_B)$ 上计算的，该分布表示随机选择的一条边连接一个来自划分A的度为 $k_A$ 的节点和一个来自划分B的度为 $k_B$ 的节点的概率。该分布由度混合矩阵 $M_{k_A, k_B}$ 推导得出，其中 $M_{k_A, k_B}$ 统计了此类边的数量。如果 $L = |E|$ 是边的总数，则：\n$$p(k_A, k_B) = \\frac{M_{k_A, k_B}}{L}$$\n期望则由以下公式给出：\n$$E[g(K_A, K_B)] = \\sum_{k_A, k_B} g(k_A, k_B) p(k_A, k_B) = \\frac{1}{L} \\sum_{k_A, k_B} g(k_A, k_B) M_{k_A, k_B}$$\n现在我们将此框架应用于给定的特定网络。\n\n首先，我们确定所有节点的度。边的总数为 $L=12$。\n划分 A 中节点的度为：\n$k_A(A_1) = 3$\n$k_A(A_2) = 2$\n$k_A(A_3) = 4$\n$k_A(A_4) = 1$\n$k_A(A_5) = 2$\n\n划分 B 中节点的度为：\n$k_B(B_1) = 2$ (连接到 $A_1, A_4$)\n$k_B(B_2) = 3$ (连接到 $A_1, A_2, A_3$)\n$k_B(B_3) = 2$ (连接到 $A_1, A_3$)\n$k_B(B_4) = 2$ (连接到 $A_2, A_5$)\n$k_B(B_5) = 2$ (连接到 $A_3, A_5$)\n$k_B(B_6) = 1$ (连接到 $A_3$)\n\n划分 A 的度类为 $\\{1, 2, 3, 4\\}$，划分 B 的度类为 $\\{1, 2, 3\\}$。\n\n接下来，我们通过为 12 条边中的每一条制表其端点度来构建度混合矩阵 $M_{k_A, k_B}$：\n\\begin{itemize}\n    \\item $(A_1,B_1) \\rightarrow (k_A,k_B)=(3,2)$\n    \\item $(A_1,B_2) \\rightarrow (k_A,k_B)=(3,3)$\n    \\item $(A_1,B_3) \\rightarrow (k_A,k_B)=(3,2)$\n    \\item $(A_2,B_2) \\rightarrow (k_A,k_B)=(2,3)$\n    \\item $(A_2,B_4) \\rightarrow (k_A,k_B)=(2,2)$\n    \\item $(A_3,B_2) \\rightarrow (k_A,k_B)=(4,3)$\n    \\item $(A_3,B_3) \\rightarrow (k_A,k_B)=(4,2)$\n    \\item $(A_3,B_5) \\rightarrow (k_A,k_B)=(4,2)$\n    \\item $(A_3,B_6) \\rightarrow (k_A,k_B)=(4,1)$\n    \\item $(A_4,B_1) \\rightarrow (k_A,k_B)=(1,2)$\n    \\item $(A_5,B_4) \\rightarrow (k_A,k_B)=(2,2)$\n    \\item $(A_5,B_5) \\rightarrow (k_A,k_B)=(2,2)$\n\\end{itemize}\n将这些出现次数相加，我们得到矩阵 $M_{k_A, k_B}$，其行由 $k_A$ 索引，列由 $k_B$ 索引：\n$$\nM = \\begin{pmatrix}\n   & k_B=1 & k_B=2 & k_B=3 \\\\\nk_A=1 & 0 & 1 & 0 \\\\\nk_A=2 & 0 & 3 & 1 \\\\\nk_A=3 & 0 & 2 & 1 \\\\\nk_A=4 & 1 & 2 & 1\n\\end{pmatrix}\n$$\n所有条目的总和为 $1+3+1+2+1+1+2+1 = 12 = L$，符合要求。\n\n现在，我们计算必要的期望。令 $p_A(k_A) = \\frac{1}{L} \\sum_{k_B} M_{k_A,k_B}$ 和 $p_B(k_B) = \\frac{1}{L} \\sum_{k_A} M_{k_A,k_B}$ 为边缘分布。$M$ 的行和与列和分别为：\n行和 ($k_A=1,2,3,4$): $1, 4, 3, 4$。\n列和 ($k_B=1,2,3$): $1, 8, 3$。\n\n$E[K_A] = \\sum_{k_A} k_A \\cdot p_A(k_A) = \\frac{1}{12} \\left[ (1)(1) + (2)(4) + (3)(3) + (4)(4) \\right] = \\frac{1+8+9+16}{12} = \\frac{34}{12} = \\frac{17}{6}$。\n\n$E[K_B] = \\sum_{k_B} k_B \\cdot p_B(k_B) = \\frac{1}{12} \\left[ (1)(1) + (2)(8) + (3)(3) \\right] = \\frac{1+16+9}{12} = \\frac{26}{12} = \\frac{13}{6}$。\n\n$E[K_A^2] = \\sum_{k_A} k_A^2 \\cdot p_A(k_A) = \\frac{1}{12} \\left[ (1^2)(1) + (2^2)(4) + (3^2)(3) + (4^2)(4) \\right] = \\frac{1+16+27+64}{12} = \\frac{108}{12} = 9$。\n\n$E[K_B^2] = \\sum_{k_B} k_B^2 \\cdot p_B(k_B) = \\frac{1}{12} \\left[ (1^2)(1) + (2^2)(8) + (3^2)(3) \\right] = \\frac{1+32+27}{12} = \\frac{60}{12} = 5$。\n\n$E[K_A K_B] = \\frac{1}{L} \\sum_{k_A, k_B} k_A k_B M_{k_A, k_B} = \\frac{1}{12} \\left[ (1)(2)(1) + (2)(2)(3) + (2)(3)(1) + (3)(2)(2) + (3)(3)(1) + (4)(1)(1) + (4)(2)(2) + (4)(3)(1) \\right] = \\frac{1}{12} [2+12+6+12+9+4+16+12] = \\frac{73}{12}$。\n\n有了这些矩，我们就可以计算协方差和方差。\n$\\text{Cov}(K_A, K_B) = E[K_A K_B] - E[K_A]E[K_B] = \\frac{73}{12} - \\left(\\frac{17}{6}\\right)\\left(\\frac{13}{6}\\right) = \\frac{73}{12} - \\frac{221}{36} = \\frac{3 \\times 73}{36} - \\frac{221}{36} = \\frac{219 - 221}{36} = -\\frac{2}{36} = -\\frac{1}{18}$。\n\n$\\sigma_{K_A}^2 = E[K_A^2] - (E[K_A])^2 = 9 - \\left(\\frac{17}{6}\\right)^2 = 9 - \\frac{289}{36} = \\frac{324 - 289}{36} = \\frac{35}{36}$。\n\n$\\sigma_{K_B}^2 = E[K_B^2] - (E[K_B])^2 = 5 - \\left(\\frac{13}{6}\\right)^2 = 5 - \\frac{169}{36} = \\frac{180 - 169}{36} = \\frac{11}{36}$。\n\n最后，我们将这些值代入皮尔逊相关系数的公式中：\n$r_{AB} = \\frac{\\text{Cov}(K_A, K_B)}{\\sqrt{\\sigma_{K_A}^2 \\sigma_{K_B}^2}} = \\frac{-1/18}{\\sqrt{(\\frac{35}{36})(\\frac{11}{36})}} = \\frac{-1/18}{\\frac{\\sqrt{35 \\times 11}}{36}} = \\frac{-1/18}{\\frac{\\sqrt{385}}{36}} = -\\frac{1}{18} \\cdot \\frac{36}{\\sqrt{385}} = -\\frac{2}{\\sqrt{385}}$。\n\n这就是给定二分网络的端点度相关性的精确封闭形式表达式。负号表示该网络表现出异配混合，意味着在一个划分中的高度节点有与另一个划分中的低度节点连接的微弱趋势。",
            "answer": "$$\\boxed{-\\frac{2}{\\sqrt{385}}}$$"
        },
        {
            "introduction": "在掌握了如何计算同配性之后，理解其局限性至关重要。一个单一的同配性系数 $r$ 可能无法完全捕捉网络中所有重要的混合特征。本练习旨在区分全局的度相关性（由 $r$ 度量）与高阶节点倾向于形成紧密连接的“富人俱乐部”的局部现象（由 $\\phi(k)$ 度量）。通过构建并分析一个特定的网络，您将亲眼看到，即使全局的同配性为零，一个强大的富人俱乐部现象也可能存在，这凸显了在分析混合模式时采用多方面方法的必要性。",
            "id": "4287846",
            "problem": "考虑简单、无向、无权图。度的混合模式是指在一条均匀选择的边的两端测得的度的联合分布，当度被视为离散类别时，可以用度混合矩阵来表示。从度、边关联和皮尔逊相关系数的定义开始。\n\n给定以下网络 $G$，其节点为 $\\{R_1,R_2,R_3,R_4\\}$（富节点）和 $\\{L_1,L_2,\\dots,L_{12}\\}$（低度节点）。边集 $E$ 的规定如下：\n- 在富节点之间：所有节点对都相连，即边 $(R_1,R_2)$、$(R_1,R_3)$、$(R_1,R_4)$、$(R_2,R_3)$、$(R_2,R_4)$、$(R_3,R_4)$。\n- 每个富节点 $R_i$ 都恰好与三个不同的低度节点相连：对于 $i\\in\\{1,2,3,4\\}$，将 $R_i$ 连接到 $L_{3i-2}$、$L_{3i-1}$ 和 $L_{3i}$，得到边 $(R_1,L_1)$、$(R_1,L_2)$、$(R_1,L_3)$、$(R_2,L_4)$、$(R_2,L_5)$、$(R_2,L_6)$、$(R_3,L_7)$、$(R_3,L_8)$、$(R_3,L_9)$、$(R_4,L_{10})$、$(R_4,L_{11})$、$(R_4,L_{12})$。\n- 低度节点之间两两配对形成边 $(L_1,L_2)$、$(L_3,L_4)$、$(L_5,L_6)$、$(L_7,L_8)$、$(L_9,L_{10})$、$(L_{11},L_{12})$。\n\n任务：\n- 仅使用第一性原理和皮尔逊相关系数的定义，推导无向图的度分类系数 $r$ 的显式表达式，该系数为在一条均匀选择的边的两端测得的度之间的相关性。用关于度的逐边求和形式表达你最终的可计算表达式（不要引用或假设任何预先推导的分类性公式）。\n- 从度超过阈值 $k$ 的节点间的子图密度的基本概念出发，定义富人俱乐部系数 $\\phi(k)$。解释相对于度分类性，它捕捉了混合模式的什么特征。\n- 对于上述特定网络 $G$，评估在阈值 $k=6$ 时的 $\\phi(k)$，并精确计算度分类系数 $r$。你最终报告的答案必须是 $r$ 的精确数值（不进行四舍五入）。",
            "solution": "我们从核心定义开始。对于一个简单无向图 $G=(V,E)$，节点 $u\\in V$ 的度记为 $\\deg(u)$，等于与 $u$ 相关联的边的数量。设 $M=|E|$ 为边的数量。一条均匀随机选择的边 $e\\in E$ 有两个端点，我们记为 $u$ 和 $v$。定义随机变量 $X$ 和 $Y$ 为在一条均匀随机选择的边的两个端点上观察到的度：\n$$\nX=\\deg(u),\\qquad Y=\\deg(v).\n$$\n在一个对边进行均匀采样的无向图中，根据对称性，$X$ 和 $Y$ 的边缘分布是相同的。度分类系数 $r$ 定义为 $X$ 和 $Y$ 之间的皮尔逊相关系数：\n$$\nr=\\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}.\n$$\n由于在这种对称设置下 $\\operatorname{Var}(X)=\\operatorname{Var}(Y)$，我们有 $\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}=\\operatorname{Var}(X)$。因此\n$$\nr=\\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}.\n$$\n我们现在用关于度的逐边求和来表示 $\\operatorname{Cov}(X,Y)$ 和 $\\operatorname{Var}(X)$。对于经验（有限）图，这些可以通过对所有边求和来计算。将边 $e$ 两端的度记为 $x_e=\\deg(u_e)$ 和 $y_e=\\deg(v_e)$，并注意 $(x_e,y_e)$ 是 $(X,Y)$ 在所有边上的实现。经验均值为\n$$\n\\mu_X=\\frac{1}{M}\\sum_{e\\in E} x_e,\\qquad \\mu_Y=\\frac{1}{M}\\sum_{e\\in E} y_e.\n$$\n根据无向图的对称性，\n$$\n\\mu_X=\\mu_Y=\\frac{1}{M}\\sum_{e\\in E}\\frac{x_e+y_e}{2}.\n$$\n我们记\n$$\nm_1=\\frac{1}{M}\\sum_{e\\in E}\\frac{x_e+y_e}{2},\n$$\n这是共同的均值。经验协方差为\n$$\n\\operatorname{Cov}(X,Y)=\\frac{1}{M}\\sum_{e\\in E} x_e y_e - \\mu_X\\mu_Y = \\frac{1}{M}\\sum_{e\\in E} x_e y_e - m_1^2.\n$$\n$X$ 的经验方差等于 $Y$ 的经验方差，可以写成\n$$\n\\operatorname{Var}(X)=\\frac{1}{M}\\sum_{e\\in E} \\left(x_e^2\\right) - \\mu_X^2,\n$$\n但利用边的对称性来写出两端方差的平均值更方便，这也等于 $\\operatorname{Var}(X)$：\n$$\n\\operatorname{Var}(X)=\\frac{1}{M}\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2} - m_1^2.\n$$\n因此，结合这些表达式，一个可计算的分类性逐边表达式是\n$$\nr=\\frac{\\frac{1}{M}\\sum_{e\\in E} x_e y_e - m_1^2}{\\frac{1}{M}\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2} - m_1^2}.\n$$\n这个表达式是直接从皮尔逊相关定义和无向图的对称性推导出来的；它与跨边端点的度的相关性一致。用混合矩阵的语言来说，跨边的 $(X,Y)$ 的联合分布对应于离散度值上的度混合矩阵，分子计算了联合度乘积与边缘均值乘积的偏差。\n\n接下来，我们定义富人俱乐部系数。考虑一个阈值 $k\\in\\mathbb{N}$。设 $V_{>k}=\\{u\\in V:\\deg(u)>k\\}$ 是度超过 $k$ 的节点集合。设 $N_{>k}=|V_{>k}|$ 表示这类节点的数量，设 $E_{>k}$ 是两个端点都在 $V_{>k}$ 中的边的数量（即由 $V_{>k}$ 诱导的子图的边数）。富人俱乐部系数 $\\phi(k)$ 是该诱导子图相对于 $N_{>k}$ 个节点上的完全图的密度：\n$$\n\\phi(k)=\\frac{E_{>k}}{\\binom{N_{>k}}{2}}.\n$$\n该系数衡量高度“富”节点之间连接的紧密程度，而与它们如何连接到图的其余部分无关。相比之下，度分类性 $r$ 捕捉了整个网络中边两端度之间的全局相关结构。因此，一个网络可以表现出强大的富人俱乐部（高 $\\phi(k)$），但其全局度分类性仍然很弱或为零，如果富节点与非富节点之间的连接抵消了富-富连接和穷-穷连接所贡献的正相关性。\n\n我们现在为特定网络 $G$ 计算相关量。\n\n步骤 1：节点的度。\n- 每个富节点 $R_i$ 都与其他所有富节点有边（每个 $R_i$ 有 $3$ 条这样的边），并且与 $3$ 个不同的低度节点有边，所以\n$$\n\\deg(R_i)=3+3=6\\quad\\text{对于所有 }i\\in\\{1,2,3,4\\}.\n$$\n- 每个低度节点 $L_j$ 都有一条边连接到一个富节点，以及一条边连接到其配对的另一个低度节点，所以\n$$\n\\deg(L_j)=1+1=2\\quad\\text{对于所有 }j\\in\\{1,2,\\dots,12\\}.\n$$\n\n步骤 2：按类型计算边数。有三种边类型：\n- 富-富边：有 $6$ 条类型为 $(6,6)$ 的边。\n- 富-低边：有 $12$ 条类型为 $(6,2)$ 的边。\n- 低-低边：有 $6$ 条类型为 $(2,2)$ 的边。\n因此，总边数为\n$$\nM=6+12+6=24.\n$$\n\n步骤 3：计算 $m_1$，即跨边端点度的共同均值。\n我们有\n$$\nm_1=\\frac{1}{M}\\sum_{e\\in E}\\frac{x_e+y_e}{2}.\n$$\n按边类型计算贡献：\n- 对于一条 $(6,6)$ 的边，$\\frac{x_e+y_e}{2}=\\frac{6+6}{2}=6$；有 $6$ 条这样的边，贡献为 $6\\cdot 6=36$。\n- 对于一条 $(6,2)$ 的边，$\\frac{x_e+y_e}{2}=\\frac{6+2}{2}=4$；有 $12$ 条这样的边，贡献为 $12\\cdot 4=48$。\n- 对于一条 $(2,2)$ 的边，$\\frac{x_e+y_e}{2}=\\frac{2+2}{2}=2$；有 $6$ 条这样的边，贡献为 $6\\cdot 2=12$。\n将这些相加，\n$$\n\\sum_{e\\in E}\\frac{x_e+y_e}{2}=36+48+12=96,\n$$\n因此\n$$\nm_1=\\frac{96}{24}=4.\n$$\n\n步骤 4：计算分子 $\\frac{1}{M}\\sum_{e\\in E} x_e y_e - m_1^2$。\n按边类型计算 $\\sum_{e\\in E} x_e y_e$：\n- 对于 $(6,6)$ 的边，$x_e y_e=36$；有 $6$ 条边，贡献为 $6\\cdot 36=216$。\n- 对于 $(6,2)$ 的边，$x_e y_e=12$；有 $12$ 条边，贡献为 $12\\cdot 12=144$。\n- 对于 $(2,2)$ 的边，$x_e y_e=4$；有 $6$ 条边，贡献为 $6\\cdot 4=24$。\n求和，\n$$\n\\sum_{e\\in E} x_e y_e=216+144+24=384,\n$$\n因此\n$$\n\\frac{1}{M}\\sum_{e\\in E} x_e y_e=\\frac{384}{24}=16.\n$$\n由于 $m_1=4$，我们有 $m_1^2=16$，所以分子为\n$$\n16-16=0.\n$$\n\n步骤 5：计算分母 $\\frac{1}{M}\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2} - m_1^2$。\n按边类型计算 $\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2}$：\n- 对于 $(6,6)$ 的边，$\\frac{x_e^2+y_e^2}{2}=\\frac{36+36}{2}=36$；有 $6$ 条边，贡献为 $6\\cdot 36=216$。\n- 对于 $(6,2)$ 的边，$\\frac{x_e^2+y_e^2}{2}=\\frac{36+4}{2}=20$；有 $12$ 条边，贡献为 $12\\cdot 20=240$。\n- 对于 $(2,2)$ 的边，$\\frac{x_e^2+y_e^2}{2}=\\frac{4+4}{2}=4$；有 $6$ 条边，贡献为 $6\\cdot 4=24$。\n求和，\n$$\n\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2}=216+240+24=480,\n$$\n从而\n$$\n\\frac{1}{M}\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2}=\\frac{480}{24}=20.\n$$\n减去 $m_1^2=16$ 得到分母\n$$\n20-16=4.\n$$\n\n步骤 6：组合 $r$。\n根据以上计算，\n$$\nr=\\frac{0}{4}=0.\n$$\n这表明该网络的全局度分类性恰好为零，尽管存在许多富-富 $(6,6)$ 连接和许多低-低 $(2,2)$ 连接；富-低 $(6,2)$ 连接的平衡存在导致了净相关为零。\n\n步骤 7：计算阈值为 $k=6$ 时的富人俱乐部系数。\n我们有 $V_{>6}=\\emptyset$，因为富节点的度为 $6$，并不严格大于 $6$。这里为了捕捉最高度节点集合，常规选择是取阈值 $k=5$，这样 $V_{>5}=\\{R_1,R_2,R_3,R_4\\}$ 且 $N_{>5}=4$。这 $4$ 个节点之间的边数为 $E_{>5}=6$（它们构成一个完全子图）。因此，\n$$\n\\phi(5)=\\frac{E_{>5}}{\\binom{N_{>5}}{2}}=\\frac{6}{\\binom{4}{2}}=\\frac{6}{6}=1.\n$$\n如果我们遵循问题中规定的阈值 $k=6$ 和严格不等式定义 $\\deg(u)>k$，那么 $V_{>6}=\\emptyset$，$ \\phi(6)$ 由于没有节点而无定义或按惯例视为 $0$；然而，在 $k=5$ 时实现了展示强大富人俱乐部的目的，表明 $\\phi(5)=1$ 而 $r=0$。\n\n解释：富人俱乐部系数 $\\phi(k)$ 孤立地衡量高度节点之间的连接密度，当这些节点形成一个团（clique）时可以达到最大值。度分类性 $r$ 对所有边进行平均，并测量端点度之间的相关性；当高-高和低-低边的贡献被高-低边抵消时，它可以为零。因此，这个构造的网络区分了这两个概念：一个强大的富人俱乐部（在 $k=5$ 时）与零度分类性并存。",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "在真实世界的网络分析中，仅提供同配性的点估计值是不够的；我们还需要量化其不确定性。这个基于代码的高级练习将向您介绍非参数自举法（nonparametric bootstrap），这是一种为同配性系数构建置信区间的强大方法。本练习还探讨了由共享节点引起的边依赖性这一微妙但重要的问题，并指导您实现一个更稳健的自举法变体，为您提供进行严谨科学研究所需的实用工具。",
            "id": "4287879",
            "problem": "给定无向简单图，要求您使用同配系数来量化基于度的混合模式，并使用基于边的非参数自助法为该系数构建置信区间。核心关注参数是同配系数 $r$，其定义为在均匀采样的边两端的节点度的皮尔逊相关系数。您必须实现一种有放回地重抽样边的自助法，并构建基于百分位数的置信区间；您还必须实现一个 $m$-out-of-$n$ 自助法变体，以解决由共享节点引起的有限样本边依赖性问题。\n\n定义与假设：\n- 设 $G = (V, E)$ 是一个无向简单图，其节点集为 $V$，边集为 $E$。设 $|V| = n$ 且 $|E| = m$。\n- 对于 $v \\in V$，定义度 $d(v)$ 为与 $v$ 关联的边的数量。\n- 对于一条无向边 $e = \\{u, v\\} \\in E$，定义度对 $\\left(D_u, D_v\\right) = \\left(d(u), d(v)\\right)$。\n- 从 $E$ 中均匀采样一条边，会产生一个随机度对 $\\left(D_u, D_v\\right)$，其分布与 $E$ 中的经验边端度对相同。\n- 同配系数 $r$ 是在此边的经验分布下，$D_u$ 和 $D_v$ 之间的皮尔逊相关系数。\n\n您的任务：\n1. 实现一个函数，根据 $G$ 的无向边列表计算同配系数 $r$。如果在计算中，$D_u$ 或 $D_v$ 在边上的样本方差为零，则将 $r$ 定义为 $0$。\n2. 实现一个基于边的非参数自助法，为 $r$ 构建一个 $95\\%$ 水平的百分位数置信区间。对于自助法：\n   - 固定总共 $B$ 次重抽样。对于每次重抽样，从观测到的 $m$ 条边中有放回地抽取 $m$ 条边，并计算自助法副本 $r^\\star$。\n   - 如果一个自助法副本的 $D_u$ 或 $D_v$ 的方差为零，按照惯例将该 $r^\\star$ 设置为 $0$，以避免未定义的值。\n   - 使用 $B$ 个副本在水平 $0.025$ 和 $0.975$ 处的经验分位数，分别构成百分位数置信区间的下限和上限。\n3. 实现一个 $m$-out-of-$n$ 自助法变体，以减轻有限样本中的边依赖性。具体来说，对于每次重抽样，有放回地抽取 $m_b = \\lfloor m^\\alpha \\rfloor$ 条边（其中 $\\alpha \\in (0, 1)$），重新计算 $r^\\star$，并再次使用水平 $0.025$ 和 $0.975$ 的百分位数区间。使用相同的处理规则，即如果一次重抽样的 $D_u$ 或 $D_v$ 的方差为零，则将该副本设置为 $0$。\n4. 为了可复现性，在进行任何自助法重抽样之前，将伪随机数生成器的种子设置为一个固定值。\n\n基本原理和正确性标准：\n- 您必须从核心定义出发：经验边分布、度 $d(v)$、作为从经验对 $\\{(d(u_e), d(v_e)) : e \\in E\\}$ 计算出的 $r = \\mathrm{Corr}(D_u, D_v)$ 的皮尔逊相关系数，以及对观测单元（此处为边）的非参数自助法。\n- 除了自助法重抽样方案所引入的独立性外，您不能假设边之间有任何独立性。相反，应依赖于 $r$ 是经验边矩的光滑泛函这一原理，并使用对经验单元的重抽样来近似抽样分布。\n- 您必须明确指定所使用的 $B$ 和 $\\alpha$。\n\n测试套件：\n为以下 $4$ 个测试用例生成输出。所有图都是无向简单图。\n\n- 测试用例 $1$（同配结构，手动构建）：\n  - 节点：从 $0$ 到 $15$ 的整数（含），因此 $n = 16$。\n  - 边：节点 $\\{0,1,2,3,4,5\\}$ 上的一个完全图，节点 $\\{6,7,8,9,10,11,12,13,14,15\\}$ 上的一个环，边为 $\\{(6,7),(7,8),\\dots,(15,6)\\}$，以及两条跨边 $\\{(2,6),(3,10)\\}$。\n- 测试用例 $2$（强异配，星形图）：\n  - 节点：从 $0$ 到 $12$ 的整数（含），因此 $n = 13$。\n  - 边：以节点 $0$ 为中心，叶节点为 $\\{1,2,\\dots,12\\}$ 的星形图。\n- 测试用例 $3$（近似中性，Erdős–Rényi模型）：\n  - 节点：从 $0$ 到 $24$ 的整数（含），因此 $n = 25$。\n  - 图：一个 Erdős–Rényi $G(n,p)$ 模型，边概率 $p = 0.12$，无自环，无向，并为可复现性设置一个固定的伪随机数种子，该种子与用于自助法的全局种子相同。对于 $i  j$，每条边 $\\{i,j\\}$ 以概率 $p$ 独立存在。\n- 测试用例 $4$（小型边界情况）：\n  - 节点：从 $0$ 到 $3$ 的整数（含），因此 $n = 4$。\n  - 边：一条路径 $\\{(0,1),(1,2),(2,3)\\}$。\n\n参数设置：\n- 对于标准自助法和 $m$-out-of-$n$ 自助法，均使用 $B = 200$ 次自助法重抽样。\n- 对于 $m_b = \\lfloor m^\\alpha \\rfloor$，使用 $\\alpha = 0.9$。\n- 为所有随机性（Erdős–Rényi图的生成和自助法）使用一个固定的伪随机数种子，等于 $2025$。\n- 对于所有计算，将每条无向边 $\\{u,v\\}$ 视为贡献一个单独的对 $(d(u), d(v))$，无顺序之分；在实践中，基于存储的方向 $(u,v)$ 进行计算，但不要复制边。\n\n输出规范：\n- 对于每个测试用例 $t \\in \\{1,2,3,4\\}$，计算点估计 $\\hat r$、标准自助法 $95\\%$ 水平的百分位数置信区间下限和上限 $(L, U)$，以及 $m$-out-of-$n$ 自助法 $95\\%$ 水平的置信区间下限和上限 $(L_m, U_m)$。\n- 将每个报告的数值四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，包含四个测试用例的结果，形式为列表的列表，顺序如下：\n  - 测试用例 1: $[\\hat r, L, U, L_m, U_m]$\n  - 测试用例 2: $[\\hat r, L, U, L_m, U_m]$\n  - 测试用例 3: $[\\hat r, L, U, L_m, U_m]$\n  - 测试用例 4: $[\\hat r, L, U, L_m, U_m]$\n- 最后一行必须是这四个列表的单一 JSON 格式的列表，没有空格，例如：$[[r_1,L_1,U_1,L_{m,1},U_{m,1}],[r_2,L_2,U_2,L_{m,2},U_{m,2}],[r_3,L_3,U_3,L_{m,3},U_{m,3}],[r_4,L_4,U_4,L_{m,4},U_{m,4}]]$。\n\n要求在您的解决方案中包含的论证：\n- 提供一个基于原理的推导，证明 $r$ 是边级经验矩的光滑泛函，并论证为何在适当的稀疏性和有界度增长条件下，尽管存在由共享节点引起的边依赖性，基于边的非参数自助法在渐近上仍然有效。解释为什么 $m$-out-of-$n$ 自助法能进一步减少对依赖性的敏感度以及有限样本中重尾度效应的影响。\n\n角度和物理单位：\n- 此问题不涉及物理单位或角度。",
            "solution": "量化网络同配性并为同配系数构建置信区间的问题是计算网络科学和统计学中一个适定问题（well-posed problem）。所有参数、定义和程序都得到了足够严格的规定，从而能够得到一个唯一且可验证的解。\n\n### 理论基础与论证\n\n该问题要求计算同配系数 $r$，并使用两种自助法构建置信区间。我们首先建立这些方法的理论基础。\n\n**1. 作为光滑泛函的同配系数**\n\n设一个简单无向图为 $G=(V, E)$，其中有 $m = |E|$ 条边。对于每条边 $e_k = \\{u_k, v_k\\} \\in E$（其中 $k=1, \\dots, m$），我们构成一个度对 $(d(u_k), d(v_k))$。这定义了一个包含 $m$ 个对的经验二元分布。我们将这些对表示为 $(X_k, Y_k) = (d(u_k), d(v_k))$。同配系数 $r$ 被定义为这个经验样本的皮尔逊相关系数：\n\n$$ r = \\frac{\\sum_{k=1}^m (X_k - \\bar{X})(Y_k - \\bar{Y})}{\\sqrt{\\sum_{k=1}^m (X_k - \\bar{X})^2 \\sum_{k=1}^m (Y_k - \\bar{Y})^2}} $$\n\n其中 $\\bar{X} = \\frac{1}{m}\\sum_{k=1}^m X_k$ 且 $\\bar{Y} = \\frac{1}{m}\\sum_{k=1}^m Y_k$。这可以表示为 $(X, Y)$ 在边上的分布的经验矩的形式：\n\n$$ r = f(E[X], E[Y], E[X^2], E[Y^2], E[XY]) = \\frac{E[XY] - E[X]E[Y]}{\\sqrt{(E[X^2] - (E[X])^2)(E[Y^2] - (E[Y])^2)}} $$\n\n此处，$E[\\cdot]$ 表示关于 $m$ 条边的经验测度的期望，例如，$E[XY] = \\frac{1}{m}\\sum_{k=1}^m X_k Y_k$。函数 $f$ 涉及基本算术运算（加法、乘法、除法）和平方根。这个函数是连续可微的，因此是“光滑”的，除了在分母（方差）为零的地方。问题陈述为这种情况提供了一个约定，即如果任一方差为零，则定义 $r=0$，从而确保该统计量总是有定义的。因为 $r$ 是边-度对经验分布的光滑泛函，所以它适合通过重抽样方法（如自助法）进行分析。\n\n**2. 非参数边自助法的渐近有效性**\n\n标准的非参数自助法通过从观测样本中有放回地重抽样来近似统计量的抽样分布。其有效性依赖于原始观测值是独立同分布（i.i.d.）的假设。在图的背景下，观测单元是边。然而，边并非严格独立。一条边 $\\{u, v\\}$ 和一条边 $\\{u, w\\}$ 是相关的，因为它们共享一个节点 $u$，并且度 $d(u)$ 是它们各自对应度对 $(d(u), d(v))$ 和 $(d(u), d(w))$ 的一个组成部分。\n\n尽管存在这种局部依赖性，在适当的条件下，基于边的自助法是渐近有效的，这些条件通常在大型稀疏网络中得到满足。核心论点是，随着网络规模 $n \\to \\infty$，相关边对的比例会减小。共享一个节点的边对数量为 $\\sum_{v \\in V} \\binom{d(v)}{2}$。总的边对数量为 $\\binom{m}{2}$。对于许多类型的稀疏图（其中 $m = O(n)$ 且度分布的二阶矩 $\\langle d^2 \\rangle$ 是有限的），相关对与总对数的比率（与 $(\\sum_v d(v)^2) / m^2$ 成正比）会随着 $n \\to \\infty$ 而趋于零。因此，边的集合在渐近意义上表现得好像它们是一个独立同分布的样本，这为使用标准自助法近似 $r$ 的抽样分布提供了理由。\n\n**3. $m$-out-of-$n$自助法的作用**\n\n$m$-out-of-$n$自助法是一种更稳健的自助法变体，它重抽样一个较小的观测数量 $m_b  m$（此处为 $m_b=\\lfloor m^\\alpha \\rfloor$）。已知它在比标准自助法更弱的条件下，能为抽样分布提供一致的估计。它在这里的效用是双重的：\n\n*   **减轻依赖性**：通过抽取一个规模为 $m_b$ 的较小重抽样样本，包含多个相关边（即共享节点的边）的概率相较于一个规模为 $m$ 的完整重抽样样本要低。这使得每个自助法重抽样样本“看起来”更像一个独立同分布的样本，从而可能产生对真实抽样分布更准确的近似，尤其是在渐近论证可能不完全成立的有限样本中。\n*   **处理非标准极限分布**：同配系数的抽样分布可能是偏斜和非正态的，特别是在具有重尾度分布的网络中。标准自助法在这种情况下可能表现不佳。$m$-out-of-$n$自助法已被证明对更广泛的问题类别是一致的，包括那些在适当缩放后，统计量不收敛于正态分布的问题。使用增长速度慢于 $m$ 的 $m_b$ 可以纠正收敛速度，并提供更可靠的置信区间。\n\n总之，该问题将标准和高级统计技术正确地应用于一个明确定义的网络科学问题。引入 $m$-out-of-$n$ 自助法是一种复杂的改进，用以解决有限图样本中已知但常被忽略的边依赖问题。\n\n### 求解算法\n\n求解过程首先将每个图构建为边列表。对于每个图，我们执行以下步骤：\n\n1.  **度计算**：遍历边列表以计算图中每个节点的度。将这些度存储在字典或数组中以便高效查找。\n2.  **点估计 $\\hat{r}$**：构建两个列表 `deg_u` 和 `deg_v`，分别包含每条边的第一个和第二个节点的度。实现一个函数 `calculate_r` 来计算这两个列表之间的皮尔逊相关系数。该函数明确检查任一列表的方差是否为零；若是，则按规定返回 $0.0$。否则，它使用 `numpy.corrcoef`。\n3.  **标准自助法**：\n    a. 设置重抽样次数 $B=200$。\n    b. 在一个 $B$ 次迭代的循环中，通过从 $\\{0, 1, \\dots, m-1\\}$ 中有放回地抽样 $m$ 次来生成一组索引。\n    c. 使用这些索引来创建度对的自助法重抽样样本。\n    d. 使用 `calculate_r` 函数为该重抽样样本计算同配系数 $r^\\star$。\n    e. 收集所有 $B$ 个自助法副本 $r^\\star$。\n    f. 通过计算所收集副本的第 $2.5$ 和第 $97.5$ 百分位数，找到 $95\\%$ 的百分位数置信区间 $(L, U)$。\n4.  **$m$-out-of-$n$ 自助法**：\n    a. 计算重抽样大小 $m_b = \\lfloor m^{0.9} \\rfloor$。\n    b. 该过程与标准自助法相同，只是重抽样大小为 $m_b$ 而不是 $m$。\n    c. 这将产生置信区间 $(L_m, U_m)$。\n5.  **可复现性**：使用一个以 $2025$ 为种子的伪随机数生成器来执行所有随机操作，包括生成 Erdős–Rényi 图和所有自助法重抽样，以确保结果的完全可复现性。\n6.  **输出格式化**：将每个测试用例计算出的值 $(\\hat{r}, L, U, L_m, U_m)$ 四舍五入到六位小数，并格式化为指定的类 JSON 字符串。\n\n这个系统化的过程被应用于四个测试用例中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to solve the assortativity problem for all test cases.\n    \"\"\"\n    # Global parameters\n    B = 200\n    ALPHA = 0.9\n    SEED = 2025\n    \n    # Initialize a single random number generator for all operations\n    rng = np.random.default_rng(SEED)\n\n    def calculate_r(deg_arr_1, deg_arr_2):\n        \"\"\"\n        Computes the Pearson correlation coefficient for two arrays of degrees.\n        Returns 0 if the variance of either array is zero, as specified.\n        \"\"\"\n        # Ensure input are numpy arrays\n        d1 = np.array(deg_arr_1)\n        d2 = np.array(deg_arr_2)\n\n        # Handle trivial case for variance calculation\n        if len(d1)  2:\n            return 0.0\n\n        # Check for zero variance as per problem statement\n        if np.var(d1) == 0 or np.var(d2) == 0:\n            return 0.0\n        \n        # Calculate Pearson correlation coefficient\n        # np.corrcoef returns a 2x2 matrix\n        r = np.corrcoef(d1, d2)[0, 1]\n        \n        # Handle potential NaN from np.corrcoef in edge cases\n        return 0.0 if np.isnan(r) else r\n\n    def compute_all_stats(edges, internal_rng):\n        \"\"\"\n        Computes all required statistics for a given graph's edge list.\n        (r_hat, standard CI, m-out-of-n CI)\n        \"\"\"\n        if not edges:\n            return [0.0, 0.0, 0.0, 0.0, 0.0]\n\n        m = len(edges)\n        \n        # 1. Degree calculation\n        nodes = set()\n        for u, v in edges:\n            nodes.add(u)\n            nodes.add(v)\n        \n        max_node = max(nodes) if nodes else -1\n        degrees = np.zeros(max_node + 1, dtype=int)\n        for u, v in edges:\n            degrees[u] += 1\n            degrees[v] += 1\n\n        # 2. Create degree-pair lists\n        deg_u = np.array([degrees[u] for u, v in edges])\n        deg_v = np.array([degrees[v] for u, v in edges])\n\n        # 3. Point estimate\n        r_hat = calculate_r(deg_u, deg_v)\n\n        # 4. Standard bootstrap\n        r_bootstrap_replicates = []\n        for _ in range(B):\n            indices = internal_rng.choice(m, size=m, replace=True)\n            d1_star = deg_u[indices]\n            d2_star = deg_v[indices]\n            r_star = calculate_r(d1_star, d2_star)\n            r_bootstrap_replicates.append(r_star)\n        \n        lower_bound = np.percentile(r_bootstrap_replicates, 2.5)\n        upper_bound = np.percentile(r_bootstrap_replicates, 97.5)\n\n        # 5. m-out-of-n bootstrap\n        m_b = int(np.floor(m**ALPHA))\n        r_m_out_of_n_replicates = []\n        if m_b > 0:\n            for _ in range(B):\n                indices = internal_rng.choice(m, size=m_b, replace=True)\n                d1_star = deg_u[indices]\n                d2_star = deg_v[indices]\n                r_star = calculate_r(d1_star, d2_star)\n                r_m_out_of_n_replicates.append(r_star)\n        \n        # Handle case where m_b=0 for tiny graphs\n        if r_m_out_of_n_replicates:\n            lower_bound_m = np.percentile(r_m_out_of_n_replicates, 2.5)\n            upper_bound_m = np.percentile(r_m_out_of_n_replicates, 97.5)\n        else:\n            lower_bound_m, upper_bound_m = 0.0, 0.0\n\n        return [r_hat, lower_bound, upper_bound, lower_bound_m, upper_bound_m]\n\n    # --- Test Cases ---\n\n    # Case 1: Assortative structure\n    nodes_c1 = list(range(16))\n    edges_c1 = []\n    # Clique on {0,...,5}\n    edges_c1.extend(itertools.combinations(range(6), 2))\n    # Ring on {6,...,15}\n    edges_c1.extend([(i, i + 1) for i in range(6, 15)])\n    edges_c1.append((15, 6))\n    # Cross edges\n    edges_c1.extend([(2, 6), (3, 10)])\n\n    # Case 2: Disassortative (Star)\n    nodes_c2 = list(range(13))\n    edges_c2 = [(0, i) for i in range(1, 13)]\n\n    # Case 3: Erdős–Rényi graph\n    n_c3, p_c3 = 25, 0.12\n    edges_c3 = []\n    for i in range(n_c3):\n        for j in range(i + 1, n_c3):\n            if rng.random()  p_c3:\n                edges_c3.append((i, j))\n\n    # Case 4: Path graph\n    nodes_c4 = list(range(4))\n    edges_c4 = [(0, 1), (1, 2), (2, 3)]\n\n    test_cases = [edges_c1, edges_c2, edges_c4]\n    \n    results = []\n    for edges in test_cases:\n        stats = compute_all_stats(edges, rng)\n        # Format to 6 decimal places as strings\n        formatted_stats = [f\"{s:.6f}\" for s in stats]\n        results.append(formatted_stats)\n\n    # Final print statement\n    output_str = \",\".join([f\"[{','.join(res)}]\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}