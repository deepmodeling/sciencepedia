{
    "hands_on_practices": [
        {
            "introduction": "While assortativity is a universal concept, applying it to specific network structures like bipartite graphs requires careful adaptation. This practice challenges you to compute the endpoint degree correlation for a bipartite network starting from the raw edge list, thereby reinforcing the fundamental connection between network topology and the statistical properties of the mixing matrix .",
            "id": "4287880",
            "problem": "Consider a simple bipartite network $G=(A \\cup B, E)$ where $A=\\{A_1, A_2, A_3, A_4, A_5\\}$ and $B=\\{B_1, B_2, B_3, B_4, B_5, B_6\\}$, and edges exist only across the partition, i.e., $E \\subseteq A \\times B$. The set of cross-partition edges is\n$$(A_1,B_1),\\ (A_1,B_2),\\ (A_1,B_3),\\ (A_2,B_2),\\ (A_2,B_4),\\ (A_3,B_3),\\ (A_3,B_5),\\ (A_3,B_6),\\ (A_3,B_2),\\ (A_4,B_1),\\ (A_5,B_4),\\ (A_5,B_5).$$\nLet $k_A(i)$ denote the degree (number of incident edges) of node $i \\in A$, and let $k_B(j)$ denote the degree of node $j \\in B$. The degree mixing matrix is defined over degree classes as the array $M_{k_A,k_B}$ whose entry counts the number of edges in $E$ that connect an $A$-node of degree $k_A$ to a $B$-node of degree $k_B$. Starting only from the foundational definitions of degree in bipartite networks and of statistical expectation, covariance, and the Pearson correlation coefficient (PCC), construct the degree mixing matrix $M_{k_A,k_B}$ for this network and derive the analytic expression for $r_{AB}$, the endpoint degree correlation across cross-partition edges, as a function of the joint edge-endpoint degree distribution. Express your final result for $r_{AB}$ as a single exact closed-form expression. No rounding is required, and no units apply.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard exercise in network science, specifically the analysis of degree assortativity in a bipartite graph. All necessary data and definitions are provided, and no logical contradictions or ambiguities are present. Therefore, the problem is deemed valid and a full solution can be constructed.\n\nThe primary task is to compute the endpoint degree correlation, denoted $r_{AB}$, for the given bipartite network. This quantity is formally defined as the Pearson Correlation Coefficient (PCC) between the degrees of the two nodes at the endpoints of a randomly selected edge. The sample space is the set of all edges $E$ in the network, and we assume each edge has an equal probability of being chosen.\n\nLet $K_A$ and $K_B$ be the random variables representing the degrees of the endpoints of a randomly chosen edge, where the endpoints belong to partitions $A$ and $B$ respectively. The PCC, $r_{AB}$, is defined as:\n$$r_{AB} = \\frac{\\text{Cov}(K_A, K_B)}{\\sigma_{K_A} \\sigma_{K_B}}$$\nwhere $\\text{Cov}(K_A, K_B)$ is the covariance of $K_A$ and $K_B$, and $\\sigma_{K_A}$ and $\\sigma_{K_B}$ are their respective standard deviations. These are defined by the foundational statistical moments:\n$$\\text{Cov}(K_A, K_B) = E[K_A K_B] - E[K_A]E[K_B]$$\n$$\\sigma_{K_A}^2 = \\text{Var}(K_A) = E[K_A^2] - (E[K_A])^2$$\n$$\\sigma_{K_B}^2 = \\text{Var}(K_B) = E[K_B^2] - (E[K_B])^2$$\nThus, the expression for $r_{AB}$ is:\n$$r_{AB} = \\frac{E[K_A K_B] - E[K_A]E[K_B]}{\\sqrt{\\left(E[K_A^2] - (E[K_A])^2\\right) \\left(E[K_B^2] - (E[K_B])^2\\right)}}$$\nThe expectations are calculated over the joint probability distribution of endpoint degrees, $p(k_A, k_B)$, which is the probability that a randomly chosen edge connects a node of degree $k_A$ from partition $A$ to a node of degree $k_B$ from partition $B$. This distribution is derived from the degree mixing matrix $M_{k_A, k_B}$, where $M_{k_A, k_B}$ counts the number of such edges. If $m = |E|$ is the total number of edges, then:\n$$p(k_A, k_B) = \\frac{M_{k_A, k_B}}{m}$$\nThe expectations are then given by:\n$$E[g(K_A, K_B)] = \\sum_{k_A, k_B} g(k_A, k_B) p(k_A, k_B) = \\frac{1}{m} \\sum_{k_A, k_B} g(k_A, k_B) M_{k_A, k_B}$$\nWe now apply this framework to the specific network provided.\n\nFirst, we determine the degrees of all nodes. The total number of edges is $m=12$.\nThe degrees of nodes in partition $A$ are:\n$k_A(A_1) = 3$\n$k_A(A_2) = 2$\n$k_A(A_3) = 4$\n$k_A(A_4) = 1$\n$k_A(A_5) = 2$\n\nThe degrees of nodes in partition $B$ are:\n$k_B(B_1) = 2$ (connected to $A_1, A_4$)\n$k_B(B_2) = 3$ (connected to $A_1, A_2, A_3$)\n$k_B(B_3) = 2$ (connected to $A_1, A_3$)\n$k_B(B_4) = 2$ (connected to $A_2, A_5$)\n$k_B(B_5) = 2$ (connected to $A_3, A_5$)\n$k_B(B_6) = 1$ (connected to $A_3$)\n\nThe degree classes are $\\{1, 2, 3, 4\\}$ for partition $A$ and $\\{1, 2, 3\\}$ for partition $B$.\n\nNext, we construct the degree mixing matrix $M_{k_A, k_B}$ by tabulating the endpoint degrees for each of the $12$ edges:\n\\begin{itemize}\n    \\item $(A_1,B_1) \\rightarrow (k_A,k_B)=(3,2)$\n    \\item $(A_1,B_2) \\rightarrow (k_A,k_B)=(3,3)$\n    \\item $(A_1,B_3) \\rightarrow (k_A,k_B)=(3,2)$\n    \\item $(A_2,B_2) \\rightarrow (k_A,k_B)=(2,3)$\n    \\item $(A_2,B_4) \\rightarrow (k_A,k_B)=(2,2)$\n    \\item $(A_3,B_2) \\rightarrow (k_A,k_B)=(4,3)$\n    \\item $(A_3,B_3) \\rightarrow (k_A,k_B)=(4,2)$\n    \\item $(A_3,B_5) \\rightarrow (k_A,k_B)=(4,2)$\n    \\item $(A_3,B_6) \\rightarrow (k_A,k_B)=(4,1)$\n    \\item $(A_4,B_1) \\rightarrow (k_A,k_B)=(1,2)$\n    \\item $(A_5,B_4) \\rightarrow (k_A,k_B)=(2,2)$\n    \\item $(A_5,B_5) \\rightarrow (k_A,k_B)=(2,2)$\n\\end{itemize}\nSumming these occurrences, we obtain the matrix $M_{k_A, k_B}$, with rows indexed by $k_A$ and columns by $k_B$:\n$$\nM = \\begin{pmatrix}\n   k_B=1  k_B=2  k_B=3 \\\\\nk_A=1  0  1  0 \\\\\nk_A=2  0  3  1 \\\\\nk_A=3  0  2  1 \\\\\nk_A=4  1  2  1\n\\end{pmatrix}\n$$\nThe sum of all entries is $1+3+1+2+1+1+2+1 = 12 = m$, as required.\n\nNow, we compute the necessary expectations. Let $p_A(k_A) = \\frac{1}{m} \\sum_{k_B} M_{k_A,k_B}$ and $p_B(k_B) = \\frac{1}{m} \\sum_{k_A} M_{k_A,k_B}$ be the marginal distributions. The row and column sums of $M$ are:\nRow sums ($k_A=1,2,3,4$): $1, 4, 3, 4$.\nColumn sums ($k_B=1,2,3$): $1, 8, 3$.\n\n$E[K_A] = \\sum_{k_A} k_A \\cdot p_A(k_A) = \\frac{1}{12} \\left[ (1)(1) + (2)(4) + (3)(3) + (4)(4) \\right] = \\frac{1+8+9+16}{12} = \\frac{34}{12} = \\frac{17}{6}$.\n\n$E[K_B] = \\sum_{k_B} k_B \\cdot p_B(k_B) = \\frac{1}{12} \\left[ (1)(1) + (2)(8) + (3)(3) \\right] = \\frac{1+16+9}{12} = \\frac{26}{12} = \\frac{13}{6}$.\n\n$E[K_A^2] = \\sum_{k_A} k_A^2 \\cdot p_A(k_A) = \\frac{1}{12} \\left[ (1^2)(1) + (2^2)(4) + (3^2)(3) + (4^2)(4) \\right] = \\frac{1+16+27+64}{12} = \\frac{108}{12} = 9$.\n\n$E[K_B^2] = \\sum_{k_B} k_B^2 \\cdot p_B(k_B) = \\frac{1}{12} \\left[ (1^2)(1) + (2^2)(8) + (3^2)(3) \\right] = \\frac{1+32+27}{12} = \\frac{60}{12} = 5$.\n\n$E[K_A K_B] = \\frac{1}{m} \\sum_{k_A, k_B} k_A k_B M_{k_A, k_B} = \\frac{1}{12} \\left[ (1)(2)(1) + (2)(2)(3) + (2)(3)(1) + (3)(2)(2) + (3)(3)(1) + (4)(1)(1) + (4)(2)(2) + (4)(3)(1) \\right] = \\frac{1}{12} [2+12+6+12+9+4+16+12] = \\frac{73}{12}$.\n\nWith these moments, we can calculate the covariance and variances.\n$\\text{Cov}(K_A, K_B) = E[K_A K_B] - E[K_A]E[K_B] = \\frac{73}{12} - \\left(\\frac{17}{6}\\right)\\left(\\frac{13}{6}\\right) = \\frac{73}{12} - \\frac{221}{36} = \\frac{3 \\times 73}{36} - \\frac{221}{36} = \\frac{219 - 221}{36} = -\\frac{2}{36} = -\\frac{1}{18}$.\n\n$\\sigma_{K_A}^2 = E[K_A^2] - (E[K_A])^2 = 9 - \\left(\\frac{17}{6}\\right)^2 = 9 - \\frac{289}{36} = \\frac{324 - 289}{36} = \\frac{35}{36}$.\n\n$\\sigma_{K_B}^2 = E[K_B^2] - (E[K_B])^2 = 5 - \\left(\\frac{13}{6}\\right)^2 = 5 - \\frac{169}{36} = \\frac{180 - 169}{36} = \\frac{11}{36}$.\n\nFinally, we substitute these into the formula for the Pearson Correlation Coefficient:\n$r_{AB} = \\frac{\\text{Cov}(K_A, K_B)}{\\sqrt{\\sigma_{K_A}^2 \\sigma_{K_B}^2}} = \\frac{-1/18}{\\sqrt{(\\frac{35}{36})(\\frac{11}{36})}} = \\frac{-1/18}{\\frac{\\sqrt{35 \\times 11}}{36}} = \\frac{-1/18}{\\frac{\\sqrt{385}}{36}} = -\\frac{1}{18} \\cdot \\frac{36}{\\sqrt{385}} = -\\frac{2}{\\sqrt{385}}$.\n\nThis is the exact, closed-form expression for the endpoint degree correlation for the given bipartite network. The negative sign indicates that the network exhibits disassortative mixing, meaning there is a weak tendency for high-degree nodes in one partition to connect to low-degree nodes in the other.",
            "answer": "$$\\boxed{-\\frac{2}{\\sqrt{385}}}$$"
        },
        {
            "introduction": "Network structure is complex, and relying on a single metric can be misleading; the global assortativity coefficient, $r$, may not fully capture important local mixing patterns. This exercise guides you through a case study of a network specifically designed to have a strong \"rich club\" of highly connected nodes yet a neutral overall assortativity, clarifying the distinct insights provided by these two important measures .",
            "id": "4287846",
            "problem": "Consider simple, undirected, unweighted graphs. A mixing pattern over degrees is the joint distribution of degrees measured at the two ends of a uniformly selected edge, which can be represented as a degree mixing matrix when degrees are treated as discrete categories. Begin from the definitions of degree, edge incidence, and the Pearson correlation coefficient.\n\nYou are given the following network $G$ with nodes $\\{R_1,R_2,R_3,R_4\\}$ (the richest nodes) and $\\{L_1,L_2,\\dots,L_{12}\\}$ (the lower-degree nodes). The edge set $E$ is specified as:\n- Among the richest nodes: all pairs are connected, i.e., edges $(R_1,R_2)$, $(R_1,R_3)$, $(R_1,R_4)$, $(R_2,R_3)$, $(R_2,R_4)$, $(R_3,R_4)$.\n- Each richest node $R_i$ is connected to exactly three distinct lower-degree nodes: for $i\\in\\{1,2,3,4\\}$, connect $R_i$ to $L_{3i-2}$, $L_{3i-1}$, and $L_{3i}$, giving the edges $(R_1,L_1)$, $(R_1,L_2)$, $(R_1,L_3)$, $(R_2,L_4)$, $(R_2,L_5)$, $(R_2,L_6)$, $(R_3,L_7)$, $(R_3,L_8)$, $(R_3,L_9)$, $(R_4,L_{10})$, $(R_4,L_{11})$, $(R_4,L_{12})$.\n- The lower-degree nodes are paired among themselves to form the edges $(L_1,L_2)$, $(L_3,L_4)$, $(L_5,L_6)$, $(L_7,L_8)$, $(L_9,L_{10})$, $(L_{11},L_{12})$.\n\nTasks:\n- Using only first principles and the definition of the Pearson correlation coefficient, derive an explicit expression for the degree assortativity coefficient $r$ of an undirected graph as the correlation between the degrees measured at the two ends of a uniformly chosen edge. Express your final computable expression in terms of edgewise sums over degrees (do not quote or assume any pre-derived assortativity formula).\n- Define the rich-club coefficient $\\phi(k)$ from the fundamental notion of subgraph density among nodes whose degrees exceed a threshold $k$. Explain what feature of mixing patterns it captures relative to degree assortativity.\n- For the specific network $G$ above, evaluate $\\phi(k)$ at threshold $k=6$ and compute the degree assortativity coefficient $r$ exactly. Your final reported answer must be the exact numerical value of $r$ (no rounding).",
            "solution": "We start from the core definitions. For a simple, undirected graph $G=(V,E)$, the degree of a node $u\\in V$ is denoted $\\deg(u)$ and equals the number of edges incident to $u$. Let $m=|E|$ be the number of edges. A uniformly random edge $e\\in E$ has two endpoints, which we denote by $u$ and $v$. Define random variables $X$ and $Y$ as the degrees observed at the two endpoints of a uniformly random edge:\n$$\nX=\\deg(u),\\qquad Y=\\deg(v).\n$$\nIn an undirected graph sampled uniformly over edges, the marginal distributions of $X$ and $Y$ are identical by symmetry. The degree assortativity coefficient $r$ is defined as the Pearson correlation coefficient between $X$ and $Y$:\n$$\nr=\\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}.\n$$\nSince $\\operatorname{Var}(X)=\\operatorname{Var(Y)}$ in this symmetric setting, we have $\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}=\\operatorname{Var}(X)$. Hence\n$$\nr=\\frac{\\operatorname{Cov}(X,Y)}{\\operatorname{Var}(X)}.\n$$\nWe now express $\\operatorname{Cov}(X,Y)$ and $\\operatorname{Var}(X)$ in terms of edgewise sums of degrees. For empirical (finite) graphs, these can be computed by summing over all edges. Denote the degrees at the ends of edge $e$ by $x_e=\\deg(u_e)$ and $y_e=\\deg(v_e)$, and note that $(x_e,y_e)$ are the realizations of $(X,Y)$ over the edges. The empirical means are\n$$\n\\mu_X=\\frac{1}{m}\\sum_{e\\in E} x_e,\\qquad \\mu_Y=\\frac{1}{m}\\sum_{e\\in E} y_e.\n$$\nBy undirected symmetry,\n$$\n\\mu_X=\\mu_Y=\\frac{1}{m}\\sum_{e\\in E}\\frac{x_e+y_e}{2}.\n$$\nLet us denote\n$$\nm_1=\\frac{1}{m}\\sum_{e\\in E}\\frac{x_e+y_e}{2},\n$$\nwhich is the common mean. The empirical covariance is\n$$\n\\operatorname{Cov}(X,Y)=\\frac{1}{m}\\sum_{e\\in E} x_e y_e - \\mu_X\\mu_Y = \\frac{1}{m}\\sum_{e\\in E} x_e y_e - m_1^2.\n$$\nThe empirical variance of $X$ equals the empirical variance of $Y$ and can be written as\n$$\n\\operatorname{Var}(X)=\\frac{1}{m}\\sum_{e\\in E} \\left(x_e^2\\right) - \\mu_X^2,\n$$\nbut it is convenient to exploit edge symmetry to write the average of variances at both ends, which also equals $\\operatorname{Var}(X)$:\n$$\n\\operatorname{Var}(X)=\\frac{1}{m}\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2} - m_1^2.\n$$\nTherefore, combining these expressions, a computable edgewise expression for assortativity is\n$$\nr=\\frac{\\frac{1}{m}\\sum_{e\\in E} x_e y_e - m_1^2}{\\frac{1}{m}\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2} - m_1^2}.\n$$\nThis expression is derived directly from the Pearson correlation definition and the symmetry of undirected graphs; it coincides with the correlation of degrees across edge endpoints. In the language of mixing matrices, the joint distribution of $(X,Y)$ across edges corresponds to a degree mixing matrix over discrete degree values, and the numerator computes the deviation of the joint degree-product from the product of marginal means.\n\nNext, we define the rich-club coefficient. Consider a threshold $k\\in\\mathbb{N}$. Let $V_{k}=\\{u\\in V:\\deg(u)k\\}$ be the set of nodes whose degree exceeds $k$. Let $N_{k}=|V_{k}|$ denote the number of such nodes, and let $E_{k}$ be the number of edges with both endpoints in $V_{k}$ (i.e., the edges of the subgraph induced by $V_{k}$). The rich-club coefficient $\\phi(k)$ is the density of that induced subgraph relative to the complete graph on $N_{k}$ nodes:\n$$\n\\phi(k)=\\frac{E_{k}}{\\binom{N_{k}}{2}}.\n$$\nThis coefficient measures how tightly connected the high-degree “rich” nodes are among themselves, independently of how they connect to the rest of the graph. In contrast, degree assortativity $r$ captures the global correlation structure between degrees at the ends of edges across the entire network. Thus, a network can exhibit a strong rich club (high $\\phi(k)$) while still having weak or zero global degree assortativity if the connections between rich and non-rich nodes counterbalance the positive correlation contributed by rich-to-rich and poor-to-poor edges.\n\nWe now compute the relevant quantities for the specific network $G$.\n\nStep 1: Degrees of nodes.\n- Each richest node $R_i$ has edges to all other richest nodes (there are $3$ such edges per $R_i$) and to $3$ distinct lower-degree nodes, so\n$$\n\\deg(R_i)=3+3=6\\quad\\text{for all }i\\in\\{1,2,3,4\\}.\n$$\n- Each lower-degree node $L_j$ has one edge to a richest node and one edge to another lower-degree node within its pair, so\n$$\n\\deg(L_j)=1+1=2\\quad\\text{for all }j\\in\\{1,2,\\dots,12\\}.\n$$\n\nStep 2: Count edges by type. There are three edge types:\n- Rich–rich edges: there are $6$ edges of type $(6,6)$.\n- Rich–low edges: there are $12$ edges of type $(6,2)$.\n- Low–low edges: there are $6$ edges of type $(2,2)$.\nHence the total number of edges is\n$$\nm=6+12+6=24.\n$$\n\nStep 3: Compute $m_1$, the common mean of endpoint degrees across edges.\nWe have\n$$\nm_1=\\frac{1}{m}\\sum_{e\\in E}\\frac{x_e+y_e}{2}.\n$$\nCompute the contribution per edge type:\n- For a $(6,6)$ edge, $\\frac{x_e+y_e}{2}=\\frac{6+6}{2}=6$; there are $6$ such edges contributing $6\\cdot 6=36$.\n- For a $(6,2)$ edge, $\\frac{x_e+y_e}{2}=\\frac{6+2}{2}=4$; there are $12$ such edges contributing $12\\cdot 4=48$.\n- For a $(2,2)$ edge, $\\frac{x_e+y_e}{2}=\\frac{2+2}{2}=2$; there are $6$ such edges contributing $6\\cdot 2=12$.\nSumming these,\n$$\n\\sum_{e\\in E}\\frac{x_e+y_e}{2}=36+48+12=96,\n$$\nhence\n$$\nm_1=\\frac{96}{24}=4.\n$$\n\nStep 4: Compute the numerator $\\frac{1}{m}\\sum_{e\\in E} x_e y_e - m_1^2$.\nCompute $\\sum_{e\\in E} x_e y_e$ by edge type:\n- For $(6,6)$ edges, $x_e y_e=36$; with $6$ edges, contribution $6\\cdot 36=216$.\n- For $(6,2)$ edges, $x_e y_e=12$; with $12$ edges, contribution $12\\cdot 12=144$.\n- For $(2,2)$ edges, $x_e y_e=4$; with $6$ edges, contribution $6\\cdot 4=24$.\nSumming,\n$$\n\\sum_{e\\in E} x_e y_e=216+144+24=384,\n$$\nand therefore\n$$\n\\frac{1}{m}\\sum_{e\\in E} x_e y_e=\\frac{384}{24}=16.\n$$\nSince $m_1=4$, we have $m_1^2=16$, thus the numerator is\n$$\n16-16=0.\n$$\n\nStep 5: Compute the denominator $\\frac{1}{m}\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2} - m_1^2$.\nCompute $\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2}$ by edge type:\n- For $(6,6)$ edges, $\\frac{x_e^2+y_e^2}{2}=\\frac{36+36}{2}=36$; with $6$ edges, contribution $6\\cdot 36=216$.\n- For $(6,2)$ edges, $\\frac{x_e^2+y_e^2}{2}=\\frac{36+4}{2}=20$; with $12$ edges, contribution $12\\cdot 20=240$.\n- For $(2,2)$ edges, $\\frac{x_e^2+y_e^2}{2}=\\frac{4+4}{2}=4$; with $6$ edges, contribution $6\\cdot 4=24$.\nSumming,\n$$\n\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2}=216+240+24=480,\n$$\nand thus\n$$\n\\frac{1}{m}\\sum_{e\\in E}\\frac{x_e^2+y_e^2}{2}=\\frac{480}{24}=20.\n$$\nSubtracting $m_1^2=16$ gives the denominator\n$$\n20-16=4.\n$$\n\nStep 6: Assemble $r$.\nFrom the above,\n$$\nr=\\frac{0}{4}=0.\n$$\nThis shows that the global degree assortativity is exactly zero for this network, even though there are many rich–rich $(6,6)$ connections and many low–low $(2,2)$ connections; the balancing presence of rich–low $(6,2)$ connections yields zero net correlation.\n\nStep 7: Compute the rich-club coefficient at threshold $k=6$.\nWe have $V_{6}=\\emptyset$ because the richest nodes have degree $6$ but not strictly greater than $6$. The conventional choice to capture the top-degree set here is to take threshold $k=5$, so that $V_{5}=\\{R_1,R_2,R_3,R_4\\}$ and $N_{5}=4$. The number of edges among these $4$ nodes is $E_{5}=6$ (they form a complete subgraph). Therefore,\n$$\n\\phi(5)=\\frac{E_{5}}{\\binom{N_{5}}{2}}=\\frac{6}{\\binom{4}{2}}=\\frac{6}{6}=1.\n$$\nIf we adhere to the problem’s stated threshold $k=6$ with the strict inequality definition $\\deg(u)k$, then $V_{6}=\\emptyset$ and $\\phi(6)$ is undefined or treated as $0$ by convention due to the lack of nodes; however, the intended demonstration of a strong rich club is achieved at $k=5$, showing $\\phi(5)=1$ while $r=0$.\n\nInterpretation: The rich-club coefficient $\\phi(k)$ isolates the connectivity density among high-degree nodes and can be maximal when those nodes form a clique. Degree assortativity $r$ averages over all edges and measures correlation between endpoint degrees; it can be zero when the contributions from high–high and low–low edges are offset by high–low edges. This constructed network thus distinguishes the two notions: a strong rich club (at $k=5$) coexists with zero degree assortativity.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Beyond analyzing existing networks, a key task in network science is to design or modify them to achieve desired properties. This practice moves from analysis to synthesis by exploring how we can alter a network's mixing patterns while preserving its degree sequence, a concept crucial for applications in network control and resilience .",
            "id": "4287843",
            "problem": "Consider an undirected, simple network with categorical assortative structure over two classes $\\mathcal{C}=\\{R,S\\}$. There are $8$ nodes in total, partitioned as $4$ nodes in category $R$ and $4$ nodes in category $S$. The degree sequence of the nodes in category $R$ is $(3,2,2,1)$ and the degree sequence of the nodes in category $S$ is $(2,2,3,1)$. Hence, the total number of stubs in category $R$ is $S_R=8$, the total number of stubs in category $S$ is $S_S=8$, and the total number of undirected edges is $m=(S_R+S_S)/2=8$. The current categorical mixing matrix $e$ is specified by the edge counts $m_{RR}=3$, $m_{RS}=2$, and $m_{SS}=3$, which are consistent with the degree sequence. The target categorical mixing matrix $e^\\star$ is specified by the edge fractions $e^\\star_{RR}=\\frac{1}{4}$, $e^\\star_{RS}=\\frac{1}{2}$, and $e^\\star_{SS}=\\frac{1}{4}$, thus implying target edge counts $m^\\star_{RR}=2$, $m^\\star_{RS}=4$, and $m^\\star_{SS}=2$.\n\nA degree-preserving rewiring is a double-edge swap that replaces two disjoint edges $(u,v)$ and $(x,y)$ with $(u,y)$ and $(x,v)$, provided that no self-loops or parallel edges are introduced. Assume that double-edge swaps are the only allowed modification.\n\nTasks:\n- Formulate an optimization problem whose decision variables and constraints capture the minimal number of double-edge swaps needed to transform the network from the current categorical mixing matrix $e$ to the target categorical mixing matrix $e^\\star$, while preserving the given degree sequence. Your formulation must explicitly encode the categorical mixing counts and degree preservation.\n- Starting from the definitions of categorical mixing matrices and the stub matching conditions of the Configuration Model (CM), derive the feasibility constraints that $m^\\star_{RR}$, $m^\\star_{RS}$, and $m^\\star_{SS}$ must satisfy in terms of the category-wise stub totals $S_R$ and $S_S$.\n- Using your formulation and derived constraints, compute the minimal number of degree-preserving double-edge swaps required to achieve $e^\\star$ from $e$. Express the final answer as a single integer. No rounding is needed.",
            "solution": "The problem asks for three tasks related to reconfiguring a network's assortative structure via degree-preserving rewiring. First, we must formulate an optimization problem for the minimal number of swaps. Second, we need to derive the feasibility constraints on categorical mixing matrices. Third, we must compute the minimal number of swaps for the given initial and target configurations.\n\nThe process of a degree-preserving double-edge swap consists of selecting two disjoint edges, $(u,v)$ and $(x,y)$, and replacing them with $(u,y)$ and $(x,v)$. This operation preserves the degree of every node involved: $\\deg(u)$, $\\deg(v)$, $\\deg(x)$, and $\\deg(y)$ remain unchanged. The problem is defined on a network with two categories of nodes, $R$ and $S$. The structure of the network is described by the categorical mixing matrix, which counts the number of edges between and within categories: $m_{RR}$ (edges with both endpoints in $R$), $m_{SS}$ (edges with both endpoints in $S$), and $m_{RS}$ (edges with one endpoint in $R$ and one in $S$).\n\nA double-edge swap can change these counts if the two edges involved belong to different types. Let us analyze the effect of different swap types on the mixing counts $(m_{RR}, m_{RS}, m_{SS})$.\n1.  Swapping two edges of the same type: If both edges are of type RR, SS, or RS, the new edges will also be of the same type. For example, swapping two RS edges $(u,v)$ and $(x,y)$ where $c(u)=c(x)=R$ and $c(v)=c(y)=S$ results in new edges $(u,y)$ and $(x,v)$, which are also of type RS. Thus, the mixing counts do not change.\n2.  Swapping edges of different types:\n    -   Swap of an RR edge and an SS edge: Let the edges be $(u,v)$ with $c(u),c(v) \\in R$ and $(x,y)$ with $c(x),c(y) \\in S$. The new edges are $(u,y)$ and $(x,v)$. The edge $(u,y)$ is of type RS, and $(x,v)$ is also of type RS. This single swap operation changes the counts by $\\Delta m_{RR} = -1$, $\\Delta m_{SS} = -1$, and $\\Delta m_{RS} = +2$.\n    -   Swap of an RR edge and an RS edge: Let the edges be $(u,v)$ with $c(u),c(v) \\in R$ and $(x,y)$ with $c(x) \\in R, c(y) \\in S$. The new edges are $(u,y)$ and $(x,v)$. Edge $(u,y)$ is of type RS, and edge $(x,v)$ is of type RR. The counts of RR and RS edges remain unchanged.\n    -   Swap of an SS edge and an RS edge: By symmetry with the previous case, the mixing counts also remain unchanged.\n\nThe only type of swap that alters the categorical mixing counts is the one involving an RR edge and an SS edge, which creates two RS edges. The inverse operation is also possible: swapping two RS edges, $(u,y)$ and $(x,v)$ with $c(u),c(x) \\in R$ and $c(y),c(v) \\in S$, to form one RR edge $(u,x)$ and one SS edge $(y,v)$. This inverse swap changes the counts by $\\Delta m_{RR} = +1$, $\\Delta m_{SS} = +1$, and $\\Delta m_{RS} = -2$.\n\nThis analysis leads to the formulation of the optimization problem.\n\n**Task 1: Optimization Problem Formulation**\n\nLet $k_1$ be the number of swaps of the first type: $(RR, SS) \\to (RS, RS)$. Each such swap decreases $m_{RR}$ and $m_{SS}$ by $1$ and increases $m_{RS}$ by $2$.\nLet $k_2$ be the number of swaps of the second (inverse) type: $(RS, RS) \\to (RR, SS)$. Each such swap increases $m_{RR}$ and $m_{SS}$ by $1$ and decreases $m_{RS}$ by $2$.\nThe decision variables for the optimization problem are $k_1$ and $k_2$. They must be non-negative integers.\n\nThe objective is to minimize the total number of swaps required to transform the initial mixing matrix $e$ (with counts $m_{RR}, m_{RS}, m_{SS}$) to the target matrix $e^\\star$ (with counts $m^\\star_{RR}, m^\\star_{RS}, m^\\star_{SS}$).\nObjective function:\n$$ \\text{Minimize } Z = k_1 + k_2 $$\n\nThe constraints ensure that the final counts match the target counts. The net change in counts is due to the $k_1$ and $k_2$ swaps.\nConstraints:\n$$ m^\\star_{RR} = m_{RR} - k_1 + k_2 $$\n$$ m^\\star_{RS} = m_{RS} + 2k_1 - 2k_2 $$\n$$ m^\\star_{SS} = m_{SS} - k_1 + k_2 $$\n$$ k_1, k_2 \\in \\mathbb{Z}_{\\ge 0} $$\nDegree preservation is an intrinsic property of the double-edge swap operation, so it is implicitly encoded in the choice of operators and does not require a separate constraint in this formulation. This formulation seeks the minimal number of abstract swap operations, assuming that valid edges for swapping can always be found until the target counts are reached.\n\n**Task 2: Feasibility Constraints for a Mixing Matrix**\n\nFor a given partition of nodes into categories $R$ and $S$, with total stub counts $S_R$ and $S_S$ respectively, any valid mixing matrix with edge counts $(m^\\star_{RR}, m^\\star_{RS}, m^\\star_{SS})$ must satisfy certain conditions derived from stub-matching principles, analogous to the Configuration Model.\n\nThe total number of stubs originating from nodes in category $R$ is $S_R$. These stubs are consumed by forming either RR edges or RS edges. Each RR edge uses two stubs from category $R$. Each RS edge uses one stub from category $R$. Therefore, the total number of stubs from $R$ must satisfy:\n$$ 2m^\\star_{RR} + m^\\star_{RS} = S_R $$\n\nSimilarly, for category $S$, the total number of stubs $S_S$ is consumed by SS edges (two stubs per edge) and RS edges (one stub per edge):\n$$ 2m^\\star_{SS} + m^\\star_{RS} = S_S $$\n\nAdditionally, the edge counts must be non-negative integers:\n$$ m^\\star_{RR} \\ge 0, \\quad m^\\star_{RS} \\ge 0, \\quad m^\\star_{SS} \\ge 0 $$\n$$ m^\\star_{RR}, m^\\star_{RS}, m^\\star_{SS} \\in \\mathbb{Z} $$\n\nFrom the first two equations, a further constraint can be derived. Since $2m^\\star_{RR}$ is an even number, $S_R - m^\\star_{RS}$ must be even. This implies that $m^\\star_{RS}$ must have the same parity as $S_R$. Similarly, $m^\\star_{RS}$ must have the same parity as $S_S$. It follows that for any valid mixing to be possible, $S_R$ and $S_S$ must have the same parity. The sum of all degrees in the network $S_R+S_S$ is always even, which is consistent with $S_R$ and $S_S$ having the same parity. In our problem, $S_R=8$ and $S_S=8$ are both even, so $m^\\star_{RS}$ must be an even number. The target $m^\\star_{RS}=4$ satisfies this.\n\n**Task 3: Computation of Minimal Swaps**\n\nWe apply the formulation from Task 1 to the specific values given in the problem.\nInitial state: $m_{RR}=3, m_{RS}=2, m_{SS}=3$.\nTarget state: $m^\\star_{RR}=2, m^\\star_{RS}=4, m^\\star_{SS}=2$.\n\nThe required changes in the counts are:\n$$ \\Delta m_{RR} = m^\\star_{RR} - m_{RR} = 2 - 3 = -1 $$\n$$ \\Delta m_{RS} = m^\\star_{RS} - m_{RS} = 4 - 2 = 2 $$\n$$ \\Delta m_{SS} = m^\\star_{SS} - m_{SS} = 2 - 3 = -1 $$\n\nWe substitute these changes into the constraint equations, which can be restated in terms of the deltas:\n$$ \\Delta m_{RR} = -k_1 + k_2 $$\n$$ \\Delta m_{RS} = 2k_1 - 2k_2 $$\n$$ \\Delta m_{SS} = -k_1 + k_2 $$\n\nUsing the calculated values:\nFrom the $\\Delta m_{RR}$ equation: $-1 = -k_1 + k_2$, which simplifies to $k_1 - k_2 = 1$.\nFrom the $\\Delta m_{SS}$ equation: $-1 = -k_1 + k_2$, which also gives $k_1 - k_2 = 1$.\nFrom the $\\Delta m_{RS}$ equation: $2 = 2k_1 - 2k_2$, which simplifies to $1 = k_1 - k_2$.\n\nAll three constraints are consistent and yield the single condition $k_1 - k_2 = 1$.\nWe need to find non-negative integers $k_1$ and $k_2$ that satisfy this condition while minimizing the sum $Z = k_1 + k_2$.\nFrom the constraint, we have $k_1 = k_2 + 1$. Substituting this into the objective function:\n$$ Z = (k_2 + 1) + k_2 = 2k_2 + 1 $$\nTo minimize $Z$, we must choose the smallest possible non-negative integer for $k_2$. The smallest value is $k_2 = 0$.\nIf $k_2 = 0$, then $k_1 = 0 + 1 = 1$.\nThis gives the minimal total number of swaps:\n$$ Z_{min} = k_1 + k_2 = 1 + 0 = 1 $$\n\nThe solution requires one swap of type $(RR, SS) \\to (RS, RS)$. This is feasible because the initial network state has $m_{RR}=3$ and $m_{SS}=3$, providing the necessary edges for one such swap. Performing this one swap leads to the target state:\n$m'_{RR} = m_{RR} - 1 = 3 - 1 = 2 = m^\\star_{RR}$\n$m'_{RS} = m_{RS} + 2 = 2 + 2 = 4 = m^\\star_{RS}$\n$m'_{SS} = m_{SS} - 1 = 3 - 1 = 2 = m^\\star_{SS}$\nThe result is consistent. Therefore, the minimal number of degree-preserving double-edge swaps required is $1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}