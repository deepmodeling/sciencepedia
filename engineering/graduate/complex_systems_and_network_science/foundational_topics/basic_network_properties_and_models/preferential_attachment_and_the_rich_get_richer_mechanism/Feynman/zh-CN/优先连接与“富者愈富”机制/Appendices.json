{
    "hands_on_practices": [
        {
            "introduction": "掌握如何使用连续介质方法从第一性原理推导度分布指数 $\\gamma$ 是网络科学中的一项基本功。这个练习  将引导你推导广义的 $k+A$ 模型的度指数，这是对基础 Barabási-Albert 模型的一个强大扩展，能帮助你深入理解“富者愈富”机制的数学核心。",
            "id": "4298183",
            "problem": "考虑一个增长的无向网络，它在时间 $t=0$ 时初始化，拥有一个包含 $N_{0}$ 个节点和 $E_{0}$ 条边的有限种子。在每个离散时间步 $t \\mapsto t+1$，引入一个新节点，该节点通过 $m$ 条新边连接到 $m \\ge 1$ 个已存在的节点。这 $m$ 条边的目标节点是独立选择的，不允许自环，其选择概率与节点的当前度数加上一个常数吸引力成正比。也就是说，一个度为 $k_{i}(t)$ 的节点 $i$ 被选为一条新边的端点，其瞬时概率与 $k_{i}(t) + A$ 成正比，其中 $A$ 是一个与 $i$ 和 $t$ 无关的实常数。假设没有边的删除或重连，并且除了新节点的连接之外，旧节点之间不会创建新的边。\n\n使用基于速率方程的连续介质方法和大型网络极限（即，用其期望值代替总和，用其平均场轨迹代替 $k_{i}(t)$），推导度分布 $P(k) \\sim k^{-\\gamma}$ 的渐近幂律指数 $\\gamma$ 作为 $m$ 和 $A$ 的函数。清晰地阐明您的推导在何种建模和参数条件下有效。以一个关于 $m$ 和 $A$ 的单一解析表达式的形式给出 $\\gamma$ 的最终答案。",
            "solution": "该问题要求推导一个增长网络的度分布 $P(k)$ 的渐近幂律指数 $\\gamma$。网络从 $N_0$ 个节点和 $E_0$ 条边开始。在每个时间步，会添加一个新节点，该节点与现有节点形成 $m \\ge 1$ 条边。度为 $k_i(t)$ 的现有节点 $i$ 被选为新边目标的概率与 $k_i(t) + A$ 成正比，其中 $A$ 是一个常数。我们将按照指定，在大网络极限下使用连续介质方法。\n\n首先，我们形式化连接概率。$m$ 条新边中的一条连接到特定现有节点 $i$ 的概率 $\\Pi_i(t)$ 由下式给出：\n$$\n\\Pi_i(t) = \\frac{k_i(t) + A}{\\sum_{j=1}^{N(t)} (k_j(t) + A)}\n$$\n其中 $N(t)$ 是在时间 $t$ 的总节点数。为了使概率解释有效，连接核 $k_i(t)+A$ 对所有节点必须是非负的。一个节点在创建后的最小度数是 $m$。因此，我们必须对所有 $k \\ge m$ 施加条件 $k+A \\ge 0$，这导致约束条件 $m+A \\ge 0$，或 $A \\ge -m$。\n\n在时间 $t$ 的总节点数是 $N(t) = N_0 + t$。\n在时间 $t$ 的总边数是 $E(t) = E_0 + mt$。\n网络中所有度数的总和是 $\\sum_j k_j(t) = 2E(t) = 2(E_0 + mt)$。\n\n我们现在可以计算 $\\Pi_i(t)$ 的分母：\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) = \\sum_{j=1}^{N(t)} k_j(t) + \\sum_{j=1}^{N(t)} A = 2E(t) + A \\cdot N(t)\n$$\n代入 $N(t)$ 和 $E(t)$ 的表达式：\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) = 2(E_0 + mt) + A(N_0 + t) = (2m+A)t + 2E_0 + AN_0\n$$\n在大网络极限下 ($t \\to \\infty$)，我们可以忽略常数项：\n$$\n\\sum_{j=1}^{N(t)} (k_j(t) + A) \\approx (2m+A)t\n$$\n这个近似在 $2m+A \\ne 0$ 时有效。条件 $A \\ge -m$ 和 $m \\ge 1$ 确保 $2m+A \\ge 2m-m = m \\ge 1$，因此分母总是正且非零的。\n\n我们现在应用连续介质方法来建模节点 $i$ 的度演化。设 $k_i(t)$ 是节点 $i$ 在时间 $t$ 的期望度。其变化率与它接收到新连接的概率成正比。由于每个单位时间添加 $m$ 条新边，速率方程为：\n$$\n\\frac{dk_i}{dt} = m \\cdot \\Pi_i(t) = m \\frac{k_i(t) + A}{(2m+A)t}\n$$\n这是一个关于 $k_i(t)$ 的一阶线性常微分方程。我们可以通过分离变量法求解它：\n$$\n\\frac{dk_i}{k_i + A} = \\frac{m}{2m+A} \\frac{dt}{t}\n$$\n让我们对在时间 $t_i$ 引入的节点 $i$ 对该方程进行积分。该节点的初始条件是其在创建时刻的度为 $m$，所以 $k_i(t_i) = m$。我们从引入时间 $t_i$ 积分到当前时间 $t$：\n$$\n\\int_{m}^{k_i(t)} \\frac{dk}{k+A} = \\int_{t_i}^{t} \\frac{m}{2m+A} \\frac{d\\tau}{\\tau}\n$$\n积分得到：\n$$\n\\left[ \\ln(k+A) \\right]_{m}^{k_i(t)} = \\frac{m}{2m+A} \\left[ \\ln(\\tau) \\right]_{t_i}^{t}\n$$\n$$\n\\ln(k_i(t)+A) - \\ln(m+A) = \\frac{m}{2m+A} (\\ln(t) - \\ln(t_i))\n$$\n整理各项：\n$$\n\\ln\\left(\\frac{k_i(t)+A}{m+A}\\right) = \\ln\\left(\\left(\\frac{t}{t_i}\\right)^{\\frac{m}{2m+A}}\\right)\n$$\n对两边取指数，得到 $k_i(t)$ 的解：\n$$\nk_i(t) = (m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A\n$$\n其中指数 $\\beta$ 定义为：\n$$\n\\beta = \\frac{m}{2m+A}\n$$\n这个方程描述了在时间 $t_i$ 创建的节点的度的确定性演化。为了找到度分布 $P(k)$，我们将度 $k$ 与创建时间 $t_i$ 联系起来。由于节点以恒定速率添加，在时间 $t$ 随机选择一个节点的创建时间 $t_i$ 的分布在区间 $[0, t]$ 上是均匀的（对于大的 $N_0$ 和 $t$，我们可以将起始时间近似为 $0$）。一个节点在时间 $t_i$ 之前被创建的概率是 $P(\\text{创建时间}  t_i) = t_i/t$。\n\n累积度分布 $P(K>k)$ 是一个随机选择的节点度数大于 $k$ 的概率。这等价于其创建时间 $t_i$ 足够小，以使其度数增长到大于 $k$ 的概率。\n$$\nP(K>k) = P(k_i(t) > k)\n$$\n使用 $k_i(t)$ 的表达式：\n$$\n(m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A > k \\implies \\left(\\frac{t}{t_i}\\right)^{\\beta} > \\frac{k+A}{m+A}\n$$\n$$\n\\frac{t}{t_i} > \\left(\\frac{k+A}{m+A}\\right)^{1/\\beta} \\implies t_i  t \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}\n$$\n所以，$P(K>k)$ 是一个节点的创建时间 $t_i$ 小于这个值的概率：\n$$\nP(K>k) = \\frac{t \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}}{t} = \\left(\\frac{k+A}{m+A}\\right)^{-1/\\beta}\n$$\n概率密度函数 $P(k)$ 是累积分布 $P(K>k)$ 关于 $k$ 的负导数：\n$$\nP(k) = -\\frac{d}{dk} P(K>k) = -\\frac{d}{dk} \\left[ (m+A)^{1/\\beta} (k+A)^{-1/\\beta} \\right]\n$$\n$$\nP(k) = - (m+A)^{1/\\beta} \\left(-\\frac{1}{\\beta}\\right) (k+A)^{-1/\\beta - 1} = \\frac{(m+A)^{1/\\beta}}{\\beta} (k+A)^{-(1+1/\\beta)}\n$$\n对于大的度 $k$，项 $(k+A)$ 渐近地与 $k$ 成正比。因此，度分布遵循幂律：\n$$\nP(k) \\sim k^{-\\gamma}\n$$\n通过比较指数，我们确定幂律指数 $\\gamma$：\n$$\n\\gamma = 1 + \\frac{1}{\\beta}\n$$\n代入 $\\beta = \\frac{m}{2m+A}$ 的表达式：\n$$\n\\gamma = 1 + \\frac{2m+A}{m} = 1 + 2 + \\frac{A}{m} = 3 + \\frac{A}{m}\n$$\n这个推导在以下条件下有效：\n1.  **大网络和长时间极限**：该推导假设 $t \\to \\infty$ 和 $N(t) \\to \\infty$，这证明了连续介质近似以及忽略初始条件 $N_0$ 和 $E_0$ 的合理性。\n2.  **平均场近似**：节点度演化的随机性被确定性的平均场轨迹 $k_i(t)$ 所取代。这忽略了涨落，对于大型网络是有效的。\n3.  **物理概率**：连接核 $k+A$ 对于所有可能的度 $k \\ge m$ 必须是非负的。这施加了条件 $m+A \\ge 0$，或 $A \\ge -m$。如果满足此条件，那么对于 $t>0$ 和 $m\\ge1$，分母 $(2m+A)t$ 也保证为正。\n\n因此，度分布的渐近幂律指数是 $\\gamma = 3 + A/m$。",
            "answer": "$$ \\boxed{3 + \\frac{A}{m}} $$"
        },
        {
            "introduction": "在推导出模型的数学解之后，探索其参数空间是获得深刻理解的关键一步。这个练习  旨在研究初始吸引力参数 $A$ 的两个极端极限，揭示广义优先连接模型如何统一两种典型的网络增长机制：产生无标度网络的 Barabási-Albert 模型和产生指数网络的随机连接模型。",
            "id": "4298187",
            "problem": "考虑一个增长网络，在每个离散时间步 $t$，一个带 $m$ 条边的新节点被添加进来，这些边连接到现有节点。连接核为 $\\Pi_i \\propto k_i + A$，其中 $k_i$ 是节点 $i$ 的度，$A \\ge 0$ 是一个恒定的初始吸引力。使用以下基本事实：度之和满足 $\\sum_j k_j = 2mt$，节点数在 $t$ 很大时满足 $N(t) \\approx t$。在连续极限下，通过推导在大 $k$ 值下得到的度分布 $P(k)$ 的定性和定量形式，以及在时间 $t_i$ 诞生的节点的度的典型时间演化 $k_i(t)$，来解释两种极端情况 $A \\to 0$ 和 $A \\to \\infty$。哪个选项最能描述正确的极限行为？\n\nA. 当 $A \\to 0$ 时，$P(k) \\sim k^{-3}$；当 $A \\to \\infty$ 时，$P(k)$ 是指数分布，特征尺度约为 $A$，并且 $k_i(t)$ 随年龄线性增长。\n\nB. 当 $A \\to 0$ 时，$P(k) \\sim k^{-2}$；当 $A \\to \\infty$ 时，$P(k)$ 仍然是幂律分布，其指数与 $A$ 无关。\n\nC. 当 $A \\to 0$ 时，$P(k)$ 是指数分布，因为初始吸引力消除了优先偏好；当 $A \\to \\infty$ 时，$P(k)$ 变为指数为 $\\gamma = 3$ 的幂律分布。\n\nD. 当 $A \\to 0$ 时，$P(k) \\sim k^{-3}$；当 $A \\to \\infty$ 时，连接变为均匀的，$k_i(t) = m + m \\ln(t/t_i)$，并且 $P(k) \\sim \\exp(-(k - m)/m)$。\n\nE. 当 $A \\to 0$ 时，$P(k) \\sim (k + A)^{-(3 + A/m)}$；当 $A \\to \\infty$ 时，$P(k)$ 趋近于一个指数 $\\gamma \\to \\infty$ 的幂律分布，同时保持重尾特性。",
            "solution": "题目陈述是使用速率方程形式主义研究增长网络的一个有效练习。它在科学上基于已建立的优先连接理论，问题提法良好且客观。所有提供的信息都是自洽且一致的。我们可以开始解题了。\n\n问题的核心在于连接核 $\\Pi_i$，即新边连接到现有节点 $i$ 的概率。它由以下公式给出：\n$$\n\\Pi_i = \\frac{k_i + A}{\\sum_{j} (k_j + A)}\n$$\n可以使用已知事实计算分母。在一个很大的时间 $t$，网络中有 $N(t) \\approx t$ 个节点。度之和为 $\\sum_j k_j = 2mt$。\n$$\n\\sum_{j=1}^{N(t)} (k_j + A) = \\sum_{j=1}^{N(t)} k_j + \\sum_{j=1}^{N(t)} A = 2mt + N(t)A \\approx 2mt + tA = t(2m+A)\n$$\n因此，连接概率为：\n$$\n\\Pi_i = \\frac{k_i + A}{t(2m+A)}\n$$\n我们使用连续近似，其中节点 $i$ 的度 $k_i$ 被视为随时间 $t$ 演化的连续变量。$k_i$ 的变化率由每个时间步的新边数 $m$ 乘以连接到节点 $i$ 的概率给出：\n$$\n\\frac{dk_i}{dt} = m \\Pi_i = m \\frac{k_i + A}{t(2m+A)}\n$$\n这是一个可分离的一阶常微分方程。我们可以求解它，对于一个在时间 $t_i$ 引入、初始度为 $k_i(t_i)=m$ 的节点 $i$。\n$$\n\\int_{m}^{k_i(t)} \\frac{dk'}{k' + A} = \\int_{t_i}^{t} \\frac{m}{2m+A} \\frac{d\\tau}{\\tau}\n$$\n对两边积分得到：\n$$\n\\big[\\ln(k' + A)\\big]_{m}^{k_i(t)} = \\frac{m}{2m+A} \\big[\\ln(\\tau)\\big]_{t_i}^{t}\n$$\n$$\n\\ln(k_i(t) + A) - \\ln(m + A) = \\frac{m}{2m+A} \\big(\\ln(t) - \\ln(t_i)\\big)\n$$\n$$\n\\ln\\left(\\frac{k_i(t) + A}{m + A}\\right) = \\ln\\left(\\left(\\frac{t}{t_i}\\right)^{\\frac{m}{2m+A}}\\right)\n$$\n对两边取指数，得到节点 $i$ 的度的时间演化：\n$$\nk_i(t) = (m+A)\\left(\\frac{t}{t_i}\\right)^{\\beta} - A, \\quad \\text{其中} \\quad \\beta = \\frac{m}{2m+A}\n$$\n为了求得度分布 $P(k)$，我们可以使用 $k_i(t)$ 的时间演化。累积概率 $P(K > k)$ 是度大于 $k$ 的节点所占的比例。由于节点以恒定速率添加，其年龄分布是均匀的。在某个时间 $t_c$ 之前诞生的节点所占的比例是 $t_c/t$。我们求出节点必须在哪个时间 $t_c$ 诞生，才能在时间 $t$ 时度恰好为 $k$。\n$$\nk = (m+A)\\left(\\frac{t}{t_c}\\right)^{\\beta} - A \\implies t_c = t \\left(\\frac{m+A}{k+A}\\right)^{1/\\beta}\n$$\n累积分布为 $P(K > k) \\approx t_c/t$：\n$$\nP(K > k) = \\left(\\frac{m+A}{k+A}\\right)^{1/\\beta}\n$$\n概率密度函数 $P(k)$ 通过求导得到：$P(k) = - \\frac{d}{dk}P(K>k)$。\n$$\nP(k) = - \\frac{d}{dk} \\left[ (m+A)^{1/\\beta} (k+A)^{-1/\\beta} \\right] = (m+A)^{1/\\beta} \\left(\\frac{1}{\\beta}\\right) (k+A)^{-(1+1/\\beta)}\n$$\n指数为 $\\gamma = 1 + 1/\\beta = 1 + \\frac{2m+A}{m} = 1 + 2 + \\frac{A}{m} = 3 + \\frac{A}{m}$。\n所以，度分布的一般形式是：\n$$\nP(k) \\propto (k+A)^{-(3+A/m)}\n$$\n\n现在我们分析两种极端情况。\n\n**情况1：$A \\to 0$ (纯优先连接)**\n在此极限下，该模型简化为标准的巴拉巴西-阿尔伯特(Barabási-Albert)模型。\n- 对于度演化，指数 $\\beta = \\frac{m}{2m+A} \\to \\frac{m}{2m} = \\frac{1}{2}$。\n  度演化方程变为 $k_i(t) = m\\left(\\frac{t}{t_i}\\right)^{1/2}$。节点的度随其年龄的平方根增长。\n- 对于度分布，指数 $\\gamma = 3 + \\frac{A}{m} \\to 3$。\n  分布变为幂律，$P(k) \\propto k^{-3}$。这是指数为 $\\gamma=3$ 的典型无标度分布。\n\n**情况2：$A \\to \\infty$ (均匀连接)**\n在此极限下，初始吸引力 $A$ 远大于度 $k_i$。连接核 $\\Pi_i \\propto k_i + A \\approx A$。这意味着连接概率与节点的度无关，因此连接在所有现有节点上是均匀的。\n- 对于度演化，我们可以直接分析具有均匀连接概率 $\\Pi_i \\approx 1/N(t) \\approx 1/t$ 的速率方程：\n  $$\n  \\frac{dk_i}{dt} = m \\Pi_i \\approx \\frac{m}{t}\n  $$\n  从 $t_i$ 到 $t$ 积分，初始条件为 $k_i(t_i) = m$：\n  $$\n  \\int_m^{k_i(t)} dk' = \\int_{t_i}^t \\frac{m}{\\tau}d\\tau \\implies k_i(t) - m = m \\ln(t/t_i)\n  $$\n  $$\n  k_i(t) = m + m \\ln(t/t_i)\n  $$\n  节点的度随其年龄对数增长。\n- 对于度分布，我们使用此对数增长定律来求 $P(Kk)$。\n  $$\n  k = m + m \\ln(t/t_c) \\implies \\frac{k-m}{m} = \\ln(t/t_c) \\implies t_c = t \\exp\\left(-\\frac{k-m}{m}\\right)\n  $$\n  累积分布为 $P(K  k) \\approx t_c/t = \\exp\\left(-\\frac{k-m}{m}\\right)$。\n  求导得到概率密度：\n  $$\n  P(k) = -\\frac{d}{dk} \\exp\\left(-\\frac{k-m}{m}\\right) = \\frac{1}{m} \\exp\\left(-\\frac{k-m}{m}\\right)\n  $$\n  对于大的 $k$，这是一个指数分布，$P(k) \\sim \\exp(-k/m)$。\n\n**逐项分析**\n\n*   **A. 当 $A \\to 0$ 时，$P(k) \\sim k^{-3}$；当 $A \\to \\infty$ 时，$P(k)$ 是指数分布，特征尺度约为 $A$，并且 $k_i(t)$ 随年龄线性增长。**\n    - $A \\to 0$ 的部分是正确的。\n    - 对于 $A \\to \\infty$，$P(k)$ 确实是指数分布，但特征尺度（衰减常数）是 $m$，而不是 $A$。声称 $k_i(t)$ 随年龄线性增长也是不正确的；它以对数方式增长。\n    - 结论：**错误**。\n\n*   **B. 当 $A \\to 0$ 时，$P(k) \\sim k^{-2}$；当 $A \\to \\infty$ 时，$P(k)$ 仍然是幂律分布，其指数与 $A$ 无关。**\n    - $A \\to 0$ 情况下的指数是 $3$，不是 $2$。\n    - $A \\to \\infty$ 的分布是指数分布，不是幂律分布。\n    - 结论：**错误**。\n\n*   **C. 当 $A \\to 0$ 时，$P(k)$ 是指数分布，因为初始吸引力消除了优先偏好；当 $A \\to \\infty$ 时，$P(k)$ 变为指数为 $\\gamma = 3$ 的幂律分布。**\n    - 这个选项颠倒了两种极限下的行为。当 $A \\to 0$ 时，我们得到一个幂律分布。优先偏好是最强的，而不是被消除。当 $A \\to \\infty$ 时，分布是指数的。指数为 $\\gamma=3$ 的幂律分布对应于 $A \\to 0$。\n    - 结论：**错误**。\n\n*   **D. 当 $A \\to 0$ 时，$P(k) \\sim k^{-3}$；当 $A \\to \\infty$ 时，连接变为均匀的，$k_i(t) = m + m \\ln(t/t_i)$，并且 $P(k) \\sim \\exp(-(k - m)/m)$。**\n    - 对于 $A \\to 0$，$P(k) \\sim k^{-3}$ 是正确的。\n    - 对于 $A \\to \\infty$，连接变为均匀的说法是正确的。推导出的度演化方程 $k_i(t) = m + m \\ln(t/t_i)$ 是正确的。得到的度分布形式 $P(k) \\sim \\exp(-(k-m)/m)$ 也是正确的。\n    - 结论：**正确**。\n\n*   **E. 当 $A \\to 0$ 时，$P(k) \\sim (k + A)^{-(3 + A/m)}$；当 $A \\to \\infty$ 时，$P(k)$ 趋近于一个指数 $\\gamma \\to \\infty$ 的幂律分布，同时保持重尾特性。**\n    - 第一部分给出了 $P(k)$ 的通用公式，这对于任何有限的 $A \\ge 0$ 都成立，但问题要求的是 $A \\to 0$ 极限下的行为，即 $P(k) \\sim k^{-3}$。\n    - 第二部分是不正确的。虽然指数 $\\gamma = 3+A/m$ 在 $A \\to \\infty$ 时确实趋于无穷大，但这并不意味着极限分布是幂律分布。分布的函数形式从幂律变为指数。此外，一个具有无限快衰减幂律尾的分布是“重尾”的对立面。指数衰减被认为是轻尾的。\n    - 结论：**错误**。",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "理论模型的最终检验在于其描述真实世界数据的能力，从而连接抽象理论与实证研究。这项最终的实践  介绍了一种强大的统计方法——最大似然估计（MLE），用于从观测到的网络增长序列中推断模型的关键参数（$A$ 和 $\\alpha$）。对于任何希望将网络模型应用于实证数据集的研究者来说，这都是一项至关重要的技能。",
            "id": "4298151",
            "problem": "给定一个符合广义优先连接机制的离散时间网络增长过程。在每个事件中，一个新节点到达并只连接到一个现有节点。设 $k_i^{(t)}$ 表示在事件 $t$ 发生前夕（即，严格在事件 $t$ 新节点到达和连接之前）现有节点 $i$ 的度。假设新节点在事件 $t$ 选择现有节点 $i$ 的连接概率与非负核 $(k_i^{(t)} + A)^\\alpha$ 成正比，其中 $A \\ge 0$ 是初始吸引力，$\\alpha  0$ 控制优先连接的非线性程度。\n\n基本依据：\n- 优先连接的核心定义：连接到现有节点的概率是其当前度的一个非递减函数的正比。\n- 此处使用的经过充分检验的建模事实：形式为 $(k + A)^\\alpha$（其中 $A \\ge 0$ 且 $\\alpha  0$）的广义优先连接核可以捕捉初始吸引力和非线性的“富者愈富”机制。\n\n您观察到一个完整的增长数据集，包括：\n- 时间 $t=0$ 时已存在节点的初始度向量。\n- 每个事件一个的所选索引序列，指示在该事件中哪个现有节点接收了新边。\n\n假设：\n- 每个事件恰好增加一个新节点和一条新边，将新节点连接到从事件前存在的节点集合中选出的那个节点。\n- 节点标签按到达顺序固定：初始节点从 $0$ 开始标记，在事件 $t$ 引入的新节点获得的标签等于事件前当前节点数。\n- 度确定性地更新：当节点 $i$ 在事件 $t$ 被选中时，其度增加 $1$，新加入的节点以度 $1$ 开始，并在后续事件中可用。\n\n任务：\n1. 构建参数 $A$ 和 $\\alpha$ 的对数似然函数。假设在每个事件发生前夕的度配置条件下，观测到的连接事件是从在所有现有节点上归一化的核 $(k_i^{(t)} + A)^\\alpha$ 中抽取的。\n2. 实现最大似然估计（MLE），通过在约束条件 $A \\ge 0$ 和 $\\alpha  0$ 下，对 $A$ 和 $\\alpha$ 数值最大化对数似然函数，从而从给定数据集中估计 $A$ 和 $\\alpha$。\n3. 使用基于梯度的数值优化。您必须提供并使用对数似然函数关于 $A$ 和 $\\alpha$ 的解析梯度。\n\n测试套件：\n为以下三个数据集提供MLE结果。在每个数据集中，都指定了初始度向量和所选索引的序列。\n\n- 数据集 $1$（一般情况）：\n  - 初始度：$[1,1]$。\n  - 按事件排列的所选索引：$[0,0,1,0,1,0,0,1,0,0]$。\n\n- 数据集 $2$（平衡连接，检验对 $A$ 的敏感度）：\n  - 初始度：$[1,1,1]$。\n  - 按事件排列的所选索引：$[0,1,2,0,1,2,0,1,2,0,1,2]$。\n\n- 数据集 $3$（极端集中，检验对 $\\alpha$ 的敏感度）：\n  - 初始度：$[1,1,1]$。\n  - 按事件排列的所选索引：$[0,0,0,0,0,0,0,0,0,0,0,0]$。\n\n数值与输出要求：\n- 在边界 $A \\in [0,100]$ 和 $\\alpha \\in [0.1,2.0]$ 内对 $A$ 和 $\\alpha$ 进行优化。\n- 每个数据集应产生一对估计值 $\\widehat{A}$ 和 $\\widehat{\\alpha}$。\n- 将每个估计值表示为四舍五入到 $6$ 位小数的浮点数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个数据集的结果本身是一个按 $[\\widehat{A},\\widehat{\\alpha}]$ 顺序排列的双元素列表。例如：$[[\\widehat{A}_1,\\widehat{\\alpha}_1],[\\widehat{A}_2,\\widehat{\\alpha}_2],[\\widehat{A}_3,\\widehat{\\alpha}_3]]$。",
            "solution": "该问题要求对一个广义优先连接模型的参数执行最大似然估计（MLE）。该模型由一个与 $(k+A)^\\alpha$ 成正比的连接概率核定义，其中 $k$ 是节点度，$A \\ge 0$ 是一个初始吸引力参数，$\\alpha > 0$ 控制连接偏好的超线性或亚线性。我们获得了三个不同网络增长历史的完整观测数据，并且必须为每个历史估计参数对 $(A, \\alpha)$。这需要推导对数似然函数及其梯度，然后使用数值优化算法找到使该函数最大化的参数。\n\n**1. 对数似然函数推导**\n\n设参数由向量 $\\theta = (A, \\alpha)$ 表示。单个增长过程的观测数据包括 $N_0$ 个初始节点的初始度向量 $\\mathbf{k}^{(0)} = (k_0^{(0)}, \\dots, k_{N_0-1}^{(0)})$，以及在 $T$ 个离散时间事件中选择的节点序列 $\\mathbf{c} = (c_1, c_2, \\dots, c_T)$。\n\n在每个事件 $t \\in \\{1, 2, \\dots, T\\}$ 中，一个新节点到达并连接到 $N^{(t)} = N_0 + t - 1$ 个现有节点之一。在事件 $t$ 发生前夕存在的节点集合是 $V^{(t)} = \\{0, 1, \\dots, N^{(t)}-1\\}$，它们的度由向量 $\\mathbf{k}^{(t-1)}$ 给出。\n\n根据模型，新节点连接到特定现有节点 $i \\in V^{(t)}$ 的概率由归一化核给出：\n$$ P(\\text{choose } i | \\mathbf{k}^{(t-1)}, A, \\alpha) = \\frac{(k_i^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} $$\n假设在每一步网络状态给定的条件下，事件是相互独立的。因此，观测到整个所选节点序列 $\\mathbf{c}$ 的总似然是每个独立事件概率的乘积：\n$$ L(A, \\alpha | \\mathbf{k}^{(0)}, \\mathbf{c}) = \\prod_{t=1}^{T} P(\\text{chosen node is } c_t | \\mathbf{k}^{(t-1)}, A, \\alpha) $$\n代入概率表达式，我们得到：\n$$ L(A, \\alpha) = \\prod_{t=1}^{T} \\frac{(k_{c_t}^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} $$\n为保证数值稳定性和数学上的便利，我们使用对数似然函数 $\\mathcal{L}(A, \\alpha) = \\ln L(A, \\alpha)$。对数将乘积转换为和：\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\ln \\left( \\frac{(k_{c_t}^{(t-1)} + A)^\\alpha}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right) $$\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\left[ \\alpha \\ln(k_{c_t}^{(t-1)} + A) - \\ln \\left( \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\right) \\right] $$\n此函数表示在特定参数 $A$ 和 $\\alpha$ 选择下，观测到给定连接序列的对数概率。为了找到最大似然估计值 $(\\widehat{A}, \\widehat{\\alpha})$，我们必须找到在给定约束条件下使该函数最大化的 $A$ 和 $\\alpha$ 的值。\n\n度向量 $\\mathbf{k}^{(t-1)}$ 不是固定的，而是根据过程演化。从 $\\mathbf{k}^{(0)}$ 开始，事件 $t$ 结束时的度向量（表示为 $\\mathbf{k}^{(t)}$）是通过更新 $\\mathbf{k}^{(t-1)}$ 得到的：被选中的节点 $c_t$ 的度增加 $1$，一个度为 $1$ 的新节点被添加到网络中。\n\n**2. 梯度推导**\n为了进行高效的数值优化，我们必须计算对数似然函数关于 $A$ 和 $\\alpha$ 的偏导数。\n\n设事件 $t$ 的归一化和为 $S_t(A, \\alpha) = \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha$。对数似然函数为：\n$$ \\mathcal{L}(A, \\alpha) = \\sum_{t=1}^{T} \\left[ \\alpha \\ln(k_{c_t}^{(t-1)} + A) - \\ln(S_t(A, \\alpha)) \\right] $$\n\n**关于 $A$ 的梯度**：\n我们将 $\\mathcal{L}$ 对 $A$ 求导：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\partial}{\\partial A} \\left(\\alpha \\ln(k_{c_t}^{(t-1)} + A)\\right) - \\frac{\\partial}{\\partial A} \\left(\\ln(S_t)\\right) \\right] $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_{t=1}^{T} \\left[ \\frac{\\alpha}{k_{c_t}^{(t-1)} + A} - \\frac{1}{S_t} \\frac{\\partial S_t}{\\partial A} \\right] $$\n和 $S_t$ 关于 $A$ 的导数是：\n$$ \\frac{\\partial S_t}{\\partial A} = \\sum_{j=0}^{N^{(t)}-1} \\frac{\\partial}{\\partial A} (k_j^{(t-1)} + A)^\\alpha = \\sum_{j=0}^{N^{(t)}-1} \\alpha (k_j^{(t-1)} + A)^{\\alpha - 1} $$\n将其代回，得到关于 $A$ 的完整梯度：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A} = \\alpha \\sum_{t=1}^{T} \\left[ \\frac{1}{k_{c_t}^{(t-1)} + A} - \\frac{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^{\\alpha - 1}}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] $$\n\n**关于 $\\alpha$ 的梯度**：\n同样，我们将 $\\mathcal{L}$ 对 $\\alpha$ 求导：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\frac{\\partial}{\\partial \\alpha} \\left(\\alpha \\ln(k_{c_t}^{(t-1)} + A)\\right) - \\frac{\\partial}{\\partial \\alpha} \\left(\\ln(S_t)\\right) \\right] $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\ln(k_{c_t}^{(t-1)} + A) - \\frac{1}{S_t} \\frac{\\partial S_t}{\\partial \\alpha} \\right] $$\n和 $S_t$ 关于 $\\alpha$ 的导数是（使用 $\\frac{d}{dx} a^x = a^x \\ln a$）：\n$$ \\frac{\\partial S_t}{\\partial \\alpha} = \\sum_{j=0}^{N^{(t)}-1} \\frac{\\partial}{\\partial \\alpha} (k_j^{(t-1)} + A)^\\alpha = \\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\ln(k_j^{(t-1)} + A) $$\n将其代回，得到关于 $\\alpha$ 的完整梯度：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = \\sum_{t=1}^{T} \\left[ \\ln(k_{c_t}^{(t-1)} + A) - \\frac{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha \\ln(k_j^{(t-1)} + A)}{\\sum_{j=0}^{N^{(t)}-1} (k_j^{(t-1)} + A)^\\alpha} \\right] $$\n分数项是在事件 $t$ 时模型概率分布下，对数吸引力 $E[\\ln(k+A)]$ 的期望值。\n\n**3. 数值最大化**\n我们将通过数值最小化负对数似然函数 $-\\mathcal{L}(A, \\alpha)$ 来找到最大似然估计参数 $(\\widehat{A}, \\widehat{\\alpha})$，其约束条件为 $A \\in [0, 100]$ 和 $\\alpha \\in [0.1, 2.0]$。我们使用 L-BFGS-B 算法，这是一种拟牛顿法，非常适用于带箱式约束的优化，并能利用我们推导出的解析梯度。\n\n实现的核心是一个函数，该函数对于给定的参数集 $(A, \\alpha)$ 和观测数据集，逐个事件地模拟网络增长。在每个事件中，它计算对总对数似然及其梯度的贡献，并累加这些值。该函数返回总负对数似然及其对应的负梯度向量 $[-\\frac{\\partial \\mathcal{L}}{\\partial A}, -\\frac{\\partial \\mathcal{L}}{\\partial \\alpha}]$，然后将它们传递给优化器。对所提供的三个数据集中的每一个重复此过程，以找到它们各自的参数估计值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef solve():\n    \"\"\"\n    Main function to perform MLE for the generalized preferential attachment model\n    on three datasets.\n    \"\"\"\n    test_cases = [\n        # Dataset 1 (general case)\n        {\n            \"initial_degrees\": [1, 1],\n            \"chosen_indices\": [0, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n        },\n        # Dataset 2 (balanced attachments)\n        {\n            \"initial_degrees\": [1, 1, 1],\n            \"chosen_indices\": [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]\n        },\n        # Dataset 3 (extreme concentration)\n        {\n            \"initial_degrees\": [1, 1, 1],\n            \"chosen_indices\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        }\n    ]\n\n    def neg_log_likelihood_and_grad(params, initial_degrees, chosen_indices):\n        \"\"\"\n        Calculates the negative log-likelihood and its gradient for the given data.\n        \n        This function simulates the network growth process step-by-step based on the\n        observed attachment sequence. At each step, it calculates and accumulates\n        the log-likelihood and its partial derivatives with respect to A and alpha.\n        \"\"\"\n        A, alpha = params\n        \n        # Ensure parameters are within valid domain for calculation.\n        # This is mainly a safeguard, as bounds are handled by the optimizer.\n        if A  0 or alpha = 0:\n            return np.inf, np.zeros(2)\n\n        log_likelihood = 0.0\n        grad_A = 0.0\n        grad_alpha = 0.0\n        \n        # Use a list for degrees as it grows dynamically\n        degrees = list(initial_degrees)\n        \n        for t, chosen_node_idx in enumerate(chosen_indices):\n            # Convert to numpy array for vectorized operations\n            k_vals = np.array(degrees, dtype=np.float64)\n            \n            # The base of the attachment kernel\n            base = k_vals + A\n            \n            # The attachment kernel (k+A)^alpha\n            kernel_vals = base**alpha\n            S_t = np.sum(kernel_vals)\n\n            if S_t == 0: # Avoid division by zero\n                # This case should not be reached with the given constraints k>=1, A>=0, alpha>0\n                return np.inf, np.zeros(2)\n\n            k_chosen = k_vals[chosen_node_idx]\n            base_chosen = k_chosen + A\n\n            # Log-likelihood contribution for event t\n            log_likelihood += alpha * np.log(base_chosen) - np.log(S_t)\n            \n            # Gradient contribution for event t\n            # Gradient w.r.t. A\n            grad_S_t_A_num = np.sum(alpha * base**(alpha - 1.0))\n            grad_A += alpha * (1.0 / base_chosen - grad_S_t_A_num / S_t)\n            \n            # Gradient w.r.t. alpha\n            log_base = np.log(base)\n            grad_S_t_alpha_num = np.sum(kernel_vals * log_base)\n            grad_alpha += np.log(base_chosen) - grad_S_t_alpha_num / S_t\n            \n            # Update degrees for the next event\n            degrees[chosen_node_idx] += 1\n            degrees.append(1) # New node with degree 1\n            \n        return -log_likelihood, -np.array([grad_A, grad_alpha])\n\n    results = []\n    bounds = [(0, 100), (0.1, 2.0)]\n    initial_guess = [1.0, 1.0]\n\n    for case in test_cases:\n        args = (case[\"initial_degrees\"], case[\"chosen_indices\"])\n        \n        res = optimize.minimize(\n            fun=neg_log_likelihood_and_grad,\n            x0=initial_guess,\n            args=args,\n            method='L-BFGS-B',\n            jac=True,  # Our function returns the jacobian (gradient)\n            bounds=bounds,\n            options={'disp': False}\n        )\n        \n        A_hat, alpha_hat = res.x\n        results.append([round(A_hat, 6), round(alpha_hat, 6)])\n\n    # The final print statement must match the required format exactly.\n    # repr() creates a string representation, and .replace removes spaces\n    # to match the symbolic format in the prompt.\n    final_output_string = repr(results).replace(\" \", \"\")\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}