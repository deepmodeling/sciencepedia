{
    "hands_on_practices": [
        {
            "introduction": "Understanding scale-free networks begins with grasping their generative mechanism: preferential attachment. This exercise provides a direct application of the \"rich-get-richer\" principle, where the probability of a new node connecting to an existing node is proportional to its current degree. By working through this simple calculation , you will solidify your understanding of the fundamental rule that gives rise to hubs and the power-law degree distribution.",
            "id": "1705406",
            "problem": "In the study of network theory, the growth of many real-world networks, such as citation networks or the World Wide Web, can be modeled by a process of preferential attachment. Consider a small, isolated citation network consisting of five foundational papers. The importance of each paper is quantified by its degree, which represents the number of connections it has within this network. The degrees of these five papers are $4, 3, 2, 2,$ and $1$.\n\nA new research paper is being written and will cite exactly one of these five foundational papers. The model for this citation process states that the probability of the new paper connecting to any given foundational paper is directly proportional to the degree of that foundational paper.\n\nCalculate the probability that the new paper will cite the most influential paper in the existing network, which is the one with a degree of 4. Express your answer as a simplified fraction.",
            "solution": "Under preferential attachment, the probability that the new paper cites a given foundational paper is proportional to that paper's degree. Let the degrees be $k_{1}=4$, $k_{2}=3$, $k_{3}=2$, $k_{4}=2$, and $k_{5}=1$. The total degree is the sum\n$$\n\\sum_{i=1}^{5} k_{i} = 4+3+2+2+1 = 12.\n$$\nThe probability of citing the paper with degree $4$ is\n$$\nP = \\frac{k_{1}}{\\sum_{i=1}^{5} k_{i}} = \\frac{4}{12} = \\frac{1}{3}.\n$$\nThus, the desired probability is the simplified fraction $\\frac{1}{3}$.",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "A defining feature of scale-free networks is their unusual combination of robustness to random failures and fragility to targeted attacks. This practice delves into this duality using the analytical framework of percolation theory, a powerful tool for studying network integrity. By calculating and comparing the percolation thresholds for random versus targeted node removal , you will mathematically demonstrate why targeting high-degree hubs is a dramatically effective strategy for disrupting these networks.",
            "id": "1705397",
            "problem": "The robustness of complex networks, such as the internet or biological interaction networks, against failures can be studied using percolation theory. Consider a large scale-free network whose degree distribution $P(k)$ follows a power law. We model this distribution by treating the degree $k$ as a continuous variable, described by the probability density function $P(k) = C k^{-\\gamma}$ for $k_{\\min} \\le k \\le k_{\\max}$, and $P(k) = 0$ otherwise. Here, $C$ is a normalization constant. The network parameters are given as $\\gamma = 2.5$, $k_{\\min} = 2$, and $k_{\\max} = 1000$.\n\nWe analyze the network's integrity under two different site percolation scenarios, where a fraction $p$ of nodes are kept active and a fraction $1-p$ are removed. The network is considered functional if a giant connected component exists.\n\n1.  **Random Failure**: Nodes are removed uniformly at random. The percolation threshold, $p_{c, \\text{rand}}$, is the minimum fraction of nodes that must remain for a giant connected component to exist. For a network with degree distribution $P(k)$, this threshold is given by the formula $p_{c, \\text{rand}} = \\frac{\\langle k \\rangle}{\\langle k^2 \\rangle - \\langle k \\rangle}$, where $\\langle k^n \\rangle = \\int_{k_{\\min}}^{k_{\\max}} k^n P(k) dk$ is the $n$-th moment of the degree distribution.\n\n2.  **Targeted Attack**: Nodes are removed in a targeted manner, starting from the one with the highest degree, then the next highest, and so on. For this type of attack on our specific network model, the percolation threshold, $p_{c, \\text{targ}}$, is given by the formula $p_{c, \\text{targ}} = \\left(\\frac{k_{\\min}}{k_{\\max}}\\right)^{\\frac{\\gamma-2}{\\gamma-1}}$.\n\nCalculate the numerical value of the ratio $\\frac{p_{c, \\text{rand}}}{p_{c, \\text{targ}}}$. Round your final answer to three significant figures.",
            "solution": "We are given a continuous degree distribution $P(k)=C k^{-\\gamma}$ on $[k_{\\min},k_{\\max}]$ with $\\gamma=\\frac{5}{2}$, $k_{\\min}=2$, and $k_{\\max}=1000$. The normalization constant $C$ follows from\n$$\n\\int_{k_{\\min}}^{k_{\\max}} C k^{-\\gamma}\\,dk=1 \\;\\;\\Rightarrow\\;\\; C\\frac{k^{1-\\gamma}}{1-\\gamma}\\Big|_{k_{\\min}}^{k_{\\max}}=1,\n$$\nwhich gives\n$$\nC=\\frac{\\gamma-1}{k_{\\min}^{1-\\gamma}-k_{\\max}^{1-\\gamma}}=\\frac{\\frac{3}{2}}{k_{\\min}^{-3/2}-k_{\\max}^{-3/2}}.\n$$\nThe $n$-th moment is\n$$\n\\langle k^{n}\\rangle=\\int_{k_{\\min}}^{k_{\\max}}k^{n}P(k)\\,dk=C\\int_{k_{\\min}}^{k_{\\max}}k^{n-\\gamma}\\,dk=\\frac{C}{n+1-\\gamma}\\left(k_{\\max}^{n+1-\\gamma}-k_{\\min}^{n+1-\\gamma}\\right),\n$$\nvalid for $n+1\\neq\\gamma$. For $\\gamma=\\frac{5}{2}$ we obtain\n$$\n\\langle k\\rangle=\\frac{C}{2-\\gamma}\\left(k_{\\max}^{2-\\gamma}-k_{\\min}^{2-\\gamma}\\right)=2C\\left(k_{\\min}^{-1/2}-k_{\\max}^{-1/2}\\right),\n$$\n$$\n\\langle k^{2}\\rangle=\\frac{C}{3-\\gamma}\\left(k_{\\max}^{3-\\gamma}-k_{\\min}^{3-\\gamma}\\right)=2C\\left(k_{\\max}^{1/2}-k_{\\min}^{1/2}\\right).\n$$\nUsing $C=\\frac{3/2}{k_{\\min}^{-3/2}-k_{\\max}^{-3/2}}$, these simplify to\n$$\n\\langle k\\rangle=3\\frac{k_{\\min}^{-1/2}-k_{\\max}^{-1/2}}{k_{\\min}^{-3/2}-k_{\\max}^{-3/2}},\\qquad\n\\langle k^{2}\\rangle=3\\frac{k_{\\max}^{1/2}-k_{\\min}^{1/2}}{k_{\\min}^{-3/2}-k_{\\max}^{-3/2}}.\n$$\nWith $k_{\\min}=2$ and $k_{\\max}=1000$,\n$$\nk_{\\min}^{-1/2}=\\frac{1}{\\sqrt{2}},\\quad k_{\\max}^{-1/2}=\\frac{1}{\\sqrt{1000}},\\quad\nk_{\\min}^{-3/2}=\\frac{1}{2\\sqrt{2}},\\quad k_{\\max}^{-3/2}=\\frac{1}{1000\\sqrt{1000}},\n$$\n$$\nk_{\\max}^{1/2}=\\sqrt{1000},\\quad k_{\\min}^{1/2}=\\sqrt{2}.\n$$\nNumerically, this yields\n$$\n\\langle k\\rangle\\approx 5.732184545,\\qquad \\langle k^{2}\\rangle\\approx 256.351086044.\n$$\nThe random-failure percolation threshold is\n$$\np_{c,\\text{rand}}=\\frac{\\langle k\\rangle}{\\langle k^{2}\\rangle-\\langle k\\rangle}\\approx \\frac{5.732184545}{256.351086044-5.732184545}\\approx 0.0228721158.\n$$\nFor targeted attack, the threshold is\n$$\np_{c,\\text{targ}}=\\left(\\frac{k_{\\min}}{k_{\\max}}\\right)^{\\frac{\\gamma-2}{\\gamma-1}}=\\left(\\frac{2}{1000}\\right)^{\\frac{1/2}{3/2}}=\\left(\\frac{2}{1000}\\right)^{1/3}=\\frac{1}{10}\\,2^{1/3}\\approx 0.125992105.\n$$\nTherefore, the requested ratio is\n$$\n\\frac{p_{c,\\text{rand}}}{p_{c,\\text{targ}}}\\approx \\frac{0.0228721158}{0.125992105}\\approx 0.181536,\n$$\nwhich to three significant figures is $0.182$.",
            "answer": "$$\\boxed{0.182}$$"
        },
        {
            "introduction": "While simple models like the Barabási-Albert model successfully explain the emergence of scale-free degree distributions, real-world networks exhibit more complex structural patterns, such as hierarchical clustering. This advanced computational exercise  challenges you to move beyond a single model and engage in the practical work of a network scientist: implementing, comparing, and validating different generative mechanisms against a network's empirical properties. This practice emphasizes that model selection is a critical step in accurately capturing the intricate topology of complex systems.",
            "id": "2427984",
            "problem": "You are given three generative mechanisms for biological interaction networks that are widely used to model scale-free structure in computational biology and bioinformatics: the Barabási-Albert preferential attachment mechanism, the Bianconi-Barabási fitness mechanism, and the duplication-divergence mechanism. Your task is to implement these mechanisms, compute the clustering spectrum of the resulting networks, and decide which mechanism best reproduces the clustering spectrum of a provided reference network that serves as a proxy for a metabolic network. Your program must be a complete, runnable program that produces the required outputs, with no external input.\n\nFundamental base and required derivations:\n- A network is represented as a simple undirected graph with an adjacency list. For a node $v$, its degree is $k_v = \\lvert N(v) \\rvert$ where $N(v)$ is the set of its neighbors. The number of triangles incident to $v$ is $t_v$, equal to the number of unordered pairs of neighbors of $v$ that are adjacent to each other. The local clustering coefficient (LCC) at $v$ is defined by\n$$\nC_v = \n\\begin{cases}\n\\frac{2 t_v}{k_v(k_v-1)},  \\text{if } $k_v \\ge 2$, \\\\\n0,  \\text{if } $k_v  2$.\n\\end{cases}\n$$\nThe clustering spectrum $C(k)$ is the degree-resolved mean of $C_v$ across nodes of similar degree. To make this quantity numerically stable and comparable across models, define $B$ logarithmically spaced degree bins spanning from $1$ to $k_{\\max}$ where $k_{\\max}$ is the maximum degree observed in the reference network. Each node with degree $k_v$ contributes its $C_v$ to the bin containing $\\max(1,k_v)$, and the binned spectrum is the mean of $C_v$ within each bin. If a bin receives no nodes for a given model, define its $C$ value as $0$ for that model.\n- The Barabási-Albert model grows a network from an initial connected core by adding one node at a time, attaching $m$ new edges to existing nodes with probability proportional to their degree. The Bianconi-Barabási fitness model generalizes this by assigning each node a fixed positive fitness $\\eta_i$ at its creation, and attaching with probability proportional to $\\eta_i k_i$. The duplication-divergence model starts from a small connected seed; at each step it picks a node $u$ uniformly at random, creates a new node $v$, and for each neighbor $w$ of $u$ independently connects $v$ to $w$ with probability $p$, and additionally connects $u$ and $v$ with probability $q$. If the new node $v$ would be isolated, connect it to $u$ to keep the graph simple and connected.\n- To compare a candidate model to the reference network, compute the mean squared error between their binned clustering spectra:\n$$\n\\mathrm{MSE} = \\frac{1}{B} \\sum_{b=1}^{B} \\left( \\widehat{C}_{\\text{cand}}[b] - \\widehat{C}_{\\text{ref}}[b] \\right)^2,\n$$\nwhere $\\widehat{C}[\\cdot]$ denotes the binned spectrum. For stochastic models, average the binned spectrum over a specified number of independent replicates before computing the error.\n\nImplement the three generators precisely from these definitions, ensuring graphs are simple and undirected, and compute $C_v$ exactly from the definition using triangle counts derived from neighbor intersections.\n\nYour program must evaluate all three models on each test case, compute their errors, and output the index of the model with the smallest error using the tie-breaking rule “smallest index wins.” Use the model index mapping $0 \\mapsto$ Barabási-Albert, $1 \\mapsto$ fitness, and $2 \\mapsto$ duplication-divergence.\n\nReference networks are provided procedurally within each test case, together with seeds and all parameters necessary for reproducibility. All random number generation must be driven by the specified seeds using a reproducible pseudo-random number generator. Angles and physical units are not involved. All probabilities must be treated as real numbers in the unit interval. All outputs must be integers.\n\nTest suite:\n- Case $1$:\n  - Number of nodes: $N = 120$.\n  - Number of bins: $B = 6$.\n  - Replicates per candidate model: $R = 6$.\n  - Candidate base seed: $5000$.\n  - Reference network: duplication-divergence with duplication-retention probability $p = 0.72$, link-back probability $q = 0.30$, seed $1337$.\n  - Candidate model parameters:\n    - Barabási-Albert: $m = 2$.\n    - Fitness: $m = 2$, fitness distribution $\\log\\textnormal{-normal}$ with $\\sigma = 1.0$ and $\\mu = 0$ in log-space.\n    - Duplication-divergence: $p = 0.72$, $q = 0.30$.\n- Case $2$:\n  - Number of nodes: $N = 150$.\n  - Number of bins: $B = 7$.\n  - Replicates per candidate model: $R = 5$.\n  - Candidate base seed: $6000$.\n  - Reference network: Barabási-Albert with $m = 2$, seed $2024$.\n  - Candidate model parameters:\n    - Barabási-Albert: $m = 2$.\n    - Fitness: $m = 2$, fitness distribution Pareto with shape $\\alpha = 2.5$ and scale $1$.\n    - Duplication-divergence: $p = 0.65$, $q = 0.25$.\n- Case $3$:\n  - Number of nodes: $N = 200$.\n  - Number of bins: $B = 7$.\n  - Replicates per candidate model: $R = 4$.\n  - Candidate base seed: $7000$.\n  - Reference network: fitness with $m = 2$, fitness distribution $\\log\\textnormal{-normal}$ with $\\sigma = 1.2$, $\\mu = 0$, seed $7$.\n  - Candidate model parameters:\n    - Barabási-Albert: $m = 2$.\n    - Fitness: $m = 2$, fitness distribution $\\log\\textnormal{-normal}$ with $\\sigma = 1.2$, $\\mu = 0$.\n    - Duplication-divergence: $p = 0.60$, $q = 0.10$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of model indices enclosed in square brackets, in the same order as the test cases, for example, “$[0,1,2]$”. No spaces are allowed inside the brackets.\n\nDeliverable:\n- A single, complete program that, when run, constructs each specified reference network, evaluates the three candidate models using the indicated parameters and seeds, computes the binned clustering spectra and their mean squared errors, selects the winner per test case according to the smallest error with the specified tie-breaking rule, and prints the results in the exact required format. No further text should be printed.",
            "solution": "The problem presented is a well-defined task in computational network biology, requiring the implementation and comparison of three canonical generative models for scale-free networks: the Barabási-Albert ($BA$) model, the Bianconi-Barabási ($BB$) fitness model, and the Duplication-Divergence ($DD$) model. The problem is scientifically grounded, algorithmically specific, and all parameters and procedures for validation are provided. It is therefore deemed a valid problem.\n\nThe solution proceeds by implementing the specified components in a structured, step-by-step manner. For each test case, the primary goal is to determine which of the three candidate models best reproduces the clustering properties of a procedurally generated reference network.\n\nFirst, the core data structure for a network is an adjacency list, implemented as a dictionary mapping each node index (an integer from $0$ to $N-1$) to a set of its neighbors. This choice facilitates efficient neighbor lookups and edge modifications, which are critical for calculating network properties and for the growth algorithms. A graph is kept simple and undirected by ensuring no self-loops and adding each edge $(u,v)$ symmetrically to the adjacency lists of both node $u$ and node $v$.\n\nThe primary metric for comparison is the clustering spectrum, $C(k)$. This requires computing the local clustering coefficient ($LCC$), $C_v$, for each node $v$. Given a node $v$ with degree $k_v = |N(v)|$, where $N(v)$ is the set of neighbors of $v$, the number of triangles $t_v$ incident to $v$ is calculated. This is achieved by iterating through all unique pairs of neighbors of $v$, say $(u,w)$, and checking if an edge exists between them (i.e., if $u \\in N(w)$). The LCC is then given by the formula:\n$$\nC_v = \n\\begin{cases}\n\\frac{2 t_v}{k_v(k_v-1)},  \\text{if } k_v \\ge 2, \\\\\n0,  \\text{if } k_v  2.\n\\end{cases}\n$$\nTo obtain the clustering spectrum, node degrees are partitioned into $B$ logarithmically spaced bins. First, the maximum degree $k_{\\max}$ of the reference network is determined. The bin edges are then defined using `numpy.logspace` from $1$ to $k_{\\max}$ to create $B$ consecutive intervals. For each node $v$, its degree $k_v$ (or $1$ if $k_v=0$) is mapped to one of these bins. The value of the spectrum for a given bin, $\\widehat{C}[b]$, is the average of all $C_v$ values from nodes whose degrees fall into that bin. If a bin is empty, its value is $0$.\n\nThe three generative models are implemented as follows, each driven by a seeded pseudorandom number generator for reproducibility:\n\n$1$. **Barabási-Albert (BA) Model**: The network starts with a small initial core, specifically a complete graph of $m+1$ nodes, to ensure initial degrees are non-zero. The network then grows to the target size $N$ by adding one node at a time. Each new node forms $m$ edges to existing nodes, chosen via preferential attachment. The probability of a new node connecting to an existing node $j$ is proportional to the degree of that node, $k_j$. This is implemented by sampling $m$ distinct nodes without replacement from the existing population, with weights given by their degrees.\n\n$2$. **Bianconi-Barabási (BB) Fitness Model**: This model generalizes the BA mechanism. At the outset, each of the $N$ potential nodes is assigned a fitness value $\\eta_i$ drawn from a specified distribution (e.g., log-normal or Pareto). The network growth process is similar to BA, starting with an initial complete graph of $m+1$ nodes. However, the connection probability for a new node to an existing node $j$ is now proportional to the product of its fitness and degree, $\\eta_j k_j$. Again, $m$ distinct nodes are chosen without replacement based on these weighted probabilities.\n\n$3$. **Duplication-Divergence (DD) Model**: This model simulates gene duplication and subsequent mutation. It starts with a minimal seed network of two nodes connected by an edge. The network grows by iteratively picking an existing node $u$ uniformly at random to serve as a template. A new node $v$ is created, and for each neighbor $w$ of $u$, an edge $(v,w)$ is formed with probability $p$. Additionally, an edge $(u,v)$ is formed with probability $q$. A crucial final step ensures the graph remains connected: if the new node $v$ has no connections after this process, an edge is deterministically created between $v$ and its template $u$.\n\nFor each test case, the overall procedure is as follows:\n$1$. A reference network is generated using its specified model, parameters, and seed.\n$2$. The maximum degree $k_{\\max}$ of the reference network is determined, and the $B$ degree bins are established.\n$3$. The binned clustering spectrum of the reference network, $\\widehat{C}_{\\text{ref}}$, is computed.\n$4$. For each of the three candidate models, $R$ replicate networks are generated. For each replicate, its binned clustering spectrum is computed using the bins defined from the reference network. These spectra are then averaged across the $R$ replicates to produce a single representative spectrum, $\\widehat{C}_{\\text{cand}}$.\n$5$. The performance of each candidate model is quantified by the Mean Squared Error ($MSE$) between its averaged spectrum and the reference spectrum:\n$$\n\\mathrm{MSE} = \\frac{1}{B} \\sum_{b=1}^{B} \\left( \\widehat{C}_{\\text{cand}}[b] - \\widehat{C}_{\\text{ref}}[b] \\right)^2\n$$\n$6$. The model with the lowest $MSE$ is declared the winner for that test case. In case of a tie, the model with the smallest index ($0$ for BA, $1$ for BB, $2$ for DD) wins.\n\nThis entire process is automated in a single Python script. The script iterates through the provided test cases, performs the simulations and comparisons, and outputs the final list of winning model indices in the required format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import defaultdict\n\ndef _calculate_lcc_and_deg(adj, n_nodes):\n    \"\"\"Calculates degrees and local clustering coefficients for all nodes.\"\"\"\n    degrees = {i: len(adj.get(i, set())) for i in range(n_nodes)}\n    lccs = {}\n    for i in range(n_nodes):\n        k_v = degrees[i]\n        if k_v  2:\n            lccs[i] = 0.0\n            continue\n        \n        neighbors = list(adj.get(i, set()))\n        t_v = 0\n        for idx1 in range(len(neighbors)):\n            for idx2 in range(idx1 + 1, len(neighbors)):\n                u, w = neighbors[idx1], neighbors[idx2]\n                if w in adj.get(u, set()):\n                    t_v += 1\n        \n        lccs[i] = (2 * t_v) / (k_v * (k_v - 1))\n        \n    return degrees, lccs\n\ndef compute_binned_spectrum(adj, n_nodes, B, k_max):\n    \"\"\"Computes the binned clustering spectrum for a given network.\"\"\"\n    if k_max = 1:\n        bin_edges = np.array([1.0, 2.0]) # A single bin for k=1\n        effective_B = 1\n    else:\n        bin_edges = np.logspace(0, np.log10(k_max), B + 1)\n        effective_B = B\n\n    degrees, lccs = _calculate_lcc_and_deg(adj, n_nodes)\n    \n    lcc_sum = np.zeros(effective_B)\n    node_count = np.zeros(effective_B)\n    \n    internal_edges = bin_edges[1:-1]\n\n    for i in range(n_nodes):\n        k = degrees[i]\n        c = lccs[i]\n        \n        k_eff = max(1, k)\n        \n        if k_max = 1:\n            bin_idx = 0\n        else:\n            bin_idx = np.digitize(k_eff, internal_edges)\n        \n        lcc_sum[bin_idx] += c\n        node_count[bin_idx] += 1\n        \n    spectrum = np.zeros(effective_B)\n    non_zero_counts = node_count  0\n    spectrum[non_zero_counts] = lcc_sum[non_zero_counts] / node_count[non_zero_counts]\n\n    if effective_B  B:\n        # Pad with zeros if fewer bins were used\n        padded_spectrum = np.zeros(B)\n        padded_spectrum[:effective_B] = spectrum\n        return padded_spectrum\n    \n    return spectrum\n\ndef generate_ba(N, m, seed):\n    \"\"\"Generates a Barabási-Albert network.\"\"\"\n    rng = np.random.default_rng(seed)\n    adj = defaultdict(set)\n    m0 = m + 1\n    \n    if N  m0:\n        return adj\n\n    # Initial complete graph K_{m+1}\n    for i in range(m0):\n        for j in range(i + 1, m0):\n            adj[i].add(j)\n            adj[j].add(i)\n\n    degrees = {i: m for i in range(m0)}\n    node_list = list(range(m0))\n    \n    for i in range(m0, N):\n        current_nodes = np.array(node_list)\n        current_degrees = np.array([degrees[node] for node in current_nodes])\n        \n        total_degree = np.sum(current_degrees)\n        if total_degree == 0:\n            targets = rng.choice(current_nodes, size=m, replace=False)\n        else:\n            probs = current_degrees / total_degree\n            targets = rng.choice(current_nodes, size=m, replace=False, p=probs)\n        \n        adj[i] = set(targets)\n        for target in targets:\n            adj[target].add(i)\n        \n        degrees[i] = m\n        for target in targets:\n            degrees[target] += 1\n        node_list.append(i)\n        \n    return adj\n\ndef generate_bb(N, m, fitness, seed):\n    \"\"\"Generates a Bianconi-Barabási fitness network.\"\"\"\n    rng = np.random.default_rng(seed)\n    adj = defaultdict(set)\n    m0 = m + 1\n    \n    if N  m0:\n        return adj\n\n    for i in range(m0):\n        for j in range(i + 1, m0):\n            adj[i].add(j)\n            adj[j].add(i)\n\n    degrees = {i: m for i in range(m0)}\n    node_list = list(range(m0))\n\n    for i in range(m0, N):\n        current_nodes = np.array(node_list)\n        current_degrees = np.array([degrees[node] for node in current_nodes])\n        current_fitness = np.array([fitness[node] for node in current_nodes])\n\n        weights = current_degrees * current_fitness\n        total_weight = np.sum(weights)\n\n        if total_weight == 0:\n            targets = rng.choice(current_nodes, size=m, replace=False)\n        else:\n            probs = weights / total_weight\n            targets = rng.choice(current_nodes, size=m, replace=False, p=probs)\n        \n        adj[i] = set(targets)\n        for target in targets:\n            adj[target].add(i)\n        \n        degrees[i] = m\n        for target in targets:\n            degrees[target] += 1\n        node_list.append(i)\n        \n    return adj\n\ndef generate_dd(N, p, q, seed):\n    \"\"\"Generates a Duplication-Divergence network.\"\"\"\n    rng = np.random.default_rng(seed)\n    adj = defaultdict(set)\n    \n    if N  2:\n        return adj\n    \n    # Initial seed: 2 nodes, 1 edge\n    adj[0].add(1)\n    adj[1].add(0)\n    \n    nodes = [0, 1]\n    for i in range(2, N):\n        template_node = rng.choice(nodes)\n        \n        new_neighbors = set()\n        \n        # Divergence from template's neighbors\n        for neighbor in adj[template_node]:\n            if rng.random()  p:\n                new_neighbors.add(neighbor)\n        \n        # Link-back to template\n        if rng.random()  q:\n            new_neighbors.add(template_node)\n        \n        # Check for isolation\n        if not new_neighbors:\n            adj[i].add(template_node)\n            adj[template_node].add(i)\n        else:\n            adj[i] = new_neighbors\n            for neighbor in new_neighbors:\n                adj[neighbor].add(i)\n        nodes.append(i)\n        \n    return adj\n\ndef solve():\n    test_cases = [\n        {\n            \"N\": 120, \"B\": 6, \"R\": 6, \"candidate_base_seed\": 5000,\n            \"ref_model\": \"dd\", \"ref_params\": {\"p\": 0.72, \"q\": 0.30}, \"ref_seed\": 1337,\n            \"cand_params\": [\n                {\"model\": \"ba\", \"m\": 2},\n                {\"model\": \"bb\", \"m\": 2, \"dist\": \"lognormal\", \"dist_params\": {\"mean\": 0, \"sigma\": 1.0}},\n                {\"model\": \"dd\", \"p\": 0.72, \"q\": 0.30},\n            ]\n        },\n        {\n            \"N\": 150, \"B\": 7, \"R\": 5, \"candidate_base_seed\": 6000,\n            \"ref_model\": \"ba\", \"ref_params\": {\"m\": 2}, \"ref_seed\": 2024,\n            \"cand_params\": [\n                {\"model\": \"ba\", \"m\": 2},\n                {\"model\": \"bb\", \"m\": 2, \"dist\": \"pareto\", \"dist_params\": {\"a\": 2.5, \"scale\": 1}},\n                {\"model\": \"dd\", \"p\": 0.65, \"q\": 0.25},\n            ]\n        },\n        {\n            \"N\": 200, \"B\": 7, \"R\": 4, \"candidate_base_seed\": 7000,\n            \"ref_model\": \"bb\", \"ref_params\": {\"m\": 2, \"dist\": \"lognormal\", \"dist_params\": {\"mean\": 0, \"sigma\": 1.2}}, \"ref_seed\": 7,\n            \"cand_params\": [\n                {\"model\": \"ba\", \"m\": 2},\n                {\"model\": \"bb\", \"m\": 2, \"dist\": \"lognormal\", \"dist_params\": {\"mean\": 0, \"sigma\": 1.2}},\n                {\"model\": \"dd\", \"p\": 0.60, \"q\": 0.10},\n            ]\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        N, B, R = case[\"N\"], case[\"B\"], case[\"R\"]\n        \n        # Generate reference network\n        if case[\"ref_model\"] == \"dd\":\n            ref_adj = generate_dd(N, case[\"ref_params\"][\"p\"], case[\"ref_params\"][\"q\"], case[\"ref_seed\"])\n        elif case[\"ref_model\"] == \"ba\":\n            ref_adj = generate_ba(N, case[\"ref_params\"][\"m\"], case[\"ref_seed\"])\n        else: # bb\n            rng_fit = np.random.default_rng(case[\"ref_seed\"])\n            if case[\"ref_params\"][\"dist\"] == \"lognormal\":\n                fitness = rng_fit.lognormal(mean=case[\"ref_params\"][\"dist_params\"][\"mean\"], sigma=case[\"ref_params\"][\"dist_params\"][\"sigma\"], size=N)\n            adj = generate_bb(N, case[\"ref_params\"][\"m\"], fitness, case[\"ref_seed\"])\n            ref_adj = adj\n\n        ref_degrees, _ = _calculate_lcc_and_deg(ref_adj, N)\n        k_max = max(ref_degrees.values()) if ref_degrees else 0\n        ref_spectrum = compute_binned_spectrum(ref_adj, N, B, k_max)\n\n        mse_scores = []\n        for model_idx, params in enumerate(case[\"cand_params\"]):\n            avg_cand_spectrum = np.zeros(B)\n            \n            # Pre-generate fitness values for BB model if needed\n            if params[\"model\"] == \"bb\":\n                rng_fit = np.random.default_rng(case[\"candidate_base_seed\"])\n                if params[\"dist\"] == \"lognormal\":\n                    fitness_vals = rng_fit.lognormal(mean=params[\"dist_params\"][\"mean\"], sigma=params[\"dist_params\"][\"sigma\"], size=N)\n                elif params[\"dist\"] == \"pareto\":\n                    fitness_vals = (rng_fit.pareto(a=params[\"dist_params\"][\"a\"], size=N) + 1) * params[\"dist_params\"][\"scale\"]\n\n            for rep_idx in range(R):\n                seed = case[\"candidate_base_seed\"] + model_idx * R + rep_idx\n                if params[\"model\"] == \"ba\":\n                    cand_adj = generate_ba(N, params[\"m\"], seed)\n                elif params[\"model\"] == \"bb\":\n                    cand_adj = generate_bb(N, params[\"m\"], fitness_vals, seed)\n                else: # dd\n                    cand_adj = generate_dd(N, params[\"p\"], params[\"q\"], seed)\n\n                cand_spectrum = compute_binned_spectrum(cand_adj, N, B, k_max)\n                avg_cand_spectrum += cand_spectrum\n            \n            avg_cand_spectrum /= R\n            \n            mse = np.mean((avg_cand_spectrum - ref_spectrum)**2)\n            mse_scores.append(mse)\n            \n        winner_idx = np.argmin(mse_scores)\n        final_results.append(winner_idx)\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}