{
    "hands_on_practices": [
        {
            "introduction": "The Erdős–Rényi model serves as the canonical starting point for network science, representing a \"null model\" where edges are formed purely at random. Before exploring more complex structures, it is essential to understand the statistical properties of this fundamental baseline. This practice  will guide you through a first-principles derivation of the mean degree and its variance, building intuition for how network size and connection probability shape the degree distribution.",
            "id": "4272033",
            "problem": "Consider an undirected simple graph generated by the Erdős–Rényi (ER) model $G(n,p)$ with $n$ labeled nodes, where each of the $\\binom{n}{2}$ possible undirected edges is present independently with probability $p \\in (0,1)$. Choose a node uniformly at random and let its degree be denoted by $k$. Using only the definitions of degree, independence of edge indicators, and properties of sums of independent Bernoulli random variables, derive the exact expressions for the expected degree $\\langle k \\rangle$ and the degree variance $\\mathrm{Var}(k)$ as functions of $n$ and $p$. Then, define the degree fluctuation scale as the standard deviation $\\sigma_{k} = \\sqrt{\\mathrm{Var}(k)}$ and interpret how $\\sigma_{k}$ and the relative fluctuations $\\sigma_{k}/\\langle k \\rangle$ scale with $n$ and $p$ in scientifically relevant regimes where $p$ is fixed in $(0,1)$ (dense), and where $p$ scales as $p = c/(n-1)$ for a constant $c \\in (0,\\infty)$ (sparse). Express the final answer as the ordered pair of the exact closed-form analytic expressions for $\\langle k \\rangle$ and $\\mathrm{Var}(k)$. No numerical evaluation is required.",
            "solution": "The Erdős–Rényi (ER) model $G(n,p)$ defines an undirected simple graph with $n$ nodes in which each pair of distinct nodes is connected independently with probability $p$. Let the nodes be indexed by $i \\in \\{1,\\dots,n\\}$, and choose a node $i$ uniformly at random. For every $j \\neq i$, define the edge indicator random variable $X_{ij}$ by $X_{ij} = 1$ if the undirected edge between $i$ and $j$ is present, and $X_{ij} = 0$ otherwise. By construction, for fixed $i$, the family $\\{X_{ij}\\}_{j \\neq i}$ is independent and identically distributed with $X_{ij} \\sim \\mathrm{Bernoulli}(p)$, and there are exactly $n-1$ such indicators.\n\nBy the definition of degree, the degree $k$ of node $i$ is given by\n$$\nk \\;=\\; \\sum_{j \\neq i} X_{ij}.\n$$\nThis is a sum of $n-1$ independent Bernoulli random variables with common success probability $p$. Therefore, $k$ has a binomial distribution,\n$$\nk \\sim \\mathrm{Binomial}(n-1, p).\n$$\nWe now compute the expectation and variance from first principles using linearity of expectation and independence.\n\nFor the expectation,\n$$\n\\langle k \\rangle \\;=\\; \\left\\langle \\sum_{j \\neq i} X_{ij} \\right\\rangle \\;=\\; \\sum_{j \\neq i} \\langle X_{ij} \\rangle \\;=\\; \\sum_{j \\neq i} p \\;=\\; (n-1)\\, p.\n$$\n\nFor the variance, use independence to sum the variances:\n$$\n\\mathrm{Var}(k) \\;=\\; \\mathrm{Var}\\!\\left(\\sum_{j \\neq i} X_{ij}\\right) \\;=\\; \\sum_{j \\neq i} \\mathrm{Var}(X_{ij}) \\;=\\; \\sum_{j \\neq i} p(1-p) \\;=\\; (n-1)\\, p(1-p).\n$$\nThus, the fluctuation scale defined by the standard deviation is\n$$\n\\sigma_{k} \\;=\\; \\sqrt{\\mathrm{Var}(k)} \\;=\\; \\sqrt{(n-1)\\, p(1-p)}.\n$$\n\nWe next interpret the scaling of fluctuations with $n$ and $p$ by inspecting $\\sigma_{k}$ and the relative fluctuations $\\sigma_{k}/\\langle k \\rangle$ in two regimes.\n\nDense regime with fixed $p \\in (0,1)$ independent of $n$: Here,\n$$\n\\langle k \\rangle \\;=\\; (n-1)\\, p \\;\\sim\\; n\\, p, \n\\quad \\sigma_{k} \\;=\\; \\sqrt{(n-1)\\, p(1-p)} \\;\\sim\\; \\sqrt{n}\\, \\sqrt{p(1-p)}.\n$$\nThe relative fluctuations scale as\n$$\n\\frac{\\sigma_{k}}{\\langle k \\rangle} \\;=\\; \\frac{\\sqrt{(n-1)\\, p(1-p)}}{(n-1)\\, p} \\;=\\; \\sqrt{\\frac{1-p}{(n-1)\\, p}} \\;\\sim\\; \\frac{\\mathrm{const}}{\\sqrt{n}},\n$$\nso they vanish as $n$ grows. This indicates self-averaging of degrees in the dense ER model: degrees concentrate around their mean, and the coefficient of variation decays as $n^{-1/2}$.\n\nSparse regime with $p = \\frac{c}{n-1}$ for a constant $c \\in (0,\\infty)$: Here,\n$$\n\\langle k \\rangle \\;=\\; (n-1)\\, \\frac{c}{n-1} \\;=\\; c,\n\\quad \n\\mathrm{Var}(k) \\;=\\; (n-1)\\, \\frac{c}{n-1}\\!\\left(1-\\frac{c}{n-1}\\right) \\;=\\; c \\left(1-\\frac{c}{n-1}\\right),\n$$\nand\n$$\n\\sigma_{k} \\;=\\; \\sqrt{c \\left(1-\\frac{c}{n-1}\\right)} \\;\\xrightarrow[n \\to \\infty]{} \\sqrt{c}.\n$$\nThe relative fluctuations are\n$$\n\\frac{\\sigma_{k}}{\\langle k \\rangle} \\;=\\; \\frac{\\sqrt{c \\left(1-\\frac{c}{n-1}\\right)}}{c} \\;=\\; \\frac{1}{\\sqrt{c}} \\sqrt{1-\\frac{c}{n-1}} \\;\\xrightarrow[n \\to \\infty]{} \\frac{1}{\\sqrt{c}},\n$$\nwhich remain $O(1)$ as $n$ grows, reflecting that in the sparse regime the degrees do not self-average in the same sense; they exhibit fluctuations of the same order as their mean. In the canonical sparse scaling with $p = \\frac{c}{n-1}$, the degree distribution converges to a Poisson distribution with mean $c$, consistent with the limiting variance and fluctuation scaling.\n\nIn summary, by first-principles derivation from the independent Bernoulli edge indicators in $G(n,p)$, the exact formulas are $\\langle k \\rangle = (n-1)\\, p$ and $\\mathrm{Var}(k) = (n-1)\\, p(1-p)$, with fluctuation scaling given by $\\sigma_{k} = \\sqrt{(n-1)\\, p(1-p)}$ and relative fluctuations decaying as $n^{-1/2}$ in the dense regime but remaining $O(1)$ in the sparse regime $p = c/(n-1)$.",
            "answer": "$$\\boxed{\\begin{pmatrix}(n-1)\\, p  (n-1)\\, p(1-p)\\end{pmatrix}}$$"
        },
        {
            "introduction": "Many real-world networks exhibit a \"scale-free\" property, with highly heterogeneous degree distributions starkly different from the homogeneous Erdős–Rényi model. The preferential attachment mechanism provides a classic explanation for the emergence of this feature, linking network growth to a \"rich-get-richer\" phenomenon. This exercise  delves into the powerful master equation method to analytically derive the famous power-law degree distribution that results from this growth process.",
            "id": "4271972",
            "problem": "Consider an undirected network growth process defined as follows. Start from an arbitrary finite connected seed with $N_{0}$ nodes and $M_{0}$ edges. At each discrete time step $t \\mapsto t+1$, add one new node and attach $m \\in \\mathbb{N}$ new edges from this node to $m$ distinct pre-existing nodes. The attachment is linear preferential attachment with initial attractiveness: a pre-existing node of degree $k$ is chosen for attachment with probability proportional to $k + \\delta$, where $\\delta  -m$ is a fixed attractiveness parameter ensuring nonnegative attachment weights for all nodes.\n\nLet $N_{k}(t)$ denote the expected number of nodes of degree $k$ at time $t$, and define the stationary degree distribution $P_{k}$ as the large-time limit $P_{k} = \\lim_{t \\to \\infty} \\frac{N_{k}(t)}{t}$, assuming this limit exists. Using only fundamental definitions of degree, conservation of edges in undirected networks, and the stated attachment mechanism, perform the following:\n\n- Derive the master equation governing the evolution of $N_{k}(t)$ for $k \\geq m$.\n- From this master equation, determine the normalization factor for the attachment probabilities in terms of $t$ and show how it behaves asymptotically.\n- Use these results to obtain the asymptotic form of $P_{k}$ for large $k$, demonstrating that it is a power law $P_{k} \\sim k^{-\\gamma}$, and express the exponent $\\gamma$ as a function of $m$ and $\\delta$.\n\nProvide your final answer as a single closed-form expression in terms of $m$ and $\\delta$. No numerical approximation or rounding is required.",
            "solution": "The problem asks for the derivation of the master equation for a network growth process, an analysis of the attachment probability normalization, and the derivation of the power-law exponent $\\gamma$ for the stationary degree distribution $P_k$. We proceed by first validating the problem and then providing a step-by-step derivation.\n\nThe problem statement describes a well-defined network growth model, a variant of the Barabási-Albert model with an initial attractiveness parameter $\\delta$. The components of the problem—the master equation formalism, the concept of a stationary degree distribution, and the asymptotic power-law behavior—are standard and central topics in the field of complex systems and network science. The problem is scientifically grounded, well-posed, and all necessary parameters and conditions are provided for an asymptotic analysis. The condition $\\delta  -m$ ensures that for any node of degree $k \\geq m$, the attachment weight $k+\\delta$ is positive. The problem is thus deemed valid.\n\nWe begin by establishing the properties of the network at time $t$. The network starts with $N_0$ nodes and $M_0$ edges. At each time step, one node and $m$ edges are added.\nThe total number of nodes at time $t$ is $N(t) = N_0 + t$.\nThe total number of edges at time $t$ is $M(t) = M_0 + mt$.\nFor an undirected network, the sum of all node degrees is twice the number of edges: $\\sum_{i} k_i(t) = 2M(t) = 2(M_0 + mt)$.\n\nThe attachment probability for a new node to connect to a pre-existing node $i$ with degree $k_i$ is given as proportional to $k_i + \\delta$. The normalized probability is:\n$$\n\\Pi_i = \\frac{k_i + \\delta}{\\sum_{j} (k_j + \\delta)}\n$$\nThe denominator is the normalization factor, which we denote as $Z(t)$. We can express $Z(t)$ in terms of the network's macroscopic properties:\n$$\nZ(t) = \\sum_{j \\in V(t)} (k_j(t) + \\delta) = \\sum_{j} k_j(t) + \\sum_{j} \\delta = 2M(t) + \\delta N(t)\n$$\nSubstituting the expressions for $N(t)$ and $M(t)$:\n$$\nZ(t) = 2(M_0 + mt) + \\delta(N_0 + t) = (2m + \\delta)t + (2M_0 + \\delta N_0)\n$$\nFor large times $t \\to \\infty$, the constant term becomes negligible, and the normalization factor behaves asymptotically as:\n$$\nZ(t) \\sim (2m + \\delta)t\n$$\nThis completes the second part of the problem's request.\n\nNext, we derive the master equation for $N_k(t)$, the expected number of nodes of degree $k$ at time $t$, for $k \\geq m$. We use a continuum-time approximation, where $N_k(t+1) - N_k(t) \\approx \\frac{dN_k(t)}{dt}$. The change in $N_k$ is due to two processes for $km$:\n1.  An increase in $N_k$ when a node of degree $k-1$ is chosen for attachment, increasing its degree to $k$.\n2.  A decrease in $N_k$ when a node of degree $k$ is chosen for attachment, increasing its degree to $k+1$.\n\nThe probability that one of the $m$ new edges attaches to any of the $N_{k-1}(t)$ nodes of degree $k-1$ is given by $m \\times \\frac{N_{k-1}(t)((k-1)+\\delta)}{Z(t)}$. This is the rate of increase of $N_k$ due to this process.\nSimilarly, the rate of decrease of $N_k$ is the rate at which nodes of degree $k$ are chosen for attachment: $m \\times \\frac{N_k(t)(k+\\delta)}{Z(t)}$.\n\nThus, for $k  m$, the master equation is:\n$$\n\\frac{dN_k}{dt} = m \\frac{(k-1+\\delta)N_{k-1}(t)}{Z(t)} - m \\frac{(k+\\delta)N_k(t)}{Z(t)}\n$$\nFor the special case $k=m$, we must also account for the new node introduced at each time step, which has degree $m$. This adds a source term of $1$ to the rate equation for $N_m$. Nodes of degree less than $m$ are not created in the process (for $t0$), so there is no inflow to the $k=m$ state from a lower degree state. The master equation for $k = m$ is:\n$$\n\\frac{dN_m}{dt} = 1 - m \\frac{(m+\\delta)N_m(t)}{Z(t)}\n$$\nThese two equations constitute the master equation for $k \\geq m$, which was the first part of the problem's request.\n\nTo find the stationary degree distribution $P_k = \\lim_{t \\to \\infty} \\frac{N_k(t)}{t}$, we assume a scaling solution for large $t$ of the form $N_k(t) = P_k t$. Note that $\\lim_{t \\to \\infty} \\frac{N_k(t)}{N(t)} = \\lim_{t \\to \\infty} \\frac{P_k t}{N_0 + t} = P_k$.\nDifferentiating with respect to $t$ gives $\\frac{dN_k}{dt} = P_k$.\nSubstituting this ansatz and the asymptotic form $Z(t) \\sim (2m+\\delta)t$ into the master equation for $km$:\n$$\nP_k = m \\frac{(k-1+\\delta)P_{k-1} t}{(2m+\\delta)t} - m \\frac{(k+\\delta)P_k t}{(2m+\\delta)t}\n$$\n$$\nP_k = \\frac{m}{2m+\\delta} \\left[ (k-1+\\delta)P_{k-1} - (k+\\delta)P_k \\right]\n$$\nWe can rearrange this to find a recurrence relation for $P_k$:\n$$\nP_k \\left( 1 + \\frac{m(k+\\delta)}{2m+\\delta} \\right) = \\frac{m(k-1+\\delta)}{2m+\\delta} P_{k-1}\n$$\n$$\nP_k \\left( \\frac{2m+\\delta + mk + m\\delta}{2m+\\delta} \\right) = \\frac{m(k-1+\\delta)}{2m+\\delta} P_{k-1}\n$$\n$$\nP_k \\left( mk + 2m + \\delta + m\\delta \\right) = m(k-1+\\delta)P_{k-1}\n$$\nThis gives the ratio:\n$$\n\\frac{P_k}{P_{k-1}} = \\frac{m(k-1+\\delta)}{mk + 2m + \\delta + m\\delta} = \\frac{k-1+\\delta}{k + 2 + \\frac{\\delta}{m} + \\delta}\n$$\nWe are interested in the asymptotic behavior for large $k$, where $P_k \\sim k^{-\\gamma}$. For such a power-law, we can analyze the ratio $P_k/P_{k-1}$:\n$$\n\\frac{P_k}{P_{k-1}} \\sim \\frac{k^{-\\gamma}}{(k-1)^{-\\gamma}} = \\left(\\frac{k-1}{k}\\right)^\\gamma = \\left(1 - \\frac{1}{k}\\right)^\\gamma\n$$\nUsing the binomial expansion $(1-x)^a \\approx 1-ax$ for large $k$ (small $1/k$):\n$$\n\\frac{P_k}{P_{k-1}} \\approx 1 - \\frac{\\gamma}{k}\n$$\nNow, we expand the ratio from our recurrence relation for large $k$:\n$$\n\\frac{P_k}{P_{k-1}} = \\frac{k-(1-\\delta)}{k+(2+\\delta+\\frac{\\delta}{m})} = \\frac{1-\\frac{1-\\delta}{k}}{1+\\frac{2+\\delta+\\delta/m}{k}}\n$$\nUsing the expansion $(1+x)^{-1} \\approx 1-x$:\n$$\n\\frac{P_k}{P_{k-1}} \\approx \\left( 1 - \\frac{1-\\delta}{k} \\right) \\left( 1 - \\frac{2+\\delta+\\frac{\\delta}{m}}{k} \\right)\n$$\nExpanding and keeping terms up to order $k^{-1}$:\n$$\n\\frac{P_k}{P_{k-1}} \\approx 1 - \\frac{1-\\delta}{k} - \\frac{2+\\delta+\\frac{\\delta}{m}}{k} = 1 - \\frac{1-\\delta+2+\\delta+\\frac{\\delta}{m}}{k} = 1 - \\frac{3+\\frac{\\delta}{m}}{k}\n$$\nAn alternative, more robust method is to use logarithms. For large $k$, $\\ln(P_k/P_{k-1}) \\approx \\ln(1+\\gamma/k) \\approx \\gamma/k$. No, it should be $\\ln(1-\\frac{1}{k})^{-\\gamma} = -\\gamma \\ln(1-\\frac{1}{k}) \\approx \\frac{\\gamma}{k}$. The other way: $\\ln(P_{k-1}/P_k) \\approx \\gamma \\ln(k/(k-1)) = \\gamma \\ln(1+1/(k-1)) \\approx \\gamma/(k-1) \\approx \\gamma/k$.\nLet's analyze $\\ln(P_{k-1}/P_k)$ from our recurrence:\n$$\n\\frac{P_{k-1}}{P_k} = \\frac{k + 2+\\delta+\\frac{\\delta}{m}}{k-1+\\delta}\n$$\n$$\n\\ln\\left(\\frac{P_{k-1}}{P_k}\\right) = \\ln\\left(k + 2+\\delta+\\frac{\\delta}{m}\\right) - \\ln(k-1+\\delta)\n$$\nUsing $\\ln(k+a) = \\ln(k(1+a/k)) = \\ln k + \\ln(1+a/k) \\approx \\ln k + a/k$:\n$$\n\\ln\\left(\\frac{P_{k-1}}{P_k}\\right) \\approx \\left(\\ln k + \\frac{2+\\delta+\\frac{\\delta}{m}}{k}\\right) - \\left(\\ln k + \\frac{\\delta-1}{k}\\right)\n$$\n$$\n\\ln\\left(\\frac{P_{k-1}}{P_k}\\right) \\approx \\frac{2+\\delta+\\frac{\\delta}{m} - (\\delta-1)}{k} = \\frac{3+\\frac{\\delta}{m}}{k}\n$$\nComparing $\\ln(P_{k-1}/P_k) \\approx \\gamma/k$ with our derived expression, we find:\n$$\n\\gamma = 3 + \\frac{\\delta}{m}\n$$\nThis is the exponent of the power-law degree distribution for large $k$. This result correctly recovers the standard Barabási-Albert model exponent $\\gamma=3$ when the initial attractiveness is zero ($\\delta=0$).",
            "answer": "$$\\boxed{3 + \\frac{\\delta}{m}}$$"
        },
        {
            "introduction": "After identifying different theoretical models that generate distinct degree distributions, a critical task for the network scientist is to determine which model best describes empirical data. This practice  moves from theory to application, introducing the rigorous framework of maximum likelihood estimation for model comparison. You will derive the estimators for power-law and log-normal distributions and uncover a fundamental challenge in distinguishing between these heavy-tailed distributions in finite samples.",
            "id": "4271979",
            "problem": "You observe the upper-tail degrees of a large undirected network under the standard continuous-tail approximation commonly used in complex systems and network science. Specifically, let $k_{1},\\dots,k_{n}$ be independent and identically distributed realizations of node degrees satisfying $k_{i}\\geq k_{\\min}0$, obtained by discarding all observations below the threshold $k_{\\min}$. You wish to compare two competing models for the tail degree distribution:\n\n1. A continuous power-law tail on $[k_{\\min},\\infty)$ with parameter $\\alpha1$, having density $f_{\\mathrm{PL}}(k\\mid \\alpha,k_{\\min})$ supported on $k\\geq k_{\\min}$.\n\n2. A log-normal model on $(0,\\infty)$ with parameters $\\mu\\in\\mathbb{R}$ and $\\sigma0$, having density $f_{\\mathrm{LN}}(k\\mid \\mu,\\sigma)$ supported on $k0$.\n\nAssume independent observations and adopt Maximum Likelihood Estimation (MLE). Starting only from the fundamental definition of the likelihood for independent samples and the model densities, complete the following tasks:\n\n(a) Write down the explicit forms of $f_{\\mathrm{PL}}(k\\mid \\alpha,k_{\\min})$ on $[k_{\\min},\\infty)$ and $f_{\\mathrm{LN}}(k\\mid \\mu,\\sigma)$ on $(0,\\infty)$. Using these, derive the log-likelihood functions $\\ell_{\\mathrm{PL}}(\\alpha)$ and $\\ell_{\\mathrm{LN}}(\\mu,\\sigma)$ for the observed sample $\\{k_{i}\\}_{i=1}^{n}$.\n\n(b) Derive the MLE $\\hat{\\alpha}$ for the power-law model and the MLEs $(\\hat{\\mu},\\hat{\\sigma})$ for the log-normal model.\n\n(c) Introduce the statistics $S_{0}=\\sum_{i=1}^{n}\\ln k_{i}$, $S_{1}=\\sum_{i=1}^{n}\\ln\\!\\big(k_{i}/k_{\\min}\\big)=S_{0}-n\\ln k_{\\min}$, and $s^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(\\ln k_{i}-\\bar{y})^{2}$, where $\\bar{y}=\\frac{1}{n}\\sum_{i=1}^{n}\\ln k_{i}$. Express the maximized log-likelihoods $\\ell_{\\mathrm{PL}}(\\hat{\\alpha})$ and $\\ell_{\\mathrm{LN}}(\\hat{\\mu},\\hat{\\sigma})$ in terms of $n$, $k_{\\min}$, $S_{0}$, $S_{1}$, and $s^{2}$.\n\n(d) Define the log-likelihood ratio statistic $\\Lambda=2\\big(\\ell_{\\mathrm{PL}}(\\hat{\\alpha})-\\ell_{\\mathrm{LN}}(\\hat{\\mu},\\hat{\\sigma})\\big)$. Provide a single closed-form analytic expression for $\\Lambda$ in terms of $n$, $k_{\\min}$, $S_{1}$, and $s$ only. This is the only quantity you must submit as your final answer.\n\n(e) Discuss, from first principles and without numerical computation, an identifiability challenge that arises when attempting to distinguish between the power-law and log-normal tails in heavy-tailed samples. Your discussion should be grounded in the asymptotic behavior of the log-normal tail and how it can approximate a power-law over limited ranges of $k$ in finite samples.\n\nYour final answer must be the expression for $\\Lambda$ in part (d). No numerical evaluation or rounding is required.",
            "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem in statistical inference as applied to complex network analysis. All givens are consistent and sufficient for a unique solution.\n\n(a) Derivation of Densities and Log-Likelihoods\n\nFirst, we derive the normalized probability density function (PDF) for the continuous power-law model. The density $f_{\\mathrm{PL}}(k)$ is proportional to $k^{-\\alpha}$ for $k \\geq k_{\\min}$. We find the normalization constant $C$ by integrating the density from $k_{\\min}$ to $\\infty$ and setting the result to $1$.\n$$ \\int_{k_{\\min}}^{\\infty} C k^{-\\alpha} \\, dk = 1 $$\nThe integral requires $\\alpha  1$ for convergence.\n$$ C \\left[ \\frac{k^{1-\\alpha}}{1-\\alpha} \\right]_{k_{\\min}}^{\\infty} = C \\left( 0 - \\frac{k_{\\min}^{1-\\alpha}}{1-\\alpha} \\right) = C \\frac{k_{\\min}^{1-\\alpha}}{\\alpha-1} = 1 $$\nSolving for $C$ gives $C = (\\alpha-1) k_{\\min}^{\\alpha-1}$. Therefore, the power-law PDF is:\n$$ f_{\\mathrm{PL}}(k \\mid \\alpha, k_{\\min}) = (\\alpha-1)k_{\\min}^{\\alpha-1} k^{-\\alpha}, \\quad k \\geq k_{\\min} $$\nThe log-normal PDF for a random variable $k$ is defined by the property that $\\ln k$ is normally distributed with mean $\\mu$ and variance $\\sigma^2$. Its standard form is:\n$$ f_{\\mathrm{LN}}(k \\mid \\mu, \\sigma) = \\frac{1}{k \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right), \\quad k  0 $$\n\nNext, we derive the log-likelihood functions. For a sample of $n$ independent and identically distributed observations $\\{k_i\\}_{i=1}^n$, the likelihood is the product of the individual probabilities, $L = \\prod_{i=1}^n f(k_i)$. The log-likelihood $\\ell$ is the natural logarithm of the likelihood, $\\ell = \\ln L = \\sum_{i=1}^n \\ln f(k_i)$.\n\nFor the power-law model, the log-likelihood function $\\ell_{\\mathrm{PL}}(\\alpha)$ is:\n$$ \\ell_{\\mathrm{PL}}(\\alpha) = \\sum_{i=1}^{n} \\ln\\left( (\\alpha-1)k_{\\min}^{\\alpha-1} k_i^{-\\alpha} \\right) $$\n$$ \\ell_{\\mathrm{PL}}(\\alpha) = \\sum_{i=1}^{n} \\left( \\ln(\\alpha-1) + (\\alpha-1)\\ln k_{\\min} - \\alpha\\ln k_i \\right) $$\n$$ \\ell_{\\mathrm{PL}}(\\alpha) = n\\ln(\\alpha-1) + n(\\alpha-1)\\ln k_{\\min} - \\alpha \\sum_{i=1}^{n} \\ln k_i $$\n\nFor the log-normal model, the log-likelihood function $\\ell_{\\mathrm{LN}}(\\mu, \\sigma)$ is:\n$$ \\ell_{\\mathrm{LN}}(\\mu, \\sigma) = \\sum_{i=1}^{n} \\ln\\left( \\frac{1}{k_i \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k_i - \\mu)^2}{2\\sigma^2}\\right) \\right) $$\n$$ \\ell_{\\mathrm{LN}}(\\mu, \\sigma) = \\sum_{i=1}^{n} \\left( -\\ln k_i - \\ln\\sigma - \\frac{1}{2}\\ln(2\\pi) - \\frac{(\\ln k_i - \\mu)^2}{2\\sigma^2} \\right) $$\n$$ \\ell_{\\mathrm{LN}}(\\mu, \\sigma) = -\\sum_{i=1}^{n} \\ln k_i - n\\ln\\sigma - \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (\\ln k_i - \\mu)^2 $$\n\n(b) Derivation of Maximum Likelihood Estimators (MLEs)\n\nTo find the MLE $\\hat{\\alpha}$, we differentiate $\\ell_{\\mathrm{PL}}(\\alpha)$ with respect to $\\alpha$ and set the result to zero:\n$$ \\frac{d\\ell_{\\mathrm{PL}}}{d\\alpha} = \\frac{n}{\\alpha-1} + n\\ln k_{\\min} - \\sum_{i=1}^{n} \\ln k_i = 0 $$\n$$ \\frac{n}{\\alpha-1} = \\sum_{i=1}^{n} \\ln k_i - n\\ln k_{\\min} = \\sum_{i=1}^{n} (\\ln k_i - \\ln k_{\\min}) = \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right) $$\nSolving for $\\alpha$ gives the MLE $\\hat{\\alpha}$:\n$$ \\hat{\\alpha} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)} $$\n\nTo find the MLEs $(\\hat{\\mu}, \\hat{\\sigma})$, we take the partial derivatives of $\\ell_{\\mathrm{LN}}(\\mu, \\sigma)$ with respect to $\\mu$ and $\\sigma$ and set them to zero.\n$$ \\frac{\\partial\\ell_{\\mathrm{LN}}}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(\\ln k_i - \\mu)(-1) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(\\ln k_i - \\mu) = 0 $$\nThis implies $\\sum_{i=1}^{n} \\ln k_i - n\\mu = 0$. Solving for $\\mu$ gives the MLE $\\hat{\\mu}$:\n$$ \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} \\ln k_i $$\nNow, for $\\sigma$:\n$$ \\frac{\\partial\\ell_{\\mathrm{LN}}}{\\partial \\sigma} = -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3}\\sum_{i=1}^{n}(\\ln k_i - \\mu)^2 = 0 $$\n$$ n\\sigma^2 = \\sum_{i=1}^{n}(\\ln k_i - \\mu)^2 $$\nSubstituting $\\hat{\\mu}$ for $\\mu$ and solving for $\\sigma^2$ gives the MLE $\\hat{\\sigma}^2$:\n$$ \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(\\ln k_i - \\hat{\\mu})^2 $$\nSo, $\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\ln k_i - \\hat{\\mu})^2}$.\n\n(c) Maximized Log-Likelihoods\n\nWe now express the maximized log-likelihoods in terms of the given statistics: $S_{0}=\\sum_{i=1}^{n}\\ln k_{i}$, $S_{1}=\\sum_{i=1}^{n}\\ln(k_{i}/k_{\\min})$, $\\bar{y}=S_0/n$, and $s^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(\\ln k_{i}-\\bar{y})^{2}$.\n\nFrom part (b), $\\hat{\\alpha} = 1 + n/S_1$, so $\\hat{\\alpha}-1 = n/S_1$. We can simplify the expression for $\\ell_{\\mathrm{PL}}(\\alpha)$ first:\n$ \\ell_{\\mathrm{PL}}(\\alpha) = n\\ln(\\alpha-1) + n(\\alpha-1)\\ln k_{\\min} - \\alpha S_0 = n\\ln(\\alpha-1) + n(\\alpha-1)\\ln k_{\\min} - \\alpha (S_1 + n\\ln k_{\\min}) = n\\ln(\\alpha-1) - n\\ln k_{\\min} - \\alpha S_1 $.\nSubstituting $\\hat{\\alpha}$:\n$$ \\ell_{\\mathrm{PL}}(\\hat{\\alpha}) = n\\ln\\left(\\frac{n}{S_1}\\right) - n\\ln k_{\\min} - \\left(1 + \\frac{n}{S_1}\\right)S_1 $$\n$$ \\ell_{\\mathrm{PL}}(\\hat{\\alpha}) = n(\\ln n - \\ln S_1) - n\\ln k_{\\min} - (S_1 + n) $$\n$$ \\ell_{\\mathrm{PL}}(\\hat{\\alpha}) = n\\ln n - n\\ln S_1 - n\\ln k_{\\min} - S_1 - n $$\n\nFor the log-normal model, the MLEs are $\\hat{\\mu} = \\bar{y} = S_0/n$ and $\\hat{\\sigma}^2 = s^2$, so $\\hat{\\sigma} = s$. We substitute these into $\\ell_{\\mathrm{LN}}(\\mu,\\sigma)$:\n$$ \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = -S_0 - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2s^2}\\sum_{i=1}^{n} (\\ln k_i - \\bar{y})^2 $$\nSince $\\sum_{i=1}^{n} (\\ln k_i - \\bar{y})^2 = ns^2$, this simplifies to:\n$$ \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = -S_0 - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{ns^2}{2s^2} $$\n$$ \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = -S_0 - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2} $$\nUsing $S_0 = S_1 + n\\ln k_{\\min}$:\n$$ \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = -(S_1 + n\\ln k_{\\min}) - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2} $$\n\n(d) Log-Likelihood Ratio Statistic $\\Lambda$\n\nWe compute $\\Lambda = 2(\\ell_{\\mathrm{PL}}(\\hat{\\alpha}) - \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}))$.\n$$ \\ell_{\\mathrm{PL}}(\\hat{\\alpha}) - \\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma}) = (n\\ln n - n\\ln S_1 - n\\ln k_{\\min} - S_1 - n) - \\left(-(S_1 + n\\ln k_{\\min}) - n\\ln s - \\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\right) $$\n$$ = n\\ln n - n\\ln S_1 - n\\ln k_{\\min} - S_1 - n + S_1 + n\\ln k_{\\min} + n\\ln s + \\frac{n}{2}\\ln(2\\pi) + \\frac{n}{2} $$\nThe terms $-n\\ln k_{\\min}$ and $+n\\ln k_{\\min}$ cancel. The terms $-S_1$ and $+S_1$ cancel.\n$$ = n\\ln n - n\\ln S_1 + n\\ln s - n + \\frac{n}{2} + \\frac{n}{2}\\ln(2\\pi) $$\n$$ = n\\ln n - n\\ln S_1 + n\\ln s - \\frac{n}{2} + \\frac{n}{2}\\ln(2\\pi) $$\nThe log-likelihood ratio statistic $\\Lambda$ is twice this quantity:\n$$ \\Lambda = 2\\left( n\\ln n - n\\ln S_1 + n\\ln s - \\frac{n}{2} + \\frac{n}{2}\\ln(2\\pi) \\right) $$\n$$ \\Lambda = 2n\\ln n - 2n\\ln S_1 + 2n\\ln s - n + n\\ln(2\\pi) $$\nWe can group terms using properties of logarithms:\n$$ \\Lambda = n\\left( 2\\ln n - 2\\ln S_1 + 2\\ln s + \\ln(2\\pi) - 1 \\right) $$\n$$ \\Lambda = n\\left( 2(\\ln n - \\ln S_1 + \\ln s) + \\ln(2\\pi) - 1 \\right) $$\n$$ \\Lambda = n\\left( 2\\ln\\left(\\frac{ns}{S_1}\\right) + \\ln(2\\pi) - 1 \\right) $$\nThis expression depends only on $n$, $S_1$, and $s$, as required (the allowance for $k_{\\min}$ is not needed as it cancels).\n\n(e) Identifiability Challenge\n\nThe difficulty in distinguishing between a power-law tail and a log-normal tail, particularly from finite empirical samples, is a fundamental identifiability challenge. This arises from the mathematical behavior of the log-normal probability density function.\n\nA power-law distribution has a PDF $f(k) \\propto k^{-\\alpha}$, which yields a straight line with slope $-\\alpha$ when its logarithm is plotted against $\\ln k$: $\\ln f(k) = C - \\alpha \\ln k$.\nThe log-normal PDF, $f_{\\mathrm{LN}}(k) = \\frac{1}{k \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln k - \\mu)^2}{2\\sigma^2}\\right)$, does not have this property globally. Taking its logarithm gives:\n$$ \\ln f_{\\mathrm{LN}}(k) = -\\ln k - \\frac{(\\ln k)^2 - 2\\mu\\ln k + \\mu^2}{2\\sigma^2} - \\ln(\\sigma\\sqrt{2\\pi}) $$\n$$ \\ln f_{\\mathrm{LN}}(k) = -\\frac{1}{2\\sigma^2}(\\ln k)^2 + \\left(\\frac{\\mu}{\\sigma^2} - 1\\right)\\ln k - \\left(\\frac{\\mu^2}{2\\sigma^2} + \\ln(\\sigma\\sqrt{2\\pi})\\right) $$\nThis expression is a quadratic function of $\\ln k$, meaning a plot of $\\ln f_{\\mathrm{LN}}(k)$ versus $\\ln k$ is a parabola.\n\nThe challenge arises because a segment of a parabola can be arbitrarily well-approximated by a straight line over a finite interval. The local slope of the log-log plot for the log-normal distribution can be interpreted as a \"local\" power-law exponent, $\\alpha(k) = -d(\\ln f(k))/d(\\ln k)$. For the log-normal, this exponent is:\n$$ \\alpha(k) = -\\frac{d}{d(\\ln k)}\\left[ \\ln f_{\\mathrm{LN}}(k) \\right] = 1 + \\frac{\\ln k - \\mu}{\\sigma^2} $$\nThis local exponent $\\alpha(k)$ is not constant; it changes linearly with $\\ln k$. However, for heavy-tailed phenomena, the parameter $\\sigma$ in a fitted log-normal model is often large. When $\\sigma$ is large, the slope of $\\alpha(k)$ with respect to $\\ln k$, which is $1/\\sigma^2$, becomes very small. This means that $\\alpha(k)$ changes very slowly.\n\nConsequently, over a limited range of observed $k$ values—even a range spanning several orders of magnitude—the parabolic curve of the log-normal density on a log-log plot is nearly a straight line. An empirical dataset generated from a log-normal distribution with large $\\sigma$ can therefore appear to be drawn from a power-law distribution. Statistical methods like MLE will find the best-fitting parameters for each model, but the maximized likelihood values, $\\ell_{\\mathrm{PL}}(\\hat{\\alpha})$ and $\\ell_{\\mathrm{LN}}(\\hat{\\mu}, \\hat{\\sigma})$, may be very close. This leads to a log-likelihood ratio statistic $\\Lambda$ near zero, making it statistically difficult to favor one model over the other with high confidence. This confounding effect is a classic example of model mimicry, representing a core challenge in the empirical study of heavy-tailed systems.",
            "answer": "$$\n\\boxed{n\\left( 2\\ln\\left(\\frac{ns}{S_1}\\right) + \\ln(2\\pi) - 1 \\right)}\n$$"
        }
    ]
}