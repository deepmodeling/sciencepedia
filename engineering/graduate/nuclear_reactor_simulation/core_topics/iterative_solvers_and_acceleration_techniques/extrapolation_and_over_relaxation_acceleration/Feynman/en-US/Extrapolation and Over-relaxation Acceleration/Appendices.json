{
    "hands_on_practices": [
        {
            "introduction": "Before using a numerical simulation for production runs, we must verify that the code is implemented correctly. This practice guides you through a cornerstone of code verification: using a sequence of mesh refinements to experimentally determine the order of accuracy of a discretization scheme . By analyzing the convergence of the effective multiplication factor, $k_{\\text{eff}}$, you will derive and apply the formula for the observed order of convergence, a critical skill for validating simulation tools.",
            "id": "4224934",
            "problem": "In a two-dimensional homogenized neutron diffusion eigenvalue problem for a light water reactor core, the effective multiplication factor $k_{\\text{eff}}$ is computed using a cell-centered finite-volume discretization on a sequence of uniform, geometrically similar Cartesian meshes. Let the mesh spacings be $h_{1}$, $h_{2}$, and $h_{3}$, with a constant isotropic refinement ratio $r$, so that $h_{2} = h_{1}/r$ and $h_{3} = h_{2}/r$. Assume the sequence is in the asymptotic regime in which the leading-order discretization error in $k_{\\text{eff}}$ has the form $C h^{p}$, where $C$ is a nonzero constant and $p$ is the observed (experimental) order of accuracy of the discretization for $k_{\\text{eff}}$. The transport-physics and material properties are fixed; only the spatial mesh changes across the sequence, and the eigenvalue solves are driven to a sufficiently tight residual tolerance such that iterative (solver) error is negligible compared to discretization error.\n\nYou are given the following data:\n- Isotropic refinement ratio $r = 2$.\n- Mesh sizes $h_{1} = 10$ centimeters, $h_{2} = 5$ centimeters, and $h_{3} = 2.5$ centimeters.\n- Computed effective multiplication factors: $k_{\\text{eff}}(h_{1}) = 1.010000$, $k_{\\text{eff}}(h_{2}) = 1.002500$, and $k_{\\text{eff}}(h_{3}) = 1.000625$.\n\nStarting from a consistent asymptotic error model appropriate for elliptic eigenvalue problems and without assuming any specific value of $p$ a priori, derive an expression for the observed order $p$ based on three mesh levels and the constant refinement ratio $r$, then evaluate $p$ from the given data.\n\nFinally, briefly analyze—without performing any further numerical calculations—how the reliability of Richardson extrapolation for $k_{\\text{eff}}$ would be affected if, instead of isotropic refinement, an anisotropic mesh sequence were used in which $h_{x}$ is halved between levels while $h_{y}$ remains fixed, given that the neutron flux has strong boundary layers aligned with the $y$-direction.\n\nReport only the computed value of $p$ as your final answer, rounded to three significant figures. No units are required for $p$.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- Problem type: Two-dimensional homogenized neutron diffusion eigenvalue problem.\n- Numerical method: Cell-centered finite-volume discretization on uniform, geometrically similar Cartesian meshes.\n- Mesh spacings: $h_{1}$, $h_{2}$, $h_{3}$.\n- Refinement ratio: $r$, with $h_{2} = h_{1}/r$ and $h_{3} = h_{2}/r$.\n- Asymptotic error model: Leading-order discretization error in $k_{\\text{eff}}$ is $C h^{p}$, where $C$ is a nonzero constant and $p$ is the observed order of accuracy.\n- Solver error: Negligible compared to discretization error.\n- Data:\n  - Isotropic refinement ratio $r = 2$.\n  - Mesh sizes: $h_{1} = 10$ cm, $h_{2} = 5$ cm, $h_{3} = 2.5$ cm.\n  - Computed eigenvalues: $k_{\\text{eff}}(h_{1}) = 1.010000$, $k_{\\text{eff}}(h_{2}) = 1.002500$, $k_{\\text{eff}}(h_{3}) = 1.000625$.\n- Tasks:\n  1. Derive an expression for the observed order $p$ based on three mesh levels and the constant refinement ratio $r$.\n  2. Evaluate $p$ from the given data.\n  3. Briefly analyze the reliability of Richardson extrapolation for $k_{\\text{eff}}$ under anisotropic refinement.\n- Final Answer Requirement: Report only the computed value of $p$, rounded to three significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the field of computational nuclear engineering and numerical analysis. The neutron diffusion equation, its eigenvalue form, finite-volume methods, and the analysis of discretization error via grid refinement studies are standard, foundational concepts. The asymptotic error model is a cornerstone of numerical analysis.\n- **Well-Posed**: The problem is well-posed. It provides three data points for the numerical solution, which is the minimum number required to determine the observed order of accuracy $p$ from the three-parameter error model ($k_{\\text{exact}}$, $C$, $p$). A unique solution for $p$ can be determined.\n- **Objective**: The problem statement is written in precise, objective, and technical language, free from any subjectivity or bias.\n- **Complete and Consistent**: All necessary data ($h_i$, $k_{\\text{eff}}(h_i)$, $r$) are provided and are internally consistent (e.g., $h_{2} = h_{1}/r$ holds for the given values). The problem is self-contained.\n- **Realistic and Feasible**: The setup is a realistic representation of a convergence study in reactor physics simulation. The values for $k_{\\text{eff}}$ are plausible for a near-critical system.\n- **Other Flaws**: The problem is not ill-posed, trivial, or based on any other listed flaw.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Solution Derivation\n\nThe analysis proceeds by deriving the formula for the observed order of accuracy, calculating its value, and then discussing the implications of anisotropic refinement.\n\nLet $k_{\\text{exact}}$ be the exact continuum value of the effective multiplication factor. The problem states that for a sufficiently small mesh spacing $h$, the numerically computed eigenvalue $k(h)$ can be related to the exact value through an asymptotic error expansion. The leading-order term of this expansion is given as:\n$$k(h) \\approx k_{\\text{exact}} + C h^{p}$$\nwhere $C$ is a constant independent of $h$, and $p$ is the order of accuracy of the numerical method.\n\nWe apply this model to the three given mesh levels, with spacings $h_{1}$, $h_{2}$, and $h_{3}$:\n1. $k(h_{1}) = k_{\\text{exact}} + C h_{1}^{p}$\n2. $k(h_{2}) = k_{\\text{exact}} + C h_{2}^{p}$\n3. $k(h_{3}) = k_{\\text{exact}} + C h_{3}^{p}$\n\nTo find an expression for $p$, we must eliminate the unknown constants $k_{\\text{exact}}$ and $C$. First, we eliminate $k_{\\text{exact}}$ by taking the differences between successive equations:\n$$k(h_{1}) - k(h_{2}) = (k_{\\text{exact}} + C h_{1}^{p}) - (k_{\\text{exact}} + C h_{2}^{p}) = C (h_{1}^{p} - h_{2}^{p})$$\n$$k(h_{2}) - k(h_{3}) = (k_{\\text{exact}} + C h_{2}^{p}) - (k_{\\text{exact}} + C h_{3}^{p}) = C (h_{2}^{p} - h_{3}^{p})$$\n\nNext, we eliminate the constant $C$ by taking the ratio of these two difference equations:\n$$\\frac{k(h_{1}) - k(h_{2})}{k(h_{2}) - k(h_{3})} = \\frac{C (h_{1}^{p} - h_{2}^{p})}{C (h_{2}^{p} - h_{3}^{p})} = \\frac{h_{1}^{p} - h_{2}^{p}}{h_{2}^{p} - h_{3}^{p}}$$\n\nThe mesh spacings are related by a constant isotropic refinement ratio $r$, such that $h_{2} = h_{1}/r$ and $h_{3} = h_{2}/r = h_{1}/r^{2}$. Substituting these relationships into the right-hand side of the equation yields:\n$$\\frac{h_{1}^{p} - (h_{1}/r)^{p}}{(h_{1}/r)^{p} - (h_{1}/r^{2})^{p}} = \\frac{h_{1}^{p}(1 - 1/r^{p})}{h_{1}^{p}(1/r^{p} - 1/r^{2p})} = \\frac{1 - r^{-p}}{r^{-p} - r^{-2p}}$$\nTo simplify this fraction, we can multiply the numerator and the denominator by $r^{2p}$:\n$$\\frac{r^{2p}(1 - r^{-p})}{r^{2p}(r^{-p} - r^{-2p})} = \\frac{r^{2p} - r^{p}}{r^{p} - 1}$$\nFactoring out $r^{p}$ from the numerator gives:\n$$\\frac{r^{p}(r^{p} - 1)}{r^{p} - 1} = r^{p}$$\n\nThus, we arrive at the expression relating the ratio of solution differences to the order of accuracy $p$:\n$$\\frac{k(h_{1}) - k(h_{2})}{k(h_{2}) - k(h_{3})} = r^{p}$$\nTo solve for $p$, we take the natural logarithm of both sides:\n$$\\ln\\left( \\frac{k(h_{1}) - k(h_{2})}{k(h_{2}) - k(h_{3})} \\right) = \\ln(r^{p}) = p \\ln(r)$$\nThis gives the final expression for the observed order of accuracy:\n$$p = \\frac{\\ln\\left( \\frac{k(h_{1}) - k(h_{2})}{k(h_{2}) - k(h_{3})} \\right)}{\\ln(r)}$$\n\nNow, we evaluate $p$ using the provided data:\n- Refinement ratio: $r = 2$.\n- Eigenvalues: $k(h_{1}) = 1.010000$, $k(h_{2}) = 1.002500$, $k(h_{3}) = 1.000625$.\n\nFirst, calculate the differences in the eigenvalues:\n$$k(h_{1}) - k(h_{2}) = 1.010000 - 1.002500 = 0.007500$$\n$$k(h_{2}) - k(h_{3}) = 1.002500 - 1.000625 = 0.001875$$\nNext, calculate the ratio of these differences:\n$$\\frac{k(h_{1}) - k(h_{2})}{k(h_{2}) - k(h_{3})} = \\frac{0.007500}{0.001875} = 4$$\nFinally, substitute this result and the value of $r$ into the formula for $p$:\n$$p = \\frac{\\ln(4)}{\\ln(2)} = \\frac{\\ln(2^{2})}{\\ln(2)} = \\frac{2 \\ln(2)}{\\ln(2)} = 2$$\nThe observed order of accuracy is exactly $2$. The cell-centered finite-volume method on a Cartesian grid for the diffusion equation is expected to be second-order accurate, so this result is consistent with theory. Rounding to three significant figures gives $2.00$.\n\n### Analysis of Anisotropic Refinement\nRichardson extrapolation is a technique for improving the accuracy of a numerical solution by combining results from different mesh levels. It relies fundamentally on the existence of a well-behaved asymptotic error series of the form $A(h) = A_{\\text{exact}} + C h^{p} + O(h^{p+q})$, where $h$ is a single parameter representing the mesh spacing that tends to zero.\n\nIn the case of anisotropic refinement, where the mesh spacing $h_{x}$ is refined while $h_{y}$ remains fixed, this fundamental assumption is violated. The discretization error is no longer a function of a single parameter $h$, but rather a function of both $h_{x}$ and $h_{y}$. The leading-order error model would take the form:\n$$E(h_{x}, h_{y}) \\approx C_{x} h_{x}^{p_x} + C_{y} h_{y}^{p_y}$$\nThe computed eigenvalue is thus $k(h_{x}, h_{y}) \\approx k_{\\text{exact}} + C_{x} h_{x}^{p_x} + C_{y} h_{y}^{p_y}$.\n\nIf a sequence of meshes is generated by halving $h_{x}$ while keeping $h_{y}$ constant, the computed eigenvalues will behave as:\n$$k(h_{x}) \\approx (k_{\\text{exact}} + C_{y} h_{y}^{p_y}) + C_{x} h_{x}^{p_x}$$\nAs $h_{x} \\rightarrow 0$, the sequence of solutions $k(h_x)$ does not converge to the true continuum eigenvalue $k_{\\text{exact}}$. Instead, it converges to a biased value, $k_{\\text{exact}} + C_{y} h_{y}^{p_y}$, where the bias $E_{\\text{bias}} = C_{y} h_{y}^{p_y}$ is the non-vanishing error component from the unrefined $y$-direction.\n\nThe fact that the neutron flux has strong boundary layers aligned with the $y$-direction implies large gradients in the $x$-direction, making refinement in $x$ crucial. However, it does not imply that gradients in $y$ are zero. Even if gradients in $y$ are smaller, the fixed, potentially significant error contribution $E_{\\text{bias}}$ will remain.\n\nRichardson extrapolation, when applied to this sequence, would only act to cancel the leading error term dependent on $h_{x}$, but it would extrapolate towards the incorrect, biased limit. Therefore, the reliability of Richardson extrapolation is severely compromised. It would produce a result that is more accurate with respect to the $x$-discretization, but it would fail to converge to the true physical solution due to the contamination by the fixed error from the $y$-discretization. The procedure would be fundamentally flawed.\n\nThe final answer required is the numerical value of $p$ only.\nThe exact value is $p=2$. Rounded to three significant figures, this is $2.00$.",
            "answer": "$$\\boxed{2.00}$$"
        },
        {
            "introduction": "While extrapolation can be used to estimate error, its most common application is to accelerate the convergence of iterative solvers. However, a naive application of scalar methods to vector problems can lead to unexpected failures. This exercise  explores this pitfall by asking you to apply Aitken's $\\Delta^2$ method component-wise to a neutron flux vector, revealing why a deeper understanding of the underlying error modes is critical for robust acceleration.",
            "id": "4224951",
            "problem": "Consider a one-dimensional homogeneous slab reactor modeled by the multigroup neutron diffusion equation discretized into two spatial degrees of freedom, resulting in a vector neutron flux $\\boldsymbol{\\phi}^{(n)} \\in \\mathbb{R}^2$ at iteration $n$. A standard source iteration seeks the fixed point of a linear map, written abstractly as $\\boldsymbol{\\phi}^{(n+1)} = \\mathbf{M}\\,\\boldsymbol{\\phi}^{(n)} + \\boldsymbol{s}$, where $\\mathbf{M}$ is the iteration operator induced by scattering and diffusion, and $\\boldsymbol{s}$ is a fixed source. For such linear fixed-point iterations, the error $\\boldsymbol{e}^{(n)} = \\boldsymbol{\\phi}^{(n)} - \\boldsymbol{\\phi}^\\star$ relative to the fixed point $\\boldsymbol{\\phi}^\\star$ propagates according to $\\boldsymbol{e}^{(n+1)} = \\mathbf{M}\\,\\boldsymbol{e}^{(n)}$, and for diagonalizable $\\mathbf{M}$ the error admits a modal decomposition $\\boldsymbol{e}^{(n)} = \\sum_{j} c_j \\lambda_j^n \\boldsymbol{v}_j$, where $\\lambda_j$ and $\\boldsymbol{v}_j$ are the eigenvalues and eigenvectors of $\\mathbf{M}$.\n\nSuppose three successive iterates of the flux produced by source iteration are\n$$\n\\boldsymbol{\\phi}^{(0)} = \\begin{bmatrix} 4.0 \\\\ -2.0 \\end{bmatrix},\\quad\n\\boldsymbol{\\phi}^{(1)} = \\begin{bmatrix} 1.85 \\\\ 0.05 \\end{bmatrix},\\quad\n\\boldsymbol{\\phi}^{(2)} = \\begin{bmatrix} 1.1725 \\\\ 0.6325 \\end{bmatrix}.\n$$\nAn engineer proposes to accelerate convergence by applying Aitken $\\Delta^2$ extrapolation component-wise to these three iterates—i.e., treating each entry of $\\boldsymbol{\\phi}^{(n)}$ as an independent scalar sequence and forming an accelerated estimate for each component separately—and then to proceed with the iteration from the accelerated vector estimate.\n\nBased on the fundamental error-propagation model $\\boldsymbol{e}^{(n+1)} = \\mathbf{M}\\,\\boldsymbol{e}^{(n)}$ and the modal decomposition of linear iterations, answer the following:\n\n1. Compute the component-wise Aitken $\\Delta^2$ accelerated estimate $\\boldsymbol{\\phi}^{(\\mathrm{A})}$ using the data above.\n2. Select the statement(s) that are most consistent with first-principles reasoning about error modes in neutron flux distributions and the behavior of component-wise Aitken extrapolation.\n\nOptions:\nA. Using the data provided, the component-wise Aitken $\\Delta^2$ accelerated estimate is approximately $\\boldsymbol{\\phi}^{(\\mathrm{A})} \\approx \\begin{bmatrix} 0.858 \\\\ 0.865 \\end{bmatrix}$; this can worsen the second component because each component is generally a superposition of error modes, so the scalar geometric-convergence assumption underpinning Aitken can be violated component-wise.\n\nB. For any linear source iteration with spectral radius less than $1$, component-wise Aitken $\\Delta^2$ always reduces the Euclidean norm of the error after two steps.\n\nC. Component-wise Aitken $\\Delta^2$ produces the exact fixed-point solution in two steps whenever the dominant error mode has a strictly largest eigenvalue, regardless of how that mode projects onto the components.\n\nD. A safer alternative in reactor simulations is to extract (by projection) the dominant spatial error mode from successive differences and apply scalar Aitken to its modal amplitude; this targets the slowest-converging mode and avoids the pitfalls of mixed-component acceleration.\n\nChoose all that apply.",
            "solution": "The user wants me to solve a problem about numerical acceleration methods in nuclear reactor simulation.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n- The context is a one-dimensional homogeneous slab reactor modeled by the multigroup neutron diffusion equation.\n- The discretized system results in a vector neutron flux $\\boldsymbol{\\phi}^{(n)} \\in \\mathbb{R}^2$ at iteration $n$.\n- The iteration scheme is a linear fixed-point iteration (source iteration): $\\boldsymbol{\\phi}^{(n+1)} = \\mathbf{M}\\,\\boldsymbol{\\phi}^{(n)} + \\boldsymbol{s}$.\n- The error propagation is given by $\\boldsymbol{e}^{(n+1)} = \\mathbf{M}\\,\\boldsymbol{e}^{(n)}$, where $\\boldsymbol{e}^{(n)} = \\boldsymbol{\\phi}^{(n)} - \\boldsymbol{\\phi}^\\star$.\n- For a diagonalizable $\\mathbf{M}$, the error has a modal decomposition $\\boldsymbol{e}^{(n)} = \\sum_{j} c_j \\lambda_j^n \\boldsymbol{v}_j$.\n- Three successive iterates are provided:\n$$\n\\boldsymbol{\\phi}^{(0)} = \\begin{bmatrix} 4.0 \\\\ -2.0 \\end{bmatrix},\\quad\n\\boldsymbol{\\phi}^{(1)} = \\begin{bmatrix} 1.85 \\\\ 0.05 \\end{bmatrix},\\quad\n\\boldsymbol{\\phi}^{(2)} = \\begin{bmatrix} 1.1725 \\\\ 0.6325 \\end{bmatrix}.\n$$\n- The task is to compute the component-wise Aitken $\\Delta^2$ accelerated estimate $\\boldsymbol{\\phi}^{(\\mathrm{A})}$ and to evaluate several statements about the process.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-grounded in the field of numerical methods for nuclear reactor physics. The source iteration method, modal analysis of error, and Aitken's $\\Delta^2$ extrapolation are all standard and well-established concepts.\n- **Well-Posed**: The problem is well-posed. The provided data is sufficient to perform the required calculations. The questions asked are precise and can be answered using the provided data and first principles of numerical linear algebra.\n- **Objective**: The problem statement is objective and uses precise, technical language.\n- **Completeness and Consistency**: The problem is self-contained and free of contradictions.\n- **Realism**: The setup is a simplified but standard representation of a numerical problem in reactor analysis. The values are synthetic but plausible.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. I will proceed with the derivation and analysis.\n\n### Solution Derivation\n\nThe problem requires two main tasks: first, to compute the component-wise Aitken $\\Delta^2$ accelerated estimate, and second, to analyze the behavior of this method in the context of the underlying modal error propagation.\n\n**1. Computation of the Component-wise Aitken $\\Delta^2$ Estimate**\n\nAitken's $\\Delta^2$ extrapolation method for a scalar sequence $\\{x_n\\}_{n=0}^\\infty$ provides an estimate of the limit, $x^\\star$, using three consecutive terms $x_0$, $x_1$, and $x_2$. The formula is:\n$$ x^{(\\mathrm{A})} = x_0 - \\frac{(\\Delta x_0)^2}{\\Delta^2 x_0} = x_0 - \\frac{(x_1 - x_0)^2}{x_2 - 2x_1 + x_0} $$\nWe apply this formula independently to each component of the vector sequence $\\boldsymbol{\\phi}^{(n)}$.\n\n**First component ($\\phi_1$):**\nThe sequence is $\\phi_1^{(0)} = 4.0$, $\\phi_1^{(1)} = 1.85$, $\\phi_1^{(2)} = 1.1725$.\n\nFirst, we compute the forward differences:\n- $\\Delta \\phi_1^{(0)} = \\phi_1^{(1)} - \\phi_1^{(0)} = 1.85 - 4.0 = -2.15$\n- $\\Delta \\phi_1^{(1)} = \\phi_1^{(2)} - \\phi_1^{(1)} = 1.1725 - 1.85 = -0.6775$\n\nNext, we compute the second-order forward difference:\n- $\\Delta^2 \\phi_1^{(0)} = \\Delta \\phi_1^{(1)} - \\Delta \\phi_1^{(0)} = -0.6775 - (-2.15) = 1.4725$\n\nNow, we apply the Aitken formula:\n$$ \\phi_1^{(\\mathrm{A})} = \\phi_1^{(0)} - \\frac{(\\Delta \\phi_1^{(0)})^2}{\\Delta^2 \\phi_1^{(0)}} = 4.0 - \\frac{(-2.15)^2}{1.4725} = 4.0 - \\frac{4.6225}{1.4725} \\approx 4.0 - 3.139219 = 0.860781 $$\n\n**Second component ($\\phi_2$):**\nThe sequence is $\\phi_2^{(0)} = -2.0$, $\\phi_2^{(1)} = 0.05$, $\\phi_2^{(2)} = 0.6325$.\n\nFirst, we compute the forward differences:\n- $\\Delta \\phi_2^{(0)} = \\phi_2^{(1)} - \\phi_2^{(0)} = 0.05 - (-2.0) = 2.05$\n- $\\Delta \\phi_2^{(1)} = \\phi_2^{(2)} - \\phi_2^{(1)} = 0.6325 - 0.05 = 0.5825$\n\nNext, we compute the second-order forward difference:\n- $\\Delta^2 \\phi_2^{(0)} = \\Delta \\phi_2^{(1)} - \\Delta \\phi_2^{(0)} = 0.5825 - 2.05 = -1.4675$\n\nNow, we apply the Aitken formula:\n$$ \\phi_2^{(\\mathrm{A})} = \\phi_2^{(0)} - \\frac{(\\Delta \\phi_2^{(0)})^2}{\\Delta^2 \\phi_2^{(0)}} = -2.0 - \\frac{(2.05)^2}{-1.4675} = -2.0 + \\frac{4.2025}{1.4675} \\approx -2.0 + 2.863714 = 0.863714 $$\n\nSo, the component-wise Aitken accelerated estimate is $\\boldsymbol{\\phi}^{(\\mathrm{A})} \\approx \\begin{bmatrix} 0.8608 \\\\ 0.8637 \\end{bmatrix}$.\n\n**2. Analysis of Principles and Options**\n\nAitken's $\\Delta^2$ method is derived assuming the sequence's error $e_n = x_n - x^\\star$ behaves geometrically, i.e., $e_{n+1} = \\lambda e_n$ for a constant convergence ratio $\\lambda$. In the context of a vector iteration $\\boldsymbol{e}^{(n+1)} = \\mathbf{M} \\boldsymbol{e}^{(n)}$, the error is a superposition of modes: $\\boldsymbol{e}^{(n)} = \\sum_{j} c_j \\lambda_j^n \\boldsymbol{v}_j$.\n\nFor the component-wise error $e_i^{(n)}$ to be geometric, the total vector error $\\boldsymbol{e}^{(n)}$ must be dominated by a single mode, i.e., $\\boldsymbol{e}^{(n)} \\approx c_1 \\lambda_1^n \\boldsymbol{v}_1$. This happens asymptotically as $n \\to \\infty$ if there is a strictly dominant eigenvalue $|\\lambda_1|  |\\lambda_2| \\ge \\dots$. If multiple modes are significant (e.g., in early iterations or when eigenvalues are close in magnitude), the ratio $e_i^{(n+1)}/e_i^{(n)}$ is not constant, and the assumption for Aitken's method is violated.\n\nLet's check if the given iteration is dominated by a single mode. If it were, the vector of differences $\\boldsymbol{d}^{(n)} = \\boldsymbol{\\phi}^{(n+1)} - \\boldsymbol{\\phi}^{(n)}$ would satisfy $\\boldsymbol{d}^{(n+1)} \\approx \\lambda_1 \\boldsymbol{d}^{(n)}$. Let's compute the difference vectors:\n$$ \\boldsymbol{d}^{(0)} = \\boldsymbol{\\phi}^{(1)} - \\boldsymbol{\\phi}^{(0)} = \\begin{bmatrix} 1.85 - 4.0 \\\\ 0.05 - (-2.0) \\end{bmatrix} = \\begin{bmatrix} -2.15 \\\\ 2.05 \\end{bmatrix} $$\n$$ \\boldsymbol{d}^{(1)} = \\boldsymbol{\\phi}^{(2)} - \\boldsymbol{\\phi}^{(1)} = \\begin{bmatrix} 1.1725 - 1.85 \\\\ 0.6325 - 0.05 \\end{bmatrix} = \\begin{bmatrix} -0.6775 \\\\ 0.5825 \\end{bmatrix} $$\nLet's find the ratio $\\boldsymbol{d}^{(1)} / \\boldsymbol{d}^{(0)}$ component-wise:\n- Ratio for component $1$: $\\frac{-0.6775}{-2.15} \\approx 0.3151$\n- Ratio for component $2$: $\\frac{0.5825}{2.05} \\approx 0.2841$\nThe ratios are not equal. This definitively proves that the error is not dominated by a single mode; at least two error modes with different eigenvalues are significant. Thus, applying component-wise Aitken's method is not guaranteed to be effective.\n\nNow we evaluate each option:\n\n**Option A**: Using the data provided, the component-wise Aitken $\\Delta^2$ accelerated estimate is approximately $\\boldsymbol{\\phi}^{(\\mathrm{A})} \\approx \\begin{bmatrix} 0.858 \\\\ 0.865 \\end{bmatrix}$; this can worsen the second component because each component is generally a superposition of error modes, so the scalar geometric-convergence assumption underpinning Aitken can be violated component-wise.\n- The calculated estimate $\\boldsymbol{\\phi}^{(\\mathrm{A})} \\approx \\begin{bmatrix} 0.8608 \\\\ 0.8637 \\end{bmatrix}$ is very close to the values given in the option. The minor discrepancy ($0.4\\%$) is likely due to rounding in the problem's formulation. The calculation is consistent.\n- The reasoning is entirely correct. As demonstrated above, the error is a superposition of at least two modes. This violates the geometric convergence assumption for each component's scalar sequence. When this assumption is violated, Aitken's method is not guaranteed to accelerate convergence and can indeed produce a worse estimate (i.e., increase the error).\n- **Verdict**: **Correct**.\n\n**Option B**: For any linear source iteration with spectral radius less than $1$, component-wise Aitken $\\Delta^2$ always reduces the Euclidean norm of the error after two steps.\n- This statement is false. The qualifier \"always\" is too strong. As explained for Option A, when the underlying assumption of geometric convergence is not met, the method can fail and increase the error. A spectral radius $\\rho(\\mathbf{M})  1$ guarantees the underlying iteration converges, but it does not guarantee that any particular acceleration attempt will be successful, especially in early iterations where multiple modes are present. One can construct simple scalar counterexamples where Aitken's method increases the error magnitude, which can be embedded in a vector problem.\n- **Verdict**: **Incorrect**.\n\n**Option C**: Component-wise Aitken $\\Delta^2$ produces the exact fixed-point solution in two steps whenever the dominant error mode has a strictly largest eigenvalue, regardless of how that mode projects onto the components.\n- This statement is false. Aitken's method yields the exact solution only if the sequence is *perfectly* geometric. The presence of a strictly dominant eigenvalue only guarantees that the sequence becomes *asymptotically* geometric. Subdominant modes are still present, meaning the error is $\\boldsymbol{e}^{(n)} = c_1 \\lambda_1^n \\boldsymbol{v}_1 + \\text{subdominant terms}$. Unless the initial error $\\boldsymbol{e}^{(0)}$ happens to be a pure eigenvector $\\boldsymbol{v}_1$ (so that all other $c_j=0$), the method will not produce the exact solution.\n- **Verdict**: **Incorrect**.\n\n**Option D**: A safer alternative in reactor simulations is to extract (by projection) the dominant spatial error mode from successive differences and apply scalar Aitken to its modal amplitude; this targets the slowest-converging mode and avoids the pitfalls of mixed-component acceleration.\n- This statement describes a modal acceleration method, which is a more sophisticated and robust approach. The dominant error mode (associated with $\\lambda_1$, the eigenvalue with largest magnitude) is what governs the overall (slowest) convergence rate. By estimating the shape of this mode (e.g., from $\\boldsymbol{\\phi}^{(n+1)} - \\boldsymbol{\\phi}^{(n)}$) and projecting the error onto this shape, one can track the scalar amplitude of this single mode. This resulting scalar sequence of amplitudes is much more likely to exhibit geometric convergence than the raw component-wise values of $\\boldsymbol{\\phi}^{(n)}$, which are a mixture of all modes. Applying Aitken's method to this amplitude sequence is therefore more reliable and \"safer,\" as it avoids the \"pitfalls of mixed-component acceleration\" that arise from violating the geometric assumption. This approach is conceptually related to powerful techniques like Coarse Mesh Rebalance (CMR) and synthetic acceleration.\n- **Verdict**: **Correct**.",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "Modern acceleration techniques like Anderson acceleration are powerful but come with their own computational and memory overhead. This advanced practice challenges you to perform a detailed cost-benefit analysis of implementing such a scheme within a neutron transport solver . By modeling the algorithmic complexity and memory footprint, you will derive the optimal \"break-even\" depth, gaining insight into the practical performance engineering required for large-scale simulations.",
            "id": "4224953",
            "problem": "Consider a steady-state neutron transport calculation in a three-dimensional reactor core using the discrete ordinates (also known as $S_{N}$) method with source iteration. Let the unknown vector have size $N$, and let one transport sweep (one application of the transport operator to advance the flux unknowns once through all angles and energy groups) cost $S = \\alpha N$ floating-point operations (flops), where $\\alpha  0$ is a problem-dependent constant that encapsulates angular quadrature, mesh traversal, and within-group scattering costs. To accelerate the fixed-point iteration, Anderson acceleration (type-$I$) of depth $m$ is applied at each iteration once a history of length $m$ has been accumulated.\n\nAssume the following implementation model for one Anderson acceleration step at depth $m$ on vectors of length $N$:\n- A tall-skinny least-squares problem of dimension $N \\times m$ is solved by a Householder-based economy-sized $QR$ factorization of the $N \\times m$ matrix of residual differences, followed by application of $Q^{T}$ to a residual vector and a back-substitution with the $m \\times m$ upper-triangular factor $R$.\n- The cost to form the $N \\times m$ matrix of residual differences from stored residuals is $N m$ flops.\n- The Householder $QR$ factorization of an $N \\times m$ matrix costs $2 N m^{2} - \\tfrac{2}{3} m^{3}$ flops.\n- Applying $Q^{T}$ to a length-$N$ right-hand side costs $2 N m$ flops.\n- Back-substitution in the $m \\times m$ upper-triangular system costs $m^{2}$ flops.\n- Forming the accelerated update as a linear combination of at most $m+1$ stored vectors of length $N$ costs $2 N m$ flops.\n\nFor storage, suppose Anderson acceleration maintains two histories of length $m$: iterate differences and residual differences, each as an $N \\times m$ dense matrix, along with two working vectors of length $N$. If the baseline transport sweep requires storing two vectors of length $N$, and the total additional storage available for acceleration beyond the baseline is limited to $\\Lambda N$ doubles (that is, at most $\\Lambda$ additional vectors of length $N$ can be stored), then the memory feasibility constraint for the Anderson depth is $2 m + 2 \\le \\Lambda$.\n\nTasks:\n1. Derive the per-iteration Anderson acceleration overhead, $C_{\\mathrm{AA}}(m,N)$, in flops by adding the costs listed above. Then form the dimensionless overhead-to-sweep ratio\n$$\nR(m;N,\\alpha) \\equiv \\frac{C_{\\mathrm{AA}}(m,N)}{S} = \\frac{C_{\\mathrm{AA}}(m,N)}{\\alpha N}.\n$$\n2. In the asymptotic regime $N \\gg m$ relevant to large-scale reactor simulations, retain the leading terms in $m$ and $N$ to obtain an approximation $R_{\\mathrm{asym}}(m;\\alpha)$ that is independent of $N$ to leading order. Using this approximation, determine the break-even depth $m_{\\star}$ such that $R_{\\mathrm{asym}}(m_{\\star};\\alpha) = 1$, and express $m_{\\star}$ in closed form as a function of $\\alpha$.\n3. Using the storage model and the constraint $2 m + 2 \\le \\Lambda$, determine the memory-limited maximum feasible depth $m_{\\mathrm{mem}}$ in closed form.\n4. Report the break-even depth that is simultaneously compute-feasible and memory-feasible as a single closed-form analytical expression\n$$\nm_{\\mathrm{be}} \\equiv \\min\\!\\big(m_{\\star},\\, m_{\\mathrm{mem}}\\big).\n$$\n\nYour final answer must be the single closed-form analytical expression for $m_{\\mathrm{be}}$ in terms of $\\alpha$ and $\\Lambda$.",
            "solution": "The problem has been validated and is deemed scientifically grounded, well-posed, and objective. The provided information is self-contained and sufficient to derive the requested analytical expressions.\n\nThe solution proceeds by addressing the four tasks in sequence.\n\nTask 1: Derive the per-iteration Anderson acceleration overhead, $C_{\\mathrm{AA}}(m,N)$, and the dimensionless overhead-to-sweep ratio, $R(m;N,\\alpha)$.\n\nThe total cost of one Anderson acceleration step, $C_{\\mathrm{AA}}(m,N)$, is the sum of the flops for each sub-task as provided in the problem statement. Let $N$ be the vector size and $m$ be the acceleration depth.\nThe components of the cost are:\n1.  Forming the $N \\times m$ matrix of residual differences: $N m$ flops.\n2.  Householder QR factorization of the $N \\times m$ matrix: $2 N m^{2} - \\frac{2}{3} m^{3}$ flops.\n3.  Applying $Q^{T}$ to the residual vector: $2 N m$ flops.\n4.  Back-substitution with the $m \\times m$ matrix $R$: $m^{2}$ flops.\n5.  Forming the updated solution vector: $2 N m$ flops.\n\nSumming these contributions gives the total overhead $C_{\\mathrm{AA}}(m,N)$:\n$$\nC_{\\mathrm{AA}}(m,N) = (N m) + \\left(2 N m^{2} - \\frac{2}{3} m^{3}\\right) + (2 N m) + (m^{2}) + (2 N m)\n$$\nWe group terms that are proportional to $N$ and those that are not:\n$$\nC_{\\mathrm{AA}}(m,N) = N(m + 2m^2 + 2m + 2m) + \\left(m^2 - \\frac{2}{3} m^3\\right)\n$$\n$$\nC_{\\mathrm{AA}}(m,N) = N(2m^2 + 5m) + m^2 - \\frac{2}{3} m^3\n$$\nThe cost of a single transport sweep is given as $S = \\alpha N$ flops. The dimensionless overhead-to-sweep ratio $R(m;N,\\alpha)$ is defined as $\\frac{C_{\\mathrm{AA}}(m,N)}{S}$.\n$$\nR(m;N,\\alpha) = \\frac{N(2m^2 + 5m) + m^2 - \\frac{2}{3} m^3}{\\alpha N} = \\frac{2m^2 + 5m}{\\alpha} + \\frac{m^2 - \\frac{2}{3} m^3}{\\alpha N}\n$$\n\nTask 2: Determine the asymptotic ratio $R_{\\mathrm{asym}}(m;\\alpha)$ and the break-even depth $m_{\\star}$.\n\nIn the asymptotic regime where the problem size $N$ is much larger than the acceleration depth $m$ (i.e., $N \\gg m$), terms of order $\\frac{m^k}{N}$ for $k0$ become negligible. We can thus approximate the ratio $R$ by retaining only the leading order terms.\n$$\nR_{\\mathrm{asym}}(m;\\alpha) = \\lim_{N\\to\\infty} R(m;N,\\alpha) = \\frac{2m^2 + 5m}{\\alpha}\n$$\nThis asymptotic ratio is independent of $N$, as requested.\n\nThe break-even depth, $m_{\\star}$, is defined as the depth for which the computational overhead of acceleration is equal to the cost of one transport sweep. This corresponds to the condition $R_{\\mathrm{asym}}(m_{\\star};\\alpha) = 1$.\n$$\n\\frac{2m_{\\star}^2 + 5m_{\\star}}{\\alpha} = 1\n$$\n$$\n2m_{\\star}^2 + 5m_{\\star} - \\alpha = 0\n$$\nThis is a quadratic equation for $m_{\\star}$. We solve it using the quadratic formula, $m_{\\star} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=2$, $b=5$, and $c=-\\alpha$.\n$$\nm_{\\star} = \\frac{-5 \\pm \\sqrt{5^2 - 4(2)(-\\alpha)}}{2(2)} = \\frac{-5 \\pm \\sqrt{25 + 8\\alpha}}{4}\n$$\nSince the depth $m$ must be a positive quantity, and it is given that $\\alpha  0$, the term under the square root $\\sqrt{25 + 8\\alpha}$ is greater than $\\sqrt{25} = 5$. Therefore, we must select the positive root to ensure $m_{\\star}  0$.\n$$\nm_{\\star} = \\frac{-5 + \\sqrt{25 + 8\\alpha}}{4}\n$$\n\nTask 3: Determine the memory-limited maximum feasible depth $m_{\\mathrm{mem}}$.\n\nThe storage model states that Anderson acceleration requires storage for two $N \\times m$ matrices and two working vectors of length $N$. This amounts to a total of $2m+2$ vectors of length $N$. The available additional storage beyond the baseline is $\\Lambda N$ doubles, which can hold $\\Lambda$ vectors of length $N$. The memory feasibility constraint is therefore:\n$$\n2m + 2 \\le \\Lambda\n$$\nThe maximum feasible depth, which we treat as a continuous variable for analytical purposes, is found by solving the equality:\n$$\n2m_{\\mathrm{mem}} + 2 = \\Lambda\n$$\nSolving for $m_{\\mathrm{mem}}$ gives:\n$$\n2m_{\\mathrm{mem}} = \\Lambda - 2\n$$\n$$\nm_{\\mathrm{mem}} = \\frac{\\Lambda - 2}{2}\n$$\n\nTask 4: Report the simultaneously compute-feasible and memory-feasible break-even depth $m_{\\mathrm{be}}$.\n\nThe overall break-even depth, $m_{\\mathrm{be}}$, must satisfy both the computational break-even condition and the memory constraint. This means the depth cannot exceed either the computationally optimal value $m_{\\star}$ (beyond which the overhead is too high) or the memory-limited value $m_{\\mathrm{mem}}$ (beyond which there is insufficient storage). Therefore, the effective break-even depth is the minimum of these two values.\n$$\nm_{\\mathrm{be}} \\equiv \\min\\!\\big(m_{\\star},\\, m_{\\mathrm{mem}}\\big)\n$$\nSubstituting the expressions derived for $m_{\\star}$ and $m_{\\mathrm{mem}}$ yields the final closed-form analytical expression for $m_{\\mathrm{be}}$ in terms of $\\alpha$ and $\\Lambda$.\n$$\nm_{\\mathrm{be}} = \\min\\left( \\frac{\\sqrt{25 + 8\\alpha} - 5}{4}, \\frac{\\Lambda - 2}{2} \\right)\n$$",
            "answer": "$$\n\\boxed{\\min\\left( \\frac{\\sqrt{25 + 8\\alpha} - 5}{4}, \\frac{\\Lambda - 2}{2} \\right)}\n$$"
        }
    ]
}