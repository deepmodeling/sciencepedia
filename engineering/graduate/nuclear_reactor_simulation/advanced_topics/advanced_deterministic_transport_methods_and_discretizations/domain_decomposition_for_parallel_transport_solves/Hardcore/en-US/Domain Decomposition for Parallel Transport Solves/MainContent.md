## Introduction
Solving the Boltzmann transport equation is fundamental to the high-fidelity simulation of nuclear reactors and other complex physical systems. However, the immense computational cost of these simulations for realistic, full-core models makes them intractable on a single processor. The key to unlocking this predictive power lies in **parallel computing**, and the foundational method for this is **[domain decomposition](@entry_id:165934)**. This technique addresses the challenge of distributing a massive problem across thousands of processors by breaking it down into smaller, manageable subdomains that can be solved concurrently.

This article provides a comprehensive overview of [domain decomposition](@entry_id:165934) for [parallel transport](@entry_id:160671) solves, bridging the gap from fundamental theory to practical application. It is structured to guide you through the essential concepts systematically. First, the **Principles and Mechanisms** chapter will delve into the core mechanics of partitioning, data exchange at interfaces, and the design of parallel sweep algorithms. Next, the **Applications and Interdisciplinary Connections** chapter will explore how these principles are applied to solve real-world problems in nuclear reactor analysis and demonstrate their universal relevance across diverse scientific fields like computational fluid dynamics and astrophysics. Finally, the **Hands-On Practices** section offers practical exercises to solidify your understanding of communication costs, iterative solution schemes, and [task scheduling](@entry_id:268244). We begin by examining the underlying principles that make this powerful parallel methodology possible.

## Principles and Mechanisms

The parallel solution of the linear Boltzmann transport equation relies on the principle of **domain decomposition**, a strategy wherein a large computational problem is broken down into smaller, more manageable subproblems that can be solved concurrently on multiple processors. The efficiency and correctness of this approach hinge on a deep understanding of the underlying physics of [particle transport](@entry_id:1129401) and the mathematical structure of the [numerical algorithms](@entry_id:752770) employed. This chapter elucidates the core principles and mechanisms governing parallel transport solves, from the fundamentals of partitioning and [interface physics](@entry_id:143998) to the design of scalable [parallel algorithms](@entry_id:271337).

### Spatial Partitioning Strategies

The first step in any domain decomposition approach is to partition the [computational mesh](@entry_id:168560), which represents the physical problem domain, into a set of subdomains. For the transport equation, we primarily consider **non-overlapping [domain decomposition](@entry_id:165934)**, where the global mesh $\mathcal{T}$ is divided into $P$ submeshes (or subdomains) $\{\mathcal{T}_i\}_{i=1}^{P}$ such that the interior of each subdomain is disjoint from the others ($\mathcal{T}_i \cap \mathcal{T}_j = \emptyset$ for $i \neq j$) and their union covers the entire domain ($\cup_{i=1}^{P} \overline{\mathcal{T}_i} = \overline{\mathcal{D}}$).

The geometric boundary shared between any two adjacent subdomains, $\mathcal{T}_i$ and $\mathcal{T}_j$, is known as the **interface**, formally defined as the intersection of their [closures](@entry_id:747387), excluding any part that lies on the global domain boundary $\partial\mathcal{D}$ . Mathematically, the interface $\Gamma_{ij}$ is given by:
$$
\Gamma_{ij} = (\overline{\mathcal{T}_i} \cap \overline{\mathcal{T}_j}) \setminus \partial\mathcal{D}
$$
For two subdomains to be neighbors, this interface must be a surface of non-zero measure.

The method by which these subdomains are created has significant performance implications. Two primary strategies exist :

1.  **Geometric Domain Decomposition (GDD)**: This approach partitions the mesh based purely on the spatial coordinates of its elements (cells or vertices). Algorithms such as recursive coordinate bisection (RCB) or partitioning based on a [space-filling curve](@entry_id:149207) are used to divide the domain into spatially compact and contiguous subdomains. GDD is agnostic to the physics of the transport operator and aims to heuristically minimize the communication cost by producing partitions with a low [surface-area-to-volume ratio](@entry_id:141558).

2.  **Algebraic Domain Decomposition (ADD)**: This strategy operates on the graph representing the connectivity of the discretized system. The mesh cells are the vertices of a graph, and an edge exists between two vertices if the corresponding cells are adjacent. Graph partitioning algorithms, such as those implemented in libraries like METIS or ParMETIS, are then employed to partition the graph's vertices. The primary objective is to directly minimize the **edge-cut**—the number of edges connecting vertices in different subdomains—while ensuring each subdomain receives an approximately equal number of vertices (load balancing). Since cut edges correspond to communication, ADD aims to directly minimize inter-processor data exchange.

Once the partition is established, a clear and unambiguous **processor ownership** model must be defined for all mesh entities to ensure [data consistency](@entry_id:748190). Each cell is uniquely owned by the processor corresponding to its subdomain. Shared entities, such as faces and vertices on an interface, require a deterministic, static rule to assign a unique owner. A common rule assigns ownership of an interface face to the processor that owns the adjacent cell with the smaller global index. The processor on the other side of the face maintains a "ghost" copy of the required data but is not the owner .

### Interface Coupling and Data Management

The defining characteristic of the transport equation for a fixed angular direction $\boldsymbol{\Omega}$ is its first-order hyperbolic nature. This property, which describes the streaming of particles along straight-line paths (characteristics), dictates that information flows in a specific, predictable direction. Numerical methods must respect this "[upwinding](@entry_id:756372)" principle: the state of the flux in a given cell is determined by the flux entering through its upwind faces.

In a parallel setting, this physical principle translates directly into a rule for inter-subdomain communication. To solve the transport equation within a subdomain $\mathcal{T}_j$, the value of the angular flux $\psi$ must be known on its inflow boundaries for each discrete direction $\boldsymbol{\Omega}_m$. These inflow boundaries consist of faces on the global domain boundary and interfaces shared with neighboring subdomains. The data required from an adjacent subdomain $\mathcal{T}_i$ is precisely the flux that is outgoing from $\mathcal{T}_i$ and incoming to $\mathcal{T}_j$.

Therefore, the minimal data required to ensure continuity and enable a [parallel transport](@entry_id:160671) sweep is the **incoming angular flux** on the subdomain interfaces for every discrete direction and energy group . This corresponds to the **outgoing half-range angular flux** from the upwind neighbor. If $\mathbf{n}_i$ is the outward normal on the boundary of subdomain $\mathcal{T}_i$, then $\mathcal{T}_i$ must compute and communicate the trace of $\psi(\mathbf{r}, \boldsymbol{\Omega}_m)$ on the interface $\Gamma_{ij}$ for all directions $\boldsymbol{\Omega}_m$ satisfying the outflow condition $\boldsymbol{\Omega}_m \cdot \mathbf{n}_i > 0$ .

In practice, this data exchange is often managed using **ghost cells**. These are layers of data structures on a processor that correspond to cells physically located on a neighboring processor. Before a local computation, these ghost cells are populated with the necessary outgoing flux data communicated from the upwind neighbor processor. This allows the computational stencil for cells on the subdomain boundary to be completed using local memory access, as if the domain were not partitioned. This implementation detail of non-overlapping decompositions should be distinguished from **overlapping Schwarz methods**, where subdomains are extended to include a physically overlapping layer of cells from neighbors, and the local problem is solved on this larger, overlapping region .

From a conservation perspective, the continuity of the angular flux across an interface ensures the conservation of particles. This can be expressed elegantly using **partial currents**. The outgoing partial current $J_i^{+}$ from subdomain $\mathcal{T}_i$ across an interface is the integral of the normal component of the angular flux over the outgoing hemisphere of directions. Similarly, the incoming partial current $J_i^{-}$ is the integral over the incoming hemisphere. With normals defined such that $\mathbf{n}_j = -\mathbf{n}_i$ on the interface $\Gamma_{ij}$, the physical continuity of flux implies that the outgoing current from one side must equal the incoming current to the other:
$$
J_i^{+}(\mathbf{x}) = -J_j^{-}(\mathbf{x}) \quad \text{and} \quad J_j^{+}(\mathbf{x}) = -J_i^{-}(\mathbf{x})
$$
for all $\mathbf{x} \in \Gamma_{ij}$. This condition ensures that when the [particle balance](@entry_id:753197) equations for each subdomain are summed, the interface flux terms cancel perfectly, recovering the global conservation law .

### Algorithms for Parallel Transport Sweeps

The execution of a transport solve for a single discrete direction $\boldsymbol{\Omega}_m$ is known as a **[transport sweep](@entry_id:1133407)**. This process involves updating the flux in each cell of the mesh in an order that respects the upwind data dependencies. These dependencies can be formally represented by a **Directed Acyclic Graph (DAG)**, denoted $G_{\boldsymbol{\Omega}} = (V, E)$, where the vertices $V$ are the mesh cells and a directed edge $(i, j) \in E$ exists if cell $i$ is immediately upwind of cell $j$ for the direction $\boldsymbol{\Omega}_m$ .

The acyclic nature of this graph for a non-periodic domain is guaranteed by the physics of streaming. One can define a potential function $s(\mathbf{x}) = \boldsymbol{\Omega}_m \cdot \mathbf{x}$, which represents the projected distance along the sweep direction. For any directed edge $(i, j)$, the potential is strictly increasing. A cycle in the graph would imply a path that continuously moves downstream yet returns to its starting point, which is a contradiction.

For a parallel sweep to be possible, the partition itself must not introduce deadlocks. This property is known as **sweepability**. A partition is sweepable for a direction $\boldsymbol{\Omega}_m$ if and only if the induced subdomain [dependency graph](@entry_id:275217), $\mathcal{Q}_{m,g}$, is also a DAG . A cyclic dependency, e.g., $P_1 \to P_2$ and $P_2 \to P_1$, would mean that subdomain $P_1$ must wait for data from $P_2$, while $P_2$ must wait for data from $P_1$, creating a deadlock.

For example, consider a simple $2 \times 2$ Cartesian grid with cells $C_{11}$ (lower-left), $C_{12}$ (upper-left), $C_{21}$ (lower-right), and $C_{22}$ (upper-right). For a sweep direction $\boldsymbol{\Omega} = (1,1)/\sqrt{2}$, flow is from lower-left to upper-right. If we create a "checkerboard" partition with $P_1 = \{C_{11}, C_{22}\}$ and $P_2 = \{C_{12}, C_{21}\}$, we find that $P_1 \to P_2$ (because $C_{12}$ and $C_{21}$ are downwind of $C_{11}$), but also $P_2 \to P_1$ (because $C_{22}$ is downwind of both $C_{12}$ and $C_{21}$). This cyclic dependency makes the partition non-sweepable for this direction .

Assuming a sweepable partition, the parallel algorithm proceeds as a **parallel [topological sort](@entry_id:269002)** of the dependency DAG. This is often called a [wavefront algorithm](@entry_id:1133980) or KBA (Koch-Baker-Alcouffe) algorithm . It executes in waves:
1.  **Initialization**: Each processor determines the in-degrees of its local cells (number of upwind dependencies).
2.  **Wave 1**: Each processor identifies a local "ready set" of cells with an in-degree of zero. These are cells that only depend on the global boundary condition or have no upwind neighbors. The flux in these cells can be computed concurrently across all processors.
3.  **Propagation**: As each cell is solved, its processor sends its computed outgoing flux to its downwind neighbors. If a neighbor is on another processor, an MPI message is sent. The receiving processor decrements the in-degree counter of the destination cell.
4.  **Subsequent Waves**: Once a cell's in-degree becomes zero, it is added to the ready set for the next wave. The process repeats, with global synchronization between waves, until all cells in the domain are solved.

This parallel sweep can be understood within the broader framework of iterative methods for the global transport system, $(L-S)\psi = Q$. The way interface data is handled defines two main classes of algorithms :

-   **Multiplicative Schwarz (Gauss-Seidel-like)**: This corresponds directly to the KBA sweep described above. Subdomains are processed in a sequential, [topological order](@entry_id:147345). The solution on subdomain $\Omega_i$ uses the most recently computed interface data from its upwind predecessors within the same global iteration. This allows information to propagate across the entire domain in a single sweep. It is computationally efficient but has limited [parallelism](@entry_id:753103), as processors must wait for their upwind dependencies to be satisfied.

-   **Additive Schwarz (Jacobi-like)**: In this approach, all subdomains are processed in parallel, without waiting for one another. To enable this, each subdomain's local solve uses interface boundary data from the *previous* global iterate, $\psi^{(k)}$. This maximizes concurrency but introduces a lag in [information propagation](@entry_id:1126500); a change in one subdomain is not felt by its neighbor until the next global iteration.

### Performance, Convergence, and Scalability

The choice of parallel algorithm has profound consequences for both performance and convergence. While the Additive Schwarz (Jacobi-like) approach offers greater [parallelism](@entry_id:753103), its convergence rate is often substantially lower than the Multiplicative Schwarz (Gauss-Seidel-like) sweep.

This can be analyzed by examining the **spectral radius** of the [source iteration](@entry_id:1131994) operator, $\rho(L^{-1}S)$. The Jacobi-like DD method with its delayed interface updates effectively modifies the iteration operator. The new operator, $M_{DD-J} = L_D^{-1}(L_O + S)$ (where $L_D$ and $L_O$ are the block-diagonal and off-diagonal parts of the streaming operator $L$), has a spectral radius that is typically larger than that of the monolithic operator $M_{SI} = L^{-1}S$. This increase signifies slower convergence. The degradation is more severe as the number of subdomains increases or as the subdomains become more optically thick, and in the challenging **[diffusion limit](@entry_id:168181)** (highly scattering, optically thick problems), the spectral radius can approach 1, causing the iteration to stall . The KBA sweep, being equivalent to a Multiplicative Schwarz scheme, exactly reproduces the action of the monolithic operator $L^{-1}$ and thus preserves the original convergence rate .

The slow convergence of global, long-wavelength error modes is a fundamental limitation of these "one-level" [domain decomposition methods](@entry_id:165176). These methods act as local "smoothers," effectively reducing high-frequency errors but failing to damp global errors that span many subdomains. To build a truly scalable solver, a **two-level method** is required. This involves adding a **global [coarse-grid correction](@entry_id:140868)** to the algorithm. The purpose of the coarse grid is to solve for the global error component in a single step. Asymptotic analysis shows that the slow-to-converge transport modes are well-approximated by the eigenfunctions of the diffusion equation . Therefore, an effective coarse operator is a discretization of the diffusion operator:
$$
L_c \phi_P = \Sigma_{a,P} \phi_P - \sum_{f \in \partial P} \frac{A_f D_f}{\delta_f} \left( \phi_N - \phi_P \right)
$$
Here, $\phi_P$ is the cell-averaged [scalar flux](@entry_id:1131249), $\Sigma_a$ is the absorption cross section, and $D_f$ is a suitably averaged diffusion coefficient on the face $f$. This physics-based coarse correction captures the problematic global modes, dramatically accelerating convergence and enabling scalability to large numbers of processors and subdomains .

Finally, the initial quality of the partition itself is critical. Several metrics are used to guide partitioning algorithms :
-   **Edge-Cut**: The total number of dependencies (weighted by data volume) that cross subdomain boundaries. Minimizing the edge-cut reduces the total MPI communication volume.
-   **Surface-to-Volume Ratio (SVR)**: The ratio of total interface area to total subdomain volume. Minimizing SVR increases the amount of computation performed relative to the amount of communication required, improving [parallel efficiency](@entry_id:637464).
-   **Interface Angle Alignment**: A metric that averages $|\mathbf{n} \cdot \boldsymbol{\Omega}_m|$ over all interfaces and directions. Minimizing this metric corresponds to creating partitions whose boundaries are, on average, more orthogonal to the sweep directions. This breaks up long dependency chains in the subdomain graph, increasing [concurrency](@entry_id:747654) and reducing processor idle time during sweeps.

By carefully considering these principles—partitioning strategies, [interface physics](@entry_id:143998), sweep algorithms, convergence behavior, and quality metrics—it is possible to design and implement robust, efficient, and scalable [parallel solvers](@entry_id:753145) for the most challenging neutron transport problems.