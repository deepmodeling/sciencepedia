{
    "hands_on_practices": [
        {
            "introduction": "Calculating the total worth of a control rod bank is a primary objective in reactor analysis. Simulations often provide the differential worth—the reactivity change per unit of insertion—at various points along the rod's path. This exercise provides hands-on practice in converting this detailed differential data into the integral worth, a single value representing the total reactivity effect, using standard numerical integration techniques . You will also explore how this integral worth changes as the nuclear fuel is depleted over time, a critical aspect of operational and safety analysis.",
            "id": "4218299",
            "problem": "You are tasked with computing the change in integral control rod bank worth over specified burnup intervals by integrating differential worth curves at discrete burnup states and interpolating between the resulting integral worths. The physical and mathematical basis is as follows. The integral worth $W(B)$ of a control rod bank at burnup $B$ is defined, from first principles, as the line integral of the differential worth $w(s,B)$ with respect to the insertion depth $s$, from zero insertion to a specified full insertion depth $S_{\\max}$, expressed as $$W(B)=\\int_{0}^{S_{\\max}} w(s,B)\\,ds.$$ The differential worth $w(s,B)$ is provided at discrete insertion depths for several discrete burnup states. You must (i) compute the integral worth at those discrete burnup states using an appropriate numerical quadrature that is consistent with linearly varying behavior between sample points, and then (ii) compute the integral worth at requested burnup values by interpolating between the discrete burnup states. Finally, (iii) compute the change in integral worth across the specified burnup interval as the difference $W(B_{\\text{end}})-W(B_{\\text{start}})$. All integral worth values must be expressed in per cent mille (pcm), where per cent mille (pcm) is a unit of reactivity such that $1$ pcm $=10^{-5}$ in $\\Delta k/k$.\n\nYour program must implement the following requirements:\n\n- Use the definition of integral worth as an integral over insertion depth $s$ and the principle of a consistent numerical quadrature for piecewise-linear data to aggregate discrete differential worth samples into integral worth values. Then use a mathematically justified interpolation scheme to evaluate $W(B)$ at intermediate burnup values between provided discrete burnup states. Do not assume any specific functional form beyond linear behavior between adjacent samples in $s$ and $B$.\n- Treat insertion depth $s$ in centimeters (cm), burnup $B$ in gigawatt-days per metric ton uranium (GWd/tU), and differential worth $w(s,B)$ in per cent mille per centimeter (pcm/cm). The integral worth $W(B)$ must be computed in per cent mille (pcm). All final change values must be floats expressed in pcm, rounded to one decimal place.\n\nImplement and evaluate the following test suite. Each test case provides:\n- a list of discrete burnup states $[B_1,B_2,\\dots]$ in $\\text{GWd}/\\text{tU}$,\n- a single common insertion grid $[s_0,s_1,\\dots,s_M]$ in $\\text{cm}$,\n- differential worth samples $w(s_j,B_i)$ in $\\text{pcm}/\\text{cm}$ for each burnup state $B_i$ and grid point $s_j$, and\n- target burnup values $B_{\\text{start}}$ and $B_{\\text{end}}$ in $\\text{GWd}/\\text{tU}$, for which the change $W(B_{\\text{end}})-W(B_{\\text{start}})$ must be reported in $\\text{pcm}$.\n\nTest Case $1$ (happy path, uniform insertion grid):\n- Burnup states: $[0,20,40]$ $\\text{GWd}/\\text{tU}$.\n- Insertion grid: $[0,10,20,30,40,50]$ $\\text{cm}$.\n- Differential worth at $B=0$: $[25,20,16,12,8,5]$ $\\text{pcm}/\\text{cm}$.\n- Differential worth at $B=20$: $[22,18,14,10,7,4]$ $\\text{pcm}/\\text{cm}$.\n- Differential worth at $B=40$: $[19,16,12,9,6,3]$ $\\text{pcm}/\\text{cm}$.\n- Evaluate change from $B_{\\text{start}}=0$ to $B_{\\text{end}}=40$ $\\text{GWd}/\\text{tU}$.\n\nTest Case $2$ (interpolation in burnup, uniform but different insertion grid):\n- Burnup states: $[0,40]$ $\\text{GWd}/\\text{tU}$.\n- Insertion grid: $[0,8,16,24,40]$ $\\text{cm}$.\n- Differential worth at $B=0$: $[24,21,17,12,7]$ $\\text{pcm}/\\text{cm}$.\n- Differential worth at $B=40$: $[18,16,13,9,5]$ $\\text{pcm}/\\text{cm}$.\n- Evaluate change from $B_{\\text{start}}=10$ to $B_{\\text{end}}=30$ $\\text{GWd}/\\text{tU}$.\n\nTest Case $3$ (edge case with non-uniform insertion grid):\n- Burnup states: $[5,15,30]$ $\\text{GWd}/\\text{tU}$.\n- Insertion grid: $[0,5,15,25,45]$ $\\text{cm}$.\n- Differential worth at $B=5$: $[30,24,18,11,6]$ $\\text{pcm}/\\text{cm}$.\n- Differential worth at $B=15$: $[28,22,16,10,5]$ $\\text{pcm}/\\text{cm}$.\n- Differential worth at $B=30$: $[25,19,14,9,4]$ $\\text{pcm}/\\text{cm}$.\n- Evaluate change from $B_{\\text{start}}=15$ to $B_{\\text{end}}=30$ $\\text{GWd}/\\text{tU}$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the three test cases, with each value being a float in $\\text{pcm}$ rounded to one decimal place (e.g., $[x_1,x_2,x_3]$ where each $x_i$ is in $\\text{pcm}$).",
            "solution": "The problem is valid. It is scientifically grounded in the principles of nuclear reactor physics, specifically concerning control rod worth and its evolution with fuel burnup. The problem is well-posed, providing all necessary data and clearly defining the mathematical procedures for its solution. The terminology and units (per cent mille, GWd/tU) are standard in the field.\n\nThe problem requires a two-step numerical procedure to determine the change in integral control rod bank worth, $\\Delta W$, over a specified burnup interval $[B_{\\text{start}}, B_{\\text{end}}]$. The steps are:\n1.  Numerical integration to find the integral worth $W(B_i)$ at discrete burnup states $B_i$.\n2.  Linear interpolation to find the integral worth $W(B)$ at the target burnup values $B_{\\text{start}}$ and $B_{\\text{end}}$.\n\n**Step 1: Calculation of Integral Worth at Discrete Burnup States**\n\nThe integral worth of a control rod bank, $W(B)$, at a given fuel burnup state $B$, is defined as the integral of the differential worth, $w(s, B)$, over the full insertion depth of the rod, from $s = 0$ to $s = S_{\\max}$.\n$$\nW(B) = \\int_{0}^{S_{\\max}} w(s, B) \\, ds\n$$\nThe problem provides sampled values of the differential worth $w(s_j, B_i)$ at a set of discrete insertion depths $\\{s_0, s_1, \\dots, s_M\\}$ for several discrete burnup states $\\{B_1, B_2, \\dots\\}$. The problem states to assume linear behavior for $w(s, B)$ between these discrete sample points in $s$. This defines $w(s,B)$ as a piecewise linear function of insertion depth $s$.\n\nThe integral of a piecewise linear function defined by points $(x_j, y_j)$ is most appropriately calculated using the trapezoidal rule. The total area is the sum of the areas of the trapezoids formed between each pair of adjacent points. For a given burnup state $B_i$, the integral worth $W(B_i)$ is calculated as:\n$$\nW(B_i) \\approx \\sum_{j=0}^{M-1} \\frac{w(s_{j+1}, B_i) + w(s_j, B_i)}{2} (s_{j+1} - s_j)\n$$\nwhere $M+1$ is the number of points in the insertion grid. This calculation is performed for each discrete burnup state $B_i$ provided in a test case, yielding a set of discrete integral worth values $\\{W(B_1), W(B_2), \\dots\\}$. The units are consistent: the differential worth in pcm/cm multiplied by the insertion depth in cm yields an integral worth in pcm.\n\n**Step 2: Interpolation of Integral Worth for Target Burnup Values**\n\nAfter Step 1, we possess a set of discrete data points $(B_i, W(B_i))$ that describes how the integral worth changes with fuel burnup. The problem states to assume linear behavior for the integral worth $W(B)$ between these discrete burnup states. Therefore, to find the integral worth $W(B)$ at any intermediate burnup $B$ that falls between two given states, $B_i \\leq B \\leq B_{i+1}$, we employ linear interpolation. The formula for linear interpolation is:\n$$\nW(B) = W(B_i) + (B - B_i) \\frac{W(B_{i+1}) - W(B_i)}{B_{i+1} - B_i}\n$$\nThis formula is used to calculate the integral worths at the specified start and end burnup values, $W(B_{\\text{start}})$ and $W(B_{\\text{end}})$. If a target burnup value coincides with one of the discrete burnup states $B_i$, no interpolation is necessary, and we directly use the value $W(B_i)$ calculated in Step 1.\n\n**Step 3: Final Calculation of Worth Change**\n\nThe final quantity to be reported is the change in integral worth over the interval, which is simply the difference between the worth at the end and start burnups:\n$$\n\\Delta W = W(B_{\\text{end}}) - W(B_{\\text{start}})\n$$\nThis value, in units of pcm, is then rounded to one decimal place as required.\n\nThe algorithmic implementation will follow these steps precisely. For each test case, we first compute the integral worth for each given burnup state using the trapezoidal rule. Then, we use these computed integral worths to perform linear interpolation to find the worths at the target start and end burnups. Finally, we compute their difference. This procedure is implemented for each of the three test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the change in integral control rod bank worth over specified burnup intervals.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"burnup_states\": [0, 20, 40],\n            \"insertion_grid\": [0, 10, 20, 30, 40, 50],\n            \"diff_worths\": [\n                [25, 20, 16, 12, 8, 5],  # B = 0\n                [22, 18, 14, 10, 7, 4],  # B = 20\n                [19, 16, 12, 9, 6, 3],   # B = 40\n            ],\n            \"b_start\": 0,\n            \"b_end\": 40,\n        },\n        {\n            \"burnup_states\": [0, 40],\n            \"insertion_grid\": [0, 8, 16, 24, 40],\n            \"diff_worths\": [\n                [24, 21, 17, 12, 7],     # B = 0\n                [18, 16, 13, 9, 5],      # B = 40\n            ],\n            \"b_start\": 10,\n            \"b_end\": 30,\n        },\n        {\n            \"burnup_states\": [5, 15, 30],\n            \"insertion_grid\": [0, 5, 15, 25, 45],\n            \"diff_worths\": [\n                [30, 24, 18, 11, 6],     # B = 5\n                [28, 22, 16, 10, 5],     # B = 15\n                [25, 19, 14, 9, 4],      # B = 30\n            ],\n            \"b_start\": 15,\n            \"b_end\": 30,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        burnup_states = case[\"burnup_states\"]\n        insertion_grid = case[\"insertion_grid\"]\n        diff_worth_curves = case[\"diff_worths\"]\n        b_start = case[\"b_start\"]\n        b_end = case[\"b_end\"]\n\n        # Step 1: Compute integral worth W(B_i) at each discrete burnup state.\n        # The integral of a piecewise linear function is calculated using the trapezoidal rule.\n        # numpy.trapz is the direct implementation of this rule.\n        integral_worths_at_states = []\n        for diff_worth_curve in diff_worth_curves:\n            integral_worth = np.trapz(diff_worth_curve, x=insertion_grid)\n            integral_worths_at_states.append(integral_worth)\n\n        # Step 2: Linearly interpolate to find W(B_start) and W(B_end).\n        # numpy.interp performs linear interpolation. It can compute values for\n        # multiple points at once.\n        target_burnups = [b_start, b_end]\n        w_start, w_end = np.interp(\n            target_burnups, burnup_states, integral_worths_at_states\n        )\n        \n        # Step 3: Compute the change in integral worth.\n        delta_w = w_end - w_start\n        \n        # Round the result to one decimal place and add to the list.\n        results.append(round(delta_w, 1))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Any result from a numerical simulation is an approximation, with its accuracy fundamentally limited by the discretization of space, energy, and time. This practice introduces a cornerstone of Verification and Validation (VV): quantifying the numerical uncertainty due to spatial mesh resolution . You will implement the Richardson extrapolation method to estimate the \"exact\" solution that would be obtained with an infinitely fine mesh and calculate the associated error bars on your result. This powerful technique is essential for establishing the credibility and reliability of any computational model.",
            "id": "4218284",
            "problem": "You will write a complete, runnable program that quantifies the mesh resolution uncertainty in the axial differential control rod bank worth distribution $W(z)$ using Richardson extrapolation and error bar estimation. The physical context is a one-dimensional slab reactor of axial height $H$ with a homogeneous material, where the stationary neutron flux fundamental mode is approximated by a separable axial shape. The control rod bank worth distribution $W(z)$ is modeled via first-order perturbation theory grounded in the neutron diffusion eigenvalue problem, assuming a uniform perturbation of absorption cross section in the inserted region so that $W(z)$ is proportional to the product of the forward and adjoint fluxes. In a homogeneous slab, the adjoint flux equals the forward flux for the fundamental mode, leading to a shape proportional to the squared mode shape. For this benchmarking problem, the underlying exact distribution is defined as\n$$\nW_{\\text{true}}(z) = A \\sin^2\\!\\left(\\frac{\\pi z}{H}\\right),\n$$\nwith all angles in radians, $z \\in [0,H]$, and $W$ expressed in pcm/cm, where $1$ pcm equals $10^{-5}$ in units of $\\Delta k/k$, and lengths are in centimeters. The program must not use $W_{\\text{true}}(z)$ to directly compute the final answer; instead, it must rely on simulated numerical solutions on successively refined meshes and use a principled extrapolation and uncertainty estimation procedure.\n\nYou are given a synthetic numerical solver model that returns mesh-discrete approximations $W_h(z_i)$ on uniform meshes with spacing $h$, where the cell centers are $z_i = \\left(i+\\frac{1}{2}\\right) h_{\\text{eff}}$ for $i = 0,1,\\dots,N-1$ and $N = H/h_{\\text{eff}}$ is an integer, with $h_{\\text{eff}} = H/N$. The numerical approximation includes an asymptotic truncation error consistent with a second-order method and a controllable higher-order contamination:\n$$\nW_h(z_i) = W_{\\text{true}}(z_i) + \\alpha h_{\\text{eff}}^{p} + \\beta h_{\\text{eff}}^{p+1} + \\gamma h_{\\text{eff}}^{p} \\sin\\!\\left(\\frac{3\\pi z_i}{H}\\right),\n$$\nwhere $p$ is the nominal order, $\\alpha$, $\\beta$, and $\\gamma$ are coefficients, and $\\sin(\\cdot)$ uses radians. This $W_h$ serves as the computed observable on a given mesh. For any Richardson-based comparison, all mesh solutions must be represented on a common set of axial locations; your program should use the finest mesh’s cell centers as the common locations and interpolate coarser solutions to these locations using linear interpolation.\n\nYour task is to:\n- Compute three mesh solutions $W_{h_1}$, $W_{h_2}$, $W_{h_3}$ for each test case, where $h_1  h_2  h_3$ and the refinement ratio $r$ defined by $r = h_1/h_2 = h_2/h_3$ is specified.\n- Use a three-level Richardson extrapolation framework: estimate a single, global observed order $p_{\\text{obs}}$ from the triplet using a norm-based approach across $z$, and then form a two-solution Richardson extrapolation from the finest two meshes at each $z$ to approximate the grid-converged value and the associated local discretization error magnitude.\n- From the local error magnitude, form two-sided error bars using a safety factor calibrated for a $95\\%$ coverage Grid Convergence Index (GCI). Use a standard constant safety factor equal to $1.25$ to inflate the local Richardson estimate into a conservative error bar half-width at each $z$.\n- Quantify the mesh resolution uncertainty as the maximum error bar half-width across all $z$ for each test case.\n\nExpress all final reported uncertainties in pcm/cm, rounded to six decimal places. Angles in all trigonometric functions must be treated in radians.\n\nTest suite specification:\n- Use the following three test cases. For each case, compute three meshes and apply the procedure above to return a single scalar: the maximum estimated two-sided error bar half-width across $z$ in pcm/cm.\n- Test case parameters are provided as $(H,\\ A,\\ p,\\ h_1,\\ h_2,\\ h_3,\\ r,\\ \\alpha,\\ \\beta,\\ \\gamma)$, where $H$ is in cm, $A$, $\\alpha$, $\\beta$, $\\gamma$ are in pcm/cm, $p$ is dimensionless, $h_k$ are in cm, and $r$ is dimensionless.\n\n- Test case $1$:\n  - $(H,\\ A,\\ p,\\ h_1,\\ h_2,\\ h_3,\\ r,\\ \\alpha,\\ \\beta,\\ \\gamma) = (400,\\ 10,\\ 2,\\ 25,\\ 12.5,\\ 6.25,\\ 2,\\ 0.8,\\ 0.3,\\ 0.1)$.\n- Test case $2$:\n  - $(H,\\ A,\\ p,\\ h_1,\\ h_2,\\ h_3,\\ r,\\ \\alpha,\\ \\beta,\\ \\gamma) = (288,\\ 8,\\ 2,\\ 24,\\ 16,\\ 10.\\overline{6},\\ 1.5,\\ 1.5,\\ 0.5,\\ 0.2)$, where $10.\\overline{6}$ denotes $32/3$ cm exactly.\n- Test case $3$:\n  - $(H,\\ A,\\ p,\\ h_1,\\ h_2,\\ h_3,\\ r,\\ \\alpha,\\ \\beta,\\ \\gamma) = (450,\\ 12,\\ 2,\\ 50,\\ 25,\\ 12.5,\\ 2,\\ 2.0,\\ 1.0,\\ 0.05)$.\n\nAlgorithmic requirements and constraints:\n- Begin from the asymptotic truncation error model of mesh-discrete observables in numerical solutions of the neutron diffusion eigenvalue problem and derive all necessary extrapolation steps from that base. Do not use or assume any formula not derivable from these fundamentals.\n- Use the finest mesh’s cell centers as the common axial coordinates for Richardson analysis. Interpolate coarser mesh fields linearly to these points.\n- Estimate a single global observed order $p_{\\text{obs}}$ for each test case using the ratio of norms of differences between successive mesh solutions over $z$ and the known refinement ratio $r$.\n- Use two-solution Richardson extrapolation based on $p_{\\text{obs}}$ at each axial point to compute local error bar half-widths and then inflate them by a factor $1.25$ to obtain conservative $95\\%$ coverage error bars.\n- Return, for each test case, a single float equal to the maximum error bar half-width over the axial domain in pcm/cm, rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, for example $[x_1,x_2,x_3]$, where each $x_k$ is the maximum estimated uncertainty for test case $k$ in pcm/cm, rounded to six decimal places.",
            "solution": "The problem requires the quantification of mesh resolution uncertainty for a simulated control rod bank worth distribution, $W(z)$, using a three-level Richardson extrapolation framework. The process is grounded in the analysis of the asymptotic truncation error of numerical solutions to the neutron diffusion equation.\n\nThe fundamental premise of Richardson extrapolation is that the error in a numerical solution, $W_h$, obtained with a characteristic mesh spacing $h$, can be expressed as an asymptotic series in powers of $h$. For a method with a nominal order of accuracy $p$, the solution on a discrete grid can be written as:\n$$\nW_h(z) = W_{\\text{true}}(z) + C(z)h^p + \\mathcal{O}(h^{p+1})\n$$\nwhere $W_{\\text{true}}(z)$ is the exact, continuous solution, and $C(z)$ is a function, independent of $h$, that characterizes the leading-order error. The problem provides a synthetic model for this numerical solution which we must use:\n$$\nW_h(z_i) = W_{\\text{true}}(z_i) + \\alpha h_{\\text{eff}}^{p} + \\beta h_{\\text{eff}}^{p+1} + \\gamma h_{\\text{eff}}^{p} \\sin\\!\\left(\\frac{3\\pi z_i}{H}\\right)\n$$\nwhere $W_{\\text{true}}(z) = A \\sin^2(\\frac{\\pi z}{H})$, and the effective mesh spacing $h_{\\text{eff}}$ is determined for a given nominal spacing $h_k$ by finding an integer number of cells $N_k$ such that $h_{\\text{eff}, k} = H/N_k$. For the provided test cases, $H/h_k$ is always an integer, so we have $N_k=H/h_k$ and $h_{\\text{eff}, k}=h_k$.\n\nThe procedure involves three steps: estimating the observed order of convergence, performing Richardson extrapolation to estimate the error, and calculating a conservative uncertainty bound.\n\nFirst, we compute three solutions, $W_1$, $W_2$, and $W_3$, on meshes with spacings $h_1$, $h_2$, and $h_3$, respectively. The meshes are systematically refined with a constant ratio $r = h_1/h_2 = h_2/h_3$. Since the solutions are defined on different discrete sets of axial points $z_i$, they must be compared on a common grid. As stipulated, we select the cell centers of the finest mesh, $h_3$, as the common grid. The solutions from the coarser meshes, $W_1$ and $W_2$, are transferred to this fine grid using linear interpolation. Let us denote the interpolated solutions as $W_{1,\\text{interp}}$ and $W_{2,\\text{interp}}$, and the fine-grid solution as $W_3$.\n\nSecond, we estimate the global observed order of convergence, $p_{\\text{obs}}$. Assuming the higher-order terms are negligible, the differences between successive solutions are:\n$$\nW_{1,\\text{interp}}(z) - W_{2,\\text{interp}}(z) \\approx C(z)(h_1^p - h_2^p) = C(z)h_2^p(r^p - 1)\n$$\n$$\nW_{2,\\text{interp}}(z) - W_3(z) \\approx C(z)(h_2^p - h_3^p) = C(z)h_3^p(r^p - 1) = C(z)h_2^p(1 - (1/r)^p)\n$$\nTaking the ratio of the differences gives:\n$$\n\\frac{W_{1,\\text{interp}}(z) - W_{2,\\text{interp}}(z)}{W_{2,\\text{interp}}(z) - W_3(z)} \\approx \\frac{C(z)h_2^p(r^p - 1)}{C(z)h_3^p(r^p - 1)} = \\frac{h_2^p}{h_3^p} = r^p\n$$\nTo obtain a single, robust estimate for $p_{\\text{obs}}$ that is representative of the entire domain, we take the ratio of the norms of the solution differences:\n$$\n\\frac{\\Vert W_{1,\\text{interp}} - W_{2,\\text{interp}} \\Vert}{\\Vert W_{2,\\text{interp}} - W_3 \\Vert} \\approx r^{p_{\\text{obs}}}\n$$\nUsing the discrete $L_2$-norm, where for a vector $\\mathbf{v}$, $\\Vert \\mathbf{v} \\Vert_2 = \\sqrt{\\sum_i v_i^2}$, we solve for $p_{\\text{obs}}$:\n$$\np_{\\text{obs}} = \\frac{\\ln\\left(\\frac{\\Vert W_{1,\\text{interp}} - W_{2,\\text{interp}} \\Vert_2}{\\Vert W_{2,\\text{interp}} - W_3 \\Vert_2}\\right)}{\\ln(r)}\n$$\n\nThird, we use the two finest solutions, $W_{2,\\text{interp}}$ and $W_3$, along with the estimated order $p_{\\text{obs}}$, to perform a Richardson extrapolation at each point $z_j$ on the fine grid. We establish a system of two equations for the unknowns $W_{\\text{true}}(z_j)$ and $C(z_j)$:\n$$\nW_{2,\\text{interp}}(z_j) \\approx W_{\\text{true}}(z_j) + C(z_j) h_2^{p_{\\text{obs}}}\n$$\n$$\nW_3(z_j) \\approx W_{\\text{true}}(z_j) + C(z_j) h_3^{p_{\\text{obs}}} = W_{\\text{true}}(z_j) + C(z_j) (h_2/r)^{p_{\\text{obs}}}\n$$\nSolving this system for $W_{\\text{true}}(z_j)$ yields the Richardson-extrapolated solution, $W_{\\text{RE}}(z_j)$:\n$$\nW_{\\text{RE}}(z_j) = \\frac{r^{p_{\\text{obs}}} W_3(z_j) - W_{2,\\text{interp}}(z_j)}{r^{p_{\\text{obs}}} - 1}\n$$\nThis can be expressed as a correction to the finest-grid solution:\n$$\nW_{\\text{RE}}(z_j) = W_3(z_j) + \\frac{W_3(z_j) - W_{2,\\text{interp}}(z_j)}{r^{p_{\\text{obs}}} - 1}\n$$\nThe second term on the right-hand side is the leading-order error estimate for the solution $W_3(z_j)$. The local discretization error magnitude, $|\\delta_{\\text{RE}}(z_j)|$, is therefore:\n$$\n|\\delta_{\\text{RE}}(z_j)| = \\left| \\frac{W_3(z_j) - W_{2,\\text{interp}}(z_j)}{r^{p_{\\text{obs}}} - 1} \\right|\n$$\nTo form a conservative estimate of the uncertainty, following the Grid Convergence Index (GCI) methodology, this error estimate is multiplied by a safety factor, $F_S$. The problem specifies $F_S = 1.25$. The local error bar half-width, $E_{\\text{bar}}(z_j)$, is thus:\n$$\nE_{\\text{bar}}(z_j) = F_S \\times |\\delta_{\\text{RE}}(z_j)| = 1.25 \\times \\left| \\frac{W_3(z_j) - W_{2,\\text{interp}}(z_j)}{r^{p_{\\text{obs}}} - 1} \\right|\n$$\nFinally, the overall mesh resolution uncertainty for a given test case is quantified as the maximum value of this error bar half-width across the entire axial domain:\n$$\n\\text{Uncertainty} = \\max_{j} \\left( E_{\\text{bar}}(z_j) \\right)\n$$\nThis procedure is applied to each test case to yield the final numerical results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes mesh resolution uncertainty for control rod bank worth using Richardson extrapolation.\n    \"\"\"\n\n    test_cases = [\n        # (H, A, p, h1, h2, h3, r, alpha, beta, gamma)\n        (400.0, 10.0, 2.0, 25.0, 12.5, 6.25, 2.0, 0.8, 0.3, 0.1),\n        (288.0, 8.0, 2.0, 24.0, 16.0, 32.0/3.0, 1.5, 1.5, 0.5, 0.2),\n        (450.0, 12.0, 2.0, 50.0, 25.0, 12.5, 2.0, 2.0, 1.0, 0.05),\n    ]\n\n    results = []\n    \n    # Safety factor for GCI\n    F_S = 1.25\n\n    def get_w_h_data(params, h):\n        \"\"\"\n        Generates the discrete rod worth data for a given mesh size h.\n        \"\"\"\n        H, A, p, _, _, _, _, alpha, beta, gamma = params\n        \n        # Determine the number of cells N, ensuring it's an integer.\n        # The problem statement ensures H/h is integer for all test cases.\n        N = int(round(H / h))\n        h_eff = H / N\n\n        # Generate cell center coordinates\n        z_coords = (np.arange(N) + 0.5) * h_eff\n\n        # Calculate W_true at these coordinates\n        w_true = A * np.sin(np.pi * z_coords / H)**2\n        \n        # Calculate the synthetic numerical solution W_h\n        error_term = (alpha * h_eff**p + \n                      beta * h_eff**(p + 1) + \n                      gamma * h_eff**p * np.sin(3 * np.pi * z_coords / H))\n        w_h = w_true + error_term\n        \n        return z_coords, w_h\n\n    for case in test_cases:\n        H, _, _, h1, h2, h3, r, _, _, _ = case\n\n        # Step 1: Generate solutions on three meshes\n        z1, w1 = get_w_h_data(case, h1)  # Coarse\n        z2, w2 = get_w_h_data(case, h2)  # Medium\n        z3, w3 = get_w_h_data(case, h3)  # Fine\n\n        # Step 2: Interpolate coarser solutions to the finest mesh coordinates (z3)\n        # numpy.interp performs linear interpolation\n        w1_interp = np.interp(z3, z1, w1)\n        w2_interp = np.interp(z3, z2, w2)\n        # w3 is already on the fine grid\n\n        # Step 3: Estimate a single global observed order p_obs\n        # Use L2 norm of the differences\n        diff_12 = w1_interp - w2_interp\n        diff_23 = w2_interp - w3\n        \n        norm_diff_12 = np.linalg.norm(diff_12)\n        norm_diff_23 = np.linalg.norm(diff_23)\n        \n        # If norm_diff_23 is zero, it implies perfect agreement, which is unlikely.\n        # But as a safe guard:\n        if norm_diff_23 == 0.0:\n            if norm_diff_12 == 0.0:\n                # All solutions are identical; error is zero.\n                p_obs = case[2] # Use nominal order, though it won't matter\n            else:\n                # Unstable case, but we must proceed. Let p_obs be large.\n                p_obs = 20.0\n        else:\n            ratio_of_norms = norm_diff_12 / norm_diff_23\n            # If ratio is = 0, something is wrong, e.g. non-convergence.\n            # The calculation is robust for positive ratios.\n            if ratio_of_norms = 0:\n                p_obs = case[2] # Fallback to nominal order\n            else:\n                p_obs = np.log(ratio_of_norms) / np.log(r)\n\n        # Step 4 and 5: Compute local error bar half-width for each point z\n        # Error estimate from Richardson extrapolation on the two finest grids\n        # Note: the difference is w3 - w2_interp in the numerator, not w2-w3\n        # This is based on the formula: RE = w3 + (w3 - w2) / (r^p - 1)\n        # The error term is the correction itself.\n        local_error_mag = np.abs((w3 - w2_interp) / (r**p_obs - 1.0))\n        \n        # Inflate with safety factor to get error bar half-width\n        error_bar_half_width = F_S * local_error_mag\n\n        # Step 6: Find the maximum error bar half-width across the domain\n        max_uncertainty = np.max(error_bar_half_width)\n        \n        # Round to six decimal places as required\n        results.append(round(max_uncertainty, 6))\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond numerical errors, the accuracy of reactor simulations depends on the fundamental nuclear data used as inputs, which carry their own experimental uncertainties. This advanced exercise focuses on uncertainty quantification (UQ), demonstrating how to propagate uncertainties from input parameters, such as nuclear cross-sections, to the final computed rod worth . You will use a linear sensitivity-based approach and covariance matrices to calculate the total variance of the bank worth and decompose it to identify the dominant sources of physical uncertainty. This analysis is vital for robust reactor design and for prioritizing future nuclear data improvements.",
            "id": "4218377",
            "problem": "You are asked to construct a principled variance decomposition of control rod bank worth given parameterized cross-section uncertainty and to identify dominant uncertainty contributors. Your task must begin from a valid base: the definition of reactivity and first-order perturbation of a response with respect to uncertain inputs, and the definition of covariance and variance for random variables.\n\nThe control rod bank worth is defined as the difference in reactivity between an initial state with a set of control rods withdrawn and a final state with that same set of control rods inserted. Let the reactivity be defined by $ \\rho = \\dfrac{k_{\\text{eff}} - 1}{k_{\\text{eff}}} $ where $ k_{\\text{eff}} $ is the effective multiplication factor. Under a small perturbation of microscopic or macroscopic cross-sections and related physics inputs, the change in any scalar response can be approximated through first-order perturbation theory and linear sensitivity analysis, leveraging the concept of sensitivity coefficients and the statistical definitions of covariance and variance. Your program must implement this first-principles propagation from inputs to the variance of the bank worth and then decompose that variance into parameter-level contributions that are consistent with the covariance structure.\n\nAssume the following modeling framework for all test cases:\n- The bank worth change $ W $ is approximated to first order by a linear functional of $ n $ dimensionless input perturbations $ x_i $ (each $ x_i $ is a relative change of a physics parameter such as absorption cross-section or scattering cross-section). The sensitivity coefficient $ S_i $ measures the $ W $ response per unit relative change of $ x_i $, expressed in reactivity units. Under these assumptions, $ W $ is a linear function of the $ x_i $ to leading order.\n- Let the vector of sensitivities be $ \\mathbf{S} \\in \\mathbb{R}^n $ with entries in $ \\text{pcm} $ per unit relative change, where pcm denotes $ 10^{-5} $ fractional reactivity. Let the random input perturbations be $ \\mathbf{x} \\in \\mathbb{R}^n $ (dimensionless), and let their uncertainty be described by a mean-zero covariance matrix $ \\mathbf{C} \\in \\mathbb{R}^{n \\times n} $. The diagonal elements of $ \\mathbf{C} $ are variances and the off-diagonal elements are covariances.\n- Your program must compute the total variance of $ W $ due to $ \\mathbf{x} $ and provide a principled decomposition of that variance into parameter-level contributions indexed by $ i \\in \\{0,\\dots,n-1\\} $, consistent with the covariance matrix $ \\mathbf{C} $ and the linear sensitivity model. Then, identify dominant contributors by ranking the absolute magnitude of parameter-level contributions, and provide a material-category aggregation to identify which material class contributes the largest aggregate absolute uncertainty.\n\nUnits and reporting requirements:\n- Report the total variance of $ W $ in $ \\text{pcm}^2 $ as a floating-point number.\n- Report the standard deviation of $ W $ in $ \\text{pcm} $ as a floating-point number.\n- Report the top three parameter indices (using zero-based indexing) ordered by decreasing absolute magnitude of their parameter-level variance contributions.\n- Report the dominant material category as an integer ID, defined below, based on the largest aggregate absolute contribution summed over parameters in that material.\n\nYou are provided $ n = 6 $ parameters with a categorical mapping for each parameter $ i $:\n- Material category $ m_i \\in \\{0,1,2\\} $ where $ 0 $ is fuel, $ 1 $ is moderator, and $ 2 $ is absorber.\n- Reaction category $ r_i \\in \\{0,1,2\\} $ where $ 0 $ is absorption, $ 1 $ is scattering, and $ 2 $ is fission.\n- Energy group $ g_i \\in \\{0,1\\} $ where $ 0 $ is fast and $ 1 $ is thermal.\n\nFor all test cases, use the same parameter-to-category mapping vectors:\n- Materials $ m = [\\,2,\\,0,\\,0,\\,1,\\,2,\\,1\\,] $ meaning parameter $ i=0 $ is absorber, $ i=1 $ is fuel, etc.\n- Reactions $ r = [\\,0,\\,2,\\,0,\\,1,\\,0,\\,1\\,] $ meaning parameter $ i=0 $ is absorption, $ i=1 $ is fission, etc.\n- Energy groups $ g = [\\,0,\\,0,\\,1,\\,0,\\,1,\\,1\\,] $ meaning parameter $ i=0 $ is fast, $ i=2 $ is thermal, etc.\n\nConstruct the covariance matrices from per-parameter standard deviations $ \\sigma_i $ and a specified tridiagonal correlation structure. For a given correlation nearest-neighbor coefficient $ \\alpha $, define the correlation matrix $ \\mathbf{R} $ by\n- $ R_{ii} = 1 $ for all $ i $,\n- $ R_{i,i+1} = R_{i+1,i} = \\alpha $ for all adjacent pairs $ i $ and $ i+1 $,\n- all other off-diagonal entries $ R_{ij} = 0 $ for $ |i-j| > 1 $.\nThen the covariance matrix is $ C_{ij} = \\sigma_i \\, \\sigma_j \\, R_{ij} $.\n\nYour program must evaluate the following three test cases:\n\nTest Case A (well-conditioned, moderate positive nearest-neighbor correlation):\n- Sensitivities $ \\mathbf{S}^{(A)} = [\\,150.0,\\,-90.0,\\,60.0,\\,40.0,\\,30.0,\\,-20.0\\,] $ in $ \\text{pcm} $ per unit relative change.\n- Standard deviations $ \\boldsymbol{\\sigma}^{(A)} = [\\,0.02,\\,0.015,\\,0.01,\\,0.01,\\,0.008,\\,0.005\\,] $ (dimensionless).\n- Nearest-neighbor correlation $ \\alpha^{(A)} = 0.25 $.\n\nTest Case B (stronger positive correlation, larger standard deviations):\n- Sensitivities $ \\mathbf{S}^{(B)} = [\\,-120.0,\\,100.0,\\,50.0,\\,-45.0,\\,25.0,\\,15.0\\,] $ in $ \\text{pcm} $ per unit relative change.\n- Standard deviations $ \\boldsymbol{\\sigma}^{(B)} = [\\,0.025,\\,0.02,\\,0.015,\\,0.012,\\,0.01,\\,0.008\\,] $ (dimensionless).\n- Nearest-neighbor correlation $ \\alpha^{(B)} = 0.45 $.\n\nTest Case C (negative nearest-neighbor correlation, singular edge case with one zero variance):\n- Sensitivities $ \\mathbf{S}^{(C)} = [\\,80.0,\\,-70.0,\\,0.0,\\,35.0,\\,-20.0,\\,10.0\\,] $ in $ \\text{pcm} $ per unit relative change.\n- Standard deviations $ \\boldsymbol{\\sigma}^{(C)} = [\\,0.02,\\,0.015,\\,0.0,\\,0.008,\\,0.006,\\,0.005\\,] $ (dimensionless); note $ \\sigma_2 = 0.0 $.\n- Nearest-neighbor correlation $ \\alpha^{(C)} = -0.30 $.\n\nVariance decomposition and dominance identification requirements:\n- Compute the total variance $ \\operatorname{Var}(W) $ using the linear sensitivity model and the covariance matrix $ \\mathbf{C} $ constructed as above.\n- Decompose the variance into parameter-level contributions $ c_i $ that sum to $ \\operatorname{Var}(W) $ and are consistent with the covariance structure.\n- Rank parameters by $ |c_i| $ (absolute contributions) in descending order and return the top three indices.\n- Aggregate contributions by material category $ k \\in \\{0,1,2\\} $ by summing $ |c_i| $ across all parameters belonging to material $ k $; identify the dominant material category as the one with the largest aggregate absolute contribution.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[\\,\\operatorname{Var}(W),\\,\\operatorname{Std}(W),\\,[i_1,i_2,i_3],\\,m^\\star\\,]$ where $ \\operatorname{Var}(W) $ is in $ \\text{pcm}^2 $, $ \\operatorname{Std}(W) $ is in $ \\text{pcm} $, $[i_1,i_2,i_3]$ are the top three parameter indices by absolute contribution, and $ m^\\star $ is the dominant material category ID. For example, the final printed line should look like $[[v_1,s_1,[a,b,c],u_1],[v_2,s_2,[d,e,f],u_2],[v_3,s_3,[g,h,i],u_3]]$ with numerical values in place of symbols.",
            "solution": "The user problem is valid as it is scientifically grounded in the principles of nuclear reactor physics and statistical uncertainty analysis, is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language. We proceed with the solution.\n\nThe problem requires the calculation and decomposition of the variance of a control rod bank worth, denoted by $W$. The bank worth is a measure of reactivity difference, and its uncertainty arises from uncertainties in underlying nuclear physics parameters.\n\nThe relationship between the bank worth $W$ and the uncertain input parameters is given by a first-order linear approximation:\n$$\nW \\approx \\sum_{i=0}^{n-1} S_i x_i = \\mathbf{S}^T \\mathbf{x}\n$$\nwhere $n$ is the number of uncertain parameters, $\\mathbf{x} \\in \\mathbb{R}^n$ is the vector of dimensionless relative perturbations of these parameters, and $\\mathbf{S} \\in \\mathbb{R}^n$ is the vector of sensitivity coefficients. The coefficient $S_i$ represents the change in $W$ (in units of pcm) per unit change in the dimensionless parameter $x_i$. The parameters $x_i$ are treated as random variables with a mean of zero, $E[\\mathbf{x}] = \\mathbf{0}$, and their statistical correlations are described by a covariance matrix $\\mathbf{C} \\in \\mathbb{R}^{n \\times n}$, where $C_{ij} = \\operatorname{Cov}(x_i, x_j)$.\n\nThe total variance of the bank worth, $\\operatorname{Var}(W)$, can be computed using the general formula for the variance of a linear combination of random variables:\n$$\n\\operatorname{Var}(W) = \\operatorname{Var}(\\mathbf{S}^T \\mathbf{x}) = \\mathbf{S}^T \\operatorname{Cov}(\\mathbf{x}) \\mathbf{S} = \\mathbf{S}^T \\mathbf{C} \\mathbf{S}\n$$\nIn summation form, this is expressed as:\n$$\n\\operatorname{Var}(W) = \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} S_i S_j C_{ij}\n$$\nThe covariance matrix $\\mathbf{C}$ is constructed from the parameter standard deviations, $\\sigma_i = \\sqrt{C_{ii}}$, and the correlation matrix $\\mathbf{R}$, where $R_{ij} = \\frac{C_{ij}}{\\sigma_i \\sigma_j}$. The problem specifies the relationship $C_{ij} = \\sigma_i \\sigma_j R_{ij}$. The correlation matrix $\\mathbf{R}$ has a tridiagonal structure with $R_{ii}=1$, $R_{i,i+1}=R_{i+1,i}=\\alpha$, and all other elements being zero.\n\nA principled decomposition of the total variance into parameter-level contributions, $c_i$, must satisfy the condition $\\sum_{i=0}^{n-1} c_i = \\operatorname{Var}(W)$. A standard and consistent approach is to define the contribution of parameter $i$ as its covariance with the total response $W$. This captures the direct effect of parameter $i$ via its own variance and its indirect effects via its covariance with all other parameters.\nLet $W_i = S_i x_i$ be the contribution to $W$ from parameter $i$. Then $W = \\sum_i W_i$. The total variance is $\\operatorname{Var}(W) = \\operatorname{Cov}(W, W) = \\operatorname{Cov}(\\sum_i W_i, W) = \\sum_i \\operatorname{Cov}(W_i, W)$. Defining the parameter-level contribution as $c_i = \\operatorname{Cov}(W_i, W)$ satisfies the summation constraint. The term $c_i$ is calculated as:\n$$\nc_i = \\operatorname{Cov}(S_i x_i, \\sum_{j=0}^{n-1} S_j x_j) = S_i \\sum_{j=0}^{n-1} S_j \\operatorname{Cov}(x_i, x_j) = S_i \\sum_{j=0}^{n-1} C_{ij} S_j\n$$\nIn matrix notation, the vector of contributions $\\mathbf{c} = [c_0, c_1, \\dots, c_{n-1}]^T$ can be calculated as the element-wise product (Hadamard product, $\\circ$) of the sensitivity vector $\\mathbf{S}$ and the vector $\\mathbf{C S}$:\n$$\n\\mathbf{c} = \\mathbf{S} \\circ (\\mathbf{C} \\mathbf{S})\n$$\nThe sum of these contributions is $\\sum_i c_i = \\mathbf{1}^T \\mathbf{c} = \\mathbf{S}^T (\\mathbf{C S}) = \\mathbf{S}^T \\mathbf{C S} = \\operatorname{Var}(W)$, confirming consistency.\n\nTo identify the dominant uncertainty contributors, we perform two analyses:\n1.  **Parameter-Level Dominance:** The absolute values of the contributions, $|c_i|$, are ranked in descending order. The indices of the top three parameters constitute the answer. The absolute value is used because a large negative contribution (from anti-correlation effects) is as significant in magnitude as a large positive one.\n2.  **Material-Category Dominance:** The contributions are aggregated by material type. The problem defines a mapping $m_i$ from each parameter index $i$ to a material category. The aggregate absolute contribution for a material category $k$ is the sum of $|c_i|$ for all parameters $i$ belonging to that category:\n    $$\n    A_k = \\sum_{i | m_i = k} |c_i|\n    $$\nThe dominant material category, $m^\\star$, is the one with the largest aggregate contribution $A_k$.\n\nThe computational procedure for each test case is as follows:\n1.  Given the parameter standard deviations $\\boldsymbol{\\sigma}$ and the nearest-neighbor correlation $\\alpha$, construct the $n \\times n$ correlation matrix $\\mathbf{R}$.\n2.  Construct the covariance matrix $\\mathbf{C}$ using $C_{ij} = \\sigma_i \\sigma_j R_{ij}$. In matrix form, $\\mathbf{C} = \\mathbf{D}_\\sigma \\mathbf{R} \\mathbf{D}_\\sigma$, where $\\mathbf{D}_\\sigma$ is the diagonal matrix of standard deviations.\n3.  Calculate the total variance $\\operatorname{Var}(W) = \\mathbf{S}^T \\mathbf{C} \\mathbf{S}$ and the standard deviation $\\operatorname{Std}(W) = \\sqrt{\\operatorname{Var}(W)}$.\n4.  Compute the vector of parameter-level contributions $\\mathbf{c} = \\mathbf{S} \\circ (\\mathbf{C} \\mathbf{S})$.\n5.  Determine the top three parameter indices by sorting the indices based on the descending order of $|\\mathbf{c}|$.\n6.  Calculate the aggregate absolute contributions $A_k$ for each material category and identify the category $m^\\star$ with the maximum value.\n7.  Format the results as a list $[\\operatorname{Var}(W), \\operatorname{Std}(W), [\\text{top indices}], m^\\star]$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the solution for all test cases.\n    \"\"\"\n\n    # Parameter-to-category mapping, common for all test cases.\n    # n = 6 parameters.\n    # Material category: 0=fuel, 1=moderator, 2=absorber\n    m_map = np.array([2, 0, 0, 1, 2, 1])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"S\": np.array([150.0, -90.0, 60.0, 40.0, 30.0, -20.0]),\n            \"sigma\": np.array([0.02, 0.015, 0.01, 0.01, 0.008, 0.005]),\n            \"alpha\": 0.25,\n        },\n        {\n            \"S\": np.array([-120.0, 100.0, 50.0, -45.0, 25.0, 15.0]),\n            \"sigma\": np.array([0.025, 0.02, 0.015, 0.012, 0.01, 0.008]),\n            \"alpha\": 0.45,\n        },\n        {\n            \"S\": np.array([80.0, -70.0, 0.0, 35.0, -20.0, 10.0]),\n            \"sigma\": np.array([0.02, 0.015, 0.0, 0.008, 0.006, 0.005]),\n            \"alpha\": -0.30,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        S = case[\"S\"]\n        sigma = case[\"sigma\"]\n        alpha = case[\"alpha\"]\n        result = _solve_case(S, sigma, alpha, m_map)\n        results.append(result)\n\n    # Format the final output string to be compact with no whitespace issues.\n    def format_result(res):\n        var_w, std_w, top_indices, dom_mat = res\n        top_indices_str = f\"[{','.join(map(str, top_indices))}]\"\n        return f\"[{var_w},{std_w},{top_indices_str},{dom_mat}]\"\n\n    formatted_results = [format_result(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _solve_case(S, sigma, alpha, m_map):\n    \"\"\"\n    Solves a single test case for variance calculation and decomposition.\n    \n    Args:\n        S (np.ndarray): Sensitivity vector (n,).\n        sigma (np.ndarray): Vector of standard deviations (n,).\n        alpha (float): Nearest-neighbor correlation coefficient.\n        m_map (np.ndarray): Material category mapping for each parameter (n,).\n\n    Returns:\n        list: A list containing [Var(W), Std(W), [top_indices], dominant_material_id].\n    \"\"\"\n    n = len(S)\n\n    # 1. Construct the correlation matrix R\n    R = np.eye(n)\n    if n > 1:\n        off_diag = np.full(n - 1, alpha)\n        R += np.diag(off_diag, k=1)\n        R += np.diag(off_diag, k=-1)\n\n    # 2. Construct the covariance matrix C\n    # C_ij = sigma_i * sigma_j * R_ij\n    # This can be written as C = D_sigma @ R @ D_sigma\n    # where D_sigma is a diagonal matrix of standard deviations.\n    D_sigma = np.diag(sigma)\n    C = D_sigma @ R @ D_sigma\n\n    # 3. Calculate total variance Var(W) = S.T @ C @ S\n    var_W = S.T @ C @ S\n\n    # 4. Calculate total standard deviation Std(W)\n    std_W = np.sqrt(var_W)\n\n    # 5. Decompose variance into parameter-level contributions\n    # c_i = S_i * sum_j(C_ij * S_j)\n    # In vector form: c = S * (C @ S) where * is element-wise product.\n    c = S * (C @ S)\n\n    # 6. Identify top three contributing parameters by absolute contribution\n    abs_c = np.abs(c)\n    # argsort gives indices from smallest to largest, so we reverse it.\n    top_indices = np.argsort(abs_c)[::-1][:3].tolist()\n\n    # 7. Identify dominant material category\n    num_materials = np.max(m_map) + 1 if len(m_map) > 0 else 0\n    agg_contributions = np.zeros(num_materials)\n    for i in range(n):\n        agg_contributions[m_map[i]] += abs_c[i]\n    \n    dominant_material_id = np.argmax(agg_contributions) if num_materials > 0 else -1\n\n    return [var_W, std_W, top_indices, int(dominant_material_id)]\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}