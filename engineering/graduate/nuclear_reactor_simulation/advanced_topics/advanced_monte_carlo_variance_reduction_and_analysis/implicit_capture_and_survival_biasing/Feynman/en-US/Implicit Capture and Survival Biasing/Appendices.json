{
    "hands_on_practices": [
        {
            "introduction": "Implicit capture, or survival biasing, is a powerful variance reduction technique that replaces the stochastic absorption event with a deterministic weight reduction. This first practice provides a foundational understanding by tasking you to derive the variance of a capture rate estimator from first principles in an idealized infinite medium. By directly comparing the analog Monte Carlo method with implicit capture, you will discover the remarkable theoretical efficiency of this technique .",
            "id": "4231607",
            "problem": "Consider an infinite, homogeneous, non-multiplying medium with macroscopic capture cross section $\\Sigma_{c} > 0$ and macroscopic scattering cross section $\\Sigma_{s} \\ge 0$, so that the macroscopic total cross section is $\\Sigma_{t} = \\Sigma_{c} + \\Sigma_{s}$. A neutron history begins with weight $1$ at the origin, and undergoes a sequence of free flights and collisions. The free-flight path length is exponentially distributed with parameter $\\Sigma_{t}$, reflecting the standard interaction law in Monte Carlo (MC) particle transport. At each collision, an analog history samples a reaction outcome: capture with probability $p = \\Sigma_{c}/\\Sigma_{t}$ or scatter with probability $q = \\Sigma_{s}/\\Sigma_{t}$, independently of all previous events. In contrast, a survival-biasing (implicit capture) history forces every collision to be a scatter while reducing the particle weight by the survival factor $q$ at each collision. Assume isotropic scattering, no boundaries (infinite medium), and no leakage or fission.\n\nDefine the collision estimator for the capture rate per source history as follows: at each collision $i$, score $w_{i-1}\\,p$, where $w_{i-1}$ is the particle weight just before the $i$-th collision. For analog absorption, $w_{i-1} = 1$ until the history terminates on the first sampled capture; for survival biasing (implicit capture), $w_{i-1} = q^{i-1}$ and the history continues indefinitely.\n\nStarting solely from the exponential free-flight law and the Bernoulli selection of reaction outcomes at collisions, derive from first principles the variance of the per-history collision estimator of the capture rate for:\n- analog absorption, and\n- survival biasing (implicit capture).\n\nExpress your final answers in closed form as functions of $\\Sigma_{c}$ and $\\Sigma_{s}$ only. Provide the two variances as a single row matrix in the order $\\left(\\text{implicit},\\,\\text{analog}\\right)$. No numerical rounding is required, and no physical units are to be reported in the final boxed answer.",
            "solution": "The problem asks for the variance of the per-history collision estimator for the capture rate in an infinite, homogeneous medium, for two different Monte Carlo schemes: survival biasing (implicit capture) and analog absorption. The variance of a random variable $X$, representing the total score for one history, is given by $\\text{Var}(X) = E[X^2] - (E[X])^2$. We will derive this for each case.\n\nLet $p = \\frac{\\Sigma_c}{\\Sigma_t}$ be the single-collision capture probability and $q = \\frac{\\Sigma_s}{\\Sigma_t}$ be the single-collision survival probability, where $\\Sigma_t = \\Sigma_c + \\Sigma_s$. By definition, $p+q=1$. The problem states $\\Sigma_c > 0$, which implies $p > 0$ and $q  1$.\n\nThe collision estimator scores a value of $w_{i-1}p$ at each collision $i$, where $w_{i-1}$ is the neutron weight before the collision. The total score for a history, $X$, is the sum of these scores over all collisions that occur in that history.\n\n**1. Survival Biasing (Implicit Capture)**\n\nIn this scheme, every collision is forced to be a scatter. The history never terminates. The particle's weight is adjusted at each collision to account for the probability of capture. The initial weight is $w_0 = 1$. After the first collision (which is a forced scatter), the new weight is $w_1 = w_0 q = q$. After the second collision, it is $w_2 = w_1 q = q^2$. In general, the weight before the $i$-th collision is $w_{i-1} = q^{i-1}$.\n\nThe score contributed at the $i$-th collision is $w_{i-1}p = q^{i-1}p$. Since the history is infinite, the total score for any given history, $X_{\\text{implicit}}$, is the sum over all collisions:\n$$X_{\\text{implicit}} = \\sum_{i=1}^{\\infty} w_{i-1}p = \\sum_{i=1}^{\\infty} q^{i-1}p$$\nThis is a deterministic quantity, not a random variable, because every history is identical in terms of its sequence of weight multipliers. The sum is a geometric series:\n$$X_{\\text{implicit}} = p \\sum_{i=1}^{\\infty} q^{i-1} = p \\sum_{j=0}^{\\infty} q^j$$\nSince $\\Sigma_c > 0$, we have $q = \\Sigma_s / (\\Sigma_c + \\Sigma_s)  1$. Therefore, the geometric series converges to $\\frac{1}{1-q}$.\n$$X_{\\text{implicit}} = p \\left(\\frac{1}{1-q}\\right)$$\nUsing the identity $p = 1-q$, we find:\n$$X_{\\text{implicit}} = p \\left(\\frac{1}{p}\\right) = 1$$\nThe score for every history is exactly $1$. For a constant random variable $C$, the variance is $\\text{Var}(C) = E[(C - E[C])^2] = E[(C-C)^2] = 0$. Thus, the variance of the implicit capture estimator is:\n$$\\text{Var}(X_{\\text{implicit}}) = 0$$\n\n**2. Analog Absorption**\n\nIn this scheme, the particle weight remains $w_i = 1$ for the entire history. At each collision, the outcome is sampled: capture with probability $p$, or scatter with probability $q$. The history terminates upon the first sampled capture event.\n\nLet $N$ be the random variable representing the collision number at which the history terminates. The probability that the first $k-1$ collisions are scatters and the $k$-th collision is a capture is given by the geometric probability mass function:\n$$P(N=k) = q^{k-1}p, \\quad \\text{for } k = 1, 2, 3, \\ldots$$\nFor a history that terminates at collision $k$, there are exactly $k$ collisions. The weight before each of these collisions is $w_{i-1} = 1$. The score contribution at each collision $i \\in \\{1, \\ldots, k\\}$ is $w_{i-1}p = 1 \\cdot p = p$.\nThe total score for a history of length $k$, which we denote $X_k$, is the sum of the scores from these $k$ collisions:\n$$X_k = \\sum_{i=1}^{k} p = kp$$\nSince the length of the history $k$ is a random variable, the total score $X_{\\text{analog}}$ is a random variable that takes values $kp$ with probability $q^{k-1}p$.\n\nFirst, we compute the expected value of the score, $E[X_{\\text{analog}}]$.\n$$E[X_{\\text{analog}}] = \\sum_{k=1}^{\\infty} X_k P(N=k) = \\sum_{k=1}^{\\infty} (kp)(q^{k-1}p) = p^2 \\sum_{k=1}^{\\infty} k q^{k-1}$$\nThe sum is the derivative of the geometric series formula $\\sum_{k=0}^{\\infty} q^k = (1-q)^{-1}$:\n$$\\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{d}{dq} \\sum_{k=0}^{\\infty} q^k = \\frac{d}{dq}(1-q)^{-1} = (-1)(1-q)^{-2}(-1) = \\frac{1}{(1-q)^2}$$\nSubstituting this back, and using $1-q=p$:\n$$E[X_{\\text{analog}}] = p^2 \\left(\\frac{1}{(1-q)^2}\\right) = p^2 \\left(\\frac{1}{p^2}\\right) = 1$$\nThis result confirms that the estimator is unbiased, as the expected capture rate per source history is correctly found to be $1$.\n\nNext, we compute the second moment, $E[X_{\\text{analog}}^2]$.\n$$E[X_{\\text{analog}}^2] = \\sum_{k=1}^{\\infty} X_k^2 P(N=k) = \\sum_{k=1}^{\\infty} (kp)^2(q^{k-1}p) = p^3 \\sum_{k=1}^{\\infty} k^2 q^{k-1}$$\nTo evaluate the sum $\\sum k^2 q^{k-1}$, we can differentiate the identity $\\sum_{k=1}^{\\infty} k q^{k-1} = (1-q)^{-2}$. Multiplying by $q$ gives $\\sum_{k=1}^{\\infty} k q^k = q(1-q)^{-2}$. Differentiating with respect to $q$:\n$$\\sum_{k=1}^{\\infty} k^2 q^{k-1} = \\frac{d}{dq} \\left[q(1-q)^{-2}\\right] = (1)(1-q)^{-2} + q(-2)(1-q)^{-3}(-1) = \\frac{1}{(1-q)^2} + \\frac{2q}{(1-q)^3}$$\n$$= \\frac{1-q+2q}{(1-q)^3} = \\frac{1+q}{(1-q)^3}$$\nSubstituting this result into the expression for the second moment:\n$$E[X_{\\text{analog}}^2] = p^3 \\left(\\frac{1+q}{(1-q)^3}\\right) = p^3 \\left(\\frac{1+q}{p^3}\\right) = 1+q$$\nFinally, we compute the variance:\n$$\\text{Var}(X_{\\text{analog}}) = E[X_{\\text{analog}}^2] - (E[X_{\\text{analog}}])^2 = (1+q) - (1)^2 = q$$\n\nThe variance of the analog estimator is $q$. Expressing this in terms of the given cross sections:\n$$\\text{Var}(X_{\\text{analog}}) = q = \\frac{\\Sigma_s}{\\Sigma_t} = \\frac{\\Sigma_s}{\\Sigma_c + \\Sigma_s}$$\n\n**Summary of Results**\n\nThe variances for the two methods are:\n-   Variance for survival biasing (implicit capture): $\\text{Var}(X_{\\text{implicit}}) = 0$.\n-   Variance for analog absorption: $\\text{Var}(X_{\\text{analog}}) = q = \\frac{\\Sigma_s}{\\Sigma_c + \\Sigma_s}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  \\frac{\\Sigma_s}{\\Sigma_c + \\Sigma_s}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While implicit capture effectively reduces variance by ensuring particle survival, it creates a practical challenge: the proliferation of computationally expensive particles with low statistical weight, $w$. To manage this, a technique called Russian roulette is employed to terminate unimportant particles in an unbiased way. This exercise delves into this crucial trade-off by having you derive the variance introduced by the Russian roulette game itself, demonstrating that there is often a cost associated with variance reduction .",
            "id": "4231626",
            "problem": "In Monte Carlo (MC) neutron transport for nuclear reactor simulation, implicit capture is often used as a variance reduction (VR) technique: each collision is treated as scatter, and the particle weight is multiplied by the scattering survival probability (the ratio of scattering macroscopic cross section to total macroscopic cross section), so that the weight decreases deterministically without explicit absorption. To prevent the proliferation of very low-weight particles after repeated implicit capture, Russian roulette survival biasing is applied. In one widely used scheme, a particle with incoming weight $w$ is subjected to a survival trial with survival probability $p$, and its post-roulette weight $w'$ is defined as $w' = 0$ with probability $1 - p$ and $w' = \\frac{w}{p}$ with probability $p$. This scheme is designed so that the expected weight is preserved under survival biasing in order to maintain unbiased tallies.\n\nStarting from the definitions of expectation $\\mathbb{E}[X]$ and variance $\\mathrm{Var}(X) = \\mathbb{E}[X^{2}] - \\left(\\mathbb{E}[X]\\right)^{2}$, derive a closed-form expression for $\\mathrm{Var}(w')$ as a function of the survival probability $p$ and incoming weight $w$. Then, based on first principles for linear tallies that are proportional to particle weight, explain how the dependence of $\\mathrm{Var}(w')$ on $p$ qualitatively affects tally variance when Russian roulette is used in conjunction with implicit capture.\n\nExpress the final expression for $\\mathrm{Var}(w')$ in a fully simplified, exact symbolic form. No numerical rounding is required, and no physical units are involved.",
            "solution": "The task is to derive a closed-form expression for the variance of the post-roulette particle weight, $\\mathrm{Var}(w')$, and to explain its implications for tally variance. Let $w$ be the incoming particle weight and $p$ be the survival probability in the Russian roulette game. The post-roulette weight, $w'$, is a discrete random variable. Its value is $\\frac{w}{p}$ with probability $p$, and $0$ with probability $1-p$.\n\nThe variance of a random variable $X$ is defined as $\\mathrm{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}$. We will first compute the expectation (first moment), $\\mathbb{E}[w']$, and the second moment, $\\mathbb{E}[(w')^{2}]$.\n\nThe expectation of $w'$ is the sum of its possible values weighted by their respective probabilities:\n$$\n\\mathbb{E}[w'] = \\left(\\frac{w}{p}\\right) \\cdot p + (0) \\cdot (1-p) = w + 0 = w\n$$\nThis confirms that the Russian roulette scheme is unbiased, as the expected weight of the particle is conserved.\n\nThe second moment of $w'$ is the expectation of its square:\n$$\n\\mathbb{E}[(w')^{2}] = \\left(\\frac{w}{p}\\right)^{2} \\cdot p + (0)^{2} \\cdot (1-p) = \\frac{w^{2}}{p^{2}} \\cdot p = \\frac{w^{2}}{p}\n$$\nNow, we can substitute these moments into the formula for variance:\n$$\n\\mathrm{Var}(w') = \\mathbb{E}[(w')^{2}] - (\\mathbb{E}[w'])^{2}\n$$\n$$\n\\mathrm{Var}(w') = \\frac{w^{2}}{p} - w^{2}\n$$\nFactoring out the $w^{2}$ term, we obtain the simplified closed-form expression:\n$$\n\\mathrm{Var}(w') = w^{2} \\left(\\frac{1}{p} - 1\\right) = w^{2} \\left(\\frac{1-p}{p}\\right)\n$$\nThis is the derived expression for the variance of the post-roulette weight.\n\nNext, we analyze the effect of this variance on simulation tallies. In Monte Carlo methods, tallies are estimators for physical quantities, and for linear tallies, the score contributed by a particle is proportional to its weight. The total variance of a tally is the aggregate of variances from all stochastic events in the particle histories. The Russian roulette game is one such stochastic event, and it introduces an amount of variance given by $\\mathrm{Var}(w')$.\n\nThe derived expression $\\mathrm{Var}(w') = w^{2} \\frac{1-p}{p}$ shows that the variance is a function of the survival probability $p$. Given that $p \\in (0, 1]$, an analysis of this function reveals its behavior. The term $\\frac{1-p}{p}$ is a monotonically decreasing function of $p$.\n- As the survival probability $p$ approaches $1$, the variance $\\mathrm{Var}(w')$ approaches $0$. A value of $p=1$ corresponds to a deterministic game where the particle always survives with its weight unchanged ($w' = w/1 = w$), hence there is no added variance.\n- As the survival probability $p$ approaches $0$, the variance $\\mathrm{Var}(w')$ approaches infinity. A very small $p$ represents a game with a high probability of termination (outcome $0$) and a very low probability of survival, but if the particle survives, its weight becomes extremely large ($\\frac{w}{p}$). This large spread in possible outcomes results in a massive variance.\n\nThis relationship is critical for the efficiency of the simulation. Russian roulette is employed to manage computational resources by terminating particles with low importance (low weight). However, the choice of the survival probability $p$ involves a trade-off. Using a very small $p$ aggressively culls particles, which can reduce computation time per particle history. But, as the derivation shows, a small $p$ introduces a large variance. This increase in variance degrades the statistical quality of the tallies, meaning more particle histories are required to achieve a desired level of precision. The overall efficiency of a Monte Carlo simulation is often measured by a figure of merit (FOM), which is inversely proportional to the product of variance and computation time. An excessively large variance introduced by an aggressive Russian roulette strategy can decrease the FOM, making the simulation less efficient. Therefore, the parameter $p$ must be chosen carefully to balance the need for population control against the introduction of statistical noise.",
            "answer": "$$\n\\boxed{w^{2} \\frac{1-p}{p}}\n$$"
        },
        {
            "introduction": "In sophisticated Monte Carlo codes, variance reduction methods are often used in combination, which requires a deep understanding of each technique to avoid subtle errors. This practice explores the interaction between implicit capture and delta-tracking, a method that uses 'fictitious' collisions to simplify particle transport. You will analyze a common but incorrect implementation to quantify the resulting bias, reinforcing the critical lesson that simulation techniques must be grounded in correct physical principles .",
            "id": "4231578",
            "problem": "Consider a homogeneous region in a nuclear reactor simulation with constant macroscopic cross sections: total cross section $\\sigma_{t}$, capture cross section $\\sigma_{c}$, and scattering cross section $\\sigma_{s}$, where $\\sigma_{t} = \\sigma_{c} + \\sigma_{s}$. A neutron is tracked using Woodcock (also called delta-tracking) with a majorant cross section $\\sigma_{M}$ such that $\\sigma_{M} \\ge \\sigma_{t}$ everywhere. In Woodcock sampling, free-flight distances are sampled using $\\sigma_{M}$, generating a sequence of collision sites. At each sampled site, with probability $\\sigma_{t}/\\sigma_{M}$ a collision is real and is then classified into capture or scattering according to the actual branching ratios; otherwise the collision is fictitious and produces no physical reaction.\n\nWe focus on survival biasing (implicit capture), where the neutron is not terminated upon capture; instead, the neutronâ€™s statistical weight is reduced according to a rule intended to preserve the expected reaction rates. Define the capture tally for a single straight flight segment of length $L$ with initial weight $w_{0}$ as the expected absorbed weight along the segment, equivalently the expected weight loss due to capture during the segment. For scientific realism, assume there are no boundaries or sources within $L$, and the neutron continues along a straight path without geometry-induced termination.\n\nStarting from the fundamental base that:\n- The hazard rate for capture in a homogeneous medium is $\\sigma_{c}$, so the survival probability over a path length $L$ under correct physics is $\\exp(-\\sigma_{c} L)$.\n- Under Woodcock sampling, the number of sampled collision sites along a path of length $L$ is a Poisson random variable with mean $\\sigma_{M} L$.\n\nSuppose a practitioner mistakenly applies implicit capture at every sampled collision site (real and fictitious) by reducing the neutron weight by the factor $1 - \\sigma_{c}/\\sigma_{t}$ at each site. This practice is sometimes informally motivated by extending collision-based estimators to null collisions, but it is scientifically incorrect because fictitious collisions do not correspond to physical hazards at the rate $\\sigma_{M}$.\n\nUsing only the above foundations and consistent reasoning, derive a closed-form expression for the ratio\n$$R(L)$$\ndefined as the incorrect capture tally divided by the correct capture tally for the flight segment of length $L$. Express your final answer as a dimensionless analytic expression in terms of $\\sigma_{c}$, $\\sigma_{t}$, $\\sigma_{M}$, and $L$. No numerical evaluation is required. If you need to simplify exponentials, do so exactly. Round nothing. Provide $R(L)$ only, as a single closed-form expression.",
            "solution": "First, we determine the correct capture tally. The capture tally is defined as the expected weight loss due to capture. For a homogeneous medium, the process of weight reduction due to capture is governed by the continuous attenuation law. The rate of weight change, $\\frac{dw}{dx}$, for a neutron of weight $w(x)$ at position $x$ along its path is proportional to the macroscopic capture cross section $\\sigma_c$ and the weight itself. This is described by the differential equation:\n$$\n\\frac{dw(x)}{dx} = -\\sigma_c w(x)\n$$\nGiven an initial weight $w_0$ at the start of the segment ($x=0$), we can solve this equation by separation of variables:\n$$\n\\int_{w_0}^{w(L)} \\frac{dw}{w} = \\int_{0}^{L} (-\\sigma_c) dx\n$$\n$$\n\\ln\\left(\\frac{w(L)}{w_0}\\right) = -\\sigma_c L\n$$\nThe weight of the neutron after traveling a distance $L$ is therefore:\n$$\nw(L) = w_0 \\exp(-\\sigma_c L)\n$$\nThis result is consistent with the provided foundation that the physical survival probability over a path length $L$ is $\\exp(-\\sigma_c L)$. The correct capture tally, denoted $T_{correct}$, is the initial weight minus the final weight:\n$$\nT_{correct} = w_0 - w(L) = w_0 - w_0 \\exp(-\\sigma_c L) = w_0 (1 - \\exp(-\\sigma_c L))\n$$\n\nNext, we determine the incorrect capture tally resulting from the practitioner's mistaken procedure. The problem states that the number of sampled collision sites, $N$, over a path of length $L$ follows a Poisson distribution with mean $\\lambda = \\sigma_M L$. The probability of observing exactly $k$ sampled sites is:\n$$\nP(N=k) = \\frac{(\\sigma_M L)^k \\exp(-\\sigma_M L)}{k!}\n$$\nAccording to the incorrect procedure, at each of these $k$ sampled sites (both real and fictitious), the neutron's weight is reduced by a factor of $(1 - \\sigma_c/\\sigma_t)$. If there are $k$ sites, the final weight $w_k$ after traversing the segment of length $L$ is:\n$$\nw_k = w_0 \\left(1 - \\frac{\\sigma_c}{\\sigma_t}\\right)^k\n$$\nThe incorrect capture tally, $T_{incorrect}$, is the expected value of the weight loss, $w_0 - w_N$, where the expectation is taken over the random variable $N$.\n$$\nT_{incorrect} = E[w_0 - w_N] = w_0 - E[w_N]\n$$\nWe compute the expected final weight $E[w_N]$:\n$$\nE[w_N] = E\\left[w_0 \\left(1 - \\frac{\\sigma_c}{\\sigma_t}\\right)^N\\right] = w_0 E\\left[\\left(1 - \\frac{\\sigma_c}{\\sigma_t}\\right)^N\\right]\n$$\nTo evaluate the expectation of $a^N$ where $a = (1 - \\sigma_c/\\sigma_t)$ and $N \\sim \\text{Poisson}(\\lambda)$, we use the definition of expectation for a discrete random variable:\n$$\nE[a^N] = \\sum_{k=0}^{\\infty} a^k P(N=k) = \\sum_{k=0}^{\\infty} a^k \\frac{\\lambda^k \\exp(-\\lambda)}{k!}\n$$\n$$\nE[a^N] = \\exp(-\\lambda) \\sum_{k=0}^{\\infty} \\frac{(a\\lambda)^k}{k!}\n$$\nThe summation is the Maclaurin series for $\\exp(a\\lambda)$. Therefore:\n$$\nE[a^N] = \\exp(-\\lambda) \\exp(a\\lambda) = \\exp(\\lambda(a-1))\n$$\nSubstituting back $\\lambda = \\sigma_M L$ and $a = (1 - \\sigma_c/\\sigma_t)$:\n$$\nE\\left[\\left(1 - \\frac{\\sigma_c}{\\sigma_t}\\right)^N\\right] = \\exp\\left(\\sigma_M L \\left(\\left(1 - \\frac{\\sigma_c}{\\sigma_t}\\right) - 1\\right)\\right) = \\exp\\left(\\sigma_M L \\left(-\\frac{\\sigma_c}{\\sigma_t}\\right)\\right) = \\exp\\left(-\\frac{\\sigma_M \\sigma_c}{\\sigma_t} L\\right)\n$$\nThe expected final weight under the incorrect procedure is:\n$$\nE[w_N] = w_0 \\exp\\left(-\\frac{\\sigma_M \\sigma_c}{\\sigma_t} L\\right)\n$$\nThe incorrect capture tally is thus:\n$$\nT_{incorrect} = w_0 - E[w_N] = w_0 \\left(1 - \\exp\\left(-\\frac{\\sigma_M \\sigma_c}{\\sigma_t} L\\right)\\right)\n$$\n\nFinally, we compute the ratio $R(L)$ as the incorrect tally divided by the correct tally:\n$$\nR(L) = \\frac{T_{incorrect}}{T_{correct}} = \\frac{w_0 \\left(1 - \\exp\\left(-\\frac{\\sigma_M \\sigma_c}{\\sigma_t} L\\right)\\right)}{w_0 (1 - \\exp(-\\sigma_c L))}\n$$\nThe initial weight $w_0$ cancels, yielding the final expression for the ratio:\n$$\nR(L) = \\frac{1 - \\exp\\left(-\\frac{\\sigma_M \\sigma_c}{\\sigma_t} L\\right)}{1 - \\exp(-\\sigma_c L)}\n$$\nThis expression is dimensionless and depends only on the given parameters $\\sigma_c$, $\\sigma_t$, $\\sigma_M$, and $L$, as required.",
            "answer": "$$\\boxed{\\frac{1 - \\exp\\left(-\\frac{\\sigma_M \\sigma_c}{\\sigma_t} L\\right)}{1 - \\exp(-\\sigma_c L)}}$$"
        }
    ]
}