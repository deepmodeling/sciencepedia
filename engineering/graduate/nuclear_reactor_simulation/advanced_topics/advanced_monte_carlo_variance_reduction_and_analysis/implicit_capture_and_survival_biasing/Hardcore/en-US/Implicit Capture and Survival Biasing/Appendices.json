{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration, we will analyze the performance of a collision-based capture estimator. This exercise contrasts the statistical behavior of a traditional analog simulation, where particles may be absorbed at collisions, with a simulation employing implicit capture, where particles always survive with a reduced statistical weight. By deriving the variance from first principles in an idealized infinite medium, you will directly and quantitatively observe the powerful impact of this technique .",
            "id": "4231607",
            "problem": "Consider an infinite, homogeneous, non-multiplying medium with macroscopic capture cross section $\\Sigma_{c} > 0$ and macroscopic scattering cross section $\\Sigma_{s} \\ge 0$, so that the macroscopic total cross section is $\\Sigma_{t} = \\Sigma_{c} + \\Sigma_{s}$. A neutron history begins with weight $1$ at the origin, and undergoes a sequence of free flights and collisions. The free-flight path length is exponentially distributed with parameter $\\Sigma_{t}$, reflecting the standard interaction law in Monte Carlo (MC) particle transport. At each collision, an analog history samples a reaction outcome: capture with probability $p = \\Sigma_{c}/\\Sigma_{t}$ or scatter with probability $q = \\Sigma_{s}/\\Sigma_{t}$, independently of all previous events. In contrast, a survival-biasing (implicit capture) history forces every collision to be a scatter while reducing the particle weight by the survival factor $q$ at each collision. Assume isotropic scattering, no boundaries (infinite medium), and no leakage or fission.\n\nDefine the collision estimator for the capture rate per source history as follows: at each collision $i$, score $w_{i-1}\\,p$, where $w_{i-1}$ is the particle weight just before the $i$-th collision. For analog absorption, $w_{i-1} = 1$ until the history terminates on the first sampled capture; for survival biasing (implicit capture), $w_{i-1} = q^{i-1}$ and the history continues indefinitely.\n\nStarting solely from the exponential free-flight law and the Bernoulli selection of reaction outcomes at collisions, derive from first principles the variance of the per-history collision estimator of the capture rate for:\n- analog absorption, and\n- survival biasing (implicit capture).\n\nExpress your final answers in closed form as functions of $\\Sigma_{c}$ and $\\Sigma_{s}$ only. Provide the two variances as a single row matrix in the order $\\left(\\text{implicit},\\,\\text{analog}\\right)$. No numerical rounding is required, and no physical units are to be reported in the final boxed answer.",
            "solution": "The problem asks for the variance of the per-history collision estimator for the capture rate in an infinite, homogeneous medium, for two different Monte Carlo schemes: survival biasing (implicit capture) and analog absorption. The variance of a random variable $X$, representing the total score for one history, is given by $\\text{Var}(X) = E[X^2] - (E[X])^2$. We will derive this for each case.\n\nLet $p = \\frac{\\Sigma_{c}}{\\Sigma_{t}}$ be the single-collision capture probability and $q = \\frac{\\Sigma_{s}}{\\Sigma_{t}}$ be the single-collision survival probability, where $\\Sigma_{t} = \\Sigma_{c} + \\Sigma_{s}$. By definition, $p+q=1$. The problem states $\\Sigma_{c} > 0$, which implies $p > 0$ and $q  1$.\n\nThe collision estimator scores a value of $w_{i-1}p$ at each collision $i$, where $w_{i-1}$ is the neutron weight before the collision. The total score for a history, $X$, is the sum of these scores over all collisions that occur in that history.\n\n**1. Survival Biasing (Implicit Capture)**\n\nIn this scheme, every collision is forced to be a scatter. The history never terminates. The particle's weight is adjusted at each collision to account for the probability of capture. The initial weight is $w_0 = 1$. After the first collision (which is a forced scatter), the new weight is $w_1 = w_0 q = q$. After the second collision, it is $w_2 = w_1 q = q^2$. In general, the weight before the $i$-th collision is $w_{i-1} = q^{i-1}$.\n\nThe score contributed at the $i$-th collision is $w_{i-1}p = q^{i-1}p$. Since the history is infinite, the total score for any given history, $X_{\\text{implicit}}$, is the sum over all collisions:\n$$X_{\\text{implicit}} = \\sum_{i=1}^{\\infty} w_{i-1}p = \\sum_{i=1}^{\\infty} q^{i-1}p$$\nThis is a deterministic quantity, not a random variable, because every history is identical in terms of its sequence of weight multipliers. The sum is a geometric series:\n$$X_{\\text{implicit}} = p \\sum_{i=1}^{\\infty} q^{i-1} = p \\sum_{j=0}^{\\infty} q^j$$\nSince $\\Sigma_{c} > 0$, we have $q = \\Sigma_{s} / (\\Sigma_{c} + \\Sigma_{s})  1$. Therefore, the geometric series converges to $\\frac{1}{1-q}$.\n$$X_{\\text{implicit}} = p \\left(\\frac{1}{1-q}\\right)$$\nUsing the identity $p = 1-q$, we find:\n$$X_{\\text{implicit}} = p \\left(\\frac{1}{p}\\right) = 1$$\nThe score for every history is exactly $1$. For a constant random variable $C$, the variance is $\\text{Var}(C) = E[(C - E[C])^2] = E[(C-C)^2] = 0$. Thus, the variance of the implicit capture estimator is:\n$$\\text{Var}(X_{\\text{implicit}}) = 0$$\n\n**2. Analog Absorption**\n\nIn this scheme, the particle weight remains $w_i = 1$ for the entire history. At each collision, the outcome is sampled: capture with probability $p$, or scatter with probability $q$. The history terminates upon the first sampled capture event.\n\nLet $N$ be the random variable representing the collision number at which the history terminates. The probability that the first $k-1$ collisions are scatters and the $k$-th collision is a capture is given by the geometric probability mass function:\n$$P(N=k) = q^{k-1}p, \\quad \\text{for } k = 1, 2, 3, \\ldots$$\nFor a history that terminates at collision $k$, there are exactly $k$ collisions. The weight before each of these collisions is $w_{i-1} = 1$. The score contribution at each collision $i \\in \\{1, \\ldots, k\\}$ is $w_{i-1}p = 1 \\cdot p = p$.\nThe total score for a history of length $k$, which we denote $X_k$, is the sum of the scores from these $k$ collisions:\n$$X_k = \\sum_{i=1}^{k} p = kp$$\nSince the length of the history $k$ is a random variable, the total score $X_{\\text{analog}}$ is a random variable that takes values $kp$ with probability $q^{k-1}p$.\n\nFirst, we compute the expected value of the score, $E[X_{\\text{analog}}]$.\n$$E[X_{\\text{analog}}] = \\sum_{k=1}^{\\infty} X_k P(N=k) = \\sum_{k=1}^{\\infty} (kp)(q^{k-1}p) = p^2 \\sum_{k=1}^{\\infty} k q^{k-1}$$\nThe sum is the derivative of the geometric series formula $\\sum_{k=0}^{\\infty} q^k = (1-q)^{-1}$:\n$$\\sum_{k=1}^{\\infty} k q^{k-1} = \\frac{d}{dq} \\sum_{k=0}^{\\infty} q^k = \\frac{d}{dq}(1-q)^{-1} = (-1)(1-q)^{-2}(-1) = \\frac{1}{(1-q)^2}$$\nSubstituting this back, and using $1-q=p$:\n$$E[X_{\\text{analog}}] = p^2 \\left(\\frac{1}{(1-q)^2}\\right) = p^2 \\left(\\frac{1}{p^2}\\right) = 1$$\nThis result confirms that the estimator is unbiased, as the expected capture rate per source history is correctly found to be $1$.\n\nNext, we compute the second moment, $E[X_{\\text{analog}}^2]$.\n$$E[X_{\\text{analog}}^2] = \\sum_{k=1}^{\\infty} X_k^2 P(N=k) = \\sum_{k=1}^{\\infty} (kp)^2(q^{k-1}p) = p^3 \\sum_{k=1}^{\\infty} k^2 q^{k-1}$$\nTo evaluate the sum $\\sum k^2 q^{k-1}$, we can differentiate the identity $\\sum_{k=1}^{\\infty} k q^{k-1} = (1-q)^{-2}$. Multiplying by $q$ gives $\\sum_{k=1}^{\\infty} k q^k = q(1-q)^{-2}$. Differentiating with respect to $q$:\n$$\\sum_{k=1}^{\\infty} k^2 q^{k-1} = \\frac{d}{dq} \\left[q(1-q)^{-2}\\right] = (1)(1-q)^{-2} + q(-2)(1-q)^{-3}(-1) = \\frac{1}{(1-q)^2} + \\frac{2q}{(1-q)^3}$$\n$$= \\frac{1-q+2q}{(1-q)^3} = \\frac{1+q}{(1-q)^3}$$\nSubstituting this result into the expression for the second moment:\n$$E[X_{\\text{analog}}^2] = p^3 \\left(\\frac{1+q}{(1-q)^3}\\right) = p^3 \\left(\\frac{1+q}{p^3}\\right) = 1+q$$\nFinally, we compute the variance:\n$$\\text{Var}(X_{\\text{analog}}) = E[X_{\\text{analog}}^2] - (E[X_{\\text{analog}}])^2 = (1+q) - (1)^2 = q$$\n\nThe variance of the analog estimator is $q$. Expressing this in terms of the given cross sections:\n$$\\text{Var}(X_{\\text{analog}}) = q = \\frac{\\Sigma_{s}}{\\Sigma_{t}} = \\frac{\\Sigma_{s}}{\\Sigma_{c} + \\Sigma_{s}}$$\n\n**Summary of Results**\n\nThe variances for the two methods are:\n-   Variance for survival biasing (implicit capture): $\\text{Var}(X_{\\text{implicit}}) = 0$.\n-   Variance for analog absorption: $\\text{Var}(X_{\\text{analog}}) = q = \\frac{\\Sigma_{s}}{\\Sigma_{c} + \\Sigma_{s}}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  \\frac{\\Sigma_{s}}{\\Sigma_{c} + \\Sigma_{s}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having seen the dramatic variance reduction in a specific case, we now seek a more general and formal justification for why survival biasing is so effective. This problem uses the powerful Law of Total Variance to prove that implicit capture, as an application of the Conditional Monte Carlo method, is guaranteed to reduce or maintain variance. By working through this proof in the context of a simplified eigenvalue problem , you will solidify your theoretical understanding of this fundamental principle.",
            "id": "4231631",
            "problem": "Consider a highly absorbing one-dimensional slab of material of thickness $d$ expressed in centimeters, where a monoenergetic neutron source is uniformly distributed along the entry face and neutrons travel in straight lines perpendicular to that face. Assume negligible scattering in the interior of the slab so that the only interaction in the slab is absorption characterized by a constant macroscopic absorption cross section $\\Sigma_a$ expressed in $\\text{cm}^{-1}$. At the far face of the slab there is a thin fission-producing layer that, upon neutron arrival, produces a deterministic number $\\nu$ of fission neutrons per incident neutron. The neutron multiplication factor (commonly denoted $k$ in $k$-eigenvalue problems) for this simplified system can be estimated per generation by the expected number of fission neutrons produced per source neutron; this estimate is dimensionless.\n\nThe goal is to demonstrate, in the context of Monte Carlo (MC) neutron transport, that implicit capture (also known as survival biasing), which continues histories with attenuated statistical weight rather than terminating them upon absorption, reduces the variance of the $k$-eigenvalue estimator in highly absorbing regions by sustaining histories that would otherwise terminate prematurely.\n\nBase your derivation on fundamental laws and core definitions:\n- The Beer–Lambert law of exponential attenuation, which states that the survival probability along a straight path of length $L$ in a purely absorbing medium is $p(L) = e^{-\\Sigma_a L}$.\n- The definition of variance $\\operatorname{Var}[\\cdot]$ and the Law of Total Variance: for a random variable $X$ and a sigma-field $\\mathcal{G}$, the identity $\\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X \\mid \\mathcal{G})] + \\operatorname{Var}(\\mathbb{E}[X \\mid \\mathcal{G}])$.\n\nDefine two estimators for the per-particle contribution to the $k$-eigenvalue estimate for a source neutron located at a random starting position uniformly distributed along the entry face:\n1. Analog estimator: Let $Y$ be the per-particle fission production under the analog MC scheme, where the neutron either reaches the fission-producing layer and yields $\\nu$ fission neutrons, or is absorbed in the slab and yields $0$. Conditional on the random path length $L$ from the starting position to the fission layer, $Y \\mid L$ is a Bernoulli random variable with success probability $p(L) = e^{-\\Sigma_a L}$ and score $\\nu$ on success, so $\\mathbb{E}[Y \\mid L] = \\nu p(L)$ and $\\operatorname{Var}(Y \\mid L) = \\nu^2 p(L)(1 - p(L))$.\n2. Implicit-capture estimator: Let $Z$ be the per-particle fission production under survival biasing, where the neutron history is sustained deterministically with attenuated weight equal to the survival probability to the far face, so $Z = \\nu p(L)$. This is the conditional expectation of $Y$ given $L$.\n\nTasks:\n- Using the Law of Total Variance, prove that $\\operatorname{Var}(Z) \\le \\operatorname{Var}(Y)$ with equality only when $p(L)$ is non-random, thereby demonstrating variance reduction due to implicit capture. Clearly articulate why $Z = \\mathbb{E}[Y \\mid L]$ corresponds to sustaining histories with attenuated weight rather than terminating them upon absorption.\n- Specialize to the case where the source positions are uniformly distributed so that $L$ is uniformly distributed on the interval $[0,d]$. Derive closed-form expressions for $\\mathbb{E}[p(L)]$ and $\\mathbb{E}[p(L)^2]$:\n  $$\\mathbb{E}[p(L)] = \\frac{1 - e^{-\\Sigma_a d}}{\\Sigma_a d}, \\quad \\mathbb{E}[p(L)^2] = \\frac{1 - e^{-2 \\Sigma_a d}}{2 \\Sigma_a d}.$$\n  Then derive:\n  $$\\operatorname{Var}(Z) = \\nu^2\\left(\\mathbb{E}[p(L)^2] - \\left(\\mathbb{E}[p(L)]\\right)^2\\right), \\quad \\operatorname{Var}(Y) = \\nu^2 \\mathbb{E}[p(L)]\\left(1 - \\mathbb{E}[p(L)]\\right),$$\n  and the variance reduction ratio\n  $$R = \\frac{\\operatorname{Var}(Y)}{\\operatorname{Var}(Z)} = \\frac{\\mathbb{E}[p(L)]\\left(1 - \\mathbb{E}[p(L)]\\right)}{\\mathbb{E}[p(L)^2] - \\left(\\mathbb{E}[p(L)]\\right)^2}.$$\n  Note that $R \\ge 1$ for all $\\Sigma_a > 0$ and $d > 0$, and quantify how $R$ behaves in the limits of highly absorbing and weakly absorbing slabs.\n- Explain why the above demonstrates that implicit capture reduces variance in the $k$-eigenvalue estimate by replacing the binary termination outcome with its conditional expectation, thus sustaining histories through weights rather than terminating them.\n\nYour program must compute the variance reduction ratio $R$ for the following test suite, and output the results as floats in a single line of output containing the results as a comma-separated list enclosed in square brackets:\n- Test case 1 (general case): $\\Sigma_a = 1.0$ $\\text{cm}^{-1}$, $d = 5.0$ $\\text{cm}$, $\\nu = 2.5$.\n- Test case 2 (highly absorbing region): $\\Sigma_a = 5.0$ $\\text{cm}^{-1}$, $d = 5.0$ $\\text{cm}$, $\\nu = 2.5$.\n- Test case 3 (weakly absorbing region): $\\Sigma_a = 0.1$ $\\text{cm}^{-1}$, $d = 5.0$ $\\text{cm}$, $\\nu = 2.5$.\n- Test case 4 (short slab, high absorption): $\\Sigma_a = 5.0$ $\\text{cm}^{-1}$, $d = 0.5$ $\\text{cm}$, $\\nu = 2.5$.\n\nAll outputs are dimensionless floats; no physical units should appear in the output. Your program should produce a single line of output containing a list in the exact format \"[result1,result2,result3,result4]\".",
            "solution": "### Part 1: Proof of Variance Reduction\n\nWe are given two estimators for the per-particle fission production: the analog estimator $Y$ and the implicit-capture estimator $Z$. The path length $L$ of a neutron to the fission-producing layer is a random variable.\n\nThe analog estimator, $Y$, is defined such that its value conditional on the path length $L$ is a Bernoulli random variable scaled by the fission yield $\\nu$. A neutron either survives the path of length $L$ with probability $p(L) = e^{-\\Sigma_a L}$ and produces $\\nu$ fission neutrons, or it is absorbed with probability $1 - p(L)$ and produces $0$. The conditional expectation and variance are given as:\n$$ \\mathbb{E}[Y \\mid L] = \\nu p(L) $$\n$$ \\operatorname{Var}(Y \\mid L) = \\nu^2 p(L)(1 - p(L)) $$\n\nThe implicit-capture estimator, $Z$, is defined as the score obtained when a history is always continued to the fission layer, but its statistical weight is attenuated by the survival probability. The resulting score is therefore deterministic for a given path length $L$:\n$$ Z = \\nu p(L) $$\n\nBy comparing these definitions, we immediately recognize that the implicit-capture estimator $Z$ is identical to the conditional expectation of the analog estimator $Y$ given the path length $L$:\n$$ Z = \\mathbb{E}[Y \\mid L] $$\nThis relationship is the foundation of the proof. This technique, where a random variable is replaced by its conditional expectation, is a general variance reduction method known as Conditional Monte Carlo.\n\nTo prove that $\\operatorname{Var}(Z) \\le \\operatorname{Var}(Y)$, we employ the Law of Total Variance on the estimator $Y$, conditioning on the random variable $L$. The law states:\n$$ \\operatorname{Var}(Y) = \\mathbb{E}[\\operatorname{Var}(Y \\mid L)] + \\operatorname{Var}(\\mathbb{E}[Y \\mid L]) $$\n\nSubstituting the definitions of $Z$ and $\\operatorname{Var}(Y \\mid L)$ into this equation, we get:\n$$ \\operatorname{Var}(Y) = \\mathbb{E}[\\nu^2 p(L)(1 - p(L))] + \\operatorname{Var}(Z) $$\n\nThe first term on the right-hand side, $\\mathbb{E}[\\operatorname{Var}(Y \\mid L)]$, represents the expected variance arising from the stochastic analog absorption/survival event. Let's analyze this term. The survival probability $p(L) = e^{-\\Sigma_a L}$ satisfies $0  p(L) \\le 1$ for any finite non-negative path length $L \\ge 0$ and $\\Sigma_a \\ge 0$ (assuming not both $L$ and $\\Sigma_a$ are infinite). Therefore, the quantity $p(L)(1 - p(L))$ is always non-negative. Since $\\nu^2$ is also non-negative, the conditional variance $\\operatorname{Var}(Y \\mid L) = \\nu^2 p(L)(1 - p(L))$ is a non-negative random variable. The expectation of a non-negative random variable must be non-negative:\n$$ \\mathbb{E}[\\operatorname{Var}(Y \\mid L)] \\ge 0 $$\n\nThus, we have shown that $\\operatorname{Var}(Y)$ is the sum of $\\operatorname{Var}(Z)$ and a non-negative term:\n$$ \\operatorname{Var}(Y) = \\operatorname{Var}(Z) + \\text{(a non-negative quantity)} $$\nThis directly implies the inequality:\n$$ \\operatorname{Var}(Y) \\ge \\operatorname{Var}(Z) $$\n\nEquality, $\\operatorname{Var}(Y) = \\operatorname{Var}(Z)$, holds if and only if the additional term is zero:\n$$ \\mathbb{E}[\\operatorname{Var}(Y \\mid L)] = 0 $$\nSince $\\operatorname{Var}(Y \\mid L)$ is a non-negative random variable, its expectation is zero if and only if the variable itself is zero with probability $1$.\n$$ \\operatorname{Var}(Y \\mid L) = \\nu^2 p(L)(1 - p(L)) = 0 \\quad (\\text{almost surely}) $$\nAssuming $\\nu \\neq 0$, this requires $p(L)(1-p(L)) = 0$, which means that $p(L)$ must take a value of either $0$ or $1$. If $p(L)$ is non-random, i.e., it is a constant $c$, then this condition is met only if $c=0$ or $c=1$. A non-random $p(L)$ implies that either the path length $L$ is constant or $\\Sigma_a=0$. If $p(L)$ is a constant other than $0$ or $1$, the equality does not hold. The problem statement's condition for equality, \"$p(L)$ is non-random,\" is thus slightly imprecise but captures the essential idea: the stochasticity of the absorption process must vanish, which happens if survival is either impossible or certain.\n\n### Part 2: Derivations for Uniform Path Length Distribution\n\nWe now specialize to the case where the path length $L$ is a random variable uniformly distributed on the interval $[0, d]$. The probability density function is $f_L(l) = 1/d$ for $l \\in [0, d]$ and $0$ otherwise.\n\n**Derivation of $\\mathbb{E}[p(L)]$ and $\\mathbb{E}[p(L)^2]$**\n\nThe expected value of the survival probability $p(L) = e^{-\\Sigma_a L}$ is:\n$$ \\mathbb{E}[p(L)] = \\int_{-\\infty}^{\\infty} p(l) f_L(l) \\, dl = \\int_{0}^{d} e^{-\\Sigma_a l} \\frac{1}{d} \\, dl $$\n$$ \\mathbb{E}[p(L)] = \\frac{1}{d} \\left[ -\\frac{1}{\\Sigma_a} e^{-\\Sigma_a l} \\right]_0^d = \\frac{1}{d} \\left( -\\frac{e^{-\\Sigma_a d}}{\\Sigma_a} - \\left(-\\frac{e^{0}}{\\Sigma_a}\\right) \\right) = \\frac{1 - e^{-\\Sigma_a d}}{\\Sigma_a d} $$\nThis confirms the first expression.\n\nThe expected value of $p(L)^2 = (e^{-\\Sigma_a L})^2 = e^{-2\\Sigma_a L}$ is found similarly, by replacing $\\Sigma_a$ with $2\\Sigma_a$ in the previous result:\n$$ \\mathbb{E}[p(L)^2] = \\int_{0}^{d} e^{-2\\Sigma_a l} \\frac{1}{d} \\, dl = \\frac{1 - e^{-2\\Sigma_a d}}{2\\Sigma_a d} $$\nThis confirms the second expression.\n\n**Derivation of $\\operatorname{Var}(Z)$ and $\\operatorname{Var}(Y)$**\n\nThe variance of the implicit-capture estimator $Z = \\nu p(L)$ is:\n$$ \\operatorname{Var}(Z) = \\operatorname{Var}(\\nu p(L)) = \\nu^2 \\operatorname{Var}(p(L)) $$\nUsing the standard formula $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$, we have:\n$$ \\operatorname{Var}(Z) = \\nu^2 \\left( \\mathbb{E}[p(L)^2] - (\\mathbb{E}[p(L)])^2 \\right) $$\nThis matches the provided expression.\n\nThe variance of the analog estimator $Y$ can be derived from its definition. First, we find $\\mathbb{E}[Y]$ and $\\mathbb{E}[Y^2]$.\nThe law of total expectation gives:\n$$ \\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y \\mid L]] = \\mathbb{E}[\\nu p(L)] = \\nu \\mathbb{E}[p(L)] $$\nFor the second moment, we use the law of total expectation on $Y^2$:\n$$ \\mathbb{E}[Y^2] = \\mathbb{E}[\\mathbb{E}[Y^2 \\mid L]] $$\nGiven $L$, $Y$ is $\\nu$ with probability $p(L)$ and $0$ otherwise. Thus $Y^2$ is $\\nu^2$ with probability $p(L)$ and $0$ otherwise.\n$$ \\mathbb{E}[Y^2 \\mid L] = \\nu^2 p(L) \\cdot 1 + 0 \\cdot (1-p(L)) = \\nu^2 p(L) $$\nTaking the expectation over $L$:\n$$ \\mathbb{E}[Y^2] = \\mathbb{E}[\\nu^2 p(L)] = \\nu^2 \\mathbb{E}[p(L)] $$\nThe variance of $Y$ is then:\n$$ \\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\nu^2 \\mathbb{E}[p(L)] - (\\nu \\mathbb{E}[p(L)])^2 $$\n$$ \\operatorname{Var}(Y) = \\nu^2 \\left( \\mathbb{E}[p(L)] - (\\mathbb{E}[p(L)])^2 \\right) = \\nu^2 \\mathbb{E}[p(L)](1 - \\mathbb{E}[p(L)]) $$\nThis confirms the provided expression for $\\operatorname{Var}(Y)$.\n\n**Derivation and Analysis of the Variance Reduction Ratio $R$**\n\nThe variance reduction ratio $R$ is the ratio of the two variances. The factor $\\nu^2$ cancels, demonstrating that the reduction ratio is independent of the fission yield $\\nu$.\n$$ R = \\frac{\\operatorname{Var}(Y)}{\\operatorname{Var}(Z)} = \\frac{\\nu^2 \\mathbb{E}[p(L)](1 - \\mathbb{E}[p(L)])}{\\nu^2 \\left(\\mathbb{E}[p(L)^2] - (\\mathbb{E}[p(L)])^2\\right)} = \\frac{\\mathbb{E}[p(L)](1 - \\mathbb{E}[p(L)])}{\\mathbb{E}[p(L)^2] - \\left(\\mathbb{E}[p(L)]\\right)^2} $$\nThis matches the problem statement. From Part 1, we know $\\operatorname{Var}(Y) \\ge \\operatorname{Var}(Z)$, so $R \\ge 1$.\n\nLet's analyze the behavior of $R$ in two limits by defining $x = \\Sigma_a d$.\n\n1.  **Weakly absorbing limit ($\\Sigma_a d \\to 0$ or $x \\to 0^+$):**\n    Using Taylor series expansions for small $x$:\n    $e^{-x} = 1 - x + \\frac{x^2}{2} - O(x^3)$\n    $\\mathbb{E}[p(L)] = \\frac{1 - (1 - x + x^2/2 - \\dots)}{x} = 1 - \\frac{x}{2} + O(x^2)$\n    $\\mathbb{E}[p(L)^2] = \\frac{1 - e^{-2x}}{2x} = 1 - x + O(x^2)$\n    The numerator of $R$ is:\n    $\\mathbb{E}[p(L)](1 - \\mathbb{E}[p(L)]) \\approx (1 - x/2)(1 - (1 - x/2)) = (1-x/2)(x/2) \\approx x/2$\n    The denominator of $R$, which is $\\operatorname{Var}(p(L))$, is:\n    $\\mathbb{E}[p(L)^2] - (\\mathbb{E}[p(L)])^2 \\approx (1 - x) - (1-x/2)^2 = (1-x) - (1-x+x^2/4) = x^2/12 - O(x^3) \\approx x^2/12$\n    The ratio becomes:\n    $R(x) \\approx \\frac{x/2}{x^2/12} = \\frac{6}{x} = \\frac{6}{\\Sigma_a d}$\n    As $x \\to 0^+$, $R \\to \\infty$. This implies that for very weakly absorbing systems where survival is highly likely, implicit capture yields a very large variance reduction.\n\n2.  **Highly absorbing limit ($\\Sigma_a d \\to \\infty$ or $x \\to \\infty$):**\n    In this limit, $e^{-x} \\to 0$ and $e^{-2x} \\to 0$.\n    $\\mathbb{E}[p(L)] = \\frac{1 - e^{-x}}{x} \\approx \\frac{1}{x}$\n    $\\mathbb{E}[p(L)^2] = \\frac{1 - e^{-2x}}{2x} \\approx \\frac{1}{2x}$\n    The numerator of $R$ is:\n    $\\mathbb{E}[p(L)](1 - \\mathbb{E}[p(L)]) \\approx \\frac{1}{x}(1 - \\frac{1}{x}) \\approx \\frac{1}{x}$\n    The denominator of $R$ is:\n    $\\mathbb{E}[p(L)^2] - (\\mathbb{E}[p(L)])^2 \\approx \\frac{1}{2x} - (\\frac{1}{x})^2 = \\frac{1}{2x} - \\frac{1}{x^2} \\approx \\frac{1}{2x}$\n    The ratio becomes:\n    $R(x) \\approx \\frac{1/x}{1/(2x)} = 2$\n    As $x \\to \\infty$, $R \\to 2$. In highly absorbing systems, where survival is very unlikely, implicit capture still provides a consistent variance reduction factor of approximately $2$.\n\n### Part 3: Mechanism of Variance Reduction\n\nThe derivations above demonstrate that implicit capture reduces the variance of the $k$-eigenvalue estimator. The fundamental reason lies in the elimination of a specific source of stochasticity from the simulation.\n\nIn an **analog** Monte Carlo simulation, a neutron's history involves a series of random choices. In this simplified problem, the key random choice is whether the neutron is absorbed within the slab or survives to reach the fission layer. This is a binary outcome modeled by a Bernoulli trial. The variance of the estimator $Y$ includes a component, $\\mathbb{E}[\\operatorname{Var}(Y \\mid L)]$, that arises directly from the random nature of this absorption event. A particle might be \"lucky\" and survive a long path, contributing a large score ($\\nu$), or \"unlucky\" and be absorbed early, contributing nothing. This high variability in outcomes for a given path length increases the overall statistical uncertainty of the final estimate.\n\n**Implicit capture** (or survival biasing) circumvents this binary, stochastic event. Instead of simulating the absorption process as a random choice, it deterministically accounts for its effect. The algorithm analytically computes the probability of survival, $p(L)$, and uses this probability as a statistical weight. The neutron history is never terminated by absorption; it is always followed to the fission layer. However, its contribution to the final tally is attenuated by its weight, which represents the fraction of an ensemble of such particles that would have survived. The final score for a particle, $Z = \\nu p(L)$, is precisely the conditional expectation of the analog score.\n\nBy replacing the random variable $Y \\mid L$ with its expectation $\\mathbb{E}[Y \\mid L]$, we have integrated out the variance associated with the absorption \"coin flip.\" The only remaining source of variance in the implicit capture estimator $Z$ is the natural variation due to the distribution of the initial random variable, the path length $L$. As proven by the Law of Total Variance and the Rao-Blackwell theorem, this procedure systematically reduces the variance of the estimator, leading to a more efficient simulation that requires fewer particle histories to achieve a desired level of precision.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the variance reduction ratio R for given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple is (sigma_a, d, nu). The value of nu is not used for R.\n    test_cases = [\n        (1.0, 5.0, 2.5),  # Test case 1 (general case)\n        (5.0, 5.0, 2.5),  # Test case 2 (highly absorbing region)\n        (0.1, 5.0, 2.5),  # Test case 3 (weakly absorbing region)\n        (5.0, 0.5, 2.5),  # Test case 4 (short slab, high absorption)\n    ]\n\n    results = []\n    \n    # A small number to prevent division by zero, though problem constraints ensure this won't happen.\n    epsilon = 1e-12\n\n    for case in test_cases:\n        sigma_a, d, _ = case\n        x = sigma_a * d\n\n        # The parameter x must be greater than 0 based on problem constraints.\n        if x = 0:\n            # In a perfectly transparent medium (x=0), p(L)=1 is non-random,\n            # so Var(Z) = 0, leading to a division by zero.\n            # R would be infinite. We adhere to problem constraints where x  0.\n            # Using np.inf for this theoretical case.\n            results.append(np.inf)\n            continue\n            \n        # Calculate E[p(L)]\n        # E[p(L)] = (1 - exp(-x)) / x\n        # We use np.expm1(-x) = exp(-x) - 1 for better numerical precision for small x.\n        # So, 1 - exp(-x) = -expm1(-x)\n        e_p = -np.expm1(-x) / x\n\n        # Calculate E[p(L)^2]\n        # E[p(L)^2] = (1 - exp(-2x)) / (2x)\n        e_p2 = -np.expm1(-2 * x) / (2 * x)\n        \n        # Numerator of R: Var(Y) / nu^2 = E[p(L)] * (1 - E[p(L)])\n        # This is the variance of a Bernoulli trial with success probability E[p(L)]\n        numerator_r = e_p * (1.0 - e_p)\n        \n        # Denominator of R: Var(Z) / nu^2 = Var(p(L)) = E[p(L)^2] - (E[p(L)])^2\n        denominator_r = e_p2 - e_p**2\n        \n        # The denominator, being a variance, is non-negative. It is zero only\n        # if p(L) is non-random (i.e., x=0), a case handled above.\n        if denominator_r  epsilon:\n            # This case corresponds to x being very close to 0, R - infinity.\n            variance_ratio_r = np.inf\n        else:\n            variance_ratio_r = numerator_r / denominator_r\n            \n        results.append(variance_ratio_r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Implicit capture solves one problem—termination variance—but creates another: the proliferation of computationally expensive, low-weight particles. This exercise introduces Russian roulette, the standard technique used to manage this particle population. Your task is to analyze the statistical 'cost' of this solution by deriving the variance introduced by the Russian roulette game, revealing the essential trade-off between population control and statistical noise in practical simulations .",
            "id": "4231626",
            "problem": "In Monte Carlo (MC) neutron transport for nuclear reactor simulation, implicit capture is often used as a variance reduction (VR) technique: each collision is treated as scatter, and the particle weight is multiplied by the scattering survival probability (the ratio of scattering macroscopic cross section to total macroscopic cross section), so that the weight decreases deterministically without explicit absorption. To prevent the proliferation of very low-weight particles after repeated implicit capture, Russian roulette survival biasing is applied. In one widely used scheme, a particle with incoming weight $w$ is subjected to a survival trial with survival probability $p$, and its post-roulette weight $w'$ is defined as $w' = 0$ with probability $1 - p$ and $w' = \\frac{w}{p}$ with probability $p$. This scheme is designed so that the expected weight is preserved under survival biasing in order to maintain unbiased tallies.\n\nStarting from the definitions of expectation $\\mathbb{E}[X]$ and variance $\\mathrm{Var}(X) = \\mathbb{E}[X^{2}] - \\left(\\mathbb{E}[X]\\right)^{2}$, derive a closed-form expression for $\\mathrm{Var}(w')$ as a function of the survival probability $p$ and incoming weight $w$. Then, based on first principles for linear tallies that are proportional to particle weight, explain how the dependence of $\\mathrm{Var}(w')$ on $p$ qualitatively affects tally variance when Russian roulette is used in conjunction with implicit capture.\n\nExpress the final expression for $\\mathrm{Var}(w')$ in a fully simplified, exact symbolic form. No numerical rounding is required, and no physical units are involved.",
            "solution": "The task is to derive a closed-form expression for the variance of the post-roulette particle weight, $\\mathrm{Var}(w')$, and to explain its implications for tally variance. Let $w$ be the incoming particle weight and $p$ be the survival probability in the Russian roulette game. The post-roulette weight, $w'$, is a discrete random variable. Its value is $\\frac{w}{p}$ with probability $p$, and $0$ with probability $1-p$.\n\nThe variance of a random variable $X$ is defined as $\\mathrm{Var}(X) = \\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}$. We will first compute the expectation (first moment), $\\mathbb{E}[w']$, and the second moment, $\\mathbb{E}[(w')^{2}]$.\n\nThe expectation of $w'$ is the sum of its possible values weighted by their respective probabilities:\n$$\n\\mathbb{E}[w'] = \\left(\\frac{w}{p}\\right) \\cdot p + (0) \\cdot (1-p) = w + 0 = w\n$$\nThis confirms that the Russian roulette scheme is unbiased, as the expected weight of the particle is conserved.\n\nThe second moment of $w'$ is the expectation of its square:\n$$\n\\mathbb{E}[(w')^{2}] = \\left(\\frac{w}{p}\\right)^{2} \\cdot p + (0)^{2} \\cdot (1-p) = \\frac{w^{2}}{p^{2}} \\cdot p = \\frac{w^{2}}{p}\n$$\nNow, we can substitute these moments into the formula for variance:\n$$\n\\mathrm{Var}(w') = \\mathbb{E}[(w')^{2}] - (\\mathbb{E}[w'])^{2}\n$$\n$$\n\\mathrm{Var}(w') = \\frac{w^{2}}{p} - w^{2}\n$$\nFactoring out the $w^{2}$ term, we obtain the simplified closed-form expression:\n$$\n\\mathrm{Var}(w') = w^{2} \\left(\\frac{1}{p} - 1\\right) = w^{2} \\left(\\frac{1-p}{p}\\right)\n$$\nThis is the derived expression for the variance of the post-roulette weight.\n\nNext, we analyze the effect of this variance on simulation tallies. In Monte Carlo methods, tallies are estimators for physical quantities, and for linear tallies, the score contributed by a particle is proportional to its weight. The total variance of a tally is the aggregate of variances from all stochastic events in the particle histories. The Russian roulette game is one such stochastic event, and it introduces an amount of variance given by $\\mathrm{Var}(w')$.\n\nThe derived expression $\\mathrm{Var}(w') = w^{2} \\frac{1-p}{p}$ shows that the variance is a function of the survival probability $p$. Given that $p \\in (0, 1]$, an analysis of this function reveals its behavior. The term $\\frac{1-p}{p}$ is a monotonically decreasing function of $p$.\n- As the survival probability $p$ approaches $1$, the variance $\\mathrm{Var}(w')$ approaches $0$. A value of $p=1$ corresponds to a deterministic game where the particle always survives with its weight unchanged ($w' = w/1 = w$), hence there is no added variance.\n- As the survival probability $p$ approaches $0$, the variance $\\mathrm{Var}(w')$ approaches infinity. A very small $p$ represents a game with a high probability of termination (outcome $0$) and a very low probability of survival, but if the particle survives, its weight becomes extremely large ($\\frac{w}{p}$). This large spread in possible outcomes results in a massive variance.\n\nThis relationship is critical for the efficiency of the simulation. Russian roulette is employed to manage computational resources by terminating particles with low importance (low weight). However, the choice of the survival probability $p$ involves a trade-off. Using a very small $p$ aggressively culls particles, which can reduce computation time per particle history. But, as the derivation shows, a small $p$ introduces a large variance. This increase in variance degrades the statistical quality of the tallies, meaning more particle histories are required to achieve a desired level of precision. The overall efficiency of a Monte Carlo simulation is often measured by a figure of merit (FOM), which is inversely proportional to the product of variance and computation time. An excessively large variance introduced by an aggressive Russian roulette strategy can decrease the FOM, making the simulation less efficient. Therefore, the parameter $p$ must be chosen carefully to balance the need for population control against the introduction of statistical noise.",
            "answer": "$$\n\\boxed{w^{2} \\frac{1-p}{p}}\n$$"
        }
    ]
}