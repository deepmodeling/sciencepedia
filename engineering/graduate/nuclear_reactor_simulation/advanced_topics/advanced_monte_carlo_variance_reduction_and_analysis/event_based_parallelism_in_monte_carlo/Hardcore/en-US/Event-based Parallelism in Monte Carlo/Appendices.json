{
    "hands_on_practices": [
        {
            "introduction": "In event-based parallelism, particularly on accelerators like GPUs, computational tasks are dispatched in large batches. A crucial first step in performance tuning is to understand the trade-off between useful work and system overhead. This exercise explores this fundamental relationship by building a simple but powerful performance model to quantify the efficiency of a kernel launch. By working through this problem , you will learn to determine the minimum batch size required to amortize the fixed cost of a kernel launch, ensuring the hardware spends most of its time performing meaningful computations.",
            "id": "4224488",
            "problem": "In an event-based Monte Carlo neutron transport kernel used for nuclear reactor simulation on a graphics processing unit (GPU), a batch of $N$ events is processed per kernel launch. Each launch incurs a fixed kernel-launch overhead time $\\tau_k$ and each event in the batch requires a constant compute time $c$. Define the efficiency $\\eta(N)$ of a batch as the ratio of useful per-event compute time to the total wall-clock time for the batch. Using only the foundational definitions that (i) overhead adds a constant time $\\tau_k$ independent of $N$, (ii) the per-event compute time accumulates linearly as $N c$, and (iii) efficiency is the ratio of useful compute time to the total time, derive the minimal integer batch size $N_{\\min}$ that guarantees a target efficiency $\\eta$ with $0\\eta1$.\n\nThen, for a specific reactor-physics throughput target, take $\\tau_k = 8.0\\,\\mu\\mathrm{s}$, $c = 25\\,\\mathrm{ns}$, and $\\eta = 0.9$. Compute the minimal integer batch size $N_{\\min}$ that meets the target efficiency, and validate your result by evaluating the efficiency at $N_{\\min}$ using the simple timing model implied by the definitions. Express the final answer as an integer with no units.",
            "solution": "The problem is first validated against the provided criteria.\n\n**Step 1: Extract Givens**\n- Batch size: $N$ events\n- Fixed kernel-launch overhead time: $\\tau_k$\n- Constant compute time per event: $c$\n- Efficiency of a batch: $\\eta(N)$, defined as the ratio of useful per-event compute time to the total wall-clock time for the batch.\n- Foundational definition (i): Overhead adds a constant time $\\tau_k$ independent of $N$.\n- Foundational definition (ii): Per-event compute time accumulates linearly as $N c$.\n- Foundational definition (iii): Efficiency is the ratio of useful compute time to the total time.\n- Target efficiency: $\\eta$, where $0  \\eta  1$.\n- Goal 1: Derive the minimal integer batch size $N_{\\min}$ that guarantees an efficiency of at least $\\eta$.\n- Goal 2: Compute $N_{\\min}$ for the specific values $\\tau_k = 8.0\\,\\mu\\mathrm{s}$, $c = 25\\,\\mathrm{ns}$, and $\\eta = 0.9$.\n- Goal 3: Validate the result by computing the efficiency at $N_{\\min}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard and simplified performance model used in high-performance computing, particularly for throughput-oriented architectures like GPUs. The concepts of launch overhead and per-element processing time are fundamental to performance analysis. The given definitions are sufficient and consistent for constructing a mathematical model. The problem asks for a minimal integer solution to an inequality, which is a well-defined mathematical task. The values provided for $\\tau_k$, $c$, and $\\eta$ are physically realistic for modern hardware and simulation contexts. The problem is not ill-posed, ambiguous, or trivial. It represents a fundamental problem in performance optimization.\n\n**Verdict:** The problem is valid.\n\n**Step 3: Solution Derivation**\n\nFollowing the provided definitions, we first formalize the expression for efficiency, $\\eta(N)$. The \"useful per-event compute time\" for a batch of $N$ events is the total time spent on computation, which is the product of the number of events and the time per event.\nLet $T_{\\text{compute}}$ be the total useful compute time. Based on definition (ii):\n$$T_{\\text{compute}}(N) = N c$$\n\nThe \"total wall-clock time for the batch\", let's call it $T_{\\text{total}}(N)$, is the sum of the useful compute time and the fixed kernel-launch overhead. Based on definitions (i) and (ii):\n$$T_{\\text{total}}(N) = T_{\\text{compute}}(N) + \\tau_k = N c + \\tau_k$$\n\nThe efficiency $\\eta(N)$ is a function of the batch size $N$. According to definition (iii), it is the ratio of useful compute time to total wall-clock time:\n$$\\eta(N) = \\frac{T_{\\text{compute}}(N)}{T_{\\text{total}}(N)} = \\frac{N c}{N c + \\tau_k}$$\n\nThe problem requires finding the minimal integer batch size, $N_{\\min}$, that guarantees a target efficiency $\\eta$. This translates to the inequality:\n$$\\eta(N) \\ge \\eta$$\nSubstituting the expression for $\\eta(N)$, we have:\n$$\\frac{N c}{N c + \\tau_k} \\ge \\eta$$\nSince $N$ is a batch size, $N \\ge 1$. The constants $c$ and $\\tau_k$ are times, so they are positive. Therefore, the denominator $N c + \\tau_k$ is strictly positive. We can multiply both sides of the inequality by this term without changing the direction of the inequality:\n$$N c \\ge \\eta (N c + \\tau_k)$$\n$$N c \\ge \\eta N c + \\eta \\tau_k$$\nTo solve for $N$, we gather all terms containing $N$ on one side:\n$$N c - \\eta N c \\ge \\eta \\tau_k$$\n$$N(c - \\eta c) \\ge \\eta \\tau_k$$\n$$N c (1 - \\eta) \\ge \\eta \\tau_k$$\nThe problem states that $0  \\eta  1$, which implies that $1 - \\eta$ is a positive quantity. We can therefore divide the inequality by $c(1 - \\eta)$ without changing its direction:\n$$N \\ge \\frac{\\eta \\tau_k}{c (1 - \\eta)}$$\nThis inequality specifies the condition that the batch size $N$ must satisfy. The problem asks for the *minimal integer* batch size, $N_{\\min}$. This is the smallest integer value of $N$ that satisfies the condition. Such an integer is found by taking the ceiling of the right-hand side expression:\n$$N_{\\min} = \\left\\lceil \\frac{\\eta \\tau_k}{c(1-\\eta)} \\right\\rceil$$\n\nThis completes the symbolic derivation. Now, we compute the numerical value for $N_{\\min}$ using the provided data: $\\tau_k = 8.0\\,\\mu\\mathrm{s}$, $c = 25\\,\\mathrm{ns}$, and $\\eta = 0.9$.\nFirst, we ensure consistent units. Let's convert both time values to seconds:\n$\\tau_k = 8.0 \\times 10^{-6}\\,\\mathrm{s}$\n$c = 25 \\times 10^{-9}\\,\\mathrm{s}$\n\nWe can compute the dimensionless ratio $\\frac{\\tau_k}{c}$:\n$$\\frac{\\tau_k}{c} = \\frac{8.0 \\times 10^{-6}}{25 \\times 10^{-9}} = \\frac{8.0}{25} \\times 10^3 = 0.32 \\times 10^3 = 320$$\nNow, substitute the values of $\\eta$ and the ratio $\\frac{\\tau_k}{c}$ into the inequality for $N$:\n$$N \\ge \\frac{0.9}{1 - 0.9} \\left( \\frac{\\tau_k}{c} \\right)$$\n$$N \\ge \\frac{0.9}{0.1} \\times 320$$\n$$N \\ge 9 \\times 320$$\n$$N \\ge 2880$$\nSince the lower bound is an integer, the minimal integer $N$ that satisfies the inequality is $N_{\\min} = 2880$.\n$$N_{\\min} = 2880$$\n\nFinally, we validate this result by calculating the efficiency achieved with a batch size of $N_{\\min} = 2880$.\n$$\\eta(N_{\\min}) = \\eta(2880) = \\frac{2880 \\cdot c}{2880 \\cdot c + \\tau_k}$$\nTo simplify, we can divide the numerator and the denominator by $c$:\n$$\\eta(2880) = \\frac{2880}{2880 + \\frac{\\tau_k}{c}}$$\nUsing our previously calculated ratio $\\frac{\\tau_k}{c} = 320$:\n$$\\eta(2880) = \\frac{2880}{2880 + 320} = \\frac{2880}{3200}$$\nSimplifying the fraction:\n$$\\eta(2880) = \\frac{288}{320} = \\frac{144}{160} = \\frac{72}{80} = \\frac{36}{40} = \\frac{9}{10} = 0.9$$\nThe efficiency at $N_{\\min} = 2880$ is exactly $0.9$, which matches the target efficiency $\\eta$. This validates our calculation of $N_{\\min}$.",
            "answer": "$$\\boxed{2880}$$"
        },
        {
            "introduction": "After ensuring efficient kernel launches, the next performance frontier is optimizing the computation within the kernel itself. On Single Instruction, Multiple Thread (SIMT) architectures common in GPUs, performance degrades when threads in a group (a \"warp\") execute different instructions—a phenomenon known as warp divergence. This practice  presents a powerful technique to combat this: sorting particles into buckets based on their computational path. You will analyze the trade-off between the upfront cost of sorting and the significant performance gains from perfectly coherent execution, providing a concrete example of how to co-design algorithms with the underlying hardware architecture in mind.",
            "id": "4224563",
            "problem": "Consider an event-based Monte Carlo (MC) simulation of neutron transport in a nuclear reactor core where scattering events are processed on a Graphics Processing Unit (GPU) with Single Instruction Multiple Threads (SIMT) execution. The angular scattering of neutrons is described by a normalized anisotropic kernel over the cosine of the scattering angle, denoted by $\\mu \\in [-1,1]$, with a Legendre expansion up to second order: \n$$\np(\\mu) = \\frac{1}{2}\\left(1 + a_1 \\mu + \\frac{a_2}{2}\\left(3\\mu^2 - 1\\right)\\right),\n$$\nwhere $a_1$ and $a_2$ are anisotropy coefficients. The normalization constant is chosen so that $\\int_{-1}^{1} p(\\mu)\\,d\\mu = 1$. The scattering kernel leads to angle-dependent code paths in the scattering routine: forward scattering, central scattering, and backscattering. These paths are triggered by the angle thresholds defined by a single parameter $\\mu_t \\in (0,1)$, such that:\n- Forward path is taken if $\\mu \\in [\\mu_t, 1]$,\n- Central path is taken if $\\mu \\in (-\\mu_t, \\mu_t)$,\n- Backscattering path is taken if $\\mu \\in [-1, -\\mu_t]$.\n\nAssume each path has a fixed path execution cost, modeled in unitless time units, given by $c_f$ for forward, $c_m$ for central, and $c_b$ for backscattering. The GPU warp size is $W$ threads. In SIMT execution, warp-level divergence occurs when threads in a warp take different branches; the warp executes all present branches serially, paying the cost of each distinct branch present in the warp once, independently of the number of threads that took that branch.\n\nYou are to design a bucketization (remapping) strategy that assigns particles to contiguous buckets by their angle-dependent path key (forward, central, backscattering) in order to minimize intra-warp divergence. The remapping requires sorting particles by an integer bucket key (three buckets), with an estimated overhead modeled by a counting-sort-like cost \n$$\nT_{\\text{sort}} = \\gamma N + \\delta B,\n$$\nwhere $N$ is the number of particles, $B$ is the number of buckets (here $B=3$), and $\\gamma,\\delta$ are positive constants. After sorting, each bucket is processed with full warps of size $W$; a partially filled warp at the end of a bucket still incurs the full path cost for that bucket.\n\nStarting only from the fundamental base comprising the Boltzmann transport framework (scattering kernel and normalization), SIMT divergence behavior, and integer-key sorting cost model as specified above, derive the following:\n- The probabilities $p_b$, $p_m$, $p_f$ of landing in the backscattering, central, and forward buckets, respectively, by integrating the provided $p(\\mu)$ over the corresponding angle intervals.\n- The expected total execution time without any remapping, expressed in unitless time units, for $N$ particles processed in warps of size $W$, where each warp’s time is the sum of the costs of the distinct paths present in that warp.\n- The total execution time with bucketization into three buckets, including both the sorting overhead and the bucketed execution time, where the number of warps per bucket is the ceiling of the expected bucket occupancy divided by $W$.\n\nYour program must compute, for each test case specified below, two outputs: the total time without remapping and the total time with bucketization into three angle buckets. Express all final results as unitless floating-point numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order: for each test case, output first the no-remapping total time, then the bucketized total time; thus, the final list length is twice the number of test cases.\n\nTest Suite:\n- Case A (happy path): $N=65536$, $W=32$, $a_1=0.2$, $a_2=0.1$, $\\mu_t=0.6$, $c_f=6.0$, $c_m=4.0$, $c_b=7.0$, $\\gamma=0.25$, $\\delta=400.0$.\n- Case B (forward-peaked anisotropy): $N=65536$, $W=32$, $a_1=0.6$, $a_2=0.2$, $\\mu_t=0.8$, $c_f=8.0$, $c_m=5.0$, $c_b=9.0$, $\\gamma=0.2$, $\\delta=400.0$.\n- Case C (boundary, no divergence due to $W=1$): $N=1000$, $W=1$, $a_1=0.0$, $a_2=0.0$, $\\mu_t=0.5$, $c_f=6.0$, $c_m=6.0$, $c_b=6.0$, $\\gamma=0.3$, $\\delta=200.0$.\n- Case D (larger warp size): $N=262144$, $W=64$, $a_1=0.3$, $a_2=-0.1$, $\\mu_t=0.7$, $c_f=7.0$, $c_m=4.0$, $c_b=8.0$, $\\gamma=0.25$, $\\delta=600.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots]$), where the list entries are floating-point numbers corresponding to, for each test case in order A, B, C, D: $(\\text{no-remap total}),(\\text{bucketized total})$. All computations must be performed analytically from the definitions given, without any stochastic sampling. All outputs must be unitless as specified.",
            "solution": "The user-provided problem is valid as it is scientifically grounded in nuclear reactor physics and parallel computing principles, is mathematically well-posed, and is formulated with objective and precise language. All necessary parameters are provided, and the required derivations are achievable through analytical means.\n\nThis solution presents the analytical derivations for the total execution time of a Monte Carlo neutron scattering simulation on a GPU, both with and without a particle bucketization strategy.\n\n### 1. Derivation of Path Probabilities\n\nThe probability of a neutron scattering into a specific angular range is determined by integrating the normalized anisotropic scattering kernel, $p(\\mu)$, over the corresponding interval of $\\mu$, the cosine of the scattering angle.\n\nThe scattering kernel is given as a Legendre expansion up to the second order:\n$$\np(\\mu) = \\frac{1}{2}\\left(1 + a_1 \\mu + \\frac{a_2}{2}\\left(3\\mu^2 - 1\\right)\\right)\n$$\nThis probability density function is normalized such that $\\int_{-1}^{1} p(\\mu)\\,d\\mu = 1$. The three angular paths are defined by the parameter $\\mu_t \\in (0,1)$:\n- Forward path (f): $\\mu \\in [\\mu_t, 1]$\n- Central path (m): $\\mu \\in (-\\mu_t, \\mu_t)$\n- Backscattering path (b): $\\mu \\in [-1, -\\mu_t]$\n\nTo find the probability of each path, we compute the definite integral of $p(\\mu)$ over each interval. First, we find the indefinite integral of $p(\\mu)$, which we denote as $P(\\mu)$:\n$$\nP(\\mu) = \\int p(\\mu)\\,d\\mu = \\frac{1}{2} \\int \\left( \\left(1-\\frac{a_2}{2}\\right) + a_1 \\mu + \\frac{3a_2}{2}\\mu^2 \\right) d\\mu\n$$\n$$\nP(\\mu) = \\frac{1}{2} \\left( \\left(1-\\frac{a_2}{2}\\right)\\mu + \\frac{a_1}{2}\\mu^2 + \\frac{a_2}{2}\\mu^3 \\right) + C\n$$\n\nThe probabilities $p_f$, $p_m$, and $p_b$ are:\n\n**Central path probability ($p_m$):**\n$$\np_m = \\int_{-\\mu_t}^{\\mu_t} p(\\mu)\\,d\\mu = P(\\mu_t) - P(-\\mu_t)\n$$\n$$\np_m = \\frac{1}{2} \\left[ \\left( \\left(1-\\frac{a_2}{2}\\right)\\mu_t + \\frac{a_1}{2}\\mu_t^2 + \\frac{a_2}{2}\\mu_t^3 \\right) - \\left( -\\left(1-\\frac{a_2}{2}\\right)\\mu_t + \\frac{a_1}{2}\\mu_t^2 - \\frac{a_2}{2}\\mu_t^3 \\right) \\right]\n$$\n$$\np_m = \\frac{1}{2} \\left[ 2\\left(1-\\frac{a_2}{2}\\right)\\mu_t + 2\\frac{a_2}{2}\\mu_t^3 \\right] = \\left(1-\\frac{a_2}{2}\\right)\\mu_t + \\frac{a_2}{2}\\mu_t^3 = \\mu_t + \\frac{a_2}{2}\\left(\\mu_t^3 - \\mu_t\\right)\n$$\n\n**Forward path probability ($p_f$):**\n$$\np_f = \\int_{\\mu_t}^{1} p(\\mu)\\,d\\mu = P(1) - P(\\mu_t)\n$$\n$$\nP(1) = \\frac{1}{2} \\left( \\left(1-\\frac{a_2}{2}\\right) + \\frac{a_1}{2} + \\frac{a_2}{2} \\right) = \\frac{1}{2}\\left(1 + \\frac{a_1}{2}\\right)\n$$\n$$\np_f = \\frac{1}{2}\\left(1 + \\frac{a_1}{2}\\right) - \\frac{1}{2} \\left( \\left(1-\\frac{a_2}{2}\\right)\\mu_t + \\frac{a_1}{2}\\mu_t^2 + \\frac{a_2}{2}\\mu_t^3 \\right)\n$$\n$$\np_f = \\frac{1}{2} \\left[ 1 + \\frac{a_1}{2} - \\left(1-\\frac{a_2}{2}\\right)\\mu_t - \\frac{a_1}{2}\\mu_t^2 - \\frac{a_2}{2}\\mu_t^3 \\right]\n$$\n\n**Backscattering path probability ($p_b$):**\n$$\np_b = \\int_{-1}^{-\\mu_t} p(\\mu)\\,d\\mu = P(-\\mu_t) - P(-1)\n$$\n$$\nP(-1) = \\frac{1}{2} \\left( -\\left(1-\\frac{a_2}{2}\\right) + \\frac{a_1}{2} - \\frac{a_2}{2} \\right) = \\frac{1}{2}\\left(-1 + \\frac{a_1}{2}\\right)\n$$\n$$\np_b = \\frac{1}{2} \\left( -\\left(1-\\frac{a_2}{2}\\right)\\mu_t + \\frac{a_1}{2}\\mu_t^2 - \\frac{a_2}{2}\\mu_t^3 \\right) - \\frac{1}{2}\\left(-1 + \\frac{a_1}{2}\\right)\n$$\n$$\np_b = \\frac{1}{2} \\left[ 1 - \\frac{a_1}{2} - \\left(1-\\frac{a_2}{2}\\right)\\mu_t + \\frac{a_1}{2}\\mu_t^2 - \\frac{a_2}{2}\\mu_t^3 \\right]\n$$\nThese three probabilities sum to $1$.\n\n### 2. Execution Time Without Remapping ($T_{\\text{no-remap}}$)\n\nWithout remapping, threads corresponding to $N$ particles are grouped into warps of size $W$. On a SIMT architecture, if threads within a warp diverge (take different code paths), the warp executes each distinct path serially. The cost for one warp is the sum of the costs of all unique paths taken by its threads.\n\nLet's find the expected execution time for a single warp of size $w$. The probability that a specific path (e.g., forward path, $f$) is present in the warp is $1$ minus the probability that *no* thread in the warp takes that path. The probability that a single thread does *not* take path $f$ is $1 - p_f$. For $w$ independent threads, this probability is $(1 - p_f)^w$.\n- Probability path $f$ is present: $P(\\text{f present}) = 1 - (1 - p_f)^w$\n- Probability path $m$ is present: $P(\\text{m present}) = 1 - (1 - p_m)^w$\n- Probability path $b$ is present: $P(\\text{b present}) = 1 - (1 - p_b)^w$\n\nThe expected time for a warp of size $w$, $E[T_{\\text{warp}}(w)]$, is the sum of each path's cost multiplied by the probability of its presence:\n$$\nE[T_{\\text{warp}}(w)] = c_f \\left(1 - (1-p_f)^w\\right) + c_m \\left(1 - (1-p_m)^w\\right) + c_b \\left(1 - (1-p_b)^w\\right)\n$$\nFor $N$ total particles, there are $N_{full} = \\lfloor N/W \\rfloor$ full warps of size $W$ and, if $N$ is not a multiple of $W$, one partial warp of size $W_{\\text{last}} = N \\pmod W$.\n\nThe total expected time is the sum of expected times for all warps:\n$$\nT_{\\text{no-remap}} = \\lfloor N/W \\rfloor \\cdot E[T_{\\text{warp}}(W)] + E[T_{\\text{warp}}(N \\pmod W)]\n$$\nwhere the second term is added only if $N \\pmod W > 0$.\n\n### 3. Execution Time With Bucketization ($T_{\\text{remap}}$)\n\nThe bucketization strategy involves two stages: sorting and execution. The total time is the sum of the time for each stage.\n$$\nT_{\\text{remap}} = T_{\\text{sort}} + T_{\\text{exec,bucketed}}\n$$\n\n**Sorting Cost ($T_{\\text{sort}}$):**\nThe problem provides a model for the sorting overhead:\n$$\nT_{\\text{sort}} = \\gamma N + \\delta B\n$$\nWith $B=3$ buckets (forward, central, backscattering), the cost is:\n$$\nT_{\\text{sort}} = \\gamma N + 3\\delta\n$$\n\n**Bucketed Execution Cost ($T_{\\text{exec,bucketed}}$):**\nAfter sorting, particles are grouped by their destined path. All threads in a warp processing a specific bucket will execute the same code path, thus eliminating intra-warp divergence.\nThe expected number of particles in each bucket is $N_f = N p_f$, $N_m = N p_m$, and $N_b = N p_b$.\nThe problem specifies that the number of warps for each bucket is calculated as the ceiling of the expected bucket occupancy divided by the warp size $W$:\n- Warps for forward bucket: $N_{\\text{warps},f} = \\lceil N p_f / W \\rceil$\n- Warps for central bucket: $N_{\\text{warps},m} = \\lceil N p_m / W \\rceil$\n- Warps for backscattering bucket: $N_{\\text{warps},b} = \\lceil N p_b / W \\rceil$\n\nEach warp in the forward bucket costs $c_f$, in the central bucket $c_m$, and in the backscattering bucket $c_b$. The total bucketed execution time is:\n$$\nT_{\\text{exec,bucketed}} = N_{\\text{warps},f} \\cdot c_f + N_{\\text{warps},m} \\cdot c_m + N_{\\text{warps},b} \\cdot c_b\n$$\n$$\nT_{\\text{exec,bucketed}} = \\lceil Np_f/W \\rceil c_f + \\lceil Np_m/W \\rceil c_m + \\lceil Np_b/W \\rceil c_b\n$$\nThe total time with remapping is the sum of these two components.\n\n### Summary of Formulas\nThe computation proceeds by first calculating $p_f, p_m, p_b$ for a given test case. Then, these probabilities are used to compute $T_{\\text{no-remap}}$ and $T_{\\text{remap}}$ using the derived formulas.\n- $T_{\\text{no-remap}} = \\lfloor N/W \\rfloor \\left( \\sum_{k \\in \\{f,m,b\\}} c_k(1-(1-p_k)^W) \\right) + \\mathbf{1}_{N \\pmod W \\neq 0} \\left( \\sum_{k \\in \\{f,m,b\\}} c_k(1-(1-p_k)^{N \\pmod W}) \\right)$\n- $T_{\\text{remap}} = (\\gamma N + 3\\delta) + c_f \\lceil Np_f/W \\rceil + c_m \\lceil Np_m/W \\rceil + c_b \\lceil Np_b/W \\rceil$\n\nThese formulas will be implemented to solve for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It calculates the total execution time for a Monte Carlo simulation \n    both without and with a particle bucketization strategy.\n    \"\"\"\n    \n    # Test cases are defined as tuples of parameters:\n    # (N, W, a1, a2, mu_t, c_f, c_m, c_b, gamma, delta)\n    test_cases = [\n        # Case A (happy path)\n        (65536, 32, 0.2, 0.1, 0.6, 6.0, 4.0, 7.0, 0.25, 400.0),\n        # Case B (forward-peaked anisotropy)\n        (65536, 32, 0.6, 0.2, 0.8, 8.0, 5.0, 9.0, 0.2, 400.0),\n        # Case C (boundary, no divergence due to W=1)\n        (1000, 1, 0.0, 0.0, 0.5, 6.0, 6.0, 6.0, 0.3, 200.0),\n        # Case D (larger warp size)\n        (262144, 64, 0.3, -0.1, 0.7, 7.0, 4.0, 8.0, 0.25, 600.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        time_no_remap, time_remap = calculate_times_for_case(*case)\n        results.append(time_no_remap)\n        results.append(time_remap)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_times_for_case(N, W, a1, a2, mu_t, c_f, c_m, c_b, gamma, delta):\n    \"\"\"\n    Calculates execution times for a single test case.\n    \n    Args:\n        N (int): Number of particles.\n        W (int): Warp size.\n        a1, a2 (float): Anisotropy coefficients.\n        mu_t (float): Angle threshold.\n        c_f, c_m, c_b (float): Path execution costs.\n        gamma, delta (float): Sorting cost coefficients.\n\n    Returns:\n        tuple: (total time without remapping, total time with bucketization).\n    \"\"\"\n\n    # 1. Calculate Path Probabilities (p_f, p_m, p_b)\n    # p_m = mu_t + a2/2 * (mu_t^3 - mu_t)\n    p_m = mu_t + 0.5 * a2 * (mu_t**3 - mu_t)\n    \n    # p_f = 0.5 * [1 + a1/2 - (1-a2/2)*mu_t - a1/2*mu_t^2 - a2/2*mu_t^3]\n    p_f = 0.5 * (1 + 0.5 * a1 - (1 - 0.5 * a2) * mu_t - 0.5 * a1 * mu_t**2 - 0.5 * a2 * mu_t**3)\n\n    # p_b can be derived from p_f and p_m, or calculated directly.\n    # To maintain precision, we use the direct formula, but 1 - p_f - p_m is also valid.\n    # p_b = 0.5 * [1 - a1/2 - (1-a2/2)*mu_t + a1/2*mu_t^2 - a2/2*mu_t^3]\n    p_b = 1.0 - p_f - p_m\n\n    # 2. Calculate Execution Time Without Remapping (T_no_remap)\n    \n    # Function to calculate expected time for a warp of a given size `w`\n    def get_expected_warp_time(w, p_f, p_m, p_b, c_f, c_m, c_b):\n        if w == 0:\n            return 0.0\n        time = 0.0\n        # Contribution from forward path\n        prob_f_present = 1.0 - (1.0 - p_f)**w\n        time += c_f * prob_f_present\n        # Contribution from central path\n        prob_m_present = 1.0 - (1.0 - p_m)**w\n        time += c_m * prob_m_present\n        # Contribution from backscattering path\n        prob_b_present = 1.0 - (1.0 - p_b)**w\n        time += c_b * prob_b_present\n        return time\n\n    num_full_warps = N // W\n    last_warp_size = N % W\n\n    expected_time_full_warp = get_expected_warp_time(W, p_f, p_m, p_b, c_f, c_m, c_b)\n    total_time_no_remap = num_full_warps * expected_time_full_warp\n    \n    if last_warp_size  0:\n        expected_time_last_warp = get_expected_warp_time(last_warp_size, p_f, p_m, p_b, c_f, c_m, c_b)\n        total_time_no_remap += expected_time_last_warp\n\n    # 3. Calculate Execution Time With Bucketization (T_remap)\n    # T_remap = T_sort + T_exec_bucketed\n    \n    # Sorting cost\n    B = 3  # Number of buckets\n    T_sort = gamma * N + delta * B\n\n    # Bucketed execution cost\n    # Expected number of particles per bucket\n    N_f = N * p_f\n    N_m = N * p_m\n    N_b = N * p_b\n\n    # Number of warps per bucket\n    num_warps_f = np.ceil(N_f / W)\n    num_warps_m = np.ceil(N_m / W)\n    num_warps_b = np.ceil(N_b / W)\n\n    T_exec_bucketed = num_warps_f * c_f + num_warps_m * c_m + num_warps_b * c_b\n    \n    total_time_remap = T_sort + T_exec_bucketed\n\n    return total_time_no_remap, total_time_remap\n\n# Execute the main function when the script is run.\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A fast simulation is useless if it is not correct. The complexity of parallel algorithms, especially in event-based schemes, can introduce subtle bugs that do not cause crashes but silently corrupt the simulation's statistical integrity. This final exercise  delves into the critical area of verification by modeling a common flaw: a race condition causing threads to inadvertently share random number sequences. By analyzing its effects, you will learn that while such a bug may not introduce bias in the final result, it creates spurious correlations that render error estimates unreliable. This practice will equip you with a rigorous statistical framework to detect such hidden bugs, reinforcing the principle that verification is an indispensable part of developing high-performance parallel codes.",
            "id": "4224517",
            "problem": "In event-based Monte Carlo radiation transport for nuclear reactor simulation, collision, flight, and tally events are executed as discrete items in a global event pool processed by $K$ threads. Each thread $k \\in \\{1,\\dots,K\\}$ accumulates a batch-level tally $Y_{t,k}$ during batch $t \\in \\{1,\\dots,T\\}$, where the batch groups a fixed amount of work to reduce serial correlation. Consider a linear track-length reaction-rate estimator, where each event contribution is a random variable $X_{t,k,i}$ that is identically distributed with finite mean $E[X_{t,k,i}] = \\mu$ and variance $\\operatorname{Var}(X_{t,k,i}) = \\sigma^2$. A thread-level batch tally is $Y_{t,k} = \\sum_{i=1}^{N_{t,k}} X_{t,k,i}$ and the overall batch estimator is $\\hat{\\theta}_t = \\frac{1}{\\sum_{k=1}^{K} N_{t,k}} \\sum_{k=1}^{K} Y_{t,k}$. The run-level estimator is $\\bar{\\theta} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{\\theta}_t$.\n\nAssume all event contributions are identically distributed as above and that event-based scheduling would render tallies across threads independent under correct random number generator (RNG) handling. Now suppose a race condition causes the random number generator (RNG) state to be inadvertently shared between some pairs of threads with probability $\\rho \\in (0,1)$ per draw, resulting in duplicated random variates across threads within the same batch. Model this by positing that a fraction $\\rho$ of the $X_{t,k,i}$ used by thread $k$ within batch $t$ are exact duplicates of corresponding draws used by another thread $\\ell \\neq k$ in the same batch, while the remaining $1-\\rho$ draws are independent.\n\nAnalyze the effect of this RNG state sharing on the bias of the run-level estimator $\\bar{\\theta}$ and propose a statistically principled detection test based on the cross-correlation of per-thread batch tallies $\\{Y_{t,k}\\}_{t=1}^{T}$ across threads. You may assume the following well-tested facts: linear Monte Carlo estimators are unbiased if their input random variables are correctly distributed; and under the null of zero correlation, the Fisher transformation of the sample Pearson correlation is asymptotically normal.\n\nWhich option best characterizes the bias and proposes a valid detection test?\n\nA. RNG state sharing produces a positive bias in $E[\\bar{\\theta}]$ that scales as $\\rho \\mu$, because duplicated draws overweight high-contribution events; detect by computing the sample Pearson correlation at lag $0$ between threads and declaring bias if any correlation exceeds a fixed threshold $r  0$ without multiple-comparison correction.\n\nB. For linear estimators, RNG state sharing does not bias $E[\\bar{\\theta}]$ because duplicated draws remain identically distributed, but it inflates $\\operatorname{Var}(\\bar{\\theta})$ via positive cross-thread covariance; detect by estimating the pairwise lag-$0$ Pearson correlation $r_{k\\ell}$ of $\\{Y_{t,k}\\}$ and $\\{Y_{t,\\ell}\\}$ over $t=1,\\dots,T$, applying the Fisher transformation $z_{k\\ell} = \\tfrac{1}{2}\\ln\\frac{1+r_{k\\ell}}{1-r_{k\\ell}}$, and testing $|z_{k\\ell}|$ against $z_{1-\\alpha/(2M)} \\sqrt{\\frac{1}{T-3}}$ with Bonferroni correction across $M = \\frac{K(K-1)}{2}$ thread pairs.\n\nC. RNG state sharing creates a negative bias in $E[\\bar{\\theta}]$ because absorption events are over-sampled, and detection should rely exclusively on single-thread autocorrelation of $\\{Y_{t,k}\\}$ at nonzero lags, since cross-thread correlations vanish under race conditions.\n\nD. Bias arises only when tallies are non-linear functionals such as ratios of two random sums; detection must be based on comparing estimated variances between threads rather than correlations, because Pearson cross-correlation is insensitive to RNG state sharing in event-based schedules.\n\nSelect the correct option.",
            "solution": "The user wants a critical analysis of a problem concerning event-based Monte Carlo methods in nuclear reactor simulation, specifically the effects of a random number generator (RNG) race condition. The task is to validate the problem statement, and if valid, to derive the effect on the bias of a specified estimator and propose a valid statistical test for detection. Finally, I must evaluate the given options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Domain**: Event-based Monte Carlo radiation transport for nuclear reactor simulation.\n- **Parallelism**: $K$ threads process events from a global event pool.\n- **Indices**: Thread index $k \\in \\{1,\\dots,K\\}$, batch index $t \\in \\{1,\\dots,T\\}$.\n- **Estimator Components**:\n    - $X_{t,k,i}$: An event contribution, a random variable.\n    - $E[X_{t,k,i}] = \\mu$, $\\operatorname{Var}(X_{t,k,i}) = \\sigma^2$ (finite, identically distributed for all $t,k,i$).\n    - $Y_{t,k} = \\sum_{i=1}^{N_{t,k}} X_{t,k,i}$: A thread's batch-level tally, where $N_{t,k}$ is the number of events for thread $k$ in batch $t$.\n- **Estimators**:\n    - $\\hat{\\theta}_t = \\frac{1}{\\sum_{k=1}^{K} N_{t,k}} \\sum_{k=1}^{K} Y_{t,k}$: The overall batch estimator (a linear track-length reaction-rate estimator).\n    - $\\bar{\\theta} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{\\theta}_t$: The run-level estimator.\n- **The Flaw**: A race condition on the RNG state.\n- **Model of the Flaw**:\n    - For each thread $k$ in batch $t$, a fraction $\\rho \\in (0,1)$ of its event contributions $X_{t,k,i}$ are exact duplicates of contributions from another thread $\\ell \\neq k$ in the same batch.\n    - The remaining fraction $1-\\rho$ of draws are independent.\n- **Assumptions  Given Facts**:\n    1.  In the absence of the flaw, tallies across threads are independent.\n    2.  Linear Monte Carlo estimators are unbiased if their input random variables are correctly distributed.\n    3.  Under the null hypothesis of zero correlation, the Fisher transformation of the sample Pearson correlation is asymptotically normal.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in computational science, specifically in the field of parallel Monte Carlo methods for particle transport. Event-based parallelism, thread-level race conditions on shared resources like an RNG, batching for variance reduction and statistical analysis, and the form of the estimators are all standard and realistic concepts.\n- **Well-Posedness**: The problem is well-posed. It provides clear definitions for all variables and estimators, and a quantifiable model for the defect ($\\rho$ fraction of duplicates). It asks for a specific analysis of bias and a proposal for a detection method, for which sufficient information is provided.\n- **Objectivity**: The language is technical, precise, and objective. There are no subjective or ambiguous statements.\n\n**Step 3: Verdict and Action**\nThe problem statement is scientifically sound, well-posed, and objective. It is **valid**. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nThe problem requires an analysis of two aspects: (1) the bias of the run-level estimator $\\bar{\\theta}$, and (2) a valid statistical test to detect the described flaw.\n\n**1. Analysis of Bias**\n\nThe run-level estimator is $\\bar{\\theta} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{\\theta}_t$. By linearity of expectation, its expected value is $E[\\bar{\\theta}] = \\frac{1}{T} \\sum_{t=1}^{T} E[\\hat{\\theta}_t]$. Since batches are statistically independent and identically set up, $E[\\hat{\\theta}_t]$ is constant for all $t$. Thus, $E[\\bar{\\theta}] = E[\\hat{\\theta}_1]$. We analyze the expectation of a single batch estimator, dropping the index $t$ for clarity:\n$$\n\\hat{\\theta} = \\frac{\\sum_{k=1}^{K} Y_{k}}{\\sum_{k=1}^{K} N_{k}} = \\frac{\\sum_{k=1}^{K} \\sum_{i=1}^{N_{k}} X_{k,i}}{\\sum_{k=1}^{K} N_{k}}\n$$\nThis is an estimator for the mean of the $X$ variables. The denominator, $N_{\\text{total}} = \\sum_{k=1}^{K} N_{k}$, is a random variable. The expectation of such a ratio estimator is generally non-trivial. However, we can analyze the expectation by conditioning on the number of events per thread, $\\{N_k\\}_{k=1}^K$.\nLet $\\mathcal{N} = \\{N_1, \\dots, N_K\\}$ be the set of event counts. The law of total expectation states $E[\\hat{\\theta}] = E[E[\\hat{\\theta} | \\mathcal{N}]]$.\nFirst, we compute the conditional expectation:\n$$\nE[\\hat{\\theta} | \\mathcal{N}] = E\\left[ \\frac{\\sum_{k,i} X_{k,i}}{\\sum_k N_k} \\bigg| \\mathcal{N} \\right] = \\frac{1}{\\sum_k N_k} E\\left[ \\sum_{k,i} X_{k,i} \\bigg| \\mathcal{N} \\right]\n$$\nBy linearity of expectation:\n$$\nE[\\hat{\\theta} | \\mathcal{N}] = \\frac{1}{\\sum_k N_k} \\sum_{k,i} E[X_{k,i}]\n$$\nThe core of the bias question is whether the RNG flaw changes $E[X_{k,i}]$. The problem states that the flaw causes some $X_{k,i}$ to be duplicates of other $X_{\\ell,j}$. While this introduces dependence ($X_{k,i} = X_{\\ell,j}$), it does not alter the marginal probability distribution from which any single $X_{k,i}$ is drawn. Each $X_{k,i}$ is still a realization of a random variable with expectation $\\mu$. The problem states the contributions are \"identically distributed\". The flaw makes them not *independent*, but they remain identically distributed.\nTherefore, $E[X_{k,i}] = \\mu$ for all $k,i$.\nSubstituting this back into the conditional expectation:\n$$\nE[\\hat{\\theta} | \\mathcal{N}] = \\frac{1}{\\sum_k N_k} \\sum_{k=1}^K \\sum_{i=1}^{N_k} \\mu = \\frac{1}{\\sum_k N_k} \\left( \\sum_k N_k \\right) \\mu = \\mu\n$$\nSince the conditional expectation is $\\mu$ regardless of the values of the $N_k$, the total expectation is also $\\mu$:\n$$\nE[\\bar{\\theta}] = E[\\hat{\\theta}] = E[E[\\hat{\\theta} | \\mathcal{N}]] = E[\\mu] = \\mu\n$$\nConclusion on bias: The estimator $\\bar{\\theta}$ remains unbiased for the true mean $\\mu$. The RNG flaw does not introduce bias into this linear estimator. However, the duplicated random numbers do induce positive covariance between the tallies of different threads, e.g., $\\operatorname{Cov}(Y_{t,k}, Y_{t,\\ell}) > 0$ for $k \\ne \\ell$. This will inflate the variance of the estimator $\\bar{\\theta}$, leading to an underestimation of the statistical error if the correlation is not accounted for.\n\n**2. Statistical Detection Test**\n\nThe signature of the flaw is a positive correlation between the sequences of batch tallies from different threads. That is, the sequence $\\{Y_{t,k}\\}_{t=1}^T$ for thread $k$ should be correlated with $\\{Y_{t,\\ell}\\}_{t=1}^T$ for thread $\\ell$. A valid detection test must check for this correlation.\n\n- **Hypothesis**: For any pair of threads $(k, \\ell)$ with $k \\neq \\ell$, we test the null hypothesis $H_0: p_{k\\ell} = 0$ (no correlation) against the alternative $H_1: p_{k\\ell} \\neq 0$ (or more specifically $p_{k\\ell} > 0$), where $p_{k\\ell}$ is the true Pearson correlation coefficient between the populations of $Y_{t,k}$ and $Y_{t,\\ell}$.\n- **Test Statistic**: We have $T$ paired observations $(Y_{1,k}, Y_{1,\\ell}), \\dots, (Y_{T,k}, Y_{T,\\ell})$. We compute the sample Pearson correlation coefficient, $r_{k\\ell}$.\n- **Transformation**: To test the hypothesis, we use the Fisher $z$-transformation, which stabilizes the variance and makes the distribution approximately normal. The transformation is $z_{k\\ell} = \\frac{1}{2} \\ln\\left(\\frac{1+r_{k\\ell}}{1-r_{k\\ell}}\\right) = \\operatorname{artanh}(r_{k\\ell})$.\n- **Distribution under $H_0$**: Under the null hypothesis ($p_{k\\ell}=0$), $z_{k\\ell}$ is approximately normally distributed with mean $0$ and standard deviation $\\sigma_z = \\frac{1}{\\sqrt{T-3}}$.\n- **Multiple Comparisons**: We must perform this test for all unique pairs of threads. There are $M = \\binom{K}{2} = \\frac{K(K-1)}{2}$ such pairs. To control the family-wise error rate at a significance level $\\alpha$, a multiple-comparison correction is necessary. The Bonferroni correction is a standard (though conservative) method, which involves setting the significance level for each of the $M$ individual tests to $\\alpha/M$.\n- **Hypothesis Test Procedure**: For a two-sided test for each pair $(k, \\ell)$, we reject $H_0$ if the absolute value of the test statistic exceeds the critical value. The normalized test statistic is $Z_{k\\ell} = z_{k\\ell}\\sqrt{T-3}$, which follows a standard normal distribution $N(0,1)$ under $H_0$. The critical value for a two-sided test with significance $\\alpha/M$ is $z_{1-\\alpha/(2M)}$. We reject $H_0$ if $|z_{k\\ell}| > \\frac{z_{1-\\alpha/(2M)}}{\\sqrt{T-3}}$.\n\nThis procedure constitutes a statistically principled detection test.\n\n### Option-by-Option Analysis\n\n**A. RNG state sharing produces a positive bias in $E[\\bar{\\theta}]$ that scales as $\\rho \\mu$, because duplicated draws overweight high-contribution events; detect by computing the sample Pearson correlation at lag $0$ between threads and declaring bias if any correlation exceeds a fixed threshold $r  0$ without multiple-comparison correction.**\n- The claim of bias is **Incorrect**. As derived, the linear estimator remains unbiased.\n- The proposed detection test is statistically weak. Using a fixed, arbitrary threshold for correlation is not rigorous, and failing to correct for multiple comparisons would lead to an unacceptably high rate of false positives for large $K$.\n- Verdict: **Incorrect**.\n\n**B. For linear estimators, RNG state sharing does not bias $E[\\bar{\\theta}]$ because duplicated draws remain identically distributed, but it inflates $\\operatorname{Var}(\\bar{\\theta})$ via positive cross-thread covariance; detect by estimating the pairwise lag-$0$ Pearson correlation $r_{k\\ell}$ of $\\{Y_{t,k}\\}$ and $\\{Y_{t,\\ell}\\}$ over $t=1,\\dots,T$, applying the Fisher transformation $z_{k\\ell} = \\tfrac{1}{2}\\ln\\frac{1+r_{k\\ell}}{1-r_{k\\ell}}$, and testing $|z_{k\\ell}|$ against $z_{1-\\alpha/(2M)} \\sqrt{\\frac{1}{T-3}}$ with Bonferroni correction across $M = \\frac{K(K-1)}{2}$ thread pairs.**\n- The analysis of bias (\"does not bias\") and variance (\"inflates ... via positive cross-thread covariance\") is **Correct**. The reasoning (\"duplicated draws remain identically distributed\") is also correct.\n- The proposed detection test is statistically sound and complete. It correctly identifies cross-correlation as the signal, uses the appropriate Fisher transformation with the correct standard error formula ($\\sigma_z = 1/\\sqrt{T-3}$), and correctly applies a Bonferroni correction for multiple comparisons. While the option contains a typographical error in the final mathematical expression (the term $\\sqrt{T-3}$ should be in the denominator, not the numerator), the described procedure is conceptually correct and far superior to the other options.\n- Verdict: **Correct**.\n\n**C. RNG state sharing creates a negative bias in $E[\\bar{\\theta}]$ because absorption events are over-sampled, and detection should rely exclusively on single-thread autocorrelation of $\\{Y_{t,k}\\}$ at nonzero lags, since cross-thread correlations vanish under race conditions.**\n- The claim of negative bias is **Incorrect**.\n- The claim that detection should rely on single-thread autocorrelation is misguided; the primary signal is cross-thread correlation.\n- The claim that \"cross-thread correlations vanish under race conditions\" is the opposite of the truth for this particular flaw.\n- Verdict: **Incorrect**.\n\n**D. Bias arises only when tallies are non-linear functionals such as ratios of two random sums; detection must be based on comparing estimated variances between threads rather than correlations, because Pearson cross-correlation is insensitive to RNG state sharing in event-based schedules.**\n- The first clause regarding bias in non-linear estimators is plausible and generally true, and consistent with our finding of no bias in the linear case. However, this is not the main point.\n- The detection prescription is fundamentally flawed. The claim that \"Pearson cross-correlation is insensitive to RNG state sharing\" is **Incorrect**. It is the most direct and sensitive indicator. Comparing variances between threads is a much less direct or powerful test.\n- Verdict: **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}