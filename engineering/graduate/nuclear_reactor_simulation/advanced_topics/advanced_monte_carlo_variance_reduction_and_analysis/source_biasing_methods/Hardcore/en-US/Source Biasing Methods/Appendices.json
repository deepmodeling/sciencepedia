{
    "hands_on_practices": [
        {
            "introduction": "This problem lays the groundwork for understanding any Monte Carlo simulation by focusing on the analog source. Before we can intelligently bias a source distribution, we must first be able to construct its natural, unbiased probability density function (PDF) from physical principles. This exercise guides you through the derivation of a joint PDF for a particle's initial position, energy, and direction, reinforcing the core concepts of normalization and statistical independence .",
            "id": "4250366",
            "problem": "Consider a Monte Carlo (MC) neutron transport simulation in which neutrons are born from a spatial source in a bounded region with isotropic direction and an independent energy spectrum. The goal is to construct the analog joint Probability Density Function (PDF) for sampling the initial source state $(\\mathbf{r},E,\\Omega)$ and to justify the isotropy factor for the directional component. Start from the fundamental requirements that (i) the joint PDF integrates to $1$ over the full sampling domain, (ii) the source position, energy, and direction are independent under analog sampling, and (iii) the directional distribution for an isotropic source is uniform over the unit sphere in solid angle.\n\nLet the source occupy a spherical volume $V$ of radius $R$ centered at the origin. The spatial source intensity depends only on the radial coordinate $r=|\\mathbf{r}|$ and is given by $S(\\mathbf{r})=S_{0}\\exp(-\\gamma r)$ for $0 \\le r \\le R$, with constants $S_{0}0$ and $\\gamma0$. The energy spectrum is taken to be Maxwell–Boltzmann with effective temperature $T0$, so that the energy PDF for $E\\ge 0$ is\n$$\n\\chi(E)=\\frac{2}{\\sqrt{\\pi}}\\,\\frac{E^{1/2}}{T^{3/2}}\\,\\exp\\!\\left(-\\frac{E}{T}\\right).\n$$\nAssume that directions are isotropic in three dimensions.\n\nUsing only the normalization condition for PDFs, the independence of $(\\mathbf{r},E,\\Omega)$ under analog sampling, and the uniformity of solid angle for isotropy, derive the closed-form expression for the analog joint PDF $p(\\mathbf{r},E,\\Omega)$ over $V\\times[0,\\infty)\\times\\mathbb{S}^{2}$, expressed in terms of $r$, $E$, $\\Omega$, and the parameters $R$, $\\gamma$, and $T$. Your final answer must be a single analytic expression for $p(\\mathbf{r},E,\\Omega)$.\n\nNo numerical evaluation or rounding is required. Do not include any physical units in your final expression.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It presents a standard problem in the field of Monte Carlo methods for particle transport, with all necessary information provided and no internal contradictions.\n\nThe task is to derive the joint probability density function (PDF) $p(\\mathbf{r}, E, \\Omega)$ for sampling the initial state of a source neutron, where $\\mathbf{r}$ is the position, $E$ is the energy, and $\\Omega$ is the direction of emission.\n\nThe derivation starts from the fundamental requirements provided:\n(i) The joint PDF must be normalized to unity: $\\int_{V} d\\mathbf{r} \\int_{0}^{\\infty} dE \\int_{\\mathbb{S}^2} d\\Omega \\, p(\\mathbf{r}, E, \\Omega) = 1$.\n(ii) The source variables $(\\mathbf{r}, E, \\Omega)$ are independent under analog sampling.\n(iii) The directional distribution is isotropic, meaning it is uniform in solid angle.\n\nFrom requirement (ii), the joint PDF can be expressed as the product of the individual PDFs for position, energy, and direction:\n$$\np(\\mathbf{r}, E, \\Omega) = p_{\\mathbf{r}}(\\mathbf{r}) \\, p_E(E) \\, p_{\\Omega}(\\Omega)\n$$\nEach of these component PDFs must be normalized over its respective domain. We will now derive each component.\n\n**1. Directional PDF, $p_{\\Omega}(\\Omega)$**\n\nRequirement (iii) states that the direction of emission is isotropic, implying that the probability of emission into any differential solid angle $d\\Omega$ is the same, regardless of the direction $\\Omega$. Therefore, the PDF $p_{\\Omega}(\\Omega)$ must be a constant, let's call it $C_{\\Omega}$. This constant is determined by the normalization condition for PDFs, which is a specific instance of requirement (i):\n$$\n\\int_{\\mathbb{S}^2} p_{\\Omega}(\\Omega) \\, d\\Omega = 1\n$$\nwhere $\\mathbb{S}^2$ represents the surface of the unit sphere. Substituting $p_{\\Omega}(\\Omega) = C_{\\Omega}$:\n$$\n\\int_{\\mathbb{S}^2} C_{\\Omega} \\, d\\Omega = C_{\\Omega} \\int_{\\mathbb{S}^2} d\\Omega = 1\n$$\nThe integral of the differential solid angle over the entire unit sphere is the total solid angle of a sphere, which is $4\\pi$ steradians.\n$$\nC_{\\Omega} (4\\pi) = 1 \\implies C_{\\Omega} = \\frac{1}{4\\pi}\n$$\nThus, the \"isotropy factor\" is the value of the directional PDF itself. For any direction $\\Omega$, the PDF is:\n$$\np_{\\Omega}(\\Omega) = \\frac{1}{4\\pi}\n$$\n\n**2. Energy PDF, $p_E(E)$**\n\nThe problem explicitly provides the normalized energy-dependent PDF, $\\chi(E)$. Therefore, we have:\n$$\np_E(E) = \\chi(E) = \\frac{2}{\\sqrt{\\pi}}\\,\\frac{E^{1/2}}{T^{3/2}}\\,\\exp\\left(-\\frac{E}{T}\\right) \\quad \\text{for } E \\ge 0\n$$\nThis function is given as a PDF and is indeed normalized to unity over its domain $E \\in [0, \\infty)$.\n\n**3. Spatial PDF, $p_{\\mathbf{r}}(\\mathbf{r})$**\n\nThe problem gives the source intensity as $S(\\mathbf{r}) = S_0 \\exp(-\\gamma r)$ for $r=|\\mathbf{r}| \\in [0, R]$. The spatial PDF $p_{\\mathbf{r}}(\\mathbf{r})$ must be proportional to this intensity. To be a valid PDF, it must be normalized over the source volume $V$, which is a sphere of radius $R$.\n$$\np_{\\mathbf{r}}(\\mathbf{r}) = \\frac{S(\\mathbf{r})}{\\int_V S(\\mathbf{r'}) \\, d\\mathbf{r'}} = \\frac{S_0 \\exp(-\\gamma r)}{\\int_V S_0 \\exp(-\\gamma r') \\, d\\mathbf{r'}} = \\frac{\\exp(-\\gamma r)}{\\int_V \\exp(-\\gamma r') \\, d\\mathbf{r'}}\n$$\nThe normalization constant is the integral in the denominator. We evaluate this integral in spherical coordinates, where $d\\mathbf{r'} = (r')^2 \\sin\\theta' \\, dr' \\, d\\theta' \\, d\\phi'$.\n$$\n\\int_V \\exp(-\\gamma r') \\, d\\mathbf{r'} = \\int_0^{2\\pi} d\\phi' \\int_0^{\\pi} \\sin\\theta' \\, d\\theta' \\int_0^R (r')^2 \\exp(-\\gamma r') \\, dr'\n$$\nThe angular integrals are standard:\n$$\n\\int_0^{2\\pi} d\\phi' = 2\\pi \\quad \\text{and} \\quad \\int_0^{\\pi} \\sin\\theta' \\, d\\theta' = [-\\cos\\theta']_0^{\\pi} = 2\n$$\nThe product of the angular integrals is $4\\pi$. The normalization integral reduces to:\n$$\n\\int_V \\exp(-\\gamma r') \\, d\\mathbf{r'} = 4\\pi \\int_0^R r^2 \\exp(-\\gamma r) \\, dr\n$$\nThe remaining radial integral is solved using integration by parts. The indefinite integral is given by:\n$$\n\\int r^2 \\exp(-\\gamma r) \\, dr = -\\frac{\\exp(-\\gamma r)}{\\gamma} \\left( r^2 + \\frac{2r}{\\gamma} + \\frac{2}{\\gamma^2} \\right)\n$$\nEvaluating this at the limits $r=0$ and $r=R$:\n$$\n\\int_0^R r^2 \\exp(-\\gamma r) \\, dr = \\left[ -\\frac{\\exp(-\\gamma r)}{\\gamma} \\left( r^2 + \\frac{2r}{\\gamma} + \\frac{2}{\\gamma^2} \\right) \\right]_0^R\n$$\n$$\n= \\left( -\\frac{\\exp(-\\gamma R)}{\\gamma} \\left( R^2 + \\frac{2R}{\\gamma} + \\frac{2}{\\gamma^2} \\right) \\right) - \\left( -\\frac{\\exp(0)}{\\gamma} \\left( 0 + 0 + \\frac{2}{\\gamma^2} \\right) \\right)\n$$\n$$\n= \\frac{2}{\\gamma^3} - \\exp(-\\gamma R) \\left( \\frac{R^2}{\\gamma} + \\frac{2R}{\\gamma^2} + \\frac{2}{\\gamma^3} \\right)\n$$\n$$\n= \\frac{1}{\\gamma^3} \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]\n$$\nLet's call the full normalization integral $N_r = \\int_V \\exp(-\\gamma r') \\, d\\mathbf{r'}$.\n$$\nN_r = \\frac{4\\pi}{\\gamma^3} \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]\n$$\nThe spatial PDF is therefore:\n$$\np_{\\mathbf{r}}(\\mathbf{r}) = \\frac{\\exp(-\\gamma r)}{N_r} = \\frac{\\gamma^3 \\exp(-\\gamma r)}{4\\pi \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]} \\quad \\text{for } r \\le R\n$$\n\n**4. Assembling the Joint PDF**\n\nFinally, we combine the three component PDFs to obtain the joint PDF $p(\\mathbf{r}, E, \\Omega)$:\n$$\np(\\mathbf{r}, E, \\Omega) = p_{\\mathbf{r}}(\\mathbf{r}) \\, p_E(E) \\, p_{\\Omega}(\\Omega)\n$$\n$$\np(\\mathbf{r}, E, \\Omega) = \\left( \\frac{\\gamma^3 \\exp(-\\gamma r)}{4\\pi \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]} \\right) \\left( \\frac{2 E^{1/2} \\exp(-E/T)}{\\sqrt{\\pi} T^{3/2}} \\right) \\left( \\frac{1}{4\\pi} \\right)\n$$\nCombining the constant terms and the exponential terms yields the final expression:\n$$\np(\\mathbf{r}, E, \\Omega) = \\frac{2\\gamma^3}{16\\pi^2 \\sqrt{\\pi} T^{3/2} \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]} E^{1/2} \\exp\\left(-\\gamma r - \\frac{E}{T}\\right)\n$$\n$$\np(\\mathbf{r}, E, \\Omega) = \\frac{\\gamma^3}{8\\pi^{5/2} T^{3/2} \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]} E^{1/2} \\exp\\left(-\\gamma r - \\frac{E}{T}\\right)\n$$\nThis expression is the analog joint PDF for the source state $(\\mathbf{r}, E, \\Omega)$ over the domain $V \\times [0, \\infty) \\times \\mathbb{S}^2$, where $r=|\\mathbf{r}|$. It is expressed purely in terms of the state variables and the given parameters.",
            "answer": "$$\n\\boxed{\\frac{\\gamma^3 E^{1/2} \\exp\\left(-\\gamma r - \\frac{E}{T}\\right)}{8 \\pi^{5/2} T^{3/2} \\left[ 2 - \\left(\\gamma^2 R^2 + 2\\gamma R + 2\\right)\\exp(-\\gamma R) \\right]}}\n$$"
        },
        {
            "introduction": "Having established the analog sampling framework, we now explore how to improve it using importance sampling, a powerful variance reduction technique. This problem presents a classic deep-penetration scenario where analog methods are inefficient and introduces the exponential transform to \"stretch\" particle paths toward a region of interest. By deriving the optimal biasing parameter, you will gain quantitative insight into how a well-chosen biased distribution can dramatically reduce statistical uncertainty .",
            "id": "4250382",
            "problem": "Consider a monoenergetic neutral particle beam incident normally on a homogeneous, purely absorbing slab of thickness $L$ and macroscopic total cross section $\\Sigma_{t}$. Assume no scattering and no internal sources. Each history launches a particle at the entry face $x=0$ with unit weight and straight-line motion in the $+x$ direction. The free-path distance to the first collision, $s$, is distributed according to the exponential attenuation law implied by the Beer–Lambert law, i.e., the analog probability density function (PDF) is $f(s) = \\Sigma_{t} \\exp(-\\Sigma_{t} s)$ for $s \\geq 0$. A surface flux tally at the exit face $x=L$ registers a contribution if and only if the sampled $s$ exceeds $L$ (i.e., the particle exits without colliding), and the tally contribution is otherwise zero.\n\nYou will compare the variance of two unbiased Monte Carlo (MC) estimators for the exit-face flux tally and then derive the biasing parameter that minimizes the variance of the importance-sampled estimator:\n- The analog estimator, which scores $Y = \\mathbb{I}[s \\geq L]$ under $f(s)$, where $\\mathbb{I}[\\cdot]$ denotes the indicator function.\n- The exponentially transformed importance-sampled estimator, which samples $s$ from the biased PDF $g_{q}(s) = (\\Sigma_{t} - q)\\exp\\!\\big[-(\\Sigma_{t} - q)s\\big]$ for $s \\geq 0$ with $q$ satisfying $0  q  \\Sigma_{t}$, and scores $X_{q} = w_{q}(s)\\,\\mathbb{I}[s \\geq L]$, where the weight is the likelihood ratio $w_{q}(s) = \\dfrac{f(s)}{g_{q}(s)} = \\dfrac{\\Sigma_{t}}{\\Sigma_{t} - q}\\exp(-q s)$.\n\nTasks:\n- Starting from the exponential attenuation law and the definitions above, derive the mean and variance of the analog estimator $Y$ for the exit-face tally.\n- Prove the unbiasedness of the importance-sampled estimator $X_{q}$ and derive its variance as a function of $q$, $\\Sigma_{t}$, and $L$.\n- By direct minimization with respect to $q$ over the admissible interval $0  q  \\Sigma_{t}$, derive the condition on $q$ that minimizes the variance of $X_{q}$, and express the optimal value as a dimensionless ratio $r^{\\star} = q^{\\star}/\\Sigma_{t}$ in terms of the dimensionless optical thickness $\\tau = \\Sigma_{t} L$.\n\nAnswer specification:\n- Report the final answer as the single closed-form analytic expression for $r^{\\star}$ in terms of $\\tau$.\n- No numerical evaluation is required, and no physical units should be included in the final answer.",
            "solution": "### Solution Derivation\n\nThe analysis proceeds by addressing each of the three tasks in order.\n\n**1. Mean and Variance of the Analog Estimator $Y$**\n\nThe analog estimator is $Y = \\mathbb{I}[s \\geq L]$, where the random variable $s$ is drawn from the analog PDF $f(s) = \\Sigma_{t} \\exp(-\\Sigma_{t} s)$. $Y$ is a Bernoulli random variable.\n\nThe mean (expected value) of $Y$ is the probability of the event $\\{s \\geq L\\}$.\n$$E[Y] = P(s \\geq L) = \\int_L^{\\infty} f(s) \\, ds = \\int_L^{\\infty} \\Sigma_{t} \\exp(-\\Sigma_{t} s) \\, ds$$\n$$E[Y] = \\left[ -\\exp(-\\Sigma_{t} s) \\right]_L^{\\infty} = 0 - (-\\exp(-\\Sigma_t L)) = \\exp(-\\Sigma_t L)$$\nThis is the physical probability of transmission without collision, so the estimator is unbiased.\n\nThe variance of $Y$ is given by $\\text{Var}(Y) = E[Y^2] - (E[Y])^2$. Since $Y$ is an indicator function, its values are either $0$ or $1$, which implies $Y^2 = Y$.\nTherefore, $E[Y^2] = E[Y] = \\exp(-\\Sigma_t L)$.\nThe variance is:\n$$\\text{Var}(Y) = \\exp(-\\Sigma_t L) - (\\exp(-\\Sigma_t L))^2 = \\exp(-\\Sigma_t L) \\left(1 - \\exp(-\\Sigma_t L)\\right)$$\n\n**2. Unbiasedness and Variance of the Importance-Sampled Estimator $X_q$**\n\nThe importance-sampled estimator is $X_q = w_q(s) \\mathbb{I}[s \\geq L]$, where $s$ is now sampled from the biased PDF $g_q(s)$. The expectation is taken with respect to $g_q(s)$, denoted by $E_q[\\cdot]$.\n\nTo prove unbiasedness, we must show that $E_q[X_q] = E[Y]$.\n$$E_q[X_q] = \\int_0^{\\infty} X_q(s) g_q(s) \\, ds = \\int_0^{\\infty} w_q(s) \\mathbb{I}[s \\geq L] g_q(s) \\, ds$$\nThe indicator function restricts the lower limit of integration to $L$. Substituting $w_q(s) = f(s)/g_q(s)$:\n$$E_q[X_q] = \\int_L^{\\infty} \\frac{f(s)}{g_q(s)} g_q(s) \\, ds = \\int_L^{\\infty} f(s) \\, ds = E[Y] = \\exp(-\\Sigma_t L)$$\nThe estimator $X_q$ is therefore unbiased for any valid choice of $q$.\n\nThe variance of $X_q$ is $\\text{Var}_q(X_q) = E_q[X_q^2] - (E_q[X_q])^2$. The second term is $(E[Y])^2 = \\exp(-2\\Sigma_t L)$. We need to compute the second moment, $E_q[X_q^2]$.\n$$E_q[X_q^2] = \\int_0^{\\infty} (w_q(s) \\mathbb{I}[s \\geq L])^2 g_q(s) \\, ds = \\int_L^{\\infty} w_q^2(s) g_q(s) \\, ds$$\nLet's analyze the integrand:\n$$w_q^2(s) g_q(s) = \\left(\\frac{f(s)}{g_q(s)}\\right)^2 g_q(s) = \\frac{f^2(s)}{g_q(s)}$$\nSubstituting the explicit forms of the PDFs:\n$$f^2(s) = \\Sigma_t^2 \\exp(-2\\Sigma_t s)$$\n$$g_q(s) = (\\Sigma_t - q) \\exp(-(\\Sigma_t - q)s)$$\nThe integrand becomes:\n$$\\frac{f^2(s)}{g_q(s)} = \\frac{\\Sigma_t^2 \\exp(-2\\Sigma_t s)}{(\\Sigma_t - q) \\exp(-(\\Sigma_t - q)s)} = \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\exp(-(2\\Sigma_t - (\\Sigma_t - q))s) = \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\exp(-(\\Sigma_t + q)s)$$\nNow, we integrate this expression from $L$ to $\\infty$:\n$$E_q[X_q^2] = \\int_L^{\\infty} \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\exp(-(\\Sigma_t + q)s) \\, ds = \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\left[ \\frac{\\exp(-(\\Sigma_t + q)s)}{-(\\Sigma_t + q)} \\right]_L^{\\infty}$$\n$$E_q[X_q^2] = \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\left( 0 - \\frac{\\exp(-(\\Sigma_t + q)L)}{-(\\Sigma_t + q)} \\right) = \\frac{\\Sigma_t^2}{(\\Sigma_t - q)(\\Sigma_t + q)} \\exp(-(\\Sigma_t + q)L)$$\n$$E_q[X_q^2] = \\frac{\\Sigma_t^2}{\\Sigma_t^2 - q^2} \\exp(-(\\Sigma_t + q)L)$$\nThe variance is then:\n$$\\text{Var}_q(X_q) = \\frac{\\Sigma_t^2}{\\Sigma_t^2 - q^2} \\exp(-(\\Sigma_t + q)L) - \\exp(-2\\Sigma_t L)$$\n\n**3. Minimization of the Variance of $X_q$**\n\nTo find the optimal parameter $q^{\\star}$ that minimizes $\\text{Var}_q(X_q)$, we need to find the value of $q$ in the interval $(0, \\Sigma_t)$ for which $\\frac{d}{dq}\\text{Var}_q(X_q) = 0$. Since the term $(E_q[X_q])^2 = \\exp(-2\\Sigma_t L)$ is independent of $q$, minimizing the variance is equivalent to minimizing the second moment $E_q[X_q^2]$.\nLet $V_2(q) = E_q[X_q^2] = \\frac{\\Sigma_t^2}{\\Sigma_t^2 - q^2} \\exp(-(\\Sigma_t + q)L)$.\nTo simplify differentiation, we can minimize its natural logarithm, $\\ln(V_2(q))$, since the logarithm is a monotonically increasing function.\n$$\\ln(V_2(q)) = \\ln(\\Sigma_t^2) - \\ln(\\Sigma_t^2 - q^2) - (\\Sigma_t + q)L$$\nDifferentiating with respect to $q$:\n$$\\frac{d}{dq}\\ln(V_2(q)) = 0 - \\frac{-2q}{\\Sigma_t^2 - q^2} - L = \\frac{2q}{\\Sigma_t^2 - q^2} - L$$\nSetting the derivative to zero to find the extremum:\n$$\\frac{2q}{\\Sigma_t^2 - q^2} = L$$\n$$2q = L(\\Sigma_t^2 - q^2) \\implies Lq^2 + 2q - L\\Sigma_t^2 = 0$$\nThis is a quadratic equation in $q$. We solve for $q$ using the quadratic formula $q = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ with $a=L$, $b=2$, and $c=-L\\Sigma_t^2$:\n$$q = \\frac{-2 \\pm \\sqrt{2^2 - 4(L)(-L\\Sigma_t^2)}}{2L} = \\frac{-2 \\pm \\sqrt{4 + 4L^2\\Sigma_t^2}}{2L} = \\frac{-1 \\pm \\sqrt{1 + L^2\\Sigma_t^2}}{L}$$\nWe have two potential solutions for $q$. We must choose the one that satisfies the given constraint $0  q  \\Sigma_t$.\nThe solution $q = \\frac{-1 - \\sqrt{1 + L^2\\Sigma_t^2}}{L}$ is always negative since $L>0$, so it is discarded.\nThe other solution is $q^{\\star} = \\frac{-1 + \\sqrt{1 + L^2\\Sigma_t^2}}{L}$. Since $\\sqrt{1+L^2\\Sigma_t^2} > 1$, this value for $q^{\\star}$ is positive. We also must verify that $q^{\\star}  \\Sigma_t$:\n$$\\frac{-1 + \\sqrt{1 + L^2\\Sigma_t^2}}{L}  \\Sigma_t \\implies -1 + \\sqrt{1 + L^2\\Sigma_t^2}  L\\Sigma_t \\implies \\sqrt{1 + L^2\\Sigma_t^2}  1 + L\\Sigma_t$$\nSince both sides are positive for $L>0$ and $\\Sigma_t>0$, we can square them without changing the inequality:\n$$1 + L^2\\Sigma_t^2  (1 + L\\Sigma_t)^2 = 1 + 2L\\Sigma_t + L^2\\Sigma_t^2$$\n$$0  2L\\Sigma_t$$\nThis inequality is always true, so $q^{\\star}$ is within the valid range $(0, \\Sigma_t)$. The second derivative of $\\ln(V_2(q))$ is $\\frac{2(\\Sigma_t^2+q^2)}{(\\Sigma_t^2-q^2)^2}$, which is positive for $q \\in (0, \\Sigma_t)$, confirming that $q^{\\star}$ corresponds to a minimum.\n\nThe final step is to express the result as the dimensionless ratio $r^{\\star} = q^{\\star}/\\Sigma_t$ in terms of the optical thickness $\\tau = \\Sigma_t L$.\n$$r^{\\star} = \\frac{q^{\\star}}{\\Sigma_t} = \\frac{1}{\\Sigma_t} \\left( \\frac{-1 + \\sqrt{1 + L^2\\Sigma_t^2}}{L} \\right) = \\frac{-1 + \\sqrt{1 + (L\\Sigma_t)^2}}{L\\Sigma_t}$$\nSubstituting $\\tau = \\Sigma_t L$:\n$$r^{\\star} = \\frac{-1 + \\sqrt{1 + \\tau^2}}{\\tau}$$\nThis is the final expression for the optimal biasing parameter ratio.",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{1+\\tau^2}-1}{\\tau}}\n$$"
        },
        {
            "introduction": "While source biasing can significantly reduce variance, its benefits are not limitless and can come with computational costs. This practice explores the critical concept of simulation efficiency through the Figure of Merit (FOM), which balances variance reduction against the time required to achieve it. By analyzing a simplified model of aggressive source biasing, you will derive an efficiency curve that demonstrates the principle of diminishing returns and highlights the importance of finding a practical, not just theoretical, optimum .",
            "id": "4250409",
            "problem": "Consider a simplified but scientifically consistent model of source biasing in Monte Carlo (MC) nuclear reactor simulation to estimate a scalar response due to source particles emitted from two disjoint phase-space regions. Let the original source probability distribution place probability $p_{A}$ on region $A$ and $p_{B}$ on region $B$, with $p_{A} + p_{B} = 1$ and $p_{A}, p_{B} \\in (0,1)$. A single source history sampled from region $i \\in \\{A,B\\}$ contributes a deterministic nonnegative score $g_{i}$ to the tally, representing the expected contribution of a history launched from that region to the response of interest. Let the true mean response be $\\mu = p_{A} g_{A} + p_{B} g_{B}$.\n\nWe introduce source biasing via a mixture importance sampling construction indexed by an intensity parameter $b \\in [0,1]$. The biased source distribution $q(\\cdot \\, ; b)$ redistributes probability mass away from the original source and toward region $A$ as\n$$\nq_{A}(b) = p_{A} + b \\left(1 - p_{A}\\right), \\qquad q_{B}(b) = p_{B} - b p_{B} = p_{B} \\left(1 - b\\right),\n$$\nwhich satisfies $q_{A}(b) + q_{B}(b) = 1$ for all $b \\in [0,1]$. Source particles are sampled from $q(\\cdot \\, ; b)$ and reweighted by the standard importance factor $w_{i}(b) = \\frac{p_{i}}{q_{i}(b)}$ to preserve unbiasedness of the estimator. Assume that each biased source history incurs a dimensionless computational cost $C(b)$ modeled as\n$$\nC(b) = C_{0} \\left(1 + \\gamma b\\right),\n$$\nwhere $C_{0}  0$ is the baseline cost per source history under the original source and $\\gamma \\geq 0$ quantifies overhead from implementing the biased sampling mechanism. Computational time is measured in units of the baseline time per history, so the Figure of Merit (FOM) defined below is dimensionless.\n\nStarting from the definitions of the unbiased weighted estimator, its variance under the biased distribution, and the standard MC efficiency metric Figure of Merit (FOM), which is defined as the inverse of the product of squared relative error and computational time, derive a closed-form analytic expression for the efficiency curve $\\mathrm{FOM}(b)$ as a function of $b$, $p_{A}$, $p_{B}$, $g_{A}$, $g_{B}$, $C_{0}$, and $\\gamma$. Your derivation must start from first principles of importance sampling and the definition of variance under a change of measure, and must show explicitly how diminishing returns arise as $b \\to 1$.\n\nExpress your final answer as a single simplified analytic expression for $\\mathrm{FOM}(b)$ in terms of $b$, $p_{A}$, $p_{B}$, $g_{A}$, $g_{B}$, $C_{0}$, and $\\gamma$. No numerical evaluation is required. The final answer must be dimensionless. No rounding is required.",
            "solution": "We begin from the fundamental framework of importance sampling in Monte Carlo (MC) estimation. Let the original source distribution be a probability mass function over the two regions, $p_{A}$ and $p_{B}$, and let the biased distribution be $q_{A}(b)$ and $q_{B}(b)$ with $q_{A}(b) + q_{B}(b) = 1$. A single-history estimator that preserves unbiasedness under the biased distribution uses an importance weight $w_{i}(b) = \\frac{p_{i}}{q_{i}(b)}$ multiplying the score $g_{i}$, because the expectation under the biased distribution satisfies\n$$\n\\mathbb{E}_{q}\\!\\left[w(b) \\, g\\right] = \\sum_{i \\in \\{A,B\\}} q_{i}(b) \\, \\frac{p_{i}}{q_{i}(b)} \\, g_{i} = \\sum_{i \\in \\{A,B\\}} p_{i} g_{i} = \\mu.\n$$\nTherefore, the estimator remains unbiased for all $b \\in [0,1]$. The variance of the single-history weighted estimator under the biased distribution is given by the second moment under $q$ minus the square of the mean:\n$$\n\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right] = \\mathbb{E}_{q}\\!\\left[\\left(w(b) \\, g\\right)^{2}\\right] - \\mu^{2}.\n$$\nSince $w(b) \\, g$ takes the values $\\frac{p_{A}}{q_{A}(b)} g_{A}$ with probability $q_{A}(b)$ and $\\frac{p_{B}}{q_{B}(b)} g_{B}$ with probability $q_{B}(b)$, the second moment evaluates to\n$$\n\\mathbb{E}_{q}\\!\\left[\\left(w(b) \\, g\\right)^{2}\\right]\n= q_{A}(b) \\left(\\frac{p_{A}}{q_{A}(b)} g_{A}\\right)^{2} + q_{B}(b) \\left(\\frac{p_{B}}{q_{B}(b)} g_{B}\\right)^{2}\n= \\frac{p_{A}^{2} g_{A}^{2}}{q_{A}(b)} + \\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)}.\n$$\nThus, the single-history variance under the biased distribution is\n$$\n\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right] = \\frac{p_{A}^{2} g_{A}^{2}}{q_{A}(b)} + \\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)} - \\mu^{2},\n$$\nwith $\\mu = p_{A} g_{A} + p_{B} g_{B}$.\n\nThe Figure of Merit (FOM) for MC, defined as the inverse of the product of squared relative error and computational time, can be expressed in terms of per-history quantities. For $N$ independent histories, the sample mean has variance $\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right] / N$, so the squared relative error is\n$$\nR^{2}(b) = \\frac{\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right]}{N \\, \\mu^{2}}.\n$$\nIf the average computational cost per history is $C(b)$ in units of baseline time per history, then the total computational time is $T = N \\, C(b)$. The FOM is\n$$\n\\mathrm{FOM}(b) = \\frac{1}{R^{2}(b) \\, T} = \\frac{1}{\\left(\\frac{\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right]}{N \\, \\mu^{2}}\\right) \\, N \\, C(b)} = \\frac{\\mu^{2}}{\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right] \\, C(b)}.\n$$\nConsequently, $\\mathrm{FOM}(b)$ is independent of $N$ and depends only on the single-history variance and the per-history cost under the biased distribution.\n\nWe substitute the expressions derived above. The cost model is taken as\n$$\nC(b) = C_{0} \\left(1 + \\gamma b\\right),\n$$\nwhere $C_{0}  0$ and $\\gamma \\geq 0$. The biased probabilities are\n$$\nq_{A}(b) = p_{A} + b \\left(1 - p_{A}\\right), \\qquad q_{B}(b) = p_{B} \\left(1 - b\\right).\n$$\nCollecting these yields\n$$\n\\mathrm{FOM}(b) = \\frac{\\mu^{2}}{C_{0} \\left(1 + \\gamma b\\right)} \\left[ \\frac{1}{\\frac{p_{A}^{2} g_{A}^{2}}{q_{A}(b)} + \\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)} - \\mu^{2}} \\right],\n$$\nwith $\\mu = p_{A} g_{A} + p_{B} g_{B}$.\n\nTo make the dependence explicit, we replace $q_{A}(b)$ and $q_{B}(b)$:\n$$\n\\mathrm{FOM}(b) = \\frac{\\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2}}{C_{0} \\left(1 + \\gamma b\\right) \\left( \\frac{p_{A}^{2} g_{A}^{2}}{p_{A} + b \\left(1 - p_{A}\\right)} + \\frac{p_{B}^{2} g_{B}^{2}}{p_{B} \\left(1 - b\\right)} - \\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2} \\right)}.\n$$\n\nWe now analyze diminishing returns. As $b \\to 0$, $q_{A}(b) \\to p_{A}$ and $q_{B}(b) \\to p_{B}$, and the variance tends to\n$$\n\\mathrm{Var}_{q}\\!\\left[w(0) \\, g\\right] = \\frac{p_{A}^{2} g_{A}^{2}}{p_{A}} + \\frac{p_{B}^{2} g_{B}^{2}}{p_{B}} - \\mu^{2} = p_{A} g_{A}^{2} + p_{B} g_{B}^{2} - \\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2},\n$$\nthe original source variance, and the cost tends to $C(0) = C_{0}$. As $b$ increases from zero, the term $\\frac{p_{A}^{2} g_{A}^{2}}{q_{A}(b)}$ decreases because $q_{A}(b)$ increases, reflecting variance reduction due to more frequent sampling from the important region $A$. Simultaneously, the term $\\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)}$ increases because $q_{B}(b) = p_{B} (1 - b)$ decreases, penalizing neglect of region $B$. In the limit $b \\to 1^{-}$,\n$$\nq_{B}(b) \\to 0^{+} \\quad \\Rightarrow \\quad \\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)} \\to \\infty,\n$$\nso the variance diverges unless $g_{B} = 0$. Additionally, $C(b)$ increases linearly with $b$. These competing effects produce diminishing returns: initial increases in $b$ reduce the dominant variance term from region $A$, but aggressive biasing eventually causes the region $B$ weight term to dominate and blow up the variance while also increasing cost, thereby reducing $\\mathrm{FOM}(b)$ for large $b$.\n\nTherefore, the efficiency curve $\\mathrm{FOM}(b)$, derived directly from the foundational definitions of unbiased weighted estimators, variance under a change of measure, and MC efficiency, is the explicit analytic function\n$$\n\\mathrm{FOM}(b) = \\frac{\\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2}}{C_{0} \\left(1 + \\gamma b\\right) \\left( \\frac{p_{A}^{2} g_{A}^{2}}{p_{A} + b \\left(1 - p_{A}\\right)} + \\frac{p_{B}^{2} g_{B}^{2}}{p_{B} \\left(1 - b\\right)} - \\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2} \\right)}.\n$$\nThis curve explicitly displays the diminishing returns in variance reduction as bias intensity $b$ becomes increasingly aggressive.",
            "answer": "$$\\boxed{\\frac{\\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2}}{C_{0} \\left(1 + \\gamma b\\right) \\left( \\frac{p_{A}^{2} g_{A}^{2}}{p_{A} + b \\left(1 - p_{A}\\right)} + \\frac{p_{B}^{2} g_{B}^{2}}{p_{B} \\left(1 - b\\right)} - \\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2} \\right)}}$$"
        }
    ]
}