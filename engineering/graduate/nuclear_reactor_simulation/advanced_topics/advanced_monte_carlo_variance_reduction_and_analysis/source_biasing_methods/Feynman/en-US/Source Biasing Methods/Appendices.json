{
    "hands_on_practices": [
        {
            "introduction": "Before we can strategically alter a source distribution for variance reduction, we must first master the art of constructing the natural, or \"analog,\" source probability density function (PDF). This exercise  guides you through the process of deriving the joint PDF $p(\\mathbf{r},E,\\Omega)$ for a neutron source from first principles. By correctly normalizing the spatial, energy, and angular components, you will build the foundational model upon which all biasing techniques are based.",
            "id": "4250366",
            "problem": "Consider a Monte Carlo (MC) neutron transport simulation in which neutrons are born from a spatial source in a bounded region with isotropic direction and an independent energy spectrum. The goal is to construct the analog joint Probability Density Function (PDF) for sampling the initial source state $(\\mathbf{r},E,\\Omega)$ and to justify the isotropy factor for the directional component. Start from the fundamental requirements that (i) the joint PDF integrates to $1$ over the full sampling domain, (ii) the source position, energy, and direction are independent under analog sampling, and (iii) the directional distribution for an isotropic source is uniform over the unit sphere in solid angle.\n\nLet the source occupy a spherical volume $V$ of radius $R$ centered at the origin. The spatial source intensity depends only on the radial coordinate $r=|\\mathbf{r}|$ and is given by $S(\\mathbf{r})=S_{0}\\exp(-\\gamma r)$ for $0 \\le r \\le R$, with constants $S_{0}0$ and $\\gamma0$. The energy spectrum is taken to be Maxwell–Boltzmann with effective temperature $T0$, so that the energy PDF for $E\\ge 0$ is\n$$\n\\chi(E)=\\frac{2}{\\sqrt{\\pi}}\\,\\frac{E^{1/2}}{T^{3/2}}\\,\\exp\\!\\left(-\\frac{E}{T}\\right).\n$$\nAssume that directions are isotropic in three dimensions.\n\nUsing only the normalization condition for PDFs, the independence of $(\\mathbf{r},E,\\Omega)$ under analog sampling, and the uniformity of solid angle for isotropy, derive the closed-form expression for the analog joint PDF $p(\\mathbf{r},E,\\Omega)$ over $V\\times[0,\\infty)\\times\\mathbb{S}^{2}$, expressed in terms of $r$, $E$, $\\Omega$, and the parameters $R$, $\\gamma$, and $T$. Your final answer must be a single analytic expression for $p(\\mathbf{r},E,\\Omega)$.\n\nNo numerical evaluation or rounding is required. Do not include any physical units in your final expression.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It presents a standard problem in the field of Monte Carlo methods for particle transport, with all necessary information provided and no internal contradictions.\n\nThe task is to derive the joint probability density function (PDF) $p(\\mathbf{r}, E, \\Omega)$ for sampling the initial state of a source neutron, where $\\mathbf{r}$ is the position, $E$ is the energy, and $\\Omega$ is the direction of emission.\n\nThe derivation starts from the fundamental requirements provided:\n(i) The joint PDF must be normalized to unity: $\\int_{V} d\\mathbf{r} \\int_{0}^{\\infty} dE \\int_{\\mathbb{S}^2} d\\Omega \\, p(\\mathbf{r}, E, \\Omega) = 1$.\n(ii) The source variables $(\\mathbf{r}, E, \\Omega)$ are independent under analog sampling.\n(iii) The directional distribution is isotropic, meaning it is uniform in solid angle.\n\nFrom requirement (ii), the joint PDF can be expressed as the product of the individual PDFs for position, energy, and direction:\n$$\np(\\mathbf{r}, E, \\Omega) = p_{\\mathbf{r}}(\\mathbf{r}) \\, p_E(E) \\, p_{\\Omega}(\\Omega)\n$$\nEach of these component PDFs must be normalized over its respective domain. We will now derive each component.\n\n**1. Directional PDF, $p_{\\Omega}(\\Omega)$**\n\nRequirement (iii) states that the direction of emission is isotropic, implying that the probability of emission into any differential solid angle $d\\Omega$ is the same, regardless of the direction $\\Omega$. Therefore, the PDF $p_{\\Omega}(\\Omega)$ must be a constant, let's call it $C_{\\Omega}$. This constant is determined by the normalization condition for PDFs, which is a specific instance of requirement (i):\n$$\n\\int_{\\mathbb{S}^2} p_{\\Omega}(\\Omega) \\, d\\Omega = 1\n$$\nwhere $\\mathbb{S}^2$ represents the surface of the unit sphere. Substituting $p_{\\Omega}(\\Omega) = C_{\\Omega}$:\n$$\n\\int_{\\mathbb{S}^2} C_{\\Omega} \\, d\\Omega = C_{\\Omega} \\int_{\\mathbb{S}^2} d\\Omega = 1\n$$\nThe integral of the differential solid angle over the entire unit sphere is the total solid angle of a sphere, which is $4\\pi$ steradians.\n$$\nC_{\\Omega} (4\\pi) = 1 \\implies C_{\\Omega} = \\frac{1}{4\\pi}\n$$\nThus, the \"isotropy factor\" is the value of the directional PDF itself. For any direction $\\Omega$, the PDF is:\n$$\np_{\\Omega}(\\Omega) = \\frac{1}{4\\pi}\n$$\n\n**2. Energy PDF, $p_E(E)$**\n\nThe problem explicitly provides the normalized energy-dependent PDF, $\\chi(E)$. Therefore, we have:\n$$\np_E(E) = \\chi(E) = \\frac{2}{\\sqrt{\\pi}}\\,\\frac{E^{1/2}}{T^{3/2}}\\,\\exp\\left(-\\frac{E}{T}\\right) \\quad \\text{for } E \\ge 0\n$$\nThis function is given as a PDF and is indeed normalized to unity over its domain $E \\in [0, \\infty)$.\n\n**3. Spatial PDF, $p_{\\mathbf{r}}(\\mathbf{r})$**\n\nThe problem gives the source intensity as $S(\\mathbf{r}) = S_0 \\exp(-\\gamma r)$ for $r=|\\mathbf{r}| \\in [0, R]$. The spatial PDF $p_{\\mathbf{r}}(\\mathbf{r})$ must be proportional to this intensity. To be a valid PDF, it must be normalized over the source volume $V$, which is a sphere of radius $R$.\n$$\np_{\\mathbf{r}}(\\mathbf{r}) = \\frac{S(\\mathbf{r})}{\\int_V S(\\mathbf{r'}) \\, d\\mathbf{r'}} = \\frac{S_0 \\exp(-\\gamma r)}{\\int_V S_0 \\exp(-\\gamma r') \\, d\\mathbf{r'}} = \\frac{\\exp(-\\gamma r)}{\\int_V \\exp(-\\gamma r') \\, d\\mathbf{r'}}\n$$\nThe normalization constant is the integral in the denominator. We evaluate this integral in spherical coordinates, where $d\\mathbf{r'} = (r')^2 \\sin\\theta' \\, dr' \\, d\\theta' \\, d\\phi'$.\n$$\n\\int_V \\exp(-\\gamma r') \\, d\\mathbf{r'} = \\int_0^{2\\pi} d\\phi' \\int_0^{\\pi} \\sin\\theta' \\, d\\theta' \\int_0^R (r')^2 \\exp(-\\gamma r') \\, dr'\n$$\nThe angular integrals are standard:\n$$\n\\int_0^{2\\pi} d\\phi' = 2\\pi \\quad \\text{and} \\quad \\int_0^{\\pi} \\sin\\theta' \\, d\\theta' = [-\\cos\\theta']_0^{\\pi} = 2\n$$\nThe product of the angular integrals is $4\\pi$. The normalization integral reduces to:\n$$\n\\int_V \\exp(-\\gamma r') \\, d\\mathbf{r'} = 4\\pi \\int_0^R r^2 \\exp(-\\gamma r) \\, dr\n$$\nThe remaining radial integral is solved using integration by parts. The indefinite integral is given by:\n$$\n\\int r^2 \\exp(-\\gamma r) \\, dr = -\\frac{\\exp(-\\gamma r)}{\\gamma} \\left( r^2 + \\frac{2r}{\\gamma} + \\frac{2}{\\gamma^2} \\right)\n$$\nEvaluating this at the limits $r=0$ and $r=R$:\n$$\n\\int_0^R r^2 \\exp(-\\gamma r) \\, dr = \\left[ -\\frac{\\exp(-\\gamma r)}{\\gamma} \\left( r^2 + \\frac{2r}{\\gamma} + \\frac{2}{\\gamma^2} \\right) \\right]_0^R\n$$\n$$\n= \\left( -\\frac{\\exp(-\\gamma R)}{\\gamma} \\left( R^2 + \\frac{2R}{\\gamma} + \\frac{2}{\\gamma^2} \\right) \\right) - \\left( -\\frac{\\exp(0)}{\\gamma} \\left( 0 + 0 + \\frac{2}{\\gamma^2} \\right) \\right)\n$$\n$$\n= \\frac{2}{\\gamma^3} - \\exp(-\\gamma R) \\left( \\frac{R^2}{\\gamma} + \\frac{2R}{\\gamma^2} + \\frac{2}{\\gamma^3} \\right)\n$$\n$$\n= \\frac{1}{\\gamma^3} \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]\n$$\nLet's call the full normalization integral $N_r = \\int_V \\exp(-\\gamma r') \\, d\\mathbf{r'}$.\n$$\nN_r = \\frac{4\\pi}{\\gamma^3} \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]\n$$\nThe spatial PDF is therefore:\n$$\np_{\\mathbf{r}}(\\mathbf{r}) = \\frac{\\exp(-\\gamma r)}{N_r} = \\frac{\\gamma^3 \\exp(-\\gamma r)}{4\\pi \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]} \\quad \\text{for } r \\le R\n$$\n\n**4. Assembling the Joint PDF**\n\nFinally, we combine the three component PDFs to obtain the joint PDF $p(\\mathbf{r}, E, \\Omega)$:\n$$\np(\\mathbf{r}, E, \\Omega) = p_{\\mathbf{r}}(\\mathbf{r}) \\, p_E(E) \\, p_{\\Omega}(\\Omega)\n$$\n$$\np(\\mathbf{r}, E, \\Omega) = \\left( \\frac{\\gamma^3 \\exp(-\\gamma r)}{4\\pi \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]} \\right) \\left( \\frac{2 E^{1/2} \\exp(-E/T)}{\\sqrt{\\pi} T^{3/2}} \\right) \\left( \\frac{1}{4\\pi} \\right)\n$$\nCombining the constant terms and the exponential terms yields the final expression:\n$$\np(\\mathbf{r}, E, \\Omega) = \\frac{2\\gamma^3}{16\\pi^2 \\sqrt{\\pi} T^{3/2} \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]} E^{1/2} \\exp\\left(-\\gamma r - \\frac{E}{T}\\right)\n$$\n$$\np(\\mathbf{r}, E, \\Omega) = \\frac{\\gamma^3}{8\\pi^{5/2} T^{3/2} \\left[ 2 - (\\gamma^2 R^2 + 2\\gamma R + 2)\\exp(-\\gamma R) \\right]} E^{1/2} \\exp\\left(-\\gamma r - \\frac{E}{T}\\right)\n$$\nThis expression is the analog joint PDF for the source state $(\\mathbf{r}, E, \\Omega)$ over the domain $V \\times [0, \\infty) \\times \\mathbb{S}^2$, where $r=|\\mathbf{r}|$. It is expressed purely in terms of the state variables and the given parameters.",
            "answer": "$$\n\\boxed{\\frac{\\gamma^3 E^{1/2} \\exp\\left(-\\gamma r - \\frac{E}{T}\\right)}{8 \\pi^{5/2} T^{3/2} \\left[ 2 - \\left(\\gamma^2 R^2 + 2\\gamma R + 2\\right)\\exp(-\\gamma R) \\right]}}\n$$"
        },
        {
            "introduction": "With the analog source defined, we now turn to the core purpose of source biasing: reducing statistical variance to improve simulation efficiency for rare events. This practice  presents a classic deep-penetration problem where particles must traverse a thick, absorbing slab to be tallied. You will compare an analog estimator with an importance-sampled one, and, most importantly, derive the optimal biasing parameter $q^{\\star}$ that minimizes the variance, providing a concrete demonstration of the power and mathematical basis of the exponential transform method.",
            "id": "4250382",
            "problem": "Consider a monoenergetic neutral particle beam incident normally on a homogeneous, purely absorbing slab of thickness $L$ and macroscopic total cross section $\\Sigma_{t}$. Assume no scattering and no internal sources. Each history launches a particle at the entry face $x=0$ with unit weight and straight-line motion in the $+x$ direction. The free-path distance to the first collision, $s$, is distributed according to the exponential attenuation law implied by the Beer–Lambert law, i.e., the analog probability density function (PDF) is $f(s) = \\Sigma_{t} \\exp(-\\Sigma_{t} s)$ for $s \\geq 0$. A surface flux tally at the exit face $x=L$ registers a contribution if and only if the sampled $s$ exceeds $L$ (i.e., the particle exits without colliding), and the tally contribution is otherwise zero.\n\nYou will compare the variance of two unbiased Monte Carlo (MC) estimators for the exit-face flux tally and then derive the biasing parameter that minimizes the variance of the importance-sampled estimator:\n- The analog estimator, which scores $Y = \\mathbb{I}[s \\geq L]$ under $f(s)$, where $\\mathbb{I}[\\cdot]$ denotes the indicator function.\n- The exponentially transformed importance-sampled estimator, which samples $s$ from the biased PDF $g_{q}(s) = (\\Sigma_{t} - q)\\exp\\!\\big[-(\\Sigma_{t} - q)s\\big]$ for $s \\geq 0$ with $q$ satisfying $0  q  \\Sigma_{t}$, and scores $X_{q} = w_{q}(s)\\,\\mathbb{I}[s \\geq L]$, where the weight is the likelihood ratio $w_{q}(s) = \\dfrac{f(s)}{g_{q}(s)} = \\dfrac{\\Sigma_{t}}{\\Sigma_{t} - q}\\exp(-q s)$.\n\nTasks:\n- Starting from the exponential attenuation law and the definitions above, derive the mean and variance of the analog estimator $Y$ for the exit-face tally.\n- Prove the unbiasedness of the importance-sampled estimator $X_{q}$ and derive its variance as a function of $q$, $\\Sigma_{t}$, and $L$.\n- By direct minimization with respect to $q$ over the admissible interval $0  q  \\Sigma_{t}$, derive the condition on $q$ that minimizes the variance of $X_{q}$, and express the optimal value as a dimensionless ratio $r^{\\star} = q^{\\star}/\\Sigma_{t}$ in terms of the dimensionless optical thickness $\\tau = \\Sigma_{t} L$.\n\nAnswer specification:\n- Report the final answer as the single closed-form analytic expression for $r^{\\star}$ in terms of $\\tau$.\n- No numerical evaluation is required, and no physical units should be included in the final answer.",
            "solution": "The problem is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Physical System**: A monoenergetic neutral particle beam is incident normally on a homogeneous, purely absorbing slab.\n- **Slab Properties**: Thickness $L$, macroscopic total cross section $\\Sigma_{t}$.\n- **Particle Dynamics**: Initial position $x=0$, initial direction is $+x$, straight-line motion. No scattering, no internal sources.\n- **Analog Sampling**: The free-path distance $s$ is sampled from the probability density function (PDF) $f(s) = \\Sigma_{t} \\exp(-\\Sigma_{t} s)$ for $s \\geq 0$.\n- **Analog Estimator**: $Y = \\mathbb{I}[s \\geq L]$, where $\\mathbb{I}[\\cdot]$ is the indicator function for the event that the particle exits at $x=L$.\n- **Biased Sampling**: An alternative sampling method uses the biased PDF $g_{q}(s) = (\\Sigma_{t} - q)\\exp\\!\\big[-(\\Sigma_{t} - q)s\\big]$ for $s \\geq 0$.\n- **Biasing Parameter**: $q$ is a parameter satisfying the constraint $0  q  \\Sigma_{t}$.\n- **Importance-Sampled Estimator**: $X_{q} = w_{q}(s)\\,\\mathbb{I}[s \\geq L]$.\n- **Statistical Weight**: The weight is the likelihood ratio $w_{q}(s) = \\dfrac{f(s)}{g_{q}(s)} = \\dfrac{\\Sigma_{t}}{\\Sigma_{t} - q}\\exp(-q s)$.\n- **Tasks**:\n    1.  Derive the mean and variance of the analog estimator $Y$.\n    2.  Prove the unbiasedness of the importance-sampled estimator $X_{q}$ and derive its variance as a function of $q$, $\\Sigma_{t}$, and $L$.\n    3.  Find the optimal value of the biasing parameter, $q^{\\star}$, that minimizes the variance of $X_{q}$. Express the result as a dimensionless ratio $r^{\\star} = q^{\\star}/\\Sigma_{t}$ in terms of the dimensionless optical thickness $\\tau = \\Sigma_{t} L$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Groundedness**: The problem is fundamentally sound. It describes a classic application of Monte Carlo variance reduction techniques (specifically, the exponential transform or path stretching) to a canonical problem in particle transport theory. The exponential attenuation law, the definitions of the estimators, and the weight function are all standard and correct within this field.\n- **Well-Posedness**: The problem is well-posed. The objective is clearly stated: derive expressions for mean and variance, and then minimize the variance of the biased estimator. All necessary functions and parameters ($f(s)$, $g_q(s)$, $Y$, $X_q$) are precisely defined. The constraint $0  q  \\Sigma_{t}$ ensures that the biased PDF $g_q(s)$ is well-defined (i.e., its pre-exponential factor $\\Sigma_t - q$ is positive) and normalizable.\n- **Objectivity**: The problem is stated in objective, mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically sound, well-posed, and objective. A complete solution will be provided.\n\n### Solution Derivation\n\nThe analysis proceeds by addressing each of the three tasks in order.\n\n**1. Mean and Variance of the Analog Estimator $Y$**\n\nThe analog estimator is $Y = \\mathbb{I}[s \\geq L]$, where the random variable $s$ is drawn from the analog PDF $f(s) = \\Sigma_{t} \\exp(-\\Sigma_{t} s)$. $Y$ is a Bernoulli random variable.\n\nThe mean (expected value) of $Y$ is the probability of the event $\\{s \\geq L\\}$.\n$$E[Y] = P(s \\geq L) = \\int_L^{\\infty} f(s) \\, ds = \\int_L^{\\infty} \\Sigma_{t} \\exp(-\\Sigma_{t} s) \\, ds$$\n$$E[Y] = \\left[ -\\exp(-\\Sigma_{t} s) \\right]_L^{\\infty} = 0 - (-\\exp(-\\Sigma_t L)) = \\exp(-\\Sigma_t L)$$\nThis is the physical probability of transmission without collision, so the estimator is unbiased.\n\nThe variance of $Y$ is given by $\\text{Var}(Y) = E[Y^2] - (E[Y])^2$. Since $Y$ is an indicator function, its values are either $0$ or $1$, which implies $Y^2 = Y$.\nTherefore, $E[Y^2] = E[Y] = \\exp(-\\Sigma_t L)$.\nThe variance is:\n$$\\text{Var}(Y) = \\exp(-\\Sigma_t L) - (\\exp(-\\Sigma_t L))^2 = \\exp(-\\Sigma_t L) \\left(1 - \\exp(-\\Sigma_t L)\\right)$$\n\n**2. Unbiasedness and Variance of the Importance-Sampled Estimator $X_q$**\n\nThe importance-sampled estimator is $X_q = w_q(s) \\mathbb{I}[s \\geq L]$, where $s$ is now sampled from the biased PDF $g_q(s)$. The expectation is taken with respect to $g_q(s)$, denoted by $E_q[\\cdot]$.\n\nTo prove unbiasedness, we must show that $E_q[X_q] = E[Y]$.\n$$E_q[X_q] = \\int_0^{\\infty} X_q(s) g_q(s) \\, ds = \\int_0^{\\infty} w_q(s) \\mathbb{I}[s \\geq L] g_q(s) \\, ds$$\nThe indicator function restricts the lower limit of integration to $L$. Substituting $w_q(s) = f(s)/g_q(s)$:\n$$E_q[X_q] = \\int_L^{\\infty} \\frac{f(s)}{g_q(s)} g_q(s) \\, ds = \\int_L^{\\infty} f(s) \\, ds = E[Y] = \\exp(-\\Sigma_t L)$$\nThe estimator $X_q$ is therefore unbiased for any valid choice of $q$.\n\nThe variance of $X_q$ is $\\text{Var}_q(X_q) = E_q[X_q^2] - (E_q[X_q])^2$. The second term is $(E[Y])^2 = \\exp(-2\\Sigma_t L)$. We need to compute the second moment, $E_q[X_q^2]$.\n$$E_q[X_q^2] = \\int_0^{\\infty} (w_q(s) \\mathbb{I}[s \\geq L])^2 g_q(s) \\, ds = \\int_L^{\\infty} w_q^2(s) g_q(s) \\, ds$$\nLet's analyze the integrand:\n$$w_q^2(s) g_q(s) = \\left(\\frac{f(s)}{g_q(s)}\\right)^2 g_q(s) = \\frac{f^2(s)}{g_q(s)}$$\nSubstituting the explicit forms of the PDFs:\n$$f^2(s) = \\Sigma_t^2 \\exp(-2\\Sigma_t s)$$\n$$g_q(s) = (\\Sigma_t - q) \\exp(-(\\Sigma_t - q)s)$$\nThe integrand becomes:\n$$\\frac{f^2(s)}{g_q(s)} = \\frac{\\Sigma_t^2 \\exp(-2\\Sigma_t s)}{(\\Sigma_t - q) \\exp(-(\\Sigma_t - q)s)} = \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\exp(-(2\\Sigma_t - (\\Sigma_t - q))s) = \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\exp(-(\\Sigma_t + q)s)$$\nNow, we integrate this expression from $L$ to $\\infty$:\n$$E_q[X_q^2] = \\int_L^{\\infty} \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\exp(-(\\Sigma_t + q)s) \\, ds = \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\left[ \\frac{\\exp(-(\\Sigma_t + q)s)}{-(\\Sigma_t + q)} \\right]_L^{\\infty}$$\n$$E_q[X_q^2] = \\frac{\\Sigma_t^2}{\\Sigma_t - q} \\left( 0 - \\frac{\\exp(-(\\Sigma_t + q)L)}{-(\\Sigma_t + q)} \\right) = \\frac{\\Sigma_t^2}{(\\Sigma_t - q)(\\Sigma_t + q)} \\exp(-(\\Sigma_t + q)L)$$\n$$E_q[X_q^2] = \\frac{\\Sigma_t^2}{\\Sigma_t^2 - q^2} \\exp(-(\\Sigma_t + q)L)$$\nThe variance is then:\n$$\\text{Var}_q(X_q) = \\frac{\\Sigma_t^2}{\\Sigma_t^2 - q^2} \\exp(-(\\Sigma_t + q)L) - \\exp(-2\\Sigma_t L)$$\n\n**3. Minimization of the Variance of $X_q$**\n\nTo find the optimal parameter $q^{\\star}$ that minimizes $\\text{Var}_q(X_q)$, we need to find the value of $q$ in the interval $(0, \\Sigma_t)$ for which $\\frac{d}{dq}\\text{Var}_q(X_q) = 0$. Since the term $(E_q[X_q])^2 = \\exp(-2\\Sigma_t L)$ is independent of $q$, minimizing the variance is equivalent to minimizing the second moment $E_q[X_q^2]$.\nLet $V_2(q) = E_q[X_q^2] = \\frac{\\Sigma_t^2}{\\Sigma_t^2 - q^2} \\exp(-(\\Sigma_t + q)L)$.\nTo simplify differentiation, we can minimize its natural logarithm, $\\ln(V_2(q))$, since the logarithm is a monotonically increasing function.\n$$\\ln(V_2(q)) = \\ln(\\Sigma_t^2) - \\ln(\\Sigma_t^2 - q^2) - (\\Sigma_t + q)L$$\nDifferentiating with respect to $q$:\n$$\\frac{d}{dq}\\ln(V_2(q)) = 0 - \\frac{-2q}{\\Sigma_t^2 - q^2} - L = \\frac{2q}{\\Sigma_t^2 - q^2} - L$$\nSetting the derivative to zero to find the extremum:\n$$\\frac{2q}{\\Sigma_t^2 - q^2} = L$$\n$$2q = L(\\Sigma_t^2 - q^2) \\implies Lq^2 + 2q - L\\Sigma_t^2 = 0$$\nThis is a quadratic equation in $q$. We solve for $q$ using the quadratic formula $q = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ with $a=L$, $b=2$, and $c=-L\\Sigma_t^2$:\n$$q = \\frac{-2 \\pm \\sqrt{2^2 - 4(L)(-L\\Sigma_t^2)}}{2L} = \\frac{-2 \\pm \\sqrt{4 + 4L^2\\Sigma_t^2}}{2L} = \\frac{-1 \\pm \\sqrt{1 + L^2\\Sigma_t^2}}{L}$$\nWe have two potential solutions for $q$. We must choose the one that satisfies the given constraint $0  q  \\Sigma_t$.\nThe solution $q = \\frac{-1 - \\sqrt{1 + L^2\\Sigma_t^2}}{L}$ is always negative since $L0$, so it is discarded.\nThe other solution is $q^{\\star} = \\frac{-1 + \\sqrt{1 + L^2\\Sigma_t^2}}{L}$. Since $\\sqrt{1+L^2\\Sigma_t^2}  1$, this value for $q^{\\star}$ is positive. We also must verify that $q^{\\star}  \\Sigma_t$:\n$$\\frac{-1 + \\sqrt{1 + L^2\\Sigma_t^2}}{L}  \\Sigma_t \\implies -1 + \\sqrt{1 + L^2\\Sigma_t^2}  L\\Sigma_t \\implies \\sqrt{1 + L^2\\Sigma_t^2}  1 + L\\Sigma_t$$\nSince both sides are positive for $L0$ and $\\Sigma_t0$, we can square them without changing the inequality:\n$$1 + L^2\\Sigma_t^2  (1 + L\\Sigma_t)^2 = 1 + 2L\\Sigma_t + L^2\\Sigma_t^2$$\n$$0  2L\\Sigma_t$$\nThis inequality is always true, so $q^{\\star}$ is within the valid range $(0, \\Sigma_t)$. The second derivative of $\\ln(V_2(q))$ is $\\frac{2(\\Sigma_t^2+q^2)}{(\\Sigma_t^2-q^2)^2}$, which is positive for $q \\in (0, \\Sigma_t)$, confirming that $q^{\\star}$ corresponds to a minimum.\n\nThe final step is to express the result as the dimensionless ratio $r^{\\star} = q^{\\star}/\\Sigma_t$ in terms of the optical thickness $\\tau = \\Sigma_t L$.\n$$r^{\\star} = \\frac{q^{\\star}}{\\Sigma_t} = \\frac{1}{\\Sigma_t} \\left( \\frac{-1 + \\sqrt{1 + L^2\\Sigma_t^2}}{L} \\right) = \\frac{-1 + \\sqrt{1 + (L\\Sigma_t)^2}}{L\\Sigma_t}$$\nSubstituting $\\tau = \\Sigma_t L$:\n$$r^{\\star} = \\frac{-1 + \\sqrt{1 + \\tau^2}}{\\tau}$$\nThis is the final expression for the optimal biasing parameter ratio.",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{1+\\tau^2}-1}{\\tau}}\n$$"
        },
        {
            "introduction": "While powerful, biasing techniques are not without their limits; more aggressive biasing is not always better. This exercise  explores the crucial concept of diminishing returns by analyzing the Figure of Merit (FOM), the ultimate measure of simulation efficiency that balances variance with computational cost. By deriving the FOM for a simple two-region source model, you will quantitatively see how overly aggressive biasing can degrade performance, teaching the critical skill of evaluating the overall efficiency of a variance reduction strategy.",
            "id": "4250409",
            "problem": "Consider a simplified but scientifically consistent model of source biasing in Monte Carlo (MC) nuclear reactor simulation to estimate a scalar response due to source particles emitted from two disjoint phase-space regions. Let the original source probability distribution place probability $p_A$ on region A and $p_B$ on region B, with $p_A + p_B = 1$ and $p_A, p_B \\in (0,1)$. A single source history sampled from region $i \\in \\{A,B\\}$ contributes a deterministic nonnegative score $g_i$ to the tally, representing the expected contribution of a history launched from that region to the response of interest. Let the true mean response be $\\mu = p_A g_A + p_B g_B$.\n\nWe introduce source biasing via a mixture importance sampling construction indexed by an intensity parameter $b \\in [0,1]$. The biased source distribution $q(\\cdot \\, ; b)$ redistributes probability mass away from the original source and toward region $A$ as\n$$\nq_{A}(b) = p_{A} + b \\left(1 - p_{A}\\right), \\qquad q_{B}(b) = p_{B} - b p_{B} = p_{B} \\left(1 - b\\right),\n$$\nwhich satisfies $q_A(b) + q_B(b) = 1$ for all $b \\in [0,1]$. Source particles are sampled from $q(\\cdot \\, ; b)$ and reweighted by the standard importance factor $w_{i}(b) = \\frac{p_{i}}{q_{i}(b)}$ to preserve unbiasedness of the estimator. Assume that each biased source history incurs a dimensionless computational cost $C(b)$ modeled as\n$$\nC(b) = C_{0} \\left(1 + \\gamma b\\right),\n$$\nwhere $C_0  0$ is the baseline cost per source history under the original source and $\\gamma \\geq 0$ quantifies overhead from implementing the biased sampling mechanism. Computational time is measured in units of the baseline time per history, so the Figure of Merit (FOM) defined below is dimensionless.\n\nStarting from the definitions of the unbiased weighted estimator, its variance under the biased distribution, and the standard MC efficiency metric Figure of Merit (FOM), which is defined as the inverse of the product of squared relative error and computational time, derive a closed-form analytic expression for the efficiency curve $\\mathrm{FOM}(b)$ as a function of $b$, $p_A$, $p_B$, $g_A$, $g_B$, $C_0$, and $\\gamma$. Your derivation must start from first principles of importance sampling and the definition of variance under a change of measure, and must show explicitly how diminishing returns arise as $b \\to 1$.\n\nExpress your final answer as a single simplified analytic expression for $\\mathrm{FOM}(b)$ in terms of $b$, $p_A$, $p_B$, $g_A$, $g_B$, $C_0$, and $\\gamma$. No numerical evaluation is required. The final answer must be dimensionless. No rounding is required.",
            "solution": "We begin from the fundamental framework of importance sampling in Monte Carlo (MC) estimation. Let the original source distribution be a probability mass function over the two regions, $p_{A}$ and $p_{B}$, and let the biased distribution be $q_{A}(b)$ and $q_{B}(b)$ with $q_{A}(b) + q_{B}(b) = 1$. A single-history estimator that preserves unbiasedness under the biased distribution uses an importance weight $w_{i}(b) = \\frac{p_{i}}{q_{i}(b)}$ multiplying the score $g_{i}$, because the expectation under the biased distribution satisfies\n$$\n\\mathbb{E}_{q}\\!\\left[w(b) \\, g\\right] = \\sum_{i \\in \\{A,B\\}} q_{i}(b) \\, \\frac{p_{i}}{q_{i}(b)} \\, g_{i} = \\sum_{i \\in \\{A,B\\}} p_{i} g_{i} = \\mu.\n$$\nTherefore, the estimator remains unbiased for all $b \\in [0,1]$. The variance of the single-history weighted estimator under the biased distribution is given by the second moment under $q$ minus the square of the mean:\n$$\n\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right] = \\mathbb{E}_{q}\\!\\left[\\left(w(b) \\, g\\right)^{2}\\right] - \\mu^{2}.\n$$\nSince $w(b) \\, g$ takes the values $\\frac{p_{A}}{q_{A}(b)} g_{A}$ with probability $q_{A}(b)$ and $\\frac{p_{B}}{q_{B}(b)} g_{B}$ with probability $q_{B}(b)$, the second moment evaluates to\n$$\n\\mathbb{E}_{q}\\!\\left[\\left(w(b) \\, g\\right)^{2}\\right]\n= q_{A}(b) \\left(\\frac{p_{A}}{q_{A}(b)} g_{A}\\right)^{2} + q_{B}(b) \\left(\\frac{p_{B}}{q_{B}(b)} g_{B}\\right)^{2}\n= \\frac{p_{A}^{2} g_{A}^{2}}{q_{A}(b)} + \\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)}.\n$$\nThus, the single-history variance under the biased distribution is\n$$\n\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right] = \\frac{p_{A}^{2} g_{A}^{2}}{q_{A}(b)} + \\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)} - \\mu^{2},\n$$\nwith $\\mu = p_{A} g_{A} + p_{B} g_{B}$.\n\nThe Figure of Merit (FOM) for MC, defined as the inverse of the product of squared relative error and computational time, can be expressed in terms of per-history quantities. For $N$ independent histories, the sample mean has variance $\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right] / N$, so the squared relative error is\n$$\nR^{2}(b) = \\frac{\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right]}{N \\, \\mu^{2}}.\n$$\nIf the average computational cost per history is $C(b)$ in units of baseline time per history, then the total computational time is $T = N \\, C(b)$. The FOM is\n$$\n\\mathrm{FOM}(b) = \\frac{1}{R^{2}(b) \\, T} = \\frac{1}{\\left(\\frac{\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right]}{N \\, \\mu^{2}}\\right) \\, N \\, C(b)} = \\frac{\\mu^{2}}{\\mathrm{Var}_{q}\\!\\left[w(b) \\, g\\right] \\, C(b)}.\n$$\nConsequently, $\\mathrm{FOM}(b)$ is independent of $N$ and depends only on the single-history variance and the per-history cost under the biased distribution.\n\nWe substitute the expressions derived above. The cost model is taken as\n$$\nC(b) = C_{0} \\left(1 + \\gamma b\\right),\n$$\nwhere $C_{0}  0$ and $\\gamma \\geq 0$. The biased probabilities are\n$$\nq_{A}(b) = p_{A} + b \\left(1 - p_{A}\\right), \\qquad q_{B}(b) = p_{B} \\left(1 - b\\right).\n$$\nCollecting these yields\n$$\n\\mathrm{FOM}(b) = \\frac{\\mu^{2}}{C_{0} \\left(1 + \\gamma b\\right)} \\left[ \\frac{1}{\\frac{p_{A}^{2} g_{A}^{2}}{q_{A}(b)} + \\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)} - \\mu^{2}} \\right],\n$$\nwith $\\mu = p_{A} g_{A} + p_{B} g_{B}$.\n\nTo make the dependence explicit, we replace $q_{A}(b)$ and $q_{B}(b)$:\n$$\n\\mathrm{FOM}(b) = \\frac{\\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2}}{C_{0} \\left(1 + \\gamma b\\right) \\left( \\frac{p_{A}^{2} g_{A}^{2}}{p_{A} + b \\left(1 - p_{A}\\right)} + \\frac{p_{B}^{2} g_{B}^{2}}{p_{B} \\left(1 - b\\right)} - \\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2} \\right)}.\n$$\n\nWe now analyze diminishing returns. As $b \\to 0$, $q_{A}(b) \\to p_{A}$ and $q_{B}(b) \\to p_{B}$, and the variance tends to\n$$\n\\mathrm{Var}_{q}\\!\\left[w(0) \\, g\\right] = \\frac{p_{A}^{2} g_{A}^{2}}{p_{A}} + \\frac{p_{B}^{2} g_{B}^{2}}{p_{B}} - \\mu^{2} = p_{A} g_{A}^{2} + p_{B} g_{B}^{2} - \\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2},\n$$\nthe original source variance, and the cost tends to $C(0) = C_{0}$. As $b$ increases from zero, the term $\\frac{p_{A}^{2} g_{A}^{2}}{q_{A}(b)}$ decreases because $q_{A}(b)$ increases, reflecting variance reduction due to more frequent sampling from the important region $A$. Simultaneously, the term $\\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)}$ increases because $q_{B}(b) = p_{B} (1 - b)$ decreases, penalizing neglect of region $B$. In the limit $b \\to 1^{-}$,\n$$\nq_{B}(b) \\to 0^{+} \\quad \\Rightarrow \\quad \\frac{p_{B}^{2} g_{B}^{2}}{q_{B}(b)} \\to \\infty,\n$$\nso the variance diverges unless $g_{B} = 0$. Additionally, $C(b)$ increases linearly with $b$. These competing effects produce diminishing returns: initial increases in $b$ reduce the dominant variance term from region $A$, but aggressive biasing eventually causes the region $B$ weight term to dominate and blow up the variance while also increasing cost, thereby reducing $\\mathrm{FOM}(b)$ for large $b$.\n\nTherefore, the efficiency curve $\\mathrm{FOM}(b)$, derived directly from the foundational definitions of unbiased weighted estimators, variance under a change of measure, and MC efficiency, is the explicit analytic function\n$$\n\\mathrm{FOM}(b) = \\frac{\\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2}}{C_{0} \\left(1 + \\gamma b\\right) \\left( \\frac{p_{A}^{2} g_{A}^{2}}{p_{A} + b \\left(1 - p_{A}\\right)} + \\frac{p_{B}^{2} g_{B}^{2}}{p_{B} \\left(1 - b\\right)} - \\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2} \\right)}.\n$$\nThis curve explicitly displays the diminishing returns in variance reduction as bias intensity $b$ becomes increasingly aggressive.",
            "answer": "$$\\boxed{\\frac{\\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2}}{C_{0} \\left(1 + \\gamma b\\right) \\left( \\frac{p_{A}^{2} g_{A}^{2}}{p_{A} + b \\left(1 - p_{A}\\right)} + \\frac{p_{B}^{2} g_{B}^{2}}{p_{B} \\left(1 - b\\right)} - \\left(p_{A} g_{A} + p_{B} g_{B}\\right)^{2} \\right)}}$$"
        }
    ]
}