{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundational understanding of weight windows, it is helpful to first strip away the stochastic nature of Monte Carlo simulations and observe the core mechanics in a simplified, deterministic setting. This exercise provides exactly that by modeling a particle's journey through a one-dimensional slab where an adjoint flux map dictates its importance . By calculating the evolution of particle weight and multiplicity step-by-step, you will gain a concrete intuition for how an importance function directly translates into splitting and an overall variance reduction factor.",
            "id": "3535413",
            "problem": "A monoenergetic photon transport problem through a one-dimensional slab of thickness $L$ is considered. Photons are emitted at the source surface ($x=0$) and a detector tallies the transmitted particle current at the far surface ($x=L$). The medium is homogeneous with macroscopic total cross section $\\Sigma_t$ (in $\\mathrm{m}^{-1}$), and deep-penetration transmission is modeled by an absorbing process without scattering. An adjoint flux map $\\psi^\\dagger(x)$, discretized on a set of $N+1$ positions $x_0=0, x_1, \\dots, x_N=L$, is provided. The goal is to design weight-window bounds using this adjoint map and predict, from first principles, the variance reduction factor of the weight-window method relative to analog Monte Carlo.\n\nFundamental base:\n- The rate of change of expectation for a random variable is governed by the Kolmogorov forward equation, which for particle transport in matter reduces, under suitable assumptions, to the linear Boltzmann transport equation. Its adjoint form defines the adjoint flux $\\psi^\\dagger(\\vec{r}, E, \\Omega)$ that quantifies the importance of particles at phase-space point $(\\vec{r}, E, \\Omega)$ to a specified response functional.\n- In one-dimensional slab geometry with monoenergetic photons and purely absorbing medium, the probability for a particle to be transmitted analogly is $P = \\exp(-\\Sigma_t L)$.\n- In Monte Carlo (MC) radiation transport, the weight-window (WW) method is a variance reduction (VR) technique based on importance sampling: given a scalarized importance function $I(x)$, the target particle weight is $W_t(x) \\propto 1/I(x)$, so that particle weights are kept near a prescribed band via splitting when $w > W_u(x)$ and Russian roulette when $w  W_l(x)$, where $W_u(x)$ and $W_l(x)$ are the upper and lower bounds of the window. This heuristic seeks the zero-variance limit obtained when the exact adjoint solution is used to define importance.\n\nTask:\n1. Let the scalar importance be $I(x_i) = \\psi^\\dagger(x_i)$ for the discrete positions $x_i$. Define the target weight at position $x_i$ by $W_t(x_i) = C / I(x_i)$, where $C$ is a normalization constant chosen such that $W_t(x_0) = 1$ (i.e., $C = I(x_0)$).\n2. Given a window half-width parameter $\\alpha \\geq 1$, define the weight-window bounds at position $x_i$ as $W_u(x_i) = \\alpha W_t(x_i)$ and $W_l(x_i) = W_t(x_i) / \\alpha$.\n3. Consider a particle history that traverses the slab from $x_0$ to $x_N$ in order. The particle begins with weight $w_0 = 1$. At each $x_i$ for $i=1,2,\\dots,N$, perform the following deterministic approximation of the standard WW logic:\n   - If $w_{i-1} > W_u(x_i)$, split into $n_i = \\lceil w_{i-1} / W_t(x_i) \\rceil$ daughters, each with new weight $w_i = w_{i-1} / n_i$. Accumulate total split multiplicity $M \\leftarrow M \\cdot n_i$.\n   - Else if $w_{i-1}  W_l(x_i)$, apply expected Russian roulette adjustment: set $w_i = W_t(x_i)$. Do not change $M$ (roulette does not increase expected multiplicity).\n   - Else, set $w_i = w_{i-1}$ and do not change $M$.\n   Begin with $M=1$.\n4. Model the analog transmitted-current score per history as a Bernoulli random variable $X$ with $\\mathbb{P}(X=1) = P = \\exp(-\\Sigma_t L)$. Under WW splitting, the score becomes $S = \\sum_{j=1}^{M} (1/M) X_j$ where $X_j$ are independent and identically distributed Bernoulli variables with the same transmission probability $P$. Using the variance of a sum of independent variables, deduce that the variance under WW is $\\mathrm{Var}(S) = P(1-P)/M$, while the analog variance is $\\mathrm{Var}(X) = P(1-P)$. Therefore, the variance reduction factor relative to analog is $\\mathrm{VRF} = \\mathrm{Var}(X) / \\mathrm{Var}(S) = M$.\n5. Implement a program that, for each test case, constructs $W_t(x_i)$, the window bounds $\\{W_l(x_i), W_u(x_i)\\}$, evolves the weight and multiplicity $M$ across the positions using the above deterministic WW approximation, and outputs the predicted variance reduction factor $\\mathrm{VRF} = M$.\n\nPhysical units:\n- Use $\\Sigma_t$ in $\\mathrm{m}^{-1}$ and $L$ in $\\mathrm{m}$. The output $\\mathrm{VRF}$ is dimensionless.\n\nAngle unit:\n- Not applicable because the problem is one-dimensional.\n\nPercentages:\n- Any probability values must be treated as decimals, not percentages.\n\nTest suite:\n- Case $1$ (happy path): $\\Sigma_t = 1.0\\,\\mathrm{m}^{-1}$, $L = 1.0\\,\\mathrm{m}$, $\\alpha = 2.0$, and $\\psi^\\dagger$ at positions $[x_0, x_1, x_2, x_3, x_4, x_5]$ is $[1.0, 1.5, 2.5, 4.0, 6.0, 9.0]$.\n- Case $2$ (boundary condition, no splitting): $\\Sigma_t = 0.8\\,\\mathrm{m}^{-1}$, $L = 1.0\\,\\mathrm{m}$, $\\alpha = 10.0$, and $\\psi^\\dagger$ is $[1.0, 1.0, 1.0, 1.0]$.\n- Case $3$ (strong deep penetration): $\\Sigma_t = 2.0\\,\\mathrm{m}^{-1}$, $L = 2.0\\,\\mathrm{m}$, $\\alpha = 1.5$, and $\\psi^\\dagger$ is $[1.0, 3.0, 9.0, 27.0]$.\n- Case $4$ (oscillatory importance causing roulette): $\\Sigma_t = 0.5\\,\\mathrm{m}^{-1}$, $L = 1.0\\,\\mathrm{m}$, $\\alpha = 1.2$, and $\\psi^\\dagger$ is $[1.0, 0.8, 1.2, 0.9, 1.5]$.\n\nAnswer specification:\n- For each test case, compute the variance reduction factor $\\mathrm{VRF}$ as a float.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\texttt{[result1,result2,result3,result4]}$).",
            "solution": "The problem requires the calculation of a predicted variance reduction factor (VRF) for a monoenergetic photon transport problem in a one-dimensional slab. The VRF is determined by implementing a deterministic, simplified version of the weight-window variance reduction technique. The calculation is based on a provided discrete adjoint flux map, $\\psi^\\dagger(x_i)$, which serves as an importance function.\n\nThe fundamental principle is that the particle weight should be inversely proportional to the importance of its location. This keeps the product of weight and importance, a measure of the expected future contribution to the score, roughly constant. The importance function $I(x)$ is given by the adjoint flux, $I(x_i) = \\psi^\\dagger(x_i)$. The target weight at position $x_i$, denoted $W_t(x_i)$, is defined as $W_t(x_i) = C / I(x_i)$, where $C$ is a normalization constant. The problem specifies that the source particles at $x_0 = 0$ have a weight of $1$, so we set $W_t(x_0) = 1$. This implies that $C = I(x_0) = \\psi^\\dagger(x_0)$, yielding the formula for the target weight at any position $x_i$:\n$$\nW_t(x_i) = \\frac{\\psi^\\dagger(x_0)}{\\psi^\\dagger(x_i)}\n$$\n\nThe weight-window method uses this target weight to define a range of acceptable weights, $[W_l(x_i), W_u(x_i)]$, at each position $x_i$. These lower and upper bounds are determined by a window half-width parameter $\\alpha \\geq 1$:\n$$\nW_u(x_i) = \\alpha W_t(x_i)\n$$\n$$\nW_l(x_i) = \\frac{W_t(x_i)}{\\alpha}\n$$\n\nThe algorithm simulates the evolution of a particle's weight as it traverses the slab across the discrete positions $x_0, x_1, \\dots, x_N$. It starts with an initial weight $w_0 = 1$ and an initial total multiplicity $M=1$. At each subsequent position $x_i$ (for $i=1, \\dots, N$), the incoming weight $w_{i-1}$ is compared to the window $[W_l(x_i), W_u(x_i)]$. The problem defines a deterministic set of rules:\n$1$. If the weight $w_{i-1}$ is above the window, $w_{i-1} > W_u(x_i)$, the particle is split. This is to increase the number of samples in a region where each particle is deemed more important (as indicated by a lower target weight). The number of split daughters, $n_i$, is calculated to bring the new weight close to the target weight:\n$$\nn_i = \\left\\lceil \\frac{w_{i-1}}{W_t(x_i)} \\right\\rceil\n$$\nThe new weight for each daughter particle becomes $w_i = w_{i-1} / n_i$. The total particle multiplicity is updated by multiplying it by the split factor: $M \\leftarrow M \\cdot n_i$.\n\n$2$. If the weight $w_{i-1}$ is below the window, $w_{i-1}  W_l(x_i)$, an expected value approximation of Russian roulette is applied. Instead of probabilistically terminating the particle, its weight is deterministically reset to the target weight, $w_i = W_t(x_i)$. In this simplified model, Russian roulette does not change the total multiplicity $M$, as it is a process that reduces particle population on average.\n\n$3$. If the weight $w_{i-1}$ is within the window, $W_l(x_i) \\leq w_{i-1} \\leq W_u(x_i)$, no action is taken. The weight is carried over, $w_i = w_{i-1}$, and the multiplicity $M$ remains unchanged.\n\nThe problem posits a model where the variance reduction factor is directly equal to the total multiplicity $M$ accumulated by the end of the particle's traversal from $x_0$ to $x_N=L$. This is based on the heuristic that splitting a particle into $M$ independent daughters is equivalent to running $M$ independent simulations, thereby reducing the variance of the mean estimate by a factor of $M$.\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(\\text{Analog})}{\\mathrm{Var}(\\text{WW})} = M\n$$\n\nThe solution algorithm proceeds by first calculating the target weights $W_t(x_i)$ for all $i \\in \\{0, 1, \\dots, N\\}$ using the given $\\psi^\\dagger$ values. Then, it initializes $w=1.0$ and $M=1$ and iterates from $i=1$ to $N$, applying the deterministic splitting and roulette logic at each step to update $w$ and $M$. The final value of $M$ after the last step at $x_N$ is the computed VRF.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    It iterates through each test case, calculates the VRF, and prints\n    the results in the specified format.\n    \"\"\"\n    # Test suite as defined in the problem statement.\n    # Each tuple contains: (Sigma_t, L, alpha, psi_dagger_list)\n    test_cases = [\n        (1.0, 1.0, 2.0, [1.0, 1.5, 2.5, 4.0, 6.0, 9.0]),\n        (0.8, 1.0, 10.0, [1.0, 1.0, 1.0, 1.0]),\n        (2.0, 2.0, 1.5, [1.0, 3.0, 9.0, 27.0]),\n        (0.5, 1.0, 1.2, [1.0, 0.8, 1.2, 0.9, 1.5]),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current case.\n        # Sigma_t and L are not directly used in the VRF calculation as defined,\n        # but are part of the problem's physical context.\n        _sigma_t, _L, alpha, psi_dagger = case\n        \n        # Initialize simulation state for a single particle history\n        current_w = 1.0\n        # M is the total multiplicity, which is the predicted VRF.\n        # It's an integer value, as it comes from products of integers.\n        M = 1\n\n        # The importance at the source (x=0) is used for normalization.\n        psi_dagger_0 = psi_dagger[0]\n        \n        # Calculate target weights W_t(x_i) = psi_dagger(x_0) / psi_dagger(x_i)\n        target_weights = [psi_dagger_0 / p_d for p_d in psi_dagger]\n        \n        # Iterate through the slab positions from x_1 to x_N\n        # The loop starts at index 1 because x_0 is the source position.\n        for i in range(1, len(psi_dagger)):\n            # Define the weight-window for the current position x_i\n            w_t_i = target_weights[i]\n            w_u_i = alpha * w_t_i\n            w_l_i = w_t_i / alpha\n            \n            # Apply the deterministic weight-window logic\n            if current_w > w_u_i:\n                # Splitting logic: if weight is above the upper bound.\n                # n_i is the number of particles to split into.\n                n_i = int(np.ceil(current_w / w_t_i))\n                M *= n_i\n                current_w /= n_i\n            elif current_w  w_l_i:\n                # Russian roulette logic: if weight is below the lower bound.\n                # In this deterministic model, the weight is reset to the target weight.\n                current_w = w_t_i\n            # If the weight is within the window, no action is taken.\n        \n        # The final multiplicity M is the variance reduction factor.\n        # The result is required as a float.\n        results.append(float(M))\n\n    # Print the final results in the specified format: [result1,result2,...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Moving from a deterministic, single-particle view to the statistical reality of a full simulation, we must consider the distribution of particle weights entering the weight-window check. This practice problem delves into the expected behavior of the splitting and Russian roulette mechanisms when particle weights follow a plausible log-normal distribution . By calculating the expected branching factor, you will analyze a critical metric for simulation stability, ensuring that the variance reduction scheme does not lead to an uncontrolled explosion or depletion of the particle population.",
            "id": "4260917",
            "problem": "A Monte Carlo (MC) weight-window variance-reduction scheme for neutron transport in nuclear reactor simulation enforces splitting and Russian roulette based on a target particle weight. Let the incoming particle weight be $W$, and define the dimensionless ratio $X = W / w_T$, where $w_T$ is the target weight. A weight window is imposed with lower and upper bounds $w_L = \\frac{w_T}{2}$ and $w_U = 2 w_T$. The splitting and Russian roulette rules are:\n\n- If $X > \\frac{w_U}{w_T} = 2$, perform unbiased splitting such that the expected number of daughter particles equals $X$ and the expected total outgoing weight equals $W$. One unbiased implementation is randomized rounding, which achieves an expected multiplicity $X$ without biasing the expected weight.\n- If $X  \\frac{w_L}{w_T} = \\frac{1}{2}$, perform Russian roulette with survival probability $p = X$. If the particle survives, its weight is raised to $w_T$; otherwise, it is terminated.\n- If $\\frac{1}{2} \\leq X \\leq 2$, no splitting or roulette is applied and the particle continues with its current weight.\n\nAssume that $X$ is log-normally distributed with parameters $\\mu = 0$ and $\\sigma = 0.5$; equivalently, $Y = \\ln X$ is normally distributed with mean $0$ and variance $\\sigma^2 = 0.25$. Using only these definitions and the standard properties of the normal and log-normal distributions, derive and compute the expected branching factor $B$, defined as the expected number of particles leaving the weight-window operation per incident particle, under this scheme. Round your final numerical result to four significant figures.",
            "solution": "The branching factor $B$ is defined as the expected number of particles exiting the weight-window operation, $N_{out}$, for each incident particle. We can find this by using the law of total expectation: $B = E[N_{out}] = E[E[N_{out}|X]]$.\n\nLet's determine the conditional expectation $E[N_{out}|X=x]$ for the three cases defined in the problem.\n1.  For splitting, when $x > 2$: The problem states that the expected number of daughter particles is $x$. Thus, $E[N_{out}|X=x] = x$.\n2.  For Russian roulette, when $x  \\frac{1}{2}$: A particle survives with probability $p=x$, resulting in $1$ particle, and is terminated with probability $1-x$, resulting in $0$ particles. The expected number of particles is $E[N_{out}|X=x] = 1 \\cdot p + 0 \\cdot (1-p) = p = x$.\n3.  For the pass-through case, when $\\frac{1}{2} \\leq x \\leq 2$: The single particle continues without modification. Thus, $E[N_{out}|X=x] = 1$.\n\nWe can define a function $g(x) = E[N_{out}|X=x]$:\n$$\ng(x) =\n\\begin{cases}\nx  \\text{if } 0  x  1/2 \\\\\n1  \\text{if } 1/2 \\leq x \\leq 2 \\\\\nx  \\text{if } x > 2\n\\end{cases}\n$$\n\nThe overall expected branching factor $B$ is the expectation of $g(X)$, which is computed by integrating $g(x)$ against the probability density function (PDF) of $X$, denoted by $f(x)$.\n$$\nB = E[g(X)] = \\int_0^\\infty g(x) f(x) dx\n$$\nThis integral can be broken into three parts based on the definition of $g(x)$:\n$$\nB = \\int_0^{1/2} x f(x) dx + \\int_{1/2}^2 1 \\cdot f(x) dx + \\int_2^\\infty x f(x) dx\n$$\nThe variable $X$ follows a log-normal distribution with parameters $\\mu=0$ and $\\sigma=0.5$. Its PDF is given by:\n$$\nf(x) = \\frac{1}{x\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}\\right) \\quad \\text{for } x > 0\n$$\nTo evaluate the integrals, it is more convenient to change the variable of integration to $Y = \\ln X$. The variable $Y$ is normally distributed with mean $\\mu_Y = \\mu = 0$ and variance $\\sigma_Y^2 = \\sigma^2 = 0.25$. The PDF of $Y$, which we denote $\\phi(y)$, is:\n$$\n\\phi(y) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\n$$\nThe transformation is $x = e^y$ and $dx = e^y dy$. The integration limits for $Y$ become: $x \\to 0 \\implies y \\to -\\infty$; $x=1/2 \\implies y=\\ln(1/2)=-\\ln(2)$; $x=2 \\implies y=\\ln(2)$; $x \\to \\infty \\implies y \\to \\infty$.\n\nLet's evaluate each term of $B$.\n\nThe middle term is the probability that $X$ falls within the pass-through window:\n$$\n\\int_{1/2}^2 f(x) dx = P\\left(\\frac{1}{2} \\leq X \\leq 2\\right) = P(-\\ln(2) \\leq Y \\leq \\ln(2))\n$$\nStandardizing $Y$ with $Z = \\frac{Y-\\mu}{\\sigma} = \\frac{Y}{0.5} = 2Y$, which follows a standard normal distribution $N(0,1)$, we get:\n$$\nP(-2\\ln(2) \\leq Z \\leq 2\\ln(2)) = \\Phi(2\\ln(2)) - \\Phi(-2\\ln(2))\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution.\n\nFor the first and third terms, which involve $\\int x f(x) dx$, we use a known property of the log-normal distribution. The term $x f(x)$ simplifies nicely under the change of variables:\n$$\nx f(x) dx = x \\left( \\frac{1}{x\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}\\right) \\right) dx = \\phi(\\ln x) dx\n$$\nWith the substitution $y = \\ln x$, we have $dy = \\frac{1}{x} dx$, so $dx = x dy = e^y dy$. The integrand becomes:\n$$\nx f(x) dx = \\phi(y) e^y dy = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right) e^y dy\n$$\nWe complete the square in the exponent:\n$$\ny - \\frac{(y-\\mu)^2}{2\\sigma^2} = -\\frac{y^2 - 2y(\\mu+\\sigma^2) + \\mu^2}{2\\sigma^2} = -\\frac{(y - (\\mu+\\sigma^2))^2 - (\\mu+\\sigma^2)^2 + \\mu^2}{2\\sigma^2} = -\\frac{(y - \\mu')^2}{2\\sigma^2} + \\mu + \\frac{\\sigma^2}{2}\n$$\nwhere $\\mu' = \\mu + \\sigma^2$. The integrand becomes:\n$$\n\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(y - \\mu')^2}{2\\sigma^2}\\right) dy = E[X] \\cdot \\phi'(y) dy\n$$\nwhere $E[X] = \\exp(\\mu + \\sigma^2/2)$ is the mean of the log-normal distribution $X$, and $\\phi'(y)$ is the PDF of a normal distribution with mean $\\mu' = \\mu + \\sigma^2$ and variance $\\sigma^2$.\nLet $Y'$ be a random variable distributed as $N(\\mu', \\sigma^2)$. Then $\\int_a^b x f(x) dx = E[X] \\cdot P(a \\le Y' \\le b)$.\n\nNow we can compute the first and third terms of $B$:\n$$\n\\int_0^{1/2} x f(x) dx = E[X] \\int_{-\\infty}^{-\\ln(2)} \\phi'(y) dy = E[X] \\cdot P(Y' \\leq -\\ln(2))\n$$\nStandardizing $Y'$ with $Z = \\frac{Y'-\\mu'}{\\sigma}$, we get:\n$$\nP\\left(Z \\leq \\frac{-\\ln(2) - \\mu'}{\\sigma}\\right) = \\Phi\\left(\\frac{-\\ln(2) - (\\mu+\\sigma^2)}{\\sigma}\\right)\n$$\nAnd for the third term:\n$$\n\\int_2^\\infty x f(x) dx = E[X] \\int_{\\ln(2)}^{\\infty} \\phi'(y) dy = E[X] \\cdot P(Y' \\geq \\ln(2)) = E[X] \\left(1 - \\Phi\\left(\\frac{\\ln(2) - \\mu'}{\\sigma}\\right)\\right)\n$$\n\nNow, we substitute the given values: $\\mu=0$ and $\\sigma=0.5$ (so $\\sigma^2=0.25$).\n$E[X] = \\exp(0 + 0.25/2) = \\exp(0.125)$.\n$\\mu' = 0 + 0.25 = 0.25$.\nThe arguments for the $\\Phi$ functions are:\nFor the middle term:\n$z_1 = \\frac{\\ln(2)}{\\sigma} = \\frac{\\ln(2)}{0.5} = 2\\ln(2) \\approx 1.38629$.\n$\\int_{1/2}^2 f(x) dx = \\Phi(2\\ln(2)) - \\Phi(-2\\ln(2)) \\approx \\Phi(1.38629) - \\Phi(-1.38629) \\approx 0.917165 - 0.082835 = 0.834330$.\n\nFor the first term:\n$z_2 = \\frac{-\\ln(2) - \\mu'}{\\sigma} = \\frac{-\\ln(2) - 0.25}{0.5} = -2\\ln(2) - 0.5 \\approx -1.38629 - 0.5 = -1.88629$.\n$\\int_0^{1/2} x f(x) dx = \\exp(0.125) \\cdot \\Phi(-1.88629) \\approx 1.133148 \\times 0.029630 = 0.033586$.\n\nFor the third term:\n$z_3 = \\frac{\\ln(2) - \\mu'}{\\sigma} = \\frac{\\ln(2) - 0.25}{0.5} = 2\\ln(2) - 0.5 \\approx 1.38629 - 0.5 = 0.88629$.\n$\\int_2^\\infty x f(x) dx = \\exp(0.125) \\cdot (1 - \\Phi(0.88629)) \\approx 1.133148 \\times (1 - 0.812284) = 1.133148 \\times 0.187716 = 0.212711$.\n\nFinally, we sum the three terms to find $B$:\n$$\nB \\approx 0.033586 + 0.834330 + 0.212711 = 1.080627\n$$\nThe problem requires the result to be rounded to four significant figures. The first four significant figures are $1.080$, and the fifth digit is $6$. Therefore, we round up.\n$$\nB \\approx 1.081\n$$",
            "answer": "$$\n\\boxed{1.081}\n$$"
        },
        {
            "introduction": "Effective variance reduction relies not only on sound theory but also on thoughtful implementation. This exercise explores a practical trade-off in the application of Russian roulette for underweight particles: is it better to apply a single, aggressive roulette to reach the target weight, or to use a gentler, multi-stage approach ? You will derive the survival probability and expected computational cost for a multi-stage process, providing a quantitative basis for making design choices that balance algorithmic efficiency with the preservation of particle histories in low-importance regions.",
            "id": "4260955",
            "problem": "In Monte Carlo (MC) neutron transport with weight windows, underweight particles with weight $w_0$ below a target lower bound $w_T$ are often adjusted using Russian roulette to preserve unbiasedness. Consider a neutron entering a low-importance region with weight $w_0$ satisfying $0  w_0  w_T$. Two strategies are possible to promote the weight toward $w_T$ while maintaining unbiasedness.\n\nStrategy A (direct reassignment): Apply a single Russian roulette operation that either kills the particle or raises its weight directly to $w_T$.\n\nStrategy B (multi-stage reassignment): Apply $m$ successive Russian roulette operations through a geometric ladder of intermediate targets $w_1, w_2, \\ldots, w_m$ with $w_i = w_0 \\gamma^i$ and $w_m = w_T$, where $\\gamma = \\left(\\frac{w_T}{w_0}\\right)^{\\frac{1}{m}}$. At each stage $i \\in \\{1,2,\\ldots,m\\}$, the particle either dies or is promoted from $w_{i-1}$ to $w_i$. Each roulette test has unit expected computational cost, independent of outcome, and if a particle dies at some stage, no further stages are attempted. The direct reassignment (Strategy A) uses a single roulette test and thus has unit expected computational cost.\n\nStarting only from the unbiasedness requirement that the expected post-operation weight must equal the pre-operation weight at each stage, do the following:\n\n1. Derive the stage survival probability as a function of the pair $(w_{i-1}, w_i)$ and then derive the overall survival probability $p_m$ under Strategy B.\n2. Compute the expected number of roulette tests under Strategy B and express the expected computational work relative to Strategy A.\n3. For compactness, define $r = \\frac{w_0}{w_T}$ and let $J(m)$ denote the product of the overall survival probability and the relative expected work, i.e., $J(m) = p_m \\times \\left(\\text{relative expected work of Strategy B versus Strategy A}\\right)$.\n\nProvide a closed-form expression for $J(m)$ in terms of $r$ and $m$. No numerical rounding is required, and no units are involved. Express your final answer in terms of $r$ and $m$ only.",
            "solution": "The problem asks for a closed-form expression for the quantity $J(m)$, which is defined as the product of the overall survival probability under a multi-stage Russian roulette scheme (Strategy B) and the expected computational work of this scheme relative to a single-stage scheme (Strategy A). The derivation is broken down into three parts as outlined in the problem statement.\n\n**1. Derivation of Survival Probability**\n\nLet us first consider a single, generic stage $i$ of the Russian roulette process, where a particle with weight $w_{i-1}$ is promoted to a higher weight $w_i$. Let $p_{\\text{stage}, i}$ be the probability that the particle \"survives\" this stage. If it survives, its new weight is $w_i$. If it does not survive (with probability $1 - p_{\\text{stage}, i}$), it is \"killed,\" and its weight becomes $0$.\n\nThe process must be unbiased, meaning the expected weight after the operation must equal the weight before the operation. This is a fundamental requirement of any variance reduction technique like Russian roulette.\nThe expected weight, $E[w_{\\text{after}}]$, is given by:\n$$E[w_{\\text{after}}] = p_{\\text{stage}, i} \\cdot w_i + (1 - p_{\\text{stage}, i}) \\cdot 0$$\nSetting this equal to the pre-operation weight, $w_{\\text{before}} = w_{i-1}$:\n$$p_{\\text{stage}, i} \\cdot w_i = w_{i-1}$$\nSolving for the stage survival probability gives:\n$$p_{\\text{stage}, i} = \\frac{w_{i-1}}{w_i}$$\nFor Strategy B, a particle must survive $m$ consecutive stages to be successfully promoted from the initial weight $w_0$ to the final target weight $w_m = w_T$. The overall survival probability, $p_m$, is the product of the individual stage survival probabilities, as survival at stage $i-1$ is a prerequisite for attempting stage $i$.\n$$p_m = \\prod_{i=1}^{m} p_{\\text{stage}, i} = \\prod_{i=1}^{m} \\frac{w_{i-1}}{w_i}$$\nThis forms a telescoping product:\n$$p_m = \\left(\\frac{w_0}{w_1}\\right) \\cdot \\left(\\frac{w_1}{w_2}\\right) \\cdot \\ldots \\cdot \\left(\\frac{w_{m-1}}{w_m}\\right) = \\frac{w_0}{w_m}$$\nGiven that the final target weight is $w_m = w_T$, the overall survival probability is:\n$$p_m = \\frac{w_0}{w_T}$$\nUsing the problem's definition, $r = \\frac{w_0}{w_T}$, we find that the overall survival probability is simply:\n$$p_m = r$$\n\n**2. Derivation of Relative Expected Work**\n\nThe computational work is measured by the number of roulette tests performed. Strategy A involves a single test, so its expected computational work is $E[C_A] = 1$.\n\nFor Strategy B, a test is performed at stage $i$ if and only if the particle has survived all preceding $i-1$ stages. The probability of this event is:\n$$P(\\text{reaching stage } i) = \\prod_{j=1}^{i-1} p_{\\text{stage}, j} = \\prod_{j=1}^{i-1} \\frac{w_{j-1}}{w_j} = \\frac{w_0}{w_{i-1}}$$\nFor the first stage ($i=1$), the product is empty and taken to be $1$, so $P(\\text{reaching stage } 1) = \\frac{w_0}{w_0} = 1$, which is correct as the first test is always performed.\n\nThe total expected number of tests for Strategy B, $E[C_B]$, is the sum of the probabilities of performing a test at each stage:\n$$E[C_B] = \\sum_{i=1}^{m} P(\\text{reaching stage } i) = \\sum_{i=1}^{m} \\frac{w_0}{w_{i-1}}$$\nWe are given the definition of the intermediate weights, $w_i = w_0 \\gamma^i$. Therefore, $w_{i-1} = w_0 \\gamma^{i-1}$. Substituting this into the sum:\n$$E[C_B] = \\sum_{i=1}^{m} \\frac{w_0}{w_0 \\gamma^{i-1}} = \\sum_{i=1}^{m} \\left(\\frac{1}{\\gamma}\\right)^{i-1}$$\nThis is a finite geometric series with $m$ terms, a first term $a=1$, and a common ratio $q = \\frac{1}{\\gamma}$. The sum is given by the formula $\\frac{a(1-q^m)}{1-q}$.\n$$E[C_B] = \\frac{1 \\cdot \\left(1 - \\left(\\frac{1}{\\gamma}\\right)^m\\right)}{1 - \\frac{1}{\\gamma}} = \\frac{1 - \\gamma^{-m}}{1 - \\gamma^{-1}}$$\nNow, we use the definition of $\\gamma = \\left(\\frac{w_T}{w_0}\\right)^{\\frac{1}{m}}$. In terms of $r = \\frac{w_0}{w_T}$, we have $\\gamma = \\left(\\frac{1}{r}\\right)^{\\frac{1}{m}} = r^{-\\frac{1}{m}}$.\nFrom this, we find the terms needed for our expression for $E[C_B]$:\n$$\\gamma^{-1} = \\left(r^{-\\frac{1}{m}}\\right)^{-1} = r^{\\frac{1}{m}}$$\n$$\\gamma^{-m} = \\left(r^{-\\frac{1}{m}}\\right)^{-m} = r$$\nSubstituting these into the expression for $E[C_B]$:\n$$E[C_B] = \\frac{1-r}{1 - r^{\\frac{1}{m}}}$$\nThe relative expected work of Strategy B versus Strategy A is the ratio of their expected costs:\n$$\\text{Relative Expected Work} = \\frac{E[C_B]}{E[C_A]} = \\frac{ \\frac{1-r}{1 - r^{\\frac{1}{m}}} }{1} = \\frac{1-r}{1 - r^{\\frac{1}{m}}}$$\n\n**3. Final Expression for $J(m)$**\n\nThe problem defines $J(m)$ as the product of the overall survival probability $p_m$ and the relative expected work.\n$$J(m) = p_m \\times (\\text{Relative Expected Work})$$\nSubstituting the expressions derived in the previous parts:\n$$J(m) = r \\cdot \\left(\\frac{1-r}{1 - r^{\\frac{1}{m}}}\\right)$$\nThis yields the final closed-form expression:\n$$J(m) = \\frac{r(1-r)}{1 - r^{\\frac{1}{m}}}$$",
            "answer": "$$\n\\boxed{\\frac{r(1-r)}{1 - r^{\\frac{1}{m}}}}\n$$"
        }
    ]
}