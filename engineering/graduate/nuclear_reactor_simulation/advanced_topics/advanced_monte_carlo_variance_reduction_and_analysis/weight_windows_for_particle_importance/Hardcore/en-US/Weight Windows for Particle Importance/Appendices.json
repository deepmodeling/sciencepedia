{
    "hands_on_practices": [
        {
            "introduction": "Russian roulette is a cornerstone of variance reduction, ensuring that computational effort is not wasted on low-importance particles. This exercise delves into the mechanics of this technique, moving beyond the simple idea of \"killing\" a particle to explore how different application strategies affect survival probability and computational cost, all while strictly preserving an unbiased result. By comparing a direct, single-step weight adjustment to a gradual, multi-stage approach, you will gain a deeper appreciation for the design choices involved in implementing efficient Monte Carlo simulations. ",
            "id": "4260955",
            "problem": "In Monte Carlo (MC) neutron transport with weight windows, underweight particles with weight $w_0$ below a target lower bound $w_T$ are often adjusted using Russian roulette to preserve unbiasedness. Consider a neutron entering a low-importance region with weight $w_0$ satisfying $0  w_0  w_T$. Two strategies are possible to promote the weight toward $w_T$ while maintaining unbiasedness.\n\nStrategy A (direct reassignment): Apply a single Russian roulette operation that either kills the particle or raises its weight directly to $w_T$.\n\nStrategy B (multi-stage reassignment): Apply $m$ successive Russian roulette operations through a geometric ladder of intermediate targets $w_1, w_2, \\ldots, w_m$ with $w_i = w_0 \\gamma^i$ and $w_m = w_T$, where $\\gamma = \\left(\\frac{w_T}{w_0}\\right)^{\\frac{1}{m}}$. At each stage $i \\in \\{1,2,\\ldots,m\\}$, the particle either dies or is promoted from $w_{i-1}$ to $w_i$. Each roulette test has unit expected computational cost, independent of outcome, and if a particle dies at some stage, no further stages are attempted. The direct reassignment (Strategy A) uses a single roulette test and thus has unit expected computational cost.\n\nStarting only from the unbiasedness requirement that the expected post-operation weight must equal the pre-operation weight at each stage, do the following:\n\n1. Derive the stage survival probability as a function of the pair $(w_{i-1}, w_i)$ and then derive the overall survival probability $p_m$ under Strategy B.\n2. Compute the expected number of roulette tests under Strategy B and express the expected computational work relative to Strategy A.\n3. For compactness, define $r = \\frac{w_0}{w_T}$ and let $J(m)$ denote the product of the overall survival probability and the relative expected work, i.e., $J(m) = p_m \\times \\left(\\text{relative expected work of Strategy B versus Strategy A}\\right)$.\n\nProvide a closed-form expression for $J(m)$ in terms of $r$ and $m$. No numerical rounding is required, and no units are involved. Express your final answer in terms of $r$ and $m$ only.",
            "solution": "The problem asks for a closed-form expression for the quantity $J(m)$, which is defined as the product of the overall survival probability under a multi-stage Russian roulette scheme (Strategy B) and the expected computational work of this scheme relative to a single-stage scheme (Strategy A). The derivation is broken down into three parts as outlined in the problem statement.\n\n**1. Derivation of Survival Probability**\n\nLet us first consider a single, generic stage $i$ of the Russian roulette process, where a particle with weight $w_{i-1}$ is promoted to a higher weight $w_i$. Let $p_{\\text{stage}, i}$ be the probability that the particle \"survives\" this stage. If it survives, its new weight is $w_i$. If it does not survive (with probability $1 - p_{\\text{stage}, i}$), it is \"killed,\" and its weight becomes $0$.\n\nThe process must be unbiased, meaning the expected weight after the operation must equal the weight before the operation. This is a fundamental requirement of any variance reduction technique like Russian roulette.\nThe expected weight, $E[w_{\\text{after}}]$, is given by:\n$$E[w_{\\text{after}}] = p_{\\text{stage}, i} \\cdot w_i + (1 - p_{\\text{stage}, i}) \\cdot 0$$\nSetting this equal to the pre-operation weight, $w_{\\text{before}} = w_{i-1}$:\n$$p_{\\text{stage}, i} \\cdot w_i = w_{i-1}$$\nSolving for the stage survival probability gives:\n$$p_{\\text{stage}, i} = \\frac{w_{i-1}}{w_i}$$\nFor Strategy B, a particle must survive $m$ consecutive stages to be successfully promoted from the initial weight $w_0$ to the final target weight $w_m = w_T$. The overall survival probability, $p_m$, is the product of the individual stage survival probabilities, as survival at stage $i-1$ is a prerequisite for attempting stage $i$.\n$$p_m = \\prod_{i=1}^{m} p_{\\text{stage}, i} = \\prod_{i=1}^{m} \\frac{w_{i-1}}{w_i}$$\nThis forms a telescoping product:\n$$p_m = \\left(\\frac{w_0}{w_1}\\right) \\cdot \\left(\\frac{w_1}{w_2}\\right) \\cdot \\ldots \\cdot \\left(\\frac{w_{m-1}}{w_m}\\right) = \\frac{w_0}{w_m}$$\nGiven that the final target weight is $w_m = w_T$, the overall survival probability is:\n$$p_m = \\frac{w_0}{w_T}$$\nUsing the problem's definition, $r = \\frac{w_0}{w_T}$, we find that the overall survival probability is simply:\n$$p_m = r$$\n\n**2. Derivation of Relative Expected Work**\n\nThe computational work is measured by the number of roulette tests performed. Strategy A involves a single test, so its expected computational work is $E[C_A] = 1$.\n\nFor Strategy B, a test is performed at stage $i$ if and only if the particle has survived all preceding $i-1$ stages. The probability of this event is:\n$$P(\\text{reaching stage } i) = \\prod_{j=1}^{i-1} p_{\\text{stage}, j} = \\prod_{j=1}^{i-1} \\frac{w_{j-1}}{w_j} = \\frac{w_0}{w_{i-1}}$$\nFor the first stage ($i=1$), the product is empty and taken to be $1$, so $P(\\text{reaching stage } 1) = \\frac{w_0}{w_0} = 1$, which is correct as the first test is always performed.\n\nThe total expected number of tests for Strategy B, $E[C_B]$, is the sum of the probabilities of performing a test at each stage:\n$$E[C_B] = \\sum_{i=1}^{m} P(\\text{reaching stage } i) = \\sum_{i=1}^{m} \\frac{w_0}{w_{i-1}}$$\nWe are given the definition of the intermediate weights, $w_i = w_0 \\gamma^i$. Therefore, $w_{i-1} = w_0 \\gamma^{i-1}$. Substituting this into the sum:\n$$E[C_B] = \\sum_{i=1}^{m} \\frac{w_0}{w_0 \\gamma^{i-1}} = \\sum_{i=1}^{m} \\left(\\frac{1}{\\gamma}\\right)^{i-1}$$\nThis is a finite geometric series with $m$ terms, a first term $a=1$, and a common ratio $q = \\frac{1}{\\gamma}$. The sum is given by the formula $\\frac{a(1-q^m)}{1-q}$.\n$$E[C_B] = \\frac{1 \\cdot \\left(1 - \\left(\\frac{1}{\\gamma}\\right)^m\\right)}{1 - \\frac{1}{\\gamma}} = \\frac{1 - \\gamma^{-m}}{1 - \\gamma^{-1}}$$\nNow, we use the definition of $\\gamma = \\left(\\frac{w_T}{w_0}\\right)^{\\frac{1}{m}}$. In terms of $r = \\frac{w_0}{w_T}$, we have $\\gamma = \\left(\\frac{1}{r}\\right)^{\\frac{1}{m}} = r^{-\\frac{1}{m}}$.\nFrom this, we find the terms needed for our expression for $E[C_B]$:\n$$\\gamma^{-1} = \\left(r^{-\\frac{1}{m}}\\right)^{-1} = r^{\\frac{1}{m}}$$\n$$\\gamma^{-m} = \\left(r^{-\\frac{1}{m}}\\right)^{-m} = r$$\nSubstituting these into the expression for $E[C_B]$:\n$$E[C_B] = \\frac{1-r}{1 - r^{\\frac{1}{m}}}$$\nThe relative expected work of Strategy B versus Strategy A is the ratio of their expected costs:\n$$\\text{Relative Expected Work} = \\frac{E[C_B]}{E[C_A]} = \\frac{ \\frac{1-r}{1 - r^{\\frac{1}{m}}} }{1} = \\frac{1-r}{1 - r^{\\frac{1}{m}}}$$\n\n**3. Final Expression for $J(m)$**\n\nThe problem defines $J(m)$ as the product of the overall survival probability $p_m$ and the relative expected work.\n$$J(m) = p_m \\times (\\text{Relative Expected Work})$$\nSubstituting the expressions derived in the previous parts:\n$$J(m) = r \\cdot \\left(\\frac{1-r}{1 - r^{\\frac{1}{m}}}\\right)$$\nThis yields the final closed-form expression:\n$$J(m) = \\frac{r(1-r)}{1 - r^{\\frac{1}{m}}}$$",
            "answer": "$$\n\\boxed{\\frac{r(1-r)}{1 - r^{\\frac{1}{m}}}}\n$$"
        },
        {
            "introduction": "A well-configured weight-window system should ideally maintain a stable particle population, avoiding explosive growth or rapid depletion. This practice challenges you to quantify the net effect of splitting and Russian roulette by calculating the expected branching factorâ€”the average number of outgoing particles for each incoming one. By analyzing a hypothetical scenario with a log-normal distribution of particle weights, you will learn how to connect the microscopic rules of the weight window to the macroscopic population dynamics of the simulation. ",
            "id": "4260917",
            "problem": "A Monte Carlo (MC) weight-window variance-reduction scheme for neutron transport in nuclear reactor simulation enforces splitting and Russian roulette based on a target particle weight. Let the incoming particle weight be $W$, and define the dimensionless ratio $X = W / w_T$, where $w_T$ is the target weight. A weight window is imposed with lower and upper bounds $w_L = \\frac{w_T}{2}$ and $w_U = 2 w_T$. The splitting and Russian roulette rules are:\n\n- If $X  \\frac{w_U}{w_T} = 2$, perform unbiased splitting such that the expected number of daughter particles equals $X$ and the expected total outgoing weight equals $W$. One unbiased implementation is randomized rounding, which achieves an expected multiplicity $X$ without biasing the expected weight.\n- If $X  \\frac{w_L}{w_T} = \\frac{1}{2}$, perform Russian roulette with survival probability $p = X$. If the particle survives, its weight is raised to $w_T$; otherwise, it is terminated.\n- If $\\frac{1}{2} \\leq X \\leq 2$, no splitting or roulette is applied and the particle continues with its current weight.\n\nAssume that $X$ is log-normally distributed with parameters $\\mu = 0$ and $\\sigma = 0.5$; equivalently, $Y = \\ln X$ is normally distributed with mean $0$ and variance $\\sigma^2 = 0.25$. Using only these definitions and the standard properties of the normal and log-normal distributions, derive and compute the expected branching factor $B$, defined as the expected number of particles leaving the weight-window operation per incident particle, under this scheme. Round your final numerical result to four significant figures.",
            "solution": "The branching factor $B$ is defined as the expected number of particles exiting the weight-window operation, $N_{out}$, for each incident particle. We can find this by using the law of total expectation: $B = E[N_{out}] = E[E[N_{out}|X]]$.\n\nLet's determine the conditional expectation $E[N_{out}|X=x]$ for the three cases defined in the problem.\n1.  For splitting, when $x > 2$: The problem states that the expected number of daughter particles is $x$. Thus, $E[N_{out}|X=x] = x$.\n2.  For Russian roulette, when $x  \\frac{1}{2}$: A particle survives with probability $p=x$, resulting in $1$ particle, and is terminated with probability $1-x$, resulting in $0$ particles. The expected number of particles is $E[N_{out}|X=x] = 1 \\cdot p + 0 \\cdot (1-p) = p = x$.\n3.  For the pass-through case, when $\\frac{1}{2} \\leq x \\leq 2$: The single particle continues without modification. Thus, $E[N_{out}|X=x] = 1$.\n\nWe can define a function $g(x) = E[N_{out}|X=x]$:\n$$\ng(x) =\n\\begin{cases}\nx  \\text{if } 0  x  1/2 \\\\\n1  \\text{if } 1/2 \\leq x \\leq 2 \\\\\nx  \\text{if } x  2\n\\end{cases}\n$$\n\nThe overall expected branching factor $B$ is the expectation of $g(X)$, which is computed by integrating $g(x)$ against the probability density function (PDF) of $X$, denoted by $f(x)$.\n$$\nB = E[g(X)] = \\int_0^\\infty g(x) f(x) dx\n$$\nThis integral can be broken into three parts based on the definition of $g(x)$:\n$$\nB = \\int_0^{1/2} x f(x) dx + \\int_{1/2}^2 1 \\cdot f(x) dx + \\int_2^\\infty x f(x) dx\n$$\nThe variable $X$ follows a log-normal distribution with parameters $\\mu=0$ and $\\sigma=0.5$. It is more convenient to change the variable of integration to $Y = \\ln X$. The variable $Y$ is normally distributed with mean $\\mu_Y = \\mu = 0$ and variance $\\sigma_Y^2 = \\sigma^2 = 0.25$.\n\nThe middle term is the probability that $X$ falls within the pass-through window:\n$$\n\\int_{1/2}^2 f(x) dx = P\\left(\\frac{1}{2} \\leq X \\leq 2\\right) = P(-\\ln(2) \\leq Y \\leq \\ln(2))\n$$\nStandardizing $Y$ with $Z = \\frac{Y-\\mu}{\\sigma} = \\frac{Y}{0.5} = 2Y$, which follows a standard normal distribution $N(0,1)$, we get:\n$$\nP(-2\\ln(2) \\leq Z \\leq 2\\ln(2)) = \\Phi(2\\ln(2)) - \\Phi(-2\\ln(2))\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution.\n\nFor the first and third terms, which involve $\\int x f(x) dx$, we use a known property of the log-normal distribution. The expectation of $X$ over an interval $[a, b]$ can be expressed using a related normal distribution $Y' \\sim N(\\mu+\\sigma^2, \\sigma^2)$. Specifically, $\\int_a^b x f(x) dx = E[X] \\cdot P(\\ln a \\le Y' \\le \\ln b)$, where $E[X] = \\exp(\\mu + \\sigma^2/2)$.\n\nWith the given values $\\mu=0$ and $\\sigma=0.5$ (so $\\sigma^2=0.25$):\n$E[X] = \\exp(0 + 0.25/2) = \\exp(0.125)$.\nThe related normal distribution is $Y' \\sim N(\\mu' = 0.25, \\sigma^2 = 0.25)$.\n\nNow we can compute the three terms for $B$:\n1. Middle Term:\n$z_1 = \\frac{\\ln(2)}{\\sigma} = \\frac{\\ln(2)}{0.5} = 2\\ln(2) \\approx 1.38629$.\n$\\int_{1/2}^2 f(x) dx = \\Phi(1.38629) - \\Phi(-1.38629) \\approx 0.917165 - 0.082835 = 0.834330$.\n\n2. First Term (Russian Roulette):\n$\\int_0^{1/2} x f(x) dx = E[X] \\cdot P(Y' \\leq \\ln(1/2)) = E[X] \\cdot P(Y' \\leq -\\ln(2))$.\nStandardizing $Y'$ with $Z = \\frac{Y'-\\mu'}{\\sigma} = \\frac{Y'-0.25}{0.5}$:\n$P\\left(Z \\leq \\frac{-\\ln(2) - 0.25}{0.5}\\right) = \\Phi(-2\\ln(2) - 0.5) \\approx \\Phi(-1.88629)$.\n$\\int_0^{1/2} x f(x) dx = \\exp(0.125) \\cdot \\Phi(-1.88629) \\approx 1.133148 \\times 0.029630 = 0.033586$.\n\n3. Third Term (Splitting):\n$\\int_2^\\infty x f(x) dx = E[X] \\cdot P(Y' \\geq \\ln(2))$.\nStandardizing $Y'$:\n$P\\left(Z \\geq \\frac{\\ln(2) - 0.25}{0.5}\\right) = P(Z \\geq 2\\ln(2) - 0.5) \\approx P(Z \\geq 0.88629) = 1 - \\Phi(0.88629)$.\n$\\int_2^\\infty x f(x) dx = \\exp(0.125) \\cdot (1 - \\Phi(0.88629)) \\approx 1.133148 \\times (1 - 0.812284) = 1.133148 \\times 0.187716 = 0.212711$.\n\nFinally, we sum the three terms to find $B$:\n$$\nB \\approx 0.033586 + 0.834330 + 0.212711 = 1.080627\n$$\nRounding the result to four significant figures gives:\n$$\nB \\approx 1.081\n$$",
            "answer": "$$\n\\boxed{1.081}\n$$"
        },
        {
            "introduction": "In an ideal world, we would use the exact adjoint importance function to guide our variance reduction, leading to dramatic efficiency gains. In reality, the importance maps used to generate weight windows are approximations containing inherent errors. This advanced exercise confronts this practical limitation head-on, asking you to derive how small errors in the importance map propagate into increased statistical variance and degrade the overall simulation Figure of Merit (FOM). ",
            "id": "4260973",
            "problem": "Consider a deep-penetration neutron response tally in a reactor shielding problem, discretized into $B$ disjoint phase-space bins indexed by $b \\in \\{1,\\dots,B\\}$. Let the analog probability for a history to realize its contribution in bin $b$ be $p_b$, and let the deterministic adjoint importance, proportional to the bin-wise contribution to the tally, be $I_b$. Assume the Monte Carlo biasing induced by weight windows uses a sampling distribution proportional to the product of the deterministic adjoint and the analog probability. Specifically, the intended optimal importance sampling distribution would be $q_b^{\\ast} \\propto I_b p_b$. In practice, the deterministic adjoint contains small, bin-dependent fractional errors, so that the implemented importance is $\\hat{I}_b = I_b (1 + \\varepsilon_b)$ with $|\\varepsilon_b| \\ll 1$. Consequently, the implemented sampling distribution is $q_b = C (1 + \\varepsilon_b) I_b p_b$, where $C$ is the normalization constant ensuring $\\sum_{b=1}^{B} q_b = 1$.\n\nYou will use the standard importance sampling estimator for the response,\n$$\n\\mu = \\sum_{b=1}^{B} I_b p_b,\n$$\nwith per-history weight\n$$\nw_b = \\frac{I_b p_b}{q_b}.\n$$\nStart from the fundamentals of importance sampling: the estimator is unbiased, $E_{q}[w] = \\mu$, and the variance per history is $\\sigma_{1}^{2} = E_{q}[w^{2}] - \\mu^{2}$. Derive, to second order in the small parameters $\\varepsilon_b$, a closed-form expression for the window-induced variance increment, i.e., the leading nonzero term in $\\sigma_{1}^{2}(\\boldsymbol{\\varepsilon})$ caused solely by the mismatch between $q_b$ and $q_b^{\\ast}$. Then, adopting the standard definition of Figure of Merit (FOM), where Figure of Merit (FOM) is $F = 1/(\\sigma^{2} T)$ and the central processing unit (CPU) time per history is constant, $T = \\kappa N$ for $N$ histories with constant $\\kappa > 0$, compute the sensitivity of $F$ to the fractional errors $\\varepsilon_b$ across bins, expressed as the gradient vector $\\nabla_{\\boldsymbol{\\varepsilon}} F$ whose $b$-th component is $\\partial F / \\partial \\varepsilon_b$.\n\nYour derivation must begin from the unbiasedness and variance definitions of importance sampling and proceed by systematic expansion in the small parameters $\\varepsilon_b$. Express your final answer for the sensitivity as a single closed-form analytic row vector in terms of $I_b$, $p_b$, $\\varepsilon_b$, $\\kappa$, and $\\mu$. No numerical evaluation is required. The final answer must be an analytical expression. Do not include units inside the final boxed answer.",
            "solution": "The response to be estimated is given by the sum over all phase-space bins $b$:\n$$\n\\mu = \\sum_{b=1}^{B} I_b p_b\n$$\nwhere $I_b$ is the true adjoint importance and $p_b$ is the analog probability of a history contributing in bin $b$.\n\nThe implemented, non-ideal sampling distribution is $q_b = C \\hat{I}_b p_b = C I_b (1 + \\varepsilon_b) p_b$, where $\\hat{I}_b$ is the implemented importance with a small fractional error $\\varepsilon_b$, and $C$ is a normalization constant. The condition $\\sum_{b=1}^{B} q_b = 1$ determines $C$:\n$$\nC = \\frac{1}{\\mu + \\sum_{j=1}^{B} \\varepsilon_j I_j p_j}\n$$\nThe weight for a history contributing in bin $b$ is given by $w_b = \\frac{I_b p_b}{q_b}$. Substituting the expressions for $q_b$ and $C$:\n$$\nw_b = \\frac{\\mu + \\sum_{j=1}^{B} \\varepsilon_j I_j p_j}{1 + \\varepsilon_b}\n$$\nThe variance per history, $\\sigma_{1}^{2}$, is defined as $\\sigma_{1}^{2} = E_{q}[w^{2}] - \\mu^{2}$, since the estimator is unbiased. We compute $E_{q}[w^{2}] = \\sum_{b=1}^{B} w_b^2 q_b$:\n$$\nE_{q}[w^{2}] = \\sum_{b=1}^{B} \\frac{(I_b p_b)^2}{q_b} = \\frac{1}{C} \\sum_{b=1}^{B} \\frac{I_b p_b}{1 + \\varepsilon_b}\n$$\nSubstituting the expression for $C$:\n$$\nE_{q}[w^{2}] = \\left(\\mu + \\sum_{j=1}^{B} \\varepsilon_j I_j p_j\\right) \\left(\\sum_{b=1}^{B} \\frac{I_b p_b}{1 + \\varepsilon_b}\\right)\n$$\nSince $|\\varepsilon_b| \\ll 1$, we use the Taylor series expansion $\\frac{1}{1+x} \\approx 1 - x + x^2$. We need to retain terms up to second order in $\\varepsilon_b$ to find the leading nonzero term in the variance.\n$$\n\\sum_{b=1}^{B} \\frac{I_b p_b}{1 + \\varepsilon_b} \\approx \\sum_{b=1}^{B} I_b p_b (1 - \\varepsilon_b + \\varepsilon_b^2) = \\mu - \\sum_{b=1}^{B} \\varepsilon_b I_b p_b + \\sum_{b=1}^{B} \\varepsilon_b^2 I_b p_b\n$$\nLet us define $S_1 = \\sum_{b=1}^{B} \\varepsilon_b I_b p_b$ and $S_2 = \\sum_{b=1}^{B} \\varepsilon_b^2 I_b p_b$. The expression above becomes $\\mu - S_1 + S_2$.\nNow, we substitute this back into the expression for $E_{q}[w^{2}]$ and keep terms up to second order in $\\varepsilon$:\n$$\nE_{q}[w^{2}] \\approx (\\mu + S_1) (\\mu - S_1 + S_2) = \\mu^2 - \\mu S_1 + \\mu S_2 + S_1 \\mu - S_1^2 + O(\\varepsilon^3) \\approx \\mu^2 + \\mu S_2 - S_1^2\n$$\nThe variance increment $\\sigma_{1}^{2}(\\boldsymbol{\\varepsilon})$ is then:\n$$\n\\sigma_{1}^{2}(\\boldsymbol{\\varepsilon}) = E_{q}[w^{2}] - \\mu^2 \\approx \\mu \\sum_{b=1}^{B} \\varepsilon_b^2 I_b p_b - \\left(\\sum_{b=1}^{B} \\varepsilon_b I_b p_b\\right)^2\n$$\nThis is the closed-form expression for the window-induced variance increment to second order.\n\nNext, we compute the sensitivity of the Figure of Merit (FOM), $F$. Given $F = 1/(\\sigma^2 T)$ and $T=\\kappa N$, the FOM simplifies to $F = 1/(\\kappa \\sigma_1^2)$. We need to compute the gradient vector $\\nabla_{\\boldsymbol{\\varepsilon}} F$, whose $b$-th component is $\\frac{\\partial F}{\\partial \\varepsilon_b}$. Using the chain rule:\n$$\n\\frac{\\partial F}{\\partial \\varepsilon_b} = \\frac{dF}{d\\sigma_1^2} \\frac{\\partial \\sigma_1^2}{\\partial \\varepsilon_b} = -\\frac{1}{\\kappa (\\sigma_1^2)^2} \\frac{\\partial \\sigma_1^2}{\\partial \\varepsilon_b}\n$$\nWe differentiate our expression for $\\sigma_1^2$ with respect to a specific error component, $\\varepsilon_k$:\n$$\n\\frac{\\partial \\sigma_1^2}{\\partial \\varepsilon_k} = \\frac{\\partial}{\\partial \\varepsilon_k} \\left[ \\mu \\sum_{b=1}^{B} \\varepsilon_b^2 I_b p_b - \\left(\\sum_{b=1}^{B} \\varepsilon_b I_b p_b\\right)^2 \\right]\n$$\n$$\n\\frac{\\partial \\sigma_1^2}{\\partial \\varepsilon_k} = 2 \\mu \\varepsilon_k I_k p_k - 2 \\left( \\sum_{b=1}^{B} \\varepsilon_b I_b p_b \\right) (I_k p_k) = 2 I_k p_k \\left( \\mu \\varepsilon_k - \\sum_{b=1}^{B} \\varepsilon_b I_b p_b \\right)\n$$\nAssembling the component of the gradient of $F$:\n$$\n\\frac{\\partial F}{\\partial \\varepsilon_k} = -\\frac{1}{\\kappa (\\sigma_1^2)^2} \\left[ 2 I_k p_k \\left( \\mu \\varepsilon_k - \\sum_{b=1}^{B} \\varepsilon_b I_b p_b \\right) \\right]\n$$\nSubstituting the expression for $\\sigma_1^2$, the general $b$-th component of the gradient vector is:\n$$\n\\left( \\nabla_{\\boldsymbol{\\varepsilon}} F \\right)_b = - \\frac{2 I_b p_b \\left( \\mu \\varepsilon_b - \\sum_{j=1}^{B} \\varepsilon_j I_j p_j \\right)}{\\kappa \\left[ \\mu \\sum_{j=1}^{B} \\varepsilon_j^2 I_j p_j - \\left(\\sum_{j=1}^{B} \\varepsilon_j I_j p_j\\right)^2 \\right]^2}\n$$\nThis provides the final analytical expression for the sensitivity.",
            "answer": "$$\n\\boxed{\n\\left( - \\frac{2 I_b p_b \\left( \\mu \\varepsilon_b - \\sum_{j=1}^{B} \\varepsilon_j I_j p_j \\right)}{\\kappa \\left[ \\mu \\sum_{j=1}^{B} \\varepsilon_j^2 I_j p_j - \\left(\\sum_{j=1}^{B} \\varepsilon_j I_j p_j\\right)^2 \\right]^2} \\right)_{b=1, \\dots, B}\n}\n$$"
        }
    ]
}