{
    "hands_on_practices": [
        {
            "introduction": "The effectiveness of the Exponential Transform hinges on the choice of the biasing parameter $\\alpha$. A poor choice can lead to little or no improvement, while an optimal choice can reduce variance by orders of magnitude. This first practice guides you through the analytical derivation to find the exact value of $\\alpha$ that minimizes the variance for a simple transmission problem, providing a theoretical target for achieving maximum efficiency. ",
            "id": "4224668",
            "problem": "Consider one-speed neutral particle transport through a homogeneous planar slab of thickness $d$ and macroscopic total cross section $\\Sigma_{t}0$. The slab is purely absorbing (no scattering and no sources within the slab). A monoenergetic, mono-directional particle starts at $x=0$ and moves along the slab normal toward $x=d$.\n\nFundamental facts to be used:\n- In a homogeneous absorber, the free-path length $s$ to the first collision has the exponential probability density function (PDF) $f(s)=\\Sigma_{t}\\exp(-\\Sigma_{t} s)$ for $s\\geq 0$.\n- The leakage (transmission) probability through the slab is the probability that the first collision distance exceeds $d$, namely $P_{\\mathrm{leak}}=\\exp(-\\Sigma_{t} d)$.\n- In Monte Carlo (MC) simulation, variance reduction by importance sampling must preserve unbiasedness via a likelihood ratio. If a biased PDF $f_{\\alpha}(s)$ is used to sample $s$, then an unbiased estimator of any event functional is obtained by multiplying the event indicator by the likelihood ratio $L(s)=f(s)/f_{\\alpha}(s)$.\n\nTo estimate $P_{\\mathrm{leak}}$ using the exponential transform path-stretching method, introduce a bias parameter $\\alpha$ with the constraint $\\alpha1$ and replace $f(s)$ by the biased PDF\n$$\nf_{\\alpha}(s)=(1-\\alpha)\\,\\Sigma_{t}\\,\\exp\\!\\big(-(1-\\alpha)\\,\\Sigma_{t}\\,s\\big), \\quad s\\geq 0,\n$$\nwhile using the unbiased leakage estimator\n$$\nY_{\\alpha}=\\mathbf{1}_{\\{sd\\}}\\,L(s), \\quad L(s)=\\frac{f(s)}{f_{\\alpha}(s)}.\n$$\nHere $\\mathbf{1}_{\\{sd\\}}$ is the indicator of the event that the sampled free path exceeds the slab thickness.\n\nStarting only from the fundamental facts and definitions above, and without introducing any additional pre-derived formulas, do the following:\n\n1. Derive a closed-form expression for the variance $\\mathrm{Var}[Y_{\\alpha}]$ as a function of $\\alpha$, $\\Sigma_{t}$, and $d$.\n2. Determine, for $\\alpha1$, the value $\\alpha^{\\star}$ that minimizes $\\mathrm{Var}[Y_{\\alpha}]$. Express your result in closed form as a function of the optical thickness $\\Sigma_{t}d$.\n3. Based on your expression for $\\alpha^{\\star}$, explain qualitatively how the optimum behaves as $\\Sigma_{t} d$ increases, but report only the exact closed-form $\\alpha^{\\star}$ as your final answer.\n\nReport the final answer as a single, closed-form analytical expression for $\\alpha^{\\star}$, with no units. Do not round.",
            "solution": "The problem asks for the derivation of the variance of a Monte Carlo estimator for particle leakage, the optimization of a biasing parameter, and a qualitative analysis of the result. We will address these three parts in sequence.\n\nFirst, the problem must be validated.\n**Step 1: Extract Givens**\n- One-speed neutral particle transport, homogeneous planar slab, thickness $d$, macroscopic total cross section $\\Sigma_{t}0$.\n- Purely absorbing slab (no scattering, no internal sources).\n- Monoenergetic, mono-directional particle starts at $x=0$, moves toward $x=d$.\n- Free-path length probability density function (PDF): $f(s)=\\Sigma_{t}\\exp(-\\Sigma_{t} s)$ for $s\\geq 0$.\n- Leakage (transmission) probability: $P_{\\mathrm{leak}}=\\exp(-\\Sigma_{t} d)$.\n- Biased PDF for importance sampling: $f_{\\alpha}(s)=(1-\\alpha)\\,\\Sigma_{t}\\,\\exp(-(1-\\alpha)\\,\\Sigma_{t}\\,s)$, with $s\\geq 0$ and $\\alpha1$.\n- Unbiased leakage estimator: $Y_{\\alpha}=\\mathbf{1}_{\\{sd\\}}\\,L(s)$, where $L(s)=f(s)/f_{\\alpha}(s)$ is the likelihood ratio and $\\mathbf{1}_{\\{sd\\}}$ is the indicator function for the event $sd$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard exercise in the theory of Monte Carlo methods for particle transport, specifically concerning the exponential transform variance reduction technique. It is well-posed, with all necessary definitions and constraints provided for a unique mathematical solution. The language is objective and precise. The problem is self-contained and free of contradictions or scientifically unsound premises.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\n**1. Derivation of the Variance $\\mathrm{Var}[Y_{\\alpha}]$**\n\nThe variance of the random variable $Y_{\\alpha}$, where the expectation $\\mathrm{E}_{\\alpha}[\\cdot]$ is taken with respect to the biased PDF $f_{\\alpha}(s)$, is given by the formula:\n$$\n\\mathrm{Var}[Y_{\\alpha}] = \\mathrm{E}_{\\alpha}[Y_{\\alpha}^2] - (\\mathrm{E}_{\\alpha}[Y_{\\alpha}])^2\n$$\nWe first compute the mean of the estimator, $\\mathrm{E}_{\\alpha}[Y_{\\alpha}]$. By definition,\n$$\n\\mathrm{E}_{\\alpha}[Y_{\\alpha}] = \\int_{0}^{\\infty} Y_{\\alpha}(s) f_{\\alpha}(s) \\, ds = \\int_{0}^{\\infty} \\mathbf{1}_{\\{sd\\}} L(s) f_{\\alpha}(s) \\, ds\n$$\nSubstituting $L(s) = f(s)/f_{\\alpha}(s)$, we get:\n$$\n\\mathrm{E}_{\\alpha}[Y_{\\alpha}] = \\int_{0}^{\\infty} \\mathbf{1}_{\\{sd\\}} \\frac{f(s)}{f_{\\alpha}(s)} f_{\\alpha}(s) \\, ds = \\int_{0}^{\\infty} \\mathbf{1}_{\\{sd\\}} f(s) \\, ds\n$$\nThe indicator function restricts the integration domain to $sd$:\n$$\n\\mathrm{E}_{\\alpha}[Y_{\\alpha}] = \\int_{d}^{\\infty} f(s) \\, ds = \\int_{d}^{\\infty} \\Sigma_{t}\\exp(-\\Sigma_{t} s) \\, ds = \\left[-\\exp(-\\Sigma_{t} s)\\right]_{d}^{\\infty} = 0 - (-\\exp(-\\Sigma_{t} d)) = \\exp(-\\Sigma_{t} d)\n$$\nThis confirms that the estimator is unbiased, i.e., its expected value is the true leakage probability $P_{\\mathrm{leak}}$. The square of the mean is therefore $(\\mathrm{E}_{\\alpha}[Y_{\\alpha}])^2 = (\\exp(-\\Sigma_{t} d))^2 = \\exp(-2\\Sigma_{t} d)$.\n\nNext, we compute the second moment, $\\mathrm{E}_{\\alpha}[Y_{\\alpha}^2]$.\n$$\n\\mathrm{E}_{\\alpha}[Y_{\\alpha}^2] = \\int_{0}^{\\infty} Y_{\\alpha}^2(s) f_{\\alpha}(s) \\, ds = \\int_{0}^{\\infty} (\\mathbf{1}_{\\{sd\\}} L(s))^2 f_{\\alpha}(s) \\, ds\n$$\nSince $(\\mathbf{1}_{\\{sd\\}})^2 = \\mathbf{1}_{\\{sd\\}}$, we have:\n$$\n\\mathrm{E}_{\\alpha}[Y_{\\alpha}^2] = \\int_{d}^{\\infty} (L(s))^2 f_{\\alpha}(s) \\, ds\n$$\nFirst, we express the likelihood ratio $L(s)$ explicitly:\n$$\nL(s) = \\frac{f(s)}{f_{\\alpha}(s)} = \\frac{\\Sigma_{t}\\exp(-\\Sigma_{t} s)}{(1-\\alpha)\\Sigma_{t}\\exp(-(1-\\alpha)\\Sigma_{t}s)} = \\frac{1}{1-\\alpha} \\exp(-\\Sigma_{t}s + (1-\\alpha)\\Sigma_{t}s) = \\frac{1}{1-\\alpha} \\exp(-\\alpha\\Sigma_{t}s)\n$$\nNow we substitute $L(s)$ and $f_{\\alpha}(s)$ into the integral for the second moment:\n$$\n\\mathrm{E}_{\\alpha}[Y_{\\alpha}^2] = \\int_{d}^{\\infty} \\left(\\frac{1}{1-\\alpha} \\exp(-\\alpha\\Sigma_{t}s)\\right)^2 \\left((1-\\alpha)\\Sigma_{t}\\exp(-(1-\\alpha)\\Sigma_{t}s)\\right) \\, ds\n$$\n$$\n= \\int_{d}^{\\infty} \\frac{1}{(1-\\alpha)^2} \\exp(-2\\alpha\\Sigma_{t}s) (1-\\alpha)\\Sigma_{t}\\exp(-(1-\\alpha)\\Sigma_{t}s) \\, ds\n$$\n$$\n= \\frac{\\Sigma_{t}}{1-\\alpha} \\int_{d}^{\\infty} \\exp\\big(-2\\alpha\\Sigma_{t}s - (1-\\alpha)\\Sigma_{t}s\\big) \\, ds\n$$\nThe exponent simplifies to $-2\\alpha\\Sigma_{t}s - \\Sigma_{t}s + \\alpha\\Sigma_{t}s = -(1+\\alpha)\\Sigma_{t}s$. The integral becomes:\n$$\n\\mathrm{E}_{\\alpha}[Y_{\\alpha}^2] = \\frac{\\Sigma_{t}}{1-\\alpha} \\int_{d}^{\\infty} \\exp(-(1+\\alpha)\\Sigma_{t}s) \\, ds\n$$\nFor this integral to converge, the argument of the exponential must be negative, which requires $(1+\\alpha)\\Sigma_{t}  0$. Since $\\Sigma_{t}0$, we must have $1+\\alpha0$, or $\\alpha  -1$. Evaluating the integral:\n$$\n\\int_{d}^{\\infty} \\exp(-(1+\\alpha)\\Sigma_{t}s) \\, ds = \\left[ \\frac{\\exp(-(1+\\alpha)\\Sigma_{t}s)}{-(1+\\alpha)\\Sigma_{t}} \\right]_d^{\\infty} = 0 - \\frac{\\exp(-(1+\\alpha)\\Sigma_{t}d)}{-(1+\\alpha)\\Sigma_{t}} = \\frac{\\exp(-(1+\\alpha)\\Sigma_{t}d)}{(1+\\alpha)\\Sigma_{t}}\n$$\nSubstituting this result back into the expression for the second moment:\n$$\n\\mathrm{E}_{\\alpha}[Y_{\\alpha}^2] = \\frac{\\Sigma_{t}}{1-\\alpha} \\cdot \\frac{\\exp(-(1+\\alpha)\\Sigma_{t}d)}{(1+\\alpha)\\Sigma_{t}} = \\frac{\\exp(-(1+\\alpha)\\Sigma_{t}d)}{(1-\\alpha)(1+\\alpha)} = \\frac{\\exp(-(1+\\alpha)\\Sigma_{t}d)}{1-\\alpha^2}\n$$\nFinally, we assemble the variance:\n$$\n\\mathrm{Var}[Y_{\\alpha}] = \\mathrm{E}_{\\alpha}[Y_{\\alpha}^2] - (\\mathrm{E}_{\\alpha}[Y_{\\alpha}])^2 = \\frac{\\exp(-(1+\\alpha)\\Sigma_{t}d)}{1-\\alpha^2} - \\exp(-2\\Sigma_{t}d)\n$$\n\n**2. Determination of the Optimal Parameter $\\alpha^{\\star}$**\n\nTo find the value of $\\alpha$ that minimizes $\\mathrm{Var}[Y_{\\alpha}]$, we differentiate the variance with respect to $\\alpha$ and set the derivative to zero. Let's denote the optical thickness as $T = \\Sigma_{t}d  0$. The variance is:\n$$\nV(\\alpha) = \\frac{\\exp(-(1+\\alpha)T)}{1-\\alpha^2} - \\exp(-2T)\n$$\nWe need to solve $\\frac{dV}{d\\alpha} = 0$. The term $\\exp(-2T)$ is constant with respect to $\\alpha$.\n$$\n\\frac{dV}{d\\alpha} = \\frac{d}{d\\alpha} \\left( \\frac{\\exp(-T - \\alpha T)}{1-\\alpha^2} \\right) = 0\n$$\nUsing the quotient rule for differentiation, $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$:\n$$\n\\frac{(-T \\exp(-T-\\alpha T))(1-\\alpha^2) - (\\exp(-T-\\alpha T))(-2\\alpha)}{(1-\\alpha^2)^2} = 0\n$$\nSince $\\exp(-T-\\alpha T)$ is always positive and the denominator is non-zero for $\\alpha \\in (-1, 1)$, we can simplify the equation by setting the numerator to zero:\n$$\n-T(1-\\alpha^2) + 2\\alpha = 0\n$$\n$$\n-T + T\\alpha^2 + 2\\alpha = 0 \\implies T\\alpha^2 + 2\\alpha - T = 0\n$$\nThis is a quadratic equation for $\\alpha$. Using the quadratic formula $\\alpha = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ with $a=T$, $b=2$, and $c=-T$:\n$$\n\\alpha = \\frac{-2 \\pm \\sqrt{2^2 - 4(T)(-T)}}{2T} = \\frac{-2 \\pm \\sqrt{4 + 4T^2}}{2T} = \\frac{-2 \\pm 2\\sqrt{1+T^2}}{2T} = \\frac{-1 \\pm \\sqrt{1+T^2}}{T}\n$$\nThis gives two possible solutions:\n$$\n\\alpha_1 = \\frac{-1 + \\sqrt{1+T^2}}{T} \\quad \\text{and} \\quad \\alpha_2 = \\frac{-1 - \\sqrt{1+T^2}}{T}\n$$\nWe must check these solutions against the constraints $\\alpha  1$ and $\\alpha  -1$. For $T  0$, $\\sqrt{1+T^2}  \\sqrt{T^2} = T$, and $\\sqrt{1+T^2}  \\sqrt{1+2T+T^2} = 1+T$.\nFor $\\alpha_1$:\nThe numerator satisfies $0  -1 + \\sqrt{1+T^2}  T$. Since $T0$, dividing by $T$ gives $0  \\alpha_1  1$. This solution is within the valid range.\nFor $\\alpha_2$:\nThe numerator is $-1 - \\sqrt{1+T^2}  -2$. Thus, $\\alpha_2 = \\frac{-1 - \\sqrt{1+T^2}}{T}$ is negative. Further analysis shows that $\\alpha_2  -1$ for all $T0$. This solution lies outside the domain where the variance is finite.\nThe second derivative test confirms that $\\alpha_1$ corresponds to a minimum. Therefore, the optimal parameter is $\\alpha^{\\star} = \\alpha_1$. Substituting $T=\\Sigma_{t}d$:\n$$\n\\alpha^{\\star} = \\frac{-1 + \\sqrt{1 + (\\Sigma_t d)^2}}{\\Sigma_t d}\n$$\n\n**3. Qualitative Behavior of $\\alpha^{\\star}$**\n\nWe analyze the behavior of $\\alpha^{\\star}$ as a function of the optical thickness $T = \\Sigma_{t} d$.\n- As the optical thickness becomes very large ($T \\to \\infty$), leakage becomes a rare event. We examine the limit:\n$$\n\\lim_{T\\to\\infty} \\alpha^{\\star} = \\lim_{T\\to\\infty} \\frac{-1 + \\sqrt{1+T^2}}{T} = \\lim_{T\\to\\infty} \\frac{T\\sqrt{1/T^2+1}-1}{T} = \\lim_{T\\to\\infty} \\left(\\sqrt{1/T^2+1} - 1/T\\right) = 1\n$$\nSo, $\\alpha^{\\star}$ approaches $1$. An $\\alpha$ value close to $1$ corresponds to a heavily biased (\"stretched\") path length distribution that preferentially samples long paths, increasing the likelihood of observing the rare leakage event in a simulation.\n- As the optical thickness becomes very small ($T \\to 0$), leakage is a very probable event. Using the Taylor expansion $\\sqrt{1+x} \\approx 1+\\frac{1}{2}x$ for small $x$:\n$$\n\\alpha^{\\star} \\approx \\frac{-1 + (1 + \\frac{1}{2}T^2)}{T} = \\frac{\\frac{1}{2}T^2}{T} = \\frac{1}{2}T\n$$\nSo, $\\alpha^{\\star}$ approaches $0$. An $\\alpha$ value close to $0$ corresponds to almost no biasing, which is appropriate when the event of interest is not rare.\nIn summary, as the slab's optical thickness $\\Sigma_t d$ increases, the optimal biasing parameter $\\alpha^{\\star}$ increases from $0$ towards $1$, reflecting the need for stronger importance sampling to efficiently simulate the increasingly rare event of particle transmission.",
            "answer": "$$\\boxed{\\frac{-1 + \\sqrt{1 + (\\Sigma_{t} d)^{2}}}{\\Sigma_{t} d}}$$"
        },
        {
            "introduction": "Theoretical derivations are powerful, but seeing the numbers brings the concept to life. This exercise moves from the abstract formula for variance to a concrete numerical calculation for a challenging deep-penetration scenario. By computing and comparing the variance for an analog simulation versus Exponential Transform simulations with different biasing parameters, you will gain a tangible appreciation for the dramatic performance gains this technique offers. ",
            "id": "4224672",
            "problem": "A monoenergetic neutron beam traverses a purely absorbing slab of thickness $d$ with macroscopic total cross section $\\Sigma_{t}$. The transmission probability through the slab is governed by exponential attenuation. In Monte Carlo (MC) neutron transport, the analog sampling of flight distance $s$ to collision uses the exponential distribution consistent with the attenuation law. The Exponential Transform (ET) path-stretching technique modifies the sampling of $s$ by biasing the exponential distribution with a parameter $\\alpha \\in [0,1)$ while compensating with an appropriate statistical weight to preserve unbiased estimates.\n\nStarting only from the fundamental attenuation law and the definition of analog MC sampling for an exponential collision process, do the following:\n\n1. Derive the analog MC estimator for the transmission probability $T$ through the slab and its variance in terms of $\\Sigma_{t}$ and $d$.\n2. Define a biased exponential sampling law for the flight distance $s$ based on a path-stretching parameter $\\alpha \\in [0,1)$, and derive the corresponding statistical weight that ensures unbiased estimation of $T$.\n3. Using this biased law and weight, derive a closed-form expression for the variance of the ET estimator as a function of $\\alpha$, $\\Sigma_{t}$, and $d$.\n4. For the deep-penetration regime with $\\Sigma_{t} d = 10$, compute numerically the analog variance and the ET variance for $\\alpha = 0$, $\\alpha = 0.5$, and $\\alpha = 0.9$. Round all numerical variances to four significant figures.\n5. Interpret the numerical results in terms of practical variance reduction in deep penetration.\n6. Determine analytically the value of $\\alpha$ that minimizes the ET variance for a general product $c \\equiv \\Sigma_{t} d  0$, and then evaluate this optimal $\\alpha$ for $c = 10$. Express the optimal $\\alpha$ as a single number rounded to four significant figures.\n\nAll answers are dimensionless; do not include units. The final reported number should be the optimal $\\alpha$ rounded to four significant figures.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a complete solution. It describes a standard variance reduction technique, the Exponential Transform, used in Monte Carlo particle transport simulations. We shall proceed by deriving the required quantities based on fundamental principles.\n\nLet $\\Sigma_t$ be the macroscopic total cross section, $d$ be the slab thickness, and $s$ be the neutron flight distance. The analysis begins with the fundamental law of radiation attenuation.\n\nThe probability that a neutron travels a distance $s$ without a collision is given by $P(\\text{flight}  s) = \\exp(-\\Sigma_t s)$. The corresponding probability density function (PDF) for the flight distance $s$ to the first collision is obtained by differentiating the cumulative distribution function $F(s) = P(\\text{flight} \\le s) = 1 - \\exp(-\\Sigma_t s)$:\n$$\np(s) = \\frac{dF(s)}{ds} = \\Sigma_t \\exp(-\\Sigma_t s), \\quad s \\ge 0\n$$\n\nThe analytical transmission probability $T$ through a purely absorbing slab of thickness $d$ is the probability that the first-flight distance $s$ is greater than $d$:\n$$\nT = P(s  d) = \\int_{d}^{\\infty} p(s) \\,ds = \\int_{d}^{\\infty} \\Sigma_t \\exp(-\\Sigma_t s) \\,ds = [-\\exp(-\\Sigma_t s)]_{d}^{\\infty} = \\exp(-\\Sigma_t d)\n$$\n\nWe now address the six parts of the problem.\n\n**1. Analog MC Estimator and Variance**\n\nIn an analog Monte Carlo simulation, we sample the flight distance $s$ directly from the physical PDF, $p(s)$. For the transmission problem, a history's score, $\\xi_{an}$, is $1$ if the neutron passes through the slab ($s  d$) and $0$ if it collides within the slab ($s \\le d$). This is a Bernoulli random variable.\n\nThe expected value of the analog estimator $\\xi_{an}$ is:\n$$\nE[\\xi_{an}] = (1) \\cdot P(s  d) + (0) \\cdot P(s \\le d) = P(s  d) = \\exp(-\\Sigma_t d) = T\n$$\nSince $E[\\xi_{an}] = T$, the estimator is unbiased.\n\nThe variance of a single history's score, $\\text{Var}(\\xi_{an})$, is given by $\\text{Var}(\\xi_{an}) = E[\\xi_{an}^2] - (E[\\xi_{an}])^2$. For a Bernoulli variable where $\\xi_{an}$ is either $0$ or $1$, we have $\\xi_{an}^2 = \\xi_{an}$. Therefore, $E[\\xi_{an}^2] = E[\\xi_{an}] = T$. The variance is:\n$$\n\\text{Var}(\\xi_{an}) = T - T^2 = T(1 - T) = \\exp(-\\Sigma_t d) \\left(1 - \\exp(-\\Sigma_t d)\\right)\n$$\n\n**2. Biased Sampling Law and Statistical Weight**\n\nThe Exponential Transform (ET) method introduces a biased PDF, $p^*(s)$, to sample the flight distance $s$. This is achieved by modifying the total cross section to a biased value $\\Sigma_t' = (1-\\alpha)\\Sigma_t$, where $\\alpha \\in [0,1)$ is the path-stretching parameter. The condition $\\alpha  1$ ensures that $\\Sigma_t'  0$.\n\nThe biased sampling law (PDF) is:\n$$\np^*(s) = \\Sigma_t' \\exp(-\\Sigma_t' s) = (1-\\alpha)\\Sigma_t \\exp\\left(-(1-\\alpha)\\Sigma_t s\\right)\n$$\nTo ensure the final estimate remains unbiased, each particle score must be multiplied by a statistical weight, $w(s)$, which is the ratio of the true PDF to the biased PDF:\n$$\nw(s) = \\frac{p(s)}{p^*(s)} = \\frac{\\Sigma_t \\exp(-\\Sigma_t s)}{(1-\\alpha)\\Sigma_t \\exp\\left(-(1-\\alpha)\\Sigma_t s\\right)} = \\frac{1}{1-\\alpha} \\exp(-\\Sigma_t s + (1-\\alpha)\\Sigma_t s)\n$$\n$$\nw(s) = \\frac{1}{1-\\alpha} \\exp(-\\alpha \\Sigma_t s)\n$$\n\nThe ET estimator for transmission, $\\xi_{ET}$, is the weight $w(s)$ if the particle transmits ($s  d$) and $0$ otherwise. The expected value is $E[\\xi_{ET}] = \\int_d^\\infty w(s) p^*(s) \\,ds = \\int_d^\\infty p(s) \\,ds = T$, confirming the estimator is unbiased.\n\n**3. Variance of the ET Estimator**\n\nThe variance of the ET estimator is $\\text{Var}(\\xi_{ET}) = E[\\xi_{ET}^2] - (E[\\xi_{ET}])^2$. We have $E[\\xi_{ET}] = T$. We now compute the second moment, $E[\\xi_{ET}^2]$:\n$$\nE[\\xi_{ET}^2] = \\int_0^\\infty (\\xi_{ET}(s))^2 p^*(s) \\,ds = \\int_d^\\infty (w(s))^2 p^*(s) \\,ds\n$$\nSubstituting the expressions for $w(s)$ and $p^*(s)$:\n$$\nE[\\xi_{ET}^2] = \\int_d^\\infty \\left( \\frac{1}{1-\\alpha} \\exp(-\\alpha \\Sigma_t s) \\right)^2 \\left( (1-\\alpha)\\Sigma_t \\exp(-(1-\\alpha)\\Sigma_t s) \\right) \\,ds\n$$\n$$\nE[\\xi_{ET}^2] = \\int_d^\\infty \\frac{1}{(1-\\alpha)^2} \\exp(-2\\alpha \\Sigma_t s) (1-\\alpha)\\Sigma_t \\exp(-(1-\\alpha)\\Sigma_t s) \\,ds\n$$\n$$\nE[\\xi_{ET}^2] = \\frac{\\Sigma_t}{1-\\alpha} \\int_d^\\infty \\exp\\left( -2\\alpha \\Sigma_t s - (1-\\alpha)\\Sigma_t s \\right) \\,ds\n$$\nThe exponent is $-2\\alpha \\Sigma_t s - \\Sigma_t s + \\alpha \\Sigma_t s = -(1+\\alpha)\\Sigma_t s$.\n$$\nE[\\xi_{ET}^2] = \\frac{\\Sigma_t}{1-\\alpha} \\int_d^\\infty \\exp\\left( -(1+\\alpha)\\Sigma_t s \\right) \\,ds\n$$\n$$\nE[\\xi_{ET}^2] = \\frac{\\Sigma_t}{1-\\alpha} \\left[ \\frac{\\exp(-(1+\\alpha)\\Sigma_t s)}{-(1+\\alpha)\\Sigma_t} \\right]_d^\\infty = \\frac{\\Sigma_t}{1-\\alpha} \\left( 0 - \\frac{\\exp(-(1+\\alpha)\\Sigma_t d)}{-(1+\\alpha)\\Sigma_t} \\right)\n$$\n$$\nE[\\xi_{ET}^2] = \\frac{1}{(1-\\alpha)(1+\\alpha)} \\exp(-(1+\\alpha)\\Sigma_t d) = \\frac{\\exp(-(1+\\alpha)\\Sigma_t d)}{1-\\alpha^2}\n$$\nThe variance of the ET estimator as a function of $\\alpha$, $\\Sigma_t$, and $d$ is:\n$$\n\\text{Var}(\\xi_{ET}) = E[\\xi_{ET}^2] - T^2 = \\frac{\\exp(-(1+\\alpha)\\Sigma_t d)}{1-\\alpha^2} - (\\exp(-\\Sigma_t d))^2\n$$\n$$\n\\text{Var}(\\xi_{ET}) = \\frac{\\exp(-\\Sigma_t d) \\exp(-\\alpha \\Sigma_t d)}{1-\\alpha^2} - \\exp(-2\\Sigma_t d)\n$$\n\n**4. Numerical Variances for $\\Sigma_t d = 10$**\n\nWe are given $c \\equiv \\Sigma_t d = 10$. The transmission is $T = \\exp(-10)$.\n\nThe analog variance ($\\alpha=0$) is:\n$$\n\\text{Var}(\\xi_{an}) = \\exp(-10)(1 - \\exp(-10)) \\approx (4.53999 \\times 10^{-5})(1 - 4.53999 \\times 10^{-5}) \\approx 4.5398 \\times 10^{-5}\n$$\nRounding to four significant figures, the analog variance is $4.540 \\times 10^{-5}$. Note that for $\\alpha=0$, the ET variance formula correctly reduces to the analog variance:\n$$\n\\text{Var}(\\xi_{ET})|_{\\alpha=0} = \\frac{\\exp(-10)}{1-0} - \\exp(-20) = \\exp(-10)(1 - \\exp(-10))\n$$\n\nFor $\\alpha = 0.5$:\n$$\n\\text{Var}(\\xi_{ET})|_{\\alpha=0.5} = \\frac{\\exp(-(1+0.5)10)}{1-0.5^2} - \\exp(-2 \\cdot 10) = \\frac{\\exp(-15)}{1-0.25} - \\exp(-20)\n$$\n$$\n\\text{Var}(\\xi_{ET})|_{\\alpha=0.5} = \\frac{\\exp(-15)}{0.75} - \\exp(-20) \\approx \\frac{3.0590 \\times 10^{-7}}{0.75} - 2.0612 \\times 10^{-9} \\approx 4.0787 \\times 10^{-7} - 2.0612 \\times 10^{-9} \\approx 4.058 \\times 10^{-7}\n$$\n\nFor $\\alpha = 0.9$:\n$$\n\\text{Var}(\\xi_{ET})|_{\\alpha=0.9} = \\frac{\\exp(-(1+0.9)10)}{1-0.9^2} - \\exp(-20) = \\frac{\\exp(-19)}{1-0.81} - \\exp(-20)\n$$\n$$\n\\text{Var}(\\xi_{ET})|_{\\alpha=0.9} = \\frac{\\exp(-19)}{0.19} - \\exp(-20) \\approx \\frac{5.5891 \\times 10^{-9}}{0.19} - 2.0612 \\times 10^{-9} \\approx 2.9416 \\times 10^{-8} - 2.0612 \\times 10^{-9} \\approx 2.736 \\times 10^{-8}\n$$\n\n**5. Interpretation of Results**\n\nThe numerical results are:\n- Analog variance ($\\alpha=0$): $4.540 \\times 10^{-5}$\n- ET variance ($\\alpha=0.5$): $4.058 \\times 10^{-7}$\n- ET variance ($\\alpha=0.9$): $2.736 \\times 10^{-8}$\n\nFor the deep-penetration problem ($\\Sigma_t d = 10$), analog sampling is highly inefficient because transmission is a rare event ($T \\approx 4.5 \\times 10^{-5}$). Most histories result in a score of $0$, leading to high relative variance. By choosing $\\alpha  0$, the ET method \"stretches\" particle paths by sampling from a distribution with a smaller effective cross section, thus increasing the probability of a particle reaching and passing thickness $d$.\nIncreasing $\\alpha$ from $0$ to $0.5$ reduces the variance by a factor of approximately $112$. Increasing $\\alpha$ further to $0.9$ reduces the variance by a factor of over $1600$ compared to the analog case. This demonstrates that the ET method is a powerful variance reduction technique for deep-penetration problems, with larger values of $\\alpha$ (approaching $1$) yielding progressively greater reductions in variance, provided an optimal value is chosen.\n\n**6. Optimal Path-Stretching Parameter $\\alpha$**\n\nTo find the value of $\\alpha$ that minimizes the variance, we differentiate $\\text{Var}(\\xi_{ET})$ with respect to $\\alpha$ and set the result to zero. Let $c = \\Sigma_t d$.\n$$\n\\text{Var}(\\xi_{ET}) = \\frac{\\exp(-(1+\\alpha)c)}{1-\\alpha^2} - \\exp(-2c)\n$$\nMinimizing the variance is equivalent to minimizing the first term, as the second term $\\exp(-2c)$ is independent of $\\alpha$. Let $f(\\alpha) = \\frac{\\exp(-(1+\\alpha)c)}{1-\\alpha^2}$. We can ignore the constant factor $\\exp(-c)$ and minimize $g(\\alpha) = \\frac{\\exp(-\\alpha c)}{1-\\alpha^2}$.\nUsing the quotient rule for differentiation:\n$$\n\\frac{dg}{d\\alpha} = \\frac{(-c \\exp(-\\alpha c))(1-\\alpha^2) - (\\exp(-\\alpha c))(-2\\alpha)}{(1-\\alpha^2)^2} = 0\n$$\nSince $\\exp(-\\alpha c)  0$ and $(1-\\alpha^2)^2  0$ for $\\alpha \\in [0,1)$, we only need the numerator to be zero:\n$$\n-c(1-\\alpha^2) + 2\\alpha = 0\n$$\n$$\nc\\alpha^2 + 2\\alpha - c = 0\n$$\nThis is a quadratic equation in $\\alpha$. Applying the quadratic formula:\n$$\n\\alpha = \\frac{-2 \\pm \\sqrt{2^2 - 4(c)(-c)}}{2c} = \\frac{-2 \\pm \\sqrt{4 + 4c^2}}{2c} = \\frac{-2 \\pm 2\\sqrt{1+c^2}}{2c} = \\frac{-1 \\pm \\sqrt{1+c^2}}{c}\n$$\nSince $c  0$, the term $\\sqrt{1+c^2}  1$. The solution $\\alpha = \\frac{-1-\\sqrt{1+c^2}}{c}$ is negative and thus outside the domain $\\alpha \\in [0,1)$. The only physically meaningful solution is:\n$$\n\\alpha_{opt} = \\frac{-1 + \\sqrt{1+c^2}}{c}\n$$\nFor this solution to be a minimum, the second derivative should be positive. This can be confirmed, but is a standard result. We must also verify that this $\\alpha_{opt}$ is in the range $[0, 1)$. For $c  0$, $\\sqrt{1+c^2}1$, so $\\alpha_{opt}  0$. Also, $\\sqrt{1+c^2}  \\sqrt{1+2c+c^2} = \\sqrt{(1+c)^2} = 1+c$, which implies $\\sqrt{1+c^2}-1  c$, so $\\alpha_{opt}  1$. Thus, $\\alpha_{opt} \\in (0,1)$.\n\nFor the specific case $c = \\Sigma_t d = 10$:\n$$\n\\alpha_{opt} = \\frac{-1 + \\sqrt{1+10^2}}{10} = \\frac{-1 + \\sqrt{101}}{10}\n$$\nNumerically, $\\sqrt{101} \\approx 10.0498756$.\n$$\n\\alpha_{opt} \\approx \\frac{-1 + 10.0498756}{10} = \\frac{9.0498756}{10} = 0.90498756\n$$\nRounding to four significant figures, the optimal value of $\\alpha$ is $0.9050$.",
            "answer": "$$\\boxed{0.9050}$$"
        },
        {
            "introduction": "The ultimate validation of a simulation technique lies in its computational performance. This final practice bridges the gap between theory and application by tasking you with building a Monte Carlo simulation from the ground up. You will implement the Exponential Transform, use a pilot study to empirically find a good biasing parameter, and evaluate the method's efficiency using the industry-standard Figure of Merit (FOM), demonstrating how theoretical variance reduction translates directly into saved computational time. ",
            "id": "4224733",
            "problem": "Consider a one-dimensional slab shielding benchmark in nuclear reactor simulation. A monoenergetic particle starts at the entrance $x=0$ and travels in the $+x$ direction through a homogeneous, purely absorbing medium of macroscopic total cross section $\\Sigma_t$ measured in $\\mathrm{cm}^{-1}$. The slab has thickness $L$ measured in $\\mathrm{cm}$. The quantity of interest is the transmitted fraction (flux) at $x=L$, which equals the probability that a particle has no interaction before leaving the slab.\n\nFundamental base:\n- Beerâ€“Lambert law of exponential attenuation for a purely absorbing medium states that the survival probability through a path length $s$ is $\\exp(-\\Sigma_t s)$.\n- In analog Monte Carlo sampling, the free path length $s$ is sampled from an exponential distribution with rate $\\Sigma_t$ according to the macroscopic total cross section definition.\n\nExponential transform path stretching is a variance-reduction technique that biases the free-path sampling by reducing the exponential rate and compensates with an appropriate weight to preserve unbiasedness. Let $\\alpha$ be the dimensionless path-stretching parameter, chosen such that $0 \\le \\alpha  1$ to keep the biased sampling well-defined. The biased sampling rate becomes $(1-\\alpha)\\Sigma_t$, and an unbiased estimator for the transmitted fraction must be derived using a rigorous likelihood ratio between the analog and biased sampling laws and the indicator of transmission. No shortcut formulas for the unbiased estimator are provided here; the estimator must be obtained from first principles beginning from the stated fundamental base.\n\nDefine the Figure of Merit (FOM) as follows. Let $N$ denote the number of particle histories (this is the computational effort; treat it as the \"time\" measured in units of histories), $\\hat{\\mu}$ the sample mean estimator of the transmitted fraction, and $\\hat{\\sigma}^2$ its unbiased sample variance. The relative error is $R = \\hat{\\sigma}/|\\hat{\\mu}|$. The Figure of Merit (FOM) is $1/(R^2 N)$ and must be reported in units of $\\mathrm{histories}^{-1}$.\n\nTask:\n1. For a given $(\\Sigma_t, L)$, perform a pilot study over a discrete set of candidate $\\alpha$ values to estimate the FOM for each candidate. Use the pilot to choose the $\\alpha$ that maximizes the estimated FOM. This $\\alpha$ is deemed \"optimal\" under the pilot discretization.\n2. Validate the choice by performing a production run and comparing the FOM of:\n   - the analog case ($\\alpha = 0$),\n   - the pilot-chosen optimal $\\alpha$,\n   - a demonstrably nonoptimal $\\alpha$ selected from the pilot set that has strictly lower pilot FOM than the optimal choice.\n3. For each test case, report:\n   - the dimensionless optimal $\\alpha$,\n   - the production-run FOM for the optimal $\\alpha$ in $\\mathrm{histories}^{-1}$,\n   - the production-run FOM for the analog case in $\\mathrm{histories}^{-1}$,\n   - the production-run FOM for the chosen nonoptimal $\\alpha$ in $\\mathrm{histories}^{-1}$,\n   - a boolean confirming the optimal FOM is strictly greater than the analog FOM,\n   - a boolean confirming the optimal FOM is strictly greater than the nonoptimal FOM.\n\nEstimator and algorithm requirements:\n- Derive the unbiased estimator for the transmitted fraction under exponential transform path stretching starting from the fundamental base above, using the change-of-distribution likelihood ratio combined with the transmission indicator $I[s \\ge L]$ and the biased sampling law with rate $(1-\\alpha)\\Sigma_t$.\n- Implement the Monte Carlo using independent and identically distributed sampling of free path length $s$ from the biased exponential distribution with rate $(1-\\alpha)\\Sigma_t$ for each candidate $\\alpha$, including the analog case by setting $\\alpha = 0$.\n- For each run, compute the sample mean, unbiased sample variance, the relative error $R$, and the FOM $1/(R^2 N)$, all in the units specified above.\n\nTest suite:\nExecute your program for the following test cases, each with its own pilot and production run sizes:\n- Test case $1$: $\\Sigma_t = 1.0\\,\\mathrm{cm}^{-1}$, $L = 10.0\\,\\mathrm{cm}$, $N_{\\text{pilot}} = 20000$ histories, $N_{\\text{prod}} = 100000$ histories.\n- Test case $2$: $\\Sigma_t = 0.2\\,\\mathrm{cm}^{-1}$, $L = 5.0\\,\\mathrm{cm}$, $N_{\\text{pilot}} = 20000$ histories, $N_{\\text{prod}} = 100000$ histories.\n- Test case $3$: $\\Sigma_t = 2.0\\,\\mathrm{cm}^{-1}$, $L = 1.0\\,\\mathrm{cm}$, $N_{\\text{pilot}} = 20000$ histories, $N_{\\text{prod}} = 100000$ histories.\n\nPilot candidate set:\nFor each test case, consider a discrete set of five dimensionless candidate $\\alpha$ values: $\\{0.0, 0.2, 0.5, 0.8, 0.95\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order:\n$[\\alpha_{\\text{opt}}, \\mathrm{FOM}_{\\text{opt}}, \\mathrm{FOM}_{\\text{analog}}, \\mathrm{FOM}_{\\text{nonopt}}, \\text{opt\\_gt\\_analog}, \\text{opt\\_gt\\_nonopt}]$.\nFor example, the overall outer list should appear as $[[\\cdots],[\\cdots],[\\cdots]]$ with numerical values and booleans. All FOM values must be in $\\mathrm{histories}^{-1}$.",
            "solution": "The problem presented is a valid and well-posed exercise in the field of Monte Carlo methods for particle transport. It is scientifically grounded in the principles of radiation physics and statistical estimation, and all necessary parameters and objectives are clearly defined. We shall therefore proceed with a complete solution.\n\nThe primary objective is to evaluate the effectiveness of the exponential transform path stretching variance reduction technique for a one-dimensional, purely absorbing shielding problem. This involves deriving the unbiased estimator, implementing a Monte Carlo simulation, and using it to find an optimal path-stretching parameter $\\alpha$.\n\n**1. Derivation of the Unbiased Estimator**\n\nThe quantity of interest is the transmitted fraction, $\\mu$, which is the probability that a particle travels a distance greater than or equal to the slab thickness $L$ without an interaction. The particle's free path length, $s$, is a random variable.\n\nIn the analog (unbiased) physical model, the probability density function (PDF) for the free path length $s$ is given by the exponential distribution:\n$$\np(s) = \\Sigma_t e^{-\\Sigma_t s}, \\quad s \\ge 0\n$$\nwhere $\\Sigma_t$ is the macroscopic total cross section.\n\nThe transmitted fraction $\\mu$ is the survival probability $P(s \\ge L)$. This can be expressed as the expectation of an indicator function, $I[s \\ge L]$, which is $1$ if $s \\ge L$ and $0$ otherwise.\n$$\n\\mu = E_p[I[s \\ge L]] = \\int_0^\\infty I[s \\ge L] p(s) ds = \\int_L^\\infty \\Sigma_t e^{-\\Sigma_t s} ds = e^{-\\Sigma_t L}\n$$\nThe analog Monte Carlo method estimates $\\mu$ by sampling $s_i$ from $p(s)$ and averaging $I[s_i \\ge L]$. For deep penetration problems where $\\Sigma_t L \\gg 1$, $\\mu$ is very small, and this analog approach is inefficient as most histories result in a score of $0$.\n\nThe exponential transform technique, a form of importance sampling, biases the sampling to increase the probability of transmission. We sample the path length $s'$ from a biased PDF, $p'(s)$:\n$$\np'(s) = (1-\\alpha)\\Sigma_t e^{-(1-\\alpha)\\Sigma_t s}, \\quad s \\ge 0\n$$\nwhere $\\alpha$ is the dimensionless path-stretching parameter, with $0 \\le \\alpha  1$. This biased distribution has a larger mean free path ($1/((1-\\alpha)\\Sigma_t)$) than the analog one ($1/\\Sigma_t$), hence encouraging longer paths.\n\nTo ensure the estimator remains unbiased, we must introduce a weight, $w(s')$, to correct for the change in probability measure. The expectation is rewritten as:\n$$\n\\mu = \\int_0^\\infty I[s \\ge L] p(s) ds = \\int_0^\\infty I[s \\ge L] \\frac{p(s)}{p'(s)} p'(s) ds = E_{p'}\\left[I[s' \\ge L] \\frac{p(s')}{p'(s')}\\right]\n$$\nThe term in the expectation is the score for a single history sampled from $p'(s')$. The weight $w(s')$ is the likelihood ratio:\n$$\nw(s') = \\frac{p(s')}{p'(s')} = \\frac{\\Sigma_t e^{-\\Sigma_t s'}}{(1-\\alpha)\\Sigma_t e^{-(1-\\alpha)\\Sigma_t s'}} = \\frac{1}{1-\\alpha} e^{-[\\Sigma_t - (1-\\alpha)\\Sigma_t]s'} = \\frac{1}{1-\\alpha} e^{-\\alpha\\Sigma_t s'}\n$$\nThus, for each particle history $i$, we sample a path length $s'_i$ from $p'(s')$. The score for that history, $X_i$, is:\n$$\nX_i = \\begin{cases} \\frac{1}{1-\\alpha} e^{-\\alpha\\Sigma_t s'_i}  \\text{if } s'_i \\ge L \\\\ 0  \\text{if } s'_i  L \\end{cases}\n$$\nThis is the unbiased estimator for a single history. The sample mean over $N$ histories, $\\hat{\\mu} = \\frac{1}{N} \\sum_{i=1}^N X_i$, is an unbiased estimator of the true transmitted fraction $\\mu$.\n\n**2. Monte Carlo Algorithm and Figure of Merit (FOM)**\n\nThe simulation proceeds as follows for a given set of parameters $(\\Sigma_t, L, \\alpha)$ and a total of $N$ particle histories:\n1.  For each history $i=1, \\dots, N$, sample a path length $s'_i$ from an exponential distribution with rate $\\Sigma'_t = (1-\\alpha)\\Sigma_t$.\n2.  If $s'_i \\ge L$, calculate the score $X_i = \\frac{1}{1-\\alpha} e^{-\\alpha\\Sigma_t s'_i}$.\n3.  If $s'_i  L$, the score is $X_i = 0$.\n\nAfter running $N$ histories, we compute the following statistics from the set of scores $\\{X_1, X_2, \\dots, X_N\\}$:\n-   **Sample Mean:** $\\hat{\\mu} = \\frac{1}{N} \\sum_{i=1}^N X_i$\n-   **Unbiased Sample Variance:** $\\hat{\\sigma}^2 = \\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\hat{\\mu})^2$\n\nAs per the problem definition, the relative error $R$ and the Figure of Merit (FOM) are defined as:\n-   **Relative Error:** $R = \\frac{\\hat{\\sigma}}{|\\hat{\\mu}|}$\n-   **Figure of Merit:** $\\mathrm{FOM} = \\frac{1}{R^2 N} = \\frac{\\hat{\\mu}^2}{N \\hat{\\sigma}^2}$\n\nThe units of FOM are $\\mathrm{histories}^{-1}$, as required. A higher FOM indicates a more efficient simulation, meaning fewer histories are needed to achieve a given level of statistical precision.\n\n**3. Parameter Optimization and Validation**\n\nThe efficacy of the exponential transform is highly dependent on the choice of $\\alpha$. The procedure to find a suitable $\\alpha$ and validate its performance is:\n\n1.  **Pilot Study:** For a given test case $(\\Sigma_t, L)$, simulations are run with $N_{\\text{pilot}}$ histories for the discrete set of candidate $\\alpha$ values: $\\{0.0, 0.2, 0.5, 0.8, 0.95\\}$. The FOM is estimated for each candidate. The value of $\\alpha$ that yields the highest estimated FOM is designated as the optimal parameter, $\\alpha_{\\text{opt}}$.\n\n2.  **Production Run and Validation:** To obtain more statistically robust results, production runs are performed with a larger number of histories, $N_{\\text{prod}}$. The FOM is calculated for three specific cases:\n    -   The **analog case**: $\\alpha = 0$. This represents the baseline performance without variance reduction.\n    -   The **optimal case**: $\\alpha = \\alpha_{\\text{opt}}$, as determined by the pilot study.\n    -   The **non-optimal case**: $\\alpha = \\alpha_{\\text{nonopt}}$. This is chosen to be the candidate $\\alpha$ from the pilot study that resulted in the lowest FOM (which will typically be the analog case itself, or another poorly performing choice), providing a benchmark for a poor parameter choice.\n\nFinally, we report the key results: $\\alpha_{\\text{opt}}$, and the production-run FOMs for the optimal, analog, and non-optimal cases. We also confirm that the optimal FOM is strictly greater than both the analog and non-optimal FOMs, thereby demonstrating the success of the optimization procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_monte_carlo_simulation(sigma_t, L, alpha, N):\n    \"\"\"\n    Performs a Monte Carlo simulation for the 1D shielding problem.\n\n    Args:\n        sigma_t (float): Macroscopic total cross section (cm^-1).\n        L (float): Slab thickness (cm).\n        alpha (float): Dimensionless path-stretching parameter.\n        N (int): Number of particle histories.\n\n    Returns:\n        tuple: (sample_mean, sample_variance, figure_of_merit)\n    \"\"\"\n    if N = 1:\n        return 0.0, 0.0, 0.0\n\n    if not (0 = alpha  1):\n        raise ValueError(\"alpha must be in the range [0, 1).\")\n\n    biased_rate = (1.0 - alpha) * sigma_t\n    \n    # Generate N random path lengths from the biased exponential distribution\n    # scale = 1/lambda, where lambda is the rate.\n    path_lengths = np.random.exponential(scale=1.0/biased_rate, size=N)\n\n    # Identify transmitted particles\n    transmitted_mask = path_lengths = L\n    transmitted_paths = path_lengths[transmitted_mask]\n\n    # Calculate scores for all histories\n    scores = np.zeros(N)\n    if transmitted_paths.size  0:\n        # The weight is applied only to transmitted particles\n        weight_factor = 1.0 / (1.0 - alpha)\n        weights = weight_factor * np.exp(-alpha * sigma_t * transmitted_paths)\n        scores[transmitted_mask] = weights\n\n    # Calculate statistics\n    mu_hat = np.mean(scores)\n\n    # If mu_hat is 0, it means no particles were transmitted (or scores were zero).\n    # In this case, the variance is 0, and the FOM is 0.\n    if mu_hat == 0.0:\n        return 0.0, 0.0, 0.0\n\n    # Unbiased sample variance (ddof=1 for N-1 in denominator)\n    sigma_sq = np.var(scores, ddof=1)\n    \n    # If variance is 0 but mean is not, this implies a perfect estimator (infinite FOM).\n    # This shouldn't happen for this problem with N > 1.\n    if sigma_sq == 0.0:\n        fom = np.inf\n    else:\n        # Relative error squared: R^2 = (sigma/mu)^2 = sigma^2 / mu^2\n        R_sq = sigma_sq / (mu_hat**2)\n        # Figure of Merit: FOM = 1 / (R^2 * N)\n        fom = 1.0 / (R_sq * N)\n\n    return mu_hat, sigma_sq, fom\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        # (Sigma_t, L, N_pilot, N_prod)\n        (1.0, 10.0, 20000, 100000),\n        (0.2, 5.0, 20000, 100000),\n        (2.0, 1.0, 20000, 100000),\n    ]\n\n    candidate_alphas = [0.0, 0.2, 0.5, 0.8, 0.95]\n    \n    final_results = []\n\n    for sigma_t, L, n_pilot, n_prod in test_cases:\n        # --- Pilot Study ---\n        pilot_results = {}\n\n        for alpha in candidate_alphas:\n            _, _, fom = run_monte_carlo_simulation(sigma_t, L, alpha, n_pilot)\n            pilot_results[alpha] = fom\n        \n        # --- Parameter Selection ---\n        # Find optimal alpha (max FOM in pilot run)\n        alpha_opt = max(pilot_results, key=pilot_results.get)\n        \n        # Find a demonstrably non-optimal alpha (min FOM in pilot run)\n        alpha_nonopt = min(pilot_results, key=pilot_results.get)\n\n        # The analog case is always alpha = 0.0\n        alpha_analog = 0.0\n\n        # --- Production Runs ---\n        _, _, fom_opt = run_monte_carlo_simulation(sigma_t, L, alpha_opt, n_prod)\n        _, _, fom_analog = run_monte_carlo_simulation(sigma_t, L, alpha_analog, n_prod)\n        _, _, fom_nonopt = run_monte_carlo_simulation(sigma_t, L, alpha_nonopt, n_prod)\n\n        # --- Validation  Reporting ---\n        opt_gt_analog = fom_opt  fom_analog\n        opt_gt_nonopt = fom_opt  fom_nonopt\n\n        result_for_case = [\n            alpha_opt,\n            fom_opt,\n            fom_analog,\n            fom_nonopt,\n            opt_gt_analog,\n            opt_gt_nonopt\n        ]\n        final_results.append(result_for_case)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}