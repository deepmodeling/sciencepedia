{
    "hands_on_practices": [
        {
            "introduction": "在蒙特卡罗模拟中应用香农熵的第一步是将连续的中子源分布离散化到空间网格上。本实践旨在探索熵值对这一基本离散化选择的敏感性，通过改变网格分辨率来量化其对熵值的影响。完成这个练习将帮助你理解，计算出的熵并非连续源的绝对内在属性，而是依赖于观测尺度，这对于正确解释收敛性诊断图至关重要。",
            "id": "4248386",
            "problem": "您的任务是设计一项基于与核反应堆模拟相关的概率建模的计算研究，以评估香农熵 $H(p)$ 对空间网格分辨率的敏感性。在反应堆源迭代中，空间变化的源强度通常被概括为网格单元上的离散空间分布，并监测此分布的香农熵以判断收敛。您的程序必须量化对于相同的底层源形状，该度量如何随网格的细化和粗化而变化。\n\n使用以下纯数学设置。考虑一个二维板状域 $D = [-1,1] \\times [-1,1]$，以及在 $D$ 上定义的非负源密度 $s(x,y)$。对于给定的网格分辨率参数 $N \\in \\mathbb{Z}_{>0}$，将 $D$ 划分为 $N \\times N$ 个相等的矩形单元。通过在每个单元的中点评估 $s(x,y)$ 并乘以单元面积来近似计算每个单元中的概率质量，然后在所有单元上进行归一化，以获得离散概率向量 $p = (p_1,\\dots,p_{M})$，其中 $M = N^2$。使用自然对数计算 $p$ 的香农熵，并以纳特 (nats) 为单位表示结果。\n\n待测试的源形状为：\n- 均匀板状源：$s_{\\text{uni}}(x,y) = 1$。\n- 以原点为中心的各向同性高斯源：$s_{\\text{iso}}(x,y) = \\exp\\!\\left(-\\dfrac{x^2 + y^2}{2\\sigma^2}\\right)$，其中 $\\sigma = 0.25$。\n- 以原点为中心的各向异性高斯源：$s_{\\text{aniso}}(x,y) = \\exp\\!\\left(-\\dfrac{1}{2}\\left(\\dfrac{x^2}{\\sigma_x^2} + \\dfrac{y^2}{\\sigma_y^2}\\right)\\right)$，其中 $\\sigma_x = 0.10$ 且 $\\sigma_y = 0.50$。\n\n您的程序必须按规定实现从 $s(x,y)$ 构建离散分布 $p$ 的过程，并为每个测试用例计算以纳特为单位的香农熵 $H(p)$。无需用户输入。\n\n测试套件：\n- 案例 1：$s_{\\text{uni}}$，$N = 1$（极端粗化基准）。\n- 案例 2：$s_{\\text{uni}}$，$N = 64$（基准的强细化）。\n- 案例 3：$s_{\\text{iso}}$，$\\sigma = 0.25$，$N = 4$（粗网格）。\n- 案例 4：$s_{\\text{iso}}$，$\\sigma = 0.25$，$N = 64$（细化网格）。\n- 案例 5：$s_{\\text{aniso}}$，$\\sigma_x = 0.10$，$\\sigma_y = 0.50$，$N = 4$（带各向异性的粗网格）。\n- 案例 6：$s_{\\text{aniso}}$，$\\sigma_x = 0.10$，$\\sigma_y = 0.50$，$N = 64$（带各向异性的细化网格）。\n\n答案规格：\n- 按描述为每个测试用例计算以纳特为单位的 $H(p)$。将每个值四舍五入到小数点后 $6$ 位。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按上述案例的顺序排列（例如，`[h1,h2,h3,h4,h5,h6]`），其中每个 $h_k$ 是以纳特为单位、四舍五入到小数点后 $6$ 位的 $H(p)$ 的十进制表示。\n\n科学和算法约束：\n- 从基本概率定义开始：$D$ 上的概率测度，通过单元上的求积构建离散分布，以及基于自然对数的离散分布信息度量。\n- 您的算法必须在每个单元内使用中点评估和等面积分箱。\n- 域 $D$、源形状和参数是无量纲的；除了熵单位外，没有其他物理单位。熵值必须以纳特表示。",
            "solution": "目标是评估在底层源形状固定的情况下，作为核反应堆模拟中离散源分布扩展的信息论度量，香农熵 $H(p)$ 如何依赖于空间网格的分辨率。我们从基本概率原理和离散分布的香农熵定义开始。\n\n理论基础：\n- 令 $D = [-1,1] \\times [-1,1]$ 表示板状域。一个非负源密度 $s(x,y) \\ge 0$ 通过 $s(x,y)$ 在 $D$ 的子集上的积分来定义 $D$ 上的一个有限测度。\n- 对于给定的网格分辨率 $N \\in \\mathbb{Z}_{>0}$，我们将 $D$ 划分为 $N \\times N$ 个等面积的单元。每个单元的边长为 $\\Delta = \\dfrac{2}{N}$，面积为 $a = \\Delta^2 = \\dfrac{4}{N^2}$。\n- 离散概率向量 $p = (p_1, \\dots, p_M)$（其中 $M = N^2$）是通过使用中点法则近似每个单元中的质量来构建的。令 $(x_i, y_i)$ 为单元 $i$ 的中心。定义未归一化的质量 $m_i = s(x_i, y_i) \\cdot a$。归一化后的概率为\n$$\np_i = \\frac{m_i}{\\sum_{j=1}^{M} m_j} = \\frac{s(x_i,y_i) \\, a}{\\sum_{j=1}^{M} s(x_j,y_j) \\, a} = \\frac{s(x_i,y_i)}{\\sum_{j=1}^{M} s(x_j,y_j)},\n$$\n其中，由于对所有单元而言 $a$ 是常数，因此等式成立。这样就构建了一个与底层源密度和等面积分箱一致的离散分布。\n\n香农熵：\n- 对于离散分布 $p = (p_1,\\dots,p_M)$，以自然单位（纳特）表示的香农熵定义为\n$$\nH(p) = - \\sum_{i=1}^{M} p_i \\ln p_i,\n$$\n其中 $\\ln$ 表示自然对数。该度量量化了分布的信息内容或不确定性。对于给定单元上的均匀分布，该值最大化，并随着分布变得更尖锐而减小。\n\n网格分辨率敏感性（推导）：\n- 考虑近似 $p_i \\approx \\frac{s(x_i,y_i) a}{S}$，其中 $S = \\iint_{D} s(x,y) \\, dx \\, dy$ 是总质量；在中点求积法下，$\\sum_i s(x_i,y_i) a \\approx S$，得出 $p_i \\approx s(x_i,y_i) a / S$。\n- 将其代入熵的定义中可得\n$$\nH(p) \\approx - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln\\left(\\frac{s(x_i,y_i) a}{S}\\right)\n= - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln\\left(\\frac{s(x_i,y_i)}{S}\\right) - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln(a).\n$$\n- 由于 $\\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\approx 1$，第二项简化为 $- \\ln(a)$。第一项是连续微分熵的黎曼和近似\n$$\nh(s) = - \\iint_{D} \\frac{s(x,y)}{S} \\ln\\left(\\frac{s(x,y)}{S}\\right) \\, dx \\, dy.\n$$\n- 因此，当网格被均匀细化时（即 $a \\to 0$ 且 $M \\to \\infty$），离散熵的行为类似于\n$$\nH(p) \\approx h(s) - \\ln(a) = h(s) + \\ln\\left(\\frac{1}{a}\\right).\n$$\n- 因为在我们的域中 $a = \\frac{4}{N^2}$，我们得到\n$$\nH(p) \\approx h(s) + \\ln\\left(\\frac{N^2}{4}\\right) = h(s) + 2 \\ln(N) - \\ln(4).\n$$\n- 这表明对于固定的源形状 $s(x,y)$，$H(p)$ 在 $\\ln(N)$ 中近似线性增长，其加性常数取决于域面积和归一化连续密度的微分熵。对于单元上完全均匀的源，对所有 $i$ 都有 $p_i = \\frac{1}{M}$ 且 $H(p) = \\ln(M) = 2 \\ln(N)$，这与预期的最大值完全匹配。对于有峰值的源，$H(p)$ 小于均匀情况下的界限，但由于 $-\\ln(a)$ 项的存在，它仍然随着细化而增加。\n\n算法设计：\n- 构造 $D$ 上 $N \\times N$ 个单元的中心 $(x_i,y_i)$，使用中点坐标：$x_k = -1 + \\left(k + \\frac{1}{2}\\right)\\Delta$ 和 $y_\\ell = -1 + \\left(\\ell + \\frac{1}{2}\\right)\\Delta$，其中 $k,\\ell \\in \\{0,\\dots,N-1\\}$ 且 $\\Delta = \\frac{2}{N}$。\n- 为每个源形状评估 $s(x_i,y_i)$：\n  - 均匀源：$s_{\\text{uni}}(x,y) = 1$。\n  - 各向同性高斯源：$s_{\\text{iso}}(x,y) = \\exp\\!\\left(-\\frac{x^2 + y^2}{2 \\sigma^2}\\right)$，其中 $\\sigma = 0.25$。\n  - 各向异性高斯源：$s_{\\text{aniso}}(x,y) = \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x^2}{\\sigma_x^2} + \\frac{y^2}{\\sigma_y^2}\\right)\\right)$，其中 $\\sigma_x = 0.10$ 且 $\\sigma_y = 0.50$。\n- 形成未归一化向量 $m_i = s(x_i,y_i)$（常数面积因子在归一化中抵消），然后归一化得到 $p_i = \\frac{m_i}{\\sum_j m_j}$。\n- 通过 $H(p) = - \\sum_i p_i \\ln p_i$ 计算以纳特为单位的 $H(p)$。\n- 应用于测试套件案例，并将每个结果四舍五入到小数点后 $6$ 位。\n\n预期行为和边缘情况：\n- $N=1$ 的案例 1 产生 $M=1$ 和一个单单元分布 $p_1=1$，因此 $H(p)=0$ 纳特，代表了极端粗化的基准。\n- 对于均匀源，$H(p)$ 精确地趋近于 $2 \\ln(N)$；因此从 $N=1$ 到 $N=64$，它会显著增加。\n- 对于高斯源，对于相同的 $N$，$H(p)$ 严格小于均匀分布的最大值，并因空间变异性的更精细分辨率而随细化增加。\n- 对于相同的 $N$，如果各向异性增加了沿某一轴的集中度，则各向异性高斯源相对于各向同性源表现出熵的降低，但仍然显示出对 $N$ 的加性 $-\\ln(a)$ 敏感性。\n\n最终的程序实现了这些步骤，计算了所需的以纳特为单位的熵，四舍五入到小数点后 6 位，并按规定顺序输出单行带方括号的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_grid_centers(N, domain_min=-1.0, domain_max=1.0):\n    # Compute midpoints of N bins along each axis over [domain_min, domain_max]\n    length = domain_max - domain_min\n    delta = length / N\n    # Midpoints: domain_min + (k + 0.5) * delta\n    coords = domain_min + (np.arange(N) + 0.5) * delta\n    X, Y = np.meshgrid(coords, coords, indexing='xy')\n    return X, Y\n\ndef s_uniform(x, y):\n    return np.ones_like(x)\n\ndef s_isotropic_gaussian(x, y, sigma=0.25):\n    return np.exp(-((x**2 + y**2) / (2.0 * sigma**2)))\n\ndef s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50):\n    return np.exp(-0.5 * ((x / sigma_x)**2 + (y / sigma_y)**2))\n\ndef discrete_entropy_from_density(N, source_fn):\n    # Construct grid\n    X, Y = make_grid_centers(N)\n    # Evaluate source\n    S = source_fn(X, Y)\n    # Normalize to probabilities; area factor cancels as it's constant for all bins\n    total = np.sum(S)\n    # Guard against pathological zero total (should not occur for provided sources)\n    if total == 0.0:\n        # If total is zero, define zero-entropy (degenerate); but avoid division by zero.\n        return 0.0\n    p = S / total\n    # Compute Shannon entropy in nats: -sum p ln p, treating p=0 terms as 0\n    # Since Gaussian and uniform are positive, zeros should not occur; still robust handling:\n    mask = p > 0.0\n    H = -np.sum(p[mask] * np.log(p[mask]))\n    return float(H)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (label, N, source_function)\n    test_cases = [\n        (\"uniform_N1\", 1, lambda x, y: s_uniform(x, y)),\n        (\"uniform_N64\", 64, lambda x, y: s_uniform(x, y)),\n        (\"isoGauss_sigma0.25_N4\", 4, lambda x, y: s_isotropic_gaussian(x, y, sigma=0.25)),\n        (\"isoGauss_sigma0.25_N64\", 64, lambda x, y: s_isotropic_gaussian(x, y, sigma=0.25)),\n        (\"anisoGauss_sx0.10_sy0.50_N4\", 4, lambda x, y: s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50)),\n        (\"anisoGauss_sx0.10_sy0.50_N64\", 64, lambda x, y: s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50)),\n    ]\n\n    results = []\n    for _, N, src in test_cases:\n        H = discrete_entropy_from_density(N, src)\n        # Round to 6 decimal places as specified\n        results.append(round(H, 6))\n\n    # Final print statement in the exact required format with 6-decimal formatting.\n    # Ensure fixed decimal representation\n    results_str = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了熵对网格划分的依赖性之后，下一个关键问题是如何在实际计算中保证其准确性和稳定性。当中子源分布在许多网格中的概率极低时，直接计算 $p_i \\log p_i$ 会面临浮点数下溢和求和误差的挑战。本练习通过分析几种代数上等价但数值特性迥异的计算策略，让你学会辨别和实现数值上更稳健的算法，这是科学计算中必须具备的核心素养。",
            "id": "4248400",
            "problem": "在中子输运蒙特卡罗 $k$ 特征值模拟中，一个用于判断裂变源收敛性的常用诊断指标是空间源分布的香农熵。考虑一个包含 $M$ 个栅元的网格，在给定的非活跃代中，统计每个栅元 $i$ 内的整数计数值 $n_i$（在栅元 $i$ 中产生的裂变中子数）或非负权重值 $W_i$（栅元 $i$ 中所有裂变权的权重和）。令总计数值为 $N = \\sum_{i=1}^{M} n_i$，总权重为 $W = \\sum_{i=1}^{M} W_i$。在无权情况下，经验栅元概率为 $p_i = n_i / N$；在加权情况下，为 $p_i = W_i / W$。根据定义，该分布的香农熵（使用自然对数）为 $H = -\\sum_{i=1}^{M} p_i \\log p_i$。在实践中，许多栅元的 $n_i = 0$ 或 $W_i = 0$，并且 $N$ 和 $W$ 的动态范围很广，例如 $N$ 可大至 $10^{12}$，$M$ 可大至 $10^{6}$。所有算术运算均采用 IEEE $754$ 双精度浮点数，单位舍入为 $u \\approx 2^{-53}$。\n\n在这种情况下，下列哪种实现策略在精确算术下是数学无偏的，并且在双精度浮点数下是数值稳定的，同时没有引入人为的下限参数？选择所有适用的选项。\n\nA. 直接以双精度计算 $p_i = n_i/N$；对于 $n_i = 0$ 的栅元，在计算 $\\log p_i$ 之前，用最小的正非规格化双精度数替换 $p_i$；然后按索引顺序使用朴素求和计算 $H = -\\sum_{i=1}^{M} p_i \\log p_i$。\n\nB. 对于整数计数 $n_i$，通过将熵在代数上重写为 $N$ 和 $n_i$ 的表达式来避免计算 $\\log p_i$；跳过 $n_i=0$ 的栅元以避免计算 $\\log 0$，并使用成对求和或补偿求和来累加总和。\n\nC. 对于从 $n_i/N$ 获得的概率 $p_i$，使用一个固定的 $\\epsilon$ 从下方截断每个概率，即用 $\\max(p_i,\\epsilon)$ 替换 $p_i$（其中 $\\epsilon = 10^{-12}$），并计算 $H_{\\epsilon} = -\\sum_{i=1}^{M} \\max(p_i,\\epsilon)\\,\\log\\big(\\max(p_i,\\epsilon)\\big)$ 而不进行重新归一化。\n\nD. 使用 log-sum-exp 技巧将求和转换到对数域以避免小的 $p_i$，即计算 $H$ 为 $-\\log\\!\\left(\\sum_{i=1}^{M} \\exp\\big(\\log p_i + \\log(\\log p_i)\\big)\\right)$。\n\nE. 对于加权统计值 $W_i \\ge 0$，不显式计算 $p_i$，而是仅使用 $W$ 和非零的 $W_i$ 来计算熵；使用补偿求和方案来累加最终结果，并跳过 $W_i=0$ 的栅元。\n\n在选择答案时，请给出基于香农熵定义、浮点数下溢/上溢行为以及求和误差控制等第一性原理的简要理由。假设对数为自然对数，并且约定 $\\lim_{p \\to 0^{+}} p \\log p = 0$ 在数学上成立，但在任何提议的实现中，你仍必须避免如 $\\log 0$ 这样的未定义浮点运算。",
            "solution": "问题要求评估几种在蒙特卡罗模拟中计算空间裂变源分布香农熵的数值策略。评估标准是：该策略必须在精确算术下是数学无偏的，在 IEEE $754$ 双精度下是数值稳定的，并且不引入人为参数。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- **系统：** 一个包含 $M$ 个空间栅元的网格。\n- **数据（无权）：** 每个栅元 $i$ 的整数计数值 $n_i \\ge 0$，总计数值为 $N = \\sum_{i=1}^{M} n_i$。\n- **数据（加权）：** 每个栅元 $i$ 的非负权重值 $W_i \\ge 0$，总权重为 $W = \\sum_{i=1}^{M} W_i$。\n- **概率：** $p_i = n_i / N$ 或 $p_i = W_i / W$。\n- **定义：** 香农熵为 $H = -\\sum_{i=1}^{M} p_i \\log p_i$，使用自然对数。\n- **数学约定：** 根据极限 $\\lim_{p \\to 0^{+}} p \\log p = 0$，当 $p_i=0$ 时，该项的贡献为 $0$。\n- **数值计算背景：**\n    - 算术运算采用 IEEE $754$ 双精度，单位舍入 $u \\approx 2^{-53}$。\n    - 许多栅元可能为空，即 $n_i = 0$ 或 $W_i = 0$。\n    - 典型规模：$N$ 高达 $10^{12}$，$M$ 高达 $10^{6}$。\n- **核心要求：** 识别同时满足以下条件的策略：(1) 数学上无偏，(2) 数值上稳定，以及 (3) 不使用人为的下限参数。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学基础：** 该问题在计算科学领域，特别是在核反应堆物理模拟和数值分析领域，具有坚实的科学基础。使用香农熵作为源收敛的诊断方法是一种标准技术。所描述的数值挑战（大数求和、宽动态范围、浮点限制）是真实且实际的。该问题在科学上是合理的。\n- **适定性与客观性：** 问题陈述精确，使用了标准的数学和计算术语。评估标准（无偏、稳定、无人工参数）是客观的，允许对所提出的算法进行严格分析。该问题是适定的。\n- **完整性与一致性：** 提供了所有必要的定义和约束，没有内部矛盾。\n\n**步骤3：结论与行动**\n问题陈述有效。它提出了一个清晰、形式化且在数值分析领域具有科学相关性的问题。我将继续提供完整解答。\n\n### 推导与选项分析\n\n对于离散概率分布 $\\{p_i\\}_{i=1}^M$，香农熵的基本定义是 $H = -\\sum_{i=1}^{M} p_i \\log p_i$。处理空栅元（$p_i=0$）的关键数学性质是极限 $\\lim_{x \\to 0^+} x \\log x = 0$。如果一个算法在精确算术下能精确计算此和，那么它就是数学无偏的。一个数值稳定的算法是指在计算过程中能最小化舍入误差增长的算法，尤其是在处理大量运算或操作数幅度范围很广时。\n\n让我们根据这些标准评估每个选项。\n\n**A. 直接以双精度计算 $p_i = n_i/N$；对于 $n_i = 0$ 的栅元，在计算 $\\log p_i$ 之前，用最小的正非规格化双精度数替换 $p_i$；然后按索引顺序使用朴素求和计算 $H = -\\sum_{i=1}^{M} p_i \\log p_i$。**\n\n- **数学偏差：** 此策略存在数学偏差。对于 $n_i=0$（因此 $p_i=0$）的栅元，其对熵总和的正确贡献恰好是 $0$。通过用一个小的正数 $\\delta_{sub}$（最小的正非规格化双精度数）替换 $p_i=0$，该算法为每个空栅元增加了一个虚假的项 $-\\delta_{sub} \\log \\delta_{sub} > 0$。当 $M$ 高达 $10^6$ 且空栅元比例可能很大时，这些非零项的总和会给计算出的熵带来显著的、系统性的正偏差。这违反了无偏性要求，并构成了引入人为下限参数的行为，而这是被明确禁止的。\n- **数值稳定性：** 已知对 $M \\approx 10^6$ 个浮点数进行朴素求和（$S_{k+1} = S_k + x_{k+1}$）在各项数值大小差异显著或累加和远大于待加项时是数值不稳定的。这可能导致舍入误差的大量累积。在这种情况下，需要使用如补偿求和等更稳健的方法来保证精度。\n- **结论：** **不正确**。该策略既有数学偏差，又在数值上不稳定。\n\n**B. 对于整数计数 $n_i$，通过将熵在代数上重写为 $N$ 和 $n_i$ 的表达式来避免计算 $\\log p_i$；跳过 $n_i=0$ 的栅元以避免计算 $\\log 0$，并使用成对求和或补偿求和来累加总和。**\n\n- **数学偏差：** 提议的代数重写如下：\n$$H = -\\sum_{i=1}^{M} p_i \\log p_i = -\\sum_{i \\mid n_i>0} \\frac{n_i}{N} \\log\\left(\\frac{n_i}{N}\\right)$$\n$$H = -\\sum_{i \\mid n_i>0} \\frac{n_i}{N} (\\log n_i - \\log N)$$\n$$H = -\\frac{1}{N}\\left(\\sum_{i \\mid n_i>0} n_i \\log n_i\\right) + \\frac{\\log N}{N}\\left(\\sum_{i \\mid n_i>0} n_i\\right)$$\n由于 $\\sum_{i \\mid n_i>0} n_i = N$，这可以简化为：\n$$H = \\log N - \\frac{1}{N} \\sum_{i \\mid n_i>0} n_i \\log n_i$$\n这个恒等式是精确的。跳过 $n_i=0$ 的栅元正确地实现了 $\\lim_{p \\to 0^+} p \\log p = 0$ 的约定。因此，该方法是数学无偏的。\n- **数值稳定性：** 这种形式避免了对可能丢失精度的小概率 $p_i = n_i/N$ 的显式计算。取而代之的是，它计算项 $n_i \\log n_i$ 的和。虽然和中的各项可能很大，但这种计算避免了对数量级差异巨大的量（例如，大的 $p_i$ 对应的 $p_i \\log p_i$ 与小的 $p_i$ 对应的 $p_i \\log p_i$）求和时可能出现的有效位损失。最后的减法 $\\log N - (\\text{sum}/N)$ 通常是良态的。至关重要的是，该策略指定使用“成对求和或补偿求和”来计算和 $\\sum n_i \\log n_i$，这些是已知的用于控制大数求和中舍入误差累积的技术。\n- **结论：** **正确**。该策略是数学无偏的，避免了人为参数，并使用了数值稳定的计算和求和技术。\n\n**C. 对于从 $n_i/N$ 获得的概率 $p_i$，使用一个固定的 $\\epsilon$ 从下方截断每个概率，即用 $\\max(p_i,\\epsilon)$ 替换 $p_i$（其中 $\\epsilon = 10^{-12}$），并计算 $H_{\\epsilon} = -\\sum_{i=1}^{M} \\max(p_i,\\epsilon)\\,\\log\\big(\\max(p_i,\\epsilon)\\big)$ 而不进行重新归一化。**\n\n- **数学偏差：** 这是一个有偏估计量的教科书式例子。它引入了一个人为的下限参数 $\\epsilon = 10^{-12}$，这直接与问题陈述中的一个约束相矛盾。对于任何 $p_i < \\epsilon$ 的栅元（包括所有空栅元），真实概率被替换为 $\\epsilon$。这系统性地改变了计算的值。此外，“不进行重新归一化”的指令意味着所使用的值集合 $\\tilde{p}_i = \\max(p_i, \\epsilon)$ 的和不为 $1$。最终的计算结果不是任何有效概率分布的香农熵，更不用说原始分布的了。\n- **数值稳定性：** 虽然这种方法避免了 `log(0)` 的计算，但其严重的数学不正确性使其数值属性对于计算真实熵的任务而言变得无关紧要。\n- **结论：** **不正确**。该策略存在数学偏差，引入了被禁止的人为参数，并违反了熵的定义。\n\n**D. 使用 log-sum-exp 技巧将求和转换到对数域以避免小的 $p_i$，即计算 $H$ 为 $-\\log\\!\\left(\\sum_{i=1}^{M} \\exp\\big(\\log p_i + \\log(\\log p_i)\\big)\\right)$。**\n\n- **数学偏差：** 这个提议的变换在数学上是荒谬的。让我们简化表达式：\n$\\exp\\big(\\log p_i + \\log(\\log p_i)\\big) = \\exp\\big(\\log(p_i \\log p_i)\\big) = p_i \\log p_i$。\n因此，提议的公式变为：\n$$H_{\\text{proposed}} = -\\log\\left(\\sum_{i=1}^{M} p_i \\log p_i\\right)$$\n而熵的实际定义是 $H = -\\sum_{i=1}^{M} p_i \\log p_i$。\n所以，这个公式试图将 $H$ 计算为 $-\\log(-H)$。这是根本错误的。log-sum-exp 技巧用于计算形如 $\\log(\\sum_i \\exp(x_i))$ 的量，在此处以这种形式没有有效的应用。\n- **数值稳定性：** 这个表达式不仅是错误的，而且对于大多数输入是不可计算的。对于任何 $p_i < 1$ 的栅元，$\\log p_i$ 是负数。那么项 $\\log(\\log p_i)$ 就需要对一个负数取对数，这在实数算术中是未定义的。即使对于 $p_i=1$，$\\log p_i = 0$，而 $\\log(0)$ 也是未定义的。该公式无效。\n- **结论：** **不正确**。所提出的公式是一个数学谬误，无法实现。\n\n**E. 对于加权统计值 $W_i \\ge 0$，不显式计算 $p_i$，而是仅使用 $W$ 和非零的 $W_i$ 来计算熵；使用补偿求和方案来累加最终结果，并跳过 $W_i=0$ 的栅元。**\n\n- **数学偏差：** 这是选项B在加权统计情况下的类比。概率为 $p_i = W_i/W$。同样的代数变换适用：\n$$H = \\log W - \\frac{1}{W} \\sum_{i \\mid W_i>0} W_i \\log W_i$$\n这种形式在数学上与定义等价。通过明确只使用非零的 $W_i$，它正确地处理了空栅元，因此是无偏的。它没有引入任何人为参数。\n- **数值稳定性：** 与选项B一样，这种形式在数值上是稳健的。它避免了小概率的显式构造，而是使用原始的统计数据 $W_i$ 和 $W$。最重要的是，它强制要求使用“补偿求和方案”，这对于在对大量（$M$ 个）项求和时保持精度至关重要。该策略直接解决了此计算中数值误差的主要来源。\n- **结论：** **正确**。该策略是选项B中合理方法在加权统计情况下的正确扩展。它是数学无偏且数值稳定的。",
            "answer": "$$\\boxed{BE}$$"
        },
        {
            "introduction": "本练习将引导你应用香农熵解决一个高级的物理诊断问题：探测源迭代过程中的模式竞争现象。你将通过实验发现，过于粗糙的网格会掩盖物理信号，而过于精细的网格则会使其淹没在统计噪声中。最终，你将设计一种自适应网格策略来巧妙地平衡信号分辨率与统计方差，深刻体会物理、统计与数值方法之间的精妙结合。",
            "id": "4248434",
            "problem": "考虑一个一维空间域 $[0,1]$，它代表一个具有非均匀增殖的板式反应堆。该反应堆产生的裂变源密度，根据中子输运算符的主导本征模，会更偏向于其中一个边界。令 $f_0(x)$ 和 $f_1(x)$ 为裂变源密度的两个非负、归一化的原型形状，由下式给出：\n$$\nf_0(x) = c \\, e^{\\beta x}, \\quad f_1(x) = c \\, e^{\\beta (1 - x)}, \\quad c = \\frac{\\beta}{e^{\\beta} - 1},\n$$\n其中 $x \\in [0,1]$，$\\beta > 0$ 是一个固定的形状参数。在一个蒙特卡罗 $k$-本征值迭代的第 $n$ 个循环（$n \\in \\{1,2,\\dots,N\\}$）中，连续裂变源密度被建模为归一化混合形式：\n$$\nS_n(x) = \\alpha_n f_0(x) + (1 - \\alpha_n) f_1(x),\n$$\n其混合系数随时间变化：\n$$\n\\alpha_n = \\alpha^\\star + A \\, r^n \\cos(\\pi n),\n$$\n其中 $0 < r < 1$ 是支配比（即，迭代算符第二大本征值的模除以最大本征值的模），$A$ 是一个小振幅，$\\alpha^\\star$ 是渐近混合系数。在第 $n$ 个循环中，分箱裂变源估计的香农熵（SE）定义为：\n$$\nH_n = -\\sum_{b=1}^{B} p_{n,b} \\log p_{n,b},\n$$\n其中 $B$ 是空间箱的数量，$p_{n,b}$ 是在第 $n$ 个循环中第 $b$ 个箱内的概率质量，对数为自然对数。$p_{n,b}$ 的蒙特卡罗估计量是根据每个循环从 $S_n(x)$ 中抽取的 $M$ 个独立 $x$ 样本以及一组划分 $[0,1]$ 的选定箱边界构建的。\n\n从上述核心定义出发，构建一个完整的、可执行的程序，该程序针对一组指定的分箱策略和抽样参数，演示以下推理任务：\n\n1.  通过计算时间序列 $\\{H_n\\}_{n=1}^N$，从算法上证明，粗糙分箱可以隐藏模式竞争特征。这里的“模式竞争特征”指的是由 $\\alpha_n$ 中的周期因子 $\\cos(\\pi n)$ 引起的、在香农熵中可检测到的二周期振荡。\n\n2.  证明在固定样本大小 $M$ 的情况下，过度精细的分箱会增大 $\\{H_n\\}$ 的估计量方差，以至于模式竞争特征变得不可检测。\n\n3.  设计一组自适应分箱，既能揭示特征又不会产生过度方差。使用以下原则：对于每箱最多 $\\varepsilon$ 的目标相对标准差，要求每箱的预期计数满足 $M \\int_{\\text{bin}} p_{\\mathrm{wr}}(x) \\, dx \\ge 1/\\varepsilon^2$，其中 $p_{\\mathrm{wr}}(x)$ 是一个最坏情况包络，定义为：\n    $$\n    p_{\\mathrm{wr}}(x) = \\min\\left\\{ \\alpha_{\\min} f_0(x) + (1 - \\alpha_{\\min}) f_1(x), \\; \\alpha_{\\max} f_0(x) + (1 - \\alpha_{\\max}) f_1(x) \\right\\}, \n    $$\n    其中 $\\alpha_{\\min} = \\alpha^\\star - A$ 和 $\\alpha_{\\max} = \\alpha^\\star + A$。通过将 $[0,1]$ 划分为连续的区间来构建箱边界，每个区间的 $p_{\\mathrm{wr}}$-质量约等于 $1/(\\varepsilon^2 M)$，使用数值积分来近似计算积分。\n\n使用以下纯粹基于香农熵时间序列的二周期检测统计量：\n$$\nS_{\\mathrm{det}} = \\left| \\frac{1}{N} \\sum_{n=1}^{N} (-1)^n H_n \\right|,\n$$\n如果满足以下条件，则宣布检测到特征：\n$$\nS_{\\mathrm{det}} > \\tau, \\quad \\text{其中} \\quad \\tau = k \\frac{\\sigma_H}{\\sqrt{N}},\n$$\n其中 $\\sigma_H$ 是 $\\{H_n\\}_{n=1}^N$ 的样本标准差，$k$ 是一个固定常数。此标准将 $\\{H_n\\}$ 中测得的二周期振幅与其随机可变性的估计值进行比较。\n\n您的程序必须为每个测试用例实现以下步骤：\n- 使用提供的 $(\\alpha^\\star, A, r)$ 生成 $\\{ \\alpha_n \\}_{n=1}^N$。\n- 对每个 $n$，通过对 $f_0$ 和 $f_1$ 的混合进行逆变换采样，从 $S_n(x)$ 中精确抽取 $M$ 个位置样本。\n- 根据指定箱边界上的直方图计算 $H_n$。\n- 计算 $S_{\\mathrm{det}}$、$\\sigma_H$，并应用检测规则返回一个布尔值，指示是否检测到模式竞争特征。\n\n测试套件：\n使用固定参数 $\\beta = 3.0$，$\\alpha^\\star = 0.65$，$A = 0.25$，$r = 0.95$，$N = 60$ 和 $k = 2.0$。为保证可复现性，在伪随机数生成器中使用固定的随机种子。\n\n评估以下五个测试用例：\n- 案例1（粗糙均匀）：$B = 2$，$M = 4000$ 的均匀分箱。\n- 案例2（粗糙均匀）：$B = 4$，$M = 4000$ 的均匀分箱。\n- 案例3（精细均匀，有噪声）：$B = 100$，$M = 2000$ 的均匀分箱。\n- 案例4（自适应，方差控制）：$\\varepsilon = 0.1$，$M = 2000$，基于 $p_{\\mathrm{wr}}(x)$ 构建的自适应分箱。\n- 案例5（中等均匀，采样良好）：$B = 40$，$M = 20000$ 的均匀分箱。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3,result4,result5]\"），其中每个 $result_i$ 是一个布尔值，指示在案例 $i$ 中是否检测到特征。不涉及物理单位，并且通过 $\\cos(\\pi n)$ 的构造，所有角度都以弧度为单位。给定固定种子，输出必须是确定性的。",
            "solution": "该问题要求通过计算来演示不同的统计估计策略如何影响使用香农熵检测物理特征（核反应堆裂变源中的模式竞争）的能力。这需要基于简化的裂变源收敛一维双模式模型来实现蒙特卡罗模拟。解决方案涉及从概率分布中抽样，应用不同的数据分箱技术，并对所得时间序列进行统计分析。\n\n模型的核心在于第 $n$ 个循环中随时间变化的裂变源密度 $S_n(x)$：\n$$S_n(x) = \\alpha_n f_0(x) + (1 - \\alpha_n) f_1(x)$$\n其中 $f_0(x) = c \\, e^{\\beta x}$ 和 $f_1(x) = c \\, e^{\\beta (1 - x)}$ 是在 $x \\in [0,1]$ 上的两个非对称、归一化的分布，其中 $c = \\beta/(e^{\\beta} - 1)$。混合系数 $\\alpha_n = \\alpha^\\star + A \\, r^n \\cos(\\pi n) = \\alpha^\\star + A \\, r^n (-1)^n$ 导致源形状在偶数循环中以 $f_0(x)$ 为主，在奇数循环中以 $f_1(x)$ 为主（或更接近于它），振荡幅度以 $r^n$ 的因子衰减。这种振荡就是我们旨在检测的“模式竞争特征”。\n\n香农熵 $H_n = -\\sum_{b=1}^{B} p_{n,b} \\log p_{n,b}$ 作为源空间分布的标量度量。这里，$p_{n,b}$ 是第 $n$ 个循环中空间箱 $b$ 内的概率质量。随着 $S_n(x)$ 的振荡，我们预期 $H_n$ 也会振荡。源的空间集中度发生变化（例如，从在 $x=1$ 处达到峰值变为在 $x=0$ 处达到峰值）通常会导致其分箱表示的熵发生变化。在 $H_n$ 中检测此特征被公式化为一个信噪比问题。信号是熵序列 $\\{H_n\\}_{n=1}^N$ 中二周期振荡的振幅，由统计量 $S_{\\mathrm{det}} = \\left| \\frac{1}{N} \\sum_{n=1}^{N} (-1)^n H_n \\right|$ 衡量。噪声由熵估计中的统计不确定性量化，通过阈值 $\\tau = k \\frac{\\sigma_H}{\\sqrt{N}}$ 捕捉，其中 $\\sigma_H$ 是熵序列的样本标准差。如果 $S_{\\mathrm{det}} > \\tau$，则宣布“检测到”特征。\n\n模拟过程为每个循环 $n$ 从 $S_n(x)$ 生成 $M$ 个独立随机样本。这是通过从混合分布中抽样来实现的，这是一个两步过程：首先，以概率 $\\alpha_n$ 和 $1-\\alpha_n$ 随机选择 $f_0$ 或 $f_1$；其次，使用逆变换采样从所选分布中抽取一个样本。$f_0(x)$ 和 $f_1(x)$ 的累积分布函数（CDF）为：\n$$F_0(x) = \\int_0^x c e^{\\beta t} dt = \\frac{e^{\\beta x} - 1}{e^{\\beta} - 1}$$\n$$F_1(x) = \\int_0^x c e^{\\beta (1-t)} dt = \\frac{e^\\beta - e^{\\beta(1-x)}}{e^\\beta - 1}$$\n通过对这些函数求逆，我们得到将均匀随机变量 $u \\in [0,1]$ 转换为样本 $x$ 的公式：\n$$F_0^{-1}(u) = \\frac{1}{\\beta} \\log\\left(u(e^\\beta - 1) + 1\\right)$$\n$$F_1^{-1}(u) = 1 - \\frac{1}{\\beta} \\log\\left(e^\\beta - u(e^\\beta - 1)\\right)$$\n\n分箱策略的选择至关重要，因为它在解析 $S_n(x)$ 的空间特征与控制统计噪声之间进行权衡。\n1.  **粗糙分箱**：使用非常少的箱数 $B$，无法捕捉 $S_n(x)$ 的变化形状。例如，当 $B=2$ 且箱为 $[0, 0.5]$ 和 $[0.5, 1]$ 时，随着 $\\alpha_n$ 的振荡，每个箱中的概率质量可能不会显著变化，特别是如果分布 $f_0$ 和 $f_1$ 的不对称性不强。因此，$H_n$ 几乎保持不变，特征被错过（$S_{\\mathrm{det}} \\approx 0$）。\n2.  **精细分箱**：使用大量的箱数 $B$，可以创建高分辨率的直方图。然而，对于固定的样本大小 $M$，这会导致每箱的样本非常少。箱概率的统计估计量 $\\hat{p}_{n,b} = N_{n,b}/M$（其中 $N_{n,b}$ 是箱 $b$ 中的计数）的方差约为 $\\text{Var}(\\hat{p}_{n,b}) \\approx p_{n,b}/M$。对于小计数，相对不确定性很大。$p_{n,b}$ 上的这种高统计噪声会传播到 $H_n$ 的计算中，产生一个噪声很大的时间序列，其中微小的确定性振荡被完全淹没（$\\sigma_H$ 变大，使得 $\\tau$ 变大，检测可能性降低）。\n3.  **自适应分箱**：该策略旨在平衡分辨率和噪声。其原则是创建具有大致相等预期粒子计数的箱，确保没有箱会“缺少”样本。条件是，对于选定的最坏情况概率密度 $p_{\\mathrm{wr}}(x)$，每个箱 $b$ 的预期计数应满足 $M \\int_b p_{\\mathrm{wr}}(x) dx \\ge 1/\\varepsilon^2$，其中 $\\varepsilon$ 是任何箱中计数的目标最大相对标准差。问题将 $p_{\\mathrm{wr}}(x)$ 定义为对应于 $\\alpha_{\\min}=\\alpha^\\star - A$ 和 $\\alpha_{\\max}=\\alpha^\\star + A$ 的两种极端源形状的逐点最小值“包络”。通过数值求解包含目标质量 $1/(\\varepsilon^2 M)$ 的区间来找到箱边界，这需要使用数值积分（`scipy.integrate.quad`）来计算质量积分，并使用数值求根器（`scipy.optimize.brentq`）来定位箱边界。这种方法确保了在整个域内每箱有足够的样本数量，稳定了熵的估计，并使得潜在的物理振荡能够被检测到。\n\n程序将为五个不同的测试用例执行这些步骤，每个用例代表分箱策略和样本大小的不同组合，以算法方式演示这些效应，并为每个案例报告布尔检测结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\n\n# -- 1. PROBLEM PARAMETERS AND MODEL DEFINITIONS --\n\n# Fixed physical and simulation parameters from the problem statement\nBETA = 3.0\nALPHA_STAR = 0.65\nA = 0.25\nR = 0.95\nN_CYCLES = 60\nK_FACTOR = 2.0\nSEED = 12345\n\n# Normalization constant for f0 and f1\nC = BETA / (np.exp(BETA) - 1.0)\n\ndef f0(x, beta, c):\n    \"\"\"Prototype fission source density f_0(x).\"\"\"\n    return c * np.exp(beta * x)\n\ndef f1(x, beta, c):\n    \"\"\"Prototype fission source density f_1(x).\"\"\"\n    return c * np.exp(beta * (1.0 - x))\n\ndef invF0(u, beta):\n    \"\"\"Inverse CDF of f_0(x) for sampling.\"\"\"\n    return (1.0 / beta) * np.log(u * (np.exp(beta) - 1.0) + 1.0)\n\ndef invF1(u, beta):\n    \"\"\"Inverse CDF of f_1(x) for sampling.\"\"\"\n    return 1.0 - (1.0 / beta) * np.log(np.exp(beta) - u * (np.exp(beta) - 1.0))\n\ndef sample_Sn(alpha, M, beta, rng):\n    \"\"\"Samples M positions from the mixture distribution S_n(x) using inverse transform sampling.\"\"\"\n    component_choice = rng.uniform(0.0, 1.0, size=M)\n    u_for_inversion = rng.uniform(0.0, 1.0, size=M)\n    \n    samples = np.zeros(M)\n    \n    f0_mask = component_choice  alpha\n    num_f0 = np.sum(f0_mask)\n    if num_f0 > 0:\n        samples[f0_mask] = invF0(u_for_inversion[f0_mask], beta)\n        \n    f1_mask = ~f0_mask\n    num_f1 = M - num_f0\n    if num_f1 > 0:\n        samples[f1_mask] = invF1(u_for_inversion[f1_mask], beta)\n        \n    return samples\n\n# -- 2. ADAPTIVE BINNING ALGORITHM --\n\ndef get_adaptive_bins(M, epsilon, beta, c, alpha_star, A):\n    \"\"\"\n    Constructs adaptive bin edges based on the worst-case envelope p_wr(x).\n    \"\"\"\n    alpha_min = alpha_star - A\n    alpha_max = alpha_star + A\n    \n    p_wr_args = (beta, c, alpha_min, alpha_max)\n\n    def p_wr(x, beta_val, c_val, alpha_min_val, alpha_max_val):\n        \"\"\"The worst-case envelope PDF.\"\"\"\n        s_min = alpha_min_val * f0(x, beta_val, c_val) + (1.0 - alpha_min_val) * f1(x, beta_val, c_val)\n        s_max = alpha_max_val * f0(x, beta_val, c_val) + (1.0 - alpha_max_val) * f1(x, beta_val, c_val)\n        return np.minimum(s_min, s_max)\n\n    target_mass = 1.0 / (epsilon**2 * M)\n    bin_edges = [0.0]\n    \n    while bin_edges[-1]  1.0:\n        current_edge = bin_edges[-1]\n        \n        remaining_mass, _ = quad(p_wr, current_edge, 1.0, args=p_wr_args)\n        \n        # If remaining mass is less than or slightly above the target, just add the final edge and stop.\n        if remaining_mass  target_mass * 1.05:\n            break\n            \n        def objective(x):\n            \"\"\"Function to find root of: integral from current_edge to x equals target_mass.\"\"\"\n            mass, _ = quad(p_wr, current_edge, x, args=p_wr_args)\n            return mass - target_mass\n\n        try:\n            # Bracket for root finding is (current_edge, 1.0)\n            next_edge = brentq(objective, current_edge, 1.0)\n            bin_edges.append(next_edge)\n        except ValueError:\n            # Could happen if signs are not different, e.g., due to precision.\n            break\n\n    if bin_edges[-1]  1.0:\n        bin_edges.append(1.0)\n        \n    return np.array(bin_edges)\n\n# -- 3. SIMULATION AND ANALYSIS --\n\ndef run_one_case(case_config):\n    \"\"\"\n    Runs the full simulation for one test case and returns the detection result.\n    \"\"\"\n    M, bin_type, bin_param, epsilon = case_config\n    \n    rng = np.random.default_rng(SEED)\n\n    if bin_type == 'uniform':\n        B = bin_param\n        bin_edges = np.linspace(0.0, 1.0, B + 1)\n    elif bin_type == 'adaptive':\n        bin_edges = get_adaptive_bins(M, epsilon, BETA, C, ALPHA_STAR, A)\n    else:\n        raise ValueError(\"Unknown binning type\")\n\n    n_vals = np.arange(1, N_CYCLES + 1)\n    alpha_n_series = ALPHA_STAR + A * (R**n_vals) * np.cos(np.pi * n_vals)\n    \n    H_series = np.zeros(N_CYCLES)\n    for i in range(N_CYCLES):\n        alpha_n = alpha_n_series[i]\n        \n        samples = sample_Sn(alpha_n, M, BETA, rng)\n        \n        counts, _ = np.histogram(samples, bins=bin_edges)\n        \n        # We must use the actual number of generated particles for normalization\n        total_samples = np.sum(counts) \n        if total_samples > 0:\n            probs = counts / total_samples\n            # Filter out zero probabilities for log calculation\n            nz_probs = probs[probs > 0]\n            H_n = -np.sum(nz_probs * np.log(nz_probs))\n            H_series[i] = H_n\n\n    # Calculate detection statistic and threshold\n    S_det = np.abs(np.mean((-1.0)**n_vals * H_series))\n    sigma_H = np.std(H_series, ddof=1)\n    \n    # Avoid division by zero if all H_n are identical\n    if sigma_H == 0:\n        return False\n        \n    tau = K_FACTOR * sigma_H / np.sqrt(N_CYCLES)\n    \n    return S_det > tau\n\n# -- 4. MAIN EXECUTION --\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the simulations, and prints the final results.\n    \"\"\"\n    test_cases = [\n        # ( M,      bin_type,  bin_param, epsilon)\n        (4000,   'uniform', 2,         None),  # Case 1: Coarse uniform\n        (4000,   'uniform', 4,         None),  # Case 2: Coarse uniform\n        (2000,   'uniform', 100,       None),  # Case 3: Fine uniform, noisy\n        (2000,   'adaptive',None,      0.1),   # Case 4: Adaptive, variance controlled\n        (20000,  'uniform', 40,        None),  # Case 5: Moderate uniform, well-sampled\n    ]\n\n    results = []\n    for case in test_cases:\n        is_detected = run_one_case(case)\n        results.append(is_detected)\n\n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}