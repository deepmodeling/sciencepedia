{
    "hands_on_practices": [
        {
            "introduction": "Before using Shannon entropy as a diagnostic, it is essential to understand its fundamental properties. This first exercise explores how the calculated entropy value is sensitive to the spatial discretization, or binning, that you choose. By analyzing how entropy changes for a fixed source shape as the mesh is refined, you will gain a crucial insight: the absolute value of discrete entropy is not an invariant, but its change with mesh resolution follows a predictable logarithmic relationship. ",
            "id": "4248386",
            "problem": "You are tasked with designing a computational study, grounded in probabilistic modeling relevant to nuclear reactor simulation, to evaluate the sensitivity of the Shannon entropy $H(p)$ to the resolution of a spatial bin mesh. In reactor source iteration, a spatially varying source intensity is often summarized by a discrete spatial distribution over mesh bins, and the Shannon entropy of this distribution is monitored for convergence. Your program must quantify how this measure changes with mesh refinement and coarsening for the same underlying source shape.\n\nUse the following purely mathematical setup. Consider a two-dimensional slab domain $D = [-1,1] \\times [-1,1]$, and an underlying nonnegative source density $s(x,y)$ defined over $D$. For a given mesh resolution parameter $N \\in \\mathbb{Z}_{>0}$, partition $D$ into $N \\times N$ equal rectangular bins. Approximate the probability mass in each bin by evaluating $s(x,y)$ at the bin midpoint multiplied by the bin area, and then normalize across all bins to obtain a discrete probability vector $p = (p_1,\\dots,p_{M})$ with $M = N^2$. Compute the Shannon entropy of $p$ using the natural logarithm, and express the result in nats.\n\nThe source shapes to be tested are:\n- Uniform slab source: $s_{\\text{uni}}(x,y) = 1$.\n- Isotropic Gaussian source centered at the origin: $s_{\\text{iso}}(x,y) = \\exp\\!\\left(-\\dfrac{x^2 + y^2}{2\\sigma^2}\\right)$ with $\\sigma = 0.25$.\n- Anisotropic Gaussian source centered at the origin: $s_{\\text{aniso}}(x,y) = \\exp\\!\\left(-\\dfrac{1}{2}\\left(\\dfrac{x^2}{\\sigma_x^2} + \\dfrac{y^2}{\\sigma_y^2}\\right)\\right)$ with $\\sigma_x = 0.10$ and $\\sigma_y = 0.50$.\n\nYour program must implement the construction of the discrete distribution $p$ from $s(x,y)$ as specified, and compute the Shannon entropy $H(p)$ in nats for each test case. No user input is required.\n\nTest Suite:\n- Case $1$: $s_{\\text{uni}}$, $N = 1$ (extreme coarsening baseline).\n- Case $2$: $s_{\\text{uni}}$, $N = 64$ (strong refinement of the baseline).\n- Case $3$: $s_{\\text{iso}}$ with $\\sigma = 0.25$, $N = 4$ (coarse mesh).\n- Case $4$: $s_{\\text{iso}}$ with $\\sigma = 0.25$, $N = 64$ (refined mesh).\n- Case $5$: $s_{\\text{aniso}}$ with $\\sigma_x = 0.10$, $\\sigma_y = 0.50$, $N = 4$ (coarse mesh with anisotropy).\n- Case $6$: $s_{\\text{aniso}}$ with $\\sigma_x = 0.10$, $\\sigma_y = 0.50$, $N = 64$ (refined mesh with anisotropy).\n\nAnswer Specification:\n- Compute $H(p)$ in nats for each test case as described. Round each value to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above (e.g., $\\texttt{[h1,h2,h3,h4,h5,h6]}$), where each $h_k$ is a decimal representation of $H(p)$ in nats rounded to $6$ decimal places.\n\nScientific and Algorithmic Constraints:\n- Begin from the foundational probabilistic definitions: probability measure over $D$, construction of a discrete distribution by quadrature over bins, and the natural-logarithm-based information measure for a discrete distribution.\n- Your algorithm must use midpoint evaluation within each bin and equal-area binning.\n- The domain $D$, source shapes, and parameters are dimensionless; there are no physical units beyond the entropy unit. Entropy values must be expressed in nats.",
            "solution": "The objective is to evaluate how the Shannon entropy $H(p)$, used as an information-theoretic measure of spread of a discrete source distribution in nuclear reactor simulation, depends on the resolution of a spatial bin mesh when the underlying source shape is fixed. We start from fundamental probabilistic principles and the definition of Shannon entropy for discrete distributions.\n\nFoundational Base:\n- Let $D = [-1,1] \\times [-1,1]$ denote the slab domain. A nonnegative source density $s(x,y) \\ge 0$ defines a finite measure on $D$ via the integral of $s(x,y)$ over subsets of $D$.\n- For a given mesh resolution $N \\in \\mathbb{Z}_{>0}$, we partition $D$ into $N \\times N$ equal-area bins. Each bin has side length $\\Delta = \\dfrac{2}{N}$ and area $a = \\Delta^2 = \\dfrac{4}{N^2}$.\n- The discrete probability vector $p = (p_1, \\dots, p_M)$ with $M = N^2$ is constructed by approximating the mass in each bin using the midpoint rule. Let $(x_i, y_i)$ be the center of bin $i$. Define the unnormalized mass $m_i = s(x_i, y_i) \\cdot a$. The normalized probabilities are\n$$\np_i = \\frac{m_i}{\\sum_{j=1}^{M} m_j} = \\frac{s(x_i,y_i) \\, a}{\\sum_{j=1}^{M} s(x_j,y_j) \\, a} = \\frac{s(x_i,y_i)}{\\sum_{j=1}^{M} s(x_j,y_j)},\n$$\nwhere the equality follows since $a$ is constant for all bins. This constructs a discrete distribution consistent with the underlying source density and equal-area binning.\n\nShannon Entropy:\n- For a discrete distribution $p = (p_1,\\dots,p_M)$, the Shannon entropy in natural units (nats) is defined by\n$$\nH(p) = - \\sum_{i=1}^{M} p_i \\ln p_i,\n$$\nwhere $\\ln$ denotes the natural logarithm. This measure quantifies the information content or uncertainty of the distribution. It is maximized for the uniform distribution over the given bins and decreases as the distribution becomes more peaked.\n\nMesh-Resolution Sensitivity (Derivation):\n- Consider the approximation $p_i \\approx \\frac{s(x_i,y_i) a}{S}$ where $S = \\iint_{D} s(x,y) \\, dx \\, dy$ is the total mass; under midpoint quadrature, $\\sum_i s(x_i,y_i) a \\approx S$, yielding $p_i \\approx s(x_i,y_i) a / S$.\n- Substituting into the entropy definition yields\n$$\nH(p) \\approx - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln\\left(\\frac{s(x_i,y_i) a}{S}\\right)\n= - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln\\left(\\frac{s(x_i,y_i)}{S}\\right) - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln(a).\n$$\n- Since $\\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\approx 1$, the second term simplifies to $- \\ln(a)$. The first term is a Riemann sum approximation to the continuous differential entropy\n$$\nh(s) = - \\iint_{D} \\frac{s(x,y)}{S} \\ln\\left(\\frac{s(x,y)}{S}\\right) \\, dx \\, dy.\n$$\n- Therefore, as the mesh is uniformly refined (i.e., $a \\to 0$ and $M \\to \\infty$), the discrete entropy behaves like\n$$\nH(p) \\approx h(s) - \\ln(a) = h(s) + \\ln\\left(\\frac{1}{a}\\right).\n$$\n- Because $a = \\frac{4}{N^2}$ in our domain, we obtain\n$$\nH(p) \\approx h(s) + \\ln\\left(\\frac{N^2}{4}\\right) = h(s) + 2 \\ln(N) - \\ln(4).\n$$\n- This shows that for a fixed source shape $s(x,y)$, $H(p)$ grows approximately linearly in $\\ln(N)$, with an additive constant depending on the domain area and the differential entropy of the normalized continuous density. For a perfectly uniform source over bins, $p_i = \\frac{1}{M}$ for all $i$ and $H(p) = \\ln(M) = 2 \\ln(N)$ exactly, matching the expected maximum. For peaked sources, $H(p)$ is smaller than the uniform-case bound and still increases with refinement due to the $-\\ln(a)$ term.\n\nAlgorithmic Design:\n- Construct bin centers $(x_i,y_i)$ for $N \\times N$ bins over $D$ using $x$ and $y$ coordinates at midpoints: $x_k = -1 + \\left(k + \\frac{1}{2}\\right)\\Delta$ and $y_\\ell = -1 + \\left(\\ell + \\frac{1}{2}\\right)\\Delta$ for $k,\\ell \\in \\{0,\\dots,N-1\\}$ with $\\Delta = \\frac{2}{N}$.\n- Evaluate $s(x_i,y_i)$ for each source shape:\n  - Uniform: $s_{\\text{uni}}(x,y) = 1$.\n  - Isotropic Gaussian: $s_{\\text{iso}}(x,y) = \\exp\\!\\left(-\\frac{x^2 + y^2}{2 \\sigma^2}\\right)$ with $\\sigma = 0.25$.\n  - Anisotropic Gaussian: $s_{\\text{aniso}}(x,y) = \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x^2}{\\sigma_x^2} + \\frac{y^2}{\\sigma_y^2}\\right)\\right)$ with $\\sigma_x = 0.10$ and $\\sigma_y = 0.50$.\n- Form the unnormalized vector $m_i = s(x_i,y_i)$ (the constant area factor cancels in normalization) and normalize to obtain $p_i = \\frac{m_i}{\\sum_j m_j}$.\n- Compute $H(p)$ via $H(p) = - \\sum_i p_i \\ln p_i$ in nats.\n- Apply to the test suite cases and round each result to $6$ decimal places.\n\nExpected Behaviors and Edge Cases:\n- Case $1$ with $N=1$ yields $M=1$ and a single-bin distribution $p_1=1$, hence $H(p)=0$ in nats, representing the extreme coarsening baseline.\n- For the uniform source, $H(p)$ approaches $2 \\ln(N)$ exactly; thus it increases substantially from $N=1$ to $N=64$.\n- For Gaussian sources, $H(p)$ is strictly less than the uniform maximum for the same $N$ and increases with refinement due to finer resolution of the spatial variability.\n- The anisotropic Gaussian source exhibits reduced entropy relative to isotropic for the same $N$ if the anisotropy increases concentration along one axis, but still shows the additive $-\\ln(a)$ sensitivity to $N$.\n\nThe final program implements these steps, computes the required entropies in nats, rounds to $6$ decimal places, and outputs the single-line bracketed list in the prescribed order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_grid_centers(N, domain_min=-1.0, domain_max=1.0):\n    # Compute midpoints of N bins along each axis over [domain_min, domain_max]\n    length = domain_max - domain_min\n    delta = length / N\n    # Midpoints: domain_min + (k + 0.5) * delta\n    coords = domain_min + (np.arange(N) + 0.5) * delta\n    X, Y = np.meshgrid(coords, coords, indexing='xy')\n    return X, Y\n\ndef s_uniform(x, y):\n    return np.ones_like(x)\n\ndef s_isotropic_gaussian(x, y, sigma=0.25):\n    return np.exp(-((x**2 + y**2) / (2.0 * sigma**2)))\n\ndef s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50):\n    return np.exp(-0.5 * ((x / sigma_x)**2 + (y / sigma_y)**2))\n\ndef discrete_entropy_from_density(N, source_fn):\n    # Construct grid\n    X, Y = make_grid_centers(N)\n    # Evaluate source\n    S = source_fn(X, Y)\n    # Normalize to probabilities; area factor cancels as it's constant for all bins\n    total = np.sum(S)\n    # Guard against pathological zero total (should not occur for provided sources)\n    if total == 0.0:\n        # If total is zero, define zero-entropy (degenerate); but avoid division by zero.\n        return 0.0\n    p = S / total\n    # Compute Shannon entropy in nats: -sum p ln p, treating p=0 terms as 0\n    # Since Gaussian and uniform are positive, zeros should not occur; still robust handling:\n    mask = p > 0.0\n    H = -np.sum(p[mask] * np.log(p[mask]))\n    return float(H)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (label, N, source_function)\n    test_cases = [\n        (\"uniform_N1\", 1, lambda x, y: s_uniform(x, y)),\n        (\"uniform_N64\", 64, lambda x, y: s_uniform(x, y)),\n        (\"isoGauss_sigma0.25_N4\", 4, lambda x, y: s_isotropic_gaussian(x, y, sigma=0.25)),\n        (\"isoGauss_sigma0.25_N64\", 64, lambda x, y: s_isotropic_gaussian(x, y, sigma=0.25)),\n        (\"anisoGauss_sx0.10_sy0.50_N4\", 4, lambda x, y: s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50)),\n        (\"anisoGauss_sx0.10_sy0.50_N64\", 64, lambda x, y: s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50)),\n    ]\n\n    results = []\n    for _, N, src in test_cases:\n        H = discrete_entropy_from_density(N, src)\n        # Round to 6 decimal places as specified\n        results.append(round(H, 6))\n\n    # Final print statement in the exact required format with 6-decimal formatting.\n    # Ensure fixed decimal representation\n    results_str = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from mathematical definition to robust code requires careful attention to numerical stability. The formula for Shannon entropy, $H = -\\sum_{i=1}^{M} p_i \\log p_i$, appears straightforward but contains hidden pitfalls when implemented with finite-precision arithmetic, especially when dealing with very small probabilities. This exercise challenges you to analyze several computational strategies and identify those that are both mathematically correct and numerically sound, a critical skill for any computational physicist. ",
            "id": "4248400",
            "problem": "In neutron transport Monte Carlo $k$-eigenvalue simulations, a common diagnostic for fission-source convergence is the Shannon entropy of the spatial source distribution. Consider a mesh of $M$ bins over which, in a given inactive cycle, either integer counts $n_i$ (number of fission neutrons born in bin $i$) or nonnegative weights $W_i$ (sum of fission weights in bin $i$) are tallied. Let the total count be $N = \\sum_{i=1}^{M} n_i$ and the total weight be $W = \\sum_{i=1}^{M} W_i$. The empirical bin probabilities are $p_i = n_i / N$ in the unweighted case or $p_i = W_i / W$ in the weighted case, and the Shannon entropy (natural logarithm) of the distribution is, by definition, $H = -\\sum_{i=1}^{M} p_i \\log p_i$. In practice, many bins have $n_i = 0$ or $W_i = 0$, and $N$ and $W$ can span wide dynamic ranges, for example $N$ as large as $10^{12}$ and $M$ as large as $10^{6}$. All arithmetic is in IEEE $754$ double precision, with unit roundoff $u \\approx 2^{-53}$.\n\nWhich of the following implementation strategies are both mathematically unbiased in exact arithmetic and numerically stable in double precision for this setting, without introducing artificial floor parameters? Select all that apply.\n\nA. Directly compute $p_i = n_i/N$ in double precision; for bins with $n_i = 0$, replace $p_i$ by the smallest positive subnormal double before evaluating $\\log p_i$; then compute $H = -\\sum_{i=1}^{M} p_i \\log p_i$ using naive summation in index order.\n\nB. For integer counts $n_i$, avoid computing $\\log p_i$ by algebraically rewriting the entropy in terms of $N$ and $n_i$; skip bins with $n_i = 0$ to avoid evaluating $\\log 0$, and accumulate the sum using pairwise or compensated summation.\n\nC. For probabilities $p_i$ obtained from $n_i/N$, clip each probability from below by a fixed $\\epsilon$, i.e., replace $p_i$ by $\\max(p_i,\\epsilon)$ with $\\epsilon = 10^{-12}$, and compute $H_{\\epsilon} = -\\sum_{i=1}^{M} \\max(p_i,\\epsilon)\\,\\log\\big(\\max(p_i,\\epsilon)\\big)$ without renormalizing.\n\nD. Transform the sum to the log-domain using a log-sum-exp trick to avoid small $p_i$, i.e., compute $H$ as $-\\log\\!\\left(\\sum_{i=1}^{M} \\exp\\big(\\log p_i + \\log(\\log p_i)\\big)\\right)$.\n\nE. For weighted tallies $W_i \\ge 0$, compute the entropy without forming $p_i$ by using only $W$ and the nonzero $W_i$; use a compensated summation scheme to accumulate the final result, and skip bins with $W_i = 0$.\n\nGive a brief justification rooted in first principles of the definition of Shannon entropy, floating-point underflow/overflow behavior, and summation error control when selecting your answers. Assume that logarithms are natural and that the convention $\\lim_{p \\to 0^{+}} p \\log p = 0$ holds mathematically, but you must still avoid undefined floating-point evaluations such as $\\log 0$ in any proposed implementation.",
            "solution": "The problem asks for an evaluation of several proposed numerical strategies for computing the Shannon entropy of a spatial fission source distribution in a Monte Carlo simulation. The evaluation criteria are that the strategy must be both mathematically unbiased in exact arithmetic and numerically stable in IEEE $754$ double precision, without introducing artificial parameters.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **System:** A mesh of $M$ spatial bins.\n- **Data (unweighted):** Integer counts $n_i \\ge 0$ for each bin $i$, with total count $N = \\sum_{i=1}^{M} n_i$.\n- **Data (weighted):** Nonnegative weights $W_i \\ge 0$ for each bin $i$, with total weight $W = \\sum_{i=1}^{M} W_i$.\n- **Probabilities:** $p_i = n_i / N$ or $p_i = W_i / W$.\n- **Definition:** Shannon entropy is $H = -\\sum_{i=1}^{M} p_i \\log p_i$, using the natural logarithm.\n- **Mathematical Convention:** It is understood that the contribution of a term is $0$ if $p_i=0$, based on the limit $\\lim_{p \\to 0^{+}} p \\log p = 0$.\n- **Numerical Context:**\n    - Arithmetic is IEEE $754$ double precision, with unit roundoff $u \\approx 2^{-53}$.\n    - Many bins can be empty, i.e., $n_i = 0$ or $W_i = 0$.\n    - Typical scales: $N$ up to $10^{12}$, $M$ up to $10^{6}$.\n- **Core Requirement:** Identify strategies that are simultaneously (1) mathematically unbiased, (2) numerically stable, and (3) do not use artificial floor parameters.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is well-grounded in computational science, specifically in the field of nuclear reactor physics simulation and numerical analysis. The use of Shannon entropy as a diagnostic for source convergence is a standard technique. The numerical challenges described (large sums, wide dynamic range, floating-point limitations) are real and practical. The problem is scientifically sound.\n- **Well-Posedness & Objectivity:** The problem statement is precise, using standard mathematical and computational terminology. The criteria for evaluation (unbiased, stable, no artificial parameters) are objective and allow for a rigorous analysis of the proposed algorithms. The problem is well-posed.\n- **Completeness & Consistency:** All necessary definitions and constraints are provided. There are no internal contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It presents a clear, formal, and scientifically relevant question in numerical analysis. I will proceed with a full solution.\n\n### Derivation and Option Analysis\n\nThe fundamental definition of the Shannon entropy for a discrete probability distribution $\\{p_i\\}_{i=1}^M$ is $H = -\\sum_{i=1}^{M} p_i \\log p_i$. The key mathematical property for dealing with empty bins ($p_i=0$) is the limit $\\lim_{x \\to 0^+} x \\log x = 0$. An algorithm is mathematically unbiased if, under exact arithmetic, it computes exactly this sum. A numerically stable algorithm is one that minimizes the growth of rounding errors during computation, particularly when dealing with a large number of operations or a wide range of operand magnitudes.\n\nLet's evaluate each option against these criteria.\n\n**A. Directly compute $p_i = n_i/N$ in double precision; for bins with $n_i = 0$, replace $p_i$ by the smallest positive subnormal double before evaluating $\\log p_i$; then compute $H = -\\sum_{i=1}^{M} p_i \\log p_i$ using naive summation in index order.**\n\n- **Mathematical Bias:** This strategy is mathematically biased. The correct contribution to the entropy sum from a bin with $n_i=0$ (and thus $p_i=0$) is exactly $0$. By replacing $p_i=0$ with a small positive number $\\delta_{sub}$ (the smallest positive subnormal double), the algorithm adds a spurious term $-\\delta_{sub} \\log \\delta_{sub} > 0$ for every empty bin. With $M$ up to $10^6$ and a potentially large fraction of empty bins, the sum of these non-zero terms can introduce a significant, systematic positive bias to the computed entropy. This violates the unbiasedness requirement and constitutes the introduction of an artificial floor parameter, which is explicitly forbidden.\n- **Numerical Stability:** Naive summation ($S_{k+1} = S_k + x_{k+1}$) of $M \\approx 10^6$ floating-point numbers is known to be numerically unstable if the terms vary significantly in magnitude or if the running sum becomes much larger than the terms being added. This can lead to a large accumulation of round-off error. More robust methods like compensated summation are required for accuracy in such cases.\n- **Verdict:** **Incorrect**. The strategy is both mathematically biased and numerically unstable.\n\n**B. For integer counts $n_i$, avoid computing $\\log p_i$ by algebraically rewriting the entropy in terms of $N$ and $n_i$; skip bins with $n_i = 0$ to avoid evaluating $\\log 0$, and accumulate the sum using pairwise or compensated summation.**\n\n- **Mathematical Bias:** The proposed algebraic rewrite is as follows:\n$$H = -\\sum_{i=1}^{M} p_i \\log p_i = -\\sum_{i \\mid n_i>0} \\frac{n_i}{N} \\log\\left(\\frac{n_i}{N}\\right)$$\n$$H = -\\sum_{i \\mid n_i>0} \\frac{n_i}{N} (\\log n_i - \\log N)$$\n$$H = -\\frac{1}{N}\\left(\\sum_{i \\mid n_i>0} n_i \\log n_i\\right) + \\frac{\\log N}{N}\\left(\\sum_{i \\mid n_i>0} n_i\\right)$$\nSince $\\sum_{i \\mid n_i>0} n_i = N$, this simplifies to:\n$$H = \\log N - \\frac{1}{N} \\sum_{i \\mid n_i>0} n_i \\log n_i$$\nThis identity is exact. Skipping bins with $n_i=0$ correctly implements the $\\lim_{p \\to 0^+} p \\log p = 0$ convention. This method is therefore mathematically unbiased.\n- **Numerical Stability:** This formulation avoids the explicit calculation of small probabilities $p_i = n_i/N$, where precision could be lost. Instead, it computes a sum of terms $n_i \\log n_i$. While the terms in the sum can be large, the calculation avoids the potential loss of significance from summing quantities of vastly different magnitudes (e.g., $p_i \\log p_i$ for a large $p_i$ versus a small $p_i$). The final subtraction, $\\log N - (\\text{sum}/N)$, is generally well-conditioned. Crucially, the strategy specifies using \"pairwise or compensated summation\" to compute the sum $\\sum n_i \\log n_i$, which are known techniques to control round-off error accumulation for large sums.\n- **Verdict:** **Correct**. The strategy is mathematically unbiased, avoids artificial parameters, and uses numerically stable techniques for calculation and summation.\n\n**C. For probabilities $p_i$ obtained from $n_i/N$, clip each probability from below by a fixed $\\epsilon$, i.e., replace $p_i$ by $\\max(p_i,\\epsilon)$ with $\\epsilon = 10^{-12}$, and compute $H_{\\epsilon} = -\\sum_{i=1}^{M} \\max(p_i,\\epsilon)\\,\\log\\big(\\max(p_i,\\epsilon)\\big)$ without renormalizing.**\n\n- **Mathematical Bias:** This is a textbook example of a biased estimator. It introduces an artificial floor parameter $\\epsilon = 10^{-12}$, directly contradicting a constraint in the problem statement. For any bin where $p_i < \\epsilon$ (including all empty bins), the true probability is replaced by $\\epsilon$. This systematically alters the value being computed. Furthermore, the instruction \"without renormalizing\" means that the set of values being used, $\\tilde{p}_i = \\max(p_i, \\epsilon)$, do not sum to $1$. The resulting computation is not the Shannon entropy of any valid probability distribution, let alone the original one.\n- **Numerical Stability:** While this approach avoids the `log(0)` evaluation, its profound mathematical incorrectness makes its numerical properties irrelevant to the task of computing the true entropy.\n- **Verdict:** **Incorrect**. The strategy is mathematically biased, introduces a forbidden artificial parameter, and violates the definition of entropy.\n\n**D. Transform the sum to the log-domain using a log-sum-exp trick to avoid small $p_i$, i.e., compute $H$ as $-\\log\\!\\left(\\sum_{i=1}^{M} \\exp\\big(\\log p_i + \\log(\\log p_i)\\big)\\right)$.**\n\n- **Mathematical Bias:** This proposed transformation is mathematically nonsensical. Let's simplify the expression:\n$\\exp\\big(\\log p_i + \\log(\\log p_i)\\big) = \\exp\\big(\\log(p_i \\log p_i)\\big) = p_i \\log p_i$.\nThe proposed formula thus becomes:\n$$H_{\\text{proposed}} = -\\log\\left(\\sum_{i=1}^{M} p_i \\log p_i\\right)$$\nThe actual definition of entropy is $H = -\\sum_{i=1}^{M} p_i \\log p_i$.\nSo, this formula attempts to compute $H$ as $-\\log(-H)$. This is fundamentally incorrect. The log-sum-exp trick is used to compute quantities of the form $\\log(\\sum_i \\exp(x_i))$ and has no valid application here in this form.\n- **Numerical Stability:** The expression is not just wrong, it is incomputable for most inputs. For any bin with $p_i < 1$, $\\log p_i$ is negative. The term $\\log(\\log p_i)$ would then require taking the logarithm of a negative number, which is undefined in real arithmetic. Even for $p_i=1$, $\\log p_i = 0$, and $\\log(0)$ is undefined. The formula is invalid.\n- **Verdict:** **Incorrect**. The presented formula is a mathematical fallacy and cannot be implemented.\n\n**E. For weighted tallies $W_i \\ge 0$, compute the entropy without forming $p_i$ by using only $W$ and the nonzero $W_i$; use a compensated summation scheme to accumulate the final result, and skip bins with $W_i = 0$.**\n\n- **Mathematical Bias:** This is the weighted-tally analogue of option B. The probability is $p_i = W_i/W$. The same algebraic manipulation applies:\n$$H = \\log W - \\frac{1}{W} \\sum_{i \\mid W_i>0} W_i \\log W_i$$\nThis form is mathematically equivalent to the definition. By explicitly using only nonzero $W_i$, it correctly handles empty bins and is therefore unbiased. It does not introduce any artificial parameters.\n- **Numerical Stability:** As with option B, this formulation is numerically robust. It avoids explicit formation of small probabilities and uses the original tally data $W_i$ and $W$. Most importantly, it mandates the use of a \"compensated summation scheme,\" which is critical for maintaining accuracy when summing a large number ($M$) of terms. This strategy directly addresses the primary sources of numerical error in this calculation.\n- **Verdict:** **Correct**. This strategy is the proper extension of the sound method in option B to the case of weighted tallies. It is mathematically unbiased and numerically stable.",
            "answer": "$$\\boxed{BE}$$"
        },
        {
            "introduction": "With a solid grasp of entropy's properties and its stable computation, we can now apply it to a realistic diagnostic challenge: detecting physical signatures of source convergence. This capstone exercise simulates a reactor experiencing 'mode competition' and asks you to investigate how the choice of binning strategy impacts your ability to detect this phenomenon in the entropy time series. You will discover the fundamental trade-off between resolving spatial features and managing statistical noise, learning to design a measurement that optimizes the signal-to-noise ratio. ",
            "id": "4248434",
            "problem": "Consider a one-dimensional spatial domain $[0,1]$ representing a slab reactor with heterogeneous multiplication that produces a fission source density that is heavier near one boundary or the other depending on the dominant eigenmode of the neutron transport operator. Let $f_0(x)$ and $f_1(x)$ be two nonnegative, normalized prototype shapes for the fission source density, given by\n$$\nf_0(x) = c \\, e^{\\beta x}, \\quad f_1(x) = c \\, e^{\\beta (1 - x)}, \\quad c = \\frac{\\beta}{e^{\\beta} - 1},\n$$\nfor $x \\in [0,1]$ and a fixed shape parameter $\\beta > 0$. At cycle $n \\in \\{1,2,\\dots,N\\}$ of a Monte Carlo $k$-eigenvalue iteration, the continuous fission source density is modeled as the normalized mixture\n$$\nS_n(x) = \\alpha_n f_0(x) + (1 - \\alpha_n) f_1(x),\n$$\nwith a time-dependent mixture coefficient\n$$\n\\alpha_n = \\alpha^\\star + A \\, r^n \\cos(\\pi n),\n$$\nwhere $0 < r < 1$ is the dominance ratio (i.e., the magnitude of the second largest eigenvalue of the iteration operator divided by the largest), $A$ is a small amplitude, and $\\alpha^\\star$ is the asymptotic mixture. The Shannon entropy (SE) of the binned fission source estimate at cycle $n$ is defined by\n$$\nH_n = -\\sum_{b=1}^{B} p_{n,b} \\log p_{n,b},\n$$\nwhere $B$ is the number of spatial bins, $p_{n,b}$ is the probability mass in bin $b$ at cycle $n$, and the logarithm is the natural logarithm. The Monte Carlo estimator of $p_{n,b}$ is constructed from $M$ independent samples of $x$ from $S_n(x)$ per cycle and a chosen set of bin edges partitioning $[0,1]$.\n\nStarting from the above core definitions, build a complete, executable program that, for a specified set of binning strategies and sampling parameters, demonstrates the following reasoning tasks:\n\n1. Establish algorithmically, by computing the time series $\\{H_n\\}_{n=1}^N$, that coarse binning can hide mode competition signatures. Here, a \"mode competition signature\" refers to a detectable two-cycle oscillation in the Shannon entropy arising from the periodic factor $\\cos(\\pi n)$ in $\\alpha_n$.\n\n2. Show that excessively fine binning can inflate estimator variance for $\\{H_n\\}$ to the extent that the mode competition signature becomes undetectable given a fixed sample size $M$.\n\n3. Design an adaptive set of bins that reveals the signature without excessive variance. Use the following principle: for a target relative standard deviation per-bin of at most $\\varepsilon$, require the expected count per bin to satisfy $M \\int_{\\text{bin}} p_{\\mathrm{wr}}(x) \\, dx \\ge 1/\\varepsilon^2$, where $p_{\\mathrm{wr}}(x)$ is a worst-case envelope defined by\n$$\np_{\\mathrm{wr}}(x) = \\min\\left\\{ \\alpha_{\\min} f_0(x) + (1 - \\alpha_{\\min}) f_1(x), \\; \\alpha_{\\max} f_0(x) + (1 - \\alpha_{\\max}) f_1(x) \\right\\}, \n$$\nwith $\\alpha_{\\min} = \\alpha^\\star - A$ and $\\alpha_{\\max} = \\alpha^\\star + A$. Construct bin edges by partitioning $[0,1]$ into consecutive intervals whose $p_{\\mathrm{wr}}$-mass is approximately equal to $1/(\\varepsilon^2 M)$, using numerical quadrature to approximate the integrals.\n\nUse the following two-cycle detection statistic based purely on the Shannon entropy time series:\n$$\nS_{\\mathrm{det}} = \\left| \\frac{1}{N} \\sum_{n=1}^{N} (-1)^n H_n \\right|,\n$$\nand declare that a signature is detected if\n$$\nS_{\\mathrm{det}} > \\tau, \\quad \\text{with} \\quad \\tau = k \\frac{\\sigma_H}{\\sqrt{N}},\n$$\nwhere $\\sigma_H$ is the sample standard deviation of $\\{H_n\\}_{n=1}^N$ and $k$ is a fixed constant. This criterion compares the measured two-cycle amplitude in $\\{H_n\\}$ to an estimate of its stochastic variability.\n\nYour program must implement the following steps for each test case:\n- Generate $\\{ \\alpha_n \\}_{n=1}^N$ using the provided $(\\alpha^\\star, A, r)$.\n- For each $n$, sample $M$ positions from $S_n(x)$ exactly by inverse transform sampling of the mixture of $f_0$ and $f_1$.\n- Compute $H_n$ from the histogram over the specified bin edges.\n- Compute $S_{\\mathrm{det}}$, $\\sigma_H$, and apply the detection rule to return a boolean indicating whether the mode competition signature is detected.\n\nTest Suite:\nUse the fixed parameters $\\beta = 3.0$, $\\alpha^\\star = 0.65$, $A = 0.25$, $r = 0.95$, $N = 60$, and $k = 2.0$. For reproducibility, use a fixed random seed in the pseudorandom number generator.\n\nEvaluate the following five test cases:\n- Case $1$ (coarse uniform): uniform binning with $B = 2$ and $M = 4000$.\n- Case $2$ (coarse uniform): uniform binning with $B = 4$ and $M = 4000$.\n- Case $3$ (fine uniform, noisy): uniform binning with $B = 100$ and $M = 2000$.\n- Case $4$ (adaptive, variance controlled): adaptive binning with $\\varepsilon = 0.1$ and $M = 2000$ built from $p_{\\mathrm{wr}}(x)$.\n- Case $5$ (moderate uniform, well-sampled): uniform binning with $B = 40$ and $M = 20000$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each $result_i$ is a boolean value indicating whether the signature was detected in Case $i$. No physical units are involved, and all angles are in radians by construction (through $\\cos(\\pi n)$). The outputs must be deterministic given the fixed seed.",
            "solution": "The user wants to solve the problem, so I will now generate the solution.\nThe problem is validated as sound and well-posed. The a priori analysis confirms its scientific and mathematical integrity. I will now provide the complete solution.\n\nThe problem asks for a computational demonstration of how different statistical estimation strategies affect the ability to detect a physical signature—mode competition in a nuclear reactor fission source—using Shannon entropy. This requires implementing a Monte Carlo simulation based on a simplified one-dimensional, two-mode model of fission source convergence. The solution involves sampling from probability distributions, applying different data binning techniques, and performing a statistical analysis on the resulting time series.\n\nThe core of the model lies in the time-dependent fission source density, $S_n(x)$, for cycle $n$:\n$$S_n(x) = \\alpha_n f_0(x) + (1 - \\alpha_n) f_1(x)$$\nwhere $f_0(x) = c \\, e^{\\beta x}$ and $f_1(x) = c \\, e^{\\beta (1 - x)}$ are two asymmetric, normalized distributions on $x \\in [0,1]$, with $c = \\beta/(e^{\\beta} - 1)$. The mixture coefficient, $\\alpha_n = \\alpha^\\star + A \\, r^n \\cos(\\pi n) = \\alpha^\\star + A \\, r^n (-1)^n$, causes the source shape to oscillate between being dominated by $f_0(x)$ on even cycles and $f_1(x)$ (or a shape closer to it) on odd cycles, with the oscillation amplitude decaying with a factor $r^n$. This oscillation is the \"mode competition signature\" we aim to detect.\n\nThe Shannon entropy, $H_n = -\\sum_{b=1}^{B} p_{n,b} \\log p_{n,b}$, serves as a scalar metric of the source's spatial distribution. Here, $p_{n,b}$ is the probability mass in spatial bin $b$ at cycle $n$. As $S_n(x)$ oscillates, we expect $H_n$ to oscillate as well. A change in the spatial concentration of the source (e.g., from being peaked at $x=1$ to being peaked at $x=0$) will generally lead to a change in the entropy of its binned representation. The detection of this signature in $H_n$ is formulated as a signal-to-noise problem. The signal is the amplitude of the two-cycle oscillation in the entropy series $\\{H_n\\}_{n=1}^N$, measured by the statistic $S_{\\mathrm{det}} = \\left| \\frac{1}{N} \\sum_{n=1}^{N} (-1)^n H_n \\right|$. The noise is quantified by the statistical uncertainty in the entropy estimates, captured by the threshold $\\tau = k \\frac{\\sigma_H}{\\sqrt{N}}$, where $\\sigma_H$ is the sample standard deviation of the entropy series. A signature is declared \"detected\" if $S_{\\mathrm{det}} > \\tau$.\n\nThe simulation proceeds by generating $M$ independent random samples from $S_n(x)$ for each cycle $n$. This is achieved by sampling from a mixture distribution, which is a two-step process: first, a random choice is made to select either $f_0$ or $f_1$ with probabilities $\\alpha_n$ and $1-\\alpha_n$, respectively; second, a sample is drawn from the selected distribution using inverse transform sampling. The cumulative distribution functions (CDFs) for $f_0(x)$ and $f_1(x)$ are:\n$$F_0(x) = \\int_0^x c e^{\\beta t} dt = \\frac{e^{\\beta x} - 1}{e^{\\beta} - 1}$$\n$$F_1(x) = \\int_0^x c e^{\\beta (1-t)} dt = \\frac{e^\\beta - e^{\\beta(1-x)}}{e^\\beta - 1}$$\nBy inverting these functions, we obtain the formulas to transform a uniform random variate $u \\in [0,1]$ into a sample $x$:\n$$F_0^{-1}(u) = \\frac{1}{\\beta} \\log\\left(u(e^\\beta - 1) + 1\\right)$$\n$$F_1^{-1}(u) = 1 - \\frac{1}{\\beta} \\log\\left(e^\\beta - u(e^\\beta - 1)\\right)$$\n\nThe choice of binning strategy is critical, as it mediates the trade-off between resolving the spatial features of $S_n(x)$ and controlling statistical noise.\n1.  **Coarse Binning**: Using a very small number of bins, $B$, fails to capture the changing shape of $S_n(x)$. For example, with $B=2$ and bins $[0, 0.5]$ and $[0.5, 1]$, the probability mass in each bin might not change significantly as $\\alpha_n$ oscillates, especially if the distributions $f_0$ and $f_1$ are not strongly asymmetric. Consequently, $H_n$ remains nearly constant, and the signature is missed ($S_{\\mathrm{det}} \\approx 0$).\n2.  **Fine Binning**: Using a large number of bins, $B$, creates a high-resolution histogram. However, for a fixed sample size $M$, this results in very few samples per bin. The statistical estimator for the bin probability, $\\hat{p}_{n,b} = N_{n,b}/M$, where $N_{n,b}$ is the count in bin $b$, has a variance that scales as $\\text{Var}(\\hat{p}_{n,b}) \\approx p_{n,b}/M$. For small counts, the relative uncertainty is large. This high statistical noise on each $p_{n,b}$ propagates into the calculation of $H_n$, creating a noisy time series where the small deterministic oscillation is completely swamped ($\\sigma_H$ becomes large, making $\\tau$ large and detection unlikely).\n3.  **Adaptive Binning**: This strategy is designed to balance resolution and noise. The principle is to create bins that have roughly equal expected particle counts, ensuring that no bin is \"starved\" of samples. The condition is that for a chosen worst-case probability density $p_{\\mathrm{wr}}(x)$, the expected count in each bin $b$ should satisfy $M \\int_b p_{\\mathrm{wr}}(x) dx \\ge 1/\\varepsilon^2$, where $\\varepsilon$ is the target maximum relative standard deviation for the count in any bin. The problem defines $p_{\\mathrm{wr}}(x)$ as the \"envelope\" that is pointwise minimum of the two extreme source shapes corresponding to $\\alpha_{\\min}=\\alpha^\\star - A$ and $\\alpha_{\\max}=\\alpha^\\star + A$. Bin edges are found by numerically solving for intervals that contain a target mass of $1/(\\varepsilon^2 M)$ according to $p_{\\mathrmWR}(x')$. This requires numerical quadrature (`scipy.integrate.quad`) to compute the mass integral and a numerical root-finder (`scipy.optimize.brentq`) to locate the bin edges. This approach ensures an adequate number of samples per bin across the domain, stabilizing the entropy estimates and allowing the underlying physical oscillation to be detected.\n\nThe program will execute these steps for five distinct test cases, each representing a different combination of binning strategy and sample size, to demonstrate these effects algorithmically and report a boolean detection result for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\n\n# -- 1. PROBLEM PARAMETERS AND MODEL DEFINITIONS --\n\n# Fixed physical and simulation parameters from the problem statement\nBETA = 3.0\nALPHA_STAR = 0.65\nA = 0.25\nR = 0.95\nN_CYCLES = 60\nK_FACTOR = 2.0\nSEED = 12345\n\n# Normalization constant for f0 and f1\nC = BETA / (np.exp(BETA) - 1.0)\n\ndef f0(x, beta, c):\n    \"\"\"Prototype fission source density f_0(x).\"\"\"\n    return c * np.exp(beta * x)\n\ndef f1(x, beta, c):\n    \"\"\"Prototype fission source density f_1(x).\"\"\"\n    return c * np.exp(beta * (1.0 - x))\n\ndef invF0(u, beta):\n    \"\"\"Inverse CDF of f_0(x) for sampling.\"\"\"\n    return (1.0 / beta) * np.log(u * (np.exp(beta) - 1.0) + 1.0)\n\ndef invF1(u, beta):\n    \"\"\"Inverse CDF of f_1(x) for sampling.\"\"\"\n    return 1.0 - (1.0 / beta) * np.log(np.exp(beta) - u * (np.exp(beta) - 1.0))\n\ndef sample_Sn(alpha, M, beta, rng):\n    \"\"\"Samples M positions from the mixture distribution S_n(x) using inverse transform sampling.\"\"\"\n    component_choice = rng.uniform(0.0, 1.0, size=M)\n    u_for_inversion = rng.uniform(0.0, 1.0, size=M)\n    \n    samples = np.zeros(M)\n    \n    f0_mask = component_choice < alpha\n    num_f0 = np.sum(f0_mask)\n    if num_f0 > 0:\n        samples[f0_mask] = invF0(u_for_inversion[f0_mask], beta)\n        \n    f1_mask = ~f0_mask\n    num_f1 = M - num_f0\n    if num_f1 > 0:\n        samples[f1_mask] = invF1(u_for_inversion[f1_mask], beta)\n        \n    return samples\n\n# -- 2. ADAPTIVE BINNING ALGORITHM --\n\ndef get_adaptive_bins(M, epsilon, beta, c, alpha_star, A):\n    \"\"\"\n    Constructs adaptive bin edges based on the worst-case envelope p_wr(x).\n    \"\"\"\n    alpha_min = alpha_star - A\n    alpha_max = alpha_star + A\n    \n    p_wr_args = (beta, c, alpha_min, alpha_max)\n\n    def p_wr(x, beta_val, c_val, alpha_min_val, alpha_max_val):\n        \"\"\"The worst-case envelope PDF.\"\"\"\n        s_min = alpha_min_val * f0(x, beta_val, c_val) + (1.0 - alpha_min_val) * f1(x, beta_val, c_val)\n        s_max = alpha_max_val * f0(x, beta_val, c_val) + (1.0 - alpha_max_val) * f1(x, beta_val, c_val)\n        return np.minimum(s_min, s_max)\n\n    target_mass = 1.0 / (epsilon**2 * M)\n    bin_edges = [0.0]\n    \n    while bin_edges[-1] < 1.0:\n        current_edge = bin_edges[-1]\n        \n        remaining_mass, _ = quad(p_wr, current_edge, 1.0, args=p_wr_args)\n        \n        # If remaining mass is less than or slightly above the target, just add the final edge and stop.\n        if remaining_mass < target_mass * 1.05:\n            break\n            \n        def objective(x):\n            \"\"\"Function to find root of: integral from current_edge to x equals target_mass.\"\"\"\n            mass, _ = quad(p_wr, current_edge, x, args=p_wr_args)\n            return mass - target_mass\n\n        try:\n            # Bracket for root finding is (current_edge, 1.0)\n            next_edge = brentq(objective, current_edge, 1.0)\n            bin_edges.append(next_edge)\n        except ValueError:\n            # Could happen if signs are not different, e.g., due to precision.\n            break\n\n    if bin_edges[-1] < 1.0:\n        bin_edges.append(1.0)\n        \n    return np.array(bin_edges)\n\n# -- 3. SIMULATION AND ANALYSIS --\n\ndef run_one_case(case_config):\n    \"\"\"\n    Runs the full simulation for one test case and returns the detection result.\n    \"\"\"\n    M, bin_type, bin_param, epsilon = case_config\n    \n    rng = np.random.default_rng(SEED)\n\n    if bin_type == 'uniform':\n        B = bin_param\n        bin_edges = np.linspace(0.0, 1.0, B + 1)\n    elif bin_type == 'adaptive':\n        bin_edges = get_adaptive_bins(M, epsilon, BETA, C, ALPHA_STAR, A)\n    else:\n        raise ValueError(\"Unknown binning type\")\n\n    n_vals = np.arange(1, N_CYCLES + 1)\n    alpha_n_series = ALPHA_STAR + A * (R**n_vals) * np.cos(np.pi * n_vals)\n    \n    H_series = np.zeros(N_CYCLES)\n    for i in range(N_CYCLES):\n        alpha_n = alpha_n_series[i]\n        \n        samples = sample_Sn(alpha_n, M, BETA, rng)\n        \n        counts, _ = np.histogram(samples, bins=bin_edges)\n        \n        # We must use the actual number of generated particles for normalization\n        total_samples = np.sum(counts) \n        if total_samples > 0:\n            probs = counts / total_samples\n            # Filter out zero probabilities for log calculation\n            nz_probs = probs[probs > 0]\n            H_n = -np.sum(nz_probs * np.log(nz_probs))\n            H_series[i] = H_n\n\n    # Calculate detection statistic and threshold\n    S_det = np.abs(np.mean((-1.0)**n_vals * H_series))\n    sigma_H = np.std(H_series, ddof=1)\n    \n    # Avoid division by zero if all H_n are identical\n    if sigma_H == 0:\n        return False\n        \n    tau = K_FACTOR * sigma_H / np.sqrt(N_CYCLES)\n    \n    return S_det > tau\n\n# -- 4. MAIN EXECUTION --\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the simulations, and prints the final results.\n    \"\"\"\n    test_cases = [\n        # ( M,      bin_type,  bin_param, epsilon)\n        (4000,   'uniform', 2,         None),  # Case 1: Coarse uniform\n        (4000,   'uniform', 4,         None),  # Case 2: Coarse uniform\n        (2000,   'uniform', 100,       None),  # Case 3: Fine uniform, noisy\n        (2000,   'adaptive',None,      0.1),   # Case 4: Adaptive, variance controlled\n        (20000,  'uniform', 40,        None),  # Case 5: Moderate uniform, well-sampled\n    ]\n\n    results = []\n    for case in test_cases:\n        is_detected = run_one_case(case)\n        results.append(is_detected)\n\n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}