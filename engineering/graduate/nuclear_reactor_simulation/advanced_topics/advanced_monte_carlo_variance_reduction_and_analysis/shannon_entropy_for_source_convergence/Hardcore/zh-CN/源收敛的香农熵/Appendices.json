{
    "hands_on_practices": [
        {
            "introduction": "第一个实践将建立香农熵在应用于离散化连续系统时的一个基本性质。我们将探讨计算出的熵值 $H$ 如何不仅仅是底层连续源形状的内在属性，而是强烈依赖于所选网格的分辨率。这项练习旨在帮助您建立解读模拟中熵趋势的直觉，因为在实际应用中，网格划分是一个至关重要的参数。",
            "id": "4248386",
            "problem": "你的任务是设计一项基于与核反应堆模拟相关的概率建模的计算研究，以评估香non熵 $H(p)$ 对空间栅格网格分辨率的敏感性。在反应堆源迭代中，一个空间变化的源强度通常通过网格栅元上的离散空间分布来概括，并监测该分布的香农熵以判断收敛性。你的程序必须量化，对于相同的底层源形状，该度量如何随着网格的加密和粗化而变化。\n\n使用以下纯数学设定。考虑一个二维平板区域 $D = [-1,1] \\times [-1,1]$，以及在此区域 $D$ 上定义的非负源密度 $s(x,y)$。对于给定的网格分辨率参数 $N \\in \\mathbb{Z}_{0}$，将 $D$ 划分成 $N \\times N$ 个相等的矩形栅元。通过在栅元中点处评估 $s(x,y)$ 的值并乘以栅元面积来近似计算每个栅元中的概率质量，然后在所有栅元上进行归一化，以获得一个离散概率向量 $p = (p_1,\\dots,p_{M})$，其中 $M = N^2$。使用自然对数计算 $p$ 的香农熵，并以奈特（nats）为单位表示结果。\n\n待测试的源形状为：\n- 均匀平板源：$s_{\\text{uni}}(x,y) = 1$。\n- 以原点为中心的各向同性高斯源：$s_{\\text{iso}}(x,y) = \\exp\\!\\left(-\\dfrac{x^2 + y^2}{2\\sigma^2}\\right)$，其中 $\\sigma = 0.25$。\n- 以原点为中心的各向异性高斯源：$s_{\\text{aniso}}(x,y) = \\exp\\!\\left(-\\dfrac{1}{2}\\left(\\dfrac{x^2}{\\sigma_x^2} + \\dfrac{y^2}{\\sigma_y^2}\\right)\\right)$，其中 $\\sigma_x = 0.10$ 且 $\\sigma_y = 0.50$。\n\n你的程序必须按照规定实现从 $s(x,y)$ 构建离散分布 $p$ 的过程，并为每个测试用例计算以奈特为单位的香农熵 $H(p)$。无需用户输入。\n\n测试套件：\n- 用例 $1$：$s_{\\text{uni}}$，$N = 1$（极端粗化基线）。\n- 用例 $2$：$s_{\\text{uni}}$，$N = 64$（基线的强加密）。\n- 用例 $3$：$s_{\\text{iso}}$，其中 $\\sigma = 0.25$，$N = 4$（粗网格）。\n- 用例 $4$：$s_{\\text{iso}}$，其中 $\\sigma = 0.25$，$N = 64$（加密网格）。\n- 用例 $5$：$s_{\\text{aniso}}$，其中 $\\sigma_x = 0.10$, $\\sigma_y = 0.50$，$N = 4$（带各向异性的粗网格）。\n- 用例 $6$：$s_{\\text{aniso}}$，其中 $\\sigma_x = 0.10$, $\\sigma_y = 0.50$，$N = 64$（带各向异性的加密网格）。\n\n答案规格：\n- 按上文所述为每个测试用例计算以奈特为单位的 $H(p)$。将每个值四舍五入到 $6$ 位小数。\n- 你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序与上述用例一致（例如 $\\texttt{[h1,h2,h3,h4,h5,h6]}$），其中每个 $h_k$ 是以奈特为单位、四舍五入到 $6$ 位小数的 $H(p)$ 的十进制表示。\n\n科学和算法约束：\n- 从基础概率定义开始：$D$ 上的概率测度、通过在栅元上进行求积来构建离散分布，以及基于自然对数的离散分布信息度量。\n- 你的算法必须在每个栅元内使用中点评估和等面积分箱。\n- 区域 $D$、源形状和参数都是无量纲的；除了熵单位外，没有其他物理单位。熵值必须以奈特（nats）表示。",
            "solution": "目标是评估在核反应堆模拟中用作离散源分布离散度的信息论度量的香农熵 $H(p)$，在底层源形状固定的情况下，如何依赖于空间栅格网格的分辨率。我们从基本概率原理和离散分布的香农熵定义开始。\n\n理论基础：\n- 令 $D = [-1,1] \\times [-1,1]$ 表示平板区域。一个非负源密度 $s(x,y) \\ge 0$ 通过在 $D$ 的子集上对 $s(x,y)$ 进行积分来定义 $D$ 上的一个有限测度。\n- 对于给定的网格分辨率 $N \\in \\mathbb{Z}_{0}$，我们将 $D$ 划分成 $N \\times N$ 个等面积栅元。每个栅元的边长为 $\\Delta = \\dfrac{2}{N}$，面积为 $a = \\Delta^2 = \\dfrac{4}{N^2}$。\n- 离散概率向量 $p = (p_1, \\dots, p_M)$（其中 $M=N^2$）是通过使用中点规则近似每个栅元中的质量来构建的。令 $(x_i, y_i)$ 为栅元 $i$ 的中心。定义未归一化的质量 $m_i = s(x_i, y_i) \\cdot a$。归一化后的概率为\n$$\np_i = \\frac{m_i}{\\sum_{j=1}^{M} m_j} = \\frac{s(x_i,y_i) \\, a}{\\sum_{j=1}^{M} s(x_j,y_j) \\, a} = \\frac{s(x_i,y_i)}{\\sum_{j=1}^{M} s(x_j,y_j)},\n$$\n其中等式成立的原因是所有栅元的面积 $a$ 都是常数。这样就构建了一个与底层源密度和等面积分箱一致的离散分布。\n\n香农熵：\n- 对于一个离散分布 $p = (p_1,\\dots,p_M)$，以自然单位（奈特）表示的香农熵定义为\n$$\nH(p) = - \\sum_{i=1}^{M} p_i \\ln p_i,\n$$\n其中 $\\ln$ 表示自然对数。这个度量量化了分布的信息内容或不确定性。对于给定栅元上的均匀分布，它达到最大值，并随着分布变得更加尖锐而减小。\n\n网格分辨率敏感性（推导）：\n- 考虑近似 $p_i \\approx \\frac{s(x_i,y_i) a}{S}$，其中 $S = \\iint_{D} s(x,y) \\, dx \\, dy$ 是总质量；在中点求积法下，$\\sum_i s(x_i,y_i) a \\approx S$，从而得到 $p_i \\approx s(x_i,y_i) a / S$。\n- 将其代入熵的定义可得\n$$\nH(p) \\approx - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln\\left(\\frac{s(x_i,y_i) a}{S}\\right)\n= - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln\\left(\\frac{s(x_i,y_i)}{S}\\right) - \\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\ln(a).\n$$\n- 由于 $\\sum_{i=1}^{M} \\frac{s(x_i,y_i) a}{S} \\approx 1$，第二项简化为 $- \\ln(a)$。第一项是连续微分熵的黎曼和近似\n$$\nh(s) = - \\iint_{D} \\frac{s(x,y)}{S} \\ln\\left(\\frac{s(x,y)}{S}\\right) \\, dx \\, dy.\n$$\n- 因此，当网格被均匀加密时（即 $a \\to 0$ 且 $M \\to \\infty$），离散熵的行为类似于\n$$\nH(p) \\approx h(s) - \\ln(a) = h(s) + \\ln\\left(\\frac{1}{a}\\right).\n$$\n- 因为在我们的区域中 $a = \\frac{4}{N^2}$，我们得到\n$$\nH(p) \\approx h(s) + 2 \\ln(N) - \\ln(4).\n$$\n- 这表明对于一个固定的源形状 $s(x,y)$，$H(p)$ 随 $\\ln(N)$ 近似线性增长，其加性常数取决于区域面积和归一化连续密度的微分熵。对于栅元上完全均匀的源，$p_i = \\frac{1}{M}$ 对所有 $i$ 成立，且 $H(p) = \\ln(M) = 2 \\ln(N)$ 精确成立，与预期的最大值相符。对于尖峰源，$H(p)$ 小于同样条件下的均匀情况界限，但由于 $-\\ln(a)$ 项的存在，仍会随着网格加密而增加。\n\n算法设计：\n- 为 $D$ 上的 $N \\times N$ 个栅元构建中心点 $(x_i,y_i)$，使用中点坐标：$x_k = -1 + \\left(k + \\frac{1}{2}\\right)\\Delta$ 和 $y_\\ell = -1 + \\left(\\ell + \\frac{1}{2}\\right)\\Delta$，其中 $k,\\ell \\in \\{0,\\dots,N-1\\}$ 且 $\\Delta = \\frac{2}{N}$。\n- 对每种源形状评估 $s(x_i,y_i)$：\n  - 均匀源：$s_{\\text{uni}}(x,y) = 1$。\n  - 各向同性高斯源：$s_{\\text{iso}}(x,y) = \\exp\\!\\left(-\\frac{x^2 + y^2}{2 \\sigma^2}\\right)$，其中 $\\sigma = 0.25$。\n  - 各向异性高斯源：$s_{\\text{aniso}}(x,y) = \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x^2}{\\sigma_x^2} + \\frac{y^2}{\\sigma_y^2}\\right)\\right)$，其中 $\\sigma_x = 0.10$ 且 $\\sigma_y = 0.50$。\n- 形成未归一化的向量 $m_i = s(x_i,y_i)$（恒定的面积因子在归一化过程中被抵消），并进行归一化以获得 $p_i = \\frac{m_i}{\\sum_j m_j}$。\n- 通过 $H(p) = - \\sum_i p_i \\ln p_i$ 计算以奈特为单位的 $H(p)$。\n- 将此方法应用于测试套件中的各个用例，并将每个结果四舍五入到 $6$ 位小数。\n\n预期行为和边界情况：\n- 用例 $1$ 中 $N=1$ 产生 $M=1$ 和单栅元分布 $p_1=1$，因此以奈特为单位的 $H(p)=0$，代表极端粗化基线。\n- 对于均匀源，$H(p)$ 精确地等于 $2 \\ln(N)$；因此从 $N=1$ 到 $N=64$，它会大幅增加。\n- 对于高斯源，在相同的 $N$ 值下，$H(p)$ 严格小于均匀分布的最大值，并随着网格加密而增加，这是因为对空间变异性的解析更加精细。\n- 如果各向异性增加了沿某一轴的集中度，那么在相同的 $N$ 值下，各向异性高斯源相对于各向同性源表现出更低的熵，但仍然表现出对 $N$ 的加性 $-\\ln(a)$ 敏感性。\n\n最终的程序实现了这些步骤，计算了所需的以奈特为单位的熵，四舍五入到 $6$ 位小数，并按规定顺序输出单行带方括号的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_grid_centers(N, domain_min=-1.0, domain_max=1.0):\n    # Compute midpoints of N bins along each axis over [domain_min, domain_max]\n    length = domain_max - domain_min\n    delta = length / N\n    # Midpoints: domain_min + (k + 0.5) * delta\n    coords = domain_min + (np.arange(N) + 0.5) * delta\n    X, Y = np.meshgrid(coords, coords, indexing='xy')\n    return X, Y\n\ndef s_uniform(x, y):\n    return np.ones_like(x)\n\ndef s_isotropic_gaussian(x, y, sigma=0.25):\n    return np.exp(-((x**2 + y**2) / (2.0 * sigma**2)))\n\ndef s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50):\n    return np.exp(-0.5 * ((x / sigma_x)**2 + (y / sigma_y)**2))\n\ndef discrete_entropy_from_density(N, source_fn):\n    # Construct grid\n    X, Y = make_grid_centers(N)\n    # Evaluate source\n    S = source_fn(X, Y)\n    # Normalize to probabilities; area factor cancels as it's constant for all bins\n    total = np.sum(S)\n    # Guard against pathological zero total (should not occur for provided sources)\n    if total == 0.0:\n        # If total is zero, define zero-entropy (degenerate); but avoid division by zero.\n        return 0.0\n    p = S / total\n    # Compute Shannon entropy in nats: -sum p ln p, treating p=0 terms as 0\n    # Since Gaussian and uniform are positive, zeros should not occur; still robust handling:\n    mask = p > 0.0\n    H = -np.sum(p[mask] * np.log(p[mask]))\n    return float(H)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (label, N, source_function)\n    test_cases = [\n        (\"uniform_N1\", 1, lambda x, y: s_uniform(x, y)),\n        (\"uniform_N64\", 64, lambda x, y: s_uniform(x, y)),\n        (\"isoGauss_sigma0.25_N4\", 4, lambda x, y: s_isotropic_gaussian(x, y, sigma=0.25)),\n        (\"isoGauss_sigma0.25_N64\", 64, lambda x, y: s_isotropic_gaussian(x, y, sigma=0.25)),\n        (\"anisoGauss_sx0.10_sy0.50_N4\", 4, lambda x, y: s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50)),\n        (\"anisoGauss_sx0.10_sy0.50_N64\", 64, lambda x, y: s_anisotropic_gaussian(x, y, sigma_x=0.10, sigma_y=0.50)),\n    ]\n\n    results = []\n    for _, N, src in test_cases:\n        H = discrete_entropy_from_density(N, src)\n        # Round to 6 decimal places as specified\n        results.append(round(H, 6))\n\n    # Final print statement in the exact required format with 6-decimal formatting.\n    # Ensure fixed decimal representation\n    results_str = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在真实的蒙特卡洛模拟中，数据往往是稀疏的，许多区域的源贡献为零或接近于零。本实践旨在解决一个实际挑战：这些“空区”或“近空区”如何影响熵的计算。您将实现并比较不同的处理策略，包括简单的阈值排除和考虑空间连续性的聚合方法，从而掌握处理稀疏数据的稳健技术，确保熵度量作为一个有意义的诊断工具。",
            "id": "4248439",
            "problem": "在蒙特卡洛（MC）核反应堆模拟的背景下，通常使用离散化空间域上的信息度量来监测裂变源的收敛性。考虑一个由 $M$ 个栅元组成的一维空间网格。令非负裂变源统计值为 $\\mathbf{f} = (f_1, f_2, \\dots, f_M)$，其可解释为在固定时间间隔内每个栅元的预期中子产额率的比例。使用 Kolmogorov 概率公理，通过对统计值进行归一化来构建离散概率分布 $\\mathbf{p} = (p_1, p_2, \\dots, p_M)$，其中 $p_i = f_i / \\sum_{j=1}^M f_j$。在空区或近空区中，许多栅元可能具有 $p_i = 0$ 或极小的值，这使得使用信息度量进行收敛性评估变得复杂。\n\n您将实施三种策略来量化和减轻空区或近零通量区域对信息度量的影响：\n- 使用所有栅元的基准策略。\n- 使用固定阈值排除近零概率栅元。\n- 将连续的近零栅元聚合为单个粗化栅元的邻接感知聚合策略。\n\n您的推导基础必须是 Kolmogorov 概率公理和信息论中使用的信息度量的既定属性（连续性、重标记下的对称性和分组属性）。您不得假设或引用任何专门的简化公式。您的程序必须：\n1. 通过归一化从 $\\mathbf{f}$ 构建 $\\mathbf{p}$。\n2. 以自然单位（nats）计算与上述公理一致的信息度量 $H(\\mathbf{p})$。对于 $p_i = 0$ 的栅元，通过极限值解释 $p_i \\log p_i$，使这些栅元的贡献为 $0$。\n3. 计算两种归一化形式以评估栅元数量的敏感性：\n   - 按总栅元数 $M$ 归一化，定义为当 $M \\ge 2$ 时为 $H(\\mathbf{p}) / \\log M$，当 $M = 1$ 时定义为 $0$。\n   - 按活跃栅元数 $M_{\\text{active}}$ 归一化，其中 $M_{\\text{active}} = \\#\\{ i \\mid p_i  0 \\}$，定义为当 $M_{\\text{active}} \\ge 2$ 时为 $H(\\mathbf{p}) / \\log M_{\\text{active}}$，当 $M_{\\text{active}} = 1$ 时定义为 $0$。\n4. 排除策略：对于给定的阈值 $\\varepsilon  0$，通过移除所有 $p_i \\le \\varepsilon$ 的栅元 $i$ 来形成 $\\mathbf{p}^{(\\text{excl})}$，然后重新归一化剩余的概率，使其总和为 $1$。计算 $H(\\mathbf{p}^{(\\text{excl})})$。\n5. 邻接感知聚合策略：对于相同的阈值 $\\varepsilon$，从左到右扫描 $\\mathbf{p}$，并将任何满足对于所有 $k \\in \\{i, \\dots, j\\}$ 都有 $p_k \\le \\varepsilon$ 的最大连续索引序列 $\\{i, i+1, \\dots, j\\}$ 压缩成一个概率等于 $\\sum_{k=i}^j p_k$ 的聚合栅元。如果聚合概率等于 $0$，则省略此聚合栅元。保留所有 $p_\\ell  \\varepsilon$ 的栅元作为独立栅元。将得到的粗化分布表示为 $\\mathbf{q}$，其具有 $M_{\\text{agg}}$ 个栅元。计算 $H(\\mathbf{q})$ 并报告 $M_{\\text{agg}}$。\n6. 所有对数均使用自然对数底数 $e$，以确保信息度量以 nats 表示。最终结果中不需要物理单位。\n\n测试套件：\n为以下四个测试用例实施上述操作，每个用例由 $(\\mathbf{f}, \\varepsilon)$ 指定。\n\n- 用例 1（包含几个精确空区的理想情况）：\n  - $M = 10$\n  - $\\mathbf{f} = (1, 1, 0, 1, 0, 1, 1, 0, 1, 1)$\n  - $\\varepsilon = 0$\n- 用例 2（精确空区和近零统计值的混合）：\n  - $M = 10$\n  - $\\mathbf{f} = (10^{-14}, 0, 10^{-14}, 0, 5, 10^{-14}, 0, 10^{-14}, 0, 5)$\n  - $\\varepsilon = 10^{-12}$\n- 用例 3（边界情况：一个主导栅元和一段扩展的近零序列）：\n  - $M = 12$\n  - $\\mathbf{f} = (10^{-15}, 10^{-15}, 10^{-15}, 10^{-15}, 10^{-15}, 10^{-15}, 10^{-15}, 10^{-15}, 10^{-15}, 10^{-15}, 10^{-15}, 1)$\n  - $\\varepsilon = 10^{-12}$\n- 用例 4（均匀活跃区域周围的空区段）：\n  - $M = 20$\n  - $\\mathbf{f} = (0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0)$\n  - $\\varepsilon = 0$\n\n对于每个用例，您的程序必须输出一个包含六个值的列表：\n- $H(\\mathbf{p})$，以 nats 为单位，\n- $H(\\mathbf{p}) / \\log M$，\n- $H(\\mathbf{p}) / \\log M_{\\text{active}}$，\n- $H(\\mathbf{p}^{(\\text{excl})})$，以 nats 为单位，\n- $H(\\mathbf{q})$，以 nats 为单位，\n- $M_{\\text{agg}}$，作为整数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个用例的结果，格式为方括号括起来的逗号分隔列表，并严格按照测试套件的顺序，例如 $[[\\dots], [\\dots], [\\dots], [\\dots]]$。所有数值输出必须是浮点值或整数，不带任何单位符号。不涉及角度。不涉及百分比；所有分数量必须表示为小数。",
            "solution": "我们从 Kolmogorov 概率公理开始。设 $\\Omega$ 是与空间栅元对应的索引 $\\{1, \\dots, M\\}$ 的有限样本空间，并设 $P$ 是 $\\Omega$ 上的一个概率测度。在蒙特卡洛核反应堆模拟的背景下，非负统计值 $\\mathbf{f} = (f_1, \\dots, f_M)$ 与固定时间间隔内每个栅元的预期中子产额强度成正比。根据公理，一个合法的离散概率分布 $\\mathbf{p} = (p_1, \\dots, p_M)$ 满足 $p_i \\ge 0$ 和 $\\sum_{i=1}^M p_i = 1$。从 $\\mathbf{f}$ 构建 $\\mathbf{p}$ 是通过归一化 $p_i = f_i / \\sum_{j=1}^M f_j$ 完成的，这保留了非负性，并确保在 $\\sum_j f_j  0$ 的条件下 $\\sum_i p_i = 1$。\n\n我们需要一个信息度量 $H(\\mathbf{p})$，它与信息论的基本属性一致：在 $\\mathbf{p}$ 上的连续性、在栅元重标记下的对称性以及分组属性（也称为递归性或关于粗粒化的可加性）。这些属性唯一地刻画了香农熵（SE），仅相差一个对数底数的选择。在自然单位（nats）下，香农熵为\n$$\nH(\\mathbf{p}) = -\\sum_{i=1}^M p_i \\log p_i ,\n$$\n其中 $\\log$ 表示自然对数。分组属性进一步意味着，如果将一个栅元子集分组为一个粗化栅元，其概率等于该子集中概率的总和，那么粗化熵等于分组后分布的熵加上组内熵的期望值。当被分组的栅元具有极小的概率时，它们的组内贡献可以忽略不计。对于 $p_i = 0$ 的栅元，其贡献通过极限 $\\lim_{x \\to 0^+} x \\log x = 0$ 定义，因此为保持一致性，设 $p_i \\log p_i = 0$。\n\n在反应堆源收敛监测中，空区或近零通量区域会导致许多栅元的 $p_i \\approx 0$。在比较网格或当许多栅元为空时，这可能会使归一化解释产生偏差。为了量化和减轻影响：\n- 我们在完整分布上计算 $H(\\mathbf{p})$。\n- 我们计算归一化熵以评估栅元数量敏感性：$H(\\mathbf{p}) / \\log M$ 和 $H(\\mathbf{p}) / \\log M_{\\text{active}}$，其中 $M_{\\text{active}} = \\#\\{ i \\mid p_i  0 \\}$。如果 $M = 1$ 或 $M_{\\text{active}} = 1$，分母为零；在这种情况下，我们定义相应的归一化值为 $0$，以维持一个明确定义的边界行为。\n- 使用阈值 $\\varepsilon  0$ 的排除策略：移除所有 $p_i \\le \\varepsilon$ 的栅元，并重新归一化剩余的概率，使其总和为 $1$。这会得到 $\\mathbf{p}^{(\\text{excl})}$。由于被移除的栅元概率很小或为零，它们各自的 $p_i \\log p_i$ 贡献要么恰好为 $0$，要么量级非常小。因此，相对于 $H(\\mathbf{p})$，$H(\\mathbf{p}^{(\\text{excl})})$ 通常会略微减小，反映出排除后分布更加尖锐。\n- 使用相同阈值 $\\varepsilon$ 的邻接感知聚合策略：从左到右扫描索引，找到满足在序列中所有 $k$ 都有 $p_k \\le \\varepsilon$ 的最大连续序列 $\\{ i, i+1, \\dots, j \\}$。将每个这样的序列替换为一个概率为 $\\sum_{k=i}^j p_k$ 的聚合栅元，并保留所有 $p_\\ell  \\varepsilon$ 的栅元作为独立栅元。如果聚合概率等于 $0$，则省略该聚合栅元。将得到的粗化分布表示为 $\\mathbf{q}$，其具有 $M_{\\text{agg}}$ 个栅元。根据分组属性，\n$$\nH(\\mathbf{p}) = H(\\mathbf{q}) + \\sum_{\\text{runs}} \\left( \\sum_{k \\in \\text{run}} p_k \\right) H\\!\\left( \\left\\{ \\frac{p_k}{\\sum_{u \\in \\text{run}} p_u} : k \\in \\text{run} \\right\\} \\right) ,\n$$\n当序列由近零或零概率组成时，第二项可以忽略，因为前置因子 $\\sum_{k \\in \\text{run}} p_k$ 和内部熵都很小。因此，$H(\\mathbf{q})$ 通常接近但不会超过 $H(\\mathbf{p})$，并且 $M_{\\text{agg}}$ 相对于 $M$ 会减小。\n\n算法设计：\n1. 归一化统计值以获得 $\\mathbf{p}$。检查 $\\sum_j f_j  0$ 以避免退化。\n2. 使用 $p_i  0$ 的掩码根据定义计算 $H(\\mathbf{p})$，以实现 $0 \\log 0 = 0$ 的约定。\n3. 计算 $M_{\\text{active}} = \\#\\{ i \\mid p_i  0 \\}$，然后使用分母为 $0$ 的边界约定计算 $H(\\mathbf{p}) / \\log M$ 和 $H(\\mathbf{p}) / \\log M_{\\text{active}}$。\n4. 对于排除策略，构建一个 $p_i  \\varepsilon$ 的掩码，通过重新归一化所选概率来形成 $\\mathbf{p}^{(\\text{excl})}$，并计算 $H(\\mathbf{p}^{(\\text{excl})})$。\n5. 对于聚合策略，线性扫描 $\\mathbf{p}$。对于 $p_i \\le \\varepsilon$，启动/运行一个累加器来对连续的近零概率求和。在序列结束时，如果和为正，则附加该和；对于 $p_i  \\varepsilon$，直接附加 $p_i$。结果 $\\mathbf{q}$ 是自动归一化的，因为它是通过对 $\\mathbf{p}$ 的条目进行分区和求和形成的。计算 $H(\\mathbf{q})$ 和 $M_{\\text{agg}} = \\text{length}(\\mathbf{q})$。\n6. 将上述方法应用于四个指定用例，每个用例产生一个包含六个输出的列表：\n   - $H(\\mathbf{p})$,\n   - $H(\\mathbf{p}) / \\log M$,\n   - $H(\\mathbf{p}) / \\log M_{\\text{active}}$,\n   - $H(\\mathbf{p}^{(\\text{excl})})$,\n   - $H(\\mathbf{q})$,\n   - $M_{\\text{agg}}$。\n\n科学真实性：统计值代表了包括空区和近空区的合理非均匀源分布。阈值 $\\varepsilon$ 的物理动机是蒙特卡洛统计中的数值灵敏度和统计噪声基底。所有计算都使用自然对数，产生 nats，这是适用于信息论收敛诊断的无量纲度量。\n\n该程序实现了上述方法，并以列表的列表形式单行打印四个用例的结果，以确保可复现性和可测试性。测试套件涵盖了边缘情况（例如，精确零、近零栅元、单个主导栅元），以展示排除和聚合如何影响信息度量和栅元数量敏感性。",
            "answer": "```python\nimport numpy as np\n\ndef normalize_tallies(tallies: np.ndarray) -> np.ndarray:\n    \"\"\"Normalize nonnegative tallies to a probability distribution.\"\"\"\n    total = np.sum(tallies)\n    if total == 0.0:\n        raise ValueError(\"Sum of tallies must be positive to form a probability distribution.\")\n    return tallies / total\n\ndef shannon_entropy(p: np.ndarray) -> float:\n    \"\"\"Compute Shannon entropy in nats for discrete distribution p, with 0*log(0)=0.\"\"\"\n    mask = p > 0.0\n    if not np.any(mask):\n        return 0.0\n    return float(-np.sum(p[mask] * np.log(p[mask])))\n\ndef normalized_entropy(H: float, M: int) -> float:\n    \"\"\"Compute H / ln(M) with boundary convention: if M == 1, return 0.0.\"\"\"\n    if M == 1:\n        return 0.0\n    return float(H / np.log(M))\n\ndef exclude_by_threshold(p: np.ndarray, eps: float) -> np.ndarray:\n    \"\"\"Exclude bins with p_i = eps and renormalize the remaining probabilities.\"\"\"\n    mask = p > eps\n    p_kept = p[mask]\n    if p_kept.size == 0:\n        # Degenerate case: everything excluded; return an empty array\n        return np.array([], dtype=float)\n    return p_kept / np.sum(p_kept)\n\ndef aggregate_contiguous_near_zero(p: np.ndarray, eps: float) -> np.ndarray:\n    \"\"\"\n    Aggregate maximal contiguous runs of bins with p_i = eps into single bins with summed probability.\n    Omit aggregated bins if summed probability equals 0. Retain bins with p_i > eps as-is.\n    \"\"\"\n    aggregated = []\n    i = 0\n    n = len(p)\n    while i  n:\n        if p[i] = eps:\n            # Start a run of near-zero bins\n            run_sum = 0.0\n            while i  n and p[i] = eps:\n                run_sum += p[i]\n                i += 1\n            if run_sum > 0.0:\n                aggregated.append(run_sum)\n            # If run_sum == 0, omit this aggregated bin\n        else:\n            aggregated.append(p[i])\n            i += 1\n    if len(aggregated) == 0:\n        # If all bins were zero-probability and eps >= 0, return empty.\n        return np.array([], dtype=float)\n    # The aggregated distribution should already sum to 1.0 because we partition p.\n    # Minor floating-point deviations can be tolerated.\n    return np.array(aggregated, dtype=float)\n\ndef process_case(tallies: np.ndarray, eps: float):\n    \"\"\"Process one test case and return the requested six outputs.\"\"\"\n    p = normalize_tallies(tallies)\n    H = shannon_entropy(p)\n    M_total = int(len(tallies))\n    M_active = int(np.count_nonzero(p > 0.0))\n    H_norm_total = normalized_entropy(H, M_total)\n    H_norm_active = normalized_entropy(H, M_active)\n\n    p_excl = exclude_by_threshold(p, eps)\n    H_excl = shannon_entropy(p_excl) if p_excl.size > 0 else 0.0\n\n    p_agg = aggregate_contiguous_near_zero(p, eps)\n    H_agg = shannon_entropy(p_agg) if p_agg.size > 0 else 0.0\n    M_agg = int(len(p_agg))\n\n    return [H, H_norm_total, H_norm_active, H_excl, H_agg, M_agg]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (np.array([1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0], dtype=float), 0.0),\n        # Case 2\n        (np.array([1e-14, 0.0, 1e-14, 0.0, 5.0, 1e-14, 0.0, 1e-14, 0.0, 5.0], dtype=float), 1e-12),\n        # Case 3\n        (np.array([1e-15, 1e-15, 1e-15, 1e-15, 1e-15, 1e-15, 1e-15, 1e-15, 1e-15, 1e-15, 1e-15, 1.0], dtype=float), 1e-12),\n        # Case 4\n        (np.array([0.0, 0.0, 0.0, 0.0, 0.0,\n                   1.0, 1.0, 1.0, 1.0, 1.0,\n                   1.0, 1.0, 1.0, 1.0, 1.0,\n                   0.0, 0.0, 0.0, 0.0, 0.0], dtype=float), 0.0),\n    ]\n\n    results = []\n    for tallies, eps in test_cases:\n        result = process_case(tallies, eps)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Ensure only the single line is printed.\n    # Convert nested lists to string with default Python formatting to meet the bracketed list requirement.\n    print(f\"[{','.join([str(r) for r in results])}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在前两个实践的基础上，这个高级练习将展示香农熵作为一个强大诊断工具的威力，用以揭示模拟中微妙的物理现象。我们将设计一个计算实验，展示如何通过优化空间网格的划分，从统计噪声中有效地区分出真实的物理信号（如裂变源的模态竞争）。这项练习突出了在信号检测问题中，空间分辨率和统计方差之间至关重要的权衡关系。",
            "id": "4248434",
            "problem": "考虑一个一维空间域 $[0,1]$，它代表一个具有非均匀增殖的板式反应堆。该反应堆产生的裂变源密度根据中子输运算符的主导本征模而更偏向于某个边界。设 $f_0(x)$ 和 $f_1(x)$ 为裂变源密度的两个非负、归一化的原型形状，由下式给出：\n$$\nf_0(x) = c \\, e^{\\beta x}, \\quad f_1(x) = c \\, e^{\\beta (1 - x)}, \\quad c = \\frac{\\beta}{e^{\\beta} - 1},\n$$\n其中 $x \\in [0,1]$ 且形状参数 $\\beta  0$ 是一个固定值。在蒙特卡洛 $k$-本征值迭代的第 $n$ 个循环（$n \\in \\{1,2,\\dots,N\\}$），连续裂变源密度被建模为归一化的混合形式：\n$$\nS_n(x) = \\alpha_n f_0(x) + (1 - \\alpha_n) f_1(x),\n$$\n其混合系数随时间变化：\n$$\n\\alpha_n = \\alpha^\\star + A \\, r^n \\cos(\\pi n),\n$$\n其中 $0  r  1$ 是优势比（即迭代算符的第二大本征值的模除以最大本征值），$A$ 是一个小振幅，$\\alpha^\\star$ 是渐近混合系数。在第 $n$ 个循环，分箱裂变源估计的香农熵 (SE) 定义为：\n$$\nH_n = -\\sum_{b=1}^{B} p_{n,b} \\log p_{n,b},\n$$\n其中 $B$ 是空间箱的数量，$p_{n,b}$ 是在第 $n$ 个循环中箱 $b$ 内的概率质量，对数为自然对数。$p_{n,b}$ 的蒙特卡洛估计量是根据每个循环从 $S_n(x)$ 中抽取的 $M$ 个独立样本 $x$ 和一组划分 $[0,1]$ 的选定箱边界构建的。\n\n从上述核心定义出发，构建一个完整的、可执行的程序，对于一组指定的分箱策略和抽样参数，证明以下推理任务：\n\n1. 通过计算时间序列 $\\{H_n\\}_{n=1}^N$，从算法上证明粗糙分箱会隐藏模式竞争特征。这里的“模式竞争特征”指的是由 $\\alpha_n$ 中的周期因子 $\\cos(\\pi n)$ 引起的香农熵中可检测到的双循环振荡。\n\n2. 表明在给定固定样本量 $M$ 的情况下，过细的分箱会使 $\\{H_n\\}$ 的估计量方差膨胀，以至于模式竞争特征变得无法检测。\n\n3. 设计一组自适应分箱，既能揭示特征又不会产生过大的方差。使用以下原则：对于每个箱最多为 $\\varepsilon$ 的目标相对标准差，要求每个箱的期望计数满足 $M \\int_{\\text{bin}} p_{\\mathrm{wr}}(x) \\, dx \\ge 1/\\varepsilon^2$，其中 $p_{\\mathrm{wr}}(x)$ 是一个最坏情况包络，定义为：\n$$\np_{\\mathrm{wr}}(x) = \\min\\left\\{ \\alpha_{\\min} f_0(x) + (1 - \\alpha_{\\min}) f_1(x), \\; \\alpha_{\\max} f_0(x) + (1 - \\alpha_{\\max}) f_1(x) \\right\\}, \n$$\n其中 $\\alpha_{\\min} = \\alpha^\\star - A$ 和 $\\alpha_{\\max} = \\alpha^\\star + A$。通过将 $[0,1]$ 划分成连续的区间来构建箱边界，这些区间的 $p_{\\mathrm{wr}}$-质量约等于 $1/(\\varepsilon^2 M)$，使用数值积分来近似计算积分。\n\n使用以下纯粹基于香农熵时间序列的双循环探测统计量：\n$$\nS_{\\mathrm{det}} = \\left| \\frac{1}{N} \\sum_{n=1}^{N} (-1)^n H_n \\right|,\n$$\n如果满足以下条件，则声明检测到特征：\n$$\nS_{\\mathrm{det}}  \\tau, \\quad \\text{with} \\quad \\tau = k \\frac{\\sigma_H}{\\sqrt{N}},\n$$\n其中 $\\sigma_H$ 是 $\\{H_n\\}_{n=1}^N$ 的样本标准差，$k$ 是一个固定常数。此标准将 $\\{H_n\\}$ 中测得的双循环振幅与其随机变异性的估计值进行比较。\n\n您的程序必须为每个测试用例实现以下步骤：\n- 使用给定的 $(\\alpha^\\star, A, r)$ 生成 $\\{ \\alpha_n \\}_{n=1}^N$。\n- 对于每个 $n$，通过对 $f_0$ 和 $f_1$ 的混合体进行逆变换采样，精确地从 $S_n(x)$ 中抽取 $M$ 个位置样本。\n- 根据指定箱边界的直方图计算 $H_n$。\n- 计算 $S_{\\mathrm{det}}$、$\\sigma_H$，并应用检测规则返回一个布尔值，指示是否检测到模式竞争特征。\n\n测试套件：\n使用固定参数 $\\beta = 3.0$，$\\alpha^\\star = 0.65$，$A = 0.25$，$r = 0.95$，$N = 60$，以及 $k = 2.0$。为保证可复现性，请在伪随机数生成器中使用固定的随机种子。\n\n评估以下五个测试用例：\n- 用例 1（粗糙均匀）：均匀分箱，$B = 2$，$M = 4000$。\n- 用例 2（粗糙均匀）：均匀分箱，$B = 4$，$M = 4000$。\n- 用例 3（精细均匀，噪声大）：均匀分箱，$B = 100$，$M = 2000$。\n- 用例 4（自适应，方差控制）：自适应分箱，$\\varepsilon = 0.1$，$M = 2000$，基于 $p_{\\mathrm{wr}}(x)$ 构建。\n- 用例 5（中等均匀，充分采样）：均匀分箱，$B = 40$，$M = 20000$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3,result4,result5]\"），其中每个 $result_i$ 是一个布尔值，指示在用例 $i$ 中是否检测到特征。不涉及物理单位，并且根据构造（通过 $\\cos(\\pi n)$），所有角度均以弧度为单位。在给定固定种子的情况下，输出必须是确定性的。",
            "solution": "该问题被验证为合理且定义明确。先验分析证实了其科学和数学上的完整性。我现在将提供完整的解决方案。\n\n该问题要求通过计算来演示不同的统计估计策略如何影响使用香农熵检测物理特征（核反应堆裂变源中的模式竞争）的能力。这需要基于一个简化的、一维、双模的裂变源收敛模型来实现蒙特卡洛模拟。解决方案涉及从概率分布中抽样，应用不同的数据分箱技术，以及对结果时间序列进行统计分析。\n\n模型的核心在于第 $n$ 个循环中随时间变化的裂变源密度 $S_n(x)$：\n$$S_n(x) = \\alpha_n f_0(x) + (1 - \\alpha_n) f_1(x)$$\n其中 $f_0(x) = c \\, e^{\\beta x}$ 和 $f_1(x) = c \\, e^{\\beta (1 - x)}$ 是在 $x \\in [0,1]$ 上的两个非对称、归一化的分布，其中 $c = \\beta/(e^{\\beta} - 1)$。混合系数 $\\alpha_n = \\alpha^\\star + A \\, r^n \\cos(\\pi n) = \\alpha^\\star + A \\, r^n (-1)^n$，导致源的形状在偶数循环中由 $f_0(x)$ 主导，在奇数循环中由 $f_1(x)$（或更接近它的形状）主导，振荡幅度以因子 $r^n$ 衰减。这种振荡就是我们旨在检测的“模式竞争特征”。\n\n香农熵 $H_n = -\\sum_{b=1}^{B} p_{n,b} \\log p_{n,b}$ 作为源空间分布的标量度量。这里，$p_{n,b}$ 是第 $n$ 个循环中空间箱 $b$ 内的概率质量。随着 $S_n(x)$ 的振荡，我们期望 $H_n$ 也会振荡。源的空间集中度的变化（例如，从在 $x=1$ 处达到峰值变为在 $x=0$ 处达到峰值）通常会导致其分箱表示的熵发生变化。在 $H_n$ 中检测此特征被表述为一个信噪比问题。信号是熵序列 $\\{H_n\\}_{n=1}^N$ 中双循环振荡的幅度，由统计量 $S_{\\mathrm{det}} = \\left| \\frac{1}{N} \\sum_{n=1}^{N} (-1)^n H_n \\right|$ 衡量。噪声由熵估计的统计不确定性量化，通过阈值 $\\tau = k \\frac{\\sigma_H}{\\sqrt{N}}$ 捕捉，其中 $\\sigma_H$ 是熵序列的样本标准差。如果 $S_{\\mathrm{det}}  \\tau$，则声明“检测到”特征。\n\n模拟过程为每个循环 $n$ 从 $S_n(x)$ 生成 $M$ 个独立随机样本。这是通过从混合分布中抽样来实现的，这是一个两步过程：首先，以概率 $\\alpha_n$ 和 $1-\\alpha_n$ 分别随机选择 $f_0$ 或 $f_1$；其次，使用逆变换采样从所选分布中抽取一个样本。$f_0(x)$ 和 $f_1(x)$ 的累积分布函数 (CDF) 分别是：\n$$F_0(x) = \\int_0^x c e^{\\beta t} dt = \\frac{e^{\\beta x} - 1}{e^{\\beta} - 1}$$\n$$F_1(x) = \\int_0^x c e^{\\beta (1-t)} dt = \\frac{e^\\beta - e^{\\beta(1-x)}}{e^\\beta - 1}$$\n通过对这些函数求逆，我们得到将均匀随机变量 $u \\in [0,1]$ 转换为样本 $x$ 的公式：\n$$F_0^{-1}(u) = \\frac{1}{\\beta} \\log\\left(u(e^\\beta - 1) + 1\\right)$$\n$$F_1^{-1}(u) = 1 - \\frac{1}{\\beta} \\log\\left(e^\\beta - u(e^\\beta - 1)\\right)$$\n\n分箱策略的选择至关重要，因为它调节了分辨 $S_n(x)$ 的空间特征与控制统计噪声之间的权衡。\n1. **粗糙分箱**：使用非常少的箱数 $B$ 无法捕捉 $S_n(x)$ 的变化形状。例如，当 $B=2$ 且箱为 $[0, 0.5]$ 和 $[0.5, 1]$ 时，随着 $\\alpha_n$ 的振荡，每个箱中的概率质量可能不会显著变化，特别是如果分布 $f_0$ 和 $f_1$ 的不对称性不强。因此，$H_n$ 保持近似恒定，特征被遗漏 ($S_{\\mathrm{det}} \\approx 0$)。\n2. **精细分箱**：使用大量的箱数 $B$ 会创建高分辨率的直方图。然而，对于固定的样本量 $M$，这会导致每个箱的样本非常少。箱概率的统计估计量 $\\hat{p}_{n,b} = N_{n,b}/M$（其中 $N_{n,b}$ 是箱 $b$ 中的计数）的方差大约为 $\\text{Var}(\\hat{p}_{n,b}) \\approx p_{n,b}/M$。对于小计数，相对不确定性很大。这种对每个 $p_{n,b}$ 的高统计噪声会传播到 $H_n$ 的计算中，创建一个噪声大的时间序列，其中小的确定性振荡被完全淹没（$\\sigma_H$ 变大，使得 $\\tau$ 变大，检测可能性降低）。\n3. **自适应分箱**：该策略旨在平衡分辨率和噪声。其原则是创建具有大致相等期望粒子数的箱，确保没有箱“缺少”样本。条件是对于选定的最坏情况概率密度 $p_{\\mathrm{wr}}(x)$，每个箱 $b$ 中的期望计数应满足 $M \\int_b p_{\\mathrm{wr}}(x) dx \\ge 1/\\varepsilon^2$，其中 $\\varepsilon$ 是任何箱中计数的目标最大相对标准差。问题将 $p_{\\mathrm{wr}}(x)$ 定义为对应于 $\\alpha_{\\min}=\\alpha^\\star - A$ 和 $\\alpha_{\\max}=\\alpha^\\star + A$ 的两个极端源形状的逐点最小值构成的“包络”。通过数值求解包含目标质量 $1/(\\varepsilon^2 M)$ 的区间来找到箱边界，根据 $p_{\\mathrm{wr}}(x')$ 计算。这需要数值积分（`scipy.integrate.quad`）来计算质量积分，以及数值求根器（`scipy.optimize.brentq`）来定位箱边界。这种方法确保了在整个域内每个箱都有足够的样本数，稳定了熵估计，并允许检测到底层的物理振荡。\n\n程序将为五个不同的测试用例执行这些步骤，每个用例代表分箱策略和样本大小的不同组合，以算法方式演示这些效应，并为每个用例报告一个布尔检测结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\n\n# -- 1. PROBLEM PARAMETERS AND MODEL DEFINITIONS --\n\n# Fixed physical and simulation parameters from the problem statement\nBETA = 3.0\nALPHA_STAR = 0.65\nA = 0.25\nR = 0.95\nN_CYCLES = 60\nK_FACTOR = 2.0\nSEED = 12345\n\n# Normalization constant for f0 and f1\nC = BETA / (np.exp(BETA) - 1.0)\n\ndef f0(x, beta, c):\n    \"\"\"Prototype fission source density f_0(x).\"\"\"\n    return c * np.exp(beta * x)\n\ndef f1(x, beta, c):\n    \"\"\"Prototype fission source density f_1(x).\"\"\"\n    return c * np.exp(beta * (1.0 - x))\n\ndef invF0(u, beta):\n    \"\"\"Inverse CDF of f_0(x) for sampling.\"\"\"\n    return (1.0 / beta) * np.log(u * (np.exp(beta) - 1.0) + 1.0)\n\ndef invF1(u, beta):\n    \"\"\"Inverse CDF of f_1(x) for sampling.\"\"\"\n    return 1.0 - (1.0 / beta) * np.log(np.exp(beta) - u * (np.exp(beta) - 1.0))\n\ndef sample_Sn(alpha, M, beta, rng):\n    \"\"\"Samples M positions from the mixture distribution S_n(x) using inverse transform sampling.\"\"\"\n    component_choice = rng.uniform(0.0, 1.0, size=M)\n    u_for_inversion = rng.uniform(0.0, 1.0, size=M)\n    \n    samples = np.zeros(M)\n    \n    f0_mask = component_choice  alpha\n    num_f0 = np.sum(f0_mask)\n    if num_f0 > 0:\n        samples[f0_mask] = invF0(u_for_inversion[f0_mask], beta)\n        \n    f1_mask = ~f0_mask\n    num_f1 = M - num_f0\n    if num_f1 > 0:\n        samples[f1_mask] = invF1(u_for_inversion[f1_mask], beta)\n        \n    return samples\n\n# -- 2. ADAPTIVE BINNING ALGORITHM --\n\ndef get_adaptive_bins(M, epsilon, beta, c, alpha_star, A):\n    \"\"\"\n    Constructs adaptive bin edges based on the worst-case envelope p_wr(x).\n    \"\"\"\n    alpha_min = alpha_star - A\n    alpha_max = alpha_star + A\n    \n    p_wr_args = (beta, c, alpha_min, alpha_max)\n\n    def p_wr(x, beta_val, c_val, alpha_min_val, alpha_max_val):\n        \"\"\"The worst-case envelope PDF.\"\"\"\n        s_min = alpha_min_val * f0(x, beta_val, c_val) + (1.0 - alpha_min_val) * f1(x, beta_val, c_val)\n        s_max = alpha_max_val * f0(x, beta_val, c_val) + (1.0 - alpha_max_val) * f1(x, beta_val, c_val)\n        return np.minimum(s_min, s_max)\n\n    target_mass = 1.0 / (epsilon**2 * M)\n    bin_edges = [0.0]\n    \n    while bin_edges[-1]  1.0:\n        current_edge = bin_edges[-1]\n        \n        remaining_mass, _ = quad(p_wr, current_edge, 1.0, args=p_wr_args)\n        \n        # If remaining mass is less than or slightly above the target, just add the final edge and stop.\n        if remaining_mass = target_mass * 1.05:\n            break\n            \n        def objective(x):\n            \"\"\"Function to find root of: integral from current_edge to x equals target_mass.\"\"\"\n            mass, _ = quad(p_wr, current_edge, x, args=p_wr_args)\n            return mass - target_mass\n\n        try:\n            # Bracket for root finding is (current_edge, 1.0)\n            next_edge = brentq(objective, current_edge, 1.0)\n            bin_edges.append(next_edge)\n        except ValueError:\n            # Could happen if signs are not different, e.g., due to precision.\n            break\n\n    if bin_edges[-1]  1.0:\n        bin_edges.append(1.0)\n        \n    return np.array(bin_edges)\n\n# -- 3. SIMULATION AND ANALYSIS --\n\ndef run_one_case(case_config):\n    \"\"\"\n    Runs the full simulation for one test case and returns the detection result.\n    \"\"\"\n    M, bin_type, bin_param, epsilon = case_config\n    \n    rng = np.random.default_rng(SEED)\n\n    if bin_type == 'uniform':\n        B = bin_param\n        bin_edges = np.linspace(0.0, 1.0, B + 1)\n    elif bin_type == 'adaptive':\n        bin_edges = get_adaptive_bins(M, epsilon, BETA, C, ALPHA_STAR, A)\n    else:\n        raise ValueError(\"Unknown binning type\")\n\n    n_vals = np.arange(1, N_CYCLES + 1)\n    alpha_n_series = ALPHA_STAR + A * (R**n_vals) * np.cos(np.pi * n_vals)\n    \n    H_series = np.zeros(N_CYCLES)\n    for i in range(N_CYCLES):\n        alpha_n = alpha_n_series[i]\n        \n        samples = sample_Sn(alpha_n, M, BETA, rng)\n        \n        counts, _ = np.histogram(samples, bins=bin_edges)\n        \n        # We must use the actual number of generated particles for normalization\n        total_samples = np.sum(counts) \n        if total_samples > 0:\n            probs = counts / total_samples\n            # Filter out zero probabilities for log calculation\n            nz_probs = probs[probs > 0]\n            H_n = -np.sum(nz_probs * np.log(nz_probs))\n            H_series[i] = H_n\n\n    # Calculate detection statistic and threshold\n    S_det = np.abs(np.mean((-1.0)**n_vals * H_series))\n    sigma_H = np.std(H_series, ddof=1)\n    \n    # Avoid division by zero if all H_n are identical\n    if sigma_H == 0:\n        return False\n        \n    tau = K_FACTOR * sigma_H / np.sqrt(N_CYCLES)\n    \n    return S_det > tau\n\n# -- 4. MAIN EXECUTION --\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the simulations, and prints the final results.\n    \"\"\"\n    test_cases = [\n        # ( M,      bin_type,  bin_param, epsilon)\n        (4000,   'uniform', 2,         None),  # Case 1: Coarse uniform\n        (4000,   'uniform', 4,         None),  # Case 2: Coarse uniform\n        (2000,   'uniform', 100,       None),  # Case 3: Fine uniform, noisy\n        (2000,   'adaptive',None,      0.1),   # Case 4: Adaptive, variance controlled\n        (20000,  'uniform', 40,        None),  # Case 5: Moderate uniform, well-sampled\n    ]\n\n    results = []\n    for case in test_cases:\n        is_detected = run_one_case(case)\n        results.append(is_detected)\n\n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}