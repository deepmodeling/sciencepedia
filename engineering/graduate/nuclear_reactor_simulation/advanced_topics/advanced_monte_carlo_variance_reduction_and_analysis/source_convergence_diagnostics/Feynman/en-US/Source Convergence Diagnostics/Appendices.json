{
    "hands_on_practices": [
        {
            "introduction": "To diagnose source convergence, we must first understand the characteristics of a truly converged source. Once stationary, the fission source distribution has a stable, fundamental-mode shape. This exercise provides hands-on practice in characterizing such a distribution using higher-order statistical moments, such as skewness and kurtosis, which are sensitive indicators of the source shape's asymmetry and tails . By analytically calculating these properties for a given source model, you will build a foundational understanding of the quantitative metrics used in advanced diagnostic tools.",
            "id": "4250579",
            "problem": "In neutron transport Monte Carlo (MC) eigenvalue simulations for nuclear reactors, the fission source distribution is iteratively updated and should converge to the fundamental-mode stationary source. One way to diagnose source convergence is to monitor standardized shape descriptors of the spatial source distribution. Consider a one-dimensional slab model with spatial coordinate $x \\in [0,1]$, where the late-iteration fission source shape is modeled as a normalized probability density $S(x)$ on $[0,1]$. Due to material heterogeneity and boundary leakage, assume the source density can be represented by a Beta-family shape:\n$$\nS(x) = C\\, x^{a-1}(1-x)^{b-1}, \\quad x \\in [0,1],\n$$\nwith parameters $a = 4$ and $b = 2$, and normalization constant $C$ determined by $\\int_{0}^{1} S(x)\\,\\mathrm{d}x = 1$. Let $X$ be a random variable with probability density function $S(x)$. Define the mean $\\mu = \\mathbb{E}[X]$, the variance $\\sigma^{2} = \\mathbb{E}[(X-\\mu)^{2}]$, the third central moment $\\mu_{3} = \\mathbb{E}[(X-\\mu)^{3}]$, and the fourth central moment $\\mu_{4} = \\mathbb{E}[(X-\\mu)^{4}]$. The skewness is $\\gamma_{1} = \\mu_{3}/\\sigma^{3}$, and the excess kurtosis is $\\gamma_{2} = \\mu_{4}/\\sigma^{4} - 3$.\n\nA combined normality-based diagnostic used in source convergence assessment is the Jarque–Bera (JB) statistic, defined for a sample size $n$ as\n$$\nJ = \\frac{n}{6}\\,\\gamma_{1}^{2} + \\frac{n}{24}\\,\\gamma_{2}^{2}.\n$$\nAssume the source bank contains $n = 2.0 \\times 10^{5}$ fission sites sampled from $S(x)$. Using only fundamental definitions of moments and normalization, compute the value of $J$ for the given $a$ and $b$. Round your final result to four significant figures. Express your answer as a dimensionless number with no units.",
            "solution": "The problem requires the computation of the Jarque-Bera (JB) statistic for a given probability density modeling a fission source distribution. The problem is well-posed and scientifically grounded in statistical methods used in nuclear engineering simulations.\n\nThe fission source density is given by $S(x) = C\\, x^{a-1}(1-x)^{b-1}$ for $x \\in [0,1]$, with parameters $a=4$ and $b=2$. This is the probability density function (PDF) of a Beta distribution, $X \\sim \\text{Beta}(a,b)$.\n\nFirst, we determine the normalization constant $C$ from the condition $\\int_{0}^{1} S(x)\\,\\mathrm{d}x = 1$. The integral is the Beta function, $B(a,b)$.\n$$\nB(a,b) = \\int_{0}^{1} x^{a-1}(1-x)^{b-1}\\,\\mathrm{d}x\n$$\nFor $a=4$ and $b=2$:\n$$\nB(4,2) = \\int_{0}^{1} x^{4-1}(1-x)^{2-1}\\,\\mathrm{d}x = \\int_{0}^{1} x^3(1-x)\\,\\mathrm{d}x = \\int_{0}^{1} (x^3 - x^4)\\,\\mathrm{d}x\n$$\n$$\nB(4,2) = \\left[ \\frac{x^4}{4} - \\frac{x^5}{5} \\right]_{0}^{1} = \\frac{1}{4} - \\frac{1}{5} = \\frac{5-4}{20} = \\frac{1}{20}\n$$\nThe normalization constant is $C = 1/B(4,2) = 20$. Thus, the PDF is $S(x) = 20x^3(1-x)$ for $x \\in [0,1]$.\n\nNext, we compute the first four raw moments, $\\mathbb{E}[X^k] = \\int_{0}^{1} x^k S(x)\\,\\mathrm{d}x$.\n\nThe mean, $\\mu = \\mathbb{E}[X]$:\n$$\n\\mu = \\mathbb{E}[X] = \\int_{0}^{1} x (20x^3(1-x))\\,\\mathrm{d}x = 20 \\int_{0}^{1} (x^4 - x^5)\\,\\mathrm{d}x = 20 \\left[ \\frac{x^5}{5} - \\frac{x^6}{6} \\right]_{0}^{1} = 20 \\left( \\frac{1}{5} - \\frac{1}{6} \\right) = 20 \\left( \\frac{1}{30} \\right) = \\frac{2}{3}\n$$\nThe second raw moment, $\\mathbb{E}[X^2]$:\n$$\n\\mathbb{E}[X^2] = \\int_{0}^{1} x^2 (20x^3(1-x))\\,\\mathrm{d}x = 20 \\int_{0}^{1} (x^5 - x^6)\\,\\mathrm{d}x = 20 \\left[ \\frac{x^6}{6} - \\frac{x^7}{7} \\right]_{0}^{1} = 20 \\left( \\frac{1}{6} - \\frac{1}{7} \\right) = 20 \\left( \\frac{1}{42} \\right) = \\frac{10}{21}\n$$\nThe third raw moment, $\\mathbb{E}[X^3]$:\n$$\n\\mathbb{E}[X^3] = \\int_{0}^{1} x^3 (20x^3(1-x))\\,\\mathrm{d}x = 20 \\int_{0}^{1} (x^6 - x^7)\\,\\mathrm{d}x = 20 \\left[ \\frac{x^7}{7} - \\frac{x^8}{8} \\right]_{0}^{1} = 20 \\left( \\frac{1}{7} - \\frac{1}{8} \\right) = 20 \\left( \\frac{1}{56} \\right) = \\frac{5}{14}\n$$\nThe fourth raw moment, $\\mathbb{E}[X^4]$:\n$$\n\\mathbb{E}[X^4] = \\int_{0}^{1} x^4 (20x^3(1-x))\\,\\mathrm{d}x = 20 \\int_{0}^{1} (x^7 - x^8)\\,\\mathrm{d}x = 20 \\left[ \\frac{x^8}{8} - \\frac{x^9}{9} \\right]_{0}^{1} = 20 \\left( \\frac{1}{8} - \\frac{1}{9} \\right) = 20 \\left( \\frac{1}{72} \\right) = \\frac{5}{18}\n$$\nNow, we compute the central moments.\nThe variance, $\\sigma^2 = \\mu_2 = \\mathbb{E}[(X-\\mu)^2] = \\mathbb{E}[X^2] - \\mu^2$:\n$$\n\\sigma^2 = \\frac{10}{21} - \\left(\\frac{2}{3}\\right)^2 = \\frac{10}{21} - \\frac{4}{9} = \\frac{30 - 28}{63} = \\frac{2}{63}\n$$\nThe third central moment, $\\mu_3 = \\mathbb{E}[(X-\\mu)^3] = \\mathbb{E}[X^3] - 3\\mu\\mathbb{E}[X^2] + 2\\mu^3$:\n$$\n\\mu_3 = \\frac{5}{14} - 3\\left(\\frac{2}{3}\\right)\\left(\\frac{10}{21}\\right) + 2\\left(\\frac{2}{3}\\right)^3 = \\frac{5}{14} - \\frac{20}{21} + \\frac{16}{27}\n$$\nThe least common denominator of $14$, $21$, and $27$ is $378$.\n$$\n\\mu_3 = \\frac{5 \\cdot 27}{378} - \\frac{20 \\cdot 18}{378} + \\frac{16 \\cdot 14}{378} = \\frac{135 - 360 + 224}{378} = \\frac{-1}{378}\n$$\nThe fourth central moment, $\\mu_4 = \\mathbb{E}[(X-\\mu)^4] = \\mathbb{E}[X^4] - 4\\mu\\mathbb{E}[X^3] + 6\\mu^2\\mathbb{E}[X^2] - 3\\mu^4$:\n$$\n\\mu_4 = \\frac{5}{18} - 4\\left(\\frac{2}{3}\\right)\\left(\\frac{5}{14}\\right) + 6\\left(\\frac{2}{3}\\right)^2\\left(\\frac{10}{21}\\right) - 3\\left(\\frac{2}{3}\\right)^4 = \\frac{5}{18} - \\frac{20}{21} + \\frac{80}{63} - \\frac{16}{27}\n$$\nThe least common denominator of $18$, $21$, $63$, and $27$ is $378$.\n$$\n\\mu_4 = \\frac{5 \\cdot 21}{378} - \\frac{20 \\cdot 18}{378} + \\frac{80 \\cdot 6}{378} - \\frac{16 \\cdot 14}{378} = \\frac{105 - 360 + 480 - 224}{378} = \\frac{1}{378}\n$$\nNext, we calculate the skewness $\\gamma_1$ and excess kurtosis $\\gamma_2$.\nThe skewness is $\\gamma_1 = \\mu_3 / \\sigma^3$.\n$$\n\\gamma_1^2 = \\frac{\\mu_3^2}{\\sigma^6} = \\frac{(-1/378)^2}{(2/63)^3} = \\frac{1/378^2}{8/63^3} = \\frac{63^3}{8 \\cdot 378^2} = \\frac{63^3}{8 \\cdot (6 \\cdot 63)^2} = \\frac{63^3}{8 \\cdot 36 \\cdot 63^2} = \\frac{63}{288} = \\frac{7}{32}\n$$\nThe excess kurtosis is $\\gamma_2 = \\mu_4 / \\sigma^4 - 3$.\n$$\n\\frac{\\mu_4}{\\sigma^4} = \\frac{1/378}{(2/63)^2} = \\frac{1/378}{4/3969} = \\frac{3969}{4 \\cdot 378} = \\frac{3969}{1512}\n$$\nSince $3969 = 63^2$ and $1512 = 24 \\cdot 63$, the ratio is $63/24 = 21/8$.\n$$\n\\gamma_2 = \\frac{21}{8} - 3 = \\frac{21 - 24}{8} = -\\frac{3}{8}\n$$\nSo, $\\gamma_2^2 = (-3/8)^2 = 9/64$.\n\nFinally, we compute the Jarque-Bera statistic $J$ with $n = 2.0 \\times 10^5$.\n$$\nJ = \\frac{n}{6}\\gamma_1^2 + \\frac{n}{24}\\gamma_2^2 = \\frac{2.0 \\times 10^5}{6}\\left(\\frac{7}{32}\\right) + \\frac{2.0 \\times 10^5}{24}\\left(\\frac{9}{64}\\right)\n$$\n$$\nJ = (2.0 \\times 10^5) \\left[ \\frac{7}{192} + \\frac{9}{1536} \\right]\n$$\nThe common denominator is $1536 = 8 \\cdot 192$.\n$$\nJ = (2.0 \\times 10^5) \\left[ \\frac{7 \\cdot 8}{1536} + \\frac{9}{1536} \\right] = (2.0 \\times 10^5) \\left[ \\frac{56 + 9}{1536} \\right] = (2.0 \\times 10^5) \\left( \\frac{65}{1536} \\right)\n$$\n$$\nJ = \\frac{200000 \\times 65}{1536} = \\frac{13000000}{1536} = \\frac{203125}{24} \\approx 8463.541666...\n$$\nThe problem requires rounding the result to four significant figures. The fifth significant figure is $5$, so we round up the fourth digit.\n$$\nJ \\approx 8464\n$$",
            "answer": "$$\\boxed{8464}$$"
        },
        {
            "introduction": "The speed at which a simulation converges to the fundamental source mode is crucial for efficiency and is governed by the dominance ratio—the ratio of the subdominant to the dominant eigenvalue of the fission operator. This exercise employs a simplified two-region matrix model to provide a clear, first-principles look at how this convergence rate is determined and, critically, how it can be degraded by systematic biases from misapplied variance reduction techniques . Working through this model will build your intuition for why some simulations converge slowly and how operator biases manifest mathematically.",
            "id": "4250530",
            "problem": "Consider a $2$-region model of a critical slab reactor used in Monte Carlo Neutron Transport (MCNT) $k$-eigenvalue calculations for Source Convergence Diagnostics (SCD). Let the fission-source distribution over regions $A$ and $B$ be represented by a column vector $s_n \\in \\mathbb{R}^2_{\\ge 0}$ at iteration (generation) $n$, and let the mean linear fission source mapping in the absence of any bias be represented by a nonnegative matrix $\\mathbf{M}_0 \\in \\mathbb{R}^{2 \\times 2}$,\n$$\n\\mathbf{M}_0 = \\begin{pmatrix}\n0.6 & 0.2 \\\\\n0.4 & 0.8\n\\end{pmatrix}.\n$$\nAssume that the system is critical under unbiased sampling and population management, and that the source normalization to unit total weight is applied between generations.\n\nNow suppose a mis-specified importance function is used in conjunction with Weight Window (WW) population control, which together with Russian Roulette (RR) and splitting introduces a systematic bias in the expected contribution to the fission source in each region: contributions landing in region $A$ are scaled by a factor $b_A = 1.10$, while those landing in region $B$ are scaled by a factor $b_B = 0.95$. Model this as a diagonal scaling $\\mathbf{B} = \\mathrm{diag}(b_A,b_B)$ applied to the mapping, giving a biased mean operator\n$$\n\\mathbf{M}_b = \\mathbf{B}\\mathbf{M}_0.\n$$\nAssume the initial source $s_0$ is decomposed into the dominant right-eigenvector $v_1$ of $\\mathbf{M}_b$ and the subdominant right-eigenvector $v_2$ of $\\mathbf{M}_b$ as $s_0 = c_1 v_1 + \\varepsilon v_2$, where the SCD-relevant subdominant-mode amplitude is $\\varepsilon = 0.3$. The SCD threshold requires that the subdominant-mode amplitude after $n$ generations be at most $\\delta = 1.0 \\times 10^{-5}$.\n\nUsing first principles of linear operator iteration for nonnegative matrices and the stated modeling assumptions, determine the minimal integer number of source iterations $n_{\\min}$ under the biased operator $\\mathbf{M}_b$ such that the subdominant-mode amplitude falls below the threshold $\\delta$. Provide $n_{\\min}$ as a single integer. No rounding to a specified number of significant figures is required; report the exact minimal integer.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions:\n- The unbiased fission source mapping matrix:\n$$\n\\mathbf{M}_0 = \\begin{pmatrix}\n0.6 & 0.2 \\\\\n0.4 & 0.8\n\\end{pmatrix}\n$$\n- The system is critical under unbiased sampling, implying the dominant eigenvalue of $\\mathbf{M}_0$ is $1$.\n- A bias is introduced by a diagonal scaling matrix $\\mathbf{B} = \\mathrm{diag}(b_A, b_B)$, where the scaling factors are $b_A = 1.10$ and $b_B = 0.95$.\n- The biased mean operator is given by $\\mathbf{M}_b = \\mathbf{B}\\mathbf{M}_0$.\n- The initial source distribution $s_0$ is decomposed into eigenvectors of $\\mathbf{M}_b$ as $s_0 = c_1 v_1 + \\varepsilon v_2$, where $v_1$ is the dominant eigenvector and $v_2$ is the subdominant eigenvector.\n- The initial amplitude of the subdominant mode is $\\varepsilon = 0.3$.\n- The convergence threshold for the subdominant-mode amplitude is $\\delta = 1.0 \\times 10^{-5}$.\n- The goal is to find the minimal integer number of source iterations, $n_{\\min}$, for the subdominant-mode amplitude to fall below $\\delta$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n- **Scientifically Grounded:** The problem uses a well-established linear algebra model (power iteration on a fission matrix) to analyze source convergence in Monte Carlo simulations, a standard topic in nuclear reactor physics. The use of a matrix to represent neutron transport between discrete regions, the concept of a bias introduced by variance reduction techniques like Weight Windows, and the analysis of convergence rate via the dominance ratio are all fundamental and scientifically correct principles in this field. The given matrix $\\mathbf{M}_0$ is non-negative, as required for a fission operator, and its dominant eigenvalue is indeed $1$, consistent with the \"critical\" system statement.\n- **Well-Posed:** The problem is mathematically well-defined. It asks for the minimum number of iterations for a quantity to decay below a threshold, which is a standard problem in numerical analysis and has a unique solution. All necessary parameters ($\\mathbf{M}_0$, $\\mathbf{B}$, $\\varepsilon$, $\\delta$) are provided.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased language, using standard terminology from nuclear engineering and linear algebra. There are no subjective or ambiguous statements.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and objective. A solution will be formulated.\n\n## SOLUTION\n\nThe problem describes a power iteration process for finding the dominant eigenvector of the biased fission operator $\\mathbf{M}_b$. The source vector at iteration $n$, denoted $s_n$, is obtained by applying the operator $\\mathbf{M}_b$ to the previous source vector $s_{n-1}$ and normalizing the result. The initial source vector $s_0$ is given as a linear combination of the dominant eigenvector $v_1$ and the subdominant eigenvector $v_2$ of $\\mathbf{M}_b$:\n$$\ns_0 = c_1 v_1 + \\varepsilon v_2\n$$\nwhere $\\mathbf{M}_b v_1 = \\lambda_1 v_1$ and $\\mathbf{M}_b v_2 = \\lambda_2 v_2$, with $|\\lambda_1| > |\\lambda_2|$. The matrix $\\mathbf{M}_b$ will be a positive matrix, so by the Perron-Frobenius theorem, $\\lambda_1$ is real, positive, and strictly greater in magnitude than any other eigenvalue.\n\nAfter one iteration, before normalization, the source vector becomes:\n$$\ns'_1 = \\mathbf{M}_b s_0 = \\mathbf{M}_b (c_1 v_1 + \\varepsilon v_2) = c_1 \\lambda_1 v_1 + \\varepsilon \\lambda_2 v_2\n$$\nAfter $n$ iterations, the unnormalized source vector is:\n$$\ns'_n = \\mathbf{M}_b^n s_0 = c_1 \\lambda_1^n v_1 + \\varepsilon \\lambda_2^n v_2\n$$\nThe source vector $s_n$ is normalized at each step, but the relative amplitude of the eigenvectors is unaffected by this scalar normalization. We can analyze the decay of the subdominant mode by factoring out the dominant eigenvalue term:\n$$\ns'_n = \\lambda_1^n \\left( c_1 v_1 + \\varepsilon \\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^n v_2 \\right)\n$$\nThe amplitude of the subdominant mode after $n$ iterations, denoted $\\varepsilon_n$, is the initial amplitude multiplied by the $n$-th power of the dominance ratio, $\\rho = |\\lambda_2 / \\lambda_1|$.\n$$\n\\varepsilon_n = \\varepsilon \\rho^n = \\varepsilon \\left| \\frac{\\lambda_2}{\\lambda_1} \\right|^n\n$$\nWe need to find the minimal integer $n$ such that $\\varepsilon_n \\le \\delta$. This leads to the inequality:\n$$\n\\varepsilon \\left| \\frac{\\lambda_2}{\\lambda_1} \\right|^n \\le \\delta\n$$\nFirst, we compute the biased operator $\\mathbf{M}_b$:\n$$\n\\mathbf{M}_b = \\mathbf{B}\\mathbf{M}_0 = \\begin{pmatrix} 1.10 & 0 \\\\ 0 & 0.95 \\end{pmatrix} \\begin{pmatrix} 0.6 & 0.2 \\\\ 0.4 & 0.8 \\end{pmatrix} = \\begin{pmatrix} (1.10)(0.6) & (1.10)(0.2) \\\\ (0.95)(0.4) & (0.95)(0.8) \\end{pmatrix} = \\begin{pmatrix} 0.66 & 0.22 \\\\ 0.38 & 0.76 \\end{pmatrix}\n$$\nNext, we find the eigenvalues of $\\mathbf{M}_b$ by solving the characteristic equation $\\det(\\mathbf{M}_b - \\lambda\\mathbf{I}) = 0$.\n$$\n\\det \\begin{pmatrix} 0.66 - \\lambda & 0.22 \\\\ 0.38 & 0.76 - \\lambda \\end{pmatrix} = 0\n$$\n$$\n(0.66 - \\lambda)(0.76 - \\lambda) - (0.22)(0.38) = 0\n$$\n$$\n\\lambda^2 - (0.66 + 0.76)\\lambda + (0.66)(0.76) - (0.0836) = 0\n$$\n$$\n\\lambda^2 - 1.42\\lambda + 0.5016 - 0.0836 = 0\n$$\n$$\n\\lambda^2 - 1.42\\lambda + 0.418 = 0\n$$\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{1.42 \\pm \\sqrt{(-1.42)^2 - 4(1)(0.418)}}{2} = \\frac{1.42 \\pm \\sqrt{2.0164 - 1.672}}{2} = \\frac{1.42 \\pm \\sqrt{0.3444}}{2}\n$$\nThe dominant eigenvalue is $\\lambda_1 = \\frac{1.42 + \\sqrt{0.3444}}{2}$ and the subdominant eigenvalue is $\\lambda_2 = \\frac{1.42 - \\sqrt{0.3444}}{2}$. Both eigenvalues are positive, so the absolute value signs for the dominance ratio can be dropped.\n$$\n\\rho = \\frac{\\lambda_2}{\\lambda_1} = \\frac{\\frac{1.42 - \\sqrt{0.3444}}{2}}{\\frac{1.42 + \\sqrt{0.3444}}{2}} = \\frac{1.42 - \\sqrt{0.3444}}{1.42 + \\sqrt{0.3444}}\n$$\nNow we solve the inequality for $n$:\n$$\n\\varepsilon \\rho^n \\le \\delta\n$$\n$$\n\\rho^n \\le \\frac{\\delta}{\\varepsilon}\n$$\nTaking the natural logarithm of both sides:\n$$\nn \\ln(\\rho) \\le \\ln\\left(\\frac{\\delta}{\\varepsilon}\\right)\n$$\nSince $\\rho < 1$, $\\ln(\\rho)$ is negative. Dividing by $\\ln(\\rho)$ reverses the inequality sign:\n$$\nn \\ge \\frac{\\ln(\\delta/\\varepsilon)}{\\ln(\\rho)}\n$$\nWe substitute the given values $\\varepsilon = 0.3$ and $\\delta = 1.0 \\times 10^{-5}$.\n$$\n\\frac{\\delta}{\\varepsilon} = \\frac{1.0 \\times 10^{-5}}{0.3} = \\frac{1}{30000}\n$$\nNow, we compute the numerical values for the logarithms:\n$$\n\\ln\\left(\\frac{\\delta}{\\varepsilon}\\right) = \\ln\\left(\\frac{1}{30000}\\right) = -\\ln(30000) \\approx -10.30895\n$$\n$$\n\\rho = \\frac{1.42 - \\sqrt{0.3444}}{1.42 + \\sqrt{0.3444}} \\approx \\frac{1.42 - 0.586856}{1.42 + 0.586856} \\approx \\frac{0.833144}{2.006856} \\approx 0.415143\n$$\nNote: A slightly more precise calculation of $\\rho$ is $0.4151213$. Let's use that for accuracy.\n$$\n\\ln(\\rho) \\approx \\ln(0.4151213) \\approx -0.87913\n$$\nSubstituting these into the inequality for $n$:\n$$\nn \\ge \\frac{-10.30895}{-0.87913} \\approx 11.7263\n$$\nSince the number of iterations $n$ must be an integer, we take the ceiling of this value.\n$$\nn_{\\min} = \\lceil 11.7263 \\rceil = 12\n$$\nTherefore, a minimum of $12$ source iterations are required for the subdominant-mode amplitude to fall below the specified threshold.",
            "answer": "$$\\boxed{12}$$"
        },
        {
            "introduction": "Building on the theoretical understanding of convergence rates, this practice transitions to the implementation of a practical diagnostic tool. You will write a program to estimate the dominance ratio not from an analytical model, but directly from the history of source vectors generated during a power iteration . This exercise provides a valuable hands-on coding experience, demonstrating how the abstract concept of the dominance ratio can be measured and used to create an automated check for the near-degeneracy that causes slow source convergence.",
            "id": "4250531",
            "problem": "Consider a discrete-source iteration used in nuclear reactor simulation to approximate the fundamental fission source shape. Let a non-negative matrix $A \\in \\mathbb{R}^{n \\times n}$ represent a linearized transport-fission operator acting on a discretized source distribution $s \\in \\mathbb{R}^n$. Assume that $A$ is non-negative and that the initial source vector $s_0$ is strictly positive. In the idealized case of an irreducible and aperiodic $A$, the source iteration converges (up to normalization) to a unique dominant eigenvector associated with the largest eigenvalue by the Perron-Frobenius theorem. In practice, degeneracy or near-degeneracy of the leading eigenstructure manifests as slow convergence or non-unique asymptotic behavior, which must be diagnosed reliably.\n\nYour task is to implement a diagnostic that infers degeneracy or near-degeneracy of the leading eigenstructure of $A$ using only the source iteration history and norm-based comparisons. The diagnostic must be based on the following steps:\n\n- Initialization: Use an initial source $s_0$ with components proportional to the index, i.e., $s_{0,i} \\propto i+1$ for $i = 0,1,\\dots,n-1$, and normalize so that the components sum to $1$ (i.e., use the $\\ell_1$ normalization).\n- Iteration: For $k = 0,1,\\dots,K-1$, compute $s_{k+1} = A s_k$, then normalize $s_{k+1}$ to unit $\\ell_1$ norm by dividing by its $\\ell_1$ norm. Use $K = 200$ iterations.\n- Differences: Compute the $\\ell_1$ differences $d_k = \\|s_{k+1} - s_k\\|_1$ for $k = 0,1,\\dots,K-1$.\n- Tail ratio estimate: For a tail window of size $T = 50$, compute the ratios $r_k = d_{k+1}/d_k$ over the last $T$ steps wherever $d_k$ exceeds a small threshold $\\varepsilon = 10^{-12}$. If no such ratio can be computed (for example, all $d_k$ in the tail are below $\\varepsilon$), set the effective tail ratio $R$ to $1.0$; otherwise define $R$ as the median of the computable $r_k$ values.\n- Decision rule: Declare degeneracy or near-degeneracy present if $R \\ge \\tau_r$, with threshold $\\tau_r = 0.9$.\n\nThis diagnostic should be understood as a proxy for the asymptotic convergence rate of the source iteration, which in the ideal case is governed by the ratio of the subdominant eigenvalue magnitude to the dominant eigenvalue magnitude.\n\nImplement the above diagnostic and apply it to the following test suite of matrices $A$, each representing a different scenario:\n\n1. Rank-one-perturbed identity in dimension $n = 5$ with parameters $\\alpha = 0.1$, $\\beta = 0.9$:\n   $$A^{(1)} = \\alpha I + \\frac{\\beta}{n} J,$$\n   where $I$ is the $n \\times n$ identity and $J$ is the $n \\times n$ all-ones matrix. Here $n = 5$, $\\alpha = 0.1$, $\\beta = 0.9$.\n2. Near-degenerate rank-one-perturbed identity in dimension $n = 5$ with parameters $\\alpha = 0.95$, $\\beta = 0.05$:\n   $$A^{(2)} = \\alpha I + \\frac{\\beta}{n} J,$$\n   with $n = 5$, $\\alpha = 0.95$, $\\beta = 0.05$.\n3. Exactly degenerate identity in dimension $n = 5$ with parameters $\\alpha = 1.0$, $\\beta = 0.0$:\n   $$A^{(3)} = \\alpha I + \\frac{\\beta}{n} J = I,$$\n   with $n = 5$, $\\alpha = 1.0$, $\\beta = 0.0$.\n4. Borderline near-degenerate rank-one-perturbed identity in dimension $n = 5$ with parameters $\\alpha = 0.9$, $\\beta = 0.1$:\n   $$A^{(4)} = \\alpha I + \\frac{\\beta}{n} J,$$\n   with $n = 5$, $\\alpha = 0.9$, $\\beta = 0.1$.\n5. A $2 \\times 2$ permutation (swap) matrix capturing periodicity:\n   $$A^{(5)} = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}.$$\n\nYour program must implement the diagnostic exactly as described, apply it to the matrices $A^{(1)}, A^{(2)}, A^{(3)}, A^{(4)}, A^{(5)}$, and produce as final output a single line containing a comma-separated list enclosed in square brackets with the boolean results for the five cases in order, for example, $[\\text{True},\\text{False},\\dots]$. No additional text should be printed.\n\nThere are no physical units required for this problem. Angles do not appear. Percentages should not be used; thresholds and ratios are pure decimals.\n\nThe matrices in the test suite are designed to exercise the diagnostic across a \"happy path\" case (well-separated dominant eigenvalue), near-degenerate case, exactly degenerate case, borderline case at the decision threshold, and a periodic case. The final answer must be a complete, runnable program that implements the diagnostic and outputs the decision booleans in the specified format on a single line.",
            "solution": "The problem statement is assessed to be valid. It presents a well-defined numerical algorithm grounded in the established principles of linear algebra, specifically the power iteration method and its convergence properties. The task is to implement a specific diagnostic heuristic, with all parameters, procedures, and test cases defined unambiguously. The problem is scientifically sound, self-contained, and objective, allowing for a unique and verifiable solution.\n\nThe core of this problem is the implementation of a numerical diagnostic to detect slow convergence in the power iteration method, which is a common algorithm used in nuclear reactor physics to find the fundamental mode of the neutron fission source. The iteration is given by\n$$ s_{k+1} = \\frac{A s_k}{\\|A s_k\\|_1} $$\nwhere $s_k \\in \\mathbb{R}^n$ is the source vector at iteration $k$, $A \\in \\mathbb{R}^{n \\times n}$ is a non-negative matrix representing the discretized neutron transport and fission processes, and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm.\n\nAccording to the Perron-Frobenius theorem for non-negative, irreducible, and aperiodic matrices, this iteration converges to a unique, strictly positive eigenvector $v_1$ associated with the dominant (largest in magnitude) eigenvalue $\\lambda_1$. The rate of convergence is governed by the dominance ratio, $\\rho = |\\lambda_2|/|\\lambda_1|$, where $\\lambda_2$ is the subdominant eigenvalue (the one with the second-largest magnitude). The distance between the iterate $s_k$ and the true eigenvector $v_1$ diminishes at a rate proportional to $\\rho^k$.\n\nWhen $\\rho$ is close to $1$, the convergence becomes very slow. This condition, where $|\\lambda_1| \\approx |\\lambda_2|$, is known as near-degeneracy of the leading eigenstructure. The diagnostic algorithm described in the problem is a practical method to estimate this dominance ratio $\\rho$ directly from the sequence of iterates $\\{s_k\\}$.\n\nThe change in the source vector at each iteration is measured by the difference $d_k = \\|s_{k+1} - s_k\\|_1$. For large $k$, it can be shown that the sequence of these differences converges geometrically with a rate determined by $\\rho$. Consequently, the ratio of successive differences,\n$$ r_k = \\frac{d_{k+1}}{d_k} = \\frac{\\|s_{k+2} - s_{k+1}\\|_1}{\\|s_{k+1} - s_k\\|_1} $$\nprovides an estimate of the dominance ratio $\\rho$. A value of $r_k$ close to $1$ indicates slow convergence and suggests near-degeneracy.\n\nThe specified algorithm formalizes this concept:\n\n1.  **Initialization**: An initial vector $s_0$ is constructed with components $s_{0,i}$ proportional to $i+1$. Normalizing to unit $\\ell_1$ norm gives:\n    $$ s_0 = \\frac{(1, 2, \\dots, n)^T}{\\sum_{j=1}^n j} = \\frac{2}{n(n+1)} (1, 2, \\dots, n)^T $$\n\n2.  **Iteration**: For $k=0, 1, \\dots, K-1$ with the number of iterations $K=200$, the next source vector is computed and normalized:\n    $$s_{k+1} = \\frac{A s_k}{\\|A s_k\\|_1}$$\n\n3.  **Differences**: The sequence of $\\ell_1$ differences $d_k = \\|s_{k+1} - s_k\\|_1$ is computed for all iterations $k = 0, \\dots, K-1$.\n\n4.  **Tail Ratio Estimate**: To get a stable estimate of the asymptotic ratio, we examine the tail of the sequence of ratios. We consider a window of the last $T=50$ potential ratios, which are $r_k = d_{k+1}/d_k$ for $k \\in [K-T-1, K-2]$. The set of computable ratios is formed as $\\mathcal{R} = \\{ r_k \\mid d_k > \\varepsilon \\}$, where $\\varepsilon=10^{-12}$ is a small threshold to avoid division by zero or numerically unstable values. The effective tail ratio $R$ is defined as the median of the values in $\\mathcal{R}$. If $\\mathcal{R}$ is empty (i.e., convergence was so fast that all differences in the tail are below the threshold, or the vector did not change), $R$ is set to $1.0$.\n\n5.  **Decision Rule**: A decision on near-degeneracy is made by comparing the estimated ratio $R$ to a threshold $\\tau_r = 0.9$. If $R \\ge \\tau_r$, the system is flagged as having a (near-)degenerate dominant eigenstructure.\n\nThis procedure will be applied to each of the five provided test matrices. For the matrix family $A = \\alpha I + \\frac{\\beta}{n} J$, the eigenvalues are $\\lambda_1 = \\alpha + \\beta$ and $\\lambda_2 = \\dots = \\lambda_n = \\alpha$. The true dominance ratio is $\\rho = \\frac{\\alpha}{\\alpha+\\beta}$, which allows us to anticipate the diagnostic's behavior. For instance, in Case 1 ($\\alpha=0.1, \\beta=0.9$), $\\rho=0.1$, indicating rapid convergence. In Case 2 ($\\alpha=0.95, \\beta=0.05$), $\\rho=0.95$, indicating near-degeneracy. The other cases test exact degeneracy, a borderline condition, and periodicity.",
            "answer": "```python\nimport numpy as np\n\ndef run_diagnostic(A, n):\n    \"\"\"\n    Implements the source iteration convergence diagnostic for a given matrix.\n\n    Args:\n        A (np.ndarray): The n x n matrix for the power iteration.\n        n (int): The dimension of the matrix.\n\n    Returns:\n        bool: True if near-degeneracy is detected, False otherwise.\n    \"\"\"\n    # Parameters from the problem statement\n    K = 200\n    T = 50\n    epsilon = 1e-12\n    tau_r = 0.9\n\n    # Step 1: Initialization\n    # Initial source s_0 with components proportional to index i+1, l1-normalized.\n    s0_unnormalized = np.arange(1, n + 1, dtype=np.float64)\n    s0_norm = np.linalg.norm(s0_unnormalized, ord=1)\n    s = s0_unnormalized / s0_norm\n\n    # Store history of source vectors\n    s_history = np.zeros((K + 1, n))\n    s_history[0] = s\n\n    # Step 2: Iteration\n    # For k = 0, ..., K-1, compute s_{k+1}\n    for k in range(K):\n        s_prev = s_history[k]\n        s_next_unnorm = A @ s_prev\n        \n        norm_val = np.linalg.norm(s_next_unnorm, ord=1)\n        # Handle the case where the norm is zero to avoid division by zero.\n        if norm_val == 0:\n            s_next = np.zeros_like(s_next_unnorm)\n        else:\n            s_next = s_next_unnorm / norm_val\n        \n        s_history[k + 1] = s_next\n\n    # Step 3: Differences\n    # Compute d_k = ||s_{k+1} - s_k||_1 for k = 0, ..., K-1\n    d = np.zeros(K)\n    for k in range(K):\n        d[k] = np.linalg.norm(s_history[k + 1] - s_history[k], ord=1)\n    \n    # Step 4: Tail ratio estimate\n    ratios = []\n    # Loop over the indices corresponding to the last T potential ratios.\n    # These are r_k for k from K-T-1 to K-2.\n    start_k = K - T - 1\n    end_k = K - 1 \n    for k in range(start_k, end_k):\n        # Check if the denominator is above the threshold\n        if d[k] > epsilon:\n            ratios.append(d[k + 1] / d[k])\n    \n    if not ratios:\n        R = 1.0\n    else:\n        R = np.median(ratios)\n\n    # Step 5: Decision rule\n    return R >= tau_r\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the diagnostic on each.\n    \"\"\"\n    \n    # --- Define Test Cases ---\n    \n    # Case 1: n=5, alpha=0.1, beta=0.9\n    n1 = 5\n    alpha1 = 0.1\n    beta1 = 0.9\n    J1 = np.ones((n1, n1))\n    A1 = alpha1 * np.identity(n1) + (beta1 / n1) * J1\n    \n    # Case 2: n=5, alpha=0.95, beta=0.05\n    n2 = 5\n    alpha2 = 0.95\n    beta2 = 0.05\n    J2 = np.ones((n2, n2))\n    A2 = alpha2 * np.identity(n2) + (beta2 / n2) * J2\n    \n    # Case 3: n=5, alpha=1.0, beta=0.0\n    n3 = 5\n    alpha3 = 1.0\n    beta3 = 0.0\n    J3 = np.ones((n3, n3))\n    A3 = alpha3 * np.identity(n3) + (beta3 / n3) * J3 # This is just Identity\n    \n    # Case 4: n=5, alpha=0.9, beta=0.1\n    n4 = 5\n    alpha4 = 0.9\n    beta4 = 0.1\n    J4 = np.ones((n4, n4))\n    A4 = alpha4 * np.identity(n4) + (beta4 / n4) * J4\n    \n    # Case 5: 2x2 permutation matrix\n    n5 = 2\n    A5 = np.array([[0, 1], [1, 0]], dtype=np.float64)\n\n    test_cases = [\n        (A1, n1),\n        (A2, n2),\n        (A3, n3),\n        (A4, n4),\n        (A5, n5)\n    ]\n\n    results = []\n    for (A, n) in test_cases:\n        result = run_diagnostic(A, n)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}