{
    "hands_on_practices": [
        {
            "introduction": "To effectively apply weight roulette and splitting, we first need to build a solid theoretical foundation for why population control is essential. This exercise frames a Monte Carlo simulation as a mathematical branching process to analyze its stability and computational cost. By exploring this simplified model, you will gain a fundamental understanding of how Russian roulette prevents uncontrolled particle population growth and ensures that the simulation remains computationally tractable .",
            "id": "4260701",
            "problem": "In a Monte Carlo neutron transport simulation, consider the evolution of particle histories as a branching process. Model the number of tracked particles as a Galton–Watson process, where each processed particle produces a random number of progeny with mean $m$ and finite variance. Assume that the computational cost is proportional to the total number of particles tracked, with each tracked particle contributing unit cost. Splitting is not used.\n\nTo control population and compute time, weight-based Russian roulette is applied to every particle before it branches: with probability $p \\in (0,1]$ a particle is retained and its statistical weight is multiplied by $1/p$ to preserve unbiasedness of tallies, and with probability $1-p$ the particle is terminated and does not branch. Assume independence between roulette decisions and progeny generation.\n\nStarting only from fundamental properties of expectations and the branching structure, and assuming subcriticality conditions necessary for finiteness whenever you compute an expectation, do the following:\n\n1. Derive an expression for the expected total number of tracked particles (i.e., expected computational cost) without roulette as a function of $m$.\n2. Derive an expression for the expected total number of tracked particles when roulette with parameter $p$ is applied as described above, as a function of $p$ and $m$.\n3. Define the relative expected computational cost reduction due to roulette as\n$$\\Delta(p,m) \\equiv 1 - \\frac{\\mathbb{E}[T_{\\text{with roulette}}]}{\\mathbb{E}[T_{\\text{without roulette}}]}.$$\nCompute a closed-form expression for $\\Delta(p,m)$ in terms of $p$ and $m$ under conditions where both expectations are finite.\n4. Briefly discuss, in terms of the mean progeny, the stability of the thinned process when $pm1$.\n\nProvide your final answer as the simplified closed-form expression for $\\Delta(p,m)$. No numerical evaluation is required, and no units are needed. The discussion in task 4 should support your derivation but will not be graded in the final numeric expression.",
            "solution": "The problem asks for the derivation of the relative computational cost reduction when using Russian roulette in a Monte Carlo particle simulation. The simulation is modeled as a Galton–Watson branching process. We will solve the problem by following the four specified tasks. We assume the process starts with a single initial particle.\n\nLet $Z_n$ be the number of particles in generation $n$, with the process initiating from a single particle, so $Z_0 = 1$. The total number of tracked particles, $T$, is the sum of particles over all generations: $T = \\sum_{n=0}^{\\infty} Z_n$. The computational cost is proportional to $T$, and by setting the cost per particle to $1$, the expected cost is $\\mathbb{E}[T]$.\n\n1. Expected total tracked particles without roulette.\nIn a standard Galton–Watson process, the expected number of particles in generation $n+1$, conditioned on the number in generation $n$, is given by $\\mathbb{E}[Z_{n+1} | Z_n] = m Z_n$, where $m$ is the mean number of progeny per particle.\n\nUsing the law of total expectation (tower property), the unconditional expectation is:\n$$ \\mathbb{E}[Z_{n+1}] = \\mathbb{E}[\\mathbb{E}[Z_{n+1} | Z_n]] = \\mathbb{E}[m Z_n] = m \\mathbb{E}[Z_n] $$\nThis is a recursive relationship for the expected number of particles in each generation. Starting with $\\mathbb{E}[Z_0] = \\mathbb{E}[1] = 1$, we can solve this recurrence by induction:\n$$ \\mathbb{E}[Z_n] = m^n \\mathbb{E}[Z_0] = m^n $$\nThe expected total number of tracked particles, $\\mathbb{E}[T_{\\text{without roulette}}]$, is the sum of the expected number of particles in each generation. By linearity of expectation:\n$$ \\mathbb{E}[T_{\\text{without roulette}}] = \\mathbb{E}\\left[\\sum_{n=0}^{\\infty} Z_n\\right] = \\sum_{n=0}^{\\infty} \\mathbb{E}[Z_n] = \\sum_{n=0}^{\\infty} m^n $$\nThis is a geometric series. The problem states to assume subcriticality conditions for finiteness, which for this series requires $|m|  1$. Since $m$ is a mean number of progeny, $m \\ge 0$, so the condition is $0 \\le m  1$. Under this condition, the series converges to:\n$$ \\mathbb{E}[T_{\\text{without roulette}}] = \\frac{1}{1-m} $$\n\n2. Expected total tracked particles with roulette.\nWhen Russian roulette is applied, each particle at the beginning of a generation is subjected to a survival game before it can branch. Let $Z'_n$ be the number of particles at the start of generation $n$ in this modified process. We again start with $Z'_0 = 1$.\nEach of the $Z'_n$ particles has a probability $p$ of surviving to produce offspring and a probability $1-p$ of being terminated. Let $S_n$ be the number of particles that survive roulette in generation $n$. Given $Z'_n$, $S_n$ follows a binomial distribution, $S_n \\sim \\text{Bin}(Z'_n, p)$, so its conditional expectation is $\\mathbb{E}[S_n | Z'_n] = p Z'_n$.\n\nThe $S_n$ surviving particles then branch, each producing a random number of progeny with mean $m$. The number of particles in the next generation, $Z'_{n+1}$, is the sum of all progeny from the $S_n$ survivors. The conditional expectation of $Z'_{n+1}$ given $S_n$ is $\\mathbb{E}[Z'_{n+1} | S_n] = m S_n$.\n\nWe can now find the relationship between $\\mathbb{E}[Z'_{n+1}]$ and $\\mathbb{E}[Z'_n]$ using the tower property:\n$$ \\mathbb{E}[Z'_{n+1} | Z'_n] = \\mathbb{E}[\\mathbb{E}[Z'_{n+1} | S_n, Z'_n] | Z'_n] = \\mathbb{E}[m S_n | Z'_n] = m \\mathbb{E}[S_n | Z'_n] = m(pZ'_n) = (pm)Z'_n $$\nTaking the expectation over $Z'_n$:\n$$ \\mathbb{E}[Z'_{n+1}] = \\mathbb{E}[\\mathbb{E}[Z'_{n+1} | Z'_n]] = \\mathbb{E}[(pm)Z'_n] = pm \\mathbb{E}[Z'_n] $$\nThus, the sequence of expectations $\\{\\mathbb{E}[Z'_n]\\}$ follows the same recursive form as the case without roulette, but with an effective progeny mean of $m' = pm$. With $\\mathbb{E}[Z'_0] = 1$, the solution is $\\mathbb{E}[Z'_n] = (pm)^n$.\n\nThe total number of tracked particles is $T_{\\text{with roulette}} = \\sum_{n=0}^{\\infty} Z'_n$. Its expectation is:\n$$ \\mathbb{E}[T_{\\text{with roulette}}] = \\sum_{n=0}^{\\infty} \\mathbb{E}[Z'_n] = \\sum_{n=0}^{\\infty} (pm)^n $$\nFor this sum to be finite, we require $|pm|  1$. Since $p \\in (0, 1]$ and $m \\ge 0$, this condition is $pm  1$. Under this subcriticality condition for the thinned process, the expected total cost is:\n$$ \\mathbb{E}[T_{\\text{with roulette}}] = \\frac{1}{1-pm} $$\n\n3. Relative expected computational cost reduction.\nThe relative cost reduction $\\Delta(p,m)$ is defined as:\n$$ \\Delta(p,m) \\equiv 1 - \\frac{\\mathbb{E}[T_{\\text{with roulette}}]}{\\mathbb{E}[T_{\\text{without roulette}}]} $$\nBoth expectations must be finite, which requires $m  1$ and $pm  1$. Since $p \\in (0, 1]$, the condition $m  1$ is sufficient to guarantee both. Substituting the derived expressions:\n$$ \\Delta(p,m) = 1 - \\frac{\\frac{1}{1-pm}}{\\frac{1}{1-m}} = 1 - \\frac{1-m}{1-pm} $$\nTo find a single closed-form expression, we combine the terms over a common denominator:\n$$ \\Delta(p,m) = \\frac{(1-pm) - (1-m)}{1-pm} = \\frac{1-pm-1+m}{1-pm} = \\frac{m-pm}{1-pm} $$\nFactoring out $m$ from the numerator gives the final simplified expression:\n$$ \\Delta(p,m) = \\frac{m(1-p)}{1-pm} $$\n\n4. Discussion of stability.\nThe process with roulette, $\\{Z'_n\\}$, is a Galton–Watson process with an effective progeny mean of $m' = pm$. A fundamental result in the theory of branching processes is that if the mean number of offspring per individual is strictly less than $1$, the process is \"subcritical\" and becomes extinct with probability $1$. Extinction means that for some generation $N$, $Z'_N = 0$, and thus all subsequent generations are also empty ($Z'_{n}=0$ for all $n \\ge N$).\n\nThe condition $pm  1$ ensures this subcriticality. When this condition holds:\na) The expected population size in generation $n$, $\\mathbb{E}[Z'_n] = (pm)^n$, decays to $0$ as $n \\to \\infty$.\nb) The expected total number of particles tracked throughout the simulation, $\\mathbb{E}[T_{\\text{with roulette}}] = \\frac{1}{1-pm}$, is finite.\n\nThis finiteness of the expected total cost is a manifestation of the stability of the simulation. A supercritical process ($pm  1$) would have an expected population size that grows exponentially, leading to an infinite expected computational cost, making the simulation computationally intractable. Therefore, the condition $pm  1$ is the criterion for ensuring that the population of simulated particles is controlled and the computational effort remains bounded in expectation. The statistical weights of surviving particles are adjusted by a factor of $1/p$ to maintain an unbiased estimate of physical quantities, but the number of computational histories is managed by the thinning process of roulette.",
            "answer": "$$\n\\boxed{\\frac{m(1-p)}{1-pm}}\n$$"
        },
        {
            "introduction": "Moving from theory to a practical design scenario, this problem challenges you to implement an energy-dependent splitting scheme, a common technique in realistic reactor simulations. Your goal is not just to reduce variance, but to do so intelligently by balancing the variance contributions from different particle energy groups to meet a specific target. This exercise demonstrates how the abstract principles of variance reduction are translated into concrete engineering decisions to optimize a complex Monte Carlo tally .",
            "id": "4260711",
            "problem": "A Monte Carlo (MC) neutron transport simulation estimates a point-detector response in a small spherical region embedded within a reactor core. To reduce estimator variance while maintaining unbiasedness, the simulation applies weight roulette upstream to low-weight particles and energy-dependent splitting within a thin spherical shell (the “window”) immediately surrounding the detector. The detector response is linear in particle weight and is scored as a sum of independent per-clone contributions.\n\nUpstream of the window, a particle of initial weight $w_{0}$ survives Russian roulette with probability $w_{0}/w_{c}$ and, if it survives, its weight is set to $w_{c}$, so that the expected weight is conserved. Within the window, energy is partitioned into three groups $G_{1}$, $G_{2}$, and $G_{3}$ with probabilities $p_{1}$, $p_{2}$, and $p_{3}$ that a particle entering the window falls into each group. Upon entering group $G_{g}$, the particle is split into $n_{g}$ independent clones, each of weight $w_{c}/n_{g}$. For a unit-weight particle in group $G_{g}$, the score contribution to the detector has zero mean and standard deviation $\\sigma_{g}$; for a particle of weight $w$, the score contribution is linearly scaled, and its variance scales as $w^{2}\\sigma_{g}^{2}$. Assume independence of clone scores and independence across groups.\n\nYou are tasked with designing the group-dependent splitting factors $n_{g}$ to equalize the per-history variance contributions from each group within the window, subject to a specified target total per-history variance $V_{\\text{target}}$ for the window. The upstream cutoff weight is $w_{c}=2.0$. The energy-group probabilities and unit-weight score standard deviations are\n$$\np_{1}=0.4,\\quad p_{2}=0.35,\\quad p_{3}=0.25,\\qquad \\sigma_{1}=1.2,\\quad \\sigma_{2}=0.8,\\quad \\sigma_{3}=2.0.\n$$\nLet the target total per-history variance from the window be\n$$\nV_{\\text{target}}=0.5.\n$$\nUnder the balancing criterion that each group contributes equally to the total variance, compute the expected number of splits per history in the window,\n$$\nB \\equiv \\sum_{g=1}^{3} p_{g} n_{g}.\n$$\nExpress the final value as a pure number and round your answer to four significant figures.",
            "solution": "The goal is to find the expected number of splits per history, $B$, under the constraints that the total variance from the window is fixed at $V_{\\text{target}}$ and that the variance contribution from each energy group is equal.\n\nFirst, let's derive the expression for the total per-history variance from the window, $V$. A particle entering the window has weight $w_c$. With probability $p_g$, it enters group $G_g$. In group $G_g$, it is split into $n_g$ clones, each with weight $w' = w_c / n_g$. The score from a single clone in group $G_g$ has a mean of zero and a variance of $(w')^2 \\sigma_g^2 = (w_c/n_g)^2 \\sigma_g^2$.\n\nSince the clones are independent, the variance of the total score from all $n_g$ clones, given that the particle entered group $G_g$, is the sum of their individual variances:\n$$ \\text{Var}(\\text{score} | G_g) = n_g \\times \\left( \\frac{w_c}{n_g} \\right)^2 \\sigma_g^2 = \\frac{w_c^2 \\sigma_g^2}{n_g} $$\nThe total per-history variance, $V$, is found using the law of total variance: $V = \\mathbb{E}[\\text{Var}(\\text{score}|G)] + \\text{Var}(\\mathbb{E}[\\text{score}|G])$. Since the mean score for any group is zero, $\\mathbb{E}[\\text{score}|G] = 0$, and thus its variance is also zero. The total variance is then the expectation of the conditional variances:\n$$ V = \\sum_{g=1}^{3} p_g \\text{Var}(\\text{score} | G_g) = \\sum_{g=1}^{3} p_g \\frac{w_c^2 \\sigma_g^2}{n_g} $$\nThe problem states two constraints:\n1.  The total variance equals the target variance: $V = V_{\\text{target}}$.\n2.  The per-history variance contribution from each group is equal. The contribution from group $g$ is the term $p_g \\frac{w_c^2 \\sigma_g^2}{n_g}$.\n\nLet this equal contribution be $V_{\\text{contrib}}$. Then $V = \\sum_{g=1}^3 V_{\\text{contrib}} = 3 V_{\\text{contrib}}$. Combining with the first constraint, we have $3 V_{\\text{contrib}} = V_{\\text{target}}$, which means the contribution from each group must be:\n$$ p_g \\frac{w_c^2 \\sigma_g^2}{n_g} = \\frac{V_{\\text{target}}}{3} $$\nWe can solve this for the splitting factor $n_g$:\n$$ n_g = \\frac{3 p_g w_c^2 \\sigma_g^2}{V_{\\text{target}}} $$\nThe quantity we need to compute is the expected number of splits per history in the window, $B = \\sum_{g=1}^3 p_g n_g$. Substituting our expression for $n_g$:\n$$ B = \\sum_{g=1}^{3} p_g \\left( \\frac{3 p_g w_c^2 \\sigma_g^2}{V_{\\text{target}}} \\right) = \\frac{3 w_c^2}{V_{\\text{target}}} \\sum_{g=1}^{3} p_g^2 \\sigma_g^2 $$\nNow we can plug in the given numerical values:\n- $w_c = 2.0$\n- $V_{\\text{target}} = 0.5$\n- $p_1=0.4, \\sigma_1=1.2$\n- $p_2=0.35, \\sigma_2=0.8$\n- $p_3=0.25, \\sigma_3=2.0$\n\nFirst, compute the sum:\n$$ \\sum_{g=1}^{3} p_g^2 \\sigma_g^2 = (0.4)^2(1.2)^2 + (0.35)^2(0.8)^2 + (0.25)^2(2.0)^2 $$\n$$ = (0.16)(1.44) + (0.1225)(0.64) + (0.0625)(4.0) $$\n$$ = 0.2304 + 0.0784 + 0.2500 = 0.5588 $$\nNow, compute $B$:\n$$ B = \\frac{3 \\times (2.0)^2}{0.5} \\times 0.5588 = \\frac{12}{0.5} \\times 0.5588 = 24 \\times 0.5588 = 13.4112 $$\nRounding the final answer to four significant figures gives $13.41$.",
            "answer": "$$\\boxed{13.41}$$"
        },
        {
            "introduction": "The ideal models we use in theory often require refinement when implemented in actual code. This advanced problem delves into a crucial detail of splitting: since we can only create an integer number of new particles, we must use a method like stochastic rounding to decide the split count. You will use the delta method, a powerful approximation technique, to analyze the variance introduced by this rounding process, revealing the subtle statistical effects of moving from a continuous model to a discrete, practical algorithm .",
            "id": "4260739",
            "problem": "A Monte Carlo (MC) particle transport in a nuclear reactor simulation uses weight splitting to control variance: a particle of pre-splitting weight $W$ is replaced by $N$ identical offspring, each of weight $W_{\\text{tgt}}$, where $N$ is chosen by stochastic rounding of the ratio $\\rho = W / W_{\\text{tgt}}$. Specifically, if $\\rho \\in [k, k+1)$ for some integer $k$, then with probability $f = \\rho - k$ we set $N = k+1$, and with probability $1 - f$ we set $N = k$. This “roulette-and-splitting” rule is widely used to maintain unbiasedness in MC tallies.\n\nAssume $W$ is a random variable with mean $\\mu$ and variance $\\sigma^{2}$, and that $\\sigma$ is sufficiently small compared to $W_{\\text{tgt}}$ so that the fractional part of $\\rho$ concentrates without crossing integer boundaries with high probability. Using first principles of conditional expectation and the law of total variance, together with the delta method, derive the leading-order approximation to the unconditional variance of the total post-splitting weight $T = N W_{\\text{tgt}}$.\n\nFor the concrete case with $W \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ where $\\mu = 1.7$ and $\\sigma^{2} = 5.0 \\times 10^{-3}$, and with $W_{\\text{tgt}} = 0.5$, compute the delta-method approximation to $\\operatorname{Var}(T)$. Round your final numerical answer to four significant figures and report it without units.",
            "solution": "We begin from the standard weight-splitting principle in Monte Carlo (MC) particle transport: splitting is constructed to preserve expected total weight by suitable stochastic rounding. Let $W$ denote the pre-splitting weight, $W_{\\text{tgt}}$ denote the target offspring weight, $\\rho = W / W_{\\text{tgt}}$, and $N$ denote the integer split count determined by stochastic rounding:\n- If $\\rho \\in [k, k+1)$ with integer $k$, define the fractional part $f = \\rho - k \\in [0,1)$.\n- Set $N = k+1$ with probability $f$, and $N = k$ with probability $1 - f$.\n\nDefine the total post-splitting weight $T = N W_{\\text{tgt}}$. We will establish unbiasedness and then derive a leading-order approximation to $\\operatorname{Var}(T)$ using the delta method.\n\nUnbiasedness follows from the definition of the stochastic rounding:\n\n$$\n\\mathbb{E}[N \\mid W] = (k+1) f + k (1 - f) = k + f = \\rho = \\frac{W}{W_{\\text{tgt}}}.\n$$\n\nTherefore,\n\n$$\n\\mathbb{E}[T \\mid W] = \\mathbb{E}[N W_{\\text{tgt}} \\mid W] = W_{\\text{tgt}} \\, \\mathbb{E}[N \\mid W] = W,\n$$\n\nand hence\n\n$$\n\\mathbb{E}[T] = \\mathbb{E}\\big[ \\mathbb{E}[T \\mid W] \\big] = \\mathbb{E}[W] = \\mu,\n$$\n\nestablishing unbiasedness.\n\nTo analyze variance, we invoke the law of total variance:\n\n$$\n\\operatorname{Var}(T) = \\operatorname{Var}\\big( \\mathbb{E}[T \\mid W] \\big) + \\mathbb{E}\\big[ \\operatorname{Var}(T \\mid W) \\big].\n$$\n\nFrom $\\mathbb{E}[T \\mid W] = W$, it follows that\n\n$$\n\\operatorname{Var}\\big( \\mathbb{E}[T \\mid W] \\big) = \\operatorname{Var}(W) = \\sigma^{2}.\n$$\n\nAlso, since $T \\mid W = N W_{\\text{tgt}}$ and $N \\mid W$ is a Bernoulli mixture between $k$ and $k+1$ with success probability $f$, we have\n\n$$\n\\operatorname{Var}(N \\mid W) = f (1 - f),\n$$\n\nand therefore\n\n$$\n\\operatorname{Var}(T \\mid W) = W_{\\text{tgt}}^{2} \\, \\operatorname{Var}(N \\mid W) = W_{\\text{tgt}}^{2} f(1 - f).\n$$\n\nCombining,\n\n$$\n\\operatorname{Var}(T) = \\sigma^{2} + \\mathbb{E}\\big[ W_{\\text{tgt}}^{2} f(1 - f) \\big] = \\sigma^{2} + W_{\\text{tgt}}^{2} \\, \\mathbb{E}\\big[ f(1 - f) \\big].\n$$\n\n\nWe now apply the delta method to approximate $\\mathbb{E}[ f(1 - f) ]$ when $\\sigma$ is small relative to $W_{\\text{tgt}}$. Let $\\rho = W / W_{\\text{tgt}}$, and denote by $\\phi$ the fractional part of the mean ratio $\\mu / W_{\\text{tgt}}$:\n\n$$\n\\phi = \\left\\{ \\frac{\\mu}{W_{\\text{tgt}}} \\right\\} \\in [0,1).\n$$\n\nWhen fluctuations are small enough that wrapping across integer boundaries is negligible, the fractional part $f$ can be locally parameterized as\n\n$$\nf \\approx \\phi + Z, \\quad Z = \\frac{W - \\mu}{W_{\\text{tgt}}}.\n$$\n\nUnder $W \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, we have $Z \\sim \\mathcal{N}(0, \\sigma^{2} / W_{\\text{tgt}}^{2})$. Define $g(u) = u(1 - u) = u - u^{2}$. Then\n\n$$\n\\mathbb{E}[ f(1 - f) ] \\approx \\mathbb{E}[ g(\\phi + Z) ].\n$$\n\nThe second-order delta method gives\n\n$$\n\\mathbb{E}[ g(\\phi + Z) ] \\approx g(\\phi) + \\frac{1}{2} g''(\\phi) \\operatorname{Var}(Z),\n$$\n\nbecause $\\mathbb{E}[Z] = 0$. Since $g'(u) = 1 - 2u$ and $g''(u) = -2$, we obtain\n\n$$\n\\mathbb{E}[ f(1 - f) ] \\approx \\phi(1 - \\phi) - \\operatorname{Var}(Z) = \\phi(1 - \\phi) - \\frac{\\sigma^{2}}{W_{\\text{tgt}}^{2}}.\n$$\n\nSubstituting back into the law of total variance,\n\n$$\n\\operatorname{Var}(T) \\approx \\sigma^{2} + W_{\\text{tgt}}^{2} \\left( \\phi(1 - \\phi) - \\frac{\\sigma^{2}}{W_{\\text{tgt}}^{2}} \\right) = W_{\\text{tgt}}^{2} \\, \\phi(1 - \\phi).\n$$\n\nThus, to leading order under the no-wrap assumption, the unconditional variance of the total post-splitting weight depends on $W_{\\text{tgt}}$ and the fractional part of $\\mu / W_{\\text{tgt}}$, and is independent of $\\sigma$ at this order.\n\nWe now evaluate this for the given numerical parameters. With $\\mu = 1.7$ and $W_{\\text{tgt}} = 0.5$,\n\n$$\n\\frac{\\mu}{W_{\\text{tgt}}} = \\frac{1.7}{0.5} = 3.4, \\quad \\phi = \\{ 3.4 \\} = 0.4.\n$$\n\nTherefore,\n\n$$\n\\operatorname{Var}(T) \\approx W_{\\text{tgt}}^{2} \\, \\phi(1 - \\phi) = (0.5)^{2} \\times 0.4 \\times 0.6 = 0.25 \\times 0.24 = 0.06.\n$$\n\nGiven the small $\\sigma$ and the distance $\\phi = 0.4$ from the nearest integer boundary, the probability of wrap-around,\n\n$$\n\\mathbb{P}\\big( |Z|  \\min\\{\\phi, 1 - \\phi\\} \\big) = \\mathbb{P}\\big( |Z|  0.4 \\big),\n$$\n\nwith $Z \\sim \\mathcal{N}\\!\\left(0, \\sigma^{2} / W_{\\text{tgt}}^{2}\\right)$ and $\\sigma^{2} / W_{\\text{tgt}}^{2} = (5.0 \\times 10^{-3}) / (0.5)^{2} = 0.02$, is small (on the order of $10^{-3}$), justifying the delta-method approximation.\n\nFinally, rounding to four significant figures, the approximate unconditional variance is $0.06000$.",
            "answer": "$$\\boxed{0.06000}$$"
        }
    ]
}