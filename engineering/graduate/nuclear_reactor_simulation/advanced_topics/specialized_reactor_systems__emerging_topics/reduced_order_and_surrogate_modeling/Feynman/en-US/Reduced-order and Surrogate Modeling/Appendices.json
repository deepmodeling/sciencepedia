{
    "hands_on_practices": [
        {
            "introduction": "The foundation of many reduced-order models lies in their ability to capture the essential dynamics of a complex system using a small number of basis functions. This practice introduces Proper Orthogonal Decomposition (POD), a powerful technique for extracting these dominant modes from a collection of high-fidelity simulation data, or \"snapshots\". By applying Singular Value Decomposition (SVD) to the snapshot data, we can derive an optimal, low-dimensional basis and quantify the \"energy\" or variance captured by each mode, allowing us to make an informed trade-off between model accuracy and computational cost .",
            "id": "4245495",
            "problem": "A nuclear reactor core state estimator produces a matrix of spatial snapshots representing states such as temperature or neutron flux at discrete spatial locations under different operating conditions. Let the snapshot matrix be denoted by $X \\in \\mathbb{R}^{n \\times m}$, where each column is one snapshot across $n$ spatial degrees of freedom and there are $m$ snapshots. In reduced-order modeling, Proper Orthogonal Decomposition (POD) modes can be computed from the mean-centered snapshot matrix using the standard Euclidean inner product. Mean-centering, singular value decomposition, and energy capture are defined as follows.\n\nGiven $X$, define the snapshot mean as $\\mu = \\frac{1}{m} X \\mathbf{1}_m \\in \\mathbb{R}^{n}$, where $\\mathbf{1}_m$ is the $m$-dimensional vector of ones. Construct the mean-centered matrix $X_c = X - \\mu \\mathbf{1}_m^{\\top}$. Compute a singular value decomposition (SVD) of $X_c$ under the Euclidean inner product, that is, $X_c = U \\Sigma V^{\\top}$, where the singular values are on the diagonal of $\\Sigma$ and denoted by $\\{\\sigma_i\\}_{i=1}^{p}$ with $p = \\min(n,m)$ and ordered nonincreasingly. Define the total snapshot energy as the squared Frobenius norm $E_{\\text{tot}} = \\lVert X_c \\rVert_F^2$, and define the energy captured by the first $r$ POD modes as $E_r = \\lVert U_r U_r^{\\top} X_c \\rVert_F^2$, where $U_r$ contains the first $r$ columns of $U$. The energy capture ratio is the dimensionless quantity $E_r / E_{\\text{tot}}$.\n\nStarting from these definitions, derive the relationship between $E_r / E_{\\text{tot}}$ and the singular values $\\{\\sigma_i\\}$ and design an algorithm to determine the smallest positive integer $r$ such that the captured energy ratio satisfies $E_r / E_{\\text{tot}} \\geq 0.99$. If $E_{\\text{tot}} = 0$, define the answer to be $r = 0$. All ratios in this problem are dimensionless.\n\nYour task is to implement a program that, for each provided test case, computes the smallest $r$ as defined above. The program must not require any user input and must process the following test suite of snapshot matrices in order:\n\n- Test case $1$ (happy path, rank-one dominated, already mean-centered):\n  $$\n  X_1 = \\begin{bmatrix}\n  -2 & -1 & 0 & 1 & 2 \\\\\n  -4 & -2 & 0 & 2 & 4 \\\\\n  -6 & -3 & 0 & 3 & 6\n  \\end{bmatrix}\n  $$\n- Test case $2$ (two nonzero modes, already mean-centered):\n  $$\n  X_2 = \\begin{bmatrix}\n  -3 & -2 & -1 & 1 & 2 & 3 \\\\\n  1 & -1 & 1 & -1 & 1 & -1 \\\\\n  0 & 0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0 & 0\n  \\end{bmatrix}\n  $$\n- Test case $3$ (two equal-energy modes, already mean-centered):\n  $$\n  X_3 = \\begin{bmatrix}\n  1 & -1 & 1 & -1 \\\\\n  1 & 1 & -1 & -1\n  \\end{bmatrix}\n  $$\n- Test case $4$ (boundary case, zero energy after centering):\n  $$\n  X_4 = \\begin{bmatrix}\n  5 \\\\\n  -2 \\\\\n  7\n  \\end{bmatrix}\n  $$\n  Note: For $X_4$, since $m = 1$, the mean-centering yields $X_c = 0$, hence $E_{\\text{tot}} = 0$ and the defined answer is $r = 0$.\n\nYour program should compute the smallest $r$ for each test case in the order given, using the SVD of the mean-centered snapshots. If $E_{\\text{tot}} = 0$, return $r = 0$ for that test case. Otherwise, find the smallest $r \\in \\{1,2,\\dots,p\\}$ such that the captured energy ratio is at least $0.99$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[1,2,3,4]\"). The results must be integers corresponding to the $r$ values for $X_1$, $X_2$, $X_3$, and $X_4$, respectively. All energy ratios are dimensionless; no physical units are required in the output.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of linear algebra and reduced-order modeling, specifically Proper Orthogonal Decomposition (POD). The problem is well-posed, with a clear objective and sufficient data to find a unique solution for each test case. All terms are formally defined, and the premises are factually correct and internally consistent.\n\nThe core of the problem is to derive an expression for the energy capture ratio in terms of singular values and then use this relationship to find the minimum number of modes, $r$, required to capture at least $99\\%$ of the total energy.\n\nFirst, we establish the relationship between the total energy, $E_{\\text{tot}}$, and the singular values, $\\{\\sigma_i\\}$. The total energy is the squared Frobenius norm of the mean-centered snapshot matrix, $X_c \\in \\mathbb{R}^{n \\times m}$.\n$$\nE_{\\text{tot}} = \\lVert X_c \\rVert_F^2\n$$\nThe squared Frobenius norm of a matrix is equal to the trace of $X_c^{\\top} X_c$. Using the singular value decomposition (SVD) of $X_c = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices ($U^{\\top} U = I$, $V^{\\top} V = I$), we can write:\n$$\nX_c^{\\top} X_c = (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} = V (\\Sigma^{\\top} \\Sigma) V^{\\top}\n$$\nUsing the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$:\n$$\nE_{\\text{tot}} = \\text{Tr}(X_c^{\\top} X_c) = \\text{Tr}(V (\\Sigma^{\\top} \\Sigma) V^{\\top}) = \\text{Tr}(V^{\\top} V (\\Sigma^{\\top} \\Sigma)) = \\text{Tr}(\\Sigma^{\\top} \\Sigma)\n$$\nThe matrix $\\Sigma \\in \\mathbb{R}^{n \\times m}$ has the singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_p \\geq 0$ on its main diagonal, where $p = \\min(n, m)$. The matrix $\\Sigma^{\\top} \\Sigma$ is an $m \\times m$ diagonal matrix with diagonal entries $\\{\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_p^2\\}$, with the remaining diagonal entries being zero if $m > p$. The trace is the sum of these diagonal elements.\n$$\nE_{\\text{tot}} = \\sum_{i=1}^{p} \\sigma_i^2\n$$\nNext, we derive an expression for the energy captured by the first $r$ modes, $E_r$. This is defined as $E_r = \\lVert U_r U_r^{\\top} X_c \\rVert_F^2$, where $U_r$ consists of the first $r$ columns of $U$. The matrix $P_r = U_r U_r^{\\top}$ is the orthogonal projector onto the subspace spanned by these first $r$ POD modes.\n$$\nE_r = \\lVert P_r X_c \\rVert_F^2 = \\text{Tr}((P_r X_c)^{\\top} (P_r X_c)) = \\text{Tr}(X_c^{\\top} P_r^{\\top} P_r X_c)\n$$\nSince $P_r$ is a projector, it is idempotent ($P_r^2 = P_r$) and self-adjoint ($P_r^{\\top} = P_r$). Therefore, $P_r^{\\top} P_r = P_r$.\n$$\nE_r = \\text{Tr}(X_c^{\\top} P_r X_c)\n$$\nApplying the cyclic property of the trace and substituting $X_c = U \\Sigma V^{\\top}$:\n$$\nE_r = \\text{Tr}(P_r X_c X_c^{\\top}) = \\text{Tr}(U_r U_r^{\\top} (U \\Sigma V^{\\top})(U \\Sigma V^{\\top})^{\\top}) = \\text{Tr}(U_r U_r^{\\top} U \\Sigma \\Sigma^{\\top} U^{\\top})\n$$\nApplying the cyclic property again:\n$$\nE_r = \\text{Tr}(U^{\\top} U_r U_r^{\\top} U (\\Sigma \\Sigma^{\\top}))\n$$\nThe matrix product $U^{\\top} U_r U_r^{\\top} U$ results in a diagonal matrix of size $n \\times n$ with $1$s in the first $r$ diagonal positions and $0$s elsewhere. This is because $U^{\\top} U_r$ is an $n \\times r$ matrix whose top $r \\times r$ block is the identity matrix $I_r$ and whose lower $(n-r) \\times r$ block is zero. The product with its transpose yields the described diagonal structure. The trace of the product of this matrix and the diagonal matrix $\\Sigma \\Sigma^{\\top}$ (which has diagonal entries $\\sigma_i^2$) isolates the first $r$ squared singular values.\n$$\nE_r = \\sum_{i=1}^{r} \\sigma_i^2\n$$\nFinally, the energy capture ratio is given by:\n$$\n\\frac{E_r}{E_{\\text{tot}}} = \\frac{\\sum_{i=1}^{r} \\sigma_i^2}{\\sum_{i=1}^{p} \\sigma_i^2}\n$$\nThe algorithm to find the smallest positive integer $r$ such that this ratio is at least $0.99$ proceeds as follows:\n$1$. For a given snapshot matrix $X$, compute the mean snapshot vector $\\mu$ and the mean-centered matrix $X_c$.\n$2$. Calculate the total energy $E_{\\text{tot}} = \\lVert X_c \\rVert_F^2$. If $E_{\\text{tot}}$ is zero (or numerically indistinguishable from zero), the problem states that $r=0$.\n$3$. If $E_{\\text{tot}} > 0$, compute the singular values $\\{\\sigma_i\\}$ of $X_c$ using SVD.\n$4$. The total energy can be confirmed as $E_{\\text{tot}} = \\sum_{i=1}^p \\sigma_i^2$.\n$5$. Iterate from $k=1$ to $p$, calculating the cumulative sum of squared singular values, $S_k = \\sum_{i=1}^k \\sigma_i^2$.\n$6$. In each step, check if the ratio $S_k / E_{\\text{tot}} \\geq 0.99$. The first value of $k$ for which this inequality holds is the desired minimal rank $r$.\n\nThis algorithm is implemented for each provided test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_r(X: np.ndarray) -> int:\n    \"\"\"\n    Computes the smallest number of POD modes 'r' to capture at least 99% of the energy.\n\n    Args:\n        X: The snapshot matrix of size (n, m).\n\n    Returns:\n        The smallest integer r.\n    \"\"\"\n    # Defensive check to ensure X is a 2D array\n    X = np.atleast_2d(X)\n    \n    # Get dimensions n (spatial DOFs) and m (number of snapshots)\n    n, m = X.shape\n    \n    # Step 1: Compute the mean snapshot and the mean-centered matrix X_c\n    if m == 1:\n        # If there is only one snapshot, the mean is the snapshot itself\n        mu = X\n        X_c = X - mu  # This results in a zero matrix\n    else:\n        mu = X.mean(axis=1, keepdims=True)\n        X_c = X - mu\n        \n    # Step 2: Check for the zero energy case\n    # The total energy is the squared Frobenius norm of X_c.\n    # We can check if X_c is all zeros, which is more robust than floating point comparison of energy.\n    if not np.any(X_c):\n        return 0\n        \n    # Step 3: Compute the singular values of X_c\n    # We only need the singular values, so we can set compute_uv=False for efficiency\n    s = np.linalg.svd(X_c, compute_uv=False)\n    \n    # Step 4: Calculate the total energy from the squared singular values\n    s_squared = s**2\n    total_energy = np.sum(s_squared)\n    \n    # This check handles cases where energy is numerically very close to zero\n    if total_energy < 1e-15:\n        return 0\n\n    # Step 5: Find the smallest r satisfying the energy capture condition\n    # Calculate the cumulative sum of energy captured by the modes\n    cumulative_energy = np.cumsum(s_squared)\n    \n    # Calculate the ratio of captured energy to total energy\n    energy_ratios = cumulative_energy / total_energy\n    \n    # Find the first index where the ratio is >= 0.99\n    # np.where returns a tuple of arrays; we need the first element of the first array.\n    # The result is a 0-based index, so we add 1 to get the number of modes r.\n    r = np.where(energy_ratios >= 0.99)[0][0] + 1\n    \n    return int(r)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        np.array([\n            [-2.0, -1.0, 0.0, 1.0, 2.0],\n            [-4.0, -2.0, 0.0, 2.0, 4.0],\n            [-6.0, -3.0, 0.0, 3.0, 6.0]\n        ]),\n        # Test case 2\n        np.array([\n            [-3.0, -2.0, -1.0, 1.0, 2.0, 3.0],\n            [1.0, -1.0, 1.0, -1.0, 1.0, -1.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n        ]),\n        # Test case 3\n        np.array([\n            [1.0, -1.0, 1.0, -1.0],\n            [1.0, 1.0, -1.0, -1.0]\n        ]),\n        # Test case 4 (must be a column vector, shape (3,1))\n        np.array([\n            [5.0],\n            [-2.0],\n            [7.0]\n        ])\n    ]\n\n    results = []\n    for case in test_cases:\n        r = compute_r(case)\n        results.append(r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once an efficient basis has been extracted using a method like POD, the next step is to create a functional reduced-order model that evolves on this basis. This exercise demonstrates how to achieve this using Galerkin projection, a cornerstone of projection-based model reduction. You will apply this technique to the neutron diffusion equation, projecting the high-dimensional governing PDE onto the low-dimensional subspace spanned by the POD modes to construct a small, computationally inexpensive system of equations that approximates the original model's behavior .",
            "id": "4245475",
            "problem": "Consider the steady-state two-dimensional neutron diffusion model on a rectangular assembly domain $\\Omega = [0,L_x] \\times [0,L_y]$ with homogeneous Neumann boundary conditions. The nondimensionalized model is based on the steady neutron balance and Fick's law, resulting in the diffusion equation $-\\nabla \\cdot (D(x,y) \\nabla \\phi(x,y)) + \\Sigma_a(x,y)\\,\\phi(x,y) = q(x,y)$, where $D(x,y)$ is the diffusion coefficient, $\\Sigma_a(x,y)$ is the absorption cross-section, and $q(x,y)$ is a source term. In reduced-order modeling, a Galerkin projection onto a subspace spanned by a set of basis functions $\\{\\varphi_i\\}_{i=1}^r$ leads to a reduced operator whose entries are given by the bilinear form $a(\\varphi_i,\\varphi_j) = \\int_{\\Omega} D\\,\\nabla \\varphi_i \\cdot \\nabla \\varphi_j\\,\\mathrm{d}x\\,\\mathrm{d}y + \\int_{\\Omega} \\Sigma_a\\, \\varphi_i\\,\\varphi_j\\,\\mathrm{d}x\\,\\mathrm{d}y$. The basis $\\{\\varphi_i\\}_{i=1}^r$ is obtained from Proper Orthogonal Decomposition (POD) using a set of snapshot fields $\\{u^{(k)}(x,y)\\}_{k=1}^m$. For a uniform grid discretization with $N_x \\times N_y$ cell centers, cell spacings $h_x = L_x/N_x$ and $h_y = L_y/N_y$, and cell areas $h_x h_y$, let the discrete inner product be $(u,v)_W = \\sum_{i=1}^{N_y} \\sum_{j=1}^{N_x} u_{i,j} v_{i,j}\\, h_x h_y$. Use a weighted singular value decomposition to form a POD basis that is orthonormal with respect to $(\\cdot,\\cdot)_W$, namely $\\Phi = W^{-1/2} U_r$, where $U_r$ are the first $r$ left singular vectors of $Y = W^{1/2} X$ with $X$ being the snapshot matrix (each column is a flattened snapshot field) and $W^{1/2}$ representing multiplication by $\\sqrt{h_x h_y}$ per grid cell. Approximate the integrals defining $a(\\varphi_i,\\varphi_j)$ via the discrete quadrature $\\sum_{i=1}^{N_y} \\sum_{j=1}^{N_x} (\\cdot)\\, h_x h_y$ using cell-centered values. Approximate spatial gradients of each basis field $\\varphi_\\ell$ via finite differences: for interior indices use the centered formulas $\\partial_x \\varphi_\\ell|_{i,j} \\approx (\\varphi_{\\ell,i,j+1} - \\varphi_{\\ell,i,j-1})/(2 h_x)$ and $\\partial_y \\varphi_\\ell|_{i,j} \\approx (\\varphi_{\\ell,i+1,j} - \\varphi_{\\ell,i-1,j})/(2 h_y)$; on boundaries use one-sided differences $\\partial_x \\varphi_\\ell|_{i,0} \\approx (\\varphi_{\\ell,i,1} - \\varphi_{\\ell,i,0})/h_x$, $\\partial_x \\varphi_\\ell|_{i,N_x-1} \\approx (\\varphi_{\\ell,i,N_x-1} - \\varphi_{\\ell,i,N_x-2})/h_x$, and analogously for $\\partial_y$ at $i=0$ and $i=N_y-1$. With these definitions, compute the reduced operator entries $A_{p,q} = a(\\varphi_p,\\varphi_q)$ for a given POD basis order $r$.\n\nAll quantities are nondimensional; report numerical results without physical units. Angles do not appear. No percentages appear.\n\nYour program must construct and evaluate three test cases, each defining $(L_x,L_y)$, $(N_x,N_y)$, heterogeneous $D(x,y)$ and $\\Sigma_a(x,y)$, the snapshot set, and the POD order $r$. For each test case, return the reduced operator matrix entries flattened in row-major order as a list of floats. Aggregate the results for all test cases into a single line of output in the format described at the end of this problem statement.\n\nTest Suite:\n\n- Test Case $1$ (typical heterogeneous interior inclusion):\n  - Geometry: $L_x = 1$, $L_y = 1$, $N_x = 16$, $N_y = 16$, cell-centered grid.\n  - Diffusion: $D(x,y) = 3$ for $|x - 0.5| \\le 0.1$ and $|y - 0.5| \\le 0.1$, otherwise $D(x,y) = 1$.\n  - Absorption: $\\Sigma_a(x,y) = 0.4$ for $x \\ge 0.5$ and $y \\ge 0.5$, otherwise $\\Sigma_a(x,y) = 0.1$.\n  - Snapshots ($m = 5$), evaluated on the grid at cell centers $(x_j,y_i)$ with $x_j = (j+0.5) h_x$, $y_i = (i+0.5) h_y$. Define $x' = x/L_x$, $y' = y/L_y$ and let\n    - $u^{(1)}(x,y) = \\cos(\\pi x') \\cos(\\pi y')$,\n    - $u^{(2)}(x,y) = \\cos(2\\pi x') \\cos(\\pi y')$,\n    - $u^{(3)}(x,y) = \\sin(\\pi x') \\sin(2\\pi y')$,\n    - $u^{(4)}(x,y) = \\exp(-50[(x-0.5)^2 + (y-0.7)^2])$,\n    - $u^{(5)}(x,y) = \\exp(-40[(x-0.25)^2 + (y-0.25)^2])$.\n  - POD order: $r = 3$.\n\n- Test Case $2$ (near-homogeneous medium, edge case $r=1$):\n  - Geometry: $L_x = 1$, $L_y = 0.5$, $N_x = 8$, $N_y = 8$.\n  - Diffusion: $D(x,y) = 1$.\n  - Absorption: $\\Sigma_a(x,y) = 0.2$.\n  - Snapshots ($m = 3$), with $x' = x/L_x$, $y' = y/L_y$:\n    - $u^{(1)}(x,y) = 1$,\n    - $u^{(2)}(x,y) = \\cos(\\pi x')$,\n    - $u^{(3)}(x,y) = \\cos(\\pi y')$.\n  - POD order: $r = 1$.\n\n- Test Case $3$ (strongly heterogeneous inclusion and stripe, higher $r$):\n  - Geometry: $L_x = 1.2$, $L_y = 0.8$, $N_x = 20$, $N_y = 12$.\n  - Diffusion: $D(x,y) = 0.8 + 2.5 \\exp\\left(-\\left[\\frac{(x - 0.7 L_x)^2}{(0.15 L_x)^2} + \\frac{(y - 0.4 L_y)^2}{(0.12 L_y)^2}\\right]\\right) + 0.6\\,\\mathbf{1}_{|x - 0.3 L_x| \\le 0.08 L_x}$, where $\\mathbf{1}$ is the indicator function equal to $1$ if the condition holds, else $0$.\n  - Absorption: $\\Sigma_a(x,y) = 0.15 + 0.35 \\exp\\left(-\\left[\\frac{(x - 0.3 L_x)^2}{(0.18 L_x)^2} + \\frac{(y - 0.6 L_y)^2}{(0.15 L_y)^2}\\right]\\right) + 0.1\\,\\mathbf{1}_{|x - 0.6 L_x| \\le 0.1 L_x}$.\n  - Snapshots ($m = 6$), with $x' = x/L_x$, $y' = y/L_y$:\n    - $u^{(1)}(x,y) = \\cos(\\pi x') \\cos(\\pi y')$,\n    - $u^{(2)}(x,y) = \\cos(2\\pi x') \\cos(2\\pi y')$,\n    - $u^{(3)}(x,y) = \\sin(2\\pi x') \\cos(\\pi y')$,\n    - $u^{(4)}(x,y) = \\exp(-30[(x-0.5 L_x)^2 + (y-0.25 L_y)^2])$,\n    - $u^{(5)}(x,y) = \\exp(-25[(x-0.9 L_x)^2 + (y-0.6 L_y)^2])$,\n    - $u^{(6)}(x,y) = \\sin(\\pi x') \\sin(2\\pi y')$.\n  - POD order: $r = 4$.\n\nImplementation requirements:\n- Construct the grid and evaluate $D(x,y)$, $\\Sigma_a(x,y)$, and the snapshot fields on cell centers $(x_j,y_i)$ for each test case.\n- Build the snapshot matrix $X \\in \\mathbb{R}^{(N_x N_y) \\times m}$ by flattening each snapshot field in row-major order.\n- Use the weighted singular value decomposition with the $L^2$ inner product weight $(h_x h_y)$ per cell: compute $Y = W^{1/2} X$ by multiplying each row by $\\sqrt{h_x h_y}$, then compute the first $r$ left singular vectors $U_r$ of $Y$, and set the POD basis matrix $\\Phi = W^{-1/2} U_r$, where $W^{-1/2}$ divides each row by $\\sqrt{h_x h_y}$. Reshape each column of $\\Phi$ back to the $N_y \\times N_x$ grid to obtain basis fields $\\varphi_\\ell(x,y)$.\n- Approximate $\\nabla \\varphi_\\ell$ via the specified finite differences, and form the reduced operator entries $A_{p,q}$ using the discrete quadrature $\\sum_{i,j} h_x h_y \\left( D\\, \\nabla \\varphi_p \\cdot \\nabla \\varphi_q + \\Sigma_a\\, \\varphi_p \\varphi_q \\right)$.\n- For each test case, output the flattened reduced operator matrix entries in row-major order as a list of floats.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the row-major flattened list of reduced operator entries for a test case. For example, the format must be exactly like $[\\text{list\\_for\\_case\\_1},\\text{list\\_for\\_case\\_2},\\text{list\\_for\\_case\\_3}]$ with no extra text. Each inner list must contain $r \\times r$ floating-point numbers.",
            "solution": "The problem is valid as it presents a well-posed, scientifically grounded computational task in the field of reduced-order modeling for nuclear reactor physics simulations. All necessary data and procedural steps are provided.\n\nThe objective is to compute the entries of a reduced system operator, $A_{p,q}$, obtained via a Galerkin projection of the steady-state neutron diffusion operator onto a low-dimensional subspace. The subspace is spanned by a set of basis functions, $\\{\\varphi_i\\}_{i=1}^r$, derived from Proper Orthogonal Decomposition (POD). The analysis is performed for three distinct test cases.\n\nThe governing equation for the neutron flux $\\phi(x,y)$ on a domain $\\Omega = [0,L_x] \\times [0,L_y]$ is the two-dimensional steady-state neutron diffusion equation:\n$$-\\nabla \\cdot (D(x,y) \\nabla \\phi(x,y)) + \\Sigma_a(x,y)\\,\\phi(x,y) = q(x,y)$$\nwhere $D$ is the diffusion coefficient, $\\Sigma_a$ is the macroscopic absorption cross-section, and $q$ is a neutron source term. With homogeneous Neumann boundary conditions, the associated weak formulation involves the symmetric bilinear form $a(u,v)$:\n$$a(u,v) = \\int_{\\Omega} D(x,y)\\,\\nabla u \\cdot \\nabla v\\,\\mathrm{d}x\\,\\mathrm{d}y + \\int_{\\Omega} \\Sigma_a(x,y)\\, u\\,v\\,\\mathrm{d}x\\,\\mathrm{d}y$$\nIn the Galerkin method, the solution is approximated as a linear combination of basis functions, $\\phi(x,y) \\approx \\sum_{i=1}^r c_i \\varphi_i(x,y)$. Projecting the weak form onto the basis $\\{\\varphi_j\\}_{j=1}^r$ leads to a reduced linear system $A c = f$, where the entries of the reduced operator $A$ are given by $A_{p,q} = a(\\varphi_p,\\varphi_q)$.\n\nThe computational procedure for determining the matrix $A$ for each test case is as follows:\n\n1.  **Discretization**: The continuous domain $\\Omega$ is discretized into a uniform grid of $N_y \\times N_x$ cells. The cell centers are located at $(x_j, y_i)$, where $x_j = (j+0.5)h_x$ for $j \\in \\{0, \\dots, N_x-1\\}$ and $y_i = (i+0.5)h_y$ for $i \\in \\{0, \\dots, N_y-1\\}$, with cell dimensions $h_x = L_x/N_x$ and $h_y = L_y/N_y$. The material properties $D(x,y)$ and $\\Sigma_a(x,y)$ are evaluated at these cell centers.\n\n2.  **Snapshot Collection**: A set of $m$ snapshot fields, $\\{u^{(k)}(x,y)\\}_{k=1}^m$, which represent characteristic states of the system, are evaluated on the discrete grid. Each snapshot is flattened into a vector of length $N = N_x N_y$. These vectors are assembled as columns of the snapshot matrix $X \\in \\mathbb{R}^{N \\times m}$.\n\n3.  **POD Basis Construction**: A basis that is orthonormal with respect to the discrete $L^2$ inner product, $(u,v)_W = \\sum_{k=1}^N u_k v_k (h_x h_y)$, is constructed using a weighted Singular Value Decomposition (SVD).\n    -   The discrete inner product can be written in matrix form as $(u,v)_W = u^T W v$, where $W$ is a diagonal matrix with the cell area $dA = h_x h_y$ on its diagonal. Since the grid is uniform, $W = (h_x h_y) I$.\n    -   A weighted snapshot matrix $Y = W^{1/2} X = \\sqrt{h_x h_y} X$ is formed.\n    -   The SVD of $Y$ is computed: $Y = U S V^T$. The columns of $U$ are the left singular vectors.\n    -   The first $r$ columns of $U$ are selected to form $U_r \\in \\mathbb{R}^{N \\times r}$.\n    -   The POD basis vectors (as columns of a matrix $\\Phi$) are obtained by un-weighting $U_r$: $\\Phi = W^{-1/2} U_r = (1/\\sqrt{h_x h_y}) U_r$. The columns of $\\Phi$, denoted $\\vec{\\varphi}_p$, are the flattened representations of the basis functions $\\varphi_p$. By construction, these basis vectors are orthonormal with respect to the $(\\cdot, \\cdot)_W$ inner product: $(\\vec{\\varphi}_p, \\vec{\\varphi}_q)_W = \\delta_{pq}$.\n\n4.  **Gradient Calculation**: To evaluate the bilinear form, the gradients of the basis functions, $\\nabla \\varphi_p$, are required. Each basis vector $\\vec{\\varphi}_p$ is reshaped into an $N_y \\times N_x$ grid. The partial derivatives $\\partial_x \\varphi_p$ and $\\partial_y \\varphi_p$ are then approximated at each grid cell $(i,j)$ using the specified finite difference formulas:\n    -   **Interior points**: Second-order centered differences.\n        $$ \\frac{\\partial \\varphi_p}{\\partial x}\\bigg|_{i,j} \\approx \\frac{\\varphi_{p,i,j+1} - \\varphi_{p,i,j-1}}{2 h_x}, \\quad \\frac{\\partial \\varphi_p}{\\partial y}\\bigg|_{i,j} \\approx \\frac{\\varphi_{p,i+1,j} - \\varphi_{p,i-1,j}}{2 h_y} $$\n    -   **Boundary points**: First-order one-sided differences. For example, at the boundary $j=0$:\n        $$ \\frac{\\partial \\varphi_p}{\\partial x}\\bigg|_{i,0} \\approx \\frac{\\varphi_{p,i,1} - \\varphi_{p,i,0}}{h_x} $$\n        Analogous formulas apply to the other boundaries ($j=N_x-1$, $i=0$, $i=N_y-1$).\n\n5.  **Reduced Operator Assembly**: The entries $A_{p,q}$ of the $r \\times r$ reduced operator are calculated by approximating the integrals in the bilinear form $a(\\varphi_p, \\varphi_q)$ with a numerical quadrature over the grid cells:\n    $$ A_{p,q} = \\sum_{i=0}^{N_y-1} \\sum_{j=0}^{N_x-1} \\left[ D_{i,j} \\left( \\frac{\\partial \\varphi_p}{\\partial x}\\frac{\\partial \\varphi_q}{\\partial x} + \\frac{\\partial \\varphi_p}{\\partial y}\\frac{\\partial \\varphi_q}{\\partial y} \\right)_{i,j} + (\\Sigma_a)_{i,j} (\\varphi_p)_{i,j} (\\varphi_q)_{i,j} \\right] (h_x h_y) $$\n    This computation is performed for all pairs $(p,q)$ where $p,q \\in \\{0, \\dots, r-1\\}$. The resulting matrix $A$ is then flattened into a list in row-major order. This entire procedure is repeated for each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the solution for all test cases.\n    \"\"\"\n\n    # Test Case 1: Heterogeneous interior inclusion\n    case1_params = {\n        'Lx': 1.0, 'Ly': 1.0, 'Nx': 16, 'Ny': 16, 'r': 3,\n        'D_func': lambda x, y, Lx, Ly: 3.0 if np.abs(x - 0.5) <= 0.1 and np.abs(y - 0.5) <= 0.1 else 1.0,\n        'Sigma_a_func': lambda x, y, Lx, Ly: 0.4 if x >= 0.5 and y >= 0.5 else 0.1,\n        'snapshots': [\n            lambda x, y, Lx, Ly: np.cos(np.pi * x/Lx) * np.cos(np.pi * y/Ly),\n            lambda x, y, Lx, Ly: np.cos(2 * np.pi * x/Lx) * np.cos(np.pi * y/Ly),\n            lambda x, y, Lx, Ly: np.sin(np.pi * x/Lx) * np.sin(2 * np.pi * y/Ly),\n            lambda x, y, Lx, Ly: np.exp(-50 * ((x - 0.5)**2 + (y - 0.7)**2)),\n            lambda x, y, Lx, Ly: np.exp(-40 * ((x - 0.25)**2 + (y - 0.25)**2))\n        ]\n    }\n\n    # Test Case 2: Near-homogeneous medium\n    case2_params = {\n        'Lx': 1.0, 'Ly': 0.5, 'Nx': 8, 'Ny': 8, 'r': 1,\n        'D_func': lambda x, y, Lx, Ly: 1.0,\n        'Sigma_a_func': lambda x, y, Lx, Ly: 0.2,\n        'snapshots': [\n            lambda x, y, Lx, Ly: 1.0,\n            lambda x, y, Lx, Ly: np.cos(np.pi * x/Lx),\n            lambda x, y, Lx, Ly: np.cos(np.pi * y/Ly)\n        ]\n    }\n\n    # Test Case 3: Strongly heterogeneous inclusion and stripe\n    case3_params = {\n        'Lx': 1.2, 'Ly': 0.8, 'Nx': 20, 'Ny': 12, 'r': 4,\n        'D_func': lambda x, y, Lx, Ly: 0.8 + 2.5 * np.exp(-(((x-0.7*Lx)/(0.15*Lx))**2 + ((y-0.4*Ly)/(0.12*Ly))**2)) \\\n                                        + 0.6 * (np.abs(x - 0.3*Lx) <= 0.08*Lx),\n        'Sigma_a_func': lambda x, y, Lx, Ly: 0.15 + 0.35 * np.exp(-(((x-0.3*Lx)/(0.18*Lx))**2 + ((y-0.6*Ly)/(0.15*Ly))**2)) \\\n                                          + 0.1 * (np.abs(x - 0.6*Lx) <= 0.1*Lx),\n        'snapshots': [\n            lambda x, y, Lx, Ly: np.cos(np.pi * x/Lx) * np.cos(np.pi * y/Ly),\n            lambda x, y, Lx, Ly: np.cos(2*np.pi * x/Lx) * np.cos(2*np.pi * y/Ly),\n            lambda x, y, Lx, Ly: np.sin(2*np.pi * x/Lx) * np.cos(np.pi * y/Ly),\n            lambda x, y, Lx, Ly: np.exp(-30 * ((x-0.5*Lx)**2 + (y-0.25*Ly)**2)),\n            lambda x, y, Lx, Ly: np.exp(-25 * ((x-0.9*Lx)**2 + (y-0.6*Ly)**2)),\n            lambda x, y, Lx, Ly: np.sin(np.pi * x/Lx) * np.sin(2*np.pi * y/Ly)\n        ]\n    }\n\n    test_cases = [case1_params, case2_params, case3_params]\n    results = [process_case(case) for case in test_cases]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef get_gradient(phi, hx, hy):\n    \"\"\"\n    Computes the gradient of a field on the grid using specified finite differences.\n    \"\"\"\n    Ny, Nx = phi.shape\n    grad_x = np.zeros_like(phi)\n    grad_y = np.zeros_like(phi)\n\n    # x-gradient\n    grad_x[:, 1:-1] = (phi[:, 2:] - phi[:, :-2]) / (2 * hx)\n    grad_x[:, 0] = (phi[:, 1] - phi[:, 0]) / hx\n    grad_x[:, -1] = (phi[:, -1] - phi[:, -2]) / hx\n\n    # y-gradient\n    grad_y[1:-1, :] = (phi[2:, :] - phi[:-2, :]) / (2 * hy)\n    grad_y[0, :] = (phi[1, :] - phi[0, :]) / hy\n    grad_y[-1, :] = (phi[-1, :] - phi[-2, :]) / hy\n    \n    return grad_x, grad_y\n\ndef process_case(params):\n    \"\"\"\n    Processes a single test case to compute the reduced operator.\n    \"\"\"\n    Lx, Ly = params['Lx'], params['Ly']\n    Nx, Ny = params['Nx'], params['Ny']\n    r = params['r']\n    \n    hx, hy = Lx / Nx, Ly / Ny\n    dA = hx * hy\n    w_sqrt = np.sqrt(dA)\n\n    # 1. Grid and Parameters Setup\n    x = (np.arange(Nx) + 0.5) * hx\n    y = (np.arange(Ny) + 0.5) * hy\n    xx, yy = np.meshgrid(x, y)\n\n    D_grid = np.vectorize(params['D_func'])(xx, yy, Lx, Ly)\n    Sigma_a_grid = np.vectorize(params['Sigma_a_func'])(xx, yy, Lx, Ly)\n\n    # 2. Snapshot Generation\n    m = len(params['snapshots'])\n    snapshot_matrix_X = np.zeros((Nx * Ny, m))\n    for k, u_func in enumerate(params['snapshots']):\n        snapshot_grid = np.vectorize(u_func)(xx, yy, Lx, Ly)\n        snapshot_matrix_X[:, k] = snapshot_grid.flatten()\n\n    # 3. POD Basis Construction\n    Y = w_sqrt * snapshot_matrix_X\n    U, s, vh = svd(Y, full_matrices=False)\n    \n    Ur = U[:, :r]\n    Phi_matrix = (1.0 / w_sqrt) * Ur\n\n    # 4. Reduced Operator Assembly\n    A = np.zeros((r, r))\n    phi_basis_grids = [Phi_matrix[:, p].reshape(Ny, Nx) for p in range(r)]\n    phi_grads = [get_gradient(phi_p, hx, hy) for phi_p in phi_basis_grids]\n\n    for p in range(r):\n        for q in range(r):\n            phi_p_grid = phi_basis_grids[p]\n            phi_q_grid = phi_basis_grids[q]\n\n            grad_x_p, grad_y_p = phi_grads[p]\n            grad_x_q, grad_y_q = phi_grads[q]\n\n            # Stiffness term integrand: D * (grad(p) . grad(q))\n            stiffness_integrand = D_grid * (grad_x_p * grad_x_q + grad_y_p * grad_y_q)\n            \n            # Mass term integrand: Sigma_a * p * q\n            mass_integrand = Sigma_a_grid * phi_p_grid * phi_q_grid\n\n            # Numerical quadrature\n            A[p, q] = np.sum(stiffness_integrand + mass_integrand) * dA\n            \n    return A.flatten().tolist()\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Beyond projection-based methods, non-intrusive surrogate modeling offers a powerful alternative, especially when the governing equations are too complex or treated as a black box. This practice explores the construction of a Gaussian Process (GP) surrogate, a probabilistic model that learns the relationship between system inputs and outputs from a limited set of training data. A critical aspect of building any surrogate is assessing its predictive power, and this exercise emphasizes this by using Leave-One-Out (LOO) cross-validation to rigorously evaluate the model's generalization performance .",
            "id": "4245522",
            "problem": "You are tasked with constructing and evaluating a Gaussian Process (GP) surrogate model for the reactivity worth of a control rod in a nuclear reactor core as a function of insertion depth and fuel temperature. The goal is to assess generalization using Leave-One-Out (LOO) cross-validation. The reactor physics quantity of interest is the rod worth expressed in per cent mille (pcm), defined as the reactivity change induced by a control rod movement, with $1\\,\\text{pcm} = 10^{-5}$ in reactivity units.\n\nStarting point and assumptions:\n- The surrogate model is a Gaussian Process (GP) with a squared exponential kernel with automatic relevance determination (ARD). The input is a two-dimensional vector $\\mathbf{x} = [u, T]$, where $u$ is the control rod insertion fraction (unitless, between $0$ and $1$), and $T$ is the fuel temperature in Kelvin.\n- Observations follow $y_i = f(\\mathbf{x}_i) + \\epsilon_i$, where $\\epsilon_i$ is independent Gaussian measurement noise with variance $\\sigma_n^2$ and $f(\\cdot)$ is a latent function modeled by a GP prior.\n- The kernel is $k(\\mathbf{x}, \\mathbf{x}') = \\sigma_f^2 \\exp\\left(-\\dfrac{1}{2}\\left(\\dfrac{(u-u')^2}{\\ell_u^2} + \\dfrac{(T-T')^2}{\\ell_T^2}\\right)\\right)$, where $\\sigma_f$ is the signal amplitude (in pcm), and $\\ell_u$ and $\\ell_T$ are characteristic length scales for $u$ (unitless) and $T$ (Kelvin), respectively.\n\nTraining data construction:\n- Use the input grid with insertion fractions $u \\in \\{0.0, 0.5, 1.0\\}$ and temperatures $T \\in \\{560, 590, 620\\}\\,\\text{K}$, ordered lexicographically by $u$ ascending and, within each $u$, by $T$ ascending. This yields $9$ inputs $\\{\\mathbf{x}_i\\}_{i=0}^{8}$.\n- The physically informed latent function $w_{\\text{true}}(u, T)$ is\n$$\nw_{\\text{true}}(u, T) = W_{\\max} \\left(1 - e^{-c u}\\right) \\left(1 - a\\,(T - T_{\\text{ref}})\\right) + b\\,u\\,(T - T_{\\text{ref}}),\n$$\nwith constants $W_{\\max} = 7000\\ \\text{pcm}$, $c = 3.5$, $a = 5 \\times 10^{-4}\\ \\text{K}^{-1}$, $T_{\\text{ref}} = 590\\ \\text{K}$, and $b = -6\\,\\text{pcm/K}$. This functional form encodes that deeper insertion increases worth nonlinearly while elevated temperature reduces worth due to Doppler broadening, with a weak cross-effect.\n- Deterministic pseudo-noise is added to form observed training targets:\n$$\ny_i = w_{\\text{true}}(u_i, T_i) + \\eta_i,\\quad \\text{with}\\quad \\eta_i = A_{\\text{noise}} \\sin(0.7\\, i)\\ \\text{pcm},\n$$\nfor index $i \\in \\{0,1,\\dots,8\\}$ following the ordering specified above. The parameter $A_{\\text{noise}}$ controls the noise amplitude.\n\nModeling and evaluation task:\n- For each specified test case, construct the covariance matrix $\\mathbf{K}$ with entries $K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j) + \\sigma_n^2 \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. Using the GP posterior and properties of multivariate Gaussians, compute the Leave-One-Out (LOO) predictive mean at each training input without refitting $9$ separate models, and then compute the LOO error by comparing these predictions to the observed $y_i$ values.\n- For each test case, evaluate the Root Mean Square Error (RMSE) of LOO predictions,\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=0}^{N-1} \\left(\\widehat{y}_i^{\\text{LOO}} - y_i\\right)^2},\n$$\nwith $N = 9$. Express the RMSE in pcm.\n\nTest suite:\n- Use the following three test cases, each specified as a tuple $(\\sigma_f, \\ell_u, \\ell_T, \\sigma_n, A_{\\text{noise}})$ with units:\n    1. Case A (general moderate-noise scenario): $(3000, 0.35, 25.0, 50.0, 20.0)$, where $\\sigma_f$ in pcm, $\\ell_u$ unitless, $\\ell_T$ in Kelvin, $\\sigma_n$ in pcm, $A_{\\text{noise}}$ in pcm.\n    2. Case B (boundary condition, noise-free observations): $(3000, 0.35, 25.0, 10^{-6}, 0.0)$ to emulate negligible measurement noise while maintaining numerical stability.\n    3. Case C (edge case with mismatched length-scales): $(1500, 0.05, 200.0, 50.0, 20.0)$, representing over-sensitivity in $u$ and excessive smoothness in $T$.\n\nRequired output:\n- Your program should produce a single line of output containing the LOO RMSE for each of the three test cases as a comma-separated list enclosed in square brackets, ordered as Case A, Case B, Case C, for example $[\\text{rmse}_A,\\text{rmse}_B,\\text{rmse}_C]$.\n- The numerical values must represent RMSE in pcm as floating-point numbers.\n\nAngle units do not apply. All physical units have been specified. No external data should be read; all computations must be performed within the program.",
            "solution": "The problem requires the construction and evaluation of a Gaussian Process (GP) surrogate model for a nuclear reactor physics quantity. The evaluation is to be performed using Leave-One-Out (LOO) cross-validation. The final output is the Root Mean Square Error (RMSE) of the LOO predictions for three distinct sets of model hyperparameters. The solution proceeds by first generating the training data, then constructing the components of the GP model, and finally applying the analytical formula for LOO-CV to compute the RMSE.\n\nFirst, we establish the training dataset. The input domain is two-dimensional, consisting of the control rod insertion fraction $u$ and the fuel temperature $T$. The input vectors are denoted as $\\mathbf{x} = [u, T]$. The problem specifies a grid of $N=9$ training points, formed by the Cartesian product of $u \\in \\{0.0, 0.5, 1.0\\}$ and $T \\in \\{560, 590, 620\\}\\ \\text{K}$. The points $\\{\\mathbf{x}_i\\}_{i=0}^{8}$ are ordered lexicographically, with $u$ as the primary key (ascending) and $T$ as the secondary key (ascending). This yields the following sequence of training inputs:\n$\\mathbf{x}_0 = [0.0, 560]$, $\\mathbf{x}_1 = [0.0, 590]$, $\\mathbf{x}_2 = [0.0, 620]$,\n$\\mathbf{x}_3 = [0.5, 560]$, $\\mathbf{x}_4 = [0.5, 590]$, $\\mathbf{x}_5 = [0.5, 620]$,\n$\\mathbf{x}_6 = [1.0, 560]$, $\\mathbf{x}_7 = [1.0, 590]$, $\\mathbf{x}_8 = [1.0, 620]$.\n\nThe corresponding training outputs (targets), $y_i$, are generated from a physically-informed latent function $w_{\\text{true}}(u, T)$ with added deterministic pseudo-noise. The latent function is given by:\n$$\nw_{\\text{true}}(u, T) = W_{\\max} \\left(1 - e^{-c u}\\right) \\left(1 - a\\,(T - T_{\\text{ref}})\\right) + b\\,u\\,(T - T_{\\text{ref}})\n$$\nwith constants $W_{\\max} = 7000\\ \\text{pcm}$, $c = 3.5$, $a = 5 \\times 10^{-4}\\ \\text{K}^{-1}$, $T_{\\text{ref}} = 590\\ \\text{K}$, and $b = -6\\ \\text{pcm/K}$.\nThe observed targets $y_i$ are then constructed as:\n$$\ny_i = w_{\\text{true}}(u_i, T_i) + \\eta_i\n$$\nwhere the pseudo-noise term $\\eta_i$ is defined as $\\eta_i = A_{\\text{noise}} \\sin(0.7\\, i)$ for $i \\in \\{0, 1, \\dots, 8\\}$. The amplitude $A_{\\text{noise}}$ is one of the parameters specified in each test case.\n\nThe surrogate model is a Gaussian Process. A GP defines a prior over functions, $f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))$. We assume a zero mean function, $m(\\mathbf{x})=0$. The covariance function, or kernel, is the squared exponential with automatic relevance determination (ARD):\n$$\nk(\\mathbf{x}, \\mathbf{x}') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{(u-u')^2}{\\ell_u^2} + \\frac{(T-T')^2}{\\ell_T^2}\\right)\\right)\n$$\nThe hyperparameters $(\\sigma_f, \\ell_u, \\ell_T)$ are the signal amplitude and the characteristic length-scales for each input dimension, respectively. The observations are assumed to be corrupted by independent Gaussian noise, $y_i = f(\\mathbf{x}_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nGiven a set of training data $\\mathcal{D} = \\{\\mathbf{X}, \\mathbf{y}\\}$, where $\\mathbf{X}$ is the $N \\times 2$ matrix of inputs and $\\mathbf{y}$ is the $N \\times 1$ vector of outputs, the GP model is defined. The covariance matrix of the training inputs is denoted by $\\mathbf{K}$, with entries $K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$. The covariance matrix of the noisy observations is $\\mathbf{K}_y = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$, where $\\mathbf{I}$ is the $N \\times N$ identity matrix.\n\nThe core of the task is to compute the LOO-CV error. For each point $i$, we need to calculate the prediction $\\widehat{y}_i^{\\text{LOO}}$ using a GP trained on all data except $(\\mathbf{x}_i, y_i)$. Performing this by retraining the model $N$ times is computationally inefficient. Fortunately, for GPs, there exists an analytical shortcut. The LOO predictive mean $\\mu_{-i}(\\mathbf{x}_i) = \\widehat{y}_i^{\\text{LOO}}$ and variance can be expressed in terms of quantities derived from the full model.\n\nLet $\\mathbf{C} = \\mathbf{K}_y^{-1} = (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1}$ be the inverse of the full data covariance matrix. Let $\\mathbf{\\alpha} = \\mathbf{C}\\mathbf{y}$. The LOO predictive mean for the $i$-th point is given by:\n$$\n\\widehat{y}_i^{\\text{LOO}} = \\mu_{-i}(\\mathbf{x}_i) = y_i - \\frac{\\alpha_i}{C_{ii}}\n$$\nwhere $\\alpha_i$ is the $i$-th element of the vector $\\mathbf{\\alpha}$ and $C_{ii}$ is the $i$-th diagonal element of the matrix $\\mathbf{C}$.\n\nThe performance metric is the RMSE of the LOO predictions, defined as:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=0}^{N-1} \\left(\\widehat{y}_i^{\\text{LOO}} - y_i\\right)^2}\n$$\nThe difference term inside the summation is the LOO prediction error for point $i$. Using the formula for $\\widehat{y}_i^{\\text{LOO}}$, this error is:\n$$\n\\widehat{y}_i^{\\text{LOO}} - y_i = \\left(y_i - \\frac{\\alpha_i}{C_{ii}}\\right) - y_i = -\\frac{\\alpha_i}{C_{ii}}\n$$\nSubstituting this into the RMSE formula, we get:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=0}^{N-1} \\left(-\\frac{\\alpha_i}{C_{ii}}\\right)^2} = \\sqrt{\\frac{1}{N} \\sum_{i=0}^{N-1} \\left(\\frac{\\alpha_i}{C_{ii}}\\right)^2}\n$$\n\nThe overall algorithm for each test case is as follows:\n1.  Set the hyperparameters $(\\sigma_f, \\ell_u, \\ell_T, \\sigma_n, A_{\\text{noise}})$ from the test case.\n2.  Generate the $N=9$ training input vectors $\\mathbf{X}$ and the corresponding target values $\\mathbf{y}$.\n3.  Compute the $9 \\times 9$ kernel matrix $\\mathbf{K}$ using the ARD SE kernel.\n4.  Form the noisy covariance matrix $\\mathbf{K}_y = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$.\n5.  Compute the inverse matrix $\\mathbf{C} = \\mathbf{K}_y^{-1}$.\n6.  Compute the vector $\\mathbf{\\alpha} = \\mathbf{C}\\mathbf{y}$.\n7.  For each $i$ from $0$ to $8$, calculate the LOO residual $r_i = \\alpha_i/C_{ii}$.\n8.  Compute the RMSE as $\\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} r_i^2}$.\n\nThis procedure is repeated for all three test cases, and the resulting RMSE values are reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates a Gaussian Process surrogate model for nuclear reactor\n    control rod worth using Leave-One-Out (LOO) cross-validation.\n    \"\"\"\n    \n    # Define physical constants for the true function\n    W_max = 7000.0  # pcm\n    c = 3.5         # unitless\n    a = 5.0e-4      # K^-1\n    T_ref = 590.0   # K\n    b = -6.0        # pcm/K\n\n    def w_true(u, T):\n        \"\"\"Calculates the physically informed latent function value.\"\"\"\n        term1 = W_max * (1.0 - np.exp(-c * u)) * (1.0 - a * (T - T_ref))\n        term2 = b * u * (T - T_ref)\n        return term1 + term2\n\n    # Generate the training input grid as specified\n    u_vals = np.array([0.0, 0.5, 1.0])\n    T_vals = np.array([560.0, 590.0, 620.0])\n    \n    X_list = []\n    for u_val in u_vals:\n        for T_val in T_vals:\n            X_list.append([u_val, T_val])\n    X = np.array(X_list)\n    N = X.shape[0]\n\n    def ard_se_kernel(X_data, sigma_f, l_u, l_T):\n        \"\"\"Computes the ARD SE kernel matrix using numpy broadcasting.\"\"\"\n        length_scales = np.array([l_u, l_T])\n        scaled_X = X_data / length_scales\n        \n        # Compute squared Euclidean distance matrix on scaled data\n        # (N, 1, D) - (1, N, D) -> (N, N, D), then sum over D\n        diffs = scaled_X[:, np.newaxis, :] - scaled_X[np.newaxis, :, :]\n        sq_dists = np.sum(diffs**2, axis=-1)\n        \n        return sigma_f**2 * np.exp(-0.5 * sq_dists)\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # (sigma_f, l_u, l_T, sigma_n, A_noise)\n        (3000.0, 0.35, 25.0, 50.0, 20.0),   # Case A\n        (3000.0, 0.35, 25.0, 1e-6, 0.0),    # Case B\n        (1500.0, 0.05, 200.0, 50.0, 20.0),  # Case C\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        sigma_f, l_u, l_T, sigma_n, A_noise = case\n\n        # Generate training targets Y based on the current case's noise amplitude\n        Y_true = np.array([w_true(u, T) for u, T in X])\n        noise_term = np.array([A_noise * np.sin(0.7 * i) for i in range(N)])\n        Y = Y_true + noise_term\n        \n        # Construct the GP covariance matrices\n        K = ard_se_kernel(X, sigma_f, l_u, l_T)\n        K_y = K + (sigma_n**2) * np.eye(N)\n\n        # Compute quantities for the analytical LOO-CV formula\n        # C = inv(K_y)\n        # alpha = C @ Y\n        # Using a direct call to inv is acceptable for a small 9x9 matrix.\n        C = np.linalg.inv(K_y)\n        alpha = C.dot(Y)\n\n        # The i-th LOO residual (y_i - y_hat_loo_i) is alpha_i / C_ii.\n        # The error term in the RMSE formula (y_hat_loo_i - y_i) is -alpha_i / C_ii.\n        # Squaring this gives (alpha_i / C_ii)^2.\n        loo_residuals = alpha / np.diag(C)\n        \n        # Compute the Root Mean Square Error of LOO predictions\n        rmse = np.sqrt(np.mean(loo_residuals**2))\n        \n        results.append(rmse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}