{
    "hands_on_practices": [
        {
            "introduction": "The Chebyshev Rational Approximation Method (CRAM) is built upon rational functions that approximate the matrix exponential. This first exercise  gives you direct practice in deriving such a function from first principles to meet specific accuracy goals. By constructing an approximant that prioritizes accuracy for physical phenomena with slow dynamics (like long-lived nuclides), you will develop a fundamental understanding of how these powerful numerical tools are engineered.",
            "id": "4216709",
            "problem": "In neutron-induced transmutation and radioactive decay simulations for nuclear fuel depletion, the nuclide number density vector $\\mathbf{N}(t)$ satisfies the Bateman transmutation equation $\\frac{d\\mathbf{N}}{dt} = \\mathbf{A}(t)\\,\\mathbf{N}(t)$, where $\\mathbf{A}(t)$ is the time-dependent transmutation-decay matrix whose eigenvalues have nonpositive real parts. Over a time step of duration $t0$ during which $\\mathbf{A}$ is frozen, the formal solution is $\\mathbf{N}(t+\\Delta t) = \\exp(t\\,\\mathbf{A})\\,\\mathbf{N}(t)$. In practice, the Chebyshev Rational Approximation Method (CRAM) replaces $\\exp(t\\,\\mathbf{A})$ with a rational function of $\\mathbf{A}$ to achieve high accuracy and stability across the spectrum of $\\mathbf{A}$.\n\nLong-lived nuclides correspond to small decay constants, which map to eigenvalues $\\lambda$ of $\\mathbf{A}$ near $\\lambda=0$ on the negative real axis. To prioritize accuracy for such modes, consider the scalar target function $f(\\lambda) = \\exp(-t\\,\\lambda)$ for $\\lambda \\ge 0$, representing the action of $\\exp(t\\,\\mathbf{A})$ on real nonnegative $\\lambda$ via the mapping $\\lambda \\mapsto -\\lambda$.\n\nConstruct a weighted rational approximant $R(\\lambda)$ of degree $[1/1]$ to $f(\\lambda)$ that prioritizes accuracy near $\\lambda=0$ by enforcing the following design constraints:\n- Exact value preservation at $\\lambda=0$: $R(0) = f(0)$.\n- Exact slope preservation at $\\lambda=0$: $R'(0) = f'(0)$.\n- A weighted near-zero interpolation at a small positive abscissa $\\sigma0$: $R(\\sigma) = f(\\sigma)$, where $\\sigma$ is chosen by the practitioner to reflect the desired emphasis near $\\lambda=0$.\n\nAssume the general form $R(\\lambda) = \\frac{1 + a\\,\\lambda}{1 + b\\,\\lambda}$ with real coefficients $a$ and $b$. Derive the explicit analytical expression for $R(\\lambda)$ in terms of $t$ and $\\sigma$. Provide your final answer as a single closed-form expression. No numerical evaluation is required, and no rounding should be performed. The final answer must contain no units.",
            "solution": "The problem asks for the construction of a specific weighted rational approximant $R(\\lambda)$ of degree $[1/1]$ to the function $f(\\lambda) = \\exp(-t\\,\\lambda)$. The approximant must satisfy a set of interpolation and derivative-matching conditions at $\\lambda=0$ and at a small positive abscissa $\\sigma  0$.\n\nFirst, we perform a validation of the problem statement.\n\n### Step 1: Extract Givens\n- **Target Function**: $f(\\lambda) = \\exp(-t\\,\\lambda)$, for $\\lambda \\ge 0$.\n- **Rational Approximant Form**: $R(\\lambda) = \\frac{1 + a\\,\\lambda}{1 + b\\,\\lambda}$, where $a$ and $b$ are real coefficients.\n- **Constraints**:\n    1.  $R(0) = f(0)$ (Exact value at $\\lambda=0$)\n    2.  $R'(0) = f'(0)$ (Exact slope at $\\lambda=0$)\n    3.  $R(\\sigma) = f(\\sigma)$ for $\\sigma  0$ (Interpolation at $\\lambda=\\sigma$)\n- **Objective**: Derive the explicit analytical expression for $R(\\lambda)$ in terms of $t$ and $\\sigma$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, situated within the well-established field of numerical methods for solving systems of ordinary differential equations that arise in nuclear engineering. The use of rational approximations for the matrix exponential is a standard and powerful technique (e.g., Pad√© approximations, CRAM). The problem is well-posed: it provides a specific form for the approximant with two unknown parameters, $a$ and $b$, and imposes three constraints. As we will see, one constraint is automatically satisfied by the chosen form of $R(\\lambda)$, leaving two independent constraints to determine the two unknowns. The problem is objective, mathematically precise, and contains no contradictions or ambiguities.\n\n### Verdict and Action\nThe problem is valid. We proceed with the derivation.\n\n### Derivation of the Rational Approximant\n\nThe target function is $f(\\lambda) = \\exp(-t\\,\\lambda)$. Its first derivative with respect to $\\lambda$ is $f'(\\lambda) = -t\\,\\exp(-t\\,\\lambda)$.\n\nThe proposed rational approximant is $R(\\lambda) = \\frac{1 + a\\,\\lambda}{1 + b\\,\\lambda}$. Its first derivative, found using the quotient rule, is:\n$$\nR'(\\lambda) = \\frac{d}{d\\lambda} \\left( \\frac{1 + a\\,\\lambda}{1 + b\\,\\lambda} \\right) = \\frac{a(1 + b\\,\\lambda) - (1 + a\\,\\lambda)b}{(1 + b\\,\\lambda)^2} = \\frac{a + ab\\,\\lambda - b - ab\\,\\lambda}{(1 + b\\,\\lambda)^2} = \\frac{a - b}{(1 + b\\,\\lambda)^2}\n$$\n\nWe now apply the three given constraints to determine the coefficients $a$ and $b$.\n\n**Constraint 1: $R(0) = f(0)$**\nWe evaluate both functions at $\\lambda=0$:\n$$\nf(0) = \\exp(-t \\cdot 0) = \\exp(0) = 1\n$$\n$$\nR(0) = \\frac{1 + a \\cdot 0}{1 + b \\cdot 0} = \\frac{1}{1} = 1\n$$\nThe condition $R(0) = f(0)$ is thus satisfied for any choice of $a$ and $b$ due to the prescribed form of $R(\\lambda)$. This constraint, while important for context, is automatically fulfilled and does not yield an equation for $a$ or $b$.\n\n**Constraint 2: $R'(0) = f'(0)$**\nWe evaluate the derivatives at $\\lambda=0$:\n$$\nf'(0) = -t\\,\\exp(-t \\cdot 0) = -t\n$$\n$$\nR'(0) = \\frac{a - b}{(1 + b \\cdot 0)^2} = a - b\n$$\nEquating these gives our first equation relating $a$ and $b$:\n$$\na - b = -t \\implies a = b - t\n$$\n\n**Constraint 3: $R(\\sigma) = f(\\sigma)$**\nWe evaluate the functions at $\\lambda=\\sigma$:\n$$\nf(\\sigma) = \\exp(-t\\,\\sigma)\n$$\n$$\nR(\\sigma) = \\frac{1 + a\\,\\sigma}{1 + b\\,\\sigma}\n$$\nEquating these gives our second equation:\n$$\n\\frac{1 + a\\,\\sigma}{1 + b\\,\\sigma} = \\exp(-t\\,\\sigma)\n$$\n\nNow, we solve the system of two equations for the two unknowns, $a$ and $b$. We substitute the expression for $a$ from the second constraint into the equation from the third constraint:\n$$\n\\frac{1 + (b - t)\\sigma}{1 + b\\,\\sigma} = \\exp(-t\\,\\sigma)\n$$\nWe proceed to solve for $b$:\n$$\n1 + b\\,\\sigma - t\\,\\sigma = (1 + b\\,\\sigma)\\exp(-t\\,\\sigma)\n$$\n$$\n1 + b\\,\\sigma - t\\,\\sigma = \\exp(-t\\,\\sigma) + b\\,\\sigma\\,\\exp(-t\\,\\sigma)\n$$\nGathering terms with $b$:\n$$\nb\\,\\sigma - b\\,\\sigma\\,\\exp(-t\\,\\sigma) = t\\,\\sigma - 1 + \\exp(-t\\,\\sigma)\n$$\n$$\nb\\,\\sigma(1 - \\exp(-t\\,\\sigma)) = t\\,\\sigma - 1 + \\exp(-t\\,\\sigma)\n$$\nSolving for $b$:\n$$\nb = \\frac{t\\,\\sigma - 1 + \\exp(-t\\,\\sigma)}{\\sigma(1 - \\exp(-t\\,\\sigma))}\n$$\nNow we find $a$ using $a = b - t$:\n$$\na = \\frac{t\\,\\sigma - 1 + \\exp(-t\\,\\sigma)}{\\sigma(1 - \\exp(-t\\,\\sigma))} - t\n$$\n$$\na = \\frac{t\\,\\sigma - 1 + \\exp(-t\\,\\sigma) - t\\,\\sigma(1 - \\exp(-t\\,\\sigma))}{\\sigma(1 - \\exp(-t\\,\\sigma))}\n$$\n$$\na = \\frac{t\\,\\sigma - 1 + \\exp(-t\\,\\sigma) - t\\,\\sigma + t\\,\\sigma\\,\\exp(-t\\,\\sigma)}{\\sigma(1 - \\exp(-t\\,\\sigma))}\n$$\n$$\na = \\frac{-1 + \\exp(-t\\,\\sigma) + t\\,\\sigma\\,\\exp(-t\\,\\sigma)}{\\sigma(1 - \\exp(-t\\,\\sigma))} = \\frac{\\exp(-t\\,\\sigma)(1+t\\,\\sigma) - 1}{\\sigma(1 - \\exp(-t\\,\\sigma))}\n$$\nWith the coefficients $a$ and $b$ determined, we substitute them back into the expression for $R(\\lambda)$:\n$$\nR(\\lambda) = \\frac{1 + a\\,\\lambda}{1 + b\\,\\lambda} = \\frac{1 + \\lambda \\left( \\frac{\\exp(-t\\,\\sigma)(1+t\\,\\sigma) - 1}{\\sigma(1 - \\exp(-t\\,\\sigma))} \\right)}{1 + \\lambda \\left( \\frac{t\\,\\sigma - 1 + \\exp(-t\\,\\sigma)}{\\sigma(1 - \\exp(-t\\,\\sigma))} \\right)}\n$$\nTo simplify this complex fraction, we multiply the numerator and denominator by $\\sigma(1 - \\exp(-t\\,\\sigma))$:\n$$\nR(\\lambda) = \\frac{\\sigma(1 - \\exp(-t\\,\\sigma)) + \\lambda(\\exp(-t\\,\\sigma)(1+t\\,\\sigma) - 1)}{\\sigma(1 - \\exp(-t\\,\\sigma)) + \\lambda(t\\,\\sigma - 1 + \\exp(-t\\,\\sigma))}\n$$\nThis expression can be rearranged into a more revealing structure.\nNumerator:\n$$\nN(\\lambda) = \\sigma - \\sigma\\,\\exp(-t\\,\\sigma) + \\lambda\\,t\\,\\sigma\\,\\exp(-t\\,\\sigma) + \\lambda\\,\\exp(-t\\,\\sigma) - \\lambda\n$$\n$$\nN(\\lambda) = (\\sigma-\\lambda)(1-\\exp(-t\\,\\sigma)) + \\lambda-\\lambda\\exp(-t\\,\\sigma) + \\lambda\\,t\\,\\sigma\\,\\exp(-t\\,\\sigma) + \\lambda\\,\\exp(-t\\,\\sigma) - \\lambda = (\\sigma-\\lambda)(1-\\exp(-t\\,\\sigma)) + \\lambda\\,t\\,\\sigma\\,\\exp(-t\\,\\sigma)\n$$\nDenominator:\n$$\nD(\\lambda) = \\sigma - \\sigma\\,\\exp(-t\\,\\sigma) + \\lambda\\,t\\,\\sigma - \\lambda + \\lambda\\,\\exp(-t\\,\\sigma)\n$$\n$$\nD(\\lambda) = (\\sigma-\\lambda)(1-\\exp(-t\\,\\sigma)) + \\lambda - \\lambda\\exp(-t\\,\\sigma) + \\lambda\\,t\\,\\sigma + \\lambda\\,\\exp(-t\\,\\sigma) - \\lambda = (\\sigma-\\lambda)(1-\\exp(-t\\,\\sigma)) + \\lambda\\,t\\,\\sigma\n$$\nCombining these gives the final, simplified expression for the rational approximant $R(\\lambda)$:\n$$\nR(\\lambda) = \\frac{(\\sigma-\\lambda)(1-\\exp(-t\\,\\sigma)) + \\lambda\\,t\\,\\sigma\\,\\exp(-t\\,\\sigma)}{(\\sigma-\\lambda)(1-\\exp(-t\\,\\sigma)) + \\lambda\\,t\\,\\sigma}\n$$\nThis is the desired closed-form expression for the approximant in terms of $\\lambda$, $t$, and $\\sigma$.",
            "answer": "$$\n\\boxed{\\frac{(\\sigma-\\lambda)(1-\\exp(-t\\,\\sigma)) + \\lambda\\,t\\,\\sigma\\,\\exp(-t\\,\\sigma)}{(\\sigma-\\lambda)(1-\\exp(-t\\,\\sigma)) + \\lambda\\,t\\,\\sigma}}\n$$"
        },
        {
            "introduction": "Moving from mathematical theory to robust software requires careful verification, especially when dealing with the nuances of floating-point arithmetic. CRAM often uses complex poles and residues to efficiently compute a real-valued physical result, a property guaranteed by conjugate symmetry. This coding practice  challenges you to implement unit tests that confirm this crucial property, teaching you how to ensure that abstract mathematical guarantees hold in a practical computational setting.",
            "id": "4216692",
            "problem": "You are given the task of verifying a critical numerical property that underpins the Chebyshev rational approximation method (CRAM) used in nuclear reactor simulation, specifically in the numerical evaluation of the matrix exponential for the burnup operator in depletion calculations. The fundamental basis to be used is the algebra of complex conjugation and standard properties of rational functions.\n\nConsider a rational function in partial fraction form used in CRAM, where complex poles and residues appear in conjugate pairs. Let the rational function be defined by the scalar mapping $r : \\mathbb{C} \\to \\mathbb{C}$,\n$$\nr(x) = \\alpha_0 + \\sum_{k=1}^{m} \\left( \\frac{\\alpha_k}{x - \\theta_k} + \\frac{\\overline{\\alpha_k}}{x - \\overline{\\theta_k}} \\right),\n$$\nwhere $x \\in \\mathbb{R}$, $\\alpha_0 \\in \\mathbb{R}$, and $(\\alpha_k, \\theta_k)$ are complex numbers with $\\overline{\\alpha_k}$ and $\\overline{\\theta_k}$ denoting their complex conjugates. In CRAM, this structure guarantees that for real inputs $x$, the contributions of each conjugate pair sum to a real value, a fact essential to obtaining real-valued nuclide densities from real initial densities.\n\nYour task is to write a complete, runnable program that constructs unit tests verifying the real-valued output property to machine precision, and detecting its failure when conjugate symmetry is broken. The tests should be framed in purely mathematical terms, but the scenario is consistent with real nuclear reactor depletion calculations where $x$ corresponds to the scalar product of a decay constant and a time step, which is dimensionless. No physical units are required.\n\nDefinition of machine precision: Let $\\epsilon$ denote the machine epsilon for double precision arithmetic, i.e., $\\epsilon = 2^{-52}$. For a computed complex value $y$, define an absolute tolerance\n$$\n\\tau(y) = 10^3 \\, \\epsilon \\, \\left( 1 + |y| \\right).\n$$\nA computed value $y$ will be considered real to machine precision if its imaginary part satisfies $|\\operatorname{Im}(y)| \\le \\tau(y)$.\n\nYou will use the following deterministic test suite of parameters. The base conjugate pairs are given by $m = 6$ complex pairs with a real scalar offset $\\alpha_0$:\n- Real offset: $\\alpha_0 = 0.1$.\n- Poles $(\\theta_k)$ and residues $(\\alpha_k)$ for $k=1,\\dots,6$:\n  - $k=1$: $\\theta_1 = -0.1 + 0.3 i$, $\\alpha_1 = 0.2 + 0.05 i$.\n  - $k=2$: $\\theta_2 = -0.5 + 0.8 i$, $\\alpha_2 = 0.1 + 0.2 i$.\n  - $k=3$: $\\theta_3 = -1.2 + 1.5 i$, $\\alpha_3 = 0.05 + 0.3 i$.\n  - $k=4$: $\\theta_4 = -3.0 + 2.7 i$, $\\alpha_4 = 0.02 + 0.25 i$.\n  - $k=5$: $\\theta_5 = -5.5 + 3.8 i$, $\\alpha_5 = 0.01 + 0.15 i$.\n  - $k=6$: $\\theta_6 = -8.0 + 5.0 i$, $\\alpha_6 = 0.005 + 0.1 i$.\n- The full set of poles and residues is formed by adding each conjugate counterpart, i.e., also include $\\overline{\\theta_k}$ and $\\overline{\\alpha_k}$ for every $k$.\n\nDefine the four test cases as follows:\n- Test case 1 (exact conjugate symmetry on real inputs): Use the exact conjugate pairs above. Evaluate $r(x)$ at the real inputs\n  $$\n  x \\in \\{-10^{-12}, -10^{-6}, -10^{-3}, -0.1, -1.0, -10.0, -10^{3}\\}.\n  $$\n  The test result is a boolean equal to true if and only if the maximum of $|\\operatorname{Im}(r(x))|$ over the specified inputs is less than or equal to $\\tau(r(x))$ for each input.\n- Test case 2 (broken conjugate symmetry detection): Modify only the conjugate residue corresponding to $k=1$ by adding a small imaginary perturbation $\\delta = 10^{-6}$ to $\\overline{\\alpha_1}$, i.e., replace $\\overline{\\alpha_1}$ by $\\overline{\\alpha_1} + i \\delta$, while leaving all other pairs exact. Evaluate $r(x)$ at the same inputs as in Test case 1. The test result is a boolean equal to true if and only if the maximum of $|\\operatorname{Im}(r(x))|$ over the inputs strictly exceeds $10^3 \\epsilon$.\n- Test case 3 (boundary inputs including zero and very large magnitude): Use the exact conjugate pairs. Evaluate $r(x)$ at\n  $$\n  x \\in \\{0.0, -10^{-9}, -10^{-3}, -1.0, -10^{6}, -10^{12}\\}.\n  $$\n  The test result is a boolean equal to true if and only if the maximum of $|\\operatorname{Im}(r(x))|$ over the inputs is less than or equal to $\\tau(r(x))$ for each input.\n- Test case 4 (diagonal matrix evaluation consistent with reactor depletion operators): Consider the diagonal real matrix $A = \\operatorname{diag}(a_1,\\dots,a_5)$ with diagonal entries\n  $$\n  (a_1,\\dots,a_5) = (-10^{-3}, -0.2, -1.5, -10.0, -100.0).\n  $$\n  Evaluate the rational map on this diagonal matrix via elementwise evaluation on its spectrum, i.e., compute $r(a_j)$ for $j=1,\\dots,5$. The test result is a boolean equal to true if and only if the maximum of $|\\operatorname{Im}(r(a_j))|$ over $j=1,\\dots,5$ is less than or equal to $\\tau(r(a_j))$ for each $j$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result should be a boolean corresponding to the four test cases above, in order. No other output should be printed.",
            "solution": "The problem requires the numerical verification of a fundamental property of rational functions constructed with conjugate symmetry, as used in the Chebyshev rational approximation method (CRAM) for nuclear reactor physics simulations. Specifically, we must verify that for a real input $x$, the rational function $r(x)$ produces a real output, and that this property is compromised when the conjugate symmetry is broken.\n\nThe rational function is given by\n$$\nr(x) = \\alpha_0 + \\sum_{k=1}^{m} \\left( \\frac{\\alpha_k}{x - \\theta_k} + \\frac{\\overline{\\alpha_k}}{x - \\overline{\\theta_k}} \\right)\n$$\nwhere $x, \\alpha_0 \\in \\mathbb{R}$ and $\\alpha_k, \\theta_k \\in \\mathbb{C}$. The notation $\\overline{z}$ represents the complex conjugate of $z$.\n\nThe core mathematical principle is that a function constructed with this conjugate-pair symmetry maps real numbers to real numbers. We can prove this by examining the complex conjugate of $r(x)$ for a real input $x$. Since $x$ is real, $\\overline{x} = x$.\nThe conjugate of $r(x)$ is:\n$$\n\\overline{r(x)} = \\overline{\\alpha_0 + \\sum_{k=1}^{m} \\left( \\frac{\\alpha_k}{x - \\theta_k} + \\frac{\\overline{\\alpha_k}}{x - \\overline{\\theta_k}} \\right)}\n$$\nUsing the properties of complex conjugation ($\\overline{z_1 + z_2} = \\overline{z_1} + \\overline{z_2}$, $\\overline{z_1 / z_2} = \\overline{z_1} / \\overline{z_2}$, and $\\overline{\\overline{z}} = z$):\n$$\n\\overline{r(x)} = \\overline{\\alpha_0} + \\sum_{k=1}^{m} \\left( \\overline{\\left(\\frac{\\alpha_k}{x - \\theta_k}\\right)} + \\overline{\\left(\\frac{\\overline{\\alpha_k}}{x - \\overline{\\theta_k}}\\right)} \\right)\n$$\n$$\n\\overline{r(x)} = \\alpha_0 + \\sum_{k=1}^{m} \\left( \\frac{\\overline{\\alpha_k}}{\\overline{x} - \\overline{\\theta_k}} + \\frac{\\overline{\\overline{\\alpha_k}}}{\\overline{x} - \\overline{\\overline{\\theta_k}}} \\right)\n$$\nSince $\\overline{x} = x$ and $\\overline{\\alpha_0} = \\alpha_0$ (as it is real), this simplifies to:\n$$\n\\overline{r(x)} = \\alpha_0 + \\sum_{k=1}^{m} \\left( \\frac{\\overline{\\alpha_k}}{x - \\overline{\\theta_k}} + \\frac{\\alpha_k}{x - \\theta_k} \\right) = r(x)\n$$\nA complex number $y$ is real if and only if $y = \\overline{y}$. Since we have shown that $\\overline{r(x)} = r(x)$ for real $x$, $r(x)$ must be a real number.\n\nIn numerical computation using finite-precision floating-point arithmetic, the calculated imaginary part of $r(x)$ may not be exactly zero due to round-off errors. Therefore, we must test if the imaginary part is negligible within a defined tolerance. The problem provides a specific tolerance criterion: a computed complex value $y$ is considered real if its imaginary part satisfies $|\\operatorname{Im}(y)| \\le \\tau(y)$, where the absolute tolerance $\\tau(y)$ is given by:\n$$\n\\tau(y) = 10^3 \\, \\epsilon \\, \\left( 1 + |y| \\right)\n$$\nHere, $\\epsilon = 2^{-52}$ is the machine epsilon for double precision. This is a mixed absolute-relative tolerance, which is robust for values of $y$ that are close to zero or very large. The factor of $10^3$ accommodates the accumulation of floating-point errors from the multiple arithmetic operations in the sum.\n\nThe four test cases are designed to verify this property under different conditions. The implementation will proceed as follows:\n\n1.  A core function, `evaluate_r(x, ...)` will be created to compute $r(x)$ for scalar or vector inputs `x`. This function will take the model parameters $\\alpha_0$, $\\alpha_k$, $\\theta_k$ as arguments and will include a flag to handle the symmetry-breaking modification required for Test Case 2.\n\n2.  **Test Case 1** evaluates $r(x)$ for a set of real inputs using the exact conjugate-symmetric parameters. The test verifies that for every input $x_i$ in the set, the computed value $y_i = r(x_i)$ is real to machine precision, i.e., $|\\operatorname{Im}(y_i)| \\le \\tau(y_i)$. The final result is the logical AND of all these individual checks.\n\n3.  **Test Case 2** introduces a small perturbation to one of the conjugate residues, $\\overline{\\alpha_1}$, breaking the perfect symmetry. The test is designed to detect this failure. It is expected that the imaginary part of $r(x)$ will become significant. The test passes if the maximum absolute value of the imaginary part, over all inputs, exceeds a fixed small threshold of $10^3 \\epsilon$.\n\n4.  **Test Case 3** is similar to Test Case 1 but uses a different set of inputs, including zero and values with very large magnitudes, to test the robustness of the property at boundary conditions. The success criterion is the same as in Test Case 1.\n\n5.  **Test Case 4** applies the rational function to the eigenvalues of a diagonal matrix. In the context of matrix functions, $r(A)$ for a diagonalizable matrix $A=PDP^{-1}$ is $PD_rP^{-1}$ where $D_r$ is the diagonal matrix with elements $r(\\lambda_i)$ for each eigenvalue $\\lambda_i$. For a diagonal matrix $A$, this simplifies to element-wise evaluation on its diagonal entries. The success criterion is again the same as in Test Case 1, applied to the results for each diagonal element.\n\nThe final program will systematically execute these four tests using the provided parameters and report a boolean result for each, encapsulating the entire logic in a self-contained script.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Executes the four test cases for verifying the real-valued property\n    of a CRAM rational approximation.\n    \"\"\"\n    \n    # Define machine epsilon for double precision\n    epsilon = np.finfo(np.float64).eps\n\n    # Define the base parameters for the rational function\n    alpha_0 = 0.1\n    m = 6\n    # Poles (theta_k) and residues (alpha_k) for k=1,...,6\n    thetas = np.array([\n        -0.1 + 0.3j,\n        -0.5 + 0.8j,\n        -1.2 + 1.5j,\n        -3.0 + 2.7j,\n        -5.5 + 3.8j,\n        -8.0 + 5.0j\n    ], dtype=np.complex128)\n    \n    alphas = np.array([\n        0.2 + 0.05j,\n        0.1 + 0.2j,\n        0.05 + 0.3j,\n        0.02 + 0.25j,\n        0.01 + 0.15j,\n        0.005 + 0.1j\n    ], dtype=np.complex128)\n\n    def evaluate_r(x, a0, ak_list, thk_list, break_symmetry=False, delta=0.0):\n        \"\"\"\n        Evaluates the rational function r(x).\n        \n        Args:\n            x (np.ndarray): Real-valued inputs.\n            a0 (float): Real offset term.\n            ak_list (np.ndarray): Complex residues alpha_k.\n            thk_list (np.ndarray): Complex poles theta_k.\n            break_symmetry (bool): If True, breaks symmetry for test case 2.\n            delta (float): Perturbation for test case 2.\n\n        Returns:\n            np.ndarray: Complex-valued results r(x).\n        \"\"\"\n        x = np.asarray(x, dtype=np.float64)\n        total_sum = np.zeros_like(x, dtype=np.complex128)\n        \n        ak_conj_list = np.conj(ak_list)\n        if break_symmetry:\n            ak_conj_list[0] += 1j * delta\n            \n        thk_conj_list = np.conj(thk_list)\n        \n        # Vectorized calculation for efficiency\n        # Reshape x to (N, 1) to broadcast against (M,) arrays of poles/residues\n        x_reshaped = x[:, np.newaxis]\n        \n        # Calculate sum of terms for primary poles and residues\n        term1_sum = np.sum(ak_list / (x_reshaped - thk_list), axis=1)\n        \n        # Calculate sum of terms for conjugate poles and residues\n        term2_sum = np.sum(ak_conj_list / (x_reshaped - thk_conj_list), axis=1)\n        \n        return a0 + term1_sum + term2_sum\n\n    def tolerance(y):\n        \"\"\"Calculates the tolerance tau(y).\"\"\"\n        return 1e3 * epsilon * (1.0 + np.abs(y))\n\n    results = []\n\n    # Test Case 1: Exact conjugate symmetry on real inputs\n    x1 = np.array([-1e-12, -1e-6, -1e-3, -0.1, -1.0, -10.0, -1e3])\n    y1 = evaluate_r(x1, alpha_0, alphas, thetas)\n    tau1 = tolerance(y1)\n    result1 = np.all(np.abs(y1.imag) = tau1)\n    results.append(result1)\n\n    # Test Case 2: Broken conjugate symmetry detection\n    delta_pert = 1e-6\n    x2 = x1  # Same inputs as Test Case 1\n    y2 = evaluate_r(x2, alpha_0, alphas, thetas, break_symmetry=True, delta=delta_pert)\n    result2 = np.max(np.abs(y2.imag))  1e3 * epsilon\n    results.append(result2)\n\n    # Test Case 3: Boundary inputs including zero and very large magnitude\n    x3 = np.array([0.0, -1e-9, -1e-3, -1.0, -1e6, -1e12])\n    y3 = evaluate_r(x3, alpha_0, alphas, thetas)\n    tau3 = tolerance(y3)\n    result3 = np.all(np.abs(y3.imag) = tau3)\n    results.append(result3)\n\n    # Test Case 4: Diagonal matrix evaluation\n    x4 = np.array([-1e-3, -0.2, -1.5, -10.0, -100.0])\n    y4 = evaluate_r(x4, alpha_0, alphas, thetas)\n    tau4 = tolerance(y4)\n    result4 = np.all(np.abs(y4.imag) = tau4)\n    results.append(result4)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [bool(r) for r in results]))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In large-scale simulations, computational efficiency is often as critical as accuracy. Higher-order CRAM approximations offer greater precision but at a higher computational cost, presenting a classic engineering trade-off. This advanced exercise  places you in the role of an algorithm designer, tasking you with creating an adaptive scheme that intelligently selects the most cost-effective CRAM order based on the system's properties, a key skill for developing efficient and robust simulation software.",
            "id": "4216710",
            "problem": "You are asked to design and implement an adaptive selection scheme that decides, for a given linear operator represented by a real square matrix, whether to use the Chebyshev Rational Approximation Method (CRAM) of order $16$ (CRAM-16) or order $32$ (CRAM-32) to approximate the matrix exponential applied to a state vector in a nuclear reactor simulation context. The selection must be based on a principled, computationally inexpensive error proxy that does not require computing either CRAM-16 or CRAM-32 explicitly.\n\nThe physical and mathematical context is as follows. In many reactor physics models, the time evolution of a state vector $y(t)$ (for example, nuclide densities or multigroup neutron fluxes) is governed by a stiff linear system of ordinary differential equations\n$$\n\\frac{d}{dt} y(t) \\;=\\; -A\\,y(t),\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is a matrix whose entries carry physical rates in $\\mathrm{s}^{-1}$ and $t$ is time in seconds. The exact solution at time $t$ is\n$$\ny(t) \\;=\\; \\exp\\big(-\\,A\\,t\\big)\\,y(0).\n$$\nThe Chebyshev Rational Approximation Method (CRAM) approximates the scalar function $f(z) = \\exp(-z)$ on the positive real axis by a rational function, and then lifts this approximation to matrices via the functional calculus to approximate $\\exp(-A t)$ efficiently and stably for stiff $A$. Higher-order CRAM (such as CRAM-32) typically yields smaller uniform approximation error over the positive real axis than lower-order CRAM (such as CRAM-16), but at higher computational cost. The goal here is to select between CRAM-16 and CRAM-32 before any CRAM computation, using a cheap proxy derived from the spectrum of $tA$.\n\nYour task is to implement the following:\n\n1. Define an error proxy that upper bounds the spread of the spectrum of the scaled operator $tA$ on the positive real axis. You may use any of the following well-tested and inexpensive bounds:\n   - A Gershgorin-disc-based bound:\n     $$\n     \\mu_G \\;=\\; t \\cdot \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} \\big|a_{ij}\\big|,\n     $$\n     where $a_{ij}$ is the $(i,j)$ element of $A$.\n   - A matrix norm bound using an operator norm:\n     $$\n     \\mu_1 \\;=\\; t \\cdot \\lVert A \\rVert_{1}, \\quad \\mu_{\\infty} \\;=\\; t \\cdot \\lVert A \\rVert_{\\infty},\n     $$\n     where $\\lVert \\cdot \\rVert_{1}$ is the maximum absolute column sum and $\\lVert \\cdot \\rVert_{\\infty}$ is the maximum absolute row sum.\n   - An optional refinement via a few steps of power iteration to estimate the spectral radius of $tA$ in the $2$-norm sense:\n     $$\n     \\mu_{2,\\mathrm{est}} \\;\\approx\\; \\frac{\\lVert (tA) v \\rVert_{2}}{\\lVert v \\rVert_{2}},\n     $$\n     for a nonzero vector $v$ updated by repeated multiplication by $tA$, where $\\lVert \\cdot \\rVert_{2}$ is the Euclidean norm.\n\n2. Using these proxies, define thresholds $\\tau_{\\mathrm{low}}$ and $\\tau_{\\mathrm{high}}$ based on the following principle: when the spectrum of $tA$ is tightly clustered near the origin on the positive real axis (small proxy value), CRAM-16 is adequate; when the spectrum extends farther along the positive real axis (large proxy value), CRAM-32 is preferred. For ambiguous proxy values between the thresholds, use the optional power iteration refinement to make a final decision.\n\n3. Implement a function that, given a real matrix $A$ with entries in $\\mathrm{s}^{-1}$ and a time step $t$ in seconds, returns the integer $16$ or $32$ according to the above adaptive rule.\n\nYour program must run as-is and produce results for the following test suite. For each test case, you are given a matrix $A$ and a time $t$:\n- Test case $1$ (happy path, weakly coupled slow rates): \n  $$\n  A_1 \\;=\\; \\begin{bmatrix}\n  0.10  -0.02  0.00 \\\\\n  0.02  0.07  -0.01 \\\\\n  0.00  0.01  0.03\n  \\end{bmatrix}, \\quad t_1 \\;=\\; 0.10 \\text{ s}.\n  $$\n- Test case $2$ (stiff system, fast removal rates):\n  $$\n  A_2 \\;=\\; \\begin{bmatrix}\n  10^{5}  -10^{3}  0  0 \\\\\n  10^{3}  10^{5}  -10^{3}  0 \\\\\n  0  10^{3}  10^{5}  -10^{3} \\\\\n  0  0  10^{3}  10^{5}\n  \\end{bmatrix}, \\quad t_2 \\;=\\; 10^{-3} \\text{ s}.\n  $$\n- Test case $3$ (near-identity step, very small time):\n  $$\n  A_3 \\;=\\; \\begin{bmatrix}\n  0.50  -0.20  0.10  0.00  0.00 \\\\\n  0.10  0.40  -0.15  0.05  0.00 \\\\\n  0.00  0.10  0.60  -0.25  0.05 \\\\\n  0.00  0.00  0.10  0.30  -0.10 \\\\\n  0.00  0.00  0.00  0.05  0.20\n  \\end{bmatrix}, \\quad t_3 \\;=\\; 10^{-8} \\text{ s}.\n  $$\n- Test case $4$ (non-normal coupling, moderate-to-large effective scale):\n  $$\n  A_4 \\;=\\; \\begin{bmatrix}\n  10  -50  0 \\\\\n  0  10  -50 \\\\\n  0  0  10\n  \\end{bmatrix}, \\quad t_4 \\;=\\; 0.60 \\text{ s}.\n  $$\n- Test case $5$ (borderline coupling and time):\n  $$\n  A_5 \\;=\\; \\begin{bmatrix}\n  5  -1  0  0  0 \\\\\n  1  5  -1  0  0 \\\\\n  0  1  5  -1  0 \\\\\n  0  0  1  5  -1 \\\\\n  0  0  0  1  5\n  \\end{bmatrix}, \\quad t_5 \\;=\\; 1.50 \\text{ s}.\n  $$\n\nAll matrix entries $a_{ij}$ are in $\\mathrm{s}^{-1}$, and all times $t$ are in $\\mathrm{s}$. You must use radians for any angular quantities should they occur, but no angles appear in these test cases.\n\nYour program should produce a single line of output containing the selected orders for the five test cases as a comma-separated list enclosed in square brackets, for example, \n$$\n[\\dots]\n$$\nwhere each entry is the integer $16$ or $32$ for the corresponding test case. The output must be exactly in this format with no additional text.",
            "solution": "The solution is built from first principles connecting the underlying physics, the matrix exponential, rational approximation theory, and inexpensive spectral bounding techniques.\n\n1. Start from the stiff linear initial value problem\n$$\n\\frac{d}{dt} y(t) \\;=\\; -A\\,y(t),\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is a real matrix with entries in $\\mathrm{s}^{-1}$. The exact solution is\n$$\ny(t) \\;=\\; \\exp\\big(-A\\,t\\big)\\,y(0).\n$$\nWhen $A$ is stiff (large eigenvalues or strong non-normal couplings), directly computing the matrix exponential may be expensive or unstable if not handled carefully.\n\n2. The Chebyshev Rational Approximation Method (CRAM) constructs a rational function $R_n(z)$ that uniformly approximates $f(z) = \\exp(-z)$ on the positive real axis. CRAM of order $n$ (for instance $n = 16$ or $n = 32$) uses a partial fraction decomposition of $R_n(z)$, with complex-conjugate pole-residue pairs, to compute $R_n(tA)$ efficiently via solving shifted linear systems of the form\n$$\n\\left( tA - \\theta_k I \\right) w_k \\;=\\; y(0).\n$$\nHigher $n$ generally yields a smaller uniform approximation error\n$$\n\\varepsilon_n \\;=\\; \\sup_{x \\in [0,\\infty)} \\big| \\exp(-x) - R_n(x) \\big|,\n$$\nwith $\\varepsilon_{32} \\ll \\varepsilon_{16}$, at the cost of more linear solves.\n\n3. To select an order adaptively before any CRAM computation, we use spectral proxies that bound the spread of the spectrum $\\sigma(tA)$ on the positive real axis. The matrix functional calculus implies that the error of $R_n(tA)$ satisfies\n$$\n\\big\\| \\exp(-tA) - R_n(tA) \\big\\| \\;\\le\\; \\sup_{\\lambda \\in \\sigma(tA)} \\big| \\exp(-\\lambda) - R_n(\\lambda) \\big| \\;\\le\\; \\sup_{x \\in [0, \\mu]} \\big| \\exp(-x) - R_n(x) \\big|,\n$$\nwhere $\\mu$ is any upper bound on the real part location of $\\sigma(tA)$ on the positive real axis. Therefore, if $\\mu$ is small, a lower-order CRAM can achieve a desired error level; if $\\mu$ is large, a higher-order CRAM is preferable.\n\n4. We obtain $\\mu$ by inexpensive, conservative bounds:\n   - Gershgorin circle theorem yields that every eigenvalue $\\lambda$ of $A$ lies in at least one disc centered at $a_{ii}$ with radius $R_i = \\sum_{j \\ne i} |a_{ij}|$. A simple bound for nonnegative scale on the positive real axis is\n     $$\n     \\mu_G \\;=\\; t \\cdot \\max_{i} \\sum_{j} |a_{ij}|,\n     $$\n     which equals $t$ times the maximum absolute row sum. This captures both diagonal and coupling strengths.\n   - Operator norms give\n     $$\n     \\mu_1 \\;=\\; t \\lVert A \\rVert_1, \\quad \\mu_{\\infty} \\;=\\; t \\lVert A \\rVert_{\\infty},\n     $$\n     both cheap to compute. For convenience and conservatism, one can use $\\mu \\;=\\; \\max\\{\\mu_G, \\mu_1, \\mu_{\\infty}\\}$ or simply $\\mu_G$ (since $\\mu_G = t\\lVert A \\rVert_{\\infty}$).\n   - In an ambiguous regime where $\\mu$ is neither small nor large, a few iterations of power iteration refine the estimate in the $2$-norm sense:\n     $$\n     v_{k+1} \\;=\\; \\frac{tA\\,v_k}{\\lVert tA\\,v_k \\rVert_2}, \\quad \\mu_{2,\\mathrm{est}} \\;\\approx\\; \\frac{\\lVert tA\\,v_k \\rVert_2}{\\lVert v_k \\rVert_2}.\n     $$\n     This estimates the operator norm in the $2$-norm, informative for non-normal matrices.\n\n5. Decision logic. Select thresholds $\\tau_{\\mathrm{low}}$ and $\\tau_{\\mathrm{high}}$ to reflect typical uniform error breakpoints for CRAM-16 and CRAM-32. The precise values depend on published minimax error curves; here we set conservative values to capture the intent: if $\\mu \\le \\tau_{\\mathrm{low}}$, select CRAM-16; if $\\mu \\ge \\tau_{\\mathrm{high}}$, select CRAM-32; and if $\\tau_{\\mathrm{low}}  \\mu  \\tau_{\\mathrm{high}}$, use the refined estimate $\\mu_{2,\\mathrm{est}}$ and compare it to a middle threshold to decide. This scheme is computationally trivial compared to actually forming CRAM-32 and robust across a range of stiffness and non-normality.\n\n6. Test suite coverage:\n   - Test case $1$ is a weakly coupled, slow-rate system with $t$ small, so $\\mu$ is very small and CRAM-16 is adequate.\n   - Test case $2$ has very large removal rates with small $t$, but $t\\lVert A \\rVert_{\\infty}$ is large enough to favor CRAM-32 for accuracy.\n   - Test case $3$ uses an extremely small time, pushing the step close to the identity, favoring CRAM-16.\n   - Test case $4$ is non-normal with strong off-diagonal couplings and moderate time, yielding a large $\\mu$ and favoring CRAM-32.\n   - Test case $5$ sits near the lower threshold, chosen to favor CRAM-16 by the conservative bound.\n\n7. Implementation details. Compute $\\mu_G = t \\max_i \\sum_j |a_{ij}|$ and compare to $\\tau_{\\mathrm{low}}$ and $\\tau_{\\mathrm{high}}$. If needed, run a few (e.g., $20$) power iterations to estimate $\\mu_{2,\\mathrm{est}}$ and compare to a midpoint threshold. The final output for each test case is the integer $16$ or $32$. No physical quantity beyond matrix entries in $\\mathrm{s}^{-1}$ and time in $\\mathrm{s}$ is printed; thus, the output is dimensionless integers.\n\nThis principle-based selection leverages tightly connected foundations: the role of $\\exp(-tA)$ in stiff linear reactor models, uniform rational approximation error of CRAM on the positive real axis, and classical spectral bounds (Gershgorin and operator norms) to decide the appropriate approximation order with negligible computational overhead.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gershgorin_row_sum_bound(A: np.ndarray) - float:\n    \"\"\"\n    Compute the maximum absolute row sum of A, which equals the infinity norm ||A||_inf.\n    This serves as a Gershgorin-based bound on the spectral spread relevant to the positive real axis.\n    \"\"\"\n    return np.max(np.sum(np.abs(A), axis=1))\n\ndef power_iteration_norm_estimate(M: np.ndarray, iterations: int = 20, seed: int = 0) - float:\n    \"\"\"\n    Estimate the operator 2-norm of matrix M using power iteration on M^T M implicitly\n    by repeatedly applying M to a vector and measuring amplification.\n    For non-normal matrices this is an estimate; it is inexpensive and often informative.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    v = rng.standard_normal(M.shape[1])\n    v_norm = np.linalg.norm(v)\n    if v_norm == 0.0:\n        v = np.ones(M.shape[1])\n        v_norm = np.linalg.norm(v)\n    v = v / v_norm\n    for _ in range(iterations):\n        w = M @ v\n        w_norm = np.linalg.norm(w)\n        if w_norm == 0.0:\n            # M annihilates v; return zero estimate\n            return 0.0\n        v = w / w_norm\n    # Final amplification factor of the last step as a proxy\n    return np.linalg.norm(M @ v) / np.linalg.norm(v)\n\ndef choose_cram_order(A: np.ndarray, t: float,\n                      tau_low: float = 12.0,\n                      tau_high: float = 30.0,\n                      mid_factor: float = 20.0) - int:\n    \"\"\"\n    Adaptive selection between CRAM-16 and CRAM-32 using cheap spectral proxies.\n\n    Parameters:\n        A: Real square matrix with entries in s^{-1}.\n        t: Time step in seconds.\n        tau_low: Lower threshold for proxy deciding CRAM-16.\n        tau_high: Upper threshold for proxy deciding CRAM-32.\n        mid_factor: Midpoint threshold used with power iteration when proxy is ambiguous.\n\n    Returns:\n        16 or 32 indicating selected CRAM order.\n    \"\"\"\n    # Cheap Gershgorin (infinity norm) bound scaled by t\n    mu_g = t * gershgorin_row_sum_bound(A)\n\n    if mu_g = tau_low:\n        return 16\n    if mu_g = tau_high:\n        return 32\n\n    # Ambiguous region: refine using a few steps of power iteration on t*A\n    M = t * A\n    mu2_est = power_iteration_norm_estimate(M, iterations=20, seed=42)\n\n    # Decide using mid_factor as a middle threshold\n    if mu2_est = mid_factor:\n        return 16\n    else:\n        return 32\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Each case is (A, t)\n        (\n            np.array([\n                [0.10, -0.02, 0.00],\n                [0.02,  0.07, -0.01],\n                [0.00,  0.01,  0.03]\n            ], dtype=float),\n            0.10\n        ),\n        (\n            np.array([\n                [1e5, -1e3, 0.0, 0.0],\n                [1e3,  1e5, -1e3, 0.0],\n                [0.0,  1e3,  1e5, -1e3],\n                [0.0,  0.0,  1e3,  1e5]\n            ], dtype=float),\n            1e-3\n        ),\n        (\n            np.array([\n                [0.50, -0.20,  0.10,  0.00,  0.00],\n                [0.10,  0.40, -0.15,  0.05,  0.00],\n                [0.00,  0.10,  0.60, -0.25,  0.05],\n                [0.00,  0.00,  0.10,  0.30, -0.10],\n                [0.00,  0.00,  0.00,  0.05,  0.20]\n            ], dtype=float),\n            1e-8\n        ),\n        (\n            np.array([\n                [10.0, -50.0,   0.0],\n                [ 0.0,  10.0, -50.0],\n                [ 0.0,   0.0,  10.0]\n            ], dtype=float),\n            0.60\n        ),\n        (\n            np.array([\n                [5.0, -1.0,  0.0,  0.0,  0.0],\n                [1.0,  5.0, -1.0,  0.0,  0.0],\n                [0.0,  1.0,  5.0, -1.0,  0.0],\n                [0.0,  0.0,  1.0,  5.0, -1.0],\n                [0.0,  0.0,  0.0,  1.0,  5.0]\n            ], dtype=float),\n            1.50\n        )\n    ]\n\n    results = []\n    for A, t in test_cases:\n        order = choose_cram_order(A, t,\n                                  tau_low=12.0,\n                                  tau_high=30.0,\n                                  mid_factor=20.0)\n        results.append(order)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}