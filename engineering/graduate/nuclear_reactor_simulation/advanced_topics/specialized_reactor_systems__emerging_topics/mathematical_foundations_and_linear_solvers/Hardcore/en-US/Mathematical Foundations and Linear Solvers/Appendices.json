{
    "hands_on_practices": [
        {
            "introduction": "The Conjugate Gradient (CG) method is a cornerstone of scientific computing, prized for its efficiency in solving the large symmetric positive-definite (SPD) linear systems that arise from discretizing diffusion equations. Rather than simply memorizing the algorithm, deriving it from first principles provides a much deeper understanding of its power. This practice guides you through the derivation of the CG recurrence relations by minimizing a quadratic energy functional, revealing the origin of A-conjugate search directions and the reason for its monotonic convergence in the energy norm .",
            "id": "4234482",
            "problem": "Consider a steady-state, one-energy-group neutron diffusion model in a one-dimensional slab of length $L$ with homogeneous Dirichlet boundary conditions, where the scalar flux $\\phi(x)$ satisfies the differential equation $- \\frac{\\mathrm{d}}{\\mathrm{d}x}\\!\\left(D \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}\\right) + \\Sigma_{a}\\,\\phi = S(x)$. Assume constant diffusion coefficient $D0$, positive macroscopic absorption cross section $\\Sigma_{a}0$, and a spatially uniform source $S(x)=S_{0}$. Under these conditions, the diffusion operator is self-adjoint and positive definite on the appropriate Hilbert space with the induced energy inner product. A second-order finite-volume discretization on a uniform two-cell partition of the slab yields a linear system $A x = b$ for the cell-averaged flux vector $x \\in \\mathbb{R}^{2}$, where $A \\in \\mathbb{R}^{2\\times 2}$ is symmetric positive definite (SPD) and $b \\in \\mathbb{R}^{2}$ is the discrete source.\n\nDefine the $A$-inner product $\\langle x, y \\rangle_{A} = x^{T} A y$ and the induced $A$-norm $\\|x\\|_{A} = \\sqrt{x^{T} A x}$. Starting from the quadratic functional $J(x) = \\frac{1}{2}\\, x^{T} A x - x^{T} b$ and the fact that $A$ is SPD, derive from first principles the Conjugate Gradient (CG) recurrence that ensures that the search directions are $A$-conjugate and that the error decreases monotonically in the $A$-norm at each iteration. Specifically, justify the line-search step and the recurrence coefficients required to enforce $A$-conjugacy and explain why the $A$-norm of the error strictly decreases unless the exact solution is attained.\n\nThen, consider a physically consistent two-cell discretization with uniform cell width $h$, constant $D$ and $\\Sigma_{a}$, and zero Dirichlet boundary conditions, leading to the SPD matrix\n$$\nA = \\begin{pmatrix}\n\\frac{2D}{h} + \\Sigma_{a} h  -\\frac{D}{h} \\\\\n-\\frac{D}{h}  \\frac{2D}{h} + \\Sigma_{a} h\n\\end{pmatrix},\n$$\nwith a uniform discrete source vector $b = \\begin{pmatrix} S_{0}\\, h \\\\ S_{0}\\, h \\end{pmatrix}.$\nFor the parameter values $D = 1$, $\\Sigma_{a} = 0.2$, $h = 1$, and $S_{0} = 1$, take the initial guess $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and perform one CG iteration using your derived recurrence. Compute the contraction factor of the squared $A$-norm of the error after the first iteration, namely the ratio\n$$\n\\frac{\\|e_{1}\\|_{A}^{2}}{\\|e_{0}\\|_{A}^{2}},\n$$\nwhere $e_{k} = x^{\\ast} - x_{k}$ and $x^{\\ast}$ solves $A x^{\\ast} = b$. Express your final numerical answer as a single real number and round your answer to four significant figures.",
            "solution": "The steady-state, one-energy-group neutron diffusion operator $-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\!\\left(D \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}\\right) + \\Sigma_{a}\\,\\phi$ with homogeneous Dirichlet boundary conditions is self-adjoint with respect to the energy inner product and coercive for $D0$ and $\\Sigma_{a}0$. A standard second-order finite-volume discretization on a uniform grid yields an SPD matrix $A$ consistent with this operator; the discrete energy is $J(x) = \\frac{1}{2} x^{T} A x - x^{T} b$, whose unique minimizer is the solution $x^{\\ast}$ of $A x^{\\ast} = b$.\n\nWe define the $A$-inner product and induced norm by $\\langle x, y \\rangle_{A} = x^{T} A y$ and $\\|x\\|_{A} = \\sqrt{x^{T} A x}$. The gradient of $J$ at $x$ is $\\nabla J(x) = A x - b = -r$, where $r = b - A x$ is the residual. Consider an iterative method $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$ with a search direction $p_{k}$. To ensure a decrease of $J$ and, equivalently, of $\\|e_{k}\\|_{A}$, we perform an exact line search along $p_{k}$:\n$$\n\\alpha_{k} = \\arg\\min_{\\alpha \\in \\mathbb{R}} J(x_{k} + \\alpha p_{k}).\n$$\nExpanding $J(x_{k} + \\alpha p_{k})$ gives\n$$\nJ(x_{k} + \\alpha p_{k}) = \\frac{1}{2} (x_{k} + \\alpha p_{k})^{T} A (x_{k} + \\alpha p_{k}) - (x_{k} + \\alpha p_{k})^{T} b,\n$$\nwhose derivative with respect to $\\alpha$ is\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\alpha} J(x_{k} + \\alpha p_{k}) = p_{k}^{T} A x_{k} + \\alpha p_{k}^{T} A p_{k} - p_{k}^{T} b = -p_{k}^{T} r_{k} + \\alpha p_{k}^{T} A p_{k}.\n$$\nSetting the derivative to zero yields the optimal step length\n$$\n\\alpha_{k} = \\frac{p_{k}^{T} r_{k}}{p_{k}^{T} A p_{k}}.\n$$\nA natural initial choice is $p_{0} = r_{0}$, and the subsequent directions are constructed to be $A$-conjugate, that is, $\\langle p_{i}, p_{j} \\rangle_{A} = p_{i}^{T} A p_{j} = 0$ for $i \\neq j$. We consider a recurrence of the form\n$$\np_{k+1} = r_{k+1} + \\beta_{k} p_{k},\n$$\nand choose $\\beta_{k}$ to enforce $A$-conjugacy with $p_{k}$. Imposing $p_{k+1}^{T} A p_{k} = 0$ gives\n$$\n(r_{k+1} + \\beta_{k} p_{k})^{T} A p_{k} = 0 \\quad \\Rightarrow \\quad r_{k+1}^{T} A p_{k} + \\beta_{k}\\, p_{k}^{T} A p_{k} = 0,\n$$\nwhich yields\n$$\n\\beta_{k} = -\\frac{r_{k+1}^{T} A p_{k}}{p_{k}^{T} A p_{k}}.\n$$\nTo obtain a computationally simpler formula, we exploit properties of the residuals. From the update $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$ and the residual $r_{k+1} = b - A x_{k+1}$, we have\n$$\nr_{k+1} = r_{k} - \\alpha_{k} A p_{k}.\n$$\nUsing the optimal $\\alpha_{k}$, one can show that successive residuals are orthogonal in the Euclidean inner product, namely $r_{k+1}^{T} r_{k} = 0$. This orthogonality, combined with the recurrence and the $A$-conjugacy requirement, leads to the classic Conjugate Gradient (CG) choice\n$$\n\\beta_{k} = \\frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}}.\n$$\nThus the CG recurrence is\n$$\n\\begin{aligned}\n\\text{Initialize } x_{0} \\text{ given, } r_{0} = b - A x_{0}, \\; p_{0} = r_{0}. \\\\\n\\alpha_{k} = \\frac{r_{k}^{T} p_{k}}{p_{k}^{T} A p_{k}}, \\quad x_{k+1} = x_{k} + \\alpha_{k} p_{k}, \\quad r_{k+1} = r_{k} - \\alpha_{k} A p_{k}, \\\\\n\\beta_{k} = \\frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}}, \\quad p_{k+1} = r_{k+1} + \\beta_{k} p_{k}.\n\\end{aligned}\n$$\nThe $A$-conjugacy $p_{i}^{T} A p_{j} = 0$ for $i \\neq j$ ensures that each step minimizes $J$ over the affine space $x_{0} + \\operatorname{span}\\{p_{0}, \\dots, p_{k}\\}$, and the exact line search guarantees that $J(x_{k+1})  J(x_{k})$ unless $r_{k} = 0$. Since $J(x) - J(x^{\\ast}) = \\frac{1}{2} \\|x - x^{\\ast}\\|_{A}^{2}$, monotonic decrease of $J$ implies monotonic decrease of $\\|e_{k}\\|_{A}$.\n\nWe now compute the requested contraction factor for the specified two-cell discretization. With $D = 1$, $\\Sigma_{a} = 0.2$, $h = 1$, the SPD matrix and source are\n$$\nA = \\begin{pmatrix} 2.2  -1 \\\\ -1  2.2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nWe take $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The exact solution is $x^{\\ast} = A^{-1} b$, and the initial error is $e_{0} = x^{\\ast} - x_{0} = x^{\\ast}$. Because $A$ is $2 \\times 2$ SPD, we can compute $A^{-1}$ explicitly. The determinant is $\\det(A) = 2.2 \\cdot 2.2 - (-1)\\cdot(-1) = 4.84 - 1 = 3.84$, and\n$$\nA^{-1} = \\frac{1}{3.84} \\begin{pmatrix} 2.2  1 \\\\ 1  2.2 \\end{pmatrix}.\n$$\nTherefore,\n$$\ne_{0} = x^{\\ast} = A^{-1} b = \\frac{1}{3.84} \\begin{pmatrix} 3.2 \\\\ 3.2 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6} \\\\ \\frac{5}{6} \\end{pmatrix}.\n$$\nWe perform one CG step. With $x_{0} = 0$, we have $r_{0} = b - A x_{0} = b$, and we choose $p_{0} = r_{0} = b$. The optimal step length is\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} p_{0}}{p_{0}^{T} A p_{0}} = \\frac{b^{T} b}{b^{T} A b}.\n$$\nCompute the necessary scalars:\n$$\nb^{T} b = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2,\n$$\n$$\nA b = \\begin{pmatrix} 2.2  -1 \\\\ -1  2.2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1.2 \\\\ 1.2 \\end{pmatrix}, \\quad b^{T} A b = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1.2 \\\\ 1.2 \\end{pmatrix} = 2.4.\n$$\nThus\n$$\n\\alpha_{0} = \\frac{2}{2.4} = \\frac{5}{6}.\n$$\nThe new iterate is $x_{1} = x_{0} + \\alpha_{0} p_{0} = \\alpha_{0} b = \\begin{pmatrix} \\frac{5}{6} \\\\ \\frac{5}{6} \\end{pmatrix}$, which coincides with $x^{\\ast}$ computed above. Therefore the new error is\n$$\ne_{1} = x^{\\ast} - x_{1} = 0.\n$$\nConsequently, the contraction factor of the squared $A$-norm of the error after the first iteration is\n$$\n\\frac{\\|e_{1}\\|_{A}^{2}}{\\|e_{0}\\|_{A}^{2}} = \\frac{0}{\\|e_{0}\\|_{A}^{2}} = 0.\n$$\nFor completeness, one can also express this ratio symbolically in terms of $s_{1} = e_{0}^{T} A e_{0}$, $s_{2} = e_{0}^{T} A^{2} e_{0} = r_{0}^{T} r_{0}$, and $s_{3} = e_{0}^{T} A^{3} e_{0} = r_{0}^{T} A r_{0}$, using $p_{0} = r_{0}$ and $\\alpha_{0} = s_{2}/s_{3}$:\n$$\n\\frac{\\|e_{1}\\|_{A}^{2}}{\\|e_{0}\\|_{A}^{2}} = 1 - \\frac{s_{2}^{2}}{s_{1} s_{3}},\n$$\nwhich evaluates here to $1 - \\frac{4}{\\left(\\frac{5}{3}\\right) \\cdot 2.4} = 1 - \\frac{4}{4} = 0$. Rounded to four significant figures, the numeric value remains $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "While powerful, the convergence rate of iterative solvers like CG can be slow for ill-conditioned systems, which are common in reactor physics problems with heterogeneous materials. Preconditioning is the essential technique for accelerating these solvers by transforming the system into one with more favorable spectral properties. This exercise provides a clear and tangible introduction to this concept by analyzing a simple two-region diffusion problem, allowing you to analytically compute the eigenvalues of the system matrix before and after applying a basic Jacobi preconditioner to see the improvement directly .",
            "id": "4234539",
            "problem": "Consider a steady-state, one-group neutron diffusion model for a one-dimensional slab consisting of two homogeneous regions with reflective boundaries at both ends. Let the left region have thickness $h_1$, diffusion coefficient $D_1$, and macroscopic absorption cross section $\\Sigma_{a,1}$, and the right region have thickness $h_2$, diffusion coefficient $D_2$, and macroscopic absorption cross section $\\Sigma_{a,2}$. The slab has constant cross-sectional area $A_c$. Using the finite volume formulation and the two-point flux approximation with Fick’s law for the interface current, the interface conductance $G$ between the two control volumes is given by the harmonic average derived from series resistances. Specifically, the current crossing the interface is approximated as $J \\approx -G\\left(\\phi_2 - \\phi_1\\right)$ with\n$$\nG \\;=\\; \\frac{2\\,A_c}{\\frac{h_1}{D_1} + \\frac{h_2}{D_2}}\n$$\nand there are no boundary leakage terms due to the reflective boundaries. The resulting symmetric positive definite (SPD) $2\\times 2$ discrete operator $\\mathbf{A}$ acting on the cell-average fluxes $(\\phi_1,\\phi_2)$ has entries\n$$\nA_{11} \\;=\\; \\Sigma_{a,1}\\,V_1 + G,\\quad A_{22} \\;=\\; \\Sigma_{a,2}\\,V_2 + G,\\quad A_{12} \\;=\\; A_{21} \\;=\\; -G,\n$$\nwhere $V_i = A_c\\,h_i$ are the control-volume measures.\n\nDefine the block Jacobi preconditioner $\\mathbf{M}$ as the block diagonal of $\\mathbf{A}$, i.e., $\\mathbf{M} = \\operatorname{diag}(A_{11},A_{22})$. Starting from the one-group neutron diffusion equation $-\\nabla\\cdot\\left(D\\nabla\\phi\\right) + \\Sigma_a\\,\\phi = q$ and the finite volume balance over each region, derive the preconditioned operator $\\mathbf{M}^{-1}\\mathbf{A}$ and compute its eigenvalues in exact form for the following physically consistent parameters:\n- $h_1 = 30\\,\\text{cm}$, $h_2 = 20\\,\\text{cm}$,\n- $A_c = 200\\,\\text{cm}^2$,\n- $D_1 = 1.2\\,\\text{cm}$, $D_2 = 0.8\\,\\text{cm}$,\n- $\\Sigma_{a,1} = 0.010\\,\\text{cm}^{-1}$, $\\Sigma_{a,2} = 0.015\\,\\text{cm}^{-1}$.\n\nIn your derivation, start from the governing equation and the definition of the two-point flux approximation and interface conductance. Then, construct $\\mathbf{A}$ and $\\mathbf{M}$, form $\\mathbf{M}^{-1}\\mathbf{A}$, and compute its eigenvalues analytically. For context, in your explanation, also obtain the eigenvalues of the unpreconditioned operator $\\mathbf{A}$ and discuss the clustering of the spectra qualitatively; however, the final answer to be reported is only the eigenvalues of $\\mathbf{M}^{-1}\\mathbf{A}$. No rounding is required; provide the exact values. Do not include units in the final answer.",
            "solution": "The problem asks for the eigenvalues of the Jacobi-preconditioned matrix $\\mathbf{M}^{-1}\\mathbf{A}$, derived from a finite volume discretization of the neutron diffusion equation.\n\nFirst, we integrate the governing equation, $-\\nabla\\cdot\\left(D\\nabla\\phi\\right) + \\Sigma_a\\,\\phi = q$, over each control volume $V_i$. Applying the divergence theorem to the leakage term gives:\n$$\n-\\int_{\\partial V_i} (D\\nabla\\phi) \\cdot d\\mathbf{S} + \\int_{V_i} \\Sigma_a \\phi \\,dV = \\int_{V_i} q \\,dV\n$$\nThe first term represents the net neutron current leaving the volume, and the second is the total absorption rate within the volume. The right-hand side is the total source rate. For our two-volume slab system with reflective outer boundaries (zero current), the balance equations are:\n\nFor volume $V_1$:\n$$\n-I_{1\\to\\text{right}} + \\Sigma_{a,1}\\phi_1 V_1 = q_1 V_1\n$$\nFor volume $V_2$:\n$$\n-I_{2\\to\\text{left}} + \\Sigma_{a,2}\\phi_2 V_2 = q_2 V_2\n$$\nwhere $I$ represents the total current (current density integrated over area $A_c$). The currents are related by $I_{2\\to\\text{left}} = -I_{1\\to\\text{right}}$. Let's define the net current from cell $1$ to cell $2$ as $J_{12} = I_{1\\to\\text{right}}$. The problem provides the approximation $J_{12} = -G(\\phi_2 - \\phi_1) = G(\\phi_1 - \\phi_2)$. Substituting this into the balance equations:\n\nFor $V_1$: $G(\\phi_1 - \\phi_2) + \\Sigma_{a,1}V_1\\phi_1 = q_1 V_1 \\implies (\\Sigma_{a,1}V_1 + G)\\phi_1 - G\\phi_2 = q_1 V_1$\nFor $V_2$: $-G(\\phi_1 - \\phi_2) + \\Sigma_{a,2}V_2\\phi_2 = q_2 V_2 \\implies -G\\phi_1 + (\\Sigma_{a,2}V_2 + G)\\phi_2 = q_2 V_2$\n\nThese two equations form the linear system $\\mathbf{A}\\boldsymbol{\\phi} = \\mathbf{q}$, where $\\boldsymbol{\\phi} = \\begin{pmatrix} \\phi_1  \\phi_2 \\end{pmatrix}^T$, and the matrix $\\mathbf{A}$ is given by:\n$$\n\\mathbf{A} = \\begin{pmatrix} \\Sigma_{a,1}V_1 + G  -G \\\\ -G  \\Sigma_{a,2}V_2 + G \\end{pmatrix}\n$$\nThis matches the structure provided in the problem statement.\n\nNext, we compute the numerical values of the components of $\\mathbf{A}$.\nThe control volumes are:\n$V_1 = A_c h_1 = (200\\,\\text{cm}^2)(30\\,\\text{cm}) = 6000\\,\\text{cm}^3$\n$V_2 = A_c h_2 = (200\\,\\text{cm}^2)(20\\,\\text{cm}) = 4000\\,\\text{cm}^3$\n\nThe interface conductance $G$ is:\n$$\nG = \\frac{2\\,A_c}{\\frac{h_1}{D_1} + \\frac{h_2}{D_2}} = \\frac{2(200)}{\\frac{30}{1.2} + \\frac{20}{0.8}} = \\frac{400}{25 + 25} = \\frac{400}{50} = 8\\,\\text{cm}^2\n$$\n\nNow we find the entries of the matrix $\\mathbf{A}$:\n$A_{11} = \\Sigma_{a,1}V_1 + G = (0.010\\,\\text{cm}^{-1})(6000\\,\\text{cm}^3) + 8\\,\\text{cm}^2 = 60 + 8 = 68$\n$A_{22} = \\Sigma_{a,2}V_2 + G = (0.015\\,\\text{cm}^{-1})(4000\\,\\text{cm}^3) + 8\\,\\text{cm}^2 = 60 + 8 = 68$\n$A_{12} = A_{21} = -G = -8$\n\nSo, the operator matrix $\\mathbf{A}$ is:\n$$\n\\mathbf{A} = \\begin{pmatrix} 68  -8 \\\\ -8  68 \\end{pmatrix}\n$$\n\nFor context, we first compute the eigenvalues of the unpreconditioned matrix $\\mathbf{A}$. The characteristic equation is $\\det(\\mathbf{A} - \\lambda\\mathbf{I}) = 0$:\n$$\n\\det\\begin{pmatrix} 68 - \\lambda  -8 \\\\ -8  68 - \\lambda \\end{pmatrix} = (68 - \\lambda)^2 - (-8)^2 = 0\n$$\n$$\n(68 - \\lambda)^2 = 64 \\implies 68 - \\lambda = \\pm\\sqrt{64} = \\pm 8\n$$\n$$\n\\lambda = 68 \\mp 8\n$$\nThe eigenvalues of $\\mathbf{A}$ are $\\lambda_1(\\mathbf{A}) = 60$ and $\\lambda_2(\\mathbf{A}) = 76$. The spectrum of $\\mathbf{A}$ is $\\{60, 76\\}$.\n\nNow, we construct the Jacobi preconditioner $\\mathbf{M}$, which is the diagonal of $\\mathbf{A}$:\n$$\n\\mathbf{M} = \\operatorname{diag}(A_{11}, A_{22}) = \\begin{pmatrix} 68  0 \\\\ 0  68 \\end{pmatrix}\n$$\nIts inverse is:\n$$\n\\mathbf{M}^{-1} = \\begin{pmatrix} 1/68  0 \\\\ 0  1/68 \\end{pmatrix}\n$$\nThe preconditioned matrix is $\\mathbf{M}^{-1}\\mathbf{A}$:\n$$\n\\mathbf{M}^{-1}\\mathbf{A} = \\begin{pmatrix} 1/68  0 \\\\ 0  1/68 \\end{pmatrix} \\begin{pmatrix} 68  -8 \\\\ -8  68 \\end{pmatrix} = \\begin{pmatrix} \\frac{68}{68}  \\frac{-8}{68} \\\\ \\frac{-8}{68}  \\frac{68}{68} \\end{pmatrix} = \\begin{pmatrix} 1  -\\frac{2}{17} \\\\ -\\frac{2}{17}  1 \\end{pmatrix}\n$$\nFinally, we compute the eigenvalues of the preconditioned matrix $\\mathbf{M}^{-1}\\mathbf{A}$. Let these eigenvalues be $\\mu$. The characteristic equation is $\\det(\\mathbf{M}^{-1}\\mathbf{A} - \\mu\\mathbf{I}) = 0$:\n$$\n\\det\\begin{pmatrix} 1 - \\mu  -2/17 \\\\ -2/17  1 - \\mu \\end{pmatrix} = (1 - \\mu)^2 - \\left(-\\frac{2}{17}\\right)^2 = 0\n$$\n$$\n(1 - \\mu)^2 = \\left(\\frac{2}{17}\\right)^2 \\implies 1 - \\mu = \\pm \\frac{2}{17}\n$$\n$$\n\\mu = 1 \\mp \\frac{2}{17}\n$$\nThe eigenvalues of the preconditioned matrix $\\mathbf{M}^{-1}\\mathbf{A}$ are:\n$\\mu_1 = 1 - \\frac{2}{17} = \\frac{15}{17}$\n$\\mu_2 = 1 + \\frac{2}{17} = \\frac{19}{17}$\n\nQualitatively, the Jacobi preconditioning has clustered the eigenvalues tightly around $1$. The original spectrum of $\\mathbf{A}$ is $\\{60, 76\\}$, with a condition number $\\kappa(\\mathbf{A}) = 76/60 = 19/15$. The preconditioned spectrum is $\\{15/17, 19/17\\}$, which is approximately $\\{0.882, 1.118\\}$. The condition number of the preconditioned system is $\\kappa(\\mathbf{M}^{-1}\\mathbf{A}) = (19/17)/(15/17) = 19/15$. In this specific case, due to the diagonal entries of $\\mathbf{A}$ being equal ($A_{11}=A_{22}$), the preconditioner $\\mathbf{M}$ is a scalar multiple of the identity matrix, i.e., $\\mathbf{M} = 68\\mathbf{I}$. Consequently, the preconditioned matrix $\\mathbf{M}^{-1}\\mathbf{A}$ is simply a scaled version of $\\mathbf{A}$, and the eigenvectors are preserved while the eigenvalues are scaled by $1/68$. This scaling explains why the condition number is unchanged. However, the goal of preconditioning—clustering eigenvalues around $1$ to accelerate convergence of iterative solvers—is clearly achieved.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{15}{17}  \\frac{19}{17} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving beyond simple diagonal scaling, more sophisticated preconditioners are needed for challenging multi-dimensional problems. Incomplete LU (ILU) factorization offers a powerful and robust alternative that is often used as a \"smoother\" within advanced multigrid solvers. This practice delves into the mechanics of the simplest ILU variant, ILU(0), for a classic 2D diffusion problem, exploring its sparsity pattern and using Local Fourier Analysis (LFA) to rigorously quantify its effectiveness at damping high-frequency errors—the defining task of a good smoother .",
            "id": "4234460",
            "problem": "Consider the steady-state one-group neutron diffusion equation in a homogeneous, rectangular reactor core with isotropic scattering neglected and homogeneous Dirichlet boundary conditions, given by $-\\nabla \\cdot (D \\nabla \\phi) + \\Sigma_{a} \\phi = q$, where $D  0$ is the diffusion coefficient, $\\Sigma_{a} \\ge 0$ is the macroscopic absorption cross section, $\\phi$ is the neutron flux, and $q$ is a source term. Discretize this equation over a square domain using a cell-centered finite volume method on a uniform $n \\times n$ grid of interior cells with mesh spacing $h$, and natural lexicographic (row-major) ordering of unknowns. The resulting linear system $A \\phi = b$ has a symmetric positive definite five-point stencil, with diagonal entries $a_{pp} = \\Sigma_{a} + \\frac{4D}{h^{2}}$ and off-diagonal entries $a_{pq} = -\\frac{D}{h^{2}}$ for the four nearest neighbors $q$ of any interior point $p$.\n\nDefine the Incomplete Lower-Upper factorization with zero fill (ILU(0)) for this matrix $A$ under the given ordering, with the convention that the incomplete lower factor $L$ is unit lower triangular and its unit diagonal is not stored, while the incomplete upper factor $U$ contains the diagonal. Assume the usual restriction that ILU(0) permits nonzero entries only at positions where $A$ is nonzero.\n\nStarting from these foundations:\n1) Determine the sparsity (fill-in) pattern of the incomplete lower factor $L$ and the incomplete upper factor $U$ produced by ILU(0) for this five-point stencil under the given ordering.\n2) Compute the total number of stored nonzero entries in $L$ and $U$ combined, counting the diagonal of $U$ once and not storing the unit diagonal of $L$. Express your answer as a closed-form analytic expression in terms of $n$.\n3) Using Local Fourier Analysis (LFA) on the infinite periodic-grid surrogate for the same five-point operator, characterize the forward Gauss–Seidel iteration matrix constructed from the standard splitting $A = L_{A} + D_{A} + U_{A}$, where $L_{A}$ and $U_{A}$ are the strictly lower and strictly upper parts of $A$ in the same ordering and $D_{A}$ is the diagonal of $A$. Derive the Fourier symbol of its error-propagation operator and evaluate its high-frequency amplification at the mode with $(\\theta_{x}, \\theta_{y}) = (\\pi, \\pi)$ when $\\Sigma_{a} = 0$. Briefly compare this action to that of ILU(0) used as a left preconditioner within a stationary Richardson iteration in the context of multigrid cycles.\n\nYour final recorded answer must be the expression from part (2) for the total number of stored nonzero entries as a function of $n$. No rounding is required and no units should be used in the final expression.",
            "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in the field of numerical methods for reactor physics.\n\nThe problem asks for three distinct analyses based on the discretization of the one-group neutron diffusion equation on a uniform $n \\times n$ grid of interior cells. The resulting linear system $A \\phi = b$ involves a symmetric positive definite matrix $A$ with a five-point stencil structure arising from a cell-centered finite volume method with homogeneous Dirichlet boundary conditions. The total number of unknowns is $N=n^2$.\n\n### Part 1: Sparsity Pattern of ILU(0) Factors\n\nThe Incomplete Lower-Upper factorization with zero fill-in, denoted ILU($0$), produces factors $\\tilde{L}$ and $\\tilde{U}$ such that $A \\approx \\tilde{L}\\tilde{U}$. The defining constraint of ILU($0$) is that the sparsity pattern of the factors is a subset of the sparsity pattern of the original matrix $A$. That is, if $a_{ij} = 0$, then $\\tilde{l}_{ij}$ (for $ij$) and $\\tilde{u}_{ij}$ (for $i \\ge j$) are also forced to be zero.\n\nThe matrix $A$ is a five-point stencil matrix. For a grid point $p$ with lexicographic index $p = (i-1)n+j$, where $i,j \\in \\{1,...,n\\}$ are the row and column indices in the grid, the non-zero entries in row $p$ of $A$ are:\n-   $a_{p,p}$: The diagonal entry.\n-   $a_{p, p-1}$: Connection to the West neighbor $(i, j-1)$, if $j1$.\n-   $a_{p, p+1}$: Connection to the East neighbor $(i, j+1)$, if $jn$.\n-   $a_{p, p-n}$: Connection to the North neighbor $(i-1, j)$, if $i1$.\n-   $a_{p, p+n}$: Connection to the South neighbor $(i+1, j)$, if $in$.\n\nThe ILU($0$) algorithm proceeds by variants of Gaussian elimination, but with any new non-zero entry (fill-in) being discarded. For the five-point stencil, fill-in would typically occur at positions corresponding to neighbors of neighbors, for example, at an index corresponding to the North-West neighbor. Specifically, the product of a non-zero in the $-n$ sub-diagonal of $\\tilde{L}$ and a non-zero in the $-1$ sub-diagonal of an intermediate matrix could create fill-in. In ILU(0), this is prevented.\n\nAs a result, the non-zero structure of the factors $\\tilde{L}$ and $\\tilde{U}$ is identical to the non-zero structure of the lower and upper parts of $A$, respectively.\n-   The incomplete lower factor $\\tilde{L}$ is a unit lower triangular matrix. Its non-zero entries (apart from the unit diagonal) are located on the sub-diagonals with offsets $-1$ and $-n$, corresponding to the West and North connections.\n-   The incomplete upper factor $\\tilde{U}$ is an upper triangular matrix. Its non-zero entries are located on the main diagonal and on the super-diagonals with offsets $+1$ and $+n$, corresponding to the center, East, and South connections.\n\nThus, the sparsity pattern of $\\tilde{L}$ and $\\tilde{U}$ combined is the same as the sparsity pattern of $A$.\n\n### Part 2: Total Number of Stored Nonzero Entries\n\nWe need to count the total number of stored entries in $\\tilde{L}$ and $\\tilde{U}$. According to the problem's convention:\n1.  For $\\tilde{L}$, the unit diagonal is not stored. We only store its strictly lower triangular part.\n2.  For $\\tilde{U}$, we store all its entries (diagonal and strictly upper triangular part).\n\nThe total number of stored entries is therefore the number of non-zero entries in the strictly lower part of $A$ plus the number of non-zero entries in the upper part of $A$ (including the diagonal). This is simply the total number of non-zero entries in the original matrix $A$.\n\nLet's calculate the number of non-zero entries in $A$, an $n^2 \\times n^2$ matrix.\n-   **Diagonal entries**: There is one diagonal entry for each of the $n^2$ grid points. Total: $n^2$.\n-   **Off-diagonal entries**: These correspond to connections between adjacent interior grid points.\n    -   **Horizontal connections**: In each of the $n$ rows of the grid, there are $n-1$ connections between adjacent cells. This gives a total of $n(n-1)$ horizontal connections.\n    -   **Vertical connections**: In each of the $n$ columns of the grid, there are $n-1$ connections between adjacent cells. This gives a total of $n(n-1)$ vertical connections.\n\nThe total number of unique connections (or edges in the grid graph) is $n(n-1) + n(n-1) = 2n(n-1)$.\nSince the matrix $A$ is symmetric, each connection corresponds to two non-zero off-diagonal entries (e.g., $a_{pq}$ and $a_{qp}$).\nThe total number of off-diagonal non-zero entries is $2 \\times (\\text{number of connections}) = 2 \\times [2n(n-1)] = 4n(n-1) = 4n^2 - 4n$.\n\nThe total number of non-zero entries in $A$ is the sum of diagonal and off-diagonal entries:\n$NZ(A) = n^2 + (4n^2 - 4n) = 5n^2 - 4n$.\n\nThis is the total number of stored entries for the ILU($0$) factorization:\n-   Number of stored entries in $\\tilde{L}$ (strictly lower) = number of connections = $2n^2 - 2n$.\n-   Number of stored entries in $\\tilde{U}$ (diagonal + upper) = (diagonal entries) + (number of connections) = $n^2 + (2n^2 - 2n) = 3n^2 - 2n$.\n-   Total stored entries = $(2n^2 - 2n) + (3n^2 - 2n) = 5n^2 - 4n$.\n\n### Part 3: Local Fourier Analysis and Comparison\n\nLocal Fourier Analysis (LFA) is used to study the properties of an iterative method on an infinite grid, which approximates the behavior away from boundaries. We analyze the action of the iteration operator on a Fourier mode $\\phi_{j,k} = \\exp(i(j\\theta_x + k\\theta_y))$.\n\n**Gauss-Seidel Iteration:**\nThe matrix $A$ is split as $A = L_A + D_A + U_A$. The forward Gauss-Seidel error propagation operator is $M_{GS} = -(L_A + D_A)^{-1}U_A$. For lexicographic ordering, $L_A$ represents connections to \"past\" nodes (North and West), and $U_A$ represents connections to \"future\" nodes (South and East). Their Fourier symbols are:\n-   $\\tilde{L}_A(\\theta_x, \\theta_y) = -\\frac{D}{h^2}(\\exp(-i\\theta_x) + \\exp(-i\\theta_y))$\n-   $\\tilde{U}_A(\\theta_x, \\theta_y) = -\\frac{D}{h^2}(\\exp(i\\theta_x) + \\exp(i\\theta_y))$\n-   $\\tilde{D}_A(\\theta_x, \\theta_y) = \\Sigma_a + \\frac{4D}{h^2}$\n\nThe Fourier symbol of the Gauss-Seidel operator (amplification factor) is $\\tilde{M}_{GS} = - \\tilde{U}_A / (\\tilde{L}_A + \\tilde{D}_A)$:\n$$ \\tilde{M}_{GS}(\\theta_x, \\theta_y) = \\frac{\\frac{D}{h^2}(\\exp(i\\theta_x) + \\exp(i\\theta_y))}{\\Sigma_a + \\frac{4D}{h^2} - \\frac{D}{h^2}(\\exp(-i\\theta_x) + \\exp(-i\\theta_y))} $$\nFor the case $\\Sigma_a=0$ and the high-frequency mode $(\\theta_x, \\theta_y) = (\\pi, \\pi)$, we have $\\exp(i\\pi)=-1$ and $\\exp(-i\\pi)=-1$. The amplification factor is:\n$$ \\tilde{M}_{GS}(\\pi, \\pi) = \\frac{\\frac{D}{h^2}(-1 - 1)}{\\frac{4D}{h^2} - \\frac{D}{h^2}(-1 - 1)} = \\frac{-2\\frac{D}{h^2}}{4\\frac{D}{h^2} + 2\\frac{D}{h^2}} = \\frac{-2}{6} = -\\frac{1}{3} $$\nThe absolute value, $|-1/3| = 1/3$, is the high-frequency amplification factor. Since this value is significantly less than $1$, Gauss-Seidel is an effective smoother for this problem, as it strongly damps high-frequency error components.\n\n**Comparison with ILU(0) Preconditioning:**\nThe ILU($0$) factorization can be used as a preconditioner $M = \\tilde{L}\\tilde{U}$ in a stationary Richardson iteration. The error propagation operator is $E = I - M^{-1}A$.\n-   **Smoothing Mechanism:** Both Gauss-Seidel and ILU($0$) act as smoothers in a multigrid context. A smoother's role is to reduce high-frequency error components.\n-   **Gauss-Seidel:** Its smoothing property arises from the asymmetric nature of the update, where information from different directions is treated differently (old vs. new values). It is computationally simple but its effectiveness can degrade for problems with strong anisotropy, and its convergence can be slow due to its explicit, point-wise nature and dependence on ordering.\n-   **ILU(0):** This is a more implicit smoother. The error in the factorization, $R = \\tilde{L}\\tilde{U} - A$, consists of the fill-in that was discarded. This error is largest for high-frequency modes. Therefore, the preconditioner inverse $M^{-1} = (\\tilde{L}\\tilde{U})^{-1}$ acts very differently from $A^{-1}$ on these modes. When used in a Richardson iteration, this leads to strong damping of high-frequency errors. ILU-based smoothers are generally more robust than point-wise smoothers like Gauss-Seidel, especially for more complex problems with anisotropic coefficients, as they capture more of the couplings within the operator $A$. For the simple isotropic problem here, both methods are effective smoothers.",
            "answer": "$$\\boxed{5n^{2} - 4n}$$"
        }
    ]
}