{
    "hands_on_practices": [
        {
            "introduction": "The core principle of a Physics-Informed Neural Network (PINN) is to train the network not just on data, but also to respect the governing physical laws of a system. This is achieved by incorporating the differential equations directly into the loss function. This first practice challenges you to construct the complete loss function for a PINN designed to solve the foundational point reactor kinetics equations, combining physics residuals, initial conditions, and data-driven terms. ",
            "id": "4234282",
            "problem": "Consider the time-dependent point reactor kinetics model with $m$ delayed neutron precursor groups. The neutron population $n(t)$ and the delayed neutron precursor concentrations $C_i(t)$, for $i \\in \\{1,2,\\dots,m\\}$, evolve according to the widely accepted point kinetics ordinary differential equations derived from neutron balance and precursor decay-production processes:\n$$\n\\dot{n}(t) = \\frac{\\rho(t) - \\beta}{\\Lambda}\\, n(t) + \\sum_{i=1}^{m} \\lambda_i\\, C_i(t),\n$$\n$$\n\\dot{C}_i(t) = \\frac{\\beta_i}{\\Lambda}\\, n(t) - \\lambda_i\\, C_i(t),\n$$\nwhere $\\rho(t)$ is the reactivity, $\\beta = \\sum_{i=1}^{m} \\beta_i$ is the total delayed neutron fraction, $\\Lambda$ is the prompt neutron generation time, and $\\lambda_i$ are the precursor decay constants. Assume $\\rho(t)$ is a known differentiable function on the interval $[0,T]$, and all parameters $\\beta_i$, $\\lambda_i$, $\\Lambda$ are known positive constants. Initial conditions $n(0)=n_0$ and $C_i(0)=C_{i0}$ are given.\n\nA Physics-Informed Neural Network (PINN) uses Automatic Differentiation (AD) to approximate the solution by parameterizing $n(t)$ and $C_i(t)$ as $n_{\\theta}(t)$ and $C_{i,\\theta}(t)$, where $\\theta$ are the trainable network parameters. Let $\\{t_j\\}_{j=1}^{N}$ be interior collocation points in $(0,T)$ to enforce the physics, and optionally let $\\{t_k\\}_{k=1}^{K}$ be measurement times at which $n(t)$ has observed values $n^{\\mathrm{obs}}(t_k)$.\n\nUsing only the principles of neutron balance encoded in the point kinetics equations and the definition of a Physics-Informed Neural Network residual as the mismatch between the time derivative (computed by Automatic Differentiation) and the model right-hand side, derive the explicit analytic expression for a weighted mean-squared composite loss function $L(\\theta)$ that penalizes:\n- the neutron kinetics residual over $\\{t_j\\}$,\n- each precursor residual over $\\{t_j\\}$ and all $i \\in \\{1,\\dots,m\\}$,\n- the initial condition mismatches at $t=0$,\n- and the data mismatch at the measurement times $\\{t_k\\}$.\n\nYour loss must be a single closed-form expression in terms of sums over the collocation and measurement sets, with nonnegative weights $w_n$, $w_{C_i}$, $w_{\\mathrm{ic},n}$, $w_{\\mathrm{ic},C_i}$, and $w_d$ explicitly included, and it must use the residuals defined from the given differential equations. Express the final loss $L(\\theta)$ symbolically, without substituting numerical values. The final answer must be a single closed-form analytic expression. Do not include any units in your final answer.",
            "solution": "The problem statement is a valid scientific problem. It is grounded in the standard model of point reactor kinetics and the well-established methodology of Physics-Informed Neural Networks (PINNs). The problem is self-contained, well-posed, objective, and asks for a formal mathematical derivation based on the provided principles. All necessary information is given, and there are no internal contradictions, scientific inaccuracies, or ambiguities.\n\nWe are tasked with deriving a composite loss function, $L(\\theta)$, for a PINN designed to solve the point reactor kinetics equations. The loss function is a weighted sum of mean-squared errors that penalize deviations from physical laws, initial conditions, and observed data. The neural network approximations for the neutron population and precursor concentrations are denoted by $n_{\\theta}(t)$ and $C_{i,\\theta}(t)$, respectively, where $\\theta$ represents the trainable network parameters. The time derivatives of these network outputs, $\\frac{d n_{\\theta}}{dt}$ and $\\frac{d C_{i,\\theta}}{dt}$, are computed using Automatic Differentiation (AD).\n\nThe derivation proceeds by constructing each component of the loss function separately and then combining them into a single expression.\n\nFirst, we define the residuals for the governing ordinary differential equations (ODEs). The residual represents the extent to which the neural network approximation fails to satisfy the differential equation.\n\nThe residual for the neutron population equation, $r_n(t; \\theta)$, is defined by substituting the network approximations $n_{\\theta}(t)$ and $C_{i,\\theta}(t)$ into the first ODE:\n$$\nr_n(t; \\theta) = \\frac{d n_{\\theta}}{dt}(t) - \\left( \\frac{\\rho(t) - \\beta}{\\Lambda}\\, n_{\\theta}(t) + \\sum_{i=1}^{m} \\lambda_i\\, C_{i,\\theta}(t) \\right)\n$$\n\nSimilarly, for each of the $m$ delayed neutron precursor groups, the residual $r_{C_i}(t; \\theta)$ is defined by substituting the network approximations into the second set of ODEs:\n$$\nr_{C_i}(t; \\theta) = \\frac{d C_{i,\\theta}}{dt}(t) - \\left( \\frac{\\beta_i}{\\Lambda}\\, n_{\\theta}(t) - \\lambda_i\\, C_{i,\\theta}(t) \\right) \\quad \\text{for } i \\in \\{1, 2, \\dots, m\\}\n$$\n\nThe total loss function $L(\\theta)$ is a weighted sum of four distinct components:\n$1$. The physics loss for the neutron kinetics equation.\n$2$. The physics loss for the precursor concentration equations.\n$3$. The loss for the initial conditions.\n$4$. The loss for the data mismatch.\n\nWe construct each component as a mean-squared error (or squared error for single-point conditions).\n\n$1$. **Neutron Kinetics Residual Loss ($L_n$)**: This term penalizes the neutron kinetics residual over the set of $N$ interior collocation points $\\{t_j\\}_{j=1}^{N}$. It is the weighted mean-squared residual:\n$$\nL_n(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( r_n(t_j; \\theta) \\right)^2\n$$\nSubstituting the expression for $r_n(t_j; \\theta)$:\n$$\nL_n(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda}\\, n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2\n$$\n\n$2$. **Precursor Residuals Loss ($L_C$)**: This term penalizes the residuals for all $m$ precursor equations over the same collocation points. The problem specifies weights $w_{C_i}$ for each precursor group $i$. The most direct interpretation is to compute a weighted mean-squared error for each group and sum these contributions.\n$$\nL_C(\\theta) = \\sum_{i=1}^{m} \\left( w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( r_{C_i}(t_j; \\theta) \\right)^2 \\right)\n$$\nSubstituting the expression for $r_{C_i}(t_j; \\theta)$:\n$$\nL_C(\\theta) = \\sum_{i=1}^{m} w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda}\\, n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2\n$$\n\n$3$. **Initial Condition Loss ($L_{\\mathrm{ic}}$)**: This term penalizes the mismatch between the network's prediction at $t=0$ and the given initial conditions $n(0)=n_0$ and $C_i(0)=C_{i0}$. This is a sum of weighted squared errors (not a mean, as it's evaluated at a single point).\n$$\nL_{\\mathrm{ic}}(\\theta) = w_{\\mathrm{ic},n} \\left( n_{\\theta}(0) - n_0 \\right)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} \\left( C_{i,\\theta}(0) - C_{i0} \\right)^2\n$$\n\n$4$. **Data Mismatch Loss ($L_d$)**: This term penalizes the deviation of the network's prediction for the neutron population, $n_{\\theta}(t_k)$, from the observed values, $n^{\\mathrm{obs}}(t_k)$, at the $K$ measurement times $\\{t_k\\}_{k=1}^{K}$. This is the weighted mean-squared error of the data mismatch.\n$$\nL_d(\\theta) = w_d \\frac{1}{K} \\sum_{k=1}^{K} \\left( n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k) \\right)^2\n$$\n\nFinally, the total composite loss function $L(\\theta)$ is the sum of these individual components:\n$$\nL(\\theta) = L_n(\\theta) + L_C(\\theta) + L_{\\mathrm{ic}}(\\theta) + L_d(\\theta)\n$$\nCombining all the terms yields the final explicit analytic expression for the loss function:\n$$\nL(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + w_{\\mathrm{ic},n} (n_{\\theta}(0) - n_0)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} (C_{i,\\theta}(0) - C_{i0})^2 + w_d \\frac{1}{K} \\sum_{k=1}^{K} (n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k))^2\n$$\nThis expression represents the complete objective function to be minimized during the training of the Physics-Informed Neural Network to find the optimal parameters $\\theta$.",
            "answer": "$$\n\\boxed{\nL(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} \\left( w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2 \\right) + w_{\\mathrm{ic},n} (n_{\\theta}(0) - n_0)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} (C_{i,\\theta}(0) - C_{i0})^2 + w_d \\frac{1}{K} \\sum_{k=1}^{K} (n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k))^2\n}\n$$"
        },
        {
            "introduction": "While physics can be enforced through the loss function, some fundamental constraints like non-negativity can be guaranteed by the network's architecture itself, which is a more robust approach. This exercise explores the use of the softplus activation function to ensure that physical quantities like neutron flux and temperature remain positive. It delves into the practical necessity of calibrating this function to prevent vanishing gradients, a common pitfall in training deep networks for scientific applications. ",
            "id": "4234298",
            "problem": "A reactor-physics surrogate uses a Neural Network (NN) to map spatial-material features to two physically constrained outputs: the steady-state scalar neutron flux, denoted by $\\phi(\\mathbf{r})$, and the coolant temperature, denoted by $T(\\mathbf{r})$. Physical realizability requires $\\phi(\\mathbf{r}) \\ge 0$ and $T(\\mathbf{r}) \\ge 0$. To encode these non-negativity constraints at the network outputs while retaining smoothness needed for gradient-based training on Partial Differential Equation (PDE)-informed losses, the model employs a parameterized softplus activation at the final layer,\n$$\ns_{\\beta}(z) = \\frac{1}{\\beta}\\,\\ln\\!\\bigl(1 + \\exp(\\beta z)\\bigr),\n$$\nwith a positive scale parameter $\\beta > 0$. The outputs are given by $\\phi(\\mathbf{r}) = s_{\\beta}(a_{\\phi}(\\mathbf{r}))$ and $T(\\mathbf{r}) = s_{\\beta}(a_{T}(\\mathbf{r}))$, where $a_{\\phi}$ and $a_{T}$ are the corresponding pre-activations.\n\nAssume that, due to Batch Normalization (BN), the pre-activation variables are approximately Gaussian with zero mean and unit variance at initialization, that is, $a_{\\phi} \\sim \\mathcal{N}(0,1)$ and $a_{T} \\sim \\mathcal{N}(0,1)$. To mitigate vanishing gradients in physics-informed training, you require that with probability $p = 0.95$ over the pre-activation distribution, the output-layer local gradient magnitude with respect to its input satisfies\n$$\n\\delta \\le \\frac{d}{dz} s_{\\beta}(z) \\le 1 - \\delta,\n$$\nwith $\\delta = 0.05$. Starting from fundamental definitions and standard Gaussian properties, derive a closed-form expression for the required $\\beta$ in terms of the standard normal cumulative distribution function and its inverse, and then evaluate $\\beta$ numerically for the given $p$ and $\\delta$ under the stated Gaussian assumption. Round your numerical answer to four significant figures. The final answer must be a single real number with no units.",
            "solution": "The problem requires the derivation of a parameter $\\beta$ for a parameterized softplus activation function, $s_{\\beta}(z)$, used in a neural network surrogate for reactor physics simulation. The derivation is subject to a probabilistic constraint on the gradient of this activation function.\n\nFirst, we identify the given function and its properties. The parameterized softplus activation function is defined as:\n$$s_{\\beta}(z) = \\frac{1}{\\beta}\\,\\ln\\bigl(1 + \\exp(\\beta z)\\bigr)$$\nwhere $\\beta > 0$. The local gradient, or the derivative of $s_{\\beta}(z)$ with respect to its input $z$, is found using the chain rule:\n$$\n\\frac{d}{dz} s_{\\beta}(z) = \\frac{d}{dz} \\left[ \\frac{1}{\\beta}\\,\\ln\\bigl(1 + \\exp(\\beta z)\\bigr) \\right] = \\frac{1}{\\beta} \\cdot \\frac{1}{1 + \\exp(\\beta z)} \\cdot \\frac{d}{dz}\\bigl(1 + \\exp(\\beta z)\\bigr)\n$$\n$$\n\\frac{d}{dz} s_{\\beta}(z) = \\frac{1}{\\beta} \\cdot \\frac{1}{1 + \\exp(\\beta z)} \\cdot \\bigl(\\beta \\exp(\\beta z)\\bigr) = \\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)}\n$$\nThis expression is the standard logistic sigmoid function, often denoted as $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$. We can see this by rewriting our expression:\n$$\n\\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)} = \\frac{1}{\\frac{1 + \\exp(\\beta z)}{\\exp(\\beta z)}} = \\frac{1}{\\exp(-\\beta z) + 1} = \\sigma(\\beta z)\n$$\nThe problem specifies a constraint on this gradient to prevent it from vanishing or saturating. The constraint is:\n$$\n\\delta \\le \\frac{d}{dz} s_{\\beta}(z) \\le 1 - \\delta\n$$\nSubstituting the expression for the derivative, we have:\n$$\n\\delta \\le \\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)} \\le 1 - \\delta\n$$\nThis compound inequality can be split into two separate inequalities. Solving for $z$, we find the interval where the condition holds:\n$$\n\\frac{1}{\\beta} \\ln\\left(\\frac{\\delta}{1 - \\delta}\\right) \\le z \\le \\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)\n$$\nUsing the property $\\ln(1/x) = -\\ln(x)$, we can write this interval as $[-z_{max}, z_{max}]$, where $z_{max} = \\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)$.\n\nThe problem states that the pre-activation $z$ follows a standard normal distribution, $z \\sim \\mathcal{N}(0,1)$, and that the probability of $z$ falling into this interval is $p$.\n$$\nP(-z_{max} \\le z \\le z_{max}) = p\n$$\nLet $\\Phi(x)$ denote the cumulative distribution function (CDF) of the standard normal distribution. The probability can be expressed as $\\Phi(z_{max}) - \\Phi(-z_{max})$. Using the symmetry property $\\Phi(-x) = 1 - \\Phi(x)$, this becomes $2\\Phi(z_{max}) - 1 = p$.\nSolving for $\\Phi(z_{max})$:\n$$\n\\Phi(z_{max}) = \\frac{1 + p}{2}\n$$\nTo find $z_{max}$, we apply the inverse of the standard normal CDF, $\\Phi^{-1}$:\n$$\nz_{max} = \\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)\n$$\nWe now have two expressions for $z_{max}$. Equating them allows us to solve for $\\beta$:\n$$\n\\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right) = \\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)\n$$\nSolving for $\\beta$, we obtain the required closed-form expression:\n$$\n\\beta = \\frac{\\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)}{\\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)}\n$$\nNow, we substitute the numerical values provided in the problem: $p = 0.95$ and $\\delta = 0.05$.\nFirst, we compute the argument of the logarithm:\n$$\n\\frac{1 - \\delta}{\\delta} = \\frac{1 - 0.05}{0.05} = \\frac{0.95}{0.05} = 19\n$$\nThe numerator of the expression for $\\beta$ is $\\ln(19)$.\nNext, we compute the argument of the inverse CDF:\n$$\n\\frac{1 + p}{2} = \\frac{1 + 0.95}{2} = \\frac{1.95}{2} = 0.975\n$$\nThe denominator is $\\Phi^{-1}(0.975)$. This is the z-score that corresponds to a cumulative probability of 0.975, which is approximately $1.960$.\nNow we compute $\\beta$:\n$$\n\\beta = \\frac{\\ln(19)}{\\Phi^{-1}(0.975)} \\approx \\frac{2.944439}{1.959964} \\approx 1.50229\n$$\nThe problem requires the answer to be rounded to four significant figures.\nThe final numerical value for $\\beta$ is $1.502$.",
            "answer": "$$\\boxed{1.502}$$"
        },
        {
            "introduction": "Beyond simple positivity, many physical systems exhibit more complex behaviors like monotonicity, where an output consistently increases or decreases with an input. This practice demonstrates how to encode such knowledge into the training process using a physics-based regularizer in the loss function. You will formulate and derive the gradient for a regularizer that encourages a surrogate model for $k_{\\mathrm{eff}}$ to respect its known monotonic relationship with fuel enrichment and absorber concentration. ",
            "id": "4234321",
            "problem": "A deep neural network parameterized by weights and biases $\\boldsymbol{\\theta}$, denoted $f_{\\boldsymbol{\\theta}}:\\mathbb{R}^{d}\\to\\mathbb{R}$, is trained as a surrogate for the effective neutron multiplication factor $k_{\\mathrm{eff}}$ in a pressurized water reactor steady-state core model. The input feature vector $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ includes uranium-$\\mathrm{^{235}U}$ enrichment $e$ (mass fraction) and soluble boron absorber concentration $a$ (moles per unit volume), along with other thermohydraulic and geometric features held fixed for this task. From reactor physics, $k_{\\mathrm{eff}}$ increases monotonically with $e$ and decreases monotonically with $a$, because enrichment increases the average number of fissions per neutron generation while absorber content increases neutron capture without fission. The training set consists of $N$ base states $\\{\\boldsymbol{x}_{i}\\}_{i=1}^{N}$, where each $\\boldsymbol{x}_{i}$ contains $(e_{i},a_{i})$ and fixed contextual features. For each base state, define two augmented states by small controlled perturbations $\\Delta e>0$ and $\\Delta a>0$ applied exclusively to the enrichment and absorber components:\n$$\n\\boldsymbol{x}_{i}^{e+} := \\boldsymbol{x}_{i}\\ \\text{with $e$ replaced by $e_{i}+\\Delta e$},\\qquad\n\\boldsymbol{x}_{i}^{a+} := \\boldsymbol{x}_{i}\\ \\text{with $a$ replaced by $a_{i}+\\Delta a$}.\n$$\nYou are tasked to construct a differentiable monotonicity regularizer $R(\\boldsymbol{\\theta})$ that penalizes deviations from the physically expected local monotonic behavior, namely $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})\\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$ and $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+})\\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$ for each $i$. Use separate nonnegative weights $\\lambda_{e}$ and $\\lambda_{a}$ to control the strength of the enrichment and absorber monotonic components. Formulate $R(\\boldsymbol{\\theta})$ using a squared hinge penalty on the signed finite-difference violations so that the penalty is zero when the inequalities are satisfied and grows quadratically otherwise. Then, starting only from the definition of this regularizer, derive a closed-form analytical expression for the gradient $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ in terms of $\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ evaluated at $\\boldsymbol{x}_{i}$, $\\boldsymbol{x}_{i}^{e+}$, and $\\boldsymbol{x}_{i}^{a+}$. Express the final gradient using standard functions and clearly indicate any indicator behavior through a Heaviside step function.\n\nYour final answer must be a single closed-form analytic expression for $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$. No numerical evaluation is required.",
            "solution": "The problem requires the construction of a differentiable monotonicity regularizer, $R(\\boldsymbol{\\theta})$, and the derivation of its gradient, $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$.\n\nFirst, we formulate the regularizer using a squared hinge penalty. The penalty is applied to violations of the expected monotonic behavior.\n- For enrichment, the desired condition is $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}) \\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$, or $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) \\ge 0$. A violation occurs when this is negative. The quantity to penalize is the positive part of its negation, i.e., $\\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}))$.\n- For absorber concentration, the desired condition is $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) \\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$, or $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) \\ge 0$. The violation penalty is based on $\\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}))$.\n\nThe total regularizer $R(\\boldsymbol{\\theta})$ is the weighted sum of these squared penalties over all $N$ training samples:\n$$\nR(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\left( \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) \\right)^2 + \\lambda_{a} \\left( \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right)^2 \\right]\n$$\nTo derive the gradient $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$, we use the chain rule. The gradient of a term of the form $(\\max(0, v))^2$ with respect to $\\boldsymbol{\\theta}$ is $2\\max(0, v) \\nabla_{\\boldsymbol{\\theta}} v$.\n\nApplying this to the enrichment term, where $v_{e,i} = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$, the gradient contribution is:\n$$\n2 \\lambda_e \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}))\n$$\nApplying this to the absorber term, where $v_{a,i} = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$, the gradient contribution is:\n$$\n2 \\lambda_a \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}))\n$$\nThe problem asks to use the Heaviside step function, $H(z)$, to represent the indicator behavior. We can write $\\max(0,v) = v H(v)$, where $H(v)=1$ if $v>0$ and $0$ otherwise. Substituting this into the gradient expressions and summing over all samples gives the final result:\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = 2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) \\right.\n$$\n$$\n\\left. + \\lambda_{a} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n$$\nThis is the complete analytical gradient of the specified monotonicity regularizer.",
            "answer": "$$\n\\boxed{\n2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) + \\lambda_{a} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n}\n$$"
        }
    ]
}