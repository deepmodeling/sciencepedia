## 引言
机器学习与神经网络正在深刻地改变科学与工程领域，核反应堆模拟也不例外。这些先进技术并非旨在取代经过数十年验证的物理模型，而是作为一种强大的增强手段，与成熟的物理原理协同工作。当前，高保真度物理仿真面临着巨大的计算成本挑战，这限制了我们在参数研究、不确定性量化和实时决策方面的能力。机器学习通过提供高效的代理模型、融合多源信息以及从复杂数据中发现隐藏模式，为解决这些长期存在的瓶颈开辟了新的道路。

本文将系统地介绍机器学习在现代[核反应堆模拟](@entry_id:1128946)中的应用。在第一章“原理与机制”中，我们将深入探讨几种核心技术，包括如何将物理定律编码进神经网络（PINNs）、如何学习函数间的映射（[神经算子](@entry_id:1128605)），以及如何利用系统的几何结构（[图神经网络](@entry_id:136853)）。随后的第二章“应用与交叉学科联系”将展示这些原理如何应用于现实世界问题，例如构建加速仿真的代理模型、开发用于健康监测的[数字孪生](@entry_id:171650)，以及实现智能自主控制。最后，在“动手实践”部分，您将有机会通过具体练习来巩固所学知识。让我们首先深入探讨支撑这些变革性应用的机器学习核心原理与机制。

## 原理与机制

在核反应堆模拟领域，机器学习和神经网络的出现并非旨在取代经过数十年发展的物理模型，而是为了增强它们。通过与成熟的物理原理和数值方法相结合，机器学习为解决长期存在的计算瓶颈、加速参数研究和实现全面的[不确定性量化](@entry_id:138597)开辟了新的途径。本章深入探讨了支撑这些应用的核心原理和机制，从利用物理定律约束神经网络，到学习复杂的算子和利用图结构，再到在严格的不确定性框架内集成数据驱动模型。

### 从物理中学习：[残差最小化](@entry_id:754272)原理

传统[机器学习范式](@entry_id:637731)严重依赖大量标记数据。然而，在许多科学和工程领域，数据可能稀缺、昂贵或难以获得。幸运的是，我们通常对系统有深刻的物理理解，这些理解以[偏微分](@entry_id:194612)方程（PDEs）、[积分微分方程](@entry_id:165050)或代数守恒定律的形式存在。**[物理信息神经网络](@entry_id:145229)（Physics-Informed Neural Networks, [PINNs](@entry_id:145229)）** 的核心思想是利用这些已知的物理定律作为一种正则化形式，直接在训练过程中指导神经网络。

PINNs并非学习输入-输出对的映射关系，而是学习一个函数，这个函数本身就是物理定律的解。其基本机制是将神经网络的输出代入控制方程，形成一个**残差**。网络的参数通过最小化这个残差的范数（通常是$L^2$范数）来优化，从而迫使网络的输出满足物理定律。

#### 基本概念：扩散方程的PINN

让我们以一个简单但基础的例子来说明这个概念：一维板中的[稳态](@entry_id:139253)[中子扩散](@entry_id:158469)。中子标量通量 $\phi(x)$ 由以下扩散方程描述：
$$
- D \frac{d^{2} \phi}{d x^{2}} + \Sigma_{a} \phi = Q
$$
其中 $D$ 是扩散系数，$\Sigma_{a}$ 是宏观吸收截面，而 $Q$ 是一个恒定的中子源项。所有这些参数都假设为正常数。

为了构建一个PINN代理模型，我们将标量通量 $\phi(x)$ 表示为一个以 $\theta$ 为参数的神经网络 $\phi_{\theta}(x)$。训练的目标不是匹配 $\phi(x)$ 的已知数据点，而是找到一组参数 $\theta$，使得 $\phi_{\theta}(x)$ 在整个域上满足[扩散方程](@entry_id:170713)。为此，我们定义了**逐点残差** $r(x; \theta)$，它衡量了神经网络解在何种程度上违反了PDE：
$$
r(x; \theta) = - D \frac{d^{2} \phi_{\theta}(x)}{d x^{2}} + \Sigma_{a} \phi_{\theta}(x) - Q
$$
实现这一点的关键技术是**自动微分（Automatic Differentiation, AD）**。现代深度学习框架可以精确地计算网络输出相对于其输入的导数，而无需进行符号推导或[数值近似](@entry_id:161970)。这使得我们能够高效地计算像 $\frac{d^{2} \phi_{\theta}}{d x^{2}}$ 这样的[高阶导数](@entry_id:140882)项。

一旦残差被定义，我们就可以构建一个**物理[损失函数](@entry_id:634569)** $\mathcal{L}_{phys}(\theta)$，通常定义为在整个计算域 $\Omega$ 上残差的平方积分：
$$
\mathcal{L}_{phys}(\theta) = \int_{\Omega} r(x; \theta)^{2} \, dx
$$
通过使用[梯度下降](@entry_id:145942)等[优化算法](@entry_id:147840)最小化 $\mathcal{L}_{phys}(\theta)$，我们驱动残差在整个域上趋近于零，从而使神经网络 $\phi_{\theta}(x)$ 收敛到扩散方程的真实解。

为了更具体地理解这一点，我们可以考虑一个极简的单[参数模型](@entry_id:170911)，其中解的形式被假设为 $\phi_{\theta}(x) = A \sin\left( \frac{\pi x}{L} \right)$，其中 $A$ 是唯一需要训练的参数 ()。在这种情况下，残差变为：
$$
r(x) = A \left(D \frac{\pi^2}{L^2} + \Sigma_{a}\right) \sin\left(\frac{\pi x}{L}\right) - Q
$$
通过最小化损失函数 $\mathcal{L}(A) = \int_{0}^{L} r(x)^{2} \, dx$，我们可以解析地求解出最优振幅 $A^{\star}$：
$$
A^{\star} = \frac{\int_{0}^{L} Q \sin(\frac{\pi x}{L}) dx}{\left(D \frac{\pi^2}{L^2} + \Sigma_{a}\right) \int_{0}^{L} \sin^{2}(\frac{\pi x}{L}) dx} = \frac{4Q}{\pi \left(D \frac{\pi^2}{L^2} + \Sigma_{a}\right)}
$$
这个简单的例子揭示了PINN的核心机制：将PDE求解问题转化为一个优化问题，其中物理定律本身就是[目标函数](@entry_id:267263)。

#### 扩展到复杂算子：[玻尔兹曼输运方程](@entry_id:140472)

PINN原理的优越性在于其通用性，它可以应用于比简单扩散方程更复杂的物理模型。一个重要的例子是中子**玻尔兹曼输运方程**，这是一个积分-[微分](@entry_id:158422)方程，更精确地描述了中子在相空间（位置和角度）中的行为。在一维板几何中，对于角通量 $\psi(x, \mu)$，该方程为：
$$
\mu\,\frac{\partial \psi(x,\mu)}{\partial x} + \Sigma_{t}\,\psi(x,\mu) = \frac{\Sigma_{s}}{2}\int_{-1}^{1}\psi(x,\mu')\,d\mu' + q
$$
其中，$\mu$ 是粒子飞行方向与x轴夹角的余弦，$\Sigma_t$ 和 $\Sigma_s$ 分别是总截面和[散射截面](@entry_id:140322)。方程左侧的项代表由于粒子**流串（streaming）**和**碰撞（collision）**造成的损失，右侧的项代表来自各向同性**散射源（scattering source）**和外部源 $q$ 的增益。

为这个方程构建[PINN损失函数](@entry_id:137288)时，会遇到新的挑战 ()。
$$
\mathcal{R}[\psi] = \int_{0}^{1}\int_{-1}^{1}\left(\mu\,\partial_{x}\psi + \Sigma_{t}\,\psi - \frac{\Sigma_{s}}{2}\int_{-1}^{1}\psi(x,\mu')\,d\mu' - q\right)^{2}\,d\mu\,dx
$$
- **[微分](@entry_id:158422)项** $\mu\,\partial_{x}\psi$ 仍然通过[自动微分](@entry_id:144512)处理。
- **积分项** $\int_{-1}^{1}\psi(x,\mu')\,d\mu'$（即标量通量 $\phi(x)$）需要在训练期间进行数值计算。这通常通过**数值求积（numerical quadrature）**（如[高斯-勒让德求积](@entry_id:138201)）在每个训练步中对一批角度 $\mu_k$ 上的网络输出进行加权求和来近似。

此外，物理问题通常伴随着**边界条件**。例如，[真空边界条件](@entry_id:1133678)意味着在边界上没有指向域内的入射中子通量。这些条件必须通过一个额外的**边界损失项** $\mathcal{R}_{\mathrm{BC}}[\psi]$ 来强制执行。例如，对于 $x=0$ 的边界，入射方向为 $\mu > 0$，因此损失项会惩罚在这些方向上的任何非零通量：
$$
\mathcal{R}_{\mathrm{BC}}[\psi] = \int_{0}^{1}\psi(0,\mu)^{2}\,d\mu \quad \text{for } \mu \in (0,1]
$$
总[损失函数](@entry_id:634569)是物理损失和边界损失的加权和 $\mathcal{L}_{total} = \mathcal{L}_{phys} + \lambda \mathcal{R}_{\mathrm{BC}}$。通过最小化这个总损失，PINN能够学习一个同时满足控制方程和边界条件的解。

### 学习算子：从函数到泛函

PINN非常适合求解单个、特定的PDE实例。然而，在许多工程应用中，我们更感兴趣的是理解系统如何响应变化的输入参数或场。例如，我们可能想知道当[中子截面](@entry_id:1128688)场 $\Sigma_a(x)$ 或源项 $S(x)$ 变化时，中子通量场 $\phi(x)$ 如何变化。这需要学习一个**解算子（solution operator）**，即一个将输入函数（如系数场）映射到输出函数（如解场）的映射。

#### [算子学习](@entry_id:752958)的形式化

形式上，对于一个由[参数化](@entry_id:265163)PDE定义的系统，例如[稳态扩散](@entry_id:154663)方程
$$
-\nabla \cdot \big(D(x)\,\nabla \phi(x)\big) + \Sigma_a(x)\,\phi(x) = S(x) \quad \text{in } \Omega
$$
我们的目标是学习解算子 $\mathcal{G}: (D, \Sigma_a, S) \mapsto \phi$。

要成功学习这个算子，一个关键的前提是该算子是**良态的（well-posed）**。这意味着对于输入空间中的任何一组有效系数，都存在一个唯一的、稳定的解。这要求我们仔细定义算子的输入和输出**[函数空间](@entry_id:143478)** ()。
- **输出空间**：对于带[真空边界条件](@entry_id:1133678)（即齐次[狄利克雷边界条件](@entry_id:173524) $\phi|_{\partial\Omega}=0$）的扩散问题，其解的自然空间是**[索博列夫空间](@entry_id:141995)** $H^1_0(\Omega)$。这个空间包含了[弱导数](@entry_id:189356)平方可积且在边界上为零的函数。
- **输入空间**：为了保证解的存在性和唯一性（通过[Lax-Milgram定理](@entry_id:137966)），输入系数场必须满足一定条件。例如，扩散张量 $D(x)$ 必须是**一致椭圆的**（即其特征值有正下界），并且它和[吸收截面](@entry_id:172609) $\Sigma_a(x)$ 都应属于 $L^\infty(\Omega)$（[有界函数](@entry_id:176803)空间）。源项 $S(x)$ 通常要求是 $L^2(\Omega)$（[平方可积函数](@entry_id:200316)）或更广义的 $H^{-1}(\Omega)$。

解算子的**连续性**是可学习性的另一个理论基石。例如，扩散问题的解算子可以被证明是**[利普希茨连续的](@entry_id:267396)** ()。这意味着输入系数场的微小变化只会导致解场的微小变化。这种稳定性保证了从一组离散的训练样本中学习到的算子可以泛化到未见过的输入。

#### [傅里叶神经算子](@entry_id:189138)

**[神经算子](@entry_id:1128605)（Neural Operators）**是一类专门为学习无限维[函数空间](@entry_id:143478)之间的映射而设计的[神经网络架构](@entry_id:637524)。其中，**[傅里叶神经算子](@entry_id:189138)（Fourier Neural Operator, FNO）** 因其效率和强大的[表达能力](@entry_id:149863)而备受关注。

FNO之所以有效，其背后有深刻的数学原理，尤其适用于具有周期性边界条件的线性PDE ()。对于[常系数](@entry_id:269842)[扩散方程](@entry_id:170713) $-D \Delta \phi + \Sigma_a \phi = s$，其解算子 $\mathcal{S}$ 在傅里叶域中表现为一个简单的**傅里叶乘子**。也就是说，解的第 $k$ 个[傅里叶系数](@entry_id:144886) $\hat{\phi}_k$ 可以通过将源的第 $k$ 个[傅里叶系数](@entry_id:144886) $\hat{s}_k$ 乘以一个与频率 $k$ 相关的因子得到：
$$
\hat{\phi}_k = \frac{1}{D |k|^2 + \Sigma_a} \hat{s}_k
$$
从[算子理论](@entry_id:139990)的角度来看，由于这个乘子随着频率 $|k| \to \infty$ 而衰减至零，解算子 $\mathcal{S}$ 是一个**[紧算子](@entry_id:139189)**。[紧算子](@entry_id:139189)一个非常重要的性质是，它们可以被**[有限秩算子](@entry_id:274418)**任意逼近。在傅里叶域中，这意味着我们可以通过仅保留有限数量的低频模式（即截断[高频模式](@entry_id:750297)）来很好地近似整个算子。

FNO的架构正是为了利用这一性质而设计的。它的核心操作包括：
1.  使用[快速傅里叶变换](@entry_id:143432)（FFT）将输入函数转换到频域。
2.  在频域中，通过一个小的神经网络（或直接通过可学习的权重）对选定的傅里叶模式进行乘法和线性组合。
3.  使用逆[快速傅里叶变换](@entry_id:143432)（IFFT）将结果转换回物理空间。

这种架构使得FNO能够非常有效地学习傅里叶乘子，从而高效地逼近一大类PDE的解算子。

#### 实践中的挑战

尽管理论上很优雅，但在实践中应用[神经算子](@entry_id:1128605)时仍需考虑一些挑战 ()：
- **[参数可辨识性](@entry_id:197485)**：提供给算子的输入必须足以唯一确定输出。例如，对于[输运方程](@entry_id:174281)，仅提供[吸收截面](@entry_id:172609) $\sigma_a$ 和源项 $q$ 是不够的，因为总截面 $\sigma_t$ 同样决定了通量分布，因此也必须作为输入。
- **边界条件**：标准FNO架构天然地强加了[周期性边界条件](@entry_id:753346)。对于[非周期性](@entry_id:275873)问题（如狄利克雷或诺伊曼边界），需要对架构进行修改，例如通过对输入进行特殊延拓或在[网络设计](@entry_id:267673)中明确强制执行边界值。简单的[零填充](@entry_id:637925)通常不足以精确地施加这些条件。
- **多尺度问题和刚度**：当问题涉及多个尺度时（例如，在输运到扩散的极限下），训练可能会变得**刚性**。在[扩散极限](@entry_id:168181)下，控制方程的某些项变得非常小，导致梯度在不同尺度上极不平衡。解决这个问题可能需要自适应损失加权等先进技术，简单的固定权重缩放（如按尺度参数的倒数缩放）可能无法在所有情况下都稳定地改善训练。

### 学习结构：用于核心物理的[图神经网络](@entry_id:136853)

许多核工程问题天然具有**图（graph）**结构。例如，一个压水堆（PWR）的堆芯由排列成格点的燃料组件构成。组件之间的相互作用（如中子泄漏、热量传递）可以自然地表示为图中的边。**图神经网络（Graph Neural Networks, GNNs）** 是专门为处理这种[结构化数据](@entry_id:914605)而设计的强大工具。

GNN的核心思想是**[消息传递](@entry_id:751915)（message passing）**，其中每个节点（即燃料组件）通过聚合其邻居节点的信息来更新自身的状态。一个关键的设计原则是，图的构建（节点、边和特征）必须由底层物理驱动。

#### 用于[中子扩散](@entry_id:158469)的物理一致GNN

考虑将GNN用于模拟堆芯的[中子扩散](@entry_id:158469)。我们可以将每个燃料组件视为一个节点，其状态为该组件内的平均中子通量 $\phi_i$。如果两个组件物理上相邻，则在它们对应的节点之间创建一条边。GNN的一层将执行一次迭代更新 $\phi_i^{(t)} \mapsto \phi_i^{(t+1)}$。

一个常见的GNN层，如基本的[图卷积网络](@entry_id:194500)（GCN），可能会简单地对邻居节点的通量进行平均。然而，这种朴素的方法忽略了扩散的物理本质。[中子扩散](@entry_id:158469)是由通量**梯度**驱动的，而不是通量的绝对值。

一个物理上一致的GNN更新规则应该模仿求解离散化[扩散方程](@entry_id:170713)的迭代步骤 ()。离散化的[稳态扩散](@entry_id:154663)方程在节点 $i$ 处可以写作：
$$
\underbrace{\sum_{j \in \mathcal{N}(i)} \frac{D_{ij}}{h^2} (\phi_j - \phi_i)}_{\text{从邻居净泄漏}} - \underbrace{\Sigma_{a,i} \phi_i}_{\text{吸收}} + \underbrace{\frac{1}{k} \nu \Sigma_{f,i} \phi_i}_{\text{裂变源}} = 0
$$
其中 $\mathcal{N}(i)$ 是节点 $i$ 的邻居集合，$h$ 是格点间距。这个方程的[理查森迭代](@entry_id:635109)求解格式为：
$$
\phi_i^{(t+1)} = \phi_i^{(t)} - \Delta t \left[ \sum_{j \in \mathcal{N}(i)} \frac{D_{ij}}{h^2} (\phi_i^{(t)} - \phi_j^{(t)}) + \Sigma_{a,i} \phi_i^{(t)} - \frac{1}{k} \nu \Sigma_{f,i} \phi_i^{(t)} \right]
$$
这个更新规则本身就是一个有效的GNN消息传递层。它在物理上是合理的，因为它正确地包含了基于通量差异的扩散项、局部的吸收项和源项。相比之下，其他[启发式方法](@entry_id:637904)，如使用[注意力机制](@entry_id:917648)或简单的邻居平均，若没有以物理为基础，则无法捕捉到正确的相互作用。

#### 用于热工水力的图表示

同样，我们可以为堆芯的热工水力学建立一个物理上合理的图表示 ()。在这种情况下，节[点特征](@entry_id:155984)可以包括富集度、燃耗、控制棒插入深度以及冷却剂入口温度和流速等。边的设计应该编码组件间的[热耦合](@entry_id:1132992)。

热量主要通过相邻组件间的导热和冷却剂混合进行交换。这种相互作用是**局部的**和**互易的**。因此，图应该是稀疏的（仅连接物理上相邻的组件），并且是无向的。

**边权重**的设计至关重要，它们应该量化热耦合的强度。基于能量守恒，一个有物理意义的边权重可以是组件间[热导](@entry_id:189019)纳与组件内对流换热能力的无量纲比值。例如，权重 $w_{ij}$可以定义为：
$$
w_{ij} = \frac{h_{ij} A_{ij}^{\text{int}}}{c_p \overline{\dot{m}}_{ij}}
$$
其中 $h_{ij} A_{ij}^{\text{int}}$ 是组件间界面的有效热导，而 $c_p \overline{\dot{m}}_{ij}$ 代表了该界面处[对流传热](@entry_id:151658)的特征能力。这种基于第一性原理的权重设计，远优于那些仅基于几何距离或特征相似度的启发式方法，因为它直接将物理机制编码到了图的结构中。

### 从数据中学习：[数据驱动建模](@entry_id:184110)与不确定性

尽管物理信息方法功能强大，但许多成功的机器学习应用本质上仍然是数据驱动的。在[核模拟](@entry_id:1128947)中，数据驱动模型通常用于创建现有高保真代码的快速代理模型，或从高维模拟输出中提取低维特征。

#### 使用自编码器进行降维

[核反应堆模拟](@entry_id:1128946)（例如，使用蒙特卡罗方法）可以产生极其高维的数据，如整个堆芯的三维中子通量分布快照。分析或存储大量此类快照可能不切实际。**自编码器（Autoencoders）**是一种无监督神经网络，非常适合用于**降维（dimensionality reduction）**。

自编码器由两部分组成：一个**编码器**将高维输入数据 $x$ 压缩到一个低维的**潜空间（latent space）**表示 $z$，一个**解码器**尝试从 $z$ 重构出原始输入 $\hat{x}$。网络通过最小化重构误差（例如，$\|x - \hat{x}\|^2$）进行训练。

自编码器与经典的[降维技术](@entry_id:169164)**主成分分析（Principal Component Analysis, PCA）** 有着深刻的联系 ()。对于一个经过线性化处理的自编码器，其最优的编码-解码过程等价于将数据投影到由[数据协方差](@entry_id:748192)矩阵的前 $k$ 个[特征向量](@entry_id:151813)（即主成分）张成的子空间上，其中 $k$ 是潜空间的维度。

在这种情况下，最小的期望重构误差等于被舍弃的特征值之和：
$$
\mathbb{E}\left[\|x - \hat{x}\|_{2}^{2}\right]_{opt} = \sum_{i=k+1}^{n} \lambda_i
$$
其中 $\lambda_i$ 是按降序排列的[协方差矩阵](@entry_id:139155)的特征值。这个结论非常有用，因为它允许我们先对数据进行PCA分析，然后根据特征值的衰减情况，先验地确定满足给定误差容限所需的最小[潜空间](@entry_id:171820)维度 $k$，从而指导自编码器的设计。

#### 在[不确定性量化](@entry_id:138597)框架中集成机器学习代理

无论是物理驱动还是数据驱动，[机器学习代理模型](@entry_id:1127558)的一个主要用途是加速**[不确定性量化](@entry_id:138597)（Uncertainty Quantification, UQ）**。UQ旨在量化输入不确定性（如材料属性、边界条件的不确定性）如何传播到模型输出。这通常需要成千上万次[模型评估](@entry_id:164873)，对于昂贵的[物理模拟](@entry_id:144318)来说是不可行的。快速的ML代理模型使得这些计算成为可能。

##### 多级蒙特卡罗方法

**多级蒙特卡罗（Multi-Level Monte Carlo, MLMC）** 是一种强大的UQ技术，它通过结合不同保真度和成本的模型来有效估计[期望值](@entry_id:150961) ()。ML模型可以自然地融入这个框架，形成一个模型层级：
- **低保真度（$\ell=0$）**：一个非常快速但可能不太准确的ML代理模型。
- **中保真度（$\ell=1$）**：一个更精确的模型，例如PINN或经过校正的代理模型。
- **高保真度（$\ell=2$）**：完整的高保真[物理模拟](@entry_id:144318)（其本身也可能由ML加速）。

MLMC估计量是一个伸缩和：
$$
\widehat{Q} = \overline{Q_0} + (\overline{Q_1 - Q_0}) + (\overline{Q_2 - Q_1})
$$
其中大部分样本（$N_0$）用于廉价的低保真模型，而只有很少的样本（$N_2$）用于昂贵的高保真模型。通过优化每个层级上的样本数量 $N_\ell$，MLMC可以在满足给定统计误差容限的前提下，最小化总计算成本。最优样本数量 $N_\ell$ 与该层级差分项的方差 $\sqrt{V_\ell}$ 和每样本成本 $\sqrt{C_\ell}$ 的比值成正比：
$$
N_\ell \propto \sqrt{\frac{V_\ell}{C_\ell}}
$$
这个框架允许我们明智地分配计算资源，将高保真模拟的准确性与低保真代理的速度优势结合起来。

##### 验证、确认和不确定性量化（VVUQ）

在核安全等高风险领域，盲目信任一个机器学习模型是不可接受的。必须在一个严格的**验证、确认和不确定性量化（VVUQ）** 框架内评估和使用ML代理模型 ()。这涉及到对所有不确定性来源的系统性核算：
- **[偶然不确定性](@entry_id:634772)（Aleatoric Uncertainty）**：由系统固有的随机性引起，例如物理过程中的噪声。一些M[L模](@entry_id:1126990)型（如[贝叶斯神经网络](@entry_id:746725)）可以直接量化这种不确定性。
- **认知不确定性（Epistemic Uncertainty）**：源于我们知识的缺乏。对于ML模型，这表现为**[模型形式误差](@entry_id:274198)**或**[模型差异](@entry_id:198101)（model discrepancy）**，即代理模型与真实物理之间的系统性偏差。这种偏差必须通过与独立的实验数据进行**验证（validation）**来估计。
- **[数值误差](@entry_id:635587)**：源于物理模型的离散化和数值求解。这需要通过**确认（verification）**研究（如[网格收敛性](@entry_id:167447)分析）来估计。
- **场景不确定性**：由操作条件的变化引起。

在一个完整的UQ分析中，所有这些不确定性源（通常被建模为具有特定均值和方差的[随机变量](@entry_id:195330)）必须通过模型链传播到最终的关心量（Quantity of Interest, QoI）。例如，最终预测的QoI的均值不仅包括模型的名义预测，还必须包含由[模型差异](@entry_id:198101)引入的**偏置校正**。最终预测的方差是所有不确定性来源方差的（经[过敏](@entry_id:188097)感性加权的）总和。只有通过这种全面的VVUQ流程，我们才能建立对ML增强模拟结果的[置信度](@entry_id:267904)。