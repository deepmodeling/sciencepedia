{
    "hands_on_practices": [
        {
            "introduction": "神经网络的输出可以是任意值，但诸如中子通量和温度等物理量必须是非负的。我们可以通过在输出层使用特定的激活函数（如softplus函数）来强制实现这一约束。本练习  不仅仅是选择一个激活函数，它还展示了如何校准其参数以确保良好的训练动态（避免梯度消失），这是在实际应用中取得成功的关键一步。",
            "id": "4234298",
            "problem": "一个反应堆物理代理模型使用神经网络（NN）将空间-材料特征映射到两个物理约束的输出：稳态标量中子通量，表示为 $\\phi(\\mathbf{r})$，和冷却剂温度，表示为 $T(\\mathbf{r})$。物理可实现性要求 $\\phi(\\mathbf{r}) \\ge 0$ 和 $T(\\mathbf{r}) \\ge 0$。为了在网络输出端编码这些非负性约束，同时保留基于梯度的、偏微分方程（PDE）信息损失函数训练所需的光滑性，该模型在最后一层采用了一个参数化的softplus激活函数，\n$$\ns_{\\beta}(z) = \\frac{1}{\\beta}\\,\\ln\\!\\bigl(1 + \\exp(\\beta z)\\bigr),\n$$\n其中缩放参数 $\\beta > 0$。输出由 $\\phi(\\mathbf{r}) = s_{\\beta}(a_{\\phi}(\\mathbf{r}))$ 和 $T(\\mathbf{r}) = s_{\\beta}(a_{T}(\\mathbf{r}))$ 给出，其中 $a_{\\phi}$ 和 $a_{T}$ 是相应的预激活值。\n\n假设由于批量归一化（BN），预激活变量在初始化时近似服从零均值和单位方差的高斯分布，即 $a_{\\phi} \\sim \\mathcal{N}(0,1)$ 和 $a_{T} \\sim \\mathcal{N}(0,1)$。为了在物理信息训练中减轻梯度消失问题，要求在预激活分布上以 $p = 0.95$ 的概率，输出层局部梯度相对于其输入的幅值满足\n$$\n\\delta \\le \\frac{d}{dz} s_{\\beta}(z) \\le 1 - \\delta,\n$$\n其中 $\\delta = 0.05$。从基本定义和标准高斯性质出发，推导所需 $\\beta$ 的闭式表达式，该表达式应由标准正态累积分布函数及其反函数表示，然后在所述高斯假设下，根据给定的 $p$ 和 $\\delta$ 数值计算 $\\beta$。将您的数值答案四舍五入到四位有效数字。最终答案必须是一个无单位的实数。",
            "solution": "问题要求为一个用于反应堆物理模拟的神经网络代理模型中的参数化softplus激活函数 $s_{\\beta}(z)$ 推导参数 $\\beta$。该推导受制于对此激活函数梯度的概率约束。\n\n首先，我们确定给定的函数及其性质。参数化的softplus激活函数定义为：\n$$s_{\\beta}(z) = \\frac{1}{\\beta}\\,\\ln\\bigl(1 + \\exp(\\beta z)\\bigr)$$\n其中 $\\beta > 0$。局部梯度，即 $s_{\\beta}(z)$ 相对于其输入 $z$ 的导数，可以使用链式法则求得：\n$$\n\\frac{d}{dz} s_{\\beta}(z) = \\frac{d}{dz} \\left[ \\frac{1}{\\beta}\\,\\ln\\bigl(1 + \\exp(\\beta z)\\bigr) \\right] = \\frac{1}{\\beta} \\cdot \\frac{1}{1 + \\exp(\\beta z)} \\cdot \\frac{d}{dz}\\bigl(1 + \\exp(\\beta z)\\bigr)\n$$\n$$\n\\frac{d}{dz} s_{\\beta}(z) = \\frac{1}{\\beta} \\cdot \\frac{1}{1 + \\exp(\\beta z)} \\cdot \\bigl(\\beta \\exp(\\beta z)\\bigr) = \\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)}\n$$\n这个表达式是标准的logistic sigmoid函数，通常表示为 $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$。我们可以通过重写我们的表达式来看到这一点：\n$$\n\\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)} = \\frac{1}{\\frac{1 + \\exp(\\beta z)}{\\exp(\\beta z)}} = \\frac{1}{\\exp(-\\beta z) + 1} = \\sigma(\\beta z)\n$$\n问题对此梯度规定了一个约束，以防止其消失或饱和。约束条件是：\n$$\n\\delta \\le \\frac{d}{dz} s_{\\beta}(z) \\le 1 - \\delta\n$$\n代入导数的表达式，我们得到：\n$$\n\\delta \\le \\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)} \\le 1 - \\delta\n$$\n这个复合不等式可以分成两个独立的不等式。我们先解第一个：\n$$\n\\delta \\le \\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)}\n$$\n由于 $1 + \\exp(\\beta z)$ 总是正的，我们可以用它乘以不等式两边：\n$$\n\\delta(1 + \\exp(\\beta z)) \\le \\exp(\\beta z) \\implies \\delta + \\delta\\exp(\\beta z) \\le \\exp(\\beta z)\n$$\n$$\n\\delta \\le \\exp(\\beta z)(1 - \\delta) \\implies \\exp(\\beta z) \\ge \\frac{\\delta}{1 - \\delta}\n$$\n对两边取自然对数，这是一个单调递增函数：\n$$\n\\beta z \\ge \\ln\\left(\\frac{\\delta}{1 - \\delta}\\right)\n$$\n由于 $\\beta > 0$，我们可以除以 $\\beta$ 而不改变不等号方向：\n$$\nz \\ge \\frac{1}{\\beta} \\ln\\left(\\frac{\\delta}{1 - \\delta}\\right)\n$$\n现在，我们解第二个不等式：\n$$\n\\frac{\\exp(\\beta z)}{1 + \\exp(\\beta z)} \\le 1 - \\delta\n$$\n$$\n\\exp(\\beta z) \\le (1 - \\delta)(1 + \\exp(\\beta z)) \\implies \\exp(\\beta z) \\le 1 - \\delta + (1 - \\delta)\\exp(\\beta z)\n$$\n$$\n\\exp(\\beta z) - (1 - \\delta)\\exp(\\beta z) \\le 1 - \\delta \\implies \\exp(\\beta z)(1 - (1 - \\delta)) \\le 1 - \\delta\n$$\n$$\n\\delta \\exp(\\beta z) \\le 1 - \\delta \\implies \\exp(\\beta z) \\le \\frac{1 - \\delta}{\\delta}\n$$\n对两边取自然对数：\n$$\n\\beta z \\le \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)\n$$\n由于 $\\beta > 0$：\n$$\nz \\le \\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)\n$$\n结合两个结果，预激活值 $z$ 必须位于区间：\n$$\n\\frac{1}{\\beta} \\ln\\left(\\frac{\\delta}{1 - \\delta}\\right) \\le z \\le \\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)\n$$\n利用属性 $\\ln(1/x) = -\\ln(x)$，我们可以写出 $\\ln\\left(\\frac{\\delta}{1 - \\delta}\\right) = -\\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)$。我们定义 $z_{max} = \\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)$。$z$ 的区间变为 $[-z_{max}, z_{max}]$。\n\n问题陈述预激活值 $z$ 服从标准正态分布，$z \\sim \\mathcal{N}(0,1)$，并且 $z$ 落入此区间的概率为 $p$。\n$$\nP(-z_{max} \\le z \\le z_{max}) = p\n$$\n令 $\\Phi(x)$ 表示标准正态分布的累积分布函数（CDF）。该概率可以表示为：\n$$\nP(-z_{max} \\le z \\le z_{max}) = \\Phi(z_{max}) - \\Phi(-z_{max})\n$$\n利用标准正态分布的对称性 $\\Phi(-x) = 1 - \\Phi(x)$，我们有：\n$$\n\\Phi(z_{max}) - (1 - \\Phi(z_{max})) = 2\\Phi(z_{max}) - 1 = p\n$$\n解出 $\\Phi(z_{max})$：\n$$\n\\Phi(z_{max}) = \\frac{1 + p}{2}\n$$\n为了求 $z_{max}$，我们应用标准正态CDF的反函数 $\\Phi^{-1}$：\n$$\nz_{max} = \\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)\n$$\n我们现在有两个关于 $z_{max}$ 的表达式。将它们相等可以让我们解出 $\\beta$：\n$$\n\\frac{1}{\\beta} \\ln\\left(\\frac{1 - \\delta}{\\delta}\\right) = \\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)\n$$\n解出 $\\beta$，我们得到所需的闭式表达式：\n$$\n\\beta = \\frac{\\ln\\left(\\frac{1 - \\delta}{\\delta}\\right)}{\\Phi^{-1}\\left(\\frac{1 + p}{2}\\right)}\n$$\n现在，我们代入问题中给出的数值：$p = 0.95$ 和 $\\delta = 0.05$。\n首先，我们计算对数的参数：\n$$\n\\frac{1 - \\delta}{\\delta} = \\frac{1 - 0.05}{0.05} = \\frac{0.95}{0.05} = 19\n$$\n$\\beta$ 表达式的分子是 $\\ln(19)$。\n接下来，我们计算反CDF的参数：\n$$\n\\frac{1 + p}{2} = \\frac{1 + 0.95}{2} = \\frac{1.95}{2} = 0.975\n$$\n分母是 $\\Phi^{-1}(0.975)$。这是标准正态分布在 95% 置信水平下的上临界值。标准统计表或计算得出该值约为 $1.959964$。\n现在我们计算 $\\beta$：\n$$\n\\beta = \\frac{\\ln(19)}{\\Phi^{-1}(0.975)} \\approx \\frac{2.944438979}{1.959963985} \\approx 1.50229204\n$$\n问题要求将答案四舍五入到四位有效数字。前四位有效数字是 $1$、$5$、$0$ 和 $2$。第五位有效数字是 $2$，小于 $5$，因此我们向下舍入。\n$\\beta$ 的最终数值是 $1.502$。",
            "answer": "$$\\boxed{1.502}$$"
        },
        {
            "introduction": "除了像非负性这样的简单约束外，我们通常还知道系统应如何定性地响应变化，例如，反应性会随着燃料富集度的增加而单调增加。我们可以通过在损失函数中添加一个惩罚项（正则化器）来“教会”神经网络这种行为，该惩罚项会对不符合物理规律的趋势进行惩罚。这项练习  演示了如何将更深层次的物理直觉嵌入到数据驱动模型中，使其更加稳健和准确，并且您将推导此正则化器的梯度，这是指导网络训练的核心机制。",
            "id": "4234321",
            "problem": "一个由权重和偏置 $\\boldsymbol{\\theta}$ 参数化的深度神经网络，表示为 $f_{\\boldsymbol{\\theta}}:\\mathbb{R}^{d}\\to\\mathbb{R}$，被训练用作压水反应堆稳态堆芯模型中有效中子倍增因子 $k_{\\mathrm{eff}}$ 的代理模型。输入特征向量 $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ 包括铀-$\\mathrm{^{235}U}$ 富集度 $e$ (质量分数) 和可溶性硼吸收剂浓度 $a$ (单位体积摩尔数)，以及在此任务中保持固定的其他热工水力和几何特征。根据反应堆物理学，$k_{\\mathrm{eff}}$ 随 $e$ 单调增加，随 $a$ 单调减少，因为富集度增加了每一代中子的平均裂变数，而吸收剂含量增加了无裂变的中子俘获。训练集由 $N$ 个基态 $\\{\\boldsymbol{x}_{i}\\}_{i=1}^{N}$ 组成，其中每个 $\\boldsymbol{x}_{i}$ 包含 $(e_{i},a_{i})$ 和固定的上下文特征。对每个基态，通过对富集度和吸收剂组分专门施加小的受控扰动 $\\Delta e>0$ 和 $\\Delta a>0$ 来定义两个增广状态：\n$$\n\\boldsymbol{x}_{i}^{e+} := \\boldsymbol{x}_{i}\\ \\text{with $e$ replaced by $e_{i}+\\Delta e$},\\qquad\n\\boldsymbol{x}_{i}^{a+} := \\boldsymbol{x}_{i}\\ \\text{with $a$ replaced by $a_{i}+\\Delta a$}.\n$$\n您的任务是构建一个可微的单调性正则化器 $R(\\boldsymbol{\\theta})$，该正则化器惩罚对物理预期的局部单调行为的偏离，即对每个 $i$ 都有 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})\\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$ 和 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+})\\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$。使用独立的非负权重 $\\lambda_{e}$ 和 $\\lambda_{a}$ 来控制富集度和吸收剂单调性分量的强度。使用平方铰链惩罚（squared hinge penalty）来构造 $R(\\boldsymbol{\\theta})$，该惩罚作用于带符号的有限差分违规量，使得当不等式满足时惩罚为零，否则呈二次方增长。然后，仅从此正则化器的定义出发，推导其梯度 $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ 的闭式解析表达式，该表达式应以在 $\\boldsymbol{x}_{i}$、$\\boldsymbol{x}_{i}^{e+}$ 和 $\\boldsymbol{x}_{i}^{a+}$ 处求值的 $\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ 来表示。使用标准函数表示最终梯度，并通过亥维赛阶跃函数（Heaviside step function）明确指示任何指示函数行为。\n\n您的最终答案必须是 $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ 的单个闭式解析表达式。不需要进行数值计算。",
            "solution": "我们的目标是构建单调性正则化器 $R(\\boldsymbol{\\theta})$ 并推导其关于网络参数 $\\boldsymbol{\\theta}$ 的梯度 $\\nabla_{\\boldsymbol{\\theta}}R(\\boldsymbol{\\theta})$。\n\n**1. 正则化器 $R(\\boldsymbol{\\theta})$ 的构建**\n\n正则化器旨在惩罚对两个物理单调性条件的违背。我们将使用平方铰链惩罚函数。对于一个应满足 $v \\ge 0$ 的量，其惩罚形式为 $(\\max(0, -v))^2$。当条件满足时（$v \\ge 0$），惩罚为零；当条件被违背时（$v < 0$），惩罚为 $v^2$。\n\n首先，考虑关于富集度 $e$ 的单调性条件：$f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})\\ge f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$。这等价于 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) \\ge 0$。违背此条件意味着该差值为负。因此，需要惩罚的违规量是 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$。对于样本 $i$，此部分的惩罚项为 $(\\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})))^2$。\n\n其次，考虑关于吸收剂浓度 $a$ 的单调性条件：$f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+})\\le f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$。这等价于 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) \\ge 0$。违背此条件意味着该差值为负。因此，需要惩罚的违规量是 $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$。对于样本 $i$，此部分的惩罚项为 $(\\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})))^2$。\n\n总正则化器 $R(\\boldsymbol{\\theta})$ 是对所有 $N$ 个训练样本的这些惩罚项的加权和：\n$$\nR(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\left( \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) \\right)^2 + \\lambda_{a} \\left( \\max(0, f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right)^2 \\right]\n$$\n\n**2. 梯度 $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ 的推导**\n\n为了计算 $R(\\boldsymbol{\\theta})$ 的梯度，我们应用链式法则。考虑一个通用的平方铰链损失项 $L(v) = (\\max(0, v))^2$，其中 $v$ 是 $\\boldsymbol{\\theta}$ 的函数。其梯度为 $\\nabla_{\\boldsymbol{\\theta}} L(v(\\boldsymbol{\\theta})) = \\frac{dL}{dv} \\nabla_{\\boldsymbol{\\theta}} v(\\boldsymbol{\\theta})$。\n\n$L(v)$ 对 $v$ 的导数是 $2\\max(0, v)$。因此，我们可以对 $R(\\boldsymbol{\\theta})$ 的表达式逐项求导。\n\n我们定义违规项如下：\n- 富集度违规项: $v_{e,i}(\\boldsymbol{\\theta}) = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$\n- 吸收剂违规项: $v_{a,i}(\\boldsymbol{\\theta}) = f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$\n\n应用链式法则于 $R(\\boldsymbol{\\theta})$：\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\cdot 2 \\max(0, v_{e,i}) \\nabla_{\\boldsymbol{\\theta}} v_{e,i} + \\lambda_{a} \\cdot 2 \\max(0, v_{a,i}) \\nabla_{\\boldsymbol{\\theta}} v_{a,i} \\right]\n$$\n接下来，计算违规项的梯度：\n- $\\nabla_{\\boldsymbol{\\theta}} v_{e,i} = \\nabla_{\\boldsymbol{\\theta}} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) = \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})$\n- $\\nabla_{\\boldsymbol{\\theta}} v_{a,i} = \\nabla_{\\boldsymbol{\\theta}} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) = \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$\n\n将这些梯度代入 $\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta})$ 的表达式中：\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = 2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} \\max(0, v_{e,i}) (\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) + \\lambda_{a} \\max(0, v_{a,i}) (\\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}}f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n$$\n问题要求使用亥维赛阶跃函数 $H(z)$ 来表示指示函数行为，其中当 $z > 0$ 时 $H(z)=1$，否则 $H(z)=0$。函数 $\\max(0, v)$ 可以写成 $v \\cdot H(v)$。将此形式代入，我们得到最终的梯度表达式：\n$$\n\\nabla_{\\boldsymbol{\\theta}} R(\\boldsymbol{\\theta}) = 2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) \\right.\n$$\n$$\n\\left. + \\lambda_{a} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n$$\n该表达式即为所求的正则化器梯度的闭式解析形式。",
            "answer": "$$\n\\boxed{\n2 \\sum_{i=1}^{N} \\left[ \\lambda_{e} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{e+})) + \\lambda_{a} (f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) H(f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) (\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}^{a+}) - \\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})) \\right]\n}\n$$"
        },
        {
            "introduction": "神经网络能否直接求解控制反应堆行为的微分方程？物理信息神经网络（Physics-Informed Neural Networks, PINNs）正是为此而设计的。PINNs的训练不仅依赖于数据，还依赖于一个核心要求：它们必须满足控制物理过程的方程。这项练习  将指导您为反应堆点动力学方程（核工程中的一个基石模型）构建一个完整的PINN损失函数，这项技能是将PINNs应用于各种物理和工程问题的基础。",
            "id": "4234282",
            "problem": "考虑具有 $m$ 个缓发中子先驱核群的含时点堆动力学模型。中子布居 $n(t)$ 和缓发中子先驱核浓度 $C_i(t)$（其中 $i \\in \\{1,2,\\dots,m\\}$）的演化遵循从 中子平衡 和 先驱核衰变-产生 过程推导出的广为接受的点堆动力学常微分方程：\n$$\n\\dot{n}(t) = \\frac{\\rho(t) - \\beta}{\\Lambda}\\, n(t) + \\sum_{i=1}^{m} \\lambda_i\\, C_i(t),\n$$\n$$\n\\dot{C}_i(t) = \\frac{\\beta_i}{\\Lambda}\\, n(t) - \\lambda_i\\, C_i(t),\n$$\n其中 $\\rho(t)$ 是反应性，$\\beta = \\sum_{i=1}^{m} \\beta_i$ 是总缓发中子份额，$\\Lambda$ 是瞬发中子代长时间，$\\lambda_i$ 是先驱核衰变常数。假设 $\\rho(t)$ 是在区间 $[0,T]$ 上的已知可微函数，且所有参数 $\\beta_i$、$\\lambda_i$、$\\Lambda$ 均为已知的正常数。初始条件 $n(0)=n_0$ 和 $C_i(0)=C_{i0}$ 已给定。\n\n物理信息神经网络 (PINN) 使用自动微分 (AD) 来近似求解，它将 $n(t)$ 和 $C_i(t)$ 参数化为 $n_{\\theta}(t)$ 和 $C_{i,\\theta}(t)$，其中 $\\theta$ 是可训练的网络参数。令 $\\{t_j\\}_{j=1}^{N}$ 为 $(0,T)$ 内用于强制执行物理定律的内部配置点，并可选地令 $\\{t_k\\}_{k=1}^{K}$ 为测量时间点，在这些时间点上 $n(t)$ 具有观测值 $n^{\\mathrm{obs}}(t_k)$。\n\n请仅使用点堆动力学方程中编码的中子平衡原理，以及物理信息神经网络残差的定义（即通过自动微分计算的时间导数与模型右端项之间的不匹配），推导出一个加权均方复合损失函数 $L(\\theta)$ 的显式解析表达式。该损失函数应惩罚以下各项：\n- 在 $\\{t_j\\}$ 上的中子动力学残差，\n- 对于所有 $i \\in \\{1,\\dots,m\\}$，在 $\\{t_j\\}$ 上的每个先驱核残差，\n- 在 $t=0$ 时的初始条件不匹配，\n- 以及在测量时间 $\\{t_k\\}$ 上的数据不匹配。\n\n您的损失函数必须是关于配置点集和测量点集求和的单个闭式表达式，其中明确包含非负权重 $w_n$、$w_{C_i}$、$w_{\\mathrm{ic},n}$、$w_{\\mathrm{ic},C_i}$ 和 $w_d$，并且必须使用根据给定微分方程定义的残差。请以符号形式表示最终的损失 $L(\\theta)$，不要代入数值。最终答案必须是单个闭式解析表达式。最终答案中不应包含任何单位。",
            "solution": "我们的任务是为求解点堆动力学方程的PINN推导一个复合损失函数 $L(\\theta)$。该函数是用于惩罚物理定律、初始条件和观测数据偏离的均方误差的加权和。神经网络对中子布居和先驱核浓度的近似分别表示为 $n_{\\theta}(t)$ 和 $C_{i,\\theta}(t)$。\n\n首先，我们为控制常微分方程(ODEs)定义残差。残差衡量了神经网络近似解违反微分方程的程度。\n\n中子布居方程的残差 $r_n(t; \\theta)$ 通过将网络近似值代入第一个ODE得到：\n$$\nr_n(t; \\theta) = \\frac{d n_{\\theta}}{dt}(t) - \\left( \\frac{\\rho(t) - \\beta}{\\Lambda}\\, n_{\\theta}(t) + \\sum_{i=1}^{m} \\lambda_i\\, C_{i,\\theta}(t) \\right)\n$$\n同样，第 $i$ 个缓发中子先驱核群的残差 $r_{C_i}(t; \\theta)$ 定义为：\n$$\nr_{C_i}(t; \\theta) = \\frac{d C_{i,\\theta}}{dt}(t) - \\left( \\frac{\\beta_i}{\\Lambda}\\, n_{\\theta}(t) - \\lambda_i\\, C_{i,\\theta}(t) \\right) \\quad \\text{for } i \\in \\{1, 2, \\dots, m\\}\n$$\n总损失函数 $L(\\theta)$ 是四个不同部分的加权和：\n\n1.  **物理残差损失**：这部分惩罚在一组 $N$ 个内部配置点 $\\{t_j\\}_{j=1}^{N}$ 上对ODE的违反。它由中子动力学残差和所有先驱核残差的加权均方误差组成。\n    $$\n    L_{\\text{phys}}(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( r_n(t_j; \\theta) \\right)^2 + \\sum_{i=1}^{m} w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( r_{C_i}(t_j; \\theta) \\right)^2\n    $$\n    代入残差的表达式，得到：\n    $$\n    w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2\n    $$\n\n2.  **初始条件损失 ($L_{\\mathrm{ic}}$)**：这部分惩罚网络在 $t=0$ 时的预测值与给定初始条件 $n(0)=n_0$ 和 $C_i(0)=C_{i0}$ 之间的不匹配。这是加权平方误差的总和。\n    $$\n    L_{\\mathrm{ic}}(\\theta) = w_{\\mathrm{ic},n} \\left( n_{\\theta}(0) - n_0 \\right)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} \\left( C_{i,\\theta}(0) - C_{i0} \\right)^2\n    $$\n\n3.  **数据不匹配损失 ($L_d$)**：这部分惩罚在 $K$ 个测量时间点 $\\{t_k\\}_{k=1}^{K}$ 上，网络对中子布居的预测值 $n_{\\theta}(t_k)$ 与观测值 $n^{\\mathrm{obs}}(t_k)$ 之间的偏差。这是加权均方误差。\n    $$\n    L_d(\\theta) = w_d \\frac{1}{K} \\sum_{k=1}^{K} \\left( n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k) \\right)^2\n$$\n\n最后，总复合损失函数 $L(\\theta)$ 是这些单个分量的和：\n$$\nL(\\theta) = L_{\\text{phys}}(\\theta) + L_{\\mathrm{ic}}(\\theta) + L_d(\\theta)\n$$\n将所有项组合起来，得到损失函数的最终显式解析表达式：\n$$\nL(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} \\left( w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2 \\right) + w_{\\mathrm{ic},n} (n_{\\theta}(0) - n_0)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} (C_{i,\\theta}(0) - C_{i0})^2 + w_d \\frac{1}{K} \\sum_{k=1}^{K} (n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k))^2\n$$",
            "answer": "$$\n\\boxed{\nL(\\theta) = w_n \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d n_{\\theta}}{dt}(t_j) - \\frac{\\rho(t_j) - \\beta}{\\Lambda} n_{\\theta}(t_j) - \\sum_{i=1}^{m} \\lambda_i C_{i,\\theta}(t_j) \\right)^2 + \\sum_{i=1}^{m} \\left( w_{C_i} \\frac{1}{N} \\sum_{j=1}^{N} \\left( \\frac{d C_{i,\\theta}}{dt}(t_j) - \\frac{\\beta_i}{\\Lambda} n_{\\theta}(t_j) + \\lambda_i C_{i,\\theta}(t_j) \\right)^2 \\right) + w_{\\mathrm{ic},n} (n_{\\theta}(0) - n_0)^2 + \\sum_{i=1}^{m} w_{\\mathrm{ic},C_i} (C_{i,\\theta}(0) - C_{i0})^2 + w_d \\frac{1}{K} \\sum_{k=1}^{K} (n_{\\theta}(t_k) - n^{\\mathrm{obs}}(t_k))^2\n}\n$$"
        }
    ]
}