## Applications and Interdisciplinary Connections

Having established the fundamental principles and mechanisms of machine learning (ML) and neural networks in the preceding chapters, we now turn our attention to their practical application. This chapter explores how these powerful computational tools are being leveraged to address complex, real-world challenges across nuclear science and engineering. The objective is not to reiterate the core theory but to demonstrate its utility, versatility, and integration within diverse, interdisciplinary contexts. We will examine how ML techniques are accelerating computationally intensive simulations, enabling the solution of challenging inverse problems, enhancing [reactor diagnostics](@entry_id:1130673) and control, and providing new avenues for modeling physics on complex geometries. Through this survey, the profound impact of machine learning on the present and future of the nuclear field will become evident.

### Surrogate Modeling for Accelerated Simulation and Uncertainty Quantification

A pervasive challenge in nuclear engineering is the high computational cost of high-fidelity [multiphysics](@entry_id:164478) simulations. Simulators based on Monte Carlo transport, computational fluid dynamics (CFD), or [finite element analysis](@entry_id:138109) (FEA) can require hours or days to solve for a single set of input parameters. This cost becomes prohibitive for applications requiring many thousands of evaluations, such as uncertainty quantification (UQ), design optimization, and sensitivity analysis. Machine learning offers a powerful solution through [surrogate modeling](@entry_id:145866): training a fast-to-evaluate ML model to approximate the input-output map of a slow, high-fidelity code.

A primary application of surrogates is in [uncertainty quantification](@entry_id:138597). For instance, consider estimating the statistical distribution of the peak fuel centerline temperature in a reactor, given that the coolant inlet temperature and mass flux are subject to operational uncertainties. A traditional Monte Carlo approach would require running the thermal-hydraulics simulator thousands of times. A surrogate, trained on a small, smartly chosen set of high-fidelity runs, can perform these evaluations nearly instantly. Models like Gaussian Processes (GPs) are particularly well-suited for this, as they provide not only a mean prediction but also a predictive variance, quantifying the surrogate's own uncertainty. This is a crucial distinction from deterministic surrogates like Polynomial Chaos Expansions (PCEs), as a GP-based UQ analysis can naturally propagate both the input physical uncertainty and the surrogate's epistemic uncertainty. Rigorous comparison between different surrogate models is essential and can be accomplished using unified metrics such as the [leave-one-out cross-validation](@entry_id:633953) error, which can be computed efficiently for linear smoothers like GPs and regularized PCEs without repeated model refitting .

The concept of [surrogate modeling](@entry_id:145866) can be extended to fuse information from simulations of varying fidelity and cost. In thermal-hydraulics, for example, one might have access to a few expensive, high-fidelity Direct Numerical Simulation (DNS) results, a larger set of medium-fidelity Large Eddy Simulation (LES) runs, and an even larger database of low-fidelity Reynolds-Averaged Navier-Stokes (RANS) simulations. A multi-fidelity surrogate model, such as one based on a hierarchical [co-kriging](@entry_id:747413) (Gaussian Process) framework, can learn the relationships and discrepancies between these different levels. It models the high-fidelity output as a function of the low-fidelity output plus a learned discrepancy function. This allows the abundant low-fidelity data to inform the model about the general trends of the system, while the sparse high-fidelity data serves to correct for the biases of the cheaper models. Such an approach enables more accurate predictions than could be achieved using any single-fidelity dataset alone .

Furthermore, the uncertainty estimates provided by surrogates can be used to actively guide the [data acquisition](@entry_id:273490) process itself. In an "[active learning](@entry_id:157812)" or "[optimal experimental design](@entry_id:165340)" framework, the surrogate's predictive uncertainty is used to decide where to perform the next expensive high-fidelity simulation to be most informative. For instance, if a deep neural network surrogate is used to predict a critical reactor parameter, like the prompt neutron exponent $\alpha$ during a fast transient, one can design the optimal time to take a physical measurement to maximally reduce the posterior uncertainty on $\alpha$. By formulating a metric like the Fisher Information, which quantifies the amount of information an observation carries about a parameter, one can solve a robust design problem to find the experimental conditions that maximize the worst-case information gain over the parameter's [credible interval](@entry_id:175131). This creates an intelligent, closed loop where the ML model actively helps to improve itself in the most efficient manner .

Finally, for surrogates to be practical, they must be adaptable. A model trained for one type of fuel, such as uranium dioxide (UOX), may not be accurate for another, like mixed-oxide (MOX) fuel. Retraining from scratch on the new fuel type may not be feasible if data is limited. Here, [transfer learning](@entry_id:178540) provides a solution. By understanding which parts of the neural network architecture correspond to which parts of the underlying physical operator (e.g., the neutron diffusion equation), one can devise a principled [fine-tuning](@entry_id:159910) strategy. The deeper convolutional layers of a network often learn to represent the fundamental, geometry-dependent spatial operators (like the Laplacian), which are invariant between fuel types. In contrast, shallower layers, such as $1 \times 1$ convolutions that mix energy groups or feature-wise modulation layers, learn to represent the material-dependent coefficients ([cross-sections](@entry_id:168295)). By freezing the geometry-learning layers and fine-tuning only the material-learning layers on a small dataset for the new fuel, the model can be adapted efficiently and effectively .

### Physics-Informed Neural Networks and Operator Learning

While surrogate models learn the input-output map of an existing solver, Physics-Informed Neural Networks (PINNs) represent a paradigm shift where the neural network itself is used to solve the governing partial differential equations (PDEs). A PINN is trained to minimize a loss function that includes not only discrepancies with known data (e.g., boundary conditions) but also the residual of the PDEs themselves, evaluated at a large number of collocation points in the domain. Automatic differentiation is used to compute the derivatives of the network's output with respect to its inputs, allowing for the direct encoding of [differential operators](@entry_id:275037) in the loss.

This approach is particularly powerful for [coupled multiphysics](@entry_id:747969) problems, which are ubiquitous in reactor analysis. For example, a single neural network architecture can be designed to output both the neutron flux field $\phi(x)$ and the temperature field $T(x)$ in a reactor core. The loss function would then incorporate the residuals of both the neutron diffusion equation and the [heat conduction equation](@entry_id:1125966), including the physical coupling terms like temperature-dependent [cross-sections](@entry_id:168295). Successful training of such a composite PINN often requires careful formulation of the loss, such as using characteristic physical scales to non-dimensionalize the PDE residuals. This ensures that the gradients from different physical terms are of comparable magnitude, preventing one part of the physics from dominating the training process. An appropriate balance can be found by analyzing the magnitude of the residuals at initialization .

The PINN paradigm of solving a single instance of a PDE can be generalized to the more ambitious goal of learning the entire solution operator. An operator network learns a mapping between infinite-dimensional [function spaces](@entry_id:143478), for instance, learning the operator that maps any valid material property function and boundary condition function to the corresponding solution field. Architectures like the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO) are designed for this task. DeepONet uses a "branch" network to encode the input function and a "trunk" network to encode the output spatial coordinates, while FNO learns a [convolution kernel](@entry_id:1123051) in Fourier space, allowing it to perform nonlocal transformations efficiently. A key advantage of FNOs is their discretization-invariance, meaning a model trained on one mesh resolution can be evaluated on another. In nuclear engineering, these networks can learn the solution operator for problems like the thermo-mechanical response of fuel cladding, mapping a field of material properties to a field of displacements and stresses, which is invaluable for inverse problems and design optimization .

The choice of ML architecture must be guided by the underlying physics. In the challenging domain of turbulence modeling for thermal-hydraulics, such as in Large-Eddy Simulation (LES), the closure problem for the subgrid-scale (SGS) stress tensor is fundamentally nonlocal. That is, the stress at a point depends on the state of the resolved flow in a finite neighborhood around it. A standard PINN that assumes a local closure (e.g., an eddy-viscosity model) may only be feasible in idealized regimes with large separation between resolved and unresolved scales. In contrast, an operator network, which is inherently designed to capture nonlocal function-to-function mappings, is a more natural and powerful choice for learning SGS [closures](@entry_id:747387) in complex, anisotropic flows typical of reactor components .

### Inverse Problems, Data Assimilation, and Field Reconstruction

In contrast to [forward problems](@entry_id:749532), where one predicts effects from known causes, [inverse problems](@entry_id:143129) seek to infer unknown causes from observed effects. This is a common task in reactor analysis, from characterizing material properties to locating sources. Machine learning, particularly when combined with Bayesian inference, provides a powerful framework for tackling these often [ill-posed problems](@entry_id:182873).

A common scenario involves using real-time sensor data to update and correct the internal state of a simulation or digital twin, a process known as data assimilation. A neural network surrogate can serve as the forward model within a Bayesian data assimilation scheme. For instance, if a set of detector readings differs from the surrogate's prediction, Bayes' theorem can be used to update the prior belief about an unknown physical parameter, such as the macroscopic [absorption cross-section](@entry_id:172609), to a posterior belief that better explains the measurements. Under linear-Gaussian assumptions, this update can be computed analytically, yielding the Maximum A Posteriori (MAP) estimate of the parameter that optimally fuses the prior knowledge encoded in the model with the information from the sensor data .

Another class of inverse problems is field reconstruction, where the goal is to infer a high-resolution data field from sparse or coarse measurements. A prominent example in core physics is the reconstruction of a fine-grained, pin-by-pin power map from coarse, assembly-averaged measurements. Convolutional Neural Networks (CNNs), with their hierarchical feature-learning capabilities, are exceptionally well-suited for this [image-to-image translation](@entry_id:636973) task, often termed "super-resolution". A successful design, however, must be informed by physics. For example, a custom normalization layer can be added to the end of the network to enforce a critical physical constraint: that the sum of the reconstructed pin powers within an assembly must equal the measured assembly-[average power](@entry_id:271791). Furthermore, principles from signal processing, such as the Nyquist-Shannon [sampling theorem](@entry_id:262499), can be used to justify the network's [upsampling](@entry_id:275608) architecture. By analyzing the spatial power spectrum of the underlying physics, one can determine the necessary output grid resolution to represent all significant physical features without aliasing, thus providing a rigorous, physics-based foundation for the network design .

### Diagnostics, Prognostics, and Autonomous Control

The creation of a "digital twin"—a living, virtual counterpart of a physical system that is continuously updated with real-world data—is a central goal in modern engineering. Machine learning is the engine that drives the functionality of these twins, enabling advanced diagnostics, prognostics, and eventually, autonomous control.

Anomaly detection is a critical diagnostic task for ensuring reactor safety. By training a deep generative model, such as a Variational Autoencoder (VAE), exclusively on [time-series data](@entry_id:262935) from nominal (normal) operations, the model learns a low-dimensional representation of "normalcy". When presented with new data, the model's reconstruction error will be low for normal states but high for anomalous states it has never seen before. This reconstruction error serves as a powerful anomaly score. To deploy such a system, one must set a decision threshold with a principled statistical basis to control the false alarm rate. Methods ranging from non-parametric empirical quantile estimation to the more sophisticated Extreme Value Theory (EVT), which models the extreme tails of the score distribution, can be used to set this threshold in a statistically rigorous manner .

Digital twins excel at fusing data from heterogeneous sensors to maintain an accurate estimate of the system's true state. This process can be formalized within a linear-Gaussian framework, where the state estimate from the model acts as a prior, and each sensor measurement is used to form a likelihood. The combined posterior provides an updated state. This same framework provides a natural mechanism for [anomaly detection](@entry_id:634040) through the "innovation" vector—the difference between the actual measurement and the model's prediction. The Mahalanobis distance of this vector, scaled by its expected covariance, forms a [test statistic](@entry_id:167372) that, under nominal conditions, follows a chi-squared ($\chi^2$) distribution. A measurement yielding a statistic that falls in the tail of this distribution is flagged as anomalous, providing a unified framework for both state estimation and [fault detection](@entry_id:270968) .

Beyond detecting current faults, a key goal is prognostics: predicting the future health and remaining useful life of components. A major challenge for ML-based prognostics is extrapolation, as the model must predict [damage evolution](@entry_id:184965) far beyond the time scales present in its training data. Naive extrapolation is notoriously unreliable. The key to building robust prognostic models is to bake in physical knowledge. For cumulative damage processes, this often takes the form of a monotonicity constraint: the damage state can only increase or stay constant with usage. This constraint can be enforced "by construction" within a neural network, for example, by designing a network with non-negative weights and non-decreasing [activation functions](@entry_id:141784), or by modeling the damage *rate* with a strictly non-negative network (e.g., using a softplus activation) and integrating it over time. Such physics-constrained models are far more likely to produce physically plausible and reliable extrapolations .

Ultimately, the goal of a digital twin is to enable intelligent, autonomous control. Reinforcement Learning (RL) provides a framework for training an agent to make optimal decisions to achieve a long-term goal. Formulating a reactor control problem as a Markov Decision Process (MDP) is the first step. This requires careful selection of the state vector. To satisfy the Markov property—that the future depends only on the present state, not the past—the state vector must include all variables whose history influences future dynamics. For a simple point-kinetics reactor model, this includes not only power and temperature but also the precursor concentrations and control rod positions .

For more complex, safety-critical tasks like suppressing xenon-induced power oscillations, a more advanced formulation like a Constrained Markov Decision Process (CMDP) is required. Here, the agent learns to maximize a [reward function](@entry_id:138436) (e.g., by penalizing axial power oscillations) while being explicitly forbidden from violating a set of safety constraints (e.g., a hard limit on the core [power peaking factor](@entry_id:1130053)). The design of a successful CMDP requires deep physical insight: the state must be Markovian (including both xenon and its precursor, [iodine](@entry_id:148908)), the action space must respect physical actuator limits (e.g., rod speed and position limits), and the reward and constraints must precisely encode the operational objectives and safety envelope .

### Advanced Geometries and Graph Neural Networks

Many problems in nuclear science and engineering are defined on domains with irregular or non-Euclidean geometries. Examples include unstructured meshes used in CFD and neutronics codes, or the chart of nuclides itself, which forms an irregular "peninsula" in the plane of proton ($Z$) and neutron ($N$) numbers. While CNNs are powerful, they are designed for regular, grid-like data and struggle with such domains. Applying a CNN to an irregular domain often requires embedding it in a larger rectangular grid and "padding" the missing values, a process that introduces artificial data and can severely bias the model, especially near the boundaries.

Graph Neural Networks (GNNs) have emerged as the natural tool for learning on such irregular structures. In a GNN, the data is represented as a graph, where nodes hold features and edges define their relationships. For example, to predict nuclear masses, each known nucleus $(Z,N)$ can be a node, with edges connecting it to its physically adjacent neighbors (nuclei with $Z \pm 1$ or $N \pm 1$). GNNs operate by passing "messages" between connected nodes, allowing the network to learn from the local topology of the graph. This approach respects the true structure of the domain, as information is only propagated between physically existing entities. This is conceptually linked to classical physics, where the graph Laplacian operator, central to [semi-supervised learning](@entry_id:636420) on graphs, is the natural discrete counterpart to the continuous Laplacian operator for diffusion-like processes on irregular domains. The principles of GNNs, demonstrated in nuclear physics for mass predictions, are directly applicable to engineering problems like solving PDEs on unstructured meshes in a reactor core .