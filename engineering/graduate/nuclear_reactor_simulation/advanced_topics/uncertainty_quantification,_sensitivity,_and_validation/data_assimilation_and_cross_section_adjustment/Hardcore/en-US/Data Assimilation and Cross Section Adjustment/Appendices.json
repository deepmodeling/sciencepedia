{
    "hands_on_practices": [
        {
            "introduction": "At the heart of data assimilation is the Bayesian update, where prior knowledge is formally combined with new measurements to yield an improved posterior estimate. This fundamental exercise () guides you through deriving the posterior mean and covariance for a simplified reactor physics problem. By working through the derivation and a numerical example, you will gain hands-on practice with the core mathematical machinery of cross section adjustment.",
            "id": "4221177",
            "problem": "In a two-group infinite-medium nuclear reactor simulation, a reactor physicist seeks to assimilate a single scalar in-core detector reading to adjust the macroscopic absorption cross sections. Let the state vector be $x = [\\Sigma_{a,1}, \\Sigma_{a,2}]^{T}$, where $\\Sigma_{a,1}$ and $\\Sigma_{a,2}$ are the group-wise macroscopic absorption cross sections in barns. The model-predicted scalar observable is $y = h(x)$, where $h$ represents the detector response model. The background (prior) distribution for $x$ is Gaussian with mean $x_{b}$ and covariance matrix $B$, and the measurement error is Gaussian with zero mean and variance $R$.\n\nThe observation operator is nonlinear in general, but for analysis near the background state, consider the first-order Taylor approximation of $h(x)$ around $x_{b}$:\n$$\ny \\approx h(x_{b}) + g^{T}(x - x_{b}),\n$$\nwhere $g$ is the gradient of $h$ with respect to $x$ evaluated at $x_{b}$.\n\nUsing Bayes' theorem and treating the linearization as locally valid, do the following:\n\n1) Starting from the Gaussian prior for $x$ and the Gaussian likelihood for the scalar observation $y_{o}$ under the linearized model, derive the approximate posterior density $p(x \\mid y_{o})$. Your derivation must identify the posterior mean and posterior covariance in closed form in terms of $x_{b}$, $B$, $g$, $R$, $h(x_{b})$, and $y_{o}$.\n\n2) Explain why, for the exactly linear scalar observation model $y = h(x_{b}) + g^{T}(x - x_{b}) + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,R)$, the posterior obtained in part 1) is in fact the exact posterior, and state the same closed-form expressions for the posterior mean and posterior covariance.\n\n3) Now instantiate a physically plausible data assimilation for cross section adjustment with the following numerical values:\n- Background mean $x_{b} = \\begin{pmatrix} 0.25 \\\\ 0.10 \\end{pmatrix}$ barns,\n- Background covariance $B = \\begin{pmatrix} (0.04)^{2} & 0.5 \\times 0.04 \\times 0.02 \\\\ 0.5 \\times 0.04 \\times 0.02 & (0.02)^{2} \\end{pmatrix}$ barns$^{2}$,\n- Gradient $g = \\begin{pmatrix} -1.8 \\\\ -0.6 \\end{pmatrix}$ in units of the observable per barn,\n- Measurement error variance $R = (0.01)^{2}$,\n- Model evaluation at background $h(x_{b}) = 1.000$,\n- Measured value $y_{o} = 1.015$.\n\nUsing your derived formulas, compute the posterior mean vector for $x$ under the linearized model. Express your final numerical answer as the two components of the posterior mean in barns. Round your answer to four significant figures.",
            "solution": "The user has provided a problem on Bayesian data assimilation for nuclear reactor cross section adjustment. The problem is to be validated before solving.\n\n### Step 1: Extract Givens\n- State vector: $x = [\\Sigma_{a,1}, \\Sigma_{a,2}]^{T}$, representing group-wise macroscopic absorption cross sections.\n- Prior distribution for $x$: Gaussian with mean $x_{b}$ and covariance matrix $B$, denoted as $x \\sim \\mathcal{N}(x_{b}, B)$.\n- Measurement error: Gaussian with zero mean and variance $R$, denoted as $\\varepsilon \\sim \\mathcal{N}(0, R)$.\n- Observation operator: $y = h(x)$.\n- Linearized observation operator: $y \\approx h(x_{b}) + g^{T}(x - x_{b})$, where $g = \\nabla_x h(x)|_{x=x_b}$.\n- Scalar measured observation: $y_o$.\n- Part 3 numerical values:\n    - Background mean: $x_{b} = \\begin{pmatrix} 0.25 \\\\ 0.10 \\end{pmatrix}$ barns\n    - Background covariance: $B = \\begin{pmatrix} (0.04)^{2} & 0.5 \\times 0.04 \\times 0.02 \\\\ 0.5 \\times 0.04 \\times 0.02 & (0.02)^{2} \\end{pmatrix}$ barns$^{2}$\n    - Gradient: $g = \\begin{pmatrix} -1.8 \\\\ -0.6 \\end{pmatrix}$\n    - Measurement error variance: $R = (0.01)^{2}$\n    - Model evaluation at background: $h(x_{b}) = 1.000$\n    - Measured value: $y_{o} = 1.015$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard application of Bayesian inference, specifically the method of 3D-Var or the Kalman update, to the field of nuclear reactor physics. Adjusting cross sections using integral experimental data (like detector readings) is a fundamental technique in reactor analysis. The framework is sound.\n- **Well-Posed:** The problem is clearly stated. Part 1 requires a standard derivation in Bayesian statistics. Part 2 asks for a correct interpretation of the result. Part 3 provides all necessary numerical data to compute a unique solution. The problem is well-posed.\n- **Objective:** The language is formal and quantitative, with no subjective or ambiguous statements.\n- **Completeness and Consistency:** The problem provides all necessary definitions, variables, and numerical values. There are no contradictions. The units are physically consistent.\n- **Realism:** The numerical values for macroscopic cross sections ($0.25$ and $0.10$ barns, which are $0.25$ cm$^{-1}$ and $0.10$ cm$^{-1}$ if we assume the unit 'barn' is used colloquially for inverse cm) and their uncertainties are plausible for a thermal reactor system. The detector measurement and its uncertainty are also realistic.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and all necessary information is provided. I will now proceed with the solution.\n\n### Part 1: Derivation of the Posterior Density\n\nThe posterior probability density function (PDF) $p(x \\mid y_{o})$ is given by Bayes' theorem:\n$$\np(x \\mid y_{o}) \\propto p(y_{o} \\mid x) p(x)\n$$\nwhere $p(y_o \\mid x)$ is the likelihood and $p(x)$ is the prior PDF.\n\nThe prior for $x$ is given as a multivariate Gaussian distribution:\n$$\np(x) = \\frac{1}{\\sqrt{(2\\pi)^{k} \\det(B)}} \\exp\\left(-\\frac{1}{2}(x - x_{b})^T B^{-1} (x - x_{b})\\right)\n$$\nwhere $k=2$ is the dimension of $x$.\n\nThe likelihood function $p(y_{o} \\mid x)$ is derived from the observation model and the measurement error. Under the linearized model, the measurement $y_o$ is related to the state $x$ by:\n$$\ny_o = h(x_b) + g^T(x - x_b) + \\varepsilon\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0, R)$. This implies that for a given $x$, $y_o$ is a random variable with a Gaussian distribution centered at $h(x_b) + g^T(x - x_b)$ with variance $R$.\n$$\np(y_{o} \\mid x) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{1}{2R} (y_{o} - (h(x_{b}) + g^{T}(x - x_{b})))^2\\right)\n$$\n\nThe posterior PDF is the product of the prior and the likelihood. Since the normalization constants do not depend on $x$, we can write:\n$$\np(x \\mid y_{o}) \\propto \\exp\\left(-\\frac{1}{2}(x - x_{b})^T B^{-1} (x - x_{b})\\right) \\exp\\left(-\\frac{1}{2R}(y_o - h(x_b) - g^T(x - x_b))^2\\right)\n$$\n$$\np(x \\mid y_{o}) \\propto \\exp\\left(-\\frac{1}{2}\\left[(x - x_{b})^T B^{-1} (x - x_{b}) + R^{-1}(y_o - h(x_b) - g^T(x - x_b))^2\\right]\\right)\n$$\nThe expression in the exponent is a quadratic function of $x$. This implies that the posterior distribution $p(x \\mid y_{o})$ is also a multivariate Gaussian. Let the posterior mean be $x_a$ (the analysis state) and the posterior covariance be $A$. The posterior PDF has the form $p(x \\mid y_{o}) \\propto \\exp(-\\frac{1}{2}(x - x_a)^T A^{-1}(x - x_a))$.\n\nThe posterior mean $x_a$ is the value of $x$ that maximizes the posterior PDF, which is equivalent to minimizing the cost function $J(x)$ in the exponent:\n$$\nJ(x) = (x - x_b)^T B^{-1} (x - x_b) + R^{-1}(y_o - h(x_b) - g^T(x - x_b))^2\n$$\nTo find the minimum, we take the gradient of $J(x)$ with respect to $x$ and set it to zero:\n$$\n\\nabla_x J(x) = 2B^{-1}(x - x_b) + 2R^{-1}(y_o - h(x_b) - g^T(x - x_b))(-g) = 0\n$$\nSetting $x=x_a$ at the minimum:\n$$\nB^{-1}(x_a - x_b) - R^{-1}g(y_o - h(x_b) - g^T(x_a - x_b)) = 0\n$$\n$$\nB^{-1}(x_a - x_b) + R^{-1}gg^T(x_a - x_b) = R^{-1}g(y_o - h(x_b))\n$$\n$$\n(B^{-1} + R^{-1}gg^T)(x_a - x_b) = R^{-1}g(y_o - h(x_b))\n$$\nThe posterior covariance matrix $A$ is the inverse of the Hessian of $J(x)/2$. The Hessian is $\\nabla_x^2 (J(x)/2) = B^{-1} + R^{-1}gg^T$. Thus, the posterior covariance is:\n$$\nA = (B^{-1} + R^{-1}gg^T)^{-1}\n$$\nThe posterior mean $x_a$ can be found by solving for $(x_a - x_b)$:\n$$\nx_a - x_b = (B^{-1} + R^{-1}gg^T)^{-1} R^{-1}g(y_o - h(x_b))\n$$\nUsing the Woodbury matrix identity, which states $(B^{-1} + UV)^{-1}U = B(I+VBU)^{-1}V$, or more specifically for this form, $(B^{-1} + gR^{-1}g^T)^{-1}gR^{-1} = Bg(R+g^T B g)^{-1}$, we can simplify the expression.\nLet $K = Bg(g^T B g + R)^{-1}$, which is the Kalman gain.\nThe update to the mean becomes:\n$$\nx_a - x_b = K(y_o - h(x_b))\n$$\nSo, the posterior mean is:\n$$\nx_a = x_b + Bg(g^T B g + R)^{-1}(y_o - h(x_b))\n$$\nThe posterior PDF is $p(x \\mid y_{o}) \\sim \\mathcal{N}(x_a, A)$, with the posterior mean $x_a$ and posterior covariance $A$ given by the closed-form expressions above.\n\n### Part 2: Exactness for a Truly Linear Model\n\nIf the observation model is exactly linear, $y = h(x_b) + g^T(x-x_b) + \\varepsilon$, then the relationship between the measurement $y_o$ and the state $x$ is given by a linear function of $x$ plus a Gaussian noise term $\\varepsilon$.\nThe prior distribution $p(x)$ is exactly Gaussian: $x \\sim \\mathcal{N}(x_b, B)$.\nThe likelihood function $p(y_o \\mid x)$ is also exactly Gaussian, as it describes the probability of observing $y_o$ given $x$, which is $\\mathcal{N}(h(x_b) + g^T(x-x_b), R)$.\nThe posterior PDF $p(x \\mid y_o)$ is proportional to the product of these two Gaussian functions. The product of two Gaussian functions is another (unnormalized) Gaussian function. The derivation in Part 1 manipulated the exponents of these Gaussian functions. The negative logarithm of the posterior is proportional to the cost function $J(x)$. Since the prior and likelihood are exactly Gaussian, $J(x)$ is an exactly quadratic function of $x$.\nThe minimization of a quadratic function has a unique solution, which exactly corresponds to the mean of the resulting Gaussian posterior distribution. The Hessian of this quadratic function is constant and its inverse exactly gives the posterior covariance.\nTherefore, the derivation in Part 1, which involved finding the minimum of $J(x)$, made no approximations under the assumption of a linear model. The derived expressions for the posterior mean $x_a$ and posterior covariance $A$ are exact for the linear case, not approximations.\n\nThe closed-form expressions are the same as derived in Part 1:\nPosterior mean: $x_a = x_b + Bg(g^T B g + R)^{-1}(y_o - h(x_b))$\nPosterior covariance: $A = (B^{-1} + R^{-1}gg^T)^{-1} = B - Bg(g^T B g + R)^{-1}g^T B$\n\n### Part 3: Numerical Calculation\n\nWe are given the following numerical values:\n- $x_{b} = \\begin{pmatrix} 0.25 \\\\ 0.10 \\end{pmatrix}$\n- $B = \\begin{pmatrix} (0.04)^{2} & 0.5 \\times 0.04 \\times 0.02 \\\\ 0.5 \\times 0.04 \\times 0.02 & (0.02)^{2} \\end{pmatrix} = \\begin{pmatrix} 0.0016 & 0.0004 \\\\ 0.0004 & 0.0004 \\end{pmatrix}$\n- $g = \\begin{pmatrix} -1.8 \\\\ -0.6 \\end{pmatrix}$\n- $R = (0.01)^{2} = 0.0001$\n- $h(x_{b}) = 1.000$\n- $y_{o} = 1.015$\n\nThe goal is to compute the posterior mean $x_a = x_b + Bg(g^T B g + R)^{-1}(y_o - h(x_b))$.\n\n1.  Calculate the innovation (measurement residual):\n    $$\n    d = y_o - h(x_b) = 1.015 - 1.000 = 0.015\n    $$\n2.  Calculate the term $Bg$:\n    $$\n    Bg = \\begin{pmatrix} 0.0016 & 0.0004 \\\\ 0.0004 & 0.0004 \\end{pmatrix} \\begin{pmatrix} -1.8 \\\\ -0.6 \\end{pmatrix} = \\begin{pmatrix} 0.0016(-1.8) + 0.0004(-0.6) \\\\ 0.0004(-1.8) + 0.0004(-0.6) \\end{pmatrix} = \\begin{pmatrix} -0.00288 - 0.00024 \\\\ -0.00072 - 0.00024 \\end{pmatrix} = \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix}\n    $$\n3.  Calculate the projected background covariance $g^T B g$:\n    $$\n    g^T B g = g^T(Bg) = \\begin{pmatrix} -1.8 & -0.6 \\end{pmatrix} \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} = (-1.8)(-0.00312) + (-0.6)(-0.00096) = 0.005616 + 0.000576 = 0.006192\n    $$\n4.  Calculate the innovation variance, which is a scalar:\n    $$\n    g^T B g + R = 0.006192 + 0.0001 = 0.006292\n    $$\n5.  Calculate the Kalman gain vector $K = Bg(g^T B g + R)^{-1}$:\n    $$\n    K = \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} (0.006292)^{-1} = \\frac{1}{0.006292} \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix}\n    $$\n6.  Calculate the update term $Kd$:\n    $$\n    Kd = \\frac{1}{0.006292} \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} (0.015) = \\frac{0.015}{0.006292} \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} \\approx 2.3840 \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} \\approx \\begin{pmatrix} -0.007438 \\\\ -0.002289 \\end{pmatrix}\n    $$\n7.  Compute the posterior mean $x_a = x_b + Kd$:\n    $$\n    x_a = \\begin{pmatrix} 0.25 \\\\ 0.10 \\end{pmatrix} + \\begin{pmatrix} -0.007438 \\\\ -0.002289 \\end{pmatrix} = \\begin{pmatrix} 0.25 - 0.007438 \\\\ 0.10 - 0.002289 \\end{pmatrix} = \\begin{pmatrix} 0.242562 \\\\ 0.097711 \\end{pmatrix}\n    $$\nRounding the components of the posterior mean to four significant figures:\n- $x_{a,1} \\approx 0.2426$\n- $x_{a,2} \\approx 0.09771$\n\nThe posterior mean vector is $\\begin{pmatrix} 0.2426 \\\\ 0.09771 \\end{pmatrix}$ barns.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.2426 \\\\ 0.09771 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The quality of a data assimilation analysis depends critically on the statistical model of prior uncertainty, especially the correlations between different parameters. Off-diagonal elements in the background error covariance matrix, $B$, encode this crucial information. This practice problem  provides a quantitative look at this effect by comparing the posterior variance reduction with and without prior correlations, highlighting why a well-constructed covariance matrix is essential for accurate adjustment.",
            "id": "4221186",
            "problem": "Consider a two-parameter cross section adjustment problem arising in nuclear reactor simulation, where first-order perturbation theory states that a small integral response, such as a reactivity-like metric, varies linearly with small relative changes in input macroscopic cross sections. Specifically, let the unknown parameter vector of dimensionless relative deviations be $x = \\begin{pmatrix}\\delta \\sigma_{a,1} \\\\ \\delta \\sigma_{f,1}\\end{pmatrix}$, where $\\delta \\sigma_{a,1}$ and $\\delta \\sigma_{f,1}$ are the group-wise relative changes in absorption and fission macroscopic cross sections for a single energy group. The observed scalar response $y$ is modeled as $y = H x + \\epsilon$, where $H = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix}$ collects the first-order sensitivity coefficients, and $\\epsilon$ is the measurement noise with zero mean and variance $R = \\sigma_{y}^{2}$.\n\nAssume a Gaussian prior for $x$ with zero mean and a background error covariance matrix $B$ that encodes physically motivated correlations between cross section uncertainties (e.g., due to shared nuclear data and modeling approximations). Take\n$$\nB = \\begin{pmatrix}\n0.0036 & 0.00324 \\\\\n0.00324 & 0.0081\n\\end{pmatrix},\n$$\nwhich corresponds to prior standard deviations $\\sigma_{\\delta \\sigma_{a,1}} = 0.06$ and $\\sigma_{\\delta \\sigma_{f,1}} = 0.09$ with correlation coefficient $\\rho = 0.6$. The observation error variance is $R = 0.0025$.\n\nUsing the linear-Gaussian data assimilation framework and starting from first principles of Bayesian inference for Gaussian priors and likelihoods, derive the posterior covariance for $x$ and compute, for the first parameter $\\delta \\sigma_{a,1}$, the variance reduction factor defined as\n$$\n\\mathrm{VRF} = \\frac{\\sigma_{\\delta \\sigma_{a,1}}^{2} - \\mathrm{Var}_{\\text{post}}(\\delta \\sigma_{a,1})}{\\sigma_{\\delta \\sigma_{a,1}}^{2}}.\n$$\nThen quantify the effect of neglecting the off-diagonal terms in $B$ by recomputing the variance reduction factor when the prior covariance is replaced by its diagonal counterpart\n$$\nB_{\\text{diag}} = \\begin{pmatrix}\n0.0036 & 0 \\\\\n0 & 0.0081\n\\end{pmatrix}.\n$$\n\nReport the difference in variance reduction factor\n$$\n\\Delta \\mathrm{VRF} = \\mathrm{VRF}\\big|_{B_{\\text{diag}}} - \\mathrm{VRF}\\big|_{B},\n$$\nexpressed as a decimal and rounded to four significant figures. No units are required for the final answer.",
            "solution": "The user wants to solve a data assimilation problem based on a linear-Gaussian model.\n\n### Step 1: Problem Validation\n\nThe first step is to validate the problem statement.\n\n#### Extraction of Givens\n-   Parameter vector of relative deviations: $x = \\begin{pmatrix}\\delta \\sigma_{a,1} \\\\ \\delta \\sigma_{f,1}\\end{pmatrix}$\n-   Linear observation model: $y = H x + \\epsilon$\n-   Sensitivity matrix: $H = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix}$\n-   Measurement noise $\\epsilon$: Zero mean, $\\mathbb{E}[\\epsilon]=0$, and variance $R = \\sigma_{y}^{2} = 0.0025$\n-   Prior distribution for $x$: Gaussian with zero mean, $\\mathbb{E}[x]=0$, and prior covariance matrix\n    $$\n    B = \\begin{pmatrix}\n    0.0036 & 0.00324 \\\\\n    0.00324 & 0.0081\n    \\end{pmatrix}\n    $$\n-   Prior standard deviations and correlation (for verification): $\\sigma_{\\delta \\sigma_{a,1}} = 0.06$, $\\sigma_{\\delta \\sigma_{f,1}} = 0.09$, $\\rho = 0.6$. Let's check: $\\sigma_{\\delta \\sigma_{a,1}}^{2} = (0.06)^2 = 0.0036 = B_{11}$. $\\sigma_{\\delta \\sigma_{f,1}}^{2} = (0.09)^2 = 0.0081 = B_{22}$. $\\rho \\sigma_{\\delta \\sigma_{a,1}} \\sigma_{\\delta \\sigma_{f,1}} = 0.6 \\times 0.06 \\times 0.09 = 0.00324 = B_{12}$. The numbers are consistent.\n-   Diagonalized prior covariance matrix for the second part of the problem:\n    $$\n    B_{\\text{diag}} = \\begin{pmatrix}\n    0.0036 & 0 \\\\\n    0 & 0.0081\n    \\end{pmatrix}\n    $$\n-   Definition of Variance Reduction Factor for $\\delta \\sigma_{a,1}$:\n    $$\n    \\mathrm{VRF} = \\frac{\\sigma_{\\delta \\sigma_{a,1}}^{2} - \\mathrm{Var}_{\\text{post}}(\\delta \\sigma_{a,1})}{\\sigma_{\\delta \\sigma_{a,1}}^{2}}\n    $$\n-   Quantity to compute: $\\Delta \\mathrm{VRF} = \\mathrm{VRF}\\big|_{B_{\\text{diag}}} - \\mathrm{VRF}\\big|_{B}$, rounded to four significant figures.\n\n#### Validation against Criteria\n1.  **Scientifically Grounded**: The problem is a standard application of Bayesian inference (specifically, the Best Linear Unbiased Estimator, or BLUE) in the context of nuclear data assimilation and cross-section adjustment. All concepts—sensitivity analysis, covariance matrices, Bayesian updates—are fundamental to this field. The problem is scientifically sound.\n2.  **Well-Posed**: The problem is well-defined within the linear-Gaussian framework. The prior covariance matrix $B$ is symmetric and positive definite ($\\det(B) = (0.0036)(0.0081) - (0.00324)^2 = 2.916 \\times 10^{-5} - 1.04976 \\times 10^{-5} = 1.86624 \\times 10^{-5} > 0$, and trace is positive). The observation error variance $R$ is a positive scalar. This guarantees the existence of a unique posterior distribution and thus a unique solution.\n3.  **Objective**: The problem is expressed in precise mathematical terms, free of ambiguity or subjective claims.\n\n#### Verdict\nThe problem is valid. We can proceed with the solution.\n\n### Step 2: Derivation and Solution\n\nThe problem requires a derivation from first principles of Bayesian inference. Given a prior distribution $p(x)$ and a likelihood function $p(y|x)$, the posterior distribution $p(x|y)$ is given by Bayes' theorem:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nThe prior for $x$ is a zero-mean Gaussian distribution with covariance $B$:\n$$\np(x) \\propto \\exp\\left(-\\frac{1}{2} x^T B^{-1} x\\right)\n$$\nThe observation model is $y = Hx + \\epsilon$, with noise $\\epsilon \\sim N(0, R)$. This implies that the likelihood of observing $y$ given $x$ is a Gaussian distribution with mean $Hx$ and covariance $R$:\n$$\np(y|x) \\propto \\exp\\left(-\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)\\right)\n$$\nCombining these, the posterior distribution is:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (y - Hx)^T R^{-1} (y - Hx) + x^T B^{-1} x \\right]\\right)\n$$\nThe argument of the exponential is a quadratic function of $x$. This means the posterior distribution is also Gaussian, $p(x|y) \\sim N(\\hat{x}, A)$, where $\\hat{x}$ is the posterior mean and $A$ is the posterior covariance matrix. The posterior covariance $A$ is the inverse of the Hessian of the quadratic form (divided by $2$). The expression in the square brackets is the cost function $J(x)$.\n$$\nJ(x) = (y-Hx)^T R^{-1} (y-Hx) + x^T B^{-1} x\n$$\n$$\nJ(x) = y^T R^{-1} y - 2y^T R^{-1} Hx + x^T H^T R^{-1} Hx + x^T B^{-1} x\n$$\n$$\nJ(x) = x^T (H^T R^{-1} H + B^{-1}) x - 2y^T R^{-1} Hx + \\text{const}\n$$\nThe Hessian of $\\frac{1}{2}J(x)$ with respect to $x$ is $H^T R^{-1} H + B^{-1}$. The posterior covariance $A$ is the inverse of this Hessian:\n$$\nA = (B^{-1} + H^T R^{-1} H)^{-1}\n$$\nFor computational purposes, especially when the dimension of the observation $y$ is less than the dimension of the state $x$ (as is the case here, $1 < 2$), it is advantageous to use the Woodbury matrix identity:\n$$\nA = B - B H^T (H B H^T + R)^{-1} H B\n$$\nThe prior variance of the first parameter, $\\delta \\sigma_{a,1}$, is $\\sigma_{\\delta \\sigma_{a,1}}^{2} = B_{11} = 0.0036$. The posterior variance is $\\mathrm{Var}_{\\text{post}}(\\delta \\sigma_{a,1}) = A_{11}$.\nThe variance reduction is the numerator of the VRF: $\\sigma_{\\delta \\sigma_{a,1}}^{2} - \\mathrm{Var}_{\\text{post}}(\\delta \\sigma_{a,1}) = B_{11} - A_{11}$.\nFrom the Woodbury formula, we can identify $A_{11}$:\n$$\nA_{11} = B_{11} - \\left( B H^T (H B H^T + R)^{-1} H B \\right)_{11}\n$$\nLet's define the innovation covariance $S = H B H^T + R$ and the prior covariance between the state vector and the observable $v = B H^T$. Since $S$ is a scalar, its inverse is $1/S$. The matrix $B H^T (H B)^T / S = v v^T / S$ is an outer product. The $(1,1)$ component of this matrix is $v_1^2 / S$.\nSo, $A_{11} = B_{11} - \\frac{v_1^2}{S}$.\nThe variance reduction is $B_{11} - A_{11} = \\frac{v_1^2}{S}$.\nThe Variance Reduction Factor is then:\n$$\n\\mathrm{VRF} = \\frac{B_{11} - A_{11}}{B_{11}} = \\frac{v_1^2}{S \\cdot B_{11}} = \\frac{( (B H^T)_1 )^2}{(H B H^T + R) B_{11}}\n$$\n\n#### Part 1: Calculation with Full Covariance Matrix $B$\n\nFirst, we calculate the terms needed for $\\mathrm{VRF}\\big|_{B}$.\nThe sensitivity matrix is $H = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix}$.\nThe prior covariance is $B = \\begin{pmatrix} 0.0036 & 0.00324 \\\\ 0.00324 & 0.0081 \\end{pmatrix}$.\nThe vector $v = B H^T$ is:\n$$\nv = \\begin{pmatrix} 0.0036 & 0.00324 \\\\ 0.00324 & 0.0081 \\end{pmatrix} \\begin{pmatrix} -0.8 \\\\ 1.1 \\end{pmatrix} = \\begin{pmatrix} 0.0036(-0.8) + 0.00324(1.1) \\\\ 0.00324(-0.8) + 0.0081(1.1) \\end{pmatrix} = \\begin{pmatrix} -0.00288 + 0.003564 \\\\ -0.002592 + 0.00891 \\end{pmatrix} = \\begin{pmatrix} 0.000684 \\\\ 0.006318 \\end{pmatrix}\n$$\nThe first element is $v_1 = 0.000684$.\nThe innovation covariance term $H B H^T$ is:\n$$\nH B H^T = H v = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix} \\begin{pmatrix} 0.000684 \\\\ 0.006318 \\end{pmatrix} = (-0.8)(0.000684) + (1.1)(0.006318) = -0.0005472 + 0.0069498 = 0.0064026\n$$\nThe full innovation covariance $S = H B H^T + R$, with $R = 0.0025$:\n$$\nS = 0.0064026 + 0.0025 = 0.0089026\n$$\nThe prior variance of the first parameter is $B_{11} = 0.0036$.\nNow, we compute $\\mathrm{VRF}\\big|_{B}$:\n$$\n\\mathrm{VRF}\\big|_{B} = \\frac{v_1^2}{S \\cdot B_{11}} = \\frac{(0.000684)^2}{(0.0089026)(0.0036)} = \\frac{4.67856 \\times 10^{-7}}{3.204936 \\times 10^{-5}} \\approx 0.0145979\n$$\n\n#### Part 2: Calculation with Diagonal Covariance Matrix $B_{\\text{diag}}$\n\nNext, we repeat the calculation using $B_{\\text{diag}} = \\begin{pmatrix} 0.0036 & 0 \\\\ 0 & 0.0081 \\end{pmatrix}$.\nThe vector $v_{\\text{diag}} = B_{\\text{diag}} H^T$ is:\n$$\nv_{\\text{diag}} = \\begin{pmatrix} 0.0036 & 0 \\\\ 0 & 0.0081 \\end{pmatrix} \\begin{pmatrix} -0.8 \\\\ 1.1 \\end{pmatrix} = \\begin{pmatrix} 0.0036(-0.8) + 0(1.1) \\\\ 0(-0.8) + 0.0081(1.1) \\end{pmatrix} = \\begin{pmatrix} -0.00288 \\\\ 0.00891 \\end{pmatrix}\n$$\nThe first element is $(v_{\\text{diag}})_1 = -0.00288$.\nThe innovation covariance term $H B_{\\text{diag}} H^T$ is:\n$$\nH B_{\\text{diag}} H^T = H v_{\\text{diag}} = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix} \\begin{pmatrix} -0.00288 \\\\ 0.00891 \\end{pmatrix} = (-0.8)(-0.00288) + (1.1)(0.00891) = 0.002304 + 0.009801 = 0.012105\n$$\nThe full innovation covariance $S_{\\text{diag}} = H B_{\\text{diag}} H^T + R$:\n$$\nS_{\\text{diag}} = 0.012105 + 0.0025 = 0.014605\n$$\nThe prior variance of the first parameter is $(B_{\\text{diag}})_{11} = 0.0036$.\nNow, we compute $\\mathrm{VRF}\\big|_{B_{\\text{diag}}}$:\n$$\n\\mathrm{VRF}\\big|_{B_{\\text{diag}}} = \\frac{((v_{\\text{diag}})_1)^2}{S_{\\text{diag}} \\cdot (B_{\\text{diag}})_{11}} = \\frac{(-0.00288)^2}{(0.014605)(0.0036)} = \\frac{8.2944 \\times 10^{-6}}{5.2578 \\times 10^{-5}} \\approx 0.157754\n$$\n\n#### Part 3: Difference in Variance Reduction Factors\n\nFinally, we compute the difference $\\Delta \\mathrm{VRF} = \\mathrm{VRF}\\big|_{B_{\\text{diag}}} - \\mathrm{VRF}\\big|_{B}$.\n$$\n\\Delta \\mathrm{VRF} \\approx 0.157754 - 0.0145979 = 0.1431561\n$$\nRounding to four significant figures, we get $0.1432$.\nNeglecting the physically motivated positive correlation between the absorption and fission cross-section uncertainties leads to a significant overestimation of the variance reduction achievable for the absorption cross-section from the given integral measurement. This highlights the critical importance of accurate prior covariance models in data assimilation.",
            "answer": "$$\\boxed{0.1432}$$"
        },
        {
            "introduction": "In realistic reactor simulations, the state vector of unknown parameters can be enormous, rendering a full-rank covariance update computationally infeasible. Advanced methods are needed to make the problem tractable without losing essential information. This practice  introduces the concept of a low-rank approximation, demonstrating how the Singular Value Decomposition (SVD) can be used to capture the most significant information from measurements and quantify the error introduced by this essential simplification.",
            "id": "4221125",
            "problem": "Consider a linearized data assimilation step for adjusting multigroup macroscopic cross sections in a pressurized water reactor core simulation. Let the unknown cross-section adjustment vector be $x \\in \\mathbb{R}^{5}$, representing small corrections to five grouped macroscopic cross sections. Let the measurement vector be $y \\in \\mathbb{R}^{4}$, consisting of four detector responses (e.g., flux or reaction-rate tallies) affected by measurement noise. Assume a Gaussian prior $x \\sim \\mathcal{N}(0, P_{f})$ with prior covariance $P_{f} \\in \\mathbb{R}^{5 \\times 5}$, a linearized observation operator $H \\in \\mathbb{R}^{4 \\times 5}$ relating cross sections to measurements, and Gaussian measurement noise $e \\sim \\mathcal{N}(0, R)$, with $R \\in \\mathbb{R}^{4 \\times 4}$ symmetric positive definite. The forward model for the data assimilation is $y = H x + e$.\n\nStarting from the standard linear Gaussian Bayesian framework and the Woodbury identity, derive how a low-rank posterior covariance update arises by truncating singular values of the prior- and noise-preconditioned sensitivity operator. Define the prior- and noise-preconditioned sensitivity matrix $S = R^{-1/2} H P_{f}^{1/2}$ and its Singular Value Decomposition (SVD) $S = U \\Sigma V^{\\top}$, with $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, \\sigma_{4})$ and $\\sigma_{1} \\ge \\sigma_{2} \\ge \\sigma_{3} \\ge \\sigma_{4} \\ge 0$. Explain how truncating singular values at a threshold $\\tau$ yields a rank-$k$ approximation to the posterior covariance in the prior-whitened coordinates, and justify a norm with which to quantify the approximation error.\n\nIn a specific reactor data assimilation instance, numerical preprocessing yields the singular values of $S$ as $\\sigma_{1} = 2.7$, $\\sigma_{2} = 1.4$, $\\sigma_{3} = 0.75$, and $\\sigma_{4} = 0.30$. Suppose that the low-rank update is chosen by truncation at the threshold $\\tau = 1.0$, so that only singular values strictly greater than $\\tau$ are retained. Quantify the approximation error in the posterior covariance due to this truncation by computing the Frobenius norm of the difference between the exact and truncated posterior covariances in the prior-whitened coordinates, i.e., compute the Frobenius norm of $P_{f}^{-1/2} \\left(P_{a} - P_{a}^{(k)}\\right) P_{f}^{-1/2}$, where $P_{a}$ is the exact posterior covariance and $P_{a}^{(k)}$ is the truncated approximation. Express the result as a dimensionless number and round your answer to four significant figures.",
            "solution": "The problem is first validated to be scientifically grounded, well-posed, and objective. It describes a standard linear Gaussian Bayesian inverse problem, a cornerstone of data assimilation, using well-defined mathematical objects and language. It is self-contained and provides all necessary information for a unique solution.\n\nThe problem asks for a derivation of a low-rank posterior covariance update and a quantitative calculation of the approximation error resulting from this update.\n\nWe begin with the fundamental equation for the posterior probability density in a linear Gaussian Bayesian framework. The prior distribution for the cross-section adjustments $x \\in \\mathbb{R}^{5}$ is given as $x \\sim \\mathcal{N}(0, P_{f})$, and the measurement model is $y = Hx + e$ with noise $e \\sim \\mathcal{N}(0, R)$. The corresponding probability densities are:\n$$\np(x) \\propto \\exp\\left(-\\frac{1}{2} x^{\\top} P_{f}^{-1} x\\right)\n$$\n$$\np(y|x) \\propto \\exp\\left(-\\frac{1}{2} (y - Hx)^{\\top} R^{-1} (y - Hx)\\right)\n$$\nAccording to Bayes' theorem, the posterior density $p(x|y)$ is proportional to the product of the likelihood and the prior, $p(x|y) \\propto p(y|x)p(x)$. The negative logarithm of the posterior is therefore proportional to:\n$$\nJ(x) = x^{\\top} P_{f}^{-1} x + (y - Hx)^{\\top} R^{-1} (y - Hx)\n$$\nThe posterior distribution is also Gaussian, and its covariance matrix, $P_a$, is the inverse of the Hessian of $\\frac{1}{2}J(x)$. The Hessian of $\\frac{1}{2}J(x)$ with respect to $x$ is:\n$$\n\\nabla_x^2 \\left(\\frac{1}{2}J(x)\\right) = P_{f}^{-1} + H^{\\top}R^{-1}H\n$$\nThus, the inverse of the posterior covariance matrix $P_a$ is:\n$$\nP_{a}^{-1} = P_{f}^{-1} + H^{\\top}R^{-1}H\n$$\nTo analyze the structure of the update, it is advantageous to work in \"prior-whitened\" coordinates. Let's define a new variable $x' = P_{f}^{-1/2}x$. The prior covariance of $x'$ is $\\text{Cov}(x') = P_f^{-1/2} P_f (P_f^{-1/2})^{\\top} = I$, where $I$ is the identity matrix. The posterior covariance of $x'$, denoted $P_{a'}$, is related to $P_a$ by $P_{a'} = P_f^{-1/2} P_a (P_f^{-1/2})^{\\top} = P_f^{-1/2} P_a P_f^{-1/2}$ (since $P_f$ is symmetric).\n\nWe can express $P_{a'}^{-1}$ by pre- and post-multiplying the equation for $P_a^{-1}$ by $P_f^{1/2}$:\n$$\nP_{a'}^{-1} = P_f^{1/2} P_{a}^{-1} P_f^{1/2} = P_f^{1/2} (P_f^{-1} + H^{\\top}R^{-1}H) P_f^{1/2}\n$$\n$$\nP_{a'}^{-1} = I + P_f^{1/2} H^{\\top}R^{-1}H P_f^{1/2}\n$$\nUsing the definition of the prior- and noise-preconditioned sensitivity matrix, $S = R^{-1/2} H P_f^{1/2}$, and noting that $S^{\\top} = P_f^{1/2} H^{\\top} R^{-1/2}$ (since $P_f, R$ are symmetric), we can rewrite the second term:\n$$\nP_f^{1/2} H^{\\top}R^{-1}H P_f^{1/2} = (P_f^{1/2} H^{\\top}R^{-1/2})(R^{-1/2}H P_f^{1/2}) = S^{\\top}S\n$$\nSo, the inverse posterior covariance in whitened coordinates is:\n$$\nP_{a'}^{-1} = I + S^{\\top}S\n$$\nThe posterior covariance in whitened coordinates is $P_{a'} = (I + S^{\\top}S)^{-1}$.\nNow, we introduce the Singular Value Decomposition (SVD) of $S \\in \\mathbb{R}^{4 \\times 5}$, which is given as $S = U \\Sigma V^{\\top}$. Here, $U \\in \\mathbb{R}^{4 \\times 4}$ and $V \\in \\mathbb{R}^{5 \\times 5}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{4 \\times 5}$ is a rectangular diagonal matrix of singular values $\\sigma_i$. Specifically, $\\Sigma = [\\text{diag}(\\sigma_1, \\sigma_2, \\sigma_3, \\sigma_4) \\ | \\ \\mathbf{0}_{4 \\times 1}]$.\nLet's compute $S^{\\top}S$:\n$$\nS^{\\top}S = (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} = V (\\Sigma^{\\top}\\Sigma) V^{\\top}\n$$\nThe matrix $\\Lambda^2 = \\Sigma^{\\top}\\Sigma$ is a $5 \\times 5$ diagonal matrix with entries $\\sigma_1^2, \\sigma_2^2, \\sigma_3^2, \\sigma_4^2, 0$.\nSubstituting this into the expression for $P_{a'}$:\n$$\nP_{a'} = (I + V \\Lambda^2 V^{\\top})^{-1} = (V V^{\\top} + V \\Lambda^2 V^{\\top})^{-1} = (V(I + \\Lambda^2)V^{\\top})^{-1} = V(I + \\Lambda^2)^{-1}V^{\\top}\n$$\nThe matrix $(I + \\Lambda^2)^{-1}$ is diagonal with entries $\\frac{1}{1+\\sigma_1^2}, \\frac{1}{1+\\sigma_2^2}, \\frac{1}{1+\\sigma_3^2}, \\frac{1}{1+\\sigma_4^2}, 1$.\nA low-rank update arises from truncating the SVD of $S$. Let $S^{(k)}$ be a rank-$k$ approximation of $S$ obtained by keeping only the $k$ largest singular values and setting the others to zero. $S^{(k)} = U \\Sigma^{(k)} V^{\\top}$. This leads to an approximate whitened posterior covariance $P_{a'}^{(k)}$:\n$$\nP_{a'}^{(k)} = (I + (S^{(k)})^{\\top}S^{(k)})^{-1} = V(I + (\\Lambda^{(k)})^2)^{-1}V^{\\top}\n$$\nwhere $(\\Lambda^{(k)})^2$ is a diagonal matrix with diagonal entries $\\sigma_1^2, \\dots, \\sigma_k^2, 0, \\dots, 0$. The effect of this approximation is to ignore the information from the measurements in the directions corresponding to the small, truncated singular values. In these directions (spanned by the corresponding columns of $V$), the posterior covariance remains equal to the prior covariance (which is $I$ in whitened coordinates). This is why the update from the prior to the posterior is considered \"low-rank\". The Frobenius norm is a suitable metric to quantify the approximation error, as it is invariant under orthogonal transformations and represents the root-mean-square error of the matrix elements.\n\nWe are asked to compute the Frobenius norm of the error matrix $E = P_{f}^{-1/2} \\left(P_{a} - P_{a}^{(k)}\\right) P_{f}^{-1/2} = P_{a'} - P_{a'}^{(k)}$.\n$$\nE = V(I + \\Lambda^2)^{-1}V^{\\top} - V(I + (\\Lambda^{(k)})^2)^{-1}V^{\\top} = V \\left[ (I + \\Lambda^2)^{-1} - (I + (\\Lambda^{(k)})^2)^{-1} \\right] V^{\\top}\n$$\nDue to the unitary invariance of the Frobenius norm, $\\|A\\|_F = \\|UAV^\\top\\|_F$ for orthogonal $U, V$.\n$$\n\\|E\\|_F = \\left\\| (I + \\Lambda^2)^{-1} - (I + (\\Lambda^{(k)})^2)^{-1} \\right\\|_F\n$$\nThe matrix inside the norm is a diagonal matrix, $D = \\operatorname{diag}(d_1, \\dots, d_5)$. The Frobenius norm is $\\sqrt{\\sum_{i=1}^5 d_i^2}$.\n\nThe singular values are $\\sigma_1 = 2.7$, $\\sigma_2 = 1.4$, $\\sigma_3 = 0.75$, and $\\sigma_4 = 0.30$. The truncation threshold is $\\tau = 1.0$. We retain singular values strictly greater than $\\tau$.\n$\\sigma_1 = 2.7 > 1.0$ and $\\sigma_2 = 1.4 > 1.0$ are retained.\n$\\sigma_3 = 0.75 \\le 1.0$ and $\\sigma_4 = 0.30 \\le 1.0$ are truncated.\nSo, $k=2$. The approximated matrix $(\\Lambda^{(k)})^2$ has diagonal entries $\\sigma_1^2, \\sigma_2^2, 0, 0, 0$.\n\nThe diagonal entries of the difference matrix $D$ are:\n$d_1 = \\frac{1}{1+\\sigma_1^2} - \\frac{1}{1+\\sigma_1^2} = 0$\n$d_2 = \\frac{1}{1+\\sigma_2^2} - \\frac{1}{1+\\sigma_2^2} = 0$\n$d_3 = \\frac{1}{1+\\sigma_3^2} - \\frac{1}{1+0} = \\frac{1 - (1+\\sigma_3^2)}{1+\\sigma_3^2} = \\frac{-\\sigma_3^2}{1+\\sigma_3^2}$\n$d_4 = \\frac{1}{1+\\sigma_4^2} - \\frac{1}{1+0} = \\frac{-\\sigma_4^2}{1+\\sigma_4^2}$\n$d_5 = \\frac{1}{1+0} - \\frac{1}{1+0} = 0$\n\nThe Frobenius norm of the error is:\n$$\n\\|E\\|_F = \\sqrt{0^2 + 0^2 + \\left(\\frac{-\\sigma_3^2}{1+\\sigma_3^2}\\right)^2 + \\left(\\frac{-\\sigma_4^2}{1+\\sigma_4^2}\\right)^2 + 0^2} = \\sqrt{\\left(\\frac{\\sigma_3^2}{1+\\sigma_3^2}\\right)^2 + \\left(\\frac{\\sigma_4^2}{1+\\sigma_4^2}\\right)^2}\n$$\nNow, we substitute the numerical values:\n$\\sigma_3 = 0.75$ and $\\sigma_4 = 0.30$.\n$\\sigma_3^2 = (0.75)^2 = 0.5625$.\n$\\sigma_4^2 = (0.30)^2 = 0.09$.\n\nCalculate the two terms:\nTerm for $\\sigma_3$: $\\frac{\\sigma_3^2}{1+\\sigma_3^2} = \\frac{0.5625}{1+0.5625} = \\frac{0.5625}{1.5625} = 0.36$.\nTerm for $\\sigma_4$: $\\frac{\\sigma_4^2}{1+\\sigma_4^2} = \\frac{0.09}{1+0.09} = \\frac{0.09}{1.09}$.\n\nNow compute the norm:\n$$\n\\|E\\|_F = \\sqrt{(0.36)^2 + \\left(\\frac{0.09}{1.09}\\right)^2}\n$$\n$$\n\\|E\\|_F = \\sqrt{0.1296 + (0.0825688...)^2}\n$$\n$$\n\\|E\\|_F = \\sqrt{0.1296 + 0.0068176...}\n$$\n$$\n\\|E\\|_F = \\sqrt{0.1364176...}\n$$\n$$\n\\|E\\|_F \\approx 0.3693475...\n$$\nRounding to four significant figures, the result is $0.3693$.",
            "answer": "$$\n\\boxed{0.3693}\n$$"
        }
    ]
}