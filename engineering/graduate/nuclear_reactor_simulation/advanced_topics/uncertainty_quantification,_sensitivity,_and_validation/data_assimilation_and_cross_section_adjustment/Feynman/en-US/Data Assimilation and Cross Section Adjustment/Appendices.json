{
    "hands_on_practices": [
        {
            "introduction": "The foundation of data assimilation lies in systematically updating our knowledge of a system in light of new evidence. This first exercise walks through the core mathematical procedure of this process, starting from Bayes' theorem. By deriving the posterior distribution for a set of nuclear cross sections after a single detector measurement, you will build a concrete understanding of how prior information and new data are blended to produce a more accurate estimate .",
            "id": "4221177",
            "problem": "In a two-group infinite-medium nuclear reactor simulation, a reactor physicist seeks to assimilate a single scalar in-core detector reading to adjust the macroscopic absorption cross sections. Let the state vector be $x = [\\Sigma_{a,1}, \\Sigma_{a,2}]^{T}$, where $\\Sigma_{a,1}$ and $\\Sigma_{a,2}$ are the group-wise macroscopic absorption cross sections in units of cm⁻¹. The model-predicted scalar observable is $y = h(x)$, where $h$ represents the detector response model. The background (prior) distribution for $x$ is Gaussian with mean $x_{b}$ and covariance matrix $B$, and the measurement error is Gaussian with zero mean and variance $R$.\n\nThe observation operator is nonlinear in general, but for analysis near the background state, consider the first-order Taylor approximation of $h(x)$ around $x_{b}$:\n$$\ny \\approx h(x_{b}) + g^{T}(x - x_{b}),\n$$\nwhere $g$ is the gradient of $h$ with respect to $x$ evaluated at $x_{b}$.\n\nUsing Bayes' theorem and treating the linearization as locally valid, do the following:\n\n1) Starting from the Gaussian prior for $x$ and the Gaussian likelihood for the scalar observation $y_{o}$ under the linearized model, derive the approximate posterior density $p(x \\mid y_{o})$. Your derivation must identify the posterior mean and posterior covariance in closed form in terms of $x_{b}$, $B$, $g$, $R$, $h(x_{b})$, and $y_{o}$.\n\n2) Explain why, for the exactly linear scalar observation model $y = h(x_{b}) + g^{T}(x - x_{b}) + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,R)$, the posterior obtained in part 1) is in fact the exact posterior, and state the same closed-form expressions for the posterior mean and posterior covariance.\n\n3) Now instantiate a physically plausible data assimilation for cross section adjustment with the following numerical values:\n- Background mean $x_{b} = \\begin{pmatrix} 0.25 \\\\ 0.10 \\end{pmatrix}$ cm⁻¹,\n- Background covariance $B = \\begin{pmatrix} (0.04)^{2} & 0.5 \\times 0.04 \\times 0.02 \\\\ 0.5 \\times 0.04 \\times 0.02 & (0.02)^{2} \\end{pmatrix}$ cm⁻²,\n- Gradient $g = \\begin{pmatrix} -1.8 \\\\ -0.6 \\end{pmatrix}$ in units of the observable per cm⁻¹,\n- Measurement error variance $R = (0.01)^{2}$,\n- Model evaluation at background $h(x_{b}) = 1.000$,\n- Measured value $y_{o} = 1.015$.\n\nUsing your derived formulas, compute the posterior mean vector for $x$ under the linearized model. Express your final numerical answer as the two components of the posterior mean in cm⁻¹. Round your answer to four significant figures.",
            "solution": "### Part 1: Derivation of the Posterior Density\n\nThe posterior probability density function (PDF) $p(x \\mid y_{o})$ is given by Bayes' theorem:\n$$\np(x \\mid y_{o}) \\propto p(y_{o} \\mid x) p(x)\n$$\nwhere $p(y_o \\mid x)$ is the likelihood and $p(x)$ is the prior PDF.\n\nThe prior for $x$ is given as a multivariate Gaussian distribution:\n$$\np(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_{b})^T B^{-1} (x - x_{b})\\right)\n$$\n\nThe likelihood function $p(y_{o} \\mid x)$ is derived from the observation model and the measurement error. Under the linearized model, the measurement $y_o$ is related to the state $x$ by $y_o = h(x_b) + g^T(x - x_b) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, R)$. This implies that for a given $x$, $y_o$ is a random variable with a Gaussian distribution centered at $h(x_b) + g^T(x - x_b)$ with variance $R$.\n$$\np(y_{o} \\mid x) \\propto \\exp\\left(-\\frac{1}{2R} (y_{o} - (h(x_{b}) + g^{T}(x - x_{b})))^2\\right)\n$$\n\nThe posterior PDF is proportional to the product of the prior and the likelihood:\n$$\np(x \\mid y_{o}) \\propto \\exp\\left(-\\frac{1}{2}\\left[(x - x_{b})^T B^{-1} (x - x_{b}) + R^{-1}(y_o - h(x_b) - g^T(x - x_b))^2\\right]\\right)\n$$\nThe expression in the exponent is a quadratic function of $x$, which means the posterior distribution $p(x \\mid y_{o})$ is also a multivariate Gaussian. The posterior mean $x_a$ (the analysis state) is the value of $x$ that minimizes the cost function $J(x)$ in the exponent:\n$$\nJ(x) = (x - x_b)^T B^{-1} (x - x_b) + R^{-1}(y_o - h(x_b) - g^T(x - x_b))^2\n$$\nTo find the minimum, we take the gradient of $J(x)$ with respect to $x$ and set it to zero. Setting $x=x_a$ at the minimum:\n$$\n\\nabla_x J(x)|_{x=x_a} = 2B^{-1}(x_a - x_b) - 2R^{-1}g(y_o - h(x_b) - g^T(x_a - x_b)) = 0\n$$\n$$\n(B^{-1} + R^{-1}gg^T)(x_a - x_b) = R^{-1}g(y_o - h(x_b))\n$$\nThe inverse of the posterior covariance matrix $A$ is the Hessian of $J(x)/2$, which is $A^{-1} = B^{-1} + R^{-1}gg^T$.\nSo, the posterior covariance is:\n$$\nA = (B^{-1} + R^{-1}gg^T)^{-1}\n$$\nSolving for the posterior mean $x_a$ using the Woodbury matrix identity leads to the standard Kalman update equations:\n$$\nx_a = x_b + K(y_o - h(x_b))\n$$\nwhere the Kalman Gain is $K = Bg(g^T B g + R)^{-1}$.\nSo, the posterior mean is:\n$$\nx_a = x_b + Bg(g^T B g + R)^{-1}(y_o - h(x_b))\n$$\nThe posterior PDF is $p(x \\mid y_{o}) \\sim \\mathcal{N}(x_a, A)$.\n\n### Part 2: Exactness for a Truly Linear Model\n\nIf the observation model is exactly linear, $y = h(x_b) + g^T(x-x_b) + \\varepsilon$, then the relationship between the measurement $y_o$ and the state $x$ is given by a linear function of $x$ plus a Gaussian noise term $\\varepsilon$. The prior distribution $p(x)$ is exactly Gaussian. The likelihood function $p(y_o \\mid x)$ is also exactly Gaussian. The product of two Gaussian functions is another (unnormalized) Gaussian function. The derivation in Part 1 manipulated the exponents of these Gaussian functions. The negative logarithm of the posterior is proportional to the cost function $J(x)$. Since the prior and likelihood are exactly Gaussian, $J(x)$ is an exactly quadratic function of $x$. The minimization of a quadratic function has a unique solution, which exactly corresponds to the mean of the resulting Gaussian posterior distribution. Therefore, the derived expressions for the posterior mean $x_a$ and posterior covariance $A$ are exact for the linear case, not approximations.\n\nThe closed-form expressions are the same as derived in Part 1:\nPosterior mean: $x_a = x_b + Bg(g^T B g + R)^{-1}(y_o - h(x_b))$\nPosterior covariance: $A = (B^{-1} + R^{-1}gg^T)^{-1} = B - Bg(g^T B g + R)^{-1}g^T B$\n\n### Part 3: Numerical Calculation\n\nGiven the numerical values:\n- $x_{b} = \\begin{pmatrix} 0.25 \\\\ 0.10 \\end{pmatrix}$\n- $B = \\begin{pmatrix} 0.0016 & 0.0004 \\\\ 0.0004 & 0.0004 \\end{pmatrix}$\n- $g = \\begin{pmatrix} -1.8 \\\\ -0.6 \\end{pmatrix}$\n- $R = (0.01)^{2} = 0.0001$\n- $h(x_{b}) = 1.000$\n- $y_{o} = 1.015$\n\nWe compute the posterior mean $x_a = x_b + Bg(g^T B g + R)^{-1}(y_o - h(x_b))$.\n\n1.  Calculate the innovation (measurement residual):\n    $$\n    d = y_o - h(x_b) = 1.015 - 1.000 = 0.015\n    $$\n2.  Calculate the term $Bg$:\n    $$\n    Bg = \\begin{pmatrix} 0.0016 & 0.0004 \\\\ 0.0004 & 0.0004 \\end{pmatrix} \\begin{pmatrix} -1.8 \\\\ -0.6 \\end{pmatrix} = \\begin{pmatrix} -0.00288 - 0.00024 \\\\ -0.00072 - 0.00024 \\end{pmatrix} = \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix}\n    $$\n3.  Calculate the projected background covariance $g^T B g$:\n    $$\n    g^T B g = \\begin{pmatrix} -1.8 & -0.6 \\end{pmatrix} \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} = 0.005616 + 0.000576 = 0.006192\n    $$\n4.  Calculate the innovation variance (a scalar):\n    $$\n    g^T B g + R = 0.006192 + 0.0001 = 0.006292\n    $$\n5.  Calculate the Kalman gain vector $K = Bg(g^T B g + R)^{-1}$:\n    $$\n    K = \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} (0.006292)^{-1}\n    $$\n6.  Calculate the update term $Kd$:\n    $$\n    Kd = \\frac{0.015}{0.006292} \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} \\approx 2.3840 \\begin{pmatrix} -0.00312 \\\\ -0.00096 \\end{pmatrix} \\approx \\begin{pmatrix} -0.007438 \\\\ -0.002289 \\end{pmatrix}\n    $$\n7.  Compute the posterior mean $x_a = x_b + Kd$:\n    $$\n    x_a = \\begin{pmatrix} 0.25 \\\\ 0.10 \\end{pmatrix} + \\begin{pmatrix} -0.007438 \\\\ -0.002289 \\end{pmatrix} = \\begin{pmatrix} 0.242562 \\\\ 0.097711 \\end{pmatrix}\n    $$\nRounding the components of the posterior mean to four significant figures:\n- $x_{a,1} \\approx 0.2426$\n- $x_{a,2} \\approx 0.09771$\n\nThe posterior mean vector is $\\begin{pmatrix} 0.2426 \\\\ 0.09771 \\end{pmatrix}$ cm⁻¹.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.2426 \\\\ 0.09771 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having established the fundamental update equations, we now scrutinize one of their most critical inputs: the background error covariance matrix, $B$. This matrix does more than just specify the initial uncertainty of each parameter; its off-diagonal elements encode correlations that dictate how information from a measurement is distributed across different parameters. This practice quantitatively demonstrates the profound impact of these prior correlations, showing how neglecting them can lead to a significant misinterpretation of an experiment's constraining power .",
            "id": "4221186",
            "problem": "Consider a two-parameter cross section adjustment problem arising in nuclear reactor simulation, where first-order perturbation theory states that a small integral response, such as a reactivity-like metric, varies linearly with small relative changes in input macroscopic cross sections. Specifically, let the unknown parameter vector of dimensionless relative deviations be $x = \\begin{pmatrix}\\delta \\sigma_{a,1} \\\\ \\delta \\sigma_{f,1}\\end{pmatrix}$, where $\\delta \\sigma_{a,1}$ and $\\delta \\sigma_{f,1}$ are the group-wise relative changes in absorption and fission macroscopic cross sections for a single energy group. The observed scalar response $y$ is modeled as $y = H x + \\epsilon$, where $H = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix}$ collects the first-order sensitivity coefficients, and $\\epsilon$ is the measurement noise with zero mean and variance $R = \\sigma_{y}^{2}$.\n\nAssume a Gaussian prior for $x$ with zero mean and a background error covariance matrix $B$ that encodes physically motivated correlations between cross section uncertainties (e.g., due to shared nuclear data and modeling approximations). Take\n$$\nB = \\begin{pmatrix}\n0.0036 & 0.00324 \\\\\n0.00324 & 0.0081\n\\end{pmatrix},\n$$\nwhich corresponds to prior standard deviations $\\sigma_{\\delta \\sigma_{a,1}} = 0.06$ and $\\sigma_{\\delta \\sigma_{f,1}} = 0.09$ with correlation coefficient $\\rho = 0.6$. The observation error variance is $R = 0.0025$.\n\nUsing the linear-Gaussian data assimilation framework and starting from first principles of Bayesian inference for Gaussian priors and likelihoods, derive the posterior covariance for $x$ and compute, for the first parameter $\\delta \\sigma_{a,1}$, the variance reduction factor defined as\n$$\n\\mathrm{VRF} = \\frac{\\sigma_{\\delta \\sigma_{a,1}}^{2} - \\mathrm{Var}_{\\text{post}}(\\delta \\sigma_{a,1})}{\\sigma_{\\delta \\sigma_{a,1}}^{2}}.\n$$\nThen quantify the effect of neglecting the off-diagonal terms in $B$ by recomputing the variance reduction factor when the prior covariance is replaced by its diagonal counterpart\n$$\nB_{\\text{diag}} = \\begin{pmatrix}\n0.0036 & 0 \\\\\n0 & 0.0081\n\\end{pmatrix}.\n$$\n\nReport the difference in variance reduction factor\n$$\n\\Delta \\mathrm{VRF} = \\mathrm{VRF}\\big|_{B_{\\text{diag}}} - \\mathrm{VRF}\\big|_{B},\n$$\nexpressed as a decimal and rounded to four significant figures. No units are required for the final answer.",
            "solution": "The posterior distribution $p(x|y)$ is proportional to the product of the prior $p(x) \\propto \\exp(-\\frac{1}{2} x^T B^{-1} x)$ and the likelihood $p(y|x) \\propto \\exp(-\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx))$. The posterior is a Gaussian distribution with covariance matrix $A$ given by the inverse of the Hessian of the negative log-posterior, leading to $A^{-1} = B^{-1} + H^T R^{-1} H$.\nUsing the Woodbury matrix identity, the posterior covariance can be written as:\n$$\nA = B - B H^T (H B H^T + R)^{-1} H B\n$$\nThe prior variance of the first parameter, $\\delta \\sigma_{a,1}$, is $\\sigma_{\\delta \\sigma_{a,1}}^{2} = B_{11} = 0.0036$. The posterior variance is $\\mathrm{Var}_{\\text{post}}(\\delta \\sigma_{a,1}) = A_{11}$.\nThe variance reduction is $B_{11} - A_{11}$. From the Woodbury formula, this is the $(1,1)$ element of the matrix $B H^T (H B H^T + R)^{-1} H B$. Let $S = H B H^T + R$ (a scalar) and $v = B H^T$ (a vector). Then the variance reduction is $(v v^T/S)_{11} = v_1^2/S$.\nThe Variance Reduction Factor is:\n$$\n\\mathrm{VRF} = \\frac{B_{11} - A_{11}}{B_{11}} = \\frac{v_1^2}{S \\cdot B_{11}}\n$$\n\n#### Part 1: Calculation with Full Covariance Matrix $B$\n\nFirst, we calculate the terms needed for $\\mathrm{VRF}\\big|_{B}$ using $B = \\begin{pmatrix} 0.0036 & 0.00324 \\\\ 0.00324 & 0.0081 \\end{pmatrix}$ and $H = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix}$.\nThe vector $v = B H^T$:\n$$\nv = \\begin{pmatrix} 0.0036 & 0.00324 \\\\ 0.00324 & 0.0081 \\end{pmatrix} \\begin{pmatrix} -0.8 \\\\ 1.1 \\end{pmatrix} = \\begin{pmatrix} 0.0036(-0.8) + 0.00324(1.1) \\\\ 0.00324(-0.8) + 0.0081(1.1) \\end{pmatrix} = \\begin{pmatrix} 0.000684 \\\\ 0.006318 \\end{pmatrix}\n$$\nThe innovation covariance term $H B H^T$:\n$$\nH B H^T = H v = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix} \\begin{pmatrix} 0.000684 \\\\ 0.006318 \\end{pmatrix} = 0.0064026\n$$\nThe full innovation covariance $S = H B H^T + R$, with $R = 0.0025$:\n$$\nS = 0.0064026 + 0.0025 = 0.0089026\n$$\nNow, we compute $\\mathrm{VRF}\\big|_{B}$ with $v_1 = 0.000684$ and $B_{11} = 0.0036$:\n$$\n\\mathrm{VRF}\\big|_{B} = \\frac{v_1^2}{S \\cdot B_{11}} = \\frac{(0.000684)^2}{(0.0089026)(0.0036)} = \\frac{4.67856 \\times 10^{-7}}{3.204936 \\times 10^{-5}} \\approx 0.0145979\n$$\n\n#### Part 2: Calculation with Diagonal Covariance Matrix $B_{\\text{diag}}$\n\nNext, we repeat the calculation using $B_{\\text{diag}} = \\begin{pmatrix} 0.0036 & 0 \\\\ 0 & 0.0081 \\end{pmatrix}$.\nThe vector $v_{\\text{diag}} = B_{\\text{diag}} H^T$:\n$$\nv_{\\text{diag}} = \\begin{pmatrix} 0.0036 & 0 \\\\ 0 & 0.0081 \\end{pmatrix} \\begin{pmatrix} -0.8 \\\\ 1.1 \\end{pmatrix} = \\begin{pmatrix} -0.00288 \\\\ 0.00891 \\end{pmatrix}\n$$\nThe innovation covariance term $H B_{\\text{diag}} H^T$:\n$$\nH B_{\\text{diag}} H^T = H v_{\\text{diag}} = \\begin{pmatrix} -0.8 & 1.1 \\end{pmatrix} \\begin{pmatrix} -0.00288 \\\\ 0.00891 \\end{pmatrix} = 0.012105\n$$\nThe full innovation covariance $S_{\\text{diag}} = H B_{\\text{diag}} H^T + R$:\n$$\nS_{\\text{diag}} = 0.012105 + 0.0025 = 0.014605\n$$\nNow, we compute $\\mathrm{VRF}\\big|_{B_{\\text{diag}}}$ with $(v_{\\text{diag}})_1 = -0.00288$ and $(B_{\\text{diag}})_{11} = 0.0036$:\n$$\n\\mathrm{VRF}\\big|_{B_{\\text{diag}}} = \\frac{((-0.00288))^2}{(0.014605)(0.0036)} = \\frac{8.2944 \\times 10^{-6}}{5.2578 \\times 10^{-5}} \\approx 0.157754\n$$\n\n#### Part 3: Difference in Variance Reduction Factors\n\nFinally, we compute the difference $\\Delta \\mathrm{VRF} = \\mathrm{VRF}\\big|_{B_{\\text{diag}}} - \\mathrm{VRF}\\big|_{B}$.\n$$\n\\Delta \\mathrm{VRF} \\approx 0.157754 - 0.0145979 = 0.1431561\n$$\nRounding to four significant figures gives $0.1432$.",
            "answer": "$$\\boxed{0.1432}$$"
        },
        {
            "introduction": "While exact analytical solutions are instructive, real-world nuclear data adjustment involves a vast number of parameters, making direct computation of the posterior covariance intractable. This advanced exercise introduces a powerful technique to manage this complexity by creating a low-rank approximation of the update, guided by Singular Value Decomposition (SVD). You will see how SVD can be used to isolate the directions in the parameter space that are most informed by the measurements, forming the basis for computationally efficient assimilation schemes in high-dimensional systems .",
            "id": "4221125",
            "problem": "Consider a linearized data assimilation step for adjusting multigroup macroscopic cross sections in a pressurized water reactor core simulation. Let the unknown cross-section adjustment vector be $x \\in \\mathbb{R}^{5}$, representing small corrections to five grouped macroscopic cross sections. Let the measurement vector be $y \\in \\mathbb{R}^{4}$, consisting of four detector responses (e.g., flux or reaction-rate tallies) affected by measurement noise. Assume a Gaussian prior $x \\sim \\mathcal{N}(0, P_{f})$ with prior covariance $P_{f} \\in \\mathbb{R}^{5 \\times 5}$, a linearized observation operator $H \\in \\mathbb{R}^{4 \\times 5}$ relating cross sections to measurements, and Gaussian measurement noise $e \\sim \\mathcal{N}(0, R)$, with $R \\in \\mathbb{R}^{4 \\times 4}$ symmetric positive definite. The forward model for the data assimilation is $y = H x + e$.\n\nStarting from the standard linear Gaussian Bayesian framework and the Woodbury identity, derive how a low-rank posterior covariance update arises by truncating singular values of the prior- and noise-preconditioned sensitivity operator. Define the prior- and noise-preconditioned sensitivity matrix $S = R^{-1/2} H P_{f}^{1/2}$ and its Singular Value Decomposition (SVD) $S = U \\Sigma V^{\\top}$, with singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\sigma_{3} \\ge \\sigma_{4} \\ge 0$. Explain how truncating singular values at a threshold $\\tau$ yields a rank-$k$ approximation to the posterior covariance in the prior-whitened coordinates, and justify a norm with which to quantify the approximation error.\n\nIn a specific reactor data assimilation instance, numerical preprocessing yields the singular values of $S$ as $\\sigma_{1} = 2.7$, $\\sigma_{2} = 1.4$, $\\sigma_{3} = 0.75$, and $\\sigma_{4} = 0.30$. Suppose that the low-rank update is chosen by truncation at the threshold $\\tau = 1.0$, so that only singular values strictly greater than $\\tau$ are retained. Quantify the approximation error in the posterior covariance due to this truncation by computing the Frobenius norm of the difference between the exact and truncated posterior covariances in the prior-whitened coordinates, i.e., compute the Frobenius norm of $P_{f}^{-1/2} \\left(P_{a} - P_{a}^{(k)}\\right) P_{f}^{-1/2}$, where $P_{a}$ is the exact posterior covariance and $P_{a}^{(k)}$ is the truncated approximation. Express the result as a dimensionless number and round your answer to four significant figures.",
            "solution": "The inverse of the posterior covariance matrix $P_a$ is given by $P_{a}^{-1} = P_{f}^{-1} + H^{\\top}R^{-1}H$. To analyze the structure of the update, we work in \"prior-whitened\" coordinates. Let $x' = P_{f}^{-1/2}x$. The prior covariance of $x'$ is the identity matrix, $I$. The posterior covariance of $x'$, denoted $P_{a'}$, is $P_{a'} = P_f^{-1/2} P_a P_f^{-1/2}$. By pre- and post-multiplying the equation for $P_a^{-1}$ by $P_f^{1/2}$, we find the inverse posterior covariance in whitened coordinates:\n$$\nP_{a'}^{-1} = P_f^{1/2} (P_f^{-1} + H^{\\top}R^{-1}H) P_f^{1/2} = I + (P_f^{1/2} H^{\\top}R^{-1/2})(R^{-1/2}H P_f^{1/2}) = I + S^{\\top}S\n$$\nwhere $S = R^{-1/2} H P_f^{1/2}$ is the preconditioned sensitivity matrix. The posterior covariance in whitened coordinates is $P_{a'} = (I + S^{\\top}S)^{-1}$.\nUsing the Singular Value Decomposition (SVD) of $S = U \\Sigma V^{\\top}$, we can express $S^{\\top}S = V (\\Sigma^{\\top}\\Sigma) V^{\\top} = V \\Lambda^2 V^{\\top}$, where $\\Lambda^2 = \\Sigma^{\\top}\\Sigma$ is a $5 \\times 5$ diagonal matrix with entries $\\sigma_1^2, \\sigma_2^2, \\sigma_3^2, \\sigma_4^2, 0$.\nThe whitened posterior covariance becomes:\n$$\nP_{a'} = (I + V \\Lambda^2 V^{\\top})^{-1} = V(I + \\Lambda^2)^{-1}V^{\\top}\n$$\nThe matrix $(I + \\Lambda^2)^{-1}$ is diagonal with entries $\\frac{1}{1+\\sigma_i^2}$. A low-rank update arises from truncating the SVD, keeping only the $k$ largest singular values $\\sigma_1, \\dots, \\sigma_k$. This yields an approximate whitened posterior covariance $P_{a'}^{(k)} = V(I + (\\Lambda^{(k)})^2)^{-1}V^{\\top}$, where $(\\Lambda^{(k)})^2$ is a diagonal matrix with entries $\\sigma_1^2, \\dots, \\sigma_k^2, 0, \\dots, 0$. This approximation ignores information from directions corresponding to small singular values, making the update low-rank. The Frobenius norm is a suitable metric to quantify the matrix approximation error.\n\nWe compute the Frobenius norm of the error matrix $E = P_{a'} - P_{a'}^{(k)}$.\n$$\nE = V \\left[ (I + \\Lambda^2)^{-1} - (I + (\\Lambda^{(k)})^2)^{-1} \\right] V^{\\top}\n$$\nDue to the unitary invariance of the Frobenius norm, we have:\n$$\n\\|E\\|_F = \\left\\| (I + \\Lambda^2)^{-1} - (I + (\\Lambda^{(k)})^2)^{-1} \\right\\|_F\n$$\nThe matrix inside the norm is a diagonal matrix, $D = \\operatorname{diag}(d_1, \\dots, d_5)$. The singular values are $\\sigma_1 = 2.7, \\sigma_2 = 1.4, \\sigma_3 = 0.75, \\sigma_4 = 0.30$. The truncation threshold is $\\tau = 1.0$, so we retain $\\sigma_1, \\sigma_2$ ($k=2$). The diagonal entries of the error matrix $D$ are non-zero only for the truncated modes:\n$d_1 = 0$\n$d_2 = 0$\n$d_3 = \\frac{1}{1+\\sigma_3^2} - \\frac{1}{1+0} = \\frac{-\\sigma_3^2}{1+\\sigma_3^2}$\n$d_4 = \\frac{1}{1+\\sigma_4^2} - \\frac{1}{1+0} = \\frac{-\\sigma_4^2}{1+\\sigma_4^2}$\n$d_5 = \\frac{1}{1+0} - \\frac{1}{1+0} = 0$\n\nThe Frobenius norm of the error is the root sum of squares of these elements:\n$$\n\\|E\\|_F = \\sqrt{0^2 + 0^2 + \\left(\\frac{-\\sigma_3^2}{1+\\sigma_3^2}\\right)^2 + \\left(\\frac{-\\sigma_4^2}{1+\\sigma_4^2}\\right)^2 + 0^2} = \\sqrt{\\left(\\frac{\\sigma_3^2}{1+\\sigma_3^2}\\right)^2 + \\left(\\frac{\\sigma_4^2}{1+\\sigma_4^2}\\right)^2}\n$$\nSubstituting numerical values: $\\sigma_3^2 = (0.75)^2 = 0.5625$ and $\\sigma_4^2 = (0.30)^2 = 0.09$.\nTerm for $\\sigma_3$: $\\frac{0.5625}{1+0.5625} = \\frac{0.5625}{1.5625} = 0.36$.\nTerm for $\\sigma_4$: $\\frac{0.09}{1+0.09} = \\frac{0.09}{1.09}$.\n\nNow compute the norm:\n$$\n\\|E\\|_F = \\sqrt{(0.36)^2 + \\left(\\frac{0.09}{1.09}\\right)^2}\n$$\n$$\n\\|E\\|_F = \\sqrt{0.1296 + (0.0825688...)^2}\n$$\n$$\n\\|E\\|_F = \\sqrt{0.1296 + 0.0068176...}\n$$\n$$\n\\|E\\|_F = \\sqrt{0.1364176...} \\approx 0.3693475...\n$$\nRounding to four significant figures, the result is $0.3693$.",
            "answer": "$$\n\\boxed{0.3693}\n$$"
        }
    ]
}