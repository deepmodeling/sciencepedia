{
    "hands_on_practices": [
        {
            "introduction": "The first step in constructing a Polynomial Chaos Expansion (PCE) is to define the set of basis polynomials. The size of this basis is a critical parameter, as it directly influences the computational expense of building and using the surrogate model. This exercise explores the combinatorial nature of a total-degree polynomial basis, which is a common choice for managing model complexity. By deriving the formula for the number of basis functions, you will gain a concrete understanding of how the problem size scales with the number of uncertain parameters and the polynomial degree, a challenge often termed the \"curse of dimensionality\" .",
            "id": "4252603",
            "problem": "In a steady-state nuclear reactor simulation, the dominant uncertain input parameters for the one-group neutron diffusion model are represented by $d$ independent standardized random variables. A Polynomial Chaos Expansion (PCE) is constructed to approximate a scalar quantity of interest (e.g., an eigenvalue or an integral flux functional) as a finite sum of multivariate orthogonal polynomials in these $d$ variables. The basis is truncated by total degree, meaning all multivariate monomials with multi-index $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\ldots, \\alpha_{d}) \\in \\mathbb{N}_{0}^{d}$ such that $\\sum_{i=1}^{d} \\alpha_{i} \\leq p$ are retained for a prescribed total degree $p \\in \\mathbb{N}_{0}$. Starting from the definition of this total-degree truncation and the notion that basis functions correspond bijectively to such multi-indices, derive a closed-form combinatorial expression for the total number of retained basis polynomials as a function of $d$ and $p$, using only first principles from elementary counting. Then evaluate this expression for $d = 15$ and $p = 4$. Provide the final integer value for this case. No rounding is required.",
            "solution": "The problem requires the derivation of a closed-form combinatorial expression for the number of basis polynomials in a total-degree truncated Polynomial Chaos Expansion (PCE), and its subsequent evaluation for a specific case.\n\nThe problem statement has been validated and is deemed valid. It is scientifically grounded in the field of uncertainty quantification for nuclear engineering, well-posed as a combinatorial counting problem, and stated objectively with precise definitions.\n\nLet $d$ be the number of independent random variables and $p$ be the prescribed total degree for the truncation. The set of retained basis polynomials is in a bijective correspondence with the set of multi-indices $\\boldsymbol{\\alpha} = (\\alpha_{1}, \\ldots, \\alpha_{d}) \\in \\mathbb{N}_{0}^{d}$ such that the sum of their components is no greater than $p$. This condition is expressed by the inequality:\n$$ \\sum_{i=1}^{d} \\alpha_{i} \\leq p $$\nwhere each $\\alpha_{i}$ is a non-negative integer, i.e., $\\alpha_{i} \\in \\{0, 1, 2, \\ldots\\}$.\n\nThe task is to find the cardinality of the set of all such multi-indices, which we denote as $N$. This is a problem in elementary combinatorics. To derive the solution from first principles, we transform the inequality into an equation. We introduce an auxiliary non-negative integer variable, often called a \"slack\" variable, $\\alpha_{d+1} \\in \\mathbb{N}_{0}$. This variable is defined as:\n$$ \\alpha_{d+1} = p - \\sum_{i=1}^{d} \\alpha_{i} $$\nSince $\\sum_{i=1}^{d} \\alpha_{i} \\leq p$, it is guaranteed that $\\alpha_{d+1} \\geq 0$, so $\\alpha_{d+1}$ is indeed a non-negative integer. The original inequality can now be rewritten as an equality:\n$$ \\alpha_{1} + \\alpha_{2} + \\ldots + \\alpha_{d} + \\alpha_{d+1} = p $$\nThere exists a one-to-one correspondence between the set of solutions $(\\alpha_1, \\ldots, \\alpha_d)$ to the inequality and the set of solutions $(\\alpha_1, \\ldots, \\alpha_d, \\alpha_{d+1})$ to the equation. Therefore, counting the number of solutions to the inequality is equivalent to counting the number of non-negative integer solutions to this new equation with $d+1$ variables.\n\nThis is a classic combinatorial problem known as a \"stars and bars\" problem. We are seeking the number of ways to write the integer $p$ as a sum of $d+1$ non-negative integers. This is equivalent to distributing $p$ identical items (the \"stars\") into $d+1$ distinct bins (the variables $\\alpha_1, \\ldots, \\alpha_{d+1}$).\n\nTo visualize this, imagine a sequence of $p$ stars ($*$) and $d$ bars ($|$). The bars act as dividers that partition the stars into $d+1$ groups. For instance, the sequence $* * | * | | * * *$ for $p=6$ and $d+1=4$ would correspond to the solution $\\alpha_1=2$, $\\alpha_2=1$, $\\alpha_3=0$, $\\alpha_4=3$.\n\nThe total number of objects in the sequence is $p$ stars plus $d$ bars, which is $p+d$ objects. The total number of positions in the sequence is thus $p+d$. The problem reduces to finding the number of ways to choose the positions for the $d$ bars from the $p+d$ available positions. The remaining $p$ positions will be filled by the stars. This is a standard combination problem, and the number of ways is given by the binomial coefficient:\n$$ N = \\binom{p+d}{d} $$\nEquivalently, one could choose the $p$ positions for the stars out of the $p+d$ total positions, which yields the identical result:\n$$ N = \\binom{p+d}{p} $$\nThis is the required closed-form combinatorial expression for the total number of retained basis polynomials as a function of $d$ and $p$.\n\nThe problem then asks to evaluate this expression for $d=15$ and $p=4$. Using the derived formula, the number of basis functions is:\n$$ N = \\binom{p+d}{p} = \\binom{4+15}{4} = \\binom{19}{4} $$\nThe value of this binomial coefficient is calculated as:\n$$ \\binom{19}{4} = \\frac{19!}{4!(19-4)!} = \\frac{19!}{4!15!} $$\nExpanding the factorials for computation:\n$$ N = \\frac{19 \\times 18 \\times 17 \\times 16}{4 \\times 3 \\times 2 \\times 1} $$\nWe can simplify the expression by canceling terms in the numerator and the denominator:\n$$ N = 19 \\times \\frac{18}{3 \\times 2} \\times 17 \\times \\frac{16}{4} $$\n$$ N = 19 \\times 3 \\times 17 \\times 4 $$\nPerforming the multiplication:\n$$ N = (19 \\times 3) \\times (17 \\times 4) = 57 \\times 68 $$\n$$ N = 3876 $$\nThus, for a PCE with $d=15$ uncertain parameters and a total-degree truncation of $p=4$, there are $3876$ basis polynomials.",
            "answer": "$$\\boxed{3876}$$"
        },
        {
            "introduction": "After defining the polynomial basis, the core task in a non-intrusive approach is computing the PCE coefficients. These coefficients are determined by projecting the model output onto each basis polynomial, a process that requires evaluating complex, high-dimensional integrals. This practice guides you through the design and implementation of an adaptive quadrature algorithm, a sophisticated numerical technique that intelligently allocates computational resources to regions of the integration domain where the function shows high variation. Mastering this method provides you with a robust and efficient tool for accurately calculating spectral coefficients for realistic, non-polynomial models like the reactor simulation for $k_{\\mathrm{eff}}$ .",
            "id": "4252630",
            "problem": "Consider an uncertainty quantification task in nuclear reactor simulation, where the effective multiplication factor $k_{\\mathrm{eff}}$ depends on uncertain inputs $\\mathbf{X}$ that are modeled as independent random variables uniformly distributed on $[-1,1]$. Let the probability density be $\\rho(\\mathbf{x}) = (1/2)^d$ on $[-1,1]^d$ with $d \\in \\{1,2\\}$. The goal is to compute spectral coefficients in a Polynomial Chaos (PC) expansion, where the basis functions are orthonormal Legendre polynomials. Specifically, define the one-dimensional orthonormal Legendre basis functions $\\phi_n(x)$ by $\\phi_n(x) = \\sqrt{\\frac{2n+1}{2}}\\,P_n(x)$, where $P_n(x)$ is the $n$-th Legendre polynomial, and extend to multiple dimensions by tensor products $\\Psi_{\\alpha}(\\mathbf{x}) = \\prod_{i=1}^{d} \\phi_{\\alpha_i}(x_i)$ for multi-index $\\alpha \\in \\mathbb{N}_0^d$. The spectral coefficient is the integral\n$$\nI_{\\alpha} = \\int_{[-1,1]^d} k_{\\mathrm{eff}}(\\mathbf{x})\\,\\Psi_{\\alpha}(\\mathbf{x})\\,\\rho(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}.\n$$\nAngles in any trigonometric function below must be interpreted in radians.\n\nIn dimension $d = 1$, use\n$$\nk_{\\mathrm{eff}}(x) = 1.01 + 0.12\\,x + 0.18\\,\\exp\\!\\left(-7\\,(x - 0.2)^2\\right) + 0.04\\,\\sin(7\\,x).\n$$\nIn dimension $d = 2$, use\n$$\nk_{\\mathrm{eff}}(x,y) = 1.02 + 0.09\\,x - 0.06\\,y + 0.14\\,\\exp\\!\\left(-5\\,\\left[(x - 0.25)^2 + (y + 0.35)^2\\right]\\right) + 0.05\\,\\sin(4\\,x + 3\\,y) + 0.03\\,\\sin(6\\,x\\,y).\n$$\n\nDesign and implement an adaptive quadrature scheme that targets integrand regions of high variation in $k_{\\mathrm{eff}}(\\mathbf{X})\\Psi_{\\alpha}(\\mathbf{X})$ and estimates error via a posteriori indicators. The scheme must satisfy all of the following requirements:\n\n- Represent the domain $[-1,1]^d$ as a hierarchical partition of axis-aligned hyperrectangles (cells). Start from a single root cell covering the entire domain.\n- On each cell, approximate the local integral of $k_{\\mathrm{eff}}(\\mathbf{x})\\,\\Psi_{\\alpha}(\\mathbf{x})\\,\\rho(\\mathbf{x})$ using two embedded tensor-product Gauss–Legendre quadratures with $n_{\\text{low}}$ and $n_{\\text{high}}$ points per dimension on the reference interval $[-1,1]$, mapped affinely to the cell. Let the Jacobian of the affine map be $J = \\prod_{i=1}^{d} \\frac{b_i - a_i}{2}$ for a cell with bounds $[a_i,b_i]$.\n- Use the difference of the two approximations on each cell, $\\eta_{\\text{cell}} = |I_{\\text{high,cell}} - I_{\\text{low,cell}}|$, as an a posteriori error indicator. Let the global integral approximation be the sum of fine approximations over all cells, $I_{\\text{global}} = \\sum_{\\text{cells}} I_{\\text{high,cell}}$, and the global error indicator be the sum of cell indicators, $\\eta_{\\text{global}} = \\sum_{\\text{cells}} \\eta_{\\text{cell}}$.\n- Refine adaptively by repeatedly selecting the cell with the largest error indicator and bisecting it along its longest edge (i.e., split the interval $[a_j,b_j]$ where $j = \\arg\\max_i (b_i - a_i)$ at its midpoint). Replace the parent cell by its two children and update $I_{\\text{global}}$ and $\\eta_{\\text{global}}$ accordingly.\n- Terminate when either $\\eta_{\\text{global}} \\leq \\varepsilon$ (tolerance) or a maximum number of cells $N_{\\max}$ has been reached. Report whether the tolerance was met.\n\nYour program must compute $I_{\\alpha}$ using the adaptive scheme described above for the following test suite. In every case, $n_{\\text{low}}$ and $n_{\\text{high}}$ are the numbers of Gauss–Legendre nodes per dimension for the embedded quadrature pair, $\\varepsilon$ is the absolute tolerance, and $N_{\\max}$ is the maximum number of cells.\n\n- Test $1$: $d = 1$, $\\alpha = (0)$, $n_{\\text{low}} = 3$, $n_{\\text{high}} = 7$, $\\varepsilon = 10^{-9}$, $N_{\\max} = 200$.\n- Test $2$: $d = 1$, $\\alpha = (4)$, $n_{\\text{low}} = 3$, $n_{\\text{high}} = 9$, $\\varepsilon = 10^{-8}$, $N_{\\max} = 200$.\n- Test $3$: $d = 2$, $\\alpha = (0,0)$, $n_{\\text{low}} = 3$, $n_{\\text{high}} = 7$, $\\varepsilon = 10^{-7}$, $N_{\\max} = 300$.\n- Test $4$: $d = 2$, $\\alpha = (2,3)$, $n_{\\text{low}} = 3$, $n_{\\text{high}} = 9$, $\\varepsilon = 10^{-7}$, $N_{\\max} = 300$.\n- Test $5$ (boundary condition): $d = 2$, $\\alpha = (0,0)$, $n_{\\text{low}} = 3$, $n_{\\text{high}} = 9$, $\\varepsilon = 10^{-12}$, $N_{\\max} = 100$.\n\nThe final output format must be a single line containing a list of lists, one per test case, where each inner list holds three entries: the global integral approximation $I_{\\text{global}}$ as a floating-point number, the global error indicator $\\eta_{\\text{global}}$ as a floating-point number, and a boolean indicating whether the tolerance was achieved. For example,\n$[\\,[I_1,\\eta_1,\\text{True}],\\,[I_2,\\eta_2,\\text{False}],\\,\\dots]$.\n\nNo user input is required. There are no physical units in this problem; all quantities are dimensionless. All angles are in radians. Your program should produce this single line as its only output.",
            "solution": "The task is to compute the spectral coefficient $I_{\\alpha}$ of a Polynomial Chaos (PC) expansion for $k_{\\mathrm{eff}}(\\mathbf{X})$ with respect to orthonormal Legendre basis functions. The PC expansion is a spectral representation of a random quantity built upon orthogonal polynomials with respect to the distribution of the inputs. The fundamental base consists of the definitions of expectation, orthogonality, and tensor-product integration.\n\nStarting from first principles, the spectral coefficient is an expectation with respect to the joint distribution of $\\mathbf{X}$,\n$$\nI_{\\alpha} = \\mathbb{E}\\left[k_{\\mathrm{eff}}(\\mathbf{X})\\,\\Psi_{\\alpha}(\\mathbf{X})\\right]\n= \\int_{[-1,1]^d} k_{\\mathrm{eff}}(\\mathbf{x})\\,\\Psi_{\\alpha}(\\mathbf{x})\\,\\rho(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x},\n$$\nwhere $\\rho(\\mathbf{x}) = \\left(\\frac{1}{2}\\right)^d$ is the probability density for independent uniform inputs, and the basis functions are orthonormal, i.e., for one dimension,\n$$\n\\int_{-1}^1 \\phi_m(x)\\,\\phi_n(x)\\,\\left(\\frac{1}{2}\\right)\\,\\mathrm{d}x = \\delta_{mn}.\n$$\nIn higher dimensions, orthonormality extends via tensor products,\n$$\n\\int_{[-1,1]^d} \\Psi_{\\alpha}(\\mathbf{x})\\,\\Psi_{\\beta}(\\mathbf{x})\\,\\rho(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x} = \\delta_{\\alpha\\beta}.\n$$\nFor the uniform measure, the appropriate orthonormal polynomials are scaled Legendre polynomials:\n$$\n\\phi_n(x) = \\sqrt{\\frac{2n+1}{2}}\\,P_n(x),\n\\quad\n\\Psi_{\\alpha}(\\mathbf{x}) = \\prod_{i=1}^d \\sqrt{\\frac{2\\alpha_i+1}{2}}\\,P_{\\alpha_i}(x_i).\n$$\n\nSince closed-form evaluation of $I_{\\alpha}$ for the given $k_{\\mathrm{eff}}$ is not available, we turn to numerical quadrature. To ensure accuracy efficiently across regions of varying integrand complexity, we employ adaptive quadrature. The core design elements rely on:\n\n- Mapping and tensor-product quadrature: Any hyperrectangle (cell) $[a_1,b_1]\\times\\cdots\\times[a_d,b_d] \\subseteq [-1,1]^d$ can be mapped from the reference domain $[-1,1]^d$ via the affine map\n$$\nx_i = \\frac{a_i + b_i}{2} + \\frac{b_i - a_i}{2}\\,\\hat{x}_i,\n\\quad \\text{with Jacobian } J = \\prod_{i=1}^d \\frac{b_i - a_i}{2}.\n$$\nUsing tensor-product Gauss–Legendre quadrature of order $n$ on the reference coordinates $\\hat{\\mathbf{x}}$, we approximate the cell integral as\n$$\nI_{\\text{cell}}^{(n)} \\approx \\sum_{q_1=1}^n \\cdots \\sum_{q_d=1}^n\n\\left[ k_{\\mathrm{eff}}(\\mathbf{x}(\\hat{\\mathbf{x}}))\\,\\Psi_{\\alpha}(\\mathbf{x}(\\hat{\\mathbf{x}}))\\,\\rho(\\mathbf{x}(\\hat{\\mathbf{x}})) \\right]\n\\left( \\prod_{i=1}^d w_{q_i}^{(n)} \\right) J,\n$$\nwhere $w_{q_i}^{(n)}$ are the one-dimensional Gauss–Legendre weights.\n\n- Embedded quadrature for a posteriori error: Use two nested quadrature orders, $n_{\\text{low}}$ and $n_{\\text{high}}$, to obtain two approximations on the same cell. The absolute difference\n$$\n\\eta_{\\text{cell}} = \\left| I_{\\text{cell}}^{(n_{\\text{high}})} - I_{\\text{cell}}^{(n_{\\text{low}})} \\right|\n$$\nserves as an a posteriori error indicator. This indicator is robust in practice because higher-order Gauss–Legendre quadrature provides a superior approximation for sufficiently smooth integrands, and the difference with a lower-order rule estimates the local quadrature error.\n\n- Adaptive refinement criterion: To target regions of high variation in the integrand $k_{\\mathrm{eff}}(\\mathbf{x})\\,\\Psi_{\\alpha}(\\mathbf{x})$, repeatedly refine the most erroneous cell. The simplest and effective strategy is bisection along the cell’s longest edge. This improves resolution preferentially where the spatial scale is largest and the error is highest, capturing gradients or localized features (bumps and oscillations) efficiently.\n\nAlgorithmic steps:\n1. Initialize with a single cell covering $[-1,1]^d$.\n2. Compute $I_{\\text{cell}}^{(n_{\\text{low}})}$, $I_{\\text{cell}}^{(n_{\\text{high}})}$, and $\\eta_{\\text{cell}}$ on the current set of cells.\n3. Aggregate $I_{\\text{global}} = \\sum I_{\\text{cell}}^{(n_{\\text{high}})}$ and $\\eta_{\\text{global}} = \\sum \\eta_{\\text{cell}}$.\n4. If $\\eta_{\\text{global}} \\leq \\varepsilon$, stop and report convergence. Otherwise, identify the cell with the largest $\\eta_{\\text{cell}}$, bisect it along the longest dimension, and replace it by two children.\n5. Recompute local integrals on the new children, update $I_{\\text{global}}$ and $\\eta_{\\text{global}}$, and iterate until the stopping criterion or maximum cell limit $N_{\\max}$ is reached.\n\nJustification of correctness:\n- The expectation integral is well-defined under the uniform distribution and orthonormal PC basis. The tensor-product Gauss–Legendre quadrature converges for smooth integrands as the order increases. The embedded pair provides a consistent local error indicator that reflects the integrand’s variation within each cell due to the improved accuracy of the higher-order rule.\n- Adaptive partitioning focuses computational effort on cells with large $\\eta_{\\text{cell}}$, which correspond to regions where $k_{\\mathrm{eff}}(\\mathbf{x})\\,\\Psi_{\\alpha}(\\mathbf{x})$ has significant variation, such as near localized exponentials or oscillations from sine terms. Bisection along the longest edge is an effective heuristic that balances refinement across dimensions and aligns with anisotropic features.\n- Summing the fine quadrature contributions across all cells approximates the global integral; summing the local indicators provides a conservative bound-like indicator of total quadrature error. While this sum is not a strict mathematical bound, it is a standard and practical a posteriori measure in adaptive integration.\n\nImplementation details:\n- Use the function $P_n(x)$ via standard Legendre polynomial evaluation and compute $\\phi_n(x) = \\sqrt{\\frac{2n+1}{2}}\\,P_n(x)$.\n- For $d = 1$, $\\Psi_{\\alpha}(x) = \\phi_{\\alpha_1}(x)$; for $d = 2$, $\\Psi_{\\alpha}(x,y) = \\phi_{\\alpha_1}(x)\\,\\phi_{\\alpha_2}(y)$.\n- Include the density $\\rho(\\mathbf{x}) = \\left(\\frac{1}{2}\\right)^d$ directly in the integrand.\n- Construct tensor-product quadrature nodes and weights with Gauss–Legendre rules, map to each cell, and compute the local integral.\n\nThe program evaluates the five specified test cases and outputs a single line with the list of $[I_{\\text{global}}, \\eta_{\\text{global}}, \\text{converged}]$ triplets, one per test case, ensuring reproducibility and coverage of normal and boundary behavior.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Orthonormal Legendre basis: phi_n(x) = sqrt((2n+1)/2) * P_n(x)\ndef normalized_legendre(n, x):\n    # Evaluate P_n(x) using numpy.polynomial.legendre.legval with coefficients\n    coeffs = np.zeros(n + 1)\n    coeffs[n] = 1.0\n    Pn = np.polynomial.legendre.legval(x, coeffs)\n    return np.sqrt((2 * n + 1) / 2.0) * Pn\n\ndef psi_alpha_1d(alpha, x):\n    # alpha is a tuple/list of length 1\n    return normalized_legendre(alpha[0], x)\n\ndef psi_alpha_2d(alpha, x, y):\n    return normalized_legendre(alpha[0], x) * normalized_legendre(alpha[1], y)\n\n# k_eff definitions\ndef k_eff_1d(x):\n    # k_eff(x) = 1.01 + 0.12 x + 0.18 exp(-7 (x - 0.2)^2) + 0.04 sin(7 x)\n    return 1.01 + 0.12 * x + 0.18 * np.exp(-7.0 * (x - 0.2) ** 2) + 0.04 * np.sin(7.0 * x)\n\ndef k_eff_2d(x, y):\n    # k_eff(x,y) = 1.02 + 0.09 x - 0.06 y + 0.14 exp(-5[(x-0.25)^2 + (y+0.35)^2]) + 0.05 sin(4x+3y) + 0.03 sin(6xy)\n    return (\n        1.02\n        + 0.09 * x\n        - 0.06 * y\n        + 0.14 * np.exp(-5.0 * ((x - 0.25) ** 2 + (y + 0.35) ** 2))\n        + 0.05 * np.sin(4.0 * x + 3.0 * y)\n        + 0.03 * np.sin(6.0 * x * y)\n    )\n\ndef tensor_quadrature_on_cell(d, alpha, a, b, n):\n    \"\"\"\n    Compute tensor-product Gauss-Legendre quadrature of order n per dimension\n    for the integrand g(x) = k_eff(x) * Psi_alpha(x) * rho on a hyperrectangular cell with bounds a,b.\n    \"\"\"\n    # One-dimensional Gauss-Legendre nodes and weights on [-1,1]\n    nodes, weights = np.polynomial.legendre.leggauss(n)\n\n    # Precompute scaling and center for affine map\n    a = np.asarray(a, dtype=float)\n    b = np.asarray(b, dtype=float)\n    centers = (a + b) / 2.0\n    scales = (b - a) / 2.0\n    J = np.prod(scales)  # Jacobian\n\n    # Build tensor grids for nodes and weights\n    node_grids = np.meshgrid(*([nodes] * d), indexing='ij')\n    weight_grids = np.meshgrid(*([weights] * d), indexing='ij')\n    # Map to physical coordinates\n    # Each coordinate array is centers[i] + scales[i] * node_grids[i]\n    phys_coords = [centers[i] + scales[i] * node_grids[i] for i in range(d)]\n    # Product weights across dimensions\n    wprod = np.prod(np.array(weight_grids), axis=0)\n\n    rho = (0.5) ** d\n\n    if d == 1:\n        x = phys_coords[0]\n        integrand = k_eff_1d(x) * psi_alpha_1d(alpha, x) * rho\n    elif d == 2:\n        x = phys_coords[0]\n        y = phys_coords[1]\n        integrand = k_eff_2d(x, y) * psi_alpha_2d(alpha, x, y) * rho\n    else:\n        raise ValueError(\"Unsupported dimension d: {}\".format(d))\n\n    # Sum over all points\n    I = np.sum(integrand * wprod) * J\n    return I\n\ndef adaptive_integral(d, alpha, n_low, n_high, tol, max_cells):\n    \"\"\"\n    Adaptive quadrature over [-1,1]^d using embedded Gauss-Legendre tensor-product rules.\n    \"\"\"\n    # Initial single cell covering the domain\n    a0 = np.array([-1.0] * d)\n    b0 = np.array([1.0] * d)\n    I_low = tensor_quadrature_on_cell(d, alpha, a0, b0, n_low)\n    I_high = tensor_quadrature_on_cell(d, alpha, a0, b0, n_high)\n    err = abs(I_high - I_low)\n    cells = [{\n        'a': a0,\n        'b': b0,\n        'I_low': I_low,\n        'I_high': I_high,\n        'err': err\n    }]\n\n    I_global = I_high\n    eta_global = err\n\n    converged = eta_global <= tol\n    # Adaptive loop\n    while (not converged) and (len(cells) < max_cells):\n        # Select cell with maximum error\n        idx = int(np.argmax([c['err'] for c in cells]))\n        cell = cells[idx]\n        a = cell['a']\n        b = cell['b']\n        lengths = b - a\n        split_dim = int(np.argmax(lengths))\n        mid = 0.5 * (a[split_dim] + b[split_dim])\n\n        # Create two child cells by bisecting along split_dim\n        a_left = a.copy()\n        b_left = b.copy()\n        b_left[split_dim] = mid\n\n        a_right = a.copy()\n        b_right = b.copy()\n        a_right[split_dim] = mid\n\n        # Compute integrals on children\n        I_low_left = tensor_quadrature_on_cell(d, alpha, a_left, b_left, n_low)\n        I_high_left = tensor_quadrature_on_cell(d, alpha, a_left, b_left, n_high)\n        err_left = abs(I_high_left - I_low_left)\n\n        I_low_right = tensor_quadrature_on_cell(d, alpha, a_right, b_right, n_low)\n        I_high_right = tensor_quadrature_on_cell(d, alpha, a_right, b_right, n_high)\n        err_right = abs(I_high_right - I_low_right)\n\n        # Update global sums: remove parent, add children\n        I_global += (I_high_left + I_high_right - cell['I_high'])\n        eta_global += (err_left + err_right - cell['err'])\n\n        # Replace parent cell with children\n        cells.pop(idx)\n        cells.append({\n            'a': a_left, 'b': b_left,\n            'I_low': I_low_left, 'I_high': I_high_left, 'err': err_left\n        })\n        cells.append({\n            'a': a_right, 'b': b_right,\n            'I_low': I_low_right, 'I_high': I_high_right, 'err': err_right\n        })\n\n        converged = eta_global <= tol\n\n    return I_global, eta_global, converged\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (d, alpha, n_low, n_high, tol, max_cells)\n    test_cases = [\n        (1, (0,), 3, 7, 1e-9, 200),\n        (1, (4,), 3, 9, 1e-8, 200),\n        (2, (0, 0), 3, 7, 1e-7, 300),\n        (2, (2, 3), 3, 9, 1e-7, 300),\n        (2, (0, 0), 3, 9, 1e-12, 100),\n    ]\n\n    results = []\n    for case in test_cases:\n        d, alpha, n_low, n_high, tol, max_cells = case\n        I_est, eta, conv = adaptive_integral(d, alpha, n_low, n_high, tol, max_cells)\n        results.append([I_est, eta, bool(conv)])\n\n    # Final print statement in the exact required format.\n    # Print single line: list of lists with [I_global, eta_global, converged]\n    # Use default str() representation; ensure one line.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A constructed PCE is more than a fast-running approximation; it is a powerful analytical tool for sensitivity analysis. This practice demonstrates how to use a PCE to quantify the influence of an uncertain input on the model output. You will derive and compute the correlation ratio, $\\eta^2$, a variance-based measure that captures the total contribution of an input to the output's variance, accounting for both linear and nonlinear relationships. By contrasting $\\eta^2$ with the simpler Pearson correlation coefficient, you will develop a deeper insight into how to properly interpret sensitivity, a critical skill for any uncertainty quantification analysis .",
            "id": "4252595",
            "problem": "In a pressurized water reactor core model, the moderator density $\\delta$ (in $\\mathrm{kg/m^3}$) is uncertain due to thermal-hydraulic fluctuations. Treat $\\delta$ as a Gaussian random variable with $\\delta \\sim \\mathcal{N}(\\delta_0,\\sigma_{\\delta}^2)$, where $\\delta_0 = 700$ and $\\sigma_{\\delta} = 7$. Define the standardized variable $\\xi = (\\delta - \\delta_0)/\\sigma_{\\delta}$ so that $\\xi \\sim \\mathcal{N}(0,1)$. The reactor reactivity $\\rho$ is approximated by a third-order Polynomial Chaos Expansion (PCE), using the probabilists’ Hermite polynomials with respect to the standard normal measure:\n$$\n\\rho(\\xi) = a_0 + a_1 \\,\\xi + a_2 \\,(\\xi^2 - 1) + a_3 \\,(\\xi^3 - 3\\,\\xi) + \\varepsilon,\n$$\nwhere $\\varepsilon$ is an independent stochastic term representing unresolved modeling and input uncertainties, with $\\mathbb{E}[\\varepsilon]=0$ and $\\mathrm{Var}(\\varepsilon)=\\sigma_{\\varepsilon}^2$. The coefficients are estimated from high-fidelity stochastic sampling of the coupled neutron transport and thermal-hydraulics simulation and are given by\n$$\na_0 = 5.0 \\times 10^{-3}, \\quad a_1 = 2.0 \\times 10^{-3}, \\quad a_2 = 3.0 \\times 10^{-3}, \\quad a_3 = 1.0 \\times 10^{-3}, \\quad \\sigma_{\\varepsilon}^2 = 1.2 \\times 10^{-5}.\n$$\n\nStarting from the definition of the correlation ratio $\\eta^2(\\rho \\mid \\delta) = \\frac{\\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\delta])}{\\mathrm{Var}(\\rho)}$ and the law of total variance, derive a closed-form expression for $\\eta^2(\\rho \\mid \\delta)$ in terms of the expansion coefficients $a_1, a_2, a_3$ and $\\sigma_{\\varepsilon}^2$, and then evaluate it numerically for the given values. Finally, explain in words, using the derived expressions, why $\\eta^2$ differs from the Pearson correlation coefficient when the relationship between $\\rho$ and $\\delta$ is nonlinear.\n\nExpress the final value of $\\eta^2$ as a pure number with no units, and round your answer to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in established principles of probability theory and uncertainty quantification (Polynomial Chaos Expansion, law of total variance), well-posed with sufficient information for a unique solution, and expressed in objective, precise language.\n\nThe objective is to derive and evaluate the correlation ratio, $\\eta^2(\\rho \\mid \\delta)$, and to explain its distinction from the squared Pearson correlation coefficient. The correlation ratio is defined as:\n$$\n\\eta^2(\\rho \\mid \\delta) = \\frac{\\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\delta])}{\\mathrm{Var}(\\rho)}\n$$\nThe analysis begins by computing the two variance terms in this expression.\n\nFirst, we determine the numerator, $\\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\delta])$. The standardized random variable $\\xi$ is a linear transformation of the moderator density $\\delta$, given by $\\xi = (\\delta - \\delta_0)/\\sigma_{\\delta}$. Therefore, conditioning on $\\delta$ is equivalent to conditioning on $\\xi$. The conditional expectation of the reactivity $\\rho$ given $\\xi$ is:\n$$\n\\mathbb{E}[\\rho \\mid \\xi] = \\mathbb{E}\\left[ a_0 + a_1 \\xi + a_2 (\\xi^2 - 1) + a_3 (\\xi^3 - 3\\xi) + \\varepsilon \\mid \\xi \\right]\n$$\nGiven $\\xi$, all terms involving $\\xi$ are deterministic. The term $\\varepsilon$ is defined as an independent stochastic variable with $\\mathbb{E}[\\varepsilon] = 0$. Its independence from $\\xi$ implies $\\mathbb{E}[\\varepsilon \\mid \\xi] = \\mathbb{E}[\\varepsilon] = 0$. Thus, the conditional expectation simplifies to the Polynomial Chaos Expansion (PCE) part of the model:\n$$\n\\mathbb{E}[\\rho \\mid \\xi] = a_0 + a_1 \\xi + a_2 (\\xi^2 - 1) + a_3 (\\xi^3 - 3\\xi)\n$$\nThe polynomials in the expansion, $H_0(\\xi)=1$, $H_1(\\xi)=\\xi$, $H_2(\\xi)=\\xi^2 - 1$, and $H_3(\\xi)=\\xi^3 - 3\\xi$, are the probabilists' Hermite polynomials. A fundamental property of these polynomials is their orthogonality with respect to the standard normal probability measure, which is the distribution of $\\xi \\sim \\mathcal{N}(0,1)$:\n$$\n\\mathbb{E}[H_i(\\xi) H_j(\\xi)] = \\int_{-\\infty}^{\\infty} H_i(x)H_j(x) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx = \\delta_{ij} i!\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta.\n\nTo find the variance of $\\mathbb{E}[\\rho \\mid \\xi]$, we first find its mean:\n$$\n\\mathbb{E}\\left[\\mathbb{E}[\\rho \\mid \\xi]\\right] = \\mathbb{E}[a_0 H_0(\\xi) + a_1 H_1(\\xi) + a_2 H_2(\\xi) + a_3 H_3(\\xi)] = a_0 \\mathbb{E}[H_0] + a_1 \\mathbb{E}[H_1] + \\dots\n$$\nFrom the orthogonality property, $\\mathbb{E}[H_i] = \\mathbb{E}[H_i H_0] = 0$ for $i > 0$, and $\\mathbb{E}[H_0] = 1$. Therefore, $\\mathbb{E}[\\mathbb{E}[\\rho \\mid \\xi]] = a_0$.\nThe variance is then:\n$$\n\\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\xi]) = \\mathbb{E}\\left[ (\\mathbb{E}[\\rho \\mid \\xi] - a_0)^2 \\right] = \\mathbb{E}\\left[ (a_1 H_1(\\xi) + a_2 H_2(\\xi) + a_3 H_3(\\xi))^2 \\right]\n$$\nExpanding the square and applying the expectation operator, the cross-product terms vanish due to orthogonality, leaving:\n$$\n\\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\xi]) = a_1^2 \\mathbb{E}[H_1^2] + a_2^2 \\mathbb{E}[H_2^2] + a_3^2 \\mathbb{E}[H_3^2]\n$$\nUsing $\\mathbb{E}[H_i^2] = i!$, we find the numerator of the correlation ratio:\n$$\n\\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\delta]) = a_1^2 (1!) + a_2^2 (2!) + a_3^2 (3!) = a_1^2 + 2a_2^2 + 6a_3^2\n$$\nNext, we determine the denominator, $\\mathrm{Var}(\\rho)$. This is the total variance of the reactivity. We use the law of total variance:\n$$\n\\mathrm{Var}(\\rho) = \\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\delta]) + \\mathbb{E}[\\mathrm{Var}(\\rho \\mid \\delta)]\n$$\nWe have already calculated the first term. The second term is the expectation of the conditional variance. The conditional variance is:\n$$\n\\mathrm{Var}(\\rho \\mid \\xi) = \\mathrm{Var}\\left( a_0 + a_1 \\xi + a_2 (\\xi^2 - 1) + a_3 (\\xi^3 - 3\\xi) + \\varepsilon \\mid \\xi \\right)\n$$\nGiven $\\xi$, the entire PCE component is a constant. The variance of a constant is zero, and the variance of a sum with an independent variable is just the variance of that variable.\n$$\n\\mathrm{Var}(\\rho \\mid \\xi) = \\mathrm{Var}(\\varepsilon \\mid \\xi)\n$$\nSince $\\varepsilon$ is independent of $\\xi$, $\\mathrm{Var}(\\varepsilon \\mid \\xi) = \\mathrm{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^2$.\nTherefore, $\\mathbb{E}[\\mathrm{Var}(\\rho \\mid \\delta)] = \\mathbb{E}[\\sigma_{\\varepsilon}^2] = \\sigma_{\\varepsilon}^2$.\nThe total variance is the sum of the variance of the conditional expectation and the mean of the conditional variance:\n$$\n\\mathrm{Var}(\\rho) = (a_1^2 + 2a_2^2 + 6a_3^2) + \\sigma_{\\varepsilon}^2\n$$\nThe closed-form expression for the correlation ratio is the ratio of these two results:\n$$\n\\eta^2(\\rho \\mid \\delta) = \\frac{a_1^2 + 2a_2^2 + 6a_3^2}{a_1^2 + 2a_2^2 + 6a_3^2 + \\sigma_{\\varepsilon}^2}\n$$\nNow, we evaluate this expression numerically using the given values:\n$a_1 = 2.0 \\times 10^{-3}$, $a_2 = 3.0 \\times 10^{-3}$, $a_3 = 1.0 \\times 10^{-3}$, and $\\sigma_{\\varepsilon}^2 = 1.2 \\times 10^{-5}$.\nThe numerator is:\n$$\n\\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\delta]) = (2.0 \\times 10^{-3})^2 + 2(3.0 \\times 10^{-3})^2 + 6(1.0 \\times 10^{-3})^2 \\\\\n= 4.0 \\times 10^{-6} + 2(9.0 \\times 10^{-6}) + 6(1.0 \\times 10^{-6}) \\\\\n= (4.0 + 18.0 + 6.0) \\times 10^{-6} = 28.0 \\times 10^{-6} = 2.8 \\times 10^{-5}\n$$\nThe denominator is:\n$$\n\\mathrm{Var}(\\rho) = (2.8 \\times 10^{-5}) + (1.2 \\times 10^{-5}) = 4.0 \\times 10^{-5}\n$$\nThe numerical value of the correlation ratio is:\n$$\n\\eta^2(\\rho \\mid \\delta) = \\frac{2.8 \\times 10^{-5}}{4.0 \\times 10^{-5}} = \\frac{2.8}{4.0} = 0.7\n$$\nRounding to four significant figures gives $0.7000$.\n\nFinally, we explain why $\\eta^2$ differs from the squared Pearson correlation coefficient, $R^2$. The Pearson correlation coefficient between $\\rho$ and $\\delta$ (or equivalently, $\\xi$) is defined as $R(\\rho, \\xi) = \\frac{\\mathrm{Cov}(\\rho, \\xi)}{\\sqrt{\\mathrm{Var}(\\rho)\\mathrm{Var}(\\xi)}}$. The squared coefficient is:\n$$\nR^2(\\rho, \\xi) = \\frac{[\\mathrm{Cov}(\\rho, \\xi)]^2}{\\mathrm{Var}(\\rho)\\mathrm{Var}(\\xi)}\n$$\nWe know $\\mathrm{Var}(\\rho)$ and $\\mathrm{Var}(\\xi) = 1$. The covariance term is:\n$$\n\\mathrm{Cov}(\\rho, \\xi) = \\mathbb{E}[(\\rho - \\mathbb{E}[\\rho])(\\xi - \\mathbb{E}[\\xi])] = \\mathbb{E}[(\\rho - a_0)\\xi]\n$$\nSubstituting the PCE for $\\rho-a_0$ and using $\\xi = H_1(\\xi)$:\n$$\n\\mathrm{Cov}(\\rho, \\xi) = \\mathbb{E}[(a_1 H_1 + a_2 H_2 + a_3 H_3 + \\varepsilon) H_1] = a_1 \\mathbb{E}[H_1^2] = a_1\n$$\nThe other terms vanish due to orthogonality and independence. Thus, the squared Pearson coefficient is:\n$$\nR^2(\\rho, \\delta) = \\frac{a_1^2}{a_1^2 + 2a_2^2 + 6a_3^2 + \\sigma_{\\varepsilon}^2}\n$$\nComparing the expressions for $\\eta^2$ and $R^2$:\n$$\n\\eta^2 = \\frac{a_1^2 + 2a_2^2 + 6a_3^2}{\\mathrm{Var}(\\rho)} \\quad \\text{and} \\quad R^2 = \\frac{a_1^2}{\\mathrm{Var}(\\rho)}\n$$\nThe Pearson correlation coefficient, by its construction, only quantifies the strength of the *linear* relationship between two variables. Its numerator, $[\\mathrm{Cov}(\\rho, \\xi)]^2 = a_1^2$, represents the portion of variance in $\\rho$ attributable to the best linear fit with respect to $\\xi$, which corresponds to the variance of the linear term $a_1 \\xi$.\nIn contrast, the correlation ratio $\\eta^2$ measures the proportion of variance in $\\rho$ that is explained by *any* functional relationship with $\\delta$ (or $\\xi$), as captured by the conditional expectation $\\mathbb{E}[\\rho \\mid \\delta]$. Its numerator, $\\mathrm{Var}(\\mathbb{E}[\\rho \\mid \\delta]) = a_1^2 + 2a_2^2 + 6a_3^2$, accounts for the variance contributed by all terms in the PCE model of $\\rho$ that depend on $\\delta$—linear ($a_1^2$), quadratic ($2a_2^2$), and cubic ($6a_3^2$).\nSince the relationship between $\\rho$ and $\\delta$ is nonlinear, as indicated by the non-zero coefficients $a_2$ and $a_3$, the correlation ratio captures the explanatory power of these nonlinear dependencies, whereas the Pearson coefficient does not. This is why $\\eta^2 > R^2$ in this problem. If the relationship were purely linear ($a_2=0, a_3=0$), the numerators would be identical, and $\\eta^2$ would equal $R^2$.",
            "answer": "$$\\boxed{0.7000}$$"
        }
    ]
}