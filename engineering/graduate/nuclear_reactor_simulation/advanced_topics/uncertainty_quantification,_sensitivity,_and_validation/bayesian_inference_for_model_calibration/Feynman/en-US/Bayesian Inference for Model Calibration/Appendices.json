{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex computational methods, it's essential to master the fundamental mechanics of Bayesian inference. This practice focuses on the concept of conjugate priors, a cornerstone of Bayesian analysis where the posterior distribution retains the same functional form as the prior, allowing for a clean, analytical solution. By deriving the posterior for a Gaussian likelihood with a Gamma prior from first principles , you will build a solid mathematical foundation for understanding how data updates our beliefs about model parameters.",
            "id": "4215202",
            "problem": "In a pressurized water reactor core simulation, suppose a high-fidelity neutron transport model predicts a stable in-core detector count rate with known expected value $\\mu$ under a fixed operating condition. You collect $n$ independent detector readings $y_{1},\\dots,y_{n}$ from the same condition, and model the measurement and unresolved modeling noise as Gaussian with precision $\\tau = 1/\\sigma^{2}$, so that the likelihood for each observation satisfies $y_{i} \\mid \\tau \\sim \\mathcal{N}(\\mu,\\tau^{-1})$ and the observations are conditionally independent given $\\tau$. To reflect prior knowledge of the noise precision in this reactor configuration, you place a Gamma prior on $\\tau$ with shape-rate parameters $(a_{0}, b_{0})$, so that $p(\\tau) \\propto \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$ for $\\tau > 0$.\n\nStarting only from Bayes’ theorem and the standard definition of the Gaussian likelihood and Gamma prior density, derive the normalized posterior density $p(\\tau \\mid y_{1},\\dots,y_{n},\\mu)$ in closed form and demonstrate that the Gamma family is conjugate to the Gaussian likelihood with known mean.\n\nYour final answer must be a single closed-form analytic expression for the normalized posterior density $p(\\tau \\mid y_{1},\\dots,y_{n},\\mu)$, expressed symbolically in terms of $a_{0}$, $b_{0}$, $n$, $\\mu$, and $\\sum_{i=1}^{n} (y_{i}-\\mu)^{2}$. Do not substitute numerical values. No rounding is required. Do not include units in the final expression.",
            "solution": "The problem statement is evaluated as scientifically sound, well-posed, objective, and complete. It presents a standard, formalizable problem in Bayesian statistics applied to a plausible scenario in nuclear reactor simulation. No flaws are identified. We may therefore proceed with the derivation.\n\nThe objective is to derive the posterior probability density function (PDF) for the noise precision $\\tau$, denoted as $p(\\tau \\mid \\mathbf{y}, \\mu)$, where $\\mathbf{y}$ is the vector of observations $(y_{1}, \\dots, y_{n})$. The derivation begins with Bayes' theorem, which states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto p(\\mathbf{y} \\mid \\tau, \\mu) p(\\tau)$$\n\nFirst, we formulate the likelihood function, $p(\\mathbf{y} \\mid \\tau, \\mu)$. The problem states that the observations $y_{i}$ are conditionally independent given $\\tau$ and $\\mu$. Therefore, the joint likelihood of the entire dataset $\\mathbf{y}$ is the product of the individual likelihoods for each observation $y_{i}$:\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\prod_{i=1}^{n} p(y_{i} \\mid \\tau, \\mu)$$\nEach observation $y_{i}$ is drawn from a Gaussian (Normal) distribution with a known mean $\\mu$ and precision $\\tau$, i.e., $y_{i} \\sim \\mathcal{N}(\\mu, \\tau^{-1})$. The PDF for a single observation is:\n$$p(y_{i} \\mid \\tau, \\mu) = \\frac{1}{\\sqrt{2\\pi(\\tau^{-1})}} \\exp\\left( -\\frac{(y_{i} - \\mu)^{2}}{2(\\tau^{-1})} \\right) = \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{1}{2}} \\exp\\left( -\\frac{\\tau}{2}(y_{i} - \\mu)^{2} \\right)$$\nSubstituting this into the product for the joint likelihood gives:\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\prod_{i=1}^{n} \\left[ \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{1}{2}} \\exp\\left( -\\frac{\\tau}{2}(y_{i} - \\mu)^{2} \\right) \\right]$$\n$$p(\\mathbf{y} \\mid \\tau, \\mu) = \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right)$$\n\nNext, we specify the prior distribution for $\\tau$. The problem states that $\\tau$ follows a Gamma distribution with shape parameter $a_{0}$ and rate parameter $b_{0}$, denoted $\\tau \\sim \\text{Gamma}(a_{0}, b_{0})$. The PDF of the prior is given in its proportional form:\n$$p(\\tau) \\propto \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$$\n\nNow, we combine the likelihood and the prior using Bayes' theorem. We are interested in the posterior distribution of $\\tau$, so we can drop any factors that are not functions of $\\tau$.\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto p(\\mathbf{y} \\mid \\tau, \\mu) p(\\tau)$$\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\left[ \\left(\\frac{\\tau}{2\\pi}\\right)^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right) \\right] \\times \\left[ \\tau^{a_{0}-1} \\exp(-b_{0}\\tau) \\right]$$\nDropping the constant factor $(2\\pi)^{-\\frac{n}{2}}$ and combining the terms involving $\\tau$:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{\\frac{n}{2}} \\exp\\left( -\\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right) \\tau^{a_{0}-1} \\exp(-b_{0}\\tau)$$\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{\\frac{n}{2} + a_{0} - 1} \\exp\\left( -b_{0}\\tau - \\frac{\\tau}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} \\right)$$\nFactoring $\\tau$ out of the exponent:\n$$p(\\tau \\mid \\mathbf{y}, \\mu) \\propto \\tau^{(a_{0} + \\frac{n}{2}) - 1} \\exp\\left( -\\left[b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2}\\right]\\tau \\right)$$\n\nThis expression is the kernel of a probability distribution for $\\tau$. We recognize this functional form, $\\tau^{\\text{shape}-1}\\exp(-\\text{rate} \\cdot \\tau)$, as the kernel of a Gamma distribution. The posterior distribution for $\\tau$ is therefore another Gamma distribution, $p(\\tau \\mid \\mathbf{y}, \\mu) \\sim \\text{Gamma}(a_{n}, b_{n})$, with updated parameters:\nThe posterior shape parameter, $a_{n}$, is:\n$$a_{n} = a_{0} + \\frac{n}{2}$$\nThe posterior rate parameter, $b_{n}$, is:\n$$b_{n} = b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2}$$\n\nSince the prior distribution for $\\tau$ is from the Gamma family and the resulting posterior distribution is also from the Gamma family, we have demonstrated that the Gamma distribution is a conjugate prior for the precision parameter of a Gaussian likelihood with known mean.\n\nTo obtain the normalized posterior density, we use the standard form of the Gamma PDF, which is $p(x \\mid a, b) = \\frac{b^{a}}{\\Gamma(a)}x^{a-1}\\exp(-bx)$, where $\\Gamma(\\cdot)$ is the gamma function. By substituting the posterior parameters $a_{n}$ and $b_{n}$, we arrive at the final closed-form expression for the normalized posterior density of $\\tau$:\n$$p(\\tau \\mid y_{1},\\dots,y_{n},\\mu) = \\frac{b_{n}^{a_{n}}}{\\Gamma(a_{n})} \\tau^{a_{n}-1} \\exp(-b_{n}\\tau)$$\n$$p(\\tau \\mid y_{1},\\dots,y_{n},\\mu) = \\frac{\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)^{a_{0} + \\frac{n}{2}}}{\\Gamma\\left(a_{0} + \\frac{n}{2}\\right)} \\tau^{\\left(a_{0} + \\frac{n}{2}\\right) - 1} \\exp\\left(-\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)\\tau\\right)$$\nThis is the complete analytical expression for the normalized posterior density.",
            "answer": "$$\\boxed{\\frac{\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)^{a_{0} + \\frac{n}{2}}}{\\Gamma\\left(a_{0} + \\frac{n}{2}\\right)} \\tau^{\\left(a_{0} + \\frac{n}{2}\\right) - 1} \\exp\\left(-\\left(b_{0} + \\frac{1}{2} \\sum_{i=1}^{n} (y_{i}-\\mu)^{2}\\right)\\tau\\right)}$$"
        },
        {
            "introduction": "While the full posterior distribution provides a complete picture of uncertainty, we often need a single point estimate for a parameter. A common choice is the Maximum A Posteriori (MAP) estimate, which represents the most probable parameter value. This exercise  reveals a powerful connection between Bayesian inference and regularization techniques common in machine learning, demonstrating how choosing a Laplace prior is equivalent to performing $L^1$ regularization (LASSO).",
            "id": "4215228",
            "problem": "In a non-multiplying, homogeneous thermal-neutron absorber slab (e.g., borated water), the one-dimensional steady-state neutron balance reduces to the Beer–Lambert relation for the scalar neutron flux, which for path length $x$ can be written as the exponential attenuation law derived from the macroscopic absorption cross section $\\Sigma_{a}$: $$\\phi(x) = \\phi(0)\\,\\exp(-\\Sigma_{a}\\,x).$$ A collimated detector aligned along the slab thickness records count rates proportional to $\\phi(x)$. To linearize the dependence on $\\Sigma_{a}$, experimentalists use the log-ratio observable $$y_i \\equiv \\ln\\!\\left(\\frac{C(0)}{C(L_i)}\\right),$$ where $C(0)$ and $C(L_i)$ are the count rates at the entry face and at depth $L_i$, respectively. Under the exponential attenuation model and high-count conditions, the Central Limit Theorem justifies modeling the measurement noise as additive Gaussian, yielding $$y_i = \\Sigma_{a}\\,L_i + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\\quad i=1,\\dots,n,$$ with independent, identically distributed $\\varepsilon_i$.\n\nYou are tasked with Bayesian calibration of the scalar parameter $\\Sigma_{a}$ using a Laplace (double-exponential) prior centered at zero to induce shrinkage while enforcing the physical nonnegativity constraint. Specifically, assume the prior density on $\\theta \\equiv \\Sigma_{a}$ is proportional to $$p(\\theta) \\propto \\exp\\!\\left(-\\frac{|\\theta|}{b}\\right)\\,\\mathbf{1}_{\\{\\theta \\ge 0\\}},$$ where $b>0$ is the scale parameter and $\\mathbf{1}_{\\{\\cdot\\}}$ denotes the indicator function. The likelihood under the Gaussian noise model is $$p(\\mathbf{y}\\mid \\theta) \\propto \\exp\\!\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}\\big(y_i - L_i\\,\\theta\\big)^2\\right).$$\n\nUsing these ingredients, derive from first principles the analytic form of the Maximum A Posteriori (MAP, Maximum A Posteriori) estimator, identify the induced regularization, and compute the posterior mode $\\hat{\\theta}_{\\text{MAP}}$ for the following experimental data:\n- Path lengths: $L_1 = 12\\,\\mathrm{cm}$, $L_2 = 18\\,\\mathrm{cm}$, $L_3 = 25\\,\\mathrm{cm}$.\n- Log-ratio measurements: $y_1 = 0.90$, $y_2 = 1.50$, $y_3 = 1.95$.\n- Noise standard deviation: $\\sigma = 0.30$.\n- Laplace prior scale: $b = 0.005\\,\\mathrm{cm}^{-1}$.\n\nExpress your final numerical result for $\\hat{\\theta}_{\\text{MAP}}$ in $\\mathrm{cm}^{-1}$, and round your answer to four significant figures.",
            "solution": "The problem is subjected to validation and is determined to be valid. It is scientifically grounded in established principles of nuclear physics and Bayesian statistics, well-posed with all necessary information provided, and objective in its formulation. No flaws under the specified criteria are identified. We may therefore proceed with the solution.\n\nThe objective is to find the Maximum A Posteriori (MAP) estimate for the parameter $\\theta \\equiv \\Sigma_a$. By Bayes' theorem, the posterior probability density for $\\theta$ given the data $\\mathbf{y} = (y_1, \\dots, y_n)$ is proportional to the product of the likelihood and the prior:\n$$p(\\theta \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\theta) \\, p(\\theta)$$\nThe MAP estimator, $\\hat{\\theta}_{\\text{MAP}}$, is the value of $\\theta$ that maximizes this posterior density. It is computationally more convenient to maximize the logarithm of the posterior, as the logarithm is a monotonic function. The log-posterior, denoted $\\mathcal{L}(\\theta)$, is given by:\n$$\\mathcal{L}(\\theta) = \\ln p(\\theta \\mid \\mathbf{y}) = \\ln p(\\mathbf{y} \\mid \\theta) + \\ln p(\\theta) + \\text{constant}$$\nThe MAP estimate is then:\n$$\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\mathcal{L}(\\theta)$$\n\nFirst, we specify the log-likelihood. The likelihood is given as:\n$$p(\\mathbf{y}\\mid \\theta) \\propto \\exp\\!\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}\\big(y_i - L_i\\,\\theta\\big)^2\\right)$$\nTaking the natural logarithm yields the log-likelihood:\n$$\\ln p(\\mathbf{y}\\mid \\theta) = C_1 - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - L_i\\,\\theta)^2$$\nwhere $C_1$ is a constant that does not depend on $\\theta$.\n\nNext, we specify the log-prior. The prior density is given by:\n$$p(\\theta) \\propto \\exp\\!\\left(-\\frac{|\\theta|}{b}\\right)\\,\\mathbf{1}_{\\{\\theta \\ge 0\\}}$$\nThe physical nonnegativity constraint, $\\theta \\ge 0$, is enforced by the indicator function $\\mathbf{1}_{\\{\\theta \\ge 0\\}}$. For this domain, the absolute value $|\\theta|$ simplifies to $\\theta$. Thus, the prior for $\\theta \\ge 0$ is an exponential distribution:\n$$p(\\theta) \\propto \\exp\\!\\left(-\\frac{\\theta}{b}\\right) \\quad \\text{for} \\quad \\theta \\ge 0$$\nTaking the natural logarithm gives the log-prior:\n$$\\ln p(\\theta) = C_2 - \\frac{\\theta}{b} \\quad \\text{for} \\quad \\theta \\ge 0$$\nwhere $C_2$ is another constant.\n\nCombining the log-likelihood and the log-prior, the log-posterior function to be maximized for $\\theta \\ge 0$ is:\n$$\\mathcal{L}(\\theta) = C - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - L_i\\,\\theta)^2 - \\frac{\\theta}{b}$$\nwhere $C = C_1 + C_2$. To find the maximum, we can equivalently minimize the negative of the terms that depend on $\\theta$:\n$$\\hat{\\theta}_{\\text{MAP}} = \\arg\\min_{\\theta \\ge 0} \\left[ \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - L_i\\,\\theta)^2 + \\frac{\\theta}{b} \\right]$$\nThis formulation reveals the nature of the estimator. The first term, $\\sum(y_i - L_i\\theta)^2$, is the sum-of-squared-errors, representing the data-fidelity term. The second term, $\\theta/b$, is a penalty term that penalizes large values of $\\theta$. This is a form of regularized least squares. The penalty is linear in the parameter magnitude $\\theta$ (since $\\theta \\ge 0$), which is characteristic of $L_1$ regularization, commonly known as LASSO (Least Absolute Shrinkage and Selection Operator) regression. The Laplace prior induces this sparsity-promoting regularization, which acts to shrink the estimate towards zero.\n\nTo find the minimum, we differentiate the objective function with respect to $\\theta$ and set the result to zero. Let $J(\\theta) = \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i - L_i\\,\\theta)^2 + \\frac{\\theta}{b}$.\n$$\\frac{dJ}{d\\theta} = \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} 2(y_i - L_i\\,\\theta)(-L_i) + \\frac{1}{b} = -\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(y_iL_i - L_i^2\\theta) + \\frac{1}{b}$$\nSetting the derivative to zero to find the unconstrained extremum, $\\theta^*$:\n$$-\\frac{1}{\\sigma^2}\\left(\\sum_{i=1}^{n}y_iL_i - \\theta^*\\sum_{i=1}^{n}L_i^2\\right) + \\frac{1}{b} = 0$$\n$$\\sum_{i=1}^{n}y_iL_i - \\theta^*\\sum_{i=1}^{n}L_i^2 = \\frac{\\sigma^2}{b}$$\n$$\\theta^*\\sum_{i=1}^{n}L_i^2 = \\sum_{i=1}^{n}y_iL_i - \\frac{\\sigma^2}{b}$$\n$$\\theta^* = \\frac{\\sum_{i=1}^{n}y_iL_i - \\frac{\\sigma^2}{b}}{\\sum_{i=1}^{n}L_i^2}$$\nThe second derivative, $\\frac{d^2J}{d\\theta^2} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}L_i^2 > 0$, confirms that $\\theta^*$ is a minimum.\nThe objective function $J(\\theta)$ is a convex parabola. The constrained minimum over $\\theta \\ge 0$ is at $\\theta^*$ if $\\theta^* \\ge 0$, and at the boundary $\\theta=0$ if $\\theta^* < 0$. Therefore, the MAP estimator is:\n$$\\hat{\\theta}_{\\text{MAP}} = \\max\\left(0, \\frac{\\sum_{i=1}^{n}y_iL_i - \\frac{\\sigma^2}{b}}{\\sum_{i=1}^{n}L_i^2}\\right)$$\n\nNow, we substitute the provided experimental data to compute the numerical value.\nThe data are:\n- Path lengths: $L_1 = 12\\,\\mathrm{cm}$, $L_2 = 18\\,\\mathrm{cm}$, $L_3 = 25\\,\\mathrm{cm}$.\n- Log-ratio measurements: $y_1 = 0.90$, $y_2 = 1.50$, $y_3 = 1.95$.\n- Noise standard deviation: $\\sigma = 0.30$.\n- Laplace prior scale: $b = 0.005\\,\\mathrm{cm}^{-1}$.\n\nWe calculate the required sums:\n$$\\sum_{i=1}^{3}L_i^2 = (12)^2 + (18)^2 + (25)^2 = 144 + 324 + 625 = 1093\\,\\mathrm{cm}^2$$\n$$\\sum_{i=1}^{3}y_iL_i = (0.90)(12) + (1.50)(18) + (1.95)(25) = 10.8 + 27.0 + 48.75 = 86.55\\,\\mathrm{cm}$$\nNext, we compute the regularization term:\n$$\\sigma^2 = (0.30)^2 = 0.09$$\n$$\\frac{\\sigma^2}{b} = \\frac{0.09}{0.005\\,\\mathrm{cm}^{-1}} = 18\\,\\mathrm{cm}$$\nNow we compute the unconstrained estimate $\\theta^*$:\n$$\\theta^* = \\frac{86.55\\,\\mathrm{cm} - 18\\,\\mathrm{cm}}{1093\\,\\mathrm{cm}^2} = \\frac{68.55\\,\\mathrm{cm}}{1093\\,\\mathrm{cm}^2} \\approx 0.062717...\\,\\mathrm{cm}^{-1}$$\nSince $\\theta^* > 0$, the MAP estimate is equal to this value.\n$$\\hat{\\theta}_{\\text{MAP}} = \\theta^* \\approx 0.062717...\\,\\mathrm{cm}^{-1}$$\nRounding the result to four significant figures, as requested:\n$$\\hat{\\theta}_{\\text{MAP}} \\approx 0.06272\\,\\mathrm{cm}^{-1}$$",
            "answer": "$$\\boxed{0.06272}$$"
        },
        {
            "introduction": "The process of calibrating a model does not end with parameter estimation; we must also rigorously assess whether the model provides a good fit to the data. This practice introduces the Posterior Predictive Check (PPC), a powerful technique for model validation. By simulating replicate datasets from your fitted model and comparing them to the observed data , you can diagnose potential model misspecifications and gain confidence in your inferences.",
            "id": "4215171",
            "problem": "Consider the axial power distribution in a pressurized water reactor core modeled along the axial coordinate $z \\in [0,H]$ where $H$ is the active core height expressed in meters. Under the One-Group Neutron Diffusion Equation in slab geometry with vacuum boundary conditions, axial neutron flux eigenfunctions are sinusoidal. A truncated basis is used to approximate the normalized axial power profile. Define three basis functions $f_0(z) = 1$, $f_1(z) = \\sin\\left(\\pi z / H\\right)$, and $f_2(z) = \\sin\\left(2\\pi z / H\\right)$. Let $N$ be the number of instrumented axial levels, and let $z_i = i \\cdot H/(N-1)$ for $i = 0,1,\\dots,N-1$.\n\nAssume a linear model for the observed axial power at these levels,\n$$\ny_i = \\theta_0 f_0(z_i) + \\theta_1 f_1(z_i) + \\theta_2 f_2(z_i) + \\varepsilon_i,\n$$\nwhere $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ independently across $i$ and $\\sigma^2$ is known. In matrix form, with $y \\in \\mathbb{R}^N$, $\\theta \\in \\mathbb{R}^3$, and design matrix $A \\in \\mathbb{R}^{N\\times 3}$ whose columns are the basis functions evaluated at $\\{z_i\\}$, this is\n$$\ny = A \\theta + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_N).\n$$\n\nAdopt a Gaussian prior on the coefficients,\n$$\n\\theta \\sim \\mathcal{N}(\\mu_0,\\Sigma_0),\n$$\nwith specified mean vector $\\mu_0 \\in \\mathbb{R}^3$ and covariance matrix $\\Sigma_0 \\in \\mathbb{R}^{3\\times 3}$ that is symmetric positive definite.\n\nYour task is to calibrate the model via Bayesian inference and then construct a Posterior Predictive Check (PPC). Let the axial peaking factor statistic be\n$$\nT(y) = \\frac{\\max_i y_i}{\\frac{1}{N} \\sum_{i=1}^N y_i}.\n$$\nDefine the posterior predictive $p$-value\n$$\np = \\Pr\\{T(y^{\\text{rep}}) \\ge T(y) \\mid y\\},\n$$\nwhere $y^{\\text{rep}}$ is a replicate dataset generated from the posterior predictive distribution. The posterior predictive distribution is defined by integrating the sampling distribution $y^{\\text{rep}} \\mid \\theta$ against the posterior distribution $\\theta \\mid y$. The program must approximate $p$ by Monte Carlo sampling: draw $\\theta^{(m)} \\sim p(\\theta \\mid y)$ and then $y^{\\text{rep},(m)} \\sim p(y^{\\text{rep}} \\mid \\theta^{(m)})$, for $m=1,\\dots,M$, and estimate\n$$\n\\hat{p} = \\frac{1}{M} \\sum_{m=1}^M \\mathbf{1}\\left\\{T\\left(y^{\\text{rep},(m)}\\right) \\ge T(y)\\right\\}.\n$$\n\nStart from well-tested physical and statistical foundations: the One-Group Neutron Diffusion Equation motivates sinusoidal axial modes, and linear-Gaussian models imply Gaussian posteriors under Gaussian priors. Derive the posterior distribution $p(\\theta \\mid y)$ from these foundations, without invoking any shortcut formulas not implied directly by these bases.\n\nThe observed data $y$ for each test case must be generated inside your program using the specified parameters and a fixed random seed as part of the test definition. For each test case, use the positions $z_i$ defined above and generate $y$ via the sampling model using a specified “true” coefficient vector $\\theta^{\\text{true}}$; then compute the posterior and the PPC $p$-value based on that $y$.\n\nPhysical units: The height $H$ must be specified in meters. The axial peaking factor $T(y)$ and the $p$-value are dimensionless quantities. Report the final $p$-values as decimals.\n\nAngle unit: All angles inside trigonometric functions are dimensionless arguments formed as ratios of lengths; no angle unit conversion is required because $\\sin(\\cdot)$ is evaluated on dimensionless quantities $\\pi z/H$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[p_1,p_2,p_3,p_4]$).\n\nTest suite. Implement the following four test cases, each with a fixed Monte Carlo sample size $M = 20000$ and independent random seed. For reproducibility, use the specified seed for both generating the observed data and for posterior predictive sampling within that case.\n\n- Case $1$ (general case): $H=4.0$ meters, $N=41$, $\\sigma = 0.02$, $\\mu_0 = [1.0, 0.0, 0.0]$, $\\Sigma_0 = \\operatorname{diag}([0.05^2, 0.2^2, 0.2^2])$, $\\theta^{\\text{true}} = [1.05, 0.15, -0.05]$, seed $= 12345$.\n- Case $2$ (near-deterministic boundary): $H=4.0$ meters, $N=41$, $\\sigma = 0.0001$, $\\mu_0 = [1.0, 0.0, 0.0]$, $\\Sigma_0 = \\operatorname{diag}([0.05^2, 0.2^2, 0.2^2])$, $\\theta^{\\text{true}} = [1.05, 0.15, -0.05]$, seed $= 23456$.\n- Case $3$ (low-resolution edge): $H=4.0$ meters, $N=3$, $\\sigma = 0.05$, $\\mu_0 = [1.0, 0.1, 0.0]$, $\\Sigma_0 = \\operatorname{diag}([0.1^2, 0.3^2, 0.3^2])$, $\\theta^{\\text{true}} = [1.1, 0.3, 0.0]$, seed $= 34567$.\n- Case $4$ (strong prior influence): $H=4.0$ meters, $N=21$, $\\sigma = 0.02$, $\\mu_0 = [0.9, 0.0, 0.0]$, $\\Sigma_0 = \\operatorname{diag}([0.01^2, 0.02^2, 0.02^2])$, $\\theta^{\\text{true}} = [1.2, 0.25, 0.05]$, seed $= 45678$.\n\nOutput specification: Your program should produce a single line of output containing the four estimated posterior predictive $p$-values for the above cases, formatted as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places, in the order of the cases $1$ through $4$, i.e., $[p_1,p_2,p_3,p_4]$.",
            "solution": "The user-provided problem has been analyzed and validated.\n\n\\textbf{Step 1: Extract Givens}\n- Domain: Axial coordinate $z \\in [0,H]$, $H$ is core height in meters.\n- Basis Functions: $f_0(z) = 1$, $f_1(z) = \\sin(\\pi z / H)$, $f_2(z) = \\sin(2\\pi z / H)$.\n- Discretization: $N$ levels, $z_i = i \\cdot H/(N-1)$ for $i=0,\\dots,N-1$.\n- Likelihood Model: $y_i = \\theta_0 f_0(z_i) + \\theta_1 f_1(z_i) + \\theta_2 f_2(z_i) + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ i.i.d.\n- Matrix Form: $y = A \\theta + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_N)$.\n- Prior Distribution: $\\theta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$, with $\\Sigma_0$ symmetric positive definite.\n- Test Statistic: Axial peaking factor, $T(y) = \\frac{\\max_i y_i}{\\frac{1}{N} \\sum_{i=1}^N y_i}$.\n- Objective: Estimate the posterior predictive $p$-value, $p = \\Pr\\{T(y^{\\text{rep}}) \\ge T(y) \\mid y\\}$, via Monte Carlo simulation with $M$ samples.\n- Data Generation: Observed data $y$ for each case is to be generated from the likelihood model using a provided \"true\" parameter vector $\\theta^{\\text{true}}$ and a fixed random seed.\n- Test Suite: Four specific test cases are provided with all necessary parameters: $H, N, \\sigma, \\mu_0, \\Sigma_0, \\theta^{\\text{true}}, M, \\text{seed}$.\n\n\\textbf{Step 2: Validate Using Extracted Givens}\nThe problem is assessed to be valid.\n- It is scientifically grounded, using a standard simplified model for reactor physics (diffusion theory basis functions) and a standard statistical methodology (Bayesian inference for a linear-Gaussian model).\n- It is well-posed. All mathematical objects are well-defined, and the provision of a positive definite prior covariance matrix $\\Sigma_0$ ensures that the posterior distribution is well-defined and proper, even if the design matrix $A$ were to have linearly dependent columns (which occurs in Case 3, but is handled correctly by the Bayesian framework).\n- It is objective and self-contained, providing all necessary data and definitions for a unique solution.\n- The parameters are physically plausible within the context of a simplified simulation.\n\n\\textbf{Step 3: Verdict and Action}\nThe problem is valid. A complete, reasoned solution will be provided.\n\n\\rule{12cm}{0.4pt}\n\nThe task is to perform a Bayesian model calibration and a subsequent posterior predictive check. This requires deriving the posterior distribution for the model parameters $\\theta$ and then using this distribution to generate replicate data for comparison with the observed data.\n\n\\textbf{Derivation of the Posterior Distribution $p(\\theta \\mid y)$}\n\nThe posterior distribution is given by Bayes' theorem:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n$$\nThe likelihood $p(y \\mid \\theta)$ corresponds to the model $y = A\\theta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_N)$. Its probability density function is:\n$$\np(y \\mid \\theta) = \\frac{1}{(2\\pi\\sigma^2)^{N/2}} \\exp\\left(-\\frac{1}{2\\sigma^2}(y - A\\theta)^T(y - A\\theta)\\right)\n$$\nThe prior $p(\\theta)$ is given as a multivariate normal distribution $\\mathcal{N}(\\mu_0, \\Sigma_0)$:\n$$\np(\\theta) = \\frac{1}{(2\\pi)^{3/2}|\\Sigma_0|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\theta - \\mu_0)^T \\Sigma_0^{-1}(\\theta - \\mu_0)\\right)\n$$\nThe posterior density is proportional to the product of these two densities. We focus on the exponents, as the normalization constant can be determined at the end. The argument of the exponential for the posterior is the sum of the arguments of the exponentials for the likelihood and the prior:\n$$\n\\text{Exponent} = -\\frac{1}{2\\sigma^2}(y - A\\theta)^T(y - A\\theta) - \\frac{1}{2}(\\theta - \\mu_0)^T \\Sigma_0^{-1}(\\theta - \\mu_0)\n$$\nExpanding the terms:\n$$\n\\text{Exponent} = -\\frac{1}{2}\\left[ \\frac{1}{\\sigma^2}(y^T y - 2y^T A\\theta + \\theta^T A^T A \\theta) + (\\theta^T \\Sigma_0^{-1}\\theta - 2\\mu_0^T \\Sigma_0^{-1}\\theta + \\mu_0^T \\Sigma_0^{-1}\\mu_0) \\right]\n$$\nTo identify the form of the posterior distribution, we collect terms involving $\\theta$:\n$$\n\\text{Exponent} = -\\frac{1}{2} \\left[ \\theta^T\\left(\\frac{1}{\\sigma^2}A^T A + \\Sigma_0^{-1}\\right)\\theta - 2\\left(\\frac{1}{\\sigma^2}y^T A + \\mu_0^T \\Sigma_0^{-1}\\right)\\theta \\right] + C\n$$\nwhere $C$ contains terms not dependent on $\\theta$. This expression is a quadratic form in $\\theta$, which indicates that the posterior distribution $p(\\theta \\mid y)$ is also a multivariate normal distribution, let's say $\\mathcal{N}(\\mu_N, \\Sigma_N)$. The density of such a distribution is proportional to $\\exp\\left(-\\frac{1}{2}(\\theta - \\mu_N)^T\\Sigma_N^{-1}(\\theta - \\mu_N)\\right)$, which expands to:\n$$\n\\text{Exponent} = -\\frac{1}{2}\\left[ \\theta^T \\Sigma_N^{-1}\\theta - 2\\mu_N^T \\Sigma_N^{-1}\\theta \\right] + C'\n$$\nBy comparing the coefficients of the quadratic and linear terms in $\\theta$, we can identify the posterior parameters $\\mu_N$ and $\\Sigma_N$.\n\nThe inverse posterior covariance, $\\Sigma_N^{-1}$, is the term multiplying $\\theta^T(\\dots)\\theta$:\n$$\n\\Sigma_N^{-1} = \\frac{1}{\\sigma^2}A^T A + \\Sigma_0^{-1}\n$$\nThe posterior covariance is its inverse:\n$$\n\\Sigma_N = \\left(\\frac{1}{\\sigma^2}A^T A + \\Sigma_0^{-1}\\right)^{-1}\n$$\nThe linear term gives the relation for the posterior mean $\\mu_N$:\n$$\n\\mu_N^T \\Sigma_N^{-1} = \\frac{1}{\\sigma^2}y^T A + \\mu_0^T \\Sigma_0^{-1}\n$$\nTransposing and right-multiplying by $\\Sigma_N$ yields:\n$$\n\\mu_N = \\Sigma_N \\left( \\frac{1}{\\sigma^2}A^T y + \\Sigma_0^{-1} \\mu_0 \\right)\n$$\nThus, the posterior distribution is $p(\\theta \\mid y) = \\mathcal{N}(\\mu_N, \\Sigma_N)$ with the parameters as derived.\n\n\\textbf{Posterior Predictive Check (PPC)}\n\nThe PPC is a method to assess the goodness-of-fit of a model. We generate replicate data $y^{\\text{rep}}$ from the posterior predictive distribution, $p(y^{\\text{rep}} \\mid y)$, and compare its properties to the observed data $y$. The posterior predictive distribution is defined as:\n$$\np(y^{\\text{rep}} \\mid y) = \\int p(y^{\\text{rep}} \\mid \\theta) p(\\theta \\mid y) d\\theta\n$$\nSampling from this distribution is a two-step generative process:\n1.  Draw a parameter vector $\\theta^{(m)}$ from the posterior distribution: $\\theta^{(m)} \\sim p(\\theta \\mid y)$.\n2.  Generate a replicate dataset $y^{\\text{rep},(m)}$ from the likelihood using this parameter vector: $y^{\\text{rep},(m)} \\sim p(y \\mid \\theta^{(m)})$.\n\nThe check is performed using a test statistic $T(y)$. The problem defines the axial peaking factor:\n$$\nT(y) = \\frac{\\max_i y_i}{\\frac{1}{N} \\sum_{i=0}^{N-1} y_i}\n$$\nNote: The problem text notation $\\sum_{i=1}^N y_i$ is interpreted as the sum over all $N$ elements, which are indexed $0$ to $N-1$, as this represents the standard definition of a sample mean.\n\nThe posterior predictive $p$-value measures the probability that the test statistic calculated on replicate data is more extreme than that calculated on the observed data. It is estimated via Monte Carlo simulation:\n$$\n\\hat{p} = \\frac{1}{M} \\sum_{m=1}^{M} \\mathbf{1}\\left\\{T\\left(y^{\\text{rep},(m)}\\right) \\ge T(y)\\right\\}\n$$\nA $p$-value close to $0$ or $1$ suggests that the model is unable to reproduce the specific aspect of the data captured by the test statistic, indicating potential model misspecification or prior-data conflict.\n\n\\textbf{Algorithm for Implementation}\n\nFor each test case, the following steps are executed:\n1.  Initialize parameters from the test case: $H, N, \\sigma, \\mu_0, \\Sigma_0, \\theta^{\\text{true}}, M$, and the random number generator `seed`.\n2.  Construct the $N \\times 3$ design matrix $A$, where the columns are the basis functions $f_0, f_1, f_2$ evaluated at the $N$ axial locations $z_i = i \\cdot H/(N-1)$.\n3.  Generate the observed data vector $y$. First, compute the true mean $\\mu_y = A \\theta^{\\text{true}}$. Then, add Gaussian noise: $y = \\mu_y + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_N)$.\n4.  Calculate the posterior distribution parameters $\\mu_N$ and $\\Sigma_N$ using the derived formulas and the generated data $y$. This involves matrix inversions and multiplications.\n5.  Calculate the observed test statistic $T_{obs} = T(y)$.\n6.  Begin the Monte Carlo simulation with $M$ iterations:\n    a. Draw a parameter sample $\\theta^{(m)}$ from the posterior distribution $\\mathcal{N}(\\mu_N, \\Sigma_N)$.\n    b. Generate a replicate dataset $y^{\\text{rep},(m)} = A\\theta^{(m)} + \\varepsilon^{(m)}$, where $\\varepsilon^{(m)} \\sim \\mathcal{N}(0, \\sigma^2 I_N)$ is a new noise sample.\n    c. Calculate the replicate test statistic $T_{rep}^{(m)} = T(y^{\\text{rep},(m)})$.\n7.  Count the number of iterations where $T_{rep}^{(m)} \\ge T_{obs}$.\n8.  The estimated $p$-value is this count divided by $M$.\nThe final results for all test cases are collected and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are needed for this problem.\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian model calibration and posterior predictive check problem\n    for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"H\": 4.0, \"N\": 41, \"sigma\": 0.02,\n            \"mu0\": np.array([1.0, 0.0, 0.0]),\n            \"Sigma0\": np.diag([0.05**2, 0.2**2, 0.2**2]),\n            \"theta_true\": np.array([1.05, 0.15, -0.05]),\n            \"M\": 20000, \"seed\": 12345\n        },\n        {\n            \"H\": 4.0, \"N\": 41, \"sigma\": 0.0001,\n            \"mu0\": np.array([1.0, 0.0, 0.0]),\n            \"Sigma0\": np.diag([0.05**2, 0.2**2, 0.2**2]),\n            \"theta_true\": np.array([1.05, 0.15, -0.05]),\n            \"M\": 20000, \"seed\": 23456\n        },\n        {\n            \"H\": 4.0, \"N\": 3, \"sigma\": 0.05,\n            \"mu0\": np.array([1.0, 0.1, 0.0]),\n            \"Sigma0\": np.diag([0.1**2, 0.3**2, 0.3**2]),\n            \"theta_true\": np.array([1.1, 0.3, 0.0]),\n            \"M\": 20000, \"seed\": 34567\n        },\n        {\n            \"H\": 4.0, \"N\": 21, \"sigma\": 0.02,\n            \"mu0\": np.array([0.9, 0.0, 0.0]),\n            \"Sigma0\": np.diag([0.01**2, 0.02**2, 0.02**2]),\n            \"theta_true\": np.array([1.2, 0.25, 0.05]),\n            \"M\": 20000, \"seed\": 45678\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Unpack parameters for the current case\n        H, N, sigma, mu0, Sigma0, theta_true, M, seed = case.values()\n        \n        # Initialize Random Number Generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct the design matrix A\n        z = np.linspace(0, H, N)\n        A = np.zeros((N, 3))\n        A[:, 0] = 1.0\n        A[:, 1] = np.sin(np.pi * z / H)\n        A[:, 2] = np.sin(2 * np.pi * z / H)\n\n        # 2. Generate the \"observed\" data y\n        mu_y_true = A @ theta_true\n        epsilon_obs = rng.normal(loc=0.0, scale=sigma, size=N)\n        y_obs = mu_y_true + epsilon_obs\n\n        # 3. Calculate posterior distribution parameters (mu_N, Sigma_N)\n        Sigma0_inv = np.linalg.inv(Sigma0)\n        AtA = A.T @ A\n        \n        # Posterior inverse covariance\n        Sigma_N_inv = (1 / sigma**2) * AtA + Sigma0_inv\n        \n        # Posterior covariance\n        Sigma_N = np.linalg.inv(Sigma_N_inv)\n        \n        # Posterior mean\n        Aty_obs = A.T @ y_obs\n        mu_N = Sigma_N @ ((1 / sigma**2) * Aty_obs + Sigma0_inv @ mu0)\n\n        # 4. Define the test statistic T(y)\n        def T(y_data):\n            mean_val = np.mean(y_data)\n            if mean_val == 0:\n                # This case is highly improbable given the model setup\n                return np.inf if np.max(y_data) > 0 else 0\n            return np.max(y_data) / mean_val\n\n        # 5. Calculate the statistic for the observed data\n        T_obs = T(y_obs)\n\n        # 6. Perform Monte Carlo simulation for PPC\n        \n        # Step 6a: Draw M samples from the posterior distribution of theta\n        theta_samples = rng.multivariate_normal(mu_N, Sigma_N, size=M)\n\n        # Step 6b: Generate M replicate datasets y_rep\n        # Vectorized calculation for efficiency\n        mu_rep_samples = theta_samples @ A.T  # Shape (M, N)\n        epsilon_rep_samples = rng.normal(loc=0.0, scale=sigma, size=(M, N))\n        y_rep_samples = mu_rep_samples + epsilon_rep_samples\n\n        # Step 6c: Calculate the test statistic for each replicate dataset\n        max_y_rep = np.max(y_rep_samples, axis=1)\n        mean_y_rep = np.mean(y_rep_samples, axis=1)\n        \n        # Create an array to hold T_rep values, handling potential division by zero\n        T_rep = np.full_like(mean_y_rep, np.inf)\n        # Avoid division by zero, though unlikely\n        valid_means_mask = mean_y_rep != 0\n        T_rep[valid_means_mask] = max_y_rep[valid_means_mask] / mean_y_rep[valid_means_mask]\n\n        # Step 6d  7: Count and estimate the p-value\n        num_exceed = np.sum(T_rep >= T_obs)\n        p_value = num_exceed / M\n        \n        results.append(f\"{p_value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}