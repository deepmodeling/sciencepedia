{"hands_on_practices": [{"introduction": "物理信息神经网络（PINN）的核心在于其损失函数，该函数将控制物理定律（偏微分方程）和边界条件编码在内。本练习将引导您为经典的泊松方程构建这一基础损失函数，教会您如何将物理学原理转化为神经网络能够理解和优化的语言。通过这个过程，您将掌握使用“软约束”方法来施加物理定律和边界条件的基本技巧 [@problem_id:2126324]。", "problem": "一位研究人员正在构建一个物理信息神经网络（PINN），用于在一个二维方形区域内寻找静电势 $V(x,y)$ 的近似解。该电势的物理行为由泊松方程描述：\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\n其中 $f(x,y)$ 表示给定的电荷分布密度，$\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ 是拉普拉斯算子。电势定义在域 $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$ 上。该域的边界 $\\partial D$ 保持在零电势（接地），这施加了边界条件 $V(x,y) = 0$ 对所有 $(x,y) \\in \\partial D$ 成立。\n\nPINN 模型，记为 $\\hat{V}(x,y; \\theta)$，通过最小化一个包含了问题物理特性的损失函数 $L(\\theta)$ 来学习近似 $V(x,y)$。这里，$\\theta$ 代表神经网络的所有可训练参数。损失函数使用两组离散点计算：\n1.  一组位于域 $D$ 内部的 $N_{pde}$ 个配置点，$S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$。\n2.  一组位于边界 $\\partial D$ 上的 $N_{bc}$ 个边界点，$S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$。\n\n总损失函数 $L(\\theta)$ 是两个均方误差项的和：一项用于控制偏微分方程（$L_{pde}$），另一项用于边界条件（$L_{bc}$）。\n\n构建总损失函数 $L(\\theta) = L_{pde} + L_{bc}$ 的数学表达式。您的表达式应使用网络输出 $\\hat{V}$、其二阶偏导数、函数 $f$、给定的点集及其各自的大小 $N_{pde}$ 和 $N_{bc}$ 来表示。", "solution": "我们从控制泊松方程和边界条件开始：\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\n物理信息神经网络用 $\\hat{V}(x,y;\\theta)$ 来近似 $V$。在内部配置点 $(x_{i},y_{i})\\in S_{pde}$ 处的偏微分方程残差通过将泊松方程应用于 $\\hat{V}$ 来定义：\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n使用二维拉普拉斯算子的定义，这等价于\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\n那么，在 $S_{pde}$ 上强制执行偏微分方程的均方误差是\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\n在边界 $\\partial D$ 上的边界条件 $V=0$ 是通过惩罚在边界点 $(x_{j},y_{j})\\in S_{bc}$ 处 $\\hat{V}$ 与零的偏差来强制执行的：\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\n因此，总损失是这两个均方误差项的和：\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$", "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$", "id": "2126324"}, {"introduction": "在上一个练习中，我们使用“软”惩罚项来处理边界条件，而本练习将介绍一种更为巧妙和稳健的“硬”施加方法。您将学习如何通过设计网络输出架构，使其在构造上自动满足边界条件。这项技术对于提高PINN的准确性和训练稳定性至关重要 [@problem_id:2126300]。", "problem": "在科学计算领域，物理信息神经网络 (Physics-Informed Neural Networks, PINNs) 已成为一种求解微分方程的强大工具。设计PINN的一个关键方面是确保其输出（即解的近似）满足给定的边界条件。实现这一点的一种可靠方法是，构造网络的最终输出函数，使其在构造上就满足这些条件。\n\n考虑一个定义在空间域 $x \\in [0, L]$ 上的一维问题。一个神经网络提供了一个原始、无约束的输出函数，记为 $\\hat{u}_{NN}(x)$。我们希望使用这个网络来为一个满足以下非齐次狄利克雷边界条件的微分方程找到一个近似解 $u(x)$：\n$$u(0) = A$$\n$$u(L) = B$$\n其中，$A$，$B$ 和 $L > 0$ 是给定的实常数。\n\n你的任务是设计一个变换，它将原始网络输出 $\\hat{u}_{NN}(x)$ 转换为一个新函数 $u_{NN}(x)$，并用后者作为最终的近似解。这个变换必须保证，无论网络生成的函数 $\\hat{u}_{NN}(x)$ 是什么，$u_{NN}(x)$ 都能严格满足指定的边界条件。\n\n请用原始网络输出 $\\hat{u}_{NN}(x)$以及参数 $x$、$L$、$A$ 和 $B$ 来表示 $u_{NN}(x)$。", "solution": "我们寻求一个变换，将原始网络输出 $\\hat{u}_{NN}(x)$ 映射到一个函数 $u_{NN}(x)$，以强制施加狄利克雷边界条件 $u_{NN}(0)=A$ 和 $u_{NN}(L)=B$，且该变换对任意 $\\hat{u}_{NN}(x)$ 均有效。一种标准的构造方法是将 $u_{NN}(x)$ 分解为\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\n其中 $g(x)$ 是任何满足边界条件的固定函数，而 $s(x)$ 是任何在两个边界点上都为零的函数。具体来说，我们要求\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\n一个方便的选择是为 $g(x)$ 取线性插值函数，\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\n以及一个简单的、在边界上为零的因子\n$$\ns(x)=x(L-x),\n$$\n该函数满足 $s(0)=0$ 和 $s(L)=0$。因此，定义\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\n为了验证边界条件，在 $x=0$ 和 $x=L$ 处求值：\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\n因此，对于任意的 $\\hat{u}_{NN}(x)$，所构造的 $u_{NN}(x)$ 都严格满足 $u_{NN}(0)=A$ 和 $u_{NN}(L)=B$。", "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$", "id": "2126300"}, {"introduction": "最后的这项练习将从理论走向实践，提供一个完整的编码项目，用以展示“谱偏差”（spectral bias）现象——即神经网络优先学习低频模式的倾向。通过训练一个PINN来求解一个具有多频率解的常微分方程，您将亲身体验并直观地理解这一决定PINN性能和行为的最重要特征之一 [@problem_id:2427229]。", "problem": "您将实现一个完整的、可运行的程序，以演示物理信息神经网络（PINN）的谱偏见。其核心思想是训练一个PINN来求解一个一维边值问题，该问题的已知解是低频和高频正弦波的叠加，即 $u(x) = \\sin(x) + \\sin(25x)$，并定量观察在训练过程中哪个频率分量被首先学习到。在整个过程中，角度必须以弧度为单位。\n\n从以下具有周期性边界条件的物理一致常微分方程（ODE）开始：\n给定域 $x \\in [0, 2\\pi]$，考虑\n$$\nu''(x) + u(x) = -624 \\sin(25x),\n$$\n其周期性边界条件为\n$$\nu(0) = u(2\\pi), \\quad u'(0) = u'(2\\pi).\n$$\n一个经过充分检验的事实是，如果 $u(x) = \\sin(x) + \\sin(25x)$，那么 $u''(x) + u(x) = -624 \\sin(25x)$ 并且周期性边界条件成立。除了边界条件外，您不得使用任何关于 $u(x)$ 的标记训练数据；相反，应按照物理信息神经网络（PINN）的标准做法，在损失函数中使用ODE残差和边界残差。\n\n构建一个具有 $H$ 个隐藏单元和双曲正切激活函数的单隐藏层神经网络 $u_{\\theta}(x)$ 作为试探解。将隐藏层的预激活值定义为 $z_i(x) = w_i x + b_i$（其中 $i \\in \\{1,\\dots,H\\}$），隐藏层的激活值定义为 $h_i(x) = \\tanh(z_i(x))$，输出定义为\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i h_i(x) + c.\n$$\n使用链式法则和乘积法则，以闭式形式计算 $u_{\\theta}(x)$ 关于 $x$ 的一阶和二阶导数。回想一下双曲正切函数及其导数的标准恒等式：\n$$\n\\tanh'(z) = \\operatorname{sech}^2(z), \\quad \\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z), \\quad \\operatorname{sech}^2(z) = 1 - \\tanh^2(z).\n$$\n将配置点 $\\{x_n\\}_{n=1}^{N}$ 的逐点物理残差定义为\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) - \\left(-624 \\sin(25 x_n)\\right),\n$$\n并将周期性边界残差定义为\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi), \\quad r_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi).\n$$\n使用带有边界权重 $\\lambda_{\\text{bc}}$ 的均方残差损失：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right).\n$$\n通过从随机初始化开始的基于梯度的优化来训练参数 $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$。为了定量评估谱偏见，在较短的训练预算结束后，通过最小二乘法将学习到的函数 $u_{\\theta}(x)$ 投影到 $[0, 2\\pi)$ 上密集均匀网格上的两个基函数 $\\sin(x)$ 和 $\\sin(25x)$ 上。也就是说，找到最小化以下表达式的系数 $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$：\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2,\n$$\n其中 $x_m$ 在 $[0, 2\\pi)$ 中均匀分布。将学习到的振幅定义为 $A_1 = |\\hat{\\alpha}_1|$ 和 $A_{25} = |\\hat{\\alpha}_{25}|$。如果在早期训练中 $A_1 > A_{25}$，则认为存在谱偏见。\n\n实现该程序时，应包含一个完全向量化的训练循环，并仅使用ODE残差和边界残差来计算所有网络参数的闭式梯度。不要使用任何外部的自动微分库。\n\n测试套件和输出规范：\n- 使用以下三个测试用例来检验不同情况。每个用例指定 $(H, N, K, \\eta)$，其中 $H$ 是隐藏单元的数量， $N$ 是配置点的数量， $K$ 是梯度步数， $\\eta$ 是学习率。在所有用例中，使用 $\\lambda_{\\text{bc}} = 1$。角度以弧度为单位。\n  1. 用例 1： $(H, N, K, \\eta) = (20, 128, 60, 0.01)$。\n  2. 用例 2： $(H, N, K, \\eta) = (10, 64, 80, 0.01)$。\n  3. 用例 3： $(H, N, K, \\eta) = (5, 128, 120, 0.01)$。\n- 对每个用例，使用固定的种子初始化参数，以使结果具有确定性。在训练 $K$ 步后，通过在包含 $M$ 个点的密集网格（$M = 4096$）上进行最小二乘投影来计算 $A_1$ 和 $A_{25}$。为每个用例记录一个布尔结果，定义如下：\n$$\n\\text{result} = \\begin{cases}\n\\text{True},  \\text{如果 } A_1 > A_{25},\\\\\n\\text{False},  \\text{其他情况。}\n\\end{cases}\n$$\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，\"[True,True,False]\"）。\n\n您的程序必须是自包含的，不接收任何输入，并且能够直接运行。角度必须以弧度为单位。所有数值答案都是无量纲的，最终输出是布尔值。训练和投影必须使用上述公式以纯线性代数方式实现，不使用任何外部机器学习框架。目标是通过这些测试用例证明，物理信息神经网络（PINN）学习低频分量 $\\sin(x)$ 的速度早于高频分量 $\\sin(25x)$，这与谱偏见一致。", "solution": "所提出的问题是计算物理学中一个有效且适定的练习，具体地展示了物理信息神经网络（PINN）中的谱偏见现象。它具有科学依据，微分方程及其解析解的表述都是正确的。所有参数和步骤都已指定，从而可以得到唯一且可验证的计算结果。我将着手提供一个解决方案。\n\n目标是训练一个神经网络 $u_{\\theta}(x)$ 来近似一维常微分方程（ODE）的解：\n$$\nu''(x) + u(x) = -624 \\sin(25x)\n$$\n该方程定义在域 $x \\in [0, 2\\pi]$ 上，并带有周期性边界条件 $u(0) = u(2\\pi)$ 和 $u'(0) = u'(2\\pi)$。其解析解 $u(x) = \\sin(x) + \\sin(25x)$ 是一个低频分量和一个高频分量的叠加。我们将证明，对PINN损失函数进行基于梯度的优化会使网络学习低频分量 $\\sin(x)$ 的速度快于高频分量 $\\sin(25x)$。\n\n首先，我们定义神经网络的拟设，这是一个具有 $H$ 个神经元和 $\\tanh$ 激活函数的单隐藏层感知机：\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i \\tanh(w_i x + b_i) + c\n$$\n网络的参数为 $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$。为了施加ODE的约束，我们必须计算 $u_{\\theta}(x)$ 关于 $x$ 的一阶和二阶导数。使用链式法则和恒等式 $\\frac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ 以及 $\\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z)$，我们得到：\n$$\nu'_{\\theta}(x) = \\frac{d u_{\\theta}}{dx} = \\sum_{i=1}^{H} a_i w_i \\operatorname{sech}^2(w_i x + b_i)\n$$\n$$\nu''_{\\theta}(x) = \\frac{d^2 u_{\\theta}}{dx^2} = -2 \\sum_{i=1}^{H} a_i w_i^2 \\operatorname{sech}^2(w_i x + b_i) \\tanh(w_i x + b_i)\n$$\n\n网络通过最小化一个由ODE残差的均方误差和边界条件残差组成的损失函数来进行训练。在一组包含 $N$ 个配置点 $\\{x_n\\}$ 上的物理残差为：\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n)\n$$\n周期性边界条件的残差为：\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi)\n$$\n$$\nr_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi)\n$$\n总损失函数是一个加权和：\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{phys}} + \\lambda_{\\text{bc}} \\mathcal{L}_{\\text{bc}} = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right)\n$$\n其中 $\\lambda_{\\text{bc}}$ 是一个用于平衡各项的超参数，给定为 $\\lambda_{\\text{bc}} = 1$。\n\n训练使用梯度下降法进行。参数根据 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$ 进行更新，其中 $\\eta$ 是学习率。我们必须推导解析梯度 $\\nabla_{\\theta} \\mathcal{L}(\\theta)$。损失函数关于任意参数 $p \\in \\theta$ 的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{2}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n) \\left(\\frac{\\partial u''_{\\theta}(x_n)}{\\partial p} + \\frac{\\partial u_{\\theta}(x_n)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},1}\\left(\\frac{\\partial u_{\\theta}(0)}{\\partial p} - \\frac{\\partial u_{\\theta}(2\\pi)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},2}\\left(\\frac{\\partial u'_{\\theta}(0)}{\\partial p} - \\frac{\\partial u'_{\\theta}(2\\pi)}{\\partial p}\\right)\n$$\n网络输出及其空间导数关于参数 $\\{a_k, w_k, b_k, c\\}$ 的导数通过链式法则计算。这些推导虽然繁琐但系统，并以向量化形式实现以提高计算效率。例如，关于输出权重 $a_k$ 的梯度涉及诸如 $\\frac{\\partial u_{\\theta}(x)}{\\partial a_k} = \\tanh(w_k x + b_k)$ 之类的项。所有梯度的完整表达式都在代码中实现。\n\n在训练指定步数后，我们量化学习到的频率分量。我们在 $[0, 2\\pi)$ 上的一个包含 $M$ 个点的密集网格 $\\{x_m\\}$ 上评估训练好的网络 $u_{\\theta}(x)$。然后，我们通过求解一个线性最小二乘问题，将这个学习到的函数投影到基函数 $\\sin(x)$ 和 $\\sin(25x)$ 上，以找到最小化以下表达式的系数 $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$：\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2\n$$\n该问题的解由 $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{B}^T\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{y}$ 给出，其中 $\\mathbf{y}$ 是网络预测值 $u_{\\theta}(x_m)$ 的向量，$\\mathbf{B}$ 是以 $\\sin(x_m)$ 和 $\\sin(25x_m)$ 为列的设计矩阵。学习到的振幅为 $A_1 = |\\hat{\\alpha}_1|$ 和 $A_{25} = |\\hat{\\alpha}_{25}|$。如果 $A_1 > A_{25}$，我们断定观察到了谱偏见。\n\n实现将遵循这些原则，使用 `numpy` 进行向量化数值计算，包括完全解析的梯度计算和标准的梯度下降循环。参数初始化将使用固定的随机种子和Glorot/Xavier缩放，以保证可复现性和稳定的训练。", "answer": "```python\nimport numpy as np\n\nclass PINN:\n    \"\"\"\n    A Physics-Informed Neural Network to demonstrate spectral bias.\n    The implementation is fully vectorized and uses analytical gradients.\n    \"\"\"\n    def __init__(self, H, N, seed):\n        \"\"\"\n        Initializes the PINN.\n        H: number of hidden units\n        N: number of collocation points\n        seed: random seed for parameter initialization\n        \"\"\"\n        self.H = H\n        self.N = N\n        self.lambda_bc = 1.0\n        self.rng = np.random.default_rng(seed)\n\n        # Xavier/Glorot initialization\n        # For weights w, n_in=1, n_out=1 (conceptual). limit = sqrt(6 / (1+1)) = sqrt(3)\n        limit_w = np.sqrt(3.0)\n        self.w = self.rng.uniform(-limit_w, limit_w, size=(1, self.H))\n        \n        # For weights a, n_in=H, n_out=1. limit = sqrt(6 / (H+1))\n        limit_a = np.sqrt(6.0 / (self.H + 1.0))\n        self.a = self.rng.uniform(-limit_a, limit_a, size=(self.H, 1))\n\n        self.b = np.zeros((1, self.H))\n        self.c = np.zeros((1, 1))\n\n        self.x_colloc = np.linspace(0, 2 * np.pi, self.N, endpoint=False).reshape(-1, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Computes the network output u and its derivatives u', u'' w.r.t. x.\n        x: input points, shape (num_points, 1)\n        \"\"\"\n        z = x @ self.w + self.b\n        h = np.tanh(z)\n        s = 1.0 - h**2  # sech^2(z)\n\n        u = h @ self.a + self.c\n        u_prime = (s * self.w) @ self.a\n        u_double_prime = (-2.0 * s * h * (self.w**2)) @ self.a\n\n        return u, u_prime, u_double_prime, h, s\n\n    def _compute_gradients(self):\n        \"\"\"\n        Computes the loss and the gradients of the loss w.r.t. all parameters.\n        All calculations are vectorized.\n        \"\"\"\n        # --- Physics Loss and Gradients ---\n        u, _, u_pp, H_c, S_c = self.forward(self.x_colloc)\n        \n        f_term = -624.0 * np.sin(25.0 * self.x_colloc)\n        r_phys = u_pp + u - f_term\n        loss_phys = np.mean(r_phys**2)\n\n        # Common factor for physics gradients\n        grad_common_phys = (2.0 / self.N) * r_phys\n        \n        # Gradient w.r.t. a\n        d_u_da = H_c\n        d_u_pp_da = -2.0 * S_c * H_c * self.w**2\n        grad_a_phys = (d_u_da + d_u_pp_da).T @ grad_common_phys\n\n        # Gradient w.r.t. b\n        d_u_db = self.a.T * S_c\n        d_u_pp_db = self.a.T * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        grad_b_phys = np.sum(grad_common_phys * (d_u_db + d_u_pp_db), axis=0)\n\n        # Gradient w.r.t. w\n        d_u_dw = self.a.T * self.x_colloc * S_c\n        d_u_pp_dw = self.a.T * (\n            -4.0 * self.w * S_c * H_c + \n            self.x_colloc * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        )\n        grad_w_phys = np.sum(grad_common_phys * (d_u_dw + d_u_pp_dw), axis=0)\n        \n        # Gradient w.r.t. c\n        grad_c_phys = np.sum(grad_common_phys)\n\n        # --- Boundary Loss and Gradients ---\n        x_bc = np.array([[0.0], [2 * np.pi]])\n        u_bc, u_p_bc, _, H_bc, S_bc = self.forward(x_bc)\n        \n        u0, u2pi = u_bc[0], u_bc[1]\n        u0_p, u2pi_p = u_p_bc[0], u_p_bc[1]\n\n        r_bc1 = u0 - u2pi\n        r_bc2 = u0_p - u2pi_p\n        loss_bc = r_bc1**2 + r_bc2**2\n        \n        H0, H2pi = H_bc[0:1, :], H_bc[1:2, :]\n        S0, S2pi = S_bc[0:1, :], S_bc[1:2, :]\n        \n        # Common factors for BC gradients\n        common1 = 2.0 * self.lambda_bc * r_bc1\n        common2 = 2.0 * self.lambda_bc * r_bc2\n\n        # Gradient w.r.t. a\n        delta_u_da = (H0 - H2pi).T\n        delta_u_p_da = (self.w * (S0 - S2pi)).T\n        grad_a_bc = common1 * delta_u_da + common2 * delta_u_p_da\n        \n        # Gradient w.r.t. b\n        delta_u_db = self.a.T * (S0 - S2pi)\n        delta_u_p_db = -2.0 * self.a.T * self.w * (S0 * H0 - S2pi * H2pi)\n        grad_b_bc = common1 * delta_u_db + common2 * delta_u_p_db\n\n        # Gradient w.r.t. w\n        delta_u_dw = -2.0 * np.pi * self.a.T * S2pi\n        delta_u_p_dw = self.a.T * (S0 - S2pi) + 4.0 * np.pi * self.a.T * self.w * S2pi * H2pi\n        grad_w_bc = common1 * delta_u_dw + common2 * delta_u_p_dw\n\n        # Gradient w.r.t. c (is zero)\n        grad_c_bc = 0.0\n\n        # --- Total Loss and Gradients ---\n        loss = loss_phys + self.lambda_bc * loss_bc\n        grad_a = grad_a_phys + grad_a_bc\n        grad_w = grad_w_phys.reshape(1, -1) + grad_w_bc\n        grad_b = grad_b_phys.reshape(1, -1) + grad_b_bc\n        grad_c = grad_c_phys + grad_c_bc\n\n        return loss, grad_a, grad_w, grad_b, grad_c\n\n    def train(self, K, eta):\n        \"\"\"\n        Trains the network using gradient descent.\n        K: number of training steps\n        eta: learning rate\n        \"\"\"\n        for _ in range(K):\n            loss, grad_a, grad_w, grad_b, grad_c = self._compute_gradients()\n            \n            self.a -= eta * grad_a\n            self.w -= eta * grad_w\n            self.b -= eta * grad_b\n            self.c -= eta * grad_c\n\n    def project_and_analyze(self):\n        \"\"\"\n        Projects the learned function onto sin(x) and sin(25x) and checks for spectral bias.\n        \"\"\"\n        M = 4096\n        x_dense = np.linspace(0, 2 * np.pi, M, endpoint=False).reshape(-1, 1)\n        \n        u_pred, _, _, _, _ = self.forward(x_dense)\n        u_pred = u_pred.flatten()\n        \n        # Create design matrix for least squares\n        B = np.zeros((M, 2))\n        B[:, 0] = np.sin(x_dense.flatten())\n        B[:, 1] = np.sin(25.0 * x_dense.flatten())\n        \n        # Solve least squares problem: B * alpha = u_pred\n        alpha, _, _, _ = np.linalg.lstsq(B, u_pred, rcond=None)\n        \n        A1 = np.abs(alpha[0])\n        A25 = np.abs(alpha[1])\n        \n        return A1 > A25\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (20, 128, 60, 0.01),  # Case 1: (H, N, K, eta)\n        (10, 64, 80, 0.01),   # Case 2\n        (5, 128, 120, 0.01),  # Case 3\n    ]\n\n    results = []\n    base_seed = 42\n\n    for i, (H, N, K, eta) in enumerate(test_cases):\n        seed = base_seed + i\n        pinn = PINN(H=H, N=N, seed=seed)\n        pinn.train(K=K, eta=eta)\n        result = pinn.project_and_analyze()\n        results.append(result)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2427229"}]}