## 引言
[物理信息神经网络](@entry_id:145229)（Physics-Informed Neural Networks, PINN）作为一种融合了[深度学习](@entry_id:142022)与物理学第一性原理的新兴计算[范式](@entry_id:161181)，正深刻地改变着我们求解偏微分方程（PDE）的方式。传统数值方法如[有限元法](@entry_id:749389)（FEM）虽然强大，但在处理高维问题、复杂几何或需要从[稀疏数据](@entry_id:636194)中进行[反问题](@entry_id:143129)推断时常面临挑战。PINN通过将物理定律直接编码到[神经网](@entry_id:276355)络的训练过程中，为这些难题提供了一个灵活且强大的无网格解决方案，弥合了纯数据驱动模型与物理现实之间的鸿沟。

本文旨在为读者提供一个关于PINN的系统性理解，从核心理论到前沿应用。通过学习本文，您将能够掌握如何构建、训练和应用PINN来解决复杂的科学与工程问题。文章分为三个核心部分：在“原理与机制”一章中，我们将深入剖析PINN的理论基石，包括其独特的[损失函数](@entry_id:634569)、[自动微分](@entry_id:144512)的角色以及网络设计的物理内涵。随后，在“应用与交叉学科联系”一章中，我们将展示PINN在[固体力学](@entry_id:164042)中的高级应用，并探索其在[流体力学](@entry_id:136788)、生物学等多个领域的[交叉](@entry_id:147634)潜力。最后，“动手实践”部分将通过具体的编码示例，帮助您巩固理论知识，亲身体验PINN的强大功能与内在挑战。

## 原理与机制

物理信息神经网络（Physics-Informed Neural Networks, PINN）将[深度学习](@entry_id:142022)的函数逼近能力与物理定律的数学描述相结合，为[求解偏微分方程](@entry_id:138485)（PDE）提供了一种全新的方法。在本章中，我们将深入探讨PINN的核心工作原理与关键机制，从损失函数的构建到网络架构的选择，再到高级的变分公式和优化策略。我们的目标是建立一个坚实的理论基础，以便在后续章节中解决复杂的[固体力学](@entry_id:164042)问题。

### 物理约束的[损失函数](@entry_id:634569)：核心思想

PINN的基石是一个精心设计的**损失函数**（loss function），它将物理定律和边界/[初始条件](@entry_id:152863)编码为可优化的目标。训练网络的过程，本质上是寻找一组网络参数 $\theta$，使得网络输出的函数能够最小化这个损失函数，从而近似满足所有的物理约束。

#### 定义残差

PINN的核心思想是，如果一个函数 $u_\theta(x)$ 是某个PDE的解，那么当它被代入PDE算子时，结果应为零。这个代入后的结果被称为**残差**（residual）。对于一个通用PDE，可以写作 $\mathcal{N}[u(x)] = f(x)$，其残差为 $r(x; \theta) = \mathcal{N}[u_\theta(x)] - f(x)$。理想情况下，对于域内所有点 $x$，残差 $r(x; \theta)$ 都应为零。

除了PDE本身，解还必须满足**边界条件**（Boundary Conditions, BC）和**初始条件**（Initial Conditions, IC）。同样，我们可以为这些条件定义残差。例如，对于[狄利克雷边界条件](@entry_id:173524) $u(x) = g(x)$ on $\Gamma_D$，其残差为 $r_{BC}(x; \theta) = u_\theta(x) - g(x)$。

让我们以一维[热传导方程](@entry_id:194763)为例来说明 [@problem_id:2126325]。该方程描述了在空间域 $x \in [0, 1]$ 和时间 $t \ge 0$ 内的温度[分布](@entry_id:182848) $u(x, t)$：
$$
\frac{\partial u}{\partial t} - \alpha \frac{\partial^2 u}{\partial x^2} = 0
$$
其中 $\alpha$ 是热扩散系数。设[初始条件](@entry_id:152863)为 $u(x, 0) = g(x)$，边界条件为 $u(0, t) = h_0(t)$ 和 $u(1, t) = h_1(t)$。

对于一个由参数为 $\theta$ 的[神经网](@entry_id:276355)络所代表的近似解 $u_\theta(x, t)$，我们可以定义三个残差：
1.  **PDE残差**：$r_{PDE}(x, t; \theta) = \frac{\partial u_\theta}{\partial t} - \alpha \frac{\partial^2 u_\theta}{\partial x^2}$，在域内部 $(x, t) \in (0, 1) \times (0, T]$。
2.  **边界条件残差**：$r_{BC_0}(t; \theta) = u_\theta(0, t) - h_0(t)$ 和 $r_{BC_1}(t; \theta) = u_\theta(1, t) - h_1(t)$，在空间边界上。
3.  **初始条件残差**：$r_{IC}(x; \theta) = u_\theta(x, 0) - g(x)$，在初始时刻。

#### 构建[损失函数](@entry_id:634569)与权重平衡

有了残差的定义，我们可以通过惩罚这些残差的非零值来构建总损失函数。通常采用**均方误差**（Mean Squared Error, MSE）的形式，对在求解域、边界和初始时刻采样的大量**[配置点](@entry_id:169000)**（collocation points）上的残差进行平均。

总损失函数 $\mathcal{L}_{total}$ 是各项残差损失的加权和：
$$
\mathcal{L}_{total}(\theta) = \lambda_{PDE} \mathcal{L}_{PDE} + \lambda_{BC} \mathcal{L}_{BC} + \lambda_{IC} \mathcal{L}_{IC}
$$
其中，$\mathcal{L}_{PDE} = \frac{1}{N_{PDE}} \sum_{i=1}^{N_{PDE}} |r_{PDE}(x_i, t_i; \theta)|^2$，其他项类似。$\lambda_{PDE}$、$\lambda_{BC}$ 和 $\lambda_{IC}$ 是正的**权重**（weights），用于平衡不同损失项的重要性。

这些权重深刻影响训练动态和最终解的质量 [@problem_id:2126325]。选择不当的权重是PINN训练失败的常见原因。
-   如果**边界/初始条件的权重过大**（例如 $\lambda_{BC}, \lambda_{IC} \gg \lambda_{PDE}$），优化器会优先确保网络输出满足边界和[初始条件](@entry_id:152863)，但可能会忽略PDE本身。结果是，解在边界上看起来很完美，但在域内部却严重违反物理定律，导致PDE残差很大。
-   反之，如果**PDE残差的权重过大**（$\lambda_{PDE} \gg \lambda_{BC}, \lambda_{IC}$），网络会努力学习一个在域内部满足PDE的函数，但这个函数可能与给定的边界或初始条件严重偏离。

因此，寻找一个合适的权重平衡，使得所有物理约束都能被同等重视，是成功训练PINN的关键一步。

#### 高级权重策略

虽然手动调整权重是一种方法，但对于复杂问题，这既繁琐又不可靠。幸运的是，存在多种更系统化的策略来设定或调整这些权重 [@problem_id:2668878]。

1.  **无量纲化（Nondimensionalization）**：不同残差项通常具有不同的物理单位。例如，在线弹性问题中，PDE残差的单位是力/体积（$[\text{F}/\text{L}^3]$），而[位移边界条件](@entry_id:203261)的残差单位是长度（$[\text{L}]$）。直接将它们的平方相加在物理上是无意义的。一种健壮的策略是使用问题的[特征长度](@entry_id:265857) $L^*$、特征应力 $\sigma^*$ 和[特征位移](@entry_id:140262) $u^*$ 对各项进行无量纲化。通过选择权重如 $\alpha_{\Omega} = (L^*/\sigma^*)^2$ 和 $\alpha_{u} = (1/u^*)^2$，可以使得[损失函数](@entry_id:634569)中的每一项都变为无量纲量，并且量级相当，从而实现物理上一致的平衡 [@problem_id:2668878]。

2.  **自适应权重（Adaptive Weighting）**：在训练过程中动态调整权重是一种更先进的技术。其核心思想是监测每个损失项对网络参数 $\theta$ 产生的梯度。如果某个损失项的梯度比其他项小几个[数量级](@entry_id:264888)，那么它在参数更新中的“发言权”就很小。[自适应算法](@entry_id:142170)会动态增大这个损失项的权重，以确保所有物理约束都能对训练过程做出有意义的贡献。平衡不同损失项的梯度范数是一种常见的策略 [@problem_id:2668878]。

3.  **硬约束（Hard Constraints）**：对于[狄利克雷边界条件](@entry_id:173524)，一种非常有效的方法是构造一个特殊的网络**[拟设](@entry_id:184384)**（ansatz），使其通过构造精确满足边界条件。例如，要在一个边界 $\Gamma_u$ 上强制 $u_\theta(x) = \bar{u}(x)$，可以设计网络输出为：
    $$
    u_\theta(x) = \bar{u}(x) + g(x) \hat{u}_\theta(x)
    $$
    其中 $\hat{u}_\theta(x)$ 是一个标准[神经网](@entry_id:276355)络的输出，而 $g(x)$ 是一个已知的函数，它在 $\Gamma_u$ 上为零，在域内非零。这样，无论 $\hat{u}_\theta(x)$ 输出什么， $u_\theta(x)$ 在 $\Gamma_u$ 上总能精确等于 $\bar{u}(x)$。这种方法将有约束的[优化问题](@entry_id:266749)转化为了无约束问题，完全消除了对相应边界损失项及其权重的需求，极大地简化了训练 [@problem_id:2668878]。

### 从物理到代码：[自动微分](@entry_id:144512)的角色

我们已经知道PINN的核心是最小化包含导数的PDE残差。但是，[神经网](@entry_id:276355)络本身只是一个复杂的[复合函数](@entry_id:147347)，我们如何计算它的导数呢？答案是**[自动微分](@entry_id:144512)**（Automatic Differentiation, AD）。

#### 用[自动微分](@entry_id:144512)计算导数

[自动微分](@entry_id:144512)是现代[深度学习](@entry_id:142022)框架（如TensorFlow和PyTorch）的基石。与[符号微分](@entry_id:177213)（如Mathematica）或[数值微分](@entry_id:144452)（如[有限差分](@entry_id:167874)）不同，AD通过**链式法则**在[计算图](@entry_id:636350)上精确地计算任意阶导数，其精度达到机器精度。对于PINN，AD被用来计算网络输出（例如位移场 $u_\theta$）关于其输入（例如空间坐标 $x, y, z$）的导数。

#### 应用实例：线弹性力学

让我们通过一个具体的[固体力学](@entry_id:164042)问题——二维线[弹性静力学](@entry_id:198298)，来理解AD在PINN中的工作流程 [@problem_id:2668906]。该问题的控制方程是力的平衡方程：
$$
\nabla \cdot \sigma + b = 0
$$
其中 $\sigma$ 是柯西应力张量，$b$ 是[体力](@entry_id:174230)密度。[应力与应变](@entry_id:137374) $\varepsilon$ 通过本构关系（[胡克定律](@entry_id:149682)） $\sigma = C:\varepsilon$ 联系，而应变与位移 $u$ 通过几何关系 $\varepsilon = \frac{1}{2}(\nabla u + \nabla u^T)$ 联系。

为了让PINN求解这个问题，我们需要计算PDE残差 $r(x) = \nabla \cdot \sigma(u_\theta(x)) + b(x)$。这个过程涉及一系列环环相扣的计算，AD在其中扮演了两次关键角色：

1.  **输入坐标** $x = (x_1, x_2)$ 被送入[神经网](@entry_id:276355)络 $u_\theta$，输出位移向量 $u_\theta(x) = (u_{\theta,1}, u_{\theta,2})$。
2.  **第一次AD调用**：我们使用AD计算 $u_\theta$ 关于输入 $x$ 的**雅可比矩阵**，即[位移梯度](@entry_id:165352) $\nabla u_\theta$。其分量为 $\partial_j u_{\theta,i}$。
3.  **代数计算**：利用[位移梯度](@entry_id:165352)的结果，通过代数运算计算应变张量 $\varepsilon_{ij} = \frac{1}{2}(\partial_i u_{\theta,j} + \partial_j u_{\theta,i})$。
4.  **代数计算**：利用[应变张量](@entry_id:193332)，通过[本构关系](@entry_id:186508)计算应力张量 $\sigma_{ij} = C_{ijkl} \varepsilon_{kl}$。注意，此时的 $\sigma_{ij}$ 是一个依赖于 $u_\theta$ 一阶导数的复杂函数。
5.  **第二次AD调用**：我们再次使用AD，计算应力张量 $\sigma_{ij}$ 关于空间坐标 $x$ 的**散度**，即 $\partial_j \sigma_{ij}$。由于 $\sigma_{ij}$ 已经是 $u_\theta$ 一阶导数的函数，计算它的导数实际上是在计算 $u_\theta$ 的**[二阶导数](@entry_id:144508)**。
6.  **组装残差**：最后，将计算出的应力散度与体力项 $b_i$ 相加，得到最终的PDE残差 $r_i(x) = \partial_j \sigma_{ij}(x) + b_i(x)$。

这个过程清楚地表明，对于像弹性力学这样的[二阶PDE](@entry_id:175326)，PINN必须计算网络输出的[二阶导数](@entry_id:144508)。这得益于AD框架能够处理高阶导数的能力，通常是通过嵌套AD调用或构建包含高阶导数信息的[计算图](@entry_id:636350)来实现的 [@problem_id:2668906] [@problem_id:2668954]。

与[有限差分](@entry_id:167874)等数值方法相比，AD在计算[二阶导数](@entry_id:144508)时不仅没有截断误差，而且在[计算效率](@entry_id:270255)上也具有优势。对于一个 $d$ 维问题，使用[中心差分法](@entry_id:163679)估算所有[二阶偏导数](@entry_id:635213)需要 $O(d^2)$ 次网络[前向传播](@entry_id:193086)，而使用高效的AD组合策略（如前向-反向模式结合）的计算成本大约为 $O(d)$ 次[前向传播](@entry_id:193086)的量级，这在三维问题中尤为重要 [@problem_id:2668954]。

#### 应用实例：[超弹性](@entry_id:159356)力学

PINN和AD的威力同样可以扩展到[几何非线性](@entry_id:169896)的有限应变问题，例如超弹性力学 [@problem_id:2668881]。在这种情况下，我们不再求解位移场，而是直接让[神经网](@entry_id:276355)络 $ \varphi_\theta $ 学习从参考构型中的物[质点](@entry_id:186768)坐标 $ X $ 到当前构型中空间点坐标 $ x $ 的**变形映射**（deformation mapping）$ x = \varphi_\theta(X) $。

这里的关键物理量是**变形梯度**（deformation gradient）$F$，它定义为变形映射的雅可比矩阵：
$$
F = \frac{\partial \varphi}{\partial X}
$$
在PINN的框架下，这变得异常简单：$F$ 就是网络输出 $\varphi_\theta$ 关于其输入 $X$ 的[雅可比矩阵](@entry_id:264467)，可以直接通过一次AD调用获得。

一旦获得了变形梯度 $F$，所有其他的[有限应变运动学](@entry_id:168563)量，如[右柯西-格林张量](@entry_id:174156) $C = F^T F$ 和[格林-拉格朗日应变](@entry_id:170427) $E = \frac{1}{2}(C - I)$，都可以通过后续的代数运算得到。对于一个[应变能密度函数](@entry_id:755490)为 $W(C)$ 的[超弹性材料](@entry_id:190241)，其[第二皮奥拉-基尔霍夫应力](@entry_id:173163) $S$ 可以通过对 $W$ 求导得到：
$$
S = 2 \frac{\partial W}{\partial C}
$$
这个导数同样可以通过AD计算，展示了AD在处理复杂、[非线性](@entry_id:637147)的本构关系中的强大能力。最终，将所有量代入力的[平衡方程](@entry_id:172166)（以参考构型描述），即可构建PINN的PDE残差。

### 网络架构的物理内涵

[神经网](@entry_id:276355)络的架构并非与物理问题无关。相反，网络的设计选择，特别是[激活函数](@entry_id:141784)的类型，直接影响其逼近[PDE解](@entry_id:166250)的能力和训练的成败。

#### [激活函数](@entry_id:141784)的选择

激活函数决定了[神经网](@entry_id:276355)络的表达能力和其输出函数的**正则性**（regularity），即光滑程度。对于需要计算[二阶导数](@entry_id:144508)的[固体力学](@entry_id:164042)问题，[激活函数](@entry_id:141784)的选择至关重要 [@problem_id:2668888]。

-   **ReLU (Rectified Linear Unit)**：$\phi(z) = \max(0, z)$。ReLU是[深度学习](@entry_id:142022)中最流行的激活函数之一，但它对于求解[二阶PDE](@entry_id:175326)的强形式PINN来说是一个**糟糕的选择**。[ReLU网络](@entry_id:637021)代表的是一个连续[分段线性函数](@entry_id:273766)。它的[二阶导数](@entry_id:144508)[几乎处处](@entry_id:146631)为零，仅在“折痕”处以[狄拉克δ函数](@entry_id:153299)的形式存在。当PINN在[配置点](@entry_id:169000)计算PDE残差时，这些点几乎肯定落在[二阶导数](@entry_id:144508)为零的[线性区](@entry_id:276444)域内。这导致 $\nabla \cdot \sigma(u_\theta)$ 项人为地变为零，优化器会错误地认为PDE已被满足，而实际上网络可能只是学到了一个与真实解毫不相干的[简单函数](@entry_id:137521)。这种现象被称为“[二阶导数](@entry_id:144508)消失”，使得训练过程产生误导。

-   **Tanh 和 GELU**：[双曲正切函数](@entry_id:634307) $\tanh(z)$ 和[高斯误差线性单元](@entry_id:638032) $\mathrm{GELU}(z)$ 都是无限次可微的（$C^\infty$）[光滑函数](@entry_id:267124)。使用这些[激活函数](@entry_id:141784)的[神经网](@entry_id:276355)络所代表的函数也是 $C^\infty$ 的。这保证了所有高阶导数（包括计算 $\nabla \cdot \sigma$ 所需的[二阶导数](@entry_id:144508)）都是良好定义的，可以通过AD稳定地计算出来。因此，对于[固体力学](@entry_id:164042)中的[二阶PDE](@entry_id:175326)，这些光滑激活函数是比ReLU远为合适的选择。

-   **Sine**：正弦函数 $\sin(z)$ 也是一个 $C^\infty$ 函数。使用它作为激活函数（如在SIREN架构中）不仅能提供高质量的导数，还能有效缓解下面将要讨论的“谱偏差”问题，使其在表示具有复杂高频细节的解时表现出色。

#### 谱偏差问题

标准的PINN（例如使用 `[tanh](@entry_id:636446)` 激活函数）在训练中表现出一种强烈的**谱偏差**（spectral bias）：它们会优先学习[目标函数](@entry_id:267263)的低频成分，而学习高频成分则非常缓慢和困难 [@problem_id:2411070]。

我们可以通过亥姆霍兹方程 $u''(x) + k^2 u(x) = 0$ 来直观理解这个问题。当[波数](@entry_id:172452) $k$ 很大时，其解 $u(x) = C \sin(kx)$ 是一个高频[振荡](@entry_id:267781)函数。然而，对于PINN的优化器来说，平凡解 $u(x) = 0$ 是一个极具吸[引力](@entry_id:175476)的“捷径”。这个零函数频率为零，完全满足边界条件 $u(0)=0, u(\pi)=0$，并且其PDE残差也恒为零。由于谱偏差，从随机初始化开始的梯度下降过程很容易陷入这个平凡解的“[引力](@entry_id:175476)盆”中，而无法发现正确的高频[振荡](@entry_id:267781)解。

这种现象在固体力学中也普遍存在，例如在波传播或细观结构力学问题中，解可能包含丰富的多尺度、高频特征。谱偏差是PINN在这些领域应用的一个核心挑战。

#### 缓解谱偏差的策略

为了克服谱偏差，研究者们提出了多种策略，主要思想是改变网络的架构或输入，使其更容易地表达高频函数 [@problem_id:2411070] [@problem_id:2668888]。

1.  **傅里叶特征嵌入（Fourier Feature Embedding）**：这种方法不是将原始坐标 $x$ 直接输入网络，而是先将其映射到一组高频的傅里叶特征上：
    $$
    x \mapsto [\cos(\omega_1 x), \sin(\omega_1 x), \dots, \cos(\omega_L x), \sin(\omega_L x)]
    $$
    其中频率 $\{\omega_j\}$ 是精心挑选的。这样，网络不再需要从零开始“合成”高频函数，而是可以直接“组合”这些已经提供的高频[基函数](@entry_id:170178)。这极大地简化了学习任务，使网络能够有效地拟合高频解。

2.  **周期性激活函数（Periodic Activations）**：如前所述，直接使用 $\sin$作为激活函数是另一种强大的策略。这类网络（如SIREN）的导数本身就是相位移动的正弦函数，其内在的[归纳偏置](@entry_id:137419)与表示[振荡](@entry_id:267781)信号和复杂几何形状天然契合，从而能够逃脱标准[激活函数](@entry_id:141784)的低频陷阱。

3.  **采样密度**：根据[奈奎斯特-香农采样定理](@entry_id:262499)，为了分辨一个频率为 $k$ 的信号，[采样频率](@entry_id:264884)必须大于 $2k$。在PINN中，这意味着[配置点](@entry_id:169000)的密度必须足够高，以“看”到解的高频[振荡](@entry_id:267781)。如果[配置点](@entry_id:169000)过于稀疏，PDE残差在这些点上可能碰巧都很小，从而欺骗优化器，使其认为已经找到了一个好解 [@problemid:2411070]。

### 替代公式：超越强形式残差

到目前为止，我们讨论的都是基于**强形式**（strong form）PDE的PINN，即在每个点上强制PDE残差为零。然而，在固体力学中，特别是在处理不光滑解或复杂边界条件时，基于**弱形式**（weak form）或**[变分形式](@entry_id:166033)**（variational form）的PINN可能更具优势。

#### 强形式与弱形式的对比

从数学上讲，[弱形式](@entry_id:142897)是通过将PDE乘以一个**测试函数**（test function） $v$ 并在整个域上积分得到的。通过**[分部积分](@entry_id:136350)**（integration by parts），可以将导数从待求解 $u$ "转移"到测试函数 $v$ 上。

以弹性力学为例，其强形式 $\int_\Omega (\nabla \cdot \sigma + b) \cdot v \, dV = 0$ 经过分部积分后，变为[弱形式](@entry_id:142897)：
$$
\int_\Omega \sigma(u) : \nabla v \, dV = \int_\Omega b \cdot v \, dV + \int_{\Gamma_t} \bar{t} \cdot v \, dS
$$
其中 $\bar{t}$ 是在诺伊曼边界 $\Gamma_t$ 上施加的面力。这两种形式在PINN的应用中各有优劣 [@problem_id:2668902]：

-   **正则性要求**：强形式需要计算 $u$ 的[二阶导数](@entry_id:144508)，理论上要求解属于 $H^2$ 空间。而[弱形式](@entry_id:142897)只涉及 $u$ 的[一阶导数](@entry_id:749425)（包含在 $\sigma(u)$ 中），因此只要求解属于 $H^1$ 空间。对于包含裂纹尖端或尖锐拐角的问题，其解的应[力场](@entry_id:147325)是奇异的，[位移场](@entry_id:141476)通常属于 $H^1$ 但不属于 $H^2$。在这种情况下，强形式PINN会因试图在[奇点](@entry_id:137764)处评估无界的[二阶导数](@entry_id:144508)而失败，而[弱形式](@entry_id:142897)PINN则天然适用。

-   **边界条件处理**：弱形式自然地将[诺伊曼边界条件](@entry_id:142124)（如面力 $\bar{t}$）作为积分项融入其公式中。这使得它能处理非常粗糙的载荷，例如点载荷（在数学上是[狄拉克δ分布](@entry_id:267680)），而强形式PINN要求在[边界点](@entry_id:176493)上评估应力，无法处理这种情况。

-   **计算成本**：强形式PINN只需在离散的[配置点](@entry_id:169000)上评估残差，计算成本相对较低。[弱形式](@entry_id:142897)PINN（如[变分PINN](@entry_id:756443)）需要计算定义在整个域上的积分，这通常需要通过数值求积（如高斯求积）来近似，计算量远大于点态评估。

因此，对于解光滑且几何简单的“学术”问题，强形式PINN可能更高效。而对于具有几何奇异性、[材料界面](@entry_id:751731)或复杂载荷的“工程”问题，[弱形式](@entry_id:142897)PINN则更加稳健和强大 [@problem_id:2668902]。

#### 基于能量（变分）的PINN

超弹性力学中的**[最小势能原理](@entry_id:173340)**（principle of minimum potential energy）是弱形式思想的一个完美体现。该原理指出，一个弹性体在平衡状态时，其总势能 $\Pi[u]$ 达到最小值。总势能泛函由储存的应变能和外力所做的功组成：
$$
\Pi[u] = \int_{\Omega} W(\nabla u) \, d\Omega - \int_{\Omega} b \cdot u \, d\Omega - \int_{\Gamma_t} \bar{t} \cdot u \, d\Gamma
$$
基于能量的PINN（有时也称为Deep Ritz Method）直接将这个物理上含义明确的标量泛函 $\Pi[u_\theta]$ 作为其损失函数进行最小化 [@problem_id:2668890]。

这种方法的优点是显而易见的：
1.  **无需权重**：整个物理问题被浓缩成一个单一的标量（能量）。应变能和外力功的相对重要性由物理原理本身决定，无需任何人为的、难以调整的权重因子 $\lambda$。
2.  **更低的导数阶数**：[能量泛函](@entry_id:170311)通常只涉及位移的一阶导数（例如在[应变能密度](@entry_id:200085) $W$ 中），降低了对网络输出[函数光滑性](@entry_id:161935)的要求。
3.  **物理可解释性**：最小化能量是一个比最小化抽象的PDE[残差平方和](@entry_id:174395)更具物理直觉的目标。

#### 优化前景：[凸性](@entry_id:138568)与伪最优解

一个深刻且关键的问题是：如果物理问题本身是**凸的**（convex），例如在线弹性力学中，[能量泛函](@entry_id:170311) $\Pi[u]$ 是关于[位移场](@entry_id:141476) $u$ 的凸泛函，保证了存在唯一的[全局最优解](@entry_id:175747)，那么PINN的[优化问题](@entry_id:266749)也是凸的吗？

答案是**否定的** [@problem_id:2668890]。PINN的损失函数 $L(\theta) = \Pi[u_\theta]$ 是关于**网络参数** $\theta$ 的函数。从[参数空间](@entry_id:178581) $\theta$到[函数空间](@entry_id:143478) $u_\theta$ 的映射 $ \theta \mapsto u_\theta $ 是一个高度[非线性](@entry_id:637147)的过程。即使 $\Pi[\cdot]$ 是一个凸泛函，将其与这个[非线性映射](@entry_id:272931)复合后得到的 $L(\theta)$ 几乎总是关于 $\theta$ 的一个**高度非凸**（non-convex）函数。

这种固有的非凸性意味着PINN的优化前景充满了大量的**局部最优解**（local minima）。其中一些可能是“好的”局部最优解，它们对应的函数 $u_\theta$ 非常接近真实的物理最优解。但其中也可能存在**伪最优解**（spurious local minima），即网络陷入了一个在数学上是局部最小但与真实物理毫不相干的状态。这是PINN训练困难和结果不稳定的根本原因之一。

### 训练过程：优化器的选择

最后，选择合适的优化算法来驾驭这一复杂、非凸的优化前景对于成功训练至关重要。用于PINN的两个最常见的优化器家族是一阶随机方法（如Adam）和准牛顿方法（如[L-BFGS](@entry_id:167263)）。

#### 一阶与[二阶优化](@entry_id:175310)器：Adam vs. [L-BFGS](@entry_id:167263)

**Adam** (Adaptive Moment Estimation) 是一种基于梯度下降的**一阶**优化器。它通过计算梯度的一阶矩（动量）和二阶矩（[自适应学习率](@entry_id:634918)）的指数移动平均值来更新参数。其更新规则可概括为：
$$
\theta_{k+1} = \theta_k - \alpha \frac{\hat{m}_k}{\sqrt{\hat{v}_k} + \epsilon}
$$
其中 $\hat{m}_k$ 和 $\hat{v}_k$ 是经过偏差校正的动量和[二阶矩估计](@entry_id:635769)。Adam的主要优势在于其对**随机性**的鲁棒性。PINN的损失通常是在每一轮迭代中从大量[配置点](@entry_id:169000)中随机抽取一个小[子集](@entry_id:261956)（mini-batch）来估计的，这会给梯度带来噪声。Adam的动量项可以平滑这些噪声，而其[自适应学习率](@entry_id:634918)机制使其能够很好地处理[PINN损失函数](@entry_id:137288)中常见的病态条件 [@problem_id:2668893]。

**[L-BFGS](@entry_id:167263)** (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) 是一种**准牛顿**（quasi-Newton）方法，属于**二阶**优化器的范畴。它试图利用损失[函数的曲率](@entry_id:173664)信息（Hessian矩阵）来寻找更有效的[下降方向](@entry_id:637058)。[L-BFGS](@entry_id:167263)通过存储最近几次迭代的参数和梯度变化（$(s_k, y_k)$ 对）来近似Hessian[矩阵的逆](@entry_id:140380)，并使用该近似来计算更新步长 $p_k = -H_k g_k$。

[L-BFGS](@entry_id:167263)和Adam之间的选择是一个典型的权衡 [@problem_id:2668893]：
-   **在确定性环境下**：当损失函数是在整个数据集（full-batch）上计算，且没有随机噪声时，[L-BFGS](@entry_id:167263)通常表现出色。它利用二阶信息，能够以更少的迭代次数实现快速收敛，特别是在训练的[后期](@entry_id:165003)阶段，当解接近一个局部最优点时。
-   **在随机环境下**：当使用小批量（mini-batch）训练或数据本身含有噪声时，[L-BFGS](@entry_id:167263)会变得非常脆弱。梯度的噪声会污染用于近似Hessian的信息，导致[曲率估计](@entry_id:192169)不可靠，[线搜索](@entry_id:141607)过程也可能失败。在这种情况下，Adam凭借其强大的噪声鲁棒性，通常是更稳定和可靠的选择。

在实践中，一种常见的有效策略是**混合使用**：在训练初期使用Adam进行快速探索，当损失下降到一定程度后，切换到[L-BFGS](@entry_id:167263)进行精细的局部优化，以期达到更高的精度。