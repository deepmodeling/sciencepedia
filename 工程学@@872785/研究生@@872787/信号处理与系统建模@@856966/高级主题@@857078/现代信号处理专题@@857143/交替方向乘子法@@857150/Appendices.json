{"hands_on_practices": [{"introduction": "掌握交替方向乘子法（ADMM）的第一步是熟悉其核心迭代过程。本练习将通过一个具体实例，引导你推导在目标函数的一部分为简单二次型时变量的更新规则 [@problem_id:2153727]。这个练习不仅能帮助你理解 ADMM 的计算机制，也为解决更复杂的优化问题（如最小二乘问题）打下坚实的基础。", "problem": "交替方向乘子法 (ADMM) 是一种求解以下形式优化问题的算法：\n$$ \\min_{x, z} f(x) + g(z) $$\n$$ \\text{subject to } Ax + Bz = b $$\n其中变量为 $x \\in \\mathbb{R}^n$ 和 $z \\in \\mathbb{R}^m$，问题数据由矩阵 $A \\in \\mathbb{R}^{p \\times n}$、$B \\in \\mathbb{R}^{p \\times m}$、向量 $b \\in \\mathbb{R}^p$ 以及凸函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 和 $g: \\mathbb{R}^m \\to \\mathbb{R}$ 给出。\n\n该算法基于增广拉格朗日量：\n$$ L_\\rho(x, z, y) = f(x) + g(z) + y^\\mathsf{T}(Ax + Bz - b) + \\frac{\\rho}{2}\\|Ax + Bz - b\\|_2^2 $$\n其中 $y \\in \\mathbb{R}^p$ 是对偶变量（或拉格朗日乘子），$\\rho > 0$ 是一个惩罚参数。在每次迭代 $k$ 中，ADMM 依次执行以下更新：\n1.  $x^{k+1} := \\arg\\min_x L_\\rho(x, z^k, y^k)$\n2.  $z^{k+1} := \\arg\\min_z L_\\rho(x^{k+1}, z, y^k)$\n3.  $y^{k+1} := y^k + \\rho(Ax^{k+1} + Bz^{k+1} - b)$\n\n考虑该问题的一个具体实例，其中函数 $f(x)$ 定义为二次函数：\n$$ f(x) = \\frac{1}{2}\\|x - c\\|_2^2 $$\n对于一个给定的常数向量 $c \\in \\mathbb{R}^n$。\n\n为 $x$ 更新步骤 $x^{k+1}$ 推导一个闭式解析表达式。您的表达式应以问题数据 $A, B, b, c$、惩罚参数 $\\rho$ 以及前一次迭代的值 $z^k$ 和 $y^k$ 表示。在您的推导中，令 $I$ 表示 $n \\times n$ 单位矩阵，并假设矩阵 $(I + \\rho A^\\mathsf{T} A)$ 是可逆的。", "solution": "我们通过最小化增广拉格朗日量关于 $x$ 的值来推导 $x$ 的更新，同时保持 $z^{k}$ 和 $y^{k}$ 固定。$x$ 子问题是\n$$\nx^{k+1} := \\arg\\min_{x}\\left\\{\\frac{1}{2}\\|x-c\\|_{2}^{2} + (y^{k})^{\\mathsf{T}}(Ax + B z^{k} - b) + \\frac{\\rho}{2}\\|Ax + B z^{k} - b\\|_{2}^{2}\\right\\}.\n$$\n定义 $d := B z^{k} - b$，则目标函数变为\n$$\n\\frac{1}{2}\\|x-c\\|_{2}^{2} + (y^{k})^{\\mathsf{T}}(A x + d) + \\frac{\\rho}{2}\\|A x + d\\|_{2}^{2}.\n$$\n与 $x$ 无关的项不影响最小化子，因此我们关注与 $x$ 相关的部分。对 $x$ 求梯度并将其设为零，得到\n$$\n\\nabla_{x}\\left(\\frac{1}{2}\\|x-c\\|_{2}^{2}\\right) + \\nabla_{x}\\left((y^{k})^{\\mathsf{T}}A x\\right) + \\nabla_{x}\\left(\\frac{\\rho}{2}\\|A x + d\\|_{2}^{2}\\right) = 0,\n$$\n化简为\n$$\n(x - c) + A^{\\mathsf{T}} y^{k} + \\rho A^{\\mathsf{T}}(A x + d) = 0.\n$$\n合并关于 $x$ 的项，得到\n$$\n\\left(I + \\rho A^{\\mathsf{T}}A\\right) x = c - A^{\\mathsf{T}} y^{k} - \\rho A^{\\mathsf{T}} d.\n$$\n代入 $d = B z^{k} - b$，我们有\n$$\n\\left(I + \\rho A^{\\mathsf{T}}A\\right) x = c - A^{\\mathsf{T}} y^{k} - \\rho A^{\\mathsf{T}}(B z^{k} - b).\n$$\n在 $\\left(I + \\rho A^{\\mathsf{T}}A\\right)$ 可逆的假设下，唯一的最小化子是\n$$\nx^{k+1} = \\left(I + \\rho A^{\\mathsf{T}}A\\right)^{-1}\\left(c - A^{\\mathsf{T}} y^{k} - \\rho A^{\\mathsf{T}}(B z^{k} - b)\\right).\n$$", "answer": "$$\\boxed{\\left(I+\\rho A^{\\mathsf{T}}A\\right)^{-1}\\left(c-A^{\\mathsf{T}}y^{k}-\\rho A^{\\mathsf{T}}\\left(Bz^{k}-b\\right)\\right)}$$", "id": "2153727"}, {"introduction": "在处理了平滑的目标函数之后，我们来探讨 ADMM 如何优雅地处理硬约束。许多实际问题要求解必须位于某个特定的凸集中，这可以通过指示函数来建模 [@problem_id:2852062]。本练习将揭示 ADMM 在这种情况下的更新步骤如何等价于一个几何上的投影操作，从而加深你对该算法如何利用问题结构的理解。", "problem": "考虑一个离散时间信号重构问题，其中估计值 $x \\in \\mathbb{R}^{n}$ 必须满足已知的凸约束，这些约束模拟了关于可行信号的先验知识。将该重构问题表述为分裂问题\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} \\; f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0,\n$$\n其中 $f:\\mathbb{R}^{n} \\to \\mathbb{R}$ 是一个正常的、闭的、凸函数，编码了信号模型的数据保真度和正则化，且 $g:\\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 是一个非空、闭、凸集 $C \\subset \\mathbb{R}^{n}$ 的指示函数，定义为\n$$\ng(z) = \\begin{cases}\n0,  \\text{if } z \\in C,\\\\\n+\\infty,  \\text{if } z \\notin C.\n\\end{cases}\n$$\n将采用尺度形式的交替方向乘子法（ADMM）来解决此问题。令 $\\rho > 0$ 为惩罚参数，$u \\in \\mathbb{R}^{n}$ 表示尺度对偶变量。尺度增广拉格朗日量为\n$$\n\\mathcal{L}_{\\rho}(x,z,u) = f(x) + g(z) + \\frac{\\rho}{2}\\,\\|x - z + u\\|_{2}^{2} - \\frac{\\rho}{2}\\,\\|u\\|_{2}^{2}.\n$$\n一个点 $v \\in \\mathbb{R}^{n}$ 到集合 $C$ 上的欧几里得投影定义为\n$$\n\\Pi_{C}(v) := \\arg\\min_{z \\in C} \\;\\|z - v\\|_{2}.\n$$\n仅从这些定义出发，推导 ADMM 迭代中 $z$-更新的闭式表达式，\n$$\nz^{k+1} \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ g(z) + \\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2} \\right\\}.\n$$\n将您的最终结果明确表示为一个仿射参数到集合 $C$ 上的欧几里得投影。您的最终答案必须是一个仅包含 $\\Pi_{C}$、$x^{k+1}$ 和 $u^{k}$ 的符号表达式。最终答案中不要包含任何等号。不需要数值近似，也不涉及单位。", "solution": "我们从问题结构和尺度增广拉格朗日量开始。问题是\n$$\n\\min_{x,z} \\; f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0,\n$$\n其中 $g$ 是一个非空、闭、凸集 $C \\subset \\mathbb{R}^{n}$ 的指示函数。在尺度交替方向乘子法（ADMM）中，我们交替地最小化尺度增广拉格朗日量\n$$\n\\mathcal{L}_{\\rho}(x,z,u) = f(x) + g(z) + \\frac{\\rho}{2}\\,\\|x - z + u\\|_{2}^{2} - \\frac{\\rho}{2}\\,\\|u\\|_{2}^{2}\n$$\n关于 $x$ 和 $z$，然后更新 $u$。我们关注 $z$-更新，对于固定的 $x^{k+1}$ 和 $u^{k}$，其表达式为\n$$\nz^{k+1} \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ g(z) + \\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2} \\right\\}.\n$$\n根据指示函数 $g$ 的定义，在 $z \\in \\mathbb{R}^{n}$ 上最小化 $g(z)$ 加上任何其他函数，等价于在约束 $z \\in C$ 下最小化另一个函数。因此，$z$-更新简化为有约束的二次最小化问题\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\frac{\\rho}{2}\\,\\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\n由于 $\\rho > 0$ 是一个正常数，它不改变最小化器的位置。因此，等价地有\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\n我们将范数的平方重写，以将其展示为投影目标：\n$$\n\\|x^{k+1} - z + u^{k}\\|_{2}^{2} = \\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\n因此，\n$$\nz^{k+1} \\in \\arg\\min_{z \\in C} \\;\\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\n根据到非空、闭、凸集 $C$ 上的欧几里得投影的定义，最小化器是点 $x^{k+1} + u^{k}$ 到 $C$ 上的投影。特别地，由于 $C$ 是非空、闭和凸的，投影是唯一确定的，我们有\n$$\nz^{k+1} = \\Pi_{C}\\!\\big(x^{k+1} + u^{k}\\big).\n$$\n这将 $z$-更新以闭式形式表示为仿射参数 $x^{k+1} + u^{k}$ 到凸集 $C$ 上的欧几里得投影，这是直接从指示函数、尺度增广拉格朗日量和欧几里得投影的定义推导出来的。", "answer": "$$\\boxed{\\Pi_{C}\\!\\left(x^{k+1} + u^{k}\\right)}$$", "id": "2852062"}, {"introduction": "算法的理论分析与实际性能之间总有值得探索的关联，惩罚参数 $\\rho$ 的选择对 ADMM 的收敛速度有至关重要的影响。这个综合性练习将引导你从理论上推导收敛因子，并通过编程实现来比较固定 $\\rho$ 与自适应 $\\rho$ 策略的性能 [@problem_id:2852079]。通过这项实践，你将获得宝贵的算法调优经验，真正做到理论与实践相结合。", "problem": "考虑交替方向乘子法 (ADMM)，应用于以下形式的强凸二次一致性问题：最小化 $f(x) + g(z)$，约束条件为 $x - z = 0$，其中 $f(x) = \\tfrac{1}{2} x^{\\mathsf{T}} P x$ 且 $g(z) = \\tfrac{1}{2} z^{\\mathsf{T}} R z$。假设 $P$ 和 $R$ 是对称正定对角矩阵，其对角线元素严格为正。在缩放形式的 ADMM 中进行计算，罚参数为 $\\rho > 0$，缩放对偶变量为 $u$。对此类问题使用 ADMM 的标准更新规则，并定义原始残差 $r^{k} = x^{k} - z^{k}$ 和对偶残差 $s^{k} = \\rho (z^{k} - z^{k-1})$。设初始条件 $z^{0}$ 为适当维度的全1向量，$u^{0}$ 为全0向量；$x^{0}$ 由算法更新隐式定义。所有向量范数均使用欧几里得范数。\n\n你的任务是通过以下方式，在合成的对角二次实例上推导、实现和比较固定 $\\rho$ 和自适应 $\\rho$ 策略：\n- 从第一性原理出发，推导在曲率为 $p > 0$ 和 $r > 0$ 以及固定 $\\rho > 0$ 的单个标量坐标上 ADMM 的线性迭代，然后推广到对角多维情况。据此，计算固定 $\\rho$ 的预测渐近线性收敛因子，该因子为误差动态上诱导的每次迭代的线性映射的谱半径，用 $p$、$r$ 和 $\\rho$ 表示。\n- 实现具有固定 $\\rho$ 的 ADMM，并从残差范数序列中估计观测到的渐近线性收敛因子，其中一个合理的估计是在迭代的最后阶段，比率 $\\|e^{k+1}\\|/\\|e^{k}\\|$ 的中位数，其中 $e^{k}$ 定义为包含 $r^{k}$ 和 $s^{k}$ 的拼接残差向量。\n- 使用带有参数 $\\mu > 1$、$\\tau_{\\mathrm{incr}} > 1$ 和 $\\tau_{\\mathrm{decr}} \\in (0,1)$ 的残差平衡规则，实现一种自适应 $\\rho$ 启发式方法：如果 $\\|r^{k}\\| > \\mu \\|s^{k}\\|$，则设置 $\\rho \\leftarrow \\tau_{\\mathrm{incr}} \\rho$；如果 $\\|s^{k}\\| > \\mu \\|r^{k}\\|$，则设置 $\\rho \\leftarrow \\tau_{\\mathrm{decr}} \\rho$；否则保持 $\\rho$ 不变。当 $\\rho$ 改变时，通过设置 $u \\leftarrow (\\rho_{\\mathrm{old}}/\\rho_{\\mathrm{new}}) u$ 来缩放对偶变量以保持一致性。对于自适应运行，使用 $\\rho$ 的最终值以与固定情况相同的方式计算预测因子，并以与固定情况类似的方式计算观测因子。\n\n推导的基本依据包括强凸二次函数的邻近算子定义、缩放形式的 ADMM 以及二次映射的诱导迭代的线性化。\n\n实现一个单一的程序，该程序：\n- 对下面的每个测试用例，运行固定 $\\rho$ 和自适应 $\\rho$ 的 ADMM，迭代次数恰好为 $K$ 次，从迭代的最后阶段估计观测到的渐近因子，并按所述计算预测因子。\n- 在所有用例中使用以下常量：迭代次数 $K = 400$，残差平衡参数 $\\mu = 10$，$\\tau_{\\mathrm{incr}} = 2$，$\\tau_{\\mathrm{decr}} = 0.5$。\n- 使用误差度量 $\\|e^{k}\\| = \\sqrt{ \\|r^{k}\\|^{2} + \\|s^{k}\\|^{2} }$。\n- 将观测因子估计为最后 100 次迭代中 $\\|e^{k+1}\\|/\\|e^{k}\\|$ 的中位数，不包括分母为 0 的比率。\n- 通过取你在解中推导出的各坐标预测因子的最大值，来计算对角矩阵 $P$ 和 $R$ 的固定 $\\rho$ 预测因子。\n\n测试套件：\n- 用例1：维度3，$P$ 的对角线元素为 $1, 2, 3$，$R$ 的对角线元素为 $1, 2, 4$，固定 $\\rho = 1$。\n- 用例2：维度5，$P$ 的对角线元素为 $10^{-2}, 10^{-1}, 1, 10, 100$，$R$ 的对角线元素为 $100, 10, 1, 10^{-1}, 10^{-2}$，固定 $\\rho = 1$。\n- 用例3：维度3，$P$ 的对角线元素为 $0.5, 5, 50$，$R$ 的对角线元素为 $0.2, 2, 200$，固定 $\\rho = 5$。\n- 用例4：维度3，$P$ 的对角线元素为 $10^{-4}, 2\\cdot 10^{-4}, 5\\cdot 10^{-4}$，$R$ 的对角线元素为 $10^{-4}, 3\\cdot 10^{-4}, 7\\cdot 10^{-4}$，固定 $\\rho = 10^{-3}$。\n\n角度单位不适用。不涉及物理单位。\n\n要求的最终输出格式：\n- 你的程序应产生单行输出，其中包含一个扁平列表，按顺序为每个测试用例提供四个浮点数，顺序如下：预测固定值、观测固定值、预测自适应最终值、观测自适应值。也就是说，输出应是一个用方括号括起来的逗号分隔列表，对于 $N$ 个测试用例，包含 $4N$ 个数字，没有附加文本。例如，对于两个用例，格式将是 $[p_{1},o_{1},p'_{1},o'_{1},p_{2},o_{2},p'_{2},o'_{2}]$，其中 $p_{i}$ 是用例 $i$ 的预测固定值，$o_{i}$ 是观测固定值，$p'_{i}$ 是预测自适应最终值，$o'_{i}$ 是观测自适应值。", "solution": "该问题要求对一个特定的强凸二次一致性问题分析并实现交替方向乘子法 (ADMM)。作为任何严谨科学探究的必要步骤，第一步是验证问题陈述。\n\n步骤1：提取已知条件。\n问题是求解：\n$$ \\min_{x,z} f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0 $$\n其中 $f(x) = \\frac{1}{2} x^{\\mathsf{T}} P x$ 且 $g(z) = \\frac{1}{2} z^{\\mathsf{T}} R z$。\n-   $P$ 和 $R$ 是对称、正定、对角矩阵，其对角线元素严格为正。\n-   算法是缩放形式的ADMM，罚参数为 $\\rho > 0$，缩放对偶变量为 $u$。\n-   初始条件：$z^{0}$ 是全1向量，$u^{0}$ 是全0向量。\n-   原始残差：$r^{k} = x^{k} - z^{k}$。\n-   对偶残差：$s^{k} = \\rho (z^{k} - z^{k-1})$。\n-   误差度量：$\\|e^{k}\\| = \\sqrt{ \\|r^{k}\\|^{2} + \\|s^{k}\\|^{2} }$，使用欧几里得范数。\n-   迭代次数：$K = 400$。\n-   自适应 $\\rho$ 参数：$\\mu = 10$，$\\tau_{\\mathrm{incr}} = 2$，$\\tau_{\\mathrm{decr}} = 0.5$。\n-   自适应 $\\rho$ 规则：如果 $\\|r^{k}\\| > \\mu \\|s^{k}\\|$，则设置 $\\rho \\leftarrow \\tau_{\\mathrm{incr}} \\rho$；如果 $\\|s^{k}\\| > \\mu \\|r^{k}\\|$，则设置 $\\rho \\leftarrow \\tau_{\\mathrm{decr}} \\rho$。\n-   对偶变量缩放：当 $\\rho$ 改变时，$u \\leftarrow (\\rho_{\\mathrm{old}}/\\rho_{\\mathrm{new}}) u$。\n-   观测因子估计：最后100次迭代中 $\\|e^{k+1}\\|/\\|e^{k}\\|$ 的中位数。\n-   测试用例指定了维度、$P$ 和 $R$ 的对角线元素以及初始固定 $\\rho$。\n\n步骤2：使用提取的已知条件进行验证。\n-   **科学基础**：该问题植根于凸优化和数值方法的既定理论。ADMM 是一种标准算法，其在二次规划中的应用是一个经典课题。前提条件在事实上是合理的。\n-   **适定性**：由于 $P$ 和 $R$ 是正定的，目标函数是强凸的。约束是线性的。这保证了唯一解的存在性。算法结构定义明确。\n-   **客观性**：问题以精确的数学语言陈述，没有主观或含糊的术语。\n-   问题是自洽的，为每个测试用例提供了所有必要的数据和参数。约束和条件是一致的。该设置并非无足轻重，需要适当的推导和实现。\n\n步骤3：结论与行动。\n问题有效。我现在将进行推导和求解。\n\n此问题的缩放增广拉格朗日函数是：\n$$ L_{\\rho}(x, z, u) = \\frac{1}{2} x^{\\mathsf{T}} P x + \\frac{1}{2} z^{\\mathsf{T}} R z + \\frac{\\rho}{2} \\|x - z + u\\|^2 - \\frac{\\rho}{2} \\|u\\|^2 $$\nADMM迭代包括三个步骤：\n1.  $x$-最小化：$x^{k+1} = \\arg\\min_x L_{\\rho}(x, z^k, u^k)$\n2.  $z$-最小化：$z^{k+1} = \\arg\\min_z L_{\\rho}(x^{k+1}, z, u^k)$\n3.  对偶更新：$u^{k+1} = u^k + x^{k+1} - z^{k+1}$\n\n我们推导更新步骤的显式形式。\n对于 $x$-更新，我们求解最小化问题：\n$$ x^{k+1} = \\arg\\min_x \\left( \\frac{1}{2} x^{\\mathsf{T}} P x + \\frac{\\rho}{2} \\|x - z^k + u^k\\|^2 \\right) $$\n目标函数是关于 $x$ 的严格凸二次函数。通过将关于 $x$ 的梯度设为零来找到唯一最小值：\n$$ Px + \\rho(x - (z^k - u^k)) = 0 $$\n$$ (P + \\rho I)x = \\rho(z^k - u^k) $$\n$$ x^{k+1} = \\rho (P + \\rho I)^{-1} (z^k - u^k) $$\n对于 $z$-更新，我们求解：\n$$ z^{k+1} = \\arg\\min_z \\left( \\frac{1}{2} z^{\\mathsf{T}} R z + \\frac{\\rho}{2} \\|x^{k+1} - z + u^k\\|^2 \\right) $$\n将关于 $z$ 的梯度设为零：\n$$ Rz - \\rho(x^{k+1} - z + u^k) = 0 $$\n$$ (R + \\rho I)z = \\rho(x^{k+1} + u^k) $$\n$$ z^{k+1} = \\rho (R + \\rho I)^{-1} (x^{k+1} + u^k) $$\n\n由于 $P$ 和 $R$ 是对角矩阵，矩阵 $(P + \\rho I)$ 和 $(R + \\rho I)$ 也是对角矩阵。问题在各个坐标上完全解耦。我们可以通过检查单个标量坐标来分析收敛性。设 $p>0$ 和 $r>0$ 是给定坐标上 $P$ 和 $R$ 的对角线元素。标量更新如下：\n1. $x_i^{k+1} = \\frac{\\rho}{p_i+\\rho} (z_i^k - u_i^k)$\n2. $z_i^{k+1} = \\frac{\\rho}{r_i+\\rho} (x_i^{k+1} + u_i^k)$\n3. $u_i^{k+1} = u_i^k + x_i^{k+1} - z_i^{k+1}$\n\n问题的唯一解是 $x=z=0$，这意味着最优对偶变量 $u$ 也为 $0$。因此，迭代量 $(z_i^k, u_i^k)$ 表示与最优点 $(0,0)$ 的误差。我们可以将此误差的演化表示为线性系统。\n将 $x_i^{k+1}$ 的表达式代入 $z_i^{k+1}$ 的更新中：\n$$ z_i^{k+1} = \\frac{\\rho}{r_i+\\rho} \\left( \\frac{\\rho}{p_i+\\rho} (z_i^k - u_i^k) + u_i^k \\right) = \\frac{\\rho}{r_i+\\rho} \\left( \\frac{\\rho}{p_i+\\rho} z_i^k + \\left(1 - \\frac{\\rho}{p_i+\\rho}\\right) u_i^k \\right) $$\n$$ z_i^{k+1} = \\frac{\\rho}{(r_i+\\rho)(p_i+\\rho)} (\\rho z_i^k + p_i u_i^k) $$\n现在，将 $x_i^{k+1}$ 和 $z_i^{k+1}$ 的表达式代入 $u_i^{k+1}$ 的更新中：\n$$ u_i^{k+1} = u_i^k + \\frac{\\rho}{p_i+\\rho} (z_i^k - u_i^k) - z_i^{k+1} = \\frac{p_i}{p_i+\\rho} u_i^k + \\frac{\\rho}{p_i+\\rho} z_i^k - z_i^{k+1} $$\n$$ u_i^{k+1} = \\frac{p_i}{p_i+\\rho} u_i^k + \\frac{\\rho}{p_i+\\rho} z_i^k - \\frac{\\rho}{(r_i+\\rho)(p_i+\\rho)} (\\rho z_i^k + p_i u_i^k) $$\n$$ u_i^{k+1} = \\left( \\frac{p_i}{p_i+\\rho} - \\frac{\\rho p_i}{(r_i+\\rho)(p_i+\\rho)} \\right) u_i^k + \\left( \\frac{\\rho}{p_i+\\rho} - \\frac{\\rho^2}{(r_i+\\rho)(p_i+\\rho)} \\right) z_i^k $$\n$$ u_i^{k+1} = \\frac{p_i(r_i+\\rho) - \\rho p_i}{(r_i+\\rho)(p_i+\\rho)} u_i^k + \\frac{\\rho(r_i+\\rho) - \\rho^2}{(r_i+\\rho)(p_i+\\rho)} z_i^k $$\n$$ u_i^{k+1} = \\frac{p_i r_i}{(r_i+\\rho)(p_i+\\rho)} u_i^k + \\frac{\\rho r_i}{(r_i+\\rho)(p_i+\\rho)} z_i^k $$\n每个坐标的迭代可以写成矩阵形式：\n$$\n\\begin{pmatrix} z_i^{k+1} \\\\ u_i^{k+1} \\end{pmatrix} =\n\\frac{1}{(p_i+\\rho)(r_i+\\rho)}\n\\begin{pmatrix} \\rho^2  & \\rho p_i \\\\ \\rho r_i & p_i r_i \\end{pmatrix}\n\\begin{pmatrix} z_i^k \\\\ u_i^k \\end{pmatrix}\n$$\n渐近线性收敛因子由此迭代矩阵的谱半径决定，我们将其表示为 $M_i$。矩阵 $M_i$ 的秩为1，因为它可以表示为外积：\n$$ M_i = \\frac{1}{(p_i+\\rho)(r_i+\\rho)} \\begin{pmatrix} \\rho \\\\ r_i \\end{pmatrix} \\begin{pmatrix} \\rho & p_i \\end{pmatrix} $$\n秩为1的矩阵 $ab^{\\mathsf{T}}$ 的特征值是 $b^{\\mathsf{T}}a$ 和具有 $n-1$ 重数的 $0$。这里 $n=2$。非零特征值是：\n$$ \\lambda_i = \\frac{1}{(p_i+\\rho)(r_i+\\rho)} \\begin{pmatrix} \\rho & p_i \\end{pmatrix} \\begin{pmatrix} \\rho \\\\ r_i \\end{pmatrix} = \\frac{\\rho^2 + p_i r_i}{(p_i+\\rho)(r_i+\\rho)} $$\n由于 $p_i, r_i, \\rho > 0$，该特征值为正且小于1。谱半径就是这个值。整个系统的渐近收敛因子是各坐标因子的最大值，因为收敛最慢的坐标决定了整体速率：\n$$ \\kappa_{\\text{predicted}} = \\max_i \\left\\{ \\frac{p_i r_i + \\rho^2}{(p_i+\\rho)(r_i+\\rho)} \\right\\} $$\n此公式将用于计算预测的收敛因子。观测因子是根据误差范数 $\\|e^k\\|$ 序列凭经验估计的。自适应方案调整 $\\rho$ 以平衡原始残差和对偶残差，旨在加快收敛速度，其预测性能使用 $\\rho$ 的最终值进行评估。", "answer": "```python\nimport numpy as np\n\ndef run_admm(p_diag, r_diag, rho_init, n, k_max, adaptive, mu, tau_incr, tau_decr):\n    \"\"\"\n    Runs fixed or adaptive ADMM for the given quadratic problem.\n    \"\"\"\n    # Initial conditions\n    z = np.ones(n)\n    u = np.zeros(n)\n    rho = rho_init\n\n    errors = []\n    \n    # Precompute inverse matrices for speed (they are diagonal)\n    # The solver will update these within the loop if rho is adaptive\n    \n    # Main ADMM loop\n    for _ in range(k_max):\n        # Store z_k for dual residual calculation\n        z_prev = z\n\n        # Precompute denominators for element-wise updates\n        p_plus_rho = p_diag + rho\n        r_plus_rho = r_diag + rho\n\n        # x-update\n        # x_k+1 = rho * (P + rho*I)^-1 * (z_k - u_k)\n        x = (rho / p_plus_rho) * (z - u)\n\n        # z-update\n        # z_k+1 = rho * (R + rho*I)^-1 * (x_k+1 + u_k)\n        z = (rho / r_plus_rho) * (x + u)\n\n        # u-update\n        # u_k+1 = u_k + x_k+1 - z_k+1\n        u = u + x - z\n\n        # Calculate residuals\n        r_k = x - z\n        s_k = rho * (z - z_prev)\n\n        # Calculate error norm\n        norm_r_k = np.linalg.norm(r_k)\n        norm_s_k = np.linalg.norm(s_k)\n        error_norm = np.sqrt(norm_r_k**2 + norm_s_k**2)\n        errors.append(error_norm)\n\n        # Adaptive rho update\n        if adaptive:\n            rho_old = rho\n            if norm_r_k > mu * norm_s_k:\n                rho = tau_incr * rho\n            elif norm_s_k > mu * norm_r_k:\n                rho = tau_decr * rho\n            \n            if rho != rho_old:\n                # Scale dual variable u\n                u = u * (rho_old / rho)\n\n    return errors, rho\n\ndef calculate_predicted_factor(p_diag, r_diag, rho):\n    \"\"\"\n    Calculates the predicted asymptotic convergence factor.\n    \"\"\"\n    factors = (p_diag * r_diag + rho**2) / ((p_diag + rho) * (r_diag + rho))\n    return np.max(factors)\n\ndef estimate_observed_factor(errors, k_max):\n    \"\"\"\n    Estimates the observed asymptotic convergence factor from error norms.\n    \"\"\"\n    if len(errors) < k_max:\n        # Not enough iterations to estimate\n        return np.nan\n\n    # Use the last 100 iterations, which means ratios from the last 100 error updates.\n    # Indices for errors: K-101 to K-1 (e.g., 299 to 399 for K=400)\n    # Ratios e[k]/e[k-1] for k from K-100 to K-1\n    start_index = k_max - 100\n    \n    ratios = []\n    for i in range(start_index, k_max):\n        # errors are indexed from 0 to k_max-1\n        # error[i] is e_{i+1}, error[i-1] is e_{i}\n        if errors[i-1] > 1e-12: # Avoid division by zero\n            ratios.append(errors[i] / errors[i-1])\n\n    if not ratios:\n        return 0.0 # Converged to zero\n        \n    return np.median(ratios)\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and format output.\n    \"\"\"\n    # Global constants\n    K_MAX = 400\n    MU = 10.0\n    TAU_INCR = 2.0\n    TAU_DECR = 0.5\n\n    # Test suite\n    test_cases = [\n        {'n': 3, 'p': [1., 2., 3.], 'r': [1., 2., 4.], 'rho': 1.},\n        {'n': 5, 'p': [1e-2, 1e-1, 1., 10., 100.], 'r': [100., 10., 1., 1e-1, 1e-2], 'rho': 1.},\n        {'n': 3, 'p': [0.5, 5., 50.], 'r': [0.2, 2., 200.], 'rho': 5.},\n        {'n': 3, 'p': [1e-4, 2e-4, 5e-4], 'r': [1e-4, 3e-4, 7e-4], 'rho': 1e-3}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        p_diag = np.array(case['p'])\n        r_diag = np.array(case['r'])\n        rho_init = case['rho']\n        n = case['n']\n\n        # --- Fixed rho run ---\n        errors_fixed, _ = run_admm(p_diag, r_diag, rho_init, n, K_MAX, False, MU, TAU_INCR, TAU_DECR)\n        \n        # Predicted factor for fixed rho\n        predicted_fixed = calculate_predicted_factor(p_diag, r_diag, rho_init)\n        \n        # Observed factor for fixed rho\n        observed_fixed = estimate_observed_factor(errors_fixed, K_MAX)\n        \n        # --- Adaptive rho run ---\n        errors_adaptive, rho_final = run_admm(p_diag, r_diag, rho_init, n, K_MAX, True, MU, TAU_INCR, TAU_DECR)\n        \n        # Predicted factor for adaptive rho using the final rho value\n        predicted_adaptive_final = calculate_predicted_factor(p_diag, r_diag, rho_final)\n\n        # Observed factor for adaptive rho\n        observed_adaptive = estimate_observed_factor(errors_adaptive, K_MAX)\n        \n        results.extend([predicted_fixed, observed_fixed, predicted_adaptive_final, observed_adaptive])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2852079"}]}