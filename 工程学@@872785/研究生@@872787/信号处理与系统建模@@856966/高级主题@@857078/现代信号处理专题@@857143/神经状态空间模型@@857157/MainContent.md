## 引言
神经状态空间模型 (Neural State-space Models, NSSM) 作为一类新兴且功能强大的序列模型，正迅速在机器学习和[科学计算](@entry_id:143987)领域崭露头角。这些模型巧妙地将经典的系统理论与现代[深度学习架构](@entry_id:634549)相结合，通过一个潜在的[状态向量](@entry_id:154607)来捕捉[时间序列数据](@entry_id:262935)中复杂的动态演化过程。传统序列模型，如[循环神经网络 (RNN)](@entry_id:143880)，虽擅长处理动态过程但受困于梯度消失和[顺序计算](@entry_id:273887)瓶颈；而[卷积神经网络](@entry_id:178973) (CNN) 虽能高效并行计算，却在建模连续时间动态和[长程依赖](@entry_id:181727)方面存在局限。NSSM的出现正是为了弥合这一差距，它提供了一个统一的框架，既具备循环模型的动态建模能力，又拥有卷积模型的高效[并行计算](@entry_id:139241)优势。

本文将带领读者系统地穿越神经[状态空间模型](@entry_id:137993)的理论与实践世界。我们将从第一章 **“原理与机制”** 开始，深入剖析构成NSSM的数学基石，从[线性时不变系统](@entry_id:276591)到现代NSSM的高效计算策略和稳定性保证。随后，在第二章 **“应用与跨学科连接”** 中，我们将展示NSSM如何作为一座桥梁，连接机器学习中的序列[表示学习](@entry_id:634436)、不规则数据处理，以及科学与工程中的复杂动态建模、控制和因果推断。最后，在第三章 **“动手实践”** 中，您将通过具体的理论练习，将前面学到的概念应用于系统离散化、[频率响应分析](@entry_id:272367)和约束化模型设计等关键任务中。通过这一系列的学习，您将全面掌握NSSM的核心思想，并理解其为何成为连接数据、理论和应用的强大[范式](@entry_id:161181)。

## 原理与机制

本章在前一章介绍性概述的基础上，深入探讨了构成神经状态空间模型 (Neural State-space Models, NSSM) 核心的数学原理和计算机制。我们将从作为基本构建单元的线性时不变 (Linear Time-Invariant, LTI) 系统出发，系统地阐述其稳定性、[可控性](@entry_id:148402)、[可观测性](@entry_id:152062)等基本属性。随后，我们将探讨[状态空间实现](@entry_id:166670)的可辨识性问题，并介绍如何通过规范型来解决这一问题。接着，我们将分析从连续时间模型到离散时间模型的转换过程，重点关注采样带来的混叠效应。在此基础上，我们将揭示现代NSSM在处理长序列时实现计算效率的关键——即从循环计算到卷积计算的转换，以及结构化矩阵在其中的核心作用。最后，我们将回到[非线性](@entry_id:637147)领域，讨论如何解释学习到的动力学系统，并设计机制来保证[非线性](@entry_id:637147)NSSM的稳定性。

### 线性[状态空间模型](@entry_id:137993)：基本构建单元

所有[状态空间模型](@entry_id:137993)的核心思想都是通过一个潜在的、低维的[状态向量](@entry_id:154607) $x_k$ 来捕捉[序列数据](@entry_id:636380)中的动态信息。在最基础的[LTI系统](@entry_id:271946)中，状态的演化和观测过程由一组[线性方程](@entry_id:151487)描述。对于一个离散时间系统，其动力学由以下[方程组](@entry_id:193238)定义：
$$
x_{k+1} = A x_k + B u_k
$$
$$
y_k = C x_k + D u_k
$$
其中，$k$ 是时间步，$x_k \in \mathbb{R}^n$ 是状态向量，$u_k \in \mathbb{R}^m$ 是输入向量，$y_k \in \mathbb{R}^p$ 是输出向量。矩阵 $A \in \mathbb{R}^{n \times n}$、 $B \in \mathbb{R}^{n \times m}$、 $C \in \mathbb{R}^{p \times n}$ 和 $D \in \mathbb{R}^{p \times m}$ 分别被称为状态矩阵、输入矩阵、输出矩阵和直通矩阵。它们共同定义了系统的动力学特性。

为了理解输入 $u_k$ 如何影响输出 $y_k$，我们可以从零初始状态 $x_0 = 0$ 开始，逐步展开状态方程 [@problem_id:2886171]：
$$
x_1 = B u_0
$$
$$
x_2 = A x_1 + B u_1 = A(B u_0) + B u_1 = AB u_0 + B u_1
$$
$$
x_3 = A x_2 + B u_2 = A(AB u_0 + B u_1) + B u_2 = A^2 B u_0 + AB u_1 + B u_2
$$
通过归纳法，我们可以得到在任意时间步 $k$ 的[状态表](@entry_id:178995)达式：
$$
x_k = \sum_{j=0}^{k-1} A^{k-1-j} B u_j
$$
将此式代入输出方程，我们得到：
$$
y_k = C \left( \sum_{j=0}^{k-1} A^{k-1-j} B u_j \right) + D u_k
$$
这个表达式揭示了一个深刻的联系。如果我们定义一个序列 $h_k$，即系统的**脉冲响应** (impulse response)，它代表了系统在零初始状态下对[单位脉冲](@entry_id:272155)输入（即 $u_0=1$ 且后续输入为零）的响应。根据上述公式，我们可以推导出脉冲响应序列为：
$$
h_k = \begin{cases} D,   k = 0 \\ C A^{k-1} B,  k \ge 1 \end{cases}
$$
于是，输出 $y_k$ 可以被优雅地写成一个**卷积** (convolution) 的形式：
$$
y_k = \sum_{j=0}^{k} h_j u_{k-j} = (h * u)_k
$$
这个结果表明，任何LTI[状态空间模型](@entry_id:137993)都等价于一个卷积滤波器，其滤波器核就是由系统矩阵 $(A, B, C, D)$ 决定的脉冲响应。这一对偶性是现代NSSM高效计算的基石。

### 基本系统属性

为了构建有效且可靠的模型，我们必须理解并控制其基本动态属性，其中最重要的是稳定性、[可控性](@entry_id:148402)和可观测性。

#### 稳定性 (Stability)

稳定性是评价动力学系统最重要的指标，它决定了系统在没有持续外部驱动的情况下是否会回归到平衡状态，以及在有外部驱动时其响应是否会无限增长。

**内部稳定性 (Internal Stability)**，或称[渐近稳定性](@entry_id:149743)，关注的是系统在无输入 ($u_k \equiv 0$) 情况下的自身行为。如果对于任意初始状态 $x_0$，状态向量 $x_k$ 都会随着时间 $k \to \infty$ 而收敛到零（即 $x_k \to 0$），则称系统是内部稳定的。对于[LTI系统](@entry_id:271946) $x_{k+1}=Ax_k$，其解为 $x_k = A^k x_0$。该系统内部稳定的充分必要条件是状态矩阵 $A$ 的**[谱半径](@entry_id:138984)** (spectral radius) 小于1，即 $\rho(A)  1$。[谱半径](@entry_id:138984)定义为[矩阵特征值](@entry_id:156365)的[最大模](@entry_id:195246)，$\rho(A) = \max_i |\lambda_i(A)|$。当此条件满足时，矩阵的幂 $A^k$ 将指数级衰减至零 [@problem_id:2886065]。

**有界输入有界输出 (BIBO) 稳定性 (Bounded-Input Bounded-Output Stability)** 则从外部输入输出的角度来定义稳定性。如果对于任何有界的输入序列 $\{u_k\}$（即 $\sup_k \|u_k\|  \infty$），在零初始状态下，输出序列 $\{y_k\}$ 也总是有界的（即 $\sup_k \|y_k\|  \infty$），则称系统是BIBO稳定的。对于[LTI系统](@entry_id:271946)，[BIBO稳定性](@entry_id:157773)等价于其脉冲响应序列 $\{h_k\}$ 是**绝对可和的** (absolutely summable)，即 $\sum_{k=0}^{\infty} \|h_k\|  \infty$。

这两种稳定性之间存在明确的联系：内部稳定性是比[BIBO稳定性](@entry_id:157773)更强的条件。如果一个系统是内部稳定的（$\rho(A)  1$），那么它的脉冲响应 $h_k = CA^{k-1}B$ 必然会指数衰减，从而保证绝对可和，因此系统也一定是BIBO稳定的。然而，反之不尽然。一个系统可能BIBO稳定但内部不稳定。这种情况发生在该系统存在“隐藏”的不稳定模式时，我们将在下一节讨论。

#### [可控性与可观测性](@entry_id:174003) (Controllability and Observability)

**[可控性](@entry_id:148402) (Controllability)** 指的是我们能否通过设计输入序列 $\{u_k\}$，在有限时间内将系统状态从任意初始状态 $x_0$ 驱动到任意期望的最终状态 $x_f$ [@problem_id:2886054]。直观上，这意味着输入 $u_k$ 对系统的所有状态动态都有影响力。一个[LTI系统](@entry_id:271946) $(A,B)$ 是可控的，当且仅当其**[可控性矩阵](@entry_id:271824)** (controllability matrix) 是满秩的：
$$
\mathcal{C} = \begin{bmatrix} B  AB  A^2B  \cdots  A^{n-1}B \end{bmatrix} \in \mathbb{R}^{n \times nm}
$$
$$
\text{rank}(\mathcal{C}) = n
$$
这个测试条件，即卡尔曼[可控性](@entry_id:148402)秩条件，保证了由矩阵 $A^k B$ 的列向量所张成的空间能够覆盖整个[状态空间](@entry_id:177074) $\mathbb{R}^n$。

**可观测性 (Observability)** 与可控性对偶，指的是我们能否通过观测有限时间内的输出序列 $\{y_k\}$（在无输入的情况下），唯一地确定系统的初始状态 $x_0$ [@problem_id:2886054]。直观上，这意味着[状态向量](@entry_id:154607) $x_k$ 的所有动态变化都能在输出 $y_k$ 中有所体现。一个[LTI系统](@entry_id:271946) $(A,C)$ 是可观测的，当且仅当其**[可观测性矩阵](@entry_id:165052)** (observability matrix) 是满秩的：
$$
\mathcal{O} = \begin{bmatrix} C \\ CA \\ CA^2 \\ \vdots \\ CA^{n-1} \end{bmatrix} \in \mathbb{R}^{np \times n}
$$
$$
\text{rank}(\mathcal{O}) = n
$$
如果一个[状态空间实现](@entry_id:166670)既是可控的又是可观测的，我们称之为**[最小实现](@entry_id:176932) (minimal realization)**。[最小实现](@entry_id:176932)具有重要的理论意义：它确保了状态空间的维度 $n$ 是描述系统输入输出行为所需的最小维度。在[最小实现](@entry_id:176932)中，不存在无法被输入影响或无法被输出观测到的“隐藏”动态。这解释了之前提到的稳定性问题：如果一个系统是最小的，那么[BIBO稳定性](@entry_id:157773)和内部稳定性是等价的。如果系统非最小，可能存在一个不稳定的内部模式（$A$ 的某个[特征值](@entry_id:154894) $|\lambda| \ge 1$），但该模式恰好是不可控或不可观测的。在这种情况下，该不稳定模式不会出现在脉冲响应中，系统可能表现出BIBO稳定，但其内部状态可能会发散 [@problem_id:2886065]。

### [可辨识性](@entry_id:194150)与规范型

在从数据中学习[状态空间模型](@entry_id:137993)参数 $(A,B,C,D)$ 时，一个核心问题是**可辨识性 (identifiability)**：对于给定的输入输出行为，参数解是否唯一？对于[LTI系统](@entry_id:271946)，答案是否定的。

考虑一个[状态空间实现](@entry_id:166670) $(A,B,C,D)$ 和任意一个可逆矩阵 $T \in \mathbb{R}^{n \times n}$。我们可以通过 $T$ 进行**相似性变换 (similarity transform)**，定义一个新的状态 $\tilde{x} = T x$。在新[坐标系](@entry_id:156346)下，系统方程变为：
$$
\tilde{x}_{k+1} = (T A T^{-1}) \tilde{x}_k + (T B) u_k = \tilde{A} \tilde{x}_k + \tilde{B} u_k
$$
$$
y_k = (C T^{-1}) \tilde{x}_k + D u_k = \tilde{C} \tilde{x}_k + \tilde{D} u_k
$$
我们得到一个新的实现 $(\tilde{A}, \tilde{B}, \tilde{C}, \tilde{D})$。尽管这个新实现的矩阵不同，但它的输入输出行为与原系统完全相同。我们可以通过检验其[传递函数](@entry_id:273897) $G(z) = C(zI-A)^{-1}B+D$ 来证明这一点 [@problem_id:2885996]：
$$
\tilde{G}(z) = \tilde{C}(zI - \tilde{A})^{-1}\tilde{B} + \tilde{D} = (C T^{-1})(zI - T A T^{-1})^{-1}(T B) + D
$$
$$
= C T^{-1} [T(zI-A)T^{-1}]^{-1} T B + D = C T^{-1} [T^{-1}(zI-A)^{-1}T] T B + D = G(z)
$$
由于存在无限多个可逆矩阵 $T$，任何一个[状态空间实现](@entry_id:166670)都对应着一个由相似性变换连接起来的无限大的等价类。所有这些实现都具有完全相同的输入输出特性。因此，仅从输入输出数据出发，我们无法唯一确定 $(A,B,C)$。

这一根本性的模糊性给模型训练带来了问题。为了解决这个问题，我们需要在每个[等价类](@entry_id:156032)中选择一个唯一的代表。这就是**规范型 (canonical form)** 的作用。通过对矩阵 $(A,B,C)$ 施加特定的结构性约束，我们可以消除相似性变换带来的自由度。对于SISO（单输入单输出）最小系统，一个常用的规范型是**可控规范型 (controllable canonical form)**。该形式将状态矩阵 $A$ 固定为一个特定的**友矩阵 (companion matrix)**，其系数由系统[传递函数](@entry_id:273897)的分母多项式唯一确定；输入矩阵 $B$ 则被固定为一个特定的[基向量](@entry_id:199546)（如 $[0, \dots, 0, 1]^\top$）。在这种约束下，输出矩阵 $C$ 的元素就由[传递函数](@entry_id:273897)的分子系数唯一确定。通过强制模型参数满足这种规范型结构，我们为每个输入输出行为指定了唯一的[参数表示](@entry_id:173803)，从而解决了非唯一性问题 [@problem_id:2885996]。

### 从连续时间到离散时间：[采样与混叠](@entry_id:268188)

许多物理和生物过程本质上是连续的，因此用常微分方程 (ODE) 来描述它们的动态更为自然。一个[连续时间LTI系统](@entry_id:267448)可以表示为：
$$
\dot{x}(t) = A_c x(t) + B_c u(t)
$$
然而，在[数字计算](@entry_id:186530)机上处理数据时，我们通常处理的是离散的采样点。因此，我们需要将连续时间模型转换为等效的离散时间模型。一个常见的假设是**零阶保持 (Zero-Order Hold, ZOH)**，即输入 $u(t)$ 在每个采样间隔 $[k\Delta, (k+1)\Delta)$ 内保持为一个常数 $u_k$。

在这种情况下，我们可以精确地[求解ODE](@entry_id:145499)，得到从时间 $k\Delta$到 $(k+1)\Delta$ 的状态转移关系 [@problem_id:2886203]：
$$
x((k+1)\Delta) = e^{A_c \Delta} x(k\Delta) + \left( \int_0^\Delta e^{A_c \tau} d\tau \right) B_c u_k
$$
这给出了一个等效的离散时间LTI模型 $x_{k+1} = F x_k + G u_k$，其中：
$$
F = e^{A_c \Delta}
$$
$$
G = (A_c^{-1}(e^{A_c \Delta} - I)) B_c \quad (\text{若 } A_c \text{ 可逆})
$$
这种从 $(A_c, B_c)$ 到 $(F, G)$ 的转换过程称为**精确离散化**。其中最关键的关系是 $F = e^{A_c \Delta}$，它通过矩阵指数函数将连续时间动力学与离散时间动力学联系起来。

这个关系引出了一个基本的**极点映射 (pole mapping)** 原理：如果 $\lambda$ 是连续时间矩阵 $A_c$ 的一个[特征值](@entry_id:154894)（即系统的一个极点），那么 $e^{\lambda \Delta}$ 就是离散时间矩阵 $F$ 的一个[特征值](@entry_id:154894)（极点）[@problem_id:2886203]。这个映射揭示了采样过程的几个重要后果：
1.  **稳定性映射**：[连续时间系统](@entry_id:276553)的稳定区域是复平面的左半部分，即 $\text{Re}(\lambda)  0$。通过映射 $z = e^{\lambda \Delta}$，这个区域被映射到离散时间系统的[稳定区域](@entry_id:166035)——[单位圆](@entry_id:267290)内部，即 $|z|  1$。因为如果 $\text{Re}(\lambda)  0$，则 $|e^{\lambda \Delta}| = |e^{(\text{Re}(\lambda) + j\text{Im}(\lambda))\Delta}| = e^{\text{Re}(\lambda)\Delta}  1$。
2.  **混叠 (Aliasing)**：指数函数 $e^z$ 是周期性的，其虚部周期为 $2\pi j$。这意味着 $e^{j\omega\Delta} = e^{j(\omega\Delta + 2\pi m)}$ 对于任何整数 $m$ 都成立。因此，连续时间下的不同振荡频率 $\omega$ 和 $\omega' = \omega + 2\pi m / \Delta$ 在采样后会映射到完全相同的离散时间极点。这意味着，仅从离散的采样数据中，我们无法唯一地确定原始的连续时间频率。任何高于**[奈奎斯特频率](@entry_id:276417)** ($f_s/2 = 1/(2\Delta)$) 的频率分量都会“混叠”到较低的频率范围内，使得原始信号的恢复变得不可能，除非我们有关于信号带宽的先验知识 [@problem_id:2886203]。

需要注意的是，精确离散化不同于像前向欧拉法 ($x_{k+1} = (I + A_c\Delta)x_k$) 这样的[数值近似方法](@entry_id:169303)。前向欧拉法可能在 $\Delta$ 较大时引入不稳定性，即使原始连续系统是稳定的 [@problem_id:2886203]。

### 长序列的高效计算

传统上，状态空间模型的计算是通过**循环 (recurrent)** 方式进行的，即一步一步地更新状态 $x_{k+1} = A x_k + B u_k$。这种方法的计算复杂度与序列长度 $N$ 呈线性关系，即 $O(N \cdot \text{cost}(A))$。然而，它的一个主要缺点是其固有的**顺序性 (sequentiality)**，这使得它难以在现代并行硬件（如GPU）上高效利用。对于训练过程中的[反向传播](@entry_id:199535)，需要存储所有中间状态，导致内存消耗也是 $O(N)$ [@problem_id:2886140]。

现代NSSM架构的核心突破在于利用了我们在本章开头推导出的**卷积表示 (convolutional representation)**。由于[LTI系统](@entry_id:271946)的输出是输入与脉冲响应的卷积 ($y = h * u$)，我们可以采用一种完全不同的计算策略 [@problem_id:2886130]：
1.  **预计算卷积核**：给定模型参数 $(A,B,C,D)$，首先一次性地计算出长度为 $N$ 的脉冲响应序列（即[卷积核](@entry_id:635097)）$h_{0:N-1}$。
2.  **执行卷积**：利用**快速傅里叶变换 (Fast Fourier Transform, FFT)**，可以在 $O(N \log N)$ 的时间内计算输入序列 $u_{0:N-1}$ 与[卷积核](@entry_id:635097) $h_{0:N-1}$ 之间的[线性卷积](@entry_id:190500)。根据[卷积定理](@entry_id:264711)，时域的卷积等价于[频域](@entry_id:160070)的乘积：$y = \text{IFFT}(\text{FFT}(u) \odot \text{FFT}(h))$。

这种基于卷积的方法具有几个显著优势：
- **并行性**：FFT是一个高度可并行的算法，非常适合在GPU上运行，从而极大地加速了长序列的训练。
- **[计算效率](@entry_id:270255)**：当序列长度 $N$ 足够大时，$O(N \log N)$ 的复杂度优于循环方法在并行硬件上的实际表现。存在一个临界长度 $N_\star$，超过该长度，卷积方法在时间和内存效率上都更具优势 [@problem_id:2886140]。
- **高效的梯度计算**：[反向传播](@entry_id:199535)的计算同样可以被构造成卷积（实际上是[互相关](@entry_id:143353)），因此也可以通过FFT在 $O(N \log N)$ 时间内完成。具体来说，损失函数对卷积核的梯度 $\bar{h}$ 是输入 $u$ 和输出的伴随（adjoint）$\bar{y}$ 之间的互相关 [@problem_id:2886130]。

对于多输入多输出 (MIMO) 系统，这一思想同样适用。此时，脉冲响应是一个形状为 $p \times m \times N$ 的张量，而计算过程则对应于一个多通道的一维卷积 [@problem_id:2886130]。

这种循环与卷积的对偶性，使得NSSM既能像RNN一样进行高效的自回归生成（使用循环模式），又能像卷积网络一样进行高效的并行训练（使用卷积模式）。

### 结构化[状态空间](@entry_id:177074)矩阵

上述高效计算策略的可行性，取决于我们能否快速地计算脉冲响应 $h_k = C A^{k-1} B$。对于一个一般的[稠密矩阵](@entry_id:174457) $A$，计算其幂 $A^k$ 的成本是高昂的（$O(n^3)$），这会使预计算卷积核的步骤变得不可行。因此，现代NSSM的关键创新在于为状态矩阵 $A$（有时也包括 $B$ 和 $C$）设计特定的**结构 (structure)**，以实现近乎线性的计算复杂度 [@problem_id:2886004]。

两种主流的结构化方法是：

1.  **结构化对角化 (Structured Diagonalizable)**：这类方法假设 $A = Q \Lambda Q^{-1}$，其中 $\Lambda$ 是对角矩阵，而 $Q$ 是一个具有快速变换算法的矩阵。例如，如果 $Q$ 是[离散傅里叶变换](@entry_id:144032) (DFT) 矩阵，那么矩阵-向量乘法 $Qv$ 和 $Q^{-1}v$ 就可以通过FFT和IFFT在 $O(n \log n)$ 时间内完成。在这种结构下，计算矩阵的幂或指数变得非常高效：
    $$
    A^k = Q \Lambda^k Q^{-1}, \quad e^{A\Delta} = Q e^{\Lambda\Delta} Q^{-1}
    $$
    由于 $\Lambda$ 是对角矩阵，$\Lambda^k$ 和 $e^{\Lambda\Delta}$ 的计算只是元素级别的运算，成本为 $O(n)$。因此，计算 $h_k = C A^{k-1} B$ 或 $F = e^{A_c\Delta}$ 的动作都可以在 $O(n \log n)$ 时间内完成，从而实现了对整个卷积核的高效构建 [@problem_id:2886004]。

2.  **对角加低秩 (Diagonal-plus-low-rank, DPLR)**：这类方法将 $A$ 参数化为 $A = D + UW^\top$，其中 $D$ 是[对角矩阵](@entry_id:637782)，$U, W \in \mathbb{C}^{n \times r}$ 是两个瘦长的矩阵，且秩 $r \ll n$。这种结构的优点在于：
    - **快速矩阵-向量乘法**：计算 $Ax = Dx + U(W^\top x)$ 的成本为 $O(n) + O(nr) + O(nr) = O(nr)$，远低于[稠密矩阵](@entry_id:174457)的 $O(n^2)$。
    - **快速[求解线性系统](@entry_id:146035)**：对于形如 $(I - zA)y=b$ 的线性方程组（这在隐式积分方法或计算频率响应时非常常见），可以利用**Sherman-Morrison-Woodbury (SMW) 公式**来高效求解。SMW公式可以将对一个大的 $n \times n$ DPLR矩阵求逆的问题，转化为对一个小的 $r \times r$ [矩阵求逆](@entry_id:636005)，从而将求解成本从 $O(n^3)$ 大幅降低到 $O(nr^2)$ 或更低 [@problem_id:2886004]。

这些结构化参数化是NSSM能够扩展到大状态维度 $n$ 和长序列 $N$ 的秘诀，它们在保持模型[表达能力](@entry_id:149863)的同时，实现了卓越的[计算效率](@entry_id:270255)。

### 解释动力学与保证[非线性模型](@entry_id:276864)稳定性

在构建了高效的NSSM之后，我们还需要理解模型学到了什么，并确保其行为是稳定和可预测的。

#### 动力学解释

[LTI系统](@entry_id:271946)的[谱理论](@entry_id:275351)为我们提供了解读所学动力学的强大工具。对于一个（可对角化的）[连续时间系统](@entry_id:276553) $\dot{x} = A_c x$，其状态矩阵 $A_c$ 的[特征值](@entry_id:154894) $\lambda_i$ 和[特征向量](@entry_id:151813) $v_i$ 具有明确的物理意义 [@problem_id:2886154]：
- **[特征值](@entry_id:154894) $\lambda_i = \alpha_i + j\omega_i$** 定义了系统的**时间模式 (temporal modes)**。实部 $\alpha_i$ 决定了模式的增长或衰减速率（$\alpha_i  0$ 为衰减，$\alpha_i > 0$ 为增长），而虚部 $\omega_i$ 决定了模式的[振荡频率](@entry_id:269468)。
- **右[特征向量](@entry_id:151813) $v_i$** 是状态空间中的一个方向。当系统状态沿着这个方向演化时，其动态完全由对应的[特征值](@entry_id:154894) $\lambda_i$ 支配。
- **空间模式 (Spatial Patterns)**：[特征向量](@entry_id:151813) $v_i$ 本身存在于潜在的状态空间中，我们无法直接观察到。我们能观察到的是它通过输出矩阵 $C$ 投影到输出空间后的模式，即 $C v_i$。这个向量 $C v_i \in \mathbb{R}^p$ 描述了第 $i$ 个时间模式在多维输出上的空间分布或形态。

一个模式要在输出中可见，它必须既是**可观测的** ($C v_i \neq 0$)，又是被初始条件**激发的** (excited)。后者由初始状态 $x_0$ 在对应左[特征向量](@entry_id:151813) $w_i$ 上的投影 $w_i^\top x_0$ 是否为零决定。如果投影为零，即使该模式可观测，它也不会出现在[零输入响应](@entry_id:274925)中 [@problem_id:2886154]。

#### [非线性模型](@entry_id:276864)的稳定性

当我们将线性状态更新替换为[非线性](@entry_id:637147)函数 $x_{k+1} = f_\theta(x_k, u_k)$ 时（例如一个[神经网](@entry_id:276355)络），稳定性就不再是理所当然的了。一个微小的扰动或输入变化可能导致状态轨迹的发散。

为了控制这种行为，我们引入**增量稳定性 (incremental stability)** 的概念，它要求系统对于相同的输入序列，任意两个不同的初始状态所产生的轨迹最终会相互收敛。一个保证增量稳定性的强大条件是使状态[转移函数](@entry_id:273897) $f_\theta(\cdot, u)$ 成为一个**压缩映射 (contraction mapping)** [@problem_id:2886062]。这意味着存在一个常数 $\gamma \in [0, 1)$，使得对于任意状态 $x^{(1)}, x^{(2)}$ 和任意输入 $u$，都满足：
$$
\| f_\theta(x^{(1)}, u) - f_\theta(x^{(2)}, u) \| \le \gamma \| x^{(1)} - x^{(2)} \|
$$
如果这个条件成立，那么任意两条轨迹之间的距离将以 $\gamma^k$ 的速率指数衰减。

根据[向量值函数](@entry_id:261164)的均值定理，我们可以推导出保证压缩映射的一个充分条件：状态[转移函数](@entry_id:273897)关于状态 $x$ 的**雅可比矩阵 (Jacobian matrix)** $J_x f_\theta(x,u) = \frac{\partial f_\theta}{\partial x}(x,u)$ 的某个[诱导矩阵范数](@entry_id:636174) (induced matrix norm) 被一致地（uniformly）小于1的常数所界定，即 $\sup_{(x,u)} \|J_x f_\theta(x,u)\| \le \gamma  1$。

这一结论为我们提供了一种在训练过程中促进稳定性的实用方法：通过**正则化 (regularization)** 来惩罚[雅可比矩阵](@entry_id:264467)的范数。例如，我们可以向损失函数中添加一项，惩罚[雅可比矩阵](@entry_id:264467)的[谱范数](@entry_id:143091)（$\|\cdot\|_2$）或[1-范数](@entry_id:635854)（$\|\cdot\|_1$）使其小于一个目标值 $\rho \in (0,1)$。由于[谱范数](@entry_id:143091)小于等于[弗罗贝尼乌斯范数](@entry_id:143384)（$\|\cdot\|_F$），惩罚后者也是一种有效（但更严格）的策略。然而，仅仅约束[谱半径](@entry_id:138984)或[行列式](@entry_id:142978)是不足够的，因为一个谱半径小于1的矩阵其[诱导范数](@entry_id:163775)可能大于1，从而无法保证压缩性质 [@problem_id:2886062]。通过这种方式，我们可以在利用[神经网](@entry_id:276355)络强大[表达能力](@entry_id:149863)的同时，引导模型学习到稳定且可预测的动态行为。