{"hands_on_practices": [{"introduction": "投影梯度下降法（Projected Gradient Descent, PGD）是求解约束优化问题的基本方法之一，它将标准的梯度步与投影到可行集的步骤相结合。本练习 [@problem_id:2861550] 旨在通过一个具体的信号处理问题——能量约束下的最小二乘问题，引导你从第一性原理出发，完整推导PGD算法的更新规则。完成此练习将帮助你深入理解梯度计算、利普希茨连续性以及如何通过卡罗需-库恩-塔克（KKT）条件推导投影算子，这对于掌握约束优化至关重要。", "problem": "考虑欧几里得信号模型中的约束最小二乘问题：在信号能量预算的约束下，最小化残差能量，\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{2} \\le R,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是已知的感知矩阵，$b \\in \\mathbb{R}^{m}$ 是观测数据，$R \\in \\mathbb{R}_{++}$ 是一个给定的半径。令 $f(x) \\triangleq \\|A x - b\\|_{2}^{2}$。从凸分析、可微性和 Karush–Kuhn–Tucker（KKT）条件的基本原理出发，不要假设任何预先推导的优化公式。\n\n- 使用凸性和梯度利普希茨连续性的定义，推导梯度 $\\nabla f(x)$ 以及 $\\nabla f$ 的一个有效的全局利普希茨常数 $L$，并用 $A$ 的谱范数表示。\n\n- 构建投影梯度下降 (PGD) 方法，该方法定义为梯度下降后投影到可行集上，步长为常数 $t \\in (0, 1/L]$。用在闭 $\\ell_{2}$-球 $\\mathbb{B}_{2}(R) \\triangleq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{2} \\le R\\}$ 上的欧几里得投影 $\\Pi_{\\mathbb{B}_{2}(R)}(\\cdot)$ 来表示 PGD 迭代。\n\n- 从欧几里得投影的定义和应用于范数球约束的 KKT 条件出发，推导任意 $y \\in \\mathbb{R}^{n}$ 的 $\\Pi_{\\mathbb{B}_{2}(R)}(y)$ 的闭式表达式。你的推导必须从定义投影的优化问题开始，并通过平稳性、互补松弛性和可行性条件进行。\n\n- 结合你的结果，给出一个 PGD 更新 $x^{k+1}$ 的单一闭式解析表达式（无分情况讨论），用 $A$、$b$、$R$、$t$ 和 $x^{k}$ 表示。你的最终更新只应使用标准算术运算、欧几里得范数以及 $\\max$ 或 $\\min$ 算子。将此显式表达式作为最终答案。最终答案中不要包含等号。无需进行数值计算。", "solution": "所给问题是一个标准的约束二次规划问题，是信号处理和优化理论的基础。该问题是适定的，有科学依据，并包含唯一解所需的所有信息。因此，该问题是有效的。我们按照要求，从基本原理开始进行推导。\n\n目标函数为 $f(x) \\triangleq \\|A x - b\\|_{2}^{2}$，其中 $x \\in \\mathbb{R}^{n}$，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$。约束为 $\\|x\\|_{2} \\le R$，其中 $R > 0$。\n\n首先，我们推导 $f(x)$ 的梯度及其利普希茨常数。\n该函数可以写成 $f(x) = (A x - b)^{\\top}(A x - b) = x^{\\top}A^{\\top}Ax - 2b^{\\top}Ax + b^{\\top}b$。\n为了求梯度，我们考虑一个微小扰动 $h \\in \\mathbb{R}^{n}$：\n$$\n\\begin{aligned}\nf(x+h) = (A(x+h) - b)^{\\top}(A(x+h) - b) \\\\\n= ((Ax - b) + Ah)^{\\top}((Ax - b) + Ah) \\\\\n= (Ax - b)^{\\top}(Ax - b) + 2(Ax - b)^{\\top}(Ah) + (Ah)^{\\top}(Ah) \\\\\n= f(x) + 2(A^{\\top}(Ax - b))^{\\top}h + \\|Ah\\|_{2}^{2}\n\\end{aligned}\n$$\n$f(x+h)$ 的一阶近似是 $f(x) + \\langle \\nabla f(x), h \\rangle$。通过比较，梯度是满足此关系的向量。因此，我们确定梯度为：\n$$\n\\nabla f(x) = 2 A^{\\top}(A x - b)\n$$\n接下来，我们求这个梯度的全局利普希茨常数 $L$。如果存在一个常数 $L$，使得对于任意 $x, y \\in \\mathbb{R}^{n}$，不等式 $\\|\\nabla f(x) - \\nabla f(y)\\|_{2} \\le L \\|x - y\\|_{2}$ 均成立，则梯度 $\\nabla f(x)$ 是利普希茨连续的。\n我们来计算这个差值：\n$$\n\\begin{aligned}\n\\nabla f(x) - \\nabla f(y) = \\left(2 A^{\\top}(A x - b)\\right) - \\left(2 A^{\\top}(A y - b)\\right) \\\\\n= 2 A^{\\top}A x - 2 A^{\\top}b - 2 A^{\\top}A y + 2 A^{\\top}b \\\\\n= 2 A^{\\top}A (x - y)\n\\end{aligned}\n$$\n现在我们对两边取欧几里得范数：\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_{2} = \\|2 A^{\\top}A (x - y)\\|_{2}\n$$\n根据诱导矩阵范数（特别是谱范数 $\\|\\cdot\\|_{2}$）的定义，我们有 $\\|M z\\|_{2} \\le \\|M\\|_{2} \\|z\\|_{2}$。应用此不等式：\n$$\n\\|2 A^{\\top}A (x - y)\\|_{2} \\le 2 \\|A^{\\top}A\\|_{2} \\|x - y\\|_{2}\n$$\n谱范数的一个性质是 $\\|A^{\\top}A\\|_{2} = \\|A\\|_{2}^{2}$。因此，\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_{2} \\le 2 \\|A\\|_{2}^{2} \\|x - y\\|_{2}\n$$\n从这个不等式，我们可以确定 $\\nabla f(x)$ 的一个有效的全局利普希茨常数为 $L = 2 \\|A\\|_{2}^{2}$。\n\n第二，我们构建投影梯度下降 (PGD) 迭代。PGD 包括一个标准的梯度下降步骤，然后投影到可行集上。给定步长 $t \\in (0, 1/L]$，更新规则如下：\n1. 梯度下降步：$y^{k+1} = x^{k} - t \\nabla f(x^{k})$。\n2. 投影步：$x^{k+1} = \\Pi_{\\mathbb{B}_{2}(R)}(y^{k+1})$。\n这里，$\\mathbb{B}_{2}(R) = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{2} \\le R\\}$ 是半径为 $R$ 的闭欧几里得球。代入梯度的表达式，迭代过程为：\n$$\nx^{k+1} = \\Pi_{\\mathbb{B}_{2}(R)} \\left( x^{k} - 2t A^{\\top}(A x^{k} - b) \\right)\n$$\n\n第三，我们推导任意点 $y \\in \\mathbb{R}^{n}$ 的欧几里得投影 $\\Pi_{\\mathbb{B}_{2}(R)}(y)$ 的闭式表达式。该投影被定义为以下凸优化问题的解：\n$$\n\\Pi_{\\mathbb{B}_{2}(R)}(y) = \\arg \\min_{z \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|z - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|z\\|_{2} \\le R\n$$\n约束可以写成 $g(z) = \\|z\\|_{2}^{2} - R^{2} \\le 0$。这是一个凸问题，所以 Karush–Kuhn–Tucker (KKT) 条件是最优性的充要条件。拉格朗日函数为：\n$$\n\\mathcal{L}(z, \\lambda) = \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda (\\|z\\|_{2}^{2} - R^{2})\n$$\n其中 $\\lambda \\ge 0$ 是拉格朗日乘子。KKT 条件是：\n1.  **平稳性**：$\\nabla_z \\mathcal{L}(z, \\lambda) = 0$。\n    $$\n    \\nabla_z \\left( \\frac{1}{2} (z^{\\top}z - 2y^{\\top}z + y^{\\top}y) + \\lambda (z^{\\top}z - R^{2}) \\right) = z - y + 2\\lambda z = 0\n    $$\n    这意味着 $(1 + 2\\lambda)z = y$，或 $z = \\frac{y}{1 + 2\\lambda}$。\n2.  **原始可行性**：$\\|z\\|_{2}^{2} - R^{2} \\le 0$。\n3.  **对偶可行性**：$\\lambda \\ge 0$。\n4.  **互补松弛性**：$\\lambda (\\|z\\|_{2}^{2} - R^{2}) = 0$。\n\n我们分析互补松弛性条件。有两种情况：\n情况 1：$\\|y\\|_{2} \\le R$。我们可以检验 $\\lambda = 0$ 是否提供了一个有效的解。如果 $\\lambda = 0$，平稳性条件给出 $z = y$。原始可行性得到满足，因为 $\\|z\\|_{2} = \\|y\\|_{2} \\le R$。对偶可行性（$\\lambda \\ge 0$）得到满足。互补松弛性也得到满足，因为 $\\lambda=0$。因此，如果 $y$ 已经在球内或球的边界上，其投影就是 $y$ 本身。\n\n情况 2：$\\|y\\|_{2} > R$。在这种情况下，解 $z = y$ 是不可行的。因此，约束必须是激活的，即 $\\|z\\|_{2} = R$。根据互补松弛性，这意味着 $\\lambda > 0$。从平稳性条件，我们有 $z = y / (1 + 2\\lambda)$。取其范数：\n$$\n\\|z\\|_{2} = \\left\\| \\frac{y}{1 + 2\\lambda} \\right\\|_{2} = \\frac{\\|y\\|_{2}}{1 + 2\\lambda}\n$$\n由于 $\\|z\\|_{2} = R$，我们有 $R = \\frac{\\|y\\|_{2}}{1 + 2\\lambda}$，这给出 $1 + 2\\lambda = \\frac{\\|y\\|_{2}}{R}$。将此代回 $z$ 的表达式中：\n$$\nz = \\frac{y}{\\|y\\|_{2} / R} = R \\frac{y}{\\|y\\|_{2}}\n$$\n这个解对应于将点 $y$ 径向缩放回球的边界上。\n\n结合这两种情况，投影算子为：\n$$\n\\Pi_{\\mathbb{B}_{2}(R)}(y) = \\begin{cases} y  \\text{if } \\|y\\|_{2} \\le R \\\\ R \\frac{y}{\\|y\\|_{2}}  \\text{if } \\|y\\|_{2} > R \\end{cases}\n$$\n这可以用 $\\max$ 算子写成一个单一的闭式表达式，即使对于 $y=0$（因为 $R0$）也是良定义的：\n$$\n\\Pi_{\\mathbb{B}_{2}(R)}(y) = y \\frac{R}{\\max(R, \\|y\\|_{2})}\n$$\n\n最后，我们结合所有结果，得到完整的 PGD 更新表达式。令 $y^{k+1} = x^{k} - 2t A^{\\top}(A x^{k} - b)$。下一个迭代点 $x^{k+1}$ 是 $y^{k+1}$ 在 $\\mathbb{B}_{2}(R)$ 上的投影：\n$$\nx^{k+1} = \\Pi_{\\mathbb{B}_{2}(R)}(y^{k+1}) = y^{k+1} \\frac{R}{\\max(R, \\|y^{k+1}\\|_{2})}\n$$\n代入 $y^{k+1}$ 的表达式，我们得到最终的显式更新规则：\n$$\n(x^{k} - 2tA^{\\top}(Ax^{k} - b)) \\frac{R}{\\max(R, \\|x^{k} - 2tA^{\\top}(Ax^{k} - b)\\|_{2})}\n$$\n该表达式仅依赖于当前迭代点 $x^{k}$、问题数据 $A, b, R$ 和步长 $t$，并且只使用了所要求的标准矩阵向量运算、范数和标量 `max` 函数。", "answer": "$$\n\\boxed{\n(x^{k} - 2tA^{\\top}(Ax^{k} - b)) \\frac{R}{\\max(R, \\|x^{k} - 2tA^{\\top}(Ax^{k} - b)\\|_{2})}\n}\n$$", "id": "2861550"}, {"introduction": "近端算子（proximal operator）是现代一阶凸优化方法（如近端梯度法和ADMM）的核心构件，它将梯度的概念推广到了非光滑函数。本练习 [@problem_id:2861514] 的目标是为组套索（group LASSO）惩罚项推导其近端算子。通过这个过程，你将掌握处理结构化稀疏性问题的关键技术——“块软阈值”算子，并深刻理解其与标准软阈值算子的联系与区别，从而能够解决更广泛的正则化问题。", "problem": "考虑一个信号处理中的线性逆问题，目标是求解一个结构化稀疏估计 $x \\in \\mathbb{R}^{n}$。根据 $\\{1,\\dots,n\\}$ 的一个固定索引划分，该向量被划分为 $G$ 个不重叠的组 $\\{x_{g}\\}_{g=1}^{G}$。设正则化子为加权组套索惩罚项 $R(x) = \\sum_{g=1}^{G} w_{g} \\|x_{g}\\|_{2}$，其中权重 $w_{g}  0$ 均为严格正值。在许多基于近端分裂的一阶凸优化方法中，需要重复计算 $R$ 的近端算子 (prox)，其对于任意 $\\lambda  0$ 和任意 $y \\in \\mathbb{R}^{n}$ 定义为\n$$\n\\operatorname{prox}_{\\lambda R}(y) \\triangleq \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} \\right\\}.\n$$\n从近端算子的定义和凸函数的次微分最优性条件出发，推导与 $R$ 相关联的块软阈值算子，并给出其对于每个组 $g \\in \\{1,\\dots,G\\}$ 以 $y_{g}$、$\\lambda$ 和 $w_{g}$ 表示的闭式表达式。你的最终答案必须是 $[\\operatorname{prox}_{\\lambda R}(y)]_{g}$ 的单一闭式解析表达式，且该表达式对所有 $y_{g} \\in \\mathbb{R}^{|g|}$ 和所有 $g \\in \\{1,\\dots,G\\}$ 均有效。不要报告不等式或待解方程；请提供显式表达式。无需进行数值舍入。", "solution": "问题陈述已解析和验证。该问题被认定为科学上是合理的、适定的、客观的且自洽的。它是信号处理中凸优化的一个标准问题。未发现任何缺陷。可以开始推导求解。\n\n目标是找到加权组套索惩罚项 $R(x) = \\sum_{g=1}^{G} w_{g} \\|x_{g}\\|_{2}$ 的近端算子的闭式表达式。近端算子被定义为以下最小化问题的解 $z^*$：\n$$\nz^{*} = \\operatorname{prox}_{\\lambda R}(y) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} \\right\\}\n$$\n令目标函数为 $F(z) = \\frac{1}{2} \\|z - y\\|_{2}^{2} + \\lambda R(z)$。欧几里得范数的平方可以分解到各个不重叠的组上：\n$$\n\\|z - y\\|_{2}^{2} = \\sum_{g=1}^{G} \\|z_g - y_g\\|_2^2\n$$\n将此代入目标函数，我们得到：\n$$\nF(z) = \\frac{1}{2} \\sum_{g=1}^{G} \\|z_g - y_g\\|_2^2 + \\lambda \\sum_{g=1}^{G} w_{g} \\|z_{g}\\|_{2} = \\sum_{g=1}^{G} \\left( \\frac{1}{2} \\|z_g - y_g\\|_2^2 + \\lambda w_{g} \\|z_{g}\\|_{2} \\right)\n$$\n总目标函数 $F(z)$ 是一些函数的和，其中每一项只依赖于单一一组变量 $z_g$。这个性质被称为可分离性。因此，对 $F(z)$ 关于 $z \\in \\mathbb{R}^n$ 的最小化可以通过对和的每一项分别关于其对应的组变量 $z_g \\in \\mathbb{R}^{|g|}$ 进行最小化来完成。\n\n对于每个组 $g \\in \\{1,\\dots,G\\}$，其对应块的解，记作 $z_g^* = [\\operatorname{prox}_{\\lambda R}(y)]_g$，由下式给出：\n$$\nz_g^* = \\arg\\min_{z_g \\in \\mathbb{R}^{|g|}} \\left\\{ J(z_g) \\triangleq \\frac{1}{2} \\|z_g - y_g\\|_2^2 + \\lambda w_{g} \\|z_{g}\\|_{2} \\right\\}\n$$\n函数 $J(z_g)$ 是凸函数，因为它是一个严格凸函数（二次项）和一个凸函数（$\\ell_2$-范数项）的和。因此，存在唯一的最小化子 $z_g^*$。凸函数的最优性条件表明，$z_g^*$ 是一个最小化子当且仅当零向量是 $J$ 在 $z_g^*$ 处的次微分的一个元素，即 $0 \\in \\partial J(z_g^*)$。\n\n函数 $J(z_g)$ 由两项组成。第一项 $\\frac{1}{2}\\|z_g - y_g\\|_2^2$ 是可微的，其梯度为 $z_g - y_g$。第二项是 $\\lambda w_g \\|z_g\\|_2$。$\\ell_2$-范数 $\\|z_g\\|_2$ 的次微分是：\n$$\n\\partial \\|z_g\\|_2 =\n\\begin{cases}\n\\{ \\frac{z_g}{\\|z_g\\|_2} \\}  \\text{若 } z_g \\neq 0 \\\\\n\\{ v \\in \\mathbb{R}^{|g|} \\mid \\|v\\|_2 \\le 1 \\}  \\text{若 } z_g = 0\n\\end{cases}\n$$\n根据次微分的求和法则（因为其中一个函数是可微的），我们有 $\\partial J(z_g) = (z_g - y_g) + \\lambda w_g \\partial \\|z_g\\|_2$。最优性条件 $0 \\in \\partial J(z_g^*)$ 变为：\n$$\n0 \\in (z_g^* - y_g) + \\lambda w_g \\partial \\|z_g^*\\|_2\n$$\n这可以重写为：\n$$\ny_g - z_g^* \\in \\lambda w_g \\partial \\|z_g^*\\|_2\n$$\n我们现在根据 $z_g^*$ 的值分两种情况进行分析。\n\n情况 1：$z_g^* \\neq 0$。\n在这种情况下，次微分 $\\partial \\|z_g^*\\|_2$ 是单元素集 $\\{ \\frac{z_g^*}{\\|z_g^*\\|_2} \\}$。最优性条件变为一个等式：\n$$\ny_g - z_g^* = \\lambda w_g \\frac{z_g^*}{\\|z_g^*\\|_2}\n$$\n重新整理各项以求解 $y_g$：\n$$\ny_g = z_g^* + \\lambda w_g \\frac{z_g^*}{\\|z_g^*\\|_2} = z_g^* \\left( 1 + \\frac{\\lambda w_g}{\\|z_g^*\\|_2} \\right)\n$$\n从这个等式中，我们看到 $y_g$ 是 $z_g^*$ 的一个正向缩放。这意味着 $z_g^*$ 必须与 $y_g$ 共线且指向相同方向。因此，我们有 $\\frac{z_g^*}{\\|z_g^*\\|_2} = \\frac{y_g}{\\|y_g\\|_2}$。请注意，这要求 $y_g \\neq 0$。将此代回关于 $y_g - z_g^*$ 的方程中：\n$$\nz_g^* = y_g - \\lambda w_g \\frac{y_g}{\\|y_g\\|_2} = \\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g\n$$\n由于我们假设 $z_g^* \\neq 0$，其范数必须为正，即 $\\|z_g^*\\|_2 > 0$。对 $z_g^*$ 的表达式取范数：\n$$\n\\|z_g^*\\|_2 = \\left\\| \\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g \\right\\|_2 = \\left| 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right| \\|y_g\\|_2\n$$\n由于 $z_g^*$ 和 $y_g$ 方向相同，标量因子必须为正：$1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} > 0$。这意味着 $\\|y_g\\|_2 > \\lambda w_g$。如果此条件成立，我们关于 $z_g^* \\neq 0$ 的假设是一致的，解即为所推导的结果。\n\n情况 2：$z_g^* = 0$。\n在这种情况下，次微分是闭单位球：$\\partial \\|z_g^*\\|_2 = \\{ v \\in \\mathbb{R}^{|g|} \\mid \\|v\\|_2 \\le 1 \\}$。最优性条件变为：\n$$\ny_g - 0 \\in \\lambda w_g \\{ v \\mid \\|v\\|_2 \\le 1 \\}\n$$\n这等价于说 $y_g$ 位于半径为 $\\lambda w_g$ 的球内：\n$$\n\\|y_g\\|_2 \\le \\lambda w_g\n$$\n如果满足此条件，则最小化子为 $z_g^* = 0$。\n\n结合这两种情况，我们得到：\n$$\nz_g^* = [\\operatorname{prox}_{\\lambda R}(y)]_g =\n\\begin{cases}\n\\left( 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2} \\right) y_g  \\text{若 } \\|y_g\\|_2 > \\lambda w_g \\\\\n0  \\text{若 } \\|y_g\\|_2 \\le \\lambda w_g\n\\end{cases}\n$$\n这个分段表达式可以使用最大值函数紧凑地写成单个公式。收缩因子 $\\left(1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}\\right)$ 在 0 处进行阈值处理。\n$$\nz_g^* = \\max\\left\\{0, 1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}\\right\\} y_g\n$$\n该表达式是块软阈值算子的闭式表示。它对于所有 $y_g \\in \\mathbb{R}^{|g|}$ 都是良定义的。具体来说，如果 $y_g = 0$，那么 $\\|y_g\\|_2 = 0$。由于 $\\lambda w_g > 0$，项 $\\frac{\\lambda w_g}{\\|y_g\\|_2}$ 变为无穷大。max 函数的参数 $1 - \\infty$ 计算结果为 $-\\infty$。因此，$\\max\\{0, -\\infty\\} = 0$，结果为 $z_g^* = 0 \\cdot y_g = 0$，这与我们的推导一致。\n对于任何满足 $\\|y_g\\|_2 \\le \\lambda w_g$ 的非零 $y_g$，项 $1 - \\frac{\\lambda w_g}{\\|y_g\\|_2}$ 是非正的，所以 max 函数返回 0，导致 $z_g^*=0$。对于 $\\|y_g\\|_2 > \\lambda w_g$，缩放因子是正的，该表达式产生正确的收缩结果。", "answer": "$$\n\\boxed{\\max\\left\\{0, 1 - \\frac{\\lambda w_{g}}{\\|y_{g}\\|_{2}}\\right\\} y_{g}}\n$$", "id": "2861514"}, {"introduction": "本节的最终实践旨在连接理论与实际应用，将前面学到的概念融会贯通。坐标下降法是求解大规模LASSO问题的最高效的算法之一。本练习 [@problem_id:2861565] 要求你不仅要推导出LASSO问题的单坐标更新法则（即软阈值操作），还要亲手实现一个完整的、带有残差快速更新的坐标下降求解器。通过编写代码并在一系列测试用例上进行验证，你将把理论知识转化为解决实际问题的强大工具。", "problem": "您的任务是根据凸优化和信号处理的原理，推导并实现用于最小绝对收缩和选择算子（LASSO）问题的循环坐标下降算法。\n\n考虑设计矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和观测向量 $b \\in \\mathbb{R}^{m}$ 的 LASSO 目标函数：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是一个给定的正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。\n\n您的任务是：\n1. 从第一性原理出发，推导用于 $f(x)$ 的循环坐标下降法中的单坐标最小化更新规则。从 $f(x)$ 的定义和 $\\ell_1$ 范数的次梯度最优性条件开始，并就 $f(x)$ 相对于单个坐标 $x_i$ 的最小化进行推理，同时保持所有其他坐标固定。仅使用包括凸函数性质、绝对值的次梯度以及基础线性代数在内的基本事实。不要先验地假设任何特定的闭式更新。\n2. 证明坐标级最小化器是通过将软阈值算子应用于当前迭代和残差的仿射函数得到的。清晰地定义您在推导中引入的所有量。\n3. 实现一个使用所推导更新规则的循环坐标下降算法。您的实现必须：\n   - 维护残差 $r \\triangleq b - A x$ 并在每次坐标更新后增量更新它，以实现每次坐标更新 $\\mathcal{O}(m)$ 的成本。\n   - 使用由 $S_{\\tau}(z) \\triangleq \\mathrm{sign}(z)\\max(|z| - \\tau, 0)$ 定义的软阈值算子。\n   - 当一个完整周期内任何坐标的最大绝对变化小于容差 $\\varepsilon$ 或达到最大轮次数时，算法终止。\n   - 返回最终迭代结果 $x$，并在需要时返回每轮结束时的目标函数值序列，以评估单调性。\n\n您可以使用的基础知识：\n- $\\|\\cdot\\|_2^2$ 和 $\\|\\cdot\\|_1$ 的凸性及其次梯度的性质。\n- 次梯度最优性条件：对于一个凸函数 $f$ 的最优点 $x^\\star$，有 $0 \\in \\partial f(x^\\star)$。\n- 绝对值的次微分：对于 $t \\in \\mathbb{R}$，如果 $t \\ne 0$，则 $\\partial |t| = \\{\\mathrm{sign}(t)\\}$；如果 $t = 0$，则 $\\partial |t| = [-1,1]$。\n- 用于残差更新的线性代数恒等式。\n\n将目标值定义为：\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n\n测试套件：\n实现您的程序以运行以下五个测试用例，并将结果汇总到单行输出中。\n\n- 测试 1 (正交规范列，解析校验)：设置 $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$, $\\lambda = 0.7$。运行您的坐标下降算法以获得 $x_{\\mathrm{cd}}$。对于正交规范列，已知的解析解是 $x^\\star = S_{\\lambda}(A^\\top b) = S_{\\lambda}(b)$。输出标量\n  $$\n  e_1 \\triangleq \\|x_{\\mathrm{cd}} - x^\\star\\|_{\\infty}.\n  $$\n\n- 测试 2 (一般高矩阵系统，Karush–Kuhn–Tucker (KKT) 校验)：生成一个 $A \\in \\mathbb{R}^{60 \\times 30}$ 矩阵，其元素为独立的标准正态分布，然后将每列归一化为单位 $\\ell_2$ 范数。使用固定的伪随机种子 $0$ 以使实例具有确定性。定义 $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$，其在索引 $0,5,10,15,20$ 处的非零条目值分别为 $[2.5,-1.7,1.2,-0.9,1.8]$，其他地方为零。设 $b = A x_{\\mathrm{true}} + \\eta$，其中 $\\eta \\in \\mathbb{R}^{60}$ 的元素为独立的、标准差为 $0.01$ 的正态分布，并使用相同的种子 $0$ 生成。设 $\\lambda = 0.05$。运行坐标下降算法以获得 $x_{\\mathrm{cd}}$。验证 LASSO 的 KKT 条件：令 $g \\triangleq A^\\top(A x_{\\mathrm{cd}} - b)$，\n  - 如果 $x_{\\mathrm{cd},i} \\ne 0$，则 $g_i + \\lambda \\,\\mathrm{sign}(x_{\\mathrm{cd},i}) = 0$。\n  - 如果 $x_{\\mathrm{cd},i} = 0$，则 $|g_i| \\le \\lambda$。\n  由于数值误差，在这些检查中实现 $10^{-4}$ 的容差。输出布尔值 $b_2$，表示所有坐标是否在容差范围内满足 KKT 条件。\n\n- 测试 3 (大正则化项使解趋于零)：使用 $A = I_4$, $b = [3,-1,0.2,-0.5]^\\top$ 和 $\\lambda = 10^6$。输出布尔值 $b_3$，表示返回的解是否在 $10^{-12}$ 的绝对容差内为零向量。\n\n- 测试 4 (零正则化项退化为最小二乘法)：使用伪随机种子 $1$ 生成 $A \\in \\mathbb{R}^{40 \\times 10}$，其元素为独立的标准正态分布。使用种子 $2$ 生成 $b \\in \\mathbb{R}^{40}$，其元素为独立的标准正态分布。设 $\\lambda = 0$。令 $x_{\\mathrm{ls}}$ 表示最小化 $\\frac{1}{2}\\|A x - b\\|_2^2$ 的最小二乘解，通过标准线性最小二乘法计算。运行坐标下降算法以获得 $x_{\\mathrm{cd}}$。输出标量\n  $$\n  e_4 \\triangleq \\frac{\\|x_{\\mathrm{cd}} - x_{\\mathrm{ls}}\\|_2}{\\max(\\|x_{\\mathrm{ls}}\\|_2, 10^{-12})}.\n  $$\n\n- 测试 5 (目标函数值随轮次单调下降)：使用伪随机种子 $3$ 生成 $A \\in \\mathbb{R}^{30 \\times 15}$ 和 $b \\in \\mathbb{R}^{30}$，其元素为独立的标准正态分布。设 $\\lambda = 0.1$。记录每次遍历所有坐标后的目标函数值，并验证该序列在数值容差 $10^{-10}$ 内是单调非增的。输出布尔值 $b_5$，表示单调性是否成立。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，列表按 $[e_1, b_2, b_3, e_4, b_5]$ 的顺序用方括号括起来。此问题不涉及物理单位，也无角度单位。所有数值输出应为指定的实数或布尔值，不带百分号。您的实现必须对给定的实例具有鲁棒性，并且不应需要任何用户输入。", "solution": "我们从凸优化问题开始\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，且 $\\lambda \\ge 0$。函数 $f$ 是凸函数，因为它是凸函数 $\\frac{1}{2}\\|A x - b\\|_2^2$ 和凸函数 $\\lambda \\|x\\|_1$ 的和。\n\n循环坐标下降法一次最小化一个坐标上的 $f$，同时保持其他坐标固定。固定一个索引 $i \\in \\{1,\\dots,n\\}$，并用 $a_i \\in \\mathbb{R}^m$ 表示 $A$ 的第 $i$ 列。设 $x \\in \\mathbb{R}^n$ 为当前迭代，并定义残差\n$$\nr \\triangleq b - A x.\n$$\n因为 $A x = \\sum_{j=1}^n a_j x_j$，只将 $x_i$ 更改为新值 $t \\in \\mathbb{R}$ 会得到一个新向量 $x^{(i \\leftarrow t)}$ 和新残差\n$$\nr^{(i \\leftarrow t)} = b - A x^{(i \\leftarrow t)} = b - \\left(A x + a_i (t - x_i)\\right) = r - a_i (t - x_i).\n$$\n目标函数作为 $t$ 的函数（其他坐标固定）变为\n\\begin{align*}\n\\phi_i(t) \\triangleq \\frac{1}{2}\\|A x^{(i \\leftarrow t)} - b\\|_2^2 + \\lambda \\left(\\sum_{j \\ne i} |x_j| + |t|\\right) \\\\\n= \\frac{1}{2}\\|r^{(i \\leftarrow t)}\\|_2^2 + \\lambda |t| + \\text{与 } t \\text{ 无关的常数} \\\\\n= \\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2 + \\lambda |t| + \\text{常数}.\n\\end{align*}\n使用 $\\|u - v\\|_2^2 = \\|u\\|_2^2 - 2 u^\\top v + \\|v\\|_2^2$ 展开平方范数，我们得到\n\\begin{align*}\n\\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2\n= \\frac{1}{2}\\|r\\|_2^2 - (t - x_i) a_i^\\top r + \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2.\n\\end{align*}\n舍去与 $t$ 无关的项，坐标级目标函数简化为单变量凸函数\n$$\n\\tilde{\\phi}_i(t) \\triangleq \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t|.\n$$\n通过配方法，定义 $d_i \\triangleq \\|a_i\\|_2^2$ 和\n$$\nc_i \\triangleq x_i + \\frac{a_i^\\top r}{d_i} \\quad \\text{当 } d_i > 0 \\text{ 时}.\n$$\n那么\n\\begin{align*}\n\\tilde{\\phi}_i(t)\n= \\frac{1}{2} d_i (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t| \\\\\n= \\frac{1}{2} d_i \\left(t - x_i - \\frac{a_i^\\top r}{d_i}\\right)^2 - \\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i} + \\lambda |t|.\n\\end{align*}\n忽略常数项 $-\\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i}$，最小化 $\\tilde{\\phi}_i(t)$ 等价于最小化\n$$\n\\psi_i(t) \\triangleq \\frac{1}{2} d_i (t - c_i)^2 + \\lambda |t|.\n$$\n这个一维凸问题的次梯度最优性条件是\n$$\n0 \\in \\partial \\psi_i(t^\\star) = d_i (t^\\star - c_i) + \\lambda \\,\\partial |t^\\star|,\n$$\n其中绝对值的次微分是：如果 $t^\\star \\ne 0$，则 $\\partial |t^\\star| = \\{\\mathrm{sign}(t^\\star)\\}$；如果 $t^\\star = 0$，则 $\\partial |t^\\star| = [-1, 1]$。\n\n考虑两种情况。\n\n情况 1：$t^\\star \\ne 0$。则次梯度条件为\n$$\nd_i (t^\\star - c_i) + \\lambda \\,\\mathrm{sign}(t^\\star) = 0 \\;\\;\\Longleftrightarrow\\;\\; t^\\star = c_i - \\frac{\\lambda}{d_i} \\,\\mathrm{sign}(t^\\star).\n$$\n这意味着 $|c_i| > \\lambda/d_i$，解是通过将 $c_i$ 向零收缩 $\\lambda/d_i$ 同时保持符号得到的：\n$$\nt^\\star = \\mathrm{sign}(c_i)\\left(|c_i| - \\frac{\\lambda}{d_i}\\right).\n$$\n\n情况 2：$t^\\star = 0$。则次梯度条件变为\n$$\n0 \\in - d_i c_i + \\lambda [-1,1] \\;\\;\\Longleftrightarrow\\;\\; |d_i c_i| \\le \\lambda \\;\\;\\Longleftrightarrow\\;\\; |c_i| \\le \\frac{\\lambda}{d_i}.\n$$\n结合这两种情况，我们得到软阈值形式\n$$\nt^\\star = S_{\\lambda/d_i}(c_i) \\triangleq \\mathrm{sign}(c_i)\\max\\left(|c_i| - \\frac{\\lambda}{d_i}, \\, 0 \\right).\n$$\n等价地，使用残差定义 $r = b - A x$，我们有\n$$\nc_i = x_i + \\frac{a_i^\\top r}{d_i} = \\frac{a_i^\\top r + d_i x_i}{d_i},\n$$\n所以坐标级最小化器是\n$$\nx_i \\leftarrow S_{\\lambda/\\|a_i\\|_2^2}\\!\\left(\\frac{a_i^\\top r + \\|a_i\\|_2^2 x_i}{\\|a_i\\|_2^2}\\right).\n$$\n如果 $d_i = \\|a_i\\|_2^2 = 0$ (一个零列)，$x_i$ 的任何变化都不会影响二次项；对于 $\\lambda > 0$，$\\lambda |t|$ 的最小化器是 $t^\\star = 0$。在我们的实现中，如果 $d_i=0$ 且 $\\lambda > 0$，我们设置 $x_i \\leftarrow 0$；如果 $\\lambda = 0$ 且 $d_i = 0$，该坐标无关紧要，可以保持不变。\n\n高效的残差更新：如果 $\\Delta_i \\triangleq x_i^{\\text{new}} - x_i^{\\text{old}}$，那么\n$$\nr^{\\text{new}} = b - A x^{\\text{new}} = b - \\left(A x^{\\text{old}} + a_i \\Delta_i\\right) = r^{\\text{old}} - a_i \\Delta_i,\n$$\n这需要 $\\mathcal{O}(m)$ 次操作。\n\n收敛性和单调性：每个坐标更新都会在该坐标上精确地最小化 $f$，因此在每次坐标更新后 $f$ 是非增的，从而在每轮（遍历所有坐标）后也是非增的。当一轮中最大的坐标绝对变化量低于某个容差，或者达到最大轮次数时，算法终止。\n\n通过 Karush–Kuhn–Tucker (KKT) 条件进行最优性验证：令 $g(x) \\triangleq A^\\top (A x - b)$ 为光滑部分的梯度。LASSO 中 $x^\\star$ 的最优性 KKT 条件是\n$$\n0 \\in g(x^\\star) + \\lambda \\,\\partial \\|x^\\star\\|_1,\n$$\n这等价于分量形式的条件\n$$\n\\begin{cases}\ng_i(x^\\star) + \\lambda \\,\\mathrm{sign}(x_i^\\star) = 0,  \\text{如果 } x_i^\\star \\ne 0, \\\\\n|g_i(x^\\star)| \\le \\lambda,  \\text{如果 } x_i^\\star = 0.\n\\end{cases}\n$$\n在实践中，我们在一个小的数值容差内检查这些等式和不等式。\n\n测试用例和输出：我们实现了指定的五个测试用例并计算\n- $e_1 = \\|x_{\\mathrm{cd}} - S_{\\lambda}(b)\\|_\\infty$ 用于正交规范列，\n- $b_2$ 表示对于高矩阵系统，KKT 条件在容差内是否满足，\n- $b_3$ 表示对于非常大的 $\\lambda$，解是否为零，\n- $e_4$ 当 $\\lambda=0$ 时与最小二乘解的相对误差，\n- $b_5$ 表示目标函数值在各轮次中是否单调非增。\n\n最终程序将结果输出为单行列表 $[e_1, b_2, b_3, e_4, b_5]$。", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z: float, tau: float) - float:\n    \"\"\"Soft-thresholding operator S_tau(z) = sign(z) * max(|z| - tau, 0).\"\"\"\n    if tau == 0:\n        return z\n    abs_z = abs(z)\n    if abs_z = tau:\n        return 0.0\n    return np.copysign(abs_z - tau, z)\n\ndef objective_value(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float) - float:\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lambd * float(np.linalg.norm(x, 1))\n\ndef coordinate_descent_lasso(\n    A: np.ndarray,\n    b: np.ndarray,\n    lambd: float,\n    max_epochs: int = 2000,\n    tol: float = 1e-8,\n    record_objective: bool = False\n):\n    \"\"\"\n    Cyclic coordinate descent for LASSO:\n        minimize 0.5 * ||A x - b||_2^2 + lambd * ||x||_1.\n    Uses residual updates for O(m) per coordinate.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n, dtype=float)\n    r = -b.copy()  # r = A x - b, initially x=0 so r = -b\n    obj_hist = []\n\n    # Precompute squared column norms d_i = ||a_i||_2^2\n    col_sq_norms = np.sum(A * A, axis=0)\n\n    for epoch in range(max_epochs):\n        max_delta = 0.0\n        for i in range(n):\n            ai = A[:, i]\n            di = col_sq_norms[i]\n            xi_old = x[i]\n\n            if di == 0.0:\n                # If the column is zero, the quadratic does not depend on x_i.\n                # Minimizer of lambd * |t| is t=0 for lambd0; do nothing if lambd==0.\n                xi_new = 0.0 if lambd  0 else xi_old\n            else:\n                # Our residual is r = Ax - b. The derivation's residual is r' = b - Ax = -r\n                # c_i = x_i + a_i^T r' / d_i = x_i - a_i^T r / d_i\n                ci = xi_old - float(ai.T @ r) / di\n                # Update via soft-thresholding with threshold lambd / d_i\n                xi_new = soft_threshold(ci, lambd / di)\n\n            delta = xi_new - xi_old\n            if delta != 0.0:\n                # Update residual: r_new = r_old + a_i * delta\n                r += ai * delta\n                x[i] = xi_new\n                max_delta = max(max_delta, abs(delta))\n\n        if record_objective:\n            obj_hist.append(objective_value(A, b, x, lambd))\n\n        if max_delta  tol:\n            break\n\n    if record_objective:\n        return x, obj_hist\n    return x\n\ndef kkt_satisfied(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float, tol: float = 1e-4) - bool:\n    \"\"\"\n    Check KKT conditions for LASSO:\n      g = A^T (A x - b)\n      If x_i != 0: g_i + lambd * sign(x_i) = 0\n      If x_i == 0: |g_i| = lambd\n    with numerical tolerance tol.\n    \"\"\"\n    g = A.T @ (A @ x - b)\n    for i in range(x.size):\n        xi = x[i]\n        gi = g[i]\n        if abs(xi)  1e-12:\n            if abs(gi + lambd * np.sign(xi))  tol:\n                return False\n        else:\n            if abs(gi) - lambd  tol:\n                return False\n    return True\n\ndef test_suite():\n    results = []\n\n    # Test 1: Orthonormal columns (A = I), analytical solution S_lambda(b)\n    A1 = np.eye(4)\n    b1 = np.array([3.0, -1.0, 0.2, -0.5])\n    lambd1 = 0.7\n    x1 = coordinate_descent_lasso(A1, b1, lambd1, max_epochs=100, tol=1e-12)\n    # Analytical solution: S_lambda(A^T b) = S_lambda(b)\n    x1_star = np.array([soft_threshold(b1[i], lambd1) for i in range(4)])\n    e1 = float(np.max(np.abs(x1 - x1_star)))\n    results.append(e1)\n\n    # Test 2: General tall system, KKT check\n    rng = np.random.default_rng(0)\n    A2 = rng.standard_normal((60, 30))\n    # Normalize columns to unit norm\n    col_norms = np.linalg.norm(A2, axis=0)\n    # Avoid division by zero in extremely unlikely all-zero columns\n    col_norms[col_norms == 0.0] = 1.0\n    A2 = A2 / col_norms\n    x_true = np.zeros(30)\n    nz_idx = [0, 5, 10, 15, 20]\n    nz_vals = [2.5, -1.7, 1.2, -0.9, 1.8]\n    for idx, val in zip(nz_idx, nz_vals):\n        x_true[idx] = val\n    noise_rng = np.random.default_rng(0)\n    noise = noise_rng.normal(0.0, 0.01, size=60)\n    b2 = A2 @ x_true + noise\n    lambd2 = 0.05\n    x2 = coordinate_descent_lasso(A2, b2, lambd2, max_epochs=2000, tol=1e-10)\n    b2_ok = kkt_satisfied(A2, b2, x2, lambd2, tol=1e-4)\n    results.append(bool(b2_ok))\n\n    # Test 3: Large lambda - zero solution\n    lambd3 = 1e6\n    x3 = coordinate_descent_lasso(A1, b1, lambd3, max_epochs=50, tol=1e-14)\n    b3_ok = bool(np.allclose(x3, 0.0, atol=1e-12))\n    results.append(b3_ok)\n\n    # Test 4: Zero lambda - least squares\n    rng1 = np.random.default_rng(1)\n    A4 = rng1.standard_normal((40, 10))\n    rng2 = np.random.default_rng(2)\n    b4 = rng2.standard_normal(40)\n    lambd4 = 0.0\n    x4_cd = coordinate_descent_lasso(A4, b4, lambd4, max_epochs=5000, tol=1e-12)\n    # Least squares solution via numpy\n    x4_ls, *_ = np.linalg.lstsq(A4, b4, rcond=None)\n    denom = max(np.linalg.norm(x4_ls), 1e-12)\n    e4 = float(np.linalg.norm(x4_cd - x4_ls) / denom)\n    results.append(e4)\n\n    # Test 5: Monotone objective decrease across epochs\n    rng3 = np.random.default_rng(3)\n    A5 = rng3.standard_normal((30, 15))\n    b5 = rng3.standard_normal(30)\n    lambd5 = 0.1\n    x5, obj_hist = coordinate_descent_lasso(A5, b5, lambd5, max_epochs=200, tol=1e-10, record_objective=True)\n    # Check nonincreasing sequence within small tolerance\n    diffs = np.diff(obj_hist)\n    b5_ok = bool(np.all(diffs = 1e-10))\n    results.append(b5_ok)\n\n    return results\n\ndef solve():\n    results = test_suite()\n    # Format booleans and floats in a single list\n    # Convert to string with Python default formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2861565"}]}