{"hands_on_practices": [{"introduction": "要真正理解近端梯度算法，我们必须从其最基本的构件开始。第一个练习 [@problem_id:2897782] 将引导您为经典的稀疏去噪问题推导并实现该方法的单次迭代。通过对数据保真项应用梯度步，并对 $\\ell_1$ 范数应用近端步（软阈值），您将直接观察到该算法如何从噪声数据中开始揭示稀疏信号。", "problem": "给定一个在有限维实数空间中的复合凸优化问题：最小化目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$ 且 $g(x) = \\lambda \\lVert x \\rVert_1$。这里，$x \\in \\mathbb{R}^n$，$b \\in \\mathbb{R}^n$ 是数据，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是正则化参数。从光滑函数梯度和凸函数近端算子的基本定义出发，推导出近端梯度法用于求解 $F(x)$ 的单步更新映射，包括基于 $\\nabla f(x)$ 的 Lipschitz 连续性的步长可接受条件。然后，实现一个程序，对几个确定性的稀疏信号去噪测试实例应用一次近端梯度迭代，并量化相对于已知的稀疏真实值，该迭代对估计产生的即时影响。\n\n您的程序必须：\n- 仅使用一次近端梯度迭代，步长为用户指定的 $t \\in \\mathbb{R}_{0}$，初始化为 $x^{(0)} \\in \\mathbb{R}^n$。更新公式必须从定义推导得出（不得假设任何预封装的更新形式；必须明确推导）。\n- 对于每个测试用例，计算：\n  1. 一次迭代后的均方误差，定义为 $\\mathrm{MSE} = \\frac{1}{n}\\lVert x^{(1)} - x_{\\mathrm{true}} \\rVert_2^2$。\n  2. 支撑集召回率（分数形式），定义为 $\\mathrm{recall} = \\frac{\\lvert \\mathrm{supp}(x^{(1)}) \\cap \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}{\\lvert \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}$，其中 $\\mathrm{supp}(z) = \\{ i : |z_i|  \\varepsilon \\}$，阈值 $\\varepsilon = 10^{-12}$。\n  3. 在相同阈值 $\\varepsilon$ 下，$x^{(1)}$ 中非零项的数量，为一个整数。\n- 在输出中将 $\\mathrm{MSE}$ 和 $\\mathrm{recall}$ 四舍五入到 $6$ 位小数。非零项计数必须是整数。\n- 满足步长可接受条件 $t \\in (0, 2/L)$，其中 $L$ 是 $\\nabla f$ 的 Lipschitz 常数，您必须从第一性原理确定。\n\n测试套件：\n- 案例 1（理想情况，从零初始化进行精确收缩）：\n  - $n = 8$\n  - $x_{\\mathrm{true}} = [0, 1.5, 0, 0, -2.0, 0, 0.7, 0]$\n  - $\\mathrm{noise} = [0.05, -0.08, 0.10, -0.02, 0.03, 0.00, -0.05, 0.04]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $\\lambda = 0.4$\n  - $t = 1.0$\n- 案例 2（收缩阈值处的边界条件）：\n  - $n = 8$\n  - $x_{\\mathrm{true}} = [0.4, -0.4, 0, 0, 0, 0, 0, 0]$\n  - $\\mathrm{noise} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0, 0, 0, 0]$\n  - $\\lambda = 0.4$\n  - $t = 1.0$\n- 案例 3（非零热启动和部分步长）：\n  - $n = 6$\n  - $x_{\\mathrm{true}} = [0, 0, 1.0, 0, -1.2, 0]$\n  - $\\mathrm{noise} = [0.0, 0.05, -0.02, 0.00, 0.03, 0.10]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0.2, 0, 0.5, 0, -0.8, 0]$\n  - $\\lambda = 0.6$\n  - $t = 0.5$\n- 案例 4（较大但可接受的步长，接近稳定性极限）：\n  - $n = 5$\n  - $x_{\\mathrm{true}} = [0, 0, 0, 2.0, 0]$\n  - $\\mathrm{noise} = [0.02, -0.01, 0.00, -0.05, 0.01]$\n  - $b = x_{\\mathrm{true}} + \\mathrm{noise}$\n  - $x^{(0)} = [0, 0, 0, 0, 0]$\n  - $\\lambda = 0.2$\n  - $t = 1.9$\n\n实现要求：\n- 推导并实现针对 $f$ 和 $g$ 的近端梯度单步更新。\n- 直接根据 $g$ 的定义实现推导中所需的近端算子，在特化时按元素应用。\n- 对每个案例，计算 $x^{(1)}$，然后计算三元组 $[\\mathrm{MSE}, \\mathrm{recall}, \\mathrm{nnz}]$，其中 $\\mathrm{nnz}$ 是满足 $\\lvert x^{(1)}_i \\rvert  \\varepsilon$ 的索引 $i$ 的整数计数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素都是一个测试用例的结果列表 $[\\mathrm{MSE}, \\mathrm{recall}, \\mathrm{nnz}]$，其中 $\\mathrm{MSE}$ 和 $\\mathrm{recall}$ 四舍五入到 $6$ 位小数。例如，包含两个案例的输出应类似于 $[[0.123456,0.500000,3],[0.010000,1.000000,2]]$。\n\n角度单位、物理单位和百分比不适用。所有报告的分数必须显示为四舍五入到 $6$ 位的小数，而不是百分比。", "solution": "问题已经过验证。\n\n**步骤1：提取的已知条件**\n- **目标函数**：对 $x \\in \\mathbb{R}^n$ 最小化 $F(x) = f(x) + g(x)$。\n- **平滑部分**：$f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$，其中 $b \\in \\mathbb{R}^n$。\n- **非平滑部分**：$g(x) = \\lambda \\lVert x \\rVert_1$，其中 $\\lambda \\in \\mathbb{R}_{\\ge 0}$。\n- **算法**：从 $x^{(0)}$ 开始，使用步长 $t  0$ 进行一次近端梯度法迭代。\n- **步长条件**：$t \\in (0, 2/L)$，其中 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数。\n- **评估指标**：\n    1. $\\mathrm{MSE} = \\frac{1}{n}\\lVert x^{(1)} - x_{\\mathrm{true}} \\rVert_2^2$。\n    2. $\\mathrm{recall} = \\frac{\\lvert \\mathrm{supp}(x^{(1)}) \\cap \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}{\\lvert \\mathrm{supp}(x_{\\mathrm{true}}) \\rvert}$。\n    3. $\\mathrm{supp}(z) = \\{ i : |z_i|  \\varepsilon \\}$，其中 $\\varepsilon = 10^{-12}$。\n    4. $\\mathrm{nnz}$：$x^{(1)}$ 中非零项的数量。\n- **测试用例**：四个具有给定参数 $n$、$x_{\\mathrm{true}}$、噪声、$x^{(0)}$、$\\lambda$ 和 $t$ 的具体实例。\n\n**步骤2：验证**\n问题已经过严格验证。\n- **科学基础**：该问题描述了用于稀疏信号恢复的 LASSO 公式，这是信号处理和统计学中的一个典型问题。函数 $f(x)$ 和 $g(x)$ 都是凸函数。对于此类复合凸优化问题，近端梯度法是标准且合适的算法。前提条件在科学上是合理的。\n- **适定性**：目标函数 $F(x)$ 是严格凸且强制的，这保证了唯一最小化解的存在。任务是执行一个明确定义的单步算法并计算确定性指标。该问题是适定的。\n- **客观性**：语言精确且数学化，没有歧义或主观陈述。\n- **一致性**：问题是自洽的。必须确定 $\\nabla f(x)$ 的 Lipschitz 常数 $L$。对于 $f(x) = \\frac{1}{2}\\lVert x - b \\rVert_2^2$，其梯度为 $\\nabla f(x) = x-b$，Hessian 矩阵为 $\\nabla^2 f(x) = I_n$（单位矩阵）。梯度的 Lipschitz 常数是 Hessian 矩阵的最大特征值，因此 $L=1$。要求的步长条件是 $t \\in (0, 2/1) = (0, 2)$。所有测试用例提供的步长（$t=1.0, 1.0, 0.5, 1.9$）都在此区间内。问题设置是一致且完整的。\n\n**步骤3：结论**\n问题有效。这是一个定义明确且标准的信号处理优化练习。开始求解。\n\n---\n\n问题是最小化复合凸函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\lVert x-b \\rVert_2^2$ 且 $g(x) = \\lambda \\lVert x \\rVert_1$。近端梯度法是为解决此类问题而设计的迭代算法。每次迭代包括两个步骤：对平滑部分 $f(x)$ 进行一次梯度下降，然后对非平滑部分 $g(x)$ 进行一次近端步骤。\n\n迭代更新是通过最小化 $f(x)$ 在当前估计 $x^{(k)}$ 周围的二次近似加上非平滑项 $g(x)$ 来推导的。$f(x)$ 在 $x^{(k)}$ 附近的泰勒展开为 $f(x) \\approx f(x^{(k)}) + \\langle \\nabla f(x^{(k)}), x - x^{(k)} \\rangle + \\frac{1}{2}(x - x^{(k)})^T \\nabla^2 f(x^{(k)}) (x - x^{(k)})$。对于一个通用步长 $t  0$，我们用 $\\frac{1}{t}I$ 替换 Hessian 项，这引出了主化-最小化步骤：\n$$x^{(k+1)} = \\arg\\min_x \\left( f(x^{(k)}) + \\langle \\nabla f(x^{(k)}), x - x^{(k)} \\rangle + \\frac{1}{2t}\\lVert x - x^{(k)} \\rVert_2^2 + g(x) \\right)$$\n在最小化过程中，可以忽略与 $x$ 无关的项 $f(x^{(k)})$ 和其他常数。通过配方法，问题等价于：\n$$x^{(k+1)} = \\arg\\min_x \\left( \\frac{1}{2t}\\lVert x - (x^{(k)} - t\\nabla f(x^{(k)})) \\rVert_2^2 + g(x) \\right)$$\n这是函数 $tg(x)$ 的近端算子应用于对 $f(x)$ 进行梯度下降步骤后得到的点的定义。近端梯度更新的一般形式是：\n$$x^{(k+1)} = \\mathrm{prox}_{tg} \\left( x^{(k)} - t\\nabla f(x^{(k)}) \\right)$$\n\n为保证此方法的收敛，步长 $t$ 的选择必须满足 $t \\in (0, 2/L)$，其中 $L$ 是梯度 $\\nabla f(x)$ 的 Lipschitz 常数。首先，我们计算 $f(x) = \\frac{1}{2}(x-b)^T(x-b)$ 的梯度：\n$$\\nabla f(x) = x - b$$\nLipschitz 常数 $L$ 必须对所有 $x_1, x_2$ 满足 $\\lVert \\nabla f(x_1) - \\nabla f(x_2) \\rVert_2 \\le L \\lVert x_1 - x_2 \\rVert_2$。\n$$\\lVert \\nabla f(x_1) - \\nabla f(x_2) \\rVert_2 = \\lVert (x_1 - b) - (x_2 - b) \\rVert_2 = \\lVert x_1 - x_2 \\rVert_2$$\n因此，$L=1$。步长条件为 $t \\in (0, 2)$。\n\n接下来，我们必须推导 $g(x) = \\lambda \\lVert x \\rVert_1$ 的近端算子。近端算子 $\\mathrm{prox}_{\\alpha g}(v)$ 定义为：\n$$\\mathrm{prox}_{\\alpha g}(v) = \\arg\\min_x \\left( \\frac{1}{2} \\lVert x - v \\rVert_2^2 + \\alpha g(x) \\right)$$\n在我们的案例中，函数是 $tg(x) = t\\lambda\\lVert x \\rVert_1$。令 $\\gamma = t\\lambda$。优化问题变为：\n$$\\mathrm{prox}_{\\gamma \\lVert \\cdot \\rVert_1}(v) = \\arg\\min_x \\left( \\frac{1}{2} \\sum_{i=1}^n (x_i - v_i)^2 + \\gamma \\sum_{i=1}^n |x_i| \\right)$$\n目标函数是可分的，这意味着我们可以独立地求解每个分量 $x_i$：\n$$x_i^* = \\arg\\min_{x_i} \\left( \\frac{1}{2} (x_i - v_i)^2 + \\gamma |x_i| \\right)$$\n根据次梯度微积分的一阶最优性条件，要求 $0$ 位于目标函数在 $x_i^*$ 处的次微分中：\n$$0 \\in x_i^* - v_i + \\gamma \\cdot \\partial|x_i^*|$$\n其中 $\\partial|\\cdot|$ 是绝对值函数的次微分。\n如果 $x_i^*  0$，则 $\\partial|x_i^*| = \\{1\\}$，所以 $x_i^* - v_i + \\gamma = 0 \\implies x_i^* = v_i - \\gamma$。这仅在 $v_i - \\gamma  0$ 时有效，即 $v_i  \\gamma$。\n如果 $x_i^*  0$，则 $\\partial|x_i^*| = \\{-1\\}$，所以 $x_i^* - v_i - \\gamma = 0 \\implies x_i^* = v_i + \\gamma$。这仅在 $v_i + \\gamma  0$ 时有效，即 $v_i  -\\gamma$。\n如果 $x_i^* = 0$，则 $\\partial|x_i^*| = [-1, 1]$，所以 $v_i \\in \\gamma[-1, 1]$，即 $|v_i| \\le \\gamma$。\n结合这些情况，我们得到解，即著名的软阈值算子 $S_\\gamma(\\cdot)$：\n$$x_i^* = S_\\gamma(v_i) = \\begin{cases} v_i - \\gamma  \\text{if } v_i  \\gamma \\\\ v_i + \\gamma  \\text{if } v_i  -\\gamma \\\\ 0  \\text{if } |v_i| \\le \\gamma \\end{cases}$$\n这可以紧凑地写成 $S_\\gamma(v_i) = \\mathrm{sign}(v_i) \\max(|v_i| - \\gamma, 0)$。\n\n要从 $x^{(0)}$ 开始执行一次近端梯度法迭代，我们首先计算梯度下降更新：\n$$z^{(0)} = x^{(0)} - t\\nabla f(x^{(0)}) = x^{(0)} - t(x^{(0)} - b) = (1-t)x^{(0)} + tb$$\n然后，我们将近端算子应用于 $z^{(0)}$：\n$$x^{(1)} = \\mathrm{prox}_{t g}(z^{(0)}) = S_{t\\lambda}(z^{(0)})$$\n该更新是按元素应用的：$x_i^{(1)} = S_{t\\lambda}(z_i^{(0)})$，对 $i=1, \\dots, n$。这就是待实现的单步更新映射。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the sparse denoising problem for multiple test cases using one\n    iteration of the proximal gradient method.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 8,\n            \"x_true\": np.array([0, 1.5, 0, 0, -2.0, 0, 0.7, 0]),\n            \"noise\": np.array([0.05, -0.08, 0.10, -0.02, 0.03, 0.00, -0.05, 0.04]),\n            \"x0\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"lambda\": 0.4,\n            \"t\": 1.0,\n        },\n        {\n            \"n\": 8,\n            \"x_true\": np.array([0.4, -0.4, 0, 0, 0, 0, 0, 0]),\n            \"noise\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"x0\": np.array([0, 0, 0, 0, 0, 0, 0, 0]),\n            \"lambda\": 0.4,\n            \"t\": 1.0,\n        },\n        {\n            \"n\": 6,\n            \"x_true\": np.array([0, 0, 1.0, 0, -1.2, 0]),\n            \"noise\": np.array([0.0, 0.05, -0.02, 0.00, 0.03, 0.10]),\n            \"x0\": np.array([0.2, 0, 0.5, 0, -0.8, 0]),\n            \"lambda\": 0.6,\n            \"t\": 0.5,\n        },\n        {\n            \"n\": 5,\n            \"x_true\": np.array([0, 0, 0, 2.0, 0]),\n            \"noise\": np.array([0.02, -0.01, 0.00, -0.05, 0.01]),\n            \"x0\": np.array([0, 0, 0, 0, 0]),\n            \"lambda\": 0.2,\n            \"t\": 1.9,\n        },\n    ]\n\n    results = []\n    \n    # Epsilon for support calculation\n    epsilon = 1e-12\n\n    def soft_thresholding(v, gamma):\n        \"\"\"\n        Implementation of the soft-thresholding operator S_gamma(v).\n        This is the proximal operator of the L1 norm.\n        \"\"\"\n        return np.sign(v) * np.maximum(np.abs(v) - gamma, 0)\n\n    def get_support(z, eps):\n        \"\"\"\n        Computes the support of a vector z.\n        supp(z) = { i : |z_i|  eps }\n        \"\"\"\n        return set(np.where(np.abs(z)  eps)[0])\n\n    for case in test_cases:\n        n = case[\"n\"]\n        x_true = case[\"x_true\"]\n        noise = case[\"noise\"]\n        x0 = case[\"x0\"]\n        lam = case[\"lambda\"]\n        t = case[\"t\"]\n        \n        b = x_true + noise\n\n        # Step 1: Gradient descent step on the smooth part f(x)\n        # z = x0 - t * grad_f(x0) = x0 - t*(x0 - b)\n        z = (1 - t) * x0 + t * b\n\n        # Step 2: Proximal step on the non-smooth part g(x)\n        # x1 = prox_{t*g}(z) = prox_{t*lambda*||.||_1}(z)\n        # This is the soft-thresholding operator\n        gamma = t * lam\n        x1 = soft_thresholding(z, gamma)\n\n        # Calculate metrics\n        # 1. MSE\n        mse = np.mean((x1 - x_true)**2)\n        \n        # 2. Support Recall\n        supp_x_true = get_support(x_true, epsilon)\n        supp_x1 = get_support(x1, epsilon)\n        \n        if len(supp_x_true) == 0:\n            # If true support is empty, recall is 1.0 if estimated support is also empty, 0.0 otherwise.\n            recall = 1.0 if len(supp_x1) == 0 else 0.0\n        else:\n            intersection_size = len(supp_x_true.intersection(supp_x1))\n            recall = intersection_size / len(supp_x_true)\n            \n        # 3. Number of nonzeros (nnz)\n        nnz = len(supp_x1)\n        \n        results.append([mse, recall, nnz])\n\n    # Format the final output string exactly as specified.\n    results_str_list = []\n    for res in results:\n        mse_val, recall_val, nnz_val = res\n        results_str_list.append(f\"[{mse_val:.6f},{recall_val:.6f},{nnz_val}]\")\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2897782"}, {"introduction": "近端梯度法的威力在于其能够处理各种各样的正则化项。这个练习 [@problem_id:2897783] 从简单、可分的 $\\ell_1$ 范数转向了不可分的全变分 (Total Variation, TV) 范数，后者在图像和信号处理中对于保持边缘至关重要。您将实现加速的 FISTA 算法的单步迭代，并使用其强大的对偶投影特性来计算 TV 的近端算子，从而深入了解更高级的应用。", "problem": "考虑在信号处理和系统建模的背景下，对一维双像素信号进行全变分（TV）正则化去噪。设观测数据为 $b \\in \\mathbb{R}^{2}$，其中 $b = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$。我们最小化复合目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|x - b\\|_{2}^{2}$ 且 $g(x) = \\lambda \\,\\mathrm{TV}(x)$。对于一个双像素信号，其各向同性全变分为 $\\mathrm{TV}(x) = |x_{2} - x_{1}|$，前向差分算子 $D \\in \\mathbb{R}^{1 \\times 2}$ 为 $D = \\begin{pmatrix} -1  1 \\end{pmatrix}$，因此 $Dx = x_{2} - x_{1}$。\n\n使用一次快速迭代收缩阈值算法（FISTA）的迭代，该算法定义为对 $f$ 进行梯度步，然后是 $g$ 的近端算子，以及一个惯性（动量）更新。使用初始值 $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$y^{(0)} = x^{(0)}$ 以及动量标量 $s_{0} = 1$ 进行初始化。使用步长 $\\tau = 1$ 和正则化参数 $\\lambda = 1$。在你的推导中，从第一性原理出发处理数据保真项的梯度 $\\nabla f(x)$ 和 $g$ 的近端算子，并通过使用与 $D$ 相关的对偶变量的对偶投影特性来实现各向同性 TV 项的近端步骤。\n\n执行恰好一次 FISTA 迭代，明确写出：\n- 输入到近端算子的梯度步结果，\n- 用于计算各向同性 TV 项近端算子的对偶投影，\n- 动量更新和惯性点。\n\n这次 FISTA 迭代返回的 $x^{(1)}$ 的第二个分量的值是多少？请用一个精确的数字表示你的最终答案。", "solution": "对问题陈述进行验证。\n\n逐字提取给定条件：\n- 优化问题涉及一个一维双像素信号。\n- 观测数据为 $b = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} \\in \\mathbb{R}^{2}$。\n- 目标函数为 $F(x) = f(x) + g(x)$，其中 $x \\in \\mathbb{R}^{2}$。\n- 目标函数的光滑部分是数据保真项 $f(x) = \\frac{1}{2}\\|x - b\\|_{2}^{2}$。\n- 非光滑部分是正则化项 $g(x) = \\lambda \\,\\mathrm{TV}(x)$。\n- 对于双像素信号 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$，其全变分为 $\\mathrm{TV}(x) = |x_{2} - x_{1}|$。\n- 前向差分算子为 $D = \\begin{pmatrix} -1  1 \\end{pmatrix}$，使得 $Dx = x_{2} - x_{1}$。\n- 使用的算法是快速迭代收缩阈值算法（FISTA）。\n- 迭代次数恰好为一次。\n- 步长为 $\\tau = 1$。\n- 正则化参数为 $\\lambda = 1$。\n- 初始条件为 $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$y^{(0)} = x^{(0)}$，以及 $s_{0} = 1$。\n- TV 项的近端算子将通过其对偶投影特性来计算。\n- 期望的输出是 $x^{(1)}$ 的第二个分量的值。\n\n该问题在信号处理的凸优化这一成熟领域具有科学依据。全变分正则化和 FISTA 算法是标准的、有据可查的技术。该问题是适定的，为算法单次迭代的唯一且稳定的计算提供了所有必要的参数和初始条件。语言客观而精确。这个双像素信号的玩具问题是一种标准的教学工具，并不代表科学上的缺陷。该问题被认为是有效的。\n\nFISTA 迭代由以下针对迭代 $k$ 的序列定义：\n$1$. $z^{(k)} = y^{(k)} - \\tau \\nabla f(y^{(k)})$\n$2$. $x^{(k+1)} = \\mathrm{prox}_{\\tau g}(z^{(k)})$\n$3$. $s_{k+1} = \\frac{1 + \\sqrt{1 + 4s_k^2}}{2}$\n$4$. $y^{(k+1)} = x^{(k+1)} + \\frac{s_k - 1}{s_{k+1}}(x^{(k+1)} - x^{(k)})$\n\n我们从 $k=0$ 开始执行一次迭代。给定 $x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$y^{(0)} = x^{(0)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$s_0 = 1$，$\\tau=1$ 和 $\\lambda=1$。观测数据为 $b = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$。\n\n首先，我们计算 $f(x) = \\frac{1}{2}\\|x-b\\|_2^2$ 的梯度。梯度为 $\\nabla f(x) = x-b$。\n我们在惯性点 $y^{(0)}$ 处计算梯度：\n$$\n\\nabla f(y^{(0)}) = y^{(0)} - b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n\n接下来，我们执行梯度下降步骤，以找到近端算子的输入 $z^{(0)}$：\n$$\nz^{(0)} = y^{(0)} - \\tau \\nabla f(y^{(0)}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ 0-(-2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}\n$$\n这个向量 $z^{(0)}$ 是近端算子的输入。\n\n现在我们计算近端步骤，$x^{(1)} = \\mathrm{prox}_{\\tau g}(z^{(0)})$。函数为 $g(x) = \\lambda |Dx|$。需要解决的问题是：\n$$\nx^{(1)} = \\arg\\min_x \\left\\{ \\frac{1}{2}\\|x - z^{(0)}\\|_2^2 + \\tau \\lambda |Dx| \\right\\}\n$$\n按照指定，我们使用对偶投影特性。形如 $\\min_x \\frac{1}{2}\\|x-z\\|_2^2 + \\gamma \\|Dx\\|_1$ 的问题的解由 $x^* = z - D^T p^*$ 给出，其中 $p^*$ 是一个相关对偶问题的解。对于这种结构，$p^*$ 可以通过投影找到。通用解是：\n$$\nx^* = z - D^T \\mathrm{proj}_{\\mathcal{B}} \\left( (DD^T)^{-1} Dz \\right)\n$$\n其中 $\\mathcal{B}$ 是对应于 $\\gamma \\|\\cdot\\|_1$ 的对偶范数球。$\\ell_1$ 范数的对偶范数是 $\\ell_\\infty$ 范数。在我们用于 $D$ 的一维输出空间中，这只是一个区间。这个球是 $\\mathcal{B} = \\{p \\in \\mathbb{R} : |p| \\le \\gamma \\}$。在这里，$\\gamma = \\tau \\lambda = 1 \\cdot 1 = 1$。用于投影的集合是 $\\{p \\in \\mathbb{R} : |p| \\le 1\\}$，即区间 $[-1, 1]$。\n\n我们计算必要的组成部分：\n算子是 $D = \\begin{pmatrix} -1  1 \\end{pmatrix}$，所以其转置是 $D^T = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$。\n项 $DD^T$ 是：\n$$\nDD^T = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = (-1)(-1) + (1)(1) = 2\n$$\n其逆是 $(DD^T)^{-1} = \\frac{1}{2}$。\n项 $Dz^{(0)}$ 是：\n$$\nDz^{(0)} = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = (-1)(0) + (1)(2) = 2\n$$\n投影的参数是 $(DD^T)^{-1} Dz^{(0)} = \\frac{1}{2} \\cdot 2 = 1$。\n\n对偶投影是将这个值投影到区间 $[-1, 1]$ 上。\n$$\np^* = \\mathrm{proj}_{[-1,1]}(1)\n$$\n由于值 $1$ 已经在区间 $[-1, 1]$ 内，投影结果就是 $1$。因此，$p^* = 1$。\n\n现在我们计算 $x^{(1)}$：\n$$\nx^{(1)} = z^{(0)} - D^T p^* = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} (1) = \\begin{pmatrix} 0 - (-1) \\\\ 2 - 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n\n最后，我们执行动量更新。首先，我们将标量 $s_0=1$ 更新为 $s_1$：\n$$\ns_1 = \\frac{1 + \\sqrt{1 + 4s_0^2}}{2} = \\frac{1 + \\sqrt{1 + 4(1)^2}}{2} = \\frac{1 + \\sqrt{5}}{2}\n$$\n然后我们计算下一个惯性点 $y^{(1)}$：\n$$\ny^{(1)} = x^{(1)} + \\frac{s_0 - 1}{s_1}(x^{(1)} - x^{(0)})\n$$\n由于 $s_0=1$，项 $s_0-1=0$。这显著简化了计算：\n$$\ny^{(1)} = x^{(1)} + \\frac{1 - 1}{s_1}(x^{(1)} - x^{(0)}) = x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n\n问题要求的是 $x^{(1)}$ 的第二个分量的值。\n根据我们的计算，$x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。第二个分量是 $1$。", "answer": "$$\\boxed{1}$$", "id": "2897783"}, {"introduction": "一个迭代算法只有在您知道何时停止它时才具有实用价值。最后一个练习 [@problem_id:2897755] 深入探讨了为近端梯度法设计有效停止准则的关键课题。通过分析三种不同的方法——相对目标函数下降、梯度映射范数和对偶间隙——您将学会评估计算成本、数值稳定性以及收敛性理论保证之间的权衡。", "problem": "您正在为稀疏恢复问题实现一种带有回溯功能的近端梯度法，该方法使用最小绝对收缩和选择算子 (LASSO)，即，您求解的是复合凸优化问题\n$$\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\triangleq f(x) + g(x), \\quad f(x) \\triangleq \\tfrac{1}{2}\\|A x - y\\|_2^2, \\quad g(x) \\triangleq \\lambda \\|x\\|_1,$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^m$，且 $\\lambda  0$。假设 $f$ 是凸函数且连续可微，其梯度是利普希茨连续的，而 $g$ 是凸的、真凸且下半连续的。您希望选择一个在理论上有意义且计算上高效的实用停止准则。考虑基于以下几点构建的准则：相对目标函数下降、梯度映射范数以及对偶间隙（当其可计算时）。请选择所有同时给出了准则的正确且可实现的定义，并正确陈述了在近端梯度法进行稀疏恢复背景下的一个主要优点和一个局限性的选项。\n\nA. 相对目标函数下降：在第一个满足以下条件的迭代索引 $k$ 处停止\n$$\\frac{F(x^{k}) - F(x^{k+1})}{\\max\\{F(x^{k}), \\, 1\\}} \\le \\varepsilon_{\\mathrm{rel}}, \\quad \\varepsilon_{\\mathrm{rel}} \\in (0,1).$$\n优点：在回溯线搜索下，由于 $F(x^{k})$ 和 $F(x^{k+1})$ 已经可用，因此每次迭代的评估成本低廉。局限性：当 $F$ 由于强正则化或病态条件而变得平坦时，即使一阶最优性残差还不小，它也可能过早地判断收敛。\n\nB. 梯度映射范数：使用 $\\nabla f$ 的利普希茨常数的回溯估计 $L_k  0$，定义梯度映射\n$$G_{L_k}(x^{k}) \\triangleq L_k\\!\\left(x^{k} - \\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}\\!\\left(x^{k} - \\tfrac{1}{L_k}\\,A^\\top(Ax^{k} - y)\\right)\\right).$$\n当\n$$\\|G_{L_k}(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{abs}} \\quad \\text{或} \\quad \\|G_{L_k}(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{rel}} \\max\\{1,\\|x^{k}\\|_2\\},$$\n时停止，其中容差 $\\varepsilon_{\\mathrm{abs}}, \\varepsilon_{\\mathrm{rel}}  0$。优点：$\\|G_{L_k}(x^{k})\\|_2 = 0$ 当且仅当复合一阶最优性条件 $0 \\in \\nabla f(x) + \\partial g(x)$ 成立时，因此它直接衡量了非光滑问题的平稳性。局限性：它依赖于 $L_k$ 的局部选择，并且除非从迭代中重用，否则会产生额外的近端评估成本，这可能会轻微增加每次迭代的成本。\n\nC. 对偶间隙：通过重新缩放残差 $\\theta^{k} \\triangleq \\tau^{k}(A x^{k} - y)$ 来构造一个对偶可行向量，其中\n$$\\tau^{k} \\triangleq \\min\\!\\left\\{1, \\frac{\\lambda}{\\|A^\\top(Ax^{k} - y)\\|_\\infty}\\right\\}$$\n使得 $\\|A^\\top \\theta^{k}\\|_\\infty \\le \\lambda$。定义对偶目标函数\n$$d(\\theta) \\triangleq -\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta,$$\n它对所有满足 $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$ 的 $\\theta$ 均有效，并计算对偶间隙\n$$\\mathrm{gap}(x^{k},\\theta^{k}) \\triangleq F(x^{k}) - d(\\theta^{k}) = \\tfrac{1}{2}\\|A x^{k} - y\\|_2^2 + \\lambda\\|x^{k}\\|_1 + \\tfrac{1}{2}\\|\\theta^{k}\\|_2^2 + y^\\top \\theta^{k}.$$\n当 $\\mathrm{gap}(x^{k},\\theta^{k}) \\le \\varepsilon_{\\mathrm{abs}}$ 或 $\\mathrm{gap}(x^{k},\\theta^{k}) \\le \\varepsilon_{\\mathrm{rel}} F(x^{k})$ 时停止。优点：对偶间隙是原始目标函数 $\\varepsilon$-最优性的一个尺度敏感的证书。局限性：获取 $\\theta^{k}$ 需要计算 $A^\\top(Ax^{k} - y)$ 并保持对偶可行性，如果只提供梯度预言机或 $A$ 是隐式访问的，这可能是不可用或相对昂贵的。\n\nD. 光滑代理的梯度范数检验：当 $\\|\\nabla f(x^{k})\\|_2 \\le \\varepsilon_{\\mathrm{abs}}$ 时停止。优点：这是经典的光滑最优性检验，且计算成本低；非光滑项 $g$ 不改变一阶平稳性。局限性：在此设置中没有本质上的局限性。\n\nE. 约束变体捷径：对于基追踪问题 $\\min_{x}\\|x\\|_1 \\;\\text{subject to}\\; A x = y$，当通过添加二次惩罚并应用近端梯度法求解时，可以使用代理“对偶间隙” $\\tfrac{1}{2}\\|A x^{k} - y\\|_2^2$ 来停止，而无需构造对偶可行点；该量是约束问题真实对偶间隙的上界，因此可以证明次优性。优点：无需对偶计算。局限性：仅必须选择惩罚参数。", "solution": "我们从复合凸优化问题 $F(x) \\triangleq f(x) + g(x)$ 开始，其中 $f$ 是凸的、可微的，并且其梯度对于某个 $L  0$ 是 $L$-利普希茨连续的，而 $g$ 是凸的、真凸且下半连续的。近端梯度法使用由 $\\operatorname{prox}_{\\alpha g}(v) \\triangleq \\arg\\min_{x}\\{g(x) + \\tfrac{1}{2\\alpha}\\|x - v\\|_2^2\\}$ 定义的近端算子，步长为 $\\alpha_k = 1/L_k$ 来生成迭代序列\n$$x^{k+1} = \\operatorname{prox}_{\\alpha_k g}\\!\\left(x^{k} - \\alpha_k \\nabla f(x^{k})\\right).$$\n对于 $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ 和 $g(x) = \\lambda\\|x\\|_1$，我们有 $\\nabla f(x) = A^\\top(Ax - y)$ 并且 $\\operatorname{prox}_{\\alpha \\lambda \\|\\cdot\\|_1}$ 是在水平 $\\alpha \\lambda$ 上的软阈值算子。\n\n一种基于原则的停止准则设计必须与以下三类之一相关：(i) $F$ 的单调下降，(ii) 复合问题的一阶最优性，或 (iii) 通过强对偶性的原始-对偶最优性。我们现在对这三类进行论证。\n\n相对目标函数下降。在强制执行下降条件的回溯线搜索下，我们对所有 $k$ 都有一个非增序列 $F(x^{k})$。相对下降检验使用\n$$\\frac{F(x^{k}) - F(x^{k+1})}{\\max\\{F(x^{k}), \\, 1\\}} \\le \\varepsilon_{\\mathrm{rel}}.$$\n分子反映了实际的进展，分母对数值进行归一化以防止被一个非常小的值除；当 $F(x^{k})$ 很小时，使用 $\\max\\{F(x^{k}),1\\}$ 提供了数值稳定性。这个准则的成本很低，因为为了验证回溯条件，已经评估了 $f(x^{k})$ 和 $g(x^{k})$。然而，在 $F$ 变得平坦的病态或重度正则化问题中，即使远离平稳点，也可能出现小的相对下降，并且它不提供关于一阶残差或原始-对偶间隙的保证。因此，即使最优性残差还不小，它也可能提前停止。\n\n梯度映射范数。复合一阶最优性条件是\n$$0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star),$$\n其中 $\\partial g$ 是 $g$ 的次微分。在 $x$ 处带参数 $L  0$ 的梯度映射定义为\n$$G_L(x) \\triangleq L\\left(x - \\operatorname{prox}_{g/L}\\!\\left(x - \\tfrac{1}{L}\\nabla f(x)\\right)\\right).$$\n通过 Moreau 分解和近端步骤的一阶最优性，可以证明 $G_L(x) = 0$ 当且仅当 $0 \\in \\nabla f(x) + \\partial g(x)$，即 $x$ 是一个一阶平稳点（由于凸性，这里是一个最小值点）。在我们的 LASSO 设置中，对于 $g(x) = \\lambda\\|x\\|_1$，这变为\n$$G_{L_k}(x^{k}) = L_k\\!\\left(x^{k} - \\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}\\!\\left(x^{k} - \\tfrac{1}{L_k}A^\\top(Ax^{k} - y)\\right)\\right).$$\n因此，$\\|G_{L_k}(x^{k})\\|_2$ 可作为一个有原则的残差。它直接衡量了复合意义下的平稳性违背程度，不像 $\\|\\nabla f(x^{k})\\|_2$ 那样忽略了 $g$。由于近端梯度更新已经计算了在 $x^{k}$ 处的 $\\operatorname{prox}_{(\\lambda/L_k)\\|\\cdot\\|_1}$ 以形成 $x^{k+1}$，因此可以通过重用该计算以基本无额外成本的方式获得映射，尽管如果单独实现，则会产生额外的近端评估。它确实依赖于 $L_k$ 的选择；然而，$G_{L_k}$ 的零点集独立于 $L_k  0$，所以它仍然是一个有效的残差。\n\n对偶间隙。对于 LASSO，我们从 Fenchel 对偶性推导出其凸对偶问题。使用表示\n$$\\tfrac{1}{2}\\|A x - y\\|_2^2 = \\sup_{\\theta \\in \\mathbb{R}^m}\\left\\{\\theta^\\top(Ax - y) - \\tfrac{1}{2}\\|\\theta\\|_2^2\\right\\},$$\n我们写出类拉格朗日鞍点形式\n$$\\min_{x}\\left[\\sup_{\\theta}\\left\\{\\theta^\\top(Ax - y) - \\tfrac{1}{2}\\|\\theta\\|_2^2\\right\\} + \\lambda\\|x\\|_1\\right] = \\sup_{\\theta}\\left\\{-\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta + \\min_{x}\\left[\\theta^\\top A x + \\lambda\\|x\\|_1\\right]\\right\\}.$$\n内部的最小值是有限的当且仅当 $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$，此时其值为 $0$；否则为 $-\\infty$。因此，对偶问题是\n$$\\max_{\\theta \\in \\mathbb{R}^m}\\; d(\\theta) \\triangleq -\\tfrac{1}{2}\\|\\theta\\|_2^2 - y^\\top \\theta \\quad \\text{subject to} \\quad \\|A^\\top \\theta\\|_\\infty \\le \\lambda.$$\n强对偶性成立，因为 $f$ 和 $g$ 是闭的、真凸、凸的，并且 Slater 条件得到满足。对于任何原始变量 $x$ 和对偶可行变量 $\\theta$，我们有 $d(\\theta) \\le F(x)$，并且对偶间隙\n$$\\mathrm{gap}(x,\\theta) \\triangleq F(x) - d(\\theta) \\ge 0$$\n量化了次优性，并在最优时等于 $0$。给定 $x^{k}$，我们可以通过重新缩放残差 $r^{k} \\triangleq A x^{k} - y$ 来产生一个可行的 $\\theta^{k}$：\n$$\\theta^{k} \\triangleq \\tau^{k} r^{k}, \\quad \\tau^{k} \\triangleq \\min\\!\\left\\{1, \\frac{\\lambda}{\\|A^\\top r^{k}\\|_\\infty}\\right\\},$$\n这保证了 $\\|A^\\top \\theta^{k}\\|_\\infty \\le \\lambda$。然后间隙的计算结果为\n$$\\mathrm{gap}(x^{k},\\theta^{k}) = \\tfrac{1}{2}\\|r^{k}\\|_2^2 + \\lambda\\|x^{k}\\|_1 + \\tfrac{1}{2}\\|\\theta^{k}\\|_2^2 + y^\\top \\theta^{k}.$$\n当此值低于 $\\varepsilon_{\\mathrm{abs}}$ 时停止，以证书意义上给出了一个 $\\varepsilon_{\\mathrm{abs}}$-精确的原始目标值。计算 $\\theta^{k}$ 需要 $A^\\top r^{k}$ 以及 $r^{k}$，这通常比计算 $\\nabla f(x^{k}) = A^\\top r^{k}$ 多一次与 $A^\\top$ 的乘法。如果只将 $\\nabla f$ 作为黑箱提供，而没有单独访问 $A$ 和 $A^\\top$ 的权限，或者如果模型推广到最小二乘之外，可能无法获得简单的对偶构造。\n\n我们现在评估每个选项。\n\n选项 A。公式利用了在回溯下 $F(x^{k})$ 的单调性。优点是准确的：它重用了已计算的量并且成本低廉。局限性也是准确的：在强正则化或条件不佳的情况下，在达到真正平稳之前可能会出现小的相对下降，并且它不限制任何最优性残差。结论：正确。\n\n选项 B。映射 $G_{L_k}(x^{k})$ 对复合模型定义正确，其范数是一个有原则的平稳性残差，当且仅当复合最优性条件成立时为零。陈述的优点是正确的。局限性是合理的：它依赖于局部的 $L_k$，并且除非重用，否则可能涉及额外的近端计算。结论：正确。\n\n选项 C。对偶问题对于 LASSO 陈述正确，可行性条件 $\\|A^\\top \\theta\\|_\\infty \\le \\lambda$ 是正确的，并且重新缩放 $\\theta^{k} = \\tau^{k}(A x^{k} - y)$ 产生一个可行的对偶点。对偶间隙表达式 $\\mathrm{gap}(x^{k},\\theta^{k}) = F(x^{k}) - d(\\theta^{k})$ 展开为给定形式，并且是一个有效的证书：如果它 $\\le \\varepsilon$，那么 $F(x^{k}) - F^\\star \\le \\varepsilon$。优点是准确的。局限性也是准确的：计算和强制执行 $\\theta^{k}$ 的可行性需要访问 $A^\\top(Ax^{k} - y)$，并且在广义模型中可能不那么简单。结论：正确。\n\n选项 D。使用 $\\|\\nabla f(x^{k})\\|_2$ 作为停止检验忽略了非光滑项 $g(x) = \\lambda\\|x\\|_1$。一般来说，复合最优性条件是 $0 \\in \\nabla f(x) + \\partial g(x)$，而不是 $\\nabla f(x) = 0$。在 LASSO 最优解处，通常有 $\\nabla f(x^\\star) \\neq 0$，而通过 $\\ell_1$ 范数的次梯度，$-\\nabla f(x^\\star) \\in \\partial g(x^\\star)$ 成立。因此，这个检验可能具有误导性，并在真实解处声明不收敛，或在远离真实解处声明收敛。声称“非光滑项不改变一阶平稳性”是错误的。结论：不正确。\n\n选项 E。对于约束的基追踪问题 $\\min_{x}\\|x\\|_1 \\;\\text{s.t.}\\; A x = y$，真实的对偶问题涉及对对偶向量的约束和强对偶性；$\\tfrac{1}{2}\\|A x^{k} - y\\|_2^2$ 仅仅是约束的可行性残差的平方，并不是一个对偶间隙。在惩罚方法中，该残差与约束违背程度有关，但没有对偶可行点和适当的拉格朗日乘子，它不能证明约束问题的最优性。因此，声称它“上界了真实的对偶间隙”并“证明了次优性”是不正确的。结论：不正确。\n\n因此，正确的选项是 A、B 和 C。", "answer": "$$\\boxed{ABC}$$", "id": "2897755"}]}