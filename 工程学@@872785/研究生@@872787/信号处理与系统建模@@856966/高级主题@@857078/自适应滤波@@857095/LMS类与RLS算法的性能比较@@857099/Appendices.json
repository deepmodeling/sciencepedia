{"hands_on_practices": [{"introduction": "在任何自适应算法的实际应用中，确保其稳定性是首要任务。本练习将引导您使用能量守恒关系，从第一性原理推导出 LMS 算法的均方稳定性边界。通过这个推导，您将深入理解算法的步长参数 $\\mu$、滤波器长度 $M$ 以及输入信号统计特性（如 $\\sigma_x^2$）是如何共同决定算法行为的，从而为选择合适的步长提供坚实的理论基础 [@problem_id:2891085]。", "problem": "考虑用于基准测试最小均方 (LMS) 类型算法相对于递归最小二乘 (RLS) 算法收敛行为的经典自适应辨识设置。一个实值、零均值、高斯、白噪声的输入行回归量 $u_i \\in \\mathbb{R}^{1 \\times M}$，其协方差为 $R = \\sigma_x^2 I$，驱动一个具有系数向量 $w^\\circ \\in \\mathbb{R}^M$ 的未知线性系统。期望响应为 $d(i) = u_i w^\\circ + v(i)$，其中 $v(i)$ 是零均值测量噪声，其独立于 $\\{u_i\\}$，方差为 $\\sigma_v^2$。采用具有恒定步长 $\\mu$ 的 LMS 递归来估计 $w^\\circ$，并且在当前回归量 $u_i$ 与过去的权重误差向量之间采用独立性假设 (IA)。使用能量守恒关系观点，根据第一性原理推导在所述白噪声输入模型下，保证均方稳定性（即权重误差能量的二阶矩有界）的最大恒定步长 $\\mu_{\\max}$。用 $M$ 和 $\\sigma_x^2$ 将您的最终答案表示为一个闭式解析表达式。不需要数值代入。步长是无量纲的，因此不需要物理单位。您的最终答案必须是单个解析表达式。", "solution": "此问题需要经过验证。\n\n**步骤 1：提取已知条件**\n- 输入回归量：$u_i \\in \\mathbb{R}^{1 \\times M}$ 是一个实值、零均值、高斯、白噪声的输入行回归量。\n- 输入协方差：$R = \\sigma_x^2 I$，其中 $R = E[u_i^T u_i]$。\n- 未知系统：具有系数向量 $w^\\circ \\in \\mathbb{R}^M$ 的线性系统。注意，为了使乘积 $u_i w^\\circ$ 为标量，$w^\\circ$ 必须是列向量，即 $w^\\circ \\in \\mathbb{R}^{M \\times 1}$。\n- 期望响应：$d(i) = u_i w^\\circ + v(i)$。\n- 测量噪声：$v(i)$ 是零均值，方差为 $\\sigma_v^2$，且独立于 $\\{u_i\\}$。\n- 算法：具有恒定步长 $\\mu$ 的 LMS 递归。\n- 假设：采用独立性假设 (IA)，意味着当前回归量 $u_i$ 在统计上独立于过去的权重误差向量。\n- 方法论：使用能量守恒关系观点。\n- 目标：推导保证均方稳定性的最大恒定步长 $\\mu_{\\max}$。\n- 稳定性判据：权重误差能量的二阶矩的有界性。\n- 最终表达式：用 $M$ 和 $\\sigma_x^2$ 表示。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据**：该问题是自适应滤波器理论中的一个标准、典型的练习。所有概念——LMS、系统辨识、均方稳定性、独立性假设和能量守恒方法——都是信号处理领域的基础。该问题在科学上是合理的。\n- **适定性**：该问题是适定的。它提供了一套完整的条件和假设（高斯白噪声输入、IA、LMS 更新），以推导稳定界限的唯一解析表达式。\n- **客观性**：问题陈述是精确、定量的，没有任何主观或模糊的语言。\n\n**步骤 3：结论与行动**\n该问题是有效的。需要从第一性原理进行严格的推导。\n\n**求解推导**\n令 $w(i) \\in \\mathbb{R}^{M \\times 1}$ 表示在迭代次数 $i$ 时对权重向量 $w^\\circ$ 的估计。权重误差向量定义为 $\\tilde{w}(i) = w^\\circ - w(i)$。LMS 算法根据以下递归更新权重向量：\n$$w(i) = w(i-1) + \\mu u_i^T e(i)$$\n其中 $u_i^T \\in \\mathbb{R}^{M \\times 1}$ 是输入行回归量的转置，$e(i)$ 是输出估计误差：\n$$e(i) = d(i) - u_i w(i-1)$$\n代入 $d(i)$ 的模型，我们得到：\n$$e(i) = (u_i w^\\circ + v(i)) - u_i w(i-1) = u_i (w^\\circ - w(i-1)) + v(i) = u_i \\tilde{w}(i-1) + v(i)$$\n项 $e_a(i) = u_i \\tilde{w}(i-1)$ 是先验估计误差。\n\n通过从 $w^\\circ$ 中减去 LMS 更新方程，得到权重误差向量的递归式：\n$$w^\\circ - w(i) = w^\\circ - w(i-1) - \\mu u_i^T e(i)$$\n$$\\tilde{w}(i) = \\tilde{w}(i-1) - \\mu u_i^T e(i) = \\tilde{w}(i-1) - \\mu u_i^T (u_i \\tilde{w}(i-1) + v(i))$$\n$$\\tilde{w}(i) = (I - \\mu u_i^T u_i) \\tilde{w}(i-1) - \\mu v(i) u_i^T$$\n其中 $I$ 是 $M \\times M$ 单位矩阵。\n\n能量守恒方法研究权重误差向量的欧几里得范数平方 $\\|\\tilde{w}(i)\\|^2 = \\tilde{w}(i)^T \\tilde{w}(i)$ 的演化。\n$$\\|\\tilde{w}(i)\\|^2 = \\left[ (I - \\mu u_i^T u_i) \\tilde{w}(i-1) - \\mu v(i) u_i^T \\right]^T \\left[ (I - \\mu u_i^T u_i) \\tilde{w}(i-1) - \\mu v(i) u_i^T \\right]$$\n展开此表达式可得：\n$$\\|\\tilde{w}(i)\\|^2 = \\tilde{w}(i-1)^T (I - \\mu u_i^T u_i)^T (I - \\mu u_i^T u_i) \\tilde{w}(i-1) - 2\\mu v(i) \\tilde{w}(i-1)^T (I - \\mu u_i^T u_i) u_i^T + \\mu^2 v(i)^2 u_i u_i^T$$\n矩阵 $u_i^T u_i$ 是对称的。标量积 $u_i u_i^T$ 等于范数的平方 $\\|u_i\\|^2$。\n$$(I - \\mu u_i^T u_i)^2 = I - 2\\mu u_i^T u_i + \\mu^2 (u_i^T u_i)^2 = I - 2\\mu u_i^T u_i + \\mu^2 u_i^T (u_i u_i^T) u_i = I - (2\\mu - \\mu^2 \\|u_i\\|^2) u_i^T u_i$$\n所以第一项变为：\n$$\\tilde{w}(i-1)^T (I - (2\\mu - \\mu^2 \\|u_i\\|^2) u_i^T u_i) \\tilde{w}(i-1) = \\|\\tilde{w}(i-1)\\|^2 - (2\\mu - \\mu^2 \\|u_i\\|^2) (u_i \\tilde{w}(i-1))^2$$\n$$= \\|\\tilde{w}(i-1)\\|^2 - (2\\mu - \\mu^2 \\|u_i\\|^2) e_a(i)^2$$\n交叉项是：\n$$-2\\mu v(i) \\tilde{w}(i-1)^T (u_i^T - \\mu u_i^T u_i u_i^T) = -2\\mu v(i) (e_a(i) - \\mu e_a(i) \\|u_i\\|^2) = -2\\mu v(i) e_a(i) (1 - \\mu\\|u_i\\|^2)$$\n最后一项是 $\\mu^2 v(i)^2 \\|u_i\\|^2$。\n将这些组合起来，得到单次迭代的精确能量关系：\n$$\\|\\tilde{w}(i)\\|^2 = \\|\\tilde{w}(i-1)\\|^2 - (2\\mu - \\mu^2 \\|u_i\\|^2) e_a(i)^2 - 2\\mu v(i) e_a(i) (1 - \\mu\\|u_i\\|^2) + \\mu^2 v(i)^2 \\|u_i\\|^2$$\n\n均方稳定性要求均方误差能量 $J_e(i) = E[\\|\\tilde{w}(i)\\|^2]$ 的有界性。我们对能量关系取期望：\n$$J_e(i) = J_e(i-1) - E[(2\\mu - \\mu^2 \\|u_i\\|^2) e_a(i)^2] - E[2\\mu v(i) e_a(i) (1 - \\mu\\|u_i\\|^2)] + E[\\mu^2 v(i)^2 \\|u_i\\|^2]$$\n我们根据问题的假设，逐项计算每个期望值。\n1. 涉及 $v(i)$ 的交叉项：由于 $v(i)$ 是零均值且独立于 $u_i$ 和 $\\tilde{w}(i-1)$（因此也独立于 $e_a(i)$），该项为零。\n$$E[2\\mu v(i) e_a(i) (1 - \\mu\\|u_i\\|^2)] = 2\\mu E[v(i)] E[e_a(i) (1 - \\mu\\|u_i\\|^2)] = 0$$\n2. 噪声功率项：由于 $v(i)$ 独立于 $u_i$，所以 $E[v(i)^2] = \\sigma_v^2$。\n$$E[\\mu^2 v(i)^2 \\|u_i\\|^2] = \\mu^2 E[v(i)^2] E[\\|u_i\\|^2] = \\mu^2 \\sigma_v^2 E[\\text{Tr}(u_i^T u_i)] = \\mu^2 \\sigma_v^2 \\text{Tr}(E[u_i^T u_i]) = \\mu^2 \\sigma_v^2 \\text{Tr}(R)$$\n给定 $R = \\sigma_x^2 I$，我们有 $\\text{Tr}(R) = M\\sigma_x^2$。该项为 $\\mu^2 \\sigma_v^2 M \\sigma_x^2$。\n3. 主误差项：$E[(2\\mu - \\mu^2 \\|u_i\\|^2) e_a(i)^2]$。我们使用独立性假设 (IA)，该假设表明 $u_i$ 独立于 $\\tilde{w}(i-1)$。\n$$E[(2\\mu - \\mu^2 \\|u_i\\|^2) (u_i \\tilde{w}(i-1))^2] = E[\\tilde{w}(i-1)^T \\{ E[(2\\mu - \\mu^2 \\|u_i\\|^2) u_i^T u_i] \\} \\tilde{w}(i-1)]$$\n内部期望为 $2\\mu E[u_i^T u_i] - \\mu^2 E[\\|u_i\\|^2 u_i^T u_i]$。\n我们知道 $E[u_i^T u_i] = R = \\sigma_x^2 I$。\n对于四阶矩 $E[\\|u_i\\|^2 u_i^T u_i]$，我们利用 $u_i$ 的分量（比如 $u_k$）是独立同分布的零均值高斯随机变量，其方差为 $\\sigma_x^2$。对于 $\\|u_i\\|^2 u_i^T u_i$ 的一般矩阵元素 $(j, l)$：\n$E[(\\sum_{k=1}^M u_k^2) u_j u_l]$。\n如果 $j \\neq l$，由于高斯变量奇数阶矩的对称性，期望为零。\n如果 $j=l$，我们需要计算 $E[(\\sum_{k=1}^M u_k^2) u_j^2] = \\sum_{k=1}^M E[u_k^2 u_j^2]$。\n对于 $k \\neq j$，$E[u_k^2 u_j^2] = E[u_k^2] E[u_j^2] = (\\sigma_x^2)(\\sigma_x^2) = \\sigma_x^4$。共有 $M-1$ 个这样的项。\n对于 $k=j$，$E[u_j^4] = 3(E[u_j^2])^2 = 3(\\sigma_x^2)^2 = 3\\sigma_x^4$。\n因此，对角线元素为 $E[(\\sum_{k=1}^M u_k^2) u_j^2] = (M-1)\\sigma_x^4 + 3\\sigma_x^4 = (M+2)\\sigma_x^4$。\n这意味着 $E[\\|u_i\\|^2 u_i^T u_i] = (M+2)\\sigma_x^4 I$。\n代回：\n$E[(2\\mu - \\ldots) u_i^T u_i] = 2\\mu(\\sigma_x^2 I) - \\mu^2((M+2)\\sigma_x^4 I) = [2\\mu\\sigma_x^2 - \\mu^2(M+2)\\sigma_x^4]I$。\n完整的期望为：\n$$E[\\ldots] = [2\\mu\\sigma_x^2 - \\mu^2(M+2)\\sigma_x^4] E[\\tilde{w}(i-1)^T I \\tilde{w}(i-1)] = [2\\mu\\sigma_x^2 - \\mu^2(M+2)\\sigma_x^4] J_e(i-1)$$\n综合所有项，均方误差能量的递归式为：\n$$J_e(i) = J_e(i-1) - [2\\mu\\sigma_x^2 - \\mu^2(M+2)\\sigma_x^4] J_e(i-1) + \\mu^2 M \\sigma_x^2 \\sigma_v^2$$\n$$J_e(i) = [1 - 2\\mu\\sigma_x^2 + \\mu^2(M+2)\\sigma_x^4] J_e(i-1) + \\mu^2 M \\sigma_x^2 \\sigma_v^2$$\n为了使 $J_e(i)$ 在 $i \\to \\infty$ 时保持有界，$J_e(i-1)$ 的系数的绝对值必须小于 $1$：\n$$|1 - 2\\mu\\sigma_x^2 + \\mu^2(M+2)\\sigma_x^4|  1$$\n这等价于两个不等式：\n$$-1  1 - 2\\mu\\sigma_x^2 + \\mu^2(M+2)\\sigma_x^4 \\quad \\text{和} \\quad 1 - 2\\mu\\sigma_x^2 + \\mu^2(M+2)\\sigma_x^4  1$$\n我们先分析第二个不等式，因为它通常是限制性的：\n$$1 - 2\\mu\\sigma_x^2 + \\mu^2(M+2)\\sigma_x^4  1$$\n$$-2\\mu\\sigma_x^2 + \\mu^2(M+2)\\sigma_x^4  0$$\n由于 $\\mu > 0$ 且 $\\sigma_x^2 > 0$，我们可以除以 $\\mu\\sigma_x^2$：\n$$-2 + \\mu(M+2)\\sigma_x^2  0$$\n$$\\mu(M+2)\\sigma_x^2  2$$\n$$\\mu  \\frac{2}{(M+2)\\sigma_x^2}$$\n现在，对于第一个不等式：\n$$-1  1 - 2\\mu\\sigma_x^2 + \\mu^2(M+2)\\sigma_x^4$$\n$$0  2 - 2\\mu\\sigma_x^2 + \\mu^2(M+2)\\sigma_x^4$$\n这是一个关于 $\\mu$ 的二次不等式，形式为 $A\\mu^2 + B\\mu + C > 0$，其中 $A=(M+2)\\sigma_x^4 > 0$。判别式为 $\\Delta = B^2 - 4AC = (-2\\sigma_x^2)^2 - 4((M+2)\\sigma_x^4)(2) = 4\\sigma_x^4 - 8(M+2)\\sigma_x^4 = 4\\sigma_x^4(1-2M-4) = -4\\sigma_x^4(2M+3)$。由于 $M \\ge 1$，所以 $\\Delta  0$。一个首项系数为正且判别式为负的二次函数恒为正。因此，这个不等式对所有实数 $\\mu$ 都成立。\n\n均方稳定性的唯一条件是 $\\mu  \\frac{2}{(M+2)\\sigma_x^2}$。由于算法要进行自适应，$\\mu$ 必须为正，因此界限为 $0  \\mu  \\frac{2}{(M+2)\\sigma_x^2}$。\n因此，最大恒定步长为：\n$$\\mu_{\\max} = \\frac{2}{(M+2)\\sigma_x^2}$$\n这个结果是高斯输入模型和独立性假设的直接推论，由能量守恒关系严格推导得出。", "answer": "$$\\boxed{\\frac{2}{(M+2)\\sigma_x^2}}$$", "id": "2891085"}, {"introduction": "理论上的收敛性分析无法完全揭示算法在实际运行初期的动态行为。本编程练习将通过仿真，直观地对比 LMS 算法平滑、单调的收敛轨迹与 RLS 算法快速但可能非单调的收敛过程。您将亲手研究 RLS 的初始化参数 $\\delta$ 如何影响其早期的瞬态超调现象，这是在实际应用中部署 RLS 算法时一个不可忽视的关键因素 [@problem_id:2891050]。", "problem": "您将实现并比较两种用于线性时不变系统的自适应辨识算法：最小均方 (LMS) 和递归最小二乘 (RLS)。目标是量化 RLS 信息矩阵 $P(0)=\\delta^{-1} I$ 中的初始化参数 $\\delta$ 如何影响沿协方差特征模式的早期超调，并将其与 LMS 在保守步长下获得的模式逐一单调衰减进行对比。\n\n推导的基本依据：\n- 在线性系统辨识中，回归量为零均值、独立同分布的高斯回归量，模型为 $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$，其中 $R$ 是对称正定协方差矩阵。\n- 期望响应为 $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$，其中 $\\mathbf{w}^\\star$ 是一个固定的未知参数矢量，且无测量噪声。\n- 最小均方 (LMS) 算法基于瞬时平方误差的梯度下降，初始化为 $\\mathbf{w}(0)=\\mathbf{0}$。\n- 递归最小二乘 (RLS) 算法，具有指数遗忘因子 $\\lambda_{\\mathrm{RLS}} \\in (0,1]$，初始化为 $\\mathbf{w}(0)=\\mathbf{0}$ 和 $P(0)=\\delta^{-1} I$。\n- $R$ 的特征分解为 $R = U \\Lambda U^\\top$，其中 $\\Lambda=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ 且 $U$ 是标准正交的，这在独立性假设下为 LMS 引入了统计上解耦的模态动力学。\n\n您必须实现的定义：\n- 由瞬时平方误差上的梯度下降驱动的 LMS 更新：$\\mathbf{w}(t+1) = \\mathbf{w}(t) + \\mu \\,\\mathbf{x}_t \\,\\big(d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t)\\big)$，其中步长 $\\mu>0$。\n- 由带有遗忘因子 $\\lambda_{\\mathrm{RLS}}$ 和初始信息矩阵 $P(0)$ 的递归最小二乘法定义的 RLS 更新：\n  - 增益 $\\mathbf{k}_t = \\dfrac{P(t-1)\\mathbf{x}_t}{\\lambda_{\\mathrm{RLS}} + \\mathbf{x}_t^\\top P(t-1) \\mathbf{x}_t}$，\n  - 权重更新 $\\mathbf{w}(t) = \\mathbf{w}(t-1) + \\mathbf{k}_t \\big(d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t-1)\\big)$，\n  - 信息矩阵更新 $P(t) = \\lambda_{\\mathrm{RLS}}^{-1} \\big(P(t-1) - \\mathbf{k}_t \\mathbf{x}_t^\\top P(t-1)\\big)$。\n\n需要计算的性能指标：\n- 令模态失准为 $\\mathbf{z}(t) = U^\\top (\\mathbf{w}(t) - \\mathbf{w}^\\star)$ 且 $m_k(t) = \\mathbb{E}[z_k(t)^2]$。\n- 对于 RLS，定义模式 $k$ 在时间窗口 $t \\in \\{0,1,\\dots,T_0\\}$ 内的早期超调因子为\n  $$\\rho_k = \\frac{\\max_{0 \\le t \\le T_0} m_k(t)}{m_k(0)} - 1.$$\n- 通过所有模式中的最大值来总结在给定 $\\delta$ 下的 RLS 早期超调，\n  $$\\rho_{\\max} = \\max_k \\rho_k.$$\n- 对于 LMS，评估平均模态失准 $m_k(t)$ 是否在 $t \\in \\{0,1,\\dots,T_0\\}$ 期间对每个模式 $k$ 都是单调非增的。\n\n用于近似期望的仿真协议：\n- 使用 $n=4$ 个参数，其中 $R$ 是对角矩阵，因此 $U=I$ 且 $\\Lambda=\\mathrm{diag}(\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4)$，其中 $\\lambda_1=3, \\lambda_2=1, \\lambda_3=0.3, \\lambda_4=0.1$。\n- 令真实参数为 $\\mathbf{w}^\\star = [1,-0.5,0.25,-0.125]^\\top$。\n- 通过抽取 $\\mathbf{g}_t \\sim \\mathcal{N}(\\mathbf{0}, I)$ 并设置 $\\mathbf{x}_t = \\Lambda^{1/2} \\mathbf{g}_t$ 来生成独立同分布的回归量 $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$。\n- 不使用观测噪声，因此 $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$。\n- 将 LMS 步长固定为 $\\mu=0.05$ 并初始化 $\\mathbf{w}(0)=\\mathbf{0}$。\n- 将 RLS 遗忘因子固定为 $\\lambda_{\\mathrm{RLS}}=0.995$ 并初始化 $\\mathbf{w}(0)=\\mathbf{0}$ 和 $P(0)=\\delta^{-1} I$。\n- 仿真总时长为 $T=200$ 次迭代，并将早期窗口定义为 $T_0=30$。\n- 通过对 $M=200$ 次独立的蒙特卡洛试验进行平均来近似 $\\mathbb{E}[\\,\\cdot\\,]$。通过使用固定的伪随机数生成器种子 $s=2025$ 来生成所有蒙特卡洛序列，以确保可复现性，并在所有算法和参数设置中重复使用完全相同的蒙特卡洛回归量序列。\n- 评估与 RLS 初始化尺度 $\\delta \\in \\{10^{-3}, 1, 10^2\\}$ 相对应的三个测试用例，所有三个 $\\delta$ 值使用相同的 $\\lambda_{\\mathrm{RLS}}$ 和数据。LMS 的设置保持不变，并用于一次单调性评估。\n\n每个测试用例需要计算的内容：\n- 对于每个 $\\delta \\in \\{10^{-3}, 1, 10^2\\}$，根据 RLS 仿真计算如上定义的 $\\rho_{\\max}$。\n- 根据 LMS 仿真，计算一个单一的布尔值，该值指示基于蒙特卡洛平均，在窗口 $t \\in \\{0,1,\\dots,T_0\\}$ 内，$m_k(t)$ 对于所有模式 $k \\in \\{1,2,3,4\\}$ 是否在 $t$ 上是单调非增的。\n\n最终输出格式：\n- 您的程序必须生成包含一个列表的单行，格式为 $[\\rho_{\\max}(\\delta=10^{-3}), \\rho_{\\max}(\\delta=1), \\rho_{\\max}(\\delta=10^{2}), \\mathrm{LMS\\_all\\_modes\\_monotone}]$，其中前三个条目是浮点数，最后一个条目是布尔值。不应打印任何其他文本。\n\n确保覆盖范围的测试套件摘要：\n- 正常路径：$\\delta=1$，具有中等初始信息量。\n- 边界类的激进自适应：$\\delta=10^{-3}$，导致较大的 $P(0)$，可能产生更大的早期超调。\n- 边缘类的保守自适应：$\\delta=10^{2}$，导致较小的 $P(0)$，早期超调可能可以忽略不计。\n- LMS 的单调性布尔值必须在固定的 $\\mu$ 下计算一次，并在所有 $\\delta$ 测试用例中共享。\n\n此问题不涉及物理单位或角度单位。所有结果均为指定的无量纲实数或布尔值。", "solution": "该问题是有效的。这是一个在自适应信号处理领域中结构良好、有科学依据的练习，要求比较最小均方 (LMS) 和递归最小二乘 (RLS) 算法。所有参数和程序都规定得足够详细，并与已建立的理论和实践相符。对 RLS 初始化参数 $\\delta$ 及其对收敛超调影响的研究是一个经典且富有启发性的话题。我现在将提供完整的解决方案。\n\n目标是实现并数值比较 LMS 和 RLS 算法在系统辨识任务中的收敛行为。主要焦点是早期性能，特别是量化 RLS 模态误差动态中作为其初始化函数的超调，并将其与保守步长下 LMS 的单调收敛进行对比。\n\n该分析的基础是一个线性系统模型，其中期望信号 $d_t$ 由一个作用于输入回归量矢量 $\\mathbf{x}_t$ 的真实未知权重矢量 $\\mathbf{w}^\\star$ 生成，即 $d_t = \\mathbf{x}_t^\\top \\mathbf{w}^\\star$。回归量 $\\mathbf{x}_t$ 从一个已知协方差矩阵 $R$ 的零均值高斯分布中抽取，即 $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{0}, R)$。自适应算法以零权重矢量 $\\mathbf{w}(0) = \\mathbf{0}$ 初始化，旨在通过顺序处理 $(\\mathbf{x}_t, d_t)$ 对来估计 $\\mathbf{w}^\\star$。\n\n性能在由输入协方差矩阵 $R$ 的特征矢量定义的模态域中进行评估。由于 $R$ 被指定为对角矩阵 $R = \\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$，其特征矢量是标准基矢，模态分解大大简化。第 $k$ 个模式的模态失准为 $z_k(t) = (\\mathbf{w}(t) - \\mathbf{w}^\\star)_k$。性能指标是均方模态失准 $m_k(t) = \\mathbb{E}[z_k(t)^2]$，我们将通过对许多独立的蒙特卡洛试验进行平均来近似该值。\n\n仿真将根据以下步骤进行：\n\n1.  **环境与数据生成**：我们首先确定仿真参数：维度数 $n=4$，总迭代次数 $T=200$，早期时间窗口大小 $T_0=30$，以及蒙特卡洛试验次数 $M=200$。固定参数是真实权重矢量 $\\mathbf{w}^\\star = [1, -0.5, 0.25, -0.125]^\\top$ 和输入协方差矩阵 $R = \\Lambda = \\mathrm{diag}(3, 1, 0.3, 0.1)$。为确保可复现性和公平比较，我们使用固定的随机数生成器种子 $s=2025$ 一次性生成 $M$ 个输入矢量序列 $\\{\\mathbf{x}_t\\}_{t=0}^{T-1}$ 和相应的期望信号 $\\{d_t\\}_{t=0}^{T-1}$。这些数据集将对两种算法和所有参数设置重复使用。输入矢量 $\\mathbf{x}_t$ 的生成方式是采样 $\\mathbf{g}_t \\sim \\mathcal{N}(\\mathbf{0}, I)$ 并设置 $\\mathbf{x}_t = \\Lambda^{1/2}\\mathbf{g}_t$。\n\n2.  **LMS 算法仿真与分析**：LMS 算法使用简单的梯度下降法更新瞬时平方误差的权重估计。更新规则为：\n    $$ \\mathbf{w}(t+1) = \\mathbf{w}(t) + \\mu \\mathbf{x}_t (d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t)) $$\n    我们用步长 $\\mu=0.05$ 对 $M$ 个数据序列中的每一个进行仿真。对于每次试验，我们记录平方模态失准的轨迹，$(z_k^{(i)}(t))^2 = (\\mathbf{w}^{(i)}(t) - \\mathbf{w}^\\star)_k^2$，其中 $i \\in \\{1, \\dots, M\\}$，$t \\in \\{0, \\dots, T\\}$，$k \\in \\{1, \\dots, n\\}$。将这些在 $M$ 次试验中取平均，得到估计的均方模态失准曲线 $m_k(t)$。\n    \n    然后我们评估这些曲线的单调性。对于每个模式 $k$，我们检查是否对于所有 $t \\in \\{1, \\dots, T_0\\}$ 都有 $m_k(t) \\le m_k(t-1)$ 成立。最终结果是一个单一的布尔值 `LMS_all_modes_monotone`，当且仅当所有四个模式在早期时间窗口内都表现出非增的均方失准时，该值为真。\n\n3.  **RLS 算法仿真与分析**：RLS 算法为最小二乘问题提供了一个递归解。我们为遗忘因子 $\\lambda_{\\mathrm{RLS}}=0.995$ 实现指定的更新方程。对于每个时间步 $t$，从初始状态 $\\mathbf{w}(0)=\\mathbf{0}$ 和 $P(0)=\\delta^{-1}I$ 开始，算法计算：\n    -   增益矢量：$\\mathbf{k}_t = \\dfrac{P(t-1)\\mathbf{x}_t}{\\lambda_{\\mathrm{RLS}} + \\mathbf{x}_t^\\top P(t-1) \\mathbf{x}_t}$\n    -   权重矢量：$\\mathbf{w}(t) = \\mathbf{w}(t-1) + \\mathbf{k}_t (d_t - \\mathbf{x}_t^\\top \\mathbf{w}(t-1))$\n    -   逆协方差矩阵：$P(t) = \\lambda_{\\mathrm{RLS}}^{-1} (P(t-1) - \\mathbf{k}_t \\mathbf{x}_t^\\top P(t-1))$\n\n    我们为 RLS 算法执行三组独立的仿真，对应于初始化参数 $\\delta \\in \\{10^{-3}, 1, 10^2\\}$。对于每个 $\\delta$，我们使用与 LMS 仿真相同的数据运行 $M$ 次试验。与 LMS 一样，我们通过对试验结果取平均来计算均方模态失准曲线 $m_k(t)$。\n\n    RLS 的关键性能指标是早期超调因子 $\\rho_{\\max}$。对于每个模式 $k$，我们首先计算初始误差 $m_k(0) = \\mathbb{E}[z_k(0)^2] = \\mathbb{E}[(-w^\\star_k)^2] = (w^\\star_k)^2$。然后，我们找到早期时间窗口内的峰值误差 $\\max_{0 \\le t \\le T_0} m_k(t)$。该模式的超调因子为：\n    $$ \\rho_k = \\frac{\\max_{0 \\le t \\le T_0} m_k(t)}{m_k(0)} - 1 $$\n    给定 $\\delta$ 的最终指标是所有模式中的最大超调 $\\rho_{\\max} = \\max_k \\{\\rho_k\\}$。一个大于 0 的 $\\rho_{\\max}$ 值表示至少有一个模式的均方误差超过了其初始值。\n\n4.  **实现与最终输出**：所述过程在一个 Python 脚本中实现。该脚本首先生成共享数据集，然后运行 LMS 仿真以确定单调性布尔值。随后，它遍历指定的 $\\delta$ 值，为每个值运行 RLS 仿真并计算相应的 $\\rho_{\\max}$。最终结果被收集并以指定的列表格式打印出来。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares LMS and RLS algorithms for system identification\n    to analyze early-time convergence behavior.\n    \"\"\"\n    # --- Simulation Parameters ---\n    n = 4\n    T = 200\n    T0 = 30\n    M = 200\n    seed = 2025\n    \n    w_star = np.array([1.0, -0.5, 0.25, -0.125])\n    lambda_vals = np.array([3.0, 1.0, 0.3, 0.1])\n    Lambda = np.diag(lambda_vals)\n    Lambda_sqrt = np.diag(np.sqrt(lambda_vals))\n    \n    mu_lms = 0.05\n    lambda_rls = 0.995\n    delta_values = [1e-3, 1.0, 1e2]\n    \n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n    \n    # Generate M sets of data sequences, each of length T\n    # x_mc shape: (M, T, n), d_mc shape: (M, T)\n    g_mc = rng.standard_normal(size=(M, T, n))\n    x_mc = np.einsum('ij,ktj->kti', Lambda_sqrt, g_mc)\n    d_mc = np.einsum('kti,i->kt', x_mc, w_star)\n    \n    # --- LMS Simulation and Analysis ---\n    \n    # Store squared modal error for each trial\n    # mse_lms shape: (M, T+1, n)\n    mse_lms = np.zeros((M, T + 1, n))\n    \n    for i in range(M):\n        w = np.zeros(n)\n        initial_error_sq = (w - w_star)**2\n        mse_lms[i, 0, :] = initial_error_sq\n        \n        for t in range(T):\n            xt = x_mc[i, t, :]\n            dt = d_mc[i, t]\n            \n            error_signal = dt - xt.T @ w\n            w = w + mu_lms * xt * error_signal\n            \n            modal_error_sq = (w - w_star)**2\n            mse_lms[i, t + 1, :] = modal_error_sq\n            \n    # Average over Monte Carlo trials to get m_k(t)\n    m_k_lms = np.mean(mse_lms, axis=0) # Shape: (T+1, n)\n    \n    # Check for monotonicity in all modes over t in [0, T0]\n    lms_all_modes_monotone = True\n    for k in range(n):\n        for t in range(1, T0 + 1):\n            if m_k_lms[t, k] > m_k_lms[t-1, k]:\n                # Use a small tolerance for floating point comparisons\n                if not np.isclose(m_k_lms[t, k], m_k_lms[t-1, k]):\n                    lms_all_modes_monotone = False\n                    break\n        if not lms_all_modes_monotone:\n            break\n            \n    # --- RLS Simulation and Analysis ---\n    \n    rls_rho_max_results = []\n    \n    for delta in delta_values:\n        # mse_rls shape: (M, T+1, n)\n        # The first arugment of range for time is 1, so the first update is at index 1\n        # To align with LMS (0 to T), we use slightly different indexing logic from problem\n        # The logic here is: w(t+1) is updated from w(t) using x(t), d(t)\n        \n        mse_rls = np.zeros((M, T + 1, n))\n\n        for i in range(M):\n            w = np.zeros(n)\n            P = (1.0 / delta) * np.identity(n)\n            \n            initial_error_sq = (w - w_star)**2\n            mse_rls[i, 0, :] = initial_error_sq\n            \n            for t in range(T):\n                xt = x_mc[i, t, :]\n                dt = d_mc[i, t]\n                \n                # RLS update equations re-indexed for a causal loop t=0..T-1\n                # producing w(t+1) from w(t), P(t) and x(t),d(t)\n                den = lambda_rls + xt.T @ P @ xt\n                k_vec = (P @ xt) / den\n                \n                error_signal = dt - xt.T @ w\n                w = w + k_vec * error_signal\n                P = (1.0 / lambda_rls) * (P - np.outer(k_vec, xt) @ P)\n\n                modal_error_sq = (w - w_star)**2\n                mse_rls[i, t + 1, :] = modal_error_sq\n\n        # Average over Monte Carlo trials\n        m_k_rls = np.mean(mse_rls, axis=0) # Shape: (T+1, n)\n        \n        # Calculate overshoot factor rho_k for each mode\n        rho_k_list = []\n        for k in range(n):\n            m_k_0 = (w_star[k])**2\n            if m_k_0 == 0:\n                # If initial error is zero, any deviation is infinite overshoot.\n                # Avoid division by zero. This shouldn't happen with the given w_star.\n                max_val = np.max(m_k_rls[0 : T0 + 1, k])\n                rho_k = np.inf if max_val > 0 else 0.0\n            else:\n                max_m_k_t0 = np.max(m_k_rls[0 : T0 + 1, k])\n                rho_k = (max_m_k_t0 / m_k_0) - 1.0\n            rho_k_list.append(rho_k)\n            \n        rho_max = np.max(rho_k_list)\n        rls_rho_max_results.append(rho_max)\n        \n    # --- Final Output ---\n    final_results = rls_rho_max_results + [lms_all_modes_monotone]\n    # The variable name for gain vector `k` in python is changed to `k_vec`\n    # to avoid conflict with the loop variable `k` for modes.\n    # The printed output is formatted as a list of floating point numbers and a boolean.\n    print(f\"[{final_results[0]},{final_results[1]},{final_results[2]},{final_results[3]}]\")\n\nsolve()\n```", "id": "2891050"}, {"introduction": "LMS 算法的一个核心弱点在于其性能对输入信号的相关性（即有色噪声）高度敏感。本练习提供了一个强大的分析工具来量化这一性能差距。通过推导并比较 LMS 和理想化 RLS 的“累积瞬态误差”，我们将得到一个仅与输入协方差矩阵条件数 $\\kappa$ 相关的简洁表达式，它优雅地总结了 LMS 相对于 RLS 的性能劣势 [@problem_id:2891072]。", "problem": "考虑一个 $M$ 阶有限冲激响应滤波器的自适应辨识问题，其未知系数向量为 $\\,\\mathbf{w}_{\\star}\\in\\mathbb{R}^{M}\\,$。回归量 $\\,\\mathbf{x}(k)\\in\\mathbb{R}^{M}\\,$ 是一个宽平稳、零均值的过程，其协方差矩阵 $\\,\\mathbf{R}=\\mathbb{E}\\{\\mathbf{x}(k)\\mathbf{x}(k)^{\\top}\\}\\,$ 是正定的。假设期望响应是无噪声的，即 $\\,d(k)=\\mathbf{x}(k)^{\\top}\\mathbf{w}_{\\star}\\,$。设 $\\,\\mathbf{R}\\,$ 的特征值包含在紧区间 $[\\lambda_{\\min},\\lambda_{\\max}]$ 内，其中 $\\,0\\lambda_{\\min}\\le \\lambda_{\\max}\\infty\\,$，并定义条件数 $\\,\\kappa\\triangleq \\lambda_{\\max}/\\lambda_{\\min}\\,$。\n\n考虑两种算法：\n\n1) 最小均方算法 (LMS; Least Mean Squares)，采用固定步长 $\\,\\mu>0\\,$：\n$$\n\\mathbf{w}_{k+1}=\\mathbf{w}_{k}+\\mu\\,\\mathbf{x}(k)\\big(d(k)-\\mathbf{x}(k)^{\\top}\\mathbf{w}_{k}\\big).\n$$\n定义权值误差 $\\,\\mathbf{v}_{k}\\triangleq \\mathbf{w}_{k}-\\mathbf{w}_{\\star}\\,$。在标准的独立性假设下，利用均方代价 $\\,J(\\mathbf{w})=\\mathbb{E}\\{(d-\\mathbf{x}^{\\top}\\mathbf{w})^{2}\\}\\,$ 的二次型性质以及 $\\,\\mathbf{R}\\,$ 的谱特性，推导一个形如下式的最差模式指数界\n$$\n\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}\\le \\rho(\\mu)^{k}\\,\\|\\mathbf{v}_{0}\\|^{2},\n$$\n对于某个收缩因子 $\\,\\rho(\\mu)\\in(0,1)\\,$，当 $\\,\\mu\\in(0,2/\\lambda_{\\max})\\,$ 时成立。然后定义积分最差情况瞬态均方界\n$$\nS_{\\mathrm{LMS}}(\\mu)\\triangleq \\sum_{k=0}^{\\infty}\\frac{\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}}{\\|\\mathbf{v}_{0}\\|^{2}},\n$$\n并通过在允许的 $\\,\\mu\\,$ 范围内最小化该界，以获得 $\\,S_{\\mathrm{LMS}}^{\\star}\\triangleq \\inf_{\\mu\\in(0,2/\\lambda_{\\max})}S_{\\mathrm{LMS}}(\\mu)\\,$ 仅用 $\\,\\kappa\\,$ 表示的闭式表达式。\n\n2) 递推最小二乘算法 (RLS; Recursive Least Squares)，采用指数加权代价和遗忘因子 $\\,\\alpha\\in(0,1]\\,$：\n$$\nJ_{\\alpha,k}(\\mathbf{w})=\\sum_{i=0}^{k}\\alpha^{k-i}\\big(d(i)-\\mathbf{x}(i)^{\\top}\\mathbf{w}\\big)^{2}.\n$$\n在二次代价 $\\,J(\\mathbf{w})\\,$ 具有精确二阶曲率信息的理想化极限下（即，可以访问 $\\,\\mathbf{R}^{-1}\\,$ 从而实现牛顿步），确定积分最差情况瞬态均方界的最小可达值\n$$\nS_{\\mathrm{RLS}}^{\\star}\\triangleq \\inf_{\\alpha\\in(0,1]}\\ \\sum_{k=0}^{\\infty}\\frac{\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}}{\\|\\mathbf{v}_{0}\\|^{2}},\n$$\n该值通过对 $\\,\\alpha\\,$ 取下确界得到，同样以无单位的形式表示。\n\n最后，报告比率\n$$\n\\Gamma(\\kappa)\\triangleq \\frac{S_{\\mathrm{LMS}}^{\\star}}{S_{\\mathrm{RLS}}^{\\star}}\n$$\n作为一个仅含 $\\,\\kappa\\,$ 的闭式解析表达式。提供 $\\,\\Gamma(\\kappa)\\,$ 的精确符号表达式作为最终答案。不需要数值近似，也不需要包含单位。", "solution": "所述问题是自适应滤波器算法分析中的一个标准练习，特别是比较最小均方 (LMS) 算法和理想化递推最小二乘 (RLS) 算法的瞬态性能。所有给定条件都是该领域的标准条件，问题是自洽的、有科学依据且适定的。没有逻辑矛盾、信息缺失或伪科学元素。因此，该问题是有效的，并且可以解决。\n\n首先，我们分析 LMS 算法。权值误差向量定义为 $\\mathbf{v}_{k} \\triangleq \\mathbf{w}_{k}-\\mathbf{w}_{\\star}$。权值向量的更新方程为 $\\mathbf{w}_{k+1}=\\mathbf{w}_{k}+\\mu\\,\\mathbf{x}(k)(d(k)-\\mathbf{x}(k)^{\\top}\\mathbf{w}_{k})$。代入 $\\mathbf{w}_k = \\mathbf{v}_k + \\mathbf{w}_\\star$ 和无噪声的期望信号 $d(k) = \\mathbf{x}(k)^{\\top}\\mathbf{w}_{\\star}$，我们得到误差向量的递推关系：\n$$\n\\mathbf{v}_{k+1} + \\mathbf{w}_{\\star} = \\mathbf{v}_{k} + \\mathbf{w}_{\\star} + \\mu \\mathbf{x}(k)(\\mathbf{x}(k)^{\\top}\\mathbf{w}_{\\star} - \\mathbf{x}(k)^{\\top}(\\mathbf{v}_{k} + \\mathbf{w}_{\\star}))\n$$\n$$\n\\mathbf{v}_{k+1} = \\mathbf{v}_{k} - \\mu \\mathbf{x}(k)\\mathbf{x}(k)^{\\top}\\mathbf{v}_{k} = (\\mathbf{I} - \\mu \\mathbf{x}(k)\\mathbf{x}(k)^{\\top}) \\mathbf{v}_{k}\n$$\n为了分析均方性能，我们考虑 $\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}$ 的演化。推导最差模式界的一个标准方法是在由协方差矩阵 $\\mathbf{R}$ 的特征向量定义的坐标系中进行。设 $\\mathbf{R} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{\\top}$ 是 $\\mathbf{R}$ 的特征分解，其中 $\\mathbf{Q}$ 是特征向量构成的正交矩阵，$\\mathbf{\\Lambda}$ 是特征值 $\\lambda_i$ 构成的对角矩阵。定义变换后的误差向量 $\\tilde{\\mathbf{v}}_{k} \\triangleq \\mathbf{Q}^{\\top}\\mathbf{v}_{k}$。二范数的平方在此变换下是不变的：$\\|\\mathbf{v}_{k}\\|^{2} = \\|\\tilde{\\mathbf{v}}_{k}\\|^{2} = \\sum_{i=1}^{M} |\\tilde{v}_{k,i}|^2$。\n\n在标准的独立性假设下，变换后误差向量的每个模式的均方演化可以被近似或界定。用于无噪声瞬态分析的一个常用界认为，每个模式中能量的衰减由均值的衰减因子的平方决定。第 $i$ 个变换后误差分量的均值演化为 $\\mathbb{E}\\{\\tilde{v}_{k+1,i}\\} = (1-\\mu\\lambda_i)\\mathbb{E}\\{\\tilde{v}_{k,i}\\}$。这导致了每个模式均方误差演化的一个界：$\\mathbb{E}\\{|\\tilde{v}_{k,i}|^2\\} \\approx (1-\\mu\\lambda_i)^{2k} |\\tilde{v}_{0,i}|^2$。\n为了获得总均方误差的最差情况界，我们用衰减最慢的模式来界定所有模式的衰减：\n$$\n\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\} = \\sum_{i=1}^{M} \\mathbb{E}\\{|\\tilde{v}_{k,i}|^2\\} \\approx \\sum_{i=1}^{M} (1-\\mu\\lambda_i)^{2k} |\\tilde{v}_{0,i}|^2 \\le \\left(\\max_i (1-\\mu\\lambda_i)^2\\right)^k \\sum_{i=1}^{M} |\\tilde{v}_{0,i}|^2\n$$\n$$\n\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\} \\le \\left(\\max_i (1-\\mu\\lambda_i)^2\\right)^k \\|\\mathbf{v}_{0}\\|^{2}\n$$\n这与所要求的形式 $\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}\\le \\rho(\\mu)^{k}\\,\\|\\mathbf{v}_{0}\\|^{2}$ 相符，其中收缩因子定义为 $\\rho(\\mu) \\triangleq \\max_{i} (1-\\mu\\lambda_i)^2 = (\\max_{i} |1-\\mu\\lambda_i|)^2$。特征值 $\\lambda_i$ 位于区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$。项 $|1-\\mu\\lambda|$ 在此区间的边界处最大化，所以 $\\rho(\\mu) = (\\max(|1-\\mu\\lambda_{\\min}|, |1-\\mu\\lambda_{\\max}|))^2$。为保证收敛，我们需要 $\\rho(\\mu)1$，这要求 $\\mu \\in (0, 2/\\lambda_{\\max})$。\n\n积分瞬态界为 $S_{\\mathrm{LMS}}(\\mu) = \\sum_{k=0}^{\\infty}\\frac{\\mathbb{E}\\{\\|\\mathbf{v}_{k}\\|^{2}\\}}{\\|\\mathbf{v}_{0}\\|^{2}} \\le \\sum_{k=0}^{\\infty} \\rho(\\mu)^k = \\frac{1}{1-\\rho(\\mu)}$。为求得 $S_{\\mathrm{LMS}}^{\\star}$，我们必须通过选择 $\\mu$ 来最小化 $\\rho(\\mu)$，从而最小化这个上界。这等价于最小化 $\\max(|1-\\mu\\lambda_{\\min}|, |1-\\mu\\lambda_{\\max}|)$。当两个参数的绝对值相等时，达到最小值。由于 $\\mu  2/\\lambda_{\\max} \\le 2/\\lambda_{\\min}$，所以 $1-\\mu\\lambda_{\\min}$ 和 $1-\\mu\\lambda_{\\max}$ 都大于 $-1$。因此我们令 $1-\\mu\\lambda_{\\min} = -(1-\\mu\\lambda_{\\max}) = \\mu\\lambda_{\\max}-1$。\n$$\n2 = \\mu(\\lambda_{\\min}+\\lambda_{\\max}) \\implies \\mu_{\\mathrm{opt}} = \\frac{2}{\\lambda_{\\min}+\\lambda_{\\max}}\n$$\n在此最优步长下，最大值为 $1-\\mu_{\\mathrm{opt}}\\lambda_{\\min} = 1 - \\frac{2\\lambda_{\\min}}{\\lambda_{\\min}+\\lambda_{\\max}} = \\frac{\\lambda_{\\max}-\\lambda_{\\min}}{\\lambda_{\\max}+\\lambda_{\\min}}$。\n使用条件数 $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$，这变为 $\\frac{\\kappa\\lambda_{\\min}-\\lambda_{\\min}}{\\kappa\\lambda_{\\min}+\\lambda_{\\min}} = \\frac{\\kappa-1}{\\kappa+1}$。\n因此，$\\rho(\\mu)$ 的最小值为 $\\rho_{\\min} = \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2$。\n最小积分界则为：\n$$\nS_{\\mathrm{LMS}}^{\\star} = \\frac{1}{1-\\rho_{\\min}} = \\frac{1}{1 - \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2} = \\frac{(\\kappa+1)^2}{(\\kappa+1)^2 - (\\kappa-1)^2} = \\frac{(\\kappa+1)^2}{(\\kappa^2+2\\kappa+1) - (\\kappa^2-2\\kappa+1)} = \\frac{(\\kappa+1)^2}{4\\kappa}\n$$\n\n接下来，我们分析理想化的 RLS 算法。问题指出这对应于一个牛顿步，这意味着使用真实均方误差代价函数 $J(\\mathbf{w}) = \\mathbb{E}\\{(d-\\mathbf{x}^{\\top}\\mathbf{w})^2\\} = \\mathbf{v}^\\top \\mathbf{R} \\mathbf{v}$ 的海森矩阵的逆矩阵。海森矩阵为 $2\\mathbf{R}$。牛顿更新规则是 $\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\gamma H^{-1} \\nabla J(\\mathbf{w}_k)$。其中 $\\nabla J(\\mathbf{w}_k) = 2\\mathbf{R}\\mathbf{v}_k$ 且 $H=2\\mathbf{R}$，这变为：\n$$\n\\mathbf{w}_{k+1} = \\mathbf{w}_k - \\gamma (2\\mathbf{R})^{-1} (2\\mathbf{R}\\mathbf{v}_k) = \\mathbf{w}_k - \\gamma \\mathbf{v}_k\n$$\n两边减去 $\\mathbf{w}_{\\star}$，我们得到误差递推关系 $\\mathbf{v}_{k+1} = \\mathbf{v}_k - \\gamma \\mathbf{v}_k = (1-\\gamma)\\mathbf{v}_k$。\n问题将此与带有遗忘因子 $\\alpha$ 的 RLS 公式联系起来。在 RLS 中，步长由 $1-\\alpha$ 有效控制。因此，我们将牛顿步长 $\\gamma$ 等同于 $1-\\alpha$。\n$$\n\\mathbf{v}_{k+1} = (1-(1-\\alpha))\\mathbf{v}_{k} = \\alpha \\mathbf{v}_{k}\n$$\n这是一个确定性的递推关系。第 $k$ 步的误差向量是 $\\mathbf{v}_{k} = \\alpha^k \\mathbf{v}_0$。其二范数的平方为 $\\|\\mathbf{v}_{k}\\|^2 = \\alpha^{2k}\\|\\mathbf{v}_0\\|^2$。这种理想化的算法对问题进行了预白化，导致所有模式以相同的速率 $\\alpha$ 衰减，而与协方差结构 $\\mathbf{R}$ 无关。\n积分均方界为：\n$$\nS_{\\mathrm{RLS}}(\\alpha) = \\sum_{k=0}^{\\infty} \\frac{\\|\\mathbf{v}_{k}\\|^{2}}{\\|\\mathbf{v}_{0}\\|^{2}} = \\sum_{k=0}^{\\infty} \\alpha^{2k} = \\frac{1}{1-\\alpha^2}\n$$\n当 $|\\alpha|1$ 时，此和收敛。我们需要找到 $\\alpha \\in (0,1]$ 时的下确界。对于 $\\alpha=1$，和发散。函数 $\\frac{1}{1-\\alpha^2}$ 在 $\\alpha \\in [0,1)$ 上是严格递增的。因此，下确界位于区间的下边界，即当 $\\alpha \\to 0^+$ 时。\n$$\nS_{\\mathrm{RLS}}^{\\star} = \\inf_{\\alpha\\in(0,1]} \\frac{1}{1-\\alpha^2} = \\lim_{\\alpha \\to 0^+} \\frac{1}{1-\\alpha^2} = 1\n$$\n\n最后，我们计算比率 $\\Gamma(\\kappa) = S_{\\mathrm{LMS}}^{\\star} / S_{\\mathrm{RLS}}^{\\star}$。\n$$\n\\Gamma(\\kappa) = \\frac{\\frac{(\\kappa+1)^2}{4\\kappa}}{1} = \\frac{(\\kappa+1)^2}{4\\kappa}\n$$\n该比率量化了由于输入信号协方差矩阵的特征值扩散，LMS 相对于理想化的 RLS/牛顿方法的性能下降程度。对于 $\\kappa=1$（白噪声输入），$\\Gamma(1)=1$，表明两种算法具有相同的积分瞬态误差。随着 $\\kappa$ 的增加，LMS 的性能下降。", "answer": "$$\n\\boxed{\\frac{(\\kappa+1)^2}{4\\kappa}}\n$$", "id": "2891072"}]}