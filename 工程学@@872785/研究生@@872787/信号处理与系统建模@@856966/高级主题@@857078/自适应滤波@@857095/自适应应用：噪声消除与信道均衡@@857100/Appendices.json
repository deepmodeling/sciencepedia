{"hands_on_practices": [{"introduction": "在我们深入研究自适应算法之前，理解我们追求的最终目标至关重要。维纳滤波器 (Wiener filter) 代表了在均方误差 ($MSE$) 准则下理论上的最优线性滤波器。[@problem_id:2850046] 这个练习将引导你使用信号的二阶统计量——即输入自相关矩阵 $R$ 和互相关向量 $\\mathbf{p}$——来推导这个最优解。通过求解维纳滤波器，你不仅能掌握最优线性估计的基石，还能建立一个评估任何自适应滤波器性能的“黄金标准”。", "problem": "一个双抽头线性均衡器被设计用于最小化零均值期望标量信号 $d(n)$ 与一个线性估计 $\\hat{d}(n)=\\mathbf{w}^{\\top}\\mathbf{x}(n)$ 之间的均方误差，该线性估计由一个零均值、联合宽平稳的二维输入 $\\mathbf{x}(n)\\in\\mathbb{R}^{2}$ 形成。设输入自相关矩阵为 $\\mathbf{R}=\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}$，互相关向量为 $\\mathbf{p}=\\mathbb{E}\\{\\mathbf{x}(n)d(n)\\}$。假设已从数据的二阶统计量中获得 $\\mathbf{R}=\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$ 和 $\\mathbf{p}=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，并且期望信号的方差为 $\\sigma_{d}^{2}=\\mathbb{E}\\{d^{2}(n)\\}=1$。\n\n从基于二阶矩和正交性原理的最小均方误差公式出发，确定：\n\n1. 最小化 $\\mathbb{E}\\{(d(n)-\\mathbf{w}^{\\top}\\mathbf{x}(n))^{2}\\}$ 的最优维纳解 $\\mathbf{w}_{o}\\in\\mathbb{R}^{2}$。\n\n2. 在 $\\mathbf{w}_{o}$ 处达到的最小均方误差 $J_{\\min}$。\n\n请精确表达最终数值。无需四舍五入。答案必须以精确有理数的形式提供。", "solution": "所提出的问题是统计信号处理中关于维纳滤波器推导的一个标准练习。它在科学上是合理的，提法恰当，并包含了完整解答所需的所有必要信息。因此，它是有效的。我们继续进行推导。\n\n目标是找到最小化均方误差 (MSE) $J(\\mathbf{w})$ 的权重向量 $\\mathbf{w}$。MSE 定义为期望信号 $d(n)$ 与其估计值 $\\hat{d}(n)$ 之间误差平方的期望值。\n估计值是输入向量 $\\mathbf{x}(n)$ 的线性组合，由 $\\hat{d}(n) = \\mathbf{w}^{\\top}\\mathbf{x}(n)$ 给出。误差信号为 $e(n) = d(n) - \\hat{d}(n) = d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n)$。\n\nMSE 代价函数 $J(\\mathbf{w})$ 为：\n$$J(\\mathbf{w}) = \\mathbb{E}\\{e^2(n)\\} = \\mathbb{E}\\{(d(n) - \\mathbf{w}^{\\top}\\mathbf{x}(n))^2\\}$$\n展开平方项，我们得到：\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n) - 2d(n)\\mathbf{w}^{\\top}\\mathbf{x}(n) + (\\mathbf{w}^{\\top}\\mathbf{x}(n))(\\mathbf{x}^{\\top}(n)\\mathbf{w})\\}$$\n利用期望算子的线性性质，这变为：\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n)\\} - 2\\mathbb{E}\\{d(n)\\mathbf{w}^{\\top}\\mathbf{x}(n)\\} + \\mathbb{E}\\{\\mathbf{w}^{\\top}\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\mathbf{w}\\}$$\n由于 $\\mathbf{w}$ 是一个确定性系数向量，它可以移到期望之外：\n$$J(\\mathbf{w}) = \\mathbb{E}\\{d^2(n)\\} - 2\\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)d(n)\\} + \\mathbf{w}^{\\top}\\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\}\\mathbf{w}$$\n我们已知以下二阶统计量：\n期望信号的方差：$\\sigma_d^2 = \\mathbb{E}\\{d^2(n)\\} = 1$。\n输入与期望信号之间的互相关向量：$\\mathbf{p} = \\mathbb{E}\\{\\mathbf{x}(n)d(n)\\} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n输入向量的自相关矩阵：$R = \\mathbb{E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\top}(n)\\} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$。\n\n将这些量代入 $J(\\mathbf{w})$ 的表达式中，得到性能曲面：\n$$J(\\mathbf{w}) = \\sigma_d^2 - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}R\\mathbf{w}$$\n为了找到最小化此二次函数的最优权重向量 $\\mathbf{w}_o$，我们必须计算 $J(\\mathbf{w})$ 关于 $\\mathbf{w}$ 的梯度，并将其设为零向量。\n$$\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\frac{\\mathrm{d}}{\\mathrm{d}\\mathbf{w}} (\\sigma_d^2 - 2\\mathbf{w}^{\\top}\\mathbf{p} + \\mathbf{w}^{\\top}R\\mathbf{w}) = -2\\mathbf{p} + 2R\\mathbf{w}$$\n对于最优向量 $\\mathbf{w}_o$，将梯度设为零：\n$$-2\\mathbf{p} + 2R\\mathbf{w}_o = \\mathbf{0}$$\n这导出了维纳-霍夫方程：\n$$R\\mathbf{w}_o = \\mathbf{p}$$\n最优维纳解 $\\mathbf{w}_o$ 可通过求解该线性方程组找到：\n$$\\mathbf{w}_o = R^{-1}\\mathbf{p}$$\n首先，我们计算自相关矩阵 $R$ 的逆矩阵。$R$ 的行列式为：\n$$\\det(R) = (2)(2) - (1)(1) = 4 - 1 = 3$$\n由于 $\\det(R) \\neq 0$，逆矩阵存在。对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆矩阵为 $\\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$。\n$$R^{-1} = \\frac{1}{3}\\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$$\n现在，我们可以计算 $\\mathbf{w}_o$：\n$$\\mathbf{w}_o = R^{-1}\\mathbf{p} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (\\frac{2}{3})(1) + (-\\frac{1}{3})(0) \\\\ (-\\frac{1}{3})(1) + (\\frac{2}{3})(0) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\n这是滤波器抽头的最优维纳解。\n\n接下来，我们确定最小均方误差 $J_{\\min}$，它是在 $\\mathbf{w} = \\mathbf{w}_o$ 处评估的代价函数的值：\n$$J_{\\min} = J(\\mathbf{w}_o) = \\sigma_d^2 - 2\\mathbf{w}_o^{\\top}\\mathbf{p} + \\mathbf{w}_o^{\\top}R\\mathbf{w}_o$$\n使用维纳-霍夫方程 $R\\mathbf{w}_o = \\mathbf{p}$，我们可以将 $\\mathbf{p}$ 代入 $J_{\\min}$ 的表达式中：\n$$J_{\\min} = \\sigma_d^2 - 2\\mathbf{w}_o^{\\top}(R\\mathbf{w}_o) + \\mathbf{w}_o^{\\top}R\\mathbf{w}_o = \\sigma_d^2 - \\mathbf{w}_o^{\\top}R\\mathbf{w}_o$$\n或者，更直接地，将 $\\mathbf{w}_o^{\\top}R = \\mathbf{p}^{\\top}$ 代入 $J_{\\min}$ 的表达式中：\n$$J_{\\min} = \\sigma_d^2 - \\mathbf{p}^{\\top}\\mathbf{w}_o - \\mathbf{w}_o^{\\top}\\mathbf{p} + \\mathbf{p}^{\\top}\\mathbf{w}_o= \\sigma_d^2 - \\mathbf{w}_o^{\\top}\\mathbf{p}$$\n这是最小 MSE 最常见和简化的表达式。使用我们已有的值：\n$\\sigma_d^2 = 1$，$\\mathbf{w}_o = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$，以及 $\\mathbf{p} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n$$\\mathbf{w}_o^{\\top}\\mathbf{p} = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (\\frac{2}{3})(1) + (-\\frac{1}{3})(0) = \\frac{2}{3}$$\n因此，最小均方误差是：\n$$J_{\\min} = \\sigma_d^2 - \\mathbf{w}_o^{\\top}\\mathbf{p} = 1 - \\frac{2}{3} = \\frac{1}{3}$$\n最优维纳滤波器为 $\\mathbf{w}_o = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$，最小 MSE 为 $J_{\\min} = \\frac{1}{3}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3}  -\\frac{1}{3}  \\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "2850046"}, {"introduction": "维纳滤波器需要完整的统计先验知识，这在实时应用中通常无法获得。最小均方 (LMS) 算法通过仅使用当前数据，以迭代方式逼近最优解，提供了一种高效的替代方案。[@problem_id:2850033] 这个练习将带你亲手完成LMS算法的单个更新步骤，它基于随机梯度下降原理。通过这个基础计算，你将具体理解滤波器权重如何根据瞬时误差进行调整，这是掌握几乎所有基于梯度的自适应滤波器的关键第一步。", "problem": "一个基带通信接收机使用一个$2$阶有限脉冲响应自适应均衡器来减轻码间串扰。在离散时间 $n=0$ 时，均衡器的输入回归向量为 $x_0=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$，期望符号为 $d_0=1$，当前系数向量为 $w_0=\\begin{bmatrix}0\\\\0\\end{bmatrix}$。该均衡器从 $w_0$ 开始，对瞬时平方误差代价函数 $J_0(w)=\\tfrac{1}{2}\\left(d_0 - x_0^{\\top}w\\right)^{2}$ 执行一步步长为 $\\mu=0.1$ 的随机梯度下降。将时间 $0$ 时的先验误差定义为 $e^{\\text{ap}}(0)=d_0 - x_0^{\\top}w_0$，后验误差定义为 $e^{\\text{po}}(0)=d_0 - x_0^{\\top}w_1$，其中 $w_1$ 是单次梯度步骤后的系数向量。\n\n仅使用应用于 $J_0(w)$ 的梯度下降原理和给定数据，计算更新后的系数向量 $w_1$ 和后验误差 $e^{\\text{po}}(0)$。请以单行矩阵的形式提供您的最终答案，顺序为 $\\big(w_{1,1},\\,w_{1,2},\\,e^{\\text{po}}(0)\\big)$。无需四舍五入。", "solution": "该问题陈述为随机梯度下降（SGD）算法的应用，特别是自适应信号处理中标准的最小均方（LMS）变体，提供了一个完整且适定的场景。该问题在科学上是合理的，并且为求得唯一解提供了所有必要的数据。因此，该问题是有效的。\n\n任务是从离散时间 $n=0$ 时的初始向量 $w_0$ 开始，对自适应均衡器的系数向量 $w$ 执行一步更新。该更新基于最小化瞬时平方误差代价函数 $J_0(w)$：\n$$J_0(w) = \\frac{1}{2}(d_0 - x_0^{\\top}w)^{2}$$\n梯度下降更新的通用公式为：\n$$w_1 = w_0 - \\mu \\nabla_w J_0(w) \\bigg|_{w=w_0}$$\n其中 $\\mu$ 是步长，$\\nabla_w J_0(w)$ 是代价函数关于向量 $w$ 的梯度。\n\n首先，我们必须推导代价函数的梯度。设时间 $n=0$ 时的瞬时误差为 $w$ 的函数，定义为 $e_0(w) = d_0 - x_0^{\\top}w$。则代价函数为 $J_0(w) = \\frac{1}{2}e_0(w)^2$。使用向量微分的链式法则：\n$$\\nabla_w J_0(w) = \\frac{\\partial J_0}{\\partial e_0} \\nabla_w e_0(w)$$\n各个导数分别为：\n$$\\frac{\\partial J_0}{\\partial e_0} = e_0(w)$$\n$$\\nabla_w e_0(w) = \\nabla_w (d_0 - x_0^{\\top}w) = -x_0$$\n综合这些，我们得到梯度：\n$$\\nabla_w J_0(w) = e_0(w)(-x_0) = -(d_0 - x_0^{\\top}w)x_0$$\n梯度必须在当前系数向量 $w_0$ 处求值：\n$$\\nabla_w J_0(w_0) = -(d_0 - x_0^{\\top}w_0)x_0$$\n项 $d_0 - x_0^{\\top}w_0$ 在问题中被定义为先验误差 $e^{\\text{ap}}(0)$。因此，梯度更新项为：\n$$\\nabla_w J_0(w_0) = -e^{\\text{ap}}(0) x_0$$\n将此代回梯度下降公式，得到此问题的特定更新规则，即LMS更新方程：\n$$w_1 = w_0 - \\mu (-e^{\\text{ap}}(0) x_0) = w_0 + \\mu e^{\\text{ap}}(0) x_0$$\n\n给定以下值：\n-   期望符号：$d_0 = 1$\n-   输入回归向量：$x_0 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$\n-   初始系数向量：$w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n-   步长：$\\mu = 0.1$\n\n第一步是计算先验误差 $e^{\\text{ap}}(0)$：\n$$e^{\\text{ap}}(0) = d_0 - x_0^{\\top}w_0 = 1 - \\begin{bmatrix} 1  -1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = 1 - (1 \\cdot 0 + (-1) \\cdot 0) = 1 - 0 = 1$$\n接下来，我们计算更新后的系数向量 $w_1$：\n$$w_1 = w_0 + \\mu e^{\\text{ap}}(0)x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + (0.1)(1)\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$$\n因此，更新后向量的分量为 $w_{1,1} = 0.1$ 和 $w_{1,2} = -0.1$。\n\n最后，我们计算后验误差 $e^{\\text{po}}(0)$，这是使用更新后的系数向量 $w_1$ 计算的误差：\n$$e^{\\text{po}}(0) = d_0 - x_0^{\\top}w_1$$\n代入已知值和计算值：\n$$e^{\\text{po}}(0) = 1 - \\begin{bmatrix} 1  -1 \\end{bmatrix}\\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix} = 1 - (1 \\cdot 0.1 + (-1) \\cdot (-0.1)) = 1 - (0.1 + 0.1) = 1 - 0.2 = 0.8$$\n所需的量为 $w_{1,1}=0.1$，$w_{1,2}=-0.1$ 和 $e^{\\text{po}}(0)=0.8$。这些将以一个单行矩阵的形式呈现。", "answer": "$$\\boxed{\\begin{pmatrix} 0.1  -0.1  0.8 \\end{pmatrix}}$$", "id": "2850033"}, {"introduction": "尽管LMS算法功能强大，但其收敛速度和稳定性对输入信号的功率很敏感，这在实际应用中可能导致性能问题。归一化最小均方 (NLMS) 算法通过对更新步长进行归一化来解决这一问题，从而提高了算法的鲁棒性。[@problem_id:2850035] 这个练习将向你展示NLMS算法的核心更新机制，其中步长会根据输入向量的能量进行动态调整。完成这个练习有助于你理解对基础LMS算法的一项关键改进，这对于设计在真实多变环境中可靠工作的自适应系统至关重要。", "problem": "一个双抽头自适应滤波器被用作单通道传感场景中噪声消除的简化模型。在时间索引 $n$ 处，滤波器权重向量为 $w(n) \\in \\mathbb{R}^{2}$，输入回归量为 $x(n) \\in \\mathbb{R}^{2}$，期望信号为 $d(n) \\in \\mathbb{R}$。在时间 $n$ 的目标是最小化瞬时平方误差，定义为 $J(n) = \\tfrac{1}{2} e^{2}(n)$，其中 $e(n) = d(n) - w^{\\top}(n) x(n)$。从这个目标和关于 $w$ 的梯度 $\\nabla_{w} J(n)$ 出发，最小均方 (LMS) 方法执行一个梯度步长。归一化最小均方 (NLMS) 方法通过瞬时输入功率重新缩放此步长，使其对输入幅度不变，并带有一个小的正正则化项以避免除以零。\n\n给定初始权重向量 $w(0) = [0, 0]^{\\top}$，步长 $\\mu = 1$，正则化项 $\\delta = 10^{-3}$，输入 $x(0) = [3, 4]^{\\top}$，以及期望响应 $d(0) = 5$，从 $J(n)$ 及其梯度的定义开始，执行一次归一化最小均方 (NLMS) 算法的更新，并计算：\n- 更新后的系数向量 $w(1)$，以及\n- 得到的后验瞬时误差 $e^{+}(0) \\equiv d(0) - w^{\\top}(1)\\,x(0)$。\n\n将您的最终答案表示为一个单行矩阵，按顺序包含 $w(1)$ 的两个分量，后跟 $e^{+}(0)$。无需四舍五入；请提供精确表达式。", "solution": "首先对问题陈述进行验证。\n\n**步骤1：提取已知条件**\n- **模型：** 双抽头自适应滤波器\n- **权重向量：** $w(n) \\in \\mathbb{R}^{2}$\n- **输入回归量：** $x(n) \\in \\mathbb{R}^{2}$\n- **期望信号：** $d(n) \\in \\mathbb{R}$\n- **目标函数：** $J(n) = \\frac{1}{2} e^{2}(n)$\n- **误差信号：** $e(n) = d(n) - w^{\\top}(n) x(n)$\n- **算法：** 归一化最小均方 (NLMS)\n- **初始权重向量：** $w(0) = [0, 0]^{\\top}$\n- **步长：** $\\mu = 1$\n- **正则化项：** $\\delta = 10^{-3}$\n- **$n=0$ 时的输入：** $x(0) = [3, 4]^{\\top}$\n- **$n=0$ 时的期望响应：** $d(0) = 5$\n- **要求输出：** $w(1)$ 和 $e^{+}(0) \\equiv d(0) - w^{\\top}(1)\\,x(0)$\n\n**步骤2：使用提取的已知条件进行验证**\n该问题具有科学依据，因为它涉及标准的 NLMS 算法，这是自适应信号处理中的一个基本主题。这是一个适定问题，为唯一解提供了所有必要的参数和初始条件。语言客观而精确。数据一致且完整。因此，该问题是有效的。\n\n**步骤3：结论与行动**\n该问题有效。将提供解答。\n\n目标是执行一次归一化最小均方 (NLMS) 算法的更新，以找到更新后的权重向量 $w(1)$ 和后验误差 $e^{+}(0)$。\n\n时间 $n$ 的瞬时成本函数是平方误差：\n$$J(n) = \\frac{1}{2} e^{2}(n) = \\frac{1}{2} (d(n) - w^{\\top}(n) x(n))^{2}$$\nNLMS 算法是随机梯度下降的一种形式。第一步是计算成本函数 $J(n)$ 相对于权重向量 $w(n)$ 的梯度。使用链式法则：\n$$\\nabla_{w} J(n) = \\frac{\\partial J(n)}{\\partial w(n)} = \\frac{\\partial J(n)}{\\partial e(n)} \\frac{\\partial e(n)}{\\partial w(n)}$$\n导数是：\n$$\\frac{\\partial J(n)}{\\partial e(n)} = e(n)$$\n$$\\frac{\\partial e(n)}{\\partial w(n)} = \\frac{\\partial}{\\partial w(n)} (d(n) - w^{\\top}(n) x(n)) = -x(n)$$\n因此，梯度是：\n$$\\nabla_{w} J(n) = -e(n) x(n)$$\n通用的随机梯度下降更新法则是 $w(n+1) = w(n) - \\alpha \\nabla_{w} J(n)$，其中 $\\alpha$ 是一个步长参数。对于 NLMS 算法，更新法则具体定义为：\n$$w(n+1) = w(n) - \\left( \\frac{\\mu}{\\delta + \\|x(n)\\|^{2}} \\right) \\nabla_{w} J(n)$$\n代入梯度的表达式，我们得到 NLMS 更新方程：\n$$w(n+1) = w(n) + \\frac{\\mu}{\\delta + \\|x(n)\\|^{2}} e(n) x(n)$$\n其中 $\\|x(n)\\|^{2} = x^{\\top}(n) x(n)$ 是输入向量的欧几里得范数的平方（瞬时功率）。\n\n我们被要求计算 $n=0$ 时的更新。给定的值是：\n$w(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，$x(0) = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$，$d(0) = 5$，$\\mu = 1$，以及 $\\delta = 10^{-3}$。\n\n首先，我们计算时间 $n=0$ 时的先验误差：\n$$e(0) = d(0) - w^{\\top}(0) x(0) = 5 - \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = 5 - 0 = 5$$\n接下来，我们计算输入向量 $x(0)$ 的范数平方：\n$$\\|x(0)\\|^{2} = x^{\\top}(0) x(0) = 3^{2} + 4^{2} = 9 + 16 = 25$$\n现在我们可以计算更新后的权重向量 $w(1)$：\n$$w(1) = w(0) + \\frac{\\mu}{\\delta + \\|x(0)\\|^{2}} e(0) x(0)$$\n$$w(1) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{10^{-3} + 25} (5) \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\n项 $\\delta + \\|x(0)\\|^2$ 是 $25.001$，即 $\\frac{25001}{1000}$。更新的系数是：\n$$\\frac{5}{25.001} = \\frac{5}{25001/1000} = \\frac{5000}{25001}$$\n所以，更新后的权重向量是：\n$$w(1) = \\frac{5000}{25001} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{15000}{25001} \\\\ \\frac{20000}{25001} \\end{pmatrix}$$\n更新后的权重向量的分量是 $w_{1}(1) = \\frac{15000}{25001}$ 和 $w_{2}(1) = \\frac{20000}{25001}$。\n\n最后，我们计算后验误差 $e^{+}(0)$：\n$$e^{+}(0) = d(0) - w^{\\top}(1) x(0)$$\n$$e^{+}(0) = 5 - \\begin{pmatrix} \\frac{15000}{25001}  \\frac{20000}{25001} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\n$$e^{+}(0) = 5 - \\left( \\frac{15000 \\cdot 3 + 20000 \\cdot 4}{25001} \\right)$$\n$$e^{+}(0) = 5 - \\left( \\frac{45000 + 80000}{25001} \\right) = 5 - \\frac{125000}{25001}$$\n$$e^{+}(0) = \\frac{5 \\cdot 25001 - 125000}{25001} = \\frac{125005 - 125000}{25001} = \\frac{5}{25001}$$\n\n所需的输出是 $w(1)$ 的两个分量和 $e^{+}(0)$ 的值。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{15000}{25001}  \\frac{20000}{25001}  \\frac{5}{25001} \\end{pmatrix}}$$", "id": "2850035"}]}