{"hands_on_practices": [{"introduction": "这是一项基础性练习，旨在引导您从优化第一性原理出发，推导递推最小二乘（RLS）算法。通过最小化一个指数加权的最小二乘代价函数，您将亲眼见证参数估计及其协方差的递推结构是如何自然产生的。这项实践将揭开RLS更新方程的神秘面紗，并将其牢固地建立在优化理论的基础之上。[@problem_id:2718871]", "problem": "考虑离散时间中的标量线性回归模型，其测量值 $y_k \\in \\mathbb{R}$ 根据 $y_k = \\varphi \\,\\theta^{\\star} + v_k$ 生成，其中回归量 $\\varphi \\in \\mathbb{R}$ 是一个已知的非零常数（即 $\\varphi \\neq 0$），未知常数参数为 $\\theta^{\\star} \\in \\mathbb{R}$，而 $\\{v_k\\}$ 是一个零均值、独立同分布且具有有限方差的测量噪声序列。您需要在每个时刻 $k \\geq 1$ 通过最小化指数加权最小二乘代价来在线估计 $\\theta^{\\star}$\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi \\,\\theta\\big)^2 + \\lambda^{k} (\\theta - \\theta_0)^2 P_0^{-1},\n$$\n其中 $\\lambda \\in (0,1)$ 是一个已知的遗忘因子，$\\theta_0 \\in \\mathbb{R}$ 是先验均值，$P_0  0$ 是先验方差。从最小二乘估计量作为 $J_k(\\theta)$ 的最小化器的核心定义出发，推导估计值 $\\theta_k$ 及其相关协方差 $P_k$ 的标量递归最小二乘 (RLS) 递推公式，且不假设任何预先给定的递推关系。然后，在常数回归量的特殊情况下，建立 $P_k$ 的闭式表达式，并利用该表达式在给定的噪声假设下分析当 $k \\to \\infty$ 时 $\\theta_k$ 和 $P_k$ 的收敛性质。作为最终答案，请仅用 $\\lambda$ 和 $\\varphi$ 报告闭式极限 $\\lim_{k\\to\\infty} P_k$。无需进行数值四舍五入，也不涉及物理单位。", "solution": "所提出的问题是递归参数估计中的一个标准练习，它具有科学依据、是适定的且客观的。因此，该问题被认为是有效的。我们按要求进行推导和分析。\n\n目标是在每个时间步 $k \\geq 1$ 找到最小化指数加权最小二乘代价函数的参数估计值 $\\theta_k$：\n$$\nJ_k(\\theta) \\triangleq \\sum_{i=1}^{k} \\lambda^{k-i} \\big(y_i - \\varphi \\,\\theta\\big)^2 + \\lambda^{k} (\\theta - \\theta_0)^2 P_0^{-1}\n$$\n其中 $\\lambda \\in (0,1)$。由于 $J_k(\\theta)$ 是关于 $\\theta$ 的二次凸函数，其最小值可以通过将其关于 $\\theta$ 的一阶导数设为零来找到。\n\n首先，我们计算导数：\n$$\n\\frac{dJ_k(\\theta)}{d\\theta} = \\sum_{i=1}^{k} \\lambda^{k-i} \\cdot 2 \\big(y_i - \\varphi \\,\\theta\\big) (-\\varphi) + \\lambda^{k} \\cdot 2 (\\theta - \\theta_0) P_0^{-1}\n$$\n在 $\\theta = \\theta_k$ 处令 $\\frac{dJ_k(\\theta)}{d\\theta} = 0$ 并除以 2：\n$$\n-\\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\varphi^2 \\theta_k \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1} (\\theta_k - \\theta_0) = 0\n$$\n重新整理各项以求解 $\\theta_k$：\n$$\n\\theta_k \\left( \\varphi^2 \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1} \\right) = \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0\n$$\n代价函数的海森矩阵是 $\\frac{d^2J_k(\\theta)}{d\\theta^2}$，估计的协方差 $P_k$ 定义为海森矩阵一半的逆。因此，\n$$\nP_k^{-1} \\triangleq \\varphi^2 \\sum_{i=1}^{k} \\lambda^{k-i} + \\lambda^{k} P_0^{-1}\n$$\n根据这个定义，$\\theta_k$ 的批处理解为：\n$$\n\\theta_k = P_k \\left( \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0 \\right)\n$$\n为了推导递推形式，我们首先建立 $P_k^{-1}$ 的递推关系。我们将 $P_k^{-1}$ 与 $P_{k-1}^{-1}$ 联系起来：\n$$\nP_k^{-1} = \\varphi^2 \\lambda^{k-k} + \\varphi^2 \\sum_{i=1}^{k-1} \\lambda^{k-i} + \\lambda^{k} P_0^{-1}\n$$\n$$\nP_k^{-1} = \\varphi^2 + \\lambda \\left( \\varphi^2 \\sum_{i=1}^{k-1} \\lambda^{k-1-i} + \\lambda^{k-1} P_0^{-1} \\right)\n$$\n括号中的项正是 $P_{k-1}^{-1}$ 的定义。这就得到了逆协方差的简单递推关系：\n$$\nP_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2\n$$\n为了得到 $P_k$ 的递推关系，我们应用 Sherman-Morrison-Woodbury 公式（或者，在这种标量情况下，直接求逆）：\n$$\nP_k = (\\lambda P_{k-1}^{-1} + \\varphi^2)^{-1} = \\left(\\frac{\\lambda}{P_{k-1}} + \\varphi^2\\right)^{-1} = \\left(\\frac{\\lambda + \\varphi^2 P_{k-1}}{P_{k-1}}\\right)^{-1} = \\frac{P_{k-1}}{\\lambda + \\varphi^2 P_{k-1}}\n$$\n现在，我们推导 $\\theta_k$ 的递推关系。我们展开 $P_k^{-1}\\theta_k$ 的表达式：\n$$\nP_k^{-1}\\theta_k = \\varphi \\sum_{i=1}^{k} \\lambda^{k-i} y_i + \\lambda^{k} P_0^{-1} \\theta_0 = \\varphi y_k + \\lambda \\left( \\varphi \\sum_{i=1}^{k-1} \\lambda^{k-1-i} y_i + \\lambda^{k-1} P_0^{-1} \\theta_0 \\right)\n$$\n将括号中的项识别为 $P_{k-1}^{-1}\\theta_{k-1}$，我们得到：\n$$\nP_k^{-1}\\theta_k = \\varphi y_k + \\lambda P_{k-1}^{-1} \\theta_{k-1}\n$$\n代入 $P_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2$，这意味着 $\\lambda P_{k-1}^{-1} = P_k^{-1} - \\varphi^2$：\n$$\nP_k^{-1}\\theta_k = \\varphi y_k + (P_k^{-1} - \\varphi^2) \\theta_{k-1}\n$$\n从左侧乘以 $P_k$：\n$$\n\\theta_k = P_k \\varphi y_k + (I - P_k \\varphi^2) \\theta_{k-1} = \\theta_{k-1} + P_k \\varphi (y_k - \\varphi \\theta_{k-1})\n$$\n因此，完整的标量 RLS 递推公式为：\n$$\n\\theta_k = \\theta_{k-1} + K_k (y_k - \\varphi \\theta_{k-1})\n$$\n$$\nK_k = P_k \\varphi = \\frac{P_{k-1}\\varphi}{\\lambda + \\varphi^2 P_{k-1}}\n$$\n$$\nP_k = (1 - K_k \\varphi) P_{k-1} \\lambda^{-1} \\text{ 或直接 } P_k = \\frac{P_{k-1}}{\\lambda + \\varphi^2 P_{k-1}}\n$$\n接下来，我们寻找 $P_k$ 的闭式表达式。递推式 $P_k^{-1} = \\lambda P_{k-1}^{-1} + \\varphi^2$ 是关于变量 $x_k \\triangleq P_k^{-1}$ 的线性一阶非齐次差分方程。其解可通过展开递推得到：\n$$\nP_k^{-1} = \\lambda^k P_0^{-1} + \\varphi^2 \\sum_{j=0}^{k-1} \\lambda^j\n$$\n有限几何级数的和为 $\\sum_{j=0}^{k-1} \\lambda^j = \\frac{1-\\lambda^k}{1-\\lambda}$。代入此式可得 $P_k^{-1}$ 的闭式表达式：\n$$\nP_k^{-1} = \\lambda^k P_0^{-1} + \\varphi^2 \\left(\\frac{1-\\lambda^k}{1-\\lambda}\\right)\n$$\n$P_k$ 的闭式表达式是上式的逆：\n$$\nP_k = \\left( \\lambda^k P_0^{-1} + \\frac{\\varphi^2(1-\\lambda^k)}{1-\\lambda} \\right)^{-1}\n$$\n我们现在分析当 $k \\to \\infty$ 时的收敛性。鉴于 $\\lambda \\in (0,1)$，我们有 $\\lim_{k\\to\\infty} \\lambda^k = 0$。\n逆协方差的极限为：\n$$\n\\lim_{k\\to\\infty} P_k^{-1} = \\lim_{k\\to\\infty} \\left( \\lambda^k P_0^{-1} + \\frac{\\varphi^2(1-\\lambda^k)}{1-\\lambda} \\right) = 0 \\cdot P_0^{-1} + \\frac{\\varphi^2(1-0)}{1-\\lambda} = \\frac{\\varphi^2}{1-\\lambda}\n$$\n由于 $\\varphi \\neq 0$ 且 $\\lambda \\in (0,1)$，该极限是有限且为正的。协方差 $P_k$ 的极限是该值的倒数：\n$$\n\\lim_{k\\to\\infty} P_k = \\left( \\lim_{k\\to\\infty} P_k^{-1} \\right)^{-1} = \\left(\\frac{\\varphi^2}{1-\\lambda}\\right)^{-1} = \\frac{1-\\lambda}{\\varphi^2}\n$$\n关于 $\\theta_k$ 的收敛性，考虑估计误差 $\\tilde{\\theta}_k \\triangleq \\theta_k - \\theta^{\\star}$。误差动态由 $\\tilde{\\theta}_k = (1 - P_k \\varphi^2)\\tilde{\\theta}_{k-1} + P_k \\varphi v_k$ 给出。当 $k \\to \\infty$ 时，$P_k \\to \\frac{1-\\lambda}{\\varphi^2}$，因此收缩因子 $(1 - P_k \\varphi^2) \\to (1 - (\\frac{1-\\lambda}{\\varphi^2})\\varphi^2) = \\lambda$。由于 $|\\lambda|  1$，该估计是渐近无偏的，即 $\\lim_{k\\to\\infty} E[\\theta_k] = \\theta^{\\star}$。估计误差的方差收敛到一个非零的稳态值 $E[\\tilde{\\theta}_{\\infty}^2] = \\frac{(1-\\lambda)\\sigma_v^2}{\\varphi^2(1+\\lambda)}$，其中 $\\sigma_v^2$ 是噪声 $v_k$ 的方差。这意味着 $\\theta_k$ 依分布收敛到一个以真值 $\\theta^{\\star}$ 为中心的随机变量，而不是收敛到真值本身。这是带有遗忘因子 $\\lambda  1$ 的 RLS 的预期行为。\n\n问题明确要求 $P_k$ 的闭式极限。", "answer": "$$\\boxed{\\frac{1-\\lambda}{\\varphi^2}}$$", "id": "2718871"}, {"introduction": "当遗忘因子为1时，批处理最小二乘法与递推最小二乘法是等价的，这是估计理论的基石之一。这个动手编程练习要求您通过数值计算来验证这一基本原理。通过同时实现批处理解（一次性使用所有数据）和RLS算法（逐个处理数据），您将证实它们能够得出完全相同的结果，从而巩固对两者内在联系的深刻理解。[@problem_id:2718829]", "problem": "考虑一个具有确定性回归量序列和未知参数向量高斯先验的线性回归测量模型。设参数为一个向量 $\\theta \\in \\mathbb{R}^n$，对于每个样本索引 $k \\in \\{1,\\dots,N\\}$，标量测量值 $y_k \\in \\mathbb{R}$ 通过线性模型 $y_k = \\varphi_k^\\top \\theta + v_k$ 与回归量向量 $\\varphi_k \\in \\mathbb{R}^n$ 相关联，其中 $v_k$ 是一个已知方差为 $R_k  0$ 的零均值噪声。假设一个高斯先验 $\\theta \\sim \\mathcal{N}(\\theta_0, P_0)$，其均值 $\\theta_0 \\in \\mathbb{R}^n$ 和对称正定协方差 $P_0 \\in \\mathbb{R}^{n \\times n}$ 均已知。该问题的基本基础是带有二次先验的加权最小二乘准则的定义，以及线性高斯建模假设。批量估计器定义为严格凸二次目标函数\n$$\nJ(\\theta) = (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N \\frac{\\left(y_k - \\varphi_k^\\top \\theta\\right)^2}{R_k}.\n$$\n的最小化器。递归最小二乘 (RLS) 估计器从相同的先验 $(\\theta_0, P_0)$ 开始，使用相同的测量方差 $\\{R_k\\}$，按给定顺序序贯处理相同的数据，并在处理完所有 $N$ 个样本后，生成一个更新后的估计值 $\\hat{\\theta}_N$。\n\n任务：编写一个完整、可运行的程序，为下面列出的每个测试用例计算：\n- 批量估计器 $\\hat{\\theta}_{\\text{batch}}$，作为 $J(\\theta)$ 的唯一最小化器，以及\n- 在按给定顺序处理完所有样本后，使用相同的先验 $(\\theta_0, P_0)$ 进行初始化的最终 RLS 估计器 $\\hat{\\theta}_{N}$。\n\n然后，对于每个测试用例，计算欧几里得范数 $\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$，将其与容差 $\\tau = 10^{-7}$ 进行比较，并返回一个布尔结果：当且仅当范数小于或等于 $\\tau$ 时，结果为 $true$，否则为 $false$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[true,false,true,true,false]”）。该列表必须为每个测试用例包含一个布尔值，顺序与下方呈现的顺序相同。\n\n测试套件（每个用例指定了 $n$、$N$、回归量矩阵 $\\Phi \\in \\mathbb{R}^{N \\times n}$（其第 $k$ 行为 $\\varphi_k^\\top$）、测量向量 $y \\in \\mathbb{R}^N$、测量方差 $R \\in \\mathbb{R}^N$（其条目为 $R_k$）、先验均值 $\\theta_0 \\in \\mathbb{R}^n$ 和先验协方差 $P_0 \\in \\mathbb{R}^{n \\times n}$）：\n\n- 用例 1（理想路径，满秩，异构方差）：\n  - $n = 3$, $N = 6$。\n  - $\\Phi = \\begin{bmatrix}\n  1.0  0.5  -0.3 \\\\\n  0.2  -1.0  0.7 \\\\\n  1.5  0.0  0.5 \\\\\n  -0.3  0.8  1.2 \\\\\n  0.0  -0.4  2.0 \\\\\n  1.0  1.0  1.0\n  \\end{bmatrix}$。\n  - $y = [\\, 0.7,\\, -1.2,\\, 1.5,\\, 0.3,\\, 0.0,\\, 2.0 \\,]$。\n  - $R = [\\, 0.5,\\, 1.2,\\, 0.8,\\, 1.0,\\, 0.3,\\, 2.0 \\,]$。\n  - $\\theta_0 = [\\, 0.1,\\, -0.2,\\, 0.3 \\,]$。\n  - $P_0 = \\mathrm{diag}([\\, 1.0,\\, 2.0,\\, 0.5 \\,])$。\n\n- 用例 2（秩亏设计，先验正则化保证唯一性）：\n  - $n = 4$, $N = 5$。\n  - $\\Phi = \\begin{bmatrix}\n  1.0  0.0  0.0  0.0 \\\\\n  0.0  1.0  0.0  0.0 \\\\\n  1.0  0.0  0.0  0.0 \\\\\n  0.0  1.0  0.0  0.0 \\\\\n  1.0  1.0  0.0  0.0\n  \\end{bmatrix}$。\n  - $y = [\\, 1.0,\\, 2.0,\\, 1.5,\\, 1.8,\\, 3.1 \\,]$。\n  - $R = [\\, 0.1,\\, 0.2,\\, 0.3,\\, 0.4,\\, 0.5 \\,]$。\n  - $\\theta_0 = [\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0 \\,]$。\n  - $P_0 = \\mathrm{diag}([\\, 10.0,\\, 10.0,\\, 1.0,\\, 1.0 \\,])$。\n\n- 用例 3（单样本边界情况）：\n  - $n = 2$, $N = 1$。\n  - $\\Phi = \\begin{bmatrix} 2.0  -1.0 \\end{bmatrix}$。\n  - $y = [\\, 0.5 \\,]$。\n  - $R = [\\, 0.05 \\,]$。\n  - $\\theta_0 = [\\, 1.0,\\, -1.0 \\,]$。\n  - $P_0 = \\begin{bmatrix} 0.2  0.05 \\\\ 0.05  0.5 \\end{bmatrix}$。\n\n- 用例 4（病态缩放，异构先验方差）：\n  - $n = 3$, $N = 4$。\n  - $\\Phi = \\begin{bmatrix}\n  10^{6}  10^{-6}  1.0 \\\\\n  10^{6}  -10^{-6}  -1.0 \\\\\n  2\\cdot 10^{6}  0.0  0.5 \\\\\n  -10^{6}  10^{-6}  -0.5\n  \\end{bmatrix}$。\n  - $y = [\\, 1.0,\\, -1.0,\\, 0.5,\\, -0.2 \\,]$。\n  - $R = [\\, 1.0,\\, 2.0,\\, 1.5,\\, 0.7 \\,]$。\n  - $\\theta_0 = [\\, 0.0,\\, 0.0,\\, 0.0 \\,]$。\n  - $P_0 = \\mathrm{diag}([\\, 10^{-12},\\, 10^{12},\\, 1.0 \\,])$。\n\n- 用例 5（不等测量方差，强调加权行为）：\n  - $n = 3$, $N = 3$。\n  - $\\Phi = \\begin{bmatrix}\n  1.0  2.0  3.0 \\\\\n  4.0  5.0  6.0 \\\\\n  7.0  8.0  10.0\n  \\end{bmatrix}$。\n  - $y = [\\, 14.0,\\, 32.0,\\, 50.0 \\,]$。\n  - $R = [\\, 0.5,\\, 100.0,\\, 0.1 \\,]$。\n  - $\\theta_0 = [\\, 0.0,\\, 0.0,\\, 0.0 \\,]$。\n  - $P_0 = I_3$（$3 \\times 3$ 单位矩阵）。\n\n角度单位不适用。不涉及物理单位。容差为 $\\tau = 10^{-7}$，必须将欧几里得范数与此容差进行比较。最终输出必须是包含与五个用例相对应的布尔值列表的单行文本，并按顺序排列。", "solution": "在尝试任何解决方案之前，首先对问题陈述的有效性进行严格审查。\n\n**步骤 1：提取已知条件**\n\n测量模型由 $y_k = \\varphi_k^\\top \\theta + v_k$ 给出，其中 $k \\in \\{1, \\dots, N\\}$，$\\theta \\in \\mathbb{R}^n$ 是未知参数向量，$y_k \\in \\mathbb{R}$ 是标量测量值，$\\varphi_k \\in \\mathbb{R}^n$ 是回归量向量，$v_k$ 是一个已知方差为 $R_k  0$ 的零均值噪声。\n参数的先验是一个高斯分布 $\\theta \\sim \\mathcal{N}(\\theta_0, P_0)$，其已知均值为 $\\theta_0 \\in \\mathbb{R}^n$，已知对称正定协方差为 $P_0 \\in \\mathbb{R}^{n \\times n}$。\n批量估计器 $\\hat{\\theta}_{\\text{batch}}$ 被定义为目标函数的唯一最小化器：\n$$\nJ(\\theta) = (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N \\frac{\\left(y_k - \\varphi_k^\\top \\theta\\right)^2}{R_k}.\n$$\n递归最小二乘 (RLS) 估计器 $\\hat{\\theta}_N$ 被定义为从相同的先验 $(\\theta_0, P_0)$ 开始，对测量值 $\\{y_k\\}_{k=1}^N$ 进行序贯处理的结果。\n该任务要求比较两种方法的最终估计器 $\\hat{\\theta}_{\\text{batch}}$ 和 $\\hat{\\theta}_N$，通过计算它们之间差值的欧几里得范数 $\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$，并检查该范数是否小于或等于容差 $\\tau = 10^{-7}$。\n提供了五个测试用例，每个用例都指定了参数 $n$、$N$、$\\Phi$、$y$、$R$、$\\theta_0$ 和 $P_0$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n根据指定的验证标准对问题进行分析。\n\n1.  **有科学依据**：该问题是线性高斯系统中贝叶斯估计的一个标准练习。带有二次先验（对应于高斯先验）的加权最小二乘、批量估计和递归估计（特别是用于静态参数的卡尔曼滤波器）等概念是统计信号处理和控制理论中基本且公认的原则。该问题在科学上是合理的。\n\n2.  **适定性**：目标函数 $J(\\theta)$ 是二次项之和。先验项 $(\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0)$ 是关于 $\\theta$ 的严格凸函数，因为 $P_0$ 被给定为对称正定，这意味着其逆 $P_0^{-1}$ 也是对称正定的。测量项 $\\sum_{k=1}^N \\frac{(y_k - \\varphi_k^\\top \\theta)^2}{R_k}$ 是凸的（半正定二次型）。一个严格凸函数与一个或多个凸函数之和是严格凸的。严格凸函数具有唯一的最小值。因此，最小化 $J(\\theta)$ 的唯一解 $\\hat{\\theta}_{\\text{batch}}$ 总是存在的。即使在仅凭测量信息是秩亏的情况下（如用例 2），这也成立，因为先验项起到了对问题进行正则化的作用。该问题是适定的。\n\n3.  **客观性**：该问题用精确的数学语言表述。所有变量和目标都得到了明确的定义。它不含任何主观或基于意见的陈述。\n\n4.  **完整性**：为五个测试用例中的每一个都提供了所有必要的数据（$n$、$N$、$\\Phi$、$y$、$R$、$\\theta_0$、$P_0$）。该问题是自洽的。\n\n5.  **结论**：该问题是有效的。它是估计理论领域中一个适定的、有科学依据的、客观的任务。它是对批量和递归贝叶斯最小二乘估计之间基本代数等价性的验证。\n\n**步骤 3：判定与行动**\n\n该问题被判定为**有效**。将提供一个解决方案。\n\n问题的核心是证明线性高斯模型的批量估计器和递归估计器产生相同的结果。我们将首先推导批量估计器的闭式表达式，然后推导递归估计器的迭代方程。\n\n**批量估计器推导**\n\n批量估计器 $\\hat{\\theta}_{\\text{batch}}$ 最小化成本函数 $J(\\theta)$。为找到最小值，我们计算 $J(\\theta)$ 相对于 $\\theta$ 的梯度并将其设为零。\n$$\n\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\left( (\\theta - \\theta_0)^\\top P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N R_k^{-1} (y_k - \\varphi_k^\\top \\theta)^2 \\right) = 0\n$$\n使用向量微积分的标准法则，梯度为：\n$$\n\\nabla_\\theta J(\\theta) = 2 P_0^{-1} (\\theta - \\theta_0) + \\sum_{k=1}^N R_k^{-1} \\cdot 2(y_k - \\varphi_k^\\top \\theta)(-\\varphi_k) = 0\n$$\n除以 $2$ 并重新排列各项：\n$$\nP_0^{-1} \\theta - P_0^{-1} \\theta_0 - \\sum_{k=1}^N R_k^{-1} \\varphi_k y_k + \\sum_{k=1}^N R_k^{-1} \\varphi_k \\varphi_k^\\top \\theta = 0\n$$\n将包含 $\\theta$ 的项分组：\n$$\n\\left( P_0^{-1} + \\sum_{k=1}^N \\frac{\\varphi_k \\varphi_k^\\top}{R_k} \\right) \\theta = P_0^{-1} \\theta_0 + \\sum_{k=1}^N \\frac{\\varphi_k y_k}{R_k}\n$$\n令回归量矩阵为 $\\Phi \\in \\mathbb{R}^{N \\times n}$，其行为 $\\varphi_k^\\top$，测量向量为 $y \\in \\mathbb{R}^N$，加权矩阵为 $W = \\mathrm{diag}(R_1^{-1}, \\dots, R_N^{-1})$。这些和可以用矩阵形式表示：$\\sum_{k=1}^N \\frac{\\varphi_k \\varphi_k^\\top}{R_k} = \\Phi^\\top W \\Phi$ 和 $\\sum_{k=1}^N \\frac{\\varphi_k y_k}{R_k} = \\Phi^\\top W y$。\n方程变为：\n$$\n\\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right) \\hat{\\theta}_{\\text{batch}} = P_0^{-1} \\theta_0 + \\Phi^\\top W y\n$$\n因此，批量估计器的解为：\n$$\n\\hat{\\theta}_{\\text{batch}} = \\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right)^{-1} \\left( P_0^{-1} \\theta_0 + \\Phi^\\top W y \\right)\n$$\n矩阵 $\\left( P_0^{-1} + \\Phi^\\top W \\Phi \\right)$ 是后验协方差 $P_N$ 的逆，即 $P_N^{-1}$，并且保证是可逆的。\n\n**递归最小二乘 (RLS) 估计器推导**\n\nRLS 估计器在每个测量值 $y_k$ 可用时，序贯地更新 $\\theta$ 的估计。这是卡尔曼滤波方程在静态参数系统（$\\theta_{k} = \\theta_{k-1}$）中的直接应用。第 $k$ 步的估计值 $\\hat{\\theta}_k$ 及其相关协方差 $P_k$ 是基于第 $k-1$ 步的估计值和新的测量值 $y_k$ 计算得出的。\n\n该过程从先验信息开始：\n- 初始估计：$\\hat{\\theta}_0 = \\theta_0$\n- 初始协方差：$P_0 = P_0$\n\n对于每个测量 $k = 1, 2, \\dots, N$，执行以下更新步骤：\n1.  **新息（预测误差）**：实际测量值与预测测量值之间的差值。\n    $$e_k = y_k - \\varphi_k^\\top \\hat{\\theta}_{k-1}$$\n2.  **新息协方差**：新息的方差。\n    $$S_k = \\varphi_k^\\top P_{k-1} \\varphi_k + R_k$$\n3.  **卡尔曼增益**：赋予新息的权重。它平衡了当前估计的不确定性和新测量的不确定性。\n    $$K_k = P_{k-1} \\varphi_k S_k^{-1}$$\n4.  **状态（参数）更新**：新估计是旧估计加上一个与新息成比例的修正量。\n    $$\\hat{\\theta}_k = \\hat{\\theta}_{k-1} + K_k e_k$$\n5.  **协方差更新**：估计的不确定性降低。使用数值稳定的形式。\n    $$P_k = (I - K_k \\varphi_k^\\top) P_{k-1}$$\n\n在遍历所有 $N$ 个测量值后，最终的 RLS 估计为 $\\hat{\\theta}_N$，最终的协方差为 $P_N$。估计理论中的一个基本结果是，在理想算术条件下，$\\hat{\\theta}_N$ 与 $\\hat{\\theta}_{\\text{batch}}$ 在代数上是完全相同的。数值实现将在指定的浮点容差范围内验证此等价性。\n\n**数值实现计划**\n\n将编写一个程序，为每个测试用例执行以下操作：\n1.  使用推导出的矩阵公式计算 $\\hat{\\theta}_{\\text{batch}}$。为保证数值稳定性，将使用线性系统求解器而非显式矩阵求逆：求解 $A x = b$ 来得到 $x$ 优于计算 $A^{-1}b$。\n2.  通过实现 RLS 循环来计算 $\\hat{\\theta}_N$，从 $(\\theta_0, P_0)$ 开始，并遍历 $N$ 个测量值。为保持数值稳定性和正确性，可以在每一步显式地对协方差矩阵 $P_k$ 进行对称化，以抵消潜在的浮点不精确性。\n3.  计算差值的欧几里得范数：$\\|\\hat{\\theta}_{\\text{batch}} - \\hat{\\theta}_N\\|_2$。\n4.  将此范数与容差 $\\tau = 10^{-7}$ 进行比较，并生成布尔结果。\n\n最终输出将是这些布尔结果的列表，每个测试用例一个，并按要求格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    \n    # Tolerance for comparison of the Euclidean norm.\n    tau = 1e-7\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, full rank, heterogeneous variances)\n        {\n            \"n\": 3, \"N\": 6,\n            \"Phi\": np.array([\n                [1.0, 0.5, -0.3], [0.2, -1.0, 0.7], [1.5, 0.0, 0.5],\n                [-0.3, 0.8, 1.2], [0.0, -0.4, 2.0], [1.0, 1.0, 1.0]\n            ]),\n            \"y\": np.array([0.7, -1.2, 1.5, 0.3, 0.0, 2.0]),\n            \"R\": np.array([0.5, 1.2, 0.8, 1.0, 0.3, 2.0]),\n            \"theta0\": np.array([0.1, -0.2, 0.3]),\n            \"P0\": np.diag([1.0, 2.0, 0.5]),\n        },\n        # Case 2 (rank-deficient design, regularization from prior ensures uniqueness)\n        {\n            \"n\": 4, \"N\": 5,\n            \"Phi\": np.array([\n                [1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0]\n            ]),\n            \"y\": np.array([1.0, 2.0, 1.5, 1.8, 3.1]),\n            \"R\": np.array([0.1, 0.2, 0.3, 0.4, 0.5]),\n            \"theta0\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"P0\": np.diag([10.0, 10.0, 1.0, 1.0]),\n        },\n        # Case 3 (single-sample boundary case)\n        {\n            \"n\": 2, \"N\": 1,\n            \"Phi\": np.array([[2.0, -1.0]]),\n            \"y\": np.array([0.5]),\n            \"R\": np.array([0.05]),\n            \"theta0\": np.array([1.0, -1.0]),\n            \"P0\": np.array([[0.2, 0.05], [0.05, 0.5]]),\n        },\n        # Case 4 (ill-conditioned scaling, heterogeneous prior variances)\n        {\n            \"n\": 3, \"N\": 4,\n            \"Phi\": np.array([\n                [1e6, 1e-6, 1.0], [1e6, -1e-6, -1.0],\n                [2e6, 0.0, 0.5], [-1e6, 1e-6, -0.5]\n            ]),\n            \"y\": np.array([1.0, -1.0, 0.5, -0.2]),\n            \"R\": np.array([1.0, 2.0, 1.5, 0.7]),\n            \"theta0\": np.array([0.0, 0.0, 0.0]),\n            \"P0\": np.diag([1e-12, 1e12, 1.0]),\n        },\n        # Case 5 (unequal measurement variances emphasizing weighting behavior)\n        {\n            \"n\": 3, \"N\": 3,\n            \"Phi\": np.array([\n                [1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 10.0]\n            ]),\n            \"y\": np.array([14.0, 32.0, 50.0]),\n            \"R\": np.array([0.5, 100.0, 0.1]),\n            \"theta0\": np.array([0.0, 0.0, 0.0]),\n            \"P0\": np.eye(3),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack case parameters\n        n, N = case[\"n\"], case[\"N\"]\n        Phi, y = case[\"Phi\"], case[\"y\"]\n        R, theta0, P0 = case[\"R\"], case[\"theta0\"], case[\"P0\"]\n\n        # 1. Batch Estimator\n        # theta_batch = (P0_inv + Phi^T W Phi)^-1 (P0_inv theta0 + Phi^T W y)\n        # where W = diag(1/R_k)\n        P0_inv = np.linalg.inv(P0)\n        W = np.diag(1.0 / R)\n        \n        # Information matrix: I = P0_inv + Phi^T W Phi\n        info_matrix = P0_inv + Phi.T @ W @ Phi\n        # Information vector: i = P0_inv theta0 + Phi^T W y\n        info_vector = P0_inv @ theta0 + Phi.T @ W @ y\n        \n        # Solve I * theta = i for theta. Numerically more stable than inv(I) @ i.\n        theta_batch = np.linalg.solve(info_matrix, info_vector)\n\n        # 2. Recursive Least Squares (RLS) Estimator\n        theta_rls = theta0.copy().astype(np.float64)\n        P_rls = P0.copy().astype(np.float64)\n        \n        for k in range(N):\n            # Get k-th measurement and regressor\n            phi_k = Phi[k, :].reshape(n, 1)\n            y_k = y[k]\n            R_k = R[k]\n            \n            # Innovation\n            e_k = y_k - phi_k.T @ theta_rls\n            \n            # Innovation covariance\n            S_k = phi_k.T @ P_rls @ phi_k + R_k\n            \n            # Kalman Gain\n            K_k = (P_rls @ phi_k) / S_k\n            \n            # State update\n            theta_rls = theta_rls + (K_k * e_k).flatten()\n            \n            # Covariance update (Joseph form is most stable, but this is simpler and sufficient)\n            P_rls = (np.eye(n) - K_k @ phi_k.T) @ P_rls\n            # Enforce symmetry to prevent numerical drift\n            P_rls = 0.5 * (P_rls + P_rls.T)\n\n        # 3. Comparison\n        # Calculate the Euclidean norm of the difference vector\n        norm_diff = np.linalg.norm(theta_batch - theta_rls)\n        \n        # Compare against the tolerance\n        is_equivalent = norm_diff = tau\n        results.append(str(is_equivalent).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2718829"}, {"introduction": "虽然正规方程组为求解最小二乘问题提供了一条直接路径，但当问题是病态的（ill-conditioned）时，其数值稳定性非常差。本实践将介绍QR分解作为一种在实际计算中更为优越的方法。您将利用QR分解推导最小二乘解，并重点分析为何该方法能避免因条件数平方而导致的数值陷阱，这对于稳健的工程应用至关重要。[@problem_id:2718842]", "problem": "考虑在面向控制的系统辨识中的超定线性模型，其中测量输出向量 $\\mathbf{y} \\in \\mathbb{R}^{m}$ 与回归矩阵 $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ 和未知参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{n}$ 通过 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$ 相关联，其中 $m \\geq n$ 且 $\\operatorname{rank}(\\mathbf{X}) = n$。最小二乘 (LS) 估计 $\\widehat{\\boldsymbol{\\theta}}$ 最小化残差的欧几里得范数，即 $\\widehat{\\boldsymbol{\\theta}} \\in \\arg\\min_{\\boldsymbol{\\theta} \\in \\mathbb{R}^{n}} \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}$。假设存在瘦正交三角分解 $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$，其中 $\\mathbf{Q} \\in \\mathbb{R}^{m \\times n}$ 满足 $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$，$\\mathbf{R} \\in \\mathbb{R}^{n \\times n}$ 是对角元为正的上三角矩阵。\n\n仅使用最小二乘准则 $\\min_{\\boldsymbol{\\theta}} \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}$ 的定义、与具有标准正交列的矩阵相乘保持欧几里得范数不变这一事实，以及上三角系统的标准性质，推导唯一最小化子 $\\widehat{\\boldsymbol{\\theta}}$ 关于 $\\mathbf{Q}$、$\\mathbf{R}$ 和 $\\mathbf{y}$ 的显式表达式。\n\n然后，从数值线性代数和有限精度算法的基本原理出发，论证为什么通过 $\\mathbf{Q}\\mathbf{R}$ 分解求解最小二乘问题比构建并求解正规方程 $\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$ 在数值上更稳定。你的论证必须基于欧几里得范数、条件数以及条件数平方的影响。\n\n最后，计算给定数据的最小二乘估计：\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  1 \\\\\n1  -1 \\\\\n0  1\n\\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix}\n2 \\\\ 0 \\\\ 1\n\\end{pmatrix}.\n$$\n使用你基于 $\\mathbf{Q}\\mathbf{R}$ 的推导来获得 $\\widehat{\\boldsymbol{\\theta}}$，而无需构建 $\\mathbf{X}^{\\top}\\mathbf{X}$。使用 $\\mathrm{pmatrix}$ 环境将最终答案以单行向量的形式报告。无需四舍五入。", "solution": "该问题是适定的，有科学依据，并包含了完整解答所需的所有信息。我们将按要求分三部分进行。\n\n首先，我们使用所提供的回归矩阵 $\\mathbf{X}$ 的瘦正交三角 ($\\mathbf{Q}\\mathbf{R}$) 分解来推导最小二乘估计量 $\\widehat{\\boldsymbol{\\theta}}$。目标是找到最小化残差向量 $\\mathbf{r} = \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}$ 的欧几里得范数平方的 $\\widehat{\\boldsymbol{\\theta}}$，这由代价函数 $J(\\boldsymbol{\\theta}) = \\|\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}^{2}$ 给出。\n\n我们将瘦 $\\mathbf{Q}\\mathbf{R}$ 分解 $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ 代入代价函数：\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}\\|_{2}^{2}\n$$\n此处，$\\mathbf{Q} \\in \\mathbb{R}^{m \\times n}$ 具有标准正交列，即 $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$。为了利用正交矩阵的保范性，我们考虑一个完整的正交矩阵 $\\mathbf{Q}_{\\text{full}} \\in \\mathbb{R}^{m \\times m}$，它可以通过向 $\\mathbf{Q}$ 追加 $m-n$ 个标准正交列 $\\mathbf{Q}_{\\perp}$ 来构造，使得 $\\mathbf{Q}_{\\text{full}} = \\begin{pmatrix} \\mathbf{Q}  \\mathbf{Q}_{\\perp} \\end{pmatrix}$。由于 $\\mathbf{Q}_{\\text{full}}$ 是一个正交矩阵，所以 $\\mathbf{Q}_{\\text{full}}^{\\top}\\mathbf{Q}_{\\text{full}} = \\mathbf{Q}_{\\text{full}}\\mathbf{Q}_{\\text{full}}^{\\top} = \\mathbf{I}_{m}$。\n\n向量乘以一个正交矩阵会保持其欧几里得范数不变。因此，我们有：\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{Q}_{\\text{full}}^{\\top}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y})\\|_{2}^{2}\n$$\n我们使用 $\\mathbf{Q}_{\\text{full}}^{\\top} = \\begin{pmatrix} \\mathbf{Q}^{\\top} \\\\ \\mathbf{Q}_{\\perp}^{\\top} \\end{pmatrix}$ 的分块形式展开范数内的表达式：\n$$\n\\mathbf{Q}_{\\text{full}}^{\\top}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}) = \\begin{pmatrix} \\mathbf{Q}^{\\top} \\\\ \\mathbf{Q}_{\\perp}^{\\top} \\end{pmatrix}(\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{y}) = \\begin{pmatrix} \\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\n使用性质 $\\mathbf{Q}^{\\top}\\mathbf{Q} = \\mathbf{I}_{n}$，上式化简为：\n$$\n\\begin{pmatrix} \\mathbf{I}_{n}\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y} \\\\ -\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\n这个分块向量的范数平方是其各分量范数平方之和：\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{R}\\boldsymbol{\\theta} - \\mathbf{Q}^{\\top}\\mathbf{y}\\|_{2}^{2} + \\|-\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y}\\|_{2}^{2}\n$$\n为了最小化 $J(\\boldsymbol{\\theta})$，我们必须选择 $\\boldsymbol{\\theta}$ 来最小化这个和。第二项 $\\|-\\mathbf{Q}_{\\perp}^{\\top}\\mathbf{y}\\|_{2}^{2}$ 是与 $\\mathbf{X}$ 的列空间正交的残差分量的范数平方，且与 $\\boldsymbol{\\theta}$ 无关。因此，最小化是通过最小化第一项来实现的。范数平方的最小可能值为 $0$。这个最小值是可以达到的，因此我们将第一项设为零：\n$$\n\\|\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} - \\mathbf{Q}^{\\top}\\mathbf{y}\\|_{2}^{2} = 0 \\implies \\mathbf{R}\\widehat{\\boldsymbol{\\theta}} - \\mathbf{Q}^{\\top}\\mathbf{y} = \\mathbf{0}\n$$\n这就得出了上三角线性方程组：\n$$\n\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^{\\top}\\mathbf{y}\n$$\n问题陈述了 $\\operatorname{rank}(\\mathbf{X}) = n$ 且 $\\mathbf{R}$ 的对角元为正。这确保了 $\\mathbf{R}$ 是可逆的。因此，唯一的最小二乘估计 $\\widehat{\\boldsymbol{\\theta}}$ 由下式给出：\n$$\n\\widehat{\\boldsymbol{\\theta}} = \\mathbf{R}^{-1}\\mathbf{Q}^{\\top}\\mathbf{y}\n$$\n这个方程组可以通过回代法高效求解，而无需显式计算 $\\mathbf{R}^{-1}$。\n\n第二，我们论证 $\\mathbf{Q}\\mathbf{R}$ 方法相对于求解正规方程 $\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$ 的优越数值稳定性。求解线性系统的稳定性与系统矩阵的条件数有关。矩阵 $\\mathbf{A}$ 关于欧几里得范数的条件数是 $\\kappa_2(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\|\\mathbf{A}^{-1}\\|_2$。对于一个满秩长方矩阵 $\\mathbf{X}$，最小二乘问题的条件数是 $\\kappa_2(\\mathbf{X}) = \\frac{\\sigma_{\\max}(\\mathbf{X})}{\\sigma_{\\min}(\\mathbf{X})}$，其中 $\\sigma_{\\max}$ 和 $\\sigma_{\\min}$ 分别是 $\\mathbf{X}$ 的最大和最小奇异值。\n\n正规方程法涉及构建矩阵 $\\mathbf{A} = \\mathbf{X}^{\\top}\\mathbf{X}$，然后求解 $n \\times n$ 系统 $\\mathbf{A}\\boldsymbol{\\theta} = \\mathbf{X}^{\\top}\\mathbf{y}$。该系统的条件数是 $\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X})$。对称正定矩阵 $\\mathbf{X}^{\\top}\\mathbf{X}$ 的特征值是 $\\mathbf{X}$ 的奇异值的平方。因此，$\\lambda_{\\max}(\\mathbf{X}^{\\top}\\mathbf{X}) = \\sigma_{\\max}(\\mathbf{X})^2$ 且 $\\lambda_{\\min}(\\mathbf{X}^{\\top}\\mathbf{X}) = \\sigma_{\\min}(\\mathbf{X})^2$。$\\mathbf{X}^{\\top}\\mathbf{X}$ 的条件数是：\n$$\n\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X}) = \\frac{\\lambda_{\\max}(\\mathbf{X}^{\\top}\\mathbf{X})}{\\lambda_{\\min}(\\mathbf{X}^{\\top}\\mathbf{X})} = \\frac{\\sigma_{\\max}(\\mathbf{X})^2}{\\sigma_{\\min}(\\mathbf{X})^2} = \\left(\\frac{\\sigma_{\\max}(\\mathbf{X})}{\\sigma_{\\min}(\\mathbf{X})}\\right)^2 = (\\kappa_2(\\mathbf{X}))^2\n$$\n这表明构建正规方程会将原问题的条件数平方。如果 $\\mathbf{X}$ 是病态的（即 $\\kappa_2(\\mathbf{X})$ 很大），$\\kappa_2(\\mathbf{X}^{\\top}\\mathbf{X})$ 将会大得多，从而严重放大有限精度算法和舍入误差的影响。在求解系统之前，信息可能在构建 $\\mathbf{X}^{\\top}\\mathbf{X}$ 的过程中就已经丢失了。\n\n相比之下，$\\mathbf{Q}\\mathbf{R}$ 方法求解系统 $\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^{\\top}\\mathbf{y}$。该系统的条件数是 $\\kappa_2(\\mathbf{R})$。由于 $\\mathbf{X} = \\mathbf{Q}\\mathbf{R}$ 且 $\\mathbf{Q}$ 具有标准正交列，$\\mathbf{Q}$ 的作用等同于一个等距映射。可以证明 $\\kappa_2(\\mathbf{X}) = \\kappa_2(\\mathbf{Q}\\mathbf{R}) = \\kappa_2(\\mathbf{R})$。因此，$\\mathbf{Q}\\mathbf{R}$ 方法求解的是一个条件数为 $\\kappa_2(\\mathbf{X})$ 而非其平方的系统。通过避免构建 $\\mathbf{X}^{\\top}\\mathbf{X}$，$\\mathbf{Q}\\mathbf{R}$ 方法绕过了与条件数平方相关的数值不稳定性，使其成为求解最小二乘问题的一种更鲁棒、更精确的方法，特别是对于病态矩阵 $\\mathbf{X}$。\n\n最后，我们计算给定数据的最小二乘估计：\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\\\ 0  1 \\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n我们首先使用 Gram-Schmidt 过程对其列向量 $\\mathbf{x}_1 = \\begin{pmatrix} 1  1  0 \\end{pmatrix}^\\top$ 和 $\\mathbf{x}_2 = \\begin{pmatrix} 1  -1  1 \\end{pmatrix}^\\top$ 计算 $\\mathbf{X}$ 的瘦 $\\mathbf{Q}\\mathbf{R}$ 分解。\n对于第一列：\n$r_{11} = \\|\\mathbf{x}_1\\|_2 = \\sqrt{1^2+1^2+0^2} = \\sqrt{2}$。\n$\\mathbf{q}_1 = \\frac{\\mathbf{x}_1}{r_{11}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n对于第二列：\n$r_{12} = \\mathbf{q}_1^\\top \\mathbf{x}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}(1-1) = 0$。\n$\\mathbf{u}_2 = \\mathbf{x}_2 - r_{12}\\mathbf{q}_1 = \\mathbf{x}_2 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$。\n$r_{22} = \\|\\mathbf{u}_2\\|_2 = \\sqrt{1^2+(-1)^2+1^2} = \\sqrt{3}$。\n$\\mathbf{q}_2 = \\frac{\\mathbf{u}_2}{r_{22}} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$。\n\n因此，我们得到：\n$$\n\\mathbf{Q} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{3}} \\\\ 0  \\frac{1}{\\sqrt{3}} \\end{pmatrix}, \\quad\n\\mathbf{R} = \\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  \\sqrt{3} \\end{pmatrix}\n$$\n接下来，我们计算向量 $\\mathbf{Q}^\\top\\mathbf{y}$：\n$$\n\\mathbf{Q}^\\top\\mathbf{y} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{3}}  -\\frac{1}{\\sqrt{3}}  \\frac{1}{\\sqrt{3}} \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{\\sqrt{2}} \\\\ \\frac{2}{\\sqrt{3}} + \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\frac{3}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{3} \\end{pmatrix}\n$$\n现在我们求解系统 $\\mathbf{R}\\widehat{\\boldsymbol{\\theta}} = \\mathbf{Q}^\\top\\mathbf{y}$：\n$$\n\\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  \\sqrt{3} \\end{pmatrix} \\begin{pmatrix} \\widehat{\\theta}_1 \\\\ \\widehat{\\theta}_2 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{3} \\end{pmatrix}\n$$\n这个对角系统直接给出解：\n$\\sqrt{2} \\cdot \\widehat{\\theta}_1 = \\sqrt{2} \\implies \\widehat{\\theta}_1 = 1$。\n$\\sqrt{3} \\cdot \\widehat{\\theta}_2 = \\sqrt{3} \\implies \\widehat{\\theta}_2 = 1$。\n最小二乘估计为 $\\widehat{\\boldsymbol{\\theta}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1\n\\end{pmatrix}\n}\n$$", "id": "2718842"}]}