## 引言
最小二乘（Least Squares, LS）及其递归形式（Recursive Least Squares, RLS）是控制理论、信号处理和机器学习领域中基石般的[参数估计](@entry_id:139349)算法。它们为从观测数据中提取数学模型提供了一套强大而优雅的框架，其思想贯穿了从离线系统辨识到在线[自适应控制](@entry_id:262887)的众多应用。然而，要真正精通并有效运用这些工具，必须深刻理解其背后的数学原理、统计假设、数值特性及其在复杂现实问题中的各种扩展。

本文旨在系统性地剖析最小二乘法的世界，解决从基础理论到高级应用的知识鸿沟。读者将学习到如何从一个简单的[优化问题](@entry_id:266749)出发，推导出[最小二乘解](@entry_id:152054)，并理解其几何与统计意义。我们将探讨该方法何时有效，以及在何种条件下（如回归量与噪声相关）会产生有偏估计。在此基础上，我们将揭示递归最小二乘算法如何巧妙地将批量计算转化为高效的在线更新过程，以及如何通过高级数值技术克服其固有的稳定性挑战。

为构建一个完整的知识体系，本文将分为三个核心章节。在“**原理与机制**”中，我们将奠定坚实的理论基础，深入探讨LS与RLS的推导、统计特性和[数值稳定性](@entry_id:146550)问题。接着，在“**应用与[交叉](@entry_id:147634)学科联系**”中，我们将展示这些算法如何在系统辨识、[自适应控制](@entry_id:262887)和信号处理等关键领域大放异彩，并揭示其与[贝叶斯推断](@entry_id:146958)、卡尔曼滤波等更广阔理论的深刻联系。最后，通过“**动手实践**”部分，读者将有机会通过编程练习来验证理论，巩固对算法内在联系和稳健实现的理解，从而将理论知识转化为解决实际问题的能力。

## 原理与机制

本章深入探讨最小二乘（Least Squares, LS）估计及其递归形式的核心原理与机制。我们将从批量最小二乘法的基本思想出发，建立其代数和几何解释，并探讨其统计合理性。随后，我们将分析该方法在实际应用中的局限性，例如在特定[系统辨识](@entry_id:201290)问题中的有偏性以及数值敏感性。在此基础上，我们将自然地过渡到递归最小二乘（Recursive Least Squares, RLS）算法，阐明其为在线和自适应应用提供的计算优势。最后，我们将讨论[RLS算法](@entry_id:180846)在有限精度计算环境下的实际实现挑战，并介绍确保其[数值稳定性](@entry_id:146550)的先进技术。

### [最小二乘原理](@entry_id:164326)

在众多参数估计方法中，最小二乘法因其简洁、有效和深刻的理论基础而占据核心地位。其基本目标是寻找一个参数模型，使其预测输出与实际观测数据之间的[误差平方和](@entry_id:149299)最小。

#### [优化问题](@entry_id:266749)与正规方程

考虑一个线性参数模型，其中一组测量输出 $y \in \mathbb{R}^m$ 被建模为已知回归（或特征）矩阵 $X \in \mathbb{R}^{m \times n}$ 与未知参数矢量 $\theta \in \mathbb{R}^n$ 的线性组合：

$y \approx X\theta$

最小二乘法的核心是定义一个[代价函数](@entry_id:138681) $J(\theta)$，它量化了模型预测 $X\theta$ 与实际观测 $y$ 之间的“不匹配”程度。这个代价函数通常是[残差向量](@entry_id:165091) $e = y - X\theta$ 的[欧几里得范数](@entry_id:172687)（$L_2$ 范数）的平方：

$J(\theta) = \|y - X\theta\|_2^2 = (y - X\theta)^\top(y - X\theta)$

为了找到使 $J(\theta)$ 最小化的[参数估计](@entry_id:139349) $\hat{\theta}$，我们寻求该二次函数的[驻点](@entry_id:136617)，即其梯度为零的点。通过对 $J(\theta)$ 求关于 $\theta$ 的梯度并令其为零，我们可以得到：

$\nabla_\theta J(\theta) = -2X^\top(y - X\theta) = 0$

整理后，我们得到一组线性方程，这组方程是最小二乘理论的基石，被称为**正规方程**（Normal Equations）：

$(X^\top X)\hat{\theta} = X^\top y$

任何满足此方程的 $\hat{\theta}$ 都是最小二乘问题的解。

#### [解的唯一性](@entry_id:143619)与[可辨识性](@entry_id:194150)

正规方程是一个形式为 $Az=b$ 的[线性系统](@entry_id:147850)，其中矩阵为 $A = X^\top X \in \mathbb{R}^{n \times n}$，待求向量为 $z = \hat{\theta}$。这个[方程组](@entry_id:193238)是否存在唯一解，完全取决于方阵 $X^\top X$ 的性质。这个 $n \times n$ 的矩阵通常被称为**[信息矩阵](@entry_id:750640)**或格拉姆矩阵。

一个[线性方程组](@entry_id:148943)有唯一解的充分必要条件是其系数矩阵可逆（非奇异）。因此，[最小二乘估计](@entry_id:262764) $\hat{\theta}$ 的唯一性取决于 $X^\top X$ 是否可逆。我们可以证明，$X^\top X$ 的[零空间](@entry_id:171336)与 $X$ 的零空间相同，即 $\mathcal{N}(X^\top X) = \mathcal{N}(X)$ [@problem_id:2718860]。因此，$X^\top X$ 可逆等价于 $\mathcal{N}(X) = \{0\}$，这意味着矩阵 $X$ 的列是[线性无关](@entry_id:148207)的。

当 $X$ 的列线性无关时，我们称其为**[满列秩](@entry_id:749628)**，即 $\mathrm{rank}(X) = n$。这一条件在[系统辨识](@entry_id:201290)领域中具有深刻的物理意义，它构成了**[参数可辨识性](@entry_id:197485)**（Identifiability）的基础 [@problem_id:2718876]。[参数可辨识性](@entry_id:197485)意味着，对于任意两个不同的参数向量 $\theta_1 \neq \theta_2$，它们产生的无噪声输出序列 $X\theta_1$ 和 $X\theta_2$ 也必然不同。换言之，数据矩阵 $X$ 包含足够丰富的信息，使得我们可以从输出中唯一地反推参数。如果 $X^\top X$ 的秩小于 $n$（即矩阵是奇异的），则存在非零的 $\Delta\theta$ 使得 $X\Delta\theta=0$，这意味着 $\theta_1$ 和 $\theta_2 = \theta_1 + \Delta\theta$ 这两个不同的参数会产生完全相同的无噪声输出，使得它们在观测上无法区分 [@problem_id:2718876]。

当满足[可辨识性](@entry_id:194150)条件时，唯一的[最小二乘解](@entry_id:152054)为：

$\hat{\theta} = (X^\top X)^{-1}X^\top y$

#### 几何解释与[秩亏](@entry_id:754065)情况

[最小二乘法](@entry_id:137100)具有优美的几何解释。向量 $X\theta$ 是矩阵 $X$ 各[列的线性组合](@entry_id:150240)，因此它总是位于 $X$ 的[列空间](@entry_id:156444) $\mathcal{R}(X)$ 中。最小二乘问题 $\|y - X\theta\|_2^2$ 的最小化，等价于在[子空间](@entry_id:150286) $\mathcal{R}(X)$ 中寻找一个向量 $\hat{y} = X\hat{\theta}$，使其与观测向量 $y$ 的距离最近。

根据射影定理，这个最近的向量 $\hat{y}$ 正是 $y$ 在 $\mathcal{R}(X)$ 上的**正交投影**。这意味着[残差向量](@entry_id:165091) $y - \hat{y}$ 必须与 $\mathcal{R}(X)$ 中的任何向量正交。特别地，它必须与 $X$ 的所有列向量正交，这可以表示为 $X^\top(y - X\hat{\theta}) = 0$，这正是我们之[前推](@entry_id:158718)导出的[正规方程](@entry_id:142238)。

当 $X$ 不是[满列秩](@entry_id:749628)时，即 $\mathrm{rank}(X)  n$，信息矩阵 $X^\top X$ 是奇异的，正规方程有无穷多组解。这些解构成一个仿射[子空间](@entry_id:150286)。在这种情况下，虽然我们无法唯一确定 $\theta$，但所有解都能使[残差平方和](@entry_id:174395)达到相同的最小值。此时，一个自然的选择是寻找那个在所有解中自身范数最小的解，即**[最小范数解](@entry_id:751996)**。

可以证明，这个唯一的[最小范数解](@entry_id:751996)可以通过**[Moore-Penrose伪逆](@entry_id:147255)**（Moore-Penrose Pseudoinverse）$X^+$ 得到 [@problem_id:2718860]：

$\hat{\theta}_{\text{min-norm}} = X^+ y$

这个解是所有[最小二乘解](@entry_id:152054)中唯一位于 $X$ [行空间](@entry_id:148831) $\mathcal{R}(X^\top)$ 中的那个。[伪逆](@entry_id:140762)为处理非唯一解和病态问题提供了一个统一而强大的框架。例如，对于一个具体的[秩亏](@entry_id:754065)问题，如回归矩阵为 $X = \begin{pmatrix} 1  0  1 \\ 0  1  0 \end{pmatrix}$，我们可以首先求出通解集，然后通过最小化解[向量的范数](@entry_id:154882)来确定唯一的[最小范数解](@entry_id:751996)为 $\begin{pmatrix} 1/2  2  1/2 \end{pmatrix}^\top$ [@problem_id:2718860]。

### 统计特性与性能分析

虽然最小二乘法最初是作为一个纯粹的代数或[几何优化](@entry_id:151817)问题提出的，但它的真正威力在于其深刻的统计学背景。通过对数据生成过程做出合理的概率假设，我们可以为[最小二乘估计](@entry_id:262764)的合理性提供强有力的支持，并对其性能进行量化分析。

#### 与[最大似然估计](@entry_id:142509)的联系

假设测量值 $y_k$ 是由真实参数 $\theta_\star$ 通过[线性模型](@entry_id:178302) $\phi_k^\top \theta_\star$ 生成，并叠加上一个随机噪声项 $v_k$：

$y_k = \phi_k^\top \theta_\star + v_k$

一个非常普遍且关键的假设是，噪声序列 $\{v_k\}$ 是**[独立同分布](@entry_id:169067) (i.i.d.)** 的，且服从均值为零、[方差](@entry_id:200758)为 $\sigma^2$ 的**[高斯分布](@entry_id:154414)**（即[正态分布](@entry_id:154414)）。在这种情况下，观测向量 $y$ 的[概率密度函数](@entry_id:140610)（或似然函数）为：

$L(\theta | y) \propto \exp\left(-\frac{1}{2\sigma^2} \|y - \Phi \theta\|_2^2\right)$

**[最大似然估计](@entry_id:142509) (Maximum Likelihood Estimation, MLE)** 的目标是寻找能使观测数据出现的概率（即[似然函数](@entry_id:141927)）最大化的参数 $\theta$。为了最大化 $L(\theta | y)$，我们等价地需要最小化指数项中的 $\|y - \Phi \theta\|_2^2$。这正是最小二乘的[代价函数](@entry_id:138681)。

因此，在[高斯白噪声](@entry_id:749762)的假设下，**[最小二乘估计](@entry_id:262764)等价于最大似然估计** [@problem_id:2718817]。这一结论无论噪声[方差](@entry_id:200758) $\sigma^2$ 已知与否都成立，并为最小二乘法提供了强有力的统计学依据。值得注意的是，如果噪声服从其他[分布](@entry_id:182848)（例如[拉普拉斯分布](@entry_id:266437)），或者噪声是有色的（即各分量不独立或[方差](@entry_id:200758)不同），则标准的[最小二乘估计](@entry_id:262764)将不再是最大似然估计 [@problem_id:2718817]。

#### [渐近性质](@entry_id:177569)：有偏性与协[方差](@entry_id:200758)

在实践中，我们关心估计器在数据量趋于无穷大时的行为，即其**[渐近性质](@entry_id:177569)**。一个理想的估计器应该是**无偏**（unbiased）且**一致**（consistent）的，即随着数据增多，估计值会收敛到真实值。

标准[最小二乘估计](@entry_id:262764)的无偏性和一致性依赖于一个关键条件：回归量与噪声项不相关。然而，在许多实际问题中，尤其是在动态系统的辨识中，这个条件很容易被违反。一个典型的例子是带有[测量噪声](@entry_id:275238)的**自回归外源输入 (ARX) 模型**辨识 [@problem_id:2718808]。

考虑一个简单的ARX系统 $y_t = a_0 y_{t-1} + b_0 u_{t-1} + w_t$，其中 $w_t$ 是[过程噪声](@entry_id:270644)。如果我们只能测量到带噪的输出 $y_t^m = y_t + v_t$，并使用 $y_{t-1}^m$ 作为回归量来估计参数 $a_0$，就会出现问题。回归量 $\phi_t = [y_{t-1}^m, u_{t-1}]^\top$ 中的 $y_{t-1}^m = y_{t-1} + v_{t-1}$ 项与模型真实残差 $\eta_t = w_t + v_t - a_0 v_{t-1}$ 通过共同的噪声项 $v_{t-1}$ 变得相关。这种相关性破坏了[最小二乘估计](@entry_id:262764)一致性的基本前提，导致**渐近有偏**。具体来说，参数 $a$ 的估计会系统性地偏离[真值](@entry_id:636547) $a_0$，其偏差大小与测量噪声[方差](@entry_id:200758) $\sigma_v^2$ 直接相关 [@problem_id:2718808]。这类问题属于更广泛的**变量含误差（errors-in-variables）**模型范畴。

在满足一致性条件的情况下（例如，回归量与噪声无关），我们可以进一步分析估计的精度。假设回归量序列 $\{\varphi_k\}$ 是平稳遍历的，且满足**[持续激励](@entry_id:263834) (persistently exciting)** 条件，即其[协方差矩阵](@entry_id:139155) $R_\varphi = \mathbb{E}[\varphi_k \varphi_k^\top]$ 是正定的。同时，噪声 $\{w_k\}$ 是零均值白噪声，[方差](@entry_id:200758)为 $\sigma_w^2$。在这种情况下，可以证明[最小二乘估计](@entry_id:262764) $\hat{\theta}_N$ 的渐近协方差矩阵为 [@problem_id:2718804]：

$\lim_{N\to\infty} N \cdot \mathrm{Cov}(\hat{\theta}_N) = \sigma_w^2 R_\varphi^{-1}$

这个重要的公式揭示了估计精度的几个关键因素：
1.  **数据量 $N$**：估计误差的[方差](@entry_id:200758)以 $1/N$ 的速率减小，即数据越多，估计越准。
2.  **噪声水平 $\sigma_w^2$**：估计误差与噪声[方差](@entry_id:200758)成正比。噪声越大，估计越不准。
3.  **[数据质量](@entry_id:185007) $R_\varphi^{-1}$**：回归量协方差矩阵 $R_\varphi$ 的“大小”决定了估计精度。一个“大”的 $R_\varphi$（意味着回归量在各个方向上都有丰富的变化）对应一个“小”的 $R_\varphi^{-1}$，从而得到更精确的估计。这就是[持续激励](@entry_id:263834)条件的直观意义。

#### [模型选择](@entry_id:155601)与增量信息

在构建模型时，我们常常面临一个问题：增加更多的回归量是否能显著改善模型性能？[最小二乘法](@entry_id:137100)的几何框架为回答这个问题提供了有力的工具 [@problem_id:2718795]。

假设我们有一个基准模型 $y = X_1 \theta_1 + v$，并考虑是否要加入一组新的回归量 $X_2$ 形成扩展模型 $y = X_1 \theta_1 + X_2 \theta_2 + v$。加入 $X_2$ 带来的模型性能提升，可以通过比较两个模型的[残差平方和](@entry_id:174395) (RSS) 来衡量。这种RSS的减小量可以精确地表示为：

$\Delta = \mathrm{RSS}_1 - \mathrm{RSS}_{1,2} = y^\top (P_{[X_1, X_2]} - P_{X_1}) y$

这里，$P_X = X(X^\top X)^{-1}X^\top$ 是向 $X$ 列空间投影的正交投影算子。矩阵 $P_{X_2 | X_1} = P_{[X_1, X_2]} - P_{X_1}$ 本身也是一个[投影算子](@entry_id:154142)，它将[向量投影](@entry_id:147046)到 $X_2$ 的[列空间](@entry_id:156444)中正交于 $X_1$ [列空间](@entry_id:156444)的分量上。因此，RSS的减小量完全取决于 $y$ 在这个“新”空间上的投影大小。如果 $X_2$ 的列向量完全可以由 $X_1$ 的列向量[线性表示](@entry_id:139970)（即 $\mathcal{R}(X_2) \subseteq \mathcal{R}(X_1)$），那么 $P_{X_2 | X_1} = 0$，增加 $X_2$ 不会带来任何[信息增益](@entry_id:262008) [@problem_id:2718795]。

在统计上，我们可以构造一个 **F-检验** 来判断这种RSS的减小是否“显著”。在噪声为[高斯白噪声](@entry_id:749762)的[零假设](@entry_id:265441)（$H_0: \theta_2=0$）下，如下定义的[F统计量](@entry_id:148252)服从[F分布](@entry_id:261265)：

$F = \frac{\Delta / p_2}{\mathrm{RSS}_{1,2} / (n - p_1 - p_2)} \sim F_{p_2, n-p_1-p_2}$

其中 $p_1, p_2$ 分别是 $X_1, X_2$ 的列数。通过比较计算出的[F值](@entry_id:178445)与[F分布](@entry_id:261265)的临界值，我们可以做出是否接纳新回归量 $X_2$ 的[统计决策](@entry_id:170796)。

### [数值稳定性](@entry_id:146550)与敏感性

除了统计特性，最小二乘问题在数值计算上的表现也至关重要。一个理论上完美的解，在实际的有限精度计算机上可能因为[数值误差](@entry_id:635587)而变得毫无用处。

#### 条件数与噪声放大

[最小二乘解](@entry_id:152054)的质量对数据中的扰动（如测量噪声）有多敏感？这个问题的答案与回归矩阵 $X$ 的**条件数** $\kappa(X)$ 密切相关。通过**[奇异值分解](@entry_id:138057) (Singular Value Decomposition, SVD)**，$X = U \Sigma V^\top$，我们可以深入理解噪声的传播机制。

设真实模型为 $y = X \theta^\star + e$，其中 $e$ 是噪声。[最小二乘估计](@entry_id:262764)的误差为 $\hat{\theta} - \theta^\star = X^+ e$。利用SVD，这个关系可以表示为 [@problem_id:2718864]：

$\hat{\theta} - \theta^\star = V \Sigma^+ U^\top e$

这表明，噪声向量 $e$ 沿第 $i$ 个[左奇异向量](@entry_id:751233) $u_i$ 的分量，会被放大 $1/\sigma_i$ 倍，然后体现在参数误差沿第 $i$ 个[右奇异向量](@entry_id:754365) $v_i$ 的方向上。其中 $\sigma_i$ 是 $X$ 的第 $i$ 个奇异值。如果某个[奇异值](@entry_id:152907) $\sigma_n$ 非常小，那么对应的[放大因子](@entry_id:144315) $1/\sigma_n$ 就会非常大。这意味着，与小奇异值方向对齐的噪声分量会被极大地放大，从而严重污染参数估计。

这种敏感性可以通过一个经典的不等式来量化，它给出了相对参数误差的一个[上界](@entry_id:274738) [@problem_id:2718864]：

$\frac{\|\hat{\theta} - \theta^{\star}\|_2}{\|\theta^{\star}\|_2} \le \kappa(X) \frac{\|e\|_2}{\|X \theta^{\star}\|_2}$

其中 $\kappa(X) = \sigma_{\max}/\sigma_{\min}$ 是 $X$ 的谱[条件数](@entry_id:145150)。这个不等式清晰地表明，一个**病态**（ill-conditioned）的回归矩阵（即条件数非常大）会使[最小二乘解](@entry_id:152054)对噪声极其敏感。例如，对于一个[条件数](@entry_id:145150)为200的矩阵，即使噪声范数仅为信号范数的很小一部分，相对参数误差也可能被放大到5倍以上 [@problem_id:2718864]。

### 递归最小二乘算法 (RLS)

在许多控制和信号处理应用中，数据是依次到达的，我们需要实时更新[参数估计](@entry_id:139349)。这种在线场景对算法的[计算效率](@entry_id:270255)提出了很高的要求。

#### RLS的动机与计算效率

一种朴素的方法是，每当一个新数据点 $(\varphi_k, y_k)$ 到达时，就重新构建整个数据矩阵 $X_k$ 和输出向量 $y_{1:k}$，然后从头求解批量最小二乘问题。然而，这种方法的计算成本是高昂的。在第 $k$ 步，形成 $X_k^\top X_k$ 需要 $\mathcal{O}(kn^2)$ 次运算，求解 $n \times n$ 的[正规方程](@entry_id:142238)需要 $\mathcal{O}(n^3)$ 次运算。总计算量随数据量 $k$ 线性增长，这对于实时应用是不可接受的。此外，存储所有历史数据也需要 $\mathcal{O}(kn)$ 的内存 [@problem_id:2718833]。

**递归最小二乘 (RLS)** 算法正是为了解决这个问题而生。它通过一种巧妙的递推方式，仅利用前一步的估计结果和当前的新数据，来更新[参数估计](@entry_id:139349)。[RLS算法](@entry_id:180846)的关键优势在于其**固定的单步计算复杂度**。无论已经处理了多少数据，每一步更新的计算量都保持在 $\mathcal{O}(n^2)$，内存需求也固定为 $\mathcal{O}(n^2)$（用于存储一个 $n \times n$ 的协方差矩阵和一个 $n \times 1$ 的参数向量）。这种效率使其成为在线自适应系统的理想选择 [@problem_id:2718833]。

#### [遗忘因子](@entry_id:175644)与参数跟踪

标准最小二乘法对所有历史数据一视同仁，这使得它难以跟踪随时间变化的参数。为了让估计器能够“忘记”旧数据，更关注近期数据，RLS引入了**指数加权**代价函数：

$J_k(\theta) = \sum_{i=1}^{k} \lambda^{k-i} (y_i - \phi_i^\top \theta)^2$

其中 $\lambda \in (0, 1)$ 是**[遗忘因子](@entry_id:175644)**。当 $\lambda  1$ 时，时间上更久远的数据点 $(i \ll k)$ 会被乘以一个很小的权重 $\lambda^{k-i}$，从而其对当前估计的影响会指数衰减。

这种加权方案导致了[信息矩阵](@entry_id:750640) $R_k = \sum_{i=1}^k \lambda^{k-i} \phi_i \phi_i^\top$ 的一个简单递归更新关系 [@problem_id:2718840]：

$R_k = \lambda R_{k-1} + \phi_k \phi_k^\top$

[遗忘因子](@entry_id:175644)的影响可以用一个**有效记忆长度** $N_{\text{eff}}$ 来直观理解。它被定义为等效的[矩形窗](@entry_id:262826)长度，其权重之和与指数窗的权重总和相等。对于一个无限长的指数窗，其权重和为[几何级数](@entry_id:158490)求和 $\sum_{j=0}^{\infty} \lambda^j$，其结果为 [@problem_id:2718840]：

$N_{\text{eff}} = \frac{1}{1-\lambda}$

例如，$\lambda=0.99$ 对应约100个样本的记忆长度，而 $\lambda=0.9$ 则对应约10个样本。选择合适的 $\lambda$ 是在跟踪速度（小 $\lambda$）和估计精度（大 $\lambda$）之间进行权衡。

完整的[RLS算法](@entry_id:180846)通过应用[矩阵求逆](@entry_id:636005)引理（Sherman-Morrison-Woodbury公式），可以推导出一套完全递归的[更新方程](@entry_id:264802)，用于更新参数估计 $\hat{\theta}_k$ 和逆信息矩阵 $P_k = R_k^{-1}$（通常被称为[协方差矩阵](@entry_id:139155)）。

### RLS的实现与[数值鲁棒性](@entry_id:188030)

尽管[RLS算法](@entry_id:180846)在理论上十分优美，但其在有限精度计算机上的直接实现却面临着严重的**数值稳定性**问题。

#### 数值退化问题

常规的RLS协[方差](@entry_id:200758)更新公式为：

$P_k = \frac{1}{\lambda} \left[ P_{k-1} - \frac{P_{k-1} \phi_k \phi_k^\top P_{k-1}}{\lambda + \phi_k^\top P_{k-1} \phi_k} \right]$

这个公式在数学上是精确的，但在数值计算上却很脆弱。问题出在括号内的减法运算。当新数据的[信息量](@entry_id:272315)较少时，被减去的项会非常小，导致两个几乎相等的矩阵相减。这种操作会引发**灾难性抵消**（catastrophic cancellation），使得计算结果的有效数字大量丢失 [@problem_id:2718866]。

其后果是，理论上应保持对称和正定（或至少半正定）的协方差矩阵 $P_k$ 会因为累积的舍入误差而逐渐失去这些性质。它可能变得不再对称，甚至出现负的[特征值](@entry_id:154894)。一旦 $P_k$ 失去正定性，估计器就会变得不稳定，参数估计可能会发散，导致整个自适应系统崩溃 [@problem_id:2718866]。简单地将计算出的 $P_k$ 对称化，即用 $\frac{1}{2}(P_k + P_k^\top)$ 替换它，虽然可以恢复对称性，但无法消除已经出现的负[特征值](@entry_id:154894)，因此不能解决问题 [@problem_id:2718866]。

#### 鲁棒的实现方法

为了克服常规RLS的数值缺陷，研究人员开发了多种数值上更为鲁棒的算法。

一种改进是**Joseph型更新**。它将协[方差](@entry_id:200758)更新重写为一个只包含正定项相加的形式，从而避免了灾难性的减法运算。Joseph型更新在数值上远比传统形式稳定，能更好地保持 $P_k$ 的[正定性](@entry_id:149643) [@problem_id:2718866]。

更根本的解决方案是**平方根RLS (Square-Root RLS, SR-RLS)** 算法。这类算法的核心思想是，不再直接递推协方差矩阵 $P_k$ 本身，而是递推它的一个“平方根”因子 $S_k$，例如通过[乔列斯基分解](@entry_id:166031)（Cholesky decomposition）得到的下三角矩阵，使得 $P_k = S_k S_k^\top$。

平方根因子的更新是通过一系列**正交变换**（如[Givens旋转](@entry_id:167475)或[Householder反射](@entry_id:637383)）来实现的。这些正交变换以其卓越的[数值稳定性](@entry_id:146550)而著称。通过这种方式，计算出的[协方差矩阵](@entry_id:139155) $P_k = S_k S_k^\top$ 在结构上保证了对称性和[半正定性](@entry_id:147720)。此外，通过对平方根因子进行操作，算法的有效条件数大约是原协方差矩阵条件数的平方根，这极大地改善了算法在病态问题中的数值表现 [@problem_id:2718866]。因此，SR-RLS及其变种被认为是实现高性能、高鲁棒性[自适应滤波](@entry_id:185698)器的标准方法。