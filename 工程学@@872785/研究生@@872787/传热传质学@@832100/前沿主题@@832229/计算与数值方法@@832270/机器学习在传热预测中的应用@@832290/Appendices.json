{"hands_on_practices": [{"introduction": "机器学习模型训练的第一步是获取数据。本实践从一个经典的一维稳态热传导问题入手，展示了如何从其精确的解析解出发，生成用于监督学习的高保真数据集[@problem_id:2503006]。这个练习不仅连接了传统的物理分析方法与数据驱动的范式，也为构建更复杂的代理模型奠定了基础。", "problem": "您的任务是将一个具有均匀内部热生成的一维稳态热传导模型转换为用于温度预测的合成监督学习数据集。从热传导的第一性原理出发，推导其控制方程和闭式解，然后实施一个数值格式来验证解的正确性，并为监督学习生成标签。您必须实现一个单一的程序来执行以下规范，并生成一个单行的浮点数结果列表作为最终输出。\n\n基本原理和建模假设：\n- 考虑一个占据区间 $0 \\le x \\le L$ 的均匀平板，其热导率 $k$ 为常数，均匀体积热生成率为 $q$。假设热传递是一维、稳态、纯导热的，除了规定的边界外没有热量损失。从能量守恒和傅里叶热传导定律出发，得到带均匀生成项的标准一维稳态热传导方程。推导必须从能量平衡和本构关系开始，而不是从预先记下的目标公式开始。施加狄利克雷边界条件 $T(0)=T_0$ 和 $T(L)=T_L$。\n- 控制方程是一个通过稳态能量平衡得到的二阶线性常微分方程。您的推导应说明为何常系数算子和均匀源导致二次温度分布，以及边界数据如何确定积分常数。\n\n需要在程序中编码的实现任务：\n1) 解析标签生成：\n   - 根据上述假设和边界条件，通过对控制方程积分并应用边界条件，推导出闭式解 $T(x)$。使用此 $T(x)$ 作为监督学习的真实值标签生成器。程序必须明确使用这个推导出的 $T(x)$。\n2) 数值有限差分验证：\n   - 在一个包含边界在内，具有 $N_{\\text{fd}}=101$ 个节点的均匀网格上，实现控制方程的二阶中心差分有限差分离散。在 $x=0$ 和 $x=L$ 处强施加狄利克雷边界条件。为内部温度组装并求解所得的三对角线性系统。在同一网格上，将数值解与解析解进行比较，并计算最大绝对误差 $e_{\\text{fd}}$。以开尔文为单位报告 $e_{\\text{fd}}$。\n3) 监督学习数据集构建和代理模型训练：\n   - 对于每种情况，在 $[0,L]$ 区间内生成一个包含 $N_{\\text{train}}$ 个等距点的训练集，其输入（特征）由向量 $[1,\\, \\xi,\\, \\xi^2]$ 给出，其中 $\\xi = x/L$，标签由解析温度 $T(x)$ 给出。通过普通最小二乘法 (OLS) 训练一个多项式回归模型，以实现从 $[1,\\, \\xi,\\, \\xi^2]$ 到 $T$ 的映射。然后，使用相同的特征，在一个包含 $N_{\\text{eval}}$ 个等距点的独立评估集上评估训练好的模型，并计算模型预测与解析解之间的均方根误差 (RMSE) $e_{\\text{ml}}$。以开尔文为单位报告 $e_{\\text{ml}}$。\n4) 测试套件和输出：\n   - 使用以下测试套件，其中所有参数均采用国际单位制 (SI)：\n     - 情况 1：$(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,10.0,\\,1.0\\times 10^{5},\\,0.1,\\,300.0,\\,350.0,\\,15,\\,51\\,)$。\n     - 情况 2：$(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,200.0,\\,0.0,\\,0.2,\\,400.0,\\,300.0,\\,7,\\,41\\,)$。\n     - 情况 3：$(k,q,L,T_0,T_L,N_{\\text{train}},N_{\\text{eval}}) = (\\,15.0,\\,5.0\\times 10^{4},\\,0.05,\\,350.0,\\,350.0,\\,11,\\,61\\,)$。\n   - 对于每种情况，计算并收集 $e_{\\text{fd}}$ 和 $e_{\\text{ml}}$ 作为浮点数。所有误差度量必须以开尔文 $(\\mathrm{K})$ 为单位报告，并应以双精度计算。\n   - 最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\,e_{\\text{fd}}^{(1)},\\,e_{\\text{ml}}^{(1)},\\,e_{\\text{fd}}^{(2)},\\,e_{\\text{ml}}^{(2)},\\,e_{\\text{fd}}^{(3)},\\,e_{\\text{ml}}^{(3)}\\,]$，不含空格。\n\n科学真实性要求：\n- 所有物理和数值计算必须与所描述的具有均匀热生成的一维稳态热传导相一致。使用 $k>0$，$L>0$，以及实数值 $q$、$T_0$、$T_L$。数据集标签必须来自物理模型所蕴含的解析解 $T(x)$。监督学习任务的合理性应在于，对于每种情况，从 $[1,\\, \\xi,\\, \\xi^2]$ 到 $T$ 的目标映射是一个固定的二次函数，因此适合使用普通最小二乘法 (OLS) 进行训练。\n\n角度单位不适用。将所有报告的误差度量以开尔文 $(\\mathrm{K})$ 为单位表示为浮点值。最终输出必须严格匹配上面指定的单行格式。", "solution": "该问题陈述是有效的。它在科学上以热传递原理为基础，在数学上是适定的，并为所有要求的任务提供了完整而明确的规范。将首先通过推导控制物理及其解析解来解决该问题，然后实施指定的数值和机器学习程序。\n\n**第 1 部分：控制方程和解析解的推导**\n\n分析从具有内部热生成的一维稳态系统的能量守恒第一性原理开始。考虑平板内一个长度为 $dx$、横截面积为常数 $A$ 的无穷小控制体。能量平衡方程为：\n$$ \\dot{E}_{\\text{in}} - \\dot{E}_{\\text{out}} + \\dot{E}_{\\text{gen}} = 0 $$\n其中 $\\dot{E}_{\\text{in}}$ 是在位置 $x$ 传入控制体的热传导速率，$\\dot{E}_{\\text{out}}$ 是在位置 $x+dx$ 传出控制体的热传导速率，$\\dot{E}_{\\text{gen}}$ 是在控制体内的热生成速率。这些项表示为：\n$$ \\dot{E}_{\\text{in}} = q_x(x) A $$\n$$ \\dot{E}_{\\text{out}} = q_x(x+dx) A $$\n$$ \\dot{E}_{\\text{gen}} = q A dx $$\n这里，$q_x$ 是 $x$ 方向的热通量（单位面积的热量速率），$q$ 是均匀的体积热生成率。将这些代入能量平衡方程得到：\n$$ q_x(x) A - q_x(x+dx) A + q A dx = 0 $$\n两边除以体积 $A dx$ 并重新整理得：\n$$ -\\frac{q_x(x+dx) - q_x(x)}{dx} + q = 0 $$\n取 $dx \\to 0$ 的极限，得到能量守恒方程的微分形式：\n$$ -\\frac{d q_x}{dx} + q = 0 $$\n热传导的本构关系是傅里叶定律，它指出热通量与负温度梯度成正比：\n$$ q_x = -k \\frac{dT}{dx} $$\n其中 $k$ 是材料的热导率，假定为常数。将傅里叶定律代入能量方程，得到温度 $T(x)$ 的控制常微分方程：\n$$ -\\frac{d}{dx}\\left(-k \\frac{dT}{dx}\\right) + q = 0 $$\n$$ k \\frac{d^2 T}{dx^2} + q = 0 $$\n$$ \\frac{d^2 T}{dx^2} = -\\frac{q}{k} $$\n这是一个二阶线性常微分方程。为了找到温度分布 $T(x)$，我们对 $x$ 进行两次积分：\n$$ \\frac{dT}{dx} = -\\frac{q}{k} x + C_1 $$\n$$ T(x) = -\\frac{q}{2k} x^2 + C_1 x + C_2 $$\n积分常数 $C_1$ 和 $C_2$ 由指定的狄利克雷边界条件确定：$T(0) = T_0$ 和 $T(L) = T_L$。\n在 $x=0$ 处应用第一个边界条件：\n$$ T(0) = T_0 = -\\frac{q}{2k}(0)^2 + C_1(0) + C_2 \\implies C_2 = T_0 $$\n在 $x=L$ 处应用第二个边界条件：\n$$ T(L) = T_L = -\\frac{q}{2k} L^2 + C_1 L + T_0 $$\n求解 $C_1$：\n$$ C_1 L = T_L - T_0 + \\frac{q L^2}{2k} $$\n$$ C_1 = \\frac{T_L - T_0}{L} + \\frac{qL}{2k} $$\n将 $C_1$ 和 $C_2$ 的表达式代回通解，得到温度分布的最终解析表达式：\n$$ T(x) = -\\frac{q}{2k} x^2 + \\left(\\frac{T_L - T_0}{L} + \\frac{qL}{2k}\\right)x + T_0 $$\n此方程提供了平板中任意位置 $x$ 处的真实温度，并将用于为监督学习任务生成标签。\n\n**第 2 部分：数值有限差分验证**\n\n使用有限差分法 (FDM) 对控制方程进行数值求解并验证解析解。将域 $[0, L]$ 离散化为一个包含 $N_{\\text{fd}} = 101$ 个节点的均匀网格，$x_i = i \\cdot \\Delta x$，$i \\in \\{0, 1, \\dots, 100\\}$，网格间距为 $\\Delta x = L / (N_{\\text{fd}} - 1)$。在每个内部节点 $x_i$（$i \\in \\{1, \\dots, 99\\}$）处，使用二阶精度的中心差分格式来近似二阶导数 $\\frac{d^2 T}{dx^2}$：\n$$ \\frac{d^2 T}{dx^2}\\bigg|_{x_i} \\approx \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} $$\n其中 $T_i \\approx T(x_i)$。将此代入控制方程，得到一个线性代数方程组：\n$$ \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} = -\\frac{q}{k} $$\n$$ T_{i-1} - 2T_i + T_{i+1} = -\\frac{q(\\Delta x)^2}{k} $$\n这为未知的内部温度 $\\mathbf{T}_{\\text{int}} = [T_1, T_2, \\dots, T_{99}]^T$ 构成了一个包含 $N_{\\text{fd}}-2=99$ 个方程的三对角系统。边界温度 $T_0$ 和 $T_{N_{\\text{fd}}-1}=T_L$ 是已知的。求解该系统，并计算最大绝对误差 $e_{\\text{fd}} = \\max_i |T_i^{\\text{numerical}} - T(x_i)^{\\text{analytic}}|$ 来评估 FDM 解的准确性。\n\n**第 3 部分：监督学习代理模型**\n\n使用监督学习构建一个用于温度预测的代理模型。解析解表明 $T(x)$ 是 $x$ 的二次函数。这意味着它也是无量纲坐标 $\\xi = x/L$ 的二次函数。因此，机器学习模型选择为二次多项式回归模型：\n$$ \\hat{T}(\\xi) = w_0 \\cdot 1 + w_1 \\cdot \\xi + w_2 \\cdot \\xi^2 $$\n特征是向量 $[1, \\xi, \\xi^2]$ 的分量。生成一个在区间 $[0, L]$ 上有 $N_{\\text{train}}$ 个等距点 $x_j$ 的训练数据集。对于每个点，计算特征向量 $\\mathbf{x}_{\\text{train}, j} = [1, x_j/L, (x_j/L)^2]$，并使用解析解生成相应的标签 $y_{\\text{train}, j} = T(x_j)$。模型权重 $\\mathbf{w} = [w_0, w_1, w_2]^T$ 通过解决普通最小二乘法 (OLS) 问题来确定，该问题旨在最小化预测标签与实际标签之间的残差平方和。解由下式给出：\n$$ \\mathbf{w} = (X_{\\text{train}}^T X_{\\text{train}})^{-1} X_{\\text{train}}^T \\mathbf{y}_{\\text{train}} $$\n其中 $X_{\\text{train}}$ 是特征向量矩阵，$\\mathbf{y}_{\\text{train}}$ 是标签向量。由于模型族与真实函数完美匹配，预计 OLS 过程能够以接近浮点精度的误差学习该函数。然后，在另一个包含 $N_{\\text{eval}}$ 个点的独立集合上评估训练好的模型。通过该评估集上模型预测与解析温度之间的均方根误差 ($e_{\\text{ml}}$) 来衡量预测性能。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the heat conduction problem for three test cases, computing\n    finite difference error (e_fd) and machine learning error (e_ml) for each.\n    \"\"\"\n\n    # Test suite as per the problem statement.\n    # (k, q, L, T0, TL, N_train, N_eval)\n    test_cases = [\n        (10.0, 1.0e5, 0.1, 300.0, 350.0, 15, 51),\n        (200.0, 0.0, 0.2, 400.0, 300.0, 7, 41),\n        (15.0, 5.0e4, 0.05, 350.0, 350.0, 11, 61),\n    ]\n\n    results = []\n\n    def get_analytic_T(x, k, q, L, T0, TL):\n        \"\"\"\n        Computes the analytic solution for the temperature T at position(s) x.\n        T(x) = -q/(2k) * x^2 + ( (T_L-T_0)/L + qL/(2k) ) * x + T_0\n        \"\"\"\n        C1 = (TL - T0) / L + q * L / (2 * k)\n        C2 = T0\n        return -q / (2 * k) * x**2 + C1 * x + C2\n\n    for case in test_cases:\n        k, q, L, T0, TL, N_train, N_eval = case\n\n        # --- 2) Numerical Finite-Difference Verification ---\n        N_fd = 101\n        N_int = N_fd - 2  # Number of interior nodes\n        x_fd = np.linspace(0, L, N_fd)\n        dx = L / (N_fd - 1)\n\n        # Assemble the tridiagonal matrix A for the interior nodes\n        A = np.zeros((N_int, N_int))\n        main_diag = -2 * np.ones(N_int)\n        off_diag = np.ones(N_int - 1)\n        np.fill_diagonal(A, main_diag)\n        np.fill_diagonal(A[1:], off_diag)\n        np.fill_diagonal(A[:, 1:], off_diag)\n\n        # Assemble the right-hand side vector b\n        b = np.full(N_int, -q * dx**2 / k)\n        b[0] -= T0\n        b[-1] -= TL\n\n        # Solve the linear system A * T_int = b\n        T_int = linalg.solve(A, b)\n\n        # Combine with boundary conditions to get the full numerical solution\n        T_numerical = np.concatenate(([T0], T_int, [TL]))\n\n        # Compute analytic solution on the same grid for comparison\n        T_analytic_fd = get_analytic_T(x_fd, k, q, L, T0, TL)\n\n        # Compute the maximum absolute error e_fd\n        e_fd = np.max(np.abs(T_numerical - T_analytic_fd))\n        results.append(e_fd)\n\n        # --- 3) Supervised Learning Dataset Construction and Surrogate Training ---\n        # Generate training data\n        x_train = np.linspace(0, L, N_train)\n        xi_train = x_train / L\n        # Feature matrix X_train: [1, xi, xi^2]\n        X_train = np.vstack([np.ones_like(xi_train), xi_train, xi_train**2]).T\n        y_train = get_analytic_T(x_train, k, q, L, T0, TL)\n\n        # Train polynomial regression model using Ordinary Least Squares (OLS)\n        # np.linalg.lstsq solves the equation Xw = y for w\n        w, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n        # Evaluate the model\n        x_eval = np.linspace(0, L, N_eval)\n        xi_eval = x_eval / L\n        # Feature matrix for evaluation\n        X_eval = np.vstack([np.ones_like(xi_eval), xi_eval, xi_eval**2]).T\n        \n        # Get model predictions\n        y_pred = X_eval @ w\n        \n        # Get true labels for the evaluation set\n        y_true = get_analytic_T(x_eval, k, q, L, T0, TL)\n\n        # Compute the Root-Mean-Squared Error (RMSE) e_ml\n        e_ml = np.sqrt(np.mean((y_pred - y_true)**2))\n        results.append(e_ml)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "2503006"}, {"introduction": "一个精确的预测模型不仅要拟合数据，更要遵守基本的物理定律。本实践探讨了如何将热力学一致性等物理约束嵌入到机器学习模型中，确保其预测结果的物理合理性[@problem_id:2502951]。通过为比热容 $c_{p}(T)$ 设计一个保证其非负的参数化形式，并利用已知的物理参考量来正则化损失函数，我们学习了构建物理上可靠的数据驱动模型的核心技术。", "problem": "正在开发一个数据驱动的传热模型，旨在使用机器学习（ML）来预测恒压下单相流体随温度变化的焓。该模型的目标是建立从温度到焓的映射关系，它在焓的测量数据上进行训练，并利用热力学一致性来正则化拟合过程。\n\n从以下定义出发：在恒定压力下，焓的微分满足 $dh = c_{p}(T)\\,dT$，熵的微分满足 $ds = \\frac{c_{p}(T)}{T}\\,dT$。在温度区间 $[T_{\\min},T_{\\max}]$（其中 $T_{\\min} > 0$）内，一个物理上合理的焓，其比热容应为非负且可积，并且熵是有限的且随温度升高而增加。\n\n给定 $M$ 个焓的测量值 $\\{(T_{i},\\hat{h}_{i})\\}_{i=1}^{M}$，每个值都参考一个基准温度 $T_{r}\\in[T_{\\min},T_{\\max}]$，因此真实焓满足 $h(T_{r})=0$。学习器不直接预测 $h(T)$；而是预测一个参数化的比热容\n$$\nc_{p,\\theta}(T) = \\big(a + b\\,T + c\\,T^{2}\\big)^{2},\n$$\n其参数为 $\\theta = (a,b,c)$，并通过下式定义预测的焓\n$$\nh_{\\theta}(T) = \\int_{T_{r}}^{T} c_{p,\\theta}(\\tau)\\,d\\tau.\n$$\n为了在模型中嵌入超越单纯单调性的热力学一致性，我们有一个已知的量热参考：两个校准温度 $T_{a},T_{b}\\in[T_{\\min},T_{\\max}]$ 之间的焓差满足 $h(T_{b})-h(T_{a})=\\Delta h_{\\mathrm{ref}}$。学习目标通过一个强制实施该焓差的惩罚项来增强经验风险：\n$$\n\\mathcal{J}(a,b,c) = \\frac{1}{M}\\sum_{i=1}^{M}\\Big(h_{\\theta}(T_{i})-\\hat{h}_{i}\\Big)^{2} \\;+\\; \\mu\\,\\Big(h_{\\theta}(T_{b})-h_{\\theta}(T_{a})-\\Delta h_{\\mathrm{ref}}\\Big)^{2},\n$$\n其中 $\\mu>0$ 是一个给定的惩罚权重。\n\n任务：\n- 使用上述基本定义，陈述在恒压条件下，保证 $h(T)$ 和 $s(T)$ 物理上合理的对比热容 $c_{p}(T)$ 和温度域的充分条件。\n- 证明所选的参数化形式 $c_{p,\\theta}(T) = \\big(a + b\\,T + c\\,T^{2}\\big)^{2}$ 在 $[T_{\\min},T_{\\max}]$（其中 $T_{\\min}>0$）上满足这些条件。\n- 计算 $h_{\\theta}(T)$ 的闭式解，然后推导出 $\\mathcal{J}(a,b,c)$ 关于 $(a,b,c)$、$\\{(T_{i},\\hat{h}_{i})\\}_{i=1}^{M}$、$T_{r}$、$T_{a}$、$T_{b}$、$\\Delta h_{\\mathrm{ref}}$ 和 $\\mu$ 的显式解析表达式。\n\n将您的最终答案表示为 $\\mathcal{J}(a,b,c)$ 的单个闭式表达式。所有温度单位必须为 $\\text{K}$，焓的单位必须为 $\\text{kJ}\\,\\text{kg}^{-1}$。最终损失以 $(\\text{kJ}\\,\\text{kg}^{-1})^{2}$ 为单位表示。不需要进行数值代入，也不需要四舍五入。最终答案必须是单个解析表达式。", "solution": "所提出的问题是一个定义明确的练习，旨在应用基本热力学原理为机器学习模型构建一个正则化的目标函数。该问题在科学上是合理的，内部是一致的，并包含了所有必要的信息。我们将系统地解决它。\n\n任务是为一个数据驱动的焓模型推导学习目标函数 $\\mathcal{J}(a,b,c)$ 的显式解析形式。这将分三个阶段完成：首先，形式化物理约束；其次，验证所提出的模型是否满足这些约束；第三，直接计算目标函数。\n\n首先，我们讨论在恒压条件下，对于温度范围为 $[T_{\\min}, T_{\\max}]$（其中 $T_{\\min} > 0$）的单相流体，其焓 $h(T)$ 和熵 $s(T)$ 的热力学模型要物理上合理所需满足的条件。分析从所提供的基本吉布斯关系开始：\n$$\ndh = c_{p}(T)\\,dT\n$$\n$$\nds = \\frac{c_{p}(T)}{T}\\,dT\n$$\n为使这些关系能够描述一个物理上合理的系统，比热容 $c_{p}(T)$ 必须满足某些条件。\n1.  **热力学稳定性**：比热容必须为非负，即 $c_{p}(T) \\ge 0$。如果比热容为负，热量可能会自发地从系统的较冷部分转移到较热部分，这违反了热力学第二定律。此条件确保了局部热稳定性。\n2.  **熵增原理**：封闭系统的熵必须是其内能的非递减函数。在恒压下，对于单相物质，这意味着熵必须是温度的非递减函数。从熵变关系可知，熵随温度的变化率为 $\\frac{ds}{dT} = \\frac{c_{p}(T)}{T}$。由于绝对温度 $T$（以开尔文为单位）在给定域上是严格为正的（$T \\ge T_{\\min} > 0$），因此条件 $\\frac{ds}{dT} \\ge 0$ 等价于 $c_{p}(T) \\ge 0$。问题要求熵增加，这对应于严格不等式 $c_p(T) > 0$。模型至少应满足非负性约束。\n3.  **可积性和有限性**：为了通过积分明确定义焓 $h(T)$ 和熵 $s(T)$，函数 $c_{p}(T)$ 和 $\\frac{c_{p}(T)}{T}$ 都必须在定义域上可积。一个充分条件是 $c_{p}(T)$ 在闭合有界区间 $[T_{\\min}, T_{\\max}]$ 上连续。连续性保证了可积性。条件 $T_{\\min} > 0$ 至关重要，它确保被积函数 $\\frac{c_{p}(T)}{T}$ 在 $T=0$ 处没有奇点，从而保证熵的积分是有限的。\n\n总之，在 $[T_{\\min}, T_{\\max}]$（其中 $T_{\\min} > 0$）上建立一个物理上合理的模型的充分条件是 $c_{p}(T)$ 是一个连续的非负函数。\n\n其次，我们验证所提出的比热容参数模型 $c_{p,\\theta}(T) = \\big(a + bT + cT^{2}\\big)^{2}$ 是否满足这些条件。参数为 $\\theta = (a, b, c)$。\n1.  函数 $P(T) = a + bT + cT^{2}$ 是关于温度 $T$ 的多项式。多项式对所有实数 $T$ 都是连续的。函数 $c_{p,\\theta}(T)$ 是 $P(T)$ 的平方，而连续函数的复合仍然是连续的。因此，$c_{p,\\theta}(T)$ 在任何区间上都是连续的，包括 $[T_{\\min}, T_{\\max}]$。\n2.  对于实数值参数 $a$、$b$、$c$ 和实数温度 $T$，$P(T)$ 的值是一个实数。任何实数的平方都是非负的。因此，对于所有 $T$，$c_{p,\\theta}(T) = (P(T))^{2} \\ge 0$。这种结构形式以编程方式强制执行了非负性约束，这是物理合理性的一个关键要求。\n\n因此，该模型形式是有效的。\n\n第三，我们推导目标函数 $\\mathcal{J}(a,b,c)$ 的闭式表达式。这需要计算预测的焓 $h_{\\theta}(T)$。我们首先展开 $c_{p,\\theta}(T)$ 的表达式：\n$$\nc_{p,\\theta}(\\tau) = \\big(a + b\\tau + c\\tau^{2}\\big)^{2} = a^{2} + b^{2}\\tau^{2} + c^{2}\\tau^{4} + 2ab\\tau + 2ac\\tau^{2} + 2bc\\tau^{3}\n$$\n按 $\\tau$ 的幂次分组：\n$$\nc_{p,\\theta}(\\tau) = a^{2} + (2ab)\\tau + (b^{2}+2ac)\\tau^{2} + (2bc)\\tau^{3} + c^{2}\\tau^{4}\n$$\n预测的焓 $h_{\\theta}(T)$ 定义为 $c_{p,\\theta}(\\tau)$ 从参考温度 $T_{r}$ 到 $T$ 的积分：\n$$\nh_{\\theta}(T) = \\int_{T_{r}}^{T} c_{p,\\theta}(\\tau)\\,d\\tau = \\int_{T_{r}}^{T} \\left( a^{2} + 2ab\\tau + (b^{2}+2ac)\\tau^{2} + 2bc\\tau^{3} + c^{2}\\tau^{4} \\right) d\\tau\n$$\n为了计算这个积分，我们求出原函数，记为 $H(\\tau)$：\n$$\nH(\\tau) = a^{2}\\tau + ab\\tau^{2} + \\frac{b^{2}+2ac}{3}\\tau^{3} + \\frac{bc}{2}\\tau^{4} + \\frac{c^{2}}{5}\\tau^{5}\n$$\n于是定积分为 $h_{\\theta}(T) = H(T) - H(T_{r})$。展开此式可得：\n$$\nh_{\\theta}(T) = \\left( a^{2}T + abT^{2} + \\frac{b^{2}+2ac}{3}T^{3} + \\frac{bc}{2}T^{4} + \\frac{c^{2}}{5}T^{5} \\right) - \\left( a^{2}T_{r} + abT_{r}^{2} + \\frac{b^{2}+2ac}{3}T_{r}^{3} + \\frac{bc}{2}T_{r}^{4} + \\frac{c^{2}}{5}T_{r}^{5} \\right)\n$$\n这可以按参数乘积重新分组：\n$$\nh_{\\theta}(T) = a^{2}(T - T_{r}) + ab(T^{2} - T_{r}^{2}) + \\frac{2ac}{3}(T^{3} - T_{r}^{3}) + \\frac{b^{2}}{3}(T^{3} - T_{r}^{3}) + \\frac{bc}{2}(T^{4} - T_{r}^{4}) + \\frac{c^{2}}{5}(T^{5} - T_{r}^{5})\n$$\n目标函数 $\\mathcal{J}(a,b,c)$ 有两部分。第一部分是数据点 $\\{(T_{i},\\hat{h}_{i})\\}$ 上的均方误差：\n$$\n\\text{第一项} = \\frac{1}{M}\\sum_{i=1}^{M}\\Big(h_{\\theta}(T_{i})-\\hat{h}_{i}\\Big)^{2}\n$$\n第二部分是强制执行量热约束的惩罚项：\n$$\n\\text{第二项} = \\mu\\,\\Big(h_{\\theta}(T_{b})-h_{\\theta}(T_{a})-\\Delta h_{\\mathrm{ref}}\\Big)^{2}\n$$\n对于惩罚项，焓差为：\n$$\nh_{\\theta}(T_{b}) - h_{\\theta}(T_{a}) = (H(T_{b}) - H(T_{r})) - (H(T_{a}) - H(T_{r})) = H(T_{b}) - H(T_{a})\n$$\n这得到：\n$$\nh_{\\theta}(T_{b})-h_{\\theta}(T_{a}) = a^{2}(T_{b}- T_{a}) + ab(T_{b}^{2} - T_{a}^{2}) + \\frac{2ac}{3}(T_{b}^{3} - T_{a}^{3}) + \\frac{b^{2}}{3}(T_{b}^{3} - T_{a}^{3}) + \\frac{bc}{2}(T_{b}^{4} - T_{a}^{4}) + \\frac{c^{2}}{5}(T_{b}^{5} - T_{a}^{5})\n$$\n将这些表达式代入目标函数，即可得到最终的显式形式。\n\n$\\mathcal{J}(a,b,c)$ 的完整表达式为：\n$$\n\\mathcal{J}(a,b,c) = \\frac{1}{M}\\sum_{i=1}^{M}\\left( a^{2}(T_{i}-T_{r}) + ab(T_{i}^{2}-T_{r}^{2}) + \\frac{2ac}{3}(T_{i}^{3}-T_{r}^{3}) + \\frac{b^{2}}{3}(T_{i}^{3}-T_{r}^{3}) + \\frac{bc}{2}(T_{i}^{4}-T_{r}^{4}) + \\frac{c^{2}}{5}(T_{i}^{5}-T_{r}^{5}) - \\hat{h}_{i} \\right)^{2} \\\\\n+ \\mu \\left( a^{2}(T_{b}-T_{a}) + ab(T_{b}^{2}-T_{a}^{2}) + \\frac{2ac}{3}(T_{b}^{3}-T_{a}^{3}) + \\frac{b^{2}}{3}(T_{b}^{3}-T_{a}^{3}) + \\frac{bc}{2}(T_{b}^{4}-T_{a}^{4}) + \\frac{c^{2}}{5}(T_{b}^{5}-T_{a}^{5}) - \\Delta h_{\\mathrm{ref}} \\right)^{2}\n$$\n这就是所要求的结果。", "answer": "$$\n\\boxed{\\mathcal{J}(a,b,c) = \\frac{1}{M}\\sum_{i=1}^{M}\\left( a^{2}(T_{i}-T_{r}) + ab(T_{i}^{2}-T_{r}^{2}) + \\frac{2ac}{3}(T_{i}^{3}-T_{r}^{3}) + \\frac{b^{2}}{3}(T_{i}^{3}-T_{r}^{3}) + \\frac{bc}{2}(T_{i}^{4}-T_{r}^{4}) + \\frac{c^{2}}{5}(T_{i}^{5}-T_{r}^{5}) - \\hat{h}_{i} \\right)^{2} + \\mu \\left( a^{2}(T_{b}-T_{a}) + ab(T_{b}^{2}-T_{a}^{2}) + \\frac{2ac}{3}(T_{b}^{3}-T_{a}^{3}) + \\frac{b^{2}}{3}(T_{b}^{3}-T_{a}^{3}) + \\frac{bc}{2}(T_{b}^{4}-T_{a}^{4}) + \\frac{c^{2}}{5}(T_{b}^{5}-T_{a}^{5}) - \\Delta h_{\\mathrm{ref}} \\right)^{2}}\n$$", "id": "2502951"}, {"introduction": "将机器学习应用于加速动态系统的仿真是其最有价值的应用之一。本实践构建了一个用于求解非线性热传导问题的机器学习时间积分代理模型，并让你直接面对一个核心挑战：数值稳定性[@problem_id:2502968]。通过实现并比较显式与隐式两种时间步进格式，你将深入理解不同模型结构在长期预测精度和计算效率之间的权衡。", "problem": "您将实现并分析一个用于一维非线性热传导的机器学习时间隐式代理模型，该模型被表述为一个离散残差方程，其下一时刻的温度 $T^{n+1}$ 通过定点迭代获得。您将使用相同的学习残差，将其稳定性与显式（前向 Euler）展开进行比较。所有计算都将以无量纲形式进行，因此不需要物理单位。三角函数中的所有角度必须以弧度为单位。最终程序必须按如下规定生成单行输出。\n\n考虑从能量守恒和 Fourier 热传导定律导出的一维无量纲热方程，\n$$\n\\frac{\\partial T}{\\partial t} = \\frac{\\partial^2 T}{\\partial x^2} + \\beta \\, s(T),\n$$\n在空间区间 $[0,1]$ 上，边界条件为齐次 Dirichlet 条件 $T(0,t)=0$ 和 $T(1,t)=0$。令 $x_i = i h$，$i = 1,2,\\dots,M$，其中 $h = \\frac{1}{M+1}$ 是均匀的网格间距。内部网格上拉普拉斯算子的中心有限差分近似产生了作用于内部向量 $T \\in \\mathbb{R}^M$ 的半离散算子 $L \\in \\mathbb{R}^{M \\times M}$，其形式为\n$$\n(LT)_i = \\frac{T_{i-1} - 2 T_i + T_{i+1}}{h^2}, \\quad i \\in \\{1,2,\\dots,M\\},\n$$\n根据边界条件，约定 $T_0 = 0$ 和 $T_{M+1} = 0$。我们将非线性源定义为 $s(T) = \\sin(T)$，并按元素方式应用。\n\n令 $T^n \\in \\mathbb{R}^M$ 为时间 $t^n$ 时的温度向量，并考虑一个与后向 Euler 时间步进相对应的学习残差代理模型，\n$$\nR_\\theta\\left(T^{n+1}; T^n\\right) \\equiv T^{n+1} - T^n - \\Delta t \\left( (1+\\theta_L) \\, L T^{n+1} + (1+\\theta_s) \\, \\beta \\, s\\!\\left(T^{n+1}\\right) \\right),\n$$\n其中 $\\Delta t$ 是时间步长，$\\theta = (\\theta_L,\\theta_s)$ 是编码模型形式误差的学习标量，$\\beta$ 是控制源强度的标量。\n\n您的任务分为三个部分：\n- 实现一个时间隐式代理模型，通过使用定点迭代求解 $R_\\theta\\left(T^{n+1}; T^n\\right) = 0$ 来将 $T^n$推进到 $T^{n+1}$。使用映射\n$$\n\\mathcal{G}(U; T^n) = T^n + \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right),\n$$\n并执行阻尼 Picard 迭代\n$$\nU^{(k+1)} = (1-\\lambda) \\, U^{(k)} + \\lambda \\, \\mathcal{G}\\!\\left(U^{(k)}; T^n\\right),\n$$\n迭代初始值为 $U^{(0)} = T^n$，其中 $\\lambda \\in (0,1]$ 是一个阻尼参数。当 $\\|U^{(k+1)} - U^{(k)}\\|_2 \\le \\varepsilon$ 或达到最大迭代次数 $K$ 时终止迭代。如果未达到收敛（即，在未满足容差的情况下达到了最大迭代次数），则将该时间步分类为对隐式代理模型不稳定，并停止该测试用例的进一步步进。\n- 使用相同的学习残差模型实现一个显式展开代理模型，即采用前向 Euler 方法，其形式为\n$$\nT^{n+1} = T^n + \\Delta t \\left( (1+\\theta_L) \\, L T^n + (1+\\theta_s) \\, \\beta \\, s\\!\\left(T^n\\right) \\right).\n$$\n- 在多个时间步上比较稳定性，直到指定的最终时间。稳定性判据定义如下：对于一个测试用例，如果在每个时间步，迭代结果不包含非数（Not-a-Number）和无穷大，并且满足 $\\|T^n\\|_2 \\le B_{\\max}$（其中 $B_{\\max}$ 是一个指定的界限），则该方法是稳定的。如果一个时间步违反了这些检查中的任何一项，则将该方法分类为不稳定，并停止该测试用例的进一步步进。\n\n初始化和通用设置：\n- 使用初始条件 $T_i^0 = \\sin(\\pi x_i)$，$i \\in \\{1,2,\\dots,M\\}$。\n- 通过仅作用于内部的算子 $L$ 并结合 $T_0 = 0$ 和 $T_{M+1} = 0$ 来隐式地实现齐次 Dirichlet 边界条件。\n- 使用以下定点参数：阻尼 $\\lambda = 0.2$，容差 $\\varepsilon = 10^{-8}$，以及最大迭代次数 $K = 200$。\n- 使用稳定性界限 $B_{\\max} = 10^6$。\n- 所有三角函数参数均以弧度为单位。\n\n测试套件：\n所有测试均使用 $M = 20$ 个内部节点。对于每个测试用例，使用隐式代理模型和显式代理模型，从 $t^0 = 0$ 到 $t^{N} = t_{\\mathrm{final}}$ 进行时间步进，步数为 $N = \\lfloor t_{\\mathrm{final}} / \\Delta t \\rfloor$。四个测试用例如下：\n- 情况 1：$(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.0006, 0.05, 0.1, 0.5, 0.05)$。\n- 情况 2：$(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.002, 0.05, 0.1, 0.5, 0.05)$。\n- 情况 3：$(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.00115, 0.05, 0.1, 0.5, 0.05)$。\n- 情况 4：$(\\Delta t, \\theta_L, \\theta_s, \\beta, t_{\\mathrm{final}}) = (0.0018, -0.4, -0.2, 0.5, 0.05)$。\n\n输出规格：\n- 对于每个测试用例，计算一个由以下规则定义的稳定性代码 $c$：\n  - 如果隐式和显式代理模型都稳定，则 $c = 0$；\n  - 如果隐式代理模型稳定而显式代理模型不稳定，则 $c = 1$；\n  - 如果两者都不稳定，则 $c = 2$；\n  - 如果隐式代理模型不稳定而显式代理模型稳定，则 $c = 3$。\n- 同时，为每个测试用例计算终端范数 $n_{\\mathrm{imp}} = \\|T^N_{\\mathrm{imp}}\\|_2$ 和 $n_{\\mathrm{exp}} = \\|T^N_{\\mathrm{exp}}\\|_2$，其中 $T^N_{\\mathrm{imp}}$ 和 $T^N_{\\mathrm{exp}}$ 分别表示由隐式和显式代理模型产生的终端状态。\n- 您的程序应生成单行输出，其中包含所有四个情况的汇总结果，格式为方括号内以逗号分隔的列表，顺序如下\n$$\n[c_1, \\text{round}(n_{\\mathrm{imp},1}, 6), \\text{round}(n_{\\mathrm{exp},1}, 6), c_2, \\text{round}(n_{\\mathrm{imp},2}, 6), \\text{round}(n_{\\mathrm{exp},2}, 6), c_3, \\text{round}(n_{\\mathrm{imp},3}, 6), \\text{round}(n_{\\mathrm{exp},3}, 6), c_4, \\text{round}(n_{\\mathrm{imp},4}, 6), \\text{round}(n_{\\mathrm{exp},4}, 6)].\n$$\n每个浮点数在打印输出时必须精确到 $6$ 位小数。", "solution": "该问题要求实现并分析两种用于一维非线性热方程的数值代理模型，并进行稳定性分析。一个代理模型基于隐式时间步进格式，通过定点迭代求解；另一个是显式前向时间积分。这两种方法的稳定性将在一个测试套件中进行比较。\n\n控制偏微分方程是无量纲的反应扩散方程：\n$$\n\\frac{\\partial T}{\\partial t} = \\frac{\\partial^2 T}{\\partial x^2} + \\beta \\, s(T)\n$$\n在这里，$T(x,t)$ 是温度，$x \\in [0,1]$ 是空间坐标，$t$ 是时间，$\\beta$ 是源项强度的标量参数，$s(T) = \\sin(T)$ 是非线性源函数。施加了齐次 Dirichlet 边界条件 $T(0,t) = T(1,t) = 0$。\n\n首先，我们对空间域进行离散化。我们使用一个包含 $M$ 个内部点的均匀网格，$x_i = i h$，$i \\in \\{1, 2, \\dots, M\\}$，其中网格间距为 $h = \\frac{1}{M+1}$。空间导数项 $\\frac{\\partial^2 T}{\\partial x^2}$ 使用二阶中心有限差分格式进行近似。这产生了一个关于内部网格点温度向量 $T(t) \\in \\mathbb{R}^M$ 的常微分方程组。空间算子由一个矩阵 $L \\in \\mathbb{R}^{M \\times M}$ 表示，其中 $(LT)_i = \\frac{T_{i-1} - 2T_i + T_{i+1}}{h^2}$。边界条件 $T_0 = T_{M+1} = 0$ 已被整合到该矩阵的结构中。具体来说，$L$ 是一个对称三对角矩阵，主对角线上的元素为 $-\\frac{2}{h^2}$，第一超对角线和次对角线上的元素为 $\\frac{1}{h^2}$。\n\n初始条件由正弦剖面给出：$T_i^0 = \\sin(\\pi x_i)$，$i \\in \\{1,\\dots,M\\}$。\n\n然后，问题引入了一个“机器学习”代理模型，该模型使用标量参数 $\\theta_L$ 和 $\\theta_s$ 来修正标准数值格式。这些参数代表了对物理模型的扩散项和源项的学习修正。\n\n显式代理模型使用类似前向 Euler 的方法在时间上推进解：\n$$\nT^{n+1} = T^n + \\Delta t \\left( (1+\\theta_L) \\, L T^n + (1+\\theta_s) \\, \\beta \\, s(T^n) \\right)\n$$\n其中 $T^n$ 是在时间 $t^n = n \\Delta t$ 时的温度向量。\n\n隐式代理模型基于类似后向 Euler 的格式。下一个时间步的解 $T^{n+1}$ 是残差方程 $R_\\theta(T^{n+1}; T^n) = 0$ 的根，其中：\n$$\nR_\\theta\\left(U; T^n\\right) \\equiv U - T^n - \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right)\n$$\n为了求解这个关于 $U = T^{n+1}$ 的非线性系统，我们将方程重排为一个定点问题 $U = \\mathcal{G}(U; T^n)$，映射函数定义为：\n$$\n\\mathcal{G}(U; T^n) = T^n + \\Delta t \\left( (1+\\theta_L) \\, L U + (1+\\theta_s) \\, \\beta \\, s(U) \\right)\n$$\n这个问题通过使用阻尼 Picard 迭代来求解，初始值为 $U^{(0)} = T^n$。迭代更新规则是：\n$$\nU^{(k+1)} = (1-\\lambda) \\, U^{(k)} + \\lambda \\, \\mathcal{G}\\!\\left(U^{(k)}; T^n\\right)\n$$\n其中 $\\lambda = 0.2$ 是阻尼参数。迭代持续进行，直到连续迭代之间的变化足够小，即 $\\|U^{(k+1)} - U^{(k)}\\|_2 \\le \\varepsilon = 10^{-8}$，或者达到最大迭代次数 $K=200$。\n\n在每个时间步都会评估每种方法的稳定性。对于给定的测试用例，如果在任何时间步 $n$，计算出的解 $T^n$ 包含非数（NaN）或无穷大（inf）值，或者其欧几里得范数超过一个界限，即 $\\|T^n\\|_2 > B_{\\max} = 10^6$，则该方法被认为是不稳定的。对于隐式方法，Picard 迭代未能在 $K$ 步内收敛也构成不稳定。如果一个方法被认为不稳定，则该方法的时间步进过程将被终止。\n\n对于四个测试用例中的每一个，总体流程如下：\n1. 使用给定的初始条件初始化温度向量 $T^0$。\n2. 对于隐式和显式代理模型，都从 $t=0$ 到 $t_{\\mathrm{final}}$ 进行时间步进，总共 $N = \\lfloor t_{\\mathrm{final}} / \\Delta t \\rfloor$ 步。\n3. 在每次模拟期间，监测指定的失稳条件。\n4. 记录每种方法是否稳定地完成了所有 $N$ 步。\n5. 根据稳定性结果，确定稳定性代码 $c$：如果两者都稳定，则为 $0$；如果隐式稳定而显式不稳定，则为 $1$；如果两者都不稳定，则为 $2$；如果隐式不稳定而显式稳定，则为 $3$。\n6. 计算每种方法的最终温度向量的欧几里得范数。如果一个方法在第 $k  N$ 步变得不稳定，则状态 $T^k$ 被视为该方法的终端状态。\n7. 将所有四个测试用例的这些结果（$c$ 和两个终端范数）整理成一个单一格式的输出字符串。范数报告时格式化为 $6$ 位小数。\n\n这涉及到对矩阵算子、时间步进循环、迭代求解器以及指定的稳定性检查的仔细实现。", "answer": "```python\nimport numpy as np\nimport math\n\n# Define global constants from the problem statement\nM = 20\nLAMBDA = 0.2\nEPSILON = 1e-8\nK_MAX = 200\nB_MAX = 1e6\n\ndef get_laplacian(m_nodes):\n    \"\"\"Constructs the 1D finite difference Laplacian matrix L.\"\"\"\n    h = 1.0 / (m_nodes + 1)\n    h2 = h * h\n    main_diag = -2.0 / h2 * np.ones(m_nodes)\n    off_diag = 1.0 / h2 * np.ones(m_nodes - 1)\n    L = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n    return L\n\ndef source_term(T):\n    \"\"\"Computes the nonlinear source term s(T) = sin(T).\"\"\"\n    return np.sin(T)\n\ndef solve_implicit_step(Tn, dt, theta_L, theta_s, beta, L):\n    \"\"\"\n    Solves for T^{n+1} using damped Picard iterations.\n    Returns the solution and a boolean indicating convergence.\n    \"\"\"\n    U_k = Tn.copy()\n    \n    for _ in range(K_MAX):\n        # Calculate G(U_k; Tn)\n        Lu = L @ U_k\n        s_U = source_term(U_k)\n        G_U = Tn + dt * ((1 + theta_L) * Lu + (1 + theta_s) * beta * s_U)\n        \n        # Damped Picard update\n        U_k_plus_1 = (1 - LAMBDA) * U_k + LAMBDA * G_U\n        \n        # Check for convergence\n        diff = np.linalg.norm(U_k_plus_1 - U_k)\n        \n        # Check for numerical issues in the iteration itself\n        if np.isnan(diff) or np.isinf(diff):\n            return U_k_plus_1, False\n        \n        if diff = EPSILON:\n            return U_k_plus_1, True\n            \n        U_k = U_k_plus_1\n        \n    return U_k, False # Did not converge\n\ndef run_simulation(params, L, x_grid, method):\n    \"\"\"\n    Runs a simulation for a given method ('implicit' or 'explicit').\n    Returns a stability flag and the final terminal norm.\n    \"\"\"\n    dt, theta_L, theta_s, beta, t_final = params\n    \n    # Initial condition\n    T = np.sin(np.pi * x_grid)\n    \n    # Check initial condition stability (unlikely to be unstable)\n    norm_T = np.linalg.norm(T)\n    if not np.isfinite(norm_T) or norm_T > B_MAX:\n        return False, norm_T\n        \n    num_steps = math.floor(t_final / dt)\n    \n    for _ in range(num_steps):\n        if method == 'implicit':\n            T_next, converged = solve_implicit_step(T, dt, theta_L, theta_s, beta, L)\n            if not converged:\n                # Instability due to non-convergence\n                return False, np.linalg.norm(T) # Return norm of last good state\n        else: # explicit\n            LTn = L @ T\n            s_T = source_term(T)\n            T_next = T + dt * ((1 + theta_L) * LTn + (1 + theta_s) * beta * s_T)\n            \n        # General stability check for the new state T_next\n        norm_T_next = np.linalg.norm(T_next)\n        if not np.isfinite(norm_T_next) or norm_T_next > B_MAX:\n            # Instability due to blow-up or NaN\n            return False, norm_T_next\n            \n        T = T_next\n        \n    # If all steps completed successfully\n    return True, np.linalg.norm(T)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print the final result.\"\"\"\n    \n    # Setup spatial discretization\n    L = get_laplacian(M)\n    h = 1.0 / (M + 1)\n    x_grid = np.array([i * h for i in range(1, M + 1)])\n\n    test_cases = [\n        (0.0006, 0.05, 0.1, 0.5, 0.05),\n        (0.002, 0.05, 0.1, 0.5, 0.05),\n        (0.00115, 0.05, 0.1, 0.5, 0.05),\n        (0.0018, -0.4, -0.2, 0.5, 0.05),\n    ]\n\n    results = []\n    \n    for case_params in test_cases:\n        # Run implicit simulation\n        imp_stable, n_imp = run_simulation(case_params, L, x_grid, 'implicit')\n        \n        # Run explicit simulation\n        exp_stable, n_exp = run_simulation(case_params, L, x_grid, 'explicit')\n        \n        # Determine stability code c\n        if imp_stable and exp_stable:\n            c = 0\n        elif imp_stable and not exp_stable:\n            c = 1\n        elif not imp_stable and not exp_stable:\n            c = 2\n        else: # not imp_stable and exp_stable\n            c = 3\n        \n        # Append results for this case\n        results.append(c)\n        results.append(f\"{n_imp:.6f}\")\n        results.append(f\"{n_exp:.6f}\")\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2502968"}]}