{"hands_on_practices": [{"introduction": "反幂法（Inverse Power Method）的核心在于一个迭代过程，每一步都涉及求解一个线性方程组。这个练习将带你完成该算法中最基本的一步计算。通过求解 $(A-\\sigma I)y_1 = x_0$，你将亲手实践如何从一个初始向量生成序列中的下一个（未归一化的）向量，这是理解整个迭代过程如何运作的基石。[@problem_id:1395843]", "problem": "在一个数值算法中，从一个初始向量 $x_0$ 开始生成一个向量序列。此序列中的第一个未归一化向量（记为 $y_1$）可通过求解线性方程组 $(A-\\sigma I)y_1 = x_0$ 得到。其中 $A$ 为方阵，$\\sigma$ 为标量位移，$I$是与 $A$ 维度相同的单位矩阵。\n\n给定矩阵 $A = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$，位移 $\\sigma = 1.5$ 和初始向量 $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，求向量 $y_1$ 的分量。将答案表示为一个行矩阵，其中每个分量为精确分数或小数。", "solution": "我们需要为 $y_{1}$ 求解线性方程组 $(A-\\sigma I) y_{1} = x_{0}$，其中 $A = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$，$\\sigma = \\frac{3}{2}$，以及 $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n首先计算位移矩阵：\n$$\nA - \\sigma I = \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} - \\frac{3}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & -1 \\\\ -1 & \\frac{3}{2} \\end{pmatrix}.\n$$\n记 $M = A - \\sigma I$。则 $y_{1} = M^{-1} x_{0}$。对于一个 $2 \\times 2$ 矩阵 $M = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$，其逆矩阵为 $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$。此处 $a = d = \\frac{3}{2}$ 且 $b = c = -1$，因此\n$$\n\\det(M) = \\left(\\frac{3}{2}\\right)\\left(\\frac{3}{2}\\right) - (-1)(-1) = \\frac{9}{4} - 1 = \\frac{5}{4},\n$$\n并且\n$$\n\\operatorname{adj}(M) = \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix}.\n$$\n因此，\n$$\nM^{-1} = \\frac{1}{\\frac{5}{4}} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix}.\n$$\n再乘以 $x_{0}$，\n$$\ny_{1} = M^{-1} x_{0} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} & 1 \\\\ 1 & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ \\frac{4}{5} \\end{pmatrix}.\n$$\n因此，$y_{1}$ 的分量是 $\\frac{6}{5}$ 和 $\\frac{4}{5}$，我们将其表示为一个行矩阵。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{6}{5} & \\frac{4}{5} \\end{pmatrix}}$$", "id": "1395843"}, {"introduction": "带位移的反幂法通常会收敛到最接近位移 $\\sigma$ 的特征值，但这并非绝对。这个练习揭示了一个重要的例外情况，展示了初始向量 $x_0$ 的选择如何能够“欺骗”算法，使其收敛到另一个特征向量。通过分析初始向量在特征基下的分量，你将理解为什么算法的行为可能出乎意料，这对于深刻掌握该方法的收敛性至关重要。[@problem_id:1395866]", "problem": "考虑如下给出的实值矩阵 $A$：\n$$A = \\begin{pmatrix} 2.5 & -1.5 & 0 \\\\ -1.5 & 2.5 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}$$\n该矩阵的特征值为 $\\lambda_1 = 1$，$\\lambda_2 = 4$ 和 $\\lambda_3 = 5$。对应的未归一化特征向量分别为 $v_1 = [1, 1, 0]^T$，$v_2 = [1, -1, 0]^T$ 和 $v_3 = [0, 0, 1]^T$。\n\n使用反幂法来寻找矩阵 $A$ 的一个特征值-特征向量对。该方法从一个初始向量 $x_0$ 开始，并使用一个位移 $\\sigma$。其迭代步骤定义为：求解 $(A - \\sigma I) y_{k+1} = x_k$ 得到 $y_{k+1}$，然后进行归一化以获得下一个向量 $x_{k+1} = y_{k+1} / \\|y_{k+1}\\|$。当 $k \\to \\infty$ 时，向量 $x_k$ 收敛到 $A$ 的一个特征向量。\n\n假设该方法以位移 $\\sigma = 0.9$ 和初始向量 $x_0 = [1, -1, 1]^T$ 执行。向量序列 $\\{x_k\\}$ 将收敛到 $A$ 的一个特征向量。确定与此特征向量对应的特征值。", "solution": "带位移 $\\sigma$ 的反幂法应用以下迭代\n$$\n(A-\\sigma I) y_{k+1} = x_{k}, \\quad x_{k+1} = \\frac{y_{k+1}}{\\|y_{k+1}\\|}.\n$$\n设 $A$ 的特征对为 $(\\lambda_{i}, v_{i})$，其中 $i \\in \\{1,2,3\\}$，$\\lambda_{1}=1$、$\\lambda_{2}=4$、$\\lambda_{3}=5$ 且 $v_{1} = [1, 1, 0]^{T}$、$v_{2} = [1, -1, 0]^{T}$、$v_{3} = [0, 0, 1]^{T}$。\n\n将初始向量在特征基中分解：\n$$\nx_{0} = \\alpha_{1} v_{1} + \\alpha_{2} v_{2} + \\alpha_{3} v_{3}.\n$$\n匹配分量，对于前两个分量，我们求解\n$$\n\\alpha_{1} + \\alpha_{2} = 1, \\quad \\alpha_{1} - \\alpha_{2} = -1,\n$$\n解得 $\\alpha_{1} = 0$，$\\alpha_{2} = 1$。对于第三个分量，我们有 $\\alpha_{3} = 1$。因此\n$$\nx_{0} = 0 \\cdot v_{1} + 1 \\cdot v_{2} + 1 \\cdot v_{3}.\n$$\n\n因为 $(A-\\sigma I) v_{i} = (\\lambda_{i} - \\sigma) v_{i}$，所以有\n$$\n(A-\\sigma I)^{-1} v_{i} = \\frac{1}{\\lambda_{i} - \\sigma} \\, v_{i}.\n$$\n经过 $k$ 次反向迭代后（在归一化之前），该向量与下式成正比\n$$\n\\sum_{i=1}^{3} \\alpha_{i} (\\lambda_{i} - \\sigma)^{-k} v_{i}.\n$$\n因此，当 $k \\to \\infty$ 时，在那些 $\\alpha_{i} \\neq 0$ 的项中，具有最大幅值因子 $\\left|(\\lambda_{i} - \\sigma)^{-1}\\right|$ 的分量将占主导地位。\n\n当 $\\sigma = 0.9$ 时，我们有\n$$\n\\left|\\frac{1}{\\lambda_{2} - \\sigma}\\right| = \\frac{1}{|4 - 0.9|} = \\frac{1}{3.1}, \\qquad\n\\left|\\frac{1}{\\lambda_{3} - \\sigma}\\right| = \\frac{1}{|5 - 0.9|} = \\frac{1}{4.1}.\n$$\n由于 $\\frac{1}{3.1} > \\frac{1}{4.1}$ 且 $\\alpha_{2}, \\alpha_{3} \\neq 0$ 但 $\\alpha_{1} = 0$，序列 $\\{x_{k}\\}$ 收敛到对应于 $\\lambda_{2} = 4$ 的特征向量 $v_{2}$。\n\n因此，与极限特征向量对应的特征值为 $4$。", "answer": "$$\\boxed{4}$$", "id": "1395866"}, {"introduction": "从理论走向实践，当我们将反幂法应用于大型稀疏矩阵时，计算效率成为一个关键问题。算法的每一步都需要求解一个大型线性系统，而求解方式的选择会极大地影响性能。本练习将引导你分析两种主流策略——基于LU分解的直接法和迭代法——的计算成本，并推导出它们之间的“盈亏平衡点”，这对于任何从事数值计算的科学家或工程师来说都是一项核心技能。[@problem_id:1395838]", "problem": "在数值线性代数中，带位移的反幂法用于寻找矩阵 $A$ 的最接近给定偏移量 $\\sigma$ 的特征值。该算法的一个关键步骤是重复求解形如 $(A - \\sigma I)\\boldsymbol{z}_{k+1} = \\boldsymbol{y}_k$ 的线性系统。\n\n考虑一个大小为 $N \\times N$ 的大型稀疏实对称矩阵 $A$。该矩阵为带状矩阵，在主对角线的上方和下方各有 $m$ 条非零对角线（总计 $2m+1$ 条非零对角线）。我们欲执行 $K$ 次带位移反幂法迭代。\n\n为求解每次迭代中的线性系统，提出了两种策略：\n\n**策略1（直接法）：** 首先，计算矩阵 $M = A - \\sigma I$ 的下上（LU）分解。这是一次性成本。然后，在 $K$ 次迭代的每一次中，使用前向和后向代换求解该系统。对于较大的 $N$，主要计算成本（以浮点运算或flops为单位）如下：\n- 此带状矩阵的LU分解：$2Nm^2$ flops。\n- 一次前向和后向代换：$4Nm$ flops。\n\n**策略2（迭代法）：** 对于 $K$ 次幂法迭代中的每一次，使用迭代求解器（如Jacobi或Gauss-Seidel方法）求解线性系统。我们假设对于每个系统，此迭代求解器需要固定次数的内迭代（$J$次）来达到所需精度。此方法的主要计算成本为：\n- 迭代求解器的一次内迭代：$2N(2m+1)$ flops。\n\n你的任务是确定这两种策略之间的平衡点。找到内迭代次数 $J$ 的一个表达式，在该迭代次数下，策略2在 $K$ 次幂法迭代过程中的总计算成本恰好等于策略1的总计算成本。假设 $N$、$K$ 和 $m$ 足够大，以致所提供的主要成本项是精确的。你的最终表达式应为 $m$ 和 $K$ 的函数。", "solution": "我们比较两种策略在 $K$ 次外迭代过程中的总浮点运算次数。\n\n对于策略1（使用带状LU分解的直接法），其总成本为一次性LU分解的成本加上 $K$ 次三角系统求解的成本：\n$$\nC_{1}=2Nm^{2}+K\\cdot(4Nm)=2Nm^{2}+4KNm.\n$$\n\n对于策略2（每次外迭代均采用迭代求解），总共进行 $K$ 次外迭代，每次外迭代执行 $J$ 次内迭代，而每次内迭代的成本为 $2N(2m+1)$ flops，因此总成本为：\n$$\nC_{2}=K\\cdot J\\cdot 2N(2m+1).\n$$\n\n在平衡点处，我们设 $C_{1}=C_{2}$，并求解 $J$：\n$$\n2Nm^{2}+4KNm=KJ\\cdot 2N(2m+1).\n$$\n两边同除以 $2N$：\n$$\nm^{2}+2Km=KJ(2m+1).\n$$\n因此，\n$$\nJ=\\frac{m^{2}+2Km}{K(2m+1)}=\\frac{m(m+2K)}{K(2m+1)}.\n$$\n此表达式仅与 $m$ 和 $K$ 有关，符合题意要求。", "answer": "$$\\boxed{\\frac{m\\left(m+2K\\right)}{K\\left(2m+1\\right)}}$$", "id": "1395838"}]}