{"hands_on_practices": [{"introduction": "近端算子是近端梯度方法的核心引擎。要掌握该算法，第一步是理解并能够计算这个基本构建块。本练习将通过一个简单的一维二次函数，引导你直接根据其定义推导近端算子，这是一个基本但至关重要的技能，可以帮助你深入理解近端算子是如何将一个复杂的优化问题分解为一系列更简单子问题来求解的。[@problem_id:2195112]", "problem": "在数值优化领域，近端算子是一种基本工具，用于设计求解不可微或约束问题的算法。对于给定的标量函数 $g(x)$ 和一个正标度参数 $\\lambda > 0$，$\\lambda g$ 作用于点 $v$ 的近端算子定义为使复合目标函数最小化的 $x$ 的值。\n\n其定义由下式正式给出：\n$$\n\\text{prox}_{\\lambda g}(v) = \\mathop{\\arg\\min}_{x \\in \\mathbb{R}} \\left( g(x) + \\frac{1}{2\\lambda} (x-v)^2 \\right)\n$$\n你的任务是求出一般二次函数的近端算子。考虑函数 $g(x) = \\frac{1}{2}ax^2 + bx$，其中 $a$ 和 $b$ 是实值常数且 $a > 0$。\n\n推导 $\\text{prox}_{\\lambda g}(v)$ 关于参数 $a$、$b$、$v$ 和 $\\lambda$ 的闭式解析表达式。", "solution": "题目要求我们计算 $g(x)=\\frac{1}{2}a x^{2}+b x$（其中 $a>0$ 且 $\\lambda>0$）的 $\\text{prox}_{\\lambda g}(v)$。根据定义，\n$$\n\\text{prox}_{\\lambda g}(v)=\\mathop{\\arg\\min}_{x\\in\\mathbb{R}}\\left(\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}\\right).\n$$\n定义目标函数\n$$\nJ(x)=\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}.\n$$\n由于 $a>0$ 且 $\\lambda>0$，函数 $J$ 是严格凸的，因为其二阶导数为\n$$\nJ''(x)=a+\\frac{1}{\\lambda}>0,\n$$\n所以它有唯一一个由一阶最优性条件 $J'(x)=0$ 刻画的最小值点。计算其导数：\n$$\nJ'(x)=a x+b+\\frac{1}{\\lambda}(x-v)=(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}.\n$$\n令 $J'(x)=0$，解出 $x$：\n$$\n(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}=0\n\\;\\;\\Longrightarrow\\;\\;\nx=\\frac{\\frac{v}{\\lambda}-b}{a+\\frac{1}{\\lambda}}.\n$$\n分子分母同乘以 $\\lambda$ 可得\n$$\nx=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$\n因此，\n$$\n\\text{prox}_{\\lambda g}(v)=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$", "answer": "$$\\boxed{\\frac{v-b\\lambda}{1+a\\lambda}}$$", "id": "2195112"}, {"introduction": "理解了近端算子的概念后，下一步是观察它如何在近端梯度方法的一次完整迭代中发挥作用。该算法的每一步都巧妙地结合了两个操作：首先对函数的光滑部分进行一次标准的梯度下降，然后应用近端算子来处理非光滑部分或约束。这个练习将让你手动执行一个完整的迭代步骤，通过处理一个带非负约束的最小化问题，你将直观地看到近端算子如何作为一个“投影”工具，确保迭代点始终保持在可行域内。[@problem_id:2195110]", "problem": "考虑这样一个优化问题：找到一个点 $\\mathbf{x} = (x_1, x_2) \\in \\mathbb{R}^2$ 来最小化函数 $F(\\mathbf{x})$，并满足其分量的非负约束，即 $x_1 \\ge 0$ 和 $x_2 \\ge 0$。要最小化的函数是从 $\\mathbf{x}$ 到目标点 $\\mathbf{a}$ 的欧几里得距离的平方，由 $F(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{a}\\|_2^2$ 给出。\n\n通过将光滑部分定义为 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{a}\\|_2^2$，非光滑部分 $g(\\mathbf{x})$ 定义为非负象限的指示函数，该问题可以被表示为邻近算法的标准形式 $\\min_{\\mathbf{x}} f(\\mathbf{x}) + g(\\mathbf{x})$。如果 $x_1 \\ge 0$ 且 $x_2 \\ge 0$，指示函数 $g(\\mathbf{x})$ 为零，否则为无穷大。\n\n您的任务是应用邻近梯度法来解决这个问题。邻近梯度法的迭代更新规则由下式给出：\n$$ \\mathbf{x}_{k+1} = \\text{prox}_{\\gamma g}(\\mathbf{x}_k - \\gamma \\nabla f(\\mathbf{x}_k)) $$\n其中 $\\gamma$ 是步长，$\\text{prox}_{\\gamma g}$ 是与函数 $g$ 相关联的邻近算子。\n\n给定目标点 $\\mathbf{a} = (5, -4)$，初始点 $\\mathbf{x}_0 = (1, 1)$，以及步长 $\\gamma = 0.2$，计算下一个迭代点 $\\mathbf{x}_1$。将您的答案表示为一个行向量 $(x_{1,1}, x_{1,2})$，其中 $x_{1,1}$ 和 $x_{1,2}$ 是向量 $\\mathbf{x}_1$ 的分量。", "solution": "我们要在非负象限上最小化 $F(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{x}-\\mathbf{a}\\|_{2}^{2}$。在邻近梯度分解中，设 $f(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{x}-\\mathbf{a}\\|_{2}^{2}$ 且 $g(\\mathbf{x})=\\iota_{\\mathbb{R}_{+}^{2}}(\\mathbf{x})$，即为可行集 $\\mathbb{R}_{+}^{2}=\\{\\mathbf{x}\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$ 的指示函数。\n\n$f$ 的梯度由下式给出\n$$\n\\nabla f(\\mathbf{x})=\\mathbf{x}-\\mathbf{a}.\n$$\n在点 $\\mathbf{z}$ 处，$\\gamma g$ 的邻近算子是到 $\\mathbb{R}_{+}^{2}$ 上的欧几里得投影：\n$$\n\\text{prox}_{\\gamma g}(\\mathbf{z})=\\mathop{\\arg\\min}_{\\mathbf{y}\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(\\mathbf{y})+\\frac{1}{2\\gamma}\\|\\mathbf{y}-\\mathbf{z}\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(\\mathbf{z}),\n$$\n它是逐分量在零处的截断：\n$$\nP_{\\mathbb{R}_{+}^{2}}(\\mathbf{z})=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\n给定 $\\mathbf{a}=(5,-4)$，$\\mathbf{x}_{0}=(1,1)$ 和 $\\gamma=0.2$，计算在 $\\mathbf{x}_{0}$ 处的梯度：\n$$\n\\nabla f(\\mathbf{x}_{0})=\\mathbf{x}_{0}-\\mathbf{a}=(1,1)-(5,-4)=(-4,5).\n$$\n执行梯度步：\n$$\n\\mathbf{x}_{0}-\\gamma \\nabla f(\\mathbf{x}_{0})=(1,1)-0.2(-4,5)=(1+0.8, 1-1)=(1.8, 0).\n$$\n应用邻近映射，即到 $\\mathbb{R}_{+}^{2}$ 的投影：\n$$\n\\mathbf{x}_{1}=\\text{prox}_{\\gamma g}\\big(\\mathbf{x}_{0}-\\gamma \\nabla f(\\mathbf{x}_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}(1.8,0)=(1.8, 0),\n$$\n因为两个分量都已是非负的。由于 $1.8 = 9/5$，我们可以写成 $\\mathbf{x}_1 = (9/5, 0)$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5} & 0 \\end{pmatrix}}$$", "id": "2195110"}, {"introduction": "从单步迭代到完整的算法，我们现在将近端梯度方法应用于解决一个在统计学和机器学习中至关重要的问题——LASSO。LASSO 通过引入 $L_1$ 范数正则化，能够实现特征选择，而近端梯度方法是求解它的经典工具。这个练习将模拟一个完整的算法流程：你将从头开始，通过多次迭代求解一个具体的LASSO问题，并应用一个实际的停止准则来终止算法，从而体验从理论到实践的全过程。[@problem_id:2195138]", "problem": "考虑一个名为 LASSO (Least Absolute Shrinkage and Selection Operator) 的优化问题，其目标是找到一个向量 $\\mathbf{x} \\in \\mathbb{R}^n$ 来最小化目标函数 $F(\\mathbf{x}) = f(\\mathbf{x}) + g(\\mathbf{x})$。该函数由一个光滑部分 $f(\\mathbf{x}) = \\frac{1}{2} \\|A\\mathbf{x} - \\mathbf{b}\\|_2^2$ 和一个非光滑正则化部分 $g(\\mathbf{x}) = \\lambda \\|\\mathbf{x}\\|_1$ 组成，其中 $\\lambda > 0$ 是一个正则化参数。\n\n这个问题可以使用近端梯度法来求解，该方法根据以下更新规则生成一系列迭代点 $\\{\\mathbf{x}_k\\}$：\n$$\\mathbf{x}_{k+1} = \\text{prox}_{\\alpha g}(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k))$$\n其中 $\\alpha > 0$ 是一个常数步长。对于 $g(\\mathbf{x}) = \\lambda \\|\\mathbf{x}\\|_1$，其近端算子是软阈值算子 $\\text{prox}_{\\alpha g}(\\mathbf{v}) = S_{\\alpha\\lambda}(\\mathbf{v})$，它对向量 $\\mathbf{v}$ 的作用是分量级别的：\n$$[S_{c}(\\mathbf{v})]_i = \\text{sign}(v_i) \\max(|v_i| - c, 0)$$\n其中 $c$ 为一个正常数。\n\n你的任务是针对一个特定配置来模拟这个算法。设向量 $\\mathbf{x} \\in \\mathbb{R}^2$。系统参数由下式给出：\n$$A = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}, \\quad \\lambda = 4$$\n算法从初始点 $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，并使用步长 $\\alpha = 0.25$。\n\n一个常见的实用停止准则是，当连续迭代点之间的变化足够小时，终止算法。如果条件 $\\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\|_2 < \\epsilon$ 在完成第 $k+1$ 次迭代后首次满足，则称算法已收敛并终止，其中 $\\epsilon$ 是一个预定义的容差。对于这个问题，使用容差 $\\epsilon = 1.0$。\n\n确定在算法终止前执行的总迭代次数。", "solution": "该问题要求计算近端梯度法在给定的停止准则下收敛所需的迭代次数。目标函数为 $F(\\mathbf{x}) = \\frac{1}{2} \\|A\\mathbf{x} - \\mathbf{b}\\|_2^2 + \\lambda \\|\\mathbf{x}\\|_1$。\n\n首先，我们需要求出光滑部分 $f(\\mathbf{x}) = \\frac{1}{2} \\|A\\mathbf{x} - \\mathbf{b}\\|_2^2$ 的梯度。\n函数 $f(\\mathbf{x})$ 可以写成 $f(\\mathbf{x}) = \\frac{1}{2} (A\\mathbf{x} - \\mathbf{b})^T (A\\mathbf{x} - \\mathbf{b}) = \\frac{1}{2} (\\mathbf{x}^T A^T - \\mathbf{b}^T)(A\\mathbf{x} - \\mathbf{b}) = \\frac{1}{2} (\\mathbf{x}^T A^T A \\mathbf{x} - \\mathbf{x}^T A^T \\mathbf{b} - \\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b})$。\n关于 $\\mathbf{x}$ 的梯度是 $\\nabla f(\\mathbf{x}) = \\frac{1}{2} (2 A^T A \\mathbf{x} - A^T \\mathbf{b} - ( \\mathbf{b}^T A)^T) = A^T A \\mathbf{x} - A^T \\mathbf{b} = A^T(A\\mathbf{x} - \\mathbf{b})$。\n\n我们来计算矩阵 $A^T A$：\n$$A^T A = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 1 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + (-1) \\cdot 1 & 1 \\cdot 1 + (-1) \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = 2I$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n我们再计算向量 $A^T \\mathbf{b}$：\n$$A^T \\mathbf{b} = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 6 + 1 \\cdot 2 \\\\ 1 \\cdot 6 - 1 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$$\n所以，梯度为 $\\nabla f(\\mathbf{x}) = 2I\\mathbf{x} - A^T \\mathbf{b} = 2\\mathbf{x} - \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$。\n\n近端梯度迭代为 $\\mathbf{x}_{k+1} = \\text{prox}_{\\alpha g}(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k))$。\n近端算子的参数是 $\\mathbf{v}_k = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k) = \\mathbf{x}_k - \\alpha \\left(2\\mathbf{x}_k - \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}\\right) = (1 - 2\\alpha)\\mathbf{x}_k + \\alpha \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$。\n更新为 $\\mathbf{x}_{k+1} = S_{\\alpha\\lambda}(\\mathbf{v}_k)$。\n\n我们已知参数 $\\alpha = 0.25$ 和 $\\lambda = 4$。\n软阈值算子的常数是 $c = \\alpha \\lambda = 0.25 \\times 4 = 1$。\n$\\mathbf{v}_k$ 的表达式变为：\n$\\mathbf{v}_k = (1 - 2(0.25))\\mathbf{x}_k + 0.25 \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix} = 0.5 \\mathbf{x}_k + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n因此迭代规则为：\n$\\mathbf{x}_{k+1} = S_1\\left(0.5 \\mathbf{x}_k + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\right)$。\n\n我们从 $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，停止容差为 $\\epsilon = 1.0$。\n\n**第一次迭代 (k=0):**\n我们从 $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始。\n首先，计算 $\\mathbf{v}_0$：\n$$\\mathbf{v}_0 = 0.5 \\mathbf{x}_0 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\n接着，应用软阈值算子来求得 $\\mathbf{x}_1$：\n$$\\mathbf{x}_1 = S_1(\\mathbf{v}_0) = \\begin{pmatrix} S_1(2) \\\\ S_1(1) \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(2)\\max(|2|-1,0) \\\\ \\text{sign}(1)\\max(|1|-1,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\max(1,0) \\\\ 1 \\cdot \\max(0,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n现在，检查停止准则：$\\|\\mathbf{x}_1 - \\mathbf{x}_0\\|_2 < \\epsilon$。\n$$\\|\\mathbf{x}_1 - \\mathbf{x}_0\\|_2 = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{1^2 + 0^2} = 1$$\n条件是 $1 < 1.0$，这不成立。算法不终止。我们继续下一次迭代。\n\n**第二次迭代 (k=1):**\n我们从 $\\mathbf{x}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 开始。\n首先，计算 $\\mathbf{v}_1$：\n$$\\mathbf{v}_1 = 0.5 \\mathbf{x}_1 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2.5 \\\\ 1 \\end{pmatrix}$$\n接着，应用软阈值算子来求得 $\\mathbf{x}_2$：\n$$\\mathbf{x}_2 = S_1(\\mathbf{v}_1) = \\begin{pmatrix} S_1(2.5) \\\\ S_1(1) \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(2.5)\\max(|2.5|-1,0) \\\\ \\text{sign}(1)\\max(|1|-1,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\max(1.5,0) \\\\ 1 \\cdot \\max(0,0) \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix}$$\n现在，检查停止准则：$\\|\\mathbf{x}_2 - \\mathbf{x}_1\\|_2 < \\epsilon$。\n$$\\|\\mathbf{x}_2 - \\mathbf{x}_1\\|_2 = \\left\\| \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} 0.5 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{0.5^2 + 0^2} = 0.5$$\n条件是 $0.5 < 1.0$，这成立。算法终止。\n\n第一次迭代 (k=0) 产生了 $\\mathbf{x}_1$。第二次迭代 (k=1) 产生了 $\\mathbf{x}_2$。在第二次迭代完成后，停止条件得到满足。因此，执行的总迭代次数为 2。", "answer": "$$\\boxed{2}$$", "id": "2195138"}]}