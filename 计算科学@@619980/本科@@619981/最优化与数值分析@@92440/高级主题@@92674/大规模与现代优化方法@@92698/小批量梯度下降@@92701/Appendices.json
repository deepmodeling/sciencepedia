{"hands_on_practices": [{"introduction": "第一个练习将带你实践梯度下降中最核心的步骤：参数更新。在这个问题中，我们已经为你计算好了梯度，这样你就可以专注于理解学习率和梯度如何共同作用来调整模型参数。掌握这个基本机制是进行任何优化算法实践的第一步。[@problem_id:2187026]", "problem": "一名计算科学专业的学生正在训练一个简单的预测模型。该模型的行为由一个无量纲参数 $\\theta$ 控制。目标是找到使模型的预测误差最小化的 $\\theta$ 值，该误差由成本函数 $J(\\theta)$ 来衡量。\n\n该学生采用了一种称为梯度下降的数值优化技术。该过程从参数的初始猜测值开始，并迭代地对其进行优化。每一步更新都旨在将参数值沿成本函数梯度的相反方向移动。每一步的大小由一个称为学习率的参数决定。\n\n该学生将参数初始化为 $\\theta_0 = 2$。对于第一次迭代，他们使用一部分数据来计算成本函数在该初始点上关于参数的梯度。计算得到的梯度值为 $\\nabla J(\\theta_0) = 4$。优化过程的学习率设置为 $\\eta = 0.01$。\n\n计算第一次迭代后参数更新后的值 $\\theta_1$。将答案保留三位有效数字。", "solution": "我们使用单个参数的标准梯度下降更新规则：\n$$\n\\theta_{k+1} = \\theta_{k} - \\eta \\frac{dJ}{d\\theta}\\bigg|_{\\theta=\\theta_{k}}.\n$$\n对于 $k=0$ 的第一次迭代，给定值为 $\\theta_{0} = 2$，学习率 $\\eta = 0.01$，梯度 $\\nabla J(\\theta_{0}) = 4$。将这些值代入更新规则可得：\n$$\n\\theta_{1} = \\theta_{0} - \\eta \\nabla J(\\theta_{0}) = 2 - 0.01 \\times 4.\n$$\n计算乘积：\n$$\n0.01 \\times 4 = 0.04,\n$$\n所以\n$$\n\\theta_{1} = 2 - 0.04 = 1.96.\n$$\n保留三位有效数字，该值为 $1.96$。", "answer": "$$\\boxed{1.96}$$", "id": "2187026"}, {"introduction": "在上一个练习的基础上，这个实践增加了计算梯度的步骤。你需要根据给定的成本函数和一小批数据，亲手计算出梯度值，然后完成参数更新。这个过程将抽象的梯度概念与具体的计算联系起来，模拟了一个更完整、更真实的简单模型训练步骤。[@problem_id:2187016]", "problem": "一位工程师正在开发一款简单的智能恒温器。该恒温器的目标是学习一个理想的室温。该恒温器有一个内部参数 $w$，代表其当前的温度设置，单位为摄氏度。该恒温器的设置模型很简单：输出的设置值就是参数 $w$ 的值，与任何外部传感器读数无关。\n\n为了训练该参数，工程师使用了一个成本函数 $J(w; y) = (w - y)^2$，其中 $y$ 是用户提供的目标温度，单位为摄氏度。参数 $w$ 使用小批量梯度下降法进行更新。\n\n恒温器的初始设置为 $w_0 = 5.0$ 摄氏度。更新算法的学习率设置为 $\\eta = 0.1$。在第一个训练步骤中，使用了一个仅包含单个数据点的小批量。该数据点对应于用户期望的目标温度 $y = 10.0$ 摄氏度。\n\n计算经过这单次更新步骤后，恒温器参数 $w_1$ 的值。请以摄氏度为单位表示你的答案。", "solution": "成本函数为 $J(w; y) = (w - y)^{2}$。其关于 $w$ 的梯度通过求导得到：\n$$\n\\frac{\\partial J}{\\partial w} = 2(w - y).\n$$\n使用学习率为 $\\eta$ 的小批量梯度下降法，更新规则为\n$$\nw_{1} = w_{0} - \\eta \\left.\\frac{\\partial J}{\\partial w}\\right|_{w=w_{0}}.\n$$\n代入 $w_{0} = 5.0$，$y = 10.0$ 和 $\\eta = 0.1$：\n$$\n\\left.\\frac{\\partial J}{\\partial w}\\right|_{w=w_{0}} = 2(5.0 - 10.0) = 2(-5.0) = -10.0,\n$$\n$$\nw_{1} = 5.0 - 0.1 \\times (-10.0) = 5.0 + 1.0 = 6.\n$$\n因此，经过一次更新步骤后，恒温器参数为 $6$ 摄氏度。", "answer": "$$\\boxed{6}$$", "id": "2187016"}, {"introduction": "最后的这个练习将引导你从计算转向概念的深度思考。通过一个精心设计的思想实验，它挑战你反思小批量梯度的根本性质。理解这个特殊的“无噪声”场景将帮助你阐明为什么小批量梯度下降通常是“随机的”，以及它与全批量梯度下降的本质联系。[@problem_id:2187032]", "problem": "考虑一个机器学习任务，其目标是找到一组模型参数（表示为向量 $w$），以最小化总损失函数 $L(w)$。总损失定义为包含 $N$ 个数据点的数据集上的平均损失：$L(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell_i(w)$，其中 $\\ell_i(w)$ 是与第 $i$ 个数据点相关的损失。\n\n我们将使用小批量梯度下降 (GD) 来执行此优化。在该算法的每一步中，从完整数据集中随机选择一个包含 $b$ 个数据点的小批量（mini-batch），其中 $1 < b < N$。损失的梯度仅基于这个小批量进行计算，然后更新参数。\n\n现在，考虑一个非常特殊且不寻常的数据集，其中每个数据点都完全相同。也就是说，数据集中所有 $N$ 个点的特征和对应的标签都完全相同。假设在初始参数值 $w_0$ 处，对于这个唯一的单个数据点，其损失的梯度不为零。\n\n鉴于此场景，假设小批量 GD 算法和标准的全批量 GD 都从相同的初始参数 $w_0$ 开始，并使用相同的学习率 $\\eta$，以下哪个陈述最准确地描述了小批量 GD 算法相对于全批量 GD 的行为？\n\nA. 小批量 GD 将比全批量 GD 显著更快地收敛到最小值，因为其每步的计算量更轻，同时遵循相似的优化路径。\n\nB. 为任何小批量计算出的梯度都将为零，导致学习过程立即停滞。\n\nC. 小批量 GD 将遵循与全批量 GD 完全相同的优化轨迹（即，生成相同的参数向量序列）。\n\nD. 来自不同小批量的梯度估计的方差将被最大化，导致一个高度不规则且不稳定的优化路径。\n\nE. 不同的小批量将产生不同的梯度估计，导致参数沿着一条随机路径朝最小值移动，这是小批量 GD 的标准行为。", "solution": "将单个样本的损失定义为 $\\ell_{i}(w)$（其中 $i \\in \\{1,\\dots,N\\}$），并将经验风险定义为\n$$\nL(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell_{i}(w).\n$$\n在给定的数据集中，每个数据点都相同，因此存在一个单一的函数 $\\ell(w)$，使得对于所有的 $i$ 都有 $\\ell_{i}(w) = \\ell(w)$。因此，\n$$\nL(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(w) = \\ell(w).\n$$\n求梯度，\n$$\n\\nabla L(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla \\ell_{i}(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla \\ell(w) = \\nabla \\ell(w).\n$$\n学习率为 $\\eta$、初始值为 $w_{0}$ 的全批量梯度下降 (GD) 执行如下更新\n$$\nw_{t+1}^{\\mathrm{FB}} = w_{t}^{\\mathrm{FB}} - \\eta \\nabla L\\!\\left(w_{t}^{\\mathrm{FB}}\\right) = w_{t}^{\\mathrm{FB}} - \\eta \\nabla \\ell\\!\\left(w_{t}^{\\mathrm{FB}}\\right).\n$$\n批量大小为 $b$ 的小批量梯度下降选择索引 $\\{i_{1},\\dots,i_{b}\\}$ 并使用小批量梯度\n$$\ng_{t}^{\\mathrm{MB}}(w) = \\frac{1}{b} \\sum_{j=1}^{b} \\nabla \\ell_{i_{j}}(w).\n$$\n因为所有样本都相同，所以对于所有的 $j$ 都有 $\\nabla \\ell_{i_{j}}(w) = \\nabla \\ell(w)$，因此\n$$\ng_{t}^{\\mathrm{MB}}(w) = \\frac{1}{b} \\sum_{j=1}^{b} \\nabla \\ell(w) = \\nabla \\ell(w) = \\nabla L(w).\n$$\n因此小批量的更新为\n$$\nw_{t+1}^{\\mathrm{MB}} = w_{t}^{\\mathrm{MB}} - \\eta\\, g_{t}^{\\mathrm{MB}}\\!\\left(w_{t}^{\\mathrm{MB}}\\right) = w_{t}^{\\mathrm{MB}} - \\eta \\nabla \\ell\\!\\left(w_{t}^{\\mathrm{MB}}\\right).\n$$\n比较全批量和小批量的更新可以看出，在从相同的 $w_{0}$ 初始化并使用相同的 $\\eta$ 的情况下，它们在每次迭代中的定义是完全相同的。对 $t$ 使用归纳法，可知对于所有的 $t$ 都有 $w_{t}^{\\mathrm{MB}} = w_{t}^{\\mathrm{FB}}$。$\\nabla \\ell(w_{0}) \\neq 0$ 的假设确保了第一步是非平凡的，但并不改变更新规则的等价性。\n\n因此：\n- 小批量梯度在不同批次间的方差为零，因此没有随机性（排除了 D 和 E）。\n- 在 $w_{0}$ 处的梯度不为零，所以更新不会停滞（排除了 B）。\n- 尽管小批量的每步计算量比全批量要轻，但优化轨迹和步数是完全相同的；因此，关于其行为最准确的陈述是两种方法遵循完全相同的参数序列（排除了 A 的陈述）。\n\n因此，正确的选项是小批量 GD 遵循与全批量 GD 完全相同的优化轨迹。", "answer": "$$\\boxed{C}$$", "id": "2187032"}]}