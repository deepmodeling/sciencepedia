{"hands_on_practices": [{"introduction": "Adam 优化器的核心在于它能够为每个参数计算自适应的学习率，而这是通过追踪梯度的“一阶矩”（均值）和“二阶矩”（非中心方差）的估计值来实现的。这个练习将带你亲手实践 Adam 算法最基础的计算步骤：根据给定的梯度更新一阶矩 $m_t$ 和二阶矩 $v_t$ 的估计值。掌握这一步是理解整个优化器工作原理的关键。[@problem_id:2152288]", "problem": "在机器学习领域，优化器是用于调整模型参数以最小化损失函数的算法。Adam（自适应矩估计）优化器是一种常用的选择，它通过对梯度的一阶矩和二阶矩的估计来为每个参数计算自适应学习率。\n\n在时间步 $t$，一阶矩估计（梯度的移动平均值，$m$）和二阶矩估计（梯度平方的移动平均值，$v$）的更新规则如下：\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n其中，$g_t$ 是在步骤 $t$ 时损失函数关于模型参数的梯度，而平方运算 $g_t^2$ 是逐元素进行的。超参数 $\\beta_1$ 和 $\\beta_2$ 是移动平均值的衰减率。\n\n考虑一个简单的双参数模型。在优化过程的开始（$t=1$），初始矩估计被初始化为零向量，即 $m_0 = [0, 0]^T$ 和 $v_0 = [0, 0]^T$。超参数被设置为其常用值：$\\beta_1 = 0.9$ 和 $\\beta_2 = 0.999$。在第一个训练步骤中，计算出的损失函数梯度为 $g_1 = [2.0, -4.0]^T$。\n\n计算更新后的一阶矩向量 $m_1 = [m_{1,1}, m_{1,2}]^T$ 和更新后的二阶矩向量 $v_1 = [v_{1,1}, v_{1,2}]^T$。你的最终答案应该是一个行矩阵，按特定顺序 $[m_{1,1}, m_{1,2}, v_{1,1}, v_{1,2}]$ 包含这四个数值分量。", "solution": "我们在步骤 $t=1$ 使用 Adam 矩更新方程：\n$$m_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2},$$\n其中 $m_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，$v_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，$\\beta_{1}=0.9$，$\\beta_{2}=0.999$，以及 $g_{1}=\\begin{bmatrix}2.0 \\\\ -4.0\\end{bmatrix}$。平方 $g_{1}^{2}$ 是逐元素计算的。\n\n一阶矩：\n由于 $m_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，项 $\\beta_{1}m_{0}$ 为 $\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$。因此，\n$$m_{1}=(1-\\beta_{1})g_{1}=0.1\\begin{bmatrix}2.0 \\\\ -4.0\\end{bmatrix}=\\begin{bmatrix}0.2 \\\\ -0.4\\end{bmatrix}.$$\n所以，$m_{1,1}=0.2$ 且 $m_{1,2}=-0.4$。\n\n二阶矩：\n计算逐元素的平方 $g_{1}^{2}$：\n$$g_{1}^{2}=\\begin{bmatrix}(2.0)^{2} \\\\ (-4.0)^{2}\\end{bmatrix}=\\begin{bmatrix}4.0 \\\\ 16.0\\end{bmatrix}.$$\n由于 $v_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，项 $\\beta_{2}v_{0}$ 为 $\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$。因此，\n$$v_{1}=(1-\\beta_{2})g_{1}^{2}=0.001\\begin{bmatrix}4.0 \\\\ 16.0\\end{bmatrix}=\\begin{bmatrix}0.004 \\\\ 0.016\\end{bmatrix}.$$\n所以，$v_{1,1}=0.004$ 且 $v_{1,2}=0.016$。\n\n按顺序 $[m_{1,1}, m_{1,2}, v_{1,1}, v_{1,2}]$ 所要求的行矩阵是 $\\begin{pmatrix}0.2 & -0.4 & 0.004 & 0.016\\end{pmatrix}$。", "answer": "$$\\boxed{\\begin{pmatrix}0.2 & -0.4 & 0.004 & 0.016\\end{pmatrix}}$$", "id": "2152288"}, {"introduction": "在上一个练习中，我们计算了 Adam 的核心组成部分：矩估计。现在，我们将把所有部分整合起来，完成一个完整的参数更新步骤。这个练习将引导你从计算梯度开始，到更新并修正矩估计，最终应用 Adam 的更新规则来调整参数。通过亲自走完这一个完整的迭代流程，你将对 Adam 算法的运作机制有一个全面的、动手实践层面的认识。[@problem_id:2152250]", "problem": "在数值优化领域，Adam 算法是一种被广泛用于寻找函数最小值的优化方法。考虑一维代价函数 $f(x) = 5x^2$。我们希望从初始值 $x_0 = 2$ 开始，找到使该函数最小化的 $x$ 值。\n\n使用 Adam 算法，计算经过一次更新步骤后的参数值，记为 $x_1$。该算法配置了以下超参数：\n- 学习率 (Learning rate)，$\\alpha = 0.1$\n- 一阶矩估计的指数衰减率 (Exponential decay rate for the first moment estimate)，$\\beta_1 = 0.9$\n- 二阶矩估计的指数衰减率 (Exponential decay rate for the second moment estimate)，$\\beta_2 = 0.999$\n- 用于数值稳定性的小常数 (A small constant for numerical stability)，$\\epsilon = 10^{-8}$\n\n初始的一阶矩估计 $m_0$ 和二阶矩估计 $v_0$ 均初始化为零。\n\n将您的最终答案四舍五入到五位有效数字。", "solution": "我们使用 Adam 算法从 $x_{0}=2$ 开始，最小化 $f(x)=5x^{2}$。梯度为 $\\nabla f(x)=10x$。在第一步 ($t=1$) 时，在 $x_{0}$ 处计算的梯度为\n$$\ng_{1}=\\nabla f(x_{0})=10x_{0}=10\\cdot 2=20.\n$$\nAdam 的矩更新公式为\n$$\nm_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2}.\n$$\n已知 $m_{0}=0$、$v_{0}=0$、$\\beta_{1}=0.9$ 和 $\\beta_{2}=0.999$，\n$$\nm_{1}=0.9\\cdot 0+(1-0.9)\\cdot 20=2, \\quad v_{1}=0.999\\cdot 0+(1-0.999)\\cdot 20^{2}=0.001\\cdot 400=0.4.\n$$\n偏差校正后的估计值为\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{2}{1-0.9}=\\frac{2}{0.1}=20, \\quad \\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{0.4}{1-0.999}=\\frac{0.4}{0.001}=400.\n$$\nAdam 的更新步骤为\n$$\nx_{1}=x_{0}-\\alpha\\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}=2-0.1\\cdot \\frac{20}{\\sqrt{400}+10^{-8}}=2-0.1\\cdot \\frac{20}{20+10^{-8}}=2-\\frac{2}{20+10^{-8}}.\n$$\n计算该分式，\n$$\n\\frac{2}{20+10^{-8}} \\approx 0.1-5\\times 10^{-11},\n$$\n所以\n$$\nx_{1}\\approx 2-\\left(0.1-5\\times 10^{-11}\\right)=1.90000000005.\n$$\n四舍五入到五位有效数字，结果是 $1.9000$。", "answer": "$$\\boxed{1.9000}$$", "id": "2152250"}, {"introduction": "掌握了 Adam 的基本计算后，让我们来探讨一个更有趣的场景，以加深对算法动态行为的理解。本练习通过一个思想实验，分析 Adam 在优化一个带有“尖点”的函数 $f(x) = |x|$ 时的表现。你将观察到，由于动量（momentum）的存在，优化器在接近最小值时可能会“冲过头”，然后进行修正。这个过程清晰地揭示了 Adam 的自适应特性，并帮助你建立关于优化器在复杂优化地形中如何导航的直观感觉。[@problem_id:2152240]", "problem": "一个名为Adam（自适应矩估计的简称）的优化算法正用于寻找一维目标函数 $f(x) = |x|$ 的最小值。对于任何非零的 $x$，该函数的梯度为 $g(x) = \\text{sign}(x)$。\n\nAdam 更新规则以离散时间步 $t=1, 2, 3, \\ldots$ 进行。在每一步中，给定当前参数值 $x_{t-1}$，算法会更新梯度的一阶矩（均值）估计值 $m_t$ 和二阶矩（非中心方差）估计值 $v_t$。然后，它使用这些经过偏差校正的估计值来更新参数 $x_t$。完整的更新方程组如下：\n1.  计算梯度：$g_t = \\nabla f(x_{t-1})$\n2.  更新一阶矩估计：$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n3.  更新二阶矩估计：$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n4.  计算偏差校正后的一阶矩：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n5.  计算偏差校正后的二阶矩：$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n6.  更新参数：$x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n\n该算法的超参数包括学习率 $\\alpha$、矩估计的指数衰减率 $\\beta_1$ 和 $\\beta_2$，以及一个为防止除以零而设的小稳定化常数 $\\epsilon$。所有超参数均为正实数，且 $\\beta_1, \\beta_2 \\in (0, 1)$。\n\n考虑一个优化过程，该过程从时间 $t=0$ 开始，初始参数值为 $x_0 = \\delta$，其中 $\\delta$ 是一个很小的正常数。初始矩估计为 $m_0 = 0$ 和 $v_0 = 0$。$\\delta$ 的值被选取得足够小，使得第一步更新导致 $x_1 < 0$，从而使迭代值穿过原点。\n\n要求是确定下一步的参数值 $x_2$。将答案表示为关于 $\\delta, \\alpha, \\beta_1, \\beta_2$ 和 $\\epsilon$ 的单个闭式解析表达式。", "solution": "我们严格按照所述的 Adam 更新规则进行计算。对于一维函数 $f(x)=|x|$，当 $x_{t-1}\\neq 0$ 时，其梯度为 $g_{t}=\\text{sign}(x_{t-1})$ 且 $g_{t}^{2}=1$。\n\n在 $t=1$ 时，有 $x_{0}=\\delta>0$，因此 $g_{1}=\\text{sign}(x_{0})=1$。使用 $m_{0}=0$ 和 $v_{0}=0$ 可得：\n$$\nm_{1}=\\beta_{1} m_{0}+(1-\\beta_{1})g_{1}=(1-\\beta_{1}),\n$$\n$$\nv_{1}=\\beta_{2} v_{0}+(1-\\beta_{2}) g_{1}^{2}=(1-\\beta_{2}).\n$$\n偏差校正后可得：\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{1-\\beta_{1}}{1-\\beta_{1}}=1,\\qquad\n\\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{1-\\beta_{2}}{1-\\beta_{2}}=1.\n$$\n因此参数更新为：\n$$\nx_{1}=x_{0}-\\alpha \\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}\n=\\delta-\\alpha\\frac{1}{1+\\epsilon}.\n$$\n根据假设，$\\delta$ 足够小以至于 $x_{1}<0$，所以 $g_{2}=\\text{sign}(x_{1})=-1$。\n\n在 $t=2$ 时，计算矩：\n$$\nm_{2}=\\beta_{1} m_{1}+(1-\\beta_{1}) g_{2}=\\beta_{1}(1-\\beta_{1})-(1-\\beta_{1})\n=(1-\\beta_{1})(\\beta_{1}-1)=-(1-\\beta_{1})^{2},\n$$\n$$\nv_{2}=\\beta_{2} v_{1}+(1-\\beta_{2}) g_{2}^{2}\n=\\beta_{2}(1-\\beta_{2})+(1-\\beta_{2})=(1-\\beta_{2})(1+\\beta_{2})=1-\\beta_{2}^{2}.\n$$\n偏差校正后得到：\n$$\n\\hat{m}_{2}=\\frac{m_{2}}{1-\\beta_{1}^{2}}\n=\\frac{-(1-\\beta_{1})^{2}}{(1-\\beta_{1})(1+\\beta_{1})}\n=-\\frac{1-\\beta_{1}}{1+\\beta_{1}},\n\\qquad\n\\hat{v}_{2}=\\frac{v_{2}}{1-\\beta_{2}^{2}}=1.\n$$\n因此，\n$$\nx_{2}=x_{1}-\\alpha \\frac{\\hat{m}_{2}}{\\sqrt{\\hat{v}_{2}}+\\epsilon}\n=x_{1}-\\alpha \\frac{-\\frac{1-\\beta_{1}}{1+\\beta_{1}}}{1+\\epsilon}\n=x_{1}+\\alpha \\frac{1-\\beta_{1}}{(1+\\beta_{1})(1+\\epsilon)}.\n$$\n将 $x_{1}=\\delta-\\alpha/(1+\\epsilon)$ 代入，可得：\n$$\nx_{2}=\\delta-\\frac{\\alpha}{1+\\epsilon}+\\frac{\\alpha(1-\\beta_{1})}{(1+\\beta_{1})(1+\\epsilon)}\n=\\delta+\\frac{\\alpha}{1+\\epsilon}\\left(-1+\\frac{1-\\beta_{1}}{1+\\beta_{1}}\\right)\n=\\delta-\\frac{2\\alpha \\beta_{1}}{(1+\\beta_{1})(1+\\epsilon)}.\n$$\n这就是所求的闭式表达式。值得注意的是，由于在 $g_{1}^{2}=g_{2}^{2}=1$ 的条件下有 $\\hat{v}_{1}=\\hat{v}_{2}=1$，因此到第二步为止，该表达式不依赖于 $\\beta_{2}$。", "answer": "$$\\boxed{\\delta-\\frac{2\\alpha \\beta_{1}}{(1+\\beta_{1})(1+\\epsilon)}}$$", "id": "2152240"}]}