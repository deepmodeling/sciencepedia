{"hands_on_practices": [{"introduction": "在数据分析中，特征之间的共线性（或线性相关性）可能会导致模型不稳定等问题。奇异值分解（SVD）为我们提供了一个精确的数学工具来诊断这种冗余。这项练习将向你展示，中心化数据矩阵的一个奇异值为零如何直接表明特征之间存在完美的线性依赖关系 [@problem_id:2154133]。", "problem": "一位数据科学家正在分析一个数据集，其中包含四个不同样本的两个属性（特征A和特征B）的测量值。数据集如下：\n\n- 样本 1：特征 A = 3，特征 B = 3\n- 样本 2：特征 A = 5，特征 B = 7\n- 样本 3：特征 A = 2，特征 B = 1\n- 样本 4：特征 A = 6，特征 B = 9\n\n为了分析数据的结构，该科学家决定研究其奇异值。首先，创建一个大小为 $4 \\times 2$ 的中心化数据矩阵 $X$，其中每列对应一个特征，每行对应一个样本。中心化是通过从每个特征（列）的所有值中减去该特征列的均值来执行的。\n\n矩阵的奇异值，记为 $\\sigma_i$，是数据沿其主轴方差的度量，定义为矩阵 $X^T X$ 的特征值的非负平方根。奇异值为零对于矩阵列之间的关系有特定的含义。\n\n在计算中心化数据矩阵 $X$ 的奇异值后，确定下列哪个陈述准确描述了特征A和特征B之间的关系。\n\nA. 特征是线性无关的。\n\nB. 特征是完全共线的。\n\nC. 特征表现出强但非完美的正相关性。\n\nD. 特征表现出强但非完美的负相关性。\n\nE. 数据不足以确定关系。", "solution": "设未中心化的数据矩阵为，其中行是样本，列是特征：\n$$\nX_{\\text{raw}}=\\begin{pmatrix}\n3 & 3\\\\\n5 & 7\\\\\n2 & 1\\\\\n6 & 9\n\\end{pmatrix}.\n$$\n计算列均值\n$$\n\\mu_{A}=\\frac{3+5+2+6}{4}=4,\\qquad \\mu_{B}=\\frac{3+7+1+9}{4}=5.\n$$\n通过减去其均值来中心化每一列，得到中心化的列\n$$\na=\\begin{pmatrix}3-4\\\\5-4\\\\2-4\\\\6-4\\end{pmatrix}=\\begin{pmatrix}-1\\\\1\\\\-2\\\\2\\end{pmatrix},\\qquad\nb=\\begin{pmatrix}3-5\\\\7-5\\\\1-5\\\\9-5\\end{pmatrix}=\\begin{pmatrix}-2\\\\2\\\\-4\\\\4\\end{pmatrix}.\n$$\n观察到\n$$\nb=2a,\n$$\n因此中心化的列是完全共线的。中心化数据矩阵为\n$$\nX=\\begin{pmatrix}\n-1 & -2\\\\\n\\phantom{-}1 & \\phantom{-}2\\\\\n-2 & -4\\\\\n\\phantom{-}2 & \\phantom{-}4\n\\end{pmatrix}.\n$$\n计算\n$$\nX^{T}X=\\begin{pmatrix}\na^{T}a & a^{T}b\\\\\nb^{T}a & b^{T}b\n\\end{pmatrix}\n=\\begin{pmatrix}\n10 & 20\\\\\n20 & 40\n\\end{pmatrix},\n$$\n因为\n$$\na^{T}a=(-1)^{2}+1^{2}+(-2)^{2}+2^{2}=10,\\quad b^{T}b=(-2)^{2}+2^{2}+(-4)^{2}+4^{2}=40,\\quad a^{T}b=(-1)(-2)+1\\cdot 2+(-2)(-4)+2\\cdot 4=20.\n$$\n$X^{T}X$ 的特征值是以下方程的解\n$$\n\\det\\begin{pmatrix}10-\\lambda & 20 \\\\ 20 & 40-\\lambda\\end{pmatrix}=(10-\\lambda)(40-\\lambda)-20^{2}=0,\n$$\n化简为\n$$\n\\lambda^{2}-50\\lambda=0 \\quad\\Rightarrow\\quad \\lambda\\in\\{0,50\\}.\n$$\n因此奇异值为\n$$\n\\sigma_{1}=\\sqrt{50},\\qquad \\sigma_{2}=0.\n$$\n奇异值为零意味着 $X$ 的列是线性相关的，即特征是完全共线的。因此，正确的陈述是选项 B。", "answer": "$$\\boxed{B}$$", "id": "2154133"}, {"introduction": "SVD最引人入胜的应用之一是在推荐系统中预测用户评分。其核心思想是，数据中的主要模式（例如，用户的潜在品味）可以通过少数几个奇异值和对应的奇异向量来捕捉。本练习将指导你如何使用SVD中最重要的部分——最佳一阶近似（best rank-1 approximation）——来填充数据矩阵中的缺失值，从而预测未知评分 [@problem_id:2154142]。", "problem": "在数据分析领域，尤其是在推荐系统中，一个常见的任务是预测用户对他们尚未评价过的项目的潜在评分。这可以通过填充用户-项目矩阵中的缺失条目来建模。\n\n考虑一个小的用户-项目评分矩阵 $R$，它表示两个用户（行）对三个不同项目（列）的评分。其中一个评分是未知的，用问号表示。\n\n$$\nR = \\begin{pmatrix}\n1 & 1 & ? \\\\\n0 & 1 & 1\n\\end{pmatrix}\n$$\n\n一种估算缺失评分的标准方法是利用已知数据的主导结构。步骤如下：\n1.  创建一个新矩阵 $A$，方法是取矩阵 $R$ 并将缺失条目 “?” 替换为值 0。\n2.  确定矩阵 $A$ 的奇异值分解（SVD）。\n3.  仅使用SVD的最重要分量来构建预测模型。该模型被称为 $A$ 的最佳秩一近似。\n\n按照此步骤，确定缺失评分的预测数值。将您的答案表示为四舍五入到三位有效数字的实数。", "solution": "将缺失条目设置为零的已知条目矩阵为\n$$\nA=\\begin{pmatrix}\n1 & 1 & 0\\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\n$A$ 的SVD最佳秩一近似是 $\\sigma_{1} u_{1} v_{1}^{T}$，其中 $\\sigma_{1}$ 是最大的奇异值，$u_{1},v_{1}$ 是相应的左、右奇异向量。\n\n计算 $A A^{T}$：\n$$\nA A^{T}=\n\\begin{pmatrix}\n1 & 1 & 0\\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0\\\\\n1 & 1\\\\\n0 & 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 & 1\\\\\n1 & 2\n\\end{pmatrix}.\n$$\n从下式求其特征值\n$$\n\\det\\!\\left(\\begin{pmatrix}2 & 1\\\\ 1 & 2\\end{pmatrix}-\\lambda I\\right)\n=\\det\\!\\begin{pmatrix}2-\\lambda & 1\\\\ 1 & 2-\\lambda\\end{pmatrix}\n=(2-\\lambda)^{2}-1=\\lambda^{2}-4\\lambda+3=0,\n$$\n可得 $\\lambda_{1}=3$ 和 $\\lambda_{2}=1$。因此，奇异值为 $\\sigma_{1}=\\sqrt{3}$ 和 $\\sigma_{2}=1$。\n\n对于 $\\lambda_{1}=3$，$A A^{T}$ 的一个特征向量满足\n$$\n\\begin{pmatrix}-1 & 1\\\\ 1 & -1\\end{pmatrix}u=0 \\quad\\Rightarrow\\quad u\\propto \\begin{pmatrix}1\\\\ 1\\end{pmatrix}.\n$$\n归一化得到\n$$\nu_{1}=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\ 1\\end{pmatrix}.\n$$\n相应的右奇异向量为\n$$\nv_{1}=\\frac{1}{\\sigma_{1}}A^{T}u_{1}\n=\\frac{1}{\\sqrt{3}}\n\\begin{pmatrix}\n1 & 0\\\\\n1 & 1\\\\\n0 & 1\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\ 1\\end{pmatrix}\n=\\frac{1}{\\sqrt{6}}\\begin{pmatrix}1\\\\ 2\\\\ 1\\end{pmatrix}.\n$$\n因此，最佳秩一近似为\n$$\nA_{1}=\\sigma_{1} u_{1} v_{1}^{T}=\\sqrt{3}\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\ 1\\end{pmatrix}\\right)\\left(\\frac{1}{\\sqrt{6}}\\begin{pmatrix}1 & 2 & 1\\end{pmatrix}\\right)\n=\\frac{1}{2}\\begin{pmatrix}1 & 2 & 1\\\\ 1 & 2 & 1\\end{pmatrix}.\n$$\n预测的缺失条目是 $A_{1}$ 的 $(1,3)$ 元素：\n$$\n(A_{1})_{1,3}=\\sigma_{1}\\,u_{1,1}\\,v_{1,3}\n=\\sqrt{3}\\cdot \\frac{1}{\\sqrt{2}}\\cdot \\frac{1}{\\sqrt{6}}\n=\\frac{\\sqrt{3}}{\\sqrt{12}}=\\frac{1}{2}=0.5.\n$$\n四舍五入到三位有效数字，预测值为 $0.500$。", "answer": "$$\\boxed{0.500}$$", "id": "2154142"}, {"introduction": "线性回归是数据科学中最基础的模型之一，其目标是找到一条“最佳拟合”直线。当数据点数量超过模型参数时，我们会得到一个无精确解的超定方程组。本练习将引导你运用SVD来构建摩尔-彭若斯伪逆（Moore-Penrose pseudoinverse），从而为这类问题找到唯一的最小二乘解，这展示了SVD作为解决实际优化问题的强大能力 [@problem_id:2154101]。", "problem": "一位数据科学家试图使用形式为 $y = mx + c$ 的简单线性模型来建模特征 $x$ 和目标变量 $y$ 之间的关系，其中 $m$ 是斜率，$c$ 是 y 轴截距。为了确定 $m$ 和 $c$ 的最优值，该科学家收集了以下四个数据点 $(x, y)$：$(-2, -1)$、$(-1, 1)$、$(1, 2)$ 和 $(2, 4)$。\n\n每个数据点都提供了一个关于 $m$ 和 $c$ 的线性方程。由于数据点（方程）的数量多于未知参数的数量，这构成了一个形式为 $A\\mathbf{x} = \\mathbf{b}$ 的超定线性方程组，其中解向量为 $\\mathbf{x} = \\begin{pmatrix} m \\\\ c \\end{pmatrix}$。目标是找到 $\\mathbf{x}$ 的最小二乘解，该解可以最小化模型预测值与实际 $y$ 值之间的平方差之和。\n\n你的任务是找到这个最小二乘解。你必须遵循一个特定的步骤：首先，根据给定数据构建矩阵 $A$ 和向量 $\\mathbf{b}$。然后，计算 $A$ 的奇异值分解（SVD）。使用 SVD，找到 Moore-Penrose 伪逆 $A^\\dagger$。最后，计算最小二乘解向量 $\\mathbf{x} = A^\\dagger\\mathbf{b}$。\n\n请以闭式解析表达式的形式，给出斜率 $m$ 和 y 轴截距 $c$ 的值。", "solution": "我们将 $y$ 建模为 $y = mx + c$。对于每个数据点 $(x_{i},y_{i})$，可得到 $m x_{i} + c = y_{i}$。根据给定的四个点，矩阵 $A$ 和向量 $\\mathbf{b}$ 为\n$$\nA = \\begin{pmatrix}\n-2 & 1 \\\\\n-1 & 1 \\\\\n1 & 1 \\\\\n2 & 1\n\\end{pmatrix},\\quad\n\\mathbf{b} = \\begin{pmatrix}\n-1 \\\\\n1 \\\\\n2 \\\\\n4\n\\end{pmatrix}.\n$$\n我们计算奇异值分解 $A = U \\Sigma V^{T}$。从 $A^{T}A$ 开始：\n$$\nA^{T}A = \\begin{pmatrix}\n\\sum x_{i}^{2} & \\sum x_{i} \\\\\n\\sum x_{i} & \\sum 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n10 & 0 \\\\\n0 & 4\n\\end{pmatrix}.\n$$\n因此 $A^{T}A$ 的特征值为 $10$ 和 $4$，其特征向量由标准基给出，所以我们可以取\n$$\nV = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix},\\qquad\n\\Sigma_{r} = \\begin{pmatrix}\n\\sqrt{10} & 0 \\\\\n0 & 2\n\\end{pmatrix}.\n$$\n对于瘦奇异值分解（thin SVD），$U_{r}$ 的列 $u_{1}$ 和 $u_{2}$ 是通过对 $A$ 的列进行归一化得到的：\n$$\nu_{1} = \\frac{1}{\\sqrt{10}}\\begin{pmatrix}-2\\\\-1\\\\1\\\\2\\end{pmatrix},\\qquad\nu_{2} = \\frac{1}{2}\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix},\n$$\n所以 $U_{r} = \\begin{pmatrix}u_{1} & u_{2}\\end{pmatrix}$。因此，Moore-Penrose 伪逆是\n$$\nA^{\\dagger} = V \\Sigma_{r}^{-1} U_{r}^{T} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{10}} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{1}^{T} \\\\\nu_{2}^{T}\n\\end{pmatrix}.\n$$\n最小二乘解为 $\\mathbf{x} = A^{\\dagger}\\mathbf{b} = V \\Sigma_{r}^{-1} U_{r}^{T}\\mathbf{b} = \\Sigma_{r}^{-1} U_{r}^{T}\\mathbf{b}$。计算投影：\n$$\nu_{1}^{T}\\mathbf{b} = \\frac{1}{\\sqrt{10}}\\big((-2)(-1)+(-1)(1)+(1)(2)+(2)(4)\\big) = \\frac{11}{\\sqrt{10}},\n$$\n$$\nu_{2}^{T}\\mathbf{b} = \\frac{1}{2}\\big((-1)+1+2+4\\big) = 3.\n$$\n乘以 $\\Sigma_{r}^{-1}$：\n$$\n\\mathbf{x} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{10}} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{11}{\\sqrt{10}} \\\\\n3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{11}{10} \\\\\n\\frac{3}{2}\n\\end{pmatrix}.\n$$\n因此，最小二乘斜率和截距分别为 $m = \\frac{11}{10}$ 和 $c = \\frac{3}{2}$。", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{11}{10} \\\\ \\frac{3}{2}\\end{pmatrix}}$$", "id": "2154101"}]}