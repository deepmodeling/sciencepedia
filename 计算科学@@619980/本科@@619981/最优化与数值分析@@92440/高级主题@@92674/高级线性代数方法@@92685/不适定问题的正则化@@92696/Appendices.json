{"hands_on_practices": [{"introduction": "让我们从一个直观的几何问题开始，热身一下。这个练习揭示了吉洪诺夫（Tikhonov）正则化（即 $L_2$ 正则化）的核心思想：当一个问题有无限多个解时，我们如何挑选出“最好”或“最简单”的那一个？通过求解一个简单的约束优化问题，你将亲身体会到正则化是如何帮助我们找到范数最小的解，也就是在几何上离原点最近的那个解。[@problem_id:2197177]", "problem": "一个资源分配系统的简化模型包含两个控制变量 $x_1$ 和 $x_2$。为达到目标输出，这些变量必须满足线性约束方程 $3x_1 + 2x_2 = 10$。该系统的设计目标是以最小的能量消耗运行。运行成本（记作 $L$）与控制变量的平方和成正比，这意味着需要最小化 $L = x_1^2 + x_2^2$。\n\n在满足约束条件的无限多对 $(x_1, x_2)$ 中，确定能同时最小化成本函数 $L$ 的唯一一对。以数值对 $(x_1, x_2)$ 的形式给出你的答案。", "solution": "该问题要求我们找到在满足线性约束条件 $3x_1 + 2x_2 = 10$ 的情况下，使目标函数 $L(x_1, x_2) = x_1^2 + x_2^2$ 最小化的 $x_1$ 和 $x_2$ 的值。这是一个约束优化问题。\n\n首先，我们可以利用约束方程将一个变量用另一个变量表示。让我们求解约束方程以得到 $x_2$：\n$$3x_1 + 2x_2 = 10$$\n$$2x_2 = 10 - 3x_1$$\n$$x_2 = 5 - \\frac{3}{2}x_1$$\n\n现在，我们可以将这个 $x_2$ 的表达式代入目标函数 $L$。这将 $L$ 转换为了一个单变量函数 $L(x_1)$。\n$$L(x_1) = x_1^2 + \\left(5 - \\frac{3}{2}x_1\\right)^2$$\n\n为了找到使 $L$ 最小化的 $x_1$ 的值，我们首先展开 $L(x_1)$ 的表达式：\n$$L(x_1) = x_1^2 + \\left(5^2 - 2(5)\\left(\\frac{3}{2}x_1\\right) + \\left(\\frac{3}{2}x_1\\right)^2\\right)$$\n$$L(x_1) = x_1^2 + 25 - 15x_1 + \\frac{9}{4}x_1^2$$\n\n合并包含 $x_1^2$ 的项：\n$$L(x_1) = \\left(1 + \\frac{9}{4}\\right)x_1^2 - 15x_1 + 25$$\n$$L(x_1) = \\frac{13}{4}x_1^2 - 15x_1 + 25$$\n\n这是一个关于 $x_1$ 的二次函数，其图像开口向上（因为系数 $\\frac{13}{4}$ 是正数），所以它有唯一的最小值。为了找到最小值，我们对 $L(x_1)$ 关于 $x_1$ 求导，并令其等于零。\n$$\\frac{dL}{dx_1} = \\frac{d}{dx_1}\\left(\\frac{13}{4}x_1^2 - 15x_1 + 25\\right)$$\n$$\\frac{dL}{dx_1} = 2 \\cdot \\frac{13}{4}x_1 - 15$$\n$$\\frac{dL}{dx_1} = \\frac{13}{2}x_1 - 15$$\n\n将导数设为零以找到临界点：\n$$\\frac{13}{2}x_1 - 15 = 0$$\n$$\\frac{13}{2}x_1 = 15$$\n$$x_1 = 15 \\cdot \\frac{2}{13}$$\n$$x_1 = \\frac{30}{13}$$\n\n现在我们得到了 $x_1$ 的值，可以将其代回到从约束条件中导出的 $x_2$ 的表达式中：\n$$x_2 = 5 - \\frac{3}{2}x_1$$\n$$x_2 = 5 - \\frac{3}{2}\\left(\\frac{30}{13}\\right)$$\n$$x_2 = 5 - \\frac{90}{26}$$\n$$x_2 = 5 - \\frac{45}{13}$$\n\n为了进行分数相减，我们通分：\n$$x_2 = \\frac{5 \\cdot 13}{13} - \\frac{45}{13}$$\n$$x_2 = \\frac{65 - 45}{13}$$\n$$x_2 = \\frac{20}{13}$$\n\n因此，满足约束条件并使成本函数最小化的控制变量对是 $(x_1, x_2) = \\left(\\frac{30}{13}, \\frac{20}{13}\\right)$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{30}{13} & \\frac{20}{13} \\end{pmatrix}}$$", "id": "2197177"}, {"introduction": "在理解了 $L_2$ 正则化的基本原理后，是时候将它与另一种强大的技术——LASSO（$L_1$ 正则化）进行正面比较了。这个练习将通过解决同一个欠定系统，让你清晰地看到这两种方法在解的特性上的根本差异。你将亲自计算并对比吉洪诺夫正则化得到的稠密解和 LASSO 得到的稀疏解，从而理解为什么 LASSO 在特征选择等领域如此受欢迎。[@problem_id:2197169]", "problem": "考虑一个欠定线性系统 $Ax = b$，其中解向量为 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$，矩阵为 $A = \\begin{pmatrix} 2 & 1 \\end{pmatrix}$，右端项为标量 $b=4$。该系统表示单个方程 $2x_1 + x_2 = 4$，有无穷多解。为了选择一个唯一且理想的解，我们可以采用正则化技术。\n\n本问题探讨两种常见的正则化方法：\n\n1.  **Tikhonov 正则化 (L2 范数)：** Tikhonov 正则化解（记为 $x_T$）是使目标函数 $F(x) = \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2$ 最小化的向量 $x$。此处，$\\|v\\|_2 = \\sqrt{\\sum_i v_i^2}$ 是向量 $v$ 的标准欧几里得范数（或 L2 范数）。\n\n2.  **LASSO 正则化 (L1 范数)：** LASSO 解（记为 $x_L$）是使目标函数 $G(x) = \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_1$ 最小化的向量 $x$。此处，$\\|v\\|_1 = \\sum_i |v_i|$ 是向量 $v$ 的出租车范数（或 L1 范数）。LASSO 是“最小绝对收缩和选择算子”(Least Absolute Shrinkage and Selection Operator) 的缩写。\n\n对两种方法均使用正则化参数 $\\lambda = 4$，计算两个所得解向量的 L1 范数之比 $\\frac{\\|x_T\\|_1}{\\|x_L\\|_1}$。\n\n请将答案表示为最简精确分数。", "solution": "我们有 $A=\\begin{pmatrix}2 & 1\\end{pmatrix}$，$b=4$ 及 $x=\\begin{pmatrix}x_{1} \\\\ x_{2}\\end{pmatrix}$。\n\n对于参数为 $\\lambda=4$ 的 Tikhonov 正则化，我们最小化 $F(x)=\\|Ax-b\\|_{2}^{2}+4\\|x\\|_{2}^{2}$。岭回归的一阶最优性条件是\n$$(A^{T}A+\\lambda I)x=A^{T}b.$$\n计算\n$$A^{T}A=\\begin{pmatrix}4 & 2 \\\\ 2 & 1\\end{pmatrix},\\quad \\lambda I=\\begin{pmatrix}4 & 0 \\\\ 0 & 4\\end{pmatrix},\\quad A^{T}b=\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}.$$\n因此\n$$\\begin{pmatrix}8 & 2 \\\\ 2 & 5\\end{pmatrix}x=\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}.$$\n求解得，\n$$\\det=8\\cdot 5-2\\cdot 2=36,\\quad x_{T}=\\frac{1}{36}\\begin{pmatrix}5 & -2 \\\\ -2 & 8\\end{pmatrix}\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}=\\frac{1}{36}\\begin{pmatrix}32 \\\\ 16\\end{pmatrix}=\\begin{pmatrix}\\frac{8}{9} \\\\ \\frac{4}{9}\\end{pmatrix}.$$\n于是\n$$\\|x_{T}\\|_{1}=\\left|\\frac{8}{9}\\right|+\\left|\\frac{4}{9}\\right|=\\frac{12}{9}=\\frac{4}{3}.$$\n\n对于 $\\lambda=4$ 的 LASSO，我们最小化 $G(x)=\\|Ax-b\\|_{2}^{2}+4\\|x\\|_{1}$。令 $r=Ax-b=2x_{1}+x_{2}-4$。次梯度最优性条件是\n$$2A^{T}(Ax-b)+\\lambda z=0,\\quad z\\in\\partial\\|x\\|_{1}.$$\n由于 $A^{T}(Ax-b)=\\begin{pmatrix}2r \\\\ r\\end{pmatrix}$，我们得到\n$$\\begin{pmatrix}4r \\\\ 2r\\end{pmatrix}+4\\begin{pmatrix}z_{1} \\\\ z_{2}\\end{pmatrix}=0\\quad\\Rightarrow\\quad z_{1}=-r,\\; z_{2}=-\\frac{r}{2}.$$\n可行性要求 $z_{i}\\in\\partial|x_{i}|$：如果 $x_{i}\\neq 0$，则 $z_{i}=\\operatorname{sign}(x_{i})\\in\\{\\pm 1\\}$；如果 $x_{i}=0$，则 $z_{i}\\in[-1,1]$。\n\n- 如果 $x_{1}\\neq 0$ 且 $x_{2}\\neq 0$，则 $z_{1},z_{2}\\in\\{\\pm 1\\}$，这将要求同时满足 $-r\\in\\{\\pm 1\\}$ 和 $-r/2\\in\\{\\pm 1\\}$；不存在满足这两个等式的 $r$，因此这是不可能的。\n\n- 如果 $x_{1}=0$ 且 $x_{2}\\neq 0$，则 $z_{1}=-r\\in[-1,1]$ 意味着 $r\\in[-1,1]$，但 $z_{2}=-r/2=\\operatorname{sign}(x_{2})\\in\\{\\pm 1\\}$ 迫使 $r\\in\\{-2,2\\}$，这是一个矛盾。\n\n- 如果 $x_{2}=0$ 且 $x_{1}\\neq 0$，则 $z_{1}=-r=\\operatorname{sign}(x_{1})\\in\\{\\pm 1\\}$ 意味着 $r\\in\\{-1,1\\}$，并且 $z_{2}=-r/2\\in[-1,1]$ 对这两个 $r$ 值都自动成立。检验与 $r=2x_{1}-4$ 的一致性：\n  - 如果 $r=-1$，则 $2x_{1}-4=-1$ 得到 $x_{1}=\\frac{3}{2}$，这与 $\\operatorname{sign}(x_{1})=+1$ 一致，因此 $z_{1}=-r=1$。\n  - 如果 $r=1$，则 $2x_{1}-4=1$ 得到 $x_{1}=\\frac{5}{2}$，这将要求 $z_{1}=-1$，与 $\\operatorname{sign}(x_{1})=+1$ 矛盾。\n\n因此，LASSO 最小化子是\n$$x_{L}=\\begin{pmatrix}\\frac{3}{2} \\\\ 0\\end{pmatrix},\\quad \\|x_{L}\\|_{1}=\\frac{3}{2}.$$\n\n最后，所求的比率为\n$$\\frac{\\|x_{T}\\|_{1}}{\\|x_{L}\\|_{1}}=\\frac{\\frac{4}{3}}{\\frac{3}{2}}=\\frac{4}{3}\\cdot\\frac{2}{3}=\\frac{8}{9}.$$", "answer": "$$\\boxed{\\frac{8}{9}}$$", "id": "2197169"}, {"introduction": "现在，让我们进入一个在现实数据分析中更为微妙和普遍的场景：处理高度相关的特征。这个练习模拟了当两个预测变量高度相关时，标准的 LASSO 方法可能表现出不稳定的行为。通过对比 LASSO 和弹性网络（Elastic Net）的解，你将理解为什么结合了 $L_1$ 和 $L_2$ 惩罚项的弹性网络能够提供一个更稳健、更可解释的模型，有效地解决了 LASSO 在此类情况下的局限性。[@problem_id:2197145]", "problem": "一位数据科学家正在构建一个线性回归模型，以预测一家科技公司的季度收入。有两个可用的预测变量：$x_1$ 是全国电子产品消费支出，$x_2$ 是一个专有的“科技热情指数”。在训练数据中，发现这两个变量是完全相关的。对于形式为 $\\hat{y} = w_1 x_1 + w_2 x_2$ 的模型，其平方和误差损失函数简化为 $L_{SE}(w_1, w_2) = (9 - (w_1 + w_2))^2$。\n\n为了防止过拟合并处理相关性问题，考虑了两种不同的正则化技术。\n\n1.  **LASSO 回归**：LASSO（Least Absolute Shrinkage and Selection Operator）目标函数由下式给出：\n    $$J_{L}(w_1, w_2) = (9 - (w_1 + w_2))^2 + 4(|w_1| + |w_2|)$$\n\n2.  **弹性网络回归**：弹性网络目标函数由下式给出：\n    $$J_{EN}(w_1, w_2) = (9 - (w_1 + w_2))^2 + 2(|w_1| + |w_2|) + (w_1^2 + w_2^2)$$\n\nLASSO 模型的最优权重向量 $(w_{1,L}, w_{2,L})$ 和弹性网络模型的最优权重向量 $(w_{1,EN}, w_{2,EN})$ 是通过最小化它们各自的目标函数来计算的。\n\n关于计算出的权重，以下哪些陈述是正确的？选择所有适用的选项。\n\nA. 一个可能的 LASSO 解是 $(w_{1,L}, w_{2,L}) = (7, 0)$。\n\nB. 一个可能的 LASSO 解是 $(w_{1,L}, w_{2,L}) = (0, 7)$。\n\nC. 弹性网络解是 $(w_{1,EN}, w_{2,EN}) = (\\frac{8}{3}, \\frac{8}{3})$。\n\nD. 弹性网络解是 $(w_{1,EN}, w_{2,EN}) = (4.5, 4.5)$。\n\nE. 一个可能的 LASSO 解是 $(w_{1,L}, w_{2,L}) = (4.5, 4.5)$。", "solution": "该问题要求我们找到两种不同正则化回归模型（LASSO 和弹性网络）的最优权重向量，然后评估给定的选项。\n\n**第一部分：最小化 LASSO 目标函数**\n\nLASSO 目标函数为 $J_{L}(w_1, w_2) = (9 - (w_1 + w_2))^2 + 4(|w_1| + |w_2|)$。\n令 $S = w_1 + w_2$。目标函数变为 $J_{L} = (9 - S)^2 + 4(|w_1| + |w_2|)$。\n由于 $L_1$ 范数 ($|w_1| + |w_2|$) 的性质，当特征完全相关时，LASSO 会任意选择一个特征并将其余特征的权重设为零。这是因为对于一个固定的和 $S = w_1 + w_2$，当全部“权重”都集中在一个系数上时，即一个系数为 $S$ 而另一个为 $0$ 时，惩罚项 $|w_1| + |w_2|$ 才能被最小化。在这种情况下， $|w_1| + |w_2| = |S|$。\n\n不失一般性，我们假设 $w_2=0$。这意味着 $S=w_1$。目标函数简化为一个单变量问题：\n$J_L(w_1) = (9 - w_1)^2 + 4|w_1|$。\n\n我们必须考虑绝对值项的两种情况。\n\n情况 1：$w_1 \\ge 0$。\n目标函数为 $f(w_1) = (9 - w_1)^2 + 4w_1$。\n为了找到最小值，我们对 $w_1$ 求导并令其为零：\n$\\frac{df}{dw_1} = 2(9 - w_1)(-1) + 4 = -18 + 2w_1 + 4 = 2w_1 - 14$。\n将导数设为零：$2w_1 - 14 = 0 \\implies w_1 = 7$。\n该解与我们的假设 $w_1 \\ge 0$ 一致。所以，$(w_1, w_2) = (7, 0)$ 是一个潜在的最小值点。\n\n情况 2：$w_1 < 0$。\n目标函数为 $g(w_1) = (9 - w_1)^2 - 4w_1$。\n求导：\n$\\frac{dg}{dw_1} = 2(9 - w_1)(-1) - 4 = -18 + 2w_1 - 4 = 2w_1 - 22$。\n将导数设为零：$2w_1 - 22 = 0 \\implies w_1 = 11$。\n该解与我们的假设 $w_1 < 0$ 相矛盾，所以该定义域内的最小值不在此驻点。函数 $g(w_1)$ 在所有 $w_1 < 11$ 的范围内是递减的，因此对于 $w_1 < 0$，其最小值将在边界 $w_1 \\to 0^-$ 处取得，此时函数值接近 $g(0)=81$。我们在情况 1 中找到的最小值给出的函数值为 $f(7) = (9-7)^2 + 4(7) = 4 + 28 = 32$，这个值更低。\n\n因此，当 $w_1=7$（且 $w_2=0$）时达到最小值。由于问题关于 $w_1$ 和 $w_2$ 的对称性，如果 $(7, 0)$ 是一个解，那么 $(0, 7)$ 也必定是一个解。两者都得到相同的最小目标函数值 32。\n\n因此，陈述 **A** 和 **B** 是正确的。陈述 E 是不正确的，因为已知 LASSO 对完全相关的变量会产生稀疏解（某些系数恰好为零），而不是平均分配权重。\n\n**第二部分：最小化弹性网络目标函数**\n\n弹性网络目标函数为 $J_{EN}(w_1, w_2) = (9 - (w_1 + w_2))^2 + 2(|w_1| + |w_2|) + (w_1^2 + w_2^2)$。\n由于存在 $L_2$ 项 ($w_1^2 + w_2^2$)，该目标函数是严格凸的，这保证了其有唯一的最小值。\n该目标函数关于 $w_1$ 和 $w_2$ 是对称的。这意味着在最小值点，我们必须有 $w_1 = w_2$。我们将这个共同的值称为 $w$。\n\n将 $w_1 = w_2 = w$ 代入目标函数：\n$J_{EN}(w) = (9 - 2w)^2 + 2|w| + 2|w| + w^2 + w^2 = (9 - 2w)^2 + 4|w| + 2w^2$。\n假设 $w > 0$ 是合理的，因为设置 $w_1=w_2=0$ 会得到 $J_{EN}(0) = 81$，而目标 $S=w_1+w_2=9$ 表明系数应为正。对于 $w > 0$，目标函数为：\n$f_{EN}(w) = (9 - 2w)^2 + 4w + 2w^2$。\n为了找到最小值，我们对 $w$ 求导并令其为零：\n$\\frac{df_{EN}}{dw} = 2(9 - 2w)(-2) + 4 + 4w = -36 + 8w + 4 + 4w = 12w - 32$。\n将导数设为零：$12w - 32 = 0 \\implies w = \\frac{32}{12} = \\frac{8}{3}$。\n由于 $w=8/3 > 0$，我们的假设是有效的。唯一的最小值出现在 $w_1 = w_2 = 8/3$ 处。\n因此，弹性网络的解是 $(w_{1,EN}, w_{2,EN}) = (\\frac{8}{3}, \\frac{8}{3})$。\n\n因此，陈述 **C** 是正确的。陈述 D，$(4.5, 4.5)$，对应于未经正则化的解，其中 $w_1+w_2=9$ 且 $w_1=w_2$，但这并不是弹性网络目标函数的最小值。当 $w=4.5$ 时，我们的弹性网络函数的导数为 $12(4.5) - 32 = 54 - 32 = 22 \\neq 0$。所以 D 是不正确的。\n\n**结论**\n\n根据以上分析，正确的陈述是 A、B 和 C。", "answer": "$$\\boxed{ABC}$$", "id": "2197145"}]}