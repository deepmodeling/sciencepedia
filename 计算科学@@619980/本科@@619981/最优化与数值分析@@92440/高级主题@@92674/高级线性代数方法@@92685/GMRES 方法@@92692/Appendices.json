{"hands_on_practices": [{"introduction": "要真正掌握 GMRES，我们必须从其最基本的动作开始：第一个迭代步。本练习将引导您在一个小规模系统上完成单次迭代，揭示 GMRES 如何在其初始搜索空间中找到最优解。通过亲手完成这个计算，您将对该方法核心的“最小残差”原则获得一个具体而直观的理解 [@problem_id:2214790]。", "problem": "广义最小残差（GMRES）方法是一种迭代算法，用于求解线性方程组 $Ax=b$ 的近似解。考虑由矩阵 $A$ 和向量 $b$ 定义的线性系统：\n$$\nA = \\begin{pmatrix} 2 & 1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n从初始猜测 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，应用一步 GMRES 算法求出第一个近似解 $x_1$。将答案表示为分量为有理数的列向量。", "solution": "GMRES 在仿射空间 $x_{0}+\\mathcal{K}_{1}(A,r_{0})$ 上最小化残差的 2-范数，其中 $r_{0}=b-Ax_{0}$ 且 $\\mathcal{K}_{1}(A,r_{0})=\\mathrm{span}\\{r_{0}\\}$。对于 $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$，我们有\n$$\nr_{0}=b=\\begin{pmatrix}1\\\\2\\end{pmatrix}.\n$$\n经过一步 GMRES 后，$x_{1}$ 的形式为 $x_{1}=x_{0}+\\alpha r_{0}$，其中选择 $\\alpha$ 以最小化\n$$\n\\|b-A(x_{0}+\\alpha r_{0})\\|_{2}=\\|r_{0}-\\alpha A r_{0}\\|_{2}.\n$$\n令 $w=A r_{0}$。那么最小化问题是 $\\min_{\\alpha}\\|r_{0}-\\alpha w\\|_{2}^{2}$。使用欧几里得内积，定义\n$$\n\\phi(\\alpha)=(r_{0}-\\alpha w)^{T}(r_{0}-\\alpha w)=r_{0}^{T}r_{0}-2\\alpha w^{T}r_{0}+\\alpha^{2}w^{T}w.\n$$\n令 $\\frac{d\\phi}{d\\alpha}=0$，可得\n$$\n-2\\,w^{T}r_{0}+2\\alpha\\,w^{T}w=0 \\quad\\Rightarrow\\quad \\alpha=\\frac{w^{T}r_{0}}{w^{T}w}.\n$$\n计算 $w$ 和所需的内积：\n$$\nw=A r_{0}=A b=\\begin{pmatrix}2 & 1\\\\ -1 & 3\\end{pmatrix}\\begin{pmatrix}1\\\\2\\end{pmatrix}=\\begin{pmatrix}4\\\\5\\end{pmatrix},\\quad\nw^{T}r_{0}=\\begin{pmatrix}4 & 5\\end{pmatrix}\\begin{pmatrix}1\\\\2\\end{pmatrix}=14,\\quad\nw^{T}w=4^{2}+5^{2}=41.\n$$\n因此\n$$\n\\alpha=\\frac{14}{41},\\qquad x_{1}=x_{0}+\\alpha r_{0}=\\frac{14}{41}\\begin{pmatrix}1\\\\2\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{41}\\\\ \\frac{28}{41}\\end{pmatrix}.\n$$\n这个 $x_{1}$ 是 GMRES(1) 的迭代结果，即残差范数在 $x_{0}+\\mathrm{span}\\{r_{0}\\}$ 上的最小化子。", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{14}{41}\\\\ \\frac{28}{41}\\end{pmatrix}}$$", "id": "2214790"}, {"introduction": "GMRES 旨在最小化残差，但这是否总是意味着我们离真实解更近了呢？本练习提出了一个看似违反直觉的场景，在第一步迭代后，解的误差范数实际上增加了。完成这个例子对于理解残差最小化与误差减小之间的关键区别至关重要，这是 GMRES 在处理非正规矩阵时一个微妙但极其重要的行为特性 [@problem_id:2398705]。", "problem": "考虑线性系统 $A x = b$，其中 $2 \\times 2$ 矩阵 A 为\n$$\nA = \\begin{pmatrix}\n1 & 10 \\\\\n0 & 1\n\\end{pmatrix},\n$$\n右端项 $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，初始猜测解 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。令 $\\|\\cdot\\|$ 表示欧几里得范数。对于任意 $x$，残差为 $r(x) = b - A x$。广义最小残差（GMRES）方法将第一次迭代的结果 $x_{1}$ 定义为仿射空间 $x_{0} + \\mathcal{K}_{1}(A, r_{0})$ 中的唯一向量，该向量使得 $\\|r(x)\\|$ 最小化，其中 $r_{0} = r(x_{0})$。\n\n计算比值\n$$\n\\frac{\\|x - x_{1}\\|}{\\|x - x_{0}\\|},\n$$\n其中 $x$ 是 $A x = b$ 的精确解，使用欧几里得范数。将您的答案四舍五入到四位有效数字。", "solution": "目标是计算比值 $\\frac{\\|x - x_{1}\\|}{\\|x - x_{0}\\|}$，其中 $x$ 是线性系统 $A x = b$ 的精确解，$x_{0}$ 是给定的初始猜测解，而 $x_{1}$ 是由 GMRES 方法生成的第一次迭代结果。范数为欧几里得范数。\n\n首先，我们确定精确解 $x$。该线性系统为：\n$$\nA x = b \\implies \\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_a \\\\ x_b \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n使用回代法，第二行给出 $0 \\cdot x_a + 1 \\cdot x_b = 1$，这意味着 $x_b = 1$。\n将 $x_b=1$ 代入第一行，得到 $1 \\cdot x_a + 10 \\cdot (1) = 1$，从而得出 $x_a = 1 - 10 = -9$。\n因此，精确解为 $x = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix}$。\n\n接下来，我们计算初始残差 $r_0$。初始猜测解为 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n残差定义为 $r(x) = b - A x$。初始残差 $r_0$ 为：\n$$\nr_0 = r(x_0) = b - A x_0 = b - A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n第一次 GMRES 迭代结果 $x_1$ 定义为仿射空间 $x_0 + \\mathcal{K}_1(A, r_0)$ 中使残差范数 $\\|r(x)\\|$ 最小化的向量。第一个 Krylov 子空间 $\\mathcal{K}_1(A, r_0)$ 是由初始残差向量张成的空间，即 $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$。\n因此，$x_1$ 可以表示为：\n$$\nx_1 = x_0 + \\alpha r_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n其中 $\\alpha \\in \\mathbb{R}$ 为某一标量。\n\n为求最优的 $\\alpha$，我们必须最小化相应残差的范数，即 $r(x_1) = b - A x_1$。\n代入 $x_1$的表达式并使用 $b=r_0$，我们得到：\n$$\nr(x_1) = b - A (\\alpha r_0) = r_0 - \\alpha (A r_0)\n$$\n我们需要找到使欧几里得范数的平方 $\\|r_0 - \\alpha A r_0\\|^2$ 最小化的 $\\alpha$。这是一个标准的线性最小二乘问题，其解是 $r_0$ 在 $A r_0$ 上的正交投影。系数 $\\alpha$ 由下式给出：\n$$\n\\alpha = \\frac{\\langle r_0, A r_0 \\rangle}{\\|A r_0\\|^2}\n$$\n我们计算向量 $A r_0$：\n$$\nA r_0 = \\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 10 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 1 \\end{pmatrix}\n$$\n现在，我们计算内积和范数的平方：\n$$\n\\langle r_0, A r_0 \\rangle = (1)(11) + (1)(1) = 12\n$$\n$$\n\\|A r_0\\|^2 = 11^2 + 1^2 = 121 + 1 = 122\n$$\n代入这些值，我们求得 $\\alpha$：\n$$\n\\alpha = \\frac{12}{122} = \\frac{6}{61}\n$$\n根据这个 $\\alpha$ 值，第一次 GMRES 迭代结果为：\n$$\nx_1 = \\frac{6}{61} r_0 = \\frac{6}{61} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 6/61 \\\\ 6/61 \\end{pmatrix}\n$$\n现在我们计算误差向量 $e_0 = x - x_0$ 和 $e_1 = x - x_1$ 及其范数。\n初始误差为：\n$$\ne_0 = x - x_0 = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix}\n$$\n初始误差的范数为：\n$$\n\\|e_0\\| = \\|x - x_0\\| = \\sqrt{(-9)^2 + 1^2} = \\sqrt{81 + 1} = \\sqrt{82}\n$$\n第一次迭代后的误差为：\n$$\ne_1 = x - x_1 = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 6/61 \\\\ 6/61 \\end{pmatrix} = \\begin{pmatrix} -9 - \\frac{6}{61} \\\\ 1 - \\frac{6}{61} \\end{pmatrix} = \\begin{pmatrix} -\\frac{549+6}{61} \\\\ \\frac{61-6}{61} \\end{pmatrix} = \\begin{pmatrix} -555/61 \\\\ 55/61 \\end{pmatrix}\n$$\n该误差向量的范数为：\n$$\n\\|e_1\\| = \\|x - x_1\\| = \\sqrt{\\left(-\\frac{555}{61}\\right)^2 + \\left(\\frac{55}{61}\\right)^2} = \\frac{1}{61}\\sqrt{(-555)^2 + 55^2} = \\frac{1}{61}\\sqrt{308025 + 3025} = \\frac{\\sqrt{311050}}{61}\n$$\n最后，我们计算所求的比值：\n$$\n\\frac{\\|x - x_1\\|}{\\|x - x_0\\|} = \\frac{\\sqrt{311050}/61}{\\sqrt{82}} = \\sqrt{\\frac{311050}{61^2 \\cdot 82}} = \\sqrt{\\frac{311050}{3721 \\cdot 82}} = \\sqrt{\\frac{311050}{305122}}\n$$\n我们现在对这个表达式进行数值计算：\n$$\n\\frac{\\|x - x_1\\|}{\\|x - x_0\\|} \\approx \\sqrt{1.0194280} \\approx 1.00966727\n$$\n问题要求将答案四舍五入到四位有效数字。将 $1.00966727$ 四舍五入得到 $1.010$。对于应用于本问题中给出的这类高度非正规矩阵，误差范数增大的事实是 GMRES 方法的一个已知现象。虽然可以保证残差范数是单调不增的，但误差范数并非如此。", "answer": "$$\\boxed{1.010}$$", "id": "2398705"}, {"introduction": "从理论走向实践，预处理是使 GMRES 成为解决实际问题强大工具的关键。这项编程挑战要求您实现并比较左预处理和右预处理这两种标准技术在不同类型线性系统上的表现。这个实践性的任务将帮助您深入洞察预处理如何加速收敛，以及为何预处理策略的选择在数值计算中是一个关键决策 [@problem_id:2398732]。", "problem": "给定方形、非奇异实矩阵 $A \\in \\mathbb{R}^{n \\times n}$、右端向量 $b \\in \\mathbb{R}^{n}$ 以及非奇异实预条件子 $M \\in \\mathbb{R}^{n \\times n}$。考虑线性系统 $A x = b$。将左预处理定义为求解 $M^{-1} A x = M^{-1} b$ 以得到 $x$，右预处理定义为求解 $A M^{-1} y = b$ 以得到 $y$，然后计算 $x = M^{-1} y$。对于每种情况，以零向量作为初始猜测，生成在每次迭代中最小化相应 Krylov 子空间上残差欧几里得范数的近似解序列。在每次迭代 $k$ 中，计算真实残差向量 $r_k = b - A x_k$ 及其欧几里得范数 $\\|r_k\\|_2$。\n\n你的任务是实现一个程序，对以下每个指定的测试用例，生成：\n- 最终左预处理近似解与右预处理近似解之差的欧几里得范数，即 $\\|x^{(L)}_{\\text{final}} - x^{(R)}_{\\text{final}}\\|_2$，\n- 左预处理公式的真实残差范数的完整历史记录 $\\{\\|r_k^{(L)}\\|_2\\}_{k=1}^{K_L}$，\n- 右预处理公式的真实残差范数的完整历史记录 $\\{\\|r_k^{(R)}\\|_2\\}_{k=1}^{K_R}$，\n\n其中 $K_L$ 和 $K_R$ 是每种公式满足停止准则所需的实际迭代次数。使用停止准则 $\\|r_k\\|_2 \\le \\tau \\|b\\|_2$，其中相对容差 $\\tau = 10^{-10}$，或者当迭代次数达到矩阵维度 $n$ 时终止。所有角度（若适用）都必须以弧度为单位。\n\n测试套件（请严格使用以下规范）：\n\n- 测试用例 $1$（对称正定，Jacobi 预条件子，构造解）：\n  - 大小: $n = 20$。\n  - 矩阵 $A$：三对角矩阵，主对角线元素为 $2$，第一亚对角线和第一超对角线元素为 $-1$（标准一维 Poisson 模板，带齐次 Dirichlet 边界）。\n  - 预条件子 $M$：$A$ 的对角部分。\n  - 真实解 $x^{\\star}$，其元素为 $x^{\\star}_i = \\sin\\left(\\frac{\\pi i}{n+1}\\right)$，其中 $i = 1, 2, \\dots, n$（角度以弧度为单位）。\n  - 右端项 $b = A x^{\\star}$。\n\n- 测试用例 $2$（非对称对流扩散，Jacobi 预条件子）：\n  - 大小: $n = 30$。\n  - 网格间距 $h = \\frac{1}{n+1}$。\n  - 参数：扩散系数 $\\varepsilon = 10^{-2}$，对流速度 $c = 1$。\n  - 矩阵 $A = \\frac{\\varepsilon}{h^2} T_{\\text{diff}} + \\frac{c}{h} T_{\\text{adv}}$，其中 $T_{\\text{diff}}$ 是三对角矩阵，主对角线元素为 $2$，亚对角线和超对角线元素为 $-1$；$T_{\\text{adv}}$ 对应于后向差分，主对角线元素为 $-1$，第一亚对角线元素为 $+1$（其他位置为零）。\n  - 预条件子 $M$：$A$ 的对角部分。\n  - 右端项 $b$ 是 $\\mathbb{R}^n$ 中的全一向量。\n\n- 测试用例 $3$（单位预条件子边界情况）：\n  - 与测试用例 1 中的 $A$ 和 $b$ 相同。\n  - 预条件子 $M = I$（$n \\times n$ 单位矩阵）。\n\n数值规范：\n- 所有运行的初始猜测：$x_0 = 0$。\n- 使用上文定义的相对容差 $\\tau = 10^{-10}$。\n- 最大迭代次数 $k_{\\max} = n$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，输出一个形式为 $[\\|x^{(L)}_{\\text{final}} - x^{(R)}_{\\text{final}}\\|_2, \\text{residuals\\_left}, \\text{residuals\\_right}]$ 的列表，其中 $\\text{residuals\\_left}$ 和 $\\text{residuals\\_right}$ 是每次迭代的浮点数残差范数列表。\n- 具体来说，最终输出必须看起来像一个长度为 $3$ 的 Python 风格列表，其元素分别对应于测试用例 1、2 和 3。例如，打印的结构必须具有以下形式：\n  $[[d_1,[r_{1,1},\\dots,r_{1,K_L}],[s_{1,1},\\dots,s_{1,K_R}]], [d_2,[\\dots],[\\dots]], [d_3,[\\dots],[\\dots]]]$\n  所有数字均以十进制或科学记数法显示（无百分号）。\n\n所有量都是无量纲的。不应读取任何外部输入；所有数据都必须在程序内部构建。所需的输出是如上所述的浮点数和浮点数列表，程序必须严格遵守本说明中描述的最终输出格式。", "solution": "该问题要求实现广义最小残差法（Generalized Minimal Residual, GMRES），用于求解形如 $A x = b$ 的线性系统，并考虑使用给定预条件子 $M$ 的左预处理和右预处理。对于每个测试用例，我们必须计算通过左预处理和右预处理得到的最终解之间的差异，并跟踪每次迭代 $k$ 中真实残差范数 $\\|r_k\\|_2 = \\|b - A x_k\\|_2$ 的历史记录。\n\n解决方案的基础是 GMRES 算法的自定义实现，因为标准库函数可能无法对停止准则提供必要的控制，该准则根据真实残差而非预处理后的残差来指定。\n\nGMRES 方法生成一个近似解序列 $x_k$，使得所求解系统的残差欧几里得范数在维度为 $k$ 的相应 Krylov 子空间上最小化。该残差的性质取决于预处理策略。\n\n**1. 预处理策略**\n\n设原始系统为 $A x = b$。预条件子 $M$ 是一个近似于 $A$ 的矩阵，但对于它，求解系统 $Mz=d$ 比求解原始系统容易得多。\n\n- **左预处理**：系统被转换为 $M^{-1} A x = M^{-1} b$。令 $\\tilde{A} = M^{-1}A$ 和 $\\tilde{b} = M^{-1}b$。GMRES 应用于 $\\tilde{A}x = \\tilde{b}$。在迭代 $k$ 时，该方法找到一个迭代解 $x_k$，它在 Krylov 子空间 $\\mathcal{K}_k(\\tilde{A}, \\tilde{r}_0)$ 上最小化预处理后残差的范数，即 $\\|\\tilde{r}_k\\|_2 = \\|\\tilde{b} - \\tilde{A} x_k\\|_2 = \\|M^{-1}(b - A x_k)\\|_2 = \\|M^{-1}r_k\\|_2$。问题要求监视真实残差范数 $\\|r_k\\|_2$。这个范数无法从预处理系统的 GMRES 最小化过程中直接获得。因此，在每次迭代 $k$ 中，必须显式地构建迭代解 $x_k$ 以计算 $\\|r_k\\|_2 = \\|b - A x_k\\|_2$。\n\n- **右预处理**：通过变量替换来转换系统。令 $x = M^{-1}y$。系统变为 $A M^{-1} y = b$。令 $\\hat{A} = AM^{-1}$。GMRES 应用于 $\\hat{A}y = b$。在迭代 $k$ 时，该方法找到一个迭代解 $y_k$，它在 Krylov 子空间 $\\mathcal{K}_k(\\hat{A}, \\hat{r}_0)$ 上最小化 $\\|\\hat{r}_k\\|_2 = \\|b - \\hat{A} y_k\\|_2$。原始系统对应的解是 $x_k = M^{-1}y_k$。真实残差是 $r_k = b - A x_k = b - A(M^{-1} y_k) = b - \\hat{A} y_k = \\hat{r}_k$。在这种情况下，GMRES 最小化的残差与真实残差相同。因此，真实残差范数 $\\|r_k\\|_2$ 可以在 GMRES 算法的每一步经济地获得。最终解通过计算 $x_{\\text{final}} = M^{-1}y_{\\text{final}}$ 得到。\n\n**2. GMRES 算法实现**\n\n该方法的核心是 Arnoldi 迭代，它为 Krylov 子空间 $\\mathcal{K}_{k+1}(C, r_0)$ 构建一个标准正交基 $V_{k+1} = [v_1, \\dots, v_{k+1}]$，其中 $C$ 是正在求解的系统的算子（$M^{-1}A$ 或 $AM^{-1}$），$r_0$ 是初始残差。我们使用修正的 Gram-Schmidt 过程以保证其数值稳定性。该过程产生关系式 $C V_k = V_{k+1} \\bar{H}_k$，其中 $\\bar{H}_k$ 是一个 $(k+1) \\times k$ 的上海森伯格（upper-Hessenberg）矩阵。\n\nGMRES 迭代解 $x_k$ 表示为 $x_k = x_0 + V_k y_k$（对于左预处理）或 $y_k = y_0 + V_k z_k$（对于右预处理），其中 $y_k$ 或 $z_k$ 是以下小型最小二乘问题的解：\n$$ \\min_{z \\in \\mathbb{R}^k} \\|\\beta e_1 - \\bar{H}_k z\\|_2 $$\n其中 $\\beta = \\|r_0\\|_2$ 且 $e_1 = [1, 0, \\dots, 0]^T \\in \\mathbb{R}^{k+1}$。通过应用一系列 Givens 旋转将 $\\bar{H}_k$ 转换为一个上三角矩阵 $R_k$ 并将右端向量 $\\beta e_1$ 更新为一个向量 $g_k$，可以有效地解决这个问题。迭代 $k$ 时的残差范数由变换后的右端向量的第 $(k+1)$ 个分量的大小给出。\n\n我的 GMRES 实现如下：\n1. 以 $x_0 = 0$ 初始化。根据预处理类型确定系统算子 $C$ 和初始残差 $r_0$。\n2. 标准化初始残差以获得第一个基向量 $v_1 = r_0 / \\|r_0\\|_2$。\n3. 对于每次迭代 $k = 1, 2, \\dots, n$：\n    a. 使用带修正 Gram-Schmidt 的 Arnoldi 过程生成下一个基向量 $v_{k+1}$ 和海森伯格矩阵 $\\bar{H}_k$ 的第 $k$ 列。\n    b. 使用 Givens 旋转更新 $\\bar{H}_k$ 的 QR 分解。这将三角矩阵 $R_{k-1}$ 更新为 $R_k$，并将变换后的右端项 $g_{k-1}$ 更新为 $g_k$。\n    c. 对于右预处理（和无预处理），真实残差范数 $\\|r_k\\|_2$ 就是 $g_k$ 最后一个元素的大小 $|g_{k+1}|$。\n    d. 对于左预处理，必须显式计算真实残差范数。我们求解三角系统 $R_k y_k = g_k(1:k)$ 得到 $y_k$，构造解迭代 $x_k = x_0 + V_k y_k$，然后计算 $\\|r_k\\|_2 = \\|b - A x_k\\|_2$。\n    e. 存储计算出的真实残差范数 $\\|r_k\\|_2$。\n    f. 检查停止准则：$\\|r_k\\|_2 \\le \\tau \\|b\\|_2$。如果满足，或者如果 $k=n$，则迭代终止。\n4. 循环在迭代 $K$ 次后终止，通过求解最终的三角系统得到系数，并形成基向量的线性组合来计算最终解。对于右预处理情况，需要一个额外的逆变换 $x_{\\text{final}} = M^{-1} y_{\\text{final}}$。\n\n**3. 测试用例构建**\n\n测试用例如规范所述构建：\n- **用例 1**：来自一维 Poisson 问题的对称正定系统。$A$ 是一个对角元素为 $(-1, 2, -1)$ 的三对角矩阵。大小为 $n=20$。预条件子 $M$ 是 $A$ 的对角线，即 $M = 2I$。右端项 $b$ 是根据已知解 $x^{\\star}_i = \\sin\\left(\\frac{\\pi i}{n+1}\\right)$ 制造的，使得 $b = A x^{\\star}$。\n- **用例 2**：来自一维对流扩散问题的非对称系统。矩阵为 $A = \\frac{\\varepsilon}{h^2} T_{\\text{diff}} + \\frac{c}{h} T_{\\text{adv}}$，其中 $n=30$，$\\varepsilon=10^{-2}$，$c=1$，$h=\\frac{1}{n+1}$。$T_{\\text{diff}}$ 的对角元素为 $(-1, 2, -1)$，$T_{\\text{adv}}$ 在次对角、主对角和超对角上的元素分别为 $(1, -1, 0)$。$M$ 是最终 $A$ 的对角线。向量 $b$ 的所有元素都为 1。\n- **用例 3**：与用例 1 相同，但使用单位预条件子 $M=I$。这等同于无预处理的 GMRES。\n\n代码实现了这些步骤，为每个测试用例计算了所需的量，并按要求将输出格式化为单行字符串。", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef gmres_custom(A, b, precon_type='none', M_inv=None, tol=1e-10, maxiter=None):\n    \"\"\"\n    Custom GMRES implementation to solve Ax=b.\n\n    This implementation provides fine-grained control over the stopping criterion,\n    which is based on the true residual norm ||b - A*x_k||_2, and records its history.\n    \"\"\"\n    n = A.shape[0]\n    if maxiter is None:\n        maxiter = n\n    \n    x0 = np.zeros(n)\n    res_hist = []\n\n    # System setup based on preconditioning type\n    if precon_type == 'left':\n        op = lambda v: M_inv(A @ v)\n        r_start = M_inv(b - A @ x0)\n    elif precon_type == 'right':\n        # We solve A M^-1 y = b for y, and then x = M^-1 y.\n        # Initial guess y0 = M x0. Since x0=0, y0=0.\n        op = lambda v: A @ M_inv(v)\n        r_start = b - op(np.zeros(n))\n    else: # 'none'\n        op = lambda v: A @ v\n        r_start = b - A @ x0\n    \n    b_norm = np.linalg.norm(b)\n    stop_thresh = tol * b_norm\n\n    r_norm = np.linalg.norm(r_start)\n    if r_norm == 0:\n        return x0, []\n\n    V = np.zeros((n, maxiter + 1))\n    H = np.zeros((maxiter + 1, maxiter))\n    \n    V[:, 0] = r_start / r_norm\n    \n    s = np.zeros(maxiter + 1)\n    s[0] = r_norm\n    \n    cs = np.zeros(maxiter)\n    sn = np.zeros(maxiter)\n\n    k = -1\n    for k in range(maxiter):\n        # Arnoldi process with Modified Gram-Schmidt\n        w = op(V[:, k])\n        for j in range(k + 1):\n            H[j, k] = np.dot(w, V[:, j])\n            w -= H[j, k] * V[:, j]\n        \n        H[k+1, k] = np.linalg.norm(w)\n\n        # Apply previous Givens rotations to new column of H\n        for j in range(k):\n            temp = cs[j] * H[j, k] + sn[j] * H[j+1, k]\n            H[j+1, k] = -sn[j] * H[j, k] + cs[j] * H[j+1, k]\n            H[j, k] = temp\n        \n        # Calculate new Givens rotation\n        denom = np.sqrt(H[k, k]**2 + H[k+1, k]**2)\n        if denom == 0:\n            cs[k] = 1.0\n            sn[k] = 0.0\n        else:\n            cs[k] = H[k, k] / denom\n            sn[k] = H[k+1, k] / denom\n        \n        # Apply new rotation\n        H[k, k] = cs[k] * H[k, k] + sn[k] * H[k+1, k]\n        H[k+1, k] = 0.0\n        \n        s[k+1] = -sn[k] * s[k]\n        s[k] = cs[k] * s[k]\n        \n        # Calculate true residual and check for convergence\n        true_res_norm = 0\n        if precon_type == 'left':\n            y = scipy.linalg.solve_triangular(H[:k+1, :k+1], s[:k+1], check_finite=False)\n            x_k = x0 + V[:, :k+1] @ y\n            true_res_norm = np.linalg.norm(b - A @ x_k)\n        else: # 'right' or 'none'\n            true_res_norm = abs(s[k+1])\n        \n        res_hist.append(true_res_norm)\n        \n        if true_res_norm  stop_thresh or H[k+1, k] == 0:\n            break\n    \n    # After loop, k is the last iteration index (0-based)\n    # Total iterations = k + 1\n    num_iters = k + 1\n\n    # Solve for the final solution\n    y = scipy.linalg.solve_triangular(H[:num_iters, :num_iters], s[:num_iters], check_finite=False)\n    \n    if precon_type == 'right':\n        y_final = V[:, :num_iters] @ y\n        x_final = M_inv(y_final)\n    else: # 'left' or 'none'\n        x_final = x0 + V[:, :num_iters] @ y\n        \n    return x_final, res_hist\n\ndef setup_case_1():\n    n = 20\n    diag = np.full(n, 2.0)\n    off_diag = np.full(n - 1, -1.0)\n    A = np.diag(diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n    \n    M_diag = np.diag(A)\n    M_inv = lambda v: v / M_diag\n    \n    i = np.arange(1, n + 1)\n    x_true = np.sin(np.pi * i / (n + 1))\n    b = A @ x_true\n    \n    return A, b, M_inv, n\n\ndef setup_case_2():\n    n = 30\n    h = 1.0 / (n + 1)\n    eps = 1e-2\n    c = 1.0\n\n    T_diff = np.diag(np.full(n, 2.0)) + np.diag(np.full(n - 1, -1.0), k=1) + np.diag(np.full(n - 1, -1.0), k=-1)\n    T_adv = np.diag(np.full(n, -1.0)) + np.diag(np.full(n - 1, 1.0), k=-1)\n    \n    A = (eps / h**2) * T_diff + (c / h) * T_adv\n    \n    M_diag = np.diag(A)\n    M_inv = lambda v: v / M_diag\n    \n    b = np.ones(n)\n\n    return A, b, M_inv, n\n\ndef setup_case_3():\n    A, b, _, n = setup_case_1()\n    M_inv = lambda v: v # Identity preconditioner\n    return A, b, M_inv, n\n\ndef solve():\n    test_cases_setup = [setup_case_1, setup_case_2, setup_case_3]\n    results = []\n    \n    for setup_func in test_cases_setup:\n        A, b, M_inv, n = setup_func()\n        tau = 1e-10\n\n        # Left preconditioning\n        x_L, res_L = gmres_custom(A, b, precon_type='left', M_inv=M_inv, tol=tau, maxiter=n)\n        \n        # Right preconditioning\n        x_R, res_R = gmres_custom(A, b, precon_type='right', M_inv=M_inv, tol=tau, maxiter=n)\n\n        # Calculate norm of difference\n        diff_norm = np.linalg.norm(x_L - x_R)\n\n        results.append([diff_norm, res_L, res_R])\n\n    # Final print statement in the exact required format (list of lists, no spaces)\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "2398732"}]}