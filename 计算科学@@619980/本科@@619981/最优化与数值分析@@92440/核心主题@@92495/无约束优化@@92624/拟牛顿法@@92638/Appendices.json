{"hands_on_practices": [{"introduction": "准牛顿法的起点通常是对海森矩阵（Hessian matrix）进行最简单的近似，即单位矩阵 $I$。本练习将向您展示，在这个普遍的初始选择下，算法的第一步如何简化。通过计算初始搜索方向，您会发现它恰好是负梯度方向，这揭示了许多复杂优化算法与最速下降法之间的基本联系。[@problem_id:2195903]", "problem": "一个迭代优化算法被用来寻找二元函数 $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$ 的一个局部最小值。该算法在点 $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$ 处初始化。在第一步（迭代 $k=0$）中，通过求解线性方程组 $B_0 p_0 = -\\nabla f(x_0)$ 来计算搜索方向 $p_0$，其中 $\\nabla f(x_0)$ 是函数 $f$ 在 $x_0$ 处的梯度，而 $B_0$ 是海森矩阵的一个近似。在此过程中，初始近似被选为 $2 \\times 2$ 的单位矩阵 $I$。确定初始搜索方向向量 $p_0$ 的分量。", "solution": "问题要求解初始搜索方向向量 $p_0$，它是线性系统 $B_0 p_0 = -\\nabla f(x_0)$ 的解。我们可以通过以下三个主要步骤解决这个问题：首先，计算函数 $f(x_1, x_2)$ 的梯度；其次，在初始点 $x_0$ 处计算该梯度的值；第三，求解给定的线性系统以得到 $p_0$。\n\n步骤 1：计算函数的梯度。\n函数由 $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$ 给出。\n$f$ 的梯度，记为 $\\nabla f$，是其偏导数组成的向量：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}\n$$\n关于 $x_1$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (\\sin(x_1) + \\cosh(x_2)) = \\cos(x_1)\n$$\n关于 $x_2$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (\\sin(x_1) + \\cosh(x_2)) = \\sinh(x_2)\n$$\n因此，梯度向量是：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\cos(x_1) \\\\ \\sinh(x_2) \\end{pmatrix}\n$$\n\n步骤 2：在初始点 $x_0$ 处计算梯度值。\n初始点给定为 $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$。我们将这些值代入梯度表达式：\n$$\n\\nabla f(x_0) = \\nabla f(0, \\ln(2)) = \\begin{pmatrix} \\cos(0) \\\\ \\sinh(\\ln(2)) \\end{pmatrix}\n$$\n我们计算每个分量。0 的余弦是：\n$$\n\\cos(0) = 1\n$$\n双曲正弦函数定义为 $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$。对于 $y = \\ln(2)$：\n$$\n\\sinh(\\ln(2)) = \\frac{\\exp(\\ln(2)) - \\exp(-\\ln(2))}{2} = \\frac{2 - \\exp(\\ln(2^{-1}))}{2} = \\frac{2 - \\frac{1}{2}}{2} = \\frac{\\frac{3}{2}}{2} = \\frac{3}{4}\n$$\n所以，在初始点处的梯度向量是：\n$$\n\\nabla f(x_0) = \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n\n步骤 3：求解线性系统以得到 $p_0$。\n需求解的系统是 $B_0 p_0 = -\\nabla f(x_0)$。给定 $B_0$ 是 $2 \\times 2$ 的单位矩阵 $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$。\n该系统变为：\n$$\nI p_0 = -\\nabla f(x_0)\n$$\n任何向量乘以单位矩阵，其值保持不变，所以 $p_0 = -\\nabla f(x_0)$。\n代入我们求得的梯度值：\n$$\np_0 = - \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}\n$$\n因此，初始搜索方向向量 $p_0$ 的分量是 $-1$ 和 $-\\frac{3}{4}$。", "answer": "$$\\boxed{\\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}}$$", "id": "2195903"}, {"introduction": "在迈出第一步之后，准牛顿法的精髓在于如何利用上一步的信息来迭代地改进海森矩阵的近似。这个练习将带您完整地走过一次 BFGS 更新循环，这是最流行和最稳健的拟牛顿算法之一。通过动手计算，您将深入理解算法如何通过向量 $s_k$ 和 $y_k$ “学习”目标函数的曲率信息，从而生成一个更精确的二次模型。[@problem_id:2195901]", "problem": "一个优化程序被用来寻找定义在 $x \\in \\mathbb{R}^2$ 上的二次函数 $f(x)$ 的最小值：\n$$f(x) = \\frac{1}{2} x^T A x - b^T x$$\n其中矩阵 $A$ 和向量 $b$ 由下式给出：\n$$A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\n该程序采用一种拟牛顿法，该方法迭代地改进Hessian矩阵的近似。该方法从初始点 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，初始Hessian近似为 $B_0 = I$，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n\n在每次迭代 $k$ 中，通过求解线性系统 $B_k p_k = -\\nabla f(x_k)$ 来计算搜索方向 $p_k$。然后使用固定的单位步长找到下一个点，即 $x_{k+1} = x_k + p_k$。\n\n在确定新点 $x_{k+1}$ 后，使用 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新公式将Hessian近似从 $B_k$ 更新为 $B_{k+1}$：\n$$B_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}$$\n其中 $s_k = x_{k+1} - x_k$ 且 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$。\n\n你的任务是执行此算法的第一次完整迭代，以确定第二步的搜索方向 $p_1$。计算向量 $p_1$ 的第一个分量的值。将最终答案四舍五入到四位有效数字。", "solution": "给定二次函数 $f(x) = \\frac{1}{2} x^{T} A x - b^{T} x$，其中 $A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$，$b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。其梯度为 $\\nabla f(x) = A x - b$。\n\n在初始点 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 且 $B_{0} = I$ 的情况下，通过求解下式来计算第一个搜索方向 $p_{0}$\n$$\nB_{0} p_{0} = -\\nabla f(x_{0}).\n$$\n因为 $\\nabla f(x_{0}) = A x_{0} - b = -b = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$ 且 $B_{0} = I$，我们有\n$$\np_{0} = -\\nabla f(x_{0}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n采用单位步长，下一个点是\n$$\nx_{1} = x_{0} + p_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n定义 $s_{0} = x_{1} - x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。计算梯度：\n$$\n\\nabla f(x_{1}) = A x_{1} - b = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\nabla f(x_{0}) = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\n因此\n$$\ny_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0}) = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n使用BFGS公式更新Hessian近似\n$$\nB_{1} = B_{0} - \\frac{B_{0} s_{0} s_{0}^{T} B_{0}}{s_{0}^{T} B_{0} s_{0}} + \\frac{y_{0} y_{0}^{T}}{y_{0}^{T} s_{0}}.\n$$\n当 $B_{0} = I$, $s_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{0} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ 时，我们有\n$$\ns_{0}^{T} s_{0} = 1, \\quad s_{0} s_{0}^{T} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad y_{0} y_{0}^{T} = \\begin{pmatrix} 9 & 3 \\\\ 3 & 1 \\end{pmatrix}, \\quad y_{0}^{T} s_{0} = 3,\n$$\n所以\n$$\nB_{1} = I - \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 9 & 3 \\\\ 3 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 \\\\ 1 & \\frac{4}{3} \\end{pmatrix}.\n$$\n第二步的搜索方向 $p_{1}$ 可通过求解下式得到\n$$\nB_{1} p_{1} = -\\nabla f(x_{1}) = -\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.\n$$\n令 $p_{1} = \\begin{pmatrix} p \\\\ q \\end{pmatrix}$。则\n$$\n\\begin{cases}\n3p + q = -2, \\\\\np + \\frac{4}{3} q = -1.\n\\end{cases}\n$$\n从第一个方程可得 $q = -2 - 3p$。代入第二个方程：\n$$\np + \\frac{4}{3}(-2 - 3p) = -1 \\;\\Rightarrow\\; p - \\frac{8}{3} - 4p = -1 \\;\\Rightarrow\\; -3p - \\frac{8}{3} = -1 \\;\\Rightarrow\\; -3p = \\frac{5}{3} \\;\\Rightarrow\\; p = -\\frac{5}{9}.\n$$\n因此，$p_1$ 的第一个分量是 $-\\frac{5}{9}$，精确到四位有效数字为 $-0.5556$。", "answer": "$$\\boxed{-0.5556}$$", "id": "2195901"}, {"introduction": "从手动单步计算到构建一个完整的优化求解器是理论联系实际的关键一步。这个高级实践要求您实现一个基于对称一秩（SR1）更新的拟牛顿方法，并处理实际应用中至关重要的数值稳定性问题。您将实现一个“跳过更新”机制来避免数值问题，并确保算法在面对非正定近似时仍能稳健运行，最终将理论知识转化为解决实际问题的代码。[@problem_id:2417336]", "problem": "要求您编写一个完整的程序，通过使用对称秩一更新来迭代地逼近 Hessian 矩阵，从而最小化给定的二次连续可微目标函数。该方法必须维护一个对称矩阵序列 $\\{B_k\\}_{k \\ge 0}$，使用从 $B_k$ 计算出的方向构建步长 $\\{s_k\\}$，并应用一个跳过准则，在相关分母过小时避免秩一更新。当梯度范数降至预设容差以下或达到最大迭代次数时，该方法应终止。\n\n设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 为目标函数，$\\nabla f(x)$ 表示其梯度，而 $B_k \\in \\mathbb{R}^{n \\times n}$ 是在第 $k$ 次迭代时 Hessian 矩阵的对称近似。给定当前迭代点 $x_k \\in \\mathbb{R}^n$，其梯度为 $g_k = \\nabla f(x_k)$，以及求解 $B_k p_k = -g_k$ 得到的方向 $p_k$，定义步长 $s_k = \\alpha_k p_k$（其中步长因子 $\\alpha_k \\in (0,1]$），下一个迭代点 $x_{k+1} = x_k + s_k$，以及梯度位移 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$。定义 $u_k = y_k - B_k s_k$。对称秩一更新公式为\n$$\nB_{k+1} = B_k + \\frac{u_k u_k^\\top}{s_k^\\top u_k}.\n$$\n引入一个由容差 $\\tau > 0$ 参数化的跳过机制，仅当\n$$\n|s_k^\\top u_k| > \\tau \\,\\|s_k\\|_2 \\,\\|u_k\\|_2,\n$$\n时应用更新，否则设置 $B_{k+1} = B_k$（跳过更新）。初始值设为 $B_0 = I$。\n\n您的程序必须为下述每个测试用例产生以下输出：\n- 执行的总迭代次数 $k_{\\text{end}}$（一个整数），\n- 最终的目标函数值 $f(x_{k_{\\text{end}}})$（一个浮点数），\n- 最终的梯度范数 $\\|\\nabla f(x_{k_{\\text{end}}})\\|_2$（一个浮点数），\n- 跳过更新的次数（一个整数），\n- 搜索方向 $p_k$ 未能成为下降方向（即 $g_k^\\top p_k \\ge 0$）并使用了备用方向的次数（一个整数）。\n\n如果求解 $B_k p_k = -g_k$ 不可能或产生了一个非下降方向，则在该次迭代中您必须回退到最速下降方向 $p_k = -g_k$。使用强制执行 Armijo 条件的回溯线搜索\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\, g_k^\\top p_k,\n$$\n固定参数为 $c_1 = 10^{-4}$，缩减因子为 $\\beta = \\tfrac{1}{2}$，从 $\\alpha_k = 1$ 开始，每次乘以因子 $\\beta$ 进行缩减，直到条件满足，或者 $\\alpha_k$ 小于 $10^{-16}$ 为止，此时可按原样采纳该步长。当 $\\|\\nabla f(x_k)\\|_2 \\le 10^{-6}$ 或 $k$ 达到最大迭代预算时终止。\n\n测试套件。将该方法应用于以下六个测试用例。在所有情况下，使用初始 Hessian 矩阵近似 $B_0 = I$，容差 $\\|\\nabla f(x_k)\\|_2 \\le 10^{-6}$，以及最大迭代次数 $200$。\n- 用例 A（理想路径，非凸窄谷）：\n  - 目标函数：二维 Rosenbrock 函数，\n    $$\n    f(x) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.\n    $$\n  - 初始点：$x_0 = (-1.2,\\, 1.0)$。\n  - 跳过容差：$\\tau = 10^{-8}$。\n- 用例 B（与用例 A 相同，但跳过策略更激进）：\n  - 目标函数：与用例 A 相同。\n  - 初始点：$x_0 = (-1.2,\\, 1.0)$。\n  - 跳过容差：$\\tau = 10^{-2}$。\n- 用例 C（病态严格凸二次函数）：\n  - 目标函数：\n    $$\n    f(x) = \\tfrac{1}{2}\\left(1\\cdot x_1^2 + 1000\\cdot x_2^2\\right).\n    $$\n  - 初始点：$x_0 = (1.0,\\, 1.0)$。\n  - 跳过容差：$\\tau = 10^{-8}$。\n- 用例 D（与用例 C 相同，但跳过策略更激进）：\n  - 目标函数：与用例 C 相同。\n  - 初始点：$x_0 = (1.0,\\, 1.0)$。\n  - 跳过容差：$\\tau = 10^{-2}$。\n- 用例 E（具有鞍点结构的非凸函数）：\n  - 目标函数：\n    $$\n    f(x) = x_1^4 - x_1^2 + x_2^2.\n    $$\n  - 初始点：$x_0 = (0.5,\\, 0.5)$。\n  - 跳过容差：$\\tau = 10^{-8}$。\n- 用例 F（与用例 E 相同，但跳过策略更激进）：\n  - 目标函数：与用例 E 相同。\n  - 初始点：$x_0 = (0.5,\\, 0.5)$。\n  - 跳过容差：$\\tau = 10^{-2}$。\n\n不涉及角度单位。不出现物理单位。您的程序必须忠实地实现上述数学规范。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个含六个列表的列表，每个内部列表按 A 到 F 的顺序对应一个测试用例，结构如下\n$$\n[\\;k_{\\text{end}},\\; f(x_{k_{\\text{end}}}),\\; \\|\\nabla f(x_{k_{\\text{end}}})\\|_2,\\; \\text{skipped},\\; \\text{fallback}\\;],\n$$\n其中两个浮点数值必须四舍五入到恰好六位小数。例如，最终的打印输出必须如下所示\n$$\n\\big[\\,[k_1, f_1, g_1, s_1, b_1],\\; [k_2, f_2, g_2, s_2, b_2],\\; \\dots,\\; [k_6, f_6, g_6, s_6, b_6]\\,\\big].\n$$", "solution": "我们通过将对称秩一更新与鲁棒的步长计算和充分下降线搜索相结合，来将迭代过程形式化。目标是使用跨测试问题的一致终止和线搜索策略，考察跳过阈值参数如何影响稳定性和速度。\n\n原理。考虑一个目标函数 $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$ 及其梯度 $\\nabla f(x)$。牛顿法使用 Hessian 矩阵 $\\nabla^2 f(x_k)$ 来定义一个局部二次模型，并通过求解 $\\nabla^2 f(x_k) p_k = -\\nabla f(x_k)$ 得到步长。拟牛顿法用一个近似矩阵 $B_k$ 替代精确的 Hessian 矩阵，该矩阵利用梯度差信息进行更新，以满足一个类割线条件。对称秩一（SR1）更新是通过施加对称性、Frobenius 范数下的最小变化以及沿 $s_k$ 的割线条件（即 $B_{k+1} s_k = y_k$，其中 $s_k = x_{k+1} - x_k$ 且 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$）推导出来的。强制执行此条件的 SR1 更新公式为\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k) (y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k},\n$$\n只要分母不为零。定义 $u_k = y_k - B_k s_k$，并将分母表示为 $d_k = s_k^\\top u_k$。\n\n稳定性机制。当 $|d_k|$ 很小时，SR1 校正项的范数会变得非常大，这可能注入严重的不稳定性（例如曲率估计的大幅变化、失去下降性以及潜在的数值奇异性）。一个常见的保障措施是在 $|d_k|$ 相对于 $s_k$ 和 $u_k$ 的范数乘积很小时跳过更新。我们采用以下准则：\n$$\n|s_k^\\top u_k| > \\tau \\,\\|s_k\\|_2 \\,\\|u_k\\|_2,\n$$\n其中 $\\tau > 0$。如果此条件成立，我们应用秩一更新；否则我们设置 $B_{k+1} = B_k$（跳过）。\n\n方向计算与备用策略。在每次迭代中，我们通过求解 $B_k p_k = -g_k$（其中 $g_k = \\nabla f(x_k)$）来寻找 $p_k$。由于 SR1 不保持正定性，$B_k$ 可能是不定的或奇异的。如果线性求解失败，或者 $g_k^\\top p_k \\ge 0$（不是下降方向），我们就回退到最速下降方向 $p_k = -g_k$。这确保我们总能有一个满足 $g_k^\\top p_k \\le 0$ 的可行方向。\n\n通过充分下降实现全局化。为确保稳定性和目标函数的下降，我们使用带 Armijo 条件的回溯法：\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\, g_k^\\top p_k,\n$$\n其中固定参数 $c_1 = 10^{-4}$，缩减因子 $\\beta = \\tfrac{1}{2}$。我们从 $\\alpha = 1$ 开始，通过 $\\alpha \\leftarrow \\beta \\alpha$ 缩减 $\\alpha$，直到条件满足或 $\\alpha$ 变得极小（低于 $10^{-16}$）。选定的 $\\alpha_k$ 定义了 $s_k = \\alpha_k p_k$ 并得到 $x_{k+1} = x_k + s_k$。\n\n终止条件与度量指标。当 $\\|\\nabla f(x_k)\\|_2 \\le 10^{-6}$ 或 $k$ 达到最大迭代预算（$200$）时，我们终止算法。对于每个测试，我们返回：\n- 迭代次数 $k_{\\text{end}}$，\n- $f(x_{k_{\\text{end}}})$，\n- $\\|\\nabla f(x_{k_{\\text{end}}})\\|_2$，\n- 跳过的 SR1 更新次数，以及\n- 因非下降或求解失败而使用备用策略的次数。\n\n测试套件与覆盖范围。我们使用六个用例：\n- 在二维 Rosenbrock 函数上，从 $x_0 = (-1.2, 1.0)$ 开始，使用 $\\tau = 10^{-8}$ 和 $\\tau = 10^{-2}$。这将探测一个非凸窄谷，测试曲率学习能力以及跳过操作对速度的影响。\n- 在病态凸二次函数 $f(x) = \\tfrac{1}{2} (x_1^2 + 1000 x_2^2)$ 上，从 $x_0 = (1,1)$ 开始，使用 $\\tau = 10^{-8}$ 和 $\\tau = 10^{-2}$。这将测试对条件数的敏感性以及曲率更新相对于跳过的好处。\n- 在非凸四次函数 $f(x) = x_1^4 - x_1^2 + x_2^2$ 上，从 $x_0 = (0.5, 0.5)$ 开始，使用 $\\tau = 10^{-8}$ 和 $\\tau = 10^{-2}$，其包含一个鞍点方向，用于探测下降方向备用策略和稳定性。\n\n权衡分析。当 $\\tau$ 较小（例如 $\\tau = 10^{-8}$）时，该方法会更频繁地应用 SR1 校正。这往往会加速收敛，因为曲率信息被频繁更新，但当 $|s_k^\\top u_k|$ 很小但未低于阈值时，这也增加了大校正的风险，可能导致不定性或不稳定的步长，我们通过备用策略和线搜索来缓解此问题。当 $\\tau$ 较大（例如 $\\tau = 10^{-2}$）时，该方法会跳过更多更新。这通过避免被认为是病态的校正来稳定序列 $\\{B_k\\}$，但会减慢曲率学习，并可能增加迭代次数或对最速下降行为的依赖。该程序通过迭代次数、最终梯度范数、跳过更新的次数以及备用事件的次数来量化这种权衡，从而可以直接比较 $\\tau = 10^{-8}$ 和 $\\tau = 10^{-2}$ 在三个目标函数上的表现。\n\n正确性与第一性原理。该方法直接遵循：\n- 割线条件 $B_{k+1} s_k = y_k$，\n- SR1 最小变化推导，\n- Armijo 充分下降规则，以及\n- 通过 $g_k^\\top p_k < 0$ 定义的下降方向。\n除了范数相关的跳过阈值外，没有使用任何特定于问题的人工启发式方法，所有量都根据 $f$、$\\nabla f$ 和更新规则的定义计算得出。", "answer": "```python\nimport numpy as np\n\ndef rosenbrock(x: np.ndarray) -> float:\n    # f(x1, x2) = 100 (x2 - x1^2)^2 + (1 - x1)^2\n    x1, x2 = x[0], x[1]\n    return 100.0 * (x2 - x1**2)**2 + (1.0 - x1)**2\n\ndef rosenbrock_grad(x: np.ndarray) -> np.ndarray:\n    x1, x2 = x[0], x[1]\n    df_dx1 = -400.0 * (x2 - x1**2) * x1 - 2.0 * (1.0 - x1)\n    df_dx2 = 200.0 * (x2 - x1**2)\n    return np.array([df_dx1, df_dx2], dtype=float)\n\ndef quad_ic(x: np.ndarray) -> float:\n    # f(x) = 1/2 (1*x1^2 + 1000*x2^2)\n    x1, x2 = x[0], x[1]\n    return 0.5 * (1.0 * x1**2 + 1000.0 * x2**2)\n\ndef quad_ic_grad(x: np.ndarray) -> np.ndarray:\n    x1, x2 = x[0], x[1]\n    return np.array([1.0 * x1, 1000.0 * x2], dtype=float)\n\ndef quartic_saddle(x: np.ndarray) -> float:\n    # f(x) = x1^4 - x1^2 + x2^2\n    x1, x2 = x[0], x[1]\n    return x1**4 - x1**2 + x2**2\n\ndef quartic_saddle_grad(x: np.ndarray) -> np.ndarray:\n    x1, x2 = x[0], x[1]\n    return np.array([4.0 * x1**3 - 2.0 * x1, 2.0 * x2], dtype=float)\n\ndef armijo_backtracking(f, grad, x, p, gTp, c1=1e-4, beta=0.5, alpha0=1.0, min_alpha=1e-16):\n    alpha = alpha0\n    fx = f(x)\n    while True:\n        xn = x + alpha * p\n        if f(xn) <= fx + c1 * alpha * gTp:\n            break\n        alpha *= beta\n        if alpha < min_alpha:\n            break\n    return alpha\n\ndef sr1_optimize(f, grad, x0, tau, max_iter=200, tol=1e-6):\n    n = len(x0)\n    x = x0.copy()\n    B = np.eye(n)\n    skipped_updates = 0\n    fallback_count = 0\n    k = 0\n\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = np.linalg.norm(g)\n        if gnorm <= tol:\n            break\n\n        # Attempt to solve B p = -g\n        descent_ok = False\n        try:\n            p = np.linalg.solve(B, -g)\n            gTp = float(np.dot(g, p))\n            if gTp < 0.0 and np.all(np.isfinite(p)):\n                descent_ok = True\n        except np.linalg.LinAlgError:\n            descent_ok = False\n\n        if not descent_ok:\n            p = -g\n            gTp = -float(np.dot(g, g))\n            fallback_count += 1\n\n        # Armijo backtracking line search\n        alpha = armijo_backtracking(f, grad, x, p, gTp, c1=1e-4, beta=0.5, alpha0=1.0, min_alpha=1e-16)\n\n        s = alpha * p\n        x_new = x + s\n        g_new = grad(x_new)\n        y = g_new - g\n\n        # SR1 update with skipping\n        u = y - B.dot(s)\n        denom = float(np.dot(s, u))\n        norm_s = np.linalg.norm(s)\n        norm_u = np.linalg.norm(u)\n\n        # Apply skip criterion only if norms are finite and nonzero\n        apply_update = True\n        if norm_s == 0.0 or not np.isfinite(norm_s) or not np.isfinite(norm_u):\n            apply_update = False\n        else:\n            if abs(denom) <= tau * norm_s * norm_u:\n                apply_update = False\n\n        if apply_update:\n            # Rank-one update\n            B = B + np.outer(u, u) / denom\n        else:\n            skipped_updates += 1\n\n        x = x_new\n\n    # Final metrics\n    final_f = float(f(x))\n    final_gnorm = float(np.linalg.norm(grad(x)))\n    return k, final_f, final_gnorm, skipped_updates, fallback_count\n\ndef solve():\n    # Define test cases A-F\n    test_cases = [\n        # (name, f, grad, x0, tau)\n        (\"A_rosen_tau1e-8\", rosenbrock, rosenbrock_grad, np.array([-1.2, 1.0], dtype=float), 1e-8),\n        (\"B_rosen_tau1e-2\", rosenbrock, rosenbrock_grad, np.array([-1.2, 1.0], dtype=float), 1e-2),\n        (\"C_quad_tau1e-8\", quad_ic, quad_ic_grad, np.array([1.0, 1.0], dtype=float), 1e-8),\n        (\"D_quad_tau1e-2\", quad_ic, quad_ic_grad, np.array([1.0, 1.0], dtype=float), 1e-2),\n        (\"E_quartic_tau1e-8\", quartic_saddle, quartic_saddle_grad, np.array([0.5, 0.5], dtype=float), 1e-8),\n        (\"F_quartic_tau1e-2\", quartic_saddle, quartic_saddle_grad, np.array([0.5, 0.5], dtype=float), 1e-2),\n    ]\n\n    results = []\n    for _, f, grad, x0, tau in test_cases:\n        k, fval, gnorm, skipped, fallback = sr1_optimize(f, grad, x0, tau, max_iter=200, tol=1e-6)\n        # Round floats to exactly six decimals as required\n        results.append([int(k), float(f\"{fval:.6f}\"), float(f\"{gnorm:.6f}\"), int(skipped), int(fallback)])\n\n    # Print in the exact required format: a single line with the list of lists\n    # Ensure no extra text\n    def format_inner(lst):\n        # lst: [k, f, g, s, b]\n        return f\"[{lst[0]},{lst[1]:.6f},{lst[2]:.6f},{lst[3]},{lst[4]}]\"\n    print(f\"[{','.join(format_inner(r) for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2417336"}]}