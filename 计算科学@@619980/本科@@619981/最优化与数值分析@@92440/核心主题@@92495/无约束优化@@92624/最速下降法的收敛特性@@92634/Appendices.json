{"hands_on_practices": [{"introduction": "为了掌握最速下降法，我们首先从最简单的情形入手。这个练习将向您展示，对于一维二次函数，采用精确线性搜索的最速下降法仅需一步迭代即可找到最小值。通过这个理想化的例子 [@problem_id:2162624]，您将建立对该算法核心机制的直观理解。", "problem": "一位优化专家正在使用最速下降算法来寻找一个一维目标函数的最小值。该函数为：\n$f(x) = 3x^2 - 7x + 11$\n\n最速下降算法的迭代过程由以下更新规则定义：\n$x_{k+1} = x_k - \\alpha_k f'(x_k)$\n其中 $x_k$ 是第 $k$ 次迭代时的当前位置，$f'(x_k)$ 是函数在 $x_k$ 处的导数，而 $\\alpha_k$ 是步长。\n\n该专家使用“精确线搜索”来确定每次迭代中的最优步长 $\\alpha_k$。该方法通过选择 $\\alpha_k$ 来最小化函数 $f$ 在从 $x_k$ 开始的搜索方向上的值。换句话说，对于给定的 $x_k$，选择 $\\alpha_k$ 以最小化关于步长的新函数 $g(\\alpha) = f(x_k - \\alpha f'(x_k))$。\n\n从初始点 $x_0 = 5$ 开始，用精确线搜索计算经过一次最速下降算法迭代后的位置 $x_1$。请用分数表示你的答案。", "solution": "问题要求计算从 $x_0 = 5$ 开始，经过一次最速下降算法迭代后的位置 $x_1$。需要最小化的函数是 $f(x) = 3x^2 - 7x + 11$。\n\n最速下降算法的更新规则如下：\n$x_{k+1} = x_k - \\alpha_k f'(x_k)$\n\n对于第一次迭代（从 $k=0$ 到 $k=1$），规则是：\n$x_1 = x_0 - \\alpha_0 f'(x_0)$\n\n首先，我们需要计算函数 $f(x)$ 的导数：\n$f'(x) = \\frac{d}{dx}(3x^2 - 7x + 11) = 6x - 7$\n\n接下来，我们在起始点 $x_0 = 5$ 处计算该导数的值：\n$f'(x_0) = f'(5) = 6(5) - 7 = 30 - 7 = 23$\n\n现在我们得到了以未知步长 $\\alpha_0$ 表示的更新规则：\n$x_1 = 5 - \\alpha_0 (23)$\n\n题目说明使用精确线搜索来找到 $\\alpha_0$。这意味着我们必须选择 $\\alpha_0$ 来最小化函数 $g(\\alpha) = f(x_0 - \\alpha f'(x_0))$。代入已知值：\n$g(\\alpha) = f(5 - 23\\alpha)$\n\n为了找到最小化 $g(\\alpha)$ 的 $\\alpha$ 值，我们对 $g(\\alpha)$ 关于 $\\alpha$ 求导，并令其等于零。我们使用链式法则进行求导：\n$g'(\\alpha) = \\frac{d}{d\\alpha} f(5 - 23\\alpha) = f'(5 - 23\\alpha) \\cdot \\frac{d}{d\\alpha}(5 - 23\\alpha)$\n$g'(\\alpha) = f'(5 - 23\\alpha) \\cdot (-23)$\n\n令 $g'(\\alpha_0) = 0$ 以求得最优步长 $\\alpha_0$：\n$f'(5 - 23\\alpha_0) \\cdot (-23) = 0$\n\n因为 $-23 \\neq 0$，此条件简化为：\n$f'(5 - 23\\alpha_0) = 0$\n\n导数内部的表达式 $5 - 23\\alpha_0$ 正是我们目标点 $x_1$ 的公式。因此，最优步长的条件意味着：\n$f'(x_1) = 0$\n\n这意味着使用精确线搜索进行一次迭代后找到的点 $x_1$，是原函数 $f(x)$ 导数为零的点。对于一个凸二次函数，该点即为全局最小值点。\n\n我们现在可以使用导数表达式 $f'(x) = 6x - 7$ 来求解 $x_1$：\n$f'(x_1) = 6x_1 - 7 = 0$\n$6x_1 = 7$\n$x_1 = \\frac{7}{6}$\n\n因此，从 $x_0=5$ 开始，使用精确线搜索的最速下降法进行单次迭代，直接得到了函数的最小值点 $x_1 = 7/6$。", "answer": "$$\\boxed{\\frac{7}{6}}$$", "id": "2162624"}, {"introduction": "最速下降法一个著名的特性是在求解病态问题时表现出的“之字形”收敛路径。本练习将通过一个精心设计的二维二次函数来揭示这一现象。您将计算出迭代点在两条不同直线上交替前进时形成的夹角，从而亲身体验并量化这种看似低效的收敛行为 [@problem_id:2162600]。", "problem": "考虑最小化二次函数 $f(x, y) = \\frac{1}{2}(x^2 + \\gamma y^2)$ 的优化问题，其中 $\\gamma$ 是一个正常数。我们使用最速下降算法来寻找该函数的最小值。该算法从一个初始点 $\\mathbf{x}_0$ 开始，生成一个点序列 $\\{\\mathbf{x}_k\\}$。更新规则由 $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k$ 给出，其中 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$ 是 $f$ 在 $\\mathbf{x}_k$ 处的梯度。步长 $\\alpha_k$ 在每一步选择时都以最小化 $f(\\mathbf{x}_{k+1})$ 为目标；这种方法被称为精确线搜索。\n\n对于形式为 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x}$ 的二次函数，其中 $Q$ 是一个正定矩阵，第 $k$ 次迭代的最优步长 $\\alpha_k$ 由以下公式给出：\n$$\n\\alpha_k = \\frac{\\mathbf{g}_k^T \\mathbf{g}_k}{\\mathbf{g}_k^T Q \\mathbf{g}_k}\n$$\n假设我们设置 $\\gamma = 49$ 并从点 $\\mathbf{x}_0 = (49, 1)^T$ 开始算法。可以证明，对于这种特定设置，偶数索引的迭代点（$ \\mathbf{x}_0, \\mathbf{x}_2, \\mathbf{x}_4, \\dots $）位于一条穿过原点的直线上，而奇数索引的迭代点（$ \\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_5, \\dots $）位于另一条也穿过原点的直线上。这个点序列形成一条“之”字形路径，收敛到位于 $(0, 0)$ 的最小值点。\n\n确定这两条直线之间锐角 $\\theta$ 的余弦值。", "solution": "我们有 $f(x,y)=\\frac{1}{2}(x^{2}+\\gamma y^{2})$，其中 $Q=\\mathrm{diag}(1,\\gamma)$，因此对于 $\\mathbf{x}_{k}=(x_{k},y_{k})^{T}$，梯度为 $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})=(x_{k},\\gamma y_{k})^{T}$。精确线搜索的步长为\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{T}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{T}Q\\mathbf{g}_{k}}=\\frac{x_{k}^{2}+\\gamma^{2}y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}.\n$$\n最速下降更新 $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha_{k}\\mathbf{g}_{k}$ 按分量给出\n$$\nx_{k+1}=(1-\\alpha_{k})x_{k},\\qquad y_{k+1}=(1-\\alpha_{k}\\gamma)y_{k}.\n$$\n定义斜率 $m_{k}=y_{k}/x_{k}$（穿过原点的直线）。那么\n$$\nm_{k+1}=\\frac{y_{k+1}}{x_{k+1}}=\\frac{1-\\alpha_{k}\\gamma}{1-\\alpha_{k}}\\,m_{k}.\n$$\n使用 $\\alpha_{k}$ 的表达式，\n$$\n1-\\alpha_{k}=\\frac{\\gamma^{2}(\\gamma-1)y_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}},\n$$\n$$\n1-\\alpha_{k}\\gamma=\\frac{(1-\\gamma)x_{k}^{2}}{x_{k}^{2}+\\gamma^{3}y_{k}^{2}}.\n$$\n因此\n$$\n\\frac{1-\\alpha_{k}\\gamma}{1-\\alpha_{k}}=\\frac{(1-\\gamma)x_{k}^{2}}{\\gamma^{2}(\\gamma-1)y_{k}^{2}}=-\\frac{x_{k}^{2}}{\\gamma^{2}y_{k}^{2}},\n$$\n从而\n$$\nm_{k+1}=-\\frac{x_{k}^{2}}{\\gamma^{2}y_{k}^{2}}\\,m_{k}=-\\frac{1}{\\gamma^{2}m_{k}}.\n$$\n由此可得 $m_{k+2}=-\\frac{1}{\\gamma^{2}m_{k+1}}=-\\frac{1}{\\gamma^{2}\\left(-\\frac{1}{\\gamma^{2}m_{k}}\\right)}=m_{k}$，因此所有偶数斜率相等，所有奇数斜率也相等，这证实了存在两条直线。\n\n当 $\\gamma=49$ 且 $\\mathbf{x}_{0}=(49,1)^{T}$ 时，我们有 $m_{0}=y_{0}/x_{0}=1/49=1/\\gamma$，所以\n$$\nm_{1}=-\\frac{1}{\\gamma^{2}m_{0}}=-\\frac{1}{\\gamma^{2}\\cdot(1/\\gamma)}=-\\frac{1}{\\gamma}=-\\frac{1}{49}.\n$$\n因此，这两条直线的斜率分别为 $m$ 和 $-m$，其中 $m=1/49$。斜率分别为 $m$ 和 $-m$ 的两条直线之间的锐角 $\\theta$ 的余弦值等于\n$$\n\\cos\\theta=\\frac{(1,m)\\cdot(1,-m)}{\\|(1,m)\\|\\,\\|(1,-m)\\|}=\\frac{1-m^{2}}{1+m^{2}}.\n$$\n代入 $m=1/49$，\n$$\n\\cos\\theta=\\frac{1-\\frac{1}{49^{2}}}{1+\\frac{1}{49^{2}}}=\\frac{49^{2}-1}{49^{2}+1}=\\frac{2400}{2402}=\\frac{1200}{1201}.\n$$", "answer": "$$\\boxed{\\frac{1200}{1201}}$$", "id": "2162600"}, {"introduction": "理解了“之字形”行为后，我们来探讨一个重要的实际问题：我们应该何时停止算法？这个练习探讨了一种常见的停止准则——梯度范数足够小——在病态问题中的陷阱。您将发现，即使梯度范数 $\\\\|\\nabla f(x_k)\\|$ 已经满足了停止条件，当前点 $x_k$ 仍可能离真正的最小值点很远 [@problem_id:2162662]，这突显了选择合适停止准则的重要性。", "problem": "一个优化算法用于寻找凸二次函数 $f(x_1, x_2) = \\frac{1}{2} (x_1^2 + \\lambda x_2^2)$ 的最小值，其中 $\\lambda$ 是一个小的正常数，代表了该问题的条件状况。该函数的唯一全局最小值点位于 $x^* = (0, 0)$。该算法使用一个标准终止准则：当梯度的欧几里得范数 $\\|\\nabla f(x_k)\\|$ 小于预定义的容差 $\\epsilon$ 时，它停止迭代。\n\n在某个特定应用中，函数参数为 $\\lambda = 10^{-8}$，终止容差设置为 $\\epsilon = 10^{-5}$。算法在一个点 $x_{term} = (x_1, x_2)$ 停止，该点恰好满足终止条件，即 $\\|\\nabla f(x_{term})\\| = \\epsilon$。这种情况对应于终止区域的边界。\n\n这个小的梯度范数并不能保证点 $x_{term}$ 接近真实的最小值点 $x^*$。你的任务是量化这个最坏情况误差。对于满足终止条件 $\\|\\nabla f(x_{term})\\| = \\epsilon$ 的点，计算其与真实最小值点之间的最大可能欧几里得距离 $\\|x_{term} - x^*\\|$。", "solution": "给定凸二次函数 $f(x_{1}, x_{2}) = \\frac{1}{2}\\left(x_{1}^{2} + \\lambda x_{2}^{2}\\right)$，其唯一极小值点为 $x^{*} = (0, 0)$。其梯度为\n$$\n\\nabla f(x_{1}, x_{2}) = \\begin{pmatrix} x_{1} \\\\ \\lambda x_{2} \\end{pmatrix}.\n$$\n终止条件为 $\\|\\nabla f(x_{term})\\| = \\epsilon$，这明确地施加了约束\n$$\nx_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}.\n$$\n我们需要计算到极小值点的最大可能欧几里得距离，\n$$\n\\|x_{term} - x^{*}\\| = \\sqrt{x_{1}^{2} + x_{2}^{2}},\n$$\n其中 $(x_{1}, x_{2})$ 满足上述约束条件。这是一个约束优化问题\n$$\n\\max_{x_{1}, x_{2}} \\; x_{1}^{2} + x_{2}^{2} \\quad \\text{subject to} \\quad x_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}.\n$$\n使用拉格朗日乘子法，定义\n$$\n\\mathcal{L}(x_{1}, x_{2}, \\mu) = x_{1}^{2} + x_{2}^{2} - \\mu \\left(x_{1}^{2} + \\lambda^{2} x_{2}^{2} - \\epsilon^{2}\\right).\n$$\n驻点条件为\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{1}} = 2 x_{1} - 2 \\mu x_{1} = 2 x_{1} (1 - \\mu) = 0,\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{2}} = 2 x_{2} - 2 \\mu \\lambda^{2} x_{2} = 2 x_{2} (1 - \\mu \\lambda^{2}) = 0,\n$$\n以及约束条件 $x_{1}^{2} + \\lambda^{2} x_{2}^{2} = \\epsilon^{2}$。\n\n从驻点条件方程可知，要么 $x_{1} = 0$ 或 $\\mu = 1$，要么 $x_{2} = 0$ 或 $\\mu = \\frac{1}{\\lambda^{2}}$。因为 $\\lambda \\neq 1$，我们不能同时有 $\\mu = 1$ 和 $\\mu = \\frac{1}{\\lambda^{2}}$，所以极值在其中一个坐标为零时出现。\n\n- 如果 $x_{2} = 0$，则约束条件给出 $x_{1}^{2} = \\epsilon^{2}$，目标值为 $x_{1}^{2} + x_{2}^{2} = \\epsilon^{2}$，因此距离为 $\\epsilon$。\n- 如果 $x_{1} = 0$，则约束条件给出 $\\lambda^{2} x_{2}^{2} = \\epsilon^{2}$，所以 $x_{2}^{2} = \\frac{\\epsilon^{2}}{\\lambda^{2}}$，目标值为 $x_{1}^{2} + x_{2}^{2} = \\frac{\\epsilon^{2}}{\\lambda^{2}}$，因此距离为 $\\frac{\\epsilon}{\\lambda}$。\n\n在这两个候选值之间，因为 $\\lambda \\in (0, 1)$，最大值在 $x_{1} = 0$ 时出现，得到最坏情况距离\n$$\n\\|x_{term} - x^{*}\\|_{\\max} = \\frac{\\epsilon}{\\lambda}.\n$$\n代入给定值 $\\lambda = 10^{-8}$ 和 $\\epsilon = 10^{-5}$，\n$$\n\\|x_{term} - x^{*}\\|_{\\max} = \\frac{10^{-5}}{10^{-8}} = 10^{3} = 1 \\times 10^{3}.\n$$", "answer": "$$\\boxed{1 \\times 10^{3}}$$", "id": "2162662"}]}