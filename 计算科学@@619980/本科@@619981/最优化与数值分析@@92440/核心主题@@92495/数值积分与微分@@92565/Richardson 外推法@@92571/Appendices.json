{"hands_on_practices": [{"introduction": "理论学习之后，最好的巩固方式莫过于亲手实践。第一个练习将理查森外推法置于一个真实的工程场景——航天探测器再入大气层的减速度估算。本题旨在练习将理查森外推公式直接应用于一个一阶精度 ($O(h)$) 的数值方法，通过结合两次不同步长的模拟结果，来获取一个更高精度的估计值。这个练习将帮助你牢固掌握理查森外推法的基本计算步骤。[@problem_id:2197905]", "problem": "一个航空航天工程师团队正在模拟一个新型太空探测器的大气层再入过程。他们特别关注的是估算探测器所经历的最大减速度，这是结构设计的一个关键参数。他们用来求解运动方程的数值算法具有一阶全局截断误差，这意味着估计值的误差为 $O(h)$ 阶，其中 $h$ 是模拟中使用的时间步长。\n\n设当模拟以时间步长 $h$ 运行时，$D(h)$ 为估算出的最大减速度。\n该团队进行了两次模拟：\n1.  当时间步长为 $h_1 = 0.4$ 秒时，他们测得最大减速度为 $D(0.4) = 58.6$ m/s$^{2}$。\n2.  当时间步长减小到 $h_2 = 0.2$ 秒时，他们测得最大减速度为 $D(0.2) = 59.3$ m/s$^{2}$。\n\n为了在不使用更小时间步长进行另一次耗时模拟的情况下获得更精确的结果，工程师们决定对这两个估计值应用 Richardson 外推法。计算改进后的最大减速度估计值。答案以 m/s$^{2}$ 为单位，并四舍五入到三位有效数字。", "solution": "由于该数值方法具有一阶全局截断误差，估算量满足以下渐近误差模型\n$$\nD(h) = D^{\\ast} + C h + O(h^{2}),\n$$\n其中 $D^{\\ast}$ 是真实的最大减速度，$C$ 是一个与 $h$ 无关的常数。\n\n设 $h_{1} = 0.4$，$h_{2} = 0.2$，并定义细化比 $r = \\frac{h_{1}}{h_{2}} = 2$。则\n$$\nD(h_{1}) = D^{\\ast} + C h_{1} + O(h_{1}^{2}), \\quad D(h_{2}) = D^{\\ast} + C \\frac{h_{1}}{r} + O\\!\\left(\\frac{h_{1}^{2}}{r^{2}}\\right).\n$$\n通过 Richardson 外推法消去 $O(h)$ 项可得\n$$\nD_{\\text{RE}} = \\frac{r D(h_{2}) - D(h_{1})}{r - 1} = D(h_{2}) + \\frac{D(h_{2}) - D(h_{1})}{r - 1} = D^{\\ast} + O(h_{1}^{2}).\n$$\n代入 $r=2$、$D(0.4)=58.6$ 和 $D(0.2)=59.3$，\n$$\nD_{\\text{RE}} = \\frac{2 \\cdot 59.3 - 58.6}{2 - 1} = 118.6 - 58.6 = 60.0.\n$$\n四舍五入到三位有效数字，改进后的估计值为 $60.0$ m/s$^{2}$。", "answer": "$$\\boxed{60.0}$$", "id": "2197905"}, {"introduction": "掌握了基本计算后，深入理解其背后的理论变得至关重要。本题是一个概念性的“思想实验”，它不要求复杂的计算，而是挑战你分析不同数值方法的误差结构。通过比较两种误差形式（一种包含 $h$ 的所有次幂，另一种仅包含偶次幂）的方法，你将揭示误差结构如何影响理查森外推法的效率，并理解为何该方法在某些常见情况下（如中心差分法）表现得异常强大。[@problem_id:2197912]", "problem": "两位工程师，Anya和Ben，的任务是数值估计一个物理常数的值，其精确值用 $L$ 表示。他们使用了不同的数值方法。\n\nAnya使用方法A，对于给定的步长 $h$，该方法产生一个近似值 $A(h)$。已知该方法的误差具有 $h$ 的幂级数形式：\n$A(h) = L + c_1 h + c_2 h^2 + c_3 h^3 + O(h^4)$\n其中 $c_1, c_2, c_3$ 是不依赖于 $h$ 的非零常数。该方法的精度阶数是误差项 $A(h) - L$ 中 $h$ 的最低次幂。\n\nBen使用方法B，该方法产生一个近似值 $B(h)$。这个方法更为复杂，其误差级数仅包含 $h$ 的偶数次幂：\n$B(h) = L + d_2 h^2 + d_4 h^4 + d_6 h^6 + O(h^8)$\n其中 $d_2, d_4, d_6$ 是不依赖于 $h$ 的非零常数。\n\n为了改进他们的初始结果，两位工程师都决定应用单步Richardson外推法。他们各自计算了两个近似值：一个使用步长 $h$，另一个使用步长 $h/2$。然后他们将这两个近似值结合起来，创建一个新的、更准确的估计值。\n\n设方法A的初始精度阶数为 $p_{A, \\text{orig}}$，方法B的初始精度阶数为 $p_{B, \\text{orig}}$。应用Richardson外推法后，他们的新估计值分别具有精度阶数 $p_{A, \\text{new}}$ 和 $p_{B, \\text{new}}$。精度阶数的“改进量”定义为新旧阶数之差，即对于Anya是 $\\Delta p_A = p_{A, \\text{new}} - p_{A, \\text{orig}}$，对于Ben是 $\\Delta p_B = p_{B, \\text{new}} - p_{B, \\text{orig}}$。\n\n以下哪个陈述正确地比较了改进量 $\\Delta p_A$ 和 $\\Delta p_B$？\n\nA. $\\Delta p_A > \\Delta p_B$\n\nB. $\\Delta p_A = \\Delta p_B$\n\nC. $\\Delta p_A < \\Delta p_B$\n\nD. 如果不知道 $h$、$c_i$ 和 $d_i$ 的值，就无法比较。\n\nE. 两种方法都未能改进，所以 $\\Delta p_A = \\Delta p_B = 0$。", "solution": "对于一个误差展开式为 $S(h)=L+\\sum_{k} a_{k} h^{p_{k}}$ 的方法，其精度阶数是具有非零系数的最小指数 $p_{k}$。对于细化比 $r=2$ 和主阶 $p$，单步Richardson外推法使用以下公式\n$$\nR(h)=\\frac{2^{p} S\\!\\left(\\frac{h}{2}\\right)-S(h)}{2^{p}-1},\n$$\n该公式消除了 $h^{p}$ 项，并将阶数提升到误差级数中下一个非零次幂。\n\n对于方法A，\n$$\nA(h)=L+c_{1} h+c_{2} h^{2}+c_{3} h^{3}+O(h^{4}),\n$$\n所以 $p_{A,\\text{orig}}=1$。在 $h/2$ 处展开：\n$$\nA\\!\\left(\\frac{h}{2}\\right)=L+\\frac{c_{1}}{2} h+\\frac{c_{2}}{4} h^{2}+\\frac{c_{3}}{8} h^{3}+O(h^{4}).\n$$\n应用 $p=1$ 的Richardson外推法：\n$$\n2 A\\!\\left(\\frac{h}{2}\\right)-A(h)=\\left(2L+c_{1} h+\\frac{c_{2}}{2} h^{2}+\\frac{c_{3}}{4} h^{3}+\\cdots\\right)-\\left(L+c_{1} h+c_{2} h^{2}+c_{3} h^{3}+\\cdots\\right)\n$$\n$$\n= L-\\frac{c_{2}}{2} h^{2}-\\frac{3 c_{3}}{4} h^{3}+O(h^{4}).\n$$\n因此\n$$\nR_{A}(h)=\\frac{2 A\\!\\left(\\frac{h}{2}\\right)-A(h)}{1}=L-\\frac{c_{2}}{2} h^{2}-\\frac{3 c_{3}}{4} h^{3}+O(h^{4}),\n$$\n所以新的主误差项是 $O(h^{2})$，即 $p_{A,\\text{new}}=2$ 并且\n$$\n\\Delta p_{A}=p_{A,\\text{new}}-p_{A,\\text{orig}}=2-1=1.\n$$\n\n对于方法B，\n$$\nB(h)=L+d_{2} h^{2}+d_{4} h^{4}+d_{6} h^{6}+O(h^{8}),\n$$\n所以 $p_{B,\\text{orig}}=2$。在 $h/2$ 处展开：\n$$\nB\\!\\left(\\frac{h}{2}\\right)=L+\\frac{d_{2}}{4} h^{2}+\\frac{d_{4}}{16} h^{4}+\\frac{d_{6}}{64} h^{6}+O(h^{8}).\n$$\n应用 $p=2$ 的Richardson外推法：\n$$\n4 B\\!\\left(\\frac{h}{2}\\right)-B(h)=\\left(4L+d_{2} h^{2}+\\frac{d_{4}}{4} h^{4}+\\frac{d_{6}}{16} h^{6}+\\cdots\\right)-\\left(L+d_{2} h^{2}+d_{4} h^{4}+d_{6} h^{6}+\\cdots\\right)\n$$\n$$\n=3L-\\frac{3}{4} d_{4} h^{4}-\\frac{15}{16} d_{6} h^{6}+O(h^{8}).\n$$\n除以 $4-1=3$：\n$$\nR_{B}(h)=L-\\frac{1}{4} d_{4} h^{4}-\\frac{5}{16} d_{6} h^{6}+O(h^{8}),\n$$\n所以新的主误差项是 $O(h^{4})$，即 $p_{B,\\text{new}}=4$ 并且\n$$\n\\Delta p_{B}=p_{B,\\text{new}}-p_{B,\\text{orig}}=4-2=2.\n$$\n\n因此，$\\Delta p_{A}=1$ 且 $\\Delta p_{B}=2$，得出 $\\Delta p_{A}<\\Delta p_{B}$。正确选项是C。", "answer": "$$\\boxed{C}$$", "id": "2197912"}, {"introduction": "将理论与实践相结合，是科学家和工程师必备的关键技能。最后一个练习是一个编程挑战，要求你为数值微分实现理查森外推法。通过实现并比较一个朴素的递归算法和一个高效的迭代式表格算法，你将亲身体会到算法设计、计算成本和数值稳定性等关键概念的重要性，并深入理解该方法在实际应用中的权衡。[@problem_id:2435030]", "problem": "为一个实值函数实现两个基于 Richardson 外推法的、在数学上等价的一阶导数估计量。目标是比较它们在应用于一个小规模测试套件时的计算成本和数值稳定性。假设一个可微函数表示为 $f(x)$，其在点 $x=x_{0}$ 处的导数表示为 $f^{\\prime}(x_{0})$。考虑对于步长 $h>0$ 定义的对称有限差分基本近似：\n$$\nD(h) \\equiv \\frac{f(x_{0}+h)-f(x_{0}-h)}{2h},\n$$\n对于足够光滑的函数 $f$，该近似的截断误差从 $h^{2}$ 阶开始。请实现以下两个使用 Richardson 外推法（加细比为 $r$）来改进 $D(h)$ 的估计量：\n- 一个递归的 Richardson 外推估计量，它通过对更小的子问题进行递归调用来计算外推值，并且不缓存中间结果。\n- 一个基于表格的迭代 Richardson 外推估计量，它自底向上地构建外推值的三角形表格。\n\n对于这两个估计量，均使用相同的加细比 $r=2$，并将角度解释为弧度。对于下述每个测试用例，两个估计量都必须生成与连续外推层级 $k=0,1,\\dots,L$ 对应的外推估计值序列，其中 $k=0$ 表示基本近似，$k=L$ 表示最高外推层级。对于每种方法和每个测试用例，报告：\n- 在层级 $k=L$ 处的最终外推估计值，\n- 该最终估计值相对于精确导数的绝对误差，\n- 生成直至层级 $k=L$ 的整个估计值序列所需的函数 $f(x)$ 的总调用次数，\n- 一个布尔值，指示在各层级 $k=0,1,\\dots,L$ 上的绝对误差是否单调非增。\n\n你的程序必须为以下测试套件实现上述要求，其中每个测试指定了函数 $f(x)$、求值点 $x_{0}$、初始步长 $h_{0}$以及外推层级数 $L$：\n- 测试 A（顺利路径，光滑，中等步长）：$f(x)=e^{x}$，$x_{0}=1.0$，$h_{0}=0.1$，$L=4$，$r=2$。精确导数为 $f^{\\prime}(x)=e^{x}$ 在 $x=1.0$ 处的值。\n- 测试 B（光滑三角函数，中等步长）：$f(x)=\\sin(x)$，$x_{0}=1.3$，$h_{0}=0.2$，$L=5$，$r=2$。精确导数为 $f^{\\prime}(x)=\\cos(x)$ 在 $x=1.3$ 处的值。\n- 测试 C（振荡函数，局部频率较高）：$f(x)=\\sin(50\\,x)$，$x_{0}=0.2$，$h_{0}=0.05$，$L=4$，$r=2$。精确导数为 $f^{\\prime}(x)=50\\cos(50\\,x)$ 在 $x=0.2$ 处的值。\n- 测试 D（舍入误差敏感，非常小的步长）：$f(x)=e^{x}$，$x_{0}=1.0$，$h_{0}=10^{-8}$，$L=3$，$r=2$。精确导数为 $f^{\\prime}(x)=e^{x}$ 在 $x=1.0$ 处的值。\n\n对于每个测试用例，将结果汇总到一个列表中，该列表按顺序包含以下八个条目：\n$[$递归方法在层级 $L$ 处的最终估计值，迭代方法在层级 $L$ 处的最终估计值，递归方法最终估计值的绝对误差，迭代方法最终估计值的绝对误差，递归方法生成所有 $L+1$ 个层级所用的函数求值总次数，迭代方法生成所有 $L+1$ 个层级所用的函数求值总次数，递归方法的误差是否单调非增（布尔值），迭代方法的误差是否单调非增（布尔值）$]$。\n\n你的程序应产生单行输出，其中包含所有测试用例的结果，格式为一个由每个测试列表组成的、以逗号分隔的列表，并用方括号括起来（例如，$[ [\\dots],[\\dots],\\dots ]$）。所有量均为无量纲，并且角度必须解释为弧度。", "solution": "该问题陈述是有效的。它提出了数值分析领域一个定义明确的任务：为计算函数导数，实现并比较两种基于 Richardson 外推法的等价算法。该问题有科学依据、内容自洽，并且测试套件的所有参数都已明确指定。\n\n问题的核心在于近似函数 $f(x)$ 在点 $x_0$ 处的一阶导数 $f'(x_0)$。建议的基本近似方法是对称有限差分公式：\n$$\nD(h) = \\frac{f(x_{0}+h)-f(x_{0}-h)}{2h}\n$$\n对于一个足够光滑的函数 $f(x)$，在 $x_0$ 点的泰勒级数展开表明，该近似的误差具有一种特定结构：\n$$\nD(h) = f'(x_0) + c_1 h^2 + c_2 h^4 + c_3 h^6 + \\dots\n$$\n该误差是一个关于步长 $h$ 的偶数次幂的无穷级数。Richardson 外推法是一种系统性地消除这些误差项，从而产生一系列精度不断提高的估计值的技术。\n\n给定一个误差级数为 $h^2$ 幂次的基本近似 $A(h)$，我们可以组合用不同步长计算的近似值来消去主误差项。使用步长 $h$ 和一个更小的步长 $h/r$（其中 $r$ 是加细比，给定为 $r=2$），我们有：\n$$\nA(h) \\approx I + c_1 h^2 \\\\\nA(h/r) \\approx I + c_1 (h/r)^2\n$$\n对 $I$ 求解，可以得到一个更精确的估计值 $A^{(1)}(h)$：\n$$\nA^{(1)}(h) = \\frac{r^2 A(h/r) - A(h)}{r^2 - 1}\n$$\n这个新估计值的误差起始于一个与 $h^4$ 成正比的项。这个过程可以重复。经过 $k$ 级外推后，误差项 $c_k h^{2k}$ 被消除。从第 $(k-1)$ 级估计值创建第 $k$ 级估计值的通用外推公式是：\n$$\nA^{(k)}(h) = \\frac{r^{2k} A^{(k-1)}(h/r) - A^{(k-1)}(h)}{r^{2k}-1}\n$$\n这可以使用一个三角形表格来计算，其中 $r=2$：\n令 $T_{i,0} = D(h_0 / 2^i)$。然后表格中的各项可计算如下：\n$$\nT_{i,k} = T_{i, k-1} + \\frac{T_{i, k-1} - T_{i-1, k-1}}{4^k - 1} \\quad \\text{for } k \\geq 1\n$$\n精度不断提高的估计值序列，对应于外推层级 $k=0, 1, \\dots, L$，由该表格的对角线元素给出：$\\{T_{0,0}, T_{1,1}, \\dots, T_{L,L}\\}$。\n\n问题要求实现两种方法来生成此序列：\n\n1.  **基于表格的迭代估计量**：此方法直接实现了 Richardson 表格的自底向上构建。首先，它计算近似值的初始列，$T_{i,0} = D(h_0/2^i)$，其中 $i=0, \\dots, L$。这需要对 $D(h)$ 公式进行 $L+1$ 次求值。然后，它迭代计算表格的后续列，直到找到所需的对角线元素 $T_{k,k}$。这种方法计算效率很高，因为每个中间值 $T_{i,k}$ 只计算一次并被存储以备后用。函数 $f(x)$ 的总求值次数为 $2(L+1)$。\n\n2.  **递归估计量**：此方法通过一个体现了外推公式的递归函数来计算每个对角线元素 $T_{k,k}$，并按规定不进行任何记忆化或中间结果的缓存。一个计算 $T_{k,k}$ 的调用将递归调用自身来计算 $T_{k, k-1}$ 和 $T_{k-1, k-1}$。由于没有缓存，共享的子问题（例如，两个子调用都需要 $T_{k-1,k-2}$）会被重复计算。计算 $T_{k,k}$ 需要调用基本函数 $D(h)$ 的次数是 $2^k$。为了生成完整的序列 $\\{T_{0,0}, T_{1,1}, \\dots, T_{L,L}\\}$，我们将每个元素的成本相加，导致对 $D(h)$ 的总调用次数为 $\\sum_{k=0}^{L} 2^k = 2^{L+1}-1$ 次，因此对 $f(x)$ 的求值次数为 $2(2^{L+1}-1)$ 次。\n\n尽管两种方法在数学上是等价的（因此预期会产生相同的数值结果），但它们的计算成本却截然不同。递归方法的成本是关于 $L$ 的指数级的，而迭代方法是关于 $L$ 的线性的。这种比较凸显了算法设计的重要性，特别是动态规划（或记忆化）原理，而迭代方法正隐式地使用了该原理。\n\n测试套件中包括一个初始步长非常小的用例（测试 D），$h_0 = 10^{-8}$。对于如此小的 $h$ 值，$D(h)$ 的数值计算会受到相消误差和浮点舍入误差的主导，该误差通常按 $\\epsilon/h$ 的比例变化（其中 $\\epsilon$ 是机器精度）。由于 Richardson 外推法涉及对近似值求差，它倾向于放大舍入误差。我们预计，对于这个测试用例，这将表现为绝对误差的非单调递减。\n\n实现将通过创建两种估计量，将它们应用于测试套件，并收集指定的指标（最终估计值、绝对误差、函数总调用次数和误差单调性）来完成。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares recursive and iterative Richardson extrapolation\n    for numerical differentiation on a given test suite.\n    \"\"\"\n\n    test_cases = [\n        # Test A: happy path, smooth, moderate step\n        {\n            \"f\": lambda x: np.exp(x),\n            \"f_prime\": lambda x: np.exp(x),\n            \"x0\": 1.0, \"h0\": 0.1, \"L\": 4,\n        },\n        # Test B: smooth trigonometric, moderate step\n        {\n            \"f\": lambda x: np.sin(x),\n            \"f_prime\": lambda x: np.cos(x),\n            \"x0\": 1.3, \"h0\": 0.2, \"L\": 5,\n        },\n        # Test C: oscillatory, higher local frequency\n        {\n            \"f\": lambda x: np.sin(50 * x),\n            \"f_prime\": lambda x: 50 * np.cos(50 * x),\n            \"x0\": 0.2, \"h0\": 0.05, \"L\": 4,\n        },\n        # Test D: round-off sensitive, very small step\n        {\n            \"f\": lambda x: np.exp(x),\n            \"f_prime\": lambda x: np.exp(x),\n            \"x0\": 1.0, \"h0\": 1e-8, \"L\": 3,\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        f, f_prime, x0, h0, L = case[\"f\"], case[\"f_prime\"], case[\"x0\"], case[\"h0\"], case[\"L\"]\n        r = 2.0  # Refinement ratio\n\n        # --- Recursive Estimator ---\n        class FuncCounter:\n            def __init__(self, func):\n                self.func = func\n                self.count = 0\n            def __call__(self, x):\n                self.count += 1\n                return self.func(x)\n            def reset(self):\n                self.count = 0\n\n        f_wrapped_rec = FuncCounter(f)\n\n        def recursive_T(i, k):\n            if k == 0:\n                h = h0 / (r**i)\n                return (f_wrapped_rec(x0 + h) - f_wrapped_rec(x0 - h)) / (2.0 * h)\n            \n            term1 = recursive_T(i, k - 1)\n            term2 = recursive_T(i - 1, k - 1)\n            \n            # Using r=2, the power for eliminating error h^(2k) is 4^k\n            return term1 + (term1 - term2) / (np.power(r, 2*k) - 1.0)\n\n        f_wrapped_rec.reset()\n        estimates_rec = [recursive_T(k, k) for k in range(L + 1)]\n        total_f_calls_rec = f_wrapped_rec.count\n\n        # --- Iterative Tableau-Based Estimator ---\n        f_wrapped_iter = FuncCounter(f)\n\n        def D(h_val):\n            return (f_wrapped_iter(x0 + h_val) - f_wrapped_iter(x0 - h_val)) / (2.0 * h_val)\n        \n        T = np.zeros((L + 1, L + 1))\n        \n        f_wrapped_iter.reset()\n        for i in range(L + 1):\n            h_i = h0 / (r**i)\n            T[i, 0] = D(h_i)\n        \n        total_f_calls_iter = f_wrapped_iter.count\n        \n        for k in range(1, L + 1):\n            for i in range(k, L + 1):\n                T[i, k] = T[i, k - 1] + (T[i, k - 1] - T[i - 1, k - 1]) / (np.power(r, 2*k) - 1.0)\n        \n        estimates_iter = T.diagonal().tolist()\n\n        # --- Collect and report results ---\n        exact_val = f_prime(x0)\n\n        final_est_rec = estimates_rec[L]\n        final_est_iter = estimates_iter[L]\n\n        abs_err_rec = np.abs(final_est_rec - exact_val)\n        abs_err_iter = np.abs(final_est_iter - exact_val)\n\n        errors_rec = [np.abs(est - exact_val) for est in estimates_rec]\n        errors_iter = [np.abs(est - exact_val) for est in estimates_iter]\n        \n        # Check if absolute error is monotonically nonincreasing\n        is_mono_rec = all(errors_rec[i] = errors_rec[i-1] for i in range(1, L + 1))\n        is_mono_iter = all(errors_iter[i] = errors_iter[i-1] for i in range(1, L + 1))\n\n        case_results = [\n            final_est_rec,\n            final_est_iter,\n            abs_err_rec,\n            abs_err_iter,\n            total_f_calls_rec,\n            total_f_calls_iter,\n            is_mono_rec,\n            is_mono_iter\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2435030"}]}