{"hands_on_practices": [{"introduction": "本练习将我们对误差的理解置于一个实际的数值方法背景中。我们将应用前向差分公式来近似计算一个函数的导数，然后使用我们学到的定义来量化其相对误差，这是评估任何数值近似准确性的关键技能。[@problem_id:2152036]", "problem": "一位数值分析师的任务是评估用于计算函数一阶导数的前向差分近似的精度。所考虑的函数为 $f(x) = 2x^3 + x^2 - 4x + 1$。该分析师决定使用步长 $h = 0.2$ 来近似在特定点 $x_0 = 1$ 处的导数。\n\n用于近似的前向差分公式如下：\n$$\nf'_{\\text{approx}}(x_0) = \\frac{f(x_0+h) - f(x_0)}{h}\n$$\n此近似值将与通过解析微分求得的导数真值 $f'_{\\text{true}}(x_0)$ 进行比较。\n\n计算此近似的相对误差。相对误差 $E_{\\text{rel}}$ 定义为近似值与真值之差的绝对值除以真值的绝对值。\n\n将您的最终答案四舍五入到三位有效数字。", "solution": "给定函数 $f(x)=2x^{3}+x^{2}-4x+1$，通过解析微分求得其真导数：\n$$\nf'(x)=6x^{2}+2x-4\n$$\n在 $x_{0}=1$ 处，\n$$\nf'_{\\text{true}}(1)=6\\cdot 1^{2}+2\\cdot 1-4=4\n$$\n使用步长 $h=0.2$ 的前向差分，计算\n$$\nf(1)=2\\cdot 1^{3}+1^{2}-4\\cdot 1+1=0\n$$\n$$\nf(1.2)=2\\cdot (1.2)^{3}+(1.2)^{2}-4\\cdot 1.2+1=2\\cdot 1.728+1.44-4.8+1=3.456+1.44-4.8+1=1.096\n$$\n因此前向差分近似值为\n$$\nf'_{\\text{approx}}(1)=\\frac{f(1.2)-f(1)}{0.2}=\\frac{1.096-0}{0.2}=5.48\n$$\n相对误差为\n$$\nE_{\\text{rel}}=\\frac{|f'_{\\text{approx}}(1)-f'_{\\text{true}}(1)|}{|f'_{\\text{true}}(1)|}=\\frac{|5.48-4|}{4}=\\frac{1.48}{4}=0.37\n$$\n四舍五入到三位有效数字，结果为 $0.370$。", "answer": "$$\\boxed{0.370}$$", "id": "2152036"}, {"introduction": "当处理数量级差异巨大的数值时，一个小的绝对误差是否总意味着更高的精度？这个思想实验挑战我们去比较两种算法的准确性，并判断哪种误差度量——绝对误差还是相对误差——提供了更有意义的评估。理解这一区别对于正确解读计算结果和选择合适的算法至关重要。[@problem_id:2198986]", "problem": "在一个阻尼振荡系统的分析中，其特征多项式为 $P(x) = x^2 - (10 + 10^{-6})x + 10^{-5} = 0$。该方程的两个精确根（代表了系统的特征频率）为一个大根 $\\beta = 10$ 和一个非常小的根 $\\alpha = 10^{-6}$。\n\n使用了两种不同的数值算法来寻找这些根的近似值。\n-   算法 A 用于近似小根 $\\alpha$，其返回值为 $\\tilde{\\alpha} = 2 \\times 10^{-6}$。\n-   算法 B 用于近似大根 $\\beta$，其返回值为 $\\tilde{\\beta} = 10.01$。\n\n根据这些结果，以下哪个陈述对这两种算法的精度提供了最有意义的评估？\n\nA. 算法 A 更精确，因为其绝对误差显著小于算法 B 的绝对误差。\n\nB. 算法 B 更精确，因为其相对误差显著小于算法 A 的相对误差。\n\nC. 两种算法的精度都很差，因为两个近似值导致的相对误差都大于 0.0001。\n\nD. 两种算法的精度无法进行有意义的比较，因为它们近似的根在数量级上差异巨大。\n\nE. 两种算法的精度相当，因为算法 A 的绝对误差与算法 B 的相对误差在同一数量级上。", "solution": "给定二次方程 $P(x) = x^{2} - (10 + 10^{-6})x + 10^{-5} = 0$，其精确根为 $\\alpha = 10^{-6}$ 和 $\\beta = 10$。数值近似值为 $\\tilde{\\alpha} = 2 \\times 10^{-6}$ 和 $\\tilde{\\beta} = 10.01$。\n\n为了评估精度，我们计算每种算法的绝对误差和相对误差。对于小根：\n$$|\\tilde{\\alpha} - \\alpha| = |2 \\times 10^{-6} - 10^{-6}| = 10^{-6},$$\n$$\\text{relative error for } \\alpha = \\frac{|\\tilde{\\alpha} - \\alpha|}{|\\alpha|} = \\frac{10^{-6}}{10^{-6}} = 1.$$\n\n对于大根：\n$$|\\tilde{\\beta} - \\beta| = |10.01 - 10| = 0.01,$$\n$$\\text{relative error for } \\beta = \\frac{|\\tilde{\\beta} - \\beta|}{|\\beta|} = \\frac{0.01}{10} = 0.001.$$\n\n在比较数量级差异巨大的量的近似值时，相对误差是更有意义的度量标准。算法 A 的相对误差为 $1$，而算法 B 的相对误差为 $0.001$。因此，算法 B 更精确，因为其相对误差显著小于算法 A 的相对误差。\n\n评估各个选项：\n- A 是不正确的，因为它依赖于绝对误差，而绝对误差不适用于在不同量级上进行比较，并且事实上算法 A 的相对误差要差得多。\n- B 是正确的：算法 B 更精确，因为其相对误差小得多。\n- C 不是最有意义的评估；尽管两个相对误差都超过了 $0.0001$，但在没有指定容差的情况下就断言两者都很差是武断的，并且忽略了它们在相对精度上的巨大差异。\n- D 是不正确的；相对误差允许跨尺度进行有意义的比较。\n- E 是不正确的；$10^{-6}$ 和 $0.001$ 不在同一数量级上，并且混合使用绝对误差和相对误差进行比较是没有意义的。\n\n因此，最有意义的评估由 B 给出。", "answer": "$$\\boxed{B}$$", "id": "2198986"}, {"introduction": "计算机算术的有限精度意味着运算顺序可能对最终结果产生惊人的影响。本练习通过以两种不同顺序对一个级数求和，来探索舍入误差的累积现象。你将亲眼见证一个简单的算法选择如何能够显著提高数值的稳定性和准确性。[@problem_id:2370321]", "problem": "考虑由部分和定义的有限调和和 $$S(N)=\\sum_{n=1}^{N}\\frac{1}{n}.$$ 令 $S_{\\text{asc}}(N)$ 表示使用双精度浮点运算按升序索引 $n=1,2,\\dots,N$ 计算的和，令 $S_{\\text{desc}}(N)$ 表示使用相同算术方法按降序索引 $n=N,N-1,\\dots,1$ 计算的和。定义近似值 $\\widehat{S}(N)$ 相对于精确数学值 $S(N)$ 的绝对误差为 $$E_{\\text{abs}}(\\widehat{S}(N))=\\left|\\widehat{S}(N)-S(N)\\right|.$$ 您的任务是，对于每个指定的 $N$ 值，计算数值对 $$\\big(E_{\\text{abs}}(S_{\\text{asc}}(N)),\\;E_{\\text{abs}}(S_{\\text{desc}}(N))\\big),$$ 其中所有算术运算均在电气和电子工程师协会 (IEEE) $754$ binary64 (双精度) 浮点格式下进行。精确值 $S(N)$ 是如上定义的有限级数值。\n\n$N$ 值的测试集为：$N\\in\\{\\,1,\\;10^{4},\\;10^{6},\\;10^{7}\\,\\}$。\n\n不涉及物理单位。不使用角度。所有输出必须为实数。您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，其顺序为 $$\\big[E_{\\text{abs}}(S_{\\text{asc}}(1)),\\,E_{\\text{abs}}(S_{\\text{desc}}(1)),\\,E_{\\text{abs}}(S_{\\text{asc}}(10^{4})),\\,E_{\\text{abs}}(S_{\\text{desc}}(10^{4})),\\,E_{\\text{abs}}(S_{\\text{asc}}(10^{6})),\\,E_{\\text{abs}}(S_{\\text{desc}}(10^{6})),\\,E_{\\text{abs}}(S_{\\text{asc}}(10^{7})),\\,E_{\\text{abs}}(S_{\\text{desc}}(10^{7}))\\big].$$ 最终输出必须为单行，且不得包含任何其他字符或文本。", "solution": "该问题要求分析在计算不同 $N$ 值下的有限调和和 $S(N)=\\sum_{n=1}^{N}\\frac{1}{n}$ 时，浮点误差的累积情况。我们需要比较两种求和顺序的结果：升序 ($n=1, \\dots, N$) 和降序 ($n=N, \\dots, 1$)，两者都使用 IEEE $754$ binary64 (双精度) 算术进行计算。目标是计算每个计算结果相对于有限和的真实数学值的绝对误差。\n\n此问题是有效的。这是一个数值分析中的良置问题，它演示了浮点计算的一个基本原理：运算顺序至关重要。\n\n其核心科学原理在于浮点加法中舍入误差的管理。一个 IEEE $754$ 双精度数有一个固定大小的尾数（$52$ 位），这将其精度限制在约 $15$ 到 $17$ 个十进制数字。当两个数量级差异巨大的浮点数相加时，会发生精度损失。考虑加法 $x+y$，其中 $|x| \\gg |y|$。为了执行加法，阶码必须对齐。这涉及到将较小数 $y$ 的尾数右移。超出尾数容量的比特位将被丢弃。这种现象被称为“吸收”或“淹没”(swamping)，并导致信息丢失。其结果是，较小数 $y$ 的贡献被部分或完全丢失。\n\n让我们根据这一原理分析指定的两种求和方法。\n\n$1$. **升序求和, $S_{\\text{asc}}(N)$**:\n求和过程为 $S_{\\text{asc}}(N) = (\\dots((1 + \\frac{1}{2}) + \\frac{1}{3}) + \\dots + \\frac{1}{N})$。项 $1/n$ 按量级递减的顺序相加。部分和在开始时增长迅速。对于大的 $n$，项 $1/n$ 与累积的部分和相比变得非常小。例如，调和级数呈对数增长，即 $S(N) \\approx \\ln(N) + \\gamma$，其中 $\\gamma$ 是 Euler–Mascheroni 常数。对于 $N=10^7$，$S(10^7) \\approx \\ln(10^7) \\approx 16.1$。最后加入的项是 $1/10^7 = 10^{-7}$。将 $10^{-7}$ 加到一个量级约为 $16.1$ 的数上将导致显著的精度损失。累积和的量级很大，反复向其添加小值会导致吸收误差的系统性累积。因此，我们预测 $S_{\\text{asc}}(N)$ 将是精度较低的方法。\n\n$2$. **降序求和, $S_{\\text{desc}}(N)$**:\n求和过程为 $S_{\\text{desc}}(N) = (\\dots((\\frac{1}{N} + \\frac{1}{N-1}) + \\frac{1}{N-2}) + \\dots + 1)$。在这里，项按量级递增的顺序相加。求和从最小的项开始。部分和增长缓慢。在每一步中，相加的两个数（当前的部分和与下一项）的量级更可能相当。这最小化了尾数的右移，从而减少了每一步的精度损失。对于正项单调递减级数的求和，这是一种提高精度的通用启发式技术。我们预测 $S_{\\text{desc}}(N)$ 将显著比 $S_{\\text{asc}}(N)$ 更精确。\n\n$3$. **参考值, $S(N)$**:\n为了计算绝对误差 $E_{\\text{abs}}(\\widehat{S}(N))=\\left|\\widehat{S}(N)-S(N)\\right|$，我们需要一个真实数学和 $S(N)$ 的高精度表示。由于我们的计算 $\\widehat{S}(N)$ 是在双精度（约 $16$ 位十进制精度）下进行的，我们的参考值必须具有高得多的精度，才能在此被视为“精确”。例如，使用 $50$ 位十进制数字的精度就绰绰有余了。我们将为此任务使用一个高精度算术库。Python 的 `decimal` 模块提供了必要的功能。\n\n$4$. **算法**:\n对于测试集 $\\{1, 10^4, 10^6, 10^7\\}$ 中的每个 $N$ 值，将执行以下步骤：\n- 对于 $N=1$，情况是平凡的。$S(1) = 1$, $S_{\\text{asc}}(1) = 1.0$, 且 $S_{\\text{desc}}(1) = 1.0$。数字 $1.0$ 可以被精确表示，因此两个误差都将恰好为 $0$。\n- 对于 $N > 1$：\n    - **步骤 4.1：计算参考和。** 我们将我们的任意精度算术工具（Python的`decimal`模块）的精度设置为 $50$ 位。然后，我们用此高精度计算 $S(N) = \\sum_{n=1}^{N} \\frac{1}{n}$。结果将存储为一个 `Decimal` 对象。\n    - **步骤 4.2：计算升序和。** 一个 `numpy.float64` 变量 `sum_asc` 被初始化为 $0.0$。我们从 $n=1$ 迭代到 $N$，并累加求和 `sum_asc = sum_asc + 1.0/n`。这些项被显式转换为 `float64` 以确保算术运算在正确的精度下进行。\n    - **步骤 4.3：计算降序和。** 一个 `numpy.float64` 变量 `sum_desc` 被初始化为 $0.0$。我们从 $n=N$ 递减迭代到 $1$，并累加求和 `sum_desc = sum_desc + 1.0/n`。\n    - **步骤 4.4：计算绝对误差。** 绝对误差计算如下：\n        $$E_{\\text{abs}}(S_{\\text{asc}}(N)) = \\left| \\text{Decimal}(\\text{sum\\_asc}) - S(N) \\right|$$\n        $$E_{\\text{abs}}(S_{\\text{desc}}(N)) = \\left| \\text{Decimal}(\\text{sum\\_desc}) - S(N) \\right|$$\n        通过在相减前将 `float64` 结果转换为 `Decimal` 对象，我们在高精度下执行减法，从而避免了参考值本身的任何精度损失。最终结果是这个差值的绝对值。\n\n我们将实施此过程，以生成按问题陈述中指定顺序排列的八个所需误差值。预计结果将显示，对于大的 $N$，升序和的误差明显大于降序和的误差，从而证实我们的理论分析。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [1, 10**4, 10**6, 10**7]\n\n    results = []\n\n    # Set precision for the high-accuracy reference calculation.\n    # 50 digits is sufficient to be an \"exact\" reference for double-precision (approx. 16 digits).\n    getcontext().prec = 50\n\n    for N in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        \n        # For N=1, the sums are trivially exact (1.0), so errors are 0.\n        if N == 1:\n            results.extend([0.0, 0.0])\n            continue\n\n        # 1. Compute the reference value S(N) using high-precision arithmetic.\n        # This serves as the \"exact\" value for error calculation.\n        s_exact = Decimal(0)\n        one_decimal = Decimal(1)\n        # Summation order for the exact value does not matter here due to high precision.\n        for i in range(1, N + 1):\n            s_exact += one_decimal / Decimal(i)\n\n        # 2. Compute S_asc(N) summing in ascending order (1 to N).\n        # This is expected to be less accurate due to adding small numbers to a large accumulator.\n        s_asc = np.float64(0.0)\n        for i in range(1, N + 1):\n            # Using np.float64 ensures IEEE 754 double-precision arithmetic.\n            s_asc += np.float64(1.0) / np.float64(i)\n\n        # 3. Compute S_desc(N) summing in descending order (N to 1).\n        # This is expected to be more accurate as it sums numbers of similar magnitude first.\n        s_desc = np.float64(0.0)\n        for i in range(N, 0, -1):\n            s_desc += np.float64(1.0) / np.float64(i)\n\n        # 4. Calculate absolute errors against the high-precision reference.\n        # The float64 results are converted to Decimal to perform the subtraction\n        # at high precision, preserving the accuracy of the reference value.\n        error_asc = abs(Decimal(s_asc) - s_exact)\n        error_desc = abs(Decimal(s_desc) - s_exact)\n        \n        # Append results as standard Python floats.\n        results.extend([float(error_asc), float(error_desc)])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2370321"}]}