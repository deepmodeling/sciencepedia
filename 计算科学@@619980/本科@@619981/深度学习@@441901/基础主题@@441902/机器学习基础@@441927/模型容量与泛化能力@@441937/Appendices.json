{"hands_on_practices": [{"introduction": "在机器学习中，一个核心挑战是在模型容量之间找到平衡点。过于简单的模型可能无法捕捉数据中的基本模式（高偏差），而过于复杂的模型则可能学习到训练数据中的噪声（高方差），导致泛化能力下降。这个练习将引导你通过一个经典的多项式回归场景，动手实践如何量化和分析这种权衡，通过对泛化差距建模来选择最佳模型复杂度，从而加深对欠拟合与过拟合的理解。[@problem_id:3107026]", "problem": "给定在同一数据集上拟合的 $d$ 次多项式回归模型的观测结果。对于每个测试用例，您将获得一组次数 $d \\in \\mathbb{N}$ 对应的经验训练均方误差 (Mean Squared Error, MSE) $E_{\\text{train}}(d)$ 和经验交叉验证均方误差 $E_{\\text{val}}(d)$。您的任务是通过将泛化差距 $g(d)$ 对 $d$ 进行回归来预测过拟合风险，并分类出最小化预期泛化误差估计值的最优次数。\n\n基本原理和定义：\n- 复杂度为 $d$ 时的泛化误差可以表示为 $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$，其中 $g(d)$ 是泛化差距，定义为 $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$。在实践中，$E_{\\text{test}}(d)$ 是未知的，交叉验证被用作一种经过充分检验的代理方法，因此我们估计差距为 $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$。\n- 过拟合风险随模型复杂度增加而增大的情况，反映为 $g(d)$ 作为 $d$ 的函数的正斜率。\n\n算法要求：\n- 对于每个测试用例，计算每个观测次数 $d_k$ 下的 $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$。\n- 使用普通最小二乘法 (Ordinary Least Squares, OLS) 将直线 $g(d) \\approx a + b\\,d$ 拟合到观测对 $\\{(d_k, g(d_k))\\}$。\n- 将每个观测次数下的估计预期泛化误差构建为 $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$。\n- 将最优次数 $\\widehat{d}^{\\star}$ 分类为给定集合中使 $\\widehat{E}_{\\text{gen}}(d_k)$ 最小化的整数次数。如果在数值容差范围内有多个次数达到相同的最小值，则选择其中最小的次数（偏好较低的复杂度）。\n- 每个测试用例报告两个量：泛化差距对次数回归的估计斜率 $\\widehat{b}$（四舍五入到4位小数），以及分类出的整数最优次数 $\\widehat{d}^{\\star}$。\n\n测试套件：\n- 测试用例 1：$d = [1,2,3,4,5]$，$E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$，$E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$。\n- 测试用例 2：$d = [1,2,3,4]$，$E_{\\text{train}} = [5.0,4.2,3.6,3.3]$，$E_{\\text{val}} = [5.0,4.2,3.6,3.3]$。\n- 测试用例 3：$d = [1,2,3,4,5,6]$，$E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$，$E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$。\n- 测试用例 4：$d = [1,10]$，$E_{\\text{train}} = [5.0,0.1]$，$E_{\\text{val}} = [5.1,6.0]$。\n- 测试用例 5：$d = [2,3,4]$，$E_{\\text{train}} = [8.0,6.0,5.0]$，$E_{\\text{val}} = [8.7,6.6,5.4]$。\n\n输出规格：\n- 对于每个测试用例，输出一个双元素列表 $[\\widehat{b}, \\widehat{d}^{\\star}]$，其中 $\\widehat{b}$ 是四舍五入到4位小数的斜率，$\\widehat{d}^{\\star}$ 是一个整数。\n- 您的程序应生成单行输出，其中包含一个列表，该列表按顺序汇总了每个测试用例的输出，且不含空格。每个元素本身必须是如上所述的双元素列表。最终打印输出必须只有一行。\n\n不涉及物理单位。所有角度（如有）在此均不相关。不需要百分比。\n\n您的程序必须是一个完整的、可运行的程序，它在内部定义了上述测试用例，并产生指定的单行输出。", "solution": "### 第一步：提取已知信息\n- **模型**：$d \\in \\mathbb{N}$ 次的多项式回归模型。\n- **每个测试用例的数据**：一组次数 $d_k$、对应的训练均方误差 (MSE) $E_{\\text{train}}(d_k)$ 和交叉验证均方误差 $E_{\\text{val}}(d_k)$。\n- **定义**：\n    - 泛化误差：$E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$。\n    - 泛化差距：$g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$。\n    - 经验泛化差距估计：$g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$。\n- **算法要求**：\n    1.  为每个次数计算经验差距：$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$。\n    2.  使用普通最小二乘法 (OLS) 将直线 $g(d) \\approx a + b\\,d$ 拟合到数据点 $\\{(d_k, g(d_k))\\}$，以找到估计的截距 $\\widehat{a}$ 和斜率 $\\widehat{b}$。\n    3.  估计每个次数下的预期泛化误差：$\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}d_k$。\n    4.  将最优次数 $\\widehat{d}^{\\star}$ 分类为给定集合中最小化 $\\widehat{E}_{\\text{gen}}(d_k)$ 的整数次数。平局规则是，如果多个次数产生相同的最小值，则选择最小的次数。\n    5.  报告斜率 $\\widehat{b}$（四舍五入到4位小数）和整数最优次数 $\\widehat{d}^{\\star}$。\n- **测试套件**：\n    - **测试用例 1**：$d = [1,2,3,4,5]$，$E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$，$E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$。\n    - **测试用例 2**：$d = [1,2,3,4]$，$E_{\\text{train}} = [5.0,4.2,3.6,3.3]$，$E_{\\text{val}} = [5.0,4.2,3.6,3.3]$。\n    - **测试用例 3**：$d = [1,2,3,4,5,6]$，$E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$，$E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$。\n    - **测试用例 4**：$d = [1,10]$，$E_{\\text{train}} = [5.0,0.1]$，$E_{\\text{val}} = [5.1,6.0]$。\n    - **测试用例 5**：$d = [2,3,4]$，$E_{\\text{train}} = [8.0,6.0,5.0]$，$E_{\\text{val}} = [8.7,6.6,5.4]$。\n- **输出规格**：对于每个测试用例，输出一个单行的双元素列表 $[\\widehat{b}, \\widehat{d}^{\\star}]$，最终输出字符串中没有空格。\n\n### 第二步：使用提取的已知信息进行验证\n根据验证标准对问题陈述进行评估。\n- **科学性**：该问题基于统计学习理论的基本概念，包括偏差-方差权衡、过拟合、训练误差、验证误差和泛化误差。使用普通最小二乘法来建模模型复杂性与泛化差距之间的关系是一种标准且合理的分析技术。\n- **适定性**：问题是适定的。对于每个测试用例，用于 OLS 回归的不同数据点数量至少为2，确保了线性参数的唯一解。对最优次数的搜索是在一个有限集合上进行的最小化过程，保证有解。平局规则确保了解的唯一性。\n- **客观性**：问题使用了精确的数学定义和清晰、明确的算法过程来陈述。输入是数值的，要求的输出是明确定义的。\n- **缺陷清单**：该问题不违反任何指定的缺陷。它具有科学合理性、可形式化、完整性、现实性和适定性。它需要进行重要的计算和推理。\n\n### 第三步：结论与行动\n问题是有效的。将提供一个完整的解决方案。\n\n该问题要求分析模型性能随复杂度（具体为多项式回归模型的次数 $d$）变化的函数。分析的核心在于理解和建模泛化差距 $g(d)$，它代表了模型在未见数据上的性能（通过验证误差 $E_{\\text{val}}$ 近似）与其在训练数据上的性能（$E_{\\text{train}}$）之间的差异。随着复杂度的增加，差距不断扩大是过拟合的标志。\n\n指定的算法旨在创建泛化误差的平滑估计，以便做出比简单地选择具有最低验证误差的模型更稳健的模型选择决策。验证误差本身可能有噪声，对泛化差距的趋势进行建模可以帮助滤除这种噪声。\n\n每个测试用例的步骤如下：\n\n1.  **计算经验泛化差距**：对于每个给定的多项式次数 $d_k$，我们使用提供的训练和验证误差计算经验泛化差距 $g(d_k)$：\n    $$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$$\n\n2.  **通过 OLS 建模泛化差距**：我们假设模型复杂度 $d$ 与泛化差距 $g(d)$ 之间存在线性关系。我们使用普通最小二乘法 (OLS) 将一条线 $\\widehat{g}(d) = \\widehat{a} + \\widehat{b}d$ 拟合到观测点集 $\\{(d_k, g(d_k))\\}$。选择斜率 $\\widehat{b}$ 和截距 $\\widehat{a}$ 以最小化平方差之和 $\\sum_k (g(d_k) - (\\widehat{a} + \\widehat{b}d_k))^2$。对于一组 $n$ 个点，OLS 估计量由以下公式给出：\n    $$ \\widehat{b} = \\frac{\\sum_{k=1}^{n} (d_k - \\bar{d})(g_k - \\bar{g})}{\\sum_{k=1}^{n} (d_k - \\bar{d})^2} $$\n    $$ \\widehat{a} = \\bar{g} - \\widehat{b}\\bar{d} $$\n    其中 $\\bar{d} = \\frac{1}{n}\\sum_k d_k$ 和 $\\bar{g} = \\frac{1}{n}\\sum_k g(d_k)$ 是样本均值。斜率 $\\widehat{b}$ 可作为过拟合风险的直接度量；一个正的 $\\widehat{b}$ 表明训练和验证性能之间的差距随着模型复杂度的增加而扩大。\n\n3.  **估计泛化误差**：使用差距的线性模型，我们构建真实泛化误差的平滑估计 $\\widehat{E}_{\\text{gen}}(d_k)$：\n    $$ \\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{g}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}d_k $$\n    该估计结合了直接观测到的训练误差（反映模型拟合数据的程度）和泛化惩罚的正则化估计（考虑了复杂度）。\n\n4.  **确定最优次数**：最优次数 $\\widehat{d}^{\\star}$ 被选为给定集合 $\\{d_k\\}$ 中最小化我们估计的泛化误差 $\\widehat{E}_{\\text{gen}}(d_k)$ 的次数。\n    $$ \\widehat{d}^{\\star} = \\underset{d_k}{\\arg\\min} \\{ \\widehat{E}_{\\text{gen}}(d_k) \\} $$\n    问题规定，如果出现平局，应选择达到最小误差的次数中最小的那个。这反映了简约原则（奥卡姆剃刀）：当所有其他条件相同时，偏好更简单的模型。\n\n最后，对于每个测试用例，我们报告计算出的斜率 $\\widehat{b}$（过拟合风险的度量）和确定的最优次数 $\\widehat{d}^{\\star}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {'d': [1, 2, 3, 4, 5], \n         'E_train': [9.0, 6.0, 4.5, 4.0, 3.8], \n         'E_val': [10.0, 7.0, 5.0, 5.2, 6.0]},\n        # Test case 2\n        {'d': [1, 2, 3, 4], \n         'E_train': [5.0, 4.2, 3.6, 3.3], \n         'E_val': [5.0, 4.2, 3.6, 3.3]},\n        # Test case 3\n        {'d': [1, 2, 3, 4, 5, 6], \n         'E_train': [12.0, 8.0, 6.0, 5.2, 5.0, 4.9], \n         'E_val': [11.5, 7.3, 5.1, 4.1, 3.7, 3.4]},\n        # Test case 4\n        {'d': [1, 10], \n         'E_train': [5.0, 0.1], \n         'E_val': [5.1, 6.0]},\n        # Test case 5\n        {'d': [2, 3, 4], \n         'E_train': [8.0, 6.0, 5.0], \n         'E_val': [8.7, 6.6, 5.4]},\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        # Extract and convert data to numpy arrays for vectorized operations\n        d = np.array(case['d'], dtype=float)\n        E_train = np.array(case['E_train'], dtype=float)\n        E_val = np.array(case['E_val'], dtype=float)\n        \n        # Step 1: Compute the empirical generalization gap\n        g = E_val - E_train\n        \n        # Step 2: Fit a straight line g(d) = a + b*d using OLS.\n        # np.polyfit with degree 1 returns the coefficients [b, a] for slope and intercept.\n        b_hat, a_hat = np.polyfit(d, g, 1)\n        \n        # Step 3: Form the estimated expected generalization error\n        # E_gen_hat(d) = E_train(d) + (a_hat + b_hat*d)\n        E_gen_hat = E_train + a_hat + b_hat * d\n        \n        # Step 4: Classify the optimal degree d_star\n        # np.argmin finds the index of the first occurrence of the minimum value.\n        # Since the input degrees 'd' are sorted, this satisfies the tie-breaking rule\n        # of choosing the smallest degree.\n        min_error_index = np.argmin(E_gen_hat)\n        d_star = int(d[min_error_index])\n        \n        # Format the result for this case as a string \"[b,d_star]\" with no spaces\n        # and b rounded to 4 decimal places.\n        case_result_str = f\"[{b_hat:.4f},{d_star}]\"\n        results_as_strings.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    # The output is a single list containing the results for all test cases.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3107026"}, {"introduction": "经典观点认为，完美拟合含噪声训练数据（即训练误差为零）的模型会因过度拟合而丧失泛化能力。然而，现代深度学习理论揭示了一个令人惊讶的现象：“良性过拟合”。这个练习将带你进入过参数化线性回归的世界，通过实现最小范数插值求解器，你将亲手验证在何种条件下，模型可以在完美“记忆”训练集的同时，仍然在测试集上表现出色。[@problem_id:3152379]", "problem": "您的任务是设计并执行一个计算实验，以探究在受控噪声下，过参数化线性回归中模型容量和泛化能力如何相互作用。您将使用一个带有高斯特征和加性高斯噪声的合成线性模型，并检验插值何时是良性的，即模型在完美拟合训练数据的同时，其测试误差接近不可约的噪声水平。最终答案必须是一个完整的、可运行的程序。\n\n将要使用的基本基础和定义：\n- 监督线性回归假设数据由 $y = \\langle \\mathbf{w}_\\star, \\mathbf{x} \\rangle + \\varepsilon$ 生成，其中 $\\mathbf{x} \\in \\mathbb{R}^p$ 是特征向量，$\\mathbf{w}_\\star \\in \\mathbb{R}^p$ 是一个固定但未知的参数向量，而 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 是方差为 $\\sigma^2$ 的加性噪声。\n- 对于一个预测器 $f(\\mathbf{x}) = \\langle \\widehat{\\mathbf{w}}, \\mathbf{x} \\rangle$，在均方误差下的泛化误差是 $\\mathbb{E}\\left[(f(\\mathbf{x}) - y)^2\\right]$，其中期望是关于 $(\\mathbf{x}, y)$ 的联合分布计算的。\n- 在过参数化情况 $p > n$ 下拟合线性模型时，Moore–Penrose 伪逆可以得到最小欧几里得范数的插值解。\n\n问题要求：\n1. 数据模型和估计器：\n   - 对于每个测试用例 $(n, p, \\sigma^2)$，生成一个设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$，其元素独立地从 $\\mathcal{N}(0, 1)$ 中抽取。\n   - 定义一个确定性的目标向量 $\\mathbf{w}_\\star \\in \\mathbb{R}^p$，其分量为 $w_{\\star, j} = \\frac{1}{j} \\cdot \\frac{s}{\\sqrt{\\sum_{k=1}^p \\frac{1}{k^2}}}$，其中 $j \\in \\{1, 2, \\dots, p\\}$ 且固定尺度 $s=1$。这将信号分散到许多坐标上。\n   - 生成噪声 $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$，其元素独立地从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取，并构建标签 $\\mathbf{y} = \\mathbf{X}\\mathbf{w}_\\star + \\boldsymbol{\\varepsilon}$。\n   - 使用 Moore–Penrose 伪逆拟合最小范数插值器：$\\widehat{\\mathbf{w}} = \\mathbf{X}^+ \\mathbf{y}$。\n2. 评价指标：\n   - 计算训练均方误差 $E_{\\text{train}} = \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\mathbf{w}} - \\mathbf{y} \\rVert_2^2$。\n   - 构建一个大小为 $N_{\\text{test}} = 20000$ 的独立测试集 $(\\mathbf{X}_{\\text{test}}, \\mathbf{y}_{\\text{test}})$，其生成模型与训练集相同，并计算测试均方误差 $E_{\\text{test}} = \\frac{1}{N_{\\text{test}}} \\lVert \\mathbf{X}_{\\text{test}}\\widehat{\\mathbf{w}} - \\mathbf{y}_{\\text{test}} \\rVert_2^2$。\n3. 良性过拟合准则：\n   - 如果以下两个条件都成立，则判定一个测试用例表现出良性过拟合：\n     - $E_{\\text{train}} \\le \\tau$，其中 $\\tau = 10^{-10}$。\n     - $E_{\\text{test}}$ 在相对意义上接近不可约噪声水平：$\\left| \\frac{E_{\\text{test}}}{\\sigma^2} - 1 \\right| \\le \\delta$，其中 $\\delta = 0.4$。\n   - 为每个测试用例生成一个整数指示符：如果良性过拟合成立，则输出 $1$，否则输出 $0$。\n4. 随机性与可复现性：\n   - 所有随机抽样必须是可复现的。对于每个测试用例 $(n, p, \\sigma^2)$，从 $(n, p, \\sigma^2)$ 确定性地派生一个伪随机种子，以初始化该用例的随机数生成器。该用例的训练数据和测试数据的生成都只使用此用例专属的生成器。\n5. 测试套件：\n   - 使用以下测试用例列表，这些用例探索了过参数化、欠参数化和边界情况，以及噪声递减的情况：\n     - 用例 1：$(n, p, \\sigma^2) = (60, 200, 1.0)$。\n     - 用例 2：$(n, p, \\sigma^2) = (60, 200, 0.25)$。\n     - 用例 3：$(n, p, \\sigma^2) = (60, 200, 0.01)$。\n     - 用例 4：$(n, p, \\sigma^2) = (200, 60, 0.25)$。\n     - 用例 5：$(n, p, \\sigma^2) = (100, 100, 0.25)$。\n6. 输出格式：\n   - 您的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的整数列表。第 $i$ 个整数必须是第 $i$ 个用例的良性过拟合指示符。例如，一个有效的输出看起来像 $[1,0,1,0,1]$。\n\n所有计算都是纯数学的，使用无量纲的量；不涉及物理单位。不使用角度。您的程序必须是完全自包含的，不需要用户输入或外部文件。确保数值稳定性和实现清晰。您生成的映射应展示在相同的评估程序下，当 $\\sigma^2$ 减小时，良性过拟合状态如何变化，并比较过参数化、欠参数化和边界情况。", "solution": "该问题要求设计并执行一个计算实验，以研究过参数化线性回归中的良性过拟合现象。该分析涉及生成合成数据，拟合特定类型的线性模型，并根据插值和泛化的标准来评估其性能。\n\n该框架植根于监督线性回归，其中特征 $\\mathbf{x} \\in \\mathbb{R}^p$ 和响应 $y \\in \\mathbb{R}$ 之间的关系被建模为：\n$$y = \\langle \\mathbf{w}_\\star, \\mathbf{x} \\rangle + \\varepsilon$$\n这里，$\\mathbf{w}_\\star \\in \\mathbb{R}^p$ 是一个固定的、真实的参数向量，而 $\\varepsilon$ 是从方差为 $\\sigma^2$ 的正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的随机噪声。值 $\\sigma^2$ 代表不可约误差；平均而言，没有哪个预测器能够实现低于此值的均方误差。\n\n给定一个包含 $n$ 个样本的训练数据集 $(\\mathbf{X}, \\mathbf{y})$，其中 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\mathbf{y} \\in \\mathbb{R}^n$ 是标签向量，我们寻求 $\\mathbf{w}_\\star$ 的一个估计器 $\\widehat{\\mathbf{w}}$。问题指定使用最小范数插值解，这在特征数量 $p$ 超过样本数量 $n$ 的过参数化情况下尤其重要。该解由以下公式给出：\n$$\\widehat{\\mathbf{w}} = \\mathbf{X}^+ \\mathbf{y}$$\n其中 $\\mathbf{X}^+$ 是 $\\mathbf{X}$ 的 Moore-Penrose 伪逆。\n\n在过参数化情况 ($p > n$) 下，假设矩阵 $\\mathbf{X}$ 具有满行秩（对于从像高斯分布这样的连续分布中抽取的元素，这种情况以概率 $1$ 发生），该估计器能完美拟合训练数据。也就是说，$\\mathbf{X}\\widehat{\\mathbf{w}} = \\mathbf{y}$。这种完美拟合，即插值，意味着训练均方误差为零：\n$$E_{\\text{train}} = \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\mathbf{w}} - \\mathbf{y} \\rVert_2^2 = 0$$\n经典的统计学习理论会预测，这种拟合了训练数据中噪声的插值模型，在泛化到新的、未见过的数据时表现会很差。然而，最近的理论和实证结果发现了一种被称为“良性过拟合”的现象，其中某些插值模型可以达到近乎最优的泛化误差，即其测试误差接近不可约误差 $\\sigma^2$。\n\n本实验旨在特定条件下测试这一现象。我们现在将详细说明分步计算过程。\n\n对于由参数 $(n, p, \\sigma^2)$ 定义的每个测试用例：\n\n**1. 可复现的随机性**\n为确保结果是确定性和可复现的，我们从用例参数 $(n, p, \\sigma^2)$ 中派生出伪随机数生成器的唯一种子。可以使用一个简单、稳健的整数函数，例如 `seed = n * 1000000 + p * 1000 + int(sigma2 * 100000)`。使用此种子初始化一个 `numpy.random.Generator` 实例，并将其用于该测试用例内的所有随机操作。\n\n**2. 数据生成**\n- 构建一个真实参数向量 $\\mathbf{w}_\\star \\in \\mathbb{R}^p$。其分量定义为 $w_{\\star, j} = \\frac{1}{j} \\cdot \\frac{s}{\\sqrt{\\sum_{k=1}^p \\frac{1}{k^2}}}$，其中 $j \\in \\{1, 2, \\dots, p\\}$，尺度因子 $s=1$。这种结构确保了信号能量分散在各个坐标上，并随索引 $j$ 衰减。\n- 训练设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 的元素从标准正态分布 $\\mathcal{N}(0, 1)$ 中独立抽取生成。\n- 训练噪声向量 $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$ 的元素从 $\\mathcal{N}(0, \\sigma^2)$ 中独立抽取生成。用于生成的标准差是 $\\sigma$。\n- 训练标签合成为 $\\mathbf{y} = \\mathbf{X}\\mathbf{w}_\\star + \\boldsymbol{\\varepsilon}$。\n\n**3. 模型拟合与评估**\n- 使用 Moore-Penrose 伪逆计算估计的参数向量 $\\widehat{\\mathbf{w}}$：$\\widehat{\\mathbf{w}} = \\mathbf{X}^+ \\mathbf{y}$。这通过 `numpy.linalg.pinv` 实现。\n- 计算训练误差 $E_{\\text{train}}$。由于有限精度算术，即使理论预测 $E_{\\text{train}}$ 应为 $0$，我们也不期望它精确为 $0$。因此，我们检查 $E_{\\text{train}}$ 是否低于一个小的数值容差 $\\tau = 10^{-10}$。\n- 使用相同的统计模型和相同的种子随机数生成器，生成一个大的、独立的测试集 $(\\mathbf{X}_{\\text{test}}, \\mathbf{y}_{\\text{test}})$，其大小为 $N_{\\text{test}} = 20000$。\n- 计算测试均方误差：$E_{\\text{test}} = \\frac{1}{N_{\\text{test}}} \\lVert \\mathbf{X}_{\\text{test}}\\widehat{\\mathbf{w}} - \\mathbf{y}_{\\text{test}} \\rVert_2^2$。\n\n**4. 良性过拟合准则**\n如果同时满足两个条件，则一个用例被分类为表现出良性过拟合：\n- **插值：** 训练误差可忽略不计：$E_{\\text{train}} \\le \\tau = 10^{-10}$。\n- **泛化：** 测试误差接近不可约噪声水平 $\\sigma^2$。这通过相对差异来量化：$\\left| \\frac{E_{\\text{test}}}{\\sigma^2} - 1 \\right| \\le \\delta = 0.4$。\n\n每个测试用例的输出是一个整数指示符：如果两个条件都满足，则为 $1$，否则为 $0$。\n\n**测试情况分析：**\n- **用例 1, 2, 3 ($(n, p) = (60, 200)$)：** 这些处于过参数化情况 ($p/n \\approx 3.33$)。我们预期 $E_{\\text{train}} \\leq \\tau$ 会成立。良性过拟合理论预测，对于足够高的过参数化比率，随着噪声方差 $\\sigma^2$ 的减小，测试性能应会提高。因此，用例 3 ($\\sigma^2=0.01$) 最有可能满足泛化准则，其次是用例 2 ($\\sigma^2=0.25$)，而用例 1 ($\\sigma^2=1.0$) 最不可能。\n- **用例 4 ($(n, p) = (200, 60)$)：** 这是欠参数化或经典情况 ($p  n$)。伪逆计算的是普通最小二乘解。由于数据点多于参数，模型无法完美拟合含噪声的数据，因此不会发生插值。我们预期 $E_{\\text{train}} > \\tau$，不满足第一个条件。\n- **用例 5 ($(n, p) = (100, 100)$)：** 这是边界或临界参数化情况。设计矩阵 $\\mathbf{X}$ 是方阵，并且以概率 $1$ 是可逆的。解为 $\\widehat{\\mathbf{w}} = \\mathbf{X}^{-1}\\mathbf{y}$，它完美地插值了数据，因此 $E_{\\text{train}} \\leq \\tau$ 将成立。然而，正是在这种情况下，测试误差已知会达到峰值（双下降曲线的“临界峰”）。对一个随机方阵求逆会导致对 $\\mathbf{y}$ 中噪声的极度敏感，使得 $\\widehat{\\mathbf{w}}$ 成为 $\\mathbf{w}_\\star$ 的一个很差的估计。因此，预期 $E_{\\text{test}}$ 会非常大，不满足泛化条件。\n\n该实验系统地评估了这些不同的情况，为关于模型容量、插值和泛化的理论预测提供了计算验证。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes a computational experiment to test for benign overfitting\n    in linear regression across different parameter regimes.\n    \"\"\"\n    # Define the test cases as tuples of (n, p, sigma^2).\n    test_cases = [\n        (60, 200, 1.0),   # Case 1: Overparameterized, high noise\n        (60, 200, 0.25),  # Case 2: Overparameterized, medium noise\n        (60, 200, 0.01),  # Case 3: Overparameterized, low noise\n        (200, 60, 0.25),  # Case 4: Underparameterized\n        (100, 100, 0.25), # Case 5: Boundary case\n    ]\n\n    # Define constants from the problem statement.\n    N_test = 20000\n    tau = 1e-10\n    delta = 0.4\n    s = 1.0\n\n    results = []\n\n    for n, p, sigma2 in test_cases:\n        # 1. Reproducible Randomness: Derive a deterministic seed.\n        # This function ensures a unique seed for the given test cases.\n        seed = n * 1000000 + p * 1000 + int(sigma2 * 100000)\n        rng = np.random.default_rng(seed)\n\n        # 2. Data Generation\n        # Construct the true parameter vector w_star.\n        j = np.arange(1, p + 1)\n        w_star_unnorm = 1.0 / j\n        norm_const = np.linalg.norm(w_star_unnorm)\n        w_star = s / norm_const * w_star_unnorm\n\n        # Generate training data.\n        X_train = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n        sigma = np.sqrt(sigma2)\n        epsilon_train = rng.normal(loc=0.0, scale=sigma, size=n)\n        y_train = X_train @ w_star + epsilon_train\n\n        # 3. Model Fitting\n        # Compute the minimum-norm interpolator using the Moore-Penrose pseudo-inverse.\n        # This handles over-, under-, and critically-parameterized cases correctly.\n        w_hat = np.linalg.pinv(X_train) @ y_train\n\n        # 4. Evaluation\n        # Compute training error.\n        y_train_pred = X_train @ w_hat\n        E_train = np.mean((y_train_pred - y_train)**2)\n\n        # Generate test data.\n        X_test = rng.normal(loc=0.0, scale=1.0, size=(N_test, p))\n        epsilon_test = rng.normal(loc=0.0, scale=sigma, size=N_test)\n        y_test = X_test @ w_star + epsilon_test\n\n        # Compute test error.\n        y_test_pred = X_test @ w_hat\n        E_test = np.mean((y_test_pred - y_test)**2)\n\n        # 5. Benign Overfitting Criterion\n        # Check for interpolation.\n        is_interpolating = E_train = tau\n\n        # Check for generalization.\n        # This check is only meaningful if sigma2 > 0.\n        if sigma2 > 0:\n            relative_error_diff = abs(E_test / sigma2 - 1.0)\n            is_generalizing = relative_error_diff = delta\n        else:\n            # If there's no noise, test error should also be near zero.\n            # This case is not in the test suite but is handled for completeness.\n            is_generalizing = E_test = delta \n\n        # Determine the result for this case.\n        result = 1 if is_interpolating and is_generalizing else 0\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3152379"}, {"introduction": "当我们不断增加模型容量时，测试误差的变化曲线真的只是一个简单的“U”形吗？本练习将通过一个线性自编码器的仿真实验，为你揭示一个更完整、更深刻的现象——“双下降”曲线。你将观察到，随着模型容量跨越足以完美拟合训练数据的“插值阈值”，测试误差在经历一个峰值后，会出乎意料地再次下降，这为我们理解现代大型模型的泛化行为提供了关键视角。[@problem_id:3183618]", "problem": "您将实现一个模拟，通过改变瓶颈大小来研究线性自编码器中的双峰现象。目标是将训练重构误差和测试时泛化能力与模型容量如何跨越插值阈值并趋近于单位映射联系起来。使用一个基于经验风险最小化（ERM）、主成分分析（PCA）和正交投影性质的纯数学框架，并设计一个程序，为多个测试用例生成可量化的输出。\n\n考虑一个权重共享的线性自编码器，其输入为向量 $x \\in \\mathbb{R}^d$，编码器为 $z = W^\\top x$（其中 $W \\in \\mathbb{R}^{d \\times m}$），解码器重构为 $\\hat{x} = W z = W W^\\top x$。设训练数据矩阵为 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$，测试数据矩阵为 $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times d}$。通过其经验均值 $\\mu_{\\text{train}} \\in \\mathbb{R}^d$ 对训练数据进行中心化，并使用相同的 $\\mu_{\\text{train}}$ 对测试数据进行中心化，以评估固定估计器的泛化能力。对于给定的瓶颈大小 $m$（$1 \\le m \\le d$），定义重构矩阵 $P_m = W_m W_m^\\top$，其中 $W_m$ 具有标准正交列，这些列张成 $\\mathbb{R}^d$ 的一个 $m$ 维子空间。在瓶颈大小为 $m$ 时的训练重构误差为\n$$\nE_{\\text{train}}(m) = \\frac{1}{n} \\sum_{i=1}^n \\left\\| x_i - P_m x_i \\right\\|_2^2,\n$$\n测试重构误差为\n$$\nE_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\left\\| x^{\\text{test}}_j - P_m x^{\\text{test}}_j \\right\\|_2^2,\n$$\n其中所有的 $x_i$ 和 $x^{\\text{test}}_j$ 都已通过 $\\mu_{\\text{train}}$ 进行中心化。\n\n基本和建模假设：\n- 经验风险最小化（ERM）：估计器在 $W$ 的列为标准正交的约束下，最小化平均训练重构误差 $E_{\\text{train}}(m)$。\n- 主成分分析（PCA）：对于具有平方误差和权重共享的线性自编码器，ERM 解得到的 $W_m$ 的列是中心化训练矩阵的前 $m$ 个右奇异向量（等价于经验协方差矩阵的前 $m$ 个特征向量），因此 $P_m$ 是到该 $m$ 维子空间的正交投影算子。\n- 正交投影：对于任何 $m$，$P_m$ 是一个秩为 $m$ 的幂等对称矩阵，并且 $\\left\\| x - P_m x \\right\\|_2^2$ 是 $x$ 到由 $W_m$ 张成的子空间的平方距离。\n\n容量穿越与单位映射：\n- 插值阈值：设 $r = \\operatorname{rank}(X_{\\text{train}})$。当 $m \\ge r$ 时，存在使 $E_{\\text{train}}(m) = 0$ 的解，因为投影算子可以张成训练数据的子空间。满足此条件的最小 $m$ 记为 $m_{\\text{interp}}$。\n- 单位映射：当 $m = d$ 且 $W_d$ 有 $d$ 个标准正交列时，$P_d = I_d$，这使得 $E_{\\text{train}}(d) = 0$ 且 $E_{\\text{test}}(d) = 0$，从而将模型容量与单位映射联系起来。\n\n双峰现象研究：\n- 随着 $m$ 从 $1$ 增加到 $d$，测试误差 $E_{\\text{test}}(m)$ 可能会先下降（增加与信号对齐的方向），然后在 $m \\approx r$ 附近上升（对训练子空间过拟合），接着随着 $m \\to d$（趋近于单位映射）再次下降。这种形状被称为双峰现象。\n\n实现要求：\n- 通过从 $\\mathbb{R}^d$ 上的零均值多元正态分布 $\\mathcal{N}(0, \\Sigma)$ 中独立采样来构建 $X_{\\text{train}}$ 和 $X_{\\text{test}}$，其中 $\\Sigma$ 是通过选择一个随机标准正交基并在对角线上放置指定的特征值来构建的。设 $Q \\in \\mathbb{R}^{d \\times d}$ 为标准正交矩阵，并设置 $\\Sigma = Q \\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_d) Q^\\top$，其中用户指定的 $\\lambda_i$ 值反映了少数较大的“信号”方差和较小的“噪声”方差。\n- 使用训练均值 $\\mu_{\\text{train}}$ 对 $X_{\\text{train}}$ 和 $X_{\\text{test}}$ 进行中心化。\n- 计算中心化训练矩阵的奇异值分解（SVD），以获得由右奇异向量构成的标准正交基 $V \\in \\mathbb{R}^{d \\times d}$。对于每个 $m$，将 $W_m$ 设置为 $V$ 的前 $m$ 列，并计算 $P_m = W_m W_m^\\top$。\n- 对于每个 $m \\in \\{1, 2, \\ldots, d\\}$，计算 $E_{\\text{train}}(m)$ 和 $E_{\\text{test}}(m)$。\n- 令 $m_{\\text{peak}}$ 为最大化 $E_{\\text{test}}(m)$ 的索引 $m$（如有多个，则取最小的 $m$）。\n- 令 $m_{\\text{interp}}$ 为满足 $E_{\\text{train}}(m) \\le \\tau$ 的最小 $m$，容差 $\\tau = 10^{-12}$。\n- 定义一个布尔值 $b_{\\text{dd}}$，用于指示是否观察到双峰现象，使用以下标准：\n    - 令 $k = m_{\\text{peak}}$，并考虑峰值前后的序列。设 $E_{\\text{pre}} = \\{E_{\\text{test}}(1), \\ldots, E_{\\text{test}}(k-1)\\}$ 和 $E_{\\text{post}} = \\{E_{\\text{test}}(k+1), \\ldots, E_{\\text{test}}(d)\\}$（空序列意味着 $b_{\\text{dd}} = \\text{False}$）。\n    - 如果同时满足 $\\min(E_{\\text{pre}})  E_{\\text{test}}(k)$ 和 $\\min(E_{\\text{post}})  E_{\\text{test}}(k)$，并且 $E_{\\text{pre}}$ 中至少存在一次严格下降，且 $E_{\\text{post}}$ 中至少有一个值严格小于 $E_{\\text{test}}(k)$，则声明存在双峰现象。\n- 对于每个测试用例，输出列表 $[m_{\\text{peak}}, \\text{round}(E_{\\text{test}}(m_{\\text{peak}}), 6), m_{\\text{interp}}, b_{\\text{dd}}]$。\n\n测试套件：\n对于每个案例，参数为 $(d, n, n_{\\text{test}}, s, \\sigma_{\\text{signal}}^2, \\sigma_{\\text{noise}}^2, \\text{seed})$，其中 $d$ 是维度，$n$ 是训练样本数，$n_{\\text{test}}$ 是测试样本数，$s$ 是信号维度数，$\\sigma_{\\text{signal}}^2$ 是信号方差，$\\sigma_{\\text{noise}}^2$ 是噪声方差，$\\text{seed}$ 用于设置随机数生成器。通过将前 $s$ 个特征值设置为 $\\sigma_{\\text{signal}}^2$，其余 $d - s$ 个设置为 $\\sigma_{\\text{noise}}^2$ 来构建特征值。\n- 案例 1：$(d, n, n_{\\text{test}}, s, \\sigma_{\\text{signal}}^2, \\sigma_{\\text{noise}}^2, \\text{seed}) = (\\,60,\\,40,\\,2000,\\,12,\\,5.0,\\,0.5,\\,1\\,)$。\n- 案例 2：$(\\,60,\\,58,\\,2000,\\,12,\\,4.0,\\,0.6,\\,2\\,)$。\n- 案例 3：$(\\,30,\\,15,\\,3000,\\,0,\\,0.0,\\,1.0,\\,3\\,)$。\n- 案例 4：$(\\,20,\\,5,\\,5000,\\,8,\\,3.0,\\,0.3,\\,4\\,)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，并且本身是一个形如 $[m_{\\text{peak}}, \\text{round}(E_{\\text{test}}(m_{\\text{peak}}), 6), m_{\\text{interp}}, b_{\\text{dd}}]$ 的列表。浮点数必须四舍五入到 $6$ 位小数。例如，输出应类似于 $[[1,0.123456,1,True],[\\ldots],[\\ldots],[\\ldots]]$。", "solution": "用户的请求是验证并解决一个计算问题，该问题涉及在线性自编码器中模拟双峰现象。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n-   **模型**：线性自编码器，输入为 $x \\in \\mathbb{R}^d$，编码器为 $z = W^\\top x$，解码器为 $\\hat{x} = W W^\\top x$，其中 $W \\in \\mathbb{R}^{d \\times m}$ 的列是标准正交的。重构矩阵为 $P_m = W W^\\top$。\n-   **数据**：训练集 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$，测试集 $X_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times d}$。数据从 $\\mathcal{N}(0, \\Sigma)$ 采样。\n-   **协方差**：$\\Sigma = Q \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_d) Q^\\top$，其中 $Q$ 是一个随机标准正交矩阵。前 $s$ 个特征值 $\\{\\lambda_i\\}$ 为 $\\sigma_{\\text{signal}}^2$，其余 $d-s$ 个为 $\\sigma_{\\text{noise}}^2$。\n-   **中心化**：使用训练数据的经验均值 $\\mu_{\\text{train}}$ 对 $X_{\\text{train}}$ 和 $X_{\\text{test}}$ 进行中心化。\n-   **优化**：选择模型参数 $W_m$ 以最小化训练重构误差（经验风险最小化），这等价于主成分分析（PCA）。$W_m$ 的列是中心化训练矩阵的前 $m$ 个右奇异向量。\n-   **误差度量**：\n    -   训练误差：$E_{\\text{train}}(m) = \\frac{1}{n} \\sum_{i=1}^n \\left\\| x_i - P_m x_i \\right\\|_2^2$\n    -   测试误差：$E_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\left\\| x^{\\text{test}}_j - P_m x^{\\text{test}}_j \\right\\|_2^2$\n-   **待计算的量**：\n    -   $m_{\\text{peak}}$：最大化 $E_{\\text{test}}(m)$ 的最小瓶颈大小 $m$。\n    -   $m_{\\text{interp}}$：使 $E_{\\text{train}}(m) \\le \\tau$（其中 $\\tau = 10^{-12}$）的最小 $m$。\n    -   $b_{\\text{dd}}$：一个布尔值，指示是否根据 $E_{\\text{test}}(m)$ 曲线在 $m_{\\text{peak}}$ 周围的形状观察到双峰现象。\n-   **测试用例**：提供了四组特定的参数集 $(d, n, n_{\\text{test}}, s, \\sigma_{\\text{signal}}^2, \\sigma_{\\text{noise}}^2, \\text{seed})$。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n根据验证标准对问题进行评估：\n\n-   **科学性**：该问题基于成熟的原理。线性自编码器的 ERM 与 PCA 的等价性是机器学习中的一个标准结论，其根源在于 Eckart-Young-Mirsky 定理。通过改变模型容量（$m$）来模拟双峰现象是当代研究中用于理解过参数化模型泛化能力的一种标准技术。数据生成过程是创建结构化合成数据的常规方法。\n-   **适定性**：问题定义清晰，没有歧义。所有参数、数据生成和分析的方法（SVD）以及输出指标（$m_{\\text{peak}}$, $m_{\\text{interp}}$, $b_{\\text{dd}}$）的定义都精确指定。这确保了在给定随机种子的情况下，可以为每个测试用例计算出唯一且稳定的解。\n-   **客观性**：语言正式且客观。分析标准是定量的，没有主观解释的余地。\n-   **完整性和一致性**：问题提供了所有必要的信息（超参数、种子、容差），并且不包含内部矛盾。\n-   **可行性**：指定的维度（$d \\le 60, n \\le 58$）在计算上是可控的，使得所需的 SVD 和矩阵运算在标准硬件上是可行的。\n\n**第 3 步：结论与行动**\n\n问题是**有效的**。这是一个适定、科学合理且计算上可行的任务，直接探讨了现代机器学习理论中的一个相关主题。将按要求继续进行求解。\n\n### 解法\n\n模拟将遵循问题陈述中概述的理论原理和计算步骤来实现。分析的核心是将由瓶颈维度 $m$ 控制的模型容量与其在训练和测试数据上的性能联系起来。\n\n1.  **理论框架**：问题指定了一个权重共享的线性自编码器，其中解码器权重矩阵是编码器的转置。重构由 $\\hat{x} = W_m W_m^\\top x$ 给出。矩阵 $P_m = W_m W_m^\\top$ 是一个到由 $W_m$ 的列张成的 $m$ 维子空间的正交投影算子。经验风险最小化（ERM）原则要求我们选择 $W_m$ 来最小化平均训练重构误差 $E_{\\text{train}}(m)$。对于平方 $\\ell_2$ 范数损失，此优化问题等价于主成分分析（PCA）。最优子空间是由中心化训练数据的前 $m$ 个主成分张成的空间。这些成分是中心化训练数据矩阵 $X_c = X_{\\text{train}} - \\mu_{\\text{train}}$ 的右奇异向量。\n\n2.  **模拟算法**：\n    -   **数据生成**：对于每个测试用例，我们首先用给定的种子建立一个随机数生成器。我们构建 $d \\times d$ 的协方差矩阵 $\\Sigma = Q \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_d) Q^\\top$。设置特征值 $\\lambda_i$ 以区分“信号”和“噪声”维度，而 $Q$ 是使用 `scipy.stats` 中的例程生成的随机标准正交矩阵。然后我们从多元正态分布 $\\mathcal{N}(0, \\Sigma)$ 中采样 $n$ 个训练向量和 $n_{\\text{test}}$ 个测试向量。\n    -   **数据中心化**：通过减去其经验均值 $\\mu_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^n x_i$ 来对训练数据进行中心化，得到 $X_c$。关键是，测试数据也使用相同的 $\\mu_{\\text{train}}$ 进行中心化，以评估学习到的模型如何泛化到来自同一分布的未见数据。\n    -   **主成分基**：我们计算中心化训练矩阵的奇异值分解（SVD）：$X_c = U S V^\\top$。$V$ 的列（或 $V^\\top$ 的行）构成了训练数据的主成分标准正交基，按 $S$ 中相应的奇异值排序。\n    -   **误差曲线计算**：我们从 $1$ 到 $d$ 遍历模型容量 $m$。在每一步中：\n        -   投影算子 $P_m$ 由前 $m$ 个主向量构成：$W_m = V_{:, 1:m}$，$P_m = W_m W_m^\\top$。\n        -   使用其定义直接计算测试重构误差 $E_{\\text{test}}(m)$。这可以高效地计算为 $E_{\\text{test}}(m) = \\frac{1}{n_{\\text{test}}} \\|X_{test, c} (I_d - P_m)\\|_F^2$，其中 $\\|\\cdot\\|_F$ 是 Frobenius 范数。\n        -   训练重构误差 $E_{\\text{train}}(m)$ 可以更高效地计算。由于 SVD 和 PCA 的性质，使用前 $m$ 个分量在训练集上的总重构误差是剩余奇异值平方和。即，$E_{\\text{train}}(m) = \\frac{1}{n} \\sum_{k=m+1}^{\\text{rank}} s_k^2$，其中 $s_k$ 是 $X_c$ 的奇异值。\n\n3.  **分析与指标提取**：在计算完 $m=1, \\ldots, d$ 的完整误差曲线 $E_{\\text{train}}(m)$ 和 $E_{\\text{test}}(m)$ 后：\n    -   $m_{\\text{peak}}$ 被确定为最大化测试误差曲线 $E_{\\text{test}}$ 的最小 $m$。这一点通常标志着过拟合的开始，此时模型容量刚好足以拟合训练数据，但捕获了损害泛化能力的伪相关性。\n    -   通过找到训练误差 $E_{\\text{train}}(m)$ 首次低于数值容差 $\\tau = 10^{-12}$ 的 $m$ 来找到 $m_{\\text{interp}}$。这标志着插值阈值，在此处模型容量足以完美记住训练数据（$m \\ge \\operatorname{rank}(X_c)$）。\n    -   $b_{\\text{dd}}$ 由一个特定标准确定，该标准检查峰值（$m_{\\text{peak}}$）前是否存在特有的“U形”以及随后的测试误差下降，这构成了“第二次下降”。这第二次下降发生在模型容量趋近于 $d$ 时，投影算子 $P_m$ 趋近于单位矩阵，单位矩阵在任何数据上都能轻易实现零误差。\n\n这个全面的过程允许在一个受控且有理论基础的线性模型内，对双峰现象进行定量研究。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ortho_group\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (60, 40, 2000, 12, 5.0, 0.5, 1),\n        (60, 58, 2000, 12, 4.0, 0.6, 2),\n        (30, 15, 3000, 0, 0.0, 1.0, 3),\n        (20, 5, 5000, 8, 3.0, 0.3, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        results.append(result)\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(d, n, n_test, s, sigma_signal_sq, sigma_noise_sq, seed):\n    \"\"\"\n    Runs the double descent simulation for a single set of parameters.\n\n    Args:\n        d (int): Dimensionality of the data space.\n        n (int): Number of training samples.\n        n_test (int): Number of test samples.\n        s (int): Number of signal dimensions.\n        sigma_signal_sq (float): Signal variance.\n        sigma_noise_sq (float): Noise variance.\n        seed (int): Random seed.\n\n    Returns:\n        list: A list containing [m_peak, E_test_at_peak, m_interp, b_dd].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct the covariance matrix Sigma\n    lambdas = np.array([sigma_signal_sq] * s + [sigma_noise_sq] * (d - s))\n    Q = ortho_group.rvs(dim=d, random_state=rng)\n    Sigma = Q @ np.diag(lambdas) @ Q.T\n\n    # 2. Generate training and test data\n    mean = np.zeros(d)\n    X_train = rng.multivariate_normal(mean=mean, cov=Sigma, size=n)\n    X_test = rng.multivariate_normal(mean=mean, cov=Sigma, size=n_test)\n\n    # 3. Center data using the training mean\n    mu_train = np.mean(X_train, axis=0)\n    X_train_centered = X_train - mu_train\n    X_test_centered = X_test - mu_train\n\n    # 4. Compute SVD of the centered training data\n    # U, S, Vt where V are the right singular vectors (principal components)\n    U, S_vals, Vt = np.linalg.svd(X_train_centered, full_matrices=False)\n    V = Vt.T\n\n    E_train_curve = []\n    E_test_curve = []\n\n    # Pad singular values array with zeros up to dimension d if necessary\n    S_sq_full = np.zeros(d)\n    S_sq_full[:len(S_vals)] = S_vals**2\n\n    # 5. Iterate over bottleneck size m from 1 to d\n    for m in range(1, d + 1):\n        # Efficiently compute training error using singular values\n        # E_train(m) = (1/n) * sum_{k=m to rank-1} s_k^2\n        train_error = np.sum(S_sq_full[m:]) / n\n        E_train_curve.append(train_error)\n\n        # Compute test error via projection\n        W_m = V[:, :m]\n        P_m = W_m @ W_m.T\n        I = np.identity(d)\n        \n        # Error matrix for test set: X_test_c - X_test_c @ P_m = X_test_c @ (I - P_m)\n        error_matrix_test = X_test_centered @ (I - P_m)\n        # Sum of squared L2 norms is the squared Frobenius norm\n        test_error = np.sum(error_matrix_test**2) / n_test\n        E_test_curve.append(test_error)\n\n    # 6. Calculate the required metrics\n    E_test_curve = np.array(E_test_curve)\n    E_train_curve = np.array(E_train_curve)\n\n    # m_peak: smallest m that maximizes test error\n    m_peak = np.argmax(E_test_curve) + 1\n    e_test_at_peak = E_test_curve[m_peak - 1]\n\n    # m_interp: smallest m where training error is effectively zero\n    tau = 1e-12\n    interp_indices = np.where(E_train_curve = tau)[0]\n    # The problem setup guarantees interpolation happens\n    m_interp = interp_indices[0] + 1 if len(interp_indices) > 0 else d + 1\n\n    # b_dd: boolean for double descent\n    k = m_peak\n    E_pre = E_test_curve[:k-1]\n    E_post = E_test_curve[k:] # from index k to the end\n\n    b_dd = False\n    # Check for empty sequences before proceeding\n    if E_pre.size > 0 and E_post.size > 0:\n        cond1 = np.min(E_pre)  e_test_at_peak\n        cond2 = np.min(E_post)  e_test_at_peak\n        \n        # \"at least one strict decrease somewhere in E_pre\"\n        # Implemented as not being monotonically non-decreasing\n        # Or more directly, there is at least one adjacent pair with a decrease\n        # This correctly handles sequences of length 1 (returns False)\n        cond3 = any(E_pre[i] > E_pre[i+1] for i in range(len(E_pre)-1))\n\n        if cond1 and cond2 and cond3:\n            b_dd = True\n            \n    return [m_peak, round(e_test_at_peak, 6), m_interp, b_dd]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3183618"}]}