{"hands_on_practices": [{"introduction": "在标准的监督学习中，交叉熵损失函数平等地对待所有分类错误。然而，许多现实世界的问题具有固有的层级结构，其中混淆兄弟节点（例如，将一种狗误认为另一种狗）比混淆远亲节点（例如，将狗误认为汽车）的代价要小。本练习将指导您实现并比较三种不同的损失函数——标准的“扁平”交叉熵损失和两种考虑了层级结构的损失函数，从而亲身体验如何通过定制化风险函数将领域知识嵌入到学习过程中。[@problem_id:3160884]", "problem": "考虑一个分层分类任务，其中的类别是分类树的叶节点。设该分类法为一个有根树，其具有单个根节点和代表类别的内部节点。学习范式是经验风险最小化：给定一个输入，模型输出一个实值分数（logits）向量，该向量通过 Softmax 函数转换为叶类别上的概率分布。目标是评估损失函数的设计选择如何反映分类体系，并将其与扁平交叉熵进行比较，从而分析一种层次感知风险。\n\n基本基础：\n- 经验风险定义为损失函数在数据分布上的期望。对于真实叶类别索引为 $y$、预测的叶类别 $k$ 上的类别概率为 $p(k)$ 的单个样本，瞬时风险是所选的损失 $\\ell(y, p)$。\n- Softmax 使用自然对数底数 $e$ 将 logits $z(k)$ 转换为概率 $p(k) = \\exp(z(k)) \\big/ \\sum_{j} \\exp(z(j))$。\n- 扁平交叉熵（在叶节点级别定义）为 $\\ell_{\\mathrm{CE}}(y, p) = -\\log p(y)$。\n- 层次感知的期望成本是根据叶类别之间的分类距离函数 $d(y, k)$ 构建的，得到 $\\ell_{\\mathrm{HEC}}(y, p) = \\sum_{k} p(k) \\, d(y, k)$。\n- 层次感知的路径交叉熵通过塑造损失函数，将概率质量分配到从根节点到真实叶节点的路径上。设 $P(n)$ 为内部节点或叶节点 $n$ 的概率质量，定义为其所有后代叶节点概率的总和。设 $\\mathrm{path}(y)$ 表示从根节点到 $y$ 的唯一路径上的节点集合。排除根节点（其概率质量恒为 $1$），定义 $\\ell_{\\mathrm{HPCE}}(y, p) = -\\sum_{n \\in \\mathrm{path}(y) \\setminus \\{\\mathrm{root}\\}} \\log P(n)$。这种损失函数对那些将概率质量分配到正确分支之外的预测施加的惩罚，比对那些在正确分支内混淆兄弟节点的预测更为严厉。\n\n分类体系：\n- 该分类体系有一个根节点和两个内部节点，每个内部节点有两个叶节点：\n  - 内部节点 $A$ 及其叶节点 $A1$ 和 $A2$。\n  - 内部节点 $B$ 及其叶节点 $B1$ 和 $B2$。\n- 叶节点的索引如下：\n  - $0 \\leftrightarrow A1$, $1 \\leftrightarrow A2$, $2 \\leftrightarrow B1$, $3 \\leftrightarrow B2$。\n- 叶节点 $i$ 和 $j$ 之间的分类距离 $d(i, j)$ 是树中两个叶节点之间最短路径的长度（以边的数量计算）。因此：\n  - 对所有 $i$，$d(i, i) = 0$。\n  - 对于同一内部节点下的兄弟叶节点，$d(0, 1) = 2$ 且 $d(2, 3) = 2$。\n  - 对于不同内部节点下的叶节点，$d(0, 2) = 4$，$d(0, 3) = 4$，$d(1, 2) = 4$，$d(1, 3) = 4$。\n\n测试套件：\n每个测试用例包含一对 $(\\text{logits}, y)$，其中 logits 是一个包含 4 个实数的列表， $y$ 是真实的叶节点索引。在评估损失之前，必须从 logits 计算出 Softmax 概率 $p(k)$。使用自然对数，并将所有损失计算为实数。将每个损失四舍五入到 6 位小数。\n\n- 用例 $1$（理想情况，正确的叶节点得分很高）：logits $[4, 1, -1, -2]$，$y = 0$。\n- 用例 $2$（在 $A$ 内部的兄弟节点混淆）：logits $[1, 3.5, -1, -2]$，$y = 0$。\n- 用例 $3$（与 $B$ 分支的跨分支混淆）：logits $[-1, -1.5, 3.0, 2.5]$，$y = 0$。\n- 用例 $4$（无信息的均匀 logits）：logits $[0, 0, 0, 0]$，$y = 1$。\n- 用例 $5$（极端错误分支集中）：logits $[10, -10, -10, -10]$，$y = 3$。\n\n对每个测试用例，计算：\n- 扁平交叉熵 $\\ell_{\\mathrm{CE}}(y, p)$。\n- 层次感知的期望成本 $\\ell_{\\mathrm{HEC}}(y, p)$。\n- 层次感知的路径交叉熵 $\\ell_{\\mathrm{HPCE}}(y, p)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的、针对所有测试用例的逗号分隔的结果列表。每个测试用例的结果本身必须是包含三个浮点数的列表，顺序为 $[\\ell_{\\mathrm{CE}}, \\ell_{\\mathrm{HEC}}, \\ell_{\\mathrm{HPCE}}]$，每个浮点数都四舍五入到 6 位小数。最终输出必须如下所示：\n$[[\\ell_{\\mathrm{CE}}^{(1)}, \\ell_{\\mathrm{HEC}}^{(1)}, \\ell_{\\mathrm{HPCE}}^{(1)}],[\\ell_{\\mathrm{CE}}^{(2)}, \\ell_{\\mathrm{HEC}}^{(2)}, \\ell_{\\mathrm{HPCE}}^{(2)}],\\dots]$。", "solution": "该问题要求计算和比较用于分层分类任务的三种不同损失函数。该任务定义在一个特定的树状结构分类体系上。这三种损失函数分别是标准的扁平交叉熵（$\\ell_{\\mathrm{CE}}$）、层次感知的期望成本（$\\ell_{\\mathrm{HEC}}$）和层次感知的路径交叉熵（$\\ell_{\\mathrm{HPCE}}$）。目标是针对由测试用例表示的五种不同场景评估这些损失，每个测试用例由一个 logits 向量和一个真实类别标签组成。\n\n首先，我们对问题的组成部分进行形式化。给定一个 logits 向量 $z = [z(0), z(1), z(2), z(3)]$，4 个叶类别上的概率分布 $p = [p(0), p(1), p(2), p(3)]$ 是通过使用 Softmax 函数得到的：\n$$ p(k) = \\frac{\\exp(z(k))}{\\sum_{j=0}^{3} \\exp(z(j))} \\quad \\text{for } k \\in \\{0, 1, 2, 3\\} $$\n在整个计算过程中都使用自然对数 $\\log$。\n\n该分类体系是一个具有两级分支的有根树。根节点有两个子节点，即内部节点 $A$ 和 $B$。节点 $A$ 有两个叶子节点，$A1$（索引 0）和 $A2$（索引 1）。节点 $B$ 有两个叶子节点，$B1$（索引 2）和 $B2$（索引 3）。\n\n对于给定的真实叶节点索引 $y$ 和预测的概率分布 $p$，这三种损失函数的定义如下：\n\n$1$。 **扁平交叉熵（$\\ell_{\\mathrm{CE}}$）**：这是真实类别的负对数似然。它将所有类别视为独立的，不考虑层次结构。\n$$ \\ell_{\\mathrm{CE}}(y, p) = -\\log p(y) $$\n\n$2$。 **层次感知的期望成本（$\\ell_{\\mathrm{HEC}}$）**：该损失函数将风险定义为真实类别与预测类别之间的期望距离，其中期望是基于模型的预测概率分布计算的。\n$$ \\ell_{\\mathrm{HEC}}(y, p) = \\sum_{k=0}^{3} p(k) \\, d(y, k) $$\n距离 $d(i, j)$ 是叶节点 $i$ 和叶节点 $j$ 之间最短路径上的边数。对于我们的分类体系，距离矩阵 $D$（其中 $D_{ij}=d(i,j)$）是：\n$$\nD = \\begin{pmatrix}\n0  & 2 & 4 & 4 \\\\\n2 & 0 & 4 & 4 \\\\\n4 & 4 & 0 & 2 \\\\\n4 & 4 & 2 & 0\n\\end{pmatrix}\n$$\n这种损失根据误分类在层次结构中的距离进行惩罚，对兄弟节点混淆（例如，$A1$ vs. $A2$，距离为 $2$）的惩罚比对跨分支混淆（例如，$A1$ vs. $B1$，距离为 $4$）的惩罚要轻。\n\n$3$。 **层次感知的路径交叉熵（$\\ell_{\\mathrm{HPCE}}$）**：这种损失鼓励模型沿着从根节点到真实叶节点的正确路径分配概率质量。它是通过对该路径上所有节点（不包括根节点）的负对数概率求和来计算的。\n$$ \\ell_{\\mathrm{HPCE}}(y, p) = -\\sum_{n \\in \\mathrm{path}(y) \\setminus \\{\\mathrm{root}\\}} \\log P(n) $$\n$P(n)$ 是节点 $n$ 的总概率质量，通过对 $n$ 的所有后代叶节点的概率求和来计算。对于我们的分类体系：\n-   内部节点 $A$ 的概率：$P(A) = p(0) + p(1)$。\n-   内部节点 $B$ 的概率：$P(B) = p(2) + p(3)$。\n-   叶节点（例如 $P(A1)$）的概率就是它自身的概率 $p(0)$。\n\n对于每个真实类别 $y$，$\\ell_{\\mathrm{HPCE}}$ 的具体公式是：\n-   如果 $y=0$（叶节点 $A1$）：$\\mathrm{path}(0) = \\{\\text{root}, A, A1\\}$。\n    $\\ell_{\\mathrm{HPCE}}(0, p) = -[\\log P(A) + \\log P(A1)] = -[\\log(p(0)+p(1)) + \\log p(0)]$。\n-   如果 $y=1$（叶节点 $A2$）：$\\mathrm{path}(1) = \\{\\text{root}, A, A2\\}$。\n    $\\ell_{\\mathrm{HPCE}}(1, p) = -[\\log P(A) + \\log P(A2)] = -[\\log(p(0)+p(1)) + \\log p(1)]$。\n-   如果 $y=2$（叶节点 $B1$）：$\\mathrm{path}(2) = \\{\\text{root}, B, B1\\}$。\n    $\\ell_{\\mathrm{HPCE}}(2, p) = -[\\log P(B) + \\log P(B1)] = -[\\log(p(2)+p(3)) + \\log p(2)]$。\n-   如果 $y=3$（叶节点 $B2$）：$\\mathrm{path}(3) = \\{\\text{root}, B, B2\\}$。\n    $\\ell_{\\mathrm{HPCE}}(3, p) = -[\\log P(B) + \\log P(B2)] = -[\\log(p(2)+p(3)) + \\log p(3)]$。\n\n让我们逐步计算用例 1：logits $[4, 1, -1, -2]$，$y = 0$。\n-   **第 1 步：计算概率。**\n    logits 的指数为 $[\\exp(4), \\exp(1), \\exp(-1), \\exp(-2)] \\approx [54.598, 2.718, 0.368, 0.135]$。\n    总和为 $\\approx 57.820$。\n    概率向量为 $p \\approx [54.598/57.820, 2.718/57.820, 0.368/57.820, 0.135/57.820] \\approx [0.944285, 0.047012, 0.006363, 0.002341]$。\n-   **第 2 步：计算损失。**\n    -   $\\ell_{\\mathrm{CE}}(0, p) = -\\log(p(0)) = -\\log(0.944285) \\approx 0.057310$。\n    -   $\\ell_{\\mathrm{HEC}}(0, p) = \\sum_k p(k) d(0, k) = p(0) \\cdot 0 + p(1) \\cdot 2 + p(2) \\cdot 4 + p(3) \\cdot 4 \\approx (0.047012 \\cdot 2) + (0.006363 \\cdot 4) + (0.002341 \\cdot 4) \\approx 0.094024 + 0.025452 + 0.009364 \\approx 0.128840$。\n    -   $\\ell_{\\mathrm{HPCE}}(0, p) = -[\\log(p(0)+p(1)) + \\log(p(0))] = -[\\log(0.944285+0.047012) + \\log(0.944285)] = -[\\log(0.991297) + 0.057310] \\approx -[-0.008745 + 0.057310] \\approx -0.048565$。\n    - 勘误：在原始 solution 的计算中，$\\ell_{HPCE}$ 的 $-\\log(p(0))$ 被错误地用 $\\ell_{CE}$ 的值 0.057310 替代，正确的应该是 $-\\log(0.944285) \\approx 0.057310$。但 $\\log(0.991297)$ 的值是负的，因此 $-[\\log(0.991297) + \\log(0.944285)]$ 应该是 `-[-0.008745 - 0.057310] = 0.066055`。原始文本的计算存在符号错误。\n    - 正确的计算：$\\ell_{\\mathrm{HPCE}}(0, p) = -(\\log(0.991297) + \\log(0.944285)) \\approx -(-0.008745 - 0.057310) = 0.066055$。\n    - *编辑注：原始 solution 文本的计算示例部分存在小错误，但不影响最终代码的正确性，因此保留其核心逻辑，只修正此处的解释。*\n\n结果展示了每种损失的特性。例如，在用例 2（兄弟节点混淆）中，$\\ell_{\\mathrm{HEC}}$ 显著低于用例 3（跨分支混淆），这反映了错误的分类距离更小。在用例 2 中，$\\ell_{\\mathrm{HPCE}}$ 比 $\\ell_{\\mathrm{CE}}$ 更宽容，因为大部分概率被保持在正确的超类内，但在用例 3 中，由于概率质量泄露到了一个完全不同的分类树分支，其值很高。下面的 Python 代码系统地将此方法应用于所有测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes three different loss functions for hierarchical classification\n    based on a defined taxonomy and a set of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([4, 1, -1, -2], 0),   # Case 1: Happy path\n        ([1, 3.5, -1, -2], 0), # Case 2: Sibling confusion\n        ([-1, -1.5, 3.0, 2.5], 0), # Case 3: Cross-branch confusion\n        ([0, 0, 0, 0], 1),      # Case 4: Uninformative uniform\n        ([10, -10, -10, -10], 3)    # Case 5: Extreme wrong branch\n    ]\n\n    # Taxonomy distance matrix d(i, j)\n    # Leaves: 0=A1, 1=A2, 2=B1, 3=B2\n    # d(i,i)=0\n    # d(0,1)=2, d(2,3)=2\n    # d(0,2)=4, d(0,3)=4, d(1,2)=4, d(1,3)=4\n    distance_matrix = np.array([\n        [0, 2, 4, 4],\n        [2, 0, 4, 4],\n        [4, 4, 0, 2],\n        [4, 4, 2, 0]\n    ])\n\n    results = []\n    \n    for logits, y in test_cases:\n        # Convert logits to numpy array for vectorized operations\n        z = np.array(logits, dtype=np.float64)\n\n        # Compute probabilities using the Softmax function\n        # A numerically stable version of Softmax is used: exp(z - max(z))\n        exps = np.exp(z - np.max(z))\n        p = exps / np.sum(exps)\n\n        # --- Loss Calculation ---\n\n        # 1. Flat Cross-Entropy (l_ce)\n        l_ce = -np.log(p[y])\n\n        # 2. Hierarchy-aware Expected Cost (l_hec)\n        l_hec = np.sum(p * distance_matrix[y])\n\n        # 3. Hierarchy-aware Path Cross-Entropy (l_hpce)\n        # Taxonomy: root -> {A, B}; A -> {0, 1}; B -> {2, 3}\n        if y in [0, 1]:  # True class is in group A\n            # P(A) = p(0) + p(1)\n            p_group = p[0] + p[1]\n            # l_hpce = -(log(P(A)) + log(P(leaf)))\n            l_hpce = -(np.log(p_group) + np.log(p[y]))\n        else:  # True class is in group B (y in [2, 3])\n            # P(B) = p(2) + p(3)\n            p_group = p[2] + p[3]\n            # l_hpce = -(log(P(B)) + log(P(leaf)))\n            l_hpce = -(np.log(p_group) + np.log(p[y]))\n            \n        # Round results to 6 decimal places as required\n        case_result = [\n            round(l_ce, 6),\n            round(l_hec, 6),\n            round(l_hpce, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) correctly formats the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160884"}, {"introduction": "自监督学习是利用无标签数据学习有效表征的强大范式，其核心思想是设计一个“代理任务”（pretext task）来生成伪标签。本练习将通过一个简化的数学模型，让您定量地比较两种流行的代理任务——旋转预测和拼图游戏——在保留下游任务所需信息方面的表现。通过计算贝叶斯风险和互信息，您将深入理解设计有效自监督方法的关键原则，即代理任务与下游任务之间的“隐式不变性”对齐。[@problem_id:3160860]", "problem": "您将通过一个数学上精确的代理模型，分析深度学习中的两种自监督学习范式。潜变量是一个连续的平面方向角 $\\theta \\in [0,2\\pi)$，以弧度为单位。下游任务是仅使用由前置任务保留的信息来估计 $\\theta$。您将比较两种定义了不同隐式不变性的前置任务，并量化哪一种与下游几何结构更吻合。\n\n定义与设置：\n- 令 $\\theta$ 为一个在区间 $[a,b] \\subseteq [0,2\\pi)$ 上服从均匀分布的随机变量（角度以弧度为单位）。\n- 令 $p$ 为一个与 $\\theta$ 无关的离散排列标签。\n- 旋转预测前置任务通过将 $\\theta$ 在 $[0,2\\pi)$ 上分箱到 $K$ 个等宽类别中来创建标签 $Y_{\\mathrm{rot}}$：$Y_{\\mathrm{rot}} = \\left\\lfloor K \\cdot \\theta / (2\\pi) \\right\\rfloor \\in \\{0,1,\\dots,K-1\\}$。\n- 拼图前置任务创建一个与 $\\theta$ 无关的标签 $Y_{\\mathrm{jig}} = p$。\n\n使用的基本原理：\n- 在平方误差损失下，给定观测变量 $Y$，目标变量 $Z$ 的贝叶斯估计量是条件均值 $\\hat{Z}(Y) = \\mathbb{E}[Z \\mid Y]$，相应的最小均方误差（贝叶斯风险）是 $\\mathcal{R}^\\star(Y) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z \\mid Y])^2\\big] = \\mathbb{E}\\big[\\mathrm{Var}(Z \\mid Y)\\big]$。\n- 互信息（MI）定义为 $I(U;V) = \\mathbb{E}\\left[\\log \\frac{p_{U,V}(U,V)}{p_U(U) p_V(V)}\\right]$，并满足数据处理不等式。角度必须以弧度处理。\n\n您的目标：\n- 对于下方的每个测试用例，计算以下两个量：\n  1. 对齐得分 $\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}})$，其中 $\\mathcal{R}^\\star(Y)$ 是仅使用 $Y$ 估计 $\\theta$ 的贝叶斯最优均方误差。正值表示对于平方误差估计，旋转预测比拼图保留了更多关于 $\\theta$ 的信息。\n  2. 互信息 $I(\\theta; Y_{\\mathrm{rot}})$，单位为奈特（nats）。\n- 仅使用上述数学原理。角度必须以弧度为单位。\n\n测试套件（角度以弧度为单位）：\n- 用例1：$K=4$, $[a,b]=[0,2\\pi)$。\n- 用例2：$K=8$, $[a,b]=[0,2\\pi)$。\n- 用例3：$K=4$, $[a,b]=[0,\\pi)$。\n- 用例4：$K=1$, $[a,b]=[0,2\\pi)$。\n- 用例5：$K=4$, $[a,b]=[0,\\pi/8)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为逗号分隔的配对列表，每个配对的形式为 $[\\Delta_{\\mathrm{MSE}}, I(\\theta; Y_{\\mathrm{rot}})]$，按上述用例的顺序排列，所有内容都包含在一对最外层的方括号内。例如，输出应类似于 $[[x1,y1],[x2,y2],...,[x5,y5]]$，其中每个 $x_i$ 和 $y_i$ 都是浮点值。", "solution": "### 步骤1：提取已知条件\n- **潜变量**：一个连续的平面方向角 $\\theta \\in [0,2\\pi)$，以弧度为单位。\n- **$\\theta$ 的分布**：在区间 $[a,b] \\subseteq [0,2\\pi)$ 上均匀分布。\n- **独立变量**：一个与 $\\theta$ 无关的离散排列标签 $p$。\n- **旋转前置任务标签**：$Y_{\\mathrm{rot}} = \\left\\lfloor K \\cdot \\theta / (2\\pi) \\right\\rfloor \\in \\{0,1,\\dots,K-1\\}$。\n- **拼图前置任务标签**：$Y_{\\mathrm{jig}} = p$。\n- **贝叶斯估计量**：对于给定的观测变量 $Y$，在平方误差损失下，目标变量 $Z$ 的贝叶斯估计量是条件均值 $\\hat{Z}(Y) = \\mathbb{E}[Z \\mid Y]$。\n- **贝叶斯风险**：最小均方误差是 $\\mathcal{R}^\\star(Y) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z \\mid Y])^2\\big] = \\mathbb{E}\\big[\\mathrm{Var}(Z \\mid Y)\\big]$。\n- **互信息 (MI)**: $I(U;V) = \\mathbb{E}\\left[\\log \\frac{p_{U,V}(U,V)}{p_U(U) p_V(V)}\\right]$。\n- **目标**：计算 $\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}})$ 和 $I(\\theta; Y_{\\mathrm{rot}})$，其中目标变量是 $\\theta$。\n- **测试套件**：\n    - 用例1：$K=4$, $[a,b]=[0,2\\pi)$。\n    - 用例2：$K=8$, $[a,b]=[0,2\\pi)$。\n    - 用例3：$K=4$, $[a,b]=[0,\\pi)$。\n    - 用例4：$K=1$, $[a,b]=[0,2\\pi)$。\n    - 用例5：$K=4$, $[a,b]=[0,\\pi/8)$。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题是科学上合理的，所有待计算的变量、分布和量都有精确的定义。\n\n### 基于原理的解决方案\n目标是为每个测试用例计算两个量：对齐得分 $\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}})$ 和互信息 $I(\\theta; Y_{\\mathrm{rot}})$。估计的目标变量是角度 $\\theta$。\n\n$\\theta$ 的概率密度函数 (PDF)，它在 $[a,b]$ 上均匀分布，由下式给出：\n$$\np(\\theta) = \\begin{cases} \\frac{1}{b-a} & \\text{if } a \\le \\theta < b \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\n\n**1. $\\mathcal{R}^\\star(Y_{\\mathrm{jig}})$ 的计算**\n拼图前置任务的贝叶斯风险是 $\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\mathbb{E}[\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{jig}})]$。问题陈述指出，拼图标签 $p$（因此 $Y_{\\mathrm{jig}}$）与 $\\theta$ 无关。对一个独立变量进行条件化不提供任何信息，因此给定 $Y_{\\mathrm{jig}}$ 时 $\\theta$ 的条件分布与其边际分布相同。\n$$\np(\\theta \\mid Y_{\\mathrm{jig}}) = p(\\theta)\n$$\n因此，条件方差等于边际方差：$\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{jig}}) = \\mathrm{Var}(\\theta)$。在区间 $[a,b]$ 上的连续均匀分布的方差是 $\\frac{(b-a)^2}{12}$。\n$$\n\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\mathrm{Var}(\\theta) = \\frac{(b-a)^2}{12}\n$$\n\n**2. $\\mathcal{R}^\\star(Y_{\\mathrm{rot}})$ 的计算**\n旋转前置任务的贝叶斯风险是 $\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\mathbb{E}[\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}})]$。这可以使用全期望定律来计算：\n$$\n\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\sum_{k=0}^{K-1} P(Y_{\\mathrm{rot}}=k) \\cdot \\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}}=k)\n$$\n条件 $Y_{\\mathrm{rot}}=k$ 意味着 $\\lfloor K \\theta / (2\\pi) \\rfloor = k$，这将 $\\theta$ 限制在分箱 $B_k = [\\frac{2\\pi k}{K}, \\frac{2\\pi (k+1)}{K})$ 内。$\\theta$ 的原始分布在 $[a,b]$ 上是均匀的。因此，给定 $Y_{\\mathrm{rot}}=k$，我们知道 $\\theta$ 必须位于交集 $I_k = [a,b] \\cap B_k$ 中。给定 $Y_{\\mathrm{rot}}=k$ 时 $\\theta$ 的条件分布在区间 $I_k$ 上是均匀的。\n\n令 $L_k$ 为区间 $I_k$ 的长度。观测到 $Y_{\\mathrm{rot}}=k$ 的概率是 $P(Y_{\\mathrm{rot}}=k) = \\frac{L_k}{b-a}$。在长度为 $L_k$ 的区间上均匀分布的方差是 $\\frac{L_k^2}{12}$。因此，$\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}}=k) = \\frac{L_k^2}{12}$。\n\n将这些代入 $\\mathcal{R}^\\star(Y_{\\mathrm{rot}})$ 的公式中：\n$$\n\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\sum_{k=0}^{K-1} \\left( \\frac{L_k}{b-a} \\right) \\left( \\frac{L_k^2}{12} \\right) = \\frac{1}{12(b-a)} \\sum_{k=0}^{K-1} L_k^3\n$$\n\n**3. $\\Delta_{\\mathrm{MSE}}$ 的计算**\n对齐得分是两个风险之差：\n$$\n\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\frac{(b-a)^2}{12} - \\frac{1}{12(b-a)} \\sum_{k=0}^{K-1} L_k^3\n$$\n\n**4. $I(\\theta; Y_{\\mathrm{rot}})$ 的计算**\n互信息 $I(\\theta; Y_{\\mathrm{rot}})$可以表示为 $I(\\theta; Y_{\\mathrm{rot}}) = H(Y_{\\mathrm{rot}}) - H(Y_{\\mathrm{rot}} \\mid \\theta)$。由于 $Y_{\\mathrm{rot}}$ 是 $\\theta$ 的一个确定性函数，条件熵 $H(Y_{\\mathrm{rot}} \\mid \\theta) = 0$。互信息简化为标签 $Y_{\\mathrm{rot}}$ 的熵：\n$$\nI(\\theta; Y_{\\mathrm{rot}}) = H(Y_{\\mathrm{rot}})\n$$\n对于离散随机变量，熵由香农公式给出。令 $p_k = P(Y_{\\mathrm{rot}}=k) = \\frac{L_k}{b-a}$。\n$$\nI(\\theta; Y_{\\mathrm{rot}}) = -\\sum_{k=0}^{K-1} p_k \\log(p_k) = -\\sum_{k \\text{ s.t. } L_k>0} \\frac{L_k}{b-a} \\log\\left(\\frac{L_k}{b-a}\\right)\n$$\n对数是自然对数（底为 $e$），单位是奈特（nats）。\n\n**每个测试用例的计算摘要**\n通过确定分箱交集长度 $L_k$，将上述公式应用于每种情况。\n\n- **用例1**：$K=4$, $[a,b]=[0,2\\pi)$。\n  $b-a=2\\pi$。$L_0=L_1=L_2=L_3=\\pi/2$。\n  $\\Delta_{\\mathrm{MSE}} = \\frac{\\pi^2}{3} - \\frac{\\pi^2}{48} = \\frac{5\\pi^2}{16}$。\n  $p_k = 1/4$。$I(\\theta; Y_{\\mathrm{rot}}) = \\log(4)$。\n\n- **用例2**：$K=8$, $[a,b]=[0,2\\pi)$。\n  $b-a=2\\pi$。$L_k=\\pi/4$ for $k=0,\\dots,7$。\n  $\\Delta_{\\mathrm{MSE}} = \\frac{\\pi^2}{3} - \\frac{\\pi^2}{192} = \\frac{21\\pi^2}{64}$。\n  $p_k = 1/8$。$I(\\theta; Y_{\\mathrm{rot}}) = \\log(8)$。\n\n- **用例3**：$K=4$, $[a,b]=[0,\\pi)$。\n  $b-a=\\pi$。$L_0=\\pi/2, L_1=\\pi/2, L_2=0, L_3=0$。\n  $\\Delta_{\\mathrm{MSE}} = \\frac{\\pi^2}{12} - \\frac{\\pi^2}{48} = \\frac{\\pi^2}{16}$。\n  $p_0=p_1=1/2$。$I(\\theta; Y_{\\mathrm{rot}}) = \\log(2)$。\n\n- **用例4**：$K=1$, $[a,b]=[0,2\\pi)$。\n  $Y_{\\mathrm{rot}}$ 始终为 $0$，不提供任何信息。$\\Delta_{\\mathrm{MSE}} = 0$, $I(\\theta; Y_{\\mathrm{rot}}) = 0$。\n\n- **用例5**：$K=4$, $[a,b]=[0,\\pi/8)$。\n  $Y_{\\mathrm{rot}}$ 始终为 $0$，不提供任何信息。$\\Delta_{\\mathrm{MSE}} = 0$, $I(\\theta; Y_{\\mathrm{rot}}) = 0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics(K, a, b):\n    \"\"\"\n    Calculates the alignment score Delta_MSE and mutual information I(theta; Y_rot).\n\n    Args:\n        K (int): The number of bins for rotation prediction.\n        a (float): The start of the uniform interval for theta.\n        b (float): The end of the uniform interval for theta.\n\n    Returns:\n        list: A list containing [Delta_MSE, I(theta; Y_rot)].\n    \"\"\"\n    # Edge case: if a=b, theta is a constant. All variances and MIs are 0.\n    if np.isclose(a, b):\n        return [0.0, 0.0]\n    \n    b_minus_a = b - a\n\n    # Calculate Risk for Jigsaw Pretext\n    # R_jig = Var(theta) for theta ~ U(a, b)\n    risk_jig = (b_minus_a)**2 / 12.0\n\n    # Calculate Risk for Rotation Pretext\n    sum_L_cubed = 0.0\n    L_values = []\n    \n    for k in range(K):\n        bin_start = 2.0 * np.pi * k / K\n        bin_end = 2.0 * np.pi * (k + 1.0) / K\n        \n        # Calculate intersection of [a, b] and [bin_start, bin_end)\n        intersect_start = max(a, bin_start)\n        intersect_end = min(b, bin_end)\n        \n        Lk = max(0.0, intersect_end - intersect_start)\n        \n        sum_L_cubed += Lk**3\n        L_values.append(Lk)\n\n    # R_rot = E[Var(theta | Y_rot)]\n    risk_rot = sum_L_cubed / (12.0 * b_minus_a)\n    \n    # Calculate Delta_MSE\n    delta_mse = risk_jig - risk_rot\n\n    # Calculate Mutual Information I(theta; Y_rot)\n    # I(theta; Y_rot) = H(Y_rot)\n    mutual_info = 0.0\n    for Lk in L_values:\n        if Lk > 0:\n            pk = Lk / b_minus_a\n            # Ensure pk is not slightly > 1 due to float precision\n            pk = min(pk, 1.0) \n            mutual_info -= pk * np.log(pk)\n\n    return [delta_mse, mutual_info]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes results, and prints them in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (4, 0.0, 2.0 * np.pi),  # Case 1\n        (8, 0.0, 2.0 * np.pi),  # Case 2\n        (4, 0.0, np.pi),        # Case 3\n        (1, 0.0, 2.0 * np.pi),  # Case 4\n        (4, 0.0, np.pi / 8.0)    # Case 5\n    ]\n\n    results_str = []\n    for case in test_cases:\n        K, a, b = case\n        result = calculate_metrics(K, a, b)\n        # Format each pair as [x,y] without spaces\n        results_str.append(f\"[{result[0]},{result[1]}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3160860"}, {"introduction": "生成模型旨在学习数据的底层分布以创造全新的样本，这是机器学习的一个核心范式。本练习将引导您探索两种强大但概念上截然不同的方法：自回归模型，它通过逐步预测来构建序列；以及受扩散启发的模型，它学习逆转一个逐步加噪的过程。通过在一个可控的序列任务上对比它们的性能，您将使用对数似然和样本质量两种度量标准，来揭示这两种主流生成范式之间的根本性权衡。[@problem_id:3160960]", "problem": "您需要在一个可控的、可解析处理的序列预测任务上，实现并研究两种不同的机器学习范式：一种是具有条件密度 $p_{\\theta}(x_t \\mid x_{t-1})$ 的自回归模型，另一种是受去噪扩散启发的生成器。数据源自一阶线性高斯自回归（AR(1)）过程。您的任务是在此数据上训练每个模型，并根据两个关键指标评估它们的性能：(a) 在评估集上的每维度平均负对数似然（NLL），以及 (b) 模型隐含的平稳协方差与真实协方差之间的 2-瓦瑟斯坦距离，作为样本质量的代理。为此，您将为三种不同的 AR(1) 参数配置运行实验，并报告每种配置下两种模型的四项指标。", "solution": "我们从第一性原理阐述推导和算法，仅使用概率论和多元高斯理论的基本定义，然后解释反映有限计算资源限制的实现选择。\n\n数据生成过程及其协方差。序列模型是一个一阶平稳线性高斯自回归过程，定义为 $x_1 \\sim \\mathcal{N}(0,\\sigma_x^2)$，其中 $\\sigma_x^2 = \\sigma_{\\varepsilon}^2/(1-a^2)$，且对于 $t \\in \\{2,\\dots,T\\}$，$x_t = a\\,x_{t-1} + \\varepsilon_t$，其中 $\\varepsilon_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma_{\\varepsilon}^2)$。这是一个高斯马尔可夫链。根据平稳自回归过程的性质，在 $(x_1,\\dots,x_T)$ 上诱导的联合分布是零均值高斯分布，其托普利茨（Toeplitz）协方差 $\\Sigma_{\\text{true}}$ 由 $\\Sigma_{\\text{true}}[i,j] = \\sigma_x^2\\,a^{|i-j|}$ 给出。这是因为在平稳性下，对于所有 $k$，$\\mathrm{Cov}(x_t,x_{t+k}) = \\sigma_x^2\\,a^{|k|}$。\n\n自回归模型：估计与评估。自回归范式使用概率链式法则将联合密度分解为 $p_{\\theta}(x_1,\\dots,x_T) = p_{\\theta}(x_1)\\prod_{t=2}^T p_{\\theta}(x_t \\mid x_{t-1})$，其中，在线性高斯假设类别下，$p_{\\theta}(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; a\\,x_{t-1},\\sigma_{\\varepsilon}^2)$ 且 $p_{\\theta}(x_1) = \\mathcal{N}(x_1;0,\\sigma_{\\varepsilon}^2/(1-a^2))$。给定跨所有序列和 $t \\in \\{2,\\dots,T\\}$ 汇集的训练对 $(x_{t-1},x_t)$，我们通过岭回归正则化的最小二乘法估计 $a$，最小化经验风险加惩罚项：\n$$\n\\min_{a \\in \\mathbb{R}} \\sum_{n,t} (x_t^{(n)} - a\\,x_{t-1}^{(n)})^2 + \\lambda a^2,\n$$\n其一阶最优性条件得出闭合形式的估计量：\n$$\n\\hat{a} = \\frac{\\sum_{n,t} x_{t-1}^{(n)} x_t^{(n)}}{\\sum_{n,t} (x_{t-1}^{(n)})^2 + \\lambda}.\n$$\n条件噪声方差通过均方残差估计：\n$$\n\\hat{\\sigma}_{\\varepsilon}^2 = \\frac{1}{N_{\\text{pairs}}}\\sum_{n,t} \\left(x_t^{(n)} - \\hat{a}\\,x_{t-1}^{(n)}\\right)^2,\n$$\n其中 $N_{\\text{pairs}}$ 是汇集的 $(x_{t-1},x_t)$ 对的数量。由拟合模型隐含的 $x_1$ 的边际方差是 $\\hat{\\sigma}_x^2 = \\hat{\\sigma}_{\\varepsilon}^2/(1-\\hat{a}^2)$，前提是 $|\\hat{a}| < 1$；为确保在有限数据下有有效的平稳先验，我们在需要时将 $|\\hat{a}|$ 裁剪到略小于 $1$（例如 $0.999$）。\n\n评估集上每维度的平均负对数似然遵循高斯对数密度公式。对于单个序列，在拟合的自回归模型下的对数似然是：\n$$\n\\log p_{\\hat{\\theta}}(x_1,\\dots,x_T) = \\log \\mathcal{N}(x_1;0,\\hat{\\sigma}_x^2) + \\sum_{t=2}^T \\log \\mathcal{N}(x_t;\\hat{a}\\,x_{t-1},\\hat{\\sigma}_{\\varepsilon}^2).\n$$\n在评估集上取平均并除以 $T$ 得到每维度的平均负对数似然：\n$$\n\\text{AR\\_NLL\\_per\\_dim} = -\\frac{1}{T}\\,\\frac{1}{N_{\\text{eval}}} \\sum_{n=1}^{N_{\\text{eval}}} \\log p_{\\hat{\\theta}}(x^{(n)}).\n$$\n\n对于样本质量，我们将拟合的参数转换为模型隐含的平稳协方差，这是一个托普利茨矩阵，其元素为 $\\Sigma_{\\text{AR}}[i,j] = \\hat{\\sigma}_x^2 \\hat{a}^{|i-j|}$。对于高斯分布，一个基于几何的有原则的样本质量概念是真实高斯分布与模型高斯分布之间的 2-瓦瑟斯坦距离，其具有闭合形式：\n$$\nW_2^2(\\Sigma_{\\text{true}},\\Sigma_{\\text{AR}}) = \\mathrm{tr}\\!\\left(\\Sigma_{\\text{true}} + \\Sigma_{\\text{AR}} - 2\\left(\\Sigma_{\\text{true}}^{1/2}\\,\\Sigma_{\\text{AR}}\\,\\Sigma_{\\text{true}}^{1/2}\\right)^{1/2}\\right).\n$$\n我们报告按维度归一化的距离 $W_{\\text{AR}} = \\sqrt{W_2^2(\\Sigma_{\\text{true}},\\Sigma_{\\text{AR}})/T}$。\n\n受去噪扩散启发的生成器：线性去噪器及其隐含模型。去噪扩散范式通过一个学习到的去噪器对数据的加噪版本进行建模。我们考虑一个单步前向加噪过程 $y = \\sqrt{1-\\beta}\\,x + \\sqrt{\\beta}\\,\\eta$，其中 $\\eta \\sim \\mathcal{N}(0,I)$，以及一个对角线性去噪器 $\\hat{x} = D y$，其中 $D=\\mathrm{diag}(\\alpha_1,\\dots,\\alpha_T)$，在对角协方差近似下，仅使用经验性的每维度训练方差 $v_i$ 进行估计。从 $y$ 到 $x$ 的线性最小均方误差估计量是 $D^* = \\mathrm{Cov}(x,y)\\,\\mathrm{Cov}(y)^{-1}$。由于 $y$ 和 $x$ 是联合高斯分布，其中 $\\mathrm{Cov}(x,y) = \\sqrt{1-\\beta}\\,\\mathrm{Cov}(x)$ 和 $\\mathrm{Cov}(y) = (1-\\beta)\\,\\mathrm{Cov}(x) + \\beta I$，对角近似将 $\\mathrm{Cov}(x)$ 替换为 $\\mathrm{diag}(v_1,\\dots,v_T)$，从而得到：\n$$\n\\alpha_i = \\frac{\\sqrt{1-\\beta}\\,v_i}{(1-\\beta)\\,v_i + \\beta}.\n$$\n该去噪器定义了一个迭代生成器 $x_{k+1} = D\\left(\\sqrt{1-\\beta}\\,x_k + \\sqrt{\\beta}\\,\\eta_k\\right)$，其中 $\\eta_k \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,I)$。这是一个线性高斯递归，其转移矩阵为 $C = \\sqrt{1-\\beta}\\,D$，且加性噪声协方差为 $Q = \\beta D^2$。唯一平稳协方差 $\\Sigma_{\\text{diff}}$ 求解离散时间李雅普诺夫方程 $\\Sigma_{\\text{diff}} = C\\,\\Sigma_{\\text{diff}}\\,C^\\top + Q$。因为 $C$ 和 $Q$ 是对角的，所以解也是对角的，其元素为：\n$$\ns_i = \\frac{\\alpha_i^2\\,\\beta}{1 - \\left(\\sqrt{1-\\beta}\\,\\alpha_i\\right)^2}.\n$$\n这构成了隐式似然模型 $\\mathcal{N}(0,\\Sigma_{\\text{diff}})$。评估集上每维度的平均负对数似然则为：\n$$\n\\text{DIFF\\_NLL\\_per\\_dim} = -\\frac{1}{T}\\,\\frac{1}{N_{\\text{eval}}}\\sum_{n=1}^{N_{\\text{eval}}} \\log \\mathcal{N}(x^{(n)};\\,0,\\Sigma_{\\text{diff}}),\n$$\n由于 $\\Sigma_{\\text{diff}}$ 是对角的，这是可以处理的。样本质量通过相同的按维度归一化的 2-瓦瑟斯坦距离进行评估：\n$$\nW_{\\text{diff}} = \\sqrt{\\frac{1}{T}\\,\\mathrm{tr}\\!\\left(\\Sigma_{\\text{true}} + \\Sigma_{\\text{diff}} - 2\\left(\\Sigma_{\\text{true}}^{1/2}\\,\\Sigma_{\\text{diff}}\\,\\Sigma_{\\text{true}}^{1/2}\\right)^{1/2}\\right)}.\n$$\n\n算法设计与有限计算。有限的计算资源体现在两个方面。对于自回归模型，当 $N_{\\text{train}}$ 很小时，使用超参数 $\\lambda$ 的岭回归正则化会使参数估计产生偏差，并且通过单次闭合形式的计算拟合单个全局标量 $\\hat{a}$，而没有进行迭代优化。对于去噪模型，仅使用每维度的方差 $v_i$ 来构建对角去噪器，从而忽略了时间相关性；所得到的平稳协方差被约束为对角的，并且当 $a \\ne 0$ 时无法匹配真实的托普利茨协方差。该生成器使用单一的噪声水平 $\\beta$ 和隐式平稳定律进行评估，而不是采用多步非时齐扩散，这使得计算量保持在最低水平。\n\n指标计算。维度为 $T$、均值为零、协方差为 $\\Sigma$ 的高斯对数密度为：\n$$\n\\log \\mathcal{N}(x;0,\\Sigma) = -\\frac{1}{2}\\left(x^\\top \\Sigma^{-1} x + \\log \\det (2\\pi \\Sigma)\\right).\n$$\n对于自回归模型，链式法则分解的高斯条件概率产生了一系列单变量高斯对数密度的和。对于瓦瑟斯坦距离，我们使用零均值高斯分布的闭合形式，其中涉及矩阵平方根；数值平方根通过标准的主平方根算法计算，我们取实部以减轻微小的数值虚部分量。\n\n测试套件与输出。对于三个指定用例中的每一个，我们使用给定的 $(a,\\sigma_{\\varepsilon},T)$ 和种子从真实过程中生成 $N_{\\text{train}}$ 个训练序列和 $N_{\\text{eval}}$ 个评估序列。我们计算：\n- $\\text{AR\\_NLL\\_per\\_dim}$，\n- $\\text{DIFF\\_NLL\\_per\\_dim}$，\n- $W_{\\text{AR}}$，\n- $W_{\\text{diff}}$，\n并按用例 1、用例 2、用例 3 的顺序将它们打印出来，作为一个用方括号括起来的单一展平列表。\n\n范式权衡，如指标所示。自回归模型直接对条件似然进行建模，因此当其假设类别与数据良好对齐时（此处两者均为一阶线性高斯马尔可夫过程），它往往能获得更优的测试对数似然。其隐含的样本质量（作为与真实协方差的分布匹配度）取决于参数估计的准确性。受去噪扩散启发的模型，被限制为仅从每维度方差学习的对角去噪器，通常会因为忽略相关性而产生较差的似然，但它可以产生具有合理边际方差的平稳分布；当时间相关性强时，其样本质量（以 2-瓦瑟斯坦距离衡量）会下降。在数据非常有限（$N_{\\text{train}}$ 小）且正则化较强的情况下，自回归估计可能会有偏差，导致 $W_{\\text{AR}}$ 和负对数似然增加，而去噪器的对角性质可能会产生更稳定但性能根本受限的表现，因为它无法捕捉非对角结构。这些权衡由所要求的输出进行量化。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import sqrtm\n\ndef ar1_generate_sequences(a, sigma_eps, T, N, rng):\n    var_x = sigma_eps**2 / (1.0 - a**2)\n    x = np.zeros((N, T))\n    x[:, 0] = rng.normal(loc=0.0, scale=np.sqrt(var_x), size=N)\n    for t in range(1, T):\n        eps = rng.normal(loc=0.0, scale=sigma_eps, size=N)\n        x[:, t] = a * x[:, t - 1] + eps\n    return x\n\ndef estimate_ar_ridge(seqs, lam):\n    # Pool all pairs (x_{t-1}, x_t) across sequences\n    x_prev = seqs[:, :-1].ravel()\n    x_curr = seqs[:, 1:].ravel()\n    denom = (x_prev @ x_prev) + lam\n    num = x_prev @ x_curr\n    a_hat = num / denom if denom != 0.0 else 0.0\n    resid = x_curr - a_hat * x_prev\n    sigma_eps_hat2 = float(np.mean(resid ** 2))\n    return a_hat, sigma_eps_hat2\n\ndef stationary_cov_ar(a, sigma_eps, T):\n    # Ensure stability\n    a_eff = np.clip(a, -0.999, 0.999)\n    var_x = sigma_eps**2 / (1.0 - a_eff**2)\n    # Toeplitz covariance with entries var_x * a^{|i-j|}\n    idx = np.arange(T)\n    power = np.abs(idx[:, None] - idx[None, :])\n    Sigma = var_x * (a_eff ** power)\n    return Sigma\n\ndef wasserstein_gaussian_per_dim(Sigma1, Sigma2):\n    # Compute W2 distance between zero-mean Gaussians with covariances Sigma1 and Sigma2\n    # W2^2 = tr(S1 + S2 - 2 * sqrt( sqrt(S1) * S2 * sqrt(S1) ))\n    S1 = Sigma1\n    S2 = Sigma2\n    # Numerical square roots\n    S1_sqrt = sqrtm(S1)\n    middle = S1_sqrt @ S2 @ S1_sqrt\n    middle_sqrt = sqrtm(middle)\n    W2_sq = np.trace(S1 + S2 - 2.0 * middle_sqrt)\n    W2_sq = float(np.real_if_close(W2_sq))\n    # Normalize per dimension\n    T = S1.shape[0]\n    W2_per_dim = np.sqrt(max(W2_sq, 0.0) / T)\n    return W2_per_dim\n\ndef ar_average_nll_per_dim(eval_seqs, a_hat, sigma_eps_hat2):\n    N, T = eval_seqs.shape\n    # Ensure stationarity for prior\n    a_eff = float(np.clip(a_hat, -0.999, 0.999))\n    if sigma_eps_hat2 <= 0 or (1.0 - a_eff**2) <= 0: # Avoid division by zero or log of non-positive\n        return np.inf\n    var_x_hat = sigma_eps_hat2 / (1.0 - a_eff**2)\n    # Log-likelihood components\n    x = eval_seqs\n    # Prior term for x1\n    ll = -0.5 * (np.log(2.0 * np.pi * var_x_hat) + (x[:, 0] ** 2) / var_x_hat)\n    # Conditional terms\n    for t in range(1, T):\n        mu = a_hat * x[:, t - 1]\n        ll += -0.5 * (np.log(2.0 * np.pi * sigma_eps_hat2) + ((x[:, t] - mu) ** 2) / sigma_eps_hat2)\n    avg_nll_per_dim = float(-np.mean(ll) / T)\n    return avg_nll_per_dim\n\ndef diag_denoiser_alpha_from_training(train_seqs, beta):\n    # Empirical per-dimension variances\n    v = np.var(train_seqs, axis=0, ddof=0)\n    # Optimal linear coefficient under diagonal approximation: alpha_i = sqrt(1-beta) * v_i / ((1-beta) v_i + beta)\n    alpha = np.sqrt(1.0 - beta) * v / ((1.0 - beta) * v + beta)\n    return alpha, v\n\ndef diffusion_stationary_cov_diag(alpha, beta):\n    # s_i = (alpha_i^2 * beta) / (1 - (sqrt(1-beta) * alpha_i)^2)\n    c = np.sqrt(1.0 - beta) * alpha\n    denom = 1.0 - c**2\n    # numerical safety\n    denom = np.maximum(denom, 1e-12)\n    s = (alpha**2) * beta / denom\n    return s\n\ndef diag_gaussian_average_nll_per_dim(eval_seqs, s_diag):\n    N, T = eval_seqs.shape\n    # Ensure positivity\n    s = np.maximum(s_diag, 1e-12)\n    # Log-density of diagonal Gaussian\n    sign, logdet = np.linalg.slogdet(np.diag(s))\n    if sign <= 0: return np.inf\n    \n    const = T * np.log(2.0 * np.pi) + np.sum(np.log(s))\n    quad = np.sum((eval_seqs ** 2) / s, axis=1)\n    ll = -0.5 * (const + quad)\n    avg_nll_per_dim = float(-np.mean(ll) / T)\n    return avg_nll_per_dim\n\ndef true_covariance(a, sigma_eps, T):\n    return stationary_cov_ar(a, sigma_eps, T)\n\ndef run_case(a, sigma_eps, T, N_train, N_eval, lam, beta, seed):\n    rng = np.random.default_rng(seed)\n    # Generate data\n    train = ar1_generate_sequences(a, sigma_eps, T, N_train, rng)\n    eval_data = ar1_generate_sequences(a, sigma_eps, T, N_eval, rng)\n    # Fit AR model\n    a_hat, sigma_eps_hat2 = estimate_ar_ridge(train, lam)\n    # AR metrics\n    ar_nll = ar_average_nll_per_dim(eval_data, a_hat, sigma_eps_hat2)\n    Sigma_true = true_covariance(a, sigma_eps, T)\n    Sigma_ar = stationary_cov_ar(a_hat, np.sqrt(sigma_eps_hat2), T)\n    w2_ar = wasserstein_gaussian_per_dim(Sigma_true, Sigma_ar)\n    # Diffusion denoiser and metrics\n    alpha, _ = diag_denoiser_alpha_from_training(train, beta)\n    s_diag = diffusion_stationary_cov_diag(alpha, beta)\n    diff_nll = diag_gaussian_average_nll_per_dim(eval_data, s_diag)\n    Sigma_diff = np.diag(s_diag)\n    w2_diff = wasserstein_gaussian_per_dim(Sigma_true, Sigma_diff)\n    return ar_nll, diff_nll, w2_ar, w2_diff\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (a, sigma_eps, T, N_train, N_eval, lambda, beta, seed)\n    test_cases = [\n        (0.7, 0.6, 6, 256, 8192, 0.0, 0.2, 0),    # Case 1\n        (0.9, 0.4, 6, 16,  8192, 5.0, 0.2, 1),    # Case 2\n        (-0.8, 0.5, 6, 128, 8192, 0.5, 0.2, 2),   # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        a, sigma_eps, T, N_train, N_eval, lam, beta, seed = case\n        ar_nll, diff_nll, w2_ar, w2_diff = run_case(a, sigma_eps, T, N_train, N_eval, lam, beta, seed)\n        # Format to 6 decimal places for stability\n        results.extend([f\"{ar_nll:.6f}\", f\"{diff_nll:.6f}\", f\"{w2_ar:.6f}\", f\"{w2_diff:.6f}\"])\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3160960"}]}