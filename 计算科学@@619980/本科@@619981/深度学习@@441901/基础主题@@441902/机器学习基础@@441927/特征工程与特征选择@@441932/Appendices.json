{"hands_on_practices": [{"introduction": "这项实践深入探讨了特定领域特征工程，这是将机器学习应用于科学问题的关键技能。您将学习如何将原始生物数据（DNA序列）转化为一个有意义的数值特征，用以表示转录因子的结合亲和力。这个练习 [@problem_id:2389767] 展示了如何利用概率模型，如位置权重矩阵 (Position Weight Matrices, PWM)，来捕捉复杂模式并创造强大的预测性特征。", "problem": "给定一个特定转录因子的位置权重矩阵 (PWM) 和一个背景核苷酸分布。对于任意脱氧核糖核酸 (DNA) 序列，定义一个单一数值特征，该特征等于在所有窗口和两条链上计算得到的最大对数几率 PWM 分数，作为预测的结合亲和力。请使用以下精确的数学定义。\n\n设字母表为核苷酸集合 $\\{A,C,G,T\\}$。设 $L$ 表示基序长度，设 $P \\in \\mathbb{R}^{4 \\times L}$ 是一个列概率矩阵，其行按 $(A,C,G,T)$ 的顺序排列，使得对于每个列索引 $i \\in \\{0,1,\\dots,L-1\\}$，都有 $\\sum_{b \\in \\{A,C,G,T\\}} P[b,i] = 1$。设 $Q \\in \\mathbb{R}^{4}$ 是核苷酸的背景分布，满足 $\\sum_{b \\in \\{A,C,G,T\\}} Q[b] = 1$ 且对于所有 $b$ 都有 $Q[b] > 0$。\n\n通过以下公式定义对数几率权重矩阵 $W \\in \\mathbb{R}^{4 \\times L}$：\n$$\nW[b,i] = \\ln\\left(\\frac{P[b,i]}{Q[b]}\\right),\n$$\n其中 $\\ln$ 表示自然对数。\n\n对于一个长度为 $n$、符号在 $\\{A,C,G,T\\}$ 中的 DNA 序列 $S$，以及任意窗口起始位置 $t \\in \\{0,1,\\dots,n-L\\}$，定义正向窗口得分为\n$$\n\\mathrm{score}_{\\mathrm{fwd}}(S,t) = \\sum_{j=0}^{L-1} W[S_{t+j}, j],\n$$\n以及反向窗口得分（扫描反向互补链）\n$$\n\\mathrm{score}_{\\mathrm{rev}}(S,t) = \\sum_{j=0}^{L-1} W[\\mathrm{comp}(S_{t+L-1-j}), j],\n$$\n其中 $\\mathrm{comp}(\\cdot)$ 函数将 $A \\leftrightarrow T$ 和 $C \\leftrightarrow G$ 进行映射。\n\n一个窗口 $S[t..t+L-1]$ 是有效的，当且仅当其所有符号都在 $\\{A,C,G,T\\}$ 集合中。对于任意序列 $S$，定义预测的结合亲和力特征为\n$$\n\\phi(S) = \\max\\left\\{ \\mathrm{score}_{\\mathrm{fwd}}(S,t), \\mathrm{score}_{\\mathrm{rev}}(S,t) \\,:\\, t \\in \\{0,\\dots,n-L\\}, \\, S[t..t+L-1]\\ \\text{valid} \\right\\}。\n$$\n如果没有有效的窗口（包括 $n  L$ 的情况），则设 $\\phi(S) = -\\infty$。\n\n使用以下固定参数：\n- 基序长度 $L = 4$。\n- 背景分布 $Q$ 按 $(A,C,G,T)$ 顺序排列：\n$$\nQ = [\\,0.3,\\,0.2,\\,0.2,\\,0.3\\,].\n$$\n- 位置权重矩阵 $P$，行按 $(A,C,G,T)$ 顺序排列，列索引为 $i=0,1,2,3$：\n$$\nP = \\begin{bmatrix}\n0.7  0.1  0.1  0.1 \\\\\n0.1  0.1  0.1  0.1 \\\\\n0.1  0.7  0.7  0.1 \\\\\n0.1  0.1  0.1  0.7 \\\\\n\\end{bmatrix}.\n$$\n\n测试套件。按给定顺序为以下每个序列计算 $\\phi(S)$：\n1. $S_1 =$ TTTAGGTAA\n2. $S_2 =$ AG\n3. $S_3 =$ GGGACCTCCC\n4. $S_4 =$ NAGGTN\n5. $S_5 =$ CCCCCCCC\n\n所有扫描都必须遵守上述定义。包含 $\\{A,C,G,T\\}$ 之外任何字符的窗口都是无效的，必须被忽略。如果一个序列不存在有效窗口，则该序列的输出为 $-\\infty$。\n\n最终输出格式。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，并与测试用例的顺序相同，例如 $[\\phi(S_1),\\phi(S_2),\\phi(S_3),\\phi(S_4),\\phi(S_5)]$。这些值必须是以自然对数为单位的实数，在适用的情况下允许使用 $-\\infty$。不应打印任何额外文本。", "solution": "问题陈述已经过严格验证，并被确定为是合理的。它在科学上基于生物信息学的原理，特别是在转录因子结合位点的建模方面。所有的参数、定义和数学公式都是适定的、完整的且无歧义的。与已建立的科学理论没有矛盾、事实错误或时代错误。因此，可以推导出一个确定的解决方案。\n\n任务是为几个给定的脱氧核糖核酸 (DNA) 序列计算一个数值特征，记为 $\\phi(S)$。该特征表示转录因子与序列的预测结合亲和力，定义为使用位置权重矩阵 (PWM) 在正向和反向互补链上扫描序列所获得的最大对数几率分数。\n\n这个问题的基本组成部分是核苷酸字母表 $\\mathcal{A} = \\{A, C, G, T\\}$、长度为 $L=4$ 的基序、一个位置特异性概率矩阵 $P$ 和一个背景核苷酸分布 $Q$。\n\n给定的参数是：\n- 基序长度 $L=4$。\n- 碱基 $(A,C,G,T)$ 的背景分布 $Q$：\n$$\nQ = [\\,0.3,\\,0.2,\\,0.2,\\,0.3\\,].\n$$\n- 位置权重矩阵 $P$，行为 $(A,C,G,T)$，列为位置 $i \\in \\{0, 1, 2, 3\\}$：\n$$\nP = \\begin{bmatrix}\n0.7  0.1  0.1  0.1 \\\\\n0.1  0.1  0.1  0.1 \\\\\n0.1  0.7  0.7  0.1 \\\\\n0.1  0.1  0.1  0.7 \\\\\n\\end{bmatrix}.\n$$\n\n第一步是从 $P$ 和 $Q$ 构建对数几率权重矩阵 $W$。每个元素 $W[b,i]$ 的计算方法是：基序中位置 $i$ 处碱基 $b$ 的概率与其背景概率之比的自然对数。公式为：\n$$\nW[b,i] = \\ln\\left(\\frac{P[b,i]}{Q[b]}\\right).\n$$\n我们建立从核苷酸到行索引的映射：$A \\to 0$, $C \\to 1$, $G \\to 2$, $T \\to 3$。得到的矩阵 $W$ 是：\n$$\nW = \\begin{bmatrix}\n\\ln(0.7/0.3)  \\ln(0.1/0.3)  \\ln(0.1/0.3)  \\ln(0.1/0.3) \\\\\n\\ln(0.1/0.2)  \\ln(0.1/0.2)  \\ln(0.1/0.2)  \\ln(0.1/0.2) \\\\\n\\ln(0.1/0.2)  \\ln(0.7/0.2)  \\ln(0.7/0.2)  \\ln(0.1/0.2) \\\\\n\\ln(0.1/0.3)  \\ln(0.1/0.3)  \\ln(0.1/0.3)  \\ln(0.7/0.3) \\\\\n\\end{bmatrix}\n\\approx\n\\begin{bmatrix}\n 0.8473  -1.0986  -1.0986  -1.0986 \\\\\n-0.6931  -0.6931  -0.6931  -0.6931 \\\\\n-0.6931   1.2528   1.2528  -0.6931 \\\\\n-1.0986  -1.0986  -1.0986   0.8473\n\\end{bmatrix}.\n$$\n\n第二步是处理每个长度为 $n$ 的序列 $S$。我们在序列上滑动一个长度为 $L=4$ 的窗口。对于每个起始位置 $t \\in \\{0, 1, \\dots, n-L\\}$，我们考虑窗口 $S[t..t+L-1]$。一个窗口只有在它完全由集合 $\\{A,C,G,T\\}$ 中的字符组成时才有效。对于每个有效窗口，我们计算两个分数：\n1.  正向得分，$\\mathrm{score}_{\\mathrm{fwd}}(S,t) = \\sum_{j=0}^{L-1} W[S_{t+j}, j]$。\n2.  反向得分，$\\mathrm{score}_{\\mathrm{rev}}(S,t) = \\sum_{j=0}^{L-1} W[\\mathrm{comp}(S_{t+L-1-j}), j]$，其中 $\\mathrm{comp}(\\cdot)$ 是互补映射（$A \\leftrightarrow T$, $C \\leftrightarrow G$）。这等同于用 $W$ 对窗口序列的反向互补序列进行评分。\n\n特征 $\\phi(S)$ 是为所有有效窗口计算出的所有分数中的最大值。如果没有有效窗口存在（例如，如果 $n  L$），则分数为 $-\\infty$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predicted binding affinity for a set of DNA sequences\n    based on a log-odds Position Weight Matrix (PWM) score.\n    \"\"\"\n    # Define fixed parameters as specified in the problem statement.\n    L = 4\n    P = np.array([\n        [0.7, 0.1, 0.1, 0.1],  # Row for 'A'\n        [0.1, 0.1, 0.1, 0.1],  # Row for 'C'\n        [0.1, 0.7, 0.7, 0.1],  # Row for 'G'\n        [0.1, 0.1, 0.1, 0.7]   # Row for 'T'\n    ])\n    Q = np.array([0.3, 0.2, 0.2, 0.3]) # Background for A, C, G, T\n\n    # Calculate the log-odds weight matrix W.\n    # The division is broadcasted: each column of P is divided by the vector Q.\n    # Q[:, np.newaxis] reshapes Q to a column vector for division.\n    W = np.log(P / Q[:, np.newaxis])\n\n    # Helper dictionaries for mapping and sequence manipulation.\n    nuc_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    comp_map = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    valid_nucs = set(nuc_to_idx.keys())\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        \"TTTAGGTAA\",\n        \"AG\",\n        \"GGGACCTCCC\",\n        \"NAGGTN\",\n        \"CCCCCCCC\"\n    ]\n\n    results = []\n    for S in test_cases:\n        n = len(S)\n        max_score = -np.inf\n\n        # Iterate through all possible window starting positions.\n        # The loop range will be empty if n  L, correctly handling such cases.\n        for t in range(n - L + 1):\n            window = S[t:t+L]\n            \n            # A window is valid only if all its characters are in {A, C, G, T}.\n            if not all(c in valid_nucs for c in window):\n                continue\n\n            # Calculate the forward score for the window.\n            fwd_score = sum(W[nuc_to_idx[window[j]], j] for j in range(L))\n            \n            # Calculate the reverse-complement score. This is done by scoring\n            # the reverse complement of the window against the PWM.\n            rev_comp_window = \"\".join(comp_map[c] for c in reversed(window))\n            rev_score = sum(W[nuc_to_idx[rev_comp_window[j]], j] for j in range(L))\n\n            # Update the maximum score found so far for the sequence.\n            current_max = max(fwd_score, rev_score)\n            if current_max > max_score:\n                max_score = current_max\n        \n        results.append(max_score)\n\n    # Final print statement in the exact required format.\n    # The str() function correctly converts float('-inf') to '-inf'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2389767"}, {"introduction": "真实世界的数据很少是干净的，异常值会严重扭曲特征选择和模型训练的结果。这项实践聚焦于特征变换，将其作为增强鲁棒性的工具。通过一个受控的模拟实验 [@problem_id:3124173]，您将研究分位数归一化（一种基于排序的方法）如何减轻重尾分布和极端值的影响，从而实现更稳定、更可靠的特征选择。", "problem": "您需要设计并实现一个完整的程序，用于研究在存在重尾分布和注入离群值的情况下，特征的分位数归一化如何影响特征选择。该程序必须能生成合成数据，使用经验皮尔逊相关性进行特征选择，应用分位数归一化作为到标准正态分布的基于秩的变换，并为每个测试用例测量两个量：在离群值存在下由分位数归一化引起的选择特征集的变化，以及分位数归一化带来的对离群值的鲁棒性增益，该增益通过与干净基线相比的集合相似度变化来量化。\n\n此任务的基础包括以下经过充分检验的定义和事实：\n\n- 两个实值变量之间的经验皮尔逊相关性是根据观测值定义的线性关联度量，并且对极值敏感。\n- 基于秩的分位数归一化使用经验秩，通过逆累积分布函数将每个特征变换到目标分布，从而在保留顺序关系的同时，减少极值的影响力。\n- 重尾分布比轻尾分布更频繁地产生极端观测值，从而改变有限样本中的经验矩。低自由度的学生 t 分布是一个典型的重尾模型。\n- 集合相似度可以使用 Jaccard 指数来量化，该指数比较交集和并集的基数，并产生一个在区间 $[0,1]$ 内的实数。\n\n您必须遵守以下数学上精确的规范：\n\n1. 数据生成：\n   - 令 $n$ 表示样本数，令 $d$ 表示特征数。从学生 t 分布（自由度为 $\\nu$）中独立抽样，生成特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$。当 $\\nu$ 较小时，此分布为重尾分布。\n   - 选择一个大小为 $s$ 的索引子集 $S_{\\text{true}} \\subset \\{0,1,\\dots,d-1\\}$ 作为信号特征。\n   - 令 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{s}$ 表示为每个测试用例抽样一次的固定系数。\n   - 定义目标为 $y = X_{:,S_{\\text{true}}}\\boldsymbol{\\alpha} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 且独立于 $X$。\n   - 生成两个版本的特征矩阵：一个干净版本 $X_{\\text{clean}}$ 和一个被离群值污染的版本 $X_{\\text{out}}$。对于离群值污染，每个条目独立地以概率 $p$ 加上一个形式为 $m \\cdot u$ 的扰动，其中 $m  0$ 是离群值幅度，$u \\in \\{-1,+1\\}$ 是一个随机符号。\n\n2. 分位数归一化：\n   - 对每个特征（$X$ 的每一列），计算其值的经验秩 $r_i \\in \\{1,2,\\dots,n\\}$。将每个秩映射到一个概率 $q_i = (r_i - 0.5)/n \\in (0,1)$。\n   - 通过 $z_i = \\Phi^{-1}(q_i)$ 变换到标准正态分布，其中 $\\Phi^{-1}$ 是标准正态分布的逆累积分布函数。用 $z_i$ 替换该特征的原始值。对每个特征独立应用此变换。\n\n3. 特征选择：\n   - 对于指定的整数 $k$（$1 \\le k \\le d$），计算每个特征索引 $j \\in \\{0,1,\\dots,d-1\\}$ 的绝对经验皮尔逊相关性 $\\rho_j = \\left|\\text{corr}(X_{:,j}, y)\\right|$。\n   - 选择与最大的 $k$ 个 $\\rho_j$ 值相对应的 $k$ 个特征索引的集合 $S_k$，数值相同时按索引从小到大排序。\n\n4. 评估指标：\n   - 定义在离群值存在下，由分位数归一化引起的偏移为从未进行和进行分位数归一化的离群值污染数据中选择的集合之间的对称差的大小，即\n     $$\\Delta = \\left|S_k(X_{\\text{out}}) \\triangle S_k(\\text{QN}(X_{\\text{out}}))\\right|,$$\n     其中 $\\text{QN}(\\cdot)$ 表示逐特征的分位数归一化，$\\triangle$ 表示对称差。\n   - 通过干净数据和离群值数据所选集合之间的 Jaccard 指数来定义对离群值的鲁棒性。计算\n     $$J_{\\text{raw}} = \\frac{\\left|S_k(X_{\\text{clean}}) \\cap S_k(X_{\\text{out}})\\right|}{\\left|S_k(X_{\\text{clean}}) \\cup S_k(X_{\\text{out}})\\right|}, \\quad J_{\\text{qn}} = \\frac{\\left|S_k(\\text{QN}(X_{\\text{clean}})) \\cap S_k(\\text{QN}(X_{\\text{out}}))\\right|}{\\left|S_k(\\text{QN}(X_{\\text{clean}})) \\cup S_k(\\text{QN}(X_{\\text{out}}))\\right|}.$$\n     报告鲁棒性增益 $G = J_{\\text{qn}} - J_{\\text{raw}}$。\n\n5. 程序输出：\n   - 对于每个测试用例，输出一个列表 $[\\Delta, G]$，其中包含对称差大小 $\\Delta$（整数）和鲁棒性增益 $G$（浮点数）。\n   - 您的程序应生成单行输出，其中包含所有测试用例的结果，形式为用方括号括起来的逗号分隔列表。此列表中的每个元素必须是针对一个测试用例的双元素列表 $[\\Delta, G]$，例如 $\\left[[\\Delta_1,G_1],[\\Delta_2,G_2],\\dots\\right]$。\n\n使用以下测试套件。对于所有测试用例，请确保使用提供的种子使随机数生成过程可复现。所有量均为无量纲，不涉及角度。\n\n- 测试用例 1（一般情况）：\n  - $n=200$, $d=20$, $k=5$, $\\nu=3$, $p=0.05$, $m=25.0$, $\\sigma=1.0$, $s=5$, seed $=42$。\n- 测试用例 2（更重的尾部和更多的离群值）：\n  - $n=200$, $d=20$, $k=5$, $\\nu=1$, $p=0.10$, $m=50.0$, $\\sigma=1.0$, $s=5$, seed $=123$。\n- 测试用例 3（无离群值用于基线比较）：\n  - $n=200$, $d=20$, $k=5$, $\\nu=3$, $p=0.0$, $m=0.0$, $\\sigma=1.0$, $s=5$, seed $=7$。\n- 测试用例 4（小样本量和显著的离群值）：\n  - $n=50$, $d=20$, $k=5$, $\\nu=2$, $p=0.10$, $m=40.0$, $\\sigma=1.0$, $s=5$, seed $=2024$。\n- 测试用例 5（更高维的特征空间）：\n  - $n=300$, $d=100$, $k=10$, $\\nu=5$, $p=0.05$, $m=30.0$, $\\sigma=1.0$, $s=8$, seed $=314159$。\n\n您的程序必须精确实现上述步骤，并输出形式为 $\\left[[\\Delta_1,G_1],[\\Delta_2,G_2],[\\Delta_3,G_3],[\\Delta_4,G_4],[\\Delta_5,G_5]\\right]$ 的单行结果，其中数值是通过您的实现计算得出的。", "solution": "该问题是有效的。它提出了一个在计算统计学领域定义明确且具有科学依据的任务，旨在评估分位数归一化对特征选择鲁棒性的影响。该问题是自包含的，所有参数和过程都已指定，从而可以得出一个唯一的、可复现的解决方案。关于信号系数采样和真实特征索引选择的微小模糊性，通过采用合成数据生成中的标准实践得以解决，这并未损害问题的完整性。这些假设是：信号系数 $\\boldsymbol{\\alpha}$ 从标准正态分布中抽取，真实信号特征集 $S_{\\text{true}}$ 是均匀随机选择的。\n\n解决方案是通过遵循问题陈述中指定的步骤顺序来实现的。每个测试用例的过程都涉及数据生成、特征变换、特征选择和评估指标的计算。\n\n**1. 数据生成**\n\n本分析的基础是一个合成数据集，旨在模拟真实世界数据所面临的挑战，即重尾分布和离群值的存在。对于每个测试用例，我们生成一个特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其中 $n$ 是样本数，$d$ 是特征数。\n\n-   **基础特征分布**：干净特征矩阵 $X_{\\text{clean}}$ 的条目是来自自由度为 $\\nu$ 的学生 t 分布的独立同分布样本。选择该分布是因为其重尾特性，对于较小的 $\\nu$，它比正态分布更频繁地产生极值，从而对那些对此类值敏感的统计方法构成挑战。\n-   **目标变量**：在特征和目标变量 $y$ 之间建立一个真实关系。随机选择 $s$ 个特征的子集（由集合 $S_{\\text{true}}$ 索引）作为“信号”特征。它们对应的系数 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{s}$ 从标准正态分布 $\\mathcal{N}(0, 1)$ 中采样。然后，将目标 $y$ 构建为这些信号特征的线性组合，并加上高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$：\n    $$y = X_{\\text{clean},:,S_{\\text{true}}}\\boldsymbol{\\alpha} + \\varepsilon$$\n-   **离群值污染**：为模拟数据污染，从 $X_{\\text{clean}}$ 创建一个注入了离群值的矩阵 $X_{\\text{out}}$。$X_{\\text{clean}}$ 的每个元素以概率 $p$ 被污染。被污染的元素会被加上一个离群项 $m \\cdot u$，其中 $m  0$ 是一个较大的幅值，$u$ 是一个随机符号（+1 或 -1）。此过程引入了稀疏的、高杠杆作用的离群值，这些离群值会严重扭曲像皮尔逊相关性这样的统计度量。\n\n**2. 分位数归一化**\n\n分位数归一化是一种非线性变换，旨在减轻特征原始分布（尤其是极值）的影响。该过程独立应用于每个特征列。\n\n1.  对于每个特征向量，计算其 $n$ 个数据点的秩 $r_i$，范围从 1 到 $n$。\n2.  使用公式 $q_i = \\frac{r_i - 0.5}{n}$ 将这些秩转换为经验分位数。这将秩映射到区间 $(0, 1)$。\n3.  然后，每个分位数 $q_i$ 被映射到来自目标分布（在此情况下为标准正态分布 $\\mathcal{N}(0, 1)$）的值 $z_i$。这是通过应用标准正态分布的逆累积分布函数（CDF）（也称为概率单位函数）实现的：$z_i = \\Phi^{-1}(q_i)$。\n\n生成的特征向量由值 $\\{z_1, z_2, \\dots, z_n\\}$ 组成，根据构造，这些值遵循标准正态分布。这种基于秩的方法保留了数据点的顺序，但替换了它们的大小，从而有效地消除了离群值的影响。\n\n**3. 特征选择**\n\n特征选择方法基于经验皮尔逊相关系数，该系数用于度量线性关联。\n\n-   对于给定的特征矩阵（例如 $X_{\\text{out}}$ 或 $\\text{QN}(X_{\\text{out}})$）和固定的目标 $y$，为每个特征 $j \\in \\{0, 1, \\dots, d-1\\}$ 计算绝对皮尔逊相关性 $\\rho_j = |\\text{corr}(X_{:,j}, y)|$。\n-   选择具有最高 $\\rho_j$ 值的 $k$ 个特征。为确保结果的确定性，相关性值的平局通过选择索引较小的特征来解决。这是通过对主键（绝对相关性降序）和次键（特征索引升序）进行字典序排序来实现的。\n\n**4. 评估指标**\n\n计算两个指标来量化分位数归一化的效果。\n\n-   **由归一化引起的偏移（$\\Delta$）**：此指标捕捉当分位数归一化应用于受离群值污染的数据时，所选特征集发生多大变化。它是从原始离群值数据中选择的特征集 $S_k(X_{\\text{out}})$ 与从归一化离群值数据中选择的特征集 $S_k(\\text{QN}(X_{\\text{out}}))$ 之间的对称差的大小：\n    $$\\Delta = \\left|S_k(X_{\\text{out}}) \\triangle S_k(\\text{QN}(X_{\\text{out}}))\\right|$$\n-   **鲁棒性增益（$G$）**：此指标评估分位数归一化是否提高了特征选择过程在存在离群值时的稳定性。它比较了在有和没有归一化的情况下，干净数据和受离群值污染数据之间特征选择的一致性。这种一致性通过 Jaccard 指数 $J(A, B) = |A \\cap B| / |A \\cup B|$ 来衡量。\n    -   原始数据的 Jaccard 指数为 $J_{\\text{raw}} = J(S_k(X_{\\text{clean}}), S_k(X_{\\text{out}}))$。\n    -   分位数归一化数据的 Jaccard 指数为 $J_{\\text{qn}} = J(S_k(\\text{QN}(X_{\\text{clean}})), S_k(\\text{QN}(X_{\\text{out}})))$。\n    -   鲁棒性增益是两者之差：$G = J_{\\text{qn}} - J_{\\text{raw}}$。$G$ 的正值表示分位数归一化使特征选择过程对离群值更具鲁棒性。\n\n整个程序会遍历每个测试用例，使用指定的随机种子执行这些步骤以保证可复现性，并收集最终输出的 $[\\Delta, G]$ 对。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t, norm, rankdata\n\ndef quantile_normalize(X):\n    \"\"\"\n    Applies quantile normalization to each column of X, mapping to a standard\n    normal distribution.\n    \"\"\"\n    n_samples = X.shape[0]\n    ranks = rankdata(X, axis=0)  # Ranks are from 1 to n\n    quantiles = (ranks - 0.5) / n_samples\n    X_norm = norm.ppf(quantiles)\n    return X_norm\n\ndef select_features(X, y, k):\n    \"\"\"\n    Selects top k features based on absolute Pearson correlation with y.\n    Ties are broken by selecting the smaller feature index.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Vectorized computation of Pearson correlation\n    X_c = X - np.mean(X, axis=0)\n    y_c = y - np.mean(y)\n\n    # (n-1) * cov = sum((X_i - mu_X) * (y - mu_y)) = X_c.T @ y_c\n    cov_xy = X_c.T @ y_c\n    \n    std_X = np.std(X, axis=0, ddof=1)\n    std_y = np.std(y, ddof=1)\n    \n    corr = np.zeros(n_features)\n    # Mask to avoid division by zero for constant features or if y is constant\n    valid_mask = (std_X > 0)  (std_y > 0)\n    if np.any(valid_mask):\n      denominator = std_X[valid_mask] * std_y\n      corr[valid_mask] = (cov_xy[valid_mask] / (n_samples - 1)) / denominator\n    \n    abs_corr = np.abs(corr)\n    \n    # Tie-breaking: sort by descending abs_corr, then ascending index.\n    # np.lexsort sorts by the last key first.\n    indices = np.arange(n_features)\n    sorted_indices = np.lexsort((indices, -abs_corr))\n    \n    top_k_indices = sorted_indices[:k]\n    \n    return set(top_k_indices)\n\ndef jaccard_index(set1, set2):\n    \"\"\"Computes the Jaccard index between two sets.\"\"\"\n    intersection_size = len(set1.intersection(set2))\n    union_size = len(set1.union(set2))\n    if union_size == 0:\n        return 1.0\n    return intersection_size / union_size\n\ndef run_simulation(n, d, k, nu, p, m, sigma, s, seed):\n    \"\"\"\n    Runs one full simulation for a given set of parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Data Generation\n    # Generate clean feature matrix X from Student's t-distribution\n    X_clean = t.rvs(df=nu, size=(n, d), random_state=rng)\n    \n    # Choose true signal features and coefficients\n    S_true_indices = rng.choice(d, size=s, replace=False)\n    alpha_coeffs = rng.standard_normal(size=s)\n    \n    # Generate target variable y\n    signal = X_clean[:, S_true_indices] @ alpha_coeffs\n    noise = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = signal + noise\n    \n    # Generate outlier-corrupted data\n    X_out = X_clean.copy()\n    outlier_mask = rng.random(size=(n, d))  p\n    signs = rng.choice([-1, 1], size=(n, d))\n    X_out[outlier_mask] += signs[outlier_mask] * m\n\n    # 2. Quantile Normalization\n    X_clean_qn = quantile_normalize(X_clean)\n    X_out_qn = quantile_normalize(X_out)\n\n    # 3. Feature Selection for all four data versions\n    S_clean = select_features(X_clean, y, k)\n    S_out = select_features(X_out, y, k)\n    S_clean_qn = select_features(X_clean_qn, y, k)\n    S_out_qn = select_features(X_out_qn, y, k)\n\n    # 4. Evaluation Metrics\n    # Shift Delta\n    delta = len(S_out.symmetric_difference(S_out_qn))\n    \n    # Robustness Gain G\n    J_raw = jaccard_index(S_clean, S_out)\n    J_qn = jaccard_index(S_clean_qn, S_out_qn)\n    G = J_qn - J_raw\n    \n    return [delta, G]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, d, k, nu, p, m, sigma, s, seed)\n        (200, 20, 5, 3, 0.05, 25.0, 1.0, 5, 42),\n        (200, 20, 5, 1, 0.10, 50.0, 1.0, 5, 123),\n        (200, 20, 5, 3, 0.0, 0.0, 1.0, 5, 7),\n        (50, 20, 5, 2, 0.10, 40.0, 1.0, 5, 2024),\n        (300, 100, 10, 5, 0.05, 30.0, 1.0, 8, 314159),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(*params)\n        results.append(result)\n\n    # Format the output string precisely as [[d1,g1],[d2,g2],...]\n    formatted_results = [f\"[{d},{g}]\" for d, g in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3124173"}, {"introduction": "在机器学习中，最微妙也最危险的陷阱之一是标签泄漏 (label leakage)，即特征无意中包含了在真实预测场景中无法获得的目标信息。这个练习 [@problem_id:3124151] 提供了一个动手实践指南，用于检测和防范此类泄漏。您将实现一个完整的诊断流程，使用排列重要性 (permutation importance) 和噪声哨兵特征 (noise sentinel features) 来识别并惩罚那些具有可疑预测能力的特征，这是构建可信赖模型的关键技能。", "problem": "您将实现一个完整、确定性的程序，通过有意添加泄露特征、执行特征选择、计算排列重要性，并应用一种惩罚可疑高重要性的保障措施，来检测用于二元分类器的特征工程中的标签泄露。该程序必须是自包含的，并为给定的测试套件产生结果。本问题中的所有数学符号均使用 LaTeX 指定，每个符号、变量、函数、运算符和数字都使用行内数学模式书写。\n\n本问题的基础是用于训练的经验风险最小化 (ERM)、用于衡量分类误差的二元交叉熵 (BCE) 以及用于评估特征相关性的基于排列的重要性，这些都是机器学习中经过充分检验的原则。\n\n按如下方式构建一个合成的二元分类数据集。对于每个测试用例，有 $n$ 个样本和三个特征块：\n- 对标签有信息价值的 $d_b$ 维基础特征。\n- 与标签独立的 $d_n$ 维噪声哨兵特征。\n- 被有意构建以携带关于标签的直接信息的 $d_\\ell$ 维泄露特征。\n\n数据生成：\n1. 对每个样本索引 $i \\in \\{1,\\dots,n\\}$，独立地抽取基础特征 $\\mathbf{x}_i^b \\in \\mathbb{R}^{d_b}$，其服从分布 $\\mathbf{x}_i^b \\sim \\mathcal{N}(\\mathbf{0}, I)$。\n2. 从一个伪随机种子确定性地抽取一个真实权重向量 $\\mathbf{w}^\\star \\in \\mathbb{R}^{d_b}$ 和偏置 $b^\\star \\in \\mathbb{R}$，并定义 logit $z_i = \\mathbf{w}^{\\star \\top}\\mathbf{x}_i^b + b^\\star$。定义标签概率 $p_i = \\sigma(z_i)$，其中 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$，并从参数为 $p_i$ 的伯努利分布中抽样二元标签 $y_i \\in \\{0,1\\}$。\n3. 独立地抽取噪声哨兵特征 $\\mathbf{x}_i^n \\in \\mathbb{R}^{d_n}$，其服从分布 $\\mathbf{x}_i^n \\sim \\mathcal{N}(\\mathbf{0}, I)$，且所有特征均与 $\\mathbf{x}_i^b$ 和 $y_i$ 独立。\n4. 对每个泄露特征索引 $j \\in \\{1,\\dots,d_\\ell\\}$，构建泄露特征 $\\mathbf{x}_i^\\ell \\in \\mathbb{R}^{d_\\ell}$ 为 $x_{i,j}^\\ell = y_i + \\eta_{i,j}$，其中 $\\eta_{i,j} \\sim \\mathcal{N}(0, s_\\ell^2)$，$s_\\ell$ 是一个给定的泄露噪声标准差。较小的 $s_\\ell$ 产生更强的泄露。\n5. 构成完整的特征向量 $\\mathbf{x}_i = [\\mathbf{x}_i^b, \\mathbf{x}_i^n, \\mathbf{x}_i^\\ell] \\in \\mathbb{R}^{d}$，其中 $d = d_b + d_n + d_\\ell$。\n\n将数据集划分为训练集和验证集，使用 $0.7$ 的训练比例，使得训练集有 $\\lfloor 0.7n \\rfloor$ 个样本，验证集有 $n - \\lfloor 0.7n \\rfloor$ 个样本。使用训练集的统计数据对每个特征维度进行标准化：对于每个特征索引 $j \\in \\{1,\\dots,d\\}$，计算训练均值 $m_j$ 和标准差 $s_j$，然后通过 $\\tilde{x}_{i,j} = \\frac{x_{i,j} - m_j}{s_j}$ 对训练和验证特征进行转换。\n\n训练一个具有一个隐藏层的前馈人工神经网络 (ANN) 分类器：\n- 隐藏层宽度为固定常数 $h$。\n- 隐藏层激活函数为修正线性单元 (ReLU)，$\\text{ReLU}(u) = \\max(0,u)$。\n- 输出是带有 logistic 激活函数 $\\sigma$ 的单个 logit，以产生 $\\hat{y}_i \\in (0,1)$。\n- 在训练集上最小化二元交叉熵 (BCE) 经验风险：\n$$\n\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}}\\left( y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right),\n$$\n使用全批量梯度下降法，训练固定的轮数 (epochs) 和固定的学习率，这两者都在程序中指定。\n\n在验证集上计算排列重要性，以量化对每个特征坐标 $j$ 的依赖程度：\n1. 使用训练好的 ANN 和未经排列的验证特征计算基线验证损失 $L_0$。\n2. 对于每个特征索引 $j \\in \\{1,\\dots,d\\}$，独立地在样本间排列验证特征的第 $j$ 列以产生一个扰动后的验证集，计算损失 $L_{(j)}$，并定义重要性：\n$$\nI_j = L_{(j)} - L_0.\n$$\n较大的 $I_j$ 表示对特征 $j$ 的依赖性更强。\n\n使用噪声哨兵特征作为参考零分布，定义一个防止泄露的保障措施：\n- 令 $J_n \\subset \\{1,\\dots,d\\}$ 表示对应于噪声哨兵特征（基础特征之后的块）的索引集合。\n- 计算重要性的噪声参考均值和标准差：\n$$\n\\mu_n = \\frac{1}{|J_n|}\\sum_{j \\in J_n} I_j,\n\\qquad\n\\sigma_n = \\sqrt{\\frac{1}{|J_n|}\\sum_{j \\in J_n}(I_j - \\mu_n)^2}.\n$$\n- 定义一个阈值 $\\tau = \\mu_n + z \\sigma_n$，其中 $z$ 为固定值，惩罚系数 $\\lambda  0$。\n- 为每个特征索引 $j$ 定义惩罚后的重要性：\n$$\nP_j = I_j - \\lambda \\max(0, I_j - \\tau).\n$$\n\n在使用和不使用保障措施的情况下执行选择：\n- 无惩罚选择通过降序排列 $I_j$ 来选择前 $k$ 个索引。\n- 有惩罚选择通过降序排列 $P_j$ 来选择前 $k$ 个索引。\n- 如果 $I_j  \\tau$，则特征索引 $j$ 被标记为可疑。\n- 将泄露检测定义为一个布尔值，如果至少有一个被标记为可疑的特征，在无惩罚选择中被选中，但在有惩罚选择中被排除，则该值为真：\n$$\n\\text{detected} = \\left( \\left\\{ j \\mid I_j  \\tau \\right\\} \\cap \\text{TopK}(I) \\right) \\setminus \\text{TopK}(P) \\neq \\emptyset,\n$$\n其中 $\\text{TopK}(I)$ 是 $I$ 中 $k$ 个最大条目的索引集合，$\\text{TopK}(P)$ 类似。\n\n您的程序必须实现上述过程，并评估以下测试套件。每个测试用例是一个包含 $(\\text{seed}, n, d_b, d_n, d_\\ell, s_\\ell, k)$ 的元组：\n- 无泄露的理想情况：$(42, 600, 5, 5, 0, 0.0, 3)$\n- 轻度泄露：$(43, 600, 5, 5, 1, 0.9, 3)$\n- 强泄露：$(44, 600, 5, 5, 2, 0.05, 3)$\n- 边界情况（更小的样本，更多的噪声哨兵，单个强泄露）：$(45, 200, 5, 10, 1, 0.1, 3)$\n\n对 ANN 使用固定的超参数：隐藏层宽度 $h = 12$，学习率 $\\alpha = 0.1$，轮数 $300$。对保障措施使用 $z = 3$ 和 $\\lambda = 0.8$。所有随机操作必须为每个测试用例使用所提供的种子，以确保确定性。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的检测布尔值，形式为用方括号括起来的逗号分隔列表（例如，$[b_1,b_2,b_3,b_4]$），其中每个 $b_i$ 为 $True$ 或 $False$。", "solution": "用户提供的问题已经过严格验证，并被确定为是合理的。它在科学上基于已确立的机器学习原则，是一个具有清晰、确定性过程的良构问题，并且表述客观。因此，这里提供一个完整的解决方案。\n\n目标是构建一个确定性程序来检测特征工程中的标签泄露。实现方法是：创建一个包含有意泄露特征的合成数据集，训练一个神经网络分类器，使用排列方法评估特征重要性，并应用一种统计保障措施来识别和惩罚相对于已知无信息量特征而言重要性可疑过高的特征。\n\n过程结构如下：\n\n首先，为每个测试用例生成一个合成的二元分类数据集。每个测试用例由一个参数元组定义：$(\\text{seed}, n, d_b, d_n, d_\\ell, s_\\ell, k)$，这些参数控制随机种子、样本数量和特征块的维度。所有随机操作都使用种子以确保确定性。\n-   具有信息量的基础特征 $\\mathbf{x}_i^b \\in \\mathbb{R}^{d_b}$ 从标准正态分布 $\\mathcal{N}(\\mathbf{0}, I)$ 中抽取。\n-   使用确定性生成的权重向量 $\\mathbf{w}^\\star$ 和偏置 $b^\\star$ 定义一个真实的线性模型 $z_i = \\mathbf{w}^{\\star \\top}\\mathbf{x}_i^b + b^\\star$。正类的真实概率由 logistic 函数给出，$p_i = \\sigma(z_i) = (1 + e^{-z_i})^{-1}$。然后从参数为 $p_i$ 的伯努利分布中抽样二元标签 $y_i \\in \\{0, 1\\}$。\n-   无信息量的噪声哨兵特征 $\\mathbf{x}_i^n \\in \\mathbb{R}^{d_n}$ 从 $\\mathcal{N}(\\mathbf{0}, I)$ 中抽取，并且在统计上与基础特征和标签都独立。\n-   潜在的泄露特征 $\\mathbf{x}_i^\\ell \\in \\mathbb{R}^{d_\\ell}$ 是通过用高斯噪声直接扰乱标签来构建的：$x_{i,j}^\\ell = y_i + \\eta_{i,j}$，其中 $\\eta_{i,j} \\sim \\mathcal{N}(0, s_\\ell^2)$。参数 $s_\\ell$ 控制泄露的强度；较小的 $s_\\ell$ 意味着更强、更明显的泄露。如果 $d_\\ell=0$，则此块为空。\n-   每个样本 $i$ 的最终特征向量是拼接而成的 $\\mathbf{x}_i = [\\mathbf{x}_i^b, \\mathbf{x}_i^n, \\mathbf{x}_i^\\ell] \\in \\mathbb{R}^{d}$，其中总维度为 $d = d_b + d_n + d_\\ell$。\n\n接下来，对数据进行预处理。数据集按顺序被分割成一个包含前 $\\lfloor 0.7n \\rfloor$ 个样本的训练集和一个包含剩余 $n - \\lfloor 0.7n \\rfloor$ 个样本的验证集。为防止来自验证集的信息泄露，特征标准化通过*仅*在训练集上计算每个特征 $j$ 的均值 $m_j$ 和标准差 $s_j$ 来执行。然后使用这些统计数据根据 $\\tilde{x}_{i,j} = (x_{i,j} - m_j) / s_j$ 来转换训练和验证数据。向 $s_j$ 添加一个很小的 epsilon 以防止对常数特征进行除零操作。\n\n然后，在标准化的训练数据上训练一个人工神经网络 (ANN)。该网络有一个固定宽度 $h=12$ 的单隐藏层，使用修正线性单元 (ReLU) 激活函数，$\\text{ReLU}(u) = \\max(0,u)$。输出层由一个带有 logistic 激活函数 $\\sigma(u)$ 的单个神经元组成，以预测概率 $\\hat{y}_i$。模型被训练以最小化训练集上的二元交叉熵 (BCE) 损失：\n$$\n\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}}\\left( y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right).\n$$\n训练使用全批量梯度下降法进行，轮数为 $300$，学习率为 $\\alpha = 0.1$。\n\n训练后，使用验证集上的排列重要性方法来量化特征的重要性。此方法衡量模型对每个特征的依赖程度。\n-   首先，在原始（未排列的）验证集上计算基线损失 $L_0$。\n-   然后，对于每个特征索引 $j \\in \\{1, \\dots, d\\}$，验证特征矩阵的第 $j$ 列的值被随机排列。在这个扰动过的数据集上重新计算损失 $L_{(j)}$。\n-   特征 $j$ 的排列重要性定义为损失的增加量：$I_j = L_{(j)} - L_0$。一个大的正值 $I_j$ 表示模型严重依赖特征 $j$ 来做出准确的预测。\n\n实施了一种统计保障措施，以区分合法的高重要性特征和可能指示泄露的可疑重要特征。该保障措施使用噪声哨兵特征作为零点参考。\n-   仅根据噪声哨兵特征集计算重要性的均值 $\\mu_n$ 和标准差 $\\sigma_n$：\n$$\n\\mu_n = \\frac{1}{|J_n|}\\sum_{j \\in J_n} I_j, \\qquad \\sigma_n = \\sqrt{\\frac{1}{|J_n|}\\sum_{j \\in J_n}(I_j - \\mu_n)^2},\n$$\n其中 $J_n$ 是噪声特征的索引集。\n-   使用 z-score 乘数 $z=3$ 定义一个统计阈值 $\\tau = \\mu_n + z \\sigma_n$。任何重要性 $I_j  \\tau$ 的特征 $j$ 都会被标记为“可疑”，因为其重要性显著大于一个无信息量特征的预期值。\n-   然后为每个特征计算一个惩罚后的重要性分数 $P_j$，以折减任何可疑特征的重要性：\n$$\nP_j = I_j - \\lambda \\max(0, I_j - \\tau),\n$$\n惩罚系数为 $\\lambda = 0.8$。这种惩罚降低了重要性超过阈值 $\\tau$ 的特征的分数，使它们不太可能被选中。\n\n最后，通过比较使用和不使用保障措施所做的特征选择来应用泄露检测逻辑。\n-   无惩罚选择 $\\text{TopK}(I)$ 是按原始重要性 $I_j$ 排序的前 $k$ 个特征的索引集合。\n-   有惩罚选择 $\\text{TopK}(P)$ 是按惩罚后重要性 $P_j$ 排序的前 $k$ 个特征的索引集合。\n-   如果存在至少一个特征同时满足以下条件，则检测到泄露（结果为 True）：(a) 被标记为可疑（即 $I_j  \\tau$），(b) 是无惩罚前 $k$ 选择的一部分，以及 (c) 从有惩罚前 $k$ 选择中被排除。形式上，如果以下集合非空，则发生检测：\n$$\n\\left( \\left\\{ j \\mid I_j  \\tau \\right\\} \\cap \\text{TopK}(I) \\right) \\setminus \\text{TopK}(P) \\neq \\emptyset.\n$$\n程序对套件中的每个测试用例执行这整个过程，并报告最终的布尔检测结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic procedure to detect label leakage in feature engineering.\n    \"\"\"\n\n    test_cases = [\n        # (seed, n, d_b, d_n, d_ell, s_ell, k)\n        (42, 600, 5, 5, 0, 0.0, 3),   # Happy path with no leak\n        (43, 600, 5, 5, 1, 0.9, 3),   # Mild leak\n        (44, 600, 5, 5, 2, 0.05, 3),  # Strong leak\n        (45, 200, 5, 10, 1, 0.1, 3), # Boundary case\n    ]\n\n    results = []\n    \n    # Fixed hyperparameters\n    h = 12\n    learning_rate = 0.1\n    epochs = 300\n    z_score_threshold = 3.0\n    penalty_lambda = 0.8\n    train_frac = 0.7\n\n    for case in test_cases:\n        seed, n, d_b, d_n, d_ell, s_ell, k = case\n        \n        # Set seed for determinism in all random operations\n        np.random.seed(seed)\n        \n        # --- 1. Data Generation ---\n        # Ground-truth weights and bias\n        w_star = np.random.randn(d_b, 1)\n        b_star = np.random.randn(1)\n        \n        # Base features\n        X_b = np.random.randn(n, d_b)\n        \n        # Labels\n        z_logits = X_b @ w_star + b_star\n        p_labels = 1 / (1 + np.exp(-z_logits))\n        y = (np.random.rand(n, 1)  p_labels).astype(float)\n        \n        # Noise sentinel features\n        X_n = np.random.randn(n, d_n)\n        \n        # Leaked features\n        if d_ell > 0:\n            eta = np.random.randn(n, d_ell) * s_ell\n            X_ell = y + eta\n        else:\n            X_ell = np.empty((n, 0))\n            \n        # Concatenate all feature blocks\n        X = np.hstack([X_b, X_n, X_ell])\n        d = d_b + d_n + d_ell\n\n        # --- 2. Data Preprocessing ---\n        n_train = int(np.floor(train_frac * n))\n        X_train, X_val = X[:n_train], X[n_train:]\n        y_train, y_val = y[:n_train], y[n_train:]\n\n        train_mean = np.mean(X_train, axis=0)\n        train_std = np.std(X_train, axis=0)\n        # Add epsilon for numerical stability if a feature has zero standard deviation\n        train_std[train_std == 0] = 1e-8\n        \n        X_train_std = (X_train - train_mean) / train_std\n        X_val_std = (X_val - train_mean) / train_std\n\n        # --- 3. ANN Model and Training ---\n        # Initialize weights with Xavier/Glorot scaling\n        W1 = np.random.randn(d, h) * np.sqrt(2.0 / (d + h))\n        b1 = np.zeros(h)\n        W2 = np.random.randn(h, 1) * np.sqrt(2.0 / (h + 1))\n        b2 = np.zeros(1)\n\n        def bce_loss(y_true, y_pred, epsilon=1e-9):\n            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n            return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n        # Full-batch gradient descent\n        for _ in range(epochs):\n            # Forward pass\n            z1 = X_train_std @ W1 + b1\n            a1 = np.maximum(0, z1)\n            z2 = a1 @ W2 + b2\n            y_hat = 1 / (1 + np.exp(-z2))\n\n            # Backward pass\n            d_z2 = (y_hat - y_train) / n_train\n            d_W2 = a1.T @ d_z2\n            d_b2 = np.sum(d_z2, axis=0)\n            \n            d_a1 = d_z2 @ W2.T\n            d_z1 = d_a1 * (z1 > 0)\n            d_W1 = X_train_std.T @ d_z1\n            d_b1 = np.sum(d_z1, axis=0)\n\n            # Update weights\n            W1 -= learning_rate * d_W1\n            b1 -= learning_rate * d_b1\n            W2 -= learning_rate * d_W2\n            b2 -= learning_rate * d_b2\n        \n        def predict(X_in):\n            z1 = X_in @ W1 + b1\n            a1 = np.maximum(0, z1)\n            z2 = a1 @ W2 + b2\n            return 1 / (1 + np.exp(-z2))\n\n        # --- 4. Permutation Importance ---\n        baseline_loss = bce_loss(y_val, predict(X_val_std))\n        importances = np.zeros(d)\n        \n        for j in range(d):\n            X_val_perm = X_val_std.copy()\n            # Shuffle in place using the global RNG stream\n            np.random.shuffle(X_val_perm[:, j])\n            permuted_loss = bce_loss(y_val, predict(X_val_perm))\n            importances[j] = permuted_loss - baseline_loss\n\n        # --- 5. Safeguard and Detection Logic ---\n        I = importances\n        noise_indices = range(d_b, d_b + d_n)\n        \n        if d_n > 1:\n            I_noise = I[noise_indices]\n            mu_n = np.mean(I_noise)\n            sigma_n = np.std(I_noise)\n        elif d_n == 1:\n            mu_n = I[noise_indices[0]]\n            sigma_n = 0.0 # Std of a single point is 0\n        else: # d_n == 0\n            mu_n = 0.0\n            sigma_n = 0.0 # Cannot compute threshold without sentinels\n\n        tau = mu_n + z_score_threshold * sigma_n\n        \n        # Penalized importance\n        P = I - penalty_lambda * np.maximum(0, I - tau)\n        \n        # Get top-k indices\n        topk_I_indices = np.argsort(I)[-k:][::-1]\n        topk_P_indices = np.argsort(P)[-k:][::-1]\n        \n        # Convert to sets for formal comparison\n        set_topk_I = set(topk_I_indices)\n        set_topk_P = set(topk_P_indices)\n        \n        # Identify suspicious features\n        suspicious_indices = {j for j, imp in enumerate(I) if imp > tau}\n        \n        # Find suspicious features that are selected by I but deselected by P\n        suspiciously_selected_by_I = suspicious_indices.intersection(set_topk_I)\n        deselected_by_P = suspiciously_selected_by_I.difference(set_topk_P)\n        \n        is_detected = len(deselected_by_P) > 0\n        results.append(is_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3124151"}]}