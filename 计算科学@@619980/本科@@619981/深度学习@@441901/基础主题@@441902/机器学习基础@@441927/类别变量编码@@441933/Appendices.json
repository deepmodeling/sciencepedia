{"hands_on_practices": [{"introduction": "独热编码（或称虚拟变量编码）是处理分类变量的标准方法，但其系数的解释可能颇具挑战性。本练习将揭示一个基本属性：虽然单个系数会随着参考类别的改变而变化，但该分类变量作为一个整体的预测能力是保持不变的。这一洞察对于正确评估分类特征的重要性至关重要，它指导我们使用 F 检验等整体性工具，而不是过分关注不稳定的单个 t 检验统计量 [@problem_id:3130441]。", "problem": "一个包含 $n=60$ 个观测值的数据集使用普通最小二乘法 (OLS) 进行分析。结果变量 $Y$ 被建模为一个中心化的连续协变量 $X$（带截距）和一个具有三个水平 $A$、$B$ 和 $C$ 的分类因子 $G$ 的线性函数。在一个包含截距、$X$ 和指示变量的完整模型中，因子 $G$ 由两个指示变量表示。一个简化模型仅包含截距和 $X$。\n\n使用了两种等价的 $G$ 编码方案：\n\n- 方案 S1（参考水平为 $A$）：分别为水平 $B$ 和 $C$ 设置指示变量 $I_{B}$ 和 $I_{C}$。\n- 方案 S2（参考水平为 $C$）：分别为水平 $A$ 和 $B$ 设置指示变量 $J_{A}$ 和 $J_{B}$。\n\n从模型拟合中，您获得了以下信息：\n\n- 简化模型（仅含截距和 $X$）得到的残差平方和为 $RSS_{R}=820$，并估计了 $p_{R}=2$ 个参数。\n- 完整模型（含截距、$X$ 和两个 $G$ 的指示变量）得到的残差平方和为 $RSS_{F}=760$，并估计了 $p_{F}=4$ 个参数。\n- 在方案 S1 下，两个组指示变量的 OLS 估计值及其标准误为：$\\hat{\\beta}_{B}=-3.0$，标准误为 $1.8$；$\\hat{\\beta}_{C}=-1.5$，标准误为 $1.9$。\n- 在方案 S2 下，两个组指示变量的 OLS 估计值及其标准误为：$\\hat{\\gamma}_{A}=+1.5$，标准误为 $1.6$；$\\hat{\\gamma}_{B}=-1.5$，标准误为 $1.7$。\n\n任务：\n\n1) 仅使用一般线性模型的定义，判断改变 $G$ 的参考水平是否会改变完整模型的拟合值或残差平方和，并解释这对 $G$ 作为一个整体的联合检验意味着什么。\n\n2) 计算在 S1 和 S2 方案下组指示变量的各自的 $t$ 统计量，并说明它们是否因编码方式的改变而改变。\n\n3) 计算用于检验在给定截距和 $X$ 的情况下 $G$ 没有效应的原假设（即检验两个指示变量的系数联合为零）的公共 $F$ 统计量值。将最终数值答案四舍五入到四位有效数字。", "solution": "该问题被验证为自洽的，科学上基于线性模型理论，并且提法明确。所有提供的数据都是一致的，足以完成任务。\n\n### 任务1：拟合值、RSS 的不变性及其对联合检验的启示\n\n一般线性模型用矩阵形式表示为 $\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$，其中 $\\mathbf{Y}$ 是响应向量，$\\mathbf{X}$ 是设计矩阵，$\\boldsymbol{\\beta}$ 是参数向量，$\\boldsymbol{\\epsilon}$ 是误差向量。$\\boldsymbol{\\beta}$ 的普通最小二乘 (OLS) 估计量是 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}$。拟合值向量由 $\\mathbf{Y}$ 在 $\\mathbf{X}$ 的列空间（记为 $C(\\mathbf{X})$）上的正交投影给出。这个投影是 $\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{H}\\mathbf{Y}$，其中 $\\mathbf{H}$ 是投影矩阵，通常称为“帽子矩阵”。\n\n分类因子 $G$ 的两种编码方案 S1 和 S2 代表了完整模型的一种重新参数化。设方案 S1 的设计矩阵为 $\\mathbf{X}_1$，方案 S2 的设计矩阵为 $\\mathbf{X}_2$。\n对于 S1（参考水平 A），设计矩阵的列是 $[\\mathbf{1}, \\mathbf{X}_{\\text{cov}}, \\mathbf{I}_B, \\mathbf{I}_C]$，其中 $\\mathbf{1}$ 是代表截距的全一向量，$\\mathbf{X}_{\\text{cov}}$ 是连续协变量 $X$ 的向量，$\\mathbf{I}_B, \\mathbf{I}_C$ 是水平 $B$ 和 $C$ 的指示向量。\n对于 S2（参考水平 C），设计矩阵的列是 $[\\mathbf{1}, \\mathbf{X}_{\\text{cov}}, \\mathbf{J}_A, \\mathbf{J}_B]$。\n\n这些指示向量通过恒等式 $\\mathbf{I}_A + \\mathbf{I}_B + \\mathbf{I}_C = \\mathbf{1}$ 相关联。两种方案中使用的向量也直接相关：$\\mathbf{J}_A = \\mathbf{I}_A$ 和 $\\mathbf{J}_B = \\mathbf{I}_B$。\n在 S1 中，与因子 $G$ 相关的子空间的基向量是 $\\{\\mathbf{I}_B, \\mathbf{I}_C\\}$（在有截距的情况下），而在 S2 中它们是 $\\{\\mathbf{J}_A, \\mathbf{J}_B\\}$。让我们证明这两组向量，当与截距和协变量结合时，张成的是同一个空间。\n列空间 $C(\\mathbf{X}_1)$ 的基向量是 $\\{\\mathbf{1}, \\mathbf{X}_{\\text{cov}}, \\mathbf{I}_B, \\mathbf{I}_C\\}$。\n列空间 $C(\\mathbf{X}_2)$ 的基向量是 $\\{\\mathbf{1}, \\mathbf{X}_{\\text{cov}}, \\mathbf{J}_A, \\mathbf{J}_B\\}$。\n我们可以用 S1 的基向量来表示 S2 的基向量：\n$\\mathbf{J}_A = \\mathbf{I}_A = \\mathbf{1} - \\mathbf{I}_B - \\mathbf{I}_C$。\n$\\mathbf{J}_B = \\mathbf{I}_B$。\n由于 $C(\\mathbf{X}_2)$ 的每个基向量都是 $C(\\mathbf{X}_1)$ 基向量的线性组合，因此 $C(\\mathbf{X}_2) \\subseteq C(\\mathbf{X}_1)$。类似地，我们可以用 S2 的基来表示 S1 的基向量：\n$\\mathbf{I}_C = \\mathbf{1} - \\mathbf{I}_A - \\mathbf{I}_B = \\mathbf{1} - \\mathbf{J}_A - \\mathbf{J}_B$。\n$\\mathbf{I}_B = \\mathbf{J}_B$。\n这表明 $C(\\mathbf{X}_1) \\subseteq C(\\mathbf{X}_2)$。\n因此，这两个列空间是相同的：$C(\\mathbf{X}_1) = C(\\mathbf{X}_2)$。\n\n由于列空间相同，两种方案的投影矩阵 $\\mathbf{H}$ 是相同的。因此，拟合值 $\\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y}$ 对于参考水平的选择是不变的。因此，残差向量 $\\mathbf{e} = \\mathbf{Y} - \\hat{\\mathbf{Y}}$ 也是不变的。所以，残差平方和 $RSS_F = \\mathbf{e}^T\\mathbf{e}$ 不会因重新参数化而改变。问题陈述通过提供单个值 $RSS_{F}=760$ 证实了这一点。\n\n这种不变性对于 $G$ 的联合检验至关重要。检验代表 $G$ 的变量块显著性的 $F$ 检验，比较的是完整模型（含 $G$）和简化模型（不含 $G$）。$F$ 统计量是 $RSS_R$、$RSS_F$、$n$、$p_R$ 和 $p_F$ 的函数。由于这些量都不依赖于为完整模型选择的具体编码方案，因此对于联合原假设 $H_0: \\beta_B = \\beta_C = 0$（在 S1 下）或 $H_0: \\gamma_A = \\gamma_B = 0$（在 S2 下）所得的 $F$ 统计量将是相同的。联合检验为因子 $G$ 对模型的总体贡献提供了一个明确的评估。\n\n### 任务2：各自的 t 统计量\n\n系数估计值 $\\hat{\\beta}$ 的 $t$ 统计量计算公式为 $t = \\frac{\\hat{\\beta}}{SE(\\hat{\\beta})}$。\n\n对于方案 S1（参考水平 A）：\n指示变量 $I_B$ 的 $t$ 统计量为：\n$$ t_{B, S1} = \\frac{\\hat{\\beta}_{B}}{SE(\\hat{\\beta}_{B})} = \\frac{-3.0}{1.8} = -1.666... $$\n指示变量 $I_C$ 的 $t$ 统计量为：\n$$ t_{C, S1} = \\frac{\\hat{\\beta}_{C}}{SE(\\hat{\\beta}_{C})} = \\frac{-1.5}{1.9} \\approx -0.78947... $$\n\n对于方案 S2（参考水平 C）：\n指示变量 $J_A$ 的 $t$ 统计量为：\n$$ t_{A, S2} = \\frac{\\hat{\\gamma}_{A}}{SE(\\hat{\\gamma}_{A})} = \\frac{+1.5}{1.6} = 0.9375 $$\n指示变量 $J_B$ 的 $t$ 统计量为：\n$$ t_{B, S2} = \\frac{\\hat{\\gamma}_{B}}{SE(\\hat{\\gamma}_{B})} = \\frac{-1.5}{1.7} \\approx -0.88235... $$\n\n各自的 $t$ 统计量在不同编码方案下明显改变。这是因为它们检验的是不同的假设。例如，$t_{B, S1}$ 检验的是 B 组的平均响应是否与参考组 A 的不同，而 $t_{B, S2}$ 检验的是 B 组是否与参考组 C 的不同。\n\n### 任务3：公共的 F 统计量\n\n用于检验一组系数为零的原假设的 $F$ 统计量是通过比较完整模型 ($RSS_F$) 和简化模型 ($RSS_R$) 的残差平方和来计算的。其公式为：\n$$ F = \\frac{(RSS_R - RSS_F) / (p_F - p_R)}{RSS_F / (n - p_F)} $$\n其中，$p_F$ 和 $p_R$ 分别是完整模型和简化模型中的参数数量，$n$ 是观测数量。\n\n已知条件如下：\n- 简化模型残差平方和：$RSS_{R} = 820$\n- 完整模型残差平方和：$RSS_{F} = 760$\n- 简化模型中的参数数量：$p_{R} = 2$（截距和 $X$）\n- 完整模型中的参数数量：$p_{F} = 4$（截距、$X$ 和两个 $G$ 的指示变量）\n- 观测数量：$n = 60$\n\n被检验的参数数量（即 $G$ 的指示变量数量）是 $p_F - p_R = 4 - 2 = 2$。\n分子的自由度是 $df_1 = p_F - p_R = 2$。\n分母的自由度是 $df_2 = n - p_F = 60 - 4 = 56$。\n\n将数值代入公式：\n$$ F = \\frac{(820 - 760) / (4 - 2)}{760 / (60 - 4)} $$\n$$ F = \\frac{60 / 2}{760 / 56} $$\n$$ F = \\frac{30}{760 / 56} $$\n$$ F = \\frac{30 \\times 56}{760} = \\frac{1680}{760} = \\frac{168}{76} = \\frac{42}{19} $$\n$$ F \\approx 2.2105263... $$\n\n四舍五入到四位有效数字，其值为 $2.211$。\n此 $F$ 统计量检验的是因子 $G$ 没有效应的原假设，即其两个指示变量的系数联合为零。如任务1中所解释，该值对于参考水平的选择是不变的。", "answer": "$$\\boxed{2.211}$$", "id": "3130441"}, {"introduction": "在掌握了独热编码的基础之后，我们进一步探索：如果类别本身具有内在结构，例如一年中的月份或一周中的日期，我们该如何更有效地编码？通用的独热编码无法捕捉这种周期性的邻近关系。这个动手编程练习将傅里叶特征作为一种优雅的解决方案，通过实际案例展示编码数据的周期性结构如何能显著提高模型在未见过的类别之间进行泛化和插值的能力 [@problem_id:3121725]。", "problem": "给定一个表示季节的循环分类变量，其周期为 $12$，由索引 $c \\in \\{0,1,2,\\dots,11\\}$ 标记。考虑类别 $c$ 的两种编码方式：(i) 独热编码 $e(c) \\in \\mathbb{R}^{12}$，在索引 $c$ 处有一个等于 $1$ 的条目，其余处为 $0$；(ii) 傅里叶特征编码 $\\phi(c) \\in \\mathbb{R}^{1+2|M|}$，由一组谐波 $M = \\{1,2\\}$ 构建，其形式为 $\\phi(c) = [1,\\sin(2\\pi m c / 12),\\cos(2\\pi m c / 12)]_{m \\in M}$，其中所有角度均以弧度为单位。您将通过最小化均方误差（MSE）来拟合一个线性模型 $f(x) = w^\\top x$，并使用 Moore–Penrose 伪逆计算普通最小二乘（OLS）解。目标是测试在存在循环分类变量的情况下，周期性傅里叶特征是否有益于模型。\n\n使用的基本定义：\n- 对于数据集 $\\{(x_i,y_i)\\}_{i=1}^n$ 上的模型 $f$，均方误差（MSE）为 $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))^2$。\n- 普通最小二乘（OLS）选择 $w$ 来最小化 MSE，其最小范数解由 $w = X^+ y$ 给出，其中 $X^+$ 是设计矩阵 $X$ 的 Moore–Penrose 伪逆， $y$ 是目标向量。\n\n构建一个玩具设置，其中目标是类别索引 $c$ 的确定性函数，不含噪声。对于每个测试用例，您将：\n1. 按照指定枚举类别，生成训练集和测试集。\n2. 使用独热编码 $e(c)$ 和谐波为 $M = \\{1,2\\}$ 的傅里叶特征 $\\phi(c)$ 对输入进行编码。\n3. 对于两种编码，在训练集上使用 OLS 拟合线性模型。\n4. 在指定的测试类别上评估两种模型的测试 MSE。\n5. 输出一个布尔值，指示傅里叶特征模型的测试 MSE 是否严格低于独热模型的测试 MSE。\n\n角度单位说明：三角函数中的所有角度必须以弧度表示。\n\n测试套件：\n- 用例 $1$ (理想周期情况)：目标 $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right)$；训练类别 $S_{\\text{train}} = \\{0,2,4,6,8,10\\}$；测试类别 $S_{\\text{test}} = \\{1,3,5,7,9,11\\}$。\n- 用例 $2$ (对抗性周期情况，谐波未被选择)：目标 $y(c) = \\cos(\\pi c)$，等价于 $y(c) = (-1)^c$，对应于周期 $12$ 的谐波 $m=6$；训练类别 $S_{\\text{train}} = \\{0,2,4,6,8,10\\}$；测试类别 $S_{\\text{test}} = \\{1,3,5,7,9,11\\}$。\n- 用例 $3$ (特征覆盖的谐波混合，有缺失类别)：目标 $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right) + \\frac{1}{2}\\cos\\!\\left(\\frac{4\\pi c}{12}\\right)$；训练类别 $S_{\\text{train}} = \\{0,1,2,4,5,6,8,9,10\\}$；测试类别 $S_{\\text{test}} = \\{3,7,11\\}$。\n- 用例 $4$ (边界情况，覆盖全类别)：目标 $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right) + \\frac{1}{2}\\cos\\!\\left(\\frac{4\\pi c}{12}\\right)$；训练类别 $S_{\\text{train}} = \\{0,1,2,3,4,5,6,7,8,9,10,11\\}$；测试类别 $S_{\\text{test}} = \\{0,1,2,3,4,5,6,7,8,9,10,11\\}$。\n\n模型和评估细节：\n- 对于每个用例，通过堆叠训练类别的编码来形成训练设计矩阵 $X_{\\text{one-hot}}$ 和 $X_{\\text{fourier}}$，并通过将指定的 $y(c)$ 应用于每个训练类别来形成目标向量 $y_{\\text{train}}$。\n- 计算 $w_{\\text{one-hot}} = X_{\\text{one-hot}}^+ y_{\\text{train}}$ 和 $w_{\\text{fourier}} = X_{\\text{fourier}}^+ y_{\\text{train}}$。\n- 使用相应的编码形成 $X_{\\text{test}}$，在测试类别上评估预测，并计算每个模型的测试 MSE。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是对应于用例 $1$ 到 $4$ 的布尔值。如果傅里叶特征模型的测试 MSE 严格低于独热模型的测试 MSE，则为 $True$，否则为 $False$。例如，输出应类似于 $[True,False,True,False]$。", "solution": "该问题要求对循环分类变量的两种编码方案进行比较分析：独热编码和傅里叶特征编码。性能评估基于普通最小二乘（OLS）线性模型在测试集上的均方误差（MSE）。该问题是有效的，因为它在科学上基于线性代数和机器学习原理，问题设定良好并提供了所有必要信息，并且其评估标准是客观的。\n\n分析的核心在于理解每种编码方法所隐含的假设（归纳偏置）。独热编码创建了一个特征空间，其中每个类别由一个正交基向量表示。在此空间中的线性模型 $f(e(c)) = w^\\top e(c) = w_c$，为每个类别 $c$ 分配一个独立的权重 $w_c$。这使得模型能够表示类别集合上的任何任意函数，但它没有提供对未见类别的泛化机制。相比之下，傅里叶特征编码通过一组正弦基函数的值构成的向量 $\\phi(c)$ 来表示每个类别 $c$。线性模型 $f(\\phi(c)) = w^\\top \\phi(c)$ 被约束为将目标函数表示为这些基函数的线性组合。这提供了一个强大的归纳偏置，即假设目标函数是周期性和平滑的，这有助于泛化，但将模型的表达能力限制在所选谐波的张成空间内。\n\n问题指定了周期 $P=12$ 和一组谐波 $M = \\{1,2\\}$。对于类别 $c \\in \\{0, 1, \\dots, 11\\}$，傅里叶特征向量为 $\\phi(c) \\in \\mathbb{R}^{5}$：\n$$\n\\phi(c) = \\left[1, \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot c}{12}\\right), \\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot c}{12}\\right), \\sin\\left(\\frac{2\\pi \\cdot 2 \\cdot c}{12}\\right), \\cos\\left(\\frac{2\\pi \\cdot 2 \\cdot c}{12}\\right)\\right]^\\top\n$$\n独热编码 $e(c) \\in \\mathbb{R}^{12}$ 是一个在索引 $c$ 处为 $1$，其余处为 $0$ 的向量。\n\n对于每个用例，我们通过堆叠训练集 $S_{\\text{train}}$ 中所有 $c$ 的行向量 $e(c)^\\top$ 和 $\\phi(c)^\\top$ 来构建训练设计矩阵 $X_{\\text{one-hot}}$ 和 $X_{\\text{fourier}}$。目标向量 $y_{\\text{train}}$ 是通过将给定的目标函数应用于每个 $c \\in S_{\\text{train}}$ 形成的。OLS 权重向量使用 Moore-Penrose 伪逆求得，$w = X^+ y_{\\text{train}}$。这提供了最小化平方误差和的唯一最小范数解。然后，我们为测试集 $S_{\\text{test}}$ 构建测试矩阵 $X_{\\text{test}}$，计算预测 $y_{\\text{pred}} = X_{\\text{test}} w$，并计算测试 MSE：$\\text{MSE} = \\frac{1}{|S_{\\text{test}}|} \\sum_{i \\in S_{\\text{test}}} (y_i - y_{\\text{pred},i})^2$。\n\n**用例 1：周期性目标，数据不完整**\n- 目标：$y(c) = \\sin(\\frac{2\\pi c}{12})$。该函数是傅里叶编码中的基函数之一（对应于谐波 $m=1$）。\n- 训练集：$S_{\\text{train}} = \\{0,2,4,6,8,10\\}$。测试集：$S_{\\text{test}} = \\{1,3,5,7,9,11\\}$。\n- **傅里叶模型**：由于目标函数完全位于傅里叶特征的张成空间内，模型可以完美地表示它。训练数据足够多样化，使得 OLS 能够识别正确的权重，主要是为 $\\sin(\\frac{2\\pi c}{12})$ 特征赋予权重 $1$，其他权重接近于零。这个学到的函数能够完美地泛化到未见的测试类别。因此，测试 MSE 将约等于 $0$。\n- **独热模型**：模型在偶数索引的类别上进行训练。对于任何训练类别 $c_{\\text{train}} \\in S_{\\text{train}}$，权重 $w_{c_{\\text{train}}}$ 将学习目标值 $y(c_{\\text{train}})$。对于未见的类别 $c_{\\text{test}} \\in S_{\\text{test}}$，其在训练设计矩阵中对应的列全为零。伪逆解的最小范数属性将这些未见类别的权重 $w_{c_{\\text{test}}}$ 设置为 $0$。因此，模型对所有测试点都预测 $f(e(c_{\\text{test}})) = 0$。而真实的测试目标 $y(c_{\\text{test}})$ 是非零的，导致显著的测试 MSE。\n- **结论**：$\\text{MSE}_{\\text{fourier}} \\approx 0  \\text{MSE}_{\\text{one-hot}}$。结果为 **True**。\n\n**用例 2：不匹配的周期性目标**\n- 目标：$y(c) = \\cos(\\pi c) = \\cos(\\frac{2\\pi \\cdot 6 \\cdot c}{12})$。该函数对应于谐波 $m=6$，而这并不在模型的特征集 $M=\\{1,2\\}$ 中。\n- 训练/测试集与用例 1 相同。\n- 对于偶数类别的训练集，目标是 $y(c) = \\cos(\\pi c) = (-1)^c = 1$，对所有 $c \\in S_{\\text{train}}$ 均成立。\n- **傅里叶模型**：训练目标向量是一个全为 1 的常数向量。该向量与傅里叶设计矩阵的第一列（偏置项）相同。因此，OLS 解将是 $w = [1,0,0,0,0]^\\top$，导致模型对所有输入都预测 $f(c)=1$。对于测试集（奇数类别），真实目标是 $y(c) = -1$。模型预测为 $1$ 导致每个测试样本的误差为 $(-1 - 1)^2 = 4$。因此，$\\text{MSE}_{\\text{fourier}} = 4$。\n- **独热模型**：与用例 1 一样，模型对所有未见的测试类别预测为 $0$。真实的测试目标是 $-1$。每个测试样本的误差是 $(-1 - 0)^2 = 1$。因此，$\\text{MSE}_{\\text{one-hot}} = 1$。\n- **结论**：$\\text{MSE}_{\\text{fourier}} = 4$，$\\text{MSE}_{\\text{one-hot}} = 1$。条件 $4  1$ 不成立。结果为 **False**。\n\n**用例 3：混合谐波目标，数据不完整**\n- 目标：$y(c) = \\sin(\\frac{2\\pi c}{12}) + \\frac{1}{2}\\cos(\\frac{4\\pi c}{12})$。该函数是 $m=1$ 和 $m=2$ 的基函数的线性组合，这两者都包含在傅里叶特征集中。\n- 训练集：$S_{\\text{train}} = \\{0,1,2,4,5,6,8,9,10\\}$。测试集：$S_{\\text{test}} = \\{3,7,11\\}$。\n- **傅里叶模型**：推理与用例 1 相同。模型的归纳偏置与目标函数完美对齐。对于 5 个参数，有 9 个训练样本，OLS 拟合将准确地恢复底层函数，然后完美地泛化到测试集。测试 MSE 将约等于 $0$。\n- **独热模型**：模型未在类别 $\\{3,7,11\\}$ 上训练。它将对这些测试点预测为 $0$。而真实目标是非零的，导致一个正的测试 MSE。\n- **结论**：$\\text{MSE}_{\\text{fourier}} \\approx 0  \\text{MSE}_{\\text{one-hot}}$。结果为 **True**。\n\n**用例 4：完整数据覆盖**\n- 目标：与用例 3 相同。\n- 训练集和测试集相同且完整：$S_{\\text{train}} = S_{\\text{test}} = \\{0, 1, \\dots, 11\\}$。\n- **独热模型**：由于训练集中包含了所有 12 个类别，独热编码的设计矩阵 $X_{\\text{one-hot}}$ 是一个 $12 \\times 12$ 的单位矩阵 $I_{12}$。其伪逆也是 $I_{12}$。权重为 $w_{\\text{one-hot}} = I_{12} y_{\\text{train}} = y_{\\text{train}}$，这意味着对于每个类别 $w_c = y(c)$。该模型完美地记忆了训练数据。由于测试集与训练集相同，预测是完美的，因此 $\\text{MSE}_{\\text{one-hot}} = 0$。\n- **傅里叶模型**：与用例 3 一样，目标函数在特征的张成空间内。使用完整数据集，OLS 拟合保证能找到精确的权重以完美地重现该函数。模型在训练/测试集上也将实现零误差。因此，$\\text{MSE}_{\\text{fourier}} = 0$。\n- **结论**：由于 $\\text{MSE}_{\\text{fourier}} = 0$ 且 $\\text{MSE}_{\\text{one-hot}} = 0$，严格不等式 $\\text{MSE}_{\\text{fourier}}  \\text{MSE}_{\\text{one-hot}}$（即 $0  0$）不成立。结果为 **False**。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def solve_case(target_func, s_train, s_test, P, M):\n        \"\"\"\n        Solves a single test case for the problem.\n\n        Args:\n            target_func (callable): The function generating the target values y(c).\n            s_train (list): The list of training categories.\n            s_test (list): The list of test categories.\n            P (int): The period of the cyclic variable.\n            M (list): The set of harmonics for Fourier features.\n        \n        Returns:\n            bool: True if the Fourier model's MSE is strictly less than the one-hot model's MSE.\n        \"\"\"\n        # Convert category lists to numpy arrays for vectorized operations\n        c_train = np.array(s_train)\n        c_test = np.array(s_test)\n        \n        # 1. Generate target values for training and test sets\n        y_train = target_func(c_train)\n        y_test = target_func(c_test)\n\n        # 2. Encode inputs for both training and test sets\n        \n        # One-hot encoding\n        num_classes = P\n        X_train_onehot = np.zeros((len(c_train), num_classes))\n        # Use advanced indexing to set the '1's in the one-hot vectors\n        X_train_onehot[np.arange(len(c_train)), c_train] = 1\n        \n        X_test_onehot = np.zeros((len(c_test), num_classes))\n        X_test_onehot[np.arange(len(c_test)), c_test] = 1\n\n        # Fourier feature encoding\n        num_features_fourier = 1 + 2 * len(M)\n        X_train_fourier = np.ones((len(c_train), num_features_fourier))\n        X_test_fourier = np.ones((len(c_test), num_features_fourier))\n        \n        feature_idx = 1\n        for m in M:\n            # Training set features\n            angle_train = 2.0 * np.pi * m * c_train / P\n            X_train_fourier[:, feature_idx] = np.sin(angle_train)\n            X_train_fourier[:, feature_idx + 1] = np.cos(angle_train)\n            \n            # Test set features\n            angle_test = 2.0 * np.pi * m * c_test / P\n            X_test_fourier[:, feature_idx] = np.sin(angle_test)\n            X_test_fourier[:, feature_idx + 1] = np.cos(angle_test)\n            \n            feature_idx += 2\n\n        # 3. Fit linear models using OLS with the Moore-Penrose pseudoinverse\n        w_onehot = np.linalg.pinv(X_train_onehot) @ y_train\n        w_fourier = np.linalg.pinv(X_train_fourier) @ y_train\n        \n        # 4. Evaluate test MSE for both models\n        \n        # Predictions\n        y_pred_onehot = X_test_onehot @ w_onehot\n        y_pred_fourier = X_test_fourier @ w_fourier\n        \n        # Mean Squared Error calculation\n        mse_onehot = np.mean((y_test - y_pred_onehot)**2)\n        mse_fourier = np.mean((y_test - y_pred_fourier)**2)\n        \n        # 5. Return boolean indicating if Fourier MSE is strictly lower\n        return mse_fourier  mse_onehot\n\n    # Define the test cases from the problem statement.\n    P = 12\n    M = [1, 2]\n    \n    test_cases = [\n        # Case 1\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P),\n         's_train': [0, 2, 4, 6, 8, 10],\n         's_test': [1, 3, 5, 7, 9, 11]},\n        # Case 2\n        {'target_func': lambda c: np.cos(np.pi * c),\n         's_train': [0, 2, 4, 6, 8, 10],\n         's_test': [1, 3, 5, 7, 9, 11]},\n        # Case 3\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P) + 0.5 * np.cos(4.0 * np.pi * c / P),\n         's_train': [0, 1, 2, 4, 5, 6, 8, 9, 10],\n         's_test': [3, 7, 11]},\n        # Case 4\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P) + 0.5 * np.cos(4.0 * np.pi * c / P),\n         's_train': list(range(P)),\n         's_test': list(range(P))},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case['target_func'], case['s_train'], case['s_test'], P, M)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3121725"}, {"introduction": "现在，让我们进入现代深度学习的前沿，在这一领域，端到端的可微性是核心要求。如果我们不将一个类别“硬性”地指派给一种表示，而是将其建模为多种基础表示的一种“软性”、可学习的混合体，会怎么样？这个高级实践将通过一个由温度 $\\tau$ 控制的 Softmax 函数来探索这样一种编码系统。通过分析损失景观的梯度和海森矩阵，您将深入理解温度如何平滑优化问题，从而使得基于梯度的优化方法能够学习到离散的分类表示，这一概念是 Gumbel-Softmax 等高级技术的核心思想 [@problem_id:3121698]。", "problem": "考虑一个具有 $M$ 个基代码的分类变量，其中每个基代码是向量 $\\mathbf{b}_i \\in \\mathbb{R}^d$（$i \\in \\{1, \\dots, M\\}$），这些基代码被组装成一个矩阵 $B \\in \\mathbb{R}^{M \\times d}$，其第 $i$ 行为 $\\mathbf{b}_i^\\top$。一个类别 $c$ 由一个 logits 向量 $\\mathbf{z}_c \\in \\mathbb{R}^M$ 参数化，并表示为基代码上的一个由温度控制的软混合，其权重 $\\boldsymbol{\\pi}(\\tau) \\in \\mathbb{R}^M$ 由 Softmax 函数给出\n$$\n\\pi_i(\\tau) = \\frac{\\exp\\left(z_{c,i} / \\tau\\right)}{\\sum_{j=1}^M \\exp\\left(z_{c,j} / \\tau\\right)}, \\quad i \\in \\{1,\\dots,M\\},\n$$\n其中 $\\tau \\in \\mathbb{R}_{0}$ 是温度。类别 $c$ 在温度 $\\tau$ 下的嵌入定义为\n$$\n\\mathbf{e}_c(\\tau) = \\sum_{i=1}^M \\pi_i(\\tau)\\, \\mathbf{b}_i = B^\\top \\boldsymbol{\\pi}(\\tau).\n$$\n一个标量预测器由参数 $\\mathbf{w} \\in \\mathbb{R}^d$ 和 $b_0 \\in \\mathbb{R}$ 给出，产生\n$$\n\\hat{y}(\\tau) = \\mathbf{w}^\\top \\mathbf{e}_c(\\tau) + b_0 = \\mathbf{w}^\\top B^\\top \\boldsymbol{\\pi}(\\tau) + b_0.\n$$\n对于一个标量目标 $y \\in \\mathbb{R}$，考虑平方误差损失\n$$\nL(\\tau) = \\frac{1}{2}\\left(\\hat{y}(\\tau) - y\\right)^2.\n$$\n从基础微积分（链式法则）和 Softmax 的定义出发，通过温度控制的软分配对分类变量进行编码，并通过为每个测试用例计算以下两个量来分析关于 logits $\\mathbf{z}_c$ 的优化景观：\n- 梯度 $\\left\\lVert \\nabla_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2$ 的欧几里得范数。\n- 海森矩阵 $\\left\\lVert \\nabla^2_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2$ 的谱范数，定义为最大奇异值，对于对称矩阵，它等于最大绝对特征值。\n\n你的程序必须实现所述模型，在适用时使用 Softmax 的解析导数精确计算这两个量，并聚合以下测试套件的结果。使用下面给出的固定基代码矩阵 $B$、预测器参数 $\\mathbf{w}$ 和 $b_0$ 以及目标 $y$，并为每个测试用例改变 logits $\\mathbf{z}_c$ 和温度 $\\tau$。除非进行转置，否则所有向量均为行向量。\n\n使用：\n- $M = 4$, $d = 3$,\n- $B = \\begin{bmatrix}\n0.9  -0.4  0.1 \\\\\n0.3  0.8  -0.5 \\\\\n-0.6  0.2  0.7 \\\\\n0.5  -0.1  -0.3\n\\end{bmatrix}$,\n- $\\mathbf{w} = \\left[0.7, -1.1, 0.9\\right]$,\n- $b_0 = 0.2$,\n- $y = 0.5$.\n\n测试套件：\n1. 正常路径：$\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 1.0$。\n2. 平滑区域：$\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 2.0$。\n3. 过渡区域：$\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 0.5$。\n4. 近似硬性区域：$\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 0.1$。\n5. 边界情况（logits 相等）：$\\mathbf{z}_c = \\left[0.0, 0.0, 0.0, 0.0\\right]$, $\\tau = 1.0$。\n6. 边界情况（logits 相等，温度极低）：$\\mathbf{z}_c = \\left[0.0, 0.0, 0.0, 0.0\\right]$, $\\tau = 0.01$。\n\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果是一个双元素列表 $\\left[\\left\\lVert \\nabla_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2, \\left\\lVert \\nabla^2_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2\\right]$。例如，一个包含三个测试用例的输出应如下所示：$\\left[[g_1,h_1],[g_2,h_2],[g_3,h_3]\\right]$，所有条目均表示为不带单位的实数。此任务有助于理解温度如何影响分类编码中从平滑到硬性的过渡，及其对诸如随机梯度下降 (Stochastic Gradient Descent, SGD) 等方法的影响。", "solution": "该问题要求计算平方误差损失函数关于 logits 向量 $\\mathbf{z}_c$ 的梯度的欧几里得范数和海森矩阵的谱范数。该模型使用基代码的温度控制软混合来形成分类嵌入。\n\n### 步骤 1：定义变量和损失函数\n\n设给定参数为：\n-   基代码数量：$M \\in \\mathbb{N}$\n-   基代码维度：$d \\in \\mathbb{N}$\n-   基代码矩阵：$B \\in \\mathbb{R}^{M \\times d}$，行为 $\\mathbf{b}_i^\\top$\n-   预测器权重：$\\mathbf{w} \\in \\mathbb{R}^d$\n-   预测器偏置：$b_0 \\in \\mathbb{R}$\n-   目标值：$y \\in \\mathbb{R}$\n-   类别 c 的 Logits 向量：$\\mathbf{z}_c \\in \\mathbb{R}^M$\n-   温度：$\\tau \\in \\mathbb{R}_{0}$\n\n软混合权重 $\\boldsymbol{\\pi}(\\tau) \\in \\mathbb{R}^M$ 是通过将 Softmax 函数应用于缩放后的 logits $\\mathbf{z}_c/\\tau$ 得到的：\n$$\n\\pi_i(\\tau) = \\frac{\\exp\\left(z_{c,i} / \\tau\\right)}{\\sum_{j=1}^M \\exp\\left(z_{c,j} / \\tau\\right)}\n$$\n类别嵌入 $\\mathbf{e}_c(\\tau) \\in \\mathbb{R}^d$ 是基代码的加权和：\n$$\n\\mathbf{e}_c(\\tau) = \\sum_{i=1}^M \\pi_i(\\tau)\\, \\mathbf{b}_i = B^\\top \\boldsymbol{\\pi}(\\tau)\n$$\n标量预测 $\\hat{y}(\\tau)$ 为：\n$$\n\\hat{y}(\\tau) = \\mathbf{w}^\\top \\mathbf{e}_c(\\tau) + b_0 = \\mathbf{w}^\\top B^\\top \\boldsymbol{\\pi}(\\tau) + b_0\n$$\n让我们定义一个向量 $\\mathbf{v} = B\\mathbf{w} \\in \\mathbb{R}^M$。该向量的分量为 $v_i = \\mathbf{b}_i^\\top \\mathbf{w}$。我们可以将预测重写为：\n$$\n\\hat{y}(\\tau) = (B\\mathbf{w})^\\top \\boldsymbol{\\pi}(\\tau) + b_0 = \\mathbf{v}^\\top \\boldsymbol{\\pi}(\\tau) + b_0\n$$\n平方误差损失为：\n$$\nL(\\tau) = \\frac{1}{2}\\left(\\hat{y}(\\tau) - y\\right)^2\n$$\n\n### 步骤 2：计算梯度 $\\nabla_{\\mathbf{z}_c} L(\\tau)$\n\n我们使用链式法则来求 $L$ 关于 $\\mathbf{z}_c$ 的梯度。为简洁起见，我们将 $\\mathbf{z}_c$ 表示为 $\\mathbf{z}$。\n$$\n\\nabla_{\\mathbf{z}} L = \\frac{\\partial L}{\\partial \\hat{y}} \\nabla_{\\mathbf{z}} \\hat{y}\n$$\n第一项是损失相对于预测的导数：\n$$\n\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y}(\\tau) - y\n$$\n第二项是预测相对于 logits 的梯度：\n$$\n\\nabla_{\\mathbf{z}} \\hat{y} = \\nabla_{\\mathbf{z}} (\\mathbf{v}^\\top \\boldsymbol{\\pi} + b_0) = (\\nabla_{\\mathbf{z}} \\boldsymbol{\\pi})^\\top \\mathbf{v}\n$$\nSoftmax 函数 $\\boldsymbol{\\pi}$ 相对于其输入 $\\mathbf{u}=\\mathbf{z}/\\tau$ 的雅可比矩阵是一个矩阵 $J_{\\boldsymbol{\\pi},\\mathbf{u}}$，其元素为 $(J_{\\boldsymbol{\\pi},\\mathbf{u}})_{ik} = \\frac{\\partial \\pi_i}{\\partial u_k} = \\pi_i(\\delta_{ik} - \\pi_k)$。这可以写成 $J_{\\boldsymbol{\\pi},\\mathbf{u}} = \\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top$。\n根据链式法则，$\\boldsymbol{\\pi}$ 关于 $\\mathbf{z}$ 的雅可比矩阵为：\n$$\nJ_{\\boldsymbol{\\pi},\\mathbf{z}} = \\frac{\\partial \\boldsymbol{\\pi}}{\\partial \\mathbf{z}} = \\frac{\\partial \\boldsymbol{\\pi}}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{z}} = \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\frac{1}{\\tau} I = \\frac{1}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right)\n$$\n由于这个雅可比矩阵是对称的，因此 $(\\nabla_{\\mathbf{z}} \\boldsymbol{\\pi})^\\top = J_{\\boldsymbol{\\pi},\\mathbf{z}}$。因此，\n$$\n\\nabla_{\\mathbf{z}} \\hat{y} = J_{\\boldsymbol{\\pi},\\mathbf{z}} \\mathbf{v} = \\frac{1}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\mathbf{v}\n$$\n该梯度的第 k 个分量是：\n$$\n(\\nabla_{\\mathbf{z}} \\hat{y})_k = \\frac{1}{\\tau} \\left( \\pi_k v_k - \\pi_k \\sum_j \\pi_j v_j \\right) = \\frac{1}{\\tau} \\pi_k(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi})\n$$\n组合各项，损失的梯度为：\n$$\n\\mathbf{g} = \\nabla_{\\mathbf{z}} L = (\\hat{y} - y) \\nabla_{\\mathbf{z}} \\hat{y} = \\frac{\\hat{y} - y}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\mathbf{v}\n$$\n需要计算的量是该向量的欧几里得范数 $\\left\\lVert \\mathbf{g} \\right\\rVert_2$。\n\n### 步骤 3：计算海森矩阵 $\\nabla^2_{\\mathbf{z}_c} L(\\tau)$\n\n海森矩阵 $H_L = \\nabla^2_{\\mathbf{z}} L$ 是 $\\nabla_{\\mathbf{z}} L$ 的梯度。使用向量微积分的乘法法则：\n$$\nH_L = \\nabla_{\\mathbf{z}} \\left( (\\hat{y} - y) \\nabla_{\\mathbf{z}} \\hat{y} \\right) = (\\nabla_{\\mathbf{z}} \\hat{y})(\\nabla_{\\mathbf{z}} \\hat{y})^\\top + (\\hat{y} - y) \\nabla^2_{\\mathbf{z}} \\hat{y}\n$$\n设 $\\mathbf{g}_{\\hat{y}} = \\nabla_{\\mathbf{z}} \\hat{y}$。第一项是外积 $\\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top$。第二项需要预测的海森矩阵，$H_{\\hat{y}} = \\nabla^2_{\\mathbf{z}} \\hat{y}$。\n$H_{\\hat{y}}$ 的 $(k,l)$ 项是 $\\frac{\\partial^2 \\hat{y}}{\\partial z_k \\partial z_l}$。让我们对 $\\mathbf{g}_{\\hat{y}}$ 的第 k 个分量关于 $z_l$ 进行微分：\n$$\n(H_{\\hat{y}})_{kl} = \\frac{\\partial}{\\partial z_l} \\left( \\frac{1}{\\tau} \\pi_k(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi}) \\right)\n$$\n再次使用乘法法则和链式法则：\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau} \\left[ \\frac{\\partial \\pi_k}{\\partial z_l}(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi}) - \\pi_k \\frac{\\partial (\\mathbf{v}^\\top \\boldsymbol{\\pi})}{\\partial z_l} \\right]\n$$\n我们知道 $\\frac{\\partial \\pi_k}{\\partial z_l} = \\frac{1}{\\tau}\\pi_k(\\delta_{kl}-\\pi_l)$。期望值 $\\bar{v} = \\mathbf{v}^\\top\\boldsymbol{\\pi}$ 的导数是：\n$$\n\\frac{\\partial (\\mathbf{v}^\\top \\boldsymbol{\\pi})}{\\partial z_l} = \\mathbf{v}^\\top \\frac{\\partial \\boldsymbol{\\pi}}{\\partial z_l} = \\sum_j v_j \\frac{\\partial \\pi_j}{\\partial z_l} = \\sum_j v_j \\frac{1}{\\tau} \\pi_j(\\delta_{jl}-\\pi_l) = \\frac{1}{\\tau} (v_l\\pi_l - \\pi_l \\sum_j v_j\\pi_j) = \\frac{1}{\\tau}\\pi_l(v_l - \\bar{v})\n$$\n将这些代入 $(H_{\\hat{y}})_{kl}$ 的表达式中：\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau} \\left[ \\frac{1}{\\tau}\\pi_k(\\delta_{kl}-\\pi_l)(v_k - \\bar{v}) - \\pi_k \\frac{1}{\\tau}\\pi_l(v_l - \\bar{v}) \\right]\n$$\n让我们定义一个差分向量 $\\mathbf{d} = \\mathbf{v} - \\bar{v}\\mathbf{1}$，所以 $d_k = v_k - \\bar{v}$。\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} \\left[ \\pi_k(\\delta_{kl}-\\pi_l) d_k - \\pi_k\\pi_l d_l \\right] = \\frac{1}{\\tau^2} \\left( \\pi_k d_k \\delta_{kl} - \\pi_k\\pi_l d_k - \\pi_k\\pi_l d_l \\right)\n$$\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} \\left( \\pi_k d_k \\delta_{kl} - \\pi_k\\pi_l (d_k + d_l) \\right)\n$$\n这个矩阵是对称的，正如预期。损失的完整海森矩阵是：\n$$\nH_L = \\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top + (\\hat{y} - y) H_{\\hat{y}}\n$$\n对称矩阵的谱范数是其最大绝对特征值：$\\left\\lVert H_L \\right\\rVert_2 = \\max_i |\\lambda_i(H_L)|$。\n\n### 步骤 4：实现算法\n对于每个测试用例 $(\\mathbf{z}_c, \\tau)$：\n1.  定义常量 $B, \\mathbf{w}, b_0, y$。设 $\\mathbf{z}_c$ 和 $\\tau$ 为当前用例的输入。设所有向量为列向量。\n2.  计算 $\\mathbf{v} = B \\mathbf{w}$。\n3.  计算缩放后的 logits $\\mathbf{p} = \\mathbf{z}_c / \\tau$。\n4.  计算 $\\boldsymbol{\\pi} = \\text{Softmax}(\\mathbf{p})$，使用数值稳定的实现（在求幂之前减去最大 logit）。\n5.  计算预测 $\\hat{y} = \\mathbf{v}^\\top \\boldsymbol{\\pi} + b_0$。\n6.  计算预测误差 $\\delta_L = \\hat{y} - y$。\n7.  计算预测的梯度 $\\mathbf{g}_{\\hat{y}} = \\frac{1}{\\tau}(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top) \\mathbf{v}$。一个更简单的方法是计算 $\\bar{v} = \\mathbf{v}^\\top \\boldsymbol{\\pi}$，然后将 $\\mathbf{g}_{\\hat{y}}$ 的分量计算为 $(\\mathbf{g}_{\\hat{y}})_k = \\frac{1}{\\tau} \\pi_k (v_k - \\bar{v})$。\n8.  计算损失梯度 $\\mathbf{g} = \\delta_L \\mathbf{g}_{\\hat{y}}$。\n9.  计算梯度范数 $\\left\\lVert \\mathbf{g} \\right\\rVert_2$。\n10. 计算预测的海森矩阵 $H_{\\hat{y}}$。首先计算 $\\bar{v}$ 和 $\\mathbf{d} = \\mathbf{v} - \\bar{v}\\mathbf{1}$。然后构造 $M \\times M$ 矩阵 $H_{\\hat{y}}$，其元素为 $(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} (\\pi_k d_k \\delta_{kl} - \\pi_k \\pi_l (d_k + d_l))$。\n11. 构造损失的海森矩阵 $H_L = \\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top + \\delta_L H_{\\hat{y}}$。\n12. 计算对称矩阵 $H_L$ 的特征值。\n13. 将海森矩阵的谱范数计算为最大绝对特征值。\n14. 存储当前测试用例的梯度范数和海森矩阵范数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing the gradient and Hessian norms for a\n    temperature-controlled categorical encoding model.\n    \"\"\"\n\n    # Define fixed parameters\n    M = 4\n    d = 3\n    B = np.array([\n        [0.9, -0.4, 0.1],\n        [0.3, 0.8, -0.5],\n        [-0.6, 0.2, 0.7],\n        [0.5, -0.1, -0.3]\n    ])\n    w = np.array([[0.7], [-1.1], [0.9]])\n    b0 = 0.2\n    y = 0.5\n\n    # Define the test suite\n    test_cases = [\n        # (z_c, tau)\n        (np.array([1.2, -0.5, 0.3, 2.0]), 1.0),    # Happy path\n        (np.array([1.2, -0.5, 0.3, 2.0]), 2.0),    # Smooth regime\n        (np.array([1.2, -0.5, 0.3, 2.0]), 0.5),    # Transition regime\n        (np.array([1.2, -0.5, 0.3, 2.0]), 0.1),    # Near-hard regime\n        (np.array([0.0, 0.0, 0.0, 0.0]), 1.0),    # Edge case (equal logits)\n        (np.array([0.0, 0.0, 0.0, 0.0]), 0.01),   # Edge case (equal logits, low temp)\n    ]\n\n    results = []\n\n    # Pre-compute v = Bw as it is constant\n    v = B @ w\n\n    for z_c_flat, tau in test_cases:\n        z_c = z_c_flat.reshape(-1, 1)\n\n        # 1. Compute softmax weights pi\n        p = z_c / tau\n        # Numerically stable softmax\n        p_stable = p - np.max(p)\n        exp_p = np.exp(p_stable)\n        pi = exp_p / np.sum(exp_p)\n\n        # 2. Compute prediction y_hat and error delta_L\n        # y_hat = w.T @ B.T @ pi + b0, which is v.T @ pi + b0\n        y_hat = v.T @ pi + b0\n        delta_L = (y_hat - y).item()\n        \n        # 3. Compute gradient of L w.r.t. z_c\n        v_bar = (v.T @ pi).item()\n        g_y_hat = (1.0 / tau) * pi * (v - v_bar)\n        g_L = delta_L * g_y_hat\n        grad_norm = np.linalg.norm(g_L)\n\n        # 4. Compute Hessian of L w.r.t. z_c\n        \n        # First term of H_L: g_y_hat @ g_y_hat.T\n        H_L_term1 = g_y_hat @ g_y_hat.T\n        \n        # Second term of H_L: delta_L * H_y_hat\n        d_vec = v - v_bar\n        \n        # Construct H_y_hat\n        # (H_y_hat)_kl = (1/tau^2) * (pi_k*d_k*delta_kl - pi_k*pi_l*(d_k + d_l))\n        # This can be vectorized as:\n        # Diagonal part: diag(pi * d / tau^2)\n        # Off-diagonal part: - (1/tau^2) * (pi @ pi.T) element-wise-mult (d_vec @ 1.T + 1 @ d_vec.T)\n        pi_d_diag = np.diag((pi * d_vec).flatten())\n        pi_outer = pi @ pi.T\n        d_sum_outer = d_vec @ np.ones((1, M)) + np.ones((M, 1)) @ d_vec.T\n        \n        H_y_hat = (1.0 / tau**2) * (pi_d_diag - pi_outer * d_sum_outer)\n\n        H_L = H_L_term1 + delta_L * H_y_hat\n        \n        # 5. Compute spectral norm of the Hessian\n        # For a symmetric matrix, this is the max absolute eigenvalue.\n        eigenvalues = np.linalg.eigvalsh(H_L)\n        hess_norm = np.max(np.abs(eigenvalues))\n\n        results.append([grad_norm, hess_norm])\n\n    # Format output as specified: [[g1,h1],[g2,h2],...]\n    formatted_results = [f\"[{g},{h}]\" for g, h in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3121698"}]}