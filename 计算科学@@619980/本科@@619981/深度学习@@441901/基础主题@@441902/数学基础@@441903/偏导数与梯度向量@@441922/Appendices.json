{"hands_on_practices": [{"introduction": "在深度学习中，损失函数的选择对模型训练至关重要。它不仅衡量模型预测的误差，更直接定义了梯度下降的路径。本练习通过一个包含异常值的简单回归问题，让你亲手计算并比较均方误差（$L_{\\mathrm{MSE}}$）和平均绝对误差（$L_{\\mathrm{MAE}}$）下的梯度。通过分析这两个梯度向量的差异 [@problem_id:3162520]，你将深刻理解为何 $L_{\\mathrm{MAE}}$ 在处理异常值时比 $L_{\\mathrm{MSE}}$ 更具鲁棒性。", "problem": "考虑一个用于回归的单个线性神经元，它有两个实值特征，由参数模型 $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$ 定义，其中 $\\mathbf{w} \\in \\mathbb{R}^{2}$ 且 $\\mathbf{x} \\in \\mathbb{R}^{2}$。给定一个包含 $3$ 个训练样本的小批量：\n- 样本 1：$\\mathbf{x}^{(1)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，目标值为 $y^{(1)} = 1$。\n- 样本 2：$\\mathbf{x}^{(2)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，目标值为 $y^{(2)} = 1$。\n- 样本 3（其目标值为一个离群点）：$\\mathbf{x}^{(3)} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，目标值为 $y^{(3)} = -10$。\n\n假设当前参数为 $\\mathbf{w} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$。将小批量上的均方误差 (MSE) 和平均绝对误差 (MAE) 定义为\n$$\nL_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2}, \n\\quad\nL_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\hat{y}^{(i)} - y^{(i)}\\right|.\n$$\n仅使用偏导数和梯度向量的基本定义，计算在给定的 $\\mathbf{w}$ 处、每种损失函数下关于 $\\mathbf{w}$ 的批量梯度，然后计算这两个梯度向量之间的夹角。将最终答案表示为夹角的单个精确封闭形式表达式，以弧度为单位（不要进行近似）。以弧度为单位给出最终答案（不要转换为角度，也不要进行数值四舍五入）。", "solution": "首先根据所需程序对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **模型**：一个具有两个实值特征的单个线性神经元，$\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$，其中 $\\mathbf{w} \\in \\mathbb{R}^{2}$ 且 $\\mathbf{x} \\in \\mathbb{R}^{2}$。\n- **训练数据（大小为 3 的小批量）**：\n  - 样本 1：$\\mathbf{x}^{(1)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，$y^{(1)} = 1$。\n  - 样本 2：$\\mathbf{x}^{(2)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，$y^{(2)} = 1$。\n  - 样本 3：$\\mathbf{x}^{(3)} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$y^{(3)} = -10$。\n- **当前参数**：$\\mathbf{w} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$。\n- **损失函数**：\n  - 均方误差 (MSE)：$L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2}$。\n  - 平均绝对误差 (MAE)：$L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\hat{y}^{(i)} - y^{(i)}\\right|$。\n- **任务**：\n  1. 计算在给定的 $\\mathbf{w}$ 处，$L_{\\mathrm{MSE}}$ 关于 $\\mathbf{w}$ 的批量梯度。\n  2. 计算在给定的 $\\mathbf{w}$ 处，$L_{\\mathrm{MAE}}$ 关于 $\\mathbf{w}$ 的批量梯度。\n  3. 计算这两个梯度向量之间的夹角（以弧度为单位）。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学依据**：该问题是机器学习和优化领域的标准练习。它涉及线性模型、损失函数（MSE、MAE）和梯度计算等基本概念，这些是训练神经网络的核心。其设置在科学和数学上都是合理的。\n- **适定性**：该问题提供了计算唯一解所需的所有数据和定义。模型、数据、初始参数和损失函数都已明确定义。MAE 损失函数的梯度在指定点是良定义的，因为绝对值函数的参数为非零值，从而保证了可微性。\n- **客观性**：该问题使用精确的数学语言陈述，没有任何歧义、主观看法或意见。\n\n该问题是自洽的、一致的，并遵循既定的科学原则。它没有违反任何无效性标准。\n\n### 步骤 3：结论与行动\n问题是**有效的**。将提供完整的解答。\n\n### 解\n\n模型是一个线性神经元，因此对于输入向量 $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 和权重 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$，其预测值 $\\hat{y}$ 由 $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x} = w_1 x_1 + w_2 x_2$ 给出。\n\n任务要求计算在特定权重向量 $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处，MSE 和 MAE 损失函数的梯度。\n\n首先，我们计算在 $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 时，模型对小批量中每个样本的预测值：\n$\\hat{y}^{(i)} = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)} = \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\end{pmatrix} = 0$，对于所有 $i \\in \\{1, 2, 3\\}$。\n\n接下来，我们计算每个样本的误差 $(\\hat{y}^{(i)} - y^{(i)})$：\n- 样本 1 的误差：$e_1 = \\hat{y}^{(1)} - y^{(1)} = 0 - 1 = -1$。\n- 样本 2 的误差：$e_2 = \\hat{y}^{(2)} - y^{(2)} = 0 - 1 = -1$。\n- 样本 3 的误差：$e_3 = \\hat{y}^{(3)} - y^{(3)} = 0 - (-10) = 10$。\n\n**1. 均方误差 (MSE) 损失的梯度**\n\nMSE 损失为 $L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} - y^{(i)}\\right)^{2}$。\n$L_{\\mathrm{MSE}}$ 关于权重向量 $\\mathbf{w}$ 的梯度由下式给出：\n$$ \\nabla_{\\mathbf{w}} L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w}} \\left( \\frac{1}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^{2} \\right) = \\frac{1}{3}\\sum_{i=1}^{3} 2\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} $$\n由于 $\\hat{y}^{(i)} = \\mathbf{w}^{\\top}\\mathbf{x}^{(i)}$，其关于 $\\mathbf{w}$ 的梯度为 $\\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} = \\mathbf{x}^{(i)}$。\n因此，MSE 损失的梯度为：\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\nabla_{\\mathbf{w}} L_{\\mathrm{MSE}}(\\mathbf{w}) = \\frac{2}{3}\\sum_{i=1}^{3}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)\\mathbf{x}^{(i)} $$\n使用预先计算的误差，在 $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处计算该梯度：\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( (-1)\\mathbf{x}^{(1)} + (-1)\\mathbf{x}^{(2)} + (10)\\mathbf{x}^{(3)} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (10)\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MSE}} = \\frac{2}{3} \\left( \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}10 \\\\ 0\\end{pmatrix} \\right) = \\frac{2}{3} \\begin{pmatrix}10 \\\\ -2\\end{pmatrix} = \\begin{pmatrix} \\frac{20}{3} \\\\ -\\frac{4}{3} \\end{pmatrix} $$\n\n**2. 平均绝对误差 (MAE) 损失的梯度**\n\nMAE 损失为 $L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3}\\left|\\mathbf{w}^{\\top}\\mathbf{x}^{(i)} - y^{(i)}\\right|$。\n绝对值函数 $|u|$ 的导数是符号函数 $\\text{sgn}(u)$，它在 $u \\neq 0$ 时有定义。由于所有误差 $e_i$ 都是非零的（$-1, -1, 10$），因此 $L_{\\mathrm{MAE}}$ 的梯度在这一点上是良定义的。\n$L_{\\mathrm{MAE}}$ 关于 $\\mathbf{w}$ 的梯度为：\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\nabla_{\\mathbf{w}} L_{\\mathrm{MAE}}(\\mathbf{w}) = \\frac{1}{3}\\sum_{i=1}^{3} \\text{sgn}\\left(\\hat{y}^{(i)} - y^{(i)}\\right) \\frac{\\partial \\hat{y}^{(i)}}{\\partial \\mathbf{w}} = \\frac{1}{3}\\sum_{i=1}^{3} \\text{sgn}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)\\mathbf{x}^{(i)} $$\n我们需要误差的符号：\n- $\\text{sgn}(e_1) = \\text{sgn}(-1) = -1$。\n- $\\text{sgn}(e_2) = \\text{sgn}(-1) = -1$。\n- $\\text{sgn}(e_3) = \\text{sgn}(10) = 1$。\n在 $\\mathbf{w} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处计算梯度：\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( (-1)\\mathbf{x}^{(1)} + (-1)\\mathbf{x}^{(2)} + (1)\\mathbf{x}^{(3)} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (-1)\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + (1)\\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) $$\n$$ \\mathbf{g}_{\\mathrm{MAE}} = \\frac{1}{3} \\left( \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -1\\end{pmatrix} + \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{2}{3} \\end{pmatrix} $$\n\n**3. 梯度向量之间的夹角**\n\n设 $\\theta$ 为两个梯度向量 $\\mathbf{g}_{\\mathrm{MSE}}$ 和 $\\mathbf{g}_{\\mathrm{MAE}}$ 之间的夹角。该夹角由以下公式给出：\n$$ \\theta = \\arccos\\left(\\frac{\\mathbf{g}_{\\mathrm{MSE}} \\cdot \\mathbf{g}_{\\mathrm{MAE}}}{\\|\\mathbf{g}_{\\mathrm{MSE}}\\| \\|\\mathbf{g}_{\\mathrm{MAE}}\\|}\\right) $$\n我们计算此公式的各个组成部分：\n- **点积**：\n$$ \\mathbf{g}_{\\mathrm{MSE}} \\cdot \\mathbf{g}_{\\mathrm{MAE}} = \\begin{pmatrix} \\frac{20}{3} \\\\ -\\frac{4}{3} \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{2}{3} \\end{pmatrix} = \\left(\\frac{20}{3}\\right)\\left(\\frac{1}{3}\\right) + \\left(-\\frac{4}{3}\\right)\\left(-\\frac{2}{3}\\right) = \\frac{20}{9} + \\frac{8}{9} = \\frac{28}{9} $$\n- **模长**：\n$$ \\|\\mathbf{g}_{\\mathrm{MSE}}\\| = \\sqrt{\\left(\\frac{20}{3}\\right)^2 + \\left(-\\frac{4}{3}\\right)^2} = \\sqrt{\\frac{400}{9} + \\frac{16}{9}} = \\sqrt{\\frac{416}{9}} = \\frac{\\sqrt{16 \\times 26}}{3} = \\frac{4\\sqrt{26}}{3} $$\n$$ \\|\\mathbf{g}_{\\mathrm{MAE}}\\| = \\sqrt{\\left(\\frac{1}{3}\\right)^2 + \\left(-\\frac{2}{3}\\right)^2} = \\sqrt{\\frac{1}{9} + \\frac{4}{9}} = \\sqrt{\\frac{5}{9}} = \\frac{\\sqrt{5}}{3} $$\n- **模长之积**：\n$$ \\|\\mathbf{g}_{\\mathrm{MSE}}\\| \\|\\mathbf{g}_{\\mathrm{MAE}}\\| = \\left(\\frac{4\\sqrt{26}}{3}\\right) \\left(\\frac{\\sqrt{5}}{3}\\right) = \\frac{4\\sqrt{26 \\times 5}}{9} = \\frac{4\\sqrt{130}}{9} $$\n- **夹角的余弦**：\n$$ \\cos(\\theta) = \\frac{\\frac{28}{9}}{\\frac{4\\sqrt{130}}{9}} = \\frac{28}{4\\sqrt{130}} = \\frac{7}{\\sqrt{130}} $$\n- **夹角 $\\theta$**：\n以弧度表示的夹角为：\n$$ \\theta = \\arccos\\left(\\frac{7}{\\sqrt{130}}\\right) $$\n这就是夹角的最终精确封闭形式表达式。", "answer": "$$\\boxed{\\arccos\\left(\\frac{7}{\\sqrt{130}}\\right)}$$", "id": "3162520"}, {"introduction": "批标准化（Batch Normalization）是现代深度网络中的一个关键组件，但其梯度计算比看起来要复杂得多。由于其统计量（均值和方差）是在整个小批量上计算的，因此单个样本的梯度会受到批次中所有其他样本的影响。在本练习中，你将从头推导批标准化的反向传播公式 [@problem_id:3162556]，揭示这种“非局部”的梯度依赖关系，并理解一个样本如何通过批次统计量影响整个梯度计算。", "problem": "考虑一个标量特征通道，在训练模式下通过批量归一化 (BN) 处理一个大小为 $m$ 的小批量数据。其前向传播定义为：批量均值 $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_i$，批量方差 $\\sigma^{2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^{2}$，标准差 $\\sigma = \\sqrt{\\sigma^{2} + \\varepsilon}$，归一化激活值 $\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma}$，以及 BN 输出 $y_i = \\gamma \\hat{x}_i + \\beta$。一个标量损失应用于此 BN 层，其上游灵敏度为 $g_i = \\frac{\\partial L}{\\partial y_i}$。\n\n仅从这些前向传播定义和微积分的链式法则出发，推导偏导数 $\\frac{\\partial L}{\\partial x_i}$ 的通用表达式，并明确说明 $\\mu$ 和 $\\sigma$ 对小批量中所有样本的依赖性如何通过梯度产生非局部项，从而将所有 $\\{x_j\\}_{j=1}^{m}$ 耦合起来。然后，使用您推导的表达式，对以下科学上合理且表现出单个样本通过批量统计量主导梯度贡献的配置，计算 $\\frac{\\partial L}{\\partial x_1}$ 的值：\n\n- 小批量大小 $m = 3$。\n- 输入 $(x_1, x_2, x_3) = (5, 0, 1)$。\n- BN 参数 $\\gamma = 1$，$\\beta = 0$，以及 $\\varepsilon = 0$。\n- 线性损失 $L = \\sum_{i=1}^{m} c_i y_i$，系数为 $(c_1, c_2, c_3) = (1, 0, 0)$，这意味着上游灵敏度为 $g_i = c_i$。\n\n您的最终答案必须是此配置下 $\\frac{\\partial L}{\\partial x_1}$ 的单一实数值。将您的答案四舍五入到四位有效数字。", "solution": "该问题陈述经过严格验证，被认为是有效的。它科学地基于深度学习的既定原则，特别是批量归一化算法。问题定义明确，所有必要的参数和函数都得到了无歧义的定义，提供了一个自洽且一致的设定，从而能够得到唯一且有意义的解。该问题要求推导和应用偏导数，这是指定主题的核心。\n\n我们首先推导损失 $L$ 相对于小批量输入 $x_i$ 的偏导数的通用表达式。损失 $L$ 是批量归一化 (BN) 输出 $\\{y_j\\}_{j=1}^m$ 的函数。$L$ 对特定输入 $x_i$ 的依赖关系通过链式法则建立：\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}\n$$\n给定上游灵敏度 $g_j = \\frac{\\partial L}{\\partial y_j}$，上式变为：\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} g_j \\frac{\\partial y_j}{\\partial x_i}\n$$\nBN 输出 $y_j$ 定义为 $y_j = \\gamma \\hat{x}_j + \\beta$，其中 $\\hat{x}_j = \\frac{x_j - \\mu}{\\sigma}$。参数 $\\gamma$ 和 $\\beta$ 与输入 $\\{x_k\\}$ 无关。因此，我们必须计算 $\\frac{\\partial y_j}{\\partial x_i}$：\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\gamma \\frac{x_j - \\mu}{\\sigma} + \\beta \\right) = \\gamma \\frac{\\partial}{\\partial x_i} \\left( \\frac{x_j - \\mu}{\\sigma} \\right)\n$$\n使用商法则，其中分子 $N_j = x_j - \\mu$ 和分母 $D = \\sigma$ 都依赖于 $x_i$：\n$$\n\\frac{\\partial}{\\partial x_i} \\left( \\frac{N_j}{D} \\right) = \\frac{D \\frac{\\partial N_j}{\\partial x_i} - N_j \\frac{\\partial D}{\\partial x_i}}{D^2} = \\frac{1}{\\sigma} \\frac{\\partial (x_j - \\mu)}{\\partial x_i} - \\frac{x_j - \\mu}{\\sigma^2} \\frac{\\partial \\sigma}{\\partial x_i}\n$$\n我们必须求出批量统计量 $\\mu$ 和 $\\sigma$ 相对于 $x_i$ 的偏导数。\n批量均值为 $\\mu = \\frac{1}{m} \\sum_{k=1}^{m} x_k$。其偏导数为：\n$$\n\\frac{\\partial \\mu}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\frac{1}{m} \\sum_{k=1}^{m} x_k \\right) = \\frac{1}{m}\n$$\n分子项 $N_j = x_j - \\mu$ 的导数为：\n$$\n\\frac{\\partial (x_j - \\mu)}{\\partial x_i} = \\frac{\\partial x_j}{\\partial x_i} - \\frac{\\partial \\mu}{\\partial x_i} = \\delta_{ij} - \\frac{1}{m}\n$$\n其中 $\\delta_{ij}$ 是克罗内克 δ (Kronecker delta)。\n\n接下来，我们处理标准差 $\\sigma = \\sqrt{\\sigma^2 + \\varepsilon}$ 的导数：\n$$\n\\frac{\\partial \\sigma}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\sqrt{\\sigma^2 + \\varepsilon} = \\frac{1}{2\\sqrt{\\sigma^2 + \\varepsilon}} \\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{1}{2\\sigma} \\frac{\\partial \\sigma^2}{\\partial x_i}\n$$\n批量方差为 $\\sigma^2 = \\frac{1}{m} \\sum_{k=1}^{m} (x_k - \\mu)^2$。其偏导数为：\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{1}{m} \\sum_{k=1}^{m} \\frac{\\partial}{\\partial x_i} (x_k - \\mu)^2 = \\frac{1}{m} \\sum_{k=1}^{m} 2(x_k - \\mu) \\frac{\\partial (x_k - \\mu)}{\\partial x_i}\n$$\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{2}{m} \\sum_{k=1}^{m} (x_k - \\mu) (\\delta_{ki} - \\frac{1}{m}) = \\frac{2}{m} \\left( (x_i - \\mu) - \\frac{1}{m} \\sum_{k=1}^{m} (x_k - \\mu) \\right)\n$$\n根据均值的定义，$\\sum_{k=1}^{m} (x_k - \\mu) = 0$。因此：\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_i} = \\frac{2}{m} (x_i - \\mu)\n$$\n将此结果代回到 $\\sigma$ 的导数中：\n$$\n\\frac{\\partial \\sigma}{\\partial x_i} = \\frac{1}{2\\sigma} \\left( \\frac{2}{m} (x_i - \\mu) \\right) = \\frac{x_i - \\mu}{m\\sigma}\n$$\n现在我们可以组合出 $\\frac{\\partial y_j}{\\partial x_i}$ 的表达式：\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\gamma \\left( \\frac{1}{\\sigma}(\\delta_{ij} - \\frac{1}{m}) - \\frac{x_j - \\mu}{\\sigma^2} \\frac{x_i - \\mu}{m\\sigma} \\right)\n$$\n使用归一化激活值的定义 $\\hat{x}_k = \\frac{x_k - \\mu}{\\sigma}$，我们可以简化这个表达式：\n$$\n\\frac{\\partial y_j}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( (\\delta_{ij} - \\frac{1}{m}) - \\frac{\\hat{x}_j \\hat{x}_i}{m} \\right) = \\frac{\\gamma}{\\sigma} \\left( \\delta_{ij} - \\frac{1}{m}(1 + \\hat{x}_i \\hat{x}_j) \\right)\n$$\n最后，我们将此代入 $\\frac{\\partial L}{\\partial x_i}$ 的求和式中：\n$$\n\\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^{m} g_j \\left[ \\frac{\\gamma}{\\sigma} \\left( \\delta_{ij} - \\frac{1}{m} - \\frac{\\hat{x}_i \\hat{x}_j}{m} \\right) \\right]\n$$\n$$\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( \\sum_{j=1}^{m} g_j \\delta_{ij} - \\frac{1}{m} \\sum_{j=1}^{m} g_j - \\frac{\\hat{x}_i}{m} \\sum_{j=1}^{m} g_j \\hat{x}_j \\right)\n$$\n这简化为关于输入特征 $x_i$ 的梯度的通用表达式：\n$$\n\\frac{\\partial L}{\\partial x_i} = \\frac{\\gamma}{\\sigma} \\left( g_i - \\frac{1}{m} \\sum_{j=1}^{m} g_j - \\frac{\\hat{x}_i}{m} \\sum_{j=1}^{m} g_j \\hat{x}_j \\right)\n$$\n这个表达式展示了非局部耦合：梯度 $\\frac{\\partial L}{\\partial x_i}$ 不仅依赖于局部上游梯度 $g_i$ 和局部激活值 $\\hat{x}_i$，还依赖于整个小批量的平均项（$\\sum g_j$ 和 $\\sum g_j \\hat{x}_j$）。\n\n现在，我们针对给定的特定配置计算这个表达式的值。\n给定条件如下：\n- 小批量大小：$m = 3$\n- 输入：$(x_1, x_2, x_3) = (5, 0, 1)$\n- BN 参数：$\\gamma = 1$，$\\beta = 0$，$\\varepsilon = 0$\n- 上游灵敏度：$(g_1, g_2, g_3) = (1, 0, 0)$\n\n首先，我们计算批量统计量：\n1.  批量均值：$\\mu = \\frac{1}{3}(5 + 0 + 1) = \\frac{6}{3} = 2$。\n2.  中心化输入：$(x_1 - \\mu, x_2 - \\mu, x_3 - \\mu) = (5-2, 0-2, 1-2) = (3, -2, -1)$。\n3.  批量方差：$\\sigma^2 = \\frac{1}{3}((3)^2 + (-2)^2 + (-1)^2) = \\frac{1}{3}(9 + 4 + 1) = \\frac{14}{3}$。\n4.  标准差：$\\sigma = \\sqrt{\\sigma^2 + \\varepsilon} = \\sqrt{\\frac{14}{3} + 0} = \\sqrt{\\frac{14}{3}}$。\n\n接下来，我们计算梯度公式中 $i=1$ 所需的项：\n- $\\sum_{j=1}^{3} g_j = g_1 + g_2 + g_3 = 1 + 0 + 0 = 1$。\n- 我们需要归一化激活值 $\\hat{x}_1$：$\\hat{x}_1 = \\frac{x_1 - \\mu}{\\sigma} = \\frac{3}{\\sqrt{14/3}}$。\n- 我们需要求和项 $\\sum_{j=1}^{3} g_j \\hat{x}_j = g_1\\hat{x}_1 + g_2\\hat{x}_2 + g_3\\hat{x}_3 = (1)\\hat{x}_1 + (0)\\hat{x}_2 + (0)\\hat{x}_3 = \\hat{x}_1$。\n\n将这些代入 $\\frac{\\partial L}{\\partial x_1}$ 的通用公式中：\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{\\gamma}{\\sigma} \\left( g_1 - \\frac{1}{m} \\sum_{j=1}^{3} g_j - \\frac{\\hat{x}_1}{m} \\sum_{j=1}^{3} g_j \\hat{x}_j \\right)\n$$\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{1}{\\sqrt{14/3}} \\left( 1 - \\frac{1}{3}(1) - \\frac{\\hat{x}_1}{3}(\\hat{x}_1) \\right) = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{\\hat{x}_1^2}{3} \\right)\n$$\n我们计算 $\\hat{x}_1^2$：\n$$\n\\hat{x}_1^2 = \\left( \\frac{3}{\\sqrt{14/3}} \\right)^2 = \\frac{9}{14/3} = \\frac{27}{14}\n$$\n现在将这个值代回：\n$$\n\\frac{\\partial L}{\\partial x_1} = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{1}{3} \\cdot \\frac{27}{14} \\right) = \\sqrt{\\frac{3}{14}} \\left( \\frac{2}{3} - \\frac{9}{14} \\right)\n$$\n为了简化括号中的项，我们找到公分母 $42$：\n$$\n\\frac{2}{3} - \\frac{9}{14} = \\frac{2 \\cdot 14}{42} - \\frac{9 \\cdot 3}{42} = \\frac{28 - 27}{42} = \\frac{1}{42}\n$$\n因此，偏导数的精确值为：\n$$\n\\frac{\\partial L}{\\partial x_1} = \\sqrt{\\frac{3}{14}} \\cdot \\frac{1}{42} = \\frac{\\sqrt{3}}{42\\sqrt{14}} = \\frac{\\sqrt{3}\\sqrt{14}}{42 \\cdot 14} = \\frac{\\sqrt{42}}{588}\n$$\n最后，我们计算数值并四舍五入到四位有效数字：\n$$\n\\frac{\\partial L}{\\partial x_1} = \\frac{\\sqrt{42}}{588} \\approx \\frac{6.4807407}{588} \\approx 0.01102166785\n$$\n四舍五入到四位有效数字得到 $0.01102$。", "answer": "$$\\boxed{0.01102}$$", "id": "3162556"}, {"introduction": "梯度下降是基于可微函数设计的，但如果模型中包含不可微的操作（如取整）该怎么办？本练习将探讨一种在模型量化等领域广泛使用的技术——直通估计器（Straight-Through Estimator, STE）。你将为一个包含取整函数的简单模型计算梯度，并精确量化 STE 引入的偏差 [@problem_id:3162528]。这个过程将帮助你理解在实践中如何近似梯度，以及这种近似所带来的理论代价。", "problem": "考虑一个单参数线性预测器，其量化前参数为 $\\tilde{w} \\in \\mathbb{R}$，量化后参数为 $w \\in \\mathbb{Z}$，由 $w=\\mathrm{round}(\\tilde{w})$ 给出，其中 $\\mathrm{round}(\\cdot)$ 表示四舍五入到最近的整数。对于输入 $x$，模型输出为 $f(x)=w\\,x$。您使用平方误差损失在一个单个确定性训练对 $(x,y)$（其中 $x=\\frac{3}{2}$，$y=\\frac{5}{4}$）上进行训练\n$$\nL(\\tilde{w})=\\big(w\\,x-y\\big)^{2}.\n$$\n在反向传播过程中，采用直通估计器 (STE)，即在需要时将雅可比矩阵 $\\frac{\\partial w}{\\partial \\tilde{w}}$ 近似为 $1$。\n\n在开区间 $\\big(\\frac{1}{2},\\frac{3}{2}\\big)$ 内的任意 $\\tilde{w}$（其中 $w$ 是常数）下进行计算。仅从偏导数的定义、链式法则和平方误差损失的定义出发，完成以下任务：\n\n- 计算在此类 $\\tilde{w}$ 处基于 STE 的偏导数 $\\frac{\\partial L}{\\partial \\tilde{w}}$。\n- 使用精确的量化目标 $L(\\tilde{w})=\\big(\\mathrm{round}(\\tilde{w})\\,x-y\\big)^{2}$，确定在此类 $\\tilde{w}$ 处的真实偏导数 $\\frac{\\partial L}{\\partial \\tilde{w}}$。\n- 将此类 $\\tilde{w}$ 处 STE 梯度的偏差定义为 $B=\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}-\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$。精确计算 $B$。\n\n您的最终答案必须是 $B$ 的一个精确数字，无需四舍五入。", "solution": "问题要求计算直通估计器 (STE) 对损失函数关于量化前参数的梯度的偏差。我们给定的损失函数是 $L(\\tilde{w})=\\big(w\\,x-y\\big)^{2}$，其中 $w = \\mathrm{round}(\\tilde{w})$。偏差定义为 $B=\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}-\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$。我们必须在 $\\tilde{w} \\in \\big(\\frac{1}{2},\\frac{3}{2}\\big)$ 的条件下，使用特定训练数据 $x=\\frac{3}{2}$ 和 $y=\\frac{5}{4}$ 来计算这个值。\n\n首先，我们计算基于 STE 的偏导数 $\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}}$。\n损失函数 $L$ 是关于 $\\tilde{w}$ 的复合函数。我们应用链式法则来求其关于 $\\tilde{w}$ 的导数：\n$$\n\\frac{\\partial L}{\\partial \\tilde{w}} = \\frac{\\partial L}{\\partial w} \\frac{\\partial w}{\\partial \\tilde{w}}\n$$\n让我们计算 $L$ 关于 $w$ 的偏导数：\n$$\n\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\big((w\\,x-y)^{2}\\big) = 2(w\\,x-y) \\cdot \\frac{\\partial}{\\partial w}(w\\,x-y) = 2(w\\,x-y)x\n$$\n将此代入链式法则表达式中，得到：\n$$\n\\frac{\\partial L}{\\partial \\tilde{w}} = 2x(w\\,x-y) \\frac{\\partial w}{\\partial \\tilde{w}}\n$$\n直通估计器 (STE) 将量化函数的导数 $\\frac{\\partial w}{\\partial \\tilde{w}}$ 近似为 $1$。\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} = 2x(w\\,x-y) \\cdot 1 = 2x(w\\,x-y)\n$$\n问题指定我们在开区间 $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$（即区间 $(0.5, 1.5)$）内的任意 $\\tilde{w}$ 处计算此导数。对于此区间内的任何 $\\tilde{w}$ 值，`round` 函数都会得出整数 $1$。因此，$w = \\mathrm{round}(\\tilde{w}) = 1$。\n现在我们代入给定的值 $x=\\frac{3}{2}$、$y=\\frac{5}{4}$ 以及 $w=1$：\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} = 2\\left(\\frac{3}{2}\\right)\\left( (1)\\left(\\frac{3}{2}\\right) - \\frac{5}{4} \\right) = 3\\left(\\frac{6}{4} - \\frac{5}{4}\\right) = 3\\left(\\frac{1}{4}\\right) = \\frac{3}{4}\n$$\n所以，基于 STE 的梯度是 $\\frac{3}{4}$。\n\n接下来，我们计算真实的偏导数 $\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}$。\n损失函数为 $L(\\tilde{w})=\\big(\\mathrm{round}(\\tilde{w})\\,x-y\\big)^{2}$。\n如前所述，对于开区间 $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$ 内的任何 $\\tilde{w}$，$\\mathrm{round}(\\tilde{w})$ 的值是常数，等于 $1$。\n因此，对于 $\\tilde{w} \\in \\big(\\frac{1}{2}, \\frac{3}{2}\\big)$，损失函数简化为：\n$$\nL(\\tilde{w}) = \\big((1)x-y\\big)^{2}\n$$\n代入常数值 $x=\\frac{3}{2}$ 和 $y=\\frac{5}{4}$：\n$$\nL(\\tilde{w}) = \\left(\\frac{3}{2} - \\frac{5}{4}\\right)^{2} = \\left(\\frac{6}{4} - \\frac{5}{4}\\right)^{2} = \\left(\\frac{1}{4}\\right)^{2} = \\frac{1}{16}\n$$\n在指定区间内，$L(\\tilde{w})$ 是一个关于 $\\tilde{w}$ 的常数函数。任何常数函数的导数都为 $0$。\n因此，真实的偏导数是：\n$$\n\\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}} = \\frac{\\partial}{\\partial \\tilde{w}}\\left(\\frac{1}{16}\\right) = 0\n$$\n这与 `round` 函数的导数处处为 $0$（除了在不连续点，即对于任何整数 $n$，在 $\\tilde{w}=n+0.5$ 的值处）的事实相符，而这些点不包含在开区间 $\\big(\\frac{1}{2}, \\frac{3}{2}\\big)$ 内。\n\n最后，我们计算偏差 $B$。\n偏差定义为基于 STE 的梯度与真实梯度之差：\n$$\nB = \\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{STE}} - \\Big(\\frac{\\partial L}{\\partial \\tilde{w}}\\Big)_{\\mathrm{true}}\n$$\n代入我们计算出的值：\n$$\nB = \\frac{3}{4} - 0 = \\frac{3}{4}\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3162528"}]}