{"hands_on_practices": [{"introduction": "在深度学习中，统计学的一个关键应用是量化模型的不确定性。本练习将介绍蒙特卡洛丢弃 (Monte Carlo dropout)，这是一种估计神经网络预测均值和方差的实用技术。通过观察方差如何随丢弃率 $p$ 变化，我们能具体地理解认知不确定性 (epistemic uncertainty)，即模型对其自身预测的不确定性。[@problem_id:3123387]", "problem": "给定一个带有修正线性单元 (ReLU) 激活函数和隐藏层 dropout 的单隐藏层前馈神经网络。该网络将一个二维输入向量 $x \\in \\mathbb{R}^2$ 映射到一个标量输出。网络参数是固定且已知的。您必须使用蒙特卡洛 dropout 来近似网络输出 $f(x)$ 在 dropout 引入的随机性下的预测均值 $\\mathbb{E}[f(x)]$ 和预测方差 $\\operatorname{Var}[f(x)]$，然后通过检查不同测试用例的方差值，阐述认知不确定性 (epistemic uncertainty) 如何随 dropout 率 $p$ 变化。\n\n推导算法的基础理论：\n- 实值随机变量 $Z$ 的期望是 $\\mathbb{E}[Z]$。\n- 实值随机变量 $Z$ 的方差是 $\\operatorname{Var}(Z) = \\mathbb{E}\\left[(Z - \\mathbb{E}[Z])^2\\right]$。\n- 蒙特卡洛估计使用独立样本 $Z_1, Z_2, \\dots, Z_T$ 通过样本均值来近似 $\\mathbb{E}[Z]$，通过样本方差来近似 $\\operatorname{Var}(Z)$。\n\n网络定义：\n- 大小为 $H=3$ 的隐藏层，其参数为\n$$\n\\mathbf{W}_1 =\n\\begin{bmatrix}\n1.0  -0.5 \\\\\n0.3  0.8 \\\\\n-0.7  0.2\n\\end{bmatrix}, \\quad\n\\mathbf{b}_1 =\n\\begin{bmatrix}\n0.1 \\\\\n-0.2 \\\\\n0.0\n\\end{bmatrix}.\n$$\n- 输出层参数\n$$\n\\mathbf{W}_2 =\n\\begin{bmatrix}\n0.5 \\\\\n-1.0 \\\\\n0.3\n\\end{bmatrix}, \\quad\nb_2 = 0.05.\n$$\n- ReLU 激活函数定义为 $\\operatorname{ReLU}(z) = \\max(z, 0)$，逐元素应用。\n- 对激活后的隐藏向量 $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$ 应用 dropout 率为 $p \\in [0,1)$ 的隐藏层 dropout。使用倒置 dropout (inverted dropout)，经过 dropout 处理的隐藏向量为\n$$\n\\tilde{h} = \\begin{cases}\nh,  & \\text{if } p = 0, \\\\\n\\frac{m \\odot h}{1-p},  & \\text{if } p > 0,\n\\end{cases}\n$$\n其中 $m \\in \\{0,1\\}^H$ 是一个随机掩码 (mask)，其条目 $m_i \\sim \\operatorname{Bernoulli}(1-p)$ 独立同分布，$\\odot$ 表示逐元素乘法。网络输出为\n$$\nf(x) = \\mathbf{W}_2^\\top \\tilde{h} + b_2.\n$$\n\n蒙特卡洛 dropout 设置：\n- 对于给定的输入 $x$、dropout 率 $p$ 和蒙特卡洛样本数量 $T \\in \\mathbb{N}$，通过独立抽取掩码 $m$ 并计算 $f(x)$ 来生成 $T$ 个 $f(x)$ 的独立样本。\n- 通过 $T$ 个输出的样本均值来近似 $\\mathbb{E}[f(x)]$。\n- 通过 $T$ 个输出的样本方差来近似 $\\operatorname{Var}[f(x)]$。\n\n任务：\n- 实现一个程序，为指定的测试套件执行上述蒙特卡洛过程。\n- 您的程序不得训练网络；必须使用上面给出的固定参数。\n- 您的输出必须是浮点数。此问题不涉及任何物理单位。\n\n测试套件：\n为以下 $(x, p, T)$ 三元组计算预测均值和方差。每个 $x$ 都以二维向量的形式给出。请使用所示的精确数值。\n\n1. $x = \\begin{bmatrix} 0.5 \\\\ -1.0 \\end{bmatrix}$, $p = 0.0$, $T = 100$.\n2. $x = \\begin{bmatrix} 0.5 \\\\ -1.0 \\end{bmatrix}$, $p = 0.2$, $T = 1000$.\n3. $x = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}$, $p = 0.5$, $T = 1000$.\n4. $x = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}$, $p = 0.9$, $T = 2000$.\n5. $x = \\begin{bmatrix} -1.5 \\\\ 0.3 \\end{bmatrix}$, $p = 0.5$, $T = 25$.\n\n要求的最终输出格式：\n- 您的程序应生成一行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。\n- 每个测试用例的结果本身必须是一个包含两个浮点数的列表，顺序为 $[\\text{均值}, \\text{方差}]$，分别对应于该用例的 $\\mathbb{E}[f(x)]$ 和 $\\operatorname{Var}[f(x)]$。\n- 例如，$K$ 个测试用例的有效输出格式为\n$$\n\\text{[}[m_1,v_1],[m_2,v_2],\\dots,[m_K,v_K]\\text{]}.\n$$\n\n覆盖设计：\n- 用例 1 是一个边界条件，其中 $p = 0.0$（无 dropout），此时应表现出零方差。\n- 用例 2 和 3 是具有中等 dropout 率的一般情况。\n- 用例 4 探讨了 $p = 0.9$ 的高 dropout 率情况，以说明认知不确定性的增加。\n- 用例 5 使用了少量样本 $T = 25$，以说明蒙特卡洛估计中的抽样可变性。\n\n您的实现必须通过为蒙特卡洛抽样使用固定的随机种子来确保每次运行结果的确定性。", "solution": "我们从概率论的核心定义开始。对于一个实值随机变量 $Z$，其期望为 $\\mathbb{E}[Z]$，方差为 $\\operatorname{Var}(Z) = \\mathbb{E}\\left[(Z - \\mathbb{E}[Z])^2\\right]$。当 $Z$ 的精确分布难以处理时，蒙特卡洛方法使用独立样本来近似这些量。给定独立样本 $Z_1, Z_2, \\dots, Z_T$，样本均值 $\\hat{\\mu}$ 和样本方差 $\\hat{\\sigma}^2$ 提供了相合估计量：\n$$\n\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T Z_t, \\quad\n\\hat{\\sigma}^2 \\approx \\frac{1}{T-1} \\sum_{t=1}^T (Z_t - \\hat{\\mu})^2.\n$$\n无偏样本方差使用分母 $T-1$，并在 $T \\to \\infty$ 时收敛于 $\\operatorname{Var}(Z)$。\n\n在深度学习中，dropout 通过在训练期间随机将隐藏单元置零来引入随机性。蒙特卡洛 dropout 在推理时应用同样的随机性来近似贝叶斯预测，将输出的可变性归因于认知不确定性（由于数据有限或参数不确定性而产生的模型不确定性）。倒置 dropout (Inverted dropout) 通过 $\\frac{1}{1-p}$ 来重新缩放保留的激活值，以使期望激活值保持不变。具体来说，对于隐藏激活向量 $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$ 和一个掩码 $m \\in \\{0,1\\}^H$（其条目 $m_i \\sim \\operatorname{Bernoulli}(1-p)$ 独立同分布），经过 dropout 处理的隐藏向量为\n$$\n\\tilde{h} = \\frac{m \\odot h}{1-p} \\quad \\text{for } p > 0, \\quad \\tilde{h} = h \\quad \\text{for } p = 0.\n$$\n然后，标量输出为\n$$\nf(x) = \\mathbf{W}_2^\\top \\tilde{h} + b_2.\n$$\n在 dropout 的作用下，由于 $m$ 的随机性，$f(x)$ 成为一个随机变量。我们通过抽取 $T$ 个独立的掩码 $m^{(1)}, \\dots, m^{(T)}$ 并计算输出 $f^{(t)}(x)$，然后构成样本均值和样本方差，来估计 $\\mathbb{E}[f(x)]$ 和 $\\operatorname{Var}[f(x)]$。\n\n算法步骤：\n1. 按照规定，固定网络参数 $\\mathbf{W}_1$、$\\mathbf{b}_1$、$\\mathbf{W}_2$、$b_2$。\n2. 对于每个测试用例 $(x, p, T)$：\n   - 计算确定性的隐藏激活值 $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$。\n   - 如果 $p = 0$，则对于所有样本 $\\tilde{h} = h$，因此 $f(x)$ 是确定性的；$\\operatorname{Var}[f(x)] = 0$。\n   - 如果 $p > 0$，对 $t = 1, \\dots, T$ 重复以下步骤：\n     - 抽取一个掩码 $m^{(t)} \\in \\{0,1\\}^H$，其中 $m_i^{(t)} \\sim \\operatorname{Bernoulli}(1-p)$ 独立同分布。\n     - 构建 $\\tilde{h}^{(t)} = \\frac{m^{(t)} \\odot h}{1-p}$。\n     - 计算 $f^{(t)}(x) = \\mathbf{W}_2^\\top \\tilde{h}^{(t)} + b_2$。\n   - 通过 $\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T f^{(t)}(x)$ 估计 $\\mathbb{E}[f(x)]$，并通过无偏样本方差 $\\hat{\\sigma}^2 = \\frac{1}{T-1} \\sum_{t=1}^T (f^{(t)}(x) - \\hat{\\mu})^2$ 估计 $\\operatorname{Var}[f(x)]$（适用于 $T \\geq 2$；对于 $p=0$，所有样本都相同，方差为 $0$）。\n3. 将所有测试用例的结果聚合成一个列表，并采用指定的输出格式。\n\n认知不确定性与 dropout 率 $p$ 的关系：\n- 认知不确定性反映了模型的不确定性。Dropout 随机地移除隐藏单元。随着 $p$ 的增加，将单元置零的概率也增加，这导致通过掩码抽样的不同网络实现之间具有更高的可变性。\n- 使用倒置 dropout 时，保留的激活值会按 $\\frac{1}{1-p}$ 进行缩放。对于较大的 $p$（接近 1），当少数激活单元被保留时，这个因子会放大它们的贡献，而大多数样本会将它们置零。这种混合情况（罕见的大贡献与频繁的零）增加了 $f(x)$ 的方差。\n- 因此，在其他条件相同的情况下，预测方差 $\\operatorname{Var}[f(x)]$ 通常会随着 $p$ 的增加而增加，这表明认知不确定性更大。\n\n测试套件覆盖范围说明：\n- 用例 1 ($p=0$) 产生零方差，因为 $m_i \\equiv 1$ 并且不存在随机性。\n- 用例 2 和 3 是具有中等 $p$ 和较大 $T$ 的典型蒙特卡洛 dropout 估计，提供了对 $\\mathbb{E}[f(x)]$ 和 $\\operatorname{Var}[f(x)]$ 的准确估计。\n- 用例 4 使用了高 dropout 率 $p=0.9$ 和更大的 $T$，以稳定地估计由强 dropout 引起的方差增加。\n- 用例 5 展示了小样本量 $T=25$ 对估计值的影响，强调了抽样可变性。\n\n程序将使用固定的随机种子实现这些步骤，以确保每次运行结果的确定性，并将按顺序为每个测试用例打印一行包含 $[\\text{均值}, \\text{方差}]$ 列表的输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(x, 0.0)\n\ndef mc_dropout_predict(x, p, T, rng, W1, b1, W2, b2):\n    \"\"\"\n    Perform Monte Carlo dropout predictions for a single input x.\n    Returns an array of T scalar outputs.\n    \"\"\"\n    # Compute deterministic hidden activation h = ReLU(W1 x + b1)\n    a = W1 @ x + b1\n    h = relu(a)\n\n    # If p == 0, output is deterministic; replicate the same value T times\n    if p == 0.0:\n        y = float(W2 @ h + b2)\n        return np.full(T, y, dtype=float)\n\n    keep_prob = 1.0 - p\n    scale = 1.0 / keep_prob\n\n    # Sample T masks and compute outputs\n    outputs = np.empty(T, dtype=float)\n    for t in range(T):\n        # Bernoulli(keep_prob) for each hidden unit\n        m = rng.binomial(1, keep_prob, size=h.shape)\n        h_drop = (m * h) * scale\n        y = float(W2 @ h_drop + b2)\n        outputs[t] = y\n    return outputs\n\ndef solve():\n    # Fixed random seed for determinism\n    rng = np.random.default_rng(42)\n\n    # Define fixed network parameters\n    W1 = np.array([[1.0, -0.5],\n                   [0.3,  0.8],\n                   [-0.7, 0.2]], dtype=float)\n    b1 = np.array([0.1, -0.2, 0.0], dtype=float)\n    W2 = np.array([0.5, -1.0, 0.3], dtype=float)\n    b2 = 0.05\n\n    # Define the test cases from the problem statement.\n    # Each test case is (x, p, T)\n    test_cases = [\n        (np.array([0.5, -1.0], dtype=float), 0.0, 100),\n        (np.array([0.5, -1.0], dtype=float), 0.2, 1000),\n        (np.array([2.0,  1.0], dtype=float), 0.5, 1000),\n        (np.array([2.0,  1.0], dtype=float), 0.9, 2000),\n        (np.array([-1.5, 0.3], dtype=float), 0.5, 25),\n    ]\n\n    results = []\n    for x, p, T in test_cases:\n        outputs = mc_dropout_predict(x, p, T, rng, W1, b1, W2, b2)\n        mean = float(np.mean(outputs))\n        # Unbiased sample variance; for T=1, set variance to 0.0 to avoid NaN\n        var = float(np.var(outputs, ddof=1)) if T > 1 else 0.0\n        # Round for stable presentation while keeping numeric type\n        mean_rounded = round(mean, 6)\n        var_rounded = round(var, 6)\n        results.append([mean_rounded, var_rounded])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n```", "id": "3123387"}, {"introduction": "在分析模型输出的基础上，本练习将探索如何利用统计学原理来主动改进预测。我们将研究测试时增强 (Test-Time Augmentation, TTA)，并推导一种最优策略来组合来自不同数据增强的预测结果。这个练习展示了如何通过最小化方差来构建更鲁棒和准确的估计器，从而将我们从被动的分析带向主动的算法设计。[@problem_id:3123404]", "problem": "考虑一个在深度学习推理上下文中的确定性仿射预测器，其定义为 $f(\\mathbf{y}) = \\mathbf{w}^\\top \\mathbf{y} + b$，其中 $\\mathbf{y} \\in \\mathbb{R}^d$ 是输入，$\\mathbf{w} \\in \\mathbb{R}^d$ 是权重向量， $b \\in \\mathbb{R}$ 是偏置。在测试时，通过索引为 $i \\in \\{1,2,3\\}$ 的策略对固定输入 $\\mathbf{x} \\in \\mathbb{R}^d$ 应用随机增强，其形式为 $a_i(\\mathbf{x}) = \\mathbf{x} + \\boldsymbol{\\varepsilon}_i$，其中 $\\boldsymbol{\\varepsilon}_i$ 是一个零均值随机向量，其协方差矩阵为对角矩阵 $\\Sigma_i \\in \\mathbb{R}^{d \\times d}$。对于每个策略 $i$，使用 $n_i$ 次独立的增强抽样来形成该策略的样本均值预测。目标是对于给定策略，估计期望 $\\mathbb{E}[f(a_i(\\mathbf{x}))]$ 和方差 $\\operatorname{Var}[f(a_i(\\mathbf{x}))]$，并设计一种方差感知聚合方法，该方法将每个策略的样本均值组合成一个单一的最终估计，并在无偏性约束下使其方差最小。\n\n仅从期望、方差和协方差的核心定义以及增强抽样的独立性出发，推导出实现以下内容所需的表达式：\n- 一个蒙特卡洛（MC）估计器（蒙特卡洛（MC）定义为通过重复随机抽样进行近似），用于估计给定策略 $i$ 的 $\\mathbb{E}[f(a_i(\\mathbf{x}))]$ 和 $\\operatorname{Var}[f(a_i(\\mathbf{x}))]$。\n- 一种方差感知聚合规则，它将三个策略各自的样本均值组合成一个对共同期望的单一无偏估计，并在假设所有三个策略由于 $\\mathbb{E}[\\boldsymbol{\\varepsilon}_i] = \\mathbf{0}$ 和相同的均值输入 $\\mathbf{x}$ 而共享相同的 $f(a_i(\\mathbf{x}))$ 期望值的前提下，最小化其方差。请从第一性原理明确推导出最优权重（不要使用任何预先记下的快捷公式），并提供最终最小方差的公式。同时，也请推导朴素均值聚合器（简单地对所有样本的所有单个预测值进行平均）的方差。\n\n您的程序必须实现推导出的公式，并为以下测试套件生成定量输出。在每种情况下，随机数生成器必须以确定性的方式设定种子，并且MC估计必须使用每个策略指定的样本数量。\n\n测试套件：\n- 情况1（理想情况）：\n  - 维度 $d = 3$。\n  - 输入 $\\mathbf{x} = (0.8, -1.0, 0.5)$。\n  - 权重 $\\mathbf{w} = (1.2, -0.7, 0.3)$。\n  - 偏置 $b = 0.1$。\n  - 策略：\n    - 策略1：$\\Sigma_1 = \\operatorname{diag}(0.04, 0.01, 0.09)$，$n_1 = 400$。\n    - 策略2：$\\Sigma_2 = \\operatorname{diag}(0.25, 0.16, 0.36)$，$n_2 = 300$。\n    - 策略3：$\\Sigma_3 = \\operatorname{diag}(1.00, 0.49, 0.64)$，$n_3 = 200$。\n  - 种子 $42$。\n\n- 情况2（包含零方差策略的边界情况）：\n  - 维度 $d = 3$。\n  - 输入 $\\mathbf{x} = (0.2, -0.3, 1.5)$。\n  - 权重 $\\mathbf{w} = (0.7, 1.1, -0.4)$。\n  - 偏置 $b = -0.05$。\n  - 策略：\n    - 策略1：$\\Sigma_1 = \\operatorname{diag}(0.0, 0.0, 0.0)$，$n_1 = 50$。\n    - 策略2：$\\Sigma_2 = \\operatorname{diag}(0.09, 0.09, 0.09)$，$n_2 = 100$。\n    - 策略3：$\\Sigma_3 = \\operatorname{diag}(0.36, 0.49, 0.25)$，$n_3 = 150$。\n  - 种子 $123$。\n\n- 情况3（异方差高维混合情况）：\n  - 维度 $d = 5$。\n  - 输入 $\\mathbf{x} = (1.0, -0.5, 0.3, 0.7, -1.2)$。\n  - 权重 $\\mathbf{w} = (0.6, -0.4, 0.9, 0.1, -0.2)$。\n  - 偏置 $b = 0.0$。\n  - 策略：\n    - 策略1：$\\Sigma_1 = \\operatorname{diag}(0.16, 0.04, 0.09, 0.00, 0.25)$，$n_1 = 500$。\n    - 策略2：$\\Sigma_2 = \\operatorname{diag}(0.49, 0.36, 0.81, 0.25, 0.16)$，$n_2 = 400$。\n    - 策略3：$\\Sigma_3 = \\operatorname{diag}(0.01, 0.01, 0.01, 0.01, 0.01)$，$n_3 = 600$。\n  - 种子 $7$。\n\n在每种情况下，计算并返回以下七个量：\n$1$) 使用策略1对 $\\mathbb{E}[f(a_1(\\mathbf{x}))]$ 的MC估计值，\n$2$) 使用策略1对 $\\operatorname{Var}[f(a_1(\\mathbf{x}))]$ 的MC估计值，\n$3$) $\\mathbb{E}[f(a_i(\\mathbf{x}))]$ 的解析推导值（所有策略通用），\n$4$) $\\operatorname{Var}[f(a_1(\\mathbf{x}))]$ 的解析推导值，\n$5$) 对三个策略使用 $n_1$、$n_2$ 和 $n_3$ 个样本的方差感知聚合估计器的解析推导最小方差，\n$6$) 对所有策略的所有样本的所有单个预测值进行平均的朴素均值聚合器的解析推导方差，\n$7$) 一个布尔值，指示第5项中的方差感知聚合方差是否严格小于第6项中的朴素均值方差。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，依次为情况1的七个量，然后是情况2的七个量，最后是情况3的七个量。例如，整体格式必须是 $[\\text{case1\\_item1},\\text{case1\\_item2},\\dots,\\text{case3\\_item7}]$。所有返回的值必须是布尔型、整型、浮点型或这些基本类型的列表。本问题不涉及物理单位、角度单位或百分比。", "solution": "问题要求推导和实现一个在随机输入增强下的仿射预测器的期望和方差的估计器，以及为来自不同增强策略的估计设计一种最优聚合策略。我们将首先根据概率论的基本原理推导出必要的解析表达式，然后将它们应用于具体的测试用例。\n\n设仿射预测器为 $f(\\mathbf{y}) = \\mathbf{w}^\\top \\mathbf{y} + b$，其中 $\\mathbf{y} \\in \\mathbb{R}^d$ 是输入，$\\mathbf{w} \\in \\mathbb{R}^d$ 是权重向量， $b \\in \\mathbb{R}$ 是偏置标量。增强策略的形式为 $a_i(\\mathbf{x}) = \\mathbf{x} + \\boldsymbol{\\varepsilon}_i$，其中 $\\mathbf{x} \\in \\mathbb{R}^d$ 是固定输入，$\\boldsymbol{\\varepsilon}_i$ 是一个随机向量，满足 $\\mathbb{E}[\\boldsymbol{\\varepsilon}_i] = \\mathbf{0}$ 和 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_i) = \\Sigma_i$（一个对角矩阵）。\n\n首先，我们定义代表策略 $i$ 的预测器输出的随机变量：\n$$Z_i = f(a_i(\\mathbf{x})) = f(\\mathbf{x} + \\boldsymbol{\\varepsilon}_i) = \\mathbf{w}^\\top (\\mathbf{x} + \\boldsymbol{\\varepsilon}_i) + b = \\mathbf{w}^\\top \\mathbf{x} + \\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i + b$$\n\n**解析期望和方差**\n\n$Z_i$ 的期望，记为 $\\mu_i$，可利用期望算子的线性性求得：\n$$\\mu_i = \\mathbb{E}[Z_i] = \\mathbb{E}[\\mathbf{w}^\\top \\mathbf{x} + \\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i + b]$$\n由于 $\\mathbf{w}$、$\\mathbf{x}$ 和 $b$ 是常数，我们有：\n$$\\mu_i = \\mathbf{w}^\\top \\mathbf{x} + \\mathbb{E}[\\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i] + b = \\mathbf{w}^\\top \\mathbf{x} + \\mathbf{w}^\\top \\mathbb{E}[\\boldsymbol{\\varepsilon}_i] + b$$\n鉴于 $\\mathbb{E}[\\boldsymbol{\\varepsilon}_i] = \\mathbf{0}$，期望简化为：\n$$\\mu_i = \\mathbf{w}^\\top \\mathbf{x} + b$$\n这个结果与策略索引 $i$ 无关，证实了所有策略共享一个共同的期望 $\\mu = \\mathbf{w}^\\top \\mathbf{x} + b$。这是第3项所需的解析期望。\n\n$Z_i$ 的方差，记为 $\\sigma_i^2$，可利用方差的性质求得。常数不影响方差，所以：\n$$\\sigma_i^2 = \\operatorname{Var}[Z_i] = \\operatorname{Var}[\\mathbf{w}^\\top \\mathbf{x} + \\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i + b] = \\operatorname{Var}[\\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i]$$\n对于一个随机向量 $\\mathbf{v}$ 经过矩阵 $\\mathbf{A}$ 的线性变换，其方差为 $\\operatorname{Var}[\\mathbf{A}\\mathbf{v}] = \\mathbf{A}\\operatorname{Cov}(\\mathbf{v})\\mathbf{A}^\\top$。此处，$\\mathbf{A}$ 是行向量 $\\mathbf{w}^\\top$，随机向量是 $\\boldsymbol{\\varepsilon}_i$。\n$$\\sigma_i^2 = \\mathbf{w}^\\top \\operatorname{Cov}(\\boldsymbol{\\varepsilon}_i) \\mathbf{w}$$\n鉴于 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_i) = \\Sigma_i$，我们得到：\n$$\\sigma_i^2 = \\mathbf{w}^\\top \\Sigma_i \\mathbf{w}$$\n由于 $\\Sigma_i$ 是对角矩阵，其对角元素为 $\\Sigma_{i,jj}$，且 $\\mathbf{w}$ 的分量为 $w_j$，这个二次型可简化为 $\\sigma_i^2 = \\sum_{j=1}^d w_j^2 \\Sigma_{i,jj}$。这个表达式给出了策略 $i$ 的解析方差，对于 $i=1$，这就是第4项的值。\n\n**蒙特卡洛估计器**\n\n对于给定的策略 $i$，我们生成 $n_i$ 个独立样本 $\\{ \\boldsymbol{\\varepsilon}_i^{(k)} \\}_{k=1}^{n_i}$，从而得到 $n_i$ 个预测值 $\\{ z_{i,k} \\}_{k=1}^{n_i}$，其中 $z_{i,k} = f(\\mathbf{x} + \\boldsymbol{\\varepsilon}_i^{(k)})$。\n期望 $\\mu$ 的蒙特卡洛估计器是样本均值：\n$$\\hat{\\mu}_i = \\frac{1}{n_i} \\sum_{k=1}^{n_i} z_{i,k}$$\n对于策略 $i=1$，这是第1项的估计器。\n方差 $\\sigma_i^2$ 的无偏蒙特卡洛估计器是样本方差：\n$$\\hat{\\sigma}_i^2 = \\frac{1}{n_i - 1} \\sum_{k=1}^{n_i} (z_{i,k} - \\hat{\\mu}_i)^2$$\n对于策略 $i=1$，这是第2项的估计器。\n\n**方差感知聚合**\n\n我们有三个独立的、对 $\\mu$ 的无偏估计器，即样本均值 $\\hat{\\mu}_1, \\hat{\\mu}_2, \\hat{\\mu}_3$。每个估计器的方差是：\n$$v_i = \\operatorname{Var}[\\hat{\\mu}_i] = \\operatorname{Var}\\left[\\frac{1}{n_i} \\sum_{k=1}^{n_i} z_{i,k}\\right] = \\frac{1}{n_i^2} \\sum_{k=1}^{n_i} \\operatorname{Var}[z_{i,k}] = \\frac{n_i \\sigma_i^2}{n_i^2} = \\frac{\\sigma_i^2}{n_i}$$\n我们寻求一个聚合估计器 $\\hat{\\mu}_{agg} = \\sum_{i=1}^3 c_i \\hat{\\mu}_i$，使其无偏且方差最小。\n无偏性要求 $\\mathbb{E}[\\hat{\\mu}_{agg}] = \\mu$，这意味着 $\\sum_{i=1}^3 c_i = 1$。\n需要最小化的方差是 $\\operatorname{Var}[\\hat{\\mu}_{agg}] = \\sum_{i=1}^3 c_i^2 \\operatorname{Var}[\\hat{\\mu}_i] = \\sum_{i=1}^3 c_i^2 v_i$，这是由于估计器是独立的。\n\n我们使用拉格朗日乘数法来最小化 $V(c_1, c_2, c_3) = \\sum_{i=1}^3 c_i^2 v_i$，约束条件为 $g(c_1, c_2, c_3) = \\sum_{i=1}^3 c_i - 1 = 0$。\n拉格朗日函数为 $\\mathcal{L}(c_1, c_2, c_3, \\lambda) = \\sum_{i=1}^3 c_i^2 v_i - \\lambda(\\sum_{i=1}^3 c_i - 1)$。\n将关于每个 $c_i$ 的偏导数设为零，得到 $2c_i v_i - \\lambda = 0$，因此 $c_i = \\lambda / (2v_i)$。\n将此代入约束条件：$\\sum_{i=1}^3 \\frac{\\lambda}{2v_i} = 1 \\implies \\frac{\\lambda}{2} \\sum_{i=1}^3 \\frac{1}{v_i} = 1 \\implies \\lambda = 2 / (\\sum_{j=1}^3 1/v_j)$。\n因此，最优权重为 $c_i = \\frac{1}{v_i} / (\\sum_{j=1}^3 1/v_j)$。\n\n最终的最小方差 $V_{min}$ 为：\n$$V_{min} = \\sum_{i=1}^3 c_i^2 v_i = \\sum_{i=1}^3 \\left( \\frac{\\lambda}{2v_i} \\right)^2 v_i = \\frac{\\lambda^2}{4} \\sum_{i=1}^3 \\frac{1}{v_i}$$\n代入 $\\lambda/2 = 1 / (\\sum_j 1/v_j)$:\n$$V_{min} = \\left(\\frac{1}{\\sum_j 1/v_j}\\right)^2 \\sum_i \\frac{1}{v_i} = \\frac{1}{\\sum_{j=1}^3 1/v_j} = \\left(\\sum_{i=1}^3 \\frac{1}{v_i}\\right)^{-1} = \\left(\\frac{n_1}{\\sigma_1^2} + \\frac{n_2}{\\sigma_2^2} + \\frac{n_3}{\\sigma_3^2}\\right)^{-1}$$\n如果任何 $\\sigma_i^2 = 0$，则 $v_i=0$，那么项 $n_i/\\sigma_i^2$ 变为无穷大，使得和为无穷大，而 $V_{min}$ 等于 $0$。这是正确的，因为一个零方差估计器应被赋予全部权重，从而得到一个方差为零的聚合估计器。这是第5项的表达式。\n\n**朴素均值聚合器**\n\n朴素聚合器对所有 $N = n_1 + n_2 + n_3$ 个单独预测值取平均。其估计器可以写成各策略均值的加权平均：\n$$\\hat{\\mu}_{naive} = \\frac{1}{N} \\sum_{i=1}^3 \\sum_{k=1}^{n_i} z_{i,k} = \\sum_{i=1}^3 \\frac{n_i}{N} \\hat{\\mu}_i$$\n这是一个无偏线性估计器，其权重为 $c_i^{naive} = n_i/N$。其方差 $V_{naive}$ 为：\n$$V_{naive} = \\operatorname{Var}[\\hat{\\mu}_{naive}] = \\sum_{i=1}^3 (c_i^{naive})^2 v_i = \\sum_{i=1}^3 \\left(\\frac{n_i}{N}\\right)^2 \\frac{\\sigma_i^2}{n_i} = \\frac{1}{N^2} \\sum_{i=1}^3 n_i \\sigma_i^2$$\n$$V_{naive} = \\frac{n_1 \\sigma_1^2 + n_2 \\sigma_2^2 + n_3 \\sigma_3^2}{(n_1 + n_2 + n_3)^2}$$\n这是第6项的表达式。\n\n**方差比较**\n\n根据构造，方差感知聚合器是最小方差无偏线性估计器。朴素聚合器也是一个无偏线性估计器，但通常不是最优的。因此，必然有 $V_{min} \\le V_{naive}$。严格不等式 $V_{min} < V_{naive}$ 成立，除非朴素权重恰好是最优的。最优权重为 $c_i \\propto 1/v_i = n_i/\\sigma_i^2$，而朴素权重为 $c_i^{naive} \\propto n_i$。仅当所有 $\\sigma_i^2$ 相等时（对于 $\\sigma_i^2 > 0$），这两者成比例。如果各策略的方差 $\\sigma_i^2$ 不全相等，则 $V_{min} < V_{naive}$。第7项是此比较的结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"d\": 3,\n            \"x\": np.array([0.8, -1.0, 0.5]),\n            \"w\": np.array([1.2, -0.7, 0.3]),\n            \"b\": 0.1,\n            \"policies\": [\n                {\"Sigma_diag\": np.array([0.04, 0.01, 0.09]), \"n\": 400},\n                {\"Sigma_diag\": np.array([0.25, 0.16, 0.36]), \"n\": 300},\n                {\"Sigma_diag\": np.array([1.00, 0.49, 0.64]), \"n\": 200},\n            ],\n            \"seed\": 42\n        },\n        # Case 2\n        {\n            \"d\": 3,\n            \"x\": np.array([0.2, -0.3, 1.5]),\n            \"w\": np.array([0.7, 1.1, -0.4]),\n            \"b\": -0.05,\n            \"policies\": [\n                {\"Sigma_diag\": np.array([0.0, 0.0, 0.0]), \"n\": 50},\n                {\"Sigma_diag\": np.array([0.09, 0.09, 0.09]), \"n\": 100},\n                {\"Sigma_diag\": np.array([0.36, 0.49, 0.25]), \"n\": 150},\n            ],\n            \"seed\": 123\n        },\n        # Case 3\n        {\n            \"d\": 5,\n            \"x\": np.array([1.0, -0.5, 0.3, 0.7, -1.2]),\n            \"w\": np.array([0.6, -0.4, 0.9, 0.1, -0.2]),\n            \"b\": 0.0,\n            \"policies\": [\n                {\"Sigma_diag\": np.array([0.16, 0.04, 0.09, 0.00, 0.25]), \"n\": 500},\n                {\"Sigma_diag\": np.array([0.49, 0.36, 0.81, 0.25, 0.16]), \"n\": 400},\n                {\"Sigma_diag\": np.array([0.01, 0.01, 0.01, 0.01, 0.01]), \"n\": 600},\n            ],\n            \"seed\": 7\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = solve_case(case)\n        all_results.extend(case_results)\n\n    # Format the final output string as specified\n    # The default str() for a boolean is 'True' or 'False' with a capital letter.\n    # The problem description does not specify lowercase.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef solve_case(case):\n    \"\"\"\n    Computes the seven required quantities for a single test case.\n    \"\"\"\n    w, x, b, seed = case[\"w\"], case[\"x\"], case[\"b\"], case[\"seed\"]\n    policies = case[\"policies\"]\n    \n    # Initialize random number generator\n    rng = np.random.default_rng(seed)\n\n    # --- Monte Carlo estimations for Policy 1 ---\n    policy1 = policies[0]\n    n1 = policy1[\"n\"]\n    Sigma1_diag = policy1[\"Sigma_diag\"]\n    \n    # Generate random noise samples for policy 1\n    std_devs1 = np.sqrt(Sigma1_diag)\n    # epsilon_samples is (n1, d)\n    epsilon_samples1 = rng.normal(0, std_devs1, size=(n1, case[\"d\"]))\n    \n    # Calculate predictions z = w^T * (x + epsilon) + b\n    # (n1, d) @ (d,) -> (n1,)\n    predictions1 = epsilon_samples1 @ w + (x @ w + b)\n    \n    # Item 1: MC estimate of E[f(a_1(x))]\n    mc_mean_p1 = np.mean(predictions1)\n    \n    # Item 2: MC estimate of Var[f(a_1(x))]\n    # ddof=1 for unbiased sample variance\n    mc_var_p1 = np.var(predictions1, ddof=1)\n    \n    # --- Analytical derivations ---\n    \n    # Item 3: Analytical expectation E[f(a_i(x))]\n    analytical_mean = w @ x + b\n    \n    # Calculate analytical variances for all policies\n    sigmas_sq = []\n    for pol in policies:\n        # sigma_i^2 = w^T * Sigma_i * w\n        sigma_sq = w @ (pol[\"Sigma_diag\"] * w)  # Since Sigma is diagonal\n        sigmas_sq.append(sigma_sq)\n        \n    # Item 4: Analytical variance Var[f(a_1(x))]\n    analytical_var_p1 = sigmas_sq[0]\n    \n    # Item 5: Minimal variance of the variance-aware aggregated estimator\n    sum_inverse_v = 0\n    is_zero_var_policy = False\n    for i, pol in enumerate(policies):\n        sigma_sq_i = sigmas_sq[i]\n        n_i = pol[\"n\"]\n        if sigma_sq_i == 0:\n            is_zero_var_policy = True\n            break\n        # v_i = sigma_i^2 / n_i\n        sum_inverse_v += n_i / sigma_sq_i\n        \n    if is_zero_var_policy:\n        min_var = 0.0\n    else:\n        min_var = 1.0 / sum_inverse_v\n        \n    # Item 6: Variance of the naive mean aggregator\n    ns = np.array([p[\"n\"] for p in policies])\n    total_n = np.sum(ns)\n    weighted_sum_sigmas_sq = np.sum(ns * np.array(sigmas_sq))\n    naive_var = weighted_sum_sigmas_sq / (total_n**2)\n    \n    # Item 7: Boolean comparison\n    is_optimal_better = min_var  naive_var\n    \n    return [\n        mc_mean_p1,\n        mc_var_p1,\n        analytical_mean,\n        analytical_var_p1,\n        min_var,\n        naive_var,\n        is_optimal_better\n    ]\n\n# Run the solver\nsolve()\n\n```", "id": "3123404"}, {"introduction": "最后的这个练习将深入探讨深度网络所学到特征的几何结构，这是一个被称为“神经坍塌 (neural collapse)”的前沿课题。我们将使用协方差矩阵来量化同一类别的特征如何聚集在一起（类内协方差），以及这些类别中心的分布情况（类间协方差）。这个模拟练习提供了一个具体的动手实践，让我们理解期望和协方差如何揭示深度学习中高维表征的深刻结构特性。[@problem_id:3123405]", "problem": "您的任务是设计并实现一个程序，用于量化在监督式深度学习表示中普遍观察到的被称为神经坍塌的现象。重点是追踪类内协方差的收缩和类别均值的类间协方差的增长，并使用在特定参数缩放下拉近一个常数的比率度量来衡量坍塌程度。您的程序必须仅使用期望、方差和协方差的定义，从第一性原理出发计算这些量。\n\n考虑一个随机向量 $\\mathbf{X} \\in \\mathbb{R}^D$ 和一个离散类别标签 $Y \\in \\{1,\\dots,C\\}$。对于每个类别 $c \\in \\{1,\\dots,C\\}$，您将根据一个带有各向同性高斯噪声和类别相关均值的类条件模型来模拟数据，如下所示。\n\n数据生成过程：\n- 对于给定的整数 $C$（类别数）、$D$（维度）和 $n$（每类样本数），以及实数标准差 $\\sigma \\ge 0$ 和实数均值尺度 $\\gamma  0$，通过以下方式构建类别均值 $\\boldsymbol{\\mu}_1,\\dots,\\boldsymbol{\\mu}_C \\in \\mathbb{R}^D$：\n  1. 在 $\\mathbb{R}^D$ 中采样 $C$ 个独立同分布的标准正态向量，将它们作为矩阵的列进行组合，并通过减去它们的列平均值来中心化这些列，使它们在所有类别上的平均值为零。\n  2. 将每个中心化后的列归一化为单位欧几里得范数，并将其缩放以使欧几里得范数为 $\\gamma$，从而对所有 $c$ 都有 $\\|\\boldsymbol{\\mu}_c\\|_2 = \\gamma$。\n- 对于每个类别 $c$，根据模型独立采样 $n$ 个观测值\n  $$ \\mathbf{X} \\mid Y=c = \\boldsymbol{\\mu}_c + \\boldsymbol{\\varepsilon}, \\quad \\text{其中 } \\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_D). $$\n\n经验估计任务：\n- 计算每个类别 $c$ 的经验类别均值：\n  $$ \\widehat{\\boldsymbol{\\mu}}_c = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_{c,i}, $$\n  其中 $\\mathbf{x}_{c,i}$ 是类别 $c$ 的采样观测值。\n- 使用分母为 $n$ 的经验期望算子计算每个类别 $c$ 的经验类内协方差：\n  $$ \\widehat{\\mathbf{\\Sigma}}_{w,c} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c\\right)\\left(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c\\right)^\\top. $$\n- 计算总体经验类内协方差，作为所有类别的平均值：\n  $$ \\widehat{\\mathbf{S}}_w = \\frac{1}{C} \\sum_{c=1}^C \\widehat{\\mathbf{\\Sigma}}_{w,c}. $$\n- 计算类别均值的经验均值：\n  $$ \\widehat{\\boldsymbol{\\mu}} = \\frac{1}{C} \\sum_{c=1}^C \\widehat{\\boldsymbol{\\mu}}_c. $$\n- 计算类别均值的经验类间协方差：\n  $$ \\widehat{\\mathbf{S}}_b = \\frac{1}{C} \\sum_{c=1}^C \\left(\\widehat{\\boldsymbol{\\mu}}_c - \\widehat{\\boldsymbol{\\mu}}\\right)\\left(\\widehat{\\boldsymbol{\\mu}}_c - \\widehat{\\boldsymbol{\\mu}}\\right)^\\top. $$\n\n坍塌比率度量：\n- 计算以下两个标量度量：\n  1. 迹比率\n     $$ r_{\\mathrm{trace}} = \\frac{\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_b\\right)}{\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_w\\right)}. $$\n  2. 弗罗贝尼乌斯范数比率\n     $$ r_{\\mathrm{fro}} = \\frac{\\left\\|\\widehat{\\mathbf{S}}_b\\right\\|_F}{\\left\\|\\widehat{\\mathbf{S}}_w\\right\\|_F}, $$\n     其中 $\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$ 表示弗罗贝尼乌斯范数。\n\n边界情况：\n- 如果 $\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_w\\right) = 0$ 或 $\\left\\|\\widehat{\\mathbf{S}}_w\\right\\|_F = 0$，则将相应的比率定义为 $+\\infty$。\n\n您的程序必须按照描述精确地实现数据生成和估计，并使用指定的种子以确保可复现性。它必须处理一套参数值的测试用例，为每个用例计算两个比率，并生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个结果必须是一个包含两个元素的列表，顺序严格为 $[r_{\\mathrm{trace}}, r_{\\mathrm{fro}}]$。因此，最终输出格式必须如下所示：\n$$ [[r_{\\mathrm{trace}}^{(1)}, r_{\\mathrm{fro}}^{(1)}],[r_{\\mathrm{trace}}^{(2)}, r_{\\mathrm{fro}}^{(2)}],\\dots]. $$\n\n测试用例：\n- 用例 1（正常路径）：种子 $0$，$C=5$，$D=10$，$n=200$，$\\sigma=1.0$，$\\gamma=2.0$。\n- 用例 2（恒定比率缩放）：种子 $0$，$C=5$，$D=10$，$n=200$，$\\sigma=2.0$，$\\gamma=4.0$。\n- 用例 3（高噪声，低分离度）：种子 $1$，$C=5$，$D=10$，$n=200$，$\\sigma=3.0$，$\\gamma=1.0$。\n- 用例 4（低噪声，高分离度）：种子 $2$，$C=5$，$D=10$，$n=200$，$\\sigma=0.1$，$\\gamma=3.0$。\n- 用例 5（边界情况，零类内方差）：种子 $3$，$C=3$，$D=4$，$n=50$，$\\sigma=0.0$，$\\gamma=1.5$。\n\n最终输出规范：\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。列表中的每个元素本身就是一个包含两个元素的列表，其中包含相应测试用例的值 $[r_{\\mathrm{trace}}, r_{\\mathrm{fro}}]$，其顺序与测试用例的顺序相同。", "solution": "该问题是有效的。它在科学上基于深度学习表示的统计建模，其定义和参数完整一致，问题阐述清晰，并且其表述是客观的。我们将继续提供解决方案。\n\n目标是模拟和量化神经坍塌现象，该现象描述了在训练过程中，监督式深度网络最后一层特征表示的一种特定几何构型。该现象具有以下特征：\n1. 单个类别内特征的可变性坍塌，意味着类内协方差矩阵收缩。\n2. 不同类别的特征经验均值变得最大程度地分离且等距，形成一个单纯形等角紧框架。这导致了结构化的、大的类间协方差。\n\n该问题提供了一个简化的生成模型来研究这一现象。我们将实现这个模型，并计算两个衡量坍塌程度的关键比率：类间和类内协方差矩阵的迹之比以及弗罗贝尼乌斯范数之比。\n\n解决方案的结构如下：\n1.  **数据生成**：对于每个测试用例，我们首先生成类别原型向量，然后为每个类别采样数据点。\n2.  **经验协方差估计**：使用生成的数据，我们根据其基本定义计算经验类内和类间协方差矩阵。\n3.  **度量计算**：我们根据估计出的协方差矩阵计算迹比率和弗罗贝尼乌斯范数比率。\n\n### 1. 数据生成\n\n该过程首先为 $c \\in \\{1,\\ldots,C\\}$ 构建真实的类别均值 $\\boldsymbol{\\mu}_c \\in \\mathbb{R}^D$。这些是每个类别的理想化特征表示。\n- 首先，我们从一个 $D$ 维标准正态分布中采样 $C$ 个向量，$\\mathbf{v}_c \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_D)$。这些向量被排列成一个矩阵 $\\mathbf{V} \\in \\mathbb{R}^{D \\times C}$ 的列。\n- 从每个向量中减去这些向量的总均值 $\\bar{\\mathbf{v}} = \\frac{1}{C} \\sum_{c=1}^C \\mathbf{v}_c$，以确保生成的均值集是中心化的：$\\mathbf{v}'_c = \\mathbf{v}_c - \\bar{\\mathbf{v}}$。这保证了 $\\sum_{c=1}^C \\mathbf{v}'_c = \\mathbf{0}$。\n- 每个中心化后的向量 $\\mathbf{v}'_c$ 被归一化，使其欧几里得范数为 $\\gamma$。这通过先归一化到单位长度然后进行缩放来完成：\n  $$ \\boldsymbol{\\mu}_c = \\gamma \\frac{\\mathbf{v}'_c}{\\|\\mathbf{v}'_c\\|_2} $$\n  这确保了对于所有类别 $c$，都有 $\\|\\boldsymbol{\\mu}_c\\|_2 = \\gamma$。\n\n在确定了真实类别均值后，我们为每个类别模拟 $n$ 个数据点。每个数据点 $\\mathbf{x}_{c,i}$ 都是从一个类条件高斯分布中采样的：\n$$ \\mathbf{x}_{c,i} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_c, \\sigma^2 \\mathbf{I}_D) $$\n这等效于在真实类别均值上添加各向同性高斯噪声 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_D)$：$\\mathbf{x}_{c,i} = \\boldsymbol{\\mu}_c + \\boldsymbol{\\varepsilon}_{c,i}$。\n\n### 2. 经验协方差估计\n\n根据为每个类别 $c$ 生成的数据样本 $\\{\\mathbf{x}_{c,i}\\}_{i=1}^n$，我们计算各种统计量的经验估计值。\n\n- **经验类别均值**：每个类别的均值向量通过对其样本进行平均来估计：\n  $$ \\widehat{\\boldsymbol{\\mu}}_c = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_{c,i} $$\n\n- **类内协方差**：每个类别内的特征协方差 $\\widehat{\\mathbf{\\Sigma}}_{w,c}$ 衡量数据点围绕其经验类别均值 $\\widehat{\\boldsymbol{\\mu}}_c$ 的散布情况。它使用分母为 $n$ 的样本协方差公式计算：\n  $$ \\widehat{\\mathbf{\\Sigma}}_{w,c} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c\\right)\\left(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c\\right)^\\top $$\n  总体类内协方差 $\\widehat{\\mathbf{S}}_w$ 是这些单独协方差矩阵在所有类别上的平均值：\n  $$ \\widehat{\\mathbf{S}}_w = \\frac{1}{C} \\sum_{c=1}^C \\widehat{\\mathbf{\\Sigma}}_{w,c} $$\n\n- **类间协方差**：该度量 $\\widehat{\\mathbf{S}}_b$ 量化了*经验类别均值* $\\widehat{\\boldsymbol{\\mu}}_c$ 围绕它们自身的总均值 $\\widehat{\\boldsymbol{\\mu}} = \\frac{1}{C} \\sum_{c=1}^C \\widehat{\\boldsymbol{\\mu}}_c$ 的散布情况。其公式为：\n  $$ \\widehat{\\mathbf{S}}_b = \\frac{1}{C} \\sum_{c=1}^C \\left(\\widehat{\\boldsymbol{\\mu}}_c - \\widehat{\\boldsymbol{\\mu}}\\right)\\left(\\widehat{\\boldsymbol{\\mu}}_c - \\widehat{\\boldsymbol{\\mu}}\\right)^\\top $$\n\n### 3. 坍塌比率度量\n\n神经坍塌的程度通过比较类间协方差与类内协方差的“大小”来量化。我们使用两个标量度量来衡量这些矩阵的大小：迹和弗罗贝尼乌斯范数。\n\n- **迹比率**：方阵的迹是其对角线元素之和，$\\operatorname{tr}(\\mathbf{A}) = \\sum_i A_{ii}$。对于协方差矩阵，迹代表总方差，即沿每个维度的方差之和。迹比率为：\n  $$ r_{\\mathrm{trace}} = \\frac{\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_b\\right)}{\\operatorname{tr}\\left(\\widehat{\\mathbf{S}}_w\\right)} $$\n  一个较大的值表明类别均值之间的分离度相对于每个类别内数据的散布程度要大。\n\n- **弗罗贝尼乌斯范数比率**：矩阵 $\\mathbf{A}$ 的弗罗贝尼乌斯范数是其所有元素平方和的平方根，$\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$。它提供了衡量矩阵总体大小的另一种方法。弗罗贝尼乌斯范数比率为：\n  $$ r_{\\mathrm{fro}} = \\frac{\\left\\|\\widehat{\\mathbf{S}}_b\\right\\|_F}{\\left\\|\\widehat{\\mathbf{S}}_w\\right\\|_F} $$\n\n- **边界情况**：如果类内噪声标准差 $\\sigma$ 为 $0$，则一个类的所有样本都与该类均值相同，即 $\\mathbf{x}_{c,i} = \\boldsymbol{\\mu}_c$。因此，经验类别均值也就是真实均值，$\\widehat{\\boldsymbol{\\mu}}_c = \\boldsymbol{\\mu}_c$。类内偏差 $(\\mathbf{x}_{c,i} - \\widehat{\\boldsymbol{\\mu}}_c)$ 全为零，使得类内协方差矩阵 $\\widehat{\\mathbf{S}}_w$ 成为零矩阵。在这种情况下，其迹和弗罗贝尼乌斯范数都为 $0$。根据规定，比率 $r_{\\mathrm{trace}}$ 和 $r_{\\mathrm{fro}}$ 被定义为 $+\\infty$。\n\n该实现将为测试用例中提供的每组参数系统地执行这些步骤，并通过为每次运行设置指定的随机种子来确保可复现性。", "answer": "```python\nimport numpy as np\n# scipy is not strictly needed as numpy provides all necessary functionalities.\n# from scipy import linalg # For example, though np.linalg is used here.\n\ndef solve():\n    \"\"\"\n    Simulates neural collapse phenomena and computes collapse ratio metrics.\n    \"\"\"\n    \n    # Test suite: (seed, C, D, n, sigma, gamma)\n    test_cases = [\n        (0, 5, 10, 200, 1.0, 2.0),\n        (0, 5, 10, 200, 2.0, 4.0),\n        (1, 5, 10, 200, 3.0, 1.0),\n        (2, 5, 10, 200, 0.1, 3.0),\n        (3, 3, 4, 50, 0.0, 1.5),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        seed, C, D, n, sigma, gamma = case\n        np.random.seed(seed)\n\n        # === 1. Data Generation ===\n        \n        # --- Generate true class means mu_c ---\n        # Sample C i.i.d. standard normal vectors\n        mu_raw = np.random.randn(D, C)\n        \n        # Center the columns by subtracting their mean vector\n        mu_mean = mu_raw.mean(axis=1, keepdims=True)\n        mu_centered = mu_raw - mu_mean\n        \n        # Normalize each column to unit norm\n        norms = np.linalg.norm(mu_centered, axis=0, keepdims=True)\n        # Avoid division by zero, though highly unlikely with continuous distributions\n        norms[norms == 0] = 1.0  \n        mu_unit_norm = mu_centered / norms\n        \n        # Scale to have norm gamma\n        mus = mu_unit_norm * gamma  # Shape (D, C)\n\n        # --- Sample data points for each class ---\n        data_by_class = []\n        for c in range(C):\n            mu_c = mus[:, c].reshape(1, D)  # Shape (1, D) for broadcasting\n            # Generate isotropic Gaussian noise\n            epsilon = np.random.randn(n, D) * sigma\n            # Generate class data\n            X_c = mu_c + epsilon  # Shape (n, D)\n            data_by_class.append(X_c)\n\n        # === 2. Empirical Covariance Estimation ===\n        \n        # --- Compute empirical class means hat_mu_c ---\n        hat_mus = np.array([X_c.mean(axis=0) for X_c in data_by_class]) # Shape (C, D)\n        \n        # --- Compute overall within-class covariance S_w ---\n        # List to store individual within-class covariance matrices\n        hat_Sigma_w_c_list = []\n        for c in range(C):\n            X_c = data_by_class[c]\n            hat_mu_c = hat_mus[c]\n            # Deviations from the class mean\n            diffs = X_c - hat_mu_c # Shape (n, D)\n            # Covariance for class c, using denominator n\n            hat_Sigma_w_c = (diffs.T @ diffs) / n # Shape (D, D)\n            hat_Sigma_w_c_list.append(hat_Sigma_w_c)\n        \n        # Average across classes to get S_w\n        hat_S_w = np.mean(np.array(hat_Sigma_w_c_list), axis=0) # Shape (D, D)\n\n        # --- Compute between-class covariance S_b ---\n        # Mean of empirical class means\n        hat_mu = hat_mus.mean(axis=0) # Shape (D,)\n        \n        # Deviations of class means from the grand mean\n        diffs_b = hat_mus - hat_mu # Shape (C, D)\n        \n        # Covariance of class means, using denominator C\n        hat_S_b = (diffs_b.T @ diffs_b) / C # Shape (D, D)\n        \n        # === 3. Metric Calculation ===\n        \n        tr_S_w = np.trace(hat_S_w)\n        tr_S_b = np.trace(hat_S_b)\n        \n        fro_S_w = np.linalg.norm(hat_S_w, 'fro')\n        fro_S_b = np.linalg.norm(hat_S_b, 'fro')\n\n        # Calculate ratios with division-by-zero handling\n        # If sigma is 0, S_w will be a zero matrix, tr_S_w and fro_S_w will be 0.\n        r_trace = tr_S_b / tr_S_w if tr_S_w != 0 else np.inf\n        r_fro = fro_S_b / fro_S_w if fro_S_w != 0 else np.inf\n        \n        results.append([r_trace, r_fro])\n\n    # Format output as a string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3123405"}]}