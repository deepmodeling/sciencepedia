{"hands_on_practices": [{"introduction": "温度缩放是校准模型预测置信度的常用技术，它通过超参数 $T$ 调整Softmax函数的输出。本练习 [@problem_id:3190253] 将指导你运用多元链式法则，推导损失函数关于 $T$ 的梯度，这为通过梯度下降法优化超参数提供了可能。通过这个过程，你将掌握在复杂函数组合中进行梯度计算的技能，并初步接触模型参数可辨识性的重要概念。", "problem": "考虑一个具有 $K$ 个类别的分类模型，该模型输出一个 logit 向量 $z \\in \\mathbb{R}^{K}$，并使用温度缩放的 softmax 函数生成概率 $p \\in \\mathbb{R}^{K}$，其中 $p_{i}=\\frac{\\exp\\left(z_{i}/T\\right)}{\\sum_{j=1}^{K}\\exp\\left(z_{j}/T\\right)}$ 且 $T>0$。训练目标是交叉熵 (CE) 损失 $L(z,T;y)=-\\sum_{i=1}^{K}y_{i}\\ln\\left(p_{i}\\right)$，其中 $y \\in \\mathbb{R}^{K}$ 是概率单纯形上的一个目标分布，因此 $\\sum_{i=1}^{K}y_{i}=1$ 且 $y_{i}\\geq 0$。仅使用基本定义和多元链式法则，推导 $\\frac{\\partial L}{\\partial T}$ 关于 $z$、$y$、$p$ 和 $T$ 的闭式表达式。然后，考虑一个在 softmax 前缩放 logit 的上游标量参数 $s>0$，使得模型概率变为 $p_{i}(s,T)=\\frac{\\exp\\left(s z_{i}/T\\right)}{\\sum_{j=1}^{K}\\exp\\left(s z_{j}/T\\right)}$。基于从 $(s,T)$ 到 $p(s,T)$ 再到 $L$ 的映射复合，讨论 $(s,T)$ 是否可以从损失中联合识别，并解释原因。你的最终答案必须是 $\\frac{\\partial L}{\\partial T}$ 的单个闭式解析表达式。", "solution": "问题要求做两件事：首先，推导交叉熵损失 $L$ 关于温度参数 $T$ 的偏导数的闭式表达式；其次，讨论 logit 缩放参数 $s$ 和温度 $T$ 的联合可识别性。\n\n这个问题是有效的，因为它在科学上基于机器学习和微积分的既定原则，是适定的，有足够的信息得出唯一解和合理解释，并且以客观、正式的语言表述。它是多元链式法则在深度学习相关问题上的直接应用。\n\n首先，我们推导 $\\frac{\\partial L}{\\partial T}$ 的表达式。\n损失函数由 $L(z,T;y)=-\\sum_{i=1}^{K}y_{i}\\ln(p_{i})$ 给出，其中概率 $p_i$ 是 logit $z$ 和温度 $T$ 的函数。具体来说，$p_{i}(z, T) = \\frac{\\exp(z_{i}/T)}{\\sum_{j=1}^{K}\\exp(z_{j}/T)}$。\n损失 $L$ 通过中间概率向量 $p$ 成为 $T$ 的函数。为了求导数 $\\frac{\\partial L}{\\partial T}$，我们应用多元链式法则。一个直接的应用是 $\\frac{\\partial L}{\\partial T} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial T}$。然而，一个更结构化的方法是定义一个代表缩放后 logit 的中间变量。\n\n令 $a_i = \\frac{z_i}{T}$。概率 $p_i$ 是 softmax 函数应用于向量 $a \\in \\mathbb{R}^K$ 的第 $i$ 个分量：$p_i = \\frac{\\exp(a_i)}{\\sum_{j=1}^K \\exp(a_j)}$。\n损失 $L$ 是向量 $a$ 的函数，而 $a$ 又是 $T$ 的函数。依赖关系结构是 $T \\rightarrow a \\rightarrow p \\rightarrow L$。我们可以按如下方式应用链式法则：\n$$ \\frac{\\partial L}{\\partial T} = \\sum_{i=1}^{K} \\frac{\\partial L}{\\partial a_i} \\frac{\\partial a_i}{\\partial T} $$\n我们将分别计算和式中的两项。\n\n1.  计算 $\\frac{\\partial L}{\\partial a_i}$：\n    损失函数可以用 $a_i$ 表示为：\n    $$ L = -\\sum_{j=1}^{K} y_j \\ln(p_j) = -\\sum_{j=1}^{K} y_j \\ln\\left(\\frac{\\exp(a_j)}{\\sum_{k=1}^K \\exp(a_k)}\\right) $$\n    利用对数的性质，我们展开这个表达式：\n    $$ L = -\\sum_{j=1}^{K} y_j \\left( a_j - \\ln\\left(\\sum_{k=1}^K \\exp(a_k)\\right) \\right) $$\n    $$ L = -\\sum_{j=1}^{K} y_j a_j + \\left(\\sum_{j=1}^{K} y_j\\right) \\ln\\left(\\sum_{k=1}^K \\exp(a_k)\\right) $$\n    由于 $y$ 是概率单纯形上的目标分布，我们有 $\\sum_{j=1}^{K} y_j = 1$。损失的表达式简化为：\n    $$ L = -\\sum_{j=1}^{K} y_j a_j + \\ln\\left(\\sum_{k=1}^K \\exp(a_k)\\right) $$\n    现在，我们对 $a_i$ 求偏导：\n    $$ \\frac{\\partial L}{\\partial a_i} = \\frac{\\partial}{\\partial a_i} \\left( -\\sum_{j=1}^{K} y_j a_j \\right) + \\frac{\\partial}{\\partial a_i} \\left( \\ln\\left(\\sum_{k=1}^K \\exp(a_k)\\right) \\right) $$\n    $$ \\frac{\\partial L}{\\partial a_i} = -y_i + \\frac{1}{\\sum_{k=1}^K \\exp(a_k)} \\cdot \\frac{\\partial}{\\partial a_i}\\left(\\sum_{k=1}^K \\exp(a_k)\\right) $$\n    $$ \\frac{\\partial L}{\\partial a_i} = -y_i + \\frac{\\exp(a_i)}{\\sum_{k=1}^K \\exp(a_k)} $$\n    根据 $p_i$ 的定义，我们得到了交叉熵损失关于 softmax 前激活值的导数的著名结果：\n    $$ \\frac{\\partial L}{\\partial a_i} = p_i - y_i $$\n\n2.  计算 $\\frac{\\partial a_i}{\\partial T}$：\n    这个导数可以从定义 $a_i = z_i/T$ 直接得出：\n    $$ \\frac{\\partial a_i}{\\partial T} = \\frac{\\partial}{\\partial T}\\left(\\frac{z_i}{T}\\right) = -\\frac{z_i}{T^2} $$\n\n3.  使用链式法则结合结果：\n    现在我们将 $\\frac{\\partial L}{\\partial a_i}$ 和 $\\frac{\\partial a_i}{\\partial T}$ 的表达式代回链式法则公式：\n    $$ \\frac{\\partial L}{\\partial T} = \\sum_{i=1}^{K} \\frac{\\partial L}{\\partial a_i} \\frac{\\partial a_i}{\\partial T} = \\sum_{i=1}^{K} (p_i - y_i) \\left(-\\frac{z_i}{T^2}\\right) $$\n    我们可以提出因子 $-\\frac{1}{T^2}$，因为它不依赖于求和索引 $i$：\n    $$ \\frac{\\partial L}{\\partial T} = -\\frac{1}{T^2} \\sum_{i=1}^{K} (p_i z_i - y_i z_i) $$\n    重新整理求和内的项，得到最终的简洁形式：\n    $$ \\frac{\\partial L}{\\partial T} = \\frac{1}{T^2} \\left( \\sum_{i=1}^{K} y_i z_i - \\sum_{i=1}^{K} p_i z_i \\right) $$\n    该表达式如题所求，依赖于 $z$、$y$、$p$ 和 $T$。用向量表示法，可以写成 $\\frac{1}{T^2}(y^Tz - p^Tz)$。\n\n接下来，我们讨论参数 $(s, T)$ 的可识别性。\n问题引入了一个标量参数 $s>0$ 来缩放 logit，使得概率变为 $p_{i}(s,T)=\\frac{\\exp(s z_{i}/T)}{\\sum_{j=1}^{K}\\exp(s z_{j}/T)}$。损失是这些概率的函数，$L(s,T) = -\\sum_{i=1}^K y_i \\ln(p_i(s,T))$。\n如果两个参数值的改变必然导致模型输出或损失函数的改变，那么这两个参数是联合可识别的。如果不同的参数值集合对所有输入都能产生完全相同的模型输出，那么这些参数是不可识别的。\n\n让我们考察 $p_i(s, T)$ 定义中指数函数的参数：该项为 $\\frac{s z_i}{T}$。这可以重写为 $\\frac{z_i}{T/s}$。\n让我们定义一个新参数，有效温度 $\\tau$，作为 $T$ 和 $s$ 的比值：\n$$ \\tau = \\frac{T}{s} $$\n有了这个定义，概率可以用 $\\tau$ 来表示：\n$$ p_i(s, T) = \\frac{\\exp(z_i / \\tau)}{\\sum_{j=1}^K \\exp(z_j / \\tau)} $$\n这表明概率向量 $p$ 仅依赖于单一的组合参数 $\\tau = T/s$，而不独立地依赖于 $s$ 和 $T$。\n由于损失函数 $L$ 是概率 $p$ 的函数，因此损失 $L$ 也仅依赖于比值 $\\tau = T/s$。\n这意味着，对于任何一对参数 $(s, T)$（其中 $s>0, T>0$），只要比值相同，即 $T/s = T'/s'$，它们将产生与任何其他对 $(s', T')$ 相同的概率向量 $p$，从而产生相同的损失 $L$。\n例如，让我们考虑一对参数 $(s_1, T_1)$。现在考虑另一对不同的参数 $(s_2, T_2) = (c s_1, c T_1)$，其中 $c$ 是任意常数且 $c > 0, c \\neq 1$。\n新的参数对与原始的不同：$(s_2, T_2) \\neq (s_1, T_1)$。\n然而，新参数对的比值为：\n$$ \\frac{T_2}{s_2} = \\frac{c T_1}{c s_1} = \\frac{T_1}{s_1} $$\n由于比值不变，有效温度 $\\tau$ 是相同的。这意味着对于任何输入 logit $z$，两对参数的概率向量 $p$ 是相同的。因此，对于任何目标分布 $y$，都有 $L(s_1, T_1; y) = L(s_2, T_2; y)$。\n因为我们可以找到无限多对不同的 $(s, T)$，它们对所有可能的输入都产生完全相同的损失值，所以参数 $s$ 和 $T$ 无法从损失中被联合识别。只有它们的比值，即有效温度 $\\tau=T/s$，可以被唯一确定。", "answer": "$$ \\boxed{\\frac{1}{T^2} \\left( \\sum_{i=1}^{K} y_i z_i - \\sum_{i=1}^{K} p_i z_i \\right)} $$", "id": "3190253"}, {"introduction": "链式法则不仅是计算工具，更是架构设计的蓝图，梯度反转层（GRL）便是其创造性应用的典范。GRL在反向传播中反转梯度，广泛用于领域自适应等高级任务，以学习不变性特征。在练习 [@problem_id:3190232] 中，你将计算通过GRL的梯度，亲身体验如何通过定制反向传播规则来实现复杂的学习动态。", "problem": "考虑一个简单的深度学习架构，它有一个共享的线性特征提取器和两个线性头：一个分类头和一个域头。设输入为列向量 $x \\in \\mathbb{R}^{2}$，共享特征为 $h \\in \\mathbb{R}^{2}$，分类 logits 为 $u \\in \\mathbb{R}^{2}$，域 logits 为 $v \\in \\mathbb{R}^{2}$。映射定义如下：\n$$h = W x$$\n$$u = A h$$\n$$\\text{GRL: } y = h \\text{ 在前向传播中；在反向传播中, } \\frac{\\partial L}{\\partial h} = -\\lambda \\frac{\\partial L}{\\partial y}$$\n$$v = B y$$\n总损失为\n$$L(x) = \\frac{1}{2}\\|u - c\\|^{2} + \\frac{1}{2}\\|v - d\\|^{2}$$\n其中，梯度反转层 (GRL) 是一个带有标量超参数 $\\lambda > 0$ 的梯度反转层。所有向量均为列向量，$\\|\\cdot\\|$ 表示欧几里得范数。\n\n仅使用基于第一性原理的多元链式法则（通过雅可比矩阵）和平方范数的基本导数，将梯度反转层在反向传播中建模为链式法则的乘数，并推导在下面给出的特定点处的上游梯度 $\\nabla_{x} L$。然后，报告 $\\nabla_{x} L$ 的第二个分量。\n\n使用以下数值：\n$$x = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix}, \\quad W = \\begin{pmatrix}2  1 \\\\ 0  -1\\end{pmatrix}, \\quad A = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}, \\quad B = \\begin{pmatrix}1  1 \\\\ 2  -1\\end{pmatrix},$$\n$$c = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}, \\quad d = \\begin{pmatrix}3 \\\\ -1\\end{pmatrix}, \\quad \\lambda = \\frac{3}{2}.$$\n\n你的最终答案必须是在指定点处 $\\nabla_{x} L$ 第二个分量的精确值。请勿对答案进行四舍五入。不需要单位。", "solution": "该问题经评估有效。其科学基础是多元微积分原理及其在深度学习中的应用，特别是在通过域对抗架构进行反向传播的背景下。该问题定义明确，提供了计算唯一解所需的所有函数、变量和常数。语言客观且数学上精确。\n\n目标是计算总损失 $L$ 相对于输入 $x$ 的梯度，记为 $\\nabla_{x} L$，然后报告其第二个分量。梯度向量 $\\nabla_{x} L$ 是导数行向量 $\\frac{\\partial L}{\\partial x}$ 的转置。我们将使用多元链式法则来推导 $\\frac{\\partial L}{\\partial x}$ 的表达式。\n\n总损失 $L$ 是 $u$ 和 $v$ 的函数，而 $u$ 和 $v$ 又依赖于 $h$， $h$ 依赖于 $x$。依赖关系图如下：$x \\to h$，从 $h$ 开始有两个分支：$h \\to u \\to L$ 和 $h \\to y \\to v \\to L$。总梯度是所有从输出 $L$ 到输入 $x$ 的路径贡献之和。\n\n总损失由 $L(u, v) = \\frac{1}{2}\\|u - c\\|^{2} + \\frac{1}{2}\\|v - d\\|^{2}$ 给出。设 $L_u = \\frac{1}{2}\\|u - c\\|^{2}$ 和 $L_v = \\frac{1}{2}\\|v - d\\|^{2}$。\n\n我们从损失 $L$ 开始应用链式法则，反向计算到输入 $x$。损失相对于 $x$ 的导数可以通过中间变量 $h$ 表示：\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial h} \\frac{\\partial h}{\\partial x}\n$$\n损失相对于共享特征 $h$ 的导数接收来自两个部分的贡献：分类头（通过 $u$）和域头（通过 $v$ 和 $y$）：\n$$\n\\frac{\\partial L}{\\partial h} = \\left(\\frac{\\partial L}{\\partial h}\\right)_{\\text{路径 } u} + \\left(\\frac{\\partial L}{\\partial h}\\right)_{\\text{路径 } v}\n$$\n让我们分别计算每个贡献。\n\n1.  来自自分类头的贡献 ($L \\to u \\to h$)：\n    损失相对于 $u$ 的导数是 $\\frac{\\partial L}{\\partial u} = (u-c)^T$。\n    函数 $u = Ah$ 的雅可比矩阵是 $\\frac{\\partial u}{\\partial h} = A$。\n    根据链式法则，此路径对 $\\frac{\\partial L}{\\partial h}$ 的贡献是：\n    $$\n    \\left(\\frac{\\partial L}{\\partial h}\\right)_{\\text{路径 } u} = \\frac{\\partial L}{\\partial u} \\frac{\\partial u}{\\partial h} = (u-c)^T A\n    $$\n\n2.  来自自域头的贡献 ($L \\to v \\to y \\to h$)：\n    该路径涉及梯度反转层 (GRL)。首先，我们找到到达 GRL 输出 $y$ 的梯度信号。\n    损失相对于 $v$ 的导数是 $\\frac{\\partial L}{\\partial v} = (v-d)^T$。\n    函数 $v = By$ 的雅可比矩阵是 $\\frac{\\partial v}{\\partial y} = B$。\n    损失相对于 $y$ 的导数是：\n    $$\n    \\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial v} \\frac{\\partial v}{\\partial y} = (v-d)^T B\n    $$\n    GRL 在前向传播中定义为映射 $y=h$。在反向传播中，它接收上游梯度 $\\frac{\\partial L}{\\partial y}$ 并输出相对于其输入 $h$ 的梯度。问题陈述将此行为指定为：``在反向传播中，$\\frac{\\partial L}{\\partial h} = -\\lambda \\frac{\\partial L}{\\partial y}$``。这定义了域头对 $h$ 处梯度的贡献。\n    $$\n    \\left(\\frac{\\partial L}{\\partial h}\\right)_{\\text{路径 } v} = -\\lambda \\frac{\\partial L}{\\partial y} = -\\lambda (v-d)^T B\n    $$\n    这将 GRL 建模为在此分支的链式法则计算中，用一个缩放的负单位矩阵 ($-\\lambda I$) 替换单位函数 ($I$) 的标准雅可比矩阵。\n\n结合两个贡献，损失相对于 $h$ 的总导数是：\n$$\n\\frac{\\partial L}{\\partial h} = (u-c)^T A - \\lambda (v-d)^T B\n$$\n最后，我们将此梯度反向传播到 $x$。函数 $h = Wx$ 的雅可比矩阵是 $\\frac{\\partial h}{\\partial x} = W$。\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial h} \\frac{\\partial h}{\\partial x} = \\left( (u-c)^T A - \\lambda (v-d)^T B \\right) W\n$$\n梯度向量 $\\nabla_x L$ 是行向量 $\\frac{\\partial L}{\\partial x}$ 的转置：\n$$\n\\nabla_x L = \\left( \\frac{\\partial L}{\\partial x} \\right)^T = W^T \\left( A^T(u-c) - \\lambda B^T(v-d) \\right)\n$$\n现在，我们代入给定的数值。首先，我们执行前向传播来计算中间向量的值。\n给定值：\n$$\nx = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix}, \\quad W = \\begin{pmatrix}2  1 \\\\ 0  -1\\end{pmatrix}, \\quad A = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}, \\quad B = \\begin{pmatrix}1  1 \\\\ 2  -1\\end{pmatrix},\n$$\n$$\nc = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}, \\quad d = \\begin{pmatrix}3 \\\\ -1\\end{pmatrix}, \\quad \\lambda = \\frac{3}{2}.\n$$\n\n前向传播：\n$$\nh = Wx = \\begin{pmatrix}2  1 \\\\ 0  -1\\end{pmatrix} \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = \\begin{pmatrix}2(1)+1(-2) \\\\ 0(1)+(-1)(-2)\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}\n$$\n$$\nu = Ah = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}\n$$\nGRL 的前向传播是 $y=h$：\n$$\ny = h = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}\n$$\n$$\nv = By = \\begin{pmatrix}1  1 \\\\ 2  -1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}1(0)+1(2) \\\\ 2(0)+(-1)(2)\\end{pmatrix} = \\begin{pmatrix}2 \\\\ -2\\end{pmatrix}\n$$\n\n反向传播（梯度计算）：\n首先，计算误差向量：\n$$\nu-c = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\n$$\n$$\nv-d = \\begin{pmatrix}2 \\\\ -2\\end{pmatrix} - \\begin{pmatrix}3 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix}\n$$\n现在，将这些代入 $\\nabla_x L$ 的表达式中。\n我们需要 $A$、$B$ 和 $W$ 的转置：\n$$\nA^T = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}, \\quad B^T = \\begin{pmatrix}1  2 \\\\ 1  -1\\end{pmatrix}, \\quad W^T = \\begin{pmatrix}2  0 \\\\ 1  -1\\end{pmatrix}\n$$\n计算梯度表达式中括号内的项：\n$$\nA^T(u-c) = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\n$$\n$$\nB^T(v-d) = \\begin{pmatrix}1  2 \\\\ 1  -1\\end{pmatrix} \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1(-1)+2(-1) \\\\ 1(-1)+(-1)(-1)\\end{pmatrix} = \\begin{pmatrix}-3 \\\\ 0\\end{pmatrix}\n$$\n括号内的完整项是：\n$$\nA^T(u-c) - \\lambda B^T(v-d) = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\frac{3}{2}\\begin{pmatrix}-3 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}-\\frac{9}{2} \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\frac{9}{2} \\\\ 1\\end{pmatrix}\n$$\n最后，计算 $\\nabla_x L$：\n$$\n\\nabla_x L = W^T \\left( A^T(u-c) - \\lambda B^T(v-d) \\right) = \\begin{pmatrix}2  0 \\\\ 1  -1\\end{pmatrix} \\begin{pmatrix}\\frac{9}{2} \\\\ 1\\end{pmatrix}\n$$\n$$\n\\nabla_x L = \\begin{pmatrix}2\\left(\\frac{9}{2}\\right) + 0(1) \\\\ 1\\left(\\frac{9}{2}\\right) + (-1)(1)\\end{pmatrix} = \\begin{pmatrix}9 \\\\ \\frac{9}{2}-1\\end{pmatrix} = \\begin{pmatrix}9 \\\\ \\frac{7}{2}\\end{pmatrix}\n$$\n损失函数相对于输入 $x$ 的梯度是 $\\nabla_x L = \\begin{pmatrix}9 \\\\ \\frac{7}{2}\\end{pmatrix}$。\n问题要求的是这个梯度向量的第二个分量。\n第二个分量是 $\\frac{7}{2}$。", "answer": "$$\\boxed{\\frac{7}{2}}$$", "id": "3190232"}, {"introduction": "面对神经网络中的量化等不可微操作，梯度几乎处处为零，这给训练带来了挑战。直通估计器（STE）是一种实用的启发式方法，它在反向传播时用一个代理梯度（如单位矩阵的雅可比矩阵）替代真实的零梯度。练习 [@problem_id:3190214] 让你分析这种近似如何通过链式法则影响最终的参数更新，并理解其引入的梯度偏差，这对于训练量化网络至关重要。", "problem": "考虑一个二维参数向量 $\\boldsymbol{x} \\in \\mathbb{R}^2$，它通过一个逐元素量化函数 $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$，其中 $\\mathrm{round}(\\cdot)$ 将每个分量映射到最近的整数。设输出为 $\\boldsymbol{y} = \\boldsymbol{q}(\\boldsymbol{x})$，并定义损失函数 $$L(\\boldsymbol{y}) = \\tfrac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}\\|_2^2$$ 其中 $\\boldsymbol{A} \\in \\mathbb{R}^{2 \\times 2}$ 和 $\\boldsymbol{b} \\in \\mathbb{R}^2$ 是固定的。假设训练过程使用直通估计器 (Straight-Through Estimator, STE)，此处定义为在前向传播中使用精确的 $\\boldsymbol{q}(\\boldsymbol{x})$，但在反向传播中将其雅可比矩阵近似为单位矩阵，即逐元素地 $\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} \\approx \\boldsymbol{I}$。您的目标是根据第一性原理，使用多元链式法则来刻画梯度 $\\nabla_{\\boldsymbol{x}} L$ 以及由 STE 引入的偏差。\n\n使用具体数值 $\\boldsymbol{A} = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}$，$\\boldsymbol{b} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ 和 $\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$ 进行计算。使用逐元素计算的 $\\boldsymbol{y} = \\mathrm{round}(\\boldsymbol{x})$，应用多元链式法则来分析在精确量化下和在 STE 近似下的 $\\nabla_{\\boldsymbol{x}} L$，并评估在给定点处由 STE 导致的 $\\frac{\\partial L}{\\partial \\boldsymbol{x}}$ 中的偏差。\n\n下列哪个陈述是正确的？\n\nA. 使用多元链式法则和直通估计器 (STE)，在给定点，梯度满足 $\\nabla_{\\boldsymbol{x}} L \\approx \\nabla_{\\boldsymbol{y}} L$，且得到的估计值等于 $\\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$。\n\nB. 在给定的 $\\boldsymbol{x}$ 处，精确梯度 $\\nabla_{\\boldsymbol{x}} L$ 等于 $\\boldsymbol{0}$，因为 $\\boldsymbol{q}(\\boldsymbol{x})$ 几乎处处都是局部常数，因此复合函数 $L(\\boldsymbol{q}(\\boldsymbol{x}))$ 在远离量化阈值的地方对于 $\\boldsymbol{x}$ 是局部平坦的。\n\nC. 在给定的 $\\boldsymbol{x}$ 处，STE 梯度是真实梯度 $\\nabla_{\\boldsymbol{x}} L$ 的一个无偏估计量。\n\nD. 对于给定的可逆矩阵 $\\boldsymbol{A}$，由 STE 引入的 $\\nabla_{\\boldsymbol{x}} L$ 中的偏差消失，当且仅当 $\\boldsymbol{A}\\boldsymbol{y} = \\boldsymbol{b}$。\n\nE. 在精确量化 $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$ 下，对几乎所有的 $\\boldsymbol{x}$，真实雅可比矩阵 $\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}}$ 等于单位矩阵 $\\boldsymbol{I}$。", "solution": "问题陈述被评估为有效。它在深度学习（特别是使用不可微函数进行训练）的背景下具有科学依据，提法恰当并提供了所有必要的数据，而且使用客观、精确的数学语言进行表述。没有矛盾、歧义或事实上的不健全之处。\n\n我们已知以下信息：\n- 参数向量 $\\boldsymbol{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\in \\mathbb{R}^2$。\n- 一个逐元素的量化函数 $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$，其中 $\\mathrm{round}(\\cdot)$ 将一个实数映射到最近的整数。\n- 输出 $\\boldsymbol{y} = \\boldsymbol{q}(\\boldsymbol{x})$。\n- 一个损失函数 $L(\\boldsymbol{y}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}\\|_2^2$。\n- 用于反向传播的直通估计器 (STE) 近似：$\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} \\approx \\boldsymbol{I}$，其中 $\\boldsymbol{I}$ 是 $2 \\times 2$ 单位矩阵。\n- 具体数值：$\\boldsymbol{A} = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}$，$\\boldsymbol{b} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，和 $\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$。\n\n我们的目标是同时使用精确导数和 STE 近似来分析梯度 $\\nabla_{\\boldsymbol{x}} L$。\n\n首先，我们使用多元链式法则来建立梯度的一般形式。损失 $L$ 是函数 $L(\\boldsymbol{y}(\\boldsymbol{x}))$ 的复合。链式法则给出了 $L$ 关于 $\\boldsymbol{x}$ 的梯度为：\n$$ \\nabla_{\\boldsymbol{x}} L = \\left(\\frac{\\partial \\boldsymbol{y}}{\\partial \\boldsymbol{x}}\\right)^T \\nabla_{\\boldsymbol{y}} L $$\n其中 $\\frac{\\partial \\boldsymbol{y}}{\\partial \\boldsymbol{x}}$ 是 $\\boldsymbol{y} = \\boldsymbol{q}(\\boldsymbol{x})$ 关于 $\\boldsymbol{x}$ 的雅可比矩阵。\n\n我们来计算损失关于 $\\boldsymbol{y}$ 的梯度，即 $\\nabla_{\\boldsymbol{y}} L$。\n损失为 $L(\\boldsymbol{y}) = \\frac{1}{2}(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b})^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b})$。将其展开得到：\n$$ L(\\boldsymbol{y}) = \\frac{1}{2}(\\boldsymbol{y}^T\\boldsymbol{A}^T\\boldsymbol{A}\\boldsymbol{y} - 2\\boldsymbol{b}^T\\boldsymbol{A}\\boldsymbol{y} + \\boldsymbol{b}^T\\boldsymbol{b}) $$\n对 $\\boldsymbol{y}$ 求梯度得到：\n$$ \\nabla_{\\boldsymbol{y}} L = \\frac{1}{2}(2\\boldsymbol{A}^T\\boldsymbol{A}\\boldsymbol{y} - 2\\boldsymbol{A}^T\\boldsymbol{b}) = \\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) $$\n\n现在，我们用给定的值执行前向传播：\n$\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$。\n输出 $\\boldsymbol{y}$ 是使用精确量化函数计算的：\n$$ \\boldsymbol{y} = \\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}\\left(\\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}\\right) = \\begin{bmatrix} \\mathrm{round}(0.3) \\\\ \\mathrm{round}(-1.7) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix} $$\n\n使用这个 $\\boldsymbol{y}$ 值，我们可以计算 $\\nabla_{\\boldsymbol{y}} L$：\n$$ \\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b} = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix} $$\n矩阵 $\\boldsymbol{A}$ 是对角矩阵，所以 $\\boldsymbol{A}^T = \\boldsymbol{A}$。\n$$ \\nabla_{\\boldsymbol{y}} L = \\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}\\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} $$\n\n现在，我们来分析关于 $\\boldsymbol{x}$ 的两种不同梯度。\n\n**1. 精确梯度 $\\nabla_{\\boldsymbol{x}} L$**\n函数 $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$ 是一个阶跃函数。它的导数几乎处处有定义且为零。不可微点出现在参数为半整数（例如 $0.5, -0.5, 1.5, \\dots$）的地方。\n给定点为 $\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$。两个分量都不是半整数。因此，在该点的一个小的开邻域内，函数 $\\boldsymbol{q}(\\boldsymbol{x})$ 是常数：$\\boldsymbol{q}(\\boldsymbol{x}) = \\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix}$。\n由于 $\\boldsymbol{q}(\\boldsymbol{x})$ 在此邻域内是常数，复合函数 $L(\\boldsymbol{q}(\\boldsymbol{x}))$ 也是常数。常数函数的梯度是零向量。\n因此，精确梯度为 $\\nabla_{\\boldsymbol{x}} L = \\boldsymbol{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n或者，$\\boldsymbol{q}(\\boldsymbol{x})$ 在此点的真实雅可比矩阵是零矩阵：\n$$ \\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\frac{d(\\mathrm{round}(x_1))}{dx_1}  0 \\\\ 0  \\frac{d(\\mathrm{round}(x_2))}{dx_2} \\end{bmatrix} = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix} $$\n使用链式法则，$\\nabla_{\\boldsymbol{x}} L = \\left(\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}}\\right)^T \\nabla_{\\boldsymbol{y}} L = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}^T \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n**2. 直通估计器 (STE) 梯度 $(\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}}$**\nSTE 使用精确的前向传播，但在反向传播中近似雅可比矩阵。该近似由 $\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} \\approx \\boldsymbol{I}$ 给出。\n将此应用于链式法则：\n$$ (\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} = (\\boldsymbol{I})^T \\nabla_{\\boldsymbol{y}} L = \\boldsymbol{I} \\nabla_{\\boldsymbol{y}} L = \\nabla_{\\boldsymbol{y}} L $$\n我们已经计算出 $\\nabla_{\\boldsymbol{y}} L = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$。\n因此，$(\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$。\n\n**3. STE 的偏差**\n估计器在该点的偏差是估计梯度与真实梯度之差：\n$$ \\text{Bias} = (\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} - \\nabla_{\\boldsymbol{x}} L = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} $$\n\n现在我们来评估每个选项。\n\n**A. 使用多元链式法则和直通估计器 (STE)，在给定点，梯度满足 $\\nabla_{\\boldsymbol{x}} L \\approx \\nabla_{\\boldsymbol{y}} L$，且得到的估计值等于 $\\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$。**\nSTE 近似定义为使用 $\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} \\approx \\boldsymbol{I}$。将其应用于链式法则，$(\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} = (\\boldsymbol{I})^T \\nabla_{\\boldsymbol{y}} L = \\nabla_{\\boldsymbol{y}} L$。这在 STE 下建立了关系 $\\nabla_{\\boldsymbol{x}} L \\approx \\nabla_{\\boldsymbol{y}} L$。我们的计算表明 $\\nabla_{\\boldsymbol{y}} L = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$。该陈述与我们的推导完全一致。\n**结论：正确。**\n\n**B. 在给定的 $\\boldsymbol{x}$ 处，精确梯度 $\\nabla_{\\boldsymbol{x}} L$ 等于 $\\boldsymbol{0}$，因为 $\\boldsymbol{q}(\\boldsymbol{x})$ 几乎处处都是局部常数，因此复合函数 $L(\\boldsymbol{q}(\\boldsymbol{x}))$ 在远离量化阈值的地方对于 $\\boldsymbol{x}$ 是局部平坦的。**\n如上文分析，点 $\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$ 不在量化阈值（一个半整数）上。因此，$\\boldsymbol{q}(\\boldsymbol{x})$ 在 $\\boldsymbol{x}$ 的一个开邻域内是常数。因此，复合函数 $L(\\boldsymbol{q}(\\boldsymbol{x}))$ 也是局部常数，其梯度为零向量 $\\boldsymbol{0}$。选项中提供的推理是正确的。\n**结论：正确。**\n\n**C. 在给定的 $\\boldsymbol{x}$ 处，STE 梯度是真实梯度 $\\nabla_{\\boldsymbol{x}} L$ 的一个无偏估计量。**\n在确定性设置中，如果一个估计量在某点等于真实值，则该估计量在该点是无偏的。\nSTE 梯度为 $(\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$。\n真实梯度为 $\\nabla_{\\boldsymbol{x}} L = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n由于 $\\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} \\neq \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，该估计量在这一点上是有偏的。\n**结论：不正确。**\n\n**D. 对于给定的可逆矩阵 $\\boldsymbol{A}$，由 STE 引入的 $\\nabla_{\\boldsymbol{x}} L$ 中的偏差消失，当且仅当 $\\boldsymbol{A}\\boldsymbol{y} = \\boldsymbol{b}$。**\n对于任何不在量化阈值上的 $\\boldsymbol{x}$，偏差为：\n$$ \\text{Bias} = (\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} - \\nabla_{\\boldsymbol{x}} L = \\nabla_{\\boldsymbol{y}} L - \\boldsymbol{0} = \\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) $$\n偏差在 $\\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) = \\boldsymbol{0}$ 时消失。\n问题指明 $\\boldsymbol{A}$ 是可逆的。给定的矩阵 $\\boldsymbol{A} = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}$ 的行列式为 2，因此它确实是可逆的。如果 $\\boldsymbol{A}$ 可逆，它的转置 $\\boldsymbol{A}^T$ 也可逆。对于一个可逆矩阵 $\\boldsymbol{M}$，方程 $\\boldsymbol{M}\\boldsymbol{z} = \\boldsymbol{0}$ 成立当且仅当 $\\boldsymbol{z} = \\boldsymbol{0}$。\n将此应用于此处，设 $\\boldsymbol{M} = \\boldsymbol{A}^T$ 和 $\\boldsymbol{z} = \\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}$，我们得到 $\\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) = \\boldsymbol{0}$ 当且仅当 $\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b} = \\boldsymbol{0}$，这等价于 $\\boldsymbol{A}\\boldsymbol{y} = \\boldsymbol{b}$。条件 $\\boldsymbol{A}\\boldsymbol{y} = \\boldsymbol{b}$ 对应于损失达到其可能的最小值 0。\n**结论：正确。**\n\n**E. 在精确量化 $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$ 下，对几乎所有的 $\\boldsymbol{x}$，真实雅可比矩阵 $\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}}$ 等于单位矩阵 $\\boldsymbol{I}$。**\n函数 $\\boldsymbol{q}(\\boldsymbol{x})$ 是一个逐元素 `round` 函数的向量。单个 `round` 函数的导数 $\\frac{d}{dt}\\mathrm{round}(t)$，对于所有非半整数的 $t$ 来说是 $0$，而在半整数处无定义。它永远不等于 $1$。\n雅可比矩阵是：\n$$ \\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\frac{\\partial q_1}{\\partial x_1}  \\frac{\\partial q_1}{\\partial x_2} \\\\ \\frac{\\partial q_2}{\\partial x_1}  \\frac{\\partial q_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} \\frac{d}{dx_1}\\mathrm{round}(x_1)  0 \\\\ 0  \\frac{d}{dx_2}\\mathrm{round}(x_2) \\end{bmatrix} $$\n对于几乎所有的 $\\boldsymbol{x}$（即那些没有半整数分量的 $\\boldsymbol{x}$），这个雅可比矩阵是零矩阵 $\\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$。它不是单位矩阵 $\\boldsymbol{I}$。关于真实雅可比矩阵等于 $\\boldsymbol{I}$ 的陈述是 STE 做出的核心（且错误的）假设，而不是真实雅可比矩阵本身的性质。\n**结论：不正确。**", "answer": "$$\\boxed{ABD}$$", "id": "3190214"}]}