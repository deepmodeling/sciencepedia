{"hands_on_practices": [{"introduction": "梯度下降是优化领域的基石，但其成功在很大程度上取决于步长（学习率）的选择。本练习将带你回归第一性原理，从 $L$-光滑的定义出发，推导出一个理论上可靠且实用的步长选择策略——回溯线搜索。通过将此方法应用于一个具体的逻辑回归问题 [@problem_id:3126951]，你将亲身体验如何将核心的凸优化理论转化为有效的算法组件。", "problem": "您正在研究用于训练二元逻辑回归模型的基于梯度的优化方法，并希望从凸优化的第一性原理出发，论证一种实用的步长选择规则。考虑一个可微凸函数 $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$，其梯度是 $L$-利普希茨连续的（即 $f$ 是 $L$-平滑的），这意味着对于所有 $x,y\\in\\mathbb{R}^{d}$，梯度满足 $\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\|$。从这个定义和标准微积分出发，推导出一个梯度步长 $x^{+}=x-\\alpha \\nabla f(x)$ 的充分下降条件，该条件仅依赖于 $f(x)$、$\\nabla f(x)$ 和 $\\alpha$，然后用它来构建一个回溯线搜索规则，保证当 $\\alpha\\leq 1/L$ 时该步长被接受。\n\n将您推导的规则应用于一个不带截距项的二元逻辑回归问题的二元交叉熵（BCE）损失。数据集包含 $n=3$ 个样本：\n- $x_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$，对应 $y_{1}=1$，\n- $x_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix}$，对应 $y_{2}=0$，\n- $x_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$，对应 $y_{3}=1$，\n其中模型为 $p_{\\theta}(y=1\\mid x)=\\sigma(\\theta^{\\top}x)$，逻辑 sigmoid 函数为 $\\sigma(t)=\\frac{1}{1+\\exp(-t)}$，经验损失为\n$$\nf(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\Big(-y_{i}\\ln \\sigma(\\theta^{\\top}x_{i})-(1-y_{i})\\ln(1-\\sigma(\\theta^{\\top}x_{i}))\\Big).\n$$\n从 $\\theta_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$ 开始梯度法，并使用您推导的充分下降规则，沿着最速下降方向 $-\\nabla f(\\theta_{0})$ 执行回溯线搜索。使用以下回溯参数：初始步长 $\\alpha_{0}=1$，收缩因子 $\\tau=\\frac{1}{2}$，以及充分下降常数 $c=\\frac{1}{4}$。确定在第一次迭代中线搜索所接受的步长 $\\alpha$。\n\n将您的最终答案表示为一个无量纲数。将您的答案四舍五入到四位有效数字。", "solution": "问题包含两部分。首先，我们必须为一个 $L$-平滑凸函数上的梯度下降法推导一个充分下降条件，并用它来构建一个回溯线搜索规则。其次，我们必须将此规则应用于一个特定的二元逻辑回归问题。\n\n第一部分：充分下降条件和回溯规则的推导\n\n令 $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ 为一个可微凸函数，其梯度是 $L$-利普希茨连续的。梯度的 $L$-利普希茨连续性，也称为 $L$-平滑性，由以下不等式定义：\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\| \\quad \\forall x,y\\in\\mathbb{R}^{d}\n$$\n其中 $L > 0$ 是利普希茨常数。该性质的一个关键推论是下降引理，或称函数 $f$ 的二次上界。我们首先推导这个引理。\n\n考虑函数 $g(t) = f(x+t(y-x))$，其中 $t \\in [0,1]$。根据链式法则，其导数为 $g'(t) = \\nabla f(x+t(y-x))^{\\top}(y-x)$。微积分基本定理表明 $g(1) - g(0) = \\int_{0}^{1} g'(t) dt$。用 $f$ 表示，即：\n$$\nf(y) - f(x) = \\int_{0}^{1} \\nabla f(x+t(y-x))^{\\top}(y-x) dt\n$$\n我们从两边减去项 $\\nabla f(x)^{\\top}(y-x)$，该项可以写成 $\\int_{0}^{1} \\nabla f(x)^{\\top}(y-x) dt$：\n$$\nf(y) - f(x) - \\nabla f(x)^{\\top}(y-x) = \\int_{0}^{1} (\\nabla f(x+t(y-x)) - \\nabla f(x))^{\\top}(y-x) dt\n$$\n对被积函数应用柯西-施瓦茨不等式，我们得到：\n$$\n(\\nabla f(x+t(y-x)) - \\nabla f(x))^{\\top}(y-x) \\leq \\|\\nabla f(x+t(y-x)) - \\nabla f(x)\\| \\|y-x\\|\n$$\n对梯度项使用 $L$-平滑性：\n$$\n\\|\\nabla f(x+t(y-x)) - \\nabla f(x)\\| \\leq L\\|(x+t(y-x)) - x\\| = L\\|t(y-x)\\| = Lt\\|y-x\\|\n$$\n因为 $t \\geq 0$。结合这些不等式，我们对被积函数进行放缩：\n$$\n(\\nabla f(x+t(y-x)) - \\nabla f(x))^{\\top}(y-x) \\leq Lt\\|y-x\\|^{2}\n$$\n将此界限从 $t=0$ 积分到 $t=1$：\n$$\n\\int_{0}^{1} (\\nabla f(x+t(y-x)) - \\nabla f(x))^{\\top}(y-x) dt \\leq \\int_{0}^{1} Lt\\|y-x\\|^{2} dt = L\\|y-x\\|^{2} \\int_{0}^{1} t dt = \\frac{L}{2}\\|y-x\\|^{2}\n$$\n这就得到了 $f(y)$ 的二次上界：\n$$\nf(y) \\leq f(x) + \\nabla f(x)^{\\top}(y-x) + \\frac{L}{2}\\|y-x\\|^{2}\n$$\n现在，我们考虑一个梯度下降步，其中下一个迭代点是 $x^{+} = x - \\alpha \\nabla f(x)$，步长 $\\alpha > 0$。我们在上面的不等式中令 $y=x^{+}$，所以 $y-x = -\\alpha\\nabla f(x)$。\n$$\nf(x^{+}) \\leq f(x) + \\nabla f(x)^{\\top}(-\\alpha\\nabla f(x)) + \\frac{L}{2}\\|-\\alpha\\nabla f(x)\\|^{2}\n$$\n$$\nf(x^{+}) \\leq f(x) - \\alpha\\|\\nabla f(x)\\|^{2} + \\frac{L\\alpha^{2}}{2}\\|\\nabla f(x)\\|^{2}\n$$\n这个不等式，$f(x - \\alpha\\nabla f(x)) \\leq f(x) - \\alpha(1 - \\frac{L\\alpha}{2})\\|\\nabla f(x)\\|^2$，是一个从第一性原理推导出的充分下降条件。\n\n接下来，我们构建一个回溯线搜索规则。标准规则，即 Armijo-Goldstein 条件，要求步长 $\\alpha$ 满足：\n$$\nf(x - \\alpha\\nabla f(x)) \\leq f(x) - c\\alpha\\|\\nabla f(x)\\|^{2}\n$$\n其中常数 $c \\in (0,1)$。线搜索过程从一个初始步长 $\\alpha$ 开始，并以因子 $\\tau \\in (0,1)$ 不断减小它，直到满足此条件。\n\n为保证此搜索会终止，我们必须证明对于足够小的 $\\alpha$，该条件最终会得到满足。从我们推导的二次上界来看，如果满足以下条件，Armijo 条件就会成立：\n$$\nf(x) - \\alpha\\|\\nabla f(x)\\|^{2} + \\frac{L\\alpha^{2}}{2}\\|\\nabla f(x)\\|^{2} \\leq f(x) - c\\alpha\\|\\nabla f(x)\\|^{2}\n$$\n假设 $\\nabla f(x) \\neq 0$，我们可以将其简化为：\n$$\n-1 + \\frac{L\\alpha}{2} \\leq -c \\quad \\implies \\quad \\frac{L\\alpha}{2} \\leq 1-c \\quad \\implies \\quad \\alpha \\leq \\frac{2(1-c)}{L}\n$$\n问题要求一个能保证在 $\\alpha \\leq 1/L$ 时被接受的规则。为了让我们的条件对任何 $\\alpha \\leq 1/L$ 都成立，我们需要区间 $[0, 1/L]$ 是 $[0, 2(1-c)/L]$ 的一个子集。这要求：\n$$\n\\frac{1}{L} \\leq \\frac{2(1-c)}{L} \\quad \\implies \\quad 1 \\leq 2(1-c) \\quad \\implies \\quad \\frac{1}{2} \\leq 1-c \\quad \\implies \\quad c \\leq \\frac{1}{2}\n$$\n因此，使用 Armijo 条件和任何参数 $c \\in (0, 1/2]$ 的回溯线搜索规则是一个有效的构造，因为它保证任何步长 $\\alpha \\leq 1/L$ 都将被接受。\n\n第二部分：在二元逻辑回归中的应用\n\n经验损失函数是 $n=3$ 个样本上的平均二元交叉熵（BCE）：\n$$\nf(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\Big(-y_{i}\\ln \\sigma(\\theta^{\\top}x_{i})-(1-y_{i})\\ln(1-\\sigma(\\theta^{\\top}x_{i}))\\Big)\n$$\nBCE损失 $-y_i\\log(p_i)-(1-y_i)\\log(1-p_i)$（其中 $p_i = \\sigma(\\theta^T x_i)$）。当 $y_i=1$ 时，损失为 $-\\log(\\sigma(z)) = \\log(1+e^{-z})$。当 $y_i=0$ 时，损失为 $-\\log(1-\\sigma(z)) = \\log(1+e^z)$，其中 $z=\\theta^T x_i$。\n\n让我们使用给定的数据：\n- $x_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}, y_{1}=1 \\implies \\text{loss}_1(\\theta) = \\ln(1+e^{-\\theta_1})$\n- $x_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix}, y_{2}=0 \\implies \\text{loss}_2(\\theta) = \\ln(1+e^{\\theta_2})$\n- $x_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}, y_{3}=1 \\implies \\text{loss}_3(\\theta) = \\ln(1+e^{-(\\theta_1+\\theta_2)})$\n\n总损失函数为：\n$$\nf(\\theta) = \\frac{1}{3} \\left( \\ln(1+e^{-\\theta_1}) + \\ln(1+e^{\\theta_2}) + \\ln(1+e^{-(\\theta_1+\\theta_2)}) \\right)\n$$\n我们从 $\\theta_0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$ 开始。初始损失为：\n$$\nf(\\theta_0) = \\frac{1}{3}(\\ln(1+e^0) + \\ln(1+e^0) + \\ln(1+e^0)) = \\frac{1}{3}(3\\ln 2) = \\ln 2\n$$\nBCE 损失的梯度为 $\\nabla f(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}(\\sigma(\\theta^{\\top}x_i)-y_i)x_i$。在 $\\theta_0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$ 处，对于所有 $i$ 我们有 $\\theta_0^{\\top}x_i=0$，并且 $\\sigma(0) = \\frac{1}{1+e^0} = \\frac{1}{2}$。\n$$\n\\nabla f(\\theta_0) = \\frac{1}{3} \\left( \\left(\\frac{1}{2}-1\\right)x_1 + \\left(\\frac{1}{2}-0\\right)x_2 + \\left(\\frac{1}{2}-1\\right)x_3 \\right)\n$$\n$$\n\\nabla f(\\theta_0) = \\frac{1}{3} \\left( -\\frac{1}{2}\\begin{pmatrix}1\\\\0\\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix}0\\\\1\\end{pmatrix} - \\frac{1}{2}\\begin{pmatrix}1\\\\1\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}-1/2 - 1/2 \\\\ 1/2 - 1/2\\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix}-1\\\\0\\end{pmatrix} = \\begin{pmatrix}-1/3\\\\0\\end{pmatrix}\n$$\n梯度的范数平方为 $\\|\\nabla f(\\theta_0)\\|^2 = (-1/3)^2 + 0^2 = 1/9$。\n\n我们使用初始步长 $\\alpha_0=1$、收缩因子 $\\tau=1/2$ 和常数 $c=1/4$ 来执行回溯线搜索。需要检查的条件是：\n$$\nf(\\theta_0 - \\alpha \\nabla f(\\theta_0)) \\leq f(\\theta_0) - c \\alpha \\|\\nabla f(\\theta_0)\\|^2\n$$\n让我们测试初始步长 $\\alpha=1$。候选点是：\n$$\n\\theta_1 = \\theta_0 - (1)\\nabla f(\\theta_0) = \\begin{pmatrix}0\\\\0\\end{pmatrix} - \\begin{pmatrix}-1/3\\\\0\\end{pmatrix} = \\begin{pmatrix}1/3\\\\0\\end{pmatrix}\n$$\n条件的左侧（LHS）是在这个新点的损失：\n$$\nf(\\theta_1) = f\\left(\\begin{pmatrix}1/3\\\\0\\end{pmatrix}\\right) = \\frac{1}{3} \\left( \\ln(1+e^{-1/3}) + \\ln(1+e^{0}) + \\ln(1+e^{-(1/3+0)}) \\right)\n$$\n$$\nf(\\theta_1) = \\frac{1}{3} \\left( 2\\ln(1+e^{-1/3}) + \\ln 2 \\right)\n$$\n条件的右侧（RHS）是：\n$$\nf(\\theta_0) - c\\alpha\\|\\nabla f(\\theta_0)\\|^2 = \\ln 2 - \\frac{1}{4}(1)\\left(\\frac{1}{9}\\right) = \\ln 2 - \\frac{1}{36}\n$$\n对于 $\\alpha=1$ 的条件是：\n$$\n\\frac{1}{3} \\left( 2\\ln(1+e^{-1/3}) + \\ln 2 \\right) \\leq \\ln 2 - \\frac{1}{36}\n$$\n乘以3并重新整理：\n$$\n2\\ln(1+e^{-1/3}) + \\ln 2 \\leq 3\\ln 2 - \\frac{3}{36} \\implies 2\\ln(1+e^{-1/3}) \\leq 2\\ln 2 - \\frac{1}{12}\n$$\n$$\n\\ln(1+e^{-1/3}) \\leq \\ln 2 - \\frac{1}{24}\n$$\n数值上，$\\ln(2) \\approx 0.693147$ 且 $1/24 \\approx 0.041667$，所以右侧约为 $0.65148$。\n对于左侧，$e^{-1/3} \\approx 0.716531$，所以 $1+e^{-1/3} \\approx 1.716531$。那么 $\\ln(1.716531) \\approx 0.540316$。\n不等式为 $0.540316 \\leq 0.65148$，这是成立的。\n\n由于初始步长 $\\alpha_0=1$ 满足条件，线搜索立即终止并接受 $\\alpha=1$。\n问题要求答案四舍五入到四位有效数字。确切答案是 $1$，保留四位有效数字即为 $1.000$。", "answer": "$$\\boxed{1.000}$$", "id": "3126951"}, {"introduction": "许多机器学习问题，例如权重稀疏性或概率分布约束，都涉及在特定可行集内进行优化。Frank-Wolfe算法为这类约束优化问题提供了一种优雅且高效的解决方案，它通过线性最小化预言机（LMO）巧妙地避免了复杂的投影操作。本练习 [@problem_id:3126940] 是一个全面的实践，将引导你从零开始为多项逻辑回归推导并实现完整的Frank-Wolfe算法，深入理解其核心机制。", "problem": "要求您推导、设计并实现一个完整的 Frank-Wolfe (FW) 优化方法，用于解决深度学习中的一个凸学习问题：在每个类别权重向量上带单纯形约束的多项式逻辑回归。您的最终答案必须是一个单一、可运行的 Python 程序，该程序为固定的测试套件计算结果，并按指定格式在一行中打印出来。\n\n问题设置。考虑一个数据集，其特征矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，标签为 $y \\in \\{1,2,\\dots,C\\}^n$。模型参数为一个权重矩阵 $W \\in \\mathbb{R}^{d \\times C}$，其列向量被约束在概率单纯形内。也就是说，对于每个类别 $c \\in \\{1,2,\\dots,C\\}$，列向量 $W_{:,c}$ 满足对所有的 $i \\in \\{1,2,\\dots,d\\}$ 都有 $W_{i,c} \\ge 0$ 以及 $\\sum_{i=1}^d W_{i,c} = 1$。令 $Z = X W \\in \\mathbb{R}^{n \\times C}$ 为 logits 矩阵，并逐行定义 softmax 概率为 $p_{k,c}(W) = \\exp(Z_{k,c}) \\big/ \\sum_{j=1}^C \\exp(Z_{k,j})$，其中 $k \\in \\{1,\\dots,n\\}$ 为样本索引，$c \\in \\{1,\\dots,C\\}$ 为类别索引。目标函数是平均多项式交叉熵损失\n$$\nf(W) = \\frac{1}{n}\\sum_{k=1}^n \\left(- \\log p_{k,y_k}(W) \\right).\n$$\n您必须推导梯度并证明其凸性，以证明在可行集 $\\mathcal{D} = \\Delta^d \\times \\cdots \\times \\Delta^d$（$C$ 个单纯形的笛卡尔积）上使用 Frank-Wolfe 方法是合理的，其中 $\\Delta^d = \\{u \\in \\mathbb{R}^d \\mid u \\ge 0, \\ \\sum_{i=1}^d u_i = 1 \\}$。\n\n缩略语定义。Frank-Wolfe (FW) 方法是一种用于约束凸优化的无投影算法。线性最小化预言机 (Linear Minimization Oracle, LMO) 解决 $\\arg\\min_{S \\in \\mathcal{D}} \\langle S, G \\rangle$ 问题，其中 $G$ 是给定的梯度矩阵，$\\langle \\cdot,\\cdot\\rangle$ 表示 Frobenius 内积。多项式逻辑回归 (Multinomial Logistic Regression, MLR) 使用 softmax 进行多类别分类。\n\n您必须完成以下任务。\n\n$1.$ 从 softmax 和交叉熵的定义出发，使用基础微积分和链式法则推导关于 $W$ 的梯度 $\\nabla f(W)$。不要假设任何预先记忆的梯度公式。\n\n$2.$ 通过引用 log-sum-exp 函数的凸性以及复合一个仿射映射保持凸性这一事实，来证明 $f(W)$ 相对于 $W$ 是凸函数。\n\n$3.$ 为可行集 $\\mathcal{D} = \\Delta^d \\times \\cdots \\times \\Delta^d$ 推导线性最小化预言机 (LMO)。证明在一个单纯形上最小化一个线性函数会选择一个极点（一个标准基向量），并由此推断如何为一个给定的 $G \\in \\mathbb{R}^{d \\times C}$ 构建使 $\\langle S, G \\rangle$ 最小化的 $S \\in \\mathbb{R}^{d \\times C}$。\n\n$4.$ 实现包含以下组件的 Frank-Wolfe 方法：\n- 初始化 $W_0$，使其每一列都是均匀向量，即 $W_{0,i,c} = 1/d$。\n- 在第 $t$ 次迭代时，计算梯度 $G_t = \\nabla f(W_t)$。\n- 调用 LMO 以获得 $S_t \\in \\arg\\min_{S \\in \\mathcal{D}} \\langle S, G_t \\rangle$。\n- 形成 FW 方向 $D_t = S_t - W_t$。\n- 对 $\\gamma_t \\in [0,1]$ 执行精确的一维线搜索，以最小化 $\\phi_t(\\gamma) = f(W_t + \\gamma D_t)$。\n- 更新 $W_{t+1} = W_t + \\gamma_t D_t$。\n- 计算 Frank-Wolfe 对偶间隙 $g_t = \\langle W_t - S_t, G_t \\rangle$，如果 $g_t \\le \\varepsilon$ 或达到最大迭代次数，则停止。\n\n数值稳定性要求。通过在进行指数运算前从 logits 中减去行最大值，来实现数值稳定的 softmax 公式。\n\n测试套件和参数。您的程序必须在以下固定的测试用例上运行 FW 算法，并汇总指定的输出。\n\n- 线性最小化预言机测试：\n  - 测试 A (正常路径): 对于 $\\mathbb{R}^5$ 中的向量 $c = [1.5, -2.0, 0.0, 3.0, -1.0]$，在 $\\Delta^5$ 上的 LMO 必须返回最小分量处的极点，即索引为 $\\arg\\min_i c_i$ 的标准基向量。将返回的索引记录为整数（在代码中使用从零开始的索引）。\n  - 测试 B (平局/边界情况): 对于 $\\mathbb{R}^3$ 中的向量 $c = [2.0, 2.0, 2.0]$，存在多个最小值点。您的实现必须确定性地选择最小化器中最小的索引。将该索引记录为整数（从零开始）。\n\n- Frank-Wolfe 优化测试：\n  - 数据。使用 $n = 6$，$d = 3$，$C = 3$。令\n    $$\n    X = \\begin{bmatrix}\n    1  0  0\\\\\n    0  1  0\\\\\n    0  0  1\\\\\n    1  0  0\\\\\n    0  1  0\\\\\n    0  0  1\n    \\end{bmatrix}, \\quad\n    y = \\begin{bmatrix}\n    1\\\\2\\\\3\\\\1\\\\2\\\\3\n    \\end{bmatrix}.\n    $$\n    本问题不使用角度，因此不需要角度单位。不涉及物理单位。\n  - 算法设置。使用最大 $T = 200$ 次迭代和容差 $\\varepsilon = 10^{-6}$。将 $W_0$ 初始化，使其每一列都等于 $\\Delta^3$ 中的均匀向量。\n  - 需要记录的输出：\n    - 测试 C：运行 FW 后，记录最终对偶间隙 $g_T \\le 5 \\times 10^{-4}$ 是否成立，结果为布尔值。\n    - 测试 D：将最终目标值 $f(W_T)$ 记录为浮点数，四舍五入到六位小数。\n    - 测试 E：记录 $f(W_T)  f(W_0)$ 是否成立，结果为布尔值。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序如下：\n$[ \\text{测试 A 整数索引}, \\text{测试 C 布尔值}, \\text{测试 B 整数索引}, \\text{测试 D 浮点数}, \\text{测试 E 布尔值} ]$。\n例如，一个有效的输出可能看起来像 $[1,True,0,0.551445,True]$。\n\n约束和指南。\n- 仅使用执行环境中指定的标准数值库。\n- 确保您的代码是自包含的，不使用外部输入，并严格按照要求的格式打印一行输出。\n- 您的解决方案中的推导必须从上面提供的定义以及诸如 log-sum-exp 的凸性和仿射函数性质等标准的、广为接受的事实出发，而不是从目标公式本身出发。", "solution": "问题陈述经评估为 **有效**。它提出了一个定义明确、具有科学依据的约束凸优化问题，并应用于多项式逻辑回归。所有数据、参数、定义和约束都已提供，构成了一个自包含且一致的规范。所要求的任务是 Frank-Wolfe 算法的形式推导和实现，这些都是机器学习和优化领域的标准主题。\n\n### 1. 目标函数的梯度推导\n\n目标函数是平均多项式交叉熵损失函数：\n$$\nf(W) = \\frac{1}{n}\\sum_{k=1}^n L_k(W) \\quad \\text{其中} \\quad L_k(W) = -\\log p_{k,y_k}(W)\n$$\n参数是权重矩阵 $W \\in \\mathbb{R}^{d \\times C}$。logits 是 $Z = XW$。softmax 概率是 $p_{k,c}(W) = \\exp(Z_{k,c}) \\big/ \\sum_{j=1}^C \\exp(Z_{k,j})$。\n\n我们来展开单个数据样本 $k$ 的损失项 $L_k(W)$：\n$$\nL_k(W) = -\\log \\left( \\frac{\\exp(Z_{k,y_k})}{\\sum_{j=1}^C \\exp(Z_{k,j})} \\right) = -Z_{k,y_k} + \\log\\left(\\sum_{j=1}^C \\exp(Z_{k,j})\\right)\n$$\n为了找到梯度 $\\nabla f(W)$，我们计算关于矩阵 $W$ 的每个元素 $W_{ab}$ 的偏导数，其中 $a \\in \\{1, \\dots, d\\}$ 且 $b \\in \\{1, \\dots, C\\}$。\n$$\n\\frac{\\partial f(W)}{\\partial W_{ab}} = \\frac{1}{n} \\sum_{k=1}^n \\frac{\\partial L_k(W)}{\\partial W_{ab}}\n$$\n我们应用链式法则。损失 $L_k$ 通过 logits $Z_{kc}$ 依赖于 $W_{ab}$。\n$$\n\\frac{\\partial L_k(W)}{\\partial W_{ab}} = \\sum_{c=1}^C \\frac{\\partial L_k(W)}{\\partial Z_{kc}} \\frac{\\partial Z_{kc}}{\\partial W_{ab}}\n$$\n首先，我们求一个 logit 相对于一个权重的导数。logit $Z_{kc}$ 定义为 $Z_{kc} = \\sum_{m=1}^d X_{km} W_{mc}$。\n$$\n\\frac{\\partial Z_{kc}}{\\partial W_{ab}} = \\frac{\\partial}{\\partial W_{ab}} \\left( \\sum_{m=1}^d X_{km} W_{mc} \\right) = X_{ka} \\delta_{cb}\n$$\n其中 $\\delta_{cb}$ 是克罗内克 δ。仅当类别 $c=b$ 时，导数才非零。\n\n接下来，我们求 $L_k$ 相对于一个 logit $Z_{kc}$ 的导数。\n$$\n\\frac{\\partial L_k(W)}{\\partial Z_{kc}} = \\frac{\\partial}{\\partial Z_{kc}} \\left( -Z_{ky_k} + \\log\\left(\\sum_{j=1}^C \\exp(Z_{kj})\\right) \\right) = -\\delta_{c,y_k} + \\frac{\\exp(Z_{kc})}{\\sum_{j=1}^C \\exp(Z_{kj})} = p_{kc} - \\delta_{c,y_k}\n$$\n这里我们用 $y_k$ 表示样本 $k$ 的真实类别标签，如果 $c=y_k$，则 $\\delta_{c,y_k}$ 为 1，否则为 0。\n\n使用链式法则合并这些结果：\n$$\n\\frac{\\partial L_k(W)}{\\partial W_{ab}} = \\sum_{c=1}^C (p_{kc} - \\delta_{c,y_k})(X_{ka}\\delta_{cb}) = (p_{kb} - \\delta_{b,y_k})X_{ka}\n$$\n最后，我们对所有 $n$ 个样本求平均，得到梯度矩阵的分量：\n$$\n(\\nabla f(W))_{ab} = \\frac{\\partial f(W)}{\\partial W_{ab}} = \\frac{1}{n} \\sum_{k=1}^n X_{ka} (p_{kb} - \\delta_{b,y_k})\n$$\n这个表达式可以紧凑地写成矩阵形式。令 $P$ 为元素为 $p_{kc}$ 的 $n \\times C$ 矩阵，令 $Y$ 为标签的 $n \\times C$ one-hot 编码矩阵，其中 $Y_{kc} = \\delta_{c,y_k}$。则梯度为：\n$$\n\\nabla f(W) = \\frac{1}{n} X^T (P - Y)\n$$\n\n### 2. 目标函数的凸性\n\n我们需要证明 $f(W)$ 是 $W$ 的凸函数。由于 $f(W)$ 是每个样本损失函数 $L_k(W)$ 的正权重和，因此只需证明每个 $L_k(W)$ 对于 $W$ 是凸的。\n$L_k(W)$ 函数可以看作是函数的复合。令 $Z_{k,:} \\in \\mathbb{R}^C$ 为样本 $k$ 的 logit 向量。\n$L_k(Z_{k,:}) = \\log\\left(\\sum_{j=1}^C \\exp(Z_{kj})\\right) - Z_{ky_k}$。\n函数 $\\text{LSE}(\\mathbf{z}) = \\log(\\sum_j \\exp(z_j))$ 是 log-sum-exp 函数，它在 $\\mathbb{R}^C$ 上是著名的凸函数。项 $-Z_{ky_k}$ 是 $Z_{k,:}$ 的线性函数，因此既是凸函数也是凹函数。两个凸函数之和是凸函数，所以 $L_k$ 是 logit 向量 $Z_{k,:}$ 的凸函数。\n\nlogits 和模型参数 $W$ 之间的关系由 $Z = XW$ 给出。对于单个样本 $k$，logit 向量 $Z_{k,:}$ 是 $Z$ 的第 $k$ 行：\n$$\nZ_{k,:} = X_{k,:} W\n$$\n其中 $X_{k,:}$ 是 $X$ 的第 $k$ 行。这是一个将 $W \\in \\mathbb{R}^{d \\times C}$ 映射到 $Z_{k,:} \\in \\mathbb{R}^C$ 的仿射变换。\n\n凸函数的一个基本性质指出，一个凸函数与一个仿射映射的复合是凸的。由于 $L_k(Z_{k,:})$ 在 $Z_{k,:}$ 上是凸的，且映射 $W \\mapsto Z_{k,:}$ 是仿射的，因此复合函数 $L_k(W) = L_k(X_{k,:}W)$ 在 $W$ 上是凸的。\n因为 $f(W)$ 是这些凸函数的和，所以 $f(W)$ 本身在 $W$ 上是凸的。\n\n### 3. 线性最小化预言机 (LMO)\n\nFrank-Wolfe 算法需要在每次迭代中求解一个线性最小化预言机 (LMO)：\n$$\nS_t = \\arg\\min_{S \\in \\mathcal{D}} \\langle S, G_t \\rangle\n$$\n其中 $G_t = \\nabla f(W_t)$ 是当前迭代点的梯度，$\\langle \\cdot, \\cdot \\rangle$ 是 Frobenius 内积。可行集是 $C$ 个标准 $d$ 维单纯形的笛卡尔积：$\\mathcal{D} = \\Delta^d \\times \\cdots \\times \\Delta^d$。\n\nFrobenius 内积可以写成对列的求和：\n$$\n\\langle S, G_t \\rangle = \\sum_{i=1}^d \\sum_{c=1}^C S_{ic} (G_t)_{ic} = \\sum_{c=1}^C \\langle S_{:,c}, (G_t)_{:,c} \\rangle_{\\mathbb{R}^d}\n$$\n其中 $S_{:,c}$ 和 $(G_t)_{:,c}$ 分别是 $S$ 和 $G_t$ 的第 $c$ 列。$S \\in \\mathcal{D}$ 的约束意味着每一列 $S_{:,c}$ 都必须独立地位于单纯形 $\\Delta^d$ 中。\n因此，这个最小化问题可以分解为 $C$ 个独立的子问题，每个列一个：\n$$\n\\min_{S \\in \\mathcal{D}} \\sum_{c=1}^C \\langle S_{:,c}, (G_t)_{:,c} \\rangle = \\sum_{c=1}^C \\min_{s_c \\in \\Delta^d} \\langle s_c, (G_t)_{:,c} \\rangle\n$$\n对于每个类别 $c \\in \\{1, \\dots, C\\}$，我们必须求解以下线性规划问题：\n$$\n\\min_{s \\in \\Delta^d} \\langle s, g \\rangle \\quad \\text{其中 } g = (G_t)_{:,c} \\text{ 且 } \\Delta^d = \\left\\{u \\in \\mathbb{R}^d \\mid u \\ge 0, \\sum_{i=1}^d u_i = 1\\right\\}\n$$\n可行集 $\\Delta^d$ 是一个多胞体（一个紧凑的凸集）。在一个多胞体上，线性函数的最小值总是在其某个顶点（极点）处取得。标准 $d$-单纯形 $\\Delta^d$ 的顶点是标准基向量 $e_1, e_2, \\dots, e_d \\in \\mathbb{R}^d$。\n\n为了找到最小值，我们在每个顶点处评估目标函数 $\\langle s, g \\rangle$：\n$$\n\\langle e_j, g \\rangle = \\sum_{i=1}^d (e_j)_i g_i = g_j\n$$\n最小值就是向量 $g$ 的最小分量。如果我们令 $j^* = \\arg\\min_{j \\in \\{1, \\dots, d\\}} g_j$，那么子问题的解就是 $s^* = e_{j^*}$。\n\n因此，为了构建完整的 LMO 解 $S_t \\in \\mathbb{R}^{d \\times C}$，我们对每一列 $c$ 执行以下步骤：\n1.  提取梯度的第 $c$ 列，即 $g_c = (G_t)_{:,c}$。\n2.  找到 $g_c$ 的最小分量的索引 $j_c^*$。\n3.  将 $S_t$ 的第 $c$ 列设置为标准基向量 $e_{j_c^*}$。\n\n这完全定义了指定可行集上的 LMO。\n\n### 4. Frank-Wolfe 算法总结\n\n所实现的 Frank-Wolfe 算法流程如下：\n1.  **初始化**：权重矩阵 $W_0$ 被初始化，使其每一列都是 $\\Delta^d$ 中的均匀向量，即对所有 $i,c$，$W_{0,i,c} = 1/d$。\n2.  **迭代**：对于 $t = 0, 1, 2, \\dots, T-1$：\n    a.  **梯度**：使用第 1 部分推导的公式计算梯度 $G_t = \\nabla f(W_t)$。\n    b.  **LMO**：使用第 3 部分的方法求解 $S_t = \\arg\\min_{S \\in \\mathcal{D}} \\langle S, G_t \\rangle$。\n    c.  **对偶间隙**：计算 FW 对偶间隙 $g_t = \\langle G_t, W_t - S_t \\rangle$。如果 $g_t \\le \\varepsilon$，则终止。\n    d.  **方向**：确定更新方向 $D_t = S_t - W_t$。\n    e.  **线搜索**：通过精确线搜索找到步长 $\\gamma_t \\in [0,1]$，以最小化一维凸函数 $\\phi(\\gamma) = f(W_t + \\gamma D_t)$。这通过使用数值优化例程 (`scipy.optimize.minimize_scalar`) 来实现。\n    f.  **更新**：更新权重：$W_{t+1} = W_t + \\gamma_t D_t$。\n最终输出基于算法终止后的状态。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Derives, designs, and implements the Frank-Wolfe method for\n    multinomial logistic regression with simplex constraints.\n    \"\"\"\n\n    # --- Test Cases ---\n\n    # Test A: LMO on a simple vector\n    c_A = np.array([1.5, -2.0, 0.0, 3.0, -1.0])\n    res_A = np.argmin(c_A)\n\n    # Test B: LMO with a tie\n    c_B = np.array([2.0, 2.0, 2.0])\n    # np.argmin deterministically returns the first index in case of a tie.\n    res_B = np.argmin(c_B)\n\n    # --- Frank-Wolfe Optimization Setup ---\n    \n    # Data and parameters for tests C, D, E\n    n, d, C = 6, 3, 3\n    X = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0],\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n    # Labels are 1-based in the problem, convert to 0-based for indexing\n    y_1based = np.array([1, 2, 3, 1, 2, 3])\n    y_0based = y_1based - 1\n\n    # Create one-hot encoding for y\n    Y_one_hot = np.zeros((n, C))\n    Y_one_hot[np.arange(n), y_0based] = 1.0\n\n    # Algorithm settings\n    T_max = 200\n    epsilon = 1e-6\n\n    # --- Helper Functions ---\n\n    def compute_f_and_grad(W, X_data, Y_one_hot_data):\n        \"\"\"Computes the objective function value and its gradient.\"\"\"\n        n_samples = X_data.shape[0]\n        \n        # Logits\n        Z = X_data @ W\n        \n        # Numerically stable softmax\n        Z_max = Z.max(axis=1, keepdims=True)\n        exp_Z = np.exp(Z - Z_max)\n        P = exp_Z / exp_Z.sum(axis=1, keepdims=True)\n        \n        # Loss (cross-entropy)\n        # Using direct indexing for correctness\n        log_likelihood = -np.log(P[np.arange(n_samples), Y_one_hot_data.argmax(axis=1)] + 1e-12)\n        loss = np.mean(log_likelihood)\n        \n        # Gradient\n        grad = (1 / n_samples) * X_data.T @ (P - Y_one_hot_data)\n        \n        return loss, grad\n\n    def lmo_product_simplex(G):\n        \"\"\"Linear Minimization Oracle for the product of simplicies.\"\"\"\n        d_dim, C_dim = G.shape\n        S = np.zeros_like(G)\n        # For each column, find the index of the minimum element\n        indices = np.argmin(G, axis=0)\n        # Create a one-hot vector in each column of S at that index\n        S[indices, np.arange(C_dim)] = 1.0\n        return S\n\n    # --- Frank-Wolfe Algorithm Implementation ---\n\n    # 1. Initialization\n    W = np.full((d, C), 1.0 / d)\n    \n    f_initial, _ = compute_f_and_grad(W, X, Y_one_hot)\n    \n    final_gap = np.inf\n    final_f = f_initial\n    \n    for t in range(T_max):\n        # 2. Compute gradient\n        f_t, G_t = compute_f_and_grad(W, X, Y_one_hot)\n\n        # 3. Call LMO to find the FW vertex\n        S_t = lmo_product_simplex(G_t)\n        \n        # 4. Compute Frank-Wolfe duality gap and check for convergence\n        gap = np.sum((W - S_t) * G_t)\n        final_gap = gap\n        \n        if gap = epsilon:\n            break\n            \n        # 5. Determine FW direction\n        D_t = S_t - W\n        \n        # 6. Exact line search for step size gamma\n        def phi(gamma, W_curr, D_curr, X_data, Y_one_hot_data):\n            f_val, _ = compute_f_and_grad(W_curr + gamma * D_curr, X_data, Y_one_hot_data)\n            return f_val\n        \n        res = minimize_scalar(\n            phi, \n            bounds=(0, 1), \n            method='bounded', \n            args=(W, D_t, X, Y_one_hot)\n        )\n        gamma_t = res.x\n        \n        # 7. Update weights\n        W = W + gamma_t * D_t\n\n    # After loop, compute final objective value\n    final_f, _ = compute_f_and_grad(W, X, Y_one_hot)\n    \n    # --- Collect and Format Results ---\n\n    # Test C: Is the final duality gap below the threshold?\n    res_C = final_gap = 5e-4\n    \n    # Test D: What is the final objective value?\n    res_D = round(final_f, 6)\n    \n    # Test E: Did the objective value decrease from the initial value?\n    res_E = final_f  f_initial\n    \n    # Final print statement in the exact required format.\n    # Order: [Test A integer, Test C boolean, Test B integer, Test D float, Test E boolean]\n    print(f\"[{res_A},{res_C},{res_B},{res_D},{res_E}]\")\n\nsolve()\n```", "id": "3126940"}, {"introduction": "除了一阶方法，二阶方法为解决约束优化问题提供了另一条强有力的途径。内点法通过引入对数障碍函数，将约束问题转化为一系列光滑的无约束问题，并利用牛顿法进行高效求解。在本练习中 [@problem_id:3126973]，你将深入探索 $\\ell_1$ 正则化最小二乘问题的内点法推导过程，包括构建牛顿系统和分析其迭代复杂度，从而领略二阶方法的精髓。", "problem": "考虑深度学习中用于稀疏线性模型的$\\ell_{1}$正则化最小二乘问题：对 $w \\in \\mathbb{R}^{n}$ 进行最小化，\n$$\n\\frac{1}{2}\\|X w - y\\|_{2}^{2} + \\lambda \\|w\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{m \\times n}$ 且 $m \\geq n$，$y \\in \\mathbb{R}^{m}$，以及 $\\lambda  0$。一种标准的重构方法是引入非负变量 $u, v \\in \\mathbb{R}^{n}$，满足 $w = u - v$ 以及 $u \\succeq 0, v \\succeq 0$，从而使得 $\\|w\\|_{1} = \\mathbf{1}^{\\top}(u+v)$。您将推导一个对数障碍内点法，并确定其每次迭代的复杂度。\n\n任务：\n\n$1.$ 从带有显式非负约束的关于 $(u,v)$ 的重构问题出发，引入参数为 $t  0$ 的对数障碍，以获得以下形式的光滑无约束问题\n$$\n\\phi_{t}(u,v) = t\\Big(\\frac{1}{2}\\|X(u-v)-y\\|_{2}^{2} + \\lambda \\mathbf{1}^{\\top}(u+v)\\Big) - \\sum_{i=1}^{n}\\big(\\ln u_{i} + \\ln v_{i}\\big).\n$$\n陈述其定义域，并解释为什么当 $X$ 具有列满秩时，该障碍目标函数是严格凸的。\n\n$2.$ 仅使用多变量微积分（梯度和海森矩阵）和线性代数的基本定义，推导关于 $(u,v)$ 的梯度 $\\nabla \\phi_{t}(u,v)$ 和海森矩阵 $\\nabla^{2}\\phi_{t}(u,v)$。将搜索方向 $(\\Delta u, \\Delta v)$ 的牛顿系统写成 $2n \\times 2n$ 的块矩阵形式\n$$\nH \\begin{pmatrix} \\Delta u \\\\ \\Delta v \\end{pmatrix} = -g,\n$$\n通过将梯度 $g$ 和海森矩阵 $H$ 明确地用 $X$, $y$, $u$, $v$, $\\lambda$ 和 $t$ 表示出来。\n\n$3.$ 通过分块消元（不使用启发式捷径）并仅利用对称正定（半定）矩阵的性质，证明计算 $(\\Delta u, \\Delta v)$ 可以归约为求解一个关于 $n$ 维变量的对称正定线性系统。将约化系数矩阵确定为一个固定的正半定格拉姆矩阵与一个依赖于当前 $(u,v)$ 的正定对角矩阵之和。\n\n$4.$ 假设每个牛顿步都采用稠密实现，并做出以下建模选择：\n- 在每次迭代中，您都显式地构建稠密的格拉姆矩阵 $G := X^{\\top} X$。\n- 然后，您构建（来自任务3的）约化的 $n \\times n$ 对称正定系数矩阵，并计算其稠密的柯列斯基分解，之后进行常数次数的三角求解和向量运算。\n忽略常数因子和低阶项，并将主要的算术成本纯粹用 $m$ 和 $n$ 表示，1次牛顿迭代的主阶复杂度表达式是什么？请仅用 $m$ 和 $n$ 的单个表达式给出您的最终答案（不要使用大$\\mathcal{O}$记号）。", "solution": "用户提供了一个关于$\\ell_1$正则化最小二乘问题的对数障碍内点法的推导和分析的多部分问题。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 优化问题是针对 $w \\in \\mathbb{R}^{n}$ 最小化：$\\frac{1}{2}\\|X w - y\\|_{2}^{2} + \\lambda \\|w\\|_{1}$。\n- $X \\in \\mathbb{R}^{m \\times n}$ 且 $m \\geq n$。\n- $y \\in \\mathbb{R}^{m}$。\n- $\\lambda  0$。\n- 一种重构方法使用 $w = u - v$，其中 $u, v \\in \\mathbb{R}^{n}$，$u \\succeq 0$，$v \\succeq 0$。\n- 通过这种重构，$\\|w\\|_{1} = \\mathbf{1}^{\\top}(u+v)$。\n- 给出的对数障碍目标函数为 $\\phi_{t}(u,v) = t\\Big(\\frac{1}{2}\\|X(u-v)-y\\|_{2}^{2} + \\lambda \\mathbf{1}^{\\top}(u+v)\\Big) - \\sum_{i=1}^{n}\\big(\\ln u_{i} + \\ln v_{i}\\big)$，其中障碍参数 $t  0$。\n- 任务1要求陈述定义域，并解释当 $X$ 列满秩时 $\\phi_t(u,v)$ 的严格凸性。\n- 任务2要求推导梯度 $\\nabla \\phi_{t}(u,v)$ 和海森矩阵 $\\nabla^{2}\\phi_{t}(u,v)$，并陈述牛顿系统。\n- 任务3要求通过分块消元将牛顿系统约化为一个 $n$ 维对称正定系统。\n- 任务4要求在特定的稠密实现选择下，找到单次牛顿迭代的主阶复杂度。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学性：** 该问题是内点法在LASSO（最小绝对收缩和选择算子）问题上的标准应用，LASSO是现代统计学和机器学习的基石。所有概念均源自成熟的凸优化理论。\n- **良构性：** 每个任务都表述精确，提供了所有必要的信息和假设（例如，$X$ 具有列满秩）。任务的进展是合乎逻辑的，并能导出一个唯一且有意义的结果。\n- **客观性：** 该问题以形式化的数学语言表述，没有歧义或主观论断。\n\n**步骤3：结论与行动**\n该问题在科学上是合理的、良构的、客观的且完整的。它是凸优化领域中的一个有效问题。我将继续进行完整解答。\n\n### 解答\n\n**任务1：障碍目标函数的定义域与严格凸性**\n\n目标函数由下式给出\n$$ \\phi_{t}(u,v) = t\\Big(\\frac{1}{2}\\|X(u-v)-y\\|_{2}^{2} + \\lambda \\mathbf{1}^{\\top}(u+v)\\Big) - \\sum_{i=1}^{n}\\big(\\ln u_{i} + \\ln v_{i}\\big). $$\n函数的定义域是使其有定义的点的集合。对数项 $\\ln u_i$ 和 $\\ln v_i$ 仅在参数为正时有定义。因此，$\\phi_t(u,v)$ 的定义域是所有 $(u,v) \\in \\mathbb{R}^{2n}$ 的集合，满足对所有 $i \\in \\{1, \\dots, n\\}$ 都有 $u_i  0$ 和 $v_i  0$。这可以写作\n$$ \\text{dom}(\\phi_t) = \\{ (u,v) \\in \\mathbb{R}^n \\times \\mathbb{R}^n \\mid u  0, v  0 \\}, $$\n其中不等式是逐元素的。这是 $\\mathbb{R}^{2n}$ 的正象限。\n\n为了分析 $\\phi_t(u,v)$ 的凸性，我们考察其结构。它是两个函数之和：\n$1.$ 缩放后的原始目标函数 $f_{t}(u,v) = t\\Big(\\frac{1}{2}\\|X(u-v)-y\\|_{2}^{2} + \\lambda \\mathbf{1}^{\\top}(u+v)\\Big)$。\n$2.$ 对数障碍函数 $B(u,v) = - \\sum_{i=1}^{n}\\big(\\ln u_{i} + \\ln v_{i}\\big)$。\n\n第一个函数 $f_t(u,v)$ 是凸的。项 $\\frac{1}{2}\\|X(u-v)-y\\|_{2}^{2}$ 是一个仿射映射 $(u,v) \\mapsto X(u-v)-y$ 与凸函数 $z \\mapsto \\frac{1}{2}\\|z\\|_2^2$ 的复合，因此是凸的。项 $\\lambda \\mathbf{1}^{\\top}(u+v)$ 关于 $(u,v)$ 是线性的，因此也是凸的。由于 $t0$，缩放后的和 $f_t(u,v)$ 仍然是凸的。$X$ 具有列满秩的假设意味着 $X$ 的零空间为 $\\{0\\}$，这意味着函数 $w \\mapsto \\|Xw-y\\|_2^2$ 是严格凸的。这表明项 $\\frac{1}{2}\\|X(u-v)-y\\|_{2}^{2}$ 在差值 $w=u-v$ 上是严格凸的，但对于拼接向量 $(u,v)$ 并不是严格凸的。\n\n第二个函数 $B(u,v)$ 是对数障碍。它是单变量函数之和，$B(u,v) = \\sum_{i=1}^n (-\\ln u_i) + \\sum_{i=1}^n (-\\ln v_i)$。函数 $x \\mapsto -\\ln x$ 在 $x0$ 时是严格凸的。严格凸函数的和是严格凸的。因此，$B(u,v)$ 在其定义域上是严格凸函数。\n\n总的目标函数是 $\\phi_t(u,v) = f_t(u,v) + B(u,v)$。一个凸函数（$f_t$）与一个严格凸函数（$B$）的和是严格凸的。因此，$\\phi_t(u,v)$ 在其定义域上是严格凸的。这与 $X$ 的秩无关。$X$ 具有列满秩的条件通常对于确保原始正则化问题（或非正则化问题）解的唯一性很重要，但添加严格凸的障碍项足以保证障碍目标函数 $\\phi_t$ 的严格凸性。\n\n**任务2：梯度、海森矩阵和牛顿系统**\n\n为了推导牛顿系统，我们首先计算梯度 $g = \\nabla \\phi_{t}(u,v)$ 和海森矩阵 $H = \\nabla^{2}\\phi_{t}(u,v)$。我们将变量视为单个向量 $\\begin{pmatrix} u \\\\ v \\end{pmatrix} \\in \\mathbb{R}^{2n}$。\n\n令 $r = X(u-v)-y$。目标函数为 $\\phi_t = \\frac{t}{2} r^\\top r + t\\lambda \\mathbf{1}^\\top (u+v) - \\sum_i \\ln u_i - \\sum_i \\ln v_i$。\n\n梯度 $g = \\begin{pmatrix} g_u \\\\ g_v \\end{pmatrix}$，其中 $g_u = \\nabla_u \\phi_t$ 且 $g_v = \\nabla_v \\phi_t$。\n使用链式法则，$\\nabla_u(\\frac{1}{2}r^\\top r) = (\\frac{\\partial r}{\\partial u})^\\top r = X^\\top r = X^\\top(X(u-v)-y)$。\n类似地，$\\nabla_v(\\frac{1}{2}r^\\top r) = (\\frac{\\partial r}{\\partial v})^\\top r = (-X)^\\top r = -X^\\top(X(u-v)-y)$。\n其余项的梯度是直观的：\n$$ \\nabla_u(t\\lambda\\mathbf{1}^\\top(u+v)) = t\\lambda\\mathbf{1}, \\quad \\nabla_v(t\\lambda\\mathbf{1}^\\top(u+v)) = t\\lambda\\mathbf{1} $$\n$$ \\nabla_u(-\\sum_i \\ln u_i) = -u^{-1}, \\quad \\nabla_v(-\\sum_i \\ln v_i) = -v^{-1} $$\n其中 $u^{-1}$ 是元素为 $1/u_i$ 的向量，对于 $v^{-1}$ 也是如此。\n\n结合这些项，梯度 $g$ 为：\n$$ g_u = t\\left(X^{\\top}(X(u-v)-y) + \\lambda \\mathbf{1}\\right) - u^{-1} $$\n$$ g_v = t\\left(-X^{\\top}(X(u-v)-y) + \\lambda \\mathbf{1}\\right) - v^{-1} $$\n\n海森矩阵 $H = \\nabla^2 \\phi_t$ 是一个 $2n \\times 2n$ 的块矩阵 $H = \\begin{pmatrix} H_{uu}  H_{uv} \\\\ H_{vu}  H_{vv} \\end{pmatrix}$。\n$H_{uu} = \\nabla_u (g_u) = \\nabla_u(tX^\\top(Xu-Xv-y) + \\dots) = tX^\\top X + \\text{diag}(u_1^{-2}, \\dots, u_n^{-2})$。\n$H_{uv} = \\nabla_v (g_u) = \\nabla_v(tX^\\top(Xu-Xv-y) + \\dots) = -tX^\\top X$。\n$H_{vu} = \\nabla_u (g_v) = \\nabla_u(-tX^\\top(Xu-Xv-y) + \\dots) = -tX^\\top X$。\n$H_{vv} = \\nabla_v (g_v) = \\nabla_v(-tX^\\top(Xu-Xv-y) + \\dots) = tX^\\top X + \\text{diag}(v_1^{-2}, \\dots, v_n^{-2})$。\n\n令 $U^{-2} = \\text{diag}(u_i^{-2})$ 且 $V^{-2} = \\text{diag}(v_i^{-2})$。海森矩阵为：\n$$ H = \\begin{pmatrix} tX^{\\top}X + U^{-2}  -tX^{\\top}X \\\\ -tX^{\\top}X  tX^{\\top}X + V^{-2} \\end{pmatrix} $$\n搜索方向 $(\\Delta u, \\Delta v)$ 的牛顿系统为 $H \\begin{pmatrix} \\Delta u \\\\ \\Delta v \\end{pmatrix} = -g$。具体表示为：\n$$\n\\begin{pmatrix} tX^{\\top}X + U^{-2}  -tX^{\\top}X \\\\ -tX^{\\top}X  tX^{\\top}X + V^{-2} \\end{pmatrix}\n\\begin{pmatrix} \\Delta u \\\\ \\Delta v \\end{pmatrix}\n= -\n\\begin{pmatrix} t(X^{\\top}(X(u-v)-y) + \\lambda \\mathbf{1}) - u^{-1} \\\\ t(-X^{\\top}(X(u-v)-y) + \\lambda \\mathbf{1}) - v^{-1} \\end{pmatrix}\n$$\n\n**任务3：分块消元与牛顿系统的约化**\n\n牛顿系统由两个耦合方程组成：\n$1.$ $(tX^{\\top}X + U^{-2})\\Delta u - tX^{\\top}X \\Delta v = -g_u$\n$2.$ $-tX^{\\top}X \\Delta u + (tX^{\\top}X + V^{-2})\\Delta v = -g_v$\n\n我们引入一个新变量 $\\Delta w = \\Delta u - \\Delta v$。我们可以将方程重写为：\n$1.$ $tX^{\\top}X(\\Delta u - \\Delta v) + U^{-2}\\Delta u = -g_u \\implies tX^{\\top}X\\Delta w + U^{-2}\\Delta u = -g_u$\n$2.$ $-tX^{\\top}X(\\Delta u - \\Delta v) + V^{-2}\\Delta v = -g_v \\implies -tX^{\\top}X\\Delta w + V^{-2}\\Delta v = -g_v$\n\n从这两个方程中，我们可以用 $\\Delta w$ 来表示 $\\Delta u$ 和 $\\Delta v$：\n从 (1) 式： $U^{-2}\\Delta u = -g_u - tX^{\\top}X\\Delta w \\implies \\Delta u = U^2(-g_u - tX^{\\top}X\\Delta w)$\n从 (2) 式： $V^{-2}\\Delta v = -g_v + tX^{\\top}X\\Delta w \\implies \\Delta v = V^2(-g_v + tX^{\\top}X\\Delta w)$\n这里 $U^2 = \\text{diag}(u_i^2)$ 且 $V^2 = \\text{diag}(v_i^2)$。\n\n现在将这些代入定义 $\\Delta w = \\Delta u - \\Delta v$ 中：\n$$ \\Delta w = U^2(-g_u - tX^{\\top}X\\Delta w) - V^2(-g_v + tX^{\\top}X\\Delta w) $$\n合并包含 $\\Delta w$ 的项：\n$$ \\Delta w = -U^2 g_u + V^2 g_v - t U^2 X^{\\top}X\\Delta w - t V^2 X^{\\top}X\\Delta w $$\n$$ \\Delta w + t(U^2 + V^2)X^{\\top}X\\Delta w = -U^2 g_u + V^2 g_v $$\n$$ \\left( I + t(U^2 + V^2) X^{\\top}X \\right) \\Delta w = -U^2 g_u + V^2 g_v $$\n系数矩阵 $(I + t(U^2 + V^2) X^{\\top}X)$ 通常不是对称的。然而，我们可以通过左乘对角正定矩阵 $(U^2 + V^2)^{-1}$ 来获得一个对称系统：\n$$ (U^2 + V^2)^{-1} \\left( I + t(U^2 + V^2) X^{\\top}X \\right) \\Delta w = (U^2 + V^2)^{-1}(-U^2 g_u + V^2 g_v) $$\n$$ \\left( (U^2 + V^2)^{-1} + tX^{\\top}X \\right) \\Delta w = (U^2 + V^2)^{-1}(-U^2 g_u + V^2 g_v) $$\n这是一个关于 $n$ 维变量 $\\Delta w$ 的线性系统。系数矩阵为 $A_{red} = tX^{\\top}X + (U^2 + V^2)^{-1}$。\n\n这个矩阵是对称正定的：\n- $X^{\\top}X$ 是一个格拉姆矩阵，它总是对称半正定的（PSD）。由于 $t0$，$tX^{\\top}X$ 也是对称半正定的。\n- 矩阵 $D = (U^2 + V^2)^{-1}$ 是一个对角矩阵，其元素为 $d_{ii} = 1/(u_i^2 + v_i^2)$。由于在 $\\phi_t$ 的定义域中 $u_i  0$ 且 $v_i  0$，我们有 $u_i^2+v_i^2  0$，所以对所有 $i$ 都有 $d_{ii}  0$。因此，$D$ 是一个正定对角矩阵，它是对称且正定的（PD）。\n- 一个半正定矩阵（$tX^{\\top}X$）与一个正定矩阵（$D$）的和是一个对称正定矩阵。\n\n因此，我们将计算 $2n$ 维搜索方向 $(\\Delta u, \\Delta v)$ 的问题约化为求解一个关于 $\\Delta w$ 的 $n$ 维对称正定线性系统。一旦求得 $\\Delta w$，就可以通过回代恢复 $\\Delta u$ 和 $\\Delta v$。\n\n**任务4：单次牛顿迭代的主阶复杂度**\n\n我们分析单次牛顿步的主要算术成本，忽略常数因子和低阶项。复杂度用 $m$ 和 $n$ 表示。\n\n$1.$ **构建格拉姆矩阵 $G = X^{\\top}X$：**\n$X \\in \\mathbb{R}^{m \\times n}$ 且 $X^{\\top} \\in \\mathbb{R}^{n \\times m}$。乘积 $G \\in \\mathbb{R}^{n \\times n}$。利用 $G$ 的对称性，我们需要计算 $n(n+1)/2$ 个长度为 $m$ 的向量的点积。每个点积需要 $m$ 次乘法和 $m-1$ 次加法。主阶成本与乘法次数成正比，约为 $\\frac{1}{2} n^2 \\times m$。所以这一步的主要复杂度是 $mn^2$。\n\n$2.$ **构建约化的系数矩阵 $A_{red} = tG + (U^2 + V^2)^{-1}$：**\n- 这涉及到构建对角矩阵 $(U^2 + V^2)^{-1}$，需要 $O(n)$ 次运算。\n- 用 $t$ 缩放稠密矩阵 $G$ 需要 $n^2$ 次乘法。\n- 将对角矩阵加到 $tG$ 上需要 $n$ 次加法。\n这一步的成本是 $O(n^2)$。\n\n$3.$ **计算 $A_{red}$ 的稠密柯列斯基分解：**\n$A_{red}$ 是一个 $n \\times n$ 的稠密对称正定矩阵。标准的柯列斯基分解算法大约需要 $\\frac{1}{3}n^3$ 次浮点运算。主要复杂度是 $n^3$。\n\n$4.$ **求解 $\\Delta w$ 并恢复 $(\\Delta u, \\Delta v)$：**\n- 使用柯列斯基因子求解系统 $A_{red}\\Delta w = b$ 需要一次前向和一次后向替换，每次成本为 $O(n^2)$。\n- 构建右侧项 $b = (U^2 + V^2)^{-1}(-U^2 g_u + V^2 g_v)$ 需要 $O(n)$ 的向量运算。\n- 恢复 $\\Delta u$ 和 $\\Delta v$ 需要类似 $G\\Delta w$（矩阵向量乘积，$O(n^2)$）以及其他向量缩放和加法（$O(n)$）的运算。\n这些分解后步骤的总成本是 $O(n^2)$。\n\n**总主阶复杂度：**\n总体成本是主要步骤成本的总和。由于 $m \\geq n$，与包含 $mn^2$ 和 $n^3$ 的项相比，低阶的 $O(n^2)$ 项可以忽略不计。\n两个主要的计算成本是构建格拉姆矩阵（$mn^2$）和执行柯列斯基分解（$n^3$）。总的主阶复杂度是这两项之和。\n\n主阶复杂度 = (构建 $G$ 的成本) + (柯列斯基分解的成本)\n$$ \\text{复杂度} = mn^2 + n^3 $$\n该表达式捕捉了单次牛顿迭代的主要算术成本。根据 $m$ 和 $n$（两者都很大）的相对大小，任何一项都可能占主导地位。问题要求一个关于 $m$ 和 $n$ 的单一表达式，这个和满足了要求。", "answer": "$$ \\boxed{mn^2 + n^3} $$", "id": "3126973"}]}