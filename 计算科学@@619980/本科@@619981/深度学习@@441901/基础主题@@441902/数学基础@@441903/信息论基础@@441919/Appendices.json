{"hands_on_practices": [{"introduction": "数据增强是训练深度学习模型的一项关键技术，但并非所有增强策略都能带来益处。一个直观的问题是：增加输入的随机性（即提高其熵）是否总能帮助模型学习？本练习将运用信息论的基本原理，通过区分输入总熵 $H(X)$ 和与任务相关的互信息 $I(X;Y)$，来精确地回答这个问题。通过这个实践，你将学会如何辨别哪些增强策略仅仅增加了无关噪声，哪些则提供了对任务有价值的信息。[@problem_id:3138094]", "problem": "您的任务是使用合成离散实验来证明，增加输入熵 $H(X)$ 的数据增强不一定会增加输入与标签之间的互信息 $I(X;Y)$。您的任务是构建一个完整的程序，该程序从精确定义的离散分布中生成数据，应用指定的增强，并计算香农熵和互信息的经验估计值。目标是利用信息论的基本定义，将随机性与任务相关的信息区分开来。\n\n使用以下基础知识：\n- 对于具有经验概率 $p(x)$ 的离散随机变量 $X$，香农熵 $H(X)$ 定义为 $H(X) = -\\sum_{x} p(x) \\log_2 p(x)$。\n- 对于具有经验联合概率 $p(x,y)$ 和边际概率 $p(x)$、$p(y)$ 的离散随机变量 $X$ 和 $Y$，互信息 (MI) $I(X;Y)$ 定义为 $I(X;Y) = \\sum_{x,y} p(x,y)\\log_2 \\frac{p(x,y)}{p(x)p(y)}$。\n\n本问题中的所有变量都是离散的，取有限个值。您将使用大小为 $n$ 的蒙特卡洛样本的经验频率来近似 $H(\\cdot)$ 和 $I(\\cdot\\,;\\cdot)$。\n\n实验设置：\n- 设 $Y \\in \\{0,1\\}$ 是一个二元标签，其中 $Y \\sim \\text{Bernoulli}(q)$，$q = \\mathbb{P}(Y=1)$。\n- 设 $Z = Y$ 表示一个与任务相关的二元特征。\n- 数据增强引入了额外的随机性，这种随机性要么独立于 $Y$，要么是应用于观测特征的标签破坏噪声。\n- 对于复合输入，通过将元组 $(x_1, x_2)$ 编码为整数类别，将 $X$ 视为单个离散变量，以便通过上述定义计算离散的 $H(X)$ 和 $I(X;Y)$。\n\n您的程序必须实现从样本中估计 $H(X)$ 和 $I(X;Y)$ 的经验估计器，并评估布尔谓词\n$$b = \\left[H\\!\\left(X_{\\text{aug}}\\right) > H\\!\\left(X_{\\text{base}}\\right)\\right] \\wedge \\left[I\\!\\left(X_{\\text{aug}};Y\\right) \\le I\\!\\left(X_{\\text{base}};Y\\right)\\right],$$\n使用一个小的数值容差来考虑采样误差。$b$ 的值为真，表明一次数据增强在增加 $H(X)$ 的同时，没有增加与任务相关的信息 $I(X;Y)$。\n\n测试套件（每个案例都应使用指定的参数和蒙特卡洛样本大小 $n$ 来运行）：\n\n- 案例1（理想情况，添加独立随机性）：$q = 0.5$，$n = 200000$。基础输入 $X_{\\text{base}} = Z$。增强输入 $X_{\\text{aug}} = (Z, N)$，其中 $N \\sim \\text{Bernoulli}(0.5)$ 独立于 $(Z,Y)$。\n- 案例2（类别不平衡，添加独立随机性）：$q = 0.1$，$n = 200000$。基础输入 $X_{\\text{base}} = Z$。增强输入 $X_{\\text{aug}} = (Z, N)$，其中 $N \\sim \\text{Bernoulli}(0.5)$ 是独立的。\n- 案例3（独立于标签的破坏性噪声增加了 $H(X)$ 但减少了 $I(X;Y)$）：$q = 0.1$，$s = 0.3$，$n = 200000$。基础输入 $X_{\\text{base}} = Z$。增强输入 $X_{\\text{aug}} = \\tilde{Z}$，其中 $\\tilde{Z} = Z \\oplus B$，$B \\sim \\text{Bernoulli}(s)$ 独立于 $(Z,Y)$；这里 $\\oplus$ 表示按位异或。\n- 案例4（添加任务相关信息提高了 $I(X;Y)$）：$q = 0.1$，$s = 0.3$，$n = 200000$。基础输入 $X_{\\text{base}} = \\tilde{Z}$，定义如案例3。增强输入 $X_{\\text{aug}} = (\\tilde{Z}, Y)$。\n- 案例5（边界条件，无破坏）：$q = 0.1$，$s = 0.0$，$n = 200000$。基础输入 $X_{\\text{base}} = Z$。增强输入 $X_{\\text{aug}} = \\tilde{Z}$，其中 $\\tilde{Z} = Z \\oplus B$，$B \\sim \\text{Bernoulli}(s)$，即当 $s=0.0$ 时没有变化。\n\n实现要求：\n- 使用可复现的伪随机数生成器。\n- 对于每个案例，使用上述基本定义直接从样本计算经验 $H(X)$ 和 $I(X;Y)$。\n- 对于每个案例，使用 $10^{-3}$ 比特的容差比较 $H(\\cdot)$ 和 $I(\\cdot\\,;\\cdot)$，计算由谓词给出的布尔值 $b$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[result_1,result_2,\\dots,result_5]$），其中每个 $result_i$ 是案例 $i$（按上文列出的顺序）的布尔值 $b$。\n\n所有答案都是无量纲的，应按规定以布尔值报告。本问题不涉及物理单位或角度。", "solution": "目标是通过合成实验证明，数据增强可以增加输入总熵 $H(X)$，但不会增加（有时甚至会减少）其与目标标签 $Y$ 共享的互信息 $I(X;Y)$。这种区分对于理解哪些增强提供了与任务相关的信息，而哪些仅仅增加了与任务无关的随机性至关重要。\n\n该方法涉及一系列蒙特卡洛模拟。对于每种情况，我们都从指定的基础输入 $X_{\\text{base}}$、增强输入 $X_{\\text{aug}}$ 以及二元标签 $Y$ 的离散概率分布中生成大量样本（数量为 $n$）。从这些样本中，我们计算香农熵和互信息的经验估计值，以评估谓词 $b = \\left[H\\!\\left(X_{\\text{aug}}\\right) > H\\!\\left(X_{\\text{base}}\\right)\\right] \\wedge \\left[I\\!\\left(X_{\\text{aug}};Y\\right) \\le I\\!\\left(X_{\\text{base}};Y\\right)\\right]$。使用 $10^{-3}$ 的数值容差使比较对采样噪声具有鲁棒性。\n\n分析的核心在于对离散随机变量的两个信息论量进行经验估计。\n\n首先，变量 $X$（其值可在集合 $\\mathcal{X}$ 中取）的香农熵是根据其经验概率质量函数 $p(x)$ 估计的。给定 $n$ 个样本，$p(x)$ 是值 $x$ 出现的频率。以比特为单位的熵为：\n$$H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)$$\n其中，如果 $p(x)=0$，则项 $p(x) \\log_2 p(x)$ 取为 $0$。\n\n其次，两个变量 $X$ 和 $Y$ 之间的互信息是衡量它们统计依赖性的指标。为了数值稳定性和易于实现，我们使用恒等式：\n$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$\n其中 $H(X)$ 和 $H(Y)$ 是边际熵，$H(X,Y)$ 是对 $(X,Y)$ 的联合熵。联合熵由经验联合概率分布 $p(x,y)$ 计算得出：\n$$H(X,Y) = -\\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x,y) \\log_2 p(x,y)$$\n当输入 $X$ 是由多个特征（例如 $X=(F_1, F_2)$）组成的复合变量时，通过将特征值的元组编码为唯一的整数标识符，将其视为单个离散变量。这使得上述离散变量的公式可以直接应用。\n\n分析过程是对五个不同案例评估谓词 $b$。\n\n**案例1：独立随机性（类别均衡）**\n- 参数：$q = \\mathbb{P}(Y=1) = 0.5$，$n = 200000$。\n- 基础输入：$X_{\\text{base}} = Z$，其中 $Z=Y$。这里，输入是标签的完美副本。\n- 增强输入：$X_{\\text{aug}} = (Z, N)$，其中 $N \\sim \\text{Bernoulli}(0.5)$ 是独立于 $Y$ 的噪声变量。\n- 分析：$I(X_{\\text{base}};Y) = I(Y;Y) = H(Y) = 1$ 比特。添加独立的噪声变量 $N$ 会增加总熵，因为由于独立性，$H(X_{\\text{aug}}) = H(Z,N) = H(Z) + H(N) = H(Y) + H(N) = 1 + 1 = 2$ 比特。互信息保持不变，因为 $N$ 独立于 $Y$：$I(X_{\\text{aug}};Y) = I((Z,N);Y) = I(Z;Y) = 1$ 比特。因此，我们预期 $H(X_{\\text{aug}}) > H(X_{\\text{base}})$ 且 $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$。谓词 $b$ 的值应为真。\n\n**案例2：独立随机性（类别不平衡）**\n- 参数：$q = \\mathbb{P}(Y=1) = 0.1$，$n = 200000$。\n- 设置：与案例1相同，但标签分布不平衡。\n- 分析：推理与案例1相同。标签的熵为 $H(Y) = -0.1\\log_2(0.1) - 0.9\\log_2(0.9) \\approx 0.469$ 比特。我们有 $H(X_{\\text{base}}) = H(Y) \\approx 0.469$ 比特和 $I(X_{\\text{base}};Y) = H(Y) \\approx 0.469$ 比特。增强后的熵为 $H(X_{\\text{aug}}) = H(Y) + H(N) \\approx 0.469 + 1 = 1.469$ 比特。互信息保持为 $I(X_{\\text{aug}};Y) = I(Y;Y) = H(Y) \\approx 0.469$ 比特。同样，我们预期 $H(X_{\\text{aug}}) > H(X_{\\text{base}})$ 且 $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$。谓词 $b$ 应为真。\n\n**案例3：标签破坏性噪声**\n- 参数：$q = 0.1$，噪声水平 $s = 0.3$，$n = 200000$。\n- 基础输入：$X_{\\text{base}} = Z = Y$。\n- 增强输入：$X_{\\text{aug}} = \\tilde{Z} = Z \\oplus B$，其中 $B \\sim \\text{Bernoulli}(s)$ 是独立的比特翻转噪声，$\\oplus$ 是异或运算。这模拟了一个二元对称信道。\n- 分析：$H(X_{\\text{base}}) = H(Y) \\approx 0.469$ 比特。增强输入 $\\tilde{Z}$ 是 $Y$ 的一个噪声版本。概率 $\\mathbb{P}(\\tilde{Z}=1) = q(1-s) + (1-q)s = 0.1(0.7) + 0.9(0.3) = 0.34$。熵为 $H(X_{\\text{aug}}) = H_b(0.34) \\approx 0.923$ 比特，大于 $H(X_{\\text{base}})$。然而，噪声破坏了关于 $Y$ 的信息。根据数据处理不等式，$I(Y;\\tilde{Z}) \\le I(Y;Z)$。具体而言，$I(X_{\\text{aug}};Y) = H(\\tilde{Z}) - H(\\tilde{Z}|Y) = H_b(0.34) - H_b(s) \\approx 0.923-0.881=0.042$ 比特。这远小于 $I(X_{\\text{base}};Y) = H(Y) \\approx 0.469$ 比特。谓词的两个条件都满足，因此 $b$ 应为真。\n\n**案例4：添加任务相关信息**\n- 参数：$q=0.1, s=0.3, n=200000$。\n- 基础输入：$X_{\\text{base}} = \\tilde{Z}$，即案例3中的噪声变量。\n- 增强输入：$X_{\\text{aug}} = (\\tilde{Z}, Y)$。该增强明确添加了真实标签。\n- 分析：基础量源自案例3：$H(X_{\\text{base}}) \\approx 0.923$ 比特和 $I(X_{\\text{base}};Y) \\approx 0.042$ 比特。增强输入的熵为 $H(X_{\\text{aug}}) = H(\\tilde{Z}, Y) = H(Y) + H(\\tilde{Z}|Y) = H_b(0.1) + H_b(0.3) \\approx 0.469 + 0.881 = 1.35$ 比特，大于 $H(X_{\\text{base}})$。然而，现在的互信息是 $I(X_{\\text{aug}};Y) = I((\\tilde{Z},Y);Y) = H(Y) - H(Y|(\\tilde{Z},Y))$。由于给定元组 $(\\tilde{Z},Y)$ 后 $Y$ 是完全已知的，条件熵 $H(Y|(\\tilde{Z},Y))$ 为 $0$。因此，$I(X_{\\text{aug}};Y) = H(Y) \\approx 0.469$ 比特。这远大于 $I(X_{\\text{base}};Y) \\approx 0.042$ 比特。条件 $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$ 为假。谓词 $b$ 应为假。\n\n**案例5：边界条件（无破坏）**\n- 参数：$q=0.1, s=0.0, n=200000$。\n- 设置：与案例3相同，但噪声概率 $s=0$。\n- 分析：当 $s=0$ 时，噪声变量 $B$ 始终为 $0$。因此，$X_{\\text{aug}} = Z \\oplus 0 = Z = X_{\\text{base}}$。基础输入和增强输入是相同的。因此，$H(X_{\\text{aug}}) = H(X_{\\text{base}})$ 且 $I(X_{\\text{aug}};Y) = I(X_{\\text{base}};Y)$。谓词的第一部分 $H(X_{\\text{aug}}) > H(X_{\\text{base}})$ 是一个严格不等式，因此将为假（在数值容差范围内）。所以，整个谓词 $b$ 必定为假。\n\n实现部分将为每个案例生成样本，计算信息论量，并评估谓词 $b$ 以验证这些理论预期。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_entropy(samples):\n    \"\"\"\n    Computes the Shannon entropy of a discrete random variable from its samples.\n    \"\"\"\n    _, counts = np.unique(samples, return_counts=True)\n    probabilities = counts / len(samples)\n    # The contribution of p*log(p) is 0 for p=0.\n    # We filter to avoid log(0) errors.\n    probabilities = probabilities[probabilities > 0]\n    entropy = -np.sum(probabilities * np.log2(probabilities))\n    return entropy\n\ndef compute_info_metrics(x_samples, y_samples):\n    \"\"\"\n    Computes H(X) and I(X;Y) from samples.\n    I(X;Y) is calculated as H(X) + H(Y) - H(X,Y).\n    \"\"\"\n    # H(X)\n    h_x = _calculate_entropy(x_samples)\n    \n    # H(Y)\n    h_y = _calculate_entropy(y_samples)\n    \n    # H(X, Y) - Joint Entropy\n    # We get joint probabilities by counting unique pairs (rows).\n    joint_samples = np.c_[x_samples, y_samples]\n    _, joint_counts = np.unique(joint_samples, axis=0, return_counts=True)\n    joint_probabilities = joint_counts / len(x_samples)\n    joint_probabilities = joint_probabilities[joint_probabilities > 0]\n    h_xy = -np.sum(joint_probabilities * np.log2(joint_probabilities))\n    \n    # I(X;Y) = H(X) + H(Y) - H(X,Y)\n    i_xy = h_x + h_y - h_xy\n    \n    return h_x, i_xy\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the boolean predicate for each case.\n    \"\"\"\n    test_cases = [\n        # (case_id, q, s, n)\n        (1, 0.5, None, 200000),\n        (2, 0.1, None, 200000),\n        (3, 0.1, 0.3, 200000),\n        (4, 0.1, 0.3, 200000),\n        (5, 0.1, 0.0, 200000),\n    ]\n\n    results = []\n    rng = np.random.default_rng(seed=123)  # Reproducible pseudo-random number generator\n    tol = 1e-3  # Numerical tolerance in bits\n\n    for case_id, q, s, n in test_cases:\n        # Generate base data\n        y = rng.binomial(1, q, size=n)\n        z = y\n\n        if case_id == 1:\n            # Case 1: happy path, independent randomness added\n            x_base = z\n            n_rv = rng.binomial(1, 0.5, size=n)\n            # Encode tuple (Z, N) into a single integer\n            x_aug = 2 * z + n_rv\n\n        elif case_id == 2:\n            # Case 2: class imbalance, independent randomness added\n            x_base = z\n            n_rv = rng.binomial(1, 0.5, size=n)\n            # Encode tuple (Z, N) into a single integer\n            x_aug = 2 * z + n_rv\n\n        elif case_id == 3:\n            # Case 3: label-independent corrupting noise\n            x_base = z\n            b_rv = rng.binomial(1, s, size=n)\n            z_tilde = z ^ b_rv  # bitwise XOR\n            x_aug = z_tilde\n        \n        elif case_id == 4:\n            # Case 4: adding task-relevant information\n            b_rv = rng.binomial(1, s, size=n)\n            z_tilde = z ^ b_rv\n            x_base = z_tilde\n            # Encode tuple (Z_tilde, Y) into a single integer\n            x_aug = 2 * z_tilde + y\n            \n        elif case_id == 5:\n            # Case 5: boundary condition, no corruption\n            x_base = z\n            b_rv = rng.binomial(1, s, size=n) # s=0.0 means B is all zeros\n            z_tilde = z ^ b_rv\n            x_aug = z_tilde\n        else:\n            continue\n\n        # Compute information-theoretic metrics for base and augmented inputs\n        h_base, i_base = compute_info_metrics(x_base, y)\n        h_aug, i_aug = compute_info_metrics(x_aug, y)\n\n        # Evaluate the boolean predicate b with tolerance\n        is_h_aug_greater = h_aug > h_base + tol\n        is_i_aug_le_i_base = i_aug = i_base + tol\n        \n        b = is_h_aug_greater and is_i_aug_le_i_base\n        results.append(b)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3138094"}, {"introduction": "在理解了何为“有用信息”之后，我们自然会问：如何设计一个能高效捕捉这些信息的表示？这个问题是表示学习的核心。本练习将引导你探索信息瓶颈（Information Bottleneck）原理，即在压缩率（表示的简洁性）和预测能力（表示保留了多少关于目标的信息）之间进行权衡。你将学习在一个给定的信息“预算”下，如何通过对输入空间进行最优划分来构建一个压缩表示，并进一步将其与学习该表示所需的样本复杂度联系起来。[@problem_id:3138091]", "problem": "给定有限随机变量 $X$ 和 $Y$，其离散分布已知。您必须在一个熵预算为 $k$ 比特的条件下设计一个离散表示 $T = g(X)$，并评估需要多少比特才能达到一个目标总体准确率（使用贝叶斯最优决策规则）。您还必须将此比特预算与一个用于从独立同分布样本中估计 $(T,Y)$ 联合分布的样本复杂度界限联系起来。\n\n基本原理：\n- 对于一个概率质量函数为 $p(t)$ 的离散随机变量 $T$，其香农熵为 $H(T) = -\\sum_{t} p(t) \\log_2 p(t)$。\n- 对于任何有限集，当 $p(t)$ 是均匀分布时，$\\log_2 |T|$ 是 $T$ 可能的最大熵，因此 $H(T) \\le \\log_2 |T|$。\n- 用于从 $T$ 预测 $Y$ 的贝叶斯最优分类器对每个 $t$ 选择 $\\arg\\max_{y} p(y \\mid t)$，得到的总体准确率为 $\\sum_{t} p(t) \\max_{y} p(y \\mid t)$。\n- 针对有界伯努利变量的霍夫丁不等式指出，对于一个均值为 $\\mu$ 的伯努利随机变量的 $n$ 个独立同分布样本的经验平均值 $\\hat{\\mu}$，有 $\\Pr(|\\hat{\\mu} - \\mu| \\ge \\epsilon) \\le 2 e^{-2 n \\epsilon^2}$。通过对有限事件集使用联合界，这可以扩展为一个一致偏差界。\n\n问题任务：\n1. 对于每个测试用例，设 $X$ 在有限集 $\\mathcal{X}$ 中取值，其概率为 $p(x)$（$x \\in \\mathcal{X}$）；设 $Y$ 在有限集 $\\mathcal{Y}$ 中取值，其指定的条件概率为 $p(y \\mid x)$（对于每个 $x \\in \\mathcal{X}$ 和 $y \\in \\mathcal{Y}$）。将任何离散表示 $T = g(X)$ 视为从 $\\mathcal{X}$ 到有限码集 $\\mathcal{T}$ 的确定性映射。熵预算受 $H(T) \\le k$ 比特的约束。因为 $H(T) \\le \\log_2 |\\mathcal{T}| \\le k$，所以只需限制 $|\\mathcal{T}| \\le 2^k$。\n2. 对于一个固定的整数 $k \\ge 0$，在上述约束下，计算从 $T$ 预测 $Y$ 所能达到的最佳总体准确率，假设使用贝叶斯最优分类器。该准确率为\n   $$\\mathrm{Acc}_k^* = \\max_{g: \\mathcal{X} \\to \\mathcal{T},\\ |\\mathcal{T}| \\le 2^k} \\sum_{t \\in \\mathcal{T}} p(t) \\max_{y \\in \\mathcal{Y}} p(y \\mid t),$$\n   其中，对于 $p(t)  0$，有 $p(t) = \\sum_{x \\in \\mathcal{X}: g(x)=t} p(x)$ 和 $p(y \\mid t) = \\frac{\\sum_{x \\in \\mathcal{X}: g(x)=t} p(x) p(y \\mid x)}{p(t)}$。\n3. 对于每个测试用例，给定一个目标准确率阈值 $\\alpha$，确定最小整数 $k_{\\min}$，使得 $\\mathrm{Acc}_k^* \\ge \\alpha$。如果在 $0 \\le k \\le \\lceil \\log_2 |\\mathcal{X}| \\rceil$ 范围内不存在这样的 $k$，则设置 $k_{\\min} = -1$，并将报告的准确率定义为 $\\max_{0 \\le k \\le \\lceil \\log_2 |\\mathcal{X}| \\rceil} \\mathrm{Acc}_k^*$。\n4. 样本复杂度关系：设 $S$ 为实现 $\\mathrm{Acc}_{k_{\\min}}^*$ 的最优映射 $g$ 所使用的不同码的数量，即 $S = |\\{t \\in \\mathcal{T}: p(t)>0\\}|$。为了在加性误差 $\\epsilon  0$ 内以至少 $1-\\delta$ 的置信度一致地估计所有 $(t,y) \\in \\mathcal{T} \\times \\mathcal{Y}$ 的联合概率 $p(t,y)$（也就是说，每个经验估计 $\\hat{p}(t,y)$ 与 $p(t,y)$ 的偏差最多为 $\\epsilon$ 的概率至少为 $1-\\delta$），霍夫丁不等式和联合界给出了足够的样本量\n   $$n \\ge \\frac{1}{2\\epsilon^2} \\ln\\!\\left(\\frac{2 S |\\mathcal{Y}|}{\\delta}\\right)。$$\n   为每个测试用例报告 $N_{\\min} = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\!\\left(\\frac{2 S |\\mathcal{Y}|}{\\delta}\\right) \\right\\rceil$。\n\n测试套件：\n- 测试用例 1（二元，确定性，可分）：\n  - $\\mathcal{X} = \\{0,1,2,3\\}$，\n  - $p(x) = [0.4, 0.3, 0.2, 0.1]$，\n  - $\\mathcal{Y} = \\{0,1\\}$，\n  - $p(y \\mid x)$ 由 $p(1 \\mid 0) = 1.0$，$p(1 \\mid 1) = 1.0$，$p(0 \\mid 2) = 1.0$，$p(0 \\mid 3) = 1.0$ 给出（其他情况为零），\n  - 目标准确率 $\\alpha = 0.95$，\n  - 误差容限 $\\epsilon = 0.02$，\n  - 置信度松弛 $\\delta = 0.01$。\n- 测试用例 2（二元，标签独立于输入）：\n  - $\\mathcal{X} = \\{0,1,2\\}$，\n  - $p(x) = [0.5, 0.3, 0.2]$，\n  - $\\mathcal{Y} = \\{0,1\\}$，\n  - 对于所有 $x \\in \\mathcal{X}$，$p(1 \\mid x) = 0.6$ 且 $p(0 \\mid x) = 0.4$，\n  - 目标准确率 $\\alpha = 0.6$，\n  - 误差容限 $\\epsilon = 0.01$，\n  - 置信度松弛 $\\delta = 0.05$。\n- 测试用例 3（三分类，确定性，需要多于一个比特）：\n  - $\\mathcal{X} = \\{0,1,2,3\\}$，\n  - $p(x) = [0.5, 0.3, 0.1, 0.1]$，\n  - $\\mathcal{Y} = \\{0,1,2\\}$，\n  - $p(y \\mid x)$ 由 $p(0 \\mid 0) = 1.0$，$p(1 \\mid 1) = 1.0$，$p(2 \\mid 2) = 1.0$，$p(2 \\mid 3) = 1.0$ 给出（其他情况为零），\n  - 目标准确率 $\\alpha = 0.95$，\n  - 误差容限 $\\epsilon = 0.02$，\n  - 置信度松弛 $\\delta = 0.02$。\n\n要求的程序行为：\n- 对每个从 $0$ 到 $\\lceil \\log_2 |\\mathcal{X}| \\rceil$ 的整数 $k$，实现对映射 $g: \\mathcal{X} \\to \\mathcal{T}$（其中 $|\\mathcal{T}| \\le 2^k$）的穷举搜索。对于每个映射，计算其在 $T$ 上导出的分布，评估贝叶斯最优准确率，并保留满足 $\\alpha$ 的最小 $k$ 所对应的最大准确率 $\\mathrm{Acc}_k^*$ 和最优映射中使用的码的数量 $S$。\n- 对于每个测试用例，生成列表 $[k_{\\min}, \\mathrm{Acc}_{k_{\\min}}^*, N_{\\min}]$，其中：\n  - $k_{\\min}$ 是一个整数（如果目标准确率在搜索范围内无法达到，则为 $-1$），\n  - $\\mathrm{Acc}_{k_{\\min}}^*$ 是一个浮点数，\n  - $N_{\\min}$ 是一个整数，使用上述公式计算得出，其中 $S$ 等于在 $k_{\\min}$ 处的的最优映射中使用的码的数量；如果 $k_{\\min} = -1$，则使用在整个搜索范围内实现最大准确率的映射所对应的 $S$。\n- 最终输出格式：您的程序应生成单行输出，其中包含所有三个测试用例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个元素本身也是一个列表。例如，输出必须看起来像 $[[\\dots],[\\dots],[\\dots]]$，不含空格或额外文本。", "solution": "用户提供了一个问题，要求找到一个随机变量 $X$ 的最优离散表示，以预测另一个随机变量 $Y$，这一过程受到表示复杂度的约束。复杂度以香农熵为预算，并简化为对表示上域大小的约束。任务包括找到达到目标准确率所需的最小预算 $k$，并计算相应的样本复杂度界限。\n\n### 问题验证\n根据所提供的方法论对问题陈述进行严格评估。\n\n**步骤 1：提取的已知条件**\n- **变量**：有限离散随机变量 $X$ 和 $Y$。\n- **映射**：一个确定性函数 $T = g(X)$，其中 $g: \\mathcal{X} \\to \\mathcal{T}$。\n- **约束**：表示的熵 $H(T) \\le k$ 比特。这被简化为对上域的基数约束，即 $|\\mathcal{T}| \\le 2^k$。\n- **目标**：最大化从 $T$ 预测 $Y$ 的贝叶斯最优总体准确率。对于给定的映射 $g$，准确率为 $\\sum_{t} p(t) \\max_{y} p(y \\mid t)$。\n- **优化问题**：$\\mathrm{Acc}_k^* = \\max_{g: |\\mathcal{T}| \\le 2^k} \\sum_{t} p(t) \\max_{y} p(y \\mid t)$。\n- **任务 1**：对于给定的目标准确率 $\\alpha$，找到最小整数 $k_{\\min}$（在范围 $0 \\le k \\le \\lceil \\log_2 |\\mathcal{X}| \\rceil$ 内），使得 $\\mathrm{Acc}_{k_{\\min}}^* \\ge \\alpha$。如果不存在这样的 $k$，则 $k_{\\min} = -1$。\n- **任务 2**：计算样本复杂度界限 $N_{\\min} = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\!\\left(\\frac{2 S |\\mathcal{Y}|}{\\delta}\\right) \\right\\rceil$，其中 $S$ 是在 $k_{\\min}$ 处的最优映射所使用的活性码的数量。\n- **输入**：为每个测试用例提供了集合 $\\mathcal{X}$、$\\mathcal{Y}$，分布 $p(x)$ 和 $p(y \\mid x)$，以及参数 $\\alpha$、$\\epsilon$、$\\delta$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据**：该问题在信息论、概率论和统计学习理论中有充分的依据。香农熵、贝叶斯最优分类器和通过霍夫丁不等式得出的样本复杂度等概念都是标准的，并且应用正确。核心任务是一种率失真优化，特别与信息瓶颈原理相关。\n- **适定性**：该问题是适定的。目标函数（准确率）是在一个有限的可能映射 $g$ 集合上最大化的。一个映射 $g: \\mathcal{X} \\to \\mathcal{T}$ 且 $|\\mathcal{T}| \\le C$ 会导出集合 $\\mathcal{X}$ 的一个划分为 $S \\le C$ 个非空子集。一个有限集的划分数量是有限的（由贝尔数给出）。由于搜索空间是有限的，保证对每个 $k$ 都存在一个最大准确率。后续步骤是确定性的。\n- **客观性**：问题以精确的数学语言陈述，没有任何主观性。\n\n该问题没有表现出任何无效性缺陷。它在科学上是合理的，可形式化的，完整的，对于给定的测试用例是可行的，并且结构良好。\n\n**步骤 3：结论与行动**\n该问题被判定为**有效**。将提供完整解决方案。\n\n### 基于原理的解决方案\n该问题要求我们找到一个随机变量 $X$ 的最优压缩，生成一个新变量 $T=g(X)$，使其最大程度地保留关于目标变量 $Y$ 的预测信息。压缩受到限制 $T$ 可以取的可能值的数量，即 $|\\mathcal{T}| \\le 2^k$，其中 $k$ 是以比特为单位的预算。\n\n**1. 优化问题的形式化**\n一个确定性映射 $g: \\mathcal{X} \\to \\mathcal{T}$ 将输入空间 $\\mathcal{X}$ 划分为一组不相交的子集 $\\Pi = \\{A_t\\}_{t \\in \\mathcal{T}}$，其中 $A_t = \\{x \\in \\mathcal{X} \\mid g(x)=t\\}$。非空子集的数量 $S = |\\{t \\mid A_t \\neq \\emptyset\\}|$ 是该映射使用的“活性码”的数量。约束 $|\\mathcal{T}| \\le 2^k$ 意味着我们必须找到将 $\\mathcal{X}$ 划分为最多 $S \\le 2^k$ 个部分的最优划分。\n\n对于给定的划分 $\\Pi = \\{A_1, A_2, \\dots, A_S\\}$，我们可以计算出相应的准确率。每个子集 $A_j$ 对应一个唯一的码 $t_j$。\n- 观察到码 $t_j$ 的概率是映射到它的所有输入值的概率之和：$p(t_j) = \\sum_{x \\in A_j} p(x)$。\n- 码 $t_j$ 和输出 $y$ 的联合概率为 $p(t_j, y) = \\sum_{x \\in A_j} p(x,y) = \\sum_{x \\in A_j} p(y|x)p(x)$。\n- 贝叶斯最优分类器在给定观测 $T=t_j$ 时，会预测使条件概率 $p(y|t_j) = p(t_j, y) / p(t_j)$ 最大化的标签 $y^*$。来自该簇的准确率贡献为 $p(t_j) \\max_y p(y|t_j) = \\max_y p(t_j,y)$。\n- 此划分的总总体准确率是所有簇的贡献之和：\n$$ \\mathrm{Acc}(\\Pi) = \\sum_{j=1}^{S} \\max_{y \\in \\mathcal{Y}} \\left( \\sum_{x \\in A_j} p(x,y) \\right) $$\n\n对于固定预算 $k$ 的优化问题是找到一个子集数量最多为 $2^k$ 的划分 $\\Pi^*$，以最大化此准确率：\n$$ \\mathrm{Acc}_k^* = \\max_{\\Pi : |\\Pi| \\le 2^k} \\mathrm{Acc}(\\Pi) $$\n\n**2. 算法策略：穷举搜索**\n由于所提供测试用例中的输入空间 $\\mathcal{X}$ 很小（$|\\mathcal{X}| \\le 4$），对 $\\mathcal{X}$ 的所有可能划分进行穷举搜索在计算上是可行的。将一个大小为 $N$ 的集合进行划分的方式数量是贝尔数 $B_N$。对于 $N=3$，$B_3=5$。对于 $N=4$，$B_4=15$。\n\n总体算法流程如下：\n1.  **生成划分**：对于给定的集合 $\\mathcal{X}$，生成所有可能的划分。\n2.  **评估划分**：对于每个划分，计算其大小（子集数量 $S$）和相应的贝叶斯最优准确率。\n3.  **确定 $\\mathrm{Acc}_k^*$**：对于从 $0$ 到 $\\lceil \\log_2 |\\mathcal{X}| \\rceil$ 的每个整数预算 $k$：\n    - 允许的最大簇数为 $C = 2^k$。\n    - 在所有已评估的 $S \\le C$ 的划分中找到最大准确率。这给出 $\\mathrm{Acc}_k^*$。记录实现此准确率的划分所对应的簇数 $S_k^*$。如果准确率出现平局，则使用确定性的平局打破规则（例如，选择具有最小 $S$ 的划分）。\n4.  **找到 $k_{\\min}$**：确定使 $\\mathrm{Acc}_k^* \\ge \\alpha$ 的最小 $k$。这就是 $k_{\\min}$。如果条件从未满足，则将 $k_{\\min}$ 设置为 $-1$。\n5.  **计算样本复杂度 $N_{\\min}$**：\n    - 如果找到了 $k_{\\min}$，则使用准确率 $\\mathrm{Acc}_{k_{\\min}}^*$ 及其对应的簇数 $S_{k_{\\min}}^*$。\n    - 如果 $k_{\\min} = -1$，则使用在所有 $k$ 中实现的最高准确率 $\\max_k \\mathrm{Acc}_k^*$ 及其对应的簇数 $S^*$。\n    - 将最终的 $S$、 $|\\mathcal{Y}|$、$\\epsilon$ 和 $\\delta$ 代入所提供的公式以计算 $N_{\\min}$：\n      $$N_{\\min} = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\!\\left(\\frac{2 S |\\mathcal{Y}|}{\\delta}\\right) \\right\\rceil$$\n\n这个系统化的过程保证能找到问题陈述所定义的最优解。", "answer": "```python\nimport numpy as np\nimport math\n\ndef generate_all_partitions(elements):\n    \"\"\"\n    Generates all partitions of a set of elements.\n    A partition is a list of lists (subsets).\n    \"\"\"\n    if not elements:\n        yield []\n        return\n\n    first = elements[0]\n    rest = elements[1:]\n\n    for smaller_partition in generate_all_partitions(rest):\n        if not smaller_partition:\n            yield [[first]]\n            continue\n        # Option 1: Add 'first' to an existing part of the smaller partition\n        for i in range(len(smaller_partition)):\n            new_partition = [sublist[:] for sublist in smaller_partition]\n            new_partition[i].append(first)\n            yield new_partition\n        # Option 2: 'first' forms a new part\n        yield smaller_partition + [[first]]\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            'p_x': [0.4, 0.3, 0.2, 0.1],\n            'p_y_given_x': [[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 0.0]],\n            'alpha': 0.95,\n            'epsilon': 0.02,\n            'delta': 0.01\n        },\n        # Test case 2\n        {\n            'p_x': [0.5, 0.3, 0.2],\n            'p_y_given_x': [[0.4, 0.4, 0.4], [0.6, 0.6, 0.6]],\n            'alpha': 0.6,\n            'epsilon': 0.01,\n            'delta': 0.05\n        },\n        # Test case 3\n        {\n            'p_x': [0.5, 0.3, 0.1, 0.1],\n            'p_y_given_x': [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0]],\n            'alpha': 0.95,\n            'epsilon': 0.02,\n            'delta': 0.02\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_x = np.array(case['p_x'])\n        p_y_given_x = np.array(case['p_y_given_x'])\n        alpha = case['alpha']\n        epsilon = case['epsilon']\n        delta = case['delta']\n\n        num_x = len(p_x)\n        num_y = p_y_given_x.shape[0]\n        \n        # Precompute joint probabilities p(x, y) = p(y|x) * p(x)\n        p_xy = p_y_given_x * p_x[np.newaxis, :]\n\n        # Generate and evaluate all partitions of the input space X\n        elements_to_partition = list(range(num_x))\n        all_partitions = list(generate_all_partitions(elements_to_partition))\n        \n        partition_results = []\n        for p in all_partitions:\n            if not p: continue\n            \n            accuracy = 0.0\n            for cluster in p:\n                if not cluster: continue\n                # Sum p(x,y) over all x in the cluster to get p(t,y)\n                cluster_p_y = np.sum(p_xy[:, cluster], axis=1)\n                # Bayes-optimal decision for this cluster contributes max_y p(t,y)\n                accuracy += np.max(cluster_p_y)\n            \n            partition_results.append({'S': len(p), 'acc': accuracy})\n\n        # Determine k_min and related values\n        max_k = math.ceil(math.log2(num_x)) if num_x > 1 else 0\n        k_min = -1\n        \n        best_acc_overall = -1.0\n        best_S_overall = -1\n        acc_at_k_min = -1.0\n        S_at_k_min = -1\n\n        for k in range(max_k + 1):\n            max_clusters = 2**k\n            \n            acc_k_star = -1.0\n            S_k_star = -1\n            \n            # Find the best accuracy for the current budget k\n            for res in partition_results:\n                if res['S'] = max_clusters:\n                    if res['acc'] > acc_k_star + 1e-12:\n                        acc_k_star = res['acc']\n                        S_k_star = res['S']\n                    elif abs(res['acc'] - acc_k_star)  1e-12:\n                        S_k_star = min(S_k_star, res['S'])\n\n            # Update overall best performance across all k\n            if acc_k_star > best_acc_overall + 1e-12:\n                best_acc_overall = acc_k_star\n                best_S_overall = S_k_star\n            elif abs(acc_k_star - best_acc_overall)  1e-12:\n                best_S_overall = min(best_S_overall, S_k_star)\n\n            # Check if we have met the target accuracy for the first time\n            if k_min == -1 and acc_k_star >= alpha - 1e-12:\n                k_min = k\n                acc_at_k_min = acc_k_star\n                S_at_k_min = S_k_star\n        \n        # Determine final parameters for reporting\n        final_k = k_min\n        if final_k == -1:\n            final_acc = best_acc_overall\n            final_S = best_S_overall\n        else:\n            final_acc = acc_at_k_min\n            final_S = S_at_k_min\n\n        # Calculate N_min using the final selected S\n        if final_S = 0:\n            N_min = 0\n        else:\n            log_term = math.log((2 * final_S * num_y) / delta)\n            N_min = math.ceil((1 / (2 * epsilon**2)) * log_term)\n\n        results.append([final_k, final_acc, N_min])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3138091"}, {"introduction": "信息论不仅可以指导模型设计，还能作为强大的诊断工具来分析模型的行为。生成模型是深度学习的一大热点，但常常会遇到“模式崩溃”（mode collapse）或“覆盖不足”（poor coverage）等问题。本练习将展示如何使用信息论工具来量化这些常见的失败模式。你将计算互信息 $I(Z;X)$ 和KL散度，从而学习如何从理论上诊断一个生成模型是产生了有限的多样性，还是未能生成数据全集中的某些样本。[@problem_id:3138062]", "problem": "给定一个离散潜变量生成模型，其潜变量 $Z$ 在一个有限集合中取值，可观测变量 $X$ 也在一个有限集合中取值。该模型定义了潜变量的先验概率 $p(Z)$ 和似然函数 $p(X \\mid Z)$，它们共同导出了模型的联合分布 $p(Z, X)$ 和模型的数据边缘分布 $p_{\\text{model}}(X)$。同时，也给定一个在 $X$ 的相同支撑集上的目标数据分布 $p_{\\text{data}}(X)$。您的任务是使用信息论中的基本离散定义来量化生成模型中常见的失效模式。具体来说，您需要计算 $Z$ 和 $X$ 之间的互信息、$Z$ 的熵，以及 $p_{\\text{data}}(X)$ 和 $p_{\\text{model}}(X)$ 之间的两个方向的 Kullback-Leibler (KL) 散度。\n\n所有对数都必须是自然对数，因此信息量以奈特（nats）为单位表示。对于任何涉及零概率的求和，应用标准的离散约定，即任何形式为 $0 \\cdot \\log(\\cdot)$ 的项对总和的贡献为 $0$。您的程序必须使用基本定义，从给定的分布中精确计算这些量，除了最终报告值中的四舍五入外，不进行任何近似。\n\n对于下文的每个测试用例，按所列顺序计算并报告以下四个量：\n- 模型联合分布下 $Z$ 和 $X$ 之间的互信息，记为 $I(Z; X)$。\n- 潜变量先验的熵，记为 $H(Z)$。\n- 从 $p_{\\text{data}}(X)$到 $p_{\\text{model}}(X)$ 的 Kullback-Leibler 散度，记为 $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}})$。\n- 从 $p_{\\text{model}}(X)$ 到 $p_{\\text{data}}(X)$ 的 Kullback-Leibler 散度，记为 $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}})$。\n\n重要实现细节：\n- 始终使用上述量的标准离散定义和自然对数。\n- 对于您输出的每个浮点数，四舍五入到 $6$ 位小数。\n- 您的程序必须生成单行输出，其中包含所有测试用例的结果，格式为逗号分隔的列表的列表，并用方括号括起来，例如 $[\\,[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4]\\,]$，其中每个符号代表一个四舍五入到 $6$ 位小数的浮点数。\n\n测试套件（所有概率都已明确给出，并在其各自的支撑集上总和为 $1$）：\n\n所有测试用例的共同支撑集：\n- 潜变量的支撑集为 $Z \\in \\{0, 1, 2\\}$。\n- 可观测变量的支撑集为 $X \\in \\{0, 1, 2\\}$。\n\n测试用例 $1$（均衡，接近理想覆盖）：\n- 先验 $p(Z) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$，即 $[\\,0.\\overline{3},\\, 0.\\overline{3},\\, 0.\\overline{3}\\,]$。\n- 似然 $p(X \\mid Z)$ 是确定性的且一一对应：对于 $Z=0,1,2$，该 $3 \\times 3$ 矩阵的行分别为 $[\\,1,\\,0,\\,0\\,]$、$[\\,0,\\,1,\\,0\\,]$ 和 $[\\,0,\\,0,\\,1\\,]$。\n- 数据分布 $p_{\\text{data}}(X) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$。\n\n测试用例 $2$（模式坍塌：所有潜变量的似然相同）：\n- 先验 $p(Z) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$。\n- 似然行相同：$p(X \\mid Z)$ 的每一行都等于 $[\\,0.7,\\, 0.2,\\, 0.1\\,]$。\n- 数据分布 $p_{\\text{data}}(X) = [\\,0.34,\\, 0.33,\\, 0.33\\,]$。\n\n测试用例 $3$（覆盖不匹配：重叠的似然和非均匀先验）：\n- 先验 $p(Z) = [\\,0.6,\\, 0.3,\\, 0.1\\,]$。\n- 似然矩阵的行：\n  - 对于 $Z=0$: $[\\,0.6,\\, 0.25,\\, 0.15\\,]$,\n  - 对于 $Z=1$: $[\\,0.3,\\, 0.4,\\, 0.3\\,]$,\n  - 对于 $Z=2$: $[\\,0.2,\\, 0.3,\\, 0.5\\,]$。\n- 数据分布 $p_{\\text{data}}(X) = [\\,0.5,\\, 0.3,\\, 0.2\\,]$。\n\n测试用例 $4$（边界条件：退化的潜变量先验）：\n- 先验 $p(Z) = [\\,1.0,\\, 0.0,\\, 0.0\\,]$。\n- 似然行（注意由于先验的原因，只有第一行有影响）：每一行都等于 $[\\,0.6,\\, 0.25,\\, 0.15\\,]$。\n- 数据分布 $p_{\\text{data}}(X) = [\\,0.6,\\, 0.25,\\, 0.15\\,]$。\n\n您的程序必须使用上述输入为每个测试用例计算所要求的四个量，并输出一个表示列表的列表的单行字符串，其中每个内部列表按顺序对应一个测试用例。将每个报告的浮点数四舍五入到 $6$ 位小数，并且除了指定的输出格式外，不要包含任何额外的文本。", "solution": "该问题要求计算一个离散潜变量生成模型的四个信息论量，这些量对于诊断其性能特征（如模式坍塌和覆盖范围）至关重要。该模型由潜变量的先验分布 $p(Z)$ 和给定潜变量的可观测变量的条件分布（似然）$p(X \\mid Z)$ 定义。它们结合起来形成一个联合分布 $p(Z, X) = p(X \\mid Z)p(Z)$。模型的有效性是相对于给定的目标数据分布 $p_{\\text{data}}(X)$ 进行评估的。\n\n要计算的四个量是：\n1.  模型联合分布下，潜变量 $Z$ 和可观测变量 $X$ 之间的互信息，记为 $I(Z; X)$。它衡量了在已知 $Z$ 的情况下 $X$ 的不确定性的减少程度，表明潜变量编码携带了多少关于生成输出的信息。\n2.  潜变量先验的熵，$H(Z)$。它量化了模型所使用的潜变量编码的多样性。\n3.  从数据分布到模型分布的 Kullback-Leibler (KL) 散度，$\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}})$。它衡量了模型对数据分布的近似程度，并严重惩罚模型未能在真实数据分布有支撑的区域生成样本的情况（即覆盖性差）。\n4.  从模型分布到数据分布的 KL 散度，$\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}})$。这种散度惩罚模型生成在真实数据分布下不太可能的样本的情况（即精度差或模式坍塌）。\n\n所有计算都必须遵循信息论的标准定义，使用自然对数，结果以奈特（nats）表示。\n\n**理论框架与计算策略**\n\n计算过程首先从给定的模型组件中推导出必要的分布，然后应用所需量的基本定义。\n\n**步骤 1：推导模型的边缘分布 $p_{\\text{model}}(X)$**\n模型的关于可观测变量 $X$ 的边缘分布是通过从联合分布 $p(Z, X)$ 中将潜变量 $Z$ 边缘化得到的。\n设 $Z$ 的取值为 $\\{z_i\\}_{i=1}^{N_Z}$，$X$ 的取值为 $\\{x_j\\}_{j=1}^{N_X}$。\n在模型下观测到 $x_j$ 的概率由全概率定律给出：\n$$p_{\\text{model}}(X=x_j) = \\sum_{i=1}^{N_Z} p(Z=z_i, X=x_j) = \\sum_{i=1}^{N_Z} p(X=x_j \\mid Z=z_i) p(Z=z_i)$$\n作为计算 $I(Z; X)$ 和 KL 散度的先决条件，必须为每个测试用例执行此计算。\n\n**步骤 2：计算 $H(Z)$**\n离散潜变量先验 $p(Z)$ 的熵由标准公式给出：\n$$H(Z) = - \\sum_{i=1}^{N_Z} p(Z=z_i) \\log p(Z=z_i)$$\n应用了形式为 $0 \\cdot \\log(0)$ 的项对总和的贡献为 $0$ 的约定。\n\n**步骤 3：计算 $I(Z; X)$**\n互信息 $I(Z; X)$ 可以使用几个等价公式计算。一个计算上方便的形式是 $I(Z; X) = H(X) - H(X \\mid Z)$。\n首先，我们计算模型边缘分布的熵 $H(X)$：\n$$H(X) = - \\sum_{j=1}^{N_X} p_{\\text{model}}(X=x_j) \\log p_{\\text{model}}(X=x_j)$$\n接下来，我们计算条件熵 $H(X \\mid Z)$，它是似然分布 $p(X \\mid Z=z_i)$ 的熵在先验 $p(Z)$ 下的期望值：\n$$H(X \\mid Z) = \\sum_{i=1}^{N_Z} p(Z=z_i) H(X \\mid Z=z_i)$$\n其中每个条件分布的熵为：\n$$H(X \\mid Z=z_i) = - \\sum_{j=1}^{N_X} p(X=x_j \\mid Z=z_i) \\log p(X=x_j \\mid Z=z_i)$$\n然后，互信息是这个差值：\n$$I(Z; X) = H(X) - H(X \\mid Z)$$\n\n**步骤 4：计算 KL 散度**\n在相同支撑集上的两个离散概率分布 $P(Y)$ 和 $Q(Y)$ 之间的 KL 散度定义为：\n$$\\mathrm{KL}(P \\| Q) = \\sum_{y} P(y) \\log \\frac{P(y)}{Q(y)}$$\n所要求的量是此定义的实例：\n-   $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}}) = \\sum_{j=1}^{N_X} p_{\\text{data}}(X=x_j) \\log \\frac{p_{\\text{data}}(X=x_j)}{p_{\\text{model}}(X=x_j)}$\n-   $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}}) = \\sum_{j=1}^{N_X} p_{\\text{model}}(X=x_j) \\log \\frac{p_{\\text{model}}(X=x_j)}{p_{\\text{data}}(X=x_j)}$\n\n在分母中的概率为 $0$ 而对应的分子非零的情况下，KL 散度是无穷大。然而，所提供的测试用例没有出现这个问题。如果分子中的概率为 $0$，则该项为 $0$ 的约定也同样适用。\n\n这种对基本定义的系统性应用，为计算每个测试用例所需的量提供了一种完整而精确的方法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Computes four information-theoretic quantities for a series of\n    latent-variable generative model test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"p_z\": np.array([1/3, 1/3, 1/3]),\n            \"p_x_cond_z\": np.array([[1.0, 0.0, 0.0],\n                                    [0.0, 1.0, 0.0],\n                                    [0.0, 0.0, 1.0]]),\n            \"p_data_x\": np.array([1/3, 1/3, 1/3])\n        },\n        {\n            \"p_z\": np.array([1/3, 1/3, 1/3]),\n            \"p_x_cond_z\": np.array([[0.7, 0.2, 0.1],\n                                    [0.7, 0.2, 0.1],\n                                    [0.7, 0.2, 0.1]]),\n            \"p_data_x\": np.array([0.34, 0.33, 0.33])\n        },\n        {\n            \"p_z\": np.array([0.6, 0.3, 0.1]),\n            \"p_x_cond_z\": np.array([[0.6, 0.25, 0.15],\n                                    [0.3, 0.4, 0.3],\n                                    [0.2, 0.3, 0.5]]),\n            \"p_data_x\": np.array([0.5, 0.3, 0.2])\n        },\n        {\n            \"p_z\": np.array([1.0, 0.0, 0.0]),\n            \"p_x_cond_z\": np.array([[0.6, 0.25, 0.15],\n                                    [0.6, 0.25, 0.15],\n                                    [0.6, 0.25, 0.15]]),\n            \"p_data_x\": np.array([0.6, 0.25, 0.15])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p_z = case[\"p_z\"]\n        p_x_cond_z = case[\"p_x_cond_z\"]\n        p_data_x = case[\"p_data_x\"]\n\n        # Calculate H(Z), the entropy of the latent prior.\n        h_z = entropy(p_z, base=np.e)\n\n        # Calculate p_model(X), the model's marginal distribution.\n        # This is sum_z p(x|z)p(z), which is a matrix multiplication.\n        p_model_x = p_z @ p_x_cond_z\n\n        # Calculate I(Z; X) = H(X) - H(X|Z)\n        # H(X) is the entropy of the model's marginal.\n        h_x = entropy(p_model_x, base=np.e)\n        \n        # H(X|Z) is the expectation of the entropy of the likelihoods.\n        # E_p(z) [H(p(X|Z=z))]\n        h_x_cond_z_per_z = np.array([entropy(p, base=np.e) for p in p_x_cond_z])\n        h_x_cond_z = np.dot(p_z, h_x_cond_z_per_z)\n        \n        i_zx = h_x - h_x_cond_z\n\n        # Calculate KL(p_data || p_model)\n        # scipy.stats.entropy calculates KL divergence when qk is provided.\n        kl_data_model = entropy(pk=p_data_x, qk=p_model_x, base=np.e)\n\n        # Calculate KL(p_model || p_data)\n        kl_model_data = entropy(pk=p_model_x, qk=p_data_x, base=np.e)\n\n        # Store the four required quantities in order.\n        result_for_case = [i_zx, h_z, kl_data_model, kl_model_data]\n        all_results.append(result_for_case)\n\n    # Format the final output string as a list of lists with 6 decimal places.\n    inner_lists_str = []\n    for r in all_results:\n        # Using f-string formatting to ensure 6 decimal places are shown.\n        r_formatted_str = ','.join([f'{val:.6f}' for val in r])\n        inner_lists_str.append(f\"[{r_formatted_str}]\")\n    \n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "3138062"}]}