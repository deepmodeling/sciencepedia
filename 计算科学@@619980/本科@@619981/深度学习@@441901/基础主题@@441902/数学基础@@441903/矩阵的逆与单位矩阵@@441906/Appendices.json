{"hands_on_practices": [{"introduction": "理论上，线性方程组 $Ax=b$ 的解是 $x = A^{-1}b$。然而，在实践中，直接计算矩阵的逆 $A^{-1}$ 往往是一个坏主意，尤其是在处理接近奇异或病态的矩阵时，这会导致严重的数值不稳定性。这个练习将通过受控的数值实验，让你直观地理解这一问题，并展示如何通过添加一个小的单位矩阵项（即 $A + \\lambda I$）来显著改善矩阵的条件数，从而提高求解过程的鲁棒性。[@problem_id:3147728]", "problem": "给定一系列线性系统，这些系统模拟了深度训练循环中的内部迭代。在每次迭代中，一个对称半正定矩阵 $A$ 由局部二次近似产生，然后通过求解 $A x = b$ 来计算参数更新。您的任务是在受控的合成实验中，量化三种方法的数值稳定性，并确定使用单位矩阵移位的系统如何提高鲁棒性：显式矩阵求逆以计算 $A^{-1} b$、直接线性求解 $A x = b$ 以及移位的线性求解 $(A + \\lambda I) x = b$（其中 $\\lambda > 0$）。您必须采用编程方法，以双精度实现所有计算，并将所有测试用例的结果汇总到单行、机器可读的输出中。\n\n本问题的基本原理：\n- 线性系统 $A x = b$ 和单位矩阵 $I$。\n- 2-范数 $\\| \\cdot \\|_2$ 和 2-范数条件数 $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$。对于对称正定矩阵 $A$，$\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$，其中 $\\lambda_{\\max}(A)$ 和 $\\lambda_{\\min}(A)$ 分别是 $A$ 的最大和最小特征值。\n- 对于对称矩阵，$A + \\lambda I$ 的特征值为 $\\lambda_i(A) + \\lambda$，因此 $\\kappa_2(A + \\lambda I) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}$，其中 $\\lambda > 0$ 会减小条件数。\n- 通过高斯消元法或 Cholesky 分解求解线性系统的后向稳定性，与在浮点运算中显式构造 $A^{-1}$ 的不稳定性相对比。\n\n待计算的度量定义：\n- 对于一个方法，其放大因子 $g$ 定义为 $g = \\frac{\\|x - x^\\star\\|_2}{\\|\\eta\\|_2}$，其中给定噪声向量 $\\eta$ 和基准真相 $x^\\star$，$x$ 是该方法在带噪声的右侧项 $b = A x^\\star + \\eta$ 上产生的估计值。\n- 条件数 $\\kappa_2(A)$ 和 $\\kappa_2(A + \\lambda I)$。\n- 在干净的右侧项 $b_0 = A x^\\star$ 上，显式求逆和直接求解之间的算法差异 $\\delta_{\\text{inv}}$ 定义为 $\\delta_{\\text{inv}} = \\frac{\\|x_{\\text{inv,clean}} - x_{\\text{solve,clean}}\\|_2}{\\|x_{\\text{solve,clean}}\\|_2}$，其中 $x_{\\text{inv,clean}} = A^{-1} b_0$，$x_{\\text{solve,clean}}$ 是通过直接求解器求解 $A x = b_0$ 得到的解。\n\n实验设置：\n- 所有随机数必须由一个使用指定种子初始化的可复现伪随机数生成器生成。\n- 对每个测试用例，构造矩阵 $A$，采样一个各项在 $[-1,1]$ 内均匀分布的基准真相向量 $x^\\star$，构成干净的右侧项 $b_0 = A x^\\star$，从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取具有独立分量的高斯噪声向量 $\\eta$，并设置 $b = b_0 + \\eta$。\n- 通过显式求逆计算 $x_{\\text{inv}} = A^{-1} b$，通过求解 $A x = b$ 计算 $x_{\\text{solve}}$，通过求解 $(A + \\lambda I) x = b$ 计算 $x_{\\text{shift}}$。\n- 对于因奇异性或数值崩溃而失败的方法，将相应的度量设置为 $+\\infty$。\n- 对每个测试用例，报告列表 $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$，其中 $\\text{is\\_superior}$ 是 $g_{\\text{shift}}  \\min(g_{\\text{solve}}, g_{\\text{inv}})$ 的布尔值。\n\n带参数值的测试套件：\n- 用例 1（随机对称正定）：维度 $n = 10$，用种子 $7$ 构造 $A = Q^\\top Q + \\alpha I$，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 的各项在 $[-1,1]$ 内均匀分布，设置 $\\alpha = 10^{-2}$，基准真相种子 $11$，噪声种子 $13$，噪声标准差 $\\sigma = 10^{-6}$，移位 $\\lambda = 10^{-1}$。\n- 用例 2（希尔伯特矩阵）：维度 $n = 12$，构造希尔伯特矩阵 $A$，其元素为 $A_{ij} = \\frac{1}{i + j + 1}$，其中索引 $i, j \\in \\{0, 1, \\dots, n-1\\}$，基准真相种子 $17$，噪声种子 $19$，噪声标准差 $\\sigma = 10^{-4}$，移位 $\\lambda = 10^{-1}$。\n- 用例 3（秩亏的对称半正定）：维度 $n = 10$，用种子 $23$ 构造 $B \\in \\mathbb{R}^{r \\times n}$，其中 $r = 8$，其各项在 $[-1,1]$ 内均匀分布，并设置 $A = B^\\top B$，基准真相种子 $29$，噪声种子 $31$，噪声标准差 $\\sigma = 10^{-6}$，移位 $\\lambda = 1$。\n- 用例 4（预定谱）：维度 $n = 12$，构造 $A = U \\operatorname{diag}(e) U^\\top$，其中 $U$ 是通过对由种子 $37$ 生成的高斯矩阵进行 $\\operatorname{QR}$ 分解得到的，特征值 $e_i$ 满足 $\\log_{10}(e_i)$ 在 $-8$ 和 $0$ 之间线性间隔，即 $e_i = 10^{(-8 + 8 \\frac{i}{n - 1})}$，其中 $i \\in \\{0, 1, \\dots, n - 1\\}$，基准真相种子 $41$，噪声种子 $43$，噪声标准差 $\\sigma = 10^{-5}$，移位 $\\lambda = 10^{-2}$。\n\n您的程序必须：\n- 按指定顺序为每个用例实现上述构造和度量计算。\n- 生成单行输出，包含一个由方括号括起来的、逗号分隔的结果列表，其中每个测试用例的结果本身是一个形式为 $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$ 的、由方括号括起来的逗号分隔列表。", "solution": "我们从基础线性代数原理开始。对于线性系统 $A x = b$，当 $A$ 为非奇异矩阵时，在精确算术中，解映射为 $b \\mapsto x = A^{-1} b$。该映射对扰动的敏感性由条件数 $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$ 决定。在浮点运算中，显式求逆会放大舍入误差，因为计算 $A^{-1}$ 本身是一个数值敏感的操作，之后再乘以 $b$ 会引入更多误差。相比之下，基于高斯消元法或 Cholesky 分解的直接线性求解器是后向稳定的：它们计算的是一个轻微扰动系统 $(A + \\Delta A) x = b$ 的精确解，其中 $\\|\\Delta A\\|$ 与机器精度成正比。因此，显式构造并使用 $A^{-1}$ 的稳定性低于求解 $A x = b$。\n\n对于对称正定矩阵 $A$，其 2-范数等于最大奇异值，也即最大特征值，而 $\\|A^{-1}\\|_2$ 等于最小特征值的倒数。因此，$\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$。考虑移位系统 $(A + \\lambda I) x = b$，其中 $\\lambda  0$。$A + \\lambda I$ 的特征值是 $\\lambda_i(A) + \\lambda$，对应于 $A$ 的每个特征值 $\\lambda_i(A)$。所以，\n$$\n\\kappa_2(A + \\lambda I) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}.\n$$\n假设 $0  \\lambda_{\\min}(A) \\le \\lambda_{\\max}(A)$。定义 $f(\\lambda) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}$，其中 $\\lambda  0$。其导数为\n$$\nf'(\\lambda) = \\frac{\\lambda_{\\min}(A) - \\lambda_{\\max}(A)}{(\\lambda_{\\min}(A) + \\lambda)^2}  0,\n$$\n这意味着 $f$ 是严格递减的。因此，加上 $\\lambda I$ 会减小条件数。由于 $(A + \\lambda I)^{-1}$ 的算子范数为 $\\frac{1}{\\lambda_{\\min}(A) + \\lambda}$，移位系统减小了 $b$ 中扰动的潜在放大。\n\n当右侧项带噪声 $b = A x^\\star + \\eta$ 时，解 $x = A^{-1} b$ 的误差 $\\|x - x^\\star\\|_2$ 在一阶扰动分析中，当 $A$ 良态且算法后向稳定时，其大小与 $\\|A^{-1}\\|_2 \\|\\eta\\|_2$ 成比例。对于病态矩阵 $A$，$\\|A^{-1}\\|_2$ 很大，噪声会被显著放大。使用 $(A + \\lambda I)^{-1}$ 将 $\\|A^{-1}\\|_2$ 减小到 $\\|(A + \\lambda I)^{-1}\\|_2$，从而抑制噪声放大，其代价是使解偏离 $x^\\star$。在许多深度学习的背景下，这种移位对应于 Levenberg–Marquardt 阻尼或 Tikhonov 型正则化，通过控制步长来提高鲁棒性。我们使用的度量，即放大因子\n$$\ng = \\frac{\\|x - x^\\star\\|_2}{\\|\\eta\\|_2},\n$$\n捕捉了这种权衡，较小的 $g$ 表示对噪声有更好的鲁棒性。\n\n算法设计：\n- 对每个测试用例，确定性地遵循指定方案构造 $A$：\n  - 用例 1：$A = Q^\\top Q + \\alpha I$，其中 $Q$ 在 $[-1,1]$ 内均匀分布。\n  - 用例 2：希尔伯特矩阵 $A_{ij} = \\frac{1}{i + j + 1}$。\n  - 用例 3：$A = B^\\top B$，其中 $B$ 在 $[-1,1]$ 内均匀分布且秩 $r  n$。\n  - 用例 4：$A = U \\operatorname{diag}(e) U^\\top$，其中 $U$ 是来自 $\\operatorname{QR}$ 分解的正交规范矩阵，特征值 $e_i = 10^{(-8 + 8 \\frac{i}{n - 1})}$。\n- 用指定的种子生成在 $[-1,1]$ 内均匀分布的 $x^\\star$ 和噪声 $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I)$。计算 $b_0 = A x^\\star$ 和 $b = b_0 + \\eta$。\n- 使用 2-范数计算条件数 $\\kappa_2(A)$ 和 $\\kappa_2(A + \\lambda I)$。\n- 计算解：\n  - 显式求逆解 $x_{\\text{inv}} = A^{-1} b$ 以及在干净右侧项上的解 $x_{\\text{inv,clean}} = A^{-1} b_0$。\n  - 直接求解 $A x = b$ 得到 $x_{\\text{solve}}$ 以及在干净右侧项上的解 $x_{\\text{solve,clean}}$。\n  - 移位求解 $(A + \\lambda I) x = b$ 得到 $x_{\\text{shift}}$。\n- 计算度量：\n  - 放大因子 $g_{\\text{inv}} = \\frac{\\|x_{\\text{inv}} - x^\\star\\|_2}{\\|\\eta\\|_2}$，$g_{\\text{solve}} = \\frac{\\|x_{\\text{solve}} - x^\\star\\|_2}{\\|\\eta\\|_2}$ 和 $g_{\\text{shift}} = \\frac{\\|x_{\\text{shift}} - x^\\star\\|_2}{\\|\\eta\\|_2}$。\n  - 算法差异 $\\delta_{\\text{inv}} = \\frac{\\|x_{\\text{inv,clean}} - x_{\\text{solve,clean}}\\|_2}{\\|x_{\\text{solve,clean}}\\|_2}$。\n- 将优越性确定为 $g_{\\text{shift}}  \\min(g_{\\text{solve}}, g_{\\text{inv}})$ 的布尔值。\n- 通过捕获线性代数错误来处理奇异或近奇异情况；如果求逆或求解失败，将相应的度量设置为 $+\\infty$。\n- 将每个用例的结果汇总成指定的单行输出格式。\n\n为何这能证明其优越性：\n- 对于对称正定矩阵 $A$，加上 $\\lambda I$ 会统一增加所有特征值，从而收紧谱并降低 $\\kappa_2$。逆的算子范数从 $\\frac{1}{\\lambda_{\\min}(A)}$ 减小到 $\\frac{1}{\\lambda_{\\min}(A) + \\lambda}$，从而减少噪声放大。根据经验，$g_{\\text{shift}}$ 应该小于 $g_{\\text{inv}}$ 和 $g_{\\text{solve}}$，尤其是在病态或秩亏的情况下。\n- 显式求逆往往具有更大的 $\\delta_{\\text{inv}}$，尤其对于病态矩阵 $A$，这揭示了其相比直接求解器较差的数值稳定性。\n- 测试套件涵盖：\n  - 一个中等条件数的系统，其中所有方法表现相当，移位提供了轻微的鲁棒性提升。\n  - 一个条件数非常大的希尔伯特矩阵，其中移位显著提高了稳定性。\n  - 一个秩亏的对称半正定系统，其中未移位的方法可能会失败或产生非常大的放大，而移位对问题进行了正则化。\n  - 一个具有跨越多个数量级的预定谱的系统，以突出条件数问题并显示移位的好处。\n\n最终的输出格式是单行文本，包含一个由方括号括起来的、逗号分隔的列表，其中每个元素是对应一个用例的列表 $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$，并按四个用例的顺序排列。", "answer": "```python\nimport numpy as np\n\ndef hilbert(n: int) - np.ndarray:\n    i = np.arange(n).reshape(-1, 1)\n    j = np.arange(n).reshape(1, -1)\n    return 1.0 / (i + j + 1.0)\n\ndef make_spd_via_Q(n: int, alpha: float, seed: int) - np.ndarray:\n    rng = np.random.default_rng(seed)\n    Q = rng.uniform(-1.0, 1.0, size=(n, n))\n    A = Q.T @ Q + alpha * np.eye(n)\n    return A\n\ndef make_rank_def_spd(n: int, r: int, seed: int) - np.ndarray:\n    rng = np.random.default_rng(seed)\n    B = rng.uniform(-1.0, 1.0, size=(r, n))\n    A = B.T @ B\n    return A\n\ndef make_spd_with_eigs(n: int, seed: int) - np.ndarray:\n    # Create orthonormal U via QR of a Gaussian matrix\n    rng = np.random.default_rng(seed)\n    G = rng.normal(0.0, 1.0, size=(n, n))\n    Q, _ = np.linalg.qr(G)\n    # Prescribed eigenvalues: log10 spaced from -8 to 0\n    exponents = -8.0 + 8.0 * (np.arange(n) / (n - 1))\n    eigs = 10.0 ** exponents\n    A = Q @ np.diag(eigs) @ Q.T\n    return A\n\ndef amplification_factor(x_est: np.ndarray, x_true: np.ndarray, noise: np.ndarray) - float:\n    num = np.linalg.norm(x_est - x_true, ord=2)\n    den = np.linalg.norm(noise, ord=2)\n    # Avoid division by zero; if no noise, define amplification as +inf\n    if den == 0.0:\n        return float('inf')\n    return num / den\n\ndef safe_inv_solve(A: np.ndarray, b: np.ndarray):\n    # Returns solution via explicit inverse, or raises/returns inf if fails\n    try:\n        A_inv = np.linalg.inv(A)\n        return A_inv @ b\n    except Exception:\n        return None\n\ndef safe_direct_solve(A: np.ndarray, b: np.ndarray):\n    try:\n        return np.linalg.solve(A, b)\n    except Exception:\n        return None\n\ndef compute_case(case):\n    case_type = case['type']\n    if case_type == 'spd_q':\n        A = make_spd_via_Q(case['n'], case['alpha'], case['seed_A'])\n    elif case_type == 'hilbert':\n        A = hilbert(case['n'])\n    elif case_type == 'rank_def_spd':\n        A = make_rank_def_spd(case['n'], case['rank'], case['seed_A'])\n    elif case_type == 'spd_eigs':\n        A = make_spd_with_eigs(case['n'], case['seed_A'])\n    else:\n        raise ValueError(\"Unknown case type\")\n\n    n = case['n']\n    # Ground truth and noise\n    rng_x = np.random.default_rng(case['seed_x'])\n    x_true = rng_x.uniform(-1.0, 1.0, size=n)\n    b_clean = A @ x_true\n\n    rng_noise = np.random.default_rng(case['seed_noise'])\n    noise = rng_noise.normal(0.0, case['noise_std'], size=n)\n    b_noisy = b_clean + noise\n\n    # Condition numbers\n    try:\n        cond_A = np.linalg.cond(A)  # 2-norm condition number\n    except Exception:\n        cond_A = float('inf')\n\n    A_shift = A + case['lambda'] * np.eye(n)\n    try:\n        cond_A_shift = np.linalg.cond(A_shift)\n    except Exception:\n        cond_A_shift = float('inf')\n\n    # Solutions\n    x_inv_noisy = safe_inv_solve(A, b_noisy)\n    x_solve_noisy = safe_direct_solve(A, b_noisy)\n    x_shift_noisy = safe_direct_solve(A_shift, b_noisy)\n\n    # Clean RHS for algorithmic discrepancy\n    x_inv_clean = safe_inv_solve(A, b_clean)\n    x_solve_clean = safe_direct_solve(A, b_clean)\n\n    # Amplification factors\n    if x_inv_noisy is None:\n        g_inv = float('inf')\n    else:\n        g_inv = amplification_factor(x_inv_noisy, x_true, noise)\n\n    if x_solve_noisy is None:\n        g_solve = float('inf')\n    else:\n        g_solve = amplification_factor(x_solve_noisy, x_true, noise)\n\n    if x_shift_noisy is None:\n        g_shift = float('inf')\n    else:\n        g_shift = amplification_factor(x_shift_noisy, x_true, noise)\n\n    # Algorithmic discrepancy\n    if (x_inv_clean is None) or (x_solve_clean is None):\n        delta_inv = float('inf')\n    else:\n        denom = np.linalg.norm(x_solve_clean, ord=2)\n        if denom == 0.0:\n            delta_inv = float('inf')\n        else:\n            delta_inv = np.linalg.norm(x_inv_clean - x_solve_clean, ord=2) / denom\n\n    is_superior = g_shift  min(g_solve, g_inv)\n\n    return [g_inv, g_solve, g_shift, cond_A, cond_A_shift, delta_inv, is_superior]\n\ndef solve():\n    test_cases = [\n        {\n            'type': 'spd_q',\n            'n': 10,\n            'alpha': 1e-2,\n            'seed_A': 7,\n            'seed_x': 11,\n            'seed_noise': 13,\n            'noise_std': 1e-6,\n            'lambda': 1e-1,\n        },\n        {\n            'type': 'hilbert',\n            'n': 12,\n            'seed_A': None,  # Deterministic\n            'seed_x': 17,\n            'seed_noise': 19,\n            'noise_std': 1e-4,\n            'lambda': 1e-1,\n        },\n        {\n            'type': 'rank_def_spd',\n            'n': 10,\n            'rank': 8,\n            'seed_A': 23,\n            'seed_x': 29,\n            'seed_noise': 31,\n            'noise_std': 1e-6,\n            'lambda': 1.0,\n        },\n        {\n            'type': 'spd_eigs',\n            'n': 12,\n            'seed_A': 37,\n            'seed_x': 41,\n            'seed_noise': 43,\n            'noise_std': 1e-5,\n            'lambda': 1e-2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(compute_case(case))\n\n    # Print single-line, bracket-enclosed, comma-separated list of per-case lists\n    # Ensure booleans and floats are printed via default str\n    def format_item(item):\n        if isinstance(item, list):\n            return \"[\" + \",\".join(format_item(x) for x in item) + \"]\"\n        else:\n            return str(item)\n\n    print(\"[\" + \",\".join(format_item(r) for r in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3147728"}, {"introduction": "在优化算法中，预处理是一种旨在加速收敛的强大技术，其核心思想是用一个近似逆矩阵来“矫正”梯度。这个练习将探讨如何使用基于单位矩阵的诺伊曼级数来构造一个近似逆作为预处理器。你将推导并分析这种近似方法如何影响梯度下降算法的稳定性，从而连接起矩阵求逆、单位矩阵和优化效率这些核心概念。[@problem_id:3147772]", "problem": "考虑一个监督线性模型，其特征矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，目标向量为 $y \\in \\mathbb{R}^{n}$。平方损失为 $L(w) = \\tfrac{1}{2}\\lVert X w - y \\rVert_2^2$，其梯度为 $\\nabla L(w) = X^\\top (X w - y)$。定义特征格拉姆矩阵 (feature Gram matrix) $G = X^\\top X \\in \\mathbb{R}^{d \\times d}$。假设 $G$ 是对称正定 (SPD) 矩阵，因此其所有特征值都严格为正。考虑使用预处理器 $M \\in \\mathbb{R}^{d \\times d}$ 的预处理梯度下降：\n$$\nw_{t+1} = w_t - \\eta\\, M \\nabla L(w_t).\n$$\n该迭代导出线性误差动态\n$$\ne_{t+1} = \\left(I - \\eta\\, M G\\right) e_t,\n$$\n其中 $e_t = w_t - w^\\star$，$w^\\star$ 是唯一的最小化子。\n\n从以下基本原则出发：\n- 对于任何 SPD 矩阵 $A$，所有特征值均为实数和正数，并且存在一个标准正交特征基。\n- 线性迭代 $z_{t+1} = B z_t$ 稳定，当且仅当 $B$ 的谱半径严格小于 $1$。\n- 如果 $A$ 可对角化，其特征值为 $\\{\\lambda_i\\}$，且 $p$ 是一个多项式，那么 $p(A)$ 的特征值为 $\\{p(\\lambda_i)\\}$。\n\n定义精确逆预处理器 $M_{\\text{exact}} = G^{-1}$ 和截断诺伊曼级数预处理器（对于标量 $\\alpha  0$ 和非负整数 $K$）：\n$$\nM_{K,\\alpha} = \\frac{1}{\\alpha}\\sum_{k=0}^{K} \\left(I - \\frac{1}{\\alpha} G\\right)^k.\n$$\n始终假设 $\\alpha$ 严格大于 $G$ 的最大特征值，以确保诺伊曼级数有良好定义。\n\n您的任务是：\n1. 仅使用上面列出的基本原则和第一性原理，根据 SPD 矩阵 $M G$ 的最大特征值，推导预处理梯度下降步长 $\\eta$ 的一般稳定性条件。\n2. 将您的结果特化到 $M_{\\text{exact}}$，并解释这对该精确逆预处理器的最大稳定步长意味着什么。\n3. 将您的结果特化到 $M_{K,\\alpha}$，并用 $G$ 的特征值、截断阶数 $K$ 和标量 $\\alpha$ 表示最大稳定步长。同时，也用 $G$ 的特征值、$K$ 和 $\\alpha$ 推导算子范数 $\\lVert I - M_{K,\\alpha} G \\rVert_2$ 的公式。您的表达式必须在所述假设下有效。\n\n测试套件和数据构建：\n- 在维度 $d=5$ 中进行计算。\n- 对于每个测试用例，将 $G$ 构建为具有指定特征值 $\\{\\lambda_1,\\dots,\\lambda_5\\}$ 的对角矩阵，即 $G = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_5)$。可以取 $X = \\mathrm{diag}(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_5})$，使得 $G = X^\\top X$。\n- 对于每个测试用例，令 $\\alpha = \\text{alpha\\_factor} \\times \\lambda_{\\max}(G)$，其中 $\\lambda_{\\max}(G)$ 是 $G$ 的最大特征值。\n\n使用以下五个测试用例组成的测试套件，每个用例由一个三元组 $\\left(\\{\\lambda_i\\}_{i=1}^5,\\, \\text{alpha\\_factor},\\, K\\right)$ 给出：\n- 用例 1: $\\{\\lambda_i\\} = \\{1.0,\\, 2.5,\\, 4.0,\\, 7.0,\\, 10.0\\}$, $\\text{alpha\\_factor} = 1.5$, $K = 0$。\n- 用例 2: $\\{\\lambda_i\\} = \\{1.0,\\, 2.5,\\, 4.0,\\, 7.0,\\, 10.0\\}$, $\\text{alpha\\_factor} = 1.5$, $K = 5$。\n- 用例 3: $\\{\\lambda_i\\} = \\{1.0,\\, 10.0,\\, 50.0,\\, 200.0,\\, 1000.0\\}$, $\\text{alpha\\_factor} = 1.1$, $K = 3$。\n- 用例 4: $\\{\\lambda_i\\} = \\{1.0,\\, 1.125,\\, 1.25,\\, 1.375,\\, 1.5\\}$, $\\text{alpha\\_factor} = 1.01$, $K = 1$。\n- 用例 5: $\\{\\lambda_i\\} = \\{1.0,\\, 5.0,\\, 25.0,\\, 125.0,\\, 625.0\\}$, $\\text{alpha\\_factor} = 10.0$, $K = 0$。\n\n对于每个用例 $j \\in \\{1,2,3,4,5\\}$，计算以下三个量：\n- 精确预处理器的最大稳定步长，记为 $\\eta_{\\max}^{\\text{exact}}(j)$。\n- 截断诺伊曼预处理器的最大稳定步长，记为 $\\eta_{\\max}^{\\text{neu}}(j)$。\n- 截断诺伊曼预处理器下与单位矩阵偏差的谱算子范数，$\\lVert I - M_{K,\\alpha} G \\rVert_2(j)$。\n\n最终输出格式：\n- 您的程序必须生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，列表内精确包含 15 个浮点数，顺序如下：\n$$\n[\\eta_{\\max}^{\\text{exact}}(1),\\, \\eta_{\\max}^{\\text{neu}}(1),\\, \\lVert I - M_{K,\\alpha} G \\rVert_2(1),\\, \\eta_{\\max}^{\\text{exact}}(2),\\, \\eta_{\\max}^{\\text{neu}}(2),\\, \\lVert I - M_{K,\\alpha} G \\rVert_2(2),\\, \\dots,\\, \\eta_{\\max}^{\\text{exact}}(5),\\, \\eta_{\\max}^{\\text{neu}}(5),\\, \\lVert I - M_{K,\\alpha} G \\rVert_2(5)].\n$$\n程序不读取任何输入；所有数据都如上所述精确嵌入。所有输出都是无单位的实数。程序在打印前应将每个浮点数四舍五入到 10 位小数。", "solution": "该问题被评估为有效，因为它在数值线性代数和优化的原理上有科学依据，问题设定良好且目标明确，并且内部一致。因此，我们可以进行形式化推导。\n\n分析从提供的预处理梯度下降的线性误差动态开始：\n$$\ne_{t+1} = \\left(I - \\eta M G\\right) e_t\n$$\n其中 $e_t = w_t - w^\\star$ 是第 $t$ 次迭代的误差，$I$ 是单位矩阵，$\\eta  0$ 是步长，$M$ 是预处理器，$G$ 是对称正定 (SPD) 的特征格拉姆矩阵。\n\n根据所提供的基本原则，线性迭代 $z_{t+1} = B z_t$ 稳定，当且仅当迭代矩阵 $B$ 的谱半径（记为 $\\rho(B)$）严格小于 1。在我们的情况下，迭代矩阵是 $B = I - \\eta M G$。谱半径定义为 $\\rho(B) = \\max_j |\\lambda_j(B)|$，其中 $\\{\\lambda_j(B)\\}$ 是 $B$ 的特征值。因此，稳定性条件是：\n$$\n\\rho(I - \\eta M G)  1\n$$\n设矩阵乘积 $MG$ 的特征值为 $\\{\\mu_j\\}$。那么 $I - \\eta M G$ 的特征值为 $\\{1 - \\eta \\mu_j\\}$。稳定性条件变为 $\\max_j |1 - \\eta \\mu_j|  1$。此不等式等价于对 $MG$ 的所有特征值 $\\mu_j$ 都有 $-1  1 - \\eta \\mu_j  1$。\n\n不等式的右侧 $1 - \\eta \\mu_j  1$ 意味着 $-\\eta \\mu_j  0$。由于 $\\eta  0$，这要求对所有 $j$ 都有 $\\mu_j  0$。不等式的左侧 $-1  1 - \\eta \\mu_j$ 意味着 $\\eta \\mu_j  2$。结合这两点，稳定性条件为对 $MG$ 的所有特征值 $\\mu_j$ 都有 $0  \\eta \\mu_j  2$。\n\n问题指出 $G$ 是 SPD 矩阵。对于所考虑的两个特定预处理器 $M_{\\text{exact}} = G^{-1}$ 和 $M_{K,\\alpha}$，我们必须验证它们也是 SPD 矩阵，这将确保乘积 $MG$ 具有严格为正的特征值。由于 $G$ 是 SPD 矩阵，其逆 $G^{-1}$ 也是 SPD 矩阵。矩阵 $M_{K,\\alpha}$ 是 $G$ 的多项式。由于 $G$ 是 SPD 矩阵，其特征值为 $\\{\\lambda_i  0\\}$，因此 $M_{K,\\alpha}$ 也是 SPD 矩阵，其特征值为 $m_i = \\frac{1}{\\lambda_i}[1 - (1 - \\frac{\\lambda_i}{\\alpha})^{K+1}]$。条件 $\\alpha  \\lambda_{\\max}(G)$ 确保 $0  \\lambda_i/\\alpha  1$，这使得 $m_i  0$。因为 $M$ 和 $G$ 都是 SPD 矩阵，矩阵 $MG$ 与 SPD 矩阵 $G^{1/2} M G^{1/2}$ 相似，因此具有严格为正的实特征值 $\\{\\mu_j\\}$。\n\n因此，稳定性条件简化为 $\\eta \\max_j \\mu_j  2$，可以写成：\n$$\n0  \\eta  \\frac{2}{\\lambda_{\\max}(MG)}\n$$\n这就构成了一般的步长 $\\eta$ 的稳定性条件。最大稳定步长是 $\\eta_{\\max} = \\frac{2}{\\lambda_{\\max}(MG)}$。\n\n接下来，我们将此结果特化到给定的两个预处理器。\n\n对于精确逆预处理器，$M = M_{\\text{exact}} = G^{-1}$。矩阵乘积变为 $MG = G^{-1}G = I$。单位矩阵的特征值都等于 $1$。因此，$\\lambda_{\\max}(MG) = 1$。最大稳定步长为：\n$$\n\\eta_{\\max}^{\\text{exact}} = \\frac{2}{1} = 2\n$$\n这意味着对于精确逆预处理器，区间 $(0, 2)$ 内的任何步长都会导致稳定的迭代。步长 $\\eta = 1$ 会导致单步收敛，因为 $e_1 = (I - 1 \\cdot I)e_0 = 0$。\n\n对于截断诺伊曼级数预处理器，$M = M_{K,\\alpha} = \\frac{1}{\\alpha}\\sum_{k=0}^{K} \\left(I - \\frac{1}{\\alpha} G\\right)^k$。由于 $M_{K,\\alpha}$ 是 $G$ 的一个多项式，矩阵 $M_{K,\\alpha}$ 和 $G$ 可交换，并共享一个共同的标准正交特征基。设 $v_i$ 是 $G$ 的一个特征向量，其对应的特征值为 $\\lambda_i  0$。那么 $v_i$ 也是 $M_{K,\\alpha}$ 的一个特征向量。$M_{K,\\alpha}$ 对应的特征值可以通过对几何级数求和得到：\n$$\nm_i = \\frac{1}{\\alpha}\\sum_{k=0}^{K} \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^k = \\frac{1}{\\alpha} \\frac{1 - (1-\\lambda_i/\\alpha)^{K+1}}{1 - (1-\\lambda_i/\\alpha)} = \\frac{1}{\\lambda_i}\\left[1 - \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\\right]\n$$\n对应于特征向量 $v_i$ 的乘积 $M_{K,\\alpha}G$ 的特征值为 $\\mu_i = m_i \\lambda_i$：\n$$\n\\mu_i = 1 - \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\n$$\n为了找到 $\\lambda_{\\max}(M_{K,\\alpha}G) = \\max_i \\mu_i$，我们分析函数 $f(x) = 1 - (1 - x/\\alpha)^{K+1}$，其中 $x \\in [\\lambda_{\\min}(G), \\lambda_{\\max}(G)]$。给定 $\\alpha  \\lambda_{\\max}(G)$，项 $1 - x/\\alpha$ 总是在 $(0, 1)$ 区间内。函数 $g(x) = 1 - x/\\alpha$ 是关于 $x$ 的递减函数。由于 $K+1  0$，函数 $h(y) = y^{K+1}$ 在 $y  0$ 时是递增的。因此，$(1-x/\\alpha)^{K+1}$ 是关于 $x$ 的递减函数。所以，$f(x) = 1 - (1-x/\\alpha)^{K+1}$ 是关于 $x$ 的递增函数。因此，$\\mu_i$ 的最大值在 $\\lambda_i = \\lambda_{\\max}(G)$ 时取得。\n$$\n\\lambda_{\\max}(M_{K,\\alpha}G) = 1 - \\left(1 - \\frac{\\lambda_{\\max}(G)}{\\alpha}\\right)^{K+1}\n$$\n那么最大稳定步长为：\n$$\n\\eta_{\\max}^{\\text{neu}} = \\frac{2}{\\lambda_{\\max}(M_{K,\\alpha}G)} = \\frac{2}{1 - \\left(1 - \\frac{\\lambda_{\\max}(G)}{\\alpha}\\right)^{K+1}}\n$$\n最后，我们推导谱算子范数 $\\lVert I - M_{K,\\alpha} G \\rVert_2$ 的公式。矩阵 $C = I - M_{K,\\alpha}G$ 是对称的，因为它是对称矩阵 $G$ 的一个多项式。对于对称矩阵，谱范数（2-范数）等于其谱半径。$C$ 的特征值是 $c_i = 1 - \\mu_i$，其中 $\\mu_i$ 是 $M_{K,\\alpha}G$ 的特征值。\n$$\nc_i = 1 - \\left[1 - \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\\right] = \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\n$$\n条件 $\\alpha  \\lambda_{\\max}(G)$ 确保 $0  1 - \\lambda_i/\\alpha  1$，因此所有特征值 $c_i$ 都是正的。谱范数是这些特征值的最大值：\n$$\n\\lVert I - M_{K,\\alpha} G \\rVert_2 = \\max_i |c_i| = \\max_i \\left(1 - \\frac{\\lambda_i}{\\alpha}\\right)^{K+1}\n$$\n由于函数 $h(x) = (1 - x/\\alpha)^{K+1}$ 在特征值区间上是关于 $x$ 的递减函数，其最大值在最小特征值 $\\lambda_{\\min}(G)$ 处取得。\n$$\n\\lVert I - M_{K,\\alpha} G \\rVert_2 = \\left(1 - \\frac{\\lambda_{\\min}(G)}{\\alpha}\\right)^{K+1}\n$$\n这些推导出的公式用于计算每个测试用例所需的量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating stability limits and norms for preconditioned gradient descent.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        ({'lambdas': [1.0, 2.5, 4.0, 7.0, 10.0], 'alpha_factor': 1.5, 'K': 0}),\n        # Case 2\n        ({'lambdas': [1.0, 2.5, 4.0, 7.0, 10.0], 'alpha_factor': 1.5, 'K': 5}),\n        # Case 3\n        ({'lambdas': [1.0, 10.0, 50.0, 200.0, 1000.0], 'alpha_factor': 1.1, 'K': 3}),\n        # Case 4\n        ({'lambdas': [1.0, 1.125, 1.25, 1.375, 1.5], 'alpha_factor': 1.01, 'K': 1}),\n        # Case 5\n        ({'lambdas': [1.0, 5.0, 25.0, 125.0, 625.0], 'alpha_factor': 10.0, 'K': 0}),\n    ]\n\n    results = []\n    for case in test_cases:\n        lambdas = case['lambdas']\n        alpha_factor = case['alpha_factor']\n        K = case['K']\n\n        # Extract min and max eigenvalues\n        lambda_min = min(lambdas)\n        lambda_max = max(lambdas)\n\n        # Calculate alpha\n        alpha = alpha_factor * lambda_max\n\n        # 1. Largest stable step size for the exact preconditioner\n        eta_max_exact = 2.0\n\n        # 2. Largest stable step size for the truncated Neumann preconditioner\n        # Formula: 2 / (1 - (1 - lambda_max / alpha)^(K + 1))\n        # Note: 1 - lambda_max / alpha = 1 - lambda_max / (alpha_factor * lambda_max) = 1 - 1/alpha_factor\n        term_neu = 1.0 - (1.0 - 1.0 / alpha_factor)**(K + 1)\n        eta_max_neu = 2.0 / term_neu\n\n        # 3. Spectral norm of the deviation from identity\n        # Formula: (1 - lambda_min / alpha)^(K + 1)\n        term_norm = 1.0 - lambda_min / alpha\n        norm_val = term_norm**(K + 1)\n\n        results.extend([eta_max_exact, eta_max_neu, norm_val])\n\n    # Format the final output string with rounding to 10 decimal places.\n    output_str = f\"[{','.join([f'{r:.10f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3147772"}, {"introduction": "在深度学习的许多领域，特别是注意力机制中，我们常会遇到具有“单位矩阵加低秩矩阵”（$I + UV^\\top$）特殊结构的矩阵。直接对这种大矩阵求逆的计算成本很高。这个练习将引导你实现并评估一种高效的求逆算法——Sherman-Morrison公式，该算法利用这种结构进行迭代更新，并让你通过比较计算成本和精度，体会其在特定场景下的巨大优势。[@problem_id:3147731]", "problem": "给定形式为 $A = I + U V^\\top$ 的方阵，其中 $I$ 是大小为 $n \\times n$ 的单位矩阵，而 $U, V \\in \\mathbb{R}^{n \\times k}$ 定义了一个低秩更新。在深度学习的计算线性代数中，当近似某些核化或预处理过的变换时，会出现这种低秩加单位矩阵的结构。当这种结构在注意力机制中是块局部 (block-local) 或头局部 (head-local) 时，它们尤其重要。你的任务是实现并评估一个算法，该算法通过从 $A_0 = I$ 开始并逐个并入列向量对 $(u_i, v_i)$ 的重复秩-1更新来计算 $A$ 的逆矩阵，其中 $u_i$ 和 $v_i$ 分别是 $U$ 和 $V$ 的第 $i$ 列。你不能预先假设任何公式；相反，你必须从第一性原理出发，从矩阵逆的定义开始，推导出所需的更新规则并加以实现。\n\n你可以使用的基本知识包括：矩阵逆和单位矩阵的定义与性质、外积和矩阵-向量乘法的代数运算、范数以及标准的分块矩阵操作规则。特别是，你可以依赖以下事实：如果 $A$ 可逆，则 $A A^{-1} = I$ 和 $A^{-1} A = I$，并且当逆矩阵存在时，它是唯一的。\n\n构造矩阵的说明：\n- 对于每个测试，通过从标准正态分布中抽取元素来构造 $U$，然后按因子 $\\gamma / \\sqrt{n}$ 进行缩放，其中每个测试用例会提供 $\\gamma$。为了保证更新是良态且非奇异的，设置 $V = U$，这样对于任何实数 $\\gamma$，$A = I + U U^\\top$ 都是对称正定的。\n- 这些矩阵纯粹是数学上的；不涉及物理单位。\n\n算法任务：\n1. 从 $A_0 = I$ 及其逆矩阵 $A_0^{-1} = I$ 开始，依次并入每个秩-1项 $u_i v_i^\\top$ 以得到 $A_i = A_{i-1} + u_i v_i^\\top$，并只使用矩阵-向量乘积、向量外积、标量运算和矩阵加法来更新显式逆矩阵 $A_i^{-1}$。从逆矩阵的定义出发，推导出你需要的更新规则。\n2. 使用基于稳健数值线性代数算法的直接法，计算一个参考逆矩阵 $A_{\\mathrm{ref}}^{-1}$。\n3. 对于每个测试用例，计算你的迭代逆矩阵与参考逆矩阵之间的相对Frobenius误差：\n   $$ \\varepsilon = \\frac{\\lVert A_{\\mathrm{iter}}^{-1} - A_{\\mathrm{ref}}^{-1} \\rVert_F}{\\lVert A_{\\mathrm{ref}}^{-1} \\rVert_F}. $$\n4. 在以下标准化模型下，以标量乘加运算的次数为单位，分析计算成本：\n   - 假设通过使用三角分解求解具有 $n$ 个右侧向量的方程组 $A X = I$ 来直接计算 $A^{-1}$ 的成本为\n     $$ C_{\\text{direct}}(n) = \\frac{5}{3} \\, n^3. $$\n   - 假设你的迭代逆矩阵更新中每次秩-1更新（从 $A_{i-1}^{-1}$ 到 $A_i^{-1}$）的成本为\n     $$ C_{\\text{rank-1}}(n) = 4 n^2, $$\n     计算了主导项的两次矩阵-向量乘积、一次外积和一次矩阵更新，并忽略了低阶项。对于 $k$ 次更新，取\n     $$ C_{\\text{iter}}(n,k) = 4 k n^2. $$\n   - 报告成本比\n     $$ \\rho = \\frac{C_{\\text{iter}}(n,k)}{C_{\\text{direct}}(n)}. $$\n5. 你的程序必须实现上述内容，并严格按照规定使用 $V = U$。\n\n测试套件：\n- 测试 1 (通用小型): $n = 8$, $k = 3$, $\\gamma = 0.2$, 随机种子 $= 0$。\n- 测试 2 (中等规模): $n = 64$, $k = 4$, $\\gamma = 0.05$, 随机种子 $= 1$。\n- 测试 3 (边界情况单位矩阵): $n = 5$, $k = 0$, $\\gamma = 0.0$, 随机种子 $= 2$。\n- 测试 4 (更大且更具挑战性): $n = 50$, $k = 10$, $\\gamma = 0.5$, 随机种子 $= 3$。\n\n要求的最终输出：\n- 对于每个测试用例，按给定顺序输出两个值：首先是相对Frobenius误差 $\\varepsilon$（一个浮点数），然后是成本比 $\\rho$（一个浮点数）。\n- 将所有结果聚合到一个单行字符串中，格式化为Python风格的列表，使用逗号分隔值且不含空格，例如 $[\\varepsilon_1,\\rho_1,\\varepsilon_2,\\rho_2,\\dots]$。\n\n你的程序必须仅输出一行此格式的内容作为其唯一输出。不涉及物理单位或角度，因此不需要特殊的单位处理。", "solution": "对形如 $A = I + U V^\\top$ 的矩阵求逆是数值线性代数中的一个经典问题。Sherman-Morrison-Woodbury公式为逆矩阵提供了一个直接的表达式。然而，当前任务要求从第一性原理推导一系列秩-1修正的更新规则，我们现在就来进行这个推导。\n\n设 $A_{i-1} \\in \\mathbb{R}^{n \\times n}$ 是一个可逆矩阵，其逆矩阵 $A_{i-1}^{-1}$ 已知。我们希望找到矩阵 $A_i$ 的逆，它由一个秩-1更新构成：\n$$ A_i = A_{i-1} + u_i v_i^\\top $$\n其中 $u_i, v_i \\in \\mathbb{R}^n$ 是列向量。我们用 $A_i^{-1}$ 表示我们所求的逆矩阵。\n\n根据矩阵逆的定义，必须有 $A_i A_i^{-1} = I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。代入 $A_i$ 的表达式：\n$$ (A_{i-1} + u_i v_i^\\top) A_i^{-1} = I $$\n假设 $A_{i-1}$ 可逆，我们可以从左侧乘以 $A_{i-1}^{-1}$：\n$$ A_{i-1}^{-1} (A_{i-1} + u_i v_i^\\top) A_i^{-1} = A_{i-1}^{-1} I $$\n$$ (I + A_{i-1}^{-1} u_i v_i^\\top) A_i^{-1} = A_{i-1}^{-1} $$\n现在，我们可以通过对括号中的项求逆来分离出 $A_i^{-1}$：\n$$ A_i^{-1} = (I + A_{i-1}^{-1} u_i v_i^\\top)^{-1} A_{i-1}^{-1} $$\n问题现在简化为求矩阵 $M = I + w v_i^\\top$ 的逆，其中我们定义了列向量 $w = A_{i-1}^{-1} u_i$。\n\n我们假设 $M$ 的逆具有类似结构，即 $M^{-1} = I + \\alpha w v_i^\\top$，其中标量 $\\alpha$ 有待确定。为了求出 $\\alpha$，我们强制满足条件 $M M^{-1} = I$：\n$$ (I + w v_i^\\top)(I + \\alpha w v_i^\\top) = I $$\n使用矩阵乘法的分配律展开左侧：\n$$ I(I + \\alpha w v_i^\\top) + w v_i^\\top(I + \\alpha w v_i^\\top) = I $$\n$$ I + \\alpha w v_i^\\top + w v_i^\\top + w v_i^\\top (\\alpha w v_i^\\top) = I $$\n项 $v_i^\\top w$ 是一个标量，即 $v_i$ 和 $w$ 的内积。我们可以重新组合这些项：\n$$ I + (\\alpha w v_i^\\top + w v_i^\\top) + \\alpha (w v_i^\\top w v_i^\\top) = I $$\n$$ I + (\\alpha+1) w v_i^\\top + \\alpha w (v_i^\\top w) v_i^\\top = I $$\n由于 $v_i^\\top w$ 是一个标量，我们可以交换它的位置：\n$$ I + (\\alpha+1) w v_i^\\top + \\alpha (v_i^\\top w) w v_i^\\top = I $$\n提出外积 $w v_i^\\top$：\n$$ I + [(\\alpha+1) + \\alpha(v_i^\\top w)] w v_i^\\top = I $$\n为了使此方程对一般的非零向量 $w$ 和 $v_i$ 成立，$w v_i^\\top$ 项的标量系数必须为零：\n$$ (\\alpha+1) + \\alpha(v_i^\\top w) = 0 $$\n$$ \\alpha(1 + v_i^\\top w) = -1 $$\n解出 $\\alpha$，我们得到：\n$$ \\alpha = -\\frac{1}{1 + v_i^\\top w} $$\n该表达式在分母 $1 + v_i^\\top w \\neq 0$ 的条件下有效。如果此条件成立，逆矩阵为：\n$$ (I + w v_i^\\top)^{-1} = I - \\frac{w v_i^\\top}{1 + v_i^\\top w} $$\n将此结果代回到我们关于 $A_i^{-1}$ 的表达式中：\n$$ A_i^{-1} = \\left(I - \\frac{w v_i^\\top}{1 + v_i^\\top w}\\right) A_{i-1}^{-1} $$\n将 $w = A_{i-1}^{-1} u_i$ 代回：\n$$ A_i^{-1} = \\left(I - \\frac{(A_{i-1}^{-1} u_i) v_i^\\top}{1 + v_i^\\top (A_{i-1}^{-1} u_i)}\\right) A_{i-1}^{-1} $$\n最后，将右侧的 $A_{i-1}^{-1}$ 项分配进去：\n$$ A_i^{-1} = A_{i-1}^{-1} - \\frac{(A_{i-1}^{-1} u_i) (v_i^\\top A_{i-1}^{-1})}{1 + v_i^\\top A_{i-1}^{-1} u_i} $$\n这就是Sherman-Morrison公式，它为求逆提供了一个序贯更新规则。\n\n对于本特定问题，给定 $V = U$，这意味着对所有 $i = 1, \\dots, k$ 都有 $v_i = u_i$。所构造的矩阵是 $A_0 = I$ 以及当 $i  0$ 时 $A_i = A_{i-1} + u_i u_i^\\top$。由于 $A_0=I$ 是对称正定(SPD)的，并且每次更新 $u_i u_i^\\top$ 是对称半正定的，因此所有矩阵 $A_i$ 都是对称正定的。因此，它们的逆矩阵 $A_i^{-1}$ 也是对称正定的。这确保了对于任何非零向量 $u_i$，二次型 $u_i^\\top A_{i-1}^{-1} u_i  0$。因此，分母 $1 + u_i^\\top A_{i-1}^{-1} u_i$ 总是严格大于1，从而排除了任何除以零的可能性。\n\n因此，算法实现将按以下步骤进行：\n1. 将逆矩阵初始化为 $A_0^{-1} = I$。\n2. 对于每一步 $i = 1, \\dots, k$，使用推导出的公式（其中 $v_i=u_i$）将当前逆矩阵 $A_{i-1}^{-1}$ 更新为 $A_i^{-1}$。这就构成了迭代逆矩阵 $A_{\\mathrm{iter}}^{-1}$。\n3. 直接构造最终矩阵 $A = I + U U^\\top$ 并使用标准的数值库函数计算其逆矩阵 $A_{\\mathrm{ref}}^{-1}$，该函数通常依赖于像LU分解这样的稳健分解方法。\n4. 计算相对Frobenius误差 $\\varepsilon = \\frac{\\lVert A_{\\mathrm{iter}}^{-1} - A_{\\mathrm{ref}}^{-1} \\rVert_F}{\\lVert A_{\\mathrm{ref}}^{-1} \\rVert_F}$。\n5. 使用提供的成本模型计算成本比 $\\rho = C_{\\text{iter}}(n,k) / C_{\\text{direct}}(n)$：\n   $C_{\\text{iter}}(n,k) = 4 k n^2$\n   $C_{\\text{direct}}(n) = \\frac{5}{3} n^3$\n   该比率可简化为 $\\rho = \\frac{4 k n^2}{(5/3) n^3} = \\frac{12k}{5n}$。该比率表明，当更新次数 $k$ 相对于矩阵维度 $n$ 较小时，迭代方法更高效，特别是在 $k  \\frac{5}{12}n$ 时。\n\n实现将针对每个提供的测试用例遵循这些步骤。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of iteratively inverting a matrix of the form A = I + UU^T\n    and compares it against a direct inversion method.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, gamma, random_seed)\n        (8, 3, 0.2, 0),\n        (64, 4, 0.05, 1),\n        (5, 0, 0.0, 2),\n        (50, 10, 0.5, 3),\n    ]\n\n    results = []\n    for n, k, gamma, seed in test_cases:\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # Construct matrix U. Handle the k=0 edge case.\n        if k > 0:\n            scale_factor = gamma / np.sqrt(n)\n            U = np.random.randn(n, k) * scale_factor\n        else:\n            # An n x 0 matrix\n            U = np.zeros((n, 0))\n        \n        # As per instructions, V = U\n        V = U\n\n        # 1. Iterative inverse calculation using the derived rank-1 update rule.\n        # Start with the inverse of A_0 = I, which is I.\n        A_inv_iter = np.eye(n)\n        \n        # Sequentially apply k rank-1 updates.\n        for i in range(k):\n            u_i = U[:, i:i+1] # Get i-th column as a (n, 1) vector\n            v_i = V[:, i:i+1] # v_i is u_i\n            \n            # Sherman-Morrison formula:\n            # A_inv_new = A_inv - (A_inv @ u @ v.T @ A_inv) / (1 + v.T @ A_inv @ u)\n            \n            # Pre-calculate terms for clarity and efficiency\n            w = A_inv_iter @ u_i # This is the vector (A_{i-1}^{-1} u_i)\n            \n            # Denominator: 1 + v_i^T * (A_{i-1}^{-1} u_i)\n            denominator = 1.0 + (v_i.T @ w).item()\n            \n            # Since A_inv_iter is symmetric and v_i=u_i, the numerator term\n            # (A_inv @ u) @ (v.T @ A_inv) simplifies to w @ w.T\n            numerator_outer_product = w @ w.T\n            \n            # Update the inverse\n            A_inv_iter -= numerator_outer_product / denominator\n\n        # 2. Compute the reference inverse using a direct method.\n        # First, construct the full matrix A = I + U U^T\n        A = np.eye(n) + U @ U.T\n        \n        # Use a robust direct solver\n        A_inv_ref = np.linalg.inv(A)\n\n        # 3. Compute the relative Frobenius error.\n        norm_ref = np.linalg.norm(A_inv_ref, 'fro')\n        \n        if norm_ref == 0:\n            # This is unlikely for an invertible matrix, but for completeness:\n            # if reference is zero matrix, error is 0 if iterative is also zero, else infinity\n            epsilon = 0.0 if np.linalg.norm(A_inv_iter, 'fro') == 0.0 else np.inf\n        else:\n            norm_diff = np.linalg.norm(A_inv_iter - A_inv_ref, 'fro')\n            epsilon = norm_diff / norm_ref\n\n        # 4. Compute the computational cost ratio.\n        c_direct = (5.0 / 3.0) * (n**3)\n        c_iter = 4.0 * k * (n**2)\n        \n        # The ratio rho simplifies to 12*k / (5*n)\n        if c_direct == 0:\n            # Handle n=0 case to avoid division by zero\n            rho = np.inf if c_iter > 0 else 0.0\n        else:\n            rho = c_iter / c_direct\n            \n        results.extend([epsilon, rho])\n\n    # Final print statement in the exact required format.\n    # The format is a string representing a list, with no spaces.\n    formatted_results = [f\"{val:.12g}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3147731"}]}