{"hands_on_practices": [{"introduction": "要想真正掌握主成分分析（PCA），仅仅调用现成的软件包是远远不够的。本练习旨在通过手动计算，帮助你深入理解PCA的核心原理。你将亲手构建一个数据集，并利用约束优化方法，从第一性原理出发，推导出最大化方差的方向，从而将PCA的几何直觉（寻找数据变异最大的方向）与代数解法（求解协方差矩阵的特征向量）紧密联系起来。[@problem_id:3177001]", "problem": "考虑一个待使用主成分分析（Principal Component Analysis, PCA）进行分析的二维零均值数据集。设期望的样本协方差矩阵为\n$$\nC=\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}.\n$$\n您必须基于以下基本定义进行操作：对于中心化样本 $\\boldsymbol{x}_{i}\\in\\mathbb{R}^{2}$，样本协方差定义为\n$$\nC=\\frac{1}{n-1}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top},\n$$\n且对于任意单位方向 $\\boldsymbol{u}\\in\\mathbb{R}^{2}$（满足 $|\\boldsymbol{u}|=1$），投影数据沿 $\\boldsymbol{u}$ 方向的方差为\n$$\n\\operatorname{Var}(\\boldsymbol{u}^{\\top}\\boldsymbol{x})=\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}.\n$$\n任务：\n1. 构建一个具体的、至少包含 $n=4$ 个样本的零均值数据集，其样本协方差恰好等于 $C$，并仅使用给定的定义验证其样本协方差为 $C$。\n2. 使用第一性原理和约束优化，确定在约束 $|\\boldsymbol{u}|=1$ 下使得投影方差 $\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}$ 最大化和最小化的 $\\mathbb{R}^{2}$ 中的单位方向，并计算相应的极值方差。\n3. 根据您的结果，计算第一主成分（任务2中的最大化方向）所捕获的总方差比例，该比例定义为最大投影方差与数据集总方差之比。\n\n仅报告此比例作为您的最终答案。您可以将答案表示为最简分数。无需四舍五入，最终答案不包含任何单位。", "solution": "该问题是适定的、有科学依据的，并包含推导出最终所求量的唯一解所需的所有信息。我们将按顺序完成这三项任务。\n\n## 任务1：数据集构建与验证\n\n第一个任务是构建一个至少包含 $n=4$ 个样本的零均值数据集，其样本协方差矩阵恰好为 $C=\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}$。我们已知零均值数据的样本协方差定义为 $C=\\frac{1}{n-1}\\sum_{i=1}^{n}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top}$。\n\n我们选择指定的最小样本量 $n=4$。条件变为 $C=\\frac{1}{3}\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top}$，这意味着我们需要找到四个向量 $\\boldsymbol{x}_i \\in \\mathbb{R}^2$ 使得：\n1. 数据是零均值：$\\sum_{i=1}^{4}\\boldsymbol{x}_{i} = \\boldsymbol{0}$。\n2. 外积之和为：$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = 3C = 3\\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix} = \\begin{pmatrix} 9  6 \\\\ 6  9 \\end{pmatrix}$。\n\n构建这样一个数据集的一个系统性方法是利用协方差矩阵 $C$ 的谱性质。$C$ 的特征向量代表了数据方差的主轴。我们来求 $C$ 的特征值和特征向量。特征方程为 $\\det(C-\\lambda I)=0$。\n$$\n\\det\\begin{pmatrix} 3-\\lambda  2 \\\\ 2  3-\\lambda \\end{pmatrix} = (3-\\lambda)^2 - 4 = 0\n$$\n这得到 $3-\\lambda = \\pm 2$，所以特征值为 $\\lambda_1 = 3+2=5$ 和 $\\lambda_2 = 3-2=1$。\n\n对于第一个特征值 $\\lambda_1 = 5$，通过求解 $(C-5I)\\boldsymbol{u}_1=\\boldsymbol{0}$ 来找到特征向量 $\\boldsymbol{u}_1$：\n$$\n\\begin{pmatrix} -2  2 \\\\ 2  -2 \\end{pmatrix} \\begin{pmatrix} u_{11} \\\\ u_{12} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies -2u_{11} + 2u_{12} = 0 \\implies u_{11} = u_{12}\n$$\n对应的单位特征向量是 $\\boldsymbol{u}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n对于第二个特征值 $\\lambda_2 = 1$，通过求解 $(C-1I)\\boldsymbol{u}_2=\\boldsymbol{0}$ 来找到特征向量 $\\boldsymbol{u}_2$：\n$$\n\\begin{pmatrix} 2  2 \\\\ 2  2 \\end{pmatrix} \\begin{pmatrix} u_{21} \\\\ u_{22} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies 2u_{21} + 2u_{22} = 0 \\implies u_{21} = -u_{22}\n$$\n对应的单位特征向量是 $\\boldsymbol{u}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n$C$ 的谱分解为 $C = \\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + \\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n我们需要 $\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = 3C = 3\\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 3\\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n让我们构建一个与这些特征向量对齐的、对称的、零均值的数据点集：\n$\\boldsymbol{x}_1 = a\\boldsymbol{u}_1$, $\\boldsymbol{x}_2 = -a\\boldsymbol{u}_1$, $\\boldsymbol{x}_3 = b\\boldsymbol{u}_2$, $\\boldsymbol{x}_4 = -b\\boldsymbol{u}_2$。\n均值为 $\\boldsymbol{x}_1+\\boldsymbol{x}_2+\\boldsymbol{x}_3+\\boldsymbol{x}_4 = \\boldsymbol{0}$。外积之和为：\n$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = (a\\boldsymbol{u}_1)(a\\boldsymbol{u}_1)^\\top + (-a\\boldsymbol{u}_1)(-a\\boldsymbol{u}_1)^\\top + (b\\boldsymbol{u}_2)(b\\boldsymbol{u}_2)^\\top + (-b\\boldsymbol{u}_2)(-b\\boldsymbol{u}_2)^\\top = 2a^2 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 2b^2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$。\n将其与 $3\\lambda_1 \\boldsymbol{u}_1 \\boldsymbol{u}_1^\\top + 3\\lambda_2 \\boldsymbol{u}_2 \\boldsymbol{u}_2^\\top$ 比较可得：\n$2a^2 = 3\\lambda_1 = 3(5)=15 \\implies a^2 = \\frac{15}{2} \\implies a = \\sqrt{\\frac{15}{2}}$。\n$2b^2 = 3\\lambda_2 = 3(1)=3 \\implies b^2 = \\frac{3}{2} \\implies b = \\sqrt{\\frac{3}{2}}$。\n\n因此，我们的具体数据集为：\n$\\boldsymbol{x}_1 = \\sqrt{\\frac{15}{2}} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{\\sqrt{15}}{2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{15}}{2} \\\\ \\frac{\\sqrt{15}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_2 = -\\boldsymbol{x}_1 = \\begin{pmatrix} -\\frac{\\sqrt{15}}{2} \\\\ -\\frac{\\sqrt{15}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_3 = \\sqrt{\\frac{3}{2}} \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{\\sqrt{3}}{2}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\end{pmatrix}$\n$\\boldsymbol{x}_4 = -\\boldsymbol{x}_3 = \\begin{pmatrix} -\\frac{\\sqrt{3}}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}$\n\n验证：根据构建方法，该数据集是零均值的。我们来计算 $\\sum \\boldsymbol{x}_i \\boldsymbol{x}_i^\\top$。设 $\\boldsymbol{x}_i = (x_{i1}, x_{i2})^\\top$。\n$\\sum_{i=1}^{4} x_{i1}^2 = (\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{15}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 = \\frac{15}{4} + \\frac{15}{4} + \\frac{3}{4} + \\frac{3}{4} = \\frac{30+6}{4} = \\frac{36}{4}=9$。\n$\\sum_{i=1}^{4} x_{i2}^2 = (\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{15}}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 = \\frac{15}{4} + \\frac{15}{4} + \\frac{3}{4} + \\frac{3}{4} = 9$。\n$\\sum_{i=1}^{4} x_{i1}x_{i2} = (\\frac{\\sqrt{15}}{2})(\\frac{\\sqrt{15}}{2}) + (-\\frac{\\sqrt{15}}{2})(-\\frac{\\sqrt{15}}{2}) + (\\frac{\\sqrt{3}}{2})(-\\frac{\\sqrt{3}}{2}) + (-\\frac{\\sqrt{3}}{2})(\\frac{\\sqrt{3}}{2}) = \\frac{15}{4} + \\frac{15}{4} - \\frac{3}{4} - \\frac{3}{4} = \\frac{30-6}{4} = \\frac{24}{4}=6$。\n所以，$\\sum_{i=1}^{4}\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{\\top} = \\begin{pmatrix} 9  6 \\\\ 6  9 \\end{pmatrix} = 3C$。样本协方差为 $\\frac{1}{3}\\sum \\boldsymbol{x}_i \\boldsymbol{x}_i^\\top = C$，符合要求。\n\n## 任务2：通过约束优化寻找极值方差方向\n\n我们希望找到投影方差 $f(\\boldsymbol{u}) = \\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u}$ 在约束条件 $g(\\boldsymbol{u}) = |\\boldsymbol{u}|^2 - 1 = \\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1 = 0$ 下的极值。我们使用拉格朗日乘数法。拉格朗日函数为：\n$$\n\\mathcal{L}(\\boldsymbol{u}, \\lambda) = \\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u} - \\lambda(\\boldsymbol{u}^{\\top}\\boldsymbol{u} - 1)\n$$\n为了找到驻点，我们将关于 $\\boldsymbol{u}$ 的梯度设为零：\n$$\n\\nabla_{\\boldsymbol{u}} \\mathcal{L} = 2C\\boldsymbol{u} - 2\\lambda\\boldsymbol{u} = \\boldsymbol{0}\n$$\n这可以简化为特征值方程：\n$$\nC\\boldsymbol{u} = \\lambda\\boldsymbol{u}\n$$\n这表明，使投影方差取极值的单位向量 $\\boldsymbol{u}$ 是协方差矩阵 $C$ 的特征向量。拉格朗日乘数 $\\lambda$ 是对应的特征值。\n在特征向量 $\\boldsymbol{u}$ 处，投影方差的值为 $\\boldsymbol{u}^{\\top}C\\,\\boldsymbol{u} = \\boldsymbol{u}^{\\top}(\\lambda\\boldsymbol{u}) = \\lambda(\\boldsymbol{u}^{\\top}\\boldsymbol{u})$。根据约束条件 $\\boldsymbol{u}^{\\top}\\boldsymbol{u}=1$，投影方差就是特征值 $\\lambda$。\n\n从任务1中，我们求得 $C$ 的特征值为 $\\lambda_1 = 5$ 和 $\\lambda_2 = 1$。\n最大投影方差是最大的特征值，$\\lambda_{\\max} = 5$。达到此最大值的方向是对应的特征向量，即第一主成分：$\\boldsymbol{u}_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$（及其负方向）。\n最小投影方差是最小的特征值，$\\lambda_{\\min} = 1$。达到此最小值的方向是第二主成分：$\\boldsymbol{u}_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$（及其负方向）。\n\n## 任务3：总方差占比\n\n最后一项任务是计算第一主成分所捕获的总方差比例。第一主成分是使投影方差最大化的方向，我们已求得该方向为 $\\boldsymbol{u}_1$。此成分捕获的方差即最大投影方差，为 $\\lambda_1 = 5$。\n\n数据集的总方差是沿各个维度的方差之和，即协方差矩阵的迹 $\\operatorname{Tr}(C)$。\n$$\n\\text{Total Variance} = \\operatorname{Tr}(C) = C_{11} + C_{22} = 3+3 = 6\n$$\n注意，这也等于特征值之和：$\\lambda_1 + \\lambda_2 = 5+1=6$，与预期相符。\n\n第一主成分捕获的总方差比例是该成分上的方差与总方差之比：\n$$\n\\text{Proportion} = \\frac{\\text{Variance of PC1}}{\\text{Total Variance}} = \\frac{\\lambda_1}{\\operatorname{Tr}(C)} = \\frac{5}{6}\n$$\n此分数为最简形式。", "answer": "$$\\boxed{\\frac{5}{6}}$$", "id": "3177001"}, {"introduction": "从理论基础转向实际应用时，我们必须面对一个关键问题：数据的尺度。在真实世界的数据集中，不同变量往往具有不同的单位和量纲，例如股票价格（美元）和交易量（百万股）。本练习将通过一个计算实验，清晰地揭示PCA并非尺度不变的，并展示为何数据标准化是至关重要的预处理步骤，以确保所有变量都能在分析中得到公平的考量。[@problem_id:2421735]", "problem": "要求您使用主成分分析（PCA）的基本原理，演示当变量以不同单位度量时，若不进行标准化，会如何扭曲估计出的主方向和解释方差。您将在一个纯数学框架下进行操作，使用一个合成数据生成过程，该过程模拟典型的金融变量（如价格和交易量）。您需要实现完整的流程，并报告量化诊断结果，以比较对原始数据执行 PCA 与对标准化数据执行 PCA 的差异。\n\n基本原理：\n- PCA 旨在寻找能够最大化样本方差的标准正交方向。给定一个中心化的数据矩阵 $X \\in \\mathbb{R}^{T \\times n}$，样本协方差矩阵为 $\\Sigma = \\frac{1}{T-1} X^\\top X$。主成分是 $\\Sigma$ 的特征向量，按其对应的特征值从大到小排序。\n- 标准化将每个变量 $x_j$ 转换为 $\\tilde{x}_j = \\frac{x_j - \\bar{x}_j}{\\hat{\\sigma}_j}$，其中 $\\bar{x}_j$ 是样本均值，$\\hat{\\sigma}_j$ 是样本标准差，使得每个标准化变量的样本方差为单位1。对标准化数据进行 PCA 就是对样本相关系数矩阵进行 PCA。\n- 对变量应用对角缩放 $D = \\operatorname{diag}(s_1,\\dots,s_n)$，即 $X \\mapsto X D$，会使协方差矩阵的元素乘以 $s_i s_j$，从而改变特征向量，除非所有的 $s_j$ 都相等。\n\n数据生成过程：\n- 对于每个测试用例 $k$，固定 $T_k \\in \\mathbb{N}$（样本量），$n_k \\in \\mathbb{N}$（变量数），因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$，特异性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$，以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n- 为 $t = 1,\\dots,T_k$ 生成一个共同因子 $f_t \\sim \\mathcal{N}(0,1)$，以及特异性噪声 $e_{t,j} \\sim \\mathcal{N}(0,(u^{(k)}_j)^2)$，所有因子和噪声在 $t$ 和 $j$ 上相互独立。\n- 为 $t=1,\\dots,T_k$ 和 $j=1,\\dots,n_k$ 构建原始观测值 $x_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)$。\n- 在计算任何协方差之前，通过减去其样本均值来中心化 $X$ 的每一列。\n\n每个测试用例的计算任务：\n- 从中心化的原始数据 $X$ 计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$，并获得第一主成分的特征向量 $v_{\\text{raw}}$（单位范数）及其特征值 $\\lambda_{\\text{raw}}$。\n- 将 $X$ 的每一列标准化为单位样本方差以获得 $Z$，计算 $\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z$（即样本相关系数矩阵），并获得第一主成分的特征向量 $v_{\\text{std}}$（单位范数）及其特征值 $\\lambda_{\\text{std}}$。\n- 计算角度 $\\theta = \\arccos\\!\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)$；以弧度报告 $\\theta$。\n- 计算解释方差份额的差异 $\\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|$，必须以小数形式报告（而不是百分比）。\n\n随机性与可复现性：\n- 为确保结果可复现，整个实验请使用固定的伪随机数生成器种子 $314159$。\n\n测试套件：\n- 共有 $3$ 个测试用例。对于每个测试用例 $k$，使用以下参数 $(T_k, n_k, b^{(k)}, u^{(k)}, s^{(k)})$：\n  - 用例 1（单位相似，两个变量）：\n    - $T_1 = 500$, $n_1 = 2$,\n    - $b^{(1)} = [1.0, 0.9]$,\n    - $u^{(1)} = [0.1, 0.1]$,\n    - $s^{(1)} = [1.0, 1.2]$。\n  - 用例 2（单位不匹配，两个变量：一个因尺度而占主导）：\n    - $T_2 = 500$, $n_2 = 2$,\n    - $b^{(2)} = [1.0, 0.9]$,\n    - $u^{(2)} = [0.1, 0.1]$,\n    - $s^{(2)} = [1000.0, 1.0]$。\n  - 用例 3（单位不匹配，三个变量：一个大尺度，一个中等尺度，一个小尺度）：\n    - $T_3 = 800$, $n_3 = 3$,\n    - $b^{(3)} = [0.2, 1.0, 1.0]$,\n    - $u^{(3)} = [0.3, 0.2, 0.2]$,\n    - $s^{(3)} = [1000.0, 1.0, 0.01]$。\n\n每个测试用例的必需输出：\n- 一个包含两个浮点数的列表 $[\\theta, \\Delta]$，其中 $\\theta$ 是以弧度表示的角度，$\\Delta$ 是解释方差份额的绝对差。两个值都必须精确到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个逗号分隔的列表，该列表由每个用例的结果列表组成，并用方括号括起来，例如 $[[\\theta_1,\\Delta_1],[\\theta_2,\\Delta_2],[\\theta_3,\\Delta_3]]$，每个浮点数精确到 $6$ 位小数，角度以弧度为单位。", "solution": "所提出的问题是计算统计学中一个有效且定义明确的练习，它专门演示了主成分分析（PCA）对变量尺度的敏感性。该问题在科学上是合理的，基于线性代数和统计学的基础原理，并且所有参数和程序都已足够清晰地指定，从而可以得到一个唯一且可验证的解。我们将着手进行分析。\n\n其核心论点是，PCA 作为一种方差最大化技术，不具备尺度不变性。当变量以迥异的单位度量时（例如，以美元计价的股价与以百万股计价的交易量），方差最大的变量将在机制上主导第一个主成分。这通常是所选单位造成的人为结果，而不是真实潜在重要性的指标。标准化是标准的补救措施，它将所有变量转换到同一尺度（单位方差），使得分析关注数据的相关性结构，而非任意的度量尺度。\n\n我们首先将数据生成和分析流程形式化。\n\n**1. 数据生成过程**\n\n对于每个测试用例 $k$，我们给定样本量 $T_k$、变量数 $n_k$、因子载荷 $b^{(k)} \\in \\mathbb{R}^{n_k}$、特异性标准差 $u^{(k)} \\in \\mathbb{R}^{n_k}$ 以及单位尺度 $s^{(k)} \\in \\mathbb{R}^{n_k}$。\n\n数据由单因子模型生成。对于每个时间点 $t=1, \\dots, T_k$，从标准正态分布中抽取一个共同的潜在因子 $f_t \\sim \\mathcal{N}(0, 1)$。对于每个变量 $j=1, \\dots, n_k$，从 $\\mathcal{N}(0, (u^{(k)}_j)^2)$ 中抽取一个特异性噪声项 $e_{t,j}$。所有的 $f_t$ 和 $e_{t,j}$ 都是相互独立的。\n\n变量 $j$ 在时间 $t$ 的观测值构造如下：\n$$\nx_{t,j} = s^{(k)}_j \\left( b^{(k)}_j f_t + e_{t,j} \\right)\n$$\n这构成一个数据矩阵 $X \\in \\mathbb{R}^{T_k \\times n_k}$，其列代表不同的变量。尺度因子 $s^{(k)}_j$ 代表变量 $j$ 的任意度量单位。\n\n**2. 对原始数据进行 PCA（基于协方差的 PCA）**\n\nPCA 的第一步是通过减去列向样本均值来中心化数据。设 $\\bar{x}_j = \\frac{1}{T_k} \\sum_{t=1}^{T_k} x_{t,j}$ 为第 $j$ 个变量的样本均值。中心化后的数据矩阵记为 $X_c$，其元素为 $(X_c)_{t,j} = x_{t,j} - \\bar{x}_j$。\n\n然后计算样本协方差矩阵 $\\Sigma_{\\text{raw}}$：\n$$\n\\Sigma_{\\text{raw}} = \\frac{1}{T_k-1} X_c^\\top X_c\n$$\n主成分是 $\\Sigma_{\\text{raw}}$ 的特征向量。我们对该矩阵进行特征分解：\n$$\n\\Sigma_{\\text{raw}} V = V \\Lambda\n$$\n其中 $V$ 是标准正交特征向量矩阵，$\\Lambda$ 是对应的特征值对角矩阵。特征值按降序排列，$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_{n_k}$。第一主成分是与最大特征值 $\\lambda_1$ 相关联的特征向量 $v_1$。在本问题中，我们将此特征向量记为 $v_{\\text{raw}}$，特征值记为 $\\lambda_{\\text{raw}}$。\n\n**3. 对标准化数据进行 PCA（基于相关系数的 PCA）**\n\n为了消除任意尺度的影响，我们对数据进行标准化。对于原始数据矩阵 $X$ 的每一列 $j$，我们计算其样本标准差，$\\hat{\\sigma}_j = \\sqrt{\\frac{1}{T_k-1} \\sum_{t=1}^{T_k} (x_{t,j} - \\bar{x}_j)^2}$。\n\n标准化数据矩阵 $Z$ 的构造元素为：\n$$\nz_{t,j} = \\frac{x_{t,j} - \\bar{x}_j}{\\hat{\\sigma}_j}\n$$\n根据构造， $Z$ 的每一列的样本均值为 $0$，样本方差为 $1$。\n\n然后对这个标准化数据 $Z$ 进行 PCA。相关的矩阵是 $Z$ 的样本协方差矩阵，我们记为 $\\Sigma_{\\text{std}}$：\n$$\n\\Sigma_{\\text{std}} = \\frac{1}{T_k-1} Z^\\top Z\n$$\n由于 $Z$ 的每一列都有单位方差，$\\Sigma_{\\text{std}}$ 的对角元素全为 $1$，而非对角元素 $(i, j)$ 是原始变量 $x_i$ 和 $x_j$ 之间的样本相关系数。因此，$\\Sigma_{\\text{std}}$ 是 $X$ 的样本相关系数矩阵。\n\n我们对 $\\Sigma_{\\text{std}}$ 进行特征分解，以找到其最大特征值 $\\lambda_{\\text{std}}$ 和对应的特征向量 $v_{\\text{std}}$。\n\n**4. 诊断指标**\n\n为了量化因未标准化而引起的失真，我们计算两个指标：\n\n- **主成分之间的夹角**：主成分方向 $v_{\\text{raw}}$ 和 $v_{\\text{std}}$ 是 $\\mathbb{R}^{n_k}$ 中的单位向量。它们之间的夹角衡量了最大方差方向的偏移程度。由于特征向量仅定义到符号为止（即，如果 $v$ 是一个特征向量，$-v$ 也是），我们计算它们所张成的直线之间的锐角：\n  $$\n  \\theta = \\arccos\\left( \\left| v_{\\text{raw}}^\\top v_{\\text{std}} \\right| \\right)\n  $$\n  $\\theta=0$ 的值表示完全对齐，而一个大的角度（接近 $\\pi/2$）表示严重错位。\n\n- **解释方差份额的差异**：由第一主成分解释的总方差比例由其特征值除以所有特征值之和给出。特征值之和等于矩阵的迹，$\\operatorname{tr}(\\Sigma) = \\sum_{j=1}^{n_k} \\Sigma_{jj}$，代表数据中的总方差。我们计算解释方差份额的绝对差异：\n  $$\n  \\Delta = \\left| \\frac{\\lambda_{\\text{raw}}}{\\operatorname{tr}(\\Sigma_{\\text{raw}})} - \\frac{\\lambda_{\\text{std}}}{\\operatorname{tr}(\\Sigma_{\\text{std}})} \\right|\n  $$\n  请注意，对于标准化数据，$\\operatorname{tr}(\\Sigma_{\\text{std}}) = n_k$，即变量的数量。一个大的 $\\Delta$ 表明两种方法对第一主成分重要性的评估差异很大。\n\n该过程将针对每个测试用例，使用指定的参数和固定的随机种子以保证可复现性来执行。预计结果将显示用例 1（尺度相似）的失真最小，而用例 2 和 3（尺度迥异）的失真显著，从而验证了在实践中进行标准化的必要性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA problem by comparing results from raw and standardized data.\n\n    This function iterates through a set of predefined test cases. For each case, it:\n    1. Generates synthetic financial data based on a single-factor model with specified scales.\n    2. Performs PCA on the raw, centered data.\n    3. Performs PCA on the standardized data (equivalent to using the correlation matrix).\n    4. Computes two diagnostic metrics:\n        - The angle between the first principal components from the raw and standardized analyses.\n        - The absolute difference in the fraction of variance explained by the first component.\n    5. Collects and formats the results according to the problem specification.\n    \"\"\"\n    # Use a fixed pseudorandom number generator seed for reproducibility.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    # Test cases defined as (T_k, n_k, b^(k), u^(k), s^(k)))\n    test_cases = [\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1.0, 1.2])),\n        (500, 2, np.array([1.0, 0.9]), np.array([0.1, 0.1]), np.array([1000.0, 1.0])),\n        (800, 3, np.array([0.2, 1.0, 1.0]), np.array([0.3, 0.2, 0.2]), np.array([1000.0, 1.0, 0.01]))\n    ]\n\n    all_results = []\n\n    for T, n, b, u, s in test_cases:\n        # 1. Data Generation\n        # Generate common factor f_t ~ N(0,1)\n        f = rng.normal(loc=0.0, scale=1.0, size=T)\n        \n        # Generate idiosyncratic noises e_{t,j} ~ N(0, u_j^2)\n        # E is a T x n matrix\n        E = rng.normal(loc=0.0, scale=u, size=(T, n))\n        \n        # Construct raw observations x_{t,j} = s_j * (b_j * f_t + e_{t,j})\n        X = s * (np.outer(f, b) + E)\n\n        # 2. PCA on Raw Data\n        # Center the data matrix X\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute the sample covariance matrix (ddof=1 for unbiased estimator)\n        Sigma_raw = np.cov(X_centered, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # eigh returns eigenvalues in ascending order and corresponding eigenvectors in columns\n        eigvals_raw, eigvecs_raw = np.linalg.eigh(Sigma_raw)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_raw = eigvals_raw[-1]\n        v_raw = eigvecs_raw[:, -1]\n\n        # 3. PCA on Standardized Data\n        # Standardize the data matrix X\n        stds = np.std(X, axis=0, ddof=1)\n        Z = X_centered / stds\n        \n        # Compute the sample correlation matrix (covariance of standardized data)\n        Sigma_std = np.cov(Z, rowvar=False, ddof=1)\n        \n        # Eigendecomposition of the correlation matrix\n        eigvals_std, eigvecs_std = np.linalg.eigh(Sigma_std)\n        \n        # The first principal component corresponds to the largest eigenvalue\n        lambda_std = eigvals_std[-1]\n        v_std = eigvecs_std[:, -1]\n\n        # 4. Diagnostic Computations\n        # Angle theta between the first principal components\n        # Take absolute value of dot product to handle sign ambiguity of eigenvectors\n        cos_theta = np.abs(np.dot(v_raw, v_std))\n        # Clip to prevent domain errors with arccos due to potential floating point inaccuracies\n        theta = np.arccos(np.clip(cos_theta, -1.0, 1.0))\n\n        # Difference in explained variance shares\n        total_var_raw = np.trace(Sigma_raw)\n        total_var_std = np.trace(Sigma_std) # This is always n for a correlation matrix\n        \n        share_raw = lambda_raw / total_var_raw\n        share_std = lambda_std / total_var_std\n        \n        delta = np.abs(share_raw - share_std)\n\n        # Append results, rounded to 6 decimal places\n        all_results.append([round(theta, 6), round(delta, 6)])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{theta:.6f},{delta:.6f}]\" for theta, delta in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2421735"}, {"introduction": "在前一个练习探讨了数据标准化之后，我们来关注另一个常见的数据挑战：异常值。由于主成分分析的目标是最大化方差，因此它对数据中的极端值（即异常值）表现出高度敏感性。本练习将指导你构建一个合成数据集，亲眼见证一个孤立的异常点如何能够“劫持”第一主成分，进而扭曲整个分析的结果，这突显了在实践中识别和处理异常值的必要性。[@problem_id:2421778]", "problem": "您的任务是根据第一性原理，构建合成的横截面资产回报面板，其中主成分分析（PCA）的第一个主成分（PC）由单个极端观测值驱动。考虑一个具有 $T$ 个时间点和 $N$ 个资产的面板。令 $X \\in \\mathbb{R}^{T \\times N}$ 表示数据矩阵，其行是时间点，列是资产。通过从 $X$ 的每一列中减去其样本均值，来定义中心化矩阵 $X_c$。将第一个主成分（PC1）定义为任意单位向量 $v_1 \\in \\mathbb{R}^N$，该向量能最大化投影数据的样本方差，即 $v_1 \\in \\arg\\max_{\\|v\\|=1} \\mathrm{Var}(X_c v)$，其中 $\\mathrm{Var}(X_c v)$ 表示标量时间序列 $X_c v$ 的样本方差。将 PC1 的解释方差比定义为 $\\rho_1 = \\dfrac{\\mathrm{Var}(X_c v_1)}{\\sum_{j=1}^N \\mathrm{Var}(X_c e_j)}$，其中 $\\{e_j\\}$ 是 $\\mathbb{R}^N$ 中的标准基向量。\n\n$X$ 的构建过程如下。对于给定的整数 $N \\ge 2$、$T \\ge 3$、非负振幅 $A \\ge 0$、时间索引 $t^\\star \\in \\{0,1,\\dots,T-1\\}$ 以及等于第一个标准基向量 $e_1$ 的单位方向 $u \\in \\mathbb{R}^N$，令 $Z \\in \\mathbb{R}^{T \\times N}$ 的条目是独立的，且服从均值为零、方差为一的高斯分布。通过设置 $X = Z$，然后仅通过 $X_{t^\\star,\\cdot} \\leftarrow X_{t^\\star,\\cdot} + A u^\\top$ 修改由 $t^\\star$ 索引的单行来定义 $X$。行索引 $t^\\star$ 从零开始计数。\n\n对于每个指定的参数元组，您的程序必须：\n- 使用给定的 $(N,T,A,t^\\star)$ 按上述方式构建 $X$。\n- 计算包含离群值的数据集的 PC1 及其解释方差比 $\\rho_1$。\n- 移除离群观测值（从 $X$ 中删除由 $t^\\star$ 索引的行），按列均值重新中心化剩余数据，重新计算 PC1，并计算缩减后数据集的解释方差比 $\\tilde{\\rho}_1$。\n- 计算包含离群值的数据集的 PC1 载荷向量与方向 $u$ 之间的绝对对齐度，即 $\\alpha = |\\langle v_1, u \\rangle|$。\n- 返回一个布尔值，指示以下所有显性条件是否同时成立：$\\rho_1 > \\tau_{\\mathrm{high}}$、$\\tilde{\\rho}_1 < \\tau_{\\mathrm{low}}$ 和 $\\alpha > \\tau_{\\mathrm{align}}$，其中 $(\\tau_{\\mathrm{high}}, \\tau_{\\mathrm{low}}, \\tau_{\\mathrm{align}})$ 是下面测试套件中提供的阈值。\n\n使用以下参数值测试套件，其中每个案例都是一个元组 $(N,T,A,t^\\star,\\tau_{\\mathrm{high}},\\tau_{\\mathrm{low}},\\tau_{\\mathrm{align}},\\text{seed})$：\n- 案例 A（理想路径，强离群值）：$(5, 200, 50, 100, 0.6, 0.35, 0.95, 20231102)$。\n- 案例 B（边界幅度离群值）：$(5, 200, 12, 150, 0.28, 0.27, 0.90, 20231103)$。\n- 案例 C（边缘情况，无离群值）：$(8, 400, 0, 50, 0.25, 0.20, 0.90, 20231104)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。对于以上三个案例，输出格式必须严格符合“[bA,bB,bC]”的形式，其中 $bA$、$bB$ 和 $bC$ 分别是对应于案例 A、B 和 C 的布尔值。", "solution": "该问题要求分析在合成的横截面数据集中，第一个主成分对单个显著离群值的敏感性。我们必须首先验证问题的完整性。该问题定义明确，其科学基础根植于线性代数和统计学原理，并且所有参数都已指定。它提出了一个稳健统计学中的标准计算任务。因此，该问题是有效的，我们可以继续进行求解。\n\n问题的核心在于应用主成分分析（PCA），这是一种降维技术，用于识别数据集中方差最大的方向。令 $X \\in \\mathbb{R}^{T \\times N}$ 为数据矩阵，其中 $T$ 表示时间点，$N$ 表示资产。PCA 的第一步是通过减去每列（资产）的均值来对数据进行中心化。这将得到中心化矩阵 $X_c$。\n\n第一个主成分（PC1）被定义为载荷向量 $v_1 \\in \\mathbb{R}^N$，它是一个单位向量，能最大化数据在其上投影的方差。在数学上，这表示为：\n$$\nv_1 = \\arg\\max_{\\|v\\|=1} \\mathrm{Var}(X_c v)\n$$\n长度为 $T$ 的投影时间序列 $y = X_c v$ 的样本方差由 $\\mathrm{Var}(y) = \\frac{1}{T-1} \\sum_{i=1}^T (y_i - \\bar{y})^2$ 给出。由于 $X_c$ 是中心化的，任何投影 $X_c v$ 的均值都为零。因此方差的表达式得以简化，从而得到优化问题：\n$$\nv_1 = \\arg\\max_{\\|v\\|=1} \\frac{1}{T-1} (X_c v)^\\top (X_c v) = \\arg\\max_{\\|v\\|=1} v^\\top \\left(\\frac{1}{T-1} X_c^\\top X_c\\right) v = \\arg\\max_{\\|v\\|=1} v^\\top S v\n$$\n其中 $S = \\frac{1}{T-1} X_c^\\top X_c$ 是资产的样本协方差矩阵。根据瑞利商定理，最大化 $v^\\top S v$ 的向量 $v_1$ 是 $S$ 对应于其最大特征值 $\\lambda_1$ 的特征向量。该特征向量构成了第一个主成分的载荷向量。\n\nPC1 的解释方差比，记为 $\\rho_1$，是第一个主成分捕获的总方差的比例。总方差是所有资产方差的总和，等于协方差矩阵的迹 $\\mathrm{Tr}(S)$。PC1 捕获的方差是 $\\lambda_1$。因此，\n$$\n\\rho_1 = \\frac{\\lambda_1}{\\mathrm{Tr}(S)}\n$$\n在计算上，对协方差矩阵 $S$ 进行特征分解可能数值不稳定，特别是对于病态数据。一种更稳健的方法是使用中心化数据矩阵 $X_c$ 的奇异值分解（SVD）。令 $X_c$ 的 SVD 为：\n$$\nX_c = U \\Sigma V^\\top\n$$\n其中 $U \\in \\mathbb{R}^{T \\times T}$ 和 $V \\in \\mathbb{R}^{N \\times N}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{T \\times N}$ 在其对角线上包含奇异值 $s_k$。$V$ 的列是主成分载荷向量，因此 $v_1$ 是 $V$ 的第一列。$S$ 的特征值与 $X_c$ 的奇异值通过 $\\lambda_k = \\frac{s_k^2}{T-1}$ 相关联。解释方差比 $\\rho_1$ 可以用奇异值表示，这巧妙地避免了对样本大小项的依赖：\n$$\n\\rho_1 = \\frac{\\lambda_1}{\\sum_{k=1}^N \\lambda_k} = \\frac{s_1^2 / (T-1)}{\\sum_{k=1}^N s_k^2 / (T-1)} = \\frac{s_1^2}{\\sum_{k=1}^N s_k^2}\n$$\n所有计算都将使用这种基于 SVD 的方法。\n\n对于每个给定的案例 $(N, T, A, t^\\star, \\tau_{\\mathrm{high}}, \\tau_{\\mathrm{low}}, \\tau_{\\mathrm{align}}, \\text{seed})$，其步骤如下：\n\n1.  **数据生成：** 使用指定的随机种子以确保可复现性，生成一个基准矩阵 $Z \\in \\mathbb{R}^{T \\times N}$，其条目独立同分布于标准正态分布 $\\mathcal{N}(0, 1)$。通过在元素 $(t^\\star, 0)$ 处引入一个幅度为 $A$ 的点离群值来形成数据矩阵 $X$，该离群值对应于方向 $u = e_1$：\n    $$\n    X = Z, \\quad X_{t^\\star, 0} \\leftarrow Z_{t^\\star, 0} + A\n    $$\n\n2.  **完整数据集分析：**\n    - 通过减去列样本均值来中心化矩阵 $X$，从而得到 $X_c$。\n    - 计算 $X_c$ 的 SVD：$X_c = U \\Sigma V^\\top$。\n    - PC1 载荷向量是 $V$ 的第一列，对应于 $V^\\top$ 的第一行。令其为 $v_1$。\n    - 解释方差比计算为 $\\rho_1 = s_1^2 / \\sum_k s_k^2$。\n    - 与离群值方向 $u=e_1$ 的对齐度计算为 $\\alpha = |\\langle v_1, e_1 \\rangle| = |(v_1)_1|$，即 $v_1$ 第一个元素的绝对值。\n\n3.  **缩减数据集分析：**\n    - 将离群观测值（即索引为 $t^\\star$ 的整行）从 $X$ 中删除，形成一个新的矩阵 $X' \\in \\mathbb{R}^{(T-1) \\times N}$。\n    - 使用其自身的列样本均值对这个缩减后的矩阵 $X'$ 进行重新中心化，以生成 $X'_c$。此时观测数量为 $T-1$。\n    - 计算 $X'_c$ 的 SVD。令新的奇异值为 $s'_k$。\n    - 缩减数据集的 PC1 解释方差比计算为 $\\tilde{\\rho}_1 = (s'_1)^2 / \\sum_k (s'_k)^2$。\n\n4.  **验证显性条件：** 最后一步是评估所有三个指定条件是否同时满足：\n    $$\n    (\\rho_1 > \\tau_{\\mathrm{high}}) \\land (\\tilde{\\rho}_1 < \\tau_{\\mathrm{low}}) \\land (\\alpha > \\tau_{\\mathrm{align}})\n    $$\n    该案例的结果是一个布尔值，代表此逻辑表达式的结果。对提供的每个测试案例重复此完整过程。实现将严格遵循这些步骤。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    For each case, it constructs a synthetic dataset with an outlier,\n    analyzes its principal components, re-analyzes the dataset after\n    removing the outlier, and checks if a set of dominance conditions are met.\n    \"\"\"\n    # Test suite format: (N, T, A, t_star, tau_high, tau_low, tau_align, seed)\n    test_cases = [\n        (5, 200, 50, 100, 0.6, 0.35, 0.95, 20231102),  # Case A\n        (5, 200, 12, 150, 0.28, 0.27, 0.90, 20231103),  # Case B\n        (8, 400, 0, 50, 0.25, 0.20, 0.90, 20231104),   # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        N, T, A, t_star, tau_high, tau_low, tau_align, seed = case\n        \n        # --- Analysis with Outlier ---\n\n        # 1. Generate Data\n        rng = np.random.default_rng(seed)\n        Z = rng.standard_normal(size=(T, N))\n        X = Z.copy()\n        # Add the outlier. u is e_1, so we modify the first column (index 0).\n        if A > 0:\n            X[t_star, 0] += A\n\n        # 2. Center Data\n        X_c = X - X.mean(axis=0)\n\n        # 3. Perform PCA (via SVD) and Calculate Metrics\n        # We use full_matrices=False for efficiency\n        _, s, Vh = np.linalg.svd(X_c, full_matrices=False)\n        \n        # Explained variance ratio rho_1\n        rho_1 = (s[0]**2) / np.sum(s**2)\n        \n        # PC1 loading vector v_1 is the first row of Vh (V transpose)\n        v_1 = Vh[0, :]\n        \n        # Alignment alpha with u = e_1 (the first standard basis vector)\n        alpha = np.abs(v_1[0])\n\n        # --- Analysis without Outlier ---\n        \n        # 1. Remove the outlier row\n        X_prime = np.delete(X, t_star, axis=0)\n        \n        # 2. Re-center the reduced data\n        X_prime_c = X_prime - X_prime.mean(axis=0)\n\n        # 3. Perform PCA on the reduced data\n        _, s_prime, _ = np.linalg.svd(X_prime_c, full_matrices=False)\n        \n        # Explained variance ratio tilde_rho_1\n        tilde_rho_1 = (s_prime[0]**2) / np.sum(s_prime**2)\n        \n        # --- Validate Conditions ---\n        \n        condition_met = (rho_1 > tau_high) and (tilde_rho_1 < tau_low) and (alpha > tau_align)\n        results.append(str(condition_met))\n\n    # Format the final output as a comma-separated list of booleans\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2421778"}]}