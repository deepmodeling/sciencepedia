{"hands_on_practices": [{"introduction": "在深度学习训练中，我们通常使用随机梯度下降（SGD）及其变体，通过计算一小批（mini-batch）数据的平均损失来估计梯度的方向。这个小批量损失 $\\hat{L}_{B}$ 本身就是一个随机变量，因为它依赖于随机抽样的数据。这个练习将帮助你从基础的概率论定义出发，把每个样本的损失 $\\ell_{i}$ 视为一个随机变量，并推导出小批量损失的方差与批量大小 $B$ 之间的重要关系，从而揭示了为何调整批量大小是控制训练稳定性的一个关键手段。[@problem_id:3166774]", "problem": "考虑一个深度学习训练场景，其中每个样本的损失被视为一个随机变量。设单个样本的损失表示为 $\\ell$，并假设 $\\ell$ 具有有限的均值 $\\mu = \\mathbb{E}[\\ell]$ 和有限的方差 $\\sigma^{2} = \\mathrm{Var}(\\ell)$。设 $\\{\\ell_{i}\\}_{i=1}^{B}$ 是在一个训练步骤中从一个打乱的数据集中独立同分布地抽取的损失值，定义小批量平均损失为\n$$\n\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}.\n$$\n您使用批量大小 $B=1$ 对大量样本进行了一次试点运行，以估计 $\\ell$ 的变异性，得到了单个样本损失的无偏样本方差估计值 $\\hat{\\sigma}^{2} = 0.09$。您计划选择一个批量大小 $B$，使得小批量平均损失 $\\hat{L}_{B}$ 的方差等于目标波动水平 $v^{\\star} = 0.001$。\n\n仅从独立随机变量的期望和方差的核心定义出发，并使用大数定律（LLN）和中心极限定理（CLT）作为对样本均值波动进行建模的基础理由，推导出 $\\hat{L}_{B}$ 的方差（用 $\\sigma^{2}$ 和 $B$ 表示），并确定当 $\\sigma^{2}$ 用 $\\hat{\\sigma}^{2}$ 近似时，能使 $\\mathrm{Var}(\\hat{L}_{B}) = v^{\\star}$ 成立的批量大小 $B$。请以单个整数形式提供您的最终答案。无需四舍五入。", "solution": "首先验证问题的科学性和逻辑合理性。\n\n### 第一步：提取已知条件\n- 单个样本的损失是一个随机变量 $\\ell$。\n- 损失的均值是有限的：$\\mu = \\mathbb{E}[\\ell]$。\n- 损失的方差是有限的：$\\sigma^{2} = \\mathrm{Var}(\\ell)$。\n- 一个小批量由 $\\{\\ell_{i}\\}_{i=1}^{B}$ 个独立同分布（i.i.d.）的损失样本组成。\n- 小批量平均损失定义为 $\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}$。\n- 给定 $\\ell$ 的一个无偏样本方差估计值为 $\\hat{\\sigma}^{2} = 0.09$。\n- 指定小批量平均损失的目标方差为 $v^{\\star} = 0.001$。\n\n### 第二步：使用已知条件进行验证\n该问题具有科学依据，提法明确，且客观。它提出了一个在深度学习中使用的随机优化方法的统计分析中的标准基础问题。样本均值、总体方差和样本均值的方差等概念是统计学的基石。该问题提供了所有必要的数据（$\\hat{\\sigma}^{2}$，$v^{\\star}$）和定义，以得出一个唯一的、有意义的批量大小 $B$ 的解。所提供的值是符合实际的。提及大数定律（LLN）和中心极限定理（CLT）在上下文中是恰当的，因为这些定理为小批量平均损失 $\\hat{L}_{B}$ 为何是真实期望损失 $\\mu$ 的一个良好估计量，以及为何其方差是一个需要控制的关键量提供了理论基础。该问题没有任何在说明中列出的可能使其无效的缺陷。\n\n### 第三步：结论与行动\n问题是有效的。将提供一个完整的、有理有据的解答。\n\n### 求解推导\n目标是推导出小批量平均损失的方差 $\\mathrm{Var}(\\hat{L}_{B})$，然后确定满足目标条件的批量大小 $B$。\n\n小批量平均损失定义为：\n$$\n\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\n$$\n我们要求它的方差 $\\mathrm{Var}(\\hat{L}_{B})$。我们从方差的核心定义及其对随机变量线性组合的性质开始。\n\n首先，我们使用性质：对于任意随机变量 $X$ 和常数 $c$，有 $\\mathrm{Var}(cX) = c^{2}\\mathrm{Var}(X)$。在这里，我们的随机变量是总和 $\\sum_{i=1}^{B} \\ell_{i}$，常数是 $\\frac{1}{B}$。\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = \\mathrm{Var}\\left(\\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\\right) = \\left(\\frac{1}{B}\\right)^{2} \\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right) = \\frac{1}{B^{2}} \\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right)\n$$\n接下来，我们确定随机变量之和的方差。对于任意两个*独立*的随机变量 $X$ 和 $Y$，它们和的方差等于它们方差的和：$\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$。问题说明，单个样本损失 $\\{\\ell_{i}\\}_{i=1}^{B}$ 是独立的。通过归纳法，这个性质可以推广到 $B$ 个独立随机变量的和：\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right) = \\sum_{i=1}^{B} \\mathrm{Var}(\\ell_{i})\n$$\n问题还说明，损失 $\\{\\ell_{i}\\}_{i=1}^{B}$ 是同分布的。这意味着每个 $\\ell_{i}$ 都具有相同的方差，即对于所有 $i \\in \\{1, 2, \\dots, B\\}$，都有 $\\mathrm{Var}(\\ell_{i}) = \\sigma^{2}$。因此，方差之和为：\n$$\n\\sum_{i=1}^{B} \\mathrm{Var}(\\ell_{i}) = \\sum_{i=1}^{B} \\sigma^{2} = B\\sigma^{2}\n$$\n将此结果代回到我们关于 $\\mathrm{Var}(\\hat{L}_{B})$ 的表达式中：\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = \\frac{1}{B^{2}} (B\\sigma^{2}) = \\frac{\\sigma^{2}}{B}\n$$\n这个推导表明，小批量平均损失的方差与批量大小 $B$ 成反比。这种关系对于理解深度学习中计算成本与梯度估计精度之间的权衡至关重要。大数定律保证了当 $B \\to \\infty$ 时，$\\hat{L}_{B}$ 收敛于 $\\mu$；而中心极限定理描述了在有限 $B$ 的情况下，$\\hat{L}_{B}$ 围绕 $\\mu$ 的波动分布，其波动尺度由 $\\mathrm{Var}(\\hat{L}_{B}) = \\frac{\\sigma^{2}}{B}$ 决定。\n\n现在，我们必须找到批量大小 $B$，使得小批量平均值的方差等于目标波动水平 $v^{\\star}$。\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = v^{\\star}\n$$\n使用我们推导出的公式：\n$$\n\\frac{\\sigma^{2}}{B} = v^{\\star}\n$$\n问题提供了一个来自试点运行的无偏样本方差估计值 $\\hat{\\sigma}^{2} = 0.09$，我们用它来近似真实的总体方差 $\\sigma^{2}$。代入给定值：\n$$\n\\frac{\\hat{\\sigma}^{2}}{B} = v^{\\star} \\implies \\frac{0.09}{B} = 0.001\n$$\n解出 $B$：\n$$\nB = \\frac{0.09}{0.001} = \\frac{9 \\times 10^{-2}}{1 \\times 10^{-3}} = 9 \\times 10^{1} = 90\n$$\n因此，根据所提供的单个样本损失方差的估计值，需要批量大小 $B=90$ 才能使小批量平均损失达到 $v^{\\star} = 0.001$ 的目标方差。", "answer": "$$\n\\boxed{90}\n$$", "id": "3166774"}, {"introduction": "在掌握了单个估计量（如损失）的方差后，我们可以将视野提升到整个模型预测的不确定性上。模型集成（ensemble）是一种通过组合多个独立训练的模型来提高预测鲁棒性和准确性的强大技术。这个练习将引导你运用全方差定律（Law of Total Variance）这一核心概率工具，将模型集成的总预测方差分解为两个有意义的部分：模型认知不确定性（epistemic uncertainty）和数据固有不确定性（aleatoric uncertainty），这对于构建可靠和可信的深度学习系统至关重要。[@problem_id:3166709]", "problem": "考虑一个监督深度学习场景，其中一组独立训练的神经网络（NNs）构成一个集成（ensemble）。对于一个固定的输入数据集，令 $N$ 表示不同输入的数量，令 $K$ 表示集成成员的数量。对于每个输入索引 $i \\in \\{1,\\dots,N\\}$ 和集成成员索引 $m \\in \\{1,\\dots,K\\}$，将标量预测表示为 $y_{i,m} \\in \\mathbb{R}$。定义随机输入 $X$ 为从 $N$ 个输入中均匀随机抽取一个，定义随机预测 $Y$ 为从 $K$ 个集成成员中均匀随机抽取一个，并取其在所选输入上的预测值。仅使用期望和方差的核心定义以及条件期望的定义，推导以 $X$为条件的方差分解，然后应用此分解从有限的集成预测矩阵 $\\{y_{i,m}\\}$ 中经验性地量化两个构成项。\n\n您必须从以下基本定义出发：\n- 随机变量 $Z$ 的方差定义为 $\\mathrm{Var}(Z) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z])^2\\big] = \\mathbb{E}[Z^2] - \\big(\\mathbb{E}[Z]\\big)^2$。\n- 条件期望 $\\mathbb{E}[Z \\mid W]$ 定义为一个随机变量（$W$ 的函数），对于任何相对于 $W$ 可测的事件 $A$，它满足 $\\mathbb{E}[Z \\mathbf{1}_A] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W] \\mathbf{1}_A\\big]$。\n- 全期望定律（塔性质）指出，对于任意一对随机变量 $(Z,W)$，有 $\\mathbb{E}[Z] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W]\\big]$。\n\n您的程序必须：\n- 为每个测试用例使用以下模型生成合成的集成预测 $\\{y_{i,m}\\}$。令 $x_i \\in \\mathbb{R}$ 表示索引 $i$ 处的标量输入。定义一个确定性基函数 $f(x) = a x + c$，一个跨 $m$ 独立的逐模型偏差 $b_m \\sim \\mathcal{N}(0,\\tau^2)$，以及具有异方差标准差 $\\sigma(x) = \\sigma_0 + \\sigma_1 |x|$ 的逐预测噪声 $\\epsilon_{i,m} \\sim \\mathcal{N}\\big(0, \\sigma(x_i)^2\\big)$。然后将每个预测设置为\n$$\ny_{i,m} = f(x_i) + b_m + \\epsilon_{i,m}.\n$$\n在每个测试用例中，将经验分布视为在所有观测到的输入和集成成员上的均匀分布。具体来说，使用经验期望，其计算方式为在有限集上求平均，分母分别为 $N$、$K$ 和 $N K$（不使用无偏修正）：\n- 对于每个输入 $i$，计算条件均值 $\\mu_i = \\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}$ 和条件方差 $v_i = \\frac{1}{K} \\sum_{m=1}^{K} \\big(y_{i,m} - \\mu_i\\big)^2$。\n- 计算两个分解项和总方差的经验模拟量如下\n$$\nA = \\frac{1}{N} \\sum_{i=1}^{N} v_i, \\quad\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\big(\\mu_i - \\bar{\\mu}\\big)^2, \\quad\nC = \\frac{1}{N K} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big(y_{i,m} - \\bar{y}\\big)^2,\n$$\n其中 $\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_i$ 且 $\\bar{y} = \\frac{1}{N K} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m}$。同时计算差异\n$$\nD = C - (A + B),\n$$\n如果使用经验期望正确地实现了分解，该值在数值上必须接近于 $0$。\n\n实现并评估以下测试套件，其中每个用例提供 $(N,K,a,c,\\tau,\\sigma_0,\\sigma_1,\\text{seed})$，并且输入 $x_i$ 使用提供的种子从 $\\mathcal{N}(0,1)$ 中独立抽取：\n- 测试用例 1（正常路径，异方差，中等集成多样性）：$N=200$，$K=5$，$a=2.0$，$c=0.5$，$\\tau=0.3$，$\\sigma_0=0.2$，$\\sigma_1=0.5$，$\\text{seed}=1234$。\n- 测试用例 2（边界情况，无输入内变异性）：$N=100$，$K=7$，$a=1.5$，$c=-1.0$，$\\tau=0.0$，$\\sigma_0=0.0$，$\\sigma_1=0.0$，$\\text{seed}=42$。\n- 测试用例 3（边界情况，集成均值无输入间变异性）：$N=150$，$K=4$，$a=0.0$，$c=3.0$，$\\tau=0.8$，$\\sigma_0=0.5$，$\\sigma_1=0.0$，$\\text{seed}=7$。\n- 测试用例 4（边缘情况，单个输入）：$N=1$，$K=10$，$a=0.7$，$c=0.0$，$\\tau=1.0$，$\\sigma_0=0.3$，$\\sigma_1=0.0$，$\\text{seed}=31415$。\n- 测试用例 5（边缘情况，单个模型）：$N=120$，$K=1$，$a=1.0$，$c=0.0$，$\\tau=0.0$，$\\sigma_0=0.2$，$\\sigma_1=0.0$，$\\text{seed}=98765$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有测试用例的结果。结果是一个逗号分隔的列表，包含在方括号中，每个测试用例报告为一个子列表 $[C,A,B,D]$，包含四个浮点数，保留六位小数。例如，输出必须如下所示：\n$$\n[[C_1,A_1,B_1,D_1],[C_2,A_2,B_2,D_2],\\dots,[C_5,A_5,B_5,D_5]].\n$$\n打印的行中任何位置都不允许有空格。", "solution": "该问题要求推导全方差定律，并将其应用于一个有限的集成预测集合。解答分为两部分：首先是方差分解的理论推导，然后证明该恒等式对于所提供的特定经验估计量在代数上成立。\n\n### 第一部分：全方差定律的推导\n\n目标是证明恒等式 $\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X])$。我们从提供的基本定义开始。\n\n随机变量 $Y$ 的方差定义为：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2\n$$\n我们将全期望定律 $\\mathbb{E}[Z] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W]\\big]$ 应用于右侧的每一项，并以随机变量 $X$ 为条件。\n\n对于项 $\\mathbb{E}[Y]$，我们有：\n$$\n\\mathbb{E}[Y] = \\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\n$$\n其中外部期望的下标 $\\mathbb{E}_X[\\cdot]$ 强调期望是针对 $X$ 的分布计算的。\n\n对于项 $\\mathbb{E}[Y^2]$，我们类似地有：\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}_X[\\mathbb{E}[Y^2 \\mid X]]\n$$\n将这些代入方差定义得到：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathbb{E}[Y^2 \\mid X]] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\n现在，我们使用条件方差的定义，$\\mathrm{Var}(Y \\mid X) = \\mathbb{E}[Y^2 \\mid X] - (\\mathbb{E}[Y \\mid X])^2$。这是一个作为 $X$ 函数的随机变量。我们可以重新整理这个定义来表示 $\\mathbb{E}[Y^2 \\mid X]$：\n$$\n\\mathbb{E}[Y^2 \\mid X] = \\mathrm{Var}(Y \\mid X) + (\\mathbb{E}[Y \\mid X])^2\n$$\n将 $\\mathbb{E}[Y^2 \\mid X]$ 的这个表达式代回到我们的 $\\mathrm{Var}(Y)$ 方程中：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X\\big[ \\mathrm{Var}(Y \\mid X) + (\\mathbb{E}[Y \\mid X])^2 \\big] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\n利用外部期望 $\\mathbb{E}_X[\\cdot]$ 的线性性质，我们可以拆分第一项：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] + \\mathbb{E}_X\\big[(\\mathbb{E}[Y \\mid X])^2\\big] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\n我们来考察最后两项。如果我们定义一个新的随机变量 $Z = \\mathbb{E}[Y \\mid X]$，它是 $X$ 的一个函数，那么这两项就是 $\\mathbb{E}_X[Z^2] - (\\mathbb{E}_X[Z])^2$。这正是 $Z$ 的方差的定义，即 $\\mathrm{Var}(Z) = \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])$。\n\n因此，我们得到最终的分解式：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X])\n$$\n第一项 $\\mathbb{E}[\\mathrm{Var}(Y \\mid X)]$ 是期望条件方差，通常称为“偶然不确定性”（aleatoric uncertainty）或“不可约不确定性”（irreducible uncertainty）。它表示对于固定输入，预测值的平均方差。第二项 $\\mathrm{Var}(\\mathbb{E}[Y \\mid X])$ 是条件期望的方差，通常称为“认知不确定性”（epistemic uncertainty）或模型分歧。它表示集成模型的平均预测在不同输入上的变异性。\n\n### 第二部分：作为代数恒等式的经验分解\n\n问题基于一个有限的预测矩阵 $\\{y_{i,m}\\}$（其中 $i \\in \\{1, \\dots, N\\}$ 且 $m \\in \\{1, \\dots, K\\}$）定义了随机变量 $X$ 和 $Y$。抽样过程是均匀的，意味着任何配对 $(i,m)$ 被选中的概率为 $1/(NK)$。经验量 $A$、$B$ 和 $C$ 定义如下：\n$$\nA = \\frac{1}{N} \\sum_{i=1}^{N} v_i \\quad \\text{其中} \\quad v_i = \\frac{1}{K} \\sum_{m=1}^{K} \\big(y_{i,m} - \\mu_i\\big)^2 \\quad \\text{且} \\quad \\mu_i = \\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}\n$$\n$$\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\big(\\mu_i - \\bar{\\mu}\\big)^2 \\quad \\text{其中} \\quad \\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_i\n$$\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big(y_{i,m} - \\bar{y}\\big)^2 \\quad \\text{其中} \\quad \\bar{y} = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m}\n$$\n这些量是理论分解中各项的经验模拟。我们将证明，对于这些特定的定义，恒等式 $C = A + B$ 在代数上成立。\n\n首先，注意总均值 $\\bar{y}$ 与条件均值的均值 $\\bar{\\mu}$ 是相同的：\n$$\n\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}\\right) = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m} = \\bar{y}\n$$\n现在，我们展开总方差 $C$ 的表达式。我们在平方项内部加上并减去条件均值 $\\mu_i$：\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big( (y_{i,m} - \\mu_i) + (\\mu_i - \\bar{y}) \\big)^2\n$$\n展开平方项 $(p+q)^2 = p^2 + q^2 + 2pq$：\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\left[ (y_{i,m} - \\mu_i)^2 + (\\mu_i - \\bar{y})^2 + 2(y_{i,m} - \\mu_i)(\\mu_i - \\bar{y}) \\right]\n$$\n根据求和的线性性质，我们可以将其分为三项：\n$$\nC = \\frac{1}{NK} \\sum_{i,m} (y_{i,m} - \\mu_i)^2 + \\frac{1}{NK} \\sum_{i,m} (\\mu_i - \\bar{y})^2 + \\frac{2}{NK} \\sum_{i,m} (y_{i,m} - \\mu_i)(\\mu_i - \\bar{y})\n$$\n让我们逐项分析：\n1.  **第一项**：\n    $$\n    \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} (y_{i,m} - \\mu_i)^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{1}{K} \\sum_{m=1}^{K} (y_{i,m} - \\mu_i)^2 \\right) = \\frac{1}{N} \\sum_{i=1}^{N} v_i = A\n    $$\n2.  **第二项**：被加数 $(\\mu_i - \\bar{y})^2$ 不依赖于索引 $m$。对 $m$ 求和会产生一个因子 $K$：\n    $$\n    \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} (\\mu_i - \\bar{y})^2 = \\frac{1}{NK} \\sum_{i=1}^{N} K (\\mu_i - \\bar{y})^2 = \\frac{1}{N} \\sum_{i=1}^{N} (\\mu_i - \\bar{y})^2 = B\n    $$\n    （因为 $\\bar{y}=\\bar{\\mu}$）\n3.  **第三项（交叉项）**：我们可以将不依赖于 $m$ 的项提取出来：\n    $$\n    \\frac{2}{NK} \\sum_{i=1}^{N} (\\mu_i - \\bar{y}) \\left( \\sum_{m=1}^{K} (y_{i,m} - \\mu_i) \\right)\n    $$\n    根据 $\\mu_i$ 的定义，内部对 $m$ 的求和为零：\n    $$\n    \\sum_{m=1}^{K} (y_{i,m} - \\mu_i) = \\left( \\sum_{m=1}^{K} y_{i,m} \\right) - \\left( \\sum_{m=1}^{K} \\mu_i \\right) = (K \\mu_i) - (K \\mu_i) = 0\n    $$\n    因此，整个交叉项为零。\n\n综合这些结果，我们证明了对于给定的经验估计量：\n$$\nC = A + B\n$$\n这意味着差异 $D = C - (A+B)$ 必须精确为 $0$。在数值实现中观察到的任何非零值都将完全是由于浮点表示误差引起的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and empirically validates the law of total variance for an ensemble\n    of neural network predictions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, K, a, c, tau, sigma0, sigma1, seed)\n        (200, 5, 2.0, 0.5, 0.3, 0.2, 0.5, 1234),\n        (100, 7, 1.5, -1.0, 0.0, 0.0, 0.0, 42),\n        (150, 4, 0.0, 3.0, 0.8, 0.5, 0.0, 7),\n        (1, 10, 0.7, 0.0, 1.0, 0.3, 0.0, 31415),\n        (120, 1, 1.0, 0.0, 0.0, 0.2, 0.0, 98765),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        N, K, a, c, tau, sigma0, sigma1, seed = case\n        \n        # Set up a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Generate synthetic data according to the model.\n        # 1. Inputs x_i ~ N(0, 1)\n        x = rng.normal(loc=0.0, scale=1.0, size=N)\n        \n        # 2. Per-model biases b_m ~ N(0, tau^2)\n        b = rng.normal(loc=0.0, scale=tau, size=K)\n        \n        # 3. Base function f(x_i)\n        f_x = a * x + c\n        \n        # 4. Heteroscedastic noise standard deviation sigma(x_i)\n        sigma_x = sigma0 + sigma1 * np.abs(x)\n        \n        # 5. Per-prediction noise epsilon_{i,m} ~ N(0, sigma(x_i)^2)\n        # We broadcast sigma_x (N,) to (N, K) to generate the noise matrix.\n        epsilon_scale = sigma_x[:, np.newaxis]\n        epsilon = rng.normal(loc=0.0, scale=epsilon_scale, size=(N, K))\n        \n        # 6. Final predictions y_{i,m}\n        # We use broadcasting to combine f_x (N,), b (K,), and epsilon (N, K).\n        y = f_x[:, np.newaxis] + b[np.newaxis, :] + epsilon\n        \n        # Compute the empirical quantities as defined in the problem.\n        # ddof=0 ensures we divide by n (population variance) instead of n-1.\n        \n        # Conditional mean for each input i: mu_i\n        mu = np.mean(y, axis=1)  # Shape (N,)\n        \n        # Conditional variance for each input i: v_i\n        v = np.var(y, axis=1, ddof=0)  # Shape (N,)\n        \n        # Term A: Expected value of the conditional variance\n        # A = (1/N) * sum(v_i)\n        A = np.mean(v)\n        \n        # Term B: Variance of the conditional expectation\n        # B = (1/N) * sum((mu_i - bar_mu)^2)\n        # np.var calculates variance against the mean of the input array `mu`.\n        B = np.var(mu, ddof=0)\n        \n        # Term C: Total variance\n        # C = (1/NK) * sum((y_{i,m} - bar_y)^2)\n        # np.var applied to the entire matrix calculates this.\n        C = np.var(y, ddof=0)\n        \n        # Discrepancy D = C - (A + B)\n        # This should be zero up to floating-point precision, as shown in the derivation.\n        D = C - (A + B)\n        \n        case_results = [C, A, B, D]\n        # Format the results for this case to 6 decimal places.\n        case_str = f\"[{','.join([f'{val:.6f}' for val in case_results])}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[[{','.join(all_results_str)}]]\")\n\nsolve()\n```", "id": "3166709"}, {"introduction": "当模型的架构本身包含随机过程时（例如在变分自编码器或随机神经网络中），训练就变得更具挑战性，因为我们需要对一个期望值求梯度。这个练习将带你深入探讨两种解决此类问题的核心技术：得分函数估计器（score-function estimator）和重参数化技巧（reparameterization trick）。通过从第一性原理推导并比较这两种估计器的方差，你将深刻理解为何后者在实践中通常能带来更稳定、高效的训练，这也是现代生成模型成功的关键之一。[@problem_id:3166695]", "problem": "考虑一个深度学习回归模型，在训练期间向单个神经元的预激活值中注入加性高斯噪声。设带有噪声的预激活值为随机变量 $Z \\sim \\mathcal{N}(\\theta, \\sigma^{2})$，其中 $\\theta \\in \\mathbb{R}$ 是一个可训练的参数，$\\sigma^{2} > 0$ 是一个已知的常数。该神经元的训练目标包含期望平方预激活值 $J(\\theta) = \\mathbb{E}_{Z \\sim p_{\\theta}}[f(Z)]$，其中 $f(z) = z^{2}$。\n\n请仅使用概率论和微积分的基本定义与结论——具体包括期望的定义、方差、高斯分布的矩、微分的链式法则、在标准正则性条件下微分与期望的交换，以及变量替换——通过得分函数方法和重参数化方法推导 $\\nabla_{\\theta} J(\\theta)$ 的无偏估计量。然后，计算每个估计量在抽样分布下的方差，并求出得分函数估计量的方差与重参数化估计量的方差之比。请将你的最终答案表示为关于 $\\theta$ 和 $\\sigma$ 的单一、完全简化的封闭形式解析表达式。不需要进行数值取整。", "solution": "用户希望我解决这个问题，并且我已经验证了它。我现在将开始进行解答。\n\n### 第一步：问题验证\n\n**1.1. 提取已知条件**\n- 随机变量是带噪声的预激活值，$Z$。\n- $Z$ 的分布是高斯分布：$Z \\sim \\mathcal{N}(\\theta, \\sigma^{2})$。\n- $\\theta \\in \\mathbb{R}$ 是一个可训练的参数。\n- $\\sigma^{2} > 0$ 是一个已知的常数。\n- 训练目标是 $J(\\theta) = \\mathbb{E}_{Z \\sim p_{\\theta}}[f(Z)]$。\n- 函数是 $f(z) = z^{2}$。\n- 任务是使用得分函数和重参数化方法推导 $\\nabla_{\\theta} J(\\theta)$ 的无偏估计量。\n- 然后，计算每个估计量的方差。\n- 最后，求出得分函数估计量的方差与重参数化估计量的方差之比。\n\n**1.2. 使用提取的已知条件进行验证**\n- **科学性：** 该问题基于概率论、微积分和机器学习中的标准基本概念，即高斯分布、期望、方差、微分以及梯度估计技术（得分函数和重参数化）。这些方法是现代深度学习的基石，尤其是在变分自编码器和随机优化的背景下。该设定是用于说明这些概念的典型例子。\n- **良态性：** 该问题提供了明确的目标和推导估计量及其方差所需的所有必要信息。待计算的量是唯一确定的。\n- **客观性：** 该问题以精确的数学语言陈述，没有主观性或模糊性。\n\n**1.3. 结论与行动**\n该问题是有效的。它具有科学性、良态性、客观性，并且与指定主题直接相关。我将进行完整的推导。\n\n### 第二步：解题推导\n\n需要微分的目标函数是 $J(\\theta) = \\mathbb{E}_{Z \\sim p_{\\theta}}[f(Z)]$，其中 $Z \\sim \\mathcal{N}(\\theta, \\sigma^{2})$ 且 $f(z) = z^2$。\n首先，我们计算 $J(\\theta)$ 及其梯度的精确值，这将作为参考。\n随机变量平方的期望 $\\mathbb{E}[Z^2]$ 与其方差和均值通过恒等式 $\\text{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$ 相关联。\n对于 $Z \\sim \\mathcal{N}(\\theta, \\sigma^{2})$，我们有 $\\mathbb{E}[Z] = \\theta$ 和 $\\text{Var}(Z) = \\sigma^2$。\n因此，$J(\\theta) = \\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2 = \\sigma^2 + \\theta^2$。\n目标函数关于 $\\theta$ 的真实梯度是：\n$$\n\\nabla_{\\theta} J(\\theta) = \\frac{d}{d\\theta}(\\sigma^2 + \\theta^2) = 2\\theta\n$$\n\n**A. 得分函数估计量**\n\n得分函数方法，也称为 REINFORCE，依赖于恒等式 $\\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f(Z)] = \\mathbb{E}_{p_{\\theta}}[f(Z) \\nabla_{\\theta} \\log p_{\\theta}(Z)]$。因此，梯度的无偏单样本估计量是 $g_{SF}(z) = f(z) \\nabla_{\\theta} \\log p_{\\theta}(z)$。\n\n1.  **推导估计量：**\n    $Z$ 的概率密度函数 (PDF) 是 $p_{\\theta}(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\theta)^2}{2\\sigma^2}\\right)$。\n    对数概率密度函数是 $\\log p_{\\theta}(z) = -\\log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(z-\\theta)^2}{2\\sigma^2}$。\n    对数概率密度函数关于 $\\theta$ 的梯度就是得分函数：\n    $$\n    \\nabla_{\\theta} \\log p_{\\theta}(z) = \\frac{d}{d\\theta}\\left(-\\frac{(z-\\theta)^2}{2\\sigma^2}\\right) = -\\frac{1}{2\\sigma^2} \\cdot 2(z-\\theta) \\cdot (-1) = \\frac{z-\\theta}{\\sigma^2}\n    $$\n    当 $f(z) = z^2$ 时，得分函数估计量是：\n    $$\n    g_{SF}(z) = z^2 \\left(\\frac{z-\\theta}{\\sigma^2}\\right) = \\frac{z^3 - \\theta z^2}{\\sigma^2}\n    $$\n    为了确认其无偏性，我们计算它的期望：\n    $\\mathbb{E}[g_{SF}(Z)] = \\frac{1}{\\sigma^2} (\\mathbb{E}[Z^3] - \\theta\\mathbb{E}[Z^2])$。\n    令 $X = Z - \\theta$，则 $X \\sim \\mathcal{N}(0, \\sigma^2)$。$X$ 的矩为 $\\mathbb{E}[X]=0$，$\\mathbb{E}[X^2]=\\sigma^2$，$\\mathbb{E}[X^3]=0$。\n    $\\mathbb{E}[Z^2] = \\mathbb{E}[(X+\\theta)^2] = \\mathbb{E}[X^2+2X\\theta+\\theta^2] = \\sigma^2+\\theta^2$。\n    $\\mathbb{E}[Z^3] = \\mathbb{E}[(X+\\theta)^3] = \\mathbb{E}[X^3+3X^2\\theta+3X\\theta^2+\\theta^3] = 3\\theta\\sigma^2+\\theta^3$。\n    $$\n    \\mathbb{E}[g_{SF}(Z)] = \\frac{1}{\\sigma^2}((3\\theta\\sigma^2+\\theta^3) - \\theta(\\sigma^2+\\theta^2)) = \\frac{1}{\\sigma^2}(3\\theta\\sigma^2+\\theta^3 - \\theta\\sigma^2-\\theta^3) = \\frac{2\\theta\\sigma^2}{\\sigma^2} = 2\\theta\n    $$\n    该估计量是无偏的，因为它等于 $\\nabla_{\\theta}J(\\theta)$。\n\n2.  **计算估计量的方差：**\n    $\\text{Var}(g_{SF}(Z)) = \\mathbb{E}[g_{SF}(Z)^2] - (\\mathbb{E}[g_{SF}(Z)])^2$。\n    我们使用重参数化 $Z = \\theta + \\sigma\\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, 1)$ 是一个标准正态变量。$\\epsilon$ 的矩为 $\\mathbb{E}[\\epsilon^{2n+1}] = 0$ 和 $\\mathbb{E}[\\epsilon^{2n}] = (2n-1)!!$。具体来说，$\\mathbb{E}[\\epsilon^2]=1$，$\\mathbb{E}[\\epsilon^4]=3$，$\\mathbb{E}[\\epsilon^6]=15$。\n    用 $\\epsilon$ 表示的估计量是：\n    $$\n    g_{SF}(Z) = f(\\theta+\\sigma\\epsilon) \\frac{(\\theta+\\sigma\\epsilon)-\\theta}{\\sigma^2} = (\\theta+\\sigma\\epsilon)^2 \\frac{\\sigma\\epsilon}{\\sigma^2} = (\\theta^2 + 2\\theta\\sigma\\epsilon + \\sigma^2\\epsilon^2) \\frac{\\epsilon}{\\sigma} = \\frac{\\theta^2}{\\sigma}\\epsilon + 2\\theta\\epsilon^2 + \\sigma\\epsilon^3\n    $$\n    现在，我们计算 $\\mathbb{E}[g_{SF}(Z)^2]$：\n    $$\n    \\mathbb{E}[g_{SF}(Z)^2] = \\mathbb{E}\\left[\\left(\\frac{\\theta^2}{\\sigma}\\epsilon + 2\\theta\\epsilon^2 + \\sigma\\epsilon^3\\right)^2\\right]\n    $$\n    展开平方时，只有 $\\epsilon$ 的偶数次幂项的期望不为零。这些项是：\n    $$\n    \\mathbb{E}\\left[\\left(\\frac{\\theta^2}{\\sigma}\\epsilon\\right)^2 + (2\\theta\\epsilon^2)^2 + (\\sigma\\epsilon^3)^2 + 2\\left(\\frac{\\theta^2}{\\sigma}\\epsilon\\right)(\\sigma\\epsilon^3)\\right]\n    $$\n    $$\n    = \\mathbb{E}\\left[\\frac{\\theta^4}{\\sigma^2}\\epsilon^2 + 4\\theta^2\\epsilon^4 + \\sigma^2\\epsilon^6 + 2\\theta^2\\epsilon^4\\right]\n    $$\n    使用期望的线性和 $\\epsilon$ 的矩：\n    $$\n    = \\frac{\\theta^4}{\\sigma^2}\\mathbb{E}[\\epsilon^2] + (4\\theta^2 + 2\\theta^2)\\mathbb{E}[\\epsilon^4] + \\sigma^2\\mathbb{E}[\\epsilon^6]\n    $$\n    $$\n    = \\frac{\\theta^4}{\\sigma^2}(1) + 6\\theta^2(3) + \\sigma^2(15) = \\frac{\\theta^4}{\\sigma^2} + 18\\theta^2 + 15\\sigma^2\n    $$\n    方差是：\n    $$\n    \\text{Var}(g_{SF}(Z)) = \\mathbb{E}[g_{SF}(Z)^2] - (2\\theta)^2 = \\left(\\frac{\\theta^4}{\\sigma^2} + 18\\theta^2 + 15\\sigma^2\\right) - 4\\theta^2 = \\frac{\\theta^4}{\\sigma^2} + 14\\theta^2 + 15\\sigma^2\n    $$\n\n**B. 重参数化估计量**\n\n重参数化技巧涉及将 $Z$ 写成一个基础随机变量 $\\epsilon$ 和参数 $\\theta$ 的确定性变换：$Z = g(\\theta, \\epsilon)$，其中 $\\epsilon$ 的分布不依赖于 $\\theta$。然后 $\\nabla_{\\theta}\\mathbb{E}[f(Z)] = \\mathbb{E}[\\nabla_{\\theta}f(g(\\theta, \\epsilon))]$。\n\n1.  **推导估计量：**\n    我们使用重参数化 $Z = \\theta + \\sigma\\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, 1)$。\n    $J(\\theta) = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,1)}[f(\\theta+\\sigma\\epsilon)]$。\n    我们可以交换期望和微分：\n    $$\n    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\epsilon}[\\nabla_{\\theta} f(\\theta+\\sigma\\epsilon)]\n    $$\n    一个无偏的单样本估计量是 $g_{RP}(\\epsilon) = \\nabla_{\\theta} f(\\theta+\\sigma\\epsilon)$。\n    当 $f(z) = z^2$ 时，我们有 $f(\\theta+\\sigma\\epsilon) = (\\theta+\\sigma\\epsilon)^2$。\n    估计量是：\n    $$\n    g_{RP}(\\epsilon) = \\frac{d}{d\\theta}(\\theta+\\sigma\\epsilon)^2 = 2(\\theta+\\sigma\\epsilon)\n    $$\n    用原始变量 $z$ 表示，通过代入 $\\epsilon = (z-\\theta)/\\sigma$，我们得到 $g_{RP}(z) = 2(\\theta + \\sigma\\frac{z-\\theta}{\\sigma}) = 2z$。\n    为了确认其无偏性：$\\mathbb{E}[g_{RP}(Z)] = \\mathbb{E}[2Z] = 2\\mathbb{E}[Z] = 2\\theta$。这与 $\\nabla_{\\theta}J(\\theta)$ 相匹配。\n\n2.  **计算估计量的方差：**\n    $$\n    \\text{Var}(g_{RP}(Z)) = \\text{Var}(2Z)\n    $$\n    使用属性 $\\text{Var}(aX) = a^2\\text{Var}(X)$：\n    $$\n    \\text{Var}(g_{RP}(Z)) = 2^2 \\text{Var}(Z) = 4\\sigma^2\n    $$\n\n**C. 方差之比**\n\n最后，我们计算得分函数估计量的方差与重参数化估计量的方差之比。\n$$\n\\text{Ratio} = \\frac{\\text{Var}(g_{SF}(Z))}{\\text{Var}(g_{RP}(Z))} = \\frac{\\frac{\\theta^4}{\\sigma^2} + 14\\theta^2 + 15\\sigma^2}{4\\sigma^2}\n$$\n简化表达式：\n$$\n\\text{Ratio} = \\frac{\\theta^4}{4\\sigma^4} + \\frac{14\\theta^2}{4\\sigma^2} + \\frac{15\\sigma^2}{4\\sigma^2} = \\frac{\\theta^4}{4\\sigma^4} + \\frac{7\\theta^2}{2\\sigma^2} + \\frac{15}{4}\n$$\n这就是方差之比的最终封闭形式解析表达式。", "answer": "$$\n\\boxed{\\frac{\\theta^4}{4\\sigma^4} + \\frac{7\\theta^2}{2\\sigma^2} + \\frac{15}{4}}\n$$", "id": "3166695"}]}