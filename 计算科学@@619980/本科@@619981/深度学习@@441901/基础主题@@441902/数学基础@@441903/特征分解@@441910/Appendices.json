{"hands_on_practices": [{"introduction": "L2 正则化是深度学习中一种广泛使用的技术，用以防止模型过拟合。但它究竟是如何影响优化过程的呢？本练习将通过一个数值实验，让你亲手验证 L2 正则化如何系统地改变损失函数的几何形状。你将发现，正则化项通过平移海森矩阵的特征值来增加损失曲面的曲率，从而帮助优化过程更加平顺。[@problem_id:3120548]", "problem": "考虑标签在 $\\{0,1\\}$ 中的二元分类问题，使用经验风险最小化（ERM）和逻辑斯谛损失。设 $X \\in \\mathbb{R}^{n \\times d}$ 为设计矩阵，其行是样本；$y \\in \\{0,1\\}^{n}$ 为标签向量；$w \\in \\mathbb{R}^{d}$ 为参数向量。定义逻辑斯谛函数 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ 和经验风险 $R(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell(w; x_i, y_i)$，其中 $\\ell(w; x_i, y_i) = - y_i \\log \\sigma(x_i^\\top w) - (1 - y_i) \\log(1 - \\sigma(x_i^\\top w))$。定义强度为 $\\lambda \\ge 0$ 的平方欧几里得范数惩罚项（L2）为 $P_\\lambda(w) = \\frac{\\lambda}{2} \\lVert w \\rVert_2^2$，以及正则化风险 $R_\\lambda(w) = R(w) + P_\\lambda(w)$。一个二阶可微函数 $F$ 在 $w$ 处的Hessian矩阵是其二阶偏导数矩阵 $\\nabla^2 F(w)$。仅使用 $R(w)$、$P_\\lambda(w)$ 的定义和微分法则，解析地推导 $R_\\lambda(w)$ 的Hessian矩阵（用 $R(w)$ 的Hessian矩阵表示），并确定Hessian矩阵的特征值如何受到惩罚强度 $\\lambda$ 的影响。然后，设计并实现一个数值实验，在训练动态中分离出这种效应，这意味着您必须在一个无正则化的梯度下降轨迹上，于相同的参数迭代点 $w$ 评估曲率效应，以避免来自不同轨迹的混淆。\n\n您的程序必须从第一性原理实现以下步骤：\n- 通过对具有独立同分布标准正态项的 $X$ 进行采样，采样一个真实参数 $w_\\star$，计算 logits $z = X w_\\star$，采样 $y_i \\sim \\mathrm{Bernoulli}(\\sigma(z_i))$，并返回 $(X,y)$ 来生成合成数据集 $(X,y)$。通过固定的随机种子确保可复现性。\n- 从初始化 $w_0 = 0$ 开始，以固定的学习率 $\\eta > 0$，对无正则化的经验风险 $R(w)$ 运行全批量梯度下降（GD）$K$ 步。\n- 在每个GD迭代点 $w_k$，计算Hessian矩阵 $H_{\\text{unreg}}(w_k) = \\nabla^2 R(w_k)$，以及在同一点的正则化Hessian矩阵 $H_{\\text{reg}}(w_k,\\lambda) = \\nabla^2 R_\\lambda(w_k)$。\n- 计算两个Hessian矩阵的特征分解以获得它们的谱。为了分离出惩罚项的光谱效应，通过计算排序后特征值的差异，在相同的 $w_k$ 处比较它们的谱。对每个测试案例，聚合所有特征值和所有迭代点上的最大绝对偏差：\n$$\n\\varepsilon_{\\max} = \\max_{0 \\le k  K} \\left( \\max_j \\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\Delta \\, \\right| \\right),\n$$\n其中 $\\Delta$ 是您解析确定的恒定谱移，$\\lambda_j(\\cdot)$ 表示按升序排列的第 $j$ 个特征值。\n- 为每个测试案例报告非负实数 $\\varepsilon_{\\max}$。\n\n实现以下测试套件，以覆盖一般情况、边界条件和秩亏的边缘情况：\n- 案例A（理想情况，满秩）：$n = 200$, $d = 5$, 种子 $= 7$, $\\lambda = 0.05$, $K = 8$, $\\eta = 0.1$，满秩 $X$。\n- 案例B（边界情况，零惩罚）：$n = 200$, $d = 5$, 种子 $= 7$, $\\lambda = 0$, $K = 8$, $\\eta = 0.1$，满秩 $X$。\n- 案例C（边缘情况，秩亏且惩罚适中）：$n = 150$, $d = 6$, 种子 $= 11$, $\\lambda = 0.05$, $K = 8$, $\\eta = 0.1$，构造一个有两列相同的 $X$，以使其秩减1。\n- 案例D（边缘情况，秩亏且惩罚较大）：$n = 150$, $d = 6$, 种子 $= 11$, $\\lambda = 10.0$, $K = 8$, $\\eta = 0.1$，与案例C相同的秩亏构造。\n- 案例E（小规模可变性检查）：$n = 60$, $d = 3$, 种子 $= 3$, $\\lambda = 0.2$, $K = 10$, $\\eta = 0.2$，满秩 $X$。\n\n此问题不适用角度单位和物理单位。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序为 [案例A, 案例B, 案例C, 案例D, 案例E]。每个条目必须是该案例的实数 $\\varepsilon_{\\max}$，如上文定义。例如，格式必须与 $[\\varepsilon_A,\\varepsilon_B,\\varepsilon_C,\\varepsilon_D,\\varepsilon_E]$ 完全一样。", "solution": "经评估，问题陈述有效，因为它在科学上基于优化和机器学习的原理，问题设定良好，具有清晰且可在数值上验证的目标，并且没有矛盾或含糊之处。\n\n此处提供一个完整的解决方案，包括解析推导和数值实验的设计。\n\n### Hessian矩阵及其谱移的解析推导\n\n目标是推导L2正则化逻辑斯谛回归风险 $R_\\lambda(w)$ 的Hessian矩阵，并确定其特征值如何受到正则化参数 $\\lambda$ 的影响。\n\n正则化风险定义为经验风险 $R(w)$ 和惩罚项 $P_\\lambda(w)$ 的和：\n$$\nR_\\lambda(w) = R(w) + P_\\lambda(w)\n$$\n其中 $R(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell(w; x_i, y_i)$ 且 $P_\\lambda(w) = \\frac{\\lambda}{2} \\lVert w \\rVert_2^2$。\n\nHessian算子 $\\nabla^2$ 是一个线性算子。因此，正则化风险的Hessian矩阵是经验风险的Hessian矩阵与惩罚项的Hessian矩阵之和：\n$$\n\\nabla^2 R_\\lambda(w) = \\nabla^2 R(w) + \\nabla^2 P_\\lambda(w)\n$$\n\n让我们计算惩罚项的Hessian矩阵 $\\nabla^2 P_\\lambda(w)$。惩罚项为 $P_\\lambda(w) = \\frac{\\lambda}{2} \\sum_{j=1}^{d} w_j^2$。\n关于向量 $w$ 的分量 $w_k$ 的一阶偏导数是：\n$$\n\\frac{\\partial P_\\lambda(w)}{\\partial w_k} = \\frac{\\partial}{\\partial w_k} \\left( \\frac{\\lambda}{2} \\sum_{j=1}^{d} w_j^2 \\right) = \\frac{\\lambda}{2} \\cdot (2 w_k) = \\lambda w_k\n$$\n这意味着梯度是 $\\nabla P_\\lambda(w) = \\lambda w$。\n\n二阶偏导数构成了Hessian矩阵的元素。第 $(j,k)$ 个元素是：\n$$\n(\\nabla^2 P_\\lambda(w))_{jk} = \\frac{\\partial^2 P_\\lambda(w)}{\\partial w_j \\partial w_k} = \\frac{\\partial}{\\partial w_j} (\\lambda w_k)\n$$\n如果 $j=k$，这个导数是 $\\lambda$；如果 $j \\neq k$，则是 $0$。这可以用克罗内克δ函数 $\\delta_{jk}$ 来表示。\n$$\n(\\nabla^2 P_\\lambda(w))_{jk} = \\lambda \\delta_{jk}\n$$\n这是矩阵 $\\lambda I_d$ 的定义，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。因此，$\\nabla^2 P_\\lambda(w) = \\lambda I_d$。\n\n将此结果代回，我们得到正则化Hessian矩阵与无正则化Hessian矩阵之间的关系：\n$$\n\\nabla^2 R_\\lambda(w) = \\nabla^2 R(w) + \\lambda I_d\n$$\n我们用 $H_{\\text{unreg}}(w) = \\nabla^2 R(w)$ 表示无正则化的Hessian矩阵，用 $H_{\\text{reg}}(w, \\lambda) = \\nabla^2 R_\\lambda(w)$ 表示正则化的Hessian矩阵。关系式为：\n$$\nH_{\\text{reg}}(w, \\lambda) = H_{\\text{unreg}}(w) + \\lambda I_d\n$$\n现在，我们分析这种关系对特征值的影响。设 $v$ 是无正则化Hessian矩阵 $H_{\\text{unreg}}(w)$ 的一个特征向量，其对应的特征值为 $\\mu$。根据定义：\n$$\nH_{\\text{unreg}}(w) v = \\mu v\n$$\n将正则化Hessian矩阵 $H_{\\text{reg}}(w, \\lambda)$ 应用于这个特征向量 $v$：\n$$\nH_{\\text{reg}}(w, \\lambda) v = (H_{\\text{unreg}}(w) + \\lambda I_d) v = H_{\\text{unreg}}(w) v + \\lambda I_d v = \\mu v + \\lambda v = (\\mu + \\lambda) v\n$$\n这表明 $v$ 也是 $H_{\\text{reg}}(w, \\lambda)$ 的一个特征向量，其对应的特征值为 $\\mu + \\lambda$。由于Hessian矩阵是实对称的，它们拥有一组完备的标准正交特征向量基。这个性质对所有特征向量都成立，这意味着正则化Hessian矩阵的整个谱相对于无正则化Hessian矩阵的谱被平移了一个常数值 $\\lambda$。\n\n因此，所确定的恒定谱移为 $\\Delta = \\lambda$。设计的数值实验旨在验证 $\\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\lambda \\, \\right|$ 的值接近于零，其中与零的差异可归因于浮点精度误差。\n\n### 数值实验设计\n\n数值实验的结构旨在分离并验证这一解析发现。\n\n1.  **数据生成**：使用固定的随机种子生成合成数据集 $(X, y)$ 以保证可复现性。特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 从标准正态分布中采样。真实参数向量 $w_\\star \\in \\mathbb{R}^d$ 也被采样。计算Logits $z = X w_\\star$，然后通过逻辑斯谛函数 $\\sigma(t) = (1+e^{-t})^{-1}$ 生成概率 $p_i = \\sigma(z_i)$。然后从伯努利分布 $y_i \\sim \\text{Bernoulli}(p_i)$ 中采样二元标签 $y_i$。对于秩亏的情况，使 $X$ 的两列相同，从而使其秩至少减少一。\n\n2.  **梯度下降轨迹**：从 $w_0 = 0$ 开始，执行全批量梯度下降（GD）$K$ 次迭代。关键的是，更新使用的是*无正则化*风险 $R(w)$ 的梯度，其梯度由 $\\nabla R(w) = \\frac{1}{n} X^\\top (\\sigma(Xw) - y)$ 给出。这确保了迭代点序列 $w_0, w_1, \\dots, w_{K-1}$ 独立于 $\\lambda$，从而允许进行受控比较。\n\n3.  **Hessian矩阵评估**：在此轨迹上的每个迭代点 $w_k$，计算无正则化和正则化的Hessian矩阵。逻辑斯谛风险的无正则化Hessian矩阵由以下公式给出：\n    $$\n    H_{\\text{unreg}}(w_k) = \\nabla^2 R(w_k) = \\frac{1}{n} X^\\top S_k X\n    $$\n    其中 $S_k$ 是一个对角矩阵，其对角线元素为 $S_{ii} = p_{k,i}(1-p_{k,i})$，且 $p_k = \\sigma(Xw_k)$。然后通过 $H_{\\text{reg}}(w_k, \\lambda) = H_{\\text{unreg}}(w_k) + \\lambda I_d$ 计算正则化Hessian矩阵。\n\n4.  **谱分析和度量计算**：对于在步骤 $k$ 计算的每对Hessian矩阵，使用用于对称矩阵的数值特征求解器计算它们的特征值，该求解器按升序返回特征值。设这些排序后的特征值为 $\\lambda_j(H_{\\text{unreg}}(w_k))$ 和 $\\lambda_j(H_{\\text{reg}}(w_k, \\lambda))$。分析的核心是计算与理论预期的偏差：\n    $$\n    d_{k,j} = \\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\lambda \\, \\right|\n    $$\n    最终报告的度量 $\\varepsilon_{\\max}$ 是这些偏差在所有特征值 $j$ 和所有GD步骤 $k$ 上的最大值：\n    $$\n    \\varepsilon_{\\max} = \\max_{0 \\le k  K} \\left( \\max_j d_{k,j} \\right)\n    $$\n    该值量化了数值结果与解析理论之间的最大差异，该差异应在机器精度的数量级上。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef run_experiment(n, d, seed, lambd, K, eta, rank_deficient):\n    \"\"\"\n    Runs one instance of the numerical experiment to measure the spectral shift\n    of the Hessian due to L2 regularization.\n\n    Args:\n        n (int): Number of samples.\n        d (int): Number of features.\n        seed (int): Random seed for reproducibility.\n        lambd (float): L2 regularization strength.\n        K (int): Number of Gradient Descent steps.\n        eta (float): Learning rate for Gradient Descent.\n        rank_deficient (bool): If True, the data matrix X is made rank-deficient.\n\n    Returns:\n        float: The maximum absolute deviation, epsilon_max.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Generate synthetic dataset\n    X = np.random.randn(n, d)\n    if rank_deficient:\n        # Induce rank deficiency by making two columns identical.\n        X[:, -1] = X[:, -2]\n\n    # Sample a ground-truth parameter vector\n    w_star = np.random.randn(d, 1)\n\n    # Define logistic function with clipping for numerical stability\n    def sigma(t):\n        t = np.clip(t, -500, 500)\n        return 1 / (1 + np.exp(-t))\n\n    # Compute true probabilities and sample binary labels\n    z_true = X @ w_star\n    p_true = sigma(z_true)\n    y = (np.random.rand(n, 1)  p_true).astype(float)\n\n    # 2. Run Gradient Descent on the UNREGULARIZED risk R(w)\n    w = np.zeros((d, 1))\n    max_deviations_across_steps = []\n\n    for k in range(K):\n        # 3. At each iterate w_k, compute and analyze Hessians\n        \n        # Predictions at current iterate\n        z_k = X @ w\n        p_k = sigma(z_k)\n\n        # Compute the unregularized Hessian: H_unreg = (1/n) * X^T * S * X\n        # where S is a diagonal matrix with S_ii = p_k_i * (1 - p_k_i)\n        s_k = p_k * (1 - p_k)  # Element-wise product, shape (n, 1)\n        # Efficient computation using broadcasting: (X.T * s_k.T) is shape (d, n)\n        H_unreg = (X.T * s_k.T) @ X / n\n\n        # Compute the regularized Hessian: H_reg = H_unreg + lambda * I\n        H_reg = H_unreg + lambd * np.eye(d)\n\n        # 4. Compute eigendecompositions and find deviations\n        # linalg.eigh returns eigenvalues sorted in ascending order.\n        eigvals_unreg = linalg.eigh(H_unreg, eigvals_only=True)\n        eigvals_reg = linalg.eigh(H_reg, eigvals_only=True)\n        \n        # The theoretical spectral shift is Delta = lambda.\n        Delta = lambd\n        \n        # Deviations from the theoretical shift\n        deviations = np.abs(eigvals_reg - eigvals_unreg - Delta)\n        \n        # Store the maximum deviation for the current step k\n        if deviations.size > 0:\n            max_deviations_across_steps.append(np.max(deviations))\n        else: # Handle d=0 case, though not used in test suite\n            max_deviations_across_steps.append(0.0)\n\n        # 5. Perform GD update using the gradient of the UNREGULARIZED risk\n        grad = X.T @ (p_k - y) / n\n        w = w - eta * grad\n        \n    # Aggregate result: find the maximum deviation over all steps\n    if not max_deviations_across_steps:\n        return 0.0\n    epsilon_max = np.max(max_deviations_across_steps)\n    return epsilon_max\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, d, seed, lambda, K, eta, rank_deficient_flag)\n        (200, 5, 7, 0.05, 8, 0.1, False),   # Case A: happy path, full rank\n        (200, 5, 7, 0.0, 8, 0.1, False),    # Case B: boundary, zero penalty\n        (150, 6, 11, 0.05, 8, 0.1, True),   # Case C: edge, rank-deficient\n        (150, 6, 11, 10.0, 8, 0.1, True),   # Case D: edge, rank-deficient, large lambda\n        (60, 3, 3, 0.2, 10, 0.2, False)     # Case E: small-scale check\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_experiment(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3120548"}, {"introduction": "在高维非凸优化问题中，梯度下降算法常常会陷入鞍点区域，导致训练停滞。本实践将带你从被动观察损失曲面转向主动导航。你将实现一个更智能的优化算法，它利用特征分解来识别海森矩阵的负曲率方向，并沿着这些方向主动“逃离”鞍点，从而深入理解二阶优化方法如何克服普通梯度下降的局限性。[@problem_id:3120493]", "problem": "给定一个确定性的玩具训练目标，它捕捉了深度学习中常见的非凸现象：具有负曲率的驻留鞍点。考虑一个二维损失函数，其参数为 $\\theta = (x,y) \\in \\mathbb{R}^{2}$，定义如下：\n$$\nf(\\theta) = \\frac{1}{4}\\left(x^{4}+y^{4}\\right) - \\frac{1}{2}x^{2} + \\frac{1}{2}y^{2}.\n$$\n该目标函数具有同时包含正曲率和负曲率分量的驻点。您的任务是设计并实现一个程序，在基于梯度的训练过程中追踪 Hessian 矩阵 $H(\\theta)$ 的负特征值 $\\lambda_{i}  0$，并在当一阶信号较弱时，通过沿 $\\lambda_{i}  0$ 对应的特征向量 $v_{i}$ 移动来逃离鞍点行为。\n\n从以下基本概念开始：\n- 梯度 $\\nabla f(\\theta)$ 是 $f$ 关于 $(x,y)$ 的偏导数向量。\n- Hessian 矩阵 $H(\\theta)$ 是由二阶偏导数构成的 $2 \\times 2$ 对称矩阵。\n- $H(\\theta)$ 的一个特征对 $(\\lambda, v)$ 满足 $H(\\theta)\\,v=\\lambda\\,v$，其中 $v \\neq 0$。\n- 标准梯度下降步骤使用 $\\theta_{t+1}=\\theta_{t}-\\alpha\\,\\nabla f(\\theta_{t})$，其中步长 $\\alpha>0$。\n- $f$ 在 $\\theta$ 点附近沿方向 $v$、标量步长为 $\\varepsilon$ 的二阶泰勒展开式为\n$$\nf(\\theta+\\varepsilon v)\\approx f(\\theta)+\\varepsilon\\,\\nabla f(\\theta)^{\\top}v+\\frac{1}{2}\\varepsilon^{2}v^{\\top}H(\\theta)v.\n$$\n当 $v$ 是特征值 $\\lambda0$ 的特征向量，且 $\\|\\nabla f(\\theta)\\|$ 足够小时，二次项起主导作用，一个小的非零 $\\varepsilon$ 可以使 $f$ 减小。\n\n问题要求：\n- 实现一个迭代训练过程，该过程在标准梯度下降和一个基于原理的负曲率逃逸步骤之间交替进行。在迭代次数 $t$ 时，计算 $\\nabla f(\\theta_{t})$ 和 $H(\\theta_{t})$，对 $H(\\theta_{t})$ 进行特征分解，并找出最小的特征值 $\\lambda_{\\min}$ 及其对应的单位特征向量 $v_{\\min}$。\n- 使用一个简单的决策规则：如果 $\\lambda_{\\min}  -\\lambda_{\\mathrm{thresh}}$ 且 $\\|\\nabla f(\\theta_{t})\\| \\le g_{\\mathrm{thresh}}$，则执行一次曲率逃逸 $\\theta_{t+1}=\\theta_{t}+s_{\\mathrm{esc}}\\,\\tilde{v}$，其中 $\\tilde{v}$ 是与 $\\lambda_{\\min}$ 相关联的单位特征向量；否则，执行梯度下降步骤 $\\theta_{t+1}=\\theta_{t}-\\alpha\\,\\nabla f(\\theta_{t})$。\n- 逃逸步长应仅使用从特征分解中获得的量，根据局部曲率的大小进行缩放，并根据二阶泰勒模型选择一个既小又安全的值。您必须确保在需要时通过小的正则化使步长在所有状态下都保持有限。\n- 追踪执行了多少次逃逸步骤。\n\n测试套件和超参数：\n- 所有测试用例使用相同的超参数：步长 $\\alpha = 0.1$，最大迭代次数 $T=200$，梯度范数阈值 $g_{\\mathrm{thresh}}=10^{-6}$，负曲率阈值 $\\lambda_{\\mathrm{thresh}}=10^{-9}$，基准逃逸幅度 $s_{0}=0.1$，以及一个小的曲率正则化项 $\\varepsilon=10^{-12}$（仅在您的逃逸步长缩放中使用，如有）。\n- 测试用例为四个初始值 $\\theta_{0}$：\n    $$\n    \\theta_{0}^{(1)} = [0,0],\\quad\n    \\theta_{0}^{(2)} = [0.2,0],\\quad\n    \\theta_{0}^{(3)} = [0,0.5],\\quad\n    \\theta_{0}^{(4)} = [1.5,0.1].\n    $$\n- 对于每个测试用例，运行迭代过程最多 $T$ 步并报告：\n    $1)$ 整数类型的逃逸次数，\n    $2)$ 布尔值，指示最终的 Hessian 矩阵是否含有负特征值，\n    $3)$ 最终的损失函数值 $f(\\theta_{T})$，为实数，\n    $4)$ 最终的参数向量 $\\theta_{T}$，为实数列表。\n- 舍入：将所有实数值输出四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果是按上述顺序排列的列表。例如：\n$$\n[\\,[\\text{escape\\_count}_{1},\\text{has\\_neg}_{1},f_{1},[\\text{theta}_{1,1},\\text{theta}_{1,2}]],\\dots,[\\text{escape\\_count}_{4},\\text{has\\_neg}_{4},f_{4},[\\text{theta}_{4,1},\\text{theta}_{4,2}]]\\,].\n$$\n本问题不涉及单位。不使用角度。不使用百分比。确保打印的行与指定格式匹配，不含任何额外文本。", "solution": "用户要求设计并实现一个优化过程，用于处理非凸目标函数中的鞍点，这是训练深度学习模型时的一个常见挑战。该问题是良构的，有科学依据，并为完整解决方案提供了所有必要信息。\n\n需要最小化的目标函数由 $f: \\mathbb{R}^{2} \\to \\mathbb{R}$ 给出，其参数为 $\\theta = (x,y)$，定义如下：\n$$\nf(\\theta) = f(x,y) = \\frac{1}{4}\\left(x^{4}+y^{4}\\right) - \\frac{1}{2}x^{2} + \\frac{1}{2}y^{2}\n$$\n我们的任务是实现一个迭代算法，该算法从初始点 $\\theta_0$ 开始，生成一个迭代序列 $\\theta_t$，使其收敛到局部最小值，从而有效逃离遇到的任何鞍点。\n\n首先，我们必须推导损失函数的一阶和二阶导数，它们分别是梯度 $\\nabla f(\\theta)$ 和 Hessian 矩阵 $H(\\theta)$。这些对于指定的优化算法至关重要。\n\n梯度是偏导数向量：\n$$\n\\nabla f(\\theta) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} x^3 - x \\\\ y^3 + y \\end{pmatrix}\n$$\nHessian 矩阵是二阶偏导数矩阵：\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 3x^2 - 1  0 \\\\ 0  3y^2 + 1 \\end{pmatrix}\n$$\nHessian 矩阵是一个对角矩阵，这简化了其分析。它的特征值是对角线元素：$\\lambda_1 = 3x^2 - 1$ 和 $\\lambda_2 = 3y^2 + 1$。对应的特征向量是标准基向量 $v_1 = (1,0)^\\top$ 和 $v_2 = (0,1)^\\top$。\n\n驻点是梯度为零的位置，即 $\\nabla f(\\theta) = 0$。对于我们的函数，这在 $x^3 - x = 0$ 和 $y^3 + y = 0$ 时发生。第一个方程给出 $x \\in \\{-1, 0, 1\\}$，第二个方程要求 $y=0$。因此，驻点是 $(-1,0)$、$(0,0)$ 和 $(1,0)$。\n\n这些驻点的性质由在这些点上求值的 Hessian 矩阵的特征值决定：\n- 在 $\\theta = (\\pm 1, 0)$ 处：$H = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}$。特征值为 $\\lambda_1=2$ 和 $\\lambda_2=1$。两者均为正，因此这些点是局部极小值点。在这些极小值点处的损失为 $f(\\pm 1, 0) = \\frac{1}{4} - \\frac{1}{2} = -0.25$。\n- 在 $\\theta = (0, 0)$ 处：$H = \\begin{pmatrix} -1  0 \\\\ 0  1 \\end{pmatrix}$。特征值为 $\\lambda_1=-1$ 和 $\\lambda_2=1$。由于有一个负特征值和一个正特征值，该点是一个鞍点。负曲率方向沿 x 轴，与特征向量 $v_1=(1,0)^\\top$ 相关联。\n\n所提出的算法是一种混合策略。在每次迭代 $t$ 中，使用当前参数 $\\theta_t$：\n$1.$ 我们计算梯度 $\\nabla f(\\theta_t)$、其范数 $\\|\\nabla f(\\theta_t)\\|$ 和 Hessian 矩阵 $H(\\theta_t)$。\n$2.$ 我们对 $H(\\theta_t)$ 进行特征分解，以找到其最小特征值 $\\lambda_{\\min}$ 和对应的单位特征向量 $v_{\\min}$。\n$3.$ 基于两个阈值做出决策，梯度范数阈值 $g_{\\mathrm{thresh}}$ 和负曲率阈值 $\\lambda_{\\mathrm{thresh}}$。\n\n更新规则如下：\n- **如果 $\\lambda_{\\min}  -\\lambda_{\\mathrm{thresh}}$ 且 $\\|\\nabla f(\\theta_t)\\| \\le g_{\\mathrm{thresh}}$**：此条件表明我们接近一个驻点（弱梯度），该点具有显著的负曲率方向。标准的梯度下降步骤会非常慢。为了逃离鞍点区域，我们执行一个“负曲率逃逸”步骤。此步骤将参数沿对应于负特征值的特征向量方向移动。更新规则是：\n$$\n\\theta_{t+1} = \\theta_t + s_{\\mathrm{esc}} v_{\\min}\n$$\n问题要求设计一个步长 $s_{\\mathrm{esc}}$，该步长根据曲率大小进行缩放。一个受信赖域方法启发的、基于原理的选择是使步长与负曲率的绝对值成反比，同时使用基准幅度 $s_0$ 和一个小的正则化常数 $\\varepsilon$ 来防止除以零。因此，我们将逃逸步长定义为：\n$$\ns_{\\mathrm{esc}} = \\frac{s_0}{|\\lambda_{\\min}| + \\varepsilon}\n$$\n这种设计确保了当曲率接近平坦（$|\\lambda_{\\min}|$ 较小）时步长较大，而当曲率陡峭（$|\\lambda_{\\min}|$ 较大）时步长较小且更安全。方向取自特征分解程序返回的单位特征向量 $v_{\\min}$。\n\n- **否则**：如果梯度足够大，或者没有显著的负曲率，标准的梯度下降更新更有效。此更新为：\n$$\n\\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t)\n$$\n其中 $\\alpha$ 是学习率（步长）。\n\n对每个指定的初始点 $\\theta_0$，重复此迭代过程最多 $T=200$ 次。我们将追踪所执行的逃逸步骤数。在最后一次迭代之后，我们报告所需的指标：总逃逸次数、最终的 Hessian 矩阵是否含有负特征值、最终的损失值以及最终的参数向量，所有实数值均四舍五入到 $6$ 位小数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests a hybrid optimization algorithm that combines\n    gradient descent with a negative curvature escape mechanism to avoid\n    saddle points.\n    \"\"\"\n    # Hyperparameters\n    alpha = 0.1\n    T = 200\n    g_thresh = 1e-6\n    lambda_thresh = 1e-9\n    s0 = 0.1\n    epsilon = 1e-12\n\n    # Test cases: initial parameter vectors theta_0\n    thetas0 = [\n        np.array([0.0, 0.0]),\n        np.array([0.2, 0.0]),\n        np.array([0.0, 0.5]),\n        np.array([1.5, 0.1]),\n    ]\n\n    def f(theta):\n        \"\"\"Computes the loss function f(theta).\"\"\"\n        x, y = theta\n        return 0.25 * (x**4 + y**4) - 0.5 * x**2 + 0.5 * y**2\n\n    def grad_f(theta):\n        \"\"\"Computes the gradient of f(theta).\"\"\"\n        x, y = theta\n        return np.array([x**3 - x, y**3 + y])\n\n    def hess_f(theta):\n        \"\"\"Computes the Hessian of f(theta).\"\"\"\n        x, y = theta\n        # The Hessian is diagonal for this specific problem.\n        return np.array([[3 * x**2 - 1, 0.0], [0.0, 3 * y**2 + 1]])\n\n    all_results = []\n\n    for theta0 in thetas0:\n        theta = theta0.copy()\n        escape_count = 0\n\n        for _ in range(T):\n            # Compute gradient, Hessian, and its eigendecomposition\n            grad = grad_f(theta)\n            hess = hess_f(theta)\n            \n            # np.linalg.eigh is for symmetric matrices and returns sorted eigenvalues\n            eigenvalues, eigenvectors = np.linalg.eigh(hess)\n            lambda_min = eigenvalues[0]\n            v_min = eigenvectors[:, 0]\n            \n            grad_norm = np.linalg.norm(grad)\n\n            # Decision rule for which step to take\n            if lambda_min  -lambda_thresh and grad_norm = g_thresh:\n                # Curvature escape step\n                escape_count += 1\n                \n                # Scaled escape step size as per problem design principles\n                s_esc = s0 / (abs(lambda_min) + epsilon)\n                \n                # Update along the eigenvector of minimum eigenvalue\n                theta = theta + s_esc * v_min\n            else:\n                # Standard gradient descent step\n                theta = theta - alpha * grad\n        \n        # After T iterations, collect and format results\n        final_loss = f(theta)\n        \n        # Check final Hessian for negative eigenvalues\n        final_hess = hess_f(theta)\n        final_eigenvalues, _ = np.linalg.eigh(final_hess)\n        has_neg_eigenvalue = final_eigenvalues[0]  0\n\n        # Round all specified real-valued outputs to 6 decimal places\n        final_loss_rounded = round(final_loss, 6)\n        final_theta_rounded = [round(c, 6) for c in theta]\n        \n        result_for_case = [\n            escape_count,\n            bool(has_neg_eigenvalue), # Cast to standard Python boolean\n            final_loss_rounded,\n            final_theta_rounded\n        ]\n        all_results.append(result_for_case)\n\n    # Manually format the output string to match the exact requirement,\n    # avoiding extra spaces and using standard boolean/list representations.\n    result_strings = []\n    for res in all_results:\n        escape_count, has_neg, loss, theta_vec = res\n        theta_str = f\"[{theta_vec[0]},{theta_vec[1]}]\"\n        has_neg_str = 'True' if has_neg else 'False'\n        res_str = f\"[{escape_count},{has_neg_str},{loss},{theta_str}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3120493"}, {"introduction": "特征分解的应用远不止于优化过程，它还能用于模型分析与压缩。本练习将介绍一种基于曲率重要性的网络剪枝方法。我们将使用瑞利商（Rayleigh quotient），即 $R(v) = \\frac{v^{\\top} H v}{v^{\\top} v}$，这一尺度不变的曲率度量来判断哪些参数方向对模型功能贡献较小并将其移除，从而将特征分解的概念直接与提升模型效率的实际任务联系起来。[@problem_id:3120472]", "problem": "给定一个对称实矩阵，它表示深度学习中使用的局部二次近似中一个二阶可微损失函数的海森矩阵(Hessian)。考虑一个非零的参数方向向量以及它在该方向上的相关曲率。你的任务是从第一性原理出发，推导出一个尺度不变的方向曲率度量，将其与特征分解联系起来，并用它来设计一个剪枝规则。\n\n从以下基本依据开始：\n- 对于一个二阶可微的标量损失函数，在某点周围的二阶泰勒展开表明，对于在方向 $\\mathbf{v}$ 上大小为 $\\alpha$ 的一小步，其二次变化由二次型 $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$ 控制，其中 $\\mathbf{H}$ 是在展开点处的对称海森矩阵。\n- 在某点处，一个二阶可微标量损失的海森矩阵 $\\mathbf{H}$ 是一个实对称矩阵，因此它允许使用标准正交的特征基和实数特征值进行实数特征分解。\n\n你的程序必须：\n- 从二次型构造一个尺度不变的方向曲率度量，并将其用作剪枝的曲率重要性分数。在你的方法设计中，从第一性原理明确证明其尺度不变性。\n- 实现剪枝规则：当且仅当一个方向的曲率重要性分数小于或等于给定的阈值 $t$ 时，剪枝该方向。为了数值鲁棒性，如果对于一个预设的 $\\varepsilon$，有 $\\mathbf{v}^{\\top}\\mathbf{v}  \\varepsilon$，则按惯例将曲率重要性分数定义为 $0$。\n- 使用特征分解计算 $\\mathbf{H}$ 的最小和最大特征值，并对每个给定的非零方向进行验证，确保所实现的曲率重要性分数位于从最小特征值到最大特征值的闭区间内。在检查区间成员关系时，使用数值容差 $\\tau$，如果一个分数 $r$ 满足 $\\lambda_{\\min} - \\tau \\le r \\le \\lambda_{\\max} + \\tau$，则视其在界限内。\n\n测试套件和要求输出：\n在以下测试案例上实现你的解决方案。在每个案例中，你将得到一个对称矩阵 $\\mathbf{H}$、一组方向向量、一个阈值 $t$、一个小范数截断值 $\\varepsilon$ 和一个边界检查容差 $\\tau$。\n\n- 案例 $1$（正常路径，正定）：\n  - $\\mathbf{H}_1 = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$。\n  - 方向：$\\mathbf{v}_{1,1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{v}_{1,2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$，$\\mathbf{v}_{1,3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n  - 阈值：$t_1 = 3.5$。\n  - 小范数截断值：$\\varepsilon = 10^{-12}$。\n  - 边界容差：$\\tau = 10^{-10}$。\n\n- 案例 $2$（边界相等，重复特征值）：\n  - $\\mathbf{H}_2 = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix}$。\n  - 方向：$\\mathbf{v}_{2,1} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$，$\\mathbf{v}_{2,2} = \\begin{bmatrix} -3 \\\\ 4 \\end{bmatrix}$。\n  - 阈值：$t_2 = 2$。\n  - 小范数截断值：$\\varepsilon = 10^{-12}$。\n  - 边界容差：$\\tau = 10^{-10}$。\n\n- 案例 $3$（半正定，带零空间和近零向量）：\n  - $\\mathbf{H}_3 = \\begin{bmatrix} 1  0  0 \\\\ 0  0  0 \\\\ 0  0  3 \\end{bmatrix}$。\n  - 方向：$\\mathbf{v}_{3,1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{v}_{3,2} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$，$\\mathbf{v}_{3,3} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{v}_{3,4} = \\begin{bmatrix} 10^{-13} \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n  - 阈值：$t_3 = 0.5$。\n  - 小范数截断值：$\\varepsilon = 10^{-12}$。\n  - 边界容差：$\\tau = 10^{-10}$。\n\n- 案例 $4$（不定曲率，鞍点状）：\n  - $\\mathbf{H}_4 = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}$。\n  - 方向：$\\mathbf{v}_{4,1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，$\\mathbf{v}_{4,2} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$，$\\mathbf{v}_{4,3} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n  - 阈值：$t_4 = -0.5$。\n  - 小范数截断值：$\\varepsilon = 10^{-12}$。\n  - 边界容差：$\\tau = 10^{-10}$。\n\n对于每个案例 $k \\in \\{1,2,3,4\\}$，你的程序必须计算：\n- 在“当且仅当曲率重要性 $\\le t_k$ 时剪枝”规则下，被剪枝方向的整数计数 $c_k$。\n- 一个布尔标志 $b_k$，当且仅当该案例中计算出的每个曲率重要性分数都在容差 $\\tau$ 范围内位于特征值区间 $[\\lambda_{\\min}(\\mathbf{H}_k), \\lambda_{\\max}(\\mathbf{H}_k)]$ 内时，该标志为真。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按以下顺序排列：\n$[c_1, b_1, c_2, b_2, c_3, b_3, c_4, b_4]$。", "solution": "该问题是有效的。它在科学上植根于多元微积分和线性代数的原理，并应用于深度学习中的优化，特别涉及到通过海森矩阵分析损失函数曲面。该问题是适定的、客观的，并包含了推导出唯一、可验证解所需的所有信息。\n\n核心任务是开发并应用一个尺度不变的方向曲率度量。对于从点 $\\mathbf{w}_0$ 出发，在方向 $\\mathbf{v}$ 上大小为 $\\alpha$ 的一小步，损失函数 $L$ 的变化由二阶泰勒展开近似：\n$$ L(\\mathbf{w}_0 + \\alpha\\mathbf{v}) \\approx L(\\mathbf{w}_0) + \\alpha \\nabla L(\\mathbf{w}_0)^{\\top}\\mathbf{v} + \\frac{\\alpha^2}{2} \\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v} $$\n在梯度 $\\nabla L(\\mathbf{w}_0)$ 为零的临界点，损失的局部变化由二次型 $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$ 决定，其中 $\\mathbf{H}$ 是在 $\\mathbf{w}_0$ 处计算的海森矩阵。该项描述了损失曲面在方向 $\\mathbf{v}$ 上的曲率。\n\n**1. 尺度不变曲率重要性分数的推导**\n\n二次型 $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$ 本身并不是一个合适的方向曲率度量，因为它依赖于方向向量 $\\mathbf{v}$ 的模长。如果我们用一个非零常数 $k$ 来缩放该向量，二次型会发生改变：\n$$ (k\\mathbf{v})^{\\top}\\mathbf{H}(k\\mathbf{v}) = k^2(\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}) $$\n该度量随 $k^2$ 缩放，这意味着它反映的是向量 $\\mathbf{v}$ 的“长度”，而不仅仅是其方向。为了创建一个仅与方向本身内在相关的度量，我们必须对这个量进行归一化。一个自然的选择是用向量的欧几里得范数的平方 $\\|\\mathbf{v}\\|^2 = \\mathbf{v}^{\\top}\\mathbf{v}$ 进行归一化。这就引出了**瑞利商 (Rayleigh quotient)**，我们将其定义为曲率重要性分数 $s(\\mathbf{v})$：\n$$ s(\\mathbf{v}) = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} $$\n对于任何非零标量 $k$，该分数是尺度不变的：\n$$ s(k\\mathbf{v}) = \\frac{(k\\mathbf{v})^{\\top}\\mathbf{H}(k\\mathbf{v})}{(k\\mathbf{v})^{\\top}(k\\mathbf{v})} = \\frac{k^2(\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v})}{k^2(\\mathbf{v}^{\\top}\\mathbf{v})} = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} = s(\\mathbf{v}) $$\n这个性质使得 $s(\\mathbf{v})$ 成为一个纯粹衡量方向 $\\mathbf{v}$ 上曲率的度量，与其模长无关。为了数值稳定性，根据问题陈述，如果范数的平方 $\\mathbf{v}^{\\top}\\mathbf{v}$ 小于一个小的正常数 $\\varepsilon$，我们将分数定义为 $0$。这个惯例将模长可忽略的方向视为具有零曲率重要性。\n\n**2. 与特征分解的联系及有界性**\n\n给定的海森矩阵 $\\mathbf{H}$ 是实对称矩阵。此类矩阵的一个基本性质是它们可以正交对角化，并且具有实数特征值。瑞利-里兹定理 (Rayleigh-Ritz theorem) 建立了瑞利商与 $\\mathbf{H}$ 的特征值之间的直接联系。该定理指出，对于任何非零向量 $\\mathbf{v}$，瑞利商的值都受 $\\mathbf{H}$ 的最小和最大特征值的限制：\n$$ \\lambda_{\\min}(\\mathbf{H}) \\le s(\\mathbf{v}) = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} \\le \\lambda_{\\max}(\\mathbf{H}) $$\n当 $\\mathbf{v}$ 是对应于最小特征值的特征向量时，达到最小值 $\\lambda_{\\min}(\\mathbf{H})$。类似地，当 $\\mathbf{v}$ 是对应于最大特征值的特征向量时，达到最大值 $\\lambda_{\\max}(\\mathbf{H})$。这个定理为分析可能的曲率范围提供了一个强大的工具，并构成了问题所要求的验证步骤的基础。对于每个计算出的分数 $s$，我们必须验证它位于区间 $[\\lambda_{\\min}(\\mathbf{H}) - \\tau, \\lambda_{\\max}(\\mathbf{H}) + \\tau]$ 内，其中 $\\tau$ 是给定的数值容差。\n\n**3. 剪枝规则与验证程序**\n\n对每个测试案例实现的程序遵循以下逻辑：\n\n**对于每个具有海森矩阵 $\\mathbf{H}_k$、方向 $\\{\\mathbf{v}_{k,i}\\}$、阈值 $t_k$、截断值 $\\varepsilon$ 和容差 $\\tau$ 的案例 $k$：**\n1.  初始化一个剪枝方向的计数器 $c_k = 0$，以及一个用于边界验证的布尔标志 $b_k = \\text{true}$。\n2.  对对称矩阵 $\\mathbf{H}_k$ 执行特征分解，以找到其实数特征值。确定最小特征值 $\\lambda_{\\min}$ 和最大特征值 $\\lambda_{\\max}$。\n3.  对于每个方向向量 $\\mathbf{v}_{k,i}$：\n    a. 计算范数的平方 $n_i = \\mathbf{v}_{k,i}^{\\top}\\mathbf{v}_{k,i}$。\n    b. 计算曲率重要性分数 $s_{k,i}$。如果 $n_i  \\varepsilon$，则设置 $s_{k,i} = 0$。否则，计算 $s_{k,i} = (\\mathbf{v}_{k,i}^{\\top}\\mathbf{H}_k\\mathbf{v}_{k,i}) / n_i$。\n    c. 应用剪枝规则：如果 $s_{k,i} \\le t_k$，则增加剪枝计数 $c_k$。\n    d. 根据谱界验证分数：如果条件 $\\lambda_{\\min} - \\tau \\le s_{k,i} \\le \\lambda_{\\max} + \\tau$ 不成立，则设置 $b_k = \\text{false}$。\n4.  该案例的最终结果是数对 $(c_k, b_k)$。\n\n此过程系统地应用于所有给定的测试案例。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by processing each test case according to the\n    derived methodology for curvature calculation, pruning, and verification.\n    \"\"\"\n\n    def process_case(H, directions, t, eps, tau):\n        \"\"\"\n        Calculates the number of pruned directions and verifies that all curvature\n        scores lie within the eigenvalue bounds for a single test case.\n\n        Args:\n            H (np.ndarray): The symmetric Hessian matrix.\n            directions (list of np.ndarray): A list of direction vectors.\n            t (float): The pruning threshold.\n            eps (float): The small-norm cutoff for vectors.\n            tau (float): The tolerance for eigenvalue bound checking.\n\n        Returns:\n            tuple: A tuple (pruned_count, all_scores_in_bounds) containing\n                   the integer count of pruned directions and a boolean flag.\n        \"\"\"\n        pruned_count = 0\n        all_scores_in_bounds = True\n\n        # For a real symmetric matrix, eigh is preferred.\n        # It returns sorted eigenvalues and is numerically stable.\n        eigenvalues = np.linalg.eigh(H)[0]\n        lambda_min = eigenvalues[0]\n        lambda_max = eigenvalues[-1]\n\n        for v in directions:\n            v_dot_v = v.T @ v\n            \n            # Use single element from 1x1 array if necessary\n            if isinstance(v_dot_v, np.ndarray):\n                v_dot_v = v_dot_v.item()\n\n            score = 0.0\n            if v_dot_v >= eps:\n                # Rayleigh quotient calculation\n                numerator = v.T @ H @ v\n                if isinstance(numerator, np.ndarray):\n                    numerator = numerator.item()\n                score = numerator / v_dot_v\n\n            # Apply pruning rule\n            if score = t:\n                pruned_count += 1\n\n            # Verify score is within spectral bounds (Rayleigh-Ritz theorem)\n            if not (lambda_min - tau = score = lambda_max + tau):\n                all_scores_in_bounds = False\n\n        return pruned_count, all_scores_in_bounds\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, positive definite)\n        {\n            \"H\": np.array([[4, 1], [1, 3]]),\n            \"directions\": [np.array([[1], [0]]), np.array([[0], [1]]), np.array([[1], [1]])],\n            \"t\": 3.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 2 (boundary equality, repeated eigenvalue)\n        {\n            \"H\": np.array([[2, 0], [0, 2]]),\n            \"directions\": [np.array([[1], [2]]), np.array([[-3], [4]])],\n            \"t\": 2.0,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 3 (positive semidefinite with a nullspace and a near-zero vector)\n        {\n            \"H\": np.array([[1, 0, 0], [0, 0, 0], [0, 0, 3]]),\n            \"directions\": [\n                np.array([[0], [1], [0]]),\n                np.array([[1], [0], [0]]),\n                np.array([[1], [1], [0]]),\n                np.array([[1e-13], [0], [0]])\n            ],\n            \"t\": 0.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 4 (indefinite curvature, saddle-like)\n        {\n            \"H\": np.array([[0, 1], [1, 0]]),\n            \"directions\": [np.array([[1], [1]]), np.array([[1], [-1]]), np.array([[1], [0]])],\n            \"t\": -0.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        c, b = process_case(case[\"H\"], case[\"directions\"], case[\"t\"], case[\"eps\"], case[\"tau\"])\n        results.extend([c, b])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3120472"}]}