{"hands_on_practices": [{"introduction": "好的表征的核心目标之一是将数据转换为一种新形式，使得一个简单的模型（如线性分类器）能够解决一个复杂的任务。这个练习 [@problem_id:3108536] 提供了一个具体的例子，通过构建一个合成的分层标签任务来探索这个概念。你将亲手验证，一个精心设计的“深度”表征如何使经典的、非线性的异或（XOR）问题变得线性可分，而这在原始输入空间中是不可能的。这个实践将加深你对线性可分性以及表征如何揭示数据中不同粒度结构（粗粒度和细粒度标签）的理解。", "problem": "给定一个针对二维输入的合成二元分层标签方案和两个层次的表征。目标是评估一个给定的表征是否能在浅层为粗粒度标签产生线性可分的类别，并仅在深层为细粒度标签产生线性可分的类别。请使用以下基本原理：线性可分性的定义、特征表征作为确定性映射的概念，以及通过线性不等式组对线性可分性进行的可行性解释。\n\n定义：\n- 一个输入数据集 $\\{x_i\\}_{i=1}^N$（其中 $x_i \\in \\mathbb{R}^2$）在特征表征 $z = \\phi(x)$ 下，对于二元标签 $y_i \\in \\{-1,+1\\}$ 是线性可分的，如果存在一个权重向量 $w \\in \\mathbb{R}^d$ 和一个偏置 $b \\in \\mathbb{R}$，使得对于所有 $i \\in \\{1,\\dots,N\\}$，都有 $y_i (w^\\top z_i + b) > 0$ 成立，其中 $z_i = \\phi(x_i)$。\n- 粗粒度标签定义为 $y_i^{\\text{coarse}} = \\operatorname{sign}(x_{i,1})$，其中 $x_{i,1}$ 是 $x_i$ 的第一个坐标，并且如果 $u \\ge 0$ 则 $\\operatorname{sign}(u) = +1$，否则为 $-1$。\n- 细粒度标签定义为 $y_i^{\\text{fine}} = \\operatorname{sign}(x_{i,1} \\cdot x_{i,2})$，当在原始输入空间中观察时，这会产生经典的异或（exclusive-or）结构。\n- 浅层表征是 $\\phi_1(x) = x$（恒等映射）。\n- 深层表征是 $\\phi_2(x) = [x_1, x_2, s \\cdot x_1 x_2]$，其中 $s \\in \\{0,1\\}$ 控制交互特征 $x_1 x_2$ 是否存在（$s=1$）或不存在（$s=0$）。\n\n从基本原理出发的评估程序：\n- 对于给定的表征 $\\phi$ 和二元标签 $\\{y_i\\}$，通过求解线性不等式组 $y_i (w^\\top z_i + b) \\ge 1$（对所有 $i$ 成立）来检查线性可分性，其中变量 $(w,b)$ 在一个箱型区域内有界，以确保有界的可行性搜索。如果该系统在该箱型区域内是可行的，则该数据集在表征 $\\phi$ 下被认为是线性可分的。这通过一个正间隔来编码严格不等式，并使用一个有界搜索区域以避免可行性问题中的无界性。\n\n数据生成：\n- 对每个测试用例，独立生成 $N$ 个点，其中 $x_i$ 从 $[-1,1]^2$ 中均匀抽取。添加独立的高斯噪声 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)$ 以获得 $\\tilde{x}_i = x_i + \\epsilon_i$，其中 $\\sigma \\ge 0$ 是标准差， $I$ 是单位矩阵。标签根据 $\\tilde{x}_i$ 计算。可选地，可以翻转比例为 $p \\in [0,1]$ 的随机标签以模拟标签噪声。所有随机性必须是可复现的。\n\n在有界域中将线性可分性检查作为可行性问题：\n- 构造约束 $y_i (w^\\top z_i + b) \\ge 1$（对所有 $i$ 成立）。等价地，$-y_i (w^\\top z_i + b) \\le -1$。对所有坐标 $j$ 使用箱式约束 $-L \\le w_j \\le L$ 和 $-L \\le b \\le L$，其中 $L > 0$ 是一个大的边界值（例如，$L = 10^6$），以确保如果存在一个具有合理尺度的分离超平面，可行域能够捕捉到它的存在。\n\n您的程序必须：\n- 实现数据生成、分层标签、针对 $\\phi_1$ 和 $\\phi_2$ 的特征映射，以及通过带箱式边界的线性不等式的可行性来进行线性可分性检查。\n- 对每个测试用例，评估四个布尔值：\n  1. $B_{\\text{coarse},1}$：在 $\\phi_1$ 下粗粒度标签的线性可分性，\n  2. $B_{\\text{fine},1}$：在 $\\phi_1$ 下细粒度标签的线性可分性，\n  3. $B_{\\text{coarse},2}$：在 $\\phi_2$ 下粗粒度标签的线性可分性，\n  4. $B_{\\text{fine},2}$：在 $\\phi_2$ 下细粒度标签的线性可分性。\n- 将所有测试用例的结果汇总到单行输出中，该输出包含一个 Python 风格的列表的列表，每个内部列表按上述顺序对应一个测试用例。\n\n测试套件（请精确使用这些参数集，并使用固定的随机种子以保证可复现性）：\n- 测试用例 1（理想路径）：$N = 200$, $\\sigma = 0.05$, $s = 1$, $p = 0$, $L = 10^6$。\n- 测试用例 2（深层特征不足）：$N = 200$, $\\sigma = 0.05$, $s = 0$, $p = 0$, $L = 10^6$。\n- 测试用例 3（标签噪声边缘情况）：$N = 200$, $\\sigma = 0.05$, $s = 1$, $p = 0.3$, $L = 10^6$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个以逗号分隔的 Python 列表的列表形式的结果，每个内部列表中的布尔值严格按照 $\\left[B_{\\text{coarse},1}, B_{\\text{fine},1}, B_{\\text{coarse},2}, B_{\\text{fine},2}\\right]$ 的顺序排列。例如，如果有三个测试用例，则打印类似 $\\left[[\\text{True},\\text{False},\\text{True},\\text{True}],[\\dots],[\\dots]\\right]$ 的内容。", "solution": "问题陈述已被评估并确定为**有效**。它在机器学习的基本原理，特别是表征学习和线性可分性方面，具有科学依据。该问题是适定的，所有必要的数据、定义和程序都已明确指定。语言客观且正式，提出了一个与指定主题相关的、可解的、非平凡的任务。\n\n解决方案基于问题中概述的原理，按以下步骤进行。\n\n### 1. 数据生成和标签分配\n对于每个测试用例，都会生成一个包含 $N$ 个点的数据集。该过程包括三个步骤：首先，从二维空间 $[-1,1]^2$ 的均匀分布中独立采样 $N$ 个特征向量 $\\{x_i\\}_{i=1}^N$，即 $x_i \\sim U([-1,1]^2)$。其次，为了模拟真实世界数据的不完美性，为每个点添加独立同分布的高斯噪声。带噪声的数据点为 $\\tilde{x}_i = x_i + \\epsilon_i$，其中噪声向量 $\\epsilon_i$ 从零均值二元正态分布中抽取，该分布具有对角协方差矩阵，即 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)$。此处，$I$ 是 $2 \\times 2$ 单位矩阵，$\\sigma$ 是噪声标准差。第三，根据带噪声的数据 $\\tilde{x}_i = [\\tilde{x}_{i,1}, \\tilde{x}_{i,2}]^\\top$ 计算两组二元标签（粗粒度和细粒度）：\n- **粗粒度标签**：$y_i^{\\text{coarse}} = \\operatorname{sign}(\\tilde{x}_{i,1})$。决策边界是垂直线 $\\tilde{x}_{i,1} = 0$。带有这些标签的数据集在输入空间中是固有线性可分的。符号函数定义为：当 $u \\ge 0$ 时 $\\operatorname{sign}(u) = +1$，否则为 $-1$。\n- **细粒度标签**：$y_i^{\\text{fine}} = \\operatorname{sign}(\\tilde{x}_{i,1} \\cdot \\tilde{x}_{i,2})$。这对应于经典的异或（XOR）问题，其中第一和第三象限的点被标记为 $+1$，而第二和第四象限的点被标记为 $-1$。这种标签方案在输入空间 $\\mathbb{R}^2$ 中不是线性可分的。\n\n最后，为了模拟标签噪声，随机选择比例为 $p$ 的数据点，并将其粗粒度和细粒度标签都翻转（即 $y_i \\to -y_i$）。所有随机过程（数据生成、噪声添加和标签翻转）都设置了种子以确保可复现性。\n\n### 2. 特征表征\n该问题评估了两种不同的特征表征 $\\phi_1$ 和 $\\phi_2$，它们将输入数据 $\\tilde{x} \\in \\mathbb{R}^2$ 映射到一个新的特征空间。\n- **浅层表征**：$\\phi_1(\\tilde{x}) = \\tilde{x}$。这是一个恒等映射，因此特征空间与输入空间相同，均为 $\\mathbb{R}^2$。\n- **深层表征**：$\\phi_2(\\tilde{x}) = [\\tilde{x}_1, \\tilde{x}_2, s \\cdot \\tilde{x}_1 \\tilde{x}_2]^\\top$。该映射通过一个新特征来增强输入特征，这个新特征是原始两个特征的乘积，由参数 $s \\in \\{0, 1\\}$ 控制。当 $s=1$ 时，这个新特征旨在使异或问题变得线性可分。特征空间是 $\\mathbb{R}^3$。\n\n因此，对于每个点 $\\tilde{x}_i$，我们获得两个特征向量，$z_{i,1} = \\phi_1(\\tilde{x}_i)$ 和 $z_{i,2} = \\phi_2(\\tilde{x}_i)$。\n\n### 3. 线性可分性的验证\n任务的核心是确定一个给定的带标签的特征向量集 $\\{(z_i, y_i)\\}_{i=1}^N$ 是否线性可分。根据所提供的定义，如果存在一个权重向量 $w \\in \\mathbb{R}^d$（其中 $d$ 是特征空间的维度）和一个偏置项 $b \\in \\mathbb{R}$，使得严格不等式 $y_i (w^\\top z_i + b) > 0$ 对所有 $i=1, \\dots, N$ 都成立，那么该集合就是线性可分的。\n\n为了使这个条件在计算上可验证，通过引入一个间隔来加强该条件，从而得到包含 $N$ 个线性不等式的系统：\n$$y_i (w^\\top z_i + b) \\ge 1, \\quad \\forall i \\in \\{1, \\dots, N\\}$$\n该系统存在解 $(w, b)$ 等价于存在一个分离超平面。这是一个线性不等式组的可行性问题。为了解决它，我们使用线性规划。该问题可以写成许多求解器所要求的标准形式 $A_{\\text{ub}} x \\le b_{\\text{ub}}$。我们的未知数是向量 $x = [w_1, \\dots, w_d, b]^\\top \\in \\mathbb{R}^{d+1}$。每个不等式 $y_i (w^\\top z_i + b) \\ge 1$ 被重写为：\n$$-y_i (w^\\top z_i + b) \\le -1$$\n$$-y_i z_i^\\top w - y_i b \\le -1$$\n这定义了约束矩阵 $A_{\\text{ub}}$ 的第 $i$ 行和向量 $b_{\\text{ub}}$ 的第 $i$ 个元素。$A_{\\text{ub}}$ 的第 $i$ 行是 $[-y_i z_{i,1}, \\dots, -y_i z_{i,d}, -y_i]$，$b_{\\text{ub}}$ 的第 $i$ 个元素是 $-1$。\n\n问题还指定了对变量的箱式约束，$-L \\le w_j \\le L$ 和 $-L \\le b \\le L$，以确保对可行解的搜索在一个有界区域内进行。\n\n为了检查可行性，我们可以求解一个目标函数为零（$c = 0$）的线性规划问题。如果求解器找到了一个可行解，那么该数据集在给定的表征下就是线性可分的。对每个测试用例的四种场景执行此检查：\n1.  $B_{\\text{coarse},1}$：粗粒度标签，浅层表征 $\\phi_1$。\n2.  $B_{\\text{fine},1}$：细粒度标签，浅层表征 $\\phi_1$。\n3.  $B_{\\text{coarse},2}$：粗粒度标签，深层表征 $\\phi_2$。\n4.  $B_{\\text{fine},2}$：细粒度标签，深层表征 $\\phi_2$。\n\n实现利用 `numpy` 进行数值计算，并使用 `scipy.optimize.linprog` 来解决线性可行性问题。该函数返回的成功状态直接表明相应的数据集和表征是否是线性可分的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the entire script.\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, sigma, s, p, L)\n        (200, 0.05, 1, 0.0, 1e6), # Test case 1\n        (200, 0.05, 0, 0.0, 1e6), # Test case 2\n        (200, 0.05, 1, 0.3, 1e6), # Test case 3\n    ]\n\n    all_results = []\n    for params in test_cases:\n        case_results = evaluate_case(*params)\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is converted to a compact\n    # form without spaces to match the specified output style.\n    print(str(all_results).replace(\" \", \"\"))\n\ndef evaluate_case(N, sigma, s, p, L):\n    \"\"\"\n    Evaluates a single test case for the four specified conditions.\n\n    Args:\n        N (int): Number of data points.\n        sigma (float): Standard deviation of Gaussian noise.\n        s (int): Switch for the interaction feature in phi_2 (0 or 1).\n        p (float): Fraction of labels to flip.\n        L (float): Bound for the linear programming variables.\n\n    Returns:\n        list: A list of four booleans [B_coarse,1, B_fine,1, B_coarse,2, B_fine,2].\n    \"\"\"\n    # 1. Generate data\n    x_clean = np.random.uniform(-1, 1, size=(N, 2))\n    noise = np.random.normal(0, sigma, size=(N, 2))\n    xtilde = x_clean + noise\n\n    # 2. Compute hierarchical labels based on noisy data\n    # Coarse labels: sign(x_1)\n    y_coarse = np.ones(N)\n    y_coarse[xtilde[:, 0]  0] = -1\n\n    # Fine-grained labels: sign(x_1 * x_2)\n    y_fine = np.ones(N)\n    y_fine[(xtilde[:, 0] * xtilde[:, 1])  0] = -1\n\n    # 3. Apply label noise by flipping a fraction p of labels\n    if p > 0:\n        num_flips = int(p * N)\n        if num_flips > 0:\n            flip_indices = np.random.choice(N, size=num_flips, replace=False)\n            y_coarse[flip_indices] *= -1\n            y_fine[flip_indices] *= -1\n\n    # 4. Generate feature representations\n    # Shallow representation phi_1(x) = x\n    z1 = xtilde\n\n    # Deep representation phi_2(x) = [x1, x2, s*x1*x2]\n    z2 = np.c_[xtilde, s * xtilde[:, 0] * xtilde[:, 1]]\n\n    # 5. Evaluate linear separability for the four scenarios\n    B_coarse1 = check_linear_separability(z1, y_coarse, L)\n    B_fine1 = check_linear_separability(z1, y_fine, L)\n    B_coarse2 = check_linear_separability(z2, y_coarse, L)\n    B_fine2 = check_linear_separability(z2, y_fine, L)\n\n    return [B_coarse1, B_fine1, B_coarse2, B_fine2]\n\ndef check_linear_separability(z, y, L):\n    \"\"\"\n    Checks if a dataset is linearly separable by solving a feasibility problem.\n\n    Args:\n        z (np.ndarray): Feature vectors, shape (N, d).\n        y (np.ndarray): Labels, shape (N,).\n        L (float): Bound for weights and bias.\n\n    Returns:\n        bool: True if the dataset is linearly separable, False otherwise.\n    \"\"\"\n    N, d = z.shape\n\n    # We are solving a feasibility problem, so the objective function is zero.\n    c = np.zeros(d + 1)\n\n    # The constraints are y_i * (w^T z_i + b) >= 1, which are rewritten as\n    # -y_i * z_i^T w - y_i * b = -1 for the solver.\n    # A_ub has shape (N, d+1)\n    A_ub = -np.c_[z * y[:, np.newaxis], y]\n    \n    # b_ub has shape (N,)\n    b_ub = -np.ones(N)\n\n    # Box constraints for all variables (d weights and 1 bias)\n    bounds = [(-L, L)] * (d + 1)\n\n    # Use scipy.optimize.linprog to find if a feasible solution exists.\n    # method='highs' is robust and is the default in recent scipy versions.\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n\n    # res.success is True if the optimizer found an optimal (and thus feasible) solution.\n    return res.success\n\n# Execute the main function when the script is run.\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3108536"}, {"introduction": "在学习了如何通过特征变换简化任务之后，我们进一步探索如何利用数据本身的内在结构来改善表征。这个练习 [@problem_id:3108447] 介绍了一种基于图的方法，即拉普拉斯正则化。你将首先构建一个$k$-近邻图来捕捉数据点之间的局部邻域关系，然后通过一个平滑惩罚项来优化表征，使得相邻的点在新的表征空间中也相互靠近。这个过程不仅能让你推导出优化问题的解析解，还能通过聚类纯度的变化直观地看到表征平滑对下游任务性能的提升。", "problem": "给定一组点 $x_1,\\dots,x_N \\in \\mathbb{R}^d$，这些点被组织成 $K$ 个真实簇。考虑通过在无向邻接图上惩罚相邻表征之间的差异来学习一个平滑表征 $z_1,\\dots,z_N \\in \\mathbb{R}^d$。仅从图和欧几里得范数的核心定义出发，构建并实现一个有原则的方法来获得 $Z \\in \\mathbb{R}^{N \\times d}$，以最小化一个平滑增广目标。然后，使用簇纯度评估其对聚类质量的影响。\n\n基本要素：\n- 令 $X \\in \\mathbb{R}^{N \\times d}$ 为数据矩阵，其第 $i$ 行为 $x_i^\\top$。\n- 构建一个无向、无权的 $k$-最近邻 (k-NN) 图，其邻接矩阵为 $A \\in \\mathbb{R}^{N \\times N}$，其中如果 $j$ 是 $i$ 的 $k$ 个最近邻之一或 $i$ 是 $j$ 的 $k$ 个最近邻之一，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。不允许自环，因此对所有 $i$，$A_{ii} = 0$。距离由欧几里得范数度量。\n- 定义度矩阵 $D \\in \\mathbb{R}^{N \\times N}$ 为 $D_{ii} = \\sum_{j=1}^N A_{ij}$，以及组合图拉普拉斯算子 $L \\in \\mathbb{R}^{N \\times N}$ 为 $L = D - A$。\n- 通过对边上的欧几里得距离平方差求和来定义表征平滑惩罚项：$\\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2$，其中 $E$ 是对应于 $A$ 的无向边集。\n\n任务：\n1. 从上述定义和标准梯度演算出发，推导目标函数\n   $$J(Z) = \\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2 + \\lambda \\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2,$$\n   的最小化子 $Z^\\star \\in \\mathbb{R}^{N \\times d}$ 的一阶最优性条件，其中 $\\lambda \\ge 0$ 是一个平滑系数，$Z$ 是将 $z_i$ 按行堆叠而成的矩阵。利用此条件，获得一个用 $X$、$\\lambda$ 和 $L$ 表示的 $Z^\\star$ 的可计算表达式。\n2. 实现一个程序，该程序：\n   - 从 $K=3$ 个大小相等的各向同性高斯簇中生成一个包含 $N=120$ 个点（维度 $d=2$）的合成数据集 $X$，使用中心点 $(-4,-4)$、$(0,5)$ 和 $(5,-2)$，标准差为 $\\sigma = 0.8$。使用固定的随机种子以确保可复现性。\n   - 对指定的 $k$ 使用欧几里得距离构建无向 $k$-最近邻图，其邻接矩阵 $A$ 和拉普拉斯算子 $L$ 如上定义。\n   - 通过求解你推导的一阶最优性条件来计算平滑表征 $Z^\\star$。\n   - 在原始数据 $X$ 和平滑表征 $Z^\\star$ 上都运行 $K$-means 聚类（$K=3$），使用 Lloyd 算法，并采用固定的随机初始化种子和固定的迭代次数。如果在更新过程中有任何簇变为空，则将其质心重新初始化为一个随机数据点。\n   - 计算 $X$ 和 $Z^\\star$ 的簇纯度。对于索引 $\\{1,\\dots,N\\}$ 的一个划分 $\\mathcal{C} = \\{C_1,\\dots,C_K\\}$，相对于真实标签 $\\ell_1,\\dots,\\ell_N$ 的簇纯度为\n     $$\\text{purity}(\\mathcal{C}, \\ell) = \\frac{1}{N} \\sum_{k=1}^K \\max_{c \\in \\{1,\\dots,K\\}} \\left|\\{ i \\in C_k : \\ell_i = c \\} \\right|,$$\n     表示为 $[0,1]$ 范围内的十进制数（而非百分比）。\n   - 对每个测试用例，报告纯度变化 $\\Delta = \\text{purity}(Z^\\star) - \\text{purity}(X)$，结果为一个实数。\n\n测试套件和覆盖范围：\n- 在所有测试用例中使用相同的合成数据集，以分离图参数和平滑处理的影响。\n- 评估以下四个参数集 $(k, \\lambda)$，以覆盖一般情况、边界情况和边缘条件：\n  1. $(k = 5, \\lambda = 0.0)$：无平滑的边界情况。\n  2. $(k = 5, \\lambda = 0.1)$：轻度平滑的通用“理想”情况。\n  3. $(k = 10, \\lambda = 2.0)$：在更密集的邻域上进行强平滑。\n  4. $(k = 0, \\lambda = 5.0)$：图中无边的边缘情况（无论 $\\lambda$ 为何值，均无平滑效果）。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含四个测试用例的纯度变化，形式为一个逗号分隔的列表，并用方括号括起来，每个值四舍五入到三位小数，例如：\n  $$[0.000,0.012,0.034,0.000].$$", "solution": "该问题要求构建并实现一种基于图的表征平滑方法。解决方案需要推导出最小化给定目标函数的最优表征 $Z^\\star$，然后评估这种平滑对聚类性能的影响。在给出解决方案之前，先对问题陈述进行验证。\n\n### 问题验证\n\n**第 1 步：提取给定条件**\n- **数据**：一组点 $x_1, \\dots, x_N \\in \\mathbb{R}^d$，组织成 $K$ 个真实簇。数据矩阵为 $X \\in \\mathbb{R}^{N \\times d}$。\n- **表征**：一个平滑表征 $z_1, \\dots, z_N \\in \\mathbb{R}^d$，其矩阵为 $Z \\in \\mathbb{R}^{N \\times d}$。\n- **图**：一个无向、无权的 $k$-最近邻 (k-NN) 图，其邻接矩阵为 $A \\in \\mathbb{R}^{N \\times N}$，其中如果节点 $j$ 是 $i$ 的 $k$ 个最近邻之一或反之，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。无自环（$A_{ii} = 0$）。\n- **图矩阵**：度矩阵 $D_{ii} = \\sum_{j} A_{ij}$，以及组合图拉普拉斯算子 $L = D - A$。\n- **目标函数**：$J(Z) = \\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2 + \\lambda \\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2$，平滑系数 $\\lambda \\ge 0$。\n- **任务 1**：推导最小化子 $Z^\\star$ 的一阶最优性条件，并找到其可计算的表达式。\n- **任务 2**：实现一个程序，具体规格如下：\n    - **数据集**：$N=120$，$d=2$，$K=3$ 个大小相等的各向同性高斯簇。中心点位于 $(-4,-4)$、$(0,5)$、$(5,-2)$。标准差 $\\sigma=0.8$。使用固定的随机种子生成。\n    - **图构建**：为给定的 $k$ 构建 k-NN 图。\n    - **表征计算**：使用推导的表达式计算 $Z^\\star$。\n    - **聚类**：在 $X$ 和 $Z^\\star$ 上都运行 $K$-means（$K=3$），使用固定的随机初始化种子和固定的迭代次数。空簇将被重新初始化。\n    - **评估**：为两种聚类结果计算簇纯度。纯度公式为 $\\text{purity}(\\mathcal{C}, \\ell) = \\frac{1}{N} \\sum_{k=1}^K \\max_{c \\in \\{1,\\dots,K\\}} \\left|\\{ i \\in C_k : \\ell_i = c \\} \\right|$。\n    - **报告**：纯度变化 $\\Delta = \\text{purity}(Z^\\star) - \\text{purity}(X)$。\n- **测试套件**：评估四组参数 $(k, \\lambda)$：$(5, 0.0)$、$(5, 0.1)$、$(10, 2.0)$、$(0, 5.0)$。\n\n**第 2 步：使用提取的给定条件进行验证**\n- **科学基础**：该问题在图信号处理和流形正则化领域有坚实的理论基础，这是表征学习中的一个关键概念。目标函数是 Tikhonov 正则化的一种形式，其中平滑先验在图上定义。图拉普拉斯算子的使用是该领域的基础。评估方法（K-means、簇纯度）是无监督机器学习中的标准实践。\n- **适定性**：目标函数 $J(Z)$ 是欧几里得范数平方和。第一项 $\\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2$ 对 $Z$ 是严格凸的。第二项，涉及图拉普拉斯算子，也是凸的（因为 $L$ 是半正定的）。一个严格凸函数与一个凸函数的和是严格凸的。因此，$J(Z)$ 有一个唯一的全局最小值。该问题是适定的。\n- **客观性**：问题陈述使用了精确的数学定义和算法步骤。没有歧义、主观性或基于观点的语言。所有参数都明确指定，评估指标也已正式定义。\n\n**第 3 步：结论和行动**\n该问题在科学上是合理的，在数学上是适定的，并且其所有组成部分都已正式定义。因此，它被判定为**有效**。将继续进行求解过程。\n\n### 最优表征 $Z^\\star$ 的推导\n\n目标是找到最小化函数 $J(Z)$ 的表征矩阵 $Z^\\star$：\n$$J(Z) = \\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2 + \\lambda \\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2$$\n我们首先用矩阵表示法来表达 $J(Z)$。第一项是矩阵 $Z$ 和 $X$ 之差的弗罗贝尼乌斯范数平方：\n$$\\sum_{i=1}^N \\lVert z_i - x_i \\rVert^2 = \\lVert Z - X \\rVert_F^2 = \\text{Tr}((Z-X)^\\top(Z-X))$$\n其中 $\\text{Tr}(\\cdot)$ 表示矩阵的迹。\n\n第二项是平滑惩罚项。它可以用图拉普拉斯算子 $L$ 来表示。对无向边 $(i,j) \\in E$ 的求和可以写成对所有有序对 $(i,j)$ 按邻接矩阵 $A$ 加权求和的 $\\frac{1}{2}$：\n$$\\sum_{(i,j)\\in E} \\lVert z_i - z_j \\rVert^2 = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N A_{ij} \\lVert z_i - z_j \\rVert^2$$\n展开范数的平方并对每个维度 $k=1,\\dots,d$ 独立求和，得到了涉及拉普拉斯算子的著名二次型：\n$$\\frac{1}{2} \\sum_{i,j} A_{ij} \\lVert z_i - z_j \\rVert^2 = \\text{Tr}(Z^\\top L Z)$$\n这个恒等式可以通过注意到 $\\text{Tr}(Z^\\top L Z) = \\sum_{k=1}^d Z_{:,k}^\\top L Z_{:,k}$ 来理解，其中 $Z_{:,k}$ 是 $Z$ 的第 $k$ 列，而 $Z_{:,k}^\\top L Z_{:,k}$ 是数据第 $k$ 维的标准拉普拉斯二次型。\n\n结合这些项，目标函数的矩阵形式为：\n$$J(Z) = \\text{Tr}((Z-X)^\\top(Z-X)) + \\lambda \\text{Tr}(Z^\\top L Z)$$\n为了找到最小值，我们计算 $J(Z)$ 关于 $Z$ 的矩阵导数，并将其设为零矩阵。使用标准矩阵微积分恒等式（$\\frac{\\partial}{\\partial A} \\text{Tr}(A^\\top B) = B$ 和 $\\frac{\\partial}{\\partial A} \\text{Tr}(A^\\top C A) = (C+C^\\top)A$），我们得到：\n$$\\frac{\\partial J(Z)}{\\partial Z} = \\frac{\\partial}{\\partial Z} \\text{Tr}(Z^\\top Z - Z^\\top X - X^\\top Z) + \\lambda \\frac{\\partial}{\\partial Z} \\text{Tr}(Z^\\top L Z)$$\n$$\\frac{\\partial J(Z)}{\\partial Z} = (2Z - 2X) + \\lambda (L+L^\\top)Z$$\n由于图是无向的，其邻接矩阵 $A$ 是对称的，因此拉普拉斯算子 $L = D-A$ 也是对称的（$L=L^\\top$）。表达式简化为：\n$$\\frac{\\partial J(Z)}{\\partial Z} = 2(Z - X) + 2\\lambda L Z$$\n将梯度设为零，得到最小化子 $Z^\\star$ 的一阶最优性条件：\n$$2(Z^\\star - X) + 2\\lambda L Z^\\star = 0$$\n$$Z^\\star - X + \\lambda L Z^\\star = 0$$\n$$(I + \\lambda L) Z^\\star = X$$\n其中 $I$ 是 $N \\times N$ 的单位矩阵。这就是所需的一阶条件。\n\n为了得到 $Z^\\star$ 的可计算表达式，我们为 $Z^\\star$ 解这个线性方程组。矩阵 $(I + \\lambda L)$ 是可逆的，因为单位矩阵 $I$ 是正定的，而矩阵 $\\lambda L$ 是半正定的（因为 $\\lambda \\ge 0$ 且 $L$ 是半正定的）。一个正定矩阵和一个半正定矩阵的和是正定的，因此是可逆的。\n因此，唯一的解 $Z^\\star$ 是：\n$$Z^\\star = (I + \\lambda L)^{-1} X$$\n这个表达式提供了一种通过求解线性系统来计算最优平滑表征的方法。\n\n### 实现细节\n\n实现将遵循推导出的公式和问题中概述的步骤。\n1.  生成一个合成数据集 $X$。\n2.  对于每个测试用例 $(k, \\lambda)$：\n    a. 构建对称的 k-NN 邻接矩阵 $A$，并由此计算拉普拉斯算子 $L$。\n    b. 通过求解线性系统 $(I + \\lambda L)Z^\\star = X$ 来计算最优表征 $Z^\\star$。这比计算矩阵逆在数值上更稳定。\n    c. 在原始数据 $X$ 和平滑数据 $Z^\\star$ 上运行一个确定性的 K-means 算法（固定的种子，固定的迭代次数），以获得两组簇分配。\n    d. 相对于真实标签，计算两种分配的簇纯度。\n    e. 记录纯度变化 $\\Delta = \\text{purity}(Z^\\star) - \\text{purity}(X)$。\n3.  最终输出是四个测试用例计算出的 $\\Delta$ 值的列表，按指定格式格式化。\n$\\lambda=0$ 和 $k=0$ 的边缘情况能被正确处理。如果 $\\lambda=0$，方程变为 $I Z^\\star = X$，因此 $Z^\\star=X$。如果 $k=0$，图没有边，$A=0$，$D=0$，$L=0$。方程再次简化为 $I Z^\\star = X$，因此 $Z^\\star=X$。在这两种情况下，纯度变化 $\\Delta$ 必须为 $0.0$。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef run_kmeans(data, K, n_iter, rng):\n    \"\"\"\n    Runs K-means clustering with fixed iterations and random re-initialization \n    for empty clusters.\n    \"\"\"\n    N, d = data.shape\n    \n    # Deterministic initialization of centroids from data points\n    initial_centroid_indices = rng.choice(N, K, replace=False)\n    centroids = data[initial_centroid_indices].copy()\n\n    for _ in range(n_iter):\n        # Assignment step\n        dist_to_centroids = cdist(data, centroids, 'euclidean')\n        labels = np.argmin(dist_to_centroids, axis=1)\n        \n        # Update step\n        new_centroids = np.zeros((K, d))\n        for i in range(K):\n            points_in_cluster = data[labels == i]\n            if len(points_in_cluster) == 0:\n                # Re-initialize empty cluster to a random data point\n                reinit_idx = rng.choice(N)\n                new_centroids[i] = data[reinit_idx]\n            else:\n                new_centroids[i] = np.mean(points_in_cluster, axis=0)\n        centroids = new_centroids\n        \n    # Final assignment\n    dist_to_centroids = cdist(data, centroids, 'euclidean')\n    final_labels = np.argmin(dist_to_centroids, axis=1)\n    \n    return final_labels\n\ndef calculate_purity(y_pred, y_true, N, K):\n    \"\"\"\n    Calculates cluster purity.\n    \"\"\"\n    contingency_matrix = np.zeros((K, K), dtype=int)\n    np.add.at(contingency_matrix, (y_pred, y_true), 1)\n    \n    purity = np.sum(np.max(contingency_matrix, axis=1)) / N\n    return purity\n\ndef solve():\n    # --- Problem constants and parameters ---\n    N = 120\n    d = 2\n    K = 3\n    sigma = 0.8\n    cluster_centers = [np.array([-4, -4]), np.array([0, 5]), np.array([5, -2])]\n    points_per_cluster = N // K\n    \n    DATA_SEED = 42\n    KMEANS_SEED = 123\n    KMEANS_ITER = 20 # A reasonable fixed number of iterations\n\n    test_cases = [\n        (5, 0.0),   # Case 1: boundary case, no smoothing\n        (5, 0.1),   # Case 2: general case\n        (10, 2.0),  # Case 3: strong smoothing, denser graph\n        (0, 5.0),   # Case 4: edge case, no edges\n    ]\n\n    # --- 1. Generate synthetic dataset ---\n    data_rng = np.random.default_rng(DATA_SEED)\n    X_parts = []\n    y_true_parts = []\n    cov = np.eye(d) * (sigma**2)\n    for i in range(K):\n        X_parts.append(data_rng.multivariate_normal(cluster_centers[i], cov, size=points_per_cluster))\n        y_true_parts.append(np.full(points_per_cluster, i))\n        \n    X = np.vstack(X_parts)\n    y_true = np.hstack(y_true_parts)\n\n    results = []\n\n    for k, lam in test_cases:\n        # --- 2. Build graph and compute smoothed representation Z* ---\n        # Handle trivial cases where Z_star = X\n        if k == 0 or lam == 0.0:\n            Z_star = X\n        else:\n            # Construct k-NN graph\n            dist_matrix = cdist(X, X, 'euclidean')\n            \n            # Find k-NN for each point (excluding self)\n            # A more robust way to exclude self is to sort and take indices [1:k+1]\n            A = np.zeros((N, N))\n            sorted_indices = np.argsort(dist_matrix, axis=1)\n            # Get the indices of the k nearest neighbors for each point\n            neighbor_indices = sorted_indices[:, 1:k+1]\n            \n            # Populate A based on one-way neighborhood\n            np.put_along_axis(A, neighbor_indices, 1, axis=1)\n\n            # Symmetrize to make the graph undirected\n            A = np.maximum(A, A.T)\n            \n            # Compute Graph Laplacian L = D - A\n            D = np.diag(np.sum(A, axis=1))\n            L = D - A\n            \n            # Solve for Z* using the derived formula: (I + lambda*L)Z* = X\n            M = np.eye(N) + lam * L\n            Z_star = np.linalg.solve(M, X)\n            \n        # --- 3. Run K-means on X and Z* ---\n        # Use the same seed for fair comparison\n        kmeans_rng_x = np.random.default_rng(KMEANS_SEED)\n        labels_X = run_kmeans(X, K, KMEANS_ITER, kmeans_rng_x)\n        \n        kmeans_rng_z = np.random.default_rng(KMEANS_SEED)\n        labels_Z = run_kmeans(Z_star, K, KMEANS_ITER, kmeans_rng_z)\n\n        # --- 4. Compute purity change ---\n        purity_X = calculate_purity(labels_X, y_true, N, K)\n        purity_Z = calculate_purity(labels_Z, y_true, N, K)\n        \n        delta = purity_Z - purity_X\n        results.append(delta)\n\n    # --- Final output ---\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "3108447"}, {"introduction": "有时，仅仅依赖主任务的目标函数（例如分类损失）可能不足以学习到一个泛化能力强且信息丰富的表征，因为模型可能会“走捷径”，丢弃对主任务非必需但对其他任务有用的信息。这个练习 [@problem_id:3108515] 将向你展示一种强大的技术：使用辅助任务来引导表征学习。你将通过实验比较，在主分类任务之外增加一个预测旋转角度的辅助回归任务，是否能促使模型在其中间表征中编码关于角度的信息。这种方法是多任务学习和自监督学习中的一个核心思想。", "problem": "构建一个完整的、可运行的程序，在一个受控的合成任务上测试：添加一个辅助旋转预测头是否能促进角度信息在潜表征 $z$ 中的编码。该编码效果通过训练后从 $z$ 到角度的线性回归来衡量。您的实验必须基于表示学习和优化的核心定义进行设计和论证，不得依赖任何预计算模型或外部数据。角度必须以弧度为单位。\n\n请从以下普遍接受的基础和定义开始：\n- 经验风险最小化 (ERM)：给定数据 $\\{(x_i,y_i)\\}_{i=1}^N$ 和参数化模型 $f_\\theta$，最小化关于参数 $\\theta$ 的经验损失 $\\frac{1}{N}\\sum_{i=1}^N \\ell(f_\\theta(x_i),y_i)$。\n- 二元交叉熵 (BCE)：用于二元分类，目标 $y\\in\\{0,1\\}$，预测概率 $p\\in(0,1)$：$\\ell_{\\mathrm{BCE}}(p,y) = -\\left[y\\log p + (1-y)\\log(1-p)\\right]$。\n- 均方误差 (MSE)：用于回归，目标 $t\\in\\mathbb{R}^k$，预测 $\\hat{t}\\in\\mathbb{R}^k$：$\\ell_{\\mathrm{MSE}}(\\hat{t},t)=\\frac{1}{2}\\|\\hat{t}-t\\|_2^2$。\n- 线性探查 (Linear probing)：训练一个表征 $z$ 后，拟合一个独立的线性回归器将 $z$ 映射到一个目标量，并在一个留出的测试集上评估其泛化误差。\n\n数据生成。对于每个样本 $i$，抽取一个标量内容变量 $\\alpha_i \\sim \\mathcal{N}(0,1)$ 和一个角度 $\\theta_i \\sim \\mathrm{Uniform}([-\\pi,\\pi])$。构造一个 $3$ 维输入\n$$\nx_i = \\begin{bmatrix} \\alpha_i \\\\ \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix} + \\varepsilon_i,\n$$\n其中噪声 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2 I_3)$，$\\sigma = 0.05$。定义一个二元标签 $y_i = \\mathbb{I}[\\alpha_i \\ge 0]$ 和一个旋转目标向量 $t_i = \\begin{bmatrix} \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix}$。角度以弧度为单位。将数据划分为大小为 $N_{\\mathrm{train}} = 800$ 的训练集和大小为 $N_{\\mathrm{test}} = 200$ 的测试集。\n\n模型。使用一个线性编码器 $f_\\phi(x) = W_e x + b_e$，其潜变量为 $z \\in \\mathbb{R}^{d_z}$。使用一个二元分类器头 $g_w(z) = \\sigma(w_c^\\top z + b_c)$ 来预测 $y$，其中 $\\sigma(\\cdot)$ 是 logistic sigmoid 函数。使用一个辅助旋转头 $h_u(z) = U z + c$ 来预测 $t = [\\cos\\theta,\\sin\\theta]^\\top$。\n\n训练目标。通过最小化主分类损失和加权辅助损失的平均值进行训练：\n$$\n\\mathcal{L}(\\phi,w,u) = \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{BCE}}(g_w(f_\\phi(x_i)),y_i) + \\lambda \\cdot \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{MSE}}(h_u(f_\\phi(x_i)),t_i),\n$$\n其中 $\\lambda \\ge 0$ 控制辅助头的强度。通过批量梯度下降法优化参数 $(\\phi,w,u)$，学习率为 $\\eta$，并使用较小的 $\\ell_2$ 权重衰减 $\\gamma$。\n\n评估协议。训练后，冻结编码器，在训练集上使用岭回归（正则化系数为 $\\rho$）拟合一个从 $z$ 到 $t$ 的线性探针，然后计算测试 MSE：\n$$\n\\mathrm{MSE}_{\\mathrm{probe}} = \\frac{1}{2N_{\\mathrm{test}}}\\sum_{i=1}^{N_{\\mathrm{test}}}\\left\\| \\hat{t}_i - t_i \\right\\|_2^2,\n$$\n其中 $\\hat{t}_i$ 是探针的预测值。同时计算测试分类准确率，定义为在分类器输出上使用阈值 $0.5$ 时正确预测的比例。\n\n实验比较。对于每个测试用例，在相同的数据集和初始化种子上比较两种训练条件：\n- 基线 (Baseline)：$\\lambda = 0$ (无辅助旋转损失)。\n- 辅助 (Auxiliary)：$\\lambda = 1$ (有辅助旋转损失)。\n\n为每个测试用例定义一个布尔结果，当且仅当以下两点同时成立时为真：\n- 辅助条件将角度可解码性提高了至少一个裕度 $\\tau$，即 $\\mathrm{MSE}_{\\mathrm{probe}}^{(\\lambda=0)} - \\mathrm{MSE}_{\\mathrm{probe}}^{(\\lambda=1)} \\ge \\tau$。\n- 辅助条件保持了可接受的主任务性能，即在 $\\lambda=1$ 下的测试准确率至少为 $a_{\\min}$。\n\n超参数。使用以下固定值：$N_{\\mathrm{train}} = 800$，$N_{\\mathrm{test}} = 200$，噪声标准差 $\\sigma = 0.05$，学习率 $\\eta = 0.05$，权重衰减 $\\gamma = 10^{-4}$，训练轮次 $T = 500$，岭回归正则化系数 $\\rho = 10^{-3}$，改进裕度 $\\tau = 0.2$，准确率阈值 $a_{\\min} = 0.9$。\n\n测试套件。运行恰好三个测试用例，每个用例由潜维度和随机种子对 $(d_z,s)$ 指定：\n- 用例 1：$(d_z,s) = (3,0)$，预期具有足够容量来编码内容和角度。\n- 用例 2：$(d_z,s) = (2,1)$，一个容量边界情况，其中编码内容和角度需要压缩。\n- 用例 3：$(d_z,s) = (1,2)$，一个边缘情况，容量不足以完全编码角度。\n\n要求的最终输出。您的程序应生成单行输出，其中包含上述三个用例的三个布尔结果，按顺序排列，格式为一个逗号分隔、无空格的 Python 风格列表，例如“[true1,true2,true3]”，其中每个条目为 True 或 False。必须只打印一行，不允许有其他输出。", "solution": "我们使用表示学习和优化的核心定义，将所提出的问题形式化并加以解决。目标是确定一个辅助旋转预测头是否能促进潜表征 $z$ 以一种在训练后可线性解码的方式来编码角度信息。\n\n基础设置。我们采用经验风险最小化 (ERM) 来训练一个参数化模型。生成的数据集使得主标签 $y$ 仅依赖于一个标量内容变量 $\\alpha$，而输入还包含一个由旋转参数化的分量 $[\\cos\\theta,\\sin\\theta]^\\top$。具体而言，对于每个样本 $i$，我们抽取 $\\alpha_i \\sim \\mathcal{N}(0,1)$ 和 $\\theta_i \\sim \\mathrm{Uniform}([-\\pi,\\pi])$，构建\n$$\nx_i = \\begin{bmatrix} \\alpha_i \\\\ \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix} + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2 I_3),\n$$\n设置 $y_i = \\mathbb{I}[\\alpha_i \\ge 0]$，并定义旋转目标 $t_i = \\begin{bmatrix}\\cos\\theta_i \\\\ \\sin\\theta_i\\end{bmatrix}$。因为 $y_i$ 仅依赖于 $\\alpha_i$，主分类任务对角度 $\\theta_i$ 是不变的，而辅助回归任务仅依赖于 $\\theta_i$。\n\n模型和目标。编码器是线性的：$z = f_\\phi(x) = W_e x + b_e \\in \\mathbb{R}^{d_z}$。分类器头是 $g_w(z) = \\sigma(w_c^\\top z + b_c)$，使用二元交叉熵 (BCE) 进行训练，\n$$\n\\ell_{\\mathrm{BCE}}(p,y) = -\\left[y\\log p + (1-y)\\log(1-p)\\right],\n$$\n这是伯努利模型下的负对数似然。辅助旋转头是 $h_u(z) = U z + c \\in \\mathbb{R}^2$，使用均方误差 (MSE) 进行训练，\n$$\n\\ell_{\\mathrm{MSE}}(\\hat{t},t)=\\frac{1}{2}\\|\\hat{t}-t\\|_2^2.\n$$\n在训练集上平均的总损失为\n$$\n\\mathcal{L}(\\phi,w,u) = \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{BCE}}(g_w(f_\\phi(x_i)),y_i) + \\lambda \\cdot \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{MSE}}(h_u(f_\\phi(x_i)),t_i),\n$$\n其中权重衰减在梯度更新期间作为参数的 $\\ell_2$ 正则化实现。\n\n为什么辅助头会有帮助。考虑关于潜变量 $z_i$ 的梯度。对于分类器，\n$$\n\\frac{\\partial}{\\partial z_i}\\ell_{\\mathrm{BCE}}(g_w(z_i),y_i) = (p_i - y_i) \\, w_c,\n$$\n其中 $p_i = g_w(z_i)$。由于 $y_i$ 仅依赖于 $\\alpha_i$，且角度分量 $[\\cos\\theta_i,\\sin\\theta_i]$ 不携带关于 $y_i$ 的信息，ERM 倾向于驱动 $w_c$ 关注与 $\\alpha$ 对齐的编码器方向，并抑制与角度对齐的编码器方向（以最小化方差）。因此，如果 $\\lambda = 0$，在 $z$ 中保留角度的梯度信号是最小的，因为与角度对应的方向与分类目标的相关性趋于消失，因此几乎没有受到规范性压力被 $W_e$ 保留下来。相反，当 $\\lambda  0$ 时，辅助 MSE 引入了一个梯度项\n$$\n\\frac{\\partial}{\\partial z_i}\\left(\\lambda \\ell_{\\mathrm{MSE}}(h_u(z_i),t_i)\\right) = \\lambda \\, U^\\top \\left(h_u(z_i) - t_i\\right),\n$$\n这直接鼓励 $z_i$ 保留足够线性信息来重构 $t_i = [\\cos\\theta_i,\\sin\\theta_i]^\\top$。如果 $d_z \\ge 2$，编码器有足够的自由度来线性地传递二维旋转子空间；如果 $d_z \\ge 3$，它还可以同时保留标量内容 $\\alpha$，从而使两个任务都能被解决。如果 $d_z  2$，线性地保留 $\\cos\\theta$ 和 $\\sin\\theta$ 是不可能的，因此线性可解码性的提升是有限的。\n\n基于原理的算法设计。我们实现全批量梯度下降，学习率为 $\\eta$，权重衰减为 $\\gamma$。设 $Z = X W_e^\\top + \\mathbf{1} b_e^\\top$ 为批量潜矩阵。分类器 logits 为 $a = Z w_c + b_c \\mathbf{1}$，预测为 $p = \\sigma(a)$，辅助预测为 $\\hat{T} = Z U^\\top + \\mathbf{1} c^\\top$。梯度通过链式法则获得：\n- 对于分类器，定义 $e_c = p - y$，则\n$$\n\\nabla_{w_c} = \\frac{1}{N}\\, Z^\\top e_c + \\gamma w_c,\\quad \\nabla_{b_c} = \\frac{1}{N}\\, \\mathbf{1}^\\top e_c,\n$$\n对潜变量的贡献为 $G_c = e_c w_c^\\top$，其中除以 $N$ 在聚合编码器梯度时应用。\n- 对于辅助头，定义 $R = \\hat{T} - T$，则\n$$\n\\nabla_{U} = \\lambda\\left(\\frac{1}{N}\\, R^\\top Z + \\gamma U\\right),\\quad \\nabla_{c} = \\lambda\\left(\\frac{1}{N}\\, \\mathbf{1}^\\top R\\right),\n$$\n对潜变量的贡献为 $G_a = \\lambda\\, R U$。\n- 对于编码器，\n$$\n\\nabla_{W_e} = \\frac{1}{N}\\,(G_c + G_a)^\\top X + \\gamma W_e,\\quad \\nabla_{b_e} = \\frac{1}{N}\\,\\mathbf{1}^\\top (G_c + G_a).\n$$\n我们在每一步通过减去 $\\eta$ 乘以各自的梯度来更新参数。\n\n通过线性探查进行评估。训练后，我们计算 $Z_{\\mathrm{train}}$ 和 $Z_{\\mathrm{test}}$。我们通过给 $Z$ 增加一列全为1的列，从 $Z_{\\mathrm{train}}$ 到 $T_{\\mathrm{train}}$ 拟合一个带偏置的岭回归探针，求解正规方程：\n$$\n\\Theta^\\star = \\arg\\min_{\\Theta}\\ \\|Z_{\\mathrm{train}}^{\\mathrm{aug}} \\Theta - T_{\\mathrm{train}}\\|_F^2 + \\rho \\|\\Theta\\|_F^2,\n$$\n其闭式解为\n$$\n\\Theta^\\star = \\left((Z_{\\mathrm{train}}^{\\mathrm{aug}})^\\top Z_{\\mathrm{train}}^{\\mathrm{aug}} + \\rho I\\right)^{-1} (Z_{\\mathrm{train}}^{\\mathrm{aug}})^\\top T_{\\mathrm{train}}.\n$$\n然后我们计算 $\\hat{T}_{\\mathrm{test}} = Z_{\\mathrm{test}}^{\\mathrm{aug}} \\Theta^\\star$ 并评估\n$$\n\\mathrm{MSE}_{\\mathrm{probe}} = \\frac{1}{2N_{\\mathrm{test}}}\\sum_{i=1}^{N_{\\mathrm{test}}}\\|\\hat{t}_i - t_i\\|_2^2.\n$$\n该指标通过代理目标 $[\\cos\\theta,\\sin\\theta]$ 直接测试了角度信息从 $z$ 中线性解码的能力。\n\n可测试的假设和边缘情况。我们考虑三种情况：\n- 用例 1，$d_z = 3$，应允许编码器同时传递一个维度给 $\\alpha$ 和两个维度给 $[\\cos\\theta,\\sin\\theta]$，因此 $\\lambda=1$ 的辅助头应能将探针 MSE 降低至少 $\\tau$，同时保持准确率在 $a_{\\min}$ 以上。\n- 用例 2，$d_z = 2$，必须在内容和角度之间进行权衡；我们预计角度可解码性会提高，但分类准确率可能下降到 $a_{\\min}$ 以下，从而不满足布尔准则。\n- 用例 3，$d_z = 1$，缺乏线性表示 $\\cos\\theta$ 和 $\\sin\\theta$ 的能力，因此我们预计线性可解码性几乎没有或根本没有改善。\n\n超参数。我们设置 $N_{\\mathrm{train}} = 800$，$N_{\\mathrm{test}} = 200$，$\\sigma = 0.05$，$\\eta = 0.05$，$\\gamma = 10^{-4}$，$T = 500$，$\\rho = 10^{-3}$，$\\tau = 0.2$，以及 $a_{\\min} = 0.9$。角度以弧度为单位。\n\n最终程序行为。对于 $\\{(3,0),(2,1),(1,2)\\}$ 中的每个测试用例 $(d_z,s)$，我们在相同的数据集和种子上分别训练 $\\lambda=0$ 和 $\\lambda=1$ 的两个模型，计算探针 MSE 和测试准确率，并输出一个布尔值，指示辅助条件是否既将可解码性提高了至少 $\\tau$，又保持了至少 $a_{\\min}$ 的准确率。程序打印单行输出，其中包含按顺序排列的三个布尔值，格式为逗号分隔的 Python 列表，不含任何额外文本。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef generate_data(n_train, n_test, sigma, seed):\n    rng = np.random.default_rng(seed)\n    # Angles in radians\n    theta_train = rng.uniform(-np.pi, np.pi, size=(n_train, 1))\n    theta_test = rng.uniform(-np.pi, np.pi, size=(n_test, 1))\n    # Content variable\n    alpha_train = rng.normal(0.0, 1.0, size=(n_train, 1))\n    alpha_test = rng.normal(0.0, 1.0, size=(n_test, 1))\n    # Targets for auxiliary head (noise-free cos/sin)\n    T_train = np.hstack([np.cos(theta_train), np.sin(theta_train)])\n    T_test = np.hstack([np.cos(theta_test), np.sin(theta_test)])\n    # Inputs with noise\n    X_train_clean = np.hstack([alpha_train, T_train])\n    X_test_clean = np.hstack([alpha_test, T_test])\n    X_train = X_train_clean + rng.normal(0.0, sigma, size=X_train_clean.shape)\n    X_test = X_test_clean + rng.normal(0.0, sigma, size=X_test_clean.shape)\n    # Labels depend only on alpha\n    y_train = (alpha_train[:, 0] >= 0.0).astype(float)\n    y_test = (alpha_test[:, 0] >= 0.0).astype(float)\n    return X_train, y_train, T_train, X_test, y_test, T_test\n\ndef train_model(X, y, T, dz, lam, lr, weight_decay, epochs, seed):\n    rng = np.random.default_rng(seed)\n    n, dx = X.shape\n    # Initialize parameters\n    We = rng.normal(0.0, 0.1, size=(dz, dx))\n    be = np.zeros(dz)\n    wc = rng.normal(0.0, 0.1, size=(dz,))\n    bc = 0.0\n    U = rng.normal(0.0, 0.1, size=(2, dz))\n    c = np.zeros(2)\n\n    for _ in range(epochs):\n        # Forward\n        Z = X @ We.T + be  # (n, dz)\n        logits = Z @ wc + bc  # (n,)\n        p = sigmoid(logits)  # (n,)\n        Th = Z @ U.T + c  # (n,2)\n\n        # Errors\n        ec = (p - y)  # (n,)\n        R = (Th - T)  # (n,2)\n\n        # Gradients for classifier\n        grad_wc = (Z.T @ ec) / n + weight_decay * wc  # (dz,)\n        grad_bc = np.mean(ec)\n\n        # Contribution to Z from classifier\n        Gc = ec[:, None] * wc[None, :]  # (n,dz)\n\n        # Gradients for aux head\n        grad_U = lam * ((R.T @ Z) / n + weight_decay * U)  # (2,dz)\n        grad_c = lam * np.mean(R, axis=0)  # (2,)\n\n        # Contribution to Z from aux\n        Ga = lam * (R @ U)  # (n,dz)\n\n        # Total grad to encoder latent\n        G = Gc + Ga  # (n,dz)\n\n        # Encoder gradients\n        grad_We = (G.T @ X) / n + weight_decay * We  # (dz,dx)\n        grad_be = np.mean(G, axis=0)  # (dz,)\n\n        # Updates\n        We -= lr * grad_We\n        be -= lr * grad_be\n        wc -= lr * grad_wc\n        bc -= lr * grad_bc\n        U -= lr * grad_U\n        c -= lr * grad_c\n\n    params = {\"We\": We, \"be\": be, \"wc\": wc, \"bc\": bc, \"U\": U, \"c\": c}\n    return params\n\ndef evaluate(params, X_train, y_train, T_train, X_test, y_test, T_test, ridge_reg):\n    We = params[\"We\"]; be = params[\"be\"]; wc = params[\"wc\"]; bc = params[\"bc\"]\n    # Latents\n    Z_train = X_train @ We.T + be\n    Z_test = X_test @ We.T + be\n    # Classification accuracy\n    p_test = sigmoid(Z_test @ wc + bc)\n    y_pred = (p_test >= 0.5).astype(float)\n    acc = float(np.mean(y_pred == y_test))\n    # Linear probe ridge regression Z -> T\n    Ztr_aug = np.hstack([Z_train, np.ones((Z_train.shape[0], 1))])\n    Zte_aug = np.hstack([Z_test, np.ones((Z_test.shape[0], 1))])\n    # Closed-form ridge\n    A = Ztr_aug.T @ Ztr_aug\n    A += ridge_reg * np.eye(A.shape[0])\n    B = Ztr_aug.T @ T_train\n    Theta = np.linalg.solve(A, B)  # (dz+1, 2)\n    That = Zte_aug @ Theta\n    mse = float(np.mean(0.5 * np.sum((That - T_test) ** 2, axis=1)))\n    return acc, mse\n\ndef run_case(dz, seed, lam_aux, hyper):\n    # Generate data\n    Xtr, ytr, Ttr, Xte, yte, Tte = generate_data(\n        hyper[\"n_train\"], hyper[\"n_test\"], hyper[\"sigma\"], seed\n    )\n    # Baseline (lam=0)\n    params_base = train_model(\n        Xtr, ytr, Ttr, dz, 0.0, hyper[\"lr\"], hyper[\"wd\"], hyper[\"epochs\"], seed\n    )\n    acc_base, mse_base = evaluate(\n        params_base, Xtr, ytr, Ttr, Xte, yte, Tte, hyper[\"ridge\"]\n    )\n    # Auxiliary (lam=lam_aux)\n    params_aux = train_model(\n        Xtr, ytr, Ttr, dz, lam_aux, hyper[\"lr\"], hyper[\"wd\"], hyper[\"epochs\"], seed\n    )\n    acc_aux, mse_aux = evaluate(\n        params_aux, Xtr, ytr, Ttr, Xte, yte, Tte, hyper[\"ridge\"]\n    )\n    improved = (mse_base - mse_aux) >= hyper[\"tau\"]\n    good_acc = acc_aux >= hyper[\"acc_min\"]\n    return improved and good_acc, (acc_base, mse_base, acc_aux, mse_aux)\n\ndef solve():\n    # Hyperparameters as specified\n    hyper = {\n        \"n_train\": 800,\n        \"n_test\": 200,\n        \"sigma\": 0.05,\n        \"lr\": 0.05,\n        \"wd\": 1e-4,\n        \"epochs\": 500,\n        \"ridge\": 1e-3,\n        \"tau\": 0.2,\n        \"acc_min\": 0.9,\n    }\n    lam_aux = 1.0\n    # Test suite: (dz, seed)\n    test_cases = [\n        (3, 0),  # sufficient capacity\n        (2, 1),  # boundary capacity\n        (1, 2),  # insufficient capacity\n    ]\n    results = []\n    for dz, seed in test_cases:\n        outcome, _ = run_case(dz, seed, lam_aux, hyper)\n        results.append(outcome)\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3108515"}]}