{"hands_on_practices": [{"introduction": "分层 Softmax 的树状结构不仅仅是一种计算上的优化，它对模型的学习动态有着深远的影响。一个关键的直觉是，并非所有分类错误都是等价的：靠近树根的错误（例如，将“猫”错分为“蜥蜴”）比靠近叶节点的错误（例如，将“波斯猫”错分为“暹罗猫”）更为严重。这个练习将通过一个简化的思想实验，量化处于不同层级的标签错误对训练损失（负对数似然）的影响，从而让你对这一概念有更深刻的数学理解。[@problem_id:3134876]", "problem": "考虑一个分层 softmax 分类器，其标签空间被组织成一个深度为 $D$ 的满二叉树，因此每个叶节点距离根节点都恰好需要 $D$ 次决策。对于一个真实类别对应于特定叶节点的给定训练样本，模型在通往真实叶节点的真实路径上的每个内部节点处，对于走向真实叶节点的分支输出概率 $q \\in (0.5, 1)$，因此对于相反的分支输出概率 $1 - q$。分层 softmax 分配给真实类别的概率是从根节点到真实叶节点的路径上所有条件分支概率的乘积。训练损失是负对数似然 (NLL)，定义为分配给目标类别的概率的负自然对数。\n\n假设在路径的第 $k$ 层发生了一个标签错误（$k=1$ 表示根节点，$k=D$ 表示最深的决策层）。在第 $k$ 层发生的标签错误会反转该层的分支决策，然后目标路径会沿着相应的错误子树到达该子树中的某个叶节点。为了量化此类标签错误引起的期望损失增量，假设在错误子树中的每个节点上，模型继续为走向真实叶节点（在标签错误下，这已不是目标）的分支分配概率 $q$，并为远离真实叶节点（在标签错误下，这是目标）的分支分配概率 $1 - q$。在这些假设下，标签正确的样本的 NLL 在每一层都使用 $q$，而在第 $k$ 层发生标签错误的 NLL 在前 $k-1$ 层使用 $q$，在剩余的 $D - k + 1$ 层使用 $1 - q$。\n\n在 $D = 7$ 和 $q = 0.8$ 的条件下，计算根节点（$k = 1$）发生标签错误的期望 NLL 增量与最深层（$k = D$）发生标签错误的期望 NLL 增量之比。请以纯数字形式提供最终答案；无需四舍五入。", "solution": "该问题陈述经评估有效。它在深度学习领域具有科学依据，描述了一个分层 softmax 的理论模型。该问题是适定的，提供了一组自洽且一致的定义、假设和数据（$D=7$，$q=0.8$），从而可以计算出唯一解。其语言客观精确，并且其设定虽然经过简化，但并非不科学、不现实或无不足道。\n\n设 $D$ 为分层 softmax 分类器的满二叉树的深度。设 $q$ 为在通往真实叶种类别的路径上，于任意内部节点选择正确分支的概率。问题将训练损失定义为负对数似然 (NLL)，即 $L = -\\ln(P)$，其中 $P$ 是模型分配给目标类别的概率。\n\n首先，我们计算标签正确的样本的 NLL，记为 $L_{\\text{true}}$。从根节点到真实叶节点的路径包含 $D$ 个决策步骤。在每一步，模型都为正确的分支分配概率 $q$。真实类别的总概率 $P_{\\text{true}}$ 是这些概率的乘积：\n$$P_{\\text{true}} = \\prod_{i=1}^{D} q = q^D$$\n因此，相应的 NLL 为：\n$$L_{\\text{true}} = -\\ln(P_{\\text{true}}) = -\\ln(q^D) = -D \\ln(q)$$\n\n接下来，我们考虑在第 $k$ 层发生标签错误的情况，其中 $k=1$ 对应根节点，$k=D$ 对应最深的决策层。对于此类错误，目标路径在前 $k-1$ 层遵循正确的分支序列，在第 $k$ 层走错误的分支，然后继续前进到该错误子树中的一个叶节点。\n\n问题为计算新的、错误的目标叶节点的概率 $P_k$ 提供了一个特定假设：NLL 的计算在前 $k-1$ 层使用概率 $q$，在剩下的 $D-k+1$ 层使用概率 $1-q$。这意味着概率 $P_k$ 由以下乘积给出：\n$$P_k = \\left( \\prod_{i=1}^{k-1} q \\right) \\left( \\prod_{j=k}^{D} (1-q) \\right) = q^{k-1} (1-q)^{D-k+1}$$\n这个错误标签的 NLL, $L_k$ 是：\n$$L_k = -\\ln(P_k) = -\\ln\\left(q^{k-1} (1-q)^{D-k+1}\\right)$$\n利用自然对数的性质，我们可以展开这个表达式：\n$$L_k = - \\left[ \\ln(q^{k-1}) + \\ln((1-q)^{D-k+1}) \\right] = - \\left[ (k-1)\\ln(q) + (D-k+1)\\ln(1-q) \\right]$$\n\n由于在第 $k$ 层发生标签错误而导致的 NLL 增量，记为 $\\Delta L_k$，是带错误的 NLL 与真实标签的 NLL 之差：$\\Delta L_k = L_k - L_{\\text{true}}$。\n$$\\Delta L_k = \\left( - (k-1)\\ln(q) - (D-k+1)\\ln(1-q) \\right) - \\left( -D\\ln(q) \\right)$$\n$$\\Delta L_k = D\\ln(q) - (k-1)\\ln(q) - (D-k+1)\\ln(1-q)$$\n我们可以合并包含 $\\ln(q)$ 的项：\n$$\\Delta L_k = (D - (k-1))\\ln(q) - (D-k+1)\\ln(1-q)$$\n$$\\Delta L_k = (D-k+1)\\ln(q) - (D-k+1)\\ln(1-q)$$\n提取公因式 $(D-k+1)$：\n$$\\Delta L_k = (D-k+1) \\left[ \\ln(q) - \\ln(1-q) \\right]$$\n这可以简化为：\n$$\\Delta L_k = (D-k+1) \\ln\\left(\\frac{q}{1-q}\\right)$$\n\n问题要求计算在根节点（$k=1$）发生标签错误的期望 NLL 增量与在最深层（$k=D$）发生错误的期望 NLL 增量之比。\n\n对于根节点的标签错误，我们设 $k=1$：\n$$\\Delta L_{k=1} = (D-1+1) \\ln\\left(\\frac{q}{1-q}\\right) = D \\ln\\left(\\frac{q}{1-q}\\right)$$\n\n对于最深层的标签错误，我们设 $k=D$：\n$$\\Delta L_{k=D} = (D-D+1) \\ln\\left(\\frac{q}{1-q}\\right) = (1) \\ln\\left(\\frac{q}{1-q}\\right) = \\ln\\left(\\frac{q}{1-q}\\right)$$\n\n现在，我们计算所要求的比率：\n$$\\text{Ratio} = \\frac{\\Delta L_{k=1}}{\\Delta L_{k=D}} = \\frac{D \\ln\\left(\\frac{q}{1-q}\\right)}{\\ln\\left(\\frac{q}{1-q}\\right)}$$\n问题规定 $q \\in (0.5, 1)$，这确保了 $q \\neq 1-q$ 且 $\\frac{q}{1-q} > 1$。因此，$\\ln\\left(\\frac{q}{1-q}\\right)$ 项是一个定义明确的正数，可以从分子和分母中约去。\n$$\\text{Ratio} = D$$\n\n问题给出了具体值 $D=7$ 和 $q=0.8$。$q$ 的值对于计算该比率并非必需，但它用于确认对数项是定义明确的。代入 $D$ 的值：\n$$\\text{Ratio} = 7$$", "answer": "$$\\boxed{7}$$", "id": "3134876"}, {"introduction": "理解了分层 Softmax 的结构后，下一步便是如何使用训练好的模型进行预测。与标准 Softmax 一样，对所有类别进行完全评估可能非常耗时。本练习将引导你实现束搜索（beam search），这是一种在不进行详尽搜索的情况下高效找到最可能类别集合的实用算法。通过亲手实现，你将具体体验到在推理过程中，计算成本（延迟）与预测准确率之间的核心权衡。[@problem_id:3134831]", "problem": "给定一个用于多类分类器的分层 Softmax (Hierarchical Softmax, HS) 的二叉树表示。在此表示中，每个类别对应一个叶节点，而沿着唯一的从根到叶的路径上的每个内部节点，都贡献一个由逻辑斯谛函数建模的伯努利决策。设特征向量为 $x \\in \\mathbb{R}^d$。每个内部节点 $i$ 有一个参数向量 $v_i \\in \\mathbb{R}^d$，在节点 $i$ 处走向左子节点的概率为 $p_i(x) = \\sigma(v_i^\\top x)$，其中 $\\sigma(z)$ 是逻辑斯谛函数，定义为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。对于一个类别叶节点 $c$，其在给定 $x$ 下的条件概率是其路径上所有概率的乘积，如果在节点 $i$ 处路径走向左边，则使用 $p_i(x)$，如果走向右边，则使用 $1 - p_i(x)$。对数概率是这些因子的对数之和。\n\n您的任务是实现对此 HS 树的束搜索（beam search）推断，以近似计算给定 $x$ 的 top-$k$ 个最可能的类别。束搜索在每个深度维护（按累积对数概率计算的）前 $w$ 个部分路径，并扩展它们直到到达叶节点。您必须分析束宽 $w$ 如何影响 top-$k$ 准确率和延迟（以逻辑斯谛函数评估的总次数来衡量），并将经验延迟与理论复杂度 $O(w \\ell)$ 联系起来，其中 $\\ell$ 是路径长度（从根到叶的树深度）。\n\n使用以下固定的 HS 模型和数据集。该二叉树的深度为 $\\ell = 3$（根节点在深度 $0$，叶节点在深度 $3$），内部节点索引为 $0$ 到 $6$，叶节点索引为 $0$ 到 $7$，对应于决策代码 $(\\text{L},\\text{L},\\text{L}) \\mapsto 0$、$(\\text{L},\\text{L},\\text{R}) \\mapsto 1$、$(\\text{L},\\text{R},\\text{L}) \\mapsto 2$、$(\\text{L},\\text{R},\\text{R}) \\mapsto 3$、$(\\text{R},\\text{L},\\text{L}) \\mapsto 4$、$(\\text{R},\\text{L},\\text{R}) \\mapsto 5$、$(\\text{R},\\text{R},\\text{L}) \\mapsto 6$、$(\\text{R},\\text{R},\\text{R}) \\mapsto 7$。内部节点的邻接关系为：节点 $0$ 的子节点为 $(1,2)$，节点 $1$ 的子节点为 $(3,4)$，节点 $2$ 的子节点为 $(5,6)$。设 $d = 3$。参数向量为：\n$v_0 = [0.8, -0.5, 0.3]$，$v_1 = [0.5, 0.4, -0.6]$，$v_2 = [-0.4, 0.3, 0.7]$，$v_3 = [0.2, -0.9, 0.1]$，$v_4 = [-0.5, 0.2, -0.1]$，$v_5 = [0.6, -0.2, 0.4]$，$v_6 = [-0.3, 0.7, -0.8]$。\n\n数据集由 $6$ 个特征-标签对 $(x, y)$ 组成，包含特征向量和真实标签叶节点索引：\n$x_1 = [0.9, -0.1, 0.3], y_1 = 0$；\n$x_2 = [0.2, 0.8, -0.5], y_2 = 6$；\n$x_3 = [-0.4, 0.1, 0.9], y_3 = 2$；\n$x_4 = [0.3, -0.6, -0.2], y_4 = 1$；\n$x_5 = [-0.1, 0.5, -0.7], y_5 = 5$；\n$x_6 = [0.0, -0.3, 0.4], y_6 = 3$。\n\n实现束搜索，对于每个特征向量 $x$，返回 top-$k$ 个预测的叶节点索引（基于累积对数概率），并计算该 $x$ 搜索期间执行的逻辑斯谛函数评估次数。对于给定的 $(w,k)$，top-$k$ 准确率定义为数据集中真实标签 $y$ 包含在 top-$k$ 预测中的样本所占的比例（以小数表示）。延迟定义为每个样本的平均逻辑斯谛函数评估次数。您不应测量物理时钟时间（wall-clock time）；只需将逻辑斯谛评估的平均次数报告为浮点数。\n\n测试套件：针对以下 $(w,k)$ 对运行程序，以覆盖不同范围，包括贪心边界、中等宽度、近乎穷举的宽度以及比叶节点数量更大的宽度：\n$(w,k) = (1,1)$、$(w,k) = (2,2)$、$(w,k) = (4,3)$、$(w,k) = (10,3)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于指定顺序的每个测试用例，附加 top-$k$ 准确率（一个 $0$ 到 $1$ 之间的小数），后跟平均延迟（一个浮点数），从而产生一个长度为 $8$ 的扁平列表，格式为 $[\\text{acc}_{1}, \\text{lat}_{1}, \\text{acc}_{2}, \\text{lat}_{2}, \\text{acc}_{3}, \\text{lat}_{3}, \\text{acc}_{4}, \\text{lat}_{4}]$。\n\n您的实现必须是自包含的，且不使用任何外部输入。该分析必须纯粹是数学和逻辑上的，基于给定的 HS 结构和定义，并且必须将观察到的延迟与理论复杂度 $O(w \\ell)$ 联系起来。", "solution": "该问题要求在分层 Softmax (HS) 模型中实现并分析用于推断的束搜索。解决方案涉及三个主要组成部分：理解 HS 模型及其相关的概率计算，实现束搜索算法，以及根据一组给定的测试用例评估其在准确率和延迟方面的性能。\n\n### 1. 分层 Softmax 模型与树结构\n\nHS 模型定义在一个深度为 $\\ell=3$ 的完全二叉树上。该树由 $2^\\ell-1 = 7$ 个内部节点（索引从 $0$ 到 $6$）和 $2^\\ell=8$ 个叶节点（索引从 $0$ 到 $7$）组成，叶节点代表各个类别。\n\n节点之间的关系对于完全二叉树是标准的：对于索引为 $i$ 的内部节点，其左子节点的索引为 $2i+1$，右子节点的索引为 $2i+2$。树的根是节点 $0$。内部节点的结构如下：\n- 深度 0：节点 $0$\n- 深度 1：节点 $1, 2$\n- 深度 2：节点 $3, 4, 5, 6$\n\n每个类别（叶节点）由一条从根开始的长度为 $\\ell=3$ 的路径唯一标识。问题指定了从左/右（Left/Right）决策序列到叶节点索引的映射，例如 $(\\text{L},\\text{L},\\text{L}) \\mapsto 0$。这对应于一种二进制编码，其中 L 为 $0$，R 为 $1$。\n\n任何类别 $c$ 的路径可以由其二进制表示确定。沿着到叶节点 $c$ 的路径所访问的内部节点序列可以迭代构建。从根节点（节点 $0$）开始，路径选择由 $c$ 的二进制位决定。对于路径长度 $\\ell=3$，在深度 $d \\in \\{0, 1, 2\\}$ 处的决策由 $c$ 的二进制表示的第 $2-d$ 位确定。\n\n给定特征向量 $x \\in \\mathbb{R}^d$，类别 $c$ 的概率是其路径上每个内部节点处的条件概率的乘积。在具有参数向量 $v_i$ 的内部节点 $i$ 处，走左分支的概率是 $p_i(x) = \\sigma(v_i^\\top x)$，走右分支的概率是 $1-p_i(x) = \\sigma(-v_i^\\top x)$，其中 $\\sigma(z) = 1/(1+e^{-z})$ 是逻辑斯谛 sigmoid 函数。\n\n由于概率是相乘的，它们的对数是相加的。这在数值上更稳定。左转和右转的对数概率为：\n- 左转：$\\log(p_i(x)) = \\log(\\sigma(v_i^\\top x)) = \\log\\left(\\frac{1}{1+e^{-v_i^\\top x}}\\right) = -\\log(1+e^{-v_i^\\top x})$\n- 右转：$\\log(1-p_i(x)) = \\log(1 - \\sigma(v_i^\\top x)) = \\log\\left(\\frac{e^{-v_i^\\top x}}{1+e^{-v_i^\\top x}}\\right) = -v_i^\\top x - \\log(1+e^{-v_i^\\top x})$\n\n类别 $c$ 的总对数概率为 $\\log P(c|x) = \\sum_{j=0}^{\\ell-1} \\log P(\\text{decision}_j | \\text{node}_j, x)$，其中 $(\\text{node}_j, \\text{decision}_j)$ 是到 $c$ 的路径上的节点和决策序列。\n\n### 2. 束搜索算法\n\n束搜索（Beam search）是一种启发式搜索算法，它通过在每个深度保留有限数量 `w`（束宽）个最有希望的部分路径来探索树。\n\n对于给定的特征向量 $x$ 和束宽 $w$，算法按以下步骤进行：\n1.  **初始化**：从根节点（节点 $0$）开始。初始束包含一个假设：一个（对数概率, 节点索引）的元组，即 $(0.0, 0)$。逻辑斯谛函数评估的总次数初始化为 $0$。\n\n2.  **按深度迭代**：对于从 $0$ 到 $\\ell-2=1$ 的每个深度 $d$：\n    a. 创建一个空的 `candidates` 列表。\n    b. 对于当前束中的每个假设（当前对数概率 `log_p`，当前节点索引 `node_idx`）：\n        i. 计算点积 $z = v_{\\text{node\\_idx}}^\\top x$。\n        ii. 将评估计数增加 $1$。\n        iii. **左子节点**：新的假设是 `(log_p - \\log(1+e^{-z}), 2 \\cdot \\text{node\\_idx} + 1)`。\n        iv. **右子节点**：新的假设是 `(log_p - z - \\log(1+e^{-z}), 2 \\cdot \\text{node\\_idx} + 2)`。\n        v. 将两个新假设都添加到 `candidates` 列表中。\n    c. 按对数概率降序对 `candidates` 进行排序。\n    d. 下一个深度的束是排序后的 `candidates` 中的前 `w` 个假设。\n\n3.  **到叶节点的最终扩展**：迭代到深度 $\\ell-1=2$ 后，束中包含最多 `w` 个假设，对应于此深度的内部节点（节点 $3, 4, 5, 6$）。\n    a. 创建一个空的 `final_leaves` 列表。\n    b. 对于每个剩余的假设（`log_p`, `node_idx`）：\n        i. 执行一次与步骤 2.b 类似的最终扩展，计算两个子叶节点的对数概率。\n        ii. 叶节点索引由父节点 `node_idx` 确定。对于深度为 $\\ell=3$ 的树，深度为 $2$ 的父节点 `i` 的叶节点索引为 $2(i - (2^2-1))$ 和 $2(i - (2^2-1)) + 1$。\n        iii. 将两个（对数概率, 叶节点索引）元组添加到 `final_leaves` 中。\n    c. 按对数概率降序对 `final_leaves` 进行排序。\n\n4.  **结果**：top-$k$ 预测是排序后的 `final_leaves` 中前 $k$ 个条目的叶节点索引。逻辑斯谛函数评估的总次数是此样本记录的延迟。\n\n### 3. 延迟分析\n\n延迟定义为逻辑斯谛函数的评估次数。这个计数取决于束宽 `w` 和树的结构，但与输入向量 $x$ 无关。\n- 在深度 $d=0$（根节点），我们扩展 $1$ 个节点。这需要 $1$ 次评估。生成的候选路径数量为 $2$。下一个束的大小将为 $\\min(w, 2)$。\n- 在深度 $d=1$，我们扩展 $\\min(w, 2)$ 个节点。这需要 $\\min(w, 2)$ 次评估。候选路径的数量是 $2 \\cdot \\min(w, 2)$。下一个束的大小是 $\\min(w, 4)$。\n- 在深度 $d=2$（最终扩展），我们扩展 $\\min(w, 4)$ 个节点。这需要 $\\min(w, 4)$ 次评估。\n\n因此，对于深度为 $\\ell=3$ 的树，总评估次数（延迟）是：$1 + \\min(w, 2) + \\min(w, 4)$。\n\n对于指定的测试用例，理论延迟为：\n- $w=1$: $1 + \\min(1, 2) + \\min(1, 4) = 1 + 1 + 1 = 3$。这是贪心搜索。\n- $w=2$: $1 + \\min(2, 2) + \\min(2, 4) = 1 + 2 + 2 = 5$。\n- $w=4$: $1 + \\min(4, 2) + \\min(4, 4) = 1 + 2 + 4 = 7$。这是对所有内部节点的穷举搜索。\n- $w=10$: $1 + \\min(10, 2) + \\min(10, 4) = 1 + 2 + 4 = 7$。由于树在任何层级的宽度都不超过 $4$，大于 $4$ 的束宽不会改变搜索路径，并产生与 $w=4$ 相同的结果。\n\n理论复杂度为 $O(w\\ell)$。我们为平衡二叉树推导出的延迟公式是 $\\sum_{d=0}^{\\ell-1} \\min(w, 2^d)$。对于较小的 $w$（即 $w \\ll 2^d$），这个和约等于 $1+(\\ell-1)w$，这确实是 $O(w\\ell)$。实现所测量的经验延迟应与这些理论值完全匹配，因为计算次数是确定性的。\n\n### 4. 准确率计算\n\ntop-$k$ 准确率是指真实类别标签 $y$ 出现在束搜索返回的 $k$ 个叶节点索引集合中的数据样本所占的比例。对于大小为 $N$ 的数据集，如果 $N_{correct}$ 是 $y$ 在 top-$k$ 预测中找到的样本数，则准确率为 $N_{correct}/N$。\n\n实现现在将继续进行，对整个数据集上的每个 $(w,k)$ 对应用束搜索算法，以计算所需的准确率和延迟指标。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical softmax beam search problem.\n    \"\"\"\n    \n    # --- Givens from the problem statement ---\n\n    # Hierarchical Softmax Model Parameters\n    # Dimension d=3, depth l=3\n    V = np.array([\n        [0.8, -0.5, 0.3],   # v_0\n        [0.5, 0.4, -0.6],   # v_1\n        [-0.4, 0.3, 0.7],   # v_2\n        [0.2, -0.9, 0.1],   # v_3\n        [-0.5, 0.2, -0.1],   # v_4\n        [0.6, -0.2, 0.4],   # v_5\n        [-0.3, 0.7, -0.8]    # v_6\n    ])\n\n    # Dataset (feature vectors X and ground-truth labels y)\n    X = np.array([\n        [0.9, -0.1, 0.3],   # x_1\n        [0.2, 0.8, -0.5],   # x_2\n        [-0.4, 0.1, 0.9],   # x_3\n        [0.3, -0.6, -0.2],   # x_4\n        [-0.1, 0.5, -0.7],   # x_5\n        [0.0, -0.3, 0.4]    # x_6\n    ])\n    \n    Y = np.array([0, 6, 2, 1, 5, 3])\n\n    # Test cases: (beam_width, k)\n    test_cases = [\n        (1, 1),\n        (2, 2),\n        (4, 3),\n        (10, 3)\n    ]\n    \n    # --- Helper Functions ---\n    \n    def log_sigmoid(z):\n        \"\"\"Numerically stable log of the sigmoid function.\"\"\"\n        return -np.log(1 + np.exp(-z))\n\n    def beam_search(x, w, l=3):\n        \"\"\"\n        Performs beam search on the HS tree.\n        \n        Args:\n            x (np.ndarray): The feature vector.\n            w (int): The beam width.\n            l (int): The depth of the tree.\n            \n        Returns:\n            tuple: A tuple containing:\n                - list: A sorted list of (log_prob, leaf_index) tuples.\n                - int: The number of logistic function evaluations (latency).\n        \"\"\"\n        eval_count = 0\n        \n        # Initial beam: (log_prob, node_index)\n        beam = [(0.0, 0)]\n\n        # Expand beam layer by layer until parent of leaves\n        for depth in range(l - 1):\n            candidates = []\n            for log_prob, node_idx in beam:\n                z = V[node_idx] @ x\n                eval_count += 1\n                \n                # Left child\n                left_child_idx = 2 * node_idx + 1\n                log_prob_left = log_prob + log_sigmoid(z)\n                candidates.append((log_prob_left, left_child_idx))\n                \n                # Right child\n                right_child_idx = 2 * node_idx + 2\n                log_prob_right = log_prob + log_sigmoid(-z) # log(1-sigma(z))\n                candidates.append((log_prob_right, right_child_idx))\n                \n            # Sort all candidates by log-probability (descending)\n            candidates.sort(key=lambda item: item[0], reverse=True)\n            \n            # Prune to beam width w\n            beam = candidates[:w]\n\n        # Final expansion from last internal nodes to leaves\n        final_leaves = []\n        nodes_at_last_internal_depth = 2**(l - 1) - 1\n        for log_prob, node_idx in beam:\n            z = V[node_idx] @ x\n            eval_count += 1\n            \n            # Calculate leaf indices\n            leaf_idx_start = 2 * (node_idx - nodes_at_last_internal_depth)\n            \n            # Left leaf\n            log_prob_left = log_prob + log_sigmoid(z)\n            final_leaves.append((log_prob_left, leaf_idx_start))\n            \n            # Right leaf\n            log_prob_right = log_prob + log_sigmoid(-z)\n            final_leaves.append((log_prob_right, leaf_idx_start + 1))\n            \n        # Sort final leaves by log-probability\n        final_leaves.sort(key=lambda item: item[0], reverse=True)\n        \n        return final_leaves, eval_count\n\n    # --- Main Logic ---\n\n    results = []\n    num_samples = len(Y)\n\n    for w, k in test_cases:\n        total_correct = 0\n        total_latency = 0\n        \n        for i in range(num_samples):\n            x = X[i]\n            y_true = Y[i]\n            \n            # Perform beam search\n            sorted_leaves, latency = beam_search(x, w)\n            \n            # Get top-k predictions\n            top_k_preds = [leaf_idx for _, leaf_idx in sorted_leaves[:k]]\n            \n            # Check for accuracy\n            if y_true in top_k_preds:\n                total_correct += 1\n                \n            total_latency += latency\n        \n        # Calculate final metrics for this (w, k) pair\n        accuracy = total_correct / num_samples\n        avg_latency = total_latency / num_samples\n        \n        results.extend([accuracy, avg_latency])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3134831"}, {"introduction": "分层 Softmax 的性能在很大程度上取决于其核心——类别树的构建方式。这个压轴练习将带你探索两种构建策略：一种是基于类别频率的霍夫曼编码（Huffman coding），旨在最小化平均查找时间；另一种是基于类别嵌入向量的聚类（k-means clustering），旨在将语义相似的类别分组。你将从头开始实现这两种方法，并在一个合成数据集上评估它们对模型性能（困惑度）和计算效率的影响，从而揭示分层 Softmax 设计中的根本性权衡。[@problem_id:3134847]", "problem": "给定一个基于二叉树的多分类预测分层 softmax 分类器。分层 softmax 将一个有限的类别集合组织成一棵二叉树的叶节点，其中每个内部节点做出一个二元决策，将输入路由到其左子节点或右子节点，直到到达一个叶节点。对于一个固定的输入特征向量 $x \\in \\mathbb{R}^d$ 和一个固定的类别 $c$，该模型将类别概率定义为从根节点到类别 $c$ 对应叶节点的路径上遇到的条件概率的乘积。在每个内部节点 $n$，一个可微的二元分类器计算一个标量分数 $z_n(x)$，并通过 logistic 函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 将其映射到前往某个特定子节点的概率。对于一个带标签的样本 $(x, c)$，其负对数似然是这些路由概率乘积的负对数，而一个数据集上的困惑度（perplexity）是平均负对数似然的指数。\n\n您必须比较两种构建树结构的不同启发式方法：\n- 基于频率的 Huffman 编码：构建一棵二叉树，该树在类别频率分布下最小化期望叶节点深度。\n- 基于聚类的自顶向下 $k$-means：递归地将类别嵌入聚类成 $k = 2$ 个簇以形成一棵二叉树，持续分裂直到每个叶节点只包含一个类别。\n\n您的程序必须从概率、期望值和 logistic 二元分类的基本定义出发，实现以下内容：\n\n1) 树构建启发式方法：\n- 基于频率的 Huffman 树：给定类别上的一个离散分布 $\\{p(c)\\}_{c=1}^V$（其中 $V$ 是词汇量大小），使用 Huffman 算法构建一棵二叉树，该算法贪心地合并两个概率最小的节点，直到只剩下一个根节点。每次合并都会创建一个父节点，其子节点是两个被合并的节点。\n- 通过自顶向下 $2$-means 的聚类树：给定类别嵌入 $\\{e_c \\in \\mathbb{R}^d\\}_{c=1}^V$，使用 $k=2$ 的 $k$-means 和平方欧几里得距离，递归地将当前类别集合划分为两个非空簇，直到每个叶节点只包含一个类别。为保证可复现性，应确定性地在当前子集中沿第一个坐标具有最小值和最大值的类别处初始化 $k$-means 中心，通过分配和簇均值重新计算进行更新，并在分配不再改变或达到固定迭代次数后停止。如果某个簇变为空，则将距离非空簇中心最远的点重新分配给空簇，以保持非空性。\n\n2) 内部节点分类器：\n- 对于任何将其后代类别集合划分为不相交的左、右子集的内部节点，通过建模一个位于两侧类别嵌入均值中间的分离超平面，从第一性原理计算一个线性分类器。令 $m_L$ 为 $\\{e_c: c \\in \\text{left}\\}$ 的算术平均值， $m_R$ 为 $\\{e_c: c \\in \\text{right}\\}$ 的算术平均值。定义 $w = \\frac{m_R - m_L}{\\lVert m_R - m_L \\rVert_2}$（如果 $\\lVert m_R - m_L \\rVert_2 \\neq 0$）否则 $w = 0$，并定义 $b = -\\frac{1}{2} w^\\top (m_L + m_R)$。对于一个输入 $x$，路由到右子节点的概率是 $\\sigma(w^\\top x + b)$，路由到左子节点的概率是 $1 - \\sigma(w^\\top x + b)$。\n\n3) 类别概率和负对数似然：\n- 对于一个带标签的样本 $(x, c)$，令类别 $c$ 的从根到叶的路径为内部节点的有序序列 $\\{n_1, n_2, \\dots, n_{L(c)}\\}$，其中在每个节点 $n_\\ell$，路由决策是向左还是向右取决于哪个子节点位于通往类别 $c$ 的路径上。模型概率为\n$$\nP(c \\mid x) \\;=\\; \\prod_{\\ell = 1}^{L(c)} P(\\text{child}_\\ell \\mid x, n_\\ell),\n$$\n其中 $P(\\text{right} \\mid x, n) = \\sigma(w_n^\\top x + b_n)$ 且 $P(\\text{left} \\mid x, n) = 1 - \\sigma(w_n^\\top x + b_n)$，$(w_n, b_n)$ 的定义如上。对于 $(x, c)$ 的负对数似然为 $-\\log P(c \\mid x)$，一个数据集 $\\{(x_i, c_i)\\}_{i=1}^N$ 上的困惑度为\n$$\n\\operatorname{ppl} \\;=\\; \\exp\\!\\left(\\frac{1}{N} \\sum_{i=1}^N \\big(-\\log P(c_i \\mid x_i)\\big)\\right).\n$$\n\n4) 延迟代理：\n- 定义一个延迟代理，即每次预测的期望节点评估次数，计算为真实类别分布下的期望叶节点深度，\n$$\n\\mathbb{E}\\big[L(C)\\big] \\;=\\; \\sum_{c=1}^{V} p(c)\\,L(c),\n$$\n其中 $L(c)$ 是类别 $c$ 对应叶节点的深度（路径上的内部节点数）。\n\n5) 数据生成：\n- 通过从分类分布 $c \\sim \\text{Categorical}(p)$ 中抽样类别标签，然后根据高斯模型 $x \\sim \\mathcal{N}(e_c, \\sigma^2 I_d)$ 抽样特征向量来模拟带标签的数据，其中 $I_d$ 是 $d \\times d$ 的单位矩阵，$\\sigma > 0$ 是标准差。\n\n您必须实现这两种树，使用上述均值差超平面在内部节点构建线性分类器，生成合成数据集，计算每棵树的平均负对数似然和困惑度，并计算每棵树的延迟代理（期望深度）。\n\n测试套件和答案规范：\n实现您的程序以运行以下三个测试用例，每个用例由 $(V, d, E, p, \\sigma, N, \\text{seed})$ 完全指定。在所有用例中，$V = 8$。\n\n- 测试用例 $1$（非均匀频率，二维嵌入）：\n    - $d = 2$\n    - 嵌入 $E \\in \\mathbb{R}^{8 \\times 2}$，行从 $e_0$到 $e_7$：\n        - $e_0 = [0.0, 0.0]$\n        - $e_1 = [1.0, 0.0]$\n        - $e_2 = [0.0, 1.0]$\n        - $e_3 = [1.0, 1.0]$\n        - $e_4 = [2.0, 0.0]$\n        - $e_5 = [2.0, 1.0]$\n        - $e_6 = [3.0, 0.5]$\n        - $e_7 = [-1.0, 0.5]$\n    - 类别概率 $p = [0.30, 0.20, 0.15, 0.12, 0.10, 0.06, 0.04, 0.03]$。\n    - 高斯噪声标准差 $\\sigma = 0.20$。\n    - 样本数 $N = 3000$。\n    - 用于可复现性的随机种子 $\\text{seed} = 7$。\n\n- 测试用例 $2$（均匀频率，二维圆形排列）：\n    - $d = 2$\n    - 嵌入 $E$：\n        - $e_0 = [1.000, 0.000]$\n        - $e_1 = [0.707, 0.707]$\n        - $e_2 = [0.000, 1.000]$\n        - $e_3 = [-0.707, 0.707]$\n        - $e_4 = [-1.000, 0.000]$\n        - $e_5 = [-0.707, -0.707]$\n        - $e_6 = [0.000, -1.000]$\n        - $e_7 = [0.707, -0.707]$\n    - 类别概率 $p = [0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125]$。\n    - $\\sigma = 0.15$。\n    - $N = 3000$。\n    - $\\text{seed} = 13$。\n\n- 测试用例 $3$（高度倾斜的频率，三维嵌入）：\n    - $d = 3$\n    - 嵌入 $E$：\n        - $e_0 = [0.0, 0.0, 0.0]$\n        - $e_1 = [1.0, 0.5, -0.5]$\n        - $e_2 = [-0.5, 1.0, 0.5]$\n        - $e_3 = [0.5, -1.0, 0.5]$\n        - $e_4 = [1.5, 1.5, -1.0]$\n        - $e_5 = [-1.5, 1.0, 1.0]$\n        - $e_6 = [0.0, -1.5, -1.0]$\n        - $e_7 = [2.0, 0.0, 1.0]$\n    - 类别概率 $p = [0.6, 0.1, 0.1, 0.05, 0.05, 0.04, 0.03, 0.03]$。\n    - $\\sigma = 0.25$。\n    - $N = 4000$。\n    - $\\text{seed} = 23$。\n\n每个测试用例的必需输出：\n- 对于每个测试用例，计算：\n    - 基于聚类的 $2$-means 树下的困惑度，浮点数。\n    - 基于频率的 Huffman 树下的困惑度，浮点数。\n    - 基于聚类的 $2$-means 树下的期望深度，浮点数。\n    - 基于频率的 Huffman 树下的期望深度，浮点数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例按上述顺序贡献一个子列表。每个子列表必须为 $[\\text{ppl\\_kmeans}, \\text{ppl\\_huffman}, \\text{lat\\_kmeans}, \\text{lat\\_huffman}]$ 的形式，所有值四舍五入到 $6$ 位小数。例如，总体输出格式为\n\"[[a11,a12,a13,a14],[a21,a22,a23,a24],[a31,a32,a33,a34]]\"。", "solution": "用户提供的问题陈述已经过分析，并被确定为**有效**。它在科学上基于机器学习和概率论的原理，是自洽且适定的。唯一可计算解所需的所有组件都已指定，包括用于树构建和数据生成的确定性过程。\n\n任务是实现并比较两种为分层 softmax 分类器构建二叉树的启发式方法：一种是基于频率的 Huffman 树，另一种是基于聚类的自顶向下 $2$-means 树。比较将基于两个指标：合成数据集上的困惑度，用于衡量模型拟合度；以及定义为期望类别深度的延迟代理，用于衡量计算效率。\n\n解决方案将通过以下核心步骤实现：\n\n1.  **树的构建**：将实现两种不同的算法来构建二叉树层次结构。\n    -   **Huffman 树**：遵循问题规范，我们将实现经典的 Huffman 编码算法。这涉及使用一个最小优先队列来贪心地合并两个概率最低的节点（类别或子树）。新内部节点的概率是其子节点概率的总和。已知该算法可以构建一个前缀码，以最小化期望码长，在此上下文中，这等同于在给定类别频率 $p(c)$ 的情况下最小化期望叶节点深度 $\\mathbb{E}[L(C)]$。树中的每个叶节点代表一个唯一的类别，每个内部节点代表一个二元决策。\n    -   **聚类树**：通过递归地划分类别嵌入集来构建一棵树。在每一步中，对于给定的类别集，我们使用 $k=2$ 的 $k$-means 算法将其分成两个簇。该过程是确定性的：$2$-means 算法的初始中心被选为沿第一个坐标具有最小值和最大值的类别嵌入。聚类标准是平方欧几里得距离。递归持续进行，直到每个叶节点只包含一个类别。这种启发式方法旨在将相似的类别组合在一起，从而可能使每个内部节点的分类任务变得更容易。\n\n2.  **内部节点分类器公式**：对于所构建树中的每个内部节点，都会定义一个线性二元分类器。该分类器的目的是决定是将输入向量 $x$ 路由到其左子节点还是右子节点。给定左子树中的类别集 $\\text{classes}_L$ 和右子树中的类别集 $\\text{classes}_R$，我们首先计算它们各自类别嵌入的均值，$m_L = \\text{mean}\\{e_c : c \\in \\text{classes}_L\\}$ 和 $m_R = \\text{mean}\\{e_c : c \\in \\text{classes}_R\\}$。分类器是一个位于这两个均值中间的超平面。其参数，即权重向量 $w$ 和偏置项 $b$，定义如下：\n    $$\n    w = \\frac{m_R - m_L}{\\lVert m_R - m_L \\rVert_2}, \\quad b = -\\frac{1}{2} w^\\top (m_L + m_R)\n    $$\n    输入 $x$ 的分数为 $z(x) = w^\\top x + b$。该分数代表 $x$ 到分离超平面的投影距离。路由到右子节点的概率由 logistic 函数给出 $P(\\text{right} \\mid x) = \\sigma(z(x))$，而路由到左子节点的概率为 $P(\\text{left} \\mid x) = 1 - \\sigma(z(x))$。\n\n3.  **概率模型与困惑度**：给定输入 $x$ 的类别 $c$ 的概率 $P(c \\mid x)$，是从根到类别 $c$ 对应叶节点的路径上每个内部节点所做路由决策的概率乘积。负对数似然（NLL）为 $-\\log P(c \\mid x)$。为避免在计算可能非常小的概率的对数时出现数值不稳定性，我们在路径的每一步对对数概率求和。向左和向右转的对数概率分别为 $\\log(1-\\sigma(z))$ 和 $\\log(\\sigma(z))$，可以使用 NumPy 的 `logaddexp` 函数进行稳健计算，这等效于一个稳定的 `log(1+exp(x))` 计算。然后，一个包含 $N$ 个样本的数据集上的困惑度计算为平均 NLL 的指数：\n    $$\n    \\operatorname{ppl} = \\exp\\left(\\frac{1}{N} \\sum_{i=1}^N \\text{NLL}(x_i, c_i)\\right)\n    $$\n\n4.  **延迟代理**：预测计算延迟的一个代理是所需的内部节点评估的期望次数。这等同于按类别概率 $p(c)$ 加权的类别的期望深度：\n    $$\n    \\mathbb{E}[L(C)] = \\sum_{c=1}^{V} p(c) L(c)\n    $$\n    其中 $L(c)$ 是类别 $c$ 对应叶节点的深度。\n\n5.  **数据模拟与评估**：对于每个测试用例，将按照规定生成一个合成数据集。一组类别标签 $\\{c_i\\}$ 从由概率 $p$ 定义的分类分布中抽取。对于每个标签 $c_i$，从多元高斯分布 $\\mathcal{N}(e_{c_i}, \\sigma^2 I_d)$ 中抽样一个特征向量 $x_i$，其中 $e_{c_i}$ 是类别 $c_i$ 的嵌入。将在该数据集上为两种树结构计算评估指标（困惑度和延迟）。从数据生成到评估的整个过程通过使用指定的随机种子来确保可复现性。\n\n最终的实现将处理每个测试用例，构建两种类型的树，计算分类器，并评估所需的指标，然后根据问题规范将输出格式化为列表的列表。", "answer": "```python\nimport numpy as np\nimport heapq\nfrom collections import deque, defaultdict\n\nclass Node:\n    \"\"\"Represents a node in the binary tree for hierarchical softmax.\"\"\"\n    def __init__(self, is_leaf, class_id=None, left=None, right=None):\n        self.is_leaf = is_leaf\n        self.class_id = class_id\n        self.left = left\n        self.right = right\n        self.w = None\n        self.b = None\n        self.classes = set([class_id]) if is_leaf else set()\n\ndef generate_data(N, p, E, sigma, rng):\n    \"\"\"Generates synthetic data from a Gaussian mixture model.\"\"\"\n    d = E.shape[1]\n    class_labels = rng.choice(len(p), size=N, p=p)\n    means = E[class_labels]\n    noise = rng.normal(scale=sigma, size=(N, d))\n    X = means + noise\n    C = class_labels\n    return X, C\n\ndef build_huffman_tree(p):\n    \"\"\"Constructs a Huffman tree based on class frequencies.\"\"\"\n    pq = [(prob, i, Node(is_leaf=True, class_id=i)) for i, prob in enumerate(p)]\n    heapq.heapify(pq)\n    \n    counter = len(p)\n    while len(pq) > 1:\n        prob1, _, left_node = heapq.heappop(pq)\n        prob2, _, right_node = heapq.heappop(pq)\n        \n        parent_prob = prob1 + prob2\n        parent_node = Node(is_leaf=False, left=left_node, right=right_node)\n        parent_node.classes = left_node.classes.union(right_node.classes)\n        \n        heapq.heappush(pq, (parent_prob, counter, parent_node))\n        counter += 1\n        \n    return pq[0][2]\n\ndef build_kmeans_tree(class_indices, E, max_iter=100):\n    \"\"\"Recursively constructs a tree using top-down 2-means clustering.\"\"\"\n    if len(class_indices) == 1:\n        return Node(is_leaf=True, class_id=class_indices[0])\n\n    sub_E = E[class_indices, :]\n    \n    # Deterministic initialization of centers\n    min_idx_in_subset = np.argmin(sub_E[:, 0])\n    max_idx_in_subset = np.argmax(sub_E[:, 0])\n    \n    if min_idx_in_subset == max_idx_in_subset:\n        if E.shape[1] > 1:\n            min_idx_in_subset = np.argmin(sub_E[:, 1])\n            max_idx_in_subset = np.argmax(sub_E[:, 1])\n        if min_idx_in_subset == max_idx_in_subset:\n             min_idx_in_subset, max_idx_in_subset = 0, 1\n    \n    center1 = sub_E[min_idx_in_subset]\n    center2 = sub_E[max_idx_in_subset]\n\n    # Handle identical initial centers\n    if np.array_equal(center1, center2):\n      center1 = sub_E[0]\n      center2 = sub_E[1]\n\n    last_assignments = None\n    \n    for _ in range(max_iter):\n        assignments = defaultdict(list)\n        \n        # Assignment step\n        for i, class_idx in enumerate(class_indices):\n            emb = sub_E[i]\n            dist1 = np.sum((emb - center1)**2)\n            dist2 = np.sum((emb - center2)**2)\n            assignments[0 if dist1 <= dist2 else 1].append(class_idx)\n        \n        # Empty cluster handling\n        if not assignments[0] or not assignments[1]:\n            non_empty_key = 1 if not assignments[0] else 0\n            empty_key = 1 - non_empty_key\n            non_empty_indices = assignments[non_empty_key]\n            non_empty_center = center1 if non_empty_key == 0 else center2\n            \n            dists = np.sum((E[non_empty_indices] - non_empty_center)**2, axis=1)\n            farthest_idx_in_list = np.argmax(dists)\n            farthest_class_id = non_empty_indices[farthest_idx_in_list]\n            \n            assignments[non_empty_key].remove(farthest_class_id)\n            assignments[empty_key].append(farthest_class_id)\n        \n        # Convergence check\n        current_assignments_tuple = (tuple(sorted(assignments[0])), tuple(sorted(assignments[1])))\n        if current_assignments_tuple == last_assignments:\n            break\n        last_assignments = current_assignments_tuple\n        \n        # Update step\n        if assignments[0]: center1 = np.mean(E[assignments[0]], axis=0)\n        if assignments[1]: center2 = np.mean(E[assignments[1]], axis=0)\n    \n    # Ensure deterministic child assignment\n    left_indices, right_indices = assignments[0], assignments[1]\n    if min(left_indices) > min(right_indices):\n        left_indices, right_indices = right_indices, left_indices\n\n    left_child = build_kmeans_tree(left_indices, E, max_iter)\n    right_child = build_kmeans_tree(right_indices, E, max_iter)\n    \n    node = Node(is_leaf=False, left=left_child, right=right_child)\n    node.classes = left_child.classes.union(right_child.classes)\n    return node\n\ndef compute_classifiers(node, E):\n    \"\"\"Computes and sets classifier parameters (w, b) for all internal nodes.\"\"\"\n    if node.is_leaf:\n        return\n    \n    compute_classifiers(node.left, E)\n    compute_classifiers(node.right, E)\n    \n    m_L = np.mean(E[list(node.left.classes)], axis=0)\n    m_R = np.mean(E[list(node.right.classes)], axis=0)\n    \n    diff = m_R - m_L\n    norm = np.linalg.norm(diff)\n    \n    w = np.zeros_like(m_R) if norm == 0 else diff / norm\n    b = -0.5 * w.T @ (m_L + m_R)\n    \n    node.w, node.b = w, b\n\ndef find_paths_and_depths(root):\n    \"\"\"Traverses the tree to find the path and depth for each class leaf.\"\"\"\n    paths, depths = {}, {}\n    q = deque([(root, [])]) # (node, path_so_far)\n    \n    while q:\n        node, path = q.popleft()\n        \n        if node.is_leaf:\n            paths[node.class_id] = path\n            depths[node.class_id] = len(path)\n            continue\n            \n        q.append((node.left, path + [(node, 'L')]))\n        q.append((node.right, path + [(node, 'R')]))\n        \n    return paths, depths\n\ndef calculate_perplexity_and_latency(tree_root, E, p, X, C):\n    \"\"\"Computes perplexity and latency for a given tree and dataset.\"\"\"\n    compute_classifiers(tree_root, E)\n    paths, depths = find_paths_and_depths(tree_root)\n    \n    # Latency Proxy\n    V = len(p)\n    latency = sum(p[c] * depths[c] for c in range(V) if c in depths)\n    \n    # Perplexity\n    total_nll = 0.0\n    for i in range(X.shape[0]):\n        x_i, c_i = X[i], C[i]\n        path = paths[c_i]\n        \n        log_prob = 0.0\n        for node, direction in path:\n            z = np.dot(node.w, x_i) + node.b\n            # log P(right) = -log(1+exp(-z))\n            # log P(left) = -log(1+exp(z))\n            log_prob -= np.logaddexp(0, -z) if direction == 'R' else np.logaddexp(0, z)\n        \n        total_nll -= log_prob\n    \n    perplexity = np.exp(total_nll / X.shape[0])\n    \n    return perplexity, latency\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        {\"V\": 8, \"d\": 2, \"E\": np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [2.0, 0.0], [2.0, 1.0], [3.0, 0.5], [-1.0, 0.5]]), \"p\": np.array([0.30, 0.20, 0.15, 0.12, 0.10, 0.06, 0.04, 0.03]), \"sigma\": 0.20, \"N\": 3000, \"seed\": 7},\n        {\"V\": 8, \"d\": 2, \"E\": np.array([[1.000, 0.000], [0.707, 0.707], [0.000, 1.000], [-0.707, 0.707], [-1.000, 0.000], [-0.707, -0.707], [0.000, -1.000], [0.707, -0.707]]), \"p\": np.array([0.125] * 8), \"sigma\": 0.15, \"N\": 3000, \"seed\": 13},\n        {\"V\": 8, \"d\": 3, \"E\": np.array([[0.0, 0.0, 0.0], [1.0, 0.5, -0.5], [-0.5, 1.0, 0.5], [0.5, -1.0, 0.5], [1.5, 1.5, -1.0], [-1.5, 1.0, 1.0], [0.0, -1.5, -1.0], [2.0, 0.0, 1.0]]), \"p\": np.array([0.6, 0.1, 0.1, 0.05, 0.05, 0.04, 0.03, 0.03]), \"sigma\": 0.25, \"N\": 4000, \"seed\": 23}\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        V, E, p, sigma, N, seed = case[\"V\"], case[\"E\"], case[\"p\"], case[\"sigma\"], case[\"N\"], case[\"seed\"]\n        rng = np.random.default_rng(seed)\n        \n        X, C = generate_data(N, p, E, sigma, rng)\n        \n        # Clustering-based (k-means) tree\n        kmeans_root = build_kmeans_tree(list(range(V)), E)\n        ppl_kmeans, lat_kmeans = calculate_perplexity_and_latency(kmeans_root, E, p, X, C)\n        \n        # Frequency-based (Huffman) tree\n        huffman_root = build_huffman_tree(p)\n        ppl_huffman, lat_huffman = calculate_perplexity_and_latency(huffman_root, E, p, X, C)\n        \n        all_results.append([\n            f\"{ppl_kmeans:.6f}\", f\"{ppl_huffman:.6f}\",\n            f\"{lat_kmeans:.6f}\", f\"{lat_huffman:.6f}\"\n        ])\n\n    result_str = \"[\" + \",\".join([f\"[{','.join(res)}]\" for res in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```", "id": "3134847"}]}