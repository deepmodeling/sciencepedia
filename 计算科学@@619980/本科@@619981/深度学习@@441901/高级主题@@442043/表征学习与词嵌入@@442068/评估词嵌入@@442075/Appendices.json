{"hands_on_practices": [{"introduction": "在评估词嵌入时，我们首先需要量化词向量之间的“距离”或“相似度”。余弦相似度和欧几里得距离是两种最常用的度量标准。这个练习将引导你从第一性原理出发，探索这两种度量在单位归一化向量上的深刻联系，这是一个在理论和实践中都至关重要的基础概念。[@problem_id:3123037]", "problem": "给定一个小型词嵌入空间和一个黄金标准的词对相似度分数集。从第一性原理出发，您将形式化如何评估近邻检索以及嵌入导出的相似度与黄金标准之间的相关性。您必须实现一个完整的、可运行的程序，按照下文规定执行评估。\n\n基本基础和定义：\n- 词嵌入是词汇表到向量的映射，我们将其表示为一个嵌入矩阵 $E \\in \\mathbb{R}^{n \\times d}$，其中 $n$ 是单词数量，$d$ 是嵌入维度。对于任何单词 $w$，其嵌入为 $x_w \\in \\mathbb{R}^{d}$。\n- 向量 $x \\in \\mathbb{R}^{d}$ 的 $\\ell_2$ 范数是 $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{d} x_i^2}$。\n- 单词 $w$ 的归一化向量是 $x'_w = \\dfrac{x_w}{\\|x_w\\|_2}$。\n- 两个归一化向量 $x', y' \\in \\mathbb{R}^{d}$ 之间的余弦相似度是 $\\operatorname{cos}(x', y') = x'^\\top y'$，因为对于单位向量，夹角的余弦等于它们的点积。\n- 两个归一化向量 $x', y' \\in \\mathbb{R}^{d}$ 之间的欧几里得距离是 $d(x', y') = \\|x' - y'\\|_2$。\n\n近邻检索和重叠：\n- 对于每个查询词 $w$，将其在余弦相似度下的 top-$k$ 近邻定义为集合 $N_k^{\\mathrm{cos}}(w)$，该集合通过将所有其他词 $u \\neq w$ 按 $\\operatorname{cos}(x'_w, x'_u)$ 降序排序，并按单词字符串的字典序来打破平局以确保确定性，然后取前 $k$ 个元素得到。\n- 对于同一个查询词 $w$，将其在欧几里得距离下的 top-$k$ 近邻定义为集合 $N_k^{\\mathrm{euc}}(w)$，该集合通过将所有其他词 $u \\neq w$ 按 $d(x'_w, x'_u)$ 升序排序，并按单词字符串的字典序来打破平局，然后取前 $k$ 个元素得到。\n- 词 $w$ 的 top-$k$ 近邻重叠度是 $O_k(w) = \\dfrac{|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)|}{k}$。\n- 整个词汇表的平均重叠度是 $A_k = \\dfrac{1}{n} \\sum_{w} O_k(w)$。\n\n与黄金标准的相关性：\n- 设一个黄金标准的词对集合为 $P = \\{(w_i, w_j)\\}$，其黄金分数为 $S_{\\mathrm{gold}}(w_i, w_j) \\in \\mathbb{R}$。\n- 将模型基于余弦的词对 $(w_i, w_j)$ 相似度定义为 $S_{\\mathrm{cos}}(w_i, w_j) = \\operatorname{cos}(x'_{w_i}, x'_{w_j})$。\n- 将模型基于欧几里得的词对 $(w_i, w_j)$ 相似度定义为 $S_{\\mathrm{euc}}(w_i, w_j) = - d(x'_{w_i}, x'_{w_j})$，这样更大的值表示更高的相似度，排序与接近度保持一致。\n- 计算 $S_{\\mathrm{gold}}$ 与 $S_{\\mathrm{cos}}$ 之间，以及 $S_{\\mathrm{gold}}$ 与 $S_{\\mathrm{euc}}$ 之间，在 $P$ 中所有词对上的 Spearman 等级相关系数（Spearman 相关性）$\\rho$。\n\n您必须按照上述规定，在单位归一化嵌入 $x'_w$ 上实现所有计算。\n\n测试套件和数据：\n- 词汇表包含 $n = 6$ 个单词：$[\\text{cat}, \\text{dog}, \\text{lion}, \\text{car}, \\text{automobile}, \\text{vehicle}]$。\n- 嵌入维度 $d = 2$，具有以下原始嵌入（归一化之前）：\n  - $\\text{cat}: [0.9, 0.1]$\n  - $\\text{dog}: [0.85, 0.15]$\n  - $\\text{lion}: [0.8, 0.2]$\n  - $\\text{car}: [-0.9, 0.05]$\n  - $\\text{automobile}: [-0.88, 0.1]$\n  - $\\text{vehicle}: [-0.82, 0.2]$\n- 黄金标准词对 $P$ 及其分数 $S_{\\mathrm{gold}}$：\n  - $(\\text{cat}, \\text{dog}): 0.90$\n  - $(\\text{cat}, \\text{lion}): 0.80$\n  - $(\\text{dog}, \\text{lion}): 0.85$\n  - $(\\text{car}, \\text{automobile}): 0.95$\n  - $(\\text{car}, \\text{vehicle}): 0.82$\n  - $(\\text{automobile}, \\text{vehicle}): 0.88$\n  - $(\\text{cat}, \\text{car}): 0.06$\n  - $(\\text{dog}, \\text{car}): 0.07$\n  - $(\\text{lion}, \\text{car}): 0.05$\n  - $(\\text{cat}, \\text{vehicle}): 0.07$\n  - $(\\text{dog}, \\text{vehicle}): 0.09$\n  - $(\\text{lion}, \\text{vehicle}): 0.10$\n- 必须为 $k \\in \\{1, 3, 5\\}$（即 $k = 1$、$k = 3$ 和 $k = 5$）计算 top-$k$ 近邻重叠度。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[A_1, A_3, A_5, \\rho_{\\mathrm{cos}}, \\rho_{\\mathrm{euc}}]$，每个元素都是一个浮点数。例如，输出格式为 $[\\text{float},\\text{float},\\text{float},\\text{float},\\text{float}]$。\n- 无物理单位适用；角度通过单位向量之间的点积在余弦相似度中隐式处理。将所有实值量表示为十进制数。\n\n科学真实性和推导要求：\n- 从上述定义出发，而不是任何特定于某个实现的捷径或结果。确保近邻检索和相关性计算是从所述定义中推导出来的。\n- 通过指定的平局打破规则和在所有计算前严格遵守单位归一化 $x'_w = \\dfrac{x_w}{\\|x_w\\|_2}$ 来确保确定性行为。\n\n您的程序必须完全自包含，无需任何输入。它必须使用给定的词汇表、嵌入和黄金分数，并且必须以指定的确切格式生成单行输出。", "solution": "该任务是根据一个黄金标准的相似度分数集来评估给定的词嵌入空间。评估涉及两部分：对不同度量标准下近邻检索稳定性的分析，以及模型导出的相似度与黄金标准分数之间相关性的分析。整个过程将根据问题陈述中定义的第一性原理进行。\n\n### 步骤 1：嵌入归一化\n\n每个词的基本表示是其向量嵌入。所有的相似度和距离计算都定义在归一化向量上。对于给定的原始嵌入向量 $x_w \\in \\mathbb{R}^d$，其归一化对应向量 $x'_w$ 是通过除以其 $\\ell_2$ 范数 $\\|x_w\\|_2 = \\sqrt{\\sum_{i=1}^{d} x_{w,i}^2}$ 得到的。\n\n$x'_w = \\frac{x_w}{\\|x_w\\|_2}$\n\n词汇表包含 $n=6$ 个词，其嵌入在 $\\mathbb{R}^2$ 中。原始嵌入及其对应的归一化单位向量如下：\n\n1.  $w = \\text{cat}$, $x_w = [0.9, 0.1]$。 $\\|x_w\\|_2 = \\sqrt{0.9^2 + 0.1^2} = \\sqrt{0.82} \\approx 0.9055385$。\n    $x'_w \\approx [0.9938837, 0.1104315]$\n2.  $w = \\text{dog}$, $x_w = [0.85, 0.15]$。 $\\|x_w\\|_2 = \\sqrt{0.85^2 + 0.15^2} = \\sqrt{0.745} \\approx 0.8631338$。\n    $x'_w \\approx [0.9847864, 0.1737976]$\n3.  $w = \\text{lion}$, $x_w = [0.8, 0.2]$。 $\\|x_w\\|_2 = \\sqrt{0.8^2 + 0.2^2} = \\sqrt{0.68} \\approx 0.8246211$。\n    $x'_w \\approx [0.9701425, 0.2425356]$\n4.  $w = \\text{car}$, $x_w = [-0.9, 0.05]$。 $\\|x_w\\|_2 = \\sqrt{(-0.9)^2 + 0.05^2} = \\sqrt{0.8125} \\approx 0.9013878$。\n    $x'_w \\approx [-0.9984604, 0.0554700]$\n5.  $w = \\text{automobile}$, $x_w = [-0.88, 0.1]$。 $\\|x_w\\|_2 = \\sqrt{(-0.88)^2 + 0.1^2} = \\sqrt{0.7844} \\approx 0.8856636$。\n    $x'_w \\approx [-0.9936048, 0.1129108]$\n6.  $w = \\text{vehicle}$, $x_w = [-0.82, 0.2]$。 $\\|x_w\\|_2 = \\sqrt{(-0.82)^2 + 0.2^2} = \\sqrt{0.7124} \\approx 0.8440379$。\n    $x'_w \\approx [-0.9715197, 0.2369560]$\n\n所有后续计算都将使用这些归一化向量 $x'_w$。\n\n### 步骤 2：近邻检索和平均重叠度 ($A_k$)\n\n我们需要使用两个不同的标准为每个词 $w$ 找到 top-$k$ 近邻：降序的余弦相似度和升序的欧几里得距离。\n- 余弦相似度：$\\operatorname{cos}(x'_w, x'_u) = x'^\\top_w x'_u$\n- 欧几里得距离：$d(x'_w, x'_u) = \\|x'_w - x'_u\\|_2$\n\n对于单位向量，这两个度量之间存在一个关键的数学关系。平方欧几里得距离可以用余弦相似度表示：\n$$d(x'_w, x'_u)^2 = \\|x'_w - x'_u\\|_2^2 = (x'_w - x'_u)^\\top(x'_w - x'_u)$$\n$$= x'^\\top_w x'_w - 2x'^\\top_w x'_u + x'^\\top_u x'_u$$\n由于 $x'_w$ 和 $x'_u$ 是单位向量，$\\|x'_w\\|_2^2 = x'^\\top_w x'_w = 1$ 且 $\\|x'_u\\|_2^2 = x'^\\top_u x'_u = 1$。因此，\n$$d(x'_w, x'_u)^2 = 1 - 2\\operatorname{cos}(x'_w, x'_u) + 1 = 2(1 - \\operatorname{cos}(x'_w, x'_u))$$\n$$d(x'_w, x'_u) = \\sqrt{2(1 - \\operatorname{cos}(x'_w, x'_u))}$$\n函数 $f(c) = \\sqrt{2(1-c)}$ 对于 $c \\in [-1, 1]$ 是一个严格单调递减函数。这意味着较高的余弦相似度得分意味着较低的欧几里得距离，反之亦然。因此，按余弦相似度降序排序近邻列表，在数学上等同于按欧几里得距离升序排序它们。\n\n问题规定，任何相似度或距离值的平局必须通过单词字符串的字典序来打破。由于这个平局打破规则对于两种排序方案是相同的，并且主要排序标准产生相同的顺序，因此得到的近邻排名列表将是相同的。\n$$N_k^{\\mathrm{cos}}(w) = N_k^{\\mathrm{euc}}(w) \\quad \\forall w, k$$\n一个词 $w$ 的 top-$k$ 近邻重叠度，定义为 $O_k(w) = \\frac{|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)|}{k}$，被大大简化。因为集合是相同的，它们的交集就是集合本身：\n$$|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)| = |N_k^{\\mathrm{cos}}(w)| = k$$\n因此，任何单个词的重叠度是：\n$$O_k(w) = \\frac{k}{k} = 1$$\n整个词汇表的平均重叠度，$A_k = \\frac{1}{n} \\sum_{w} O_k(w)$，变为：\n$$A_k = \\frac{1}{n} \\sum_{w} 1 = \\frac{1}{n} \\cdot n = 1$$\n这对任何 $k$ 的选择都成立。因此，我们无需对近邻列表进行进一步的数值计算即可得出结论：\n$$A_1 = 1.0, \\quad A_3 = 1.0, \\quad A_5 = 1.0$$\n\n### 步骤 3：与黄金标准的相关性 ($\\rho$)\n\n我们需要计算黄金标准分数 $S_{\\mathrm{gold}}$ 与两个模型导出的分数 $S_{\\mathrm{cos}}$ 和 $S_{\\mathrm{euc}}$ 之间的 Spearman 等级相关系数 $\\rho$。\n-   $S_{\\mathrm{cos}}(w_i, w_j) = \\operatorname{cos}(x'_{w_i}, x'_{w_j})$\n-   $S_{\\mathrm{euc}}(w_i, w_j) = -d(x'_{w_i}, x'_{w_j})$\n\n使用在步骤 2 中推导出的关系，我们可以用 $S_{\\mathrm{cos}}$ 来表示 $S_{\\mathrm{euc}}$：\n$$S_{\\mathrm{euc}}(w_i, w_j) = -\\sqrt{2(1 - S_{\\mathrm{cos}}(w_i, w_j))}$$\n设 $g(c) = -\\sqrt{2(1-c)}$。关于 $c$ 的导数是 $g'(c) = - \\frac{1}{2\\sqrt{2(1-c)}}(-1) = \\frac{1}{\\sqrt{2(1-c)}}$。对于 $c \\in [-1, 1)$，$g'(c) > 0$。这表明 $S_{\\mathrm{euc}}$ 是 $S_{\\mathrm{cos}}$ 的一个严格单调递增函数。\n\nSpearman 相关性 $\\rho$ 作用于数据的秩，而不是它们的原始值。由于 $S_{\\mathrm{euc}}$ 是 $S_{\\mathrm{cos}}$ 的一个严格单调变换，任何一组由 $S_{\\mathrm{euc}}$ 评分的词对的秩次序将与由 $S_{\\mathrm{cos}}$ 评分的同一组词对的秩次序相同。也就是说，对于任何词对列表，$\\operatorname{rank}(S_{\\mathrm{cos}}) = \\operatorname{rank}(S_{\\mathrm{euc}})$。\n\n鉴于秩是相同的，它们与第三个变量（$S_{\\mathrm{gold}}$ 的秩）的相关性也必须相同。\n$$\\rho_{\\mathrm{cos}} = \\rho(\\operatorname{rank}(S_{\\mathrm{gold}}), \\operatorname{rank}(S_{\\mathrm{cos}}))$$\n$$\\rho_{\\mathrm{euc}} = \\rho(\\operatorname{rank}(S_{\\mathrm{gold}}), \\operatorname{rank}(S_{\\mathrm{euc}}))$$\n由于 $\\operatorname{rank}(S_{\\mathrm{cos}}) = \\operatorname{rank}(S_{\\mathrm{euc}})$，因此可以得出：\n$$\\rho_{\\mathrm{cos}} = \\rho_{\\mathrm{euc}}$$\n我们现在来计算这个值。我们为 12 个黄金标准词对中的每一个计算 $S_{\\mathrm{cos}}$ 分数。\n\n| 词对 $(w_i, w_j)$       | $S_{\\mathrm{gold}}$ | $S_{\\mathrm{cos}}(w_i, w_j)$ |\n| :---------------------- | :----------------- | :-------------------------- |\n| (cat, dog)              | $0.90$             | $0.99801$                   |\n| (cat, lion)             | $0.80$             | $0.99097$                   |\n| (dog, lion)             | $0.85$             | $0.99754$                   |\n| (car, automobile)       | $0.95$             | $0.99852$                   |\n| (car, vehicle)          | $0.82$             | $0.98184$                   |\n| (automobile, vehicle)   | $0.88$             | $0.99114$                   |\n| (cat, car)              | $0.06$             | $-0.98638$                  |\n| (dog, car)              | $0.07$             | $-0.97394$                  |\n| (lion, car)             | $0.05$             | $-0.95532$                  |\n| (cat, vehicle)          | $0.07$             | $-0.93666$                  |\n| (dog, vehicle)          | $0.09$             | $-0.91238$                  |\n| (lion, vehicle)         | $0.10$             | $-0.88126$                  |\n\n分数列表如下：\n$S_{\\mathrm{gold}} = [0.90, 0.80, 0.85, 0.95, 0.82, 0.88, 0.06, 0.07, 0.05, 0.07, 0.09, 0.10]$\n$S_{\\mathrm{cos}} = [0.99801, 0.99097, 0.99754, 0.99852, 0.98184, 0.99114, -0.98638, -0.97394, -0.95532, -0.93666, -0.91238, -0.88126]$\n\n计算这两个列表之间的 Spearman 等级相关系数，得到 $\\rho_{\\mathrm{cos}}$ 和 $\\rho_{\\mathrm{euc}}$ 的值。计算得出 $\\rho \\approx 0.93706$。\n\n### 结果总结\n\n理论推导，经直接计算证实，得出以下最终值：\n-   平均 top-k 重叠度：$A_1 = 1.0$, $A_3 = 1.0$, $A_5 = 1.0$。\n-   Spearman 相关性：$\\rho_{\\mathrm{cos}} = \\rho_{\\mathrm{euc}} \\approx 0.937062937062937$。\n\n最终输出是这些值的有序列表。\n$[1.0, 1.0, 1.0, 0.937062937062937, 0.937062937062937]$", "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Performs evaluation of word embeddings based on neighbor retrieval and correlation.\n    \"\"\"\n    # Define vocabulary and raw embeddings as per the problem statement\n    vocab = ['cat', 'dog', 'lion', 'car', 'automobile', 'vehicle']\n    raw_embeddings = {\n        'cat': np.array([0.9, 0.1]),\n        'dog': np.array([0.85, 0.15]),\n        'lion': np.array([0.8, 0.2]),\n        'car': np.array([-0.9, 0.05]),\n        'automobile': np.array([-0.88, 0.1]),\n        'vehicle': np.array([-0.82, 0.2])\n    }\n    word_to_idx = {word: i for i, word in enumerate(vocab)}\n\n    # Step 1: Normalize embeddings\n    normalized_embeddings = {}\n    for word, vec in raw_embeddings.items():\n        norm = np.linalg.norm(vec)\n        if norm > 0:\n            normalized_embeddings[word] = vec / norm\n        else:\n            # Handle zero vector case, though not present in this problem\n            normalized_embeddings[word] = vec\n\n    # Step 2: Compute average neighbor overlap A_k\n    k_values = [1, 3, 5]\n    avg_overlaps = []\n\n    for k in k_values:\n        total_overlap_ratio = 0.0\n        for query_word in vocab:\n            query_vec = normalized_embeddings[query_word]\n            \n            # Neighbors for comparison\n            other_words = [w for w in vocab if w != query_word]\n            \n            # Calculate cosine similarities and Euclidean distances\n            cos_similarities = []\n            euc_distances = []\n            for other_word in other_words:\n                other_vec = normalized_embeddings[other_word]\n                \n                # Cosine similarity\n                cos_sim = np.dot(query_vec, other_vec)\n                cos_similarities.append((other_word, cos_sim))\n                \n                # Euclidean distance\n                euc_dist = np.linalg.norm(query_vec - other_vec)\n                euc_distances.append((other_word, euc_dist))\n            \n            # Sort neighbors based on specified criteria\n            # For cosine: descending similarity, then lexicographic word order\n            # The key (-s, w) achieves this: sorts by -s (descending s) then w (ascending w)\n            cos_similarities.sort(key=lambda x: (-x[1], x[0]))\n            \n            # For Euclidean: ascending distance, then lexicographic word order\n            # The key (d, w) achieves this: sorts by d (ascending d) then w (ascending w)\n            euc_distances.sort(key=lambda x: (x[1], x[0]))\n            \n            # Get top-k neighbor sets\n            top_k_cos = set(word for word, sim in cos_similarities[:k])\n            top_k_euc = set(word for word, dist in euc_distances[:k])\n            \n            # Calculate overlap\n            intersection_size = len(top_k_cos.intersection(top_k_euc))\n            overlap_ratio = intersection_size / k\n            total_overlap_ratio += overlap_ratio\n            \n        avg_overlaps.append(total_overlap_ratio / len(vocab))\n\n    # Step 3: Compute Spearman correlation with gold standard\n    gold_standard = {\n        ('cat', 'dog'): 0.90,\n        ('cat', 'lion'): 0.80,\n        ('dog', 'lion'): 0.85,\n        ('car', 'automobile'): 0.95,\n        ('car', 'vehicle'): 0.82,\n        ('automobile', 'vehicle'): 0.88,\n        ('cat', 'car'): 0.06,\n        ('dog', 'car'): 0.07,\n        ('lion', 'car'): 0.05,\n        ('cat', 'vehicle'): 0.07,\n        ('dog', 'vehicle'): 0.09,\n        ('lion', 'vehicle'): 0.10\n    }\n\n    gold_scores = []\n    model_cos_scores = []\n    model_euc_scores = []\n\n    # Ensure a consistent order for pairs\n    pairs = sorted(list(gold_standard.keys()))\n\n    for w1, w2 in pairs:\n        gold_scores.append(gold_standard[(w1, w2)])\n        \n        vec1 = normalized_embeddings[w1]\n        vec2 = normalized_embeddings[w2]\n        \n        # Calculate model scores S_cos and S_euc\n        s_cos = np.dot(vec1, vec2)\n        s_euc = -np.linalg.norm(vec1 - vec2)\n        \n        model_cos_scores.append(s_cos)\n        model_euc_scores.append(s_euc)\n\n    # Compute Spearman correlation coefficients\n    rho_cos, _ = spearmanr(gold_scores, model_cos_scores)\n    rho_euc, _ = spearmanr(gold_scores, model_euc_scores)\n\n    # Combine all results\n    final_results = avg_overlaps + [rho_cos, rho_euc]\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3123037"}, {"introduction": "语言的魅力在于组合。单个词语的意义组合起来，可以形成复杂的句子，但词语的顺序至关重要。“狗咬人”和“人咬狗”的含义天差地别。这个练习将通过一个经典的例子，让你亲手验证简单的“词袋”模型为何无法捕捉这种由词序决定的语义差异，并了解位置感知模型是如何解决这一问题的。[@problem_id:3123059]", "problem": "考虑在自然语言处理 (NLP) 中，对基于词嵌入构建的短语表示进行评估。设每个单词由实数向量空间中的一个向量表示，并考虑两个主语和宾语顺序颠倒的短语：“dog bites man”和“man bites dog”。假设词嵌入在 $\\mathbb{R}^2$ 中由 $v_{\\text{dog}} = (1,0)$、$v_{\\text{bites}} = (0,1)$ 和 $v_{\\text{man}} = (1,1)$ 给出。为一个由三个词组成的短语 $w_1\\,w_2\\,w_3$ 定义以下两种组合函数：\n\n1. 词袋和（顺序不敏感）：$s = v_{w_1} + v_{w_2} + v_{w_3}$。\n\n2. 使用不同位置矩阵的位置感知线性组合（顺序敏感）：\n$$\nW_{\\text{subj}} = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix},\\quad\nW_{\\text{verb}} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix},\\quad\nW_{\\text{obj}} = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix},\n$$\n和 $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$。\n\n你需要通过余弦相似度和欧几里得距离，在每种组合函数下比较这两个短语，以评估其表示是否能检测到词序差异。选择所有正确的陈述：\n\nA. 在词袋和 $s = v_{w_1} + v_{w_2} + v_{w_3}$ 下，“dog bites man”和“man bites dog”的组合向量是相同的，因此它们的余弦相似度等于 $1$。\n\nB. 在使用给定矩阵的位置感知组合 $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$ 下，“dog bites man”和“man bites dog”之间的余弦相似度等于 $\\frac{12}{13}$ 且严格小于 $1$。\n\nC. 在求和前将每个词向量归一化为单位长度，即 $s = \\sum_{i=1}^{3} \\frac{v_{w_i}}{\\|v_{w_i}\\|}$，会打破加法的交换律，从而解决词袋和中的词序不敏感问题。\n\nD. 用逐元素平均值 $s = \\frac{1}{3}\\big(v_{w_1}+v_{w_2}+v_{w_3}\\big)$ 替换求和，可以防止“dog bites man”和“man bites dog”的向量对齐，因为平均法对不同位置的权重不同。\n\nE. 在词袋和下，“dog bites man”和“man bites dog”这两个短语向量之间的欧几里得距离为 $0$，因此该测试正确地指出了模型未能编码词序的问题。", "solution": "首先验证问题陈述，以确保其具有科学依据、定义明确且客观。\n\n### 第 1 步：提取已知条件\n\n-   **短语**：“dog bites man”和“man bites dog”。\n-   **词嵌入**：在 $\\mathbb{R}^2$ 中，$v_{\\text{dog}} = (1,0)$、$v_{\\text{bites}} = (0,1)$ 和 $v_{\\text{man}} = (1,1)$。\n-   **组合函数 1 (词袋和)**：对于短语 $w_1\\,w_2\\,w_3$，组合向量为 $s = v_{w_1} + v_{w_2} + v_{w_3}$。\n-   **组合函数 2 (位置感知线性组合)**：对于短语 $w_1\\,w_2\\,w_3$，组合向量为 $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$。\n-   **位置矩阵**：\n    $$\n    W_{\\text{subj}} = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix},\\quad\n    W_{\\text{verb}} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix},\\quad\n    W_{\\text{obj}} = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}.\n    $$\n-   **任务**：使用余弦相似度和欧几里得距离，在每种组合函数下比较这两个短语的表示，以评估它们是否能检测到词序差异。\n\n### 第 2 步：使用提取的已知条件进行验证\n\n-   **科学依据**：该问题使用了自然语言处理 (NLP) 和线性代数的标准概念。词袋模型、通过线性变换的组合语义学、余弦相似度和欧几里得距离都是该领域的基本工具。该场景是一个简化但概念上有效的示例，说明了如何建模和评估句法结构。\n-   **定义明确**：该问题提供了所有必要的定义、向量和矩阵。数学运算（向量加法、矩阵-向量乘法、点积、范数）都有明确的定义。问题的每个部分都可以计算出唯一、无歧义的答案。\n-   **客观性**：该问题使用精确的数学语言陈述，没有主观性或歧义。\n\n### 第 3 步：结论与行动\n\n问题陈述是有效的。这是一个清晰且独立的练习，将线性代数应用于 NLP 的一个基础概念。我们可以继续进行解题推导和选项分析。\n\n### 推导与分析\n\n我们将短语“dog bites man”记为 $P_1$，将“man bites dog”记为 $P_2$。词序暗示了主-谓-宾结构。\n\n对于 $P_1$（“dog bites man”）：$w_1 = \\text{dog}, w_2 = \\text{bites}, w_3 = \\text{man}$。\n对应的词向量为 $v_1 = v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$v_2 = v_{\\text{bites}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，$v_3 = v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n对于 $P_2$（“man bites dog”）：$w'_1 = \\text{man}, w'_2 = \\text{bites}, w'_3 = \\text{dog}$。\n对应的词向量为 $v'_1 = v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，$v'_2 = v_{\\text{bites}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，$v'_3 = v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n**组合函数 1 分析：词袋和 ($s$)**\n\n设 $s_1$ 为 $P_1$ 的向量，$s_2$ 为 $P_2$ 的向量。\n$$\ns_1 = v_{\\text{dog}} + v_{\\text{bites}} + v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+0+1 \\\\ 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n因为向量加法满足交换律，所以 $P_2$ 的和将是相同的：\n$$\ns_2 = v_{\\text{man}} + v_{\\text{bites}} + v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1+0+1 \\\\ 1+1+0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n由于 $s_1 = s_2$，该模型对词序不敏感。\n\n**组合函数 2 分析：位置感知线性组合 ($p$)**\n\n设 $p_1$ 为 $P_1$ 的向量，$p_2$ 为 $P_2$ 的向量。\n对于 $P_1$（“dog bites man”），主语是“dog”（$v_1$），谓语是“bites”（$v_2$），宾语是“man”（$v_3$）。\n$$\np_1 = W_{\\text{subj}}v_{\\text{dog}} + W_{\\text{verb}}v_{\\text{bites}} + W_{\\text{obj}}v_{\\text{man}}\n$$\n$$\np_1 = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\np_1 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+0+1 \\\\ 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\n$$\n对于 $P_2$（“man bites dog”），主语是“man”（$v'_1$），谓语是“bites”（$v'_2$），宾语是“dog”（$v'_3$）。\n$$\np_2 = W_{\\text{subj}}v_{\\text{man}} + W_{\\text{verb}}v_{\\text{bites}} + W_{\\text{obj}}v_{\\text{dog}}\n$$\n$$\np_2 = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\np_2 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+0+0 \\\\ 1+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n由于 $p_1 \\neq p_2$，该模型对词序敏感。\n\n### 逐项分析\n\n**A. 在词袋和 $s = v_{w_1} + v_{w_2} + v_{w_3}$ 下，“dog bites man”和“man bites dog”的组合向量是相同的，因此它们的余弦相似度等于 $1$。**\n\n如上计算，$s_1 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$ 且 $s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$。这两个向量是相同的。对于两个相同的非零向量，其余弦相似度 $\\cos(\\theta) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}$ 为 $\\frac{u \\cdot u}{\\|u\\| \\|u\\|} = \\frac{\\|u\\|^2}{\\|u\\|^2} = 1$。该陈述事实上是正确的。\n**结论：正确**\n\n**B. 在使用给定矩阵的位置感知组合 $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$ 下，“dog bites man”和“man bites dog”之间的余弦相似度等于 $\\frac{12}{13}$ 且严格小于 $1$。**\n\n我们有 $p_1 = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}$ 和 $p_2 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$。点积为 $p_1 \\cdot p_2 = (3)(2) + (2)(3) = 6 + 6 = 12$。范数分别为 $\\|p_1\\| = \\sqrt{3^2 + 2^2} = \\sqrt{9+4} = \\sqrt{13}$ 和 $\\|p_2\\| = \\sqrt{2^2 + 3^2} = \\sqrt{4+9} = \\sqrt{13}$。余弦相似度为 $\\cos(\\theta) = \\frac{p_1 \\cdot p_2}{\\|p_1\\| \\|p_2\\|} = \\frac{12}{\\sqrt{13} \\sqrt{13}} = \\frac{12}{13}$。由于 $12  13$，我们有 $\\frac{12}{13}  1$。该陈述事实上是正确的。\n**结论：正确**\n\n**C. 在求和前将每个词向量归一化为单位长度，即 $s = \\sum_{i=1}^{3} \\frac{v_{w_i}}{\\|v_{w_i}\\|}$，会打破加法的交换律，从而解决词袋和中的词序不敏感问题。**\n\n该陈述包含一个基本的数学错误。向量加法在任何向量空间中都是可交换的运算。和 $\\hat{v}_a + \\hat{v}_b + \\hat{v}_c$ 总是等于 $\\hat{v}_c + \\hat{v}_b + \\hat{v}_a$，其中 $\\hat{v}_i$ 是任意向量。在求和*之前*对向量进行归一化，并不会改变求和本身的交换性质。无论加数的顺序如何，最终得到的和向量都是相同的。因此，该过程无法解决词序不敏感问题。“打破了加法的交换律”这个前提是错误的。\n**结论：错误**\n\n**D. 用逐元素平均值 $s = \\frac{1}{3}\\big(v_{w_1}+v_{w_2}+v_{w_3}\\big)$ 替换求和，可以防止“dog bites man”和“man bites dog”的向量对齐，因为平均法对不同位置的权重不同。**\n\n令 $S = v_{w_1}+v_{w_2}+v_{w_3}$。平均值就是 $\\frac{1}{3}S$。在选项 A 的分析中，我们发现两个短语的和向量 $S$ 是相同的：$s_1 = s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$。因此，平均向量也将是相同的：$\\frac{1}{3}s_1 = \\frac{1}{3}s_2 = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}$。这两个短语向量仍然完全对齐。此外，“因为平均法对不同位置的权重不同”这一理由是错误的。如此处定义的标准平均法，对每个输入向量赋予相等的权重（$1/3$），而与其在序列中的位置无关。\n**结论：错误**\n\n**E. 在词袋和下，“dog bites man”和“man bites dog”这两个短语向量之间的欧几里得距离为 $0$，因此该测试正确地指出了模型未能编码词序的问题。**\n\n我们发现，在词袋和下，两个短语的向量是相同的：$s_1 = s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$。它们之间的欧几里得距离是 $d(s_1, s_2) = \\|s_1 - s_2\\| = \\|\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\| = \\|\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\| = 0$。一个能编码词序的模型应该为词序不同（因而含义不同）的短语生成不同的向量。距离为 $0$（或余弦相似度为 $1$）是模型未能区分这两个输入的数学特征。因此，（计算距离的）测试得出的结果为 $0$，正确地指出了这一失败。该推理是合理的。\n**结论：正确**", "answer": "$$\\boxed{ABE}$$", "id": "3123059"}, {"introduction": "词嵌入不仅学习语言的语义结构，也可能无意中捕捉并放大了社会中的偏见，例如性别偏见。作为一个负责任的开发者，评估和减轻这种偏见至关重要。本练习将带你实践一种前沿的去偏方法——零空间投影，并量化去偏效果与模型语义能力（如类比推理）之间的权衡，让你深入理解在构建公平AI系统时所需面对的现实挑战。[@problem_id:3123006]", "problem": "给定一个合成词嵌入空间，旨在测试对词向量中偏置和语义结构的评估。每个词都映射到实数坐标空间中的一个向量。将每个词向量视为 $\\mathbb{R}^d$ 的一个元素，其中 $d = 4$。令 $x_w \\in \\mathbb{R}^4$ 表示与词 $w$ 相关联的向量。使用以下词汇表 $V$ 和嵌入（每个坐标均明确给出）：\n\n- $x_{\\text{man}} = [\\,0.8,\\,0.0,\\,0.1,\\,1.0\\,]$\n- $x_{\\text{woman}} = [\\,0.8,\\,0.0,\\,0.1,\\,{-1.0}\\,]$\n- $x_{\\text{he}} = [\\,0.5,\\,0.0,\\,0.0,\\,1.0\\,]$\n- $x_{\\text{she}} = [\\,0.5,\\,0.0,\\,0.0,\\,{-1.0}\\,]$\n- $x_{\\text{king}} = [\\,0.9,\\,1.0,\\,0.0,\\,0.8\\,]$\n- $x_{\\text{queen}} = [\\,0.9,\\,1.0,\\,0.0,\\,{-0.8}\\,]$\n- $x_{\\text{uncle}} = [\\,0.7,\\,0.0,\\,0.0,\\,0.9\\,]$\n- $x_{\\text{aunt}} = [\\,0.7,\\,0.0,\\,0.0,\\,{-0.9}\\,]$\n- $x_{\\text{doctor}} = [\\,0.85,\\,0.0,\\,1.0,\\,0.2\\,]$\n- $x_{\\text{nurse}} = [\\,0.85,\\,0.0,\\,1.0,\\,{-0.2}\\,]$\n- $x_{\\text{child}} = [\\,0.6,\\,0.0,\\,0.0,\\,0.0\\,]$\n\n使用的基本原则和定义：\n\n- 词嵌入是一个映射 $w \\mapsto x_w \\in \\mathbb{R}^d$，配备了欧几里得内积 $u^\\top v$ 和范数 $\\lVert u \\rVert = \\sqrt{u^\\top u}$。\n- $u$ 和 $v$ 之间的余弦相似度为 $\\cos(u,v) = \\dfrac{u^\\top v}{\\lVert u \\rVert \\lVert v \\rVert}$。\n- 给定一组被视为性别对应词的配对词 $\\{(m_i,f_i)\\}_{i=1}^n$，定义差分向量 $d_i = x_{m_i} - x_{f_i}$。我们寻求一个单位方向 $b \\in \\mathbb{R}^d$，通过在单位约束 $\\lVert b \\rVert = 1$ 下最大化投影 $\\{d_i^\\top b\\}$ 的经验方差来捕获这些差异的主要共享方向。这是中心化差分集的第一主成分方向，从标准正交约束下的方差最大化一般原理中获得。\n- 用于去偏置的零空间投影：对于任何词向量 $x_w$，移除其沿 $b$ 的分量以产生一个去偏置的向量 $x_w' = x_w - (x_w^\\top b)\\,b$。此操作是向 $b$ 的张成的子空间的正交补上的投影。\n- 对于中性词集 $S$，相对于 $b$ 测量的偏置分数为 $B(S,b) = \\dfrac{1}{|S|}\\sum_{w \\in S} \\big| x_w^\\top b \\big|$。\n- 类比评估使用“三余弦加法”(3CosAdd) 规则：对于一个意为“$a$ 之于 $b$ 犹如 $c$ 之于 $d$”的类比 $(a,b,c,d)$，预测 $\\hat{d}$ 为词表中（从候选词中排除 $a, b, c$）其嵌入在 $w \\in V \\setminus \\{a,b,c\\}$ 上能最大化 $\\cos\\!\\big(x_w,\\, x_b - x_a + x_c \\big)$ 的词。准确率是 $\\hat{d} = d$ 的类比所占的比例，以小数表示。\n\n你的指令：\n\n- 通过求解中心化差分向量的单位方差最大化原理，从指定的性别词对中计算偏置方向 $b$。使用任何与此原理一致的数值稳定方法。\n- 通过零空间投影 $x_w' = x_w - (x_w^\\top b)b$ 对所有嵌入进行去偏置。\n- 通过计算去偏置前后的 $B(S,b)$ 来衡量中性词集 $S$ 上的偏置减少量。\n- 使用上述 3CosAdd 规则，在去偏置前后，对一个固定的类比集测量类比准确率。\n\n测试套件和参数：\n\n- 使用中性词集 $S = \\{\\text{doctor},\\text{nurse},\\text{child}\\}$。\n- 使用包含以下 $6$ 个类比的类比集 $A$：\n    $$\n    (\\text{king},\\text{queen},\\text{man},\\text{woman}),\\;\n    (\\text{uncle},\\text{aunt},\\text{man},\\text{woman}),\\;\n    (\\text{he},\\text{she},\\text{man},\\text{woman}),\\;\n    (\\text{king},\\text{man},\\text{queen},\\text{woman}),\\;\n    (\\text{doctor},\\text{nurse},\\text{man},\\text{woman}),\\;\n    (\\text{man},\\text{woman},\\text{king},\\text{queen})\n    $$\n- 定义三个测试用例，每个用例包括一组性别词对的选择和一种偏置方向计算方法的选择：\n    $$\n    \\text{用例 }1:\\; \\text{词对 }P_1 = \\{(\\text{he},\\text{she}),(\\text{man},\\text{woman}),(\\text{uncle},\\text{aunt}),(\\text{king},\\text{queen})\\},\\; \\text{方法 }M = \\text{variance-max}\n    $$\n    $$\n    \\text{用例 }2:\\; \\text{再次使用词对 }P_1,\\; \\text{方法 }M = \\text{mean-diff}\n    $$\n    $$\n    \\text{用例 }3:\\; \\text{词对 }P_2 = \\{(\\text{he},\\text{she}),(\\text{doctor},\\text{nurse})\\},\\; \\text{方法 }M = \\text{variance-max}\n    $$\n    此处，$\\text{variance-max}$ 指的是最大化中心化 $\\{d_i\\}$ 经验方差的单位方向 $b$，而 $\\text{mean-diff}$ 指的是平均差分 $\\bar{d} = \\dfrac{1}{n}\\sum_{i=1}^n d_i$ 的单位归一化。\n- 对于每个用例，计算并记录一个包含 $6$ 个浮点数的列表：\n    $$\n    \\big[\\, B_{\\text{before}},\\, B_{\\text{after}},\\, B_{\\text{reduction}},\\, \\text{Acc}_{\\text{before}},\\, \\text{Acc}_{\\text{after}},\\, \\Delta \\text{Acc}\\,\\big]\n    $$\n    其中 $B_{\\text{reduction}} = B_{\\text{before}} - B_{\\text{after}}$ 且 $\\Delta \\text{Acc} = \\text{Acc}_{\\text{after}} - \\text{Acc}_{\\text{before}}$。准确率必须表示为 $[\\,0,\\,1\\,]$ 范围内的小数。\n\n最终输出规范：\n\n- 你的程序应生成单行输出，其中包含三个测试用例的结果，形式为一个以逗号分隔的列表的列表，并用方括号括起来，格式与 $[\\,\\text{case}_1,\\text{case}_2,\\text{case}_3\\,]$ 完全一致，\n  其中每个 $\\text{case}_i$ 是其自己的包含 $6$ 个浮点数的列表，顺序如上所述。不应打印任何其他文本。", "solution": "用户提供的问题陈述经过严格审查，被确定为**有效**的。这是一个在计算语言学和线性代数领域中定义明确的问题，它基于评估词嵌入的标准定义和既定原则。该问题是自包含的，所有必要的数据和程序都已明确提供。尽管合成数据集的构建方式导致所有三个测试用例得出相同的结果，但这构成了该问题的一个特定的、可发现的特征，而不是一个使其无效的缺陷。\n\n解决方案通过为三个测试用例中的每一个系统地执行所需的计算来进行。\n\n### 基本设置\n\n给定了词汇表 $V$ 和相应的词嵌入 $x_w \\in \\mathbb{R}^4$。我们将它们表示为向量：\n- $x_{\\text{man}} = [0.8, 0.0, 0.1, 1.0]^\\top$\n- $x_{\\text{woman}} = [0.8, 0.0, 0.1, -1.0]^\\top$\n- $x_{\\text{he}} = [0.5, 0.0, 0.0, 1.0]^\\top$\n- $x_{\\text{she}} = [0.5, 0.0, 0.0, -1.0]^\\top$\n- $x_{\\text{king}} = [0.9, 1.0, 0.0, 0.8]^\\top$\n- $x_{\\text{queen}} = [0.9, 1.0, 0.0, -0.8]^\\top$\n- $x_{\\text{uncle}} = [0.7, 0.0, 0.0, 0.9]^\\top$\n- $x_{\\text{aunt}} = [0.7, 0.0, 0.0, -0.9]^\\top$\n- $x_{\\text{doctor}} = [0.85, 0.0, 1.0, 0.2]^\\top$\n- $x_{\\text{nurse}} = [0.85, 0.0, 1.0, -0.2]^\\top$\n- $x_{\\text{child}} = [0.6, 0.0, 0.0, 0.0]^\\top$\n\n中性词集为 $S = \\{\\text{doctor},\\text{nurse},\\text{child}\\}$。类比集 $A$ 由 $6$ 个指定的四元组组成。每个用例所需的输出为 $[\\, B_{\\text{before}},\\, B_{\\text{after}},\\, B_{\\text{reduction}},\\, \\text{Acc}_{\\text{before}},\\, \\text{Acc}_{\\text{after}},\\, \\Delta \\text{Acc}\\,]$。\n\n### 用例 1：词对 $P_1$，方法 `variance-max`\n\n**1. 计算偏置方向 $b$**\n性别词对集为 $P_1 = \\{(\\text{he},\\text{she}),(\\text{man},\\text{woman}),(\\text{uncle},\\text{aunt}),(\\text{king},\\text{queen})\\}$。我们首先计算差分向量 $d_i = x_{m_i} - x_{f_i}$：\n- $d_1 = x_{\\text{he}} - x_{\\text{she}} = [0.0, 0.0, 0.0, 2.0]^\\top$\n- $d_2 = x_{\\text{man}} - x_{\\text{woman}} = [0.0, 0.0, 0.0, 2.0]^\\top$\n- $d_3 = x_{\\text{uncle}} - x_{\\text{aunt}} = [0.0, 0.0, 0.0, 1.8]^\\top$\n- $d_4 = x_{\\text{king}} - x_{\\text{queen}} = [0.0, 0.0, 0.0, 1.6]^\\top$\n\n平均差分向量为 $\\bar{d} = \\frac{1}{4}\\sum_{i=1}^4 d_i = [0.0, 0.0, 0.0, 1.85]^\\top$。\n接下来，我们找出中心化的差分向量 $c_i = d_i - \\bar{d}$：\n- $c_1 = [0.0, 0.0, 0.0, 0.15]^\\top$\n- $c_2 = [0.0, 0.0, 0.0, 0.15]^\\top$\n- $c_3 = [0.0, 0.0, 0.0, -0.05]^\\top$\n- $c_4 = [0.0, 0.0, 0.0, -0.25]^\\top$\n\n`variance-max` 方法要求找到这些中心化向量的第一主成分。这是协方差矩阵 $C = \\sum_{i=1}^4 c_i c_i^\\top$ 的最大特征值对应的特征向量。\n由于所有 $c_i$ 向量仅在第四个分量上非零，矩阵 $C$ 将仅在 $C_{4,4}$ 处有一个非零项：\n$$C_{4,4} = (0.15)^2 + (0.15)^2 + (-0.05)^2 + (-0.25)^2 = 0.0225 + 0.0225 + 0.0025 + 0.0625 = 0.11$$\n协方差矩阵为 $C = \\text{diag}(0, 0, 0, 0.11)$。其特征值是其对角线元素。最大特征值为 $\\lambda_{max} = 0.11$，相应的特征向量是标准基向量 $e_4$。\n因此，偏置方向为 $b = [0.0, 0.0, 0.0, 1.0]^\\top$。\n\n**2. 计算初始指标 ($B_{\\text{before}}, \\text{Acc}_{\\text{before}}$)**\n去偏置前的偏置分数为 $B_{\\text{before}} = \\frac{1}{|S|}\\sum_{w \\in S} \\big| x_w^\\top b \\big|$。当 $b = [0,0,0,1]^\\top$ 时，$x_w^\\top b$ 就是 $x_w$ 的第四个分量。\n- $|x_{\\text{doctor}}^\\top b| = |0.2| = 0.2$\n- $|x_{\\text{nurse}}^\\top b| = |-0.2| = 0.2$\n- $|x_{\\text{child}}^\\top b| = |0.0| = 0.0$\n$B_{\\text{before}} = \\frac{1}{3}(0.2 + 0.2 + 0.0) = \\frac{0.4}{3} \\approx 0.133333$。\n\n类比准确率 $\\text{Acc}_{\\text{before}}$ 是 6 个类比中被正确预测的比例。数值计算表明，6 个类比中有 4 个被正确预测，得出 $\\text{Acc}_{\\text{before}} = 4/6 \\approx 0.666667$。\n\n**3. 去偏置与最终指标 ($B_{\\text{after}}, \\text{Acc}_{\\text{after}}$)**\n我们使用零空间投影 $x'_w = x_w - (x_w^\\top b)b$ 对所有向量进行去偏置。当 $b = e_4$ 时，此操作仅将每个向量的第四个分量设置为 $0$。\n去偏置后的偏置是在新向量 $x'_w$ 上计算的。根据构造，对于所有 $w$，都有 $(x'_w)^\\top b = 0$。\n$B_{\\text{after}} = \\frac{1}{|S|}\\sum_{w \\in S} \\big| (x'_w)^\\top b \\big| = \\frac{1}{3}(0+0+0) = 0.0$。\n偏置减少量为 $B_{\\text{reduction}} = B_{\\text{before}} - B_{\\text{after}} = \\frac{0.4}{3}$。\n\n去偏置后，性别词对变得相同（例如，$x'_{\\text{man}} = x'_{\\text{woman}} = [0.8, 0.0, 0.1, 0.0]^\\top$）。这简化了类比任务 $x_b - x_a + x_c$。例如，在 $(\\text{king},\\text{queen},\\text{man},\\text{woman})$ 中，目标变为 $x'_{\\text{queen}} - x'_{\\text{king}} + x'_{\\text{man}} = \\vec{0} + x'_{\\text{man}} = x'_{\\text{man}}$。由于 $x'_{\\text{man}} = x'_{\\text{woman}}$，预测的词是 `woman`，这是正确的。此模式适用于所有 6 个类比，因此 $\\text{Acc}_{\\text{after}} = 1.0$。\n准确率的变化是 $\\Delta \\text{Acc} = \\text{Acc}_{\\text{after}} - \\text{Acc}_{\\text{before}} = 1.0 - 4/6 = 1/3 \\approx 0.333333$。\n\n**用例 1 的结果：** $[\\,0.133333, 0.0, 0.133333, 0.666667, 1.0, 0.333333\\,]$\n\n### 用例 2：词对 $P_1$，方法 `mean-diff`\n\n**1. 计算偏置方向 $b$**\n词对 $P_1$ 与用例 1 中的相同。`mean-diff` 方法要求对平均差分向量 $\\bar{d}$ 进行归一化。从用例 1 中，我们有 $\\bar{d} = [0.0, 0.0, 0.0, 1.85]^\\top$。\n范数为 $\\lVert\\bar{d}\\rVert = 1.85$。\n偏置方向为 $b = \\frac{\\bar{d}}{\\lVert\\bar{d}\\rVert} = \\frac{1}{1.85}[0.0, 0.0, 0.0, 1.85]^\\top = [0.0, 0.0, 0.0, 1.0]^\\top$。\n\n**2. 比较与结果**\n计算出的偏置方向 $b$ 与用例 1 中找到的相同。这是因为 $P_1$ 中所有词对的差分向量 $d_i$ 都是共线的，这是一个特殊的结果。由于 $b$ 相同，所有后续计算（$B_{\\text{before}}$, $\\text{Acc}_{\\text{before}}$, 去偏置, $B_{\\text{after}}$, $\\text{Acc}_{\\text{after}}$）必然与用例 1 中的计算相同。\n\n**用例 2 的结果：** $[\\,0.133333, 0.0, 0.133333, 0.666667, 1.0, 0.333333\\,]$\n\n### 用例 3：词对 $P_2$，方法 `variance-max`\n\n**1. 计算偏置方向 $b$**\n性别词对集为 $P_2 = \\{(\\text{he},\\text{she}),(\\text{doctor},\\text{nurse})\\}$。差分向量为：\n- $d_1 = x_{\\text{he}} - x_{\\text{she}} = [0.0, 0.0, 0.0, 2.0]^\\top$\n- $d_2 = x_{\\text{doctor}} - x_{\\text{nurse}} = [0.0, 0.0, 0.0, 0.4]^\\top$\n\n平均差分为 $\\bar{d} = \\frac{1}{2}(d_1+d_2) = [0.0, 0.0, 0.0, 1.2]^\\top$。中心化向量为：\n- $c_1 = d_1 - \\bar{d} = [0.0, 0.0, 0.0, 0.8]^\\top$\n- $c_2 = d_2 - \\bar{d} = [0.0, 0.0, 0.0, -0.8]^\\top$\n\n协方差矩阵为 $C = c_1 c_1^\\top + c_2 c_2^\\top$。同样，只有 $C_{4,4}$ 非零：\n$$C_{4,4} = (0.8)^2 + (-0.8)^2 = 0.64 + 0.64 = 1.28$$\n协方差矩阵为 $C = \\text{diag}(0, 0, 0, 1.28)$。主特征向量同样是 $e_4$。\n因此，偏置方向为 $b = [0.0, 0.0, 0.0, 1.0]^\\top$。\n\n**2. 比较与结果**\n计算出的偏置方向 $b$ 再次与之前用例中找到的相同。这是因为 $P_2$ 中词对的差分向量也与第四个基向量共线。由于 $b$ 未改变，所有结果指标都与用例 1 和用例 2 的指标相同。\n\n**用例 3 的结果：** $[\\,0.133333, 0.0, 0.133333, 0.666667, 1.0, 0.333333\\,]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the word embedding bias evaluation problem for three test cases.\n    \"\"\"\n    \n    # Vocabulary and embeddings\n    embeddings_data = {\n        \"man\":    [0.8, 0.0, 0.1, 1.0],\n        \"woman\":  [0.8, 0.0, 0.1, -1.0],\n        \"he\":     [0.5, 0.0, 0.0, 1.0],\n        \"she\":    [0.5, 0.0, 0.0, -1.0],\n        \"king\":   [0.9, 1.0, 0.0, 0.8],\n        \"queen\":  [0.9, 1.0, 0.0, -0.8],\n        \"uncle\":  [0.7, 0.0, 0.0, 0.9],\n        \"aunt\":   [0.7, 0.0, 0.0, -0.9],\n        \"doctor\": [0.85, 0.0, 1.0, 0.2],\n        \"nurse\":  [0.85, 0.0, 1.0, -0.2],\n        \"child\":  [0.6, 0.0, 0.0, 0.0],\n    }\n    \n    vocab = list(embeddings_data.keys())\n    embeddings = {k: np.array(v) for k, v in embeddings_data.items()}\n\n    # Neutral set and analogy set\n    neutral_set = [\"doctor\", \"nurse\", \"child\"]\n    analogies = [\n        (\"king\", \"queen\", \"man\", \"woman\"),\n        (\"uncle\", \"aunt\", \"man\", \"woman\"),\n        (\"he\", \"she\", \"man\", \"woman\"),\n        (\"king\", \"man\", \"queen\", \"woman\"),\n        (\"doctor\", \"nurse\", \"man\", \"woman\"),\n        (\"man\", \"woman\", \"king\", \"queen\"),\n    ]\n\n    # Test cases definition\n    test_cases = [\n        {\n            \"pairs\": [(\"he\", \"she\"), (\"man\", \"woman\"), (\"uncle\", \"aunt\"), (\"king\", \"queen\")],\n            \"method\": \"variance-max\"\n        },\n        {\n            \"pairs\": [(\"he\", \"she\"), (\"man\", \"woman\"), (\"uncle\", \"aunt\"), (\"king\", \"queen\")],\n            \"method\": \"mean-diff\"\n        },\n        {\n            \"pairs\": [(\"he\", \"she\"), (\"doctor\", \"nurse\")],\n            \"method\": \"variance-max\"\n        }\n    ]\n\n    def cos_sim(u, v):\n        norm_u = np.linalg.norm(u)\n        norm_v = np.linalg.norm(v)\n        if norm_u == 0 or norm_v == 0:\n            return 0.0\n        return np.dot(u, v) / (norm_u * norm_v)\n\n    def get_bias_direction(pairs, method, embeddings_dict):\n        diff_vectors = [embeddings_dict[m] - embeddings_dict[f] for m, f in pairs]\n        \n        if method == \"mean-diff\":\n            mean_diff = np.mean(diff_vectors, axis=0)\n            return mean_diff / np.linalg.norm(mean_diff)\n        \n        elif method == \"variance-max\":\n            mean_diff = np.mean(diff_vectors, axis=0)\n            centered_diffs = [d - mean_diff for d in diff_vectors]\n            # Covariance matrix\n            cov_matrix = np.zeros((centered_diffs[0].shape[0], centered_diffs[0].shape[0]))\n            for c in centered_diffs:\n                cov_matrix += np.outer(c, c)\n            \n            # Eigen decomposition to find the principal component\n            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n            # The direction is the eigenvector of the largest eigenvalue\n            bias_direction = eigenvectors[:, np.argmax(eigenvalues)]\n            return bias_direction\n\n    def calculate_bias_score(neutral_words, embeddings_dict, b):\n        score = 0\n        for word in neutral_words:\n            x_w = embeddings_dict[word]\n            score += np.abs(np.dot(x_w, b))\n        return score / len(neutral_words)\n\n    def evaluate_analogies(analogy_set, embeddings_dict, vocab_list):\n        correct_predictions = 0\n        for a, b, c, d_true in analogy_set:\n            if a not in embeddings_dict or b not in embeddings_dict or c not in embeddings_dict:\n                continue\n\n            target_vec = embeddings_dict[b] - embeddings_dict[a] + embeddings_dict[c]\n            \n            best_word = None\n            max_sim = -np.inf\n            \n            candidate_words = [w for w in vocab_list if w not in [a, b, c]]\n            \n            for word_candidate in candidate_words:\n                sim = cos_sim(embeddings_dict[word_candidate], target_vec)\n                if sim  max_sim:\n                    max_sim = sim\n                    best_word = word_candidate\n            \n            if best_word == d_true:\n                correct_predictions += 1\n        \n        return correct_predictions / len(analogy_set)\n\n    def debias_embeddings(embeddings_dict, b):\n        debiased = {}\n        for word, vec in embeddings_dict.items():\n            projection = np.dot(vec, b) * b\n            debiased[word] = vec - projection\n        return debiased\n\n    all_results = []\n\n    for case in test_cases:\n        # 1. Compute bias direction\n        b = get_bias_direction(case[\"pairs\"], case[\"method\"], embeddings)\n\n        # 2. Compute metrics before debiasing\n        b_before = calculate_bias_score(neutral_set, embeddings, b)\n        acc_before = evaluate_analogies(analogies, embeddings, vocab)\n\n        # 3. Debias embeddings\n        debiased_embeddings = debias_embeddings(embeddings, b)\n\n        # 4. Compute metrics after debiasing\n        b_after = calculate_bias_score(neutral_set, debiased_embeddings, b)\n        acc_after = evaluate_analogies(analogies, debiased_embeddings, vocab)\n\n        # 5. Consolidate results\n        b_reduction = b_before - b_after\n        delta_acc = acc_after - acc_before\n        \n        case_results = [b_before, b_after, b_reduction, acc_before, acc_after, delta_acc]\n        all_results.append(case_results)\n    \n    # Using np.round for consistent float representation in the final output\n    formatted_results = [\n        \"[\" + \",\".join([f\"{val:.6f}\" for val in res]) + \"]\" \n        for res in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3123006"}]}