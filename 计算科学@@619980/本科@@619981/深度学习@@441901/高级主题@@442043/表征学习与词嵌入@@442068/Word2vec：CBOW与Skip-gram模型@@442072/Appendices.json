{"hands_on_practices": [{"introduction": "要真正理解像 Word2vec 这样的模型，最好的方法就是亲自动手进行计算。这个练习将学习过程分解为最小的单元：对一个中心词和上下文词对进行单步随机梯度下降（SGD）更新。通过这个计算 [@problem_id:3200045]，你将清晰地看到 Skip-gram 模型如何通过“推拉”动力学来学习——将正样本对（实际的词-上下文对）的向量在向量空间中拉近，同时将负样本对的向量推远。", "problem": "考虑一个词汇表为 $\\{a,b,c\\}$ 的玩具语料库，它包含两个句子：$a\\; b\\; a$ 和 $b\\; c\\; b$。在此语料库上训练一个带有负采样的 skip-gram Word2Vec 模型。使用以下设置。\n\n- 使用大小为 $1$ 的对称上下文窗口。关注由第一个句子 $a\\; b\\; a$ 中位置 2 的中心词 $b$ 和位置 1 的上下文词 $a$ 构成的单个训练实例。因此，正样本对是 $(b \\to a)$。\n- 使用负采样，其中 $k=2$ 个负样本从词汇表的均匀分布中抽取，但在此步骤中，将负样本确定性地固定为 $\\{b,c\\}$（按此顺序）。\n- 嵌入维度为 $d=2$。输入（中心）嵌入和输出（上下文）嵌入是分开的。初始输入嵌入为\n$$\nu_a=\\begin{pmatrix}0\\\\0\\end{pmatrix},\\quad\nu_b=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\nu_c=\\begin{pmatrix}0\\\\1\\end{pmatrix},\n$$\n初始输出嵌入为\n$$\nv_a=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\quad\nv_b=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\nv_c=\\begin{pmatrix}0\\\\-1\\end{pmatrix}.\n$$\n- 使用学习率为 $\\alpha=0.2$ 的随机梯度下降（SGD; Stochastic Gradient Descent）。\n\n对于带有负样本 $\\{b,c\\}$ 的实例 $(b \\to a)$ 的一个训练步骤，使用单实例负采样损失\n$$\n\\ell\\!\\left(u_b, v_a, \\{v_b, v_c\\}\\right)\\;=\\;-\\ln \\sigma\\!\\left(v_a^{\\top} u_b\\right)\\;-\\;\\sum_{n\\in\\{b,c\\}} \\ln \\sigma\\!\\left(-\\,v_n^{\\top} u_b\\right),\n$$\n其中 $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ 是逻辑 sigmoid 函数。从上述初始嵌入开始，基于此单个实例对 $u_b$、$v_a$、$v_b$ 和 $v_c$ 执行一次 SGD 更新（所有其他参数保持不变）。使用第一性原理，从给定的损失和 $\\sigma(x)$ 的定义中推导出必要的梯度。\n\n经过这单步 SGD 后，输入向量 $u_b$ 的第一个分量的更新值是多少？将你的答案四舍五入到四位有效数字。\n\n此外，在你的演算过程中，简要解释更新的符号和大小如何反映正样本对和负样本之间的推拉动态。你最终报告的答案必须仅为所要求的标量值。", "solution": "该问题要求我们对中心词 $b$ 的输入向量 $u_b$ 执行一步随机梯度下降（SGD）更新。更新规则是 $u_b^{\\text{new}} = u_b - \\alpha \\frac{\\partial \\ell}{\\partial u_b}$，其中 $\\alpha=0.2$ 是学习率，$\\ell$ 是给定的损失函数。\n\n**1. 推导损失函数关于 $u_b$ 的梯度**\n\n损失函数为：\n$$\n\\ell = -\\ln \\sigma(v_a^{\\top} u_b) - \\ln \\sigma(-v_b^{\\top} u_b) - \\ln \\sigma(-v_c^{\\top} u_b)\n$$\n我们利用对数 sigmoid 函数的导数性质：\n$\\frac{d}{dx}\\ln\\sigma(x) = 1-\\sigma(x)$ 和 $\\frac{d}{dx}\\ln\\sigma(-x) = -\\sigma(x)$。\n应用链式法则，我们得到 $\\ell$ 关于 $u_b$ 的梯度：\n\\begin{align*}\n\\frac{\\partial \\ell}{\\partial u_b} &= - \\frac{\\partial}{\\partial u_b}\\left( \\ln\\sigma(v_a^{\\top} u_b) \\right) - \\frac{\\partial}{\\partial u_b}\\left( \\ln\\sigma(-v_b^{\\top} u_b) \\right) - \\frac{\\partial}{\\partial u_b}\\left( \\ln\\sigma(-v_c^{\\top} u_b) \\right) \\\\\n&= - (1-\\sigma(v_a^{\\top} u_b))v_a - (-\\sigma(v_b^{\\top} u_b))v_b - (-\\sigma(v_c^{\\top} u_b))v_c \\\\\n&= (\\sigma(v_a^{\\top} u_b) - 1)v_a + \\sigma(v_b^{\\top} u_b)v_b + \\sigma(v_c^{\\top} u_b)v_c\n\\end{align*}\n这个梯度是更新 $u_b$ 的方向。SGD 更新规则是 $u_b \\leftarrow u_b - \\alpha \\frac{\\partial \\ell}{\\partial u_b}$，等价于 $u_b \\leftarrow u_b + \\alpha \\left( (1 - \\sigma(v_a^{\\top} u_b))v_a - \\sigma(v_b^{\\top} u_b)v_b - \\sigma(v_c^{\\top} u_b)v_c \\right)$。这清晰地展示了“推拉”动态：$u_b$ 被正样本上下文向量 $v_a$ “拉”近（系数 $1-\\sigma$ 为正），同时被负样本上下文向量 $v_b$ 和 $v_c$ “推”远（系数 $-\\sigma$ 为负）。\n\n**2. 计算初始点积和 Sigmoid 值**\n\n使用给定的初始向量：\n- 中心词输入向量：$u_b = \\begin{pmatrix}1\\\\0\\end{pmatrix}$\n- 正样本输出向量：$v_a = \\begin{pmatrix}0\\\\1\\end{pmatrix}$\n- 负样本输出向量：$v_b = \\begin{pmatrix}1\\\\0\\end{pmatrix}$, $v_c = \\begin{pmatrix}0\\\\-1\\end{pmatrix}$\n\n计算点积：\n- 正样本：$v_a^{\\top} u_b = \\begin{pmatrix}0 & 1\\end{pmatrix}\\begin{pmatrix}1\\\\0\\end{pmatrix} = 0$\n- 负样本 1：$v_b^{\\top} u_b = \\begin{pmatrix}1 & 0\\end{pmatrix}\\begin{pmatrix}1\\\\0\\end{pmatrix} = 1$\n- 负样本 2：$v_c^{\\top} u_b = \\begin{pmatrix}0 & -1\\end{pmatrix}\\begin{pmatrix}1\\\\0\\end{pmatrix} = 0$\n\n计算相应的 sigmoid 值：\n- $\\sigma(v_a^{\\top} u_b) = \\sigma(0) = \\frac{1}{1+e^0} = 0.5$\n- $\\sigma(v_b^{\\top} u_b) = \\sigma(1) = \\frac{1}{1+e^{-1}} \\approx 0.731058$\n- $\\sigma(v_c^{\\top} u_b) = \\sigma(0) = 0.5$\n\n**3. 计算梯度向量**\n\n将 sigmoid 值代入梯度公式：\n\\begin{align*}\n\\frac{\\partial \\ell}{\\partial u_b} &= (\\sigma(0) - 1)v_a + \\sigma(1)v_b + \\sigma(0)v_c \\\\\n&= (0.5 - 1)\\begin{pmatrix}0\\\\1\\end{pmatrix} + 0.731058\\begin{pmatrix}1\\\\0\\end{pmatrix} + 0.5\\begin{pmatrix}0\\\\-1\\end{pmatrix} \\\\\n&= -0.5\\begin{pmatrix}0\\\\1\\end{pmatrix} + 0.731058\\begin{pmatrix}1\\\\0\\end{pmatrix} + 0.5\\begin{pmatrix}0\\\\-1\\end{pmatrix} \\\\\n&= \\begin{pmatrix}0 \\\\ -0.5\\end{pmatrix} + \\begin{pmatrix}0.731058 \\\\ 0\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -0.5\\end{pmatrix} \\\\\n&= \\begin{pmatrix}0.731058 \\\\ -1.0\\end{pmatrix}\n\\end{align*}\n\n**4. 执行 SGD 更新**\n\n应用 SGD 更新规则：\n\\begin{align*}\nu_b^{\\text{new}} &= u_b - \\alpha \\frac{\\partial \\ell}{\\partial u_b} \\\\\n&= \\begin{pmatrix}1\\\\0\\end{pmatrix} - 0.2 \\begin{pmatrix}0.731058 \\\\ -1.0\\end{pmatrix} \\\\\n&= \\begin{pmatrix}1\\\\0\\end{pmatrix} - \\begin{pmatrix}0.1462116 \\\\ -0.2\\end{pmatrix} \\\\\n&= \\begin{pmatrix}1 - 0.1462116 \\\\ 0 - (-0.2)\\end{pmatrix} \\\\\n&= \\begin{pmatrix}0.8537884 \\\\ 0.2\\end{pmatrix}\n\\end{align*}\n\n**5. 提取最终答案**\n\n更新后的输入向量 $u_b$ 的第一个分量是 $0.8537884$。四舍五入到四位有效数字为 $0.8538$。", "answer": "$$\\boxed{0.8538}$$", "id": "3200045"}, {"introduction": "与从中心词预测上下文的 Skip-gram 模型不同，连续词袋（CBOW）模型反其道而行之，它利用上下文来预测中心词。这个练习 [@problem_id:3200079] 的核心在于理解 CBOW 如何首先将多个上下文词的向量聚合成一个单一的表示，然后利用这个聚合表示来更新参与其中的上下文词向量。这有助于你掌握 CBOW 模型中信息是如何从上下文流向目标的。", "problem": "考虑 Word2Vec 中的连续词袋 (CBOW) 模型。设上下文集合为 $C=\\{c_1,\\dots,c_{|C|}\\}$，其中每个上下文词 $c_i$ 都有一个输入嵌入向量 $v_{c_i}\\in\\mathbb{R}^d$。CBOW 的隐藏表示是平均值\n$$\nh=\\frac{1}{|C|}\\sum_{i=1}^{|C|}v_{c_i}.\n$$\n对于一个目标词 $w$，其输出嵌入向量为 $u_w\\in\\mathbb{R}^d$，定义分数 $s=u_w^{\\top}h$ 和预测概率 $\\hat{y}=\\sigma(s)$，其中 $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ 是 logistic sigmoid 函数。对于一个标签对 $(C,w,y)$，其中 $y\\in\\{0,1\\}$，需要最小化的目标是二元交叉熵\n$$\nJ(C,w,y)=-\\Big(y\\,\\ln(\\sigma(s))+(1-y)\\,\\ln\\big(1-\\sigma(s)\\big)\\Big).\n$$\n对于一个对数据对 $\\{(C^{(b)},w^{(b)},y^{(b)})\\}_{b=1}^B$ 的损失 $J$进行求和的小批量 (mini-batch)，关于每个 $v_{c_i}$ 的梯度是每个数据对梯度的总和。\n\n任务：\n1. 从以上定义和基本微积分出发，推导单个数据对 $(C,w,y)$ 的梯度 $\\frac{\\partial J}{\\partial v_{c_i}}$，并用 $u_w$、$h$、$\\sigma(u_w^{\\top}h)$、$y$ 和 $|C|$ 来表示。\n2. 然后，考虑以下具有共享上下文的特定小批量（$d=3$ 且 $|C|=2$）：\n   - 上下文嵌入：$v_{c_1}=\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix}$ 和 $v_{c_2}=\\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}$，因此该小批量中的所有数据对都使用相同的 $h$。\n   - 一个正样本对 $(y=1)$，其目标输出嵌入为 $u_{w^{(+)}}=\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix}$。\n   - 两个负样本对 $(y=0)$，其目标输出嵌入分别为 $u_{w^{(-1)}}=\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix}$ 和 $u_{w^{(-2)}}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$。\n   关于 $v_{c_1}$ 的小批量梯度是在第一部分中得到的三个数据对梯度的总和，每个梯度都在相应的 $y$ 和 $u_w$ 下计算。\n3. 设 $d=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$。计算此小批量的标量内积 $d^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right)$。将最终数值答案四舍五入到四位有效数字。\n\n你的最终答案必须是一个实数。", "solution": "该问题要求进行与连续词袋（CBOW）模型中梯度相关的三部分计算。首先，我们必须推导损失函数关于上下文词输入嵌入的梯度的一般形式。其次，我们将此形式应用于一个特定的小批量。第三，我们计算所得小批量梯度的标量投影。\n\n**第1部分：梯度 $\\frac{\\partial J}{\\partial v_{c_i}}$ 的推导**\n\n对于单个训练数据对 $(C,w,y)$，目标函数是二元交叉熵损失：\n$$\nJ(C,w,y) = -\\Big(y \\ln(\\sigma(s)) + (1-y) \\ln(1-\\sigma(s))\\Big)\n$$\n其中分数 $s$ 定义为 $s = u_w^{\\top} h$，隐藏表示 $h$ 是上下文词向量的平均值：\n$$\nh = \\frac{1}{|C|} \\sum_{j=1}^{|C|} v_{c_j}\n$$\n我们需要求 $J$ 关于其中一个上下文词向量 $v_{c_i} \\in \\mathbb{R}^d$ 的梯度。我们使用微积分的链式法则。梯度 $\\frac{\\partial J}{\\partial v_{c_i}}$ 是一个向量，其第 $k$ 个分量是 $\\frac{\\partial J}{\\partial v_{c_{i,k}}}$，其中 $v_{c_{i,k}}$ 是向量 $v_{c_i}$ 的第 $k$ 个分量。\n\n我们应用链式法则：\n$$\n\\frac{\\partial J}{\\partial v_{c_{i,k}}} = \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial v_{c_{i,k}}}\n$$\n首先，我们计算损失 $J$ 关于分数 $s$ 的导数。我们利用 sigmoid 函数的导数是 $\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1-\\sigma(x))$ 这一事实。\n\\begin{align*}\n\\frac{\\partial J}{\\partial s} &= -\\frac{\\partial}{\\partial s} \\Big(y \\ln(\\sigma(s)) + (1-y) \\ln(1-\\sigma(s))\\Big) \\\\\n&= -\\left( y \\frac{1}{\\sigma(s)} \\frac{d\\sigma(s)}{ds} + (1-y) \\frac{1}{1-\\sigma(s)} \\left(-\\frac{d\\sigma(s)}{ds}\\right) \\right) \\\\\n&= -\\left( y \\frac{1}{\\sigma(s)} \\sigma(s)(1-\\sigma(s)) - (1-y) \\frac{1}{1-\\sigma(s)} \\sigma(s)(1-\\sigma(s)) \\right) \\\\\n&= -\\big( y(1-\\sigma(s)) - (1-y)\\sigma(s) \\big) \\\\\n&= -(y - y\\sigma(s) - \\sigma(s) + y\\sigma(s)) \\\\\n&= -(y - \\sigma(s)) \\\\\n&= \\sigma(s) - y\n\\end{align*}\n对于带有最终 sigmoid 激活的二元交叉熵损失，这是一个标准结果。\n\n接下来，我们计算分数 $s$ 关于向量 $v_{c_i}$ 的第 $k$ 个分量的导数。分数 $s$ 可以用分量表示为：\n$$\ns = u_w^{\\top} h = \\sum_{j=1}^d u_{w,j} h_j = \\sum_{j=1}^d u_{w,j} \\left( \\frac{1}{|C|} \\sum_{l=1}^{|C|} v_{c_{l,j}} \\right)\n$$\n现在我们对 $v_{c_{i,k}}$ 求导：\n$$\n\\frac{\\partial s}{\\partial v_{c_{i,k}}} = \\frac{\\partial}{\\partial v_{c_{i,k}}} \\left( \\sum_{j=1}^d u_{w,j} \\frac{1}{|C|} \\sum_{l=1}^{|C|} v_{c_{l,j}} \\right)\n$$\n导数 $\\frac{\\partial v_{c_{l,j}}}{\\partial v_{c_{i,k}}}$ 在 $l=i$ 且 $j=k$ 时为 $1$，否则为 $0$。因此，关于此微分，求和中只有一项是非零的。\n$$\n\\frac{\\partial s}{\\partial v_{c_{i,k}}} = u_{w,k} \\frac{1}{|C|} (1) = \\frac{1}{|C|} u_{w,k}\n$$\n结合这两个部分：\n$$\n\\frac{\\partial J}{\\partial v_{c_{i,k}}} = \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial v_{c_{i,k}}} = (\\sigma(s)-y) \\frac{1}{|C|} u_{w,k}\n$$\n为了构成梯度向量 $\\frac{\\partial J}{\\partial v_{c_i}}$，我们将这些分量组合起来（$k=1, \\dots, d$）：\n$$\n\\frac{\\partial J}{\\partial v_{c_i}} = \\begin{pmatrix} \\frac{\\partial J}{\\partial v_{c_{i,1}}} \\\\ \\vdots \\\\ \\frac{\\partial J}{\\partial v_{c_{i,d}}} \\end{pmatrix} = \\frac{\\sigma(s)-y}{|C|} \\begin{pmatrix} u_{w,1} \\\\ \\vdots \\\\ u_{w,d} \\end{pmatrix} = \\frac{\\sigma(u_w^{\\top}h)-y}{|C|} u_w\n$$\n这就完成了任务的第一部分。\n\n**第2和第3部分：小批量梯度计算和投影**\n\n问题要求计算标量值 $d^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right)$。小批量梯度是该批次中三个数据对各自梯度的总和。设数据对由 $b \\in \\{ (+), (-1), (-2) \\}$ 索引。\n$$\n\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}} = \\sum_{b} \\frac{\\partial J^{(b)}}{\\partial v_{c_1}}\n$$\n使用第1部分的结果，我们有：\n$$\n\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}} = \\sum_{b} \\frac{\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}}{|C|} u_w^{(b)}\n$$\n我们想要计算这个梯度与向量 $d$ 的内积：\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = d^{\\top} \\sum_{b} \\frac{\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}}{|C|} u_w^{(b)}\n$$\n根据内积的线性性质，我们可以写出：\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = \\frac{1}{|C|} \\sum_{b} \\left(\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}\\right) (d^{\\top}u_w^{(b)})\n$$\n首先，我们计算共享的隐藏表示 $h$。\n给定 $|C|=2$，$v_{c_1}=\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix}$ 和 $v_{c_2}=\\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}$：\n$$\nh = \\frac{1}{2}(v_{c_1} + v_{c_2}) = \\frac{1}{2}\\left(\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix} + \\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}\\right) = \\frac{1}{2}\\begin{pmatrix}1\\\\-1\\\\3\\end{pmatrix} = \\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix}\n$$\n接下来，我们计算小批量中每个数据对的项。\n\n1.  **正样本对 (+):** $y^{(+)}=1$, $u_{w^{(+)}}=\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix}$\n    - 分数 $s^{(+)} = {u_{w^{(+)}}}^{\\top}h = \\begin{pmatrix}1  2  -1\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = 0.5 - 1.0 - 1.5 = -2$。\n    - 与 $d$ 的内积：$d^{\\top}u_{w^{(+)}} = \\begin{pmatrix}1  0  -1\\end{pmatrix}\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix} = 1+0+1 = 2$。\n    - 对总和的贡献：$(\\sigma(-2)-1)(2)$。\n\n2.  **第一个负样本对 (-1):** $y^{(-1)}=0$, $u_{w^{(-1)}}=\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix}$\n    - 分数 $s^{(-1)} = {u_{w^{(-1)}}}^{\\top}h = \\begin{pmatrix}-1  0  1\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = -0.5+0+1.5 = 1$。\n    - 与 $d$ 的内积：$d^{\\top}u_{w^{(-1)}} = \\begin{pmatrix}1  0  -1\\end{pmatrix}\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix} = -1+0-1 = -2$。\n    - 对总和的贡献：$(\\sigma(1)-0)(-2) = -2\\sigma(1)$。\n\n3.  **第二个负样本对 (-2):** $y^{(-2)}=0$, $u_{w^{(-2)}}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$\n    - 分数 $s^{(-2)} = {u_{w^{(-2)}}}^{\\top}h = \\begin{pmatrix}0  1  0\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = 0-0.5+0 = -0.5$。\n    - 与 $d$ 的内积：$d^{\\top}u_{w^{(-2)}} = \\begin{pmatrix}1  0  -1\\end{pmatrix}\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix} = 0+0+0 = 0$。\n    - 对总和的贡献：$(\\sigma(-0.5)-0)(0) = 0$。\n\n现在，我们将这些贡献相加，并乘以 $\\frac{1}{|C|} = \\frac{1}{2}$：\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = \\frac{1}{2} \\Big[ (\\sigma(-2)-1)(2) - 2\\sigma(1) + 0 \\Big] = \\sigma(-2) - 1 - \\sigma(1)\n$$\n利用属性 $\\sigma(-x) = 1 - \\sigma(x)$，我们有 $\\sigma(-2) = 1-\\sigma(2)$。代入得：\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = (1-\\sigma(2)) - 1 - \\sigma(1) = -\\sigma(2) - \\sigma(1)\n$$\n现在我们计算数值：\n$$\n-\\sigma(2) - \\sigma(1) = -\\left( \\frac{1}{1+\\exp(-2)} + \\frac{1}{1+\\exp(-1)} \\right)\n$$\n我们使用数值 $e \\approx 2.71828$，$e^2 \\approx 7.38906$：\n- $\\exp(-2) \\approx 0.135335$\n- $\\exp(-1) \\approx 0.367879$\n$$\n\\sigma(2) = \\frac{1}{1+0.135335} \\approx \\frac{1}{1.135335} \\approx 0.880797\n$$\n$$\n\\sigma(1) = \\frac{1}{1+0.367879} \\approx \\frac{1}{1.367879} \\approx 0.731059\n$$\n最终值为：\n$$\n-0.880797 - 0.731059 = -1.611856\n$$\n四舍五入到四位有效数字得到 $-1.612$。", "answer": "$$\\boxed{-1.612}$$", "id": "3200079"}, {"introduction": "在掌握了 CBOW 的基本更新机制后，一个自然而然的问题是：简单地平均上下文词向量是最佳策略吗？这个练习 [@problem_id:3200013] 引导我们深入探讨这个问题，它揭示了标准 CBOW 模型中，高频词如何对上下文表示产生不成比例的影响，从而引入偏差。通过推导和计算一种重加权方案来修正这种偏差，你将体会到设计稳健的词嵌入模型时需要考虑的微妙之处。", "problem": "考虑 Word2Vec 的连续词袋模型 (CBOW) 公式，其中目标词的分数由内积 $v_{w}^{\\top} h$ 给出，其中 $v_{w} \\in \\mathbb{R}^{d}$ 是目标词的嵌入，而 $h \\in \\mathbb{R}^{d}$ 是聚合的上下文表示。设可能的上下文类型集合为 $C = \\{c_{1}, c_{2}, \\dots, c_{|C|}\\}$，其嵌入为 $v_{c_{i}} \\in \\mathbb{R}^{d}$，并令 $p(c_{i})$ 表示每种上下文类型的经验频率。假设从语料库中随机抽取的一个上下文词元其类型为 $c_{i}$ 的概率为 $p(c_{i})$，并且一个窗口内的上下文根据 $p$ 独立同分布。\n\n定义两种聚合方式：\n1. 上下文类型间的均等平均：$h_{\\mathrm{equal}} = \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}$。\n2. 按语料库分布的频率加权聚合：$h_{\\mathrm{freq}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, v_{c_{i}}$。\n\na) 仅使用内积和期望的线性性质，推导分数中偏向于高频上下文的偏差的通用符号表达式，\n$$\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} h_{\\mathrm{freq}} - v_{w}^{\\top} h_{\\mathrm{equal}}.$$\n\nb) 考虑一个具体实例，其中 $|C| = 4$，$d = 2$，目标向量 $v_{w} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}$，上下文嵌入\n$$v_{c_{1}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad v_{c_{2}} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}, \\quad v_{c_{3}} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, \\quad v_{c_{4}} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},$$\n以及经验频率\n$$p(c_{1}) = 0.4, \\quad p(c_{2}) = 0.3, \\quad p(c_{3}) = 0.2, \\quad p(c_{4}) = 0.1.$$\n计算 $\\Delta_{\\mathrm{bias}}(w)$ 的数值。\n\nc) 你决定通过一个关于上下文经验频率的函数 $\\alpha(c)$ 对上下文进行重加权，使得在语料库分布下的期望上下文表示与跨类型的均等平均表示相匹配，即\n$$\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} \\;=\\; \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}.$$\n求解重加权函数 $\\alpha(c)$（用 $p(c)$ 和 $|C|$ 表示），该函数需保证对于任意的 $\\{v_{c_{i}}\\}$，此等式均成立。\n\nd) 使用你推导出的重加权 $\\alpha(c)$，定义 $h_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}}$ 并为 b) 部分中的数值实例计算校正后的分数差\n$$\\Delta_{\\mathrm{corr}}(w) = v_{w}^{\\top} h_{\\mathrm{corr}} - v_{w}^{\\top} h_{\\mathrm{equal}}.$$\n将最终答案表示为包含 b) 和 d) 部分中两个量的单行矩阵，形式需精确（不进行四舍五入）。", "solution": "该问题是有效的，所有必要的参数和定义都已提供，以进行全面的分析。\n\n**a) 偏差表达式的推导**\n\n分数中的偏差定义为使用频率加权的上下文表示 $h_{\\mathrm{freq}}$ 计算的分数与使用均等平均的上下文表示 $h_{\\mathrm{equal}}$ 计算的分数之差。该偏差的表达式如下：\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} h_{\\mathrm{freq}} - v_{w}^{\\top} h_{\\mathrm{equal}}\n$$\n其中 $v_{w}$ 是目标词的嵌入。\n\n根据内积的线性性质，我们可以合并这两项：\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} (h_{\\mathrm{freq}} - h_{\\mathrm{equal}})\n$$\n现在，我们代入 $h_{\\mathrm{freq}}$ 和 $h_{\\mathrm{equal}}$ 的定义：\n$$\nh_{\\mathrm{freq}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, v_{c_{i}}\n$$\n$$\nh_{\\mathrm{equal}} = \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}} = \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}}\n$$\n这两种上下文表示之间的差是：\n$$\nh_{\\mathrm{freq}} - h_{\\mathrm{equal}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, v_{c_{i}} - \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}}\n$$\n合并求和项，我们得到：\n$$\nh_{\\mathrm{freq}} - h_{\\mathrm{equal}} = \\sum_{i=1}^{|C|} \\left( p(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}}\n$$\n将此结果代回到 $\\Delta_{\\mathrm{bias}}(w)$ 的表达式中：\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} \\left( \\sum_{i=1}^{|C|} \\left( p(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}} \\right)\n$$\n再次利用内积的线性性质，我们可以将 $v_{w}^{\\top}$ 分配到求和项中：\n$$\n\\Delta_{\\mathrm{bias}}(w) = \\sum_{i=1}^{|C|} \\left( p(c_{i}) - \\frac{1}{|C|} \\right) (v_{w}^{\\top} v_{c_{i}})\n$$\n这就是偏差的通用符号表达式。它表明，偏差是目标词嵌入与每个上下文词嵌入之间的点积（相似度）的加权和。每个上下文类型 $c_i$ 的权重是其经验频率 $p(c_i)$ 与均匀频率 $1/|C|$ 之间的偏差。\n\n**b) 偏差的数值计算**\n\n我们被给予以下数值：\n$|C| = 4$, $d = 2$。\n目标向量：$v_{w} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}$。\n上下文嵌入：\n$v_{c_{1}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $v_{c_{2}} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$, $v_{c_{3}} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, $v_{c_{4}} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$。\n频率：\n$p(c_{1}) = 0.4$, $p(c_{2}) = 0.3$, $p(c_{3}) = 0.2$, $p(c_{4}) = 0.1$。\n\n首先，我们计算两种聚合的上下文表示 $h_{\\mathrm{freq}}$ 和 $h_{\\mathrm{equal}}$。\n$$\nh_{\\mathrm{freq}} = \\sum_{i=1}^{4} p(c_{i}) v_{c_{i}} = 0.4 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 0.3 \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + 0.2 \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\n$$\nh_{\\mathrm{freq}} = \\begin{pmatrix} 0.4 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0.6 \\end{pmatrix} + \\begin{pmatrix} -0.2 \\\\ 0.2 \\end{pmatrix} + \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix} = \\begin{pmatrix} 0.4 + 0 - 0.2 + 0.2 \\\\ 0 + 0.6 + 0.2 - 0.1 \\end{pmatrix} = \\begin{pmatrix} 0.4 \\\\ 0.7 \\end{pmatrix}\n$$\n$$\nh_{\\mathrm{equal}} = \\frac{1}{4} \\sum_{i=1}^{4} v_{c_{i}} = \\frac{1}{4} \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right)\n$$\n$$\nh_{\\mathrm{equal}} = \\frac{1}{4} \\begin{pmatrix} 1 + 0 - 1 + 2 \\\\ 0 + 2 + 1 - 1 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}\n$$\n现在我们计算分数及其差值：\n$$\nv_{w}^{\\top} h_{\\mathrm{freq}} = \\begin{pmatrix} 3 & -2 \\end{pmatrix} \\begin{pmatrix} 0.4 \\\\ 0.7 \\end{pmatrix} = (3)(0.4) + (-2)(0.7) = 1.2 - 1.4 = -0.2\n$$\n$$\nv_{w}^{\\top} h_{\\mathrm{equal}} = \\begin{pmatrix} 3 & -2 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} = (3)(0.5) + (-2)(0.5) = 1.5 - 1.0 = 0.5\n$$\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} h_{\\mathrm{freq}} - v_{w}^{\\top} h_{\\mathrm{equal}} = -0.2 - 0.5 = -0.7\n$$\n\n**c) 重加权函数 $\\alpha(c)$ 的推导**\n\n我们的任务是找到一个重加权函数 $\\alpha(c)$，使得对于任意的上下文嵌入集合 $\\{v_{c_{i}}\\}$，以下等式成立：\n$$\n\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} \\;=\\; \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}\n$$\n我们可以重写右侧，使其与左侧的求和结构相匹配：\n$$\n\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} \\;=\\; \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}}\n$$\n重新整理各项，我们得到：\n$$\n\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} - \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}} \\;=\\; \\vec{0}\n$$\n$$\n\\sum_{i=1}^{|C|} \\left( p(c_{i}) \\, \\alpha(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}} \\;=\\; \\vec{0}\n$$\n关键的约束是该方程必须对**任意**向量 $\\{v_{c_{i}}\\}$ 都成立。如果我们考虑向量集 $\\{v_{c_{i}}\\}_{i=1}^{|C|}$ 是线性无关的情况（例如，如果 $|C| \\le d$ 且我们选择它们为正交向量），那么这些向量的线性组合只有在所有标量系数都为零时才可能等于零向量。这个原则可以推广：为使等式对任何向量选择都普遍成立，每个系数必须为零。\n因此，对于每个 $i \\in \\{1, 2, \\dots, |C|\\}$，必须有：\n$$\np(c_{i}) \\, \\alpha(c_{i}) - \\frac{1}{|C|} = 0\n$$\n假设我们集合 $C$ 中的任何上下文类型 $c_i$ 的出现概率都非零，即 $p(c_i) > 0$，我们可以解出 $\\alpha(c_i)$:\n$$\np(c_{i}) \\, \\alpha(c_{i}) = \\frac{1}{|C|}\n$$\n$$\n\\alpha(c_{i}) = \\frac{1}{|C| \\, p(c_{i})}\n$$\n这就给出了用上下文的经验频率 $p(c)$ 和上下文类型的总数 $|C|$ 表示的重加权函数 $\\alpha(c)$。\n\n**d) 校正后分数差的计算**\n\n校正后的上下文表示定义为：\n$$\nh_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}}\n$$\n从 c) 部分的结果可知，重加权因子 $\\alpha(c_i)$ 的构造使得 $p(c_{i}) \\, \\alpha(c_{i}) = \\frac{1}{|C|}$。将此代入 $h_{\\mathrm{corr}}$ 的定义中：\n$$\nh_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} \\left( \\frac{1}{|C|} \\right) v_{c_{i}}\n$$\n这个表达式恰好是均等平均上下文表示的定义：\n$$\nh_{\\mathrm{corr}} = \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}} = h_{\\mathrm{equal}}\n$$\n问题要求计算校正后的分数差 $\\Delta_{\\mathrm{corr}}(w)$:\n$$\n\\Delta_{\\mathrm{corr}}(w) = v_{w}^{\\top} h_{\\mathrm{corr}} - v_{w}^{\\top} h_{\\mathrm{equal}}\n$$\n既然我们已经证明了 $h_{\\mathrm{corr}} = h_{\\mathrm{equal}}$，那么可以立即得出：\n$$\n\\Delta_{\\mathrm{corr}}(w) = v_{w}^{\\top} h_{\\mathrm{equal}} - v_{w}^{\\top} h_{\\mathrm{equal}} = 0\n$$\n这个结果是解析的，不依赖于 b) 部分提供的具体数值。重加权方案的设计目的就是为了消除偏差，因此根据构造，校正后的差值为零。\n\n需要报告的两个量是 b) 部分的结果 $\\Delta_{\\mathrm{bias}}(w) = -0.7$ 和 d) 部分的结果 $\\Delta_{\\mathrm{corr}}(w) = 0$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-0.7 & 0\n\\end{pmatrix}\n}\n$$", "id": "3200013"}]}