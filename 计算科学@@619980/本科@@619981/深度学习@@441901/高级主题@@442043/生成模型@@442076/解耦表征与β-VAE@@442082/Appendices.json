{"hands_on_practices": [{"introduction": "要优化模型的解耦性能，我们首先需要一种可靠的量化评估方法。这个练习 [@problem_id:3116952] 提供了一种从零开始构建解耦度量指标的动手实践，其核心是信息论中的基本概念——互信息。通过完成这项任务，你将具体地理解如何量化学习到的潜在变量与数据生成因子之间的统计依赖性，从而评估模型的解耦效果。", "problem": "给定由Beta-变分自编码器（Beta-VAE）等模型生成的成对样本，包括学习到的潜变量和真实的生成因子。设潜码由随机向量 $z = (z_1, \\dots, z_{d_z})$ 表示，真实生成因子由 $g = (g_1, \\dots, g_{d_g})$ 表示。您的任务是基于每个标量潜变量 $z_i$ 和每个标量因子 $g_j$ 之间的互信息构建一个解耦诊断，组装互信息矩阵，并以一种置换不变的方式量化对角占优性。\n\n仅从联合概率分布和边缘概率分布、香农熵和互信息（其通过Kullback–Leibler散度定义）的基本定义出发，推导并实现一个使用观测样本离散化方法来估计标量随机变量之间互信息的经验估计量。然后，使用此估计量构建所有对 $(z_i, g_j)$ 之间互信息的矩阵，并计算一个置换不变的对角占优分数，该分数反映了每个潜变量与一个唯一因子的对齐程度。\n\n您必须遵循以下设计要求：\n\n- 数据与离散化：\n  - 对于给定的成对样本集 $\\{(z^{(n)}, g^{(n)})\\}_{n=1}^N$，其中 $z^{(n)} \\in \\mathbb{R}^{d_z}$ 且 $g^{(n)} \\in \\mathbb{R}^{d_g}$，将每个标量变量在其观测范围（样本中的最小值到最大值）内离散化为 $b$ 个等宽的区间。对所有标量变量使用相同的 $b$。将最大值的区间边缘视为右闭合，以包含观测到的最大值。设 $b \\in \\mathbb{N}$ 且 $b \\ge 2$。\n  - 从这些离散化结果中，为每对 $(z_i, g_j)$ 构建经验联合频率表，并对其进行归一化以获得经验联合概率质量及其边缘概率。\n\n- 互信息矩阵：\n  - 仅使用基本定义，计算每对 $(i, j)$ 的互信息 $I(z_i; g_j)$，生成一个矩阵 $I \\in \\mathbb{R}_{\\ge 0}^{d_z \\times d_g}$。使用自然对数，单位为奈特（nats）。\n\n- 置换不变的对角占优分数：\n  - 由于潜变量维度和因子维度的排序是任意的，首先计算 $\\{z_i\\}$ 和 $\\{g_j\\}$ 之间的最优一对一分配，以最大化所选矩阵 $I$ 条目的总和。如果 $d_z \\ne d_g$，分配应在 $m = \\min(d_z, d_g)$ 对上进行匹配，且两组中的索引均不重复使用。\n  - 设 $S_{\\text{assign}}$ 为 $m$ 个选定条目的和，设 $S_{\\text{all}}$ 为 $I$ 中所有条目的和。将对角占优分数定义为 $D = S_{\\text{assign}} / S_{\\text{all}}$。如果在数值公差范围内 $S_{\\text{all}} = 0$（即 $S_{\\text{all}} \\le 10^{-12}$），则定义 $D = 0$。\n  - 分数 $D$ 必须满足 $0 \\le D \\le 1$，并且当互信息质量集中在对角线的一个置换上时，该分数应更接近1。\n\n- 输出规格：\n  - 对于下面的每个测试用例，您的程序必须输出一个三元组，包含：\n    1) 分数 $D$，四舍五入到4位小数，\n    2) 从潜变量索引到因子索引的选定分配，表示为一个长度为 $d_z$ 的列表，其中未分配的潜变量索引（如果 $d_z > d_g$）用 $-1$ 标记，\n    3) 互信息矩阵 $I$ 按行主序展平，每个条目四舍五入到4位小数。\n  - 将所有测试用例的结果汇总到单行中，该行包含这些三元组的列表。最终输出必须是严格符合以下格式的一行：\n    $[ [D_1, [a_{1,1}, \\dots, a_{1,d_z}], [I_{1,1}, \\dots]], [D_2, [\\dots], [\\dots]], \\dots ]$。\n\n- 测试套件和数据生成：\n  - 使用以下测试套件。在以下所有情况中，$N$ 是样本数，$s$ 是用于可复现生成器的伪随机种子。所有对数必须是自然对数，所有计算必须使用浮点运算。在所有情况下，将区间数量设置为 $b = 12$。\n\n  - 测试用例1（近乎理想的解耦）：\n    - 维度：$d_z = 3$，$d_g = 3$。\n    - 参数：$N = 5000$，$s = 13$。\n    - 采样：对于每个 $n \\in \\{1, \\dots, N\\}$，从 $\\text{Uniform}([-1, 1]^{3})$ 中采样 $g^{(n)}$，并从 $\\mathcal{N}(0, I_3)$ 中采样噪声 $\\epsilon^{(n)}$。\n    - 构建 $z^{(n)} = g^{(n)} + 0.05 \\cdot \\epsilon^{(n)}$。\n\n  - 测试用例2（独立的潜变量和因子）：\n    - 维度：$d_z = 3$，$d_g = 3$。\n    - 参数：$N = 5000$，$s = 7$。\n    - 采样：对于每个 $n$，从 $\\text{Uniform}([-1, 1]^{3})$ 中采样 $g^{(n)}$，并独立地从 $\\mathcal{N}(0, I_3)$ 中采样 $z^{(n)}$。\n\n  - 测试用例3（纠缠的线性混合）：\n    - 维度：$d_z = 3$，$d_g = 3$。\n    - 参数：$N = 5000$，$s = 11$。\n    - 采样：对于每个 $n$，从 $\\text{Uniform}([-1, 1]^{3})$ 中采样 $g^{(n)}$，从 $\\mathcal{N}(0, I_3)$ 中采样噪声 $\\epsilon^{(n)}$，并设置 $A \\in \\mathbb{R}^{3 \\times 3}$ 为\n      $$\n      A = \\begin{bmatrix}\n      1.0  & 0.8  & 0.5 \\\\\n      0.6  & 1.0  & 0.4 \\\\\n      0.5  & 0.4  & 1.2\n      \\end{bmatrix}.\n      $$\n      构建 $z^{(n)} = A \\, g^{(n)} + 0.2 \\cdot \\epsilon^{(n)}$。\n\n  - 测试用例4（维度不匹配，稀疏对齐）：\n    - 维度：$d_z = 2$，$d_g = 3$。\n    - 参数：$N = 5000$，$s = 17$。\n    - 采样：对于每个 $n$，从 $\\text{Uniform}([-1, 1]^{3})$ 中采样 $g^{(n)}$，从 $\\mathcal{N}(0, I_2)$ 中采样噪声 $\\epsilon^{(n)}$，并设置 $B \\in \\mathbb{R}^{2 \\times 3}$ 为\n      $$\n      B = \\begin{bmatrix}\n      1.0  & 0.0  & 0.0 \\\\\n      0.0  & 0.0  & 1.0\n      \\end{bmatrix}.\n      $$\n      构建 $z^{(n)} = B \\, g^{(n)} + 0.05 \\cdot \\epsilon^{(n)}$。\n\n- 最终输出格式要求：\n  - 您的程序应生成单行输出，其中包含4个测试用例结果的列表，该列表以逗号分隔并用方括号括起来，每个结果都是形如 $[D, \\text{assignment}, \\text{flattened\\_I}]$ 的列表，如上所述。输出中的所有浮点数必须四舍五入到4位小数，整数必须是精确值。\n\n您的解决方案必须是一个完整的、可运行的程序，能够完全按照上述规定执行采样、离散化、互信息估计、最优分配、分数计算和最终格式化，而无需任何外部输入。", "solution": "该问题要求开发一种基于互信息的解耦度量。我们将首先从第一性原理出发建立互信息的理论基础，然后详细介绍其经验估计方法，最后具体说明置换不变分数的构建方法。\n\n**1. 互信息的基本推导**\n\n本分析的基石是互信息，它量化了两个随机变量之间的统计依赖性。我们从更基本的概念——香农熵和Kullback-Leibler (KL) 散度——来推导它。\n\n设 $X$ 和 $Y$ 为两个离散随机变量，其字母表分别为 $\\mathcal{X}$ 和 $\\mathcal{Y}$，联合概率质量函数 (PMF) 为 $P_{XY}(x, y)$，边缘PMF为 $P_X(x)$ 和 $P_Y(y)$。\n\n$X$ 的**香农熵**度量其不确定性，定义为：\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} P_X(x) \\log P_X(x) $$\n其中对数为自然对数（以 $e$ 为底），单位为奈特（nats）。按照惯例，$0 \\log 0 = 0$。对 $(X, Y)$ 的联合熵定义类似：\n$$ H(X, Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P_{XY}(x, y) \\log P_{XY}(x, y) $$\n\n**Kullback-Leibler (KL) 散度**度量一个概率分布 $P$ 相对于第二个期望概率分布 $Q$ 的偏离程度。对于离散分布，其定义为：\n$$ D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} $$\n\n问题陈述要求用KL散度来表述**互信息** $I(X; Y)$。互信息度量了通过了解一个变量而减少的关于另一个变量的不确定性。形式上，它是联合分布 $P_{XY}(x, y)$ 与边际分布之积 $P_X(x) P_Y(y)$ 之间的KL散度，后者代表了假设独立性时的联合分布。\n$$ I(X; Y) = D_{KL}(P_{XY} || P_X P_Y) = \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P_{XY}(x, y) \\log \\frac{P_{XY}(x, y)}{P_X(x) P_Y(y)} $$\n\n这个定义可以展开，将互信息与熵联系起来：\n$$\n\\begin{align*}\nI(X; Y) &= \\sum_{x, y} P_{XY}(x, y) \\left[ \\log P_{XY}(x, y) - \\log P_X(x) - \\log P_Y(y) \\right] \\\\\n&= -H(X, Y) - \\sum_{x, y} P_{XY}(x, y) \\log P_X(x) - \\sum_{x, y} P_{XY}(x, y) \\log P_Y(y) \\\\\n&= -H(X, Y) - \\sum_x \\left( \\sum_y P_{XY}(x, y) \\right) \\log P_X(x) - \\sum_y \\left( \\sum_x P_{XY}(x, y) \\right) \\log P_Y(y) \\\\\n&= -H(X, Y) - \\sum_x P_X(x) \\log P_X(x) - \\sum_y P_Y(y) \\log P_Y(y) \\\\\n&= H(X) + H(Y) - H(X, Y)\n\\end{align*}\n$$\n这个最终形式 $I(X; Y) = H(X) + H(Y) - H(X, Y)$ 提供了一种方便且数值稳定的方法，可以从估计出的熵来计算互信息。\n\n**2. 从连续样本进行经验估计**\n\n潜变量 $z_i$ 和生成因子 $g_j$ 是连续的。为了应用上述离散公式，我们必须首先对数据进行离散化。\n\n给定一个标量变量 $x$ 的 $N$ 个样本 $\\{x^{(n)}\\}_{n=1}^N$，我们找到其最小值 $x_{\\min}$ 和最大值 $x_{\\max}$。我们将范围 $[x_{\\min}, x_{\\max}]$ 分成 $b$ 个等宽的区间。每个区间的宽度是 $\\Delta x = (x_{\\max} - x_{\\min}) / b$。区间边缘是 $x_{\\min}, x_{\\min} + \\Delta x, \\dots, x_{\\max}$。问题规定最后一个区间包含最大值，这对于直方图实现是标准做法。然后将每个样本 $x^{(n)}$ 分配到 $b$ 个区间中的一个。\n\n对于一对变量 $(z_i, g_j)$，我们将此过程应用于 $\\{z_i^{(n)}\\}_{n=1}^N$ 和 $\\{g_j^{(n)}\\}_{n=1}^N$，每个都使用 $b$ 个区间。然后我们可以构建一个 $b \\times b$ 的联合频率矩阵 $\\text{Count}(k, l)$，其中 $\\text{Count}(k, l)$ 是样本 $(z_i^{(n)}, g_j^{(n)})$ 中 $z_i^{(n)}$ 落入区间 $k$ 且 $g_j^{(n)}$ 落入区间 $l$ 的数量。\n\n经验联合PMF $\\hat{P}(z_i, g_j)$ 通过将频率矩阵除以总样本数 $N$ 进行归一化得到：\n$$ \\hat{P}_{Z_i, G_j}(k, l) = \\frac{\\text{Count}(k, l)}{N} $$\n经验边缘PMF通过对联合PMF的行和列求和来计算：\n$$ \\hat{P}_{Z_i}(k) = \\sum_{l=1}^b \\hat{P}_{Z_i, G_j}(k, l) \\qquad \\text{和} \\qquad \\hat{P}_{G_j}(l) = \\sum_{k=1}^b \\hat{P}_{Z_i, G_j}(k, l) $$\n有了这些经验PMF，我们可以计算经验熵 $\\hat{H}(Z_i)$、$\\hat{H}(G_j)$ 和 $\\hat{H}(Z_i, G_j)$，进而计算经验互信息 $\\hat{I}(z_i; g_j)$。\n\n**3. 互信息矩阵**\n\n通过对每一对潜变量 $z_i$（其中 $i \\in \\{1, \\dots, d_z\\}$）和生成因子 $g_j$（其中 $j \\in \\{1, \\dots, d_g\\}$）重复此估计过程，我们构建了互信息矩阵 $I \\in \\mathbb{R}_{\\ge 0}^{d_z \\times d_g}$，其中条目 $(i, j)$ 是 $I_{ij} = \\hat{I}(z_i; g_j)$。一个良好解耦的表示理想情况下在某个特定潜变量 $z_i$ 和某个特定因子 $g_j$ 之间具有高互信息，而在其他情况下互信息较低。这对应于矩阵 $I$ 是一个稀疏矩阵，且是对角矩阵的一个置换。\n\n**4. 置换不变的对角占优分数**\n\n为了以一种对潜变量维度和因子的任意排序不敏感的方式量化解耦程度，我们定义一个基于最优分配的分数。任务是找到潜变量子集和因子子集之间的一对一匹配，以最大化它们互信息的总和。\n\n这是一个经典的**分配问题**（也称为最大权二分图匹配）。给定代价矩阵 $I \\in \\mathbb{R}^{d_z \\times d_g}$，我们希望找到一个 $m = \\min(d_z, d_g)$ 个行索引到 $m$ 个列索引的分配 $\\mathcal{A}$，其中没有索引被重复使用，使得和 $\\sum_{(i,j) \\in \\mathcal{A}} I_{ij}$ 最大化。标准算法，如匈牙利算法，解决的是最小代价分配问题。为了最大化总和，我们可以在互信息矩阵的负数 $-I$ 上解决最小代价分配问题。\n\n设 $(r_1, c_1), \\dots, (r_m, c_m)$ 是最优分配的索引对，其中 $m = \\min(d_z, d_g)$。\n此最优分配的条目总和为：\n$$ S_{\\text{assign}} = \\sum_{k=1}^m I_{r_k, c_k} $$\n矩阵中所有条目的总和为：\n$$ S_{\\text{all}} = \\sum_{i=1}^{d_z} \\sum_{j=1}^{d_g} I_{ij} $$\n对角占优分数 $D$ 定义为最优分配的互信息总和与总互信息之比：\n$$ D = \\frac{S_{\\text{assign}}}{S_{\\text{all}}} $$\n该分是归一化的，$0 \\le D \\le 1$。一个接近 $1$ 的 $D$ 值表明大部分互信息集中在潜变量和因子之间的一对一映射上，标志着良好的解耦。一个接近 $1/m$（对于方阵）或更低的值可能表明存在纠缠或信息捕获不佳。根据问题要求，如果 $S_{\\text{all}} \\le 10^{-12}$，我们定义 $D=0$。这种情况发生在所有变量被发现完全独立时，导致互信息矩阵全为零。\n最终的长度为 $d_z$ 的分配列表通过将每个潜变量索引 $i$ 映射到其分配的因子索引 $j$（如果它是最优分配的一部分）来构建，否则映射到 $-1$。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Solves the disentanglement-quantification problem for all test cases.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    test_cases = [\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 13, \"b\": 12,\n            \"type\": \"ideal\",\n        },\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 7, \"b\": 12,\n            \"type\": \"independent\",\n        },\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 11, \"b\": 12,\n            \"type\": \"entangled\",\n            \"A\": np.array([[1.0, 0.8, 0.5], [0.6, 1.0, 0.4], [0.5, 0.4, 1.2]]),\n        },\n        {\n            \"d_z\": 2, \"d_g\": 3, \"N\": 5000, \"s\": 17, \"b\": 12,\n            \"type\": \"dim_mismatch\",\n            \"B\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        d_z, d_g, N, s, b = case[\"d_z\"], case[\"d_g\"], case[\"N\"], case[\"s\"], case[\"b\"]\n        rng = np.random.default_rng(seed=s)\n\n        # 1. Generate data\n        g_samples = rng.uniform(low=-1.0, high=1.0, size=(N, d_g))\n\n        if case[\"type\"] == \"ideal\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            z_samples = g_samples + 0.05 * epsilon\n        elif case[\"type\"] == \"independent\":\n            z_samples = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n        elif case[\"type\"] == \"entangled\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            A = case[\"A\"]\n            z_samples = (A @ g_samples.T).T + 0.2 * epsilon\n        elif case[\"type\"] == \"dim_mismatch\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            B = case[\"B\"]\n            z_samples = (B @ g_samples.T).T + 0.05 * epsilon\n\n        # 2. Compute Mutual Information Matrix\n        mi_matrix = np.zeros((d_z, d_g))\n\n        for i in range(d_z):\n            for j in range(d_g):\n                z_i = z_samples[:, i]\n                g_j = g_samples[:, j]\n                \n                # Discretize and get joint PMF\n                # np.histogram2d binning is [left, right) except for the last bin which is [left, right], as required.\n                hist_2d, _, _ = np.histogram2d(\n                    z_i, g_j, bins=b, \n                    range=[[z_i.min(), z_i.max()], [g_j.min(), g_j.max()]]\n                )\n                \n                p_xy = hist_2d / N\n                \n                # Compute marginal PMFs\n                p_x = np.sum(p_xy, axis=1)\n                p_y = np.sum(p_xy, axis=0)\n\n                # Compute entropies using scipy.stats.entropy which handles p*log(p) = 0 for p=0\n                # Using natural log (base=e) as required.\n                h_x = entropy(p_x, base=np.e)\n                h_y = entropy(p_y, base=np.e)\n                h_xy = entropy(p_xy.flatten(), base=np.e)\n                \n                # Mutual Information I(X;Y) = H(X) + H(Y) - H(X,Y)\n                mi = h_x + h_y - h_xy\n                mi_matrix[i, j] = mi\n\n        # 3. Compute Permutation-Invariant Score\n        s_all = np.sum(mi_matrix)\n        \n        if s_all = 1e-12:\n            d_score = 0.0\n            assignment = [-1] * d_z\n        else:\n            # We want to maximize the sum, so we find the minimum cost assignment on the negative MI matrix.\n            cost_matrix = -mi_matrix\n            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n            \n            # The sum of MI values for the optimal assignment\n            s_assign = mi_matrix[row_ind, col_ind].sum()\n            \n            d_score = s_assign / s_all\n            \n            # Create the assignment list (latent index -> factor index)\n            assignment = [-1] * d_z\n            for r, c in zip(row_ind, col_ind):\n                assignment[r] = c\n\n        # 4. Format results\n        D_rounded = round(d_score, 4)\n        I_flat_rounded = [round(val, 4) for val in mi_matrix.flatten(order='C')]\n        \n        all_results.append([D_rounded, assignment, I_flat_rounded])\n\n    # Final print statement must match the required format exactly.\n    # Custom formatting to avoid spaces after commas in lists.\n    result_str = \",\".join([\n        f\"[{res[0]},{str(res[1]).replace(' ', '')},{str(res[2]).replace(' ', '')}]\" for res in all_results\n    ])\n    print(f\"[{result_str}]\")\n\n\nsolve()\n```", "id": "3116952"}, {"introduction": "然而，实现良好的解耦并非一帆风顺，某些模型设计决策可能会产生负面影响。这个理论练习 [@problem_id:3116830] 探究了一种被称为“后验坍塌” (posterior collapse) 的关键失效模式，在这种模式下，潜在变量会变得毫无信息量。在一个简化的线性高斯设定中，你将通过解析推导，精确地揭示一个过于强大的解码器如何完全忽略潜在编码，从而导致解耦的彻底失败。这凸显了 β-VAE 目标函数中重建与正则化之间必须维持的精妙平衡。", "problem": "考虑一个带有 $\\beta$-VAE 目标函数的变分自编码器 (VAE)，其目标是学习独立真实潜在因子的解耦表示。$\\beta$-VAE 目标函数通过在编码器和解码器分布上进行优化，最大化由 Kullback-Leibler (KL) 散度上的缩放因子 $\\beta$ 修改的期望证据下界 (ELBO)。真实数据的生成过程如下：存在 $d$ 个独立的潜在因子 $f_i \\sim \\mathcal{N}(0,1)$，聚合为一个向量 $f \\in \\mathbb{R}^d$，以及由一个对角线性映射和加性高斯噪声生成的可观测数据 $x \\in \\mathbb{R}^d$，即 $x = D f + \\epsilon$，其中 $D = \\operatorname{diag}(\\alpha_1, \\dots, \\alpha_d)$，$\\alpha_i \\in \\mathbb{R}_{\\ge 0}$ 且 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2 I_d)$。考虑在相同的 $\\beta$ 值下，用于条件似然 $p_\\theta(x \\mid z)$ 的两种解码器：一个容量有限的简单解码器，建模为均值为 $W z$ 且具有固定各向同性方差的因子化高斯分布（即 $p_\\theta(x \\mid z) = \\mathcal{N}(W z, \\sigma_x^2 I_d)$，其中 $W = D$）；以及一个具有足够表达能力、能够独立于 $z$ 对精确的边缘数据分布 $p(x)$ 进行建模的强大解码器（例如，自回归模型），这意味着它可以有效地使所有 $z$ 都满足 $p_\\theta(x \\mid z) = p(x)$。编码器族被限制为对角高斯分布 $q_\\phi(z \\mid x) = \\mathcal{N}(B x, \\sigma_e^2 I_d)$，其中 $B = \\operatorname{diag}(b_1,\\dots,b_d)$，并且 $\\sigma_e^2$ 可以根据目标函数按维度进行优化。\n\n您的任务：\n\n- 从 $\\beta$-VAE 目标函数的定义、高斯分布的性质以及标准的线性高斯建模假设出发，在给定的真实数据模型下，推导出对于简单解码器 $p_\\theta(x \\mid z) = \\mathcal{N}(D z, \\sigma_x^2 I_d)$，能够最大化期望 $\\beta$-VAE 目标函数的最优编码器参数 $b_i$ 和 $\\sigma_{e,i}^2$。利用期望是基于由 $f \\sim \\mathcal{N}(0,I_d)$ 和 $x = D f + \\epsilon$（其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2 I_d)$）所蕴含的真实数据分布 $p_{\\text{data}}(x)$ 进行计算这一事实，并将解的范围限制在给定的编码器族 $q_\\phi(z \\mid x) = \\mathcal{N}(B x, \\sigma_e^2 I_d)$ 内。\n\n- 使用为简单解码器推导出的最优编码器参数，计算每个维度 $i$ 上学习到的潜在变量 $z_i$ 与其对应的真实潜在因子 $f_i$ 之间的互信息（以自然对数单位，即奈特(nats)计）。将每个维度的互信息间隙 (MIG) 定义为给定潜在变量与所有因子之间的最大互信息和次大互信息之差；在指定的对角设定中，此定义可适当简化。报告所有维度的平均 MIG。\n\n- 对于能够在相同 $\\beta$ 值下独立于 $z$ 匹配数据边缘分布 $p(x)$ 的强大解码器，论证最优编码器的行为并计算所有维度的平均 MIG。\n\n- 实现一个程序，该程序针对下面的测试套件中的每个测试用例，计算在相同 $\\beta$ 值下简单解码器和强大解码器的平均 MIG，然后输出一个列表。对于每个测试用例，该列表包含简单解码器的平均 MIG 与强大解码器的平均 MIG 之间的差值。所有互信息量均以奈特 (nats) 表示。\n\n使用以下测试套件，该套件通过改变维度数、对角映射 $D$ 的条目、编解码器噪声水平和 $\\beta$ 值来测试不同的行为：\n\n- 测试用例 1：$d = 3$，$\\alpha = [1.0, 0.8, 1.2]$，$\\sigma_\\epsilon^2 = 0.1$，$\\sigma_x^2 = 0.2$，$\\beta = 4.5$。\n\n- 测试用例 2：$d = 2$，$\\alpha = [0.0, 0.5]$，$\\sigma_\\epsilon^2 = 0.2$，$\\sigma_x^2 = 0.2$，$\\beta = 4.5$。\n\n- 测试用例 3：$d = 4$，$\\alpha = [1.0, 1.0, 1.0, 1.0]$，$\\sigma_\\epsilon^2 = 0.1$，$\\sigma_x^2 = 0.2$，$\\beta = 50.0$。\n\n最终输出格式要求：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3]$），其中每个 $r_k$ 是测试用例 $k$ 中简单解码器的平均 MIG 与强大解码器的平均 MIG 之间的差值，以奈特 (nats) 为单位。", "solution": "所提供的问题具有科学依据、适定，并包含进行严谨求解所需的所有必要信息。它在变分自编码器和信息论的标准框架内提出了一个理论问题。因此，该问题被认为是有效的。\n\n核心任务是通过计算互信息间隙 (MIG) 来分析 $\\beta$-VAE 在两种不同解码器假设（简单解码器和强大解码器）下的解耦能力。该问题的结构，以其基于对角矩阵的线性高斯模型为特点，允许进行完整的解析推导。求解过程首先为每种情况推导出最优编码器参数，然后使用这些参数计算所需的互信息量。\n\n需要最大化的 $\\beta$-VAE 目标函数由下式给出：\n$$\n\\mathcal{L}(\\phi, \\theta) = \\mathbb{E}_{p_{\\text{data}}(x)} \\left[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta \\text{KL}(q_\\phi(z|x) \\| p(z)) \\right]\n$$\n其中 $p(z) = \\mathcal{N}(0, I_d)$ 是潜在先验。模型的定义使得所有矩阵都是对角的，这意味着目标函数可以分解为按维度求和的项，$\\mathcal{L} = \\sum_{i=1}^d \\mathcal{L}_i$。因此，我们可以独立分析每个维度 $i$。\n\n维度 $i$ 的模型是：\n- 真实潜在因子：$f_i \\sim \\mathcal{N}(0, 1)$。\n- 数据生成：$x_i = \\alpha_i f_i + \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$。这意味着 $x_i$ 是一个高斯随机变量，$x_i \\sim \\mathcal{N}(0, \\alpha_i^2 + \\sigma_\\epsilon^2)$。我们将 $x_i$ 数据生成过程的方差表示为 $V_{x_i} = \\alpha_i^2 + \\sigma_\\epsilon^2$。\n- 编码器：$q_\\phi(z_i|x_i) = \\mathcal{N}(b_i x_i, \\sigma_{e,i}^2)$。\n- 简单解码器：$p_\\theta(x_i|z_i) = \\mathcal{N}(\\alpha_i z_i, \\sigma_x^2)$。\n- 先验：$p(z_i) = \\mathcal{N}(0, 1)$。\n\n按维度的目标函数是：\n$$\n\\mathcal{L}_i = \\mathbb{E}_{p(x_i)} \\left[ \\mathbb{E}_{q(z_i|x_i)}[\\log p(x_i|z_i)] - \\beta \\text{KL}(q(z_i|x_i) \\| p(z_i)) \\right]\n$$\n\n### 第一部分：简单解码器分析\n\n首先，我们为简单解码器推导出能够最大化 $\\mathcal{L}_i$ 的最优编码器参数 $b_i$ 和 $\\sigma_{e,i}^2$。\n\n重构项的期望是：\n$$\n\\mathbb{E}_{p(x_i)} \\mathbb{E}_{q(z_i|x_i)}[\\log p(x_i|z_i)] = \\mathbb{E}_{p(x_i), q(z_i|x_i)}\\left[-\\frac{(x_i - \\alpha_i z_i)^2}{2\\sigma_x^2} - \\frac{1}{2}\\log(2\\pi\\sigma_x^2)\\right]\n$$\n二次项展开为 $\\mathbb{E}[x_i^2 - 2\\alpha_i x_i z_i + \\alpha_i^2 z_i^2]$。对由 $p(x_i)$ 和 $q(z_i|x_i)$ 诱导的联合分布取期望：$\\mathbb{E}[(x_i - \\alpha_i z_i)^2] = \\text{Var}(x_i - \\alpha_i z_i) = \\text{Var}(x_i) - 2\\alpha_i\\text{Cov}(x_i,z_i) + \\alpha_i^2\\text{Var}(z_i)$。我们有 $\\text{Var}(x_i) = V_{x_i}$，$\\text{Cov}(x_i, z_i) = b_i V_{x_i}$，以及 $\\text{Var}(z_i) = \\sigma_{e,i}^2 + b_i^2 V_{x_i}$。这得到 $\\mathbb{E}[(x_i - \\alpha_i z_i)^2] = V_{x_i} - 2\\alpha_i b_i V_{x_i} + \\alpha_i^2(\\sigma_{e,i}^2 + b_i^2 V_{x_i}) = V_{x_i}(1 - \\alpha_i b_i)^2 + \\alpha_i^2 \\sigma_{e,i}^2$。因此，重构项是 $-\\frac{1}{2\\sigma_x^2}[V_{x_i}(1 - \\alpha_i b_i)^2 + \\alpha_i^2 \\sigma_{e,i}^2] + C_1$。\n\nKL 散度项是：\n$$\n\\mathbb{E}_{p(x_i)}[\\text{KL}(\\mathcal{N}(b_i x_i, \\sigma_{e,i}^2) \\| \\mathcal{N}(0, 1))] = \\mathbb{E}_{p(x_i)}\\left[\\frac{1}{2}\\left(\\sigma_{e,i}^2 + (b_i x_i)^2 - 1 - \\log(\\sigma_{e,i}^2)\\right)\\right]\n$$\n结果为 $\\frac{1}{2}(\\sigma_{e,i}^2 + b_i^2 V_{x_i} - 1 - \\log(\\sigma_{e,i}^2))$。\n\n将这些结合起来，$\\mathcal{L}_i$ 中依赖于 $b_i$ 和 $\\sigma_{e,i}^2$ 的部分是：\n$$\n\\mathcal{L}_i(b_i, \\sigma_{e,i}^2) = -\\frac{V_{x_i}(1 - \\alpha_i b_i)^2}{2\\sigma_x^2} - \\frac{\\alpha_i^2 \\sigma_{e,i}^2}{2\\sigma_x^2} - \\frac{\\beta}{2}(\\sigma_{e,i}^2 + b_i^2 V_{x_i} - \\log(\\sigma_{e,i}^2)) + C_2\n$$\n\n为了找到最优参数，我们求偏导数并令其为零。\n$\\frac{\\partial \\mathcal{L}_i}{\\partial b_i} = -\\frac{V_{x_i}}{2\\sigma_x^2} \\cdot 2(1-\\alpha_i b_i)(-\\alpha_i) - \\frac{\\beta}{2}(2b_i V_{x_i}) = 0$。\n假设 $V_{x_i} = \\alpha_i^2 + \\sigma_\\epsilon^2 > 0$，我们可以除以 $V_{x_i}$：\n$$\n\\frac{\\alpha_i(1-\\alpha_i b_i)}{\\sigma_x^2} - \\beta b_i = 0 \\implies \\frac{\\alpha_i}{\\sigma_x^2} = b_i \\left(\\frac{\\alpha_i^2}{\\sigma_x^2} + \\beta\\right) \\implies b_i^* = \\frac{\\alpha_i}{\\alpha_i^2 + \\beta \\sigma_x^2}\n$$\n$\\frac{\\partial \\mathcal{L}_i}{\\partial \\sigma_{e,i}^2} = -\\frac{\\alpha_i^2}{2\\sigma_x^2} - \\frac{\\beta}{2} + \\frac{\\beta}{2\\sigma_{e,i}^2} = 0$。\n$$\n\\frac{\\beta}{\\sigma_{e,i}^2} = \\frac{\\alpha_i^2}{\\sigma_x^2} + \\beta = \\frac{\\alpha_i^2 + \\beta\\sigma_x^2}{\\sigma_x^2} \\implies (\\sigma_{e,i}^2)^* = \\frac{\\beta \\sigma_x^2}{\\alpha_i^2 + \\beta \\sigma_x^2}\n$$\n\n接下来，我们计算互信息 $I(z_i; f_i)$。该系统是一个线性高斯级联：$f_i \\to x_i \\to z_i$。对于联合高斯变量，互信息为 $I(z_i; f_i) = \\frac{1}{2} \\log \\frac{\\text{Var}(z_i)}{\\text{Var}(z_i|f_i)}$。\n使用全方差定律：$\\text{Var}(z_i) = \\text{Var}(\\mathbb{E}[z_i|f_i]) + \\mathbb{E}[\\text{Var}(z_i|f_i)]$。条件矩为 $\\mathbb{E}[z_i|f_i] = b_i \\alpha_i f_i$ 和 $\\text{Var}(z_i|f_i) = \\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2$，后者相对于 $f_i$ 是一个常数。因此，$\\text{Var}(z_i) = \\text{Var}(b_i \\alpha_i f_i) + (\\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2) = b_i^2 \\alpha_i^2 \\text{Var}(f_i) + \\text{Var}(z_i|f_i) = b_i^2 \\alpha_i^2 + \\text{Var}(z_i|f_i)$，因为 $\\text{Var}(f_i)=1$。\n互信息是：\n$$\nI(z_i; f_i) = \\frac{1}{2} \\log \\left(\\frac{\\text{Var}(z_i|f_i) + b_i^2 \\alpha_i^2}{\\text{Var}(z_i|f_i)}\\right) = \\frac{1}{2} \\log \\left(1 + \\frac{b_i^2 \\alpha_i^2}{\\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2}\\right)\n$$\n代入最优参数 $b_i^*$ 和 $(\\sigma_{e,i}^2)^*$：\n令 $A = \\alpha_i^2 + \\beta \\sigma_x^2$。那么 $b_i^* = \\alpha_i/A$ 且 $(\\sigma_{e,i}^2)^* = \\beta\\sigma_x^2/A$。\n对数的参数变为：\n$1 + \\frac{(\\alpha_i/A)^2 \\alpha_i^2}{(\\beta\\sigma_x^2/A) + (\\alpha_i/A)^2 \\sigma_\\epsilon^2} = 1 + \\frac{\\alpha_i^4/A^2}{(\\beta\\sigma_x^2 A + \\alpha_i^2 \\sigma_\\epsilon^2)/A^2} = 1 + \\frac{\\alpha_i^4}{\\beta\\sigma_x^2(\\alpha_i^2 + \\beta\\sigma_x^2) + \\alpha_i^2 \\sigma_\\epsilon^2}$。\n$$\nI(z_i; f_i) = \\frac{1}{2} \\log \\left( 1 + \\frac{\\alpha_i^4}{\\alpha_i^2(\\beta\\sigma_x^2 + \\sigma_\\epsilon^2) + \\beta^2(\\sigma_x^2)^2} \\right)\n$$\n正如在验证阶段所确定的，由于问题的对角结构，潜在变量 $z_i$ 的 MIG 简化为 $I(z_i; f_i)$。因此，简单解码器的平均 MIG 是：\n$$\n\\text{MIG}_{\\text{simple}} = \\frac{1}{d} \\sum_{i=1}^d I(z_i; f_i)\n$$\n\n### 第二部分：强大解码器分析\n\n对于强大解码器，$p_\\theta(x|z) = p(x)$。目标函数变为：\n$$\n\\mathcal{L}_i = \\mathbb{E}_{p(x_i)}[\\log p(x_i)] - \\beta \\mathbb{E}_{p(x_i)}[\\text{KL}(q(z_i|x_i) \\| p(z_i))]\n$$\n第一项是数据的负熵，它对于编码器参数 $(b_i, \\sigma_{e,i}^2)$ 是一个常数。最大化 $\\mathcal{L}_i$ 等价于最小化 $\\mathbb{E}_{p(x_i)}[\\text{KL}(q(z_i|x_i) \\| p(z_i))]$。由于 KL 散度是非负的，其最小值为 0，在所有 $x_i$ 都满足 $q(z_i|x_i) = p(z_i)$ 时达到。\n比较 $q(z_i|x_i) = \\mathcal{N}(b_i x_i, \\sigma_{e,i}^2)$ 和 $p(z_i) = \\mathcal{N}(0, 1)$，我们必须对所有 $x_i$ 都有 $b_i x_i = 0$，这意味着 $b_i^*=0$，且 $\\sigma_{e,i}^2 = 1$。\n这种潜在变量为了最小化 KL 惩罚项而变得与数据无关的现象，被称为“后验坍缩” (posterior collapse)。\n当 $b_i^*=0$ 时，互信息公式立即给出 $I(z_i; f_i) = \\frac{1}{2}\\log(1+0) = 0$。\n因此，强大解码器的平均 MIG 是：\n$$\n\\text{MIG}_{\\text{powerful}} = \\frac{1}{d} \\sum_{i=1}^d 0 = 0\n$$\n\n### 第三部分：最终计算\n问题要求计算每个测试用例中，简单解码器的平均 MIG 与强大解码器的平均 MIG 之间的差值。\n$$\n\\Delta\\text{MIG} = \\text{MIG}_{\\text{simple}} - \\text{MIG}_{\\text{powerful}} = \\text{MIG}_{\\text{simple}} - 0 = \\text{MIG}_{\\text{simple}}\n$$\n这就是程序需要为每个测试用例计算的量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    Derives optimal encoder parameters for a beta-VAE with a simple linear decoder,\n    computes the resulting average Mutual Information Gap (MIG), and finds the\n    difference with the MIG from a powerful decoder, which is always zero.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1: d=3, alpha=[1.0, 0.8, 1.2], sigma_eps^2=0.1, sigma_x^2=0.2, beta=4.5\n        {\n            \"d\": 3,\n            \"alpha\": [1.0, 0.8, 1.2],\n            \"sigma_eps_sq\": 0.1,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 4.5,\n        },\n        # Test case 2: d=2, alpha=[0.0, 0.5], sigma_eps^2=0.2, sigma_x^2=0.2, beta=4.5\n        {\n            \"d\": 2,\n            \"alpha\": [0.0, 0.5],\n            \"sigma_eps_sq\": 0.2,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 4.5,\n        },\n        # Test case 3: d=4, alpha=[1.0, 1.0, 1.0, 1.0], sigma_eps^2=0.1, sigma_x^2=0.2, beta=50.0\n        {\n            \"d\": 4,\n            \"alpha\": [1.0, 1.0, 1.0, 1.0],\n            \"sigma_eps_sq\": 0.1,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 50.0,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = case[\"d\"]\n        alpha_vec = case[\"alpha\"]\n        sigma_eps_sq = case[\"sigma_eps_sq\"]\n        sigma_x_sq = case[\"sigma_x_sq\"]\n        beta = case[\"beta\"]\n\n        # For the simple decoder, the average MIG is the average of I(z_i; f_i)\n        # across dimensions. For the powerful decoder, the average MIG is 0.\n        # The required difference is therefore just the average MIG of the simple decoder.\n        \n        mutual_informations = []\n        for i in range(d):\n            alpha_i = alpha_vec[i]\n\n            # If alpha_i is 0, then x_i contains no information about f_i,\n            # so I(z_i; f_i) is 0.\n            if alpha_i == 0.0:\n                mi_i = 0.0\n            else:\n                alpha_i_sq = alpha_i**2\n                alpha_i_4 = alpha_i**4\n                beta_sq = beta**2\n                sigma_x_sq_sq = sigma_x_sq**2\n\n                # This is the derived formula for I(z_i; f_i) in nats.\n                # I(z_i; f_i) = 0.5 * log(1 + (alpha_i^4) /\n                # (alpha_i^2 * (beta*sigma_x^2 + sigma_eps^2) + beta^2 * (sigma_x^2)^2))\n                numerator = alpha_i_4\n                denominator = alpha_i_sq * (beta * sigma_x_sq + sigma_eps_sq) + beta_sq * sigma_x_sq_sq\n                \n                mi_i = 0.5 * np.log(1.0 + numerator / denominator)\n            \n            mutual_informations.append(mi_i)\n        \n        avg_mig_simple = np.mean(mutual_informations)\n        \n        # The average MIG for the powerful decoder is 0 due to posterior collapse.\n        avg_mig_powerful = 0.0\n        \n        difference = avg_mig_simple - avg_mig_powerful\n        results.append(difference)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.16f}' for r in results)}]\")\n\nsolve()\n```", "id": "3116830"}, {"introduction": "基于对训练 β-VAE 所面临挑战的理解，这个练习 [@problem_id:3116925] 介绍了一种旨在改善结果的实用技术。我们将不再使用固定的正则化权重 $β$，而是探索一种自适应的容量规划策略 (adaptive capacity annealing)，它会逐步增加对 KL 散度项的正则化压力。这个数值实验将展示，与固定的目标函数相比，这种动态方法如何能引导模型学习到更优的解耦表示，尤其是在比较具有相同最终性能 (ELBO) 的模型时。", "problem": "您的任务是为变分自编码器 (VAE) 设计和评估一种自适应容量规划，以促进解耦表示。请从以下基础出发，并仅使用数学上定义的量。\n\n基本基础：\n- 变分自编码器 (VAE) 优化证据下界 (ELBO)。对于一个具有先验 $p(\\mathbf{z})$ 和近似后验 $q(\\mathbf{z} \\mid \\mathbf{x})$ 的潜变量模型，每个数据点的负 ELBO 分解为一个重构项和一个正则化项：\n$$\n\\mathcal{L}_{\\text{ELBO}} = \\mathbb{E}_{q(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\ell_{\\text{rec}}(\\mathbf{x}, \\mathbf{z})\\right] + \\beta \\, \\mathrm{KL}\\left(q(\\mathbf{z} \\mid \\mathbf{x}) \\, \\| \\, p(\\mathbf{z})\\right),\n$$\n其中 $\\mathrm{KL}$ 表示 Kullback–Leibler 散度 (KL)，$\\beta \\ge 0$ 是一个标量。重构项是一个经过充分检验的替代项（例如，高斯似然下的平方误差）。\n- 对于标准正态先验 $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 和对角高斯近似后验 $q(\\mathbf{z} \\mid \\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}(\\mathbf{x}), \\mathrm{diag}(\\boldsymbol{\\sigma}^2))$，每维 KL 为\n$$\n\\mathrm{KL}_i = \\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right).\n$$\n\n风格化的线性高斯设置：\n- 观测数据 $\\mathbf{x} \\in \\mathbb{R}^2$ 由两个独立的高斯因子生成：$x_1 \\sim \\mathcal{N}(0, s_1^2)$ 和 $x_2 \\sim \\mathcal{N}(0, s_2^2)$，协方差为 $\\mathrm{Cov}(\\mathbf{x}) = \\mathrm{diag}(s_1^2, s_2^2)$。\n- 编码器均值是线性的：$\\boldsymbol{\\mu}(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}$，其中 $\\mathbf{A}$ 被约束为按角度 $\\theta$ 的旋转，即 $\\mathbf{A} = \\mathbf{R}(\\theta)$；解码器是线性的：$\\hat{\\mathbf{x}} = \\mathbf{W}\\mathbf{z}$，其中 $\\mathbf{W}$ 是按固定角度 $\\varphi$ 的旋转，即 $\\mathbf{W} = \\mathbf{R}(\\varphi)$。所有角度都以弧度为单位。\n- 该模型下的期望平方重构误差为\n$$\nR(\\theta) = \\mathbb{E}\\left[\\|\\mathbf{x} - \\mathbf{W}\\boldsymbol{\\mu}(\\mathbf{x})\\|_2^2\\right] + \\mathrm{trace}\\!\\left(\\mathbf{W}\\,\\mathrm{Cov}(\\mathbf{z}\\mid \\mathbf{x})\\,\\mathbf{W}^\\top\\right).\n$$\n当 $\\mathbf{A} = \\mathbf{R}(\\theta)$、$\\mathbf{W} = \\mathbf{R}(\\varphi)$ 且 $\\mathrm{Cov}(\\mathbf{z}\\mid \\mathbf{x}) = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2)$ 时，该式简化为\n$$\nR(\\theta) = 2\\bigl(1 - \\cos(\\theta + \\varphi)\\bigr)(s_1^2 + s_2^2) + \\sigma_1^2 + \\sigma_2^2.\n$$\n- 编码器的每维平均能量为\n$$\n\\mathbb{E}_{\\mathbf{x}}[\\mu_1(\\mathbf{x})^2] = \\cos^2\\!\\theta \\, s_1^2 + \\sin^2\\!\\theta \\, s_2^2,\\quad\n\\mathbb{E}_{\\mathbf{x}}[\\mu_2(\\mathbf{x})^2] = \\sin^2\\!\\theta \\, s_1^2 + \\cos^2\\!\\theta \\, s_2^2.\n$$\n\n自适应容量规划：\n- 用每维目标 KL 预算 $C_1(t)$ 和 $C_2(t)$ 替换固定的 $\\beta$，这些预算随时间 $t$ 通过在最大值处饱和的线性斜坡进行退火：\n$$\nC_i(t) = \\min\\!\\bigl(C_{i,\\max}, \\kappa_i \\, t\\bigr),\\quad i \\in \\{1,2\\},\n$$\n其中 $\\kappa_i  0$ 和 $C_{i,\\max}  0$ 是给定的常数。\n\n待比较的目标：\n- 静态 $\\beta$ 目标（负 ELBO）：\n$$\n\\mathcal{L}_\\beta(\\theta, \\sigma_1^2, \\sigma_2^2) = R(\\theta) + \\beta\\left(\\mathrm{KL}_1 + \\mathrm{KL}_2\\right).\n$$\n- 对预算偏差施加二次惩罚的自适应容量目标：\n$$\n\\mathcal{L}_{\\text{cap}}(\\theta, \\sigma_1^2, \\sigma_2^2; t) = R(\\theta) + \\gamma \\sum_{i=1}^{2}\\left(\\mathrm{KL}_i - C_i(t)\\right)^2,\n$$\n其中 $\\gamma  0$ 是一个给定的惩罚权重。\n\n解耦分数：\n- 定义一个解耦分数作为编码器旋转角度 $\\theta$ 的函数：\n$$\nD(\\theta) = 1 - \\left|\\sin(2\\theta)\\right|.\n$$\n当每个潜变量主要与单个因子对齐时（在排列下 $\\theta = 0$ 或 $\\theta = \\frac{\\pi}{2}$），该分数为 1；在最大混合时（$\\theta = \\frac{\\pi}{4}$）为 0。\n\n任务：\n1. 使用提供的基础，推导在此设置下 $R(\\theta)$ 和 $\\mathrm{KL}_i$ 的表达式。然后按上述规定设计退火规划 $C_i(t)$。\n2. 对于给定时间 $t$ 的自适应容量目标，确定编码器角度 $\\theta_{\\text{cap}}(t)$ 和方差 $\\sigma_1^2, \\sigma_2^2$，这些参数在 $\\theta \\in [-\\pi, \\pi]$ 和 $\\sigma_i^2  0$ 上最小化 $\\mathcal{L}_{\\text{cap}}$。\n3. 对于静态 $\\beta$ 目标，证明最优角度满足 $\\theta_\\beta^* = -\\varphi$（因为 $\\mathrm{KL}_1 + \\mathrm{KL}_2$ 独立于 $\\theta$），并通过最小化 $\\mathcal{L}_\\beta$（相对于 $\\sigma_i^2$）来推导给定 $\\beta$ 的最优 $\\sigma_i^2$。\n4. 对于一个固定的时间 $t$，选择 $\\beta$ 以使最小化的静态 $\\beta$ 损失等于最小化的自适应容量损失，从而确保在固定的 ELBO 值下进行公平比较。然后计算并比较解耦分数 $D(\\theta_{\\text{cap}}(t))$ 和 $D(\\theta_\\beta^*)$。\n5. 对于下面套件中的每个测试用例，输出一个布尔值：如果在最小化损失相等的情况下，自适应容量实现了比静态 $\\beta$ 严格更高的解耦分数，则为 true，否则为 false。\n\n角度单位：\n- 所有角度都必须以弧度为单位。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个用方括号括起来的、以逗号分隔的结果列表（例如，\"[true,false,true]\"），但使用 Python 的布尔字面量 \"True\" 和 \"False\"。\n\n测试套件：\n- 情况 1（正常路径）：$(s_1, s_2, \\varphi, \\kappa_1, \\kappa_2, C_{1,\\max}, C_{2,\\max}, \\gamma, t) = (1.5, 0.3, 0.7, 0.7, 0.1, 1.0, 0.1, 20.0, 2.0)$。\n- 情况 2（边界：无解码器混合）：$(s_1, s_2, \\varphi, \\kappa_1, \\kappa_2, C_{1,\\max}, C_{2,\\max}, \\gamma, t) = (1.0, 0.8, 0.0, 0.3, 0.3, 0.5, 0.5, 20.0, 1.0)$。\n- 情况 3（边缘：预算相等，解码器混合小）：$(s_1, s_2, \\varphi, \\kappa_1, \\kappa_2, C_{1,\\max}, C_{2,\\max}, \\gamma, t) = (1.5, 0.3, 0.1, 0.3, 0.3, 0.2, 0.2, 20.0, 1.0)$。\n\n答案规格：\n- 每个情况的答案是最终输出列表中的一个布尔值，表示在匹配的最小化损失（固定 ELBO 值）下，$D(\\theta_{\\text{cap}}(t))  D(\\theta_\\beta^*)$ 是否成立。", "solution": "我们按部就班，从核心定义开始，推导必要的表达式和算法。\n\n第 1 步：风格化模型中的重构和 Kullback–Leibler 散度。\n在线性解码器 $\\mathbf{W} = \\mathbf{R}(\\varphi)$ 和线性编码器均值 $\\boldsymbol{\\mu}(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}$（其中 $\\mathbf{A} = \\mathbf{R}(\\theta)$）下的重构项为\n$$\nR(\\theta) = \\mathbb{E}\\left[\\|\\mathbf{x} - \\mathbf{W}\\boldsymbol{\\mu}(\\mathbf{x})\\|^2\\right] + \\mathrm{trace}\\!\\left(\\mathbf{W}\\,\\mathrm{Cov}(\\mathbf{z}\\mid \\mathbf{x})\\,\\mathbf{W}^\\top\\right).\n$$\n由于 $\\mathbf{W}$ 和 $\\mathbf{A}$ 都是旋转矩阵，它们的乘积是按角度之和的旋转，$\\mathbf{W}\\mathbf{A} = \\mathbf{R}(\\varphi)\\mathbf{R}(\\theta) = \\mathbf{R}(\\theta + \\varphi)$。对于协方差为 $\\mathrm{diag}(s_1^2, s_2^2)$ 的零均值高斯 $\\mathbf{x}$，期望平方误差项简化为二次型的迹：\n$$\n\\mathbb{E}\\left[\\|\\mathbf{x} - \\mathbf{W}\\boldsymbol{\\mu}(\\mathbf{x})\\|^2\\right]\n= \\mathrm{trace}\\!\\left((\\mathbf{I} - \\mathbf{R}(\\theta + \\varphi))^\\top(\\mathbf{I}-\\mathbf{R}(\\theta + \\varphi))\\,\\mathrm{diag}(s_1^2, s_2^2)\\right).\n$$\n对于按角度 $\\alpha$ 的旋转，可以证明 $(\\mathbf{I}-\\mathbf{R}(\\alpha))^\\top(\\mathbf{I}-\\mathbf{R}(\\alpha))$ 的两列范数都等于 $2(1 - \\cos\\alpha)$，因此由 $\\mathrm{diag}(s_1^2, s_2^2)$ 加权的迹为\n$$\n2(1 - \\cos(\\theta + \\varphi))(s_1^2 + s_2^2).\n$$\n对于正交的 $\\mathbf{W}$，协方差项就是 $\\sigma_1^2 + \\sigma_2^2$。因此，\n$$\nR(\\theta) = 2\\bigl(1 - \\cos(\\theta + \\varphi)\\bigr)(s_1^2 + s_2^2) + \\sigma_1^2 + \\sigma_2^2.\n$$\n\n对于到标准正态先验的 Kullback–Leibler 散度，每维在 $q(\\mathbf{z}\\mid \\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}(\\mathbf{x}), \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2))$ 条件下，经过充分检验的公式是\n$$\n\\mathrm{KL}_i = \\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right),\\quad i \\in \\{1,2\\}.\n$$\n编码器平均能量由线性和独立性得出：\n$$\n\\mathbb{E}_{\\mathbf{x}}[\\mu_1(\\mathbf{x})^2] = \\cos^2\\!\\theta \\, s_1^2 + \\sin^2\\!\\theta \\, s_2^2,\\quad\n\\mathbb{E}_{\\mathbf{x}}[\\mu_2(\\mathbf{x})^2] = \\sin^2\\!\\theta \\, s_1^2 + \\cos^2\\!\\theta \\, s_2^2.\n$$\n\n第 2 步：自适应容量规划和目标。\n我们设计一个随时间变化的每维预算，其形式为在最大值处饱和的线性斜坡：\n$$\nC_i(t) = \\min\\!\\bigl(C_{i,\\max}, \\kappa_i \\, t\\bigr).\n$$\n在时间 $t$ 的自适应容量目标为\n$$\n\\mathcal{L}_{\\text{cap}}(\\theta, \\sigma_1^2, \\sigma_2^2; t) =\n2\\bigl(1 - \\cos(\\theta + \\varphi)\\bigr)(s_1^2 + s_2^2) + \\sigma_1^2 + \\sigma_2^2\n+ \\gamma \\sum_{i=1}^{2}\\left(\\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right) - C_i(t)\\right)^2.\n$$\n对于固定的 $\\theta$，$\\sigma_1^2$ 和 $\\sigma_2^2$ 中的项是分离的，因此每个 $\\sigma_i^2$ 可以通过最小化标量函数来独立优化\n$$\nJ_i(\\sigma_i^2;\\theta,t) = \\sigma_i^2 + \\gamma\\left(\\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right) - C_i(t)\\right)^2.\n$$\n由于对数项的存在，这个标量问题没有闭式解，但对于实际相关的范围，它是凸的，并且可以进行高效的数值最小化。\n\n最后，我们通过一维数值搜索在 $\\theta \\in [-\\pi,\\pi]$ 上最小化 $\\mathcal{L}_{\\text{cap}}$，得到 $(\\theta_{\\text{cap}}(t), \\sigma_1^2(t), \\sigma_2^2(t))$ 和最小化后的值 $\\mathcal{L}_{\\text{cap}}^{\\star}(t)$。\n\n第 3 步：静态 $\\beta$ 目标及其最小化器。\n静态 $\\beta$ 目标为\n$$\n\\mathcal{L}_{\\beta}(\\theta, \\sigma_1^2, \\sigma_2^2) =\n2\\bigl(1 - \\cos(\\theta + \\varphi)\\bigr)(s_1^2 + s_2^2) + \\sigma_1^2 + \\sigma_2^2\n+ \\beta \\sum_{i=1}^2 \\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right).\n$$\n注意 $\\mathbb{E}_{\\mathbf{x}}[\\mu_1(\\mathbf{x})^2] + \\mathbb{E}_{\\mathbf{x}}[\\mu_2(\\mathbf{x})^2] = s_1^2 + s_2^2$ 独立于 $\\theta$，因为 $\\mathbf{A}$ 是正交的。因此，$\\mathcal{L}_\\beta$ 中依赖于 $\\theta$ 的部分只有重构项，它在以下位置最小化：\n$$\n\\theta_\\beta^* = -\\varphi.\n$$\n对于每个 $\\sigma_i^2$，依赖于 $\\sigma_i^2$ 的部分的导数是\n$$\n\\frac{\\partial}{\\partial \\sigma_i^2}\\left(\\sigma_i^2 + \\frac{\\beta}{2}(\\sigma_i^2 - \\ln \\sigma_i^2 - 1)\\right) = 1 + \\frac{\\beta}{2}\\left(1 - \\frac{1}{\\sigma_i^2}\\right).\n$$\n设为零可得\n$$\n1 + \\frac{\\beta}{2} - \\frac{\\beta}{2}\\frac{1}{\\sigma_i^2} = 0\n\\quad\\Rightarrow\\quad\n\\sigma_i^{2\\,*}(\\beta) = \\frac{\\beta}{\\beta + 2}.\n$$\n因此，最优后验方差相等且独立于 $\\theta$ 和此风格化设置中的数据方差。\n\n最小化的静态 $\\beta$ 损失则为\n$$\n\\mathcal{L}_\\beta^{\\star}(\\beta) =\n2\\bigl(1 - \\cos(0)\\bigr)(s_1^2 + s_2^2) + 2\\,\\sigma^{2\\,*}(\\beta) + \\beta\\sum_{i=1}^2 \\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma^{2\\,*}(\\beta) - \\ln \\sigma^{2\\,*}(\\beta) - 1\\right),\n$$\n其中重构项在 $\\theta_\\beta^* + \\varphi = 0$ 处计算。当 $\\theta_\\beta^* = -\\varphi$ 时，编码器平均能量为\n$$\n\\mathbb{E}_{\\mathbf{x}}[\\mu_1(\\mathbf{x})^2] = \\cos^2\\!\\varphi \\, s_1^2 + \\sin^2\\!\\varphi \\, s_2^2,\\quad\n\\mathbb{E}_{\\mathbf{x}}[\\mu_2(\\mathbf{x})^2] = \\sin^2\\!\\varphi \\, s_1^2 + \\cos^2\\!\\varphi \\, s_2^2.\n$$\n\n第 4 步：匹配固定的 ELBO 值以进行公平比较。\n在给定的时间 $t$，我们首先计算最小化的自适应容量损失 $\\mathcal{L}_{\\text{cap}}^{\\star}(t)$ 及其最小化器 $\\theta_{\\text{cap}}(t)$。然后，我们选择 $\\beta$ 以使最小化的静态 $\\beta$ 损失等于 $\\mathcal{L}_{\\text{cap}}^{\\star}(t)$：\n$$\n\\mathcal{L}_\\beta^{\\star}(\\beta) = \\mathcal{L}_{\\text{cap}}^{\\star}(t).\n$$\n这个关于 $\\beta$ 的标量方程通过数值方法求解。如果在合理范围内无法找到精确解的区间，我们选择 $\\beta$ 来最小化该范围内的绝对差 $|\\mathcal{L}_\\beta^{\\star}(\\beta) - \\mathcal{L}_{\\text{cap}}^{\\star}(t)|$，以确保尽可能接近的匹配。\n\n第 5 步：解耦度量和决策。\n计算\n$$\nD(\\theta) = 1 - |\\sin(2\\theta)|\n$$\n对于 $\\theta_{\\text{cap}}(t)$ 和 $\\theta_\\beta^*$。每个测试用例的布尔结果在 $D(\\theta_{\\text{cap}}(t))  D(\\theta_\\beta^*)$ 时为 true，否则为 false。\n\n算法设计：\n- 使用在 $\\theta \\in [-\\pi, \\pi]$ 上的一维有界最小化来求解 $\\mathcal{L}_{\\text{cap}}$，其中包含对每个维度在 $\\sigma_i^2 \\in (0, \\text{upper}]$ 上的内部一维有界最小化，以最小化 $J_i(\\sigma_i^2;\\theta,t)$。\n- 对静态 $\\beta$ 使用闭式解 $\\sigma^{2\\,*}(\\beta) = \\frac{\\beta}{\\beta + 2}$ 和 $\\theta_\\beta^* = -\\varphi$，并使用带区间限定的稳健求根法求解关于 $\\beta$ 的标量方程；如果没有根被限定在区间内，则在网格上最小化绝对差。\n- 将整个测试套件的最终布尔值组合成一个用方括号括起来的、以逗号分隔的列表。\n\n所有角度都以弧度为单位；所有输出都是布尔值。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar, brentq\n\n# Helper functions for the stylized linear-Gaussian VAE model.\n\ndef capacity_schedule(t, k1, k2, C1max, C2max):\n    C1 = min(C1max, k1 * t)\n    C2 = min(C2max, k2 * t)\n    return C1, C2\n\ndef recon_data(theta, phi, s1, s2):\n    S = s1**2 + s2**2\n    return 2.0 * (1.0 - np.cos(theta + phi)) * S\n\ndef mu2_dim1(theta, s1, s2):\n    return (np.cos(theta) ** 2) * (s1 ** 2) + (np.sin(theta) ** 2) * (s2 ** 2)\n\ndef mu2_dim2(theta, s1, s2):\n    return (np.sin(theta) ** 2) * (s1 ** 2) + (np.cos(theta) ** 2) * (s2 ** 2)\n\ndef kl_per_dim(mu2, sigma2):\n    # KL_i = 0.5 * (E[mu_i^2] + sigma_i^2 - ln sigma_i^2 - 1)\n    return 0.5 * (mu2 + sigma2 - np.log(sigma2) - 1.0)\n\ndef disentanglement(theta):\n    # D(theta) = 1 - |sin(2 theta)|\n    return 1.0 - np.abs(np.sin(2.0 * theta))\n\ndef minimize_sigma_for_capacity(mu2, C, gamma, sigma2_upper=10.0):\n    # Minimize J(sigma2) = sigma2 + gamma*(KL(mu2, sigma2) - C)^2 over sigma2 in (0, sigma2_upper]\n    # Bound sigma2 away from 0 for numerical stability.\n    lower = 1e-8\n    upper = sigma2_upper\n    obj = lambda s: s + gamma * (kl_per_dim(mu2, s) - C) ** 2\n    res = minimize_scalar(obj, bounds=(lower, upper), method='bounded')\n    return res.x, res.fun\n\ndef capacity_optimum(s1, s2, phi, k1, k2, C1max, C2max, gamma, t):\n    C1, C2 = capacity_schedule(t, k1, k2, C1max, C2max)\n    # Outer minimization over theta in [-pi, pi].\n    def total_objective(theta):\n        mu1 = mu2_dim1(theta, s1, s2)\n        mu2 = mu2_dim2(theta, s1, s2)\n        # Inner minimizations for sigma1^2 and sigma2^2\n        sigma1_opt, _ = minimize_sigma_for_capacity(mu1, C1, gamma)\n        sigma2_opt, _ = minimize_sigma_for_capacity(mu2, C2, gamma)\n        # Reconstruction term\n        rec = recon_data(theta, phi, s1, s2)\n        # Penalty term\n        pen = gamma * (kl_per_dim(mu1, sigma1_opt) - C1) ** 2 + gamma * (kl_per_dim(mu2, sigma2_opt) - C2) ** 2\n        # Total objective:\n        return rec + sigma1_opt + sigma2_opt + pen\n\n    res = minimize_scalar(total_objective, bounds=(-np.pi, np.pi), method='bounded')\n    theta_star = res.x\n    final_loss = res.fun\n    return theta_star, final_loss\n\ndef static_beta_min_loss(s1, s2, phi, beta):\n    # Optimal theta is -phi\n    theta_star = -phi\n    # Handle beta -> 0 case\n    if beta  1e-9:\n        sigma2_star = 0.0\n    else:\n        sigma2_star = beta / (beta + 2.0)\n    \n    # Reconstruction at theta_star + phi = 0\n    rec = recon_data(theta_star, phi, s1, s2) + 2.0 * sigma2_star\n    \n    # KL terms with mu energies at theta_star:\n    mu1 = mu2_dim1(theta_star, s1, s2)\n    mu2 = mu2_dim2(theta_star, s1, s2)\n    \n    if sigma2_star == 0.0:\n        kl_sum = float('inf')\n    else:\n        kl_sum = kl_per_dim(mu1, sigma2_star) + kl_per_dim(mu2, sigma2_star)\n    \n    total = rec + beta * kl_sum\n    return total, theta_star\n\ndef match_beta_to_cap_loss(s1, s2, phi, target_loss):\n    # Solve L_beta_star(beta) = target_loss\n    def f(beta):\n        L, _ = static_beta_min_loss(s1, s2, phi, beta)\n        return L - target_loss\n    # Bracket beta in a wide interval\n    beta_lo = 1e-6\n    beta_hi = 100.0\n    \n    try:\n        # Check if root is bracketed\n        flo = f(beta_lo)\n        fhi = f(beta_hi)\n        if flo * fhi > 0:\n            # Fallback grid search to find minimum difference if not bracketed\n            grid = np.logspace(-6, 2, 200)\n            vals = [abs(f(b)) for b in grid]\n            idx = np.argmin(vals)\n            return grid[idx]\n        beta_star = brentq(f, beta_lo, beta_hi, maxiter=200)\n        return beta_star\n    except (ValueError, RuntimeError):\n        # Fallback grid search on error\n        grid = np.logspace(-6, 2, 200)\n        vals = [abs(f(b)) for b in grid]\n        idx = np.argmin(vals)\n        return grid[idx]\n\ndef evaluate_case(case):\n    s1, s2, phi, k1, k2, C1max, C2max, gamma, t = case\n    # Adaptive capacity optimum at time t\n    theta_cap, L_cap_star = capacity_optimum(s1, s2, phi, k1, k2, C1max, C2max, gamma, t)\n    D_cap = disentanglement(theta_cap)\n    \n    # Match beta so that static-beta minimized loss equals L_cap_star\n    beta_match = match_beta_to_cap_loss(s1, s2, phi, L_cap_star)\n    \n    # Static-beta disentanglement at matched beta (note theta_beta depends only on phi)\n    _, theta_beta = static_beta_min_loss(s1, s2, phi, beta_match)\n    D_beta = disentanglement(theta_beta)\n    \n    # Boolean: adaptive capacity yields strictly higher disentanglement than static beta\n    return D_cap > D_beta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (s1, s2, phi, k1, k2, C1max, C2max, gamma, t)\n    test_cases = [\n        (1.5, 0.3, 0.7, 0.7, 0.1, 1.0, 0.1, 20.0, 2.0),  # Case 1\n        (1.0, 0.8, 0.0, 0.3, 0.3, 0.5, 0.5, 20.0, 1.0),  # Case 2\n        (1.5, 0.3, 0.1, 0.3, 0.3, 0.2, 0.2, 20.0, 1.0),  # Case 3\n    ]\n\n    results = []\n    for case_params in test_cases:\n        res = evaluate_case(case_params)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3116925"}]}