{"hands_on_practices": [{"introduction": "生成对抗网络（GAN）的训练过程本质上是一个双人零和博弈，其动态行为可能非常复杂且不稳定。为了从根本上理解这种不稳定性，我们可以分析一个简化的线性化模型。这个练习 [@problem_id:3127265] 将引导你研究一个双线性博弈，通过推导和比较同步更新与交替更新（如双时间尺度更新规则，TTUR）方案的迭代矩阵，你将能够从谱半径的角度揭示为什么简单的梯度下降-上升方法会导致发散，而调整更新策略则可能实现局部稳定。", "problem": "考虑一个零和生成对抗网络（GAN）博弈，其双线性价值函数为 $$V(\\theta,\\phi)=\\theta^{\\top}A\\phi,$$ 其中 $\\theta\\in\\mathbb{R}^{n}$，$\\phi\\in\\mathbb{R}^{m}$，以及 $A\\in\\mathbb{R}^{n\\times m}$。生成器最小化 $V$，判别器最大化 $V$。设生成器学习率为 $\\eta_{G}>0$，判别器学习率为 $\\eta_{D}>0$，并定义比率 $$r=\\frac{\\eta_{D}}{\\eta_{G}}>0.$$\n\n分析在鞍点 $(\\theta,\\phi)=(0,0)$ 附近线性化的两种离散时间训练方案：\n1. 单时间尺度同步梯度下降-上升：\n$$\\theta_{k+1}=\\theta_{k}-\\eta_{G}\\nabla_{\\theta}V(\\theta_{k},\\phi_{k})=\\theta_{k}-\\eta_{G}A\\phi_{k},\\quad \\phi_{k+1}=\\phi_{k}+\\eta_{D}\\nabla_{\\phi}V(\\theta_{k},\\phi_{k})=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k}.$$\n2. 双时间尺度更新规则（TTUR；交替更新）：先更新判别器，再更新生成器，\n$$\\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k},\\quad \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k+1}.$$\n\n从梯度下降-上升和线性算子谱半径的定义出发，使用 $A$ 的奇异值分解来分析沿 $A$ 的左右奇异向量对的线性化动力学。推导两种方案的迭代矩阵，并用 $A$ 的奇异值和步长来表示谱半径。从第一性原理出发，判断对于交替 TTUR 方案，是否存在任何正比率 $r$ 可以在非扩张线性动力学（谱半径小于或等于 $1$）的意义上实现局部稳定，并将其与同步方案进行比较。报告交替 TTUR 方案可实现的最小谱半径（在所有 $r>0$ 的范围内），结果为一个实数。无需四舍五入。", "solution": "问题要求分析具有双线性价值函数 $V(\\theta,\\phi)=\\theta^{\\top}A\\phi$ 的生成对抗网络（GAN）的两种离散时间训练方案的局部稳定性。稳定性由在鞍点 $(\\theta,\\phi)=(0,0)$ 附近线性化的动力学的迭代矩阵的谱半径决定。\n\n设系统在步骤 $k$ 的状态为级联向量 $z_k = \\begin{pmatrix} \\theta_k \\\\ \\phi_k \\end{pmatrix}$。更新是线性的，因此对于一个合适的迭代矩阵 $M$，它们可以表示为 $z_{k+1} = M z_k$ 的形式。如果一个线性系统的迭代矩阵的谱半径 $\\rho(M)$ 小于或等于 $1$，则该系统是非扩张的（一种稳定形式）。\n\n为了分析这些矩阵，我们使用矩阵 $A$ 的奇异值分解（SVD），即 $A = U\\Sigma V^{\\top}$。其中，$U \\in \\mathbb{R}^{n\\times n}$ 和 $V \\in \\mathbb{R}^{m\\times m}$ 是正交矩阵，其列分别是 $A$ 的左奇异向量（$u_i$）和右奇异向量（$v_i$）。$\\Sigma \\in \\mathbb{R}^{n\\times m}$ 是一个包含非负奇异值 $\\sigma_i$ 的矩形对角矩阵。奇异向量满足 $Av_i = \\sigma_i u_i$ 和 $A^{\\top}u_i = \\sigma_i v_i$。通过将动力学投影到奇异向量基上，系统解耦为一组独立的 $2 \\times 2$ 系统，每个奇异值 $\\sigma_i$ 对应一个。我们在这些基中分析 $\\theta_k$ 和 $\\phi_k$ 的系数的动力学。\n\n**1. 同步梯度下降-上升 (SGDA)**\n\n更新规则为：\n$$ \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k} $$\n$$ \\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k} $$\n用矩阵形式表示，即 $z_{k+1} = M_{\\text{sim}} z_k$，迭代矩阵为：\n$$ M_{\\text{sim}} = \\begin{pmatrix} I & -\\eta_{G}A \\\\ \\eta_{D}A^{\\top} & I \\end{pmatrix} $$\n我们来分析对应于一个奇异值 $\\sigma_i$ 的单一模式的动力学。我们将 $\\theta_k = c_k u_i$ 和 $\\phi_k = d_k v_i$。系数 $(c_k, d_k)$ 的更新变为：\n$$ c_{k+1}u_i = c_k u_i - \\eta_{G} A (d_k v_i) = c_k u_i - \\eta_{G} d_k (\\sigma_i u_i) \\implies c_{k+1} = c_k - \\eta_{G}\\sigma_i d_k $$\n$$ d_{k+1}v_i = d_k v_i + \\eta_{D} A^{\\top} (c_k u_i) = d_k v_i + \\eta_{D} c_k (\\sigma_i v_i) \\implies d_{k+1} = d_k + \\eta_{D}\\sigma_i c_k $$\n这为每个模式产生一个 $2 \\times 2$ 系统：\n$$ \\begin{pmatrix} c_{k+1} \\\\ d_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & -\\eta_{G}\\sigma_i \\\\ \\eta_{D}\\sigma_i & 1 \\end{pmatrix} \\begin{pmatrix} c_k \\\\ d_k \\end{pmatrix} $$\n这个 $2 \\times 2$ 矩阵的特征值 $\\lambda$ 由特征方程 $(\\lambda-1)^2 - (-\\eta_{G}\\sigma_i)(\\eta_{D}\\sigma_i) = 0$ 给出，化简为 $(\\lambda-1)^2 = -\\eta_{G}\\eta_{D}\\sigma_i^2$。特征值为 $\\lambda = 1 \\pm i\\sigma_i\\sqrt{\\eta_{G}\\eta_{D}}$。\n这些特征值的模为 $|\\lambda| = \\sqrt{1^2 + (\\sigma_i\\sqrt{\\eta_{G}\\eta_{D}})^2} = \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_i^2}$。\n对于任何非零奇异值 $\\sigma_i > 0$ 和正学习率 $\\eta_G, \\eta_D > 0$，我们有 $|\\lambda| > 1$。完整系统矩阵 $M_{\\text{sim}}$ 的谱半径是所有奇异值对应模的最大值：\n$$ \\rho(M_{\\text{sim}}) = \\max_i \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_i^2} = \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_{\\max}^2} $$\n其中 $\\sigma_{\\max}$ 是 $A$ 的最大奇异值。由于对于任何正学习率的选择，都有 $\\rho(M_{\\text{sim}}) > 1$，所以同步 SGDA 方案总是局部扩张的，因而是不稳定的。\n\n**2. 双时间尺度更新规则（TTUR，交替）**\n\n更新规则为：\n$$ \\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k} $$\n$$ \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k+1} $$\n将第一个方程代入第二个方程，得到 $\\theta_k$ 的完整更新：\n$$ \\theta_{k+1} = \\theta_k - \\eta_G A (\\phi_k + \\eta_D A^{\\top}\\theta_k) = (I - \\eta_G \\eta_D A A^{\\top})\\theta_k - \\eta_G A \\phi_k $$\n迭代矩阵 $M_{\\text{alt}}$ 为：\n$$ M_{\\text{alt}} = \\begin{pmatrix} I - \\eta_{G}\\eta_{D}AA^{\\top} & -\\eta_{G}A \\\\ \\eta_{D}A^{\\top} & I \\end{pmatrix} $$\n再次，我们在奇异向量基中分析动力学。设 $\\theta_k = c_k u_i$ 和 $\\phi_k = d_k v_i$。系数更新为：\n$$ d_{k+1} = d_k + \\eta_{D}\\sigma_i c_k $$\n$$ c_{k+1} = c_k - \\eta_G \\sigma_i d_{k+1} = c_k - \\eta_G \\sigma_i (d_k + \\eta_D \\sigma_i c_k) = (1 - \\eta_G \\eta_D \\sigma_i^2)c_k - \\eta_G \\sigma_i d_k $$\n系数的 $2 \\times 2$ 系统为：\n$$ \\begin{pmatrix} c_{k+1} \\\\ d_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1-\\eta_{G}\\eta_{D}\\sigma_i^2 & -\\eta_{G}\\sigma_i \\\\ \\eta_{D}\\sigma_i & 1 \\end{pmatrix} \\begin{pmatrix} c_k \\\\ d_k \\end{pmatrix} $$\n设 $x_i = \\eta_G \\eta_D \\sigma_i^2$。特征值 $\\lambda$ 的特征方程为 $(1-x_i-\\lambda)(1-\\lambda) - (-\\eta_G \\sigma_i)(\\eta_D \\sigma_i) = 0$，化简为 $\\lambda^2 - (2-x_i)\\lambda + 1 = 0$。\n特征值为 $\\lambda = \\frac{(2-x_i) \\pm \\sqrt{(2-x_i)^2 - 4}}{2} = 1 - \\frac{x_i}{2} \\pm \\frac{\\sqrt{x_i^2 - 4x_i}}{2}$。\n\n我们根据 $x_i$ 的值来分析模 $|\\lambda|$：\n- **情况 $0 \\le x_i \\le 4$**：平方根下的项是非正的，所以特征值是一对共轭复数：$\\lambda = 1 - \\frac{x_i}{2} \\pm i\\frac{\\sqrt{4x_i - x_i^2}}{2}$。\n模的平方是 $|\\lambda|^2 = (1-\\frac{x_i}{2})^2 + (\\frac{\\sqrt{4x_i - x_i^2}}{2})^2 = 1 - x_i + \\frac{x_i^2}{4} + \\frac{4x_i - x_i^2}{4} = 1$。因此，$|\\lambda|=1$。\n- **情况 $x_i > 4$**：特征值是实数。根据韦达定理，它们的乘积为 $1$。这意味着如果一个的模大于 $1$，另一个的模必须小于 $1$。较大的模是 $|\\lambda| = |\\frac{x_i}{2}-1 + \\frac{\\sqrt{x_i^2-4x_i}}{2}|$。由于当 $x_i>4$ 时，$\\frac{x_i}{2}-1>1$，这个模严格大于 $1$。\n\n为了使交替 TTUR 方案是非扩张的，其谱半径 $\\rho(M_{\\text{alt}})$ 必须小于或等于 $1$。这要求对所有模式都有 $|\\lambda| \\le 1$，这又要求对所有 $i$ 都有 $x_i \\le 4$。这个条件必须对最大奇异值 $\\sigma_{\\max}$ 成立。\n因此，非扩张动力学的条件是 $\\eta_{G}\\eta_{D}\\sigma_{\\max}^2 \\le 4$。当 $r = \\eta_D/\\eta_G$ 时，这变为 $r\\eta_G^2\\sigma_{\\max}^2 \\le 4$。对于任何正比率 $r>0$，总可以选择足够小的学习率 $\\eta_G$，例如 $\\eta_G \\le \\frac{2}{\\sigma_{\\max}\\sqrt{r}}$，来满足这个条件。因此，与同步方案不同，交替 TTUR 方案对于任何正比率 $r$ 都可以被稳定。\n\n问题要求在所有 $r>0$ 的范围内可实现的最小谱半径。这等价于找到 $\\inf_{\\eta_G>0, \\eta_D>0} \\rho(M_{\\text{alt}})$。\n任何模式的特征值的模为 $|\\lambda_i| \\ge 1$。$|\\lambda_i|=1$ 的最小值在 $0 \\le x_i \\le 4$ 时达到。为了实现全系统谱半径为 $1$，我们需要对所有 $i$ 都有 $x_i \\le 4$，如果 $x_{\\max} = \\eta_G \\eta_D \\sigma_{\\max}^2 \\le 4$ 则可以保证。如前所示，通过选择足够小的学习率，这个条件对于任何 $r>0$ 都是可以实现的。当满足此条件时，每个模式的特征值模都为 $1$，所以整个系统的谱半径是 $\\rho(M_{\\text{alt}}) = \\max_i|\\lambda_i|=1$。由于模永远不会小于 $1$，可实现的最小谱半径是 $1$。", "answer": "$$\\boxed{1}$$", "id": "3127265"}, {"introduction": "在理解了 GAN 训练动态的内在不稳定性之后，一个自然的问题是如何主动地改善它。除了调整更新规则，修改目标函数也是一种强大而常用的策略。这个练习 [@problem_id:3127248] 聚焦于标签平滑（label smoothing）技术，这是一种防止判别器变得过于自信的有效正则化方法。通过推导在标签平滑下的最优判别器形式，并分析其对生成器梯度的影响，你将定量地理解该技术如何软化判别器的决策边界，从而为生成器提供更平滑、更稳定的学习信号。", "problem": "考虑标准的生成对抗网络 (GAN) 框架，其中生成器生成遵循密度 $p_{g}(x)$ 分布的样本，而真实数据遵循密度 $p_{r}(x)$。判别器输出一个标量 $D(x) \\in (0,1)$，该标量被解释为 $x$ 是真实样本的预测概率。判别器通过在真实样本和伪造样本上最大化二元交叉熵 (BCE) 目标函数来进行训练。\n\n假设我们在判别器训练期间对真实类别应用标签平滑：真实样本的目标标签从 $1$ 替换为 $1-\\alpha$（其中 $0  \\alpha  1$），而伪造样本的目标标签保持为 $0$。判别器的期望目标函数变为\n$$\nJ(D) \\;=\\; \\mathbb{E}_{x \\sim p_{r}}\\!\\left[(1-\\alpha)\\,\\ln D(x) \\;+\\; \\alpha\\,\\ln\\!\\big(1-D(x)\\big)\\right] \\;+\\; \\mathbb{E}_{x \\sim p_{g}}\\!\\left[\\ln\\!\\big(1-D(x)\\big)\\right].\n$$\n假设 $D$ 是在 $x$ 上逐点优化的，并将 $p_{r}(x)$ 和 $p_{g}(x)$ 视为固定的非负密度。\n\n你的任务是：\n1. 从 BCE 形式和 $J(D)$ 的逐点最大化出发，推导闭式最优判别器 $D^{\\star}(x)$，将其表示为 $p_{r}(x)$、$p_{g}(x)$ 和 $\\alpha$ 的函数。\n2. 考虑非饱和生成器目标 $L_{G}^{\\mathrm{ns}}(G) = -\\,\\mathbb{E}_{x \\sim p_{g}}[\\ln D(x)]$。将 $D$ 视为固定在 $D^{\\star}$，使用链式法则表示在生成样本 $x$ 处从判别器流向生成器的梯度，并分离出缩放反向传播的判别器梯度的标量乘数。计算该标量乘数在有标签平滑（$\\alpha > 0$）与无标签平滑（$\\alpha = 0$）时的比值，并将此比值简化为仅依赖于 $\\alpha$ 的闭式表达式。\n\n将你的最终答案以单行矩阵的形式给出，其第一个条目是 $D^{\\star}(x)$，第二个条目是第2部分得到的简化比值。不需要数值近似。", "solution": "该问题要求进行与带有单侧标签平滑的生成对抗网络 (GAN) 相关的两个推导。首先，我们必须推导最优判别器 $D^{\\star}(x)$。其次，我们必须分析这对生成器梯度的影响。\n\n**第1部分：最优判别器 $D^{\\star}(x)$ 的推导**\n\n判别器的目标函数 $J(D)$ 由下式给出：\n$$J(D) \\;=\\; \\mathbb{E}_{x \\sim p_{r}}\\!\\left[(1-\\alpha)\\,\\ln D(x) \\;+\\; \\alpha\\,\\ln\\!\\big(1-D(x)\\big)\\right] \\;+\\; \\mathbb{E}_{x \\sim p_{g}}\\!\\left[\\ln\\!\\big(1-D(x)\\big)\\right]$$\n其中 $p_r(x)$ 是真实数据密度，$p_g(x)$ 是生成器密度，$D(x)$ 是判别器输出，$\\alpha$ 是标签平滑参数，满足 $0  \\alpha  1$。\n\n我们可以将期望表示为数据空间上的积分：\n$$J(D) = \\int p_{r}(x)\\left[(1-\\alpha)\\ln D(x) + \\alpha\\ln(1-D(x))\\right] dx + \\int p_{g}(x)\\ln(1-D(x)) dx$$\n为了找到最优判别器 $D^{\\star}(x)$，我们可以关于函数 $D(x)$ 最大化这个泛函。由于积分是所有 $x$ 值的总和，我们可以对每个 $x$ 逐点最大化被积函数。让我们将被积函数定义为一个函数 $f(y)$，其中对于固定的 $x$，$y = D(x)$：\n$$f(y) = p_{r}(x)\\left[(1-\\alpha)\\ln(y) + \\alpha\\ln(1-y)\\right] + p_{g}(x)\\ln(1-y)$$\n我们可以收集包含 $\\ln(y)$ 和 $\\ln(1-y)$ 的项：\n$$f(y) = p_{r}(x)(1-\\alpha)\\ln(y) + \\left[p_{r}(x)\\alpha + p_{g}(x)\\right]\\ln(1-y)$$\n为了找到使 $f(y)$ 最大化的 $y$ 值，我们计算它关于 $y$ 的导数并将其设为零。\n$$\\frac{df}{dy} = \\frac{p_{r}(x)(1-\\alpha)}{y} - \\frac{p_{r}(x)\\alpha + p_{g}(x)}{1-y}$$\n将导数设为零，得到最优 $D^{\\star}(x)$ 的条件：\n$$\\frac{p_{r}(x)(1-\\alpha)}{D^{\\star}(x)} = \\frac{p_{r}(x)\\alpha + p_{g}(x)}{1-D^{\\star}(x)}$$\n现在，我们求解 $D^{\\star}(x)$：\n$$p_{r}(x)(1-\\alpha)(1-D^{\\star}(x)) = D^{\\star}(x)\\left[p_{r}(x)\\alpha + p_{g}(x)\\right]$$\n$$p_{r}(x)(1-\\alpha) - p_{r}(x)(1-\\alpha)D^{\\star}(x) = p_{r}(x)\\alpha D^{\\star}(x) + p_{g}(x)D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x)(1-\\alpha) + p_{r}(x)\\alpha + p_{g}(x)\\right]D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x) - p_{r}(x)\\alpha + p_{r}(x)\\alpha + p_{g}(x)\\right]D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x) + p_{g}(x)\\right]D^{\\star}(x)$$\n分离出 $D^{\\star}(x)$，我们得到最优判别器的闭式表达式：\n$$D^{\\star}(x) = \\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}$$\n这是问题第一部分的解。\n\n**第2部分：生成器梯度标量乘数的比值**\n\n生成器被训练以最小化非饱和目标：\n$$L_{G}^{\\mathrm{ns}}(G) = -\\,\\mathbb{E}_{x \\sim p_{g}}[\\ln D(x)]$$\n我们考虑单个生成样本 $x$，其损失贡献为 $L_{G}(x) = -\\ln D(x)$。我们需要求此损失相对于生成样本 $x$ 的梯度，然后该梯度会反向传播到生成器的参数。使用链式法则，梯度为：\n$$\\nabla_x L_{G}(x) = \\nabla_x (-\\ln D(x)) = -\\frac{1}{D(x)} \\nabla_x D(x)$$\n项 $\\nabla_x D(x)$ 是判别器输出相对于其输入的梯度，它由深度学习框架计算和反向传播。问题要求的是缩放此梯度的标量乘数。从上面的表达式中，这个标量乘数是：\n$$M(x) = -\\frac{1}{D(x)}$$\n我们被要求在判别器最优时，即 $D(x) = D^{\\star}(x)$ 时，评估这个乘数。所以乘数变为：\n$$M(x, \\alpha) = -\\frac{1}{D^{\\star}(x)} = -\\frac{1}{\\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}} = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-\\alpha)}$$\n这是有标签平滑（$\\alpha > 0$）时的乘数。为了找到没有标签平滑时的乘数，我们设置 $\\alpha=0$：\n$$M(x, 0) = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-0)} = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)}$$\n问题要求计算有标签平滑下的标量乘数与无标签平滑下的值的比值。\n$$\\text{Ratio} = \\frac{M(x, \\alpha)}{M(x, 0)} = \\frac{-\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-\\alpha)}}{-\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)}}$$\n分子中的公因子 $-\\left(p_{r}(x) + p_{g}(x)\\right)$ 和分母中的 $p_r(x)$ 相互抵消：\n$$\\text{Ratio} = \\frac{\\frac{1}{1-\\alpha}}{1} = \\frac{1}{1-\\alpha}$$\n这个简化的表达式仅依赖于 $\\alpha$，并表示由于单侧标签平滑导致的生成器梯度幅度的缩放因子。\n\n最终答案的两个结果是：\n1.  $D^{\\star}(x) = \\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}$\n2.  比值为 $\\frac{1}{1-\\alpha}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)} & \\frac{1}{1-\\alpha}\n\\end{pmatrix}\n}\n$$", "id": "3127248"}, {"introduction": "模式崩溃（mode collapse）是 GAN 训练中最臭名昭著的失败模式之一，即生成器只学会了数据分布中的少数几个模式。为了直接解决这个问题，研究人员设计了多种旨在促进样本多样性的方法。这个练习 [@problem_id:3127206] 介绍了一种经典技术——小批量判别（minibatch discrimination），它通过在判别器的输入中加入一个衡量小批量内部样本相似度的项，来明确地惩罚模式崩溃。通过推导包含该项的生成器损失函数梯度，你将深入理解其背后的力学原理，即它如何迫使生成器在每个训练批次中产出更多样化的样本。", "problem": "考虑一个生成对抗网络 (GAN)，它被定义为一个生成器和一个判别器之间的双人博弈，其中生成器 $G$ 将一个潜变量 $z$ 映射到一个数据点 $x$，而判别器 $D$ 估计一个输入来自真实数据分布而非来自 $G$ 的概率。设生成器为标量线性映射 $G_{w}(z) = w z$，其参数为 $w \\in \\mathbb{R}$ 且 $z \\in \\mathbb{R}$。设判别器的输出为 $D(x, s) = \\sigma(h(x, s))$，其中 $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$ 是 logistic sigmoid 函数，判别器的 logit 为 $h(x, s) = a x + b - \\lambda s$，其中 $a, b, \\lambda \\in \\mathbb{R}$。为了鼓励一个小批量 (minibatch) 内生成器输出的多样性，用一个小批量判别相似性项来增强判别器\n$$\nc(x, x') = \\exp\\!\\big(-\\beta (x - x')^{2}\\big),\n$$\n其中 $\\beta > 0$，并为一个小批量 $\\{z_{i}\\}_{i=1}^{n}$ 定义单样本相似性摘要\n$$\ns_{i} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} c(x_{i}, x_{j}), \\quad x_{i} = G_{w}(z_{i}).\n$$\n假设生成器使用非饱和目标\n$$\nL_{G}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\big(D(x_{i}, s_{i})\\big), \\quad \\ell(p) = -\\ln(p).\n$$\n任务：\n1. 仅从这些定义出发，推导出 $\\frac{d L_{G}}{d w}$ 关于 $a$、$b$、$\\lambda$、$\\beta$、$w$ 和 $\\{z_{i}\\}_{i=1}^{n}$ 的闭式表达式。\n2. 对于小批量大小 $n = 3$，其中 $z_{1} = -1$，$z_{2} = 0$，$z_{3} = 1$，参数 $a = 1.5$，$b = 0$，$\\lambda = 2$，$\\beta = 0.5$，在 $w = 1$ 处数值评估该导数。将最终数值答案四舍五入到四位有效数字。将最终结果表示为一个无量纲标量。\n3. 根据您推导出的梯度，定性解释小批量判别贡献如何鼓励整个小批量的输出多样性，并讨论当小批量大小 $n$ 相对于数据分布中不同模式的数量较小时的潜在问题。此部分不需要额外的计算。", "solution": "该问题被评估为有效，因为它在科学上基于机器学习和微积分的原理，问题阐述清晰，目标明确，并提供了一套完整且一致的定义。因此，我们可以进行求解。\n\n该问题分为三个任务：1) 推导生成器损失函数的梯度，2) 对特定情况下的该梯度进行数值评估，3) 定性解释小批量判别项的作用。\n\n### 任务 1：梯度 $\\frac{d L_{G}}{d w}$ 的推导\n\n生成器的损失函数由下式给出：\n$$L_{G}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\big(D(x_{i}, s_{i})\\big)$$\n其中 $\\ell(p) = -\\ln(p)$，$x_i = G_w(z_i) = w z_i$，以及 $D(x, s) = \\sigma(h(x,s))$ 是判别器输出。\n损失函数关于生成器参数 $w$ 的导数是：\n$$\\frac{d L_{G}}{d w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{d}{dw} \\left[ -\\ln\\left(D(x_{i}, s_{i})\\right) \\right]$$\n我们来分析求和中的单个项。使用链式法则，我们有：\n$$\\frac{d}{dw} \\left[ -\\ln(D_i) \\right] = -\\frac{1}{D_i} \\frac{dD_i}{dw}$$\n这里我们使用简写 $D_i = D(x_i, s_i)$。\n\n判别器输出为 $D_i = \\sigma(h_i)$，其中 $h_i = h(x_i, s_i) = a x_i + b - \\lambda s_i$。sigmoid 函数 $\\sigma(u)$ 的导数是 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$。再次应用链式法则：\n$$\\frac{dD_i}{dw} = \\frac{d}{dw} \\sigma(h_i) = \\sigma'(h_i) \\frac{dh_i}{dw} = \\sigma(h_i)(1-\\sigma(h_i)) \\frac{dh_i}{dw} = D_i(1 - D_i) \\frac{dh_i}{dw}$$\n将此代入单个损失项的导数中：\n$$\\frac{d}{dw} \\left[ -\\ln(D_i) \\right] = -\\frac{1}{D_i} \\left[ D_i(1 - D_i) \\frac{dh_i}{dw} \\right] = -(1 - D_i) \\frac{dh_i}{dw}$$\n这是非饱和生成器损失的一个标准结果。\n\n接下来，我们需要求判别器 logit $h_i$ 关于 $w$ 的导数：\n$$h_i = a x_i + b - \\lambda s_i$$\n$$\\frac{dh_i}{dw} = \\frac{d}{dw} (a x_i + b - \\lambda s_i) = a \\frac{dx_i}{dw} - \\lambda \\frac{ds_i}{dw}$$\n给定 $x_i = w z_i$，其导数为 $\\frac{dx_i}{dw} = z_i$。\n\n现在，我们计算相似性摘要项 $s_i$ 的导数：\n$$s_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} c(x_{i}, x_{j})$$\n$$\\frac{ds_i}{dw} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} \\frac{d}{dw} c(x_i, x_j)$$\n相似性函数为 $c(x, x') = \\exp(-\\beta (x - x')^2)$。其关于 $w$ 的导数通过链式法则求得：\n$$\\frac{d}{dw} c(x_i, x_j) = \\exp(-\\beta (x_i - x_j)^2) \\cdot \\frac{d}{dw} (-\\beta (x_i - x_j)^2)$$\n$$\\frac{d}{dw} c(x_i, x_j) = c(x_i, x_j) \\cdot (-2\\beta (x_i - x_j)) \\cdot \\frac{d}{dw}(x_i - x_j)$$\n由于 $x_i = w z_i$ 和 $x_j = w z_j$，我们有 $x_i - x_j = w(z_i - z_j)$，所以 $\\frac{d}{dw}(x_i - x_j) = z_i - z_j$。\n将其代入，我们得到：\n$$\\frac{d}{dw} c(x_i, x_j) = c(x_i, x_j) \\cdot (-2\\beta w(z_i - z_j)) \\cdot (z_i - z_j) = -2\\beta w (z_i - z_j)^2 c(x_i, x_j)$$\n因此，$s_i$ 的导数是：\n$$\\frac{ds_i}{dw} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} \\left[ -2\\beta w (z_i - z_j)^2 c(x_i, x_j) \\right] = -2\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j)$$\n现在我们将此代入 $\\frac{dh_i}{dw}$ 的表达式中：\n$$\\frac{dh_i}{dw} = a z_i - \\lambda \\left[ -2\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j) \\right] = a z_i + 2\\lambda\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j)$$\n最后，我们组合出 $\\frac{d L_{G}}{d w}$ 的完整表达式：\n$$\\frac{d L_{G}}{d w} = \\frac{1}{n} \\sum_{i=1}^{n} -(1 - D(x_i, s_i)) \\left[ a z_i + 2\\lambda\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j) \\right]$$\n其中 $x_i = w z_i$，$s_i = \\sum_{j \\neq i} c(x_i, x_j)$，$c(x_i, x_j) = \\exp(-\\beta(x_i-x_j)^2)$，以及 $D(x_i, s_i) = \\sigma(a x_i + b - \\lambda s_i)$。这就是所求的闭式表达式。\n\n### 任务 2：梯度的数值评估\n\n给定以下值：\n$n = 3$，$z_1 = -1$，$z_2 = 0$，$z_3 = 1$。\n$a = 1.5$，$b = 0$，$\\lambda = 2$，$\\beta = 0.5$，我们在 $w = 1$ 处进行评估。\n\n当 $w = 1$ 时，生成的样本为 $x_i = 1 \\cdot z_i = z_i$，所以 $x_1 = -1$，$x_2 = 0$，$x_3 = 1$。\n\n首先，计算相似性项 $c(x_i, x_j) = \\exp(-0.5(x_i - x_j)^2)$：\n$c(x_1, x_2) = c(x_2, x_1) = \\exp(-0.5(-1 - 0)^2) = \\exp(-0.5)$。\n$c(x_1, x_3) = c(x_3, x_1) = \\exp(-0.5(-1 - 1)^2) = \\exp(-0.5(-2)^2) = \\exp(-2)$。\n$c(x_2, x_3) = c(x_3, x_2) = \\exp(-0.5(0 - 1)^2) = \\exp(-0.5)$。\n\n接下来，计算单样本相似性摘要 $s_i = \\sum_{j \\neq i} c(x_i, x_j)$：\n$s_1 = c(x_1, x_2) + c(x_1, x_3) = \\exp(-0.5) + \\exp(-2)$。\n$s_2 = c(x_2, x_1) + c(x_2, x_3) = \\exp(-0.5) + \\exp(-0.5) = 2\\exp(-0.5)$。\n$s_3 = c(x_3, x_1) + c(x_3, x_2) = \\exp(-2) + \\exp(-0.5) = s_1$。\n\n现在计算判别器 logit $h_i = a x_i + b - \\lambda s_i = 1.5 x_i - 2 s_i$：\n$h_1 = 1.5(-1) - 2(\\exp(-0.5) + \\exp(-2))$。\n$h_2 = 1.5(0) - 2(2\\exp(-0.5)) = -4\\exp(-0.5)$。\n$h_3 = 1.5(1) - 2(\\exp(-0.5) + \\exp(-2))$。\n\n我们还需要任务 1 结果中方括号内的项，我们称之为 $K_i$：\n$K_i = a z_i + 2\\lambda\\beta w \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j) = 1.5 z_i + 2(2)(0.5)(1) \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j) = 1.5 z_i + 2 \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j)$。\n$K_1 = 1.5(-1) + 2\\left[(z_1-z_2)^2 c(x_1,x_2) + (z_1-z_3)^2 c(x_1,x_3)\\right] = -1.5 + 2\\left[(-1)^2 \\exp(-0.5) + (-2)^2 \\exp(-2)\\right] = -1.5 + 2\\exp(-0.5) + 8\\exp(-2)$。\n$K_2 = 1.5(0) + 2\\left[(z_2-z_1)^2 c(x_2,x_1) + (z_2-z_3)^2 c(x_2,x_3)\\right] = 2\\left[(1)^2 \\exp(-0.5) + (-1)^2 \\exp(-0.5)\\right] = 4\\exp(-0.5)$。\n$K_3 = 1.5(1) + 2\\left[(z_3-z_1)^2 c(x_3,x_1) + (z_3-z_2)^2 c(x_3,x_2)\\right] = 1.5 + 2\\left[(2)^2 \\exp(-2) + (1)^2 \\exp(-0.5)\\right] = 1.5 + 8\\exp(-2) + 2\\exp(-0.5)$。\n\n现在，我们进行数值评估：\n$\\exp(-0.5) \\approx 0.606531$\n$\\exp(-2) \\approx 0.135335$\n\n$s_1 = s_3 \\approx 0.606531 + 0.135335 = 0.741866$。\n$s_2 \\approx 2 \\times 0.606531 = 1.213062$。\n\n$h_1 \\approx -1.5 - 2(0.741866) = -2.983732$。\n$h_2 \\approx -4(0.606531) = -2.426124$。\n$h_3 \\approx 1.5 - 2(0.741866) = 0.016268$。\n\n$K_1 \\approx -1.5 + 2(0.606531) + 8(0.135335) = -1.5 + 1.213062 + 1.08268 = 0.795742$。\n$K_2 \\approx 4(0.606531) = 2.426124$。\n$K_3 \\approx 1.5 + 8(0.135335) + 2(0.606531) = 1.5 + 1.08268 + 1.213062 = 3.795742$。\n\n梯度为 $\\frac{d L_G}{d w} = \\frac{1}{3} \\sum_{i=1}^3 -(1 - \\sigma(h_i)) K_i = \\frac{1}{3} \\sum_{i=1}^3 -\\sigma(-h_i) K_i$。\n$\\sigma(-h_1) = \\sigma(2.983732) = \\frac{1}{1 + \\exp(-2.983732)} \\approx 0.951825$。\n$\\sigma(-h_2) = \\sigma(2.426124) = \\frac{1}{1 + \\exp(-2.426124)} \\approx 0.918790$。\n$\\sigma(-h_3) = \\sigma(-0.016268) = \\frac{1}{1 + \\exp(0.016268)} \\approx 0.495935$。\n\n求和中的项为：\n第 1 项：$-\\sigma(-h_1) K_1 \\approx -0.951825 \\times 0.795742 \\approx -0.75747$。\n第 2 项：$-\\sigma(-h_2) K_2 \\approx -0.918790 \\times 2.426124 \\approx -2.22940$。\n第 3 项：$-\\sigma(-h_3) K_3 \\approx -0.495935 \\times 3.795742 \\approx -1.88251$。\n\n和 $= -0.75747 - 2.22940 - 1.88251 = -4.86938$。\n$\\frac{d L_G}{d w} = \\frac{-4.86938}{3} \\approx -1.623127$。\n\n四舍五入到四位有效数字，结果是 $-1.623$。\n\n### 任务 3：定性解释\n\n生成器的目标是生成判别器分类为真实的样本。对于非饱和损失，这意味着生成器旨在最大化判别器的输出概率 $D(x_i, s_i) = \\sigma(h_i)$，这等同于最大化 logit $h_i = a x_i + b - \\lambda s_i$。\n\n小批量判别项以 $-\\lambda s_i$ 的形式进入 logit，其中 $s_i = \\sum_{j \\neq i} \\exp(-\\beta (x_i-x_j)^2)$。项 $s_i$ 是样本 $x_i$ 与小批量中其他样本相似度的度量。如果生成器产生的样本彼此非常接近（多样性低，可能出现模式坍塌），距离 $|x_i-x_j|$ 将会很小，使得指数项 $\\exp(-\\beta(x_i-x_j)^2)$ 接近于 $1$。这会导致一个大的 $s_i$，由于 $-\\lambda$ 系数（其中 $\\lambda > 0$），会显著惩罚 logit $h_i$。较低的 logit 意味着判别器更可能将样本分类为假的。为了抵消这一点并欺骗判别器，生成器被迫产生彼此相距较远的样本，即增加距离 $|x_i-x_j|$。这直接鼓励了小批量内的多样性。\n\n任务 1 中推导的梯度反映了这一点。$w$ 的梯度更新由诸如 $(1 - D_i) \\lambda (-\\frac{ds_i}{dw})$ 的项驱动。由于 $\\frac{ds_i}{dw} = -2\\beta w \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j)$ 通常是负的（对于 $w>0$），总梯度的这一部分是负的。在梯度下降 ($w \\leftarrow w - \\eta \\frac{dL_G}{dw}$) 中，负梯度导致 $w$ 的增加。增加 $w$ 会放大所有距离 $|x_i-x_j|=|w(z_i-z_j)|$，有效地将样本推开并促进多样性。\n\n当小批量大小 $n$ 相对于真实数据分布中不同模式的数量较小时，会出现一个显著的潜在问题。小批量判别在每个批次内*局部地*强制多样性。如果 $n$ 太小，单个批次可能只包含来自一个或少数几个真实模式的样本。该机制仍会试图将这些样本推开，这可能是有害的。它可能会阻止生成器学习单个模式内真实的（通常很小的）方差，从而有效地扭曲学习到的分布。此外，它不能防止批次间的模式坍塌。生成器可能会学会在一个批次中为一个模式生成多样化的样本，然后在下一个批次中为*同一个*模式生成多样化的样本，而永远不会发现数据分布的其他模式。因此，虽然它能有效防止完全坍塌到单个点，但其确保覆盖所有数据模式的能力受到小批量大小的限制。", "answer": "$$\\boxed{-1.623}$$", "id": "3127206"}]}