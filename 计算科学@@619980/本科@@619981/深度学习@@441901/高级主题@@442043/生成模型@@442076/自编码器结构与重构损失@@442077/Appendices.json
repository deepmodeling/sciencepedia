{"hands_on_practices": [{"introduction": "选择正确的损失函数是训练自编码器的关键第一步。虽然均方误差（MSE）是一种常见的选择，但对于具有 Sigmoid 输出和二元目标的任务，二元交叉熵（BCE）通常表现更优。本练习将通过严谨的数学推导，揭示为何 BCE 能够有效避免 MSE 在输出饱和区域遇到的梯度消失问题，从而实现更快速、更稳定的学习过程 [@problem_id:3099815]。这个“纸笔”练习将加深你对损失函数选择背后理论的理解。", "problem": "考虑一个单输出的自编码器解码器，其输出激活函数为 sigmoid 函数 $\\sigma(z)$，其中 $z$ 是标量预激活（logit）。重建目标是二元的，$x \\in \\{0,1\\}$，预测的重建结果为 $y = \\sigma(z)$。下面定义了两种常见的重建损失：\n\n- 二元交叉熵 (BCE)：$L_{\\mathrm{BCE}}(y;x) = -\\left[x \\ln(y) + (1 - x)\\ln(1 - y)\\right]$。\n- 均方误差 (MSE)：$L_{\\mathrm{MSE}}(y;x) = \\frac{1}{2}(y - x)^{2}$。\n\n仅从上述定义、sigmoid 函数的定义 $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ 以及基础微积分（链式法则）出发，完成以下任务：\n\n1. 推导每种损失关于 logit $z$ 的梯度，即 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}$ 和 $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}$，并将每个梯度表示为 $y$ 和 $x$ 的函数。\n2. 针对二元目标 $x \\in \\{0,1\\}$ 的情况，将梯度大小 $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right|$ 和 $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right|$ 简化为仅关于 $y$ 的函数。\n3. 推导梯度大小之比 $r(y)$ 的闭式表达式，\n   $$\\boxed{\\frac{1}{y(1 - y)}}$$\n   其中 $x \\in \\{0,1\\}$，并将其完全简化为关于 $y$ 的函数。\n4. 基于 $r(y)$ 的表达式，简要解释为什么当 sigmoid 输出在 $y \\approx 0$ 或 $y \\approx 1$ 附近饱和时，二元交叉熵能比均方误差带来更快的学习速度。\n5. 通过对以下两种情况进行数值计算 $r(y)$ 来经验性地验证您的符号推导结果：$x=1$ 且 $y = 10^{-4}$，以及 $x=0$ 且 $y = 1 - 10^{-4}$。将您的数值结果保留四位有效数字。\n\n你的最终答案应该仅为第3部分中得到的 $r(y)$ 的简化闭式表达式。", "solution": "目标是分析对于一个带有 sigmoid 激活函数的单输出单元，二元交叉熵 (BCE) 和均方误差 (MSE) 损失函数关于 logit $z$ 的梯度。该分析将阐明为什么在二元重建任务中，BCE 通常比 MSE 更受青睐。\n\n推导的核心依赖于微分链式法则。给定一个损失函数 $L$，它依赖于模型输出 $y$，而 $y$ 又是 logit $z$ 的函数（即 $y = \\sigma(z)$），则损失关于 logit 的梯度为：\n$$\n\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z}\n$$\n首先，我们推导公共项 $\\frac{\\partial y}{\\partial z}$，即 sigmoid 函数 $\\sigma(z)$ 的导数。\n给定 $y = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$，它关于 $z$ 的导数为：\n$$\n\\frac{\\partial y}{\\partial z} = \\frac{d}{dz} (1 + \\exp(-z))^{-1} = -1 \\cdot (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}\n$$\n该表达式可以通过用 $y = \\sigma(z)$ 来表示从而进行简化：\n$$\n\\frac{\\partial y}{\\partial z} = \\left(\\frac{1}{1 + \\exp(-z)}\\right) \\left(\\frac{\\exp(-z)}{1 + \\exp(-z)}\\right) = \\left(\\frac{1}{1 + \\exp(-z)}\\right) \\left(\\frac{1 + \\exp(-z) - 1}{1 + \\exp(-z)}\\right) = y (1 - y)\n$$\n这是 sigmoid 函数的一个标准且有用的恒等式。\n\n**1. 梯度推导**\n\n现在我们计算每种损失函数的梯度。\n\n**二元交叉熵 (BCE) 梯度：**\nBCE 损失定义为 $L_{\\mathrm{BCE}}(y;x) = -\\left[x \\ln(y) + (1 - x)\\ln(1 - y)\\right]$。\n首先，我们求关于 $y$ 的偏导数：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial y} = -\\left[ \\frac{x}{y} - \\frac{1 - x}{1 - y} \\right] = -\\frac{x(1 - y) - (1 - x)y}{y(1 - y)} = -\\frac{x - xy - y + xy}{y(1 - y)} = -\\frac{x - y}{y(1 - y)} = \\frac{y - x}{y(1 - y)}\n$$\n使用链式法则，我们求得关于 $z$ 的梯度：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{BCE}}}{\\partial y} \\frac{\\partial y}{\\partial z} = \\left( \\frac{y - x}{y(1 - y)} \\right) \\cdot (y(1 - y)) = y - x\n$$\n\n**均方误差 (MSE) 梯度：**\nMSE 损失定义为 $L_{\\mathrm{MSE}}(y;x) = \\frac{1}{2}(y - x)^{2}$。\n首先，我们求关于 $y$ 的偏导数：\n$$\n\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial y} = \\frac{1}{2} \\cdot 2(y - x) \\cdot 1 = y - x\n$$\n使用链式法则，我们求得关于 $z$ 的梯度：\n$$\n\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{MSE}}}{\\partial y} \\frac{\\partial y}{\\partial z} = (y - x) \\cdot (y(1 - y))\n$$\n\n**2. 针对二元目标的梯度大小**\n\n我们现在将这些结果具体化到二元目标值 $x \\in \\{0, 1\\}$。sigmoid 函数的输出严格满足 $y \\in (0, 1)$。\n\n**BCE 梯度大小：**\n梯度为 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y - x$。\n- 如果 $x = 0$：$\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y$。因为 $y > 0$，所以大小为 $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right| = y$。\n- 如果 $x = 1$：$\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y - 1$。因为 $y  1$，所以 $y-1$ 这一项是负数。其大小为 $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right| = -(y - 1) = 1 - y$。\n\n**MSE 梯度大小：**\n梯度为 $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y - x)y(1 - y)$。对于 $y \\in (0, 1)$，$y(1-y)$ 这一项总是正数。\n- 如果 $x = 0$：$\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = y \\cdot y(1 - y) = y^2(1 - y)$。这一项是正数。其大小为 $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right| = y^2(1 - y)$。\n- 如果 $x = 1$：$\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y - 1)y(1 - y) = -y(1 - y)^2$。这一项是负数。其大小为 $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right| = -(-y(1 - y)^2) = y(1 - y)^2$。\n\n**3. 梯度大小之比**\n\n我们针对二元目标 $x$ 的两种情况计算比率 $r(y) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right|}$。\n\n- 如果 $x = 0$：\n$$\nr(y) = \\frac{y}{y^2(1 - y)} = \\frac{1}{y(1 - y)}\n$$\n- 如果 $x = 1$：\n$$\nr(y) = \\frac{1 - y}{y(1 - y)^2} = \\frac{1}{y(1 - y)}\n$$\n在两种情况下，比率的表达式是相同的。简化的闭式表达式为：\n$$\nr(y) = \\frac{1}{y(1 - y)}\n$$\n\n**4. 学习速度解释**\n\n梯度 $\\frac{\\partial L}{\\partial z}$ 是反向传播用以更新输出单元前一层权重的误差信号。通常，更大的梯度大小会导致更大的权重更新，从而带来更快的学习速度，尤其是在预测错误时。\n\nMSE 梯度 $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y-x)y(1-y)$ 包含因子 $y(1-y) = \\sigma'(z)$。当 sigmoid 输出 $y$ 饱和时，即当 $y \\to 0$ 或 $y \\to 1$ 时，$\\sigma'(z)$ 这一项会趋近于 0。如果模型做出了一个置信但错误的预测（例如，当目标 $x=1$ 时，$y \\approx 0$），$y(1-y)$ 这一项会变得非常小，导致整个梯度“消失”。这会使得学习变得极其缓慢，因为模型几乎接收不到任何信号来纠正其巨大的错误。\n\n相比之下，BCE 梯度 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y-x$ 没有这个有问题的 $y(1-y)$ 项。对数损失函数的使用恰好抵消了链式法则中的 $\\sigma'(z)$ 因子。因此，如果模型做出了一个置信但错误的预测，其梯度大小仍然很大。例如，如果 $x=1$ 且 $y \\approx 0$，梯度 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} \\approx 0-1 = -1$，这是一个强烈的修正信号。\n\n比率 $r(y) = \\frac{1}{y(1-y)}$ 在形式上捕捉了这种行为。当输出 $y$ 在 0 或 1 附近饱和时，分母 $y(1-y)$ 趋近于 0，导致比率 $r(y)$ 趋近于无穷大。这表明在这些饱和区域，BCE 的梯度大小变得比 MSE 的梯度大小要大得多（无界），从而引发快得多的学习，并帮助模型逃离不良的局部最小值。\n\n**5. 数值验证**\n\n我们针对给定的情况计算 $r(y) = \\frac{1}{y(1 - y)}$。\n\n- 情况1：$x=1$ 且 $y = 10^{-4}$。\n  此时，$y = 0.0001$。这是一个高度置信但错误的预测。\n  $$\n  r(10^{-4}) = \\frac{1}{0.0001 \\times (1 - 0.0001)} = \\frac{1}{0.0001 \\times 0.9999} = \\frac{1}{0.00009999} \\approx 10001.0001\n  $$\n  保留四位有效数字，结果为 $1.000 \\times 10^4$。\n\n- 情况2：$x=0$ 且 $y = 1 - 10^{-4}$。\n  此时，$y = 0.9999$。这同样是一个高度置信但错误的预测。\n  $$\n  r(1 - 10^{-4}) = \\frac{1}{0.9999 \\times (1 - 0.9999)} = \\frac{1}{0.9999 \\times 0.0001} = \\frac{1}{0.00009999} \\approx 10001.0001\n  $$\n  保留四位有效数字，结果为 $1.000 \\times 10^4$。\n\n数值结果证实，对于 sigmoid 输出饱和的置信错误预测，BCE 梯度大约是 MSE 梯度的 $10000$ 倍，从而验证了理论分析。", "answer": "$$\n\\boxed{\\frac{1}{y(1 - y)}}\n$$", "id": "3099815"}, {"introduction": "标准的重建损失（如 MSE）在像素级别上衡量误差，但这往往与人类的视觉感知不完全一致。为了设计一个更符合人类感知的自编码器，我们可以构建一个对特定类型误差（例如颜色失真）比其他误差（例如亮度变化）更敏感的损失函数。本练习 [@problem_id:3099813] 将指导你实现一个感知加权损失函数，它通过将 RGB 颜色差异转换到“对立色空间” (opponent-color space) 并对不同通道施加不同权重，从而更有效地惩罚不期望的颜色偏移。", "problem": "考虑深度学习中的一个自编码器，它通过编码器和解码器的组合将输入图像张量映射到重建图像。令输入图像表示为 $X \\in \\mathbb{R}^{H \\times W \\times 3}$，重建图像表示为 $\\hat{X} \\in \\mathbb{R}^{H \\times W \\times 3}$，其中最后一个维度对应于红、绿、蓝三个通道。标准的经验风险最小化目标使用 $X$ 和 $\\hat{X}$ 之间的重建损失，该损失聚合了像素级误差。本题的目标是设计一种重建损失，通过在对立色空间中测量误差，然后在平均之前应用通道权重，从而对感知上的颜色偏移施加比亮度误差更大的惩罚。\n\n使用一个线性对立色变换，该变换由一个矩阵 $M \\in \\mathbb{R}^{3 \\times 3}$ 对每个像素进行定义，将 $\\begin{bmatrix}R  G  B\\end{bmatrix}^\\top$ 映射到 $\\begin{bmatrix}L  RG  BY\\end{bmatrix}^\\top$，其方式如下：\n$$\nM \\;=\\;\n\\begin{bmatrix}\n0.2126  0.7152  0.0722 \\\\\n1  -1  0 \\\\\n0.5  0.5  -1\n\\end{bmatrix},\n$$\n其中 $L$ 表示亮度通道，$RG$ 表示红-绿对立通道，$BY$ 表示蓝-黄对立通道。您必须通过以下步骤构建重建损失：(i) 计算由 $M$ 导出的 $\\hat{X}$ 和 $X$ 之间的逐像素对立色差，(ii) 对这些差异应用通道级的非负权重 $\\alpha_L$、$\\alpha_{RG}$ 和 $\\alpha_{BY}$，以及 (iii) 使用平方欧几里得范数对所有像素进行平均来聚合结果。整个构建过程必须仅根据线性变换、欧几里得范数和均值聚合的基本定义来表达和实现；不要引入任何额外的启发式项。\n\n您的程序必须实现此损失，并在以下测试集上进行评估。所有图像值都应为闭区间 $\\left[0,1\\right]$ 内的无量纲强度值：\n\n- 测试用例 1（混合了亮度和颜色误差的一般情况）：令 $H = 2$ 且 $W = 2$。定义\n$$\nX_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.50  0.50  0.50 \\end{bmatrix} \n\\begin{bmatrix} 0.20  0.60  0.20 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.90  0.10  0.10 \\end{bmatrix} \n\\begin{bmatrix} 0.10  0.90  0.90 \\end{bmatrix}\n\\end{bmatrix},\n\\quad\n\\hat{X}_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.51  0.50  0.49 \\end{bmatrix} \n\\begin{bmatrix} 0.18  0.62  0.20 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.90  0.12  0.08 \\end{bmatrix} \n\\begin{bmatrix} 0.13  0.89  0.88 \\end{bmatrix}\n\\end{bmatrix}.\n$$\n使用权重 $\\alpha_L = 1.0$、$\\alpha_{RG} = 3.0$ 和 $\\alpha_{BY} = 3.0$。\n\n- 测试用例 2（完美重建的边界情况）：令 $H = 2$ 且 $W = 2$。定义\n$$\nX_2 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.15  0.25  0.35 \\end{bmatrix} \n\\begin{bmatrix} 0.45  0.55  0.65 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.40  0.30  0.20 \\end{bmatrix} \n\\begin{bmatrix} 0.60  0.70  0.80 \\end{bmatrix}\n\\end{bmatrix},\n\\quad\n\\hat{X}_2 = X_2.\n$$\n使用权重 $\\alpha_L = 1.0$、$\\alpha_{RG} = 3.0$ 和 $\\alpha_{BY} = 3.0$。\n\n- 测试用例 3（每个像素只有纯色移而无亮度变化的边缘情况）：令 $H = 2$ 且 $W = 2$。定义\n$$\nX_3 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.50  0.50  0.50 \\end{bmatrix} \n\\begin{bmatrix} 0.50  0.50  0.50 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.50  0.50  0.50 \\end{bmatrix} \n\\begin{bmatrix} 0.50  0.50  0.50 \\end{bmatrix}\n\\end{bmatrix},\n$$\n并通过为每个像素设置通道增量 $\\Delta R = 0.05$、$\\Delta G = -\\left(\\frac{0.2126}{0.7152}\\right)\\Delta R$ 和 $\\Delta B = 0.0$ 来构建 $\\hat{X}_3$，然后为每个像素定义 $\\hat{X}_3 = X_3 + \\begin{bmatrix} \\Delta R  \\Delta G  \\Delta B \\end{bmatrix}$。使用权重 $\\alpha_L = 1.0$、$\\alpha_{RG} = 3.0$ 和 $\\alpha_{BY} = 3.0$。\n\n您的程序应生成单行输出，其中包含测试用例 1、2 和 3 的三个损失值，格式为用方括号括起来的逗号分隔列表。每个数字必须是四舍五入到六位小数的浮点值，例如 $\\left[0.123456,0.000000,0.987654\\right]$。", "solution": "问题陈述是有效的，因为它在科学上基于色彩理论和深度学习实践，问题设定良好，提供了所有必要信息，并且表述客观。我们将继续推导指定的重建损失，并将其应用于给定的测试用例。\n\n目标是构建一个重建损失函数 $\\mathcal{L}$，该函数作用于输入图像张量 $X \\in \\mathbb{R}^{H \\times W \\times 3}$ 及其重建 $\\hat{X} \\in \\mathbb{R}^{H \\times W \\times 3}$。该损失函数应对感知上的颜色偏移比对亮度变化更敏感。这是通过将像素级误差转换到对立色空间，应用通道特定的权重，然后聚合结果来实现的。推导过程遵循基于基础数学运算的、有原则的、分步构建方法。\n\n令 $X_{ij} \\in \\mathbb{R}^3$ 和 $\\hat{X}_{ij} \\in \\mathbb{R}^3$ 分别表示空间位置 $(i,j)$ 处像素的红-绿-蓝（RGB）颜色向量，其中 $i \\in \\{1, \\dots, H\\}$ 且 $j \\in \\{1, \\dots, W\\}$。\n\n**1. RGB 空间中的逐像素误差**\n第一步是计算标准 RGB 色彩空间中重建像素值与原始像素值之间的差异。这个逐像素差异向量表示为 $\\Delta_{ij}$：\n$$\n\\Delta_{ij} = \\hat{X}_{ij} - X_{ij} \\in \\mathbb{R}^3\n$$\n\n**2. 转换为对立色空间**\n感知加权策略的核心是在对立色空间中分析此误差。问题提供了一个线性变换矩阵 $M \\in \\mathbb{R}^{3 \\times 3}$，该矩阵将一个 RGB 向量映射到一个包含亮度（$L$）、红-绿（$RG$）和蓝-黄（$BY$）分量的向量。\n$$\nM =\n\\begin{bmatrix}\n0.2126  0.7152  0.0722 \\\\\n1  -1  0 \\\\\n0.5  0.5  -1\n\\end{bmatrix}\n$$\n将此线性变换应用于 RGB 误差向量 $\\Delta_{ij}$，得到对立色空间中的误差向量 $o_{ij} \\in \\mathbb{R}^3$：\n$$\no_{ij} = M \\Delta_{ij} = \\begin{bmatrix} o_{L,ij} \\\\ o_{RG,ij} \\\\ o_{BY,ij} \\end{bmatrix}\n$$\n在此，$o_{L,ij}$、$o_{RG,ij}$ 和 $o_{BY,ij}$ 分别表示沿亮度、红-绿和蓝-黄轴的误差分量。\n\n**3. 加权逐像素误差**\n为了比亮度误差更重地惩罚颜色误差，将非负权重 $\\alpha_L$、$\\alpha_{RG}$ 和 $\\alpha_{BY}$ 应用于对立色误差向量 $o_{ij}$ 的分量的平方。逐像素损失 $L_{ij}$ 是这些平方误差的加权和，这等同于 $o_{ij}$ 的加权平方欧几里得范数。\n$$\nL_{ij} = \\alpha_L (o_{L,ij})^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2\n$$\n\n**4. 聚合成最终损失**\n整个图像的总重建损失 $\\mathcal{L}(X, \\hat{X})$ 是所有 $N = H \\times W$ 个像素的逐像素损失 $L_{ij}$ 的平均值。\n$$\n\\mathcal{L}(X, \\hat{X}) = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} L_{ij} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\alpha_L (o_{L,ij})^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2 \\right)\n$$\n这就完成了重建损失的正式定义。我们现在为三个指定的测试用例评估此损失。在所有情况下，权重均为 $\\alpha_L = 1.0$、$\\alpha_{RG} = 3.0$ 和 $\\alpha_{BY} = 3.0$。\n\n**测试用例 1：一般情况**\n给定 $X_1$ 和 $\\hat{X}_1$。逐像素 RGB 差异张量为：\n$$\n\\Delta_1 = \\hat{X}_1 - X_1 =\n\\begin{bmatrix}\n\\begin{bmatrix} 0.01  0.00  -0.01 \\end{bmatrix} \n\\begin{bmatrix} -0.02  0.02  0.00 \\end{bmatrix} \\\\\n\\begin{bmatrix} 0.00  0.02  -0.02 \\end{bmatrix} \n\\begin{bmatrix} 0.03  -0.01  -0.01 \\end{bmatrix}\n\\end{bmatrix}\n$$\n我们应用变换 $o_{ij} = M \\Delta_{ij}$，并计算 4 个像素中每个像素的加权平方误差。计算这些误差的总和，然后除以 4。计算得出的总损失为 $\\mathcal{L}_1 \\approx 0.003986$。\n\n**测试用例 2：完美重建**\n此处，$\\hat{X}_2 = X_2$。这意味着差异张量 $\\Delta_2 = \\hat{X}_2 - X_2$ 是一个零张量。因此，对立色误差向量 $o_{ij}$ 均为零向量，逐像素损失 $L_{ij}$ 也都为 0。因此总损失为：\n$$\n\\mathcal{L}_2 = 0.0\n$$\n\n**测试用例 3：纯色移**\n图像 $X_3$ 是均匀的灰色。$\\hat{X}_3$ 是通过将一个常数向量 $[\\Delta R, \\Delta G, \\Delta B]^\\top$ 加到 $X_3$ 的每个像素上构建的，其中 $\\Delta R = 0.05$，$\\Delta G = -\\left(\\frac{0.2126}{0.7152}\\right)\\Delta R$，且 $\\Delta B = 0.0$。逐像素 RGB 差异在整个图像上是恒定的：\n$$\n\\Delta_{ij} = \\left[ 0.05, -0.05 \\left(\\frac{0.2126}{0.7152}\\right), 0 \\right]^\\top \\approx \\left[ 0.05, -0.014863, 0 \\right]^\\top\n$$\n转换到对立色空间得到 $o_{ij} = M \\Delta_{ij}$。根据构造，此误差的亮度分量为零：\n$$\no_{L,ij} = 0.2126(\\Delta R) + 0.7152(\\Delta G) = 0.2126(\\Delta R) + 0.7152\\left(-\\frac{0.2126}{0.7152}\\Delta R\\right) = 0\n$$\n颜色分量非零。由于所有像素的逐像素误差都相同，总损失 $\\mathcal{L}_3$ 等于单个像素的误差：\n$$\n\\mathcal{L}_3 = L_{ij} = \\alpha_L (0)^2 + \\alpha_{RG} (o_{RG,ij})^2 + \\alpha_{BY} (o_{BY,ij})^2\n$$\n计算得出 $o_{RG,ij} \\approx 0.064863$ 和 $o_{BY,ij} \\approx 0.017568$。最终损失为 $\\mathcal{L}_3 \\approx 0.013548$。\n最终结果是 $[0.003986, 0.000000, 0.013548]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_loss(X, X_hat, M, alpha):\n    \"\"\"\n    Calculates the perceptually-weighted reconstruction loss.\n\n    Args:\n        X (np.ndarray): The original image tensor of shape (H, W, 3).\n        X_hat (np.ndarray): The reconstructed image tensor of shape (H, W, 3).\n        M (np.ndarray): The opponent-color transformation matrix of shape (3, 3).\n        alpha (np.ndarray): The channel-wise weights of shape (3,).\n\n    Returns:\n        float: The final aggregated loss value.\n    \"\"\"\n    # Step 1: Compute the difference tensor in RGB space.\n    # The shape of delta is (H, W, 3).\n    delta = X_hat - X\n\n    # Step 2: Transform the RGB differences to the opponent-color space.\n    # We perform a matrix multiplication for each pixel's 3-element vector.\n    # In numpy, (H, W, 3) @ (3, 3) correctly performs this operation,\n    # resulting in a tensor of shape (H, W, 3).\n    # M must be transposed to align dimensions for the matmul.\n    opp_diff = delta @ M.T\n\n    # Step 3: Apply channel-wise weights and compute the squared error.\n    # Square the opponent differences element-wise.\n    opp_diff_sq = opp_diff**2\n    # Apply weights. The alpha array of shape (3,) is broadcast\n    # across the (H, W) dimensions.\n    weighted_opp_diff_sq = alpha * opp_diff_sq\n    # Sum the weighted squared errors across the color channels (axis=2)\n    # to get the per-pixel loss.\n    pixel_errors = np.sum(weighted_opp_diff_sq, axis=2)\n\n    # Step 4: Aggregate by averaging over all pixels.\n    # np.mean computes the average of all elements in the pixel_errors tensor.\n    loss = np.mean(pixel_errors)\n\n    return loss\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute their losses.\n    \"\"\"\n    # Define the opponent-color transform matrix and weights\n    M = np.array([\n        [0.2126, 0.7152, 0.0722],\n        [1.0, -1.0, 0.0],\n        [0.5, 0.5, -1.0]\n    ])\n    alpha = np.array([1.0, 3.0, 3.0])\n\n    # Test Case 1\n    X1 = np.array([\n        [[0.50, 0.50, 0.50], [0.20, 0.60, 0.20]],\n        [[0.90, 0.10, 0.10], [0.10, 0.90, 0.90]]\n    ])\n    X1_hat = np.array([\n        [[0.51, 0.50, 0.49], [0.18, 0.62, 0.20]],\n        [[0.90, 0.12, 0.08], [0.13, 0.89, 0.88]]\n    ])\n    loss1 = calculate_loss(X1, X1_hat, M, alpha)\n\n    # Test Case 2\n    X2 = np.array([\n        [[0.15, 0.25, 0.35], [0.45, 0.55, 0.65]],\n        [[0.40, 0.30, 0.20], [0.60, 0.70, 0.80]]\n    ])\n    X2_hat = X2.copy()  # Perfect reconstruction\n    loss2 = calculate_loss(X2, X2_hat, M, alpha)\n\n    # Test Case 3\n    X3 = np.full((2, 2, 3), 0.50)\n    delta_R = 0.05\n    delta_G = -(0.2126 / 0.7152) * delta_R\n    delta_B = 0.0\n    pixel_delta = np.array([delta_R, delta_G, delta_B])\n    X3_hat = X3 + pixel_delta\n    loss3 = calculate_loss(X3, X3_hat, M, alpha)\n\n    results = [loss1, loss2, loss3]\n    \n    # Format the output string as required\n    results_str = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3099813"}, {"introduction": "除了损失函数，自编码器的架构设计也直接影响其重建图像细节的能力。对于卷积自编码器，一个核心的设计参数是卷积核的大小，它决定了模型的“感受野” (receptive field)。本练习 [@problem_id:3099855] 将通过一个简化的信号处理实验，让你亲手探究感受野大小对精细细节重建质量的影响。你将观察到，虽然较大的感受野有助于平滑噪声和捕捉宏观结构，但它也可能以牺牲高频细节为代价。", "problem": "你需要实现并分析一个简化的一维卷积自编码器，以研究在两种不同的重建损失（均方误差和平均绝对误差）下，感受野大小如何影响精細细节的重建质量。你的程序必须是一个完整的、可运行的脚本，能够计算指定的量，并以要求的格式打印最终结果。\n\n请基于以下基本设定：\n\n- 一个一维自编码器通过一个编码器将输入信号 $x \\in \\mathbb{R}^N$ 映射到潜层表示，并通过一个解码器对其进行重建。在本问题中，编码器和解码器都是线性的、移位不变的算子，实现为与有限脉冲响应核的离散互相关。\n- 编码器计算互相关 $y = x \\star w$，其中，对于一个奇数长度的核 $w \\in \\mathbb{R}^K$ ($K \\ge 1$)，带零填充的离散互相关定义为\n$$\ny[n] = \\sum_{j=0}^{K-1} w[j]\\; x[n + j - c], \\quad c = \\left\\lfloor \\frac{K}{2} \\right\\rfloor,\n$$\n其中对于索引 $m$ 在 $[0, N-1]$ 之外的情况，$x[m] = 0$。这将产生一个与 $x$ 长度相同的输出 $y$，长度为 $N$。\n- 解码器使用反转的核 $\\tilde{w}[j] = w[K-1-j]$ 计算另一次互相关，从而得到重建结果 $\\hat{x} = y \\star \\tilde{w}$。这对应于一个步长为1、带零填充且权重绑定的线性卷积自编码器，因此整体映射是一个线性的、时不变的平滑算子。\n- 重建损失是逐样本定义并沿信号进行平均的：\n  - 均方误差 (MSE): $L_2(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left( \\hat{x}[n] - x[n] \\right)^2$。\n  - 平均绝对误差 (L1): $L_1(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left| \\hat{x}[n] - x[n] \\right|$。\n- 感受野是核的长度 $K$，核的条目取自一个归一化的盒式滤波器：$w[j] = \\frac{1}{K}$，对于 $j \\in \\{0,1,\\dots,K-1\\}$。\n\n你的任务是使用指定的架构（不执行训练；权重已由定义固定），量化在 $L_2$ 和 $L_1$ 两种损失下，$K$ 如何影响精细细节的重建。为了分离感受野对精细细节内容的影响，你将评估三个长度为 $N$ 的合成输入：\n\n- $x^{(1)}$: 在中心的单位脉冲，$x^{(1)}[n] = 1$ 如果 $n = \\frac{N}{2}$，否则 $x^{(1)}[n] = 0$。\n- $x^{(2)}$: 高频交替余弦，$x^{(2)}[n] = \\cos(\\pi n)$。\n- $x^{(3)}$: 低频余弦，$x^{(3)}[n] = \\cos\\!\\left( \\frac{2\\pi f_0 n}{N} \\right)$。\n\n使用 $N = 128$ 和 $f_0 = 4$。\n\n对于下面测试套件中的每个核大小 $K$，计算以下内容：\n\n1. 对于每个信号 $x^{(i)}$（$i \\in \\{1,2,3\\}$），使用上面定义的编码器-解码器构成 $\\hat{x}^{(i)}$。\n2. 计算 $L_2\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right)$ 和 $L_1\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right)$。\n3. 对三个信号的损失进行平均，以获得\n$$\n\\overline{L}_2(K) = \\frac{1}{3} \\sum_{i=1}^3 L_2\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right), \\quad\n\\overline{L}_1(K) = \\frac{1}{3} \\sum_{i=1}^3 L_1\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right).\n$$\n4. 使用脉冲信号量化精细细节的保留程度：计算比率\n$$\nr(K) = \\frac{\\hat{x}^{(1)}\\!\\left[\\frac{N}{2}\\right]}{x^{(1)}\\!\\left[\\frac{N}{2}\\right]}.\n$$\n由于 $x^{(1)}\\!\\left[\\frac{N}{2}\\right] = 1$，这可以简化为 $r(K) = \\hat{x}^{(1)}\\!\\left[\\frac{N}{2}\\right]$。\n\n测试套件：\n\n- 使用核大小 $K \\in \\{1, 3, 7, 15\\}$。\n\n答案规格：\n\n- 对于每个 $K$（按升序排列），生成一个结果列表 $[K, \\overline{L}_2(K), \\overline{L}_1(K), r(K)]$。\n- 你的程序应生成单行输出，其中包含一个由这些列表组成的、以逗号分隔的列表，并用方括号括起来。值 $\\overline{L}_2(K)$、$\\overline{L}_1(K)$ 和 $r(K)$ 必须四舍五入到六位小数。例如，一个有效的输出格式是\n$[[1,0.000000,0.000000,1.000000],[3, \\dots],[7, \\dots],[15, \\dots]]$\n（打印行中没有空格）。", "solution": "该问题要求对一个简化的一维线性卷积自编码器进行分析。任务的核心是实现指定的信号处理链，并量化由核长度 $K$ 决定的感受野大小如何影响具有不同频率特性的信号的重建。该自编码器的权重被固定为归一化的盒式滤波器，这意味着不涉及训练。分析完全基于所定义架构的信号处理特性。\n\n输入是一个一维信号 $x \\in \\mathbb{R}^N$。编码器通过与核 $w \\in \\mathbb{R}^K$ 的离散互相关将输入 $x$ 映射到潜层表示 $y$：$y = x \\star w$。解码器通过对 $y$ 应用与反转核 $\\tilde{w}$ 的另一次互相关来重建信号 $\\hat{x}$：$\\hat{x} = y \\star \\tilde{w}$。核大小 $K$ 必须是奇数。互相关定义了零填充以保持信号长度为 $N$：\n$$y[n] = \\sum_{j=0}^{K-1} w[j]\\; x[n + j - c], \\quad c = \\left\\lfloor \\frac{K}{2} \\right\\rfloor$$\n核权重由归一化的盒式滤波器给出，$w[j] = 1/K$ 对于所有 $j \\in \\{0, \\dots, K-1\\}$。该核的一个关键特性是其对称性，即 $w[j] = w[K-1-j]$。因此，反转的核 $\\tilde{w}$ 与原始核 $w$ 相同。从输入 $x$ 到重建 $\\hat{x}$ 的整体变换因此是两次相同互相关操作的级联：\n$$\\hat{x} = (x \\star w) \\star w$$\n此操作等效于输入信号 $x$ 与一个有效核的单次卷积，该有效核是 $w$ 的自卷积。这种变换构成了一个线性时不变 (LTI) 系统。\n\n核 $w$ 作为一个移动平均滤波器，是低通滤波器的一种基本形式。连续两次应用此滤波器会产生更强的低通滤波效果。完整的自编码器系统的有效脉冲响应是一个长度为 $2K-1$ 的三角滤波器。参数 $K$ 代表感受野大小，控制着该滤波器的宽度。更大的 $K$ 会导致更宽的有效滤波器，从而对信号进行更强的平滑处理。这种增强的平滑作用会更严重地衰减高频分量和精细细节。问题使用三个特定信号来探究此行为：\n1.  $x^{(1)}$: 单位脉冲。重建的 $\\hat{x}^{(1)}$ 是系统的脉冲响应，直接揭示了有效滤波器的形状。\n2.  $x^{(2)}$: 高频余弦 $\\cos(\\pi n)$。该信号会被低通滤波器强烈衰减，尤其是在 $K$ 很大时。\n3.  $x^{(3)}$: 低频余弦。预计该信号会比 $x^{(2)}$ 保留得更好，作为参考。\n\n计算过程如下。分析针对核大小 $K \\in \\{1, 3, 7, 15\\}$ 进行。对于每个 $K$ 值，执行以下步骤：\n1.  生成三个输入信号 $x^{(1)}$、$x^{(2)}$ 和 $x^{(3)}$，长度为 $N = 128$，频率参数为 $f_0 = 4$。\n    -   $x^{(1)}[n] = \\delta[n - N/2]$，其中 $N/2 = 64$。\n    -   $x^{(2)}[n] = \\cos(\\pi n)$。\n    -   $x^{(3)}[n] = \\cos(2\\pi f_0 n / N)$。\n2.  创建大小为 $K$ 的核 $w$，其条目为 $w[j] = 1/K$。\n3.  对于每个信号 $x^{(i)}$，通过两次应用指定的互相关操作来计算重建信号 $\\hat{x}^{(i)}$。\n4.  使用两个损失函数来衡量重建质量，即均方误差 ($L_2$) 和平均绝对误差 ($L_1$)，定义如下：\n    $$L_2(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{x}[n] - x[n])^2$$\n    $$L_1(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} |\\hat{x}[n] - x[n]|$$\n5.  将这些单独的损失在三个信号上进行平均，以产生最终的度量指标 $\\overline{L}_2(K)$ 和 $\\overline{L}_1(K)$。\n6.  精细细节的保留程度通过比率 $r(K) = \\hat{x}^{(1)}[N/2] / x^{(1)}[N/2]$ 来量化。由于 $x^{(1)}[N/2] = 1$，该比率简化为 $r(K) = \\hat{x}^{(1)}[N/2]$。基于系统脉冲响应的解析推导证实，该值恰好为 $1/K$，这可以作为实现的一个有价值的检验。\n最终输出汇总了测试套件中每个 $K$ 的这些计算值。", "answer": "```python\nimport numpy as np\n\ndef cross_correlate(x, w):\n    \"\"\"\n    Computes the 1D cross-correlation with zero-padding as defined in the problem.\n    y[n] = sum_{j=0}^{K-1} w[j] * x[n + j - c], where c = floor(K/2).\n    \"\"\"\n    N = len(x)\n    K = len(w)\n    # For odd K, floor(K/2) is equivalent to (K-1)//2.\n    c = (K - 1) // 2\n    y = np.zeros(N, dtype=np.float64)\n    \n    for n in range(N):\n        sum_val = 0.0\n        for j in range(K):\n            m = n + j - c\n            if 0 = m  N:\n                sum_val += w[j] * x[m]\n        y[n] = sum_val\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem as specified.\n    \"\"\"\n    # Define problem parameters\n    N = 128\n    f0 = 4\n    kernel_sizes = [1, 3, 7, 15]\n\n    # Generate input signals\n    n_indices = np.arange(N)\n    \n    # x^(1): Unit impulse at the center\n    x1 = np.zeros(N, dtype=np.float64)\n    center_idx = N // 2\n    x1[center_idx] = 1.0\n\n    # x^(2): High-frequency alternating cosine\n    x2 = np.cos(np.pi * n_indices)\n\n    # x^(3): Low-frequency cosine\n    x3 = np.cos(2 * np.pi * f0 * n_indices / N)\n    \n    signals = [x1, x2, x3]\n    \n    all_results = []\n\n    for K in kernel_sizes:\n        # Create the normalized box filter kernel\n        w = np.full(K, 1.0 / K, dtype=np.float64)\n        \n        # The reversed kernel w_tilde is the same as w because w is symmetric\n        w_tilde = w\n        \n        total_l2_loss = 0.0\n        total_l1_loss = 0.0\n        r_K = 0.0\n        \n        # Process each signal\n        for i, x in enumerate(signals):\n            # Form reconstruction x_hat by applying the operation twice\n            # Encoder pass\n            y = cross_correlate(x, w)\n            # Decoder pass\n            x_hat = cross_correlate(y, w_tilde)\n            \n            # Compute losses\n            l2_loss = np.mean((x_hat - x)**2)\n            l1_loss = np.mean(np.abs(x_hat - x))\n            \n            # Accumulate losses for averaging\n            total_l2_loss += l2_loss\n            total_l1_loss += l1_loss\n            \n            # For signal x1, compute the retention ratio r(K)\n            if i == 0:  # Signal x1 is at index 0\n                # r(K) = x_hat[N/2] / x[N/2]. Since x[N/2]=1, it's just x_hat[N/2]\n                r_K = x_hat[center_idx]\n\n        # Average the losses over the three signals\n        avg_l2 = total_l2_loss / 3.0\n        avg_l1 = total_l1_loss / 3.0\n        \n        # Store raw float values for this K\n        result_tuple = [K, avg_l2, avg_l1, r_K]\n        all_results.append(result_tuple)\n        \n    # Format the final output string as required\n    # The f-string formatting with ':.6f' handles rounding to 6 decimal places\n    formatted_results = [f\"[{k},{l2:.6f},{l1:.6f},{r:.6f}]\" for k, l2, l1, r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver\nsolve()\n```", "id": "3099855"}]}