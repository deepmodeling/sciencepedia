{"hands_on_practices": [{"introduction": "受限玻尔兹曼机 (RBM) 的训练依赖于对比散度 (Contrastive Divergence, CD) 算法，其中吉布斯采样步数 $k$ 是一个关键超参数。理论上，更大的 $k$ 值能提供更精确的梯度估计，但计算成本也更高。本练习将通过一个精心设计的实验，让你亲手比较不同 $k$ 值 ($k=1$ vs $k=10$) 对 RBM 学习数据分布能力的影响，并用“模式覆盖率”这一指标来量化其性能差异。[@problem_id:3170448]", "problem": "您的任务是设计并实现一个完整的、可运行的程序，该程序旨在凭经验比较使用 $k=1$ 步（CD-1）和 $k=10$ 步（CD-10）的对比散度（Contrastive Divergence）算法训练受限玻尔兹曼机（Restricted Boltzmann Machine, RBM）的效果。该 RBM 在一个已知其潜在模式的合成数据集上进行训练。比较的标准是通过将样本聚类分配到已知模式来衡量的模式覆盖率。\n\n请从以下基本概念开始：\n\n- 受限玻尔兹曼机（RBM）通过能量函数 $E(v,h)$ 和配分函数 $Z$ 定义了二元可见变量 $v \\in \\{0,1\\}^D$ 和二元隐藏变量 $h \\in \\{0,1\\}^H$ 上的联合分布。联合概率为 $p(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$。\n- 二元-二元 RBM 的能量函数由权重矩阵、可见偏置和隐藏偏置参数化。在给定隐藏单元的情况下，可见单元条件独立；在给定可见单元的情况下，隐藏单元条件独立。\n- 使用 $k$ 步吉布斯采样（CD-$k$）的对比散度通过从数据点开始一个短吉布斯链并运行 $k$ 次交替条件采样步骤来近似对数似然的梯度。\n\n问题要求：\n\n1. 构建一个具有已知二元基本模式集的合成数据集。令 $D=8$ 且有 $M=4$ 个基本模式 $m_1, m_2, m_3, m_4 \\in \\{0,1\\}^8$，明确定义如下：\n   - $m_1 = [1,1,0,0,1,0,1,0]$\n   - $m_2 = [0,1,1,0,0,1,0,1]$\n   - $m_3 = [1,0,1,1,0,0,1,0]$\n   - $m_4 = [0,0,0,1,1,1,0,0]$\n   对于每个模式 $m_i$，通过以概率 $p_{\\text{flip}} \\in [0,1]$ 独立翻转每一位来生成 $n_i$ 个样本，并将所有模式的样本聚合起来。这样就得到了一个具有已知模式计数 $(n_1,n_2,n_3,n_4)$ 的数据集。\n\n2. 在同一数据集上使用相同的初始参数训练两个 RBM，它们都具有二元可见单元和二元隐藏单元，唯一的区别在于对比散度步数 $k$：\n   - 使用 CD-1 的 RBM，\n   - 使用 CD-10 的 RBM。\n   使用固定的隐藏单元数 $H=6$、学习率 $\\alpha$、小批量训练和固定的周期数。确保两个 RBM 的随机初始化相同，以隔离 $k$ 的影响。\n\n3. 训练后，从每个训练好的 RBM 中抽取样本。抽样方法是从一个随机的二元可见向量开始运行吉布斯链。对于每个生成的样本，通过最小汉明距离将其分配到最近的基本模式。使用这些分配，定义模式覆盖率指标如下：\n   - 令 $\\mathcal{M} = \\{m_1, m_2, m_3, m_4\\}$ 为基本模式集。\n   - 对于一组生成的样本 $\\{v^{(s)}\\}_{s=1}^S$，通过 $i^\\star(s) = \\arg\\min_{i \\in \\{1,\\dots,M\\}} d_H(v^{(s)}, m_i)$ 为每个样本定义分配的模式索引，其中 $d_H$ 是汉明距离。\n   - 将覆盖率定义为至少有一个样本被分配到的模式所占的比例：$\\text{coverage} = \\frac{|\\{i \\in \\{1,\\dots,M\\} : \\exists s \\text{ with } i^\\star(s) = i\\}|}{M}$。\n   将覆盖率表示为 $[0,1]$ 范围的小数。\n\n4. 从 RBM 的能量定义和条件独立结构出发，实现完整的训练和评估流程。推导并实现基于数据驱动和模型驱动的相关性与偏置之差的 CD-$k$ 训练更新规则。\n\n5. 使用以下测试套件，其中指定了 $(n_1,n_2,n_3,n_4)$ 和 $p_{\\text{flip}}$：\n   - 情况 A（平衡，低噪声）：$(250,250,250,250)$，$p_{\\text{flip}} = 0.05$。\n   - 情况 B（平衡，较高噪声）：$(250,250,250,250)$，$p_{\\text{flip}} = 0.25$。\n   - 情况 C（不平衡，低噪声）：$(400,100,75,25)$，$p_{\\text{flip}} = 0.05$。\n\n6. 对于每种情况，使用相同的配置训练两个 RBM：$H=6$，学习率 $\\alpha = 0.1$，小批量大小为 $50$，训练 $100$ 个周期，并从每个训练好的模型中采样 $S=200$ 个点用于分配，每个样本的生成使用 $100$ 步吉布斯采样。\n\n7. 最终输出规范：您的程序应生成单行输出，其中包含三种情况的结果，格式为逗号分隔的对列表，每对列出 $[\\text{coverage}_{\\text{CD-1}}, \\text{coverage}_{\\text{CD-10}}]$。将每个覆盖率值四舍五入到三位小数，并严格按照以下格式输出：\n   - $[[c_{A,1},c_{A,10}],[c_{B,1},c_{B,10}],[c_{C,1},c_{C,10}]]$\n   其中 $c_{\\cdot,\\cdot} \\in [0,1]$ 是十进制数。\n\n本问题不涉及物理单位，也不存在角度。所有答案均为 $[0,1]$ 范围内的十进制数，并四舍五入到三位小数。请通过使用所述的 RBM 架构、指定的数据集生成过程和定义的评估指标来确保科学真实性。", "solution": "当前任务要求对使用 $k=1$ 和 $k=10$ 吉布斯采样步数的对比散度训练受限玻尔兹曼机（RBM）进行严格的实证比较。该解决方案围绕 RBM 的基本原理构建，从基于能量的模型定义到通过 CD-$k$ 进行梯度近似。\n\n**1. 受限玻尔兹曼机：理论框架**\n\nRBM 是一个能量模型，用于描述二元可见单元 $v \\in \\{0, 1\\}^D$ 和二元隐藏单元 $h \\in \\{0, 1\\}^H$ 的联合概率分布。该模型由参数 $\\theta = \\{W, a, b\\}$ 定义，其中 $W \\in \\mathbb{R}^{H \\times D}$ 是连接隐藏和可见单元的权重矩阵，$a \\in \\mathbb{R}^D$ 是可见单元偏置向量，$b \\in \\mathbb{R}^H$ 是隐藏单元偏置向量。\n\n能量函数 $E(v, h; \\theta)$ 定义了系统的状态：\n$$\nE(v, h) = - \\sum_{i=1}^D a_i v_i - \\sum_{j=1}^H b_j h_j - \\sum_{i=1}^D \\sum_{j=1}^H h_j W_{ji} v_i\n$$\n用矩阵表示为 $E(v, h) = -a^T v - b^T h - h^T W v$。\n\n联合概率分布由玻尔兹曼分布给出：\n$$\np(v, h) = \\frac{1}{Z} e^{-E(v, h)}\n$$\n其中 $Z = \\sum_{v', h'} e^{-E(v', h')}$ 是配分函数，一个难以计算的归一化常数。可见向量的边缘概率为 $p(v) = \\frac{1}{Z} \\sum_h e^{-E(v, h)}$。\n\nRBM 的一个关键特性是，在给定另一层状态的情况下，同一层内的单元是条件独立的。这使得高效的块吉布斯采样成为可能。条件概率为：\n$$\np(h_j=1 | v) = \\sigma\\left(b_j + \\sum_{i=1}^D W_{ji} v_i\\right) = \\sigma(b_j + (Wv)_j)\n$$\n$$\np(v_i=1 | h) = \\sigma\\left(a_i + \\sum_{j=1}^H h_j W_{ji}\\right) = \\sigma(a_i + (W^T h)_i)\n$$\n其中 $\\sigma(x) = (1 + e^{-x})^{-1}$ 是 logistic sigmoid 函数。\n\n**2. 通过对比散度（CD-$k$）进行训练**\n\n训练 RBM 的过程是调整 $\\theta$ 以最大化训练数据 $\\mathcal{D} = \\{v^{(n)}\\}$ 的对数似然。对于单个数据点 $v$，对数似然的梯度为：\n$$\n\\frac{\\partial \\log p(v)}{\\partial \\theta} = \\mathbb{E}_{h \\sim p(h|v)}\\left[-\\frac{\\partial E(v,h)}{\\partial \\theta}\\right] - \\mathbb{E}_{v',h' \\sim p(v',h')}\\left[-\\frac{\\partial E(v',h')}{\\partial \\theta}\\right]\n$$\n第一项，即“正相位”，是在给定数据下对隐藏状态的期望。第二项，即“负相位”，是对整个模型分布的期望。由于 $Z$ 的难计算性，负相位的计算在计算上是不可行的。\n\n$k$ 步对比散度（CD-$k$）近似了这个梯度。它将模型期望替换为从一个短的 $k$ 步吉布斯链中获得的样本的期望，该吉布斯链由一个数据向量初始化。对于给定的数据向量 $v^{(0)}$：\n1.  **正相位统计**: 基于数据计算相关性的期望。对于权重，此为 $v^{(0)} p(h|v^{(0)})^T$。\n2.  **负相位生成**: 运行一个 $k$ 步吉布斯链，从 $h_s^{(0)} \\sim p(h|v^{(0)})$ 开始：\n    $$\n    v^{(1)} \\sim p(v|h_s^{(0)}), h_s^{(1)} \\sim p(h|v^{(1)}), \\dots, v^{(k)} \\sim p(v|h_s^{(k-1)})\n    $$\n3.  **负相位统计**: 基于最终的“重构” $v^{(k)}$ 计算相关性，得到 $v^{(k)} p(h|v^{(k)})^T$。\n\n对于小批量的参数更新，由正相位和负相位统计量之间的平均差异推导得出，并按学习率 $\\alpha$ 进行缩放：\n$$\n\\Delta W \\propto \\alpha \\left( \\langle v^{(0)} p(h|v^{(0)})^T \\rangle_{\\text{batch}} - \\langle v^{(k)} p(h|v^{(k)})^T \\rangle_{\\text{batch}} \\right)\n$$\n$$\n\\Delta a \\propto \\alpha \\left( \\langle v^{(0)} \\rangle_{\\text{batch}} - \\langle v^{(k)} \\rangle_{\\text{batch}} \\right)\n$$\n$$\n\\Delta b \\propto \\alpha \\left( \\langle p(h|v^{(0)}) \\rangle_{\\text{batch}} - \\langle p(h|v^{(k)}) \\rangle_{\\text{batch}} \\right)\n$$\n本实验比较了 $k=1$（一种粗略但快速的近似）和 $k=10$（一种更准确但较慢的近似）。预计更大的 $k$ 会让吉布斯链从初始数据点移动得更远，更接近模型的平稳分布，从而产生更好的梯度估计。这通常会导致模型学习到对数据分布更好的表示，包括更有效地捕捉多个模式。\n\n**3. 实验设计与实现**\n\n实现严格遵循问题规范。\n-   **数据集生成**: 一个函数通过获取 $M=4$ 个基本模式，并以概率 $p_{\\text{flip}}$ 翻转位来创建每个模式 $m_i$ 的 $n_i$ 个噪声版本，从而生成合成数据。\n-   **RBM 类**: 一个 RBM 类封装了参数（$W, a, b$）和用于采样（$p(h|v)$, $p(v|h)$）与训练（CD-$k$ 更新）的方法。为确保公平比较，CD-1 和 CD-10 的 RBM 都使用相同的随机种子进行初始化，以保证初始参数相同。\n-   **训练流程**: 一个训练函数迭代固定的周期数，以小批量方式处理数据，并应用指定 $k$ 的 CD-$k$ 更新规则。在给定的测试案例中，数据的洗牌也使用相同的种子，以确保两次训练的批次顺序相同。\n-   **评估**: 训练后，通过从随机初始向量开始运行一个长的吉布斯链（$100$ 步）来从每个 RBM 生成样本。每个生成的样本使用汉明距离分配给最近的基本模式。模式覆盖率是本问题的一个实用指标，用于衡量模型复现数据中各种潜在模式的能力。一个学习了所有模式的 RBM，其生成的样本会落在所有四个基本模式附近，从而获得高的覆盖率分数。\n\n该实验针对三种不同情况进行：平衡且低噪声的数据集、平衡且高噪声的数据集，以及不平衡的数据集。这测试了算法对噪声和数据不平衡的鲁棒性。预期 CD-10 将表现出更优的模式覆盖率，尤其是在更具挑战性的不平衡情况下，因为 CD-1 的有偏梯度可能导致模型仅关注最频繁的模式。", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\n# Main class for the Restricted Boltzmann Machine\nclass RBM:\n    \"\"\"\n    A class for a binary-binary Restricted Boltzmann Machine (RBM).\n    \"\"\"\n\n    def __init__(self, n_visible, n_hidden, seed=None):\n        \"\"\"\n        Initializes the RBM with random weights and zero biases.\n        \n        Args:\n            n_visible (int): Number of visible units (D).\n            n_hidden (int): Number of hidden units (H).\n            seed (int, optional): Seed for the random number generator for reproducibility.\n        \"\"\"\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        self.rng = np.random.default_rng(seed)\n\n        # Initialize parameters\n        # Weights are initialized from a normal distribution with small variance\n        self.W = self.rng.normal(0, 0.01, (self.n_hidden, self.n_visible))\n        # Biases are initialized to zero\n        self.a = np.zeros(self.n_visible) # visible biases\n        self.b = np.zeros(self.n_hidden) # hidden biases\n\n    def _propup(self, v):\n        \"\"\"Calculates hidden layer activation probabilities given visible layer state.\"\"\"\n        return sigmoid(self.b + v @ self.W.T)\n\n    def _propdown(self, h):\n        \"\"\"Calculates visible layer activation probabilities given hidden layer state.\"\"\"\n        return sigmoid(self.a + h @ self.W)\n\n    def sample_h_given_v(self, v):\n        \"\"\"Samples hidden layer states given visible layer states.\"\"\"\n        h_probs = self._propup(v)\n        return self.rng.binomial(1, h_probs)\n\n    def sample_v_given_h(self, h):\n        \"\"\"Samples visible layer states given hidden layer states.\"\"\"\n        v_probs = self._propdown(h)\n        return self.rng.binomial(1, v_probs)\n\n    def update_params(self, v_batch, k, learning_rate):\n        \"\"\"\n        Performs a single parameter update using k-step Contrastive Divergence (CD-k).\n        \n        Args:\n            v_batch (np.ndarray): A mini-batch of data samples.\n            k (int): The number of Gibbs sampling steps.\n            learning_rate (float): The learning rate for the update.\n        \"\"\"\n        batch_size = v_batch.shape[0]\n        \n        # --- Positive Phase ---\n        v0 = v_batch\n        ph0_probs = self._propup(v0)\n        \n        # --- Negative Phase ---\n        vk = v0\n        for _ in range(k):\n            hk_samples = self.sample_h_given_v(vk)\n            vk = self.sample_v_given_h(hk_samples)\n        \n        phk_probs = self._propup(vk)\n\n        # --- Calculate Gradients ---\n        # Note: We use probabilities for the hidden units for a lower variance gradient\n        grad_W = (ph0_probs.T @ v0 - phk_probs.T @ vk) / batch_size\n        grad_a = np.mean(v0 - vk, axis=0)\n        grad_b = np.mean(ph0_probs - phk_probs, axis=0)\n\n        # --- Update Parameters ---\n        self.W += learning_rate * grad_W\n        self.a += learning_rate * grad_a\n        self.b += learning_rate * grad_b\n\n    def sample(self, n_samples, n_gibbs_steps, seed=None):\n        \"\"\"\n        Generates samples from the RBM by running a Gibbs chain.\n        \"\"\"\n        eval_rng = np.random.default_rng(seed)\n        v = eval_rng.integers(0, 2, size=(n_samples, self.n_visible))\n        for _ in range(n_gibbs_steps):\n            h = self.sample_h_given_v(v)\n            v = self.sample_v_given_h(h)\n        return v\n\ndef generate_dataset(modes, mode_counts, p_flip, seed=None):\n    \"\"\"\n    Generates a synthetic dataset from base modes by flipping bits.\n    \"\"\"\n    data_rng = np.random.default_rng(seed)\n    dataset = []\n    for mode, count in zip(modes, mode_counts):\n        for _ in range(count):\n            noise = data_rng.choice([0, 1], size=mode.shape, p=[1 - p_flip, p_flip])\n            sample = np.bitwise_xor(mode, noise)\n            dataset.append(sample)\n    \n    dataset = np.array(dataset)\n    data_rng.shuffle(dataset)\n    return dataset\n\ndef train_rbm(rbm, data, epochs, batch_size, k, learning_rate, seed=None):\n    \"\"\"\n    Trains an RBM on the provided data.\n    \"\"\"\n    train_rng = np.random.default_rng(seed)\n    n_samples = data.shape[0]\n    \n    for epoch in range(epochs):\n        indices = np.arange(n_samples)\n        train_rng.shuffle(indices)\n        \n        for i in range(0, n_samples, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            v_batch = data[batch_indices]\n            rbm.update_params(v_batch, k, learning_rate)\n\ndef calculate_coverage(samples, modes):\n    \"\"\"\n    Calculates mode coverage based on Hamming distance.\n    \"\"\"\n    # Calculate Hamming distance from each sample to each mode\n    # Broadcasting: (n_samples, 1, D) vs (1, n_modes, D)\n    distances = np.sum(samples[:, np.newaxis, :] != modes[np.newaxis, :, :], axis=2)\n    \n    # Assign each sample to the nearest mode\n    assignments = np.argmin(distances, axis=1)\n    \n    # Find the unique modes that were covered\n    covered_mode_indices = np.unique(assignments)\n    \n    # Calculate coverage fraction\n    coverage = len(covered_mode_indices) / len(modes)\n    return coverage\n\ndef solve():\n    \"\"\"\n    Main function to run the RBM comparison experiment.\n    \"\"\"\n    # --- Problem Constants and Parameters ---\n    D = 8  # Number of visible units\n    H = 6  # Number of hidden units\n    M = 4  # Number of modes\n    \n    base_modes = np.array([\n        [1, 1, 0, 0, 1, 0, 1, 0],\n        [0, 1, 1, 0, 0, 1, 0, 1],\n        [1, 0, 1, 1, 0, 0, 1, 0],\n        [0, 0, 0, 1, 1, 1, 0, 0]\n    ])\n\n    # Test cases: (mode_counts, p_flip)\n    test_cases = [\n        ((250, 250, 250, 250), 0.05),  # Case A\n        ((250, 250, 250, 250), 0.25),  # Case B\n        ((400, 100, 75, 25), 0.05),    # Case C\n    ]\n\n    # Training and evaluation parameters\n    learning_rate = 0.1\n    epochs = 100\n    batch_size = 50\n    n_eval_samples = 200\n    n_gibbs_steps_eval = 100\n\n    results = []\n    \n    # Master RNG for reproducibility of the entire experiment\n    master_rng = np.random.default_rng(42)\n\n    for mode_counts, p_flip in test_cases:\n        # Generate seeds for this case to ensure fair comparison\n        data_seed = master_rng.integers(2**32 - 1)\n        init_seed = master_rng.integers(2**32 - 1)\n        train_seed = master_rng.integers(2**32 - 1)\n        eval_seed = master_rng.integers(2**32 - 1)\n\n        # 1. Generate dataset\n        dataset = generate_dataset(base_modes, mode_counts, p_flip, seed=data_seed)\n\n        # 2. Train and evaluate for k=1 and k=10\n        case_results = []\n        for k_val in [1, 10]:\n            # Initialize RBM with identical starting weights\n            rbm = RBM(n_visible=D, n_hidden=H, seed=init_seed)\n            \n            # Train RBM with identical batch ordering\n            train_rbm(rbm, dataset, epochs=epochs, batch_size=batch_size, \n                      k=k_val, learning_rate=learning_rate, seed=train_seed)\n            \n            # Generate samples for evaluation\n            samples = rbm.sample(n_samples=n_eval_samples, \n                                 n_gibbs_steps=n_gibbs_steps_eval, \n                                 seed=eval_seed)\n            \n            # Calculate mode coverage\n            coverage = calculate_coverage(samples, base_modes)\n            case_results.append(round(coverage, 3))\n        \n        results.append(case_results)\n    \n    # Format and print the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3170448"}, {"introduction": "RBM 的一个核心目标是学习数据中解耦的、有意义的潜在特征。理想情况下，不同的隐藏单元应能捕捉到数据中相互独立的变异来源。本练习将引导你设计一个诊断测试，通过计算隐藏单元预期激活值的协方差矩阵，来量化特征的独立性，并进一步探讨权重向量的正交性与特征解耦之间的关系。[@problem_id:3170455]", "problem": "给定一个二元-二元受限玻尔兹曼机（RBM）的框架。该RBM有一个可见二元向量 $v \\in \\{0,1\\}^{m}$，一个隐藏二元向量 $h \\in \\{0,1\\}^{n}$，一个权重矩阵 $W \\in \\mathbb{R}^{m \\times n}$，一个可见偏置向量 $a \\in \\mathbb{R}^{m}$，以及一个隐藏偏置向量 $b \\in \\mathbb{R}^{n}$。在 $(v,h)$ 上的联合概率分布由基于能量的模型定义\n$$\nE(v,h) = - a^{\\top} v - b^{\\top} h - v^{\\top} W h,\n$$\n其中\n$$\np(v,h) = \\frac{1}{Z} \\exp\\left(-E(v,h)\\right),\n$$\n其中 $Z$ 是满足以下条件的配分函数\n$$\nZ = \\sum_{v \\in \\{0,1\\}^{m}} \\sum_{h \\in \\{0,1\\}^{n}} \\exp\\left(-E(v,h)\\right).\n$$\n基于这些基础，推导条件分布 $p(h \\mid v)$，并用它来获得 $\\mathbb{E}[h \\mid v]$ 的分量表达式。然后，对于一个可见向量的数据集 $\\{v^{(i)}\\}_{i=1}^{N}$，定义矩阵 $H \\in \\mathbb{R}^{N \\times n}$，其第 $i$ 行为 $\\mathbb{E}[h \\mid v^{(i)}]^{\\top}$，并计算数据集中期望隐藏激活的样本协方差矩阵 $C \\in \\mathbb{R}^{n \\times n}$。样本协方差矩阵必须按如下方式计算\n$$\n\\mu = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[h \\mid v^{(i)}], \\quad C = \\frac{1}{N-1} \\sum_{i=1}^{N} \\left(\\mathbb{E}[h \\mid v^{(i)}] - \\mu\\right)\\left(\\mathbb{E}[h \\mid v^{(i)}] - \\mu\\right)^{\\top}.\n$$\n通过测量 $C$ 接近对角矩阵的程度，设计一个用于隐藏特征近似独立性的定量测试。定义独立性偏差度量\n$$\n\\mathrm{ID}(C) = \\frac{\\sum_{i \\neq j} C_{ij}^{2}}{\\sum_{k=1}^{n} C_{kk}^{2} + \\epsilon},\n$$\n其中 $\\epsilon = 10^{-12}$ 是一个用于数值稳定性的小常数。$\\mathrm{ID}(C)$ 的一个较小值表示协方差矩阵接近对角矩阵，因此隐藏特征在期望上跨数据是近似独立的。\n\n此外，定义一个对隐藏特征权重向量的正交性惩罚，以鼓励近似对角的协方差。令 $W_{:,j}$ 表示 $W$ 的第 $j$ 列（隐藏单元 $j$ 的权重向量）。构建格拉姆矩阵 $G = W^{\\top} W \\in \\mathbb{R}^{n \\times n}$ 并定义\n$$\n\\mathrm{OP}(W) = \\frac{\\sum_{i \\neq j} G_{ij}^{2}}{\\sum_{k=1}^{n} G_{kk}^{2} + \\epsilon}.\n$$\n$\\mathrm{OP}(W)$ 的一个较小值表示隐藏单元的权重向量近似正交，这应该会促使 $\\mathrm{ID}(C)$ 变小。\n\n实现一个程序，对于给定的RBM参数和数据集，为每个测试用例计算 $\\mathrm{ID}(C)$ 和 $\\mathrm{OP}(W)$。\n\n使用以下测试套件，其中包括一个一般情况、一个相关权重情况和一个边界情况：\n\n- 测试用例 1 (一般“顺利”情况): $m = 4$, $n = 3$, $W = \\begin{bmatrix} 5  0  0 \\\\ 0  5  0 \\\\ 0  0  5 \\\\ 0  0  0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $a$ 在此任务中未使用，可以设置为 $\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。$N = 8$ 个可见向量的数据集：\n$$\n\\{(0,0,0,0),(1,0,0,0),(0,1,0,0),(0,0,1,0),(1,1,0,0),(1,0,1,0),(0,1,1,0),(1,1,1,0)\\}.\n$$\n- 测试用例 2 (相关隐藏特征): $m = 4$, $n = 3$, $W = \\begin{bmatrix} 5  5  0 \\\\ 5  5  0 \\\\ 0  0  5 \\\\ 0  0  0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, 数据集与测试用例 1 相同。\n- 测试用例 3 (边界情况): $m = 4$, $n = 3$, $W = \\begin{bmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, 数据集与测试用例 1 相同。\n\n你的程序必须：\n- 从上述RBM基础知识中推导并使用 $\\mathbb{E}[h \\mid v]$。\n- 为每个测试用例计算 $H$ 的行的样本协方差矩阵 $C$。\n- 为每个测试用例计算 $\\mathrm{ID}(C)$ 和 $\\mathrm{OP}(W)$，使用 $\\epsilon = 10^{-12}$。\n- 将结果汇总到单行输出中，按顺序包含所有测试用例的数值，每个测试用例依次贡献两个数，首先是 $\\mathrm{ID}(C)$，然后是 $\\mathrm{OP}(W)$。因此，最终输出必须是以下格式的单行\n$$\n[\\mathrm{ID}_1,\\mathrm{OP}_1,\\mathrm{ID}_2,\\mathrm{OP}_2,\\mathrm{ID}_3,\\mathrm{OP}_3],\n$$\n其中每个项目都是一个浮点数。不得打印任何其他文本。", "solution": "该问题是有效的，因为它在科学上基于受限玻尔兹曼机（RBMs）的理论，数学上是适定的、客观的，并为唯一解提供了所有必要信息。\n\n我们的目标是为三个给定的测试用例计算独立性偏差度量 $\\mathrm{ID}(C)$ 和正交性惩罚 $\\mathrm{OP}(W)$。这需要多步推导和计算。\n\n首先，我们必须推导在给定可见单元的情况下，隐藏单元的条件期望表达式 $\\mathbb{E}[h \\mid v]$。条件概率分布 $p(h \\mid v)$ 由联合概率 $p(v,h)$ 与边缘概率 $p(v)$ 的比值给出：\n$$\np(h \\mid v) = \\frac{p(v,h)}{p(v)} = \\frac{\\frac{1}{Z} \\exp(-E(v,h))}{\\sum_{h' \\in \\{0,1\\}^n} \\frac{1}{Z} \\exp(-E(v,h'))} = \\frac{\\exp(-E(v,h))}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(-E(v,h'))}.\n$$\n代入能量函数 $E(v,h) = -a^{\\top} v - b^{\\top} h - v^{\\top} W h$：\n$$\np(h \\mid v) = \\frac{\\exp(a^{\\top} v + b^{\\top} h + v^{\\top} W h)}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(a^{\\top} v + b^{\\top} h' + v^{\\top} W h')}.\n$$\n项 $\\exp(a^{\\top} v)$ 对于分母中的求和变量 $h'$ 是常数，因此可以被因子分解出来并与分子中的项相抵消。这表明条件分布 $p(h \\mid v)$ 独立于可见偏置向量 $a$，正如问题陈述中所指出的。表达式简化为：\n$$\np(h \\mid v) = \\frac{\\exp(b^{\\top} h + v^{\\top} W h)}{\\sum_{h' \\in \\{0,1\\}^n} \\exp(b^{\\top} h' + v^{\\top} W h')}.\n$$\n指数可以重写为对隐藏单元的求和：$b^{\\top} h + v^{\\top} W h = \\sum_{j=1}^{n} (b_j h_j + (v^{\\top} W)_{j} h_j) = \\sum_{j=1}^{n} (b_j + v^{\\top} W_{:,j}) h_j$。由于RBM中隐藏单元之间没有连接，联合条件概率可以分解为各个条件概率的乘积：\n$$\np(h \\mid v) = \\prod_{j=1}^{n} p(h_j \\mid v).\n$$\n这种条件独立性是RBM的一个关键属性。对于单个二元隐藏单元 $h_j \\in \\{0,1\\}$，其条件概率为：\n$$\np(h_j=1 \\mid v) = \\frac{\\exp(b_j + v^{\\top} W_{:,j})}{1 + \\exp(b_j + v^{\\top} W_{:,j})} = \\frac{1}{1 + \\exp(-(b_j + v^{\\top} W_{:,j}))} = \\sigma(b_j + v^{\\top} W_{:,j}),\n$$\n其中 $\\sigma(x) = 1/(1+e^{-x})$ 是逻辑S型函数。\n一个二元变量的期望是它为1的概率。因此，第 $j$ 个隐藏单元的期望是：\n$$\n\\mathbb{E}[h_j \\mid v] = 1 \\cdot p(h_j=1 \\mid v) + 0 \\cdot p(h_j=0 \\mid v) = p(h_j=1 \\mid v).\n$$\n以向量形式，给定可见向量 $v$ 时，隐藏向量 $h$ 的条件期望是：\n$$\n\\mathbb{E}[h \\mid v] = \\sigma(b + W^{\\top}v),\n$$\n其中S型函数 $\\sigma$ 逐元素地应用于向量参数 $b + W^{\\top}v$。\n\n有了这个结果，我们可以概述计算过程：\n\n1.  **计算期望激活**：对于数据集 $\\{v^{(i)}\\}_{i=1}^{N}$ 中的每个可见向量 $v^{(i)}$，计算期望隐藏激活向量 $h^{(i)*} = \\mathbb{E}[h \\mid v^{(i)}]$。这 $N$ 个向量，每个大小为 $n \\times 1$，然后用于形成矩阵 $H \\in \\mathbb{R}^{N \\times n}$ 的行，其中第 $i$ 行为 $(h^{(i)*})^{\\top}$。\n\n2.  **计算样本协方差矩阵 $C$**：首先，计算期望隐藏激活的样本均值，它是一个 $n$ 维列向量 $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} h^{(i)*}$。然后，使用提供的公式计算 $n \\times n$ 样本协方差矩阵 $C$，分母为 $N-1$ 以获得无偏估计：\n$$\nC = \\frac{1}{N-1} \\sum_{i=1}^{N} (h^{(i)*} - \\mu)(h^{(i)*} - \\mu)^{\\top}.\n$$\n\n3.  **计算独立性偏差 $\\mathrm{ID}(C)$**：使用计算出的矩阵 $C$，我们计算独立性偏差度量。这涉及将 $C$ 的平方元素之和划分为对角和非对角分量：\n$$\n\\mathrm{ID}(C) = \\frac{\\sum_{i \\neq j} C_{ij}^{2}}{\\sum_{k=1}^{n} C_{kk}^{2} + \\epsilon}.\n$$\n分子是总非对角协方差的度量，而分母是总方差的度量。一个较小的值表示期望的隐藏特征在整个数据集中近似不相关。常数 $\\epsilon = 10^{-12}$ 确保了在所有方差都为零的情况下数值的稳定性。\n\n4.  **计算正交性惩罚 $\\mathrm{OP}(W)$**：此度量量化了与隐藏单元相关的权重向量的非正交性。首先，我们计算格拉姆矩阵 $G = W^{\\top} W$。然后，类似于 $\\mathrm{ID}(C)$，我们计算 $\\mathrm{OP}(W)$：\n$$\n\\mathrm{OP}(W) = \\frac{\\sum_{i \\neq j} G_{ij}^{2}}{\\sum_{k=1}^{n} G_{kk}^{2} + \\epsilon}.\n$$\n项 $G_{ij} = (W_{:,i})^{\\top}W_{:,j}$ 是隐藏单元 $i$ 和 $j$ 的权重向量的点积。$\\mathrm{OP}(W)$ 的一个较小值表示这些权重向量近似正交。\n\n对所提供的三个测试用例中的每一个都实施了这些步骤。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the independence deviation and orthogonality penalty for given RBM parameters.\n    \"\"\"\n\n    def sigmoid(x):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def compute_metrics(W, b, V, epsilon):\n        \"\"\"\n        Computes ID(C) and OP(W) for a single RBM configuration.\n\n        Args:\n            W (np.ndarray): Weight matrix of size (m, n).\n            b (np.ndarray): Hidden bias vector of size (n, 1).\n            V (np.ndarray): Dataset of visible vectors of size (N, m).\n            epsilon (float): Small constant for numerical stability.\n\n        Returns:\n            tuple: A tuple containing (id_c, op_w).\n        \"\"\"\n        N, m = V.shape\n        n = W.shape[1]\n\n        # 1. Compute H, the matrix of expected hidden activations\n        H = np.zeros((N, n))\n        for i in range(N):\n            v_i = V[i, :].reshape(-1, 1)  # Shape (m, 1)\n            # Argument to sigmoid: b (n,1) + W.T (n,m) @ v_i (m,1) -> (n,1)\n            h_exp = sigmoid(b + W.T @ v_i)\n            H[i, :] = h_exp.T  # Store as a row vector (1, n)\n\n        # 2. Compute the sample covariance matrix C\n        if N == 1:\n            C = np.zeros((n, n))\n        else:\n            # np.cov with rowvar=False computes covariance of columns.\n            # ddof=1 uses N-1 in the denominator for an unbiased estimate.\n            C = np.cov(H, rowvar=False, ddof=1)\n            # Manually:\n            # mu = np.mean(H, axis=0)\n            # H_centered = H - mu.reshape(1, -1)\n            # C = (H_centered.T @ H_centered) / (N - 1)\n\n        # 3. Compute ID(C)\n        diag_C_sq_sum = np.sum(np.diag(C)**2)\n        total_C_sq_sum = np.sum(C**2)\n        off_diag_C_sq_sum = total_C_sq_sum - diag_C_sq_sum\n        id_c = off_diag_C_sq_sum / (diag_C_sq_sum + epsilon)\n\n        # 4. Compute G and OP(W)\n        G = W.T @ W\n        diag_G_sq_sum = np.sum(np.diag(G)**2)\n        total_G_sq_sum = np.sum(G**2)\n        off_diag_G_sq_sum = total_G_sq_sum - diag_G_sq_sum\n        op_w = off_diag_G_sq_sum / (diag_G_sq_sum + epsilon)\n\n        return id_c, op_w\n\n    epsilon = 1e-12\n    m, n = 4, 3\n    V_data = np.array([\n        [0., 0., 0., 0.], [1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.],\n        [1., 1., 0., 0.], [1., 0., 1., 0.], [0., 1., 1., 0.], [1., 1., 1., 0.]\n    ])\n    b_common = np.zeros((n, 1))\n\n    test_cases = [\n        # Test case 1 (general \"happy path\"): Orthogonal weights\n        (\n            np.array([[5., 0., 0.], [0., 5., 0.], [0., 0., 5.], [0., 0., 0.]]),\n            b_common,\n            V_data\n        ),\n        # Test case 2 (correlated hidden features): Correlated weights\n        (\n            np.array([[5., 5., 0.], [5., 5., 0.], [0., 0., 5.], [0., 0., 0.]]),\n            b_common,\n            V_data\n        ),\n        # Test case 3 (boundary case): Zero weights\n        (\n            np.zeros((m, n)),\n            b_common,\n            V_data\n        ),\n    ]\n\n    results = []\n    for W, b, V in test_cases:\n        id_c, op_w = compute_metrics(W, b, V, epsilon)\n        results.append(id_c)\n        results.append(op_w)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3170455"}, {"introduction": "RBM 通过能量函数定义了一个概率分布，其自由能 (free energy) 决定了数据点所处的能量“景观”。一个鲁棒的模型应该为真实数据分配较低的自由能，并且其能量景观对输入的微小扰动不敏感。本练习将带你探索 RBM 的对抗鲁棒性，通过构造旨在最大化自由能增加的对抗样本，来检验模型的能量景观，并测试一种改进的训练方法能否提升模型的鲁棒性。[@problem_id:3170453]", "problem": "考虑一个具有伯努利可见单元和伯努利隐藏单元的受限玻尔兹曼机 (RBM)。RBM 通过一个能量函数定义了可见向量 $v \\in [0,1]^n$ 和隐藏向量 $h \\in \\{0,1\\}^m$ 上的联合分布。使用以下公认的基础作为出发点。一个基于能量的模型将联合概率定义为 $P(v,h) \\propto \\exp(-E(v,h))$。对于一个伯努利-伯努利 RBM，其能量函数由 $E(v,h) = -a^{\\top} v - b^{\\top} h - v^{\\top} W h$ 给出，其中 $W \\in \\mathbb{R}^{n \\times m}$ 是权重矩阵，$a \\in \\mathbb{R}^{n}$ 是可见偏置向量，$b \\in \\mathbb{R}^{m}$ 是隐藏偏置向量。可见向量的自由能通过对隐藏单元进行边缘化来定义：$F(v) = -\\log \\sum_{h} \\exp(-E(v,h))$。在此背景下，对抗性鲁棒性指的是增加受扰动输入自由能的难度：在给定一个小的扰动预算下，如果最优扰动 $\\delta$ 导致的 $F(v + \\delta)$ 增量较小，则模型更鲁棒。\n\n您的任务是实现并评估针对增加自由能的扰动 $\\delta$ 的对抗性鲁棒性，并测试与标准训练相比，在负相位中使用带温度的采样是否能增加鲁棒性。请从第一性原理出发解决以下步骤。\n\n1. 从给定的能量函数 $E(v,h)$ 和自由能 $F(v)$ 的定义出发，推导伯努利-伯努利 RBM 的 $F(v)$ 关于 $W$、$a$ 和 $b$ 的显式形式，并推导其梯度 $\\nabla_v F(v)$。然后，从约束 $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ 和 $0 \\le v + \\delta \\le 1$ 出发，推导一个近似最大化 $F(v + \\delta)$ 的一阶对抗性扰动规则。\n\n2. 在一个包含 $N$ 个 $[0,1]^n$ 样本的合成数据集上，为 RBM 实现两种训练程序：\n   - 标准的对比散度，在负相位中使用一步吉布斯采样，温度 $\\tau = 1$。\n   - 在温度 $\\tau > 1$ 下进行带温度的负相位采样。在基于温度的采样中，玻尔兹曼分布被修改为 $P(v,h) \\propto \\exp(-E(v,h)/\\tau)$。对于吉布斯条件概率，在负相位期间使用 $p(h_j = 1 \\mid v) = \\sigma((b_j + W_{\\cdot j}^{\\top} v)/\\tau)$ 和 $p(v_i = 1 \\mid h) = \\sigma((a_i + W_{i \\cdot}^{\\top} h)/\\tau)$，同时保持正相位的温度 $\\tau = 1$。这里 $\\sigma(x)$ 表示逻辑 sigmoid 函数。\n\n3. 对每个训练好的 RBM，实现一种快速对抗性样本生成方法，该方法为每个数据点 $v$ 计算一个扰动 $\\delta$，目标是在 $\\ell_{\\infty}$ 约束 $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ 和框约束 $0 \\le v + \\delta \\le 1$ 下最大程度地增加 $F(v + \\delta)$。使用自由能关于 $v$ 的梯度来生成一个显式的扰动，并将对抗性样本裁剪回有效范围内。\n\n4. 将鲁棒性度量定义为在该对抗性扰动下数据集上的平均自由能增量：计算 $\\Delta F_{\\text{mean}} = \\frac{1}{N} \\sum_{k=1}^{N} \\left[ F(v_k^{\\text{adv}}) - F(v_k) \\right]$，其中 $v_k^{\\text{adv}}$ 是 $v_k$ 的对抗性扰动版本。如果这个平均增量较小，则认为模型更鲁棒。\n\n5. 评估对于下面指定的测试套件，经过温度训练的 RBM 是否比标准训练的 RBM 更鲁棒。对于每个测试用例，生成一个布尔值，指示温度训练产生的 $\\Delta F_{\\text{mean}}$ 是否小于或等于标准训练的 $\\Delta F_{\\text{mean}}$。\n\n使用以下参数值测试套件，该套件应探测正常情况、边界条件和较大扰动的边缘情况：\n- 情况 1：$\\epsilon = 0.0$，$\\tau = 1.5$。\n- 情况 2：$\\epsilon = 0.05$，$\\tau = 1.5$。\n- 情况 3：$\\epsilon = 0.2$，$\\tau = 2.0$。\n\n您的程序必须：\n- 构建一个小的合成数据集，位于 $[0,1]^n$ 中，其中 $n = 6$，$N = 64$，包含围绕两个不同原型模式的两个簇，并添加少量噪声并裁剪到 $[0,1]$ 范围内。\n- 训练两个具有 $m = 4$ 个隐藏单元的 RBM：一个采用标准训练（负相位 $\\tau = 1$），另一个采用带温度的负相位采样，使用测试用例的 $\\tau$。\n- 对于每个测试用例，计算步骤 5 中描述的布尔结果。\n\n您的程序应产生单行输出，其中包含三个测试用例的三个布尔结果，格式为逗号分隔的列表，并用方括号括起来，例如 $[{\\text{True}},{\\text{False}},{\\text{True}}]$。此问题不涉及物理单位。不使用角度。不使用百分比；所有量均为无量纲实数，所有要求的输出均为布尔值。程序必须是自包含的，并且不需要用户输入。", "solution": "该问题要求对伯努利-伯努利受限玻尔兹曼机 (RBM) 的自由能及其梯度进行推导，然后通过实现来比较两种训练方法的对抗性鲁棒性：标准对比散度 (CD) 和带温度的负相位采样的 CD。\n\n### 第 1 部分：解析推导\n\n我们从为 RBM 提供的基础定义开始。该模型由可见单元 $v \\in [0,1]^n$ 和隐藏单元 $h \\in \\{0,1\\}^m$ 组成。联合概率分布由一个能量函数给出：$P(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$，其中 $Z$ 是配分函数。能量函数定义为：\n$$E(v,h) = -a^{\\top} v - b^{\\top} h - v^{\\top} W h = -\\sum_{i=1}^n a_i v_i - \\sum_{j=1}^m b_j h_j - \\sum_{i=1}^n \\sum_{j=1}^m v_i W_{ij} h_j$$\n其中 $W \\in \\mathbb{R}^{n \\times m}$ 是权重矩阵，$a \\in \\mathbb{R}^{n}$ 是可见偏置向量，$b \\in \\mathbb{R}^{m}$ 是隐藏偏置向量。\n\n#### 1.1. 自由能 $F(v)$\n\n自由能 $F(v)$ 是通过对隐藏单元上的联合分布进行边缘化来定义的：\n$$F(v) = -\\log \\sum_{h} \\exp(-E(v,h))$$\n代入 $E(v,h)$ 的表达式：\n$$F(v) = -\\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(a^{\\top} v + b^{\\top} h + v^{\\top} W h\\right)$$\n我们可以分离出不依赖于 $h$ 的项：\n$$F(v) = -\\log \\left( \\exp(a^{\\top} v) \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(b^{\\top} h + v^{\\top} W h\\right) \\right)$$\n$$F(v) = -a^{\\top} v - \\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(\\sum_{j=1}^m b_j h_j + \\sum_{j=1}^m \\sum_{i=1}^n v_i W_{ij} h_j\\right)$$\n设 $W_{\\cdot j}$ 是 $W$ 的第 $j$ 列。$v^{\\top} W h$ 项可以重写为 $\\sum_{j=1}^m h_j (v^{\\top} W_{\\cdot j})$。\n$$F(v) = -a^{\\top} v - \\log \\sum_{h \\in \\{0,1\\}^m} \\exp\\left(\\sum_{j=1}^m h_j (b_j + v^{\\top} W_{\\cdot j})\\right)$$\n由于隐藏单元 $h_j$ 在求和中是二元的且独立的，我们可以将对 $h$ 的所有配置的求和分解为对单个隐藏单元的乘积：\n$$\\sum_{h \\in \\{0,1\\}^m} \\prod_{j=1}^m \\exp\\left(h_j (b_j + v^{\\top} W_{\\cdot j})\\right) = \\prod_{j=1}^m \\sum_{h_j \\in \\{0,1\\}} \\exp\\left(h_j (b_j + v^{\\top} W_{\\cdot j})\\right)$$\n对于每个 $j$，求和是针对 $h_j=0$ 和 $h_j=1$：\n$$\\sum_{h_j \\in \\{0,1\\}} \\exp\\left(h_j (b_j + v^{\\top} W_{\\cdot j})\\right) = \\exp(0) + \\exp(b_j + v^{\\top} W_{\\cdot j}) = 1 + \\exp(b_j + v^{\\top} W_{\\cdot j})$$\n将此代回 $F(v)$ 的表达式中：\n$$F(v) = -a^{\\top} v - \\log \\left(\\prod_{j=1}^m (1 + \\exp(b_j + v^{\\top} W_{\\cdot j}))\\right)$$\n$$F(v) = -a^{\\top} v - \\sum_{j=1}^m \\log(1 + \\exp(b_j + v^{\\top} W_{\\cdot j}))$$\n这就是自由能的显式形式。项 $\\log(1 + \\exp(x))$ 被称为 softplus 函数。\n\n#### 1.2. 自由能的梯度 $\\nabla_v F(v)$\n\n为了找到对抗性扰动，我们首先需要自由能相对于可见向量 $v$ 的梯度。我们对 $F(v)$ 关于 $v$ 的一个分量 $v_k$ 求导：\n$$\\frac{\\partial F(v)}{\\partial v_k} = \\frac{\\partial}{\\partial v_k} \\left( -\\sum_{i=1}^n a_i v_i - \\sum_{j=1}^m \\log(1 + \\exp(b_j + \\sum_{i=1}^n v_i W_{ij})) \\right)$$\n$$\\frac{\\partial F(v)}{\\partial v_k} = -a_k - \\sum_{j=1}^m \\frac{\\partial}{\\partial v_k} \\log(1 + \\exp(b_j + v^{\\top} W_{\\cdot j}))$$\n使用链式法则，并回想 $\\frac{d}{dx} \\log(1+e^x) = \\frac{e^x}{1+e^x} = \\sigma(x)$，其中 $\\sigma(x)$ 是逻辑 sigmoid 函数：\n$$\\frac{\\partial}{\\partial v_k} \\log(1 + \\exp(b_j + v^{\\top} W_{\\cdot j})) = \\sigma(b_j + v^{\\top} W_{\\cdot j}) \\cdot \\frac{\\partial}{\\partial v_k}(b_j + \\sum_{i=1}^n v_i W_{ij}) = \\sigma(b_j + v^{\\top} W_{\\cdot j}) \\cdot W_{kj}$$\n将此代入梯度表达式中：\n$$\\frac{\\partial F(v)}{\\partial v_k} = -a_k - \\sum_{j=1}^m W_{kj} \\sigma(b_j + v^{\\top} W_{\\cdot j})$$\n注意 $\\sigma(b_j + v^{\\top} W_{\\cdot j})$ 是条件概率 $p(h_j=1|v)$。用向量表示，梯度为：\n$$\\nabla_v F(v) = -a - W \\cdot \\sigma(b + W^{\\top} v)$$\n\n#### 1.3. 一阶对抗性扰动\n\n目标是找到一个扰动 $\\delta$ 来最大化 $F(v+\\delta)$，同时满足约束 $\\lVert \\delta \\rVert_{\\infty} \\le \\epsilon$ 和 $0 \\le v + \\delta \\le 1$。$F(v+\\delta)$ 在 $v$ 附近的一阶泰勒展开为：\n$$F(v+\\delta) \\approx F(v) + \\delta^{\\top} \\nabla_v F(v)$$\n为了最大化这个线性近似，$\\delta$ 应与梯度 $\\nabla_v F(v)$ 对齐。在 $\\ell_{\\infty}$ 范数约束下，最优扰动为：\n$$\\delta = \\epsilon \\cdot \\text{sign}(\\nabla_v F(v))$$\n这是快速梯度符号法 (FGSM) 的原理。得到的对抗性候选样本是 $v' = v + \\delta$。为了满足框约束 $0 \\le v + \\delta \\le 1$，我们必须对结果进行裁剪：\n$$v^{\\text{adv}} = \\text{clip}(v + \\epsilon \\cdot \\text{sign}(\\nabla_v F(v)), 0, 1)$$\n\n### 第 2 部分：算法设计\n\n#### 2.1. RBM 训练 (对比散度)\n\nRBM 参数 ($W, a, b$) 使用对比散度 (CD-1) 进行训练，这是对数似然梯度的一种近似。更新规则从此梯度推导而来：\n$$\\Delta W \\propto v_0 p(h|v_0)^{\\top} - v_1 p(h|v_1)^{\\top}$$\n$$\\Delta a \\propto v_0 - v_1$$\n$$\\Delta b \\propto p(h|v_0) - p(h|v_1)$$\n其中 $v_0$ 是一个数据样本，$v_1$ 是一步重构。过程如下：\n1.  **正相位**：对于一个数据向量 $v_0$，计算隐藏单元的激活概率 $p(h|v_0) = \\sigma(v_0 W + b)$。\n2.  **负相位**：从隐藏激活中生成一个重构 $v_1$。对于连续输入，我们使用概率：$p(v|h) = \\sigma(h W^{\\top} + a)$。然后我们计算新的隐藏概率 $p(h|v_1) = \\sigma(v_1 W + b)$。\n\n对于这个问题，实现了两种训练变体：\n-   **标准训练**：所有阶段都使用温度 $\\tau=1$。条件概率与上述完全相同。\n-   **温度训练**：正相位保持在 $\\tau=1$。负相位使用温度 $\\tau > 1$。条件概率通过将其参数除以 $\\tau$ 来修改：\n    $$p_{\\tau}(v=1|h) = \\sigma((h W^{\\top} + a)/\\tau)$$\n    $$p_{\\tau}(h=1|v) = \\sigma((v W + b)/\\tau)$$\n这种温度调节平滑了负相位的分布，这可能会影响学习动态和模型鲁棒性。为减少方差，我们直接使用激活概率而不是从中采样二元状态。\n\n#### 2.2. 鲁棒性评估\n\n对于每个训练好的 RBM，我们使用指定的度量来量化其对抗性鲁棒性。\n1.  对于数据集中的每个数据点 $v_k$，我们使用在第 1.3 部分推导的基于 FGSM 的规则，在给定的扰动预算 $\\epsilon$ 下生成一个对抗性对应物 $v_k^{\\text{adv}}$。\n2.  我们使用在第 1.1 部分推导的公式计算原始点 $F(v_k)$ 和对抗性点 $F(v_k^{\\text{adv}})$ 的自由能。\n3.  鲁棒性度量是整个数据集上自由能的平均增量：\n    $$\\Delta F_{\\text{mean}} = \\frac{1}{N} \\sum_{k=1}^{N} \\left[ F(v_k^{\\text{adv}}) - F(v_k) \\right]$$\n一个较小的 $\\Delta F_{\\text{mean}}$ 表示模型更鲁棒，因为对抗性攻击在增加自由能方面的效果较差。\n\n### 第 3 部分：实验方案\n\n该比较在一个由两个噪声簇组成的合成数据集 ($N=64, n=6$) 上进行。对于由 $(\\epsilon, \\tau)$ 定义的每个测试用例：\n1.  两个 RBM ($m=4$) 使用相同的随机权重和偏置进行初始化，以确保公平比较。\n2.  一个 RBM 使用标准 CD-1 ($\\tau_{\\text{neg}}=1$) 进行训练，另一个使用带温度的负相位 CD-1 ($\\tau_{\\text{neg}}=\\tau$) 进行训练。\n3.  对于两个训练好的模型，使用测试用例的 $\\epsilon$ 计算 $\\Delta F_{\\text{mean}}$。\n4.  最终结果是一个布尔值，指示温度模型是否更鲁棒或同样鲁棒，即 $\\Delta F_{\\text{mean, tempered}} \\le \\Delta F_{\\text{mean, standard}}$ 是否成立。\n\n$\\epsilon=0.0$ 的测试用例作为一个健全性检查。由于扰动为零，两个模型的 $\\Delta F_{\\text{mean}}$ 都将为 $0$，比较 $0 \\le 0$ 将正确地得出 `True`。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _sigmoid(x):\n    \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-x))\n\nclass RBM:\n    \"\"\"A Bernoulli-Bernoulli Restricted Boltzmann Machine.\"\"\"\n    \n    def __init__(self, n_vis, n_hid, seed=None):\n        \"\"\"\n        Initializes the RBM.\n        \n        Args:\n            n_vis (int): Number of visible units.\n            n_hid (int): Number of hidden units.\n            seed (int, optional): Seed for the random number generator.\n        \"\"\"\n        self.rng = np.random.default_rng(seed)\n        self.n_vis = n_vis\n        self.n_hid = n_hid\n        \n        # Initialize weights with small random values and biases to zero.\n        scale = 0.01\n        self.W = self.rng.normal(loc=0.0, scale=scale, size=(n_vis, n_hid))\n        self.a = np.zeros(n_vis) # visible biases\n        self.b = np.zeros(n_hid) # hidden biases\n        \n    def copy_params_from(self, other_rbm):\n        \"\"\"Copies parameters from another RBM instance for fair comparison.\"\"\"\n        self.W = np.copy(other_rbm.W)\n        self.a = np.copy(other_rbm.a)\n        self.b = np.copy(other_rbm.b)\n        self.rng = other_rbm.rng # ensure samplers are also identical if used\n\n    def free_energy(self, v):\n        \"\"\"\n        Computes the free energy for a batch of visible vectors.\n        \n        Args:\n            v (np.ndarray): A batch of visible vectors, shape (N, n_vis).\n        \n        Returns:\n            np.ndarray: An array of free energy values, shape (N,).\n        \"\"\"\n        if v.ndim == 1:\n            v = v.reshape(1, -1)\n        \n        va_term = v @ self.a\n        vWb = (v @ self.W) + self.b\n        # np.logaddexp(0, x) is a numerically stable way to compute log(1 + exp(x))\n        hidden_term = np.sum(np.logaddexp(0, vWb), axis=1)\n        \n        return -va_term - hidden_term\n\n    def grad_free_energy(self, v):\n        \"\"\"\n        Computes the gradient of the free energy with respect to v.\n        \n        Args:\n            v (np.ndarray): A batch of visible vectors, shape (N, n_vis).\n        \n        Returns:\n            np.ndarray: The gradient of F(v) w.r.t v, shape (N, n_vis).\n        \"\"\"\n        if v.ndim == 1:\n            v = v.reshape(1, -1)\n        \n        hidden_probs = _sigmoid((v @ self.W) + self.b)\n        # Gradient is -a - W * sigma(b + W^T * v)\n        # The -self.a term is broadcasted to match the batch size.\n        grad = -self.a - (hidden_probs @ self.W.T)\n        \n        return grad\n\n    def train(self, data, epochs, learning_rate, temp_neg_phase=1.0):\n        \"\"\"\n        Trains the RBM using Contrastive Divergence (CD-1).\n        \n        Args:\n            data (np.ndarray): The training data, shape (N, n_vis).\n            epochs (int): Number of training epochs.\n            learning_rate (float): The learning rate for parameter updates.\n            temp_neg_phase (float): Temperature for negative phase sampling.\n        \"\"\"\n        n_samples = data.shape[0]\n        v0 = data\n        \n        for _ in range(epochs):\n            # Positive phase (tau=1)\n            pos_hidden_probs = _sigmoid((v0 @ self.W) + self.b)\n            \n            # Negative phase (reconstruction)\n            # Use probabilities for reconstruction (mean-field)\n            neg_vis_probs = _sigmoid(((pos_hidden_probs @ self.W.T) + self.a) / temp_neg_phase)\n            neg_hidden_probs = _sigmoid(((neg_vis_probs @ self.W) + self.b) / temp_neg_phase)\n\n            # Update parameters\n            dW = (v0.T @ pos_hidden_probs) - (neg_vis_probs.T @ neg_hidden_probs)\n            da = np.sum(v0 - neg_vis_probs, axis=0)\n            db = np.sum(pos_hidden_probs - neg_hidden_probs, axis=0)\n            \n            self.W += learning_rate * dW / n_samples\n            self.a += learning_rate * da / n_samples\n            self.b += learning_rate * db / n_samples\n\ndef generate_dataset(N, n, seed=None):\n    \"\"\"Generates a synthetic dataset with two clusters.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    p1 = np.array([1, 1, 1, 0, 0, 0])\n    p2 = np.array([0, 0, 0, 1, 1, 1])\n    \n    data = np.zeros((N, n))\n    \n    n_half = N // 2\n    data[:n_half, :] = p1 + rng.normal(loc=0.0, scale=0.1, size=(n_half, n))\n    data[n_half:, :] = p2 + rng.normal(loc=0.0, scale=0.1, size=(N - n_half, n))\n    \n    return np.clip(data, 0, 1)\n\ndef generate_adversarial(rbm, v_data, epsilon):\n    \"\"\"Generates adversarial examples using FGSM on the free energy.\"\"\"\n    if epsilon == 0.0:\n        return np.copy(v_data)\n        \n    grad = rbm.grad_free_energy(v_data)\n    delta = epsilon * np.sign(grad)\n    v_adv = np.clip(v_data + delta, 0, 1)\n    \n    return v_adv\n\ndef evaluate_robustness(rbm, data, epsilon):\n    \"\"\"Computes the mean increase in free energy under adversarial attack.\"\"\"\n    if epsilon == 0.0:\n        return 0.0\n        \n    data_adv = generate_adversarial(rbm, data, epsilon)\n    \n    F_orig = rbm.free_energy(data)\n    F_adv = rbm.free_energy(data_adv)\n    \n    delta_F = F_adv - F_orig\n    \n    return np.mean(delta_F)\n\ndef solve():\n    # Fixed parameters for the experiment\n    N = 64\n    n_vis = 6\n    n_hid = 4\n    epochs = 1000\n    learning_rate = 0.1\n    master_seed = 42\n\n    test_cases = [\n        (0.0, 1.5),\n        (0.05, 1.5),\n        (0.2, 2.0),\n    ]\n\n    results = []\n    \n    # Generate a single dataset for all test cases\n    dataset = generate_dataset(N, n_vis, seed=master_seed)\n    \n    for i, (epsilon, tau) in enumerate(test_cases):\n        # We must re-initialize and retrain models for each case to ensure\n        # that the comparison is based on the specific tau.\n        # Use a consistent seed for RBM initialization across runs for reproducibility.\n        rbm_init_seed = master_seed + 1 + i\n\n        # Standard RBM (train with tau=1)\n        rbm_std = RBM(n_vis, n_hid, seed=rbm_init_seed)\n        rbm_std.train(dataset, epochs, learning_rate, temp_neg_phase=1.0)\n        \n        # Tempered RBM (train with given tau)\n        # Initialize with the same weights for a fair comparison.\n        rbm_temp = RBM(n_vis, n_hid, seed=rbm_init_seed) # seed ensures identical init\n        rbm_temp.train(dataset, epochs, learning_rate, temp_neg_phase=tau)\n        \n        # Evaluate robustness\n        delta_F_std = evaluate_robustness(rbm_std, dataset, epsilon)\n        delta_F_temp = evaluate_robustness(rbm_temp, dataset, epsilon)\n        \n        # Compare and store result\n        is_more_robust = delta_F_temp = delta_F_std\n        results.append(is_more_robust)\n\n    # Format the final output string\n    # str(True) -> 'True', so no special capitalization logic is needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3170453"}]}