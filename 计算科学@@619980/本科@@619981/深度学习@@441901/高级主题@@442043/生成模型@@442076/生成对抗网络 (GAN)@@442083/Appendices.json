{"hands_on_practices": [{"introduction": "模式坍塌（Mode Collapse）是训练生成对抗网络（GAN）时最臭名昭著的挑战之一，即生成器只能产生有限种类的样本，无法捕捉真实数据的多样性。本练习通过一个简化的线性生成器模型，提供了一个从数学上理解模式坍塌的窗口。你将探索模式坍塌与生成器雅可比矩阵的“低秩”特性之间的深刻联系，并通过推导不同正则化项下的最优解，亲手揭示为何鼓励“体积”或“满秩”的策略能够有效对抗模式坍塌。[@problem_id:3124524]", "problem": "考虑统计学习中生成对抗网络（GAN, Generative Adversarial Networks）中生成器的一个简化局部模型。设生成器在局部被建模为一个线性映射 $G(\\mathbf{z}) = A \\mathbf{z}$，其中 $\\mathbf{z} \\in \\mathbb{R}^{d}$ 是一个潜向量，服从分布 $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, I_d)$，而 $A \\in \\mathbb{R}^{d \\times d}$ 是一个参数矩阵。生成器在任意 $\\mathbf{z}$ 处的雅可比矩阵为 $J_G(\\mathbf{z}) = A$。定义核范数 $\\lVert A \\rVert_* = \\sum_{i=1}^{d} s_i$，其中 $s_i$ 是 $A$ 的奇异值；并通过对称半正定矩阵 $A A^\\top$ 定义局部体积元，其行列式为 $\\det(A A^\\top) = \\prod_{i=1}^{d} s_i^2$。您将探究一个假设，即模式崩溃对应于 $J_G(\\mathbf{z})$ 的低秩行为（即许多 $s_i$ 等于 $0$），而增加局部体积或惩罚秩亏的正则化项可以防止崩溃。\n\n仅使用以下基本依据：\n- 对于零均值单位协方差的高斯分布的期望恒等式：$\\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, I_d)}[\\mathbf{z}\\mathbf{z}^\\top] = I_d$。\n- 弗罗贝尼乌斯范数和核范数的酉不变性：将 $A$ 的奇异值分解（SVD, Singular Value Decomposition）写作 $A = U \\operatorname{diag}(s_1,\\dots,s_d) V^\\top$，其中 $U, V$ 是正交矩阵，则有 $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{d} s_i^2$ 和 $\\lVert A \\rVert_* = \\sum_{i=1}^{d} s_i$。\n- 对于任意 $\\epsilon \\ge 0$，$\\log \\det(A A^\\top + \\epsilon I_d) = \\sum_{i=1}^{d} \\log(s_i^2 + \\epsilon)$。\n\n任务：\n1) 用奇异值 $s_i$ 表示无正则化的目标函数 $L_0(A) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2]$，并解释为什么其最小化解是 $A = 0$（完全崩溃）。\n2) 考虑核范数增强的目标函数 $L_{\\text{nuc}}(A;\\lambda) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] - \\lambda \\lVert A \\rVert_*$，其中正则化强度 $\\lambda \\ge 0$。仅使用上述基本依据，推导出最优奇异值 $s_i^\\star$ 作为 $\\lambda$ 的函数。据此判断最优解何时是满秩的（即有多少个 $s_i^\\star$ 是严格为正的）。\n3) 考虑体积增强的目标函数 $L_{\\text{vol}}(A;\\beta,\\epsilon) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] - \\beta \\log \\det(A A^\\top + \\epsilon I_d)$，其中系数 $\\beta \\ge 0$ 和 $\\epsilon \\ge 0$。推导出最优奇异值 $s_i^\\star$ 作为 $\\beta$ 和 $\\epsilon$ 的函数，并确定最优解何时是满秩的。\n\n在最优解 $A^\\star$ 处定义以下标量崩溃度量：有效秩分数 $r_{\\text{frac}} = \\frac{1}{d} \\cdot \\#\\{ i \\in \\{1,\\dots,d\\} \\,:\\, s_i^\\star  \\tau \\}$，其中 $\\tau$ 是一个小的非负容差，可取为 $\\tau = 10^{-12}$。$r_{\\text{frac}} = 0$ 表示完全崩溃，$r_{\\text{frac}} = 1$ 表示满秩。\n\n实现要求：\n- 编写一个程序，对于下面的每个测试用例，计算由给定目标函数及其参数所蕴含的最优解 $A^\\star$ 处的 $r_{\\text{frac}}$。程序应仅使用推导出的最优奇异值的闭式刻画和上述定义。不允许进行数值优化或随机模拟。\n- 不涉及角度。\n- 不涉及物理单位。\n\n测试套件（每个用例相互独立）：\n- 用例 $1$：$d=2$，核范数目标函数，$\\lambda=0$。\n- 用例 $2$：$d=2$，核范数目标函数，$\\lambda=0.6$。\n- 用例 $3$：$d=2$，体积目标函数，$\\beta=0.1, \\epsilon=0.1$。\n- 用例 $4$：$d=3$，体积目标函数，$\\beta=2.0, \\epsilon=0.1$。\n- 用例 $5$：$d=4$，体积目标函数，$\\beta=0.05, \\epsilon=0.2$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔小数列表（例如 $[0.0,1.0,0.0]$），结果按用例 $1$ 到用例 $5$ 的顺序排列。每个条目必须是该用例的有效秩分数 $r_{\\text{frac}}$，使用容差 $\\tau = 10^{-12}$ 计算。", "solution": "首先验证问题，以确保其是自洽的、有科学依据的且是良构的。\n\n### 步骤1：提取已知条件\n- 生成器模型: $G(\\mathbf{z}) = A \\mathbf{z}$\n- 潜向量: $\\mathbf{z} \\in \\mathbb{R}^{d}$，服从分布 $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, I_d)$\n- 参数矩阵: $A \\in \\mathbb{R}^{d \\times d}$\n- 生成器雅可比矩阵: $J_G(\\mathbf{z}) = A$\n- $A$的奇异值: $s_i$，其中 $i=1, \\dots, d$\n- 核范数: $\\lVert A \\rVert_* = \\sum_{i=1}^{d} s_i$\n- $A A^\\top$的行列式: $\\det(A A^\\top) = \\prod_{i=1}^{d} s_i^2$\n- 期望恒等式: $\\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, I_d)}[\\mathbf{z}\\mathbf{z}^\\top] = I_d$\n- 基于SVD的范数恒等式: $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{d} s_i^2$ and $\\lVert A \\rVert_* = \\sum_{i=1}^{d} s_i$\n- 对数行列式恒等式: $\\log \\det(A A^\\top + \\epsilon I_d) = \\sum_{i=1}^{d} \\log(s_i^2 + \\epsilon)$，其中 $\\epsilon \\ge 0$\n- 任务1 (无正则化目标函数): $L_0(A) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2]$\n- 任务2 (核范数目标函数): $L_{\\text{nuc}}(A;\\lambda) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] - \\lambda \\lVert A \\rVert_*$，其中 $\\lambda \\ge 0$\n- 任务3 (体积目标函数): $L_{\\text{vol}}(A;\\beta,\\epsilon) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] - \\beta \\log \\det(A A^\\top + \\epsilon I_d)$，其中 $\\beta \\ge 0, \\epsilon \\ge 0$\n- 崩溃度量: $r_{\\text{frac}} = \\frac{1}{d} \\cdot \\#\\{ i \\in \\{1,\\dots,d\\} \\,:\\, s_i^\\star  \\tau \\}$\n- 容差: $\\tau = 10^{-12}$\n\n### 步骤2：使用提取的已知条件进行验证\n该问题是对一个简化生成模型中正则化项的理论探索。所有定义、恒等式和目标函数都已明确给出。其数学框架是标准的线性代数、概率论和优化理论。该模型是研究 GAN 局部性质的有效简化。任务是为良定义的目标函数推导最优解。经检查发现，这些目标函数存在良定义的最小化解（将在解法中展示）。该问题没有违反任何科学合理性原则，表述规范、完整且是良构的。\n\n### 步骤3：结论与行动\n问题有效。将提供完整解法。\n\n### 解法推导\n\n首先，我们将共同的基础目标项 $\\mathbbE_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2]$ 用矩阵 $A$ 的奇异值 $s_i$ 来表示。\n生成向量 $G(\\mathbf{z}) = A\\mathbf{z}$ 的L2范数平方为：\n$$ \\lVert A\\mathbf{z} \\rVert_2^2 = (A\\mathbf{z})^\\top(A\\mathbf{z}) = \\mathbf{z}^\\top A^\\top A \\mathbf{z} $$\n该表达式是一个标量，它等于其自身的迹。利用迹的循环性质 $\\operatorname{tr}(XYZ) = \\operatorname{tr}(ZXY)$，我们可以写出：\n$$ \\mathbf{z}^\\top A^\\top A \\mathbf{z} = \\operatorname{tr}(\\mathbf{z}^\\top A^\\top A \\mathbf{z}) = \\operatorname{tr}(A^\\top A \\mathbf{z} \\mathbf{z}^\\top) $$\n对 $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, I_d)$ 取期望，并利用期望和迹算子的线性性质：\n$$ \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] = \\mathbb{E}_{\\mathbf{z}}[\\operatorname{tr}(A^\\top A \\mathbf{z} \\mathbf{z}^\\top)] = \\operatorname{tr}(\\mathbb{E}_{\\mathbf{z}}[A^\\top A \\mathbf{z} \\mathbf{z}^\\top]) = \\operatorname{tr}(A^\\top A \\mathbb{E}_{\\mathbf{z}}[\\mathbf{z} \\mathbf{z}^\\top]) $$\n使用给定的恒等式 $\\mathbb{E}_{\\mathbf{z}}[\\mathbf{z}\\mathbf{z}^\\top] = I_d$，上式简化为：\n$$ \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] = \\operatorname{tr}(A^\\top A I_d) = \\operatorname{tr}(A^\\top A) $$\n$A^\\top A$ 的迹是弗罗贝尼乌斯范数平方 $\\lVert A \\rVert_F^2$ 的定义。使用给定的关联弗罗贝尼乌斯范数与奇异值的恒等式：\n$$ \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] = \\lVert A \\rVert_F^2 = \\sum_{i=1}^{d} s_i^2 $$\n这个表达式构成了分析所有三个目标函数的基础。对矩阵 $A$ 的优化简化为对其奇异值 $s_i \\ge 0$ 的优化。\n\n**1) 无正则化目标函数**\n目标函数为 $L_0(A) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2]$。用奇异值表示为：\n$$ L_0(s_1, \\dots, s_d) = \\sum_{i=1}^{d} s_i^2 $$\n为最小化此目标函数，我们必须找到平方和的最小值。由于奇异值是非负的，$s_i \\ge 0$，因此 $s_i^2$ 项也是非负的。当每一项都取最小值时，总和达到最小。$s_i^2$ 的最小值为 $0$，此时 $s_i = 0$。\n因此，最优奇异值为 $s_i^\\star = 0$（对所有 $i \\in \\{1, \\dots, d\\}$ 成立）。所有奇异值都为零的矩阵是零矩阵，因此最小化解是 $A^\\star = \\mathbf{0}$。这代表了完全的模式崩溃，因为生成器只产生零向量。\n\n**2) 核范数增强目标函数**\n目标函数为 $L_{\\text{nuc}}(A;\\lambda) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] - \\lambda \\lVert A \\rVert_*$。使用推导出的表达式，我们可以将其用奇异值表示：\n$$ L_{\\text{nuc}}(s_1, \\dots, s_d) = \\left(\\sum_{i=1}^{d} s_i^2\\right) - \\lambda \\left(\\sum_{i=1}^{d} s_i\\right) = \\sum_{i=1}^{d} (s_i^2 - \\lambda s_i) $$\n该目标函数对于各个奇异值是可分的。我们可以通过最小化每个 $i$ 的单值目标函数 $f(s_i) = s_i^2 - \\lambda s_i$ 来找到最优的 $s_i^\\star$，并满足约束条件 $s_i \\ge 0$。\n为求最小值，我们对 $s_i$ 求导并令其为零：\n$$ \\frac{df}{ds_i} = 2s_i - \\lambda = 0 \\implies s_i = \\frac{\\lambda}{2} $$\n二阶导数为 $\\frac{d^2f}{ds_i^2} = 2  0$，确认这是一个最小值点。由于问题指定 $\\lambda \\ge 0$，无约束最小值 $s_i = \\lambda/2$ 是非负的，因此位于可行域 $s_i \\ge 0$ 内。\n对于所有 $i$，最优奇异值是相同的：\n$$ s_i^\\star = \\frac{\\lambda}{2} $$\n当且仅当其所有奇异值都严格为正时，得到的最优解 $A^\\star$ 是满秩的。这发生在 $s_i^\\star  0$ 时，即要求 $\\lambda/2  0$，或简写为 $\\lambda  0$。如果 $\\lambda = 0$，则所有 $s_i^\\star=0$，我们就回到了任务1中的崩溃解。\n\n**3) 体积增强目标函数**\n目标函数为 $L_{\\text{vol}}(A;\\beta,\\epsilon) = \\mathbb{E}_{\\mathbf{z}}[\\lVert G(\\mathbf{z}) \\rVert_2^2] - \\beta \\log \\det(A A^\\top + \\epsilon I_d)$。使用给定的恒等式，我们将其用奇异值表示：\n$$ L_{\\text{vol}}(s_1, \\dots, s_d) = \\left(\\sum_{i=1}^{d} s_i^2\\right) - \\beta \\left(\\sum_{i=1}^{d} \\log(s_i^2 + \\epsilon)\\right) = \\sum_{i=1}^{d} \\left(s_i^2 - \\beta \\log(s_i^2 + \\epsilon)\\right) $$\n该目标函数也是可分的。我们最小化单值目标函数 $g(s_i) = s_i^2 - \\beta \\log(s_i^2 + \\epsilon)$，其中 $s_i \\ge 0$。令 $x_i = s_i^2$，则 $x_i \\ge 0$。关于 $x_i$ 的目标函数为 $h(x_i) = x_i - \\beta \\log(x_i + \\epsilon)$。\n我们通过对 $x_i$ 求导并令其为零来找到最小值：\n$$ \\frac{dh}{dx_i} = 1 - \\frac{\\beta}{x_i + \\epsilon} = 0 \\implies x_i + \\epsilon = \\beta \\implies x_i = \\beta - \\epsilon $$\n二阶导数为 $\\frac{d^2h}{dx_i^2} = \\frac{\\beta}{(x_i + \\epsilon)^2}$。给定 $\\beta \\ge 0$，该值非负，因此函数 $h(x_i)$ 是凸函数，$x_i = \\beta - \\epsilon$ 是一个全局最小值点。\n我们必须遵守约束条件 $x_i \\ge 0$。因此最优值 $x_i^\\star$ 是：\n$$ x_i^\\star = \\max(0, \\beta - \\epsilon) $$\n代回 $x_i^\\star = (s_i^\\star)^2$：\n$$ s_i^\\star = \\sqrt{\\max(0, \\beta - \\epsilon)} $$\n对于所有 $i$，最优奇异值是相同的。当且仅当其奇异值严格为正时，最优解 $A^\\star$ 是满秩的。这发生在 $s_i^\\star  0$ 时，即要求 $\\sqrt{\\max(0, \\beta - \\epsilon)}  0$。这个条件简化为 $\\beta - \\epsilon  0$，即 $\\beta  \\epsilon$。如果 $\\beta \\le \\epsilon$，所有奇异值都为零，导致崩溃。\n\n这些推导出的最优奇异值 $s_i^\\star$ 的闭式表达式被用于实现中，以计算每个测试用例的有效秩分数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the effective-rank fraction for several regularized objectives\n    in a simplified GAN model.\n    \"\"\"\n    \n    # Define the tolerance for considering a singular value as non-zero.\n    tau = 1e-12\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'d': 2, 'type': 'nuclear', 'lambda': 0.0},\n        {'d': 2, 'type': 'nuclear', 'lambda': 0.6},\n        {'d': 2, 'type': 'volume', 'beta': 0.1, 'epsilon': 0.1},\n        {'d': 3, 'type': 'volume', 'beta': 2.0, 'epsilon': 0.1},\n        {'d': 4, 'type': 'volume', 'beta': 0.05, 'epsilon': 0.2},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = case['d']\n        s_star = 0.0\n\n        if case['type'] == 'nuclear':\n            # For the nuclear-norm objective, the optimal singular values are s_i* = lambda / 2.\n            # This also covers the unregularized case where lambda = 0.\n            lam = case['lambda']\n            s_star = lam / 2.0\n            \n        elif case['type'] == 'volume':\n            # For the volume-augmented objective, the optimal singular values are\n            # s_i* = sqrt(max(0, beta - epsilon)).\n            beta = case['beta']\n            epsilon = case['epsilon']\n            s_star = np.sqrt(max(0, beta - epsilon))\n\n        # In all derived solutions, all optimal singular values s_i* are identical.\n        # We only need to check if this single value is greater than the tolerance.\n        \n        # Determine the number of non-zero singular values.\n        # If s_star  tau, all 'd' singular values are non-zero. Otherwise, none are.\n        num_positive_s_values = d if s_star > tau else 0\n        \n        # Calculate the effective-rank fraction.\n        r_frac = num_positive_s_values / d\n        \n        results.append(r_frac)\n\n    # Final print statement in the exact required format.\n    # The output should be a list of floating-point numbers.\n    print(f\"[{','.join(map(str, [float(r) for r in results]))}]\")\n\nsolve()\n```", "id": "3124524"}, {"introduction": "在解决了模式坍塌的理论理解后，我们转向一个更具体的训练稳定性问题。Wasserstein GAN (WGAN) 通过对判别器（或称“评论家”）施加$1$-Lipschitz约束，极大地提升了训练的稳定性，但这在实践中如何实现呢？本练习将带你深入三种主流的Lipschitz约束强制方法：权重裁剪、谱归一化和梯度惩罚。通过定量计算和比较，你将对这些技术的内部工作原理及其对网络行为的影响，建立起具体而深入的认识。[@problem_id:3124549]", "problem": "考虑一个在 Wasserstein 生成对抗网络 (WGAN) 框架下训练的生成对抗网络 (GAN) 中的判别器 (也称为评判器) $D$，要求 $D$ 满足 $1$-Lipschitz 约束。该网络是一个带有整流线性单元 (ReLU) 激活函数的三层前馈映射，\n$$\nD(x) \\;=\\; W_{3}\\,\\sigma\\!\\left(W_{2}\\,\\sigma\\!\\left(W_{1}\\,x\\right)\\right),\n$$\n其中 $\\sigma$ 表示 ReLU 激活函数，权重矩阵的形状为 $W_{1} \\in \\mathbb{R}^{128 \\times 64}$，$W_{2} \\in \\mathbb{R}^{64 \\times 128}$ 和 $W_{3} \\in \\mathbb{R}^{1 \\times 64}$。仅使用以下来自分析学和线性代数的基本事实：\n- 对于赋范空间之间的一个函数 $f$，如果对所有的 $x,y$ 都有 $|f(x)-f(y)| \\leq L\\,\\|x-y\\|$ 成立，则称 $f$ 是 $L$-Lipschitz 的，而满足此条件的最小 $L$ 值是 $f$ 的 Lipschitz 常数。\n- 如果 $h = g \\circ f$ 是两个分别满足 $L_{f}$- 和 $L_{g}$-Lipschitz 的映射的复合，那么 $h$ 是 $L_{g}\\,L_{f}$-Lipschitz 的。\n- 一个由矩阵 $W$ 表示的关于欧几里得范数的线性映射，其 Lipschitz 常数等于算子范数 (最大奇异值) $\\|W\\|_{2}$。\n- 整流线性单元 (ReLU) 激活函数 $\\sigma(z) = \\max\\{0,z\\}$ 是 $1$-Lipschitz 的。\n- 对于任意矩阵 $W \\in \\mathbb{R}^{m \\times n}$，若其元素满足 $|W_{ij}| \\leq c$，则其算子范数遵循 $\\|W\\|_{2} \\leq \\|W\\|_{F} \\leq c\\,\\sqrt{mn}$，其中 $\\|W\\|_{F}$ 是 Frobenius 范数。\n\n你将分析三种用于强制实现 $1$-Lipschitz 约束的机制：\n\n(1) 权重裁剪 (Weight clipping): 每次更新后，将每个 $W_{i}$ 的所有元素裁剪到 $[-c,c]$ 区间内，其中 $c = 0.01$。仅使用给定的事实，计算在裁剪条件下 $D$ 的 Lipschitz 常数的一个上界。\n\n(2) 谱归一化 (Spectral normalization): 假设使用谱归一化，训练后的权重矩阵的算子范数 (最大奇异值) 测量结果为 $(\\|W_{1}\\|_{2}, \\|W_{2}\\|_{2}, \\|W_{3}\\|_{2}) = (0.98,\\,1.03,\\,0.99)$。仅使用给定的事实，计算在谱归一化条件下 $D$ 的 Lipschitz 常数的一个上界。\n\n(3) 梯度惩罚 (Gradient penalty): 在带梯度惩罚的 WGAN (WGAN-GP) 中，惩罚项促使在真实样本和生成样本之间的连线上 $\\|\\nabla_{x} D(x)\\|_{2} \\approx 1$。在一批插值点 $\\{x_{t}\\}$ 上，观测到以下梯度范数：$\\|\\nabla_{x} D(x_{t})\\|_{2} \\in \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\}$。仅使用给定的事实，通过该批次中观测到的最大梯度范数来近似 Lipschitz 常数。\n\n按权重裁剪、谱归一化、梯度惩罚的顺序计算这三个得到的近似 Lipschitz 常数。将所有数值结果四舍五入到四位有效数字。将你的最终答案表示为包含这三个值的单行矩阵。", "solution": "该问题要求在三种不同的约束强制机制下，计算判别器网络 $D(x)$ 的 Lipschitz 常数的估计值。判别器是一个三层前馈网络，由下式给出\n$$D(x) \\;=\\; W_{3}\\,\\sigma\\!\\left(W_{2}\\,\\sigma\\!\\left(W_{1}\\,x\\right)\\right)$$\n其中 $W_{1} \\in \\mathbb{R}^{128 \\times 64}$，$W_{2} \\in \\mathbb{R}^{64 \\times 128}$ 和 $W_{3} \\in \\mathbb{R}^{1 \\times 64}$ 是权重矩阵，$\\sigma$ 是整流线性单元 (ReLU) 激活函数。\n\n为了确定 $D(x)$ 的 Lipschitz 常数，我们将其建模为函数的复合。令 $f_1(x) = W_1 x$，$g_1(y_1) = \\sigma(y_1)$，$f_2(z_1) = W_2 z_1$，$g_2(y_2) = \\sigma(y_2)$，以及 $f_3(z_2) = W_3 z_2$。那么判别器可以写成复合函数 $D = f_3 \\circ g_2 \\circ f_2 \\circ g_1 \\circ f_1$。\n\n根据所给的事实：\n- 一个 $L_f$-Lipschitz 函数 $f$ 和一个 $L_g$-Lipschitz 函数 $g$ 的复合是 $(L_g L_f)$-Lipschitz 的。由此推广， $D$ 的 Lipschitz 常数（记为 $L_D$）的上界是其各组成函数的 Lipschitz 常数的乘积：\n$$L_D \\leq L_{f_3} \\cdot L_{g_2} \\cdot L_{f_2} \\cdot L_{g_1} \\cdot L_{f_1}$$\n- 函数 $f_i$ 是由矩阵 $W_i$ 表示的线性映射。它们关于欧几里得范数的 Lipschitz 常数是它们的算子范数，即 $L_{f_i} = \\|W_i\\|_2$。\n- 函数 $g_i$ 是 ReLU 激活函数 $\\sigma$。题目说明 ReLU 是 $1$-Lipschitz 的，因此 $L_{g_1} = L_{g_2} = 1$。\n\n将这些代入不等式，我们得到判别器 Lipschitz 常数的一个上界：\n$$L_D \\leq \\|W_3\\|_2 \\cdot 1 \\cdot \\|W_2\\|_2 \\cdot 1 \\cdot \\|W_1\\|_2 = \\|W_1\\|_2 \\|W_2\\|_2 \\|W_3\\|_2$$\n这个不等式构成了前两种情况分析的基础。\n\n(1) **权重裁剪 (Weight clipping):**\n在这种情况下，每个权重矩阵 $W_i$ 的所有元素都被裁剪，使得 $|(W_i)_{jk}| \\leq c$，其中 $c = 0.01$。我们利用给定的事实：对于一个矩阵 $W \\in \\mathbb{R}^{m \\times n}$，其算子范数有上界 $\\|W\\|_2 \\leq c \\sqrt{mn}$。我们将这个界应用于每个权重矩阵。\n\n- 对于 $W_1 \\in \\mathbb{R}^{128 \\times 64}$：\n维度为 $m=128$ 和 $n=64$。其算子范数的上界是：\n$$\\|W_1\\|_2 \\leq c \\sqrt{128 \\times 64} = 0.01 \\sqrt{8192} = 0.01 \\sqrt{64^2 \\times 2} = 0.01 \\times 64\\sqrt{2} = 0.64\\sqrt{2}$$\n\n- 对于 $W_2 \\in \\mathbb{R}^{64 \\times 128}$：\n维度为 $m=64$ 和 $n=128$。其算子范数的上界是：\n$$\\|W_2\\|_2 \\leq c \\sqrt{64 \\times 128} = 0.01 \\sqrt{8192} = 0.64\\sqrt{2}$$\n\n- 对于 $W_3 \\in \\mathbb{R}^{1 \\times 64}$：\n维度为 $m=1$ 和 $n=64$。其算子范数的上界是：\n$$\\|W_3\\|_2 \\leq c \\sqrt{1 \\times 64} = 0.01 \\sqrt{64} = 0.01 \\times 8 = 0.08$$\n\nLipschitz 常数 $L_D$ 的上界是这些单个上界的乘积：\n$$L_D \\leq (0.64\\sqrt{2}) \\cdot (0.64\\sqrt{2}) \\cdot (0.08) = (0.64)^2 \\cdot (\\sqrt{2})^2 \\cdot 0.08 = 0.4096 \\cdot 2 \\cdot 0.08 = 0.8192 \\cdot 0.08 = 0.065536$$\n四舍五入到四位有效数字，近似的 Lipschitz 常数是 $0.06554$。\n\n(2) **谱归一化 (Spectral normalization):**\n在这里，直接给出了训练后权重矩阵的算子范数：$\\|W_{1}\\|_{2} = 0.98$，$\\|W_{2}\\|_{2} = 1.03$，以及 $\\|W_{3}\\|_{2} = 0.99$。我们将这些值用于我们推导出的 $L_D$ 不等式：\n$$L_D \\leq \\|W_1\\|_2 \\|W_2\\|_2 \\|W_3\\|_2 = 0.98 \\times 1.03 \\times 0.99$$\n计算乘积：\n$$L_D \\leq 1.0094 \\times 0.99 = 0.999306$$\n四舍五入到四位有效数字，近似的 Lipschitz 常数是 $0.9993$。\n\n(3) **梯度惩罚 (Gradient penalty):**\n对于像判别器 $D(x)$ 这样的可微函数，其 Lipschitz 常数 $L_D$ 等于其梯度范数在整个输入域上的上确界：$L_D = \\sup_x \\|\\nabla_x D(x)\\|_2$。梯度惩罚方法促使这个梯度范数接近于 $1$。题目提供了一批点上观测到的梯度范数集合：$\\|\\nabla_{x} D(x_{t})\\|_{2} \\in \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\}$。\n题目要求我们用这个集合中观测到的最大梯度范数来近似 Lipschitz 常数。\n$$L_D \\approx \\max \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\} = 1.10$$\n为遵循四位有效数字的要求，这表示为 $1.100$。\n\n三个得到的近似 Lipschitz 常数分别是 $0.06554$ (权重裁剪)，$0.9993$ (谱归一化) 和 $1.100$ (梯度惩罚)。", "answer": "$$\\boxed{\\begin{pmatrix} 0.06554  0.9993  1.100 \\end{pmatrix}}$$", "id": "3124549"}, {"introduction": "理论和计算是为了指导实践。现在，是时候亲自动手，通过代码来探索GAN训练的动态过程了。在GAN的训练中，生成器和判别器之间的“军备竞赛”需要精妙的平衡，其中一个关键的超参数是每次生成器更新时，判别器需要更新的次数$k$。本练习将指导你从零开始构建一个简单的GAN训练循环，并通过量化指标来系统地研究$k$值的变化如何影响训练的稳定性和最终效果，从而将理论知识转化为实践经验。[@problem_id:3128933]", "problem": "您将进行一个简化的生成对抗网络 (GAN) 实验，旨在分析在单维高斯数据集上，每个生成器步骤交替进行 $k$ 次判别器步骤如何影响训练稳定性。该实验基于以下第一性原理：生成器 $G$ 将标准正态噪声 $z \\sim \\mathcal{N}(0,1)$ 转换为 $x_g = a z + b$，判别器 $D$ 通过 $D(x) = \\sigma(w x + c)$ 来估计输入为真实样本的概率，其中 $\\sigma$ 是 logistic sigmoid 函数。真实数据分布为 $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$。判别器最大化正确分类真实样本和伪造样本的期望对数似然，而生成器则基于判别器的输出最小化非饱和损失。\n\n请从深度学习和概率论中的以下基本依据和核心定义出发：\n- 生成对抗网络 (GAN) 的目标是生成器和判别器之间的双人极小极大博弈，由数据和噪声分布上的期望值驱动。\n- 基于梯度的学习动态使用蒙特卡洛样本来近似这些期望，并应用梯度上升进行最大化，应用梯度下降进行最小化。\n- 对于线性生成器 $G(z) = a z + b$，其中 $z \\sim \\mathcal{N}(0,1)$，生成器引出的分布是均值为 $b$、标准差为 $|a|$ 的高斯分布。\n- logistic sigmoid 函数为 $\\sigma(s) = \\frac{1}{1 + e^{-s}}$，求梯度所需的导数恒等式在统计学和机器学习中得到了充分验证。\n\n您的任务：\n1. 构建交替梯度动态：对于生成器参数 $(a,b)$ 在其非饱和目标上进行的每一次梯度下降更新，判别器参数 $(w,c)$ 都在其目标上进行 $k$ 次梯度上升更新。对双方都使用固定批量大小和固定学习率的蒙特卡洛估计。通过控制输入域来确保 $\\sigma$ 的数值稳定性。\n2. 定义训练稳定性的经验性概念。使用以下复合准则：令 $d_t = \\sqrt{(\\mu_g(t) - \\mu_r)^2 + (\\sigma_g(t) - \\sigma_r)^2}$，其中 $\\mu_g(t) = b(t)$ 和 $\\sigma_g(t) = |a(t)|$ 分别是训练步骤 $t$ 时生成器的均值和标准差。定义改进分数 $I = \\frac{d_0 - d_T}{\\max(d_0, \\epsilon)}$（其中 $\\epsilon$ 是一个很小的数），并定义振荡指数 $O = \\frac{\\sum_{t=1}^T \\|\\theta_g(t) - \\theta_g(t-1)\\|_2}{\\|\\theta_g(T) - \\theta_g(0)\\|_2 + \\epsilon}$（其中 $\\theta_g(t) = [a(t), b(t)]^\\top$）。如果参数保持有限且有界，$I$ 超过最小阈值，且 $O$ 低于最大阈值，则将一次运行分类为稳定。\n3. 通过在一个简单数据集上使用多个 $k$ 值进行训练，并报告每次运行是稳定还是不稳定（稳定编码为 $1$，不稳定编码为 $0$），凭经验推导出稳定性与 $k$ 的相图。\n\n使用以下数据集、训练和评估设置：\n- 真实数据参数 $(\\mu_r, \\sigma_r)$ 和调度参数 $k$ 因测试用例而异。\n- 噪声分布为 $z \\sim \\mathcal{N}(0,1)$。\n- 生成器为 $G(z) = a z + b$，初始化为 $a = 0.2$ 和 $b = 0.0$。\n- 判别器为 $D(x) = \\sigma(w x + c)$，$(w,c)$ 初始化为 $(0.0, 0.0)$。\n- 判别器目标是真实样本上的 $\\log D(x)$ 和伪造样本上的 $\\log(1 - D(G(z)))$ 的期望总和；判别器使用梯度上升。\n- 生成器目标是 $- \\log D(G(z))$ 的期望值（非饱和）；生成器使用梯度下降。\n- 使用学习率 $\\alpha_D = 0.05$ 和 $\\alpha_G = 0.02$，批量大小 $B = 1024$，以及 $T = 200$ 个生成器步骤。\n- 为保证数值稳定性，在应用 $\\sigma(s)$ 之前，将 sigmoid 的输入 $s$ 裁剪到区间 $[-50, 50]$ 内。\n\n定义稳定性分类阈值和保障措施：\n- 使用 $\\epsilon = 10^{-8}$ 进行分母稳定。\n- 如果任何参数的绝对值在任何时候超过 $100$ 或变为非数字 (not-a-number)，则声明为发散。\n- 使用阈值 $I_{\\min} = 0.25$ 和 $O_{\\max} = 2.5$。\n- 一次运行如果未发散，且满足 $I \\ge I_{\\min}$ 和 $O \\le O_{\\max}$，则为稳定。\n\n测试套件：\n- 情况 1：$(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 0, 42)$。\n- 情况 2：$(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 1, 42)$。\n- 情况 3：$(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 5, 42)$。\n- 情况 4：$(\\mu_r, \\sigma_r, k, \\text{seed}) = (2.0, 0.5, 2, 123)$。\n- 情况 5：$(\\mu_r, \\sigma_r, k, \\text{seed}) = (-1.0, 1.5, 10, 7)$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 是一个整数，$1$ 表示稳定运行，$0$ 表示不稳定运行。", "solution": "该问题要求对一个简化的生成对抗网络（GAN）的训练稳定性进行实证研究，研究其稳定性如何随每个生成器更新所对应的判别器更新次数 $k$ 的变化而变化。这包括推导基于梯度的学习动态、实施模拟，并根据一组指定的稳定性标准评估结果。\n\n首先，我们对模型的组件进行形式化定义。真实数据分布是单维高斯分布，$x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$。生成器 $G$ 通过线性变换将一个标准正态噪声变量 $z \\sim \\mathcal{N}(0,1)$ 映射到一个样本 $x_g$：\n$$G(z) = a z + b$$\n生成器的参数是 $\\theta_G = (a, b)$。因此，由生成器引出的分布也是高斯分布，其均值为 $\\mu_g = b$，标准差为 $\\sigma_g = |a|$。判别器 $D$ 是一个逻辑回归器，用于建模输入 $x$ 来自真实数据分布的概率：\n$$D(x) = \\sigma(w x + c)$$\n其中 $\\sigma(s) = (1 + e^{-s})^{-1}$ 是 logistic sigmoid 函数。判别器的参数是 $\\theta_D = (w,c)$。\n\n训练过程是一个极小极大博弈。训练判别器以最大化目标函数 $V_D$，该函数是正确分类真实样本和伪造样本的对数似然之和：\n$$V_D(\\theta_D, \\theta_G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n训练生成器以最小化非饱和目标函数 $V_G$，该函数旨在生成被判别器分类为真实的样本：\n$$V_G(\\theta_G) = -\\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]$$\n\n为了实现学习动态，我们推导这些目标函数关于模型参数的梯度。期望值通过对一个大小为 $B$ 的批量进行蒙特卡洛估计来近似。\n\n判别器目标 $V_D$ 关于其参数 $w$ 和 $c$ 的梯度，是使用链式法则和 sigmoid 函数的导数性质（具体为 $\\frac{d}{ds}\\log \\sigma(s) = 1 - \\sigma(s)$ 和 $\\frac{d}{ds}\\log(1-\\sigma(s)) = -\\sigma(s)$）求得的。对于一批真实数据 $\\{x_i\\}_{i=1}^B$ 和生成数据 $\\{x_{g,i}\\}_{i=1}^B$，梯度为：\n$$ \\nabla_w \\hat{V}_D = \\frac{1}{B} \\sum_{i=1}^B \\left( (1 - D(x_i))x_i - D(x_{g,i})x_{g,i} \\right) $$\n$$ \\nabla_c \\hat{V}_D = \\frac{1}{B} \\sum_{i=1}^B \\left( (1 - D(x_i)) - D(x_{g,i}) \\right) $$\n判别器参数通过梯度上升进行更新：\n$$ w \\leftarrow w + \\alpha_D \\nabla_w \\hat{V}_D $$\n$$ c \\leftarrow c + \\alpha_D \\nabla_c \\hat{V}_D $$\n\n生成器的非饱和目标 $V_G$ 关于其参数 $a$ 和 $b$ 的梯度是：\n$$ \\nabla_a V_G = \\frac{\\partial V_G}{\\partial a} = -\\mathbb{E}_z\\left[ \\frac{\\partial}{\\partial a} \\log D(az+b) \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) \\frac{\\partial(w(az+b)+c)}{\\partial a} \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) w z \\right] $$\n$$ \\nabla_b V_G = \\frac{\\partial V_G}{\\partial b} = -\\mathbb{E}_z\\left[ \\frac{\\partial}{\\partial b} \\log D(az+b) \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) \\frac{\\partial(w(az+b)+c)}{\\partial b} \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) w \\right] $$\n使用一批噪声 $\\{z_i\\}_{i=1}^B$ 进行近似，生成器参数通过梯度下降进行更新：\n$$ a \\leftarrow a - \\alpha_G \\left( -\\frac{w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) z_i \\right) = a + \\frac{\\alpha_G w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) z_i $$\n$$ b \\leftarrow b - \\alpha_G \\left( -\\frac{w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) \\right) = b + \\frac{\\alpha_G w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) $$\n\n完整的训练算法会进行 $T$ 个生成器步骤。在每一步 $t \\in \\{1, \\dots, T\\}$ 中：\n1.  判别器更新 $k$ 次。对于每次判别器更新，都会抽取一批新的真实数据 $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$ 和噪声 $z \\sim \\mathcal{N}(0,1)$。参数 $(w, c)$ 使用学习率 $\\alpha_D$ 通过梯度上升进行更新。\n2.  生成器更新一次。抽取一批新的噪声 $z \\sim \\mathcal{N}(0,1)$。参数 $(a, b)$ 使用学习率 $\\alpha_G$ 通过对非饱和损失进行梯度下降来更新。\n在整个模拟过程中，参数值都会被监控。如果任何参数的绝对值超过 $100$ 或变为非有限值 (NaN)，则终止运行并分类为发散。Sigmoid 函数的输入被裁剪到 $[-50, 50]$ 以防止数值溢出。\n\n完成 $T$ 步后，对训练运行的稳定性进行评估。生成器分布参数 $(\\mu_g(t)=b(t), \\sigma_g(t)=|a(t)|)$ 与目标真实数据参数 $(\\mu_r, \\sigma_r)$ 之间的距离定义为 $d_t = \\sqrt{(\\mu_g(t) - \\mu_r)^2 + (\\sigma_g(t) - \\sigma_r)^2}$。计算两个度量指标：\n1.  改进分数 $I$，衡量从初始状态到最终状态的距离的相对减少量：\n    $$ I = \\frac{d_0 - d_T}{\\max(d_0, \\epsilon)} $$\n    其中 $d_0$ 和 $d_T$ 分别是步骤 $0$ 和 $T$ 时的距离，$\\epsilon = 10^{-8}$ 是一个用于数值稳定性的小常数。一次运行必须达到 $I \\ge I_{\\min} = 0.25$。\n2.  振荡指数 $O$，衡量生成器参数 $\\theta_g(t)=[a(t), b(t)]^\\top$ 的总路径长度与净位移的比率：\n    $$ O = \\frac{\\sum_{t=1}^T \\|\\theta_g(t) - \\theta_g(t-1)\\|_2}{\\|\\theta_g(T) - \\theta_g(0)\\|_2 + \\epsilon} $$\n    一次运行必须满足 $O \\le O_{\\max} = 2.5$。\n\n一次运行当且仅当它不发散、达到 $I \\ge I_{\\min}$ 并且满足 $O \\le O_{\\max}$ 时，被分类为稳定（输出 $1$）。否则，它是不稳定的（输出 $0$）。所提供的实现为每个测试用例执行这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not required for this problem.\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN stability analysis for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # (mu_r, sigma_r, k, seed)\n        (0.0, 1.0, 0, 42),\n        (0.0, 1.0, 1, 42),\n        (0.0, 1.0, 5, 42),\n        (2.0, 0.5, 2, 123),\n        (-1.0, 1.5, 10, 7)\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_r, sigma_r, k, seed = case\n        result = run_training_and_evaluate(mu_r, sigma_r, k, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef sigmoid(s):\n    \"\"\"\n    Computes the logistic sigmoid function with input clipping for numerical stability.\n    \"\"\"\n    s_clipped = np.clip(s, -50.0, 50.0)\n    return 1.0 / (1.0 + np.exp(-s_clipped))\n\ndef run_training_and_evaluate(mu_r, sigma_r, k, seed,\n                              a_init=0.2, b_init=0.0,\n                              w_init=0.0, c_init=0.0,\n                              alpha_D=0.05, alpha_G=0.02,\n                              B=1024, T=200,\n                              epsilon=1e-8, divergence_threshold=100.0,\n                              I_min=0.25, O_max=2.5):\n    \"\"\"\n    Simulates the GAN training for one parameter set and evaluates its stability.\n    \"\"\"\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # Initialize parameters\n    a, b = a_init, b_init\n    w, c = w_init, c_init\n\n    # History for stability metrics\n    theta_g_history = [np.array([a, b])]\n    diverged = False\n\n    # Calculate initial distance to target distribution parameters\n    d_0 = np.sqrt((b - mu_r)**2 + (np.abs(a) - sigma_r)**2)\n\n    # Main training loop (T generator steps)\n    for _ in range(T):\n        # --- Discriminator updates (k steps) ---\n        if k > 0:\n            for _ in range(k):\n                # Sample data\n                x_r = np.random.normal(loc=mu_r, scale=sigma_r, size=B)\n                z = np.random.normal(loc=0.0, scale=1.0, size=B)\n                x_g = a * z + b\n\n                # Discriminator predictions\n                d_real = sigmoid(w * x_r + c)\n                d_fake = sigmoid(w * x_g + c)\n                \n                # Discriminator gradients (for maximizing log-likelihood)\n                grad_w = np.mean((1.0 - d_real) * x_r - d_fake * x_g)\n                grad_c = np.mean((1.0 - d_real) - d_fake)\n\n                # Update D parameters via gradient ascent\n                w += alpha_D * grad_w\n                c += alpha_D * grad_c\n\n                # Check for D divergence\n                if not (np.isfinite(w) and np.isfinite(c) and\n                        abs(w)  divergence_threshold and abs(c)  divergence_threshold):\n                    diverged = True\n                    break\n            if diverged:\n                break\n\n        # --- Generator update (1 step) ---\n        z_g = np.random.normal(loc=0.0, scale=1.0, size=B)\n        x_g_g = a * z_g + b\n        d_fake_for_g = sigmoid(w * x_g_g + c)\n\n        # Generator gradients (for minimizing non-saturating loss -log(D(G(z))))\n        grad_V_G_a = -np.mean((1.0 - d_fake_for_g) * w * z_g)\n        grad_V_G_b = -np.mean((1.0 - d_fake_for_g) * w)\n        \n        # Update G parameters via gradient descent\n        a -= alpha_G * grad_V_G_a\n        b -= alpha_G * grad_V_G_b\n\n        # Check for G divergence\n        if not (np.isfinite(a) and np.isfinite(b) and\n                abs(a)  divergence_threshold and abs(b)  divergence_threshold):\n            diverged = True\n            break\n            \n        theta_g_history.append(np.array([a, b]))\n\n    # --- Stability Evaluation ---\n    if diverged:\n        return 0\n\n    a_T, b_T = theta_g_history[-1]\n    \n    # Final distance\n    d_T = np.sqrt((b_T - mu_r)**2 + (np.abs(a_T) - sigma_r)**2)\n    \n    # Improvement Fraction (I)\n    I = (d_0 - d_T) / max(d_0, epsilon)\n\n    # Oscillation Index (O)\n    path_length = np.sum([np.linalg.norm(theta_g_history[t] - theta_g_history[t-1]) for t in range(1, T + 1)])\n    net_displacement = np.linalg.norm(theta_g_history[-1] - theta_g_history[0])\n    O = path_length / (net_displacement + epsilon)\n\n    # Classify as stable (1) or unstable (0)\n    if I >= I_min and O = O_max:\n        return 1\n    else:\n        return 0\n\nsolve()\n```", "id": "3128933"}]}