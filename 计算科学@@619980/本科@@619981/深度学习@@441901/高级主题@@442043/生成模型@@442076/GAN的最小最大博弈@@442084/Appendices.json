{"hands_on_practices": [{"introduction": "生成对抗网络（GAN）的训练可以看作是生成器与判别器之间的一场极小极大博弈。这场博弈的理想终点是达到纳什均衡，此时生成器能够完美地复刻真实数据分布。通过分析一个简化的场景，我们可以清晰地从数学上理解这一均衡状态。本练习 [@problem_id:3185804] 将通过一个一维高斯分布的例子，引导你推导出GAN在理想均衡点下的参数，为你理解GAN的目标奠定坚实的理论基础。", "problem": "考虑一个标准的生成对抗网络 (GAN) 的极小极大博弈，其价值函数为\n$$\nV(D,G) \\;=\\; \\mathbb{E}_{x\\sim p_{\\text{data}}}\\big[\\ln D(x)\\big] \\;+\\; \\mathbb{E}_{z\\sim p_{z}}\\big[\\ln\\big(1 - D(G(z))\\big)\\big],\n$$\n其中，$p_{\\text{data}}=\\mathcal{N}(\\mu,\\sigma^{2})$ 是一个一维高斯数据分布，其参数 $\\mu\\in\\mathbb{R}$ 和 $\\sigma0$ 未知，潜变量分布为 $p_{z}=\\mathcal{N}(0,1)$，生成器是一个仿射映射 $G(z)=a z + b$，其参数为 $a\\in\\mathbb{R}$ 和 $b\\in\\mathbb{R}$，判别器 $D:\\mathbb{R}\\to(0,1)$ 是一个任意的可测函数。假设该参数化强制执行了可识别性约束 $a\\ge 0$。\n\n从上述极小极大博弈的定义出发，仅使用概率论、优化和信息度量的第一性原理，完成以下任务：\n- 对于一个固定的生成器 $G$（等价地，一个固定的模型分布），通过关于 $D$ 最大化价值函数来推导最优判别器 $D^{\\star}$。\n- 将此最优判别器代回价值函数，以获得生成器的诱导目标函数。利用著名信息度量的基本性质，论证该诱导目标函数在何处最小化。\n- 据此，确定均衡时的生成器参数 $(a^{\\star}, b^{\\star})$，表示为 $\\mu$ 和 $\\sigma$ 的函数。\n\n将最终答案以行矩阵 $\\begin{pmatrix} a^{\\star}  b^{\\star} \\end{pmatrix}$ 的形式给出。无需进行数值舍入。", "solution": "本题旨在求解一个标准生成对抗网络 (GAN) 设置中生成模型的均衡参数 $(a^{\\star}, b^{\\star})$。该过程包括三个主要步骤：为固定的生成器寻找最优判别器，推导生成器的目标函数，以及最小化此目标函数以找到最优的生成器参数。\n\n首先，我们确定所涉及的分布。数据分布为 $p_{\\text{data}} = \\mathcal{N}(\\mu, \\sigma^2)$。生成器是一个仿射变换 $G(z) = az + b$，应用于从标准正态分布 $p_z = \\mathcal{N}(0,1)$ 中抽取的潜变量 $z$。因此，生成数据 $p_g$ 的分布也是一个正态分布。生成数据的均值为 $\\mathbb{E}[G(z)] = \\mathbb{E}[az+b] = a\\mathbb{E}[z]+b = a \\cdot 0 + b = b$。方差为 $\\text{Var}(G(z)) = \\text{Var}(az+b) = a^2\\text{Var}(z) = a^2 \\cdot 1^2 = a^2$。因此，生成的数据分布为 $p_g = \\mathcal{N}(b, a^2)$。可识别性约束为 $a \\ge 0$。\n\n极小极大博弈由价值函数 $V(D, G)$ 定义：\n$$V(D,G) \\;=\\; \\mathbb{E}_{x\\sim p_{\\text{data}}}\\big[\\ln D(x)\\big] \\;+\\; \\mathbb{E}_{z\\sim p_{z}}\\big[\\ln\\big(1 - D(G(z))\\big)\\big]$$\n通过变量替换，第二个期望可以基于生成数据 $p_g$ 的分布来计算。这使我们可以将价值函数写成在数据空间 $\\mathbb{R}$ 上的积分：\n$$V(D,G) \\;=\\; \\int_{x \\in \\mathbb{R}} p_{\\text{data}}(x) \\ln D(x) \\,dx \\;+\\; \\int_{x \\in \\mathbb{R}} p_g(x) \\ln(1 - D(x)) \\,dx$$\n$$V(D,G) \\;=\\; \\int_{x \\in \\mathbb{R}} \\big[ p_{\\text{data}}(x) \\ln D(x) + p_g(x) \\ln(1 - D(x)) \\big] \\,dx$$\n对于一个固定的生成器 $G$（因此 $p_g$ 也固定），我们通过对函数 $D(x)$ 最大化 $V(D,G)$ 来找到最优判别器 $D^{\\star}(x)$。由于当被积函数对每个 $x$ 的值都取最大时，积分也达到最大值，我们考虑对一个固定的 $x$，函数 $f(y) = c_1 \\ln y + c_2 \\ln(1 - y)$，其中 $y = D(x)$，$c_1 = p_{\\text{data}}(x)$，$c_2 = p_g(x)$。$y$ 的定义域是 $(0,1)$。为求最大值，我们计算关于 $y$ 的导数并将其设为零：\n$$\\frac{df}{dy} = \\frac{c_1}{y} - \\frac{c_2}{1-y} = 0$$\n$$\\frac{p_{\\text{data}}(x)}{D(x)} - \\frac{p_g(x)}{1 - D(x)} = 0$$\n$$p_{\\text{data}}(x)(1 - D(x)) = p_g(x)D(x)$$\n$$p_{\\text{data}}(x) = D(x) \\big(p_{\\text{data}}(x) + p_g(x)\\big)$$\n解出 $D(x)$ 得到最优判别器 $D^{\\star}$：\n$$D^{\\star}(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n\n接下来，我们将这个最优判别器 $D^{\\star}(x)$ 代回价值函数 $V(D,G)$，以找到生成器的目标函数 $C(G) = \\max_D V(D,G) = V(D^{\\star}, G)$。\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right] + \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(1 - \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right]$$\n第二个对数内的项可以简化为：\n$$1 - \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} = \\frac{p_{\\text{data}}(x) + p_g(x) - p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} = \\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n所以生成器的目标函数变为：\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right] + \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(\\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right]$$\n我们可以将这个表达式与著名的信息度量联系起来。通过在每个对数项内加上和减去 $\\ln 2$，我们可以用 Kullback-Leibler (KL) 散度来表示 $C(G)$。两个分布 $P$ 和 $Q$ 之间的 KL 散度为 $D_{KL}(P \\| Q) = \\mathbb{E}_{x \\sim P}[\\ln(P(x)/Q(x))]$。\n让我们重写 $C(G)$ 中的各项：\n$$ \\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right) = \\ln\\left(\\frac{p_{\\text{data}}(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right) - \\ln(2) $$\n$$ \\ln\\left(\\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}\\right) = \\ln\\left(\\frac{p_g(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right) - \\ln(2) $$\n将这些代回到 $C(G)$ 的表达式中：\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right)\\right] - \\ln(2) \\;+\\; \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(\\frac{p_g(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right)\\right] - \\ln(2)$$\n识别出 KL 散度项，我们得到：\n$$C(G) = D_{KL}\\left(p_{\\text{data}} \\left\\| \\frac{p_{\\text{data}} + p_g}{2}\\right.\\right) + D_{KL}\\left(p_g \\left\\| \\frac{p_{\\text{data}} + p_g}{2}\\right.\\right) - 2\\ln(2)$$\n这两个 KL 散度的和是 $p_{\\text{data}}$ 和 $p_g$ 之间 Jensen-Shannon 散度 (JSD) 的两倍，其中 $JSD(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M)$ 且 $M = \\frac{P+Q}{2}$。\n因此，生成器的目标是：\n$$C(G) = 2 \\cdot JSD(p_{\\text{data}} \\| p_g) - 2\\ln(2)$$\n生成器的目标是最小化 $C(G)$。由于 $2$ 和 $-2\\ln(2)$ 是常数，这等价于最小化 Jensen-Shannon 散度 $JSD(p_{\\text{data}} \\| p_g)$。JSD 的一个基本性质是它总是非负的，且 $JSD(P \\| Q) = 0$ 当且仅当 $P=Q$。因此，当生成的分布 $p_g$ 与数据分布 $p_{\\text{data}}$ 相同时，$C(G)$ 达到其最小值。\n\n最后，我们通过设置 $p_g = p_{\\text{data}}$ 来确定最优的生成器参数 $(a^{\\star}, b^{\\star})$。\n我们有数据分布 $p_{\\text{data}} = \\mathcal{N}(\\mu, \\sigma^2)$ 和生成分布 $p_g = \\mathcal{N}(b, a^2)$。要使这两个正态分布相同，它们的参数（均值和方差）必须相等。\n令均值相等：\n$$b = \\mu$$\n令方差相等：\n$$a^2 = \\sigma^2$$\n这意味着 $a = \\pm \\sigma$。问题中指定了可识别性约束 $a \\ge 0$。由于标准差 $\\sigma$ 已知为正 ($\\sigma  0$)，满足该约束的唯一解是 $a = \\sigma$。\n因此，均衡时的生成器参数为 $a^{\\star} = \\sigma$ 和 $b^{\\star} = \\mu$。\n最终答案是行矩阵 $(a^{\\star}, b^{\\star})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sigma  \\mu\n\\end{pmatrix}\n}\n$$", "id": "3185804"}, {"introduction": "理论上的均衡虽然清晰，但在实践中，让生成器学会复杂的多模态数据分布却充满挑战，其中一个典型难题就是“模式坍塌”（mode collapse）。此现象指的是生成器只学会了真实数据分布中的少数几种模式，而忽略了其他模式。本编码练习 [@problem_id:3185781] 将让你亲手搭建一个简单的GAN训练流程，通过实验直观地探索隐空间（latent space）的结构如何影响生成器覆盖数据全部分布模式的能力。", "problem": "要求您设计并实现一个完整的、可运行的程序，该程序模拟生成对抗网络 (GAN) 的极小极大训练动态，并利用此模拟研究多模态隐先验分布如何与博弈相互作用，以促进或抑制生成器分布中的模式覆盖。\n\n您必须使用的定义和设置：\n- 生成对抗网络 (GAN) 由一个生成器和一个判别器组成。生成器是带参数 $\\theta$ 的函数 $g_{\\theta}$，它将隐变量 $z$ 映射到数据空间 $x$。判别器是带参数 $\\phi$ 的分类器 $D_{\\phi}$，它输出一个输入 $x$ 是真实数据而非生成数据的概率。\n- 极小极大值函数为\n$$\nV(D_{\\phi}, G_{\\theta}) \\;=\\; \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D_{\\phi}(x)] \\;+\\; \\mathbb{E}_{z \\sim p_{z}}[\\log(1 - D_{\\phi}(g_{\\theta}(z)))].\n$$\n- 在此任务中，您必须使用一维数据域和以下参数族：\n  - 生成器：$g_{\\theta}(z) \\;=\\; a z + b$，其中 $\\theta = (a,b)$ 且 $a,b \\in \\mathbb{R}$。\n  - 判别器：$D_{\\phi}(x) \\;=\\; \\sigma(w x + c)$，其中 $\\phi = (w,c)$，$w,c \\in \\mathbb{R}$，且 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ 是 logistic sigmoid 函数。\n\n要使用的数据分布和隐先验：\n- 目标数据分布 $p_{\\text{data}}$ 是实数线上的两个高斯分布的平衡混合：\n$$\np_{\\text{data}} \\;=\\; \\tfrac{1}{2}\\,\\mathcal{N}(-\\mu,\\sigma_{\\text{data}}^{2}) \\;+\\; \\tfrac{1}{2}\\,\\mathcal{N}(\\mu,\\sigma_{\\text{data}}^{2}),\n$$\n其中 $\\mu = 2.0$ 且 $\\sigma_{\\text{data}} = 0.2$。\n- 隐先验 $p_{z}$ 将是两个高斯分布的混合：\n$$\np_{z} \\;=\\; \\alpha\\,\\mathcal{N}(m_{1},\\sigma_{z}^{2}) \\;+\\; (1-\\alpha)\\,\\mathcal{N}(m_{2},\\sigma_{z}^{2}),\n$$\n其参数 $(m_{1},m_{2},\\alpha,\\sigma_{z})$ 由下面的每个测试用例指定。\n\n您必须实现的训练协议：\n- 使用如上所述的极小极大目标 $V(D_{\\phi}, G_{\\theta})$。\n- 在每一步使用蒙特卡洛方法，通过从 $p_{\\text{data}}$ 和 $p_{z}$ 中抽取的独立同分布样本来近似期望。\n- 通过计算 $V(D_{\\phi}, G_{\\theta})$ 分别关于判别器参数 $(w,c)$ 和生成器参数 $(a,b)$ 的梯度，从第一性原理推导参数更新规则。对 $(w,c)$ 使用梯度上升，对 $(a,b)$ 使用梯度下降。\n- 精确使用以下超参数：\n  - 训练迭代次数 $T = 2000$。\n  - 在每次迭代中，执行 $1$ 次判别器更新，然后执行 $1$ 次生成器更新。\n  - 每次更新的真实样本和隐样本的批量大小 $B = 512$。\n  - 判别器的学习率 $\\eta_{D} = 0.01$。\n  - 生成器的学习率 $\\eta_{G} = 0.05$。\n  - 初始化：$a_{0} = 0.1$，$b_{0} = 0.0$，$w_{0} = 0.0$，$c_{0} = 0.0$。\n  - 将随机种子设置为固定值以确保可复现性。\n\n模式覆盖的实验性度量：\n- 训练后，计算生成器对隐分量均值的映射：$m'_{1} = a\\,m_{1} + b$ 和 $m'_{2} = a\\,m_{2} + b$。\n- 定义数据均值集合 $\\{-\\mu, \\mu\\}$ 和生成均值集合 $\\{m'_{1}, m'_{2}\\}$。\n- 如果 $\\min\\{|m'_{1} - \\mu_{d}|, |m'_{2} - \\mu_{d}|\\} \\le \\tau$，则认为均值为 $\\mu_{d} \\in \\{-\\mu, \\mu\\}$ 的数据模式被“覆盖”，其中覆盖容差为 $\\tau = 0.6$。\n- 覆盖分数是在 $\\{0,1,2\\}$ 中的整数计数，等于在这种意义上被生成器覆盖的数据模式的数量。\n\n您必须实现并运行的测试套件：\n- 对 $(m_{1},m_{2},\\alpha,\\sigma_{z})$ 使用以下五个案例：\n  - 案例 $1$：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-1.0,\\,1.0,\\,0.5,\\,0.3)$。\n  - 案例 $2$：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (0.0,\\,0.0,\\,0.5,\\,0.3)$。\n  - 案例 $3$：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-1.0,\\,1.0,\\,0.95,\\,0.3)$。\n  - 案例 $4$：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-0.2,\\,0.2,\\,0.5,\\,0.3)$。\n  - 案例 $5$：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-3.0,\\,3.0,\\,0.5,\\,0.3)$。\n\n输出内容：\n- 您的程序必须运行测试套件中的所有五个案例，按规定为每个案例训练 GAN，计算每个案例的最终覆盖分数，并将这五个整数结果聚合到一个列表中，打印在一行上。\n- 最终输出格式必须是仅包含一行，内容为方括号括起来的逗号分隔的结果列表，例如：$[r_{1},r_{2},r_{3},r_{4},r_{5}]$，其中每个 $r_{i}$ 是 $\\{0,1,2\\}$ 中的一个整数，并且没有多余的文本。\n\n注释：\n- 所有量都是无量纲的；不涉及物理单位。\n- 不使用角度；不需要角度单位。\n- 百分比（例如 $\\alpha$）是 $[0,1]$ 范围内的参数，必须作为小数处理，而不是百分数。", "solution": "该问题要求设计并实现一个模拟简单生成对抗网络 (GAN) 训练动态的程序。其目标是探究多模态隐先验分布 $p_z$ 的结构如何影响生成器捕获双模态数据分布 $p_{\\text{data}}$ 模式的能力。我们将首先推导必要的数学框架，然后基于这些原理规定算法实现。\n\nGAN 框架由一个带参数 $\\theta=(a,b)$ 的生成器 $G_{\\theta}$ 和一个带参数 $\\phi=(w,c)$ 的判别器 $D_{\\phi}$ 组成。生成器是一个线性函数 $g_{\\theta}(z) = az + b$。判别器是一个 logistic 分类器 $D_{\\phi}(x) = \\sigma(wx + c)$，其中 $\\sigma(u)=(1+e^{-u})^{-1}$ 是 logistic sigmoid 函数。训练过程由价值函数 $V(D,G)$ 上的极小极大博弈所支配：\n$$\nV(D_{\\phi}, G_{\\theta}) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D_{\\phi}(x)] + \\mathbb{E}_{z \\sim p_{z}}[\\log(1 - D_{\\phi}(g_{\\theta}(z)))]\n$$\n判别器旨在最大化该值，而生成器旨在最小化它。\n\n### 参数更新的梯度推导\n\n为实现训练，我们使用基于梯度的优化。判别器参数 $\\phi=(w,c)$ 使用梯度上升进行更新，生成器参数 $\\theta=(a,b)$ 使用梯度下降进行更新。因此，我们必须推导 $V$ 关于这些参数的梯度。\n\n**1. 判别器梯度 ($\\nabla_{\\phi} V$)**\n\n判别器的更新规则是 $\\phi \\leftarrow \\phi + \\eta_D \\nabla_{\\phi} V$。我们计算 $V$ 关于 $w$ 和 $c$ 的偏导数。使用链式法则和性质 $\\frac{d}{du}\\log\\sigma(u) = 1-\\sigma(u)$：\n$$\n\\frac{\\partial}{\\partial w} \\log D_{\\phi}(x) = \\frac{\\partial}{\\partial w} \\log \\sigma(wx+c) = (1 - \\sigma(wx+c)) \\cdot x = (1 - D_{\\phi}(x))x\n$$\n$$\n\\frac{\\partial}{\\partial c} \\log D_{\\phi}(x) = \\frac{\\partial}{\\partial c} \\log \\sigma(wx+c) = (1 - \\sigma(wx+c)) \\cdot 1 = 1 - D_{\\phi}(x)\n$$\n同样地，使用性质 $\\frac{d}{du}\\log(1-\\sigma(u)) = -\\sigma(u)$：\n$$\n\\frac{\\partial}{\\partial w} \\log(1 - D_{\\phi}(g_{\\theta}(z))) = -\\sigma(wg_{\\theta}(z)+c) \\cdot g_{\\theta}(z) = -D_{\\phi}(g_{\\theta}(z))g_{\\theta}(z)\n$$\n$$\n\\frac{\\partial}{\\partial c} \\log(1 - D_{\\phi}(g_{\\theta}(z))) = -\\sigma(wg_{\\theta}(z)+c) \\cdot 1 = -D_{\\phi}(g_{\\theta}(z))\n$$\n对各自的分布求期望，我们得到梯度：\n$$\n\\frac{\\partial V}{\\partial w} = \\mathbb{E}_{x \\sim p_{\\text{data}}}[(1 - D_{\\phi}(x))x] + \\mathbb{E}_{z \\sim p_{z}}[-D_{\\phi}(g_{\\theta}(z))g_{\\theta}(z)]\n$$\n$$\n\\frac{\\partial V}{\\partial c} = \\mathbb{E}_{x \\sim p_{\\text{data}}}[1 - D_{\\phi}(x)] + \\mathbb{E}_{z \\sim p_{z}}[-D_{\\phi}(g_{\\theta}(z))]\n$$\n\n**2. 生成器梯度 ($\\nabla_{\\theta} V$)**\n\n生成器的更新规则是 $\\theta \\leftarrow \\theta - \\eta_G \\nabla_{\\theta} V$。$V$ 中的第一项与 $\\theta$ 无关。我们计算第二项关于 $a$ 和 $b$ 的偏导数：\n$$\n\\nabla_{\\theta} V = \\mathbb{E}_{z \\sim p_{z}}[\\nabla_{\\theta} \\log(1 - D_{\\phi}(g_{\\theta}(z)))]\n$$\n使用链式法则，$\\nabla_{\\theta}[\\cdot] = \\frac{\\partial[\\cdot]}{\\partial g_{\\theta}} \\nabla_{\\theta}g_{\\theta}(z)$。关于生成器输出 $g$ 的梯度为：\n$$\n\\frac{\\partial}{\\partial g} \\log(1 - D_{\\phi}(g)) = \\frac{\\partial}{\\partial g} \\log(1 - \\sigma(wg+c)) = -\\sigma(wg+c) \\cdot w = -w D_{\\phi}(g)\n$$\n生成器函数 $g_{\\theta}(z) = az+b$ 的梯度为 $\\frac{\\partial g_{\\theta}}{\\partial a} = z$ 和 $\\frac{\\partial g_{\\theta}}{\\partial b} = 1$。将它们结合起来得到：\n$$\n\\frac{\\partial V}{\\partial a} = \\mathbb{E}_{z \\sim p_{z}} \\left[ (-w D_{\\phi}(g_{\\theta}(z))) \\cdot z \\right] = -\\mathbb{E}_{z \\sim p_{z}} [w D_{\\phi}(g_{\\theta}(z)) z]\n$$\n$$\n\\frac{\\partial V}{\\partial b} = \\mathbb{E}_{z \\sim p_{z}} \\left[ (-w D_{\\phi}(g_{\\theta}(z))) \\cdot 1 \\right] = -\\mathbb{E}_{z \\sim p_{z}} [w D_{\\phi}(g_{\\theta}(z))]\n$$\n\n### 算法实现\n\n训练过程模拟 $T=2000$ 次迭代。在每次迭代中，我们执行一次判别器更新，然后执行一次生成器更新。期望使用批量大小为 $B=512$ 的蒙特卡洛估计来近似。\n\n**1. 采样**\n-   **数据分布 $p_{\\text{data}}$**：要从 $p_{\\text{data}} = \\frac{1}{2}\\,\\mathcal{N}(-\\mu,\\sigma_{\\text{data}}^{2}) + \\frac{1}{2}\\,\\mathcal{N}(\\mu,\\sigma_{\\text{data}}^{2})$ 中抽取一个样本，我们首先以相等的概率 ($0.5$) 选择两个高斯分量中的一个，然后从所选分量中抽取一个样本。对于大小为 $B$ 的批量，大约有 $B/2$ 个样本将从每个分量中抽取。\n-   **隐分布 $p_z$**：要从 $p_{z} = \\alpha\\,\\mathcal{N}(m_{1},\\sigma_{z}^{2}) + (1-\\alpha)\\,\\mathcal{N}(m_{2},\\sigma_{z}^{2})$ 中抽取一个样本，我们以概率 $\\alpha$ 选择第一个分量，以概率 $1-\\alpha$ 选择第二个分量，然后从所选的高斯分布中采样。\n\n**2. 训练循环**\n所有参数均按规定初始化：$a_0=0.1, b_0=0.0, w_0=0.0, c_0=0.0$。对于每次迭代 $t=0, \\dots, T-1$：\n\n-   **判别器更新**：\n    1.  抽取一个批量 $B$ 个真实样本 $\\{x_i\\}_{i=1}^B \\sim p_{\\text{data}}$。\n    2.  抽取一个批量 $B$ 个隐样本 $\\{z_i\\}_{i=1}^B \\sim p_{z}$。\n    3.  生成一个批量 $B$ 个伪样本 $\\{x_{g,i}\\}_{i=1}^B$，其中 $x_{g,i} = g_{\\theta_t}(z_i) = a_t z_i + b_t$。\n    4.  使用当前参数 $\\phi_t, \\theta_t$ 和批量数据计算梯度 $\\nabla_{w} V_t$ 和 $\\nabla_{c} V_t$：\n        $$\n        \\nabla_{w} V_t \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ (1 - D_{\\phi_t}(x_i))x_i - D_{\\phi_t}(x_{g,i})x_{g,i} \\right] \\\\\n        \\nabla_{c} V_t \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ (1 - D_{\\phi_t}(x_i)) - D_{\\phi_t}(x_{g,i}) \\right]\n        $$\n    5.  更新判别器参数：$w_{t+1} = w_t + \\eta_D \\nabla_{w} V_t$，$c_{t+1} = c_t + \\eta_D \\nabla_{c} V_t$。\n\n-   **生成器更新**：\n    1.  抽取一个新的批量 $B$ 个隐样本 $\\{z'_i\\}_{i=1}^B \\sim p_{z}$。\n    2.  使用旧的生成器参数生成一个新的批量 $B$ 个伪样本 $\\{x'_{g,i}\\}_{i=1}^B$：$x'_{g,i} = g_{\\theta_t}(z'_i) = a_t z'_i + b_t$。\n    3.  使用旧的生成器参数 $\\theta_t$ 但使用*新更新的*判别器参数 $\\phi_{t+1}$ 来计算梯度 $\\nabla_{a} V_t$ 和 $\\nabla_{b} V_t$：\n        $$\n        \\nabla_{a} V_t \\approx -\\frac{1}{B}\\sum_{i=1}^B w_{t+1} D_{\\phi_{t+1}}(x'_{g,i}) z'_i \\\\\n        \\nabla_{b} V_t \\approx -\\frac{1}{B}\\sum_{i=1}^B w_{t+1} D_{\\phi_{t+1}}(x'_{g,i})\n        $$\n    4.  更新生成器参数：$a_{t+1} = a_t - \\eta_G \\nabla_{a} V_t$，$b_{t+1} = b_t - \\eta_G \\nabla_{b} V_t$。\n\n### 模式覆盖的评估\n\n在 $T$ 次训练迭代后，最终的生成器参数 $(a_T, b_T)$ 用于评估模式覆盖。隐先验的分量均值为 $m_1$ 和 $m_2$。生成器将它们映射到 $m'_1 = a_T m_1 + b_T$ 和 $m'_2 = a_T m_2 + b_T$。目标数据分布的均值在 $-\\mu$ 和 $\\mu$（其中 $\\mu=2.0$）。\n\n如果至少一个生成均值接近数据模式的均值 $\\mu_d \\in \\{-\\mu, \\mu\\}$，即 $\\min\\{|m'_{1} - \\mu_{d}|, |m'_{2} - \\mu_{d}|\\} \\le \\tau$（其中容差为 $\\tau = 0.6$），则认为该数据模式被“覆盖”。给定测试用例的最终分数是覆盖的数据模式总数，可以是 $0$、$1$ 或 $2$。对问题陈述中定义的五个测试用例中的每一个重复此过程。", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN simulation for all test cases and print results.\n    \"\"\"\n\n    # --- Fixed Parameters and Hyperparameters ---\n    fixed_params = {\n        \"mu\": 2.0,\n        \"sigma_data\": 0.2,\n    }\n    hyperparams = {\n        \"T\": 2000,\n        \"B\": 512,\n        \"eta_D\": 0.01,\n        \"eta_G\": 0.05,\n        \"a0\": 0.1,\n        \"b0\": 0.0,\n        \"w0\": 0.0,\n        \"c0\": 0.0,\n        \"seed\": 1234, # Fixed seed for reproducibility\n        \"tau\": 0.6,\n    }\n\n    # --- Test Suite ---\n    test_cases = [\n        # (m1, m2, alpha, sigma_z)\n        (-1.0, 1.0, 0.5, 0.3),   # Case 1\n        (0.0, 0.0, 0.5, 0.3),    # Case 2\n        (-1.0, 1.0, 0.95, 0.3),  # Case 3\n        (-0.2, 0.2, 0.5, 0.3),   # Case 4\n        (-3.0, 3.0, 0.5, 0.3),   # Case 5\n    ]\n    \n    results = []\n    for case_params_tuple in test_cases:\n        case_params = {\n            \"m1\": case_params_tuple[0],\n            \"m2\": case_params_tuple[1],\n            \"alpha\": case_params_tuple[2],\n            \"sigma_z\": case_params_tuple[3],\n        }\n        score = _run_simulation_for_case(case_params, fixed_params, hyperparams)\n        results.append(score)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef _sample_p_data(rng, B, mu, sigma_data):\n    \"\"\"Samples a batch from the real data distribution p_data.\"\"\"\n    # Choose which Gaussian component to sample from for each point\n    # 0 for N(-mu, sigma_data^2), 1 for N(mu, sigma_data^2)\n    choices = rng.binomial(1, 0.5, size=B)\n    \n    # Sample from N(-mu, sigma_data^2)\n    n_neg = B - np.sum(choices)\n    samples_neg = rng.normal(loc=-mu, scale=sigma_data, size=n_neg)\n    \n    # Sample from N(mu, sigma_data^2)\n    n_pos = np.sum(choices)\n    samples_pos = rng.normal(loc=mu, scale=sigma_data, size=n_pos)\n    \n    return np.concatenate((samples_neg, samples_pos))\n\ndef _sample_p_z(rng, B, m1, m2, alpha, sigma_z):\n    \"\"\"Samples a batch from the latent distribution p_z.\"\"\"\n    # Choose which Gaussian component to sample from for each point\n    # 0 for N(m1, sigma_z^2), 1 for N(m2, sigma_z^2)\n    choices = rng.binomial(1, 1 - alpha, size=B)\n\n    # Sample from N(m1, sigma_z^2)\n    n_1 = B - np.sum(choices)\n    samples_1 = rng.normal(loc=m1, scale=sigma_z, size=n_1)\n\n    # Sample from N(m2, sigma_z^2)\n    n_2 = np.sum(choices)\n    samples_2 = rng.normal(loc=m2, scale=sigma_z, size=n_2)\n\n    return np.concatenate((samples_1, samples_2))\n    \n\ndef _run_simulation_for_case(case_params, fixed_params, hyperparams):\n    \"\"\"\n    Trains a GAN for a single case and computes the coverage score.\n    \"\"\"\n    # Unpack parameters for convenience\n    mu, sigma_data = fixed_params[\"mu\"], fixed_params[\"sigma_data\"]\n    m1, m2, alpha, sigma_z = case_params[\"m1\"], case_params[\"m2\"], case_params[\"alpha\"], case_params[\"sigma_z\"]\n    T, B, eta_D, eta_G, tau = hyperparams[\"T\"], hyperparams[\"B\"], hyperparams[\"eta_D\"], hyperparams[\"eta_G\"], hyperparams[\"tau\"]\n    \n    # Initialize random number generator\n    rng = np.random.default_rng(hyperparams[\"seed\"])\n\n    # Initialize parameters\n    a, b = hyperparams[\"a0\"], hyperparams[\"b0\"]\n    w, c = hyperparams[\"w0\"], hyperparams[\"c0\"]\n\n    # Training loop\n    for _ in range(T):\n        # --- Discriminator Update ---\n        x_real = _sample_p_data(rng, B, mu, sigma_data)\n        z_d = _sample_p_z(rng, B, m1, m2, alpha, sigma_z)\n        x_fake = a * z_d + b\n\n        D_real = sigmoid(w * x_real + c)\n        D_fake = sigmoid(w * x_fake + c)\n\n        grad_w_V = np.mean((1 - D_real) * x_real) - np.mean(D_fake * x_fake)\n        grad_c_V = np.mean(1 - D_real) - np.mean(D_fake)\n\n        w += eta_D * grad_w_V\n        c += eta_D * grad_c_V\n\n        # --- Generator Update ---\n        z_g = _sample_p_z(rng, B, m1, m2, alpha, sigma_z)\n        x_fake_g = a * z_g + b\n\n        # D output for fake samples using updated D parameters\n        D_fake_g = sigmoid(w * x_fake_g + c)\n        \n        # Gradients of V w.r.t a and b\n        grad_a_V = -np.mean(w * D_fake_g * z_g)\n        grad_b_V = -np.mean(w * D_fake_g)\n\n        # Gradient descent update\n        a -= eta_G * grad_a_V\n        b -= eta_G * grad_b_V\n\n    # --- Evaluation ---\n    m_prime_1 = a * m1 + b\n    m_prime_2 = a * m2 + b\n    \n    data_means = [-mu, mu]\n    gen_means = [m_prime_1, m_prime_2]\n    coverage_score = 0\n    \n    covered_data_means = [False, False] # Corresponds to [-mu, mu]\n\n    for i, data_mean in enumerate(data_means):\n        min_dist = min(abs(gm - data_mean) for gm in gen_means)\n        if min_dist = tau:\n            if not covered_data_means[i]:\n                 coverage_score += 1\n                 covered_data_means[i] = True\n            \n    return coverage_score\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3185781"}, {"introduction": "在亲手实现GAN并观察到模式覆盖等难题后，我们自然会问：为什么GAN的训练如此不稳定？根本原因在于其独特的优化动态，即同时进行的梯度下降-上升更新并不像标准优化问题那样保证收敛。本练习 [@problem_id:3185794] 通过一个极简的双线性博弈（bilinear game）来模拟GAN训练的局部动态。你将通过编码验证，朴素的梯度更新方法可能导致振荡和发散，而像 extragradient 这样的方法则为实现稳定训练提供了解决思路。", "problem": "考虑生成对抗网络 (GAN) 的极小化极大博弈，该博弈通过求解 $\\min_{\\mathbf{x}} \\max_{\\mathbf{y}} V(\\mathbf{x},\\mathbf{y})$ 来寻求价值函数 $V(\\mathbf{x},\\mathbf{y})$ 的一个鞍点。为了研究 GAN 训练中通常出现的局部动态，请分析简化的二维双线性鞍点 $V(x,y) = x y$。这种双线性形式是对抗性学习中平衡点附近参数相互作用的规范局部模型。\n\n从核心定义开始：\n- 对于一个极小化极大问题 $\\min_x \\max_y V(x,y)$，梯度下降-上升 (GDA) 更新会计算一个同步步骤，该步骤相对于 $x$ 减小 $V$ 并相对于 $y$ 增大 $V$。\n- 外梯度法执行一个两阶段步骤：一个预测步骤，使用当前迭代点的梯度形成一个中间点；接着是一个校正步骤，使用在中间点评估的梯度来更新迭代点。\n\n你的任务是针对特定的双线性价值函数 $V(x,y) = x y$ 实现以下内容：\n- 由 $V$ 的梯度驱动的单步 GDA 更新，其中 $x$ 采取下降步骤，$y$ 采取上升步骤。\n- 一个单步外梯度更新，首先使用当前点的梯度执行一个预测步骤，然后使用预测点的梯度执行一个校正步骤。\n\n通过测量在这些单步映射下欧几里得范数如何变化，来经验性地展示收缩和扩张区域。对于一个非零点 $(x,y)$ 和步长 $\\eta  0$，将单步映射 $\\Phi$ 的范数比定义为\n$$\nr(x,y;\\eta) = \\frac{\\|\\Phi(x,y;\\eta)\\|_2}{\\|(x,y)\\|_2}.\n$$\n如果 $r(x,y;\\eta)  1$，区域被分类为收缩区；如果 $r(x,y;\\eta) = 1$，为中性区；如果 $r(x,y;\\eta)  1$，为扩张区。\n\n实现一个程序，该程序：\n- 使用 $V(x,y) = x y$ 的精确双线性梯度来形成所定义的更新。\n- 在一个固定的初始点测试集上评估平均范数比：\n  - 测试集点是列表 $[(1,0), (0,1), (1,1), (-1,1), (0.5,-0.7)]$。\n  - 对测试集中的每个点 $(x,y)$ 以及给定的方法和步长 $\\eta$，计算 $r(x,y;\\eta)$ 并将这些值平均，为该情况生成一个单一的浮点数结果。\n\n测试套件：\n- 情况 $1$ (理想情况，外梯度法收缩)：方法是外梯度法，步长 $\\eta = 0.5$。\n- 情况 $2$ (边界情况，外梯度法中性)：方法是外梯度法，步长 $\\eta = 1.0$。\n- 情况 $3$ (扩张情况，外梯度法)：方法是外梯度法，步长 $\\eta = 1.2$。\n- 情况 $4$ (与 GAN 朴素更新的比较，梯度下降-上升)：方法是梯度下降-上升，步长 $\\eta = 0.5$。\n- 情况 $5$ (小步长比较，梯度下降-上升)：方法是梯度下降-上升，步长 $\\eta = 0.1$。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，结果顺序与上面列出的测试用例顺序一致。每个条目必须是对应情况下作为浮点数的平均范数比（无单位）。例如，输出格式为 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是在给定方法和步长下，对指定测试集计算的 $r(x,y;\\eta)$ 的平均值。", "solution": "所提出的问题是分析两种常见优化算法——梯度下降-上升 (GDA) 和外梯度法 (EG)——在由价值函数 $V(x,y) = xy$ 给出的规范双线性鞍点问题上的局部动态。分析将通过检查迭代向量 $(x,y)$ 在单次更新步骤后欧几里得范数的变化来进行。\n\n这个问题在科学上和数学上都是合理的。它研究了优化文献中一个与生成对抗网络 (GANs) 训练相关的明确定义的问题，使用了标准算法和一个简化但规范的模型。所有参数和条件都已指定，使得问题成为一个适定问题，并允许一个唯一、可验证的解。\n\n极小化极大问题的核心是通过求解 $\\min_{\\mathbf{x}} \\max_{\\mathbf{y}} V(\\mathbf{x}, \\mathbf{y})$ 来找到价值函数 $V(\\mathbf{x}, \\mathbf{y})$ 的一个鞍点 $(\\mathbf{x}^*, \\mathbf{y}^*)$。这可以被表述为寻找梯度场 $F(\\mathbf{z}) = (\\nabla_{\\mathbf{x}}V, -\\nabla_{\\mathbf{y}}V)^T$ 的一个零点，其中 $\\mathbf{z} = (\\mathbf{x}, \\mathbf{y})^T$。对于特定的二维双线性问题，我们有 $V(x,y) = xy$。\n\n首先，我们计算 $V(x,y)$ 的偏梯度：\n$$\n\\nabla_x V(x,y) = \\frac{\\partial}{\\partial x}(xy) = y\n$$\n$$\n\\nabla_y V(x,y) = \\frac{\\partial}{\\partial y}(xy) = x\n$$\n设系统的状态由向量 $\\mathbf{z} = (x, y)^T$ 表示。GDA 更新对 $x$ 执行梯度下降步骤，对 $y$ 执行梯度上升步骤，而 EG 方法使用一个两步预测-校正机制。我们现在为每种方法推导单步更新映射 $\\Phi(\\mathbf{z};\\eta)$。\n\n**1. 梯度下降-上升 (GDA) 方法**\n\n对于迭代点 $\\mathbf{z}_k = (x_k, y_k)^T$ 和步长 $\\eta  0$，GDA 更新定义为：\n$$\nx_{k+1} = x_k - \\eta \\nabla_x V(x_k, y_k)\n$$\n$$\ny_{k+1} = y_k + \\eta \\nabla_y V(x_k, y_k)\n$$\n代入 $V(x,y) = xy$ 的梯度：\n$$\nx_{k+1} = x_k - \\eta y_k\n$$\n$$\ny_{k+1} = y_k + \\eta x_k\n$$\n这构成了单步 GDA 映射 $\\Phi_{GDA}(x_k, y_k; \\eta) = (x_{k+1}, y_{k+1})$。为了分析其对范数的影响，我们计算新迭代点 $\\mathbf{z}_{k+1}$ 的欧几里得范数的平方：\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = x_{k+1}^2 + y_{k+1}^2 = (x_k - \\eta y_k)^2 + (y_k + \\eta x_k)^2\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = (x_k^2 - 2\\eta x_k y_k + \\eta^2 y_k^2) + (y_k^2 + 2\\eta x_k y_k + \\eta^2 x_k^2)\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = x_k^2 + y_k^2 + \\eta^2 x_k^2 + \\eta^2 y_k^2 = (1 + \\eta^2)(x_k^2 + y_k^2) = (1 + \\eta^2)\\|\\mathbf{z}_k\\|_2^2\n$$\n因此，范数比 $r_{GDA}$ 为：\n$$\nr_{GDA}(x,y;\\eta) = \\frac{\\|\\mathbf{z}_{k+1}\\|_2}{\\|\\mathbf{z}_k\\|_2} = \\sqrt{1 + \\eta^2}\n$$\n这个比率与点 $(x,y)$ 无关，并且对于任何 $\\eta  0$ 都严格大于 1。这表明 GDA 会持续扩大迭代点的范数，导致在这个简单的双线性博弈上发散。\n\n**2. 外梯度 (EG) 方法**\n\nEG 方法执行一个两阶段更新。设当前迭代点为 $\\mathbf{z}_k = (x_k, y_k)^T$。\n\n**阶段 1：预测。** 使用一个 GDA 步骤计算一个中间点 $\\mathbf{z}_{k+1/2} = (x_{k+1/2}, y_{k+1/2})^T$：\n$$\nx_{k+1/2} = x_k - \\eta \\nabla_x V(x_k, y_k) = x_k - \\eta y_k\n$$\n$$\ny_{k+1/2} = y_k + \\eta \\nabla_y V(x_k, y_k) = y_k + \\eta x_k\n$$\n\n**阶段 2：校正。** 最终的更新是根据原始点 $\\mathbf{z}_k$ 并使用在中间点 $\\mathbf{z}_{k+1/2}$ 处评估的梯度来计算的：\n$$\nx_{k+1} = x_k - \\eta \\nabla_x V(x_{k+1/2}, y_{k+1/2})\n$$\n$$\ny_{k+1} = y_k + \\eta \\nabla_y V(x_{k+1/2}, y_{k+1/2})\n$$\n在中间点的梯度为 $\\nabla_x V(x_{k+1/2}, y_{k+1/2}) = y_{k+1/2}$ 和 $\\nabla_y V(x_{k+1/2}, y_{k+1/2}) = x_{k+1/2}$。代入这些梯度得到：\n$$\nx_{k+1} = x_k - \\eta (y_k + \\eta x_k) = (1 - \\eta^2)x_k - \\eta y_k\n$$\n$$\ny_{k+1} = y_k + \\eta (x_k - \\eta y_k) = \\eta x_k + (1 - \\eta^2)y_k\n$$\n这定义了单步 EG 映射 $\\Phi_{EG}(x_k, y_k; \\eta) = (x_{k+1}, y_{k+1})$。我们现在计算新迭代点 $\\mathbf{z}_{k+1}$ 的范数平方：\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = ((1 - \\eta^2)x_k - \\eta y_k)^2 + (\\eta x_k + (1 - \\eta^2)y_k)^2\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = ((1-\\eta^2)^2 x_k^2 - 2\\eta(1-\\eta^2)x_ky_k + \\eta^2 y_k^2) + (\\eta^2 x_k^2 + 2\\eta(1-\\eta^2)x_ky_k + (1-\\eta^2)^2 y_k^2)\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = ((1-\\eta^2)^2 + \\eta^2)x_k^2 + (\\eta^2 + (1-\\eta^2)^2)y_k^2 = ((1 - 2\\eta^2 + \\eta^4) + \\eta^2)(x_k^2 + y_k^2)\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = (1 - \\eta^2 + \\eta^4)(x_k^2 + y_k^2) = (1 - \\eta^2 + \\eta^4)\\|\\mathbf{z}_k\\|_2^2\n$$\n范数比 $r_{EG}$ 为：\n$$\nr_{EG}(x,y;\\eta) = \\frac{\\|\\mathbf{z}_{k+1}\\|_2}{\\|\\mathbf{z}_k\\|_2} = \\sqrt{1 - \\eta^2 + \\eta^4}\n$$\n与 GDA 类似，这个比率与点 $(x,y)$ 无关。然而，其值关键性地取决于步长 $\\eta$：\n- 如果 $0  \\eta  1$，则 $\\eta^4  \\eta^2$，所以 $1-\\eta^2+\\eta^4  1$。该方法是**收缩的**。\n- 如果 $\\eta = 1$，则 $1 - 1 + 1 = 1$。该方法是**中性的**。\n- 如果 $\\eta  1$，则 $\\eta^4  \\eta^2$，所以 $1-\\eta^2+\\eta^4  1$。该方法是**扩张的**。\n\n这一分析证实了问题描述中所述的区域。实现将为每种情况计算这些理论比率。由于对于所有非零点，该比率是恒定的，因此测试集上的平均比率就是比率本身。尽管如此，代码仍将按照要求执行显式的平均循环。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the one-step dynamics of GDA and Extragradient methods on the\n    bilinear saddle problem V(x,y) = xy. It computes the mean norm ratio\n    for several test cases.\n    \"\"\"\n\n    # The test set of initial points.\n    test_points = [\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n        (-1.0, 1.0),\n        (0.5, -0.7)\n    ]\n\n    # The test suite of methods and step sizes.\n    test_cases = [\n        {'method': 'extragradient', 'eta': 0.5},\n        {'method': 'extragradient', 'eta': 1.0},\n        {'method': 'extragradient', 'eta': 1.2},\n        {'method': 'gda', 'eta': 0.5},\n        {'method': 'gda', 'eta': 0.1},\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        method = case['method']\n        eta = case['eta']\n        \n        ratios_for_case = []\n        for x, y in test_points:\n            # Calculate the Euclidean norm of the initial point.\n            norm_initial = np.sqrt(x**2 + y**2)\n\n            # A non-zero point is assumed as per the problem statement.\n            # If norm is zero, the ratio is undefined, but this is handled by the test set.\n            if norm_initial == 0:\n                continue\n\n            x_next, y_next = 0.0, 0.0\n\n            if method == 'gda':\n                # Apply the one-step Gradient Descent-Ascent update.\n                # x_k+1 = x_k - eta * grad_x(V) = x_k - eta * y_k\n                # y_k+1 = y_k + eta * grad_y(V) = y_k + eta * x_k\n                x_next = x - eta * y\n                y_next = y + eta * x\n            \n            elif method == 'extragradient':\n                # Apply the one-step Extragradient update.\n                # This uses the pre-derived simplified update equations.\n                # x_k+1 = (1 - eta^2) * x_k - eta * y_k\n                # y_k+1 = eta * x_k + (1 - eta^2) * y_k\n                eta_sq = eta**2\n                x_next = (1 - eta_sq) * x - eta * y\n                y_next = eta * x + (1 - eta_sq) * y\n\n            # Calculate the Euclidean norm of the updated point.\n            norm_next = np.sqrt(x_next**2 + y_next**2)\n            \n            # Compute the norm ratio for this point.\n            ratio = norm_next / norm_initial\n            ratios_for_case.append(ratio)\n        \n        # Calculate the mean norm ratio for the current case.\n        # As derived, this ratio is constant for all non-zero points,\n        # so the mean is just the ratio itself.\n        mean_ratio = np.mean(ratios_for_case)\n        final_results.append(mean_ratio)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3185794"}]}