{"hands_on_practices": [{"introduction": "在构建任何复杂的深度学习模型之前，首先估算其对计算资源的需求是至关重要的一步。这个练习将引导你从根本上理解深度信念网络（DBN）的结构，并推导出其参数数量和训练计算复杂度的表达式。通过这种分析，你将具体地了解到模型深度、层宽度等超参数如何影响模型的规模和训练成本，这对于设计高效的网络架构至关重要。[@problem_id:3112338]", "problem": "考虑一个由 $L$ 个受限玻尔兹曼机（RBM）堆叠而成的深度置信网络（DBN），其中可见层的大小为 $n_{v}$，每个隐藏层的大小为 $n_{h}$。每个RBM都是一个二分图模型，其能量函数为 $E(v,h)=-v^{\\top}Wh-b^{\\top}v-c^{\\top}h$，其中 $v \\in \\{0,1\\}^{n_{v}}$ 或 $\\{0,1\\}^{n_{h}}$ 取决于层， $h \\in \\{0,1\\}^{n_{h}}$，$W$ 是连接两层的权重矩阵，$b$ 和 $c$ 分别是可见偏置和隐藏偏置。预训练是使用对比散度（CD）逐层进行的，具体来说，每层每个数据样本进行 $k$ 步对比散度（CD-$k$）。假设一个大小为 $N$ 个样本的数据集，并且每个 epoch 每层进行一次参数更新。\n\n从这些定义和RBM的二分结构出发，并且仅使用关于矩阵-向量乘法成本的公认事实，执行以下操作：\n\n1. 推导稠密DBN的可训练参数总数，记为 $P_{\\text{dense}}$，计算所有 $L$ 个RBM中的所有权重和偏置。\n2. 在以下科学合理的假设下，推导稠密DBN的CD-$k$预训练每轮（epoch）的浮点运算数的前导阶表达式，记为 $C_{\\text{dense}}$：\n   - 每个RBM中每个数据样本的每个CD步骤的主要计算包括两次矩阵-向量乘法（上行传播 $W^{\\top}v$ 和下行传播 $W h$），且成本与 $W$ 中非零元素的数量成线性关系。\n   - 非矩阵运算（加法、激活函数求值）贡献一个常数乘法因子，该因子可以被吸收到一个抽象常数中。\n   引入一个正常数 $\\alpha$，它捕捉了每个数据样本每个CD步骤中每条边的计算成本，并用 $n_{v}$、$n_{h}$、$L$、$N$、$k$ 和 $\\alpha$ 表示 $C_{\\text{dense}}$。\n3. 提出一种稀疏连接方案，其中在每个RBM中，每个潜在的可见-隐藏边以概率 $p \\in (0,1)$ 独立存在。在此模型下，推导可训练参数的期望总数 $P_{\\text{sparse}}$ 和每轮浮点运算数的期望值 $C_{\\text{sparse}}$。\n4. 最后，计算比率 $R=C_{\\text{sparse}}/C_{\\text{dense}}$ 的闭式表达式。\n\n你的最终答案必须是 $R$ 的单一符号表达式。不需要数值近似或四舍五入。", "solution": "我们从受限玻尔兹曼机（RBM）和深度置信网络（DBN）的结构定义开始。一个RBM是二分的：所有可见单元连接到所有隐藏单元，并且层内没有横向连接。能量函数 $E(v,h)=-v^{\\top}Wh-b^{\\top}v-c^{\\top}h$ 意味着参数集由权重矩阵 $W$ 和偏置向量 $b$ 和 $c$ 组成。$W$ 的大小由所连接层的大小决定。\n\n一个DBN预训练过程堆叠了 $L$ 个RBM：第一个RBM连接大小为 $n_{v}$ 的可见层和大小为 $n_{h}$ 的第一个隐藏层，随后的每个RBM连接一个大小为 $n_{h}$ 的隐藏层到下一个大小为 $n_{h}$ 的隐藏层。因此，总共有 $L$ 个RBM：一个维度为 $n_{v} \\times n_{h}$，以及 $L-1$ 个维度为 $n_{h} \\times n_{h}$。\n\n步骤 1：参数数量 $P_{\\text{dense}}$。\n\n对于第一个RBM：\n- 权重：$n_{v} \\times n_{h}$ 个条目。\n- 偏置：$n_{v}$ 个可见偏置和 $n_{h}$ 个隐藏偏置。\n\n对于剩下的 $L-1$ 个RBM中的每一个：\n- 权重：$n_{h} \\times n_{h}$ 个条目。\n- 偏置：每个RBM有 $n_{h}$ 个可见偏置（此时下方的隐藏层充当“可见”层）和 $n_{h}$ 个隐藏偏置。然而，在堆叠式RBM预训练设置中，每一层都有自己的偏置向量。在整个堆栈中，总偏置是每个RBM的所有可见层偏置和所有隐藏层偏置的总和。在各层之间进行唯一聚合，每层贡献一个偏置向量。有 $1$ 个可见层（大小为 $n_{v}$）和 $L$ 个隐藏层（每个大小为 $n_{h}$），因此总偏置数量为 $n_{v} + L n_{h}$。\n\n因此，在稠密连接下，参数总数为\n$$\nP_{\\text{dense}} = \\underbrace{n_{v} n_{h} + (L-1) n_{h}^{2}}_{\\text{权重}} + \\underbrace{n_{v} + L n_{h}}_{\\text{偏置}}.\n$$\n\n步骤 2：每轮复杂度 $C_{\\text{dense}}$。\n\n在使用大小为 $N$ 的数据集进行对比散度（CD-$k$）预训练时，每轮的主要计算是每个RBM中每个数据样本的每个CD步骤中的两次矩阵-向量乘积：上行传播 $W^{\\top} v$ 和下行传播 $W h$。每次矩阵-向量乘积的成本与 $W$ 中非零条目的数量成比例。令 $\\alpha>0$ 表示比例常数，它捕捉了每个数据样本每个CD步骤中每条边的累积成本，包括上行和下行传播。\n\n对于第一个RBM，边的数量等于权重的数量 $n_{v} n_{h}$。对于剩下的 $L-1$ 个RBM中的每一个，边的数量是 $n_{h}^{2}$。将整个堆栈的边相加得到 $n_{v} n_{h} + (L-1) n_{h}^{2}$。\n\n因此，稠密连接的每轮复杂度的前导阶为\n$$\nC_{\\text{dense}} = \\alpha \\, N \\, k \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n\n步骤 3：边概率为 $p$ 的稀疏连接。\n\n在所提出的稀疏连接方案中，RBM中每个潜在的可见-隐藏边以概率 $p \\in (0,1)$ 独立存在。边的期望数量（因此也是权重的期望数量）变为稠密计数的 $p$ 倍，因为独立伯努利指示变量之和的期望等于它们期望的总和。\n\n因此，整个堆栈的期望权重数量为\n$$\n\\mathbb{E}[\\text{权重}] = p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n偏置保持不变，因为它们在该方案中不被剪枝。因此，\n$$\nP_{\\text{sparse}} = p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right) + n_{v} + L n_{h}.\n$$\n\n对于复杂度，由于每轮的主要成本与边的数量成线性关系，因此每轮的期望成本为\n$$\nC_{\\text{sparse}} = \\alpha \\, N \\, k \\, p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n\n步骤 4：比率 $R=C_{\\text{sparse}}/C_{\\text{dense}}$。\n\n使用上述表达式，\n$$\nR = \\frac{C_{\\text{sparse}}}{C_{\\text{dense}}} = \\frac{\\alpha \\, N \\, k \\, p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right)}{\\alpha \\, N \\, k \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right)} = p.\n$$\n\n因此，在所提出的稀疏连接方案下，计算量缩减因子的闭式表达式为 $p$。", "answer": "$$\\boxed{p}$$", "id": "3112338"}, {"introduction": "深度信念网络不仅能识别模式，还能生成数据，这得益于其同时包含的“认知”通路（自下而上）和“生成”通路（自上而下）。这个练习将通过一个具体的编程任务，让你深入探究“权重绑定”这一关键设计概念，即生成权重是认知权重的转置。通过比较权重绑定与非绑定情况下的数据重构能力，你将亲身体会到这种对称性如何源于RBM的能量函数，并深刻影响网络的生成一致性。[@problem_id:3112369]", "problem": "考虑一个两层深度信念网络（DBN），其所有层均使用二元单元，该网络通过堆叠两个受限玻尔兹曼机（RBM）构建。每个RBM对可见单元和隐藏单元使用伯努利变量，并采用逻辑激活函数。其基本基础是RBM的能量模型：对于可见向量 $\\mathbf{v} \\in \\{0,1\\}^d$ 和隐藏向量 $\\mathbf{h} \\in \\{0,1\\}^m$，能量由下式给出\n$$\nE(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{c}^\\top \\mathbf{v} - \\mathbf{b}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h},\n$$\n由于其二分图结构，条件分布可以分解为：\n$$\np(h_j = 1 \\mid \\mathbf{v}) = \\sigma\\!\\left(b_j + \\sum_{i=1}^{d} W_{ij} v_i \\right),\\quad\np(v_i = 1 \\mid \\mathbf{h}) = \\sigma\\!\\left(c_i + \\sum_{j=1}^{m} W_{ij} h_j \\right),\n$$\n其中 $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ 是逻辑函数。\n\n在一个具有可见层 $\\mathbf{v} \\in \\{0,1\\}^{d}$、第一隐藏层 $\\mathbf{h}^{(1)} \\in \\{0,1\\}^{m}$ 和第二隐藏层 $\\mathbf{h}^{(2)} \\in \\{0,1\\}^{n}$ 的两层DBN中，考虑一个确定性的平均场上行过程（识别模型），随后是一个确定性的下行过程（生成模型）。上行过程使用识别权重 $W_{1r} \\in \\mathbb{R}^{d \\times m}$ 和 $W_{2r} \\in \\mathbb{R}^{m \\times n}$ 以及偏置 $\\mathbf{b}_1 \\in \\mathbb{R}^{m}$ 和 $\\mathbf{b}_2 \\in \\mathbb{R}^{n}$：\n$$\n\\mathbf{h}^{(1)} = \\sigma\\!\\left(\\mathbf{v}^\\top W_{1r} + \\mathbf{b}_1\\right), \\quad\n\\mathbf{h}^{(2)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(1)}\\right)^\\top W_{2r} + \\mathbf{b}_2\\right).\n$$\n下行过程使用生成权重 $W_{2g} \\in \\mathbb{R}^{n \\times m}$ 和 $W_{1g} \\in \\mathbb{R}^{m \\times d}$ 以及偏置 $\\mathbf{c}_1 \\in \\mathbb{R}^{m}$ 和 $\\mathbf{c}_0 \\in \\mathbb{R}^{d}$：\n$$\n\\tilde{\\mathbf{h}}^{(1)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(2)}\\right)^\\top W_{2g} + \\mathbf{c}_1\\right), \\quad\n\\tilde{\\mathbf{v}} = \\sigma\\!\\left(\\left(\\tilde{\\mathbf{h}}^{(1)}\\right)^\\top W_{1g} + \\mathbf{c}_0\\right).\n$$\n我们将数据集 $\\mathcal{D} = \\{\\mathbf{v}^{(k)}\\}_{k=1}^{K}$ 的生成一致性得分定义为原始可见向量与其经过上行-下行过程后的确定性重构 $\\tilde{\\mathbf{v}}^{(k)}$ 之间的平均逐分量二元交叉熵：\n$$\nJ = \\frac{1}{K d} \\sum_{k=1}^{K} \\sum_{i=1}^{d} \\left[ - v^{(k)}_i \\log\\!\\left(\\tilde{v}^{(k)}_i\\right) - \\left(1 - v^{(k)}_i\\right) \\log\\!\\left(1 - \\tilde{v}^{(k)}_i\\right) \\right].\n$$\n为确保数值稳定性，在计算对数之前，使用 $\\epsilon = 10^{-12}$ 并将每个 $\\tilde{v}^{(k)}_i$ 替换为 $\\min\\!\\left(\\max\\!\\left(\\tilde{v}^{(k)}_i, \\epsilon\\right), 1 - \\epsilon\\right)$。\n\n您的任务是编写一个程序，为四个测试用例计算 $J$。这些用例探讨上行和下行过程之间权重绑定与非绑定的情况，以衡量其对生成一致性的影响。使用以下固定的数据集，其中 $K = 3$ 且 $d = 4$：\n$$\n\\mathbf{v}^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n\\mathbf{v}^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix},\\quad\n\\mathbf{v}^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\n设 $m = 3$ 且 $n = 2$。对于所有情况，上行（识别）参数固定如下：\n$$\nW_{1r} = \\begin{bmatrix}\n0.8  -0.4  0.3 \\\\\n0.1  0.5  -0.6 \\\\\n0.7  -0.2  0.2 \\\\\n-0.5  0.3  0.4\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix},\n$$\n$$\nW_{2r} = \\begin{bmatrix}\n0.6  -0.3 \\\\\n0.4  0.2 \\\\\n-0.7  0.5\n\\end{bmatrix},\\quad\n\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}.\n$$\n评估以下四种关于下行（生成）参数的情况：\n\n情况1（权重绑定，中等幅值）：\n$$\nW_{2g} = W_{2r}^\\top = \\begin{bmatrix} 0.6  0.4  -0.7 \\\\ -0.3  0.2  0.5 \\end{bmatrix},\\quad\n\\mathbf{c}_1 = \\begin{bmatrix} -0.05 \\\\ 0.1 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{1g} = W_{1r}^\\top = \\begin{bmatrix}\n0.8  0.1  0.7  -0.5 \\\\\n-0.4  0.5  -0.2  0.3 \\\\\n0.3  -0.6  0.2  0.4\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\n情况2（权重非绑定，小扰动）：\n$$\nW_{2g} = W_{2r}^\\top + \\Delta_2,\\quad\n\\Delta_2 = \\begin{bmatrix} 0.01  -0.02  0.02 \\\\ -0.02  0.03  -0.01 \\end{bmatrix},\n$$\n$$\n\\mathbf{c}_1 = \\begin{bmatrix} -0.02 \\\\ 0.05 \\\\ -0.01 \\end{bmatrix},\\quad\nW_{1g} = W_{1r}^\\top + \\Delta_1,\n$$\n$$\n\\Delta_1 = \\begin{bmatrix}\n0.02  -0.01  0.03  -0.02 \\\\\n-0.03  0.02  0.01  -0.01 \\\\\n0.01  0.00  -0.02  0.03\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.02 \\\\ -0.01 \\\\ 0.0 \\\\ -0.02 \\end{bmatrix}.\n$$\n\n情况3（权重非绑定，严重不匹配）：\n$$\nW_{2g} = - W_{2r}^\\top + M_2,\\quad M_2 = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 0.2  -0.2  0.2 \\end{bmatrix},\n$$\n$$\n\\mathbf{c}_1 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix},\\quad\nW_{1g} = - W_{1r}^\\top + M_1,\n$$\n$$\nM_1 = \\begin{bmatrix}\n0.0  0.0  0.0  0.0 \\\\\n0.1  -0.1  0.1  -0.1 \\\\\n-0.2  0.2  -0.2  0.2\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} -0.3 \\\\ 0.3 \\\\ -0.3 \\\\ 0.3 \\end{bmatrix}.\n$$\n\n情况4（边界情况的权重绑定，全零）：\n$$\nW_{2g} = W_{2r}^\\top = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 0.0  0.0  0.0 \\end{bmatrix},\\quad\n\\mathbf{c}_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{1g} = W_{1r}^\\top = \\begin{bmatrix}\n0.0  0.0  0.0  0.0 \\\\\n0.0  0.0  0.0  0.0 \\\\\n0.0  0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n其上行（识别）参数设置为\n$$\nW_{1r} = \\begin{bmatrix}\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{2r} = \\begin{bmatrix}\n0.0  0.0 \\\\\n0.0  0.0 \\\\\n0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\n按照上述规定实现确定性的平均场上行和下行映射，在交叉熵计算中应用数值稳定器 $\\epsilon$，并为每种情况计算生成一致性得分 $J$。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$）。每个结果必须是浮点数。在最终输出中将每个结果四舍五入到6位小数。", "solution": "我们从受限玻尔兹曼机（RBM）的基本定义开始。对于伯努利可见单元 $\\mathbf{v} \\in \\{0,1\\}^d$ 和伯努利隐藏单元 $\\mathbf{h} \\in \\{0,1\\}^m$，RBM的能量函数为\n$$\nE(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{c}^\\top \\mathbf{v} - \\mathbf{b}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h}.\n$$\n由于RBM图是二分的，条件分布可以按单元分解。通过边缘化并使用玻尔兹曼分布 $p(\\mathbf{x}) \\propto \\exp(-E(\\mathbf{x}))$，可以得到S型条件概率：\n$$\np(h_j = 1 \\mid \\mathbf{v}) = \\sigma\\!\\left(b_j + \\sum_{i=1}^{d} W_{ij} v_i \\right), \\quad\np(v_i = 1 \\mid \\mathbf{h}) = \\sigma\\!\\left(c_i + \\sum_{j=1}^{m} W_{ij} h_j \\right),\n$$\n其中 $\\sigma(x) = \\frac{1}{1 + e^{-x}}$。这是因为对于一个伯努利单元，其对数几率是由相应偏置加上来自连接单元的加权和给出的仿射输入。\n\n深度信念网络（DBN）是堆叠的RBM。在一个两层DBN中，上行过程（识别模型）使用平均场期望（S型概率）将可见层确定性地映射到顶层隐藏层。使用识别权重 $W_{1r}$ 和 $W_{2r}$ 以及上行偏置 $\\mathbf{b}_1$ 和 $\\mathbf{b}_2$，上行变换为：\n$$\n\\mathbf{h}^{(1)} = \\sigma\\!\\left(\\mathbf{v}^\\top W_{1r} + \\mathbf{b}_1\\right),\n$$\n$$\n\\mathbf{h}^{(2)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(1)}\\right)^\\top W_{2r} + \\mathbf{b}_2\\right).\n$$\n下行过程（生成模型）通过生成权重 $W_{2g}$ 和 $W_{1g}$ 以及下行偏置 $\\mathbf{c}_1$ 和 $\\mathbf{c}_0$ 将顶层隐藏层映射回可见层：\n$$\n\\tilde{\\mathbf{h}}^{(1)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(2)}\\right)^\\top W_{2g} + \\mathbf{c}_1\\right),\n$$\n$$\n\\tilde{\\mathbf{v}} = \\sigma\\!\\left(\\left(\\tilde{\\mathbf{h}}^{(1)}\\right)^\\top W_{1g} + \\mathbf{c}_0\\right).\n$$\n权重绑定对应于 $W_{1g} = W_{1r}^\\top$ 和 $W_{2g} = W_{2r}^\\top$，这在识别映射和生成映射之间强制实现对称性。非绑定权重通过允许 $W_{1g}$ 和 $W_{2g}$ 偏离转置矩阵来打破这种对称性。\n\n为量化生成一致性，我们使用原始可见向量与其经过上行-下行过程后的重构之间的平均逐分量二元交叉熵：\n$$\nJ = \\frac{1}{K d} \\sum_{k=1}^{K} \\sum_{i=1}^{d} \\left[ - v^{(k)}_i \\log\\!\\left(\\tilde{v}^{(k)}_i\\right) - \\left(1 - v^{(k)}_i\\right) \\log\\!\\left(1 - \\tilde{v}^{(k)}_i\\right) \\right].\n$$\n该表达式源于目标 $v^{(k)}_i$ 的独立伯努利输出 $\\tilde{v}^{(k)}_i$ 的负对数似然，并在样本和维度上取平均。为了数值稳定性，在计算 $\\log(\\cdot)$ 之前，我们通过 $\\tilde{v}^{(k)}_i \\leftarrow \\min\\!\\left(\\max\\!\\left(\\tilde{v}^{(k)}_i, \\epsilon\\right), 1 - \\epsilon\\right)$（其中 $\\epsilon = 10^{-12}$）来截断概率，确保参数位于 $(0,1)$ 区间内。\n\n算法上，对于每种情况：\n$1.$ 对于每个数据向量 $\\mathbf{v}^{(k)}$，使用 $W_{1r}$ 和 $\\mathbf{b}_1$ 计算 $\\mathbf{h}^{(1)}$。\n$2.$ 使用 $W_{2r}$ 和 $\\mathbf{b}_2$ 计算 $\\mathbf{h}^{(2)}$。\n$3.$ 使用 $W_{2g}$ 和 $\\mathbf{c}_1$ 计算 $\\tilde{\\mathbf{h}}^{(1)}$。\n$4.$ 使用 $W_{1g}$ 和 $\\mathbf{c}_0$ 计算 $\\tilde{\\mathbf{v}}$。\n$5.$ 将 $\\tilde{\\mathbf{v}}$ 的每个分量截断到 $[\\epsilon, 1 - \\epsilon]$ 区间内。\n$6.$ 累加 $\\mathbf{v}^{(k)}$ 与 $\\tilde{\\mathbf{v}}$ 之间的交叉熵，然后在所有 $K$ 个样本上取平均并除以 $d$ 得到 $J$。\n\n从原理上讲，权重绑定可以提高一致性，因为上行映射近似于RBM条件下充分统计量的推断，而使用转置权重的下行映射则强制执行一个匹配的线性变换，回到可见空间。在平均场设置中，当权重适中且偏置平衡时，这倾向于减少 $\\mathbf{v}$ 和 $\\tilde{\\mathbf{v}}$ 之间的失真。非绑定权重中的小扰动引入了不对称性，会适度降低 $J$ 的表现。严重的不匹配会反转或扭曲映射，导致 $\\tilde{\\mathbf{v}}$ 反映一个不同的生成流形，从而大幅增加交叉熵。在所有权重（和偏置）均为零的边界情况下，所有单元的输出均为 $\\sigma(0) = 0.5$，这使得重构不含任何信息，并导致一个固定的交叉熵值，该值完全由伯努利目标相对于预测值0.5的熵决定。\n\n程序直接实现了这些步骤，对每种情况使用提供的矩阵和偏置，为固定数据集计算 $J$，并将四个结果以单个方括号括起的、逗号分隔的列表形式打印出来，每个值都四舍五入到6位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    # Numerically stable sigmoid using float64\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef clamp_probs(p, eps=1e-12):\n    # Clamp probabilities to (eps, 1-eps) for numerical stability in logs\n    return np.clip(p, eps, 1.0 - eps)\n\ndef generative_consistency_score(V, W1r, b1, W2r, b2, W2g, c1, W1g, c0, eps=1e-12):\n    \"\"\"\n    Compute mean component-wise binary cross-entropy between original visible vectors V\n    and reconstructions after upward-downward deterministic mean-field pass.\n    \"\"\"\n    K, d = V.shape\n    total_ce = 0.0\n    for k in range(K):\n        v = V[k]  # shape (d,)\n        # Upward pass: v -> h1 -> h2\n        h1_input = v @ W1r + b1  # shape (m,)\n        h1 = sigmoid(h1_input)\n        h2_input = h1 @ W2r + b2  # shape (n,)\n        h2 = sigmoid(h2_input)\n        # Downward pass: h2 -> h1_down -> v_hat\n        h1_down_input = h2 @ W2g + c1  # shape (m,)\n        h1_down = sigmoid(h1_down_input)\n        v_hat_input = h1_down @ W1g + c0  # shape (d,)\n        v_hat = sigmoid(v_hat_input)\n        v_hat = clamp_probs(v_hat, eps=eps)\n        # Binary cross-entropy per component\n        ce_vec = -(v * np.log(v_hat) + (1.0 - v) * np.log(1.0 - v_hat))\n        total_ce += np.sum(ce_vec)\n    # Mean across samples and dimensions\n    J = total_ce / (K * d)\n    return J\n\ndef solve():\n    # Fixed dataset V (K=3, d=4)\n    V = np.array([\n        [1.0, 0.0, 1.0, 0.0],\n        [0.0, 1.0, 0.0, 1.0],\n        [1.0, 1.0, 0.0, 0.0]\n    ], dtype=np.float64)\n\n    # Recognition (upward) parameters for cases 1-3\n    W1r_base = np.array([\n        [0.8, -0.4, 0.3],\n        [0.1,  0.5, -0.6],\n        [0.7, -0.2, 0.2],\n        [-0.5, 0.3, 0.4]\n    ], dtype=np.float64)\n    b1_base = np.array([0.1, -0.2, 0.05], dtype=np.float64)\n\n    W2r_base = np.array([\n        [0.6, -0.3],\n        [0.4,  0.2],\n        [-0.7, 0.5]\n    ], dtype=np.float64)\n    b2_base = np.array([0.0, 0.1], dtype=np.float64)\n\n    # Case 1: tied weights\n    W2g_1 = W2r_base.T.copy()\n    c1_1 = np.array([-0.05, 0.1, 0.0], dtype=np.float64)\n    W1g_1 = W1r_base.T.copy()\n    c0_1 = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float64)\n\n    # Case 2: untied (small perturbations)\n    Delta2 = np.array([\n        [0.01, -0.02, 0.02],\n        [-0.02, 0.03, -0.01]\n    ], dtype=np.float64)\n    W2g_2 = W2r_base.T + Delta2\n    c1_2 = np.array([-0.02, 0.05, -0.01], dtype=np.float64)\n    Delta1 = np.array([\n        [0.02, -0.01, 0.03, -0.02],\n        [-0.03,  0.02, 0.01, -0.01],\n        [0.01,  0.00, -0.02, 0.03]\n    ], dtype=np.float64)\n    W1g_2 = W1r_base.T + Delta1\n    c0_2 = np.array([0.02, -0.01, 0.0, -0.02], dtype=np.float64)\n\n    # Case 3: untied (severe mismatch)\n    M2 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.2, -0.2, 0.2]\n    ], dtype=np.float64)\n    W2g_3 = -W2r_base.T + M2\n    c1_3 = np.array([0.5, -0.5, 0.5], dtype=np.float64)\n    M1 = np.array([\n        [0.0,  0.0,  0.0,  0.0],\n        [0.1, -0.1,  0.1, -0.1],\n        [-0.2, 0.2, -0.2, 0.2]\n    ], dtype=np.float64)\n    W1g_3 = -W1r_base.T + M1\n    c0_3 = np.array([-0.3, 0.3, -0.3, 0.3], dtype=np.float64)\n\n    # Case 4: boundary all zeros (tied zeros)\n    W1r_4 = np.zeros((4, 3), dtype=np.float64)\n    b1_4 = np.zeros(3, dtype=np.float64)\n    W2r_4 = np.zeros((3, 2), dtype=np.float64)\n    b2_4 = np.zeros(2, dtype=np.float64)\n    W2g_4 = W2r_4.T.copy()  # zeros\n    c1_4 = np.zeros(3, dtype=np.float64)\n    W1g_4 = W1r_4.T.copy()  # zeros\n    c0_4 = np.zeros(4, dtype=np.float64)\n\n    test_cases = [\n        # Case 1: tied weights moderate\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_1, \"c1\": c1_1, \"W1g\": W1g_1, \"c0\": c0_1\n        },\n        # Case 2: untied small perturbations\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_2, \"c1\": c1_2, \"W1g\": W1g_2, \"c0\": c0_2\n        },\n        # Case 3: untied severe mismatch\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_3, \"c1\": c1_3, \"W1g\": W1g_3, \"c0\": c0_3\n        },\n        # Case 4: boundary all zeros (tied zeros)\n        {\n            \"W1r\": W1r_4, \"b1\": b1_4, \"W2r\": W2r_4, \"b2\": b2_4,\n            \"W2g\": W2g_4, \"c1\": c1_4, \"W1g\": W1g_4, \"c0\": c0_4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        J = generative_consistency_score(\n            V,\n            case[\"W1r\"], case[\"b1\"],\n            case[\"W2r\"], case[\"b2\"],\n            case[\"W2g\"], case[\"c1\"],\n            case[\"W1g\"], case[\"c0\"],\n            eps=1e-12\n        )\n        results.append(f\"{J:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3112369"}, {"introduction": "标准的深度信念网络通常采用全连接层，这在处理像图像这样的结构化数据时效率不高且难以扩展。这个高级练习将引导你将RBM的核心思想与卷积结构相结合，从第一性原理出发，推导并实现一个卷积受限玻尔兹曼机（CRBM）。通过这个实践，你将掌握如何将权重共享机制融入能量模型，并处理因感受野重叠带来的计算挑战，为理解现代卷积神经网络的基石打下坚实基础。[@problem_id:3112333]", "problem": "要求您为具有权重共享和重叠感受野的卷积受限玻尔兹曼机 (CRBM) 实现条件概率计算，并在一组确定性测试用例上验证您的实现，这些测试用例模拟了加拿大高等研究院 (CIFAR) 数据集风格的小图像块。您的推导和实现必须仅从受限玻尔兹曼机 (RBM) 作为一种具有二元单元的无向图模型的基本定义出发。假设以下基础：一个RBM由可见单元 $v$ 和隐藏单元 $h$ 上的联合分布定义，该分布由玻尔兹曼分布 $P(v,h) \\propto \\exp\\left(-E(v,h)\\right)$ 给出，其中 $E(v,h)$ 是一个在参数上线性、在 $v$ 和 $h$ 上双线性的能量函数，并且条件分布 $P(h \\mid v)$ 和 $P(v \\mid h)$ 可按单元分解。您必须根据这些基本定义进行推理，推导出在任意步幅和有效边界条件下，具有权重共享的卷积变体的条件采样方程，其中感受野可能重叠。\n\n任务要求：\n- 模型假设：二元可见单元 $v$ 和二元隐藏单元 $h$；由一组滤波器引起的跨空间位置的权重共享；无填充的有效卷积连接；步幅由高度和宽度的两个整数指定。当步幅小于滤波器尺寸时，会出现重叠感受野。\n- 从RBM的基础出发，为具有重叠感受野和共享权重的卷积架构推导条件激活概率 $P(h \\mid v)$ 和 $P(v \\mid h)$ 的表达式。您的推导必须基于基本定义，不依赖任何预先给出的卷积RBM公式。解释重叠感受野如何改变 $P(v \\mid h)$ 中每个可见单元的贡献。\n- 实现约束：实现两个计算条件激活概率（而非样本）的确定性函数：\n  1. 一个使用滤波器组、有效边界和给定步幅计算隐藏激活概率 $P(h=1 \\mid v)$ 的函数。\n  2. 一个计算可见激活概率 $P(v=1 \\mid h)$ 的函数，该函数能正确聚合来自所有重叠感受野的贡献，这等同于使用滤波器组对隐藏图进行带步幅的转置卷积，再加上可见偏置。\n- 确定性评估协议：为避免随机性，在为给定可见输入 $v$ 计算 $P(v=1 \\mid h)$ 时，使用单次平均场传递，其中隐藏二元状态被其条件期望所替代，即，在计算 $P(v=1 \\mid h)$ 时，令 $h$ 等于 $P(h=1 \\mid v)$。不允许进行随机采样。\n- 数值约定：对二元单元激活概率使用logistic sigmoid函数。所有计算都应以标准浮点算术进行。不涉及物理单位。\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个元素对应一个测试用例，并且本身必须是一个浮点数列表。对于每个测试用例，该列表由以下两部分连接而成：首先是按滤波器主序然后按行主序展平的隐藏激活概率 $P(h=1 \\mid v)$，其次是按通道主序然后按行主序展平的可见激活概率 $P(v=1 \\mid h)$。为清晰起见：滤波器主序指滤波器0的所有位置，然后是滤波器1的所有位置，依此类推；通道主序指通道0的所有位置，然后是通道1的所有位置，依此类推。\n\n测试套件规范：\n实现您的代码以精确处理以下三个测试用例。所有数组均已明确给出；下文中的所有整数和实数均为精确的问题常数。\n\n- 测试用例A（无重叠感受野）：\n  - 可见张量 $v \\in \\{0,1\\}^{C \\times H \\times W}$，其中 $C=1$, $H=4$, $W=4$:\n    $$\n    v[0] =\n    \\begin{bmatrix}\n    1  0  1  0 \\\\\n    0  1  0  1 \\\\\n    1  1  0  0 \\\\\n    0  0  1  1\n    \\end{bmatrix}\n    $$\n  - 滤波器组 $W \\in \\mathbb{R}^{F \\times C \\times K_H \\times K_W}$，其中 $F=1$, $C=1$, $K_H=2$, $K_W=2$:\n    $$\n    W[0,0] =\n    \\begin{bmatrix}\n    0.2  -0.1 \\\\\n    0.0  0.1\n    \\end{bmatrix}\n    $$\n  - 隐藏偏置 $b \\in \\mathbb{R}^{F}$: $b = [0.0]$.\n  - 可见偏置 $a \\in \\mathbb{R}^{C}$: $a = [0.0]$.\n  - 步幅 $(s_H,s_W) = (2,2)$.\n\n- 测试用例B（最大重叠感受野）：\n  - 可见张量 $v \\in \\{0,1\\}^{1 \\times 3 \\times 3}$:\n    $$\n    v[0] =\n    \\begin{bmatrix}\n    1  1  0 \\\\\n    0  1  0 \\\\\n    1  0  1\n    \\end{bmatrix}\n    $$\n  - 滤波器组 $W \\in \\mathbb{R}^{1 \\times 1 \\times 2 \\times 2}$:\n    $$\n    W[0,0] =\n    \\begin{bmatrix}\n    0.5  0.0 \\\\\n    0.0  -0.5\n    \\end{bmatrix}\n    $$\n  - 隐藏偏置 $b = [-0.5]$.\n  - 可见偏置 $a = [0.1]$.\n  - 步幅 $(1,1)$.\n\n- 测试用例C（多通道和多滤波器，重叠感受野）：\n  - 可见张量 $v \\in \\{0,1\\}^{3 \\times 4 \\times 4}$，通道由 $c \\in \\{0,1,2\\}$ 索引，每个通道为 $4 \\times 4$:\n    $$\n    v[0] =\n    \\begin{bmatrix}\n    1  0  1  0 \\\\\n    1  1  0  0 \\\\\n    0  1  1  0 \\\\\n    0  0  1  1\n    \\end{bmatrix},\\quad\n    v[1] =\n    \\begin{bmatrix}\n    0  1  0  1 \\\\\n    1  0  1  0 \\\\\n    0  1  0  1 \\\\\n    1  0  0  1\n    \\end{bmatrix},\\quad\n    v[2] =\n    \\begin{bmatrix}\n    1  1  0  0 \\\\\n    0  1  0  1 \\\\\n    1  0  1  0 \\\\\n    0  1  1  0\n    \\end{bmatrix}\n    $$\n  - 滤波器组 $W \\in \\mathbb{R}^{2 \\times 3 \\times 2 \\times 2}$，具有两个滤波器 $f \\in \\{0,1\\}$，三个通道 $c \\in \\{0,1,2\\}$，核尺寸为 $2 \\times 2$:\n    $$\n    $W[0,0]=\\begin{bmatrix}0.1  0.0\\\\0.0  0.1\\end{bmatrix},\\;$\n    $W[0,1]=\\begin{bmatrix}0.0  0.1\\\\-0.1  0.0\\end{bmatrix},\\;$\n    $W[0,2]=\\begin{bmatrix}0.05  -0.05\\\\0.05  -0.05\\end{bmatrix},$\n    $$\n    $$\n    $W[1,0]=\\begin{bmatrix}-0.1  0.1\\\\0.1  -0.1\\end{bmatrix},\\;$\n    $W[1,1]=\\begin{bmatrix}0.05  0.05\\\\0.05  0.05\\end{bmatrix},\\;$\n    $W[1,2]=\\begin{bmatrix}0.0  0.1\\\\0.0  -0.1\\end{bmatrix}.$\n    $$\n  - 隐藏偏置 $b = [0.2, -0.1]$.\n  - 可见偏置 $a = [0.05, -0.02, 0.03]$.\n  - 步幅 $(1,1)$.\n\n需遵守的实现细节：\n- 使用有效边界，因此对于可见层高度 $H$、宽度 $W$、核高度 $K_H$、核宽度 $K_W$ 以及步幅 $(s_H,s_W)$，隐藏图的空间尺寸为 $H_h \\times W_h$，其中 $H_h = \\left\\lfloor \\frac{H-K_H}{s_H} \\right\\rfloor + 1$ 且 $W_h = \\left\\lfloor \\frac{W-K_W}{s_W} \\right\\rfloor + 1$。\n- 在有效连接和步幅下形成 $P(v=1 \\mid h)$ 时，可见层的空间尺寸必须为 $H_v \\times W_v$，其中 $H_v = (H_h - 1)\\cdot s_H + K_H$ 且 $W_v = (W_h - 1)\\cdot s_W + K_W$，并且每个可见单元的预激活值必须包括所有覆盖该可见单元的隐藏单元的贡献总和。这对应于使用相同滤波器权重进行带步幅的转置卷积，并加上可见偏置。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个元素对应一个测试用例，并且必须是一个浮点数列表，该列表等于按滤波器主序然后按行主序展平的隐藏概率和按通道主序然后按行主序展平的可见概率的串联，顺序必须如此。不应打印任何其他文本。", "solution": "该问题要求推导并实现具有二元单元的卷积受限玻尔兹曼机 (CRBM) 的条件概率计算。推导必须从标准受限玻尔兹曼机 (RBM) 的基本原理开始。\n\n一个标准的RBM是一个无向图模型，包含一层可见单元 $v$ 和一层隐藏单元 $h$。$(v, h)$ 上的联合概率分布由玻尔兹曼分布给出：\n$$P(v, h) = \\frac{1}{Z} e^{-E(v,h)}$$\n其中 $Z$ 是配分函数，$E(v, h)$ 是能量函数。对于二元单元 $v_i \\in \\{0, 1\\}$ 和 $h_j \\in \\{0, 1\\}$，一个常见的能量函数是：\n$$E(v, h) = - \\sum_{i,j} W_{ij} v_i h_j - \\sum_i a_i v_i - \\sum_j b_j h_j = -v^T W h - a^T v - b^T h$$\n在这里，$W$ 是权重矩阵，而 $a$ 和 $b$ 分别是可见偏置向量和隐藏偏置向量。RBM的一个关键特性是，由于图的二分结构（没有层内连接），条件分布可以分解：\n$$P(h \\mid v) = \\prod_j P(h_j \\mid v) \\quad \\text{和} \\quad P(v \\mid h) = \\prod_i P(v_i \\mid h)$$\n单个隐藏单元 $h_j$ 的条件激活概率是从能量函数中推导出来的。$E(v,h)$ 中涉及 $h_j$ 的项是 $-(\\sum_i v_i W_{ij} + b_j)h_j$。这使得条件概率为其输入的 logistic sigmoid 函数：\n$$P(h_j=1 \\mid v) = \\sigma\\left(\\sum_i W_{ij} v_i + b_j\\right)$$\n其中 $\\sigma(x) = 1/(1+e^{-x})$。类似地，对于单个可见单元 $v_i$：\n$$P(v_i=1 \\mid h) = \\sigma\\left(\\sum_j W_{ij} h_j + a_i\\right)$$\n\n现在我们将此公式扩展到卷积RBM (CRBM)。可见单元和隐藏单元被排列成张量，权重在空间位置上共享。\n- 可见层 $v$：一个大小为 $C \\times H \\times W$ 的张量，具有 $C$ 个通道和空间维度 $H \\times W$。\n- 隐藏层 $h$：一个大小为 $F \\times H_h \\times W_h$ 的张量，具有 $F$ 个特征图（滤波器）和空间维度 $H_h \\times W_h$。\n- 滤波器组 $W$：一个大小为 $F \\times C \\times K_H \\times K_W$ 的张量，表示 $F$ 个滤波器，每个滤波器的大小为 $C \\times K_H \\times K_W$。\n- 隐藏偏置 $b$：一个大小为 $F$ 的向量，每个特征图有一个偏置 $b_f$。\n- 可见偏置 $a$：一个大小为 $C$ 的向量，每个可见通道有一个偏置 $a_c$。\n\nCRBM的能量函数是隐藏单元与其在可见层中的感受野之间相互作用的总和，外加偏置项：\n$$E(v, h) = -\\sum_{f=0}^{F-1} \\sum_{i=0}^{H_h-1} \\sum_{j=0}^{W_h-1} h_{f,i,j} \\left( \\sum_{c=0}^{C-1} \\sum_{p=0}^{K_H-1} \\sum_{q=0}^{K_W-1} W_{f,c,p,q} v_{c, i \\cdot s_H+p, j \\cdot s_W+q} \\right) - \\sum_{f,i,j} b_f h_{f,i,j} - \\sum_{c,k,l} a_c v_{c,k,l}$$\n括号中的项是步幅为 $(s_H, s_W)$ 的有效互相关操作。我们将其表示为 $(W_f \\star v)_{i,j}$。能量函数简化为：\n$$E(v, h) = - \\sum_{f,i,j} h_{f,i,j} \\left( (W_f \\star v)_{i,j} + b_f \\right) - \\sum_{c,k,l} a_c v_{c,k,l}$$\n\n**$P(h \\mid v)$ 的推导（从可见层到隐藏层）**\n与标准RBM一样，在给定可见层的情况下，隐藏单元是条件独立的。我们考虑单个隐藏单元 $h_{f,i,j}$。能量函数中依赖于此特定单元的项是：\n$$-h_{f,i,j} \\left( (W_f \\star v)_{i,j} + b_f \\right)$$\n因此，条件激活概率是 $-h_{f,i,j}$ 系数的 sigmoid 函数：\n$$P(h_{f,i,j}=1 \\mid v) = \\sigma\\left( (W_f \\star v)_{i,j} + b_f \\right)$$\n这是一个标准的前馈卷积操作。对于每个滤波器 $f$，我们将其与可见张量 $v$ 在所有通道上进行卷积，将相应的标量偏置 $b_f$ 加到所得特征图的所有空间位置上，然后应用 sigmoid 激活函数。隐藏图的维度由有效卷积公式给出：$H_h = \\left\\lfloor \\frac{H-K_H}{s_H} \\right\\rfloor + 1$ 且 $W_h = \\left\\lfloor \\frac{W-K_W}{s_W} \\right\\rfloor + 1$。\n\n**$P(v \\mid h)$ 的推导（从隐藏层到可见层）**\n类似地，在给定隐藏层的情况下，可见单元是条件独立的。为了找到单个可见单元 $v_{c,k,l}$ 的激活概率，我们必须将能量函数中所有出现该单元的项相加。\n$$E(v, h) = -v_{c,k,l} \\left( \\sum_{\\substack{f,i,j \\text{ s.t. } \\\\ (k,l) \\in \\text{RF}(i,j)}} W_{f,c,k-i s_H, l-j s_W} h_{f,i,j} \\right) - a_c v_{c,k,l} + \\dots$$\n其中 $\\text{RF}(i,j)$ 表示隐藏单元 $h_{f,i,j}$ 的感受野。如果 $i s_H \\le k  i s_H + K_H$ 且 $j s_W \\le l  j s_W + K_W$，则可见单元 $v_{c,k,l}$ 位于 $h_{f,i,j}$ 的感受野内。\n条件激活概率是 $-v_{c,k,l}$ 系数的 sigmoid 函数：\n$$P(v_{c,k,l}=1 \\mid h) = \\sigma\\left( \\sum_{f=0}^{F-1} \\sum_{i=0}^{H_h-1} \\sum_{j=0}^{W_h-1} \\mathbb{I}((k,l) \\in \\text{RF}(i,j)) \\cdot h_{f,i,j} \\cdot W_{f,c,k-i s_H, l-j s_W} + a_c \\right)$$\n其中 $\\mathbb{I}(\\cdot)$ 是一个指示函数。此操作将所有感受野覆盖可见单元 $v_{c,k,l}$ 的隐藏单元的贡献相加。这正是带步幅的转置卷积（有时称为反卷积）的定义。对于每个滤波器图 $h_f$，我们使用相应的滤波器权重 $W_{f,c}$ 进行转置卷积，以生成对可见通道 $c$ 的预激活值的贡献。通道 $c$ 的最终预激活值是来自所有滤波器图的贡献之和，再加上可见偏置 $a_c$。只要前向传递使用了有效卷积，重建的可见层的空间尺寸将与原始输入尺寸相匹配：$H_v = (H_h-1)s_H + K_H, W_v = (W_h-1)s_W + K_W$。\n\n问题指定了一个确定性评估协议。为了计算 $P(v=1 \\mid h)$，二元隐藏状态 $h_{f,i,j}$ 被它们的条件期望所替代，即在第一步中计算出的概率 $P(h_{f,i,j}=1 \\mid v)$。这是一个单次的平均场更新步骤。\n\n实现将包括两个函数：\n1.  `v_to_h`：通过带步幅的卷积计算 $P(h=1 \\mid v)$。\n2.  `h_to_v`：通过带步幅的转置卷积计算 $P(v=1 \\mid h)$，使用 $h = P(h=1 \\mid v)$ 作为输入。\n\n这些函数将用于处理给定的测试用例。最终输出是展平的隐藏和可见概率张量的串联，并按指定格式化。", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef v_to_h(v, W, b, stride):\n    \"\"\"\n    Computes hidden activation probabilities P(h=1|v) for a CRBM.\n    This corresponds to a standard convolution operation.\n    \"\"\"\n    C, H, W_dim = v.shape\n    F, C_W, K_H, K_W = W.shape\n    s_H, s_W = stride\n\n    if C != C_W:\n        raise ValueError(\"Channel dimension mismatch between visible tensor and filter bank.\")\n\n    H_h = (H - K_H) // s_H + 1\n    W_h = (W_dim - K_W) // s_W + 1\n\n    h_probs = np.zeros((F, H_h, W_h))\n\n    for f in range(F):\n        h_pre_activation = np.zeros((H_h, W_h))\n        for i in range(H_h):\n            for j in range(W_h):\n                v_patch = v[:, i * s_H : i * s_H + K_H, j * s_W : j * s_W + K_W]\n                # Convolution is sum of element-wise product\n                h_pre_activation[i, j] = np.sum(v_patch * W[f])\n        \n        # Add bias and apply sigmoid\n        h_probs[f] = sigmoid(h_pre_activation + b[f])\n        \n    return h_probs\n\ndef h_to_v(h, W, a, stride):\n    \"\"\"\n    Computes visible activation probabilities P(v=1|h) for a CRBM.\n    This corresponds to a strided transposed convolution operation.\n    \"\"\"\n    F, H_h, W_h = h.shape\n    F_W, C, K_H, K_W = W.shape\n    s_H, s_W = stride\n\n    if F != F_W:\n        raise ValueError(\"Filter dimension mismatch between hidden tensor and filter bank.\")\n\n    H_v = (H_h - 1) * s_H + K_H\n    W_v = (W_h - 1) * s_W + K_W\n\n    v_pre_activation = np.zeros((C, H_v, W_v))\n\n    for f in range(F):\n        for i in range(H_h):\n            for j in range(W_h):\n                h_val = h[f, i, j]\n                # Add contribution to the corresponding patch in the visible pre-activation map\n                v_pre_activation[:, i * s_H : i * s_H + K_H, j * s_W : j * s_W + K_W] += h_val * W[f]\n\n    # Add visible biases, broadcasting over spatial dimensions\n    v_pre_activation += a[:, np.newaxis, np.newaxis]\n\n    # Apply sigmoid\n    v_probs = sigmoid(v_pre_activation)\n    \n    return v_probs\n\ndef solve():\n    \"\"\"\n    Main function to solve the specified test cases.\n    \"\"\"\n    test_cases = [\n        # Test Case A\n        {\n            \"v\": np.array([[[1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 0, 0], [0, 0, 1, 1]]], dtype=float),\n            \"W\": np.array([[[[0.2, -0.1], [0.0, 0.1]]]], dtype=float),\n            \"b\": np.array([0.0], dtype=float),\n            \"a\": np.array([0.0], dtype=float),\n            \"stride\": (2, 2)\n        },\n        # Test Case B\n        {\n            \"v\": np.array([[[1, 1, 0], [0, 1, 0], [1, 0, 1]]], dtype=float),\n            \"W\": np.array([[[[0.5, 0.0], [0.0, -0.5]]]], dtype=float),\n            \"b\": np.array([-0.5], dtype=float),\n            \"a\": np.array([0.1], dtype=float),\n            \"stride\": (1, 1)\n        },\n        # Test Case C\n        {\n            \"v\": np.array([\n                [[1, 0, 1, 0], [1, 1, 0, 0], [0, 1, 1, 0], [0, 0, 1, 1]],\n                [[0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 0, 1]],\n                [[1, 1, 0, 0], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 1, 0]]\n            ], dtype=float),\n            \"W\": np.array([\n                [[[0.1, 0.0], [0.0, 0.1]], [[0.0, 0.1], [-0.1, 0.0]], [[0.05, -0.05], [0.05, -0.05]]],\n                [[[-0.1, 0.1], [0.1, -0.1]], [[0.05, 0.05], [0.05, 0.05]], [[0.0, 0.1], [0.0, -0.1]]]\n            ], dtype=float),\n            \"b\": np.array([0.2, -0.1], dtype=float),\n            \"a\": np.array([0.05, -0.02, 0.03], dtype=float),\n            \"stride\": (1, 1)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        v = case[\"v\"]\n        W = case[\"W\"]\n        b = case[\"b\"]\n        a = case[\"a\"]\n        stride = case[\"stride\"]\n\n        # 1. Compute P(h=1|v)\n        h_probs = v_to_h(v, W, b, stride)\n\n        # 2. Compute P(v=1|h) using mean-field approximation for h\n        v_probs = h_to_v(h_probs, W, a, stride)\n\n        # 3. Flatten results as specified\n        # h_probs layout (F, Hh, Wh) - flatten filter-major, then row-major\n        # v_probs layout (C, Hv, Wv) - flatten channel-major, then row-major\n        # np.flatten() uses row-major (C-style) order by default, which is what is needed.\n        h_flat = h_probs.flatten()\n        v_flat = v_probs.flatten()\n\n        # Concatenate and append\n        full_result = np.concatenate((h_flat, v_flat)).tolist()\n        results.append(full_result)\n    \n    # Format the final output string\n    # str(a_list) gives \"[item1, item2, ...]\"\n    # We join these strings with commas, then enclose in brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3112333"}]}