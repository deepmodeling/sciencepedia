{"hands_on_practices": [{"introduction": "此实践将探讨深度卷积生成对抗网络 (DCGAN) 的基本架构属性。我们将通过计算感受野和特征图尺寸等关键指标，分析一个固定的网络架构如何响应不同分辨率的输入，这在课程学习 (curriculum learning) 等训练策略中十分常见。本次练习旨在巩固对卷积运算的理解，并揭示其对设计可扩展生成模型的重要影响。[@problem_id:3112718]", "problem": "您被要求形式化并分析一个深度卷积生成对抗网络 (DCGAN) 的课程学习协议，其中训练从 $16\\times 16$ 大小的低分辨率图像开始，并逐步增加到 $64\\times 64$，而不改变网络架构。您的任务是推导、实现并计算形状和感受野属性，以证明判别器和生成器是否能仅凭卷积运算处理这种尺寸缩放，假设采用全卷积的 DCGAN 设计。\n\n请使用以下背景和约束条件，您必须纯粹从数学和算法的角度来解释它们，不假设能够访问任何深度学习框架：\n\n- 模型遵循深度卷积生成对抗网络 (DCGAN) 模式，解释为全卷积网络（无线性层），其中判别器使用一堆带步长的卷积和一个最终的全局空间平均来产生一个标量，生成器使用一堆转置卷积从一个小的潜在网格上采样到一张图像。课程学习在目标分辨率 $H \\in \\{16, 32, 64\\}$（均为方形图像）上进行，并且在各个阶段架构必须保持不变。\n- 在每个判别器块中，有一个在两个空间维度上核大小为 $k$、步长为 $s$、填充为 $p$ 的卷积。经过一次此类卷积后的输出高度（和宽度）将使用离散卷积输出尺寸规则进行计算。对于一个包含 $n_d$ 个此类块的堆栈，迭代应用该规则。\n- 给定层中单个输出激活相对于输入的感受野是仅使用核大小和步长递归定义的。您必须计算最后一个判别器块之后的感受野。填充不改变感受野大小。\n- 假设生成器产生一个固定的“原生”分辨率 $H_{\\text{native}}$，该分辨率由一个大小为 $H_0$ 的基础潜在网格和 $n_g$ 个转置卷积块决定，每个块的步长为 2，可将空间尺寸加倍。对于目标分辨率为 $H_{\\text{target}} \\in \\{16, 32, 64\\}$ 的课程学习阶段，如果比率 $H_{\\text{native}}/H_{\\text{target}}$ 是 2 的整数次幂，我们认为生成器是“无需架构更改”即兼容的（这样，在训练期间就可以使用一个固定的、以 2 的幂为因子的外部降采样器，而无需修改网络）。\n- 您必须只使用标准的离散卷积输出尺寸规则和用于标准卷积（即膨胀率为1）的标准感受野递归（不使用大于1的膨胀率）。不允许使用任何其他捷径。\n\n对于下面定义的每个测试用例，以及对于每个目标分辨率 $H \\in \\{16, 32, 64\\}$，您的程序必须计算并报告：\n\n1) 经过 $n_d$ 个块后，判别器的最终特征图空间尺寸 $H_{\\text{feat}} \\times H_{\\text{feat}}$。\n2) 判别器最后一层的感受野 $R$（根据对称性，高度等于宽度）以及比率 $R/H$（以浮点数表示）。\n3) 一个布尔值，指示单个最后一层激活的感受野是否覆盖整个输入，即 $R \\ge H$ 是否成立。\n4) 一个布尔值，指示生成器是否在无需架构更改的情况下与课程分辨率兼容，即 $H_{\\text{native}} / H$ 是否为 2 的整数次幂的整数。\n5) 在最终平均化之前，对判别器的全局决策有贡献的空间位置数量， $N_{\\text{pos}} = H_{\\text{feat}}^2$。\n\n您的程序必须将给定测试用例的所有目标分辨率的结果汇总到一个列表中：\n[ [H_feat(16),H_feat(32),H_feat(64)],\n  [R_over_H(16),R_over_H(32),R_over_H(64)],\n  [D_global_coverage(16),D_global_coverage(32),D_global_coverage(64)],\n  [G_compatible(16),G_compatible(32),G_compatible(64)],\n  [N_pos(16),N_pos(32),N_pos(64)] ]\n\n最后，将三个测试用例的结果汇总到一个顶层列表中：\n[case1_results,case2_results,case3_results]\n\n所有数值计算必须严格按照给定的规则进行，任何浮点输出都应保留为标准十进制形式。如果您选择四舍五入，请将比率 $R/H$ 四舍五入到 6 位小数。不涉及任何物理单位。\n\n测试套件：\n\n- 用例 $1$：$n_g = 4$, $n_d = 4$, $k = 4$, $s = 2$, $p = 1$, $H_0 = 4$, $H \\in \\{16,32,64\\}$。\n- 用例 $2$：$n_g = 4$, $n_d = 3$, $k = 4$, $s = 2$, $p = 1$, $H_0 = 4$, $H \\in \\{16,32,64\\}$。\n- 用例 $3$：$n_g = 4$, $n_d = 4$, $k = 3$, $s = 2$, $p = 1$, $H_0 = 4$, $H \\in \\{16,32,64\\}$。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，格式与此完全一致：[case1_results,case2_results,case3_results]。", "solution": "该问题要求对深度卷积生成对抗网络 (DCGAN) 架构与课程学习协议的兼容性进行形式化分析。该协议涉及在分辨率逐渐增加的图像（具体为 $H \\in \\{16, 32, 64\\}$）上进行训练，同时保持生成器和判别器的网络架构固定。分析将纯粹通过卷积层和转置卷积层的数学属性进行，不涉及经验性训练。我们必须为三种特定的架构配置（测试用例）计算判别器和生成器的几个关键指标。\n\n### 基于原理的设计与推导\n\n该分析依赖于卷积运算和感受野计算的基本原理。\n\n**1. 判别器模型和特征图尺寸计算**\n\n判别器是一个全卷积网络，由 $n_d$ 个相同的卷积块堆叠而成。每个块应用一个核大小为 $k$、步长为 $s$、填充为 $p$ 的卷积。问题指定了从输入维度 $H_{in}$ 计算输出空间维度 $H_{out}$ 的公式：\n$$H_{out} = \\left\\lfloor \\frac{H_{in} - k + 2p}{s} \\right\\rfloor + 1$$\n对于一个有 $n_d$ 个块的判别器，此公式被迭代应用。从大小为 $H \\times H$ 的输入图像开始，我们将第 $i$ 个块后的特征图大小表示为 $H_i \\times H_i$。尺寸序列由以下公式给出：\n$$H_i = \\left\\lfloor \\frac{H_{i-1} - k + 2p}{s} \\right\\rfloor + 1 \\quad \\text{for } i = 1, \\dots, n_d$$\n其中 $H_0 = H$ 是输入图像尺寸。最终的特征图尺寸，我们表示为 $H_{\\text{feat}}$，即为 $H_{n_d}$。然后，对最终的全局平均池化有贡献的空间位置数量为 $N_{\\text{pos}} = H_{\\text{feat}}^2$。\n\n**2. 判别器感受野**\n\n最后一层神经元的感受野 ($R$) 是输入图像中影响其激活的区域的大小。问题递归地定义了其计算方法。对于一个卷积堆栈，第 $i$ 层之后的感受野 $R_i$ 是根据前一层之后的感受野 $R_{i-1}$、当前层的核大小 $k_i$ 以及到前一层为止的累积步长乘积 $J_{i-1}$ 计算的。\n递归关系如下：\n$$R_i = R_{i-1} + (k_i - 1) \\cdot J_{i-1}$$\n$$J_i = J_{i-1} \\cdot s_i$$\n其基础情况为 $R_0 = 1$（输入层的一个像素的感受野大小为 1）和 $J_0 = 1$。由于所有判别器块都是相同的，因此对于所有的 $i$，我们有 $k_i = k$ 和 $s_i = s$。对 $i = 1, \\dots, n_d$ 执行计算，以找到最终的感受野 $R = R_{n_d}$。一个关键指标是感受野是否覆盖整个输入图像，即 $R \\ge H$ 是否成立。\n\n**3. 生成器模型与兼容性**\n\n生成器是一个全卷积网络，它使用 $n_g$ 个转置卷积块将一个大小为 $H_0 \\times H_0$ 的潜在网格上采样为一张全尺寸图像。每个块被指定为具有步长 $s_g=2$ 并将空间维度加倍。生成器的“原生”分辨率 $H_{\\text{native}}$ 是经过所有 $n_g$ 个块后的最终输出尺寸。它可以计算如下：\n$$H_{\\text{native}} = H_0 \\cdot (s_g)^{n_g} = H_0 \\cdot 2^{n_g}$$\n为了使生成器被认为与目标课程分辨率 $H$ “无需架构更改”即兼容，问题陈述了比率 $H_{\\text{native}} / H$ 必须是一个同时也是 2 的幂的整数。这意味着：\n$$\\frac{H_{\\text{native}}}{H} = 2^m \\quad \\text{for some integer } m \\ge 0$$\n这个条件确保了一个简单的、固定的降采样器（例如，使用 2 的幂次步长的平均池化）可以弥合生成器的固定输出分辨率与课程学习早期阶段所需的较小目标分辨率之间的差距。\n\n### 分析步骤\n\n对于每个由一组参数 $\\{n_g, n_d, k, s, p, H_0\\}$ 定义的测试用例，我们执行以下计算。\n\n首先，我们计算对于给定架构而言，无论输入分辨率 $H$ 如何，都保持不变的属性：\n- 生成器的原生分辨率，$H_{\\text{native}}$。\n- 判别器的最终感受野，$R$。\n\n接下来，对于每个目标分辨率 $H \\in \\{16, 32, 64\\}$，我们计算五个所需的指标：\n1.  **$H_{\\text{feat}}$**：从 $H_0 = H$ 开始，迭代应用卷积输出尺寸公式 $n_d$ 次。\n2.  **$R/H$**：计算感受野与输入尺寸的比率。该值将四舍五入到 6 位小数。\n3.  **$D_{\\text{global\\_coverage}}$**：评估布尔条件 $R \\ge H$。\n4.  **$G_{\\text{compatible}}$**：评估布尔条件，即 $H_{\\text{native}} / H$ 是一个整数并且是 2 的幂。\n5.  **$N_{\\text{pos}}$**：计算最终特征图中的位置数量，$H_{\\text{feat}}^2$。\n\n每个测试用例的结果被汇总到一个列表的列表中，然后这些列表被收集到一个顶层列表中作为最终输出。下面的 Python 实现执行此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes shape and receptive-field properties for a DCGAN under a curriculum\n    training protocol.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n_g, n_d, k, s, p, H_0)\n        # Case 1\n        (4, 4, 4, 2, 1, 4),\n        # Case 2\n        (4, 3, 4, 2, 1, 4),\n        # Case 3\n        (4, 4, 3, 2, 1, 4),\n    ]\n\n    target_resolutions = [16, 32, 64]\n\n    def calculate_h_feat(h_in, n_d, k, s, p):\n        \"\"\"Calculates the discriminator's final feature map size.\"\"\"\n        h_current = h_in\n        for _ in range(n_d):\n            h_current = np.floor((h_current - k + 2 * p) / s) + 1\n        return int(h_current)\n\n    def calculate_receptive_field(n_d, k, s):\n        \"\"\"Calculates the discriminator's final receptive field size.\"\"\"\n        r = 1  # R_0\n        j = 1  # J_0\n        for _ in range(n_d):\n            r = r + (k - 1) * j\n            j = j * s\n        return r\n\n    def calculate_h_native(h_0, n_g):\n        \"\"\"Calculates the generator's native output resolution.\"\"\"\n        # Each block has stride 2 and doubles the size.\n        return h_0 * (2 ** n_g)\n\n    def is_power_of_two(n):\n        \"\"\"Checks if a positive integer is a power of two.\"\"\"\n        if n = 0:\n            return False\n        return (n  (n - 1)) == 0\n\n    all_cases_results = []\n    for case in test_cases:\n        n_g, n_d, k, s, p, h_0 = case\n\n        h_feat_list = []\n        r_over_h_list = []\n        d_global_coverage_list = []\n        g_compatible_list = []\n        n_pos_list = []\n        \n        # Calculate case-constant values\n        r = calculate_receptive_field(n_d, k, s)\n        h_native = calculate_h_native(h_0, n_g)\n\n        for h_target in target_resolutions:\n            # 1. Discriminator's final feature-map spatial size\n            h_feat = calculate_h_feat(h_target, n_d, k, s, p)\n            h_feat_list.append(h_feat)\n\n            # 2. Discriminator's last-layer receptive field ratio\n            r_over_h = round(r / h_target, 6)\n            r_over_h_list.append(r_over_h)\n\n            # 3. Boolean indicating if receptive field covers the entire input\n            d_global_coverage = r >= h_target\n            d_global_coverage_list.append(d_global_coverage)\n\n            # 4. Boolean indicating generator compatibility\n            ratio = h_native / h_target\n            g_compatible = ratio.is_integer() and is_power_of_two(int(ratio))\n            g_compatible_list.append(g_compatible)\n\n            # 5. Number of spatial positions contributing to the discriminator's decision\n            n_pos = h_feat ** 2\n            n_pos_list.append(n_pos)\n\n        case_results = [\n            h_feat_list,\n            r_over_h_list,\n            d_global_coverage_list,\n            g_compatible_list,\n            n_pos_list,\n        ]\n        all_cases_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Convert the nested list to a string and remove spaces for compact output.\n    final_output_string = str(all_cases_results).replace(\" \", \"\")\n    print(final_output_string)\n\nsolve()\n```", "id": "3112718"}, {"introduction": "在理解了静态架构之后，我们现在来研究生成器从潜在空间到图像空间的映射的动态特性。此实践将要求您实现简化的生成器模型，以观察架构选择（特别是非线性模块的引入）如何影响生成图像之间过渡的“平滑度”。这种动手分析将为理解生成模型中的解耦 (disentanglement) 和纠缠流形 (entangled manifolds) 等概念提供直观的认识。[@problem_id:3112803]", "problem": "考虑深度卷积生成对抗网络 (DCGANs)，其中生成器通过卷积计算和逐点非线性将潜向量映射到图像。设潜空间是一个维度为 $d$ 的实向量空间，潜向量表示为 $\\mathbf{z} \\in \\mathbb{R}^d$。生成器是一个函数 $G: \\mathbb{R}^d \\to \\mathbb{R}^{S \\times S}$，它从一个潜向量生成一个空间分辨率为 $S \\times S$ 的图像。目标是通过测量路径上连续生成的图像之间的 $L_2$ 范数差来评估生成器在潜空间中的线性插值路径上是表现出线性解耦，还是诱导出一个纠缠流形，从而衡量输出路径的平滑度。\n\n基本基础和定义：\n- 生成对抗网络 (GANs) 定义了一个生成器 $G$，它将潜向量 $\\mathbf{z}$ 转换为数据样本；深度卷积生成对抗网络 (DCGANs) 通过卷积层来特化 $G$。\n- 潜空间中的线性插值路径为 $ \\mathbf{z}(t) = (1-t)\\,\\mathbf{z}_1 + t\\,\\mathbf{z}_2$，其中 $t \\in [0,1]$。\n- 向量 $\\mathbf{x} \\in \\mathbb{R}^n$ 的 $L_2$ 范数是 $ \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}$。\n- 沿离散化插值 $t_i$ 的路径平滑度通过 $L_2$ 范数差序列 $d_i = \\|G(\\mathbf{z}(t_{i+1})) - G(\\mathbf{z}(t_i))\\|_2$ 进行评估。\n- 三角函数中的角度应解释为弧度。\n\n您将实现三个生成器原型，每个原型都由基于原则的卷积运算和上采样构建，并使用固定的、确定性的核和基映射。所有构建必须仅使用线性卷积和逐点非线性；卷积必须是二维的，并按指定方式逐通道应用或在通道混合后应用。以下常量在全文中使用：\n- 潜向量维度 $d = 8$。\n- 基础特征网格大小 $s = 4$ 和上采样输出空间大小 $S = 8$。\n- 两个中间通道，索引为 $c \\in \\{0,1\\}$。\n\n定义从潜向量 $\\mathbf{z} = (z_1,\\dots,z_d)$ 构建通道特征图的基函数如下。对于每个通道 $c \\in \\{0,1\\}$ 和每个空间索引 $(i,j)$，其中 $i \\in \\{0,\\dots,s-1\\}$ 且 $j \\in \\{0,\\dots,s-1\\}$：\n- 对于通道 $c=0$，定义\n$$\na_k^{(0)}(i,j) = \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) + \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right).\n$$\n- 对于通道 $c=1$，定义\n$$\na_k^{(1)}(i,j) = \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) - \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right).\n$$\n通过以下方式构建基础特征图 $F_c \\in \\mathbb{R}^{s \\times s}$：\n$$\nF_c(i,j) = \\frac{1}{d}\\sum_{k=1}^{d} z_k \\, a_k^{(c)}(i,j).\n$$\n通过最近邻复制将每个 $F_c$ 上采样到 $U_c \\in \\mathbb{R}^{S \\times S}$，即 $F_c$ 的每个元素被复制成一个 $2 \\times 2$ 的块。\n\n定义两个大小为 $3 \\times 3$ 的固定卷积核，索引为 $p,q \\in \\{0,1,2\\}$：\n$$\nK_0(p,q) = \\frac{1}{1 + p + q}, \\quad K_1(p,q) = \\frac{(-1)^{p+q}}{1 + p + q}.\n$$\n卷积以步幅为 $1$ 的二维卷积形式执行，填充对称处理，输出形状等于输入形状（标准的“相同”卷积）。必须实现以下三个生成器原型：\n1. 线性生成器 $G_{\\mathrm{lin}}$：计算\n$$\nH_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1), \\quad G_{\\mathrm{lin}}(\\mathbf{z}) = H_0 + H_1.\n$$\n2. 非线性门控生成器 $G_{\\mathrm{gate}}$：计算\n$$\nH_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1),\n$$\n然后应用逐点非线性和乘性交互，\n$$\nG_{\\mathrm{gate}}(\\mathbf{z}) = \\tanh(H_0) \\odot \\max(H_1, 0) + \\frac{1}{2}\\,\\mathrm{conv2d}(\\tanh(H_0), K_0),\n$$\n其中 $\\odot$ 表示逐元素相乘。\n3. 饱和 tanh 生成器 $G_{\\tanh}$：计算\n$$\nG_{\\tanh}(\\mathbf{z}) = \\tanh\\!\\big(\\mathrm{conv2d}(U_0 + U_1, K_0)\\big).\n$$\n\n对于给定的生成器 $G$、潜向量端点 $\\mathbf{z}_1$ 和 $\\mathbf{z}_2$、离散步数 $n \\in \\mathbb{N}$ 以及阈值 $\\varepsilon  0$，定义一个均匀离散化 $t_i = \\frac{i}{n}$（其中 $i \\in \\{0,1,\\dots,n\\}$）并计算差值\n$$\nd_i = \\left\\| \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_{i+1}))\\big) - \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_i))\\big) \\right\\|_2,\\quad i \\in \\{0,1,\\dots,n-1\\},\n$$\n其中 $\\mathrm{vec}(\\cdot)$ 表示将 $S \\times S$ 图像向量化为 $\\mathbb{R}^{S^2}$。设 $\\{d_i\\}_{i=0}^{n-1}$ 的均值和标准差分别为 $m$ 和 $s$，并定义变异系数\n$$\n\\mathrm{CV} = \\begin{cases} \\frac{s}{m},  \\text{if } m > 0 \\\\ 0,  \\text{if } m = 0 \\end{cases}\n$$\n如果 $\\mathrm{CV} \\le \\varepsilon$，则将生成器行为分类为“线性解耦”，否则分类为“纠缠流形”。为每个测试用例返回一个布尔值，其中 $\\mathrm{True}$ 表示“线性解耦”，$\\mathrm{False}$ 表示“纠缠流形”。\n\n您的程序必须实现上述定义，并为以下每个测试用例计算分类（三角函数中的所有角度均为弧度）：\n\n- 测试用例 1 (顺利路径，线性生成器)：\n  - 生成器: $G_{\\mathrm{lin}}$。\n  - $\\mathbf{z}_1 = [0.1, -0.2, 0.3, -0.4, 0.5, -0.6, 0.7, -0.8]$。\n  - $\\mathbf{z}_2 = [-0.5, 0.4, -0.3, 0.2, -0.1, 0.0, 0.1, 0.2]$。\n  - 步数: $n = 20$。\n  - 阈值: $\\varepsilon = 10^{-6}$。\n\n- 测试用例 2 (非线性门控，预期纠缠)：\n  - 生成器: $G_{\\mathrm{gate}}$。\n  - $\\mathbf{z}_1 = [-1.5, 0.0, 0.5, -0.2, 1.0, -1.2, 0.3, 0.7]$。\n  - $\\mathbf{z}_2 = [2.0, -0.5, 0.8, -1.0, 1.5, 0.0, -0.3, 0.1]$。\n  - 步数: $n = 20$。\n  - 阈值: $\\varepsilon = 0.02$。\n\n- 测试用例 3 (边界条件，相同端点)：\n  - 生成器: $G_{\\tanh}$。\n  - $\\mathbf{z}_1 = [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2]$。\n  - $\\mathbf{z}_2 = [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2]$。\n  - 步数: $n = 10$。\n  - 阈值: $\\varepsilon = 10^{-6}$。\n\n- 测试用例 4 (饱和边界情况)：\n  - 生成器: $G_{\\tanh}$。\n  - $\\mathbf{z}_1 = [3.0, -2.5, 2.2, -1.8, 1.6, -1.4, 1.2, -1.0]$。\n  - $\\mathbf{z}_2 = [-3.0, 2.4, -2.1, 1.9, -1.7, 1.5, -1.3, 1.1]$。\n  - 步数: $n = 20$。\n  - 阈值: $\\varepsilon = 0.02$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$ [result_1,result_2,result_3,result_4] $），其中每个 $result_i$ 是如上定义的布尔值。", "solution": "用户的问题陈述已经过分析和验证。它在科学上是合理的、定义明确的、客观的且内部一致的。所有定义和参数均已提供，可以进行直接且明确的实现。该问题是对一个简化的深度卷积生成对抗网络 (DCGAN) 生成器中特征纠缠的风格化但概念上合理的探索，这是深度学习中的一个相关主题。\n\n解决方案将通过逐步实现指定组件来构建，与所提供的数学定义相对应。\n\n### 1. 准备工作和常量\n问题定义了几个常量：潜向量维度 $d=8$，基础网格大小 $s=4$，以及输出网格大小 $S=8$。分析涉及两个通道，索引为 $c \\in \\{0,1\\}$。所有三角函数中的角度都指定为弧度。\n\n### 2. 卷积核\n定义了两个固定的 $3 \\times 3$ 卷积核 $K_0$ 和 $K_1$。对于索引 $p,q \\in \\{0,1,2\\}$：\n$$\nK_0(p,q) = \\frac{1}{1 + p + q}\n$$\n$$\nK_1(p,q) = \\frac{(-1)^{p+q}}{1 + p + q}\n$$\n这些核将作为 $3 \\times 3$ 矩阵被预先计算。卷积运算是标准的二维卷积，步幅为 $1$，并采用对称填充以使输出维度与输入维度匹配（“相同”卷积）。对于一个 $3 \\times 3$ 的核，这需要在输入特征图的所有侧面填充宽度为 $1$ 的边。\n\n### 3. 基映射构建\n初始特征图是使用一组基函数从潜向量 $\\mathbf{z} \\in \\mathbb{R}^d$ 构建的。对于每个通道 $c \\in \\{0,1\\}$，定义了一组 $d=8$ 个基映射 $\\{ a_k^{(c)} \\}_{k=1}^d$。对于空间索引 $i \\in \\{0, \\dots, s-1\\}$ 和 $j \\in \\{0, \\dots, s-1\\}$，以及基索引 $k \\in \\{1, \\dots, d\\}$：\n- 通道 $c=0$: $a_k^{(0)}(i,j) = \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) + \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right)$\n- 通道 $c=1$: $a_k^{(1)}(i,j) = \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) - \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right)$\n\n这些大小为 $s \\times s$ 的基映射是固定的，可以预先计算。\n\n### 4. 基础特征图生成\n给定一个潜向量 $\\mathbf{z} = (z_1, \\dots, z_d)$，基础特征图 $F_c \\in \\mathbb{R}^{s \\times s}$ 被计算为基映射的线性组合：\n$$\nF_c(i,j) = \\frac{1}{d}\\sum_{k=1}^{d} z_k \\, a_k^{(c)}(i,j)\n$$\n此操作相对于输入潜向量 $\\mathbf{z}$ 是线性的。\n\n### 5. 上采样\n大小为 $s \\times s = 4 \\times 4$ 的基础特征图 $F_c$ 被上采样到大小为 $S \\times S = 8 \\times 8$ 的 $U_c$。指定的方法是最近邻复制，其中 $F_c$ 中的每个值被复制以在 $U_c$ 中形成一个 $2 \\times 2$ 的块。此操作也是线性的。\n\n### 6. 生成器架构\n实现了三个不同的生成器函数 $G_{\\mathrm{lin}}$、$G_{\\mathrm{gate}}$ 和 $G_{\\tanh}$。每个函数都将一个潜向量 $\\mathbf{z}$ 映射到一个 $S \\times S$ 的输出图像。\n\n1.  **线性生成器 $G_{\\mathrm{lin}}$**：此生成器是一个纯粹的线性变换。\n    $$\n    H_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1)\n    $$\n    $$\n    G_{\\mathrm{lin}}(\\mathbf{z}) = H_0 + H_1\n    $$\n    由于所有组成操作（基映射组合、上采样、卷积、加法）都是线性的，所以从 $\\mathbf{z}$ 到 $G_{\\mathrm{lin}}(\\mathbf{z})$ 的整个映射是线性的。\n\n2.  **非线性门控生成器 $G_{\\mathrm{gate}}$**：此生成器引入了非线性和通道交互。\n    $$\n    H_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1)\n    $$\n    $$\n    G_{\\mathrm{gate}}(\\mathbf{z}) = \\tanh(H_0) \\odot \\max(H_1, 0) + \\frac{1}{2}\\,\\mathrm{conv2d}(\\tanh(H_0), K_0)\n    $$\n    逐点双曲正切 ($\\tanh$)、类 ReLU 的门控 ($\\max(H_1, 0)$) 和逐元素相乘 ($\\odot$) 使其成为一个关于 $\\mathbf{z}$ 的高度非线性函数。\n\n3.  **饱和 Tanh 生成器 $G_{\\tanh}$**：此生成器应用了最终的饱和非线性。\n    $$\n    G_{\\tanh}(\\mathbf{z}) = \\tanh\\!\\big(\\mathrm{conv2d}(U_0 + U_1, K_0)\\big)\n    $$\n    在这里，通道在卷积前通过加法混合。最终的 $\\tanh$ 函数将对较大的输入值产生饱和效应，这是 GANs 中的一个常见特征。\n\n### 7. 路径平滑度分析\n问题的核心是分析潜空间中线性插值所生成的输出路径的平滑度。\n- 潜路径为 $\\mathbf{z}(t) = (1-t)\\,\\mathbf{z}_1 + t\\,\\mathbf{z}_2$，其中 $t \\in [0,1]$。\n- 该路径使用 $t_i = \\frac{i}{n}$ 进行离散化，其中 $i \\in \\{0, 1, \\dots, n\\}$。\n- 对于路径的每个分段，计算连续生成的图像之间的 $L_2$ 距离：\n$$\nd_i = \\left\\| \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_{i+1}))\\big) - \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_i))\\big) \\right\\|_2, \\quad \\text{for } i \\in \\{0, 1, \\dots, n-1\\}\n$$\n这给出了一个包含 $n$ 个距离值的序列 $\\{d_i\\}_{i=0}^{n-1}$。\n\n### 8. 分类\n生成器的行为根据这些步长的一致性进行分类。\n- 计算序列 $\\{d_i\\}$ 的均值 $m$ 和标准差 $s$。\n- 计算变异系数 $\\mathrm{CV} = s/m$（如果 $m=0$ 则为 $0$）。低的 $\\mathrm{CV}$ 表示步长 $d_i$ 几乎恒定，表明沿路径的映射是线性或近线性的。高的 $\\mathrm{CV}$ 表示步长变化大，这是弯曲、纠缠流形的特征。\n- 如果 $\\mathrm{CV} \\le \\varepsilon$，生成器的行为被分类为“线性解耦”（返回 $\\mathrm{True}$），否则被分类为“纠缠流形”（返回 $\\mathrm{False}$）。\n\n### 9. 测试用例执行\n将实现的函数应用于提供的四个测试用例。\n- 对于 $G_{\\mathrm{lin}}$，我们预计所有的 $d_i$ 值几乎完全相同，因为生成器是一个线性变换。这将导致 $s \\approx 0$ 和 $\\mathrm{CV} \\approx 0$，从而得到 `True` 的分类。\n- 在 $\\mathbf{z}_1 = \\mathbf{z}_2$ 的情况下，路径是静态的。因此，所有的 $d_i=0$，导致 $m=0$，$s=0$，和 $\\mathrm{CV}=0$，从而得到 `True` 的分类。\n- 对于应用于不同端点的非线性生成器 $G_{\\mathrm{gate}}$ 和 $G_{\\tanh}$，非线性预计会将潜路径扭曲成输出空间中的一条曲线。沿这条曲线的行进速率将不是恒定的，导致步长 $d_i$ 的标准差 $s$ 很大，$\\mathrm{CV}$ 不可忽略，因此得到 `False` 的分类。\n最终程序为每个测试用例计算布尔分类，并以指定格式打印它们。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the DCGAN generator prototypes and\n    analyzing their path smoothness.\n    \"\"\"\n\n    # Define constants from the problem statement\n    D_LATENT = 8\n    S_BASE = 4\n    S_OUTPUT = 8\n\n    # Pre-compute convolution kernels\n    p, q = np.mgrid[0:3, 0:3]\n    K0 = 1 / (1 + p + q)\n    K1 = ((-1)**(p + q)) / (1 + p + q)\n\n    # Pre-compute basis maps\n    k = np.arange(1, D_LATENT + 1)\n    i = np.arange(S_BASE)\n    j = np.arange(S_BASE)\n    \n    arg_i = np.pi * (k[:, None, None] + 1) * (i[None, :, None] + 1) / S_BASE\n    arg_j = np.pi * (k[:, None, None] + 1) * (j[None, None, :] + 1) / S_BASE\n    \n    BASIS_MAPS_0 = np.sin(arg_i) + np.cos(arg_j)\n    BASIS_MAPS_1 = np.cos(arg_i) - np.sin(arg_j)\n\n    def generate_base_maps(z: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Generates the base feature maps F_0 and F_1 from a latent vector z.\n        \"\"\"\n        z_reshaped = z[:, np.newaxis, np.newaxis]\n        \n        weighted_maps_0 = z_reshaped * BASIS_MAPS_0\n        F0 = (1 / D_LATENT) * np.sum(weighted_maps_0, axis=0)\n        \n        weighted_maps_1 = z_reshaped * BASIS_MAPS_1\n        F1 = (1 / D_LATENT) * np.sum(weighted_maps_1, axis=0)\n        \n        return F0, F1\n\n    def upsample(F: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Upsamples a feature map using nearest-neighbor replication.\n        \"\"\"\n        return np.kron(F, np.ones((2, 2)))\n\n    # Define Generator Prototypes\n    def g_lin(z: np.ndarray) -> np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        H0 = convolve2d(U0, K0, mode='same', boundary='fill', fillvalue=0)\n        H1 = convolve2d(U1, K1, mode='same', boundary='fill', fillvalue=0)\n        return H0 + H1\n\n    def g_gate(z: np.ndarray) -> np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        H0 = convolve2d(U0, K0, mode='same', boundary='fill', fillvalue=0)\n        H1 = convolve2d(U1, K1, mode='same', boundary='fill', fillvalue=0)\n        tanh_H0 = np.tanh(H0)\n        relu_H1 = np.maximum(H1, 0)\n        term1 = tanh_H0 * relu_H1\n        term2 = 0.5 * convolve2d(tanh_H0, K0, mode='same', boundary='fill', fillvalue=0)\n        return term1 + term2\n\n    def g_tanh(z: np.ndarray) -> np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        U_sum = U0 + U1\n        H = convolve2d(U_sum, K0, mode='same', boundary='fill', fillvalue=0)\n        return np.tanh(H)\n\n    generator_map = {\n        \"G_lin\": g_lin,\n        \"G_gate\": g_gate,\n        \"G_tanh\": g_tanh,\n    }\n\n    def analyze_path(gen_name, z1, z2, n, epsilon):\n        \"\"\"\n        Performs the path analysis and returns the classification.\n        \"\"\"\n        gen_func = generator_map[gen_name]\n        z1_np = np.array(z1)\n        z2_np = np.array(z2)\n        \n        d_values = []\n        for i in range(n):\n            t_curr = i / n\n            t_next = (i + 1) / n\n            \n            z_curr = (1 - t_curr) * z1_np + t_curr * z2_np\n            z_next = (1 - t_next) * z1_np + t_next * z2_np\n            \n            img_curr = gen_func(z_curr)\n            img_next = gen_func(z_next)\n            \n            diff = np.linalg.norm(img_curr.flatten() - img_next.flatten())\n            d_values.append(diff)\n            \n        d_values_np = np.array(d_values)\n        \n        mean_d = np.mean(d_values_np)\n        std_d = np.std(d_values_np)\n        \n        if mean_d > 0:\n            cv = std_d / mean_d\n        else:\n            cv = 0.0\n            \n        return cv = epsilon\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"G_lin\", [0.1, -0.2, 0.3, -0.4, 0.5, -0.6, 0.7, -0.8], [-0.5, 0.4, -0.3, 0.2, -0.1, 0.0, 0.1, 0.2], 20, 1e-6),\n        (\"G_gate\", [-1.5, 0.0, 0.5, -0.2, 1.0, -1.2, 0.3, 0.7], [2.0, -0.5, 0.8, -1.0, 1.5, 0.0, -0.3, 0.1], 20, 0.02),\n        (\"G_tanh\", [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2], [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2], 10, 1e-6),\n        (\"G_tanh\", [3.0, -2.5, 2.2, -1.8, 1.6, -1.4, 1.2, -1.0], [-3.0, 2.4, -2.1, 1.9, -1.7, 1.5, -1.3, 1.1], 20, 0.02)\n    ]\n\n    results = []\n    for case in test_cases:\n        gen_name, z1, z2, n, epsilon = case\n        result = analyze_path(gen_name, z1, z2, n, epsilon)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112803"}, {"introduction": "基于对生成器映射的理解，我们现在将在潜在空间的特定点上进行更精细的局部特性分析。通过推导和计算生成器的雅可比矩阵 (Jacobian)，我们可以量化输出图像对潜在编码微小变化的敏感度。这个练习介绍了一种强大的微分工具来探究生成模型，并揭示了关于潜在空间的有效维度和冗余性的深刻见解。[@problem_id:3112792]", "problem": "考虑一个深度卷积生成对抗网络 (DCGAN)，其中生成器 $G$ 通过一系列卷积神经网络中的标准操作，将一个潜向量 $z \\in \\mathbb{R}^d$ 映射到一个空间排列的输出：一个全连接仿射变换、一个整流线性单元 (ReLU) 非线性、一个二维转置卷积以及一个双曲正切 ($\\tanh$) 输出非线性。\n\n目标是从第一性原理出发，推导雅可比矩阵 $J = \\partial G(z)/\\partial z$，解释其奇异值谱以评估空间特征对潜坐标的局部敏感性，并评估潜维度的冗余性。\n\n从以下基本原理开始：\n- 复合函数的雅可比矩阵链式法则，即对于函数复合 $G = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1$，其雅可比矩阵为 $J_G(z) = J_{f_L}(f_{L-1}(\\cdots)) \\cdots J_{f_1}(z)$。\n- 线性映射 $x \\mapsto Ax$ 的雅可比矩阵是矩阵 $A$。\n- 逐元素应用的整流线性单元 (ReLU) 的导数，在激活前的值为严格正数时为 $1$，否则为 $0$。\n- 双曲正切的导数是 $\\mathrm{sech}^2(y) = 1 - \\tanh^2(y)$，逐元素应用于 $\\tanh$ 激活前的输入 $y$。\n- 二维转置卷积是一个从输入特征图到输出特征图的线性算子，可以表示为一个作用于展平输入的矩阵 $T$。\n\n在下述所有测试用例中，生成器由以下通用复合结构定义：\n1. 一个全连接线性映射 $a_0 = W z + b$，其中 $W \\in \\mathbb{R}^{N_{\\text{in}} \\times d}$ 且 $b \\in \\mathbb{R}^{N_{\\text{in}}}$，产生 $N_{\\text{in}}$ 个激活前的特征值。\n2. 一个逐元素的整流线性单元 $h_0 = \\max(a_0, 0)$，其结果被重塑为 $C_{\\text{in}}$ 个空间大小为 $H_0 \\times W_0$ 的输入通道，采用通道优先、行优先的顺序。\n3. 一个步幅 $s = 1$ 且无填充的二维转置卷积，使用卷积核 $K$ 生成一个空间大小为 $H_{\\text{out}} \\times W_{\\text{out}}$ 的单一输出通道，其中对于卷积核高度 $k_h$ 和宽度 $k_w$，有 $H_{\\text{out}} = H_0 + k_h - 1$ 和 $W_{\\text{out}} = W_0 + k_w - 1$。该转置卷积定义为\n$$\ny[u, v] = \\sum_{c=0}^{C_{\\text{in}}-1} \\sum_{i=0}^{H_0-1} \\sum_{j=0}^{W_0-1} h_0[c, i, j] \\cdot K[c, u-i, v-j],\n$$\n其中索引范围有效，满足 $0 \\le u-i  k_h$ 和 $0 \\le v-j  k_w$；$y$ 是 $\\tanh$ 激活前的输出。\n4. 一个逐元素的双曲正切 $o = \\tanh(y)$。\n\n在此模型下，在给定点 $z$ 处，关于 $z$ 的雅可比矩阵为\n$$\nJ(z) = D_{\\tanh}(y) \\, T \\, D_{\\mathrm{ReLU}}(a_0) \\, W,\n$$\n其中 $D_{\\mathrm{ReLU}}(a_0)$ 是一个对角矩阵，如果对应的 $a_0$ 分量为严格正数，则其元素为 $1$，否则为 $0$；$T$ 是将展平的 $h_0$ 映射到展平的 $y$ 的转置卷积的矩阵表示；$D_{\\tanh}(y)$ 是一个对角矩阵，其元素为在展平的 $y$ 上计算的 $1 - \\tanh^2(y)$。\n\n您必须实现此映射，并使用奇异值分解 (SVD) 计算 $J(z)$ 的奇异值。将大的奇异值解释为空间特征对某些潜方向的高敏感性，将非常小的奇异值（相对于数值阈值）解释为局部冗余或被抑制的潜方向。对于数值秩的确定，使用一个阈值 $\\epsilon$，其中如果奇异值 $s$ 满足 $s \\ge \\epsilon$，则它对秩有贡献。\n\n您的程序必须严格按照每个测试用例的规定实现生成器，并且必须为每个测试用例计算以下输出：\n- $J(z)$ 的最大奇异值，记为 $s_{\\max}$。\n- SVD 返回的所有奇异值中 $J(z)$ 的最小奇异值（如果 $J(z)$ 是秩亏的，则等于 $0$），记为 $s_{\\min}$。\n- 有效活跃的潜维度分数，定义为数值秩除以 $d$，即\n$$\n\\text{rank fraction} = \\frac{\\#\\{s_i \\mid s_i \\ge \\epsilon\\}}{d}.\n$$\n将这三个量全部表示为十进制数。\n\n最终输出格式必须是包含每个测试用例结果列表的单行。每个测试用例的结果是一个包含三个十进制数的列表，顺序为 $[s_{\\max}, s_{\\min}, \\text{rank fraction}]$。总输出必须是这些逐测试用例列表的单一列表，打印为单行，无额外文本，例如，\n$[\\,[s_{\\max}^{(1)}, s_{\\min}^{(1)}, r^{(1)}],\\, [s_{\\max}^{(2)}, s_{\\min}^{(2)}, r^{(2)}],\\, \\ldots\\,]$。\n\n测试套件规范：\n- 测试用例 $1$ (正常路径)：\n    - 潜维度 $d = 4$。\n    - 输入通道数 $C_{\\text{in}} = 1$，输出通道数 $C_{\\text{out}} = 1$。\n    - 空间大小 $H_0 = 2$, $W_0 = 2$。\n    - 全连接权重 $W \\in \\mathbb{R}^{4 \\times 4}$ 等于 $0.5 I_4$（其中 $I_4$ 是 $4 \\times 4$ 单位矩阵），偏置 $b = 0$。\n    - 逐元素应用 ReLU。\n    - 转置卷积核 $K \\in \\mathbb{R}^{1 \\times 1 \\times 2 \\times 2}$，其单个输入通道的卷积核为\n    $$\n    K^{(0)} = \\begin{bmatrix} 0.5  -0.25 \\\\ 0.75  0.25 \\end{bmatrix}.\n    $$\n    - 输出非线性是逐元素应用的 $\\tanh$。\n    - 潜向量\n    $$\n    z^{(1)} = \\begin{bmatrix} 0.2 \\\\ -0.4 \\\\ 0.1 \\\\ 0.3 \\end{bmatrix}.\n    $$\n    - 数值阈值 $\\epsilon = 10^{-9}$。\n\n- 测试用例 $2$ (边界门控情况)：\n    - 架构和参数与测试用例 1 相同。\n    - 潜向量\n    $$\n    z^{(2)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n    $$\n    - 数值阈值 $\\epsilon = 10^{-9}$。\n    - 注意：根据 ReLU 在 $0$ 处的导数为 $0$ 的约定，在这种情况下，雅可比矩阵应坍缩为零矩阵。\n\n- 测试用例 $3$ (多通道边界情况)：\n    - 潜维度 $d = 4$。\n    - 输入通道数 $C_{\\text{in}} = 2$，输出通道数 $C_{\\text{out}} = 1$。\n    - 空间大小 $H_0 = 2$, $W_0 = 2$。\n    - 全连接权重 $W \\in \\mathbb{R}^{8 \\times 4}$，其行按通道优先、行优先的顺序为 2 个输入通道（每个大小为 $2 \\times 2$）指定。具体为\n    $$\n    W = \\begin{bmatrix}\n    0.4  -0.3  0.1  0.0 \\\\\n    0.0  0.2  -0.1  0.5 \\\\\n    -0.2  0.1  0.3  -0.4 \\\\\n    0.3  0.0  0.2  0.1 \\\\\n    -0.1  0.4  0.0  -0.2 \\\\\n    0.2  -0.5  0.3  0.0 \\\\\n    0.0  0.1  -0.3  0.4 \\\\\n    0.5  0.2  0.1  -0.1\n    \\end{bmatrix}, \\quad b = 0.\n    $$\n    - 逐元素应用 ReLU。\n    - 转置卷积核 $K \\in \\mathbb{R}^{2 \\times 1 \\times 2 \\times 2}$，每个输入通道一个（聚合到单个输出通道），由下式给出\n    $$\n    K^{(0)} = \\begin{bmatrix} 0.3  -0.1 \\\\ 0.2  0.4 \\end{bmatrix}, \\quad\n    K^{(1)} = \\begin{bmatrix} -0.2  0.5 \\\\ 0.1  -0.3 \\end{bmatrix}.\n    $$\n    - 输出非线性是逐元素应用的 $\\tanh$。\n    - 潜向量\n    $$\n    z^{(3)} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ 0.2 \\end{bmatrix}.\n    $$\n    - 数值阈值 $\\epsilon = 10^{-9}$。\n\n实现与输出要求：\n- 在数学上和算法上实现上述生成器，使用链式法则将 $J(z)$ 构建为对角雅可比矩阵和线性算子的乘积，其中 $T$ 通过其对基输入的作​​用显式构造。\n- 通过奇异值分解计算 $J(z)$ 的奇异值。\n- 对于每个测试用例，输出一个列表 $[s_{\\max}, s_{\\min}, \\text{rank fraction}]$ 作为十进制数。\n- 程序必须打印一个包含逐测试用例列表的列表的单行，例如，\n$[\\,[s_{\\max}^{(1)}, s_{\\min}^{(1)}, r^{(1)}],\\, [s_{\\max}^{(2)}, s_{\\min}^{(2)}, r^{(2)}],\\, [s_{\\max}^{(3)}, s_{\\min}^{(3)}, r^{(3)}]\\,]$。", "solution": "用户提供的问题是有效的。这是一个定义明确且具有科学依据的练习，旨在应用微分学来分析深度卷积生成对抗网络 (DCGAN) 生成器的局部属性。所有参数和架构细节都已指定，从而可以得到唯一且可验证的解决方案。\n\n问题的核心是为给定的生成器模型 $G$ 和潜向量 $z$ 计算雅可比矩阵 $J(z) = \\frac{\\partial G(z)}{\\partial z}$，然后使用其奇异值分解 (SVD) 来分析该雅可比矩阵。生成器 $G$ 是四个标准神经网络层的复合：\n$1$. 一个全连接（仿射）层：$a_0 = W z + b$\n$2$. 一个逐元素的整流线性单元 (ReLU) 激活：$h_0 = \\max(a_0, 0)$\n$3$. 一个二维转置卷积，它是一个线性算子，我们可以用矩阵 $T$ 表示：$y = T h_{0,\\text{flat}}$\n$4$. 一个逐元素的双曲正切 ($\\tanh$) 激活：$o = \\tanh(y)$\n\n因此，生成器是函数复合 $G(z) = \\tanh(T \\cdot \\mathrm{ReLU}(Wz+b))$。根据向量函数的求导链式法则，$G$ 的雅可比矩阵是其各组成层雅可比矩阵的乘积，在适当的中间值处求值。问题提供了最终的公式：\n$$\nJ(z) = D_{\\tanh}(y) \\, T \\, D_{\\mathrm{ReLU}}(a_0) \\, W\n$$\n此处，$D_{\\mathrm{ReLU}}(a_0)$ 和 $D_{\\tanh}(y)$ 分别是表示 ReLU 和 $\\tanh$ 函数逐元素导数的对角矩阵。矩阵 $W$ 和 $T$ 代表全连接层和转置卷积层的线性变换。\n\n为实现该解决方案，我们对每个测试用例遵循以下步骤：\n\n**步骤 1：前向传播**\n首先，我们必须用给定的潜向量 $z$ 对生成器网络执行一次前向传播，以计算雅可比矩阵所需的中间激活前值。\n- ReLU 激活前的值计算为 $a_0 = W z + b$。\n- ReLU 激活后的值为 $h_0 = \\max(a_0, 0)$。然后将该向量重塑为形状为 $(C_{\\text{in}}, H_0, W_0)$ 的张量。\n- $\\tanh$ 激活前的值 $y$ 是通过将转置卷积应用于 $h_0$ 张量来计算的。\n- 最终输出为 $o=\\tanh(y)$，尽管雅可比矩阵计算本身并不严格需要它。\n\n**步骤 2：雅可比矩阵构建**\n利用前向传播得到的中间值，我们构建雅可比矩阵乘积中的每个矩阵：\n- $W$：初始全连接层的权重矩阵在每个测试用例中直接给出。\n- $D_{\\mathrm{ReLU}}(a_0)$：这是一个大小为 $N_{\\text{in}} \\times N_{\\text{in}}$ 的对角矩阵，其中 $N_{\\text{in}} = C_{\\text{in}} \\times H_0 \\times W_0$。根据问题为 ReLU 指定的导数，如果对应的激活前值 $a_{0,i}  0$，则第 $i$ 个对角元素为 $1$，否则为 $0$。\n- $T$：该矩阵表示转置卷积的线性操作。其维度为 $N_{\\text{out}} \\times N_{\\text{in}}$，其中 $N_{\\text{out}} = H_{\\text{out}} \\times W_{\\text{out}}$。该矩阵的元素 $T_{pq}$ 对应第 $q$ 个输入单元（在展平的 $h_0$ 中）对第 $p$ 个输出单元（在展平的 $y$ 中）的影响。这可以通过考虑转置卷积公式来确定。$T$ 在行索引 `out_idx`（对应输出中的空间位置 $(u,v)$）和列索引 `in_idx`（对应输入通道 $c$ 和位置 $(i,j)$）处的元素由连接它们的卷积核的值给出：$K[c, u-i, v-j]$，前提是卷积核索引有效，否则为 $0$。\n- $D_{\\tanh}(y)$：这是一个大小为 $N_{\\text{out}} \\times N_{\\text{out}}$ 的对角矩阵。第 $j$ 个对角元素由在激活前值 $y_j$ 处计算的双曲正切导数 $1 - \\tanh^2(y_j)$ 给出。\n\n**步骤 3：雅可比矩阵计算与 SVD**\n完整的雅可比矩阵 $J(z)$ 是通过这四个分量的矩阵乘积计算得到的。一旦获得 $J(z)$，我们执行奇异值分解 (SVD)，它将 $J(z)$ 分解为 $U S V^T$，其中 $S$ 是由奇异值 $s_i \\ge 0$ 构成的对角矩阵。这些奇异值量化了映射 $G$ 在输入和输出空间中沿不同正交方向拉伸空间的程度。\n\n**步骤 4：奇异值分析**\n计算出的奇异值用于确定所需的指标：\n- 最大奇异值 $s_{\\max} = \\max_i s_i$ 表示输出对潜向量变化的最大局部敏感性。\n- 最小奇异值 $s_{\\min} = \\min_i s_i$ 表示最小局部敏感性。如果 $s_{\\min}$ 为零或接近零，则某些潜方向在局部被“坍缩”或对输出没有影响。\n- 秩分数计算为 $J(z)$ 的数值秩除以潜维度 $d$。数值秩是大于或等于一个小阈值 $\\epsilon = 10^{-9}$ 的奇异值 $s_i$ 的数量。该分数衡量了局部活跃或非冗余的潜维度的比例。\n\n这一完整过程被封装在提供的 Python 代码中，该代码系统地构建矩阵、计算雅可比矩阵、执行 SVD，并为每个测试用例提取指定的指标。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for a suite of test cases.\n    It orchestrates the setup of each case and the final printing of results.\n    \"\"\"\n\n    def _compute_analysis(z, d, W, b, K, C_in, H0, W0, k_h, k_w, epsilon):\n        \"\"\"\n        Helper function to compute the Jacobian and its analysis for a single test case.\n\n        This function performs the forward pass to get intermediate activations,\n        constructs the component matrices of the Jacobian, computes the full\n        Jacobian via matrix multiplication, and finally analyzes its singular\n        value spectrum.\n        \"\"\"\n        # --- 1. Forward Pass to compute intermediate activations ---\n        N_in = C_in * H0 * W0\n\n        # Layer 1: Fully connected\n        a0 = W @ z + b\n\n        # Layer 2: ReLU\n        h0 = np.maximum(a0, 0)\n        h0_tensor = h0.reshape((C_in, H0, W0))\n\n        # Layer 3: Transposed Convolution\n        H_out = H0 + k_h - 1\n        W_out = W0 + k_w - 1\n        N_out = H_out * W_out\n        \n        # Pre-tanh output y\n        y = np.zeros((H_out, W_out))\n        for c_idx in range(C_in):\n            # This check accelerates computation if a whole channel in h0 is zero\n            if np.any(h0_tensor[c_idx, :, :]):\n                for i in range(H0):\n                    for j in range(W0):\n                        # This check avoids the inner loops if the specific activation is zero\n                        if h0_tensor[c_idx, i, j] > 0:\n                            for u in range(H_out):\n                                for v in range(W_out):\n                                    ki = u - i\n                                    kj = v - j\n                                    if 0 = ki  k_h and 0 = kj  k_w:\n                                        y[u, v] += h0_tensor[c_idx, i, j] * K[c_idx, 0, ki, kj]\n        \n        y_flat = y.flatten()\n\n        # --- 2. Jacobian Matrix Construction ---\n        \n        # W is given.\n\n        # Diagonal Jacobian of ReLU, D_relu\n        d_relu_diag = (a0 > 0).astype(float)\n        D_relu = np.diag(d_relu_diag)\n\n        # Matrix for Transposed Convolution, T\n        T = np.zeros((N_out, N_in))\n        for c_idx in range(C_in):\n            for i in range(H0):\n                for j in range(W0):\n                    in_idx = c_idx * (H0 * W0) + i * W0 + j\n                    for u in range(H_out):\n                        for v in range(W_out):\n                            out_idx = u * W_out + v\n                            ki = u - i\n                            kj = v - j\n                            if 0 = ki  k_h and 0 = kj  k_w:\n                                T[out_idx, in_idx] = K[c_idx, 0, ki, kj]\n\n        # Diagonal Jacobian of tanh, D_tanh\n        d_tanh_diag = 1 - np.tanh(y_flat)**2\n        D_tanh = np.diag(d_tanh_diag)\n        \n        # Full Jacobian J(z) using the chain rule: J = D_tanh @ T @ D_relu @ W\n        # Dimensions: (N_out x N_out) @ (N_out x N_in) @ (N_in x N_in) @ (N_in x d) -> (N_out x d)\n        J = D_tanh @ T @ D_relu @ W\n\n        # --- 3. Singular Value Decomposition (SVD) and Analysis ---\n        if J.size == 0:\n            s = np.array([])\n        else:\n            s = np.linalg.svd(J, compute_uv=False)\n\n        s_max = np.max(s) if s.size > 0 else 0.0\n        \n        # The number of singular values is min(N_out, d), which is d=4 in all cases.\n        s_min = np.min(s) if s.size > 0 else 0.0\n        \n        numerical_rank = np.sum(s >= epsilon)\n        rank_fraction = numerical_rank / d\n        \n        return [float(s_max), float(s_min), float(rank_fraction)]\n\n    # --- Test Suite Specification ---\n\n    # Test Case 1: Happy path\n    d1 = 4\n    z1 = np.array([0.2, -0.4, 0.1, 0.3])\n    W1 = 0.5 * np.identity(4)\n    b1 = np.zeros(4)\n    K1_mat = np.array([[0.5, -0.25], [0.75, 0.25]])\n    K1 = K1_mat.reshape(1, 1, 2, 2)\n    case1_params = {\n        \"z\": z1, \"d\": d1, \"W\": W1, \"b\": b1, \"K\": K1,\n        \"C_in\": 1, \"H0\": 2, \"W0\": 2, \"k_h\": 2, \"k_w\": 2, \"epsilon\": 1e-9\n    }\n    result1 = _compute_analysis(**case1_params)\n\n    # Test Case 2: Boundary gating case (z=0)\n    z2 = np.zeros(4)\n    case2_params = {**case1_params, \"z\": z2}\n    result2 = _compute_analysis(**case2_params)\n\n    # Test Case 3: Multi-channel edge case\n    d3 = 4\n    z3 = np.array([0.1, -0.2, 0.05, 0.2])\n    W3 = np.array([\n        [0.4, -0.3, 0.1, 0.0], [0.0, 0.2, -0.1, 0.5],\n        [-0.2, 0.1, 0.3, -0.4], [0.3, 0.0, 0.2, 0.1],\n        [-0.1, 0.4, 0.0, -0.2], [0.2, -0.5, 0.3, 0.0],\n        [0.0, 0.1, -0.3, 0.4], [0.5, 0.2, 0.1, -0.1]\n    ])\n    b3 = np.zeros(8)\n    K3_c0 = np.array([[0.3, -0.1], [0.2, 0.4]])\n    K3_c1 = np.array([[-0.2, 0.5], [0.1, -0.3]])\n    K3 = np.stack([K3_c0, K3_c1]).reshape(2, 1, 2, 2)\n    case3_params = {\n        \"z\": z3, \"d\": d3, \"W\": W3, \"b\": b3, \"K\": K3,\n        \"C_in\": 2, \"H0\": 2, \"W0\": 2, \"k_h\": 2, \"k_w\": 2, \"epsilon\": 1e-9\n    }\n    result3 = _compute_analysis(**case3_params)\n\n    results = [result1, result2, result3]\n    \n    # Final print statement in the exact required format.\n    # str() on a list of floats includes spaces, and join adds commas.\n    # The final string is a valid representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112792"}]}