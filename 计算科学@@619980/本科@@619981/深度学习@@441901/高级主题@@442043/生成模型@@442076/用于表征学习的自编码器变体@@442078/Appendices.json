{"hands_on_practices": [{"introduction": "表示学习的一个核心目标是创建一个潜空间，其中语义相似的数据点在空间上聚集在一起。本练习将介绍一种直接且直观的评估方法：最近原型分类器。通过计算潜空间中的类中心（即原型），并检查测试样本是否与其正确的原型更近，我们可以量化表示对不同类别数据的分离效果 [@problem_id:3099275]。", "problem": "您将处理一个表示学习评估任务，该任务基于自编码器嵌入的概念。考虑一个自编码器，其编码器被建模为从输入空间到潜空间的仿射映射。形式上，对于一个输入向量 $x \\in \\mathbb{R}^d$，其潜表示为 $z = W x + b$，其中 $W \\in \\mathbb{R}^{k \\times d}$ 和 $b \\in \\mathbb{R}^k$ 是固定参数。为了评估潜空间 $z$ 是否与类别结构对齐，您需要计算类别原型，并在留出的测试数据上评估最近原型分类。您的程序必须从基本原理出发实现以下内容。\n\n使用的基本基础：\n- 将自编码器的编码器定义为一个确定性函数 $f_{\\text{enc}}: \\mathbb{R}^d \\to \\mathbb{R}^k$，由 $z = W x + b$ 给出。\n- 经验类别原型定义为一个类别的潜编码的样本均值。\n- $\\mathbb{R}^k$ 中的欧几里得距离，定义为 $\\|u - v\\|_2$。\n- 最近原型分类规则：将测试点分配给其原型在欧几里得距离上最近的类别。如果出现距离完全相等的情况（最小距离相同），则选择类别标签值最小的类别来打破平局。\n\n任务：\n- 对于下面的每个测试用例，计算所有训练输入的潜编码 $z$，计算潜空间中每个类别的原型（经验均值），然后根据最近原型（欧几里得距离）和指定的平局打破规则对每个测试输入进行分类，并输出分类准确率，即正确分类的测试样本所占的比例。报告每个准确率时，四舍五入到3位小数。\n\n重要的实现细节：\n- 类别由整数（如$0$、$1$、$2$等）索引。\n- 准确率必须计算为 $(\\text{正确预测数}) / (\\text{测试样本数})$，并以小数形式表示（而非百分比）。\n\n测试套件（四个用例），涵盖分离、重叠、坍缩以及带平局的多类别情况：\n- 用例 $1$（类别良好分离，完美对齐）：\n  - 维度：$d = 2$, $k = 2$。\n  - 编码器：$W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 训练数据：\n    - 类别 $0$：$[-1, 0]$, $[0, 0]$, $[1, 0]$。\n    - 类别 $1$：$[4, 0]$, $[5, 0]$, $[6, 0]$。\n  - 测试数据和标签：\n    - 输入：$[-0.5, 0]$, $[0.5, 0]$, $[4.5, 0]$, $[5.5, 0]$。\n    - 标签：$[0, 0, 1, 1]$。\n- 用例 $2$（类别重叠导致一些错误）：\n  - 维度：$d = 2$, $k = 2$。\n  - 编码器：$W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 训练数据：\n    - 类别 $0$：$[0, 0]$, $[0, 0]$。\n    - 类别 $1$：$[1, 0]$, $[1, 0]$。\n  - 测试数据和标签：\n    - 输入：$[0.1, 0]$, $[0.9, 0]$, $[0.49, 0]$, $[0.51, 0]$。\n    - 标签：$[0, 1, 1, 1]$。\n- 用例 $3$（坍缩的编码器；原型相同；测试平局打破规则和机会水平的准确率）：\n  - 维度：$d = 2$, $k = 2$。\n  - 编码器：$W = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 训练数据：\n    - 类别 $0$：$[1, 2]$, $[3, 4]$。\n    - 类别 $1$：$[-1, -2]$, $[-3, -4]$。\n  - 测试数据和标签：\n    - 输入：$[10, 10]$, $[-10, -10]$, $[7, -7]$, $[-7, 7]$。\n    - 标签：$[0, 1, 1, 0]$。\n- 用例 $4$（多类别、各向异性编码器及显式平局）：\n  - 维度：$d = 2$, $k = 2$。\n  - 编码器：$W = \\begin{bmatrix} 2  0 \\\\ 0  0.5 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$。\n  - 训练数据：\n    - 类别 $0$：$[-0.5, 2]$, $[-0.5, 2]$。\n    - 类别 $1$：$[1.5, 2]$, $[1.5, 2]$。\n    - 类别 $2$：$[-0.5, 8]$, $[-0.5, 8]$。\n  - 测试数据和标签：\n    - 输入：$[-0.5, 2.2]$, $[1.5, 1.8]$, $[-0.5, 5]$, $[-0.5, 8.2]$。\n    - 标签：$[0, 1, 2, 2]$。\n\n程序要求：\n- 仅使用指定的编码器模型、欧几里得距离、潜空间中的经验类别均值以及指定的平局打破规则，实现所述的精确流程。\n- 您的程序应生成单行输出，其中包含四个准确率，每个准确率四舍五入到3位小数，形式为用方括号括起来的逗号分隔列表（例如，$[0.875,0.500,1.000,0.750]$）。", "solution": "问题陈述是一个有效的计算任务，它基于线性代数和初等统计学原理，常用于表示学习方法的评估。该问题是适定的，所有必要的参数、数据和程序性规则都已明确定义，确保每个测试用例都有唯一且可验证的解。我们现在将逐步进行求解。\n\n任务的核心是在由仿射变换定义的潜空间中实现和评估一个最近原型分类器。每个测试用例的过程包括三个主要阶段：\n1.  **潜空间变换**：使用给定的编码器函数，将所有训练和测试数据点从输入空间 $\\mathbb{R}^d$ 映射到潜空间 $\\mathbb{R}^k$。\n2.  **原型计算**：对于每个类别，通过计算其训练成员的潜编码的经验均值，来计算一个单一的代表性向量，即“原型”。\n3.  **分类与评估**：对于每个测试点，通过找到欧几里得距离最近的原型来预测其类别。最终准确率是正确分类的测试点所占的比例。\n\n让我们将这些步骤形式化。编码器是一个仿射映射 $f_{\\text{enc}}: \\mathbb{R}^d \\to \\mathbb{R}^k$，定义为：\n$$z = f_{\\text{enc}}(x) = W x + b$$\n其中 $x \\in \\mathbb{R}^d$ 是一个输入向量，$W \\in \\mathbb{R}^{k \\times d}$ 是权重矩阵，$b \\in \\mathbb{R}^k$ 是偏置向量。\n\n对于一个给定的类别 $C_j$，其训练样本集为 $S_j = \\{x_1, x_2, \\ldots, x_{N_j}\\}$，对应的潜编码为 $Z_j = \\{z_1, z_2, \\ldots, z_{N_j}\\}$，其中 $z_i = f_{\\text{enc}}(x_i)$。类别原型 $P_j \\in \\mathbb{R}^k$ 是这些潜编码的样本均值：\n$$P_j = \\frac{1}{N_j} \\sum_{i=1}^{N_j} z_i$$\n对于一个新的测试输入 $x_{\\text{test}}$，其潜编码为 $z_{\\text{test}} = f_{\\text{enc}}(x_{\\text{test}})$。预测的类别标签 $\\hat{y}$ 由最近原型规则确定：\n$$\\hat{y} = \\underset{j}{\\arg\\min} \\, \\| z_{\\text{test}} - P_j \\|_2$$\n其中 $\\| \\cdot \\|_2$ 表示欧几里得距离。如果多个类别都达到最小距离，则通过选择具有最小整数标签的类别来打破平局。\n\n最后，准确率计算为正确分类的测试样本数与测试样本总数的比率。我们现在将此过程应用于四个测试用例中的每一个。\n\n**用例1：类别良好分离**\n- 维度：$d=2$, $k=2$。\n- 编码器：$W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。变换是恒等变换，$z = x$。\n- 类别0的训练数据：$\\left\\{ \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\}$。\n- 类别0的原型：$P_0 = \\frac{1}{3} \\left( \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 类别1的训练数据：$\\left\\{ \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix} \\right\\}$。\n- 类别1的原型：$P_1 = \\frac{1}{3} \\left( \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix}$。\n- 测试数据：$z_{\\text{test},1}=\\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},2}=\\begin{bmatrix} 0.5 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},3}=\\begin{bmatrix} 4.5 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},4}=\\begin{bmatrix} 5.5 \\\\ 0 \\end{bmatrix}$。真实标签：$[0, 0, 1, 1]$。\n- 预测：\n  - 对于 $z_{\\text{test},1}$：$\\|z_{\\text{test},1} - P_0\\|_2=0.5$, $\\|z_{\\text{test},1} - P_1\\|_2=5.5$。预测为0。正确。\n  - 对于 $z_{\\text{test},2}$：$\\|z_{\\text{test},2} - P_0\\|_2=0.5$, $\\|z_{\\text{test},2} - P_1\\|_2=4.5$。预测为0。正确。\n  - 对于 $z_{\\text{test},3}$：$\\|z_{\\text{test},3} - P_0\\|_2=4.5$, $\\|z_{\\text{test},3} - P_1\\|_2=0.5$。预测为1。正确。\n  - 对于 $z_{\\text{test},4}$：$\\|z_{\\text{test},4} - P_0\\|_2=5.5$, $\\|z_{\\text{test},4} - P_1\\|_2=0.5$。预测为1。正确。\n- 准确率：$4/4 = 1.0$。\n\n**用例2：类别重叠**\n- 维度：$d=2$, $k=2$。\n- 编码器：$W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。变换是恒等变换，$z = x$。\n- 类别0的训练数据：$\\left\\{ \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\right\\}$。原型 $P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 类别1的训练数据：$\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\}$。原型 $P_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n- 决策边界是连接 $P_0$ 和 $P_1$ 的线段的垂直平分线，即直线 $x=0.5$。\n- 测试数据：$z_{\\text{test},1}=\\begin{bmatrix} 0.1 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},2}=\\begin{bmatrix} 0.9 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},3}=\\begin{bmatrix} 0.49 \\\\ 0 \\end{bmatrix}$, $z_{\\text{test},4}=\\begin{bmatrix} 0.51 \\\\ 0 \\end{bmatrix}$。真实标签：$[0, 1, 1, 1]$。\n- 预测：\n  - 对于 $z_{\\text{test},1}$：$\\|z_{\\text{test},1} - P_0\\|_2=0.1$, $\\|z_{\\text{test},1} - P_1\\|_2=0.9$。预测为0。正确。\n  - 对于 $z_{\\text{test},2}$：$\\|z_{\\text{test},2} - P_0\\|_2=0.9$, $\\|z_{\\text{test},2} - P_1\\|_2=0.1$。预测为1。正确。\n  - 对于 $z_{\\text{test},3}$：$\\|z_{\\text{test},3} - P_0\\|_2=0.49$, $\\|z_{\\text{test},3} - P_1\\|_2=0.51$。预测为0。不正确（真实标签为1）。\n  - 对于 $z_{\\text{test},4}$：$\\|z_{\\text{test},4} - P_0\\|_2=0.51$, $\\|z_{\\text{test},4} - P_1\\|_2=0.49$。预测为1。正确。\n- 准确率：$3/4 = 0.75$。\n\n**用例3：坍缩的编码器**\n- 维度：$d=2$, $k=2$。\n- 编码器：$W = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。所有输入都映射到原点：$z = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 类别0的训练数据：$\\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} \\right\\}$。两者都映射到 $z=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。原型 $P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 类别1的训练数据：$\\left\\{ \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix} \\right\\}$。两者都映射到 $z=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。原型 $P_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 两个类别的原型是相同的。\n- 测试数据：任何测试输入 $x_{\\text{test}}$ 都被映射到 $z_{\\text{test}} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 预测：对于任何 $z_{\\text{test}}$，到两个原型的距离都是0：$\\|z_{\\text{test}} - P_0\\|_2 = 0$ 和 $\\|z_{\\text{test}} - P_1\\|_2 = 0$。这是一个完全的平局。根据平局打破规则，我们选择最小的类别标签，即0。因此，每个测试样本都被预测为类别0。\n- 预测标签：$[0, 0, 0, 0]$。真实标签：$[0, 1, 1, 0]$。\n- 第一个和第四个样本的预测是正确的。\n- 准确率：$2/4 = 0.5$。\n\n**用例4：多类别、各向异性编码器及平局**\n- 维度：$d=2$, $k=2$。\n- 编码器：$W = \\begin{bmatrix} 2  0 \\\\ 0  0.5 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$。映射为 $z = \\begin{bmatrix} 2x_1+1 \\\\ 0.5x_2-1 \\end{bmatrix}$。\n- 原型：\n  - 类别 $0$（训练：$\\begin{bmatrix} -0.5 \\\\ 2 \\end{bmatrix}$）：$z = \\begin{bmatrix} 2(-0.5)+1 \\\\ 0.5(2)-1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。因此，$P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 类别 $1$（训练：$\\begin{bmatrix} 1.5 \\\\ 2 \\end{bmatrix}$）：$z = \\begin{bmatrix} 2(1.5)+1 \\\\ 0.5(2)-1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}$。因此，$P_1 = \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}$。\n  - 类别 $2$（训练：$\\begin{bmatrix} -0.5 \\\\ 8 \\end{bmatrix}$）：$z = \\begin{bmatrix} 2(-0.5)+1 \\\\ 0.5(8)-1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$。因此，$P_2 = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$。\n- 测试数据变换与分类。真实标签：$[0, 1, 2, 2]$。\n  - $x_{\\text{test},1} = \\begin{bmatrix} -0.5 \\\\ 2.2 \\end{bmatrix} \\implies z_1 = \\begin{bmatrix} 0 \\\\ 0.1 \\end{bmatrix}$。\n    - $\\|z_1 - P_0\\|_2 = 0.1$, $\\|z_1 - P_1\\|_2 = \\sqrt{4^2+0.1^2} \\approx 4.001$, $\\|z_1 - P_2\\|_2 = 2.9$。预测为0。正确。\n  - $x_{\\text{test},2} = \\begin{bmatrix} 1.5 \\\\ 1.8 \\end{bmatrix} \\implies z_2 = \\begin{bmatrix} 4 \\\\ -0.1 \\end{bmatrix}$。\n    - $\\|z_2 - P_0\\|_2 \\approx 4.001$, $\\|z_2 - P_1\\|_2 = 0.1$, $\\|z_2 - P_2\\|_2 = \\sqrt{4^2+(-3.1)^2} \\approx 5.06$。预测为1。正确。\n  - $x_{\\text{test},3} = \\begin{bmatrix} -0.5 \\\\ 5 \\end{bmatrix} \\implies z_3 = \\begin{bmatrix} 0 \\\\ 1.5 \\end{bmatrix}$。\n    - $\\|z_3 - P_0\\|_2 = 1.5$, $\\|z_3 - P_1\\|_2 = \\sqrt{4^2+1.5^2} \\approx 4.27$, $\\|z_3 - P_2\\|_2 = 1.5$。\n    - 类别0和类别2之间出现平局。最小标签是0。预测为0。不正确（真实标签为2）。\n  - $x_{\\text{test},4} = \\begin{bmatrix} -0.5 \\\\ 8.2 \\end{bmatrix} \\implies z_4 = \\begin{bmatrix} 0 \\\\ 3.1 \\end{bmatrix}$。\n    - $\\|z_4 - P_0\\|_2 = 3.1$, $\\|z_4 - P_1\\|_2 \\approx 5.06$, $\\|z_4 - P_2\\|_2 = 0.1$。预测为2。正确。\n- 准确率：$3/4 = 0.75$。\n\n计算出的准确率分别为 $1.0$、$0.75$、$0.5$ 和 $0.75$。这些值将四舍五入到3位小数作为最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes classification accuracy for four test cases using a nearest-prototype\n    classifier in an autoencoder's latent space.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (well-separated classes, perfect alignment)\n        {\n            'W': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([-1.0, 0.0]), np.array([0.0, 0.0]), np.array([1.0, 0.0])],\n                1: [np.array([4.0, 0.0]), np.array([5.0, 0.0]), np.array([6.0, 0.0])]\n            },\n            'test_inputs': [\n                np.array([-0.5, 0.0]), np.array([0.5, 0.0]),\n                np.array([4.5, 0.0]), np.array([5.5, 0.0])\n            ],\n            'test_labels': [0, 0, 1, 1]\n        },\n        # Case 2 (overlapping classes causing some errors)\n        {\n            'W': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([0.0, 0.0]), np.array([0.0, 0.0])],\n                1: [np.array([1.0, 0.0]), np.array([1.0, 0.0])]\n            },\n            'test_inputs': [\n                np.array([0.1, 0.0]), np.array([0.9, 0.0]),\n                np.array([0.49, 0.0]), np.array([0.51, 0.0])\n            ],\n            'test_labels': [0, 1, 1, 1]\n        },\n        # Case 3 (collapsed encoder; tests tie-breaking)\n        {\n            'W': np.array([[0.0, 0.0], [0.0, 0.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([1.0, 2.0]), np.array([3.0, 4.0])],\n                1: [np.array([-1.0, -2.0]), np.array([-3.0, -4.0])]\n            },\n            'test_inputs': [\n                np.array([10.0, 10.0]), np.array([-10.0, -10.0]),\n                np.array([7.0, -7.0]), np.array([-7.0, 7.0])\n            ],\n            'test_labels': [0, 1, 1, 0]\n        },\n        # Case 4 (multi-class with anisotropic encoder and an explicit tie)\n        {\n            'W': np.array([[2.0, 0.0], [0.0, 0.5]]),\n            'b': np.array([1.0, -1.0]),\n            'train_data': {\n                0: [np.array([-0.5, 2.0]), np.array([-0.5, 2.0])],\n                1: [np.array([1.5, 2.0]), np.array([1.5, 2.0])],\n                2: [np.array([-0.5, 8.0]), np.array([-0.5, 8.0])]\n            },\n            'test_inputs': [\n                np.array([-0.5, 2.2]), np.array([1.5, 1.8]),\n                np.array([-0.5, 5.0]), np.array([-0.5, 8.2])\n            ],\n            'test_labels': [0, 1, 2, 2]\n        }\n    ]\n\n    accuracies = []\n    \n    for case in test_cases:\n        W = case['W']\n        b = case['b']\n        train_data = case['train_data']\n        test_inputs = case['test_inputs']\n        test_labels = case['test_labels']\n        \n        prototypes = {}\n        # Ensure class labels are sorted for consistent processing\n        class_labels = sorted(train_data.keys())\n        \n        # 1. Compute class prototypes in latent space\n        for label in class_labels:\n            points = train_data[label]\n            # Encode each training point x to latent code z = Wx + b\n            latent_codes = [W @ x + b for x in points]\n            # Prototype is the mean of the latent codes\n            prototypes[label] = np.mean(latent_codes, axis=0)\n            \n        predictions = []\n        # 2. Classify each test input\n        for x_test in test_inputs:\n            # Encode the test point\n            z_test = W @ x_test + b\n            \n            # Calculate Euclidean distance to each prototype\n            distances = []\n            for label in class_labels:\n                p = prototypes[label]\n                dist = np.linalg.norm(z_test - p)\n                distances.append((dist, label))\n            \n            # Find the minimum distance\n            min_dist = min(d for d, _ in distances)\n            \n            # Find all class labels corresponding to the minimum distance (handles ties)\n            # Use np.isclose for robust floating-point comparison\n            tied_labels = [label for dist, label in distances if np.isclose(dist, min_dist)]\n            \n            # Apply the tie-breaking rule: choose the smallest class label\n            predicted_label = min(tied_labels)\n            predictions.append(predicted_label)\n            \n        # 3. Calculate accuracy for the current case\n        correct_predictions = sum(1 for pred, true in zip(predictions, test_labels) if pred == true)\n        accuracy = correct_predictions / len(test_labels)\n        accuracies.append(accuracy)\n\n    # Format the results as specified: a list of floats rounded to 3 decimal places\n    formatted_accuracies = [f\"{acc:.3f}\" for acc in accuracies]\n    print(f\"[{','.join(formatted_accuracies)}]\")\n\nsolve()\n```", "id": "3099275"}, {"introduction": "在学会如何评估表示之后，我们进一步探索如何通过训练学习到更优的表示。在自编码器的训练中，重构损失函数的选择是一项关键的设计决策，因为它含蓄地定义了模型应优先保留哪些特征。本练习通过对比像素级的均方误差（MSE）损失和一种感知损失，展示了不同的训练目标如何塑造潜空间，从而影响其在分类等下游任务中的有效性 [@problem_id:3099257]。", "problem": "您将为一个合成图像数据集实现并比较两种线性自编码器变体以进行表示学习，这两种变体仅在重构损失上有所不同：逐像素均方误差（MSE）与在固定特征空间中计算的感知损失。目标是通过在潜码上训练一个线性探针分类器，来评估重构损失的选择如何影响学习到的潜表示。\n\n基本原理和定义：\n- 自编码器是一对参数化函数 $(f_{\\text{enc}}, f_{\\text{dec}})$，其训练目标是最小化数据 $\\{x_i\\}_{i=1}^{N}$ 上的重构目标，其中 $x_i \\in \\mathbb{R}^{D}$。编码器将 $x$ 映射到潜码 $z = f_{\\text{enc}}(x)$，解码器重构出 $\\hat{x} = f_{\\text{dec}}(z)$，目的是学习一种能捕捉数据中规律性的表示。训练遵循在指定重构损失下的经验风险最小化原则。\n- 在本问题中，您将仅限于使用线性自编码器：$z = W_e x$ 和 $\\hat{x} = W_d z$，其中 $W_e \\in \\mathbb{R}^{d \\times D}$ 和 $W_d \\in \\mathbb{R}^{D \\times d}$，$d$ 是潜维度。\n- 将比较两种重构损失：\n  1. 逐像素均方误差 (MSE)：此损失在输入空间中度量重构误差，其值为 $x$ 和 $\\hat{x}$ 之间差的平方和。\n  2. 固定特征空间 $\\phi$ 中的感知损失：此损失在经过固定的线性特征变换 $\\phi(x) = A x$ 后度量重构误差，其值为 $\\phi(x)$ 和 $\\phi(\\hat{x})$ 之间差的平方和。\n\n数据集规格：\n- 您将构建一个合成数据集，其中包含 $8 \\times 8$ 网格上的二值图像，这些图像被展平为 $\\mathbb{R}^{64}$ 中的向量。数据共有 $3$ 个类别，由简单的结构模式定义：\n  - 类别 $0$：偶数列上的垂直条纹。\n  - 类别 $1$：偶数行上的水平条纹。\n  - 类别 $2$：一条对角线；为增加可变性，允许沿对角线方向进行随机的单像素位移。\n- 生成 $N_{\\text{train}} = 120$ 个训练样本和 $N_{\\text{test}} = 60$ 个测试样本，每个类别分别有 $40$ 个和 $20$ 个，保持数据平衡。向每张图像添加标准差为 $\\sigma = 0.1$ 的独立高斯噪声，并将值裁剪到 $[0,1]$ 区间以保持有效的像素强度。为保证可复现性，请使用固定的随机种子。\n\n特征变换 $\\phi$：\n- 对于感知损失，使用一个固定的线性变换 $\\phi(x) = A x$，其中 $A \\in \\mathbb{R}^{m \\times D}$。按如下方式构造 $A$：\n  - 构建一个离散梯度算子 $G \\in \\mathbb{R}^{D_g \\times D}$，用于计算 $8 \\times 8$ 图像网格上的水平和垂直有限差分，其中 $D_g = 8 \\cdot 7 + 7 \\cdot 8 = 112$。具体来说，水平差分为 $x_{i,j+1} - x_{i,j}$，垂直差分为 $x_{i+1,j} - x_{i,j}$，并将它们堆叠成单个向量。\n  - 对于标记为 “gradient” 的情况，选择 $m$ 并创建一个随机高斯投影 $P \\in \\mathbb{R}^{m \\times D_g}$，其条目经过适当缩放，使得 $A = P G \\in \\mathbb{R}^{m \\times D}$。对于 “identity” 情况，设置 $A = I_D \\in \\mathbb{R}^{D \\times D}$。\n\n训练和评估协议：\n- 针对每个测试用例，训练两个独立的线性自编码器，它们的架构相同，仅重构损失不同：\n  - 输入空间中的逐像素 MSE 损失。\n  - 由 $A$ 定义的特征空间中的感知损失。\n- 对 $W_e$ 和 $W_d$ 使用全批量梯度下降法，学习率为 $\\eta$，周期数为 $E$，权重衰减正则化参数为 $\\gamma$。\n- 训练完每个自编码器后，为训练集和测试集计算潜码 $z$。在训练潜码上使用岭回归（即带 $\\ell_2$ 正则化的线性最小二乘法）训练一个线性探针分类器，以预测类别标签（编码为 one-hot 向量）。通过取具有最大预测分数的类别，在测试潜码上评估分类准确率。\n\n测试套件：\n为以下三个测试用例实现上述过程，每个用例由潜维度 $d$、特征维度 $m$ 和 $A$ 的类型（“gradient” 或 “identity”）指定：\n- 用例 $\\mathrm{A}$：$d = 4$，$m = 32$，$A$ 类型 = “gradient”。\n- 用例 $\\mathrm{B}$：$d = 64$，$m = 64$，$A$ 类型 = “identity”。\n- 用例 $\\mathrm{C}$：$d = 2$，$m = 16$，$A$ 类型 = “gradient”。\n\n超参数和常量：\n- 图像网格大小 $8 \\times 8$，展平后的维度 $D = 64$。\n- 训练样本数 $N_{\\text{train}} = 120$，测试样本数 $N_{\\text{test}} = 60$。\n- 高斯噪声标准差 $\\sigma = 0.1$。\n- 学习率 $\\eta = 0.1$，周期数 $E = 200$，权重衰减 $\\gamma = 10^{-4}$。\n- 岭回归正则化 $\\lambda = 10^{-3}$。\n- 对所有随机化组件使用固定的随机种子 $s = 123$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表应按顺序包含每个测试用例的结果，即使用 MSE 训练的自编码器的线性探针准确率（以小数形式表示），后跟使用感知损失训练的自编码器的准确率。具体而言，输出格式必须为\n[$a_{\\mathrm{A},\\mathrm{MSE}}$, $a_{\\mathrm{A},\\mathrm{perc}}$, $a_{\\mathrm{B},\\mathrm{MSE}}$, $a_{\\mathrm{B},\\mathrm{perc}}$, $a_{\\mathrm{C},\\mathrm{MSE}}$, $a_{\\mathrm{C},\\mathrm{perc}}$]，\n其中每个 $a_{\\cdot,\\cdot}$ 是一个在 $[0,1]$ 区间内的浮点数。", "solution": "问题陈述被认为是有效的。它在表示学习领域具有科学依据，是一个有着明确目标和约束的适定问题，并使用客观、可形式化的语言进行表述。所有参数和过程都已指定，从而能够得出一个唯一且可验证的解。\n\n该问题要求实现并比较两种线性自编码器变体，它们在重构损失函数上有所不同，目的是评估不同损失函数对学习到的表示质量的影响。评估通过在学习到的潜码上训练一个线性分类器（“探针”）来执行。\n\n### 1. 合成数据集生成\n\n数据集包含 $8 \\times 8$ 的二值图像，这些图像被展平为向量 $x \\in \\mathbb{R}^{D}$，其中 $D=64$。共有三类模式。\n- **类别 0 (垂直条纹)**：图像在偶数编号的列（$0, 2, 4, 6$）上的值为 $1$，其余位置为 $0$。\n- **类别 1 (水平条纹)**：图像在偶数编号的行（$0, 2, 4, 6$）上的值为 $1$，其余位置为 $0$。\n- **类别 2 (对角线)**：图像在主对角线（$i=j$）上的值为 $1$，其余位置为 $0$。为引入可变性，该类别的每个样本都是通过取基础对角线图像并应用一个随机位移 $(s, s)$ 生成的，其中 $s$ 从 $\\{-1, 0, 1\\}$ 中均匀选取。\n\n生成一个包含 $N_{\\text{train}}=120$ 个样本（每类 $40$ 个）的训练集和一个包含 $N_{\\text{test}}=60$ 个样本（每类 $20$ 个）的测试集。向每张图像的每个像素添加均值为 $0$、标准差为 $\\sigma=0.1$ 的独立高斯噪声。然后将像素值裁剪到 $[0, 1]$ 范围内。所有随机过程均使用固定的种子 $s=123$ 以确保可复现性。\n\n数据矩阵表示为 $X_{\\text{train}} \\in \\mathbb{R}^{D \\times N_{\\text{train}}}$ 和 $X_{\\text{test}} \\in \\mathbb{R}^{D \\times N_{\\text{test}}}$，其中每一列都是一个展平的图像向量。\n\n### 2. 线性自编码器模型\n\n自编码器由一个线性编码器 $f_{\\text{enc}}$ 和一个线性解码器 $f_{\\text{dec}}$ 组成。\n- **编码器**：$z = f_{\\text{enc}}(x) = W_e x$，其中 $x \\in \\mathbb{R}^D$ 是输入，$z \\in \\mathbb{R}^d$ 是潜码，$W_e \\in \\mathbb{R}^{d \\times D}$ 是编码器权重矩阵。\n- **解码器**：$\\hat{x} = f_{\\text{dec}}(z) = W_d z$，其中 $\\hat{x} \\in \\mathbb{R}^D$ 是重构结果，$W_d \\in \\mathbb{R}^{D \\times d}$ 是解码器权重矩阵。\n完整的映射为 $\\hat{x} = W_d W_e x$。对于一批数据 $X \\in \\mathbb{R}^{D \\times N}$，重构结果为 $\\hat{X} = W_d W_e X$。\n\n### 3. 重构损失与训练\n\n比较了两种损失函数。对于一批 $N$ 个样本，总损失 $\\mathcal{L}_{\\text{total}}$ 包括一个重构项和一个作用于权重的 $\\ell_2$ 正则化（权重衰减）项。重构损失在批次上进行平均。\n$$\n\\mathcal{L}_{\\text{total}}(W_e, W_d) = \\mathcal{L}_{\\text{rec}} + \\frac{\\gamma}{2} \\left( \\|W_e\\|_F^2 + \\|W_d\\|_F^2 \\right)\n$$\n其中 $\\gamma=10^{-4}$ 是权重衰减参数，$\\|\\cdot\\|_F$ 是弗罗贝尼乌斯范数。\n\n**a) 逐像素均方误差 (MSE) 损失**\n该损失是输入与重构之间的均方误差。\n$$\n\\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^N \\|x_i - \\hat{x}_i\\|_2^2 = \\frac{1}{N} \\|X - \\hat{X}\\|_F^2\n$$\n\n**b) 感知损失**\n该损失是在由线性变换 $\\phi(x) = Ax$ 定义的固定特征空间中的均方误差。\n$$\n\\mathcal{L}_{\\text{perc}} = \\frac{1}{N} \\sum_{i=1}^N \\|\\phi(x_i) - \\phi(\\hat{x}_i)\\|_2^2 = \\frac{1}{N} \\|A(X - \\hat{X})\\|_F^2\n$$\n矩阵 $A \\in \\mathbb{R}^{m \\times D}$ 的构造如下：\n- 对于 “identity” 情况，$A = I_D$，即大小为 $D=64$ 的单位矩阵。在这种情况下，$\\mathcal{L}_{\\text{perc}}$ 与 $\\mathcal{L}_{\\text{MSE}}$ 完全相同。\n- 对于 “gradient” 情况，$A = PG$，其中 $G \\in \\mathbb{R}^{112 \\times 64}$ 是一个离散梯度算子，用于计算 $8 \\times 8$ 网格上的水平和垂直有限差分。$P \\in \\mathbb{R}^{m \\times 112}$ 是一个随机投影矩阵，其条目从高斯分布 $N(0, 1/m)$ 中抽取，这是一种用于保持距离的适当缩放方式。\n\n**通过梯度下降进行训练**\n权重 $W_e$ 和 $W_d$ 使用全批量梯度下降法进行训练，训练周期为 $E=200$，学习率为 $\\eta=0.1$。总损失相对于权重的梯度为：\n$$\n\\nabla_{W_d} \\mathcal{L}_{\\text{total}} = \\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial W_d} + \\gamma W_d \\quad ; \\quad \\nabla_{W_e} \\mathcal{L}_{\\text{total}} = \\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial W_e} + \\gamma W_e\n$$\n令 $\\hat{X} = W_d W_e X$。重构损失项的梯度通过链式法则推导得出。对于两种损失类型，我们可以写出：\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial W_d} = \\left(\\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial \\hat{X}}\\right) (W_e X)^T \\quad ; \\quad \\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial W_e} = W_d^T \\left(\\frac{\\partial \\mathcal{L}_{\\text{rec}}}{\\partial \\hat{X}}\\right) X^T\n$$\n相对于 $\\hat{X}$ 的梯度取决于损失函数：\n- 对于 MSE：$\\frac{\\partial \\mathcal{L}_{\\text{MSE}}}{\\partial \\hat{X}} = \\frac{2}{N} (\\hat{X} - X)$\n- 对于感知损失：$\\frac{\\partial \\mathcal{L}_{\\text{perc}}}{\\partial \\hat{X}} = \\frac{2}{N} A^T A (\\hat{X} - X)$\n\n权重使用 Glorot 均匀初始化进行初始化，并在每个周期中更新：\n$W_e \\leftarrow W_e - \\eta \\nabla_{W_e} \\mathcal{L}_{\\text{total}}$ 和 $W_d \\leftarrow W_d - \\eta \\nabla_{W_d} \\mathcal{L}_{\\text{total}}$。\n\n### 4. 线性探针评估\n\n自编码器训练完成后，其编码器 $W_e$ 用于为训练集和测试集生成潜码：$Z_{\\text{train}} = W_e X_{\\text{train}}$ 和 $Z_{\\text{test}} = W_e X_{\\text{test}}$。为评估这些表示的质量，会训练一个线性分类器（探针）来从潜码中预测类别标签。\n\n探针是使用岭回归训练的线性模型。标签 $y_{\\text{train}}$ 被 one-hot 编码为一个矩阵 $Y_{\\text{oh}} \\in \\mathbb{R}^{N_{\\text{train}} \\times 3}$。探针的权重 $W_{\\text{probe}} \\in \\mathbb{R}^{d \\times 3}$ 通过求解正则化最小二乘问题得到：\n$$\n\\min_{W_{\\text{probe}}} \\|Z_{\\text{train}}^T W_{\\text{probe}} - Y_{\\text{oh}}\\|_F^2 + \\lambda \\|W_{\\text{probe}}\\|_F^2\n$$\n其中 $Z_{\\text{train}}^T \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$ 是训练潜码矩阵，$\\lambda = 10^{-3}$ 是正则化参数。其闭式解为：\n$$\nW_{\\text{probe}} = (Z_{\\text{train}} Z_{\\text{train}}^T + \\lambda I_d)^{-1} Z_{\\text{train}} Y_{\\text{oh}}\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n\n训练好的探针随后用于预测测试集的标签。预测分数为 $\\hat{Y}_{\\text{test}} = Z_{\\text{test}}^T W_{\\text{probe}}$。每个样本的预测类别是得分最高的类别，即 $\\hat{y}_i = \\arg\\max_j (\\hat{Y}_{\\text{test}})_{ij}$。最终的分类准确率是测试集上正确预测标签的比例。\n\n对三个测试用例中的每一个，都为 MSE 训练的自编码器和感知损失训练的自编码器重复此完整过程。报告所得的六个准确率值。对于用例 B，其中 $A=I_D$，MSE 损失和感知损失是相同的。因此，训练出的模型和最终的准确率预期会相同，这为实现提供了一个合理性检查。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares linear autoencoders with MSE and perceptual loss.\n    \"\"\"\n\n    # --- Problem Constants and Hyperparameters ---\n    H, W = 8, 8\n    D = H * W\n    N_TRAIN, N_TEST = 120, 60\n    N_CLASSES = 3\n    N_PER_CLASS_TRAIN = N_TRAIN // N_CLASSES\n    N_PER_CLASS_TEST = N_TEST // N_CLASSES\n    NOISE_STD = 0.1\n    ETA = 0.1\n    EPOCHS = 200\n    GAMMA = 1e-4\n    LAMBDA_RIDGE = 1e-3\n    SEED = 123\n\n    TEST_CASES = [\n        {'d': 4, 'm': 32, 'A_type': 'gradient'}, # Case A\n        {'d': 64, 'm': 64, 'A_type': 'identity'}, # Case B\n        {'d': 2, 'm': 16, 'A_type': 'gradient'},  # Case C\n    ]\n\n    rng = np.random.default_rng(SEED)\n\n    def generate_dataset():\n        \"\"\"Generates the synthetic image dataset.\"\"\"\n        # Base patterns\n        patterns = []\n        # Class 0: Vertical stripes\n        p0 = np.zeros((H, W))\n        p0[:, ::2] = 1\n        patterns.append(p0.flatten())\n        # Class 1: Horizontal stripes\n        p1 = np.zeros((H, W))\n        p1[::2, :] = 1\n        patterns.append(p1.flatten())\n        # Class 2: Diagonal line (base)\n        p2 = np.eye(H).flatten()\n        patterns.append(p2) # This is just a placeholder\n\n        def create_data_for_class(n_samples, class_idx):\n            if class_idx != 2:\n                base_pattern = patterns[class_idx]\n                X = np.tile(base_pattern, (n_samples, 1))\n            else: # Class 2: Diagonal with shifts\n                base_diag = np.eye(H)\n                X_list = []\n                shifts = rng.choice([-1, 0, 1], size=n_samples)\n                for s in shifts:\n                    shifted_img = np.roll(base_diag, shift=(s, s), axis=(0, 1))\n                    X_list.append(shifted_img.flatten())\n                X = np.array(X_list)\n            \n            # Add noise and clip\n            X += rng.normal(loc=0.0, scale=NOISE_STD, size=X.shape)\n            np.clip(X, 0, 1, out=X)\n            return X\n\n        X_train_list, y_train_list = [], []\n        X_test_list, y_test_list = [], []\n        for i in range(N_CLASSES):\n            X_train_list.append(create_data_for_class(N_PER_CLASS_TRAIN, i))\n            y_train_list.append(np.full(N_PER_CLASS_TRAIN, i))\n            X_test_list.append(create_data_for_class(N_PER_CLASS_TEST, i))\n            y_test_list.append(np.full(N_PER_CLASS_TEST, i))\n\n        X_train = np.vstack(X_train_list).T  # Shape (D, N_train)\n        y_train = np.concatenate(y_train_list)\n        X_test = np.vstack(X_test_list).T   # Shape (D, N_test)\n        y_test = np.concatenate(y_test_list)\n        \n        return X_train, y_train, X_test, y_test\n\n    def build_A(m, A_type):\n        \"\"\"Builds the feature transform matrix A.\"\"\"\n        if A_type == 'identity':\n            assert m == D\n            return np.eye(D)\n        \n        if A_type == 'gradient':\n            Dg = H * (W - 1) + (H - 1) * W  # 112\n            G = np.zeros((Dg, D))\n            row_idx = 0\n            # Horizontal differences\n            for r in range(H):\n                for c in range(W - 1):\n                    idx1 = r * W + c\n                    idx2 = r * W + c + 1\n                    G[row_idx, idx1] = -1\n                    G[row_idx, idx2] = 1\n                    row_idx += 1\n            # Vertical differences\n            for r in range(H - 1):\n                for c in range(W):\n                    idx1 = r * W + c\n                    idx2 = (r + 1) * W + c\n                    G[row_idx, idx1] = -1\n                    G[row_idx, idx2] = 1\n                    row_idx += 1\n            \n            # Random projection matrix P\n            P = rng.normal(loc=0.0, scale=1.0 / np.sqrt(m), size=(m, Dg))\n            return P @ G\n        return None\n\n    def train_autoencoder(X, loss_type, A, d):\n        \"\"\"Trains a linear autoencoder.\"\"\"\n        N = X.shape[1]\n\n        # Glorot uniform initialization\n        lim_e = np.sqrt(6.0 / (D + d))\n        We = rng.uniform(-lim_e, lim_e, (d, D))\n        lim_d = np.sqrt(6.0 / (d + D))\n        Wd = rng.uniform(-lim_d, lim_d, (D, d))\n        \n        for _ in range(EPOCHS):\n            # Forward pass\n            Z = We @ X\n            X_hat = Wd @ Z\n            \n            # Gradient computation\n            E = X_hat - X\n            \n            if loss_type == 'mse':\n                grad_X_hat = (2.0 / N) * E\n            elif loss_type == 'perceptual':\n                E_perc = A @ E\n                grad_X_hat = (2.0 / N) * A.T @ E_perc\n            else:\n                raise ValueError(\"Unknown loss type\")\n            \n            grad_Wd = grad_X_hat @ Z.T + GAMMA * Wd\n            grad_We = Wd.T @ grad_X_hat @ X.T + GAMMA * We\n            \n            # Update weights\n            Wd -= ETA * grad_Wd\n            We -= ETA * grad_We\n            \n        return We\n\n    def evaluate_probe(We, X_train, y_train, X_test, y_test):\n        \"\"\"Trains and evaluates a linear probe on latent codes.\"\"\"\n        d = We.shape[0]\n        N_train, N_test = X_train.shape[1], X_test.shape[1]\n        \n        # Get latent codes, transpose to (N, d)\n        Z_train = (We @ X_train).T\n        Z_test = (We @ X_test).T\n        \n        # One-hot encode training labels\n        Y_train_oh = np.zeros((N_train, N_CLASSES))\n        Y_train_oh[np.arange(N_train), y_train] = 1\n        \n        # Train ridge regression probe\n        # W_probe = (Z^T Z + lambda I)^-1 Z^T Y\n        ZT_Z = Z_train.T @ Z_train\n        inv_term = np.linalg.inv(ZT_Z + LAMBDA_RIDGE * np.eye(d))\n        W_probe = inv_term @ Z_train.T @ Y_train_oh\n        \n        # Evaluate on test set\n        Y_pred_test_scores = Z_test @ W_probe\n        y_pred_test = np.argmax(Y_pred_test_scores, axis=1)\n        \n        accuracy = np.mean(y_pred_test == y_test)\n        return accuracy\n\n    # --- Main Execution ---\n    X_train, y_train, X_test, y_test = generate_dataset()\n    results = []\n\n    for case in TEST_CASES:\n        d, m, A_type = case['d'], case['m'], case['A_type']\n        \n        A = build_A(m, A_type)\n        \n        # Train and evaluate MSE model\n        We_mse = train_autoencoder(X_train, 'mse', A, d)\n        acc_mse = evaluate_probe(We_mse, X_train, y_train, X_test, y_test)\n        results.append(acc_mse)\n        \n        # Train and evaluate Perceptual model\n        We_perc = train_autoencoder(X_train, 'perceptual', A, d)\n        acc_perc = evaluate_probe(We_perc, X_train, y_train, X_test, y_test)\n        results.append(acc_perc)\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\n\nsolve()\n```", "id": "3099257"}, {"introduction": "一个比简单的类别可分性更高级的目标是学习“解耦”的表示，其中每个潜在维度都对应于数据中一个单一、可解释的变化因子。本练习模拟了一个具有已知物理因子（质量、摩擦力、外力）的场景，要求您实现一种基于白化变换的方法，从混合观测中恢复这些独立的因子。成功完成此任务表明您对数据底层生成过程有了更深层次的理解和掌控 [@problem_id:3099320]。", "problem": "给定一个合成环境，用于在一种用于表示学习的线性自编码器变体中测试解耦能力。观测数据由独立的物理参数生成：质量、摩擦系数和力。设参数向量为 $p \\in \\mathbb{R}^3$，其坐标为 $p = (m, b, F)$，其中 $m$ 表示质量，$b$ 表示摩擦系数，$F$ 表示外力。观测值由带有加性高斯噪声的线性混合模型生成：\n$$\nx = A p + \\varepsilon,\n$$\n其中 $A \\in \\mathbb{R}^{3 \\times 3}$ 是一个可逆混合矩阵，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_3)$，$I_3$ 是 $3 \\times 3$ 单位矩阵，且 $\\sigma \\ge 0$。假设 $p$ 在 $N$ 个样本中是独立同分布采样的，其坐标相互独立，且每个坐标都在一个固定区间内均匀分布。\n\n定义一个线性自编码器 (AE)，其编码器为 $f(x) = W x$，解码器为 $g(z) = V z$，其中 $W, V \\in \\mathbb{R}^{3 \\times 3}$。考虑以下带有解耦正则化项的训练目标：\n$$\n\\mathcal{L}(W, V) = \\mathbb{E}\\left[\\|x - V W x\\|_2^2\\right] + \\lambda \\left\\|\\mathrm{OffDiag}\\left(\\mathrm{Cov}(z)\\right)\\right\\|_F^2,\n$$\n其中 $z = W x$，$\\mathrm{Cov}(z)$ 是 $z$ 的协方差矩阵，$\\mathrm{OffDiag}(\\cdot)$ 将其矩阵参数的对角线元素置零，$\\|\\cdot\\|_F$ 是弗罗贝尼乌斯范数，$\\lambda \\ge 0$ 控制分解惩罚项的强度。在极限 $\\lambda \\to \\infty$ 的情况下，编码器必须强制实现一个因子化的潜在表示，使得 $\\mathrm{Cov}(z)$ 是对角矩阵（即，$z$ 的坐标不相关，各自具有一定的方差）。这类解允许一种特殊约束 $\\mathrm{Cov}(z) = I_3$，也称为白化。在白化条件下，可能存在多个编码器，它们之间相差一个正交旋转。\n\n你的任务是实现一个评估器。对于一个固定的数据集和一个根据 $x$ 的样本协方差推导出的选定白化编码器 $W$，该评估器需要探究每个潜在坐标 $z_i$ 是否与 $p$ 中的一个真实物理参数分量完全对应。对应关系定义为 $z$ 的坐标与 $p$ 的坐标之间的一一匹配，该匹配旨在最大化总绝对皮尔逊相关系数。如果所有匹配的绝对相关系数都超过给定阈值 $\\tau$，则宣告成功。\n\n使用以下基本依据和定义，不要直接揭示推导目标：\n- 一个具有有限二阶矩分布的零均值随机向量 $x$ 的协方差为 $\\Sigma_x = \\mathbb{E}[x x^\\top]$。\n- 一个白化变换 $W$ 满足 $W \\Sigma_x W^\\top = I_3$。\n- 主成分分析 (PCA) 白化通过谱分解 $\\Sigma_x = U \\Lambda U^\\top$ 实现，其中 $U$ 是正交矩阵，$\\Lambda$ 是对角元素为正的对角矩阵，并设置 $W = \\Lambda^{-1/2} U^\\top$。\n- 零均值标量 $a$ 和 $b$ 之间的皮尔逊相关系数为 $\\rho(a, b) = \\frac{\\mathbb{E}[a b]}{\\sqrt{\\mathbb{E}[a^2]} \\sqrt{\\mathbb{E}[b^2]}}$。\n\n在单个程序中实现以下流程，为测试套件生成最终的评估布尔值：\n1. 固定一个随机种子，并采样 $N$ 个参数向量 $p = (m, b, F)$，其坐标独立分布，即 $m \\sim \\mathrm{Uniform}([1, 3])$，$b \\sim \\mathrm{Uniform}([0.2, 0.8])$ 和 $F \\sim \\mathrm{Uniform}([2, 10])$，其中 $N = 5000$。将数据集视为总体（即，使用总体矩）。\n2. 对于每个测试用例 $(A, \\sigma)$，独立地为每个样本生成观测值 $x = A p + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_3)$。\n3. 通过减去样本均值来中心化 $x$，计算样本协方差 $\\hat{\\Sigma}_x$，并使用特征分解 $\\hat{\\Sigma}_x = U \\Lambda U^\\top$ 构建 PCA 白化变换 $W = \\Lambda^{-1/2} U^\\top$。为了数值稳定性，在求逆之前向 $\\hat{\\Sigma}_x$ 添加一个小的正则化项 $\\epsilon I_3$，其中 $\\epsilon = 10^{-6}$。\n4. 对于中心化后的 $x$ 计算 $z = W x$，并通过计算 $3 \\times 3$ 的绝对皮尔逊相关系数矩阵 $C$ 来探究解耦性，其中 $C_{ij} = |\\rho(z_i, p_j)|$，$z_i$ 是所有样本的第 $i$ 个坐标，$p_j$ 是所有样本的第 $j$ 个真实参数。找出 $z$ 坐标和 $p$ 坐标之间的一一匹配，使得 $\\sum_{i=1}^3 C_{i, \\pi(i)}$ 在所有排列 $\\pi$ 上最大化。如果 $\\min_{i} C_{i, \\pi^\\star(i)} \\ge \\tau$，则宣告成功，其中 $\\pi^\\star$ 是最大化排列。\n5. 输出一个布尔值列表，每个测试用例对应一个布尔值，表示根据上述标准解耦成功或失败。\n\n使用以下测试套件覆盖不同情况：\n- 情况 1 (已解耦): $A = I_3$, $\\sigma = 0.01$。\n- 情况 2 (轴缩放): $A = \\mathrm{diag}(2, 0.5, 1.5)$, $\\sigma = 0.02$。\n- 情况 3 (正交旋转，存在旋转模糊性): $A = R$，其中\n$$\nR = \\begin{bmatrix}\n\\cos(\\theta)  -\\sin(\\theta)  0 \\\\\n\\sin(\\theta)  \\cos(\\theta)  0 \\\\\n0  0  1\n\\end{bmatrix}, \\quad \\theta = \\frac{\\pi}{4},\n$$\n且 $\\sigma = 0.01$。\n- 情况 4 (纠缠且含噪声):\n$$\nA = \\begin{bmatrix}\n1  1  0.2 \\\\\n0.5  1  1 \\\\\n1  0.2  1\n\\end{bmatrix}, \\quad \\sigma = 0.1.\n$$\n\n如上所述，使用阈值 $\\tau = 0.9$、样本数量 $N = 5000$ 以及协方差正则化项 $\\epsilon = 10^{-6}$。\n\n你的程序应生成单行输出，其中包含一个方括号括起来的、由逗号分隔的结果列表（例如，“[result1,result2,result3,result4]”）。每个结果都必须是布尔值。不应打印任何其他文本。", "solution": "用户要求实现一个评估流程，用以评定一种特定线性自编码器的解耦能力。该自编码器的编码器矩阵 $W$ 被构建为主成分分析 (PCA) 白化变换。评估在合成生成的数据上进行，这些数据的真实生成因子是已知的，从而可以对解耦的成功与否进行量化度量。\n\n该流程首先生成一个包含 $N=5000$ 个真实参数向量 $p \\in \\mathbb{R}^3$ 的数据集。每个向量 $p = (m, b, F)$ 的分量都独立地从均匀分布中抽取：$m \\sim \\mathrm{Uniform}([1, 3])$，$b \\sim \\mathrm{Uniform}([0.2, 0.8])$ 以及 $F \\sim \\mathrm{Uniform}([2, 10])$。由于 $p$ 的各分量在构造上是独立的，其总体协方差矩阵 $\\Sigma_p$ 是对角矩阵。\n\n对于每个由混合矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$ 和噪声水平 $\\sigma \\ge 0$ 定义的测试用例，根据线性模型 $x = A p + \\varepsilon$ 生成一组对应的观测数据向量 $x \\in \\mathbb{R}^3$，其中 $\\varepsilon$ 是一个服从分布 $\\mathcal{N}(0, \\sigma^2 I_3)$ 的各向同性高斯噪声向量。最终得到的观测数据 $x$ 代表了原始独立因子经噪声损坏后的线性混合。\n\n该方法的核心是预先定义一个编码器 $W$，它强制实现了解耦表示的一个关键属性：不相关的潜在分量。目标函数 $\\mathcal{L}(W, V)$ 表明，在极限 $\\lambda \\to \\infty$ 的情况下，潜在表示的协方差 $\\mathrm{Cov}(z)$ 必须是一个对角矩阵。该约束的一个特定版本是白化，它强制要求 $\\mathrm{Cov}(z) = I_3$。所选的编码器 $W$ 的目标就是实现这一属性。\n\n首先，从均值中心化的数据 $x_c = x - \\mathbb{E}[x]$ 计算观测数据的样本协方差矩阵 $\\hat{\\Sigma}_x$。对于一个由形状为 $(N, 3)$ 的数据矩阵 $X_c$ 表示的 $N$ 个样本的数据集，该协方差矩阵为 $\\hat{\\Sigma}_x = \\frac{1}{N} X_c^\\top X_c$。理论上，总体协方差为 $\\Sigma_x = \\mathbb{E}[(Ap_c + \\varepsilon)(Ap_c + \\varepsilon)^\\top] = A \\Sigma_p A^\\top + \\sigma^2 I_3$，其中 $p_c = p - \\mathbb{E}[p]$。\n\nPCA 白化变换 $W$ 由正则化的样本协方差矩阵 $\\hat{\\Sigma}_{x, \\text{reg}} = \\hat{\\Sigma}_x + \\epsilon I_3$ 的谱分解推导得出，其中 $\\epsilon = 10^{-6}$ 是一个用于保证数值稳定性的小常数。设其特征分解为 $\\hat{\\Sigma}_{x, \\text{reg}} = U \\Lambda U^\\top$，其中 $U$ 是特征向量构成的正交矩阵，$\\Lambda$ 是对应正特征值构成的对角矩阵。白化矩阵随后被构造为 $W = \\Lambda^{-1/2} U^\\top$。\n\n选择这样的 $W$ 可以确保得到的潜在向量 $z = W x_c$ 的样本协方差近似为单位矩阵：$\\mathrm{Cov}(z) = \\frac{1}{N} Z^\\top Z = W (\\frac{1}{N} X_c^\\top X_c) W^\\top = W \\hat{\\Sigma}_x W^\\top \\approx I_3$。矩阵 $U^\\top$ 将数据旋转至与其主成分对齐，而 $\\Lambda^{-1/2}$ 则将每个分量缩放至单位方差。\n\n核心假设是，如果观测数据 $x$ 的主要变化轴与原始因子 $p$ 的轴相对应（在旋转意义下），那么这个白化过程将能够在排列、符号和缩放的意义下恢复出 $p$。评估步骤直接检验了这一假设。首先为整个数据集计算潜在表示 $Z$。然后，计算一个 $3 \\times 3$ 的绝对皮尔逊相关系数矩阵 $C$，其中每个条目 $C_{ij} = |\\rho(z_i, p_j)|$ 度量了第 $i$ 个潜在分量与第 $j$ 个真实因子之间的线性关系强度。在此计算中，$z$ 和 $p$ 都必须进行均值中心化处理；$z$ 通过构造是零均值的，而 $p$ 则被显式中心化。\n\n如果潜在表示 $z$ 成功解耦，每个潜在坐标 $z_i$ 都应与唯一一个真实因子 $p_j$ 相对应。这意味着相关系数矩阵 $C$ 应该接近一个置换矩阵（一个在每行每列只有一个值为 $1$、其余值为零的矩阵）。为了找到最佳的一一匹配，我们在 $C$ 上解决分配问题（或称最大权二分图匹配问题）。这将找到索引 $\\{1, 2, 3\\}$ 的一个排列 $\\pi^\\star$，该排列使得匹配的相关系数之和 $\\sum_{i=1}^3 C_{i, \\pi^\\star(i)}$ 最大化。\n\n最后，当且仅当该最优分配中的每个相关系数都达到或超过一个高阈值 $\\tau = 0.9$ 时，才宣告解耦成功。也就是说，成功的条件是 $\\min_{i} C_{i, \\pi^\\star(i)} \\ge \\tau$。这一严格标准确保了每个潜在分量都与一个真实因子有强烈的、唯一的对应关系。对每个测试用例重复此流程，并为每个用例记录一个布尔结果。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Implements an evaluator for disentanglement in a linear autoencoder\n    using PCA whitening and correlation-based matching.\n    \"\"\"\n    # Define problem constants\n    N = 5000\n    TAU = 0.9\n    EPSILON = 1e-6\n    RANDOM_SEED = 42\n\n    # Set the random seed for reproducibility\n    np.random.seed(RANDOM_SEED)\n\n    # Define the test suite\n    theta = np.pi / 4\n    # Case 3: Orthogonal rotation matrix\n    R_matrix = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta),  np.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    # Case 4: General invertible mixing matrix\n    A_entangled = np.array([\n        [1.0, 1.0, 0.2],\n        [0.5, 1.0, 1.0],\n        [1.0, 0.2, 1.0]\n    ])\n    \n    test_cases = [\n        # Case 1: Identity mixing matrix (already disentangled)\n        (np.eye(3), 0.01),\n        # Case 2: Axis-aligned scaling\n        (np.diag([2.0, 0.5, 1.5]), 0.02),\n        # Case 3: Orthogonal rotation\n        (R_matrix, 0.01),\n        # Case 4: General entanglement and higher noise\n        (A_entangled, 0.1)\n    ]\n\n    # 1. Generate ground-truth parameters p\n    p_m = np.random.uniform(low=1.0, high=3.0, size=(N, 1))\n    p_b = np.random.uniform(low=0.2, high=0.8, size=(N, 1))\n    p_F = np.random.uniform(low=2.0, high=10.0, size=(N, 1))\n    P_data = np.hstack((p_m, p_b, p_F))\n\n    # Center p for later correlation calculation. This is P_centered.\n    p_mean = np.mean(P_data, axis=0)\n    P_centered = P_data - p_mean\n\n    results = []\n\n    for A, sigma in test_cases:\n        # 2. Generate observed data x = A*p + noise\n        noise = np.random.normal(loc=0.0, scale=sigma, size=(N, 3))\n        # For data matrices where each row is a sample, x_row^T = A @ p_row^T\n        # This translates to X = P @ A^T\n        X_data = P_data @ A.T + noise\n\n        # 3. Center x and compute regularized sample covariance\n        x_mean = np.mean(X_data, axis=0)\n        X_centered = X_data - x_mean\n        \n        # Sigma_x = (X_centered^T @ X_centered) / N\n        Sigma_x = (X_centered.T @ X_centered) / N\n        Sigma_x_reg = Sigma_x + EPSILON * np.eye(3)\n\n        # 4. Compute PCA whitening transform W\n        # Eigendecomposition of the symmetric covariance matrix\n        eigenvalues, U = np.linalg.eigh(Sigma_x_reg)\n        # Create diagonal matrix of 1/sqrt(eigenvalues)\n        Lambda_inv_sqrt = np.diag(1.0 / np.sqrt(eigenvalues))\n        # Whitening matrix W = (Lambda^-1/2) @ U^T\n        W = Lambda_inv_sqrt @ U.T\n\n        # 5. Compute latent representation z = W*x\n        # Z = X_centered @ W^T\n        Z_data = X_centered @ W.T\n\n        # 6. Compute absolute Pearson correlation matrix C\n        # cov(z, p) = (Z_data^T @ P_centered) / N\n        cov_zp = (Z_data.T @ P_centered) / N\n        \n        # Calculate standard deviations for z and p\n        # std_z should be ~1 due to whitening\n        std_z = np.std(Z_data, axis=0)\n        std_p = np.std(P_centered, axis=0)\n        \n        # Correlation matrix C_ij = cov(z_i,p_j) / (std(z_i)*std(p_j))\n        corr_matrix = cov_zp / np.outer(std_z, std_p)\n        C = np.abs(corr_matrix)\n        \n        # 7. Find optimal one-to-one matching and evaluate success\n        # Use linear_sum_assignment to find the permutation that maximizes the sum of correlations\n        row_ind, col_ind = linear_sum_assignment(C, maximize=True)\n        \n        # Get the correlations from the optimal assignment\n        matched_correlations = C[row_ind, col_ind]\n        \n        # Success is declared if the minimum correlation in the matching is >= TAU\n        min_matched_corr = np.min(matched_correlations)\n        success = min_matched_corr >= TAU\n        results.append(success)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3099320"}]}