{"hands_on_practices": [{"introduction": "在任何归一化流的实际应用中，计算雅可比行列式的对数都是核心步骤。此练习将揭示一个关键的数值稳定性问题：直接计算高维行列式会导致浮点数上溢或下溢，而利用对数恒等式 $\\log(\\prod_{i} a_i) = \\sum_{i} \\log a_i$ 将乘积转换为求和，可以有效规避此问题。通过这个实践，你将掌握确保模型在高维空间中计算稳定而准确的关键实现技巧。", "problem": "您的任务是分析和稳定在归一化流中出现的雅可比矩阵 $J$ 的 $\\log|\\det J|$ 计算，其中概率密度变换中的变量替换法则要求在潜在的高维环境中精确计算 $\\log|\\det J|$。从基础且广为接受的基点出发：(i) 行列式作为体积缩放因子的定义，(ii) 三角矩阵的行列式等于其对角线元素乘积的性质，以及 (iii) 对于正数 $a_i$，恒等式 $\\log\\left(\\prod_{i=1}^n a_i\\right)=\\sum_{i=1}^n\\log a_i$ 成立。使用这些基点来推断在计算许多大数或小数的乘积时，浮点运算中出现的数值上溢和下溢问题。您的目标是阐明、实现并测试能够在高维情况下计算 $\\log|\\det J|$ 时避免上溢和下溢的数值稳定参数化方法。\n\n具体而言，请为以下情况推导出有原则的计算策略，且在问题陈述本身中不给出或使用任何快捷公式：\n- 对角雅可比矩阵 $J=\\mathrm{diag}(s_1,\\dots,s_d)$，其中 $s_i$ 是无约束的实数尺度，包括负数项，确保通过 $|\\det J|$ 定义的结果是良定义的，并且计算能避免上溢/下溢。\n- 下三角因子 $L$，其对角线元素严格为正，用于参数化一个对称正定矩阵 $A=LL^\\top$，这与某些归一化流组件相关。确定如何从 $L$ 稳定地评估 $\\log\\det A$。\n\n提出适合深度学习实现的稳定参数化方法。至少包括：\n- 对于对角矩阵 $J$，使用指数参数化 $s_i=\\exp(\\alpha_i)$，以强制实现可逆性和稳定性，其中 $\\alpha_i\\in\\mathbb{R}$ 是无约束的。\n- 使用 Cholesky 参数化 $A=LL^\\top$，其中 $L$ 是下三角矩阵且对角线元素严格为正，这使得 $A$ 成为对称正定矩阵，并为计算 $\\log\\det A$ 提供了一条稳定的路径。\n\n在以下测试套件中研究浮点行为。每个测试用例指定了维度 $d$ 和 $J$ 或 $L$ 的构造方式。您的程序必须为每个用例计算两个量：一个依赖直接乘积来构建行列式然后应用对数的朴素结果，以及一个使用您所提出的推导的稳定结果。每个用例最终报告的答案必须是一个布尔值，反映数值稳定性是否根据该用例的指定规则成功。\n\n测试套件：\n1. 正常情况对角用例：$d=16$， $J=\\mathrm{diag}(s)$，其中 $s_i$ 在 $0.8$ 和 $1.2$ 之间（含两端）线性间隔。正确的结果要求朴素计算和稳定计算的结果都是有限的，并且在 $10^{-10}$ 的绝对容差内一致。\n2. 上溢对角用例：$d=1024$， $J=\\mathrm{diag}(s)$，其中对所有 $i$ 都有 $s_i=10^{10}$。正确的结果要求朴素计算结果为非有限（无穷大），而稳定计算结果为有限。\n3. 下溢对角用例：$d=1024$， $J=\\mathrm{diag}(s)$，其中对所有 $i$ 都有 $s_i=10^{-10}$。正确的结果要求朴素计算结果为非有限（对一个下溢的乘积取对数后为负无穷大），而稳定计算结果为有限。\n4. Cholesky 上溢用例：$d=256$， $L$ 是下三角矩阵，对角线元素为 $10^{10}$，非对角线元素严格为零。考虑 $A=LL^\\top$。正确的结果要求朴素计算（基于构建 $A$ 并在取对数前将 $A$ 的对角线元素相乘）为非有限，而稳定计算（直接基于 $L$）为有限。\n5. 带符号的对角上溢用例：$d=1024$， $J=\\mathrm{diag}(s)$，其中 $s_i=(-1)^i\\cdot 10^{10}$。正确的结果要求朴素计算（基于幅值的乘积）为非有限，而稳定计算（必须仅依赖于通过 $|\\det J|$ 在对数内的幅值）为有限。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表，顺序与上述五个测试用例相同。每个列表元素必须是一个布尔值，与相应案例的正确性规则匹配。不涉及角度或物理单位。程序必须是自包含的，并且不得读取任何输入。确保数值实现在存在上溢和下溢事件时是鲁棒的，并严格根据开头概述的原则构建您的逻辑，不要在问题陈述本身中插入任何快捷公式。", "solution": "该问题要求推导并实现用于计算雅可比矩阵行列式绝对值的对数 $\\log|\\det J|$ 的数值稳定方法。这一数值是归一化流中使用的变量替换公式的核心。主要挑战源于浮点运算的有限精度和有限范围，这在计算高维空间中的行列式时可能导致数值上溢或下溢。分析将基于提供的三个基本原则：(i) 行列式的几何解释，(ii) 三角矩阵的行列式，以及 (iii) 关联乘积的对数与对数之和的恒等式。\n\n设一个随机变量 $\\mathbf{z} \\in \\mathbb{R}^d$ 的概率密度函数为 $p_{\\mathbf{z}}(\\mathbf{z})$，通过一个可逆函数 $f: \\mathbb{R}^d \\to \\mathbb{R}^d$ 进行变换，得到一个新的随机变量 $\\mathbf{x} = f(\\mathbf{z})$。$\\mathbf{x}$ 的概率密度函数 $p_{\\mathbf{x}}(\\mathbf{x})$ 由变量替换公式给出：\n$$p_{\\mathbf{x}}(\\mathbf{x}) = p_{\\mathbf{z}}(f^{-1}(\\mathbf{x})) \\left| \\det \\left( \\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|$$\n在归一化流的背景下，更常见的做法是定义前向变换 $\\mathbf{x} = f(\\mathbf{z})$ 并计算其对数似然。设 $J_f = \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}}$ 为前向变换的雅可比矩阵。那么对数密度为：\n$$\\log p_{\\mathbf{x}}(\\mathbf{x}) = \\log p_{\\mathbf{z}}(\\mathbf{z}) - \\log |\\det J_f|$$\n因此，对数行列式项 $\\log |\\det J_f|$ 的高效和稳定计算至关重要。\n\n$\\det J_f$ 的朴素计算涉及许多项的乘积。在浮点运算中，将大量大于 1 的值相乘可能导致上溢（超过可表示的最大数，得到 $\\infty$），而将许多介于 0 和 1 之间的值相乘可能导致下溢（产生一个小于最小正可表示数的数，通常归零）。随后对 $\\infty$ 或 $0$ 应用对数分别得到 $\\infty$ 或 $-\\infty$，导致所有数值信息的丢失。\n\n对于正数 $a_i$，基本恒等式 $\\log\\left(\\prod_{i=1}^n a_i\\right)=\\sum_{i=1}^n\\log a_i$ 为稳定计算提供了关键。通过在计算*之前*将乘积转换为和，我们在对数空间中进行操作。对数之和远比原始数值的乘积更不容易发生上溢或下溢，因为可表示的指数范围远大于尾数的范围。\n\n我们现在将为所要求的特定矩阵结构推导稳定的计算策略。\n\n**1. 对角雅可比矩阵**\n\n考虑一个对角的雅可比矩阵 $J = \\mathrm{diag}(s_1, s_2, \\dots, s_d)$，其中 $s_i \\in \\mathbb{R}$ 是无约束的实值缩放因子。\n矩阵 $J$ 是三角矩阵的一个特例。根据所提供的原则，其行列式是其对角线元素的乘积：\n$$\\det J = \\prod_{i=1}^d s_i$$\n行列式的绝对值为：\n$$|\\det J| = \\left| \\prod_{i=1}^d s_i \\right| = \\prod_{i=1}^d |s_i|$$\n所需的对数行列式是：\n$$\\log |\\det J| = \\log \\left( \\prod_{i=1}^d |s_i| \\right)$$\n**朴素的计算方法**会首先计算乘积 $P = \\prod_{i=1}^d |s_i|$，然后取其对数 $\\log P$。如果 $d$ 很大且 $|s_i|$ 的值持续大于 1 或小于 1，中间乘积 $P$ 容易发生上溢或下溢。\n\n**稳定的计算方法**源于对数-乘积恒等式，它计算各个绝对缩放因子的对数之和：\n$$\\log |\\det J| = \\sum_{i=1}^d \\log |s_i|$$\n此计算避免了巨大或微小的中间乘积，转而对数量级更易于管理的值求和，从而防止了上溢和下溢，并保持了数值精度。\n\n对于深度学习应用，一个常用且有原则的参数化方法是定义缩放因子为指数函数的输出，$s_i = \\exp(\\alpha_i)$，其中 $\\alpha_i \\in \\mathbb{R}$ 是无约束参数（例如，神经网络的输出），以确保可逆性（$s_i \\neq 0$）。这强制 $s_i > 0$，因此 $|s_i| = s_i$。对数行列式的计算进一步简化为：\n$$\\log |\\det J| = \\sum_{i=1}^d \\log(\\exp(\\alpha_i)) = \\sum_{i=1}^d \\alpha_i$$\n对数行列式就是无约束参数的和，这种方法极其稳定且计算高效。\n\n**2. 通过 Cholesky 分解得到的对称正定矩阵**\n\n归一化流中的某些变换涉及对称正定（SPD）矩阵。一种参数化 SPD 矩阵 $A$ 的鲁棒方法是通过其 Cholesky 分解 $A = L L^\\top$，其中 $L$ 是一个具有严格为正的对角线元素（$L_{ii} > 0$）的下三角矩阵。这种参数化方法内在地保证了 $A$ 是 SPD。\n\n我们寻求稳定地计算 $\\log \\det A$。利用行列式的性质 $\\det(XY) = (\\det X)(\\det Y)$ 和 $\\det(X^\\top) = \\det X$，我们有：\n$$\\det A = \\det(L L^\\top) = (\\det L)(\\det L^\\top) = (\\det L)^2$$\n由于 $L$ 是一个下三角矩阵，其行列式是其对角线元素的乘积：\n$$\\det L = \\prod_{i=1}^d L_{ii}$$\n因此，$A$ 的行列式为：\n$$\\det A = \\left( \\prod_{i=1}^d L_{ii} \\right)^2$$\n因为给定所有 $i$ 都有 $L_{ii} > 0$，我们确信 $\\det A > 0$，所以对数是良定义的。\n$$\\log \\det A = \\log \\left( \\left( \\prod_{i=1}^d L_{ii} \\right)^2 \\right) = 2 \\log \\left( \\prod_{i=1}^d L_{ii} \\right)$$\n**朴素的计算方法**可能涉及计算 $A = LL^\\top$，然后计算其行列式（例如，作为其特征值的乘积，或者如果 $A$ 恰好是三角矩阵，则是其对角线元素的乘积），最后取对数。这在中间的行列式计算中重新引入了上溢/下溢的风险。\n\n**稳定的计算方法**直接将对数-乘积恒等式应用于 $\\log \\det A$ 的表达式：\n$$\\log \\det A = 2 \\sum_{i=1}^d \\log(L_{ii})$$\n此计算是鲁棒的，因为它依赖于 $L$ 的对角线元素的对数之和，这些元素可直接从参数化中获得并且保证为正。这避免了构建 $A$ 或计算任何大的中间行列式乘积。这是计算由其 Cholesky 因子表示的矩阵的对数行列式的有原则的策略。\n\n提供的测试套件允许对这些推导进行经验验证，展示了朴素方法在旨在引发上溢和下溢的条件下的失败以及稳定方法的成功。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests numerically stable computations for log-determinants\n    as specified in the problem statement.\n    \"\"\"\n\n    results = []\n\n    # Suppress runtime warnings for overflow/underflow, as they are expected\n    # in the 'naive' computations and are part of the test.\n    with np.errstate(over='ignore', under='ignore', divide='ignore'):\n        # Test Case 1: Happy path diagonal case\n        d1 = 16\n        s1 = np.linspace(0.8, 1.2, d1)\n        # Naive computation: log(product)\n        naive1 = np.log(np.abs(np.prod(s1)))\n        # Stable computation: sum(log)\n        stable1 = np.sum(np.log(np.abs(s1)))\n        # Rule: both finite and agree within a tight tolerance.\n        rule1 = np.isfinite(naive1) and np.isfinite(stable1) and np.isclose(naive1, stable1, atol=1e-10)\n        results.append(rule1)\n\n        # Test Case 2: Overflow diagonal case\n        d2 = 1024\n        s2 = np.full(d2, 10.0**10)\n        # Naive computation: Product will overflow to inf. log(inf) is inf.\n        naive2 = np.log(np.prod(s2))\n        # Stable computation: Sum of logs remains finite.\n        stable2 = np.sum(np.log(s2))\n        # Rule: naive is non-finite, stable is finite.\n        rule2 = not np.isfinite(naive2) and np.isfinite(stable2)\n        results.append(rule2)\n\n        # Test Case 3: Underflow diagonal case\n        d3 = 1024\n        s3 = np.full(d3, 10.0**-10)\n        # Naive computation: Product will underflow to 0. log(0) is -inf.\n        naive3 = np.log(np.prod(s3))\n        # Stable computation: Sum of logs remains finite.\n        stable3 = np.sum(np.log(s3))\n        # Rule: naive is non-finite, stable is finite.\n        rule3 = not np.isfinite(naive3) and np.isfinite(stable3)\n        results.append(rule3)\n\n        # Test Case 4: Cholesky overflow case\n        d4 = 256\n        L_diag4 = np.full(d4, 10.0**10)\n        # In this case, L is a diagonal matrix. A = L L^T = L^2.\n        # The diagonal of A consists of the squares of the diagonal of L.\n        A_diag4 = L_diag4**2\n        # Naive computation (as per problem): product of A's diagonal entries.\n        # This product (10^20)^256 = 10^5120 will overflow.\n        naive4 = np.log(np.prod(A_diag4))\n        # Stable computation: 2 * sum(log of L's diagonal entries).\n        stable4 = 2 * np.sum(np.log(L_diag4))\n        # Rule: naive is non-finite, stable is finite.\n        rule4 = not np.isfinite(naive4) and np.isfinite(stable4)\n        results.append(rule4)\n\n        # Test Case 5: Signed diagonal overflow case\n        d5 = 1024\n        s5 = np.full(d5, 10.0**10)\n        # s_i = (-1)^i * 10^10 for i=0, 1, ...\n        s5[1::2] *= -1\n        # Naive computation: product of magnitudes. Same as case 2.\n        naive5 = np.log(np.prod(np.abs(s5)))\n        # Stable computation: sum of log magnitudes. Same as case 2.\n        stable5 = np.sum(np.log(np.abs(s5)))\n        # Rule: naive is non-finite, stable is finite.\n        rule5 = not np.isfinite(naive5) and np.isfinite(stable5)\n        results.append(rule5)\n\n    # Final print statement in the exact required format.\n    # Python's str() on booleans yields \"True\" or \"False\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160140"}, {"introduction": "掌握了如何稳定地计算对数行列式后，我们可以着手构建并训练一个完整的归一化流模型。本练习将使用仿射耦合层 (affine coupling layer)—— 一种简单而有效的流模型架构，并要求你从最大似然估计的目标函数出发，手动推导训练所需的梯度。通过从零开始实现并分析这个模型的训练动态，你将深入理解归一化流的优化过程以及如何诊断潜在的训练不稳定性。", "problem": "您将实现并分析一个在恒等变换附近初始化的单层归一化流 (NF)，通过追踪雅可比行列式绝对值对数之和在训练周期（epochs）中的漂移来研究训练稳定性。该流作用于二维数据，并使用一个简单的仿射耦合变换，其恒等参数在初始化时全为零。您的程序必须从第一性原理计算训练动态，并报告每个测试用例的稳定性判定结果。\n\n定义与设置：\n- 令 $x \\in \\mathbb{R}^2$ 表示一个数据向量，其分量为 $x_1$ 和 $x_2$。定义一个参数化映射 $g_{\\theta}: \\mathbb{R}^2 \\to \\mathbb{R}^2$ 为\n  $$\n  z_1 = x_1, \\quad z_2 = x_2 \\cdot \\exp(s(x_1)) + t(x_1),\n  $$\n  其中 $s(x_1) = a x_1 + b$ 且 $t(x_1) = c x_1 + d$。参数向量为 $\\theta = (a,b,c,d)$。该映射的雅可比矩阵 $J_g(x)$ 是一个三角矩阵，其行列式为\n  $$\n  \\det J_g(x) = \\exp(s(x_1)),\n  $$\n  因此 $\\log|\\det J_g(x)| = s(x_1)$。\n- 基准密度 $p_Z(z)$ 是二维标准正态分布，即 $Z \\sim \\mathcal{N}(0, I_2)$。数据对数密度的变量替换公式为\n  $$\n  \\log p_X(x) = \\log p_Z\\!\\left(g_{\\theta}(x)\\right) + \\log \\left|\\det J_g(x)\\right|.\n  $$\n- 训练目标是在一个固定的数据集 $\\{x^{(i)}\\}_{i=1}^N$（其中 $N = 256$）上最大化平均对数似然。使用全批量梯度上升法，学习率为 $\\alpha$，进行指定数量的训练周期。您必须根据这些定义推导精确梯度；不要使用自动微分或任何外部近似方法。将参数初始化为接近恒等变换的值：$a=0$, $b=0$, $c=0$, $d=0$。\n- 对于每个训练周期 $e \\in \\{0,1,2,\\dots,E\\}$，定义雅可比行列式绝对值对数的批量和为\n  $$\n  S_e = \\sum_{i=1}^N s\\!\\left(x^{(i)}_1\\right).\n  $$\n  经过 $E$ 个训练周期后的漂移是其变化量\n  $$\n  D = S_E - S_0.\n  $$\n  一个测试用例的训练稳定性准则定义为布尔条件 $|\\D| \\le \\tau$，其中 $\\tau$ 是给定的容差。\n\n每个测试的数据生成：\n- 对于每个测试用例，使用提供的随机种子生成 $N=256$ 个独立样本。每个样本从一个零均值高斯分布中抽取，该高斯分布具有对角协方差 $\\mathrm{diag}(1, 2)$，即 $x_1 \\sim \\mathcal{N}(0, 1)$ 和 $x_2 \\sim \\mathcal{N}(0, 2)$ 独立同分布。\n\n必需的实现细节：\n- 基于上述定义，对平均对数似然 $\\frac{1}{N}\\sum_{i=1}^N \\left[ \\log p_Z\\!\\left(g_{\\theta}\\!\\left(x^{(i)}\\right)\\right) + \\log \\left| \\det J_g\\!\\left(x^{(i)}\\right) \\right| \\right]$ 实现全批量梯度上升，其中 $Z \\sim \\mathcal{N}(0, I_2)$ 且 $g_{\\theta}$ 如上指定。使用这些定义所蕴含的精确解析梯度。\n- 在恒等变换处初始化 $\\theta$：$a=0$, $b=0$, $c=0$, $d=0$。将训练周期 $e=0$ 视为任何参数更新之前的状态。执行 $E$ 个训练周期的参数更新，在初始化时测量 $S_0$，在最后一次参数更新后测量 $S_E$。\n- 对于每个测试用例，输出由 $|\\D| \\le \\tau$ 定义的布尔稳定性判定结果。\n\n测试套件：\n- 使用以下五个测试用例。每个测试用例是一个元组 $(\\text{seed}, \\alpha, E, \\tau)$：\n  $$\n  (\\;7,\\; 0.0005,\\; 100,\\; 20.0\\;)\n  $$\n  $$\n  (\\;7,\\; 0.0100,\\; 100,\\; 100.0\\;)\n  $$\n  $$\n  (\\;42,\\; 0.0050,\\; 20,\\; 30.0\\;)\n  $$\n  $$\n  (\\;123,\\; 0.0000,\\; 100,\\; 10^{-9}\\;)\n  $$\n  $$\n  (\\;999,\\; 0.0010,\\; 0,\\; 10^{-12}\\;)\n  $$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含五个测试用例的结果，格式为方括号内逗号分隔的布尔值列表，且无空格，例如\n  $$\n  [\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{False}]\n  $$\n- 必须有且仅有五个条目，与上述五个测试用例一一对应且顺序相同，并且输出必须通过单次调用 print 函数生成。不应打印任何额外文本。", "solution": "用户的请求已经过验证，并被确定为深度学习领域一个有效且定义良好的问题。它要求从第一性原理实现一个简单的归一化流并分析其训练动态。\n\n### 问题阐述与模型定义\n\n该问题涉及一个单层归一化流，它将一个二维数据变量 $x = (x_1, x_2)^T \\in \\mathbb{R}^2$ 映射到一个潜在变量 $z = (z_1, z_2)^T \\in \\mathbb{R}^2$。变换 $g_{\\theta}: \\mathbb{R}^2 \\to \\mathbb{R}^2$ 是一个仿射耦合层，定义如下：\n$$\nz_1 = x_1\n$$\n$$\nz_2 = x_2 \\cdot \\exp(s(x_1)) + t(x_1)\n$$\n尺度函数 $s(x_1)$ 和平移函数 $t(x_1)$ 是 $x_1$ 的简单仿射函数，由参数 $\\theta = (a, b, c, d)$ 决定：\n$$\ns(x_1) = a x_1 + b\n$$\n$$\nt(x_1) = c x_1 + d\n$$\n模型在对应于恒等变换的参数处进行初始化，即 $a=0, b=0, c=0, d=0$。\n\n### 雅可比矩阵与对数似然\n\n变量替换公式是归一化流的核心。它通过变换 $g_{\\theta}$ 的雅可比矩阵，将数据 $p_X(x)$ 的概率密度与潜在变量 $p_Z(z)$ 的密度联系起来。雅可比矩阵 $J_g(x)$ 为：\n$$\nJ_g(x) = \\begin{pmatrix} \\frac{\\partial z_1}{\\partial x_1}  \\frac{\\partial z_1}{\\partial x_2} \\\\ \\frac{\\partial z_2}{\\partial x_1}  \\frac{\\partial z_2}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ x_2 \\cdot e^{s(x_1)} \\cdot s'(x_1) + t'(x_1)  e^{s(x_1)} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ a x_2 e^{ax_1+b} + c  e^{ax_1+b} \\end{pmatrix}\n$$\n该矩阵是下三角矩阵，因此其行列式是对角线元素的乘积：\n$$\n\\det J_g(x) = 1 \\cdot e^{ax_1+b} = \\exp(s(x_1))\n$$\n由于 $\\exp(\\cdot)$ 总是正的，对数绝对行列式即为：\n$$\n\\log|\\det J_g(x)| = \\log(\\exp(s(x_1))) = s(x_1)\n$$\n潜在变量 $Z$ 的基准分布是一个标准的二维正态分布，$Z \\sim \\mathcal{N}(0, I_2)$，其对数概率密度函数（忽略常数项）为：\n$$\n\\log p_Z(z) = -\\frac{1}{2} (z_1^2 + z_2^2) + C\n$$\n单个数据点 $x$ 的对数似然由变量替换公式给出：\n$$\n\\mathcal{L}(\\theta; x) = \\log p_X(x) = \\log p_Z(g_{\\theta}(x)) + \\log|\\det J_g(x)|\n$$\n代入 $g_{\\theta}$、$\\log p_Z$ 和对数行列式的表达式，我们得到：\n$$\n\\mathcal{L}(\\theta; x) = C - \\frac{1}{2} \\left[ x_1^2 + (x_2 \\exp(s(x_1)) + t(x_1))^2 \\right] + s(x_1)\n$$\n其中 $C = -\\log(2\\pi)$ 是一个不影响梯度的常数。\n\n### 训练梯度推导\n\n模型通过最大化一批 $N=256$ 个数据点 $\\{x^{(i)}\\}_{i=1}^N$ 的平均对数似然进行训练。目标函数是 $L(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\theta; x^{(i)})$。我们使用学习率为 $\\alpha$ 的全批量梯度上升法：\n$$\n\\theta_{e+1} = \\theta_e + \\alpha \\nabla_{\\theta} L(\\theta_e)\n$$\n为实现此目的，我们必须推导单个样本对数似然 $\\mathcal{L}(\\theta; x)$ 关于 $\\theta = (a, b, c, d)$ 中每个参数的解析梯度。为简化符号，令 $z_2(x) = x_2 \\exp(s(x_1)) + t(x_1)$。\n\n1.  **关于 $a$ 的梯度**：\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{\\partial}{\\partial a} \\left( -\\frac{1}{2} z_2(x)^2 + s(x_1) \\right) = -z_2(x) \\frac{\\partial z_2(x)}{\\partial a} + \\frac{\\partial s(x_1)}{\\partial a}\n    $$\n    $$\n    \\frac{\\partial s(x_1)}{\\partial a} = x_1, \\quad \\frac{\\partial z_2(x)}{\\partial a} = x_2 \\exp(s(x_1)) \\frac{\\partial s(x_1)}{\\partial a} = x_1 x_2 \\exp(s(x_1))\n    $$\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial a} = x_1 - z_2(x) \\cdot x_1 x_2 \\exp(s(x_1))\n    $$\n\n2.  **关于 $b$ 的梯度**：\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial b} = -z_2(x) \\frac{\\partial z_2(x)}{\\partial b} + \\frac{\\partial s(x_1)}{\\partial b}\n    $$\n    $$\n    \\frac{\\partial s(x_1)}{\\partial b} = 1, \\quad \\frac{\\partial z_2(x)}{\\partial b} = x_2 \\exp(s(x_1)) \\frac{\\partial s(x_1)}{\\partial b} = x_2 \\exp(s(x_1))\n    $$\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial b} = 1 - z_2(x) \\cdot x_2 \\exp(s(x_1))\n    $$\n\n3.  **关于 $c$ 的梯度**：\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial c} = -z_2(x) \\frac{\\partial z_2(x)}{\\partial c} = -z_2(x) \\cdot \\frac{\\partial t(x_1)}{\\partial c} = -z_2(x) \\cdot x_1\n    $$\n\n4.  **关于 $d$ 的梯度**：\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial d} = -z_2(x) \\frac{\\partial z_2(x)}{\\partial d} = -z_2(x) \\cdot \\frac{\\partial t(x_1)}{\\partial d} = -z_2(x) \\cdot 1\n    $$\n\n全批量梯度是这些单个样本梯度在数据集上的平均值。\n\n### 稳定性分析\n\n通过追踪对数行列式项的批量和的漂移来分析训练的稳定性。在训练周期 $e$，参数为 $\\theta_e = (a_e, b_e, c_e, d_e)$ 时，这个和是：\n$$\nS_e = \\sum_{i=1}^N s_e(x_1^{(i)}) = \\sum_{i=1}^N (a_e x_1^{(i)} + b_e) = a_e \\sum_{i=1}^N x_1^{(i)} + N b_e\n$$\n经过 $E$ 个训练周期的漂移定义为 $D = S_E - S_0$。由于训练从恒等变换开始，$\\theta_0 = (0,0,0,0)$，初始尺度函数为 $s_0(x_1) = 0$。因此，$S_0 = \\sum_{i=1}^N 0 = 0$。漂移简化为 $D = S_E$。稳定性准则是布尔条件 $|D| \\le \\tau$，其中 $\\tau$ 是给定的容差。\n\n对于每个测试用例，算法按以下步骤进行：\n1.  使用提供的随机种子，生成一个包含 $N=256$ 个样本 $\\{x^{(i)}\\}$ 的固定数据集，其中 $x_1^{(i)} \\sim \\mathcal{N}(0, 1)$ 和 $x_2^{(i)} \\sim \\mathcal{N}(0, 2)$。\n2.  初始化参数 $\\theta_0 = (0,0,0,0)$。\n3.  执行 $E$ 次全批量梯度上升迭代，在每一步中使用学习率 $\\alpha$ 和推导出的梯度更新参数 $\\theta$。\n4.  在 $E$ 个训练周期后，使用最终参数 $\\theta_E=(a_E, b_E, c_E, d_E)$ 计算 $S_E$。\n5.  计算漂移 $D=S_E$ 并评估稳定性判定结果 $|D| \\le \\tau$。\n最终输出是这些布尔判定结果的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the normalizing flow stability problem for a suite of test cases.\n    \"\"\"\n    \n    # Test cases: (seed, alpha, E, tau)\n    test_cases = [\n        (7, 0.0005, 100, 20.0),\n        (7, 0.0100, 100, 100.0),\n        (42, 0.0050, 20, 30.0),\n        (123, 0.0000, 100, 10**-9),\n        (999, 0.0010, 0, 10**-12),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        seed, alpha, E, tau = case\n        \n        # --- Data Generation ---\n        N = 256\n        np.random.seed(seed)\n        # x_1 ~ N(0, 1), x_2 ~ N(0, 2)\n        # In numpy, scale is standard deviation, so sqrt(variance).\n        x1 = np.random.normal(loc=0.0, scale=1.0, size=N)\n        x2 = np.random.normal(loc=0.0, scale=np.sqrt(2.0), size=N)\n        \n        # --- Initialization ---\n        # Parameters theta = (a, b, c, d)\n        a, b, c, d = 0.0, 0.0, 0.0, 0.0\n        \n        # --- Training Loop (Full-batch Gradient Ascent) ---\n        for _ in range(E):\n            # Forward pass\n            s_x1 = a * x1 + b\n            t_x1 = c * x1 + d\n            exp_s = np.exp(s_x1)\n            \n            # z_1 = x_1 is implicit\n            z2 = x2 * exp_s + t_x1\n            \n            # Compute gradients of the mean log-likelihood\n            # grad_L = (1/N) * sum(grad_l)\n            grad_a = np.mean(x1 - z2 * x1 * x2 * exp_s)\n            grad_b = np.mean(1.0 - z2 * x2 * exp_s)\n            grad_c = np.mean(-z2 * x1)\n            grad_d = np.mean(-z2)\n            \n            # Update parameters\n            a += alpha * grad_a\n            b += alpha * grad_b\n            c += alpha * grad_c\n            d += alpha * grad_d\n            \n        # --- Stability Analysis ---\n        # S_0 is 0 because initial parameters a=0, b=0.\n        # S_E = sum(a_E * x1_i + b_E) = a_E * sum(x1_i) + N * b_E\n        S_E = a * np.sum(x1) + N * b\n        \n        # Drift D = S_E - S_0 = S_E\n        D = S_E\n        \n        # Stability verdict\n        is_stable = np.abs(D) = tau\n        results.append(is_stable)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3160121"}, {"introduction": "虽然仿射变换层结构简单，但要对复杂数据分布进行建模，我们需要更具表达能力的变换。本练习将引导你实现一个基于有理函数二次样条 (rational-quadratic spline) 的耦合层，它通过拼接简单的有理函数来构建高度灵活、可逆的非线性变换。推导并实现这一先进的层结构，将让你具备理解和构建前沿归一化流模型的能力。", "problem": "您的任务是设计并实现一个一维有理二次样条耦合层，作为深度学习中归一化流的一个组件。归一化流是一种可逆变换 $f : \\mathbb{R}^d \\to \\mathbb{R}^d$，其作用是将一个简单的基础分布转换为一个复杂的目标分布。您必须使用的核心出发点是：(i) 密度的变量替换定理，该定理指出如果 $\\mathbf{y} = f(\\mathbf{x})$，则 $\\log p_{\\mathbf{Y}}(\\mathbf{y}) = \\log p_{\\mathbf{X}}(\\mathbf{x}) - \\log |\\det J_f(\\mathbf{x})|$，其中 $J_f(\\mathbf{x})$ 是 $f$ 在 $\\mathbf{x}$ 处的雅可比矩阵；(ii) 耦合变换架构，其中一个二元掩码将 $\\mathbf{x}$ 划分为不变的分量和由一个可逆函数逐元素变换的分量，这使得雅可比矩阵呈三角形式，从而 $\\log |\\det J_f(\\mathbf{x})|$ 简化为每个坐标的对数导数之和。\n\n在基于样条的耦合层中，每个变换后的标量坐标 $x$ 通过一个在有界区间 $[x_{\\min}, x_{\\max}]$ 上使用 $K$ 个箱（bin）定义的分段函数映射到 $y$。设箱宽度为 $\\{w_i\\}_{i=1}^K$，箱高度为 $\\{h_i\\}_{i=1}^K$，使得 $\\sum_{i=1}^K w_i = x_{\\max} - x_{\\min}$ 和 $\\sum_{i=1}^K h_i = y_{\\max} - y_{\\min}$，并有 $y_{\\min} = x_{\\min}$ 和 $y_{\\max} = x_{\\max}$ 以确保整体范围保持不变。定义节点位置 $\\{x_i\\}_{i=0}^K$ 和 $\\{y_i\\}_{i=0}^K$ 为 $x_0 = x_{\\min}$，$x_{i+1} = x_i + w_{i+1}$ 以及 $y_0 = y_{\\min}$，$y_{i+1} = y_i + h_{i+1}$。对于每个箱 $i$（其中 $i \\in \\{0,1,\\dots,K-1\\}$），在左右箱边缘处指定关于原始坐标 $(x,y)$ 的正端点斜率 $\\{s_i^L, s_i^R\\}$。在区间 $[x_{\\min}, x_{\\max}]$ 之外，映射必须是恒等映射，即 $y = x$。\n\n您的任务是：\n\n1.  在经过归一化变量替换 $t = \\frac{x - x_i}{w_i}$ 和 $u = \\frac{y - y_i}{h_i}$ 后，推导每个箱上的一个闭式的、单调的有理二次函数 $f_i$，该函数满足插值约束 $f_i(0) = 0$，$f_i(1) = 1$，$f_i'(0) = s_i^L \\cdot \\frac{w_i}{h_i}$ 和 $f_i'(1) = s_i^R \\cdot \\frac{w_i}{h_i}$。您必须从约束和有理二次拟设出发，获得计算从 $t$ 到 $u$ 所需系数的显式表达式，然后进行反转以从 $u$ 恢复 $t$。您必须通过强制箱宽度、高度和端点斜率的正性，并使 $[x_{\\min}, x_{\\max}]$ 之外的尾部为恒等映射，来确保科学真实性。\n\n2.  使用变量替换定理和耦合变换的三角雅可比矩阵，从第一性原理出发，推导 $\\log |\\det J_f(\\mathbf{x})|$ 作为每个箱、每个坐标的对数导数之和的表达式。您的推导必须解释为什么雅可比矩阵是三角矩阵，以及每个箱的导数如何与宽度和高度归一化相结合，从而得到箱内的导数 $\\frac{dy}{dx}$。提供用于评估的最终闭式导数。\n\n3.  实现一个完整的、可运行的程序，该程序：\n    -   构建一个一维有理二次样条耦合层，作用于二维向量 $\\mathbf{x} = (x_0, x_1)$ 的第二个坐标上，并使用一个二元掩码保持 $x_0$ 不变，变换 $x_1$。\n    -   每个变换后的坐标使用 $K$ 个箱，其参数 $(x_{\\min}, x_{\\max})$、箱宽度 $\\{w_i\\}$、箱高度 $\\{h_i\\}$ 和每个箱的端点斜率 $\\{s_i^L, s_i^R\\}$ 均为正值，并如上所述适当求和。\n    -   实现前向变换 $\\mathbf{y} = f(\\mathbf{x})$、反向变换 $\\mathbf{x} = f^{-1}(\\mathbf{y})$ 以及 $\\log |\\det J_f(\\mathbf{x})|$ 的计算，仅使用推导出的闭式表达式，不进行数值优化。\n\n4.  使用以下测试套件验证您的实现，并生成一行包含方括号内逗号分隔列表的结果：\n    -   案例1（域内正常路径）：使用 $d = 2$，$K = 3$，$x_{\\min} = -2$，$x_{\\max} = 2$，均匀的箱宽度和高度，以及节点处的斜率 $\\{s_0, s_1, s_2, s_3\\} = \\{2, 0.5, 2, 0.5\\}$。在 $\\mathbf{x} = (0.25, -0.5)$ 处进行评估。输出一个布尔值，指示 $f^{-1}(f(\\mathbf{x}))$ 是否在 $10^{-9}$ 的容差内返回 $\\mathbf{x}$，以及一个等于 $\\log |\\det J_f(\\mathbf{x})|$ 的浮点数。\n    -   案例2（恒等映射）：相同的 $d$、$K$ 和边界，使用均匀的宽度和高度以及斜率 $\\{1, 1, 1, 1\\}$。在 $\\mathbf{x} = (0.1, 0.3)$ 处进行评估。输出一个布尔值，指示 $f(\\mathbf{x}) = \\mathbf{x}$ 是否在 $10^{-12}$ 的容差内，以及一个等于 $\\log |\\det J_f(\\mathbf{x})|$ 的浮点数。\n    -   案例3（域外尾部）：使用与案例1相同的参数，但 $\\mathbf{x} = (-1.0, 3.0)$，其中变换后的坐标满足 $x_1  x_{\\max}$。输出一个布尔值，指示 $f(\\mathbf{x}) = \\mathbf{x}$ 是否在 $10^{-12}$ 的容差内，以及一个等于 $\\log |\\det J_f(\\mathbf{x})|$ 的浮点数。\n    -   案例4（节点边界）：使用与案例1相同的参数，但在 $\\mathbf{x} = (0.0, -2.0)$ 处进行评估，其中 $x_1 = x_{\\min}$ 恰好在最左侧的节点上。输出一个等于 $\\log |\\det J_f(\\mathbf{x})|$ 的浮点数，以及一个布尔值，指示 $f^{-1}(f(\\mathbf{x}))$ 是否在 $10^{-12}$ 的容差内返回 $\\mathbf{x}$。\n    -   案例5（有限差分导数检验）：使用与案例1相同的参数，在 $\\mathbf{x} = (0.0, 0.1)$ 处进行评估，并将变换坐标的解析导数 $\\frac{dy}{dx}$ 与步长 $\\epsilon = 10^{-6}$ 的对称有限差分估计进行比较。输出一个等于解析导数和数值导数之间绝对误差的浮点数。\n\n您的程序应生成一行输出，其中包含方括号内的逗号分隔列表形式的结果（例如，“[$\\text{result1},\\text{result2},\\dots$]”）。此问题中的所有量均为无量纲实数，不涉及角度。", "solution": "用户提供的问题经评估为**有效**。这是一个在深度学习领域，特别是关于归一化流方面，表述清晰且有科学依据的问题。该问题要求推导并实现一个有理二次样条耦合层，这是一种标准且可验证的技术。所有必需的参数和约束都已提供。我们开始进行解答。\n\n### 基于原理的设计与推导\n\n这个问题的核心是构建一个可逆、可微的分段变换，并计算其雅可比行列式的对数 $\\log |\\det J_f|$。该设计依赖于两个关键原则：变量替换定理和耦合层架构。\n\n**1. 耦合层与雅可比行列式**\n\n耦合层将一个向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 变换为 $\\mathbf{y} \\in \\mathbb{R}^d$，通过使用一个二元掩码来划分 $\\mathbf{x}$ 的维度。一些分量保持不变（恒等变换），而其余分量则由一个函数进行变换，该函数的参数由不变的分量决定。对于本问题，当 $\\mathbf{x} = (x_0, x_1)$ 时，变换为：\n$$\n\\begin{cases}\ny_0 = x_0 \\\\\ny_1 = g(x_1)\n\\end{cases}\n$$\n这里，变换 $g$ 的参数是恒定的，这是对它们依赖于 $x_0$ 的一般情况的简化。该变换 $f$ 的雅可比矩阵 $J_f(\\mathbf{x})$ 是：\n$$\nJ_f(\\mathbf{x}) = \\begin{pmatrix}\n\\frac{\\partial y_0}{\\partial x_0}  \\frac{\\partial y_0}{\\partial x_1} \\\\\n\\frac{\\partial y_1}{\\partial x_0}  \\frac{\\partial y_1}{\\partial x_1}\n\\end{pmatrix} = \\begin{pmatrix}\n1  0 \\\\\n0  g'(x_1)\n\\end{pmatrix}\n$$\n雅可比矩阵是下三角矩阵，这是耦合层的一个关键特性。其行列式是对角线元素的乘积：$\\det J_f(\\mathbf{x}) = 1 \\cdot g'(x_1) = g'(x_1)$。变量替换公式需要 $\\log |\\det J_f(\\mathbf{x})|$。由于变换必须是单调的，我们需要 $g'(x_1)  0$，因此 $\\log |\\det J_f(\\mathbf{x})| = \\log(g'(x_1))$。如果变换应用于多个分量，对数行列式将是所有变换分量 $i$ 的单个对数导数之和，即 $\\sum_i \\log(\\frac{\\partial y_i}{\\partial x_i})$。\n\n**2. 有理二次样条公式**\n\n函数 $g(x_1)$ 是一个分段有理二次样条。对于从 $x_i$ 到 $x_{i+1}$ 的每个箱 $i$，都有一个独特的有理二次函数。在域 $[x_{\\min}, x_{\\max}]$ 之外，$g$ 是恒等函数，即 $g(x_1)=x_1$，所以 $g'(x_1)=1$ 且 $\\log(g'(x_1))=0$。\n\n在箱 $i$ 内部，变换是在归一化坐标中定义的。令 $t = \\frac{x_1 - x_i}{w_i} \\in [0, 1]$ 和 $u = \\frac{y_1 - y_i}{h_i} \\in [0, 1]$。我们需要一个函数 $u = G(t)$ 满足四个边界条件：$G(0)=0$，$G(1)=1$，$G'(0)=d_0$ 和 $G'(1)=d_1$。归一化导数 $d_0$ 和 $d_1$ 通过链式法则与原始导数 $s_i^L = \\frac{dy_1}{dx_1}|_{x_1=x_i}$ 和 $s_i^R = \\frac{dy_1}{dx_1}|_{x_1=x_{i+1}}$ 相关联：\n$$ \\frac{du}{dt} = \\frac{dy_1/h_i}{dx_1/w_i} = \\frac{dy_1}{dx_1} \\frac{w_i}{h_i} $$\n因此，$d_0 = s_i^L \\frac{w_i}{h_i}$ 且 $d_1 = s_i^R \\frac{w_i}{h_i}$。\n\n构建此类样条的一个关键思路是，定义反函数 $t = H(u)$ 然后求解 $u$ 以获得正向函数 $u=G(t)$ 通常更为简单。反函数的导数是正向函数导数的倒数：$H'(u) = \\frac{dt}{du} = 1/G'(t)$。因此，反函数的边界条件是 $H(0)=0$，$H(1)=1$，$H'(0)=1/d_0$ 和 $H'(1)=1/d_1$。\n\n我们对反函数 $H(u)$ 使用有理二次拟设：\n$$ t = H(u) = \\frac{\\alpha u^2 + \\beta u}{\\gamma u^2 + \\delta u + 1} $$\n这种形式已经满足 $H(0)=0$。其余三个条件和一个简化选择决定了系数。一个简化前向传递的常见选择是设置 $\\alpha=0$，使分子变为线性。\n当 $\\alpha=0$ 时：\n1.  $H'(0)=\\beta = 1/d_0$。\n2.  $H(1)=1 \\implies \\beta = \\gamma+\\delta+1$。\n3.  $H'(1)=1/d_1$。导数为 $H'(u) = \\frac{\\beta(\\gamma u^2+\\delta u+1) - \\beta u(2\\gamma u+\\delta)}{(\\gamma u^2+\\delta u+1)^2} = \\frac{\\beta(-\\gamma u^2+1)}{(\\gamma u^2+\\delta u+1)^2}$。\n    在 $u=1$ 处，$H'(1) = \\frac{\\beta(-\\gamma+1)}{(\\gamma+\\delta+1)^2} = \\frac{\\beta(-\\gamma+1)}{\\beta^2} = \\frac{1-\\gamma}{\\beta} = 1/d_1$。\n\n从条件3，我们得到 $1-\\gamma = \\beta/d_1 = (1/d_0)/d_1 = 1/(d_0 d_1)$。这给出 $\\gamma = 1 - 1/(d_0 d_1)$。\n从条件2，我们得到 $\\delta = \\beta - \\gamma - 1 = 1/d_0 - (1 - 1/(d_0 d_1)) - 1 = 1/d_0 + 1/(d_0 d_1) - 2$。\n因此，箱内的反向映射由 $t=H(u)$ 的一个闭式表达式给出，其系数为：\n$$ \\beta = \\frac{1}{d_0}, \\quad \\gamma = 1 - \\frac{1}{d_0 d_1}, \\quad \\delta = \\frac{1}{d_0} + \\frac{1}{d_0 d_1} - 2 $$\n这对于反向传递 $f^{-1}$ 在计算上是高效的。\n\n对于正向传递 $f$，我们必须求解 $t = H(u)$ 以得到 $u$：\n$$ t(\\gamma u^2 + \\delta u + 1) = \\beta u $$\n$$ (t\\gamma) u^2 + (t\\delta - \\beta) u + t = 0 $$\n这是一个形式为 $Au^2+Bu+C=0$ 的标准二次方程，其中 $A=t\\gamma$，$B=t\\delta-\\beta$，$C=t$。$u$ 的解通过二次方程求根公式找到。为保持数值稳定性，尤其是在 $t$ 很小时，我们使用 $u = \\frac{2C}{-B \\pm \\sqrt{B^2-4AC}}$ 的形式。选择正确的根以确保 $u \\in [0,1]$：\n$$ u(t) = \\frac{2t}{-(t\\delta - \\beta) + \\sqrt{(t\\delta - \\beta)^2 - 4t^2\\gamma}} $$\n\n**3. 对数导数计算**\n对数行列式为 $\\log(g'(x_1))$。我们有 $g'(x_1) = \\frac{dy_1}{dx_1} = \\frac{h_i}{w_i} G'(t) = \\frac{h_i}{w_i} \\frac{1}{H'(u)}$。\n对数导数的完整表达式为：\n$$ \\log(g'(x_1)) = \\log\\left(\\frac{h_i}{w_i}\\right) - \\log(H'(u)) $$\n$$ = \\log\\left(\\frac{h_i}{w_i}\\right) - \\left[ \\log(\\beta(-\\gamma u^2+1)) - 2\\log(\\gamma u^2+\\delta u+1) \\right] $$\n为了评估此式，对于给定的 $x_1$，我们首先执行正向传递以找到对应的箱和 $u$ 的值，然后将其代入此公式。\n\n至此，我们完成了实现前向传递、反向传递和对数行列式计算所需的推导。", "answer": "```python\nimport numpy as np\n\nclass RationalQuadraticSplineCoupling:\n    \"\"\"\n    Implements a 1D rational-quadratic spline coupling layer.\n    \n    This implementation follows the formulation where the inverse transformation\n    is defined as a simple rational-linear function, and the forward \n    transformation is found by solving a quadratic equation. This simplifies\n    the design while satisfying all constraints.\n    \"\"\"\n    \n    def __init__(self, K, x_min, x_max, widths, heights, knot_slopes):\n        \"\"\"\n        Initializes the spline parameters.\n        \n        Args:\n            K (int): Number of bins.\n            x_min (float): Lower bound of the spline domain.\n            x_max (float): Upper bound of the spline domain.\n            widths (list or np.ndarray): Widths of the K bins in the domain.\n            heights (list or np.ndarray): Heights of the K bins in the range.\n            knot_slopes (list or np.ndarray): Derivatives at the K+1 knots.\n        \"\"\"\n        self.K = K\n        self.x_min = float(x_min)\n        self.x_max = float(x_max)\n\n        self.widths = np.array(widths, dtype=float)\n        self.heights = np.array(heights, dtype=float)\n        self.knot_slopes = np.array(knot_slopes, dtype=float)\n\n        # Precompute knot coordinates\n        self.x_knots = np.concatenate(([self.x_min], self.x_min + np.cumsum(self.widths)))\n        self.y_knots = np.concatenate(([self.x_min], self.x_min + np.cumsum(self.heights)))\n\n        # Derivatives at left and right of each bin\n        s_L = self.knot_slopes[:-1]\n        s_R = self.knot_slopes[1:]\n        \n        # Normalized derivatives for the forward map g'(t)\n        # We add a small epsilon to prevent division by zero, though problem constraints imply positivity.\n        eps = 1e-9\n        d_fwd_0 = s_L * self.widths / (self.heights + eps)\n        d_fwd_1 = s_R * self.widths / (self.heights + eps)\n\n        # Normalized derivatives for the inverse map h'(u)\n        d_inv_0 = 1.0 / (d_fwd_0 + eps)\n        d_inv_1 = 1.0 / (d_fwd_1 + eps)\n        \n        # Coefficients for the inverse map h(u) = (beta * u) / (gamma * u^2 + delta * u + 1)\n        self.beta = d_inv_0\n        self.gamma = 1.0 - (d_inv_0 * d_inv_1)\n        self.delta = d_inv_0 + 1.0 / (d_fwd_0 * d_fwd_1 + eps) - 2.0\n\n    def _find_bin(self, z, knots):\n        # np.searchsorted finds insertion indices to maintain order.\n        # Subtracting 1 gives the index of the bin z falls into.\n        bin_idx = np.searchsorted(knots, z, side='right') - 1\n        return np.clip(bin_idx, 0, self.K - 1)\n\n    def forward(self, x):\n        \"\"\"Computes the forward transformation y = f(x).\"\"\"\n        x0, x1 = x\n        y0 = x0\n\n        if not (self.x_min = x1 = self.x_max):\n            return np.array([y0, x1])\n\n        bin_idx = self._find_bin(x1, self.x_knots)\n        \n        x_k, y_k = self.x_knots[bin_idx], self.y_knots[bin_idx]\n        w_k, h_k = self.widths[bin_idx], self.heights[bin_idx]\n        \n        beta, gamma, delta = self.beta[bin_idx], self.gamma[bin_idx], self.delta[bin_idx]\n        \n        t = (x1 - x_k) / w_k\n\n        # Handle boundary cases for numerical stability\n        if t == 0.0: u = 0.0\n        elif t == 1.0: u = 1.0\n        else:\n            # Solve (t*gamma)u^2 + (t*delta - beta)u + t = 0 for u\n            A = t * gamma\n            B = t * delta - beta\n            C = t\n            # Use stable quadratic formula: u = 2C / (-B + sqrt(B^2 - 4AC))\n            discriminant = np.sqrt(B**2 - 4 * A * C)\n            u = (2 * C) / (-B + discriminant)\n\n        y1 = y_k + u * h_k\n        return np.array([y0, y1])\n\n    def inverse(self, y):\n        \"\"\"Computes the inverse transformation x = f_inv(y).\"\"\"\n        y0, y1 = y\n        x0 = y0\n\n        if not (self.x_min = y1 = self.x_max):\n            return np.array([x0, y1])\n\n        bin_idx = self._find_bin(y1, self.y_knots)\n        \n        x_k, y_k = self.x_knots[bin_idx], self.y_knots[bin_idx]\n        w_k, h_k = self.widths[bin_idx], self.heights[bin_idx]\n        \n        beta, gamma, delta = self.beta[bin_idx], self.gamma[bin_idx], self.delta[bin_idx]\n        \n        u = (y1 - y_k) / h_k\n\n        if u == 0.0: t = 0.0\n        elif u == 1.0: t = 1.0\n        else:\n            # Evaluate t = h(u)\n            t = (beta * u) / (gamma * u**2 + delta * u + 1)\n        \n        x1 = x_k + t * w_k\n        return np.array([x0, x1])\n\n    def log_det_jacobian(self, x):\n        \"\"\"Computes log|det(J_f(x))|.\"\"\"\n        _x0, x1 = x\n        \n        if not (self.x_min = x1 = self.x_max):\n            return 0.0\n\n        bin_idx = self._find_bin(x1, self.x_knots)\n        \n        w_k, h_k = self.widths[bin_idx], self.heights[bin_idx]\n        beta, gamma, delta = self.beta[bin_idx], self.gamma[bin_idx], self.delta[bin_idx]\n        \n        # First, find u corresponding to x1 (part of forward pass)\n        x_k = self.x_knots[bin_idx]\n        t = (x1 - x_k) / w_k\n        \n        if t == 0.0: u = 0.0\n        elif t == 1.0: u = 1.0\n        else:\n            A = t*gamma; B = t*delta - beta; C = t\n            discriminant = np.sqrt(B**2 - 4 * A * C)\n            u = (2 * C) / (-B + discriminant)\n\n        # Derivative of inverse h'(u) = beta * (-gamma*u^2 + 1) / (gamma*u^2 + delta*u + 1)^2\n        h_prime_u_num = beta * (-gamma * u**2 + 1)\n        h_prime_u_den = (gamma * u**2 + delta * u + 1)**2\n        h_prime_u = h_prime_u_num / h_prime_u_den\n        \n        # Derivative dy/dx = (h/w) * (1/h'(u))\n        # log(dy/dx) = log(h/w) - log(h'(u))\n        return np.log(h_k / w_k) - np.log(h_prime_u)\n\ndef solve():\n    results = []\n\n    # Case 1: Happy path inside domain\n    K1, x_min1, x_max1 = 3, -2.0, 2.0\n    domain_width1 = x_max1 - x_min1\n    widths1 = [domain_width1 / K1] * K1\n    heights1 = [domain_width1 / K1] * K1\n    slopes1 = [2.0, 0.5, 2.0, 0.5]\n    coupling1 = RationalQuadraticSplineCoupling(K1, x_min1, x_max1, widths1, heights1, slopes1)\n    x1 = np.array([0.25, -0.5])\n    y1 = coupling1.forward(x1)\n    x1_recon = coupling1.inverse(y1)\n    is_invertible1 = np.allclose(x1, x1_recon, atol=1e-9)\n    log_det1 = coupling1.log_det_jacobian(x1)\n    results.extend([str(is_invertible1), f\"{log_det1:.8f}\"])\n\n    # Case 2: Identity mapping\n    K2, x_min2, x_max2 = 3, -2.0, 2.0\n    domain_width2 = x_max2 - x_min2\n    widths2 = [domain_width2 / K2] * K2\n    heights2 = [domain_width2 / K2] * K2\n    slopes2 = [1.0, 1.0, 1.0, 1.0]\n    coupling2 = RationalQuadraticSplineCoupling(K2, x_min2, x_max2, widths2, heights2, slopes2)\n    x2 = np.array([0.1, 0.3])\n    y2 = coupling2.forward(x2)\n    is_identity2 = np.allclose(x2, y2, atol=1e-12)\n    log_det2 = coupling2.log_det_jacobian(x2)\n    results.extend([str(is_identity2), f\"{log_det2:.8f}\"])\n\n    # Case 3: Outside-domain tail\n    coupling3 = coupling1 # Use same parameters as Case 1\n    x3 = np.array([-1.0, 3.0])\n    y3 = coupling3.forward(x3)\n    is_identity3 = np.allclose(x3, y3, atol=1e-12)\n    log_det3 = coupling3.log_det_jacobian(x3)\n    results.extend([str(is_identity3), f\"{log_det3:.8f}\"])\n    \n    # Case 4: Knot boundary\n    coupling4 = coupling1 # Use same parameters as Case 1\n    x4 = np.array([0.0, -2.0])\n    log_det4 = coupling4.log_det_jacobian(x4)\n    y4 = coupling4.forward(x4)\n    x4_recon = coupling4.inverse(y4)\n    is_invertible4 = np.allclose(x4, x4_recon, atol=1e-12)\n    results.extend([f\"{log_det4:.8f}\", str(is_invertible4)])\n\n    # Case 5: Finite-difference derivative check\n    coupling5 = coupling1 # Use same parameters as Case 1\n    x5_val = 0.1\n    x5 = np.array([0.0, x5_val])\n    analytic_deriv = np.exp(coupling5.log_det_jacobian(x5))\n    eps = 1e-6\n    x5_plus = np.array([0.0, x5_val + eps])\n    x5_minus = np.array([0.0, x5_val - eps])\n    y5_plus = coupling5.forward(x5_plus)\n    y5_minus = coupling5.forward(x5_minus)\n    numerical_deriv = (y5_plus[1] - y5_minus[1]) / (2 * eps)\n    error5 = abs(analytic_deriv - numerical_deriv)\n    results.append(f\"{error5:.8e}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3160093"}]}