{"hands_on_practices": [{"introduction": "确保训练的稳定性是开发生成对抗网络（GAN）时最关键的挑战之一。本练习深入探讨了这一问题的核心，通过比较两种强大的正则化技术：StyleGAN2中使用的R1惩罚项和源自WGAN-GP的梯度惩罚。通过在一个简化的一维环境中实现单步优化，您将具体地理解这些正则化器是如何强制执行利普希茨（Lipschitz）约束并影响判别器稳定性的[@problem_id:3098187]。", "problem": "考虑基于样式的生成对抗网络第二版（StyleGAN2）及其判别器正则化，并与带梯度惩罚的Wasserstein生成对抗网络（WGAN-GP）进行对比。目标是在一个受控的玩具数据集上，分离并比较它们对输入空间利普希茨性（input-space Lipschitzness）和稳定性的影响。我们在一维空间中进行研究，定义一个带有参数 $\\theta = (\\alpha,\\beta,\\gamma)$ 的可微标量判别器函数 $D(x;\\theta)$ 如下：\n$$\nD(x;\\theta) = \\alpha x + \\beta \\sin(\\gamma x),\n$$\n及其输入梯度\n$$\ng(x;\\theta) = \\frac{\\partial D(x;\\theta)}{\\partial x} = \\alpha + \\beta \\gamma \\cos(\\gamma x).\n$$\n\n使用以下基本定义和事实作为您的起点：\n- 一个在域 $\\mathcal{X}$ 上的可微函数 $f$ 的利普希茨常数满足 $L(f) \\le \\sup_{x \\in \\mathcal{X}} \\|\\nabla f(x)\\|_2$。在一维且输出为标量的情况下，这简化为 $L(f) \\le \\sup_{x \\in \\mathcal{X}} |f'(x)|$。\n- WGAN-GP正则化旨在通过惩罚梯度范数与 $1$ 的偏差来近似强制实现 $1$-利普希茨性。惩罚项为\n$$\nP_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left(\\left|g(\\hat{x};\\theta)\\right| - 1\\right)^2,\n$$\n其中 $\\hat{x} = t\\,x_r + (1-t)\\,x_f$，$t \\sim \\mathrm{Uniform}[0,1]$，$x_r \\sim p_r$ 是一个真实样本，$x_f \\sim p_f$ 是一个生成样本。\n- StyleGAN2中使用的R1正则化对真实数据的梯度范数的平方进行惩罚：\n$$\nP_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r \\sim p_r} \\left(g(x_r;\\theta)^2\\right).\n$$\n\n给定真实数据和生成数据的受控单变量高斯分布：\n- 真实数据：$x_r \\sim \\mathcal{N}(\\mu_r,\\sigma_r^2)$。\n- 生成数据：$x_f \\sim \\mathcal{N}(\\mu_f,\\sigma_f^2)$。\n\n考虑仅对正则化器进行一次学习率为 $\\eta$ 的梯度下降步骤（即，忽略对抗性损失），\n$$\n\\theta_{\\mathrm{new}} = \\theta_{\\mathrm{old}} - \\eta \\,\\nabla_{\\theta} P(\\theta_{\\mathrm{old}}),\n$$\n分别应用于 $P_{\\mathrm{R1}}$ 和 $P_{\\mathrm{GP}}$（从相同的 $\\theta_{\\mathrm{old}}$ 开始进行两次独立的更新）。每次更新后，估计：\n- 一个经验利普希茨常数\n$$\n\\widehat{L}(\\theta) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta)|,\n$$\n该常数在一个有限评估集 $\\mathcal{S}$ 上计算，$\\mathcal{S}$ 定义为以下集合的并集：\n    - 在区间 $[a,b]$ 上的 $M$ 个点的均匀网格，其中 $a = \\min(\\mu_r - 4\\sigma_r,\\, \\mu_f - 4\\sigma_f)$ 且 $b = \\max(\\mu_r + 4\\sigma_r,\\, \\mu_f + 4\\sigma_f)$；\n    - 一批 $N$ 个真实样本 $x_r$；\n    - 一批 $N$ 个生成样本 $x_f$；\n    - 一批 $N$ 个如上定义的插值样本 $\\hat{x}$。\n- 一个关于真实数据的稳定性指数，定义为 $g(x_r;\\theta)$ 的样本方差：\n$$\nS(\\theta) = \\mathrm{Var}\\left[g(x_r;\\theta)\\right],\n$$\n在同一批 $N$ 个真实样本上估计。\n\n实现一个程序，该程序：\n1. 使用指定的分布和每个测试用例的固定随机种子来构建玩具数据集批次。\n2. 使用链式法则和上述定义，从第一性原理计算参数梯度 $\\nabla_{\\theta} P_{\\mathrm{R1}}$ 和 $\\nabla_{\\theta} P_{\\mathrm{GP}}$。\n3. 对每个正则化器分别应用一个梯度下降更新步骤，得到 $\\theta_{\\mathrm{R1}}$ 和 $\\theta_{\\mathrm{GP}}$。\n4. 评估 $\\widehat{L}(\\theta_{\\mathrm{R1}})$、$\\widehat{L}(\\theta_{\\mathrm{GP}})$、$S(\\theta_{\\mathrm{R1}})$ 和 $S(\\theta_{\\mathrm{GP}})$。\n\n您的程序必须使用样本大小为 $N$ 的蒙特卡洛估计来计算期望值，并使用 $M$ 个点的均匀网格来估计利普希茨常数。所有计算都是实值浮点数。不涉及物理单位或角度。所有随机抽样必须通过在生成任何样本前设置指定的种子来确保确定性。\n\n测试套件：\n为以下四个测试用例提供结果，每个用例以参数元组 $(\\alpha_0,\\beta_0,\\gamma_0,\\mu_r,\\sigma_r,\\mu_f,\\sigma_f,\\lambda,\\eta,N,M,\\mathrm{seed})$ 的形式给出：\n- 用例 $1$ (一般非线性): $(\\alpha_0=0.8,\\ \\beta_0=0.5,\\ \\gamma_0=1.5,\\ \\mu_r=0.0,\\ \\sigma_r=1.0,\\ \\mu_f=1.0,\\ \\sigma_f=1.0,\\ \\lambda=10.0,\\ \\eta=0.01,\\ N=4000,\\ M=400,\\ \\mathrm{seed}=42)$。\n- 用例 $2$ (线性判别器边界): $(\\alpha_0=2.0,\\ \\beta_0=0.0,\\ \\gamma_0=1.0,\\ \\mu_r=-0.5,\\ \\sigma_r=0.5,\\ \\mu_f=0.5,\\ \\sigma_f=0.5,\\ \\lambda=5.0,\\ \\eta=0.02,\\ N=4000,\\ M=400,\\ \\mathrm{seed}=0)$。\n- 用例 $3$ (低频非线性): $(\\alpha_0=0.5,\\ \\beta_0=1.0,\\ \\gamma_0=0.1,\\ \\mu_r=0.0,\\ \\sigma_r=1.5,\\ \\mu_f=0.0,\\ \\sigma_f=1.5,\\ \\lambda=2.0,\\ \\eta=0.05,\\ N=4000,\\ M=400,\\ \\mathrm{seed}=123)$。\n- 用例 $4$ (高频非线性): $(\\alpha_0=0.3,\\ \\beta_0=1.0,\\ \\gamma_0=5.0,\\ \\mu_r=0.0,\\ \\sigma_r=1.0,\\ \\mu_f=-1.0,\\ \\sigma_f=1.0,\\ \\lambda=10.0,\\ \\eta=0.01,\\ N=4000,\\ M=400,\\ \\mathrm{seed}=999)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的结果列表。对于每个用例，按顺序 $[\\widehat{L}(\\theta_{\\mathrm{R1}}),\\widehat{L}(\\theta_{\\mathrm{GP}}),S(\\theta_{\\mathrm{R1}}),S(\\theta_{\\mathrm{GP}})]$ 输出包含四个浮点数的列表。将这四个用例的列表聚合到一个外部列表中。最终打印的字符串必须看起来像一个Python的列表之列表，例如 $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$。", "solution": "问题陈述已经过仔细审查，并被确定为有效。它在科学上基于深度学习和优化的原理，特别是关于生成对抗网络（GAN）中的正则化技术。该问题定义明确，提供了一套完整且无歧义的定义、参数和程序。这是一个客观、可形式化的任务，需要推导并实现一个计算实验。\n\n问题的核心是计算单次梯度下降步骤对玩具判别器 $D(x;\\theta)$ 的参数 $\\theta = (\\alpha, \\beta, \\gamma)$ 的影响，该计算针对两种不同的正则化器：R1和WGAN-GP。我们必须首先推导这些正则化惩罚项相对于 $\\theta$ 的梯度。\n\n判别器相对于其输入 $x$ 的梯度由下式给出：\n$$\ng(x;\\theta) = \\frac{\\partial D(x;\\theta)}{\\partial x} = \\alpha + \\beta \\gamma \\cos(\\gamma x)\n$$\n\n为了找到正则化器的梯度 $\\nabla_{\\theta} P(\\theta)$，我们首先需要计算 $g(x;\\theta)$ 相对于 $\\theta$ 中每个参数的偏导数：\n$$\n\\frac{\\partial g}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = 1\n$$\n$$\n\\frac{\\partial g}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = \\gamma \\cos(\\gamma x)\n$$\n使用乘法法则计算关于 $\\gamma$ 的导数：\n$$\n\\frac{\\partial g}{\\partial \\gamma} = \\frac{\\partial}{\\partial \\gamma} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = \\beta \\frac{\\partial}{\\partial \\gamma} \\left( \\gamma \\cos(\\gamma x) \\right) = \\beta \\left( 1 \\cdot \\cos(\\gamma x) + \\gamma \\cdot (-\\sin(\\gamma x) \\cdot x) \\right) = \\beta (\\cos(\\gamma x) - \\gamma x \\sin(\\gamma x))\n$$\n我们将这些偏导数收集到一个向量 $\\nabla_{\\theta} g(x;\\theta) = \\left[ \\frac{\\partial g}{\\partial \\alpha}, \\frac{\\partial g}{\\partial \\beta}, \\frac{\\partial g}{\\partial \\gamma} \\right]^T$ 中。\n\n### R1正则化的梯度\nR1正则化器定义为 $P_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r \\sim p_r} \\left( g(x_r;\\theta)^2 \\right)$。\n我们可以交换梯度和期望算子。应用链式法则，相对于 $\\theta$ 的梯度为：\n$$\n\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r} \\left[ \\nabla_{\\theta} \\left( g(x_r;\\theta)^2 \\right) \\right] = \\lambda \\,\\mathbb{E}_{x_r} \\left[ 2 g(x_r;\\theta) \\nabla_{\\theta} g(x_r;\\theta) \\right]\n$$\n期望 $\\mathbb{E}_{x_r}[\\cdot]$ 使用对一批 $N$ 个真实样本 $\\{x_{r,i}\\}_{i=1}^N$ 的蒙特卡洛估计来近似：\n$$\n\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta) \\approx \\frac{2\\lambda}{N} \\sum_{i=1}^{N} g(x_{r,i};\\theta) \\nabla_{\\theta} g(x_{r,i};\\theta)\n$$\n\n### WGAN-GP正则化的梯度\nWGAN-GP正则化器定义为 $P_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left(\\left|g(\\hat{x};\\theta)\\right| - 1\\right)^2$，其中样本 $\\hat{x}$ 是从真实数据和生成数据之间的插值中抽取的。\n同样使用链式法则，其梯度为：\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ \\nabla_{\\theta} \\left( |g(\\hat{x};\\theta)| - 1 \\right)^2 \\right] = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ 2 \\left( |g(\\hat{x};\\theta)| - 1 \\right) \\nabla_{\\theta} |g(\\hat{x};\\theta)| \\right]\n$$\n使用恒等式 $\\nabla |u| = \\mathrm{sgn}(u) \\nabla u$（对于 $u \\neq 0$）：\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) = 2\\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ \\left( |g(\\hat{x};\\theta)| - 1 \\right) \\mathrm{sgn}(g(\\hat{x};\\theta)) \\nabla_{\\theta} g(\\hat{x};\\theta) \\right]\n$$\n该期望 $\\mathbb{E}_{\\hat{x}}[\\cdot]$ 通过对一批 $N$ 个插值样本 $\\{\\hat{x}_i\\}_{i=1}^N$ 的蒙特卡洛估计来近似：\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) \\approx \\frac{2\\lambda}{N} \\sum_{i=1}^{N} \\left( \\left|g(\\hat{x}_i;\\theta)\\right| - 1 \\right) \\mathrm{sgn}(g(\\hat{x}_i;\\theta)) \\nabla_{\\theta} g(\\hat{x}_i;\\theta)\n$$\n\n### 算法流程\n每个测试用例的总体流程如下：\n1.  初始化参数 $\\theta_{\\mathrm{old}} = (\\alpha_0, \\beta_0, \\gamma_0)$ 并设置随机种子以确保可复现性。\n2.  生成数据批次：$N$ 个真实样本 $x_r \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$，$N$ 个生成样本 $x_f \\sim \\mathcal{N}(\\mu_f, \\sigma_f^2)$，以及 $N$ 个插值系数 $t \\sim \\mathrm{Uniform}[0,1]$。构建 $N$ 个插值样本 $\\hat{x}$。\n3.  通过对真实样本 $x_r$ 进行蒙特卡洛估计，计算梯度 $\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta_{\\mathrm{old}})$。\n4.  执行 R1 梯度下降更新：$\\theta_{\\mathrm{R1}} = \\theta_{\\mathrm{old}} - \\eta \\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta_{\\mathrm{old}})$。\n5.  通过对插值样本 $\\hat{x}$ 进行蒙特卡洛估计，计算梯度 $\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta_{\\mathrm{old}})$。\n6.  执行 GP 梯度下降更新：$\\theta_{\\mathrm{GP}} = \\theta_{\\mathrm{old}} - \\eta \\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta_{\\mathrm{old}})$。\n7.  构建评估集 $\\mathcal{S}$，作为数据范围上的均匀网格与生成的样本批次（$x_r, x_f, \\hat{x}$）的并集。\n8.  通过在评估集 $\\mathcal{S}$ 上找到 $|g(x;\\theta)|$ 的最大值，来为每个更新后的参数集估计利普希茨常数：\n    $$\n    \\widehat{L}(\\theta_{\\mathrm{R1}}) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta_{\\mathrm{R1}})| \\quad \\text{和} \\quad \\widehat{L}(\\theta_{\\mathrm{GP}}) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta_{\\mathrm{GP}})|\n    $$\n9.  通过在真实数据批次上计算 $g(x_r;\\theta)$ 的样本方差，来为每个更新后的参数集估计稳定性指数：\n    $$\n    S(\\theta_{\\mathrm{R1}}) = \\mathrm{Var}[g(x_r;\\theta_{\\mathrm{R1}})] \\quad \\text{和} \\quad S(\\theta_{\\mathrm{GP}}) = \\mathrm{Var}[g(x_r;\\theta_{\\mathrm{GP}})]\n    $$\n10. 整理四个结果指标 $[\\widehat{L}(\\theta_{\\mathrm{R1}}), \\widehat{L}(\\theta_{\\mathrm{GP}}), S(\\theta_{\\mathrm{R1}}), S(\\theta_{\\mathrm{GP}})]$。\n\n为每个提供的测试用例实现此流程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n    test_cases = [\n        # (alpha0, beta0, gamma0, mu_r, sigma_r, mu_f, sigma_f, lambda, eta, N, M, seed)\n        (0.8, 0.5, 1.5, 0.0, 1.0, 1.0, 1.0, 10.0, 0.01, 4000, 400, 42),\n        (2.0, 0.0, 1.0, -0.5, 0.5, 0.5, 0.5, 5.0, 0.02, 4000, 400, 0),\n        (0.5, 1.0, 0.1, 0.0, 1.5, 0.0, 1.5, 2.0, 0.05, 4000, 400, 123),\n        (0.3, 1.0, 5.0, 0.0, 1.0, -1.0, 1.0, 10.0, 0.01, 4000, 400, 999),\n    ]\n\n    # Helper functions for discriminator gradient g(x; theta) and its partials\n    def g(x, theta):\n        alpha, beta, gamma = theta\n        return alpha + beta * gamma * np.cos(gamma * x)\n\n    def dg_dalpha(x, theta):\n        return np.ones_like(x)\n\n    def dg_dbeta(x, theta):\n        alpha, beta, gamma = theta\n        return gamma * np.cos(gamma * x)\n\n    def dg_dgamma(x, theta):\n        alpha, beta, gamma = theta\n        return beta * (np.cos(gamma * x) - gamma * x * np.sin(gamma * x))\n\n    final_results = []\n    for case in test_cases:\n        alpha0, beta0, gamma0, mu_r, sigma_r, mu_f, sigma_f, lam, eta, N, M, seed = case\n        theta_old = np.array([alpha0, beta0, gamma0])\n        \n        # 1. Set seed and generate data\n        rng = np.random.default_rng(seed)\n        x_r = rng.normal(mu_r, sigma_r, N)\n        x_f = rng.normal(mu_f, sigma_f, N)\n        t = rng.uniform(0, 1, N)\n        x_hat = t * x_r + (1 - t) * x_f\n        \n        # 2. Compute R1 update\n        g_xr = g(x_r, theta_old)\n        grad_r1_alpha = (2 * lam * np.mean(g_xr * dg_dalpha(x_r, theta_old)))\n        grad_r1_beta = (2 * lam * np.mean(g_xr * dg_dbeta(x_r, theta_old)))\n        grad_r1_gamma = (2 * lam * np.mean(g_xr * dg_dgamma(x_r, theta_old)))\n        grad_r1 = np.array([grad_r1_alpha, grad_r1_beta, grad_r1_gamma])\n        theta_r1 = theta_old - eta * grad_r1\n        \n        # 3. Compute GP update\n        g_xhat = g(x_hat, theta_old)\n        gp_term = (np.abs(g_xhat) - 1) * np.sign(g_xhat)\n        grad_gp_alpha = (2 * lam * np.mean(gp_term * dg_dalpha(x_hat, theta_old)))\n        grad_gp_beta = (2 * lam * np.mean(gp_term * dg_dbeta(x_hat, theta_old)))\n        grad_gp_gamma = (2 * lam * np.mean(gp_term * dg_dgamma(x_hat, theta_old)))\n        grad_gp = np.array([grad_gp_alpha, grad_gp_beta, grad_gp_gamma])\n        theta_gp = theta_old - eta * grad_gp\n        \n        # 4. Evaluate metrics\n        # Construct evaluation set S\n        a = min(mu_r - 4 * sigma_r, mu_f - 4 * sigma_f)\n        b = max(mu_r + 4 * sigma_r, mu_f + 4 * sigma_f)\n        grid_points = np.linspace(a, b, M)\n        eval_set = np.concatenate([grid_points, x_r, x_f, x_hat])\n        \n        # Empirical Lipschitz constant\n        L_hat_r1 = np.max(np.abs(g(eval_set, theta_r1)))\n        L_hat_gp = np.max(np.abs(g(eval_set, theta_gp)))\n        \n        # Stability index\n        S_r1 = np.var(g(x_r, theta_r1))\n        S_gp = np.var(g(x_r, theta_gp))\n        \n        case_result = [L_hat_r1, L_hat_gp, S_r1, S_gp]\n        final_results.append([f\"{v:.8f}\" for v in case_result])\n\n    # Final print statement in the exact required format\n    # The format requires string representation of list of lists.\n    # e.g., [[1.0, 2.0], [3.0, 4.0]] -> '[[1.0, 2.0],[3.0, 4.0]]'\n    # The map(str,...) and join is a robust way to achieve this.\n    print(f\"[{','.join(map(str, final_results))}]\".replace(\"'\", \"\"))\n\nsolve()\n```", "id": "3098187"}, {"introduction": "像StyleGAN这样的先进架构中的一个关键创新是能够在不同尺度上控制视觉特征，从粗糙的结构到精细的纹理。本实践在一维信号上模拟了StyleGAN的多分辨率噪声注入机制。通过在不同频段选择性地添加噪声，您将定量地测量结构完整性与纹理细节之间的权衡，从而揭示分层合成背后的架构原理[@problem_id:3098258]。", "problem": "您将实现并分析一个受StyleGAN（基于风格的生成对抗网络）启发的简化逐分辨率噪声注入机制。目标是消融不同分辨率下的噪声注入，并定量地衡量其对两个量的影响：纹理真实感和结构稳定性。您将使用一维信号，并运用线性时不变系统和频域分析的原理，实现一个纯粹的数学化、程序化的实验。\n\n从以下基本原理开始：\n- 加性高斯白噪声是一个随机过程，其独立样本抽样自均值为零、方差为 $\\sigma^2$ 的高斯分布。当通过有限冲激响应核进行线性滤波时，输出仍为一个高斯过程，其功率谱密度是输入谱密度乘以滤波器频率响应的幅值平方。\n- 离散傅里叶变换将时域卷积与频域乘法联系起来。令 $\\mathcal{F}\\{x\\}[k]$ 表示 $x[n]$ 的离散傅里叶变换；则 $\\mathcal{F}\\{x \\ast h\\}[k] = \\mathcal{F}\\{x\\}[k] \\cdot \\mathcal{F}\\{h\\}[k]$，其中 $\\ast$ 表示循环卷积。\n- 两个有限长度信号 $x[n]$ 和 $y[n]$ 之间的皮尔逊相关系数定义为 $\\rho(x,y) = \\frac{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})(y[n]-\\bar{y})}{\\sqrt{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})^2}\\sqrt{\\sum_{n=0}^{N-1} (y[n]-\\bar{y})^2}}$，其中 $\\bar{x}$ 和 $\\bar{y}$ 是样本均值。\n- 快速傅里叶变换（FFT）可以高效地计算离散傅里叶变换。离散信号中的高频能量由较大频率索引处的FFT系数的幅值平方所捕获。\n\n按如下方式构建一个简化生成器。\n\n1. 域和基本结构：\n   - 使用信号长度 $N = 512$。\n   - 定义一个结构信号 $s[n]$，它通过以弧度为单位的正弦波之和来编码低频“形状”：\n     $$ s[n] = 0.6 \\sin\\left(\\frac{2\\pi \\cdot 2 \\cdot n}{N}\\right) + 0.4 \\sin\\left(\\frac{2\\pi \\cdot 5 \\cdot n}{N}\\right), \\quad n = 0,1,\\dots,N-1. $$\n     角度以弧度为单位。\n\n2. 多分辨率噪声注入：\n   - 考虑 $L = 4$ 个噪声分辨率，其层级 $\\ell = 0,1,2,3$（从最粗到最细）对应的步幅为 $s_\\ell \\in \\{64, 32, 16, 8\\}$。在每个层级 $\\ell$，抽取一个长度为 $N / s_\\ell$ 的低分辨率高斯白噪声向量 $u_\\ell[m]$，其中 $u_\\ell[m] \\sim \\mathcal{N}(0, 1)$。\n   - 通过最近邻重复法将 $u_\\ell[m]$ 上采样至长度 $N$，重复因子为 $s_\\ell$，生成 $v_\\ell[n]$。\n   - 使用长度为 $s_\\ell$ 的盒式滤波器（离散平均滤波器）对 $v_\\ell[n]$ 进行低通滤波，并采用循环边界条件以避免引入伪高频。令核 $h_\\ell[n]$ 定义为：当 $n \\in \\{0,1,\\dots,s_\\ell-1\\}$ 时 $h_\\ell[n] = 1/s_\\ell$，否则 $h_\\ell[n]=0$。滤波后的噪声为 $w_\\ell = v_\\ell \\ast h_\\ell$（循环卷积）。\n   - 在每个层级以振幅 $a_\\ell \\ge 0$ 注入缩放后的噪声，得到生成器输出\n     $$ x[n] = s[n] + \\sum_{\\ell=0}^{3} a_\\ell \\, w_\\ell[n]. $$\n   - 为所有高斯抽样使用固定的随机种子 $0$ 以保证可复现性。\n\n3. 评价指标：\n   - 结构稳定性分数 $\\mathrm{S}(x)$ 是皮尔逊相关系数 $\\rho(x, s)$。\n   - 纹理真实感分数 $\\mathrm{R}(x)$ 定义如下。计算 $x[n]$ 的单边实数FFT，不包括直流分量。令 $X[k]$ 表示索引为 $k = 0, 1, \\dots, N/2$ 的单边FFT。定义功率 $P[k] = |X[k]|^2$。令 $k_c = 32$。定义高频功率分数\n     $$ p_{\\mathrm{high}}(x) = \\frac{\\sum_{k=k_c}^{N/2} P[k]}{\\sum_{k=1}^{N/2} P[k]}. $$\n     令 $p^\\star = 0.25$ 为此简化设置中中等纹理信号的典型目标高频分数。真实感分数为\n     $$ \\mathrm{R}(x) = \\max\\left(0, \\, 1 - \\frac{|p_{\\mathrm{high}}(x) - p^\\star|}{p^\\star} \\right). $$\n\n4. 任务：\n   - 对于每个指定的噪声消融设置（振幅向量），生成 $x[n]$ 并计算 $\\mathrm{R}(x)$ 和 $\\mathrm{S}(x)$。\n   - 每种情况的输出是一个对 $[\\mathrm{R}(x), \\mathrm{S}(x)]$，其中每个值都四舍五入到恰好 $4$ 位小数。\n\n5. 测试套件：\n   - 情况A（无噪声）：$[a_0, a_1, a_2, a_3] = [0.0, 0.0, 0.0, 0.0]$。\n   - 情况B（所有尺度均有强噪声）：$[a_0, a_1, a_2, a_3] = [0.6, 0.6, 0.6, 0.6]$。\n   - 情况C（仅低分辨率）：$[a_0, a_1, a_2, a_3] = [0.6, 0.3, 0.0, 0.0]$。\n   - 情况D（仅高分辨率）：$[a_0, a_1, a_2, a_3] = [0.0, 0.0, 0.3, 0.6]$。\n   - 情况E（均衡温和）：$[a_0, a_1, a_2, a_3] = [0.2, 0.1, 0.1, 0.2]$。\n\n6. 要求的最终输出格式：\n   - 您的程序应生成单行输出，其中包含一个列表，该列表由五个双元素列表组成，顺序为A、B、C、D、E。每个内部列表包含该情况下的两个四舍五入后的浮点数分数 $[\\mathrm{R}(x), \\mathrm{S}(x)]$。外部列表和所有内部列表都必须用方括号括起来，元素之间用逗号分隔，不含空格。例如：\n     $$ [[r_A,s_A],[r_B,s_B],[r_C,s_C],[r_D,s_D],[r_E,s_E]] $$\n     其中每个 $r_\\cdot$ 和 $s_\\cdot$ 都显示为恰好 $4$ 位小数。\n\n您的程序必须是一个完整的、可运行的脚本，它使用固定的随机种子确定性地执行所有计算，并以指定格式精确打印一行输出。不涉及物理单位。正弦波中使用的角度以弧度为单位。百分比（若有）必须以上文已定义的小数形式表示，而不是使用百分号。", "solution": "用户的要求是执行一个计算实验，模拟一个简化的逐分辨率噪声注入机制，这个概念受StyleGAN启发。该任务要求通过将一个确定性的低频结构分量与不同分辨率下的随机噪声分量相结合来生成一维信号。消融这些噪声分量的效果将使用两个自定义指标进行量化：结构稳定性分数和纹理真实感分数。\n\n该问题被确定为 **有效**。这是一个定义明确、有科学依据的计算练习，基于数字信号处理和统计学原理。所有参数、方程和程序都已明确定义，确保了解决方案的唯一性和可验证性。\n\n解决方案按以下步骤进行：\n\n1.  **初始化**：我们按规定定义全局参数。信号长度为 $N = 512$。为保证可复现性，随机数生成器以 $0$ 为种子。四个噪声层级的步幅为 $s_\\ell = [64, 32, 16, 8]$。纹理真实感分数的参数是高频截止索引 $k_c = 32$ 和目标高频功率分数 $p^\\star = 0.25$。\n\n2.  **结构信号生成**：根据给定公式合成基础结构信号 $s[n]$，它代表我们生成信号中的低频内容：\n    $$\n    s[n] = 0.6 \\sin\\left(\\frac{4\\pi n}{N}\\right) + 0.4 \\sin\\left(\\frac{10\\pi n}{N}\\right)\n    $$\n    对于 $n = 0, \\dots, N-1$。该信号是确定性的，并作为衡量结构完整性的基准。\n\n3.  **多分辨率噪声合成**：对于 $L=4$ 个分辨率层级中的每一个，都会生成一个相应的滤波后噪声信号 $w_\\ell[n]$。这个过程独立于测试用例，可以预先计算。对于每个层级 $\\ell$：\n    a. 从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取一个长度为 $N/s_\\ell$ 的低分辨率噪声向量 $u_\\ell$。\n    b. 通过最近邻重复法，将该向量以上采样因子 $s_\\ell$ 上采样至长度 $N$，生成 $v_\\ell[n]$。\n    c. 为平滑块状的上采样噪声，$v_\\ell[n]$ 与一个长度为 $s_\\ell$ 的盒式滤波器 $h_\\ell$ 进行循环卷积。核 $h_\\ell$ 是一个平均滤波器，其前 $s_\\ell$ 个样本为 $h_\\ell[n] = 1/s_\\ell$，其余为零。循环卷积 $w_\\ell = v_\\ell \\ast h_\\ell$ 通过卷积定理，使用快速傅里叶变换（FFT）在频域中高效计算：$\\mathcal{F}\\{x \\ast h\\} = \\mathcal{F}\\{x\\} \\cdot \\mathcal{F}\\{h\\}$。\n    $$\n    w_\\ell[n] = \\mathcal{F}^{-1}\\left\\{ \\mathcal{F}\\{v_\\ell\\}[k] \\cdot \\mathcal{F}\\{h_\\ell\\}[k] \\right\\}\n    $$\n\n4.  **测试用例信号生成**：对于由振幅向量 $[a_0, a_1, a_2, a_3]$ 定义的五个测试用例中的每一个，最终输出信号 $x[n]$ 通过结构信号和预计算噪声信号的加权和来合成：\n    $$\n    x[n] = s[n] + \\sum_{\\ell=0}^{3} a_\\ell \\, w_\\ell[n]\n    $$\n\n5.  **指标计算**：对于每个生成的信号 $x[n]$，计算两个指标。\n    a. **结构稳定性 $\\mathrm{S}(x)$**：这是生成信号 $x[n]$ 与原始结构信号 $s[n]$ 之间的皮尔逊相关系数。它量化了在噪声注入后，原始低频结构被保留了多少。值为 $1$ 表示完美保留。\n    $$\n    \\mathrm{S}(x) = \\rho(x, s) = \\frac{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})(s[n]-\\bar{s})}{\\sqrt{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})^2}\\sqrt{\\sum_{n=0}^{N-1} (s[n]-\\bar{s})^2}}\n    $$\n    b. **纹理真实感 $\\mathrm{R}(x)$**：该指标量化了信号中高频内容（或“纹理”）相对于目标值的数量。首先，计算 $x[n]$ 的单边实数FFT以获得频域表示 $X[k]$。功率谱为 $P[k] = |X[k]|^2$。高频功率分数 $p_{\\mathrm{high}}(x)$ 是频率 $k \\ge k_c$ 中的功率与所有非直流频率（$k \\ge 1$）中总功率的比率。\n    $$\n    p_{\\mathrm{high}}(x) = \\frac{\\sum_{k=k_c}^{N/2} P[k]}{\\sum_{k=1}^{N/2} P[k]}\n    $$\n    真实感分数 $\\mathrm{R}(x)$ 衡量 $p_{\\mathrm{high}}(x)$ 与目标分数 $p^\\star$ 的接近程度，并缩放到 $0$ 和 $1$ 之间的范围。\n    $$\n    \\mathrm{R}(x) = \\max\\left(0, \\, 1 - \\frac{|p_{\\mathrm{high}}(x) - p^\\star|}{p^\\star} \\right)\n    $$\n\n6.  **输出格式化**：每个测试用例计算出的分数 $[\\mathrm{R}(x), \\mathrm{S}(x)]$ 四舍五入到 $4$ 位小数，并按要求格式化为单行的列表之列表。\n\n该实现将这些步骤整合到一个脚本中，该脚本确定性地计算所有测试用例的结果，并以指定格式打印它们。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a simplified per-resolution noise injection mechanism\n    inspired by StyleGAN.\n    \"\"\"\n    # 1. Define constants and initialization\n    N = 512\n    strides = [64, 32, 16, 8]\n    L = len(strides)\n    seed = 0\n    rng = np.random.default_rng(seed)\n    # Metric parameters\n    p_star = 0.25\n    k_c = 32\n\n    # 2. Generate the base structural signal s[n]\n    n = np.arange(N)\n    s = 0.6 * np.sin(2 * np.pi * 2 * n / N) + 0.4 * np.sin(2 * np.pi * 5 * n / N)\n\n    # 3. Pre-compute the four filtered noise signals w_ell[n]\n    w_list = []\n    for s_ell in strides:\n        # a. Generate low-resolution Gaussian noise u_ell\n        noise_len = N // s_ell\n        u_ell = rng.standard_normal(size=noise_len)\n\n        # b. Upsample u_ell to v_ell using nearest-neighbor repetition\n        v_ell = np.repeat(u_ell, s_ell)\n\n        # c. Create box filter kernel h_ell and perform circular convolution\n        # using the FFT-based convolution theorem.\n        h_ell = np.zeros(N)\n        h_ell[:s_ell] = 1.0 / s_ell\n        \n        fft_v = np.fft.fft(v_ell)\n        fft_h = np.fft.fft(h_ell)\n        \n        # The result of IFFT on real-signal convolution should be real.\n        # We take .real to discard negligible imaginary parts from numerical error.\n        w_ell = np.fft.ifft(fft_v * fft_h).real\n        w_list.append(w_ell)\n    \n    # 4. Define the test suite\n    test_cases = {\n        'A': [0.0, 0.0, 0.0, 0.0],\n        'B': [0.6, 0.6, 0.6, 0.6],\n        'C': [0.6, 0.3, 0.0, 0.0],\n        'D': [0.0, 0.0, 0.3, 0.6],\n        'E': [0.2, 0.1, 0.1, 0.2]\n    }\n    \n    all_results = []\n    # Process cases in specified order A, B, C, D, E\n    for case_id in sorted(test_cases.keys()):\n        amplitudes = test_cases[case_id]\n        \n        # 5. Generate the final signal x[n] for the current case\n        noise_component = np.zeros(N)\n        for i in range(L):\n            noise_component += amplitudes[i] * w_list[i]\n        x = s + noise_component\n\n        # 6. Compute Structural Stability S(x)\n        # np.corrcoef calculates the Pearson correlation coefficient matrix.\n        # The off-diagonal element [0, 1] is rho(x, s).\n        s_score = np.corrcoef(x, s)[0, 1]\n\n        # 7. Compute Texture Realism R(x)\n        # Use one-sided real FFT for efficiency and correctness.\n        X_fft = np.fft.rfft(x)  # Produces N/2 + 1 complex coefficients\n        \n        # Power spectrum P[k] = |X[k]|^2\n        P = np.abs(X_fft)**2\n        \n        # Calculate high-frequency power and total non-DC power\n        # P[k_c:] sums power from frequency k_c to N/2\n        # P[1:] sums power from frequency 1 to N/2\n        sum_high = np.sum(P[k_c:])\n        sum_total_no_dc = np.sum(P[1:])\n        \n        # Avoid division by zero, though unlikely in this problem setup\n        if sum_total_no_dc == 0:\n            p_high = 0.0\n        else:\n            p_high = sum_high / sum_total_no_dc\n            \n        r_score = max(0.0, 1.0 - abs(p_high - p_star) / p_star)\n\n        # 8. Store the rounded results for the current case\n        all_results.append([round(r_score, 4), round(s_score, 4)])\n\n    # 9. Format and print the final output string exactly as specified\n    output_str_parts = []\n    for r, s in all_results:\n        output_str_parts.append(f\"[{r:.4f},{s:.4f}]\")\n    \n    final_output = \"[\" + \",\".join(output_str_parts) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "3098258"}, {"introduction": "将GAN扩展到能够生成跨越多个类别、多样化且高保真度的图像（如在BigGAN中所见），需要鲁棒的类别条件化机制。本练习从分析层面比较了现代的投影判别器与经典的ACGAN分类器头，特别评估了它们对标签噪声的鲁棒性。通过推导和计算期望梯度对齐度，您将揭示投影方法在大型条件生成任务中表现出卓越性能和稳定性的数学原因[@problem_id:3098265]。", "problem": "要求您定量比较在使用两种不同类别条件机制时，大规模生成对抗网络 (BigGAN) 判别器在对称标签噪声下的鲁棒性：投影判别器与辅助分类器生成对抗网络 (ACGAN) 分类器头。您的程序必须在简化但科学上一致的模型下，仅使用概率和损失函数的基本定义，为每个指定的测试用例计算两种机制下噪声标签梯度与干净标签梯度之间的期望余弦相似度。最终程序必须是一个完整、可运行的程序，并且在没有任何外部输入的情况下产生指定的输出。\n\n基础定义如下：\n- 生成对抗网络 (GAN)：判别器将输入映射到一个标量分数，并通过一个区分真实数据与生成数据的分类损失进行训练。\n- 对称标签噪声：对于一个 $K$ 类问题，观测标签 $\\tilde{y}$ 等于真实标签 $y$ 的概率为 $1 - \\varepsilon$，否则以概率 $\\varepsilon$ 被一个从其余 $K - 1$ 个类别中均匀采样的标签所替代。形式上，对于 $j \\in \\{1, \\dots, K\\}$，令 $t$ 表示真实类别的索引，$e_j$ 表示类别 $j$ 的独热基向量，\n$$\n\\mathbb{P}(\\tilde{y} = y) = 1 - \\varepsilon, \\quad\n\\mathbb{P}(\\tilde{y} = j \\neq y) = \\frac{\\varepsilon}{K - 1}.\n$$\n- Softmax 交叉熵：对于 logits $z \\in \\mathbb{R}^K$，预测的类别概率为 $p_j = \\frac{\\exp(z_j)}{\\sum_{k=1}^K \\exp(z_k)}$，使用观测到的独热标签 $\\hat{y}$ 的交叉熵损失为 $\\ell(z, \\hat{y}) = -\\sum_{j=1}^K \\hat{y}_j \\log p_j$。其关于 logits 的梯度是 $\\nabla_z \\ell = p - \\hat{y}$。\n\n您必须计算两种余弦相似度：\n1. 投影判别器余弦相似度。考虑 BigGAN 投影判别器的类别条件分数 $s(x, y) = f(x) + \\langle v(y), \\phi(x) \\rangle$，其中 $v(y) \\in \\mathbb{R}^K$ 是类别嵌入，$\\phi(x) \\in \\mathbb{R}^K$ 是判别器特征。在 $\\{v(1), \\dots, v(K)\\}$ 构成一组标准正交单位向量的假设下，并使用标准的逻辑对抗损失，由投影项贡献的关于 $\\phi(x)$ 的梯度与 $v(\\tilde{y})$ 成正比。将干净标签梯度方向视为 $g_{\\mathrm{proj,clean}} \\propto v(y)$，噪声标签梯度方向视为 $g_{\\mathrm{proj,noisy}} \\propto v(\\tilde{y})$。将期望噪声梯度定义为在对称标签噪声模型下的期望。您的任务是计算余弦相似度\n$$\n\\cos_{\\mathrm{proj}} = \\frac{\\langle \\mathbb{E}[g_{\\mathrm{proj,noisy}}], g_{\\mathrm{proj,clean}} \\rangle}{\\|\\mathbb{E}[g_{\\mathrm{proj,noisy}}]\\| \\, \\|g_{\\mathrm{proj,clean}}\\|}.\n$$\n\n2. 辅助分类器 (ACGAN) 余弦相似度。ACGAN 分类器头产生 logits $z \\in \\mathbb{R}^K$ 和预测 $p \\in \\Delta^{K-1}$，并使用 softmax 交叉熵在观测到的噪声标签上进行训练。令 $g_{\\mathrm{ACGAN,clean}} = p - y$ 表示关于 logits 的干净标签梯度，令 $g_{\\mathrm{ACGAN,noisy}} = p - \\tilde{y}$ 表示噪声标签梯度。将期望噪声梯度定义为在对称标签噪声模型下的 $\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}] = p - \\mathbb{E}[\\tilde{y}]$。计算余弦相似度\n$$\n\\cos_{\\mathrm{ACGAN}} = \\frac{\\langle \\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}], g_{\\mathrm{ACGAN,clean}} \\rangle}{\\|\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}]\\| \\, \\|g_{\\mathrm{ACGAN,clean}}\\|}.\n$$\n\n在这两种情况下，余弦相似度都是一个在 $[-1, 1]$ 范围内的无量纲量。\n\n实现要求：\n- 仅使用上述定义。对于投影判别器，假设类别嵌入 $v(j)$（对于 $j \\in \\{1, \\dots, K\\}$）是单位范数的标准正交向量，并将来自对抗性损失的梯度标量视为在余弦相似度中被抵消。对于 ACGAN 头，使用 $g_{\\mathrm{ACGAN,clean}} = p - y$ 和 $\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}] = p - \\mathbb{E}[\\tilde{y}]$，其中 $\\mathbb{E}[\\tilde{y}]$ 由对称噪声模型隐含。\n- 对于每个测试用例中指定的参数，所有计算必须是确定性的。\n- 不涉及任何物理单位。\n- 您的程序必须为每个测试用例计算两种余弦相似度，每个测试用例输出一个双元素列表 $[\\cos_{\\mathrm{proj}}, \\cos_{\\mathrm{ACGAN}}]$，每个浮点数四舍五入到六位小数。\n\n测试套件：\n为以下参数集计算结果，其中 $K$ 是类别数，$\\varepsilon$ 是噪声率，$t$ 是真实类别索引（从零开始），$p$ 是预测概率向量：\n1. $K = 10$，$\\varepsilon = 0.2$，$t = 3$，$p_t = 0.9$，且对于所有 $j \\neq t$，$p_j = \\frac{0.1}{9}$。\n2. $K = 10$，$\\varepsilon = 0.0$，$t = 5$，$p_t = 0.7$，且对于所有 $j \\neq t$，$p_j = \\frac{0.3}{9}$。\n3. $K = 10$，$\\varepsilon = 0.9$，$t = 0$，对于所有 $j \\in \\{0, \\dots, 9\\}$，$p_j = 0.1$。\n4. $K = 2$，$\\varepsilon = 0.5$，$t = 1$，$p_0 = 0.3$，$p_1 = 0.7$。\n5. $K = 100$，$\\varepsilon = 0.3$，$t = 42$，$p_t = 0.6$，且对于所有 $j \\neq t$，$p_j = \\frac{0.4}{99}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，并且其本身是一个双元素列表 $[\\cos_{\\mathrm{proj}}, \\cos_{\\mathrm{ACGAN}}]$。例如，打印的行必须如下所示\n$$\n[\\,[c_1^{\\mathrm{proj}}, c_1^{\\mathrm{ACGAN}}],\\,[c_2^{\\mathrm{proj}}, c_2^{\\mathrm{ACGAN}}],\\,\\dots\\,]\n$$\n每个余弦值四舍五入到六位小数，且字符串中没有空格。", "solution": "该问题要求对生成对抗网络 (GAN) 判别器中两种类别条件机制的鲁棒性进行定量比较：投影判别器和辅助分类器 GAN (ACGAN) 头。比较的度量是在对称标签噪声模型下，干净标签梯度与期望噪声标签梯度之间的余弦相似度。该问题是有效的，因为它在既定的机器学习原理中有科学依据，提供了所有必要的信息使其成为一个适定问题，并且其表述是客观的。\n\n令 $K$ 为类别数，$\\varepsilon$ 为对称标签噪声率，$y$ 为真实类别的独热向量，$\\tilde{y}$ 为观测到的（可能含噪声的）标签的独热向量。真实类别索引表示为 $t$。噪声模型由以下公式给出：\n$$\n\\mathbb{P}(\\tilde{y} = y) = 1 - \\varepsilon\n$$\n$$\n\\mathbb{P}(\\tilde{y} = e_j) = \\frac{\\varepsilon}{K - 1} \\quad \\text{for } j \\neq t\n$$\n其中 $e_j$ 是类别 $j$ 的独热基向量。\n\n我们将分别为每种机制推导余弦相似度的解析表达式。\n\n### 1. 投影判别器\n\n类别条件分数为 $s(x, y) = f(x) + \\langle v(y), \\phi(x) \\rangle$，其中 $\\{v(1), \\dots, v(K)\\}$ 是代表类别嵌入的标准正交单位向量集。类别条件项关于特征向量 $\\phi(x)$ 的梯度与所用标签的类别嵌入成正比。\n干净标签梯度方向为 $g_{\\mathrm{proj,clean}} \\propto v(y) = v_t$，其中 $v_t$ 是真实类别 $t$ 的嵌入。\n噪声标签梯度方向为 $g_{\\mathrm{proj,noisy}} \\propto v(\\tilde{y})$。\n\n我们被要求计算 $\\cos_{\\mathrm{proj}} = \\frac{\\langle \\mathbb{E}[g_{\\mathrm{proj,noisy}}], g_{\\mathrm{proj,clean}} \\rangle}{\\|\\mathbb{E}[g_{\\mathrm{proj,noisy}}]\\| \\, \\|g_{\\mathrm{proj,clean}}\\|}$。\n不失一般性地，我们将比例常数设为 1，因为它会被抵消。\n$g_{\\mathrm{proj,clean}} = v_t$。\n$g_{\\mathrm{proj,noisy}} = v(\\tilde{y})$。\n\n首先，我们通过对 $\\tilde{y}$ 的分布求期望来计算期望噪声梯度 $\\mathbb{E}[g_{\\mathrm{proj,noisy}}]$：\n$$\n\\mathbb{E}[g_{\\mathrm{proj,noisy}}] = \\mathbb{E}[v(\\tilde{y})] = \\sum_{j=1}^K \\mathbb{P}(\\text{观测类别是 } j) \\cdot v_j\n$$\n使用对称噪声模型，真实类别为 $t$：\n$$\n\\mathbb{E}[v(\\tilde{y})] = \\mathbb{P}(\\text{观测类别是 } t) \\cdot v_t + \\sum_{j \\neq t} \\mathbb{P}(\\text{观测类别是 } j) \\cdot v_j\n$$\n$$\n\\mathbb{E}[v(\\tilde{y})] = (1 - \\varepsilon) v_t + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} v_j\n$$\n\n现在我们计算余弦相似度公式的各项：\n分子中的内积是：\n$$\n\\langle \\mathbb{E}[g_{\\mathrm{proj,noisy}}], g_{\\mathrm{proj,clean}} \\rangle = \\left\\langle (1 - \\varepsilon) v_t + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} v_j, v_t \\right\\rangle\n$$\n使用嵌入的标准正交性，$\\langle v_j, v_k \\rangle = \\delta_{jk}$ (克罗内克 δ)：\n$$\n= (1 - \\varepsilon) \\langle v_t, v_t \\rangle + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} \\langle v_j, v_t \\rangle = (1 - \\varepsilon) \\cdot 1 + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} \\cdot 0 = 1 - \\varepsilon\n$$\n\n分母中的范数是：\n$\\|g_{\\mathrm{proj,clean}}\\| = \\|v_t\\| = 1$，因为嵌入是单位向量。\n期望噪声梯度的范数是 $\\|\\mathbb{E}[g_{\\mathrm{proj,noisy}}]\\| = \\|\\mathbb{E}[v(\\tilde{y})]\\|$。我们计算其平方值：\n$$\n\\|\\mathbb{E}[v(\\tilde{y})]\\|^2 = \\left\\langle (1 - \\varepsilon) v_t + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} v_j, (1 - \\varepsilon) v_t + \\sum_{k \\neq t} \\frac{\\varepsilon}{K-1} v_k \\right\\rangle\n$$\n使用标准正交性展开：\n$$\n= (1 - \\varepsilon)^2 \\langle v_t, v_t \\rangle + \\sum_{j \\neq t} \\sum_{k \\neq t} \\left(\\frac{\\varepsilon}{K-1}\\right)^2 \\langle v_j, v_k \\rangle = (1 - \\varepsilon)^2 + \\sum_{j \\neq t} \\left(\\frac{\\varepsilon}{K-1}\\right)^2 \\langle v_j, v_j \\rangle\n$$\n由于和式中有 $K-1$ 个 $j=k$ 的项：\n$$\n= (1 - \\varepsilon)^2 + (K - 1) \\left(\\frac{\\varepsilon}{K-1}\\right)^2 = (1 - \\varepsilon)^2 + \\frac{\\varepsilon^2}{K-1}\n$$\n所以，$\\|\\mathbb{E}[g_{\\mathrm{proj,noisy}}]\\| = \\sqrt{(1 - \\varepsilon)^2 + \\frac{\\varepsilon^2}{K-1}}$。\n\n综合这些部分，投影判别器的余弦相似度为：\n$$\n\\cos_{\\mathrm{proj}} = \\frac{1 - \\varepsilon}{\\sqrt{(1 - \\varepsilon)^2 + \\frac{\\varepsilon^2}{K-1}}}\n$$\n该表达式仅取决于类别数 $K$ 和噪声率 $\\varepsilon$。\n\n### 2. 辅助分类器 (ACGAN)\n\nACGAN 头使用 softmax 交叉熵进行训练。损失关于 logits $z$ 的梯度是 $p - \\hat{y}$，其中 $p$ 是 softmax 概率向量，$\\hat{y}$ 是独热标签向量。\n干净标签梯度是 $g_{\\mathrm{ACGAN,clean}} = p - y = p - e_t$。\n噪声标签梯度是 $g_{\\mathrm{ACGAN,noisy}} = p - \\tilde{y}$。\n期望噪声梯度是 $\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}] = \\mathbb{E}[p - \\tilde{y}] = p - \\mathbb{E}[\\tilde{y}]$。\n\n首先，我们找到向量 $\\mathbb{E}[\\tilde{y}]$。我们将此向量表示为 $\\bar{y}$。此向量的第 $j$ 个分量是 $\\bar{y}_j = \\mathbb{E}[\\tilde{y}_j] = \\mathbb{P}(\\text{观测类别是 } j)$。\n根据对称噪声模型，其中 $t$ 是真实类别：\n$$\n\\bar{y}_t = \\mathbb{P}(\\text{观测类别是 } t) = 1 - \\varepsilon\n$$\n$$\n\\bar{y}_j = \\mathbb{P}(\\text{观测类别是 } j) = \\frac{\\varepsilon}{K-1} \\quad \\text{for } j \\neq t\n$$\n所以，$\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}] = p - \\bar{y}$。\n\n因此余弦相似度为：\n$$\n\\cos_{\\mathrm{ACGAN}} = \\frac{\\langle \\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}], g_{\\mathrm{ACGAN,clean}} \\rangle}{\\|\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}]\\| \\, \\|g_{\\mathrm{ACGAN,clean}}\\|} = \\frac{\\langle p - \\bar{y}, p - e_t \\rangle}{\\|p - \\bar{y}\\| \\, \\|p - e_t\\|}\n$$\n该表达式需要针对每个测试用例进行向量计算，使用提供的参数 $K$、$\\varepsilon$、$t$ 和概率向量 $p$。这些向量是：\n- $p$：给定的概率向量。\n- $e_t$：一个 $K$ 维独热向量，在索引 $t$ 处为 1，其他位置为 0。\n- $\\bar{y}$：一个 $K$ 维向量，在索引 $t$ 处为 $1 - \\varepsilon$，其他位置为 $\\frac{\\varepsilon}{K-1}$。\n\n在分母中任一范数为零的情况下，余弦相似度是未定义的。在这种情况下我们将其定义为 0，因为零范数的期望梯度意味着来自标签的方向信号完全丢失。\n\n实现将针对每个测试用例直接计算这些推导出的公式。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the cosine similarities for the specified test cases.\n    \"\"\"\n    \n    test_cases = [\n        (10, 0.2, 3, (0.9, \"distributed\")),\n        (10, 0.0, 5, (0.7, \"distributed\")),\n        (10, 0.9, 0, np.full(10, 0.1)),\n        (2, 0.5, 1, np.array([0.3, 0.7])),\n        (100, 0.3, 42, (0.6, \"distributed\")),\n    ]\n\n    results = []\n\n    for K, epsilon, t_idx, p_config in test_cases:\n        # --- Projection Discriminator Calculation ---\n        if K == 1:\n            cos_proj = 1.0\n        else:\n            numerator_proj = 1 - epsilon\n            denominator_proj = np.sqrt((1 - epsilon)**2 + epsilon**2 / (K - 1))\n            cos_proj = numerator_proj / denominator_proj if denominator_proj != 0 else 0.0\n\n        # --- ACGAN Calculation ---\n        # Build vectors p, e_t, and y_bar\n        if isinstance(p_config, np.ndarray):\n            p = p_config\n        else: # distributed case\n            p_t_val = p_config[0]\n            if K > 1:\n                p = np.full(K, (1.0 - p_t_val) / (K - 1))\n            else:\n                p = np.array([])\n            p[t_idx] = p_t_val\n\n        e_t = np.zeros(K)\n        e_t[t_idx] = 1.0\n\n        if K > 1:\n            y_bar = np.full(K, epsilon / (K - 1))\n        else:\n            y_bar = np.array([1.0])\n        y_bar[t_idx] = 1.0 - epsilon\n        \n        # Calculate gradient vectors and their norms\n        g_clean = p - e_t\n        g_noisy_exp = p - y_bar\n        norm_clean = np.linalg.norm(g_clean)\n        norm_noisy_exp = np.linalg.norm(g_noisy_exp)\n\n        if norm_clean == 0 or norm_noisy_exp == 0:\n            cos_acgan = 0.0\n        else:\n            dot_product = np.dot(g_noisy_exp, g_clean)\n            cos_acgan = dot_product / (norm_noisy_exp * norm_clean)\n        \n        results.append([cos_proj, cos_acgan])\n\n    # Format the final output string as per requirements\n    output_str_parts = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    final_output = f\"[{','.join(output_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3098265"}]}