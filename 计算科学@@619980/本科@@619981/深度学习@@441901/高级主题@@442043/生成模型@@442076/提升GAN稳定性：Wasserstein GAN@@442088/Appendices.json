{"hands_on_practices": [{"introduction": "在我们深入探讨如何将Wasserstein距离用作生成对抗网络（GAN）的损失函数之前，首要任务是建立对这一核心概念的深刻理解。本练习旨在连接$W_1$距离的理论定义与其实际的数值估计方法。通过在一个可控的一维高斯分布环境中推导其精确的闭式解，并将其与基于样本的经验估计值进行比较，你将能够亲手验证理论并洞察其在实践中的应用。[@problem_id:3137294]", "problem": "本题要求您在一个受控设定中，将推土机距离的数学基础与 Wasserstein 生成对抗网络 (WGAN) 中使用的实践估计方法联系起来。在该设定中，真实分布和生成分布都是一维高斯分布。请从 $1$-Wasserstein 距离的基本定义出发，推导出一个标准正态分布与一个任意正态分布之间距离的精确表达式。然后，实现一个模拟一维经验判别器行为的数值估计器，并在多个参数区间上对两者进行比较。\n\n背景与定义：\n- 生成对抗网络 (GAN) 将学习过程框架化为生成器和判别器（或评判家）之间的双人博弈。Wasserstein 生成对抗网络 (WGAN) 使用 $1$-Wasserstein 距离取代 Jensen–Shannon 散度，以提高训练稳定性。\n- 在实数线上，两个概率分布 $p_r$ 和 $p_g$ 之间的 $1$-Wasserstein 距离 $W_1$ 由成本函数为 $c(x,y) = |x-y|$ 的最优传输成本定义。等价地，通过 Kantorovich–Rubinstein (KR) 对偶性，它可以表示为所有 $1$-Lipschitz 函数 $f$ 的上确界： $$W_1(p_r, p_g) = \\sup_{\\|f\\|_{\\mathrm{Lip}} \\le 1} \\mathbb{E}_{X \\sim p_r}[f(X)] - \\mathbb{E}_{Y \\sim p_g}[f(Y)].$$\n- 在一维情况下，一个基本且经过充分验证的事实是最优耦合是单调的，这导出了分位数耦合恒等式 $$W_1(p_r, p_g) = \\int_0^1 \\left| F_r^{-1}(u) - F_g^{-1}(u) \\right| \\, du,$$ 其中 $F^{-1}$ 表示分位数函数（逆累积分布函数 (CDF)）。\n\n问题设置：\n- 设 $p_r = \\mathcal{N}(0, 1)$ 为真实数据分布，$p_g = \\mathcal{N}(\\mu, \\sigma^2)$ 为生成器分布，其参数为 $\\mu \\in \\mathbb{R}$ 和 $\\sigma \\in \\mathbb{R}_{0}$。\n- 您的任务是：\n  1. 根据上述原理，推导 $$W_1\\!\\left(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2)\\right)$$ 的闭式表达式，该表达式仅用 $\\mu$ 和 $\\sigma$ 表示，且不使用任何预设答案的捷径公式。\n  2. 基于一维单调传输，使用来自每个分布的 $n$ 个独立样本，为 $p_r$ 和 $p_g$ 之间的经验 $1$-Wasserstein 距离实现一个数值估计器。具体来说，对于样本大小为 $n$ 的等权重经验分布，经验 $1$-Wasserstein 距离等于两个样本集的顺序统计量之间的平均绝对差。\n  3. 对于几组 $(\\mu, \\sigma)$ 对，比较闭式解的值和经验估计值。\n\n您的程序必须：\n- 计算任务 1 中推导的闭式解的值。\n- 通过从每个分布中抽取 $n$ 个样本，对两组样本进行排序，并计算排序后样本之间绝对差的平均值，来估计任务 2 中的经验值。\n- 使用固定的伪随机种子以确保可复现性。\n- 对于每个测试用例，输出一个浮点数三元组 $[w_{\\mathrm{closed}}, w_{\\mathrm{empirical}}, \\Delta]$，其中 $w_{\\mathrm{closed}}$ 是闭式解的值，$w_{\\mathrm{empirical}}$ 是经验估计值，$\\Delta$ 是绝对差 $|w_{\\mathrm{closed}} - w_{\\mathrm{empirical}}|$。\n\n测试套件：\n- 在每个案例中，使用以下参数集 $(\\mu, \\sigma)$ 和每个分布 $n$ 个样本：\n  1. $(\\mu, \\sigma) = (0, 1)$，基线相同分布。\n  2. $(\\mu, \\sigma) = (1, 1)$，纯均值偏移。\n  3. $(\\mu, \\sigma) = (0, 2)$，纯尺度变化。\n  4. $(\\mu, \\sigma) = (-2, 1.5)$，同时均值和尺度变化。\n  5. $(\\mu, \\sigma) = (0.5, 0.5)$，均值为正且尺度小于 1。\n  6. $(\\mu, \\sigma) = (3, 0.1)$，大均值和极小方差。\n- 每个经验估计使用 $n = 100000$ 个样本和一个固定的种子。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个项目本身是针对一个测试用例的逗号分隔列表。例如：$$[[w_1^{\\mathrm{closed}}, w_1^{\\mathrm{emp}}, \\Delta_1],[w_2^{\\mathrm{closed}}, w_2^{\\mathrm{emp}}, \\Delta_2],\\dots].$$\n- 所有输出必须是浮点数。不适用任何物理单位或角度单位。不要使用百分号；如果出现任何小数值，必须表示为小数。", "solution": "该问题要求对两个一维高斯分布之间的 $1$-Wasserstein 距离进行两部分分析。首先，我们必须从基于分位数的定义出发，推导该距离的闭式表达式。其次，我们要实现一个基于排序后经验样本的数值估计器，并将其结果与理论值进行比较。\n\n**1. $W_1(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2))$ 的闭式表达式推导**\n\n在实数线上，两个概率分布 $p_r$ 和 $p_g$ 之间的 $1$-Wasserstein 距离 $W_1$ 可以使用它们的分位数函数 $F_r^{-1}$ 和 $F_g^{-1}$（逆 CDF）来计算。公式如下：\n$$W_1(p_r, p_g) = \\int_0^1 \\left| F_r^{-1}(u) - F_g^{-1}(u) \\right| \\, du$$\n\n问题将真实分布 $p_r$ 指定为标准正态分布 $p_r = \\mathcal{N}(0,1)$，将生成分布 $p_g$ 指定为一般正态分布 $p_g = \\mathcal{N}(\\mu, \\sigma^2)$，其中 $\\mu$ 是均值，$\\sigma  0$ 是标准差。\n\n首先，我们确定这些分布的分位数函数。设 $\\Phi(z)$ 是标准正态分布 $\\mathcal{N}(0,1)$ 的累积分布函数 (CDF)。\n分位数函数是 CDF 的逆函数。\n- 对于 $p_r = \\mathcal{N}(0,1)$，CDF 为 $F_r(x) = \\Phi(x)$。因此，分位数函数为 $F_r^{-1}(u) = \\Phi^{-1}(u)$。\n- 对于 $p_g = \\mathcal{N}(\\mu, \\sigma^2)$，一个随机变量 $Y \\sim p_g$ 可以表示为 $Y = \\mu + \\sigma Z$，其中 $Z \\sim \\mathcal{N}(0,1)$。其 CDF 为 $F_g(y) = P(Y \\le y) = P(\\mu + \\sigma Z \\le y) = P\\left(Z \\le \\frac{y-\\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right)$。为了求得分位数函数 $F_g^{-1}(u)$，我们令 $F_g(y) = u$ 并求解 $y$：\n$u = \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right) \\implies \\Phi^{-1}(u) = \\frac{y-\\mu}{\\sigma} \\implies y = \\mu + \\sigma \\Phi^{-1}(u)$。\n所以，分位数函数是 $F_g^{-1}(u) = \\mu + \\sigma \\Phi^{-1}(u)$。\n\n将这些分位数函数代入 $W_1$ 的积分中：\n$$W_1(p_r, p_g) = \\int_0^1 \\left| \\Phi^{-1}(u) - (\\mu + \\sigma \\Phi^{-1}(u)) \\right| \\, du$$\n$$W_1(p_r, p_g) = \\int_0^1 \\left| (1-\\sigma)\\Phi^{-1}(u) - \\mu \\right| \\, du$$\n\n为了计算这个积分，我们进行变量替换。设 $z = \\Phi^{-1}(u)$，这意味着 $u = \\Phi(z)$。其微分为 $du = \\phi(z) dz$，其中 $\\phi(z)$ 是标准正态分布的概率密度函数 (PDF)。当 $u$ 从 $0$ 变化到 $1$ 时，$z$ 从 $-\\infty$ 变化到 $\\infty$。积分变为：\n$$W_1(p_r, p_g) = \\int_{-\\infty}^{\\infty} \\left| (1-\\sigma)z - \\mu \\right| \\phi(z) \\, dz$$\n该表达式是期望 $\\mathbb{E}[|(1-\\sigma)Z - \\mu|]$ 的定义，其中 $Z$ 是一个服从 $\\mathcal{N}(0,1)$ 分布的随机变量。\n\n设 $W = (1-\\sigma)Z - \\mu$。由于 $Z$ 是一个正态随机变量，$W$ 也服从正态分布。其均值和方差为：\n- $\\mathbb{E}[W] = \\mathbb{E}[(1-\\sigma)Z - \\mu] = (1-\\sigma)\\mathbb{E}[Z] - \\mu = (1-\\sigma) \\cdot 0 - \\mu = -\\mu$。\n- $\\text{Var}(W) = \\text{Var}[(1-\\sigma)Z - \\mu] = (1-\\sigma)^2 \\text{Var}(Z) = (1-\\sigma)^2 \\cdot 1 = (1-\\sigma)^2$。\n因此，$W \\sim \\mathcal{N}(-\\mu, (1-\\sigma)^2)$。现在的问题是计算 $\\mathbb{E}[|W|]$。这是一个折叠正态分布的一阶矩。\n\n我们分两种情况处理 $\\sigma$：\n情况 1：$\\sigma = 1$。随机变量 $W$ 变为一个常数：$W = (1-1)Z - \\mu = -\\mu$。其期望为 $\\mathbb{E}[|-\\mu|] = |\\mu|$。\n\n情况 2：$\\sigma \\neq 1$。对于变量 $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$，其折叠正态分布的均值由以下公式给出：\n$$\\mathbb{E}[|X|] = \\mu_X \\left(1 - 2\\Phi(-\\mu_X/\\sigma_X)\\right) + \\sigma_X \\sqrt{2/\\pi} \\exp\\left(-\\frac{\\mu_X^2}{2\\sigma_X^2}\\right)$$\n在我们的情境下，$X = W$，因此我们代入 $\\mu_X = -\\mu$ 和 $\\sigma_X = \\sqrt{(1-\\sigma)^2} = |1-\\sigma|$。\n$$W_1 = (-\\mu) \\left(1 - 2\\Phi\\left(-\\frac{-\\mu}{|1-\\sigma|}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{(-\\mu)^2}{2|1-\\sigma|^2}\\right)$$\n$$W_1 = -\\mu \\left(1 - 2\\Phi\\left(\\frac{\\mu}{|1-\\sigma|}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\n利用误差函数 $\\text{erf}(x)$ 和 $\\Phi(x)$ 之间的关系，特别是 $1 - 2\\Phi(z) = -\\text{erf}(z/\\sqrt{2})$，该表达式可简化为：\n$$W_1 = -\\mu \\left(-\\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\n$$W_1 = \\mu \\cdot \\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\n这就是 $\\sigma \\neq 1$ 时的闭式解。\n\n总之，$1$-Wasserstein 距离的闭式表达式为：\n$$ W_1(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2)) = \\begin{cases} |\\mu|  \\text{if } \\sigma = 1 \\\\ \\mu \\cdot \\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)  \\text{if } \\sigma \\neq 1 \\end{cases} $$\n\n**2. 经验 $W_1$ 的数值估计**\n\n该问题提供了一维情况下经验估计 $1$-Wasserstein 距离的方法论。对于大小均为 $n$ 的两组样本，一组来自 $p_r$，另一组来自 $p_g$，经验距离是它们顺序统计量（排序后的样本）之间的平均绝对差。这是分位数恒等式应用于经验累积分布函数的直接结果。\n\n算法如下：\n1. 从真实分布 $p_r = \\mathcal{N}(0,1)$ 中生成 $n$ 个独立样本 $\\{x_1, \\dots, x_n\\}$。\n2. 从生成器分布 $p_g = \\mathcal{N}(\\mu, \\sigma^2)$ 中生成 $n$ 个独立样本 $\\{y_1, \\dots, y_n\\}$。\n3. 对两组样本进行排序以获得顺序统计量：$x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$ 和 $y_{(1)} \\le y_{(2)} \\le \\dots \\le y_{(n)}$。\n4. 经验 $1$-Wasserstein 距离 $\\hat{W}_1$ 计算为相应排序样本之间绝对差的平均值：\n$$\\hat{W}_1(p_r, p_g) = \\frac{1}{n} \\sum_{i=1}^n |x_{(i)} - y_{(i)}|$$\n使用固定的伪随机种子来确保随机样本的可复现性，从而确保经验估计值的可复现性。\n\n最终的程序将为每个指定的 $(\\mu, \\sigma)$ 对实现推导出的闭式计算和经验估计算法，并报告理论值、经验估计值以及它们之间的绝对差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef calculate_closed_form(mu: float, sigma: float) -> float:\n    \"\"\"\n    Computes the closed-form 1-Wasserstein distance between\n    N(0, 1) and N(mu, sigma^2).\n    \"\"\"\n    # The derived formula has a special case for sigma = 1\n    if np.isclose(sigma, 1.0):\n        return np.abs(mu)\n    else:\n        # General formula for sigma != 1\n        abs_one_minus_sigma = np.abs(1.0 - sigma)\n        \n        # Argument for the error function\n        erf_arg = mu / (abs_one_minus_sigma * np.sqrt(2.0))\n        \n        # First term of the sum\n        mu_term = mu * erf(erf_arg)\n        \n        # Second term of the sum\n        sigma_term_factor = abs_one_minus_sigma * np.sqrt(2.0 / np.pi)\n        exp_term = np.exp(-mu**2 / (2.0 * abs_one_minus_sigma**2))\n        sigma_term = sigma_term_factor * exp_term\n        \n        return mu_term + sigma_term\n\ndef calculate_empirical(mu: float, sigma: float, n: int, rng: np.random.Generator) -> float:\n    \"\"\"\n    Computes the empirical 1-Wasserstein distance by sampling.\n    \"\"\"\n    # Generate n samples from the real distribution N(0, 1)\n    samples_r = rng.standard_normal(n)\n    \n    # Generate n samples from the generator distribution N(mu, sigma^2)\n    # Note: rng.standard_normal generates from N(0,1). We scale by sigma and shift by mu.\n    samples_g = rng.standard_normal(n) * sigma + mu\n    \n    # Sort both sets of samples to get the order statistics\n    sorted_r = np.sort(samples_r)\n    sorted_g = np.sort(samples_g)\n    \n    # The empirical distance is the mean absolute difference of the order statistics\n    w_empirical = np.mean(np.abs(sorted_r - sorted_g))\n    \n    return w_empirical\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu, sigma)\n    test_cases = [\n        (0.0, 1.0),\n        (1.0, 1.0),\n        (0.0, 2.0),\n        (-2.0, 1.5),\n        (0.5, 0.5),\n        (3.0, 0.1)\n    ]\n    \n    # Sample size for empirical estimation\n    n_samples = 100000\n    \n    # Fixed seed for reproducibility. Use a modern Generator object.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for mu, sigma in test_cases:\n        # Task 1: Compute the closed-form value\n        w_closed = calculate_closed_form(mu, sigma)\n        \n        # Task 2: Compute the empirical estimate\n        w_empirical = calculate_empirical(mu, sigma, n_samples, rng)\n        \n        # Task 3: Compute the absolute difference\n        delta = np.abs(w_closed - w_empirical)\n        \n        # Store the triplet of floats\n        results.append([w_closed, w_empirical, delta])\n\n    # Final print statement in the exact required format.\n    # The format requires a string representation of a list of lists.\n    # e.g., [[val1, val2, val3],[...]]\n    # map(str, results) converts each inner list to its string form \"[v1, v2, v3]\"\n    # ','.join(...) combines them with commas in between\n    # f\"[{...}]\" wraps the final string in the outer list brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3137294"}, {"introduction": "Wasserstein GAN（WGAN）相比于传统GAN的一个关键优势在于，即使当真实数据分布与生成器分布的支撑集完全不重叠时，它依然能为生成器提供平滑且有意义的梯度。本练习通过一个精心设计的模拟实验，让你直观地观察并理解这一现象。你将通过在一个离散网格上优化评判家（critic）函数，来探索该函数在不同分布的“间隙”区域中的行为，从而揭示WGAN训练稳定性的根本来源。[@problem_id:3137255]", "problem": "构建一个完整、可运行的程序，以经验性地研究在Wasserstein生成对抗网络（WGAN）中，真实数据分布 $p_r$ 和生成器分布 $p_g$ 之间的支撑集不匹配如何影响评判家函数 $f$ 的训练行为。使用Wasserstein-1距离的Kantorovich–Rubinstein对偶刻画作为基本依据：Wasserstein-1距离等于目标函数 $\\,\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f]\\,$ 在所有1-Lipschitz函数 $f$ 上取得的上确界。在一维空间中进行，并在一个均匀离散网格上近似 $f$。\n\n您的程序必须实现以下纯粹的数学和算法过程：\n\n- 在有界区间 $[x_{\\min}, x_{\\max}]$ 上定义一个包含 $N$ 个点、间距为 $\\Delta x$ 的均匀网格。将 $f$ 表示为此网格上的值 $\\{f(x_i)\\}_{i=1}^N$。\n- 将 $p_r$ 和 $p_g$ 定义为 $[x_{\\min}, x_{\\max}]$ 中闭区间并集上的均匀分布。在网格上，通过为位于 $p_r$（或相应地，$p_g$）支撑集内的每个网格点分配相等的权重来近似 $\\mathbb{E}_{p_r}[f]$ 和 $\\mathbb{E}_{p_g}[f]$，并进行归一化，使得每个分布的权重之和为1。\n- 通过对离散目标函数 $L(f) = \\sum_{i=1}^N f(x_i)\\,w_i$（其中 $w_i = w^{(r)}_i - w^{(g)}_i$，$w^{(r)}_i$ 和 $w^{(g)}_i$ 分别是 $x_i$ 处的真实权重和生成器权重）执行投影梯度上升来优化评判家。使用步长为 $\\eta$ 的更新 $f \\leftarrow f + \\eta \\, w$，然后通过对所有相邻网格点强制执行有界差分约束 $|f(x_i) - f(x_{i-1})| \\le \\Delta x$ 并固定 $f(x_1) = 0$ 来解决加性规范自由度，从而投影到1-Lipschitz函数集合上。投影过程必须通过裁剪相邻差分并通过累积求和重构 $f$ 来进行，确保对所有 $i$ 都有 $|f(x_i) - f(x_{i-1})| \\le \\Delta x$ 成立。\n- 优化后，为每个测试用例计算三个定量指标：\n    $1.$ 近似的Wasserstein-1目标值 $\\,\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f] = \\sum_{i=1}^N f(x_i)\\,w_i\\,$，以浮点数形式报告。\n    $2.$ 在 $p_r$ 和 $p_g$ 均无支撑的间隙区域内，$f$ 在网格边上的归一化平均绝对斜率。具体来说，计算相邻差分 $s_i = f(x_{i+1}) - f(x_i)$，将其限制在 $x_i$ 和 $x_{i+1}$ 均位于 $p_r$ 和 $p_g$ 支撑集之外的索引 $i$ 上，然后对这些边计算 $\\frac{1}{M}\\sum |s_i|/\\Delta x$（其中 $M$ 是此类边的数量）。如果没有此类边，则将此指标定义为 $0$。将此归一化平均值以 $[0,1]$ 范围内的浮点数形式报告。\n    $3.$ 最左侧真实区间和生成器区间中点之间的评判家对比度，定义为 $f(m_r) - f(m_g)$，其中 $m_r$ 是 $p_r$ 最左侧区间的中点，$m_g$ 是 $p_g$ 最左侧区间的中点。在最接近 $m_r$ 和 $m_g$ 的网格点上评估 $f$。以浮点数形式报告此值。\n\n设计一个测试套件，用于探究 $p_r$ 和 $p_g$ 之间不同的支撑集关系。使用以下用例：\n\n- 用例 $1$ (完全不相交的支撑集，带有一个间隙): 定义域 $[0,3]$，$N=401$；$p_r$ 支撑集 $[0,1]$；$p_g$ 支撑集 $[2,3]$。\n- 用例 $2$ (具有交替不相交分量的双分量支撑集): 定义域 $[0,5]$，$N=401$；$p_r$ 支撑集 $[0,1]$ 和 $[3,4]$；$p_g$ 支撑集 $[1,2]$ 和 $[4,5]$。\n- 用例 $3$ (完全重叠): 定义域 $[0,2]$，$N=401$；$p_r$ 支撑集 $[0.5,1.5]$；$p_g$ 支撑集 $[0.5,1.5]$。\n- 用例 $4$ (边界附近的不相交小区间): 定义域 $[0,1]$，$N=401$；$p_r$ 支撑集 $[0,0.2]$；$p_g$ 支撑集 $[0.8,1.0]$。\n- 用例 $5$ (交错的多分量支撑集): 定义域 $[0,3]$，$N=401$；$p_r$ 支撑集 $[0,0.5]$ 和 $[1.5,2.0]$；$p_g$ 支撑集 $[0.5,1.0]$ 和 $[2.5,3.0]$。\n\n对于优化，使用 $T=800$ 次上升-投影迭代和步长 $\\eta=5.0$。这些值固定了计算过程并确保了可复现性。\n\n您的程序必须生成单行输出，其中包含按顺序汇总的所有测试用例的结果，格式为用方括号括起来的、逗号分隔的列表的列表。每个内部列表必须按上述顺序包含三个浮点数，并且必须四舍五入到4位小数。例如，输出格式应与\n$[[a_1,b_1,c_1],[a_2,b_2,c_2],\\ldots,[a_5,b_5,c_5]]$ 完全一样，\n行内任何地方都没有空格。\n\n此问题不涉及物理单位，也不使用角度。所有结果必须按规定以浮点数或浮点数列表的形式报告。程序必须是独立自足的，不需要用户输入或外部文件。", "solution": "用户要求对一维Wasserstein生成对抗网络（WGAN）中判别器函数的行为进行计算研究。该问题是有效的，其科学基础在于最优传输理论和约束优化，并为算法及其测试用例提供了完整、良态的规范。我们将着手提供一个完整的解决方案。\n\n这个问题的基本原理是Kantorovich-Rubinstein对偶性，它为两个概率分布 $p_r$（真实数据）和 $p_g$（生成器）之间的Wasserstein-1距离（也称为推土机距离）提供了一个公式。该对偶性表述如下：\n$$\nW_1(p_r, p_g) = \\sup_{\\|f\\|_L \\le 1} \\left( \\mathbb{E}_{x \\sim p_r}[f(x)] - \\mathbb{E}_{x \\sim p_g}[f(x)] \\right)\n$$\n在这里，上确界是在所有1-Lipschitz函数 $f: \\mathcal{X} \\to \\mathbb{R}$ 上取得的，这些函数满足对域 $\\mathcal{X}$ 中的所有 $x, y$ 都有 $|f(x) - f(y)| \\le |x - y|$。在WGAN的背景下，这个函数 $f$ 被称为评判家。评判家的训练过程涉及找到一个函数 $f$ 来最大化目标 $\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f]$。本问题研究在离散域上的这一优化过程。\n\n我们首先将问题离散化。连续域，即闭区间 $[x_{\\min}, x_{\\max}]$，被一个包含 $N$ 个点的均匀网格 $\\{x_i\\}_{i=1}^N$ 所替代，其中 $x_i = x_{\\min} + (i-1)\\Delta x$，网格间距为 $\\Delta x = (x_{\\max} - x_{\\min})/(N-1)$。评判家函数 $f$ 由其在此网格上的值的向量表示，记为 $\\mathbf{f} = (f(x_1), f(x_2), \\dots, f(x_N))$。\n\n分布 $p_r$ 和 $p_g$ 被定义为 $[x_{\\min}, x_{\\max}]$ 的指定子区间上的均匀分布。期望值通过对网格点的加权和来近似。令 $S_r$ 为使 $x_i$ 位于 $p_r$ 支撑集内的索引 $i$ 的集合，并令 $N_r = |S_r|$。对于 $p_r$ 的权重，如果 $i \\in S_r$，则 $w^{(r)}_i = 1/N_r$，否则 $w^{(r)}_i = 0$。$p_g$ 的权重 $w^{(g)}_i$ 也作类似定义。需要最大化的离散目标函数为：\n$$\nL(\\mathbf{f}) = \\sum_{i=1}^N f(x_i) w^{(r)}_i - \\sum_{i=1}^N f(x_i) w^{(g)}_i = \\sum_{i=1}^N f(x_i) (w^{(r)}_i - w^{(g)}_i) = \\mathbf{f} \\cdot \\mathbf{w}\n$$\n其中 $\\mathbf{w}$ 是分量为 $w_i = w^{(r)}_i - w^{(g)}_i$ 的向量。\n\n优化使用投影梯度上升法进行。线性目标函数 $L(\\mathbf{f})$ 关于向量 $\\mathbf{f}$ 的梯度就是权重向量 $\\mathbf{w}$。更新规则包括一个梯度上升步骤，后跟一个强制执行约束的投影步骤。\n1.  **梯度上升步骤**：通过沿梯度方向移动来计算一个中间函数 $\\mathbf{f}_{\\text{new}}$：\n    $$\n    \\mathbf{f}_{\\text{new}} \\leftarrow \\mathbf{f} + \\eta \\mathbf{w}\n    $$\n    其中 $\\eta$ 是步长。\n\n2.  **投影步骤**：将函数 $\\mathbf{f}_{\\text{new}}$ 投影回固定在原点的离散1-Lipschitz函数集合上。离散1-Lipschitz条件是 $|f(x_i) - f(x_{i-1})| \\le \\Delta x$。固定条件是 $f(x_1) = 0$。指定的投影算法如下：\n    a. 计算 $\\mathbf{f}_{\\text{new}}$ 的前向差分：$\\Delta_i = f_{\\text{new}}(x_{i+1}) - f_{\\text{new}}(x_i)$，对于 $i=1, \\dots, N-1$。\n    b. 将这些差分裁剪到允许的范围内：$\\Delta'_i = \\text{clip}(\\Delta_i, -\\Delta x, \\Delta x)$。\n    c. 通过将其第一个分量设置为零，然后对裁剪后的差分进行累积求和，来重构最终更新的函数 $\\mathbf{f}_{\\text{next}}$：\n       $$\n       f_{\\text{next}}(x_1) = 0\n       $$\n       $$\n       f_{\\text{next}}(x_i) = \\sum_{j=1}^{i-1} \\Delta'_j \\quad \\text{for } i  1\n       $$\n    此过程以步长 $\\eta=5.0$ 重复固定的迭代次数 $T=800$。\n\n在优化收敛到最终评判家函数 $\\mathbf{f}^*$ 后，我们计算三个指标来分析其性质：\n\n1.  **近似Wasserstein-1目标**：这是优化后的评判家的目标函数值，$\\sum_{i=1}^N f^*(x_i) w_i$。它提供了离散分布之间Wasserstein距离的估计值。\n\n2.  **间隙中的归一化平均绝对斜率**：该指标量化了在 $p_r$ 和 $p_g$ 均无支撑的区域中评判家的行为。我们识别出所有落在此类“间隙”内的相邻网格点对 $(x_i, x_{i+1})$。该指标是所有此类点对的归一化绝对斜率 $|f^*(x_{i+1}) - f^*(x_i)|/\\Delta x$ 的平均值。接近1的值表明评判家正在使用其最大允许斜率来跨越支撑集之间的距离，这是具有不相交支撑集的WGAN的特征行为。如果不存在这样的间隙边，则该指标定义为0。\n\n3.  **评判家对比度**：这被定义为 $f^*(m_r) - f^*(m_g)$，其中 $m_r$ 和 $m_g$ 分别是 $p_r$ 和 $p_g$ 最左侧支撑区间的中点。函数值在最接近这些中点的网格点处取得。该指标衡量评判家区分两个分布核心的能力。\n\n提供的测试用例旨在探索支撑集重叠和分离的不同场景，揭示评判家的结构如何适应分布的几何形状。实现将系统地处理每个用例并计算指定的指标。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WGAN critic optimization simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: fully disjoint supports with a single gap\n        {'domain': (0.0, 3.0), 'N': 401, 'pr_supports': [[0.0, 1.0]], 'pg_supports': [[2.0, 3.0]]},\n        # Case 2: two-component supports with alternating disjoint components\n        {'domain': (0.0, 5.0), 'N': 401, 'pr_supports': [[0.0, 1.0], [3.0, 4.0]], 'pg_supports': [[1.0, 2.0], [4.0, 5.0]]},\n        # Case 3: complete overlap\n        {'domain': (0.0, 2.0), 'N': 401, 'pr_supports': [[0.5, 1.5]], 'pg_supports': [[0.5, 1.5]]},\n        # Case 4: small disjoint intervals near boundaries\n        {'domain': (0.0, 1.0), 'N': 401, 'pr_supports': [[0.0, 0.2]], 'pg_supports': [[0.8, 1.0]]},\n        # Case 5: interleaved multi-component supports\n        {'domain': (0.0, 3.0), 'N': 401, 'pr_supports': [[0.0, 0.5], [1.5, 2.0]], 'pg_supports': [[0.5, 1.0], [2.5, 3.0]]},\n    ]\n\n    T = 800  # Number of iterations\n    eta = 5.0  # Step size\n\n    all_results = []\n\n    for case in test_cases:\n        # 1. Setup grid and distributions\n        xmin, xmax = case['domain']\n        N = case['N']\n        x, dx = np.linspace(xmin, xmax, N, retstep=True)\n\n        pr_supports = case['pr_supports']\n        pg_supports = case['pg_supports']\n\n        # Create masks for supports\n        pr_mask = np.zeros(N, dtype=bool)\n        for start, end in pr_supports:\n            pr_mask |= (x >= start)  (x = end)\n\n        pg_mask = np.zeros(N, dtype=bool)\n        for start, end in pg_supports:\n            pg_mask |= (x >= start)  (x = end)\n        \n        # Calculate weights\n        w_r = np.zeros(N)\n        num_pr_points = np.sum(pr_mask)\n        if num_pr_points > 0:\n            w_r[pr_mask] = 1.0 / num_pr_points\n\n        w_g = np.zeros(N)\n        num_pg_points = np.sum(pg_mask)\n        if num_pg_points > 0:\n            w_g[pg_mask] = 1.0 / num_pg_points\n        \n        w = w_r - w_g\n\n        # 2. Optimize the critic function f\n        f = np.zeros(N)\n        for _ in range(T):\n            # Gradient ascent step\n            f_new = f + eta * w\n            \n            # Projection step\n            diffs = f_new[1:] - f_new[:-1]\n            clipped_diffs = np.clip(diffs, -dx, dx)\n            \n            f = np.zeros(N)\n            f[1:] = np.cumsum(clipped_diffs)\n\n        # 3. Compute metrics\n        # Metric 1: Wasserstein Objective\n        metric1 = np.sum(f * w)\n\n        # Metric 2: Normalized Mean Absolute Slope in Gaps\n        gap_mask = ~pr_mask  ~pg_mask\n        gap_edge_mask = gap_mask[:-1]  gap_mask[1:]\n        \n        if np.any(gap_edge_mask):\n            diffs_f = f[1:] - f[:-1]\n            gap_diffs = diffs_f[gap_edge_mask]\n            metric2 = np.mean(np.abs(gap_diffs) / dx)\n        else:\n            metric2 = 0.0\n\n        # Metric 3: Critic Contrast\n        m_r = (pr_supports[0][0] + pr_supports[0][1]) / 2.0\n        m_g = (pg_supports[0][0] + pg_supports[0][1]) / 2.0\n\n        idx_r = np.abs(x - m_r).argmin()\n        idx_g = np.abs(x - m_g).argmin()\n        \n        metric3 = f[idx_r] - f[idx_g]\n        \n        all_results.append([metric1, metric2, metric3])\n\n    # 4. Format and print output\n    case_strs = []\n    for result_case in all_results:\n        inner_str = f\"[{result_case[0]:.4f},{result_case[1]:.4f},{result_case[2]:.4f}]\"\n        case_strs.append(inner_str)\n    \n    output_str = f\"[{','.join(case_strs)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3137255"}, {"introduction": "WGAN的成功训练依赖于评判家与生成器之间更新的精妙平衡，这一平衡通常由“双时间尺度更新法则”（Two Time-Scale Update Rule）来调控。本练习将引导你探究评判家更新次数$n_D$与生成器更新次数$n_G$的比例对训练动态的影响。通过模拟不同的更新策略，你将分析这一关键超参数如何影响模型的收敛速度和最终性能，从而掌握WGAN训练中的一个重要实践技巧。[@problem_id:3137290]", "problem": "考虑一种特定形式的生成对抗网络 (GAN)——瓦瑟斯坦生成对抗网络 (WGAN) 在双时间尺度更新规则下的训练稳定性。假设真实数据分布是一个一维、离散的均匀混合分布，其模态位于位置 $y_1, y_2, \\dots, y_m$，每个模态具有相等的质量。假设生成器产生一个一维、离散的均匀混合分布，其点位于位置 $x_1, x_2, \\dots, x_m$，每个点具有相等的质量。假设对于两种分布，$m$ 的值是固定的且相同。真实分布 $p_r$ 和生成器分布 $p_g$ 之间的瓦瑟斯坦-1距离（也称为推土机距离）表示为 $W_1(p_r, p_g)$，并可以通过 Kantorovich–Rubinstein 对偶性定义为\n$$\nW_1(p_r, p_g) = \\sup_{\\|f\\|_{\\text{Lip}} \\le 1} \\left( \\mathbb{E}_{x \\sim p_r}[f(x)] - \\mathbb{E}_{x \\sim p_g}[f(x)] \\right),\n$$\n其中，上确界取自所有 1-Lipschitz 函数 $f$。双时间尺度更新规则规定，每进行 $n_G$ 次生成器更新，就进行 $n_D$ 次评判家（判别器）更新。\n\n在一维情况下，考虑以下反映 WGAN 训练动态的基本模拟模型，该模型无需对 $f$ 进行显式参数化：\n- 评判家由其在生成器支撑点处评估的导数场表示，$d_i \\approx f'(x_i)$，该导数场必须几乎处处满足 1-Lipschitz 约束。在每次评判家更新时，场 $d_i$ 通过使用松弛率为 $\\alpha \\in (0,1)$ 的指数移动平均向最优次梯度方向松弛：\n$$\nd_i \\leftarrow (1 - \\alpha) \\, d_i + \\alpha \\, s_i,\n$$\n其中 $s_i = \\operatorname{sgn}\\big(F_r(x_i) - F_g(x_i)\\big)$，而 $F_r$ 和 $F_g$ 分别是 $p_r$ 和 $p_g$ 的累积分布函数 (CDF)。这里，$\\operatorname{sgn}$ 表示符号函数，且 $\\operatorname{sgn}(0) = 0$。\n- 在每次生成器更新时，每个 $x_i$ 使用当前的评判家导数 $d_i$ 通过一个步长为 $\\eta  0$ 的简单梯度下降步骤进行更新：\n$$\nx_i \\leftarrow x_i - \\eta \\, d_i.\n$$\n\n训练以周期形式进行。在每个周期中，先应用 $n_D$ 次评判家更新，然后应用 $n_G$ 次生成器更新。重复这些周期，直到执行了固定的总次数 $T$ 的生成器更新。训练结束后，测量以下指标：\n1. $W_1$ 收敛性：通过使用单调传输原理（在一维情况下，最优传输与排序后的支撑集匹配），精确计算一维的 $W_1(p_r, p_g)$。\n2. 模态覆盖率：对于一个容差 $\\varepsilon  0$，如果存在某个生成器点 $x_i$ 使得 $|x_i - y_j| \\le \\varepsilon$，则称一个真实模态 $y_j$ 被覆盖。模态覆盖率是已覆盖模态的比例，表示为 0 到 1 之间的小数。\n\n对所有测试用例使用以下固定参数：\n- 真实模态：$y = [-3, 0, 3]$（即 $m = 3$）。\n- 初始生成器位置：$x^{(0)} = [-6, 1, 6]$。\n- 评判家松弛率：$\\alpha = 0.2$。\n- 生成器步长：$\\eta = 0.05$。\n- 模态覆盖率容差：$\\varepsilon = 0.25$。\n- 生成器更新总次数：$T = 200$。\n\n定义收敛为一个布尔值，当且仅当 $W_1(p_r, p_g) \\le 0.1$ 且模态覆盖率等于 1 时，该值为真。\n\n实现一个程序，模拟上述训练动态，并为每个测试用例输出最终的 $W_1$（作为浮点数）、模态覆盖率（作为小数）以及收敛布尔值（编码为整数，1 表示真，0 表示假）。\n\n测试套件：\n- 情况 1（边界，无评判家学习）：$(n_D, n_G) = (0, 1)$。\n- 情况 2（平衡）：$(n_D, n_G) = (1, 1)$。\n- 情况 3（典型的 WGAN 形式）：$(n_D, n_G) = (5, 1)$。\n- 情况 4（重评判家）：$(n_D, n_G) = (20, 1)$。\n- 情况 5（生成器相对于评判家更快）：$(n_D, n_G) = (1, 5)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，该列表按顺序聚合了所有测试用例的三元组结果，并展平为单个列表。例如，输出格式必须为：\n$[w1_1, \\text{coverage}_1, \\text{converged}_1, w1_2, \\text{coverage}_2, \\text{converged}_2, \\dots]$,\n其中每个 $w1_i$ 是一个浮点数，每个 coverage 是一个小数，每个 converged 是 0 或 1。", "solution": "该问题已经过验证，被认为是**有效的**。它提供了一个独立的、有科学依据且适定的模拟任务，该任务基于瓦瑟斯坦生成对抗网络 (WGAN) 和最优传输理论的既定原理。所有必要的参数和初始条件都已指定，从而可以得到一个确定性且可验证的解。评判家导数场 $d_i$ 的初始状态未明确给出；然而，对于此类迭代算法，一个标准且合乎逻辑的选择是将其初始化为零，即对所有 $i$，$d_i^{(0)} = 0$。以下解决方案中采用了这一假设。\n\n目标是针对五种不同的 $(n_D, n_G)$ 超参数设置，模拟一个简化的 WGAN 的训练动态，并使用三个指标评估结果：最终的瓦瑟斯坦-1距离、模态覆盖率和一个二进制收敛标志。\n\n模拟遵循以下步骤序列。\n\n**1. 系统初始化**\n\n模拟使用指定的参数进行初始化。\n真实数据分布 $p_r$ 是在模态集合 $y = \\{-3, 0, 3\\}$ 上的一个离散均匀分布。模态数量为 $m=3$。\n生成器分布 $p_g$ 是在 $m=3$ 个点 $x = \\{x_1, x_2, x_3\\}$ 的集合上的一个离散均匀分布。它们的初始位置由 $x^{(0)} = [-6, 1, 6]$ 给出。\n评判家的导数场，由向量 $d = [d_1, d_2, d_3]$ 表示，被初始化为 $d^{(0)} = [0, 0, 0]$。\n其他固定参数为：\n- 评判家松弛率：$\\alpha = 0.2$\n- 生成器步长：$\\eta = 0.05$\n- 模态覆盖率容差：$\\varepsilon = 0.25$\n- 生成器更新总次数：$T = 200$\n\n**2. 累积分布函数 (CDF)**\n\n评判家更新规则依赖于瓦瑟斯坦距离的次梯度，在一维情况下，该次梯度与真实分布和生成分布的 CDF 之差有关。对于一个支撑点为 $\\{z_1, \\dots, z_m\\}$ 且均匀权重为 $1/m$ 的离散分布 $P$，其 CDF 由下式给出：\n$$\nF_P(z) = \\frac{1}{m} \\sum_{i=1}^m \\mathbf{1}_{z_i \\le z}\n$$\n其中 $\\mathbf{1}$ 是指示函数。\n因此，对于我们的特定问题，在点 $z$ 处评估的真实分布和生成器分布的 CDF 分别是：\n$$\nF_r(z) = \\frac{1}{3} \\sum_{j=1}^3 \\mathbf{1}_{y_j \\le z}\n$$\n$$\nF_g(z) = \\frac{1}{3} \\sum_{i=1}^3 \\mathbf{1}_{x_i \\le z}\n$$\n\n**3. 训练循环**\n\n训练以周期形式进行。对于由一对 $(n_D, n_G)$ 定义的每个测试用例，总周期数计算为 $N_{\\text{cycles}} = T / n_G$。由于在测试套件中 $T=200$ 且 $n_G$ 为 1 或 5，因此 $N_{\\text{cycles}}$ 始终是一个整数。每个周期包括两个阶段：评判家更新和生成器更新。\n\n**3.1. 评判家更新阶段**\n\n在此阶段的 $n_D$ 个步骤中，每一步都更新评判家的导数场 $d$。每个分量 $d_i$ 的更新是朝向目标方向 $s_i$ 的指数移动平均：\n$$\nd_i \\leftarrow (1 - \\alpha) \\, d_i + \\alpha \\, s_i \\quad \\text{for } i = 1, 2, 3\n$$\n目标方向 $s_i$ 是在生成器当前支撑点 $x_i$ 处评估的 CDF 之差的符号：\n$$\ns_i = \\operatorname{sgn}\\big(F_r(x_i) - F_g(x_i)\\big)\n$$\n其中符号函数定义为：如果 $u > 0$，则 $\\operatorname{sgn}(u) = 1$；如果 $u  0$，则为 $-1$；如果 $u = 0$，则为 $0$。对 $d$ 的所有 $m=3$ 个分量都执行这些更新。\n\n**3.2. 生成器更新阶段**\n\n在此阶段的 $n_G$ 个步骤中，每一步都通过梯度下降更新生成器的支撑点 $x$。梯度由评判家的导数场 $d$ 提供。\n$$\nx_i \\leftarrow x_i - \\eta \\, d_i \\quad \\text{for } i = 1, 2, 3\n$$\n此更新将每个点 $x_i$ 移向预期会减小瓦瑟斯坦距离的方向。\n\n**4. 训练后评估**\n\n完成所有 $N_{\\text{cycles}}$ 后，使用生成器点 $x$ 的最终状态来计算性能指标。\n\n**4.1. 瓦瑟斯坦-1距离 ($W_1$)**\n\n对于具有相同点数和均匀权重的一维离散分布，$W_1$ 距离有一个简单的闭式解。它是两个分布排序后的支撑点之间的平均绝对差。设 $x_{(1)}, x_{(2)}, x_{(3)}$ 是排序后的生成器点，$y_{(1)}, y_{(2)}, y_{(3)}$ 是排序后的真实模态。真实模态已经排好序：$y = [-3, 0, 3]$。\n$$\nW_1(p_r, p_g) = \\frac{1}{m} \\sum_{i=1}^m |x_{(i)} - y_{(i)}| = \\frac{1}{3} \\big( |x_{(1)} - (-3)| + |x_{(2)} - 0| + |x_{(3)} - 3| \\big)\n$$\n\n**4.2. 模态覆盖率**\n\n如果至少有一个最终的生成器点 $x_i$ 与真实模态 $y_j$ 的距离在 $\\varepsilon=0.25$ 以内，则认为该模态被“覆盖”。\n$$\n\\text{covered}(y_j) = \\bigvee_{i=1}^3 \\big( |x_i - y_j| \\le \\varepsilon \\big)\n$$\n模态覆盖率是已覆盖的真实模态的比例：\n$$\n\\text{Coverage} = \\frac{1}{m} \\sum_{j=1}^m \\text{covered}(y_j)\n$$\n\n**4.3. 收敛准则**\n\n当且仅当以下两个条件都满足时，布尔收敛标志设置为真（表示为 1）；否则，设置为假（表示为 0）。\n1. 最终的瓦瑟斯坦-1距离足够小：$W_1(p_r, p_g) \\le 0.1$。\n2. 所有真实模态都被覆盖：模态覆盖率 $= 1.0$。\n\n对五个 $(n_D, n_G)$ 测试用例中的每一个都重复此整个过程，并将结果聚合到一个扁平化的列表中作为最终输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates WGAN training dynamics and evaluates performance for multiple test cases.\n    \"\"\"\n\n    def run_simulation(nD, nG):\n        \"\"\"\n        Runs a single simulation for a given (nD, nG) pair.\n\n        Args:\n            nD (int): Number of critic updates per cycle.\n            nG (int): Number of generator updates per cycle.\n\n        Returns:\n            tuple: A triplet containing the final W1 distance (float),\n                   mode coverage (float), and convergence status (int).\n        \"\"\"\n        # Fixed parameters\n        y_real = np.array([-3.0, 0.0, 3.0])\n        m = len(y_real)\n        x = np.array([-6.0, 1.0, 6.0])\n        alpha = 0.2\n        eta = 0.05\n        epsilon = 0.25\n        T = 200\n\n        # Initial state\n        d = np.zeros(m)\n\n        # The problem statement implies T is a multiple of nG for all test cases.\n        if T % nG != 0:\n            num_cycles = T // nG\n            remaining_gen_updates = T % nG\n        else:\n            num_cycles = T // nG\n            remaining_gen_updates = 0\n\n        for _ in range(num_cycles):\n            # Critic update phase\n            for _ in range(nD):\n                # Calculate CDFs F_r(x_i) and F_g(x_i)\n                fr_x = np.array([np.sum(y_real = xi) for xi in x]) / m\n                fg_x = np.array([np.sum(x = xi) for xi in x]) / m\n\n                # Calculate target direction s\n                s = np.sign(fr_x - fg_x)\n\n                # Update critic's derivative field d\n                d = (1 - alpha) * d + alpha * s\n\n            # Generator update phase\n            for _ in range(nG):\n                # Update generator points x\n                x = x - eta * d\n        \n        # Handle any remaining generator updates if T is not a multiple of nG\n        if remaining_gen_updates > 0:\n             # Perform one more cycle of critic updates before final generator steps\n            for _ in range(nD):\n                fr_x = np.array([np.sum(y_real = xi) for xi in x]) / m\n                fg_x = np.array([np.sum(x = xi) for xi in x]) / m\n                s = np.sign(fr_x - fg_x)\n                d = (1 - alpha) * d + alpha * s\n            for _ in range(remaining_gen_updates):\n                x = x - eta * d\n\n\n        # --- Post-training evaluation ---\n\n        # 1. Wasserstein-1 distance\n        x_sorted = np.sort(x)\n        w1_distance = np.mean(np.abs(x_sorted - y_real))\n\n        # 2. Mode coverage\n        covered_count = 0\n        for yj in y_real:\n            is_covered = np.any(np.abs(x - yj) = epsilon)\n            if is_covered:\n                covered_count += 1\n        mode_coverage = covered_count / m\n\n        # 3. Convergence criterion\n        converged = (w1_distance = 0.1) and (mode_coverage == 1.0)\n        converged_int = 1 if converged else 0\n\n        return w1_distance, mode_coverage, converged_int\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 1),   # Case 1: no critic learning\n        (1, 1),   # Case 2: balanced\n        (5, 1),   # Case 3: typical WGAN-like\n        (20, 1),  # Case 4: heavy critic\n        (1, 5),   # Case 5: fast generator relative to critic\n    ]\n\n    results = []\n    for nD, nG in test_cases:\n        w1, coverage, converged = run_simulation(nD, nG)\n        results.extend([w1, coverage, converged])\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for i, res in enumerate(results):\n        # Format floats, keep ints as is\n        if i % 3 in [0, 1]:  # w1 and coverage\n            formatted_results.append(f\"{res:.7f}\")\n        else: # converged\n            formatted_results.append(str(res))\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3137290"}]}