{"hands_on_practices": [{"introduction": "变分自编码器（VAE）的目标函数由两部分组成：重构项和作为正则化的KL散度项。虽然KL散度在某些情况下可以解析计算，但在更复杂的模型中，我们通常依赖于蒙特卡洛估计。本练习将引导你比较解析解与蒙特卡洛估计，并分析估计误差对训练稳定性的影响，这对于成功训练VAE至关重要[@problem_id:3197900]。", "problem": "给定一个变分自编码器 (VAE) 的上下文，其中编码器定义了一个对角多元正态分布 $q_{\\phi}(z \\mid x)$，其参数为 $\\mu \\in \\mathbb{R}^k$ 和 $\\sigma^2 \\in \\mathbb{R}^k_{>0}$，先验是标准正态分布 $p(z) = \\mathcal{N}(0, I_k)$。Kullback–Leibler (KL) 散度定义为 $\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{q}[\\log q(z \\mid x) - \\log p(z)]$。您的任务是比较解析计算的 Kullback–Leibler (KL) 散度与蒙特卡洛 (MC) 估计值，并分析估计器噪声如何通过梯度信噪比这一简单的训练稳定性代理指标产生影响。\n\n使用的基本原理：\n- Kullback–Leibler (KL) 散度的定义：$\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{q}[\\log q(z) - \\log p(z)]$。\n- 重参数化：$z = \\mu + \\sigma \\odot \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, I_k)$，$\\odot$ 表示逐元素相乘。\n- 对角多元正态分布的对数密度：对于 $z \\in \\mathbb{R}^k$，$\\log \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2)) = -\\tfrac{1}{2}\\left(\\sum_{i=1}^k \\tfrac{(z_i - \\mu_i)^2}{\\sigma_i^2} + \\sum_{i=1}^k \\log(2\\pi \\sigma_i^2)\\right)$。\n- 标准正态分布的对数密度：$\\log \\mathcal{N}(z \\mid 0, I_k) = -\\tfrac{1}{2}\\left(\\|z\\|_2^2 + k \\log(2\\pi)\\right)$。\n\n要求：\n1. 基于上述基本原理，推导对角多元正态分布 $q_{\\phi}$ 和标准正态分布 $p$ 之间的 $\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z))$ 的解析表达式。使用此解析表达式为给定参数计算精确的 $\\mathrm{KL}$ 值。\n2. 通过使用重参数化 $z^{(s)} = \\mu + \\sigma \\odot \\epsilon^{(s)}$（其中 $\\epsilon^{(s)} \\sim \\mathcal{N}(0, I_k)$）从 $q_{\\phi}(z \\mid x)$ 中抽取 $S$ 个独立样本 $z^{(s)}$，并对 $s = 1, \\dots, S$ 的 $\\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)})$ 进行平均，从而构建 $\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z))$ 的蒙特卡洛估计器。\n3. 使用路径导数定义 KL 项关于 $\\mu$ 的梯度估计器。令 $g^{(s)} = \\nabla_{z}\\left[\\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)})\\right]$ 并注意 $\\nabla_{\\mu} z^{(s)} = I_k$。使用 $g^{(s)}$ 作为关于 $\\mu$ 的单样本梯度估计。对于给定的样本大小 $S$，计算 $S$ 个样本的经验均值向量 $\\bar{g}$ 和经验逐坐标标准差向量 $\\operatorname{std}(g)$。将信噪比定义为 $\\mathrm{SNR} = \\frac{\\|\\bar{g}\\|_2}{\\|\\operatorname{std}(g)\\|_2}$。如果分母为 $0$，则将该比率定义为 $0$。\n4. 对下方的每个测试用例，计算以下四个量：解析 $\\mathrm{KL}$ 值、使用 $S$ 个样本的蒙特卡洛 $\\mathrm{KL}$ 估计值、绝对估计误差 $|\\text{MC} - \\text{Analytic}|$，以及使用相同的 $S$ 个样本的关于 $\\mu$ 的梯度估计器的信噪比 $\\mathrm{SNR}$。\n\n您必须实现一个完整的、可运行的程序，该程序执行以下操作：\n- 使用固定的随机种子 $42$ 以确保可复现性。\n- 对每个测试用例，计算上述指定的四个量。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，结果按测试用例的顺序排列。对于每个测试用例 $i$，按顺序附加四个值 $K_i, M_i, E_i, R_i$。因此，对于 $N$ 个测试用例，最终输出将有 $4 \\times N$ 个条目。\n\n测试套件：\n- 案例 1（正常路径，小样本）：$k = 2$，$\\mu = [0.5, -0.5]$，$\\log \\sigma^2 = [-0.2, 0.3]$，$S = 1$。\n- 案例 2（正常路径，中等样本）：$k = 2$，$\\mu = [0.5, -0.5]$，$\\log \\sigma^2 = [-0.2, 0.3]$，$S = 10$。\n- 案例 3（正常路径，大样本）：$k = 2$，$\\mu = [0.5, -0.5]$，$\\log \\sigma^2 = [-0.2, 0.3]$，$S = 100$。\n- 案例 4（边界情况，零均值和单位方差）：$k = 2$，$\\mu = [0.0, 0.0]$，$\\log \\sigma^2 = [0.0, 0.0]$，$S = 100$。\n- 案例 5（边缘情况，近确定性编码器维度）：$k = 1$，$\\mu = [0.1]$，$\\log \\sigma^2 = [\\log(10^{-4})]$，$S = 50$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，$[\\text{result1},\\text{result2},\\text{result3}]$）。\n- 条目必须是精确顺序的数值：对于案例 1，为 $K_1, M_1, E_1, R_1$；对于案例 2，为 $K_2, M_2, E_2, R_2$；依此类推，直到案例 5。", "solution": "问题陈述已经过仔细审查，并被确定为有效。它在科学上基于成熟的变分自编码器理论，问题提法清晰，提供了所有必要信息，并以客观、正式的语言表述。因此，我们可以着手提供完整的解决方案。\n\n该问题要求我们对变分自编码器 (VAE) 的 Kullback-Leibler (KL) 散度的解析解和蒙特卡洛 (MC) 估计值进行比较分析。编码器的近似后验是一个协方差矩阵为对角阵的多元正态分布，即 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2))$，而先验是一个标准多元正态分布，即 $p(z) = \\mathcal{N}(z \\mid 0, I_k)$。我们还被要求分析 KL 散度相对于均值参数 $\\mu$ 的梯度的信噪比 (SNR)。\n\n**1. KL 散度的解析表达式**\n\n两个分布 $q(z)$ 和 $p(z)$ 之间的 Kullback-Leibler 散度定义为 $\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{z \\sim q(z)}[\\log q(z) - \\log p(z)]$。由于 $q_{\\phi}(z \\mid x)$ 和 $p(z)$ 都是可以在维度上分解的多元正态分布，总 KL 散度是每个维度的 KL 散度之和：\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)) = \\sum_{i=1}^k \\mathrm{KL}(\\mathcal{N}(z_i \\mid \\mu_i, \\sigma_i^2) \\,\\|\\, \\mathcal{N}(z_i \\mid 0, 1))\n$$\n对于单个维度 $i$，两个一元正态分布 $\\mathcal{N}(\\mu_a, \\sigma_a^2)$ 和 $\\mathcal{N}(\\mu_b, \\sigma_b^2)$ 之间的 KL 散度由以下公式给出：\n$$\n\\mathrm{KL}(\\mathcal{N}_a \\,\\|\\, \\mathcal{N}_b) = \\log\\frac{\\sigma_b}{\\sigma_a} + \\frac{\\sigma_a^2 + (\\mu_a-\\mu_b)^2}{2\\sigma_b^2} - \\frac{1}{2}\n$$\n在我们的情况下，对于每个维度 $i$，我们有 $\\mu_a = \\mu_i$，$\\sigma_a^2 = \\sigma_i^2$，$\\mu_b = 0$，以及 $\\sigma_b^2 = 1$。将这些代入公式可得：\n$$\n\\mathrm{KL}(\\mathcal{N}(z_i \\mid \\mu_i, \\sigma_i^2) \\,\\|\\, \\mathcal{N}(z_i \\mid 0, 1)) = \\log\\frac{1}{\\sigma_i} + \\frac{\\sigma_i^2 + (\\mu_i-0)^2}{2 \\cdot 1} - \\frac{1}{2}\n$$\n$$\n= -\\log\\sigma_i + \\frac{\\mu_i^2 + \\sigma_i^2}{2} - \\frac{1}{2}\n$$\n由于 $\\log\\sigma_i = \\frac{1}{2}\\log\\sigma_i^2$，我们可以将其重写为：\n$$\n= \\frac{1}{2}(\\mu_i^2 + \\sigma_i^2 - \\log\\sigma_i^2 - 1)\n$$\n对所有 $k$ 个维度求和，我们得到 KL 散度的最终解析表达式：\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)) = \\frac{1}{2} \\sum_{i=1}^k \\left( \\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1 \\right)\n$$\n该表达式允许在给定参数 $\\mu$ 和 $\\sigma^2$ 的情况下精确计算 KL 散度。\n\n**2. KL 散度的蒙特卡洛估计器**\n\nKL 散度可以使用蒙特卡洛积分进行估计。根据大数定律，期望 $\\mathbb{E}_{z \\sim q}[f(z)]$ 可以通过对从 $q(z)$ 中抽取的 $S$ 个样本计算 $f(z)$ 的平均值来近似。在这里，函数是 $f(z) = \\log q_{\\phi}(z \\mid x) - \\log p(z)$。\n\n为了从 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2))$ 中抽取样本 $z^{(s)}$，我们使用重参数化技巧。我们首先从标准正态分布中抽取 $S$ 个样本 $\\epsilon^{(s)}$，即 $\\epsilon^{(s)} \\sim \\mathcal{N}(0, I_k)$，然后如下变换它们：\n$$\nz^{(s)} = \\mu + \\sigma \\odot \\epsilon^{(s)}\n$$\n其中 $\\sigma = (\\sigma_1, \\dots, \\sigma_k)$ 是标准差向量，$\\odot$ 表示逐元素相乘。\n\nKL 散度的蒙特卡洛估计器 $\\hat{K}_S$ 于是为：\n$$\n\\hat{K}_S = \\frac{1}{S} \\sum_{s=1}^S \\left[ \\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)}) \\right]\n$$\n所需的对数密度为：\n$$\n\\log q_{\\phi}(z^{(s)} \\mid x) = -\\frac{1}{2} \\left( \\sum_{i=1}^k \\frac{(z_i^{(s)} - \\mu_i)^2}{\\sigma_i^2} + \\sum_{i=1}^k \\log(2\\pi \\sigma_i^2) \\right)\n$$\n$$\n\\log p(z^{(s)}) = -\\frac{1}{2} \\left( \\sum_{i=1}^k (z_i^{(s)})^2 + k \\log(2\\pi) \\right)\n$$\n绝对估计误差计算为 $|\\hat{K}_S - \\mathrm{KL}_{\\text{analytic}}|$。\n\n**3. 梯度估计器和信噪比**\n\n我们的任务是构建 KL 项关于 $\\mu$ 的梯度估计器，即 $\\nabla_{\\mu} \\mathrm{KL}(q_{\\phi} \\,\\|\\, p)$。重参数化技巧在这里至关重要，因为它允许将梯度传递到期望内部。\n$$\n\\nabla_{\\mu} \\mathrm{KL} = \\nabla_{\\mu} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}[\\log q(\\mu+\\sigma\\epsilon) - \\log p(\\mu+\\sigma\\epsilon)] = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}[\\nabla_{\\mu}(\\log q(z) - \\log p(z))]\n$$\n其中 $z = \\mu + \\sigma\\epsilon$。根据链式法则，$\\nabla_{\\mu}f(z) = (\\nabla_z f(z))^T \\nabla_{\\mu}z$。由于 $z_i = \\mu_i + \\sigma_i \\epsilon_i$，雅可比矩阵 $\\nabla_{\\mu}z$ 是单位矩阵 $I_k$。因此，$\\nabla_{\\mu}f(z) = \\nabla_z f(z)$。梯度为：\n$$\n\\nabla_{\\mu} \\mathrm{KL} = \\mathbb{E}_{z \\sim q}[\\nabla_z(\\log q(z) - \\log p(z))]\n$$\n问题将单样本梯度估计定义为 $g^{(s)} = \\nabla_{z}(\\log q(z^{(s)}) - \\log p(z^{(s)}))$。我们来推导其分量 $g_i(z) = \\frac{\\partial}{\\partial z_i}(\\log q(z) - \\log p(z))$。\n$$\n\\frac{\\partial}{\\partial z_i} \\log q(z) = \\frac{\\partial}{\\partial z_i} \\left[ -\\frac{1}{2} \\sum_{j=1}^k \\left( \\frac{(z_j - \\mu_j)^2}{\\sigma_j^2} + \\log(2\\pi\\sigma_j^2) \\right) \\right] = -\\frac{z_i - \\mu_i}{\\sigma_i^2}\n$$\n$$\n\\frac{\\partial}{\\partial z_i} \\log p(z) = \\frac{\\partial}{\\partial z_i} \\left[ -\\frac{1}{2} \\sum_{j=1}^k (z_j^2 + \\log(2\\pi)) \\right] = -z_i\n$$\n结合这些项，得到单样本梯度分量的表达式：\n$$\ng_i(z) = -\\frac{z_i - \\mu_i}{\\sigma_i^2} - (-z_i) = z_i - \\frac{z_i - \\mu_i}{\\sigma_i^2}\n$$\n使用重参数化 $z_i-\\mu_i = \\sigma_i \\epsilon_i$，我们可以将其简化为 $g_i(z) = z_i - \\epsilon_i / \\sigma_i$。\n\n梯度的蒙特卡洛估计是这些单样本梯度的经验均值，$\\bar{g} = \\frac{1}{S} \\sum_{s=1}^S g(z^{(s)})$。经验逐坐标标准差向量 $\\operatorname{std}(g)$ 是在 $S$ 个样本上对梯度的每个维度计算得出的。信噪比接着定义为：\n$$\n\\mathrm{SNR} = \\frac{\\|\\bar{g}\\|_2}{\\|\\operatorname{std}(g)\\|_2}\n$$\n如果分母 $\\|\\operatorname{std}(g)\\|_2$ 为零（当 $S=1$ 或梯度样本方差为零时发生），则 SNR 定义为 $0$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes analytical KL divergence, a Monte Carlo estimate, the absolute error,\n    and a gradient signal-to-noise ratio for a VAE setting for several test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, mu, log_sigma_sq, S)\n        (2, [0.5, -0.5], [-0.2, 0.3], 1),\n        (2, [0.5, -0.5], [-0.2, 0.3], 10),\n        (2, [0.5, -0.5], [-0.2, 0.3], 100),\n        (2, [0.0, 0.0], [0.0, 0.0], 100),\n        (1, [0.1], [np.log(1e-4)], 50),\n    ]\n\n    # Use a fixed random seed for reproducibility.\n    rng = np.random.default_rng(42)\n    \n    results = []\n\n    for k, mu_list, log_sigma_sq_list, S in test_cases:\n        mu = np.array(mu_list, dtype=np.float64)\n        log_sigma_sq = np.array(log_sigma_sq_list, dtype=np.float64)\n        \n        sigma_sq = np.exp(log_sigma_sq)\n        sigma = np.sqrt(sigma_sq)\n        \n        # 1. Analytical KL divergence (K_i)\n        kl_analytic = 0.5 * np.sum(mu**2 + sigma_sq - log_sigma_sq - 1)\n        \n        # Draw S samples for epsilon from a standard normal distribution\n        epsilons = rng.normal(size=(S, k))\n        \n        # Apply reparameterization trick to get samples for z\n        z_samples = mu + sigma * epsilons\n        \n        # 2. Monte Carlo estimate of KL divergence (M_i)\n        # log q(z|x) term for each sample\n        log_q_z = -0.5 * np.sum(epsilons**2 + log_sigma_sq + np.log(2 * np.pi), axis=1)\n        \n        # log p(z) term for each sample\n        log_p_z = -0.5 * np.sum(z_samples**2 + np.log(2 * np.pi), axis=1)\n        \n        # KL term for each sample and the final MC estimate\n        kl_terms = log_q_z - log_p_z\n        kl_mc = np.mean(kl_terms)\n        \n        # 3. Absolute estimation error |MC - Analytic| (E_i)\n        abs_error = np.abs(kl_mc - kl_analytic)\n        \n        # 4. Signal-to-noise ratio of the gradient estimator (R_i)\n        # Per-sample gradient estimates g(z^(s))\n        g_samples = z_samples - epsilons / sigma\n        \n        # Empirical mean of the gradient\n        g_bar = np.mean(g_samples, axis=0)\n        \n        # Empirical per-coordinate standard deviation of the gradient (using ddof=0 for population std of the sample)\n        # For S=1, this will correctly result in a vector of zeros.\n        g_std = np.std(g_samples, axis=0, ddof=0)\n        \n        # L2-norms of the mean and std vectors\n        norm_g_bar = np.linalg.norm(g_bar)\n        norm_g_std = np.linalg.norm(g_std)\n        \n        # SNR calculation, handling the case of zero standard deviation\n        snr = norm_g_bar / norm_g_std if norm_g_std > 0 else 0.0\n        \n        results.extend([kl_analytic, kl_mc, abs_error, snr])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3197900"}, {"introduction": "后验坍塌是VAE训练中一个常见的失败模式，即解码器完全忽略了隐变量，导致模型退化为自编码器。本练习通过一个简化的解析模型，揭示了某些架构设计（如跳跃连接）是如何引发这一问题的[@problem_id:3197907]。通过这个实践，你将深刻理解后验坍塌的成因，并学习如何运用$\\beta$-VAE和“自由比特”（free-bits）等方法来有效缓解该问题。", "problem": "构建一个小型、完全指定的线性高斯研究，以探究从输入 $x$ 到解码器的跳跃连接如何在变分自编码器 (VAE) 中绕过潜变量 $z$，可能导致后验坍塌，并评估缓解策略。在标量设定下进行，并采用以下在文献中标准且作为基础的假设：\n\n- 变分自编码器 (VAE) 的证据下界 (ELBO) 定义为 $\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\mathrm{KL}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z))$。\n- 对于单变量高斯分布 $q(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^{2})$ 和 $p(z) = \\mathcal{N}(0, 1)$，Kullback–Leibler 散度为 $\\frac{1}{2}\\left(\\sigma^{2} + \\mu^{2} - 1 - \\log \\sigma^{2}\\right)$。\n- 对于方差为 $\\sigma_{\\epsilon}^{2}$ 的高斯似然，负对数似然对应于一个缩放的平方误差，且期望可在线性二次型上分配。\n\n模型和目标：\n\n- 先验：$p(z) = \\mathcal{N}(0, 1)$。\n- 数据：$x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$，其中 $\\sigma_{x}^{2} = 1$。\n- 编码器：$q_{\\phi}(z \\mid x) = \\mathcal{N}(a x, v)$，其中标量参数 $a \\in \\mathbb{R}$ 和 $v \\in \\mathbb{R}_{>0}$ 待优化。\n- 带跳跃连接的解码器：$p_{\\theta}(x \\mid z, x) = \\mathcal{N}(W z + s x, \\sigma_{\\epsilon}^{2})$，其中跳跃系数 $s \\in \\mathbb{R}$ 和解码器增益 $W \\in \\mathbb{R}$ 在本研究中被视为固定的超参数。\n- 训练目标 (beta-ELBO 的负值)：对 $a$ 和 $v$ 进行最小化，目标为期望损失\n$$\n\\mathcal{L}_{\\beta}(a, v) \\;=\\; \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right] \\;+\\; \\beta \\, \\mathbb{E}_{x}\\!\\left[ \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\\right],\n$$\n其中 $\\beta \\in \\mathbb{R}_{>0}$ 是 Kullback–Leibler 散度的权重 (beta-VAE)。\n\n任务：\n\n- 仅从上述定义出发，推导在固定的 $W$、$s$、$\\sigma_{\\epsilon}^{2}$ 和 $\\beta$ 条件下，能最小化 $\\mathcal{L}_{\\beta}(a, v)$ 的闭式最优编码器参数 $a^{\\star}$ 和 $v^{\\star}$。\n- 使用最优的 $a^{\\star}$ 和 $v^{\\star}$，计算编码器下 $x$ 和 $z$ 之间的互信息，\n$$\nI(x; z) \\;=\\; \\frac{1}{2}\\log\\!\\left(1 + \\frac{a^{2}\\,\\sigma_{x}^{2}}{v}\\right),\n$$\n使用自然对数，以量化潜变量的使用情况。使用 $\\sigma_{x}^{2} = 1$。\n- 解析地证明跳跃参数 $s$ 如何影响 $a^{\\star}$，并解释为什么当 $s$ 接近 $1$ 时，会导致 $a^{\\star}$ 接近 $0$，这表明由于绕过 $z$ 可能导致后验坍塌。\n- 以编程方式实现并评估缓解策略：\n  - 减小 Kullback–Leibler 权重 $\\beta$。\n  - 减弱跳跃连接强度 $s$。\n  - 应用 free-bits 启发式方法：将 Kullback–Leibler 项替换为 $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$，其中阈值 $\\tau \\in \\mathbb{R}_{\\ge 0}$。按如下方式解释解：如果无约束的 beta-VAE 最优解的 $\\mathrm{KL} \\ge \\tau$，则保持该解；否则，在 $\\mathrm{KL} = \\tau$ 的约束下最小化重构项，并计算由此产生的 $a$ 和 $v$。全程使用自然对数。\n\n你的程序必须：\n\n- 对以下每个测试用例计算 $I(x; z)$，严格按照指定使用 beta-VAE 最优解或 free-bits 规则。在所有情况下，均使用 $\\sigma_{x}^{2} = 1$ 和自然对数。元组顺序为 $(s, W, \\sigma_{\\epsilon}^{2}, \\beta, \\tau, \\text{regime})$，其中 $\\text{regime}$ 是字符串字面量 \"beta\" 或 \"freebits\"。\n  - 情况 A：$(s = 0,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 B：$(s = 1,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 C：$(s = 0.9,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 0.1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 D：$(s = 0.5,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 E：$(s = 0.5,\\, W = 0,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 F：$(s = 0.9,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 1,\\, \\text{regime}=\\text{\"freebits\"})$。\n- 对于 \"beta\" 模式，使用你为 $\\mathcal{L}_{\\beta}(a, v)$ 推导的闭式最优解来计算 $a^{\\star}$、$v^{\\star}$，然后计算 $I(x; z)$。\n- 对于 \"freebits\" 模式，如果 beta-VAE 的闭式最优解得出 $\\mathrm{KL} \\ge \\tau$，则直接使用该解；否则，在 $\\mathrm{KL} = \\tau$ 的约束下最小化重构项，并计算 $I(x; z)$。如果 $W = 0$，请仔细处理退化情况并返回 $I(x; z) = 0$。\n\n最终输出要求：\n\n- 你的程序应生成单行输出，其中包含情况 A 到 F 的六个互信息值，格式为逗号分隔的列表并包含在方括号内，每个值四舍五入到六位小数，并以奈特 (nats) 为单位表示。例如，输出行的格式为 $[\\text{r}_{A},\\text{r}_{B},\\text{r}_{C},\\text{r}_{D},\\text{r}_{E},\\text{r}_{F}]$。", "solution": "用户提供了一个在变分自编码器（VAE）背景下定义明确的理论问题。我将首先验证问题陈述，然后进行完整的推导和求解。\n\n### 步骤1：提取给定条件\n- **先验分布**：$p(z) = \\mathcal{N}(0, 1)$。\n- **数据分布**：$x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$，其中 $\\sigma_{x}^{2} = 1$。\n- **编码器（近似后验）**：$q_{\\phi}(z \\mid x) = \\mathcal{N}(a x, v)$，待优化参数为 $a \\in \\mathbb{R}$，$v \\in \\mathbb{R}_{>0}$。\n- **解码器（似然）**：$p_{\\theta}(x \\mid z, x) = \\mathcal{N}(W z + s x, \\sigma_{\\epsilon}^{2})$，固定超参数为 $W \\in \\mathbb{R}$，$s \\in \\mathbb{R}$。\n- **目标函数（最小化）**：\n$$\n\\mathcal{L}_{\\beta}(a, v) \\;=\\; \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right] \\;+\\; \\beta \\, \\mathbb{E}_{x}\\!\\left[ \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\\right]\n$$\n- **KL 散度公式**：对于 $q(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^{2})$ 和 $p(z) = \\mathcal{N}(0, 1)$，$\\mathrm{KL}(q\\,\\|\\,p) = \\frac{1}{2}\\left(\\sigma^{2} + \\mu^{2} - 1 - \\log \\sigma^{2}\\right)$。\n- **互信息公式**：$I(x; z) \\;=\\; \\frac{1}{2}\\log\\!\\left(1 + \\frac{a^{2}\\,\\sigma_{x}^{2}}{v}\\right)$，使用自然对数。\n- **Free-Bits 启发式方法**：KL 项被替换为 $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$。\n\n### 步骤2：使用提取的给定条件进行验证\n- **科学基础**：该问题是 VAE 的标准理论分析，使用了诸如 ELBO、KL 散度和线性高斯模型等已建立的概念。这是机器学习研究中为便于分析而常用的简化方法。\n- **适定性**：该问题要求最小化一个凸函数（就我们优化的变量而言），这保证了唯一最小值的存在。任务定义清晰，能够导出一组唯一的结果。\n- **客观性**：语言精确，所有模型和目标都经过了形式化规定。没有主观性。\n- **结论**：该问题是有效的。它具有科学合理性、适定性和客观性。\n\n### 步骤3：判断与行动\n该问题是有效的。我将继续进行完整解答。\n\n### 第1部分：最优编码器参数 $a^{\\star}$ 和 $v^{\\star}$ 的推导\n\n目标函数 $\\mathcal{L}_{\\beta}(a, v)$ 由两项组成：期望重构误差和期望 KL 散度。\n\n**第1项：重构误差**\n设 $R$ 为重构项。\n$$\nR = \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right]\n$$\n期望内的表达式可以重写为 $\\left((1-s)x - Wz\\right)^{2}$。我们首先对 $z \\sim q_{\\phi}(z \\mid x) = \\mathcal{N}(ax, v)$ 取期望。\n$$\n\\mathbb{E}_{q_z}[((1-s)x - Wz)^2] = \\mathbb{E}_{q_z}[(1-s)^2x^2 - 2W(1-s)xz + W^2z^2]\n$$\n使用 $\\mathbb{E}_{q_z}[z] = ax$ 和 $\\mathbb{E}_{q_z}[z^2] = \\text{Var}_{q_z}[z] + (\\mathbb{E}_{q_z}[z])^2 = v + (ax)^2$：\n$$\n= (1-s)^2x^2 - 2W(1-s)x(ax) + W^2(v + a^2x^2)\n$$\n$$\n= x^2[(1-s)^2 - 2aW(1-s) + a^2W^2] + W^2v\n$$\n这可以简化为平方项 $x^2(1-s-aW)^2 + W^2v$。\n现在，我们对 $x \\sim \\mathcal{N}(0, \\sigma_x^2)$ 取期望。我们使用 $\\mathbb{E}_x[x^2] = \\sigma_x^2$。\n$$\n\\mathbb{E}_{x}[x^2(1-s-aW)^2 + W^2v] = \\sigma_x^2(1-s-aW)^2 + W^2v\n$$\n所以，重构项为：\n$$\nR = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( \\sigma_x^2(1-s-aW)^2 + W^2v \\right)\n$$\n\n**第2项：KL 散度**\n设 $K$ 为 KL 散度项。$q_{\\phi}(z \\mid x) = \\mathcal{N}(ax, v)$ 与 $p(z)=\\mathcal{N}(0, 1)$ 之间的 KL 散度为：\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)) = \\frac{1}{2}\\left(v + (ax)^2 - 1 - \\log v\\right)\n$$\n对 $x \\sim \\mathcal{N}(0, \\sigma_x^2)$ 取期望：\n$$\n\\mathbb{E}_x[\\mathrm{KL}] = \\mathbb{E}_x\\left[\\frac{1}{2}(v + a^2x^2 - 1 - \\log v)\\right] = \\frac{1}{2}(v + a^2\\mathbb{E}_x[x^2] - 1 - \\log v) = \\frac{1}{2}(v + a^2\\sigma_x^2 - 1 - \\log v)\n$$\n所以，完整的 KL 项为：\n$$\nK = \\frac{\\beta}{2} \\left(v + a^2\\sigma_x^2 - 1 - \\log v \\right)\n$$\n\n**完整目标与优化**\n总损失为 $\\mathcal{L}_{\\beta}(a, v) = R + K$。为了找到最优参数 $a^{\\star}$ 和 $v^{\\star}$，我们将偏导数设为零。在本问题范围内，$\\sigma_x^2=1$。\n$$\n\\mathcal{L}_{\\beta}(a, v) = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( (1-s-aW)^2 + W^2v \\right) + \\frac{\\beta}{2} \\left( v + a^2 - 1 - \\log v \\right)\n$$\n关于 $a$ 的偏导数：\n$$\n\\frac{\\partial \\mathcal{L}_{\\beta}}{\\partial a} = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( 2(1-s-aW)(-W) \\right) + \\frac{\\beta}{2} (2a) = -\\frac{W(1-s-aW)}{\\sigma_{\\epsilon}^{2}} + \\beta a = 0\n$$\n$$\n-W(1-s) + aW^2 + \\beta a \\sigma_{\\epsilon}^{2} = 0 \\implies a(W^2 + \\beta \\sigma_{\\epsilon}^{2}) = W(1-s)\n$$\n$$\na^{\\star} = \\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}\n$$\n关于 $v$ 的偏导数：\n$$\n\\frac{\\partial \\mathcal{L}_{\\beta}}{\\partial v} = \\frac{W^2}{2\\sigma_{\\epsilon}^{2}} + \\frac{\\beta}{2} \\left(1 - \\frac{1}{v}\\right) = 0\n$$\n$$\n\\frac{W^2}{\\sigma_{\\epsilon}^{2}} + \\beta\\left(1 - \\frac{1}{v}\\right) = 0 \\implies \\beta\\left(\\frac{1}{v}-1\\right) = \\frac{W^2}{\\sigma_{\\epsilon}^{2}} \\implies \\frac{1}{v} = 1 + \\frac{W^2}{\\beta \\sigma_{\\epsilon}^{2}}\n$$\n$$\n\\frac{1}{v} = \\frac{\\beta \\sigma_{\\epsilon}^{2} + W^2}{\\beta \\sigma_{\\epsilon}^{2}} \\implies v^{\\star} = \\frac{\\beta \\sigma_{\\epsilon}^{2}}{\\beta \\sigma_{\\epsilon}^{2} + W^2}\n$$\n\n### 第2部分：跳跃连接参数 $s$ 的分析\n最优编码器参数 $a^{\\star}$ 由 $a^{\\star} = \\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}$ 给出。分母严格为正。因此，$a^{\\star}$ 是 $s$ 的线性函数。\n当跳跃连接强度 $s$ 接近 $1$ 时，项 $(1-s)$ 接近 $0$。因此，$a^{\\star}$ 接近 $0$。\n当 $a^{\\star}=0$ 时，编码器变为 $q_{\\phi}(z \\mid x) = \\mathcal{N}(0, v^{\\star})$。此分布独立于输入 $x$。这意味着关于 $x$ 的信息没有被编码到潜变量 $z$ 中。解码器的跳跃连接项 $sx$ 即使没有来自 $z$ 的信息，也能准确重构 $x$，从而有效地绕过了潜变量瓶颈。这种后验 $q_{\\phi}(z \\mid x)$ 坍塌到先验 $p(z)$（或变得与 $x$ 无关）的现象被称为后验坍塌。量化 $z$ 所携带的关于 $x$ 的信息的互信息 $I(x;z)$ 变为零，因为 $I(x; z) = \\frac{1}{2}\\log(1 + (a^{\\star})^2/v^{\\star}) = \\frac{1}{2}\\log(1) = 0$。\n\n### 第3部分：缓解策略与 Free-Bits 启发式方法\n测试用例旨在评估三种缓解策略：\n1.  **减小 KL 权重 $\\beta$**：较小的 $\\beta$ 会减少偏离先验的惩罚，鼓励模型使用潜变量。情况 C（$s=0.9, \\beta=0.1$）展示了这一点。\n2.  **减弱跳跃连接强度 $s$**：较小的 $s$（离 $1$ 更远）迫使模型更多地依赖 $Wz$ 项进行重构，从而增加 $|a^{\\star}|$。情况 D（$s=0.5$）展示了这一点。\n3.  **Free-bits 启发式方法**：该方法通过强制施加一个最小容量 $\\tau$ 来防止 KL 散度变得过小。KL 项的目标变为 $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$。\n    - 如果无约束最优解 $(a^{\\star}, v^{\\star})$ 产生的 KL 散度 $\\mathrm{KL}^{\\star} \\ge \\tau$，则目标函数保持不变（除了一个常数），并使用此解。\n    - 如果 $\\mathrm{KL}^{\\star}  \\tau$，我们必须在边界 $\\mathrm{KL} = \\tau$ 上找到一个新的最优解。这通过找到一个有效的 KL 权重 $\\beta_{\\text{eff}}  \\beta$ 来实现，使得由此产生的无约束最优解满足 $\\mathrm{KL}(a^{\\star}(\\beta_{\\text{eff}}), v^{\\star}(\\beta_{\\text{eff}})) = \\tau$。由于 KL 散度是 $\\beta$ 的单调递减函数，这个 $\\beta_{\\text{eff}}$ 是唯一的，并且可以通过数值求根算法找到。所提供代码中的计算对情况 F 使用了这种方法。\n\n### 第4部分：计算互信息 $I(x;z)$\n使用推导出的 $a^{\\star}$、$v^{\\star}$ 和给定的公式，其中 $\\sigma_x^2=1$：\n$$ I(x; z) = \\frac{1}{2}\\log\\left(1 + \\frac{(a^{\\star})^2}{v^{\\star}}\\right)$$\n我们代入 $a^{\\star}$ 和 $v^{\\star}$ 的表达式：\n$$\n\\frac{(a^{\\star})^2}{v^{\\star}} = \\frac{\\left(\\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}\\right)^2}{\\frac{\\beta \\sigma_{\\epsilon}^{2}}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}} = \\frac{W^2(1-s)^2}{(W^2 + \\beta \\sigma_{\\epsilon}^{2})^2} \\frac{W^2 + \\beta \\sigma_{\\epsilon}^{2}}{\\beta \\sigma_{\\epsilon}^{2}} = \\frac{W^2(1-s)^2}{\\beta \\sigma_{\\epsilon}^{2}(W^2 + \\beta \\sigma_{\\epsilon}^{2})}\n$$\n因此，互信息为：\n$$\nI(x; z) = \\frac{1}{2}\\log\\left(1 + \\frac{W^2(1-s)^2}{\\beta \\sigma_{\\epsilon}^{2}(W^2 + \\beta \\sigma_{\\epsilon}^{2})}\\right)\n$$\n此公式用于所有“beta”模式的计算。对于“freebits”模式，适用相同的逻辑，但可能需要使用数值方法找到的有效 $\\beta$。\n$W=0$ 的特殊情况导致 $a^{\\star}=0$，因此 $I(x;z)=0$。该公式是稳健的，但问题说明中要求处理这种退化情况的指示已注意到。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mutual information I(x; z) for a linear-Gaussian VAE\n    under different hyperparameter settings to study posterior collapse.\n    \"\"\"\n\n    test_cases = [\n        # (s, W, sigma_eps_sq, beta, tau, regime)\n        (0.0, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # A: Baseline, no skip\n        (1.0, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # B: Full skip, expect collapse\n        (0.9, 1.0, 0.1, 0.1, 0.0, \"beta\"),      # C: Mitigation by low beta\n        (0.5, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # D: Mitigation by moderate s\n        (0.5, 0.0, 0.1, 1.0, 0.0, \"beta\"),      # E: Degenerate case W=0\n        (0.9, 1.0, 0.1, 1.0, 1.0, \"freebits\"), # F: Mitigation by free-bits\n    ]\n\n    results = []\n    \n    # Set sigma_x^2 = 1 as per problem\n    sigma_x_sq = 1.0\n\n    for case in test_cases:\n        s, W, sigma_eps_sq, beta, tau, regime = case\n\n        # Handle degeneracy for W=0 as specified.\n        # This applies to both beta and freebits regimes.\n        if W == 0:\n            results.append(0.0)\n            continue\n        \n        # --- Beta-VAE optimum calculation ---\n        def get_beta_optimum(b_eff):\n            # Calculate optimal a* and v* for a given beta_eff\n            denominator = W**2 + b_eff * sigma_eps_sq\n            a_star = (W * (1 - s)) / denominator\n            v_star = (b_eff * sigma_eps_sq) / denominator\n            return a_star, v_star\n\n        # --- KL divergence calculation ---\n        def get_kl(a, v):\n            # Calculate expected KL divergence for given a, v\n            if v = 0:\n                return float('inf')\n            # Using sigma_x^2 = 1\n            return 0.5 * (v + a**2 * sigma_x_sq - 1 - np.log(v))\n\n        # --- Mutual information calculation ---\n        def get_mi(a, v):\n            # Calculate mutual information I(x;z)\n            if v = 0:\n                return 0.0 # Or handle error appropriately\n            # Using sigma_x^2 = 1\n            return 0.5 * np.log(1 + (a**2 * sigma_x_sq) / v)\n            \n        a_final, v_final = 0, 1 # Default values\n\n        if regime == \"beta\":\n            a_final, v_final = get_beta_optimum(beta)\n\n        elif regime == \"freebits\":\n            # 1. Calculate the unconstrained beta-VAE optimum\n            a_beta, v_beta = get_beta_optimum(beta)\n            kl_beta = get_kl(a_beta, v_beta)\n\n            # 2. Check if the KL is above the threshold tau\n            if kl_beta = tau:\n                # If so, the optimum is the same as the unconstrained one\n                a_final, v_final = a_beta, v_beta\n            else:\n                # 3. If not, find effective beta that makes KL = tau\n                # We need to find `b_eff` where `get_kl(get_beta_optimum(b_eff)) - tau = 0`.\n                # Let gamma = b_eff * sigma_eps_sq.\n                # function to find root of: kl(gamma) - tau = 0\n                def kl_of_gamma_minus_tau(gamma):\n                    if gamma = 0: return float('inf')\n                    denom_g = W**2 + gamma\n                    a_g = (W * (1 - s)) / denom_g\n                    v_g = gamma / denom_g\n                    return get_kl(a_g, v_g) - tau\n\n                # Bisection method to find gamma_eff\n                gamma_orig = beta * sigma_eps_sq\n                low_gamma = 1e-12  # Avoid log(0)\n                high_gamma = gamma_orig\n\n                # We know kl(gamma_orig)  tau, and kl(gamma->0) -> inf.\n                # So a root exists in (0, gamma_orig).\n                for _ in range(100): # 100 iterations for high precision\n                    mid_gamma = (low_gamma + high_gamma) / 2\n                    if kl_of_gamma_minus_tau(mid_gamma)  0:\n                        low_gamma = mid_gamma\n                    else:\n                        high_gamma = mid_gamma\n                \n                gamma_eff = high_gamma\n                beta_eff = gamma_eff / sigma_eps_sq\n                \n                a_final, v_final = get_beta_optimum(beta_eff)\n\n        # Compute the final mutual information for the case\n        mi = get_mi(a_final, v_final)\n        results.append(mi)\n\n    # Format output to 6 decimal places per value\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3197907"}, {"introduction": "标准的VAE使用连续隐变量，但许多现实世界的问题需要离散的表示（例如，类别标签）。Gumbel-Softmax技巧是训练具有离散隐变量的VAE的关键技术，它通过一个连续松弛来允许梯度反向传播。本练习将指导你实现该方法，并分析由温度参数 $\\tau$ 控制的梯度估计的偏差-方差权衡，这是理解和应用此技术的根本[@problem_id:3198001]。", "problem": "考虑一个带有离散潜变量的变分自编码器（VAE）。令潜变量 $z$ 在集合 $\\{1,2,\\dots,K\\}$ 中取值，其编码器分布 $q(z \\mid x)$ 为一个分类分布，概率为 $q_k(x)$，$k \\in \\{1,\\dots,K\\}$。令编码器由 logit $\\alpha \\in \\mathbb{R}^K$ 参数化，使得 $q_k(x) = \\mathrm{softmax}(\\alpha)_k$。令先验分布 $p(z)$ 为一个分类分布，概率为 $p_k$。令解码器指定一个伯努利似然 $p(x=1 \\mid z=k) = \\theta_k$，其中观测值固定为 $x=1$，且 $\\theta_k \\in (0,1)$ 已知。\n\n我们考虑单个观测值 $x=1$ 的证据下界（ELBO），\n$$\n\\mathcal{L}(\\alpha) = \\mathbb{E}_{q(z \\mid x)}\\big[ \\log p(x=1 \\mid z) \\big] - \\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big).\n$$\n编码器使用 Gumbel-Softmax 重参数化方法进行松弛。对于温度 $\\tau  0$，定义松弛样本\n$$\ny = \\mathrm{softmax}\\left(\\frac{\\log \\pi + g}{\\tau}\\right),\n$$\n其中 $\\pi_k = q_k(x)$ 是由 $\\mathrm{softmax}(\\alpha)$ 给出的类别概率，而 $g_k$ 是独立同分布的 Gumbel 随机变量，服从 $\\mathrm{Gumbel}(0,1)$ 分布。在实现时请注意，将 $\\log \\pi$ 替换为未归一化的 logit $\\alpha$ 会得到等价的 $y = \\mathrm{softmax}\\left(\\frac{\\alpha + g}{\\tau}\\right)$，因为在 softmax 函数内部加或减任意常数都不会改变其输出。\n\n通过将期望项替换为其松弛形式来定义松弛目标函数，\n$$\n\\widetilde{\\mathcal{L}}(\\alpha,\\tau) = \\mathbb{E}_{g}\\left[ \\sum_{k=1}^K y_k \\log p(x=1 \\mid z=k) \\right] - \\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big),\n$$\n并考虑其关于 $\\alpha$ 的梯度估计器。真实梯度 $\\nabla_{\\alpha} \\mathcal{L}(\\alpha)$ 是根据第一性原理，使用分类分布的恒等式和 Kullback–Leibler 散度的定义解析计算得出的。松弛梯度 $\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha,\\tau)$ 是通过重参数化技巧，经由 $y$ 来估计的。\n\n您的任务是：\n- 根据 $\\mathbb{E}_{q(z \\mid x)}\\big[ \\log p(x=1 \\mid z) \\big]$ 的定义、分类 softmax 映射以及 $\\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big)$，实现关于 $\\alpha$ 的精确梯度 $\\nabla_{\\alpha} \\mathcal{L}(\\alpha)$。\n- 通过 Gumbel-Softmax 样本 $y$ 和重参数化技巧实现松弛梯度估计器 $\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha,\\tau)$，保持 $\\mathrm{KL}$ 项精确，仅对期望项进行松弛。\n- 对于下方测试套件中的每个温度 $\\tau$，估计松弛梯度估计器相对于精确梯度的偏差和方差：\n  - 偏差定义为松弛梯度估计器的蒙特卡洛均值与精确梯度之差的欧几里得范数，即 $\\|\\mathbb{E}[\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha,\\tau)] - \\nabla_{\\alpha} \\mathcal{L}(\\alpha)\\|_2$。\n  - 方差定义为松弛梯度估计器在独立 Gumbel 抽样下各分量的样本方差之和，即 $\\sum_{j=1}^K \\mathrm{Var}\\left[\\left(\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha,\\tau)\\right)_j\\right]$。\n- 为保证可复现性，请使用固定的随机种子。\n\n使用以下固定配置，以使问题完全明确且可测试：\n- 类别数 $K = 3$。\n- 编码器 logit $\\alpha = [0.3, -0.5, 0.2]$。\n- 先验概率 $p = [0.6, 0.3, 0.1]$。\n- 解码器概率 $\\theta = [0.9, 0.4, 0.1]$，用于 $p(x=1 \\mid z=k) = \\theta_k$。\n- 单个观测值 $x = 1$。\n- 每个温度 $\\tau$ 的蒙特卡洛样本数：$R = 10000$。\n- 随机种子：$0$。\n- Gumbel 抽样：对于 $u \\sim \\mathrm{Uniform}(0,1)$，独立地为每个分量设置 $g = -\\log(-\\log(u))$。\n\n温度 $\\tau$ 的测试套件：\n1. $\\tau = 5.0$ (高温，强平滑)。\n2. $\\tau = 1.0$ (中等温度)。\n3. $\\tau = 0.5$ (较低温度)。\n4. $\\tau = 0.1$ (极低温度)。\n5. $\\tau = 0.01$ (接近离散极限)。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个条目对应一个温度，并包含一对浮点数 $[\\text{bias}, \\text{variance}]$。具体格式如下：\n\"[[b_1,v_1],[b_2,v_2],[b_3,v_3],[b_4,v_4],[b_5,v_5]]\"\n输出行中任何位置均无空格。所有值都应以十进制浮点数形式打印（不含百分号）。", "solution": "用户的要求是分析一个带有离散潜变量的变分自编码器（VAE）的 Gumbel-Softmax 梯度估计器的偏差和方差。这是一个基于机器学习和统计学原理的、有效且适定的问题。我们将首先推导证据下界（ELBO）精确梯度的解析形式，然后推导松弛梯度估计器的形式。随后，我们将实现这些公式，以计算给定参数集下的偏差和方差。\n\n### 1. 预备知识与定义\n令离散潜变量 $z$ 在 $\\{1, \\dots, K\\}$ 中取值。模型组件如下：\n- **编码器**：$q(z=k \\mid x) = \\pi_k$，其中 $\\pi = \\mathrm{softmax}(\\alpha)$，logit 为 $\\alpha \\in \\mathbb{R}^K$。因此，$\\pi_k = \\frac{e^{\\alpha_k}}{\\sum_{j=1}^K e^{\\alpha_j}}$。\n- **先验**：$p(z=k) = p_k$，一个固定的分类分布。\n- **解码器**：$p(x=1 \\mid z=k) = \\theta_k$，一个针对固定观测值 $x=1$ 的伯努利似然。\n\n单个观测值 $x=1$ 的 ELBO 为：\n$$\n\\mathcal{L}(\\alpha) = \\mathbb{E}_{\\pi}\\big[ \\log p(x=1 \\mid z) \\big] - \\mathrm{KL}\\big(\\pi \\,\\|\\, p\\big)\n$$\n展开这些项，我们得到：\n$$\n\\mathcal{L}(\\alpha) = \\sum_{k=1}^K \\pi_k \\log \\theta_k - \\sum_{k=1}^K \\pi_k \\log\\left(\\frac{\\pi_k}{p_k}\\right) = \\sum_{k=1}^K \\pi_k \\left( \\log \\theta_k - \\log \\pi_k + \\log p_k \\right)\n$$\n\n### 2. ELBO 的精确梯度\n为了计算真实梯度 $\\nabla_{\\alpha} \\mathcal{L}(\\alpha)$，我们将 $\\mathcal{L}(\\alpha)$ 对每个分量 $\\alpha_j$ 求导。我们使用链式法则和 softmax 函数的导数 $\\frac{\\partial \\pi_k}{\\partial \\alpha_j} = \\pi_k(\\delta_{kj} - \\pi_j)$，其中 $\\delta_{kj}$ 是克罗内克 delta。\n\nELBO 关于 $\\pi_k$ 的导数是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = (\\log \\theta_k - \\log \\pi_k + \\log p_k) + \\pi_k\\left(-\\frac{1}{\\pi_k}\\right) = \\log\\left(\\frac{\\theta_k p_k}{\\pi_k}\\right) - 1\n$$\n应用链式法则：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_j} = \\sum_{k=1}^K \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} \\frac{\\partial \\pi_k}{\\partial \\alpha_j} = \\sum_{k=1}^K \\left(\\log\\left(\\frac{\\theta_k p_k}{\\pi_k}\\right) - 1\\right) \\pi_k(\\delta_{kj} - \\pi_j)\n$$\n这可以简化为变分推断中的一个标准结果：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_j} = \\pi_j \\left( \\left(\\log \\theta_j - \\log \\pi_j + \\log p_j\\right) - \\mathcal{L}(\\alpha) \\right)\n$$\n以向量形式表示，令 $C_k = \\log \\theta_k - \\log \\pi_k + \\log p_k$，则梯度为：\n$$\n\\nabla_{\\alpha} \\mathcal{L}(\\alpha) = \\pi \\odot (C - \\mathcal{L}(\\alpha)\\mathbf{1})\n$$\n其中 $\\odot$ 是逐元素乘积，$\\mathbf{1}$ 是一个全为 1 的向量。该梯度可以解析计算。\n\n### 3. 松弛梯度估计器\nGumbel-Softmax 重参数化技巧用于获得低方差的梯度估计器。松弛目标函数是：\n$$\n\\widetilde{\\mathcal{L}}(\\alpha, \\tau) = \\mathbb{E}_{g}\\left[ \\sum_{k=1}^K y_k \\log \\theta_k \\right] - \\mathrm{KL}\\big(\\pi \\,\\|\\, p\\big)\n$$\n其中 $y = \\mathrm{softmax}\\left(\\frac{\\alpha + g}{\\tau}\\right)$ 并且 $g_k \\sim \\mathrm{Gumbel}(0,1)$ 是独立同分布的噪声变量。KL 散度项保持精确，不进行重参数化。\n\n松弛目标函数的梯度是：\n$$\n\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha, \\tau) = \\nabla_{\\alpha} \\mathbb{E}_{g}\\left[ \\sum_{k=1}^K y_k \\log \\theta_k \\right] - \\nabla_{\\alpha} \\mathrm{KL}\\big(\\pi \\,\\|\\, p\\big)\n$$\n通过重参数化技巧，我们可以将梯度移到期望内部：\n$$\n\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha, \\tau) = \\mathbb{E}_{g}\\left[ \\nabla_{\\alpha} \\left(\\sum_{k=1}^K y_k \\log \\theta_k \\right) \\right] - \\nabla_{\\alpha} \\mathrm{KL}\\big(\\pi \\,\\|\\, p\\big)\n$$\n此梯度估计器的单个蒙特卡洛样本，记作 $\\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}}$，通过对 $g$ 进行一次采样并求值得到：\n$$\n\\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}} = \\nabla_{\\alpha} \\left(\\sum_{k=1}^K y_k(\\alpha, g) \\log \\theta_k \\right) - \\nabla_{\\alpha} \\mathrm{KL}\\big(\\pi(\\alpha) \\,\\|\\, p\\big)\n$$\n第一项关于 $\\alpha_j$ 的梯度是：\n$$\n\\frac{\\partial}{\\partial \\alpha_j} \\left(\\sum_{k=1}^K y_k \\log \\theta_k \\right) = \\sum_{k=1}^K \\left(\\frac{\\partial y_k}{\\partial \\alpha_j}\\right) \\log \\theta_k = \\sum_{k=1}^K \\left(\\frac{1}{\\tau} y_k (\\delta_{kj} - y_j)\\right) \\log \\theta_k = \\frac{1}{\\tau} y_j \\left(\\log \\theta_j - \\sum_{k=1}^K y_k \\log \\theta_k \\right)\n$$\nKL 项的梯度是确定性的，由下式给出：\n$$\n\\frac{\\partial}{\\partial \\alpha_j} \\mathrm{KL}(\\pi \\,\\|\\, p) = \\pi_j \\left( (\\log \\pi_j - \\log p_j) - \\mathrm{KL}(\\pi \\,\\|\\, p) \\right)\n$$\n这部分与精确梯度推导中的相应项完全相同。\n\n### 4. 偏差和方差计算\n对于每个温度 $\\tau$，我们将执行以下步骤：\n1.  计算一次精确梯度 $\\nabla_{\\alpha} \\mathcal{L}(\\alpha)$。\n2.  生成 $R=10000$ 个松弛梯度估计器样本，$\\{\\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}}^{(i)}\\}_{i=1}^R$。\n3.  估计松弛梯度的均值：$\\mathbb{E}[\\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}}] \\approx \\frac{1}{R} \\sum_{i=1}^R \\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}}^{(i)}$。\n4.  计算偏差，即均值松弛梯度与精确梯度之差的欧几里得范数：$\\|\\text{mean relaxed gradient} - \\text{exact gradient}\\|_2$。\n5.  计算总方差，即松弛梯度估计器样本各分量样本方差之和。\n\n我们预期当 $\\tau \\to 0$ 时，偏差会减小，因为 Gumbel-Softmax 分布趋近于真实的分类分布，使得估计器渐进无偏。相反，我们预期当 $\\tau \\to 0$ 时，方差会增大，这是由于松弛样本的性质变得越来越尖锐、接近离散。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias and variance of the Gumbel-Softmax gradient estimator\n    for a discrete VAE, for various temperatures.\n    \"\"\"\n    \n    def softmax(x):\n        \"\"\"Computes softmax for a 1D vector in a numerically stable way.\"\"\"\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n\n    # --- Problem Configuration ---\n    K = 3\n    alpha = np.array([0.3, -0.5, 0.2])\n    p = np.array([0.6, 0.3, 0.1])\n    theta = np.array([0.9, 0.4, 0.1])\n    R = 10000\n    random_seed = 0\n    tau_values = [5.0, 1.0, 0.5, 0.1, 0.01]\n\n    log_p = np.log(p)\n    log_theta = np.log(theta)\n\n    # --- 1. Compute the Exact Analytical Gradient of the ELBO ---\n    \n    # Encoder probabilities\n    pi = softmax(alpha)\n    log_pi = np.log(pi)\n\n    # Evidence Lower Bound (ELBO)\n    elbo = np.sum(pi * (log_theta - (log_pi - log_p)))\n\n    # Exact gradient: grad_L_j = pi_j * ( (log_theta_j - log_pi_j + log_p_j) - elbo )\n    C = log_theta - log_pi + log_p\n    grad_L_exact = pi * (C - elbo)\n\n    # --- 2. Compute the deterministic gradient of the KL divergence term ---\n    # This is needed for the relaxed gradient estimator.\n    # grad_KL_j = pi_j * ( (log_pi_j - log_p_j) - KL )\n    kl_div = np.sum(pi * (log_pi - log_p))\n    grad_kl_exact = pi * ((log_pi - log_p) - kl_div)\n\n    # --- 3. Loop over temperatures to compute Bias and Variance ---\n    results = []\n    rng = np.random.default_rng(random_seed)\n\n    for tau in tau_values:\n        # Store all Monte Carlo samples of the relaxed gradient\n        relaxed_grad_samples = np.zeros((R, K))\n\n        for i in range(R):\n            # Sample Gumbel noise G_k ~ Gumbel(0, 1)\n            # Add small epsilon to avoid log(0) in case of u=0 or u=1\n            u = rng.uniform(low=1e-10, high=1.0 - 1e-10, size=K)\n            g = -np.log(-np.log(u))\n\n            # Compute relaxed sample y using Gumbel-Softmax reparameterization\n            y = softmax((alpha + g) / tau)\n\n            # --- Compute a single sample of the relaxed gradient estimator ---\n            # It consists of the gradient of the relaxed expectation term minus the\n            # exact gradient of the KL term.\n            \n            # Gradient of the relaxed expectation term: E_g[sum y_k * log(theta_k)]\n            # dE_rel/d(alpha_j) = (1/tau) * y_j * (log(theta_j) - sum_k y_k * log(theta_k))\n            sum_y_log_theta = np.sum(y * log_theta)\n            grad_E_rel_sample = (1.0 / tau) * y * (log_theta - sum_y_log_theta)\n            \n            # The full relaxed gradient sample\n            relaxed_grad_samples[i, :] = grad_E_rel_sample - grad_kl_exact\n        \n        # --- 4. Compute Bias and Variance from the collected samples ---\n        \n        # Mean of relaxed gradient estimates\n        mean_relaxed_grad = np.mean(relaxed_grad_samples, axis=0)\n        \n        # Bias is the L2 norm of the difference between the mean estimate and the true gradient\n        bias = np.linalg.norm(mean_relaxed_grad - grad_L_exact)\n        \n        # Variance is the sum of component-wise sample variances (ddof=1)\n        var_components = np.var(relaxed_grad_samples, axis=0, ddof=1)\n        total_variance = np.sum(var_components)\n        \n        results.append([bias, total_variance])\n    \n    # --- 5. Format and print the final output ---\n    # Format: \"[[b_1,v_1],[b_2,v_2],...]\" with no spaces\n    output_str = \"[\" + \",\".join([f\"[{b},{v}]\" for b, v in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3198001"}]}