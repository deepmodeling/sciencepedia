{"hands_on_practices": [{"introduction": "神经风格迁移的核心在于如何量化和捕捉“风格”。格拉姆矩阵 (Gram matrix) 提供了一种优雅的数学方法，通过计算特征通道之间的相关性来实现这一点。本实践将引导你从零开始构建一个特征提取器和格拉姆矩阵，并动手测试它在图像旋转和裁剪等几何变换下的稳定性，从而让你直观地理解其工作原理及内在属性[@problem_id:3158617]。", "problem": "考虑一个合成实验，以评估在神经风格迁移 (NST) 中使用的基于 Gram 矩阵的风格表示的不变性特性。设风格图像为一个实值函数 $s: \\{0, \\dots, H-1\\} \\times \\{0, \\dots, W-1\\} \\rightarrow \\mathbb{R}$，其定义在空间坐标 $(x, y)$ 上，其中 $H$ 和 $W$ 已指定。使用卷积神经网络 (CNN) 中的空间卷积定义一个单层特征提取器：对于一个包含 $C$ 个滤波器 $\\{k_c\\}_{c=1}^C$ 的滤波器组，每个特征图 $f_c$ 通过二维卷积 $f_c = s \\star k_c$ 计算得出，然后经过一个整流线性单元 (ReLU) 非线性变换，得到 $a_c(x, y) = \\max(0, f_c(x, y))$。将激活值 $a_c$ 堆叠成一个张量 $A \\in \\mathbb{R}^{C \\times H \\times W}$。\n\n将第 $l$ 层的 Gram 矩阵 $G_l \\in \\mathbb{R}^{C \\times C}$ 定义为扁平化后激活值的归一化逐通道内积，\n$$\nG_l = \\frac{1}{N} F F^\\top,\n$$\n其中 $F \\in \\mathbb{R}^{C \\times N}$ 是通过重塑 $A$ 得到的，使得每一行对应一个在空间位置上扁平化的通道，且 $N = H \\cdot W$。Gram 矩阵 $G_l$ 是神经风格迁移中用于捕捉跨空间位置的特征激活的二阶统计量的核心组件。\n\n对于应用于风格图像 $s$ 的给定增强变换 $\\mathcal{T}$，定义增强后的图像 $s_{\\mathcal{T}}$，使用相同的特征提取器计算其对应的 Gram 矩阵 $G_l^{(\\mathcal{T})}$，并通过归一化的 Frobenius 差值来量化基于 Gram 矩阵的风格捕捉发生的变化\n$$\n\\Delta G_l(\\mathcal{T}) = \\frac{\\left\\|G_l^{(\\mathcal{T})} - G_l\\right\\|_F}{\\left\\|G_l\\right\\|_F},\n$$\n其中 $\\|\\cdot\\|_F$ 表示 Frobenius 范数。旋转的角度单位必须是度。\n\n您的任务是编写一个完整、可运行的程序，该程序：\n- 构建一个大小为 $H = 128$ 和 $W = 128$ 的确定性合成风格图像 $s$，该图像由混合的正弦模式定义：\n$$\ns(x,y) = \\sin\\!\\left(2\\pi f_1 \\frac{x}{H}\\right) + \\sin\\!\\left(2\\pi f_2 \\frac{y}{W}\\right) + \\sin\\!\\left(2\\pi f_3 \\left(\\frac{x}{H}\\cos\\alpha + \\frac{y}{W}\\sin\\alpha\\right)\\right),\n$$\n其中 $f_1 = 4$，$f_2 = 6$，$f_3 = 5$ 且 $\\alpha = \\frac{\\pi}{4}$。\n- 构建一个包含 $C = 6$ 个滤波器的滤波器组，其中包括 4 个定向 Gabor 滤波器（方向 $\\theta \\in \\{0^\\circ, 45^\\circ, 90^\\circ, 135^\\circ\\}$，参数为 $\\sigma = 2.5$，$\\gamma = 0.5$ 和波长 $\\lambda = 4.0$），一个标准差为 $\\sigma_{\\text{LoG}} = 2.0$ 的高斯拉普拉斯滤波器，以及一个标准差为 $\\sigma_{\\text{G}} = 1.5$ 的高斯模糊滤波器。每个滤波器都应实现为一个大小为 $K \\times K$（其中 $K = 15$）的实值核，并如上所述，与二维卷积和随后的 ReLU 一起使用。\n- 计算原始图像 $s$ 的 $G_l$，并对下面测试套件中的每个增强变换 $\\mathcal{T}$，计算 $G_l^{(\\mathcal{T})}$ 和 $\\Delta G_l(\\mathcal{T})$。\n\n待评估的增强变换 $\\mathcal{T}$（测试套件；角度单位为度，裁剪尺寸单位为像素）：\n1. 旋转 $\\theta = 0^\\circ$；不裁剪。\n2. 旋转 $\\theta = 90^\\circ$；不裁剪。\n3. 旋转 $\\theta = 180^\\circ$；不裁剪。\n4. 旋转 $\\theta = 0^\\circ$；中心裁剪，尺寸 $64 \\times 64$。\n5. 旋转 $\\theta = 0^\\circ$；左上角裁剪，尺寸 $64 \\times 64$。\n6. 旋转 $\\theta = 90^\\circ$；中心裁剪，尺寸 $64 \\times 64$。\n\n裁剪定义为从旋转后的图像中选择一个连续的、轴对齐的子数组：对于中心裁剪，裁剪窗口从 $(\\lfloor (H - h)/2 \\rfloor, \\lfloor (W - w)/2 \\rfloor)$ 开始；对于左上角裁剪，从 $(0, 0)$ 开始，其中 $(h, w)$ 是裁剪尺寸。\n\n用于将问题建立在第一性原理之上的科学依据：\n- 空间卷积 $f_c = s \\star k_c$ 定义为 $s$ 和 $k_c$ 在空间位移下的空间乘积之和，这是 CNN 中的一个基本操作。\n- Gram 矩阵 $G_l$ 聚合了跨空间位置的二阶通道统计量，并且通过 $N$ 进行归一化后，对于位置的空间置换是不变的。裁剪会改变位置集合，因此会根据特征统计量的平稳性来改变 $G_l$。\n- $90^\\circ$ 倍数的旋转会置换空间位置但不会置换通道；其对 $G_l$ 的影响取决于定向滤波器如何响应旋转后的内容。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,\\dots,r_6]$）的结果，其中每个 $r_i$ 是按上述顺序列出的第 $i$ 个测试用例的 $\\Delta G_l(\\mathcal{T}_i)$ 的浮点值。$\\Delta G_l(\\mathcal{T})$ 值不需要单位。程序不得读取任何输入，也不得访问外部文件或网络。要使用的角度单位是度，所有裁剪尺寸均以像素为单位。", "solution": "该问题要求研究在神经风格迁移 (NST) 中使用的基于 Gram 矩阵的风格表示的不变性特性。这是通过一个合成实验来完成的，该实验涉及一个程序生成的图像、一个固定的滤波器组和一组几何增强变换。风格表示的变化由 Gram 矩阵捕获，并使用归一化的 Frobenius 距离进行量化。\n\n解决方案以结构化的方式进行：首先，我们定义并实现所有必要的组件，包括合成图像、滤波器组以及计算 Gram 矩阵的过程。其次，我们将指定的增强变换应用于图像，并计算 Gram 矩阵产生的偏差。\n\n### 1. 合成风格图像生成\n风格图像 $s(x,y)$ 定义在大小为 $H \\times W$ 的离散网格上，其中 $H=128$，$W=128$。其值由三个正弦波的叠加确定：\n$$\ns(x,y) = \\sin\\!\\left(2\\pi f_1 \\frac{x}{H}\\right) + \\sin\\!\\left(2\\pi f_2 \\frac{y}{W}\\right) + \\sin\\!\\left(2\\pi f_3 \\left(\\frac{x}{H}\\cos\\alpha + \\frac{y}{W}\\sin\\alpha\\right)\\right)\n$$\n参数给定为 $f_1 = 4$，$f_2 = 6$，$f_3 = 5$ 且 $\\alpha = \\frac{\\pi}{4}$。前两项表示垂直和水平的正弦模式，而第三项表示以角度 $\\alpha$ 定向的模式。这种构造提供了一个具有已知频率和方向内容的确定性结构化输入。我们通过为 $x \\in \\{0, \\dots, W-1\\}$ 和 $y \\in \\{0, \\dots, H-1\\}$ 创建二维坐标网格并在每个点上评估该函数来生成此图像。\n\n### 2. 特征提取器和滤波器组\n特征提取器被建模为单个卷积层，后跟一个整流线性单元 (ReLU) 非线性变换。该层由一个包含 $C=6$ 个滤波器的滤波器组组成，每个滤波器的大小为 $K \\times K$，其中 $K=15$。这些滤波器被设计为对不同类型的局部图像结构敏感。\n\n- **Gabor 滤波器（4 个）**：这些是受生物学启发的滤波器，用于模拟视觉皮层中简单细胞的感受野。它们能有效地检测特定方向的边缘和纹理。一个实值二维 Gabor 滤波器核定义为：\n$$\ng(x,y) = \\exp\\left(-\\frac{x'^2 + \\gamma^2 y'^2}{2\\sigma^2}\\right) \\cos\\left(2\\pi \\frac{x'}{\\lambda}\\right)\n$$\n其中 $x' = x \\cos\\theta + y \\sin\\theta$ 且 $y' = -x \\sin\\theta + y \\cos\\theta$。所提供的参数为：方向 $\\theta \\in \\{0^\\circ, 45^\\circ, 90^\\circ, 135^\\circ\\}$，波长 $\\lambda=4.0$，标准差 $\\sigma=2.5$，以及纵横比 $\\gamma=0.5$。为确保这些滤波器作为带通滤波器工作而不影响平均亮度，它们的核被归一化为零均值。\n\n- **高斯拉普拉斯 (LoG) 滤波器（1 个）**：该滤波器是一种有效的斑点检测器，用于寻找被不同强度区域包围的均匀强度区域。它是通过将拉普拉斯算子应用于高斯函数来计算的：\n$$\n\\text{LoG}(x,y) = -\\frac{1}{\\pi\\sigma^4}\\left(1 - \\frac{x^2+y^2}{2\\sigma^2}\\right)e^{-\\frac{x^2+y^2}{2\\sigma^2}}\n$$\n其中 $\\sigma_{\\text{LoG}} = 2.0$。该核本身是零和的。\n\n- **高斯模糊滤波器（1 个）**：这是一种平滑图像的低通滤波器。其核为：\n$$\n\\text{G}(x,y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2+y^2}{2\\sigma^2}}\n$$\n其中 $\\sigma_{\\text{G}} = 1.5$。为保持图像亮度，该核被归一化为总和为 1。\n\n### 3. 特征图与 Gram 矩阵计算\n对于滤波器组中的每个滤波器 $k_c$，通过以下步骤生成一个特征图 $a_c$：\n1.  **卷积**：$f_c = s \\star k_c$。这是一个二维卷积，使用 `same` 填充以确保输出特征图与输入图像具有相同的空间维度。使用对称边界条件来处理边缘效应。\n2.  **ReLU 激活**：$a_c(x,y) = \\max(0, f_c(x,y))$。这引入了非线性，是深度神经网络的一个关键组成部分。\n\n生成的激活图 $\\{a_c\\}_{c=1}^C$ 构成一个张量 $A \\in \\mathbb{R}^{C \\times H' \\times W'}$，其中 $(H', W')$ 是图的空间维度。这些图被扁平化为一个矩阵 $F \\in \\mathbb{R}^{C \\times N}$，其中 $N = H' \\cdot W'$。$F$ 的每一行对应一个单通道的扁平化激活值。\n\n然后，Gram 矩阵 $G_l \\in \\mathbb{R}^{C \\times C}$ 被计算为该矩阵与其自身的归一化外积：\n$$\nG_l = \\frac{1}{N} F F^\\top\n$$\nGram 矩阵的每个元素 $(G_l)_{ij}$ 代表通道 $i$ 和 $j$ 的特征响应之间的相关性，该相关性在所有空间位置上取平均。它作为图像风格的统计摘要。\n\n### 4. 增强变换与差异计算\n实验的核心是评估当输入图像经受几何增强变换 $\\mathcal{T}$ 时，Gram 矩阵如何变化。对于每个增强变换，会创建一个增强后的图像 $s_{\\mathcal{T}}$，并计算其对应的 Gram 矩阵 $G_l^{(\\mathcal{T})}$。增强变换包括：\n- **旋转**：图像按指定角度 $\\theta \\in \\{0^\\circ, 90^\\circ, 180^\\circ\\}$ 旋转。旋转操作会保持图像尺寸不变，原始图像边界之外的区域用零填充。\n- **裁剪**：选择图像的一个子数组。中心裁剪从中间取一个区块，而左上角裁剪从角落取一个区块。裁剪会改变空间域，因此也改变了归一化因子 $N = H' \\cdot W'$。\n\n风格表示的变化通过归一化的 Frobenius 差值来量化：\n$$\n\\Delta G_l(\\mathcal{T}) = \\frac{\\left\\|G_l^{(\\mathcal{T})} - G_l\\right\\|_F}{\\left\\|G_l\\right\\|_F}\n$$\n其中 $\\|M\\|_F = \\sqrt{\\sum_{i,j} M_{ij}^2}$ 是 Frobenius 范数。该度量提供了一个标量值，用于衡量原始 Gram 矩阵和增强后的 Gram 矩阵之间的相对差异。\n\n### 5. 执行测试套件\n程序系统地评估指定的六个测试用例。对于每个用例，它应用相应的旋转和/或裁剪以获得 $s_{\\mathcal{T}}$，计算 $G_l^{(\\mathcal{T})}$，然后计算相对于原始未增强图像的 Gram 矩阵 $G_l$ 的 $\\Delta G_l(\\mathcal{T})$。\n\n- 测试用例 1（旋转 $0^\\circ$，不裁剪）作为对照组，因为 $s_{\\mathcal{T}} = s$，因此 $\\Delta G_l$ 必须为 $0$。\n- 预期 $90^\\circ$ 和 $180^\\circ$ 的旋转会产生非零差异，因为定向 Gabor 滤波器会对旋转后的模式产生不同的响应。\n- 预期裁剪会产生非零差异，因为特征的统计样本发生了变化，并且合成图像的内容在空间上不是平稳的。\n\n最终输出是为每个测试用例计算的 $\\Delta G_l(\\mathcal{T})$ 值的列表，并按要求格式化。", "answer": "```python\nimport numpy as np\nfrom scipy import signal, ndimage\n\ndef create_style_image(H, W, f1, f2, f3, alpha):\n    \"\"\"\n    Constructs the synthetic style image from a mixture of sinusoidal patterns.\n    \"\"\"\n    y_coords, x_coords = np.mgrid[0:H, 0:W]\n    term1 = np.sin(2 * np.pi * f1 * x_coords / W)\n    term2 = np.sin(2 * np.pi * f2 * y_coords / H)\n    term3 = np.sin(2 * np.pi * f3 * (x_coords / W * np.cos(alpha) + y_coords / H * np.sin(alpha)))\n    return term1 + term2 + term3\n\ndef create_gabor_filter(K, sigma, theta_deg, lam, gamma):\n    \"\"\"\n    Creates a real-valued, zero-mean Gabor filter kernel.\n    \"\"\"\n    theta_rad = np.deg2rad(theta_deg)\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    x_theta = x * np.cos(theta_rad) + y * np.sin(theta_rad)\n    y_theta = -x * np.sin(theta_rad) + y * np.cos(theta_rad)\n    \n    gb = np.exp(-(x_theta**2 + gamma**2 * y_theta**2) / (2 * sigma**2)) * np.cos(2 * np.pi * x_theta / lam)\n    \n    # Make the filter zero-mean\n    return gb - np.mean(gb)\n\ndef create_log_filter(K, sigma):\n    \"\"\"\n    Creates a Laplacian-of-Gaussian (LoG) filter kernel.\n    \"\"\"\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    # Unnormalized LoG\n    norm_sq = x**2 + y**2\n    factor1 = (norm_sq - 2 * sigma**2) / (sigma**4)\n    factor2 = np.exp(-norm_sq / (2 * sigma**2))\n    log_kernel = factor1 * factor2\n    \n    # Constant scaling factor is not critical as Gram matrix is correlation-based\n    # and we normalize the final difference. \n    # Let's make it zero-mean for numerical stability.\n    return log_kernel - log_kernel.mean()\n\ndef create_gaussian_filter(K, sigma):\n    \"\"\"\n    Creates a Gaussian blur filter kernel, normalized to sum to 1.\n    \"\"\"\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    g = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n    return g / np.sum(g)\n\ndef get_gram_matrix(image, filters):\n    \"\"\"\n    Computes the Gram matrix for an image given a bank of filters.\n    \"\"\"\n    activations = []\n    for k in filters:\n        # Convolve using 'same' padding to maintain dimensions\n        conv_result = signal.convolve2d(image, k, mode='same', boundary='symm')\n        # Apply ReLU nonlinearity\n        relu_result = np.maximum(0, conv_result)\n        activations.append(relu_result)\n    \n    A = np.array(activations)\n    C, H_prime, W_prime = A.shape\n    N = H_prime * W_prime\n    \n    F = A.reshape(C, N)\n    \n    if N == 0:\n        return np.zeros((C,C))\n\n    G = (1 / N) * (F @ F.T)\n    return G\n\ndef solve():\n    # Problem parameters\n    H, W = 128, 128\n    F1, F2, F3 = 4, 6, 5\n    ALPHA = np.pi / 4\n    K = 15\n    \n    GABOR_SIGMA, GABOR_GAMMA, GABOR_LAM = 2.5, 0.5, 4.0\n    GABOR_THETAS = [0, 45, 90, 135]\n    LOG_SIGMA = 2.0\n    GAUSS_SIGMA = 1.5\n    \n    # Test suite definition\n    test_cases = [\n        {'rot': 0, 'crop': None, 'crop_size': None},\n        {'rot': 90, 'crop': None, 'crop_size': None},\n        {'rot': 180, 'crop': None, 'crop_size': None},\n        {'rot': 0, 'crop': 'center', 'crop_size': (64, 64)},\n        {'rot': 0, 'crop': 'top_left', 'crop_size': (64, 64)},\n        {'rot': 90, 'crop': 'center', 'crop_size': (64, 64)},\n    ]\n\n    # 1. Generate original style image\n    s_orig = create_style_image(H, W, F1, F2, F3, ALPHA)\n    \n    # 2. Build filter bank\n    filters = []\n    for theta in GABOR_THETAS:\n        filters.append(create_gabor_filter(K, GABOR_SIGMA, theta, GABOR_LAM, GABOR_GAMMA))\n    filters.append(create_log_filter(K, LOG_SIGMA))\n    filters.append(create_gaussian_filter(K, GAUSS_SIGMA))\n    \n    # 3. Compute Gram matrix for the original image\n    G_orig = get_gram_matrix(s_orig, filters)\n    norm_G_orig = np.linalg.norm(G_orig, 'fro')\n    \n    results = []\n    for case in test_cases:\n        s_aug = s_orig.copy()\n        \n        # Apply rotation\n        if case['rot'] != 0:\n            s_aug = ndimage.rotate(s_aug, case['rot'], reshape=False, mode='constant', cval=0.0)\n            \n        # Apply cropping\n        if case['crop'] is not None:\n            h_crop, w_crop = case['crop_size']\n            h_img, w_img = s_aug.shape\n            \n            if case['crop'] == 'center':\n                y_start = (h_img - h_crop) // 2\n                x_start = (w_img - w_crop) // 2\n                s_aug = s_aug[y_start : y_start + h_crop, x_start : x_start + w_crop]\n            elif case['crop'] == 'top_left':\n                s_aug = s_aug[0:h_crop, 0:w_crop]\n\n        # Compute Gram matrix for the augmented image\n        G_aug = get_gram_matrix(s_aug, filters)\n        \n        # Compute normalized Frobenius difference\n        diff_norm = np.linalg.norm(G_aug - G_orig, 'fro')\n        \n        if norm_G_orig == 0:\n            delta_G = 0.0 if diff_norm == 0.0 else np.inf\n        else:\n            delta_G = diff_norm / norm_G_orig\n            \n        results.append(delta_G)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3158617"}, {"introduction": "理解了风格的表示方法后，下一步是探究在优化过程中，风格损失和内容损失是如何相互作用的。这个练习通过一个简化的数学模型，让你在一个受控环境中直接观察内容保留与风格模仿之间的“拉锯战”[@problem_id:3158635]。通过模拟风格特征不完整等挑战性场景，你将对神经风格迁移算法的核心权衡机制获得深刻的理解。", "problem": "您将研究一个简化的、纯数学的神经风格迁移模型，在特征空间中探讨当风格信号缺少内容中存在的关键纹理时会发生什么。考虑一个具有通道和空间维度的单层，用矩阵表示。设内容特征图为 $\\phi(c) \\in \\mathbb{R}^{C \\times N}$，风格特征图为 $\\phi(s) \\in \\mathbb{R}^{C \\times N}$，其中 $C$ 是通道数，$N$ 是空间位置数（例如，$N$ 可以是特征图中所有空间位置的像素总数）。待优化的风格化特征是 $\\Phi \\in \\mathbb{R}^{C \\times N}$。风格格拉姆矩阵定义为 $G_{s} = \\frac{1}{N} \\phi(s)\\phi(s)^{\\top} \\in \\mathbb{R}^{C \\times C}$。目标是最小化以下能量函数：\n$$\n\\mathcal{L}(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2} + \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2},\n$$\n其中 $\\alpha \\ge 0$ 和 $\\beta \\ge 0$ 是权重，$\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数。您将直接在特征空间中操作，通过梯度下降法优化 $\\Phi$，起始点为 $\\Phi^{(0)} = \\phi(c)$，使用固定的步长 $\\eta > 0$ 和固定的步数 $T$。\n\n推导和实现的基本依据：\n- 使用弗罗贝尼乌斯范数的定义及其标准变分性质。\n- 对于特征矩阵 $X$，使用格拉姆矩阵的定义 $G = \\frac{1}{N}XX^{\\top}$。\n- 使用矩阵微积分的基本法则，包括迹和内积的恒等式。\n\n您的任务：\n- 从目标函数的定义和矩阵微积分的标准法则出发，推导出 $\\mathcal{L}(\\Phi)$ 关于 $\\Phi$ 的梯度，并设计一个梯度下降算法来计算近似最小化子 $\\Phi^{*}$。\n- 通过将 $\\phi(s)$ 中相应通道置零来构建一个缺少内容中关键纹理的风格，并测试优化后的 $\\Phi^{*}$ 是否相对于 $\\phi(c)$ 坍塌了该通道的能量。将任意通道索引 $i$ 的能量比定义为 $r_{i} = \\frac{\\|\\Phi^{*}_{i,:}\\|_{2}}{\\|\\phi(c)_{i,:}\\|_{2}}$，其中 $\\|\\cdot\\|_{2}$ 是应用于行向量的欧几里得范数。对于一组通道索引 $S$，将平均比率 $R(S)$ 定义为 $r_{i}$ 在 $i \\in S$ 上的算术平均值。\n- 设“缺失”集为 $M = \\{ i \\mid \\|\\phi(s)_{i,:}\\|_{2} = 0 \\}$，“存在”集为 $P = \\{0,\\ldots,C-1\\}\\setminus M$。按照惯例，如果 $M$ 为空，则设 $R(M) = 1.0$；如果 $P$ 为空，则设 $R(P) = 1.0$。\n- 使用坍塌检测阈值 $\\tau = 0.6$：如果 $R(M) \\le \\tau$，则声明发生坍塌。除了比率之外，还需报告最终目标值 $\\mathcal{L}(\\Phi^{*})$。\n\n您必须遵循的实现细节：\n- 使用梯度下降法，采用固定的步长 $\\eta$ 和固定的步数 $T$，从 $\\Phi^{(0)} = \\phi(c)$ 开始。\n- 将 $\\phi(c)$ 和 $\\phi(s)$ 构建为列恒定的秩-1矩阵：设 $v_{c} \\in \\mathbb{R}^{C}$ 和 $v_{s} \\in \\mathbb{R}^{C}$ 为通道能量向量，并设置 $\\phi(c) = v_{c}\\mathbf{1}^{\\top}$ 和 $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$，其中 $\\mathbf{1} \\in \\mathbb{R}^{N}$ 是全1向量。\n\n测试套件：\n- 对所有测试用例，使用 $C = 3$ 和 $N = 4$，因此 $\\phi(c), \\phi(s) \\in \\mathbb{R}^{3 \\times 4}$。设 $\\mathbf{1} \\in \\mathbb{R}^{4}$。\n- 在以下所有情况中，根据指定的 $v_{c}$ 和 $v_{s}$ 构建 $\\phi(c) = v_{c}\\mathbf{1}^{\\top}$ 和 $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$。\n- 使用以下四种情况，涵盖理想情况、缺失通道的风格以及 $\\alpha$ 和 $\\beta$ 的边界条件：\n  - 情况 A（理想情况，风格匹配）：$v_{c} = [2.0, 1.0, 0.5]$，$v_{s} = [2.0, 1.0, 0.5]$，$\\alpha = 1.0$，$\\beta = 3.0$，$\\eta = 0.05$，$T = 400$。\n  - 情况 B（风格在通道 $0$ 缺少关键纹理）：$v_{c} = [2.0, 1.0, 0.5]$，$v_{s} = [0.0, 1.0, 0.5]$，$\\alpha = 1.0$，$\\beta = 3.0$，$\\eta = 0.05$，$T = 400$。\n  - 情况 C（无风格项）：$v_{c} = [2.0, 1.0, 0.5]$，$v_{s} = [0.0, 1.0, 0.5]$，$\\alpha = 1.0$，$\\beta = 0.0$，$\\eta = 0.05$，$T = 400$。\n  - 情况 D（仅有风格，内容权重为零，风格缺少通道 $0$）：$v_{c} = [2.0, 1.0, 0.5]$，$v_{s} = [0.0, 1.0, 0.5]$，$\\alpha = 0.0$，$\\beta = 3.0$，$\\eta = 0.05$，$T = 400$。\n\n对于每种情况，计算：\n- $R(M)$，即缺失通道的平均比率（如果 $M$ 为空，则为 $1.0$）。\n- $R(P)$，即存在通道的平均比率（如果 $P$ 为空，则为 $1.0$）。\n- 最终值 $\\mathcal{L}(\\Phi^{*})$。\n- 一个布尔值“检测到坍塌”，由 $R(M) \\le \\tau$ 给出，其中 $\\tau = 0.6$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。\n- 每个测试用例的结果必须是 $[R(M), R(P), \\mathcal{L}(\\Phi^{*}), \\text{collapse}]$ 形式的列表，其中 $R(M)$ 和 $R(P)$ 四舍五入到 $6$ 位小数，$\\mathcal{L}(\\Phi^{*})$ 四舍五入到 $6$ 位小数，$\\text{collapse}$ 是一个布尔值。\n- 因此，最终打印的行结构为 $[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$，不含多余的空格或文本。", "solution": "该问题是有效的，因为它具有科学依据、良态且客观。它提供了一个自洽的、数学上精确的框架，用于研究一个简化的神经风格迁移模型。所有参数和条件都已指定，从而能够得到一个唯一的、可验证的计算解。\n\n求解过程包括两个主要阶段：首先，推导目标函数 $\\mathcal{L}(\\Phi)$ 的梯度；其次，实现一个梯度下降算法，为每个指定的测试用例找到优化后的特征图 $\\Phi^{*}$。\n\n目标函数由下式给出：\n$$\n\\mathcal{L}(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2} + \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2}\n$$\n这可以写成内容损失 $\\mathcal{L}_c(\\Phi)$ 和风格损失 $\\mathcal{L}_s(\\Phi)$ 的和：\n$$\n\\mathcal{L}_c(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2}\n$$\n$$\n\\mathcal{L}_s(\\Phi) = \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2}\n$$\n$\\mathcal{L}(\\Phi)$ 关于 $\\Phi$ 的梯度是其组成部分梯度的总和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Phi} = \\frac{\\partial \\mathcal{L}_c}{\\partial \\Phi} + \\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi}\n$$\n\n我们现在将使用弗罗贝尼乌斯范数的定义 $\\|\\mathbf{A}\\|_{F}^2 = \\operatorname{tr}(\\mathbf{A}^{\\top}\\mathbf{A})$ 和矩阵微积分的标准法则来推导每个梯度项。\n\n**1. 内容损失的梯度 ($\\mathcal{L}_c$)**\n\n内容损失为 $\\mathcal{L}_c(\\Phi) = \\alpha \\operatorname{tr}\\left((\\Phi - \\phi(c))^{\\top}(\\Phi - \\phi(c))\\right)$。\n使用恒等式 $\\frac{\\partial}{\\partial \\mathbf{X}} \\operatorname{tr}((\\mathbf{X}-\\mathbf{A})^{\\top}(\\mathbf{X}-\\mathbf{A})) = 2(\\mathbf{X}-\\mathbf{A})$，我们可以直接求出梯度。\n令 $\\mathbf{X} = \\Phi$ 和 $\\mathbf{A} = \\phi(c)$。那么，内容损失关于 $\\Phi$ 的梯度是：\n$$\n\\frac{\\partial \\mathcal{L}_c}{\\partial \\Phi} = 2\\alpha(\\Phi - \\phi(c))\n$$\n\n**2. 风格损失的梯度 ($\\mathcal{L}_s$)**\n\n风格损失为 $\\mathcal{L}_s(\\Phi) = \\beta \\left\\| \\mathbf{G}_{\\Phi} - G_{s} \\right\\|_{F}^{2}$，其中 $\\mathbf{G}_{\\Phi} = \\frac{1}{N}\\Phi\\Phi^{\\top}$ 是特征图 $\\Phi$ 的格拉姆矩阵。\n我们可以将其写为 $\\mathcal{L}_s(\\Phi) = \\beta \\operatorname{tr}\\left((\\mathbf{G}_{\\Phi} - G_s)^{\\top}(\\mathbf{G}_{\\Phi} - G_s)\\right)$。\n由于 $\\mathbf{G}_{\\Phi}$ 和 $G_s$ 是对称的，$(\\mathbf{G}_{\\Phi} - G_s)^{\\top} = \\mathbf{G}_{\\Phi} - G_s$。因此，$\\mathcal{L}_s(\\Phi) = \\beta \\operatorname{tr}\\left((\\mathbf{G}_{\\Phi} - G_s)^2\\right)$。\n\n为求梯度，我们使用微分。令 $\\mathbf{E} = \\mathbf{G}_{\\Phi} - G_s$。\n$\\mathcal{L}_s$ 的微分是 $d\\mathcal{L}_s = \\beta \\cdot d(\\operatorname{tr}(\\mathbf{E}^{\\top}\\mathbf{E})) = \\beta \\cdot \\operatorname{tr}(d(\\mathbf{E}^{\\top})\\mathbf{E} + \\mathbf{E}^{\\top}d\\mathbf{E})$。由于 $\\mathbf{E}$ 是对称的，这可以简化为 $d\\mathcal{L}_s = 2\\beta \\operatorname{tr}(\\mathbf{E}^{\\top}d\\mathbf{E})$。\n$\\mathbf{E}$ 的微分是 $d\\mathbf{E} = d\\mathbf{G}_{\\Phi} = d\\left(\\frac{1}{N}\\Phi\\Phi^{\\top}\\right) = \\frac{1}{N}(d\\Phi \\cdot \\Phi^{\\top} + \\Phi \\cdot d\\Phi^{\\top})$。\n将 $d\\mathbf{E}$ 代入 $d\\mathcal{L}_s$ 的表达式中：\n$$\nd\\mathcal{L}_s = 2\\beta \\operatorname{tr}\\left( \\mathbf{E}^{\\top} \\frac{1}{N}(d\\Phi \\cdot \\Phi^{\\top} + \\Phi \\cdot d\\Phi^{\\top}) \\right) = \\frac{2\\beta}{N} \\left( \\operatorname{tr}(\\mathbf{E}^{\\top} d\\Phi \\Phi^{\\top}) + \\operatorname{tr}(\\mathbf{E}^{\\top} \\Phi d\\Phi^{\\top}) \\right)\n$$\n使用迹的循环性质 $\\operatorname{tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) = \\operatorname{tr}(\\mathbf{C}\\mathbf{A}\\mathbf{B})$ 和性质 $\\operatorname{tr}(\\mathbf{A}) = \\operatorname{tr}(\\mathbf{A}^{\\top})$：\n第一项是 $\\operatorname{tr}(\\mathbf{E}^{\\top} d\\Phi \\Phi^{\\top}) = \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}^{\\top}d\\Phi)$。\n第二项是 $\\operatorname{tr}(\\mathbf{E}^{\\top} \\Phi d\\Phi^{\\top}) = \\operatorname{tr}(d\\Phi^{\\top}\\mathbf{E}^{\\top}\\Phi) = \\operatorname{tr}((\\mathbf{E}^{\\top}\\Phi)^{\\top}d\\Phi) = \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi)$。\n由于 $\\mathbf{E}$ 是对称的，$\\mathbf{E}^{\\top}=\\mathbf{E}$。两项是相同的。\n$$\nd\\mathcal{L}_s = \\frac{2\\beta}{N} \\left( \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi) + \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi) \\right) = \\frac{4\\beta}{N} \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi)\n$$\n根据恒等式 $df = \\operatorname{tr}(\\mathbf{A}^{\\top}d\\mathbf{X}) \\implies \\frac{\\partial f}{\\partial \\mathbf{X}} = \\mathbf{A}$，我们确定梯度：\n$$\n\\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi} = \\left(\\frac{4\\beta}{N} \\Phi^{\\top}\\mathbf{E}\\right)^{\\top} = \\frac{4\\beta}{N} \\mathbf{E}^{\\top}\\Phi = \\frac{4\\beta}{N} \\mathbf{E}\\Phi\n$$\n代入 $\\mathbf{E} = \\frac{1}{N}\\Phi\\Phi^{\\top} - G_s$，我们得到：\n$$\n\\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi} = \\frac{4\\beta}{N} \\left(\\frac{1}{N}\\Phi\\Phi^{\\top} - G_s\\right) \\Phi\n$$\n\n**3. 总梯度与梯度下降算法**\n\n结合内容损失和风格损失的梯度，$\\mathcal{L}(\\Phi)$ 的总梯度是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Phi} = 2\\alpha (\\Phi - \\phi(c)) + \\frac{4\\beta}{N} \\left( \\frac{1}{N}\\Phi\\Phi^{\\top} - G_s \\right) \\Phi\n$$\n梯度下降算法迭代更新 $\\Phi$ 以最小化 $\\mathcal{L}(\\Phi)$。从 $\\Phi^{(0)} = \\phi(c)$ 开始，每一步 $t$ 的更新规则是：\n$$\n\\Phi^{(t+1)} = \\Phi^{(t)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\Phi}\\bigg|_{\\Phi=\\Phi^{(t)}}\n$$\n此过程重复固定次数的步骤 $T$。最终优化后的特征图是 $\\Phi^{*} = \\Phi^{(T)}$。\n\n对于 $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$ 的具体实现，风格格拉姆矩阵简化为一个秩-1矩阵：\n$$\nG_s = \\frac{1}{N}\\phi(s)\\phi(s)^{\\top} = \\frac{1}{N}(v_{s}\\mathbf{1}^{\\top})(\\mathbf{1}v_{s}^{\\top}) = \\frac{1}{N}v_{s}(\\mathbf{1}^{\\top}\\mathbf{1})v_{s}^{\\top} = \\frac{1}{N}v_{s}(N)v_{s}^{\\top} = v_{s}v_{s}^{\\top}\n$$\n在实现中使用了这个简化。经过 $T$ 次迭代后，我们计算所需的度量指标：$R(M)$、$R(P)$、$\\mathcal{L}(\\Phi^{*})$ 和坍塌检测布尔值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: Happy path, matching style\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([2.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n        # Case B: Style lacks key texture in channel 0\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n        # Case C: No style term\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 0.0, 'eta': 0.05, 'T': 400},\n        # Case D: Style only, content weight zero, style lacks channel 0\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 0.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    # Format the final output string as per problem specification.\n    # [[R(M),R(P),L,collapse],[...],...]\n    formatted_results = []\n    for r in results:\n        rm, rp, l_val, c_val = r\n        s = f\"[{rm:.6f},{rp:.6f},{l_val:.6f},{str(c_val).lower()}]\"\n        formatted_results.append(s)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_simulation(vc, vs, alpha, beta, eta, T):\n    \"\"\"\n    Performs the gradient descent optimization and calculates final metrics for one test case.\n    \"\"\"\n    C, N = 3, 4\n    tau = 0.6\n    \n    # Construct feature maps and style Gram matrix\n    ones_N = np.ones((1, N))\n    phi_c = vc.reshape(C, 1) @ ones_N\n    \n    # G_s simplifies to the outer product v_s * v_s^T\n    G_s = vs.reshape(C, 1) @ vs.reshape(1, C)\n\n    # Initialize Phi at the content feature map\n    Phi = phi_c.copy()\n    \n    # Gradient Descent Loop\n    for _ in range(T):\n        # Calculate Gram matrix of the current Phi\n        G_Phi = (1/N) * (Phi @ Phi.T)\n        \n        # Calculate content loss gradient\n        grad_content = 2 * alpha * (Phi - phi_c)\n        \n        # Calculate style loss gradient\n        grad_style = (4 * beta / N) * ((G_Phi - G_s) @ Phi)\n        \n        # Total gradient\n        grad_L = grad_content + grad_style\n        \n        # Update Phi\n        Phi -= eta * grad_L\n\n    Phi_star = Phi\n\n    # Calculate final objective value L(Phi*)\n    G_Phi_star = (1/N) * (Phi_star @ Phi_star.T)\n    loss_content_final = alpha * np.linalg.norm(Phi_star - phi_c, 'fro')**2\n    loss_style_final = beta * np.linalg.norm(G_Phi_star - G_s, 'fro')**2\n    L_final = loss_content_final + loss_style_final\n    \n    # Identify missing (M) and present (P) channel sets\n    M_indices = np.where(vs == 0)[0]\n    P_indices = np.where(vs != 0)[0]\n    \n    # Calculate energy ratios r_i\n    phi_c_row_norms = np.linalg.norm(phi_c, axis=1)\n    phi_star_row_norms = np.linalg.norm(Phi_star, axis=1)\n    \n    # Avoid division by zero, although not an issue with the given vc.\n    ratios = np.divide(phi_star_row_norms, phi_c_row_norms, \n                       out=np.zeros_like(phi_c_row_norms, dtype=float), \n                       where=phi_c_row_norms!=0)\n\n    # Calculate average ratio R(M)\n    if len(M_indices) == 0:\n        R_M = 1.0\n    else:\n        R_M = np.mean(ratios[M_indices])\n        \n    # Calculate average ratio R(P)\n    if len(P_indices) == 0:\n        R_P = 1.0\n    else:\n        R_P = np.mean(ratios[P_indices])\n        \n    # Detect collapse\n    collapse_detected = R_M = tau\n\n    return [R_M, R_P, L_final, collapse_detected]\n\nsolve()\n```", "id": "3158635"}, {"introduction": "经典的神经风格迁移算法在赋予图像新风格的同时，有时会破坏内容物体的关键身份特征，例如改变人脸的样貌。这个高级实践将向你展示如何扩展标准的损失函数，引入度量学习中的对比损失 (contrastive loss) [@problem_id:3158668]。通过该练习，你将学会如何通过“推开”不相关的特征并“拉近”相关的特征，来主动保护内容的身份信息，从而实现更具实用价值的风格迁移。", "problem": "要求您从基本定义出发，从头设计并实现一个保持身份的神经风格迁移（NST）目标函数以及一个基于梯度的求解器。考虑由线性映射定义的固定特征提取器。假设图像表示为向量 $x \\in \\mathbb{R}^{n}$，并定义三个特征映射：一个保持身份的映射 $\\phi_{\\mathrm{id}}(x) = A x$（其中 $A \\in \\mathbb{R}^{p \\times n}$），一个内容映射 $\\phi_{\\mathrm{c}}(x) = B x$（其中 $B \\in \\mathbb{R}^{q \\times n}$），以及一个风格映射 $\\phi_{\\mathrm{s}}(x) = W_{\\mathrm{s}} x$（其中 $W_{\\mathrm{s}} \\in \\mathbb{R}^{m t \\times n}$）。风格特征被重塑为 $F(x) = \\mathrm{reshape}(W_{\\mathrm{s}} x, (m, t))$，其格拉姆矩阵（Gram matrix）为 $G(x) = F(x)^{\\top} F(x) \\in \\mathbb{R}^{t \\times t}$。使用以下在文献中经过充分检验并基于线性代数和优化的定义：欧几里得范数（Euclidean norm）和均方误差（Mean Squared Error, MSE）。\n\n- 将内容损失定义为 $L_{\\mathrm{content}}(x; c) = \\left\\|\\phi_{\\mathrm{c}}(x) - \\phi_{\\mathrm{c}}(c)\\right\\|_{2}^{2}$，其中 $c \\in \\mathbb{R}^{n}$ 是给定的内容目标。\n- 将风格损失定义为 $L_{\\mathrm{style}}(x; s) = \\left\\|G(x) - G(s)\\right\\|_{F}^{2}$，其中 $s \\in \\mathbb{R}^{n}$ 是给定的风格目标，$\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数（Frobenius norm）。\n- 将保持身份以排斥负样本的身份对比损失定义为 $L_{\\mathrm{con}}(x; c, \\tilde c) = \\left\\|\\phi_{\\mathrm{id}}(x) - \\phi_{\\mathrm{id}}(c)\\right\\|_{2} - \\left\\|\\phi_{\\mathrm{id}}(x) - \\phi_{\\mathrm{id}}(\\tilde c)\\right\\|_{2}$，其中 $\\tilde c \\in \\mathbb{R}^{n}$ 是一个在身份特征空间中应被排斥的负样本。\n\n总目标函数为\n$$\nL(x) = \\alpha \\, L_{\\mathrm{content}}(x; c)\n+ \\beta \\, L_{\\mathrm{style}}(x; s)\n+ \\gamma \\, L_{\\mathrm{con}}(x; c, \\tilde c),\n$$\n权重为 $\\alpha, \\beta, \\gamma \\in \\mathbb{R}_{\\ge 0}$。您的任务是：\n\n- 仅使用链式法则和线性代数，推导出梯度 $\\nabla_{x} L_{\\mathrm{content}}(x; c)$、基于格拉姆矩阵的风格损失的梯度 $\\nabla_{x} L_{\\mathrm{style}}(x; s)$ 以及梯度 $\\nabla_{x} L_{\\mathrm{con}}(x; c, \\tilde c)$。您必须从欧几里得范数 $\\|z\\|_{2} = \\sqrt{z^{\\top} z}$ 和弗罗贝尼乌斯范数 $\\|M\\|_{F}^{2} = \\mathrm{trace}(M^{\\top} M)$ 的基本定义出发，并通过给定的线性映射应用链式法则。不得假设任何预先推导好的公式。\n- 实现一个梯度下降求解器，该求解器从一个初始化值 $x_{0}$ 开始，以学习率 $\\eta  0$ 迭代更新 $x \\leftarrow x - \\eta \\, \\nabla_{x} L(x)$，共执行固定的 $T$ 步。\n- 对于每个测试用例，在 $T$ 步之后，计算身份距离 $d_{\\mathrm{pos}} = \\left\\|\\phi_{\\mathrm{id}}(x^{(T)}) - \\phi_{\\mathrm{id}}(c)\\right\\|_{2}$ 和 $d_{\\mathrm{neg}} = \\left\\|\\phi_{\\mathrm{id}}(x^{(T)}) - \\phi_{\\mathrm{id}}(\\tilde c)\\right\\|_{2}$，并输出一个布尔值，指示身份是否被保留，该条件定义为 $d_{\\mathrm{pos}}  d_{\\mathrm{neg}}$。\n\n为确保通用性和确定性，请使用具有固定种子的伪随机生成器生成合成参数和数据，并使用固定的维度。具体来说，对于每个测试用例：\n\n- 使用维度 $n = 16$, $m = 8$, $t = 2$，这意味着 $W_{\\mathrm{s}} \\in \\mathbb{R}^{16 \\times 16}$ 和 $F(x) \\in \\mathbb{R}^{8 \\times 2}$，以及格拉姆矩阵 $G(x) \\in \\mathbb{R}^{2 \\times 2}$。\n- 使用每个测试给定的种子，通过零均值单位方差的高斯分布生成 $A \\in \\mathbb{R}^{16 \\times 16}$、$B \\in \\mathbb{R}^{16 \\times 16}$、$W_{\\mathrm{s}} \\in \\mathbb{R}^{16 \\times 16}$ 以及向量 $c, s, \\tilde c \\in \\mathbb{R}^{16}$。\n- 使用 $x_{0} = 0 \\in \\mathbb{R}^{16}$。\n- 使用一个保持身份的小常数 $\\varepsilon = 10^{-8}$ 来避免因欧几里得范数导数而产生的任何分母中的除零错误。\n- 使用每个测试用例指定的权重 $(\\alpha, \\beta, \\gamma)$、学习率 $\\eta$ 和步数 $T$。\n\n测试套件：\n\n- 情况1（正常路径）：种子 $s_{1} = 42$，权重 $(\\alpha, \\beta, \\gamma) = (1.0, 0.5, 1.0)$，学习率 $\\eta = 0.05$，步数 $T = 200$，负样本与 $c$ 和 $s$ 来自相同的高斯分布，但独立生成。\n- 情况2（无对比项的边界条件）：种子 $s_{2} = 7$，权重 $(\\alpha, \\beta, \\gamma) = (1.0, 0.5, 0.0)$，学习率 $\\eta = 0.05$，步数 $T = 200$，负样本独立生成。\n- 情况3（负样本等于正样本的边缘情况）：种子 $s_{3} = 123$，权重 $(\\alpha, \\beta, \\gamma) = (1.0, 0.5, 1.0)$，学习率 $\\eta = 0.05$，步数 $T = 200$，负样本设置为 $\\tilde c = c$。\n\n最终输出规范：\n\n- 您的程序应生成一行输出，其中包含一个逗号分隔的列表，用方括号括起来，列表中的每个元素是对应测试用例的布尔值，按顺序排列。例如，输出格式为 $[r_{1}, r_{2}, r_{3}]$，其中每个 $r_{i} \\in \\{\\mathrm{True}, \\mathrm{False}\\}$。[@problem_id:3158668]", "solution": "该问题要求设计并实现一个保持身份的神经风格迁移（NST）目标函数及其对应的基于梯度的求解器。解决方案包括两个主要阶段：首先，从基本原理出发，对目标函数的每个分量进行梯度的解析推导；其次，实现一个梯度下降算法来优化目标函数，并在一组测试用例上评估指定的身份保持准则。\n\n总损失函数是三个分量的加权和：\n$$L(x) = \\alpha L_{\\mathrm{content}}(x; c) + \\beta L_{\\mathrm{style}}(x; s) + \\gamma L_{\\mathrm{con}}(x; c, \\tilde c)$$\n其中 $x \\in \\mathbb{R}^{n}$ 是待优化的图像，$c \\in \\mathbb{R}^{n}$ 是内容目标，$s \\in \\mathbb{R}^{n}$ 是风格目标，$\\tilde c \\in \\mathbb{R}^{n}$ 是用于身份保持的负样本。权重由非负标量 $\\alpha, \\beta, \\gamma$ 给出。我们将通过分别计算每个分量的梯度来推导梯度 $\\nabla_x L(x)$。\n\n### 1. 内容损失的梯度\n\n内容损失定义为生成图像 $x$ 和内容目标 $c$ 的内容特征之间的平方欧几里得距离：\n$$L_{\\mathrm{content}}(x; c) = \\left\\|\\phi_{\\mathrm{c}}(x) - \\phi_{\\mathrm{c}}(c)\\right\\|_{2}^{2}$$\n给定线性内容映射 $\\phi_{\\mathrm{c}}(z) = Bz$，这变为：\n$$L_{\\mathrm{content}}(x; c) = \\|Bx - Bc\\|_{2}^{2} = \\|B(x-c)\\|_{2}^{2}$$\n为了求关于 $x$ 的梯度，我们首先展开平方欧几里得范数 $\\|v\\|_{2}^{2} = v^{\\top}v$。令 $v = B(x-c)$。\n$$L_{\\mathrm{content}}(x; c) = (B(x-c))^{\\top}(B(x-c)) = (x-c)^{\\top}B^{\\top}B(x-c)$$\n这是一个关于 $(x-c)$ 的二次型。令 $u = x-c$。表达式为 $u^{\\top}(B^{\\top}B)u$。对于一般的二次型 $u^{\\top}Mu$，其关于 $u$ 的梯度由 $\\nabla_u(u^{\\top}Mu) = (M + M^{\\top})u$ 给出。由于矩阵 $B^{\\top}B$ 是对称的，$(B^{\\top}B)^{\\top} = B^{\\top}B$。\n因此，关于 $u$ 的梯度是：\n$$\\nabla_u L_{\\mathrm{content}} = 2(B^{\\top}B)u = 2B^{\\top}B(x-c)$$\n为了求关于 $x$ 的梯度，我们应用链式法则：$\\nabla_x L_{\\mathrm{content}} = (\\frac{\\partial u}{\\partial x})^{\\top} \\nabla_u L_{\\mathrm{content}}$。因为 $u=x-c$，雅可比矩阵 $\\frac{\\partial u}{\\partial x}$ 是单位矩阵 $I$。\n因此，内容损失关于 $x$ 的梯度是：\n$$\\nabla_x L_{\\mathrm{content}}(x; c) = 2B^{\\top}B(x-c)$$\n\n### 2. 风格损失的梯度\n\n风格损失基于生成图像 $x$ 和风格目标 $s$ 的格拉姆矩阵之差的弗罗贝尼乌斯范数：\n$$L_{\\mathrm{style}}(x; s) = \\|G(x) - G(s)\\|_{F}^{2}$$\n其中 $G(z) = F(z)^{\\top}F(z)$ 且 $F(z) = \\mathrm{reshape}(W_s z, (m, t))$。平方弗罗贝尼乌斯范数定义为 $\\|M\\|_{F}^{2} = \\mathrm{trace}(M^{\\top}M)$。\n令 $E = G(x) - G(s)$。损失为 $L_{\\mathrm{style}} = \\mathrm{trace}(E^{\\top}E)$。\n我们使用微分来求解梯度。$L_{\\mathrm{style}}$ 的微分是：\n$$dL_{\\mathrm{style}} = d(\\mathrm{trace}(E^{\\top}E)) = \\mathrm{trace}(d(E^{\\top}E)) = \\mathrm{trace}((dE)^{\\top}E + E^{\\top}dE)$$\n利用性质 $\\mathrm{trace}(A) = \\mathrm{trace}(A^{\\top})$，我们有 $\\mathrm{trace}((dE)^{\\top}E) = \\mathrm{trace}(E^{\\top}dE)$。因此：\n$$dL_{\\mathrm{style}} = 2 \\mathrm{trace}(E^{\\top}dE)$$\n因为 $E = G(x) - G(s)$，其微分是 $dE = dG(x)$（因为 $G(s)$ 对于 $x$ 是常数）。所以，$dL_{\\mathrm{style}} = 2 \\mathrm{trace}((G(x)-G(s))^{\\top} dG(x))$。\n接下来，我们求格拉姆矩阵 $G(x) = F(x)^{\\top}F(x)$ 的微分：\n$$dG(x) = d(F(x)^{\\top})F(x) + F(x)^{\\top}dF(x) = (dF(x))^{\\top}F(x) + F(x)^{\\top}dF(x)$$\n将此代入 $dL_{\\mathrm{style}}$ 的表达式中：\n$$dL_{\\mathrm{style}} = 2 \\mathrm{trace}\\left((G(x)-G(s))^{\\top} \\left((dF(x))^{\\top}F(x) + F(x)^{\\top}dF(x)\\right)\\right)$$\n令 $K = G(x)-G(s)$。注意 $G(x)$ 和 $G(s)$ 是对称的，因此 $K$ 也是对称的 ($K^{\\top}=K$)。\n$$dL_{\\mathrm{style}} = 2 \\mathrm{trace}\\left(K(dF(x))^{\\top}F(x)\\right) + 2 \\mathrm{trace}\\left(K F(x)^{\\top}dF(x)\\right)$$\n对第一项使用迹的循环性质 $\\mathrm{trace}(ABC) = \\mathrm{trace}(CAB)$：$\\mathrm{trace}(K(dF(x))^{\\top}F(x)) = \\mathrm{trace}(F(x)K(dF(x))^{\\top})$。又因为 $\\mathrm{trace}(A) = \\mathrm{trace}(A^\\top)$，所以 $\\mathrm{trace}(F(x)K(dF(x))^{\\top}) = \\mathrm{trace}((F(x)K(dF(x))^{\\top})^\\top) = \\mathrm{trace}(dF(x)K^\\top F(x)^\\top) = \\mathrm{trace}(dF(x)K F(x)^\\top)$。第一项变为 $2\\mathrm{trace}(dF(x)K F(x)^\\top)$。\n第二项是 $2\\mathrm{trace}(K F(x)^{\\top}dF(x))$。\n$dL_{\\mathrm{style}} = 2\\mathrm{trace}(dF(x)K F(x)^\\top) + 2\\mathrm{trace}(K F(x)^{\\top}dF(x)) = 2\\mathrm{trace}((F(x)K^\\top)^\\top dF(x)) + 2\\mathrm{trace}(K F(x)^{\\top}dF(x)) = 2\\mathrm{trace}(K F(x)^{\\top}dF(x)) + 2\\mathrm{trace}(K F(x)^{\\top}dF(x))$。\n$$dL_{\\mathrm{style}} = 4 \\mathrm{trace}\\left((G(x)-G(s))F(x)^{\\top}dF(x)\\right)$$\n标量函数 $f(Z)$ 与其微分之间的一般关系是 $df = \\mathrm{trace}((\\nabla_Z f)^{\\top}dZ)$。由此，我们可以确定 $L_{\\mathrm{style}}$ 关于矩阵 $F(x)$ 的梯度：\n$$\\nabla_{F(x)} L_{\\mathrm{style}} = 4 F(x)(G(x)-G(s))$$\n最后一步是将此梯度与 $x$ 联系起来。令 $y = W_s x$。则 $F(x) = \\mathrm{reshape}(y, (m,t))$。向量化算子 $\\mathrm{vec}(\\cdot)$ 将矩阵展平为向量。$y$ 的元素与 $F(x)$ 的元素相同，只是排列方式不同。因此，关于 $y$ 的梯度 $\\nabla_y L_{\\mathrm{style}}$ 是 $\\nabla_{F(x)} L_{\\mathrm{style}}$ 的向量化版本。\n$$\\nabla_y L_{\\mathrm{style}} = \\mathrm{vec}(\\nabla_{F(x)} L_{\\mathrm{style}})$$\n最后，对线性映射 $y=W_s x$ 应用链式法则：\n$$\\nabla_x L_{\\mathrm{style}} = \\left(\\frac{\\partial y}{\\partial x}\\right)^{\\top}\\nabla_y L_{\\mathrm{style}} = W_s^{\\top} \\nabla_y L_{\\mathrm{style}}$$\n代入表达式，我们得到风格损失关于 $x$ 的梯度：\n$$\\nabla_x L_{\\mathrm{style}}(x; s) = W_s^{\\top} \\mathrm{vec}\\left(4 F(x)(F(x)^{\\top}F(x) - G(s))\\right)$$\n\n### 3. 身份对比损失的梯度\n\n身份对比损失定义为：\n$$L_{\\mathrm{con}}(x; c, \\tilde c) = \\left\\|\\phi_{\\mathrm{id}}(x) - \\phi_{\\mathrm{id}}(c)\\right\\|_{2} - \\left\\|\\phi_{\\mathrm{id}}(x) - \\phi_{\\mathrm{id}}(\\tilde c)\\right\\|_{2}$$\n使用线性身份映射 $\\phi_{\\mathrm{id}}(z)=Az$，这变为：\n$$L_{\\mathrm{con}}(x; c, \\tilde c) = \\|A(x-c)\\|_{2} - \\|A(x-\\tilde c)\\|_{2}$$\n该损失是两个形如 $f(x) = \\|A(x-z)\\|_{2}$ 的项之差。我们来求这类项的梯度。\n使用定义 $\\|v\\|_{2} = \\sqrt{v^{\\top}v}$，我们有 $f(x) = \\sqrt{(A(x-z))^{\\top}(A(x-z))}$。\n我们应用链式法则。令 $v(x) = A(x-z)$。则 $f(x) = \\sqrt{v^{\\top}v}$。\n$$\\nabla_x f(x) = \\frac{1}{2\\sqrt{v^{\\top}v}} \\nabla_x (v^{\\top}v)$$\n项 $\\nabla_x (v^{\\top}v)$ 是 $(x-z)^{\\top}A^{\\top}A(x-z)$ 的梯度。如内容损失部分所示，此梯度为 $2A^{\\top}A(x-z)$。\n将其代回，我们得到：\n$$\\nabla_x f(x) = \\frac{1}{2\\|A(x-z)\\|_{2}} \\left(2A^{\\top}A(x-z)\\right) = \\frac{A^{\\top}A(x-z)}{\\|A(x-z)\\|_{2}}$$\n为防止范数接近于零时出现除零错误，我们按要求在分母上加上一个小的常数 $\\varepsilon  0$。\n将此结果应用于 $L_{\\mathrm{con}}$ 中的两项：\n$$\\nabla_x L_{\\mathrm{con}}(x; c, \\tilde c) = \\frac{A^{\\top}A(x-c)}{\\|A(x-c)\\|_{2} + \\varepsilon} - \\frac{A^{\\top}A(x-\\tilde c)}{\\|A(x-\\tilde c)\\|_{2} + \\varepsilon}$$\n\n### 4. 总梯度和求解器\n\n总梯度是各个梯度的加权和：\n$$\\nabla_x L(x) = \\alpha \\nabla_x L_{\\mathrm{content}}(x; c) + \\beta \\nabla_x L_{\\mathrm{style}}(x; s) + \\gamma \\nabla_x L_{\\mathrm{con}}(x; c, \\tilde c)$$\n求解器使用梯度下降法迭代更新图像向量 $x$，从初始值 $x_0 = 0$ 开始。对于给定的学习率 $\\eta  0$，在 $k = 0, 1, \\dots, T-1$ 的迭代中：\n$$x_{k+1} = x_k - \\eta \\nabla_x L(x_k)$$\n经过 $T$ 次迭代后，最终的图像 $x^{(T)}$ 用于评估身份保持条件 $d_{\\mathrm{pos}}  d_{\\mathrm{neg}}$，其中 $d_{\\mathrm{pos}} = \\|A(x^{(T)}-c)\\|_2$ 且 $d_{\\mathrm{neg}} = \\|A(x^{(T)}-\\tilde{c})\\|_2$。\n\n实现将遵循这些推导出的公式，对指定的测试用例执行优化和评估。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the identity-preserving NST problem for the given test cases.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        {'seed': 42, 'alpha': 1.0, 'beta': 0.5, 'gamma': 1.0, 'eta': 0.05, 'T': 200, 'neg_is_pos': False},\n        {'seed': 7, 'alpha': 1.0, 'beta': 0.5, 'gamma': 0.0, 'eta': 0.05, 'T': 200, 'neg_is_pos': False},\n        {'seed': 123, 'alpha': 1.0, 'beta': 0.5, 'gamma': 1.0, 'eta': 0.05, 'T': 200, 'neg_is_pos': True},\n    ]\n\n    results = []\n\n    # Fixed parameters from the problem description\n    n_dim = 16\n    m_dim = 8\n    t_dim = 2\n    epsilon = 1e-8\n\n    for case in test_cases:\n        seed = case['seed']\n        alpha = case['alpha']\n        beta = case['beta']\n        gamma = case['gamma']\n        eta = case['eta']\n        T = case['T']\n\n        # Generate synthetic data with the specified seed\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(n_dim, n_dim))\n        B = rng.normal(size=(n_dim, n_dim))\n        Ws = rng.normal(size=(m_dim * t_dim, n_dim))\n        \n        c = rng.normal(size=n_dim)\n        s = rng.normal(size=n_dim)\n        \n        if case['neg_is_pos']:\n            tilde_c = c.copy()\n        else:\n            tilde_c = rng.normal(size=n_dim)\n\n        # Pre-compute constant matrices for efficiency\n        AtA = A.T @ A\n        BtB = B.T @ B\n        \n        # Pre-compute style target's Gram matrix\n        s_features = Ws @ s\n        Fs = s_features.reshape((m_dim, t_dim))\n        Gs = Fs.T @ Fs\n\n        # Initialize x\n        x = np.zeros(n_dim)\n\n        # Gradient Descent loop\n        for _ in range(T):\n            # 1. Content Loss Gradient\n            # grad_content = 2 * B^T * B * (x - c)\n            grad_content = 2 * (BtB @ (x - c))\n\n            # 2. Style Loss Gradient\n            # grad_style = Ws^T * vec(4 * F(x) * (G(x) - G(s)))\n            if beta > 0:\n                x_features = Ws @ x\n                Fx = x_features.reshape((m_dim, t_dim))\n                Gx = Fx.T @ Fx\n                grad_F = 4 * (Fx @ (Gx - Gs))\n                grad_y = grad_F.flatten()\n                grad_style = Ws.T @ grad_y\n            else:\n                grad_style = np.zeros(n_dim)\n            \n            # 3. Contrastive Loss Gradient\n            # grad_con = (A^T*A*(x-c))/(||A(x-c)||_2 + eps) - (A^T*A*(x-tilde_c))/(||A(x-tilde_c)||_2 + eps)\n            if gamma > 0:\n                # To handle the case where tilde_c = c, which would lead to 0/0\n                if np.array_equal(c, tilde_c):\n                    grad_con = np.zeros(n_dim)\n                else:\n                    v_pos = A @ (x - c)\n                    norm_pos = np.linalg.norm(v_pos)\n                    term_pos = (AtA @ (x - c)) / (norm_pos + epsilon)\n\n                    v_neg = A @ (x - tilde_c)\n                    norm_neg = np.linalg.norm(v_neg)\n                    term_neg = (AtA @ (x - tilde_c)) / (norm_neg + epsilon)\n                    \n                    grad_con = term_pos - term_neg\n            else:\n                grad_con = np.zeros(n_dim)\n            \n            # Total gradient\n            total_grad = alpha * grad_content + beta * grad_style + gamma * grad_con\n\n            # Update x\n            x = x - eta * total_grad\n        \n        # After T steps, compute identity distances\n        x_final = x\n        d_pos = np.linalg.norm(A @ (x_final - c))\n        d_neg = np.linalg.norm(A @ (x_final - tilde_c))\n\n        # Check preservation condition\n        is_preserved = d_pos  d_neg\n        results.append(is_preserved)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3158668"}]}