{"hands_on_practices": [{"introduction": "现代关键点检测模型通常预测概率热图，其中每个像素的值代表该位置存在关键点的置信度。然而，我们的最终目标是获得精确的亚像素坐标。一个简单的 `argmax` 操作虽然能找到热图的峰值，但它是不可微分的，这阻碍了端到端的模型训练。本练习将引导你实现“软-argmax”方法，这是一种利用 softmax 函数将热图平滑地转换为精确坐标的关键技术。通过这个实践，你将探索如何构建一个完全可微分的坐标提取层，并分析其关键参数（如温度 $\\tau$）如何影响预测的稳定性和对梯度的敏感性。 [@problem_id:3139976]", "problem": "要求您设计并分析一个用于姿态估计中关键点检测的可微非极大值抑制层，通过平滑的软性选择替代热图上的硬性 argmax 选择。从以下基本原理出发：称为 softmax 的指数族归一化定义、权重和为一的概率解释、离散随机变量的期望值以及微积分中的链式法则。\n\n假设给定一个规则网格上的离散热图，表示为矩阵 $H \\in \\mathbb{R}^{m \\times n}$，其元素为 $H_{y,x}$，其中行索引 $y \\in \\{0,\\dots,m-1\\}$，列索引 $x \\in \\{0,\\dots,n-1\\}$。将该网格展平为一个包含 $N = m \\cdot n$ 个位置的列表，由索引 $i \\in \\{0,\\dots,N-1\\}$ 标记，每个位置关联一个像素坐标 $\\mathbf{p}_i = (x_i, y_i)$ 和一个热图值 $H_i$。通过带有温度参数 $\\tau > 0$ 的 softmax 权重定义一种可微软性选择：\n$$\ns_i = \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} \\quad \\text{for } i \\in \\{0,\\dots,N-1\\}.\n$$\n将 $\\{s_i\\}$ 解释为像素位置上的一个离散概率分布。使用这些权重将可微关键点定义为期望坐标 (soft-argmax)：\n$$\n\\hat{\\mathbf{p}} = \\sum_{i=0}^{N-1} s_i \\, \\mathbf{p}_i.\n$$\n从第一性原理出发，通过对 softmax 定义应用链式法则，推导 $\\hat{\\mathbf{p}}$ 相对于热图值 $\\{H_i\\}$ 的梯度。使用推导出的表达式，将可微性灵敏度度量定义为 $\\hat{\\mathbf{p}}$ 关于 $H$ 的雅可比矩阵的 Frobenius 范数，即：\n$$\nG = \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}\\right\\|_F = \\sqrt{\\sum_{i=0}^{N-1} \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_i}\\right\\|_2^2}.\n$$\n为评估峰值的锐度，定义两个关于分布 $\\{s_i\\}$ 相对于 $\\hat{\\mathbf{p}}$ 的互补标量度量：\n- 围绕 $\\hat{\\mathbf{p}}$ 的空间方差：\n$$\nV = \\sum_{i=0}^{N-1} s_i \\, \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2.\n$$\n- 归一化 Shannon 熵：\n$$\nE = -\\frac{1}{\\log N} \\sum_{i=0}^{N-1} s_i \\log s_i,\n$$\n其值域为 $[0,1]$，接近 $0$ 的值表示尖锐的峰值，接近 $1$ 的值表示弥散的分布。\n\n您的程序必须实现上述层，并为给定的 $(H,\\tau)$ 对计算以下输出：soft-argmax 坐标 $(\\hat{x}, \\hat{y})$、空间方差 $V$、归一化熵 $E$ 和可微性灵敏度 $G$。所有计算均无单位。本任务不涉及角度。\n\n推导要求：\n- 从 softmax 定义出发，验证 $\\sum_i s_i = 1$，并使用期望值定义来定义 $\\hat{\\mathbf{p}}$。\n- 在不使用除链式法则和指数函数性质之外的任何已知快捷方式的情况下，推导 $\\frac{\\partial s_i}{\\partial H_k}$，然后推导 $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$。\n- 将 $G$ 的表达式简化为可直接由 $\\{s_i\\}$, $\\{\\mathbf{p}_i\\}$ 和 $\\hat{\\mathbf{p}}$ 计算的形式。\n\n实现要求：\n- 使用网格坐标 $\\mathbf{p}_i = (x_i, y_i)$，其中 $x_i \\in \\{0,\\dots,n-1\\}$ 和 $y_i \\in \\{0,\\dots,m-1\\}$，通过常规的行主序展平进行映射。\n- 实现数值稳定的计算；对于熵，确保仅对严格为正的参数计算对数。\n\n测试套件：\n- 所有热图大小均为 $m = 5, n = 5$ (即 $5 \\times 5$)，按行主序展平。索引从 0 开始。\n- 案例 1 (理想情况，非常尖锐的单峰)：$H$ 全为零，除了 $H_{2,2} = 10$，$\\tau = 0.1$。\n- 案例 2 (中等温度)：$H$ 与案例 1 相同，$\\tau = 1.0$。\n- 案例 3 (两个对称峰)：$H$ 全为零，除了 $H_{1,1} = 8$ 和 $H_{3,3} = 8$，$\\tau = 0.5$。\n- 案例 4 (均匀热图，边界弥散情况)：$H$ 全为一，$\\tau = 2.0$。\n\n对于每个案例，计算元组 $(\\hat{x}, \\hat{y}, V, E, G)$ 并将所有结果按以下顺序聚合到一个扁平列表中：案例 1 的结果，然后是案例 2、案例 3 和案例 4 的结果。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表 (例如，$[\\text{result1},\\text{result2},\\dots]$)。所有结果必须是浮点数。不需要单位。", "solution": "该问题是有效的，因为它在数学上是适定的，其科学基础是深度学习和微积分的原理，并提供了一套完整且一致的定义和约束。我们将继续进行推导和实现。\n\n我们的目标是设计一个可微的软性关键点选择层。这包括推导 soft-argmax 坐标相对于输入热图的梯度，并定义几个度量来分析该层的行为。\n\n**1. 基本定义**\n\n给定一个热图 $H \\in \\mathbb{R}^{m \\times n}$。我们将其展平为一个包含 $N = m \\cdot n$ 个值的向量 $\\{H_i\\}$，其中每个索引 $i$ 对应一个唯一的像素坐标 $\\mathbf{p}_i = (x_i, y_i)$。\n\n可微选择的核心是 softmax 函数，它将热图值转换为像素位置上的一个概率分布 $\\{s_i\\}$。对于温度参数 $\\tau > 0$，第 $i$ 个位置的 softmax 权重为：\n$$\ns_i = \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)}\n$$\n这些权重构成了一个有效的概率分布，因为对所有 $i$ 都有 $s_i \\ge 0$，并且它们的和为 1：\n$$\n\\sum_{i=0}^{N-1} s_i = \\sum_{i=0}^{N-1} \\frac{\\exp\\left(\\frac{H_i}{\\tau}\\right)}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} = \\frac{1}{\\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)} \\sum_{i=0}^{N-1} \\exp\\left(\\frac{H_i}{\\tau}\\right) = 1\n$$\n使用这种概率解释，soft-argmax 关键点坐标 $\\hat{\\mathbf{p}}$ 被定义为在分布 $\\{s_i\\}$ 下像素坐标 $\\mathbf{p}_i$ 的期望值：\n$$\n\\hat{\\mathbf{p}} = \\sum_{i=0}^{N-1} s_i \\, \\mathbf{p}_i\n$$\n其中 $\\hat{\\mathbf{p}}$ 是一个二维向量 $(\\hat{x}, \\hat{y})$。\n\n**2. 梯度的推导**\n\n为确保该层可用于基于梯度的优化框架（如训练神经网络），我们必须求出输出 $\\hat{\\mathbf{p}}$ 相对于输入热图值 $\\{H_k\\}$ 的导数。我们应用链式法则。$\\hat{\\mathbf{p}}$ 相对于单个热图条目 $H_k$ 的导数为：\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\left( \\sum_{i=0}^{N-1} s_i \\mathbf{p}_i \\right) = \\sum_{i=0}^{N-1} \\left( \\frac{\\partial s_i}{\\partial H_k} \\right) \\mathbf{p}_i\n$$\n核心任务是计算 softmax 函数的导数 $\\frac{\\partial s_i}{\\partial H_k}$。令 $Z = \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right)$。那么 $s_i = Z^{-1} \\exp\\left(\\frac{H_i}{\\tau}\\right)$。使用商法则（或对 $Z^{-1}$ 使用积法则）：\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\left( \\frac{\\exp(H_i/\\tau)}{Z} \\right) = \\frac{ \\left( \\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_i}{\\tau}\\right) \\right) Z - \\exp\\left(\\frac{H_i}{\\tau}\\right) \\left( \\frac{\\partial Z}{\\partial H_k} \\right) }{Z^2}\n$$\n各组成部分的导数是：\n$$\n\\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_i}{\\tau}\\right) = \\exp\\left(\\frac{H_i}{\\tau}\\right) \\cdot \\frac{\\partial}{\\partial H_k}\\left(\\frac{H_i}{\\tau}\\right) = \\exp\\left(\\frac{H_i}{\\tau}\\right) \\frac{1}{\\tau} \\delta_{ik}\n$$\n其中 $\\delta_{ik}$ 是 Kronecker delta。\n$$\n\\frac{\\partial Z}{\\partial H_k} = \\frac{\\partial}{\\partial H_k} \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right) = \\sum_{j=0}^{N-1} \\frac{\\partial}{\\partial H_k} \\exp\\left(\\frac{H_j}{\\tau}\\right) = \\sum_{j=0}^{N-1} \\exp\\left(\\frac{H_j}{\\tau}\\right) \\frac{1}{\\tau} \\delta_{jk} = \\frac{1}{\\tau} \\exp\\left(\\frac{H_k}{\\tau}\\right)\n$$\n将这些代回商法则表达式中：\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{ \\left( \\frac{1}{\\tau} \\delta_{ik} \\exp\\left(\\frac{H_i}{\\tau}\\right) \\right) Z - \\exp\\left(\\frac{H_i}{\\tau}\\right) \\left( \\frac{1}{\\tau} \\exp\\left(\\frac{H_k}{\\tau}\\right) \\right) }{Z^2}\n$$\n$$\n= \\frac{1}{\\tau} \\left( \\delta_{ik} \\frac{\\exp(H_i/\\tau)}{Z} - \\frac{\\exp(H_i/\\tau)}{Z} \\frac{\\exp(H_k/\\tau)}{Z} \\right)\n$$\n通过识别 $s_i$ 和 $s_k$ 的定义，此式可简化为众所周知的 softmax 的雅可比矩阵：\n$$\n\\frac{\\partial s_i}{\\partial H_k} = \\frac{1}{\\tau} (s_i \\delta_{ik} - s_i s_k)\n$$\n现在，我们将其代入 $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$ 的表达式中：\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\sum_{i=0}^{N-1} \\frac{1}{\\tau} (s_i \\delta_{ik} - s_i s_k) \\mathbf{p}_i = \\frac{1}{\\tau} \\left( \\sum_{i=0}^{N-1} s_i \\delta_{ik} \\mathbf{p}_i - \\sum_{i=0}^{N-1} s_i s_k \\mathbf{p}_i \\right)\n$$\n第一个求和式在 $i=k$ 时坍缩为单项：$s_k \\mathbf{p}_k$。对于第二个求和式，$s_k$ 是一个常数因子：$s_k \\sum_{i=0}^{N-1} s_i \\mathbf{p}_i = s_k \\hat{\\mathbf{p}}$。\n这就得出了梯度的最终简化表达式：\n$$\n\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k} = \\frac{1}{\\tau} (s_k \\mathbf{p}_k - s_k \\hat{\\mathbf{p}}) = \\frac{s_k}{\\tau} (\\mathbf{p}_k - \\hat{\\mathbf{p}})\n$$\n\n**3. 可微性灵敏度度量的推导**\n\n灵敏度度量 $G$ 是雅可比矩阵 $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}$ 的 Frobenius 范数，该矩阵的列是向量 $\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}$。\n$$\nG = \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H}\\right\\|_F = \\sqrt{\\sum_{k=0}^{N-1} \\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}\\right\\|_2^2}\n$$\n使用我们推导出的梯度：\n$$\n\\left\\|\\frac{\\partial \\hat{\\mathbf{p}}}{\\partial H_k}\\right\\|_2^2 = \\left\\| \\frac{s_k}{\\tau} (\\mathbf{p}_k - \\hat{\\mathbf{p}}) \\right\\|_2^2 = \\left(\\frac{s_k}{\\tau}\\right)^2 \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2\n$$\n对所有 $k$ 求和：\n$$\nG^2 = \\sum_{k=0}^{N-1} \\frac{s_k^2}{\\tau^2} \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2 = \\frac{1}{\\tau^2} \\sum_{k=0}^{N-1} s_k^2 \\left\\| \\mathbf{p}_k - \\hat{\\mathbf{p}} \\right\\|_2^2\n$$\n因此，$G$ 可以计算为：\n$$\nG = \\frac{1}{\\tau} \\sqrt{\\sum_{i=0}^{N-1} s_i^2 \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2}\n$$\n该表达式可直接由 softmax 权重 $\\{s_i\\}$、像素坐标 $\\{\\mathbf{p}_i\\}$、soft-argmax 坐标 $\\hat{\\mathbf{p}}$ 和温度 $\\tau$ 计算得出。\n\n**4. 峰值锐度度量**\n\n另外两个度量已在问题陈述中定义，不需要进一步推导。\n- 空间方差 $V$ 衡量了与平均坐标 $\\hat{\\mathbf{p}}$ 的期望平方距离：\n$$\nV = \\sum_{i=0}^{N-1} s_i \\, \\left\\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\right\\|_2^2\n$$\n- 归一化 Shannon 熵 $E$ 衡量了分布 $\\{s_i\\}$ 的不确定性或弥散度：\n$$\nE = -\\frac{1}{\\log N} \\sum_{i=0}^{N-1} s_i \\log s_i\n$$\n当 $s_i = 0$ 时，项 $s_i \\log s_i$ 取值为 $0$。\n\n**5. 实现计划**\n实现将遵循这些推导出的公式。\n1.  生成坐标网格 $\\mathbf{p}_i = (x_i, y_i)$。\n2.  给定输入热图 $H$ 和温度 $\\tau$，计算展平的热图值向量 $\\{H_i\\}$。\n3.  使用 log-sum-exp 技巧计算 softmax 权重 $\\{s_i\\}$ 以保证数值稳定性，防止上溢。\n4.  计算 soft-argmax 坐标 $\\hat{\\mathbf{p}} = \\sum s_i \\mathbf{p}_i$。\n5.  计算空间方差 $V = \\sum s_i \\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\|_2^2$。\n6.  计算归一化熵 $E$，确保处理 $s_i=0$ 的情况。\n7.  计算灵敏度 $G = \\frac{1}{\\tau} \\sqrt{\\sum s_i^2 \\|\\mathbf{p}_i - \\hat{\\mathbf{p}}\\|_2^2}$。\n8.  为每个测试案例返回计算出的元组 $(\\hat{x}, \\hat{y}, V, E, G)$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(H: np.ndarray, tau: float) -> tuple[float, float, float, float, float]:\n    \"\"\"\n    Computes soft-argmax coordinates and related metrics for a given heatmap.\n\n    Args:\n        H (np.ndarray): A 2D numpy array representing the heatmap.\n        tau (float): The temperature parameter for the softmax function.\n\n    Returns:\n        tuple[float, float, float, float, float]: A tuple containing\n        (x_hat, y_hat, V, E, G).\n    \"\"\"\n    m, n = H.shape\n    N = float(m * n)\n\n    # 1. Generate grid coordinates p_i = (x_i, y_i)\n    # np.meshgrid creates coordinate matrices from coordinate vectors.\n    # The 'ij' indexing gives row-major compatible grids.\n    x_coords, y_coords = np.meshgrid(np.arange(n), np.arange(m), indexing='xy')\n    \n    # Flatten coordinates into an (N, 2) array of (x, y) pairs\n    # This corresponds to row-major flattening of the heatmap.\n    p = np.stack([x_coords.ravel(), y_coords.ravel()], axis=1)\n\n    # Flatten the heatmap H\n    H_flat = H.ravel()\n\n    # 2. Compute softmax weights s_i with numerical stability (log-sum-exp trick)\n    a = H_flat / tau\n    # Subtracting the max value prevents overflow in exp\n    a_max = np.max(a)\n    exp_a = np.exp(a - a_max)\n    sum_exp_a = np.sum(exp_a)\n    s = exp_a / sum_exp_a\n\n    # 3. Compute soft-argmax p_hat = (x_hat, y_hat)\n    # p_hat is the expected coordinate E[p] under distribution s.\n    # s is (N,), p is (N, 2). s[:, np.newaxis] makes s (N, 1) for broadcasting.\n    p_hat = np.sum(s[:, np.newaxis] * p, axis=0)\n    x_hat, y_hat = p_hat[0], p_hat[1]\n\n    # 4. Compute spatial variance V\n    # diff_p is (N, 2) array of vectors (p_i - p_hat)\n    diff_p = p - p_hat\n    # sq_dist is (N,) array of squared Euclidean distances ||p_i - p_hat||^2\n    sq_dist = np.sum(diff_p**2, axis=1)\n    V = np.sum(s * sq_dist)\n\n    # 5. Compute normalized Shannon entropy E\n    # To avoid log(0), we select only s_i > 0.\n    # The limit of s*log(s) as s->0 is 0.\n    s_positive = s[s > 0]\n    # The entropy is sum(-s_i * log(s_i))\n    entropy = -np.sum(s_positive * np.log(s_positive))\n    E = entropy / np.log(N)\n\n    # 6. Compute differentiability sensitivity G\n    # The formular for G^2 is (1/tau^2) * sum(s_i^2 * ||p_i - p_hat||^2)\n    g_sum_term = np.sum(s**2 * sq_dist)\n    G = (1.0 / tau) * np.sqrt(g_sum_term)\n    \n    return (x_hat, y_hat, V, E, G)\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes metrics for each, and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    m, n = 5, 5\n\n    # Case 1: Sharp single peak\n    H1 = np.zeros((m, n), dtype=float)\n    H1[2, 2] = 10.0\n    tau1 = 0.1\n\n    # Case 2: Moderate temperature single peak\n    H2 = np.zeros((m, n), dtype=float)\n    H2[2, 2] = 10.0\n    tau2 = 1.0\n\n    # Case 3: Two symmetric peaks\n    H3 = np.zeros((m, n), dtype=float)\n    H3[1, 1] = 8.0\n    H3[3, 3] = 8.0\n    tau3 = 0.5\n\n    # Case 4: Uniform heatmap\n    H4 = np.ones((m, n), dtype=float)\n    tau4 = 2.0\n\n    test_cases = [\n        (H1, tau1),\n        (H2, tau2),\n        (H3, tau3),\n        (H4, tau4),\n    ]\n\n    all_results = []\n    for H, tau in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        results = compute_metrics(H, tau)\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3139976"}, {"introduction": "即使模型训练完成，我们依然可以通过巧妙的后处理技术来进一步提升其预测精度，其中测试时数据增强（Test-Time Augmentation, TTA）是一种常用且高效的策略。许多姿态估计任务的对象（例如人体或动物）都具有天然的左右对称性。本练习将带你探索如何利用这种对称性，通过对图像进行水平翻转并融合原始预测和翻转后的预测，来生成更加鲁棒和一致的关键点坐标。你将学习如何量化模型预测的对称性误差，并实现一个一致性增强步骤来校准和优化最终结果。 [@problem_id:3140014]", "problem": "考虑一个关键点检测器，它能预测图像上固定数量关键点的二维（2D）像素坐标。设图像表示为 $I$，检测器的预测为 $\\hat{K}(I) \\in \\mathbb{R}^{N \\times 2}$，其中 $N$ 是关键点的数量，每一行包含 $(x,y)$ 像素坐标。设 $F$ 表示作用于像素坐标的水平翻转变换，其定义为\n$$\nF(x,y;w) = (w-1-x, y),\n$$\n其中 $w$ 是图像的像素宽度，$x,y$ 是像素坐标。设 $S$ 是关键点索引上的一个置换，用于交换左右对应的部分，即 $S:\\{0,\\dots,N-1\\}\\rightarrow\\{0,\\dots,N-1\\}$ 是一个对合，使得对于每个左右配对 $(i,j)$，有 $S(i)=j$ 和 $S(j)=i$，而对于任何自对称索引 $k$，有 $S(k)=k$。对于带有左右交换的水平翻转，其等变性条件为\n$$\n\\hat{K}(F I) = F \\, S \\, \\hat{K}(I),\n$$\n其中 $F$ 作用于坐标，$S$ 作用于索引。\n\n对于同一主体图像及其水平翻转版本的预测对 $\\hat{K}(I)$ 和 $\\hat{K}(F I)$，定义其对称误差（以像素为单位）如下\n$$\nE_{\\text{sym}}(\\hat{K}(I),\\hat{K}(F I); w, S) \\triangleq \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left(\\hat{K}(I)_{S(i)}; w\\right)\\right) \\right\\|_2,\n$$\n其中 $\\|\\cdot\\|_2$ 是以像素为单位的欧几里得范数。考虑一个部分强制步骤，它仅通过将原始图像的预测投影到翻转-交换一致性上来更新该预测：\n$$\nK_{\\text{cons}}(I) \\triangleq \\frac{1}{2}\\left( \\hat{K}(I) + S \\, F^{-1}\\, \\hat{K}(F I) \\right),\n$$\n对于水平翻转，$F^{-1}=F$，同时保持 $\\hat{K}(F I)$ 不变。强制执行后的对称误差为\n$$\nE_{\\text{sym}}^{\\text{after}} \\triangleq E_{\\text{sym}}\\!\\left(K_{\\text{cons}}(I),\\hat{K}(F I); w, S\\right).\n$$\n\n您的任务是编写一个完整的、可运行的程序，为下面的每个测试用例计算：\n- 强制执行前的对称误差 $E_{\\text{sym}}^{\\text{before}} \\triangleq E_{\\text{sym}}\\!\\left(\\hat{K}(I),\\hat{K}(F I); w, S\\right)$，以及\n- 如上定义的强制执行后的对称误差 $E_{\\text{sym}}^{\\text{after}}$，\n\n两者都以像素为单位，表示为浮点值。所有与角度相关的量都不存在；不需要角度单位。最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，其中每个测试用例的值按 $[E_{\\text{sym}}^{\\text{before}},E_{\\text{sym}}^{\\text{after}}]$ 的顺序跨测试用例连接。每个数字必须四舍五入到 $6$ 位小数。\n\n实现以下测试套件。每个案例都明确定义了 $(w,h,N,S,\\hat{K}(I),\\hat{K}(F I))$，其中 $w$ 是像素宽度，$h$ 是像素高度，$N$ 是关键点数量，$S$ 通过其左右配对给出，预测结果以像素坐标数组的形式给出。\n\n- 案例 1（正常路径，中度不一致）：\n  - $w = 100$, $h = 80$, $N=4$，左右配对：$(0,1)$ 和 $(2,3)$。\n  - $\\hat{K}(I) = \\begin{bmatrix} 20  50 \\\\ 80  50 \\\\ 25  30 \\\\ 75  30 \\end{bmatrix}$。\n  - $\\hat{K}(F I) = \\begin{bmatrix} 22  52 \\\\ 77  49 \\\\ 26  31 \\\\ 71  29 \\end{bmatrix}$。\n\n- 案例 2（边界情况，完美对称）：\n  - $w = 100$, $h = 80$, $N=4$，左右配对：$(0,1)$ 和 $(2,3)$。\n  - $\\hat{K}(I) = \\begin{bmatrix} 20  50 \\\\ 80  50 \\\\ 25  30 \\\\ 75  30 \\end{bmatrix}$。\n  - $\\hat{K}(F I)$ 精确等于 $F S \\hat{K}(I)$，即，\n    $$\n    \\hat{K}(F I) = \\begin{bmatrix}\n    19  50 \\\\\n    79  50 \\\\\n    24  30 \\\\\n    74  30\n    \\end{bmatrix}.\n    $$\n\n- 案例 3（边缘情况，索引在对称轴上）：\n  - $w = 101$, $h = 120$, $N=4$，左右配对：$(0,1)$ 和 $(2,3)$。\n  - $\\hat{K}(I) = \\begin{bmatrix} 50  10 \\\\ 50  12 \\\\ 50  70 \\\\ 50  68 \\end{bmatrix}$。\n  - $\\hat{K}(F I) = \\begin{bmatrix} 48  11 \\\\ 52  10 \\\\ 49  71 \\\\ 51  67 \\end{bmatrix}$。\n\n- 案例 4（边缘情况，大的不对称性和错误的交换）：\n  - $w = 64$, $h = 64$, $N=4$，左右配对：$(0,1)$ 和 $(2,3)$。\n  - $\\hat{K}(I) = \\begin{bmatrix} 10  20 \\\\ 53  19 \\\\ 12  40 \\\\ 51  41 \\end{bmatrix}$。\n  - $\\hat{K}(F I) = \\begin{bmatrix} 56  23 \\\\ 7  19 \\\\ 51  39 \\\\ 12  44 \\end{bmatrix}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，排序如下\n$$\n[E_{\\text{sym}}^{\\text{before}}(\\text{案例 }1), E_{\\text{sym}}^{\\text{after}}(\\text{案例 }1), E_{\\text{sym}}^{\\text{before}}(\\text{案例 }2), E_{\\text{sym}}^{\\text{after}}(\\text{案例 }2), E_{\\text{sym}}^{\\text{before}}(\\text{案例 }3), E_{\\text{sym}}^{\\text{after}}(\\text{案例 }3), E_{\\text{sym}}^{\\text{before}}(\\text{案例 }4), E_{\\text{sym}}^{\\text{after}}(\\text{案例 }4)].\n$$\n所有值都必须是以像素为单位的浮点数，四舍五入到 $6$ 位小数。不应打印任何额外文本。", "solution": "该问题已经过验证，被认为是科学上合理的、定义明确且自洽的。所有定义和数据都足以得出一个唯一的解。\n\n任务是为一系列测试用例计算两个量：关键点检测器一致性强制步骤之前的对称误差（$E_{\\text{sym}}^{\\text{before}}$）和之后的对称误差（$E_{\\text{sym}}^{\\text{after}}$）。\n\n基本量的定义如下：\n- 图像 $I$ 的关键点预测是一个矩阵 $\\hat{K}(I) \\in \\mathbb{R}^{N \\times 2}$。\n- 对坐标向量 $(x,y)$ 的水平翻转变换是 $F(x,y;w) = (w-1-x, y)$，其中 $w$ 是图像宽度。此变换是其自身的逆，即 $F^{-1}=F$。\n- 置换 $S$ 交换对应的左右关键点的索引。由于它由交换组成，$S$ 是一个对合，意味着 $S^2$ 是单位置换。\n- 强制执行前的对称误差由下式给出：\n$$\nE_{\\text{sym}}^{\\text{before}} = E_{\\text{sym}}(\\hat{K}(I),\\hat{K}(F I); w, S) \\triangleq \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left(\\hat{K}(I)_{S(i)}; w\\right)\\right) \\right\\|_2\n$$\n这衡量了与理想的翻转-交换等变性条件 $\\hat{K}(F I) = F \\, S \\, \\hat{K}(I)$ 的偏差，其中 $F$ 作用于坐标，$S$ 作用于关键点向量列表。项 $F(\\hat{K}(I)_{S(i)}; w)$ 可以写成矩阵 $F(S\\hat{K}(I); w)$ 的第 $i$ 行。\n\n- 一致性强制步骤更新原始图像上的预测：\n$$\nK_{\\text{cons}}(I) \\triangleq \\frac{1}{2}\\left( \\hat{K}(I) + S \\, F^{-1}\\, \\hat{K}(F I) \\right)\n$$\n鉴于 $F^{-1}=F$，这可以简化为 $K_{\\text{cons}}(I) = \\frac{1}{2}\\left( \\hat{K}(I) + S F \\hat{K}(F I) \\right)$。\n\n- 然后，使用新的预测 $K_{\\text{cons}}(I)$ 计算强制执行后的对称误差，同时保持 $\\hat{K}(F I)$ 不变：\n$$\nE_{\\text{sym}}^{\\text{after}} = E_{\\text{sym}}\\!\\left(K_{\\text{cons}}(I),\\hat{K}(F I); w, S\\right) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left\\| \\hat{K}(F I)_i - \\left(F\\!\\left((K_{\\text{cons}}(I))_{S(i)}; w\\right)\\right) \\right\\|_2\n$$\n\n通过分析 $E_{\\text{sym}}^{\\text{after}}$ 和 $E_{\\text{sym}}^{\\text{before}}$ 之间的关系，可以得到一个关键的见解。让我们检查一下 $E_{\\text{sym}}^{\\text{after}}$ 表达式中范数的参数。从 $\\hat{K}(F I)$ 中减去的项是 $F S K_{\\text{cons}}(I)$。让我们展开这一项：\n$$\nF S K_{\\text{cons}}(I) = F S \\left[ \\frac{1}{2}\\left( \\hat{K}(I) + S F \\hat{K}(F I) \\right) \\right]\n$$\n置换 $S$ 是作用于 $N$ 个关键点向量集合上的线性算子。变换 $F$ 是仿射的，但它对平均运算具有分配律：$F(\\frac{1}{2}(\\vec{u}+\\vec{v})) = \\frac{1}{2}(F(\\vec{u})+F(\\vec{v}))$。应用这些性质：\n$$\nF S K_{\\text{cons}}(I) = F \\left[ \\frac{1}{2}\\left( S\\hat{K}(I) + S S F \\hat{K}(F I) \\right) \\right]\n$$\n因为 $S$ 是一个对合，$S S$ 是单位算子。\n$$\n= F \\left[ \\frac{1}{2}\\left( S\\hat{K}(I) + F \\hat{K}(F I) \\right) \\right] = \\frac{1}{2} \\left[ F(S\\hat{K}(I)) + F(F(\\hat{K}(F I))) \\right]\n$$\n因为 $F$ 是其自身的逆，$F F$ 是单位算子。\n$$\n= \\frac{1}{2} \\left[ F S \\hat{K}(I) + \\hat{K}(F I) \\right]\n$$\n现在，让我们看一下强制后误差计算中第 $i$ 个关键点的差分向量：\n$$\n\\Delta_{i}^{\\text{after}} = \\hat{K}(F I)_i - (F S K_{\\text{cons}}(I))_i = \\hat{K}(F I)_i - \\frac{1}{2} \\left[ (F S \\hat{K}(I))_i + \\hat{K}(F I)_i \\right]\n$$\n$$\n\\Delta_{i}^{\\text{after}} = \\frac{1}{2} \\left[ \\hat{K}(F I)_i - (F S \\hat{K}(I))_i \\right]\n$$\n这个差分向量恰好是原始差分向量 $\\Delta_{i}^{\\text{before}} = \\hat{K}(F I)_i - (F S \\hat{K}(I))_i$ 的一半。\n因此，范数也减半：$\\|\\Delta_{i}^{\\text{after}}\\|_2 = \\frac{1}{2} \\|\\Delta_{i}^{\\text{before}}\\|_2$。当我们将这些范数在所有 $N$ 个关键点上取平均时，这个关系对总误差也成立：\n$$\nE_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}}\n$$\n这个分析结果表明，所选的更新规则将不一致的预测投影到等变流形的一半路程上。这简化了计算，因为我们只需要为每个案例计算 $E_{\\text{sym}}^{\\text{before}}$。然而，为了严谨起见，我们实现了完整的计算。\n\n对于给定的测试用例 $(w, N, S, \\hat{K}(I), \\hat{K}(F I))$，计算过程如下：\n1.  根据给定的左右配对定义 $S$ 的置换映射。对于配对 $(0,1)$ 和 $(2,3)$，映射为 $[1,0,3,2]$。\n2.  计算理想的变换后预测，$K_{\\text{target}} = F(S \\hat{K}(I); w)$。\n3.  计算每个关键点的误差向量 $\\Delta_i = \\hat{K}(F I)_i - (K_{\\text{target}})_i$。\n4.  通过取欧几里得范数 $\\|\\Delta_i\\|_2$ 的平均值来计算 $E_{\\text{sym}}^{\\text{before}}$。\n5.  那么 $E_{\\text{sym}}^{\\text{after}}$ 就是 $\\frac{1}{2}E_{\\text{sym}}^{\\text{before}}$。实现通过显式计算 $K_{\\text{cons}}(I)$ 然后计算相应误差来证实这一点。\n\n将此过程应用于测试用例，得到以下结果：\n\n- **案例 1**：$w=100, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$。\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(\\sqrt{13} + 2\\sqrt{5} + \\sqrt{10}) \\approx 2.809993$ 像素。\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 1.404996$ 像素。\n\n- **案例 2**：$w=100, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$。预测是完美对称的，所以 $K_{target} = \\hat{K}(F I)$。\n  $E_{\\text{sym}}^{\\text{before}} = 0.0$ 像素。\n  $E_{\\text{sym}}^{\\text{after}} = 0.0$ 像素。\n\n- **案例 3**：$w=101, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$。\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(2 + \\sqrt{5} + 2\\sqrt{10}) \\approx 2.640156$ 像素。\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 1.320078$ 像素。\n\n- **案例 4**：$w=64, N=4, S \\leftrightarrow \\{(0,1), (2,3)\\}$。\n  $E_{\\text{sym}}^{\\text{before}} = \\frac{1}{4}(\\sqrt{2132} + \\sqrt{2117} + \\sqrt{1525} + \\sqrt{1537}) \\approx 42.610074$ 像素。\n  $E_{\\text{sym}}^{\\text{after}} = \\frac{1}{2} E_{\\text{sym}}^{\\text{before}} \\approx 21.305037$ 像素。\n\n这些值按指定格式进行格式化和呈现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the symmetric error calculation problem for all test cases.\n    \"\"\"\n    # Each case defines (w, h, N, S, K_I, K_FI)\n    # S is given by left-right pairs, which is a fixed (0,1), (2,3) for all cases.\n    # h is not used in calculations.\n    \n    test_cases = [\n        {\n            \"w\": 100, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[20., 50.], [80., 50.], [25., 30.], [75., 30.]]),\n            \"K_FI\": np.array([[22., 52.], [77., 49.], [26., 31.], [71., 29.]])\n        },\n        {\n            \"w\": 100, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[20., 50.], [80., 50.], [25., 30.], [75., 30.]]),\n            \"K_FI\": np.array([[19., 50.], [79., 50.], [24., 30.], [74., 30.]])\n        },\n        {\n            \"w\": 101, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[50., 10.], [50., 12.], [50., 70.], [50., 68.]]),\n            \"K_FI\": np.array([[48., 11.], [52., 10.], [49., 71.], [51., 67.]])\n        },\n        {\n            \"w\": 64, \"N\": 4, \"s_pairs\": [(0, 1), (2, 3)],\n            \"K_I\": np.array([[10., 20.], [53., 19.], [12., 40.], [51., 41.]]),\n            \"K_FI\": np.array([[56., 23.], [7., 19.], [51., 39.], [12., 44.]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        w = case[\"w\"]\n        N = case[\"N\"]\n        K_I = case[\"K_I\"]\n        K_FI = case[\"K_FI\"]\n\n        # Construct the permutation map from pairs\n        s_map = np.arange(N)\n        for i, j in case[\"s_pairs\"]:\n            s_map[i], s_map[j] = j, i\n\n        # Calculate pre- and post-enforcement errors\n        e_before, e_after = calculate_symmetric_errors(w, N, s_map, K_I, K_FI)\n        \n        results.append(f\"{e_before:.6f}\")\n        results.append(f\"{e_after:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef F_transform(k_matrix, w):\n    \"\"\"\n    Applies the horizontal flip transform F(x,y;w) = (w-1-x, y) to a matrix of keypoints.\n    \n    Args:\n        k_matrix (np.ndarray): An (N, 2) array of (x,y) coordinates.\n        w (int): The image width.\n        \n    Returns:\n        np.ndarray: The transformed (N, 2) array.\n    \"\"\"\n    k_flipped = k_matrix.copy()\n    k_flipped[:, 0] = w - 1 - k_flipped[:, 0]\n    return k_flipped\n\ndef S_permut(k_matrix, s_map):\n    \"\"\"\n    Applies the permutation S to the rows of a keypoint matrix.\n    \n    Args:\n        k_matrix (np.ndarray): An (N, 2) array of keypoints.\n        s_map (np.ndarray): A 1D array representing the permutation of indices.\n        \n    Returns:\n        np.ndarray: The permuted (N, 2) array.\n    \"\"\"\n    return k_matrix[s_map]\n\ndef calculate_symmetric_errors(w, N, s_map, K_I, K_FI):\n    \"\"\"\n    Calculates the pre- and post-enforcement symmetric errors.\n    \n    Args:\n        w (int): Image width.\n        N (int): Number of keypoints.\n        s_map (np.ndarray): Permutation map for S.\n        K_I (np.ndarray): Keypoints from original image.\n        K_FI (np.ndarray): Keypoints from flipped image.\n        \n    Returns:\n        tuple[float, float]: A tuple containing (E_sym_before, E_sym_after).\n    \"\"\"\n    # --- Calculate E_sym_before ---\n    # The ideal prediction for the flipped image, derived from K_I, is F(S(K_I)).\n    target_from_I = F_transform(S_permut(K_I, s_map), w)\n    \n    # Calculate the per-keypoint difference vectors.\n    diff_before = K_FI - target_from_I\n    \n    # Calculate the L2 norm for each difference vector.\n    norms_before = np.linalg.norm(diff_before, axis=1)\n    \n    # The error is the mean of these norms.\n    E_sym_before = np.mean(norms_before)\n    \n    # --- Calculate E_sym_after ---\n    # First, compute the consistency-enforced keypoints K_cons(I).\n    # K_cons(I) = 0.5 * (K_I + S * F^-1 * K_FI), where F^-1 = F.\n    SF_K_FI = S_permut(F_transform(K_FI, w), s_map)\n    K_cons_I = 0.5 * (K_I + SF_K_FI)\n    \n    # The post-enforcement error is E_sym(K_cons_I, K_FI).\n    # The new target is derived from K_cons_I: F(S(K_cons_I)).\n    target_from_cons = F_transform(S_permut(K_cons_I, s_map), w)\n    \n    diff_after = K_FI - target_from_cons\n    norms_after = np.linalg.norm(diff_after, axis=1)\n    E_sym_after = np.mean(norms_after)\n    \n    # As derived in the solution, E_sym_after is analytically half of E_sym_before.\n    # This implementation calculates it explicitly for rigor.\n    \n    return E_sym_before, E_sym_after\n\nsolve()\n```", "id": "3140014"}, {"introduction": "一个模型的性能表现与其所用的训练和评估数据集的特性密切相关。数据集中的偏差，例如某些关节点因遮挡而频繁不可见，可能会扭曲评估指标，从而掩盖模型的真实能力或改进潜力。本练习将引导你从一个简化的分析模型出发，深入探讨如何量化数据集偏差。你将通过计算关节点可见度的信息熵，并分析其与饱和受限的“正确关键点概率”（Probability of Correct Keypoint, PCK）增益之间的关系，来学习评估数据集的内在特性如何影响我们对模型性能的判断。 [@problem_id:3139888]", "problem": "给定一个简化的分析模型，通过检查关节的可见性分布及其对正确关键点概率 (Probability of Correct Keypoint, PCK) 的影响，来评估关键点检测和姿态估计中的数据集偏差。对于每个关节索引 $i$，其可见性被建模为一个参数为 $p_i \\in [0,1]$ 的伯努利随机变量，其中 $p_i$ 是该关节在图像中可见的概率。每个关节的可见性不确定性通过伯努利随机变量的香农熵来量化，该熵由函数 $H(v_i) = -p_i \\ln p_i - (1 - p_i) \\ln (1 - p_i)$ 定义，约定 $0 \\ln 0 = 0$，并使用自然对数，即以 $e$ 为底。\n\n对于固定距离阈值下的评估，设 $a_{v,i}$ 表示关节可见时每个关节的基础 PCK，设 $a_{o,i}$ 表示关节被遮挡时每个关节的基础 PCK。假设一个新模型对这些可见和被遮挡的 PCK 分别产生了增量改进 $d_{v,i}$ 和 $d_{o,i}$，但 PCK 值的上限为 $1$。将每个关节的基础整体 $\\text{PCK}^{\\text{base}}_i$ 和改进后的每个关节 $\\text{PCK}^{\\text{new}}_i$ 定义为\n$$\n\\text{PCK}^{\\text{base}}_i = p_i a_{v,i} + (1 - p_i) a_{o,i},\n\\qquad\n\\text{PCK}^{\\text{new}}_i = p_i \\min\\{1, a_{v,i} + d_{v,i}\\} + (1 - p_i)\\min\\{1, a_{o,i} + d_{o,i}\\}.\n$$\n将每个关节的饱和限制边际增益定义为\n$$\nG_i = \\text{PCK}^{\\text{new}}_i - \\text{PCK}^{\\text{base}}_i.\n$$\n为将可见性不确定性与改进潜力联系起来，需要计算每个测试案例中，关节间的向量 $\\mathbf{H} = [H(v_i)]$ 和 $\\mathbf{G} = [G_i]$ 之间的皮尔逊相关系数。如果 $\\mathbf{H}$ 或 $\\mathbf{G}$ 的标准差为零，则将相关性定义为 $0$ 以避免除以零。\n\n您的任务是实现一个程序，对于以下每个测试案例，计算相关系数，结果为浮点数并四舍五入到 $6$ 位小数。\n\n在所有对数计算中使用自然对数。本问题中没有物理单位或角度，因此无需进行单位转换。\n\n测试套件：\n- 测试案例 1：\n  - $\\mathbf{p} = [\\,0.9,\\, 0.7,\\, 0.5,\\, 0.3,\\, 0.1\\,]$\n  - $\\mathbf{a_v} = [\\,0.88,\\, 0.83,\\, 0.78,\\, 0.73,\\, 0.68\\,]$\n  - $\\mathbf{a_o} = [\\,0.52,\\, 0.47,\\, 0.42,\\, 0.37,\\, 0.32\\,]$\n  - $\\mathbf{d_v} = [\\,0.04,\\, 0.05,\\, 0.06,\\, 0.05,\\, 0.04\\,]$\n  - $\\mathbf{d_o} = [\\,0.18,\\, 0.17,\\, 0.16,\\, 0.17,\\, 0.18\\,]$\n- 测试案例 2（边界可见性概率和硬饱和）：\n  - $\\mathbf{p} = [\\,1.0,\\, 0.0,\\, 1.0,\\, 0.0\\,]$\n  - $\\mathbf{a_v} = [\\,0.92,\\, 0.92,\\, 0.97,\\, 0.97\\,]$\n  - $\\mathbf{a_o} = [\\,0.35,\\, 0.35,\\, 0.40,\\, 0.40\\,]$\n  - $\\mathbf{d_v} = [\\,0.20,\\, 0.20,\\, 0.10,\\, 0.10\\,]$\n  - $\\mathbf{d_o} = [\\,0.80,\\, 0.80,\\, 0.70,\\, 0.70\\,]$\n- 测试案例 3（仅遮挡改进模式）：\n  - $\\mathbf{p} = [\\,0.8,\\, 0.6,\\, 0.4,\\, 0.2\\,]$\n  - $\\mathbf{a_v} = [\\,0.90,\\, 0.87,\\, 0.84,\\, 0.81\\,]$\n  - $\\mathbf{a_o} = [\\,0.50,\\, 0.50,\\, 0.50,\\, 0.50\\,]$\n  - $\\mathbf{d_v} = [\\,0.00,\\, 0.00,\\, 0.00,\\, 0.00\\,]$\n  - $\\mathbf{d_o} = [\\,0.25,\\, 0.25,\\, 0.25,\\, 0.25\\,]$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含三个测试案例的相关系数，形式为逗号分隔的列表，并用方括号括起来，例如 $[r_1,r_2,r_3]$，其中每个 $r_j$ 四舍五入到 $6$ 位小数。", "solution": "该问题要求为三个不同的测试案例计算两个派生向量 $\\mathbf{H}$ 和 $\\mathbf{G}$ 之间的皮尔逊相关系数。向量 $\\mathbf{H}$ 代表关节可见性的香农熵，向量 $\\mathbf{G}$ 代表正确关键点概率 (PCK) 的饱和限制边际增益。\n\n首先，我们陈述问题中给出的必要数学定义。\n对于每个关节 $i$，可见性概率为 $p_i$。\n香农熵由以下公式给出：\n$$\nH_i = H(p_i) = -p_i \\ln p_i - (1 - p_i) \\ln (1 - p_i)\n$$\n约定 $0 \\ln 0 = 0$，这意味着 $H(0) = H(1) = 0$。\n\n基础 PCK 和新 PCK 值为：\n$$\n\\text{PCK}^{\\text{base}}_i = p_i a_{v,i} + (1 - p_i) a_{o,i}\n$$\n$$\n\\text{PCK}^{\\text{new}}_i = p_i \\min\\{1, a_{v,i} + d_{v,i}\\} + (1 - p_i)\\min\\{1, a_{o,i} + d_{o,i}\\}\n$$\n边际增益是这两者之差：\n$$\nG_i = \\text{PCK}^{\\text{new}}_i - \\text{PCK}^{\\text{base}}_i\n$$\n这可以重写为：\n$$\nG_i = p_i (\\min\\{1, a_{v,i} + d_{v,i}\\} - a_{v,i}) + (1-p_i)(\\min\\{1, a_{o,i} + d_{o,i}\\} - a_{o,i})\n$$\n如果没有发生饱和（即，和保持小于或等于 $1$），则简化为 $G_i = p_i d_{v,i} + (1-p_i)d_{o,i}$。\n\n两个长度为 $n$ 的向量 $\\mathbf{X}$ 和 $\\mathbf{Y}$ 之间的皮尔逊相关系数 $\\rho$ 定义为：\n$$\n\\rho_{\\mathbf{X}, \\mathbf{Y}} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\sqrt{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}}\n$$\n其中 $\\bar{X}$ 和 $\\bar{Y}$ 分别是 $\\mathbf{X}$ 和 $\\mathbf{Y}$ 的均值。问题规定，如果任一向量的标准差为零（即分母为零），则相关性定义为 $0$。\n\n我们现在将分析每个测试案例。\n\n### 测试案例 1\n- $\\mathbf{p} = [\\,0.9,\\, 0.7,\\, 0.5,\\, 0.3,\\, 0.1\\,]$\n- $\\mathbf{a_v} = [\\,0.88,\\, 0.83,\\, 0.78,\\, 0.73,\\, 0.68\\,]$\n- $\\mathbf{a_o} = [\\,0.52,\\, 0.47,\\, 0.42,\\, 0.37,\\, 0.32\\,]$\n- $\\mathbf{d_v} = [\\,0.04,\\, 0.05,\\, 0.06,\\, 0.05,\\, 0.04\\,]$\n- $\\mathbf{d_o} = [\\,0.18,\\, 0.17,\\, 0.16,\\, 0.17,\\, 0.18\\,]$\n\n**1. 计算 $\\mathbf{H}$:**\n熵函数 $H(p)$ 关于 $p=0.5$ 对称，即 $H(p) = H(1-p)$。输入向量 $\\mathbf{p}$ 关于 $0.5$ 对称。因此，得到的向量 $\\mathbf{H}$ 将是对称的。\n$H_1 = H(0.9) = -0.9\\ln(0.9) - 0.1\\ln(0.1) \\approx 0.325083$\n$H_2 = H(0.7) = -0.7\\ln(0.7) - 0.3\\ln(0.3) \\approx 0.610864$\n$H_3 = H(0.5) = -\\ln(0.5) \\approx 0.693147$\n$H_4 = H(0.3) = H(0.7) \\approx 0.610864$\n$H_5 = H(0.1) = H(0.9) \\approx 0.325083$\n向量 $\\mathbf{H} = [H_1, H_2, H_3, H_4, H_5]$ 是对称的，即对于 $i=0, \\dots, n-1$，$H_i = H_{n-1-i}$。因此，中心化向量 $\\mathbf{H} - \\bar{\\mathbf{H}}$ 也是对称的。\n\n**2. 计算 $\\mathbf{G}$:**\n对于此案例，对所有 $i$，$a_{v,i}+d_{v,i} \\le 1$ 和 $a_{o,i}+d_{o,i} \\le 1$ 均成立，因此没有发生饱和。增益为 $G_i = p_i d_{v,i} + (1-p_i)d_{o,i}$。\n$G_1 = 0.9(0.04) + 0.1(0.18) = 0.036 + 0.018 = 0.054$\n$G_2 = 0.7(0.05) + 0.3(0.17) = 0.035 + 0.051 = 0.086$\n$G_3 = 0.5(0.06) + 0.5(0.16) = 0.030 + 0.080 = 0.110$\n$G_4 = 0.3(0.05) + 0.7(0.17) = 0.015 + 0.119 = 0.134$\n$G_5 = 0.1(0.04) + 0.9(0.18) = 0.004 + 0.162 = 0.166$\n向量 $\\mathbf{G} = [\\,0.054,\\, 0.086,\\, 0.110,\\, 0.134,\\, 0.166\\,]$。\n均值为 $\\bar{G} = 0.110$。中心化向量为 $\\mathbf{G} - \\bar{\\mathbf{G}} = [\\, -0.056, -0.024, 0.000, 0.024, 0.056 \\,]$。这个向量是反对称的，即 $(\\mathbf{G}-\\bar{\\mathbf{G}})_i = -(\\mathbf{G}-\\bar{\\mathbf{G}})_{n-1-i}$。\n\n**3. 计算相关性：**\n相关性公式的分子是中心化向量的点积，即 $\\sum (H_i - \\bar{H})(G_i - \\bar{G})$。一个对称向量和一个相同奇数长度的反对称向量的点积恒为零。因此，协方差为零。\n因此，相关系数为 $0$。\n\n### 测试案例 2\n- $\\mathbf{p} = [\\,1.0,\\, 0.0,\\, 1.0,\\, 0.0\\,]$\n- $\\mathbf{a_v} = [\\,0.92,\\, 0.92,\\, 0.97,\\, 0.97\\,]$\n- $\\mathbf{a_o} = [\\,0.35,\\, 0.35,\\, 0.40,\\, 0.40\\,]$\n- $\\mathbf{d_v} = [\\,0.20,\\, 0.20,\\, 0.10,\\, 0.10\\,]$\n- $\\mathbf{d_o} = [\\,0.80,\\, 0.80,\\, 0.70,\\, 0.70\\,]$\n\n**1. 计算 $\\mathbf{H}$:**\n使用 $H(p)$ 的定义，以及特殊情况 $0 \\ln 0 = 0$：\n$H_1 = H(1.0) = 0$\n$H_2 = H(0.0) = 0$\n$H_3 = H(1.0) = 0$\n$H_4 = H(0.0) = 0$\n向量 $\\mathbf{H} = [\\,0,\\, 0,\\, 0,\\, 0\\,]$。\n\n**2. 计算相关性：**\n由于 $\\mathbf{H}$ 的所有元素都相同，其标准差为 $0$。\n根据问题陈述，如果任一向量的标准差为零，则相关性定义为 $0$。\n因此，相关系数为 $0$。\n\n### 测试案例 3\n- $\\mathbf{p} = [\\,0.8,\\, 0.6,\\, 0.4,\\, 0.2\\,]$\n- $\\mathbf{a_v} = [\\,0.90,\\, 0.87,\\, 0.84,\\, 0.81\\,]$\n- $\\mathbf{a_o} = [\\,0.50,\\, 0.50,\\, 0.50,\\, 0.50\\,]$\n- $\\mathbf{d_v} = [\\,0.00,\\, 0.00,\\, 0.00,\\, 0.00\\,]$\n- $\\mathbf{d_o} = [\\,0.25,\\, 0.25,\\, 0.25,\\, 0.25\\,]$\n\n**1. 计算 $\\mathbf{H}$:**\n与案例 1 类似，我们使用对称性 $H(p) = H(1-p)$。输入向量 $\\mathbf{p}$ 的元素关于序列的中点对称。\n$H_1 = H(0.8) \\approx 0.50040$\n$H_2 = H(0.6) \\approx 0.67301$\n$H_3 = H(0.4) = H(0.6) \\approx 0.67301$\n$H_4 = H(0.2) = H(0.8) \\approx 0.50040$\n向量 $\\mathbf{H}$ 是对称的：$H_i = H_{n-1-i}$。中心化向量 $\\mathbf{H} - \\bar{\\mathbf{H}}$ 也是对称的。\n\n**2. 计算 $\\mathbf{G}$:**\n这里，对所有 $i$，$d_{v,i}=0$。增益公式简化为 $G_i = (1-p_i)(\\min\\{1, a_{o,i}+d_{o,i}\\} - a_{o,i})$。\n对于所有关节，$a_{o,i}+d_{o,i} = 0.50+0.25 = 0.75 \\le 1$，因此没有发生饱和。\n因此，$G_i = (1-p_i)d_{o,i} = (1-p_i) \\times 0.25$。\n$G_1 = (1-0.8) \\times 0.25 = 0.05$\n$G_2 = (1-0.6) \\times 0.25 = 0.10$\n$G_3 = (1-0.4) \\times 0.25 = 0.15$\n$G_4 = (1-0.2) \\times 0.25 = 0.20$\n向量 $\\mathbf{G} = [\\,0.05,\\, 0.10,\\, 0.15,\\, 0.20\\,]$ 是一个算术级数。\n一个中心化的算术级数是反对称的。均值为 $\\bar{G} = 0.125$。\n中心化向量为 $\\mathbf{G} - \\bar{\\mathbf{G}} = [\\, -0.075, -0.025, 0.025, 0.075 \\,]$。这个向量是反对称的：$(\\mathbf{G}-\\bar{\\mathbf{G}})_i = -(\\mathbf{G}-\\bar{\\mathbf{G}})_{n-1-i}$。\n\n**3. 计算相关性：**\n相关性的分子是对称向量 $\\mathbf{H}-\\bar{\\mathbf{H}}$ 和反对称向量 $\\mathbf{G}-\\bar{\\mathbf{G}}$ 的点积。对于一个偶数长度的序列，这个点积恒为零。\n因此，相关系数为 $0$。\n\n**结果总结：**\n- 测试案例 1: $0.0$\n- 测试案例 2: $0.0$\n- 测试案例 3: $0.0$\n\n由于特殊条件（标准差为零）或输入数据中导致协方差为零的内在对称性，所有三个测试案例的相关性都恰好为 $0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases as specified.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"p\": np.array([0.9, 0.7, 0.5, 0.3, 0.1]),\n            \"a_v\": np.array([0.88, 0.83, 0.78, 0.73, 0.68]),\n            \"a_o\": np.array([0.52, 0.47, 0.42, 0.37, 0.32]),\n            \"d_v\": np.array([0.04, 0.05, 0.06, 0.05, 0.04]),\n            \"d_o\": np.array([0.18, 0.17, 0.16, 0.17, 0.18]),\n        },\n        # Test Case 2\n        {\n            \"p\": np.array([1.0, 0.0, 1.0, 0.0]),\n            \"a_v\": np.array([0.92, 0.92, 0.97, 0.97]),\n            \"a_o\": np.array([0.35, 0.35, 0.40, 0.40]),\n            \"d_v\": np.array([0.20, 0.20, 0.10, 0.10]),\n            \"d_o\": np.array([0.80, 0.80, 0.70, 0.70]),\n        },\n        # Test Case 3\n        {\n            \"p\": np.array([0.8, 0.6, 0.4, 0.2]),\n            \"a_v\": np.array([0.90, 0.87, 0.84, 0.81]),\n            \"a_o\": np.array([0.50, 0.50, 0.50, 0.50]),\n            \"d_v\": np.array([0.00, 0.00, 0.00, 0.00]),\n            \"d_o\": np.array([0.25, 0.25, 0.25, 0.25]),\n        }\n    ]\n\n    def calculate_correlation(p, a_v, a_o, d_v, d_o):\n        \"\"\"\n        Calculates the Pearson correlation coefficient between H and G.\n        \"\"\"\n        # 1. Calculate the entropy vector H\n        # The problem states 0*ln(0) = 0, so H(0)=H(1)=0.\n        # We handle this by creating a mask to avoid log(0).\n        H = np.zeros_like(p, dtype=float)\n        mask = (p > 0.0) & (p < 1.0)\n        p_safe = p[mask]\n        H[mask] = -p_safe * np.log(p_safe) - (1-p_safe) * np.log(1-p_safe)\n\n        # 2. Calculate the marginal gain vector G\n        pck_base = p * a_v + (1 - p) * a_o\n        pck_new_v = p * np.minimum(1.0, a_v + d_v)\n        pck_new_o = (1 - p) * np.minimum(1.0, a_o + d_o)\n        pck_new = pck_new_v + pck_new_o\n        G = pck_new - pck_base\n\n        # 3. Calculate the Pearson correlation coefficient\n        # Per problem spec, if std dev of either vector is 0, correlation is 0.\n        if np.std(H) == 0.0 or np.std(G) == 0.0:\n            return 0.0\n        \n        # np.corrcoef returns a matrix, we need the off-diagonal element.\n        # It handles the case of zero variance by returning nan, but the problem\n        # asks for 0, which we've already handled.\n        correlation_matrix = np.corrcoef(H, G)\n        correlation = correlation_matrix[0, 1]\n        \n        return correlation\n\n    results = []\n    for case in test_cases:\n        corr = calculate_correlation(case[\"p\"], case[\"a_v\"], case[\"a_o\"], case[\"d_v\"], case[\"d_o\"])\n        results.append(corr)\n\n    # Format the final output as specified.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3139888"}]}