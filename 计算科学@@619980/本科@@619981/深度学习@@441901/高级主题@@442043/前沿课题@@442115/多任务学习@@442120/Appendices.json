{"hands_on_practices": [{"introduction": "在多任务学习中，一个核心挑战是“负迁移”（negative transfer），即为一个任务进行优化反而损害了另一个任务的性能。这种情况通常发生在任务梯度冲突时，也就是梯度指向相反方向。这个练习 [@problem_id:3155105] 让你能够实现一种名为“梯度手术”（gradient surgery）的技术，它通过将一个任务的梯度投影到与另一个任务梯度正交的方向，从而直接解决这个问题，消除直接冲突，并可能加速收敛。", "problem": "考虑一个深度学习中的双任务多任务学习问题，其中两个任务共享一个参数向量 $w \\in \\mathbb{R}^d$。每个任务 $i \\in \\{1,2\\}$ 都有一个在数据集 $(X_i, y_i)$ 上定义的均方误差（MSE）损失 $L_i(w)$，其中包含 $n_i$ 个样本，$X_i \\in \\mathbb{R}^{n_i \\times d}$ 且 $y_i \\in \\mathbb{R}^{n_i}$。MSE 损失由下式给出\n$$\nL_i(w) = \\frac{1}{2 n_i} \\left\\| X_i w - y_i \\right\\|_2^2.\n$$\n梯度下降使用聚合梯度和固定的学习率 $\\eta > 0$ 来更新参数向量。在普通聚合机制中，算法使用每个任务梯度的总和；而在正交化机制中，算法将一个任务的梯度替换为其正交于另一个任务梯度的分量，然后进行聚合。\n\n你的任务是实现以下两种训练方案，并比较它们的收敛速度：\n\n- 普通聚合梯度下降：在每次迭代 $t$ 中，计算每个任务的梯度 $\\nabla L_1(w_t)$ 和 $\\nabla L_2(w_t)$，并更新\n$$\nw_{t+1} = w_t - \\eta \\left( \\nabla L_1(w_t) + \\nabla L_2(w_t) \\right).\n$$\n\n- 基于正交化的梯度手术：在每次迭代 $t$ 中，计算 $\\nabla L_1(w_t)$ 和 $\\nabla L_2(w_t)$，将任务 1 的梯度替换为其正交于任务 2 梯度的分量，然后使用任务 1 修改后的梯度和任务 2 未修改的梯度之和进行更新。如果 $\\left\\|\\nabla L_2(w_t)\\right\\|_2 = 0$，则在聚合前保持 $\\nabla L_1(w_t)$ 不变。\n\n对于这两种方案，都从相同的初始参数向量 $w_0$ 开始，并运行迭代，直到首次满足停止准则\n$$\nL_1(w_t) + L_2(w_t) \\le \\varepsilon\n$$\n或达到最大迭代次数 $T_{\\max}$。记录满足停止准则所用的迭代次数（如果在该限制内未满足，则记录 $T_{\\max}$）。\n\n实现程序以评估以下五个测试用例，每个用例由 $(X_1, y_1)$ 和 $(X_2, y_2)$ 指定，共享维度 $d = 2$，初始参数 $w_0$，学习率 $\\eta$，容差 $\\varepsilon$ 和最大迭代次数 $T_{\\max}$：\n\n- 测试用例 1（中等冲突）：\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 2  1 \\\\ 1  2 \\\\ 1  -1 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}$。\n\n- 测试用例 2（在 $w_0$ 处对齐的每任务梯度）：\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$。\n\n- 测试用例 3（在 $w_0$ 处正交的每任务梯度）：\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 0  1 \\\\ 0  1 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n\n- 测试用例 4（在 $w_0$ 处反平行的每任务梯度）：\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} -1 \\\\ -2 \\\\ -3 \\end{bmatrix}$。\n\n- 测试用例 5（在 $w_0$ 处任务 2 的梯度为零）：\n  - $d = 2$,\n  - $X_1 = \\begin{bmatrix} 1  1 \\\\ 1  -1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n  - $X_2 = \\begin{bmatrix} 1  2 \\\\ 3  4 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n所有测试的通用超参数：\n- $w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n- $\\eta = 0.05$,\n- $\\varepsilon = 10^{-6}$,\n- $T_{\\max} = 10000$。\n\n对于每个测试用例 $k \\in \\{1,2,3,4,5\\}$，你的程序必须：\n- 运行普通聚合梯度下降，并记录达到 $L_1(w_t) + L_2(w_t) \\le \\varepsilon$ 所需的迭代次数 $N^{(k)}_{\\mathrm{vanilla}}$，如果未达到则记录 $T_{\\max}$。\n- 运行基于正交化的梯度手术（将任务 1 的梯度相对于任务 2 的梯度进行正交化），并记录类似定义的 $N^{(k)}_{\\mathrm{ortho}}$。\n- 计算整数差 $\\Delta^{(k)} = N^{(k)}_{\\mathrm{vanilla}} - N^{(k)}_{\\mathrm{ortho}}$。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例索引 $k = 1,2,3,4,5$ 排序，即打印\n$$\n\\left[ \\Delta^{(1)}, \\Delta^{(2)}, \\Delta^{(3)}, \\Delta^{(4)}, \\Delta^{(5)} \\right].\n$$", "solution": "该问题在科学上和数学上是良定的，为进行求解提供了所有必要的信息。它基于应用于多任务学习的数值优化和线性代数的既定原则。\n\n目标是比较两种梯度下降算法在双任务线性回归问题上的收敛速度。这种比较通过达到总损失特定容差所需的迭代次数来量化。\n\n首先，我们形式化两种算法所需的数学组件。每个任务 $i \\in \\{1, 2\\}$ 的损失函数是均方误差（MSE）：\n$$\nL_i(w) = \\frac{1}{2 n_i} \\left\\| X_i w - y_i \\right\\|_2^2\n$$\n其中 $w \\in \\mathbb{R}^d$ 是共享参数向量，$X_i \\in \\mathbb{R}^{n_i \\times d}$ 是数据矩阵，$y_i \\in \\mathbb{R}^{n_i}$ 是任务 $i$ 的目标向量。\n\n任何梯度下降方法的核心是损失函数的梯度。我们推导 $L_i(w)$ 关于 $w$ 的梯度。将平方范数重写为点积，$L_i(w) = \\frac{1}{2 n_i} (X_i w - y_i)^T (X_i w - y_i)$。展开此式可得：\n$$\nL_i(w) = \\frac{1}{2 n_i} (w^T X_i^T X_i w - 2 y_i^T X_i w + y_i^T y_i)\n$$\n使用标准矩阵微积分恒等式对 $w$ 求梯度，我们得到：\n$$\n\\nabla_w L_i(w) = \\frac{1}{2 n_i} (2 X_i^T X_i w - 2 X_i^T y_i) = \\frac{1}{n_i} X_i^T (X_i w - y_i)\n$$\n我们将在迭代 $t$ 时的每个任务的梯度表示为 $g_1(w_t) = \\nabla_w L_1(w_t)$ 和 $g_2(w_t) = \\nabla_w L_2(w_t)$。\n\n定义了梯度之后，我们指定两种算法方案。\n\n**1. 普通聚合梯度下降**\n这是多任务学习中的标准方法，将所有任务的梯度相加形成更新方向。更新规则是：\n$$\nw_{t+1} = w_t - \\eta \\left( g_1(w_t) + g_2(w_t) \\right)\n$$\n这种方法很简单，但如果任务梯度相互冲突（即指向相反方向），可能会因为聚合梯度的大小可能很小而导致收敛缓慢。\n\n**2. 基于正交化的梯度手术**\n该方法旨在缓解梯度冲突问题。一个任务（任务 1）的梯度通过移除其与另一个任务（任务 2）梯度平行的分量来修改。这确保了任务 1 的更新不会干扰任务 2 的梯度指示方向。\n\n$g_1(w_t)$ 中平行于 $g_2(w_t)$ 的分量通过将 $g_1(w_t)$ 投影到 $g_2(w_t)$ 上得到：\n$$\n\\text{proj}_{g_2(w_t)}(g_1(w_t)) = \\frac{g_1(w_t) \\cdot g_2(w_t)}{\\|g_2(w_t)\\|_2^2} g_2(w_t)\n$$\n任务 1 修改后的梯度，记为 $g_1^{\\perp}(w_t)$，是 $g_1(w_t)$ 正交于 $g_2(w_t)$ 的分量：\n$$\ng_1^{\\perp}(w_t) = g_1(w_t) - \\text{proj}_{g_2(w_t)}(g_1(w_t)) = g_1(w_t) - \\frac{g_1(w_t) \\cdot g_2(w_t)}{\\|g_2(w_t)\\|_2^2} g_2(w_t)\n$$\n如题所述，如果 $\\|g_2(w_t)\\|_2 = 0$，则投影未定义。在这种情况下，我们不修改任务 1 的梯度，设置 $g_1^{\\perp}(w_t) = g_1(w_t)$。更新规则变为：\n$$\nw_{t+1} = w_t - \\eta \\left( g_1^{\\perp}(w_t) + g_2(w_t) \\right)\n$$\n\n**计算过程**\n对于五个测试用例中的每一个以及两种算法中的每一种，执行迭代优化。\n1. 初始化参数 $w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n2. 对于从 $0$ 到 $T_{\\max} - 1$ 的每次迭代 $t$：\n   a. 计算总损失 $L_{total}(w_t) = L_1(w_t) + L_2(w_t)$。\n   b. 检查停止准则：如果 $L_{total}(w_t) \\le \\varepsilon = 10^{-6}$，循环终止，并将所用迭代次数记录为 $t$。\n   c. 计算每个任务的梯度 $g_1(w_t)$ 和 $g_2(w_t)$。\n   d. 根据特定算法（普通或正交化）的规则计算参数更新 $w_{t+1}$。\n3. 如果循环完成而未满足停止准则，则将迭代次数记录为 $T_{\\max} = 10000$。\n\n令 $N^{(k)}_{\\mathrm{vanilla}}$ 和 $N^{(k)}_{\\mathrm{ortho}}$ 分别为普通算法和正交化算法在测试用例 $k \\in \\{1, 2, 3, 4, 5\\}$ 中记录的迭代次数。每个用例的最终输出是整数差 $\\Delta^{(k)} = N^{(k)}_{\\mathrm{vanilla}} - N^{(k)}_{\\mathrm{ortho}}$。该值直接比较了它们的收敛速度。", "answer": "```python\nimport numpy as np\n\ndef loss_fn(X, y, w):\n    \"\"\"Computes the Mean Squared Error loss.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return 0.0\n    error = X @ w - y\n    return (1.0 / (2.0 * n)) * np.dot(error, error)\n\ndef grad_fn(X, y, w):\n    \"\"\"Computes the gradient of the MSE loss.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return np.zeros_like(w)\n    error = X @ w - y\n    return (1.0 / n) * X.T @ error\n\ndef solve():\n    \"\"\"\n    Implements and compares vanilla and orthogonalized gradient descent for\n    several multi-task learning test cases.\n    \"\"\"\n    \n    # Common hyperparameters\n    w0 = np.array([0.0, 0.0])\n    eta = 0.05\n    epsilon = 1e-6\n    T_max = 10000\n\n    # Test cases definition\n    test_cases = [\n        # Test Case 1 (moderate conflict)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[2.0, 1.0], [1.0, 2.0], [1.0, -1.0]]),\n            \"y2\": np.array([0.0, 1.0, -1.0]),\n        },\n        # Test Case 2 (aligned gradients)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y2\": np.array([1.0, 2.0, 3.0]),\n        },\n        # Test Case 3 (orthogonal gradients at w0)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"y1\": np.array([1.0, 0.0]),\n            \"X2\": np.array([[0.0, 1.0], [0.0, 1.0]]),\n            \"y2\": np.array([1.0, 1.0]),\n        },\n        # Test Case 4 (anti-parallel gradients at w0)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y2\": np.array([-1.0, -2.0, -3.0]),\n        },\n        # Test Case 5 (zero gradient for task 2 at w0)\n        {\n            \"X1\": np.array([[1.0, 1.0], [1.0, -1.0]]),\n            \"y1\": np.array([1.0, 0.0]),\n            \"X2\": np.array([[1.0, 2.0], [3.0, 4.0]]),\n            \"y2\": np.array([0.0, 0.0]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X1, y1 = case[\"X1\"], case[\"y1\"]\n        X2, y2 = case[\"X2\"], case[\"y2\"]\n\n        # Run Vanilla Aggregated Gradient Descent\n        w = np.copy(w0)\n        n_vanilla = T_max\n        for t in range(T_max):\n            loss1 = loss_fn(X1, y1, w)\n            loss2 = loss_fn(X2, y2, w)\n            if loss1 + loss2 = epsilon:\n                n_vanilla = t\n                break\n\n            g1 = grad_fn(X1, y1, w)\n            g2 = grad_fn(X2, y2, w)\n            w -= eta * (g1 + g2)\n\n        # Run Orthogonalization-based Gradient Surgery\n        w = np.copy(w0)\n        n_ortho = T_max\n        for t in range(T_max):\n            loss1 = loss_fn(X1, y1, w)\n            loss2 = loss_fn(X2, y2, w)\n            if loss1 + loss2 = epsilon:\n                n_ortho = t\n                break\n\n            g1 = grad_fn(X1, y1, w)\n            g2 = grad_fn(X2, y2, w)\n            \n            g2_norm_sq = np.dot(g2, g2)\n            \n            # Use a small tolerance for the zero-norm check for floating point stability\n            if g2_norm_sq > 1e-12:\n                g1_ortho = g1 - (np.dot(g1, g2) / g2_norm_sq) * g2\n            else:\n                g1_ortho = g1\n\n            w -= eta * (g1_ortho + g2)\n            \n        delta = n_vanilla - n_ortho\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3155105"}, {"introduction": "虽然梯度手术提供了一种解决冲突的直接方法，但一个更根本的问题是：在多任务环境中，什么才算是“最优”解？这个实践 [@problem_id:3155072] 引入了帕累托最优（Pareto optimality）的概念，这是多目标优化的基石，它定义了一组解的集合，在其中任何单个任务的性能都无法在不损害其他任务的情况下得到改善。然后，你将实现多梯度下降算法（Multiple Gradient Descent Algorithm, MGDA），该算法通过找到任务梯度的最小范数凸组合来确定一个共享的下降方向，为在多任务之间进行权衡提供了一种有原则的方法。", "problem": "给定一个可微分的多任务学习问题，其中有 $T$ 个任务，每个任务 $t \\in \\{1,\\dots,T\\}$ 都关联一个标量损失 $L_t(\\mathbf{w})$，其中 $\\mathbf{w} \\in \\mathbb{R}^d$ 是一个共享参数向量。每个任务在 $\\mathbf{w}$ 处的梯度表示为 $\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w}) \\in \\mathbb{R}^d$。如果不存在一个点 $\\mathbf{w}$ 使得对于所有 $t$ 都有 $L_t(\\mathbf{w}) \\le L_t(\\mathbf{w}^\\star)$ 并且至少对于一个 $t'$ 有 $L_{t'}(\\mathbf{w})  L_{t'}(\\mathbf{w}^\\star)$，那么点 $\\mathbf{w}^\\star$ 就被称为帕累托最优。基础优化理论指出，对于无约束、可微分的优化问题，任何标量目标函数 $s(\\mathbf{w})$ 的局部极小值点都必须满足一阶必要条件 $\\nabla s(\\mathbf{w}) = \\mathbf{0}$。使用此结论以及多目标优化中的其他公认事实来推导帕累托平稳性。\n\n任务 A (理论)。从上述定义出发，仅使用凸锥分离定理和无约束可微分优化的一阶必要条件等基本原则，推导出以下帕累托平稳性论述：如果 $\\mathbf{w}^\\star$ 是帕累托最优的，则存在系数 $\\alpha_t \\ge 0$ 且 $\\sum_{t=1}^T \\alpha_t = 1$，使得\n$$\n\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}.\n$$\n解释为什么该条件可以由关于 $\\{L_t(\\mathbf{w})\\}_{t=1}^T$ 像集的支持超平面/分离论证以及应用于适当标量化的一阶必要条件推导得出。\n\n任务 B (算法)。在给定点 $\\mathbf{w}$ 处，多梯度下降算法 (Multiple Gradient Descent Algorithm, MGDA) 通过求解以下二次规划问题来选择系数 $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_T)$：\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^T} \\; \\frac{1}{2} \\left\\| \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}) \\right\\|_2^2 \\quad \\text{约束条件为} \\quad \\alpha_t \\ge 0 \\; \\text{对所有 } t, \\; \\sum_{t=1}^T \\alpha_t = 1.\n$$\n这将产生任务梯度的最小范数凸组合。请实现一个程序，对于下面的每个测试用例，使用闭式解和小规模情况推理，为 $T \\in \\{2,3\\}$ 精确求解此问题以计算 $\\boldsymbol{\\alpha}$，过程中要避免数值不稳定或依赖外部数据。\n\n小模型和梯度。考虑一个共享线性模型，其参数为 $\\mathbf{w} \\in \\mathbb{R}^2$，以及任务特定的平方损失\n$$\nL_t(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{A}_t \\mathbf{w} - \\mathbf{b}_t \\|_2^2,\n$$\n其中 $\\mathbf{A}_t \\in \\mathbb{R}^{2 \\times 2}$ 且 $\\mathbf{b}_t \\in \\mathbb{R}^2$。梯度为\n$$\n\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w}) = \\mathbf{A}_t^\\top (\\mathbf{A}_t \\mathbf{w} - \\mathbf{b}_t).\n$$\n在下面的所有测试用例中，对每个任务都使用 $\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 和 $\\mathbf{A}_t = \\mathbf{I}_2$，因此 $\\mathbf{g}_t(\\mathbf{w}) = -\\mathbf{b}_t$。\n\n测试套件。对于每个测试用例，您将获得一个 $\\mathbf{b}_t$ 向量列表（每个任务一个），并且必须计算 MGDA 系数 $\\boldsymbol{\\alpha}$。\n\n- 案例 1（两个任务，存在内部解）：$T=2$，$\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{b}_2 = \\begin{bmatrix} -\\frac{1}{2} \\\\ 0 \\end{bmatrix}$。\n\n- 案例 2（两个任务，共线同向，边界解）：$T=2$，$\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{b}_2 = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$。\n\n- 案例 3（三个任务，所有系数均为正的严格内部解）：$T=3$，$\\mathbf{b}_1 = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{b}_2 = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}$，$\\mathbf{b}_3 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n\n对程序的要求。\n\n- 您的程序必须通过为 $T \\in \\{2,3\\}$ 精确求解 MGDA 二次规划来计算每个案例的 $\\boldsymbol{\\alpha}$，其中 $T=2$ 使用闭式公式，$T=3$ 使用小规模情况枚举（如果可行，则为内部重心坐标解；否则沿边缘和顶点进行最小化）。\n\n- 最终的数值答案必须四舍五入到 $6$ 位小数。\n\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素是对应测试用例的系数列表。例如，输出可能看起来像 $[[a_{1},a_{2}],[b_{1},b_{2}],[c_{1},c_{2},c_{3}]]$, 不含空格。请将 $a_i,b_i,c_i$ 替换为您计算的并四舍五入到 $6$ 位小数的浮点数。\n\n- 唯一可接受的输出是浮点数列表；列表中不能有布尔值或字符串。\n\n您的程序必须是自包含的，不需要任何输入。它不能从文件中读取或写入文件。它必须实现所描述的 MGDA 求解逻辑，并计算上述三个测试用例的系数。", "solution": "根据要求，问题分析分为两个部分：帕累托平稳性条件的理论推导（任务 A）和求解多梯度下降算法 (MGDA) 系数的算法实现（任务 B）。\n\n### 任务 A：推导帕累托平稳性条件\n\n此任务要求推导帕累托最优性的一阶必要条件。该条件也称为帕累托平稳性。推导依赖于帕累托最优性的定义和凸分析的基本结果，特别是分离定理。\n\n设 $T$ 个可微分的任务特定损失函数集合为 $\\{L_t(\\mathbf{w})\\}_{t=1}^T$，其中 $\\mathbf{w} \\in \\mathbb{R}^d$ 是共享参数向量。每个损失的梯度为 $\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w})$。\n\n1.  **下降方向的条件**：\n    如果沿向量 $\\mathbf{d} \\in \\mathbb{R}^d$ 方向前进一小步可以减小任务 $t$ 在点 $\\mathbf{w}$ 处的损失 $L_t$，则 $\\mathbf{d}$ 是一个下降方向。基于一阶泰勒展开，此条件为 $\\mathbf{g}_t(\\mathbf{w})^\\top \\mathbf{d}  0$。\n\n2.  **帕累托最优性与下降方向**：\n    根据定义，如果不存在任何其他点 $\\mathbf{w}$ 在至少一个任务上严格更优，而在任何其他任务上都不差，则点 $\\mathbf{w}^\\star$ 是帕累托最优的。就局部改进而言，这意味着从 $\\mathbf{w}^\\star$ 出发，不存在一个下降方向 $\\mathbf{d}$，它能在不恶化任何其他任务损失的情况下，至少改进一个任务的损失。\n    形式上，如果 $\\mathbf{w}^\\star$ 是一个帕累托最优点，则不存在向量 $\\mathbf{d} \\in \\mathbb{R}^d$ 使得：\n    $$\n    \\mathbf{g}_t(\\mathbf{w}^\\star)^\\top \\mathbf{d} \\le 0 \\quad \\text{对于所有 } t \\in \\{1, \\dots, T\\}\n    $$\n    并且\n    $$\n    \\mathbf{g}_{t'}(\\mathbf{w}^\\star)^\\top \\mathbf{d}  0 \\quad \\text{对于至少一个 } t' \\in \\{1, \\dots, T\\}。\n    $$\n    如果存在这样的方向 $\\mathbf{d}$，那么从 $\\mathbf{w}^\\star$ 沿该方向前进一小步 $\\epsilon\\mathbf{d}$ (其中 $\\epsilon  0$)，就可以得到一个新点 $\\mathbf{w}' = \\mathbf{w}^\\star + \\epsilon\\mathbf{d}$，该点将帕累托支配 $\\mathbf{w}^\\star$，这与其最优性相矛盾。一个更简单的结论是，不存在 $\\mathbf{d}$ 使得对所有 $t$ 都有 $\\mathbf{g}_t(\\mathbf{w}^\\star)^\\top \\mathbf{d}  0$。\n\n3.  **择一法定理的应用 (Gordan's Lemma)**：\n    这种共同下降方向的不存在性可以通过择一法定理来正式确立。Gordan's Lemma 是凸分析中的一个结果，它指出对于任意向量集合 $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_T\\}$，以下两个陈述中必有一个且仅有一个为真：\n    (i) 存在一个向量 $\\mathbf{d}$ 使得对所有 $t=1, \\dots, T$ 都有 $\\mathbf{v}_t^\\top \\mathbf{d} > 0$。\n    (ii) 零向量 $\\mathbf{0}$ 是这些向量的非平凡非负组合，即存在不全为零的系数 $\\lambda_t \\ge 0$，使得 $\\sum_{t=1}^T \\lambda_t \\mathbf{v}_t = \\mathbf{0}$。\n    等价地，(ii) 指出 $\\mathbf{0}$ 位于 $\\{\\mathbf{v}_t\\}$ 的凸锥内。\n    该定理的一个更通用的版本涵盖了步骤 2 中的混合不等式（$\\le$ 和 $$），并得出相同的结论。\n\n4.  **推导帕累托平稳性条件**：\n    将此定理应用于我们的梯度集合 $\\{\\mathbf{g}_t(\\mathbf{w}^\\star)\\}$，已确立的共同下降方向不存在（排除了陈述 (i)），这意味着陈述 (ii) 必须为真。因此，必须存在系数 $\\lambda_t \\ge 0$ (对于 $t=1, \\dots, T$)，其中至少有一个 $\\lambda_t > 0$，使得：\n    $$\n    \\sum_{t=1}^T \\lambda_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}.\n    $$\n    由于系数 $\\lambda_t$ 不全为零，它们的和 $S = \\sum_{j=1}^T \\lambda_j$ 严格为正。我们可以通过定义 $\\alpha_t = \\lambda_t / S$ 来对这些系数进行归一化。这些新系数满足：\n    - 对所有 $t$，$\\alpha_t \\ge 0$。\n    - $\\sum_{t=1}^T \\alpha_t = \\frac{1}{S} \\sum_{t=1}^T \\lambda_t = \\frac{S}{S} = 1$。\n    - $\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\frac{1}{S} \\sum_{t=1}^T \\lambda_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\frac{1}{S} \\mathbf{0} = \\mathbf{0}$。\n    这就完成了帕累托平稳性条件的推导：\n    $$\n    \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}, \\quad \\text{其中 } \\alpha_t \\ge 0, \\sum_{t=1}^T \\alpha_t = 1.\n    $$\n\n5.  **与标量化和支持超平面的联系**：\n    条件 $\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}$ 正是点 $\\mathbf{w}^\\star$ 成为标量化目标函数 $S(\\mathbf{w}) = \\sum_{t=1}^T \\alpha_t L_t(\\mathbf{w})$ 的局部极小值点的一阶必要条件。这个标量化损失的梯度是 $\\nabla S(\\mathbf{w}) = \\sum_{t=1}^T \\alpha_t \\nabla L_t(\\mathbf{w})$。\n    在几何上，在由 $(L_1, \\dots, L_T)$ 构成的损失空间中，向量 $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_T)$ 定义了一个超平面。帕累托平稳性条件意味着，在点 $(L_1(\\mathbf{w}^\\star), \\dots, L_T(\\mathbf{w}^\\star))$ 处，这个超平面是可达损失向量集合的一个支持超平面。\n\n### 任务 B：MGDA 算法实现\n\n目标是求解关于 $\\boldsymbol{\\alpha}$ 的二次规划 (QP) 问题：\n$$\n\\min_{\\boldsymbol{\\alpha}} \\; \\frac{1}{2} \\left\\| \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t \\right\\|_2^2 \\quad \\text{s.t.} \\quad \\alpha_t \\ge 0 \\; \\forall t, \\; \\sum_{t=1}^T \\alpha_t = 1.\n$$\n梯度由 $\\mathbf{g}_t = -\\mathbf{b}_t$ 给出。\n\n#### 案例 $T=2$\n问题是最小化 $f(\\alpha_1, \\alpha_2) = \\frac{1}{2}\\|\\alpha_1 \\mathbf{g}_1 + \\alpha_2 \\mathbf{g}_2\\|_2^2$，约束条件为 $\\alpha_1 + \\alpha_2 = 1$ 和 $\\alpha_1, \\alpha_2 \\ge 0$。代入 $\\alpha_2 = 1-\\alpha_1$ 将问题简化为最小化一个关于 $\\alpha_1 \\in [0, 1]$ 的二次函数。对 $\\alpha_1$ 求导并令其为零，得到无约束极小值点：\n$$\n\\alpha_1^* = \\frac{\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1)}{\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2}.\n$$\n约束问题的解可以通过将 $\\alpha_1^*$ 投影到区间 $[0, 1]$ 上得到。因此，$\\alpha_1 = \\text{max}(0, \\text{min}(1, \\alpha_1^*))$，且 $\\alpha_2 = 1 - \\alpha_1$。\n\n- **案例 1:** $\\mathbf{g}_1 = [-1, 0]^\\top$, $\\mathbf{g}_2 = [0.5, 0]^\\top$.\n  $\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2 = \\|[-1.5, 0]^\\top\\|_2^2 = 2.25$.\n  $\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1) = [0.5, 0] \\cdot [1.5, 0]^\\top = 0.75$.\n  $\\alpha_1^* = 0.75 / 2.25 = 1/3$。由于 $0 \\le 1/3 \\le 1$，我们有 $\\alpha_1 = 1/3$ 和 $\\alpha_2 = 2/3$。\n\n- **案例 2:** $\\mathbf{g}_1 = [-1, 0]^\\top$, $\\mathbf{g}_2 = [-2, 0]^\\top$.\n  $\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2 = \\|[1, 0]^\\top\\|_2^2 = 1$.\n  $\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1) = [-2, 0] \\cdot [-1, 0]^\\top = 2$.\n  $\\alpha_1^* = 2 / 1 = 2$。由于 $2 > 1$，我们投影到边界上：$\\alpha_1 = 1$ 和 $\\alpha_2 = 0$。\n\n#### 案例 $T=3$\n对于 $T=3$ 且梯度在 $\\mathbb{R}^2$ 中，梯度的最小范数凸组合为零，当且仅当原点位于梯度的凸包内（即它们形成的三角形内）。这可以通过求解原点的重心坐标 $(\\alpha_1, \\alpha_2, \\alpha_3)$ 来检验。如果所有 $\\alpha_t \\ge 0$，这就是解。这需要求解方程组：\n$$\n\\begin{pmatrix} g_{1x}  g_{2x}  g_{3x} \\\\ g_{1y}  g_{2y}  g_{3y} \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n如果这失败了（例如，原点在三角形外），那么解就位于单纯形的某条边上，这简化为求解三个独立的 $T=2$ 问题并选择最优的一个。\n\n- **案例 3:** $\\mathbf{g}_1 = [1, 0]^\\top$, $\\mathbf{g}_2 = [0, 1]^\\top$, $\\mathbf{g}_3 = [-1, -1]^\\top$.\n我们观察到 $\\mathbf{g}_1 + \\mathbf{g}_2 + \\mathbf{g}_3 = \\mathbf{0}$。一个产生 $\\mathbf{0}$ 的凸组合是 $\\frac{1}{3}\\mathbf{g}_1 + \\frac{1}{3}\\mathbf{g}_2 + \\frac{1}{3}\\mathbf{g}_3 = \\mathbf{0}$。由于目标函数是范数的平方，其最小值为 $0$，这个组合可以达到。系数为 $\\alpha_1 = 1/3$, $\\alpha_2 = 1/3$, $\\alpha_3 = 1/3$。所有系数都非负且和为 $1$，所以这是唯一的有效解。解上述线性方程组可以证实这一点：\n$$\n\\begin{pmatrix} 1  0  -1 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\implies \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}.\n$$\n\n结果是：\n- 案例 1: $(\\alpha_1, \\alpha_2) = (1/3, 2/3)$\n- 案例 2: $(\\alpha_1, \\alpha_2) = (1, 0)$\n- 案例 3: $(\\alpha_1, \\alpha_2, \\alpha_3) = (1/3, 1/3, 1/3)$\n下面将实现这些计算以生成最终格式化的输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_t2(g1: np.ndarray, g2: np.ndarray) -> tuple[float, float]:\n    \"\"\"\n    Solves the MGDA QP for T=2 tasks exactly.\n    \"\"\"\n    g1_minus_g2 = g1 - g2\n    g1_minus_g2_sq_norm = np.dot(g1_minus_g2, g1_minus_g2)\n\n    # Handle the degenerate case where g1 == g2\n    if np.isclose(g1_minus_g2_sq_norm, 0):\n        return 0.5, 0.5\n\n    # Compute unconstrained minimizer for alpha_1\n    alpha1_star = np.dot(g2, g2 - g1) / g1_minus_g2_sq_norm\n\n    # Project onto the interval [0, 1]\n    alpha1 = max(0.0, min(1.0, alpha1_star))\n    alpha2 = 1.0 - alpha1\n    \n    return alpha1, alpha2\n\ndef solve_t3(g1: np.ndarray, g2: np.ndarray, g3: np.ndarray) -> tuple[float, float, float]:\n    \"\"\"\n    Solves the MGDA QP for T=3 tasks exactly, as per problem specification.\n    \"\"\"\n    # First, check for an interior solution where the combination of gradients is zero.\n    # This happens if the origin is in the convex hull of the gradients.\n    # We solve for barycentric coordinates of the origin.\n    G_ext = np.array([\n        [g1[0], g2[0], g3[0]],\n        [g1[1], g2[1], g3[1]],\n        [1.0,   1.0,   1.0]\n    ])\n    \n    b_target = np.array([0.0, 0.0, 1.0])\n\n    # Check if a unique solution exists by checking if the matrix is invertible.\n    if abs(np.linalg.det(G_ext)) > 1e-9:\n        try:\n            alphas_int = np.linalg.solve(G_ext, b_target)\n            # If all coefficients are non-negative, this is the solution.\n            if np.all(alphas_int >= -1e-9):\n                return tuple(alphas_int)\n        except np.linalg.LinAlgError:\n            # Should not happen if det is non-zero, but for robustness.\n            pass\n\n    # If no interior solution exists, solve for solutions on the edges.\n    # Edge 1-2\n    a12 = solve_t2(g1, g2)\n    alpha_A = np.array([a12[0], a12[1], 0.0])\n    grad_A = alpha_A[0] * g1 + alpha_A[1] * g2\n    obj_A = 0.5 * np.dot(grad_A, grad_A)\n    \n    # Edge 1-3\n    a13 = solve_t2(g1, g3)\n    alpha_B = np.array([a13[0], 0.0, a13[1]])\n    grad_B = alpha_B[0] * g1 + alpha_B[2] * g3\n    obj_B = 0.5 * np.dot(grad_B, grad_B)\n\n    # Edge 2-3\n    a23 = solve_t2(g2, g3)\n    alpha_C = np.array([0.0, a23[0], a23[1]])\n    grad_C = alpha_C[1] * g2 + alpha_C[2] * g3\n    obj_C = 0.5 * np.dot(grad_C, grad_C)\n    \n    # Find the edge solution with the minimum objective value.\n    objectives = [obj_A, obj_B, obj_C]\n    candidates = [alpha_A, alpha_B, alpha_C]\n    best_alpha = candidates[np.argmin(objectives)]\n    \n    return tuple(best_alpha)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Gradients are g_t = -b_t.\n    test_cases = [\n        # Case 1 (T=2)\n        {\"b_vectors\": [np.array([1.0, 0.0]), np.array([-0.5, 0.0])]},\n        # Case 2 (T=2)\n        {\"b_vectors\": [np.array([1.0, 0.0]), np.array([2.0, 0.0])]},\n        # Case 3 (T=3)\n        {\"b_vectors\": [np.array([-1.0, 0.0]), np.array([0.0, -1.0]), np.array([1.0, 1.0])]},\n    ]\n\n    results = []\n    \n    # Case 1\n    case1 = test_cases[0]\n    g_vectors1 = [-b for b in case1[\"b_vectors\"]]\n    result1 = solve_t2(g_vectors1[0], g_vectors1[1])\n    results.append(list(result1))\n    \n    # Case 2\n    case2 = test_cases[1]\n    g_vectors2 = [-b for b in case2[\"b_vectors\"]]\n    result2 = solve_t2(g_vectors2[0], g_vectors2[1])\n    results.append(list(result2))\n\n    # Case 3\n    case3 = test_cases[2]\n    g_vectors3 = [-b for b in case3[\"b_vectors\"]]\n    result3 = solve_t3(g_vectors3[0], g_vectors3[1], g_vectors3[2])\n    results.append(list(result3))\n\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res_list in results:\n        # Format each float to 6 decimal places, then join into a string like \"[f1,f2,...]\"\n        formatted_list = \"[\" + \",\".join([f\"{x:.6f}\" for x in res_list]) + \"]\"\n        formatted_results.append(formatted_list)\n    \n    # Join all case results into the final output string.\n    final_output = \"[\" + \",\".join(formatted_results) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "3155072"}, {"introduction": "多任务学习中的权衡不仅仅局限于主任务损失的梯度。在这个练习 [@problem_id:3155049] 中，你将模拟一个真实场景，其中引入了一个辅助目标——针对一个任务的域对齐，并研究它如何通过修改共享编码器对第二个不相关的任务产生无意的损害。这个实践突显了负迁移的微妙本质，以及在向多任务目标函数添加新组件时监控所有任务的重要性。", "problem": "考虑一个在多任务学习（MTL）中的双任务监督学习设置，该设置具有一个共享的线性编码器和特定于任务的线性头。对于两个任务，都观察到同一个输入向量 $x \\in \\mathbb{R}^d$。任务 $\\mathrm{A}$ 的输入分布在训练和测试之间经历了域偏移，而任务 $\\mathrm{B}$ 是平稳的（其训练和测试输入分布相同）。我们研究添加一个针对任务 $\\mathrm{A}$ 的域对齐正则化器是否会通过对共享编码器的过度正则化而损害任务 $\\mathrm{B}$。\n\n使用的基本定义：\n- 监督学习的经验风险最小化（ERM）旨在最小化训练数据上的经验损失。\n- 带有平方误差的线性模型是一个经过充分检验的凸设置。对于输入 $x \\in \\mathbb{R}^d$，线性模型预测 $\\hat{y} = \\theta^\\top x$ 并最小化均方误差。\n- 对于特征 $Z \\in \\mathbb{R}^{n \\times d}$ 和目标 $y \\in \\mathbb{R}^n$ 的岭回归（带有 $\\ell_2$ 正则化的线性最小二乘法）旨在最小化 $\\|Z \\theta - y\\|_2^2 + \\alpha \\|\\theta\\|_2^2$，其闭式解为 $\\theta^\\star = (Z^\\top Z + \\alpha I)^{-1} Z^\\top y$。\n- 在 MTL 中，共享表示可以由一个线性编码器 $W$ 参数化，它将 $x$ 映射到 $z = W x$，而特定于任务的头将 $z$ 映射到任务输出。\n\n模型规格：\n- 设共享编码器为一个对角线性映射 $W = \\operatorname{diag}(s)$，其中 $s \\in \\mathbb{R}^d_{\\ge 0}$。共享表示为 $z = s \\odot x$，其中 $\\odot$ 表示逐元素乘法。\n- 任务 $\\mathrm{A}$ 和任务 $\\mathrm{B}$ 的头是向量 $a \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}^d$，给出的预测为 $\\hat{y}_{\\mathrm{A}} = a^\\top (s \\odot x)$ 和 $\\hat{y}_{\\mathrm{B}} = b^\\top (s \\odot x)$。\n- 两个任务的训练数据均从一个共同的源域 $\\mathcal{D}_{\\mathrm{S}}$ 中抽取，其中 $x \\sim \\mathcal{N}(\\mu_{\\mathrm{S}}, \\Sigma)$；任务 $\\mathrm{A}$ 有来自 $\\mathcal{D}_{\\mathrm{T}}$ 的额外无标签目标域输入，其中 $x \\sim \\mathcal{N}(\\mu_{\\mathrm{T}}, \\Sigma)$，且 $\\mu_{\\mathrm{T}} \\ne \\mu_{\\mathrm{S}}$ 仅在一个坐标上有所不同。\n- 标签遵循带加性噪声的线性真实值模型：$y_{\\mathrm{A}} = \\beta_{\\mathrm{A}}^\\top x + \\epsilon_{\\mathrm{A}}$ 和 $y_{\\mathrm{B}} = \\beta_{\\mathrm{B}}^\\top x + \\epsilon_{\\mathrm{B}}$，其中 $\\epsilon_{\\mathrm{A}}$ 和 $\\epsilon_{\\mathrm{B}}$ 是独立的零均值噪声项。\n\n训练目标：\n- 总目标函数是任务 $\\mathrm{A}$ 和任务 $\\mathrm{B}$ 在源域训练数据上的经验平方损失之和，加上对 $a$ 和 $b$ 的岭惩罚项，再加上对 $s$ 的 $\\ell_2$ 惩罚项，以及一个域对齐项，该项惩罚任务 $\\mathrm{A}$ 的源域和目标域之间编码后均值的平方差：\n$$\n\\mathcal{L}(s,a,b) = \\underbrace{\\|Z_{\\mathrm{S}} a - y_{\\mathrm{A}}\\|_2^2}_{\\text{任务 A 损失}} + \\underbrace{\\|Z_{\\mathrm{S}} b - y_{\\mathrm{B}}\\|_2^2}_{\\text{任务 B 损失}} + \\alpha_{\\mathrm{head}} (\\|a\\|_2^2 + \\|b\\|_2^2) + \\alpha_s \\|s\\|_2^2 + \\lambda \\|\\mu_{Z}^{\\mathrm{S}} - \\mu_{Z}^{\\mathrm{T}}\\|_2^2,\n$$\n其中 $Z_{\\mathrm{S}} = X_{\\mathrm{S}} \\odot s$ 是源域编码后的设计矩阵，$\\mu_{Z}^{\\mathrm{S}} = s \\odot \\mu_{X}^{\\mathrm{S}}$ 且 $\\mu_{Z}^{\\mathrm{T}} = s \\odot \\mu_{X}^{\\mathrm{T}}$，其中 $\\mu_{X}^{\\mathrm{S}}$ 和 $\\mu_{X}^{\\mathrm{T}}$ 分别是源域和目标域的输入均值，$\\lambda \\ge 0$ 是对齐权重。对齐项可简化为\n$$\n\\|\\mu_{Z}^{\\mathrm{S}} - \\mu_{Z}^{\\mathrm{T}}\\|_2^2 = \\sum_{j=1}^d \\left(s_j (\\mu_{X,j}^{\\mathrm{S}} - \\mu_{X,j}^{\\mathrm{T}})\\right)^2 = \\sum_{j=1}^d s_j^2 \\Delta\\mu_j^2,\n$$\n其中 $\\Delta\\mu = \\mu_{X}^{\\mathrm{S}} - \\mu_{X}^{\\mathrm{T}}$。\n\n优化策略：\n- 对于固定的 $s$，将 $a$ 和 $b$ 设置为它们在 $Z_{\\mathrm{S}}$ 上的岭回归最优解：\n$$\na^\\star(s) = (Z_{\\mathrm{S}}^\\top Z_{\\mathrm{S}} + \\alpha_{\\mathrm{head}} I)^{-1} Z_{\\mathrm{S}}^\\top y_{\\mathrm{A}}, \\quad b^\\star(s) = (Z_{\\mathrm{S}}^\\top Z_{\\mathrm{S}} + \\alpha_{\\mathrm{head}} I)^{-1} Z_{\\mathrm{S}}^\\top y_{\\mathrm{B}}.\n$$\n- 使用包络定理，在保持 $a = a^\\star(s)$ 和 $b = b^\\star(s)$ 固定的情况下，对 $\\mathcal{L}$ 求偏导数，并通过梯度下降更新 $s$：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_j} = 2 \\sum_{i=1}^n \\left((\\hat{y}_{\\mathrm{A},i} - y_{\\mathrm{A},i}) a_j x_{ij} + (\\hat{y}_{\\mathrm{B},i} - y_{\\mathrm{B},i}) b_j x_{ij}\\right) + 2 \\lambda s_j \\Delta \\mu_j^2 + 2 \\alpha_s s_j.\n$$\n- 对 $s$ 的非负性约束可以通过在每次更新后进行逐元素裁剪 $s_j \\ge 0$ 来强制执行。\n\n评估目标：\n- 对任务 $\\mathrm{B}$ 的损害定义为，在一个为任务 $\\mathrm{B}$ 保留的源域测试集上，当使用 $\\lambda > 0$ 进行训练时，相对于 $\\lambda = 0$ 的情况，任务 $\\mathrm{B}$ 的测试均方误差 (MSE) 出现增加。形式上，如果 $\\mathrm{MSE}_{\\mathrm{B}}^{\\lambda} > \\mathrm{MSE}_{\\mathrm{B}}^{0}$ 的差值超过一个小的容差，则认为存在损害。\n\n您必须实现一个完整、可运行的程序，该程序能够：\n1. 根据具有指定参数的高斯模型合成数据。\n2. 训练带有岭回归头并通过对 $s$ 进行梯度下降的共享对角编码器 MTL 模型。\n3. 计算在有和没有域对齐两种情况下的任务 $\\mathrm{B}$ 测试 MSE。\n4. 对于每个测试用例，根据上述定义输出一个布尔值，指示对齐训练是否损害了任务 $\\mathrm{B}$。\n\n使用以下固定的生成规范：\n- 输入维度 $d = 5$，协方差 $\\Sigma = I_d$。\n- 真实系数 $\\beta_{\\mathrm{A}} = [0.2, 1.0, -0.5, 0.3, 0.0]$ 和 $\\beta_{\\mathrm{B}} = [3.0, 0.5, 0.0, 0.0, 0.0]$。\n- 源域均值 $\\mu_{\\mathrm{S}} = [0,0,0,0,0]^\\top$，目标域均值 $\\mu_{\\mathrm{T}}$ 仅在测试用例指定的单个索引处与 $\\mu_{\\mathrm{S}}$ 不同。\n- 噪声项 $\\epsilon_{\\mathrm{A}}$ 和 $\\epsilon_{\\mathrm{B}}$ 相互独立，且服从 $\\mathcal{N}(0,\\sigma^2)$ 分布，其中 $\\sigma = 0.1$。\n\n对所有测试用例使用以下训练超参数：\n- 编码器正则化 $\\alpha_s = 10^{-3}$。\n- 头正则化 $\\alpha_{\\mathrm{head}} = 1.0$。\n- 梯度下降学习率 $\\eta = 0.05$。\n- 梯度迭代次数 $T = 500$。\n- 将 $s$ 初始化为全1向量。\n\n测试套件：\n提供以下五种参数设置的结果。每个测试用例指定目标域偏移幅度、对齐权重、有标签源样本的数量、用于对齐的无标签目标样本的数量以及偏移维度的索引。所有数值均为实值标量；索引为整数：\n- 案例 1：$(\\text{shift\\_mag} = 2.0, \\ \\lambda = 1.0, \\ n_{\\mathrm{train}} = 400, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 0)$。\n- 案例 2：$(\\text{shift\\_mag} = 3.0, \\ \\lambda = 10.0, \\ n_{\\mathrm{train}} = 400, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 0)$。\n- 案例 3：$(\\text{shift\\_mag} = 0.0, \\ \\lambda = 10.0, \\ n_{\\mathrm{train}} = 400, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 0)$。\n- 案例 4：$(\\text{shift\\_mag} = 2.0, \\ \\lambda = 2.0, \\ n_{\\mathrm{train}} = 50, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 0)$。\n- 案例 5：$(\\text{shift\\_mag} = 3.0, \\ \\lambda = 10.0, \\ n_{\\mathrm{train}} = 400, \\ n_{\\mathrm{target}} = 400, \\ \\text{shift\\_dim} = 4)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，\"[result1,result2,result3,result4,result5]\"），其中每个结果是对应测试用例的布尔值，指示带对齐的任务 $\\mathrm{B}$ 测试 MSE 是否比基线高出超过 $10^{-3}$ 的容差。", "solution": "问题要求我们探究在一个多任务学习（MTL）框架中，一个为辅助处于域偏移下的单个任务（任务 $\\mathrm{A}$）而应用的域对齐正则化器，是否会无意中损害第二个平稳的任务（任务 $\\mathrm{B}$）。该分析在一个简化但具有说明性的线性设置中进行。\n\n问题陈述经过了严格验证，被认为是具有科学依据、定义明确且客观的。它提供了一个清晰、可形式化的模型和一套具体的计算实验。唯一一个次要的模糊之处是用于评估的测试集大小，问题没有明确提供。为了对均方误差（MSE）进行稳健和稳定的估计，我们将使用一个较大的测试集大小 $n_{\\mathrm{test}} = 10000$。这一选择不会改变问题的核心逻辑，并且是此类模拟中的标准做法。\n\n解决方案通过实现一个遵循指定模型、训练过程和评估协议的模拟来进行。\n\n**1. 数据生成**\n对于每个测试用例，我们合成必要的数据集。数据生成遵循指定的概率模型。\n- 输入向量 $x$ 的维度为 $d=5$。\n- 源域输入 $X_{\\mathrm{S}}$ 从多元正态分布 $\\mathcal{N}(\\mu_{\\mathrm{S}}, \\Sigma)$ 中抽取，其中均值为 $\\mu_{\\mathrm{S}} = \\mathbf{0} \\in \\mathbb{R}^5$，协方差为单位矩阵 $\\Sigma = I_5$。这意味着特征是独立且服从标准正态分布的。从该分布中抽取 $n_{\\mathrm{train}}$ 个有标签样本。\n- 用于任务 $\\mathrm{A}$ 对齐正则化器的目标域输入 $X_{\\mathrm{T}}$ 从 $\\mathcal{N}(\\mu_{\\mathrm{T}}, \\Sigma)$ 中抽取。均值 $\\mu_{\\mathrm{T}}$ 是一个零向量，但在索引 `shift_dim` 处有一个值为 `shift_mag` 的项。抽取 $n_{\\mathrm{target}}$ 个无标签样本。\n- 同样从源分布 $\\mathcal{N}(\\mu_{\\mathrm{S}}, \\Sigma)$ 中抽取一个大小为 $n_{\\mathrm{test}} = 10000$ 的留出测试集，用于评估最终模型。\n- 两个任务的标签根据线性模型 $y_{\\mathrm{A}} = \\beta_{\\mathrm{A}}^\\top x + \\epsilon_{\\mathrm{A}}$ 和 $y_{\\mathrm{B}} = \\beta_{\\mathrm{B}}^\\top x + \\epsilon_{\\mathrm{B}}$ 生成，其中真实值向量为 $\\beta_{\\mathrm{A}} = [0.2, 1.0, -0.5, 0.3, 0.0]$ 和 $\\beta_{\\mathrm{B}} = [3.0, 0.5, 0.0, 0.0, 0.0]$。噪声项 $\\epsilon_{\\mathrm{A}}, \\epsilon_{\\mathrm{B}}$ 从 $\\mathcal{N}(0, \\sigma^2)$ 中独立抽取，其中 $\\sigma=0.1$。\n- 为确保在有对齐和无对齐模型之间进行公平比较，每个测试用例中的两次运行都使用同一组生成的数据（训练、目标和测试数据）。\n\n**2. 模型与训练过程**\n该模型包含一个共享的对角线性编码器 $W = \\operatorname{diag}(s)$ 和两个特定于任务的线性头 $a$ 和 $b$。一个输入 $x$ 被编码为 $z = s \\odot x$，预测结果为 $\\hat{y}_{\\mathrm{A}} = a^\\top z$ 和 $\\hat{y}_{\\mathrm{B}} = b^\\top z$。\n\n需要最小化的训练目标是：\n$$\n\\mathcal{L}(s,a,b) = \\|(X_{\\mathrm{S}} \\odot s) a - y_{\\mathrm{A}}\\|_2^2 + \\|(X_{\\mathrm{S}} \\odot s) b - y_{\\mathrm{B}}\\|_2^2 + \\alpha_{\\mathrm{head}} (\\|a\\|_2^2 + \\|b\\|_2^2) + \\alpha_s \\|s\\|_2^2 + \\lambda \\|s \\odot (\\mu_{X}^{\\mathrm{S}} - \\mu_{X}^{\\mathrm{T}})\\|_2^2\n$$\n其中 $X_{\\mathrm{S}} \\odot s$ 表示向量 $s$ 与矩阵 $X_{\\mathrm{S}}$ 的各行进行逐元素乘法，$\\mu_{X}^{\\mathrm{S}}, \\mu_{X}^{\\mathrm{T}}$ 分别是生成的源数据集和目标数据集的经验均值。\n\n我们采用一种交替优化方案：\n- **对于固定的编码器缩放向量 $s$**：对头 $a$ 和 $b$ 的优化解耦为两个标准的岭回归问题。最优的头 $a^\\star(s)$ 和 $b^\\star(s)$ 通过闭式解求得：\n$$\na^\\star(s) = (Z_{\\mathrm{S}}^\\top Z_{\\mathrm{S}} + \\alpha_{\\mathrm{head}} I)^{-1} Z_{\\mathrm{S}}^\\top y_{\\mathrm{A}}\n$$\n$$\nb^\\star(s) = (Z_{\\mathrm{S}}^\\top Z_{\\mathrm{S}} + \\alpha_{\\mathrm{head}} I)^{-1} Z_{\\mathrm{S}}^\\top y_{\\mathrm{B}}\n$$\n其中 $Z_{\\mathrm{S}} = X_{\\mathrm{S}} \\odot s$。这些线性系统为保证稳定性而进行数值求解。\n\n- **对于固定的头 $a$ 和 $b$**：编码器缩放向量 $s$ 通过梯度下降进行更新。根据问题陈述并使用包络定理，目标函数关于 $s_j$ 的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_j} = 2 \\sum_{i=1}^{n_{\\mathrm{train}}} \\left( (\\hat{y}_{\\mathrm{A},i} - y_{\\mathrm{A},i}) a_j x_{ij} + (\\hat{y}_{\\mathrm{B},i} - y_{\\mathrm{B},i}) b_j x_{ij} \\right) + 2 \\alpha_s s_j + 2 \\lambda s_j (\\mu_{X,j}^{\\mathrm{S}} - \\mu_{X,j}^{\\mathrm{T}})^2\n$$\n更新规则为 $s \\leftarrow s - \\eta \\nabla_s \\mathcal{L}$。每步之后，通过逐元素裁剪来强制执行非负性约束 $s_j \\ge 0$。\n\n此两步过程重复进行 $T=500$ 次迭代，起始时 $s$ 初始化为全1向量。\n\n**3. 损害评估**\n对于每个测试用例，我们执行两次完整的训练运行：\n1.  **基线模型**：关闭对齐正则化器（即 $\\lambda = 0$）来训练 MTL 模型。这会得到参数 $(s^0, a^0, b^0)$。\n2.  **对齐模型**：使用指定的对齐权重 $\\lambda > 0$ 来训练 MTL 模型。这会得到参数 $(s^\\lambda, a^\\lambda, b^\\lambda)$。\n\n训练后，我们对两个模型在留出测试集上计算任务 $\\mathrm{B}$ 的 MSE：\n- $\\mathrm{MSE}_{\\mathrm{B}}^{0} = \\frac{1}{n_{\\mathrm{test}}} \\|(X_{\\mathrm{test}} \\odot s^0) b^0 - y_{\\mathrm{B,test}}\\|_2^2$\n- $\\mathrm{MSE}_{\\mathrm{B}}^{\\lambda} = \\frac{1}{n_{\\mathrm{test}}} \\|(X_{\\mathrm{test}} \\odot s^\\lambda) b^\\lambda - y_{\\mathrm{B,test}}\\|_2^2$\n\n如果对齐模型的 MSE 比基线 MSE 大出超过指定的 $10^{-3}$ 容差，则判定对任务 $\\mathrm{B}$ 存在损害。即，如果 $\\mathrm{MSE}_{\\mathrm{B}}^{\\lambda} > \\mathrm{MSE}_{\\mathrm{B}}^{0} + 10^{-3}$。每个案例的结果是一个指示是否存在此类损害的布尔值。\n\n整个模拟针对指定的五个测试用例运行，收集布尔结果并按要求格式化。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of the entire experiment.\n    np.random.seed(0)\n\n    # --- Fixed Generative Specifications ---\n    D = 5\n    BETA_A = np.array([0.2, 1.0, -0.5, 0.3, 0.0])\n    BETA_B = np.array([3.0, 0.5, 0.0, 0.0, 0.0])\n    MU_S_TRUE = np.zeros(D)\n    SIGMA = np.identity(D)\n    NOISE_STD = 0.1\n    N_TEST = 10000  # A large test set for stable MSE evaluation.\n\n    # --- Fixed Training Hyperparameters ---\n    ALPHA_S = 1e-3\n    ALPHA_HEAD = 1.0\n    LR = 0.05\n    N_ITER = 500\n\n    # --- Test Suite ---\n    test_cases = [\n        {'shift_mag': 2.0, 'lambda_align': 1.0, 'n_train': 400, 'n_target': 400, 'shift_dim': 0},\n        {'shift_mag': 3.0, 'lambda_align': 10.0, 'n_train': 400, 'n_target': 400, 'shift_dim': 0},\n        {'shift_mag': 0.0, 'lambda_align': 10.0, 'n_train': 400, 'n_target': 400, 'shift_dim': 0},\n        {'shift_mag': 2.0, 'lambda_align': 2.0, 'n_train': 50, 'n_target': 400, 'shift_dim': 0},\n        {'shift_mag': 3.0, 'lambda_align': 10.0, 'n_train': 400, 'n_target': 400, 'shift_dim': 4},\n    ]\n\n    results = []\n    for case in test_cases:\n        harm = train_and_eval_case(\n            case, D, BETA_A, BETA_B, MU_S_TRUE, SIGMA, NOISE_STD, N_TEST,\n            ALPHA_S, ALPHA_HEAD, LR, N_ITER\n        )\n        results.append(harm)\n\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\n\ndef generate_data(n, d, mu, sigma, beta_a, beta_b, noise_std):\n    \"\"\"Generates synthetic data for the MTL tasks.\"\"\"\n    X = np.random.multivariate_normal(mu, sigma, n)\n    epsilon_a = np.random.normal(0, noise_std, n)\n    epsilon_b = np.random.normal(0, noise_std, n)\n    y_a = X @ beta_a + epsilon_a\n    y_b = X @ beta_b + epsilon_b\n    return X, y_a, y_b\n\n\ndef train_mtl(X_s, y_a, y_b, X_t, lambda_align, d, alpha_s, alpha_head, lr, n_iter):\n    \"\"\"Trains the MTL model and returns the learned parameters.\"\"\"\n    n_train = X_s.shape[0]\n    s = np.ones(d)  # Initialize s\n\n    # Empirical means for the alignment penalty\n    mu_x_s = np.mean(X_s, axis=0)\n    mu_x_t = np.mean(X_t, axis=0)\n    delta_mu_sq = (mu_x_s - mu_x_t)**2\n    \n    # Pre-computation for ridge regression\n    I_d = np.identity(d)\n\n    for _ in range(n_iter):\n        # 1. Compute encoded features\n        Z_s = X_s * s\n\n        # 2. Solve for optimal heads a and b (Ridge Regression)\n        # For a\n        A_a = Z_s.T @ Z_s + alpha_head * I_d\n        b_a = Z_s.T @ y_a\n        a = np.linalg.solve(A_a, b_a)\n        \n        # For b\n        A_b = Z_s.T @ Z_s + alpha_head * I_d\n        b_b = Z_s.T @ y_b\n        b = np.linalg.solve(A_b, b_b)\n\n        # 3. Compute gradient w.r.t s using envelope theorem\n        y_a_hat = Z_s @ a\n        y_b_hat = Z_s @ b\n        res_a = y_a_hat - y_a\n        res_b = y_b_hat - y_b\n\n        grad_res_a = np.sum((res_a[:, np.newaxis] * a) * X_s, axis=0)\n        grad_res_b = np.sum((res_b[:, np.newaxis] * b) * X_s, axis=0)\n        \n        grad_s = 2 * (grad_res_a + grad_res_b) + 2 * alpha_s * s + 2 * lambda_align * s * delta_mu_sq\n\n        # 4. Update s with gradient descent and projection\n        s = s - lr * grad_s\n        s[s  0] = 0.0\n\n    return s, a, b\n\n\ndef train_and_eval_case(case_params, d, beta_a, beta_b, mu_s_true, sigma, noise_std, n_test,\n                        alpha_s, alpha_head, lr, n_iter):\n    \"\"\"\n    Runs the full experiment for a single test case, comparing models with and without alignment.\n    \"\"\"\n    n_train = case_params['n_train']\n    n_target = case_params['n_target']\n    shift_dim = case_params['shift_dim']\n    shift_mag = case_params['shift_mag']\n    lambda_align = case_params['lambda_align']\n    \n    # --- Generate common datasets for fair comparison ---\n    X_s_train, y_a_train, y_b_train = generate_data(n_train, d, mu_s_true, sigma, beta_a, beta_b, noise_std)\n\n    mu_t_true = np.copy(mu_s_true)\n    mu_t_true[shift_dim] = shift_mag\n    X_t_unlabeled, _, _ = generate_data(n_target, d, mu_t_true, sigma, beta_a, beta_b, noise_std)\n\n    X_test, _, y_b_test = generate_data(n_test, d, mu_s_true, sigma, beta_a, beta_b, noise_std)\n    \n    # --- Train and evaluate baseline model (lambda = 0) ---\n    s0, a0, b0 = train_mtl(X_s_train, y_a_train, y_b_train, X_t_unlabeled, 0.0, \n                           d, alpha_s, alpha_head, lr, n_iter)\n    \n    Z_test_0 = X_test * s0\n    y_b_pred_0 = Z_test_0 @ b0\n    mse_b_0 = np.mean((y_b_pred_0 - y_b_test)**2)\n\n    # --- Train and evaluate aligned model (lambda > 0) ---\n    s_lambda, a_lambda, b_lambda = train_mtl(X_s_train, y_a_train, y_b_train, X_t_unlabeled, lambda_align,\n                                             d, alpha_s, alpha_head, lr, n_iter)\n\n    Z_test_lambda = X_test * s_lambda\n    y_b_pred_lambda = Z_test_lambda @ b_lambda\n    mse_b_lambda = np.mean((y_b_pred_lambda - y_b_test)**2)\n\n    # --- Determine if harm occurred ---\n    harm_tolerance = 1e-3\n    is_harm = mse_b_lambda > mse_b_0 + harm_tolerance\n    \n    return is_harm\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3155049"}]}