{"hands_on_practices": [{"introduction": "在小样本学习中，基于原型（prototype-based）的分类器是一种常见且有效的方法，它通常通过计算查询样本与各类别原型之间的距离来进行分类。为了将这些距离转换为概率，我们常常使用带有温度参数 $\\tau$ 的Softmax函数。这个练习将揭示这种常用启发式方法与贝叶斯决策理论之间的深刻联系。通过推导，您将发现，在特定的高斯数据假设下，这种分类器形式正是贝叶斯最优分类器，而温度参数 $\\tau$ 则与特征嵌入的潜在不确定性直接相关。这项练习将帮助您从第一性原理出发，理解分类器超参数的理论意义。[@problem_id:3125741]", "problem": "考虑一个深度学习中的少样本分类场景，其中每个类别由嵌入空间中的一个原型表示。设嵌入映射表示为 $\\phi(\\cdot)$，并假设在 $\\mathbb{R}^{m}$ 中有 $C$ 个类别，其类别原型为 $\\mu_{1}, \\mu_{2}, \\dots, \\mu_{C}$。分类器通过计算与类别原型的负距离的softmax来为输入 $x$ 分配标签，其中温度参数 $\\tau > 0$，具体公式如下：\n$$\np_{\\tau}(y=c \\mid x) \\;=\\; \\frac{\\exp\\!\\big(-\\tau\\, d\\!\\big(\\phi(x), \\mu_{c}\\big)\\big)}{\\sum_{k=1}^{C} \\exp\\!\\big(-\\tau\\, d\\!\\big(\\phi(x), \\mu_{k}\\big)\\big)},\n$$\n其中 $d\\!\\big(\\phi(x), \\mu_{c}\\big) = \\|\\phi(x) - \\mu_{c}\\|_{2}^{2}$ 是平方欧几里得距离。假设嵌入的类条件分布是高斯分布，且所有类别共享各向同性的协方差，即对于每个类别 $c$，给定 $y=c$ 的随机向量 $z = \\phi(x)$ 服从均值为 $\\mu_{c}$、协方差矩阵为 $\\sigma^{2} I_{m}$ 的多元正态分布，记为 $z \\mid y=c \\sim \\mathcal{N}\\!\\big(\\mu_{c}, \\sigma^{2} I_{m}\\big)$，其中 $\\sigma^{2} > 0$，$I_{m}$ 是 $m \\times m$ 的单位矩阵。假设类别先验相等，即对于所有 $c \\in \\{1, 2, \\dots, C\\}$，$p(y=c) = \\frac{1}{C}$。\n\n从高斯概率密度函数和贝叶斯法则的定义出发，推导上述softmax分类器的温度参数 $\\tau$ 的值，使得在所述假设下，$p_{\\tau}(y=c \\mid x)$与贝叶斯最优后验 $p(y=c \\mid x)$相匹配。将你的最终答案表示为仅含 $\\sigma^{2}$ 的单个闭式符号表达式。不需要数值近似或四舍五入。", "solution": "目标是确定温度参数 $\\tau$ 的值，使得在指定的模型假设下，给定的softmax分类器的后验概率分布 $p_{\\tau}(y=c \\mid x)$ 与贝叶斯最优后验概率 $p(y=c \\mid x)$ 完全相同。\n\n首先，让我们推导贝叶斯最优后验概率 $p(y=c \\mid x)$。令 $z = \\phi(x)$ 为输入 $x$ 在 $\\mathbb{R}^{m}$ 中的嵌入。根据贝叶斯法则，给定嵌入 $z$ 时类别 $c$ 的后验概率为：\n$$p(y=c \\mid z) = \\frac{p(z \\mid y=c) p(y=c)}{p(z)}$$\n分母 $p(z)$ 是 $z$ 的边缘概率密度，可以通过全概率定律对所有可能的类别求和来表示：\n$$p(z) = \\sum_{k=1}^{C} p(z \\mid y=k) p(y=k)$$\n将此代入贝叶斯法则的表达式中，得到：\n$$p(y=c \\mid z) = \\frac{p(z \\mid y=c) p(y=c)}{\\sum_{k=1}^{C} p(z \\mid y=k) p(y=k)}$$\n\n题目提供了以下信息：\n1. 对于任意类别 $c$，嵌入 $z = \\phi(x)$ 的类条件分布是一个均值为 $\\mu_{c}$、协方差矩阵为 $\\sigma^{2} I_{m}$ 的多元正态分布。这表示为 $z \\mid y=c \\sim \\mathcal{N}(\\mu_{c}, \\sigma^{2} I_{m})$。\n2. 类别先验是均匀的，即对于所有 $c \\in \\{1, 2, \\dots, C\\}$，$p(y=c) = \\frac{1}{C}$。\n\n对于一个向量 $z \\in \\mathbb{R}^{m}$，多元正态分布 $\\mathcal{N}(\\mu_{c}, \\sigma^{2} I_{m})$ 的概率密度函数 (PDF) 为：\n$$p(z \\mid y=c) = \\frac{1}{(2\\pi)^{m/2} \\det(\\sigma^{2} I_{m})^{1/2}} \\exp\\left(-\\frac{1}{2} (z - \\mu_{c})^{\\top} (\\sigma^{2} I_{m})^{-1} (z - \\mu_{c})\\right)$$\n我们来简化这个PDF中的各项。协方差矩阵的行列式是 $\\det(\\sigma^{2} I_{m}) = (\\sigma^{2})^{m} \\det(I_{m}) = (\\sigma^{2})^{m}$。协方差矩阵的逆是 $(\\sigma^{2} I_{m})^{-1} = \\frac{1}{\\sigma^{2}} I_{m}^{-1} = \\frac{1}{\\sigma^{2}} I_{m}$。\n指数中的二次型变为：\n$$(z - \\mu_{c})^{\\top} \\left(\\frac{1}{\\sigma^{2}} I_{m}\\right) (z - \\mu_{c}) = \\frac{1}{\\sigma^{2}} (z - \\mu_{c})^{\\top} (z - \\mu_{c}) = \\frac{1}{\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}$$\n将这些代回PDF表达式中，我们得到：\n$$p(z \\mid y=c) = \\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)$$\n\n现在，我们将PDF和均匀先验 $p(y=c) = \\frac{1}{C}$ 都代入贝叶斯最优后验的表达式中：\n$$p(y=c \\mid z) = \\frac{\\left(\\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)\\right) \\left(\\frac{1}{C}\\right)}{\\sum_{k=1}^{C} \\left(\\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)\\right) \\left(\\frac{1}{C}\\right)}$$\n常数项 $\\frac{1}{(2\\pi\\sigma^{2})^{m/2}}$ 和 $\\frac{1}{C}$ 在分子和分母的和式中的每一项都是公共的，所以它们可以消掉。贝叶斯最优后验的简化表达式是：\n$$p(y=c \\mid z) = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n\n接下来，我们将这个推导出的后验与问题描述中给出的分类器模型进行比较。模型的概率分布是：\n$$p_{\\tau}(y=c \\mid x) = \\frac{\\exp\\big(-\\tau\\, d\\big(\\phi(x), \\mu_{c}\\big)\\big)}{\\sum_{k=1}^{C} \\exp\\big(-\\tau\\, d\\big(\\phi(x), \\mu_{k}\\big)\\big)}$$\n使用 $z = \\phi(x)$ 和距离度量的定义 $d(z, \\mu_c) = \\|z - \\mu_{c}\\|_{2}^{2}$，我们可以将分类器的概率重写为：\n$$p_{\\tau}(y=c \\mid x) = \\frac{\\exp\\left(-\\tau \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\tau \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n\n为了使分类器达到贝叶斯最优，对于所有的 $c$ 和 $z$，必须有 $p_{\\tau}(y=c \\mid x) = p(y=c \\mid z)$。\n$$\\frac{\\exp\\left(-\\tau \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\tau \\|z - \\mu_{k}\\|_{2}^{2}\\right)} = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n两个表达式都具有应用于一组分数的softmax函数的形式。为了使输出对于任何原型集 $\\{\\mu_{k}\\}$ 和任何嵌入 $z$ 都相同，分子和分母中相应指数函数的参数必须相等（最多相差一个与类别索引 $c$ 无关的加法常数，但在本例中该常数为零）。\n通过比较类别 $c$ 的指数，我们得到等式：\n$$-\\tau \\|z - \\mu_{c}\\|_{2}^{2} = -\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}$$\n由于这对任意 $z$ 都必须成立，我们可以选择一个 $z$ 使得 $\\|z - \\mu_{c}\\|_{2}^{2} \\neq 0$，因此可以同时除以 $-\\|z - \\mu_{c}\\|_{2}^{2}$：\n$$\\tau = \\frac{1}{2\\sigma^{2}}$$\n这就得出了温度参数 $\\tau$ 用类条件嵌入分布的方差 $\\sigma^{2}$ 表示的值。条件 $\\sigma^{2} > 0$ 确保了 $\\tau$ 是有定义的正数，符合要求。", "answer": "$$\\boxed{\\frac{1}{2\\sigma^{2}}}$$", "id": "3125741"}, {"introduction": "理论模型虽好，但现实世界的数据往往不完美。在小样本学习中，支持集可能包含异常值，或者不同类别之间的样本数量存在严重不平衡。在这些情况下，使用简单的算术平均来计算类别原型，很容易被异常数据带偏，从而导致分类性能下降。本练习将引导您构建一个更具鲁棒性的原型估计方法。您需要从概率建模的思路出发，推导一种加权平均方案，该方案能够自动降低异常值在原型计算中的权重。通过理论推导与动手编程相结合，您将实现并评估一个对噪声数据更具韧性的分类器，深刻体会鲁棒估计在构建实用机器学习系统中的关键作用。[@problem_id:3125726]", "problem": "考虑一个双类别少样本分类任务，其中一个基于度量的分类器通过比较查询的嵌入与类别原型来预测其类别。设嵌入函数表示为 $\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$。对于一个类别 $c\\in\\{0,1\\}$，其支持集嵌入为 $\\{y_{c,i}\\}_{i=1}^{k_c}$，其中 $y_{c,i}=\\phi(x_{c,i})$，一个标准原型使用未加权均值 $\\mu_c=\\frac{1}{k_c}\\sum_{i=1}^{k_c}y_{c,i}$。在 $k_1\\ll k_2$ 的类别不平衡的少样本设置中，未加权均值可能会被多数类别中的离散点所影响而产生偏差。为缓解此问题，考虑一个加权原型，其形式为 $\\mu_c=\\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$，其中 $\\alpha_{c,i}\\ge 0$ 且 $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$。\n\n从基础的概率建模和经验风险最小化原则出发，推导出一个用于选择权重 $\\alpha_{c,i}$ 的有原则的规则，该规则能减少离群点的影响，同时满足约束条件 $\\alpha_{c,i}\\ge 0$ 和 $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$。您的推导必须从关于嵌入空间中数据生成过程的一个良态假设开始，并逐步推导出一个具体的加权机制，而不使用任何特定的启发式方法。然后实现所得的分类器，并在以下条件下评估其性能。\n\n使用一个二维嵌入，其固定的线性嵌入函数为 $\\phi(x)=W x$，其中\n$$\nW=\\begin{pmatrix}\n1.2  0.3\\\\\n-0.4  0.8\n\\end{pmatrix}.\n$$\n分类必须在嵌入空间中通过与原型的最小平方欧氏距离进行，平局情况下偏向于数值较小的类别索引。对于下述每个测试用例，为每个类别计算两种原型：未加权均值和推导出的加权原型。对于每个查询，使用每组原型预测其类别，并计算准确率（定义为正确分类的查询所占的比例，以小数表示）。为了在您的加权规则所需的任何方差或尺度估计中保持数值稳定性，在任何可能出现除以零的地方加上一个小的常数 $\\varepsilon=10^{-6}$。\n\n测试套件。对于每个测试用例，支持集和查询都在 $\\mathbb{R}^2$ 中指定。所有坐标都是无单位的数值。任务是为每个测试用例输出一个浮点数，该数等于加权原型分类器的准确率减去未加权均值分类器的准确率，并四舍五入到三位小数。\n\n- 测试用例 A（中度不平衡，多数类簇离散）：\n    - 类别 $0$ 支持集 ($k_0=3$)：$(-0.1,\\,0.2)$, $(0.05,\\,-0.05)$, $(0.1,\\,0.0)$。\n    - 类别 $1$ 支持集 ($k_1=7$)：$(3.8,\\,0.1)$, $(4.1,\\,-0.2)$, $(4.0,\\,0.0)$, $(5.5,\\,2.0)$, $(3.9,\\,0.2)$, $(4.2,\\,0.1)$, $(4.1,\\,0.0)$。\n    - 查询与真实标签：\n        - $(-0.05,\\,0.0)\\rightarrow 0$, $(0.2,\\,-0.1)\\rightarrow 0$, $(4.05,\\,0.0)\\rightarrow 1$, $(3.7,\\,0.1)\\rightarrow 1$, $(2.1,\\,0.4)\\rightarrow 1$, $(2.0,\\,0.0)\\rightarrow 0$。\n\n- 测试用例 B（极端不平衡 $k_0=1\\ll k_1=10$）：\n    - 类别 $0$ 支持集 ($k_0=1$)：$(0.0,\\,0.0)$。\n    - 类别 $1$ 支持集 ($k_1=10$)：$(4.0,\\,0.0)$, $(4.1,\\,0.1)$, $(3.9,\\,-0.1)$, $(4.2,\\,0.0)$, $(3.8,\\,0.2)$, $(4.3,\\,-0.2)$, $(4.05,\\,0.05)$, $(4.2,\\,0.2)$, $(5.0,\\,1.5)$, $(3.7,\\,-0.3)$。\n    - 查询与真实标签：\n        - $(0.1,\\,0.0)\\rightarrow 0$, $(-0.2,\\,0.05)\\rightarrow 0$, $(4.05,\\,0.1)\\rightarrow 1$, $(3.9,\\,-0.05)\\rightarrow 1$, $(2.2,\\,0.2)\\rightarrow 1$, $(-1.0,\\,0.0)\\rightarrow 0$。\n\n- 测试用例 C（类别平衡，但两个类别都有强离群点）：\n    - 类别 $0$ 支持集 ($k_0=5$)：$(0.0,\\,0.0)$, $(0.1,\\,-0.1)$, $(-0.05,\\,0.1)$, $(0.2,\\,0.05)$, $(-1.8,\\,-1.2)$。\n    - 类别 $1$ 支持集 ($k_1=5$)：$(3.9,\\,0.0)$, $(4.1,\\,0.1)$, $(4.0,\\,-0.1)$, $(4.2,\\,0.0)$, $(5.2,\\,2.5)$。\n    - 查询与真实标签：\n        - $(0.1,\\,0.0)\\rightarrow 0$, $(-0.2,\\,0.15)\\rightarrow 0$, $(4.1,\\,-0.05)\\rightarrow 1$, $(3.95,\\,0.2)\\rightarrow 1$, $(2.3,\\,0.5)\\rightarrow 1$, $(1.0,\\,-0.7)\\rightarrow 0$。\n\n您的程序必须：\n- 实现推导出的加权规则来计算 $\\alpha_{c,i}$ 并构建加权原型 $\\mu_c$。\n- 通过简单的均值计算未加权原型以供比较。\n- 在嵌入空间中，根据与原型的最小平方欧氏距离对每个查询进行分类。\n- 对每个测试用例，计算加权和未加权情况下的准确率。\n- 输出单行，包含一个用逗号分隔并由方括号括起来的列表，其中包含按顺序排列的测试用例 A、B、C 的准确率差异（加权减去未加权），每个差异都四舍五入到三位小数。例如，最终输出格式必须严格为 $\\left[\\text{result}_A,\\text{result}_B,\\text{result}_C\\right]$ 的形式。[@problem_id:483]", "solution": "## 问题验证\n\n### 第1步：提取已知条件\n- **任务**：双类别少样本分类。\n- **分类器**：基于度量，通过比较查询嵌入与类别原型。\n- **类别**：$c \\in \\{0, 1\\}$。\n- **嵌入函数**：$\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$。\n- **支持集**：对于类别 $c$，$\\{x_{c,i}\\}_{i=1}^{k_c}$ 是输入点。\n- **支持集嵌入**：$y_{c,i} = \\phi(x_{c,i})$。\n- **未加权原型**：$\\mu_c = \\frac{1}{k_c}\\sum_{i=1}^{k_c}y_{c,i}$。\n- **加权原型**：$\\mu_c = \\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$，约束条件为 $\\alpha_{c,i} \\ge 0$ 和 $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$。\n- **推导目标**：从概率建模和经验风险最小化出发，为权重 $\\alpha_{c,i}$ 推导出一个有原则的规则。\n- **嵌入矩阵**：对于实现，嵌入是线性的，$\\phi(x) = Wx$，其中 $d=2$ 且\n$$\nW=\\begin{pmatrix}\n1.2  0.3\\\\\n-0.4  0.8\n\\end{pmatrix}.\n$$\n- **分类度量**：基于与原型的最小平方欧氏距离。\n- **平局处理规则**：平局情况下选择数值较小的类别索引（即类别 $0$）。\n- **数值稳定性**：在分母上加上 $\\varepsilon=10^{-6}$ 以防止除以零。\n- **测试用例**：提供了三个测试用例（A, B, C），每个都包含类别 $0$ 和类别 $1$ 的特定支持集，以及一组带真实标签的查询。\n- **输出**：对于每个测试用例，计算加权原型分类器的准确率减去未加权均值分类器的准确率，结果四舍五入到三位小数。最终输出必须为单行：`[result_A,result_B,result_C]`。\n\n### 第2步：使用提取的已知条件进行验证\n根据验证标准评估问题：\n-   **科学基础**：该问题牢固地植根于机器学习和统计学的既定原则。它要求基于概率建模和经验风险最小化进行推导，这些都是开发算法的标准、严谨的框架。使用加权均值来创建稳健的估计量是稳健统计学中一个经过充分研究的概念。该问题具有科学合理性。\n-   **良态**：问题定义清晰且自洽。它提供了所有必要的信息：数学公式、特定的嵌入函数、分类规则、平局处理规则、稳定性常数以及一套完整的测试数据。目标明确无歧义。\n-   **客观**：问题以精确、正式的语言陈述，不含任何主观或基于意见的主张。所有数据和条件都是数值化和客观的。\n\n该问题未表现出任何无效性缺陷。它并非科学上不合理、无法形式化、不完整、不切实际、病态、微不足道或超出科学可验证的范畴。\n\n### 第3步：结论与行动\n此问题**有效**。将提供解决方案。\n\n---\n## 加权规则的推导\n\n目标是为原型定义 $\\mu_c=\\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$ 中的权重 $\\alpha_{c,i}$ 推导出一个有原则的规则，该规则从概率模型和经验风险最小化原则出发，旨在减轻离群点的影响。\n\n1.  **概率模型**：我们首先将一个类别 $c$ 的支持集嵌入 $\\{y_{c,i}\\}_{i=1}^{k_c}$ 建模为对一个未知的真实类别原型 $\\theta_c$ 的带噪观测。为了考虑离群点，我们假设每个观测值 $y_{c,i}$ 均从一个以 $\\theta_c$ 为中心但各自具有独立方差 $\\sigma_{c,i}^2$ 的高斯分布中生成。即 $y_{c,i} \\sim \\mathcal{N}(\\theta_c, \\sigma_{c,i}^2 I)$，其中 $I$ 是单位矩阵。对于一个点 $y_{c,i}$，较大的方差 $\\sigma_{c,i}^2$ 表示我们对于该点是真实中心 $\\theta_c$ 的一个代表性样本的置信度较低，这为处理离群点提供了一种自然的机制。\n\n2.  **通过最大似然实现经验风险最小化**：在此模型下，经验风险最小化原则对应于找到使观测到支持集的对数似然最大化的原型 $\\theta_c$。假设观测是独立的，对数似然为：\n    $$ \\mathcal{L}(\\theta_c) = \\ln \\prod_{i=1}^{k_c} p(y_{c,i} | \\theta_c, \\sigma_{c,i}^2) = \\sum_{i=1}^{k_c} \\ln p(y_{c,i} | \\theta_c, \\sigma_{c,i}^2) $$\n    对于我们在 $d$ 维空间中的高斯模型，这变为：\n    $$ \\mathcal{L}(\\theta_c) = \\sum_{i=1}^{k_c} \\left( -\\frac{\\|y_{c,i} - \\theta_c\\|^2}{2\\sigma_{c,i}^2} - \\frac{d}{2}\\ln(2\\pi\\sigma_{c,i}^2) \\right) $$\n    相对于 $\\theta_c$ 最大化 $\\mathcal{L}(\\theta_c)$ 等价于最小化负对数似然。去掉不依赖于 $\\theta_c$ 的项后，我们需要最小化以下风险函数 $R(\\theta_c)$：\n    $$ R(\\theta_c) = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\|y_{c,i} - \\theta_c\\|^2 $$\n    这是一个加权最小二乘问题，其中每个点对损失的贡献与其方差成反比。\n\n3.  **求解原型**：为了找到最优原型（我们对 $\\theta_c$ 的估计），我们将 $R(\\theta_c)$ 关于 $\\theta_c$ 的梯度设为零：\n    $$ \\nabla_{\\theta_c} R(\\theta_c) = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\cdot 2(\\theta_c - y_{c,i}) = 0 $$\n    解出 $\\theta_c$ 得：\n    $$ \\left( \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\right) \\theta_c = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} y_{c,i} $$\n    得到的原型估计（我们记为 $\\mu_c$）是支持集嵌入的一个加权平均：\n    $$ \\mu_c = \\frac{\\sum_{i=1}^{k_c} (1/\\sigma_{c,i}^2) y_{c,i}}{\\sum_{j=1}^{k_c} (1/\\sigma_{c,j}^2)} $$\n    这对应于所需的形式 $\\mu_c = \\sum_{i=1}^{k_c} \\alpha_{c,i} y_{c,i}$，其中权重为 $\\alpha_{c,i} = \\frac{w_{c,i}}{\\sum_{j=1}^{k_c} w_{c,j}}$ 且 $w_{c,i} = 1/\\sigma_{c,i}^2$。这些权重自动满足约束条件 $\\alpha_{c,i} \\ge 0$ 和 $\\sum_i \\alpha_{c,i}=1$。\n\n4.  **推导加权规则**：最后一步是定义未知的各个方差 $\\sigma_{c,i}^2$。一个有原则的、非迭代的方法是使用一个数据驱动的代理变量来表示每个点的可靠性。一个点与其类别中心的初步估计的偏差，是其潜在“离群性”的一个良好度量。标准的未加权均值 $\\mu_{c, \\text{unw}} = \\frac{1}{k_c}\\sum_j y_{c,j}$，是这个初步估计的最简单和最自然的选择（如果假设所有方差相等，它就是最大似然估计）。\n\n    因此，我们将每个点的方差 $\\sigma_{c,i}^2$ 建模为与其到该未加权均值的平方欧氏距离成正比：\n    $$ \\sigma_{c,i}^2 \\propto \\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 $$\n    这提供了一个具体的规则：远离初始“质心”的点被认为是可靠性较低（方差较高），因此在计算最终原型时被赋予较小的权重。未归一化的权重与此方差成反比：\n    $$ w_{c,i} \\propto \\frac{1}{\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2} $$\n    为了处理点可能正好落在均值上的情况并确保数值稳定性，我们按规定加入一个小的常数 $\\varepsilon=10^{-6}$。最终，未归一化权重的具体形式为：\n    $$ w_{c,i} = \\frac{1}{\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 + \\varepsilon} $$\n    然后对这些权重进行归一化以获得 $\\alpha_{c,i}$，用于计算最终的加权原型。对于 $k_c=1$ 的特殊情况，距离为 $0$，未归一化权重为 $1/\\varepsilon$，归一化权重为 $1$，正确地得到该点本身作为原型。推导至此完成。\n\n## 算法与实现\n所推导的方法实现如下：\n1.  对每个类别 $c \\in \\{0, 1\\}$，取其支持点 $\\{x_{c,i}\\}$ 并应用嵌入函数 $\\phi(x) = Wx$ 得到嵌入 $\\{y_{c,i}\\}$。\n2.  **未加权原型**：对每个类别，计算嵌入的标准均值：$\\mu_{c, \\text{unw}} = \\frac{1}{k_c}\\sum_{i=1}^{k_c} y_{c,i}$。\n3.  **加权原型**：对每个类别：\n    a. 如果 $k_c = 1$，加权原型就是该单个支持集嵌入。\n    b. 如果 $k_c > 1$，首先计算未加权原型 $\\mu_{c, \\text{unw}}$。\n    c. 对每个支持集嵌入 $y_{c,i}$，计算一个未归一化的权重 $w_{c,i} = 1 / (\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 + \\varepsilon)$。\n    d. 归一化权重：$\\alpha_{c,i} = w_{c,i} / \\sum_{j=1}^{k_c} w_{c,j}$。\n    e. 计算加权原型：$\\mu_{c, \\text{w}} = \\sum_{i=1}^{k_c} \\alpha_{c,i} y_{c,i}$。\n4.  **分类与评估**：对每个测试用例：\n    a. 嵌入查询点 $\\{x_q\\}$。\n    b. 对每个查询嵌入 $y_q$，计算其与原型 $\\{\\mu_{0, \\text{unw}}, \\mu_{1, \\text{unw}}\\}$ 和 $\\{\\mu_{0, \\text{w}}, \\mu_{1, \\text{w}}\\}$ 的平方欧氏距离。\n    c. 对每种方法，将查询分配给最近的原型的类别标签，平局时偏向类别 $0$。\n    d. 通过将预测结果与真实标签进行比较，计算未加权和加权方法的准确率。\n    e. 计算差异：（加权准确率） - （未加权准确率）。\n5.  收集所有测试用例的准确率差异，并按要求格式化输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the few-shot classification problem by deriving and implementing a\n    principled weighting scheme for robust prototypes.\n    \"\"\"\n\n    # ---- Global parameters from the problem statement ----\n    W = np.array([[1.2, 0.3], [-0.4, 0.8]])\n    epsilon = 1e-6\n\n    # ---- Test cases ----\n    # Each case is a tuple: (support_set_dict, query_points_array, query_labels_array)\n    test_cases = [\n        (\n            {  # Support set for Test Case A\n                0: np.array([[-0.1, 0.2], [0.05, -0.05], [0.1, 0.0]]),\n                1: np.array([(3.8, 0.1), (4.1, -0.2), (4.0, 0.0), (5.5, 2.0), (3.9, 0.2), (4.2, 0.1), (4.1, 0.0)])\n            },\n            np.array([(-0.05, 0.0), (0.2, -0.1), (4.05, 0.0), (3.7, 0.1), (2.1, 0.4), (2.0, 0.0)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        ),\n        (\n            {  # Support set for Test Case B\n                0: np.array([[0.0, 0.0]]),\n                1: np.array([(4.0, 0.0), (4.1, 0.1), (3.9, -0.1), (4.2, 0.0), (3.8, 0.2), (4.3, -0.2), (4.05, 0.05), (4.2, 0.2), (5.0, 1.5), (3.7, -0.3)])\n            },\n            np.array([(0.1, 0.0), (-0.2, 0.05), (4.05, 0.1), (3.9, -0.05), (2.2, 0.2), (-1.0, 0.0)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        ),\n        (\n            {  # Support set for Test Case C\n                0: np.array([(0.0, 0.0), (0.1, -0.1), (-0.05, 0.1), (0.2, 0.05), (-1.8, -1.2)]),\n                1: np.array([(3.9, 0.0), (4.1, 0.1), (4.0, -0.1), (4.2, 0.0), (5.2, 2.5)])\n            },\n            np.array([(0.1, 0.0), (-0.2, 0.15), (4.1, -0.05), (3.95, 0.2), (2.3, 0.5), (1.0, -0.7)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        )\n    ]\n\n    results = []\n\n    # ---- Helper functions ----\n    def embed_points(points, W_matrix):\n        if points.ndim == 1:\n            points = points.reshape(1, -1)\n        return (W_matrix @ points.T).T\n\n    def get_prototypes(support_embeddings, eps):\n        prototypes_unweighted = {}\n        prototypes_weighted = {}\n        \n        for c in sorted(support_embeddings.keys()):\n            points = support_embeddings[c]\n            k_c = len(points)\n            \n            # Unweighted prototype: simple mean\n            mu_unweighted = np.mean(points, axis=0)\n            prototypes_unweighted[c] = mu_unweighted\n            \n            # Weighted prototype: derived from the robust estimation principle\n            if k_c == 1:\n                mu_weighted = points[0]\n            else:\n                dists_sq = np.sum((points - mu_unweighted)**2, axis=1)\n                raw_weights = 1.0 / (dists_sq + eps)\n                normalized_weights = raw_weights / np.sum(raw_weights)\n                mu_weighted = np.sum(points * normalized_weights[:, np.newaxis], axis=0)\n            \n            prototypes_weighted[c] = mu_weighted\n            \n        return prototypes_unweighted, prototypes_weighted\n\n    def classify_queries(prototypes_dict, query_embeddings):\n        # Prototypes are already created for sorted class indices\n        prototypes = [prototypes_dict[i] for i in sorted(prototypes_dict.keys())]\n        predictions = []\n        for q in query_embeddings:\n            dists_sq = [np.sum((q - p)**2) for p in prototypes]\n            # np.argmin breaks ties by choosing the first occurrence, which\n            # corresponds to the smaller class index as required.\n            predictions.append(np.argmin(dists_sq))\n        return np.array(predictions)\n\n    # ---- Main loop for test case evaluation ----\n    for case in test_cases:\n        support_x, query_x, labels = case\n        \n        # 1. Embed all points using the given matrix W\n        support_y = {c: embed_points(pts, W) for c, pts in support_x.items()}\n        query_y = embed_points(query_x, W)\n        \n        # 2. Get both unweighted and weighted prototypes for each class\n        protos_unw, protos_w = get_prototypes(support_y, epsilon)\n        \n        # 3. Classify queries and compute accuracy for the unweighted method\n        preds_unw = classify_queries(protos_unw, query_y)\n        acc_unw = np.mean(preds_unw == labels)\n\n        # 4. Classify queries and compute accuracy for the weighted method\n        preds_w = classify_queries(protos_w, query_y)\n        acc_w = np.mean(preds_w == labels)\n        \n        # 5. Calculate and store the accuracy difference, rounded to 3 decimal places\n        accuracy_diff = acc_w - acc_unw\n        results.append(round(accuracy_diff, 3))\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3125726"}, {"introduction": "除了基于原型的非参数方法，另一种强大的小样本学习策略是在深度网络提取的特征之上，训练一个简单的参数模型，例如线性分类器。当训练样本极少（例如“one-shot”场景）时，防止模型过拟合是成功的关键。这个练习将带您深入研究岭回归（Ridge Regression），这是一种利用 $L_2$ 正则化来训练稳定模型的经典技术。您将亲手推导出岭回归模型的闭式解（closed-form solution），并进一步在单样本（one-shot）的理想化场景下分析该解的行为。这项实践不仅能巩固您对正则化如何防止过拟合的理解，还能让您从数学上清晰地看到，即使在数据极其有限的情况下，正则化项是如何帮助分类器保持良好决策边界的。[@problem_id:3125783]", "problem": "一个深度神经网络主干产生 $\\mathbb{R}^{d}$ 中的特征向量，这些向量通过拟合一个线性头来执行少样本分类。共有 $C$ 个类别，每个类别有 $k$ 个带标签的样本，这些样本被排列成一个数据矩阵 $X \\in \\mathbb{R}^{d \\times Ck}$，其列是所有训练样本的主干特征。设标签矩阵 $Y \\in \\mathbb{R}^{C \\times Ck}$ 收集了 $X$ 中每一列的 one-hot 类别指示向量，因此对于属于类别 $c$ 的每个训练样本，$Y$ 中对应的列等于标准基向量 $e_{c} \\in \\mathbb{R}^{C}$。\n\n线性头是一个矩阵 $W \\in \\mathbb{R}^{C \\times d}$，它将一个特征 $x \\in \\mathbb{R}^{d}$ 映射到 $\\mathbb{R}^{C}$ 中的一个类别 logit 向量。该线性头通过最小化岭回归正则化的平方误差目标函数进行训练：\n$$\n\\mathcal{L}(W) = \\|Y - W X\\|_{F}^{2} + \\lambda \\|W\\|_{F}^{2},\n$$\n其中 $\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数，$\\lambda > 0$ 是正则化强度。仅从上述目标函数的定义以及矩阵微积分和线性代数的标准性质出发，推导出唯一的最小化子 $W^{\\star}$。\n\n然后，在以下主干特征的几何模型下，通过考虑精确的 $k=1$ 情况来分析单样本极限 $k \\to 1$。假设存在类别原型 $\\mu_{1}, \\dots, \\mu_{C} \\in \\mathbb{R}^{d}$，使得类别 $c$ 的单个训练特征等于 $\\mu_{c}$，并且这些原型在以下意义上是正交且等尺度的：\n$$\n\\mu_{c}^{\\top} \\mu_{c'} = m^{2} \\, \\delta_{cc'}\n$$\n对于一个固定的标量 $m > 0$，其中 $\\delta_{cc'}$ 是克罗内克δ。设 $X = [\\mu_{1} \\,\\, \\mu_{2} \\,\\, \\cdots \\,\\, \\mu_{C}] \\in \\mathbb{R}^{d \\times C}$，并设 $Y$ 是 $\\mathbb{R}^{C \\times C}$ 中对应的 one-hot 标签矩阵。使用你推导出的 $W^{\\star}$，计算分类间隔。分类间隔定义为，当在某个测试特征 $x_{\\text{test}} = \\mu_{t}$ (其中 $t \\in \\{1, \\dots, C\\}$) 上评估线性头时，真实类别的 logit 与所有错误类别中的最大 logit 之差。将你的最终答案表示为关于 $m$ 和 $\\lambda$ 的单个闭式解析表达式，无需四舍五入，也无单位。", "solution": "该问题要求两个结果。首先，推导最小化岭回归正则化平方误差目标函数的最优权重矩阵 $W^{\\star}$。其次，在特定的单样本学习场景下计算分类间隔。\n\n第1部分：最优权重矩阵 $W^{\\star}$ 的推导。\n需要最小化的目标函数由下式给出\n$$\n\\mathcal{L}(W) = \\|Y - W X\\|_{F}^{2} + \\lambda \\|W\\|_{F}^{2}\n$$\n其中 $W \\in \\mathbb{R}^{C \\times d}$ 是权重矩阵，$X \\in \\mathbb{R}^{d \\times Ck}$ 是数据矩阵，$Y \\in \\mathbb{R}^{C \\times Ck}$ 是标签矩阵，$\\lambda > 0$ 是正则化参数。项 $\\|\\cdot\\|_F$ 表示弗罗贝尼乌斯范数。\n\n目标函数 $\\mathcal{L}(W)$ 是两个平方范数之和。弗罗贝尼乌斯范数是由内积诱导的范数，因此其平方是一个凸函数。两个凸函数之和是凸函数。此外，由于 $\\lambda > 0$，项 $\\lambda \\|W\\|_{F}^{2}$ 是严格凸的。这确保了 $\\mathcal{L}(W)$ 是严格凸函数，因此有唯一的全局最小化子。通过将 $\\mathcal{L}(W)$ 对 $W$ 的梯度设为零，可以找到这个最小化子。\n\n我们将通过分别对每一项求导来找到梯度 $\\frac{\\partial \\mathcal{L}}{\\partial W}$。\n\n对于第一项，我们使用性质 $\\|A\\|_{F}^{2} = \\text{tr}(A^{\\top}A)$，其中 $\\text{tr}(\\cdot)$ 是迹算子。\n$$\n\\|Y - WX\\|_{F}^{2} = \\text{tr}\\left((Y - WX)^{\\top}(Y - WX)\\right)\n$$\n为了求梯度，我们可以使用矩阵微分。让我们求解 $d(\\|Y - WX\\|_{F}^{2})$ 相对于 $W$ 的微分变化 $dW$。\n$$\nd(\\|Y - WX\\|_{F}^{2}) = \\text{tr}\\left( d((Y - WX)^{\\top})(Y - WX) + (Y - WX)^{\\top}d(Y - WX) \\right)\n$$\n由于 $d(Y - WX) = -dW X$，我们有 $d((Y - WX)^{\\top}) = -(dW X)^{\\top} = -X^{\\top}(dW)^{\\top}$。\n$$\nd(\\|Y - WX\\|_{F}^{2}) = \\text{tr}\\left(-X^{\\top}(dW)^{\\top}(Y - WX) - (Y - WX)^{\\top}(dW X)\\right)\n$$\n使用迹的循环性质 $\\text{tr}(AB) = \\text{tr}(BA)$ 和 $\\text{tr}(A^{\\top}) = \\text{tr}(A)$，我们可以重写迹中的第一项：\n$$\n\\text{tr}\\left(-X^{\\top}(dW)^{\\top}(Y - WX)\\right) = \\text{tr}\\left(-(Y - WX)^{\\top} dW X\\right)\n$$\n因此，\n$$\nd(\\|Y - WX\\|_{F}^{2}) = -2 \\, \\text{tr}\\left((Y - WX)^{\\top}dW X\\right) = -2 \\, \\text{tr}\\left(X (Y - WX)^{\\top} dW\\right)\n$$\n关于 $W$ 的梯度是在迹内部乘以 $dW$ 的矩阵的转置。\n$$\n\\frac{\\partial}{\\partial W} \\|Y - WX\\|_{F}^{2} = -2 (X (Y - WX)^{\\top})^{\\top} = -2 (Y - WX)X^{\\top}\n$$\n对于第二项，即正则化项 $\\lambda \\|W\\|_{F}^{2} = \\lambda \\, \\text{tr}(W^{\\top}W)$：\n$$\nd(\\lambda \\, \\text{tr}(W^{\\top}W)) = \\lambda \\, \\text{tr}(d(W^{\\top}W)) = \\lambda \\, \\text{tr}((dW)^{\\top}W + W^{\\top}dW) = \\lambda \\, \\text{tr}(W^{\\top}dW + W^{\\top}dW) = 2\\lambda \\, \\text{tr}(W^{\\top}dW)\n$$\n因此，梯度为：\n$$\n\\frac{\\partial}{\\partial W} (\\lambda \\|W\\|_{F}^{2}) = 2\\lambda W\n$$\n结合这些梯度，目标函数的总梯度是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = -2(Y - WX)X^{\\top} + 2\\lambda W\n$$\n将梯度设为零以求最优权重矩阵 $W^{\\star}$：\n$$\n-2(Y - W^{\\star}X)X^{\\top} + 2\\lambda W^{\\star} = 0_{C \\times d}\n$$\n$$\n-(Y X^{\\top} - W^{\\star}XX^{\\top}) + \\lambda W^{\\star} = 0_{C \\times d}\n$$\n$$\n-Y X^{\\top} + W^{\\star}XX^{\\top} + \\lambda W^{\\star} = 0_{C \\times d}\n$$\n$$\nW^{\\star}(XX^{\\top} + \\lambda I_d) = YX^{\\top}\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。矩阵 $XX^{\\top} \\in \\mathbb{R}^{d \\times d}$ 是半正定的。由于 $\\lambda > 0$，矩阵 $(XX^{\\top} + \\lambda I_d)$ 是正定的，因此是可逆的。我们可以通过右乘其逆矩阵来求解 $W^{\\star}$：\n$$\nW^{\\star} = YX^{\\top}(XX^{\\top} + \\lambda I_d)^{-1}\n$$\n这就完成了问题的第一部分。\n\n第2部分：$k=1$ 时分类间隔的计算。\n我们现在考虑单样本学习的特定情况，即 $k=1$。训练数据由 $C$ 个类别中的每个类别的一个特征向量 $\\mu_c \\in \\mathbb{R}^d$ 组成。\n数据矩阵为 $X = [\\mu_1 \\,\\, \\mu_2 \\,\\, \\cdots \\,\\, \\mu_C] \\in \\mathbb{R}^{d \\times C}$。标签矩阵 $Y \\in \\mathbb{R}^{C \\times C}$ 的第 $c$ 列是标准基向量 $e_c$，这意味着 $Y$ 是 $C \\times C$ 的单位矩阵，即 $Y = I_C$。\n特征向量满足条件 $\\mu_c^{\\top}\\mu_{c'} = m^2 \\delta_{cc'}$，其中标量 $m > 0$。\n\n将 $Y=I_C$ 代入 $W^{\\star}$ 的表达式中：\n$$\nW^{\\star} = I_C X^{\\top}(XX^{\\top} + \\lambda I_d)^{-1} = X^{\\top}(XX^{\\top} + \\lambda I_d)^{-1}\n$$\n让我们分析 $XX^{\\top}$ 这一项。\n$$\nXX^{\\top} = \\left( \\sum_{c=1}^{C} \\mu_c e_c^{\\top} \\right) \\left( \\sum_{c'=1}^{C} e_{c'} \\mu_{c'}^{\\top} \\right) = \\sum_{c=1}^{C} \\sum_{c'=1}^{C} \\mu_c (e_c^{\\top} e_{c'}) \\mu_{c'}^{\\top} = \\sum_{c=1}^{C} \\mu_c \\mu_c^{\\top}\n$$\n需要求逆的矩阵是 $A = XX^{\\top} + \\lambda I_d = \\left(\\sum_{c=1}^{C} \\mu_c \\mu_c^{\\top}\\right) + \\lambda I_d$。\n让我们将这个矩阵 $A$ 应用于其中一个原型向量，例如 $\\mu_t$：\n$$\nA\\mu_t = \\left( \\sum_{c=1}^{C} \\mu_c \\mu_c^{\\top} + \\lambda I_d \\right) \\mu_t = \\sum_{c=1}^{C} \\mu_c(\\mu_c^{\\top} \\mu_t) + \\lambda \\mu_t\n$$\n使用给定条件 $\\mu_c^{\\top}\\mu_t = m^2 \\delta_{ct}$：\n$$\nA\\mu_t = \\sum_{c=1}^{C} \\mu_c(m^2 \\delta_{ct}) + \\lambda \\mu_t = m^2 \\mu_t + \\lambda \\mu_t = (m^2 + \\lambda)\\mu_t\n$$\n这表明每个 $\\mu_t$ 都是 $A$ 的一个特征向量，其特征值为 $m^2+\\lambda$。因此，对于逆矩阵 $A^{-1}$，我们有：\n$$\nA^{-1}\\mu_t = \\frac{1}{m^2+\\lambda}\\mu_t\n$$\n最优权重矩阵 $W^{\\star}$ 是行向量 $w_1^{\\top}, \\dots, w_C^{\\top}$ 的堆叠。第 $c$ 行是 $w_c^{\\top} = e_c^{\\top} W^{\\star}$。由 $W^{\\star} = X^{\\top}A^{-1}$，第 $c$ 个行向量由 $w_c^{\\top} = \\mu_c^{\\top}A^{-1}$ 给出。对应的列向量是 $w_c = (w_c^{\\top})^{\\top} = (A^{-1})^{\\top}\\mu_c$。由于 $A$ 是对称的，其逆矩阵 $A^{-1}$ 也是对称的。因此，$w_c = A^{-1}\\mu_c$。\n$$\nw_c = \\frac{1}{m^2+\\lambda}\\mu_c\n$$\n所以 $W^{\\star}$ 的第 $c$ 行是 $w_c^{\\top} = \\frac{1}{m^2+\\lambda}\\mu_c^{\\top}$。这意味着整个权重矩阵是：\n$$\nW^{\\star} = \\frac{1}{m^2+\\lambda} \\begin{pmatrix} \\mu_1^{\\top} \\\\ \\vdots \\\\ \\mu_C^{\\top} \\end{pmatrix} = \\frac{1}{m^2+\\lambda}X^{\\top}\n$$\n现在我们计算某个真实类别 $t$ 的测试特征 $x_{\\text{test}} = \\mu_t$ 的 logits。logit 向量是 $z = W^{\\star}x_{\\text{test}}$。\n$$\nz = \\left(\\frac{1}{m^2+\\lambda}X^{\\top}\\right) \\mu_t = \\frac{1}{m^2+\\lambda} \\begin{pmatrix} \\mu_1^{\\top} \\\\ \\vdots \\\\ \\mu_C^{\\top} \\end{pmatrix} \\mu_t = \\frac{1}{m^2+\\lambda} \\begin{pmatrix} \\mu_1^{\\top}\\mu_t \\\\ \\vdots \\\\ \\mu_C^{\\top}\\mu_t \\end{pmatrix}\n$$\n再次使用 $\\mu_c^{\\top}\\mu_t = m^2 \\delta_{ct}$，logit 向量 $z$ 的第 $c$ 个分量是：\n$$\nz_c = \\frac{m^2 \\delta_{ct}}{m^2+\\lambda}\n$$\n对于真实类别 $c=t$，logit 是 $z_t = \\frac{m^2 \\delta_{tt}}{m^2+\\lambda} = \\frac{m^2}{m^2+\\lambda}$。对于任何错误类别 $c \\neq t$，logit 是 $z_c = \\frac{m^2 \\delta_{ct}}{m^2+\\lambda} = 0$。\n\n分类间隔定义为真实类别的 logit 与所有错误类别中的最大 logit 之差。\n真实类别的 logit: $z_t = \\frac{m^2}{m^2+\\lambda}$。\n错误类别中的最大 logit: $\\max_{c \\neq t} z_c = \\max_{c \\neq t} 0 = 0$。\n因此，间隔为：\n$$\n\\text{间隔} = z_t - \\max_{c \\neq t} z_c = \\frac{m^2}{m^2+\\lambda} - 0 = \\frac{m^2}{m^2+\\lambda}\n$$", "answer": "$$\n\\boxed{\\frac{m^{2}}{m^{2}+\\lambda}}\n$$", "id": "3125783"}]}