{"hands_on_practices": [{"introduction": "本次实践将探索神经正切核 (NTK) 与经典傅里叶分析之间的深刻联系。通过为一个具有傅里叶特征的模型构建特定的 NTK，我们将发现其特征函数与傅里叶模式直接相关，这揭示了核函数固有的频率偏好。这个练习 [@problem_id:3159099] 提供了一种具体的方法来理解核的谱特性，这对它的学习行为至关重要。", "problem": "给定一个一维实数输入域，其中的点在周期性区间上采样。考虑一个与基于固定傅里叶特征的参数线性模型相关的神经正切核 (NTK)。具体来说，设模型定义为\n$$\nf(x;\\theta) \\;=\\; \\sqrt{\\alpha_0}\\,a_0 \\cdot \\phi_0(x) \\;+\\; \\sum_{k=1}^{M} \\sqrt{\\alpha_k}\\,\\Big(a_k^{(c)} \\,\\phi_k^{(c)}(x) \\;+\\; a_k^{(s)} \\,\\phi_k^{(s)}(x)\\Big),\n$$\n其参数为 $\\theta = \\big(a_0, \\{a_k^{(c)}, a_k^{(s)}\\}_{k=1}^M\\big)$，固定特征为\n$$\n\\phi_0(x)=1,\\quad \\phi_k^{(c)}(x)=\\cos(2\\pi k x),\\quad \\phi_k^{(s)}(x)=\\sin(2\\pi k x),\n$$\n对于整数 $k \\ge 1$。这里 $\\alpha_k \\ge 0$ 是非负标量，用于设定每个频率的相对贡献。使用神经正切核 (NTK) 的定义，\n$$\nk(x,x') \\;=\\; \\nabla_{\\theta} f(x;\\theta_0)^{\\top} \\nabla_{\\theta} f(x';\\theta_0),\n$$\n证明对于此模型，其核函数为\n$$\nk(x,x') \\;=\\; \\alpha_0 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\cos\\!\\big(2\\pi k (x - x')\\big)\n$$\n其中所有角度均以弧度为单位。考虑单位圆上均匀分布的离散点集，\n$$\nx_n \\;=\\; \\frac{n}{N}, \\quad n \\in \\{0,1,\\dots,N-1\\},\n$$\n并令 $K \\in \\mathbb{R}^{N\\times N}$ 为核矩阵，其元素为 $K_{n,m} = k(x_n,x_m)$。注意 $K$ 是实对称的，并且仅依赖于差值 $n-m$ 模 $N$，因此它是一个循环矩阵。\n\n任务：\n1) 数值计算特征分解 $K = U \\Lambda U^{\\top}$，其中 $U \\in \\mathbb{R}^{N\\times N}$ 是标准正交矩阵，$\\Lambda \\in \\mathbb{R}^{N\\times N}$ 是具有非负元素的对角矩阵。\n2) 将特征函数与傅里叶模式关联：对于循环矩阵，复数离散傅里叶变换 (DFT) 矩阵可将 $K$ 对角化，其特征值由 $K$ 的第一行的 DFT 给出。利用这一事实，通过计算第一行的 DFT，为特征值生成一个解析参考。\n3) 通过将其特征值与第一行 DFT 的解析特征值进行比较，验证数值特征分解，并报告最大绝对差值。\n\n你的程序必须为以下测试套件实现上述任务。对于每个测试用例，会给定 $N$ 和一个非零 $\\alpha_k$ 值的字典。任何未列出的 $\\alpha_k$ 都应视为零。所有角度均以弧度为单位。\n\n测试套件：\n- 测试 1 (单一非零频率)：$N=8$，$\\{\\alpha_0=0.0,\\;\\alpha_1=2.0\\}$。\n- 测试 2 (包含奈奎斯特项的混合)：$N=16$，$\\{\\alpha_0=1.5,\\;\\alpha_1=1.0,\\;\\alpha_2=0.5,\\;\\alpha_8=0.25\\}$。\n- 测试 3 (纯常数核)：$N=10$，$\\{\\alpha_0=3.0\\}$。\n\n你的程序应：\n- 对每个测试用例，使用上面的精确核公式构造 $K$。\n- 通过对称特征求解器计算 $K$ 的数值特征值。\n- 将 $K$ 的第一行的复数 DFT 作为解析特征值进行计算。\n- 通过计算每个测试用例的最大绝对差值，比较排序后的数值特征值和解析特征值列表。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目是对应测试用例的最大绝对差值（一个浮点数），四舍五入到十位小数，并按上述测试的顺序排列。例如，如果差值在舍入误差范围内为数值零，则形如“[0.0,0.0,0.0]”的输出是可以接受的。不应打印任何额外文本。", "solution": "我们从神经正切核 (NTK) 的定义开始。对于模型 $f(x;\\theta)$，在初始化 $\\theta_0$ 处的 NTK 为\n$$\nk(x,x') \\;=\\; \\nabla_{\\theta} f(x;\\theta_0)^{\\top}\\nabla_{\\theta} f(x';\\theta_0).\n$$\n当模型对其参数是线性的，且具有固定的特征映射时，关于参数的梯度与 $\\theta_0$ 无关，且等于特征向量。在本例中，模型为\n$$\nf(x;\\theta) \\;=\\; \\sqrt{\\alpha_0}\\,a_0\\,\\phi_0(x) \\;+\\; \\sum_{k=1}^{M} \\sqrt{\\alpha_k}\\,\\Big(a_k^{(c)} \\,\\phi_k^{(c)}(x) \\;+\\; a_k^{(s)} \\,\\phi_k^{(s)}(x)\\Big).\n$$\n关于参数的梯度为\n$$\n\\nabla_{\\theta} f(x;\\theta) \\;=\\; \\big[\\sqrt{\\alpha_0}\\,\\phi_0(x),\\;\\{\\sqrt{\\alpha_k}\\,\\phi_k^{(c)}(x),\\sqrt{\\alpha_k}\\,\\phi_k^{(s)}(x)\\}_{k=1}^M\\big]^{\\top}.\n$$\n因此，NTK 是这些梯度的内积：\n$$\n\\begin{aligned}\nk(x,x') \n&= \\alpha_0 \\,\\phi_0(x)\\phi_0(x') \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\Big(\\phi_k^{(c)}(x)\\phi_k^{(c)}(x') \\;+\\; \\phi_k^{(s)}(x)\\phi_k^{(s)}(x')\\Big) \\\\\n&= \\alpha_0 \\cdot 1 \\cdot 1 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\Big(\\cos(2\\pi k x)\\cos(2\\pi k x') \\;+\\; \\sin(2\\pi k x)\\sin(2\\pi k x')\\Big).\n\\end{aligned}\n$$\n使用三角恒等式 $\\cos(A)\\cos(B)+\\sin(A)\\sin(B)=\\cos(A-B)$，我们得到\n$$\nk(x,x') \\;=\\; \\alpha_0 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\cos\\!\\big(2\\pi k (x - x')\\big).\n$$\n所有角度均以弧度为单位。该核在圆上是平移不变的，仅依赖于差值 $x-x'$ 模 $1$。\n\n现在考虑离散点集 $x_n = n/N$，对于 $n\\in\\{0,\\dots,N-1\\}$。通过 $K_{n,m} = k(x_n,x_m)$ 定义核矩阵 $K \\in \\mathbb{R}^{N\\times N}$。因为 $k$ 仅依赖于 $x_n - x_m$ 模 $1$，且 $x_n - x_m = (n-m)/N$ 模 $1$，所以 $K$ 仅依赖于 $(n-m)\\bmod N$。因此，K 是一个实对称循环矩阵。循环矩阵的一个核心性质是它们可以被复数离散傅里叶变换 (DFT) 矩阵对角化。令 $F$ 为 $N\\times N$ DFT 矩阵，其元素为 $F_{r,m} = \\exp\\!\\big(-2\\pi i r m / N\\big)$，对于 $r,m\\in\\{0,\\dots,N-1\\}$。那么，\n$$\nK \\;=\\; \\frac{1}{N} F^{\\ast} \\,\\mathrm{diag}(\\lambda_0,\\dots,\\lambda_{N-1})\\, F,\n$$\n其中 $(\\lambda_r)_{r=0}^{N-1}$ 是 $K$ 的特征值，由 $K$ 的第一行的 DFT 给出。如果第一行是 $(c_0,\\dots,c_{N-1})$，那么\n$$\n\\lambda_r \\;=\\; \\sum_{m=0}^{N-1} c_m \\, e^{-2\\pi i r m / N}, \\quad r=0,\\dots,N-1.\n$$\n因为 $K$ 是实对称的，并且 $c_m$ 是实数且满足 $c_m=c_{N-m}$，所以特征值 $\\lambda_r$ 是实数且非负。相应的特征向量是复傅里叶模式，在实特征分解中，它们表现为跨越相同特征空间的余弦和正弦对。\n\n算法计划：\n1) 使用给定的 $N$ 和 $\\{\\alpha_k\\}$，通过以下公式构造核矩阵 $K$：\n$$\nK_{n,m} \\;=\\; \\alpha_0 \\;+\\; \\sum_{k\\ge 1} \\alpha_k \\,\\cos\\!\\Big(2\\pi k \\,\\frac{n-m}{N}\\Big).\n$$\n2) 使用对称特征求解器计算数值特征分解以获得特征值。按升序对它们进行排序。\n3) 通过对 $K$ 的第一行进行复数 DFT 来计算解析特征值。提取实部（数值上的虚部是由于浮点舍入引起的），并按升序排序。\n4) 对于每个测试用例，报告排序后的数值特征值和排序后的解析特征值之间的最大绝对差值。一个小的差值可以验证特征分解以及特征函数和傅里叶模式之间的关系。\n\n测试套件详情：\n- 测试 1：$N=8$，$\\alpha_0=0.0$，$\\alpha_1=2.0$。该核是秩为 2 的，在频率 $1$ 和 $7$ 处有非零特征值；每个预期为 $N\\alpha_1/2 = 8$。\n- 测试 2：$N=16$，$\\alpha_0=1.5$，$\\alpha_1=1.0$，$\\alpha_2=0.5$，$\\alpha_8=0.25$。预期的特征值在 $r=0$ 处为 $N\\alpha_0=24$，在 $r=1,15$ 处为 $N\\alpha_1/2=8$，在 $r=2,14$ 处为 $N\\alpha_2/2=4$，在奈奎斯特频率 $r=8$ 处为 $N\\alpha_8=4$。所有其他特征值为零。\n- 测试 3：$N=10$，$\\alpha_0=3.0$。这是一个纯常数核，在 $r=0$ 处有一个单一的非零特征值 $N\\alpha_0=30$，其他地方都为零。\n\n该程序实现这些步骤，并打印一行包含三个浮点数的列表：测试 1-3 的最大绝对特征值差值，四舍五入到十位小数。这通过循环结构和 DFT，将均匀间隔点上的 NTK 的特征分解与傅里叶模式直接联系起来。", "answer": "```python\nimport numpy as np\n\ndef build_kernel_and_first_row(N: int, alpha: dict) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Build the NTK kernel matrix K and its first row c for evenly spaced points x_n = n/N on [0,1).\n    Kernel: k(x, x') = alpha_0 + sum_{k>=1} alpha_k cos(2*pi*k*(x - x')).\n    Angles are in radians.\n\n    Parameters:\n        N: number of evenly spaced points.\n        alpha: dict mapping frequency k to alpha_k (nonnegative). Unspecified k treated as 0.\n\n    Returns:\n        K: NxN kernel matrix.\n        c: length-N first row of K.\n    \"\"\"\n    n = np.arange(N)\n    # Differences tau = n - m mod N; shape (N, N)\n    tau = (n[:, None] - n[None, :]) % N\n    tau = tau.astype(float)\n\n    # Start with alpha_0 term\n    a0 = alpha.get(0, 0.0)\n    K = np.full((N, N), fill_value=a0, dtype=float)\n\n    # Add cosine terms for k >= 1\n    for k, ak in alpha.items():\n        if k == 0 or ak == 0.0:\n            continue\n        # cos(2*pi*k*(n-m)/N)\n        K += ak * np.cos(2.0 * np.pi * k * tau / N)\n\n    # First row c_m = K_{0, m}\n    c = K[0, :].copy()\n    return K, c\n\ndef numerical_sorted_eigenvalues(K: np.ndarray) -> np.ndarray:\n    \"\"\"Compute and return sorted eigenvalues of a real symmetric matrix K.\"\"\"\n    w = np.linalg.eigh(K)[0]\n    return np.sort(w)\n\ndef analytical_sorted_eigenvalues_from_first_row(c: np.ndarray) -> np.ndarray:\n    \"\"\"\n    For a circulant matrix with first row c, eigenvalues are the DFT of c.\n    Return sorted real parts of these eigenvalues.\n    \"\"\"\n    lam = np.fft.fft(c)\n    lam_real = np.real_if_close(lam, tol=1000)  # strip negligible imaginary parts\n    lam_real = lam_real.astype(float)\n    return np.sort(lam_real)\n\ndef max_abs_diff(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Maximum absolute difference between two arrays of same shape.\"\"\"\n    return float(np.max(np.abs(a - b)))\n\ndef run_test_cases():\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        # Test 1: N=8, alpha_0=0.0, alpha_1=2.0\n        {\"N\": 8, \"alpha\": {0: 0.0, 1: 2.0}},\n        # Test 2: N=16, alpha_0=1.5, alpha_1=1.0, alpha_2=0.5, alpha_8=0.25\n        {\"N\": 16, \"alpha\": {0: 1.5, 1: 1.0, 2: 0.5, 8: 0.25}},\n        # Test 3: N=10, alpha_0=3.0\n        {\"N\": 10, \"alpha\": {0: 3.0}},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        alpha = case[\"alpha\"]\n        K, c = build_kernel_and_first_row(N, alpha)\n\n        # Numerical eigenvalues (sorted)\n        w_num = numerical_sorted_eigenvalues(K)\n\n        # Analytical eigenvalues via DFT of first row (sorted)\n        w_ana = analytical_sorted_eigenvalues_from_first_row(c)\n\n        # Compute maximum absolute difference\n        diff = max_abs_diff(w_num, w_ana)\n        results.append(diff)\n\n    # Print results as a single line with specified format: list of floats rounded to 10 decimal places.\n    formatted = \"[\" + \",\".join(f\"{val:.10f}\" for val in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    run_test_cases()\n```", "id": "3159099"}, {"introduction": "神经网络学习不同函数的快慢有何不同？本次实践将证明，在 NTK 动态下，学习速度由核矩阵的特征值决定。通过将目标函数与不同的特征向量对齐来模拟梯度下降，你将亲眼观察到与较大特征值相关的分量被学得更快 [@problem_id:3159036]。这为你理解为何某些函数对于给定架构更容易学习提供了关键的直觉。", "problem": "给定一个小型神经网络切向核 (NTK) Gram 矩阵，要求你分析在训练集上进行相应核回归时的梯度下降动力学。神经网络切向核 (NTK) Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 是对称半正定的，并作为作用于 $n$ 个训练样本预测向量上的线性算子。任务是通过显式计算来证明，当目标向量与 $K$ 的最大特征值对应的特征向量对齐时，核梯度下降的误差会在几步内迅速衰减。\n\n基本原理和设置：\n- 训练集上的核回归训练损失是平方误差 $L(f) = \\tfrac{1}{2} \\lVert f - y \\rVert_2^2$，其中 $f \\in \\mathbb{R}^n$ 是模型在 $n$ 个训练点上的预测向量，$y \\in \\mathbb{R}^n$ 是目标向量。\n- 学习率为 $\\eta$、NTK Gram 矩阵为 $K$ 的核梯度下降（在函数空间中）执行以下更新\n$$\nf_{t+1} = f_t - \\eta \\, K \\,(f_t - y)\n$$\n从 $f_0 = 0$ 开始。这是对 $L(f)$ 应用梯度下降的结果，因为 $\\nabla_f L = f - y$，并且 NTK 在函数空间中通过 $K$ 对更新进行预处理。\n- 设特征分解为 $K = U \\Lambda U^\\top$，其中 $U = [u_1, \\dots, u_n]$ 是标准正交矩阵，$\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ 且 $\\lambda_1 \\ge \\cdots \\ge \\lambda_n \\ge 0$。任何目标 $y$ 都可以分解为 $y = \\sum_{i=1}^n c_i u_i$，其中 $c_i = u_i^\\top y$。\n\n你的程序必须：\n1. 使用固定的 NTK Gram 矩阵\n$$\nK = \\begin{bmatrix}\n2  1  0 \\\\\n1  2  1 \\\\\n0  1  2\n\\end{bmatrix}\n$$\n2. 计算其特征分解，以获得特征值和相应的单位特征向量 $u_1, u_2, u_3$，其中 $u_1$ 与最大特征值相关联。\n3. 对于每个测试用例，构造指定的目标 $y \\in \\mathbb{R}^3$，使用学习率 $\\eta$ 从 $f_0 = 0$ 开始运行核梯度下降 $T$ 步，并计算训练损失\n$$\nL_T = \\tfrac{1}{2} \\lVert f_T - y \\rVert_2^2\n$$\n4. 将所有测试用例的结果输出为单行，包含一个 Python 风格的列表，其中每个浮点数四舍五入到六位小数。\n\n测试套件：\n- 案例 1：$y = u_1$（单位向量），$\\eta = 0.25$，$T = 3$。\n- 案例 2：$y = u_3$（单位向量），$\\eta = 0.25$，$T = 3$。\n- 案例 3：$y = \\mathrm{normalize}(0.8\\, u_1 + 0.2\\, u_3)$ 归一化为单位范数，$\\eta = 0.25$，$T = 3$。此处 $0.8$ 和 $0.2$ 是实数系数，$\\mathrm{normalize}(\\cdot)$ 将向量缩放至单位欧几里得范数。\n- 案例 4：$y = u_1$，$\\eta = 0.1$，$T = 3$。\n- 案例 5：$y = u_1$，$\\eta = 0.25$，$T = 1$。\n\n角度单位不适用。没有物理单位。所有输出都必须是浮点数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是对应测试用例的 $L_T$ 值，四舍五入到六位小数。", "solution": "该问题是有效的，因为它在核方法理论中有科学依据，其定义和数据完整一致，问题良构，且表述客观。我们将继续提供完整解答。\n\n该问题要求我们分析特定神经网络切向核 (NTK) Gram 矩阵 $K$ 的核梯度下降动力学。我们将首先推导训练损失动力学的一般解析表达式，然后将其应用于给定的矩阵和测试用例。这将展示收敛速度如何依赖于目标向量与核矩阵特征向量的对齐程度。\n\n设 $f_t \\in \\mathbb{R}^n$ 为模型在第 $t$ 步时对 $n$ 个训练点的预测向量，设 $y \\in \\mathbb{R}^n$ 为目标向量。训练损失为均方误差 $L(f) = \\frac{1}{2} \\lVert f - y \\rVert_2^2$。\n学习率为 $\\eta$、NTK Gram 矩阵为 $K$ 的核梯度下降更新规则如下：\n$$\nf_{t+1} = f_t - \\eta K (f_t - y)\n$$\n我们从初始条件 $f_0 = 0$ 开始。设第 $t$ 步的误差向量为 $e_t = f_t - y$。初始误差为 $e_0 = f_0 - y = -y$。\n我们可以通过从两边减去 $y$ 来用误差向量表示更新规则：\n$$\nf_{t+1} - y = (f_t - y) - \\eta K (f_t - y)\n$$\n$$\ne_{t+1} = e_t - \\eta K e_t = (I - \\eta K) e_t\n$$\n其中 $I$ 是 $n \\times n$ 的单位矩阵。这是一个关于误差向量的线性动力系统。通过展开递归，我们得到第 $t$ 步的误差：\n$$\ne_t = (I - \\eta K)^t e_0 = - (I - \\eta K)^t y\n$$\n第 $T$ 步的训练损失为 $L_T = \\frac{1}{2} \\lVert e_T \\rVert_2^2$。代入 $e_T$ 的表达式：\n$$\nL_T = \\frac{1}{2} \\left\\lVert - (I - \\eta K)^T y \\right\\rVert_2^2 = \\frac{1}{2} \\left\\lVert (I - \\eta K)^T y \\right\\rVert_2^2\n$$\n为分析此表达式，我们使用对称矩阵 $K$ 的特征分解，即 $K = U \\Lambda U^\\top$。这里，$U$ 是一个标准正交矩阵，其列是特征向量 $u_1, u_2, \\dots, u_n$，$\\Lambda$ 是由相应特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$ 构成的对角矩阵。\n算子 $(I - \\eta K)$ 可以在相同的基下对角化：\n$$\nI - \\eta K = U I U^\\top - \\eta U \\Lambda U^\\top = U (I - \\eta \\Lambda) U^\\top\n$$\n将其升至 $T$ 次幂：\n$$\n(I - \\eta K)^T = U (I - \\eta \\Lambda)^T U^\\top\n$$\n将此代入 $L_T$ 的表达式：\n$$\nL_T = \\frac{1}{2} \\left\\lVert U (I - \\eta \\Lambda)^T U^\\top y \\right\\rVert_2^2\n$$\n由于 $U$ 是标准正交矩阵，它保持欧几里得范数，即对于任何向量 $v$，都有 $\\lVert Uv \\rVert_2 = \\lVert v \\rVert_2$。因此：\n$$\nL_T = \\frac{1}{2} \\left\\lVert (I - \\eta \\Lambda)^T U^\\top y \\right\\rVert_2^2\n$$\n向量 $U^\\top y$ 表示 $y$ 在特征向量基中的坐标。其第 $i$ 个分量是 $c_i = u_i^\\top y$。矩阵 $(I - \\eta \\Lambda)^T$ 是一个对角矩阵，其对角线元素为 $(1 - \\eta \\lambda_i)^T$。将此对角矩阵应用于向量 $U^\\top y$ 会将其第 $i$ 个分量缩放 $(1 - \\eta \\lambda_i)^T$ 倍。所得向量的范数平方是其各分量平方之和：\n$$\nL_T = \\frac{1}{2} \\sum_{i=1}^n \\left( c_i (1 - \\eta \\lambda_i)^T \\right)^2 = \\frac{1}{2} \\sum_{i=1}^n c_i^2 (1 - \\eta \\lambda_i)^{2T}\n$$\n这个最终表达式揭示了核心原理：目标 $y$ 的每个特征分量对损失的贡献呈指数级衰减。第 $i$ 个分量的衰减率由因子 $(1 - \\eta \\lambda_i)^2$ 决定。对于一个稳定的学习率，其中 $0  \\eta \\lambda_i  2$，该因子小于 1。较大的特征值 $\\lambda_i$ 会导致较小的因子 $(1 - \\eta \\lambda_i)^2$，从而使该分量收敛得更快。\n\n现在，我们对给定的问题进行显式计算。\nNTK Gram 矩阵为：\n$$\nK = \\begin{bmatrix} 2  1  0 \\\\ 1  2  1 \\\\ 0  1  2 \\end{bmatrix}\n$$\n特征值是特征方程 $\\det(K - \\lambda I) = 0$ 的根，即 $\\lambda_1 = 2 + \\sqrt{2}$、$\\lambda_2 = 2$ 和 $\\lambda_3 = 2 - \\sqrt{2}$。相应的标准正交特征向量为：\n$$\nu_1 = \\begin{pmatrix} 1/2 \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}, \\quad u_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\end{pmatrix}, \\quad u_3 = \\begin{pmatrix} 1/2 \\\\ -\\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}\n$$\n我们现在为每个测试用例计算损失 $L_T$。\n\n案例 1：$y = u_1$，$\\eta = 0.25$，$T = 3$。\n目标 $y$ 与第一个特征向量 $u_1$ 对齐。系数为 $c_1 = u_1^\\top y = u_1^\\top u_1 = 1$，$c_2 = 0$，$c_3 = 0$。\n损失完全由第一个特征分量决定：\n$L_3 = \\frac{1}{2} c_1^2 (1 - \\eta \\lambda_1)^{2T} = \\frac{1}{2} (1 - 0.25 (2+\\sqrt{2}))^6 = \\frac{1}{2} (0.5 - \\frac{\\sqrt{2}}{4})^6 \\approx 0.000004$。\n\n案例 2：$y = u_3$，$\\eta = 0.25$，$T = 3$。\n目标 $y$ 与第三个特征向量 $u_3$ 对齐。系数为 $c_1 = 0$，$c_2 = 0$，$c_3 = 1$。\n损失完全由第三个特征分量决定：\n$L_3 = \\frac{1}{2} c_3^2 (1 - \\eta \\lambda_3)^{2T} = \\frac{1}{2} (1 - 0.25 (2-\\sqrt{2}))^6 = \\frac{1}{2} (0.5 + \\frac{\\sqrt{2}}{4})^6 \\approx 0.192317$。\n与案例 1 相比，收敛速度要慢得多，因为 $\\lambda_3$ 很小。\n\n案例 3：$y = \\mathrm{normalize}(0.8 u_1 + 0.2 u_3)$，$\\eta = 0.25$，$T = 3$。\n设 $v = 0.8 u_1 + 0.2 u_3$。则 $y = v / \\lVert v \\rVert_2$。由于 $u_1$ 和 $u_3$ 是标准正交的，$\\lVert v \\rVert_2 = \\sqrt{0.8^2 + 0.2^2} = \\sqrt{0.68}$。\n系数为 $c_1 = u_1^\\top y = 0.8/\\sqrt{0.68}$ 和 $c_3 = u_3^\\top y = 0.2/\\sqrt{0.68}$，其中 $c_2 = 0$。\n所以，$c_1^2 = 0.64/0.68 = 16/17$ 且 $c_3^2 = 0.04/0.68 = 1/17$。\n损失是第一和第三分量贡献的加权和：\n$L_3 = \\frac{1}{2} [c_1^2 (1 - \\eta \\lambda_1)^6 + c_3^2 (1 - \\eta \\lambda_3)^6] = \\frac{1}{2} [ \\frac{16}{17} (0.5 - \\frac{\\sqrt{2}}{4})^6 + \\frac{1}{17} (0.5 + \\frac{\\sqrt{2}}{4})^6 ] \\approx 0.011317$。\n最终误差由与 $u_3$ 相关联的缓慢衰减分量主导。\n\n案例 4：$y = u_1$，$\\eta = 0.1$，$T = 3$。\n这与案例 1 类似，但学习率较小。$c_1 = 1$，$c_2 = 0$，$c_3 = 0$。\n$L_3 = \\frac{1}{2} (1 - 0.1 \\lambda_1)^6 = \\frac{1}{2} (1 - 0.1(2+\\sqrt{2}))^6 = \\frac{1}{2} (0.8 - \\frac{\\sqrt{2}}{10})^6 \\approx 0.040727$。\n较小的学习率导致衰减因子 $(1 - \\eta \\lambda_1)$ 更接近 1，因此与案例 1 相比收敛更慢。\n\n案例 5：$y = u_1$，$\\eta = 0.25$，$T = 1$。\n这与案例 1 类似，但步数更少。$c_1 = 1$，$c_2 = 0$，$c_3 = 0$。\n$L_1 = \\frac{1}{2} (1 - \\eta \\lambda_1)^{2 \\times 1} = \\frac{1}{2} (1 - 0.25(2+\\sqrt{2}))^2 = \\frac{1}{2} (0.5 - \\frac{\\sqrt{2}}{4})^2 \\approx 0.010723$。\n仅用一步，与案例 1 相比，误差衰减的时间更少。\n\n结果证实了理论分析：在核梯度下降中，目标中与 NTK Gram 矩阵较大特征值对应的特征向量对齐的分量被学习得快得多。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training loss for kernel gradient descent under several scenarios.\n    \"\"\"\n    # 1. Define the fixed NTK Gram matrix K.\n    K = np.array([\n        [2.0, 1.0, 0.0],\n        [1.0, 2.0, 1.0],\n        [0.0, 1.0, 2.0]\n    ])\n\n    # 2. Compute the eigendecomposition of K.\n    # np.linalg.eigh returns eigenvalues in ascending order. We sort them descending.\n    eigenvalues, eigenvectors = np.linalg.eigh(K)\n    sort_indices = np.argsort(eigenvalues)[::-1]\n    # lambdas = eigenvalues[sort_indices] # Not used directly in simulation\n    U = eigenvectors[:, sort_indices]\n\n    # Extract the unit eigenvectors u1, u2, u3.\n    u1 = U[:, 0]\n    # u2 = U[:, 1] # Not used in test cases\n    u3 = U[:, 2]\n\n    # 3. Define the test suite.\n    # Each case is a tuple: (y_description, eta, T)\n    # y_description is a tuple: (type, spec)\n    test_cases = [\n        # Case 1: y = u1, eta = 0.25, T = 3\n        (('eigenvector', u1), 0.25, 3),\n        # Case 2: y = u3, eta = 0.25, T = 3\n        (('eigenvector', u3), 0.25, 3),\n        # Case 3: y = normalize(0.8*u1 + 0.2*u3), eta = 0.25, T = 3\n        (('combo', (0.8, u1, 0.2, u3)), 0.25, 3),\n        # Case 4: y = u1, eta = 0.1, T = 3\n        (('eigenvector', u1), 0.1, 3),\n        # Case 5: y = u1, eta = 0.25, T = 1\n        (('eigenvector', u1), 0.25, 1),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        y_desc, eta, T = case\n        y_type, y_spec = y_desc\n\n        # Construct the target vector y for the current case.\n        if y_type == 'eigenvector':\n            y = y_spec\n        elif y_type == 'combo':\n            c1, v1, c2, v2 = y_spec\n            v = c1 * v1 + c2 * v2\n            y = v / np.linalg.norm(v)\n        else:\n            raise ValueError(\"Unknown y_type\")\n        \n        # Initialize predictions f_0 = 0.\n        f = np.zeros_like(y)\n\n        # Run kernel gradient descent for T steps.\n        for _ in range(T):\n            gradient_term = f - y\n            update = K @ gradient_term\n            f = f - eta * update\n        \n        # This is f_T. Now compute the final loss L_T.\n        loss = 0.5 * np.linalg.norm(f - y)**2\n        results.append(loss)\n\n    # 4. Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3159036"}, {"introduction": "NTK 假设核在训练中保持不变，从而提供了一个强大的分析工具，但这只在无限宽网络中才严格成立。本次实践将通过比较一个真实的、有限宽度网络的训练轨迹与其线性化（NTK）对应物的轨迹，来检验这一近似的有效性。你将量化两者之间的偏差，并看到网络宽度和学习率等因素如何影响“特征学习”的程度 [@problem_id:3159054]——即网络调整其内部表示的过程，而这是固定核模型无法捕捉的现象。", "problem": "要求您实现并比较两种训练动态，这两种动态用于一个固定的回归任务上的双层修正线性单元 (ReLU) 神经网络，并量化它们之间的偏差。这两种动态是：(a) 对实际的有限宽度网络进行全批量梯度下降（允许特征改变），以及 (b) 训练通过网络在其随机初始化周围进行一阶泰勒展开得到的线性化模型（这会保持特征固定，并对应于在神经正切核 (NTK) 近似下的训练）。您的目标是构建一个案例，其中有限宽度训练因特征学习而偏离 NTK 的预测，并量化该偏差。\n\n定义和设置：\n- 考虑一个宽度为 $w$、具有标量输出的双层全连接 ReLU 网络，其对输入 $\\mathbf{x} \\in \\mathbb{R}^d$ 的定义为\n$$\nf(\\mathbf{x}; W, \\mathbf{a}) \\;=\\; \\frac{1}{\\sqrt{w}} \\sum_{j=1}^{w} a_j \\, \\max\\{0, \\langle \\mathbf{w}_j, \\mathbf{x} \\rangle\\}\n$$\n其中 $W \\in \\mathbb{R}^{w \\times d}$ 的行向量为 $\\mathbf{w}_j \\in \\mathbb{R}^d$，且 $\\mathbf{a} \\in \\mathbb{R}^{w}$。\n- 使用固定的随机种子 $2024$ 通过独立的标准正态分布抽样来初始化参数：每个 $W_{jk} \\sim \\mathcal{N}(0,1)$ 和 $a_j \\sim \\mathcal{N}(0,1)$。\n- 使用平均平方损失\n$$\n\\mathcal{L}(W,\\mathbf{a}) \\;=\\; \\frac{1}{2n} \\sum_{i=1}^{n} \\big(f(\\mathbf{x}_i; W,\\mathbf{a}) - y_i \\big)^2\n$$\n其中 $(\\mathbf{x}_i,y_i)$ 是训练样本。\n- 对全网络使用学习率为 $\\eta$ 和 $T$ 步的批量梯度下降：\n$$\n(W^{t+1}, \\mathbf{a}^{t+1}) \\;=\\; (W^{t}, \\mathbf{a}^{t}) - \\eta \\, \\nabla \\mathcal{L}(W^{t}, \\mathbf{a}^{t})\n$$\n从初始化给出的 $(W^0,\\mathbf{a}^0)$ 开始。\n\n线性化 (NTK) 模型：\n- 令 $\\boldsymbol{\\theta}$ 表示所有参数堆叠成的一个单一向量，令 $\\mathbf{f}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^n$ 表示网络在 $n$ 个训练输入上的输出向量。考虑围绕初始化 $\\boldsymbol{\\theta}^0$ 的一阶泰勒展开：\n$$\n\\mathbf{f}(\\boldsymbol{\\theta}) \\;\\approx\\; \\mathbf{f}(\\boldsymbol{\\theta}^0) \\;+\\; J_0 \\, (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0)\n$$\n其中 $J_0 \\in \\mathbb{R}^{n \\times p}$ 是在初始化时，训练输出相对于参数的雅可比矩阵，$p$ 是参数的数量。使用相同的学习率 $\\eta$ 和步数 $T$，通过批量梯度下降训练这个线性化模型，并在训练集上使用相同的平均平方损失。对于测试输入，使用在初始化时对应的雅可比矩阵来获得预测\n$$\n\\mathbf{f}_{\\text{test}}(\\boldsymbol{\\theta}) \\;\\approx\\; \\mathbf{f}_{\\text{test}}(\\boldsymbol{\\theta}^0) \\;+\\; J^{\\text{test}}_0 \\, (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0).\n$$\n\n数据生成：\n- 输入维度为 $d = 2$。使用随机种子 $123$，通过从标准正态分布中独立抽取每个分量，生成 $n = 12$ 个训练输入 $\\{\\mathbf{x}_i\\}_{i=1}^{n}$。使用相同的规则和随机种子 $456$ 生成 $m = 12$ 个测试输入。三角函数的所有随机角度都应理解为弧度。\n- 定义固定向量 $\\mathbf{u} = (1.0, -0.5)$ 和 $\\mathbf{v} = (0.3, 1.1)$。通过以下方式定义目标函数\n$$\ny(\\mathbf{x}) \\;=\\; \\sin(\\langle \\mathbf{u}, \\mathbf{x} \\rangle) \\;+\\; 0.5 \\, \\cos(\\langle \\mathbf{v}, \\mathbf{x} \\rangle)\n$$\n为训练集计算 $y_i = y(\\mathbf{x}_i)$，为测试集计算 $y^{\\text{test}}_j = y(\\mathbf{z}_j)$，其中 $\\{\\mathbf{z}_j\\}_{j=1}^{m}$ 是测试输入。\n\n偏差度量：\n- 在为相同的 $(w,\\eta,T)$ 训练了全网络和线性化模型之后，计算它们在 $n$ 个训练输入和 $m$ 个测试输入的并集上的预测。令 $\\widehat{f}_{\\text{full}}$ 和 $\\widehat{f}_{\\text{lin}}$ 表示这些预测。通过均方偏差来量化偏差\n$$\nD \\;=\\; \\frac{1}{n+m} \\sum_{i=1}^{n+m} \\Big(\\widehat{f}_{\\text{full},i} - \\widehat{f}_{\\text{lin},i}\\Big)^2\n$$\n\n测试套件：\n为以下参数集 $(w,\\eta,T)$ 运行实验：\n- 案例 $1$：$(w,\\eta,T) = (10, 0.1, 200)$。\n- 案例 $2$：$(w,\\eta,T) = (200, 0.1, 200)$。\n- 案例 $3$：$(w,\\eta,T) = (10, 0.5, 200)$。\n- 案例 $4$（边界条件）：$(w,\\eta,T) = (10, 0.1, 0)$。\n\n程序行为要求：\n- 您的程序必须完全按照规定实现这两种训练过程，对全网络模型和线性化模型使用相同的初始化 $(W^0,\\mathbf{a}^0)$，并在每个案例中使用相同的学习率 $\\eta$ 和步数 $T$。\n- 计算每个案例的偏差 $D$。\n- 最终输出格式：打印单行，其中包含一个 Python 风格的列表，内含上述案例顺序的四个偏差值，四舍五入到 $6$ 位小数，例如\n\"[0.123456,0.000789,0.234567,0.000000]\"。", "solution": "用户的请求是深度学习理论领域中一个有效且定义明确的问题。它要求对双层 ReLU 网络的训练动态及其一阶泰勒近似（线性化或神经正切核模型）进行数值比较。该问题具有科学依据，形式上明确，并且计算上可行。我们将提供一个完整的解决方案。\n\n问题的核心是为一个神经网络在回归任务上实现两种不同的训练过程，并量化它们最终预测的差异。\n\n**1. 模型和设置**\n\n该网络是一个宽度为 $w$、具有标量输出的双层全连接 ReLU 网络，其对输入 $\\mathbf{x} \\in \\mathbb{R}^d$ 的定义为：\n$$\nf(\\mathbf{x}; W, \\mathbf{a}) = \\frac{1}{\\sqrt{w}} \\sum_{j=1}^{w} a_j \\, \\max\\{0, \\langle \\mathbf{w}_j, \\mathbf{x} \\rangle\\}\n$$\n其中 $W \\in \\mathbb{R}^{w \\times d}$ 和 $\\mathbf{a} \\in \\mathbb{R}^w$ 是参数。训练目标是在训练集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ 上最小化平均平方损失：\n$$\n\\mathcal{L}(W, \\mathbf{a}) = \\frac{1}{2n} \\sum_{i=1}^{n} \\big(f(\\mathbf{x}_i; W, \\mathbf{a}) - y_i \\big)^2\n$$\n\n**2. 全网络梯度下降训练**\n\n第一种训练动态是在参数 $(W, \\mathbf{a})$ 上进行标准的批量梯度下降。参数通过学习率 $\\eta$ 迭代更新 $T$ 步：\n$$\n(W^{t+1}, \\mathbf{a}^{t+1}) = (W^{t}, \\mathbf{a}^{t}) - \\eta \\, \\nabla \\mathcal{L}(W^{t}, \\mathbf{a}^{t})\n$$\n梯度是使用链式法则计算的。令 $r_i = f(\\mathbf{x}_i; W, \\mathbf{a}) - y_i$ 为样本 $i$ 的残差，令 $\\sigma(z)=\\max\\{0,z\\}$ 为 ReLU 激活函数，其导数（次梯度）取为 $\\sigma'(z) = \\mathbb{1}_{z  0}$。损失对参数的梯度如下：\n\n- **关于输出层权重 $\\mathbf{a}$ 的梯度**：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a_j} = \\frac{1}{n} \\sum_{i=1}^{n} r_i \\frac{\\partial f(\\mathbf{x}_i)}{\\partial a_j} = \\frac{1}{n \\sqrt{w}} \\sum_{i=1}^{n} r_i \\, \\sigma(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)\n$$\n以矩阵形式，如果 $\\mathbf{f} \\in \\mathbb{R}^n$ 是预测向量，$\\mathbf{r} = \\mathbf{f} - \\mathbf{y}$ 是残差向量，且 $A \\in \\mathbb{R}^{n \\times w}$ 是激活后值的矩阵，其中 $A_{ij} = \\sigma(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)$，则 $\\nabla_{\\mathbf{a}}\\mathcal{L} = \\frac{1}{n \\sqrt{w}} A^T \\mathbf{r}$。\n\n- **关于输入层权重 $W$ 的梯度**：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W_{jk}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i \\frac{\\partial f(\\mathbf{x}_i)}{\\partial W_{jk}} = \\frac{1}{n \\sqrt{w}} \\sum_{i=1}^{n} r_i \\, a_j \\, \\sigma'(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle) x_{ik}\n$$\n以矩阵形式，令 $S \\in \\mathbb{R}^{n \\times w}$ 为矩阵，其中 $S_{ij} = \\sigma'(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)$。梯度矩阵 $\\nabla_W \\mathcal{L} \\in \\mathbb{R}^{w \\times d}$ 可以通过首先计算损失相对于激活前值 $H_{ij} = \\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle$ 的梯度来得到。这个中间梯度矩阵 $G_H \\in \\mathbb{R}^{n \\times w}$ 的元素为 $(G_H)_{ij} = \\frac{\\partial \\mathcal{L}}{\\partial H_{ij}} = \\frac{1}{n} r_i \\frac{a_j}{\\sqrt{w}} S_{ij}$。最终的梯度是 $\\nabla_W \\mathcal{L} = G_H^T X$，其中 $X \\in \\mathbb{R}^{n \\times d}$ 是训练数据矩阵。\n\n这些更新执行 $T$ 步以获得最终参数 $(W^T, \\mathbf{a}^T)$，用于在训练和测试数据的并集上做出预测 $\\widehat{f}_{\\text{full}}$。\n\n**3. 线性化 (NTK) 模型训练**\n\n第二种动态是训练模型的线性化版本。网络输出函数 $\\mathbf{f}(\\boldsymbol{\\theta})$（其中 $\\boldsymbol{\\theta}$ 代表所有参数）通过其围绕初始参数 $\\boldsymbol{\\theta}^0 = (W^0, \\mathbf{a}^0)$ 的一阶泰勒级数来近似：\n$$\n\\mathbf{f}_{\\text{lin}}(\\boldsymbol{\\theta}) = \\mathbf{f}(\\boldsymbol{\\theta}^0) + J_0 (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0)\n$$\n其中 $J_0 = \\nabla_{\\boldsymbol{\\theta}} \\mathbf{f}(\\boldsymbol{\\theta}^0)$ 是网络输出相对于参数的雅可比矩阵，在初始化时计算。用梯度下降法训练这个模型，会使预测本身的动态得以简化。训练集预测 $\\mathbf{f}_{\\text{lin,train}}^t$ 的更新为：\n$$\n\\mathbf{f}_{\\text{lin,train}}^{t+1} = \\mathbf{f}_{\\text{lin,train}}^{t} - \\frac{\\eta}{n} K_{\\text{train,train}} (\\mathbf{f}_{\\text{lin,train}}^t - \\mathbf{y}_{\\text{train}})\n$$\n其中 $K_{\\text{train,train}} = J_0^{\\text{train}} (J_0^{\\text{train}})^T$ 是神经正切核 (NTK) 矩阵。任何其他数据集（例如测试集）上的预测会协同演化：\n$$\n\\mathbf{f}_{\\text{lin,test}}^{t+1} = \\mathbf{f}_{\\text{lin,test}}^{t} - \\frac{\\eta}{n} K_{\\text{test,train}} (\\mathbf{f}_{\\text{lin,train}}^t - \\mathbf{y}_{\\text{train}})\n$$\n其中 $(K_{\\text{test,train}})_{ij} = \\langle \\nabla_{\\boldsymbol{\\theta}}f(\\mathbf{z}_i; \\boldsymbol{\\theta}^0), \\nabla_{\\boldsymbol{\\theta}}f(\\mathbf{x}_j; \\boldsymbol{\\theta}^0) \\rangle$。\n\n我们无需构建可能非常大的雅可比矩阵，而是可以直接计算任意两个输入 $\\mathbf{x}, \\mathbf{x}'$ 的核函数 $K(\\mathbf{x}, \\mathbf{x}')$：\n$$\nK(\\mathbf{x}, \\mathbf{x}') = \\underbrace{\\frac{1}{w} \\sum_{k=1}^{w} \\sigma(\\langle\\mathbf{w}_k^0,\\mathbf{x}\\rangle)\\sigma(\\langle\\mathbf{w}_k^0,\\mathbf{x}'\\rangle)}_{\\text{来自 } \\nabla_{\\mathbf{a}}} + \\underbrace{\\frac{\\langle\\mathbf{x},\\mathbf{x}'\\rangle}{w} \\sum_{k=1}^{w} (a_k^0)^2 \\mathbb{1}_{\\langle\\mathbf{w}_k^0,\\mathbf{x}\\rangle  0} \\mathbb{1}_{\\langle\\mathbf{w}_k^0,\\mathbf{x}'\\rangle  0}}_{\\text{来自 } \\nabla_W}\n$$\n这使得通过仅追踪预测向量，就可以高效地模拟线性化模型的动态。\n\n**4. 偏差与实现**\n\n经过 $T$ 步后，我们从两个模型获得在 $n+m$ 个训练和测试输入的组合集上的预测，表示为 $\\widehat{\\mathbf{f}}_{\\text{full}}$ 和 $\\widehat{\\mathbf{f}}_{\\text{lin}}$。偏差通过均方偏差来量化：\n$$\nD = \\frac{1}{n+m} \\sum_{i=1}^{n+m} \\left( \\widehat{f}_{\\text{full},i} - \\widehat{f}_{\\text{lin},i} \\right)^2\n$$\n对于每个测试案例 $(w, \\eta, T)$，实现将遵循以下步骤：\n1. 根据指定的分布和种子生成训练和测试数据。\n2. 使用指定的种子初始化网络参数 $(W^0, \\mathbf{a}^0)$。\n3. 对于全网络，运行梯度下降循环 $T$ 步并计算最终预测。\n4. 对于线性化模型，使用 $(W^0, \\mathbf{a}^0)$ 计算必要的核矩阵，并模拟预测的演化 $T$ 步。\n5. 计算并存储偏差 $D$。\n\n$T=0$ 的情况作为一个健全性检查，此时两个模型都保持在初始状态，因此偏差必须为 $0$。其他案例探讨网络宽度 $w$ 和学习率 $\\eta$ 如何影响 NTK 近似的有效性。我们预期，对于更大的宽度（案例2 vs. 案例1），偏差会更小，而对于更高的学习率（案例3 vs. 案例1），偏差会更大。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares full network training with linearized (NTK) dynamics \n    for a two-layer ReLU network.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (w, eta, T)\n        (10, 0.1, 200),\n        (200, 0.1, 200),\n        (10, 0.5, 200),\n        (10, 0.1, 0),\n    ]\n\n    results = []\n\n    # --- Data Generation ---\n    d = 2\n    n = 12\n    m = 12\n\n    # Generate training data\n    rng_train = np.random.default_rng(123)\n    X_train = rng_train.standard_normal((n, d), dtype=np.float64)\n\n    # Generate test data\n    rng_test = np.random.default_rng(456)\n    X_test = rng_test.standard_normal((m, d), dtype=np.float64)\n\n    # Define target function and compute labels\n    u = np.array([1.0, -0.5], dtype=np.float64)\n    v = np.array([0.3, 1.1], dtype=np.float64)\n\n    def target_function(x_data):\n        return np.sin(x_data @ u) + 0.5 * np.cos(x_data @ v)\n\n    y_train = target_function(X_train)\n    \n    # Combine training and test inputs for final predictions\n    X_all = np.vstack([X_train, X_test])\n\n    for w, eta, T in test_cases:\n        \n        # --- Initialization ---\n        rng_init = np.random.default_rng(2024)\n        W0 = rng_init.standard_normal((w, d), dtype=np.float64)\n        a0 = rng_init.standard_normal(w, dtype=np.float64)\n\n        # --- Full Network Training ---\n        W, a = W0.copy(), a0.copy()\n        for _ in range(T):\n            # Forward pass on training data\n            H_train = X_train @ W.T  # Shape: (n, w)\n            A_train = np.maximum(0, H_train)\n            f_pred = (1 / np.sqrt(w)) * (A_train @ a)\n            residuals = f_pred - y_train\n\n            # Gradient w.r.t. a\n            grad_a = (1 / (n * np.sqrt(w))) * (A_train.T @ residuals)\n\n            # Gradient w.r.t. W\n            S_train = (H_train  0).astype(np.float64)\n            d_loss_d_h = (1 / (n * np.sqrt(w))) * (residuals.reshape(-1, 1) * S_train) * a.reshape(1, -1)\n            grad_W = d_loss_d_h.T @ X_train\n\n            # Update parameters\n            W -= eta * grad_W\n            a -= eta * grad_a\n\n        # Final predictions from the fully trained network\n        def predict_network(x_data, W_final, a_final):\n            H_final = x_data @ W_final.T\n            A_final = np.maximum(0, H_final)\n            return (1 / np.sqrt(w)) * (A_final @ a_final)\n\n        f_full_final = predict_network(X_all, W, a)\n\n        # --- Linearized Model (NTK) Training ---\n\n        # Kernel computation function\n        def compute_kernel(x1, x2, width, W_init, a_init):\n            # Part 1: Contribution from derivatives w.r.t. 'a'\n            act1 = np.maximum(0, x1 @ W_init.T)\n            act2 = np.maximum(0, x2 @ W_init.T)\n            K1 = (1 / width) * (act1 @ act2.T)\n\n            # Part 2: Contribution from derivatives w.r.t. 'W'\n            dot_prod_xx = x1 @ x2.T\n            S1 = (x1 @ W_init.T  0).astype(np.float64)\n            S2 = (x2 @ W_init.T  0).astype(np.float64)\n            a_init_sq = a_init**2\n            K2_scalar_part = (S1 * a_init_sq) @ S2.T\n            K2 = (1 / width) * dot_prod_xx * K2_scalar_part\n\n            return K1 + K2\n\n        K_train_train = compute_kernel(X_train, X_train, w, W0, a0)\n        K_all_train = compute_kernel(X_all, X_train, w, W0, a0)\n\n        # Initial predictions for the linearized model\n        f0_all = predict_network(X_all, W0, a0)\n        \n        # Evolve all predictions using the linearized dynamics\n        f_lin_all = f0_all.copy()\n        \n        # Track training predictions to compute residuals\n        f_lin_train = f0_all[:n].copy()\n        \n        for _ in range(T):\n            r_lin_train = f_lin_train - y_train\n            # Update predictions on all data points\n            f_lin_all -= (eta / n) * (K_all_train @ r_lin_train)\n            # Update local copy of training predictions for the next iteration\n            f_lin_train = f_lin_all[:n]\n\n        f_lin_final = f_lin_all\n\n        # --- Deviation Calculation ---\n        deviation = np.mean((f_full_final - f_lin_final)**2)\n        results.append(deviation)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3159054"}]}