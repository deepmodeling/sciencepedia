{"hands_on_practices": [{"introduction": "多模态学习的真正威力，并不仅仅是简单地聚合来自不同来源的信息，而在于理解那些只能在模态交互中才能浮现的“组合概念”。本练习 [@problem_id:3156162] 将通过一个精心设计的合成任务来阐明这一核心思想。你将亲手构建一个场景，其中标签（例如“红色立方体”）完全由跨模态的组合关系决定，并验证单模态分类器为何在此任务上必然失败，而一个简单的跨模态注意力模型却能轻松成功，从而深刻体会到学习跨模态联合表示的关键所在。", "problem": "给定一个综合性多模态学习场景，其中两种模态，一种是符号化的“形状”模态，另一种是符号化的“颜色”模态，均被编码为独热向量。目标标签仅取决于跨模态关系（例如，组合描述“红色立方体”），而不依赖于任何单一模态。您的任务是使用基于梯度的优化方法，实现并训练三个模型：一个单模态仅形状逻辑回归分类器，一个单模态仅颜色逻辑回归分类器，以及一个实现了一种简单形式交叉注意力的跨模态双线性分类器。您必须证明单模态分类器无法恢复组合标签，而交叉注意力模型能够成功。\n\n使用的基本原理：\n- 使用 sigmoid (logistic) 函数和二元交叉熵 (BCE) 损失的逻辑回归分类器。\n- 独热特征编码和基本线性代数恒等式。\n- 分类组合上的均匀数据分布以及定义在模态对上的确定性标签函数。\n\n定义：\n- 设形状模态由独热向量 $x_{s} \\in \\{0,1\\}^{n_{s}}$ 表示，颜色模态由独热向量 $x_{c} \\in \\{0,1\\}^{n_{c}}$ 表示，其中 $n_{s}$ 和 $n_{c}$ 分别是不同形状和颜色的数量。\n- 标签 $y \\in \\{0,1\\}$ 由一个作用于索引对 $(i,j)$ 的确定性映射 $g: \\{1,\\dots,n_{s}\\} \\times \\{1,\\dots,n_{c}\\} \\to \\{0,1\\}$ 定义，该索引对对应于形状和颜色类别。数据分布在所有 $(i,j)$ 对上是均匀的。\n- Sigmoid 函数为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- 对于预测值 $\\hat{y} \\in (0,1)$ 和标签 $y \\in \\{0,1\\}$，二元交叉熵 (BCE) 损失为 $L = -\\left(y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})\\right)$。\n\n模型：\n- 仅形状逻辑回归：$\\hat{y} = \\sigma\\left(w_{s}^{\\top} x_{s} + b_{s}\\right)$，参数为 $w_{s} \\in \\mathbb{R}^{n_{s}}$ 和 $b_{s} \\in \\mathbb{R}$。\n- 仅颜色逻辑回归：$\\hat{y} = \\sigma\\left(w_{c}^{\\top} x_{c} + b_{c}\\right)$，参数为 $w_{c} \\in \\mathbb{R}^{n_{c}}$ 和 $b_{c} \\in \\mathbb{R}$。\n- 带交叉注意力门的跨模态双线性分类器：$\\hat{y} = \\sigma\\left(x_{s}^{\\top} W x_{c} + b\\right)$，参数为 $W \\in \\mathbb{R}^{n_{s} \\times n_{c}}$ 和 $b \\in \\mathbb{R}$。此处，$x_{s}^{\\top} W x_{c}$ 为形状-颜色对选择了标量兼容性权重，作为交叉注意力分数，通过 sigmoid 函数门控后产生一个概率。\n\n训练目标：\n- 对每个模型的参数使用梯度下降更新，最小化数据集上的平均 BCE 损失。\n\n您的程序必须实现以下数据集测试套件并报告准确率：\n- 测试用例 1：$n_{s} = 2$, $n_{c} = 2$。定义索引：形状 1 为“立方体”，形状 2 为“球体”，颜色 1 为“红色”，颜色 2 为“蓝色”。设 $g(1,1) = 1$ 且 $g(2,2) = 1$，而 $g(1,2) = 0$ 且 $g(2,1) = 0$。这为“红色立方体”和“蓝色球体”编码了正例。\n- 测试用例 2：$n_{s} = 2$, $n_{c} = 3$。设若 $(i \\bmod 2) = (j \\bmod 2)$，则 $g(i,j) = 1$，否则 $g(i,j) = 0$。索引为 $i \\in \\{1,2\\}$ 和 $j \\in \\{1,2,3\\}$。\n- 测试用例 3：$n_{s} = 3$, $n_{c} = 3$。设若 $i = j$，则 $g(i,j) = 1$，否则 $g(i,j) = 0$。这仅为形状-颜色匹配对角线上的项编码了正例。\n\n科学真实性和推导要求：\n- 您必须证明为什么当映射 $g(i,j)$ 不可约简为仅关于 $i$ 或仅关于 $j$ 的函数时，单模态分类器通常无法表示该映射。\n- 您必须证明跨模态双线性分类器可以通过适当的参数设置来表示任意关于对的确定性映射，因为每个 $(i,j)$ 对都有其自身的参数 $W_{ij}$ 线性地贡献于 logit。\n- 训练必须在 BCE 损失上使用梯度下降来生成模型参数，评估指标必须是在 $\\hat{y}$ 上以 $0.5$ 为阈值计算的准确率。\n\n数值规格：\n- 不涉及物理单位。\n- 不涉及角度。\n- 准确率必须以小数形式表示（例如，$0.750$），并四舍五入到 $3$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序如下：\n$[$shape\\_only\\_accuracy\\_test1, color\\_only\\_accuracy\\_test1, cross\\_attention\\_accuracy\\_test1, shape\\_only\\_accuracy\\_test2, color\\_only\\_accuracy\\_test2, cross\\_attention\\_accuracy\\_test2, shape\\_only\\_accuracy\\_test3, color\\_only\\_accuracy\\_test3, cross\\_attention\\_accuracy\\_test3$]$。每个准确率必须是四舍五入到 $3$ 位小数的小数。", "solution": "该问题要求在一个综合性多模态学习背景下，分析和演示不同模型架构的表征能力。具体而言，我们必须说明为什么单模态逻辑回归分类器在学习两个符号模态之间的组合关系时会失败，而跨模态双线性分类器会成功。解决方案既包括理论论证，也包括使用基于梯度的优化的数值实现。\n\n### 模型能力的理论论证\n\n问题的核心在于模型是否有能力表示目标函数 $g(i,j)$，该函数根据形状类别 $i \\in \\{1, \\dots, n_s\\}$ 和颜色类别 $j \\in \\{1, \\dots, n_c\\}$ 的组合来定义标签 $y \\in \\{0,1\\}$。输入是用于形状的独热向量 $x_s \\in \\{0,1\\}^{n_s}$ 和用于颜色的独热向量 $x_c \\in \\{0,1\\}^{n_c}$。\n\n**1. 单模态逻辑回归模型**\n\n考虑仅形状逻辑回归模型：$\\hat{y} = \\sigma(w_s^\\top x_s + b_s)$。\n当输入对应第 $i$ 个形状时，独热向量 $x_s$ 在第 $i$ 个位置为 $1$，其余位置为 $0$。设此向量为 $e_i$。该输入的 logit（sigmoid 函数 $\\sigma$ 的输入）为：\n$$z_i = w_s^\\top e_i + b_s = w_{s,i} + b_s$$\n其中 $w_{s,i}$ 是权重向量 $w_s$ 的第 $i$ 个分量。预测值为 $\\hat{y}_i = \\sigma(w_{s,i} + b_s)$。\n\n关键在于，这个预测*仅*取决于形状索引 $i$。当同一个形状 $i$ 与不同的颜色 $j_1, j_2, \\dots, j_{n_c}$ 配对时，该模型从根本上无法产生不同的预测。然而，对于固定的 $i$，目标标签 $y=g(i,j)$ 会随 $j$ 变化。\n\n在使用二元交叉熵 (BCE) 损失的梯度下降优化下，模型将调整其参数 $w_s$ 和 $b_s$，以最小化在均匀分布的数据集上的总损失。对于给定的形状 $i$，最小化期望 BCE 损失的最优预测 $\\hat{y}_i$ 是所有颜色上真实标签的平均值：\n$$\\hat{y}_i \\to \\mathbb{E}_{j}[g(i,j)] = \\frac{1}{n_c} \\sum_{j=1}^{n_c} g(i,j)$$\n模型学习的是边际概率 $P(y=1 | \\text{shape}=i)$。如果对于 $y=1$ 的情况，这个边际概率不能稳定地高于 $0.5$，而对于 $y=0$ 的情况，不能稳定地低于 $0.5$，那么模型将会失败。例如，在测试用例 1 中，对于形状 1（“立方体”），标签为 $g(1,1)=1$ 和 $g(1,2)=0$。边际概率为 $(1+0)/2 = 0.5$。模型的最优预测为 $\\hat{y}=0.5$，这导致准确率为 $0.5$（随机猜测），因为它位于决策边界上。\n\n同样的论证也适用于仅颜色模型 $\\hat{y} = \\sigma(w_c^\\top x_c + b_c)$。它只能学习边际概率 $P(y=1 | \\text{color}=j)$，并且如果真实标签函数 $g(i,j)$ 不能简化为仅关于 $j$ 的函数，它就会失败。\n\n**2. 跨模态双线性分类器**\n\n现在考虑跨模态双线性分类器：$\\hat{y} = \\sigma(x_s^\\top W x_c + b)$。\n当输入对应第 $i$ 个形状（$x_s = e_i$）和第 $j$ 个颜色（$x_c = e_j$）的组合时，logit 为：\n$$z_{ij} = e_i^\\top W e_j + b$$\n项 $e_i^\\top W e_j$ 是线性代数中的一个基本操作，它选取矩阵 $W$ 第 $i$ 行和第 $j$ 列的标量元素。因此，logit 可简化为：\n$$z_{ij} = W_{ij} + b$$\n对 $(i,j)$ 的预测值为 $\\hat{y}_{ij} = \\sigma(W_{ij} + b)$。\n\n这种架构为每个唯一的形状-颜色对 $(i,j)$ 提供了一个独立的、可学习的参数 $W_{ij}$。这使得模型能够表示任意的映射 $g: \\{1,\\dots,n_s\\} \\times \\{1,\\dots,n_c\\} \\to \\{0,1\\}$。为了实现完美分类（准确率为 $1.0$），模型的参数必须设置得当，使得当 $g(i,j)=1$ 时 logit $z_{ij}$ 为正，当 $g(i,j)=0$ 时为负。这总是可以实现的。例如，可以将偏置 $b$ 设为 $0$，并将权重矩阵元素设为：\n$$W_{ij} = \\begin{cases} C  \\text{if } g(i,j) = 1 \\\\ -C  \\text{if } g(i,j) = 0 \\end{cases}$$\n对于任意正常数 $C$。梯度下降会找到满足这些符号条件的 $W$ 和 $b$ 的值，从而将 BCE 损失趋向于 $0$，并在训练数据上达到 $100\\%$ 的准确率。\n\n### 通过梯度下降的算法实现\n\n为了训练这些模型，我们在数据集中的所有 $N = n_s \\times n_c$ 个数据点上最小化平均 BCE 损失。对于单个预测 $\\hat{y}$ 和标签 $y$，损失为 $L = -(y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))$。关键的梯度分量是 $\\frac{\\partial L}{\\partial z} = \\hat{y} - y$，其中 $z$ 是 logit。对于学习率 $\\alpha$，参数更新遵循规则：$\\theta \\leftarrow \\theta - \\alpha \\frac{1}{N} \\sum_{k=1}^N \\frac{\\partial L_k}{\\partial \\theta}$。\n\n每个模型的具体梯度如下：\n- **仅形状模型：**\n  $$ \\frac{\\partial L}{\\partial w_s} = (\\hat{y}-y)x_s \\quad ; \\quad \\frac{\\partial L}{\\partial b_s} = \\hat{y}-y $$\n- **仅颜色模型：**\n  $$ \\frac{\\partial L}{\\partial w_c} = (\\hat{y}-y)x_c \\quad ; \\quad \\frac{\\partial L}{\\partial b_c} = \\hat{y}-y $$\n- **跨模态双线性模型：**\n  $$ \\frac{\\partial L}{\\partial W} = (\\hat{y}-y) x_s x_c^\\top \\quad ; \\quad \\frac{\\partial L}{\\partial b} = \\hat{y}-y $$\n其中 $x_s x_c^\\top$ 是两个独热向量的外积，产生一个零矩阵，仅在对应于当前活动的形状-颜色对的位置上有一个 $1$。\n\n通过使用这些梯度实现批量梯度下降，我们可以训练每个模型，并从数值上验证关于其性能的理论预测。评估是通过计算准确率来进行的，其中如果预测值 $\\hat{y}  0.5$，则分类为 $1$，否则分类为 $0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and trains three models on three synthetic multimodal learning tasks,\n    reporting the final classification accuracy for each.\n    \"\"\"\n\n    def sigmoid(z):\n        \"\"\"The sigmoid function.\"\"\"\n        # Using np.exp is numerically stable for large inputs.\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def generate_dataset(n_s, n_c, g_func):\n        \"\"\"Generates the full dataset for a given configuration.\"\"\"\n        dataset = []\n        for i in range(1, n_s + 1):\n            for j in range(1, n_c + 1):\n                x_s = np.zeros(n_s)\n                x_s[i-1] = 1.0\n                x_c = np.zeros(n_c)\n                x_c[j-1] = 1.0\n                y = g_func(i, j)\n                dataset.append((x_s, x_c, y))\n        return dataset\n\n    def train_and_evaluate(model_type, dataset, n_s, n_c, learning_rate=1.0, epochs=1000):\n        \"\"\"Trains a specified model and evaluates its accuracy.\"\"\"\n        n_samples = len(dataset)\n        \n        # Epsilon for numerical stability in log\n        epsilon = 1e-9\n\n        # Initialize parameters\n        if model_type == 'shape_only':\n            w = np.zeros(n_s)\n            b = 0.0\n        elif model_type == 'color_only':\n            w = np.zeros(n_c)\n            b = 0.0\n        elif model_type == 'cross_attention':\n            W = np.zeros((n_s, n_c))\n            b = 0.0\n        else:\n            raise ValueError(\"Unknown model type\")\n\n        # Batch Gradient Descent\n        for epoch in range(epochs):\n            grad_w_sum = 0\n            grad_b_sum = 0\n            \n            # Aggregate gradients over the whole dataset\n            for x_s, x_c, y in dataset:\n                if model_type == 'shape_only':\n                    z = w.T @ x_s + b\n                    x = x_s\n                elif model_type == 'color_only':\n                    z = w.T @ x_c + b\n                    x = x_c\n                else: # cross_attention\n                    z = x_s.T @ W @ x_c + b\n\n                y_hat = sigmoid(z)\n                \n                # Common gradient part for BCE loss\n                d_loss_d_z = y_hat - y\n\n                if model_type in ['shape_only', 'color_only']:\n                    grad_w_sum += d_loss_d_z * x\n                    grad_b_sum += d_loss_d_z\n                else: # cross_attention\n                    grad_w_sum += d_loss_d_z * np.outer(x_s, x_c)\n                    grad_b_sum += d_loss_d_z\n            \n            # Update parameters\n            if model_type in ['shape_only', 'color_only']:\n                w -= learning_rate * (grad_w_sum / n_samples)\n                b -= learning_rate * (grad_b_sum / n_samples)\n            else: # cross_attention\n                W -= learning_rate * (grad_w_sum / n_samples)\n                b -= learning_rate * (grad_b_sum / n_samples)\n\n        # Evaluate accuracy\n        correct_predictions = 0\n        for x_s, x_c, y in dataset:\n            if model_type == 'shape_only':\n                z = w.T @ x_s + b\n            elif model_type == 'color_only':\n                z = w.T @ x_c + b\n            else: # cross_attention\n                z = x_s.T @ W @ x_c + b\n                \n            y_hat = sigmoid(z)\n            prediction = 1 if y_hat  0.5 else 0\n            if prediction == y:\n                correct_predictions += 1\n        \n        accuracy = correct_predictions / n_samples\n        return accuracy\n\n    # Define test cases\n    test_cases = [\n        {\n            \"name\": \"Test Case 1\",\n            \"n_s\": 2, \"n_c\": 2,\n            \"g_func\": lambda i, j: 1.0 if (i, j) in [(1, 1), (2, 2)] else 0.0\n        },\n        {\n            \"name\": \"Test Case 2\",\n            \"n_s\": 2, \"n_c\": 3,\n            \"g_func\": lambda i, j: 1.0 if (i % 2) == (j % 2) else 0.0\n        },\n        {\n            \"name\": \"Test Case 3\",\n            \"n_s\": 3, \"n_c\": 3,\n            \"g_func\": lambda i, j: 1.0 if i == j else 0.0\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        dataset = generate_dataset(case[\"n_s\"], case[\"n_c\"], case[\"g_func\"])\n        \n        # Shape-only model\n        acc_shape = train_and_evaluate('shape_only', dataset, case[\"n_s\"], case[\"n_c\"])\n        all_results.append(f\"{acc_shape:.3f}\")\n\n        # Color-only model\n        acc_color = train_and_evaluate('color_only', dataset, case[\"n_s\"], case[\"n_c\"])\n        all_results.append(f\"{acc_color:.3f}\")\n\n        # Cross-attention model\n        acc_cross = train_and_evaluate('cross_attention', dataset, case[\"n_s\"], case[\"n_c\"])\n        all_results.append(f\"{acc_cross:.3f}\")\n\n    # Print results in the specified format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3156162"}, {"introduction": "在认识到融合的必要性之后，下一个关键问题是：我们该如何在数学上精确地执行这一过程？本练习 [@problem_id:3156180] 将带你深入多模态融合最经典的场景之一——高斯融合的数学核心。通过从第一性原理出发，推导两个高斯分布相乘的闭式解，你将直观地看到不确定信息的融合如何产生一个更精确、更可靠的估计，并掌握许多高级多模态概率模型背后的基本运算。", "problem": "考虑一个多模态学习场景，其中潜表征 $x \\in \\mathbb{R}^{d}$ 描述了一个从两个条件独立的模态（视觉模态 $v$ 和文本模态 $t$）中推断出的共享概念。每个模态都提供了关于 $x$ 的一个高斯置信：视觉模态产生一个密度 $p_{v}(x)$，建模为均值为 $\\mu_{v} \\in \\mathbb{R}^{d}$、协方差矩阵为 $\\Sigma_{v} \\in \\mathbb{R}^{d \\times d}$ 的多元正态分布；文本模态产生一个密度 $p_{t}(x)$，建模为均值为 $\\mu_{t} \\in \\mathbb{R}^{d}$、协方差矩阵为 $\\Sigma_{t} \\in \\mathbb{R}^{d \\times d}$ 的多元正态分布。假设 $x$ 服从无信息（均匀）先验，并且给定 $x$ 时，$v$ 和 $t$ 条件独立。\n\n仅使用基本的概率定义和代数，通过将两个高斯置信相乘，推导融合后验密度 $p(x \\mid v, t)$ 的闭式表达式。你的推导必须从多元正态分布的标准概率密度函数开始，并通过显式的代数操作来确定融合后的均值和协方差。\n\n然后，在标量情况 $d=1$ 下，通过计算当特定于模态的参数为 $\\mu_{v} = 0.8$, $\\sigma_{v}^{2} = 0.09$, $\\mu_{t} = 0.5$ 和 $\\sigma_{t}^{2} = 0.25$ 时的融合均值，来对推导进行数值验证，其中 $\\sigma_{v}^{2}$ 和 $\\sigma_{t}^{2}$ 分别表示对应于 $\\Sigma_{v}$ 和 $\\Sigma_{t}$ 的标量方差。将你的最终数值答案（标量情况下的融合均值）四舍五入到四位有效数字。以一个不带单位的实数形式提供你的最终答案。", "solution": "我们被要求在无信息先验和模态条件独立的条件下，融合关于同一潜变量 $x$ 的两个高斯置信。我们使用的基本依据包括：\n- 贝叶斯定理：对于条件独立的观测 $v$ 和 $t$，在 $x$ 服从均匀先验的情况下，融合后验与关于 $x$ 的各模态似然的乘积成正比。\n- 多元正态分布的概率密度函数。\n\n设 $p_{v}(x) = \\mathcal{N}(\\mu_{v}, \\Sigma_{v})$ 且 $p_{t}(x) = \\mathcal{N}(\\mu_{t}, \\Sigma_{t})$。在均匀先验和条件独立的条件下，融合后验满足\n$$\np(x \\mid v, t) \\propto p_{v}(x) \\, p_{t}(x).\n$$\n我们将每个高斯密度以其标准形式表示。对于 $x \\in \\mathbb{R}^{d}$，\n$$\np_{v}(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_{v}|^{1/2}} \\exp\\!\\left( -\\frac{1}{2} (x - \\mu_{v})^{\\top} \\Sigma_{v}^{-1} (x - \\mu_{v}) \\right),\n$$\n$$\np_{t}(x) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_{t}|^{1/2}} \\exp\\!\\left( -\\frac{1}{2} (x - \\mu_{t})^{\\top} \\Sigma_{t}^{-1} (x - \\mu_{t}) \\right).\n$$\n它们的乘积是\n$$\np_{v}(x) \\, p_{t}(x) = C \\, \\exp\\!\\left( -\\frac{1}{2} (x - \\mu_{v})^{\\top} \\Sigma_{v}^{-1} (x - \\mu_{v}) - \\frac{1}{2} (x - \\mu_{t})^{\\top} \\Sigma_{t}^{-1} (x - \\mu_{t}) \\right),\n$$\n其中\n$$\nC = \\frac{1}{(2\\pi)^{d} |\\Sigma_{v}|^{1/2} |\\Sigma_{t}|^{1/2}}\n$$\n是一个相对于 $x$ 的常数。我们关注指数部分，并展开两个二次型：\n\n$$\n\\begin{aligned}\n-\\frac{1}{2} \\Big[ (x - \\mu_{v})^{\\top} \\Sigma_{v}^{-1} (x - \\mu_{v}) + (x - \\mu_{t})^{\\top} \\Sigma_{t}^{-1} (x - \\mu_{t}) \\Big] \\\\\n= -\\frac{1}{2} \\Big[ x^{\\top} \\Sigma_{v}^{-1} x - 2 \\mu_{v}^{\\top} \\Sigma_{v}^{-1} x + \\mu_{v}^{\\top} \\Sigma_{v}^{-1} \\mu_{v} + x^{\\top} \\Sigma_{t}^{-1} x - 2 \\mu_{t}^{\\top} \\Sigma_{t}^{-1} x + \\mu_{t}^{\\top} \\Sigma_{t}^{-1} \\mu_{t} \\Big] \\\\\n= -\\frac{1}{2} \\Big[ x^{\\top} (\\Sigma_{v}^{-1} + \\Sigma_{t}^{-1}) x - 2 (\\Sigma_{v}^{-1} \\mu_{v} + \\Sigma_{t}^{-1} \\mu_{t})^{\\top} x + \\mu_{v}^{\\top} \\Sigma_{v}^{-1} \\mu_{v} + \\mu_{t}^{\\top} \\Sigma_{t}^{-1} \\mu_{t} \\Big].\n\\end{aligned}\n$$\n\n定义融合精度矩阵\n$$\n\\Lambda = \\Sigma_{v}^{-1} + \\Sigma_{t}^{-1}\n$$\n和融合自然参数\n$$\n\\eta = \\Sigma_{v}^{-1} \\mu_{v} + \\Sigma_{t}^{-1} \\mu_{t}.\n$$\n那么指数部分变为\n$$\n-\\frac{1}{2} \\Big[ x^{\\top} \\Lambda x - 2 \\eta^{\\top} x + \\text{constant} \\Big].\n$$\n我们通过配方法将此二次型表示为中心化的高斯形式。设融合后的协方差和均值为\n$$\n\\Sigma = \\Lambda^{-1}, \\quad \\mu = \\Sigma \\eta.\n$$\n注意到\n\n$$\n\\begin{aligned}\nx^{\\top} \\Lambda x - 2 \\eta^{\\top} x\n= (x - \\mu)^{\\top} \\Lambda (x - \\mu) - \\mu^{\\top} \\Lambda \\mu \\\\\n= (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) - \\eta^{\\top} \\Sigma \\eta,\n\\end{aligned}\n$$\n\n其中我们使用了 $\\mu = \\Sigma \\eta$ 和 $\\Lambda = \\Sigma^{-1}$。因此，\n\n$$\n-\\frac{1}{2} \\Big[ x^{\\top} \\Lambda x - 2 \\eta^{\\top} x + \\text{constant} \\Big]\n= -\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1} (x - \\mu) + \\text{new constant}.\n$$\n\n因此，$p_{v}(x) \\, p_{t}(x)$ 与一个协方差为 $\\Sigma = (\\Sigma_{v}^{-1} + \\Sigma_{t}^{-1})^{-1}$、均值为 $\\mu = \\Sigma (\\Sigma_{v}^{-1} \\mu_{v} + \\Sigma_{t}^{-1} \\mu_{t})$ 的多元正态密度成正比。归一化后，融合后验 $p(x \\mid v, t)$ 是一个恰好具有这些参数的高斯分布。\n\n在标量情况 $d = 1$ 下进行数值验证。用 $\\sigma_{v}^{2}$ 和 $\\sigma_{t}^{2}$ 表示标量方差。融合参数的标量等价形式为：\n$$\n\\sigma^{2} = \\left( \\sigma_{v}^{-2} + \\sigma_{t}^{-2} \\right)^{-1}, \\quad\n\\mu = \\sigma^{2} \\left( \\sigma_{v}^{-2} \\mu_{v} + \\sigma_{t}^{-2} \\mu_{t} \\right).\n$$\n给定 $\\mu_{v} = 0.8$, $\\sigma_{v}^{2} = 0.09$, $\\mu_{t} = 0.5$ 和 $\\sigma_{t}^{2} = 0.25$：\n\n$$\n\\sigma_{v}^{-2} = \\frac{1}{0.09} = \\frac{100}{9}, \\quad\n\\sigma_{t}^{-2} = \\frac{1}{0.25} = 4.\n$$\n\n计算均值的分子和分母：\n\n$$\n\\sigma_{v}^{-2} \\mu_{v} + \\sigma_{t}^{-2} \\mu_{t}\n= \\frac{100}{9} \\cdot 0.8 + 4 \\cdot 0.5\n= \\frac{80}{9} + 2\n= \\frac{80}{9} + \\frac{18}{9} = \\frac{98}{9},\n$$\n\n$$\n\\sigma_{v}^{-2} + \\sigma_{t}^{-2}\n= \\frac{100}{9} + 4\n= \\frac{100}{9} + \\frac{36}{9} = \\frac{136}{9}.\n$$\n\n因此，融合均值为\n$$\n\\mu = \\frac{\\sigma_{v}^{-2} \\mu_{v} + \\sigma_{t}^{-2} \\mu_{t}}{\\sigma_{v}^{-2} + \\sigma_{t}^{-2}}\n= \\frac{\\frac{98}{9}}{\\frac{136}{9}}\n= \\frac{98}{136}\n= \\frac{49}{68}\n\\approx 0.7205882353\\ldots\n$$\n\n四舍五入到四位有效数字，融合均值为 $0.7206$。作为额外检验（非最终答案所要求），融合方差为\n\n$$\n\\sigma^{2} = \\left( \\frac{100}{9} + 4 \\right)^{-1}\n= \\left( \\frac{136}{9} \\right)^{-1}\n= \\frac{9}{136}\n\\approx 0.06617647,\n$$\n\n该值低于 $0.09$ 和 $0.25$，这与融合独立高斯信息源时精度增加的结论一致。融合均值 $0.7206$ 位于 $0.5$ 和 $0.8$ 之间，且更接近 $0.8$，这反映了视觉模态（$\\sigma_{v}^{-2} = \\frac{100}{9} \\approx 11.11$）相对于文本模态（$\\sigma_{t}^{-2} = 4$）具有更高的精度。", "answer": "$$\\boxed{0.7206}$$", "id": "3156180"}, {"introduction": "在设计多模态系统时，一个关键的架构决策是：我们应该在特征层级进行“早期融合”，还是在决策层级进行“晚期融合”？本练习 [@problem_id:3156125] 将引导你系统性地分析这一核心问题。你将首先从理论上推导晚期融合成为最优策略的条件，即模态间的“类条件独立性”假设；随后，通过编程实践，你将构建一个反例，清晰地展示在不满足该假设时，朴素的早期融合为何会导致性能下降，从而学会在实践中审慎地选择融合策略。", "problem": "给定一个二元分类场景，其中包含两种模态：一个视觉特征 $x_{v} \\in \\mathbb{R}$ 和一个文本特征 $x_{t} \\in \\mathbb{R}$。令联合观测为 $x = (x_{v}, x_{t}) \\in \\mathbb{R}^{2}$。类别标签为 $y \\in \\{0,1\\}$，其类别先验概率为 $\\pi_{0}$ 和 $\\pi_{1}$，满足 $\\pi_{0} + \\pi_{1} = 1$。贝叶斯决策规则将 $x$ 分配给能最大化后验概率 $p(y \\mid x)$ 的类别 $y$，这等价于将后验对数几率与 $0$ 进行比较。假设以下基本前提：\n- 贝叶斯决策理论：贝叶斯分类器通过选择具有最大后验概率 $p(y \\mid x)$ 的类别来最小化错误概率。\n- 链式法则和独立性：对于任意随机变量 $a$ 和 $b$，当且仅当 $a$ 和 $b$ 在给定 $y$ 的条件下条件独立时，才有 $p(a,b \\mid y) = p(a \\mid y)\\,p(b \\mid y)$。\n\n任务 1 (推导)。从贝叶斯决策规则和仅有的这些基本事实出发，推导出一种将各模态似然相乘的晚期融合规则成为贝叶斯最优的精确数学条件。具体来说，假设你有两个分别生成 $p(x_{v} \\mid y)$ 和 $p(x_{t} \\mid y)$ 的单模态模型，你通过为每个 $y$ 构建一个与 $p(x_{v} \\mid y)\\,p(x_{t} \\mid y)$ 成比例的联合分数来融合它们。请推导数据生成分布和单模态模型必须满足的充分必要条件，以使该晚期融合决策规则与贝叶斯分类器一致。你的推导必须用 $p(y)$、$p(x_{v}, x_{t} \\mid y)$ 和后验对数几率 $\\log \\frac{p(y=1 \\mid x)}{p(y=0 \\mid x)}$ 来表示，并且必须明确展示条件独立性 $p(x_{v}, x_{t} \\mid y) = p(x_{v} \\mid y)\\,p(x_{t} \\mid y)$ 的作用。\n\n任务 2 (高斯模型下的早期与晚期融合)。考虑一个生成模型，其中对于每个类别 $y \\in \\{0,1\\}$，$x$ 的条件分布是高斯分布，具有类别相关的均值和公共的对角协方差（类别条件同方差性和跨模态条件独立性）：\n$$\nx \\mid y=k \\sim \\mathcal{N}\\!\\left(\\mu_{k}, \\Sigma\\right), \\quad \\mu_{k} = \\begin{bmatrix}\\mu_{v k} \\\\ \\mu_{t k}\\end{bmatrix}, \\quad \\Sigma = \\begin{bmatrix}\\sigma_{v}^{2}  0 \\\\ 0  \\sigma_{t}^{2}\\end{bmatrix},\n$$\n且类别先验相等，即 $\\pi_{0} = \\pi_{1} = \\frac{1}{2}$。在此模型下：\n- 贝叶斯最优分类器（它等同于最优联合模型，并且在条件独立性下是数学上正确的晚期融合）是关于 $x$ 的线性判别式。\n- 一个朴素的早期融合基线方法是构建一个标量分数 $s = x_{v} + x_{t}$（即拼接后等权重求和），然后对 $s$ 应用最优阈值。\n\n请从第一性原理和上述高斯假设出发，推导出以下两种分类器分类错误率的闭式表达式：\n- 贝叶斯最优分类器的分类错误率，用类别均值之间的分离度和噪声方差表示。\n- 使用等权重的朴素早期融合的分类错误率，通过一维投影 $s$ 表示。\n\n你的推导必须从高斯密度和贝叶斯规则开始，不假设任何快捷公式。\n\n任务 3 (可测试的构建与程序)。实现一个程序，针对以下包含三种参数设置的测试套件，计算并报告朴素早期融合错误率与贝叶斯最优（晚期融合）错误率之间的差异：\n- 测试用例 A (等权重早期融合与贝叶斯方向一致的理想情况)：$\\mu_{0} = \\begin{bmatrix}-0.5 \\\\ -0.5\\end{bmatrix}$，$\\mu_{1} = \\begin{bmatrix}0.5 \\\\ 0.5\\end{bmatrix}$，$\\sigma_{v} = 1$，$\\sigma_{t} = 1$，$\\pi_{0} = \\pi_{1} = \\frac{1}{2}$。\n- 测试用例 B (模态为异方差，朴素的等权重是次优的)：$\\mu_{0} = \\begin{bmatrix}-1 \\\\ -1\\end{bmatrix}$，$\\mu_{1} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$，$\\sigma_{v} = 2$，$\\sigma_{t} = 1$，$\\pi_{0} = \\pi_{1} = \\frac{1}{2}$。\n- 测试用例 C (一个模态无信息量且含噪声)：$\\mu_{0} = \\begin{bmatrix}0 \\\\ -1\\end{bmatrix}$，$\\mu_{1} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$，$\\sigma_{v} = 2$，$\\sigma_{t} = 0.5$，$\\pi_{0} = \\pi_{1} = \\frac{1}{2}$。\n\n对于每个测试用例，程序必须：\n- 在完整的、类别协方差相等的高斯模型下计算贝叶斯最优错误率。\n- 使用其在 $s$ 上的最优阈值，计算朴素早期融合规则 $s = x_{v} + x_{t}$ 的错误率。\n- 输出差值 $(\\text{早期融合错误率}) - (\\text{贝叶斯错误率})$，结果为浮点数，四舍五入到六位小数。\n\n最终输出格式。你的程序应生成单行输出，其中包含三个浮点数差值，按 [A,B,C] 的顺序以逗号分隔的列表形式呈现，并用方括号括起来。例如，输出格式必须精确地类似于 $[\\text{dA},\\text{dB},\\text{dC}]$，其中每个 $\\text{dX}$ 都四舍五入到六位小数。不应打印任何单位或附加文本。所有角度（如果有）必须以弧度为单位；本问题中没有物理单位。", "solution": "该问题已经过验证，被认为是科学上合理、定义明确且客观的。它提出了一个应用于多模态学习的贝叶斯决策理论和统计模式识别的标准练习，尽管细节较多。所有必要的条件和数据都已提供，以获得唯一且有意义的解。\n\n本解答分为三个部分，对应于问题陈述中的三个任务。任务1探讨了似然乘积融合规则的理论基础。任务2推导了在高斯数据模型下两种分类器的具体错误率。任务3涉及这些结果的实现。\n\n**任务1：晚期融合贝叶斯最优性的条件**\n\n目标是推导基于各模态似然相乘的晚期融合规则成为贝叶斯最优的充分必要条件。\n\n贝叶斯决策规则将观测值 $x = (x_v, x_t)$ 归类到能最大化后验概率 $p(y|x)$ 的类别 $y \\in \\{0, 1\\}$。决策边界是后验概率相等的点集 $x$，即 $p(y=1|x) = p(y=0|x)$。这等价于后验对数几率为零：\n$$\n\\log \\frac{p(y=1|x)}{p(y=0|x)} = 0\n$$\n使用贝叶斯定理 $p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$，我们可以用类别条件似然 $p(x|y)$ 和类别先验 $p(y)$ 来表示后验对数几率：\n$$\n\\log \\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)} = \\log \\frac{p(x|y=1)}{p(x|y=0)} + \\log \\frac{p(y=1)}{p(y=0)} = 0\n$$\n这是贝叶斯最优分类器的决策函数。项 $\\log \\frac{p(x|y=1)}{p(x|y=0)}$ 是对数似然比。由于 $x=(x_v, x_t)$，真实的联合似然是 $p(x_v, x_t|y)$。因此，贝叶斯最优决策规则是在以下情况时分类为 $y=1$：\n$$\n\\log p(x_v, x_t|y=1) - \\log p(x_v, x_t|y=0) + \\log \\frac{\\pi_1}{\\pi_0}  0\n$$\n\n现在，考虑所提出的晚期融合规则。它为每个类别构建一个与各模态似然乘积 $p(x_v|y) p(x_t|y)$ 成比例的分数。当与类别先验结合以做出决策时，该规则等价于选择使 $\\pi_y p(x_v|y) p(x_t|y)$ 最大化的类别 $y$。此规则的决策边界是 $\\pi_1 p(x_v|y=1) p(x_t|y=1) = \\pi_0 p(x_v|y=0) p(x_t|y=0)$。在对数空间中，这表示为：\n$$\n\\log \\frac{p(x_v|y=1)p(x_t|y=1)}{p(x_v|y=0)p(x_t|y=0)} + \\log \\frac{\\pi_1}{\\pi_0} = 0\n$$\n这可以重写为：\n$$\n(\\log p(x_v|y=1) - \\log p(x_v|y=0)) + (\\log p(x_t|y=1) - \\log p(x_t|y=0)) + \\log \\frac{\\pi_1}{\\pi_0}  0\n$$\n\n要使此晚期融合规则是贝叶斯最优的，其决策函数必须对所有 $x$ 等价于贝叶斯最优决策函数。由于先验项 $\\log(\\pi_1/\\pi_0)$ 对两者是共同的，它们基于似然的项必须等价。具体来说，对于所有 $x$，以下等式必须成立：\n$$\n\\log \\frac{p(x_v, x_t|y=1)}{p(x_v, x_t|y=0)} = \\log \\frac{p(x_v|y=1)p(x_t|y=1)}{p(x_v|y=0)p(x_t|y=0)}\n$$\n假设单模态模型正确估计了真实的边缘似然 $p(x_v|y)$ 和 $p(x_t|y)$，此等式是似然乘积规则成为贝叶斯最优的**对数据生成分布的充分必要条件**。\n\n当我们进一步分析这个条件时，条件独立性的作用就显而易见了。根据概率的链式法则，联合似然为 $p(x_v, x_t|y) = p(x_v|y) p(x_t|x_v, y)$。如果模态 $x_v$ 和 $x_t$ 在给定类别标签 $y$ 的条件下是条件独立的，那么根据定义，$p(x_t|x_v, y) = p(x_t|y)$。在这种情况下，联合似然可以分解为：\n$$\np(x_v, x_t|y) = p(x_v|y)p(x_t|y) \\quad \\text{对于 } y \\in \\{0, 1\\}\n$$\n如果这种条件独立性对两个类别都成立，那么真实的联合对数似然比变为：\n$$\n\\log \\frac{p(x_v, x_t|y=1)}{p(x_v, x_t|y=0)} = \\log \\frac{p(x_v|y=1) p(x_t|y=1)}{p(x_v|y=0) p(x_t|y=0)}\n$$\n这恰好是晚期融合模型使用的对数似然比。因此，只要单模态模型 $p(x_v|y)$ 和 $p(x_t|y)$ 被正确指定（即它们代表了真实的边缘分布），特征的类别条件独立性是晚期融合规则成为贝叶斯最优的**充分条件**。在边缘模型被正确指定的情况下，此条件也是必要的。\n\n**任务2：高斯模型下的错误率分析**\n\n给定一个生成模型，其中 $x | y=k \\sim \\mathcal{N}(\\mu_k, \\Sigma)$ 对于 $k \\in \\{0,1\\}$，具有 $\\pi_0 = \\pi_1 = 1/2$ 和一个对角协方差矩阵 $\\Sigma = \\text{diag}(\\sigma_v^2, \\sigma_t^2)$。\n\n**贝叶斯最优分类器错误率**\n由于先验相等，贝叶斯最优分类器根据对数似然比进行决策。类别 $k$ 的对数似然为：\n$$\n\\log p(x|y=k) = C - \\frac{1}{2}(x - \\mu_k)^T \\Sigma^{-1}(x - \\mu_k)\n$$\n其中 $C = -\\frac{1}{2}\\log|2\\pi\\Sigma|$ 是一个常数。决策边界是 $\\log p(x|y=1) = \\log p(x|y=0)$ 的地方，这可以简化为：\n$$\n(x - \\mu_1)^T \\Sigma^{-1}(x - \\mu_1) = (x - \\mu_0)^T \\Sigma^{-1}(x - \\mu_0)\n$$\n展开并化简，得到一个线性决策边界：\n$$\n2(\\mu_1 - \\mu_0)^T \\Sigma^{-1} x - (\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0) = 0\n$$\n我们定义一个判别分数 $g(x) = (\\mu_1 - \\mu_0)^T \\Sigma^{-1} x - \\frac{1}{2}(\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0)$。决策规则是如果 $g(x)  0$，则选择 $y=1$。为了找到错误率，我们确定 $g(x)$ 的分布。由于 $g(x)$ 是高斯向量 $x$ 的仿射变换，所以 $g(x)$ 也是高斯分布的。\n\n$g(x)$ 在给定 $y=k$ 条件下的均值为：\n$$\nE[g(x)|y=k] = (\\mu_1 - \\mu_0)^T \\Sigma^{-1} \\mu_k - \\frac{1}{2}(\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0)\n$$\n对于 $k=0$，均值为 $\\mu_{g,0} = -\\frac{1}{2}(\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 - \\mu_0)$。\n对于 $k=1$，均值为 $\\mu_{g,1} = +\\frac{1}{2}(\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 - \\mu_0)$。\n\n由于公共协方差 $\\Sigma$，$g(x)$ 的方差对两个类别是相同的：\n$$\n\\text{Var}(g(x)|y=k) = ((\\mu_1 - \\mu_0)^T \\Sigma^{-1}) \\Sigma ((\\mu_1 - \\mu_0)^T \\Sigma^{-1})^T = (\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 - \\mu_0)\n$$\n令 $\\Delta^2 = (\\mu_1 - \\mu_0)^T \\Sigma^{-1} (\\mu_1 - \\mu_0)$ 为马氏距离的平方。那么 $\\mu_{g,0} = -\\Delta^2/2$，$\\mu_{g,1} = \\Delta^2/2$，方差 $\\sigma_g^2 = \\Delta^2$。\n错误概率为 $P_{error} = \\pi_0 P(g(x)0|y=0) + \\pi_1 P(g(x)0|y=1)$。当 $\\pi_0 = \\pi_1 = 1/2$ 时：\n$$\nP(g(x)0|y=0) = P\\left(Z  \\frac{0 - (-\\Delta^2/2)}{\\sqrt{\\Delta^2}}\\right) = P(Z  \\Delta/2) = \\Phi(-\\Delta/2)\n$$\n其中 $Z \\sim \\mathcal{N}(0,1)$，$\\Phi$ 是其累积分布函数（CDF）。根据对称性，$P(g(x)0|y=1) = \\Phi(-\\Delta/2)$。\n总的贝叶斯错误率为：\n$$\nP_{\\text{Bayes}} = \\frac{1}{2}\\Phi(-\\Delta/2) + \\frac{1}{2}\\Phi(-\\Delta/2) = \\Phi(-\\Delta/2)\n$$\n代入参数 $\\Delta\\mu = \\mu_1 - \\mu_0 = [\\Delta\\mu_v, \\Delta\\mu_t]^T$ 和 $\\Sigma^{-1} = \\text{diag}(1/\\sigma_v^2, 1/\\sigma_t^2)$：\n$$\n\\Delta^2 = \\frac{(\\Delta\\mu_v)^2}{\\sigma_v^2} + \\frac{(\\Delta\\mu_t)^2}{\\sigma_t^2}\n$$\n所以，贝叶斯最优错误率为：\n$$\nP_{\\text{Bayes}} = \\Phi\\left(-\\frac{1}{2}\\sqrt{\\frac{(\\mu_{v1}-\\mu_{v0})^2}{\\sigma_v^2} + \\frac{(\\mu_{t1}-\\mu_{t0})^2}{\\sigma_t^2}}\\right)\n$$\n\n**朴素早期融合错误率**\n朴素早期融合规则使用标量分数 $s = x_v + x_t = [1, 1]x$。由于 $x$ 是高斯分布的，$s$ 也是高斯分布的。\n$s$ 在给定 $y=k$ 条件下的均值为：\n$$\n\\mu_{s,k} = E[s|y=k] = \\mu_{vk} + \\mu_{tk}\n$$\n$s$ 的方差对两个类别是公共的：\n$$\n\\sigma_s^2 = \\text{Var}(s|y=k) = \\text{Var}(x_v+x_t) = \\text{Var}(x_v) + \\text{Var}(x_t) + 2\\text{Cov}(x_v, x_t)\n$$\n由于 $\\Sigma$ 是对角矩阵，$\\text{Cov}(x_v, x_t) = 0$，所以 $\\sigma_s^2 = \\sigma_v^2 + \\sigma_t^2$。\n我们面临一个关于 $s$ 的一维分类问题，有两个分布 $\\mathcal{N}(\\mu_{s,0}, \\sigma_s^2)$ 和 $\\mathcal{N}(\\mu_{s,1}, \\sigma_s^2)$，以及相等的先验。最优阈值是均值的中点：$T = (\\mu_{s,0}+\\mu_{s,1})/2$。\n假设 $\\mu_{s,1}  \\mu_{s,0}$，错误概率是 $P(sT|y=0)$。\n$$\nP(sT|y=0) = P\\left(Z  \\frac{T - \\mu_{s,0}}{\\sigma_s}\\right) = P\\left(Z  \\frac{(\\mu_{s,0}+\\mu_{s,1})/2 - \\mu_{s,0}}{\\sigma_s}\\right) = P\\left(Z  \\frac{\\mu_{s,1}-\\mu_{s,0}}{2\\sigma_s}\\right)\n$$\n与贝叶斯情况一样，总错误率由这个单侧概率给出：\n$$\nP_{\\text{early}} = \\Phi\\left(-\\frac{|\\mu_{s,1}-\\mu_{s,0}|}{2\\sigma_s}\\right)\n$$\n代入各项：$\\mu_{s,1}-\\mu_{s,0} = (\\mu_{v1}+\\mu_{t1}) - (\\mu_{v0}+\\mu_{t0}) = \\Delta\\mu_v + \\Delta\\mu_t$，以及 $\\sigma_s = \\sqrt{\\sigma_v^2+\\sigma_t^2}$。\n朴素早期融合错误率为：\n$$\nP_{\\text{early}} = \\Phi\\left(-\\frac{|\\Delta\\mu_v + \\Delta\\mu_t|}{2\\sqrt{\\sigma_v^2+\\sigma_t^2}}\\right)\n$$", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the difference between naive early fusion error and Bayes-optimal error\n    for three specified test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test case A: Happy path where early fusion is optimal\n        {\n            \"mu0\": np.array([-0.5, -0.5]),\n            \"mu1\": np.array([0.5, 0.5]),\n            \"sigma_v\": 1.0,\n            \"sigma_t\": 1.0,\n        },\n        # Test case B: Heteroskedastic modalities, naive weights are suboptimal\n        {\n            \"mu0\": np.array([-1.0, -1.0]),\n            \"mu1\": np.array([1.0, 1.0]),\n            \"sigma_v\": 2.0,\n            \"sigma_t\": 1.0,\n        },\n        # Test case C: One modality is non-informative and noisy\n        {\n            \"mu0\": np.array([0.0, -1.0]),\n            \"mu1\": np.array([0.0, 1.0]),\n            \"sigma_v\": 2.0,\n            \"sigma_t\": 0.5,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        mu0 = case[\"mu0\"]\n        mu1 = case[\"mu1\"]\n        sigma_v = case[\"sigma_v\"]\n        sigma_t = case[\"sigma_t\"]\n\n        # Calculate differences in means for each modality\n        delta_mu_v = mu1[0] - mu0[0]\n        delta_mu_t = mu1[1] - mu0[1]\n\n        # --- Bayes-Optimal (Late Fusion) Error Calculation ---\n        # This classifier is optimal for the given diagonal Gaussian model.\n        # The squared Mahalanobis distance is the sum of squared SNR for each modality.\n        mahalanobis_sq = (delta_mu_v**2 / sigma_v**2) + (delta_mu_t**2 / sigma_t**2)\n        \n        # The argument to the standard normal CDF is -1/2 times the Mahalanobis distance.\n        bayes_arg = -0.5 * np.sqrt(mahalanobis_sq)\n        bayes_error = norm.cdf(bayes_arg)\n\n        # --- Naive Early Fusion Error Calculation ---\n        # This classifier projects the data onto the vector [1, 1] and then classifies.\n        # The argument to the CDF is based on the SNR of the projected scalar value.\n        delta_mu_sum = delta_mu_v + delta_mu_t\n        variance_sum = sigma_v**2 + sigma_t**2\n        \n        early_arg = -np.abs(delta_mu_sum) / (2 * np.sqrt(variance_sum))\n        early_error = norm.cdf(early_arg)\n\n        # Calculate the difference in errors\n        error_difference = early_error - bayes_error\n        results.append(error_difference)\n\n    # Format the results as a comma-separated list of floats rounded to six decimals.\n    # e.g., [0.000000,0.053127,0.170135]\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3156125"}]}