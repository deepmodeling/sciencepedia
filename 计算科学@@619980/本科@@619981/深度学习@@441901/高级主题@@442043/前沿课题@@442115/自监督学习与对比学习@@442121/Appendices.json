{"hands_on_practices": [{"introduction": "对比学习的核心是InfoNCE损失函数，它通过最大化“正样本”对的相似性并最小化与“负样本”的相似性来学习表示。为了通过梯度下降来优化模型，我们必须计算损失函数相对于网络输出（即嵌入向量）的梯度。这个练习 [@problem_id:77110] 让你亲手推导这个关键的梯度，从而深入理解模型如何被驱动以拉近正样本，同时考虑负样本的分布。", "problem": "在自主材料发现领域，深度学习模型被越来越多地用于分析原位表征实验的数据，例如在薄膜结晶过程中收集的时间序列椭偏数据。其目标是学习一种能够捕捉材料相变潜在动力学路径的表示。\n\n一种常见的方法是使用自监督对比学习。一个编码器网络 $f_\\theta$ 将来自椭偏仪在时间 $t$ 的高维数据向量（表示为 $x_t$）映射到 $\\mathbb{R}^D$ 中的一个低维嵌入向量 $z_t = f_\\theta(x_t)$。为确保表示具有良好的性质，嵌入向量经过 L2 归一化，使得 $\\|z_t\\|_2 = 1$。\n\n学习过程由 InfoNCE（噪声对比估计）损失函数指导。对于一个给定的“锚点”嵌入 $z_i$，我们从时间序列中的一个邻近点选择一个“正样本” $z_j$，它代表一个相似的材料状态。我们还从遥远的时间点选择一组 $N-1$ 个“负样本” $\\{z_k\\}_{k=1}^{N-1}$，它们代表不相似的状态。\n\n锚点 $z_i$ 和正样本 $z_j$ 的 InfoNCE 损失由下式给出：\n$$\nL(z_i, z_j, \\{z_k\\}_{k=1}^{N-1}) = -\\log \\left[ \\frac{\\exp(z_i \\cdot z_j / \\tau)}{\\exp(z_i \\cdot z_j / \\tau) + \\sum_{k=1}^{N-1} \\exp(z_i \\cdot z_k / \\tau)} \\right]\n$$\n这里，$z_i \\cdot z_j$ 是点积，用作归一化向量之间的相似度得分。参数 $\\tau > 0$ 是一个标量温度，用于控制分布的锐度。\n\n网络参数 $\\theta$ 使用梯度下降法进行更新，这需要计算损失函数相对于网络输出（即嵌入向量）的梯度。你的任务是推导该损失函数相对于正样本嵌入 $z_j$ 的梯度。", "solution": "1. 定义相似度得分：\n   $$s_{ij} = \\frac{z_i \\cdot z_j}{\\tau},\\quad s_{ik} = \\frac{z_i \\cdot z_k}{\\tau}.$$  \n2. 写出损失函数：\n   $$L = -\\log\\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k=1}^{N-1}e^{s_{ik}}} = -s_{ij} + \\log\\Bigl(e^{s_{ij}}+\\sum_{k=1}^{N-1}e^{s_{ik}}\\Bigr).$$  \n3. 计算关于 $z_j$ 的梯度：\n   $$\\nabla_{z_j}(-s_{ij}) = -\\nabla_{z_j}\\frac{z_i\\cdot z_j}{\\tau} = -\\frac{1}{\\tau}z_i,$$  \n   $$\\nabla_{z_j}\\log\\Bigl(e^{s_{ij}}+\\sum_{k}e^{s_{ik}}\\Bigr)\n     = \\frac{1}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}\\,\\nabla_{z_j}e^{s_{ij}}\n     = \\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}\\,\\frac{1}{\\tau}z_i.$$  \n4. 合并各项：\n   $$\\nabla_{z_j}L\n     = -\\frac{1}{\\tau}z_i + \\frac{e^{s_{ij}}}{\\tau\\bigl(e^{s_{ij}}+\\sum_{k}e^{s_{ik}}\\bigr)}\\,z_i\n     = \\frac{1}{\\tau}\\Bigl(\\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}-1\\Bigr)z_i.$$", "answer": "$$\\boxed{\\nabla_{z_j}L=\\frac{1}{\\tau}\\Bigl(\\frac{\\exp\\bigl(z_i\\cdot z_j/\\tau\\bigr)}{\\exp\\bigl(z_i\\cdot z_j/\\tau\\bigr)+\\sum_{k=1}^{N-1}\\exp\\bigl(z_i\\cdot z_k/\\tau\\bigr)}-1\\Bigr)z_i}$$", "id": "77110"}, {"introduction": "自监督学习旨在学习具有理想几何性质的表示，例如“不变性”和“等变性”。不变性意味着表示在某些变换下保持不变，而等变性则意味着表示以一种可预测的方式进行变换。这个动手编码练习 [@problem_id:3173218] 超越了纯粹的理论，要求你构建一个微型系统来同时强制实现这两种性质，让你具体地感受损失函数如何塑造嵌入空间的几何结构。", "problem": "您将设计并分析一个简易的自监督学习设置，其中通过已知变换群的作用生成多个增强视图。目标是将一个引导不变性的对比目标与一个引导等变性的惩罚项相结合。所有角度必须以弧度表示。本问题中没有物理单位。\n\n按如下方式构建数据集、定义群作用、指定编码器并实现一个混合目标。\n\n1. 数据集构建：\n   - 在 $\\mathbb{R}^2$ 的单位圆上创建 $n$ 个点，其中 $n = 8$。对于 $k \\in \\{0,1,\\dots,n-1\\}$，令 $\\theta_k = \\frac{2\\pi k}{n}$（以弧度为单位），并定义 $\\mathbf{x}_k = [\\cos(\\theta_k), \\sin(\\theta_k)]^\\top \\in \\mathbb{R}^2$。\n\n2. 群作用：\n   - 定义平面旋转群 $\\mathcal{G}$，其元素由角度 $\\phi \\in \\mathbb{R}$ 确定，通过标准旋转作用于 $\\mathbb{R}^2$。这个群应该在学习到的表示中引导不变性，即对于任意 $\\phi$，$\\mathbf{x}$ 的表示和 $g_\\phi \\cdot \\mathbf{x}$ 的表示应该是不可区分的。\n   - 定义关于水平轴的反射群 $\\mathcal{H}$，它作用于 $\\mathbb{R}^2$ 的方式为 $(x,y) \\mapsto (x,-y)$。这个群应该在学习到的表示中引导等变性，即当 $h \\in \\mathcal{H}$ 作用于输入时，表示应根据一个已知的线性表示 $\\rho(h) \\in \\mathbb{R}^{2 \\times 2}$ 进行变换。\n\n3. 编码器：\n   - 使用一个固定的、已知的编码器 $f_{\\mathbf{W}} : \\mathbb{R}^2 \\to \\mathbb{R}^2$，定义为 $f_{\\mathbf{W}}(\\mathbf{x}) = \\tanh(\\mathbf{W}\\mathbf{x})$，其中 $\\tanh(\\cdot)$ 按元素方式应用，且\n     $$\n     \\mathbf{W} = \\begin{bmatrix}\n     1.0  0.5 \\\\\n     0.3  1.2\n     \\end{bmatrix}.\n     $$\n     本问题不执行任何训练或参数更新。\n\n4. 从基本原理实现的目标：\n   - 对比不变性目标：通过将来自 $\\mathcal{G}$ 的变换（可能是 $\\mathcal{G}$ 中两个不同的元素）应用于同一基础样本来形成正样本对，并将批次中的所有其他样本视为负样本。实现一个基于批次中所有视图之间余弦相似度的归一化温度缩放交叉熵。使用温度 $\\tau = 0.2$。此目标应仅使用来自同一基础样本的视图对来捕捉对 $\\mathcal{G}$ 的不变性要求。\n   - 等变性惩罚项：对于将 $(x,y)$ 映射到 $(x,-y)$ 的反射 $h \\in \\mathcal{H}$，使用已知的线性表示\n     $$\n     \\rho(h) = \\begin{bmatrix}\n     1  0 \\\\\n     0  -1\n     \\end{bmatrix}.\n     $$\n     通过惩罚数据集中 $f_{\\mathbf{W}}(h \\cdot \\mathbf{x}_k)$ 和 $\\rho(h)\\, f_{\\mathbf{W}}(\\mathbf{x}_k)$ 之间的平均平方偏差来强制实现等变性。此惩罚项与不变性对无关，且不使用任何负样本。\n   - 混合目标：使用非负混合权重 $\\lambda = 0.1$ 将两者结合起来，为给定的增强流水线获得单个标量目标。您必须从上述定义出发，实现每个目标分量的精确数值定义，除了基本的线性代数和向量运算外，不使用任何黑盒或外部函数。\n\n5. 增强流水线（测试套件）：\n   对于下面的每个流水线，您必须确定性地为每个基础样本生成恰好两个视图，并按给定顺序使用指定的变换。不允许有任何随机性或噪声。\n   - 流水线 A（旨在强制对 $\\mathcal{G}$ 的不变性）：使用来自 $\\mathcal{G}$ 的两次旋转，第一个视图使用 $\\phi_1 = 0$，第二个视图使用 $\\phi_2 = \\frac{\\pi}{2}$。\n   - 流水线 B（违反了预期的不变性）：用各向同性缩放替换旋转，将因子 $s_1 = 0.5$ 和 $s_2 = 2.0$ 应用于每个基础样本以形成两个视图。缩放不是 $\\mathcal{G}$ 的元素。\n   - 流水线 C（在视图中混合了不同的群）：第一个视图使用角度为 $\\phi = \\pi$ 的旋转，第二个视图使用水平反射。反射不属于 $\\mathcal{G}$，因此这两个视图不在同一个 $\\mathcal{G}$-轨道上。\n   - 流水线 D（边界情况）：对两个视图都使用恒等变换，即不对任一视图应用任何变换。\n\n6. 程序要求：\n   - 构建数据集，定义群作用和编码器，并按所述实现混合目标。余弦相似度必须在 $\\mathbb{R}^2$ 中经过 $\\ell_2$ 归一化的特征向量之间计算。\n   - 对于测试套件中的每个流水线，计算混合目标值作为一个实数：即对比不变性目标与 $\\lambda$ 乘以等变性惩罚项之和，所有流水线使用相同的数据集和编码器。\n   - 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。顺序必须是 $[\\text{流水线 A 结果}, \\text{流水线 B 结果}, \\text{流水线 C 结果}, \\text{流水线 D 结果}]$，每个条目都是一个浮点数。\n\n您的解决方案必须从群作用、不变性和等变性的核心定义推导出所实现的目标。四个测试用例的最终答案必须是明确的浮点值，并且最终的程序必须是完整的且可按规定运行。", "solution": "### 解题推导\n\n该问题要求为四个不同的数据增强流水线计算一个混合目标函数。该目标结合了用于不变性的对比损失和用于等变性的惩罚项。\n\n**1. 预备知识：数据集和编码器**\n\n数据集由 $n=8$ 个在单位圆上的向量 $\\mathbf{x}_k \\in \\mathbb{R}^2$ 组成：\n$$ \\mathbf{x}_k = \\begin{bmatrix} \\cos(2\\pi k / n) \\\\ \\sin(2\\pi k / n) \\end{bmatrix}, \\quad k \\in \\{0, 1, \\dots, 7\\} $$\n编码器是一个固定的非线性函数 $f_{\\mathbf{W}}: \\mathbb{R}^2 \\to \\mathbb{R}^2$，定义为：\n$$ f_{\\mathbf{W}}(\\mathbf{x}) = \\tanh(\\mathbf{W}\\mathbf{x}) $$\n其中 $\\mathbf{W} = \\begin{bmatrix} 1.0  0.5 \\\\ 0.3  1.2 \\end{bmatrix}$，双曲正切函数 $\\tanh(\\cdot)$ 逐元素应用于向量 $\\mathbf{W}\\mathbf{x}$。\n\n**2. 等变性惩罚项 ($\\mathcal{L}_{\\text{eqv}}$)**\n\n等变性要求输入空间中的变换对应于特征空间中可预测的变换。对于生成元为 $h \\cdot (x,y) = (x,-y)$ 的反射群 $\\mathcal{H}$，特征表示期望通过矩阵 $\\rho(h) = \\text{diag}(1, -1)$ 进行变换。与完美等变性的偏差由惩罚项 $\\mathcal{L}_{\\text{eqv}}$ 量化，定义为在基础数据集上的均方误差：\n$$ \\mathcal{L}_{\\text{eqv}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\| f_{\\mathbf{W}}(h \\cdot \\mathbf{x}_k) - \\rho(h) f_{\\mathbf{W}}(\\mathbf{x}_k) \\|_2^2 $$\n其中 $h \\cdot \\mathbf{x}_k$ 是反射作用于 $\\mathbf{x}_k$ 的结果，而 $\\rho(h) f_{\\mathbf{W}}(\\mathbf{x}_k)$ 是目标变换后的特征向量。$h$ 的作用可以用矩阵 $\\mathbf{H} = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}$ 表示，因此 $h \\cdot \\mathbf{x}_k = \\mathbf{H}\\mathbf{x}_k$。\n该惩罚项是编码器和数据集的一个固定属性，与用于对比任务的增强流水线无关。它将被计算一次，并用于所有四个测试用例。\n\n**3. 对比不变性目标 ($\\mathcal{L}_{\\text{inv}}$)**\n\n不变性要求表示对来自给定群（这里是旋转群 $\\mathcal{G}$）的变换不敏感。对比目标通过将同一对象的不同视图（正样本对）的表示拉近，同时将它们与其他对象（负样本对）的表示推开来实现这一点。\n\n对于 $n$ 个基础样本中的每一个 $\\mathbf{x}_k$，一个增强流水线生成两个视图 $\\tilde{\\mathbf{x}}_{k,1}$ 和 $\\tilde{\\mathbf{x}}_{k,2}$。这 $2n$ 个视图由编码器 $f_{\\mathbf{W}}$ 处理，生成特征向量 $\\mathbf{z}_{i} \\in \\mathbb{R}^2$。然后对这些向量进行 $\\ell_2$ 归一化，使其位于特征空间的单位球面上：\n$$ \\mathbf{u}_i = \\frac{\\mathbf{z}_i}{\\|\\mathbf{z}_i\\|_2} $$\n两个视图之间的相似度通过它们的余弦相似度来衡量，$\\text{sim}(\\mathbf{u}_i, \\mathbf{u}_j) = \\mathbf{u}_i^\\top \\mathbf{u}_j$。\n\n目标是归一化温度缩放交叉熵损失（InfoNCE）。对于一个包含 $2n$ 个视图的批次，其中每个视图 $\\mathbf{u}_i$ 都有一个唯一的正样本伙伴 $\\mathbf{u}_{j}$（来自同一基础样本的另一个视图），损失为：\n$$ \\mathcal{L}_{\\text{inv}} = - \\frac{1}{2n} \\sum_{i=1}^{2n} \\log \\frac{\\exp(\\text{sim}(\\mathbf{u}_i, \\mathbf{u}_{\\text{pos}(i)}) / \\tau)}{\\sum_{m=1, m \\ne i}^{2n} \\exp(\\text{sim}(\\mathbf{u}_i, \\mathbf{u}_m) / \\tau)} $$\n其中 $\\text{pos}(i)$ 表示视图 $i$ 的正样本对的索引，$\\tau=0.2$ 是温度参数。$\\mathcal{L}_{\\text{inv}}$ 是针对每个特定的增强流水线计算的。\n\n**4. 混合目标 ($\\mathcal{L}$)**\n\n总目标函数是不变性损失和等变性损失的加权和：\n$$ \\mathcal{L} = \\mathcal{L}_{\\text{inv}} + \\lambda \\mathcal{L}_{\\text{eqv}} $$\n混合系数为 $\\lambda = 0.1$。\n\n**5. 流水线分析与计算过程**\n\n我们现在将为四个指定的流水线中的每一个计算 $\\mathcal{L}$ 的值。过程如下：\n首先，计算常量 $\\mathcal{L}_{\\text{eqv}}$。然后对于每个流水线：\na. 生成两组 $n$ 个视图，$\\{\\tilde{\\mathbf{x}}_{k,1}\\}_{k=0}^{n-1}$ 和 $\\{\\tilde{\\mathbf{x}}_{k,2}\\}_{k=0}^{n-1}$。\nb. 形成一个包含 $2n$ 个视图的单个批次。\nc. 对批次进行编码和归一化，以获得特征向量 $\\{\\mathbf{u}_i\\}_{i=1}^{2n}$。\nd. 使用上述公式计算 $\\mathcal{L}_{\\text{inv}}$。\ne. 计算总目标 $\\mathcal{L} = \\mathcal{L}_{\\text{inv}} + \\lambda \\mathcal{L}_{\\text{eqv}}$。\n\n- **流水线 A (旋转不变性):**\n    - 视图 1: $\\tilde{\\mathbf{x}}_{k,1} = g_{0} \\cdot \\mathbf{x}_k = \\mathbf{x}_k$。\n    - 视图 2: $\\tilde{\\mathbf{x}}_{k,2} = g_{\\pi/2} \\cdot \\mathbf{x}_k$。旋转矩阵为 $\\mathbf{R}_{\\pi/2} = \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix}$。\n    - 这些视图由群 $\\mathcal{G}$ 中的变换生成，符合不变性目标的设计意图。\n\n- **流水线 B (缩放违例):**\n    - 视图 1: $\\tilde{\\mathbf{x}}_{k,1} = 0.5 \\cdot \\mathbf{x}_k$。\n    - 视图 2: $\\tilde{\\mathbf{x}}_{k,2} = 2.0 \\cdot \\mathbf{x}_k$。\n    - 缩放不属于 $\\mathcal{G}$，因此该流水线违反了不变性目标的假设。表示预计不会相似。\n\n- **流水线 C (混合群违例):**\n    - 视图 1: $\\tilde{\\mathbf{x}}_{k,1} = g_{\\pi} \\cdot \\mathbf{x}_k = -\\mathbf{x}_k$。这是来自 $\\mathcal{G}$ 的一次旋转。\n    - 视图 2: $\\tilde{\\mathbf{x}}_{k,2} = h \\cdot \\mathbf{x}_k$。这是来自 $\\mathcal{H}$ 的一次反射，不属于 $\\mathcal{G}$（除了在反射轴上的点）。这两个视图不在同一个 $\\mathcal{G}$-轨道上。\n\n- **流水线 D (恒等基线):**\n    - 视图 1: $\\tilde{\\mathbf{x}}_{k,1} = \\mathbf{x}_k$。\n    - 视图 2: $\\tilde{\\mathbf{x}}_{k,2} = \\mathbf{x}_k$。\n    - 这两个视图是相同的。编码和归一化后的表示也将是相同的，导致所有正样本对的余弦相似度最大为 $1$。这为 $\\mathcal{L}_{\\text{inv}}$ 项提供了一个下界。\n\n最终的数值结果由一个实现这些步骤的程序计算得出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mixed objective for four self-supervised learning augmentation pipelines.\n    \"\"\"\n    # 1. Define constants and problem setup\n    n = 8\n    tau = 0.2\n    lambda_ = 0.1\n    W = np.array([[1.0, 0.5], [0.3, 1.2]])\n    rho_h = np.array([[1.0, 0.0], [0.0, -1.0]])\n    H_matrix = np.array([[1.0, 0.0], [0.0, -1.0]])\n\n    # 2. Dataset construction\n    thetas = 2 * np.pi * np.arange(n) / n\n    X_base = np.stack([np.cos(thetas), np.sin(thetas)], axis=1)\n\n    # 3. Encoder function\n    def f_W(x_batch):\n        return np.tanh(x_batch @ W.T)\n\n    # 4. Compute the constant equivariance penalty (L_eqv)\n    X_reflected = X_base @ H_matrix.T\n    f_X_base = f_W(X_base)\n    f_X_reflected = f_W(X_reflected)\n    rho_f_X_base = f_X_base @ rho_h.T\n\n    squared_deviations = np.sum((f_X_reflected - rho_f_X_base)**2, axis=1)\n    L_eqv = np.mean(squared_deviations)\n\n    # 5. Define pipelines and compute mixed objective for each\n    pipelines = {\n        \"A\": {\n            \"transform1\": lambda X: X @ np.array([[np.cos(0), -np.sin(0)], [np.sin(0), np.cos(0)]]).T,\n            \"transform2\": lambda X: X @ np.array([[np.cos(np.pi/2), -np.sin(np.pi/2)], [np.sin(np.pi/2), np.cos(np.pi/2)]]).T,\n        },\n        \"B\": {\n            \"transform1\": lambda X: 0.5 * X,\n            \"transform2\": lambda X: 2.0 * X,\n        },\n        \"C\": {\n            \"transform1\": lambda X: X @ np.array([[np.cos(np.pi), -np.sin(np.pi)], [np.sin(np.pi), np.cos(np.pi)]]).T,\n            \"transform2\": lambda X: X @ H_matrix.T,\n        },\n        \"D\": {\n            \"transform1\": lambda X: X,\n            \"transform2\": lambda X: X,\n        },\n    }\n\n    results = []\n    \n    for _, config in pipelines.items():\n        # a. Generate views\n        views1 = config[\"transform1\"](X_base)\n        views2 = config[\"transform2\"](X_base)\n\n        # b. Form batch, encode, and normalize\n        batch_views = np.concatenate([views1, views2], axis=0)\n        z = f_W(batch_views)\n        \n        # Add a small epsilon to the norm to prevent division by zero for null vectors\n        norm_z = np.linalg.norm(z, axis=1, keepdims=True)\n        u = z / (norm_z + 1e-9)\n\n        # c. Compute contrastive invariance objective (L_inv)\n        # Cosine similarity matrix\n        sim_matrix = u @ u.T\n        logits = sim_matrix / tau\n\n        # Use log-sum-exp trick for numerical stability\n        logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n        \n        # Mask to exclude self-similarity from denominator\n        mask = 1.0 - np.eye(2 * n)\n        \n        exp_logits = np.exp(logits_stable)\n        log_denom = np.log(np.sum(exp_logits * mask, axis=1))\n\n        # Identify positive pairs\n        # For a concatenated batch [v1_0..v1_n-1, v2_0..v2_n-1],\n        # positives for indices 0..n-1 are at n..2n-1\n        # positives for indices n..2n-1 are at 0..n-1\n        pos_indices = np.concatenate([np.arange(n, 2 * n), np.arange(0, n)])\n        \n        # Extract logits for positive pairs\n        pos_logits_stable = logits_stable[np.arange(2 * n), pos_indices]\n\n        # Log probability of positive pairs\n        log_probs = pos_logits_stable - log_denom\n        \n        L_inv = -np.mean(log_probs)\n\n        # d. Compute final mixed objective\n        L_total = L_inv + lambda_ * L_eqv\n        results.append(L_total)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3173218"}, {"introduction": "训练自监督模型并非总是一帆风顺，它可能会以一些微妙的方式失败，例如发生“表示坍塌”。学习曲线（即训练损失和验证性能随时间变化的图表）是强大的诊断工具，对比损失的急剧下降若未伴随下游任务准确率的提升，则可能预示着模型找到了一个平凡解。这项练习 [@problem_id:3115515] 让你像一名机器学习从业者一样，通过解读学习曲线来诊断一种常见的训练病症，并选择正确的干预措施，从而锻炼你将理论故障模式与其经验特征联系起来的关键技能。", "problem": "一个自监督对比学习系统在一个大型、无标签的图像数据集上进行训练，使用了一个标准的编码器和一个线性探针评估协议。设 $L(t)$ 表示在周期 $t$ 时的训练对比损失，该损失是基于小批量的经验风险计算得出的；设 $A_{\\mathrm{val}}(t)$ 表示在周期 $t$ 后，基于冻结的表示训练的线性分类器的下游验证准确率。假设使用学习率为常数 $\\eta$ 的随机梯度下降法，批量大小和所有优化器超参数随时间保持不变，并且在预训练期间不使用任何有标签的数据。编码器参数根据更新规则 $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)$ 进行演化。\n\n根据经验，观察到以下学习曲线：\n- 从周期 $t = 1$ 到 $t = 24$，$L(t)$ 从约 $4.2$ 逐渐下降到约 $1.9$。\n- 在周期 $t = 25$ 时，$L(t)$ 急剧下降至约 $0.06$，并在 $t \\in [26, 30]$ 期间保持在约 $0.03$ 和约 $0.08$ 之间。\n- 与此同时，$A_{\\mathrm{val}}(t)$ 从 $t = 1$ 时的约 $42\\%$ 增长到 $t = 20$ 时的约 $58\\%$，然后在 $t \\in [20, 30]$ 期间稳定在约 $57\\%$ 和约 $58\\%$ 之间，尽管在 $t = 25$ 之后 $L(t)$ 急剧下降。\n- 线性探针的训练准确率 $A_{\\mathrm{train}}(t)$ 从 $t = 1$ 时的约 $45\\%$ 持续上升到 $t = 30$ 时的约 $85\\%$。\n\n仅根据这些学习曲线提供的信息，并从基本定义——即 $L(t)$ 的经验风险最小化和由 $A_{\\mathrm{val}}(t)$ 衡量的下游泛化能力——出发，选择最合理的诊断和最正当的纠正措施，以降低表示退化的风险。你的选择应基于对以下问题的推理：对比目标如何在不提升下游语义的情况下被最小化，以及为什么特定的曲线形状（损失突然下降但没有下游增益）预示了这种风险。\n\nA. 系统表现出退化的对齐-均匀性失衡（表示崩溃风险）：正样本变得过于相似，而负样本不够多样化，导致 $L(t)$ 在不增加语义内容的情况下急剧下降。措施：加强数据增强，并提高对比温度 $\\tau$（和/或批量大小），以强调负样本间的均匀性；监控嵌入协方差以确认。\n\nB. 系统明显欠拟合：编码器容量不足，因此无法在提升 $A_{\\mathrm{val}}(t)$ 的同时平滑地降低 $L(t)$。措施：增加编码器的深度和宽度以提升容量。\n\nC. 下游线性探针正在过拟合有标签数据：$A_{\\mathrm{train}}(t)$ 上升而 $A_{\\mathrm{val}}(t)$ 停滞不前。措施：对探针应用提前停止和更强的正则化，并保持预训练不变。\n\nD. 优化器在周期 $t = 25$ 附近导致梯度消失，这解释了损失的急剧下降和 $A_{\\mathrm{val}}(t)$ 的停滞。措施：更换不同的激活函数和优化器以恢复梯度流，而不改变数据增强。", "solution": "我们的分析基于一个基本前提：训练对比损失 $L(t)$ 是一个经验风险，在梯度下降更新 $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)$ 下，其降低反映了在自监督代理目标上的改进；而下游验证准确率 $A_{\\mathrm{val}}(t)$ 则衡量了所学表示对于一个独立任务的泛化效用。在自监督对比学习中，目标是鼓励正样本对（同一实例的两个增强视图）的表示比负样本对（不同实例）的表示更接近。整体行为由对齐性（将正样本拉近）和均匀性（将所有点推开以避免集中）之间的平衡所决定。\n\n对于学习曲线的一个关键诊断原则是，在良好训练的情况下，如果表示所获得的增益是语义性的且非退化的，那么代理损失 $L(t)$ 的改善应与下游 $A_{\\mathrm{val}}(t)$ 相关。反之，如果 $L(t)$ 的急剧下降没有伴随 $A_{\\mathrm{val}}(t)$ 的提升——或者伴随着停滞——则表明模型找到了一种最小化代理目标的方法，但这种方法并没有用任务相关的信息来丰富表示。在对比学习框架中，这种退化通常发生在正样本变得过于相似（例如，由于数据增强太弱或设计不当）且负样本不够多样化（例如，由于批量大小太小或对比温度 $\\tau$ 太低，降低了对实例间小距离的惩罚）时，从而通过平凡的对齐在没有足够均匀性的情况下产生低损失值。这是一种表示崩溃风险，体现在信息内容减少和分布不佳（即使不是字面意义上崩溃到单个向量，表示也可能沿着少数几个方向集中），这与观察到的曲线一致：$L(t)$ 突然下降到接近零，而 $A_{\\mathrm{val}}(t)$ 在约 $58\\%$ 处停滞不前，此后不再提升。\n\n让我们从定量和概念上分析所提供的观察结果：\n- 从 $t = 1$ 到 $t = 24$，$L(t)$ 从约 $4.2$ 下降到约 $1.9$，这是代理目标的逐步改善。在同一时间窗口内，$A_{\\mathrm{val}}(t)$ 从约 $42\\%$ 增长到约 $58\\%$，这与学习非平凡特征是一致的。\n- 在 $t = 25$ 时，$L(t)$ 急剧下降至约 $0.06$，并在此后保持在约 $0.03$ 和约 $0.08$ 之间。如果这种显著的损失降低反映了真正改善的语义，人们会预期 $A_{\\mathrm{val}}(t)$ 会增加。然而，$A_{\\mathrm{val}}(t)$ 却在约 $57\\%$ 到 $58\\%$ 之间停滞，表明没有下游增益。\n- 与此同时，$A_{\\mathrm{train}}(t)$ 上升至约 $85\\%$，这表明探针可以在学习到的表示上拟合训练数据，但这并未转化为验证集上的增益。这种差异表明瓶颈在于表示本身，而不仅仅是探针的过拟合，因为 $A_{\\mathrm{val}}(t)$ 在损失急剧下降之前就已经停止提升，并且没有从随后的近零损失中受益。\n\n从第一性原理出发，最小化 $L(t)$ 的过程可以通过过度强调对齐性——使得正样本对在嵌入空间中几乎完全相同——而没有在实例间强制施加足够的均匀性来实现。当数据增强太弱（正样本视图过于相似）、温度 $\\tau$ 太低（过度锐化相似度分数）或有效负样本数量太少（批量大小小）时，就可能发生这种情况，使得模型能够在代理目标上轻易成功，同时丢弃了广泛的语义分离。在表示几何方面，这会产生集中的嵌入，其在许多方向上的协方差减小，从而降低了由 $A_{\\mathrm{val}}(t)$ 衡量的泛化能力。因此，这种曲线形状——$L(t)$ 突然下降而 $A_{\\mathrm{val}}(t)$ 没有同步增加——是类似崩溃风险的一个标志。\n\n逐项分析：\n\nA. 该选项指出了一个退化的对齐-均匀性失衡，并将其标记为表示崩溃风险。它将 $L(t)$ 急剧下降而没有下游改进解释为正样本变得过于相似和负样本多样性不足，这与对齐性和均匀性的定义以及关联 $L(t)$ 和 $A_{\\mathrm{val}}(t)$ 的诊断原则是一致的。提议的措施——加强数据增强和提高对比温度 $\\tau$（和/或批量大小）以增强均匀性压力——直接针对那些可能在没有语义增益的情况下产生低 $L(t)$ 的机制。监控嵌入协方差可以进一步检验是否存在集中现象，这是崩溃风险的一个标志。该推理与观察到的曲线和对比学习的基本行为相符。结论：正确。\n\nB. 欠拟合会表现为高 $L(t)$ 和低 $A_{\\mathrm{val}}(t)$，并且在增加容量时会有所改善。在这里，$L(t)$ 极低（约 $0.03$ 到约 $0.08$），表明模型可以轻松优化代理目标，且 $A_{\\mathrm{val}}(t)$ 在停滞前曾有所提升。增加容量无法解决在没有语义增益的情况下目标被最小化这种不匹配问题；甚至可能加剧平凡解。结论：错误。\n\nC. 探针过拟合的标志是 $A_{\\mathrm{train}}(t)$ 上升而 $A_{\\mathrm{val}}(t)$ 下降，同时表示质量本身足够好。然而，这里的关键事件是 $t = 25$ 时 $L(t)$ 的急剧下降并未带来 $A_{\\mathrm{val}}(t)$ 的改善。平台期在损失崩溃之前就已出现并持续，这指向表示本身的问题，而不仅仅是探针正则化的问题。提前停止或更强的探针正则化可能略微减小训练与验证的差距，但它们没有解决根本原因：表示携带的语义信息不足。结论：错误。\n\nD. 梯度消失通常会导致优化停滞（损失下降更慢或进入平台期），而不会产生到近零的急剧损失下降。此外，在优化器和超参数固定的情况下，$L(t)$ 突然下降并伴随 $A_{\\mathrm{val}}(t)$ 停滞的现象不能用激活饱和来解释；更换激活函数或优化器并不能解决曲线所指示的对齐-均匀性失衡问题。结论：错误。\n\n因此，基于学习曲线形状的最合理解释和纠正措施在选项 A 中给出。", "answer": "$$\\boxed{A}$$", "id": "3115515"}]}