{"hands_on_practices": [{"introduction": "要证明一个模型在某个输入邻域内的安全性，我们必须能够确定其在该邻域所有可能输入下的输出范围。区间边界传播（Interval Bound Propagation, IBP）是实现这一目标的基础技术，它通过逐层计算网络激活值的上下界，从而“包裹”住网络在该区域内的所有行为。通过这个实践 [@problem_id:3105264]，你将亲手实现一个针对ReLU网络的IBP算法，直观地理解输出边界是如何从输入层一步步传播到最终的输出层的。", "problem": "给定一个完全指定的三层整流线性单元（ReLU）网络，每层都包含仿射变换和其后的 ReLU 激活函数。该网络的输入维度为 $2$，其各层由权重矩阵和偏置向量 $(W_1,b_1)$、$(W_2,b_2)$、$(W_3,b_3)$ 定义如下。第一层将 $\\mathbb{R}^2$ 映射到 $\\mathbb{R}^3$，第二层将 $\\mathbb{R}^3$ 映射到 $\\mathbb{R}^2$，第三层将 $\\mathbb{R}^2$ 映射到 $\\mathbb{R}^2$。所有层之后都逐元素应用 ReLU 非线性函数。具体来说，层的参数如下：\n- $W_1 = \\begin{bmatrix} 1.0  -0.5 \\\\ -0.3  0.8 \\\\ 0.6  0.6 \\end{bmatrix}$ 和 $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.0 \\end{bmatrix}$，\n- $W_2 = \\begin{bmatrix} 0.5  -1.0  0.2 \\\\ 1.2  0.3  -0.4 \\end{bmatrix}$ 和 $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\end{bmatrix}$，\n- $W_3 = \\begin{bmatrix} 0.7  -0.6 \\\\ -0.5  0.9 \\end{bmatrix}$ 和 $b_3 = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$。\n\n输入向量 $x \\in \\mathbb{R}^2$ 受两个条件约束，这两个条件共同定义了容许扰动集。首先，$x$ 必须位于轴对齐的输入框 $[l,u]$ 内，其中 $l = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$ 且 $u = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$。其次，$x$ 必须位于给定标称输入 $x_0$ 周围半径为 $\\epsilon$ 的 $L_\\infty$ 球内，即 $\\|x - x_0\\|_\\infty \\le \\epsilon$。容许集是它们的交集 $\\mathcal{S} = \\{ x \\in \\mathbb{R}^2 \\mid l \\le x \\le u \\text{ and } \\|x - x_0\\|_\\infty \\le \\epsilon \\}$，其中不等式是逐坐标的。\n\n您的任务是实现区间边界传播（IBP），这是一个计算区间边界的过程，该边界包含了在集合 $\\mathcal{S}$ 上每一层所有可能的激活值。从 $\\mathcal{S}$ 的区间表示开始，将边界通过每个仿射层和 ReLU 函数进行传播。对于每一层 $k \\in \\{1,2,3\\}$，令 $z_k(x)$ 表示 ReLU 之前的预激活向量。将该层的可验证稳定性裕度定义为标量\n$$\nm_k \\triangleq \\min_{i} \\left( \\inf_{x \\in \\mathcal{S}} |(z_k(x))_i| \\right),\n$$\n该值是该层中所有神经元在容许集上其预激活值绝对值的下界的最小值。严格为正的 $m_k$ 证明了第 $k$ 层中的所有神经元在 $\\mathcal{S}$ 上都是符号稳定的，而 $m_k = 0$ 则表示至少有一个神经元可能改变符号。\n\n实现一个程序，该程序：\n- 构造有效输入区间 $[x_L, x_U]$，它通过 $[l,u]$ 和 $x_0$ 周围的 $L_\\infty$ 球的逐坐标交集来表示 $\\mathcal{S}$，即 $x_L = \\max(l, x_0 - \\epsilon)$ 和 $x_U = \\min(u, x_0 + \\epsilon)$。\n- 逐层应用区间边界传播，以获得 $z_1$、$z_2$ 和 $z_3$ 的预激活区间以及相应的 ReLU 激活后的区间。\n- 如上定义，计算每一层的可验证稳定性裕度 $m_k$。\n\n使用以下标称输入和扰动半径的测试套件：\n- 情况 1：$x_0 = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}$ 和 $\\epsilon = 0.1$。\n- 情况 2：$x_0 = \\begin{bmatrix} 0.95 \\\\ 0.95 \\end{bmatrix}$ 和 $\\epsilon = 0.1$。\n- 情况 3：$x_0 = \\begin{bmatrix} 0.2 \\\\ 0.8 \\end{bmatrix}$ 和 $\\epsilon = 0.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。该列表必须按顺序连接每个测试用例的三个层裕度进行排序：$[m_1^{(1)}, m_2^{(1)}, m_3^{(1)}, m_1^{(2)}, m_2^{(2)}, m_3^{(2)}, m_1^{(3)}, m_2^{(3)}, m_3^{(3)}]$，其中 $m_k^{(j)}$ 表示测试用例 $j$ 下第 $k$ 层的可验证稳定性裕度。每个条目必须是实数（浮点数）。此问题不涉及物理单位或角度单位，也不需要百分比。", "solution": "用户提供了一个在深度神经网络可验证鲁棒性领域中定义明确的计算问题。该问题是将区间边界传播（IBP）应用于一个给定的三层整流线性单元（ReLU）网络，以计算在各种输入条件下每一层的可验证稳定性裕度。该问题具有科学依据，提法恰当，并包含唯一解所需的所有必要信息。\n\n解决方案的步骤是首先建立输入集的区间表示，然后系统地将这些区间边界传播到网络的每一层，最后根据预激活边界计算所需的安全裕度。\n\n设网络函数表示为 $f(x)$，其中 $x \\in \\mathbb{R}^2$。该网络由三层组成。第 $k \\in \\{1, 2, 3\\}$ 层的输出表示为 $h_k(x)$，输入为 $h_0(x) = x$。每一层执行一次仿射变换，然后是逐元素的 ReLU 激活：\n$$z_k(x) = W_k h_{k-1}(x) + b_k$$\n$$h_k(x) = \\text{ReLU}(z_k(x)) = \\max(0, z_k(x))$$\n其中 $z_k(x)$ 是第 $k$ 层的预激活向量。\n\n区间边界传播的核心是将网络中任何向量（激活值或预激活值）的可能值集合表示为一个超矩形或一个区间。对于一个向量 $v$，其对应的可能值集合由一个下界向量 $v_L$ 和一个上界向量 $v_U$ 界定，使得对于所有可能的 $v$，不等式 $v_L \\le v \\le v_U$ 逐元素成立。\n\n### 步骤 1：有效输入区间\n输入 $x$ 被限制在一个容许集 $\\mathcal{S}$ 内，该集合是轴对齐框 $[l, u]$ 和围绕标称输入 $x_0$ 的半径为 $\\epsilon$ 的 $L_\\infty$ 球的交集。该集合由下式给出：\n$$\\mathcal{S} = \\{ x \\in \\mathbb{R}^2 \\mid l \\le x \\le u \\text{ and } \\|x - x_0\\|_\\infty \\le \\epsilon \\}$$\n$L_\\infty$ 约束 $\\|x - x_0\\|_\\infty \\le \\epsilon$ 等价于 $x_0 - \\epsilon \\mathbf{1} \\le x \\le x_0 + \\epsilon \\mathbf{1}$，其中 $\\mathbf{1}$ 是全一向量。集合 $\\mathcal{S}$ 本身就是一个超矩形，其边界 $[x_L, x_U]$ 可通过取两个约束区间的逐元素交集得到：\n$$x_L = \\max(l, x_0 - \\epsilon \\mathbf{1})$$\n$$x_U = \\min(u, x_0 + \\epsilon \\mathbf{1})$$\n这里，$\\max$ 和 $\\min$ 操作是逐元素执行的。这对 $[x_L, x_U]$ 构成了用于传播的初始区间。\n\n### 步骤 2：通过仿射层的传播\n给定仿射层（参数为 $(W, b)$）的输入 $h$ 的区间 $[h_{L}, h_{U}]$，我们需要计算预激活输出 $z = Wh + b$ 的区间 $[z_{L}, z_{U}]$。为了找到 $z$ 的最紧可能区间，我们可以独立计算 $z$ 的每个分量的下界和上界。对于第 $i$ 个分量 $z_i = \\sum_j W_{ij} h_j + b_i$，其边界为：\n$$\\inf(z_i) = \\sum_j \\inf(W_{ij} h_j) + b_i$$\n$$\\sup(z_i) = \\sum_j \\sup(W_{ij} h_j) + b_i$$\n$W_{ij} h_j$ 的极值取决于 $W_{ij}$ 的符号。如果 $W_{ij}  0$，则 $\\inf(W_{ij}h_j) = W_{ij}h_{j,L}$ 且 $\\sup(W_{ij}h_j) = W_{ij}h_{j,U}$。如果 $W_{ij}  0$，则 $\\inf(W_{ij}h_j) = W_{ij}h_{j,U}$ 且 $\\sup(W_{ij}h_j) = W_{ij}h_{j,L}$。\n\n这可以通过将权重矩阵 $W$ 分解为其正部和负部来用矩阵形式简洁地表示：$W^+ = \\max(W, 0)$ 和 $W^- = \\min(W, 0)$，其中操作是逐元素的。那么预激活向量 $z$ 的边界为：\n$$z_L = W^+ h_L + W^- h_U + b$$\n$$z_U = W^+ h_U + W^- h_L + b$$\n\n### 步骤 3：通过 ReLU 激活的传播\n给定预激活区间 $[z_L, z_U]$，我们计算 $h = \\text{ReLU}(z)$ 的后激活区间 $[h_L, h_U]$。由于 ReLU 函数 $\\text{ReLU}(v) = \\max(0,v)$ 是逐元素且单调非递减的，因此可以通过将该函数直接应用于区间端点来计算边界：\n$$h_L = \\text{ReLU}(z_L) = \\max(0, z_L)$$\n$$h_U = \\text{ReLU}(z_U) = \\max(0, z_U)$$\n这些操作是逐元素执行的。\n\n### 步骤 4：计算可验证稳定性裕度\n第 $k$ 层的可验证稳定性裕度定义为：\n$$m_k \\triangleq \\min_{i} \\left( \\inf_{x \\in \\mathcal{S}} |(z_k(x))_i| \\right)$$\nIBP 为每个预激活神经元 $(z_k(x))_i$ 提供了区间 $[(z_{k,L})_i, (z_{k,U})_i]$。我们需要找到位于区间 $[l_i, u_i] = [(z_{k,L})_i, (z_{k,U})_i]$ 内的变量 $v$ 的绝对值的下确界。这个下确界，我们称之为 $\\mu_i$，由区间端点的符号决定：\n1. 如果整个区间是非负的（$l_i \\ge 0$），则最接近零的值是 $l_i$。所以，$\\mu_i = l_i$。\n2. 如果整个区间是非正的（$u_i \\le 0$），则绝对值的范围从 $-u_i$ 到 $-l_i$。最小绝对值为 $-u_i$。所以，$\\mu_i = -u_i$。\n3. 如果区间包含零（$l_i  0  u_i$），则预激活值可能为零。因此，其绝对值的下确界为 0。所以，$\\mu_i = 0$。\n\n该层的稳定性裕度 $m_k$ 是这些单个神经元裕度的最小值：\n$$m_k = \\min_i \\mu_i$$\n裕度 $m_k  0$ 证明了第 $k$ 层中的所有神经元在整个输入集 $\\mathcal{S}$ 上保持相同的符号（是“稳定的”）。裕度 $m_k=0$ 表示至少有一个神经元是“不稳定的”并且可能改变其符号，意味着其预激活区间穿过零。\n\n整个算法包括从输入区间 $[x_L, x_U]$ 开始，对网络的每一层重复应用步骤 2 和步骤 3，并在每个预激活阶段计算裕度 $m_k$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Interval Bound Propagation problem for the given network and test cases.\n    \"\"\"\n\n    # Define the network parameters\n    W1 = np.array([[1.0, -0.5], [-0.3, 0.8], [0.6, 0.6]])\n    b1 = np.array([[0.1], [-0.2], [0.0]])\n    \n    W2 = np.array([[0.5, -1.0, 0.2], [1.2, 0.3, -0.4]])\n    b2 = np.array([[0.0], [0.05]])\n\n    W3 = np.array([[0.7, -0.6], [-0.5, 0.9]])\n    b3 = np.array([[0.1], [-0.1]])\n\n    network_params = [\n        (W1, b1),\n        (W2, b2),\n        (W3, b3)\n    ]\n\n    # Define the test cases\n    test_cases = [\n        (np.array([[0.6], [0.4]]), 0.1),  # Case 1\n        (np.array([[0.95], [0.95]]), 0.1), # Case 2\n        (np.array([[0.2], [0.8]]), 0.0),  # Case 3\n    ]\n\n    l_box = np.array([[0.0], [0.0]])\n    u_box = np.array([[1.0], [1.0]])\n\n    results = []\n\n    def propagate_affine(W, b, h_L, h_U):\n        \"\"\"Propagates interval bounds through an affine layer z = Wh + b.\"\"\"\n        W_pos = np.maximum(W, 0)\n        W_neg = np.minimum(W, 0)\n        \n        z_L = W_pos @ h_L + W_neg @ h_U + b\n        z_U = W_pos @ h_U + W_neg @ h_L + b\n        \n        return z_L, z_U\n\n    def propagate_relu(z_L, z_U):\n        \"\"\"Propagates interval bounds through a ReLU activation.\"\"\"\n        h_L = np.maximum(z_L, 0)\n        h_U = np.maximum(z_U, 0)\n        return h_L, h_U\n\n    def calculate_margin(z_L, z_U):\n        \"\"\"Calculates the certified stability margin for a layer.\"\"\"\n        neuron_margins = []\n        for l_i, u_i in zip(z_L, z_U):\n            if l_i = 0:\n                neuron_margins.append(l_i[0])\n            elif u_i = 0:\n                neuron_margins.append(-u_i[0])\n            else: # l_i  0  u_i\n                neuron_margins.append(0.0)\n        return min(neuron_margins)\n\n    for x0, epsilon in test_cases:\n        # Step 1: Compute the effective input interval\n        x_L = np.maximum(l_box, x0 - epsilon)\n        x_U = np.minimum(u_box, x0 + epsilon)\n\n        h_L_current, h_U_current = x_L, x_U\n        \n        # Propagate through all layers\n        for W, b in network_params:\n            # Propagate through affine layer\n            z_L, z_U = propagate_affine(W, b, h_L_current, h_U_current)\n\n            # Calculate and store the margin for the pre-activation\n            margin = calculate_margin(z_L, z_U)\n            results.append(margin)\n\n            # Propagate through ReLU activation for the next layer's input\n            h_L_current, h_U_current = propagate_relu(z_L, z_U)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3105264"}, {"introduction": "对抗训练是一种广为人知的提升模型鲁棒性的经验性方法，但它能提供可证明的安全保证吗？这个实践将经验性防御与形式化认证联系起来，通过引入模型的利普希茨常数（Lipschitz constant）$L$这一概念，我们可以从模型的输出间隔（margin）推导出确切的认证半径。在这个练习 [@problem_id:3105220] 中，你将运用该原理，通过计算对抗训练前后模型输出的logits，来量化评估对抗训练是否真正提升了模型的可证明鲁棒性。", "problem": "考虑一个由函数 $f:\\mathbb{R}^d \\to \\mathbb{R}^C$ 表示的多类分类器，该函数将输入 $x \\in \\mathbb{R}^d$ 映射到一个实值类别 logits 向量 $f(x) = (f_1(x),\\dots,f_C(x))$。对于一个给定的、正确类别索引为 $y \\in \\{1,\\dots,C\\}$ 的带标签样本 $(x,y)$，将 $x$ 的经验间隔定义为\n$$\nm(x) \\triangleq f_y(x) - \\max_{j \\neq y} f_j(x).\n$$\n假设对于所有 logit 差函数 $g_j(x) \\triangleq f_y(x) - f_j(x)$，关于输入范数 $\\|\\cdot\\|_2$ 存在一个固定的全局 Lipschitz 界 $L  0$ 一致成立，这意味着对于所有 $j \\neq y$ 和所有 $x, x' \\in \\mathbb{R}^d$，\n$$\n\\big|g_j(x) - g_j(x')\\big| \\le L \\,\\|x - x'\\|_2.\n$$\n投影梯度下降（PGD）是指一种名为投影梯度下降（PGD）的优化策略，它是一种在对抗训练中使用的对抗样本生成方法；使用 PGD 的对抗训练试图在范数有界的扰动集上最小化最坏情况损失。经验上，这种训练可以通过将正确类别的 logit 差值向上推，以对抗决策边界附近最强的竞争类别，从而增加间隔 $m(x)$。\n\n您的任务是：\n- 根据提供的训练前和训练后 logit 向量，计算使用投影梯度下降（PGD）进行对抗训练前后的经验间隔。\n- 对每个测试用例，数值上显示对抗训练是否增加了经验间隔。\n- 仅使用 Lipschitz 连续性的定义和间隔 $m(x)$，推导并实现一个在 $\\ell_2$ 输入范数下的可验证鲁棒性半径 $r(x)$，该半径保证对于任何满足 $\\|\\delta\\|_2 \\le r(x)$ 的扰动 $\\delta$，预测的类别 $y$ 不会改变。如果 $m(x) \\le 0$，则应报告证书为 $0$，因为当样本被误分类或结果打平时，无法做出鲁棒性保证。\n\n实现一个程序，对以下测试套件执行这些计算，固定全局 Lipschitz 界为 $L = 3.0$。每个测试用例包含正确类别索引 $y$、一个训练前 logit 向量 $f_{\\text{pre}}(x)$ 和一个训练后 logit 向量 $f_{\\text{post}}(x)$。类别索引是从0开始的。\n\n测试套件：\n1. $y=2$, $f_{\\text{pre}}(x) = [1.2,\\,1.8,\\,3.0,\\,0.5]$, $f_{\\text{post}}(x) = [1.1,\\,1.9,\\,5.0,\\,0.3]$.\n2. $y=0$, $f_{\\text{pre}}(x) = [2.0,\\,2.0,\\,1.0,\\,0.7]$, $f_{\\text{post}}(x) = [2.6,\\,2.0,\\,1.0,\\,0.7]$.\n3. $y=3$, $f_{\\text{pre}}(x) = [2.2,\\,2.5,\\,3.1,\\,2.0]$, $f_{\\text{post}}(x) = [2.3,\\,2.2,\\,2.4,\\,3.0]$.\n4. $y=1$, $f_{\\text{pre}}(x) = [0.2,\\,4.1,\\,1.8,\\,2.0]$, $f_{\\text{post}}(x) = [0.1,\\,4.3,\\,1.7,\\,1.9]$.\n5. $y=2$, $f_{\\text{pre}}(x) = [1.5,\\,1.6,\\,1.61,\\,1.60]$, $f_{\\text{post}}(x) = [1.4,\\,1.5,\\,1.9,\\,1.60]$.\n\n您的程序应该：\n- 对于每个测试用例，根据 logits 计算经验间隔 $m_{\\text{pre}}(x)$ 和 $m_{\\text{post}}(x)$，然后使用推导出的表达式计算可验证半径 $r_{\\text{pre}}(x)$ 和 $r_{\\text{post}}(x)$。如果 $m(x) \\le 0$，则返回 $r(x)=0$。\n- 为每个测试用例生成一个布尔指示器，如果 PGD 对抗训练后间隔增加（即 $m_{\\text{post}}(x)  m_{\\text{pre}}(x)$），则为 $true$，否则为 $false$。\n- 生成一个最终的布尔指示器，仅当所有测试用例的间隔都增加时才为 $true$。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含结果，格式为用方括号括起来的逗号分隔列表，无空格，结构如下\n$$\n[\\,[r_{\\text{pre},1},\\dots,r_{\\text{pre},5}],\\,[r_{\\text{post},1},\\dots,r_{\\text{post},5}],\\,[b_1,\\dots,b_5],\\,B_{\\text{all}}\\,],\n$$\n其中 $r_{\\text{pre},i}$ 和 $r_{\\text{post},i}$ 是浮点数，$b_i$ 是指示测试用例 $i$ 的间隔是否增加的布尔值，$B_{\\text{all}}$ 是指示所有测试用例的间隔均增加的布尔值。不涉及物理单位或角度单位。布尔值必须是编程语言呈现的布尔值 $true$ 或 $false$。确保嵌套和顺序完全匹配。", "solution": "问题要求我们计算一个多类分类器在对抗训练前后的经验间隔和可验证鲁棒性半径。我们获得了几个测试用例的训练前和训练后 logit 向量、一个正确的类别索引 $y$ 和一个全局 Lipschitz 常数 $L$。任务的核心是首先推导出可验证半径 $r(x)$ 的公式，然后将其应用于所提供的数据。\n\n首先，我们将为一个具有正确类别标签 $y \\in \\{0, \\dots, C-1\\}$ 的输入 $x \\in \\mathbb{R}^d$ 推导可验证鲁棒性半径 $r(x)$。分类器对给定输入 $x'$ 的预测是具有最高 logit 值的类别索引，即 $\\arg\\max_k f_k(x')$。如果对于所有 $j \\neq y$ 都有 $f_y(x)  f_j(x)$，则输入 $x$ 被正确分类。这等价于经验间隔 $m(x) \\triangleq f_y(x) - \\max_{j \\neq y} f_j(x)$ 为正 ($m(x)  0$) 的条件。\n\n一个可验证半径 $r(x)$ 保证了对于任何 $\\ell_2$ 范数小于或等于此半径的扰动 $\\delta$（即 $\\|\\delta\\|_2 \\le r(x)$），分类器对受扰动输入 $x' = x + \\delta$ 的预测仍然是正确的类别 $y$。在 $x'$ 处的预测为 $y$ 的条件是，对于所有竞争类别 $j \\neq y$，都有 $f_y(x')  f_j(x')$。\n\n我们为每个 $j \\neq y$ 定义 logit 差函数 $g_j(x) \\triangleq f_y(x) - f_j(x)$。在 $x'$ 处正确分类的条件变为对于所有 $j \\neq y$，$g_j(x')  0$。我们已知这些差函数关于 $\\ell_2$ 范数是 $L$-Lipschitz 的：\n$$\n|g_j(x) - g_j(x')| \\le L \\|x - x'\\|_2\n$$\n对所有 $x, x' \\in \\mathbb{R}^d$ 和所有 $j \\neq y$ 成立。代入 $x' = x + \\delta$，这变成 $|g_j(x) - g_j(x+\\delta)| \\le L \\|\\delta\\|_2$。这个不等式意味着在受扰动点 $x'$ 处 $g_j$ 的值有一个下界：\n$$\ng_j(x+\\delta) \\ge g_j(x) - L \\|\\delta\\|_2\n$$\n为保证在 $x'$ 处的分类保持正确，我们需要确保对所有 $j \\neq y$ 都有 $g_j(x+\\delta)  0$。使用下界，如果我们能确保以下条件，则该条件得到满足：\n$$\ng_j(x) - L \\|\\delta\\|_2  0 \\quad \\text{for all } j \\neq y\n$$\n这必须对任何满足 $\\|\\delta\\|_2 \\le r(x)$ 的扰动 $\\delta$ 成立。最具挑战性的扰动是使差值 $g_j(x+\\delta)$ 最大程度减小的扰动，这对应于最大范数 $\\|\\delta\\|_2 = r(x)$。因此，我们必须满足：\n$$\ng_j(x) - L \\cdot r(x)  0 \\quad \\text{for all } j \\neq y\n$$\n求解 $r(x)$，我们得到 $r(x)  \\frac{g_j(x)}{L}$。为了对所有 $j \\neq y$ 都满足这个条件，半径必须小于这些界的最小值：\n$$\nr(x)  \\min_{j \\neq y} \\frac{g_j(x)}{L} = \\frac{1}{L} \\min_{j \\neq y} g_j(x)\n$$\n项 $\\min_{j \\neq y} g_j(x)$ 可以展开为：\n$$\n\\min_{j \\neq y} (f_y(x) - f_j(x)) = f_y(x) - \\max_{j \\neq y} f_j(x)\n$$\n这正是经验间隔 $m(x)$ 的定义。因此，条件是 $r(x)  \\frac{m(x)}{L}$。保证成立的最大可能半径 $r(x)$ 是这个区间的上界：\n$$\nr(x) = \\frac{m(x)}{L}\n$$\n问题规定，如果间隔为非正值，$m(x) \\le 0$，则可验证半径为 $0$。这是因为如果输入已经被误分类或位于决策边界上，即使对于无穷小的扰动也无法做出任何保证。这导出了可验证半径的最终表达式：\n$$\nr(x) = \\max\\left(0, \\frac{m(x)}{L}\\right) = \\max\\left(0, \\frac{f_y(x) - \\max_{j \\neq y} f_j(x)}{L}\\right)\n$$\n有了这个推导出的公式，我们将继续进行问题的计算部分。对于每个测试用例，我们都给出了正确的类别索引 $y$、一个训练前 logit 向量 $f_{\\text{pre}}(x)$、一个训练后 logit 向量 $f_{\\text{post}}(x)$ 和一个 Lipschitz 常数 $L=3.0$。\n\n每个测试用例的流程如下：\n1.  对于 $f_{\\text{pre}}(x)$ 和 $f_{\\text{post}}(x)$，使用其定义计算经验间隔 $m(x)$。这需要找到正确类别 $y$ 的 logit，并减去所有其他类别的最大 logit。\n2.  使用计算出的间隔 $m(x)$ 和给定的 Lipschitz 常数 $L=3.0$，利用公式 $r(x) = \\max(0, m(x)/L)$ 计算可验证半径 $r(x)$。这将得到 $r_{\\text{pre}}(x)$ 和 $r_{\\text{post}}(x)$。\n3.  比较间隔以确定对抗训练是否导致其增加。如果 $m_{\\text{post}}(x)  m_{\\text{pre}}(x)$，则布尔指示器 $b_i$ 设置为 $true$，否则为 $false$。\n4.  最后，确定一个全局布尔指示器 $B_{\\text{all}}$，当且仅当所有测试用例的间隔都增加时（即对所有 $i$，$b_i$ 都为 $true$），它才为 $true$。\n\n结果将被编译成最终输出所需的基于列表的格式。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the certified robustness problem by calculating margins and radii\n    for pre- and post-training logits, and determines if margins increased.\n    \"\"\"\n    \n    # Global Lipschitz constant\n    L = 3.0\n    \n    # Test suite data: (y, f_pre, f_post)\n    test_cases = [\n        (2, [1.2, 1.8, 3.0, 0.5], [1.1, 1.9, 5.0, 0.3]),\n        (0, [2.0, 2.0, 1.0, 0.7], [2.6, 2.0, 1.0, 0.7]),\n        (3, [2.2, 2.5, 3.1, 2.0], [2.3, 2.2, 2.4, 3.0]),\n        (1, [0.2, 4.1, 1.8, 2.0], [0.1, 4.3, 1.7, 1.9]),\n        (2, [1.5, 1.6, 1.61, 1.60], [1.4, 1.5, 1.9, 1.60]),\n    ]\n\n    def calculate_margin_and_radius(logits, y, L_val):\n        \"\"\"\n        Computes the empirical margin and certified radius for a given logit vector.\n        \n        Args:\n            logits (np.ndarray): The vector of class logits.\n            y (int): The index of the correct class.\n            L_val (float): The Lipschitz constant.\n            \n        Returns:\n            tuple: A tuple containing the margin (float) and radius (float).\n        \"\"\"\n        logits_np = np.array(logits)\n        \n        # Logit of the correct class\n        f_y = logits_np[y]\n        \n        # Maximum logit of other classes\n        other_logits = np.delete(logits_np, y)\n        max_f_j = np.max(other_logits)\n        \n        # Empirical margin\n        margin = f_y - max_f_j\n        \n        # Certified radius\n        radius = max(0.0, margin / L_val)\n        \n        return margin, radius\n\n    r_pre_list = []\n    r_post_list = []\n    b_list = []\n\n    for y_true, f_pre, f_post in test_cases:\n        # Calculate for pre-training logits\n        m_pre, r_pre = calculate_margin_and_radius(f_pre, y_true, L)\n        r_pre_list.append(r_pre)\n        \n        # Calculate for post-training logits\n        m_post, r_post = calculate_margin_and_radius(f_post, y_true, L)\n        r_post_list.append(r_post)\n        \n        # Check if margin increased\n        margin_increased = m_post  m_pre\n        b_list.append(margin_increased)\n\n    # Check if margin increased for all cases\n    B_all = all(b_list)\n\n    # Format the output string exactly as specified\n    r_pre_str = ','.join(map(str, r_pre_list))\n    r_post_str = ','.join(map(str, r_post_list))\n    b_list_str = ','.join(str(b).lower() for b in b_list)\n    b_all_str = str(B_all).lower()\n\n    final_output = f\"[[{r_pre_str}],[{r_post_str}],[{b_list_str}],{b_all_str}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3105220"}, {"introduction": "虽然区间边界传播（IBP）简单高效，但其计算出的边界往往过于宽松，导致无法认证本应鲁棒的模型。为了获得更精确的认证结果，我们可以采用分支定界（Branch-and-Bound）这一更强大的算法，它通过递归地划分输入空间并剪除已确认安全的分支来逐步收紧边界。在这个实践 [@problem_id:3105282] 中，你将构建一个带有时限预算的分支定界验证器，并探索不同的划分策略如何影响验证效率，从而深入理解在实际验证任务中精度与计算成本之间的权衡。", "problem": "给定一个由前馈修正线性单元（ReLU）网络实现的二元分类间隔函数。输入的间隔定义为预测类别的 logit 与次优类别的 logit 之间的差值。考虑一个输入向量 $x \\in \\mathbb{R}^d$，以及一个由以下公式定义的单隐藏层 ReLU 网络：\n$$\nm(x) = c^\\top \\,\\sigma\\!\\left(W_1 x + b_1\\right) + b_0,\n$$\n其中 $W_1 \\in \\mathbb{R}^{h \\times d}$ 是第一层的权重矩阵，$b_1 \\in \\mathbb{R}^h$ 是第一层的偏置向量，$\\sigma(\\cdot)$ 是逐元素的修正线性单元激活函数 $\\sigma(z) = \\max(0,z)$，$c \\in \\mathbb{R}^h$ 是两个类别（真实类别减去次优类别）的第二层权重向量之差，$b_0 \\in \\mathbb{R}$ 是第二层偏置之差。\n\n可验证鲁棒性要求证明，对于给定的中心点 $x_0$ 和在无穷范数下的半径 $r \\ge 0$，对于闭合 $\\ell_\\infty$ 球\n$$\n\\mathcal{B}_\\infty(x_0,r) = \\{ x \\in \\mathbb{R}^d \\mid \\|x - x_0\\|_\\infty \\le r \\}\n$$\n内的所有 $x$，间隔 $m(x)$ 保持严格为正。\n如果能够计算出 $m(x)$ 在 $\\mathcal{B}_\\infty(x_0,r)$ 上的最小值的可靠下界 $L(r)$，并且 $L(r)  0$，那么 $r$ 就是 $x_0$ 的一个可验证半径。\n\n您的任务是实现一个具有固定时间预算的分支定界验证器，使用区间边界传播（IBP）来限定 $m(x)$ 在轴对齐框上的边界。该算法必须：\n- 将域 $\\mathcal{B}_\\infty(x_0,r)$ 表示为一个初始的轴对齐框 $[l,u]$，其中 $l = x_0 - r$ 且 $u = x_0 + r$，一次沿着一个输入维度分割框，并维持一个关于 $m(x)$ 的全局下界，该下界是所有活动框的下界中的最小值。\n- 使用区间边界传播如下。对于一个框 $[l,u]$，每个预激活坐标 $z_i = (W_1 x + b_1)_i$ 位于一个通过区间算术计算出的区间 $[z_i^{\\mathrm{L}}, z_i^{\\mathrm{U}}]$ 内，\n$$\nz_i^{\\mathrm{L}} = b_{1,i} + \\sum_{j=1}^d \\left( \\min\\{ w_{ij} l_j, w_{ij} u_j \\} \\right), \\quad\nz_i^{\\mathrm{U}} = b_{1,i} + \\sum_{j=1}^d \\left( \\max\\{ w_{ij} l_j, w_{ij} u_j \\} \\right),\n$$\n其中 $w_{ij}$ 表示 $W_1$ 的 $(i,j)$ 项。经过修正线性单元后，激活区间为 $[\\alpha_i^{\\mathrm{L}}, \\alpha_i^{\\mathrm{U}}] = [\\max(0, z_i^{\\mathrm{L}}), \\max(0, z_i^{\\mathrm{U}})]$。框上的间隔满足\n$$\nm(x) \\in \\left[\\, b_0 + \\sum_{i=1}^h \\left( c_i \\cdot \\alpha_i \\right)_{\\min},\\; b_0 + \\sum_{i=1}^h \\left( c_i \\cdot \\alpha_i \\right)_{\\max} \\,\\right],\n$$\n其中，对于每个 $i$，最小贡献为 $\\left( c_i \\cdot \\alpha_i \\right)_{\\min} = \\begin{cases} c_i \\alpha_i^{\\mathrm{L}}  \\text{if } c_i \\ge 0, \\\\ c_i \\alpha_i^{\\mathrm{U}}  \\text{if } c_i  0, \\end{cases}$，最大贡献为 $\\left( c_i \\cdot \\alpha_i \\right)_{\\max} = \\begin{cases} c_i \\alpha_i^{\\mathrm{U}}  \\text{if } c_i \\ge 0, \\\\ c_i \\alpha_i^{\\mathrm{L}}  \\text{if } c_i  0. \\end{cases}$ 框上 $m(x)$ 的下界是左端点，上界是右端点。\n\n分支定界验证器应：\n- 维护一组框，每个框都有其关于 $m(x)$ 的区间下界和上界。如果全局下界（该组中最小的下界）超过 $0$，验证器声明当前半径已通过验证。如果全局上界（该组中最大的上界）不足以推翻鲁棒性，验证器继续分割不确定的框。\n- 使用固定的扩展步骤预算，每次扩展用在中间点沿所选输入坐标分割的两个框替换一个框。选择下一个要扩展的框必须是下界最小的那个，以优先处理最坏情况的子域来最小化 $m(x)$。\n- 实现两种选择要分割的输入坐标 $j$ 的分割启发式方法：\n  1. 基于宽度的启发式方法 $\\mathsf{H}_{\\mathrm{width}}$：选择使 $u_j - l_j$ 最大化的 $j$（通过最小索引解决平局）。\n  2. 基于影响力的启发式方法 $\\mathsf{H}_{\\mathrm{influence}}$：将坐标 $j$ 的影响力分数定义为\n  $$\n  I_j([l,u]) = (u_j - l_j) \\sum_{i=1}^h \\left( |c_i| \\cdot |w_{ij}| \\cdot \\mathbf{1}\\{ z_i^{\\mathrm{U}}  0 \\} \\right),\n  $$\n  其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，$[z_i^{\\mathrm{L}}, z_i^{\\mathrm{U}}]$ 如上文所述从 $[l,u]$ 计算得出。选择使 $I_j([l,u])$ 最大化的 $j$（通过最小索引解决平局）。\n  \n随时认证是通过在相同的固定预算内逐步尝试更大的半径来衡量的。对于一个候选半径列表 $r_1  r_2  \\dots  r_K$，验证器尝试使用至多每次尝试的扩展预算来验证 $r_1$，如果成功，则尝试 $r_2$，依此类推，直到总扩展预算耗尽或某个候选半径未能验证。为输入 $x_0$ 报告的可验证半径是在预算内成功验证的最大 $r_k$。此过程在每种启发式方法下分别运行，并比较所得的可验证半径。\n\n为以下具体的测试套件实现上述功能。使用输入维度 $d = 2$ 和隐藏维度 $h = 3$，参数如下\n$$\nW_1 = \\begin{bmatrix}\n1.0  -0.5 \\\\\n-0.8  1.0 \\\\\n0.6  0.6\n\\end{bmatrix},\\quad\nb_1 = \\begin{bmatrix}\n0.1 \\\\\n-0.2 \\\\\n0.0\n\\end{bmatrix},\\quad\nc = \\begin{bmatrix}\n1.2 \\\\\n-0.7 \\\\\n0.9\n\\end{bmatrix},\\quad\nb_0 = 0.05.\n$$\n使用候选半径列表\n$$\n\\{\\, 0.00,\\; 0.05,\\; 0.10,\\; 0.15,\\; 0.20,\\; 0.30 \\,\\}.\n$$\n对每个输入使用 $120$ 次扩展的总扩展预算，以及每次尝试 $20$ 次扩展的预算（因此每个输入最多可以进行 $6$ 次尝试）。测试输入为三个中心点\n$$\nx_0^{(1)} = \\begin{bmatrix} 0.5 \\\\ -0.3 \\end{bmatrix},\\quad\nx_0^{(2)} = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix},\\quad\nx_0^{(3)} = \\begin{bmatrix} 0.8 \\\\ 0.8 \\end{bmatrix}.\n$$\n选择这些案例是为了覆盖：一个典型案例（第一个），一个间隔较小的近边界案例（第二个），以及一个两个坐标都产生正贡献的边缘案例（第三个）。\n\n您的程序必须：\n- 使用所描述的区间边界传播实现分支定界验证器。\n- 对于每个 $x_0^{(i)}$，在遵守固定预算的情况下，计算在 $\\mathsf{H}_{\\mathrm{width}}$ 和 $\\mathsf{H}_{\\mathrm{influence}}$ 下的可验证半径。\n- 为每种启发式方法计算三种情况下的平均可验证半径。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含两个平均值，格式为用方括号括起来的逗号分隔列表，顺序为 $[\\text{在 } \\mathsf{H}_{\\mathrm{width}} \\text{ 下的平均值}, \\text{在 } \\mathsf{H}_{\\mathrm{influence}} \\text{ 下的平均值}]$。为清晰起见，两个值都必须打印为浮点数。", "solution": "用户提供了一个有效的问题陈述。任务是实现一个分支定界验证器，以确定一个小型神经网络在 $\\ell_\\infty$ 范数下对抗性扰动的可验证鲁棒半径。解决方案将基于可验证鲁棒性、区间边界传播和预算搜索算法的原则系统地进行开发。\n\n### 1. 可验证鲁棒性原则\n\n可验证鲁棒性的核心目标是提供一个形式化保证，即对于给定数据点 $x_0$ 的指定邻域内的任何输入，机器学习模型的预测保持不变。对于一个二元分类间隔函数 $m(x)$，这意味着要证明对于扰动集中的所有 $x$，$m(x)$ 保持为正。问题将此集合定义为半径为 $r$ 的 $\\ell_\\infty$ 球 $\\mathcal{B}_\\infty(x_0,r)$，这是一个以 $x_0$ 为中心的轴对齐的超矩形（或框）。\n\n为了证明对于所有 $x \\in \\mathcal{B}_\\infty(x_0,r)$ 都有 $m(x)  0$，我们必须找到 $m(x)$ 在此域内的全局最小值。如果这个最小值严格为正，则网络在该半径 $r$ 下被验证为鲁棒的。然而，找到神经网络函数的精确最小值是一个 NP-hard 问题。因此，我们采用可靠的松弛方法，计算一个保证的下界，$L(r) \\le \\min_{x \\in \\mathcal{B}_\\infty(x_0,r)} m(x)$。如果我们能证明 $L(r)  0$，那么就可以推断出真实的最小值也为正，网络是鲁棒的。\n\n### 2. 分支定界框架\n\n分支定界算法是解决全局优化问题的一种系统性方法。它基于递归地划分搜索空间并剪除不可能包含最优解的分区的原则进行操作。\n\n1.  **域表示**：初始域 $\\mathcal{B}_\\infty(x_0,r)$ 表示为单个框 $[l, u]$，其中 $l = x_0-r$ 且 $u=x_0+r$。\n2.  **定界**：对于每个框，我们计算间隔函数 $m(x)$ 的一个下界 $m^{\\mathrm{L}}$ 和一个上界 $m^{\\mathrm{U}}$。此方法是区间边界传播（IBP），详见下文。\n3.  **分支（分割）**：如果一个框的下界 $m^{\\mathrm{L}} \\le 0$，我们将其分割成更小的子框。在此问题中，一个框沿单个选定维度的中点被分割。这将创建两个新的、更小的框，它们覆盖了父框的域。\n4.  **搜索策略**：为了高效地找到全局最小值，我们必须优先探索最有希望的区域。使用一个最小优先队列来存储所有活动框，按其下界 $m^{\\mathrm{L}}$ 进行优先级排序。在每一步中，我们从队列中选择具有最小下界的框，因为这是最有可能包含真实全局最小值的区域。这是“分支”步骤。\n5.  **终止**：该过程持续进行，直到满足以下两个条件之一：\n    a. 在所有活动框中发现的最小下界大于 $0$。在这种情况下，该属性被验证为在整个域上为真。\n    b. 预定义的计算预算（例如，框扩展的数量）已用尽。如果此时最小下界不大于 $0$，则验证器在给定预算内未能验证该属性。\n\n### 3. 区间边界传播（IBP）\n\nIBP是一种用于计算函数在给定输入区间上的输出边界的方法。它通过逐层在网络中传播区间来工作。对于给定的输入框 $[l,u]$，其中 $x \\in [l,u]$（逐元素）：\n\n1.  **仿射层边界**：预激活值由仿射变换 $z = W_1 x + b_1$ 给出。$z$ 的最紧区间 $[z^{\\mathrm{L}}, z^{\\mathrm{U}}]$ 可以使用区间算术计算。为提高效率，公式可以向量化。设 $W_1^+ = \\max(0, W_1)$ 和 $W_1^- = \\min(0, W_1)$ 分别为权重矩阵的正部和负部。边界为：\n    $$\n    z^{\\mathrm{L}} = W_1^+ l + W_1^- u + b_1 \\\\\n    z^{\\mathrm{U}} = W_1^+ u + W_1^- l + b_1\n    $$\n2.  **ReLU 激活边界**：ReLU 函数 $\\sigma(z) = \\max(0,z)$ 逐元素应用。后激活值 $\\alpha = \\sigma(z)$ 的区间为：\n    $$\n    [\\alpha^{\\mathrm{L}}, \\alpha^{\\mathrm{U}}] = [\\max(0, z^{\\mathrm{L}}), \\max(0, z^{\\mathrm{U}})]\n    $$\n    这里，$\\max(0, z^{\\mathrm{L}})$ 是对向量 $z^{\\mathrm{L}}$ 的逐元素操作。\n3.  **输出间隔边界**：最终间隔为 $m(x) = c^\\top \\alpha + b_0$。我们应用相同的区间算术原理。设 $c^+ = \\max(0, c)$ 和 $c^- = \\min(0, c)$。间隔的下界 $m^{\\mathrm{L}}$ 和上界 $m^{\\mathrm{U}}$ 为：\n    $$\n    m^{\\mathrm{L}} = c^{+\\top} \\alpha^{\\mathrm{L}} + c^{-\\top} \\alpha^{\\mathrm{U}} + b_0 \\\\\n    m^{\\mathrm{U}} = c^{+\\top} \\alpha^{\\mathrm{U}} + c^{-\\top} \\alpha^{\\mathrm{L}} + b_0\n    $$\n\n### 4. 分割启发式方法\n\n分支定界的效率在很大程度上取决于每一步选择哪个维度进行分割。一个好的启发式方法会优先选择能导致全局下界最快增长的分割。\n\n1.  **基于宽度的启发式方法（$\\mathsf{H}_{\\mathrm{width}}$）**：这种简单的启发式方法选择具有最大宽度的维度 $j$，即 $j = \\arg\\max_k (u_k - l_k)$。其直觉是，减小框的最大维度是减少不确定性的一种通用策略。\n\n2.  **基于影响力的启发式方法（$\\mathsf{H}_{\\mathrm{influence}}$）**：这是一种更具信息量的启发式方法，它考虑了网络的结构。它旨在识别对输出间隔不确定性影响最大的输入维度。输入维度 $j$ 的影响力分数为：\n    $$\n    I_j([l,u]) = (u_j - l_j) \\sum_{i=1}^h \\left( |c_i| \\cdot |w_{ij}| \\cdot \\mathbf{1}\\{ z_i^{\\mathrm{U}}  0 \\} \\right)\n    $$\n    -   $(u_j - l_j)$ 考虑了沿维度 $j$ 的输入不确定性。\n    -   $|w_{ij}|$ 和 $|c_i|$ 解释了输出对输入 $x_j$ 变化的敏感度，该变化通过隐藏神经元 $i$ 传播。\n    -   指示函数 $\\mathbf{1}\\{ z_i^{\\mathrm{U}}  0 \\}$ 将启发式方法聚焦于在框 $[l,u]$ 内“不稳定”或“稳定开启”的神经元。如果一个神经元的预激活上界 $z_i^{\\mathrm{U}}$ 为非正，则其激活在整个框上是常数（$0$），因此分割一个输入维度不会影响其对最终间隔边界的贡献。这种启发式方法优先分割那些影响神经元不确定性对最终间隔影响最大的维度。\n\n### 5. 随时认证和预算管理\n\n在实践中，验证的计算成本高昂。问题指定了一种“随时”方法，结合预算管理，以在固定的计算限制内找到可能的最大可验证半径。\n\n-   一个排序的候选半径列表 $r_1  r_2  \\dots  r_K$ 被顺序地尝试。\n-   算法首先尝试验证最小的半径 $r_1$。这次尝试受到`每次尝试预算`的扩展次数限制。\n-   如果对 $r_k$ 的验证成功，算法继续尝试验证下一个半径 $r_{k+1}$，同样有`每次尝试预算`。`总预算`在所有尝试中累积。\n-   当一个半径未能被验证（在这种情况下，先前验证的半径是最终结果）或`总预算`的扩展次数耗尽时，过程终止。对于 $r=0$，检查是微不足道的：如果 $m(x_0)>0$，它就以 0 次扩展被验证。\n\n这种结构允许验证器快速返回一个有用的、非零的可验证半径，然后在时间允许的情况下逐步改进它。\n实现将封装此逻辑，为每次半径尝试调用核心的分支定界验证器，并管理预算。最终结果是为每个测试输入找到的最大可验证半径的平均值，对两种启发式方法分别计算。", "answer": "```python\nimport numpy as np\nimport heapq\n\n# Define network parameters and problem constants globally\nW1 = np.array([[1.0, -0.5], [-0.8, 1.0], [0.6, 0.6]])\nB1 = np.array([0.1, -0.2, 0.0])\nC = np.array([1.2, -0.7, 0.9])\nB0 = 0.05\nD_IN = 2\nH_DIM = 3\n\n# Pre-compute positive and negative parts of weight matrices for IBP\nW1_POS = np.maximum(0, W1)\nW1_NEG = np.minimum(0, W1)\nC_POS = np.maximum(0, C)\nC_NEG = np.minimum(0, C)\n\nclass Box:\n    \"\"\"Represents an axis-aligned box [l, u] with its margin bounds.\"\"\"\n    _box_counter = 0\n\n    def __init__(self, l, u):\n        self.l = l\n        self.u = u\n        self.lower_bound, self.upper_bound = self.compute_bounds()\n        self.id = Box._box_counter\n        Box._box_counter += 1\n\n    def compute_bounds(self):\n        \"\"\"Computes margin bounds using Interval Bound Propagation (IBP).\"\"\"\n        # Layer 1 (Affine)\n        z_L = W1_POS @ self.l + W1_NEG @ self.u + B1\n        z_U = W1_POS @ self.u + W1_NEG @ self.l + B1\n        \n        # Layer 1 (ReLU)\n        alpha_L = np.maximum(0, z_L)\n        alpha_U = np.maximum(0, z_U)\n        \n        # Layer 2 (Output Margin)\n        lower = C_POS @ alpha_L + C_NEG @ alpha_U + B0\n        upper = C_POS @ alpha_U + C_NEG @ alpha_L + B0\n        return lower, upper\n\n    def __lt__(self, other):\n        \"\"\"Comparison for priority queue based on lower bound.\"\"\"\n        if self.lower_bound == other.lower_bound:\n            return self.id  other.id\n        return self.lower_bound  other.lower_bound\n\ndef select_split_dim(box, heuristic):\n    \"\"\"Selects the dimension to split based on the given heuristic.\"\"\"\n    if heuristic == 'width':\n        widths = box.u - box.l\n        return np.argmax(widths)\n    \n    elif heuristic == 'influence':\n        # Recompute z_U for the current box\n        z_U = W1_POS @ box.u + W1_NEG @ box.l + B1\n        is_relevant_neuron = (z_U  0).astype(float)\n        \n        # Calculate per-neuron influence factors\n        neuron_influence = np.abs(C) * is_relevant_neuron\n        \n        # Aggregate influence per input dimension\n        # A_ij = |c_i| * |w_ij| * indicator\n        # Column sum of A gives for each j: sum_i |c_i|*|w_ij|*indicator\n        # Equivalent to W1.T @ neuron_influence\n        input_influence = np.abs(W1).T @ neuron_influence\n        \n        # Final scores\n        scores = (box.u - box.l) * input_influence\n        return np.argmax(scores)\n    else:\n        raise ValueError(\"Unknown heuristic\")\n\ndef branch_and_bound(initial_box, max_expansions, heuristic):\n    \"\"\"\n    Performs branch-and-bound verification for a single radius.\n    \n    Returns a tuple (is_verified, expansions_used).\n    \"\"\"\n    if initial_box.lower_bound  0:\n        return True, 0\n\n    pq = [initial_box]\n    expansions_used = 0\n\n    while pq and expansions_used  max_expansions:\n        # Get box with the smallest lower bound\n        current_box = heapq.heappop(pq)\n        \n        # If the minimum lower bound in the entire domain is positive, we are done\n        if current_box.lower_bound  0:\n            return True, expansions_used\n\n        expansions_used += 1\n\n        # Select dimension to split\n        split_dim = select_split_dim(current_box, heuristic)\n        \n        # Split the box at the midpoint\n        midpoint = (current_box.l[split_dim] + current_box.u[split_dim]) / 2.0\n        \n        # Create first child box\n        l1, u1 = np.copy(current_box.l), np.copy(current_box.u)\n        u1[split_dim] = midpoint\n        box1 = Box(l1, u1)\n        if box1.lower_bound = 0:\n            heapq.heappush(pq, box1)\n\n        # Create second child box\n        l2, u2 = np.copy(current_box.l), np.copy(current_box.u)\n        l2[split_dim] = midpoint\n        box2 = Box(l2, u2)\n        if box2.lower_bound = 0:\n            heapq.heappush(pq, box2)\n\n    # After loop, check if the smallest lower bound in the remaining queue is positive\n    if not pq or pq[0].lower_bound  0:\n        return True, expansions_used\n    \n    return False, expansions_used\n\ndef run_anytime_certification(x0, radii, total_budget, per_attempt_budget, heuristic):\n    \"\"\"\n    Runs the anytime certification procedure for a given input and heuristic.\n    \"\"\"\n    certified_radius = 0.0\n    total_expansions_so_far = 0\n\n    # Ensure radii are sorted\n    sorted_radii = sorted(radii)\n\n    for r in sorted_radii:\n        remaining_total = total_budget - total_expansions_so_far\n        if remaining_total = 0 and r  0:\n            break\n            \n        budget_for_this_attempt = min(per_attempt_budget, remaining_total)\n        \n        # For r=0, the domain is a single point.\n        if r == 0:\n            m_x0 = (C @ np.maximum(0, W1 @ x0 + B1)) + B0\n            if m_x0  0:\n                certified_radius = 0.0\n                continue\n            else:\n                break\n\n        # Create the initial box for the given radius\n        l, u = x0 - r, x0 + r\n        initial_box = Box(l, u)\n        \n        is_verified, expansions_this_attempt = branch_and_bound(initial_box, budget_for_this_attempt, heuristic)\n        \n        total_expansions_so_far += expansions_this_attempt\n\n        if is_verified:\n            certified_radius = r\n        else:\n            # If a radius fails, no larger radius can be certified\n            break\n            \n    return certified_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases and print the result.\n    \"\"\"\n    test_cases = [\n        np.array([0.5, -0.3]),\n        np.array([0.1, 0.1]),\n        np.array([0.8, 0.8]),\n    ]\n    \n    radii_list = [0.00, 0.05, 0.10, 0.15, 0.20, 0.30]\n    total_exp_budget = 120\n    per_attempt_exp_budget = 20\n    \n    heuristics = ['width', 'influence']\n    avg_results = []\n    \n    for heuristic in heuristics:\n        certified_radii_for_heuristic = []\n        for x0 in test_cases:\n            # Reset box counter for deterministic behavior between runs\n            Box._box_counter = 0\n            radius = run_anytime_certification(\n                x0, radii_list, total_exp_budget, per_attempt_exp_budget, heuristic\n            )\n            certified_radii_for_heuristic.append(radius)\n        \n        avg_radius = np.mean(certified_radii_for_heuristic)\n        avg_results.append(avg_radius)\n        \n    print(f\"[{','.join(map(str, avg_results))}]\")\n\nsolve()\n\n```", "id": "3105282"}]}