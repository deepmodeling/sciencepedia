{"hands_on_practices": [{"introduction": "为了开启我们的动手实践，我们首先要回答一个基本问题：为什么要进行模型压缩？本练习将通过一个量化分析来阐明压缩的巨大优势。我们将使用一个结合了计算和内存访问的简化硬件能耗模型，来计算剪枝和量化技术共同作用下能够实现的显著能耗节约，从而为我们深入学习这些技术提供具体而有力的动机 [@problem_id:3152867]。", "problem": "一个用于推理的深度神经网络在硬件上执行，其单次推理能耗可以通过一个关于计算和内存流量的线性模型来近似。设单次推理能耗模型为 $E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem}$，其中 $\\mathrm{FLOPs}$ 是每次推理的算术运算次数（此处将浮点运算 (FLOPs) 视为通用的算术运算），$\\mathrm{Mem}$ 是每次推理访问的总字节数，$a$ 是单次运算的能耗，$b$ 是单位访问字节的能耗。考虑一个使用 $32$ 位浮点表示的基准“教师”模型，以及一个通过知识蒸馏和剪枝获得，然后量化为 $8$ 位整数（int8）进行推理的压缩“学生”模型。使用以下基于经过充分验证的观测得出的、科学上合理且一致的假设：\n- 教师模型每次推理执行 $3.2 \\times 10^{9}$ 次运算，在 $32$ 位精度下每次推理访问 $1.25 \\times 10^{8}$ 字节。\n- $32$ 位下的单次运算能耗为 $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ 焦耳/次运算，单位访问字节能耗为 $b = 2.5 \\times 10^{-10}$ 焦耳/字节。\n- 学生模型由于知识蒸馏和剪枝，其运算次数减少到教师模型的 $40\\%$，内存流量（在相同精度下，以字节为单位）减少到教师模型的 $30\\%$。\n- 量化到 $8$ 位（int8）将单次运算能耗降低至 $a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ 焦耳/次运算。单位字节的内存能耗 $b$ 保持不变，但总访问字节数随表示位宽缩放，因此从 $32$ 位到 $8$ 位会使权重和激活值的访问字节数减少为原来的 $4$ 分之一。\n使用这些基本定义和假设，推导从 $32$ 位教师模型推理转变为 int8 学生模型推理的能耗节省分数，定义为\n$$ S \\equiv \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}}. $$\n计算 $S$ 并将最终答案表示为无单位小数。将答案四舍五入到四位有效数字。", "solution": "根据指定标准评估问题陈述的有效性。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 单次推理能耗模型：$E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem}$\n- $a$：单次运算能耗（焦耳/次运算）\n- $b$：单位访问字节能耗（焦耳/字节）\n- 教师模型参数（32位浮点，fp32）：\n  - $\\mathrm{FLOPs}_{\\mathrm{teacher}} = 3.2 \\times 10^9$\n  - $\\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 1.25 \\times 10^8$ 字节\n- fp32的能耗系数：\n  - $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ J/op\n  - $b = 2.5 \\times 10^{-10}$ J/byte\n- 学生模型推导（结构性变更）：\n  - $\\mathrm{FLOPs}_{\\mathrm{student}} = 0.40 \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}}$\n  - 一个假设性的32位学生模型的内存流量是教师模型内存流量（以字节计）的 $0.30$。\n- 学生模型推导（量化为8位整数，int8）：\n  - 单次运算能耗：$a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ J/op\n  - 单位字节的内存能耗 $b$ 不变。\n  - 与32位表示相比，总访问字节数减少为原来的4分之一。\n- 目标量：能耗节省分数，$S \\equiv \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}}$\n- 最终答案要求：无单位小数，四舍五入到四位有效数字。\n\n**步骤 2：使用提取的已知条件进行验证**\n从科学基础、适定性和客观性方面对问题进行评估。\n- **科学基础**：该问题基于深度学习模型压缩和硬件效率这一成熟领域。能耗的线性模型是一个标准且有效的一阶近似。指定的运算量、内存和能耗系数值对于现代计算硬件而言是科学上合理的。所描述的剪枝和量化效应是标准技术，其对FLOPs、内存和能耗的影响建模正确。\n- **适定性**：问题提供了计算唯一数值解所需的所有必要数据和定义。教师模型和学生模型之间的关系被明确且无歧义地定义。\n- **客观性**：问题以精确、定量的术语陈述，没有主观或推测性语言。\n\n**步骤 3：结论与行动**\n该问题是**有效的**，因为它科学合理、适定、客观且内部一致。可以推导出严谨的解。\n\n### 求解推导\n\n分析过程首先计算教师模型的能耗，然后确定学生模型的参数和能耗，最后计算能耗节省分数。\n\n单次推理能耗由线性模型给出：\n$$ E = a \\cdot \\mathrm{FLOPs} + b \\cdot \\mathrm{Mem} $$\n\n**1. 教师模型的能耗 ($E_{\\mathrm{teacher,fp32}}$)**\n对于在 $32$ 位精度（fp32）下运行的教师模型，已知条件为：\n- $\\mathrm{FLOPs}_{\\mathrm{teacher}} = 3.2 \\times 10^9$\n- $\\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 1.25 \\times 10^8$ 字节\n- $a_{\\mathrm{fp32}} = 2.0 \\times 10^{-12}$ J/op\n- $b = 2.5 \\times 10^{-10}$ J/byte\n\n教师模型的总能耗是计算能耗和内存能耗之和：\n$$ E_{\\mathrm{teacher,fp32}} = a_{\\mathrm{fp32}} \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}} + b \\cdot \\mathrm{Mem}_{\\mathrm{teacher,fp32}} $$\n代入数值：\n$$ E_{\\mathrm{teacher,fp32}} = (2.0 \\times 10^{-12}) \\cdot (3.2 \\times 10^9) + (2.5 \\times 10^{-10}) \\cdot (1.25 \\times 10^8) $$\n$$ E_{\\mathrm{teacher,fp32}} = 6.4 \\times 10^{-3} \\, \\mathrm{J} + 3.125 \\times 10^{-2} \\, \\mathrm{J} $$\n$$ E_{\\mathrm{teacher,fp32}} = 0.0064 \\, \\mathrm{J} + 0.03125 \\, \\mathrm{J} = 0.03765 \\, \\mathrm{J} $$\n\n**2. 学生模型的参数与能耗 ($E_{\\mathrm{student,int8}}$)**\n学生模型的参数由教师模型推导得出。\n首先，剪枝和知识蒸馏的效应在结构上减少了运算次数和内存流量。\n- $\\mathrm{FLOPs}_{\\mathrm{student}} = 0.40 \\cdot \\mathrm{FLOPs}_{\\mathrm{teacher}} = 0.40 \\cdot (3.2 \\times 10^9) = 1.28 \\times 10^9$ 次运算。\n\n其次，内存流量受到结构性剪枝和量化的双重影响。剪枝将内存流量（访问的参数/激活值数量）减少到教师模型的 $30\\%$（在相同精度下测量）。我们将一个假设性的32位学生模型的内存流量表示为 $\\mathrm{Mem}_{\\mathrm{student,fp32}}$。\n$$ \\mathrm{Mem}_{\\mathrm{student,fp32}} = 0.30 \\cdot \\mathrm{Mem}_{\\mathrm{teacher,fp32}} = 0.30 \\cdot (1.25 \\times 10^8) = 3.75 \\times 10^7 \\, \\mathrm{bytes} $$\n接下来，从 $32$ 位到 $8$ 位的量化将每个访问元素的数据大小减少了 $\\frac{32}{8} = 4$ 倍。因此，$8$ 位学生模型的最终内存流量 $\\mathrm{Mem}_{\\mathrm{student,int8}}$ 为：\n$$ \\mathrm{Mem}_{\\mathrm{student,int8}} = \\frac{\\mathrm{Mem}_{\\mathrm{student,fp32}}}{4} = \\frac{3.75 \\times 10^7}{4} = 9.375 \\times 10^6 \\, \\mathrm{bytes} $$\n\nint8学生模型的能耗系数为：\n- $a_{\\mathrm{int8}} = 5.0 \\times 10^{-13}$ J/op\n- $b = 2.5 \\times 10^{-10}$ J/byte (不变)\n\n学生模型的总能耗为：\n$$ E_{\\mathrm{student,int8}} = a_{\\mathrm{int8}} \\cdot \\mathrm{FLOPs}_{\\mathrm{student}} + b \\cdot \\mathrm{Mem}_{\\mathrm{student,int8}} $$\n代入数值：\n$$ E_{\\mathrm{student,int8}} = (5.0 \\times 10^{-13}) \\cdot (1.28 \\times 10^9) + (2.5 \\times 10^{-10}) \\cdot (9.375 \\times 10^6) $$\n$$ E_{\\mathrm{student,int8}} = 6.4 \\times 10^{-4} \\, \\mathrm{J} + 2.34375 \\times 10^{-3} \\, \\mathrm{J} $$\n$$ E_{\\mathrm{student,int8}} = 0.00064 \\, \\mathrm{J} + 0.00234375 \\, \\mathrm{J} = 0.00298375 \\, \\mathrm{J} $$\n\n**3. 能耗节省分数 (S)**\n能耗节省分数 $S$ 定义为：\n$$ S = \\frac{E_{\\mathrm{teacher,fp32}} - E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}} = 1 - \\frac{E_{\\mathrm{student,int8}}}{E_{\\mathrm{teacher,fp32}}} $$\n代入计算出的能耗值：\n$$ S = 1 - \\frac{0.00298375}{0.03765} $$\n$$ S = 1 - 0.079249667... $$\n$$ S = 0.920750332... $$\n按要求将结果四舍五入到四位有效数字：\n$$ S \\approx 0.9208 $$\n这表示单次推理的能耗减少了 $92.08\\%$。", "answer": "$$\n\\boxed{0.9208}\n$$", "id": "3152867"}, {"introduction": "在了解了模型压缩的益处之后，我们来深入探讨其中一项关键技术：量化。本练习将剖析量化实现中的一个重要细节——非对称量化中的“零点”（zero-point）。通过推导和计算，你将发现忽略零点补偿会如何引入系统性偏差，并显著影响知识蒸馏（KD）过程中学生模型与教师模型之间的对齐，这对于正确部署量化模型至关重要 [@problem_id:3152827]。", "problem": "考虑一个双类别教师网络和一个使用知识蒸馏 (KD) 训练的学生网络。对于输入向量 $\\mathbf{x} \\in \\mathbb{R}^{n}$，教师网络通过线性映射 $y_{\\text{T}} = \\mathbf{w}^{\\top}\\mathbf{x}$ 生成一个 logit 差值 $y_{\\text{T}}$，其中 $\\mathbf{w} \\in \\mathbb{R}^{n}$。对于温度为 $\\tau$ 的知识蒸馏，软化的类别1概率由经过充分验证的公式 $p(y;\\tau) = \\frac{1}{1+\\exp\\!\\left(-\\frac{y}{\\tau}\\right)}$ 定义。学生网络在权重和激活值上均使用均匀量化进行纯整数推理，其标准定义如下：对于任意实数值 $r$，量化后的整数为 $q = \\operatorname{round}\\!\\left(\\frac{r}{s}\\right) + z$，反量化为 $r \\approx s\\,(q - z)$，其中 $s > 0$ 是缩放因子 (scale)，$z \\in \\mathbb{Z}$ 是零点 (zero-point)。用于近似 $\\mathbf{w}^{\\top}\\mathbf{x}$ 的正确整数点积是 $y_{\\text{proper}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} \\left(q_{w,i} - z_{w}\\right)\\left(q_{x,i} - z_{x}\\right)$，其中 $q_{w,i}$ 和 $q_{x,i}$ 是量化后的权重和激活值。然而，一个忽略了零点补偿的朴素实现会计算 $y_{\\text{naive}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}\\,q_{x,i}$。\n\n从这些核心定义出发，推导加性偏差 $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$ 关于 $s_{w}$、$s_{x}$、$z_{w}$、$z_{x}$、$\\sum_{i=1}^{n} q_{w,i}$、$\\sum_{i=1}^{n} q_{x,i}$ 和 $n$ 的解析表达式。然后，对于下述具体实例，计算在朴素非对称量化下，教师网络的软化概率与学生网络的软化概率之间的知识蒸馏对齐误差（即绝对差值）。具体地：\n\n- 令 $n = 4$，$\\mathbf{w} = \\left[0.12,\\,-0.08,\\,0.05,\\,0.20\\right]$，以及 $\\mathbf{x} = \\left[0.50,\\,-0.40,\\,0.10,\\,0.30\\right]$，因此 $y_{\\text{T}} = \\mathbf{w}^{\\top}\\mathbf{x}$。\n- 使用缩放因子 $s_{w} = 0.01$ 和 $s_{x} = 0.01$。\n- 对于对称方案，取 $z_{w} = 0$ 和 $z_{x} = 0$。\n- 对于非对称方案，取 $z_{w} = 124$ 和 $z_{x} = 128$。\n- 通过 $q_{w,i} = \\operatorname{round}\\!\\left(\\frac{w_{i}}{s_{w}}\\right) + z_{w}$ 和 $q_{x,i} = \\operatorname{round}\\!\\left(\\frac{x_{i}}{s_{x}}\\right) + z_{x}$ 进行量化。\n- 使用知识蒸馏温度 $\\tau = 2$。\n\n假设教师网络的软化概率使用 $y_{\\text{T}}$，而学生网络在朴素非对称量化下的软化概率使用 $y_{\\text{S}} = y_{\\text{proper}} + \\Delta$，其中对于给定的缩放因子和输入，$y_{\\text{proper}}$ 是 $y_{\\text{T}}$ 的近似值。计算非对称方案下的绝对知识蒸馏对齐误差 $|\\,p(y_{\\text{S}};\\tau) - p(y_{\\text{T}};\\tau)\\,|$，并将你的最终答案表示为一个实数。将你的最终答案四舍五入到四位有效数字。", "solution": "## 问题验证 ##\n\n### 步骤 1：提取已知条件 ###\n- **教师网络**：对于输入 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 和权重向量 $\\mathbf{w} \\in \\mathbb{R}^{n}$，生成一个 logit 差值 $y_{\\text{T}} = \\mathbf{w}^{\\top}\\mathbf{x}$。\n- **知识蒸馏 (KD)**：使用温度 $\\tau$。软化的类别1概率为 $p(y;\\tau) = \\frac{1}{1+\\exp\\!\\left(-\\frac{y}{\\tau}\\right)}$。\n- **均匀量化**：对于一个实数值 $r$，量化后的整数是 $q = \\operatorname{round}\\!\\left(\\frac{r}{s}\\right) + z$。反量化的值是 $r \\approx s\\,(q - z)$。$s > 0$ 是缩放因子，$z \\in \\mathbb{Z}$ 是零点。\n- **正确的整数点积**：$y_{\\text{proper}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} \\left(q_{w,i} - z_{w}\\right)\\left(q_{x,i} - z_{x}\\right)$。\n- **朴素的整数点积**：$y_{\\text{naive}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}\\,q_{x,i}$。\n- **加性偏差**：$\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$。\n- **学生 Logit 差值**：$y_{\\text{S}} = y_{\\text{proper}} + \\Delta$。\n- **问题实例参数**：\n    - $n = 4$\n    - $\\mathbf{w} = \\left[0.12,\\,-0.08,\\,0.05,\\,0.20\\right]$\n    - $\\mathbf{x} = \\left[0.50,\\,-0.40,\\,0.10,\\,0.30\\right]$\n    - $s_{w} = 0.01$\n    - $s_{x} = 0.01$\n- **量化方案**：\n    - 对称方案：$z_{w} = 0$, $z_{x} = 0$。（这是上下文信息，在最终计算中未使用）。\n    - 非对称方案：$z_{w} = 124$, $z_{x} = 128$。\n- **量化公式**：\n    - $q_{w,i} = \\operatorname{round}\\!\\left(\\frac{w_{i}}{s_{w}}\\right) + z_{w}$\n    - $q_{x,i} = \\operatorname{round}\\!\\left(\\frac{x_{i}}{s_{x}}\\right) + z_{x}$\n- **知识蒸馏温度**：$\\tau = 2$。\n- **目标**：\n    1. 推导 $\\Delta$ 关于 $s_{w}$、$s_{x}$、$z_{w}$、$z_{x}$、$\\sum_{i=1}^{n} q_{w,i}$、$\\sum_{i=1}^{n} q_{x,i}$ 和 $n$ 的解析表达式。\n    2. 计算非对称方案下的知识蒸馏对齐误差 $|\\,p(y_{\\text{S}};\\tau) - p(y_{\\text{T}};\\tau)\\,|$。\n\n### 步骤 2：使用提取的已知条件进行验证 ###\n该问题具有科学依据，属于深度学习领域，特别是模型压缩和量化。所提供的所有关于量化、知识蒸馏和点积实现的定义在此背景下都是标准且正确的。该问题提法明确，提供了所有必要的公式、参数和一个清晰的目标。它也是客观的，用精确的数学语言陈述，没有主观断言。不存在矛盾、信息缺失或违反科学原则的情况。该问题是给定定义的直接应用，需要严谨的计算，而非推测。\n\n### 步骤 3：结论与行动 ###\n问题有效。将提供完整的解答。\n\n## 解答推导 ##\n\n问题的第一部分要求推导加性偏差 $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$ 的解析表达式。我们从 $y_{\\text{proper}}$ 和 $y_{\\text{naive}}$ 的给定定义开始。\n\n正确的整数点积的表达式为：\n$$y_{\\text{proper}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} \\left(q_{w,i} - z_{w}\\right)\\left(q_{x,i} - z_{x}\\right)$$\n我们展开求和号内的项：\n$$\\left(q_{w,i} - z_{w}\\right)\\left(q_{x,i} - z_{x}\\right) = q_{w,i}q_{x,i} - q_{w,i}z_{x} - z_{w}q_{x,i} + z_{w}z_{x}$$\n将此代回 $y_{\\text{proper}}$ 的表达式并分配求和：\n$$y_{\\text{proper}} = s_{w}\\,s_{x} \\left( \\sum_{i=1}^{n} q_{w,i}q_{x,i} - \\sum_{i=1}^{n} q_{w,i}z_{x} - \\sum_{i=1}^{n} z_{w}q_{x,i} + \\sum_{i=1}^{n} z_{w}z_{x} \\right)$$\n由于 $z_{w}$ 和 $z_{x}$ 是关于索引 $i$ 的常数，它们可以从求和中提取出来：\n$$y_{\\text{proper}} = s_{w}\\,s_{x} \\left( \\sum_{i=1}^{n} q_{w,i}q_{x,i} - z_{x}\\sum_{i=1}^{n} q_{w,i} - z_{w}\\sum_{i=1}^{n} q_{x,i} + n z_{w}z_{x} \\right)$$\n朴素点积的定义是：\n$$y_{\\text{naive}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}\\,q_{x,i}$$\n加性偏差 $\\Delta$ 定义为 $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$。代入表达式：\n$$\\Delta = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}\\,q_{x,i} - s_{w}\\,s_{x} \\left( \\sum_{i=1}^{n} q_{w,i}q_{x,i} - z_{x}\\sum_{i=1}^{n} q_{w,i} - z_{w}\\sum_{i=1}^{n} q_{x,i} + n z_{w}z_{x} \\right)$$\n提取因子 $s_{w}s_{x}$ 并化简：\n$$\\Delta = s_{w}\\,s_{x} \\left[ \\left(\\sum_{i=1}^{n} q_{w,i}q_{x,i}\\right) - \\left(\\sum_{i=1}^{n} q_{w,i}q_{x,i} - z_{x}\\sum_{i=1}^{n} q_{w,i} - z_{w}\\sum_{i=1}^{n} q_{x,i} + n z_{w}z_{x}\\right) \\right]$$\n$$\\Delta = s_{w}\\,s_{x} \\left( z_{x}\\sum_{i=1}^{n} q_{w,i} + z_{w}\\sum_{i=1}^{n} q_{x,i} - n z_{w}z_{x} \\right)$$\n这就是所求的加性偏差 $\\Delta$ 的解析表达式。\n\n问题的第二部分要求计算指定非对称方案下的知识蒸馏对齐误差 $|\\,p(y_{\\text{S}};\\tau) - p(y_{\\text{T}};\\tau)\\,|$。\n\n首先，计算教师网络的 logit 差值 $y_{\\text{T}}$：\n$$y_{\\text{T}} = \\mathbf{w}^{\\top}\\mathbf{x} = \\sum_{i=1}^{n} w_i x_i$$\n使用给定的向量 $\\mathbf{w} = \\left[0.12,\\,-0.08,\\,0.05,\\,0.20\\right]$ 和 $\\mathbf{x} = \\left[0.50,\\,-0.40,\\,0.10,\\,0.30\\right]$：\n$$y_{\\text{T}} = (0.12)(0.50) + (-0.08)(-0.40) + (0.05)(0.10) + (0.20)(0.30)$$\n$$y_{\\text{T}} = 0.060 + 0.032 + 0.005 + 0.060 = 0.157$$\n\n接下来，我们使用非对称方案的参数对权重和输入向量进行量化：$s_w = 0.01$, $z_w = 124$ 以及 $s_x = 0.01$, $z_x = 128$。\n对于权重 $\\mathbf{w}$：\n$q_{w,1} = \\operatorname{round}\\!\\left(\\frac{0.12}{0.01}\\right) + 124 = \\operatorname{round}(12) + 124 = 12 + 124 = 136$\n$q_{w,2} = \\operatorname{round}\\!\\left(\\frac{-0.08}{0.01}\\right) + 124 = \\operatorname{round}(-8) + 124 = -8 + 124 = 116$\n$q_{w,3} = \\operatorname{round}\\!\\left(\\frac{0.05}{0.01}\\right) + 124 = \\operatorname{round}(5) + 124 = 5 + 124 = 129$\n$q_{w,4} = \\operatorname{round}\\!\\left(\\frac{0.20}{0.01}\\right) + 124 = \\operatorname{round}(20) + 124 = 20 + 124 = 144$\n量化后的权重向量为 $\\mathbf{q}_w = \\left[136, 116, 129, 144\\right]$。\n\n对于输入 $\\mathbf{x}$：\n$q_{x,1} = \\operatorname{round}\\!\\left(\\frac{0.50}{0.01}\\right) + 128 = \\operatorname{round}(50) + 128 = 50 + 128 = 178$\n$q_{x,2} = \\operatorname{round}\\!\\left(\\frac{-0.40}{0.01}\\right) + 128 = \\operatorname{round}(-40) + 128 = -40 + 128 = 88$\n$q_{x,3} = \\operatorname{round}\\!\\left(\\frac{0.10}{0.01}\\right) + 128 = \\operatorname{round}(10) + 128 = 10 + 128 = 138$\n$q_{x,4} = \\operatorname{round}\\!\\left(\\frac{0.30}{0.01}\\right) + 128 = \\operatorname{round}(30) + 128 = 30 + 128 = 158$\n量化后的输入向量为 $\\mathbf{q}_x = \\left[178, 88, 138, 158\\right]$。\n\n学生的 logit 差值由 $y_{\\text{S}} = y_{\\text{proper}} + \\Delta$ 给出。根据定义 $\\Delta = y_{\\text{naive}} - y_{\\text{proper}}$，这意味着 $y_{\\text{S}} = y_{\\text{naive}}$。我们计算 $y_{\\text{naive}}$：\n$$y_{\\text{S}} = y_{\\text{naive}} = s_{w}\\,s_{x}\\sum_{i=1}^{n} q_{w,i}q_{x,i}$$\n$$y_{\\text{S}} = (0.01)(0.01) \\left[ (136)(178) + (116)(88) + (129)(138) + (144)(158) \\right]$$\n$$y_{\\text{S}} = 0.0001 \\left[ 24208 + 10208 + 17792 + 22752 \\right]$$\n$$y_{\\text{S}} = 0.0001 \\left[ 74960 \\right] = 7.496$$\n\n现在我们使用 $\\tau = 2$ 计算软化概率。\n对于教师网络：\n$$p(y_{\\text{T}}; \\tau) = \\frac{1}{1 + \\exp\\left(-\\frac{y_{\\text{T}}}{\\tau}\\right)} = \\frac{1}{1 + \\exp\\left(-\\frac{0.157}{2}\\right)} = \\frac{1}{1 + \\exp\\left(-0.0785\\right)}$$\n$$p(y_{\\text{T}}; 2) \\approx \\frac{1}{1 + 0.924490} \\approx \\frac{1}{1.924490} \\approx 0.519616$$\n\n对于学生网络（使用朴素量化）：\n$$p(y_{\\text{S}}; \\tau) = \\frac{1}{1 + \\exp\\left(-\\frac{y_{\\text{S}}}{\\tau}\\right)} = \\frac{1}{1 + \\exp\\left(-\\frac{7.496}{2}\\right)} = \\frac{1}{1 + \\exp\\left(-3.748\\right)}$$\n$$p(y_{\\text{S}}; 2) \\approx \\frac{1}{1 + 0.023565} \\approx \\frac{1}{1.023565} \\approx 0.976974$$\n\n最后，我们计算绝对知识蒸馏对齐误差：\n$$|\\,p(y_{\\text{S}};\\tau) - p(y_{\\text{T}};\\tau)\\,| \\approx |\\,0.976974 - 0.519616\\,| = 0.457358$$\n将最终结果四舍五入到四位有效数字，得到 $0.4574$。", "answer": "$$\\boxed{0.4574}$$", "id": "3152827"}, {"introduction": "除了量化，剪枝是模型压缩的另一大支柱。然而，并非所有稀疏模式都能带来同等的性能提升。本练习将引导你超越“稀疏度越高，速度越快”的简单直觉，通过一个考虑了硬件开销的性能模型，比较非结构化剪枝与结构化剪枝（如块剪枝、行剪枝）的实际加速效果。这次实践将揭示一个核心观点：实现真正的推理加速不仅取决于剪枝掉的参数数量，更取决于所产生稀疏模式的“结构”是否对硬件友好 [@problem_id:3152881]。", "problem": "给定一个用于单全连接神经网络层推理的假设执行模型，其权重矩阵为 $W \\in \\mathbb{R}^{m \\times n}$，输入向量为 $x \\in \\mathbb{R}^{n}$。该模型旨在比较剪枝诱导的稀疏模式与硬件友好型结构，并衡量实际速度增益与理论上由稀疏性带来的增益。您必须使用的基本依据是矩阵-向量乘法的操作计数原理：密集计算 $y = W x$ 执行大约 $m n$ 次乘法-累加步骤，在此模型中，每一步都被视为一个恒定成本的单元。稀疏性减少了算术运算的数量，但硬件成本还包括依赖于模式结构的非算术开销。\n\n您必须实现一个程序，对于每个指定的测试用例，根据剪枝模式为 $W$ 构建一个二进制非零掩码，计算理论和实际加速比，并汇总已实现的效率比率。理论加速比假设算术工作量与非零条目的数量成完美比例，且开销可以忽略不计。在给定的硬件模型下，实际速度包括开销，并取决于稀疏性的结构。\n\n定义和执行模型：\n\n- 浮点运算 (FLOPs) 定义：对于密集矩阵-向量乘法，算术工作量与 $m n$ 成正比。在此模型中，其时间为 $T_{\\text{dense}} = \\alpha \\cdot m n$，其中 $\\alpha$ 是一个常数，它聚合了密集计算中每次操作的成本和内存效应。\n\n- 稀疏计算模型：对于一个具有 $s$ 个非零条目的权重矩阵，实际稀疏计算时间建模为\n$$\nT_{\\text{sparse}} = \\beta \\cdot s + \\gamma \\cdot B,\n$$\n其中 $\\beta$ 是稀疏模式下每个非零算术运算的成本，$\\gamma$ 是每个已处理结构单元的开销，而 $B$ 是由稀疏模式决定的已处理结构单元的数量：\n    - 非结构化剪枝：每个非零元素是一个单元；$B = s$。\n    - 块剪枝，块大小为 $b \\times b$：每个保留的块是一个单元；$B$ 等于保留块的数量（假设块要么被完全保留，要么被完全剪枝）。\n    - 行剪枝：每个保留的行是一个单元；$B$ 等于保留行的数量。\n    - 列剪枝：每个保留的列是一个单元；$B$ 等于保留列的数量。\n\n- 忽略开销的理想理论时间是\n$$\nT_{\\text{ideal}} = \\beta \\cdot s,\n$$\n这仅由稀疏性产生理论加速比。\n\n稀疏模式和掩码构建规则：\n\n- 非结构化剪枝，目标稀疏度为 $p$：从所有条目中均匀随机选择恰好 $\\lfloor (1 - p) \\cdot m n \\rfloor$ 个条目保留为非零；其余所有条目均为零。\n\n- 块剪枝，目标稀疏度为 $p$，块大小为 $b \\times b$：将 $W$ 分割成 $\\lfloor m / b \\rfloor \\times \\lfloor n / b \\rfloor$ 个大小为 $b \\times b$ 的不重叠块（如果 $m$ 或 $n$ 不是 $b$ 的倍数，则忽略任何余数）。均匀随机选择恰好 $\\lfloor (1 - p) \\cdot \\lfloor m / b \\rfloor \\cdot \\lfloor n / b \\rfloor \\rfloor$ 个块保留；保留块内的所有条目均为非零；剪枝块内的所有条目均为零。\n\n- 行剪枝，目标稀疏度为 $p$：均匀随机选择恰好 $\\lfloor (1 - p) \\cdot m \\rfloor$ 行保留，保留行中的所有条目均为非零，剪枝行中的所有条目均为零。\n\n- 列剪枝，目标稀疏度为 $p$：均匀随机选择恰好 $\\lfloor (1 - p) \\cdot n \\rfloor$ 列保留，保留列中的所有条目均为非零，剪枝列中的所有条目均为零。\n\n需要计算的加速比定义：\n\n- 理论加速比：\n$$\n\\text{speedup}_{\\text{theory}} = \\frac{T_{\\text{dense}}}{T_{\\text{ideal}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s}.\n$$\n\n- 硬件模型下的实际加速比：\n$$\n\\text{speedup}_{\\text{actual}} = \\frac{T_{\\text{dense}}}{T_{\\text{sparse}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}.\n$$\n\n- 比较实际速度增益与理论稀疏性增益的已实现效率比：\n$$\n\\rho = \\frac{\\text{speedup}_{\\text{actual}}}{\\text{speedup}_{\\text{theory}}}.\n$$\n\n执行模型的常数（使用这些精确值）：\n- $\\alpha = 1.0$，\n- $\\beta = 1.2$，\n- $\\gamma = 5.0$。\n\n随机性要求：使用固定的随机种子 $42$ 来确保掩码构建的可复现性。\n\n测试套件（五个案例，用于探讨稀疏性和结构的不同方面）：\n\n1. $m = 512$, $n = 512$, $p = 0.9$, 非结构化剪枝, $b$ 不适用。\n2. $m = 512$, $n = 512$, $p = 0.9$, 块剪枝， $b = 4$。\n3. $m = 512$, $n = 512$, $p = 0.9$, 行剪枝, $b$ 不适用。\n4. $m = 1024$, $n = 256$, $p = 0.5$, 列剪枝, $b$ 不适用。\n5. $m = 256$, $n = 256$, $p = 0.5$, 非结构化剪枝, $b$ 不适用。\n\n您的程序必须：\n- 遵循上述规则为每个案例构建非零掩码。\n- 为每个案例计算 $s$、$B$、$T_{\\text{dense}}$、$T_{\\text{ideal}}$、$T_{\\text{sparse}}$、$\\text{speedup}_{\\text{theory}}$、$\\text{speedup}_{\\text{actual}}$ 和 $\\rho$。\n- 生成单行输出，其中包含所有测试用例的已实现效率比 $\\rho$，形式为用方括号括起来的逗号分隔列表，例如 $[\\rho_1,\\rho_2,\\rho_3,\\rho_4,\\rho_5]$。不应打印任何其他文本。\n\n您的推导中的所有数学实体和数字都必须遵守所提供的定义和指定的执行模型。不涉及物理单位；所有时间和加速比都是无量纲的量。通过严格遵循操作计数原理和依赖于模式的开销模型来确保科学真实性。最终输出必须是实值浮点数。", "solution": "问题陈述已经过验证，并被确定为是合理的。它在科学上基于神经网络中稀疏矩阵运算的计算成本建模原理，定义和常数齐全，问题设定良好，并且表述客观。我们可以开始求解。\n\n主要目标是计算权重矩阵剪枝的五种不同场景下的已实现效率比 $\\rho$。该比率比较了在假设硬件上实现的实际加速比与仅通过减少非零参数数量所期望的理论加速比。$\\rho$ 的公式由下式给出：\n$$\n\\rho = \\frac{\\text{speedup}_{\\text{actual}}}{\\text{speedup}_{\\text{theory}}}\n$$\n代入加速比的定义可以得到一个更直接的计算公式。\n$$\n\\text{speedup}_{\\text{actual}} = \\frac{T_{\\text{dense}}}{T_{\\text{sparse}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}\n$$\n$$\n\\text{speedup}_{\\text{theory}} = \\frac{T_{\\text{dense}}}{T_{\\text{ideal}}} = \\frac{\\alpha \\cdot m n}{\\beta \\cdot s}\n$$\n其中 $s$ 是非零条目的数量，$B$ 是已处理结构单元的数量，$m$ 和 $n$ 是矩阵维度，而 $\\alpha$、$\\beta$、$\\gamma$ 是成本常数。\n\n比率 $\\rho$ 可以简化为：\n$$\n\\rho = \\frac{\\frac{\\alpha \\cdot m n}{\\beta \\cdot s + \\gamma \\cdot B}}{\\frac{\\alpha \\cdot m n}{\\beta \\cdot s}} = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1}{1 + \\frac{\\gamma \\cdot B}{\\beta \\cdot s}}\n$$\n这个简化形式揭示了已实现效率由结构开销成本（$\\gamma \\cdot B$）与算术成本（$\\beta \\cdot s$）的比率决定。我们将使用提供的常数 $\\beta = 1.2$ 和 $\\gamma = 5.0$。问题为每种剪枝类型指定了确定 $s$ 和 $B$ 的规则，这些都是基于给定参数的确定性计算。虽然提到了随机种子的要求，但由于量 $s$ 和 $B$ 是由基于计数的确定性公式定义的，具体的随机选择不会改变 $\\rho$ 的最终值。\n\n我们现在为每个测试用例计算 $\\rho$。\n\n**案例 1：非结构化剪枝**\n- 参数：$m = 512$，$n = 512$，目标稀疏度 $p = 0.9$。\n- 矩阵中的总条目数为 $m \\cdot n = 512 \\cdot 512 = 262144$。\n- 要保留的非零条目数为 $s = \\lfloor (1 - p) \\cdot m n \\rfloor = \\lfloor (1 - 0.9) \\cdot 262144 \\rfloor = \\lfloor 26214.4 \\rfloor = 26214$。\n- 对于非结构化剪枝，每个非零条目都是一个结构单元，所以 $B = s = 26214$。\n- 效率比 $\\rho_1$ 为：\n$$\n\\rho_1 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot s} = \\frac{\\beta}{\\beta + \\gamma} = \\frac{1.2}{1.2 + 5.0} = \\frac{1.2}{6.2} \\approx 0.193548\n$$\n\n**案例 2：块剪枝**\n- 参数：$m = 512$，$n = 512$，$p = 0.9$，块大小 $b=4$。\n- 矩阵被划分为大小为 $4 \\times 4$ 的不重叠块。\n- 块的数量：$(\\lfloor 512/4 \\rfloor) \\times (\\lfloor 512/4 \\rfloor) = 128 \\times 128 = 16384$。\n- 要保留的块数即结构单元数：$B = \\lfloor (1 - p) \\cdot 16384 \\rfloor = \\lfloor 0.1 \\cdot 16384 \\rfloor = \\lfloor 1638.4 \\rfloor = 1638$。\n- 非零条目的总数为 $s = B \\cdot b^2 = 1638 \\cdot 4^2 = 1638 \\cdot 16 = 26208$。\n- 效率比 $\\rho_2$ 为：\n$$\n\\rho_2 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 26208}{1.2 \\cdot 26208 + 5.0 \\cdot 1638} = \\frac{31449.6}{31449.6 + 8190} = \\frac{31449.6}{39639.6} \\approx 0.793388\n$$\n\n**案例 3：行剪枝**\n- 参数：$m = 512$，$n = 512$，$p = 0.9$。\n- 要保留的行数即结构单元数：$B = \\lfloor (1 - p) \\cdot m \\rfloor = \\lfloor (1 - 0.9) \\cdot 512 \\rfloor = \\lfloor 51.2 \\rfloor = 51$。\n- 非零条目的总数为 $s = B \\cdot n = 51 \\cdot 512 = 26112$。\n- 效率比 $\\rho_3$ 为：\n$$\n\\rho_3 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 26112}{1.2 \\cdot 26112 + 5.0 \\cdot 51} = \\frac{31334.4}{31334.4 + 255} = \\frac{31334.4}{31589.4} \\approx 0.991928\n$$\n\n**案例 4：列剪枝**\n- 参数：$m = 1024$，$n = 256$，$p = 0.5$。\n- 要保留的列数即结构单元数：$B = \\lfloor (1 - p) \\cdot n \\rfloor = \\lfloor (1 - 0.5) \\cdot 256 \\rfloor = \\lfloor 128 \\rfloor = 128$。\n- 非零条目的总数为 $s = B \\cdot m = 128 \\cdot 1024 = 131072$。\n- 效率比 $\\rho_4$ 为：\n$$\n\\rho_4 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot B} = \\frac{1.2 \\cdot 131072}{1.2 \\cdot 131072 + 5.0 \\cdot 128} = \\frac{157286.4}{157286.4 + 640} = \\frac{157286.4}{157926.4} \\approx 0.995947\n$$\n\n**案例 5：非结构化剪枝**\n- 参数：$m = 256$，$n = 256$，$p = 0.5$。\n- 总条目数为 $m \\cdot n = 256 \\cdot 256 = 65536$。\n- 要保留的非零条目数为 $s = \\lfloor (1 - p) \\cdot m n \\rfloor = \\lfloor (1 - 0.5) \\cdot 65536 \\rfloor = \\lfloor 32768 \\rfloor = 32768$。\n- 对于非结构化剪枝，$B = s = 32768$。\n- 效率比 $\\rho_5$ 为：\n$$\n\\rho_5 = \\frac{\\beta \\cdot s}{\\beta \\cdot s + \\gamma \\cdot s} = \\frac{\\beta}{\\beta + \\gamma} = \\frac{1.2}{1.2 + 5.0} = \\frac{1.2}{6.2} \\approx 0.193548\n$$\n这些计算表明，结构化剪枝方法（块、行、列）的效率要高得多，因为它们相对于算术项 $\\beta s$ 大大减少了开销项 $\\gamma B$，从而导致 $\\rho$ 值接近于 $1$。在此模型中，非结构化剪枝效率极低，因为开销与非零元素的数量成正比，使得开销成本占主导地位。", "answer": "[0.1935483870967742,0.7933880228302062,0.991928014545086,0.9959473859714392,0.1935483870967742]", "id": "3152881"}]}