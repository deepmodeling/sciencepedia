{"hands_on_practices": [{"introduction": "要真正理解不确定性，最好从基本原理开始。在这个练习中，我们将使用一个经典的贝叶斯统计模型——正态-逆伽马（Normal-Inverse-Gamma）证据回归，来清晰地观察预测不确定性如何被分解为偶然不确定性（数据噪声）和认知不确定性（模型无知）。通过这个实践，你将建立一个坚实的直觉，理解当我们收集更多数据或面对不同噪声水平时，这两种不确定性分别会如何表现。[@problem_id:3197127]", "problem": "您需要实现一个用于单变量目标的贝叶斯证据回归，该回归使用正态-逆伽马（NIG）先验和后验，并显式计算不确定性分解。目标是量化和比较稀疏数据与密集数据情境下的认知不确定性和偶然不确定性。实现必须是自包含的，并为固定的测试套件生成数值输出。\n\n从概率论和贝叶斯统计中的以下基本原理开始：\n- 设数据生成过程为 $y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其中 $\\mathcal{N}$ 表示正态分布。\n- 使用共轭先验 $(\\mu, \\sigma^2) \\sim \\text{Normal–Inverse-Gamma (NIG)}(\\mu_0, \\kappa_0, \\alpha_0, \\beta_0)$，其中条件先验为 $p(\\mu \\mid \\sigma^2) = \\mathcal{N}(\\mu_0, \\sigma^2 / \\kappa_0)$，边际先验为 $p(\\sigma^2) = \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$。\n- 对于观测数据 $D = \\{y_1, \\dots, y_n\\}$，其样本均值为 $\\bar{y}$，总平方偏差和为 $S = \\sum_{i=1}^n (y_i - \\bar{y})^2$，后验分布为 $(\\mu, \\sigma^2) \\mid D \\sim \\text{NIG}(\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)$，更新后的参数为\n$$\n\\kappa_n = \\kappa_0 + n, \\quad\n\\mu_n = \\frac{\\kappa_0 \\mu_0 + n \\bar{y}}{\\kappa_0 + n}, \\quad\n\\alpha_n = \\alpha_0 + \\frac{n}{2}, \\quad\n\\beta_n = \\beta_0 + \\frac{1}{2} S + \\frac{1}{2} \\cdot \\frac{\\kappa_0 n}{\\kappa_0 + n} (\\bar{y} - \\mu_0)^2.\n$$\n- 后验预测分布是位置为 $\\mu_n$ 的学生t分布，其方差等于\n$$\nV_{\\text{pred}} = \\frac{\\beta_n}{\\alpha_n - 1} \\left( 1 + \\frac{1}{\\kappa_n} \\right),\n$$\n前提是 $\\alpha_n > 1$。该分解源于全方差定律：\n$$\nV_{\\text{alea}} = \\mathbb{E}[\\sigma^2 \\mid D] = \\frac{\\beta_n}{\\alpha_n - 1}, \\quad\nV_{\\text{epi}} = \\operatorname{Var}[\\mu \\mid D] = \\frac{\\mathbb{E}[\\sigma^2 \\mid D]}{\\kappa_n} = \\frac{\\beta_n}{(\\alpha_n - 1)\\kappa_n},\n$$\n因此 $V_{\\text{pred}} = V_{\\text{alea}} + V_{\\text{epi}}$。\n- 在证据回归中，均值的“证据水平”由类精度参数 $\\nu$ 量化，此处对应于 $\\nu = \\kappa_n$；较大的 $\\nu$ 表示该区域有更多的数据支持，因此认知不确定性较低。\n\n在单个 Python 程序中实现以下任务：\n1. 对每个测试用例，使用 $\\mathcal{N}(\\mu_{\\text{true}}, \\sigma_{\\text{true}}^2)$ 生成数据，样本大小为 $n = N$。使用固定的随机种子 $42$ 以确保可复现性。不允许外部输入。\n2. 对每个测试用例，使用上述更新方程计算后验参数 $(\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)$。\n3. 计算证据水平 $\\nu = \\kappa_n$、偶然方差 $V_{\\text{alea}}$、认知方差 $V_{\\text{epi}}$ 和预测方差 $V_{\\text{pred}}$。\n4. 将所有报告的数字四舍五入到六位小数。\n\n对所有测试用例使用以下固定的先验超参数：$\\mu_0 = 0$, $\\kappa_0 = 1$, $\\alpha_0 = 2$, $\\beta_0 = 1$。\n\n测试套件和覆盖范围：\n- 用例 A（密集，低噪声）：$N = 200$，$\\mu_{\\text{true}} = 0$，$\\sigma_{\\text{true}} = 0.2$。\n- 用例 B（稀疏，低噪声）：$N = 5$，$\\mu_{\\text{true}} = 0$，$\\sigma_{\\text{true}} = 0.2$。\n- 用例 C（密集，高噪声）：$N = 200$，$\\mu_{\\text{true}} = 0$，$\\sigma_{\\text{true}} = 1.0$。\n- 用例 D（极度稀疏，中等噪声）：$N = 1$，$\\mu_{\\text{true}} = 0$，$\\sigma_{\\text{true}} = 0.5$。\n- 用例 E（稀疏，低噪声，先验-均值冲突）：$N = 5$，$\\mu_{\\text{true}} = 3.0$，$\\sigma_{\\text{true}} = 0.2$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素本身都是一个对应于一个用例的方括号列表。对于每个用例，输出四元组 $[\\nu, V_{\\text{alea}}, V_{\\text{epi}}, V_{\\text{pred}}]$，所有值均四舍五入到六位小数。最终输出格式必须是：\n\"[[v_a,a_a,e_a,p_a],[v_b,a_b,e_b,p_b],[v_c,a_c,e_c,p_c],[v_d,a_d,e_d,p_d],[v_e,a_e,e_e,p_e]]\"\n，整行无空格。", "solution": "该问题被评估为有效。它在科学上基于贝叶斯统计理论，问题设定良好，具有唯一且可计算的解，是客观且自包含的。所有必要的数学公式、参数和测试条件均已提供，没有歧义或矛盾。因此，我们可以着手提供完整的解决方案。\n\n目标是为一个单变量输出实现贝叶斯证据回归模型。该模型使用正态-逆伽马（NIG）共轭先验来推导高斯数据生成过程中均值 $\\mu$ 和方差 $\\sigma^2$ 的后验分布。基于此后验分布，我们将预测不确定性分解为其偶然分量和认知分量，并分析这些分量如何随数据稀疏度和噪声水平的变化而变化。\n\n首先，我们对概率模型进行形式化。我们假设数据 $D = \\{y_1, \\dots, y_n\\}$ 是来自未知均值为 $\\mu$、未知方差为 $\\sigma^2$ 的正态分布的独立同分布样本：\n$$\ny_i \\sim \\mathcal{N}(\\mu, \\sigma^2)\n$$\n\n($\\mu, \\sigma^2$) 的共轭先验是正态-逆伽马分布，记为 $\\text{NIG}(\\mu_0, \\kappa_0, \\alpha_0, \\beta_0)$。该先验由均值的条件正态分布 $p(\\mu \\mid \\sigma^2) = \\mathcal{N}(\\mu_0, \\sigma^2 / \\kappa_0)$ 和方差的边际逆伽马分布 $p(\\sigma^2) = \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$ 定义。超参数 $(\\mu_0, \\kappa_0, \\alpha_0, \\beta_0)$ 编码了我们的先验信念。对于本问题，我们使用固定的先验超参数：$\\mu_0 = 0$, $\\kappa_0 = 1$, $\\alpha_0 = 2$ 和 $\\beta_0 = 1$。\n\n给定一组 $n$ 个观测值 $D$，我们更新先验信念以形成后验分布 $p(\\mu, \\sigma^2 \\mid D)$。由于 NIG 先验的共轭性，后验分布也是一个 NIG 分布，$p(\\mu, \\sigma^2 \\mid D) \\sim \\text{NIG}(\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)$。更新后的参数使用样本统计量计算：样本均值 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ 和平方偏差和 $S = \\sum_{i=1}^n (y_i - \\bar{y})^2$。后验更新方程为：\n$$\n\\kappa_n = \\kappa_0 + n \\\\\n\\mu_n = \\frac{\\kappa_0 \\mu_0 + n \\bar{y}}{\\kappa_0 + n} \\\\\n\\alpha_n = \\alpha_0 + \\frac{n}{2} \\\\\n\\beta_n = \\beta_0 + \\frac{1}{2} S + \\frac{1}{2} \\cdot \\frac{\\kappa_0 n}{\\kappa_0 + n} (\\bar{y} - \\mu_0)^2\n$$\n参数 $\\kappa_n$ 在证据回归框架中可被解释为“证据水平” $\\nu$，用于量化支持均值后验估计的数据量。较大的 $\\kappa_n$ 表示对均值位置的置信度更高。\n\n分析的核心在于总预测方差 $V_{\\text{pred}}$ 的分解。新观测值的后验预测分布是学生 t 分布。其方差 $V_{\\text{pred}}$ 由全方差定律给出：\n$$\nV_{\\text{pred}} = \\operatorname{Var}(y^* \\mid D) = \\mathbb{E}[\\operatorname{Var}(y^* \\mid \\mu, \\sigma^2)] + \\operatorname{Var}(\\mathbb{E}[y^* \\mid \\mu, \\sigma^2])\n$$\n第一项表示数据中固有的、不可约的随机性，即偶然不确定性。第二项表示我们模型参数中的不确定性，这可以通过更多数据来减少，因此是认知不确定性。对于我们的模型，该分解得出：\n1.  **偶然方差 ($V_{\\text{alea}}$)**：数据方差 $\\sigma^2$ 的后验期望。这是 $\\sigma^2$ 的后验边际逆伽马分布的均值。\n    $$\n    V_{\\text{alea}} = \\mathbb{E}[\\sigma^2 \\mid D] = \\frac{\\beta_n}{\\alpha_n - 1} \\quad (\\text{for } \\alpha_n > 1)\n    $$\n2.  **认知方差 ($V_{\\text{epi}}$)**：数据均值 $\\mu$ 的后验方差。\n    $$\n    V_{\\text{epi}} = \\operatorname{Var}[\\mu \\mid D] = \\frac{\\mathbb{E}[\\sigma^2 \\mid D]}{\\kappa_n} = \\frac{\\beta_n}{(\\alpha_n - 1)\\kappa_n}\n    $$\n总预测方差是这两个分量的和：$V_{\\text{pred}} = V_{\\text{alea}} + V_{\\text{epi}}$。\n\n对于每个测试用例，实现将按以下步骤进行：\n1.  使用固定的种子 $42$ 初始化一个 `numpy` 随机数生成器，以确保所有数据生成步骤的可复现性。\n2.  通过从 $\\mathcal{N}(\\mu_{\\text{true}}, \\sigma_{\\text{true}}^2)$ 中抽样，生成一个大小为 $N$ 的数据集。\n3.  计算样本统计量 $\\bar{y}$ 和 $S$。对于 $N=1$ 的情况，$S$ 正确地被视为 $0$。\n4.  应用更新方程计算后验超参数 $\\mu_n, \\kappa_n, \\alpha_n, \\beta_n$。\n5.  使用后验超参数计算证据水平 $\\nu = \\kappa_n$ 以及方差分量 $V_{\\text{alea}}$、$V_{\\text{epi}}$ 和 $V_{\\text{pred}}$。\n6.  将所有四个结果数值四舍五入到六位小数，并按要求格式化。\n\n此过程将对所有五个测试用例重复进行，这些用例旨在展示关键行为：\n-   **用例 A vs. B**：显示数据量（$N=200$ vs. $N=5$）对认知不确定性的影响。用例 A 中的 $V_{\\text{epi}}$ 将显著更小。\n-   **用例 A vs. C**：显示数据噪声（$\\sigma_{\\text{true}}=0.2$ vs. $\\sigma_{\\text{true}}=1.0$）的影响。用例 C 中的 $V_{\\text{alea}}$ 将显著更大，反映了更高的内在方差。\n-   **用例 D**：一个极端稀疏的情况（$N=1$），其中后验只是对先验的轻微更新。\n-   **用例 E**：一个稀疏的情况（$N=5$），存在先验-数据冲突（$\\mu_{\\text{true}}=3.0$ vs. $\\mu_0=0$）。$\\beta_n$ 更新中的冲突项预计会增加估计的偶然和认知方差。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Bayesian evidential regression with a Normal-Inverse-Gamma prior\n    to compute and decompose predictive uncertainty for several test cases.\n    \"\"\"\n    # Fixed prior hyperparameters\n    mu_0 = 0.0\n    kappa_0 = 1.0\n    alpha_0 = 2.0\n    beta_0 = 1.0\n    \n    # Test suite: (N, mu_true, sigma_true)\n    test_cases = [\n        (200, 0.0, 0.2),  # Case A: dense, low noise\n        (5, 0.0, 0.2),    # Case B: sparse, low noise\n        (200, 0.0, 1.0),  # Case C: dense, high noise\n        (1, 0.0, 0.5),    # Case D: extremely sparse, medium noise\n        (5, 3.0, 0.2),    # Case E: sparse, low noise, prior-mean conflict\n    ]\n    \n    # Initialize a single random number generator for reproducibility\n    rng = np.random.default_rng(42)\n    \n    all_results = []\n\n    for n, mu_true, sigma_true in test_cases:\n        # Step 1: Generate data\n        y_data = rng.normal(loc=mu_true, scale=sigma_true, size=n)\n        \n        # Calculate sample statistics\n        if n > 0:\n            y_bar = np.mean(y_data)\n        else:\n            # This case is not in the test suite but is handled for completeness\n            y_bar = 0.0\n            \n        if n > 1:\n            # Sum of squared deviations from the mean\n            s_stat = np.sum((y_data - y_bar)**2)\n        else:\n            # For n=0 or n=1, the sum of squared deviations is 0\n            s_stat = 0.0\n            \n        # Step 2: Compute posterior parameters\n        kappa_n = kappa_0 + n\n        mu_n = (kappa_0 * mu_0 + n * y_bar) / kappa_n\n        alpha_n = alpha_0 + n / 2.0\n        \n        beta_update_term = (kappa_0 * n / kappa_n) * (y_bar - mu_0)**2\n        beta_n = beta_0 + 0.5 * s_stat + 0.5 * beta_update_term\n        \n        # Step 3: Compute evidence level and uncertainty variances\n        # The condition alpha_n > 1 is always met since alpha_0=2 and n>=1\n        nu = kappa_n\n        v_alea = beta_n / (alpha_n - 1.0)\n        v_epi = v_alea / kappa_n\n        v_pred = v_alea + v_epi\n        \n        # Append the results for the current case\n        # nu is treated as a float for consistent formatting\n        all_results.append([float(nu), v_alea, v_epi, v_pred])\n\n    # Step 4: Format the final output string\n    case_strings = []\n    for case_result in all_results:\n        # Format each number to 6 decimal places and join into a string\n        formatted_numbers = [f\"{num:.6f}\" for num in case_result]\n        case_strings.append(f\"[{','.join(formatted_numbers)}]\")\n        \n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Print the final result in the exact required format\n    print(final_output)\n\nsolve()\n```", "id": "3197127"}, {"introduction": "在深度学习中，我们通常无法像上一个练习那样精确计算出真正的贝叶斯后验。因此，我们转而使用近似方法，而蒙特卡洛 Dropout (MC Dropout) 正是其中最著名的一种。本练习将带领你实现并比较由 MC Dropout 估计的认知不确定性与真实的解析值，从而揭示这一实用技术与严谨的贝叶斯推断之间的联系，并探索其性能如何受到超参数（即丢弃率 $p$）的影响。[@problem_id:3197106]", "problem": "要求您设计并实现一个程序，用于比较在一维线性回归问题中，由不同 dropout 率的蒙特卡洛 (MC) Dropout 估计的认知不确定性与解析上正确的贝叶斯后验认知不确定性。您的程序必须遵循全方差定律和使用高斯先验及高斯似然的共轭贝叶斯线性回归的科学基础。除了从这些基础推导出的公式外，不要使用任何启发式公式。\n\n假设一个带加性高斯噪声的一维线性模型：对于每个训练对 $(x_n, y_n)$，$y_n = w x_n + \\epsilon_n$，其中 $\\epsilon_n \\sim \\mathcal{N}(0,\\sigma^2)$ 独立同分布，且先验为 $w \\sim \\mathcal{N}(0, s_0^2)$。全方差定律指出，对于新输入 $x_\\star$，预测方差可分解为 $\\mathrm{Var}(y_\\star \\mid x_\\star, \\mathcal{D}) = \\mathbb{E}_{w \\mid \\mathcal{D}}[\\mathrm{Var}(y_\\star \\mid x_\\star, w)] + \\mathrm{Var}_{w \\mid \\mathcal{D}}(\\mathbb{E}[y_\\star \\mid x_\\star, w])$。使用此公式分别定义偶然不确定性分量和认知不确定性分量。您必须从这些基本定义出发，推导出计算后验认知方差以及在测试时对采用反向 dropout 缩放的单权重模型由 MC Dropout 引起的方差所需的表达式。\n\n使用以下大小为 $N = 5$ 的固定数据集 $\\mathcal{D}$：\n- 输入 $x$ 和输出 $y$（每个都按相同的索引顺序）：$x = [-2, -1, 0, 1, 2]$ 和 $y = [-3.12, -1.64, 0.0, 1.65, 3.14]$。此处每个数字都是实标量。\n- 已知噪声方差 $\\sigma^2 = 0.01$ 和先验方差 $s_0^2 = 1.0$。\n\n定义 $x_\\star = 3.0$。使用普通最小二乘法对给定数据进行最大似然估计，以估算单个权重参数（无截距项）。然后：\n- 基于高斯先验和高斯似然模型，计算在 $x_\\star$ 处的解析正确的贝叶斯后验认知方差。\n- 在测试时对单个权重应用单个 dropout 掩码和反向 dropout 缩放来模拟 MC Dropout。也就是说，在对 $x_\\star$ 进行预测之前，对单个已学习的权重应用一个保留概率为 $(1-p)$ 的伯努利掩码，并在保留时乘以缩放因子 $\\frac{1}{1-p}$。根据缩放后的伯努利随机变量的期望和方差定义，推导并计算由 dropout 引起的预测方差（此处不添加偶然噪声；在这种单权重设置中，将此 dropout 引起的预测方差解释为认知方差的 MC Dropout 估计值）。\n\n对于下面测试套件中的每个 dropout 率 $p$，计算 MC Dropout 认知方差相对于在 $x_\\star$ 处的贝叶斯后验认知方差的有符号分数误差：\n$$\\text{fractional\\_error}(p) = \\frac{\\widehat{v}_{\\text{drop}}(p) - v_{\\text{epistemic,true}}}{v_{\\text{epistemic,true}}}.$$\n\n使用以下 dropout 率测试套件：\n- $p = 0.0$（无 dropout；边界情况），\n- $p = 0.1$（轻度 dropout；典型情况），\n- $p = 0.5$（中度 dropout），\n- $p = 0.9$（重度 dropout；接近退化的掩码率）。\n\n您的程序必须：\n- 从提供的 $(x,y)$ 计算普通最小二乘估计值 $\\hat{w}$，\n- 计算在 $x_\\star$ 处的贝叶斯后验认知方差，\n- 对于套件中的每个 $p$，计算在对单个权重应用反向 dropout 缩放的情况下，在 $x_\\star$ 处的 MC Dropout 认知方差，然后计算上面定义的有符号分数误差，\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果与上面给出的 $p$ 的顺序相同，每个值都四舍五入到恰好六位小数。\n\n本问题不涉及物理单位或角度。所有输出都必须是实值标量。您的输出必须是单行上的一个列表，例如 $[a_1,a_2,a_3,a_4]$，其中每个 $a_k$ 是对应 $p$ 的四舍五入后的有符号分数误差。", "solution": "问题陈述已经过严格验证，并被确定为具有科学依据、良定且内部一致。它提出了一个比较，一方是通过正式的贝叶斯线性回归模型推导出的认知不确定性，另一方是通过将蒙特卡洛 (MC) Dropout 应用于频率派点估计模型所获得的估计值。这是不确定性量化中一个标准且富有洞察力的练习。提供了必要的数据、常数和定义，从而能够得到一个唯一且可验证的解。\n\n解决方案分四个阶段进行：\n1.  通过普通最小二乘法 (OLS) 估计模型权重 $\\hat{w}$。\n2.  在测试点 $x_\\star$ 处计算真实的贝叶斯后验认知方差 $v_{\\text{epistemic,true}}$。\n3.  针对不同的 dropout 率 $p$，推导并计算由 MC Dropout 引起的方差 $\\widehat{v}_{\\text{drop}}(p)$。\n4.  对每个指定的 dropout 率计算有符号分数误差。\n\n**1. 普通最小二乘法 (OLS) 权重估计**\n\n模型是单参数线性方程 $y = wx$。OLS 方法通过最小化残差平方和 $L(w) = \\sum_{n=1}^{N} (y_n - w x_n)^2$ 来求解。使 $L(w)$ 最小化的权重估计值 $\\hat{w}$ 可通过将关于 $w$ 的导数置零来找到：\n$$ \\frac{dL}{dw} = \\sum_{n=1}^{N} -2x_n(y_n - w x_n) = 0 $$\n$$ \\sum_{n=1}^{N} x_n y_n = \\hat{w} \\sum_{n=1}^{N} x_n^2 $$\n$$ \\hat{w} = \\frac{\\sum_{n=1}^{N} x_n y_n}{\\sum_{n=1}^{N} x_n^2} $$\n使用提供的数据集 $\\mathcal{D}$，其中输入为 $\\mathbf{x} = [-2, -1, 0, 1, 2]^T$，输出为 $\\mathbf{y} = [-3.12, -1.64, 0.0, 1.65, 3.14]^T$：\n$$ \\sum_{n=1}^{N} x_n y_n = (-2)(-3.12) + (-1)(-1.64) + (0)(0.0) + (1)(1.65) + (2)(3.14) = 6.24 + 1.64 + 0 + 1.65 + 6.28 = 15.81 $$\n$$ \\sum_{n=1}^{N} x_n^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10 $$\n权重的 OLS 估计值为：\n$$ \\hat{w} = \\frac{15.81}{10} = 1.581 $$\n\n**2. 真实的贝叶斯后验认知方差**\n\n问题根据全方差定律定义了在新输入 $x_\\star$ 下对新预测 $y_\\star$ 的认知不确定性：\n$$ \\mathrm{Var}(y_\\star \\mid x_\\star, \\mathcal{D}) = \\underbrace{\\mathbb{E}_{w \\mid \\mathcal{D}}[\\mathrm{Var}(y_\\star \\mid x_\\star, w)]}_{\\text{偶然方差}} + \\underbrace{\\mathrm{Var}_{w \\mid \\mathcal{D}}(\\mathbb{E}[y_\\star \\mid x_\\star, w])}_{\\text{认知方差}} $$\n对于模型 $y_\\star = w x_\\star + \\epsilon_\\star$ 及噪声 $\\epsilon_\\star \\sim \\mathcal{N}(0, \\sigma^2)$，其分量为：\n- $\\mathbb{E}[y_\\star \\mid x_\\star, w] = w x_\\star$\n- $\\mathrm{Var}(y_\\star \\mid x_\\star, w) = \\sigma^2$\n因此，认知方差为：\n$$ v_{\\text{epistemic,true}} = \\mathrm{Var}_{w \\mid \\mathcal{D}}(w x_\\star) = x_\\star^2 \\mathrm{Var}_{w \\mid \\mathcal{D}}(w) $$\n为了找到权重的后验方差 $\\mathrm{Var}_{w \\mid \\mathcal{D}}(w)$，我们使用贝叶斯推断。模型指定了高斯先验 $w \\sim \\mathcal{N}(0, s_0^2)$ 和高斯似然 $p(\\mathcal{D} \\mid w) = \\prod_n \\mathcal{N}(y_n \\mid w x_n, \\sigma^2)$。后验分布 $p(w \\mid \\mathcal{D})$ 也是高斯分布，$w \\mid \\mathcal{D} \\sim \\mathcal{N}(\\mu_N, s_N^2)$。后验精度 $s_N^{-2}$ 是先验精度与依赖于数据的似然精度之和：\n$$ \\frac{1}{s_N^2} = \\frac{1}{s_0^2} + \\frac{1}{\\sigma^2} \\sum_{n=1}^{N} x_n^2 $$\n使用给定值 $s_0^2 = 1.0$、$\\sigma^2 = 0.01$ 和 $\\sum_{n=1}^{N} x_n^2 = 10$：\n$$ \\frac{1}{s_N^2} = \\frac{1}{1.0} + \\frac{10}{0.01} = 1 + 1000 = 1001 $$\n权重的后验方差为 $\\mathrm{Var}_{w \\mid \\mathcal{D}}(w) = s_N^2 = \\frac{1}{1001}$。\n在测试点 $x_\\star = 3.0$ 处，真实的认知方差为：\n$$ v_{\\text{epistemic,true}} = x_\\star^2 s_N^2 = (3.0)^2 \\left(\\frac{1}{1001}\\right) = \\frac{9}{1001} $$\n\n**3. MC Dropout 认知方差估计**\n\n问题要求在测试时通过对 OLS 权重估计值 $\\hat{w}$ 应用一个随机掩码来模拟 MC Dropout。当 dropout 率为 $p$ 时，保留概率为 $1-p$。应用一个伯努利掩码 $z \\sim \\text{Bernoulli}(1-p)$。对于反向 dropout，随机权重 $w_{\\text{drop}}$ 为：\n$$ w_{\\text{drop}} = \\hat{w} \\frac{z}{1-p} $$\n在 $x_\\star$ 处的预测为 $y_\\star^{\\text{drop}} = w_{\\text{drop}} x_\\star$。MC Dropout 方差是此预测在掩码 $z$ 分布上的方差：\n$$ \\widehat{v}_{\\text{drop}}(p) = \\mathrm{Var}_z(y_\\star^{\\text{drop}}) = \\mathrm{Var}_z\\left(\\hat{w} \\frac{z}{1-p} x_\\star\\right) $$\n由于 $\\hat{w}$、$x_\\star$ 和 $p$ 相对于 $z$ 是常数：\n$$ \\widehat{v}_{\\text{drop}}(p) = (\\hat{w} x_\\star)^2 \\mathrm{Var}_z\\left(\\frac{z}{1-p}\\right) = \\frac{(\\hat{w} x_\\star)^2}{(1-p)^2} \\mathrm{Var}_z(z) $$\n伯努利随机变量 $z \\sim \\text{Bernoulli}(\\theta)$ 的方差为 $\\theta(1-\\theta)$。这里，$\\theta=1-p$，所以 $\\mathrm{Var}_z(z) = (1-p)(1 - (1-p)) = p(1-p)$。\n将此代入 $\\widehat{v}_{\\text{drop}}(p)$ 的表达式中：\n$$ \\widehat{v}_{\\text{drop}}(p) = \\frac{(\\hat{w} x_\\star)^2}{(1-p)^2} [p(1-p)] = (\\hat{w} x_\\star)^2 \\frac{p}{1-p} $$\n使用 $\\hat{w} = 1.581$ 和 $x_\\star = 3.0$：\n$$ \\widehat{v}_{\\text{drop}}(p) = (1.581 \\times 3.0)^2 \\frac{p}{1-p} = (4.743)^2 \\frac{p}{1-p} = 22.496049 \\frac{p}{1-p} $$\n\n**4. 有符号分数误差计算**\n\n有符号分数误差定义为：\n$$ \\text{fractional\\_error}(p) = \\frac{\\widehat{v}_{\\text{drop}}(p) - v_{\\text{epistemic,true}}}{v_{\\text{epistemic,true}}} = \\frac{\\widehat{v}_{\\text{drop}}(p)}{v_{\\text{epistemic,true}}} - 1 $$\n我们代入 $\\widehat{v}_{\\text{drop}}(p)$ 和 $v_{\\text{epistemic,true}}$ 的表达式：\n$$ \\text{fractional\\_error}(p) = \\frac{22.496049 \\frac{p}{1-p}}{9/1001} - 1 = \\left(\\frac{22.496049 \\times 1001}{9}\\right) \\frac{p}{1-p} - 1 \\approx 2502.060561 \\frac{p}{1-p} - 1 $$\n我们为测试套件 $\\{0.0, 0.1, 0.5, 0.9\\}$ 中的每个 $p$ 计算此值：\n- 对于 $p=0.0$：$\\text{fractional\\_error}(0.0) = 2502.060561 \\times 0 - 1 = -1.0$。\n- 对于 $p=0.1$：$\\text{fractional\\_error}(0.1) = 2502.060561 \\times \\frac{0.1}{0.9} - 1 \\approx 278.006729 - 1 = 277.006729$。\n- 对于 $p=0.5$：$\\text{fractional\\_error}(0.5) = 2502.060561 \\times \\frac{0.5}{0.5} - 1 = 2502.060561 - 1 = 2501.060561$。\n- 对于 $p=0.9$：$\\text{fractional\\_error}(0.9) = 2502.060561 \\times \\frac{0.9}{0.1} - 1 = 22518.545049 - 1 = 22517.545049$。\n\n将这些值四舍五入到六位小数，得到最终结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the signed fractional error of MC Dropout epistemic variance\n    with respect to the true Bayesian posterior epistemic variance for\n    a 1D linear regression problem.\n    \"\"\"\n    # Define the dataset and constants from the problem statement.\n    x_data = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data = np.array([-3.12, -1.64, 0.0, 1.65, 3.14])\n    \n    sigma2 = 0.01  # Known noise variance\n    s02 = 1.0      # Prior variance\n    x_star = 3.0   # Test point\n    \n    # Test suite of dropout rates\n    p_values = [0.0, 0.1, 0.5, 0.9]\n\n    # Step 1: Compute the Ordinary Least Squares (OLS) estimate for the weight.\n    # The model is y = w*x, so the OLS estimate is w_hat = sum(x*y) / sum(x^2).\n    sum_xy = np.sum(x_data * y_data)\n    sum_x2 = np.sum(x_data**2)\n    w_hat = sum_xy / sum_x2\n\n    # Step 2: Compute the analytically correct Bayesian posterior epistemic variance.\n    # Posterior precision s_N^-2 = s_0^-2 + sum(x^2)/sigma^2\n    # Posterior variance s_N^2 = 1 / (1/s02 + sum_x2/sigma2)\n    posterior_variance_w = 1.0 / (1.0/s02 + sum_x2/sigma2)\n    # Epistemic variance at x_star is v_epistemic = x_star^2 * Var(w|D)\n    v_epistemic_true = x_star**2 * posterior_variance_w\n\n    results = []\n    for p in p_values:\n        # Step 3: Compute the MC Dropout epistemic variance estimate.\n        # This is the variance of the prediction over the dropout mask distribution.\n        # With inverted dropout, v_drop = (w_hat * x_star)^2 * p / (1-p).\n        if p == 1.0:\n            # Although not in the test suite, this is the theoretical limit.\n            v_drop = np.inf\n        else:\n            v_drop = (w_hat * x_star)**2 * p / (1.0 - p)\n\n        # Step 4: Compute the signed fractional error.\n        # fractional_error = (v_drop - v_epistemic_true) / v_epistemic_true\n        fractional_error = (v_drop / v_epistemic_true) - 1.0\n        results.append(fractional_error)\n\n    # Format the final results as a string with each value rounded to 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3197106"}, {"introduction": "不确定性仅仅是一个标量数值吗？这个练习将挑战这一观念，引导你探索认知不确定性的内在“结构”。通过对输入施加精心设计的“对抗性”扰动，我们可以找到模型最不确定的方向。这项实践将不确定性量化与对抗鲁棒性联系起来，证明了对认知不确定性的深刻理解不仅在于知道模型“有多”不确定，更在于知道模型在“哪些方面”不确定。[@problem_id:3197111]", "problem": "考虑具有高斯先验和高斯似然的贝叶斯线性回归（BLR）。设参数向量为 $w \\in \\mathbb{R}^d$，其先验为 $p(w) = \\mathcal{N}(0, \\alpha^{-1} I_d)$，其中 $\\alpha > 0$ 是先验精度，$I_d$ 是 $d \\times d$ 的单位矩阵。给定设计矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 和目标值 $y \\in \\mathbb{R}^N$，似然为 $p(y \\mid X, w) = \\mathcal{N}(X w, \\sigma^2 I_N)$，其中 $\\sigma^2 > 0$ 是已知的观测噪声方差。对于一个测试输入 $x_* \\in \\mathbb{R}^d$，BLR 的后验分布会导出一个关于 $y_*$ 的高斯预测分布，其均值和方差由 $w$ 的后验分布确定。预测方差可加性地分解为认知不确定性分量和偶然不确定性分量。\n\n从贝叶斯定理、高斯先验和似然以及线性回归的基本定义出发，推导 $w$ 的后验分布如何是高斯分布，确定后验协方差，并证明在 $x_*$ 处的预测方差可以写成以下形式：\n$$\n\\mathrm{Var}(y_* \\mid x_*, X, y) \\;=\\; \\underbrace{x_*^\\top S_N x_*}_{\\text{认知}} \\;+\\; \\underbrace{\\sigma^2}_{\\text{偶然}},\n$$\n其中 $S_N \\in \\mathbb{R}^{d \\times d}$ 是 $w$ 的后验协方差。\n\n现在，引入受 $\\ell_2$ 范数约束 $\\|\\delta\\|_2 \\le \\epsilon$ 的输入扰动 $\\delta \\in \\mathbb{R}^d$，其中给定 $\\epsilon \\ge 0$。定义两种类型的扰动：\n- 对抗性扰动：选择 $\\delta$ 以在约束 $\\|\\delta\\|_2 \\le \\epsilon$ 下最大化认知分量 $(x_* + \\delta)^\\top S_N (x_* + \\delta)$。\n- 自然扰动：将 $\\delta$ 建模为 $\\delta \\sim \\mathcal{N}(0, \\eta^2 I_d)$，其中 $\\eta^2 \\ge 0$，并考虑期望的认知分量 $\\mathbb{E}[(x_* + \\delta)^\\top S_N (x_* + \\delta)]$。\n\n您的任务是实现一个程序，对每个提供的测试用例计算：\n1. 从 $X$、$\\alpha$ 和 $\\sigma^2$ 计算 BLR 后验协方差 $S_N$。\n2. 在 $x_*$ 处的基线预测方差及其认知和偶然部分。\n3. 最大对抗性认知方差 $(x_* + \\delta_{\\text{adv}})^\\top S_N (x_* + \\delta_{\\text{adv}})$，其中 $\\delta_{\\text{adv}}$ 是求解 $\\ell_2$ 约束最大化问题的解。\n4. 期望的自然认知方差 $\\mathbb{E}[(x_* + \\delta)^\\top S_N (x_* + \\delta)]$，其中 $\\delta \\sim \\mathcal{N}(0, \\eta^2 I_d)$。\n5. 一个布尔值，指示对抗性扰动增加的总预测方差是否大于自然扰动下的期望增量。\n6. 一个布尔值，指示偶然分量在两种扰动下是否保持不变（在 $10^{-12}$ 的数值公差范围内）。\n\n使用以下参数值的测试套件，所有数值均为指定值：\n\n- 测试用例 1（正常路径，中等先验强度和较小扰动）：\n  - $X = \\begin{bmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$，$y = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix}$，$x_* = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}$。\n  - $\\alpha = 1.0$，$\\sigma^2 = 0.04$，$\\epsilon = 0.2$，$\\eta^2 = 0.01$。\n\n- 测试用例 2（零对抗半径的边界条件）：\n  - $X = \\begin{bmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$，$y = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix}$，$x_* = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$。\n  - $\\alpha = 1.0$，$\\sigma^2 = 0.04$，$\\epsilon = 0.0$，$\\eta^2 = 0.0025$。\n\n- 测试用例 3（数据少、先验弱导致较高认知方差的边缘情况）：\n  - $X = \\begin{bmatrix} 1  -2 \\\\ 1  2 \\end{bmatrix}$，$y = \\begin{bmatrix} -1.0 \\\\ 1.2 \\end{bmatrix}$，$x_* = \\begin{bmatrix} 1.0 \\\\ 3.0 \\end{bmatrix}$。\n  - $\\alpha = 0.1$，$\\sigma^2 = 0.25$，$\\epsilon = 0.5$，$\\eta^2 = 0.0$。\n\n对于每个测试用例，令基线认知方差为 $E_{\\text{base}} = x_*^\\top S_N x_*$，基线偶然方差为 $A_{\\text{base}} = \\sigma^2$。令 $E_{\\text{adv}} = (x_* + \\delta_{\\text{adv}})^\\top S_N (x_* + \\delta_{\\text{adv}})$ 且 $E_{\\text{nat}} = \\mathbb{E}[(x_* + \\delta)^\\top S_N (x_* + \\delta)]$。对于自然扰动 $\\delta \\sim \\mathcal{N}(0, \\eta^2 I_d)$，使用恒等式\n$$\n\\mathbb{E}\\big[(x_* + \\delta)^\\top S_N (x_* + \\delta)\\big] \\;=\\; x_*^\\top S_N x_* \\;+\\; \\eta^2 \\operatorname{tr}(S_N).\n$$\n\n定义布尔值 $b_1$ 为真，如果对抗性扰动导致的总预测方差增量至少与期望的自然增量一样大，即\n$$\nb_1 \\;=\\; \\big( (E_{\\text{adv}} - E_{\\text{base}}) \\;\\ge\\; (\\eta^2 \\operatorname{tr}(S_N)) \\big).\n$$\n定义布尔值 $b_2$ 为真，如果偶然分量在两种扰动下保持不变（在 $10^{-12}$ 的公差范围内），即\n$$\nb_2 \\;=\\; \\big( |A_{\\text{base}} - \\sigma^2| < 10^{-12} \\big),\n$$\n这检查了偶然项在两种扰动下的不变性。\n\n您的程序应生成单行输出，其中包含三个测试用例的六个布尔值，顺序为 $[b_{1,1}, b_{2,1}, b_{1,2}, b_{2,2}, b_{1,3}, b_{2,3}]$，格式为方括号内以逗号分隔的列表（例如， $[{\\tt True},{\\tt False},\\dots]$）。不应打印任何额外文本。", "solution": "用户提供的问题是有效的。它在贝叶斯统计和线性代数方面有科学依据，问题设定良好，目标明确，数据充分，并且表述客观。我们将提供完整的解决方案。\n\n### 1. 理论推导\n\n#### 1.1. 权重的后验分布\n\n我们从贝叶斯线性回归开始。模型参数是权重 $w \\in \\mathbb{R}^d$。\n权重的先验分布是零均值高斯分布：\n$$p(w) = \\mathcal{N}(w \\mid 0, \\alpha^{-1} I_d) \\propto \\exp\\left(-\\frac{\\alpha}{2} w^\\top w\\right)$$\n其中 $\\alpha > 0$ 是先验精度，$I_d$ 是 $d \\times d$ 的单位矩阵。\n\n给定一个包含设计矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 和目标值 $y \\in \\mathbb{R}^N$ 的训练数据集，数据的似然也是高斯分布：\n$$p(y \\mid X, w) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_N) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top(y - Xw)\\right)$$\n其中 $\\sigma^2 > 0$ 是已知的观测噪声方差。\n\n为了找到后验分布 $p(w \\mid X, y)$，我们应用贝叶斯定理：\n$$p(w \\mid X, y) \\propto p(y \\mid X, w) p(w)$$\n代入似然和先验的表达式：\n$$p(w \\mid X, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top(y - Xw) - \\frac{\\alpha}{2} w^\\top w\\right)$$\n后验分布正比于一个关于 $w$ 的函数的指数。为了确定该分布，我们检查指数的参数，重点关注涉及 $w$ 的项：\n$$L(w) = -\\frac{1}{2\\sigma^2} (y^\\top y - 2y^\\top Xw + w^\\top X^\\top Xw) - \\frac{\\alpha}{2} w^\\top w$$\n$$L(w) = -\\frac{1}{2} \\left( \\frac{1}{\\sigma^2} w^\\top X^\\top Xw - \\frac{2}{\\sigma^2} y^\\top Xw + \\alpha w^\\top I_d w \\right) + \\text{const.}$$\n$$L(w) = -\\frac{1}{2} \\left( w^\\top \\left(\\frac{1}{\\sigma^2} X^\\top X + \\alpha I_d\\right) w - 2 \\left(\\frac{1}{\\sigma^2} X^\\top y\\right)^\\top w \\right) + \\text{const.}$$\n这是一个关于 $w$ 的二次型。一个高斯分布 $\\mathcal{N}(w \\mid m, S)$ 的对数概率密度形式为 $-\\frac{1}{2}(w-m)^\\top S^{-1}(w-m) + \\text{const} = -\\frac{1}{2}(w^\\top S^{-1}w - 2m^\\top S^{-1}w) + \\text{const}$。\n通过比较 $w$ 的二次项和线性项，我们可以确定后验分布的逆协方差（精度矩阵）$S_N^{-1}$ 和均值 $m_N$。\n\n二次项给出了后验精度矩阵：\n$$S_N^{-1} = \\frac{1}{\\sigma^2} X^\\top X + \\alpha I_d$$\n因此，后验协方差矩阵是：\n$$S_N = \\left(\\frac{1}{\\sigma^2} X^\\top X + \\alpha I_d\\right)^{-1}$$\n线性项给出了均值：$m_N^\\top S_N^{-1} = (\\frac{1}{\\sigma^2} X^\\top y)^\\top$，这导出 $m_N = S_N \\left(\\frac{1}{\\sigma^2} X^\\top y\\right)$。\n因此，权重的后验分布是一个高斯分布：\n$$p(w \\mid X, y) = \\mathcal{N}(w \\mid m_N, S_N)$$\n\n#### 1.2. 预测分布和方差分解\n\n对于一个新的测试输入 $x_* \\in \\mathbb{R}^d$，我们希望找到相应输出 $y_*$ 的预测分布。这可以通过对权重的后验进行边缘化得到：\n$$p(y_* \\mid x_*, X, y) = \\int p(y_* \\mid x_*, w) p(w \\mid X, y) dw$$\n这里，$p(y_* \\mid x_*, w) = \\mathcal{N}(y_* \\mid x_*^\\top w, \\sigma^2)$ 是测试点的似然，$p(w \\mid X, y) = \\mathcal{N}(w \\mid m_N, S_N)$ 是后验分布。这是一个两个高斯分布乘积的积分，其结果是一个高斯预测分布。\n\n我们感兴趣的是这个预测分布的方差，$\\mathrm{Var}(y_* \\mid x_*, X, y)$。我们使用全方差定律：\n$$\\mathrm{Var}(y_*) = \\mathbb{E}_{w}[\\mathrm{Var}(y_* \\mid w)] + \\mathrm{Var}_{w}[\\mathbb{E}(y_* \\mid w)]$$\n1.  **偶然不确定性**：第一项是观测噪声方差在 $w$ 的后验上的期望。\n    $$\\mathbb{E}_{w}[\\mathrm{Var}(y_* \\mid x_*, w)] = \\mathbb{E}_{w}[\\sigma^2] = \\sigma^2$$\n    这是偶然不确定性。它是不可约的，代表了数据生成过程中固有的随机性。它不依赖于数据的数量或质量。\n\n2.  **认知不确定性**：第二项是由于参数 $w$ 的不确定性而导致模型预测的方差。\n    $$\\mathrm{Var}_{w}[\\mathbb{E}(y_* \\mid x_*, w)] = \\mathrm{Var}_{w}[x_*^\\top w]$$\n    由于 $w \\sim \\mathcal{N}(m_N, S_N)$，线性变换 $x_*^\\top w$ 产生一个标量随机变量。其方差由下式给出：\n    $$\\mathrm{Var}(x_*^\\top w) = x_*^\\top \\mathrm{Cov}(w) x_* = x_*^\\top S_N x_*$$\n    这是认知不确定性。它代表了模型对参数 $w$ 的不确定性，可以通过观测更多数据来减少，因为更多数据会使后验协方差 $S_N$ 变小。\n\n将这两个分量相加，总预测方差为：\n$$\\mathrm{Var}(y_* \\mid x_*, X, y) = \\underbrace{x_*^\\top S_N x_*}_{\\text{认知}} + \\underbrace{\\sigma^2}_{\\text{偶然}}$$\n这与问题陈述中指定的形式相符。\n\n#### 1.3. 输入扰动\n\n我们分析对输入 $x_*$ 的两种类型的扰动 $\\delta$。\n\n1.  **对抗性扰动**：我们寻求找到 $\\delta_{adv}$，它在 $\\ell_2$ 范数约束 $\\|\\delta\\|_2 \\le \\epsilon$ 下，最大化受扰动输入 $x_* + \\delta$ 的认知方差分量。目标是最大化 $f(\\delta) = (x_* + \\delta)^\\top S_N (x_* + \\delta)$。由于 $S_N$ 是正定的，$f(\\delta)$ 是一个凸函数。一个凸函数在一个紧凸集（$\\ell_2$球）上的最大值必然位于集合的边界上，因此 $\\|\\delta_{adv}\\|_2 = \\epsilon$（除非 $\\epsilon=0$）。找到精确解需要解决一个类似信赖域的问题。在对抗性样本的背景下，一个标准且计算上易于处理的方法是通过从 $\\delta=0$ 开始进行单步梯度上升来近似解。目标函数关于 $\\delta$ 的梯度是 $\\nabla_\\delta f(\\delta) = 2 S_N (x_* + \\delta)$。在 $\\delta = 0$ 时，梯度是 $2 S_N x_*$。因此，上升方向是 $S_N x_*$。为满足约束，我们将此方向缩放至长度为 $\\epsilon$：\n    $$\\delta_{adv} = \\epsilon \\frac{S_N x_*}{\\|S_N x_*\\|_2}$$\n    此解释用于计算。如果 $\\|S_N x_*\\|_2=0$ 或 $\\epsilon=0$，则 $\\delta_{adv}=0$。那么，最大对抗性认知方差为 $E_{\\text{adv}} = (x_* + \\delta_{adv})^\\top S_N (x_* + \\delta_{adv})$。\n\n2.  **自然扰动**：扰动 $\\delta$ 被建模为一个随机变量 $\\delta \\sim \\mathcal{N}(0, \\eta^2 I_d)$。我们关心的是期望认知方差。如问题推导中所给出并验证的，这为：\n    $$E_{\\text{nat}} = \\mathbb{E}[(x_* + \\delta)^\\top S_N (x_* + \\delta)] = x_*^\\top S_N x_* + \\eta^2 \\operatorname{tr}(S_N)$$\n    其中 $\\operatorname{tr}(S_N)$ 是后验协方差矩阵的迹。\n\n#### 1.4. 布尔值计算\n\n问题要求为每个测试用例计算两个布尔值。\n-   $b_1 = \\big( (E_{\\text{adv}} - E_{\\text{base}}) \\ge (\\eta^2 \\operatorname{tr}(S_N)) \\big)$：这比较了由对抗性扰动引起的认知方差增量 $E_{\\text{adv}} - E_{\\text{base}}$ 与由自然扰动引起的期望增量 $E_{\\text{nat}} - E_{\\text{base}} = \\eta^2 \\operatorname{tr}(S_N)$。\n-   $b_2 = \\big( |A_{\\text{base}} - \\sigma^2|  10^{-12} \\big)$：这检查偶然分量是否保持不变。根据定义，偶然方差是 $\\sigma^2$ 并且独立于输入 $x_*$。因此，$A_{\\text{base}} = \\sigma^2$，并且扰动输入不会改变此分量。该条件简化为 $| \\sigma^2 - \\sigma^2 |  10^{-12}$，即 $0  10^{-12}$。这总是成立的，因此对于所有有效的测试用例，$b_2$ 都将为真。\n\n### 2. 实现策略\n\n对于每个测试用例，程序将执行以下步骤：\n1.  定义矩阵和标量 $X, y, x_*, \\alpha, \\sigma^2, \\epsilon, \\eta^2$。\n2.  计算后验协方差矩阵 $S_N = \\left(\\frac{1}{\\sigma^2} X^\\top X + \\alpha I_d\\right)^{-1}$。\n3.  计算基线认知方差 $E_{\\text{base}} = x_*^\\top S_N x_*$ 和偶然方差 $A_{\\text{base}} = \\sigma^2$。\n4.  计算对抗性扰动 $\\delta_{adv} = \\epsilon \\frac{S_N x_*}{\\|S_N x_*\\|_2}$（处理零范数情况）以及由此产生的对抗性认知方差 $E_{\\text{adv}} = (x_* + \\delta_{adv})^\\top S_N (x_* + \\delta_{adv})$。\n5.  确定自然扰动引起的方差期望增量为 $\\eta^2 \\operatorname{tr}(S_N)$。\n6.  评估布尔条件 $b_1$ 和 $b_2$ 并存储结果。\n7.  将最终的布尔值集合格式化为所需的输出字符串。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian Linear Regression uncertainty quantification problem for given test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"X\": np.array([[1, -1], [1, 0], [1, 1], [1, 2]], dtype=float),\n            \"y\": np.array([[0.0], [1.0], [2.0], [3.0]], dtype=float),\n            \"x_star\": np.array([1.0, 0.5], dtype=float),\n            \"alpha\": 1.0,\n            \"sigma2\": 0.04,\n            \"epsilon\": 0.2,\n            \"eta2\": 0.01,\n        },\n        # Test case 2\n        {\n            \"X\": np.array([[1, -1], [1, 0], [1, 1], [1, 2]], dtype=float),\n            \"y\": np.array([[0.0], [1.0], [2.0], [3.0]], dtype=float),\n            \"x_star\": np.array([1.0, -1.0], dtype=float),\n            \"alpha\": 1.0,\n            \"sigma2\": 0.04,\n            \"epsilon\": 0.0,\n            \"eta2\": 0.0025,\n        },\n        # Test case 3\n        {\n            \"X\": np.array([[1, -2], [1, 2]], dtype=float),\n            \"y\": np.array([[-1.0], [1.2]], dtype=float),\n            \"x_star\": np.array([1.0, 3.0], dtype=float),\n            \"alpha\": 0.1,\n            \"sigma2\": 0.25,\n            \"epsilon\": 0.5,\n            \"eta2\": 0.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        x_star = case[\"x_star\"]\n        alpha = case[\"alpha\"]\n        sigma2 = case[\"sigma2\"]\n        epsilon = case[\"epsilon\"]\n        eta2 = case[\"eta2\"]\n\n        d = X.shape[1]\n        I_d = np.identity(d)\n\n        # 1. Compute the BLR posterior covariance SN\n        precision_matrix = (1 / sigma2) * (X.T @ X) + alpha * I_d\n        SN = np.linalg.inv(precision_matrix)\n\n        # 2. Compute baseline predictive variance components\n        E_base = x_star.T @ SN @ x_star\n        A_base = sigma2\n\n        # 3. Compute maximal adversarial epistemic variance\n        grad_term = SN @ x_star\n        norm_grad_term = np.linalg.norm(grad_term)\n        \n        # Handle cases where gradient is zero or epsilon is zero\n        if norm_grad_term  1e-15 or epsilon == 0.0:\n            delta_adv = np.zeros_like(x_star)\n        else:\n            delta_adv = epsilon * grad_term / norm_grad_term\n        \n        x_adv = x_star + delta_adv\n        E_adv = x_adv.T @ SN @ x_adv\n        \n        adv_increase = E_adv - E_base\n\n        # 4. Compute expected increase from natural perturbations\n        tr_SN = np.trace(SN)\n        nat_increase = eta2 * tr_SN\n\n        # 5. Compute boolean b1\n        b1 = adv_increase >= nat_increase\n\n        # 6. Compute boolean b2\n        # The aleatoric variance is sigma2 by definition and is independent of the input x*.\n        # Therefore, it remains unchanged under any perturbation to x*.\n        # The check simplifies to abs(sigma2 - sigma2)  tol, which is always true.\n        b2 = True \n\n        results.extend([b1, b2])\n\n    # Format the output as specified\n    print(f\"[{','.join(map(lambda b: 'True' if b else 'False', results))}]\")\n\nsolve()\n\n```", "id": "3197111"}]}