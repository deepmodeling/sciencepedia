{"hands_on_practices": [{"introduction": "要理解成员推断攻击，我们首先构建一个简化的理论模型。这项练习 [@problem_id:3149406] 将指导您在这种理想化的场景下，从第一性原理出发，推导出“最佳”的（贝叶斯最优）攻击决策规则。通过这个过程，您将揭示攻击者如何根据模型输出区分训练成员与非成员的核心逻辑，为理解更复杂的攻击奠定坚实的理论基础。", "problem": "考虑一个二元分类问题，其标签为 $y \\in \\{-1,+1\\}$，特征向量为 $x \\in \\mathbb{R}^2$。数据由一个共享协方差的高斯判别模型生成：$x \\mid y \\sim \\mathcal{N}(\\mu_y,\\Sigma)$，且类别先验概率为 $\\mathbb{P}(y=+1)=\\mathbb{P}(y=-1)=\\tfrac{1}{2}$。发布的分类器在真实参数 $(\\mu_{+},\\mu_{-},\\Sigma)$ 下计算贝叶斯后验概率 $p_{\\theta}(y \\mid x)$，对于共享协方差的情况，这对应于一个 logit 为 $s(x) = w^{\\top}x + b$ 的 logistic 函数，其中 $w = \\Sigma^{-1}(\\mu_{+}-\\mu_{-})$ 且 $b = -\\tfrac{1}{2}\\left(\\mu_{+}^{\\top}\\Sigma^{-1}\\mu_{+} - \\mu_{-}^{\\top}\\Sigma^{-1}\\mu_{-}\\right)$。\n\n假设训练集成员存在一个简单的过拟合效应：对于一个真实标签为 $y$ 的数据点，模型报告的真实类别的后验概率为 $q(y \\mid x) = \\sigma\\!\\left(s(x) + b_M\\right)$，其中 $\\sigma(z) = \\tfrac{1}{1+\\exp(-z)}$ 是 logistic 函数，如果该点是训练集的成员，则 $b_M = \\beta$，如果是非成员，则 $b_M=0$。攻击者知道 $(\\mu_{+},\\mu_{-},\\Sigma)$、偏置参数 $\\beta > 0$ 以及所查询数据点的真实标签 $y$，并观察到标量 $q(y \\mid x)$。攻击者的目标是在等先验概率 $\\mathbb{P}(M=\\text{member})=\\mathbb{P}(M=\\text{non-member})=\\tfrac{1}{2}$ 的条件下，判断其成员身份 $M \\in \\{\\text{member},\\text{non-member}\\}$。\n\n从贝叶斯决策理论和高斯判别模型的基本原理出发，推导对于一个固定标签 $y$ 的最优（贝叶斯）成员推断决策，其形式为对观测到的 logit $r = \\ln\\!\\left(\\tfrac{q(y \\mid x)}{1-q(y \\mid x)}\\right)$ 的单一阈值。然后将此阈值转换为后验概率域，$t_p^{\\ast} = \\sigma(t^{\\ast})$。\n\n最后，使用以下具体参数计算您的表达式：\n- $\\mu_{+} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$\\mu_{-} = \\begin{pmatrix}-1 \\\\ 0\\end{pmatrix}$，\n- $\\Sigma = I_2$（$2 \\times 2$ 单位矩阵），\n- $\\beta = 0.8$，\n- 以及已知的真实标签 $y=+1$，\n并报告 $t_p^{\\ast}$ 的数值。将您的答案四舍五入到四位有效数字。\n\n为了进行经验验证，请考虑以下六个观测值，它们的真实标签均为 $y=+1$，其成员身份状态 $M$ 如下：\n- $x^{(1)} = \\begin{pmatrix}1.1 \\\\ -0.2\\end{pmatrix}$，其中 $M=\\text{member}$，\n- $x^{(2)} = \\begin{pmatrix}0.8 \\\\ 0.3\\end{pmatrix}$，其中 $M=\\text{member}$，\n- $x^{(3)} = \\begin{pmatrix}1.2 \\\\ -0.1\\end{pmatrix}$，其中 $M=\\text{member}$，\n- $x^{(4)} = \\begin{pmatrix}0.6 \\\\ -0.4\\end{pmatrix}$，其中 $M=\\text{non-member}$，\n- $x^{(5)} = \\begin{pmatrix}1.1 \\\\ 0.2\\end{pmatrix}$，其中 $M=\\text{non-member}$，\n- $x^{(6)} = \\begin{pmatrix}0.7 \\\\ 0.0\\end{pmatrix}$，其中 $M=\\text{non-member}$。\n\n使用推导出的最优阈值规则和已知参数，计算经验攻击决策以及由此产生的经验准确率，作为一致性检查。您最终报告的答案必须仅为数字 $t_p^{\\ast}$，四舍五入到四位有效数字，且不带单位。", "solution": "用户希望我通过推导最优贝叶斯决策规则来解决一个成员推断问题。\n\n### 问题验证\n\n**第一步：提取已知条件**\n-   二元分类标签：$y \\in \\{-1,+1\\}$\n-   特征向量：$x \\in \\mathbb{R}^2$\n-   数据生成模型：$x \\mid y \\sim \\mathcal{N}(\\mu_y,\\Sigma)$\n-   类别先验概率：$\\mathbb{P}(y=+1)=\\mathbb{P}(y=-1)=\\tfrac{1}{2}$\n-   分类器的 logit：$s(x) = w^{\\top}x + b$\n-   Logit 参数：$w = \\Sigma^{-1}(\\mu_{+}-\\mu_{-})$ 和 $b = -\\tfrac{1}{2}\\left(\\mu_{+}^{\\top}\\Sigma^{-1}\\mu_{+} - \\mu_{-}^{\\top}\\Sigma^{-1}\\mu_{-}\\right)$\n-   真实类别 $y$ 的报告后验概率：$q(y \\mid x) = \\sigma\\!\\left(s(x) + b_M\\right)$，其中 $\\sigma(z) = \\tfrac{1}{1+\\exp(-z)}$\n-   成员身份偏置：对于 member，$b_M = \\beta$；对于 non-member，$b_M=0$，其中 $\\beta > 0$\n-   攻击者已知信息：$(\\mu_{+},\\mu_{-},\\Sigma)$、$\\beta$ 和真实标签 $y$\n-   攻击者观测值：$q(y \\mid x)$ 或等价的观测 logit $r = \\ln\\!\\left(\\tfrac{q(y \\mid x)}{1-q(y \\mid x)}\\right)$\n-   攻击者目标：判断成员身份 $M \\in \\{\\text{member},\\text{non-member}\\}$\n-   成员身份先验概率：$\\mathbb{P}(M=\\text{member})=\\mathbb{P}(M=\\text{non-member})=\\tfrac{1}{2}$\n-   用于评估的具体参数：$\\mu_{+} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$\\mu_{-} = \\begin{pmatrix}-1 \\\\ 0\\end{pmatrix}$，$\\Sigma = I_2$，$\\beta = 0.8$，$y=+1$。\n-   经验验证数据（全部 $y=+1$）：\n    -   $x^{(1)} = \\begin{pmatrix}1.1 \\\\ -0.2\\end{pmatrix}$，$M=\\text{member}$\n    -   $x^{(2)} = \\begin{pmatrix}0.8 \\\\ 0.3\\end{pmatrix}$，$M=\\text{member}$\n    -   $x^{(3)} = \\begin{pmatrix}1.2 \\\\ -0.1\\end{pmatrix}$，$M=\\text{member}$\n    -   $x^{(4)} = \\begin{pmatrix}0.6 \\\\ -0.4\\end{pmatrix}$，$M=\\text{non-member}$\n    -   $x^{(5)} = \\begin{pmatrix}1.1 \\\\ 0.2\\end{pmatrix}$，$M=\\text{non-member}$\n    -   $x^{(6)} = \\begin{pmatrix}0.7 \\\\ 0.0\\end{pmatrix}$，$M=\\text{non-member}$\n\n**第二步：使用提取的已知条件进行验证**\n该问题在统计决策理论和机器学习安全领域内是良定义的。\n-   **科学依据：**该设置使用了标准模型：用于分类的高斯判别分析和用于过拟合的简化偏置模型，这是成员推断理论研究中一种常见且合理的方法。\n-   **适定性：**所有必要信息（分布、参数、先验概率和攻击者的目标）均已提供，以推导唯一的贝叶斯最优决策规则。\n-   **客观性：**问题使用形式化的数学语言陈述，没有歧义或主观性。\n-   **完整性：**问题是自洽的，没有矛盾。\n\n**第三步：结论与行动**\n问题有效。将提供完整解答。\n\n### 解答推导\n\n攻击者的任务是在两个假设之间做出决策：$H_1: M=\\text{member}$ 和 $H_0: M=\\text{non-member}$。攻击者对一个已知真实标签 $y$ 的数据点 $(x,y)$ 观测其 logit $r = \\ln\\!\\left(\\tfrac{q(y \\mid x)}{1-q(y \\mid x)}\\right)$。根据问题定义，$q(y \\mid x) = \\sigma(s(x) + b_M)$，这意味着观测到的 logit 是 $r = s(x) + b_M$。\n在这两个假设下：\n-   $H_1: M=\\text{member} \\implies b_M = \\beta \\implies r = s(x) + \\beta$\n-   $H_0: M=\\text{non-member} \\implies b_M = 0 \\implies r = s(x)$\n\n根据贝叶斯决策理论，最优决策规则是选择后验概率较高的假设。由于先验概率相等，即 $\\mathbb{P}(H_1) = \\mathbb{P}(H_0) = \\frac{1}{2}$，这等价于选择似然较高的假设。决策规则是，如果满足以下条件，则判定为 'member'（即选择 $H_1$）：\n$$ p(r \\mid H_1) > p(r \\mid H_0) $$\n为了找到似然 $p(r \\mid H_i)$，我们必须确定随机变量 $r$ 的分布。这取决于真实 logit $s(x)$ 的分布。$s(x)$ 的随机性源于数据点 $x$ 是从其类别条件分布 $x \\mid y \\sim \\mathcal{N}(\\mu_y, \\Sigma)$ 中抽取的。\n\nlogit $s(x) = w^\\top x + b$ 是高斯随机向量 $x$ 的仿射变换，因此它本身也是一个高斯随机变量。我们需要在已知标签 $y$ 的条件下，求出它的均值和方差。\n\n给定 $y$ 时，$s(x)$ 的均值为 $\\mathbb{E}[s(x) \\mid y] = w^\\top \\mathbb{E}[x \\mid y] + b = w^\\top \\mu_y + b$。\n我们将类别均值之间的马氏距离平方定义为 $\\Delta^2 = (\\mu_{+} - \\mu_{-})^\\top \\Sigma^{-1} (\\mu_{+} - \\mu_{-})$。\n$y=+1$ 时的均值为：\n$$ \\mathbb{E}[s(x) \\mid y=+1] = w^\\top \\mu_{+} + b = (\\mu_{+}-\\mu_{-})^\\top \\Sigma^{-1} \\mu_{+} - \\frac{1}{2}(\\mu_{+}^\\top\\Sigma^{-1}\\mu_{+} - \\mu_{-}^\\top\\Sigma^{-1}\\mu_{-}) $$\n$$ = \\frac{1}{2}(\\mu_{+}^\\top\\Sigma^{-1}\\mu_{+} - 2\\mu_{+}^\\top\\Sigma^{-1}\\mu_{-} + \\mu_{-}^\\top\\Sigma^{-1}\\mu_{-}) = \\frac{1}{2}(\\mu_{+}-\\mu_{-})^\\top\\Sigma^{-1}(\\mu_{+}-\\mu_{-}) = \\frac{1}{2}\\Delta^2 $$\n类似地，对于 $y=-1$：\n$$ \\mathbb{E}[s(x) \\mid y=-1] = w^\\top \\mu_{-} + b = -\\frac{1}{2}\\Delta^2 $$\n我们可以将其简写为 $\\eta_y = \\mathbb{E}[s(x) \\mid y] = y \\frac{\\Delta^2}{2}$。\n\n给定 $y$ 时，$s(x)$ 的方差独立于 $x$ 的均值：\n$$ \\text{Var}[s(x) \\mid y] = \\text{Var}[w^\\top x \\mid y] = w^\\top \\text{Cov}(x \\mid y) w = w^\\top \\Sigma w $$\n$$ = ((\\mu_{+}-\\mu_{-})^\\top \\Sigma^{-1}) \\Sigma (\\Sigma^{-1}(\\mu_{+}-\\mu_{-})) = (\\mu_{+}-\\mu_{-})^\\top \\Sigma^{-1} (\\mu_{+}-\\mu_{-}) = \\Delta^2 $$\n因此，对于一个已知的标签 $y$，真实 logit 的分布为 $s(x) \\mid y \\sim \\mathcal{N}(y \\frac{\\Delta^2}{2}, \\Delta^2)$。\n\n现在我们可以写出在两种假设下，观测到的 logit $r$ 的分布：\n-   $H_0$: $r = s(x) \\sim \\mathcal{N}\\left(y \\frac{\\Delta^2}{2}, \\Delta^2\\right)$\n-   $H_1$: $r = s(x) + \\beta \\sim \\mathcal{N}\\left(y \\frac{\\Delta^2}{2} + \\beta, \\Delta^2\\right)$\n\n似然是高斯概率密度函数：\n$$ p(r \\mid H_0) = \\frac{1}{\\sqrt{2\\pi\\Delta^2}} \\exp\\left(-\\frac{(r - y\\frac{\\Delta^2}{2})^2}{2\\Delta^2}\\right) $$\n$$ p(r \\mid H_1) = \\frac{1}{\\sqrt{2\\pi\\Delta^2}} \\exp\\left(-\\frac{(r - (y\\frac{\\Delta^2}{2} + \\beta))^2}{2\\Delta^2}\\right) $$\n决策规则 $p(r \\mid H_1) > p(r \\mid H_0)$ 变为：\n$$ \\exp\\left(-\\frac{(r - y\\frac{\\Delta^2}{2} - \\beta)^2}{2\\Delta^2}\\right) > \\exp\\left(-\\frac{(r - y\\frac{\\Delta^2}{2})^2}{2\\Delta^2}\\right) $$\n对两边取自然对数：\n$$ -\\frac{(r - y\\frac{\\Delta^2}{2} - \\beta)^2}{2\\Delta^2} > -\\frac{(r - y\\frac{\\Delta^2}{2})^2}{2\\Delta^2} $$\n两边乘以 $-2\\Delta^2$（并翻转不等号）：\n$$ (r - y\\frac{\\Delta^2}{2} - \\beta)^2 < (r - y\\frac{\\Delta^2}{2})^2 $$\n此不等式成立，当且仅当 $r$ 比起 $H_0$ 下的均值 $y\\frac{\\Delta^2}{2}$，更接近 $H_1$ 下的均值 $y\\frac{\\Delta^2}{2} + \\beta$。决策阈值是两个均值的中点：\n$$ t^* = \\frac{(y\\frac{\\Delta^2}{2}) + (y\\frac{\\Delta^2}{2} + \\beta)}{2} = y\\frac{\\Delta^2}{2} + \\frac{\\beta}{2} $$\n如果观测到的 logit $r$ 满足 $r > t^*$，攻击者就判定为 'member'。\n\n问题要求将阈值转换到概率域：\n$$ t_p^* = \\sigma(t^*) = \\sigma\\left(y\\frac{\\Delta^2}{2} + \\frac{\\beta}{2}\\right) $$\n\n### 数值计算\n我们已知的参数如下：\n-   $\\mu_{+} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$\\mu_{-} = \\begin{pmatrix}-1 \\\\ 0\\end{pmatrix}$\n-   $\\Sigma = I_2 = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}$，所以 $\\Sigma^{-1} = I_2$\n-   $\\beta = 0.8$\n-   真实标签 $y=+1$\n\n首先，我们计算 $\\Delta^2$：\n$$ \\mu_{+} - \\mu_{-} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}2 \\\\ 0\\end{pmatrix} $$\n$$ \\Delta^2 = (\\mu_{+} - \\mu_{-})^\\top \\Sigma^{-1} (\\mu_{+} - \\mu_{-}) = \\begin{pmatrix}2  0\\end{pmatrix} \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} \\begin{pmatrix}2 \\\\ 0\\end{pmatrix} = 4 $$\n接下来，我们计算 $y=+1$ 时的 logit 阈值 $t^*$：\n$$ t^* = (+1) \\cdot \\frac{4}{2} + \\frac{0.8}{2} = 2 + 0.4 = 2.4 $$\n攻击者的最优决策规则是：如果观测到的 logit $r$ 大于 2.4，则推断为 'member'。\n\n最后，我们将此阈值转换到概率域：\n$$ t_p^* = \\sigma(2.4) = \\frac{1}{1 + \\exp(-2.4)} $$\n$$ t_p^* \\approx \\frac{1}{1 + 0.09071795} \\approx \\frac{1}{1.09071795} \\approx 0.916830005 $$\n四舍五入到四位有效数字，我们得到 $t_p^* \\approx 0.9168$。\n\n### 经验验证\n作为一致性检查，我们将推导出的规则应用于所提供的数据。真实 logit 为 $s(x) = w^\\top x + b$。根据给定参数，$w = \\Sigma^{-1}(\\mu_+ - \\mu_-) = \\begin{pmatrix}2 \\\\ 0\\end{pmatrix}$ 且 $b = 0$。因此，$s(x) = 2x_1$。观测到的 logit 为 $r = s(x) + b_M = 2x_1 + b_M$。\n对于给定的数据点（所有点的 $y=+1$），决策规则是：如果 $r > 2.4$，则预测为 'member'。\n\n1.  $x^{(1)} = (1.1, -0.2)$，$M=\\text{member} \\implies b_M=0.8$。$r_1=2(1.1)+0.8=3.0$。$3.0 > 2.4 \\implies$ 预测 'member'。(正确)\n2.  $x^{(2)} = (0.8, 0.3)$，$M=\\text{member} \\implies b_M=0.8$。$r_2=2(0.8)+0.8=2.4$。$2.4 \\ngtr 2.4 \\implies$ 预测 'non-member'。(错误)\n3.  $x^{(3)} = (1.2, -0.1)$，$M=\\text{member} \\implies b_M=0.8$。$r_3=2(1.2)+0.8=3.2$。$3.2 > 2.4 \\implies$ 预测 'member'。(正确)\n4.  $x^{(4)} = (0.6, -0.4)$，$M=\\text{non-member} \\implies b_M=0$。$r_4=2(0.6)+0=1.2$。$1.2 \\ngtr 2.4 \\implies$ 预测 'non-member'。(正确)\n5.  $x^{(5)} = (1.1, 0.2)$，$M=\\text{non-member} \\implies b_M=0$。$r_5=2(1.1)+0=2.2$。$2.2 \\ngtr 2.4 \\implies$ 预测 'non-member'。(正确)\n6.  $x^{(6)} = (0.7, 0.0)$，$M=\\text{non-member} \\implies b_M=0$。$r_6=2(0.7)+0=1.4$。$1.4 \\ngtr 2.4 \\implies$ 预测 'non-member'。(正确)\n\n攻击在 6 次预测中作出了 5 次正确预测，经验准确率为 $\\frac{5}{6}$。唯一的错误发生在决策边界上，这是预料之中的，并不会使推导出的最优阈值无效。", "answer": "$$\\boxed{0.9168}$$", "id": "3149406"}, {"introduction": "在理论基础之上，我们进一步探索现实世界中更为精妙的攻击手段。简单的过拟合并非唯一的攻击突破口。本练习 [@problem_id:3149401] 探讨了一种高级攻击方法，该方法通过检测数据增强等常见训练技术留下的“指纹”或“伪影”来进行推断。这揭示了训练过程中的具体实现细节如何转化为可被利用的安全漏洞，展示了从理论到实践的深化。", "problem": "给定一个带有参数 $\\theta$ 的模拟分类模型，该模型通过对 logits 进行 softmax 操作来输出类别分布 $p_{\\theta}(y \\mid x)$。该模型使用保持标签的数据增强方法进行训练，这些增强方法在训练集成员的 logits 中引入了一种系统性的、特定于增强的伪影。您的任务是推导并实现一个有原则的成员推断规则，该规则能够在 $p_{\\theta}(y \\mid x)$ 中检测已知增强伪影的足迹，并判断单个观测到的预测是源自训练成员还是非成员。\n\n基本基础：\n- 对于一个输入 $x$，分类预测是根据 softmax 从 logits $z(x) \\in \\mathbb{R}^{C}$ 生成的，对于类别 $i \\in \\{1, \\dots, C\\}$，有 $p_{\\theta}(y=i \\mid x) = \\exp(z_i) / \\sum_{j=1}^{C} \\exp(z_j)$。\n- 训练增强在训练成员的 logits 中引入一个已知的、和为零的签名向量 $s \\in \\mathbb{R}^{C}$（即 $\\sum_{i=1}^{C} s_i = 0$）。具体来说，对于一个训练成员，其 logits 被建模为 $z = \\epsilon + \\alpha s$；对于一个非成员，其 logits 被建模为 $z = \\epsilon$。其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_C)$ 是加性高斯噪声，$\\alpha \\in \\mathbb{R}$ 是增强强度，$\\sigma > 0$ 是噪声标准差。攻击者知道增强签名 $s$、增强强度 $\\alpha$ 和噪声水平 $\\sigma$。\n- 攻击者只能观察到模型产生的概率向量 $p \\in \\Delta^{C-1}$，而不能直接观察到 logits $z$。攻击者还被给予一个先验成员概率 $\\pi = \\Pr(M=1)$，其中 $M \\in \\{0,1\\}$ 表示输入是否为训练成员。攻击者寻求在上述模型下，从 $p$ 推断 $M$ 的决策规则。\n\n原则性推导要求：\n- 利用 $p_{\\theta}(y \\mid x)$ 来自 logits 的 softmax 以及增强伪影 $s$ 是和为零的这两个事实，构建一个关于 $p$ 的标量统计量，该统计量能分离出 logits 中的加性伪影。从和为零约束和恒等式 $\\log p_i = z_i - \\log \\sum_{j=1}^{C} \\exp(z_j)$ 出发，推导一个仅通过与 $s$ 的内积来依赖于 $z$ 的统计量，因此该统计量在成员与非成员假设下具有易于处理的高斯分布。\n- 从高斯模型假设和似然的定义出发，应用 Neyman–Pearson 引理，从推导出的统计量和先验概率 $\\pi$ 中获得用于判定成员身份 $M$ 的最优似然比检验。提供一个仅用观测到的 $p$、已知参数 $(\\alpha, \\sigma, s)$ 和 $\\pi$ 表示的闭式决策规则。\n\n程序要求：\n- 在一个完整、可运行的程序中实现推导出的决策规则。对于每个测试用例，您必须首先完全按照指定的方式，使用提供的参数和确定性的 $\\epsilon$ 向量来模拟观测到的概率向量 $p$。然后，将您的成员推断规则应用于模拟出的 $p$，并为每个测试用例输出一个布尔决策，其中 $True$ 表示“成员”，$False$ 表示“非成员”。\n- 您的程序必须将所有测试用例的结果汇总到单行输出中，该输出包含一个用方括号括起来的、以逗号分隔的决策列表（例如，“[True,False,True]”）。\n\n测试套件：\n对于下方的每个测试用例，您将获得以下参数：\n- 类别数 $C$（整数）。\n- 噪声标准差 $\\sigma$（浮点数）。\n- 增强强度 $\\alpha$（浮点数）。\n- 和为零的增强签名 $s \\in \\mathbb{R}^{C}$。\n- 先验成员概率 $\\pi$（$[0,1]$ 内的小数）。\n- 确定性噪声向量 $\\epsilon \\in \\mathbb{R}^{C}$。\n- 仅用于模拟 $p$ 的真实成员身份 $M \\in \\{0,1\\}$。\n\n如果 $M=1$，您必须模拟 logits $z = \\epsilon + \\alpha s$；如果 $M=0$，则模拟 $z = \\epsilon$。然后通过对 $z$ 进行 softmax 计算 $p$，并应用您推导的决策规则输出成员身份决策。\n\n为以下四个测试用例提供您的程序结果：\n\n- 测试用例 1 (理想路径)：\n    - $C = 5$\n    - $\\sigma = 0.5$\n    - $\\alpha = 0.8$\n    - $s = [1.0, -1.0, 0.5, -0.3, -0.2]$\n    - $\\pi = 0.5$\n    - $\\epsilon = [0.2, -0.1, 0.05, -0.02, -0.13]$\n    - $M = 1$\n\n- 测试用例 2 (无伪影的边界条件)：\n    - $C = 4$\n    - $\\sigma = 0.6$\n    - $\\alpha = 0.0$\n    - $s = [0.6, -0.1, -0.2, -0.3]$\n    - $\\pi = 0.4$\n    - $\\epsilon = [0.1, -0.05, 0.02, -0.07]$\n    - $M = 1$\n\n- 测试用例 3 (高噪声情况)：\n    - $C = 3$\n    - $\\sigma = 2.0$\n    - $\\alpha = 0.3$\n    - $s = [1.0, -0.5, -0.5]$\n    - $\\pi = 0.5$\n    - $\\epsilon = [-0.2, 0.1, 0.05]$\n    - $M = 1$\n\n- 测试用例 4 (有利于成员身份的先验偏移)：\n    - $C = 6$\n    - $\\sigma = 0.4$\n    - $\\alpha = 0.5$\n    - $s = [0.9, -0.4, -0.3, 0.1, -0.1, -0.2]$\n    - $\\pi = 0.8$\n    - $\\epsilon = [-0.05, 0.02, -0.01, 0.03, -0.02, 0.03]$\n    - $M = 0$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，“[True,False,True,False]”）。不应打印任何其他文本。", "solution": "我们从概率深度学习的核心定义开始。分类器通过 softmax 生成 logits $z(x) \\in \\mathbb{R}^{C}$ 和概率向量 $p_{\\theta}(y \\mid x)$：\n$$\np_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{C} \\exp(z_j)}, \\quad i \\in \\{1,\\dots,C\\}.\n$$\n取对数，\n$$\n\\log p_i = z_i - \\log\\left(\\sum_{j=1}^{C} \\exp(z_j)\\right).\n$$\n\n增强伪影模型：\n我们假设存在一个增强签名 $s \\in \\mathbb{R}^{C}$，满足和为零约束 $\\sum_{i=1}^{C} s_i = 0$。对于训练成员（$M=1$），logits 为 $z = \\epsilon + \\alpha s$；对于非成员（$M=0$），logits 为 $z = \\epsilon$。噪声向量 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_C)$ 建模了类别无关的变化，其中 $\\sigma > 0$，$I_C$ 是 $C \\times C$ 的单位矩阵。签名强度为 $\\alpha \\in \\mathbb{R}$。攻击者知道 $(\\alpha, \\sigma, s)$。\n\n分离增强足迹的统计量：\n利用 $s$ 的和为零约束，考虑统计量\n$$\nT(p) = \\sum_{i=1}^{C} s_i \\log p_i.\n$$\n代入 $\\log p_i$，\n$$\nT(p) = \\sum_{i=1}^{C} s_i \\left( z_i - \\log\\left(\\sum_{j=1}^{C} \\exp(z_j)\\right) \\right) = \\underbrace{\\sum_{i=1}^{C} s_i z_i}_{s^\\top z} - \\left(\\sum_{i=1}^{C} s_i\\right) \\log\\left(\\sum_{j=1}^{C} \\exp(z_j)\\right).\n$$\n由于 $\\sum_{i=1}^{C} s_i = 0$，第二项消失，得到\n$$\nT(p) = s^\\top z.\n$$\n因此，尽管攻击者只能看到 $p$，但统计量 $T(p)$ 等于 $s$ 与 logits $z$ 的内积。该统计量对 logits 的加性平移不变，并精确地分离了增强足迹。\n\n成员与非成员情况下统计量的分布：\n在 $M=0$ 的情况下，$z = \\epsilon$ 且 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_C)$，所以 $T \\mid M=0 = s^\\top \\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2 \\|s\\|_2^2\\right)$，因为 $s^\\top \\epsilon$ 是多元正态分布的线性形式，其方差为 $\\sigma^2 s^\\top s = \\sigma^2 \\|s\\|_2^2$。\n\n在 $M=1$ 的情况下，$z = \\epsilon + \\alpha s$，所以\n$$\nT \\mid M=1 = s^\\top (\\epsilon + \\alpha s) = s^\\top \\epsilon + \\alpha \\, s^\\top s \\sim \\mathcal{N}\\left(\\alpha \\|s\\|_2^2, \\sigma^2 \\|s\\|_2^2\\right).\n$$\n因此，$T$ 在两种假设下都服从具有相同方差的高斯分布：\n- $M=0$ 时的均值：$\\mu_0 = 0$。\n- $M=1$ 时的均值：$\\mu_1 = \\alpha \\|s\\|_2^2$。\n- 两种情况下的方差：$v = \\sigma^2 \\|s\\|_2^2$。\n\n通过 Neyman–Pearson 引理的最优决策：\n对于具有已知密度函数 $f_0$ 和 $f_1$ 的两个简单假设，Neyman–Pearson 引理指出，在任何显著性水平下，最优检验都是对似然比 $\\Lambda(T) = f_1(T)/f_0(T)$ 进行阈值判断。结合先验概率 $\\pi = \\Pr(M=1)$ 和相等的错分代价，贝叶斯最优决策规则是：\n$$\n\\text{如果 } \\log \\Lambda(T) > \\log\\left(\\frac{1-\\pi}{\\pi}\\right), \\text{ 则判定 } M=1, \\quad \\text{否则判定 } M=0.\n$$\n当 $T \\mid M=m \\sim \\mathcal{N}(\\mu_m, v)$ 时，对数似然比简化为 $T$ 的一个线性函数：\n\\begin{align*}\n\\log \\Lambda(T) = \\log \\frac{\\phi(T; \\mu_1, v)}{\\phi(T; \\mu_0, v)} \\\\\n= -\\frac{(T-\\mu_1)^2}{2v} + \\frac{(T-\\mu_0)^2}{2v} \\\\\n= \\frac{\\mu_1 - \\mu_0}{v} \\left( T - \\frac{\\mu_1 + \\mu_0}{2} \\right),\n\\end{align*}\n其中 $\\phi(\\cdot; \\mu, v)$ 表示高斯密度函数。代入 $\\mu_0 = 0$, $\\mu_1 = \\alpha \\|s\\|_2^2$ 和 $v = \\sigma^2 \\|s\\|_2^2$ 可得\n$$\n\\log \\Lambda(T) = \\frac{\\alpha}{\\sigma^2} \\left( T - \\frac{\\alpha \\|s\\|_2^2}{2} \\right).\n$$\n等价地，无需进行显式线性化，可以直接通过二次型 $-\\frac{(T-\\mu_1)^2}{2v} + \\frac{(T-\\mu_0)^2}{2v}$ 计算 $\\log \\Lambda(T)$，这种方法即使在 $\\alpha=0$ 时也具有数值稳定性。\n\n决策规则：\n给定 $p$，通过数值稳定的 log-softmax 计算 $\\log p$，然后计算 $T = \\sum_{i} s_i \\log p_i$。按上文方式计算 $\\mu_0, \\mu_1, v$ 并构建\n$$\n\\log \\Lambda(T) = -\\frac{(T-\\mu_1)^2}{2v} + \\frac{(T-\\mu_0)^2}{2v}.\n$$\n定义阈值\n$$\n\\eta = \\log\\left(\\frac{1-\\pi}{\\pi}\\right).\n$$\n如果 $\\log \\Lambda(T) > \\eta$，则判定 $M=1$，否则判定 $M=0$。注意：\n- 当 $\\alpha = 0$ 时，$\\mu_1 = \\mu_0$，因此对于所有 $T$，$\\log \\Lambda(T) = 0$。此时贝叶斯决策简化为：如果 $\\eta < 0$（即 $\\pi > 1/2$），则选择 $M=1$，否则选择 $M=0$。这反映了在没有增强足迹的情况下，观测值不携带关于成员身份的任何信息。\n- 更大的 $\\alpha$ 和更小的 $\\sigma$ 通过增加 $T$ 中的信噪比来提高可分性。\n\n算法实现细节：\n- 对于每个测试用例，根据提供的确定性 $\\epsilon$ 和 $M$ 来模拟 logits $z$：如果 $M=1$，则 $z = \\epsilon + \\alpha s$；否则 $z = \\epsilon$。\n- 使用数值稳定的方法通过 $z$ 的 softmax 计算 $p$：在进行指数运算前减去 $\\max_i z_i$。\n- 通过 log-softmax 计算 $\\log p$：$\\log p_i = z_i - \\log \\sum_j \\exp(z_j)$，使用相同的稳定技巧。\n- 构建 $T = s^\\top \\log p$。\n- 用二次表达式计算 $\\mu_0 = 0$、$\\mu_1 = \\alpha \\|s\\|_2^2$、 $v = \\sigma^2 \\|s\\|_2^2$ 和 $\\log \\Lambda(T)$。\n- 与 $\\eta = \\log((1-\\pi)/\\pi)$ 进行比较，如果 $\\log \\Lambda(T) > \\eta$，则输出 $True$，否则输出 $False$。\n\n将此方法应用于提供的测试套件会产生四个布尔决策。程序将它们汇总到单行输出中，形式为一个用方括号括起来的、以逗号分隔的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax(z):\n    # Numerically stable softmax\n    z = np.asarray(z, dtype=np.float64)\n    m = np.max(z)\n    exp_z = np.exp(z - m)\n    return exp_z / np.sum(exp_z)\n\ndef log_softmax(z):\n    # Numerically stable log-softmax\n    z = np.asarray(z, dtype=np.float64)\n    m = np.max(z)\n    log_sum_exp = m + np.log(np.sum(np.exp(z - m)))\n    return z - log_sum_exp\n\ndef membership_decision_from_p(p, s, alpha, sigma, pi):\n    # Compute T = sum_i s_i * log p_i using log-softmax of logits implied by p.\n    # However, we only have p, so use log(p) directly.\n    # Note: In simulation we compute p from logits, but here we consume p as given.\n    log_p = np.log(np.asarray(p, dtype=np.float64))\n    s = np.asarray(s, dtype=np.float64)\n    # Statistic T\n    T = float(np.dot(s, log_p))\n    # Parameters for Gaussian models under H0 (non-member) and H1 (member)\n    norm_sq = float(np.dot(s, s))\n    mu0 = 0.0\n    mu1 = alpha * norm_sq\n    v = (sigma ** 2) * norm_sq\n    # Log-likelihood ratio (numerically stable quadratic form)\n    # If v is zero (should not happen with nonzero s and sigma > 0), guard.\n    if v == 0.0:\n        # Degenerate case: no variance -> compare means directly\n        llr = 0.0 if mu1 == mu0 else (np.inf if abs(T - mu1)  abs(T - mu0) else -np.inf)\n    else:\n        llr = -((T - mu1) ** 2) / (2.0 * v) + ((T - mu0) ** 2) / (2.0 * v)\n    # Threshold from prior\n    eta = np.log((1.0 - pi) / pi)\n    # Decision: True means \"member\"\n    return llr > eta\n\ndef simulate_p(C, sigma, alpha, s, epsilon, M):\n    # Simulate logits z and probability vector p\n    s = np.asarray(s, dtype=np.float64)\n    epsilon = np.asarray(epsilon, dtype=np.float64)\n    z = epsilon + (alpha * s if M == 1 else 0.0)\n    # Compute softmax probabilities\n    p = softmax(z)\n    return p\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Each case: (C, sigma, alpha, s, pi, epsilon, M)\n        (5, 0.5, 0.8, [1.0, -1.0, 0.5, -0.3, -0.2], 0.5, [0.2, -0.1, 0.05, -0.02, -0.13], 1),\n        (4, 0.6, 0.0, [0.6, -0.1, -0.2, -0.3], 0.4, [0.1, -0.05, 0.02, -0.07], 1),\n        (3, 2.0, 0.3, [1.0, -0.5, -0.5], 0.5, [-0.2, 0.1, 0.05], 1),\n        (6, 0.4, 0.5, [0.9, -0.4, -0.3, 0.1, -0.1, -0.2], 0.8, [-0.05, 0.02, -0.01, 0.03, -0.02, 0.03], 0),\n    ]\n\n    results = []\n    for case in test_cases:\n        C, sigma, alpha, s, pi, epsilon, M = case\n        # Simulate observed probability vector p\n        p = simulate_p(C, sigma, alpha, s, epsilon, M)\n        # Apply membership inference rule to p\n        decision = membership_decision_from_p(p, s, alpha, sigma, pi)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3149401"}, {"introduction": "除了基于模型的精确推导，实践中的攻击者通常采用更具启发性的方法，将多个弱信号（如模型置信度、预测熵和梯度信息）融合成一个综合的风险评分。这项练习 [@problem_id:3149361] 旨在教会您如何构建这样一个复合评分，并使用机器学习领域的标准度量——受试者工作特征曲线下面积（ROC AUC）——来量化评估其攻击效果。这使您能够掌握构建和评估实用攻击策略的核心技能。", "problem": "给定一个多类别分类器，其参数为每个输入实例隐式地定义了一个条件类别概率分布。对于一个输入向量 $x$，其原始 logits 为 $z(x) \\in \\mathbb{R}^C$，预测分布由 softmax 函数给出，该函数通过 $p_\\theta(k \\mid x) = \\exp(z_k(x)) \\big/ \\sum_{j=1}^C \\exp(z_j(x))$ 将 logits 映射为概率。请考虑从 $p_\\theta(\\cdot \\mid x)$ 和作为输入的每个实例的梯度范数派生出的三个逐实例信号：前两名概率边际、置信度熵和归一化梯度范数。您的任务是定义一个结合这些信号的复合隐私风险评分 $r(x)$，并通过计算使用 $r(x)$ 作为成员资格评分的基于阈值的成员推断攻击的受试者工作特征曲线下面积（ROC AUC），来验证其作为成员推断攻击脆弱性预测指标的有效性。\n\n推导的基本依据：\n- Softmax 定义 $p_\\theta(k \\mid x) = \\exp(z_k(x)) \\big/ \\sum_{j=1}^C \\exp(z_j(x))$。\n- 以自然单位（nats）表示的香农熵：$H(p) = -\\sum_{k=1}^C p_k \\log p_k$。\n- 归一化熵 $H_{\\mathrm{norm}}(p) = H(p) / \\log C$，其值在 $[0,1]$ 区间内。\n- 前两名概率边际 $m(x) = p_{(1)}(x) - p_{(2)}(x)$，其中 $p_{(1)}(x) \\ge p_{(2)}(x)$ 是 $p_\\theta(\\cdot \\mid x)$ 中最大和次大的条目；因此 $m(x) \\in [0,1]$。\n- 每个实例关于模型参数的梯度范数 $g(x) \\ge 0$ 作为输入给定。在每个测试用例中，通过最小-最大变换对其进行归一化：如果 $g_{\\max} = g_{\\min}$，则 $g_{\\mathrm{norm}}(x) = 0$，否则 $g_{\\mathrm{norm}}(x) = \\big(g(x) - g_{\\min}\\big) \\big/ \\big(g_{\\max} - g_{\\min}\\big)$。\n\n将隐私风险评分定义为一个非负加权组合\n$$\nr(x) = \\alpha \\cdot \\big(1 - m(x)\\big) + \\beta \\cdot H_{\\mathrm{norm}}\\!\\big(p_\\theta(\\cdot \\mid x)\\big) + \\gamma \\cdot g_{\\mathrm{norm}}(x),\n$$\n其中 $\\alpha, \\beta, \\gamma \\ge 0$ 是在测试用例中提供的固定权重。直观上，较大的 $r(x)$ 表示较高的成员推断攻击脆弱性，因为小边际、高熵和大梯度范数都与置信度较低且可能过拟合的行为相关。\n\n验证协议：\n- 给定标签 $y_{\\mathrm{mem}}(x) \\in \\{0,1\\}$，指示非成员（$0$）或成员（$1$）状态，通过计算将 $r(x)$ 用作成员资格评分时的受试者工作特征（ROC）曲线下面积（AUC）来验证 $r(x)$ 作为预测指标的有效性，其中较高的 $r(x)$ 意味着成为成员的可能性更高。使用 ROC AUC 与 Mann–Whitney $U$ 统计量之间的等价性：如果 $n_1$ 是成员数量，$n_0$ 是非成员数量，并且如果 $\\mathrm{rank}(r_i)$ 是所有实例分数的平均秩次（最小分数的秩次从 $1$ 开始），并对并列值进行适当的平均处理，则\n$$\n\\mathrm{AUC} = \\frac{U}{n_0 n_1}, \\quad \\text{其中} \\quad U = \\sum_{i:\\, y_{\\mathrm{mem},i} = 1} \\mathrm{rank}(r_i) - \\frac{n_1 (n_1 + 1)}{2}.\n$$\n这将产生 $\\mathrm{AUC} \\in [0,1]$，其中 $\\mathrm{AUC} = 0.5$ 表示随机水平的区分能力。\n\n实现要求：\n- 对于下面的每个测试用例，从 logits 计算概率，然后使用提供的 $\\alpha$、$\\beta$ 和 $\\gamma$ 计算 $m(x)$、$H_{\\mathrm{norm}}(p)$、$g_{\\mathrm{norm}}(x)$ 和 $r(x)$。\n- 使用基于秩次的公式和处理并列值的平均秩次来计算 ROC AUC，将较高的 $r(x)$ 视为较高的成员资格可能性。\n- 您的程序应生成一行输出，其中包含所有测试用例的 AUC 结果，形式为逗号分隔的列表并用方括号括起，每个 AUC 值四舍五入到六位小数，例如 $[0.945000,0.731234,0.500000]$。\n\n测试套件：\n- 测试用例 1（清晰分离，三类别）：\n  - 类别数 $C = 3$。\n  - 权重：$\\alpha = 0.5$，$\\beta = 0.3$，$\\gamma = 0.2$。\n  - 六个样本，包含 logits、梯度范数和成员资格标签：\n    1. Logits $[3.0, 0.2, -1.0]$，梯度范数 $0.10$，$y_{\\mathrm{mem}} = 0$。\n    2. Logits $[2.5, -0.5, 0.0]$，梯度范数 $0.15$，$y_{\\mathrm{mem}} = 0$。\n    3. Logits $[-1.0, 0.0, 3.0]$，梯度范数 $0.05$，$y_{\\mathrm{mem}} = 0$。\n    4. Logits $[0.5, 0.4, 0.3]$，梯度范数 $0.90$，$y_{\\mathrm{mem}} = 1$。\n    5. Logits $[0.0, 0.1, 0.2]$，梯度范数 $0.70$，$y_{\\mathrm{mem}} = 1$。\n    6. Logits $[0.2, 0.1, 0.2]$，梯度范数 $1.20$，$y_{\\mathrm{mem}} = 1$。\n- 测试用例 2（重叠，三类别）：\n  - 类别数 $C = 3$。\n  - 权重：$\\alpha = 0.5$，$\\beta = 0.3$，$\\gamma = 0.2$。\n  - 六个样本，包含 logits、梯度范数和成员资格标签：\n    1. Logits $[1.5, 1.4, 1.3]$，梯度范数 $0.60$，$y_{\\mathrm{mem}} = 0$。\n    2. Logits $[2.0, 0.0, 0.0]$，梯度范数 $0.40$，$y_{\\mathrm{mem}} = 0$。\n    3. Logits $[0.5, -0.2, 1.0]$，梯度范数 $0.55$，$y_{\\mathrm{mem}} = 0$。\n    4. Logits $[0.8, 0.7, 0.1]$，梯度范数 $0.50$，$y_{\\mathrm{mem}} = 1$。\n    5. Logits $[1.8, 0.9, 0.7]$，梯度范数 $0.45$，$y_{\\mathrm{mem}} = 1$。\n    6. Logits $[0.2, 0.2, 0.2]$，梯度范数 $0.65$，$y_{\\mathrm{mem}} = 1$。\n- 测试用例 3（有并列值的边界条件）：\n  - 类别数 $C = 3$。\n  - 权重：$\\alpha = 0.5$，$\\beta = 0.3$，$\\gamma = 0.2$。\n  - 四个样本，具有相同的 logits 和梯度范数，导致风险评分并列：\n    1. Logits $[0.0, 0.0, 0.0]$，梯度范数 $0.30$，$y_{\\mathrm{mem}} = 1$。\n    2. Logits $[0.0, 0.0, 0.0]$，梯度范数 $0.30$，$y_{\\mathrm{mem}} = 0$。\n    3. Logits $[0.0, 0.0, 0.0]$，梯度范数 $0.30$，$y_{\\mathrm{mem}} = 1$。\n    4. Logits $[0.0, 0.0, 0.0]$，梯度范数 $0.30$，$y_{\\mathrm{mem}} = 0$。\n\n您的程序必须实现上述定义，并输出一行包含三个 ROC AUC 值，格式需严格为 $[a_1,a_2,a_3]$，其中每个 $a_i$ 是一个四舍五入到六位小数的浮点数。", "solution": "用户提供的问题陈述已经过严格验证，并被认为是**有效**的。它在机器学习隐私领域有科学依据，问题定义明确、客观且自包含。所有定义、常量和数据均已提供，从而可以得出一个唯一且可验证的解。\n\n该问题要求实现并验证一个隐私风险评分 $r(x)$，该评分旨在量化数据实例 $x$ 对成员推断攻击的脆弱性。解决方案涉及为每个提供的测试用例执行一个多步骤过程。我们将系统地推导计算受试者工作特征曲线下面积（ROC AUC）所需的量，该面积是风险评分的验证指标。\n\n**步骤 1：计算概率分布**\n\n对于每个数据实例，给定一个来自 $C$ 类分类器的原始 logits 向量 $z(x) \\in \\mathbb{R}^C$。相应的预测概率分布 $p_\\theta(\\cdot \\mid x)$ 通过 softmax 函数获得：\n$$\np_\\theta(k \\mid x) = \\frac{\\exp(z_k(x))}{\\sum_{j=1}^C \\exp(z_j(x))}.\n$$\n为确保在处理较大的 logit 值时具有数值稳定性，防止潜在的上溢或下溢，我们使用恒等式 $p_\\theta(k \\mid x) = \\frac{\\exp(z_k(x) - z_{\\max})}{\\sum_{j=1}^C \\exp(z_j(x) - z_{\\max})}$，其中 $z_{\\max} = \\max_j z_j(x)$。\n\n**步骤 2：计算每个实例的信号分量**\n\n隐私风险评分 $r(x)$ 是三个信号的组合。对于每个实例 $x$，我们计算：\n\n1.  **前两名概率边际, $m(x)$**：此信号衡量模型对其最高预测的置信度。较小的边际表示模糊性。令 $p_{(1)}(x)$ 和 $p_{(2)}(x)$ 为分布 $p_\\theta(\\cdot \\mid x)$ 中的最大和次大概率。边际为：\n    $$\n    m(x) = p_{(1)}(x) - p_{(2)}(x).\n    $$\n    风险评分中使用的项是 $1 - m(x)$，对于置信度较低的预测，该值更大。\n\n2.  **归一化置信度熵, $H_{\\mathrm{norm}}(p)$**：此信号衡量预测分布的不确定性。较高的熵意味着较大的不确定性。香农熵以自然单位（nats）计算：\n    $$\n    H(p) = -\\sum_{k=1}^C p_k \\log p_k,\n    $$\n    其中 $p_k = p_\\theta(k \\mid x)$，我们定义 $0 \\log 0 = 0$。然后通过除以 $C$ 类分布可能的最大熵 $\\log C$ 将此熵归一化到 $[0, 1]$ 范围：\n    $$\n    H_{\\mathrm{norm}}(p) = \\frac{H(p)}{\\log C}.\n    $$\n\n3.  **归一化梯度范数, $g_{\\mathrm{norm}}(x)$**：对于每个实例，提供一个初始梯度范数 $g(x)$。该值在单个测试用例的所有实例中，使用最小-最大缩放进行归一化。令 $\\{g_i\\}_{i=1}^N$ 为具有 $N$ 个样本的测试用例的梯度范数集合。令 $g_{\\min} = \\min_i g_i$ 和 $g_{\\max} = \\max_i g_i$。实例 $i$ 的归一化范数为：\n    $$\n    g_{\\mathrm{norm}}(x_i) = \\begin{cases}\n    0  \\text{if } g_{\\max} = g_{\\min} \\\\\n    \\frac{g(x_i) - g_{\\min}}{g_{\\max} - g_{\\min}}  \\text{otherwise}\n    \\end{cases}.\n    $$\n    这将每个测试用例的梯度范数置于 $[0, 1]$ 的公共尺度上。\n\n**步骤 3：计算复合隐私风险评分, $r(x)$**\n\n这三个信号通过加权和组合成一个单一的风险评分 $r(x)$，其中非负权重 $\\alpha, \\beta, \\gamma$ 为每个测试用例提供：\n$$\nr(x) = \\alpha \\cdot \\big(1 - m(x)\\big) + \\beta \\cdot H_{\\mathrm{norm}}\\!\\big(p_\\theta(\\cdot \\mid x)\\big) + \\gamma \\cdot g_{\\mathrm{norm}}(x).\n$$\n假设较高的评分 $r(x)$ 表示 $x$ 属于训练集（“成员”）的可能性更大，对应于更高的成员推断攻击脆弱性。\n\n**步骤 4：使用 ROC AUC 进行验证**\n\n通过计算 ROC AUC 来评估 $r(x)$ 作为成员资格评分的有效性。该指标量化了评分区分成员（$y_{\\mathrm{mem}}=1$）和非成员（$y_{\\mathrm{mem}}=0$）的能力。我们使用基于秩次的 AUC 公式，该公式等价于通过每个类别样本数量的乘积进行归一化的 Mann-Whitney U 统计量。\n\n令 $\\{r_i\\}_{i=1}^N$ 为一个测试用例中所有 $N$ 个实例的风险评分集合。\n1.  **排名**：我们首先计算每个分数 $r_i$ 在所有分数的合并列表中的秩次。秩次从 1 开始。如果出现并列，所有并列的分数将获得它们所占据秩次的平均值。例如，如果两个分数并列第 2 和第 3 位，则它们都获得秩次 $(2+3)/2 = 2.5$。\n2.  **U 统计量**：令 $n_1$ 为成员计数，$n_0$ 为非成员计数。Mann-Whitney U 统计量通过对成员实例对应的分数秩次求和，并减去一个修正项来计算：\n    $$\n    U = \\sum_{i:\\, y_{\\mathrm{mem},i} = 1} \\mathrm{rank}(r_i) - \\frac{n_1 (n_1 + 1)}{2}.\n    $$\n3.  **AUC 计算**：然后 AUC 由下式给出：\n    $$\n    \\mathrm{AUC} = \\frac{U}{n_0 n_1}.\n    $$\n该值的范围从 0 到 1，其中 1.0 表示完美分离（所有成员的得分都高于所有非成员），0.5 表示性能不优于随机猜测，0.0 表示完美反向分离。\n\n将此完整过程应用于三个测试用例中的每一个，以得出最终的 AUC 值列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ROC AUC for a membership inference attack based on a composite privacy risk score.\n    \"\"\"\n    test_cases = [\n        {\n            \"C\": 3,\n            \"weights\": (0.5, 0.3, 0.2),  # alpha, beta, gamma\n            \"samples\": [\n                {\"logits\": [3.0, 0.2, -1.0], \"grad_norm\": 0.10, \"y_mem\": 0},\n                {\"logits\": [2.5, -0.5, 0.0], \"grad_norm\": 0.15, \"y_mem\": 0},\n                {\"logits\": [-1.0, 0.0, 3.0], \"grad_norm\": 0.05, \"y_mem\": 0},\n                {\"logits\": [0.5, 0.4, 0.3], \"grad_norm\": 0.90, \"y_mem\": 1},\n                {\"logits\": [0.0, 0.1, 0.2], \"grad_norm\": 0.70, \"y_mem\": 1},\n                {\"logits\": [0.2, 0.1, 0.2], \"grad_norm\": 1.20, \"y_mem\": 1},\n            ]\n        },\n        {\n            \"C\": 3,\n            \"weights\": (0.5, 0.3, 0.2),\n            \"samples\": [\n                {\"logits\": [1.5, 1.4, 1.3], \"grad_norm\": 0.60, \"y_mem\": 0},\n                {\"logits\": [2.0, 0.0, 0.0], \"grad_norm\": 0.40, \"y_mem\": 0},\n                {\"logits\": [0.5, -0.2, 1.0], \"grad_norm\": 0.55, \"y_mem\": 0},\n                {\"logits\": [0.8, 0.7, 0.1], \"grad_norm\": 0.50, \"y_mem\": 1},\n                {\"logits\": [1.8, 0.9, 0.7], \"grad_norm\": 0.45, \"y_mem\": 1},\n                {\"logits\": [0.2, 0.2, 0.2], \"grad_norm\": 0.65, \"y_mem\": 1},\n            ]\n        },\n        {\n            \"C\": 3,\n            \"weights\": (0.5, 0.3, 0.2),\n            \"samples\": [\n                {\"logits\": [0.0, 0.0, 0.0], \"grad_norm\": 0.30, \"y_mem\": 1},\n                {\"logits\": [0.0, 0.0, 0.0], \"grad_norm\": 0.30, \"y_mem\": 0},\n                {\"logits\": [0.0, 0.0, 0.0], \"grad_norm\": 0.30, \"y_mem\": 1},\n                {\"logits\": [0.0, 0.0, 0.0], \"grad_norm\": 0.30, \"y_mem\": 0},\n            ]\n        }\n    ]\n\n    def rankdata(data):\n        \"\"\"\n        Assigns ranks to data, dealing with ties by averaging. Ranks are 1-based.\n        Equivalent to scipy.stats.rankdata(method='average').\n        \"\"\"\n        n = len(data)\n        indexed_data = sorted([(data[i], i) for i in range(n)])\n        \n        ranks = [0.0] * n\n        i = 0\n        while i  n:\n            j = i\n            while j  n - 1 and indexed_data[j][0] == indexed_data[j+1][0]:\n                j += 1\n            \n            # Indices of tied items in the sorted list are from i to j.\n            # Ranks are 1-based, so they would occupy ranks from i+1 to j+1.\n            sum_ranks = sum(range(i + 1, j + 2))\n            avg_rank = sum_ranks / (j - i + 1)\n            \n            for k in range(i, j + 1):\n                original_index = indexed_data[k][1]\n                ranks[original_index] = avg_rank\n            \n            i = j + 1\n            \n        return ranks\n\n    all_results = []\n    for case in test_cases:\n        C = case[\"C\"]\n        alpha, beta, gamma = case[\"weights\"]\n        samples = case[\"samples\"]\n        \n        # Lists to store intermediate computed values for each sample\n        margins = []\n        norm_entropies = []\n        raw_grad_norms = []\n        labels = []\n\n        log_C = np.log(C)\n\n        for sample in samples:\n            logits = np.array(sample[\"logits\"])\n            \n            # 1. Compute probabilities using stable softmax\n            z_stable = logits - np.max(logits)\n            p = np.exp(z_stable) / np.sum(np.exp(z_stable))\n            \n            # 2. Compute margin\n            p_sorted = np.sort(p)[::-1]\n            margin = p_sorted[0] - p_sorted[1] if len(p_sorted) > 1 else p_sorted[0]\n            margins.append(margin)\n            \n            # 3. Compute normalized entropy\n            # Take log only of non-zero probabilities to avoid -inf from log(0)\n            non_zero_p = p[p > 0]\n            entropy = -np.sum(non_zero_p * np.log(non_zero_p))\n            norm_entropy = entropy / log_C\n            norm_entropies.append(norm_entropy)\n            \n            raw_grad_norms.append(sample[\"grad_norm\"])\n            labels.append(sample[\"y_mem\"])\n\n        # 4. Normalize gradient norms for the entire test case\n        g = np.array(raw_grad_norms)\n        g_min, g_max = np.min(g), np.max(g)\n        if g_max == g_min:\n            g_norm = np.zeros_like(g, dtype=float)\n        else:\n            g_norm = (g - g_min) / (g_max - g_min)\n\n        # 5. Compute the final risk score for each sample\n        risk_scores = [\n            alpha * (1 - m) + beta * h_norm + gamma * gn\n            for m, h_norm, gn in zip(margins, norm_entropies, g_norm)\n        ]\n        \n        # 6. Compute ROC AUC using the Mann-Whitney U statistic\n        n0 = labels.count(0)\n        n1 = labels.count(1)\n        \n        if n0 == 0 or n1 == 0:\n            # Although test cases prevent this, handle the edge case.\n            # AUC is typically defined as 0.5 if one class is missing.\n            auc = 0.5  \n        else:\n            ranks = rankdata(risk_scores)\n            sum_ranks_members = sum(ranks[i] for i, label in enumerate(labels) if label == 1)\n            \n            U = sum_ranks_members - (n1 * (n1 + 1) / 2.0)\n            auc = U / (float(n0) * float(n1))\n            \n        all_results.append(auc)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in all_results)}]\")\n\nsolve()\n```", "id": "3149361"}]}