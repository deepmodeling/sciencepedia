{"hands_on_practices": [{"introduction": "让我们通过一个基础实践来深入彩票假说（LTH）的核心。在这个练习中，你将在一个简单的线性回归模型的可控环境中，亲手实现并比较两种剪枝策略：迭代式幅度剪枝（Iterative Magnitude Pruning, IMP）和单次剪枝。通过这个过程，你不仅能掌握剪枝、重置和再训练的基本流程，还能通过一个“轨迹对齐分数”的量化指标，探索为什么迭代寻找中奖彩票可能比一次性剪枝更有效 [@problem_id:3188076]。", "problem": "要求您形式化并实现一个受控比较，对比迭代幅度剪枝（IMP）与回溯，以及在相同目标稀疏度下的单次剪枝，以测试重复回溯到固定训练迭代次数是否能提供可归因于优化轨迹对齐的独特优势。该实验必须在一个完全指定、所有组件均为解析定义的监督学习纯数学代理模型中进行。\n\n考虑一个线性模型，其参数为 $w \\in \\mathbb{R}^d$，数据矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，目标向量为 $y \\in \\mathbb{R}^n$。目标是正则化最小二乘损失\n$$\nL(w) = \\frac{1}{2n}\\,\\lVert X w - y \\rVert_2^2 + \\frac{\\lambda}{2}\\,\\lVert w \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，$n$ 和 $d$ 已给定。训练通过固定步长 $\\eta > 0$ 的全批量梯度下降进行：\n$$\nw_{t+1} = w_t - \\eta \\nabla L(w_t), \\quad \\text{其中} \\quad \\nabla L(w) = \\frac{1}{n}\\,X^\\top\\!\\left(X w - y\\right) + \\lambda w.\n$$\n假设选择的步长满足梯度下降在平滑凸函数上的标准稳定性条件，即\n$$\n0 \\le \\eta \\le \\frac{1}{L_{\\max}}, \\quad \\text{其中} \\quad L_{\\max} \\text{ 是 } \\frac{1}{n}X^\\top X + \\lambda I_d \\text{ 的最大特征值}。\n$$\n\n定义一个二元掩码 $m \\in \\{0,1\\}^d$，指示哪些参数被保留（$1$）或剪枝（$0$）。当使用掩码 $m$ 进行训练时，更新会被投影到掩码指定的坐标上，以使被剪枝的坐标保持为零。具体来说，从一个初始化 $w_{\\text{init}}$ 开始，一步带掩码的梯度下降是\n$$\nw^{+} = m \\odot \\bigl(w - \\eta \\nabla L(w)\\bigr),\n$$\n其中 $\\odot$ 表示逐元素乘法，并且在任何给定的带掩码训练阶段，$m$ 都保持固定。\n\n目标稀疏度为 $s \\in [0,1)$ 的幅度剪枝保留绝对值最大的 $K$ 个坐标，并将其余坐标设为零，其中\n$$\nK = \\left\\lceil (1 - s)\\,d \\right\\rceil.\n$$\n单次剪枝通过从 $w_0$ 开始训练密集模型 $T$ 步得到 $w_T$，然后保留 $|w_T|$ 中最大的 $K$ 个坐标，从而计算出一个掩码 $m_{\\text{SS}}$。然后，单次剪枝的解决方案从回溯点 $w_k$（从 $w_0$ 开始训练 $k$ 步后的密集参数）开始，使用掩码进行 $T$ 步训练，即在 $m_{\\text{SS}} \\odot w_k$ 处初始化。带回溯的迭代幅度剪枝（IMP）执行 $R$ 轮，索引为 $r \\in \\{1,\\dots,R\\}$：在每一轮中，从 $m^{(r-1)} \\odot w_k$（其中 $m^{(0)} = \\mathbf{1}$）开始，进行 $T$ 步带掩码的训练得到 $w_T^{(r)}$，然后在仍然未剪枝的坐标中，剪掉 $|w_T^{(r)}|$ 中绝对值最小的额外坐标以更新 $m^{(r)}$，并重复此过程，直到最终掩码 $m_{\\text{IMP}} = m^{(R)}$ 恰好有 $K$ 个保留的坐标。在最后一轮之后，从 $m_{\\text{IMP}} \\odot w_k$ 开始，进行 $T$ 步带掩码的训练，以产生 IMP 解决方案。\n\n为掩码 $m$ 定义在回溯时间 $k$ 的轨迹对齐分数，作为 $w_k$ 处的梯度能量被保留坐标捕获的比例：\n$$\n\\mathcal{A}(m; k) = \\frac{\\left\\| m \\odot \\nabla L(w_k) \\right\\|_2^2}{\\left\\| \\nabla L(w_k) \\right\\|_2^2},\n$$\n约定如果 $\\left\\| \\nabla L(w_k) \\right\\|_2 = 0$，则 $\\mathcal{A}(m;k) = 1$。\n\n您的任务是实现这两个过程，并为每个测试用例报告一个包含四个实数的列表\n$$\n\\bigl[L_{\\text{IMP}},\\,L_{\\text{SS}},\\,\\mathcal{A}(m_{\\text{IMP}};k),\\,\\mathcal{A}(m_{\\text{SS}};k)\\bigr],\n$$\n其中 $L_{\\text{IMP}}$ 和 $L_{\\text{SS}}$ 分别是 IMP 和单次剪枝解决方案的最终损失，在其各自的最终掩码下，从回溯点 $w_k$ 开始进行最后的 $T$ 步训练后计算得出，而 $\\mathcal{A}$ 是上面定义的对齐分数。\n\n实现要求：\n- 使用上面定义的确切梯度下降更新。\n- 使用稳定性步长 $\\eta = 1 / L_{\\max}$，其中 $L_{\\max}$ 根据每个测试用例提供的 $X$ 和 $\\lambda$ 计算。\n- 在 IMP 中，设置每轮的剪枝，使得最终保留的坐标数恰好为 $K$。您可以每轮剪枝一个固定的比例，但必须进行舍入校正，以确保在 $R$ 轮后恰好剩下 $K$ 个。在每一轮中，仅在仍然未剪枝的坐标中进行剪枝，选择那些在该轮 $T$ 步带掩码训练结束时绝对值最小的坐标。\n- 在单次剪枝中，从 $w_0$ 开始进行 $T$ 步密集训练结束时的幅度中选择 $m_{\\text{SS}}$，然后回溯到 $w_k$，应用掩码，并进行 $T$ 步带掩码的训练。\n\n测试套件：\n对于每个测试用例，您将获得 $X$、$y$、$w_0$、$\\lambda$、$s$、$k$、$T$ 和 $R$。所有数字都是实数且维度一致。矩阵和向量如下：\n\n- 测试用例 1：\n  - $n = 6$, $d = 8$,\n  $$\n  X = \\begin{bmatrix}\n  0.7  -0.2  0.3  0.0  0.5  -0.1  0.2  0.4 \\\\\n  -0.1  0.8  -0.3  0.6  0.0  0.2  -0.5  0.1 \\\\\n  0.4  -0.6  0.9  -0.2  0.1  0.3  0.0  -0.4 \\\\\n  0.3  0.1  0.2  0.7  -0.3  0.0  0.6  -0.2 \\\\\n  -0.5  0.4  -0.1  0.3  0.2  -0.7  0.1  0.0 \\\\\n  0.2  -0.3  0.5  -0.4  0.6  0.1  -0.2  0.3\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 0.9 \\\\ -0.2 \\\\ 0.5 \\\\ 0.7 \\\\ -0.6 \\\\ 0.1 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\\\ 0.01 \\\\ -0.04 \\\\ 0.02 \\\\ 0.0 \\\\ -0.01 \\end{bmatrix},\n  \\quad \\lambda = 0.1, \\quad s = 0.5, \\quad k = 5, \\quad T = 200, \\quad R = 4.\n  $$\n\n- 测试用例 2：\n  - $n = 8$, $d = 8$,\n  $$\n  X = I_8,\n  \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.5 \\\\ 0.8 \\\\ -1.2 \\\\ 0.3 \\\\ 0.0 \\\\ 0.7 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} -0.01 \\\\ 0.02 \\\\ -0.03 \\\\ 0.01 \\\\ 0.0 \\\\ 0.02 \\\\ -0.02 \\\\ 0.01 \\end{bmatrix},\n  \\quad \\lambda = 0.0, \\quad s = 0.1, \\quad k = 0, \\quad T = 100, \\quad R = 3.\n  $$\n\n- 测试用例 3：\n  - $n = 10$, $d = 8$,\n  $$\n  X = \\begin{bmatrix}\n  1.0  0.95  0.1  -0.1  0.0  0.2  -0.2  0.1 \\\\\n  0.9  0.85  -0.1  0.0  0.1  -0.1  0.2  -0.2 \\\\\n  1.1  1.05  0.0  0.1  -0.1  0.0  0.1  0.2 \\\\\n  0.8  0.75  0.2  -0.2  0.2  -0.2  0.0  -0.1 \\\\\n  1.2  1.15  -0.2  0.2  -0.2  0.1  -0.1  0.0 \\\\\n  1.0  0.95  0.1  -0.2  0.2  0.0  0.2  -0.2 \\\\\n  0.95  0.9  0.0  0.2  -0.1  0.1  -0.2  0.2 \\\\\n  1.05  1.0  -0.1  0.0  0.0  -0.1  0.1  -0.1 \\\\\n  0.85  0.8  0.2  -0.1  0.1  0.0  0.0  0.1 \\\\\n  1.15  1.1  -0.2  0.1  -0.1  0.1  -0.1  0.0\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 1.2 \\\\ 0.6 \\\\ 1.4 \\\\ 1.0 \\\\ 0.9 \\\\ 1.1 \\\\ 0.7 \\\\ 1.3 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} 0.02 \\\\ -0.01 \\\\ 0.0 \\\\ 0.01 \\\\ -0.02 \\\\ 0.03 \\\\ -0.01 \\\\ 0.0 \\end{bmatrix},\n  \\quad \\lambda = 0.05, \\quad s = 0.8, \\quad k = 10, \\quad T = 300, \\quad R = 5.\n  $$\n\n- 测试用例 4：\n  - $n = 5$, $d = 6$,\n  $$\n  X = \\begin{bmatrix}\n  0.6  -0.1  0.2  0.0  0.3  -0.2 \\\\\n  0.0  0.5  -0.3  0.4  -0.1  0.2 \\\\\n  0.2  -0.4  0.6  -0.2  0.0  0.1 \\\\\n  -0.3  0.2  0.1  0.5  -0.2  0.0 \\\\\n  0.1  0.0  -0.2  0.3  0.4  -0.1\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 0.4 \\\\ -0.1 \\\\ 0.5 \\\\ 0.3 \\\\ -0.2 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} -0.01 \\\\ 0.02 \\\\ 0.01 \\\\ -0.02 \\\\ 0.0 \\\\ 0.03 \\end{bmatrix},\n  \\quad \\lambda = 0.15, \\quad s = 0.0, \\quad k = 7, \\quad T = 150, \\quad R = 4.\n  $$\n\n您的程序必须按照 1, 2, 3, 4 的顺序为每个测试用例计算四元组 $\\bigl[L_{\\text{IMP}},\\,L_{\\text{SS}},\\,\\mathcal{A}(m_{\\text{IMP}};k),\\,\\mathcal{A}(m_{\\text{SS}};k)\\bigr]$，并生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个用例的结果本身是一个包含四个浮点数的列表。例如，最终输出应具有以下形式：\n$$\n\\bigl[ [a_1,b_1,c_1,d_1], [a_2,b_2,c_2,d_2], [a_3,b_3,c_3,d_3], [a_4,b_4,c_4,d_4] \\bigr],\n$$\n不含任何额外文本。您必须将数值输出为普通的实数；此任务不涉及单位或百分比。如果您愿意，可以四舍五入到固定的小数位数，只要您输出的是实数即可。", "solution": "用户提供了一个定义明确的计算问题，要求在正则化线性回归的受控环境中，形式化并比较两种神经网络剪枝策略：迭代幅度剪枝（IMP）和单次（SS）剪枝。该问题具有科学依据，数学上精确，并且算法上已指定，因此可以进行完整求解。\n\n问题的核心是模拟两种不同的剪枝流程，并根据最终性能（损失）和提出的“轨迹对齐”指标对它们进行评估。该指标假设剪枝掩码的成功与它在训练早期（“回溯”点）保持梯度方向的能力有关。\n\n解决方案将构建为一系列函数，以实现所需的数学和算法组件，然后协调这些组件来处理每个测试用例。\n\n**1. 数学预备知识**\n\n损失函数是正则化最小二乘目标函数：\n$$\nL(w) = \\frac{1}{2n}\\,\\lVert X w - y \\rVert_2^2 + \\frac{\\lambda}{2}\\,\\lVert w \\rVert_2^2\n$$\n其梯度（用于梯度下降）是：\n$$\n\\nabla L(w) = \\frac{1}{n}\\,X^\\top(X w - y) + \\lambda w\n$$\n训练算法是固定步长 $\\eta$ 的梯度下降。为了在这种凸环境下保证稳定性和收敛性，步长必须小于或等于梯度 Lipschitz 常数的倒数 $L_{\\text{max}}$。梯度的 Lipschitz 常数是 Hessian 矩阵 $\\nabla^2 L(w) = \\frac{1}{n}X^\\top X + \\lambda I_d$ 的最大特征值。我们将使用上界 $\\eta = 1/L_{\\max}$。\n\n**2. 算法实现步骤**\n\n每个测试用例的总体流程包括以下阶段：\n\n**阶段一：密集预训练与设置**\n首先，我们为两种剪枝方法设置通用参数。\n- 计算要保留的参数数量，$K = \\lceil (1-s)d \\rceil$，其中 $s$ 是目标稀疏度，$d$ 是模型维度。\n- 计算 Hessian 矩阵 $H = \\frac{1}{n}X^\\top X + \\lambda I_d$。\n- 计算 $H$ 的最大特征值 $L_{\\max}$，以确定稳定的步长 $\\eta = 1/L_{\\max}$。\n- 从初始权重 $w_0$ 开始，进行 $T$ 步的完整密集训练。我们必须存储权重向量的整个轨迹 $\\{w_0, w_1, \\dots, w_T\\}$，以便访问回溯权重 $w_k$ 和用于单次掩码生成的权重 $w_T$。\n- 计算回溯点处的梯度 $\\nabla L(w_k)$，这对于稍后计算轨迹对齐分数至关重要。\n\n**阶段二：单次（SS）剪枝**\n单次剪枝的流程如下：\n1.  **掩码生成**：掩码 $m_{\\text{SS}}$ 是根据 $T$ 步后的密集权重 $w_T$ 确定的。我们识别出 $w_T$ 中对应最大绝对值的 $K$ 个位置，并将其对应的掩码条目设为 1，其余为 0。\n2.  **回溯与训练**：模型回溯到第 $k$ 步。权重初始化为 $w_{\\text{SS, init}} = m_{\\text{SS}} \\odot w_k$。使用带掩码的梯度下降进行 $T$ 步的最终训练阶段：$w^{+} = m_{\\text{SS}} \\odot (w - \\eta \\nabla L(w))$。\n3.  **评估**：使用最终权重 $w_{\\text{SS, final}}$ 计算最终损失 $L_{\\text{SS}}$。对齐分数 $\\mathcal{A}(m_{\\text{SS}}; k)$ 计算为 $\\frac{\\|m_{\\text{SS}} \\odot \\nabla L(w_k)\\|_2^2}{\\|\\nabla L(w_k)\\|_2^2}$。\n\n**阶段三：迭代幅度剪枝（IMP）**\nIMP 涉及一个多轮剪枝过程：\n1.  **剪枝计划**：我们定义一个计划，在 $R$ 轮内剪枝权重以达到最终的目标数量 $K$。线性计划稳健且易于实现：在每一轮 $r \\in \\{1, \\dots, R\\}$ 中，我们计算目标剪枝总数为 $\\text{round}(r \\cdot (d-K)/R)$（在第 $R$ 轮进行最终校正，以确保总共剪枝 $d-K$ 个）。这决定了当前轮次需要额外剪枝多少权重。\n2.  **迭代剪枝与训练**：对于每一轮 $r=1, \\dots, R$：\n    a. 模型回溯到 $w_k$ 并使用当前掩码 $m^{(r-1)}$ 进行掩码，得到起始权重 $w_{\\text{start}}^{(r)} = m^{(r-1)} \\odot w_k$。\n    b. 使用 $m^{(r-1)}$ 进行 $T$ 步带掩码的梯度下降训练模型，得到权重 $w_T^{(r)}$。\n    c. 通过从当前活动集合（其中 $m^{(r-1)}=1$）中剪枝掉在 $w_T^{(r)}$ 中绝对值最小的、按计划数量的权重，来生成新掩码 $m^{(r)}$。\n3.  **最终训练**：经过 $R$ 轮后，得到最终掩码 $m_{\\text{IMP}} = m^{(R)}$。模型再次回溯到 $w_k$，初始化为 $w_{\\text{IMP, init}} = m_{\\text{IMP}} \\odot w_k$，并进行最后的 $T$ 步训练。\n4.  **评估**：计算最终损失 $L_{\\text{IMP}}$。对齐分数 $\\mathcal{A}(m_{\\text{IMP}}; k)$ 的计算方式与 SS 情况类似，使用最终掩码 $m_{\\text{IMP}}$。\n\n**阶段四：报告**\n对于每个测试用例，收集计算出的四个值 $[L_{\\text{IMP}}, L_{\\text{SS}}, \\mathcal{A}(m_{\\text{IMP}};k), \\mathcal{A}(m_{\\text{SS}};k)]$。最终输出将这些四元组聚合到一个列表的列表中。特殊情况逻辑处理 $\\|\\nabla L(w_k)\\|_2 = 0$，将对齐分数设置为 1。\n\n这种结构化的方法确保两种剪枝策略都完全按照规定实现，并且比较是公平和受控的。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It orchestrates the setup, simulation, and result collection.\n    \"\"\"\n    \n    # Define helper functions for loss, gradient, and training\n    def get_loss(w, X, y, lambda_reg):\n        n = X.shape[0]\n        if n == 0:\n            return 0.5 * lambda_reg * np.dot(w, w)\n        residual = X @ w - y\n        loss = (0.5 / n) * np.dot(residual, residual) + 0.5 * lambda_reg * np.dot(w, w)\n        return loss\n\n    def get_grad(w, X, y, lambda_reg):\n        n = X.shape[0]\n        if n == 0:\n            return lambda_reg * w\n        grad = (1.0 / n) * X.T @ (X @ w - y) + lambda_reg * w\n        return grad\n\n    def train_masked_gd(w_init, mask, T, eta, X, y, lambda_reg):\n        w = np.copy(w_init)\n        for _ in range(T):\n            grad = get_grad(w, X, y, lambda_reg)\n            w = mask * (w - eta * grad)\n        return w\n\n    def get_alignment_score(mask, grad_at_wk):\n        norm_grad_sq = np.dot(grad_at_wk, grad_at_wk)\n        if norm_grad_sq < 1e-12: # Handle case where gradient is zero\n            return 1.0\n        masked_grad_norm_sq = np.linalg.norm(mask * grad_at_wk)**2\n        return masked_grad_norm_sq / norm_grad_sq\n\n    # Define test cases as specified in the problem\n    test_cases = [\n        # Test case 1\n        {\n            \"X\": np.array([\n                [0.7, -0.2, 0.3, 0.0, 0.5, -0.1, 0.2, 0.4], [-0.1, 0.8, -0.3, 0.6, 0.0, 0.2, -0.5, 0.1],\n                [0.4, -0.6, 0.9, -0.2, 0.1, 0.3, 0.0, -0.4], [0.3, 0.1, 0.2, 0.7, -0.3, 0.0, 0.6, -0.2],\n                [-0.5, 0.4, -0.1, 0.3, 0.2, -0.7, 0.1, 0.0], [0.2, -0.3, 0.5, -0.4, 0.6, 0.1, -0.2, 0.3]\n            ]),\n            \"y\": np.array([0.9, -0.2, 0.5, 0.7, -0.6, 0.1]),\n            \"w0\": np.array([0.05, -0.02, 0.03, 0.01, -0.04, 0.02, 0.0, -0.01]),\n            \"lambda_reg\": 0.1, \"s\": 0.5, \"k\": 5, \"T\": 200, \"R\": 4\n        },\n        # Test case 2\n        {\n            \"X\": np.identity(8),\n            \"y\": np.array([1.0, 0.5, -0.5, 0.8, -1.2, 0.3, 0.0, 0.7]),\n            \"w0\": np.array([-0.01, 0.02, -0.03, 0.01, 0.0, 0.02, -0.02, 0.01]),\n            \"lambda_reg\": 0.0, \"s\": 0.1, \"k\": 0, \"T\": 100, \"R\": 3\n        },\n        # Test case 3\n        {\n            \"X\": np.array([\n                [1.0, 0.95, 0.1, -0.1, 0.0, 0.2, -0.2, 0.1], [0.9, 0.85, -0.1, 0.0, 0.1, -0.1, 0.2, -0.2],\n                [1.1, 1.05, 0.0, 0.1, -0.1, 0.0, 0.1, 0.2], [0.8, 0.75, 0.2, -0.2, 0.2, -0.2, 0.0, -0.1],\n                [1.2, 1.15, -0.2, 0.2, -0.2, 0.1, -0.1, 0.0], [1.0, 0.95, 0.1, -0.2, 0.2, 0.0, 0.2, -0.2],\n                [0.95, 0.9, 0.0, 0.2, -0.1, 0.1, -0.2, 0.2], [1.05, 1.0, -0.1, 0.0, 0.0, -0.1, 0.1, -0.1],\n                [0.85, 0.8, 0.2, -0.1, 0.1, 0.0, 0.0, 0.1], [1.15, 1.1, -0.2, 0.1, -0.1, 0.1, -0.1, 0.0]\n            ]),\n            \"y\": np.array([1.0, 0.8, 1.2, 0.6, 1.4, 1.0, 0.9, 1.1, 0.7, 1.3]),\n            \"w0\": np.array([0.02, -0.01, 0.0, 0.01, -0.02, 0.03, -0.01, 0.0]),\n            \"lambda_reg\": 0.05, \"s\": 0.8, \"k\": 10, \"T\": 300, \"R\": 5\n        },\n        # Test case 4\n        {\n            \"X\": np.array([\n                [0.6, -0.1, 0.2, 0.0, 0.3, -0.2], [0.0, 0.5, -0.3, 0.4, -0.1, 0.2],\n                [0.2, -0.4, 0.6, -0.2, 0.0, 0.1], [-0.3, 0.2, 0.1, 0.5, -0.2, 0.0],\n                [0.1, 0.0, -0.2, 0.3, 0.4, -0.1]\n            ]),\n            \"y\": np.array([0.4, -0.1, 0.5, 0.3, -0.2]),\n            \"w0\": np.array([-0.01, 0.02, 0.01, -0.02, 0.0, 0.03]),\n            \"lambda_reg\": 0.15, \"s\": 0.0, \"k\": 7, \"T\": 150, \"R\": 4\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        X, y, w0, lambda_reg, s, k, T, R = case.values()\n        n, d = X.shape\n        \n        # --- Stage I: Setup and Dense Pre-training ---\n        K = math.ceil((1 - s) * d)\n        \n        H = (1.0 / n) * (X.T @ X) + lambda_reg * np.eye(d)\n        L_max = np.max(np.linalg.eigvalsh(H))\n        eta = 1.0 / L_max\n        \n        w_dense_traj = [np.copy(w0)]\n        w = np.copy(w0)\n        for _ in range(T):\n            grad = get_grad(w, X, y, lambda_reg)\n            w -= eta * grad\n            w_dense_traj.append(np.copy(w))\n            \n        w_k = w_dense_traj[k]\n        w_T_dense = w_dense_traj[T]\n        grad_at_wk = get_grad(w_k, X, y, lambda_reg)\n\n        # --- Stage II: Single-Shot Pruning (SS) ---\n        m_ss = np.zeros(d, dtype=float)\n        if K > 0:\n            indices_to_keep = np.argpartition(np.abs(w_T_dense), -K)[-K:]\n            m_ss[indices_to_keep] = 1.0\n\n        w_ss_init = m_ss * w_k\n        w_ss_final = train_masked_gd(w_ss_init, m_ss, T, eta, X, y, lambda_reg)\n        L_ss = get_loss(w_ss_final, X, y, lambda_reg)\n        A_ss = get_alignment_score(m_ss, grad_at_wk)\n\n        # --- Stage III: Iterative Magnitude Pruning (IMP) ---\n        m_imp = np.ones(d, dtype=float)\n        pruned_count = 0\n        \n        for r in range(1, R + 1):\n            w_init_round = m_imp * w_k\n            w_trained_round = train_masked_gd(w_init_round, m_imp, T, eta, X, y, lambda_reg)\n            \n            if r < R:\n                target_pruned_total = round(r * (d - K) / R)\n            else: # Final round\n                target_pruned_total = d - K\n\n            num_to_prune_now = int(target_pruned_total - pruned_count)\n            \n            if num_to_prune_now > 0:\n                active_indices = np.where(m_imp == 1.0)[0]\n                active_weights_abs = np.abs(w_trained_round[active_indices])\n                \n                if num_to_prune_now >= len(active_indices):\n                    indices_to_prune_global = active_indices\n                else:\n                    prune_indices_local = np.argpartition(active_weights_abs, num_to_prune_now)[:num_to_prune_now]\n                    indices_to_prune_global = active_indices[prune_indices_local]\n\n                m_imp[indices_to_prune_global] = 0.0\n\n            pruned_count = d - np.sum(m_imp)\n\n        w_imp_init = m_imp * w_k\n        w_imp_final = train_masked_gd(w_imp_init, m_imp, T, eta, X, y, lambda_reg)\n        L_imp = get_loss(w_imp_final, X, y, lambda_reg)\n        A_imp = get_alignment_score(m_imp, grad_at_wk)\n        \n        all_results.append([L_imp, L_ss, A_imp, A_ss])\n\n    print(all_results)\n\nsolve()\n```", "id": "3188076"}, {"introduction": "在掌握了基础剪枝之后，这个练习将你的技能提升到更高级、更贴近实际应用的层面：结构化剪枝。你将在简化的CNN和Transformer模型上，对比在相同稀疏度 $s$ 下，移除整个“构建块”（如通道或头）与移除单个权重之间的差异。这有助于你理解在实践中，为了硬件效率而进行的结构化剪枝与原始的非结构化“彩票”之间的重要权衡 [@problem_id:3188072]。", "problem": "您的任务是通过在两个简化的网络族上以相同的稀疏度比较结构化剪枝和非结构化剪枝，来以编程方式测试彩票假设（LTH）。这两个网络族分别捕捉了卷积神经网络（CNN）和 Transformer 的分组逻辑。目标是为每种情况确定，是否存在一个通过幅度剪枝和回溯到原始初始化获得的稀疏子网络（即“彩票”），当使用相同的优化器从相同的初始化状态进行训练时，其验证准确率能在指定容差范围内匹配稠密模型的验证准确率。\n\n请从以下机器学习的基本概念开始：\n\n- 经验风险最小化：给定标记数据 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$，通过调整参数 $\\theta$ 来最小化经验风险 $\\frac{1}{N}\\sum_{i=1}^N \\ell(f_\\theta(\\mathbf{x}_i), y_i)$。\n- 梯度下降：学习率为 $\\eta$，参数通过 $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathbb{E}[\\ell(f_\\theta(\\mathbf{x}), y)]$ 进行更新，该期望通过数据集上的经验均值来近似。\n- $K=2$ 类别且使用 softmax 输出的交叉熵：如果 logits 为 $\\mathbf{z} \\in \\mathbb{R}^2$ 且 $\\mathbf{p} = \\mathrm{softmax}(\\mathbf{z})$，则对于 one-hot 标签 $\\mathbf{y}$ 的损失为 $\\ell(\\mathbf{z}, \\mathbf{y}) = -\\sum_{k=1}^2 y_k \\log p_k$。\n- ReLU 激活函数：$\\mathrm{ReLU}(a) = \\max(0,a)$，按元素应用。\n- 稀疏水平 $s \\in [0,1]$：剪枝后设为零的权重比例。\n- 幅度剪枝：通过对权重（或整组权重）的绝对值进行排序，并从最小的开始归零，直到达到目标稀疏度 $s$ 来移除它们。\n- 回溯：在根据从已训练的稠密模型中测得的幅度进行剪枝后，将存活的权重重置为其原始初始化值并重新训练。\n\n您将从头开始实现两种玩具架构和训练过程：\n\n- 类 CNN 的分组线性网络：输入维度 $D=16$，通道数 $C=8$，类别数 $K=2$。\n  - 参数：一个带掩码的线性层，其权重为 $W \\in \\mathbb{R}^{C \\times D}$，受一个固定的连接性掩码约束，该掩码为每个通道强制执行大小为 $D/C=2$ 的不相交感受野；之后是 ReLU，然后是一个线性分类器 $V \\in \\mathbb{R}^{C \\times K}$。\n  - 前向传播：对于一个批次 $X \\in \\mathbb{R}^{N \\times D}$：$H = \\mathrm{ReLU}(X W^\\top)$，logits 为 $Z = H V$，通过 softmax 得到概率，并计算交叉熵损失。\n  - 滤波器/通道级别的结构化剪枝：移除整个通道，即在 $W$ 中将其对应的行（在其感受野段内）归零，并在 $V$ 中将其对应的行归零。在相同的稀疏度 $s$ 下，选择要剪枝的通道数量，使得剪枝的总权重比例恰好等于 $s$。\n  - 非结构化剪枝：移除 $W$（仅在感受野段内）和 $V$ 中的单个权重，以达到精确的稀疏度 $s$。\n\n- 类 Transformer 的分组线性网络：输入维度 $D=16$，头数 $H=4$，每头投影维度 $d_h=4$，类别数 $K=2$。\n  - 参数：对于 $h \\in \\{1,\\dots,H\\}$，有每头投影 $P_h \\in \\mathbb{R}^{d_h \\times D}$，各头输出被拼接成一个 $\\mathbb{R}^{H \\cdot d_h}$ 中的向量，然后是一个线性分类器 $U \\in \\mathbb{R}^{H \\cdot d_h \\times K}$。在线性投影后，对每个头应用 ReLU。\n  - 前向传播：对于一个批次 $X \\in \\mathbb{R}^{N \\times D}$：为每个头计算 $Z_h = \\mathrm{ReLU}(X P_h^\\top)$，拼接成 $Z = [Z_1,\\dots,Z_H]$，然后 logits 为 $L = Z U$，通过 softmax 和交叉熵计算损失。\n  - 头级别的结构化剪枝：移除整个头，即将所有的 $P_h$ 以及 $U$ 中对应的行段归零。在相同的稀疏度 $s$ 下，选择要剪枝的头数量，使得剪枝的总权重比例恰好等于 $s$。\n  - 非结构化剪枝：移除所有 $P_h$ 和 $U$ 中的单个权重，以达到精确的稀疏度 $s$。\n\n训练和评估协议必须实现以下步骤：\n\n1. 数据生成：对于每个测试用例，通过采样 $N_{\\text{train}}$ 个训练点和 $N_{\\text{val}}$ 个验证点，在 $\\mathbb{R}^{D}$ 中创建一个平衡的二分类数据集。设 $u \\in \\mathbb{R}^{D}$ 为一个随机单位向量，并定义类别均值 $\\mu_+ = +a u$ 和 $\\mu_- = -a u$，其中 $a > 0$。对于类别 $+1$ 的样本 $\\mathbf{x}$ 从 $\\mathcal{N}(\\mu_+, \\sigma^2 I)$ 中采样，对于类别 $-1$ 的样本从 $\\mathcal{N}(\\mu_-, \\sigma^2 I)$ 中采样。使用 one-hot 标签，并将准确率计算为验证集上正确预测的比例。不使用物理单位；所有量均为无量纲。\n2. 初始化：使用独立的高斯分布条目初始化所有权重，并进行缩放以保持梯度稳定。对于类 CNN 的 $W$，在初始化时通过将感受野之外的条目归零来强制执行连接性掩码。\n3. 稠密训练：使用全批量梯度下降法，以学习率 $\\eta$、交叉熵损失和指定的 ReLU 激活函数，对每个稠密模型（类 CNN 和类 Transformer）训练 $T$ 个轮次。记录每个架构的稠密验证准确率 $A_{\\text{dense}}$。\n4. 剪枝：\n   - 非结构化剪枝：在已训练的稠密权重上，剪枝掉绝对值最小的权重，以在所有可训练权重上达到精确的稀疏度 $s$。为存活的权重生成一个二元掩码。\n   - 结构化剪枝：在已训练的稠密权重上，为每个通道（类 CNN）或头（类 Transformer）计算一个组重要性分数，即该组中权重的绝对值之和。剪枝掉分数最小的组，以达到精确的稀疏度 $s$。为存活的组生成一个二元掩码。\n5. 回溯和重新训练：对于每种剪枝方法和架构，将存活的权重回溯到其原始初始化值，将被剪枝的权重设为零，并使用相同的优化器和学习率重新训练 $T$ 个轮次，同时强制执行掩码，以使被剪枝的权重保持为零。重新训练后计算验证准确率 $A_{\\text{ticket}}$。\n6. 彩票检查：对于每种情况，输出一个布尔值，指示是否满足 $A_{\\text{ticket}} \\geq A_{\\text{dense}} - \\varepsilon$，其中 $\\varepsilon$ 是测试用例中指定的容差。\n\n测试套件和要求的输出：\n\n- 使用 $D=16$、$C=8$、$H=4$、$d_h=4$、$K=2$、$N_{\\text{train}}=256$、$N_{\\text{val}}=256$、$a=2$、$\\sigma=0.5$、$T=200$ 和学习率 $\\eta=0.05$。\n- 测试套件必须包含三个具有不同稀疏水平 $s$ 和随机种子的用例，以测试不同情况：\n  1. 用例 1：seed $=7$，$s=0.0$，容差 $\\varepsilon=0.005$。\n  2. 用例 2：seed $=13$，$s=0.5$，容差 $\\varepsilon=0.03$。\n  3. 用例 3：seed $=42$，$s=0.75$，容差 $\\varepsilon=0.05$。\n- 对于每个用例，按以下顺序计算四个布尔值：\n  - 类 CNN 结构化剪枝彩票存在，\n  - 类 CNN 非结构化剪枝彩票存在，\n  - 类 Transformer 结构化剪枝彩票存在，\n  - 类 Transformer 非结构化剪枝彩票存在。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个列表的列表，每个子列表对应一个测试用例，并按指定顺序包含四个布尔值，例如：$[[b_{11},b_{12},b_{13},b_{14}],[b_{21},b_{22},b_{23},b_{24}],[b_{31},b_{32},b_{33},b_{34}]]$，其中每个 $b_{ij}$ 是 $\\mathrm{True}$ 或 $\\mathrm{False}$。\n\n您的程序必须是一个完整的、可运行的程序，仅使用 Python 标准库以及在执行环境中指定的 NumPy 和 SciPy 库从头开始执行所有计算。不需要用户输入。程序必须仅以指定的精确格式打印单行输出。", "solution": "该问题要求在两种简化的、具有组结构的神经网络架构上，通过比较结构化剪枝和非结构化剪枝，对彩票假设（LTH）进行编程验证。LTH 假设一个稠密的、随机初始化的神经网络包含一个稀疏子网络（即“中奖彩票”），当该子网络从相同的初始状态被隔离训练时，其性能可以匹配原始稠密网络的性能。我们将遵循“剪枝、回溯、重新训练”的方法论，实现从数据生成到模型训练和评估的整个实验流程。\n\n该解决方案在概念上分为几个阶段：定义数学模型，指定基于梯度下降的训练过程，详细说明剪枝算法，最后对给定的测试用例执行完整的实验。\n\n**1. 模型的数学公式和学习**\n\n任务的核心是使用基于梯度的优化来最小化经验风险函数。给定一个数据集 $\\{\\left(\\mathbf{x}_i, y_i\\right)\\}_{i=1}^N$，我们寻求找到参数 $\\theta$ 以最小化平均损失：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f_{\\theta}(\\mathbf{x}_i), y_i)\n$$\n其中 $f_{\\theta}$ 是模型，$\\ell$ 是损失函数。我们将使用全批量梯度下降，其中每个轮次的参数更新由下式给出：\n$$\n\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)\n$$\n其中 $\\eta$ 是学习率。\n\n损失函数是针对 $K=2$ 个类别的交叉熵。对于单个数据点 $(\\mathbf{x}, \\mathbf{y})$，其 one-hot 标签为 $\\mathbf{y} \\in \\{[1, 0]^\\top, [0, 1]^\\top\\}$，模型 logits 为 $\\mathbf{z} = f_{\\theta}(\\mathbf{x}) \\in \\mathbb{R}^2$，概率通过 softmax 函数计算，$\\mathbf{p} = \\mathrm{softmax}(\\mathbf{z})$，其中 $p_k = e^{z_k} / \\sum_{j=1}^2 e^{z_j}$。损失是：\n$$\n\\ell(\\mathbf{z}, \\mathbf{y}) = -\\sum_{k=1}^2 y_k \\log p_k\n$$\n在数值计算上，这通常计算为 $\\ell(\\mathbf{z}, \\mathbf{y}) = \\log\\left(\\sum_{k=1}^2 e^{z_k}\\right) - \\mathbf{y} \\cdot \\mathbf{z}$ 以保持稳定性。对于一个包含 $N$ 个样本的批次，损失相对于 logits 的梯度是 $\\nabla_{\\mathbf{Z}} \\mathcal{L} = \\frac{1}{N}(\\mathbf{P} - \\mathbf{Y})$，其中 $\\mathbf{P}$ 和 $\\mathbf{Y}$ 分别是预测概率矩阵和 one-hot 标签矩阵。\n\n我们考虑两种使用 $\\mathrm{ReLU}(a) = \\max(0, a)$ 激活函数的架构。\n\n**1.1. 类 CNN 的分组线性网络**\n此模型简化了卷积层的通道化、局部感受野结构。\n- 参数：$\\theta_{CNN} = \\{W, V\\}$，其中 $W \\in \\mathbb{R}^{C \\times D}$ 和 $V \\in \\mathbb{R}^{C \\times K}$。\n- 维度：输入 $D=16$，通道 $C=8$，类别 $K=2$。\n- 结构约束：权重矩阵 $W$ 受一个固定的二元掩码 $M_W$ 约束，该掩码强制执行 $C$ 个大小为 $D/C=2$ 的不相交感受野。具体来说，仅当 $2c \\le j < 2(c+1)$ 时 $M_W[c, j] = 1$，否则为 $0$。可训练的权重实际上是 $W \\odot M_W$，其中 $\\odot$ 是逐元素乘积。\n- 前向传播：对于输入批次 $X \\in \\mathbb{R}^{N \\times D}$，隐藏表示为 $H = \\mathrm{ReLU}(X (W \\odot M_W)^\\top)$，logits 为 $Z = H V$。\n- 梯度：使用链式法则，$\\nabla_V \\mathcal{L} = H^\\top (\\nabla_Z \\mathcal{L})$ 且 $\\nabla_W \\mathcal{L} = ((\\nabla_Z \\mathcal{L} V^\\top) \\odot \\mathbb{I}[X (W \\odot M_W)^\\top > 0])^\\top X \\odot M_W$，其中 $\\mathbb{I}[\\cdot]$ 是指示函数。\n\n**1.2. 类 Transformer 的分组线性网络**\n此模型捕捉了注意力机制的多头结构，其中输入由多个“头”独立投影。\n- 参数：$\\theta_{Trans} = \\{P_1, \\dots, P_H, U\\}$，其中每个 $P_h \\in \\mathbb{R}^{d_h \\times D}$ 且 $U \\in \\mathbb{R}^{(H \\cdot d_h) \\times K}$。\n- 维度：输入 $D=16$，头 $H=4$，头维度 $d_h=4$，类别 $K=2$。\n- 前向传播：对于 $X \\in \\mathbb{R}^{N \\times D}$，每个头计算一个表示 $Z_h = \\mathrm{ReLU}(X P_h^\\top)$。这些表示被拼接成 $Z_{cat} = [Z_1 | Z_2 | \\dots | Z_H] \\in \\mathbb{R}^{N \\times (H \\cdot d_h)}$。最终的 logits 是 $L = Z_{cat} U$。\n- 梯度：$\\nabla_U \\mathcal{L} = Z_{cat}^\\top (\\nabla_L \\mathcal{L})$。相对于拼接隐藏状态的梯度 $\\nabla_{Z_{cat}} \\mathcal{L} = \\nabla_L \\mathcal{L} U^\\top$ 被分区，以找到每个头参数的梯度：$\\nabla_{P_h} \\mathcal{L} = ((\\nabla_{Z_h} \\mathcal{L}) \\odot \\mathbb{I}[X P_h^\\top > 0])^\\top X$。\n\n**2. 数据生成**\n\n生成一个合成的、线性可分的二分类数据集。\n- 选择一个随机单位向量 $\\mathbf{u} \\in \\mathbb{R}^D$。\n- 两个类别均值定义为 $\\boldsymbol{\\mu}_+ = a \\mathbf{u}$ 和 $\\boldsymbol{\\mu}_- = -a \\mathbf{u}$。\n- 训练和验证数据从各向同性高斯分布中采样：$\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_y, \\sigma^2 I)$，其中 $y \\in \\{+1, -1\\}$ 是类别标签。对于本实验，$N_{\\text{train}}=256$，$N_{\\text{val}}=256$，$a=2$，且 $\\sigma=0.5$。\n\n**3. LTH 实验协议**\n\n对于每个架构和测试用例（由随机种子、稀疏度 $s$ 和容差 $\\varepsilon$ 定义）：\n1.  **初始化**：所有可训练权重均通过从高斯分布 $\\mathcal{N}(0, \\sigma_{init}^2)$ 中采样来初始化，其中 $\\sigma_{init}^2$ 按层的扇入 (fan-in) 进行缩放，以确保梯度稳定。例如，对于权重矩阵为 $W_{\\text{layer}}$ 且扇入为 $d_{in}$ 的层，权重从 $\\mathcal{N}(0, 1/d_{in})$ 中抽取。这些初始参数 $\\theta_0$ 被存储起来。\n2.  **稠密训练**：使用全批量梯度下降法，以学习率 $\\eta=0.05$ 对完整（稠密）模型进行 $T=200$ 个轮次的训练。得到的训练后参数为 $\\theta_T$。在保留的验证集上计算验证准确率并存储为 $A_{\\text{dense}}$。\n3.  **剪枝**：使用训练后的权重 $\\theta_T$，生成一个二元掩码 $M_{prune}$ 以达到目标稀疏水平 $s$。\n    - **非结构化剪枝**：收集所有参数张量中的所有单个权重。对其绝对幅度 $|\\theta_{T,i}|$ 进行排序。找到一个阈值，使得将低于此阈值的权重设为零后，总权重中有比例为 $s$ 的权重为零。掩码 $M_{prune}$ 对存活的权重为 $1$，否则为 $0$。\n    - **结构化剪枝**：按通道（类 CNN）或头（类 Transformer）对权重进行分组。为每个组计算一个重要性分数，定义为该组内所有权重的绝对幅度之和。剪枝掉得分最低的组，直到被剪枝的参数比例恰好等于 $s$。掩码 $M_{prune}$ 将属于被剪枝组的所有权重归零。\n4.  **回溯和重新训练**：将存活的权重重置为它们在 $\\theta_0$ 中的初始值。这创建了“中奖彩票”的初始化 $\\theta'_{0} = \\theta_0 \\odot M_{prune}$。然后对稀疏子网络进行 $T=200$ 个轮次的重新训练，并在每次梯度更新后应用剪枝掩码 $M_{prune}$，以确保被剪枝的权重保持为零。\n5.  **评估**：计算重新训练后的子网络的验证准确率 $A_{\\text{ticket}}$。\n6.  **彩票检查**：如果其性能接近稠密模型的性能，即 $A_{\\text{ticket}} \\geq A_{\\text{dense}} - \\varepsilon$，则认为存在中奖彩票。对架构和剪枝方法的每种组合都进行此检查。\n\n该过程针对三个具有不同稀疏水平 $s \\in \\{0.0, 0.5, 0.75\\}$ 的测试用例执行，并将结果编译成指定的输出格式。$s=0.0$ 的情况作为基线，此时不发生剪枝，因此 $A_{\\text{ticket}}$ 预期与 $A_{\\text{dense}}$ 相同，确保对于任何 $\\varepsilon > 0$ 该检查都会通过。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, sparsity_level, tolerance)\n        (7, 0.0, 0.005),\n        (13, 0.5, 0.03),\n        (42, 0.75, 0.05),\n    ]\n    \n    # Global parameters\n    D = 16  # Input dimension\n    K = 2   # Number of classes\n    N_TRAIN = 256\n    N_VAL = 256\n    A = 2.0\n    SIGMA = 0.5\n    T_EPOCHS = 200\n    ETA = 0.05\n    \n    # CNN-like parameters\n    C = 8\n    \n    # Transformer-like parameters\n    H = 4\n    D_H = 4\n\n    results = []\n    for seed, s, eps in test_cases:\n        case_results = []\n        \n        # Set seed for reproducibility\n        np.random.seed(seed)\n        \n        # 1. Data Generation\n        u = np.random.randn(D)\n        u /= np.linalg.norm(u)\n        mu_plus = A * u\n        mu_minus = -A * u\n        \n        # Training data\n        x_train_plus = np.random.multivariate_normal(mu_plus, SIGMA**2 * np.eye(D), N_TRAIN // 2)\n        x_train_minus = np.random.multivariate_normal(mu_minus, SIGMA**2 * np.eye(D), N_TRAIN // 2)\n        x_train = np.vstack((x_train_plus, x_train_minus))\n        y_train_labels = np.array([1] * (N_TRAIN // 2) + [0] * (N_TRAIN // 2))\n        y_train = np.eye(K)[y_train_labels]\n\n        # Validation data\n        x_val_plus = np.random.multivariate_normal(mu_plus, SIGMA**2 * np.eye(D), N_VAL // 2)\n        x_val_minus = np.random.multivariate_normal(mu_minus, SIGMA**2 * np.eye(D), N_VAL // 2)\n        x_val = np.vstack((x_val_plus, x_val_minus))\n        y_val_labels = np.array([1] * (N_VAL // 2) + [0] * (N_VAL // 2))\n        y_val = np.eye(K)[y_val_labels]\n\n        # --- Helper functions ---\n        def accuracy(y_pred_logits, y_true_one_hot):\n            pred_labels = np.argmax(y_pred_logits, axis=1)\n            true_labels = np.argmax(y_true_one_hot, axis=1)\n            return np.mean(pred_labels == true_labels)\n\n        # --- CNN-like Architecture ---\n        def run_cnn_experiment():\n            # Init\n            w_init = np.random.randn(C, D) * np.sqrt(1.0 / D)\n            v_init = np.random.randn(C, K) * np.sqrt(1.0 / C)\n            \n            w_connectivity_mask = np.zeros_like(w_init)\n            receptive_field_size = D // C\n            for i in range(C):\n                w_connectivity_mask[i, i*receptive_field_size:(i+1)*receptive_field_size] = 1.0\n            \n            w_init *= w_connectivity_mask\n\n            def forward_cnn(x, w, v):\n                h = np.maximum(0, x @ w.T)\n                z = h @ v\n                return z, h\n\n            def gradients_cnn(x, y, w, v, w_mask):\n                n_samples = x.shape[0]\n                z, h = forward_cnn(x, w, v)\n                log_p = z - logsumexp(z, axis=1, keepdims=True)\n                \n                dz = (np.exp(log_p) - y) / n_samples\n                \n                dv = h.T @ dz\n                dh = dz @ v.T\n                dh[h <= 0] = 0\n                dw = (dh.T @ x) * w_mask\n\n                return dw, dv\n\n            def train(init_params, masks, grad_fn, T, eta, x, y, w_mask=None):\n                w, v = [p.copy() for p in init_params]\n                w_prune_mask, v_prune_mask = masks\n                \n                for _ in range(T):\n                    dw, dv = grad_fn(x, y, w, v, w_mask if w_mask is not None else np.ones_like(w))\n                    w -= eta * dw\n                    v -= eta * dv\n                    w *= w_prune_mask\n                    v *= v_prune_mask\n                return w, v\n\n            # Dense model training\n            dense_masks = (np.ones_like(w_init), np.ones_like(v_init))\n            w_dense, v_dense = train((w_init, v_init), dense_masks, gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_dense, _ = forward_cnn(x_val, w_dense, v_dense)\n            acc_dense = accuracy(logits_dense, y_val)\n            \n            # Structured Pruning\n            if s > 0:\n                weights_per_channel = (D // C) + K\n                total_weights = C * weights_per_channel\n                num_ch_to_prune = int(round(s * total_weights / weights_per_channel))\n                \n                channel_scores = np.sum(np.abs(w_dense), axis=1) + np.sum(np.abs(v_dense), axis=1)\n                pruned_channels_indices = np.argsort(channel_scores)[:num_ch_to_prune]\n                \n                w_mask_struct = np.ones_like(w_init)\n                v_mask_struct = np.ones_like(v_init)\n                w_mask_struct[pruned_channels_indices, :] = 0\n                v_mask_struct[pruned_channels_indices, :] = 0\n            else:\n                w_mask_struct, v_mask_struct = dense_masks\n\n            w_ticket_s, v_ticket_s = train((w_init, v_init), (w_mask_struct * w_connectivity_mask, v_mask_struct), gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_ticket_s, _ = forward_cnn(x_val, w_ticket_s, v_ticket_s)\n            acc_ticket_s = accuracy(logits_ticket_s, y_val)\n            lth_check_struct = acc_ticket_s >= acc_dense - eps\n\n            # Unstructured Pruning\n            if s > 0:\n                all_weights = np.concatenate([w_dense[w_connectivity_mask == 1], v_dense.flatten()])\n                threshold = np.percentile(np.abs(all_weights), s * 100)\n                \n                w_mask_unstruct = w_connectivity_mask.copy()\n                w_mask_unstruct[w_connectivity_mask == 1] = (np.abs(w_dense[w_connectivity_mask == 1]) >= threshold).astype(float)\n                v_mask_unstruct = (np.abs(v_dense) >= threshold).astype(float)\n            else:\n                w_mask_unstruct, v_mask_unstruct = dense_masks\n\n            w_ticket_u, v_ticket_u = train((w_init, v_init), (w_mask_unstruct, v_mask_unstruct), gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_ticket_u, _ = forward_cnn(x_val, w_ticket_u, v_ticket_u)\n            acc_ticket_u = accuracy(logits_ticket_u, y_val)\n            lth_check_unstruct = acc_ticket_u >= acc_dense - eps\n            \n            return lth_check_struct, lth_check_unstruct\n\n        cnn_struct, cnn_unstruct = run_cnn_experiment()\n        case_results.extend([cnn_struct, cnn_unstruct])\n        \n        # --- Transformer-like Architecture ---\n        def run_transformer_experiment():\n            # Init\n            p_list_init = [np.random.randn(D_H, D) * np.sqrt(1.0 / D) for _ in range(H)]\n            u_init = np.random.randn(H * D_H, K) * np.sqrt(1.0 / (H * D_H))\n\n            def forward_transformer(x, p_list, u):\n                z_h_list = [np.maximum(0, x @ p.T) for p in p_list]\n                z_cat = np.concatenate(z_h_list, axis=1)\n                l = z_cat @ u\n                return l, z_h_list, z_cat\n\n            def gradients_transformer(x, y, p_list, u):\n                n_samples = x.shape[0]\n                l, z_h_list, z_cat = forward_transformer(x, p_list, u)\n                log_p = l - logsumexp(l, axis=1, keepdims=True)\n\n                dl = (np.exp(log_p) - y) / n_samples\n                \n                du = z_cat.T @ dl\n                dz_cat = dl @ u.T\n                \n                dp_list = []\n                for h in range(H):\n                    dz_h = dz_cat[:, h*D_H:(h+1)*D_H]\n                    dz_h[z_h_list[h] <= 0] = 0\n                    dp_h = dz_h.T @ x\n                    dp_list.append(dp_h)\n                \n                return dp_list, du\n\n            def train_t(init_params, masks, T, eta, x, y):\n                p_list, u = [p.copy() for p in init_params[0]], init_params[1].copy()\n                p_masks, u_mask = masks\n                \n                for _ in range(T):\n                    dp_list, du = gradients_transformer(x, y, p_list, u)\n                    for h in range(H):\n                        p_list[h] -= eta * dp_list[h]\n                        p_list[h] *= p_masks[h]\n                    u -= eta * du\n                    u *= u_mask\n                return p_list, u\n            \n            # Dense model training\n            dense_p_masks = [np.ones_like(p) for p in p_list_init]\n            dense_u_mask = np.ones_like(u_init)\n            \n            p_list_dense, u_dense = train_t((p_list_init, u_init), (dense_p_masks, dense_u_mask), T_EPOCHS, ETA, x_train, y_train)\n            logits_dense_t, _, _ = forward_transformer(x_val, p_list_dense, u_dense)\n            acc_dense_t = accuracy(logits_dense_t, y_val)\n            \n            # Structured pruning\n            if s > 0:\n                weights_per_head = (D_H * D) + (D_H * K)\n                total_weights = H * weights_per_head\n                num_heads_to_prune = int(round(s * total_weights / weights_per_head))\n                \n                head_scores = [np.sum(np.abs(p_list_dense[h])) + np.sum(np.abs(u_dense[h*D_H:(h+1)*D_H, :])) for h in range(H)]\n                pruned_head_indices = np.argsort(head_scores)[:num_heads_to_prune]\n                \n                p_masks_struct = [np.ones_like(p) for p in p_list_init]\n                u_mask_struct = np.ones_like(u_init)\n                for h_idx in pruned_head_indices:\n                    p_masks_struct[h_idx] = 0.\n                    u_mask_struct[h_idx*D_H:(h_idx+1)*D_H, :] = 0.\n            else:\n                p_masks_struct, u_mask_struct = dense_p_masks, dense_u_mask\n            \n            p_list_ticket_s, u_ticket_s = train_t((p_list_init, u_init), (p_masks_struct, u_mask_struct), T_EPOCHS, ETA, x_train, y_train)\n            logits_ticket_s, _, _ = forward_transformer(x_val, p_list_ticket_s, u_ticket_s)\n            acc_ticket_s_t = accuracy(logits_ticket_s, y_val)\n            lth_check_struct_t = acc_ticket_s_t >= acc_dense_t - eps\n\n            # Unstructured pruning\n            if s > 0:\n                all_weights = np.concatenate([p.flatten() for p in p_list_dense] + [u_dense.flatten()])\n                threshold = np.percentile(np.abs(all_weights), s * 100)\n\n                p_masks_unstruct = [(np.abs(p) >= threshold).astype(float) for p in p_list_dense]\n                u_mask_unstruct = (np.abs(u_dense) >= threshold).astype(float)\n            else:\n                p_masks_unstruct, u_mask_unstruct = dense_p_masks, dense_u_mask\n\n            p_list_ticket_u, u_ticket_u = train_t((p_list_init, u_init), (p_masks_unstruct, u_mask_unstruct), T_EPOCHS, ETA, x_train, y_train)\n            logits_ticket_u, _, _ = forward_transformer(x_val, p_list_ticket_u, u_ticket_u)\n            acc_ticket_u_t = accuracy(logits_ticket_u, y_val)\n            lth_check_unstruct_t = acc_ticket_u_t >= acc_dense_t - eps\n            \n            return lth_check_struct_t, lth_check_unstruct_t\n\n        trans_struct, trans_unstruct = run_transformer_experiment()\n        case_results.extend([trans_struct, trans_unstruct])\n        \n        results.append(case_results)\n\n    # Convert boolean arrays to Python booleans for standard JSON-like output\n    final_results = [[bool(val) for val in row] for row in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"{final_results}\".replace(\"'\", \"\"))\n\nsolve()\n```", "id": "3188072"}, {"introduction": "最后，这个练习挑战了“将整个网络重置到初始状态”的标准做法，引导你探索一个更微妙的问题：网络的不同部分是否对重置的敏感度不同？你将通过“部分重置”实验——只重置某些层到其早期状态 $\\theta_k$，而保持其他层处于训练后期状态 $\\theta_T$ ——来探究哪些层是“中奖彩票”存在的关键。这个练习将让你对中奖彩票的性质以及网络各层在学习中的不同角色有更深刻的认识 [@problem_id:3188074]。", "problem": "要求您编写一个完整且可运行的程序，在一个小型全连接前馈网络的背景下，对彩票假设（Lottery Ticket Hypothesis）中的部分回卷（partial rewinding）进行实证测试。该研究必须纯粹以数学和逻辑术语进行构建，并遵循下述定义和程序。目标是确定网络的哪些层需要早期状态初始化来维持中奖彩票（winning tickets）。\n\n需使用的基本和核心定义：\n- 一个带有修正线性单元（ReLU）激活函数的全连接前馈网络是仿射变换和逐元素非线性函数的组合。设输入维度为 $d$，隐藏层宽度为 $h_1$ 和 $h_2$，类别数量为 $C$。参数记为 $\\theta = \\{W_1, b_1, W_2, b_2, W_3, b_3\\}$，其中 $W_1 \\in \\mathbb{R}^{d \\times h_1}$，$b_1 \\in \\mathbb{R}^{h_1}$，$W_2 \\in \\mathbb{R}^{h_1 \\times h_2}$，$b_2 \\in \\mathbb{R}^{h_2}$，$W_3 \\in \\mathbb{R}^{h_2 \\times C}$，$b_3 \\in \\mathbb{R}^{C}$。前向映射为 $f(x;\\theta) = \\mathrm{softmax}(a_3)$，其中\n$$\nz_1 = x W_1 + b_1,\\quad a_1 = \\max(0,z_1),\\quad\nz_2 = a_1 W_2 + b_2,\\quad a_2 = \\max(0,z_2),\\quad\na_3 = a_2 W_3 + b_3.\n$$\n- 训练使用全批量梯度下降（GD），即随机梯度下降（SGD）的一种特例，对平均交叉熵损失进行优化。对于 logits 矩阵 $A_3 \\in \\mathbb{R}^{N \\times C}$（其行向量为 $a_3^{(i)}$）和独热（one-hot）标签 $Y \\in \\mathbb{R}^{N \\times C}$，softmax 概率为 $P_{ij} = \\exp(a_{3,ij} - \\max_k a_{3,ik}) / \\sum_{k=1}^{C} \\exp(a_{3,ik} - \\max_\\ell a_{3,i\\ell})$，损失函数为\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{C} Y_{ij}\\log P_{ij} + \\frac{\\lambda}{2}\\sum_{\\ell=1}^{3}\\left(\\|W_\\ell\\|_F^2\\right).\n$$\n- 梯度通过链式法则计算；对于带有交叉熵的 softmax，其在 logits 处的梯度为 $(P - Y)$ 除以 $N$。\n- 量级剪枝（Magnitude pruning）通过将绝对值最小的固定比例 $s$ 的条目置零，并保留其余部分，从而为所有参数生成一个二元掩码 $M$。掩码后的参数为 $\\theta \\odot M$，其中 $\\odot$ 表示逐元素乘法，且 $M$ 的形状与 $\\theta$ 的张量相同。在使用掩码进行训练时，每次梯度步骤后都会应用投影 $\\theta \\leftarrow \\theta \\odot M$，以保持被剪枝的条目为零。\n- 回卷到步骤 $k$ 是指将选定层的参数设置为它们在训练步骤 $k$ 时的值（记为 $\\theta_k$），而其他层可以保持其在步骤 $T$ 时的值（记为 $\\theta_T$）。彩票假设考虑的是，在早期状态下初始化的掩码网络是否能达到密集网络的性能。\n\n您的程序必须：\n1. 按如下方式构建一个合成的二元分类数据集：从 $[-1,1]^2$ 上独立且均匀地采样 $N_{\\text{train}}$ 和 $N_{\\text{val}}$ 个点。对于一个点 $x = (x_1,x_2)$，如果 $x_1 x_2 > 0$，则分配标签 $y=1$，否则分配标签 $y=0$。使用 $C=2$ 个类别的独热编码。使用固定的随机种子以确保可复现性。\n2. 使用一个如上定义的、带有ReLU非线性和softmax输出的三层全连接网络，其中 $d=2$, $h_1=16$, $h_2=16$, $C=2$。所有权重和偏置的参数均使用均值为 $0$、标准差为 $\\sigma$ 的独立高斯分布条目进行初始化。\n3. 使用恒定学习率 $\\eta$ 和 $\\ell_2$ 正则化系数 $\\lambda$，对密集网络进行 $T$ 次全批量梯度下降步骤的训练。在步骤 $k$ 保存快照 $\\theta_k$，在步骤 $T$ 保存快照 $\\theta_T$。\n4. 从 $\\theta_T$ 创建一个全局量级剪枝掩码 $M$，该掩码精确地将所有参数（包括偏置）中量级最小的一部分（比例为 $s$）的条目置零，并保留剩余条目。\n5. 对于每一指定的层子集 $S \\subseteq \\{1,2,3\\}$，按如下方式形成一个部分回卷的初始化 $\\tilde{\\theta}^{(S)}$：对于层 $\\ell \\in S$，将其参数设置为来自 $\\theta_k$ 的掩码后早期值；对于层 $\\ell \\notin S$，将其参数设置为来自 $\\theta_T$ 的掩码后晚期值。在所有情况下，都应用掩码 $M$ 以生成 $\\tilde{\\theta}^{(S)} \\odot M$。\n6. 从 $\\tilde{\\theta}^{(S)} \\odot M$ 开始，使用相同的 $\\eta$ 和 $\\lambda$ 对掩码网络进行 $R$ 次全批量梯度下降步骤的重新训练，每次更新后投影到掩码上。在验证集上评估平均交叉熵，以获得 $L^{(S)}_{\\text{val}}$。\n7. 将基准掩码中奖彩票验证损失 $L^{\\text{full}}_{\\text{val}}$ 定义为 $S=\\{1,2,3\\}$ 时的 $L^{(S)}_{\\text{val}}$ 值（即所有层完全回卷到 $\\theta_k$）。如果 $L^{(S)}_{\\text{val}} \\le L^{\\text{full}}_{\\text{val}} + \\tau$，则称子集 $S$ 能够维持中奖彩票。\n\n您必须始终使用双精度或单精度实数进行所有计算。不涉及物理单位。不使用角度。\n\n使用以下固定参数：\n- 网络大小: $d=2$, $h_1=16$, $h_2=16$, $C=2$。\n- 数据集大小: $N_{\\text{train}}=512$, $N_{\\text{val}}=512$。\n- 初始化: 标准差为 $\\sigma=0.1$ 的高斯分布。\n- 训练超参数: 学习率 $\\eta=0.1$, 正则化 $\\lambda=10^{-4}$。\n- 训练步骤: 总共 $T=120$, 回卷步骤 $k=10$, 掩码后重新训练步骤 $R=110$。\n- 剪枝比例: $s=0.8$。\n- 随机种子: $r_0=1337$。\n- 维持中奖彩票的容忍度: $\\tau=0.02$。\n\n测试套件（层索引约定：层 $1$ 是 $\\{W_1,b_1\\}$，层 $2$ 是 $\\{W_2,b_2\\}$，层 $3$ 是 $\\{W_3,b_3\\}$）：\n- $S_1 = \\varnothing$ (不回卷；所有层保持在 $\\theta_T$)，\n- $S_2 = \\{1\\}$，\n- $S_3 = \\{2\\}$，\n- $S_4 = \\{3\\}$，\n- $S_5 = \\{1,2\\}$，\n- $S_6 = \\{2,3\\}$，\n- $S_7 = \\{1,3\\}$，\n- $S_8 = \\{1,2,3\\}$ (完全回卷)。\n\n对于每个 $S_i$，计算布尔值 $b_i$：如果 $L^{(S_i)}_{\\text{val}} \\le L^{\\text{full}}_{\\text{val}} + \\tau$，则为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序排列的、用方括号括起来的逗号分隔列表的结果 $[b_1,b_2,b_3,b_4,b_5,b_6,b_7,b_8]$。例如，输出可能看起来像 $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{False},\\mathrm{True}]$。", "solution": "用户提供了一个明确定义的计算问题，旨在研究彩票假设，特别关注部分层回卷对剪枝后神经网络性能的影响。该问题在科学上植根于已有的深度学习文献，在数学上精确，并且在算法上足够详细，足以得到唯一的解决方案。所有参数、定义和程序都是自洽且一致的。因此，该问题被认为是有效的，可以构建一个完整的解决方案。\n\n解决方案将遵循问题陈述中规定的步骤顺序来实现：\n\n1.  **数据集生成**：将创建一个合成的二元分类数据集。我们将从 $[-1, 1]^2$ 上的均匀分布中采样 $N_{\\text{train}}$ 和 $N_{\\text{val}}$ 个点。分类规则是如果 $x_1 x_2 > 0$ 则 $y=1$，否则 $y=0$，这定义了一个非线性可分的问题。标签将进行独热编码。固定的随机种子确保了可复现性。\n\n2.  **网络架构与初始化**：将使用一个三层全连接网络。参数 $\\theta = \\{W_1, b_1, W_2, b_2, W_3, b_3\\}$ 从均值为 $0$、标准差为 $\\sigma$ 的高斯分布中初始化。所有计算将统一使用双精度浮点数（`numpy.float64`）。\n\n3.  **初始密集训练**：网络将使用全批量梯度下降进行 $T=120$ 步的训练。损失函数是带有 $\\ell_2$ 正则化的平均交叉熵。在此阶段，我们将在训练 $k=10$ 步后存储网络参数的快照 $\\theta_k$。$T$ 步后的最终参数记为 $\\theta_T$。\n\n4.  **量级剪枝**：基于最终参数 $\\theta_T$ 创建一个全局量级剪枝掩码 $M$。我们将计算所有层中所有参数（权重和偏置）的量级，确定与这些量级的第 $s \\times 100$ 百分位数相对应的阈值，并创建一个二元掩码 $M$，该掩码保留量级大于或等于此阈值的参数。此过程确保了量级最小的一部分（比例为 $s$）的参数被精确剪枝（设置为零）。\n\n5.  **部分回卷与重新训练**：对于每个指定的层子集 $S_i$，将为重新训练构建一组新的初始参数。\n    *   首先，通过从早期快照 $\\theta_k$ 中获取 $S_i$ 中各层的参数，并从晚期训练状态 $\\theta_T$ 中获取不在 $S_i$ 中的各层的参数，来形成一个中间参数集 $\\tilde{\\theta}^{(S_i)}$。\n    *   然后，重新训练的实际起始状态由逐元素乘积 $\\tilde{\\theta}^{(S_i)} \\odot M$ 给出。\n    *   从这个状态开始，网络将重新训练 $R=110$ 步。每次梯度更新后，通过再次应用掩码将参数投影回由掩码 $M$ 定义的子网络上：$\\theta \\leftarrow \\theta \\odot M$。\n\n6.  **评估**：重新训练后，将为每个实验 $S_i$ 计算最终的验证损失 $L^{(S_i)}_{\\text{val}}$。基准损失 $L^{\\text{full}}_{\\text{val}}$ 定义为在完全回卷实验（即 $S = \\{1, 2, 3\\}$）中获得的损失。如果一个层子集 $S_i$ 的最终验证损失不比基准损失大超过一个容忍度 $\\tau$，即 $L^{(S_i)}_{\\text{val}} \\le L^{\\text{full}}_{\\text{val}} + \\tau$，则认为该子集“维持了中奖彩票”。\n\n最终输出将是一个布尔值列表，指示每个层子集 $S_i$ 是否根据此标准维持了中奖彩票。实现将封装在单个 Python 脚本中，并遵循指定的环境和库。\n\n数学运算，包括前向传播、稳定的 softmax、交叉熵损失以及用于梯度计算的反向传播，将使用 `numpy` 来实现。\n\n-   **前向传播**：$f(x;\\theta) = \\mathrm{softmax}(a_3)$，其中 $z_1 = x W_1 + b_1$，$a_1 = \\mathrm{ReLU}(z_1)$，$z_2 = a_1 W_2 + b_2$，$a_2 = \\mathrm{ReLU}(z_2)$，以及 $a_3 = a_2 W_3 + b_3$。\n-   **损失函数**：$\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i,j} Y_{ij}\\log P_{ij} + \\frac{\\lambda}{2}\\sum_{\\ell=1}^{3}\\|W_\\ell\\|_F^2$。\n-   **损失对 Logits 的梯度**：$\\nabla_{a_3}\\mathcal{L} = (P - Y) / N$。\n-   **参数更新**：$\\theta_{t+1} = \\theta_t - \\eta\\nabla\\mathcal{L}(\\theta_t)$。\n-   **掩码更新**：$\\theta_{t+1} = (\\theta_t - \\eta\\nabla\\mathcal{L}(\\theta_t)) \\odot M$。\n\n这个严谨、循序渐进的过程将产生所需的关于部分回卷策略的实证比较结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically tests partial rewinding in the context of the Lottery Ticket Hypothesis.\n    \"\"\"\n    #\n    # ------------------ 0. PROBLEM PARAMETERS AND CONSTANTS ------------------\n    #\n    DTYPE = np.float64\n    # Network architecture\n    d, h1, h2, C = 2, 16, 16, 2\n    # Dataset sizes\n    N_train, N_val = 512, 512\n    # Initialization\n    sigma = 0.1\n    # Training hyperparameters\n    eta = 0.1\n    lambda_ = 1e-4\n    # Training steps\n    T = 120\n    k = 10\n    R = 110\n    # Pruning and evaluation\n    s = 0.8\n    tau = 0.02\n    # Reproducibility\n    r_0 = 1337\n\n    #\n    # ---------------------- 1. HELPER FUNCTIONS ----------------------\n    #\n    def generate_data(n_samples, rng):\n        X = rng.uniform(-1, 1, size=(n_samples, d)).astype(DTYPE)\n        # y=1 if x1*x2 > 0 (quadrants I and III), y=0 otherwise\n        y = (X[:, 0] * X[:, 1] > 0).astype(int)\n        # One-hot encode labels\n        Y = np.zeros((n_samples, C), dtype=DTYPE)\n        Y[np.arange(n_samples), y] = 1\n        return X, Y\n\n    def initialize_params(rng):\n        params = {\n            'W1': rng.normal(0, sigma, (d, h1)).astype(DTYPE),\n            'b1': rng.normal(0, sigma, (h1,)).astype(DTYPE),\n            'W2': rng.normal(0, sigma, (h1, h2)).astype(DTYPE),\n            'b2': rng.normal(0, sigma, (h2,)).astype(DTYPE),\n            'W3': rng.normal(0, sigma, (h2, C)).astype(DTYPE),\n            'b3': rng.normal(0, sigma, (C,)).astype(DTYPE),\n        }\n        return params\n\n    def forward_pass(X, params):\n        z1 = X @ params['W1'] + params['b1']\n        a1 = np.maximum(0, z1)\n        z2 = a1 @ params['W2'] + params['b2']\n        a2 = np.maximum(0, z2)\n        logits = a2 @ params['W3'] + params['b3']\n        activations = {'a1': a1, 'a2': a2}\n        return activations, logits\n\n    def compute_loss(Y, logits, params, lambda_val):\n        N = Y.shape[0]\n        # Numerically stable softmax and cross-entropy loss\n        max_logits = np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(logits - max_logits)\n        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        true_class_probs = probs[np.arange(N), Y.argmax(axis=1)]\n        # Add a small epsilon to prevent log(0)\n        data_loss = np.mean(-np.log(true_class_probs + 1e-12))\n        \n        reg_loss = (np.sum(params['W1']**2) + np.sum(params['W2']**2) + np.sum(params['W3']**2))\n        total_loss = data_loss + (lambda_val / 2) * reg_loss\n        return total_loss\n\n    def compute_gradients(X, Y, activations, logits, params, lambda_val):\n        N = Y.shape[0]\n        # Numerically stable softmax probability calculation\n        max_logits = np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(logits - max_logits)\n        P = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        \n        # Gradient of loss with respect to pre-softmax logits\n        d_a3 = (P - Y) / N\n        \n        a2, a1 = activations['a2'], activations['a1']\n        \n        # Layer 3 gradients\n        d_W3 = a2.T @ d_a3 + lambda_val * params['W3']\n        d_b3 = d_a3.sum(axis=0)\n        \n        # Layer 2 gradients\n        d_a2 = d_a3 @ params['W3'].T\n        d_z2 = d_a2 * (a2 > 0)\n        d_W2 = a1.T @ d_z2 + lambda_val * params['W2']\n        d_b2 = d_z2.sum(axis=0)\n        \n        # Layer 1 gradients\n        d_a1 = d_z2 @ params['W2'].T\n        d_z1 = d_a1 * (a1 > 0)\n        d_W1 = X.T @ d_z1 + lambda_val * params['W1']\n        d_b1 = d_z1.sum(axis=0)\n\n        grads = {'W1': d_W1, 'b1': d_b1, 'W2': d_W2, 'b2': d_b2, 'W3': d_W3, 'b3': d_b3}\n        return grads\n\n    #\n    # ---------------------- 2. MAIN EXPERIMENT LOGIC ----------------------\n    #\n    # Setup\n    rng = np.random.default_rng(r_0)\n    X_train, Y_train = generate_data(N_train, rng)\n    X_val, Y_val = generate_data(N_val, rng)\n\n    # Initial Dense Training\n    params = initialize_params(rng)\n    params_k = None\n    for i in range(T):\n        if i == k:\n            params_k = {key: val.copy() for key, val in params.items()}\n        \n        activations, logits = forward_pass(X_train, params)\n        grads = compute_gradients(X_train, Y_train, activations, logits, params, lambda_)\n\n        for p_key in params:\n            params[p_key] -= eta * grads[p_key]\n    params_T = params\n\n    # Create Pruning Mask\n    param_info = []\n    param_keys_ordered = ['W1', 'b1', 'W2', 'b2', 'W3', 'b3']\n    for key in param_keys_ordered:\n        p = params_T[key]\n        param_info.append({'key': key, 'shape': p.shape, 'size': p.size})\n\n    flat_params = np.concatenate([params_T[info['key']].flatten() for info in param_info])\n    num_to_prune = int(s * flat_params.size)\n    prune_indices = np.argsort(np.abs(flat_params))[:num_to_prune]\n\n    flat_mask = np.ones(flat_params.size, dtype=DTYPE)\n    flat_mask[prune_indices] = 0\n    \n    masks = {}\n    current_pos = 0\n    for info in param_info:\n        masks[info['key']] = flat_mask[current_pos : current_pos + info['size']].reshape(info['shape'])\n        current_pos += info['size']\n\n    # Rewinding and Retraining Experiments\n    test_cases = [(), (1,), (2,), (3,), (1, 2), (2, 3), (1, 3), (1, 2, 3)]\n    layer_keys_map = {1: ['W1', 'b1'], 2: ['W2', 'b2'], 3: ['W3', 'b3']}\n    final_losses = []\n\n    for S in test_cases:\n        # Form the partially rewound initialization `tilde_theta`\n        tilde_theta = {}\n        for layer_idx in range(1, 4):\n            source_params = params_k if layer_idx in S else params_T\n            for p_key in layer_keys_map[layer_idx]:\n                tilde_theta[p_key] = source_params[p_key].copy()\n        \n        # Retraining starts from the masked rewound state\n        current_params = {key: tilde_theta[key] * masks[key] for key in tilde_theta}\n        \n        for _ in range(R):\n            activations, logits = forward_pass(X_train, current_params)\n            grads = compute_gradients(X_train, Y_train, activations, logits, current_params, lambda_)\n            \n            # Gradient update step\n            for p_key in current_params:\n                current_params[p_key] -= eta * grads[p_key]\n            \n            # Projection onto the mask\n            for p_key in current_params:\n                current_params[p_key] *= masks[p_key]\n        \n        # Evaluate final validation loss\n        _, val_logits = forward_pass(X_val, current_params)\n        val_loss = compute_loss(Y_val, val_logits, current_params, lambda_)\n        final_losses.append(val_loss)\n\n    # Final Analysis\n    baseline_loss = final_losses[-1]  # Loss for S = {1, 2, 3}\n    results = [loss <= baseline_loss + tau for loss in final_losses]\n    \n    # Python's bool string representation is \"True\", \"False\" (capitalized).\n    # The problem asks for this format, so a simple string conversion is fine.\n    print(f\"[{','.join(map(str, results))}]\".replace(\"'\", \"\"))\n\nsolve()\n```", "id": "3188074"}]}