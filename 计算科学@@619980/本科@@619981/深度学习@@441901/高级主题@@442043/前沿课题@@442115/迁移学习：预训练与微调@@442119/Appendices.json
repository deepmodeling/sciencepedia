{"hands_on_practices": [{"introduction": "微调的核心思想是：我们希望模型适应新任务，但又不想让它完全忘记在预训练中学到的宝贵特征。本练习探讨 L2-SP 正则化，这是一种从数学上形式化这种权衡的关键技术[@problem_id:3195259]。通过为线性模型推导闭式解，你将深刻理解正则化如何将微调后的参数“拉向”其预训练的初始点，从而有效防止灾难性遗忘。", "problem": "考虑迁移学习，其中模型首先在源数据集上训练以获得预训练参数 $\\theta_{0} \\in \\mathbb{R}^{d}$，然后使用起始点（SP）正则化（也称为L2起始点（L2-SP）正则化）在目标数据集上进行微调。设目标数据集由一个设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和一个目标响应向量 $y \\in \\mathbb{R}^{n}$ 给出。考虑一个线性模型 $f(x) = x^{\\top}\\theta$，并将在目标数据上的均方误差（MSE）经验风险定义为\n$$\n\\mathcal{L}_{\\text{target}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}.\n$$\n使用L2-SP正则化进行微调的目标函数为\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\mathcal{L}_{\\text{target}}(\\theta) + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2},\n$$\n其中 $\\lambda > 0$ 是正则化强度。从经验风险最小化的基本原理和凸二次函数的性质出发，解释为什么L2-SP项会使微调后的解偏向预训练参数 $\\theta_{0}$，并推导在线性模型下 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 的闭式最小化解。假设 $X$ 和 $\\lambda$ 的取值使得最小化解是唯一的。你的最终答案必须是 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 的最小化解 $\\theta^{\\star}$ 的单一闭式解析表达式。不需要进行数值近似。", "solution": "该问题要求两件事：首先，从概念上解释L2起始点（L2-SP）正则化如何使微调参数偏向预训练参数；其次，在线性回归情境下推导微调参数的闭式解。\n\n首先，我们来解决概念性解释。使用L2-SP正则化进行微调的目标函数由下式给出：\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\mathcal{L}_{\\text{target}}(\\theta) + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}\n$$\n该目标函数由两项组成。第一项 $\\mathcal{L}_{\\text{target}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}$ 是目标数据集上的经验风险，具体来说是均方误差（MSE）。该项衡量了带有参数 $\\theta$ 的模型拟合目标数据的好坏程度。单独最小化这一项会驱使参数趋向于能最好地解释目标数据的解，而完全不考虑预训练参数 $\\theta_{0}$。\n\n第二项 $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$ 是L2-SP正则化惩罚项。该项度量了当前参数 $\\theta$ 和预训练参数 $\\theta_{0}$ 之间的欧几里得距离的平方，并由正则化超参数 $\\lambda > 0$ 进行缩放。当 $\\theta = \\theta_{0}$ 时，该惩罚项最小化。随着 $\\theta$ 远离 $\\theta_{0}$，它会呈二次方增长。\n\n最小化总目标函数 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 的过程涉及在这两个相互竞争的目标之间进行权衡，这种权衡由正则化强度 $\\lambda$ 控制。\n-   为了最小化 $\\mathcal{L}_{\\text{target}}(\\theta)$，优化过程必须调整 $\\theta$ 以减少在目标数据集上的预测误差。\n-   为了最小化 $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$，优化过程必须使 $\\theta$ 尽可能地接近起始点 $\\theta_{0}$。\n\n最终解 $\\theta^{\\star}$ 将是平衡这两种压力的一个点。如果解 $\\theta$ 为了在目标数据上获得更好的拟合而偏离 $\\theta_{0}$ 太远，惩罚项就会增大，从而增加总损失。因此，优化过程不仅受到目标数据误差曲面的引导，还受到一股将解拉向 $\\theta_{0}$ 的“引力”的影响。这就是为什么L2-SP项被认为会使微调解偏向预训练参数。这种偏向的大小由 $\\lambda$ 决定。当 $\\lambda \\to \\infty$ 时，惩罚项占主导地位，解 $\\theta^{\\star}$ 将被迫非常接近 $\\theta_{0}$，即 $\\theta^{\\star} \\to \\theta_{0}$。相反，当 $\\lambda \\to 0$ 时，正则化效应消失，解会收敛到目标数据的标准经验风险最小化解。\n\n接下来，我们推导 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 的闭式最小化解。目标函数是：\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2} + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}\n$$\n函数 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 是两个凸函数的和。项 $\\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}$ 是凸的，并且由于 $\\lambda > 0$，项 $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$ 是严格凸的。因此，它们的和是严格凸的，这保证了存在唯一的最小化解。为了找到这个最小化解，我们计算 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 关于 $\\theta$ 的梯度，并将其设为零向量。\n\n首先，我们使用内积的定义 $\\|v\\|_{2}^{2} = v^{\\top}v$ 展开平方范数项：\n$$\n\\|X\\theta - y\\|_{2}^{2} = (X\\theta - y)^{\\top}(X\\theta - y) = (\\theta^{\\top}X^{\\top} - y^{\\top})(X\\theta - y) = \\theta^{\\top}X^{\\top}X\\theta - 2y^{\\top}X\\theta + y^{\\top}y\n$$\n$$\n\\|\\theta - \\theta_{0}\\|_{2}^{2} = (\\theta - \\theta_{0})^{\\top}(\\theta - \\theta_{0}) = \\theta^{\\top}\\theta - 2\\theta_{0}^{\\top}\\theta + \\theta_{0}^{\\top}\\theta_{0}\n$$\n将这些代回目标函数：\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}(\\theta^{\\top}X^{\\top}X\\theta - 2y^{\\top}X\\theta + y^{\\top}y) + \\lambda(\\theta^{\\top}\\theta - 2\\theta_{0}^{\\top}\\theta + \\theta_{0}^{\\top}\\theta_{0})\n$$\n现在，我们计算梯度 $\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta)$。我们使用以下标准的矩阵微积分恒等式：$\\nabla_{z}(z^{\\top}Az) = (A+A^{\\top})z$ 和 $\\nabla_{z}(b^{\\top}z) = b$。由于 $X^{\\top}X$ 和单位矩阵 $I$ 是对称的，我们有 $\\nabla_{\\theta}(\\theta^{\\top}X^{\\top}X\\theta) = 2X^{\\top}X\\theta$ 和 $\\nabla_{\\theta}(\\theta^{\\top}\\theta) = 2\\theta$。\n\n对 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 逐项求导：\n$$\n\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}(2X^{\\top}X\\theta - 2X^{\\top}y) + \\lambda(2\\theta - 2\\theta_{0})\n$$\n$$\n\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{n}(X^{\\top}X\\theta - X^{\\top}y) + 2\\lambda(\\theta - \\theta_{0})\n$$\n为了找到最小化解 $\\theta^{\\star}$，我们将梯度设为零向量：\n$$\n\\frac{1}{n}(X^{\\top}X\\theta^{\\star} - X^{\\top}y) + 2\\lambda(\\theta^{\\star} - \\theta_{0}) = 0\n$$\n现在，我们求解 $\\theta^{\\star}$。首先，两边乘以 $n$ 以消去分数：\n$$\nX^{\\top}X\\theta^{\\star} - X^{\\top}y + 2n\\lambda(\\theta^{\\star} - \\theta_{0}) = 0\n$$\n$$\nX^{\\top}X\\theta^{\\star} - X^{\\top}y + 2n\\lambda\\theta^{\\star} - 2n\\lambda\\theta_{0} = 0\n$$\n合并含有 $\\theta^{\\star}$ 的项：\n$$\n(X^{\\top}X + 2n\\lambda I)\\theta^{\\star} = X^{\\top}y + 2n\\lambda\\theta_{0}\n$$\n其中 $I$ 是 $d \\times d$ 的单位矩阵。问题假设最小化解是唯一的，这意味着矩阵 $(X^{\\top}X + 2n\\lambda I)$ 是可逆的。这一点可以得到保证，因为 $X^{\\top}X$ 是半正定的，并且由于 $n>0$ 和 $\\lambda>0$，$2n\\lambda I$ 是正定的。一个半正定矩阵和一个正定矩阵的和是正定的，因此是可逆的。\n\n最后，我们通过左乘 $(X^{\\top}X + 2n\\lambda I)$ 的逆矩阵来找到 $\\theta^{\\star}$ 的闭式表达式：\n$$\n\\theta^{\\star} = (X^{\\top}X + 2n\\lambda I)^{-1}(X^{\\top}y + 2n\\lambda\\theta_{0})\n$$\n该表达式是线性模型下L2-SP正则化微调目标的闭式最小化解。", "answer": "$$\n\\boxed{(X^{\\top}X + 2n\\lambda I)^{-1}(X^{\\top}y + 2n\\lambda\\theta_{0})}\n$$", "id": "3195259"}, {"introduction": "从理论转向一个常见的实际挑战，本练习关注训练不稳定的问题。当我们解冻预训练层时，一个草率的学习率选择可能导致“梯度爆炸”，并摧毁精心学习到的特征[@problem_id:3185080]。本练习模拟了一个经典的微调失败场景，要求你诊断根本原因并选择最有效的补救措施，从而学习到诸如分层解冻和判别性学习率等关键技巧。", "problem": "考虑一个在源数据集上预训练过的具有 $L$ 层的深度前馈网络。在迁移到目标数据集时，采用了层冻结的常规做法：在 $E$ 个周期（epoch）内，仅对最终的分类头进行微调，而较低层保持固定。在 $E$ 个周期后，所有层被一次性解冻并进行联合训练。假设观察到以下现象：\n\n- 在仅训练头的阶段，学习率（LR）为 $\\alpha_{\\mathrm{head}} = 5 \\times 10^{-4}$ 时，损失平稳下降。\n- 解冻时，对所有层应用统一的学习率 $\\alpha = 10^{-2}$，训练损失出现尖峰；在早期层测得的梯度表现出大范数和快速振荡。\n- 该网络有 $L = 12$ 层；中间雅可比矩阵（将激活值从一层映射到下一层）的经验算子范数在几个连续层上满足 $\\lVert J_k \\rVert \\approx 1.3$ 至 $1.6$。\n\n使用梯度下降更新的定义，即第 $l$ 层的参数更新幅度与 $\\lVert \\Delta \\theta_l \\rVert \\propto \\alpha_l \\lVert g_l \\rVert$ 成正比，以及反向传播的链式法则，该法则意味着上游梯度的范数会随着跨层雅可比范数的乘积而增长。假设目标数据集与源数据集存在中度偏移，导致先前冻结的层最初产生的激活值与新的头尚未良好对齐。\n\n哪一项干预措施最能直接缓解解冻时观察到的梯度爆炸的根本原因，同时又能保留迁移学习的好处？\n\nA. 一次性解冻所有层，并保持一个较高的统一学习率 $\\alpha = 10^{-2}$，以加速整个网络的适应过程。\n\nB. 逐步解冻层（从顶层到底层），并应用随深度衰减的逐层学习率缩放 $\\alpha_l$（例如，对较低层使用几何衰减），并配合一个简短的学习率预热（warmup），使 $\\alpha_l$ 从一个较小的值开始缓慢增加。\n\nC. 将优化器更换为不带动量的随机梯度下降（SGD），但在解冻时对所有层保持统一的学习率 $\\alpha = 10^{-2}$。\n\nD. 增大学习批次（batch size）以减少梯度方差，并将学习率加倍至 $\\alpha = 2 \\times 10^{-2}$ 以维持吞吐量。\n\nE. 在保持 $\\alpha = 10^{-2}$ 并一次性解冻所有层的同时，应用一个非常高的阈值进行梯度裁剪。", "solution": "首先将验证问题陈述的科学性和逻辑合理性。\n\n### 步骤 1：提取已知条件\n- 一个具有 $L=12$ 层的深度前馈网络在源数据集上进行了预训练。\n- 该网络使用层冻结的方式迁移到目标数据集。\n- 在 $E$ 个周期内，仅对最终的分类头进行微调（仅训练头的阶段）。\n- 在 $E$ 个周期后，所有层被解冻并进行联合训练。\n- 在仅训练头的阶段，学习率（LR）为 $\\alpha_{\\mathrm{head}} = 5 \\times 10^{-4}$ 时，损失平稳下降。\n- 解冻所有层后，应用统一的学习率 $\\alpha = 10^{-2}$。\n- 观察：训练损失出现尖峰，早期层的梯度显示出大范数和快速振荡。\n- 观察：几个连续层的雅可比矩阵的经验算子范数满足 $\\lVert J_k \\rVert \\approx 1.3$ 至 $1.6$。\n- 定义：第 $l$ 层的参数更新幅度与 $\\lVert \\Delta \\theta_l \\rVert \\propto \\alpha_l \\lVert g_l \\rVert$ 成正比，其中 $g_l$ 是关于参数 $\\theta_l$ 的梯度。\n- 反向传播的链式法则意味着上游梯度的范数可以作为雅可比范数的乘积而增长。\n- 假设：目标数据集与源数据集存在中度偏移，导致冻结的主干网络产生的激活值与新训练的头之间存在初始的未对齐。\n- 问题：找出在解冻瞬间最能直接缓解所观察到的梯度爆炸根本原因，同时保留迁移学习益处的单一干预措施。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述描述了深度神经网络微调中的一个常见且现实的场景。\n- **科学依据：** 所提出的概念——迁移学习、层冻结、微调、学习率、梯度爆炸、反向传播和雅可比范数——都是深度学习领域基本且公认的原则。大于 $1$ 的雅可比范数与梯度爆炸可能性之间的关系是链式法则的直接结果。所提供的数值是合理的。\n- **问题定义明确：** 该问题提供了一个清晰的因果场景，并要求从一组选项中选择最有效的干预措施。其结构旨在根据既定的优化和迁移学习理论得出一个唯一的、先验的最佳答案。\n- **客观性：** 语言技术性强、精确，没有主观或含糊的术语。\n\n问题内部是一致的。用低学习率（$\\alpha_{\\mathrm{head}} = 5 \\times 10^{-4}$）成功训练头部的初始阶段，与对整个网络使用高得多的统一学习率（$\\alpha = 10^{-2}$）时观察到的不稳定性形成对比。这种对比是核心的诊断线索，而非矛盾。问题陈述是有效的。\n\n### 步骤 3：推导与选项分析\n\n该问题描述了在微调预训练网络期间出现不稳定的经典案例。让我们根据所提供的信息分析其根本原因。\n\n损失 $\\mathcal{L}$ 关于第 $l-1$ 层激活值 $a_{l-1}$ 的梯度，是通过从第 $l$ 层激活值 $a_l$ 的梯度反向传播计算得出的：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a_{l-1}} = \\frac{\\partial \\mathcal{L}}{\\partial a_l} \\frac{\\partial a_l}{\\partial a_{l-1}} = \\frac{\\partial \\mathcal{L}}{\\partial a_l} J_l\n$$\n其中 $J_l$ 是第 $l$ 层变换的雅可比矩阵。因此，梯度的范数有如下上界：\n$$\n\\left\\lVert \\frac{\\partial \\mathcal{L}}{\\partial a_{l-1}} \\right\\rVert \\le \\left\\lVert \\frac{\\partial \\mathcal{L}}{\\partial a_l} \\right\\rVert \\left\\lVert J_l \\right\\rVert\n$$\n将此从顶层（$L-1$）向后传播到底层 $l$，梯度范数可能会被放大。经过 $m$ 层，放大因子可能大到雅可比范数的乘积：\n$$\n\\left\\lVert \\frac{\\partial \\mathcal{L}}{\\partial a_{l-1}} \\right\\rVert \\le \\left\\lVert \\frac{\\partial \\mathcal{L}}{\\partial a_{L-1}} \\right\\rVert \\prod_{k=l}^{L-1} \\left\\lVert J_k \\right\\rVert\n$$\n问题陈述中提到 $\\lVert J_k \\rVert \\approx 1.3$ 至 $1.6$。在多个层上，这些大于 $1$ 的数字的乘积将导致梯度范数的指数级增长。例如，在 $10$ 个层上，如果平均雅可比范数为 $1.4$，梯度大小可能被放大 $1.4^{10} \\approx 28.9$ 倍。这种现象被称为**梯度爆炸问题**。\n\n关于第 $l$ 层参数 $\\theta_l$ 的梯度，记为 $g_l$，是从 $\\frac{\\partial \\mathcal{L}}{\\partial a_l}$ 导出的。因此，参数梯度范数 $\\lVert g_l \\rVert$ 在较低层中也会很大。参数更新为 $\\Delta \\theta_l = -\\alpha_l g_l$，其幅度为 $\\lVert \\Delta \\theta_l \\rVert = \\alpha_l \\lVert g_l \\rVert$。\n\n观察到的不稳定性的**根本原因**是两个因素的结合：\n1.  **梯度爆炸**：预训练网络的架构和权重导致雅可比范数 $>1$，使得梯度信号在向较低层传播时被放大。\n2.  **不合适的学习率**：对所有层应用了一个较大的统一学习率 $\\alpha = 10^{-2}$。当这个大的 $\\alpha$ 与较低层中已经很大的梯度范数 $\\lVert g_l \\rVert$ 相乘时，得到的参数更新 $\\Delta \\theta_l$ 是巨大的。这个大的更新步长会破坏在预训练期间学到的、经过精细调整的通用特征，导致损失出现尖峰和“灾难性遗忘”。事实上，头部可以用小得多的 $\\alpha_{\\mathrm{head}} = 5 \\times 10^{-4}$（小了 $20$ 倍）进行训练，这强烈表明 $\\alpha=10^{-2}$ 过于激进，特别是对于敏感的较低层。\n\n理想的干预措施必须解决更新步长与损失函数局部曲率之间的这种根本性不匹配问题，特别是对于较低层。\n\n**逐项选项分析：**\n\n**A. 一次性解冻所有层，并保持一个较高的统一学习率 $\\alpha = 10^{-2}$，以加速整个网络的适应过程。**\n这正是导致训练不稳定的操作。它直接引发问题，而不是缓解问题。对一个已经良好运作的特征提取器的参数应用大的学习步长，而这些参数正在接收一个被极度放大的误差信号，这是导致发散的根源。\n**结论：错误。**\n\n**B. 逐步解冻层（从顶层到底层），并应用随深度衰减的逐层学习率缩放 $\\alpha_l$（例如，对较低层使用几何衰减），并配合一个简短的学习率预热（warmup），使 $\\alpha_l$ 从一个较小的值开始缓慢增加。**\n该选项提出了一种直接针对根本原因的多方面策略。\n- **逐层学习率缩放（判别性学习率）：** 对较低层应用较小的学习率 $\\alpha_l$（例如，$\\alpha_{l}  \\alpha_{l+1}$）可以直接抵消大的梯度范数 $\\lVert g_l \\rVert$。更新幅度 $\\lVert \\Delta \\theta_l \\rVert = \\alpha_l \\lVert g_l \\rVert$ 可以保持稳定且适当小，从而保留早期层中有价值的通用特征。这是修复不当更新步长的最直接方法。\n- **逐步解冻：** 这使得网络能够更温和地适应。更具任务特异性的上层首先进行调整。这在高度敏感的较低层变得可训练之前，减小了向后传播的误差信号的幅度。\n- **学习率预热：** 将一个已经很小的 $\\alpha_l$ 从一个更小的值开始，然后逐渐增加，可以防止解冻时的初始冲击，让优化器在采取较大步长之前找到一个稳定的下降方向。\n这种组合是现代迁移学习技术的一个主要方法（例如，由 ULMFiT 推广），并且是专门为解决所述的不稳定性问题而设计的。它直接解决了根本原因：较低层中参数更新的破坏性幅度。\n**结论：正确。**\n\n**C. 将优化器更换为不带动量的随机梯度下降（SGD），但在解冻时对所有层保持统一的学习率 $\\alpha = 10^{-2}$。**\n虽然像 Adam 或带动量的 SGD 这样的自适应优化器有时可能更具侵略性并导致过冲，但这里的主要问题是更新的幅度，它是学习率和梯度范数的乘积。仅仅切换到普通的 SGD 并不能改变 $\\alpha = 10^{-2}$ 这个学习率对于梯度正在爆炸的较低层来说太大了这一事实。$\\lVert \\Delta \\theta_l \\rVert$ 过大的根本问题仍未得到解决。\n**结论：错误。**\n\n**D. 增大学习批次（batch size）以减少梯度方差，并将学习率加倍至 $\\alpha = 2 \\times 10^{-2}$ 以维持吞吐量。**\n增大批次大小会减少梯度估计的方差（噪声），这可能具有稳定作用。然而，所描述的问题是梯度*幅度*（范数）的问题，而不是方差问题。梯度是持续性地大，而不仅仅是噪声大。此外，建议将学习率*加倍*至 $\\alpha = 2 \\times 10^{-2}$ 将会灾难性地恶化情况，因为本已过大的更新步长会变得更大。\n**结论：错误。**\n\n**E. 在保持 $\\alpha = 10^{-2}$ 并一次性解冻所有层的同时，应用一个非常高的阈值进行梯度裁剪。**\n梯度裁剪是一种当梯度范数超过指定阈值时对其进行重新缩放的机制。这是一种反应性措施，它处理的是*症状*（大梯度范数）而不是*原因*。产生大梯度的潜在动态（大雅可比矩阵和特征不匹配）并未改变。优化器仍然会尝试采取大的步长（由于高学习率 $\\alpha = 10^{-2}$），但步长被人为地限制了。虽然这可以防止损失发散到无穷大，但它不是一种最优的训练方式。较低层的梯度很可能在每一步都被裁剪，这意味着它们的更新方向得以保留，但所有的幅度信息都丢失了。这导致了低效和次优的学习。选项 B 更优，因为它从一开始就通过使用适当的学习率来解决根本原因。\n**结论：错误。**", "answer": "$$\\boxed{B}$$", "id": "3185080"}, {"introduction": "除了简单地正则化或稳定微调过程，我们有时还需要对模型行为施加严格的规则，例如出于安全性或公平性的考虑。本练习介绍了投影梯度下降法，这是一种在模型参数上强制施加线性约束的先进优化技术[@problem_id:3195172]。通过实现此方法，你将学习如何确保微调后的模型遵守特定要求，并探索此类约束如何帮助减轻负迁移。", "problem": "考虑一个带有参数向量 $\\theta \\in \\mathbb{R}^d$ 的线性模型，该模型用于在目标数据集 $(X, y)$ 上进行微调，其中 $X \\in \\mathbb{R}^{n \\times d}$ 且 $y \\in \\mathbb{R}^{n}$。目标函数为均方误差 (MSE)，定义为\n$$\nL(\\theta) = \\frac{1}{2n} \\left\\|X\\theta - y\\right\\|_2^2,\n$$\n其中 $\\left\\|\\cdot\\right\\|_2$ 表示欧几里得范数。从一个预训练的源参数 $\\theta_{\\text{src}}$ 开始的微调是使用梯度下降法进行的。为了在微调期间施加安全性或单调性约束，引入线性等式约束 $C\\theta = d$，其中 $C \\in \\mathbb{R}^{m \\times d}$ 且 $d \\in \\mathbb{R}^m$。使用投影梯度更新到仿射约束集上。\n\n从以下基本原理开始：\n- 梯度下降更新定义为 $\\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)$，其中 $\\alpha  0$ 是步长，$\\nabla L(\\theta)$ 是损失函数关于 $\\theta$ 的梯度。\n- 任意 $u \\in \\mathbb{R}^d$ 到仿射集 $\\{\\theta: C\\theta = d\\}$ 上的欧几里得投影被定义为在约束 $Cz = d$ 下的 $\\left\\|z - u\\right\\|_2^2$ 的最小化子。\n\n实现两种微调程序：\n- 从 $\\theta_{\\text{src}}$ 开始的无约束梯度下降。\n- 从 $\\theta_{\\text{src}}$ 开始的投影梯度下降，其中每个梯度步之后都进行到 $\\{\\theta \\in \\mathbb{R}^d : C\\theta = d\\}$ 上的欧几里得投影。\n同时，通过从 $\\theta_{\\text{scratch}} = 0$ 开始的无约束梯度下降计算一个从零开始的基线。\n\n将使用初始化 $\\theta_{\\text{init}}$ 的方法的负迁移量定义为\n$$\n\\operatorname{NT}(\\theta_{\\text{init}}) = L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{init}})\\bigr) - L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{scratch}})\\bigr),\n$$\n其中 $\\theta_{\\text{final}}(\\cdot)$ 表示经过固定数量的梯度步后的参数。通过检查以下不等式来测试约束是否减少了负迁移\n$$\n\\operatorname{NT}_{\\text{constrained}}  \\operatorname{NT}_{\\text{unconstrained}}.\n$$\n\n使用以下固定的目标数据集和超参数：\n- 维度 $d = 3$。\n- 样本数 $n = 4$。\n- 设计矩阵\n$$\nX = \\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  1 \\\\\n2  0  1\n\\end{bmatrix},\n$$\n和目标标签\n$$\ny = \\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 2\n\\end{bmatrix}.\n$$\n- 梯度下降步长 $\\alpha = 0.1$ 和总步数 $T = 5$。\n\n测试套件 (每个测试用例指定 $(\\theta_{\\text{src}}, C, d)$)：\n1. 正常路径 (对虚假的源特征施加安全约束)：\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$，\n   - $C = \\begin{bmatrix} 0  0  1 \\end{bmatrix}$，\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$。\n2. 边界情况 (无约束)：\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$，\n   - $C \\in \\mathbb{R}^{0 \\times 3}$ (即 $0 \\times 3$ 的空矩阵)，\n   - $d \\in \\mathbb{R}^{0}$ (即空向量)。\n3. 边缘情况 (不相关或有害的校准约束)：\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$，\n   - $C = \\begin{bmatrix} 1  1  0 \\end{bmatrix}$，\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$。\n4. 正迁移场景 (源已与目标对齐)：\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 0.0 \\end{bmatrix}$，\n   - $C = \\begin{bmatrix} 0  0  1 \\end{bmatrix}$，\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$。\n\n您的程序必须：\n- 为 MSE 目标实现梯度下降。\n- 使用每次梯度步后的欧几里得投影，实现到 $C\\theta = d$ 上的投影梯度下降。\n- 对每个测试用例，计算布尔值\n$$\nb = \\left( \\operatorname{NT}_{\\text{constrained}}  \\operatorname{NT}_{\\text{unconstrained}} \\right).\n$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个条目必须是按上述顺序排列的一个测试用例的布尔值，例如 $\\left[\\text{True},\\text{False},\\text{True},\\text{False}\\right]$。此问题不涉及物理单位或角度单位。将所有布尔结果明确表示为 $\\text{True}$ 或 $\\text{False}$。", "solution": "该问题要求比较线性模型的无约束和有约束微调，两者均使用基于梯度的方法。目标是确定施加线性等式约束是否可以减轻负迁移。这通过比较有约束微调模型的最终损失与无约束微调模型的最终损失，并相对于一个从零开始训练的基线模型来进行评估。\n\n首先，我们定义问题的数学组成部分。模型是其参数 $\\theta \\in \\mathbb{R}^d$ 的线性函数，其在具有 $n$ 个样本的目标数据集 $(X, y)$ 上的性能（其中 $X \\in \\mathbb{R}^{n \\times d}$ 且 $y \\in \\mathbb{R}^n$）由均方误差 (MSE) 损失函数衡量：\n$$\nL(\\theta) = \\frac{1}{2n} \\|X\\theta - y\\|_2^2 = \\frac{1}{2n} (X\\theta - y)^T(X\\theta - y)\n$$\n这是 $\\theta$ 的一个凸且可微的函数。\n\n为了执行梯度下降，我们需要损失函数相对于参数 $\\theta$ 的梯度。展开损失函数得到：\n$$\nL(\\theta) = \\frac{1}{2n} (\\theta^T X^T X \\theta - 2y^T X \\theta + y^T y)\n$$\n对 $\\theta$ 求导数得到梯度：\n$$\n\\nabla L(\\theta) = \\frac{1}{2n} (2 X^T X \\theta - 2 X^T y) = \\frac{1}{n} X^T (X\\theta - y)\n$$\n\n问题指定了三种训练程序，都使用固定的步长 $\\alpha  0$，共进行 $T$ 次迭代。\n\n1.  **无约束梯度下降**：此标准算法用于“从零开始”的基线（从 $\\theta_{\\text{scratch}} = 0$ 开始）和“无约束”微调（从预训练的 $\\theta_{\\text{src}}$ 开始）。在每一步 $k$ 的更新规则是：\n    $$\n    \\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)\n    $$\n\n2.  **投影梯度下降**：此算法用于“有约束”微调。它确保参数向量 $\\theta$ 始终满足线性等式约束 $C\\theta = d$，其中 $C \\in \\mathbb{R}^{m \\times d}$ 且 $d \\in \\mathbb{R}^m$。每次迭代包括两个步骤：\n    a. 一个标准的梯度下降步骤以找到一个中间点 $u_{k+1}$：\n    $$\n    u_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)\n    $$\n    b. 将 $u_{k+1}$ 投影到仿射约束集 $\\mathcal{A} = \\{\\theta \\in \\mathbb{R}^d : C\\theta = d\\}$ 上以获得下一个迭代 $\\theta_{k+1}$。该投影，记为 $P_{\\mathcal{A}}(u_{k+1})$，是 $\\mathcal{A}$ 中在欧几里得意义上最接近 $u_{k+1}$ 的点：\n    $$\n    \\theta_{k+1} = P_{\\mathcal{A}}(u_{k+1}) = \\arg\\min_{z \\in \\mathcal{A}} \\|z - u_{k+1}\\|_2^2\n    $$\n\n为了推导投影算子 $P_{\\mathcal{A}}(u)$ 的公式，我们求解在约束 $Cz = d$ 下的约束优化问题 $\\min_z \\frac{1}{2} \\|z - u\\|_2^2$。我们使用拉格朗日乘子法。拉格朗日函数是：\n$$\n\\mathcal{L}(z, \\lambda) = \\frac{1}{2} (z - u)^T(z - u) + \\lambda^T(Cz - d)\n$$\n其中 $\\lambda \\in \\mathbb{R}^m$ 是拉格朗日乘子向量。一阶最优性条件是：\n$$\n\\nabla_z \\mathcal{L} = z - u + C^T \\lambda = 0 \\implies z = u - C^T \\lambda\n$$\n将此代入约束 $Cz = d$ 中：\n$$\nC(u - C^T \\lambda) = d \\implies Cu - CC^T \\lambda = d \\implies CC^T \\lambda = Cu - d\n$$\n假设约束矩阵 $C$ 具有满行秩（即其行是线性无关的），则矩阵 $CC^T \\in \\mathbb{R}^{m \\times m}$ 是可逆的。我们可以解出 $\\lambda$：\n$$\n\\lambda = (CC^T)^{-1} (Cu - d)\n$$\n将 $\\lambda$ 代回 $z$ 的表达式中，得到投影公式：\n$$\nP_{\\mathcal{A}}(u) = z = u - C^T (CC^T)^{-1} (Cu - d)\n$$\n在没有约束的特殊情况下（$m=0$），矩阵 $C$ 为空（$C \\in \\mathbb{R}^{0 \\times d}$）。约束集是整个空间 $\\mathbb{R}^d$，投影就是简单的恒等算子，$P_{\\mathcal{A}}(u) = u$。\n\n评估指标是负迁移量 $\\operatorname{NT}(\\theta_{\\text{init}})$，定义为从 $\\theta_{\\text{init}}$ 微调的模型的最终损失与从零开始训练的模型 $\\theta_{\\text{scratch}}$ 的最终损失之间的差值：\n$$\n\\operatorname{NT}(\\theta_{\\text{init}}) = L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{init}})\\bigr) - L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{scratch}})\\bigr)\n$$\n我们必须测试约束是否减少了负迁移，这被表述为不等式：\n$$\n\\operatorname{NT}_{\\text{constrained}}  \\operatorname{NT}_{\\text{unconstrained}}\n$$\n令 $\\theta_{\\text{final, constrained}}$ 为从 $\\theta_{\\text{src}}$ 开始的投影梯度下降的最终参数，$\\theta_{\\text{final, unconstrained}}$ 为从 $\\theta_{\\text{src}}$ 开始的无约束梯度下降的最终参数。不等式变为：\n$$\nL(\\theta_{\\text{final, constrained}}) - L(\\theta_{\\text{final, scratch}})  L(\\theta_{\\text{final, unconstrained}}) - L(\\theta_{\\text{final, scratch}})\n$$\n这简化为对两种微调方法的最终损失的直接比较：\n$$\nL(\\theta_{\\text{final, constrained}})  L(\\theta_{\\text{final, unconstrained}})\n$$\n\n为了解决这个问题，我们使用提供的数据 ($X, y$)、超参数 ($\\alpha=0.1, T=5$) 以及每个测试用例 ($\\theta_{\\text{src}}, C, d$) 的具体情况来实现三种梯度下降程序。对于每种情况，我们计算最终损失 $L(\\theta_{\\text{final, constrained}})$ 和 $L(\\theta_{\\text{final, unconstrained}})$，并评估上述不等式的布尔真值。\n\n使用的值是：\n$d = 3$，$n = 4$。\n$X = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  1 \\\\ 2  0  1 \\end{bmatrix}$，$y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 2 \\end{bmatrix}$。\n$\\alpha = 0.1$，$T = 5$。\n$\\theta_{\\text{scratch}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating constrained fine-tuning against negative transfer.\n    \"\"\"\n    \n    # Fixed target dataset and hyperparameters\n    X = np.array([\n        [1, 0, 1],\n        [0, 1, 1],\n        [1, 1, 1],\n        [2, 0, 1]\n    ])\n    y = np.array([1, 2, 3, 2]).reshape(-1, 1)\n    \n    n, d_dim = X.shape\n    alpha = 0.1\n    T = 5\n    theta_scratch = np.zeros((d_dim, 1))\n\n    # Test Suite (theta_src, C, d)\n    test_cases = [\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.array([[0, 0, 1]]),\n            np.array([[0]])\n        ),\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.empty((0, d_dim)),\n            np.empty((0, 1))\n        ),\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.array([[1, 1, 0]]),\n            np.array([[0]])\n        ),\n        (\n            np.array([1.0, 2.0, 0.0]).reshape(-1, 1),\n            np.array([[0, 0, 1]]),\n            np.array([[0]])\n        )\n    ]\n\n    def compute_loss(theta):\n        \"\"\"Computes the MSE loss.\"\"\"\n        return (1 / (2 * n)) * np.sum((X @ theta - y)**2)\n\n    def compute_gradient(theta):\n        \"\"\"Computes the gradient of the MSE loss.\"\"\"\n        return (1 / n) * X.T @ (X @ theta - y)\n\n    def project(u, C, d_vec):\n        \"\"\"Projects a vector u onto the affine set {z : C z = d}.\"\"\"\n        if C.shape[0] == 0:  # No constraints\n            return u\n        \n        # u - C.T @ inv(C @ C.T) @ (C @ u - d)\n        C_T = C.T\n        CC_T = C @ C_T\n        CC_T_inv = np.linalg.inv(CC_T)\n        \n        projection_offset = C_T @ CC_T_inv @ (C @ u - d_vec)\n        return u - projection_offset\n\n    def unconstrained_gd(theta_init):\n        \"\"\"Performs unconstrained gradient descent.\"\"\"\n        theta = theta_init.copy()\n        for _ in range(T):\n            grad = compute_gradient(theta)\n            theta -= alpha * grad\n        return theta\n\n    def projected_gd(theta_init, C, d_vec):\n        \"\"\"Performs projected gradient descent.\"\"\"\n        theta = theta_init.copy()\n        for _ in range(T):\n            grad = compute_gradient(theta)\n            u = theta - alpha * grad\n            theta = project(u, C, d_vec)\n        return theta\n\n    # Calculate baseline loss (from scratch) once\n    theta_final_scratch = unconstrained_gd(theta_scratch)\n    L_final_scratch = compute_loss(theta_final_scratch)\n\n    results = []\n    for theta_src, C, d_vec in test_cases:\n        # Unconstrained fine-tuning\n        theta_final_unconstrained = unconstrained_gd(theta_src)\n        L_final_unconstrained = compute_loss(theta_final_unconstrained)\n        \n        # Constrained fine-tuning\n        theta_final_constrained = projected_gd(theta_src, C, d_vec)\n        L_final_constrained = compute_loss(theta_final_constrained)\n\n        # Negative transfer definitions\n        # NT_unconstrained = L_final_unconstrained - L_final_scratch\n        # NT_constrained = L_final_constrained - L_final_scratch\n        \n        # Check if NT_constrained  NT_unconstrained\n        # This simplifies to L_final_constrained  L_final_unconstrained\n        test_result = L_final_constrained  L_final_unconstrained\n        results.append(test_result)\n        \n    # Format and print the final output\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\nsolve()\n```", "id": "3195172"}]}