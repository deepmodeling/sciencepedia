{"hands_on_practices": [{"introduction": "本练习将介绍对抗性攻击和防御的基础概念。你将亲手实现经典的快速梯度符号法（FGSM）攻击，并探索一种简单而有效的正则化技术——标签平滑（Label Smoothing）——如何增强模型的鲁棒性。通过观察输入梯度幅值和模型在攻击下的错误分类率如何变化，你将直观地理解这些方法背后的工作机制。[@problem_id:3098457]", "problem": "给定一个固定的、带 softmax 输出的多类别线性分类器和一个合成数据集。您的目标是通过跟踪输入梯度的平均幅值以及在快速梯度符号法 (Fast Gradient Sign Method, FGSM) 和投影梯度下降 (Projected Gradient Descent, PGD) 攻击下的错分率变化，研究由标量参数 $\\alpha \\in [0,1]$ 控制的标签平滑如何影响对抗鲁棒性。您必须实现一个程序，为每个指定的 $\\alpha$ 计算平均输入梯度幅值以及在 FGSM 和 PGD 攻击后的错分率。\n\n使用的基础理论：\n- 多类别 softmax 回归：对于输入 $x \\in \\mathbb{R}^d$，其 logits $z \\in \\mathbb{R}^K$ 为 $z = W x + b$，其中 $W \\in \\mathbb{R}^{K \\times d}$ 且 $b \\in \\mathbb{R}^K$。softmax 映射为 $p_i = \\exp(z_i) / \\sum_{j=1}^K \\exp(z_j)$，其中 $i \\in \\{1,\\dots,K\\}$。\n- 交叉熵损失：对于目标分布 $y \\in \\Delta^{K-1}$（概率单纯形），损失函数为 $L(x,y) = -\\sum_{i=1}^K y_i \\log p_i$。\n- 标签平滑：对于真实类别 $t \\in \\{0,\\dots,K-1\\}$ 和平滑参数 $\\alpha \\in [0,1]$，定义 $y^{(\\alpha)}$ 为对所有 $i$ 都有 $y^{(\\alpha)}_i = \\alpha/K$，然后将 $(1-\\alpha)$ 加到坐标 $i=t$ 上，使得 $y^{(\\alpha)}_t = (1-\\alpha) + \\alpha/K$ 且对于 $i \\neq t$ 有 $y^{(\\alpha)}_i = \\alpha/K$。\n- 应用于 softmax 交叉熵模型的微积分链式法则。\n\n您不得使用超出这些定义范围的任何结果，并且必须从这些起点推导出任何所需的表达式。\n\n需要实现的攻击方法：\n- 快速梯度符号法 (FGSM)：对于给定的 $\\varepsilon > 0$，对抗样本为 $x_{\\text{FGSM}} = x + \\varepsilon \\cdot \\operatorname{sign}(\\nabla_x L(x, y^{(\\alpha)}))$，其中 $\\operatorname{sign}(\\cdot)$ 是逐元素应用的。\n- 带 $\\ell_\\infty$ 约束的投影梯度下降 (PGD)：从 $x^{(0)} = x$ 开始，对于给定的步长 $\\eta > 0$ 和步数 $T \\in \\mathbb{N}$，对 $t=0,\\dots,T-1$ 进行迭代 $x^{(t+1)} = \\Pi_{B_\\infty(x,\\varepsilon)}\\left(x^{(t)} + \\eta \\cdot \\operatorname{sign}(\\nabla_x L(x^{(t)}, y^{(\\alpha)}))\\right)$，其中 $\\Pi_{B_\\infty(x,\\varepsilon)}(\\cdot)$ 表示投影到以 $x$ 为中心、半径为 $\\varepsilon$ 的 $\\ell_\\infty$ 球上。\n\n数据集与模型规格（以确保科学真实性和确定性）：\n- 类别数：$K = 3$。\n- 输入维度：$d = 4$。\n- 类别均值：$\\mu_0 = (2,0,0,0)$，$\\mu_1 = (0,2,0,0)$，$\\mu_2 = (0,0,2,0)$。\n- 每个类别的样本数：$N_c = 20$，总计 $N = 60$。\n- 数据生成：对于每个类别 $c \\in \\{0,1,2\\}$，抽取 $N_c$ 个点 $x = \\mu_c + \\delta$，其中 $\\delta$ 是从 $\\mathbb{R}^4$ 中独立采样的零均值正态分布，每个坐标的标准差为 $0.2$。使用固定的伪随机生成器种子以确保数据集是确定性的。\n- 模型参数：$W = \\begin{bmatrix} 1.2  0  0  0 \\\\ 0  1.2  0  0 \\\\ 0  0  1.2  0 \\end{bmatrix}$ 和 $b = (0,0,0)$。\n\n目标与度量指标：\n- 梯度幅值：对于每个样本 $(x,t)$ 和给定的 $\\alpha$，计算欧几里得范数 $\\|\\nabla_x L(x, y^{(\\alpha)})\\|_2$，然后报告在所有 $N$ 个样本上的全数据集平均值。\n- 给定 $\\alpha$ 下 FGSM 的错分率：为每个样本生成 $x_{\\text{FGSM}}$，使用原始模型（不重新训练）将其分类为 $\\hat{t} = \\arg\\max_i z_i$，并报告 $\\hat{t} \\neq t$ 的样本比例（以小数值表示）。\n- 给定 $\\alpha$ 下 PGD 的错分率：类似地，在 $T$ 步后生成 $x_{\\text{PGD}}$，并报告预测类别不等于真实类别的样本比例。\n\n测试套件：\n- 平滑参数：$\\alpha \\in \\{0.0, 0.1, 0.5, 0.9, 0.99\\}$。\n- FGSM 参数：$\\varepsilon = 0.25$。\n- PGD 参数：$\\varepsilon = 0.25$，$\\eta = 0.07$, $T = 12$。\n\n角度单位不适用。不出现物理单位。所有输出必须是十进制数（浮点数）。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个元素对应于给定顺序中的一个 $\\alpha$，并且本身必须是包含四个浮点数的列表，顺序为 $[\\alpha, \\text{avg\\_grad\\_norm}, \\text{fgsm\\_misrate}, \\text{pgd\\_misrate}]$。例如，整体格式必须类似于 $[[\\alpha_1,g_1,m^{\\text{FGSM}}_1,m^{\\text{PGD}}_1],[\\alpha_2,g_2,m^{\\text{FGSM}}_2,m^{\\text{PGD}}_2],\\dots]$。", "solution": "问题陈述已经过分析并被认为是有效的。它在科学上基于深度学习的原理，在数学上是适定的（well-posed），所有必要的常数和参数都已定义，并且完全客观。它提出了一个在对抗性机器学习领域中标准的、尽管是简化的研究任务。所有组成部分——线性 softmax 模型、交叉熵损失、标签平滑和对抗性攻击方法（FGSM, PGD）——都是该领域的标准方法。为模型、数据集和攻击提供的参数是自包含且一致的，允许一个唯一的、可验证的数值解。\n\n我们接下来推导解决方案。任务的核心是计算损失函数关于输入的梯度，该梯度随后用于生成对抗性样本并分析模型的属性。\n\n### 数学公式与梯度推导\n\n该模型是一个带 softmax 激活函数的多类别线性分类器。对于一个输入向量 $x \\in \\mathbb{R}^d$，其 logits $z \\in \\mathbb{R}^K$ 计算如下：\n$$ z = Wx + b $$\n其中 $W \\in \\mathbb{R}^{K \\times d}$ 是权重， $b \\in \\mathbb{R}^K$ 是偏置。问题指定了 $d=4$ 和 $K=3$。\n\nsoftmax 函数将 logits 转换为一个概率分布 $p \\in \\Delta^{K-1}$：\n$$ p_i = \\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)} $$\n对于每个类别 $i \\in \\{1, \\dots, K\\}$。\n\n损失函数是预测概率分布 $p$ 和目标分布 $y \\in \\Delta^{K-1}$ 之间的交叉熵：\n$$ L(x, y) = -\\sum_{i=1}^K y_i \\log p_i $$\n\n目标分布 $y$ 是通过标签平滑构建的。对于一个真实类别 $t$，独热（one-hot）目标向量是 $y^{\\text{onehot}}$，其中 $(y^{\\text{onehot}})_t = 1$ 且对于 $i \\neq t$ 有 $(y^{\\text{onehot}})_i = 0$。平滑标签向量 $y^{(\\alpha)}$ 是独热向量和均匀分布 $u$（其中对所有 $i$ 都有 $u_i = 1/K$）的凸组合：\n$$ y^{(\\alpha)} = (1-\\alpha)y^{\\text{onehot}} + \\alpha u $$\n这产生了问题中提供的分量式定义：\n$$ y^{(\\alpha)}_i = \\begin{cases} (1-\\alpha) + \\alpha/K  \\text{if } i=t \\\\ \\alpha/K  \\text{if } i \\neq t \\end{cases} $$\n\n为了生成对抗性攻击，我们需要损失 $L$ 关于输入 $x$ 的梯度，记作 $\\nabla_x L$。我们应用链式法则：\n$$ \\nabla_x L(x, y^{(\\alpha)}) = \\left(\\frac{\\partial z}{\\partial x}\\right)^T \\nabla_z L(x, y^{(\\alpha)}) $$\nlogits $z=Wx+b$ 关于 $x$ 的雅可比矩阵就是权重矩阵 $W$：\n$$ \\frac{\\partial z}{\\partial x} = W $$\n接下来，我们求损失关于 logits 的梯度 $\\nabla_z L$。关于单个 logit $z_j$ 的偏导数是：\n$$ \\frac{\\partial L}{\\partial z_j} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_j} $$\n根据损失的定义，$\\frac{\\partial L}{\\partial p_i} = -y_i/p_i$。softmax 函数的导数是一个标准结果：$\\frac{\\partial p_i}{\\partial z_j} = p_i(\\delta_{ij} - p_j)$，其中 $\\delta_{ij}$ 是克罗内克（Kronecker）δ。\n将这些代入求和式中：\n$$ \\frac{\\partial L}{\\partial z_j} = \\sum_{i=1}^K \\left(-\\frac{y_i}{p_i}\\right) (p_i(\\delta_{ij} - p_j)) = \\sum_{i=1}^K -y_i(\\delta_{ij} - p_j) $$\n$$ = -y_j \\cdot 1 - \\sum_{i=1}^K (-y_i p_j) = -y_j + p_j \\sum_{i=1}^K y_i $$\n由于 $y^{(\\alpha)}$ 是一个概率分布，$\\sum_i y^{(\\alpha)}_i = 1$。这可将表达式简化为：\n$$ \\frac{\\partial L}{\\partial z_j} = p_j - y_j^{(\\alpha)} $$\n用向量形式表示，$\\nabla_z L = p - y^{(\\alpha)}$。\n\n最后，我们组合各部分得到关于输入 $x$ 的梯度：\n$$ \\nabla_x L(x, y^{(\\alpha)}) = W^T (p - y^{(\\alpha)}) $$\n这个梯度是所有后续计算的基石。\n\n### 对抗性攻击算法\n\n有了梯度表达式，我们就可以定义攻击方法了。\n\n1.  **快速梯度符号法 (FGSM):** 一种单步攻击，它在梯度符号的方向上增加一个扰动。\n    $$ x_{\\text{FGSM}} = x + \\varepsilon \\cdot \\text{sign}(\\nabla_x L(x, y^{(\\alpha)})) $$\n    $\\text{sign}(\\cdot)$ 函数是逐元素应用的，$\\varepsilon$ 是受 $\\ell_\\infty$ 范数约束的攻击强度。\n\n2.  **投影梯度下降 (PGD):** FGSM 的迭代版本，它采用多个较小的步长，并将结果投影回原始输入 $x$ 周围半径为 $\\varepsilon$ 的 $\\ell_\\infty$ 球内。\n    从 $x^{(0)} = x$ 开始，进行 $T$ 步的更新规则是：\n    $$ x^{(t+1)} = \\Pi_{B_\\infty(x,\\varepsilon)}\\left(x^{(t)} + \\eta \\cdot \\text{sign}(\\nabla_x L(x^{(t)}, y^{(\\alpha)}))\\right) $$\n    其中 $\\eta < \\varepsilon$ 是步长。投影算子 $\\Pi_{B_\\infty(x,\\varepsilon)}(z)$ 会裁剪扰动，以确保最终得到的点 $z$ 满足 $\\|z-x\\|_\\infty \\le \\varepsilon$。这是通过逐元素裁剪实现的：\n    $$ \\Pi_{B_\\infty(x,\\varepsilon)}(z) = x + \\text{clip}(z-x, -\\varepsilon, \\varepsilon) $$\n\n### 实现计划\n\n程序将执行以下步骤：\n1.  **初始化参数：** 设置模型参数 ($W, b$)、数据集参数 ($K, d, \\mu_c, N_c$) 和攻击参数 ($\\varepsilon, \\eta, T$)。\n2.  **生成数据集：** 使用固定的随机种子创建一个确定性的合成数据集。对于 $K=3$ 个类别中的每一个，通过向各自的类别均值 $\\mu_c$ 添加标准差为 $0.2$ 的高斯噪声来生成 $N_c=20$ 个样本。\n3.  **遍历平滑参数：** 循环遍历 $\\alpha \\in \\{0.0, 0.1, 0.5, 0.9, 0.99\\}$ 中的每个指定值。\n4.  **对于每个 $\\alpha$：**\n    a. 为平均梯度范数以及 FGSM 和 PGD 的错分计数初始化累加器。\n    b. 遍历数据集中的每个样本 $(x_{\\text{orig}}, t)$。\n    c. 为真实类别 $t$ 构建平滑标签向量 $y^{(\\alpha)}$。\n    d. **计算梯度范数：** 计算 $\\nabla_x L(x_{\\text{orig}}, y^{(\\alpha)})$ 及其欧几里得范数 $\\|\\cdot\\|_2$。将其值累加到累加器中。\n    e. **执行 FGSM 攻击：** 生成 $x_{\\text{FGSM}}$。使用模型 ($z=Wx+b$) 对其进行分类。如果预测类别不是 $t$，则增加 FGSM 错分计数器的值。\n    f. **执行 PGD 攻击：** 运行迭代 PGD 算法 $T=12$ 步以生成 $x_{\\text{PGD}}$。对其进行分类。如果预测不正确，则增加 PGD 错分计数器的值。\n    g. **计算指标：** 处理完所有样本后，计算平均梯度范数和错分率（计数值 / 样本总数）。\n5.  **输出结果：** 存储为每个 $\\alpha$ 计算的指标 $[\\alpha, \\text{avg\\_grad\\_norm}, \\text{fgsm\\_misrate}, \\text{pgd\\_misrate}]$，并将最终输出格式化为这些列表的列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes adversarial robustness metrics for a linear softmax classifier\n    under varying levels of label smoothing.\n    \"\"\"\n    # Dataset and Model Specifications\n    K = 3  # Number of classes\n    D = 4  # Input dimension\n    NC = 20  # Number of samples per class\n    N = K * NC  # Total number of samples\n\n    # Class means\n    MEANS = np.array([\n        [2.0, 0.0, 0.0, 0.0],\n        [0.0, 2.0, 0.0, 0.0],\n        [0.0, 0.0, 2.0, 0.0]\n    ])\n    \n    # Noise standard deviation\n    STD_DEV = 0.2\n\n    # Model parameters\n    W = np.array([\n        [1.2, 0.0, 0.0, 0.0],\n        [0.0, 1.2, 0.0, 0.0],\n        [0.0, 0.0, 1.2, 0.0]\n    ])\n    B = np.array([0.0, 0.0, 0.0])\n\n    # Test Suite Parameters\n    ALPHA_VALUES = [0.0, 0.1, 0.5, 0.9, 0.99]\n    EPSILON_FGSM = 0.25\n    EPSILON_PGD = 0.25\n    ETA_PGD = 0.07\n    T_PGD = 12\n    \n    # Fixed seed for deterministic dataset generation\n    SEED = 42\n    rng = np.random.default_rng(seed=SEED)\n\n    # Generate dataset\n    X_data = np.zeros((N, D))\n    T_labels = np.zeros(N, dtype=int)\n    for c in range(K):\n        start_idx = c * NC\n        end_idx = (c + 1) * NC\n        noise = rng.normal(loc=0.0, scale=STD_DEV, size=(NC, D))\n        X_data[start_idx:end_idx, :] = MEANS[c] + noise\n        T_labels[start_idx:end_idx] = c\n\n    def stable_softmax(z):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        z_shifted = z - np.max(z, axis=-1, keepdims=True)\n        exp_z = np.exp(z_shifted)\n        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n    \n    def get_logits(x_input):\n        \"\"\"Computes logits: z = Wx + b.\"\"\"\n        return x_input @ W.T + B\n\n    def compute_gradient(x, y_target):\n        \"\"\"Computes gradient of cross-entropy loss wrt input x.\"\"\"\n        z = get_logits(x)\n        p = stable_softmax(z)\n        grad = W.T @ (p - y_target)\n        return grad\n\n    results = []\n\n    for alpha in ALPHA_VALUES:\n        \n        total_grad_norm = 0.0\n        fgsm_misclassified_count = 0\n        pgd_misclassified_count = 0\n\n        for i in range(N):\n            x_orig = X_data[i]\n            t_true = T_labels[i]\n\n            # Construct smoothed label vector\n            y_target = np.full(K, alpha / K)\n            y_target[t_true] += (1 - alpha)\n            \n            # 1. Calculate gradient norm on original sample\n            grad_orig = compute_gradient(x_orig, y_target)\n            total_grad_norm += np.linalg.norm(grad_orig)\n\n            # 2. FGSM Attack\n            x_fgsm = x_orig + EPSILON_FGSM * np.sign(grad_orig)\n            z_fgsm = get_logits(x_fgsm)\n            pred_fgsm = np.argmax(z_fgsm)\n            if pred_fgsm != t_true:\n                fgsm_misclassified_count += 1\n            \n            # 3. PGD Attack\n            x_pgd = np.copy(x_orig)\n            for _ in range(T_PGD):\n                grad_pgd = compute_gradient(x_pgd, y_target)\n                x_pgd_step = x_pgd + ETA_PGD * np.sign(grad_pgd)\n                # Projection step\n                perturbation = x_pgd_step - x_orig\n                perturbation = np.clip(perturbation, -EPSILON_PGD, EPSILON_PGD)\n                x_pgd = x_orig + perturbation\n\n            z_pgd = get_logits(x_pgd)\n            pred_pgd = np.argmax(z_pgd)\n            if pred_pgd != t_true:\n                pgd_misclassified_count += 1\n\n        avg_grad_norm = total_grad_norm / N\n        fgsm_misrate = fgsm_misclassified_count / N\n        pgd_misrate = pgd_misclassified_count / N\n\n        results.append([alpha, avg_grad_norm, fgsm_misrate, pgd_misrate])\n\n    # Format the final output string exactly as required\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3098457"}, {"introduction": "一个防御机制看似有效，但它是否真的增强了模型的鲁棒性？本实践旨在解决一个关键问题：“梯度掩码”（Gradient Masking）现象，即模型看似安全，仅仅是因为用于生成攻击的梯度变得无用。你将学习使用迁移攻击（Transfer Attack）这一强大工具——在一个替代模型上制作对抗样本，然后用它攻击目标模型——来区分真实的鲁棒性与这种虚假的安全感。[@problem_id:3097091]", "problem": "考虑一个二元分类场景，其输入向量为 $\\mathbf{x} \\in \\mathbb{R}^d$，标签为 $y \\in \\{0,1\\}$。学习过程遵循经验风险最小化，即带参数 $\\theta$ 的模型 $f_\\theta$ 在训练数据上最小化经验损失。二元分类的标准基础损失是逻辑斯谛损失，由概率 $p_\\theta(\\mathbf{x}) \\in (0,1)$ 和交叉熵 $L(\\mathbf{x},y;\\theta) = -y \\log p_\\theta(\\mathbf{x}) - (1-y) \\log(1 - p_\\theta(\\mathbf{x}))$ 定义。对于给定的输入-标签对 $(\\mathbf{x},y)$ 和范数预算 $\\epsilon > 0$，对抗样本是任何满足 $\\|\\delta\\|_\\infty \\le \\epsilon$ 且能增加损失 $L(\\mathbf{x} + \\delta, y; \\theta)$ 的扰动 $\\delta$。对抗样本制作问题可以看作一个约束最大化问题：在集合 $\\{\\delta : \\|\\delta\\|_\\infty \\le \\epsilon\\}$ 上找到 $\\delta^\\star$ 以最大化 $L(\\mathbf{x} + \\delta, y; \\theta)$。可迁移性是指为增加一个模型 $f_\\theta$ 的损失而制作的对抗样本，同样也能导致在相同数据分布上训练的另一个不同模型 $g_\\phi$ 出错的特性。\n\n梯度掩码是一种失效模式，在这种模式下，用于优化或攻击生成的梯度变得无信息（例如，通过不可微的预处理或饱和函数），从而降低了白盒攻击的有效性，但并未提供真正的鲁棒性。一种基于原则的检测方法是，将候选掩码模型上的白盒攻击成功率与来自梯度信息有效的代理模型的迁移攻击成功率进行比较。如果在代理模型 $f_\\theta$ 上制作的对抗样本显著增加了 $g_\\phi$ 的错误率，而直接在 $g_\\phi$ 上制作的白盒攻击效果很差，这表明 $g_\\phi$ 中存在梯度掩码。\n\n您的任务是从基本原理开始编写一个完整的程序，实现以下实验：\n\n- 在相同的合成数据上构建两个模型：\n  1. 一个标准的逻辑斯谛回归分类器 $f_\\theta$，它将 $\\mathbf{x}$ 映射到 $p_\\theta(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$，其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，其参数 $\\theta = (\\mathbf{w}, b)$ 通过梯度下降法训练以最小化经验交叉熵。\n  2. 在相同数据上训练的第二个分类器 $g_\\phi$，根据测试用例的不同，有三种可能的实例化：\n     - 一个模型，其包含不可微的输入预处理 $h(\\mathbf{x})$（例如，逐元素符号函数），然后对变换后的输入进行逻辑斯谛回归，产生 $p_\\phi(\\mathbf{x}) = \\sigma(\\mathbf{u}^\\top h(\\mathbf{x}) + c)$，其参数 $\\phi = (\\mathbf{u}, c)$ 仅相对于 $(\\mathbf{u}, c)$ 通过梯度下降进行训练。在为 $g_\\phi$ 生成攻击时，损失函数关于输入 $\\mathbf{x}$ 的梯度必须被视为零。\n     - 一个使用对抗训练的标准逻辑斯谛回归模型，在每个训练轮次中，输入被替换为在 $\\ell_\\infty$ 预算内的对抗性扰动输入，以最小化最坏情况下的经验损失。\n     - 一个正常训练的标准逻辑斯谛回归模型（无掩码，无对抗训练）。\n\n- 实现一个 $\\ell_\\infty$ 约束的投影梯度上升攻击来制作对抗样本，使用损失函数关于 $\\mathbf{x}$ 的梯度。优化过程应通过沿梯度符号方向迭代上升，并投影回原始输入周围半径为 $\\epsilon$ 的 $\\ell_\\infty$ 球内来进行。该攻击必须为任意 $\\epsilon > 0$、步长 $\\alpha > 0$ 和固定步数 $T \\in \\mathbb{N}$ 实现。\n\n- 对于每个测试用例，计算两个经验成功率：\n  1. 在 $g_\\phi$ 上的白盒攻击成功率：当使用 $g_\\phi$ 自身的（可能被掩码的）梯度进行攻击时，$g_\\phi$ 的预测标签发生改变的输入所占的比例。\n  2. 从 $f_\\theta$ 到 $g_\\phi$ 的迁移攻击成功率：当使用 $f_\\theta$ 的梯度制作对抗样本并将其应用于 $g_\\phi$ 时，$g_\\phi$ 的预测标签发生改变的输入所占的比例。\n\n- 对于一个测试用例，当且仅当以下两个条件同时成立时，声明检测到梯度掩码：$g_\\phi$ 上的白盒攻击成功率小于阈值 $\\tau \\in (0,1)$，且从 $f_\\theta$ 到 $g_\\phi$ 的迁移攻击成功率比 $g_\\phi$ 上的白盒攻击成功率至少高出一个边际 $\\Delta \\in (0,1)$。\n\n数据生成必须是完全合成、统计上合理且可复现的。使用一个二维输入空间 $d = 2$，并采用类条件高斯分布：为每个类别生成 $n$ 个独立样本，其中类别 $y = 1$ 的均值为 $\\boldsymbol{\\mu}_1 \\in \\mathbb{R}^2$，对角协方差为 $\\operatorname{diag}(\\sigma^2, \\sigma^2)$；类别 $y = 0$ 的均值为 $\\boldsymbol{\\mu}_0 \\in \\mathbb{R}^2$，协方差相同。抽取输入，将它们裁剪到 $[-1, 1]^2$ 范围内，并相应地使用标签 $y \\in \\{0,1\\}$。在程序内部固定所有随机种子以确保确定性。\n\n您的程序必须实现上述内容，并评估以下参数值测试套件：\n\n- 测试用例 1（掩码化的 $g_\\phi$）：\n  - 数据：每类 $n = 200$ 个样本，$\\boldsymbol{\\mu}_1 = (0.3, 0.3)$，$\\boldsymbol{\\mu}_0 = (-0.3, -0.3)$，$\\sigma = 0.6$。\n  - 攻击：$\\epsilon = 1.0$，$\\alpha = 0.2$，T = 10。\n  - 模型 $f_\\theta$：正常训练的标准逻辑斯谛回归。\n  - 模型 $g_\\phi$：输入预处理 $h(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{x})$（逐元素），对 $h(\\mathbf{x})$ 进行逻辑斯谛回归；对 $g_\\phi$ 的攻击中，关于 $\\mathbf{x}$ 的梯度设为 $0$。\n  - 检测参数：$\\tau = 0.1$，$\\Delta = 0.2$。\n\n- 测试用例 2（对抗训练的 $g_\\phi$）：\n  - 数据：每类 $n = 200$ 个样本，$\\boldsymbol{\\mu}_1 = (0.7, 0.7)$，$\\boldsymbol{\\mu}_0 = (-0.7, -0.7)$，$\\sigma = 0.3$。\n  - 攻击：$\\epsilon = 0.3$，$\\alpha = 0.05$，T = 40。\n  - 模型 $f_\\theta$：正常训练的标准逻辑斯谛回归。\n  - 模型 $g_\\phi$：使用对抗训练的标准逻辑斯谛回归，训练时使用相同的 $\\epsilon$ 和 $\\alpha$ 进行 $50$ 个轮次。\n  - 检测参数：$\\tau = 0.2$，$\\Delta = 0.2$。\n\n- 测试用例 3（两个模型均为正常模型）：\n  - 数据：每类 $n = 200$ 个样本，$\\boldsymbol{\\mu}_1 = (0.2, 0.2)$，$\\boldsymbol{\\mu}_0 = (-0.2, -0.2)$，$\\sigma = 0.5$。\n  - 攻击：$\\epsilon = 0.3$，$\\alpha = 0.05$，T = 20。\n  - 模型 $f_\\theta$：正常训练的标准逻辑斯谛回归。\n  - 模型 $g_\\phi$：正常训练的标准逻辑斯谛回归。\n  - 检测参数：$\\tau = 0.2$，$\\Delta = 0.2$。\n\n所需输出：\n\n- 对于每个测试用例 $i \\in \\{1,2,3\\}$，计算一个布尔值，指示在所述检测规则下是否在 $g_\\phi$ 中检测到梯度掩码。\n- 您的程序应生成单行输出，其中包含三个布尔值，按测试用例顺序排列，形式为逗号分隔的列表，并用方括号括起，例如 $[{\\rm True},{\\rm False},{\\rm True}]$。\n\n所有计算都是无单位的。不使用角度。不必打印百分比；只需要布尔值。您的程序必须是自包含的，不需要用户输入，并严格遵循指定的输出格式。", "solution": "该问题要求实现一个实验来检测梯度掩码，这是一种评估机器学习模型鲁棒性时的失效模式。解决方案涉及构建、训练和攻击两个二元分类器 $f_\\theta$ 和 $g_\\phi$，并评估一个特定的检测准则。\n\n### 1. 理论框架\n\n**二元分类与逻辑斯谛损失：**\n场景是一个二元分类任务，输入向量为 $\\mathbf{x} \\in \\mathbb{R}^d$，标签为 $y \\in \\{0,1\\}$。我们使用逻辑斯谛回归模型，该模型计算正类（$y=1$）的概率为 $p_\\theta(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$，其中 $\\sigma(z) = (1 + e^{-z})^{-1}$ 是 sigmoid 函数，$\\theta = (\\mathbf{w}, b)$ 是模型参数。模型通过最小化训练数据集上二元交叉熵（逻辑斯谛）损失的经验平均值进行训练：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N L(\\mathbf{x}_i, y_i; \\theta) = \\frac{1}{N} \\sum_{i=1}^N \\left[ -y_i \\log p_\\theta(\\mathbf{x}_i) - (1-y_i) \\log(1 - p_\\theta(\\mathbf{x}_i)) \\right]\n$$\n这个最小化过程使用梯度下降法完成，参数沿损失梯度的反方向迭代更新：\n$$\n\\nabla_\\mathbf{w} \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (p_\\theta(\\mathbf{x}_i) - y_i)\\mathbf{x}_i \\quad , \\quad \\nabla_b \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N (p_\\theta(\\mathbf{x}_i) - y_i)\n$$\n\n**对抗攻击与梯度掩码：**\n对抗样本是一个经过轻微扰动的输入 $\\mathbf{x}' = \\mathbf{x} + \\delta$，旨在欺骗模型。扰动 $\\delta$ 受到约束，通常在 $\\ell_\\infty$ 范数球内：$\\|\\delta\\|_\\infty \\le \\epsilon$。在此约束下最有效的扰动是通过最大化模型损失找到的。一种常用方法是投影梯度上升（PGA），它沿着损失函数关于输入的梯度方向迭代更新输入：\n$$\n\\mathbf{x}^{(t+1)} = \\mathbf{x}^{(t)} + \\alpha \\cdot \\operatorname{sign}(\\nabla_{\\mathbf{x}^{(t)}} L(\\mathbf{x}^{(t)}, y; \\theta))\n$$\n其中 $\\alpha$ 是步长。损失函数关于单个输入 $\\mathbf{x}$ 的梯度由下式给出：\n$$\n\\nabla_\\mathbf{x} L = (p_\\theta(\\mathbf{x}) - y)\\mathbf{w}\n$$\n每一步之后，总扰动被投影（裁剪）以满足 $\\|\\mathbf{x}^{(t+1)} - \\mathbf{x}^{(0)}\\|_\\infty \\le \\epsilon$，并且得到的对抗样本被裁剪到有效的输入域（例如 $[-1, 1]^d$）。\n\n当一个模型因为其损失曲面“平坦”或其梯度因其他原因不包含信息，而对这类白盒攻击（攻击者完全访问模型）显得鲁棒时，就发生了梯度掩码。这提供了一种虚假的安全感。梯度掩码的一个关键指标是高可迁移性：为另一个非掩码模型 $f_\\theta$ 创建的对抗样本，对候选模型 $g_\\phi$ 仍然有效，即使 $g_\\phi$ 对基于其自身梯度的攻击看起来很鲁棒。\n\n### 2. 实验设计\n\n**数据生成：**\n我们在 $\\mathbb{R}^2$ 中从两个类条件高斯分布生成一个合成数据集。对于每类 $n$ 个样本，类别 $y=1$ 从 $\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2\\mathbf{I})$ 中抽取，类别 $y=0$ 从 $\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2\\mathbf{I})$ 中抽取。所有生成的点都被裁剪到超立方体 $[-1, 1]^2$ 内。所有随机过程都设置种子以保证可复现性。\n\n**模型配置：**\n- **代理模型 $f_\\theta$**：一个标准的逻辑斯谛回归分类器，通过梯度下降在生成的数据上进行训练。该模型作为迁移攻击的来源。\n\n- **目标模型 $g_\\phi$**：该模型根据测试用例的不同，采用以下三种形式之一：\n    1.  **掩码模型**：采用一个不可微的预处理步骤 $h(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{x})$。模型为 $p_\\phi(\\mathbf{x}) = \\sigma(\\mathbf{u}^\\top h(\\mathbf{x}) + c)$。它通过梯度下降在变换后的数据上正常训练其参数 $\\phi = (\\mathbf{u}, c)$。为了制作对此模型的白盒攻击，问题指定损失函数关于输入 $\\mathbf{x}$ 的梯度被视为零（$\\nabla_\\mathbf{x} L = \\mathbf{0}$）。这直接模拟了梯度掩码。\n    2.  **对抗训练模型**：一个为实现真正鲁棒性而训练的标准逻辑斯谛回归模型。训练目标是经验最坏情况损失最小化：\n        $$\n        \\min_{\\phi} \\frac{1}{N} \\sum_{i=1}^N \\max_{\\|\\delta_i\\|_\\infty \\le \\epsilon} L(\\mathbf{x}_i + \\delta_i, y_i; \\phi)\n        $$\n        在训练的每个轮次中，内部最大化问题通过使用PGA进行固定步数的近似求解。\n    3.  **正常模型**：一个标准的逻辑斯谛回归分类器，其架构和训练方式与 $f_\\theta$ 相同。\n\n**评估与检测逻辑：**\n对于每个测试用例，我们计算两个指标：\n1.  **白盒攻击成功率**：当使用 $g_\\phi$ 自身的梯度进行攻击时，其预测发生改变的样本比例。\n    $$\n    R_{\\text{white-box}} = \\frac{1}{N_{\\text{total}}} \\sum_{i=1}^{N_{\\text{total}}} \\mathbb{I} \\left[ g_\\phi(\\mathbf{x}_i^{\\text{adv-g}}) \\neq g_\\phi(\\mathbf{x}_i) \\right]\n    $$\n    其中 $\\mathbf{x}_i^{\\text{adv-g}}$ 是使用 $g_\\phi$ 的梯度从 $\\mathbf{x}_i$ 制作的对抗样本。\n2.  **迁移攻击成功率**：当受到使用代理模型 $f_\\theta$ 的梯度制作的对抗样本攻击时，$g_\\phi$ 的预测发生改变的样本比例。\n    $$\n    R_{\\text{transfer}} = \\frac{1}{N_{\\text{total}}} \\sum_{i=1}^{N_{\\text{total}}} \\mathbb{I} \\left[ g_\\phi(\\mathbf{x}_i^{\\text{adv-f}}) \\neq g_\\phi(\\mathbf{x}_i) \\right]\n    $$\n    其中 $\\mathbf{x}_i^{\\text{adv-f}}$ 是使用 $f_\\theta$ 的梯度从 $\\mathbf{x}_i$ 制作的。\n\n当且仅当以下两个条件都满足时，检测到梯度掩码：\n- $R_{\\text{white-box}} < \\tau$\n- $R_{\\text{transfer}} > R_{\\text{white-box}} + \\Delta$\n\n### 3. 预期结果\n\n- **测试用例 1（掩码化的 $g_\\phi$）**：对 $g_\\phi$ 的白盒攻击将失败，因为其输入梯度被定义为零，导致 $R_{\\text{white-box}} = 0$。来自标准模型 $f_\\theta$ 的迁移攻击预计会有效，因为扰动可以改变输入特征的符号，从而改变 $g_\\phi$ 的预处理器 $h(\\mathbf{x})$ 的输出。因此，我们预期 $R_{\\text{transfer}} > \\Delta$。检测条件应该被满足，结果为 `True`。\n- **测试用例 2（对抗训练的 $g_\\phi$）**：对抗训练应该会赋予真正的鲁棒性。我们预期 $R_{\\text{white-box}}$ 会很低，可能低于 $\\tau$。然而，这种鲁棒性也应该会降低迁移攻击的有效性，因此 $R_{\\text{transfer}}$ 预计也会很低，并且不大可能超过 $R_{\\text{white-box}}$ 达到边际 $\\Delta$。检测应该会失败，结果为 `False`。\n- **测试用例 3（正常模型的 $g_\\phi$）**：$f_\\theta$ 和 $g_\\phi$ 都是标准的、脆弱的模型。对 $g_\\phi$ 的白盒攻击应该会高度有效，所以 $R_{\\text{white-box}}$ 很可能会大于 $\\tau$。这将导致第一个检测条件失败，结果为 `False`。\n\n实现过程是创建一个能够处理所有三种模型类型的 `LogisticRegression` 类，一个 `pga_attack` 函数，以及一个主循环来为每个测试用例执行所述逻辑。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# No other libraries are permitted, not even scipy.\n\ndef solve():\n    \"\"\"\n    Main function to run the gradient masking detection experiment.\n    \"\"\"\n\n    class LogisticRegression:\n        \"\"\"\n        A logistic regression classifier with configurable training and attack modes.\n        \"\"\"\n        def __init__(self, model_type='normal', preprocessor=None):\n            if model_type not in ['normal', 'masked', 'adversarial']:\n                raise ValueError(\"Invalid model_type\")\n            self.model_type = model_type\n            self.preprocessor = preprocessor\n            self.w = None\n            self.b = None\n\n        def _sigmoid(self, z):\n            # Clip z to prevent overflow in np.exp\n            z = np.clip(z, -500, 500)\n            return 1 / (1 + np.exp(-z))\n\n        def predict_proba(self, X):\n            X_proc = self.preprocessor(X) if self.preprocessor else X\n            z = X_proc @ self.w + self.b\n            return self._sigmoid(z)\n\n        def predict(self, X):\n            return (self.predict_proba(X) >= 0.5).astype(int)\n\n        def loss(self, X, y):\n            p = self.predict_proba(X)\n            # Add a small epsilon to avoid log(0)\n            epsilon = 1e-9\n            p = np.clip(p, epsilon, 1 - epsilon)\n            return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n\n        def fit(self, X, y, lr, epochs, adv_params=None):\n            X_proc = self.preprocessor(X) if self.preprocessor else X\n            n_samples, n_features = X_proc.shape\n            \n            # Initialize weights\n            self.w = np.zeros((n_features, 1))\n            self.b = 0.0\n\n            # Reshape y to be a column vector if it's not\n            if y.ndim == 1:\n                y = y.reshape(-1, 1)\n\n            for epoch in range(epochs):\n                if self.model_type == 'adversarial' and adv_params:\n                    # Adversarial Training: generate adversarial examples for this epoch\n                    X_adv = self._pga_attack(X, y, **adv_params['adv_params'])\n                    X_train = X_adv\n                else:\n                    X_train = X\n\n                # Use preprocessor if the model is 'masked'\n                X_train_proc = self.preprocessor(X_train) if self.preprocessor else X_train\n\n                p = self.predict_proba(X_train) # Probabilities on original X for grad, but update on adversarial\n                \n                # Full-batch gradient descent\n                dw = (1 / n_samples) * X_train_proc.T @ (p - y)\n                db = (1 / n_samples) * np.sum(p - y)\n\n                self.w -= lr * dw\n                self.b -= lr * db\n        \n        def _gradient_wrt_input(self, X, y):\n            \"\"\"Computes the gradient of the loss with respect to the input X.\"\"\"\n            if self.model_type == 'masked':\n                return np.zeros_like(X)\n\n            if y.ndim == 1:\n                y = y.reshape(-1, 1)\n\n            p = self.predict_proba(X)\n            grad = (p - y) @ self.w.T\n            return grad\n\n        def _pga_attack(self, X, y, epsilon, alpha, T):\n            \"\"\"Projected Gradient Ascent attack.\"\"\"\n            X_adv = X.copy()\n            X_orig = X.copy()\n\n            for _ in range(T):\n                grad = self._gradient_wrt_input(X_adv, y)\n                X_adv = X_adv + alpha * np.sign(grad)\n                \n                # Project perturbation back to epsilon-ball\n                delta = X_adv - X_orig\n                delta = np.clip(delta, -epsilon, epsilon)\n                X_adv = X_orig + delta\n                \n                # Clip to valid input range [-1, 1]\n                X_adv = np.clip(X_adv, -1.0, 1.0)\n            \n            return X_adv\n\n    def generate_data(n_per_class, mu1, mu0, sigma, seed):\n        \"\"\"Generates synthetic 2D data from two Gaussian distributions.\"\"\"\n        np.random.seed(seed)\n        cov = np.diag([sigma**2, sigma**2])\n        \n        X1 = np.random.multivariate_normal(mu1, cov, n_per_class)\n        y1 = np.ones(n_per_class)\n        \n        X0 = np.random.multivariate_normal(mu0, cov, n_per_class)\n        y0 = np.zeros(n_per_class)\n        \n        X = np.vstack((X1, X0))\n        y = np.hstack((y1, y0))\n\n        # Clip data to [-1, 1]^2\n        X = np.clip(X, -1.0, 1.0)\n        \n        # Shuffle data\n        perm = np.random.permutation(2 * n_per_class)\n        return X[perm], y[perm]\n\n    test_cases = [\n        # Case 1: Masked g_phi\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.3, 0.3), 'mu0': (-0.3, -0.3), 'sigma': 0.6},\n            'attack_params': {'epsilon': 1.0, 'alpha': 0.2, 'T': 10},\n            'g_phi_type': 'masked',\n            'g_phi_preprocessor': np.sign,\n            'detection_params': {'tau': 0.1, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000},\n            'g_phi_adv_train_params': None,\n            'seed': 42\n        },\n        # Case 2: Adversarially trained g_phi\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.7, 0.7), 'mu0': (-0.7, -0.7), 'sigma': 0.3},\n            'attack_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 40},\n            'g_phi_type': 'adversarial',\n            'g_phi_preprocessor': None,\n            'detection_params': {'tau': 0.2, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000}, # For f_theta\n            'g_phi_adv_train_params': {'lr':0.1, 'epochs':50, 'adv_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 10}},\n            'seed': 43\n        },\n        # Case 3: Both models normal\n        {\n            'data_params': {'n_per_class': 200, 'mu1': (0.2, 0.2), 'mu0': (-0.2, -0.2), 'sigma': 0.5},\n            'attack_params': {'epsilon': 0.3, 'alpha': 0.05, 'T': 20},\n            'g_phi_type': 'normal',\n            'g_phi_preprocessor': None,\n            'detection_params': {'tau': 0.2, 'Delta': 0.2},\n            'train_params': {'lr': 0.01, 'epochs': 1000},\n            'g_phi_adv_train_params': None,\n            'seed': 44\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Data Generation\n        X, y = generate_data(**case['data_params'], seed=case['seed'])\n        \n        # 2. Train f_theta (always standard)\n        f_theta = LogisticRegression(model_type='normal')\n        f_theta.fit(X, y, **case['train_params'])\n\n        # 3. Train g_phi (type depends on case)\n        g_phi = LogisticRegression(model_type=case['g_phi_type'], preprocessor=case['g_phi_preprocessor'])\n        if g_phi.model_type == 'adversarial':\n            g_phi.fit(X, y, **case['g_phi_adv_train_params'])\n        else:\n            g_phi.fit(X, y, **case['train_params'])\n\n        # 4. Evaluation\n        y_pred_g_orig = g_phi.predict(X)\n\n        # 4a. White-box attack on g_phi\n        X_adv_g = g_phi._pga_attack(X, y, **case['attack_params'])\n        y_pred_g_whitebox = g_phi.predict(X_adv_g)\n        whitebox_rate = np.mean(y_pred_g_orig != y_pred_g_whitebox)\n        \n        # 4b. Transfer attack from f_theta to g_phi\n        X_adv_f = f_theta._pga_attack(X, y, **case['attack_params'])\n        y_pred_g_transfer = g_phi.predict(X_adv_f)\n        transfer_rate = np.mean(y_pred_g_orig != y_pred_g_transfer)\n\n        # 5. Detection Logic\n        tau = case['detection_params']['tau']\n        Delta = case['detection_params']['Delta']\n        is_masked = (whitebox_rate  tau) and (transfer_rate > whitebox_rate + Delta)\n        results.append(is_masked)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda b: str(b).capitalize(), results))}]\")\n\nsolve()\n```", "id": "3097091"}, {"introduction": "尽管经验性的攻击测试很有用，但它们无法提供绝对的安全性保证。这个高级实践将带你进入形式化验证（Formal Verification）的领域，在这里我们可以从数学上证明模型的鲁棒性。你将从零开始实现区间边界传播（Interval Bound Propagation, IBP），为神经网络计算一个“可验证半径”（Certified Radius），从而保证在该 $\\ell_\\infty$ 距离内的任何攻击都无法欺骗模型。[@problem_id:3098472]", "problem": "你的任务是实现区间界限传播 (IBP)，以计算前馈修正线性单元 (ReLU) 网络中每层的区间界限，并为给定样本的分类鲁棒性验证一个 $\\ell_\\infty$ 半径。区间界限传播 (IBP) 是一种技术，它将输入区间通过神经网络传播以获得输出区间，这可用于提供一个证书，证明在 $\\ell_\\infty$ 范数下有界的扰动下，预测的类别保持不变。推导必须从第一性原理开始。以下是核心定义和约束。\n\n网络是一个两层前馈神经网络，有一个隐藏层和 ReLU 非线性函数。设输入维度为 $d = 4$，隐藏层维度为 $h = 3$，输出类别数为 $K = 3$。第一层应用线性变换，然后是修正线性单元 (ReLU)；第二层是线性的。权重矩阵和偏置向量如下：\n- 第一层权重矩阵 $W_1 \\in \\mathbb{R}^{h \\times d}$:\n  $\\begin{bmatrix}\n  0.5  -0.4  0.3  0.1 \\\\\n  -0.2  0.6  0.1  -0.5 \\\\\n  0.3  0.2  -0.4  0.7\n  \\end{bmatrix}$，\n  和偏置向量 $b_1 \\in \\mathbb{R}^h$: $[0.1, -0.1, 0.2]$。\n- 第二层权重矩阵 $W_2 \\in \\mathbb{R}^{K \\times h}$:\n  $\\begin{bmatrix}\n  0.7  -0.3  0.2 \\\\\n  -0.5  0.8  0.1 \\\\\n  0.2  0.1  0.6\n  \\end{bmatrix}$，\n  和偏置向量 $b_2 \\in \\mathbb{R}^K$: $[0.05, -0.02, 0.01]$。\n\n基本基础和推导要求：\n- 向量 $x \\in \\mathbb{R}^d$ 的 $\\ell_\\infty$ 范数定义为 $\\|x\\|_\\infty = \\max_i |x_i|$。以点 $x_0$ 为中心、半径为 $\\varepsilon$ 的 $\\ell_\\infty$ 球是集合 $\\{ x \\in \\mathbb{R}^d \\mid \\|x - x_0\\|_\\infty \\le \\varepsilon \\}$。\n- 对于线性变换 $y = W x + b$ 和区间输入 $x \\in [\\ell, u]$（其中下界 $\\ell \\in \\mathbb{R}^d$ 和上界 $u \\in \\mathbb{R}^d$），输出区间 $y \\in [\\ell', u']$ 可以从第一性原理推导得出：对于每个分量 $y_i = \\sum_j W_{ij} x_j + b_i$，在区间上 $y_i$ 的最小值和最大值出现在每个 $x_j$ 的端点处。将矩阵分解为其非负部分 $W^+_{ij} = \\max(W_{ij}, 0)$ 和负部分 $W^-_{ij} = \\min(W_{ij}, 0)$，可得\n  $$\\ell' = W^+ \\ell + W^- u + b, \\quad u' = W^+ u + W^- \\ell + b.$$\n- 修正线性单元 (ReLU) 非线性函数 $\\operatorname{ReLU}(z)$ 是逐分量定义的，即 $\\operatorname{ReLU}(z_i) = \\max(0, z_i)$。给定预激活区间 $z \\in [\\ell_z, u_z]$，后激活区间为\n  $$\\ell_{\\text{ReLU}} = \\max(0, \\ell_z), \\quad u_{\\text{ReLU}} = \\max(0, u_z),$$\n  其中 max 函数是逐元素应用的。\n- 如果输入是以 $x_0$ 为中心、半径为 $\\varepsilon$ 的 $\\ell_\\infty$ 球，则输入区间为 $[\\ell_0, u_0]$，其中 $\\ell_0 = x_0 - \\varepsilon$ 且 $u_0 = x_0 + \\varepsilon$。\n\n验证标准：\n- 设网络输出的 logit 为 $f(x) \\in \\mathbb{R}^K$。对于一个样本 $x_0$，在中心点处定义预测类别为 $c = \\arg\\max_{k \\in \\{0, \\dots, K-1\\}} f_k(x_0)$。\n- 使用 IBP，将输入区间 $[\\ell_0, u_0]$ 通过网络传播，以获得 logit 的输出区间界限 $[\\ell_2, u_2]$。如果在半径 $\\varepsilon$ 处的鲁棒性证书能够获得，需要满足对于所有 $k \\ne c$，预测 logit 的下界超过所有其他 logit 的上界：\n  $$ \\ell_{2,c} - u_{2,k} > 0 \\quad \\text{对所有 } k \\ne c $$\n- 经过验证的 $\\ell_\\infty$ 半径是使得上述不等式在 IBP 下成立的最大 $\\varepsilon \\ge 0$。由于线性层和 ReLU 映射的 IBP 界限相对于 $\\varepsilon$ 是单调的（线性变换使区间随 $\\varepsilon$ 线性扩张，且 ReLU 是单调的），验证谓词关于 $\\varepsilon$ 是单调不增的，这使得使用二分搜索来计算最大 $\\varepsilon$ 值（在指定的数值公差内）是合适的。\n\n你的任务：\n- 为指定的网络实现 IBP。对于任何给定的 $\\varepsilon \\ge 0$，计算每层的区间界限：\n  1. 输入区间 $[\\ell_0, u_0] = [x_0 - \\varepsilon, x_0 + \\varepsilon]$，\n  2. 使用线性区间公式计算第一层线性预激活区间 $[\\ell_{1,\\text{pre}}, u_{1,\\text{pre}}]$，\n  3. 使用 ReLU 公式计算第一层后激活区间 $[\\ell_{1}, u_{1}]$，\n  4. 使用线性区间公式计算第二层线性输出区间 $[\\ell_2, u_2]$。\n- 实现一个函数，该函数给定 $x_0$，使用对 $\\varepsilon$ 的二分搜索计算经过验证的 $\\ell_\\infty$ 半径，终止公差为 $10^{-6}$，从 $\\varepsilon = 0$ 开始，并以几何级数方式扩大搜索上界，直到证书失效为止。\n\n测试套件：\n- 使用以下三个测试样本：\n  - $x^{(1)} = [\\,0.9,\\,-0.3,\\,0.2,\\,0.1\\,]$，\n  - $x^{(2)} = [\\,0.1,\\,0.2,\\,-0.1,\\,0.0\\,]$，\n  - $x^{(3)} = [\\,0.0,\\,0.0,\\,0.0,\\,0.0\\,]$。\n- 对每个测试样本，计算经过验证的 $\\ell_\\infty$ 半径，结果为实数。无物理单位。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个半径都四舍五入到小数点后六位。例如：$[r_1,r_2,r_3]$，其中 $r_i$ 是为 $x^{(i)}$ 验证的半径，四舍五入到六位小数。", "solution": "目标是针对给定的两层修正线性单元 (ReLU) 网络，在特定输入样本上计算经过验证的 $\\ell_\\infty$ 鲁棒性半径。使用的方法是区间界限传播 (IBP)，这是一种为给定样本 $x_0$ 的 $\\ell_\\infty$ 邻域内的所有输入计算网络输出 logit 界限的技术。\n\n该网络是一个前馈架构，由函数 $f(x) = z_2$ 定义，其中各层计算如下：\n1.  第一隐藏层预激活：$z_1 = W_1 x + b_1$\n2.  第一隐藏层后激活：$a_1 = \\operatorname{ReLU}(z_1)$\n3.  输出层 logit：$z_2 = W_2 a_1 + b_2$\n\n维度为输入维度 $d=4$，隐藏层维度 $h=3$，以及输出维度（类别数）$K=3$。网络参数给定如下：\n- 第一层权重矩阵 $W_1 \\in \\mathbb{R}^{3 \\times 4}$:\n$$W_1 = \\begin{bmatrix}\n0.5  -0.4  0.3  0.1 \\\\\n-0.2  0.6  0.1  -0.5 \\\\\n0.3  0.2  -0.4  0.7\n\\end{bmatrix}$$\n- 第一层偏置向量 $b_1 \\in \\mathbb{R}^3$:\n$$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.1 \\\\ 0.2 \\end{bmatrix}$$\n- 第二层权重矩阵 $W_2 \\in \\mathbb{R}^{3 \\times 3}$:\n$$W_2 = \\begin{bmatrix}\n0.7  -0.3  0.2 \\\\\n-0.5  0.8  0.1 \\\\\n0.2  0.1  0.6\n\\end{bmatrix}$$\n- 第二层偏置向量 $b_2 \\in \\mathbb{R}^3$:\n$$b_2 = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.01 \\end{bmatrix}$$\n\n输入扰动由围绕中心点 $x_0 \\in \\mathbb{R}^4$ 的半径为 $\\varepsilon \\ge 0$ 的 $\\ell_\\infty$ 球定义。这组受扰动的输入为 $\\{x \\in \\mathbb{R}^4 \\mid \\|x - x_0\\|_\\infty \\le \\varepsilon\\}$。这可以表示为一个区间，其中每个分量 $x_i$ 位于 $[x_{0,i} - \\varepsilon, x_{0,i} + \\varepsilon]$ 内。我们可以使用向量表示法将其紧凑地写为 $x \\in [\\ell_0, u_0]$，其中 $\\ell_0 = x_0 - \\varepsilon \\mathbf{1}$ 且 $u_0 = x_0 + \\varepsilon \\mathbf{1}$。\n\nIBP 过程逐层传播这些区间界限。\n\n**步骤 1：通过第一线性层的传播**\n第一隐藏层的预激活为 $z_1 = W_1 x + b_1$。为了找到 $z_1$ 的区间界限 $[\\ell_{1,\\text{pre}}, u_{1,\\text{pre}}]$，我们必须在 $x \\in [\\ell_0, u_0]$ 上找到每个分量 $z_{1,i} = \\sum_j (W_1)_{ij} x_j + (b_1)_i$ 的最小值和最大值。由于这是 $x_j$ 的线性函数，并且变量 $x_j$ 在其区间内是独立的，因此通过为每个 $x_j$ 选择 $\\ell_{0,j}$ 或 $u_{0,j}$ 来获得最小值和最大值。具体来说，为了最小化 $z_{1,i}$，如果 $(W_1)_{ij} \\ge 0$ 我们选择 $x_j = \\ell_{0,j}$，如果 $(W_1)_{ij}  0$ 我们选择 $x_j = u_{0,j}$。一种紧凑的写法是，将权重矩阵分解为其非负部分 $W_1^+$ 和负部分 $W_1^-$，其中 $(W_1^+)_{ij} = \\max((W_1)_{ij}, 0)$ 且 $(W_1^-)_{ij} = \\min((W_1)_{ij}, 0)$。\n那么预激活向量 $z_1$ 的下界和上界为：\n$$\\ell_{1,\\text{pre}} = W_1^+ \\ell_0 + W_1^- u_0 + b_1$$\n$$u_{1,\\text{pre}} = W_1^+ u_0 + W_1^- \\ell_0 + b_1$$\n\n**步骤 2：通过 ReLU 激活函数的传播**\n激活函数是修正线性单元，$a_1 = \\operatorname{ReLU}(z_1)$，逐元素应用。给定预激活区间 $z_1 \\in [\\ell_{1,\\text{pre}}, u_{1,\\text{pre}}]$，后激活区间 $[\\ell_1, u_1]$ 由 ReLU 函数的单调性决定。输出的下界是输入下界的 ReLU，输出的上界是输入上界的 ReLU。\n$$\\ell_1 = \\operatorname{ReLU}(\\ell_{1,\\text{pre}}) = \\max(0, \\ell_{1,\\text{pre}})$$\n$$u_1 = \\operatorname{ReLU}(u_{1,\\text{pre}}) = \\max(0, u_{1,\\text{pre}})$$\n其中 $\\max$ 是逐元素应用的。\n\n**步骤 3：通过第二线性层的传播**\n最终的输出 logit 为 $z_2 = W_2 a_1 + b_2$。传播规则与第一线性层相同，但现在应用于来自隐藏层的后激活区间 $[\\ell_1, u_1]$。我们定义 $W_2^+ = \\max(W_2, 0)$ 和 $W_2^- = \\min(W_2, 0)$。最终输出 logit 区间 $[\\ell_2, u_2]$ 为：\n$$\\ell_2 = W_2^+ \\ell_1 + W_2^- u_1 + b_2$$\n$$u_2 = W_2^+ u_1 + W_2^- \\ell_1 + b_2$$\n\n**鲁棒性验证标准**\n对于给定的样本 $x_0$ 和扰动半径 $\\varepsilon$，如果网络在中心点 $x_0$ 的预测类别对于 $\\ell_\\infty$ 球中的所有 $x$ 仍然是得分最高的类别，则其预测被鲁棒地验证。令 $c = \\arg\\max_k f_k(x_0)$ 为未受扰动输入的预测类别。使用 IBP 的验证条件是，类别 $c$ 的 logit 下界必须严格大于任何其他类别 $k \\neq c$ 的 logit 上界。\n$$ \\ell_{2,c} > u_{2,k} \\quad \\text{对所有 } k \\neq c $$\n\n**计算验证半径**\n目标是找到使此验证条件成立的最大 $\\varepsilon \\ge 0$。界限 $\\ell_2$ 和 $u_2$ 是 $\\varepsilon$ 的单调函数。具体来说，差值 $\\ell_{2,c} - u_{2,k}$ 是关于 $\\varepsilon$ 的单调不增函数。这一性质允许我们使用二分搜索来高效地找到满足所需数值公差的最大验证半径 $\\varepsilon^*$。\n\n算法如下：\n1.  对于给定的输入 $x_0$，首先确定预测类别 $c = \\arg\\max_k f_k(x_0)$。如果最大 logit 存在平局，则验证半径为 $0$，因为即使在 $\\varepsilon=0$ 时验证条件也失败了。\n2.  为二分搜索建立一个搜索范围 $[\\varepsilon_{\\text{low}}, \\varepsilon_{\\text{high}}]$。我们从 $\\varepsilon_{\\text{low}} = 0$ 开始。我们通过从一个小值（例如 $0.1$）开始，并以几何级数增加它（例如，加倍），直到验证检查失败，来找到一个初始的 $\\varepsilon_{\\text{high}}$。最后一个通过的值成为二分搜索范围的下界，第一个失败的值成为上界。\n3.  在范围 $[\\varepsilon_{\\text{low}}, \\varepsilon_{\\text{high}}]$ 内对 $\\varepsilon$ 执行二分搜索。在每一步中，我们测试中点 $\\varepsilon_{\\text{mid}} = (\\varepsilon_{\\text{low}} + \\varepsilon_{\\text{high}}) / 2$。\n    - 如果证书对 $\\varepsilon_{\\text{mid}}$ 成立，这意味着至少可以达到 $\\varepsilon_{\\text{mid}}$ 的半径，所以我们更新 $\\varepsilon_{\\text{low}} = \\varepsilon_{\\text{mid}}$。\n    - 如果证书失败，则半径过大，所以我们更新 $\\varepsilon_{\\text{high}} = \\varepsilon_{\\text{mid}}$。\n4.  当 $\\varepsilon_{\\text{high}} - \\varepsilon_{\\text{low}}$ 小于指定的公差（此处为 $10^{-6}$）时，搜索终止。最终的验证半径为 $\\varepsilon_{\\text{low}}$。\n\n将此过程应用于提供的三个测试样本中的每一个，以找到它们各自经过验证的 $\\ell_\\infty$ 半径。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to compute and print the certified radii\n    for the given test cases.\n    \"\"\"\n    # Define network parameters\n    W1 = np.array([\n        [0.5, -0.4, 0.3, 0.1],\n        [-0.2, 0.6, 0.1, -0.5],\n        [0.3, 0.2, -0.4, 0.7]\n    ], dtype=np.float64)\n\n    b1 = np.array([0.1, -0.1, 0.2], dtype=np.float64)\n\n    W2 = np.array([\n        [0.7, -0.3, 0.2],\n        [-0.5, 0.8, 0.1],\n        [0.2, 0.1, 0.6]\n    ], dtype=np.float64)\n\n    b2 = np.array([0.05, -0.02, 0.01], dtype=np.float64)\n\n    network_params = (W1, b1, W2, b2)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.9, -0.3, 0.2, 0.1], dtype=np.float64),\n        np.array([0.1, 0.2, -0.1, 0.0], dtype=np.float64),\n        np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float64),\n    ]\n\n    results = []\n    for x0 in test_cases:\n        radius = compute_certified_radius(x0, network_params)\n        results.append(f\"{radius:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef forward_pass(x, network_params):\n    \"\"\"Computes the network output (logits) for a single input point.\"\"\"\n    W1, b1, W2, b2 = network_params\n    z1 = W1 @ x + b1\n    a1 = np.maximum(0, z1)\n    z2 = W2 @ a1 + b2\n    return z2\n\ndef ibp_propagate(x0, epsilon, network_params):\n    \"\"\"\n    Performs Interval Bound Propagation for a given input x0 and radius epsilon.\n    Returns the final lower and upper bounds on the output logits.\n    \"\"\"\n    W1, b1, W2, b2 = network_params\n\n    # Input interval bounds\n    l0 = x0 - epsilon\n    u0 = x0 + epsilon\n\n    # Layer 1: Linear transformation\n    W1_pos = np.maximum(W1, 0)\n    W1_neg = np.minimum(W1, 0)\n    l1_pre = W1_pos @ l0 + W1_neg @ u0 + b1\n    u1_pre = W1_pos @ u0 + W1_neg @ l0 + b1\n\n    # Layer 1: ReLU activation\n    l1 = np.maximum(0, l1_pre)\n    u1 = np.maximum(0, u1_pre)\n\n    # Layer 2: Linear transformation\n    W2_pos = np.maximum(W2, 0)\n    W2_neg = np.minimum(W2, 0)\n    l2 = W2_pos @ l1 + W2_neg @ u1 + b2\n    u2 = W2_pos @ u1 + W2_neg @ l1 + b2\n\n    return l2, u2\n\ndef check_certificate(x0, epsilon, network_params, c):\n    \"\"\"\n    Checks the robustness certification criterion for a given epsilon.\n    \"\"\"\n    l2, u2 = ibp_propagate(x0, epsilon, network_params)\n    l2_c = l2[c]\n    \n    for k in range(len(l2)):\n        if k == c:\n            continue\n        if l2_c = u2[k]:\n            return False  # Certificate fails\n    \n    return True  # Certificate holds\n\ndef compute_certified_radius(x0, network_params):\n    \"\"\"\n    Computes the maximum certified L-infinity radius using binary search.\n    \"\"\"\n    tolerance = 1e-6\n\n    # Determine predicted class at the center point\n    logits_x0 = forward_pass(x0, network_params)\n    c = np.argmax(logits_x0)\n\n    # Check if robust at epsilon = 0. If not, radius is 0.\n    # This happens if there's a tie for the top logit.\n    if not check_certificate(x0, 0, network_params, c):\n        return 0.0\n\n    # Phase 1: Find an upper bound for the binary search by geometric expansion.\n    eps_low = 0.0\n    eps_high = 0.1  # Initial guess for the upper bound\n    while check_certificate(x0, eps_high, network_params, c):\n        eps_low = eps_high\n        eps_high *= 2.0\n        # Safety break for extremely robust cases to avoid long search\n        if eps_high > 100.0:\n            break\n\n    # Phase 2: Binary search in the interval [eps_low, eps_high].\n    while (eps_high - eps_low) > tolerance:\n        eps_mid = (eps_low + eps_high) / 2.0\n        if eps_mid == eps_low or eps_mid == eps_high: # Precision limit reached\n            break\n        if check_certificate(x0, eps_mid, network_params, c):\n            eps_low = eps_mid  # This radius is achievable, try for more.\n        else:\n            eps_high = eps_mid  # This radius is too large.\n\n    return eps_low\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3098472"}]}