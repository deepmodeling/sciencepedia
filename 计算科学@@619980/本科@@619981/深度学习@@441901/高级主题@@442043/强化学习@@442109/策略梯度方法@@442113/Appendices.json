{"hands_on_practices": [{"introduction": "任何策略梯度算法的核心都是“得分函数”，即 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)$。虽然理论上这个公式很简单，但在实际应用中常常会遇到各种约束，例如在特定状态下某些动作是不合法的。本练习 ([@problem_id:3158020]) 将让你亲手推导在包含“动作掩码”的实用场景下，一个 masked softmax 策略的得分函数，这对于在动态动作空间的环境中实现智能体至关重要。", "problem": "考虑强化学习（RL）中的一个离散动作策略，该策略使用动作掩码来禁止给定状态下的非法动作。设动作集为 $\\{1,2,3,4\\}$，掩码为 $m \\in \\{0,1\\}^{4}$，其中 $m_{i} = 1$ 表示动作 $i$ 是合法的，$m_{i} = 0$ 表示动作 $i$ 是非法的。该策略由一个 logits 向量 $\\theta \\in \\mathbb{R}^{4}$ 参数化，在状态 $s$ 和掩码 $m$ 下，分配给动作 $a$ 的概率定义为\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} ,\n$$\n这确保了非法动作的概率为零，并且仅对合法动作进行归一化。假设所选动作 $a$ 是合法的，因此 $\\pi(a \\mid s, m, \\theta) > 0$。\n\n从得分函数的定义和基本微分法则出发，推导在任意掩码 $m$ 下，对于一个合法的所选动作 $a$ 的梯度 $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$，并注意解释为何与非法动作对应的任何分量会消失。然后，在以下条件下评估此梯度：\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3 .\n$$\n你的最终答案必须是以单个行矩阵形式精确表示的解析形式梯度向量。不要近似；不需要四舍五入。不涉及物理单位。", "solution": "用户提供了一个问题陈述，在尝试解答之前需要进行验证。\n\n### 第 1 步：提取已知条件\n- 动作集：$\\{1, 2, 3, 4\\}$\n- 掩码：$m \\in \\{0, 1\\}^4$，其中 $m_i = 1$ 表示合法动作，$m_i=0$ 表示非法动作。\n- 策略参数 (logits)：$\\theta \\in \\mathbb{R}^4$。\n- 策略定义：$\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}$。\n- 条件：所选动作 $a$ 是合法的，这意味着 $m_a = 1$。\n- 任务 1：推导梯度 $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$。\n- 任务 2：针对以下具体情况评估梯度：\n  - $\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$\n  - $m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n  - $a = 3$\n\n### 第 2 步：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学基础：** 该问题牢固地植根于强化学习理论，特别是关于策略梯度方法。带掩码的 softmax 策略的构建及其得分函数（$\\nabla_{\\theta} \\ln \\pi$）的计算是该领域的标准和正确技术。\n- **良态的：** 问题定义清晰。它为策略提供了特定的函数形式，为评估提供了所有必要的参数，并有明确的目标。所选动作 $a$ 是合法的这一条件确保了策略概率 $\\pi(a | \\dots)$ 大于 0，因此其对数有明确定义。存在唯一、稳定且有意义的解。\n- **客观的：** 问题使用精确的数学语言陈述，没有任何主观性或歧义。\n\n该问题没有表现出任何科学上不健全、不完整、矛盾或含糊不清等缺陷。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。将提供解答。\n\n### 解题推导\n\n主要目标是推导对数策略的梯度，也称为得分函数。策略由以下公式给出：\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\n为了找到梯度 $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$，我们首先计算策略概率的对数。\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln \\left( \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\right)\n$$\n使用对数属性 $\\ln(x/y) = \\ln(x) - \\ln(y)$，我们得到：\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln(\\exp(\\theta_{a}) \\, m_{a}) - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\n问题陈述指明所选动作 $a$ 是合法的，这意味着 $m_a = 1$。这简化了第一项：$\\ln(\\exp(\\theta_{a}) \\cdot 1) = \\theta_a$。\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\n现在，我们计算此表达式关于参数向量 $\\theta$ 的梯度。梯度是一个向量，其第 $j$ 个分量是关于 $\\theta_j$ 的偏导数。\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\frac{\\partial}{\\partial \\theta_j} \\left( \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) \\right)\n$$\n我们逐项微分：\n$$\n\\frac{\\partial}{\\partial \\theta_j} (\\theta_a) = \\delta_{aj}\n$$\n其中 $\\delta_{aj}$ 是克罗内克 delta，如果 $j=a$ 则为 $1$，否则为 $0$。\n\n对于第二项，我们使用链式微分法则（$\\frac{d}{dx}\\ln(f(x)) = \\frac{f'(x)}{f(x)}$）：\n$$\n\\frac{\\partial}{\\partial \\theta_j} \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\frac{1}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\cdot \\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\n该和关于 $\\theta_j$ 的导数是：\n$$\n\\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\exp(\\theta_j) \\, m_j\n$$\n将这些结合起来，第二项的导数是：\n$$\n\\frac{\\exp(\\theta_j) \\, m_j}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\n这个表达式恰好是动作 $j$ 的策略概率 $\\pi(j \\mid s, m, \\theta)$ 的定义。\n\n所以，梯度的第 $j$ 个分量是：\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\delta_{aj} - \\pi(j \\mid s, m, \\theta)\n$$\n这是带掩码的 softmax 策略的得分函数的一般形式。\n\n问题要求解释为什么对应于非法动作的分量会消失。设 $k$ 是一个非法动作的索引，意味着 $m_k=0$。梯度的第 $k$ 个分量是：\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = \\delta_{ak} - \\pi(k \\mid s, m, \\theta)\n$$\n因为 $a$ 是一个合法动作（$m_a=1$）而 $k$ 是一个非法动作（$m_k=0$），我们必有 $a \\neq k$。因此，$\\delta_{ak} = 0$。\n非法动作 $k$ 的概率是：\n$$\n\\pi(k \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{k}) \\, m_{k}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = \\frac{\\exp(\\theta_{k}) \\cdot 0}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = 0\n$$\n因此，梯度的第 $k$ 个分量变为：\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = 0 - 0 = 0\n$$\n这证实了所有非法动作的梯度分量都为零。\n\n### 梯度评估\n\n我们给定以下值：\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3\n$$\n首先，我们计算归一化项，$Z = \\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}$：\n$$\nZ = \\exp(\\theta_1)m_1 + \\exp(\\theta_2)m_2 + \\exp(\\theta_3)m_3 + \\exp(\\theta_4)m_4\n$$\n$$\nZ = \\exp(0) \\cdot 1 + \\exp(1) \\cdot 0 + \\exp(2) \\cdot 1 + \\exp(-1) \\cdot 1\n$$\n$$\nZ = 1 \\cdot 1 + 0 + e^2 \\cdot 1 + e^{-1} \\cdot 1 = 1 + e^2 + e^{-1}\n$$\n接下来，我们计算概率向量 $\\vec{\\pi}$，其分量为 $\\pi(j \\mid s, m, \\theta)$：\n$$\n\\pi(1) = \\frac{\\exp(0) \\cdot 1}{Z} = \\frac{1}{Z}\n$$\n$$\n\\pi(2) = \\frac{\\exp(1) \\cdot 0}{Z} = 0\n$$\n$$\n\\pi(3) = \\frac{\\exp(2) \\cdot 1}{Z} = \\frac{e^2}{Z}\n$$\n$$\n\\pi(4) = \\frac{\\exp(-1) \\cdot 1}{Z} = \\frac{e^{-1}}{Z}\n$$\n梯度向量由 $\\nabla_{\\theta} \\ln \\pi(a=3 \\mid \\dots) = \\mathbf{e}_3 - \\vec{\\pi}$ 给出，其中 $\\mathbf{e}_3$ 是独热向量 $(0, 0, 1, 0)^T$。\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\pi(1) \\\\ \\pi(2) \\\\ \\pi(3) \\\\ \\pi(4) \\end{pmatrix} = \\begin{pmatrix} -\\pi(1) \\\\ -\\pi(2) \\\\ 1 - \\pi(3) \\\\ -\\pi(4) \\end{pmatrix}\n$$\n代入概率：\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ 1 - e^2/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (Z - e^2)/Z \\\\ -e^{-1}/Z \\end{pmatrix}\n$$\n代入 $Z = 1 + e^2 + e^{-1}$：\n$$\nZ - e^2 = (1 + e^2 + e^{-1}) - e^2 = 1 + e^{-1}\n$$\n所以梯度向量是：\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (1+e^{-1})/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\frac{1}{Z} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix}\n$$\n为了以更优美的形式表示，我们可以代入 $Z = 1 + e^2 + e^{-1}$ 并通过分子分母同乘以 $e$ 来消除负指数。\n$$\n\\frac{1}{Z} = \\frac{1}{1 + e^2 + e^{-1}} = \\frac{e}{e(1 + e^2 + e^{-1})} = \\frac{e}{e + e^3 + 1}\n$$\n现在将这个标量乘入向量：\n$$\n\\nabla_{\\theta} \\ln \\pi = \\frac{e}{1+e+e^3} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e(1+e^{-1}) \\\\ e(-e^{-1}) \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e+1 \\\\ -1 \\end{pmatrix}\n$$\n梯度向量的分量是：\n\\begin{itemize}\n    \\item 分量 1：$-\\frac{e}{1+e+e^3}$\n    \\item 分量 2：$0$\n    \\item 分量 3：$\\frac{e+1}{1+e+e^3}$\n    \\item 分量 4：$-\\frac{1}{1+e+e^3}$\n\\end{itemize}\n最终答案需以行矩阵形式提供。", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{e}{e^3+e+1} & 0 & \\frac{e+1}{e^3+e+1} & -\\frac{1}{e^3+e+1} \\end{pmatrix}}\n$$", "id": "3158020"}, {"introduction": "基础 REINFORCE 算法的一个主要缺点是其梯度估计具有很高的方差，这会使得学习过程缓慢且不稳定。因此，方差缩减技术是必不可少的。本练习 ([@problem_id:3158001]) 聚焦于对偶采样 (antithetic sampling)，一种简单而有效的方法，并要求你通过解析推导来量化其带来的收益，从而清晰地理解它为何以及如何工作。", "problem": "考虑一个一维随机老虎机问题，其动作 $a \\in \\mathbb{R}$ 从对称高斯策略 $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^{2})$ 中抽取。其中，方差 $\\sigma^{2} > 0$ 是固定的，只有均值 $\\theta \\in \\mathbb{R}$ 是可训练参数。奖励是动作的线性函数，$r(a) = k a$，其中 $k \\in \\mathbb{R}$ 是一个非零常数。目标函数为 $J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a)]$。请使用似然比（得分函数）恒等式 $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)]$ 作为基本出发点。\n\n为 $\\nabla_{\\theta} J(\\theta)$ 定义以下无偏梯度估计量：\n- 一个双样本独立同分布（i.i.d.）平均估计量：抽取 $a_{1}, a_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\pi_{\\theta}$，并设 $\\hat{g}_{\\text{iid}} = \\frac{1}{2}\\big[r(a_{1}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{1}) + r(a_{2}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{2})\\big]$。\n- 一个对偶配对平均估计量：抽取 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$，设 $a_{+} = \\theta + \\varepsilon$ 和 $a_{-} = \\theta - \\varepsilon$，并定义 $\\hat{g}_{\\text{anti}} = \\frac{1}{2}\\big[r(a_{+}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{+}) + r(a_{-}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{-})\\big]$。\n\n从固定方差的单变量高斯分布的 $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$ 定义以及高斯随机变量的基本矩恒等式出发，推导 $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$ 和 $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$ 的闭式表达式。然后，通过给出方差缩减因子\n$$V(\\theta, \\sigma) := \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]}$$\n的闭式解析表达式，来量化在相同的奖励评估预算下通过对偶采样实现的方差缩减。\n\n将最终结果表示为关于 $\\theta$ 和 $\\sigma$ 的单个简化表达式。无需进行数值取整。", "solution": "用户希望验证并解决一个关于策略梯度方法中方差缩减的问题。\n\n### 步骤 1：提取已知条件\n-   **策略**：$\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^{2})$，一个对称高斯策略，其均值 $\\theta \\in \\mathbb{R}$ 可训练，方差 $\\sigma^{2} > 0$ 固定。\n-   **动作**：$a \\in \\mathbb{R}$。\n-   **奖励函数**：$r(a) = k a$，其中 $k \\in \\mathbb{R}$ 是一个非零常数。\n-   **目标函数**：$J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a)]$。\n-   **梯度恒等式**：$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)]$。\n-   **独立同分布梯度估计量**：$\\hat{g}_{\\text{iid}} = \\frac{1}{2}\\big[r(a_{1}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{1}) + r(a_{2}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{2})\\big]$，其中 $a_{1}, a_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\pi_{\\theta}$。\n-   **对偶梯度估计量**：$\\hat{g}_{\\text{anti}} = \\frac{1}{2}\\big[r(a_{+}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{+}) + r(a_{-}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{-})\\big]$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$，$a_{+} = \\theta + \\varepsilon$，且 $a_{-} = \\theta - \\varepsilon$。\n-   **目标量**：方差缩减因子 $V(\\theta, \\sigma) := \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据、定义明确且客观。\n-   **科学依据**：该问题涉及强化学习和计算统计学中的标准概念：策略梯度方法、得分函数（或 REINFORCE）估计量，以及作为方差缩减技术的对偶采样。所有定义和原理都是标准且正确的。\n-   **定义明确性**：该问题提供了所有必要的定义，并要求推导特定的、明确定义的量（$\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$、$\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$ 及其比率）。整个设定是自洽的，在数学上是一致的，从而导向一个唯一的解。\n-   **客观性**：该问题使用形式化的数学语言陈述，没有任何主观或模棱两可的术语。\n\n### 步骤 3：结论与行动\n该问题是有效的。我将进行完整的推导。\n\n### 解题推导\n\n首先，我们确定一些初步的量。策略 $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^2)$ 的概率密度函数是\n$$p(a; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right).$$\n对数策略是 $\\ln \\pi_{\\theta}(a) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(a-\\theta)^2}{2\\sigma^2}$。得分函数是对数策略关于参数 $\\theta$ 的梯度，即\n$$ \\nabla_{\\theta} \\ln \\pi_{\\theta}(a) = \\frac{\\partial}{\\partial\\theta}\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right) = -\\frac{2(a-\\theta)(-1)}{2\\sigma^2} = \\frac{a-\\theta}{\\sigma^2}. $$\n让我们将单样本梯度估计定义为 $\\hat{g}(a) = r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$。使用给定的奖励和推导出的得分函数，我们有\n$$ \\hat{g}(a) = (ka) \\left(\\frac{a-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2} a(a-\\theta). $$\n真实梯度 $\\nabla_{\\theta} J(\\theta)$ 是该估计量的期望。设 $a \\sim \\mathcal{N}(\\theta, \\sigma^2)$。我们可以写作 $a = \\theta + z$，其中 $z \\sim \\mathcal{N}(0, \\sigma^2)$。\n$$ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}[\\hat{g}(a)] = \\mathbb{E}\\left[\\frac{k}{\\sigma^2} a(a-\\theta)\\right] = \\frac{k}{\\sigma^2}\\mathbb{E}[(\\theta+z)(z)] = \\frac{k}{\\sigma^2}\\mathbb{E}[\\theta z + z^2]. $$\n利用期望的线性性质，并注意到对于 $z \\sim \\mathcal{N}(0, \\sigma^2)$，我们有 $\\mathbb{E}[z]=0$ 和 $\\mathbb{E}[z^2]=\\sigma^2$，我们得到\n$$ \\nabla_{\\theta} J(\\theta) = \\frac{k}{\\sigma^2}(\\theta \\mathbb{E}[z] + \\mathbb{E}[z^2]) = \\frac{k}{\\sigma^2}(0 + \\sigma^2) = k. $$\n这证实了 $\\hat{g}(a)$ 是真实梯度 $k$ 的一个无偏估计量。\n\n现在，我们推导这两个估计量的方差。\n\n**1. 独立同分布估计量的方差, $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$**\n\n估计量 $\\hat{g}_{\\text{iid}}$ 是两个独立同分布随机变量 $\\hat{g}(a_1)$ 和 $\\hat{g}(a_2)$ 的平均值。由于独立性，方差为\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\operatorname{Var}\\left[\\frac{1}{2}(\\hat{g}(a_1) + \\hat{g}(a_2))\\right] = \\frac{1}{4}(\\operatorname{Var}[\\hat{g}(a_1)] + \\operatorname{Var}[\\hat{g}(a_2)]) = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)]. $$\n我们需要计算 $\\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2$。我们已知 $\\mathbb{E}[\\hat{g}(a)] = k$。\n让我们计算二阶矩 $\\mathbb{E}[\\hat{g}(a)^2]$：\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\mathbb{E}\\left[\\left(\\frac{k}{\\sigma^2} a(a-\\theta)\\right)^2\\right] = \\frac{k^2}{\\sigma^4} \\mathbb{E}[a^2(a-\\theta)^2]. $$\n代入 $a = \\theta + z$，其中 $z \\sim \\mathcal{N}(0, \\sigma^2)$：\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\mathbb{E}[(\\theta+z)^2 z^2] = \\mathbb{E}[(\\theta^2+2\\theta z+z^2)z^2] = \\mathbb{E}[\\theta^2 z^2 + 2\\theta z^3 + z^4]. $$\n使用中心高斯分布的矩，$\\mathbb{E}[z^2]=\\sigma^2$，$\\mathbb{E}[z^3]=0$（由对称性），以及 $\\mathbb{E}[z^4]=3\\sigma^4$：\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\theta^2\\mathbb{E}[z^2] + 2\\theta\\mathbb{E}[z^3] + \\mathbb{E}[z^4] = \\theta^2\\sigma^2 + 0 + 3\\sigma^4 = \\sigma^2(\\theta^2 + 3\\sigma^2). $$\n因此，$\\hat{g}(a)$ 的二阶矩是\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\frac{k^2}{\\sigma^4}\\sigma^2(\\theta^2 + 3\\sigma^2) = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right). $$\n那么，单样本估计量的方差是\n$$ \\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right) - k^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\n最后，双样本独立同分布估计量的方差是\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)] = \\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\n\n**2. 对偶估计量的方差, $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$**\n\n对偶估计量使用一对相关的样本。让我们首先用随机变量 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 来表示该估计量。\n该估计量的两个分量是 $\\hat{g}(a_{+})$ 和 $\\hat{g}(a_{-})$。\n对于 $a_{+} = \\theta + \\varepsilon$：\n$$ \\hat{g}(a_{+}) = r(a_{+}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{+}) = k(\\theta+\\varepsilon) \\left(\\frac{(\\theta+\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta+\\varepsilon)\\varepsilon = \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2). $$\n对于 $a_{-} = \\theta - \\varepsilon$：\n$$ \\hat{g}(a_{-}) = r(a_{-}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{-}) = k(\\theta-\\varepsilon) \\left(\\frac{(\\theta-\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta-\\varepsilon)(-\\varepsilon) = \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2). $$\n对偶估计量是这两个分量的平均值：\n$$ \\hat{g}_{\\text{anti}} = \\frac{1}{2}(\\hat{g}(a_{+}) + \\hat{g}(a_{-})) = \\frac{1}{2} \\left[ \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2) + \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2) \\right] = \\frac{1}{2} \\frac{k}{\\sigma^2} (2\\varepsilon^2) = \\frac{k}{\\sigma^2}\\varepsilon^2. $$\n依赖于 $\\theta$ 的项被消掉了。现在我们可以计算这个简化表达式的方差。\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\operatorname{Var}\\left[\\frac{k}{\\sigma^2}\\varepsilon^2\\right] = \\left(\\frac{k}{\\sigma^2}\\right)^2 \\operatorname{Var}[\\varepsilon^2] = \\frac{k^2}{\\sigma^4}\\operatorname{Var}[\\varepsilon^2]. $$\n$\\varepsilon^2$ 的方差是 $\\operatorname{Var}[\\varepsilon^2] = \\mathbb{E}[(\\varepsilon^2)^2] - (\\mathbb{E}[\\varepsilon^2])^2 = \\mathbb{E}[\\varepsilon^4] - (\\mathbb{E}[\\varepsilon^2])^2$。\n使用 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 的矩，$\\mathbb{E}[\\varepsilon^2] = \\sigma^2$ 和 $\\mathbb{E}[\\varepsilon^4] = 3\\sigma^4$，我们有\n$$ \\operatorname{Var}[\\varepsilon^2] = 3\\sigma^4 - (\\sigma^2)^2 = 2\\sigma^4. $$\n将此代回，我们得到对偶估计量的方差：\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\frac{k^2}{\\sigma^4}(2\\sigma^4) = 2k^2. $$\n\n**3. 方差缩减因子, $V(\\theta, \\sigma)$**\n\n最后，我们计算这两个方差的比率。\n$$ V(\\theta, \\sigma) = \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]} = \\frac{\\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2k^2}. $$\n由于 $k \\neq 0$，我们可以消去 $k^2$ 项：\n$$ V(\\theta, \\sigma) = \\frac{\\frac{1}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2} = \\frac{1}{4}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right) = \\frac{\\theta^2}{4\\sigma^2} + \\frac{2}{4} = \\frac{\\theta^2}{4\\sigma^2} + \\frac{1}{2}. $$\n为了将其写成一个单一的简化表达式，我们可以使用一个公分母：\n$$ V(\\theta, \\sigma) = \\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}. $$\n这就是方差缩减因子的闭式解析表达式。", "answer": "$$\\boxed{\\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}}$$", "id": "3158001"}, {"introduction": "从简单的单步动作选择，我们现在转向更复杂的组合动作空间，这在推荐系统或网络搜索等应用中很常见。这个高级练习 ([@problem_id:3158005]) 将挑战你应用策略梯度解决“板岩优化”(slate optimization) 问题，即智能体必须选择一个有序的物品集合。你将为一个 Plackett-Luce 策略模型推导出一个完整的策略梯度估计器，其中包含一个用于降低方差的基线，从而展示这些方法的强大功能和灵活性。", "problem": "考虑一个强化学习（RL）中的单状态分幕式问题，其中智能体必须从一个包含 $n$ 个物品（索引为 $\\{1,\\dots,n\\}$）的基础集中无放回地选择一个大小为 $k$ 的板块。设板块为一个顺序生成的有序序列 $\\mathbf{i}=(i_{1},\\dots,i_{k})$。在步骤 $t$，可用集为 $A_{t}$，定义为 $A_{1}=\\{1,\\dots,n\\}$ 且对于 $t=1,\\dots,k-1$，$A_{t+1}=A_{t}\\setminus\\{i_{t}\\}$。策略是一个 Plackett–Luce 模型：在步骤 $t$，选择物品 $i\\in A_{t}$ 的概率为\n$$\np_{\\theta}(i\\mid A_{t})=\\frac{\\exp\\big(s_{i}\\big)}{\\sum_{j\\in A_{t}}\\exp\\big(s_{j}\\big)},\n$$\n其中使用线性得分 $s_{i}=\\theta^{\\top}x_{i}$，$x_{i}\\in\\mathbb{R}^{d}$ 是一个已知的特征向量，$\\theta\\in\\mathbb{R}^{d}$ 是待学习的参数。一个有序板块 $\\mathbf{i}$ 的联合概率是各步骤概率的乘积。分幕式奖励是所选物品的奖励之和：\n$$\nR(\\mathbf{i})=\\sum_{t=1}^{k}r_{i_{t}},\n$$\n其中 $r_{i}$ 是与物品 $i$ 相关联的已知实值效用。学习目标是最大化期望回报\n$$\nJ(\\theta)=\\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}}\\big[R(\\mathbf{i})\\big],\n$$\n其中 $\\pi_{\\theta}$ 表示由顺序 Plackett–Luce 策略引出的板块分布。\n\n任务：\n- 从 $J(\\theta)$ 作为基于板块的期望的定义出发，仅使用关于对依赖于参数的分布的期望进行微分的基本原理，推导一个无偏的蒙特卡洛策略梯度估计器。该估计器可以包含每步基线 $b_{t}(A_{t})$，这些基线可以依赖于 $A_{t}$ 但不依赖于采样的物品 $i_{t}$。清晰地证明为什么这样的基线不会引入偏差。\n- 将通用表达式特化到给定的具有线性得分和可加奖励的 Plackett–Luce 策略。提出一个低方差的 $b_{t}(A_{t})$ 选择，该选择利用了奖励在各个步骤间的逐项分解，并将估计器完全简化为用 $\\{x_{i}\\}$、$\\{r_{i}\\}$ 和在 $A_{t}$ 上的 Plackett–Luce 概率表示的形式。\n- 提供 $\\nabla_{\\theta}J(\\theta)$ 的最终单板块估计器表达式，以闭式形式表示。你的最终答案必须是一个单一的解析表达式；不要将其呈现为方程。\n\n无需进行数值舍入。你的最终答案中不应包含单位。", "solution": "### 第1步：推导通用策略梯度估计器\n\n目标函数是 $J(\\theta) = \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}}[R(\\mathbf{i})]$。使用对数导数技巧，梯度为：\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}} \\left[ R(\\mathbf{i}) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{i}) \\right]\n$$\n板块的对数概率可以分解为各步骤对数概率之和：$\\log \\pi_{\\theta}(\\mathbf{i}) = \\sum_{t=1}^{k} \\log p_{\\theta}(i_{t} \\mid A_{t})$。\n因此，其梯度为 $\\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{i}) = \\sum_{t=1}^{k} \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t})$。\n将此代入梯度表达式中：\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}} \\left[ \\left(\\sum_{t=1}^{k} r_{i_{t}}\\right) \\left(\\sum_{t=1}^{k} \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t})\\right) \\right]\n$$\n通过利用因果性，即步骤 $t$ 的决策仅影响未来的奖励，我们可以将 $R(\\mathbf{i})$ 替换为从时间 $t$ 开始的“未来奖励总和” (reward-to-go) $\\sum_{t'=t}^{k} r_{i_{t'}}$：\n$$\n\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\mathbf{i}\\sim\\pi_{\\theta}} \\left[ \\sum_{t=1}^{k} \\left(\\sum_{t'=t}^{k} r_{i_{t'}}\\right) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right]\n$$\n现在，我们可以为每个步骤引入一个不依赖于所选动作 $i_t$ 的基线 $b_t(A_t)$。我们从未来奖励总和中减去它：\n$$\n\\hat{g}(\\mathbf{i}) = \\sum_{t=1}^{k} \\left( \\left(\\sum_{t'=t}^{k} r_{i_{t'}}\\right) - b_t(A_t) \\right) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t})\n$$\n要证明这个估计器是无偏的，我们需要证明减去基线不会改变梯度的期望值。我们只需证明 $\\mathbb{E}[b_t(A_t) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t})] = 0$。根据期望的法则，我们可以写出：\n$$\n\\mathbb{E}[b_t(A_t) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t})] = \\mathbb{E}_{\\mathbf{i}_{<t}} \\left[ \\mathbb{E}_{i_t \\sim p_{\\theta}(\\cdot|A_t)} \\left[ b_t(A_t) \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\mid A_t \\right] \\right]\n$$\n因为 $b_t(A_t)$ 对于给定 $A_t$ 是常数，我们可以将其移出内部期望：\n$$\n= \\mathbb{E}_{\\mathbf{i}_{<t}} \\left[ b_t(A_t) \\mathbb{E}_{i_t \\sim p_{\\theta}(\\cdot|A_t)} \\left[ \\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t}) \\right] \\right]\n$$\n而 $\\mathbb{E}_{i_t \\sim p_{\\theta}(\\cdot|A_t)}[\\nabla_{\\theta} \\log p_{\\theta}(i_{t} \\mid A_{t})]$ 是一个恒为零的量（得分函数的期望为零），因此整个表达式为零。这就证明了引入基线不会引入偏差。\n\n### 第2步：特化到 Plackett-Luce 并选择基线\n\n首先，我们推导 Plackett-Luce 策略的得分函数 $\\nabla_{\\theta} \\log p_{\\theta}(i_t \\mid A_t)$。\n$$\n\\log p_{\\theta}(i_t \\mid A_t) = \\log(\\exp(s_{i_t})) - \\log\\left(\\sum_{j \\in A_t} \\exp(s_j)\\right) = \\theta^\\top x_{i_t} - \\log\\left(\\sum_{j \\in A_t} \\exp(\\theta^\\top x_j)\\right)\n$$\n对 $\\theta$ 求导：\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(i_t \\mid A_t) = x_{i_t} - \\frac{\\sum_{j \\in A_t} \\exp(\\theta^\\top x_j) x_j}{\\sum_{l \\in A_t} \\exp(\\theta^\\top x_l)} = x_{i_t} - \\mathbb{E}_{j \\sim p_{\\theta}(\\cdot|A_t)}[x_j]\n$$\n接下来，我们需要选择一个基线 $b_t(A_t)$。一个有效且易于计算的选择是利用奖励的可加性，并使用在可用集 $A_t$ 上的**单步期望奖励**作为基线。这比使用整个未来奖励的期望更容易，但仍然能显著降低方差。\n$$\nb_t(A_t) = \\mathbb{E}_{i \\sim p_{\\theta}(\\cdot|A_t)}[r_i] = \\sum_{j \\in A_t} r_j p_{\\theta}(j|A_t) = \\frac{\\sum_{j \\in A_t} r_j \\exp(\\theta^\\top x_j)}{\\sum_{l \\in A_t} \\exp(\\theta^\\top x_l)}\n$$\n将所有部分组合在一起，我们得到单板块梯度估计器。\n\n### 第3步：最终估计器表达式\n\n将上述推导的得分函数和基线代入通用估计器表达式中，得到最终的单板块梯度估计器：\n$$\n\\sum_{t=1}^{k} \\left( \\left( \\sum_{t'=t}^{k} r_{i_{t'}} \\right) - \\frac{\\sum_{j \\in A_t} r_j \\exp(\\theta^\\top x_j)}{\\sum_{l \\in A_t} \\exp(\\theta^\\top x_l)} \\right) \\left( x_{i_t} - \\frac{\\sum_{j \\in A_t} x_j \\exp(\\theta^\\top x_j)}{\\sum_{l \\in A_t} \\exp(\\theta^\\top x_l)} \\right)\n$$", "answer": "$$\\boxed{\\sum_{t=1}^{k} \\left( \\left( \\sum_{t'=t}^{k} r_{i_{t'}} \\right) - \\frac{\\sum_{j \\in A_t} r_j \\exp(\\theta^\\top x_j)}{\\sum_{l \\in A_t} \\exp(\\theta^\\top x_l)} \\right) \\left( x_{i_t} - \\frac{\\sum_{j \\in A_t} x_j \\exp(\\theta^\\top x_j)}{\\sum_{l \\in A_t} \\exp(\\theta^\\top x_l)} \\right)}$$", "id": "3158005"}]}