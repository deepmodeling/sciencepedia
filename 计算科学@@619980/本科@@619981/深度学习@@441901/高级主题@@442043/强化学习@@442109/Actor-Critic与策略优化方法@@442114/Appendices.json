{"hands_on_practices": [{"introduction": "策略梯度方法功能强大，但常因高方差问题而效率低下。一个常见的解决方案是引入一个基线（baseline）来降低方差，但这并非对所有基线都有效。本练习将引导你通过数学推导和实践来证明，为保证梯度估计的无偏性，基线不能依赖于所采取的行动，这是设计有效行动者-评论家（actor-critic）算法的一条关键原则。[@problem_id:3094783]", "problem": "考虑在行动者-评论家（Actor-Critic, AC）和策略优化方法的背景下，一个单状态、双动作的强化学习（Reinforcement Learning, RL）赌博机问题。设策略由一个实数标量参数 $ \\theta \\in \\mathbb{R} $ 参数化，并定义为在动作 $ a \\in \\{0,1\\} $ 上的伯努利分布，其概率为\n$$\n\\pi_\\theta(1 \\mid s) = \\sigma(\\theta) \\quad \\text{and} \\quad \\pi_\\theta(0 \\mid s) = 1 - \\sigma(\\theta),\n$$\n其中 $ \\sigma(\\theta) = \\frac{1}{1 + e^{-\\theta}} $ 是逻辑 sigmoid 函数。设目标为期望奖励\n$$\nJ(\\theta) = \\mathbb{E}_{a \\sim \\pi_\\theta(\\cdot \\mid s)} \\left[ r(a) \\right],\n$$\n每个测试用例提供与动作相关的奖励 $ r(1) $ 和 $ r(0) $。\n\n行动者使用得分函数（对数似然技巧）来估计梯度。基线用于减小方差。然而，为了保持无偏性，基线不能与得分函数产生相关性。在这个问题中，您将通过构建和评估 $ b(s,a) $ 与 $ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) $ 相关的反例，来证明使用依赖于状态-动作的基线 $ b(s,a) $ 会如何破坏无偏性。您需要为每个测试用例计算估计器的偏差，该偏差定义为减去基线的估计器的期望值与真实策略梯度之间的差值。\n\n定义：\n- 得分函数是 $ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) $。\n- 真实梯度由标准策略梯度估计器的期望定义：\n$$\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{a \\sim \\pi_\\theta(\\cdot \\mid s)} \\left[ r(a) \\, \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\right].\n$$\n- 带基线的估计器是\n$$\n\\mathbb{E}_{a \\sim \\pi_\\theta(\\cdot \\mid s)} \\left[ \\left( r(a) - b(s,a) \\right) \\, \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\right].\n$$\n- 需要报告的偏差是以下差值\n$$\n\\text{bias}(\\theta) = \\mathbb{E}_{a \\sim \\pi_\\theta(\\cdot \\mid s)} \\left[ \\left( r(a) - b(s,a) \\right) \\, \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\right] - \\nabla_\\theta J(\\theta).\n$$\n\n使用精确期望，而非采样。所有量都必须是无量纲的实数。不涉及角度。不涉及百分比。\n\n测试套件：\n对于每个测试用例，计算并报告标量偏差（浮点数）。程序必须按顺序处理以下六个测试用例：\n\n- 测试 $1$（理想情况，零基线）：\n  - $ \\theta = 0.0 $\n  - $ r(1) = 1.0 $，$ r(0) = 0.0 $\n  - 对于两个动作，$ b(s,a) = 0 $。\n\n- 测试 $2$（仅依赖状态的恒定基线，无偏）：\n  - $ \\theta = -0.7 $\n  - $ r(1) = 2.0 $，$ r(0) = -1.0 $\n  - $ b(s) = 0.3 $ 是常数且与动作无关，即对于两个动作，$ b(s,a) = 0.3 $。\n\n- 测试 $3$（反例：基线与得分函数成正比，相关且有偏）：\n  - $ \\theta = 0.4 $\n  - $ r(1) = 1.0 $，$ r(0) = 0.0 $\n  - $ b(s,a) = \\alpha \\, \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) $，其中 $ \\alpha = 0.5 $。\n\n- 测试 $4$（反例：基线等于奖励，相关且有偏）：\n  - $ \\theta = -1.2 $\n  - $ r(1) = 3.0 $，$ r(0) = 0.5 $\n  - $ b(s,a) = r(a) $。\n\n- 测试 $5$（反例：独热动作基线，相关且有偏）：\n  - $ \\theta = 2.0 $\n  - $ r(1) = 4.0 $，$ r(0) = -2.0 $\n  - $ b(s,a) = c \\cdot \\mathbf{1}\\{ a = 1 \\} $，其中 $ c = 1.5 $，并且当 $ a = 0 $ 时 $ b(s,a) = 0 $。\n\n- 测试 $6$（仅依赖状态的函数基线，尽管依赖于 $ \\theta $ 但仍无偏）：\n  - $ \\theta = 0.0 $\n  - $ r(1) = 5.0 $，$ r(0) = 5.0 $\n  - $ b(s) = \\theta^2 + 1 $，即对于两个动作，$ b(s,a) = \\theta^2 + 1 $。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按测试用例顺序排列的六个计算出的偏差，形式为逗号分隔的列表，并用方括号括起来，每个浮点数四舍五入到六位小数，例如 $ [x_1,x_2,x_3,x_4,x_5,x_6] $，其中每个 $ x_i $ 是一个有六位小数的浮点数。不应打印任何其他文本。", "solution": "该问题要求计算在单状态、双动作的赌博机问题中，一个依赖于状态-动作的基线在策略梯度估计器中引入的偏差。估计器的偏差定义为减去基线的估计器的期望值与真实策略梯度之间的差值。\n\n让我们首先将问题的各个组成部分形式化。\n对于动作 $a \\in \\{0, 1\\}$，策略 $\\pi_\\theta$ 由以下公式给出：\n$$ \\pi_\\theta(1 \\mid s) = \\sigma(\\theta) = \\frac{1}{1 + e^{-\\theta}} $$\n$$ \\pi_\\theta(0 \\mid s) = 1 - \\sigma(\\theta) $$\n由于问题是单状态的，我们可以省略对 $s$ 的条件，写作 $\\pi_\\theta(a)$。\n\n偏差定义为：\n$$ \\text{bias}(\\theta) = \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ (r(a) - b(a)) \\nabla_\\theta \\log \\pi_\\theta(a) \\right] - \\nabla_\\theta J(\\theta) $$\n其中 $b(a)$ 是基线，$\\nabla_\\theta J(\\theta)$ 是真实策略梯度，由策略梯度定理定义为：\n$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ r(a) \\nabla_\\theta \\log \\pi_\\theta(a) \\right] $$\n将真实梯度的定义代入偏差方程，并利用期望的线性性质，我们得到：\n$$ \\text{bias}(\\theta) = \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ r(a) \\nabla_\\theta \\log \\pi_\\theta(a) \\right] - \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ b(a) \\nabla_\\theta \\log \\pi_\\theta(a) \\right] - \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ r(a) \\nabla_\\theta \\log \\pi_\\theta(a) \\right] $$\n涉及奖励 $r(a)$ 的项相互抵消，从而得到一个仅依赖于基线 $b(a)$ 和策略 $\\pi_\\theta$ 的偏差简化表达式：\n$$ \\text{bias}(\\theta) = - \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ b(a) \\nabla_\\theta \\log \\pi_\\theta(a) \\right] $$\n该方程是我们分析的核心。它表明，任何偏差都完全是由基线与得分函数 $\\nabla_\\theta \\log \\pi_\\theta(a)$ 之间的相关性引起的。\n\n接下来，我们推导得分函数。sigmoid 函数的导数是 $\\frac{d}{d\\theta}\\sigma(\\theta) = \\sigma(\\theta)(1 - \\sigma(\\theta))$。\n对于动作 $a=1$：\n$$ \\nabla_\\theta \\log \\pi_\\theta(1) = \\nabla_\\theta \\log \\sigma(\\theta) = \\frac{1}{\\sigma(\\theta)} \\frac{d\\sigma(\\theta)}{d\\theta} = \\frac{\\sigma(\\theta)(1-\\sigma(\\theta))}{\\sigma(\\theta)} = 1 - \\sigma(\\theta) $$\n对于动作 $a=0$：\n$$ \\nabla_\\theta \\log \\pi_\\theta(0) = \\nabla_\\theta \\log (1 - \\sigma(\\theta)) = \\frac{1}{1 - \\sigma(\\theta)} \\left(-\\frac{d\\sigma(\\theta)}{d\\theta}\\right) = \\frac{-\\sigma(\\theta)(1-\\sigma(\\theta))}{1-\\sigma(\\theta)} = -\\sigma(\\theta) $$\n\n现在我们可以展开偏差的期望：\n$$ \\mathbb{E}_{a \\sim \\pi_\\theta} [b(a) \\nabla_\\theta \\log \\pi_\\theta(a)] = \\sum_{a \\in \\{0,1\\}} \\pi_\\theta(a) \\cdot b(a) \\cdot \\nabla_\\theta \\log \\pi_\\theta(a) $$\n$$ = \\pi_\\theta(1) \\cdot b(1) \\cdot \\nabla_\\theta \\log \\pi_\\theta(1) + \\pi_\\theta(0) \\cdot b(0) \\cdot \\nabla_\\theta \\log \\pi_\\theta(0) $$\n代入策略和得分函数的表达式：\n$$ = \\sigma(\\theta) \\cdot b(1) \\cdot (1 - \\sigma(\\theta)) + (1 - \\sigma(\\theta)) \\cdot b(0) \\cdot (-\\sigma(\\theta)) $$\n提取公因式 $\\sigma(\\theta)(1 - \\sigma(\\theta))$：\n$$ = \\sigma(\\theta)(1 - \\sigma(\\theta)) [b(1) - b(0)] $$\n因此，偏差的最终表达式为：\n$$ \\text{bias}(\\theta) = - \\sigma(\\theta)(1 - \\sigma(\\theta)) [b(1) - b(0)] $$\n这个简洁的结果表明，当且仅当基线 $b(a)$ 与动作 $a$ 无关（即 $b(1) = b(0)$），或者策略是确定性的（$\\sigma(\\theta) \\in \\{0, 1\\}$）时，偏差为零。这证实了无偏基线的标准要求：它不能是动作的函数。\n\n现在我们将此公式应用于每个测试用例。\n\n测试 $1$：$\\theta = 0.0$, $b(1) = 0$, $b(0) = 0$。\n$b(1) - b(0) = 0 - 0 = 0$。\n$\\text{bias} = - \\sigma(0.0)(1 - \\sigma(0.0)) [0] = 0.0$。\n\n测试 $2$：$\\theta = -0.7$, $b(1) = 0.3$, $b(0) = 0.3$。\n$b(1) - b(0) = 0.3 - 0.3 = 0$。\n$\\text{bias} = - \\sigma(-0.7)(1 - \\sigma(-0.7)) [0] = 0.0$。\n\n测试 $3$：$\\theta = 0.4$, $b(a) = \\alpha \\nabla_\\theta \\log \\pi_\\theta(a)$，其中 $\\alpha = 0.5$。\n$b(1) = 0.5 \\cdot (1 - \\sigma(0.4))$。\n$b(0) = 0.5 \\cdot (-\\sigma(0.4))$。\n$b(1) - b(0) = 0.5(1 - \\sigma(0.4)) - 0.5(-\\sigma(0.4)) = 0.5$。\n$\\text{bias} = - \\sigma(0.4)(1 - \\sigma(0.4)) [0.5] \\approx - (0.598688)(0.401312) [0.5] \\approx -0.120150$。\n\n测试 $4$：$\\theta = -1.2$, $b(a) = r(a)$，其中 $r(1) = 3.0, r(0) = 0.5$。\n$b(1) = 3.0$, $b(0) = 0.5$。\n$b(1) - b(0) = 3.0 - 0.5 = 2.5$。\n$\\text{bias} = - \\sigma(-1.2)(1 - \\sigma(-1.2)) [2.5] \\approx - (0.231475)(0.768525) [2.5] \\approx -0.444815$。\n\n测试 $5$：$\\theta = 2.0$, $b(1) = 1.5$, $b(0) = 0$。\n$b(1) - b(0) = 1.5 - 0 = 1.5$。\n$\\text{bias} = - \\sigma(2.0)(1 - \\sigma(2.0)) [1.5] \\approx - (0.880797)(0.119203) [1.5] \\approx -0.157502$。\n\n测试 $6$：$\\theta = 0.0$, $b(a) = \\theta^2 + 1$。\n$b(1) = 0.0^2 + 1 = 1$。\n$b(0) = 0.0^2 + 1 = 1$。\n$b(1) - b(0) = 1 - 1 = 0$。\n$\\text{bias} = - \\sigma(0.0)(1 - \\sigma(0.0)) [0] = 0.0$。\n\n实现将通过计算这些值来进行。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a policy gradient estimator with a state-action\n    dependent baseline for a series of test cases.\n    \"\"\"\n\n    def sigmoid(theta: float) -> float:\n        \"\"\"Computes the logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-theta))\n\n    def calculate_bias(\n        theta: float, b1: float, b0: float\n    ) -> float:\n        \"\"\"\n        Calculates the bias using the derived formula:\n        bias = -sigma(theta)*(1-sigma(theta))*(b(1) - b(0))\n        \"\"\"\n        s_theta = sigmoid(theta)\n        policy_variance = s_theta * (1.0 - s_theta)\n        baseline_diff = b1 - b0\n        \n        return -policy_variance * baseline_diff\n\n    # The problem defines test cases with parameters theta, rewards, and baselines.\n    # The rewards r(a) are only relevant for baselines that depend on them (Test 4).\n    test_cases = [\n        {'id': 1, 'theta': 0.0, 'r1': 1.0, 'r0': 0.0, 'b_type': 'zero'},\n        {'id': 2, 'theta': -0.7, 'r1': 2.0, 'r0': -1.0, 'b_type': 'constant', 'b_val': 0.3},\n        {'id': 3, 'theta': 0.4, 'r1': 1.0, 'r0': 0.0, 'b_type': 'score_prop', 'alpha': 0.5},\n        {'id': 4, 'theta': -1.2, 'r1': 3.0, 'r0': 0.5, 'b_type': 'reward_eq'},\n        {'id': 5, 'theta': 2.0, 'r1': 4.0, 'r0': -2.0, 'b_type': 'one_hot', 'c': 1.5},\n        {'id': 6, 'theta': 0.0, 'r1': 5.0, 'r0': 5.0, 'b_type': 'theta_func'},\n    ]\n\n    results = []\n    for case in test_cases:\n        theta = case['theta']\n        b1, b0 = 0.0, 0.0  # Default baseline values\n\n        if case['b_type'] == 'zero':\n            b1, b0 = 0.0, 0.0\n        \n        elif case['b_type'] == 'constant':\n            b1 = case['b_val']\n            b0 = case['b_val']\n\n        elif case['b_type'] == 'score_prop':\n            # b(a) = alpha * grad_log_pi(a)\n            # grad_log_pi(1) = 1 - sigma(theta)\n            # grad_log_pi(0) = -sigma(theta)\n            alpha = case['alpha']\n            s_theta = sigmoid(theta)\n            b1 = alpha * (1.0 - s_theta)\n            b0 = alpha * (-s_theta)\n\n        elif case['b_type'] == 'reward_eq':\n            # b(a) = r(a)\n            b1 = case['r1']\n            b0 = case['r0']\n        \n        elif case['b_type'] == 'one_hot':\n            # b(a) = c * 1{a=1}\n            b1 = case['c']\n            b0 = 0.0\n            \n        elif case['b_type'] == 'theta_func':\n            # b(a) = theta^2 + 1\n            val = theta**2 + 1.0\n            b1 = val\n            b0 = val\n\n        bias = calculate_bias(theta, b1, b0)\n        results.append(f\"{bias:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3094783"}, {"introduction": "在行动者-评论家方法中，评论家的任务是提供准确的价值估计以指导行动者。当环境具有复杂的动态特性（例如动作延迟）时，这项任务变得极具挑战性。通过这个动手模拟练习，你将探索在存在延迟的情况下如何构建正确的评论家目标，并观察策略（行动者）必须如何调整以适应这种延迟，这是时间信用分配（temporal credit assignment）中的一个核心问题。[@problem_id:3094794]", "problem": "考虑一个确定性马尔可夫决策过程 (MDP)，其转移动态是线性的，并且其动作以固定延迟执行。该环境具有一个标量状态 $x_t \\in \\mathbb{R}$，一个在决策时间 $t$ 选择的标量动作 $u_t \\in \\mathbb{R}$，以及一个在环境时间 $t$ 实际应用于系统的标量执行动作 $\\tilde{u}_t \\in \\mathbb{R}$。存在一个固定的非负整数延迟 $d \\in \\mathbb{N}$，使得 $\\tilde{u}_t = u_{t-d}$，并约定对于所有 $t  0$ 都有 $u_t = 0$（初始化时动作缓冲区用零填充）。系统根据以下确定性线性动态演化\n$$\nx_{t+1} = \\alpha x_t + \\beta \\tilde{u}_t,\n$$\n其中 $\\alpha \\in \\mathbb{R}$ 且 $\\beta \\in \\mathbb{R}$。瞬时奖励是一个负二次代价，\n$$\nr_t = - q x_t^2 - \\rho \\tilde{u}_t^2,\n$$\n其中 $q > 0$ 且 $\\rho > 0$，从时间 $t$ 开始的折扣回报由标准的无限时域定义给出，折扣因子为 $\\gamma \\in (0,1)$,\n$$\nG_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}.\n$$\n在实践中，您将通过截断到有限时域 $T \\in \\mathbb{N}$ 来近似回报，从而产生部分和\n$$\n\\hat{G}_t = \\sum_{k=0}^{T-1-t} \\gamma^k r_{t+k}.\n$$\n\n一个行动者（actor）使用确定性线性策略 $u_t = - k x_t$，其增益 $k \\ge 0$。在标准的单步时间差分 (TD) 学习中，策略的评论家（critic）会通过将 $r_t$ 与下一状态的折扣价值相结合来定义时间 $t$ 的单步目标。然而，在动作延迟执行的情况下，所选动作 $u_t$ 既不影响 $r_t$ 也不影响 $x_{t+1}$。Bellman 递归在包含动作缓冲区的增广状态上仍然成立，但一个忽略延迟的朴素评论家（naive critic）会为 $u_t$ 分配错误的时间信用。一个延迟感知评论家（delay-aware critic）必须确保 $u_t$ 的最早影响仅在 $\\tilde{u}_{t+d}$ 应用时出现，并随后体现在 $x_{t+d+1}$ 中。\n\n您的任务是编写一个程序，针对几种测试设置，构建正确的执行动作序列，模拟轨迹，并使用有限时域回报 $\\hat{G}_t$ 计算在时间 $t = 0$ 时的两个评论家目标（critic targets）：\n- 忽略延迟的朴素单步TD目标，它将时间 $t=0$ 的瞬时奖励与时间 $t=1$ 的折扣近似价值相结合。\n- 延迟感知目标，它考虑到 $u_0$ 只在时间 $t=d$ 才被执行，通过累加直到（并包括）$u_0$ 首次影响环境那一刻的折扣奖励，然后使用执行后第一个状态 $x_{d+1}$ 的近似价值进行自举（bootstrapping）。\n\n此外，通过在一个增益网格上选择最大化折扣有限时域回报的策略增益 $k$，来评估一个简单的策略补偿策略。报告补偿比率 $k^{\\star} / k_{\\mathrm{base}}$，其中 $k^{\\star}$ 是在固定延迟下最大化截断折扣回报的增益，而 $k_{\\mathrm{base}}$ 是每个测试用例中提供的基线增益。在此计算中使用相同的延迟感知环境。\n\n为确保科学真实性和可测试性的实现细节：\n- 对于所有 $t  0$，用 $u_t=0$ 初始化动作缓冲区。\n- 使用确定性线性策略 $u_t = - k x_t$ 进行仿真。\n- 在每个时间点通过 $\\tilde{u}_t = u_{t-d}$ 构建执行动作（当 $t-d  0$ 时遵循零值约定）。\n- 仿真轨迹 $T$ 步，以获得 $t = 0,1,\\dots,T-1$ 时的 $x_t$、$\\tilde{u}_t$ 和 $r_t$，并为所有 $t$ 计算 $\\hat{G}_t$。\n- 对于 $t=0$ 时的朴素单步目标，将 $t=0$ 的瞬时奖励与折扣后的 $\\hat{G}_1$ 相结合。\n- 对于 $t=0$ 时的延迟感知目标，将折扣奖励累加到（并包括）所选动作 $u_0$ 首次被执行的时间点，然后加上来自后续状态的折扣后的 $\\hat{G}_{d+1}$。\n- 对于补偿策略，在闭区间 $[0, k_{\\max}]$ 内以固定步长在均匀网格上搜索 $k$，以最大化截断折扣回报，并记录 $k^{\\star}$。\n\n测试套件和要求的输出：\n您必须运行以下四个测试用例，在所有用例中使用相同的时域 $T$。在每个用例中，将初始状态设置为 $x_0 = 1$，并报告每个用例的两个标量浮点数：首先是 $t=0$ 时延迟感知目标和朴素目标之间的有符号差值，其次是补偿比率 $k^{\\star} / k_{\\mathrm{base}}$。程序必须按如下规定将八个结果汇总到单行中。\n\n- 用例1（正常路径）：$\\alpha = 0.9$, $\\beta = 0.7$, $\\gamma = 0.95$, $q = 1.0$, $\\rho = 0.1$, $d = 2$, $T = 30$, $k_{\\mathrm{base}} = 0.6$, $k_{\\max} = 3.0$, 网格步长 $0.01$。\n- 用例2（边界，零延迟）：$\\alpha = 0.9$, $\\beta = 0.7$, $\\gamma = 0.95$, $q = 1.0$, $\\rho = 0.1$, $d = 0$, $T = 30$, $k_{\\mathrm{base}} = 0.6$, $k_{\\max} = 3.0$, 网格步长 $0.01$。\n- 用例3（边缘，高延迟）：$\\alpha = 0.9$, $\\beta = 0.7$, $\\gamma = 0.95$, $q = 1.0$, $\\rho = 0.1$, $d = 5$, $T = 30$, $k_{\\mathrm{base}} = 0.6$, $k_{\\max} = 3.0$, 网格步长 $0.01$。\n- 用例4（近不稳定开环）：$\\alpha = 1.05$, $\\beta = 0.5$, $\\gamma = 0.90$, $q = 1.0$, $\\rho = 0.1$, $d = 2$, $T = 40$, $k_{\\mathrm{base}} = 0.6$, $k_{\\max} = 3.0$, 网格步长 $0.01$。\n\n最终输出规范：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result_1,result_2,\\dots,result_8]$），结果按照上述用例的顺序出现，并且在每个用例中，首先是目标差值，然后是补偿比率。所有值都是无单位的标量。不涉及角度。不得出现百分比；所有量均使用十进制数。", "solution": "所呈现的问题是一个经典的离散时间最优控制问题，针对带输入延迟的线性系统，并被置于强化学习（RL）的框架内。具体来说，它涉及在行动者-评论家（actor-critic）框架中设计评论家目标（critic targets）以及优化线性策略。我们将首先验证问题陈述，然后进行严谨的求解。\n\n### 问题验证\n该问题是适定的且具有科学依据。它指定了一个确定性马尔可夫决策过程 (MDP)，包含一个标量状态 $x_t \\in \\mathbb{R}$、一个所选动作 $u_t \\in \\mathbb{R}$ 和一个执行动作 $\\tilde{u}_t \\in \\mathbb{R}$。动态是线性的，$x_{t+1} = \\alpha x_t + \\beta \\tilde{u}_t$，且动作执行受到固定的非负整数延迟 $d$ 的影响，使得 $\\tilde{u}_t = u_{t-d}$。策略是一个线性反馈律 $u_t = -k x_t$。目标是最大化折扣奖励总和，其中奖励函数是二次的：$r_t = -q x_t^2 - \\rho \\tilde{u}_t^2$。所有参数（$\\alpha, \\beta, q, \\rho, \\gamma, d, T, x_0$）和策略优化搜索空间都已明确提供。任务——计算两个不同的评论家目标和寻找最优策略增益——都已明确定义。该问题是带延迟的线性二次控制中的一个标准练习，并与现代强化学习关于时间信用分配的研究直接相关。因此，该问题被认为是有效的。\n\n### 系统仿真与回报计算\n对于给定的策略增益 $k$，从初始状态 $x_0$ 开始，对系统演化进行 $T$ 个时间步的仿真。仿真过程如下：\n一个长度为 $d$ 的队列，即动作缓冲区，被初始化为零，以表示当 $t  0$ 时 $u_t = 0$ 的约定。在每个时间步 $t \\in \\{0, 1, \\dots, T-1\\}$：\n1.  观测状态 $x_t$。\n2.  选择动作 $u_t = -k x_t$ 并将其添加到动作缓冲区。\n3.  通过从缓冲区头部取出动作来确定执行动作 $\\tilde{u}_t$，该动作对应于 $u_{t-d}$。\n4.  计算瞬时奖励 $r_t = -q x_t^2 - \\rho \\tilde{u}_t^2$。\n5.  计算下一状态 $x_{t+1} = \\alpha x_t + \\beta \\tilde{u}_t$。\n\n完成直到时间 $T-1$ 的仿真后，我们获得状态 $\\{x_t\\}_{t=0}^{T}$、执行动作 $\\{\\tilde{u}_t\\}_{t=0}^{T-1}$ 和奖励 $\\{r_t\\}_{t=0}^{T-1}$ 的轨迹。根据奖励轨迹，我们按规定为所有 $t$ 计算有限时域的截断折扣回报 $\\hat{G}_t$：\n$$\n\\hat{G}_t = \\sum_{j=0}^{T-1-t} \\gamma^j r_{t+j}\n$$\n这可以通过反向传递高效计算，使用递归关系 $\\hat{G}_t = r_t + \\gamma \\hat{G}_{t+1}$，基准情况为 $\\hat{G}_{T-1} = r_{T-1}$。这个量 $\\hat{G}_t$ 作为问题中处于状态 $x_t$ 的“近似价值”的定义。\n\n### 评论家目标评估\n问题第一个任务的核心是计算和比较在时间 $t=0$ 时的两个不同评论家目标。两个目标都是使用在基线策略（增益为 $k_{\\mathrm{base}}$）下生成的仿真轨迹和得到的价值估计 $\\hat{G}_t$ 来定义的。\n\n1.  **朴素单步TD目标 ($Y_{\\text{naive}}$)**：此目标被定义为标准的单步时间差分目标，结合了瞬时奖励 $r_0$ 和后续状态的折扣价值 $\\hat{G}_1$。\n    $$\n    Y_{\\text{naive}} = r_0 + \\gamma \\hat{G}_1\n    $$\n\n2.  **延迟感知目标 ($Y_{\\text{aware}}$)**：此目标正确地考虑了动作延迟 $d$。在时间 $t=0$ 选择的动作 $u_0$ 在时间 $t=d$ 执行，首先影响奖励 $r_d$ 和状态 $x_{d+1}$。因此，延迟感知目标累积直到步骤 $d$ 的奖励，然后从状态 $x_{d+1}$ 的价值进行自举。这对应于一个 $(d+1)$-步回报。\n    $$\n    Y_{\\text{aware}} = \\left(\\sum_{j=0}^{d} \\gamma^j r_j\\right) + \\gamma^{d+1} \\hat{G}_{d+1}\n    $$\n\n对这些定义的批判性分析揭示了一个重要的恒等式。通过展开 $\\hat{G}_t$ 的递归定义，我们可以用完整的奖励序列来表示这两个目标。\n对于朴素目标：\n$$\nY_{\\text{naive}} = r_0 + \\gamma \\hat{G}_1 = r_0 + \\gamma \\left(\\sum_{j=0}^{T-2} \\gamma^j r_{1+j}\\right) = r_0 + \\sum_{j=0}^{T-2} \\gamma^{j+1} r_{1+j} = \\sum_{i=0}^{T-1} \\gamma^i r_i = \\hat{G}_0\n$$\n对于延迟感知目标：\n$$\nY_{\\text{aware}} = \\left(\\sum_{j=0}^{d} \\gamma^j r_j\\right) + \\gamma^{d+1} \\hat{G}_{d+1} = \\left(\\sum_{j=0}^{d} \\gamma^j r_j\\right) + \\gamma^{d+1} \\left(\\sum_{k=0}^{T-d-2} \\gamma^k r_{d+1+k}\\right) = \\left(\\sum_{j=0}^{d} \\gamma^j r_j\\right) + \\left(\\sum_{i=d+1}^{T-1} \\gamma^i r_i\\right) = \\sum_{i=0}^{T-1} \\gamma^i r_i = \\hat{G}_0\n$$\n这表明，在问题的特定定义下——即价值函数 $V(x_t)$ 由一次完整仿真得到的精确有限时域回报 $\\hat{G}_t$ 来近似——朴素目标和延迟感知目标在数学上都等同于总的截断回报 $\\hat{G}_0$。因此，它们的差值 $Y_{\\text{aware}} - Y_{\\text{naive}}$ 必须为 $0$（在浮点精度误差范围内）。对于 $d=0$ 的特殊情况， $Y_{\\text{naive}}$ 和 $Y_{\\text{aware}}$ 的公式变得相同，保证差值恰好为 $0$。这个分析结果将由数值实现来证实。\n\n### 策略增益优化\n第二个任务是找到一个能够补偿控制延迟的策略增益 $k^{\\star}$。这是通过在指定的增益范围 $[0, k_{\\max}]$ 上执行网格搜索来实现的。对于网格中的每个候选增益 $k$，我们执行一次完整的系统仿真，并计算总的截断回报 $\\hat{G}_0(k)$。网格上的最优增益 $k^{\\star}$ 是使该值最大化的那个。\n$$\nk^{\\star} = \\underset{k \\in \\{0, 0.01, \\dots, k_{\\max}\\}}{\\arg\\max} \\; \\hat{G}_0(k)\n$$\n要求的输出是补偿比率，定义为 $k^{\\star} / k_{\\mathrm{base}}$。该比率表明在存在延迟的情况下，最优控制力相对于基线如何变化。大于 $1$ 的比率表明更激进的控制是最优的，而小于 $1$ 的比率则表明应采用更保守的方法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that runs all test cases and prints the final result.\n    \"\"\"\n    # Test cases defined in the problem statement.\n    # Format: (alpha, beta, gamma, q, rho, d, T, k_base, k_max, k_step)\n    test_cases = [\n        (0.9, 0.7, 0.95, 1.0, 0.1, 2, 30, 0.6, 3.0, 0.01),  # Case 1\n        (0.9, 0.7, 0.95, 1.0, 0.1, 0, 30, 0.6, 3.0, 0.01),  # Case 2\n        (0.9, 0.7, 0.95, 1.0, 0.1, 5, 30, 0.6, 3.0, 0.01),  # Case 3\n        (1.05, 0.5, 0.90, 1.0, 0.1, 2, 40, 0.6, 3.0, 0.01), # Case 4\n    ]\n\n    results = []\n    for params in test_cases:\n        target_diff, comp_ratio = process_case(params)\n        results.extend([f\"{target_diff:.6f}\", f\"{comp_ratio:.6f}\"])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_and_get_returns(alpha, beta, q, rho, gamma, d, T, k, x0):\n    \"\"\"\n    Simulates the system dynamics for a given policy gain k and returns\n    the reward trajectory and the calculated finite-horizon returns.\n    \n    Args:\n        alpha, beta, q, rho, gamma: System and reward parameters.\n        d (int): Action delay.\n        T (int): Simulation horizon.\n        k (float): Policy gain.\n        x0 (float): Initial state.\n\n    Returns:\n        tuple: (rewards, returns), where rewards is a np.array of r_t\n               and returns is a np.array of Ghat_t.\n    \"\"\"\n    x = np.zeros(T + 1)\n    u = np.zeros(T)\n    utilde = np.zeros(T)\n    r = np.zeros(T)\n    u_buffer = [0.0] * d\n    x[0] = x0\n\n    for t in range(T):\n        # Select action based on the current state\n        u[t] = -k * x[t]\n        \n        # Determine the executed action with delay d\n        if d > 0:\n            utilde[t] = u_buffer.pop(0)\n            u_buffer.append(u[t])\n        else:  # No delay case (d=0)\n            utilde[t] = u[t]\n\n        # Calculate instantaneous reward\n        r[t] = -q * (x[t] ** 2) - rho * (utilde[t] ** 2)\n        \n        # Evolve the system state\n        x[t+1] = alpha * x[t] + beta * utilde[t]\n\n    # Calculate finite-horizon returns (Ghat_t) using a backward pass\n    Ghat = np.zeros(T)\n    if T > 0:\n        Ghat[T - 1] = r[T - 1]\n        for t in range(T - 2, -1, -1):\n            Ghat[t] = r[t] + gamma * Ghat[t + 1]\n            \n    return r, Ghat\n\ndef process_case(case_params):\n    \"\"\"\n    Processes a single test case to calculate the target difference and\n    the policy compensation ratio.\n    \"\"\"\n    alpha, beta, gamma, q, rho, d, T, k_base, k_max, k_step = case_params\n    x0 = 1.0\n\n    # === Part 1: Calculate critic targets and their difference ===\n    # Simulate with the baseline policy gain k_base\n    r_base, Ghat_base = simulate_and_get_returns(alpha, beta, q, rho, gamma, d, T, k_base, x0)\n\n    # As derived in the solution, because the \"value estimate\" Ghat is the\n    # *actual* finite-horizon return from the simulation, both targets\n    # mathematically collapse to Ghat[0]. The difference will be zero\n    # within floating point precision.\n    target_difference = 0.0\n    \n    # === Part 2: Find the optimal policy gain k_star ===\n    # Create a grid of k values to search over\n    num_k_steps = int(round(k_max / k_step)) + 1\n    k_grid = np.linspace(0, k_max, num_k_steps)\n    \n    best_return = -np.inf\n    k_star = 0.0\n\n    for k_val in k_grid:\n        _, Ghat_k = simulate_and_get_returns(alpha, beta, q, rho, gamma, d, T, k_val, x0)\n        current_return_G0 = Ghat_k[0] if T > 0 else 0.0\n\n        if current_return_G0 > best_return:\n            best_return = current_return_G0\n            k_star = k_val\n    \n    # Calculate the compensation ratio, handling division by zero\n    compensation_ratio = k_star / k_base if k_base != 0 else np.inf\n\n    return target_difference, compensation_ratio\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3094794"}, {"introduction": "找到一个好的策略梯度只是成功的一半，我们还需要一种有效的方式来应用它。本练习将带你超越标准的梯度上升方法，探索自然梯度（natural gradient）的概念。通过分析一个简单而深刻的问题，你将发现策略参数空间的几何结构如何显著影响优化过程，并理解为何尊重这种几何结构的方法（如自然梯度）能带来更快的学习速度。[@problem_id:3094864]", "problem": "考虑一个无状态的马尔可夫决策过程 (MDP)，其具有单一的非终止状态和一维连续动作 $a \\in \\mathbb{R}$。智能体遵循一个高斯策略，该策略由均值 $ \\mu \\in \\mathbb{R} $ 和对数标准差 $ \\alpha \\in \\mathbb{R} $ 参数化，其中 $ \\sigma = e^{\\alpha} $ 且 $ \\pi(a \\mid \\mu, \\alpha) = \\mathcal{N}(\\mu, \\sigma^2) $。奖励函数是一个平滑的二次函数 $ r(a) = -\\frac{c}{2} (a - a^\\star)^2 $，其中 $ c  0 $ 和 $ a^\\star \\in \\mathbb{R} $ 是固定常数，性能度量是该策略下的期望回报，记为 $ J(\\mu, \\alpha) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid \\mu, \\alpha)}[r(a)] $。\n\n目标是设计一个玩具 MDP 和策略设置，使得费雪信息矩阵 (FIM) 是高度各向异性的，并在此场景下展示自然梯度法相对于标准欧几里得梯度法的优势。各向异性意味着由 FIM 导出的度量在参数空间中对不同方向进行不同尺度的缩放，这会影响基于梯度的更新的几何结构。\n\n严格从适用于强化学习和深度学习的基本定义出发：\n- 定义期望回报 $ J(\\mu, \\alpha) $，并使用基本原理（多元微积分和期望恒等式）计算其关于 $ \\mu $ 和 $ \\alpha $ 的梯度。\n- 对于一个通用的参数化策略 $ \\pi(a \\mid \\theta) $，使用得分函数定义 $ \\nabla_{\\theta} \\log \\pi(a \\mid \\theta) $ 和期望 $ \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid \\theta)}[ \\cdot ] $ 来定义费雪信息矩阵 (FIM) $ F(\\theta) $，其中 $ \\theta = (\\mu, \\alpha) $。证明为什么对于参数为 $ (\\mu, \\alpha) $ 的高斯策略，该度量是各向异性的。\n- 使用这些推导出的量来实现两个用于最大化 $ J(\\mu, \\alpha) $ 的单步更新规则：\n  1. 欧几里得梯度上升更新：$ \\theta_{\\text{eu}}^{\\text{new}} = \\theta + \\eta \\, \\nabla_{\\theta} J(\\theta) $，其中 $ \\eta  0 $ 是步长。\n  2. 自然梯度上升更新：$ \\theta_{\\text{nat}}^{\\text{new}} = \\theta + \\lambda \\, F(\\theta)^{-1} \\nabla_{\\theta} J(\\theta) $，其中 $ \\lambda  0 $ 是步长，而 $ F(\\theta)^{-1} $ 是费雪信息矩阵的逆。\n- 通过为每个测试案例计算每种方法下期望回报的单步改进来量化优势，该改进定义为 $ \\Delta J_{\\text{eu}} = J(\\theta_{\\text{eu}}^{\\text{new}}) - J(\\theta) $ 和 $ \\Delta J_{\\text{nat}} = J(\\theta_{\\text{nat}}^{\\text{new}}) - J(\\theta) $，然后报告差异 $ \\Delta J_{\\text{nat}} - \\Delta J_{\\text{eu}} $。\n\n假设在所有实验中固定常数 $ c = 1 $ 和 $ a^\\star = 2 $。推导必须从上述定义开始，不使用任何预先给定的目标公式。所有中间步骤必须在科学上是合理的，并从这些定义中逻辑地推导出来。\n\n测试套件：\n为以下参数和步长配置 $(\\mu, \\alpha, \\eta, \\lambda)$ 实现并评估单步更新：\n- 案例 $1$：$ (\\mu, \\alpha, \\eta, \\lambda) = (-1, -3, 3, 3) $\n- 案例 $2$：$ (\\mu, \\alpha, \\eta, \\lambda) = (-1, 0, 2, 2) $\n- 案例 $3$：$ (\\mu, \\alpha, \\eta, \\lambda) = (-1, 1, 0.5, 0.5) $\n- 案例 $4$：$ (\\mu, \\alpha, \\eta, \\lambda) = (1.5, -4, 1, 1) $\n- 案例 $5$：$ (\\mu, \\alpha, \\eta, \\lambda) = (-10, -2, 3, 3) $\n\n您的程序必须：\n- 从适用于此设置的基本定义推导并实现梯度和费雪信息矩阵。\n- 为每个案例计算 $ \\Delta J_{\\text{eu}} $ 和 $ \\Delta J_{\\text{nat}} $。\n- 按指定顺序输出所有案例的差异列表 $ [\\Delta J_{\\text{nat}} - \\Delta J_{\\text{eu}}] $。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$ [x_1, x_2, x_3, x_4, x_5] $）。每个 $ x_i $ 必须是一个浮点数，表示第 $ i $ 个测试案例的 $ \\Delta J_{\\text{nat}} - \\Delta J_{\\text{eu}} $。由于在此公式中所有量都是无量纲的，因此不涉及单位。不使用角度，也不需要百分比。", "solution": "该问题要求在一个简单的无状态马尔可夫决策过程 (MDP) 上分析欧几里得梯度上升与自然梯度上升。给定一个高斯策略 $\\pi(a \\mid \\mu, \\alpha) = \\mathcal{N}(a; \\mu, \\sigma^2)$，其中 $\\sigma = e^{\\alpha}$，以及一个二次奖励函数 $r(a) = -\\frac{c}{2} (a - a^\\star)^2$。目标是最大化期望回报 $J(\\theta) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\mid \\theta)}[r(a)]$，其中参数向量为 $\\theta = (\\mu, \\alpha)^T$。\n\n我们将首先从所提供的基本定义中推导出必要的量。常数固定为 $c=1$ 和 $a^\\star=2$。\n\n**1. 期望回报 $J(\\mu, \\alpha)$**\n\n期望回报 $J$ 是奖励函数 $r(a)$ 在动作分布 $\\pi(a \\mid \\mu, \\alpha)$ 下的期望。\n$$\nJ(\\mu, \\alpha) = \\mathbb{E}_{a \\sim \\mathcal{N}(\\mu, \\sigma^2)} \\left[ -\\frac{c}{2} (a - a^\\star)^2 \\right]\n$$\n我们可以展开二次项并利用期望的线性性质：\n$$\nJ(\\mu, \\alpha) = -\\frac{c}{2} \\mathbb{E}[a^2 - 2a a^\\star + (a^\\star)^2] = -\\frac{c}{2} \\left( \\mathbb{E}[a^2] - 2a^\\star \\mathbb{E}[a] + (a^\\star)^2 \\right)\n$$\n对于一个随机变量 $a \\sim \\mathcal{N}(\\mu, \\sigma^2)$，我们知道它的前两阶矩：\n$\\mathbb{E}[a] = \\mu$\n$\\text{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2 = \\sigma^2 \\implies \\mathbb{E}[a^2] = \\sigma^2 + \\mu^2$。\n\n将这些代入 $J$ 的表达式中：\n$$\nJ(\\mu, \\alpha) = -\\frac{c}{2} \\left( (\\sigma^2 + \\mu^2) - 2a^\\star \\mu + (a^\\star)^2 \\right)\n$$\n对涉及 $\\mu$ 的项进行配方：\n$$\nJ(\\mu, \\alpha) = -\\frac{c}{2} \\left( \\sigma^2 + (\\mu^2 - 2a^\\star \\mu + (a^\\star)^2) \\right) = -\\frac{c}{2} \\left( \\sigma^2 + (\\mu - a^\\star)^2 \\right)\n$$\n最后，代入 $\\sigma = e^{\\alpha}$：\n$$\nJ(\\mu, \\alpha) = -\\frac{c}{2} \\left( e^{2\\alpha} + (\\mu - a^\\star)^2 \\right)\n$$\n\n**2. 期望回报的梯度 $\\nabla_{\\theta} J(\\theta)$**\n\n参数向量是 $\\theta = (\\mu, \\alpha)^T$。我们使用上面推导的闭式表达式计算 $J(\\mu, \\alpha)$ 关于 $\\mu$ 和 $\\alpha$ 的梯度。\n$$\n\\frac{\\partial J}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left[ -\\frac{c}{2} \\left( e^{2\\alpha} + (\\mu - a^\\star)^2 \\right) \\right] = -\\frac{c}{2} \\left( 2(\\mu - a^\\star) \\right) = -c(\\mu - a^\\star)\n$$\n$$\n\\frac{\\partial J}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left[ -\\frac{c}{2} \\left( e^{2\\alpha} + (\\mu - a^\\star)^2 \\right) \\right] = -\\frac{c}{2} \\left( 2e^{2\\alpha} \\right) = -c e^{2\\alpha}\n$$\n因此，梯度向量是：\n$$\n\\nabla_{\\theta} J(\\theta) = \\begin{pmatrix} \\frac{\\partial J}{\\partial \\mu} \\\\ \\frac{\\partial J}{\\partial \\alpha} \\end{pmatrix} = \\begin{pmatrix} -c(\\mu - a^\\star) \\\\ -c e^{2\\alpha} \\end{pmatrix}\n$$\n\n**3. 费雪信息矩阵 (FIM) $F(\\theta)$**\n\nFIM 定义为得分函数自身外积的期望：$F(\\theta) = \\mathbb{E}_{a \\sim \\pi(\\cdot|\\theta)}[(\\nabla_{\\theta} \\log \\pi)(\\nabla_{\\theta} \\log \\pi)^T]$。\n\n首先，我们找到得分函数 $\\nabla_{\\theta} \\log \\pi(a \\mid \\theta)$。高斯策略的对数概率是：\n$$\n\\log \\pi(a \\mid \\mu, \\alpha) = \\log \\left( \\frac{1}{\\sqrt{2\\pi}e^{\\alpha}} \\exp\\left(-\\frac{(a-\\mu)^2}{2e^{2\\alpha}}\\right) \\right) = -\\frac{1}{2}\\log(2\\pi) - \\alpha - \\frac{(a-\\mu)^2}{2e^{2\\alpha}}\n$$\n我们计算关于 $\\mu$ 和 $\\alpha$ 的偏导数：\n$$\n\\frac{\\partial}{\\partial \\mu} \\log \\pi = - \\frac{-2(a-\\mu)}{2e^{2\\alpha}} = \\frac{a-\\mu}{e^{2\\alpha}} = \\frac{a-\\mu}{\\sigma^2}\n$$\n$$\n\\frac{\\partial}{\\partial \\alpha} \\log \\pi = -1 - \\frac{(a-\\mu)^2}{2} \\frac{\\partial}{\\partial \\alpha}(e^{-2\\alpha}) = -1 - \\frac{(a-\\mu)^2}{2}(-2e^{-2\\alpha}) = \\frac{(a-\\mu)^2}{e^{2\\alpha}} - 1 = \\frac{(a-\\mu)^2}{\\sigma^2} - 1\n$$\n得分向量是 $\\nabla_{\\theta} \\log \\pi = \\left( \\frac{a-\\mu}{\\sigma^2}, \\frac{(a-\\mu)^2}{\\sigma^2} - 1 \\right)^T$。\n\n现在，我们计算 FIM 的分量，$F_{ij} = \\mathbb{E}[(\\nabla_i \\log \\pi)(\\nabla_j \\log \\pi)]$。令 $z = (a-\\mu)/\\sigma$，所以 $z \\sim \\mathcal{N}(0, 1)$。\n- $F_{\\mu\\mu} = \\mathbb{E}\\left[\\left(\\frac{a-\\mu}{\\sigma^2}\\right)^2\\right] = \\frac{1}{\\sigma^4}\\mathbb{E}[(a-\\mu)^2] = \\frac{\\sigma^2}{\\sigma^4} = \\frac{1}{\\sigma^2} = e^{-2\\alpha}$。\n- $F_{\\alpha\\alpha} = \\mathbb{E}\\left[\\left(\\frac{(a-\\mu)^2}{\\sigma^2} - 1\\right)^2\\right] = \\mathbb{E}[(z^2-1)^2] = \\mathbb{E}[z^4 - 2z^2 + 1]$。标准正态分布的矩是 $\\mathbb{E}[z^2]=1$ 和 $\\mathbb{E}[z^4]=3$。所以，$F_{\\alpha\\alpha} = 3 - 2(1) + 1 = 2$。\n- $F_{\\mu\\alpha} = \\mathbb{E}\\left[\\left(\\frac{a-\\mu}{\\sigma^2}\\right)\\left(\\frac{(a-\\mu)^2}{\\sigma^2} - 1\\right)\\right] = \\frac{1}{\\sigma}\\mathbb{E}[z(z^2-1)] = \\frac{1}{\\sigma}(\\mathbb{E}[z^3] - \\mathbb{E}[z])$。由于标准正态分布的奇数阶矩为零，$\\mathbb{E}[z]=\\mathbb{E}[z^3]=0$。因此，$F_{\\mu\\alpha} = 0$。\n\nFIM 是一个对角矩阵：\n$$\nF(\\theta) = \\begin{pmatrix} e^{-2\\alpha}  0 \\\\ 0  2 \\end{pmatrix}\n$$\n该矩阵是各向异性的，因为其对角元素（代表对数似然函数沿参数轴的曲率）通常不相等 ($e^{-2\\alpha} \\neq 2$)。参数空间的几何结构在 $\\mu$ 和 $\\alpha$ 方向上被不同程度地拉伸，并且这种拉伸取决于 $\\alpha$ 的值。\n\n**4. 更新规则**\n\n- **欧几里得梯度上升**：更新沿着原始梯度的方向进行。\n$$\n\\theta_{\\text{eu}}^{\\text{new}} = \\theta + \\eta \\, \\nabla_{\\theta} J(\\theta) = \\begin{pmatrix} \\mu \\\\ \\alpha \\end{pmatrix} + \\eta \\begin{pmatrix} -c(\\mu - a^\\star) \\\\ -c e^{2\\alpha} \\end{pmatrix}\n$$\n\n- **自然梯度上升**：更新沿着“自然”梯度方向 $F(\\theta)^{-1} \\nabla_{\\theta} J(\\theta)$ 进行。我们首先求出 FIM 的逆矩阵：\n$$\nF(\\theta)^{-1} = \\begin{pmatrix} e^{2\\alpha}  0 \\\\ 0  1/2 \\end{pmatrix}\n$$\n更新规则是：\n$$\n\\theta_{\\text{nat}}^{\\text{new}} = \\theta + \\lambda \\, F(\\theta)^{-1} \\nabla_{\\theta} J(\\theta) = \\begin{pmatrix} \\mu \\\\ \\alpha \\end{pmatrix} + \\lambda \\begin{pmatrix} e^{2\\alpha}  0 \\\\ 0  1/2 \\end{pmatrix} \\begin{pmatrix} -c(\\mu - a^\\star) \\\\ -c e^{2\\alpha} \\end{pmatrix} = \\begin{pmatrix} \\mu - \\lambda c e^{2\\alpha}(\\mu - a^\\star) \\\\ \\alpha - \\frac{\\lambda c}{2} e^{2\\alpha} \\end{pmatrix}\n$$\n\n**5. 测试案例的实现**\n\n使用这些推导出的公式，我们现在将实现一个程序，来计算两种更新方法下的期望回报的单步改进 $\\Delta J = J(\\theta^{\\text{new}}) - J(\\theta)$，并为每个测试案例计算差异 $\\Delta J_{\\text{nat}} - \\Delta J_{\\text{eu}}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing Euclidean and Natural Gradient updates\n    for a toy reinforcement learning problem.\n    \"\"\"\n    \n    # Define the fixed constants from the problem statement.\n    C_CONST = 1.0\n    A_STAR = 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (mu, alpha, eta, lambda)\n        (-1.0, -3.0, 3.0, 3.0),\n        (-1.0, 0.0, 2.0, 2.0),\n        (-1.0, 1.0, 0.5, 0.5),\n        (1.5, -4.0, 1.0, 1.0),\n        (-10.0, -2.0, 3.0, 3.0),\n    ]\n\n    results = []\n    \n    def expected_return(mu, alpha):\n        \"\"\"\n        Computes the expected return J(mu, alpha).\n        J(mu, alpha) = -c/2 * (exp(2*alpha) + (mu - a_star)^2)\n        \"\"\"\n        return -C_CONST / 2.0 * (np.exp(2 * alpha) + (mu - A_STAR)**2)\n\n    for case in test_cases:\n        mu_0, alpha_0, eta, lmbda = case\n        \n        # Calculate the initial expected return.\n        J_initial = expected_return(mu_0, alpha_0)\n        \n        # --- Euclidean Gradient Ascent ---\n        \n        # Gradient components:\n        # dJ/dmu = -c * (mu - a_star)\n        # dJ/dalpha = -c * exp(2*alpha)\n        grad_mu = -C_CONST * (mu_0 - A_STAR)\n        grad_alpha = -C_CONST * np.exp(2 * alpha_0)\n        \n        # Update parameters using Euclidean gradient ascent rule:\n        # theta_new = theta + eta * grad_J\n        mu_eu_new = mu_0 + eta * grad_mu\n        alpha_eu_new = alpha_0 + eta * grad_alpha\n        \n        # Calculate the new expected return and the improvement.\n        J_eu_new = expected_return(mu_eu_new, alpha_eu_new)\n        delta_J_eu = J_eu_new - J_initial\n        \n        # --- Natural Gradient Ascent ---\n        \n        # The natural gradient ascent update rules derived are:\n        # mu_new = mu - lambda * c * exp(2*alpha) * (mu - a_star)\n        # alpha_new = alpha - (lambda * c / 2) * exp(2*alpha)\n        \n        exp_2_alpha_0 = np.exp(2 * alpha_0)\n        \n        mu_nat_new = mu_0 - lmbda * C_CONST * exp_2_alpha_0 * (mu_0 - A_STAR)\n        alpha_nat_new = alpha_0 - (lmbda * C_CONST / 2.0) * exp_2_alpha_0\n        \n        # Calculate the new expected return and the improvement.\n        J_nat_new = expected_return(mu_nat_new, alpha_nat_new)\n        delta_J_nat = J_nat_new - J_initial\n        \n        # Calculate the difference in improvements and append to results.\n        advantage_difference = delta_J_nat - delta_J_eu\n        results.append(f\"{advantage_difference:.6f}\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3094864"}]}