{"hands_on_practices": [{"introduction": "在强化学习中，当我们将离策略（off-policy）学习、函数近似和自举（bootstrapping）这三个要素结合时，可能会遇到一个棘手的问题——学习过程不稳定甚至发散，这被称为“致命三位一体”。这个实践练习 [@problem_id:3163145] 将引导你亲手构建一个类似贝尔德（Baird）反例的场景，你将亲眼目睹这种发散现象的发生。通过这个过程，你不仅能深刻理解不稳定性问题的根源，还能测试目标网络（target networks）和双Q学习（Double Q-learning）等关键技术为何能有效“修复”这个问题。", "problem": "要求您设计并运行一个可复现的计算实验，以在函数近似下，探究有限马尔可夫决策过程（MDP）中带有自举（bootstrapping）的半梯度控制（semi-gradient control）的稳定性，实验中使用深度线性网络作为动作价值函数的近似器。您的任务是实现一个类似 Baird 的、能够表现出发散性的离策略（off-policy）设置，然后测试两种常见的稳定化思想——目标网络（target networks）和双重动作价值学习（Double action-value learning）——是否能改善这种不稳定性。最终答案必须是一个完整的、可运行的程序。\n\n您必须基于以下基本概念：\n- 马尔可夫决策过程（MDP）的定义，包括状态集 $\\mathcal{S}$、动作集 $\\mathcal{A}$、转移动态 $P(s' \\mid s,a)$、奖励函数 $R(s,a)$ 和折扣因子 $\\gamma \\in [0,1)$。\n- 策略 $\\pi$ 的动作价值函数 $Q^{\\pi}(s,a)$ 的定义，以及作为控制更新基础的动作价值的贝尔曼最优算子。\n- 经过充分检验的半梯度时序差分控制思想（例如在深度Q网络（DQN）中，形成一个自举目标，并且只对预测 $Q_{\\theta}(s,a)$ 进行微分），以及作为稳定化变体的、经过充分检验的目标网络和双重Q学习（Double DQN 使用在线选择和目标评估）思想。\n\n需要实现的问题设置：\n1. 环境。使用一个有限MDP，其状态集 $|\\mathcal{S}|=7$ 个状态，索引为六个上层状态 $\\{s_0,\\dots,s_5\\}$ 和一个下层状态 $s_6$，以及两个动作 $\\mathcal{A}=\\{a_{\\mathrm{solid}}, a_{\\mathrm{dashed}}\\}$。转移动态是确定性的或伪确定性的，具体如下：\n   - 对于任何上层状态 $s_i$，$i \\in \\{0,\\dots,5\\}$：\n     - 如果采取动作 $a_{\\mathrm{solid}}$，下一个状态是 $s_6$。\n     - 如果采取动作 $a_{\\mathrm{dashed}}$，下一个状态是从 $\\{s_0,\\dots,s_5\\}$ 中均匀随机抽取的一个上层状态。\n   - 对于下层状态 $s_6$：\n     - 无论采取哪个动作，下一个状态都是从 $\\{s_0,\\dots,s_5\\}$ 中均匀随机抽取的一个上层状态。\n   - 奖励始终为零，$R(s,a)=0$ 对所有 $(s,a)$ 成立。\n   - 用于采样经验的行为策略是固定的，并且相对于贪心控制是离策略的：在每一步，以概率 $\\beta=0.95$ 选择 $a_{\\mathrm{dashed}}$，以概率 $1-\\beta=0.05$ 选择 $a_{\\mathrm{solid}}$。\n   - 折扣因子为 $\\gamma \\in [0,1)$，根据每个测试用例指定。\n   所有随机选择必须由一个具有固定种子的可复现伪随机数生成器生成。\n\n2. 函数近似。使用一个没有非线性环节的深度线性网络来参数化动作价值函数 $Q_{\\theta}(s,a)$，其架构为：输入维度 $d_{\\mathrm{in}}=14$（对 $(s,a)$ 进行独热编码），一个宽度为 $h$ 的隐藏层（选择 $h=4$），以及一个标量输出。设参数为 $\\theta = \\{W_1 \\in \\mathbb{R}^{14 \\times 4}, W_2 \\in \\mathbb{R}^{4 \\times 1}\\}$，并定义\n   $$Q_{\\theta}(s,a) \\equiv x(s,a)^{\\top} W_1 W_2,$$\n   其中 $x(s,a) \\in \\mathbb{R}^{14}$ 是状态-动作对的独热编码。将 $W_1$ 和 $W_2$ 初始化为带有小随机扰动的小正值。\n\n3. 用于比较的学习规则。对于从行为策略中采样的每个转移 $(s,a,r,s')$，执行一个半梯度步骤以最小化平方时序差分误差，其中自举目标的构建方式在每个变体中都不同：\n   - 普通半梯度控制（类似于没有目标网络的基本DQN）：使用正在更新的相同参数（在线网络）来构建目标，即根据当前的 $Q_{\\theta}$ 采取贪心动作，并用相同的 $Q_{\\theta}$ 对其进行评估。\n   - 目标网络变体：维护一套独立的目标参数集 $\\bar{\\theta}$，该参数集每隔 $C$ 步从 $\\theta$ 进行周期性硬更新（复制）。完全使用 $\\bar{\\theta}$ 来构建目标。梯度仍然只通过 $Q_{\\theta}(s,a)$ 计算。\n   - 双重动作价值变体（Double DQN）：通过在线参数 $\\theta$ 在 $s'$ 处选择贪心动作，但使用目标参数 $\\bar{\\theta}$ 来评估该动作的价值，从而构建目标。\n\n4. 发散检测与轨迹摘要。在 $T$ 步的训练过程中，定期记录参数范数\n   $$\\|\\theta_t\\|_2 \\equiv \\sqrt{\\|W_{1,t}\\|_F^2 + \\|W_{2,t}\\|_F^2}$$\n   计算范数的对数序列，并对 $\\log \\|\\theta_t\\|_2$ 与步数索引 $t$ 拟合一个单变量线性模型以估计斜率。如果最终范数与初始范数之比超过指定的增长因子，并且拟合的斜率超过一个小的正阈值，或者出现任何数值溢出或非数值（not-a-number），则宣布该次运行为“发散”。具体的阈值由您选择，但它们必须是固定常数，并统一应用于所有测试用例。\n\n测试套件与要求输出：\n实现以下参数用例。每个用例是一个元组 $(\\mathrm{algo}, \\alpha, \\gamma, C, T)$，其中 $\\mathrm{algo} \\in \\{\\text{plain}, \\text{target}, \\text{double}\\}$ 是学习规则，$\\alpha$ 是学习率，$\\gamma$ 是折扣因子，$C$ 是目标参数的硬更新周期（步数，对于普通变体则忽略），$T$ 是训练步数：\n- 用例 1：$(\\text{plain}, 0.20, 0.99, 1, 20000)$\n- 用例 2：$(\\text{target}, 0.20, 0.99, 50, 20000)$\n- 用例 3：$(\\text{target}, 0.01, 0.99, 50, 20000)$\n- 用例 4：$(\\text{double}, 0.05, 0.99, 50, 20000)$\n- 用例 5（无自举结转的边界条件）：$(\\text{plain}, 0.20, 0.00, 1, 10000)$\n\n您的程序必须：\n- 对所有随机性使用固定的伪随机种子。\n- 精确地按照描述实现环境和学习规则。\n- 对每个用例，根据您统一的标准返回一个布尔值，指示运行是否发散。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起，按给定顺序显示五个用例的结果。例如，格式必须与以下完全一样\n\"[result1,result2,result3,result4,result5]\"\n其中每个result是布尔字面量，即 \"True\" 或 \"False\"。输出行中不允许有任何额外文本。", "solution": "问题陈述已经过验证，被认为是有效的。它提出了一个在深度强化学习领域中定义明确、科学严谨且客观的计算实验。任务是实现一个特定的马尔可夫决策过程（MDP）和几种半梯度Q学习算法，以研究在函数近似下的离策略学习的稳定性。该设置是经典 Baird 反例的一个变体，旨在引发发散。使用深度线性网络作为函数近似器是一种现代且相关的形式。指令是完整的、一致的，并且可以形式化为一个可运行的程序。\n\n解决方案按以下步骤进行：首先，我们形式化实验的各个组成部分——MDP、函数近似器和学习规则。其次，我们详细说明模拟循环的实现。最后，我们指定用于检测发散的标准。\n\n环境是一个由元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ 定义的有限MDP。\n- 状态集为 $\\mathcal{S} = \\{s_0, s_1, s_2, s_3, s_4, s_5, s_6\\}$，其中 $|\\mathcal{S}|=7$。状态 $\\{s_0, \\dots, s_5\\}$ 被指定为“上层状态”，$s_6$ 被指定为“下层状态”。\n- 动作集为 $\\mathcal{A} = \\{a_{\\text{solid}}, a_{\\text{dashed}}\\}$。\n- 转移动态 $P(s'|s,a)$ 对某些转移是确定性的，对其他转移是随机性的：\n  - 对于一个上层状态 $s \\in \\{s_0, \\dots, s_5\\}$：\n    - 采取 $a_{\\text{solid}}$ 导致 $s' = s_6$。\n    - 采取 $a_{\\text{dashed}}$ 导致下一个状态 $s'$ 从 $\\{s_0, \\dots, s_5\\}$ 中均匀随机抽取。\n  - 对于下层状态 $s_6$，任何动作都会导致下一个状态 $s'$ 从 $\\{s_0, \\dots, s_5\\}$ 中均匀随机抽取。\n- 奖励函数对所有转移恒为零：$R(s,a,s')=0$。\n- 用于生成经验的行为策略 $\\pi_b$ 固定为以概率 $\\beta=0.95$ 选择 $a_{\\text{dashed}}$，以概率 $1-\\beta=0.05$ 选择 $a_{\\text{solid}}$。\n- 折扣因子是每个测试用例指定的参数 $\\gamma \\in [0, 1)$。\n\n动作价值函数 $Q(s,a)$ 由一个带有参数 $\\theta = \\{W_1, W_2\\}$ 的深度线性网络来近似。该网络有一个宽度为 $h=4$ 的隐藏层。\n- 输入是一个独热向量 $x(s,a) \\in \\mathbb{R}^{14}$，代表 $7 \\times 2 = 14$ 个可能的状态-动作对之一。\n- 网络参数是两个矩阵：$W_1 \\in \\mathbb{R}^{14 \\times 4}$ 和 $W_2 \\in \\mathbb{R}^{4 \\times 1}$。\n- 动作价值计算为 $Q_{\\theta}(s,a) = x(s,a)^{\\top} W_1 W_2$。\n\n学习过程涉及使用基于单个采样转移 $(s_t, a_t, r_t, s_{t+1})$（其中 $r_t=0$）的半梯度方法更新参数 $\\theta$。更新规则是在平方TD误差上的随机梯度下降步骤：\n$$ \\theta_{t+1} \\leftarrow \\theta_t - \\alpha \\cdot (Q_{\\theta_t}(s_t, a_t) - Y_t) \\cdot \\nabla_{\\theta_t} Q_{\\theta_t}(s_t, a_t) $$\n这里，$\\alpha$ 是学习率，$Y_t$ 是自举目标。算法之间的关键区别在于 $Y_t$ 的构建方式。设 $\\theta_t$ 为正在更新的在线参数，$\\bar{\\theta}_t$ 为一套独立的目标参数。\n- **普通半梯度控制：** 目标仅使用在线参数形成。\n  $$ Y_t^{\\text{plain}} = r_t + \\gamma \\max_{a'} Q_{\\theta_t}(s_{t+1}, a') $$\n- **目标网络变体：** 目标使用固定的目标网络参数 $\\bar{\\theta}_t$ 形成，这些参数每 $C$ 步更新一次以匹配在线参数 $\\theta_t$（即，如果 $t \\pmod C = 0$，则 $\\bar{\\theta}_{t} \\leftarrow \\theta_{t}$）。\n  $$ Y_t^{\\text{target}} = r_t + \\gamma \\max_{a'} Q_{\\bar{\\theta}_t}(s_{t+1}, a') $$\n- **双重动作价值变体：** 贪心动作使用在线网络选择，但其价值使用目标网络来估计。这也使用一个每 $C$ 步更新一次的目标网络。\n  $$ Y_t^{\\text{double}} = r_t + \\gamma Q_{\\bar{\\theta}_t}(s_{t+1}, \\arg\\max_{a'} Q_{\\theta_t}(s_{t+1}, a')) $$\n\n由于 $Q_{\\theta}(s,a)$ 是其参数的线性函数（以分解形式），梯度 $\\nabla_{\\theta} Q_{\\theta}(s,a)$ 是相对于 $W_1$ 和 $W_2$ 计算的。使用链式法则，我们有：\n$$ \\nabla_{W_1} Q_{\\theta}(s,a) = x(s,a) W_2^{\\top} $$\n$$ \\nabla_{W_2} Q_{\\theta}(s,a) = W_1^{\\top} x(s,a) $$\n鉴于 $x(s,a)$ 是一个在索引 $k$ 处为 1 的独热向量，梯度更新得以简化。对 $W_1$ 的更新只影响其第 $k$ 行，而对 $W_2$ 的更新则取决于 $W_1$ 的第 $k$ 行。\n\n为了评估稳定性，我们监控参数范数 $\\|\\theta_t\\|_2 = \\sqrt{\\|W_{1,t}\\|_F^2 + \\|W_{2,t}\\|_F^2}$ 的轨迹，其中 $\\|\\cdot\\|_F$ 是弗罗贝尼乌斯范数。如果满足以下两个条件之一，则宣布一次运行发散：\n1. 在任何步骤中，参数中检测到数值溢出（`NaN` 或 `inf`）。\n2. 参数表现出持续的指数增长。这在 $T$ 步运行结束时通过检查以下两点是否都为真来评估：\n   a. 最终范数与初始范数之比超过一个增长因子阈值：$\\|\\theta_T\\|_2 / \\|\\theta_0\\|_2 > G_{\\text{thresh}}$。我们设定 $G_{\\text{thresh}} = 1000$。\n   b. 对数范数 $\\log \\|\\theta_t\\|_2$ 关于时间步 $t$ 的线性回归产生一个大于阈值的正斜率：$m > m_{\\text{thresh}}$。我们设定 $m_{\\text{thresh}} = 10^{-5}$。为进行此拟合，范数会定期记录。\n\n对五个指定的测试用例中的每一个都进行计算实验。为保证可复现性，所有随机元素（初始权重、环境转移、行为策略选择）都源自一个用固定种子初始化的伪随机数生成器。每个测试用例都会重置种子，以确保条件可比。收集并报告每个用例的布尔结果（发散或未发散）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# No other libraries are imported, satisfying the constraints.\n\n# --- Problem Constants and Configuration ---\n# MDP parameters\nNUM_STATES = 7\nNUM_ACTIONS = 2\nUPPER_STATES = list(range(6))\nLOWER_STATE = 6\nACTION_SOLID = 0\nACTION_DASHED = 1\nBEHAVIOR_PROB_DASHED = 0.95\n\n# Function approximator architecture\nINPUT_DIM = 14\nHIDDEN_DIM = 4\nOUTPUT_DIM = 1\n\n# Divergence detection parameters\nGROWTH_FACTOR_THRESH = 1000.0\nSLOPE_THRESH = 1e-5\nNORM_LOG_INTERVAL = 100\n\n# Reproducibility\nSEED = 42\n\ndef step_env(state, action, rng):\n    \"\"\"Simulates one step in the MDP.\"\"\"\n    if state in UPPER_STATES:\n        if action == ACTION_SOLID:\n            next_state = LOWER_STATE\n        else:  # ACTION_DASHED\n            next_state = rng.choice(UPPER_STATES)\n    else:  # state == LOWER_STATE\n        next_state = rng.choice(UPPER_STATES)\n    return next_state\n\ndef select_behavior_action(rng):\n    \"\"\"Samples an action from the behavior policy.\"\"\"\n    if rng.random()  BEHAVIOR_PROB_DASHED:\n        return ACTION_DASHED\n    else:\n        return ACTION_SOLID\n\ndef get_q_values(state, W_eff):\n    \"\"\"Computes Q(state, a) for all actions 'a' given an effective weight matrix.\"\"\"\n    return W_eff[2 * state: 2 * state + 2, 0]\n\ndef run_experiment(algo, alpha, gamma, C, T, rng):\n    \"\"\"\n    Runs a single experiment for a given configuration.\n\n    Returns:\n        bool: True if divergence is detected, False otherwise.\n    \"\"\"\n    # 1. Initialize parameters\n    W1 = 0.1 + 0.01 * rng.standard_normal(size=(INPUT_DIM, HIDDEN_DIM))\n    W2 = 0.1 + 0.01 * rng.standard_normal(size=(HIDDEN_DIM, OUTPUT_DIM))\n    \n    W1_target = np.copy(W1)\n    W2_target = np.copy(W2)\n    \n    # 2. Initialize tracking variables\n    initial_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n    norm_history = []\n    steps_history = []\n    has_overflown = False\n    \n    # 3. Training Loop\n    state = rng.choice(UPPER_STATES)\n    for t in range(T):\n        # Sample transition from behavior policy\n        action = select_behavior_action(rng)\n        reward = 0.0\n        next_state = step_env(state, action, rng)\n\n        # Pre-compute effective weight matrices\n        W_online = W1 @ W2\n        W_target = W1_target @ W2_target\n\n        # Get Q-value for the current state-action pair\n        sa_index = 2 * state + action\n        q_sa = W_online[sa_index, 0]\n        \n        # Calculate target value Yt based on algorithm\n        q_next_online = get_q_values(next_state, W_online)\n        target_q_val = 0.0\n        \n        if algo == 'plain':\n            target_q_val = np.max(q_next_online)\n        elif algo == 'target':\n            q_next_target = get_q_values(next_state, W_target)\n            target_q_val = np.max(q_next_target)\n        elif algo == 'double':\n            q_next_target = get_q_values(next_state, W_target)\n            best_action_online = np.argmax(q_next_online)\n            target_q_val = q_next_target[best_action_online]\n\n        td_target = reward + gamma * target_q_val\n        td_error = q_sa - td_target\n\n        # Calculate gradients and perform semi-gradient update\n        grad_wrt_w1_row = W2.T\n        grad_wrt_w2_col = W1[sa_index, :].reshape(HIDDEN_DIM, 1)\n\n        W1[sa_index, :] -= alpha * td_error * grad_wrt_w1_row.flatten()\n        W2 -= alpha * td_error * grad_wrt_w2_col\n\n        # Update target network if applicable\n        if algo in ['target', 'double'] and (t + 1) % C == 0:\n            W1_target = np.copy(W1)\n            W2_target = np.copy(W2)\n        \n        # Check for numerical overflow\n        if not np.all(np.isfinite(W1)) or not np.all(np.isfinite(W2)):\n            has_overflown = True\n            break\n            \n        # Log parameter norm\n        if t % NORM_LOG_INTERVAL == 0:\n            current_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n            norm_history.append(current_norm)\n            steps_history.append(t)\n            \n        # Update state for next iteration\n        state = next_state\n\n    # 4. Divergence Analysis\n    if has_overflown:\n        return True\n\n    if not norm_history: # In case T is very small\n        return False\n\n    final_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n\n    # Avoid division by zero if initial norm is zero\n    if initial_norm > 1e-9:\n        norm_ratio = final_norm / initial_norm\n    else:\n        norm_ratio = np.inf if final_norm > 1e-9 else 1.0\n\n    # Fit linear model to log(norm) vs. steps\n    log_norms = np.log(np.maximum(norm_history, 1e-9)) # avoid log(0)\n    \n    # np.polyfit returns [slope, intercept] for deg=1\n    if len(steps_history) > 1:\n        slope = np.polyfit(steps_history, log_norms, 1)[0]\n    else:\n        slope = 0.0\n\n    # Apply divergence criteria\n    if norm_ratio > GROWTH_FACTOR_THRESH and slope > SLOPE_THRESH:\n        return True\n        \n    return False\n\ndef solve():\n    \"\"\"\n    Main function to run the suite of experiments and print results.\n    \"\"\"\n    # Parameter cases from the problem statement\n    test_cases = [\n        # (algo, alpha, gamma, C, T)\n        ('plain', 0.20, 0.99, 1, 20000),\n        ('target', 0.20, 0.99, 50, 20000),\n        ('target', 0.01, 0.99, 50, 20000),\n        ('double', 0.05, 0.99, 50, 20000),\n        ('plain', 0.20, 0.00, 1, 10000)\n    ]\n\n    results = []\n    for case in test_cases:\n        # Reset the seed for each run to ensure comparable conditions\n        rng = np.random.default_rng(SEED)\n        diverged = run_experiment(*case, rng=rng)\n        results.append(str(diverged))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3163145"}, {"introduction": "在见证了Q学习可能出现的不稳定性之后，我们来深入探索其解决方案背后的数学原理。目标网络是保证深度Q学习稳定性的关键技术之一，但它为何有效？这个练习 [@problem_id:3163146] 让你通过实验来揭示答案，你将经验性地测量Q学习更新算子的性质，并探索像波利亚克平均（Polyak averaging）这样的软更新机制如何帮助确保更新算子成为一个“压缩映射”（contraction mapping）。这个练习将一个实用的工程技巧与其优美的数学保证联系起来，加深你对算法稳定性的理解。", "problem": "考虑一个具有 $S$ 个状态和 $A$ 个动作的有限马尔可夫决策过程 (MDP)。令状态空间表示为 $\\mathcal{S} = \\{0, 1, \\dots, S-1\\}$，动作空间表示为 $\\mathcal{A} = \\{0, 1, \\dots, A-1\\}$。对于每个动作 $a \\in \\mathcal{A}$，转移概率张量为 $\\mathbf{P} \\in \\mathbb{R}^{S \\times A \\times S}$，其中条目 $\\mathbf{P}[s,a,s']$ 表示在状态 $s \\in \\mathcal{S}$ 下采取动作 $a$ 时转移到状态 $s' \\in \\mathcal{S}$ 的概率，且对于所有的 $(s,a)$ 满足 $\\sum_{s' = 0}^{S-1} \\mathbf{P}[s,a,s'] = 1$。奖励函数为 $R \\in \\mathbb{R}^{S \\times A}$，其条目有界 $R[s,a] \\in [-1,1]$。设 $\\gamma \\in (0,1)$ 为折扣因子。\n\n定义动作价值函数 $Q \\in \\mathbb{R}^{S \\times A}$ 及其滞后的目标网络 $\\bar{Q} \\in \\mathbb{R}^{S \\times A}$。更新过程使用带有目标网络延迟的同步版Q学习目标，其中“在线”估计 $Q$朝着使用 $\\bar{Q}$ 计算的最优贝尔曼目标进行更新，而目标网络则通过系数为 $\\tau \\in [0,1]$ 的Polyak平均法进行更新。学习率 $\\alpha \\in (0,1]$ 用于控制步长。\n\n目标是经验性地测量目标网络延迟 $\\tau$ 如何影响将 $(Q, \\bar{Q})$ 映射到 $(Q', \\bar{Q}')$ 的组合算子在单次同步更新步骤中的收缩性质，并经验性地绘制出不同 $\\tau$ 值下的稳定区域。稳定区域定义为那些使得串联对 $(Q, \\bar{Q})$ 在无穷范数下的经验利普希茨比率小于 $1$ 的 $\\tau$ 值所构成的集合。\n\n您的任务是编写一个完整、可运行的程序，该程序：\n- 使用固定的伪随机种子构建具有可复现的转移张量 $\\mathbf{P}$ 和奖励矩阵 $R$ 的合成MDP，确保 $\\mathbf{P}$ 的行是有效的概率分布且 $R[s,a] \\in [-1,1]$。\n- 对于每个测试用例，扫描一个有限的 $\\tau$ 值网格，并对每个 $\\tau$ 值，通过以下方式估计单步映射的经验利普希茨比率 $c_{\\tau}$：\n    - 抽样多个随机对 $(Q^{(1)}, \\bar{Q}^{(1)})$ 和 $(Q^{(2)}, \\bar{Q}^{(2)})$，其条目在有界区间内。\n    - 使用相同的 $\\tau$、$\\alpha$、$\\gamma$、$\\mathbf{P}$ 和 $R$ 对这两个对应用一个同步更新步骤。\n    - 计算比率 $c = \\frac{\\left\\| (Q^{(1)\\prime} - Q^{(2)\\prime}, \\bar{Q}^{(1)\\prime} - \\bar{Q}^{(2)\\prime}) \\right\\|_{\\infty}}{\\left\\| (Q^{(1)} - Q^{(2)}, \\bar{Q}^{(1)} - \\bar{Q}^{(2)}) \\right\\|_{\\infty}}$，并取样本中的最大值来估计 $c_{\\tau}$。\n- 为每个测试用例确定网格中经验性收缩（即 $c_{\\tau}  1$）的 $\\tau$ 值的比例、网格上所有 $\\tau$ 的最差情况比率 $\\max_{\\tau}$，以及一个布尔值，指示是否存在至少一个经验性收缩的 $\\tau$ 值。\n\n使用以下测试套件，其中每个用例由 $(S, A, \\gamma, \\alpha, \\text{seed}, \\text{tau\\_grid})$ 定义：\n- 用例 1：$(S = 4, A = 2, \\gamma = 0.9, \\alpha = 0.5, \\text{seed} = 11, \\text{tau\\_grid} = [0.0, 0.01, 0.05, 0.1, 0.5, 0.9, 1.0])$。\n- 用例 2：$(S = 5, A = 3, \\gamma = 0.95, \\alpha = 0.7, \\text{seed} = 77, \\text{tau\\_grid} = [0.0, 0.001, 0.01, 0.1, 0.5, 1.0])$。\n- 用例 3：$(S = 3, A = 4, \\gamma = 0.99, \\alpha = 1.0, \\text{seed} = 123, \\text{tau\\_grid} = [0.0, 0.01, 0.1, 0.5, 1.0])$。\n- 用例 4：$(S = 6, A = 2, \\gamma = 0.3, \\alpha = 0.5, \\text{seed} = 2021, \\text{tau\\_grid} = [0.0, 0.05, 0.1, 0.5, 1.0])$。\n\n实现细节：\n- 对于每个测试用例，使用指定的种子，通过采样非负条目并将每个 $(s,a,\\cdot)$ 切片归一化以使其总和为 $1$ 来生成 $\\mathbf{P}$，并通过在 $[-1,1]$ 区间内均匀采样来生成 $R$。\n- 对每个 $\\tau$ 使用固定数量的随机样本对，记为 $N_{\\text{pairs}}$，其中 $N_{\\text{pairs}} = 50$，为 $Q$ 和 $\\bar{Q}$ 从 $[-1,1]$ 区间内均匀采样条目。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每一项对应一个测试用例，并且本身是一个包含三个值的列表：$[\\text{fraction\\_stable}, \\text{worst\\_ratio}, \\text{has\\_stable}]$。例如，输出应类似于 $[[0.5,0.97,True],[0.3333333333333333,1.02,False],\\dots]$。\n\n不涉及角度、物理单位或百分比；所有输出都必须是无单位的。程序必须是自包含的，不得读取任何输入，并且必须严格遵守指定的执行环境。[@problem_id:483]", "solution": "用户提供的问题已经过验证，被认为是科学上合理、定义明确且完整的。我现在将提供一个完整的解决方案。\n\n该问题要求对包含目标网络的同步Q学习更新的稳定性进行经验性研究。稳定性是通过检验在线网络 $Q$ 和目标网络 $\\bar{Q}$ 的组合更新算子是否为收缩映射来评估的。这一性质对于保证强化学习算法的收敛至关重要。通过改变控制目标网络更新的参数 $\\tau$，来绘制出稳定区域。\n\n我们学习系统的状态是动作价值表对 $(Q, \\bar{Q})$，其中 $Q, \\bar{Q} \\in \\mathbb{R}^{S \\times A}$。问题定义了一个单步同步更新算子，我们称之为 $F_{\\tau}$，它将当前对 $(Q, \\bar{Q})$ 映射到一个新的对 $(Q', \\bar{Q}')$。我们将逐步形式化这个算子。\n\n首先，使用目标网络 $\\bar{Q}$ 计算目标值 $Y \\in \\mathbb{R}^{S \\times A}$。对于一个状态-动作对 $(s,a)$，目标是即时奖励加上下一状态中折扣后的期望最大动作价值。\n$$\nY[s,a] = R[s,a] + \\gamma \\sum_{s' = 0}^{S-1} \\mathbf{P}[s,a,s'] \\left( \\max_{a' = 0}^{A-1} \\bar{Q}[s', a'] \\right)\n$$\n此计算对所有 $(s,a) \\in \\mathcal{S} \\times \\mathcal{A}$ 执行。\n\n其次，在线网络 $Q$ 使用学习率 $\\alpha \\in (0,1]$ 朝着这个目标 $Y$ 更新。这是旧Q值和新目标的凸组合：\n$$\nQ' = (1-\\alpha) Q + \\alpha Y\n$$\n\n第三，目标网络 $\\bar{Q}$ 使用Polyak平均法进行更新，这是一种软更新，使其朝着新计算的在线网络 $Q'$ 移动。更新由系数 $\\tau \\in [0,1]$ 控制。\n$$\n\\bar{Q}' = (1-\\tau) \\bar{Q} + \\tau Q'\n$$\n$\\tau=1$ 的值对应于“硬”更新，即目标网络直接从在线网络复制 ($\\bar{Q}' = Q'$)。$\\tau=0$ 的值意味着目标网络被冻结 ($\\bar{Q}' = \\bar{Q}$)。\n\n完整的单步算子 $F_{\\tau}: (Q, \\bar{Q}) \\mapsto (Q', \\bar{Q}')$ 由这两个更新的序列定义。将 $Q'$ 的表达式代入 $\\bar{Q}'$ 的更新中，揭示了系统的耦合性质：\n$$\nF_{\\tau}(Q, \\bar{Q}) = \\left( (1-\\alpha) Q + \\alpha Y, \\quad (1-\\tau) \\bar{Q} + \\tau \\left( (1-\\alpha) Q + \\alpha Y \\right) \\right)\n$$\n其中 $Y$ 是 $\\bar{Q}$ 的函数。\n\n为了分析稳定性，我们研究算子在无穷范数下的利普希茨常数 $c_{\\tau}$。我们系统状态的空间是 $\\mathbb{R}^{S \\times A} \\times \\mathbb{R}^{S \\times A}$。该空间中一个元素 $(Q, \\bar{Q})$ 的无穷范数定义为其所有分量绝对值的最大值：\n$$\n\\|(Q, \\bar{Q})\\|_{\\infty} = \\max\\left( \\max_{s,a} |Q[s,a]|, \\max_{s,a} |\\bar{Q}[s,a]| \\right)\n$$\n利普希茨常数 $c_{\\tau}$ 是满足以下条件的最小非负数，对于任意两个对 $(Q_1, \\bar{Q}_1)$ 和 $(Q_2, \\bar{Q}_2)$：\n$$\n\\| F_{\\tau}(Q_1, \\bar{Q}_1) - F_{\\tau}(Q_2, \\bar{Q}_2) \\|_{\\infty} \\le c_{\\tau} \\| (Q_1, \\bar{Q}_1) - (Q_2, \\bar{Q}_2) \\|_{\\infty}\n$$\n如果 $c_{\\tau}  1$，则该算子是收缩映射。根据巴拿赫不动点定理，重复应用 $F_{\\tau}$ 保证会收敛到一个唯一的不动点，这意味着学习动态是稳定的。\n\n任务是通过为多个随机抽样的对计算输出距离与输入距离的比率，并取最大值来经验性地估计 $c_{\\tau}$：\n$$\n\\hat{c}_{\\tau} = \\max_{i=1,\\dots,N_{\\text{pairs}}} \\frac{\\|F_{\\tau}(Q_1^{(i)}, \\bar{Q}_1^{(i)}) - F_{\\tau}(Q_2^{(i)}, \\bar{Q}_2^{(i)})\\|_{\\infty}}{\\|(Q_1^{(i)}, \\bar{Q}_1^{(i)}) - (Q_2^{(i)}, \\bar{Q}_2^{(i)})\\|_{\\infty}}\n$$\n这个经验估计 $\\hat{c}_{\\tau}$ 作为真实利普希茨常数 $c_{\\tau}$ 的一个下界。对于给定的系统参数，一个小于1的 $\\hat{c}_{\\tau}$ 值为算子是收缩的提供了强有力的证据。\n\n算法流程如下：\n对于由 $(S, A, \\gamma, \\alpha, \\text{seed}, \\text{tau\\_grid})$ 指定的每个测试用例：\n1.  使用给定的 `seed` 设置一个伪随机数生成器。\n2.  通过生成转移张量 $\\mathbf{P} \\in \\mathbb{R}^{S \\times A \\times S}$（通过采样和归一化）和奖励矩阵 $R \\in \\mathbb{R}^{S \\times A}$（通过从 $[-1,1]$ 采样）来合成MDP。\n3.  采样 $N_{\\text{pairs}} = 50$ 对Q表，$(Q_1^{(i)}, \\bar{Q}_1^{(i)})$ 和 $(Q_2^{(i)}, \\bar{Q}_2^{(i)})$，其条目从 $[-1,1]$ 均匀抽取。\n4.  对于 `tau_grid` 中的每个 $\\tau$：\n    a. 对于50个抽样初始对中的每一个，计算输入距离 $\\| (Q_1 - Q_2, \\bar{Q}_1 - \\bar{Q}_2) \\|_{\\infty}$。\n    b. 将单步更新算子 $F_{\\tau}$ 应用于 $(Q_1, \\bar{Q}_1)$ 和 $(Q_2, \\bar{Q}_2)$，以获得它们的像 $(Q'_1, \\bar{Q}'_1)$ 和 $(Q'_2, \\bar{Q}'_2)$。\n    c. 计算输出距离 $\\| (Q'_1 - Q'_2, \\bar{Q}'_1 - \\bar{Q}'_2) \\|_{\\infty}$。\n    d. 计算输出距离与输入距离的比率。\n    e. 该 $\\tau$ 的经验利普希茨常数 $\\hat{c}_{\\tau}$ 是这50个对的比率中的最大值。\n5.  扫描完所有 $\\tau$ 值后，确定其中 $\\hat{c}_{\\tau}  1$ 的比例 (`fraction_stable`)、在所有 $\\tau$ 中找到的总体最大比率 (`worst_ratio`)，以及是否找到了至少一个稳定的 $\\tau$ (`has_stable`)。\n6.  收集并格式化每个测试用例的这三个结果，作为单行输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (S, A, gamma, alpha, seed, tau_grid)\n        (4, 2, 0.9, 0.5, 11, [0.0, 0.01, 0.05, 0.1, 0.5, 0.9, 1.0]),\n        (5, 3, 0.95, 0.7, 77, [0.0, 0.001, 0.01, 0.1, 0.5, 1.0]),\n        (3, 4, 0.99, 1.0, 123, [0.0, 0.01, 0.1, 0.5, 1.0]),\n        (6, 2, 0.3, 0.5, 2021, [0.0, 0.05, 0.1, 0.5, 1.0]),\n    ]\n    \n    N_PAIRS = 50\n    Q_BOUND = 1.0\n\n    all_results = []\n\n    for s_dim, a_dim, gamma, alpha, seed, tau_grid in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct the MDP\n        # Transition tensor P\n        P = rng.random((s_dim, a_dim, s_dim))\n        P /= P.sum(axis=2, keepdims=True)\n        # Reward matrix R\n        R = rng.uniform(-Q_BOUND, Q_BOUND, size=(s_dim, a_dim))\n\n        # 2. Sample Q-table pairs for the experiment\n        q_pairs = []\n        for _ in range(N_PAIRS):\n            Q1 = rng.uniform(-Q_BOUND, Q_BOUND, size=(s_dim, a_dim))\n            Q_bar1 = rng.uniform(-Q_BOUND, Q_BOUND, size=(s_dim, a_dim))\n            Q2 = rng.uniform(-Q_BOUND, Q_BOUND, size=(s_dim, a_dim))\n            Q_bar2 = rng.uniform(-Q_BOUND, Q_BOUND, size=(s_dim, a_dim))\n            q_pairs.append(((Q1, Q_bar1), (Q2, Q_bar2)))\n\n        c_tau_list = []\n        for tau in tau_grid:\n            max_ratio_for_tau = 0.0\n            \n            for (Q1, Q_bar1), (Q2, Q_bar2) in q_pairs:\n                # 3. Compute input norm\n                delta_Q = Q1 - Q2\n                delta_Q_bar = Q_bar1 - Q_bar2\n\n                norm_in = max(np.max(np.abs(delta_Q)), np.max(np.abs(delta_Q_bar)))\n\n                if norm_in  1e-9:\n                    continue  # Avoid division by zero for identical inputs\n\n                # 4. Apply one update step to both pairs\n                Q1_prime, Q_bar1_prime = update_step(Q1, Q_bar1, P, R, gamma, alpha, tau)\n                Q2_prime, Q_bar2_prime = update_step(Q2, Q_bar2, P, R, gamma, alpha, tau)\n                \n                # 5. Compute output norm\n                delta_Q_prime = Q1_prime - Q2_prime\n                delta_Q_bar_prime = Q_bar1_prime - Q_bar2_prime\n\n                norm_out = max(np.max(np.abs(delta_Q_prime)), np.max(np.abs(delta_Q_bar_prime)))\n\n                # 6. Compute ratio and update max\n                ratio = norm_out / norm_in\n                if ratio > max_ratio_for_tau:\n                    max_ratio_for_tau = ratio\n            \n            c_tau_list.append(max_ratio_for_tau)\n            \n        # 7. Aggregate results for the test case\n        stable_count = sum(1 for c in c_tau_list if c  1.0)\n        fraction_stable = stable_count / len(tau_grid)\n        worst_ratio = max(c_tau_list) if c_tau_list else 0.0\n        has_stable = stable_count > 0\n\n        all_results.append([fraction_stable, worst_ratio, has_stable])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef update_step(Q, Q_bar, P, R, gamma, alpha, tau):\n    \"\"\"\n    Performs one synchronous update step for the (Q, Q_bar) pair.\n    \n    Args:\n        Q (np.ndarray): Online action-value function of shape (S, A).\n        Q_bar (np.ndarray): Target action-value function of shape (S, A).\n        P (np.ndarray): Transition probability tensor of shape (S, A, S).\n        R (np.ndarray): Reward function of shape (S, A).\n        gamma (float): Discount factor.\n        alpha (float): Learning rate for the Q-network.\n        tau (float): Polyak averaging coefficient for the target network.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The updated pair (Q', Q_bar').\n    \"\"\"\n    s_dim, a_dim = Q.shape\n    \n    # Compute the Bellman target Y using the target network Q_bar\n    # V_bar_max[s'] = max_{a'} Q_bar[s', a']\n    V_bar_max = np.max(Q_bar, axis=1) # Shape: (S,)\n    \n    # Expected future value: sum_{s'} P(s'|s,a) * V_bar_max[s']\n    # Using einsum for clarity and efficiency.\n    expected_future_value = np.einsum('ijk,k->ij', P, V_bar_max) # Shape: (S, A)\n    \n    # Y[s,a] = R[s,a] + gamma * E[V_bar_max]\n    Y = R + gamma * expected_future_value # Shape: (S, A)\n    \n    # Update online network Q\n    Q_prime = (1 - alpha) * Q + alpha * Y\n    \n    # Update target network Q_bar\n    Q_bar_prime = (1 - tau) * Q_bar + tau * Q_prime\n    \n    return Q_prime, Q_bar_prime\n\nsolve()\n```", "id": "3163146"}, {"introduction": "除了更新规则本身，我们用于训练的数据同样对算法的稳定性至关重要，尤其是在处理具有高方差的更新时。这个实践 [@problem_id:3163134] 将带你超越简单的均匀经验回放，探索更高级的采样策略。你将实现并比较“优先经验回放”（Prioritized Experience Replay, PER）与一种新颖的平衡采样方法，分析它们如何处理由“意外”转换（即高TD误差的样本）带来的挑战，并减轻梯度被少数异常样本主导的风险。", "problem": "要求您设计并评估一种用于基于价值的强化学习的回放缓冲与采样策略，该策略能显式控制时间差分（TD）误差幅度的分布。您的程序必须为一个与线性状态-动作价值函数（也称为线性 $Q$ 函数）一同使用的回放缓冲区实现并比较三种采样器，并定量评估平衡的 TD 误差分布是否能减轻离群值造成的梯度支配问题并稳定 $Q$ 更新。该比较必须在一个固定的合成数据集上进行，该数据集由一个具有两个动作和连续特征的 Markov 决策过程（MDP）生成。\n\n您必须从以下基础出发：\n\n- 一个 Markov 决策过程（MDP）拥有一个状态空间 $\\mathcal{S}$、一个动作空间 $\\mathcal{A}$、一个转移核 $p(\\mathbf{s}^{\\prime}\\mid \\mathbf{s}, a)$、一个奖励函数 $r(\\mathbf{s}, a)$ 和一个折扣因子 $\\gamma \\in (0,1)$。\n- 某个策略的动作价值函数（$Q$ 函数）为 $Q(\\mathbf{s}, a) = \\mathbb{E}\\big[\\sum_{t=0}^{\\infty} \\gamma^{t} r(\\mathbf{s}_{t}, a_{t}) \\,\\big|\\, \\mathbf{s}_{0}=\\mathbf{s}, a_{0}=a\\big]$。\n- 对于使用目标网络的单步 $Q$ 学习，其时间差分（TD）误差定义为 $\\delta = r + \\gamma \\max_{a^{\\prime}} Q_{\\text{tgt}}(\\mathbf{s}^{\\prime}, a^{\\prime}) - Q(\\mathbf{s}, a)$，其中 $Q_{\\text{tgt}}$ 是一个在几次参数更新期间保持固定的目标 $Q$ 函数。\n- 在函数逼近中，当使用平方 TD 误差损失 $\\ell = \\tfrac{1}{2}\\delta^{2}$ 并且目标被视为常数时，梯度步长与 $\\delta \\,\\nabla_{\\boldsymbol{\\theta}} Q(\\mathbf{s}, a; \\boldsymbol{\\theta})$ 成正比。\n\n您的程序必须：\n\n- 构建一个大小为 $N$ 的固定转移数据集 $(\\mathbf{s}, a, r, \\mathbf{s}^{\\prime})$，其中 $\\mathbf{s} \\in \\mathbb{R}^{d}$，动作 $a \\in \\{0,1\\}$ 有两种，奖励 $r$ 是动作增广特征的线性函数加上加性噪声。该噪声必须是一个带有离群值模式的混合分布，以产生重尾误差情景。动作必须是均匀分布的。每个测试用例的数据集必须只生成一次，并在该测试用例的所有采样器中重复使用，以确保公平比较。使用固定的随机种子以保证可复现性。\n- 使用线性 $Q$ 函数 $Q(\\mathbf{s}, a; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a)$，其特征 $\\boldsymbol{\\phi}(\\mathbf{s}, a) \\in \\mathbb{R}^{2d}$ 的定义为：根据动作 $a$ 将 $\\mathbf{s}$ 拼接到前 $d$ 个坐标或后 $d$ 个坐标。将 $\\boldsymbol{\\theta}$ 初始化为零。\n- 每个采样器使用一个折扣因子 $\\gamma$、一个学习率 $\\eta$、一个小批量大小 $B$、总共 $T$ 个参数更新步骤，以及一个目标网络更新周期 $K$ 步。目标网络参数 $\\boldsymbol{\\theta}_{\\text{tgt}}$ 必须每 $K$ 步与 $\\boldsymbol{\\theta}$ 同步一次。\n- 在回放缓冲区中为每次转移维护一个 TD 误差绝对值大小 $|\\delta|$ 的估计，并在训练期间为所有样本定期重新计算此值，以保持采样统计数据最新。\n\n在回放缓冲区上实现三种采样策略：\n\n- 均匀采样（Uniform）：以相等的概率对每次转移进行采样。\n- 比例化优先经验回放（Proportional Prioritized Experience Replay, PER）：以与 $(|\\delta_{i}| + \\varepsilon)^{\\alpha}$ 成正比的概率对转移 $i$ 进行采样，其中 $\\alpha \\in [0,1]$ 控制优先化强度，$\\varepsilon$ 是一个小的正数以避免零概率。\n- 平衡分位数 TD 采样器（Balanced-Quantile TD Sampler）（待评估的方法）：将 $|\\delta|$ 的分布计算出经验分位数箱的边界，分成 $Q$ 个等质量的箱，并从每个箱中均匀抽取大约 $B/Q$ 个样本（如有需要可进行有放回抽样），以使小批量包含跨 TD 误差幅度的平衡混合。\n\n为每个采样器在训练期间定义并计算以下定量诊断指标：\n\n- 离群值梯度支配度量：在每个更新步骤 $t$，为小批量中的 $B$ 个样本计算每个样本的贡献范数 $\\|\\delta_{i} \\,\\boldsymbol{\\phi}(\\mathbf{s}_{i}, a_{i})\\|_{2}$。令 $k=\\lceil 0.1 B \\rceil$ 为前十分位数的计数。令 $G_{t}$ 为前 $k$ 个样本的这些范数之和除以所有 $B$ 个样本的范数之和（使用一个微小的正稳定项以避免除以零）。单次运行的支配度量是在训练步骤的后半段（以排除早期瞬态）对 $G_{t}$ 求平均值。\n- $Q$ 更新稳定性度量：在每一步 $t$，计算平均小批量损失 $\\bar{\\ell}_{t} = \\tfrac{1}{2}\\tfrac{1}{B}\\sum_{i=1}^{B}\\delta_{i}^{2}$。单次运行的稳定性度量是在初始预烧期之后所有步骤中 $\\bar{\\ell}_{t}$ 的样本方差。\n\n对于每个测试用例，您的程序必须为三种采样器独立运行训练，并报告一组由三个决策布尔值构成的结果：\n\n- $b_{1}$：如果平衡分位数采样器相比 PER 减少的梯度支配超过一个小的容差，即如果 $\\overline{G}_{\\text{balanced}}  \\overline{G}_{\\text{PER}} - \\tau_{g}$，则为 True，否则为 False。\n- $b_{2}$：如果平衡分位数采样器相比 PER 减少的损失方差超过一个小的容差，即如果 $\\mathrm{Var}(\\bar{\\ell})_{\\text{balanced}}  \\mathrm{Var}(\\bar{\\ell})_{\\text{PER}} - \\tau_{v}$，则为 True，否则为 False。\n- $b_{3}$：如果当 $Q=1$ 时，平衡分位数采样器在支配度量上退化为均匀采样（在一个容差范围内），即如果 $|\\overline{G}_{\\text{balanced}} - \\overline{G}_{\\text{uniform}}| \\le \\tau_{m}$，则为 True，否则为 False。对于 $Q \\ne 1$ 的情况，将 $b_{3}$ 设置为 False。\n\n所有运行的通用参数设置：\n\n- 特征维度 $d = 4$。\n- 数据集大小 $N = 3000$。\n- 折扣因子 $\\gamma = 0.97$。\n- 学习率 $\\eta = 0.01$。\n- 总更新步骤 $T = 400$。\n- 目标更新周期 $K = 20$。\n- 所有转移的 TD 误差表重新计算周期为每 10 步。\n- PER 参数：指数 $\\alpha = 0.95$，偏移量 $\\varepsilon = 10^{-3}$。\n- 度量计算的稳定项可以在分母中使用 $\\epsilon = 10^{-12}$。\n\n测试套件。您的程序必须执行以下三个测试用例：\n\n- 情况 A（重尾噪声）：离群值概率 $p_{\\mathrm{out}} = 0.2$，离群值尺度因子 $s_{\\mathrm{out}} = 8.0$，平衡采样器箱数 $Q = 5$，批量大小 $B = 64$。\n- 情况 B（无离群值）：离群值概率 $p_{\\mathrm{out}} = 0.0$，离群值尺度因子 $s_{\\mathrm{out}} = 8.0$，平衡采样器箱数 $Q = 5$, 批量大小 $B = 64$。\n- 情况 C（退化的平衡采样）：离群值概率 $p_{\\mathrm{out}} = 0.2$，离群值尺度因子 $s_{\\mathrm{out}} = 8.0$，平衡采样器箱数 $Q = 1$，批量大小 $B = 64$。\n\n奖励噪声模型。设基础噪声为标准差 $\\sigma_{\\mathrm{base}} = 0.1$ 的高斯噪声。以概率 $p_{\\mathrm{out}}$，将基础噪声替换为标准差为 $\\sigma_{\\mathrm{out}} = s_{\\mathrm{out}} \\cdot \\sigma_{\\mathrm{base}}$ 的高斯噪声。\n\n用于决定布尔值的输出容差：\n\n- 梯度支配容差 $\\tau_{g} = 0.02$。\n- 损失方差容差 $\\tau_{v} = 10^{-5}$。\n- 退化匹配容差 $\\tau_{m} = 0.03$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个按 $[b_{1}, b_{2}, b_{3}]$ 顺序排列的包含三个布尔值的列表。例如：\"[[True,False,False],[False,False,False],[True,True,True]]\"。除此单行外，不得有任何多余的空格或字符。\n\n角度单位不适用。此问题中没有物理单位；所有量均为无量纲实数。本问题陈述中的所有数值必须严格按照规定使用。", "solution": "该问题陈述是有效的。它指定了一个在强化学习领域定义明确的计算实验，该实验具有科学依据、客观，并包含足够的信息以产生唯一、可验证的结果。关于精确的 Markov 决策过程（MDP）动态的次要模糊之处，已通过问题强调在固定的、生成的数据集上比较采样器而得以解决，这允许为这些底层函数做出合理的、固定的选择。\n\n目标是实现并比较三种经验回放采样策略——均匀采样（Uniform）、优先经验回放（Prioritized Experience Replay, PER）和一种提议的平衡分位数 TD 采样器（Balanced-Quantile TD Sampler）——以评估它们对 Q 学习稳定性的影响。评估将基于两个度量：离群值引起的梯度支配和 Q 更新的损失方差。\n\n首先，我们建立合成数据生成过程。环境是一个 Markov 决策过程，具有连续状态空间 $\\mathcal{S} \\subseteq \\mathbb{R}^{d}$（其中 $d=4$）和离散动作空间 $\\mathcal{A} = \\{0, 1\\}$。对于每个测试用例，会生成一个包含 $N=3000$ 次转移 $(\\mathbf{s}, a, r, \\mathbf{s}^{\\prime})$ 的固定数据集。状态向量 $\\mathbf{s}$ 从标准多元正态分布 $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$ 中独立抽取。动作 $a$ 从 $\\{0, 1\\}$ 中均匀选择。转移动态定义为一个简单的线性变换 $\\mathbf{s}^{\\prime} = \\mathbf{M}_a \\mathbf{s}$，其中 $\\mathbf{M}_0$ 和 $\\mathbf{M}_1$ 是为每个测试用例生成一次的固定随机矩阵，并通过乘以 $1/\\sqrt{d}$ 进行缩放，以防止状态的范数爆炸。奖励 $r$ 是动作增广特征的线性函数加上噪声：$r(\\mathbf{s}, a) = \\boldsymbol{w}_r^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a) + \\epsilon_r$，其中 $\\boldsymbol{w}_r$ 是一个固定的随机权重向量。噪声 $\\epsilon_r$ 从一个混合模型中抽取：以 $1-p_{\\text{out}}$ 的概率，它来自基础高斯分布 $\\mathcal{N}(0, \\sigma_{\\text{base}}^2)$（其中 $\\sigma_{\\text{base}}=0.1$）；以 $p_{\\text{out}}$ 的概率，它来自离群值分布 $\\mathcal{N}(0, \\sigma_{\\text{out}}^2)$（其中 $\\sigma_{\\text{out}} = s_{\\text{out}} \\cdot \\sigma_{\\text{base}}$）。这种设置，特别是当 $p_{\\text{out}}  0$ 时，旨在创建时间差分（TD）误差的重尾分布。在同一个测试用例中，所有三种采样器都使用相同的数据集，以确保公平比较。\n\n学习智能体使用线性函数逼近器来表示动作价值函数：$Q(\\mathbf{s}, a; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a)$。特征向量 $\\boldsymbol{\\phi}(\\mathbf{s}, a) \\in \\mathbb{R}^{2d}$ 通过根据动作将状态向量 $\\mathbf{s}$ 拼接到两个位置之一来定义：$\\boldsymbol{\\phi}(\\mathbf{s}, a=0) = [\\mathbf{s}^{\\top}, \\mathbf{0}_d^{\\top}]^{\\top}$ 和 $\\boldsymbol{\\phi}(\\mathbf{s}, a=1) = [\\mathbf{0}_d^{\\top}, \\mathbf{s}^{\\top}]^{\\top}$。参数 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2d}$ 被初始化为零。学习通过对均方 TD 误差 $\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{2}\\mathbb{E}[ (r + \\gamma \\max_{a'} Q(\\mathbf{s}', a'; \\boldsymbol{\\theta}_{\\text{tgt}}) - Q(\\mathbf{s}, a; \\boldsymbol{\\theta}))^2 ]$ 进行小批量随机梯度下降来进行。对于一个大小为 $B$ 的小批量，梯度更新为 $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\frac{\\eta}{B} \\sum_{i=1}^{B} \\delta_i \\boldsymbol{\\phi}(\\mathbf{s}_i, a_i)$，其中 $\\eta=0.01$ 是学习率。一个带有参数 $\\boldsymbol{\\theta}_{\\text{tgt}}$ 的目标网络被用来计算 TD 目标 $y = r + \\gamma \\max_{a'} Q_{\\text{tgt}}(\\mathbf{s}', a')$，它每 $K=20$ 步与 $\\boldsymbol{\\theta}$ 同步一次，以稳定学习过程。样本 $i$ 的 TD 误差为 $\\delta_i = y_i - Q(\\mathbf{s}_i, a_i; \\boldsymbol{\\theta})$。\n\n研究的核心在于三种采样策略：\n1.  **均匀采样**：这是基线策略，回放缓冲区中的 $N$ 次转移中的每一次都以相等的概率 $1/N$ 被采样。它不对任何给定转移的效用做出任何假设。\n2.  **比例化优先经验回放 (PER)**：该方法优先处理那些智能体能从中学到最多的转移，其假设是 TD 误差的幅度 $|\\delta|$ 是“惊奇”或学习潜力的良好代理指标。一次转移 $i$ 以概率 $P_i \\propto (|\\delta_i| + \\varepsilon)^{\\alpha}$ 被采样，其中优先化指数 $\\alpha=0.95$，小常数 $\\varepsilon=10^{-3}$ 用以确保概率不为零。该策略会过采样高误差的转移。为了实现这一点，所有 $N$ 次转移的绝对 TD 误差 $|\\delta_i|$ 每 10 步重新评估一次。\n3.  **平衡分位数 TD 采样器**：这个被提出的策略旨在减轻由 PER 关注高误差离群值所导致的训练不稳定风险。它的目标是构建在 TD 误差幅度方面具有多样性的小批量。其操作方式是根据 $|\\delta_i|$ 的分布将整个回放缓冲区划分为 $Q$ 个分位数箱。首先，所有 $N$ 次转移都按其当前的 $|\\delta_i|$ 值进行排序。然后将排序后的索引列表分割成 $Q$ 个连续的块，每个块包含 $N/Q$ 个索引，从而形成等质量的箱。为了构成一个大小为 $B$ 的小批量，从这 $Q$ 个箱中的每一个均匀地（有放回地）抽取大约 $B/Q$ 个样本。这确保了小批量的梯度是低、中、高误差样本贡献的平均值，从而促进更稳定的更新。在 $Q=1$ 的特定情况下，单个箱包含所有样本，此时该方法等同于均匀采样。\n\n为了量化每个采样器的性能，在 $T=400$ 个训练步骤的后半部分（即在 200 个步骤的预烧期之后）计算两个度量：\n-   **梯度支配度量 ($\\overline{G}$)**：对于每次小批量更新，我们计算每个样本对梯度更新贡献的范数 $\\|\\delta_i \\boldsymbol{\\phi}(\\mathbf{s}_i, a_i)\\|_2$。每步度量 $G_t$ 是来自前十分位数（前 $\\lceil 0.1 B \\rceil$ 个样本）的范数总和与批次中所有范数总和的比率。高值表示少数离群样本正在支配梯度。最终度量 $\\overline{G}$ 是在评估期间 $G_t$ 的平均值。\n-   **Q 更新稳定性度量 ($\\mathrm{Var}(\\bar{\\ell})$)**：对于每个小批量，计算均方 TD 误差（损失）：$\\bar{\\ell}_t = \\frac{1}{2B}\\sum_{i=1}^B \\delta_i^2$。在评估期间计算此损失序列的样本方差 $\\mathrm{Var}(\\bar{\\ell}_t)$。较低的方差表明学习动态更稳定和可预测。\n\n对于每个测试用例，三种采样器独立运行，得到的度量用于根据指定的容差（$\\tau_g=0.02$, $\\tau_v=10^{-5}$, $\\tau_m=0.03$）评估三个布尔条件，这些条件将平衡采样器与 PER 和均匀采样进行比较。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the reinforcement learning sampler comparison experiment.\n    \"\"\"\n\n    # Common parameters\n    D = 4\n    N = 3000\n    GAMMA = 0.97\n    ETA = 0.01\n    T = 400\n    K = 20\n    TD_RECOMPUTE_PERIOD = 10\n    ALPHA = 0.95\n    EPS_PER = 1e-3\n    EPS_METRIC = 1e-12\n    SIGMA_BASE = 0.1\n    TAU_G = 0.02\n    TAU_V = 1e-5\n    TAU_M = 0.03\n\n    def phi(s, a, d_val):\n        \"\"\"Constructs the action-augmented feature vector.\"\"\"\n        features = np.zeros(2 * d_val)\n        if a == 0:\n            features[:d_val] = s\n        else:\n            features[d_val:] = s\n        return features\n\n    def generate_dataset(n_val, d_val, p_out, s_out, rng):\n        \"\"\"Generates a fixed synthetic dataset of transitions.\"\"\"\n        w_r = rng.standard_normal(size=2 * d_val)\n        m_0 = rng.standard_normal(size=(d_val, d_val)) / np.sqrt(d_val)\n        m_1 = rng.standard_normal(size=(d_val, d_val)) / np.sqrt(d_val)\n        \n        states = rng.standard_normal(size=(n_val, d_val))\n        actions = rng.integers(0, 2, size=n_val)\n        \n        next_states = np.zeros_like(states)\n        rewards = np.zeros(n_val)\n        \n        sigma_out = s_out * SIGMA_BASE\n        \n        for i in range(n_val):\n            s, a = states[i], actions[i]\n            \n            if a == 0:\n                next_states[i] = m_0 @ s\n            else:\n                next_states[i] = m_1 @ s\n            \n            features = phi(s, a, d_val)\n            r_det = w_r @ features\n            \n            noise_std = sigma_out if rng.random()  p_out else SIGMA_BASE\n            noise = rng.normal(0, noise_std)\n            rewards[i] = r_det + noise\n            \n        return states, actions, rewards, next_states\n\n    def run_sampler_experiment(sampler_type, dataset, params, run_rng):\n        \"\"\"Runs the training loop for a given sampler and returns metrics.\"\"\"\n        b_val, q_val = params['B'], params['Q']\n        \n        theta = np.zeros(2 * D)\n        theta_tgt = np.zeros(2 * D)\n        td_errors_abs = np.ones(N)\n\n        g_values = []\n        loss_values = []\n        \n        states, actions, rewards, next_states = dataset\n\n        # Pre-compute features for efficiency\n        phis = np.array([phi(s, a, D) for s, a in zip(states, actions)])\n        phis_next_0 = np.array([phi(s_prime, 0, D) for s_prime in next_states])\n        phis_next_1 = np.array([phi(s_prime, 1, D) for s_prime in next_states])\n\n        for t in range(1, T + 1):\n            if (t - 1) % TD_RECOMPUTE_PERIOD == 0:\n                q_values = phis @ theta\n                q_tgt_next_0 = phis_next_0 @ theta_tgt\n                q_tgt_next_1 = phis_next_1 @ theta_tgt\n                max_q_tgt_next = np.maximum(q_tgt_next_0, q_tgt_next_1)\n                deltas = rewards + GAMMA * max_q_tgt_next - q_values\n                td_errors_abs = np.abs(deltas)\n\n            if sampler_type == 'uniform':\n                batch_indices = run_rng.choice(N, size=b_val, replace=True)\n            elif sampler_type == 'per':\n                probs = (td_errors_abs + EPS_PER) ** ALPHA\n                probs_sum = probs.sum()\n                if probs_sum > 0:\n                    probs /= probs_sum\n                else: # Fallback to uniform if all probabilities are zero\n                    probs = np.full(N, 1/N)\n                batch_indices = run_rng.choice(N, size=b_val, p=probs, replace=True)\n            else: # balanced\n                if q_val == 1:\n                    batch_indices = run_rng.choice(N, size=b_val, replace=True)\n                else:\n                    samples_per_bin_base = b_val // q_val\n                    rem = b_val % q_val\n                    bin_sample_counts = [samples_per_bin_base] * q_val\n                    for i in range(rem): bin_sample_counts[i] += 1\n                    \n                    sorted_indices = np.argsort(td_errors_abs)\n                    partitioned_indices = np.array_split(sorted_indices, q_val)\n                    \n                    batch_indices_list = []\n                    for i in range(q_val):\n                        bin_indices = partitioned_indices[i]\n                        if len(bin_indices) > 0:\n                            chosen = run_rng.choice(bin_indices, size=bin_sample_counts[i], replace=True)\n                            batch_indices_list.append(chosen)\n                    batch_indices = np.concatenate(batch_indices_list)\n\n            batch_phis = phis[batch_indices]\n            batch_rewards = rewards[batch_indices]\n            batch_q_tgt_next_0 = phis_next_0[batch_indices] @ theta_tgt\n            batch_q_tgt_next_1 = phis_next_1[batch_indices] @ theta_tgt\n            batch_max_q_tgt_next = np.maximum(batch_q_tgt_next_0, batch_q_tgt_next_1)\n            batch_q_values = batch_phis @ theta\n            batch_deltas = batch_rewards + GAMMA * batch_max_q_tgt_next - batch_q_values\n\n            grad_sum = (batch_deltas.reshape(-1, 1) * batch_phis).sum(axis=0)\n            theta += (ETA / b_val) * grad_sum\n\n            # Diagnostics\n            grad_contrib_norms = np.linalg.norm(batch_deltas.reshape(-1, 1) * batch_phis, axis=1)\n            k = int(np.ceil(0.1 * b_val))\n            sorted_norms = np.sort(grad_contrib_norms)[::-1]\n            sum_top_k = np.sum(sorted_norms[:k])\n            sum_all = np.sum(sorted_norms)\n            g_t = sum_top_k / (sum_all + EPS_METRIC)\n            g_values.append(g_t)\n            \n            loss_t = 0.5 * np.mean(batch_deltas**2)\n            loss_values.append(loss_t)\n\n            if t % K == 0:\n                theta_tgt = theta.copy()\n\n        burn_in_steps = T // 2\n        mean_g = np.mean(g_values[burn_in_steps:])\n        var_loss = np.var(loss_values[burn_in_steps:], ddof=1) if len(loss_values[burn_in_steps:]) > 1 else 0\n\n        return mean_g, var_loss\n\n    test_cases = [\n        {'p_out': 0.2, 's_out': 8.0, 'Q': 5, 'B': 64, 'id': 'A', 'seed': 123},\n        {'p_out': 0.0, 's_out': 8.0, 'Q': 5, 'B': 64, 'id': 'B', 'seed': 456},\n        {'p_out': 0.2, 's_out': 8.0, 'Q': 1, 'B': 64, 'id': 'C', 'seed': 789},\n    ]\n    \n    all_results = []\n    \n    for case_params in test_cases:\n        rng = np.random.default_rng(case_params['seed'])\n        dataset = generate_dataset(N, D, case_params['p_out'], case_params['s_out'], rng)\n\n        metrics = {}\n        for sampler in ['uniform', 'per', 'balanced']:\n            run_rng = np.random.default_rng(rng.integers(2**32 - 1))\n            params = {'B': case_params['B'], 'Q': case_params['Q']}\n            mean_g, var_loss = run_sampler_experiment(sampler, dataset, params, run_rng)\n            metrics[sampler] = {'g_dom': mean_g, 'loss_var': var_loss}\n\n        g_bal = metrics['balanced']['g_dom']\n        g_per = metrics['per']['g_dom']\n        g_uni = metrics['uniform']['g_dom']\n        v_bal = metrics['balanced']['loss_var']\n        v_per = metrics['per']['loss_var']\n        \n        b1 = bool(g_bal  g_per - TAU_G)\n        b2 = bool(v_bal  v_per - TAU_V)\n        \n        if case_params['Q'] == 1:\n            b3 = bool(abs(g_bal - g_uni) = TAU_M)\n        else:\n            b3 = False\n            \n        all_results.append([b1, b2, b3])\n\n    result_str = ','.join(f'[{b1},{b2},{b3}]' for b1, b2, b3 in all_results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3163134"}]}