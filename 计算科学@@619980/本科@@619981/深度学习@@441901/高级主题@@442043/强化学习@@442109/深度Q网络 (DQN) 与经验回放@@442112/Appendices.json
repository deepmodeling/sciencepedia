{"hands_on_practices": [{"introduction": "时序差分（TD）误差是Q学习中的核心学习信号。本练习 [@problem_id:3113150] 将帮助我们理解在训练的不同阶段——学习开始前和收敛后——TD误差的统计特性。通过在一个简化的环境中推导其期望和方差，我们可以更深入地洞察智能体实际从经验中学到了什么，以及学习过程如何改变这个信号的分布。", "problem": "您需要分析一个简化的离策略深度Q网络 (DQN) 学习设置，该设置包含经验回放 (ER) 以及一个长度为 $T_{\\text{burn}}$ 的回放缓冲区预热期。环境是一个单状态马尔可夫决策过程 (MDP)，其中有两个可用动作。奖励是随机的且依赖于动作，并且下一状态总是相同的状态。智能体使用一个相对于其当前动作价值函数的 $\\epsilon$-贪心行为策略。您的分析必须从时间差分 (TD) 学习和贝尔曼最优方程的基本定义出发，并且必须避免使用任何未从这些基础推导出的简化公式。\n\n假设与定义：\n- 存在单一状态 $s$ 和两个动作 $a_1$ 和 $a_2$。如果智能体采取动作 $a_i$，它会收到一个奖励 $R_i$，其条件分布为 $R_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$，其中 $\\mu_i \\in \\mathbb{R}$ 和 $\\sigma \\ge 0$ 是已知常数，并确定性地转换回状态 $s$。\n- 折扣因子为 $\\gamma \\in [0,1)$。\n- 回放缓冲区在一个长度为 $T_{\\text{burn}}$ 的预热期内收集转换，在此期间不执行任何参数更新。\n- 动作价值函数初始化为 $Q_0(s,a_1) = 0$ 和 $Q_0(s,a_2) = 0$。\n- 行为策略是 $\\epsilon$-贪心策略：以 $1-\\epsilon$ 的概率选择贪心动作（均匀随机地打破平局），并以 $\\epsilon$ 的概率均匀随机地选择每个动作。\n- 在时间 $t$ 的时间差分 (TD) 误差为 $\\delta_t = r_t + \\gamma \\max_{a'} Q(s, a') - Q(s, a_t)$。\n- 理想化的预热后学习假设：在预热期结束后立即，智能体通过经验回放 (ER) 充分更新 $Q$，以达到该单状态 MDP 的贝尔曼最优方程的不动点。也就是说，它达到满足 $Q^*(a) = \\mathbb{E}[R_a] + \\gamma \\max_{a'} Q^*(a')$ 的 $Q^*$。\n\n任务：\n1. 仅使用上述假设以及 TD 误差和贝尔曼最优方程的定义，推导在任何学习发生之前（即当 $Q(s,a_1) = Q(s,a_2) = 0$ 时）的 TD 误差的分布。具体来说，推导 $\\delta$ 的期望值和方差的闭式表达式，该表达式是在 $\\epsilon$-贪心探索的行为策略下聚合的，且两个动作在初始化时价值相等。\n2. 使用相同的基础，推导在理想化的预热后学习假设下，收敛到 $Q^*$ 后的 TD 误差的分布。具体来说，推导在收敛时，根据 $\\epsilon$-贪心行为策略聚合的 $\\delta$ 的期望值和方差的闭式表达式。\n3. 将早期学习速度定义为前 $N$ 个环境步骤的预期累积均方 TD 误差，记为 $C(N, T_{\\text{burn}})$。在理想化的预热后收敛假设下，推导 $C(N, T_{\\text{burn}})$ 的闭式表达式，该表达式用收敛前均方 TD 误差和收敛后均方 TD 误差表示。您的表达式必须能处理边界情况 $T_{\\text{burn}} \\ge N$。\n4. 实现一个程序，对于下面测试套件中的每个测试用例，按顺序计算并返回以下五个量的元组：\n   - 学习前的 $\\delta$ 的期望值，记为 $m_{\\text{before}}$。\n   - 学习前的 $\\delta$ 的方差，记为 $v_{\\text{before}}$。\n   - 收敛后的 $\\delta$ 的期望值，记为 $m_{\\text{after}}$。\n   - 收敛后的 $\\delta$ 的方差，记为 $v_{\\text{after}}$。\n   - 前 $N$ 步的预期累积均方 TD 误差, $C(N, T_{\\text{burn}})$。\n   您的实现不应进行模拟；它必须根据您推导出的公式计算这些值。\n\n测试套件（每个项目都是一个元组 $(\\mu_1, \\mu_2, \\sigma, \\gamma, \\epsilon, T_{\\text{burn}}, N)$）：\n- 用例 A: $(1.0, 0.2, 1.0, 0.9, 0.1, 20, 100)$\n- 用例 B: $(0.5, 0.5, 0.5, 0.0, 0.5, 0, 50)$\n- 用例 C: $(2.0, -1.0, 0.0, 0.8, 0.2, 10, 10)$\n- 用例 D: $(-0.3, -0.1, 0.7, 0.95, 0.05, 100, 30)$\n- 用例 E: $(0.0, 1.0, 0.3, 0.9, 0.3, 5, 5)$\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含所有测试用例的结果。结果是一个用方括号括起来的逗号分隔列表，其中每个用例的结果本身也是相同样式的逗号分隔列表。例如： \"[[m_b,v_b,m_a,v_a,C],[...],...]\".\n- 所有输出必须是实数；不要包含任何单位或百分号，并且除了所要求的单行文本外，不要打印任何额外文本。", "solution": "该问题陈述被评估为科学上合理、定义明确、客观且自洽的。为进行严谨分析所需的所有必要参数和定义都已提供。预热后瞬时收敛的理想化是一个有效的理论简化，它使得问题在不违反强化学习基本原则的情况下易于解析处理。因此，该问题是有效的。\n\n解决方案从第一性原理出发，为两个指定的学习阶段（学习前和收敛后）推导所要求的统计量，然后将它们组合起来以求得总预期误差。\n\n**任务1：学习前的 TD 误差分布**\n\n在任何学习更新之前，动作价值函数对两个动作都初始化为零：$Q_0(s, a_1) = 0$ 和 $Q_0(s, a_2) = 0$。\n\n在给定时间步 $t$ 的时间差分 (TD) 误差定义为 $\\delta_t = r_t + \\gamma \\max_{a'} Q(s, a') - Q(s, a_t)$。\n代入初始 Q 值，我们得到：\n$$ \\delta_t = r_t + \\gamma \\max(0, 0) - Q(s, a_t) = r_t - 0 = r_t $$\n因此，在学习之前，TD 误差就是收到的奖励。\n\n行为策略是 $\\epsilon$-贪心策略。在初始化时，$Q(s, a_1) = Q(s, a_2)$，所以动作价值相等。策略规定均匀随机地打破平局。\n- 智能体以 $1-\\epsilon$ 的概率贪心地行动，以各 $0.5$ 的概率选择 $a_1$ 或 $a_2$。\n- 智能体以 $\\epsilon$ 的概率进行探索，以各 $0.5$ 的概率选择 $a_1$ 或 $a_2$。\n因此，选择动作 $a_i$ (对于 $i \\in \\{1, 2\\}$) 的概率是：\n$$ P(A_t = a_i) = (1-\\epsilon) \\times 0.5 + \\epsilon \\times 0.5 = 0.5 $$\nTD 误差 $\\delta$ 的分布是两个奖励分布 $R_1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ 和 $R_2 \\sim \\mathcal{N}(\\mu_2, \\sigma^2)$ 的混合，混合概率相等，均为 0.5。\n\nTD 误差的期望值 $m_{\\text{before}} = \\mathbb{E}[\\delta]$，使用全期望定律求得：\n$$ \\mathbb{E}[\\delta] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[\\delta | A_t=a_i] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[R_i] $$\n$$ m_{\\text{before}} = 0.5 \\cdot \\mu_1 + 0.5 \\cdot \\mu_2 = \\frac{\\mu_1 + \\mu_2}{2} $$\nTD 误差的方差 $v_{\\text{before}} = \\text{Var}(\\delta)$，使用全方差定律求得：$\\text{Var}(\\delta) = \\mathbb{E}[\\text{Var}(\\delta | A_t)] + \\text{Var}(\\mathbb{E}[\\delta | A_t])$。\n条件方差为 $\\text{Var}(\\delta | A_t=a_i) = \\text{Var}(R_i) = \\sigma^2$。\n$$ \\mathbb{E}[\\text{Var}(\\delta | A_t)] = \\sum_{i=1}^{2} P(A_t=a_i) \\text{Var}(\\delta | A_t=a_i) = 0.5 \\cdot \\sigma^2 + 0.5 \\cdot \\sigma^2 = \\sigma^2 $$\n条件期望为 $\\mathbb{E}[\\delta | A_t=a_i] = \\mu_i$。随机变量 $\\mathbb{E}[\\delta | A_t]$ 以 0.5 的概率取值 $\\mu_1$，以 0.5 的概率取值 $\\mu_2$。这个变量的方差是：\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = \\mathbb{E}[(\\mathbb{E}[\\delta | A_t])^2] - (\\mathbb{E}[\\mathbb{E}[\\delta | A_t]])^2 $$\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = (0.5 \\cdot \\mu_1^2 + 0.5 \\cdot \\mu_2^2) - \\left(\\frac{\\mu_1 + \\mu_2}{2}\\right)^2 = \\frac{2\\mu_1^2 + 2\\mu_2^2 - (\\mu_1^2 + 2\\mu_1\\mu_2 + \\mu_2^2)}{4} = \\frac{\\mu_1^2 - 2\\mu_1\\mu_2 + \\mu_2^2}{4} = \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2 $$\n合并各项，总方差为：\n$$ v_{\\text{before}} = \\sigma^2 + \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2 $$\n\n**任务2：收敛后的 TD 误差分布**\n\n收敛后，动作价值函数 $Q^*$ 满足此单状态 MDP 的贝尔曼最优方程：$Q^*(a) = \\mathbb{E}[R_a] + \\gamma \\max_{a'} Q^*(a')$。\n设 $Q^*_i = Q^*(s, a_i)$ 且 $V^* = \\max_{a'} Q^*(s, a') = \\max(Q^*_1, Q^*_2)$。方程为：\n$$ Q^*_1 = \\mu_1 + \\gamma V^* $$\n$$ Q^*_2 = \\mu_2 + \\gamma V^* $$\n将这些代入 $V^*$ 的定义：\n$$ V^* = \\max(\\mu_1 + \\gamma V^*, \\mu_2 + \\gamma V^*) = \\max(\\mu_1, \\mu_2) + \\gamma V^* $$\n对 $\\gamma \\in [0,1)$ 求解 $V^*$：\n$$ V^*(1-\\gamma) = \\max(\\mu_1, \\mu_2) \\implies V^* = \\frac{\\max(\\mu_1, \\mu_2)}{1-\\gamma} $$\n现在考虑 TD 误差 $\\delta_t = r_t + \\gamma \\max_{a'} Q^*(s, a') - Q^*(s, a_t)$。如果采取动作 $a_i$，则 $a_t=a_i$ 且 $r_t$ 是从 $R_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$ 中抽取的样本。TD 误差为：\n$$ \\delta_t(a_i) = R_i + \\gamma V^* - Q^*_i = R_i + \\gamma V^* - (\\mu_i + \\gamma V^*) = R_i - \\mu_i $$\n这意味着，以动作 $a_i$ 为条件，TD 误差是一个服从 $\\mathcal{N}(0, \\sigma^2)$ 分布的随机变量，因为 $\\mathbb{E}[R_i - \\mu_i] = \\mu_i - \\mu_i = 0$ 且 $\\text{Var}(R_i - \\mu_i) = \\text{Var}(R_i) = \\sigma^2$。\n\nTD 误差的期望值 $m_{\\text{after}} = \\mathbb{E}[\\delta]$，再次使用全期望定律求得。由于对于任何动作，条件期望都为 $0$，因此总期望也为 $0$：\n$$ m_{\\text{after}} = \\mathbb{E}[\\delta] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[\\delta | A_t=a_i] = \\sum_{i=1}^{2} P(A_t=a_i) \\cdot 0 = 0 $$\nTD 误差的方差 $v_{\\text{after}} = \\text{Var}(\\delta)$，使用全方差定律求得。条件方差为 $\\text{Var}(\\delta | A_t=a_i) = \\sigma^2$，条件期望为 $\\mathbb{E}[\\delta|A_t=a_i]=0$。\n$$ \\mathbb{E}[\\text{Var}(\\delta | A_t)] = \\sum_{i=1}^{2} P(A_t=a_i) \\cdot \\sigma^2 = \\sigma^2 \\sum_{i=1}^{2} P(A_t=a_i) = \\sigma^2 $$\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = \\text{Var}(0) = 0 $$\n因此，收敛后的总方差为：\n$$ v_{\\text{after}} = \\sigma^2 + 0 = \\sigma^2 $$\n请注意，这些结果与动作选择概率无关，因此与 $\\epsilon$ 和哪个动作是最优的无关。\n\n**任务3：预期累积均方 TD 误差**\n\n预期累积均方 TD 误差 $C(N, T_{\\text{burn}})$ 被解释为前 $N$ 个步骤的预期平方 TD 误差之和：$C(N, T_{\\text{burn}}) = \\sum_{t=1}^{N} \\mathbb{E}[\\delta_t^2]$。\n一个随机变量 $X$ 的均方误差 (MSE) 是 $\\mathbb{E}[X^2] = \\text{Var}(X) + (\\mathbb{E}[X])^2$。\n\n对于学习前阶段（从 $t=1$ 到 $t=T_{\\text{burn}}$），MSE 为：\n$$ \\text{MSE}_{\\text{before}} = v_{\\text{before}} + m_{\\text{before}}^2 = \\left(\\sigma^2 + \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2\\right) + \\left(\\frac{\\mu_1 + \\mu_2}{2}\\right)^2 $$\n$$ \\text{MSE}_{\\text{before}} = \\sigma^2 + \\frac{\\mu_1^2 - 2\\mu_1\\mu_2 + \\mu_2^2}{4} + \\frac{\\mu_1^2 + 2\\mu_1\\mu_2 + \\mu_2^2}{4} = \\sigma^2 + \\frac{2\\mu_1^2 + 2\\mu_2^2}{4} = \\sigma^2 + \\frac{\\mu_1^2 + \\mu_2^2}{2} $$\n对于收敛后阶段（从 $t = T_{\\text{burn}}+1$ 到 $t=N$），MSE 为：\n$$ \\text{MSE}_{\\text{after}} = v_{\\text{after}} + m_{\\text{after}}^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\n总时长为 $N$ 步。预热期持续 $T_{\\text{burn}}$ 步。\n- 如果 $T_{\\text{burn}} \\ge N$，则所有 $N$ 步都发生在学习前阶段。\n- 如果 $T_{\\text{burn}}  N$，则前 $T_{\\text{burn}}$ 步是学习前阶段，剩下的 $N - T_{\\text{burn}}$ 步是收敛后阶段。\n这可以表示为一个单一的公式：\n$$ C(N, T_{\\text{burn}}) = \\min(N, T_{\\text{burn}}) \\cdot \\text{MSE}_{\\text{before}} + \\max(0, N - T_{\\text{burn}}) \\cdot \\text{MSE}_{\\text{after}} $$\n代入 MSE 的表达式：\n$$ C(N, T_{\\text{burn}}) = \\min(N, T_{\\text{burn}}) \\left(\\sigma^2 + \\frac{\\mu_1^2 + \\mu_2^2}{2}\\right) + \\max(0, N - T_{\\text{burn}}) \\cdot \\sigma^2 $$\n\n**任务4：实现**\n\n以下 Python 程序实现了所推导的公式，用于为每个测试用例计算这五个量。根据推导，输入元组中的参数 $\\gamma$ 和 $\\epsilon$ 对于最终计算不是必需的。\n- $m_{\\text{before}} = (\\mu_1 + \\mu_2)/2$\n- $v_{\\text{before}} = \\sigma^2 + ((\\mu_1 - \\mu_2)/2)^2$\n- $m_{\\text{after}} = 0$\n- $v_{\\text{after}} = \\sigma^2$\n- $C(N, T_{\\text{burn}})$ 如上推导。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes statistical properties of the TD error for a simplified DQN setup.\n    \"\"\"\n    # Test suite: each item is a tuple (mu1, mu2, sigma, gamma, epsilon, T_burn, N)\n    test_cases = [\n        (1.0, 0.2, 1.0, 0.9, 0.1, 20, 100),\n        (0.5, 0.5, 0.5, 0.0, 0.5, 0, 50),\n        (2.0, -1.0, 0.0, 0.8, 0.2, 10, 10),\n        (-0.3, -0.1, 0.7, 0.95, 0.05, 100, 30),\n        (0.0, 1.0, 0.3, 0.9, 0.3, 5, 5),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        mu1, mu2, sigma, gamma, epsilon, T_burn, N = case\n\n        # Task 1: Expected value and variance of TD error before learning\n        # m_before = (mu1 + mu2) / 2\n        m_before = 0.5 * (mu1 + mu2)\n        # v_before = sigma^2 + ((mu1 - mu2) / 2)^2\n        v_before = sigma**2 + (0.5 * (mu1 - mu2))**2\n\n        # Task 2: Expected value and variance of TD error after convergence\n        # m_after = 0\n        m_after = 0.0\n        # v_after = sigma^2\n        v_after = sigma**2\n\n        # Task 3: Expected cumulative mean squared TD error\n        # MSE_before = v_before + m_before^2, or sigma^2 + (mu1^2 + mu2^2)/2\n        mse_before = sigma**2 + 0.5 * (mu1**2 + mu2**2)\n        # MSE_after = v_after + m_after^2 = sigma^2\n        mse_after = sigma**2\n        \n        # C(N, T_burn) = min(N, T_burn) * MSE_before + max(0, N - T_burn) * MSE_after\n        num_steps_before = min(N, T_burn)\n        num_steps_after = max(0, N - T_burn)\n        \n        C = num_steps_before * mse_before + num_steps_after * mse_after\n\n        # Store results for this case\n        case_result = [m_before, v_before, m_after, v_after, C]\n        \n        # Format the result list into the required string format \" [v1,v2,...] \"\n        case_str = f\"[{','.join(map(str, case_result))}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format \"[[...],[...],...]\"\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3113150"}, {"introduction": "标准经验回放采用均匀采样，而优先经验回放（PER）则倾向于采样更“令人意外”的转换以提高学习效率。本练习 [@problem_id:3113071] 通过一个模拟，直观地展示了PER中的一个关键挑战：当TD误差呈现多种模式时，高优先级的转换如何主导采样过程，可能导致“模式坍塌”并降低样本多样性。这凸显了在高级回放方法中仔细调整参数的重要性。", "problem": "构建一个最小的马尔可夫决策过程（Markov Decision Process），以揭示随机奖励如何能引发双峰时间差分（Temporal-Difference, TD）误差，以及优先经验回放（Prioritized Experience Replay）如何使采样偏向其中一个模式，从而可能导致采样过渡多样性的崩溃。请使用以下基本原理：贝尔曼最优性方程（Bellman optimality equation）和时间差分误差的定义，以及优先采样的标准定义。\n\n给定一个单状态、单动作的环境。假设存在一个深度Q网络（Deep Q-Network, DQN），但其参数保持固定（不进行学习），因此动作价值估计保持不变。设每个时间步的奖励独立地从一个两点混合分布中抽取：\n- 以概率 $p_{\\mathrm{small}}$，奖励为 $r_{\\mathrm{small}}$。\n- 以概率 $p_{\\mathrm{big}} = 1 - p_{\\mathrm{small}}$，奖励为 $r_{\\mathrm{big}}$。\n\n设折扣因子为 $\\gamma$，对于唯一的状态-动作对，固定的Q值为 $q_0$。时间差分（TD）误差由核心的时间差分（TD）学习定义给出：\n$$\n\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a).\n$$\n因为 $Q$ 固定为 $q_0$ 且只有一个动作，您必须使用\n$$\n\\delta = r + \\gamma q_0 - q_0.\n$$\n当 $q_0 = 0$ 时，这简化为 $\\delta = r$，只要 $\\lvert r_{\\mathrm{small}} \\rvert \\neq \\lvert r_{\\mathrm{big}} \\rvert$，它就直接继承了两点奖励分布的双峰性。\n\n对于优先经验回放，存储在大小为 $N$ 的缓冲区中的过渡，会以与其绝对TD误差的幂成正比的概率进行采样，使用标准的、经过充分测试的公式：\n- 对于每个TD误差为 $\\delta_i$ 的过渡 $i$，给定 $\\alpha \\ge 0$ 和小的 $\\varepsilon \\ge 0$，分配一个未归一化的优先级 $w_i = \\left( \\lvert \\delta_i \\rvert + \\varepsilon \\right)^{\\alpha}$。\n- 采样概率为 $p_i = w_i / \\sum_j w_j$。\n- 样本是有放回抽取的。\n\n您的任务是编写一个完整的、可运行的程序，该程序：\n1. 构建所述环境，并通过从指定的混合分布中抽取奖励来填充一个大小为 $N$ 的回放缓冲区。为保证可复现性，使用固定的随机种子 $s$。\n2. 标记每个存储的过渡是由哪个混合模式生成的奖励。将“次要模式”定义为具有较小绝对TD误差幅值 $m_{\\mathrm{small}} = \\min\\{\\lvert r_{\\mathrm{small}} + (\\gamma - 1) q_0 \\rvert, \\lvert r_{\\mathrm{big}} + (\\gamma - 1) q_0 \\rvert\\}$ 的模式，将“主要模式”定义为另一个具有幅值 $m_{\\mathrm{big}}$ 的模式。\n3. 使用 $p_i \\propto (\\lvert \\delta_i \\rvert + \\varepsilon)^{\\alpha}$ 从缓冲区中执行 $M$ 次有放回的优先采样。\n4. 计算采样到的过渡中来自次要模式的观测比例 $\\hat{f}_{\\mathrm{minor}}$。\n5. 将每个测试用例的 $\\hat{f}_{\\mathrm{minor}}$ 报告为一个精确到6位小数的浮点数。\n\n基于原理的推理目标：从TD误差定义和上述优先权重规则出发，推断预期的次要模式采样比例如何依赖于混合概率和两种模式的幅值。通过蒙特卡洛采样以编程方式估计此数量并报告结果。\n\n测试套件和参数化。在以下四个参数集上运行程序，每个参数集指定为一个元组 $(N, M, p_{\\mathrm{small}}, r_{\\mathrm{small}}, r_{\\mathrm{big}}, \\alpha, \\varepsilon, \\gamma, q_0, s)$：\n- 案例 A (边界情况，均匀回放): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.2,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 3,\\; \\alpha = \\; 0,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$。\n- 案例 B (理想情况，中度优先): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.5,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 5,\\; \\alpha = \\; 1,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$。\n- 案例 C (边缘情况，强优先导致模式崩溃): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.8,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 20,\\; \\alpha = \\; 2,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$。\n- 案例 D (边缘情况，添加 $\\varepsilon$ 以缓解崩溃): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.8,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 20,\\; \\alpha = \\; 2,\\; \\varepsilon = \\; 20,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$。\n\n注意：\n- 因为在所有案例中 $q_0 = 0$ 和 $\\gamma = 0$，TD误差简化为 $\\delta = r$。\n- 案例 A 测试了边界情况 $\\alpha = 0$，此时采样必须在缓冲区上均匀进行，而与幅值无关。\n- 案例 B 展示了在中度优先下，采样偏向于较大幅值模式的现象。\n- 案例 C 展示了“模式崩溃”，此处操作化为当 $\\alpha$ 很大且幅值比很大时，$\\hat{f}_{\\mathrm{minor}}$ 非常小。\n- 案例 D 展示了使 $\\varepsilon$ 与较大幅值相当可以通过平滑优先级来缓解崩溃。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的 Python 风格列表，内含四个舍入后的次要模式比例 $[\\hat{f}_{\\mathrm{minor}}^{(A)}, \\hat{f}_{\\mathrm{minor}}^{(B)}, \\hat{f}_{\\mathrm{minor}}^{(C)}, \\hat{f}_{\\mathrm{minor}}^{(D)}]$，分别对应案例 A、B、C 和 D。例如，如果计算出的值为 $[0.200000,0.166700,0.010000,0.524000]$，则此形式的输出是可接受的。", "solution": "该问题是有效的，因为它在深度强化学习领域内提出了一个定义明确、具有科学依据的模拟任务。所有参数和定义均已提供，目标清晰明确。\n\n目标是展示在一个最小的马尔可夫决策过程（MDP）中，由优先经验回放（PER）引起的采样偏差。这个 MDP 构建为单状态、单动作，使得动作价值函数 $Q(s, a)$ 是一个固定常数 $q_0$。每一步的奖励 $r$ 是随机的，从一个两点混合分布中抽取：以概率 $p_{\\mathrm{small}}$ 得到 $r = r_{\\mathrm{small}}$，以概率 $p_{\\mathrm{big}} = 1 - p_{\\mathrm{small}}$ 得到 $r = r_{\\mathrm{big}}$。\n\n分析的核心在于时间差分（TD）误差 $\\delta$，它是计算 PER 中优先级的依据。对于一个给定的过渡，TD 误差由贝尔曼方程定义：\n$$\n\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\n$$\n在我们简化的单状态 MDP 中，下一个状态 $s'$ 与当前状态 $s$ 相同，并且只有一个动作。Q值固定为 $q_0$。因此，方程简化为：\n$$\n\\delta = r + (\\gamma - 1)q_0\n$$\n由于奖励 $r$ 是双峰的，TD 误差 $\\delta$ 也是双峰的，有两个可能的值：\n$$\n\\delta_{\\mathrm{small}} = r_{\\mathrm{small}} + (\\gamma - 1)q_0\n$$\n$$\n\\delta_{\\mathrm{big}} = r_{\\mathrm{big}} + (\\gamma - 1)q_0\n$$\n问题根据这些TD误差的绝对幅值定义了两种模式。“次要模式”对应较小的幅值 $m_{\\mathrm{minor}} = \\min(|\\delta_{\\mathrm{small}}|, |\\delta_{\\mathrm{big}}|)$，“主要模式”对应较大的幅值 $m_{\\mathrm{major}} = \\max(|\\delta_{\\mathrm{small}}|, |\\delta_{\\mathrm{big}}|)$。\n\n在 PER 中，过渡从大小为 $N$ 的回放缓冲区中以与其优先级成正比的概率进行采样。具有 TD 误差 $\\delta_i$ 的过渡 $i$ 的优先级 $w_i$ 由以下公式给出：\n$$\nw_i = (|\\delta_i| + \\varepsilon)^{\\alpha}\n$$\n其中 $\\alpha \\ge 0$ 是优先化指数，$\\varepsilon \\ge 0$ 是一个小的常数，用以防止优先级为零。采样过渡 $i$ 的概率为：\n$$\np_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j}\n$$\n设回放缓冲区包含来自次要模式的 $N_{\\mathrm{minor}}$ 个过渡和来自主要模式的 $N_{\\mathrm{major}}$ 个过渡，其中 $N_{\\mathrm{minor}} + N_{\\mathrm{major}} = N$。这些模式的优先级分别为 $w_{\\mathrm{minor}} = (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}$ 和 $w_{\\mathrm{major}} = (m_{\\mathrm{major}} + \\varepsilon)^{\\alpha}$。缓冲区中优先级的总和为 $W = N_{\\mathrm{minor}} w_{\\mathrm{minor}} + N_{\\mathrm{major}} w_{\\mathrm{major}}$。\n\n在单次抽取中，选取任何来自次要模式的过渡的理论概率是它们各自概率的总和，可以简化为：\n$$\nP(\\text{样本为次要模式}) = \\frac{N_{\\mathrm{minor}} \\cdot w_{\\mathrm{minor}}}{W} = \\frac{N_{\\mathrm{minor}} \\cdot (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}}{N_{\\mathrm{minor}} \\cdot (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha} + N_{\\mathrm{major}} \\cdot (m_{\\mathrm{major}} + \\varepsilon)^{\\alpha}}\n$$\n这个方程揭示了采样偏差的来源。\n- 当 $\\alpha = 0$ 时，$w_{\\mathrm{minor}} = w_{\\mathrm{major}} = 1$。采样概率变为 $P(\\text{样本为次要模式}) = N_{\\mathrm{minor}} / (N_{\\mathrm{minor}} + N_{\\mathrm{major}}) = N_{\\mathrm{minor}} / N$。这对应于均匀采样，其中采样比例反映了缓冲区的构成。\n- 当 $\\alpha > 0$ 时，如果 $m_{\\mathrm{major}} \\gg m_{\\mathrm{minor}}$，优先级之比 $w_{\\mathrm{major}}/w_{\\mathrm{minor}}$ 会变得非常大。这会严重地使采样偏向主要模式，即使 $N_{\\mathrm{minor}} \\gg N_{\\mathrm{major}}$。这可能导致“模式崩溃”，即来自次要模式的过渡很少被采样。\n- 一个非零的 $\\varepsilon$，特别是当其大小与 $m_{\\mathrm{major}}$ 相当时，会减小优先级之比 $(m_{\\mathrm{major}} + \\varepsilon)^{\\alpha} / (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}$，从而减轻采样偏差。\n\n为解决此问题，我们对每个测试用例执行蒙特卡洛模拟：\n1.  实例化一个包含 $N$ 个过渡的回放缓冲区。对于每个过渡，从指定的两点分布中抽取一个奖励。我们使用固定的随机种子 $s$ 以确保可复现性。\n2.  根据每个案例的参数，我们确定哪个奖励（$r_{\\mathrm{small}}$ 或 $r_{\\mathrm{big}}$）产生次要TD误差模式。缓冲区中的每个过渡都相应地被标记。对于所有给定的测试用例，$\\gamma = 0$ 且 $q_0 = 0$，因此TD误差与奖励相同，即 $\\delta = r$。因此，次要模式就是具有较小绝对奖励值的模式。\n3.  使用公式 $w_i = (|\\delta_i| + \\varepsilon)^{\\alpha}$ 计算所有 $N$ 个过渡的优先级。\n4.  将这些优先级归一化，以形成一个在 $N$ 个过渡上的采样概率分布。\n5.  根据此分布，从缓冲区中有放回地抽取 $M$ 个样本。\n6.  计算这 $M$ 个样本中属于次要模式的比例 $\\hat{f}_{\\mathrm{minor}}$。该比例作为 $P(\\text{样本为次要模式})$ 的数值估计。\n\n此过程被系统地应用于所有四个测试用例，以量化参数 $\\alpha$ 和 $\\varepsilon$ 对采样多样性的影响。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to demonstrate the effect of Prioritized\n    Experience Replay (PER) on sampling bias in a simplified MDP.\n    \"\"\"\n    test_cases = [\n        # Case A: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.2, -1, 3, 0, 0, 0, 0, 42),\n        # Case B: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.5, -1, 5, 1, 0, 0, 0, 42),\n        # Case C: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.8, -1, 20, 2, 0, 0, 0, 42),\n        # Case D: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.8, -1, 20, 2, 20, 0, 0, 42),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s = case\n\n        # 1. Initialize random number generator for reproducibility.\n        rng = np.random.default_rng(s)\n\n        # 2. Fill the replay buffer with N transitions.\n        # Draw N random numbers to determine the reward for each transition.\n        reward_choices = rng.random(size=N)\n        # Assign rewards based on the mixture probability p_small.\n        rewards = np.where(reward_choices  p_small, r_small, r_big)\n\n        # 3. Calculate TD errors and identify which transitions belong to the minor mode.\n        td_error_for_r_small = r_small + (gamma - 1) * q0\n        td_error_for_r_big = r_big + (gamma - 1) * q0\n\n        # An array indicating which transitions were generated by the r_small component.\n        is_from_small_reward_source = (rewards == r_small)\n\n        # Determine which reward source corresponds to the minor TD error mode.\n        if np.abs(td_error_for_r_small)  np.abs(td_error_for_r_big):\n            # The minor mode corresponds to r_small.\n            is_minor_mode = is_from_small_reward_source\n        else:\n            # The minor mode corresponds to r_big.\n            is_minor_mode = ~is_from_small_reward_source\n\n        # Calculate the TD error for each transition in the buffer.\n        td_errors = rewards + (gamma - 1) * q0\n\n        # 4. Compute sampling priorities and probabilities.\n        priorities = (np.abs(td_errors) + epsilon)**alpha\n        \n        total_priority = np.sum(priorities)\n        \n        if total_priority > 0:\n            sampling_probs = priorities / total_priority\n        else:\n            # Fallback to uniform sampling if all priorities are zero.\n            # This can happen if alpha > 0 and |delta_i| + epsilon is 0 for all i.\n            # Not expected for the given test cases.\n            sampling_probs = np.full(N, 1.0 / N)\n\n        # 5. Perform M prioritized samples with replacement.\n        sampled_indices = rng.choice(N, size=M, replace=True, p=sampling_probs)\n\n        # 6. Compute the observed fraction of sampled transitions from the minor mode.\n        num_minor_sampled = np.sum(is_minor_mode[sampled_indices])\n        f_minor_hat = num_minor_sampled / M\n        \n        results.append(f_minor_hat)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3113071"}, {"introduction": "在之前概念的基础上，我们现在将探索如何设计和分析更复杂的经验回放机制。本练习 [@problem_id:3113068] 介绍了一种“年龄感知”的优先级方案，该方案同时考虑了TD误差和转换的新鲜度。通过计算学习速度、稳定性和样本有效数量等一系列度量标准，这个实践让我们掌握了评估不同经验回放设计中内在权衡的分析工具。", "problem": "考虑带有经验回放（Experience Replay, ER）的深度Q网络（Deep Q-Network, DQN）训练设置。假设一个回放缓冲区包含 $N$ 个转移，索引为 $i \\in \\{1, \\dots, N\\}$。每个转移都有一个时间差分误差 $\\delta_i$ 和一个关联的年龄 $\\text{age}_i$，年龄以存储后经过的环境步数来衡量。我们提出一种感知年龄的优先回放方法，其中转移 $i$ 的未归一化优先级由下式给出：\n$$\ns_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i},\n$$\n其中 $\\alpha \\ge 0$ 控制优先化程度，$\\lambda \\ge 0$ 用于惩罚较旧的转移。采样概率为\n$$\np_i = \\frac{s_i}{\\sum_{j=1}^{N} s_j},\n$$\n我们约定，如果 $\\sum_{j=1}^{N} s_j = 0$，则对所有 $i$ 都有 $p_i = \\frac{1}{N}$。\n\n为了减轻非均匀采样带来的偏差，我们使用重要性采样权重。令 $\\beta \\in [0,1]$ 控制修正的强度。定义未归一化的权重\n$$\nw_i = (N \\, p_i)^{-\\beta}\n$$\n对于 $p_i  0$，当 $p_i = 0$ 时 $w_i = 0$。为保证数值稳定性并避免不受控制的步长缩放，定义归一化权重\n$$\n\\hat{w}_i = \n\\begin{cases}\n\\frac{w_i}{\\max_{k: p_k  0} w_k}  \\text{if } \\max_{k: p_k  0} w_k  0, \\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\n\n我们分析两个核心方面：\n- 学习速度代理 $S$，定义为在采样分布下期望的绝对更新幅度，\n$$\nS = \\sum_{i=1}^{N} p_i \\, \\hat{w}_i \\, |\\delta_i|.\n$$\n- 稳定性代理 $V$，定义为带符号更新的方差，\n$$\n\\mu = \\sum_{i=1}^{N} p_i \\, \\hat{w}_i \\, \\delta_i, \\quad\nV = \\sum_{i=1}^{N} p_i \\left(\\hat{w}_i \\, \\delta_i - \\mu \\right)^2.\n$$\n\n此外，报告有效样本量（ESS）和一个离群点敏感性因子：\n$$\n\\text{ESS} = \\frac{1}{\\sum_{i=1}^{N} p_i^2}, \\quad\nO = \\frac{\\max_{i} \\hat{w}_i}{\\frac{1}{N} \\sum_{i=1}^{N} \\hat{w}_i}.\n$$\nESS 量化了采样分布的多样性，而 $O$ 量化了最大归一化权重相对于平均归一化权重的潜在主导程度。\n\n从 DQN 使用随机梯度下降最小化均方贝尔曼误差，以及重要性采样在非均匀采样下校正梯度估计的期望这一基本原理出发，推导 $S$ 和 $V$ 如何与 $p_i$ 和 $\\hat{w}_i$ 相关联。实现一个程序，对下面的每个测试用例，计算并返回 $(S, V, \\text{ESS}, O)$。\n\n测试套件：\n- 案例 1（常规成功路径）：$N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0.6$, $\\lambda = 0.5$, $\\beta = 0.4$。\n- 案例 2（均匀基线）：$N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0$, $\\lambda = 0$, $\\beta = 0$。\n- 案例 3（强年龄惩罚）：$N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0.6$, $\\lambda = 2.0$, $\\beta = 0.4$。\n- 案例 4（离群时间差分误差）：$N=6$, $\\delta = [\\,0.01,\\,0.02,\\,0.01,\\,5.0,\\,0.0,\\,-0.01\\,]$, $\\text{age} = [\\,0,\\,1,\\,0,\\,0,\\,50,\\,2\\,]$, $\\alpha = 1.0$, $\\lambda = 0.1$, $\\beta = 0.6$。\n- 案例 5（退化的零优先级）：$N=4$, $\\delta = [\\,0,\\,0,\\,0,\\,0\\,]$, $\\text{age} = [\\,0,\\,10,\\,20,\\,30\\,]$, $\\alpha = 0.5$, $\\lambda = 1.0$, $\\beta = 0.5$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。顺序必须为 $[S_1,V_1,\\text{ESS}_1,O_1,S_2,V_2,\\text{ESS}_2,O_2,S_3,V_3,\\text{ESS}_3,O_3,S_4,V_4,\\text{ESS}_4,O_4,S_5,V_5,\\text{ESS}_5,O_5]$，其中下标表示案例索引。所有值都必须是浮点数。", "solution": "所提出的问题是针对深度Q网络（DQN）的优先经验回放机制分析的一个良构且有科学依据的练习。它提供了一套完整且一致的定义和数据，从而可以得出一个唯一且可验证的解。因此，我们可以进行完整的求解。\n\n问题的核心要求我们理解在优先回放的重要性采样修正背景下，学习速度代理 $S$ 和稳定性代理 $V$ 的作用。为此，我们必须首先回顾DQN中学习的基本原理。\n\nDQN的训练目标是最小化转移分布 $\\mathcal{D}$ 上的均方贝尔曼误差（MSBE）。对于单个转移 $i$（由状态 $s_i$、动作 $a_i$、奖励 $r_i$ 和下一状态 $s'_i$ 组成），其损失函数为 $L_i(\\theta) = \\delta_i^2$，其中时间差分（TD）误差 $\\delta_i = (r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-)) - Q(s_i, a_i; \\theta)$，$\\theta$ 是在线网络的权重，$\\theta^-$ 是目标网络的权重。\n\n训练通过随机梯度下降（SGD）进行。损失函数关于在线网络权重 $\\theta$ 对转移 $i$ 的梯度为：\n$$\n\\nabla_\\theta L_i(\\theta) = \\nabla_\\theta \\left( (r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-) - Q(s_i, a_i; \\theta))^2 \\right) \\approx -2\\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)\n$$\n在这里，我们将目标项视为一个常数，这是Q学习中的标准做法。因此，SGD的更新规则是 $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta)$，其中 $\\eta$ 是学习率。对于单个样本 $i$，更新量与 $\\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)$ 成正比。\n\n如果我们从大小为 $N$ 的回放缓冲区中均匀采样转移，则采样任何转移 $i$ 的概率为 $1/N$。在一个小批量上的期望梯度是整个缓冲区上真实梯度的无偏估计。然而，均匀采样效率低下，因为它对所有转移赋予同等重要性，包括那些误差很小、对学习贡献不大的转移。\n\n优先经验回放（PER）通过非均匀地采样转移来解决此问题，其采样概率 $p_i$ 与其TD误差的量级 $|\\delta_i|$ 单调相关。在本问题中，采样概率 $p_i$ 由分数 $s_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i}$ 导出。这引入了偏差：在此采样分布 $p_i$ 下的梯度期望不再与真实的梯度期望相匹配。\n$$\n\\mathbb{E}_{i \\sim p}[\\nabla_\\theta L_i] = \\sum_{i=1}^{N} p_i \\nabla_\\theta L_i \\neq \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_\\theta L_i = \\mathbb{E}_{i \\sim U(1/N)}[\\nabla_\\theta L_i]\n$$\n为了校正这种偏差，我们采用重要性采样（IS）。每个样本 $i$ 的更新都通过目标分布（均匀分布）与采样分布的比率 $\\frac{1/N}{p_i}$ 进行加权。样本 $i$ 的损失变为 $L_i^{IS}(\\theta) = \\frac{1}{N p_i} \\delta_i^2$。然而，使用这些原始权重可能导致不稳定性。因此，通常采用两种修改方法：\n1. 将权重提升到 $\\beta \\in [0, 1]$ 次幂，得到 $w_i = (N p_i)^{-\\beta}$。该参数允许在均匀采样（$\\beta=0$，此时 $w_i=1$）和完全的重要性采样校正（$\\beta=1$）之间进行平滑插值。这为显著减小方差而引入了可控量的偏差。\n2. 权重通过其最大值进行归一化，即 $\\hat{w}_i = w_i / \\max_k w_k$。这确保了最大权重上限为1，防止任何单个转移主导梯度更新并破坏学习过程的稳定性。\n\n因此，样本 $i$ 的梯度更新现在与 $\\hat{w}_i \\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)$ 成正比。项 $\\hat{w}_i \\delta_i$ 可被视为驱动样本 $i$ 学习的有效的、经校正的误差信号。\n\n基于此，我们现在可以证明代理 $S$ 和 $V$ 定义的合理性。\n- **学习速度代理, $S$**：样本 $i$ 的参数更新幅度与 $|\\hat{w}_i \\delta_i| = \\hat{w}_i |\\delta_i|$（因为 $\\hat{w}_i \\ge 0$）成正比。当以概率 $p_i$ 采样时，一个更新步骤的期望幅度是该量在采样分布下的期望：\n$$\n\\mathbb{E}_{i \\sim p}[\\text{update magnitude}] \\propto \\sum_{i=1}^N p_i \\hat{w}_i |\\delta_i| = S\n$$\n因此，$S$ 代表了梯度步长的期望大小。较大的 $S$ 值表明，平均而言，网络正在对其权重进行较大的调整，这是加速学习的一个合理代理。\n\n- **稳定性代理, $V$**：SGD的稳定性关键取决于梯度估计的方差。高方差可能导致学习过程振荡或发散。项 $\\hat{w}_i \\delta_i$ 是代表采样转移 $i$ 的带符号校正信号的随机变量。量 $V$ 被定义为该随机变量在采样分布 $p_i$ 下的方差：\n$$\n\\mu = \\mathbb{E}[\\hat{w} \\delta] = \\sum_{i=1}^N p_i (\\hat{w}_i \\delta_i)\n$$\n$$\nV = \\text{Var}(\\hat{w} \\delta) = \\mathbb{E}[(\\hat{w} \\delta - \\mu)^2] = \\sum_{i=1}^N p_i (\\hat{w}_i \\delta_i - \\mu)^2\n$$\n较小的 $V$ 值表示不同样本间的校正信号更加一致，这是更稳定可靠学习过程的一个代理。\n\n另外两个指标提供了额外的诊断信息：\n- **有效样本量, ESS**：ESS衡量样本的多样性。对于均匀分布（$p_i = 1/N$），$\\text{ESS} = N$。对于一个高度倾斜的分布，其中某个 $p_k \\approx 1$，ESS 接近于1。较低的ESS表示回放正在重复采样一小部分转移，可能导致过拟合和泛化能力差。\n- **离群点敏感性因子, $O$**：该指标量化了最大归一化权重 $\\hat{w}_i$ 与平均值相比的主导程度。较高的 $O$ 值表明一个或少数几个样本对更新具有不成比例的巨大影响，这可能是不稳定性的一个来源。\n\n计算给定测试用例的这四个指标的算法如下：\n1.  给定 $N, \\{\\delta_i\\}, \\{\\text{age}_i\\}, \\alpha, \\lambda, \\beta$。\n2.  计算每个转移 $i$ 的未归一化优先级：$s_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i}$。\n3.  计算优先级之和，$S_{tot} = \\sum_{j=1}^N s_j$。\n4.  如果 $S_{tot} = 0$，将采样概率设为均匀分布：对所有 $i$，$p_i = 1/N$。否则，进行归一化：$p_i = s_i / S_{tot}$。\n5.  计算未归一化的重要性采样权重 $w_i$。对于每个 $p_i  0$ 的 $i$，设置 $w_i = (N p_i)^{-\\beta}$。对于 $p_i = 0$ 的 $i$，设置 $w_i = 0$。\n6.  计算最大权重，$w_{max} = \\max_k w_k$。\n7.  如果 $w_{max}  0$，计算归一化权重 $\\hat{w}_i = w_i / w_{max}$。否则，对所有 $i$ 设置 $\\hat{w}_i = 0$。\n8.  使用它们的定义计算四个指标：\n    - $S = \\sum_{i=1}^{N} p_i \\hat{w}_i |\\delta_i|$。\n    - $\\mu = \\sum_{i=1}^{N} p_i \\hat{w}_i \\delta_i$，然后 $V = \\sum_{i=1}^{N} p_i (\\hat{w}_i \\delta_i - \\mu)^2$。\n    - $\\text{ESS} = 1 / (\\sum_{i=1}^{N} p_i^2)$。\n    - $O = (\\max_i \\hat{w}_i) / (\\frac{1}{N}\\sum_{i=1}^N \\hat{w}_i)$，假设分母不为零。\n此过程是确定性的，并且在计算上简单直接。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes learning and stability metrics for an age-aware prioritized experience replay scheme.\n    \"\"\"\n    test_cases = [\n        # Case 1: general happy path\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.6, 'lambda': 0.5, 'beta': 0.4},\n        # Case 2: uniform baseline\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.0, 'lambda': 0.0, 'beta': 0.0},\n        # Case 3: strong age penalty\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.6, 'lambda': 2.0, 'beta': 0.4},\n        # Case 4: outlier temporal-difference error\n        {'N': 6, 'delta': [0.01, 0.02, 0.01, 5.0, 0.0, -0.01], 'age': [0, 1, 0, 0, 50, 2], 'alpha': 1.0, 'lambda': 0.1, 'beta': 0.6},\n        # Case 5: degenerate zero priorities\n        {'N': 4, 'delta': [0.0, 0.0, 0.0, 0.0], 'age': [0, 10, 20, 30], 'alpha': 0.5, 'lambda': 1.0, 'beta': 0.5},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        delta = np.array(case['delta'], dtype=float)\n        age = np.array(case['age'], dtype=float)\n        alpha = case['alpha']\n        lambda_ = case['lambda']\n        beta = case['beta']\n\n        # 1. Calculate unnormalized priorities s_i\n        # np.power(0.0, 0.0) correctly returns 1.0, handling the alpha=0 case.\n        s = np.power(np.abs(delta), alpha) / (1.0 + lambda_ * age)\n\n        # 2. Calculate sampling probabilities p_i\n        sum_s = np.sum(s)\n        if sum_s == 0.0:\n            p = np.full(N, 1.0 / N)\n        else:\n            p = s / sum_s\n\n        # 3. Calculate unnormalized importance sampling weights w_i\n        w = np.zeros(N, dtype=float)\n        positive_p_mask = p > 0.0\n        if np.any(positive_p_mask):\n            w[positive_p_mask] = np.power(N * p[positive_p_mask], -beta)\n            \n        # 4. Calculate normalized importance sampling weights w_hat_i\n        w_hat = np.zeros(N, dtype=float)\n        max_w = np.max(w)\n        if max_w > 0.0:\n            w_hat = w / max_w\n            \n        # 5. Calculate metrics S, V, ESS, O\n        # S: Learning speed proxy\n        S = np.sum(p * w_hat * np.abs(delta))\n\n        # V: Stability proxy (Variance)\n        mu = np.sum(p * w_hat * delta)\n        V = np.sum(p * np.power(w_hat * delta - mu, 2))\n\n        # ESS: Effective Sample Size\n        ESS = 1.0 / np.sum(np.power(p, 2))\n\n        # O: Outlier-sensitivity factor\n        max_w_hat = np.max(w_hat)\n        mean_w_hat = np.mean(w_hat)\n        \n        if mean_w_hat > 0:\n            O = max_w_hat / mean_w_hat\n        else:\n            # This case implies all w_hat are 0, so max_w_hat is also 0.\n            # No single dominant weight exists. A ratio of 1.0 is a reasonable convention.\n            O = 1.0\n\n        results.extend([S, V, ESS, O])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3113068"}]}