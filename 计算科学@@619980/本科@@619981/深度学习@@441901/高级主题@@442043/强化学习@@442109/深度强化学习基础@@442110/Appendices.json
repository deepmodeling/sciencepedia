{"hands_on_practices": [{"introduction": "在深入研究复杂的深度网络之前，理解强化学习的核心更新法则是至关重要的一步。本练习通过一个经典的“悬崖行走”场景，突出了在策略（on-policy）方法（如 SARSA）与离策略（off-policy）方法（如 Q学习）之间的根本区别 [@problem_id:3113683]。通过亲手实现这两种算法，你将能直观地理解为什么前者会学到“保守”的策略，而后者会学到“乐观”但有风险的策略，这一洞察对于在不同应用场景中选择合适的算法至关重要。", "problem": "考虑一个有限马尔可夫决策过程 (MDP)，它有一个非终止的起始状态和两条通往目标的不相交走廊。其中一条走廊是危险的，因为其中的一个动作可能导致坠崖，并获得很大的负回报；另一条走廊是安全的，因为其中的所有动作都能避免灾难性终止。其目的是比较状态-动作-奖励-状态-动作 (SARSA) 和 Q学习 (Q-learning) 在具有探索性行为策略的随机环境中的收敛动态，通过动作价值函数 $Q^{\\pi}$ 和 $Q^{*}$ 突显在策略与离策略之间的差异。\n\n该有限MDP具体规定如下。状态集为 $\\{s_{0}, r_{1}, r_{2}, u_{1}, u_{2}, u_{3}, g, c\\}$，其中 $s_{0}$ 是起始状态，$g$ 是目标终止状态，$c$ 是悬崖终止状态。所有状态下的动作集都相同，为 $\\{0,1,2\\}$；为便于解释：\n- 在 $s_{0}$ 处，动作 $0$ 是“走危险路线”，动作 $1$ 是“走安全路线”，动作 $2$ 是“原地踏步”。\n- 在走廊状态 $r_{1}, r_{2}, u_{1}, u_{2}, u_{3}$ 处，动作 $0$ 是“前进”，动作 $1$ 是“原地踏步”，动作 $2$ 是“滑倒”。\n- 在终止状态 $g$ 和 $c$ 处，回合立即结束。\n\n确定性的转移和奖励如下：\n- 从 $s_{0}$ 出发：\n  - 动作 $0$ 转移到 $r_{1}$，奖励为 $-1$。\n  - 动作 $1$ 转移到 $u_{1}$，奖励为 $-1$。\n  - 动作 $2$ 转移到 $s_{0}$，奖励为 $-1$。\n- 危险走廊：\n  - 从 $r_{1}$ 出发：动作 $0$ 到 $r_{2}$，奖励为 $-1$；动作 $1$ 到 $r_{1}$，奖励为 $-1$；动作 $2$ 到 $c$，奖励为 $-100$。\n  - 从 $r_{2}$ 出发：动作 $0$ 到 $g$，奖励为 $0$；动作 $1$ 到 $r_{2}$，奖励为 $-1$；动作 $2$ 到 $c$，奖励为 $-100$。\n- 安全走廊：\n  - 从 $u_{1}$ 出发：动作 $0$ 到 $u_{2}$，奖励为 $-1$；动作 $1$ 到 $u_{1}$，奖励为 $-1$；动作 $2$ 到 $u_{1}$，奖励为 $-1$。\n  - 从 $u_{2}$ 出发：动作 $0$ 到 $u_{3}$，奖励为 $-1$；动作 $1$ 到 $u_{2}$，奖励为 $-1$；动作 $2$ 到 $u_{2}$，奖励为 $-1$。\n  - 从 $u_{3}$ 出发：动作 $0$ 到 $g$，奖励为 $0$；动作 $1$ 到 $u_{3}$，奖励为 $-1$；动作 $2$ 到 $u_{3}$，奖励为 $-1$。\n\n学习期间使用的行为策略是关于当前动作价值估计 $Q(s,a)$ 的 $\\epsilon$-贪心策略：以 $1-\\epsilon$ 的概率选择使 $Q(s,a)$ 最大化的动作，以 $\\epsilon$ 的概率从动作集中均匀随机选择一个动作。如果多个动作的 $Q(s,a)$ 值并列最大，则选择索引最小的动作。\n\n训练要求：\n- 实现表格化Q学习和表格化状态-动作-奖励-状态-动作 (SARSA)，使用恒定的学习率 $\\alpha$ 和折扣因子 $\\gamma$；将所有动作价值估计 $Q(s,a)$ 初始化为 $0$。\n- 回合从 $s_{0}$ 开始，在到达 $g$ 或 $c$ 时终止，或在达到固定的 $M$ 步上限后终止以防止无限循环。使用 $M = 100$。\n- 使用固定的伪随机种子，以在 $\\epsilon$-贪心探索期间实现可复现的动作选择。\n\n评估：\n- 训练后，计算差异\n  $$d_{\\mathrm{QL}} = Q_{\\mathrm{QL}}(s_{0}, 0) - Q_{\\mathrm{QL}}(s_{0}, 1), \\quad d_{\\mathrm{SARSA}} = Q_{\\mathrm{SARSA}}(s_{0}, 0) - Q_{\\mathrm{SARSA}}(s_{0}, 1),$$\n  其中 $Q_{\\mathrm{QL}}$ 和 $Q_{\\mathrm{SARSA}}$ 分别是在Q学习和SARSA下学习到的动作价值函数。正差异表示学习到了在起始时偏好危险动作，而负差异表示学习到了偏好安全动作。\n\n推导和实现中使用的基本原理：\n- 有限马尔可夫决策过程 (MDP) 中策略的动作价值函数 $Q^{\\pi}(s,a)$ 的定义。\n- $Q^{\\pi}(s,a)$ 的贝尔曼期望方程和 $Q^{*}(s,a)$ 的贝尔曼最优方程。\n- 强化学习中用于探索的 $\\epsilon$-贪心策略的构建。\n\n测试套件：\n- 情况 $1$：$\\epsilon = 0.10$, $\\alpha = 0.50$, $\\gamma = 1.00$, 回合数 $= 20000$, 种子 $= 7$。\n- 情况 $2$ (边界情况)：$\\epsilon = 0.00$, $\\alpha = 0.50$, $\\gamma = 1.00$, 回合数 $= 10000$, 种子 $= 7$。\n- 情况 $3$ (重度探索)：$\\epsilon = 0.40$, $\\alpha = 0.50, \\gamma = 1.00$, 回合数 $= 30000$, 种子 $= 7$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含每个测试用例的差异对，平铺成一个列表：\n  $$[d_{\\mathrm{QL}}^{(1)}, d_{\\mathrm{SARSA}}^{(1)}, d_{\\mathrm{QL}}^{(2)}, d_{\\mathrm{SARSA}}^{(2)}, d_{\\mathrm{QL}}^{(3)}, d_{\\mathrm{SARSA}}^{(3)}],$$\n  其中上标 $(i)$ 索引测试用例 $1$ 到 $3$。每个条目必须是实数（浮点数）。", "solution": "所提出的问题要求对强化学习中两种基本时序差分 (TD) 控制算法进行比较分析：Q学习 (Q-learning) 和 状态-动作-奖励-状态-动作 (SARSA)。该分析在一个专门构建的有限马尔可夫决策过程 (MDP) 中进行，旨在突显离策略和在策略学习之间的关键区别。\n\n首先，我们形式化基本概念。一个有限MDP是一个元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$，其中 $\\mathcal{S}$ 是有限状态集，$\\mathcal{A}$ 是有限动作集，$P$ 是状态转移概率函数 $P(s'|s,a) = \\Pr(S_{t+1}=s'|S_t=s, A_t=a)$，$R$ 是奖励函数 $R(s,a,s') = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a, S_{t+1}=s']$，$\\gamma \\in [0, 1]$ 是折扣因子。在这个问题中，转移是确定性的，因此对于特定的 $s'$，$P(s'|s,a) = 1$，否则为 $0$。\n\n智能体的目标是学习一个策略 $\\pi(a|s) = \\Pr(A_t=a|S_t=s)$，以最大化期望的折扣累积奖励。在策略 $\\pi$ 下，一个状态-动作对 $(s,a)$ 的价值由动作价值函数 $Q^{\\pi}(s,a)$ 给出：\n$$Q^{\\pi}(s,a) = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\bigg| S_t=s, A_t=a \\right]$$\n该函数满足贝尔曼期望方程：\n$$Q^{\\pi}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^{\\pi}(s',a') \\right]$$\n最优动作价值函数 $Q^{*}(s,a) = \\max_{\\pi} Q^{\\pi}(s,a)$ 提供了从任何状态-动作对可获得的最高期望回报。它满足贝尔曼最优方程：\n$$Q^{*}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma \\max_{a'} Q^{*}(s',a') \\right]$$\n\nQ学习和SARSA是TD方法，它们在没有环境模型的情况下迭代地学习这些动作价值函数。它们的更新规则揭示了它们的根本差异。\n\nQ学习是一种离策略 (off-policy) 算法。在从状态 $S_t$ 转移到 $S_{t+1}$ 并获得奖励 $R_{t+1}$ 后，其对动作价值 $Q(S_t, A_t)$ 的更新规则是：\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \\right]$$\n项 $\\max_{a'} Q(S_{t+1}, a')$ 代表从下一状态 $S_{t+1}$ 出发的最优动作的估计价值。Q学习使用这个贪心选择来更新其价值函数，而不管行为策略实际接下来采取了哪个动作。因此，它直接学习最优动作价值函数 $Q^*$ 的一个估计。\n\nSARSA是一种在策略 (on-policy) 算法。它的更新规则使用元组 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$：\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]$$\n在这里，更新目标包括 $Q(S_{t+1}, A_{t+1})$，即在状态 $S_{t+1}$ 中根据行为策略实际选择的动作 $A_{t+1}$ 的价值。因此，SARSA学习它当前所遵循的策略的动作价值函数 $Q^{\\pi}$。在这个问题中，策略 $\\pi$ 是相对于当前 $Q$ 值的 $\\epsilon$-贪心策略。\n\n现在，我们分析这个特定的MDP。处于起始状态 $s_0$ 的智能体可以选择“危险”走廊（动作 $0$）或“安全”走廊（动作 $1$）。折扣因子 $\\gamma=1$，意味着我们寻求最大化未折扣的奖励总和。\n通往目标的最优路径是经过危险走廊：$s_0 \\to r_1 \\to r_2 \\to g$。这条路径需要 $3$ 步，总奖励为 $(-1) + (-1) + 0 = -2$。\n安全路径更长：$s_0 \\to u_1 \\to u_2 \\to u_3 \\to g$。这条路径需要 $4$ 步，总奖励为 $(-1) + (-1) + (-1) + 0 = -3$。\n因此，从 $s_0$ 开始的最优动作价值是 $Q^*(s_0, 0) = -2$ 和 $Q^*(s_0, 1) = -3$。\n\n当 $\\epsilon > 0$ 时，行为策略是探索性的。\nQ学习是离策略的，其目标是找到 $Q^*$。它会学到危险路径更优（$Q(s_0, 0) \\approx -2$ 和 $Q(s_0, 1) \\approx -3$），尽管它自己的探索性动作可能偶尔会导致它坠崖。它的价值估计是基于未来会贪婪行动的假设。因此，我们预测 $Q_{\\mathrm{QL}}(s_0, 0) > Q_{\\mathrm{QL}}(s_0, 1)$，从而产生一个正差异 $d_{\\mathrm{QL}} > 0$。\n\nSARSA是在策略的，它学习的是 $\\epsilon$-贪心策略本身的价值。这个策略有非零概率（$\\epsilon/3$）在状态 $r_1$ 和 $r_2$ 中选择“滑倒”动作（动作 $2$），这会导致坠崖并获得 $-100$ 的巨大负奖励。这种可能性被计入SARSA的价值估计中。因此，选择危险路径的期望回报是最佳结果和灾难性“滑倒”结果的加权平均。对于足够大的惩罚，这个期望回报将低于安全路径的期望回报，因为在安全路径中，探索（“滑倒”或“原地踏步”）不会是灾难性的。SARSA将学习到，在它自己的探索性策略下，安全走廊是更可取的。我们预测 $Q_{\\mathrm{SARSA}}(s_0, 0)  Q_{\\mathrm{SARSA}}(s_0, 1)$，从而产生一个负差异 $d_{\\mathrm{SARSA}}  0$。这种效应的幅度应随 $\\epsilon$ 的增加而增加。\n\n对于 $\\epsilon=0.00$ 的情况，策略是纯粹贪婪的。在SARSA中选择的动作 $A_{t+1}$ 将是 $\\arg\\max_{a'} Q(S_{t+1}, a')$。在这种特定场景下，假设存在唯一的最大值，SARSA的更新规则在功能上变得与Q学习的更新规则相同。两种算法都将遵循相同的贪婪轨迹，并以相同的方式更新它们的价值。它们都将收敛到 $Q^*$，并学到危险路径是最优的。因此，我们期望 $d_{\\mathrm{QL}}^{(2)} > 0$ 和 $d_{\\mathrm{SARSA}}^{(2)} > 0$，并且它们的值会非常接近。这表明SARSA的“保守”性质是在策略评估一个探索性策略的直接结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Q-learning and SARSA on a specified MDP\n    to demonstrate the difference between off-policy and on-policy learning.\n    \"\"\"\n    \n    #\n    # Environment Setup\n    #\n    states = ['s0', 'r1', 'r2', 'u1', 'u2', 'u3', 'g', 'c']\n    state_map = {name: i for i, name in enumerate(states)}\n    num_states = len(states)\n    num_actions = 3\n    \n    s0, r1, r2, u1, u2, u3, g, c = (state_map[s] for s in states)\n    terminal_states = {g, c}\n    \n    # Transitions: T[state_idx][action_idx] = (next_state_idx, reward)\n    T = {\n        s0: {0: (r1, -1.0), 1: (u1, -1.0), 2: (s0, -1.0)},\n        r1: {0: (r2, -1.0), 1: (r1, -1.0), 2: (c, -100.0)},\n        r2: {0: (g, 0.0),  1: (r2, -1.0), 2: (c, -100.0)},\n        u1: {0: (u2, -1.0), 1: (u1, -1.0), 2: (u1, -1.0)},\n        u2: {0: (u3, -1.0), 1: (u2, -1.0), 2: (u2, -1.0)},\n        u3: {0: (g, 0.0),  1: (u3, -1.0), 2: (u3, -1.0)},\n    }\n    \n    #\n    # Agent and Policy Logic\n    #\n    def select_action(q_s, epsilon, rng):\n        \"\"\"Selects an action using an epsilon-greedy policy.\"\"\"\n        if rng.random()  epsilon:\n            return rng.integers(num_actions)\n        else:\n            # np.argmax handles tie-breaking by returning the first index\n            # with the maximum value, which matches the problem spec.\n            return np.argmax(q_s)\n\n    #\n    # Training Function\n    #\n    def run_experiment(algorithm, params, rng):\n        \"\"\"Runs a full training experiment for a given algorithm.\"\"\"\n        epsilon, alpha, gamma, total_episodes, max_steps = params\n        \n        Q = np.zeros((num_states, num_actions))\n\n        for _ in range(total_episodes):\n            state = s0\n            \n            # For SARSA, the first action must be selected before the loop begins.\n            if algorithm == 'sarsa':\n                action = select_action(Q[state], epsilon, rng)\n\n            for _ in range(max_steps):\n                if state in terminal_states:\n                    break\n                \n                # For Q-learning, action is selected inside the main loop.\n                if algorithm == 'q_learning':\n                    action = select_action(Q[state], epsilon, rng)\n\n                # Get next state and reward from the environment model.\n                next_state, reward = T[state][action]\n                \n                if algorithm == 'q_learning':\n                    # Off-policy update: uses max Q-value at next state\n                    if next_state in terminal_states:\n                        target = reward\n                    else:\n                        target = reward + gamma * np.max(Q[next_state])\n                    Q[state, action] += alpha * (target - Q[state, action])\n                \n                elif algorithm == 'sarsa':\n                    # On-policy update: uses Q-value of the actual next action\n                    if next_state in terminal_states:\n                        # No next action, target is just the reward\n                        target = reward\n                    else:\n                        next_action = select_action(Q[next_state], epsilon, rng)\n                        target = reward + gamma * Q[next_state, next_action]\n                    Q[state, action] += alpha * (target - Q[state, action])\n                    # Update for next iteration\n                    action = next_action if next_state not in terminal_states else 0\n\n                state = next_state\n        return Q\n\n    #\n    # Main Execution Logic\n    #\n    test_cases = [\n        # (epsilon, alpha, gamma, episodes, seed)\n        (0.10, 0.50, 1.00, 20000, 7),\n        (0.00, 0.50, 1.00, 10000, 7),\n        (0.40, 0.50, 1.00, 30000, 7),\n    ]\n    max_steps_per_episode = 100\n    all_results = []\n    \n    for epsilon, alpha, gamma, episodes, seed in test_cases:\n        params = (epsilon, alpha, gamma, episodes, max_steps_per_episode)\n        \n        # Run Q-learning for the current case\n        rng_ql = np.random.default_rng(seed)\n        Q_ql = run_experiment('q_learning', params, rng_ql)\n        d_ql = Q_ql[s0, 0] - Q_ql[s0, 1]\n        \n        # Run SARSA for the current case, resetting the seed for fair comparison\n        rng_sarsa = np.random.default_rng(seed)\n        Q_sarsa = run_experiment('sarsa', params, rng_sarsa)\n        d_sarsa = Q_sarsa[s0, 0] - Q_sarsa[s0, 1]\n        \n        all_results.extend([d_ql, d_sarsa])\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3113683"}, {"introduction": "当我们将简单的 Q 表格替换为强大的神经网络时，会引入新的挑战，其中最主要的就是学习过程的不稳定性。本练习 [@problem_id:3113592] 使用一个简化的线性模型，旨在探索深度 Q 网络（DQN）中的一个关键组件——目标网络（target network）——是如何帮助稳定学习的。通过分析系统动态并推导其稳定性条件，你将能够预测和观察目标网络的更新频率如何导致或抑制价值函数的振荡，从而深刻理解稳定深度强化学习智能体的设计原则。", "problem": "给定一个深度Q网络（DQN）学习动态的简化线性模型，DQN是一种深度强化学习（DRL）算法。目标是解析地近似振荡行为作为目标网络更新周期的函数何时出现，并通过使用快速傅里叶变换（FFT）测量学习到的动作价值函数的振荡谱来验证此近似。该近似必须从强化学习和线性系统理论的基本原理出发，而不依赖于快捷公式。\n\n考虑一个确定性的双状态马尔可夫决策过程（MDP），其具有两个标量动作价值函数分量 $q_1$ 和 $q_2$，分别代表两个状态的在线网络估计值。设 $h_1$ 和 $h_2$ 表示相应的目标网络估计值。环境是确定性的，从状态 $1$ 转换到状态 $2$，再从状态 $2$ 转换到状态 $1$。奖励是恒定的，由 $r_1$ 和 $r_2$ 给出。折扣因子为 $\\gamma \\in [0,1)$，学习率为 $\\alpha \\in (0,1)$。\n\n这个确定性设置下的规范贝尔曼最优性关系是\n$$\nQ^\\star_1 = r_1 + \\gamma Q^\\star_2, \\quad Q^\\star_2 = r_2 + \\gamma Q^\\star_1,\n$$\n其中 $Q^\\star_i$ 表示状态 $i$ 的最优动作价值。深度Q网络（DQN）通过时序差分更新来近似这些关系。在目标保持区间内，目标网络是固定的，每步的在线更新为\n$$\nq_1(t+1) = q_1(t) + \\alpha \\left( r_1 + \\gamma h_2(t) - q_1(t) \\right),\n$$\n$$\nq_2(t+1) = q_2(t) + \\alpha \\left( r_2 + \\gamma h_1(t) - q_2(t) \\right),\n$$\n并且目标网络每 $T$ 步通过一次硬拷贝进行更新：\n$$\nh_i(t+1) = \n\\begin{cases}\nq_i(t+1),  \\text{if } (t+1) \\bmod T = 0, \\\\\nh_i(t),  \\text{otherwise},\n\\end{cases}\n\\quad i \\in \\{1,2\\}.\n$$\n\n在任何长度为 $T$ 的目标保持区间内，将 $h_1$ 和 $h_2$ 视作等于上次复制的值 $s_1$ 和 $s_2$ 的常数。推导在经过 $T$ 次在线更新和一次目标拷贝后，将对 $(s_1, s_2)$ 推进到下一个周期的对 $(s'_1, s'_2)$ 的跨周期线性仿射映射。然后，求出所得线性部分的特征值，以确定振荡模式的存在。利用负特征值的存在来定义一个预测的每步振荡频率，并通过计算差分模式 $d(t) = q_1(t) - q_2(t)$ 的时间序列的FFT来验证这一预测。\n\n你的程序必须：\n- 从上述每步DQN动态出发，推导 $(s_1, s_2)$ 的周期到周期的线性映射，用 $\\alpha$、$\\gamma$ 和 $T$ 表示其矩阵，并计算其特征值。\n- 使用特征结构来预测振荡模式的出现。具体来说，如果较小的特征值为负，则将预测的每步角频率定义为 $\\omega_{\\text{pred}} = \\pi / T$；否则，设置 $\\omega_{\\text{pred}} = 0$。\n- 模拟 $N$ 步的每步动态以生成 $d(t)$，并计算其FFT幅值谱（不包括零频分量），以获得以弧度/步为单位的测得峰值角频率 $\\omega_{\\text{meas}}$。\n- 对每个测试用例，报告元组 $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$，其中 $\\text{resonant}$ 是一个布尔值，如果较小的特征值为负则为 $\\text{True}$，否则为 $\\text{False}$。\n\n对所有测试用例使用以下固定参数值：\n- 折扣因子 $\\gamma = 0.9$，\n- 学习率 $\\alpha = 0.2$，\n- 奖励 $r_1 = 0$ 和 $r_2 = 0$，\n- 初始在线值 $q_1(0) = 1$ 和 $q_2(0) = -1$，\n- 初始目标值 $h_1(0) = q_1(0)$ 和 $h_2(0) = q_2(0)$，\n- 模拟长度 $N = 4096$ 步。\n\n角度单位说明：所有角频率必须以弧度/步为单位表示。\n\n测试套件：\n- 扫描目标更新周期 $T$ 的值 $T \\in \\{1, 3, 4, 20, 50\\}$，以覆盖频繁更新、近阈值行为和长目标保持区间。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。\n- 对每个测试用例，输出一个嵌套列表 $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$，其中 $\\omega_{\\text{pred}}$ 和 $\\omega_{\\text{meas}}$ 均四舍五入到六位小数，$\\text{resonant}$ 为布尔值。\n- 示例格式：$[[0.785398,0.780000,True],[0.000000,0.020000,False],\\dots]$。", "solution": "该问题要求分析一个简化深度Q网络（DQN）模型中的振荡动态。这将分两部分完成：首先，基于学习更新的线性系统分析，从理论上推导振荡条件；其次，通过数值模拟来测量振荡频率并验证理论预测。\n\n### 第一部分：理论分析\n\n学习动态由一组关于在线动作价值估计值 $q_1(t)$ 和 $q_2(t)$ 的耦合一阶差分方程描述。目标网络值 $h_1(t)$ 和 $h_2(t)$ 在 $T$ 步的周期内保持不变。让我们分析系统在这样一个周期内的演化。\n\n在一个周期的开始，目标网络会用在线网络的当前值进行更新。设这些值为 $s_1$ 和 $s_2$。因此，在该周期的持续时间内（从周期内的第 $t=0$ 步到第 $t=T-1$ 步），我们有 $h_1 = s_1$ 和 $h_2 = s_2$。在线网络的每步更新由下式给出：\n$$\nq_1(t+1) = q_1(t) + \\alpha \\left( r_1 + \\gamma h_2 - q_1(t) \\right) = (1-\\alpha) q_1(t) + \\alpha (r_1 + \\gamma s_2)\n$$\n$$\nq_2(t+1) = q_2(t) + \\alpha \\left( r_2 + \\gamma h_1 - q_2(t) \\right) = (1-\\alpha) q_2(t) + \\alpha (r_2 + \\gamma s_1)\n$$\n问题规定，一个周期开始时在线网络的初始值与该周期的目标值相同，即 $q_1(0) = s_1$ 和 $q_2(0) = s_2$。\n\n这些是线性一阶非齐次差分方程。对于形式为 $x(t+1) = a x(t) + b$ 的方程，其通解为 $x(t) = a^t x(0) + b \\sum_{k=0}^{t-1} a^k = a^t x(0) + b \\frac{1-a^t}{1-a}$。\n将此应用于我们的系统，其中 $a = (1-\\alpha)$，我们得到：\n$$\nq_1(t) = (1-\\alpha)^t q_1(0) + \\alpha(r_1 + \\gamma s_2) \\frac{1-(1-\\alpha)^t}{\\alpha} = (1-\\alpha)^t s_1 + (1-(1-\\alpha)^t)(r_1 + \\gamma s_2)\n$$\n$$\nq_2(t) = (1-\\alpha)^t q_2(0) + \\alpha(r_2 + \\gamma s_1) \\frac{1-(1-\\alpha)^t}{\\alpha} = (1-\\alpha)^t s_2 + (1-(1-\\alpha)^t)(r_2 + \\gamma s_1)\n$$\n经过 $T$ 步后，在线网络的值被用来更新下一个周期的目标网络。设新的目标值为 $s'_1$ 和 $s'_2$。\n$$\ns'_1 = q_1(T) = (1-\\alpha)^T s_1 + (1-(1-\\alpha)^T)(r_1 + \\gamma s_2)\n$$\n$$\ns'_2 = q_2(T) = (1-\\alpha)^T s_2 + (1-(1-\\alpha)^T)(r_2 + \\gamma s_1)\n$$\n问题规定奖励恒为 $r_1=0$ 和 $r_2=0$。该映射简化为一个线性变换。令 $\\beta = (1-\\alpha)^T$。方程变为：\n$$\ns'_1 = \\beta s_1 + (1-\\beta)\\gamma s_2\n$$\n$$\ns'_2 = \\beta s_2 + (1-\\beta)\\gamma s_1\n$$\n其矩阵形式为 $\\mathbf{s'} = A \\mathbf{s}$，其中 $\\mathbf{s} = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix}$，周期到周期的转移矩阵 $A$ 为：\n$$\nA = \\begin{pmatrix} \\beta  (1-\\beta)\\gamma \\\\ (1-\\beta)\\gamma  \\beta \\end{pmatrix}\n$$\n\n为了确定该映射的稳定性和振荡性质，我们通过求解特征方程 $\\det(A - \\lambda I) = 0$ 来计算 $A$ 的特征值：\n$$\n\\det \\begin{pmatrix} \\beta-\\lambda  (1-\\beta)\\gamma \\\\ (1-\\beta)\\gamma  \\beta-\\lambda \\end{pmatrix} = (\\beta-\\lambda)^2 - ((1-\\beta)\\gamma)^2 = 0\n$$\n这产生两个特征值解 $\\lambda$：\n$$\n\\beta - \\lambda = \\pm (1-\\beta)\\gamma \\implies \\lambda = \\beta \\mp (1-\\beta)\\gamma\n$$\n两个特征值为：\n$$\n\\lambda_1 = \\beta + (1-\\beta)\\gamma = (1-\\alpha)^T + \\gamma(1-(1-\\alpha)^T)\n$$\n$$\n\\lambda_2 = \\beta - (1-\\beta)\\gamma = (1-\\alpha)^T - \\gamma(1-(1-\\alpha)^T)\n$$\n当系统的轨迹在状态空间中的某个方向上，从一个周期到下一个周期翻转其方向时，振荡模式就会出现。在线性系统中，这对应于一个负的特征值。由于 $0  \\alpha  1$ 和 $0 \\le \\gamma  1$，我们有 $0  \\beta  1$，这意味着 $(1-\\beta)\\gamma \\ge 0$。因此，$\\lambda_2$ 总是较小的特征值。振荡模式出现的条件（我们称之为`共振`）是 $\\lambda_2  0$。\n$$\n(1-\\alpha)^T - \\gamma(1-(1-\\alpha)^T)  0\n$$\n$$\n(1-\\alpha)^T  \\gamma - \\gamma(1-\\alpha)^T\n$$\n$$\n(1+\\gamma)(1-\\alpha)^T  \\gamma\n$$\n$$\n(1-\\alpha)^T  \\frac{\\gamma}{1+\\gamma}\n$$\n如果这个条件成立，系统就处于一个`共振`状态。对应于 $\\lambda_2$ 的特征向量与 $(1, -1)^T$ 成正比，它代表了差分模式 $d(t) = q_1(t) - q_2(t)$。负特征值意味着这个模式每个周期（每 $T$ 步）会翻转符号。这对应于一个周期为 $2T$ 步的振荡。这种振荡的基波角频率是 $\\omega = 2\\pi / (2T) = \\pi/T$ 弧度/步。因此，预测的频率是：\n$$\n\\omega_{\\text{pred}} = \\begin{cases} \\pi/T,  \\text{if } (1-\\alpha)^T  \\frac{\\gamma}{1+\\gamma} \\\\ 0,  \\text{otherwise} \\end{cases}\n$$\n\n### 第二部分：数值模拟与测量\n\n我们现在将模拟 $N=4096$ 步的每步动态，以生成差分模式 $d(t) = q_1(t) - q_2(t)$ 的时间序列。模拟直接实现了给定的 $q_i(t)$ 更新规则，以及 $h_i(t+1) = q_i(t+1)$ 当 $(t+1) \\bmod T = 0$ 时，否则 $h_i(t+1) = h_i(t)$。\n\n在模拟并存储了 $q_1(t)$ 和 $q_2(t)$ 的历史记录之后，我们构建 $t = 0, \\dots, N-1$ 的时间序列 $d(t)$。然后我们计算这个序列的快速傅里叶变换（FFT）以找到其频谱。\n测量的角频率 $\\omega_{\\text{meas}}$ 是通过在频谱中找到除零频（DC）分量外具有最大幅值的频率分量来确定的。FFT算法提供的频率 $f$ 的单位是 周期/步。它们通过关系 $\\omega = 2\\pi f$ 转换为以 弧度/步 为单位的角频率。\n\n对于每个给定的目标更新周期 $T$ 的值，我们将计算三元组 $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$ 并报告结果。固定参数为 $\\gamma = 0.9$，$\\alpha = 0.2$，$r_1=0$，$r_2=0$，以及初始条件 $q_1(0)=1, q_2(0)=-1, h_1(0)=1, h_2(0)=-1$。\n振荡的临界阈值为 $(1-0.2)^T  \\frac{0.9}{1+0.9}$，简化为 $0.8^T  0.9/1.9 \\approx 0.47368$。\n\n对于 $T=1, 3$：$0.8^1 = 0.8 \\not 0.47368$，$0.8^3 = 0.512 \\not 0.47368$。系统不是`共振`的。\n对于 $T=4, 20, 50$：$0.8^4 = 0.4096  0.47368$，$0.8^{20} \\approx 0.0115  0.47368$，$0.8^{50} \\approx 1.4 \\times 10^{-5}  0.47368$。系统是`共振`的。\n这些理论预测将通过模拟得到验证。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and simulates the dynamics of a simplified DQN to find\n    and verify oscillatory behavior based on the target network update period.\n    \"\"\"\n    \n    # Define the fixed parameters from the problem statement.\n    GAMMA = 0.9\n    ALPHA = 0.2\n    R1 = 0.0\n    R2 = 0.0\n    Q1_0 = 1.0\n    Q2_0 = -1.0\n    N_STEPS = 4096\n    \n    # Define the test cases (sweep over target update period T).\n    test_cases = [1, 3, 4, 20, 50]\n\n    results = []\n\n    for T in test_cases:\n        # Part 1: Theoretical Prediction\n        \n        # The condition for oscillatory resonance is (1-alpha)^T  gamma / (1+gamma)\n        resonant_threshold = GAMMA / (1 + GAMMA)\n        is_resonant = (1 - ALPHA)**T  resonant_threshold\n        \n        if is_resonant:\n            omega_pred = np.pi / T\n        else:\n            omega_pred = 0.0\n\n        # Part 2: Simulation\n        \n        # Initialize arrays to store the time series of q and h values.\n        # q_hist stores q(0), q(1), ..., q(N)\n        q_hist = np.zeros((N_STEPS + 1, 2))\n        h_hist = np.zeros((N_STEPS + 1, 2))\n        \n        # Set initial conditions at t=0\n        q_hist[0] = [Q1_0, Q2_0]\n        h_hist[0] = [Q1_0, Q2_0]\n        \n        for t in range(N_STEPS):\n            # Current online and target values\n            q1_t, q2_t = q_hist[t]\n            h1_t, h2_t = h_hist[t]\n            \n            # Calculate next online values q(t+1)\n            q1_tp1 = q1_t + ALPHA * (R1 + GAMMA * h2_t - q1_t)\n            q2_tp1 = q2_t + ALPHA * (R2 + GAMMA * h1_t - q2_t)\n            q_hist[t+1] = [q1_tp1, q2_tp1]\n            \n            # Update target network for next step h(t+1)\n            if (t + 1) % T == 0:\n                h_hist[t+1] = [q1_tp1, q2_tp1]\n            else:\n                h_hist[t+1] = h_hist[t]\n\n        # Part 3: Measurement via FFT\n        \n        # Create the difference mode time series d(t) = q1(t) - q2(t) for t=0...N-1\n        d_series = q_hist[:N_STEPS, 0] - q_hist[:N_STEPS, 1]\n        \n        # Compute the FFT magnitude spectrum\n        fft_magnitudes = np.abs(np.fft.fft(d_series))\n        \n        # Get the corresponding frequencies in cycles per step\n        fft_frequencies = np.fft.fftfreq(N_STEPS)\n        \n        # Find the peak frequency, excluding the DC component (at index 0)\n        # We only need to check the positive frequencies (first half of the array)\n        positive_freq_range = slice(1, N_STEPS // 2)\n        \n        if np.all(fft_magnitudes[positive_freq_range] == 0):\n             peak_idx = 0 # No signal\n        else:\n            # Add 1 to peak_idx because we searched in a slice starting at index 1\n            peak_idx = np.argmax(fft_magnitudes[positive_freq_range]) + 1\n        \n        # Peak frequency in cycles per step\n        peak_freq_cycles = fft_frequencies[peak_idx]\n        \n        # Convert to angular frequency in radians per step\n        omega_meas = 2 * np.pi * peak_freq_cycles\n\n        # Store the results for this test case\n        results.append([\n            round(omega_pred, 6), \n            round(omega_meas, 6), \n            is_resonant\n        ])\n\n    # Final print statement in the exact required format.\n    # The format requires a boolean literal 'True' or 'False'\n    result_str = ','.join([\n        f\"[{res[0]},{res[1]},{str(res[2])}]\" for res in results\n    ])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3113592"}, {"introduction": "一个强大的强化学习智能体会精确地优化你给定的目标，但这可能与你真正的意图相去甚远，这种现象被称为“奖励骇客”（reward hacking）。本练习 [@problem_id:3113621] 让你直面这个在应用强化学习时普遍存在的挑战。你将通过构建一个具体的马尔可夫决策过程来模拟奖励骇客的发生，并探索一种被称为“基于势的奖励塑造”（potential-based reward shaping）的理论工具，以修正代理奖励函数，确保智能体的行为与我们的最终目标保持一致。", "problem": "考虑一个具有有限状态空间和确定性动态的马尔可夫决策过程（MDP）。设状态集为 $\\mathcal{S} = \\{s_0, s_{\\mathrm{loop}}, s_{\\mathrm{goal}}\\}$，动作集为 $\\mathcal{A} = \\{a_{\\mathrm{goal}}, a_{\\mathrm{loop}}\\}$，起始状态分布为 $\\mu$，其中 $\\mu(s_0) = 1$，$\\mu(s_{\\mathrm{loop}}) = 0$，$\\mu(s_{\\mathrm{goal}}) = 0$。转移动态是确定性的，定义如下：在 $s_0$ 中采取动作 $a_{\\mathrm{goal}}$ 会到达 $s_{\\mathrm{goal}}$，在 $s_0$ 中采取动作 $a_{\\mathrm{loop}}$ 会到达 $s_{\\mathrm{loop}}$，在 $s_{\\mathrm{loop}}$ 中采取动作 $a_{\\mathrm{loop}}$ 会使智能体保持在 $s_{\\mathrm{loop}}$，在 $s_{\\mathrm{goal}}$ 中采取动作 $a_{\\mathrm{goal}}$ 会使智能体保持在 $s_{\\mathrm{goal}}$。\n\n在状态-动作-下一状态三元组上定义两个确定性的奖励函数：意图奖励 $r$ 和一个代理奖励 $r'$：\n- $r(s_0, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 10$, $r(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, $r(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, and $r(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$.\n- $r'(s_0, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$, $r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, $r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 1$, and $r'(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$.\n\n考虑将状态映射到动作的静态确定性策略 $\\pi$。设 $\\pi_{\\mathrm{loop}}$ 为在 $s_0$ 中选择 $a_{\\mathrm{loop}}$、在 $s_{\\mathrm{loop}}$ 中选择 $a_{\\mathrm{loop}}$、在 $s_{\\mathrm{goal}}$ 中选择 $a_{\\mathrm{goal}}$ 的策略。设 $\\pi_{\\mathrm{goal}}$ 为在 $s_0$ 中选择 $a_{\\mathrm{goal}}$、在 $s_{\\mathrm{loop}}$ 中选择 $a_{\\mathrm{loop}}$、在 $s_{\\mathrm{goal}}$ 中选择 $a_{\\mathrm{goal}}$ 的策略。\n\n对于折扣因子 $\\gamma \\in [0,1)$，在奖励函数 $r$ 和策略 $\\pi$ 下的价值函数 $V^{\\pi}$ 由 Bellman 不动点方程定义：$V^{\\pi}(s) = \\mathbb{E}[r(s, \\pi(s), s')] + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) V^{\\pi}(s')$，其中 $P(s' \\mid s, a)$ 是转移概率。从 $\\mu$ 开始的期望性能为 $J(\\pi; r) = \\sum_{s \\in \\mathcal{S}} \\mu(s) V^{\\pi}(s)$，类似地，在 $r'$ 下的期望性能为 $J(\\pi; r')$。\n\n您需要演示奖励骇客现象以及通过基于势函数的塑形来缓解该问题。通过 $r''(s,a,s') = r'(s,a,s') + \\gamma \\Phi(s') - \\Phi(s)$ 定义一个塑形后的代理奖励 $r''$，其中 $\\Phi : \\mathcal{S} \\to \\mathbb{R}$ 是一个势函数。选择 $\\Phi$ 以消除循环转移中的利用激励，方法是强制 $r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$ 和 $r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$。意图目标和代理目标之间的差异度量为 $\\left| J(\\pi; r) - J(\\pi; r') \\right|$；类似地，用 $r''$ 替代 $r'$。\n\n仅从 MDP、Bellman 不动点方程和性能 $J(\\pi; r)$ 的基本定义出发，编写一个程序，该程序：\n- 构建由给定策略 $\\pi$ 导出的转移矩阵，以及在 $r$（以及 $r'$ 和 $r''$）下的每个状态的期望即时奖励向量，使用 Bellman 方程的线性系统形式求解 $V^{\\pi}$，并为指定的起始分布 $\\mu$ 计算 $J(\\pi; r)$ 和 $J(\\pi; r')$（以及 $J(\\pi; r'')$）。\n- 对于给定的 $\\gamma$，确定一个与所述塑形约束一致的势函数 $\\Phi$，并用它来构建 $r''$。\n- 为下述每个测试用例计算差异 $\\left| J(\\pi; r) - J(\\pi; r') \\right|$（以及 $\\left| J(\\pi; r) - J(\\pi; r'') \\right|$）。\n\n测试套件：\n- 测试用例 1 (理想路径利用): $\\gamma = 0.9$，策略 $\\pi_{\\mathrm{loop}}$，差异 $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$。\n- 测试用例 2 (目标导向策略的差异): $\\gamma = 0.9$，策略 $\\pi_{\\mathrm{goal}}$，差异 $\\left| J(\\pi_{\\mathrm{goal}}; r) - J(\\pi_{\\mathrm{goal}}; r') \\right|$。\n- 测试用例 3 (塑形缓解): $\\gamma = 0.9$，策略 $\\pi_{\\mathrm{loop}}$，差异 $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r'') \\right|$，其中 $r''$ 通过满足所述约束的基于势函数的塑形构建。\n- 测试用例 4 (边界条件): $\\gamma = 0.0$，策略 $\\pi_{\\mathrm{loop}}$，差异 $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$。\n- 测试用例 5 (高折扣因子的边缘情况): $\\gamma = 0.99$，策略 $\\pi_{\\mathrm{loop}}$，差异 $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4, \\text{result}_5 \\right]$），其中每个 $\\text{result}_i$ 是一个浮点数。", "solution": "该问题要求对一个指定的马尔可夫决策过程（MDP）进行分析，以演示奖励骇客现象及其通过基于势函数的奖励塑形进行的缓解。这涉及到计算两种不同策略在三种不同奖励函数下的期望性能：意图奖励 $r$、代理奖励 $r'$ 和塑形后的代理奖励 $r''$。\n\n此分析的基础是针对静态策略 $\\pi$ 的 Bellman 方程。对于状态 $s \\in \\mathcal{S}$，价值函数 $V^{\\pi}(s)$ 是 Bellman 算子的不动点：\n$$V^{\\pi}(s) = R^{\\pi}(s) + \\gamma \\sum_{s' \\in \\mathcal{S}} P^{\\pi}(s, s') V^{\\pi}(s')$$\n其中 $R^{\\pi}(s) = \\mathbb{E}[r(s, \\pi(s), s')]$ 是在策略 $\\pi$ 下从状态 $s$ 获得的期望即时奖励，而 $P^{\\pi}(s, s')$ 是在策略 $\\pi$ 下从状态 $s$ 转移到状态 $s'$ 的概率。鉴于本问题中的确定性动态，$P^{\\pi}(s, s')$ 对于唯一的后继状态 $s'$ 为 1，否则为 0。\n\n这个线性方程组可以用矩阵-向量形式表示。设 $V^{\\pi}$ 是状态价值的列向量，$R^{\\pi}$ 是期望即时奖励的列向量，$P^{\\pi}$ 是由策略 $\\pi$ 导出的状态转移矩阵。Bellman 方程变为：\n$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi} V^{\\pi}$$\n对于折扣因子 $\\gamma \\in [0, 1)$，矩阵 $(I - \\gamma P^{\\pi})$ 是可逆的，这允许我们直接求解价值函数：\n$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$\n在奖励函数 $r$ 下，策略 $\\pi$ 的整体性能是在给定起始状态分布 $\\mu$ 的情况下初始状态的期望值：\n$$J(\\pi; r) = \\mathbb{E}_{s_0 \\sim \\mu}[V^{\\pi}(s_0)] = \\mu^T V^{\\pi}$$\n在此问题中，分布 $\\mu$ 集中在 $s_0$ 上，即 $\\mu(s_0) = 1$。因此，$J(\\pi; r) = V^{\\pi}(s_0)$。\n\n我们将使用数值表示来代表这些状态：$s_0 \\to 0$，$s_{\\mathrm{loop}} \\to 1$，$s_{\\mathrm{goal}} \\to 2$。起始状态分布向量为 $\\mu = [1, 0, 0]^T$。\n\n首先，我们为给定的策略 $\\pi_{\\mathrm{loop}}$ 和 $\\pi_{\\mathrm{goal}}$ 构建转移矩阵和奖励向量。\n对于 $\\pi_{\\mathrm{loop}} = \\{ s_0 \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{loop}} \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{goal}} \\mapsto a_{\\mathrm{goal}} \\}$：转移是 $s_0 \\to s_{\\mathrm{loop}}$，$s_{\\mathrm{loop}} \\to s_{\\mathrm{loop}}$ 以及 $s_{\\mathrm{goal}} \\to s_{\\mathrm{goal}}$。相应的转移矩阵和奖励向量为：\n$$P^{\\pi_{\\mathrm{loop}}} = \\begin{pmatrix} 0  1  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{loop}}}(r) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{loop}}}(r') = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n对于 $\\pi_{\\mathrm{goal}} = \\{ s_0 \\mapsto a_{\\mathrm{goal}}, s_{\\mathrm{loop}} \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{goal}} \\mapsto a_{\\mathrm{goal}} \\}$：转移是 $s_0 \\to s_{\\mathrm{goal}}$，$s_{\\mathrm{loop}} \\to s_{\\mathrm{loop}}$ 以及 $s_{\\mathrm{goal}} \\to s_{\\mathrm{goal}}$。\n$$P^{\\pi_{\\mathrm{goal}}} = \\begin{pmatrix} 0  0  1 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{goal}}}(r) = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{goal}}}(r') = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n\n第三个奖励函数 $r''$ 是对 $r'$ 的基于势函数的塑形：$r''(s,a,s') = r'(s,a,s') + \\gamma \\Phi(s') - \\Phi(s)$。势函数 $\\Phi: \\mathcal{S} \\to \\mathbb{R}$ 必须满足两个约束：$r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$ 和 $r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$。\n第一个约束给出：\n$r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_{\\mathrm{loop}}) = 0$\n$1 + (\\gamma - 1)\\Phi(s_{\\mathrm{loop}}) = 0 \\implies \\Phi(s_{\\mathrm{loop}}) = \\frac{1}{1-\\gamma}$\n第二个约束给出：\n$r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_0) = 0$\n$0 + \\gamma \\left(\\frac{1}{1-\\gamma}\\right) - \\Phi(s_0) = 0 \\implies \\Phi(s_0) = \\frac{\\gamma}{1-\\gamma}$\n目标状态的势函数 $\\Phi(s_{\\mathrm{goal}})$ 不受约束。为简单起见，我们设置 $\\Phi(s_{\\mathrm{goal}}) = 0$。\n然后，我们构建策略 $\\pi_{\\mathrm{loop}}$ 下塑形奖励 $r''$ 的奖励向量 $R^{\\pi_{\\mathrm{loop}}}(r'')$。\n- 对于 $s_0$：$R^{\\pi_{\\mathrm{loop}}}(s_0, r'') = r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_0) = 0 + \\gamma(\\frac{1}{1-\\gamma}) - \\frac{\\gamma}{1-\\gamma} = 0$。\n- 对于 $s_{\\mathrm{loop}}$：$R^{\\pi_{\\mathrm{loop}}}(s_{\\mathrm{loop}}, r'') = r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_{\\mathrm{loop}}) = 1 + (\\gamma-1)(\\frac{1}{1-\\gamma}) = 0$。\n- 对于 $s_{\\mathrm{goal}}$：$R^{\\pi_{\\mathrm{loop}}}(s_{\\mathrm{goal}}, r'') = r'(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) + \\gamma \\Phi(s_{\\mathrm{goal}}) - \\Phi(s_{\\mathrm{goal}}) = 0 + (\\gamma-1)(0) = 0$。\n因此，$R^{\\pi_{\\mathrm{loop}}}(r'') = [0, 0, 0]^T$。值得注意的是，这与 $R^{\\pi_{\\mathrm{loop}}}(r)$ 相同。\n\n程序将为每个测试用例实现此过程。它计算所需的转移矩阵和奖励向量，求解价值函数 $V^{\\pi}(r)$、$V^{\\pi}(r')$ 和（在适用的情况下）$V^{\\pi}(r'')$，然后计算性能 $J(\\pi; \\cdot)$ 作为起始状态 $s_0$ 的价值。最后一步是计算在意图奖励和代理/塑形奖励下的性能之间的绝对差。\n\n例如，考虑测试用例 1 ($\\gamma = 0.9, \\pi_{\\mathrm{loop}}$)。\n$V^{\\pi_{\\mathrm{loop}}}(r) = (I - 0.9 P^{\\pi_{\\mathrm{loop}}})^{-1} R^{\\pi_{\\mathrm{loop}}}(r) = (I - 0.9 P^{\\pi_{\\mathrm{loop}}})^{-1} [0, 0, 0]^T = [0, 0, 0]^T$。\n因此，$J(\\pi_{\\mathrm{loop}}; r) = V^{\\pi_{\\mathrm{loop}}}(s_0, r) = 0$。\n对于代理奖励 $r'$：\n$V^{\\pi_{\\mathrm{loop}}}(r') = \\begin{pmatrix} 1  -0.9  0 \\\\ 0  0.1  0 \\\\ 0  0  0.1 \\end{pmatrix}^{-1} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1  9  0 \\\\ 0  10  0 \\\\ 0  0  10 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 10 \\\\ 0 \\end{pmatrix}$。\n因此，$J(\\pi_{\\mathrm{loop}}; r') = V^{\\pi_{\\mathrm{loop}}}(s_0, r') = 9$。\n差异为 $|0 - 9| = 9$。策略 $\\pi_{\\mathrm{loop}}$ 获得了 0 的真实奖励，但被代理奖励激励进入循环，在 $r'$ 下产生了 9 的高分，这演示了奖励骇客。\n\n剩余的测试用例通过应用相同的计算过程并使用各自的参数来解决。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MDP problem for all test cases as specified.\n    \"\"\"\n    # State and action mappings to indices\n    S_MAP = {'s0': 0, 's_loop': 1, 's_goal': 2}\n    A_MAP = {'a_goal': 0, 'a_loop': 1}\n    NUM_STATES = 3\n\n    # Define deterministic transition dynamics T(s, a) -> s'\n    transitions = {\n        (S_MAP['s0'], A_MAP['a_goal']): S_MAP['s_goal'],\n        (S_MAP['s0'], A_MAP['a_loop']): S_MAP['s_loop'],\n        (S_MAP['s_loop'], A_MAP['a_loop']): S_MAP['s_loop'],\n        (S_MAP['s_goal'], A_MAP['a_goal']): S_MAP['s_goal'],\n    }\n\n    # Define deterministic reward functions r(s, a, s')\n    rewards_r = {\n        (S_MAP['s0'], A_MAP['a_goal'], S_MAP['s_goal']): 10.0,\n    }\n    rewards_rp = {\n        (S_MAP['s_loop'], A_MAP['a_loop'], S_MAP['s_loop']): 1.0,\n    }\n\n    # Define policies pi(s) -> a\n    pi_loop = {\n        S_MAP['s0']: A_MAP['a_loop'],\n        S_MAP['s_loop']: A_MAP['a_loop'],\n        S_MAP['s_goal']: A_MAP['a_goal'],\n    }\n    pi_goal = {\n        S_MAP['s0']: A_MAP['a_goal'],\n        S_MAP['s_loop']: A_MAP['a_loop'],\n        S_MAP['s_goal']: A_MAP['a_goal'],\n    }\n    policies = {'loop': pi_loop, 'goal': pi_goal}\n\n    # Start state distribution mu(s)\n    mu = np.zeros(NUM_STATES)\n    mu[S_MAP['s0']] = 1.0\n\n    def compute_performance(gamma, pi, R_pi):\n        \"\"\"\n        Computes the performance J(pi) = mu^T * V^pi.\n        V^pi is solved via the matrix inversion form of the Bellman equation.\n        \"\"\"\n        P_pi = np.zeros((NUM_STATES, NUM_STATES))\n        for s in range(NUM_STATES):\n            a = pi[s]\n            s_prime = transitions[(s, a)]\n            P_pi[s, s_prime] = 1.0\n\n        I = np.identity(NUM_STATES)\n        \n        # Handle gamma=0 case separately to avoid linalg with zero matrix\n        if gamma == 0.0:\n            V_pi = R_pi\n        else:\n            # V_pi = (I - gamma * P_pi)^-1 * R_pi\n            try:\n                inv_matrix = np.linalg.inv(I - gamma * P_pi)\n                V_pi = inv_matrix @ R_pi\n            except np.linalg.LinAlgError:\n                # Should not happen for gamma in [0, 1)\n                return float('nan')\n        \n        # J(pi) = sum_s mu(s) * V^pi(s)\n        J = mu @ V_pi\n        return J\n\n    def calculate_divergence(gamma, policy_name, divergence_type):\n        \"\"\"\n        Calculates the divergence for a given test case configuration.\n        \"\"\"\n        pi = policies[policy_name]\n\n        # Construct per-state reward vectors R_pi for r and r'\n        R_pi_r = np.zeros(NUM_STATES)\n        R_pi_rp = np.zeros(NUM_STATES)\n        for s in range(NUM_STATES):\n            a = pi[s]\n            s_prime = transitions[(s, a)]\n            R_pi_r[s] = rewards_r.get((s, a, s_prime), 0.0)\n            R_pi_rp[s] = rewards_rp.get((s, a, s_prime), 0.0)\n\n        # Compute performance under the intended reward 'r'\n        J_r = compute_performance(gamma, pi, R_pi_r)\n\n        if divergence_type == 'proxy':\n            # Compute performance under the proxy reward 'r_prime'\n            J_rp = compute_performance(gamma, pi, R_pi_rp)\n            return abs(J_r - J_rp)\n        \n        elif divergence_type == 'shaped':\n            # This case is only for pi_loop as per problem statement\n            # Determine potential function Phi from constraints\n            # Phi(s_loop) = 1 / (1-gamma)\n            # Phi(s0) = gamma * Phi(s_loop)\n            # Phi(s_goal) = 0 (unconstrained, set to 0 for simplicity)\n            if gamma  1.0:\n                phi_s_loop = 1.0 / (1.0 - gamma)\n                phi_s0 = gamma * phi_s_loop\n            else: # Should not be reached with gamma  1\n                phi_s_loop = float('inf')\n                phi_s0 = float('inf')\n\n            Phi = np.array([phi_s0, phi_s_loop, 0.0])\n\n            # Construct shaped reward vector R_pi_rpp\n            R_pi_rpp = np.zeros(NUM_STATES)\n            for s in range(NUM_STATES):\n                a = pi[s]\n                s_prime = transitions[(s, a)]\n                r_prime_val = rewards_rp.get((s, a, s_prime), 0.0)\n                R_pi_rpp[s] = r_prime_val + gamma * Phi[s_prime] - Phi[s]\n            \n            # Compute performance under the shaped reward 'r_double_prime'\n            J_rpp = compute_performance(gamma, pi, R_pi_rpp)\n            return abs(J_r - J_rpp)\n            \n        return float('nan')\n\n    test_cases = [\n        # (gamma, policy_name, divergence_type)\n        (0.9, 'loop', 'proxy'),\n        (0.9, 'goal', 'proxy'),\n        (0.9, 'loop', 'shaped'),\n        (0.0, 'loop', 'proxy'),\n        (0.99, 'loop', 'proxy'),\n    ]\n\n    results = []\n    for gamma, policy_name, divergence_type in test_cases:\n        result = calculate_divergence(gamma, policy_name, divergence_type)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.1f}' for r in results)}]\")\n\nsolve()\n\n```", "id": "3113621"}]}