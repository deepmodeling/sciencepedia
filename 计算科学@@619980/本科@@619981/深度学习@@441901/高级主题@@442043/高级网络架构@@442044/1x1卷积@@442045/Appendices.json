{"hands_on_practices": [{"introduction": "这个实践提供了一个动手编码练习，以建立对 $1 \\times 1$ 卷积核心功能和局限性的直观理解。通过构建两个合成数据集，你将直接观察到 $1 \\times 1$ 卷积如何作为一个空间不变的通道混合器 [@problem_id:3094354]。这将阐明为什么它在仅凭通道信息即可分离的任务上能成功，但在空间背景至关重要时，除非与其它具有空间感知能力的操作结合，否则会失败。", "problem": "在卷积神经网络 (CNN) 的背景下，思考图像张量和分类规则。CNN 中的 $1 \\times 1$ 卷积是一种线性通道混合器，它在每个空间位置上应用相同的线性映射。具体来说，对于一个输入张量 $X \\in \\mathbb{R}^{H \\times W \\times C}$，一个权重向量为 $w \\in \\mathbb{R}^{C}$ 且标量偏置为 $b \\in \\mathbb{R}$ 的 $1 \\times 1$ 卷积会产生一个单通道输出 $Z \\in \\mathbb{R}^{H \\times W}$，其中对于所有空间索引 $(i,j)$，$Z_{i,j} = b + w^\\top X_{i,j,:}$。全局平均池化 (GAP) 计算 $z = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} Z_{i,j}$。Network In Network (NIN) 的观点强调使用此类局部线性映射（可能后跟非线性变换）通过通道混合来增强建模能力。\n\n你的任务是在两个合成数据集上实现并评估两种分类模型：\n\n- 模型 $\\mathcal{M}_1$（仅通道混合）：使用单个 $1 \\times 1$ 卷积，其权重 $w \\in \\mathbb{R}^{C}$ 固定，偏置 $b = 0$，然后进行全局平均池化，并以 $0$ 为阈值进行分类。也就是说，计算 $Z_{i,j} = w^\\top X_{i,j,:}$，然后 $z = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} Z_{i,j}$，如果 $z \\ge 0$ 则预测 $\\hat{y} = 1$，否则预测 $\\hat{y} = 0$。\n\n- 模型 $\\mathcal{M}_2$（空间上下文感知线性池化）：首先应用相同的 $1 \\times 1$ 卷积 $Z_{i,j} = w^\\top X_{i,j,:}$，然后使用一个固定的掩码 $M \\in \\mathbb{R}^{H \\times W}$ 进行空间非均匀线性池化。掩码定义为：对于左半部分（所有列 $j$ 满足 $j \\le W/2$），$M_{i,j} = +1$；对于右半部分（所有列 $j$ 满足 $j > W/2$），$M_{i,j} = -1$。计算 $z = \\sum_{i=1}^{H} \\sum_{j=1}^{W} M_{i,j} Z_{i,j}$，并以阈值进行分类：如果 $z \\ge 0$ 则 $\\hat{y} = 1$，否则 $\\hat{y} = 0$。\n\n定义两个数据集：\n\n- 数据集 A（可通过通道混合分离）：对于每个图像 $X \\in \\mathbb{R}^{H \\times W \\times C}$，固定一个非零向量 $\\alpha \\in \\mathbb{R}^{C}$。让二元标签 $y \\in \\{0,1\\}$ 由一个潜在符号 $s \\in \\{-1,+1\\}$ 决定，其中 $s = +1$ 对应 $y=1$，$s = -1$ 对应 $y=0$。通过将每个空间位置设置为相同的通道向量 $X_{i,j,:} = s \\cdot \\alpha$ 来构建每个图像（对所有 $(i,j)$）。该数据集可通过通道混合器线性分离，因为类别仅取决于通道上固定线性形式的符号，而与空间排列无关。假设 $b = 0$ 并使用 $w = \\alpha$。\n\n- 数据集 B（空间纠缠变体）：对于每个宽度 $W$ 为偶数的图像 $X \\in \\mathbb{R}^{H \\times W \\times C}$，使用相同的 $\\alpha \\in \\mathbb{R}^{C}$ 和通过 $s \\in \\{-1,+1\\}$ 决定的相同二元标签 $y$。将左半部分像素设置为 $X_{i,j,:} = s \\cdot \\alpha$（对所有 $j \\le W/2$），并将右半部分像素设置为 $X_{i,j,:} = -s \\cdot \\alpha$（对所有 $j > W/2$）。所有空间位置的全局平均值为零，因此任何空间均匀的通道混合器后跟全局平均池化都会丢失正确分类所需的空间信息。假设 $b = 0$ 并使用 $w = \\alpha$。\n\n你必须实现一个程序，对于每个指定的测试用例，确定性地构建这两个数据集，其中 $N$ 个样本的标签 $s$ 交替出现（即，当样本索引 $k$ 为偶数时 $s = +1$，当 $k$ 为奇数时 $s = -1$），并评估以下各项的分类准确率（正确比例）：\n\n- $\\mathcal{M}_1$ 在数据集 A 上的表现，\n- $\\mathcal{M}_1$ 在数据集 B 上的表现，\n- $\\mathcal{M}_2$ 在数据集 B 上的表现，\n\n并将所有测试用例的结果输出为浮点数。\n\n测试套件参数值：\n\n- 案例 1（理想情况）：$H=4$, $W=4$, $C=3$, $N=200$, $\\alpha = [1,-2,3]$。\n- 案例 2（边界尺寸）：$H=2$, $W=2$, $C=2$, $N=20$, $\\alpha = [1,1]$。\n- 案例 3（增加通道数）：$H=4$, $W=6$, $C=5$, $N=50$, $\\alpha = [1,0,-1,2,-2]$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按照上述顺序，将每个案例的三个准确率跨所有案例串联起来进行排序，总共产生九个浮点数：$[\\text{acc}_{\\mathcal{M}_1\\text{ on A, case 1}}, \\text{acc}_{\\mathcal{M}_1\\text{ on B, case 1}}, \\text{acc}_{\\mathcal{M}_2\\text{ on B, case 1}}, \\text{acc}_{\\mathcal{M}_1\\text{ on A, case 2}}, \\text{acc}_{\\mathcal{M}_1\\text{ on B, case 2}}, \\text{acc}_{\\mathcal{M}_2\\text{ on B, case 2}}, \\text{acc}_{\\mathcal{M}_1\\text{ on A, case 3}}, \\text{acc}_{\\mathcal{M}_1\\text{ on B, case 3}}, \\text{acc}_{\\mathcal{M}_2\\text{ on B, case 3}}]$。每个准确率必须表示为浮点数。", "solution": "该问题是有效的。它以卷积神经网络的原理为科学基础，提法明确，具有清晰的定义和约束，并且其表述是客观的。它提出了一个可解决的任务，并会产生唯一且有意义的结果。\n\n问题的核心是在两个合成数据集 A 和 B 上，分析两种简单分类模型 $\\mathcal{M}_1$ 和 $\\mathcal{M}_2$ 的能力与局限性。模型 $\\mathcal{M}_1$ 依赖于通道混合及随后的空间不变的全局平均池化 (GAP)，而模型 $\\mathcal{M}_2$ 则包含一个空间感知的池化机制。这些数据集旨在探究这种差异。数据集 A 在空间上是均匀的，而数据集 B 具有空间变化的内容，其设计意图是被 GAP 抵消。\n\n一个关键的解释点是图像宽度为 $W$ 的“左半部分”和“右半部分”的定义。问题陈述这分别对应于索引 $j$ 满足 $j \\le W/2$ 和 $j > W/2$ 的列。结合对于数据集 B 的提示“所有空间位置的全局平均值为零”，很明显，其意图是对列进行 50/50 的对半分割。对于偶数宽度 $W$ 和标准的从 0 开始的数组索引（$j \\in \\{0, 1, \\dots, W-1\\}$），这种分割可以通过将左半部分定义为索引 $j  W/2$ 的列，右半部分定义为索引 $j \\ge W/2$ 的列来实现。这确保了每半部分都有相同数量的列，即 $W/2$ 列，从而使得数据集 B 的全局平均值如所述为零。此解释将用于所有计算。\n\n设向量 $w = \\alpha$。点积 $w^\\top \\alpha = \\alpha^\\top \\alpha = \\sum_k \\alpha_k^2 = ||\\alpha||_2^2$。由于 $\\alpha$ 是一个非零向量，该量是一个正常数，我们将其记为 $K = ||\\alpha||_2^2  0$。给定的偏置 $b$ 为 $0$。\n\n### 模型 $\\mathcal{M}_1$ 在数据集 A 上的分析\n\n1.  **数据集 A 构建**：对于一个给定符号 $s \\in \\{-1, +1\\}$ 的样本，图像张量 $X \\in \\mathbb{R}^{H \\times W \\times C}$ 中的每个像素向量都是相同的：对于所有空间位置 $(i,j)$，$X_{i,j,:} = s \\cdot \\alpha$。当 $s=+1$ 时真实标签为 $y=1$，当 $s=-1$ 时真实标签为 $y=0$。\n\n2.  **$1 \\times 1$ 卷积**：模型首先计算 $Z \\in \\mathbb{R}^{H \\times W}$，其中每个元素 $Z_{i,j}$ 由逐通道点积给出：\n    $$Z_{i,j} = w^\\top X_{i,j,:} = \\alpha^\\top (s \\cdot \\alpha) = s \\cdot (\\alpha^\\top \\alpha) = s \\cdot K$$\n    这个值在所有空间位置 $(i,j)$ 上都是恒定的。\n\n3.  **全局平均池化 (GAP)**：下一步是计算 $Z$ 中所有元素的平均值：\n    $$z = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} Z_{i,j} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (s \\cdot K) = \\frac{1}{HW} (HW \\cdot s \\cdot K) = s \\cdot K$$\n\n4.  **分类**：如果 $z \\ge 0$，预测 $\\hat{y}$ 为 $1$，否则为 $0$。由于 $K  0$，$z$ 的符号完全由 $s$ 决定。\n    -   如果 $s = +1$，则 $z = K  0$。模型预测 $\\hat{y}=1$。真实标签是 $y=1$。这是一个正确的分类。\n    -   如果 $s = -1$，则 $z = -K  0$。模型预测 $\\hat{y}=0$。真实标签是 $y=0$。这是一个正确的分类。\n\n5.  **准确率**：模型 $\\mathcal{M}_1$ 正确分类了数据集 A 中的每个样本。因此，准确率为 $1.0$。\n\n### 模型 $\\mathcal{M}_1$ 在数据集 B 上的分析\n\n1.  **数据集 B 构建**：对于一个符号为 $s$ 的样本，图像 $X$ 在空间上被分割。设 $W_{\\text{half}} = W/2$。\n    -   左半部分（$j  W_{\\text{half}}$）：$X_{i,j,:} = s \\cdot \\alpha$。\n    -   右半部分（$j \\ge W_{\\text{half}}$）：$X_{i,j,:} = -s \\cdot \\alpha$。\n\n2.  **$1 \\times 1$ 卷积**：输出 $Z$ 反映了这种空间分割：\n    -   左半部分（$j  W_{\\text{half}}$）：$Z_{i,j} = w^\\top (s \\cdot \\alpha) = s \\cdot K$。\n    -   右半部分（$j \\ge W_{\\text{half}}$）：$Z_{i,j} = w^\\top (-s \\cdot \\alpha) = -s \\cdot K$。\n\n3.  **全局平均池化 (GAP)**：池化后的值 $z$ 是在两半上取平均。每半部分有 $H \\cdot W_{\\text{half}}$ 个像素。\n    $$z = \\frac{1}{HW} \\left( \\sum_{\\text{left}} Z_{i,j} + \\sum_{\\text{right}} Z_{i,j} \\right) = \\frac{1}{HW} \\left( (H \\cdot W_{\\text{half}}) \\cdot (s \\cdot K) + (H \\cdot W_{\\text{half}}) \\cdot (-s \\cdot K) \\right)$$\n    $$z = \\frac{H W_{\\text{half}}}{HW} (sK - sK) = 0$$\n    对于每个样本，无论其类别符号 $s$ 为何，池化输出均为 $0$。\n\n4.  **分类**：决策规则是如果 $z \\ge 0$，则 $\\hat{y}=1$。由于所有样本的 $z=0$，模型一致地预测 $\\hat{y}=1$。\n    -   对于 $s = +1$ 的样本，真实标签是 $y=1$。预测 $\\hat{y}=1$ 是正确的。\n    -   对于 $s = -1$ 的样本，真实标签是 $y=0$。预测 $\\hat{y}=1$ 是不正确的。\n\n5.  **准确率**：数据集的构建方式使得每个类别的样本数量相等（在所有测试用例中，$N$ 都是偶数，且符号交替）。因此，该模型对于恰好一半的数据集是正确的。准确率为 $0.5$。\n\n### 模型 $\\mathcal{M}_2$ 在数据集 B 上的分析\n\n1.  **数据集 B 和 $1 \\times 1$ 卷积**：设置与前面的分析相同，得出左半部分的 $Z_{i,j} = s \\cdot K$ 和右半部分的 $Z_{i,j} = -s \\cdot K$。\n\n2.  **空间非均匀线性池化**：模型 $\\mathcal{M}_2$ 用一个使用掩码 $M \\in \\mathbb{R}^{H \\times W}$ 的加权和取代了 GAP。该掩码定义为左半部分 $M_{i,j} = +1$，右半部分 $M_{i,j} = -1$。\n    $$z = \\sum_{i=1}^{H} \\sum_{j=1}^{W} M_{i,j} Z_{i,j} = \\sum_{\\text{left}} (+1) \\cdot Z_{i,j} + \\sum_{\\text{right}} (-1) \\cdot Z_{i,j}$$\n    $$z = (H \\cdot W_{\\text{half}}) \\cdot (s \\cdot K) - (H \\cdot W_{\\text{half}}) \\cdot (-s \\cdot K)$$\n    $$z = (H \\cdot W_{\\text{half}}) \\cdot sK + (H \\cdot W_{\\text{half}}) \\cdot sK = 2 \\cdot (H \\cdot W_{\\text{half}}) \\cdot sK = HWsK$$\n\n3.  **分类**：池化后的值为 $z = HWsK$。由于 $H, W, K$ 都是正常数，所以 $z$ 的符号就是 $s$ 的符号。\n    -   如果 $s = +1$，则 $z = HWK  0$。模型预测 $\\hat{y}=1$。真实标签是 $y=1$。这是正确的。\n    -   如果 $s = -1$，则 $z = -HWK  0$。模型预测 $\\hat{y}=0$。真实标签是 $y=0$。这是正确的。\n\n4.  **准确率**：模型 $\\mathcal{M}_2$ 正确分类了数据集 B 中的所有样本。准确率为 $1.0$。\n\n### 总结与实现\n\n分析结果是一致的，并且独立于测试用例中提供的具体参数 $H, C, N$ 以及非零 $\\alpha$ 的选择。对于每个案例，预期的准确率是：\n-   $\\mathcal{M}_1$ 在数据集 A 上：$1.0$\n-   $\\mathcal{M}_1$ 在数据集 B 上：$0.5$\n-   $\\mathcal{M}_2$ 在数据集 B 上：$1.0$\n\n程序将为每个测试用例模拟这些过程以生成所需的输出。它将逐个样本构建数据集，应用所定义的模型，并计算分类准确率。这证实了理论分析，并遵守了任务的实现要求。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates two classification models on two synthetic datasets\n    as per the problem description.\n    \"\"\"\n    test_cases = [\n        {'H': 4, 'W': 4, 'C': 3, 'N': 200, 'alpha': np.array([1, -2, 3], dtype=np.float64)},\n        {'H': 2, 'W': 2, 'C': 2, 'N': 20, 'alpha': np.array([1, 1], dtype=np.float64)},\n        {'H': 4, 'W': 6, 'C': 5, 'N': 50, 'alpha': np.array([1, 0, -1, 2, -2], dtype=np.float64)},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        H, W, C, N = case['H'], case['W'], case['C'], case['N']\n        alpha = case['alpha']\n        w = alpha\n\n        # --- Evaluation 1: Model M1 on Dataset A ---\n        correct_count_m1a = 0\n        for k in range(N):\n            s = 1.0 if k % 2 == 0 else -1.0\n            y_true = 1 if s > 0 else 0\n            \n            # Construct image X for Dataset A\n            X = np.full((H, W, C), s * alpha)\n            \n            # Apply Model M1\n            # 1x1 convolution\n            Z = np.tensordot(X, w, axes=([-1], [0]))\n            # Global Average Pooling\n            z = np.mean(Z)\n            # Classification\n            y_pred = 1 if z >= 0 else 0\n            \n            if y_pred == y_true:\n                correct_count_m1a += 1\n                \n        all_results.append(correct_count_m1a / N)\n\n        # --- Evaluation 2: Model M1 on Dataset B ---\n        # --- and Evaluation 3: Model M2 on Dataset B ---\n        \n        # We can evaluate both models on Dataset B in a single loop\n        correct_count_m1b = 0\n        correct_count_m2b = 0\n        \n        W_half = W // 2\n        \n        # Pre-build the mask for Model M2\n        M = np.ones((H, W))\n        M[:, W_half:] = -1.0\n\n        for k in range(N):\n            s = 1.0 if k % 2 == 0 else -1.0\n            y_true = 1 if s > 0 else 0\n\n            # Construct image X for Dataset B\n            X = np.zeros((H, W, C))\n            X[:, :W_half, :] = s * alpha\n            X[:, W_half:, :] = -s * alpha\n\n            # After 1x1 convolution, Z is common for both models\n            Z = np.tensordot(X, w, axes=([-1], [0]))\n            \n            # Apply Model M1 on Z\n            z_m1 = np.mean(Z)\n            y_pred_m1 = 1 if z_m1 >= 0 else 0\n            if y_pred_m1 == y_true:\n                correct_count_m1b += 1\n\n            # Apply Model M2 on Z\n            z_m2 = np.sum(M * Z)\n            y_pred_m2 = 1 if z_m2 >= 0 else 0\n            if y_pred_m2 == y_true:\n                correct_count_m2b += 1\n\n        all_results.append(correct_count_m1b / N)\n        all_results.append(correct_count_m2b / N)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3094354"}, {"introduction": "在理解了 $1 \\times 1$ 卷积的机制之后，我们现在来探讨其最重要的架构角色：创建高效的瓶颈结构。本练习将瓶颈模块的设计构建为一个约束优化问题，在计算成本（FLOPs）和模型精度之间进行权衡 [@problem_id:3094430]。通过求解最佳瓶颈宽度，你将对 $1 \\times 1$ 卷积如何实现更深、更高效的网络有一个定量的理解。", "problem": "您正在分析一个受深度学习中 Network in Network 概念启发的瓶颈块：它由一个三卷积序列组成，首先是一个 $1 \\times 1$ 卷积，将通道数从 $C_{\\text{in}}$ 减少到 $C_{\\text{mid}}$；然后是一个空间 $k \\times k$ 卷积，将 $C_{\\text{mid}}$ 映射到 $C_{\\text{mid}}$；最后是一个 $1 \\times 1$ 卷积，将通道数从 $C_{\\text{mid}}$ 扩展到 $C_{\\text{out}}$。输入特征图的空间维度为 $H \\times W$，拥有 $C_{\\text{in}}$ 个通道；输出特征图拥有 $C_{\\text{out}}$ 个通道。所有量 $H$、$W$、$C_{\\text{in}}$、$C_{\\text{out}}$、$k$ 均为正整数。\n\n您需要选择瓶颈宽度 $C_{\\text{mid}}$ 以平衡计算成本和准确率。计算成本以浮点运算次数 (FLOPs) 衡量，其中浮点运算次数 (FLOPs) 是指每次通过该块进行前向传播时的乘加计算次数。该块对验证准确率的贡献由一个具有收益递减性质的严格递增凹函数建模，该函数为 $A(C_{\\text{mid}}) = A_{0} + \\alpha \\ln(C_{\\text{mid}})$，其中 $A_{0}$ 和 $\\alpha$ 是通过在开发集上进行经验校准得出的正常数。您需要达到目标验证准确率 $A_{\\text{t}}$，且 $A_{\\text{t}}  A_{0}$。\n\n请从离散网格上的卷积计算定义以及在所有空间位置和通道上对 $1 \\times 1$ 和 $k \\times k$ 卷积的乘法计数出发，推导出三层瓶颈块的总 FLOPs 作为 $C_{\\text{mid}}$ 的函数，并确定当 $H, W, C_{\\text{in}}, C_{\\text{out}}, k$ 为正数时，该函数对于 $C_{\\text{mid}}$ 是否是单调的。然后，构建并求解在准确率约束 $A(C_{\\text{mid}}) \\geq A_{\\text{t}}$ 下最小化总 FLOPs 的约束优化问题。以单个解析表达式的闭式解形式给出最优的 $C_{\\text{mid}}$。如果需要任何近似，请说明并证明其合理性。无需取整。", "solution": "经评估，该问题是有效的。其科学依据在于深度学习的原理，特别是关于卷积神经网络架构和计算成本分析。作为一个约束优化问题，其提法是适定的，具有明确定义的目标函数和约束函数。语言客观，设定自洽且一致，其中通过填充保留空间维度的隐式假设在此背景下是一种标准惯例。\n\n第一步是推导三层瓶颈块的总计算成本，以浮点运算次数 (FLOPs) 衡量。单次乘加运算计为一次 FLOP。假设特征图的空间维度 $H \\times W$ 在整个块中保持不变，这是通过在卷积层中使用适当的填充来实现的标准做法。\n\n对于一个卷积层，如果它从大小为 $H_{\\text{in}} \\times W_{\\text{in}} \\times C_{\\text{in}}$ 的输入图，使用大小为 $k \\times k$ 的卷积核，生成一个大小为 $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}}$ 的输出图，其计算成本由输出位置数量、输出通道数量以及每个输出值的操作数相乘得到。每个输出值是大小为 $k \\times k \\times C_{\\text{in}}$ 的点积结果。因此，FLOPs 为 $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times k^2$。\n\n我们将此公式应用于瓶颈块中的每一层：\n\n1.  **第一层：** 一个 $1 \\times 1$ 卷积，将通道数从 $C_{\\text{in}}$ 减少到 $C_{\\text{mid}}$。\n    *   输入通道数：$C_{\\text{in}}$\n    *   输出通道数：$C_{\\text{mid}}$\n    *   卷积核大小：$1 \\times 1$ (即 $k=1$)\n    *   输出图大小：$H \\times W$\n    *   第一层 FLOPs, $F_1$: $H \\times W \\times C_{\\text{mid}} \\times C_{\\text{in}} \\times 1^2 = HW C_{\\text{in}} C_{\\text{mid}}$。\n\n2.  **第二层：** 一个 $k \\times k$ 空间卷积。\n    *   输入通道数：$C_{\\text{mid}}$\n    *   输出通道数：$C_{\\text{mid}}$\n    *   卷积核大小：$k \\times k$\n    *   输出图大小：$H \\times W$\n    *   第二层 FLOPs, $F_2$: $H \\times W \\times C_{\\text{mid}} \\times C_{\\text{mid}} \\times k^2 = HW k^2 C_{\\text{mid}}^2$。\n\n3.  **第三层：** 一个 $1 \\times 1$ 卷积，将通道数从 $C_{\\text{mid}}$ 扩展到 $C_{\\text{out}}$。\n    *   输入通道数：$C_{\\text{mid}}$\n    *   输出通道数：$C_{\\text{out}}$\n    *   卷积核大小：$1 \\times 1$\n    *   输出图大小：$H \\times W$\n    *   第三层 FLOPs, $F_3$: $H \\times W \\times C_{\\text{out}} \\times C_{\\text{mid}} \\times 1^2 = HW C_{\\text{out}} C_{\\text{mid}}$。\n\n总 FLOPs $F(C_{\\text{mid}})$ 是三层 FLOPs 的和：\n$$ F(C_{\\text{mid}}) = F_1 + F_2 + F_3 = HW C_{\\text{in}} C_{\\text{mid}} + HW k^2 C_{\\text{mid}}^2 + HW C_{\\text{out}} C_{\\text{mid}} $$\n提取公因式后，我们得到总 FLOPs 作为 $C_{\\text{mid}}$ 的函数：\n$$ F(C_{\\text{mid}}) = HW \\left( k^2 C_{\\text{mid}}^2 + (C_{\\text{in}} + C_{\\text{out}})C_{\\text{mid}} \\right) $$\n这是一个关于 $C_{\\text{mid}}$ 的二次函数。\n\n为确定该函数对于 $C_{\\text{mid}}$ 是否单调，我们考察其导数。为便于分析，将 $C_{\\text{mid}}$ 视为一个连续正变量：\n$$ \\frac{dF}{dC_{\\text{mid}}} = HW \\left( 2k^2 C_{\\text{mid}} + (C_{\\text{in}} + C_{\\text{out}}) \\right) $$\n根据问题陈述，$H, W, k, C_{\\text{in}}, C_{\\text{out}}$ 均为正整数。瓶颈宽度 $C_{\\text{mid}}$ 也必须是正量。因此，导数表达式中的每一项都是正的：$H0$，$W0$，$2k^20$，$C_{\\text{mid}}0$，以及 $(C_{\\text{in}}+C_{\\text{out}})0$。所以，对于所有 $C_{\\text{mid}}  0$，都有 $\\frac{dF}{dC_{\\text{mid}}}  0$。这证明了总 FLOPs 函数 $F(C_{\\text{mid}})$ 对于正的 $C_{\\text{mid}}$ 是严格递增的。\n\n接下来，我们构建并求解约束优化问题。我们希望在满足验证准确率约束的条件下，最小化计算成本 $F(C_{\\text{mid}})$。该优化问题为：\n$$ \\min_{C_{\\text{mid}}  0} F(C_{\\text{mid}}) = HW \\left( k^2 C_{\\text{mid}}^2 + (C_{\\text{in}} + C_{\\text{out}})C_{\\text{mid}} \\right) $$\n约束条件为：\n$$ A(C_{\\text{mid}}) \\geq A_{\\text{t}} $$\n其中 $A(C_{\\text{mid}}) = A_{0} + \\alpha \\ln(C_{\\text{mid}})$。\n\n我们首先求解关于 $C_{\\text{mid}}$ 的约束条件。\n$$ A_{0} + \\alpha \\ln(C_{\\text{mid}}) \\geq A_{\\text{t}} $$\n已知 $\\alpha  0$ 且 $A_{\\text{t}}  A_{0}$：\n$$ \\alpha \\ln(C_{\\text{mid}}) \\geq A_{\\text{t}} - A_{0} $$\n$$ \\ln(C_{\\text{mid}}) \\geq \\frac{A_{\\text{t}} - A_{0}}{\\alpha} $$\n由于指数函数是严格递增的，我们可以在不等式两边同时取指数而不改变不等号方向：\n$$ C_{\\text{mid}} \\geq \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right) $$\n这个不等式定义了 $C_{\\text{mid}}$ 的可行域。\n\n我们已经确定目标函数 $F(C_{\\text{mid}})$ 对于 $C_{\\text{mid}}  0$ 是严格递增的。要最小化一个严格递增的函数，必须从可行集中选择其自变量的最小值。因此，$F(C_{\\text{mid}})$ 的最小值将在满足约束条件的 $C_{\\text{mid}}$ 的最小值处取得。\n\n满足 $C_{\\text{mid}} \\geq \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right)$ 的最小 $C_{\\text{mid}}$ 值恰好在可行域的边界上。尽管在实际实现中 $C_{\\text{mid}}$ 必须是整数，但问题要求提供一个闭式解析表达式，并且不需要取整。这意味着我们应该在连续域中解决该问题，这是对此类问题的标准松弛方法。因此，最优值为：\n$$ C_{\\text{mid, opt}} = \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right) $$\n该表达式给出了达到目标准确率 $A_{\\text{t}}$ 所需的最小瓶颈宽度，并且由于成本函数是单调的，这个 $C_{\\text{mid}}$ 的选择也最小化了计算成本。", "answer": "$$\\boxed{\\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right)}$$", "id": "3094430"}, {"introduction": "这最后一个实践将挑战你实现开创性的“网络中的网络”（NiN）概念，这也是现代 $1 \\times 1$ 卷积的起源。你将只使用 $1 \\times 1$ 卷积和 ReLU 激活函数的堆叠，来构建并训练一个逐像素的多层感知机（MLP） [@problem_id:3094438]。这个练习将展示 $1 \\times 1$ 卷积如何在不改变空间感受野的情况下，通过在通道间引入复杂的非线性变换，极大地增强模型的表征能力。", "problem": "要求您仅使用空间尺寸为 $1\\times 1$ 的逐点卷积来实现和分析来自 Network in Network (NiN) 的逐像素多层感知器卷积 (MLPConv) 思想，并在合成数据集上测试分类准确率。实现必须是一个完整、可运行的程序。请从以下基本依据开始：\n\n- 离散卷积定义：对于输入张量 $X\\in\\mathbb{R}^{H\\times W\\times C}$ 和卷积核 $K\\in\\mathbb{R}^{k_h\\times k_w\\times C\\times F}$，在通道 $f$ 的空间位置 $(i,j)$ 处的输出为\n$$\nY_{i,j,f}=\\sum_{u=1}^{k_h}\\sum_{v=1}^{k_w}\\sum_{c=1}^{C}K_{u,v,c,f}\\,X_{i+u',j+v',c}\n$$\n其中 $(u',v')$ 索引了相应的空间偏移。空间尺寸为 $1\\times 1$ 的核将其简化为逐像素、跨通道的线性投影：\n$$\nY_{i,j,f}=\\sum_{c=1}^{C}W_{c,f}\\,X_{i,j,c}+b_f\n$$\n其中权重 $W\\in\\mathbb{R}^{C\\times F}$ 和偏置 $b\\in\\mathbb{R}^{F}$ 在所有 $(i,j)$ 位置上共享。\n\n- 修正线性单元 (ReLU) 激活函数：对于标量 $z$，定义 $\\operatorname{ReLU}(z)=\\max(0,z)$，并将其逐分量地应用于向量。\n\n- Softmax 函数：对于 logits $z\\in\\mathbb{R}^{K}$，定义 $\\operatorname{softmax}(z)_k=\\exp(z_k)\\big/\\sum_{t=1}^{K}\\exp(z_t)$。\n\n- 多类别分类的交叉熵损失：给定一个 one-hot 标签向量 $y\\in\\{0,1\\}^{K}$，损失为\n$$\n\\ell(z,y)=-\\sum_{k=1}^{K}y_k\\log\\left(\\operatorname{softmax}(z)_k\\right).\n$$\n\n您的任务是：\n\n- 仅使用 $1\\times 1$ 卷积构建一个三层的逐像素 MLP。对于每个像素 $(i,j)$，网络计算\n$$\nz^{(1)}_{i,j} = W_1\\,x_{i,j} + b_1,\\quad a^{(1)}_{i,j}=\\operatorname{ReLU}\\!\\left(z^{(1)}_{i,j}\\right),\n$$\n$$\nz^{(2)}_{i,j} = W_2\\,a^{(1)}_{i,j} + b_2,\\quad a^{(2)}_{i,j}=\\operatorname{ReLU}\\!\\left(z^{(2)}_{i,j}\\right),\n$$\n$$\nz^{(3)}_{i,j} = W_3\\,a^{(2)}_{i,j} + b_3,\\quad p_{i,j}=\\operatorname{softmax}\\!\\left(z^{(3)}_{i,j}\\right),\n$$\n其中 $x_{i,j}\\in\\mathbb{R}^{C}$ 是像素 $(i,j)$ 处的输入，$p_{i,j}\\in\\mathbb{R}^{K}$ 是预测的类别概率。不允许使用尺寸 $k1$ 的空间核；只允许使用 $1\\times 1$ 的核。\n\n- 从随机初始化开始，使用全批量梯度下降法从头训练该网络，并在所有像素间共享权重（卷积权重共享）。使用在所有像素上平均的交叉熵损失：\n$$\n\\mathcal{L}=\\frac{1}{N}\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\ell\\!\\left(z^{(3)}_{i,j},y_{i,j}\\right),\n$$\n其中 $N=H\\cdot W$ 且 $y_{i,j}\\in\\{0,\\dots,K-1\\}$ 是真实标签。计算的任何地方都不得出现 $k1$ 的核。\n\n- 训练后评估分类准确率，其定义为\n$$\n\\operatorname{acc}=\\frac{1}{N}\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\mathbf{1}\\!\\left[\\arg\\max_{k}p_{i,j,k}=y_{i,j}\\right],\n$$\n其中 $\\mathbf{1}[\\cdot]$ 表示指示函数。\n\n实现一个单一程序，在以下合成测试套件上进行训练和评估。所有随机性必须由指定的种子值控制以保证可复现性。不允许使用外部数据或用户输入。不涉及任何物理单位。\n\n测试套件（每个案例指定 $H$、$W$、$C$、$K$、隐藏层宽度 $F_1$、$F_2$、优化步数、学习率 $\\eta$ 和种子；以及一个确定性数据生成规则）：\n\n- 案例 1（一般情况）：\n  - $H=4$, $W=4$, $C=3$, $K=2$, $F_1=5$, $F_2=3$, $\\text{steps}=800$, $\\eta=0.2$, $\\text{seed}=123$。\n  - 特征数据生成：对于像素坐标 $(i,j)$，其中 $i\\in\\{0,\\dots,H-1\\}$ 且 $j\\in\\{0,\\dots,W-1\\}$，\n    $x_{i,j,0}=\\frac{i+1}{H}$, $x_{i,j,1}=\\frac{j+1}{W}$, $x_{i,j,2}=\\frac{(i+1)+(j+1)}{H+W}$。\n    标签：定义一个分数 $s=0.7\\,x_{i,j,0}-0.5\\,x_{i,j,1}+0.4\\,x_{i,j,2}-0.3$；如果 $s0$ 则设置 $y_{i,j}=1$，否则 $y_{i,j}=0$。\n\n- 案例 2（多类别情况）：\n  - $H=3$, $W=3$, $C=2$, $K=3$, $F_1=4$, $F_2=3$, $\\text{steps}=1500$, $\\eta=0.2$, $\\text{seed}=456$。\n  - 特征数据生成：$x_{i,j,0}=\\frac{i+1}{H}$, $x_{i,j,1}=\\frac{j+1}{W}-0.5$。\n    真实线性分类器：$W^{\\star}\\in\\mathbb{R}^{3\\times 2}$ 的行分别为 $\\left[1.0,-1.2\\right]$、$\\left[-0.6,0.8\\right]$、$\\left[0.2,0.1\\right]$，偏置为 $b^{\\star}=\\left[0.0,0.1,-0.2\\right]$。标签：$y_{i,j}=\\arg\\max_{k}\\left((W^{\\star}x_{i,j}+b^{\\star})_k\\right)$。\n\n- 案例 3（单像素边界）：\n  - $H=1$, $W=1$, $C=4$, $K=2$, $F_1=3$, $F_2=2$, $\\text{steps}=500$, $\\eta=0.2$, $\\text{seed}=789$。\n  - 数据生成：$x_{0,0}=\\left[0.2,-0.4,0.6,-0.8\\right]$。通过阈值确定标签：令 $v=\\left[1.0,1.0,-0.5,-0.5\\right]$ 且 $b=-0.1$，如果 $v^{\\top}x_{0,0}+b0$，则设置 $y_{0,0}=1$，否则 $y_{0,0}=0$。\n\n- 案例 4（退化的单类别边缘情况）：\n  - $H=2$, $W=2$, $C=1$, $K=1$, $F_1=2$, $F_2=2$, $\\text{steps}=0$, $\\eta=0.1$, $\\text{seed}=42$。\n  - 数据生成：所有特征为零，$x_{i,j,0}=0$，且所有标签 $y_{i,j}=0$。\n\n您的程序必须按照上述架构和约束，对 $\\mathcal{L}$ 实现全批量梯度下降训练，评估每个案例的分类准确率，并生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表，每个准确率表示为四舍五入到四位小数的浮点数（例如，$\\left[\\text{acc}_1,\\text{acc}_2,\\text{acc}_3,\\text{acc}_4\\right]$ 打印为 $\\left[0.9375,1.0000,1.0000,1.0000\\right]$）。", "solution": "用户的问题陈述被确定为有效。它具有科学依据、问题良定、客观且自洽。该问题要求使用仅 $1\\times 1$ 卷积实现一种特定的神经网络架构，称为逐像素多层感知器 (MLP) 或 Network-in-Network (NiN) 块。训练将使用全批量梯度下降法进行。所有必需的数学定义、架构规范、训练参数和可复现的数据生成规则都已提供。该问题是计算机器学习中一个标准但详细的实现任务。\n\n解决方案首先实现指定的网络架构和相关的训练算法。其核心思想是，$1\\times 1$ 卷积独立地作用于每个像素的特征向量，但在所有空间位置上共享权重，这等同于将一个标准的全连接层应用于每个像素的特征向量。\n\n遵循常见的机器学习库惯例，我们将输入张量 $X \\in \\mathbb{R}^{H \\times W \\times C}$ 重塑为一个由“展平的”像素向量组成的矩阵 $X_{\\text{flat}} \\in \\mathbb{R}^{N \\times C}$，其中 $N=H \\cdot W$ 是像素总数。该矩阵的每一行对应一个像素的特征向量 $x_{i,j} \\in \\mathbb{R}^{C}$。\n\n三层逐像素 MLP 由以下前向传播方程定义：\n第一层计算激活前的值 $Z^{(1)} \\in \\mathbb{R}^{N \\times F_1}$ 和激活值 $A^{(1)} \\in \\mathbb{R}^{N \\times F_1}$：\n$$ Z^{(1)} = X_{\\text{flat}} W_1 + b_1 $$\n$$ A^{(1)} = \\operatorname{ReLU}(Z^{(1)}) $$\n此处，$W_1 \\in \\mathbb{R}^{C \\times F_1}$ 和 $b_1 \\in \\mathbb{R}^{F_1}$ 是第一层的权重和偏置，$\\operatorname{ReLU}$ 函数被逐元素地应用。\n\n第二层类似地计算激活前的值 $Z^{(2)} \\in \\mathbb{R}^{N \\times F_2}$ 和激活值 $A^{(2)} \\in \\mathbb{R}^{N \\times F_2}$：\n$$ Z^{(2)} = A^{(1)} W_2 + b_2 $$\n$$ A^{(2)} = \\operatorname{ReLU}(Z^{(2)}) $$\n其权重为 $W_2 \\in \\mathbb{R}^{F_1 \\times F_2}$，偏置为 $b_2 \\in \\mathbb{R}^{F_2}$。\n\n最终输出层为 $K$ 个类别生成 logits $Z^{(3)} \\in \\mathbb{R}^{N \\times K}$：\n$$ Z^{(3)} = A^{(2)} W_3 + b_3 $$\n其权重为 $W_3 \\in \\mathbb{R}^{F_2 \\times K}$，偏置为 $b_3 \\in \\mathbb{R}^{K}$。\n\n类别概率 $P \\in \\mathbb{R}^{N \\times K}$ 是通过对每个像素的 logits 应用 $\\operatorname{softmax}$ 函数得到的：\n$$ P_{i,k} = \\frac{\\exp((Z^{(3)})_{i,k})}{\\sum_{t=1}^{K}\\exp((Z^{(3)})_{i,t})} $$\n为保证数值稳定性，这被实现为 $\\operatorname{softmax}(z) = \\operatorname{softmax}(z - \\max(z))$。\n\n模型通过最小化所有 $N$ 个像素上的平均交叉熵损失 $\\mathcal{L}$ 来进行训练：\n$$ \\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}(Y_{\\text{oh}})_{i,k} \\log(P_{i,k}) $$\n其中 $Y_{\\text{oh}} \\in \\{0,1\\}^{N \\times K}$ 是真实标签的 one-hot 编码矩阵。\n\n训练使用全批量梯度下降法进行，这需要计算损失 $\\mathcal{L}$ 相对于所有模型参数（$W_1, b_1, W_2, b_2, W_3, b_3$）的梯度。这是通过反向传播算法实现的，该算法递归地应用链式法则。\n\n损失相对于输出 logits $Z^{(3)}$ 的梯度，记为误差信号 $\\delta^{(3)}$，是反向传播的起点：\n$$ \\delta^{(3)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(3)}} = \\frac{1}{N}(P - Y_{\\text{oh}}) $$\n然后这个误差会通过网络反向传播。\n\n第三层参数的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_3} = (A^{(2)})^T \\delta^{(3)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_3} = \\sum_{i=1}^{N} (\\delta^{(3)})_i $$\n\n误差传播到第二层的激活值，$\\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} = \\delta^{(3)} W_3^T$，然后通过 ReLU 非线性函数，其导数为 $\\operatorname{ReLU}'(z) = \\mathbf{1}[z0]$。第二层激活前值的误差信号是：\n$$ \\delta^{(2)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} = (\\delta^{(3)} W_3^T) \\odot \\operatorname{ReLU}'(Z^{(2)}) $$\n其中 $\\odot$ 表示逐元素乘法。\n\n第二层参数的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_2} = (A^{(1)})^T \\delta^{(2)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_2} = \\sum_{i=1}^{N} (\\delta^{(2)})_i $$\n\n类似地，第一层激活前值的误差信号是：\n$$ \\delta^{(1)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(1)}} = (\\delta^{(2)} W_2^T) \\odot \\operatorname{ReLU}'(Z^{(1)}) $$\n\n第一层参数的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_1} = (X_{\\text{flat}})^T \\delta^{(1)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_1} = \\sum_{i=1}^{N} (\\delta^{(1)})_i $$\n\n最后，在梯度下降算法的每一步中，使用学习率 $\\eta$ 更新参数：\n$$ W \\leftarrow W - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W}, \\quad b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b} $$\n对每一层都进行更新。在指定数量的训练步骤之后，通过比较每个像素的预测类别（最大 logit 的索引）和真实类别，在相同的数据上评估模型的准确率。\n\n整个过程在下面的程序中实现，并应用于问题陈述中定义的四个测试案例。", "answer": "```python\nimport numpy as np\n\ndef train_and_evaluate(X, Y, K, F1, F2, steps, eta, seed):\n    \"\"\"\n    Trains and evaluates the pixel-wise MLP model.\n\n    This function encapsulates the entire process:\n    1. Data preparation.\n    2. Weight initialization.\n    3. The training loop with forward and backward passes.\n    4. Final accuracy evaluation.\n    \"\"\"\n    H, W, C = X.shape\n    N = H * W\n    \n    rng = np.random.default_rng(seed)\n\n    # Reshape data for matrix operations\n    X_flat = X.reshape(N, C)\n    Y_flat = Y.flatten()\n    \n    if K > 1:\n        Y_oh = np.zeros((N, K))\n        Y_oh[np.arange(N), Y_flat] = 1\n    else: # K=1\n        Y_oh = np.ones((N, 1))\n\n    # Initialize weights and biases\n    W1 = rng.standard_normal((C, F1), dtype=np.float64) * 0.1\n    b1 = np.zeros(F1, dtype=np.float64)\n    W2 = rng.standard_normal((F1, F2), dtype=np.float64) * 0.1\n    b2 = np.zeros(F2, dtype=np.float64)\n    W3 = rng.standard_normal((F2, K), dtype=np.float64) * 0.1\n    b3 = np.zeros(K, dtype=np.float64)\n\n    # ReLU activation and its derivative\n    def relu(z):\n        return np.maximum(0, z)\n\n    def relu_derivative(z):\n        return (z > 0).astype(z.dtype)\n\n    # Softmax function (numerically stable)\n    def softmax(z):\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    # Training loop\n    for _ in range(steps):\n        # --- Forward Pass ---\n        Z1 = X_flat @ W1 + b1\n        A1 = relu(Z1)\n        Z2 = A1 @ W2 + b2\n        A2 = relu(Z2)\n        Z3 = A2 @ W3 + b3\n\n        if K > 1:\n            P = softmax(Z3)\n        else: # K=1, softmax is just 1\n            P = np.ones_like(Z3)\n        \n        # --- Backward Pass ---\n        # Gradient of loss w.r.t. Z3\n        delta3 = (P - Y_oh) / N\n\n        # Gradients for layer 3\n        grad_W3 = A2.T @ delta3\n        grad_b3 = np.sum(delta3, axis=0)\n\n        # Propagate error to A2\n        delta_A2 = delta3 @ W3.T\n        # Propagate through ReLU\n        delta2 = delta_A2 * relu_derivative(Z2)\n\n        # Gradients for layer 2\n        grad_W2 = A1.T @ delta2\n        grad_b2 = np.sum(delta2, axis=0)\n\n        # Propagate error to A1\n        delta_A1 = delta2 @ W2.T\n        # Propagate through ReLU\n        delta1 = delta_A1 * relu_derivative(Z1)\n\n        # Gradients for layer 1\n        grad_W1 = X_flat.T @ delta1\n        grad_b1 = np.sum(delta1, axis=0)\n\n        # --- Gradient Descent Update ---\n        W1 -= eta * grad_W1\n        b1 -= eta * grad_b1\n        W2 -= eta * grad_W2\n        b2 -= eta * grad_b2\n        W3 -= eta * grad_W3\n        b3 -= eta * grad_b3\n\n    # --- Final Evaluation ---\n    # Forward pass with trained weights\n    Z1 = X_flat @ W1 + b1\n    A1 = relu(Z1)\n    Z2 = A1 @ W2 + b2\n    A2 = relu(Z2)\n    Z3 = A2 @ W3 + b3\n    \n    # Predictions are the argmax of the logits\n    predictions = np.argmax(Z3, axis=1)\n    \n    # Calculate accuracy\n    accuracy = np.mean(predictions == Y_flat)\n    \n    return accuracy\n\n\ndef solve():\n    test_cases = [\n        {'id': 1, 'H': 4, 'W': 4, 'C': 3, 'K': 2, 'F1': 5, 'F2': 3, 'steps': 800, 'eta': 0.2, 'seed': 123},\n        {'id': 2, 'H': 3, 'W': 3, 'C': 2, 'K': 3, 'F1': 4, 'F2': 3, 'steps': 1500, 'eta': 0.2, 'seed': 456},\n        {'id': 3, 'H': 1, 'W': 1, 'C': 4, 'K': 2, 'F1': 3, 'F2': 2, 'steps': 500, 'eta': 0.2, 'seed': 789},\n        {'id': 4, 'H': 2, 'W': 2, 'C': 1, 'K': 1, 'F1': 2, 'F2': 2, 'steps': 0, 'eta': 0.1, 'seed': 42},\n    ]\n\n    results = []\n    for params in test_cases:\n        H, W, C = params['H'], params['W'], params['C']\n        X = np.zeros((H, W, C), dtype=np.float64)\n        Y = np.zeros((H, W), dtype=int)\n\n        if params['id'] == 1:\n            for i in range(H):\n                for j in range(W):\n                    x0 = (i + 1) / H\n                    x1 = (j + 1) / W\n                    x2 = ((i + 1) + (j + 1)) / (H + W)\n                    X[i, j, :] = [x0, x1, x2]\n                    s = 0.7 * x0 - 0.5 * x1 + 0.4 * x2 - 0.3\n                    Y[i, j] = 1 if s > 0 else 0\n        \n        elif params['id'] == 2:\n            W_star = np.array([[1.0, -1.2], [-0.6, 0.8], [0.2, 0.1]], dtype=np.float64)\n            b_star = np.array([0.0, 0.1, -0.2], dtype=np.float64)\n            for i in range(H):\n                for j in range(W):\n                    x0 = (i + 1) / H\n                    x1 = (j + 1) / W - 0.5\n                    x_ij = np.array([x0, x1], dtype=np.float64)\n                    X[i, j, :] = x_ij\n                    logits = W_star @ x_ij + b_star\n                    Y[i, j] = np.argmax(logits)\n\n        elif params['id'] == 3:\n            x_00 = np.array([0.2, -0.4, 0.6, -0.8], dtype=np.float64)\n            v = np.array([1.0, 1.0, -0.5, -0.5], dtype=np.float64)\n            b = -0.1\n            X[0, 0, :] = x_00\n            Y[0, 0] = 1 if v @ x_00 + b > 0 else 0\n            \n        elif params['id'] == 4:\n            # X and Y are already initialized to zeros.\n            pass\n\n        accuracy = train_and_evaluate(X, Y, params['K'], params['F1'], params['F2'], \n                                      params['steps'], params['eta'], params['seed'])\n        \n        results.append(f\"{accuracy:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3094438"}]}