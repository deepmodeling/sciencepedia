{"hands_on_practices": [{"introduction": "要真正掌握 EfficientNet 的精髓，理解其复合缩放（compound scaling）原理与计算成本之间的关系至关重要。本练习将引导您从基本定义出发，亲手实现一个简化的成本和准确率模型，用于模拟从 EfficientNet-B0 到 B3 的缩放过程。通过这个实践，您将不仅能够量化计算资源（如 MACs）如何随网络深度、宽度和分辨率的变化而增长，还能分析训练超参数如何影响最终的性能，从而深刻理解理论与实践之间的联系 [@problem_id:3119662]。", "problem": "您的任务是为一个高效卷积神经网络架构系列实现一个有原则的成本模型和缩放程序，重点关注 EfficientNet 的复合缩放思想以及带有压缩和激励（Squeeze-and-Excitation, SE）模块的移动端倒置瓶颈卷积（Mobile Inverted Bottleneck Convolution, MBConv）。您的程序必须完全自包含，并为预定义的测试套件产生可量化的输出。目标是评估从 EfficientNet-B0 到 EfficientNet-B3 的类似物在复合缩放下的准确率恢复情况，并量化由训练超参数引起的偏差。\n\n您必须使用的基础理论：\n\n- 对于一个核大小为 $k \\times k$、输入通道为 $C_{\\text{in}}$、输出通道为 $C_{\\text{out}}$、空间分辨率为 $H \\times W$ 的二维卷积，其乘加运算次数（MAC）为 $H W C_{\\text{in}} k^2 C_{\\text{out}}$；参数量为 $C_{\\text{in}} k^2 C_{\\text{out}}$。\n- 对于一个在 $C$ 个通道上、核大小为 $k \\times k$、空间分辨率为 $H \\times W$ 的深度卷积，其 MAC 为 $H W C k^2$；参数量为 $C k^2$。\n- 在分辨率为 $H \\times W$ 时，从 $C_{\\text{in}}$ 到 $C_{\\text{out}}$ 的逐点（$1 \\times 1$）卷积的 MAC 为 $H W C_{\\text{in}} C_{\\text{out}}$，参数量为 $C_{\\text{in}} C_{\\text{out}}$。\n- 具有扩展因子 $t$ 和核大小 $k$ 的移动端倒置瓶颈卷积（MBConv）块包括：一个从 $C_{\\text{in}}$ 到 $t C_{\\text{in}}$ 的扩展逐点卷积，一个在 $t C_{\\text{in}}$ 通道上、核大小为 $k \\times k$ 的深度卷积，一个将通道数按比例 $\\rho_{\\text{se}}$ 减少然后重新扩展的 SE 模块，以及一个投影回 $C_{\\text{out}}$ 的逐点卷积。\n- 压缩和激励（SE）模块使用两个全连接或 $1 \\times 1$ 卷积变换：一个从 $t C_{\\text{in}}$ 到 $t C_{\\text{in}} \\rho_{\\text{se}}$，另一个从 $t C_{\\text{in}} \\rho_{\\text{se}}$ 回到 $t C_{\\text{in}}$。其参数量和用于这些线性变换的 MAC 数为 $t C_{\\text{in}} \\cdot (t C_{\\text{in}} \\rho_{\\text{se}}) + (t C_{\\text{in}} \\rho_{\\text{se}}) \\cdot t C_{\\text{in}}$。在计算 MAC 时，全局平均池化和逐元素非线性操作被忽略。\n- 取整规则：任何通道数必须四舍五入到最接近的能被 $8$ 整除的整数。\n- 复合缩放原则：一个宽度为 $w$、深度为 $d$ 次重复、分辨率为 $s \\times s$ 的阶段，其总计算量大约与 $s^2 w^2 d$ 乘以来自块结构的常数的乘积成比例。对 $(d, w, s)$ 分别施加一个增量缩放系数 $\\phi$ 和固定的乘数 $(\\alpha, \\beta, \\gamma)$，计算量翻倍的约束要求 $\\phi$ 增加 $1$ 大约会使计算量翻倍，这产生一个形式为 $\\alpha \\beta^2 \\gamma^2 \\approx 2$ 的约束。\n\n您的程序必须：\n\n1. 实现一个带 SE 的 MBConv 成本模型：\n   - 输入：$C_{\\text{in}}$、$C_{\\text{out}}$、$t$、$k$、$s$、$\\rho_{\\text{se}}$。\n   - 输出：根据上述基础公式计算的每个块的 MAC 数和参数量。\n   - 对于一个有 $d$ 次重复的阶段，假设所有重复的 $C_{\\text{in}} = C_{\\text{out}} = w$，并将阶段 MAC 数计算为每个块 MAC 数的 $d$ 倍，同时对所有通道数应用可被 $8$ 整除的取整规则。\n\n2. 实现复合缩放：\n   - 输入：基线 $(w_0, d_0, s_0)$、缩放乘数 $(\\alpha, \\beta, \\gamma)$ 和缩放系数 $\\phi$。\n   - 根据所描述的计算量翻倍约束，使用给定的 $\\phi$ 和各自的乘数对每个维度进行指数缩放，计算出缩放后的 $(w, d, s)$。将 $w$ 和 $t w$ 四舍五入到能被 $8$ 整除，将 $d$ 四舍五入到最近的正整数，并将 $s$ 四舍五入到最近的整数。\n\n3. 定义一个基线阶段（EfficientNet-B0 类似物）并使用 MBConv 设置计算其阶段 MAC 数 $N_0$：\n   - 基线：$w_0 = 32$，$d_0 = 4$，$s_0 = 224$，$t = 6$，$k = 3$，$\\rho_{\\text{se}} = 0.25$，通道除数 $8$。\n\n4. 对于系数为 $\\phi$ 的缩放阶段，计算：\n   - 使用步骤 2 中缩放后的 $(w, d, s)$ 和步骤 3 中的 MBConv 设置计算缩放后的阶段 MAC 数 $N$（除非测试用例中另有规定）。\n   - 目标翻倍计算量 $N_{\\text{target}} = N_0 \\cdot 2^{\\phi}$。\n\n5. 根据广泛观察到的神经缩放行为定义一个代理准确率：\n   - 使用归一化计算量 $x = N / N_0$。\n   - 代理损失 $L(x) = c_0 x^{-p} + c_1$，其中 $c_0$、$p$ 和 $c_1$ 是正常数，代理准确率 $A(x) = 1 - L(x)$。\n   - 训练超参数引入一个有效利用率因子 $u = \\min(\\eta / \\eta^{\\ast}, 1) \\cdot \\min(B / B^{\\ast}, 1) \\cdot \\min(E / E^{\\ast}, 1)$，其中 $(\\eta, B, E)$ 分别是学习率、批量大小和训练轮数，而 $(\\eta^{\\ast}, B^{\\ast}, E^{\\ast})$ 是它们各自的最优值。\n   - 实现的准确率是 $A_{\\text{eff}} = A(x) \\cdot u$。在系数 $\\phi$ 下的理想目标准确率是 $A_{\\text{target}} = 1 - \\left(c_0 \\left(2^{\\phi}\\right)^{-p} + c_1\\right)$。\n   - 定义偏差 $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$。\n\n6. 使用以下固定的常数和缩放乘数：\n   - 缩放乘数：$\\alpha = 1.2$，$\\beta = 1.1$，$\\gamma = 1.15$。\n   - 代理准确率常数：$c_0 = 0.4$，$p = 0.2$，$c_1 = 0.1$。\n   - 最优超参数：$\\eta^{\\ast} = 0.2$，$B^{\\ast} = 128$，$E^{\\ast} = 350$。\n\n7. 测试套件：\n   - 用例 1：$\\phi = 0$，$t = 6$，$\\rho_{\\text{se}} = 0.25$，$B = 128$，$\\eta = 0.2$，$E = 350$。\n   - 用例 2：$\\phi = 1$，$t = 6$，$\\rho_{\\text{se}} = 0.25$，$B = 128$，$\\eta = 0.2$，$E = 350$。\n   - 用例 3：$\\phi = 2$，$t = 6$，$\\rho_{\\text{se}} = 0.25$，$B = 128$，$\\eta = 0.05$，$E = 350$。\n   - 用例 4：$\\phi = 3$，$t = 6$，$\\rho_{\\text{se}} = 0.25$，$B = 32$，$\\eta = 0.2$，$E = 200$。\n   - 用例 5：$\\phi = 1$，$t = 6$，$\\rho_{\\text{se}} = 0.5$，$B = 128$，$\\eta = 0.2$，$E = 350$。\n\n您的程序应生成一行输出，其中包含按上述顺序排列的各用例的偏差 $\\Delta$，格式为逗号分隔的列表并用方括号括起来，例如 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$。不应打印任何额外的文本。", "solution": "该问题要求实现一个计算模型，以分析卷积神经网络（CNN）阶段的复合缩放，其基础是为 EfficientNet 系列模型阐明的原则。该分析涉及计算一个阶段的计算成本（以乘加运算或 MACs 为单位），缩放其维度，并使用定义的代理准确率评估所得性能与理想目标的对比。此过程允许量化因架构取整和非最优训练超参数而导致的与理想缩放的偏差。\n\n解决方案建立在三个基本支柱之上：移动端倒置瓶颈卷积（MBConv）块的成本模型，网络维度的复合缩放定律，以及一个明确的准确率及其与理想目标偏差的模型。\n\n首先，我们建立阶段内单个 MBConv 块的成本模型。一个阶段由其宽度（通道数，$w$）、深度（块重复次数，$d$）和边长为 $s$ 的正方形空间分辨率定义。对于一个输入和输出通道均为 $w$、扩展因子为 $t$、深度卷积核大小为 $k \\times k$、且包含一个缩减率为 $\\rho_{\\text{se}}$ 的压缩和激励（SE）模块的块，其 MAC 计数是其构成操作的总和。问题规定了一个取整规则，即所有通道数必须四舍五入到最接近的能被给定除数（此题为8）整除的整数。设 $\\text{round}_m(x)$ 表示将 $x$ 四舍五入到 $m$ 的最接近倍数的函数。计算中使用的通道数是：\n- 扩展通道数：$C_{\\text{exp}} = \\text{round}_8(t \\cdot w)$\n- SE 缩减通道数：$C_{\\text{se}} = \\text{round}_8(C_{\\text{exp}} \\cdot \\rho_{\\text{se}})$\n\n一个 MBConv 块的总 MAC 数是每个组件 MAC 数的总和：\n1.  扩展（$1 \\times 1$ 卷积）：$s^2 \\cdot w \\cdot C_{\\text{exp}}$\n2.  深度卷积（$k \\times k$ 卷积）：$s^2 \\cdot C_{\\text{exp}} \\cdot k^2$\n3.  压缩和激励（对池化特征的两个线性层）：$2 \\cdot C_{\\text{exp}} \\cdot C_{\\text{se}}$\n4.  投影（$1 \\times 1$ 卷积）：$s^2 \\cdot C_{\\text{exp}} \\cdot w$\n\n单个块的总 MAC 数 $\\text{MACs}_{\\text{block}}$ 是这四项之和。该阶段的总 MAC 数 $N$ 则为 $d \\cdot \\text{MACs}_{\\text{block}}$。\n\n其次，我们实现复合缩放原则。给定一个具有深度 $d_0$、宽度 $w_0$ 和分辨率 $s_0$ 的基线架构，通过一个系数 $\\phi$ 以及分别用于深度、宽度和分辨率的特定乘数 $\\alpha$、$\\beta$ 和 $\\gamma$ 来缩放维度。缩放后的维度由下式给出：\n- $d(\\phi) = d_0 \\cdot \\alpha^{\\phi}$\n- $w(\\phi) = w_0 \\cdot \\beta^{\\phi}$\n- $s(\\phi) = s_0 \\cdot \\gamma^{\\phi}$\n\n然后根据问题的规则对这些原始缩放值进行取整：$d$ 四舍五入到最近的正整数，$w$ 四舍五入到最接近的能被 $8$ 整除的整数，$s$ 四舍五入到最近的整数。约束 $\\alpha \\beta^2 \\gamma^2 \\approx 2$ 确保将 $\\phi$ 增加 $1$ 大约会使总计算成本翻倍，该成本与 $d \\cdot w^2 \\cdot s^2$ 成正比。\n\n第三，我们定义准确率和偏差模型。模型的性能通过幂律关系与其计算预算相关。代理准确率 $A(x)$ 定义为 $A(x) = 1 - L(x)$，其中 $L(x) = c_0 x^{-p} + c_1$ 是代理损失。这里，$x = N/N_0$ 是缩放后模型的计算预算相对于基线模型预算 $N_0$ 的归一化值。常数 $c_0$、$c_1$ 和 $p$ 是给定的。对于缩放级别 $\\phi$ 的理想或目标准确率 $A_{\\text{target}}$，假设完美的计算缩放，即 $N = N_0 \\cdot 2^\\phi$。因此，$A_{\\text{target}}(\\phi) = 1 - (c_0 (2^\\phi)^{-p} + c_1)$。\n实现的准确率 $A_{\\text{eff}}$ 受训练超参数的影响。这通过一个利用率因子 $u$ 来建模，定义为 $u = \\min(\\eta / \\eta^{\\ast}, 1) \\cdot \\min(B / B^{\\ast}, 1) \\cdot \\min(E / E^{\\ast}, 1)$，其中 $(\\eta, B, E)$ 分别是学习率、批量大小和训练轮数，而 $(\\eta^{\\ast}, B^{\\ast}, E^{\\ast})$ 是它们的最优值。有效准确率则为 $A_{\\text{eff}} = A(x) \\cdot u$。最终我们关心的量是偏差 $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$，它衡量了模型实现的准确率与其理想目标之间的差异。\n\n每个测试用例的计算过程如下：\n1.  使用给定的基线参数为 $\\phi=0$ 计算基线 MAC 计数 $N_0$。\n2.  对于每个具有特定集合 $(\\phi, t, \\rho_{\\text{se}}, B, \\eta, E)$ 的测试用例：\n    a. 使用 $\\phi$ 和指定的取整规则计算缩放后的维度 $(w, d, s)$。\n    b. 使用这些缩放后的维度和给定的 MBConv 参数 $(t, k, \\rho_{\\text{se}})$ 计算阶段 MAC 数 $N$。\n    c. 计算归一化计算量 $x = N/N_0$。\n    d. 根据训练超参数计算利用率因子 $u$。\n    e. 计算有效准确率 $A_{\\text{eff}} = (1 - (c_0 x^{-p} + c_1)) \\cdot u$。\n    f. 计算目标准确率 $A_{\\text{target}} = 1 - (c_0 (2^\\phi)^{-p} + c_1)$。\n    g. 计算并记录偏差 $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$。\n\n以下程序实现了此逻辑，以计算所提供测试套件的偏差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a cost and accuracy model for a CNN stage based on EfficientNet principles,\n    calculates performance deviations for a set of test cases.\n    \"\"\"\n\n    # --- Constants and Baseline Definitions ---\n\n    # Scaling multipliers\n    ALPHA = 1.2\n    BETA = 1.1\n    GAMMA = 1.15\n\n    # Accuracy proxy constants\n    C0 = 0.4\n    P = 0.2\n    C1 = 0.1\n\n    # Optimal hyperparameters\n    ETA_STAR = 0.2\n    B_STAR = 128\n    E_STAR = 350\n\n    # Baseline architecture and MBConv settings\n    W0 = 32\n    D0 = 4\n    S0 = 224\n    K_BASE = 3\n    CHANNEL_DIVISOR = 8\n\n    # Test suite\n    test_cases = [\n        # (phi, t, rho_se, B, eta, E)\n        (0, 6, 0.25, 128, 0.2, 350),\n        (1, 6, 0.25, 128, 0.2, 350),\n        (2, 6, 0.25, 128, 0.05, 350),\n        (3, 6, 0.25, 32, 0.2, 200),\n        (1, 6, 0.5, 128, 0.2, 350),\n    ]\n\n    # --- Helper Functions ---\n\n    def round_divisible(n, divisor):\n        \"\"\"Rounds n to the nearest integer divisible by divisor.\"\"\"\n        if divisor == 0:\n            return int(n)\n        return int(np.round(n / divisor) * divisor)\n\n    def calculate_stage_macs(w, d, s, t, k, rho_se, divisor):\n        \"\"\"Calculates total MACs for an MBConv stage.\"\"\"\n        \n        # Apply rounding to all channel counts\n        c_in = w # w is assumed pre-rounded\n        c_expand = round_divisible(t * c_in, divisor)\n        c_se = round_divisible(c_expand * rho_se, divisor)\n        # Ensure c_se is at least divisor if c_expand is not zero to avoid div by zero in some real cases,\n        # but problem says \"round\" not \"round up\". For n  divisor/2, this can become 0.\n        if c_se == 0 and c_expand > 0:\n             # As per strict problem spec, a 0 is possible. In a real net this would be an issue.\n             # e.g., if t*w*rho_se  4. We will follow spec.\n             pass\n        c_out = w\n\n        s_squared = s * s\n        \n        # MACs for each part of a single MBConv block\n        macs_expansion = s_squared * c_in * c_expand\n        macs_depthwise = s_squared * c_expand * (k * k)\n        macs_se = 2 * c_expand * c_se\n        macs_projection = s_squared * c_expand * c_out\n        \n        macs_per_block = macs_expansion + macs_depthwise + macs_se + macs_projection\n        \n        total_macs = d * macs_per_block\n        return total_macs\n\n    def scale_dimensions(phi, w0, d0, s0, alpha, beta, gamma, divisor):\n        \"\"\"Scales network dimensions based on phi and rounds them.\"\"\"\n        d_scaled = d0 * (alpha ** phi)\n        w_scaled = w0 * (beta ** phi)\n        s_scaled = s0 * (gamma ** phi)\n        \n        # Rounding as per problem specification\n        d = max(1, int(np.round(d_scaled))) # nearest positive integer\n        w = round_divisible(w_scaled, divisor)\n        s = int(np.round(s_scaled))\n        \n        return w, d, s\n\n    # --- Main Calculation Logic ---\n\n    results = []\n\n    # 1. Calculate baseline MACs (N0)\n    w_base, d_base, s_base = scale_dimensions(0, W0, D0, S0, ALPHA, BETA, GAMMA, CHANNEL_DIVISOR)\n    base_case = test_cases[0]\n    n0 = calculate_stage_macs(\n        w=w_base, \n        d=d_base, \n        s=s_base, \n        t=base_case[1], \n        k=K_BASE, \n        rho_se=base_case[2], \n        divisor=CHANNEL_DIVISOR\n    )\n\n    if n0 == 0:\n        raise ValueError(\"Baseline MAC count (N0) is zero, preventing normalization.\")\n\n    # 2. Iterate through test cases\n    for case in test_cases:\n        phi, t, rho_se, b, eta, e = case\n        \n        # a. Compute scaled dimensions\n        w, d, s = scale_dimensions(phi, W0, D0, S0, ALPHA, BETA, GAMMA, CHANNEL_DIVISOR)\n        \n        # b. Calculate actual stage MACs (N)\n        n = calculate_stage_macs(w, d, s, t, K_BASE, rho_se, CHANNEL_DIVISOR)\n        \n        # c. Compute normalized compute (x)\n        x = n / n0\n        \n        # d. Calculate utilization factor (u)\n        u = min(eta / ETA_STAR, 1) * min(b / B_STAR, 1) * min(e / E_STAR, 1)\n        \n        # e. Compute effective accuracy (A_eff)\n        loss_x = C0 * (x ** -P) + C1\n        a_x = 1 - loss_x\n        a_eff = a_x * u\n        \n        # f. Compute target accuracy (A_target)\n        loss_target = C0 * ((2 ** phi) ** -P) + C1\n        a_target = 1 - loss_target\n        \n        # g. Calculate deviation (Delta)\n        delta = a_eff - a_target\n        results.append(delta)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3119662"}, {"introduction": "EfficientNet 的缩放系数 $\\alpha$、$\\beta$ 和 $\\gamma$ 并非随意选择，而是为了在特定计算预算下最大化模型性能的优化结果。这个练习将带您体验这一“神经架构搜索”（Neural Architecture Search, NAS）的核心思想，通过构建一个多目标优化问题，在模型准确率、推理延迟（latency）和内存占用（memory）之间进行权衡。您将学习如何使用标量化方法，为特定的硬件（由惩罚权重 $\\lambda_1$ 和 $\\lambda_2$ 体现）找到最优的复合缩放系数，这是将理论模型适配到现实世界应用的关键一步 [@problem_id:3119675]。", "problem": "您的任务是实现一个自包含程序，为 EfficientNet 风格的卷积神经网络 (CNN) 计算最优复合缩放系数。该优化必须在现实的硬件预算约束下，平衡模型精度与延迟和内存成本。您的程序必须通过标量化来解决一个多目标优化问题。\n\n从以下在卷积架构中基于经验且广泛使用的缩放关系开始：\n\n1. 对于 CNN 中的一个卷积层堆栈，所需的浮点计算量与深度、宽度的平方和输入分辨率的平方的乘积成比例缩放。设深度、宽度和分辨率的缩放乘数分别用 $\\alpha$、$\\beta$ 和 $\\gamma$ 表示。那么，一个理想化的延迟度量 $L$（以无量纲的归一化单位表示）可建模为\n$$\nL(\\alpha,\\beta,\\gamma) = k_L \\, \\alpha \\, \\beta^2 \\, \\gamma^2,\n$$\n其中 $k_L$ 是一个正常数，捕获了基线计算到延迟的比例性。\n\n2. 总内存占用 $M$（以无量纲的归一化单位表示），结合了参数内存和激活内存，可建模为\n$$\nM(\\alpha,\\beta,\\gamma) = k_P \\, \\alpha \\, \\beta^2 \\;+\\; k_A \\, \\alpha \\, \\beta \\, \\gamma^2,\n$$\n其中 $k_P$ 代表参数内存的比例系数，而 $k_A$ 代表激活内存的比例系数。这些形式源于参数大致与深度和宽度的平方的乘积成比例缩放，而激活则与深度、宽度和分辨率的平方成比例缩放。\n\n3. 精度 $A$（以一个无量纲的抽象精度得分单位表示）随着规模的增加表现出收益递减的特性。一个与经验缩放定律一致的凹代理函数是\n$$\nA(\\alpha,\\beta,\\gamma) = s_d \\, \\ln(\\alpha) \\;+\\; s_w \\, \\ln(\\beta) \\;+\\; s_r \\, \\ln(\\gamma),\n$$\n其中 $s_d$、$s_w$ 和 $s_r$ 是正系数，编码了精度对深度、宽度和分辨率的相对敏感性。\n\n多目标优化被标量化为\n$$\nJ(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2) \\;=\\; A(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_1 \\, L(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_2 \\, M(\\alpha,\\beta,\\gamma),\n$$\n其中 $\\lambda_1$ 和 $\\lambda_2$ 是非负的硬件特定权重，分别以无量纲的归一化单位惩罚延迟和内存。\n\n优化约束：\n- 复合计算预算：\n$$\n\\alpha \\, \\beta^2 \\, \\gamma^2 \\;\\le\\; B,\n$$\n其中 $B \\ge 1$ 是一个无量纲的预算。这个不等式反映了如果惩罚很高，可用计算资源可能未被充分利用；但它绝不能被超过。\n- 下界：\n$$\n\\alpha \\ge 1, \\quad \\beta \\ge 1, \\quad \\gamma \\ge 1.\n$$\n这些下界强制执行非收缩缩放（不低于基线进行缩放）。\n- 上界：\n$$\n\\alpha \\le U, \\quad \\beta \\le U, \\quad \\gamma \\le U,\n$$\n其中 $U$ 是一个固定的上限，以确保适定性并反映实际的缩放上限。\n\n所有计算中要使用的常量：\n- $k_L = 1.0$, $k_P = 1.0$, $k_A = 0.5$,\n- $s_d = 0.20$, $s_w = 0.30$, $s_r = 0.50$,\n- $U = 5.0$.\n\n您的程序必须在这些约束条件下数值最大化 $J(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2)$。您可以使用任何正确的数值方法来解决此问题；一个用于可微目标函数的约束优化器是可接受的。所有量 $L$、$M$、$A$、$J$、$\\alpha$、$\\beta$、$\\gamma$、$\\lambda_1$、$\\lambda_2$、$B$、$k_L$、$k_P$、$k_A$、$s_d$、$s_w$、$s_r$ 和 $U$ 都是无量纲的，并且必须如此处理。\n\n测试套件：\n在以下特定于硬件的惩罚权重和预算 $(\\lambda_1,\\lambda_2,B)$ 上运行您的求解器：\n- 案例 1（均衡，理想路径）：$(0.02,\\, 0.01,\\, 2.0)$\n- 案例 2（内存主导惩罚）：$(0.0,\\, 0.03,\\, 3.0)$\n- 案例 3（延迟主导惩罚）：$(0.05,\\, 0.0,\\, 3.0)$\n- 案例 4（极高惩罚，可能导致最小化缩放）：$(0.20,\\, 0.10,\\, 2.0)$\n- 案例 5（边界预算）：$(0.01,\\, 0.005,\\, 1.0)$\n\n最终输出要求：\n- 对于每个测试案例，计算最优三元组 $(\\alpha^\\star,\\beta^\\star,\\gamma^\\star)$。\n- 将三元组中的每个标量四舍五入到 $4$ 位小数。\n- 您的程序应生成单行输出，其中包含所有案例的结果，格式为一个用方括号括起来的逗号分隔列表，其中每个元素是四舍五入后的 Python 风格的三元组列表。例如，输出应如下所示：\n$$\n[\\,[\\alpha_1,\\beta_1,\\gamma_1],\\,[\\alpha_2,\\beta_2,\\gamma_2],\\,\\dots\\,]\n$$\n其中每个 $\\alpha_i$、$\\beta_i$ 和 $\\gamma_i$ 都四舍五入到 $4$ 位小数。不应打印任何额外文本。", "solution": "用户提供了一个明确定义的多目标优化问题，我将首先对其进行验证，然后求解。\n\n### 步骤 1：提取已知条件\n\n**模型：**\n- 延迟：$L(\\alpha,\\beta,\\gamma) = k_L \\, \\alpha \\, \\beta^2 \\, \\gamma^2$\n- 内存：$M(\\alpha,\\beta,\\gamma) = k_P \\, \\alpha \\, \\beta^2 \\;+\\; k_A \\, \\alpha \\, \\beta \\, \\gamma^2$\n- 精度：$A(\\alpha,\\beta,\\gamma) = s_d \\, \\ln(\\alpha) \\;+\\; s_w \\, \\ln(\\beta) \\;+\\; s_r \\, \\ln(\\gamma)$\n\n**目标函数：**\n- 要最大化的标量化目标：$J(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2) \\;=\\; A(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_1 \\, L(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_2 \\, M(\\alpha,\\beta,\\gamma)$\n\n**约束：**\n- 计算预算：$\\alpha \\, \\beta^2 \\, \\gamma^2 \\;\\le\\; B$，其中 $B \\ge 1$\n- 下界：$\\alpha \\ge 1, \\quad \\beta \\ge 1, \\quad \\gamma \\ge 1$\n- 上界：$\\alpha \\le U, \\quad \\beta \\le U, \\quad \\gamma \\le U$\n\n**常量：**\n- $k_L = 1.0$\n- $k_P = 1.0$\n- $k_A = 0.5$\n- $s_d = 0.20$\n- $s_w = 0.30$\n- $s_r = 0.50$\n- $U = 5.0$\n\n**测试套件：**\n- 案例 1：$(\\lambda_1, \\lambda_2, B) = (0.02, 0.01, 2.0)$\n- 案例 2：$(\\lambda_1, \\lambda_2, B) = (0.0, 0.03, 3.0)$\n- 案例 3：$(\\lambda_1, \\lambda_2, B) = (0.05, 0.0, 3.0)$\n- 案例 4：$(\\lambda_1, \\lambda_2, B) = (0.20, 0.10, 2.0)$\n- 案例 5：$(\\lambda_1, \\lambda_2, B) = (0.01, 0.005, 1.0)$\n\n### 步骤 2：使用提取的已知条件进行验证\n\n1.  **科学依据：** 该问题基于神经网络架构设计的原理，特别是与 EfficientNet 推广的复合缩放有关。延迟（$L$）、内存（$M$）和精度（$A$）的函数形式是简化但合理的模型。计算量随深度（$\\alpha$）、宽度的平方（$\\beta^2$）和分辨率的平方（$\\gamma^2$）的缩放是标准的。内存模型正确区分了参数（随 $\\alpha \\beta^2$ 缩放）和激活（随 $\\alpha \\beta \\gamma^2$ 缩放）。对数精度模型反映了收益递减原则。该公式是深度学习领域一个真实世界工程问题的有效抽象。\n\n2.  **适定性：** 该问题是在 $\\mathbb{R}^3$ 的一个紧（闭合有界）子集上最大化一个连续可微函数 $J$。可行域由不等式 $1 \\le \\alpha \\le U$、$1 \\le \\beta \\le U$、$1 \\le \\gamma \\le U$ 和 $\\alpha \\beta^2 \\gamma^2 \\le B$ 定义。根据极值定理，紧集上的连续函数必能取得最大值。因此，解必然存在。该问题是适定的。\n\n3.  **客观性：** 该问题使用精确的数学定义和客观、正式的语言进行陈述。所有常量和变量都已明确定义。没有歧义。\n\n### 步骤 3：结论与行动\n\n该问题是有效的。它是一个适定的、有科学依据的、客观的约束非线性优化问题。我将继续进行求解。\n\n### 求解推导\n\n任务是找到 $(\\alpha, \\beta, \\gamma)$ 的值，以在给定约束条件下最大化目标函数 $J$。这是一个约束非线性规划问题。由于数值优化库通常设计用于寻找最小值，我们将等价地最小化目标函数的负值 $-J$。\n\n设变量向量为 $\\mathbf{x} = [\\alpha, \\beta, \\gamma]$。要最小化的目标函数是：\n$$\n-J(\\mathbf{x}; \\lambda_1, \\lambda_2) = -A(\\mathbf{x}) + \\lambda_1 L(\\mathbf{x}) + \\lambda_2 M(\\mathbf{x})\n$$\n代入给定的表达式和常量：\n$$\n-J(\\alpha, \\beta, \\gamma) = -[s_d \\ln(\\alpha) + s_w \\ln(\\beta) + s_r \\ln(\\gamma)] + \\lambda_1 [k_L \\alpha \\beta^2 \\gamma^2] + \\lambda_2 [k_P \\alpha \\beta^2 + k_A \\alpha \\beta \\gamma^2]\n$$\n按变量 $\\alpha, \\beta, \\gamma$ 对各项进行分组：\n$$\n-J(\\alpha, \\beta, \\gamma) = -s_d \\ln(\\alpha) - s_w \\ln(\\beta) - s_r \\ln(\\gamma) + (\\lambda_1 k_L + \\lambda_2 k_A) \\alpha \\beta^2 \\gamma^2 + \\lambda_2 k_P \\alpha \\beta^2\n$$\n代入已知的常量值 ($k_L=1.0, k_P=1.0, k_A=0.5, s_d=0.2, s_w=0.3, s_r=0.5$)：\n$$\n-J(\\alpha, \\beta, \\gamma) = -0.2 \\ln(\\alpha) - 0.3 \\ln(\\beta) - 0.5 \\ln(\\gamma) + (\\lambda_1 + 0.5 \\lambda_2) \\alpha \\beta^2 \\gamma^2 + \\lambda_2 \\alpha \\beta^2\n$$\n这就是我们将提供给数值优化器的函数。\n\n约束条件是：\n1.  **不等式约束：** $\\alpha \\beta^2 \\gamma^2 \\le B$。对于要求 $g(\\mathbf{x}) \\ge 0$ 形式约束的标准求解器，这可以写成 $B - \\alpha \\beta^2 \\gamma^2 \\ge 0$。\n2.  **边界约束：**\n    $1 \\le \\alpha \\le U$\n    $1 \\le \\beta \\le U$\n    $1 \\le \\gamma \\le U$\n    其中 $U=5.0$。\n\n我们将使用 `scipy.optimize` 库中的序列最小二乘规划（`SLSQP`）方法，因为它非常适合处理同时带有边界约束和不等式约束的非线性优化问题。\n\n我们将遍历每个测试案例 $(\\lambda_1, \\lambda_2, B)$，并相应地定义目标函数和约束。一个稳健的优化器初始猜测是基线模型 $(\\alpha, \\beta, \\gamma) = (1, 1, 1)$，它保证对于任何 $B \\ge 1$ 都在可行域内。\n\n对于 $B=1.0$ 的特殊情况（测试案例 5），约束 $\\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1$ 和 $\\alpha \\beta^2 \\gamma^2 \\le 1$ 共同迫使唯一的可能解为 $\\alpha=1, \\beta=1, \\gamma=1$。优化器应收敛到这一点。\n\n每个测试案例的最终结果将是最优三元组 $(\\alpha^\\star, \\beta^\\star, \\gamma^\\star)$，其中每个分量都四舍五入到 4 位小数，并按指定格式输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes optimal compound scaling coefficients for a CNN model by solving\n    a constrained multi-objective optimization problem.\n    \"\"\"\n    # Define constants as specified in the problem statement.\n    # All quantities are dimensionless.\n    k_L = 1.0  # Latency proportionality\n    k_P = 1.0  # Parameter memory proportionality\n    k_A = 0.5  # Activation memory proportionality\n    s_d = 0.20 # Accuracy sensitivity to depth\n    s_w = 0.30 # Accuracy sensitivity to width\n    s_r = 0.50 # Accuracy sensitivity to resolution\n    U = 5.0    # Upper bound for scaling factors\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (lambda1, lambda2, B)\n    test_cases = [\n        (0.02, 0.01, 2.0),   # Case 1: balanced, happy path\n        (0.0, 0.03, 3.0),    # Case 2: memory-dominated penalty\n        (0.05, 0.0, 3.0),    # Case 3: latency-dominated penalty\n        (0.20, 0.10, 2.0),   # Case 4: very high penalties\n        (0.01, 0.005, 1.0),  # Case 5: boundary budget\n    ]\n\n    results = []\n\n    # Define the objective function to be minimized (-J)\n    def objective_function(x, lambda1, lambda2):\n        \"\"\"\n        Calculates the negative of the scalarized objective function J.\n        We minimize -J to maximize J.\n        \n        x: numpy array [alpha, beta, gamma]\n        lambda1: latency penalty weight\n        lambda2: memory penalty weight\n        \"\"\"\n        alpha, beta, gamma = x[0], x[1], x[2]\n        \n        # Guard against log(x) for x = 0, though bounds should prevent this.\n        if alpha = 0 or beta = 0 or gamma = 0:\n            return np.inf\n\n        accuracy_term = s_d * np.log(alpha) + s_w * np.log(beta) + s_r * np.log(gamma)\n        \n        latency_term = k_L * alpha * beta**2 * gamma**2\n        \n        memory_term = k_P * alpha * beta**2 + k_A * alpha * beta * gamma**2\n        \n        penalty = lambda1 * latency_term + lambda2 * memory_term\n        \n        # Return negative J for minimization\n        return -(accuracy_term - penalty)\n\n    for case in test_cases:\n        lambda1, lambda2, B = case\n\n        # Define the inequality constraint: alpha * beta^2 * gamma^2 = B\n        # Scipy's SLSQP expects constraints in the form g(x) >= 0\n        constraint = {\n            'type': 'ineq',\n            'fun': lambda x: B - x[0] * x[1]**2 * x[2]**2\n        }\n\n        # Define the bounds for alpha, beta, gamma: 1 = x_i = U\n        bounds = ((1.0, U), (1.0, U), (1.0, U))\n\n        # Initial guess: the baseline model, which is always feasible.\n        x0 = np.array([1.0, 1.0, 1.0])\n\n        # Perform the constrained optimization\n        opt_result = minimize(\n            fun=objective_function,\n            x0=x0,\n            args=(lambda1, lambda2),\n            method='SLSQP',\n            bounds=bounds,\n            constraints=[constraint],\n            tol=1e-9\n        )\n\n        # Extract the optimal scaling factors\n        optimal_coeffs = opt_result.x\n\n        # Round the results to 4 decimal places\n        rounded_coeffs = np.round(optimal_coeffs, 4).tolist()\n        results.append(rounded_coeffs)\n    \n    # Format the final output string exactly as required\n    # e.g., [[1.0, 1.0, 1.0],[...]] without extra spaces in the numbers.\n    # The default str() representation of a list is what's needed.\n    # Example: str([1.23, 4.56]) -> '[1.23, 4.56]'\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "3119675"}, {"introduction": "标准的复合缩放假设输入图像是各向同性的（即正方形），但现实世界的数据往往具有不同的宽高比。本练习旨在挑战您对缩放定律的理解，将其推广到各向异性分辨率的场景。您需要从第一性原理推导出计算成本的缩放法则，并解决一个约束优化问题，以确定在保持总计算量不变的情况下，应如何最小化地调整网络宽度和深度。这个练习能极大地加深您对模型计算成本背后基本原理的认识 [@problem_id:3119513]。", "problem": "要求您形式化并计算各向异性输入分辨率对遵循 EfficientNet 所推广的复合缩放原则的卷积神经网络计算预算的影响。目标是从基本定义出发，推导卷积中的乘加运算次数，并确定当输入分辨率进行各向异性缩放时，如何以最小的幅度调整宽度和深度乘数。\n\n假设和基本定义：\n- 对于单个二维卷积，若其卷积核大小为 $k_x \\times k_y$，输入通道数为 $c_{\\mathrm{in}}$，输出通道数为 $c_{\\mathrm{out}}$，输出空间尺寸为 $h \\times w$，则其所需的乘加运算次数与 $k_x k_y \\, c_{\\mathrm{in}} \\, c_{\\mathrm{out}} \\, h \\, w$ 成正比。\n- 考虑由这类卷积堆叠而成的网络。在宽度乘数 $w$ 的作用下，所有层的通道维度均与 $w$ 成比例缩放；在深度乘数 $d$ 的作用下，总层数与 $d$ 成比例缩放。\n- 网络的空间输入分辨率为 $(r_x, r_y)$，并且卷积层内部的空间分辨率与 $(r_x, r_y)$ 成比例缩放，其中的比例常数不依赖于 $w$ 或 $d$。\n\n基于这些基本事实，推导网络总乘加运算次数作为宽度乘数 $w$、深度乘数 $d$ 和输入空间维度 $(r_x, r_y)$ 的函数的缩放定律。请勿引入任何无法从所述基本事实直接推导出的公式。\n\n定义基准分辨率 $r_0 = 224$，并对标称（各向同性）系列采用以下复合缩放参数化：对于复合指数 $\\phi \\ge 0$ 和系数 $(\\alpha,\\beta,\\gamma)$，\n- 宽度乘数 $w(\\phi) = \\beta^{\\phi}$，\n- 深度乘数 $d(\\phi) = \\alpha^{\\phi}$，\n- 目标各向同性分辨率 $r(\\phi) = r_0 \\, \\gamma^{\\phi}$。\n\n使用具体系数 $(\\alpha,\\beta,\\gamma) = (1.2, 1.1, 1.15)$。\n\n任务：\n- 对于给定的 $\\phi$ 和一个各向异性输入分辨率 $(R_x, R_y)$，确定调整后的乘数 $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$，使得总乘加运算次数与相同 $\\phi$ 下的标称各向同性计算量相匹配。\n- 在所有满足计算量相等约束的 $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$ 中，选择使与标称乘数的对数偏差平方和最小的一对：\n  在计算量相等的约束下，最小化 $(\\log w_{\\mathrm{adj}} - \\log w(\\phi))^2 + (\\log d_{\\mathrm{adj}} - \\log d(\\phi))^2$。\n- 您的程序應为每个测试用例输出相对调整因子：\n  $$\n  \\frac{w_{\\mathrm{adj}}}{w(\\phi)} \\quad \\text{和} \\quad \\frac{d_{\\mathrm{adj}}}{d(\\phi)} \\, .\n  $$\n\n测试套件：\n使用 $r_0 = 224$，$(\\alpha,\\beta,\\gamma) = (1.2, 1.1, 1.15)$，以及以下五个案例。在每个案例中，程序必须计算出 $\\left(\\frac{w_{\\mathrm{adj}}}{w(\\phi)}, \\frac{d_{\\mathrm{adj}}}{d(\\phi)}\\right)$ 对。\n\n- 案例 1：$\\phi = 1$，$R_x = 224 \\cdot 1.15$，$R_y = 224 \\cdot 1.15$。\n- 案例 2：$\\phi = 2$，$R_x = 224 \\cdot 1.15^{2} \\cdot 3$，$R_y = 224 \\cdot 1.15^{2} / 3$。\n- 案例 3：$\\phi = 1$，$R_x = 224 \\cdot 1.15$，$R_y = 224 \\cdot 1.15 / 2$。\n- 案例 4：$\\phi = 1$，$R_x = 224 \\cdot 1.15$，$R_y = 224 \\cdot 1.15 \\cdot 2$。\n- 案例 5：$\\phi = 0$，$R_x = 336$，$R_y = 112$。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个以逗号分隔的列表的列表形式的结果，其中每个内部列表按给定顺序对应一个测试案例，并包含两个浮点数 $\\left[\\frac{w_{\\mathrm{adj}}}{w(\\phi)}, \\frac{d_{\\mathrm{adj}}}{d(\\phi)}\\right]$，四舍五入到 6 位小数。\n- 例如，格式必须与此完全一样：\n  $$\n  [[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5]]\n  $$\n  其中每个 $a_i$ 和 $b_i$ 是一个最多包含六位小数的十进制数。", "solution": "用户提供了一个提法明确且有科学依据的问题，要求推导神经网络计算成本的缩放定律，并解决一个约束优化问题。该问题经验证是合理且自洽的。\n\n### 1. 计算缩放定律的推导\n乘加运算（MACs）的总数是计算成本的一种度量（通常称为 FLOPs），需要作为网络缩放参数的函数来确定。\n\n单个二维卷积的 MACs 与卷积核维度 ($k_x$, $k_y$)、输入通道数 ($c_{\\mathrm{in}}$)、输出通道数 ($c_{\\mathrm{out}}$) 以及输出空间维度 ($h$, $w$) 的乘积成正比。\n$$\n\\text{MACs}_{\\text{layer}} \\propto k_x k_y c_{\\mathrm{in}} c_{\\mathrm{out}} h w\n$$\n问题陈述了网络架构如何随宽度乘数 $w$、深度乘数 $d$ 和输入分辨率 $(r_x, r_y)$ 进行缩放。\n\n- **宽度缩放**：所有通道维度（$c_{\\mathrm{in}}$, $c_{\\mathrm{out}}$）均与宽度乘数 $w$ 成比例缩放。因此，乘积 $c_{\\mathrm{in}} c_{\\mathrm{out}}$ 与 $w^2$ 成比例。\n- **分辨率缩放**：特征图的空间维度 ($h, w$) 与输入分辨率 $(r_x, r_y)$ 成比例缩放。这意味着 $h \\propto r_y$ 且 $w \\propto r_x$。因此，乘积 $h w$ 与 $r_x r_y$ 成比例。\n- **深度缩放**：网络中的总层数与深度乘数 $d$ 成比例缩放。\n\n综合这些因素，缩放后网络中任何单层的 MACs 与 $w^2 r_x r_y$ 成正比。整个网络的总 MACs 是所有层的 MACs 之和，它与每层的 MACs 乘以层数成正比。\n$$\n\\text{Total MACs} \\propto (\\text{number of layers}) \\times (\\text{MACs per layer})\n$$\n因此，我们记为函数 $F(w, d, r_x, r_y)$ 的总 MACs 按如下方式缩放：\n$$\nF(w, d, r_x, r_y) \\propto d \\cdot w^2 \\cdot r_x r_y\n$$\n我们可以用一个比例常数 $C$ 将其表示为等式，该常数 $C$ 取决于基础网络的具体架构，但与缩放因子无关：\n$$\nF(w, d, r_x, r_y) = C \\cdot d \\cdot w^2 \\cdot r_x r_y\n$$\n这个推导出的公式就是网络计算成本的缩放定律。\n\n### 2. 优化问题的公式化\n给定一个由复合指数 $\\phi \\ge 0$ 和系数 $(\\alpha, \\beta, \\gamma) = (1.2, 1.1, 1.15)$ 定义的标称（各向同性）缩放配置：\n- 宽度乘数：$w(\\phi) = \\beta^\\phi = 1.1^\\phi$\n- 深度乘数：$d(\\phi) = \\alpha^\\phi = 1.2^\\phi$\n- 各向同性分辨率：$r(\\phi) = r_0 \\gamma^\\phi = 224 \\cdot 1.15^\\phi$\n\n此标称配置的计算成本为：\n$$\nF_{\\mathrm{nom}} = F(w(\\phi), d(\\phi), r(\\phi), r(\\phi)) = C \\cdot d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2\n$$\n对于给定的 $\\phi$ 和一个各向异性输入分辨率 $(R_x, R_y)$，我们需要找到调整后的乘数 $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$，使得计算预算保持不变。这给了我们约束条件：\n$$\nF(w_{\\mathrm{adj}}, d_{\\mathrm{adj}}, R_x, R_y) = F_{\\mathrm{nom}}\n$$\n$$\nC \\cdot d_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2 \\cdot R_x R_y = C \\cdot d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2\n$$\n常数 $C$被消去，得到约束方程：\n$$\nd_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2 = \\frac{d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2}{R_x R_y}\n$$\n目标是找到满足此约束条件，同时最小化与标称乘数的对数偏差平方和的 $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$ 对：\n$$\n\\text{minimize } J(w_{\\mathrm{adj}}, d_{\\mathrm{adj}}) = (\\log w_{\\mathrm{adj}} - \\log w(\\phi))^2 + (\\log d_{\\mathrm{adj}} - \\log d(\\phi))^2\n$$\n\n### 3. 求解优化问题\n为简化问题，我们在乘数的对数空间中进行操作。令：\n- $W = \\log w_{\\mathrm{adj}}$, $D = \\log d_{\\mathrm{adj}}$\n- $W_0 = \\log w(\\phi)$, $D_0 = \\log d(\\phi)$\n\n目标函数变为最小化 $(W, D)$ 与 $(W_0, D_0)$ 之间的平方欧氏距离：\n$$\n\\text{minimize } (W - W_0)^2 + (D - D_0)^2\n$$\n对约束方程两边取对数，得到：\n$$\n\\log(d_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2) = \\log\\left(\\frac{d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2}{R_x R_y}\\right)\n$$\n$$\nD + 2W = D_0 + 2W_0 + \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n这是一个关于 $W$ 和 $D$ 的线性约束。问题转化为寻找这条直线上距离点 $(W_0, D_0)$ 最近的点。我们使用拉格朗日乘数法。设拉格朗日函数为 $\\mathcal{L}(W, D, \\lambda)$：\n$$\n\\mathcal{L} = (W - W_0)^2 + (D - D_0)^2 - \\lambda \\left( D + 2W - C' \\right)\n$$\n其中 $C'$ 是对数约束方程右侧的常数。求偏导数并令其为零：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = 2(W - W_0) - 2\\lambda = 0 \\implies W - W_0 = \\lambda\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial D} = 2(D - D_0) - \\lambda = 0 \\implies D - D_0 = \\frac{\\lambda}{2}\n$$\n将 $W = W_0 + \\lambda$ 和 $D = D_0 + \\lambda/2$ 代入约束条件 $D + 2W = C'$ 中：\n$$\n(D_0 + \\lambda/2) + 2(W_0 + \\lambda) = C' \\implies D_0 + 2W_0 + \\frac{5}{2}\\lambda = C'\n$$\n解出 $\\lambda$：\n$$\n\\lambda = \\frac{2}{5} (C' - (D_0 + 2W_0)) = \\frac{2}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n现在我们求出对数空间中的最优调整量：\n$$\n\\log w_{\\mathrm{adj}} = W = W_0 + \\lambda = \\log w(\\phi) + \\frac{2}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n$$\n\\log d_{\\mathrm{adj}} = D = D_0 + \\frac{\\lambda}{2} = \\log d(\\phi) + \\frac{1}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n对两边取指数以求得 $w_{\\mathrm{adj}}$ 和 $d_{\\mathrm{adj}}$：\n$$\nw_{\\mathrm{adj}} = w(\\phi) \\cdot \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{2/5}\n$$\n$$\nd_{\\mathrm{adj}} = d(\\phi) \\cdot \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{1/5}\n$$\n问题要求的是相对调整因子，即：\n$$\n\\frac{w_{\\mathrm{adj}}}{w(\\phi)} = \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{2/5}\n$$\n$$\n\\frac{d_{\\mathrm{adj}}}{d(\\phi)} = \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{1/5}\n$$\n这些公式提供了需要实现的解。项 $\\frac{[r(\\phi)]^2}{R_x R_y}$ 量化了目标各向同性分辨率与给定各向异性分辨率之间总像素数的不平衡。指数 $2/5$ 和 $1/5$ 源于宽度（$w^2$）和深度（$d^1$）对计算预算的相对影响以及最小化准则。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the adjusted width and depth multipliers for a CNN under\n    anisotropic input resolution to maintain a constant compute budget,\n    based on the principles of compound scaling.\n    \"\"\"\n    \n    # Define constants from the problem statement.\n    r0 = 224.0\n    alpha = 1.2\n    beta = 1.1\n    gamma = 1.15\n\n    # Define the test cases.\n    test_cases = [\n        # Case 1: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0)),\n        # Case 2: phi = 2, Rx = 224 * 1.15^2 * 3, Ry = 224 * 1.15^2 / 3\n        (2.0, r0 * np.power(gamma, 2.0) * 3.0, r0 * np.power(gamma, 2.0) / 3.0),\n        # Case 3: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15 / 2\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0) / 2.0),\n        # Case 4: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15 * 2\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0) * 2.0),\n        # Case 5: phi = 0, Rx = 336, Ry = 112\n        (0.0, 336.0, 112.0)\n    ]\n\n    results = []\n    for phi, Rx, Ry in test_cases:\n        # Calculate the nominal isotropic resolution for the given phi.\n        r_phi = r0 * np.power(gamma, phi)\n        \n        # Calculate the ratio of nominal squared resolution to the product of\n        # the anisotropic resolutions. This is the core scaling factor.\n        resolution_ratio = np.power(r_phi, 2) / (Rx * Ry)\n        \n        # Calculate the relative adjustment factors.\n        # The exponents 2/5 and 1/5 come from the constrained optimization.\n        # w_adj/w(phi) = (resolution_ratio)^(2/5)\n        # d_adj/d(phi) = (resolution_ratio)^(1/5)\n        width_adj_ratio = np.power(resolution_ratio, 0.4)\n        depth_adj_ratio = np.power(resolution_ratio, 0.2)\n        \n        # Round the results to 6 decimal places as required.\n        w_adj_rounded = round(width_adj_ratio, 6)\n        d_adj_rounded = round(depth_adj_ratio, 6)\n        \n        results.append([w_adj_rounded, d_adj_rounded])\n\n    # Format the final output string to exactly match the problem specification:\n    # \"[[a_1,b_1],[a_2,b_2],...]\"\n    # Using str() on a list of lists and removing spaces is a robust way to achieve this.\n    output_str = str(results).replace(\" \", \"\")\n    \n    print(output_str)\n\nsolve()\n```", "id": "3119513"}]}