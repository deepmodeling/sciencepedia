{"hands_on_practices": [{"introduction": "全局平均池化（GAP）最广为人知的优点之一是它能显著减少模型参数，从而降低过拟合的风险。本练习将引导你通过具体的计算，量化从传统全连接层切换到GAP层所带来的参数节省。通过这个过程，你将深刻理解GAP在构建轻量级且高效的卷积神经网络中的核心作用。[@problem_id:3129826]", "problem": "一个卷积神经网络在分类前生成一个形状为 $C \\times H \\times W$ 的最终特征张量。您考虑用两种备选方案来进行最后的分类阶段，以产生 $K$ 个输出：\n(i) 一个全连接（FC）层，直接从展平的 $C \\cdot H \\cdot W$ 输入到 $K$ 个输出，以及\n(ii) 在空间维度上进行全局平均池化（GAP；Global Average Pooling），以获得一个 $C$ 维向量，然后是一个线性的 $C \\to K$ 层。\n\n仅从以下定义出发：一个输入维度为 $n$、输出维度为 $m$ 的全连接线性层包含 $n \\cdot m$ 个权重和 $m$ 个偏置；全局平均池化将每个通道的 $H \\times W$ 特征图替换为其均值。请推导这两种备选方案的总参数数量的表达式。然后，对于特定配置 $C=256$, $H=14$, $W=14$ 和 $K=1000$，假设每个参数为 $4$ 字节且每次前向传播必须读取所有参数，计算从 FC 方案切换到 GAP 加线性层方案时，参数内存带宽的乘法缩减因子。\n\n将无量纲的缩减因子（FC 方案字节数除以 GAP 加线性层方案字节数）作为您的最终答案，并四舍五入到四位有效数字。在您的推导中简要说明内存带宽节省的方向。", "solution": "首先对问题进行验证，以确保其科学性、适定性和客观性。\n\n**步骤1：提取已知条件**\n- 输入特征张量形状：$C \\times H \\times W$\n- 最终输出（类别）数量：$K$\n- 备选方案 (i)：一个全连接（FC）层，将展平的 $C \\cdot H \\cdot W$ 输入映射到 $K$ 个输出。\n- 备选方案 (ii)：在空间维度上进行全局平均池化（GAP），然后是一个从 $C$ 个输入到 $K$ 个输出的线性层。\n- FC 层参数的定义：对于一个维度为 $n$ 的输入和维度为 $m$ 的输出，它有 $n \\cdot m$ 个权重和 $m$ 个偏置。\n- GAP 的定义：将每个通道的 $H \\times W$ 特征图替换为其均值。\n- 特定配置：$C=256$, $H=14$, $W=14$, $K=1000$。\n- 内存假设：每个参数 $4$ 字节。每次前向传播读取所有参数一次。\n- 任务：推导总参数数量的表达式，并计算从备选方案 (i) 切换到 (ii) 时参数内存带宽的乘法缩减因子。\n\n**步骤2：使用提取的已知条件进行验证**\n问题是有效的。\n- **科学性：** 该问题描述了卷积神经网络（CNN）中一个标准且基础的架构权衡，对比了传统的全连接头和更现代的全局平均池化头。所提供的线性层参数数量和 GAP功能的定义是正确的，并且是深度学习领域的核心内容。\n- **适定性：** 问题是自洽的，提供了所有必要的变量（$C, H, W, K$）、定义和数值，以推导出唯一的解析表达式并计算最终的数值答案。目标陈述清晰。\n- **客观性：** 语言正式且精确，没有主观或含糊的术语。\n\n**步骤3：结论与行动**\n问题有效。将提供解答。\n\n**参数数量的推导**\n\n令 $P$ 表示给定层或层集合中的总参数数量。根据所提供的定义，一个输入维度为 $n$、输出维度为 $m$ 的线性层总参数数量为 $n \\cdot m + m$。\n\n**备选方案 (i)：全连接（FC）层**\n在此备选方案中，形状为 $C \\times H \\times W$ 的输入特征张量首先被展平为一个向量。该输入向量的维度 $n$ 为：\n$$n = C \\cdot H \\cdot W$$\n该向量随后被送入一个产生 $K$ 个输出的单一全连接层。输出维度 $m$ 为：\n$$m = K$$\nFC 层的总参数数量 $P_{FC}$ 是权重和偏置的总和：\n$$P_{FC} = n \\cdot m + m = (C \\cdot H \\cdot W) \\cdot K + K$$\n提出公因子 $K$，我们得到：\n$$P_{FC} = K(C \\cdot H \\cdot W + 1)$$\n\n**备选方案 (ii)：全局平均池化（GAP）加线性层**\n该备选方案包括两个阶段。\n1.  **全局平均池化（GAP）：** GAP 操作接收形状为 $C \\times H \\times W$ 的输入张量，并为 $C$ 个特征图中的每一个计算空间平均值。这将产生一个长度为 $C$ 的向量。GAP 操作本身是一个固定函数（求平均值），没有可训练的参数。\n    $$P_{pool} = 0$$\n2.  **线性层：** 所得的 $C$ 维向量是最终线性层的输入。对于该层，输入维度 $n$ 为：\n    $$n = C$$\n    输出维度 $m$ 保持不变：\n    $$m = K$$\n    这个线性层的总参数数量 $P_{lin}$ 是：\n    $$P_{lin} = n \\cdot m + m = C \\cdot K + K$$\n整个 GAP 加线性层备选方案的总参数数量 $P_{GAP}$ 是两个阶段参数的总和：\n$$P_{GAP} = P_{pool} + P_{lin} = 0 + (C \\cdot K + K) = K(C + 1)$$\n\n**缩减因子的推导**\n问题将参数内存带宽定义为与总参数数量成正比，比例常数为每个参数 $4$ 字节。令 $B_{FC}$ 为 FC 备选方案的内存带宽，$B_{GAP}$ 为 GAP 备选方案的带宽。\n$$B_{FC} = 4 \\cdot P_{FC} = 4K(C \\cdot H \\cdot W + 1)$$\n$$B_{GAP} = 4 \\cdot P_{GAP} = 4K(C + 1)$$\n乘法缩减因子 $R$ 是两个带宽的比率：\n$$R = \\frac{B_{FC}}{B_{GAP}} = \\frac{4K(C \\cdot H \\cdot W + 1)}{4K(C + 1)}$$\n常数因子 $4$ 和变量 $K$ 被消去，得到缩减因子的一般表达式：\n$$R = \\frac{C \\cdot H \\cdot W + 1}{C + 1}$$\n\n这个表达式为内存节省提供了理由。FC 层的参数数量中的主导项是 $C \\cdot H \\cdot W \\cdot K$，它随着空间维度 $H \\cdot W$ 的乘积而缩放。相比之下，基于 GAP 的备选方案的主导项是 $C \\cdot K$。通过在线性变换*之前*执行池化，参数数量中对空间维度 $H \\cdot W$ 的依赖被消除，从而在 $H > 1$ 或 $W > 1$ 时导致显著的减少。\n\n**数值计算**\n现在我们将给定的数值代入推导出的 $R$ 的表达式中：\n$C=256$, $H=14$, $W=14$。\n$$R = \\frac{256 \\cdot 14 \\cdot 14 + 1}{256 + 1}$$\n首先，计算分子中的乘积：\n$$256 \\cdot 14 \\cdot 14 = 256 \\cdot 196 = 50176$$\n现在将此结果代回 $R$ 的表达式中：\n$$R = \\frac{50176 + 1}{257} = \\frac{50177}{257}$$\n进行除法运算：\n$$R \\approx 195.241245...$$\n按照题目要求，将结果四舍五入到四位有效数字，得到：\n$$R \\approx 195.2$$", "answer": "$$\n\\boxed{195.2}\n$$", "id": "3129826"}, {"introduction": "选择池化策略并不仅仅是技术选择，它还反映了对数据中特征分布的假设。本练习通过一个巧妙的“玩具模型”，对比了全局平均池化（GAP）和全局最大池化（GMP），揭示了它们各自的“表征偏差”。你将分析在哪种情境下（全局分布特征 vs. 局部显著特征），哪种池化方法表现更优，从而建立起为特定任务选择合适池化层的直觉。[@problem_id:3129750]", "problem": "考虑一个由卷积神经网络（CNN）生成的单通道卷积特征图，其空间尺寸为 $H \\times W$。令 $S \\equiv H \\cdot W$。对于一张图像，将在空间位置 $i \\in \\{1,\\dots,S\\}$ 处的激活值表示为随机变量 $X_i$。假设类条件分布遵循以下玩具模型：在负类（$Y=0$）下，所有 $X_i$ 都是独立同分布的，服从均值为 $\\mu_0$、方差为 $\\sigma^2$ 的高斯分布；在正类（$Y=1$）下，恰好有一个未知位置 $j$ 包含一个判别性部分，其 $X_j \\sim \\mathcal{N}(\\mu_1,\\sigma^2)$，而所有其他位置都是背景，其 $X_i \\sim \\mathcal{N}(\\mu_0,\\sigma^2)$（对于 $i \\neq j$）。我们假设不同位置之间相互独立，且类先验概率相等。考虑两个池化算子，其后跟一个单阈值分类器：全局平均池化（GAP），其输出为 $Z_{\\text{avg}} \\equiv \\frac{1}{S}\\sum_{i=1}^S X_i$；以及全局最大池化（GMP），其输出为 $Z_{\\max} \\equiv \\max_{1 \\le i \\le S} X_i$。如果 $Z \\ge t$，分类器预测 $\\hat{Y}=1$，否则预测 $\\hat{Y}=0$。将平衡准确率（BA）定义为 $\\text{BA} \\equiv \\frac{1}{2}\\left(\\Pr(\\hat{Y}=0 \\mid Y=0) + \\Pr(\\hat{Y}=1 \\mid Y=1)\\right)$。\n\n这个玩具模型捕捉了一种表示偏差的对比：GAP通过对所有位置的证据进行平均，鼓励全局的、空间分布式的描述符；而GMP则通过关注最强的局部证据，强调基于部分的推理。仅使用高斯随机变量、独立性的基本定义以及池化的定义来推理性能。\n\n固定参数 $H=W=10$，因此 $S=100$，$\\mu_0=0$，$\\mu_1=5$，$\\sigma=1$。对于GMP，使用阈值 $t=4$。对于GAP，假设在两个类条件池化输出服从等方差高斯分布的假设下，选择的阈值 $t$ 是贝叶斯最优的。使用对出现的任何高斯尾部概率的科学合理近似，回答以下问题。选择所有正确的陈述。\n\nA. 在GAP下，$Z_{\\text{avg}}$ 的类条件分布是等方差的高斯分布；贝叶斯最优阈值产生的平衡准确率约为 $\\text{BA} \\approx 0.60$。在GMP下，使用 $t=4$ 时，得到的平衡准确率约为 $\\text{BA} \\approx 0.92$。因此，在这个基于部分的玩具问题上，最大池化优于平均池化，并且与数据生成机制更吻合。\n\nB. 因为GAP具有平移不变性，所以随着 $S$ 的增加（固定 $\\mu_1>\\mu_0$ 和 $\\sigma$），其准确率会严格增加，并在 $S \\to \\infty$ 时接近于1。\n\nC. 在给定参数下，阈值为 $t=4$ 的GMP的假阳性率约为 $3\\%$，但真阳性率约为 $16\\%$，得到 $\\text{BA} \\approx 0.57$，因此在这个玩具问题上GMP劣于GAP。\n\nD. 使用合适的阈值，GAP和GMP必然会在此玩具分布上达到相同的贝叶斯误差，因为它们都是 $\\{X_i\\}_{i=1}^S$ 的排列不变函数。", "solution": "在进行求解之前，对问题陈述进行了严格验证。\n\n### 第1步：提取给定条件\n- 特征图空间尺寸：$H \\times W$。\n- 空间位置总数：$S \\equiv H \\cdot W$。\n- 位置 $i$ 处的激活值：随机变量 $X_i$，其中 $i \\in \\{1,\\dots,S\\}$。\n- 类别标签：$Y \\in \\{0, 1\\}$。\n- 类别 $Y=0$（负类）：对于所有 $i \\in \\{1, \\dots, S\\}$，$X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ 且独立同分布（i.i.d.）。\n- 类别 $Y=1$（正类）：对于恰好一个未知位置 $j$，$X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$。对于所有其他位置 $i \\neq j$，$X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$。所有 $X_i$ 都是独立的。\n- 类先验概率：$\\Pr(Y=0) = \\Pr(Y=1) = 1/2$。\n- 池化算子：\n  - 全局平均池化 (GAP): $Z_{\\text{avg}} \\equiv \\frac{1}{S}\\sum_{i=1}^S X_i$。\n  - 全局最大池化 (GMP): $Z_{\\text{max}} \\equiv \\max_{1 \\le i \\le S} X_i$。\n- 分类器：如果池化输出 $Z \\ge t$，则预测 $\\hat{Y}=1$，否则预测 $\\hat{Y}=0$。\n- 性能度量：平衡准确率 (BA) $\\equiv \\frac{1}{2}\\left(\\Pr(\\hat{Y}=0 \\mid Y=0) + \\Pr(\\hat{Y}=1 \\mid Y=1)\\right)$。\n- 参数：$H=10, W=10$，所以 $S=100$。$\\mu_0=0, \\mu_1=5, \\sigma=1$。\n- 阈值：对于GMP，$t=4$。对于GAP，对于 $Z_{\\text{avg}}$ 的类条件分布，$t$ 是贝叶斯最优的。\n\n### 第2步：使用提取的给定条件进行验证\n1.  **科学依据：** 该问题是一个定义明确的统计学玩具模型。它使用概率论中的标准、无争议的概念（高斯分布、独立性）来模拟表示学习中的一个可能场景（局部特征与分布式特征）。它是科学合理的。\n2.  **适定性：** 所有必要的参数（$\\mu_0, \\mu_1, \\sigma, S$）和分布都已定义。目标（计算GAP和GMP的BA）是明确的。可以推导出唯一的解。\n3.  **客观性：** 问题以精确、定量和客观的语言陈述。\n\n### 第3步：结论与行动\n问题陈述有效。将推导解答。\n\n### 解答推导\n\n首先，我们分析全局平均池化（GAP）算子。\n$Z_{\\text{avg}}$ 的类条件分布推导如下：\n\n如果 $Y=0$，所有 $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ 都是独立同分布的。由于独立高斯随机变量的和仍然是高斯分布，因此 $Z_{\\text{avg}} = \\frac{1}{S}\\sum_i X_i$ 是高斯分布。\n均值为 $E[Z_{\\text{avg}} \\mid Y=0] = \\frac{1}{S} \\sum_i E[X_i] = \\frac{1}{S} S \\mu_0 = \\mu_0$。\n方差为 $\\text{Var}(Z_{\\text{avg}} \\mid Y=0) = \\frac{1}{S^2} \\sum_i \\text{Var}(X_i) = \\frac{1}{S^2} S \\sigma^2 = \\frac{\\sigma^2}{S}$。\n因此，$Z_{\\text{avg}} \\mid (Y=0) \\sim \\mathcal{N}(\\mu_0, \\sigma^2/S)$。\n\n如果 $Y=1$，一个 $X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$，另外 $S-1$ 个变量为 $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$。$Z_{\\text{avg}}$ 同样是独立高斯变量的和，因此也是高斯分布。\n均值为 $E[Z_{\\text{avg}} \\mid Y=1] = \\frac{1}{S} \\left( E[X_j] + \\sum_{i \\neq j} E[X_i] \\right) = \\frac{1}{S}(\\mu_1 + (S-1)\\mu_0)$。\n方差为 $\\text{Var}(Z_{\\text{avg}} \\mid Y=1) = \\frac{1}{S^2} \\left( \\text{Var}(X_j) + \\sum_{i \\neq j} \\text{Var}(X_i) \\right) = \\frac{1}{S^2}(\\sigma^2 + (S-1)\\sigma^2) = \\frac{\\sigma^2}{S}$。\n因此，$Z_{\\text{avg}} \\mid (Y=1) \\sim \\mathcal{N}(\\frac{\\mu_1 + (S-1)\\mu_0}{S}, \\sigma^2/S)$。\n\n$Z_{\\text{avg}}$ 的类条件分布都是方差相等（为 $\\sigma^2/S$）的高斯分布。\n在类先验概率相等和方差相等的情况下，贝叶斯最优阈值是两个均值的中点：\n$t_{\\text{GAP}} = \\frac{1}{2} \\left( \\mu_0 + \\frac{\\mu_1 + (S-1)\\mu_0}{S} \\right)$。\n使用参数 $S=100, \\mu_0=0, \\mu_1=5, \\sigma=1$：\n- $Z_{\\text{avg}} \\mid (Y=0) \\sim \\mathcal{N}(0, 1/100) = \\mathcal{N}(0, (0.1)^2)$。\n- $Z_{\\text{avg}} \\mid (Y=1) \\sim \\mathcal{N}(\\frac{5 + 99 \\cdot 0}{100}, 1/100) = \\mathcal{N}(0.05, (0.1)^2)$。\n- $t_{\\text{GAP}} = \\frac{1}{2}(0 + 0.05) = 0.025$。\n\n真阳性率（TPR）为 $\\Pr(\\hat{Y}=1 \\mid Y=1) = \\Pr(Z_{\\text{avg}} \\ge t_{\\text{GAP}} \\mid Y=1)$。令 $\\Phi$ 为标准正态累积分布函数（CDF）。\n$\\text{TPR} = \\Pr\\left(\\frac{Z_{\\text{avg}} - 0.05}{0.1} \\ge \\frac{0.025-0.05}{0.1}\\right) = \\Pr(\\mathcal{N}(0,1) \\ge -0.25) = 1 - \\Phi(-0.25) = \\Phi(0.25)$。\n真阴性率（TNR）为 $\\Pr(\\hat{Y}=0 \\mid Y=0) = \\Pr(Z_{\\text{avg}}  t_{\\text{GAP}} \\mid Y=0)$。\n$\\text{TNR} = \\Pr\\left(\\frac{Z_{\\text{avg}} - 0}{0.1}  \\frac{0.025-0}{0.1}\\right) = \\Pr(\\mathcal{N}(0,1)  0.25) = \\Phi(0.25)$。\n使用 $\\Phi(0.25) \\approx 0.5987$：\n$\\text{BA}_{\\text{GAP}} = \\frac{1}{2}(\\text{TNR} + \\text{TPR}) = \\frac{1}{2}(\\Phi(0.25) + \\Phi(0.25)) = \\Phi(0.25) \\approx 0.5987 \\approx 0.60$。\n\n接下来，我们分析阈值为 $t=4$ 的全局最大池化（GMP）算子。\n需要 $Z_{\\max}$ 的累积分布函数（CDF）。令 $\\Phi_{\\mu, \\sigma}(x)$ 为 $\\mathcal{N}(\\mu, \\sigma^2)$ 的CDF。\n\n如果 $Y=0$，所有 $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$ 都是独立同分布的。\n$\\Pr(Z_{\\max}  t \\mid Y=0) = \\Pr(\\text{all } X_i  t) = \\prod_{i=1}^S \\Pr(X_i  t) = [\\Phi_{\\mu_0, \\sigma}(t)]^S$。\n如果 $Y=1$，一个 $X_j \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$，其他 $X_i \\sim \\mathcal{N}(\\mu_0, \\sigma^2)$。\n$\\Pr(Z_{\\max}  t \\mid Y=1) = \\Pr(X_j  t) \\prod_{i \\neq j} \\Pr(X_i  t) = \\Phi_{\\mu_1, \\sigma}(t) [\\Phi_{\\mu_0, \\sigma}(t)]^{S-1}$。\n\n使用参数 $S=100, \\mu_0=0, \\mu_1=5, \\sigma=1, t=4$：\n$\\Phi_{\\mu_0, \\sigma}(4) = \\Phi(\\frac{4-0}{1}) = \\Phi(4)$。\n$\\Phi_{\\mu_1, \\sigma}(4) = \\Phi(\\frac{4-5}{1}) = \\Phi(-1)$。\n我们使用近似值 $\\Phi(4) \\approx 1 - 3.167 \\times 10^{-5}$ 和 $\\Phi(-1) = 1 - \\Phi(1) \\approx 1 - 0.8413 = 0.1587$。\n\n$\\text{TNR} = \\Pr(\\hat{Y}=0 \\mid Y=0) = \\Pr(Z_{\\max}  4 \\mid Y=0) = [\\Phi(4)]^{100}$。\n对于小的 $\\epsilon$，使用近似公式 $(1-\\epsilon)^n \\approx 1 - n\\epsilon$：\n$\\text{TNR} \\approx (1 - 3.167 \\times 10^{-5})^{100} \\approx 1 - 100 \\cdot (3.167 \\times 10^{-5}) = 1 - 0.003167 = 0.996833$。\n\n$\\text{TPR} = \\Pr(\\hat{Y}=1 \\mid Y=1) = \\Pr(Z_{\\max} \\ge 4 \\mid Y=1) = 1 - \\Pr(Z_{\\max}  4 \\mid Y=1)$。\n$\\Pr(Z_{\\max}  4 \\mid Y=1) = \\Phi(-1) [\\Phi(4)]^{99} \\approx 0.1587 \\cdot (1 - 99 \\cdot (3.167 \\times 10^{-5}))$.\n$\\approx 0.1587 \\cdot (1 - 0.003135) \\approx 0.1587 \\cdot 0.996865 \\approx 0.15819$。\n$\\text{TPR} \\approx 1 - 0.15819 = 0.84181$。\n\n$\\text{BA}_{\\text{GMP}} = \\frac{1}{2}(\\text{TNR} + \\text{TPR}) \\approx \\frac{1}{2}(0.996833 + 0.84181) = \\frac{1.838643}{2} \\approx 0.9193 \\approx 0.92$。\n\n### 选项评估\n\n**A. 在GAP下，$Z_{\\text{avg}}$ 的类条件分布是等方差的高斯分布；贝叶斯最优阈值产生的平衡准确率约为 $\\text{BA} \\approx 0.60$。在GMP下，使用 $t=4$ 时，得到的平衡准确率约为 $\\text{BA} \\approx 0.92$。因此，在这个基于部分的玩具问题上，最大池化优于平均池化，并且与数据生成机制更吻合。**\n- 如上所述，$Z_{\\text{avg}}$ 的分布是等方差的高斯分布的说法是正确的。\n- $\\text{BA}_{\\text{GAP}} \\approx 0.60$ 的计算是正确的。\n- $\\text{BA}_{\\text{GMP}} \\approx 0.92$ 的计算是正确的。\n- GMP 在此问题上优于 GAP 的结论直接来自于 $0.92 \\gg 0.60$ 这个事实。这种优越性与 GMP 适合检测稀疏、局部化信号的直觉相符，而这正是 $Y=1$ 的数据生成过程的描述。\n- 结论：**正确**。\n\n**B. 因为GAP具有平移不变性，所以随着 $S$ 的增加（固定 $\\mu_1\\mu_0$ 和 $\\sigma$），其准确率会严格增加，并在 $S \\to \\infty$ 时接近于1。**\n- 这个推理不合逻辑。准确率的伸缩性源于 $Z_{\\text{avg}}$ 的统计特性，而不仅仅是平移不变性。\n- 让我们分析当 $S \\to \\infty$ 时 $\\text{BA}_{\\text{GAP}}$ 的行为。$Z_{\\text{avg}}$ 的两个条件分布的可分性取决于它们均值之间的距离，并按其标准差进行归一化。这通常被称为 $d'$。\n$d' = \\frac{|E[Z_{\\text{avg}}|Y=1] - E[Z_{\\text{avg}}|Y=0]|}{\\sqrt{\\text{Var}(Z_{\\text{avg}})}} = \\frac{|\\frac{\\mu_1+(S-1)\\mu_0}{S} - \\mu_0|}{\\sigma/\\sqrt{S}} = \\frac{|\\frac{\\mu_1-\\mu_0}{S}|}{\\sigma/\\sqrt{S}} = \\frac{\\mu_1-\\mu_0}{\\sigma\\sqrt{S}}$。\n当 $S \\to \\infty$ 时，$d' \\to 0$。这两个分布变得无法区分。\n最优阈值下的 BA 是 $\\text{BA}_{\\text{GAP}} = \\Phi(\\frac{d'}{2}) = \\Phi\\left(\\frac{\\mu_1-\\mu_0}{2\\sigma\\sqrt{S}}\\right)$。\n当 $S \\to \\infty$ 时，$\\Phi$ 的参数趋于0，因此 $\\text{BA}_{\\text{GAP}} \\to \\Phi(0) = 0.5$。\n这是随机猜测的性能水平。而陈述声称准确率接近1。\n- 结论：**不正确**。\n\n**C. 在给定参数下，阈值为 $t=4$ 的GMP的假阳性率约为 $3\\%$，但真阳性率约为 $16\\%$，得到 $\\text{BA} \\approx 0.57$，因此在这个玩具问题上GMP劣于GAP。**\n- 假阳性率（FPR）为 $1 - \\text{TNR} = 1 - \\Pr(Z_{\\max}  4 \\mid Y=0) = 1 - [\\Phi(4)]^{100}$。\n$\\text{FPR} \\approx 1 - (1 - 100(1-\\Phi(4))) = 100(1-\\Phi(4)) \\approx 100 \\cdot (3.167 \\times 10^{-5}) = 0.003167$，即 $0.32\\%$。这并非“约 $3\\%$”。该说法在数量级上是错误的。\n- 真阳性率（TPR）经计算约为 $0.84$（$84\\%$）。陈述声称是“$16\\%$”。$16\\%$ 约等于 $\\Pr(X_j  4)$，但这并非 TPR。该说法不正确。\n- 陈述中的 BA 计算是基于这些错误的比率。GMP劣于GAP的结论是基于这个错误的 BA，并与我们的发现相矛盾。\n- 结论：**不正确**。\n\n**D. 使用合适的阈值，GAP和GMP必然会在此玩具分布上达到相同的贝叶斯误差，因为它们都是 $\\{X_i\\}_{i=1}^S$ 的排列不变函数。**\n- GAP和GMP确实都是激活值 $\\{X_i\\}$ 的排列不变函数。\n- 然而，作为一个排列不变函数并不足以成为贝叶斯最优决策函数，也不能保证任何两个这样的函数会产生相同的性能。\n- 对于这个问题，贝叶斯最优分类器将使用似然比，它是 $\\sum_i \\exp(c X_i)$（对于某个常数 $c$）的函数，而不是 $X_i$ 的简单求和或最大值。\n- GAP和GMP是不同的摘要统计量，它们以不同的方式丢弃信息。没有先验理由认为它们应该具有相同的性能。\n- 我们在选项A的分析中的明确计算显示了非常不同的性能：$\\text{BA}_{\\text{GAP}} \\approx 0.60$ 和 $\\text{BA}_{\\text{GMP}} \\approx 0.92$。这凭经验证明了它们不会达到相同的误差。\n- 结论：**不正确**。", "answer": "$$\\boxed{A}$$", "id": "3129750"}, {"introduction": "理论上的简洁操作在实际应用中可能会遇到意想不到的复杂情况。本练习将探讨一个微妙但重要的实现细节：卷积层中的零填充（zero-padding）如何对全局平均池化的输出引入系统性偏差。通过推导和分析，你不仅能识别这种“填充偏差”的来源，还能学习到如何设计一个修正方案来消除它，从而提升模型的稳健性。[@problem_id:3129817]", "problem": "一个单滤波器卷积神经网络（CNN）对单通道图像应用离散卷积。考虑一个输入图像 $X \\in \\mathbb{R}^{N \\times N}$，其所有元素都等于一个正常数 $a$。该网络应用一个 $3 \\times 3$ 的滤波器 $K$，步长为 $1$，并在所有边上进行 $1$ 像素的零填充，生成一个特征图 $F \\in \\mathbb{R}^{N \\times N}$。该滤波器是均匀的：$K$ 的每个元素都等于 $1/9$。卷积之后，全局平均池化（GAP）计算标量 $\\bar{F}$，作为 $F$ 中所有元素的算术平均值，即 $\\bar{F} = \\frac{1}{N^{2}} \\sum_{i=1}^{N} \\sum_{j=1}^{N} F_{ij}$。\n\n从有限支撑上带零填充的离散卷积和算术平均值的定义出发，推导在 GAP 输出中由零填充引入的偏差 $B(N,a)$ 的闭式表达式，其定义为 $B(N,a) = \\bar{F} - a$。在你的推导中，通过计算每个感受野在内部、边缘和角落位置覆盖了多少有效（非填充）输入像素，来区分这些位置。\n\n然后，通过在每个空间位置 $(i,j)$ 引入一个乘法校正因子来实现掩码全局平均池化，该因子等于完整卷积核支撑大小与以 $(i,j)$ 为中心的感受野所覆盖的有效输入像素数量之比。在池化过程中使用这个因子来获得一个校正后的平均值 $\\bar{F}_{m}$。从第一性原理出发，证明这个校正后的平均值消除了上述恒定图像的偏差，并明确计算 $\\bar{F}_{m}$。\n\n提供 $B(N,a)$ 的单一闭式解析表达式作为你的最终答案。", "solution": "问题要求推导在全局平均池化（GAP）层输出中由零填充引入的偏差，并分析一种校正方法。\n\n首先，让我们将问题设置形式化。我们有一个单通道输入图像 $X \\in \\mathbb{R}^{N \\times N}$，其中所有元素都是一个正常数 $a$，使得对于所有 $i,j \\in \\{1, \\dots, N\\}$ 都有 $X_{ij} = a$。该网络应用一个 $3 \\times 3$ 的滤波器 $K$，其中每个元素为 $K_{kl} = 1/9$。卷积以步长 $S=1$ 和零填充 $P=1$ 进行。\n\n零填充在输入图像 $X$ 周围增加了一个厚度为 $1$ 的零边界，从而创建了一个大小为 $(N+2) \\times (N+2)$ 的填充图像 $X_p$。$X_p$ 的元素由 $X_{p,ij} = a$（对于 $2 \\le i, j \\le N+1$）和 $X_{p,ij} = 0$（其他情况）给出。\n\n输出特征图 $F$ 的维度由公式 $O = \\lfloor \\frac{I - K_{size} + 2P}{S} \\rfloor + 1$ 给出，其中输入大小为 $I=N$，滤波器大小为 $K_{size}=3$，填充为 $P=1$，步长为 $S=1$。这得出的输出大小为 $O = \\lfloor \\frac{N - 3 + 2(1)}{1} \\rfloor + 1 = N$。因此，特征图为 $F \\in \\mathbb{R}^{N \\times N}$。\n\n特征图的一个元素 $F_{ij}$ 是通过离散卷积计算的。使用卷积核索引 $u,v \\in \\{-1, 0, 1\\}$，卷积为：\n$$F_{ij} = \\sum_{u=-1}^{1} \\sum_{v=-1}^{1} K_{uv} X_{p, i+u+1, j+v+1}$$\n由于对于所有 $u,v$ 都有 $K_{uv} = 1/9$，这可以简化为：\n$$F_{ij} = \\frac{1}{9} \\sum_{u=-1}^{1} \\sum_{v=-1}^{1} X_{p, i+u+1, j+v+1}$$\n这个表达式表明 $F_{ij}$ 是填充图像 $X_p$ 内 $3 \\times 3$ 感受野中值的算术平均值。$F_{ij}$ 的值取决于其对应感受野中有多少个值为零的填充像素。对于 $N \\ge 2$，我们将特征图 $F$ 的像素分为三类：\n\n1.  **内部像素**：对于 $2 \\le i, j \\le N-1$，其 $3 \\times 3$ 的感受野完全包含在 $X_p$ 的非填充区域内。感受野中的所有 $9$ 个像素值都为 $a$。\n    $$F_{ij} = \\frac{1}{9} (9 \\cdot a) = a$$\n    共有 $(N-2)^2$ 个这样的内部像素。\n\n2.  **边缘像素（不包括角落）**：这些是位于 $F$ 边界上但不在角落的像素。例如，对于上边缘的一个像素，其中 $i=1$ 且 $2 \\le j \\le N-1$，其感受野覆盖了一行零（来自填充）和两行原始图像。这意味着感受野包含 $3 \\times 2 = 6$ 个值为 $a$ 的像素和 $3 \\times 1 = 3$ 个值为 $0$ 的像素。\n    $$F_{ij} = \\frac{1}{9} (6 \\cdot a + 3 \\cdot 0) = \\frac{6a}{9} = \\frac{2a}{3}$$\n    根据对称性，这个值对所有边缘像素都是相同的。有 $4$ 条边，每条边有 $N-2$ 个像素，总共有 $4(N-2)$ 个边缘像素。\n\n3.  **角落像素**：这些是 $F$ 的四个角落像素：$(1,1)$、$(1,N)$、$(N,1)$ 和 $(N,N)$。对于左上角 $(1,1)$，其感受野覆盖了原始图像的一个 $2 \\times 2$ 区域。这意味着感受野包含 $4$ 个值为 $a$ 的像素和 $5$ 个值为 $0$ 的像素。\n    $$F_{11} = \\frac{1}{9} (4 \\cdot a + 5 \\cdot 0) = \\frac{4a}{9}$$\n    这个值对所有 $4$ 个角落像素都是相同的。\n\n特征图 $F$ 中所有元素的总和是通过对每个类别的贡献求和来计算的：\n$$\\sum_{i=1}^{N} \\sum_{j=1}^{N} F_{ij} = (N-2)^2 \\cdot a + 4(N-2) \\cdot \\frac{2a}{3} + 4 \\cdot \\frac{4a}{9}$$\n提取公因数 $a$ 并找到公分母：\n$$ = a \\left[ (N^2 - 4N + 4) + \\frac{8N-16}{3} + \\frac{16}{9} \\right] = \\frac{a}{9} \\left[ 9(N^2 - 4N + 4) + 3(8N-16) + 16 \\right] $$\n$$ = \\frac{a}{9} [ 9N^2 - 36N + 36 + 24N - 48 + 16 ] = \\frac{a}{9} [ 9N^2 - 12N + 4 ] $$\n方括号中的项是一个完全平方：$9N^2 - 12N + 4 = (3N-2)^2$。所以，总和是：\n$$\\sum_{i=1}^{N} \\sum_{j=1}^{N} F_{ij} = \\frac{a}{9} (3N-2)^2$$\n全局平均池化输出 $\\bar{F}$ 是这些元素的平均值：\n$$\\bar{F} = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} F_{ij} = \\frac{1}{N^2} \\frac{a}{9} (3N-2)^2 = a \\left( \\frac{3N-2}{3N} \\right)^2 = a \\left( 1 - \\frac{2}{3N} \\right)^2$$\n偏差 $B(N,a)$ 定义为 $\\bar{F}$ 和理想值 $a$ 之间的差：\n$$B(N,a) = \\bar{F} - a = a \\left( 1 - \\frac{2}{3N} \\right)^2 - a$$\n$$B(N,a) = a \\left[ \\left(1 - \\frac{4}{3N} + \\frac{4}{9N^2}\\right) - 1 \\right] = a \\left( -\\frac{4}{3N} + \\frac{4}{9N^2} \\right)$$\n简化此表达式可得到偏差的最终闭式形式：\n$$B(N,a) = \\frac{-12aN + 4a}{9N^2} = \\frac{4a(1 - 3N)}{9N^2}$$\n\n对于问题的第二部分，我们分析掩码全局平均池化。在每个位置 $(i,j)$ 的校正因子是 $c_{ij} = \\frac{\\text{完整卷积核支撑大小}}{\\text{有效输入像素数量}}$。卷积核支撑大小为 $3 \\times 3 = 9$。令 $V_{ij}$ 为位置 $(i,j)$ 的有效（非填充）像素数量。\n- 对于内部像素，$V_{ij}=9$，所以 $c_{ij} = 9/9 = 1$。\n- 对于边缘像素，$V_{ij}=6$，所以 $c_{ij} = 9/6 = 3/2$。\n- 对于角落像素，$V_{ij}=4$，所以 $c_{ij} = 9/4$。\n\n未校正的特征图值可以一般地写为 $F_{ij} = \\frac{V_{ij}a}{9}$。每个位置的校正值为：\n$$c_{ij} F_{ij} = \\left(\\frac{9}{V_{ij}}\\right) \\left(\\frac{V_{ij}a}{9}\\right) = a$$\n这表明，应用校正因子可以在特征图的每个位置恢复真实值 $a$，而不受边界效应的影响。因此，掩码 GAP 输出 $\\bar{F}_m$ 为：\n$$\\bar{F}_m = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} c_{ij}F_{ij} = \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} a = \\frac{1}{N^2} (N^2 a) = a$$\n这证明了掩码全局平均池化消除了给定恒定图像的偏差，因为新的偏差为 $\\bar{F}_m - a = a - a = 0$。", "answer": "$$\\boxed{\\frac{4a(1 - 3N)}{9N^2}}$$", "id": "3129817"}]}