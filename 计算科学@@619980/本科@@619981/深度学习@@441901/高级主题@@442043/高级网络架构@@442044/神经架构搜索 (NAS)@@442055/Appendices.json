{"hands_on_practices": [{"introduction": "在开始搜索最佳架构之前，我们必须首先确保搜索空间中的备选架构是可训练的。这个实践练习 [@problem_id:3158074] 深入探讨了残差连接（residual connections）这一关键架构组件如何解决深度网络中的梯度消失或爆炸问题。通过对跳跃连接（skip connection）的密度进行建模，你将推导并实现一个预测网络训练稳定性的公式，从而理解架构设计的基本原则。", "problem": "考虑在神经架构搜索（NAS）的背景下设计残差网络。我们希望通过对跳跃连接密度及其对梯度在网络深度上传播的影响进行建模，来优化残差连接的模式。目标是通过对梯度范数的数学原理近似，来确定在每一层包含残差跳跃的概率（视为一个密度参数）如何与可训练性相关联。\n\n从以下基本原理和经过充分检验的事实出发：\n1. 深度网络的微分链式法则，其中通过一系列层的反向传播会以乘法方式复合雅可比矩阵。\n2. 算子范数的次乘性，它将乘积的范数限定为范数的乘积。\n3. 残差层变换建模为恒等跳跃连接加上一个学习到的变换。当存在跳跃连接时，其反向传播的梯度会经由单位矩阵与局部雅可比矩阵之和的转置进行变换；而当不存在跳跃连接时，则仅由雅可比矩阵的转置进行变换。\n4. 在各向同性假设以及各层和跳跃决策之间相互独立的假设下，局部雅可比矩阵对梯度范数的影响可以近似为每层一个标量增益随机变量。\n\n网络模型如下：\n- 网络共有 $D$ 层。在每一层 $l \\in \\{1,\\dots,D\\}$，跳跃连接以概率 $s \\in [0,1]$ 独立存在。\n- 在第 $l$ 层，局部雅可比矩阵对梯度范数的光谱效应由一个标量随机变量 $a_l > 0$ 近似。假设 $\\{a_l\\}$ 是独立同分布的，并服从由 $\\mu \\in \\mathbb{R}$ 和 $\\tau > 0$ 参数化的对数正态分布，即 $\\ln a_l \\sim \\mathcal{N}(\\mu,\\tau^2)$。\n- 当存在跳跃连接时，每层的梯度范数乘子近似为算子 $I + J_l$。在各向同性和独立性假设下，使用基于算子 2-范数的三角不等式的标量近似，将该乘子表示为 $1 + a_l$。当不存在跳跃连接时，该乘子表示为 $a_l$。\n- 设输出层 $D$ 的梯度范数归一化为 $\\|g_D\\| = 1$。\n\n任务：\n1. 基于这些原理，仅依赖独立性假设和对数正态分布的性质，推导输入梯度范数的期望值 $\\mathbb{E}[\\|g_0\\|]$ 作为 $D$、$s$、$\\mu$ 和 $\\tau$ 的函数的闭式表达式。\n2. 定义一个可训练性标准：如果期望输入梯度范数位于闭区间 $[g_{\\text{lower}}, g_{\\text{upper}}]$ 内，则模型被认为是可训练的，其中 $g_{\\text{lower}} = 0.1$ 且 $g_{\\text{upper}} = 10.0$。这些是无量纲的量，必须被视为十进制小数，而不是百分比。\n3. 实现一个完整、可运行的程序，该程序为每个测试用例计算基于推导出的期望梯度范数的布尔可训练性决策。\n\n使用以下参数值测试套件：\n- 测试 1：$D = 50$, $s = 0.2$, $\\mu = \\ln(0.8)$, $\\tau = 0.2$。\n- 测试 2：$D = 100$, $s = 0.0$, $\\mu = \\ln(0.95)$, $\\tau = 0.1$。\n- 测试 3：$D = 100$, $s = 0.9$, $\\mu = \\ln(0.8)$, $\\tau = 0.1$。\n- 测试 4：$D = 100$, $s = 0.05$, $\\mu = \\ln(0.95)$, $\\tau = 0.1$。\n- 测试 5：$D = 0$, $s = 0.5$, $\\mu = \\ln(0.9)$, $\\tau = 0.5$。\n\n最终输出格式：\n你的程序应产生一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[result_1,result_2,\\dots]$），其中每个 $result_i$ 是一个布尔值，指示该配置在所述标准下是否可训练。", "solution": "目标是为深度为 $D$ 的深度残差网络推导输入梯度范数期望值 $\\mathbb{E}[\\|g_0\\|]$ 的闭式表达式，并用它来确定网络的可训练性。推导将基于问题陈述中提供的跳跃连接和局部雅可比效应的概率模型。\n\n令 $\\|g_l\\|$ 表示第 $l$ 层输出处的梯度向量范数，其中 $l$ 的范围从 $0$（网络输入）到 $D$（网络输出）。我们已知输出层的梯度范数被归一化的边界条件，即 $\\|g_D\\| = 1$。反向传播过程决定了梯度如何从第 $l$ 层演变到前一层 $l-1$。问题将此单步反向传播对梯度范数的影响建模为一个乘法标量因子 $M_l$。\n$$ \\|g_{l-1}\\| = M_l \\|g_l\\| $$\n从第 $D$ 层到第 $0$ 层递归地应用此关系，我们将输入梯度范数 $\\|g_0\\|$ 表示为这些乘子作用于初始梯度范数 $\\|g_D\\|$ 的乘积：\n$$ \\|g_0\\| = \\left( \\prod_{l=1}^{D} M_l \\right) \\|g_D\\| $$\n由于 $\\|g_D\\| = 1$，这可以简化为：\n$$ \\|g_0\\| = \\prod_{l=1}^{D} M_l $$\n每一层 $l$ 的乘子 $M_l$ 是一个随机变量。其值取决于两个因素：是否存在跳跃连接，以及该层局部雅可比矩阵的光谱效应。令 $S_l$ 为一个伯努利随机变量，如果存在跳跃连接，则 $S_l=1$，否则 $S_l=0$。问题陈述 $P(S_l=1) = s$ 且 $P(S_l=0) = 1-s$。雅可比矩阵对梯度范数的影响由一个标量随机变量 $a_l > 0$ 近似。\n\n根据问题描述：\n- 如果存在跳跃连接（$S_l=1$），梯度范数乘子近似为 $1+a_l$。这是基于对恒等算子和局部雅可比矩阵之和的算子范数应用三角不等式，$\\|I+J_l\\| \\le \\|I\\| + \\|J_l\\| = 1+\\|J_l\\|$，然后用其标量近似 $a_l$ 替换 $\\|J_l\\|$。\n- 如果不存在跳跃连接（$S_l=0$），则乘子就是 $a_l$。\n\n我们可以将 $M_l$ 表示为包含 $S_l$ 和 $a_l$ 的单个代数表达式：\n$$ M_l = S_l \\cdot (1 + a_l) + (1 - S_l) \\cdot a_l = S_l + S_l a_l + a_l - S_l a_l = S_l + a_l $$\n我们的任务是求输入梯度范数的期望值 $\\mathbb{E}[\\|g_0\\|]$。使用 $\\|g_0\\|$ 的表达式：\n$$ \\mathbb{E}[\\|g_0\\|] = \\mathbb{E}\\left[ \\prod_{l=1}^{D} M_l \\right] $$\n问题陈述，跳跃连接在每层都是独立选择的，并且雅可比效应 $\\{a_l\\}$ 是独立同分布的（i.i.d.）。此外，跳跃决策 $\\{S_l\\}$ 独立于雅可比效应 $\\{a_l\\}$。因此，对于 $l=1, \\dots, D$，乘子 $\\{M_l = S_l + a_l\\}$ 构成一组独立同分布的随机变量。对于独立的随机变量，乘积的期望等于期望的乘积：\n$$ \\mathbb{E}\\left[ \\prod_{l=1}^{D} M_l \\right] = \\prod_{l=1}^{D} \\mathbb{E}[M_l] $$\n由于乘子 $\\{M_l\\}$ 是同分布的，它们的期望值相等。设 $\\mathbb{E}[M]$ 表示这个共同的期望值。表达式简化为：\n$$ \\mathbb{E}[\\|g_0\\|] = (\\mathbb{E}[M])^D $$\n我们现在计算 $\\mathbb{E}[M]$。根据期望的线性性质：\n$$ \\mathbb{E}[M] = \\mathbb{E}[S_l + a_l] = \\mathbb{E}[S_l] + \\mathbb{E}[a_l] $$\n伯努利随机变量 $S_l$ 的期望是其成功概率 $s$：\n$$ \\mathbb{E}[S_l] = 1 \\cdot P(S_l=1) + 0 \\cdot P(S_l=0) = s $$\n随机变量 $a_l$ 被给定服从对数正态分布，由 $\\ln a_l \\sim \\mathcal{N}(\\mu, \\tau^2)$ 指定。对于一个随机变量 $X$，其中 $\\ln X \\sim \\mathcal{N}(\\mu_{ln}, \\sigma_{ln}^2)$，其期望为 $\\mathbb{E}[X] = \\exp(\\mu_{ln} + \\sigma_{ln}^2/2)$。将此公式应用于具有参数 $\\mu$ 和 $\\tau^2$ 的 $a_l$：\n$$ \\mathbb{E}[a_l] = \\exp\\left(\\mu + \\frac{\\tau^2}{2}\\right) $$\n将 $S_l$ 和 $a_l$ 的期望代入 $\\mathbb{E}[M]$ 的表达式中：\n$$ \\mathbb{E}[M] = s + \\exp\\left(\\mu + \\frac{\\tau^2}{2}\\right) $$\n最后，我们得到输入梯度范数期望的闭式表达式：\n$$ \\mathbb{E}[\\|g_0\\|] = \\left(s + \\exp\\left(\\mu + \\frac{\\tau^2}{2}\\right)\\right)^D $$\n对于 $D=0$ 的特殊情况，此公式正确地得出 $\\mathbb{E}[\\|g_0\\|] = (\\dots)^0 = 1$，这对应于输入和输出相同，因此 $\\|g_0\\|=\\|g_D\\|=1$。\n\n可训练性标准定义为期望输入梯度范数落在指定范围内：\n$$ g_{\\text{lower}} \\le \\mathbb{E}[\\|g_0\\|] \\le g_{\\text{upper}} $$\n其中 $g_{\\text{lower}} = 0.1$ 且 $g_{\\text{upper}} = 10.0$。我们将实现一个函数，使用推导出的公式计算 $\\mathbb{E}[\\|g_0\\|]$，并检查对于给定的测试用例，此条件是否成立。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the trainability decision for a series of neural network configurations.\n\n    The decision is based on a derived formula for the expected input gradient norm\n    in a residual network model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: (D=50, s=0.2, mu=ln(0.8), tau=0.2)\n        (50, 0.2, np.log(0.8), 0.2),\n        # Test 2: (D=100, s=0.0, mu=ln(0.95), tau=0.1)\n        (100, 0.0, np.log(0.95), 0.1),\n        # Test 3: (D=100, s=0.9, mu=ln(0.8), tau=0.1)\n        (100, 0.9, np.log(0.8), 0.1),\n        # Test 4: (D=100, s=0.05, mu=ln(0.95), tau=0.1)\n        (100, 0.05, np.log(0.95), 0.1),\n        # Test 5: (D=0, s=0.5, mu=ln(0.9), tau=0.5)\n        (0, 0.5, np.log(0.9), 0.5),\n    ]\n\n    # Trainability bounds\n    g_lower = 0.1\n    g_upper = 10.0\n\n    results = []\n    for case in test_cases:\n        D, s, mu, tau = case\n\n        # The derived formula for the expected input gradient norm is:\n        # E[||g_0||] = (s + exp(mu + tau^2 / 2))^D\n\n        # Handle the D=0 edge case directly for clarity and to avoid 0^0 issues,\n        # although Python's pow(x, 0) correctly returns 1.0.\n        if D == 0:\n            expected_grad_norm = 1.0\n        else:\n            # Calculate the expected value of the Jacobian gain 'a_l'.\n            # E[a_l] = exp(mu + tau^2 / 2)\n            E_a = np.exp(mu + (tau**2) / 2.0)\n\n            # Calculate the expected value of the per-layer multiplier 'M_l'.\n            # E[M_l] = s + E[a_l]\n            E_M = s + E_a\n\n            # Calculate the final expected input gradient norm.\n            # E[||g_0||] = (E[M_l])^D\n            expected_grad_norm = E_M**D\n\n        # Apply the trainability criterion.\n        is_trainable = g_lower = expected_grad_norm = g_upper\n        results.append(is_trainable)\n\n    # Format the final list of boolean results into the required string format.\n    # Note: str(True) -> \"True\", str(False) -> \"False\".\n    # The problem specifies boolean results, and this is the standard string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3158074"}, {"introduction": "一个巨大的搜索空间使得暴力搜索最优架构变得不切实际，因此高效的搜索策略至关重要。本练习 [@problem_id:3158131] 介绍了一种现代且高效的搜索方法：可微分架构搜索（differentiable architecture search）。你将学习如何使用温度控制的 Sigmoid 门将离散的架构选择问题松弛（relax）为连续问题，并利用梯度下降进行优化，这是许多前沿 NAS 方法的核心思想。", "problem": "考虑神经架构搜索（NAS），其中使用二元剪枝掩码来选择架构组件的一个子集。为实现基于梯度的优化，通过温控 sigmoid 参数化将每个二元掩码变量从 $m_\\ell \\in \\{0,1\\}$ 松弛为连续门 $m_\\ell \\in [0,1]$，即 $m_\\ell(\\mathbf{z}, \\tau) = \\sigma\\!\\left(\\frac{z_\\ell}{\\tau}\\right)$，其中 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$，$\\mathbf{z} \\in \\mathbb{R}^L$ 是 logits 向量，$\\tau  0$ 是一个温度参数。考虑复合目标\n$$\nJ(\\mathbf{z};\\tau) \\;=\\; -\\sum_{\\ell=1}^L b_\\ell \\, m_\\ell(\\mathbf{z}, \\tau) \\;+\\; \\lambda \\sum_{\\ell=1}^L \\big| m_\\ell(\\mathbf{z}, \\tau) \\big|,\n$$\n其中 $b_\\ell \\ge 0$ 模拟了保留组件 $\\ell$ 的效用，$\\lambda \\ge 0$ 通过对松弛掩码施加 $\\ell_1$ 惩罚来控制稀疏性。该设置抽象了验证性能贡献和稀疏性压力之间的权衡，是可微分剪枝和神经架构搜索中一个常用的公式。\n\n从 sigmoid 函数和 $\\ell_1$ 范数次梯度的定义出发，使用链式法则实现对 $\\mathbf{z}$ 的梯度下降，以计算 $\\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)$。使用退火策略，以乘法方式降低温度 $\\tau$，直至达到最低温度。在每个温度水平上，使用固定的学习率对 $\\mathbf{z}$ 运行固定次数的梯度下降迭代。将 $\\mathbf{z}$ 初始化为零向量。\n\n将松弛掩码与最近离散值的距离定义为 $d(x) = \\min\\{\\,|x-0|,\\;|x-1|\\,\\}$，对于 $x \\in [0,1]$。对于每个测试用例，在完成退火策略和梯度下降后，计算所有坐标上与离散值的最大距离，即 $\\max_{\\ell} d\\!\\left(m_\\ell(\\mathbf{z},\\tau_{\\text{final}})\\right)$，其中 $\\tau_{\\text{final}}$ 是策略中使用的最后一个温度。每个测试用例的这个单一实数将是需要报告的输出。\n\n您的程序必须以自包含的方式实现以下数值过程，并为所有列出的测试用例生成结果：\n\n- 更新操作在每个退火阶段使用温度 $\\tau$ 对 $\\mathbf{z}$ 进行梯度下降，每个阶段 $T$ 步，学习率为 $\\eta$。\n- 温度策略为 $\\tau_0, \\tau_1, \\dots$，其中 $\\tau_{k+1} = \\gamma \\, \\tau_k$，直到 $\\tau_K \\le \\tau_{\\min}$，其中 $0  \\gamma  1$。\n- 按如下方式使用 $\\ell_1$ 项的次梯度：对于 $m_\\ell  0$，取 $\\frac{\\partial}{\\partial m_\\ell} |m_\\ell| = 1$；在 $m_\\ell = 0$ 时，选择次梯度 $0$。这一选择产生了一个关于 $m_\\ell$ 的分段常数导数，并且与标准凸分析一致。\n- 为确保 $\\sigma\\!\\left(\\frac{z_\\ell}{\\tau}\\right)$ 的数值稳定性，在计算 sigmoid 时，可以将 logits $z_\\ell$ 裁剪到一个有限范围内。\n\n测试套件：\n为以下四个测试用例提供输出。在每个用例中，维度为 $L$，效用向量为 $\\mathbf{b}$，稀疏性系数为 $\\lambda$，初始 logits 为 $\\mathbf{z}_0 = \\mathbf{0}$，初始温度为 $\\tau_0$，最低温度为 $\\tau_{\\min}$，乘法衰减因子为 $\\gamma$，每个温度的步数为 $T$，学习率为 $\\eta$。\n\n- 用例 1（倾向于离散选择的理想路径）：\n  - $L = 5$\n  - $\\mathbf{b} = [\\,1.0,\\;0.3,\\;0.8,\\;0.49,\\;2.0\\,]$\n  - $\\lambda = 0.5$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- 用例 2（所有 $\\ell$ 满足 $b_\\ell = \\lambda$ 的边界条件）：\n  - $L = 3$\n  - $\\mathbf{b} = [\\,0.5,\\;0.5,\\;0.5\\,]$\n  - $\\lambda = 0.5$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- 用例 3（无稀疏性压力）：\n  - $L = 4$\n  - $\\mathbf{b} = [\\,0.2,\\;0.1,\\;0.9,\\;1.5\\,]$\n  - $\\lambda = 0$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n- 用例 4（强稀疏性与混合效用）：\n  - $L = 3$\n  - $\\mathbf{b} = [\\,0.1,\\;3.0,\\;1.9\\,]$\n  - $\\lambda = 2.0$\n  - $\\tau_0 = 1.0$\n  - $\\tau_{\\min} = 10^{-3}$\n  - $\\gamma = 0.5$\n  - $T = 200$\n  - $\\eta = 0.5$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序与测试用例相同，其中每个条目是为该用例计算的与离散值的最大距离（例如，形如“[x1,x2,x3,x4]”的一行，其中每个 $x_i$ 是一个浮点数）。", "solution": "该问题要求在神经架构搜索（NAS）的背景下，为一个松弛的架构剪枝问题实现一个基于梯度的优化程序。我们被要求在退火策略之后，找出最终松弛掩码与离散值的最大距离。该问题具有科学依据，定义明确，并且为确定性数值模拟提供了所有必要的参数。\n\n任务的核心是对 logits $\\mathbf{z}$ 执行梯度下降，以最小化目标函数 $J(\\mathbf{z};\\tau)$。目标函数由下式给出：\n$$\nJ(\\mathbf{z};\\tau) \\;=\\; -\\sum_{\\ell=1}^L b_\\ell \\, m_\\ell(\\mathbf{z}, \\tau) \\;+\\; \\lambda \\sum_{\\ell=1}^L \\big| m_\\ell(\\mathbf{z}, \\tau) \\big|\n$$\n其中 $m_\\ell(\\mathbf{z}, \\tau) = \\sigma(z_\\ell / \\tau)$ 是组件 $\\ell$ 的 sigmoid 松弛二元掩码，$\\sigma(u) = (1 + e^{-u})^{-1}$。参数 $b_\\ell \\ge 0$ 代表效用，$\\lambda \\ge 0$ 控制稀疏性。\n\n为了执行梯度下降，我们必须计算目标函数相对于 logits 的梯度，即 $\\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)$。目标函数是关于各组件 $\\ell$ 的总和，而掩码 $m_\\ell$ 仅依赖于 logit $z_\\ell$。因此，梯度可以按元素计算。相对于单个 logit $z_\\ell$ 的偏导数可使用链式法则求得：\n$$\n\\frac{\\partial J}{\\partial z_\\ell} = \\frac{\\partial J}{\\partial m_\\ell} \\frac{\\partial m_\\ell}{\\partial z_\\ell}\n$$\n\n首先，我们计算 $J$ 相对于掩码 $m_\\ell$ 的偏导数：\n$$\n\\frac{\\partial J}{\\partial m_\\ell} = \\frac{\\partial}{\\partial m_\\ell} \\left( -b_\\ell m_\\ell + \\lambda |m_\\ell| \\right) = -b_\\ell + \\lambda \\frac{\\partial |m_\\ell|}{\\partial m_\\ell}\n$$\n问题指定了绝对值项的次梯度。由于 sigmoid 函数的值域是 $(0, 1)$，$m_\\ell$ 始终严格为正。因此，导数 $\\frac{\\partial |m_\\ell|}{\\partial m_\\ell}$ 明确为 $1$。这得到：\n$$\n\\frac{\\partial J}{\\partial m_\\ell} = -b_\\ell + \\lambda\n$$\n\n接下来，我们计算掩码 $m_\\ell$ 相对于 logit $z_\\ell$ 的偏导数：\n$$\nm_\\ell(z_\\ell, \\tau) = \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\n$$\n使用链式法则和 sigmoid 函数的已知导数 $\\sigma'(u) = \\sigma(u)(1 - \\sigma(u))$，我们得到：\n$$\n\\frac{\\partial m_\\ell}{\\partial z_\\ell} = \\sigma'\\left(\\frac{z_\\ell}{\\tau}\\right) \\cdot \\frac{\\partial}{\\partial z_\\ell}\\left(\\frac{z_\\ell}{\\tau}\\right) = \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\\left(1 - \\sigma\\left(\\frac{z_\\ell}{\\tau}\\right)\\right) \\cdot \\frac{1}{\\tau} = \\frac{m_\\ell(1 - m_\\ell)}{\\tau}\n$$\n\n结合这些结果，目标函数相对于 logit $z_\\ell$ 的偏导数为：\n$$\n\\frac{\\partial J}{\\partial z_\\ell} = (-b_\\ell + \\lambda) \\frac{m_\\ell(1 - m_\\ell)}{\\tau}\n$$\n完整的梯度向量 $\\nabla_{\\mathbf{z}} J$ 由每个 $\\ell = 1, \\dots, L$ 的这些偏导数组成。\n\n指定的数值过程如下：\n1. 初始化 logits $\\mathbf{z} = \\mathbf{0}$ 和温度 $\\tau = \\tau_0$。\n2. 进入一个退火循环，只要当前温度 $\\tau  \\tau_{\\min}$ 就继续。设 $\\tau_{\\text{final}}$ 为循环体执行时使用的最后一个温度值。\n3. 在退火循环内部，对于当前温度 $\\tau$，对 $\\mathbf{z}$ 执行 $T$ 步梯度下降：\n$$\n\\mathbf{z} \\leftarrow \\mathbf{z} - \\eta \\nabla_{\\mathbf{z}} J(\\mathbf{z};\\tau)\n$$\n其中 $\\eta$ 是学习率。\n4. 在 $T$ 步的内循环之后，更新下一阶段的温度：$\\tau \\leftarrow \\gamma \\tau$。\n5. 退火策略完成后，使用最终的 logit 向量 $\\mathbf{z}_{\\text{final}}$ 和最后使用的温度 $\\tau_{\\text{final}}$，计算最终的掩码值：\n$$\nm_{\\ell, \\text{final}} = \\sigma\\left(\\frac{z_{\\ell, \\text{final}}}{\\tau_{\\text{final}}}\\right)\n$$\n在实现过程中，为了在 $\\tau$ 较小时保持数值稳定性，sigmoid 函数的参数 $z_\\ell/\\tau$ 的绝对值可能会变得非常大。`scipy.special.expit` 函数提供了 sigmoid 的数值稳健实现，可以防止上溢/下溢问题，并在本解决方案中使用。\n\n最后，我们计算每个组件与离散值的距离，定义为 $d(x) = \\min(|x-0|, |x-1|)$，对于 $x \\in [0,1]$。由于 $m_{\\ell, \\text{final}} \\in (0,1)$，这可以简化为 $d(m_{\\ell, \\text{final}}) = \\min(m_{\\ell, \\text{final}}, 1 - m_{\\ell, \\text{final}})$。每个测试用例要报告的值是所有组件 $\\ell$ 上这些距离的最大值：\n$$\n\\max_{\\ell} d(m_{\\ell, \\text{final}})\n$$\n为所提供的四个测试用例中的每一个都实现了此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path favoring discrete selections)\n        {\n            \"L\": 5,\n            \"b\": np.array([1.0, 0.3, 0.8, 0.49, 2.0]),\n            \"lambda\": 0.5,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 2 (boundary condition where b_ell = lambda for all ell)\n        {\n            \"L\": 3,\n            \"b\": np.array([0.5, 0.5, 0.5]),\n            \"lambda\": 0.5,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 3 (no sparsity pressure)\n        {\n            \"L\": 4,\n            \"b\": np.array([0.2, 0.1, 0.9, 1.5]),\n            \"lambda\": 0.0,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n        # Case 4 (strong sparsity with mixed utilities)\n        {\n            \"L\": 3,\n            \"b\": np.array([0.1, 3.0, 1.9]),\n            \"lambda\": 2.0,\n            \"tau0\": 1.0,\n            \"tau_min\": 1e-3,\n            \"gamma\": 0.5,\n            \"T\": 200,\n            \"eta\": 0.5,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(\n            L=case[\"L\"],\n            b=case[\"b\"],\n            lambda_val=case[\"lambda\"],\n            tau0=case[\"tau0\"],\n            tau_min=case[\"tau_min\"],\n            gamma=case[\"gamma\"],\n            T=case[\"T\"],\n            eta=case[\"eta\"],\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(L, b, lambda_val, tau0, tau_min, gamma, T, eta):\n    \"\"\"\n    Runs the annealing and gradient descent procedure for a single test case.\n\n    Args:\n        L (int): Dimension of the logit vector.\n        b (np.ndarray): Utility vector.\n        lambda_val (float): Sparsity coefficient.\n        tau0 (float): Initial temperature.\n        tau_min (float): Minimum temperature for the schedule.\n        gamma (float): Multiplicative decay factor for temperature.\n        T (int): Number of gradient descent steps per temperature.\n        eta (float): Learning rate for gradient descent.\n\n    Returns:\n        float: The maximum distance to discreteness.\n    \"\"\"\n    z = np.zeros(L)\n    tau = tau0\n    tau_final = tau\n\n    # Annealing loop\n    while tau > tau_min:\n        tau_final = tau\n        # Gradient descent loop for the current temperature\n        for _ in range(T):\n            # Calculate relaxed masks m using a numerically stable sigmoid\n            # m_ell = sigma(z_ell / tau)\n            m = expit(z / tau)\n\n            # Calculate gradient of J w.r.t z\n            # dJ/dm_ell = -b_ell + lambda\n            dJ_dm = -b + lambda_val\n\n            # dm/dz_ell = m_ell * (1 - m_ell) / tau\n            dm_dz = (m * (1 - m)) / tau\n\n            # Gradient via chain rule: dJ/dz = dJ/dm * dm/dz\n            grad_z = dJ_dm * dm_dz\n\n            # Update logits using gradient descent\n            z = z - eta * grad_z\n        \n        # Anneal the temperature\n        tau *= gamma\n    \n    # After all optimization, calculate the final mask with the final z and tau_final\n    final_m = expit(z / tau_final)\n\n    # Calculate the distance to the nearest discrete value (0 or 1) for each component\n    # d(x) = min(|x-0|, |x-1|) which is min(x, 1-x) for x in [0,1]\n    distances = np.minimum(final_m, 1 - final_m)\n\n    # Return the maximum distance across all components\n    return np.max(distances)\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3158131"}, {"introduction": "无论是设计可训练的搜索空间还是执行高效的搜索策略，我们都离不开对架构性能的评估，而这通常是整个流程中最耗时的部分。本练习 [@problem_id:3158046] 聚焦于使用廉价的代理模型（proxy models）进行性能预测这一关键任务。你将运用秩相关系数（rank correlation coefficients）来衡量代理模型的预测能力，并设计一个综合考虑预测强度和稳定性的评估准则，以选出最可靠的代理模型。", "problem": "在神经架构搜索（Neural Architecture Search，NAS）的背景下，您会得到多个搜索空间。对于每个搜索空间，都有一组架构 $\\alpha$，它们在所有代理和最终指标中共享一个共同的索引顺序。每个架构 $\\alpha$ 都与一个廉价的代理分数 $Z(\\alpha)$ 和一个最终结果 $A(\\alpha)$（例如，验证准确率）相关联。目标是使用基于排名的相关性来量化代理分数和最终结果之间的单调关系，然后设计一个有原则的协议来选择跨空间最可靠的代理。该协议必须从第一性原理推导得出，并为其使用提供科学上合理的论证。您必须实现程序来计算所需的相关性并选择最佳代理。\n\n使用的基本定义包括：\n- Spearman 等级相关系数 $\\rho$，定义为两个变量的秩之间的 Pearson 相关性。\n- Kendall 等级相关系数 $\\tau$，根据一致对和不一致对定义，使用与 Kendall $\\tau_b$ 变体一致的、经相同值调整的定义。\n\n对于每个搜索空间 $s$ 和代理 $p$，计算序列 $Z^{(p)}_s(\\alpha)$ 和 $A_s(\\alpha)$ 之间的 $\\rho_{p,s}$ 和 $\\tau_{p,s}$。如果由于常量值（例如，所有 $Z^{(p)}_s(\\alpha)$ 都相同）导致相关性未定义，则将其值视为 $0$。\n\n然后，您必须设计并实现一个有原则的协议，以选择跨空间的“最佳”代理 $p^{\\star}$。您的协议必须：\n- 使用跨空间的 $\\rho_{p,s}$ 和 $\\tau_{p,s}$ 作为基本输入。\n- 以一种既能反映单调对齐强度又能反映跨空间稳定性的方式进行跨空间聚合。\n- 对于每个测试用例，输出所选代理的索引以及所选代理的聚合 Spearman 和 Kendall 值。\n\n假设代理按列出的顺序排列，索引为 $0,1,2,\\dots$。\n\n测试套件：\n- 测试用例 $1$（三个空间，三个代理）：\n  - 空间 $1$：\n    - $A_1 = [0.70,0.74,0.76,0.79,0.82,0.85]$\n    - $Z^{(0)}_1 = [0.68,0.73,0.75,0.80,0.81,0.86]$\n    - $Z^{(1)}_1 = [0.90,0.86,0.83,0.80,0.77,0.74]$\n    - $Z^{(2)}_1 = [0.69,0.75,0.74,0.78,0.83,0.84]$\n  - 空间 $2$：\n    - $A_2 = [0.60,0.63,0.68,0.71,0.74,0.80]$\n    - $Z^{(0)}_2 = [0.59,0.64,0.67,0.70,0.73,0.79]$\n    - $Z^{(1)}_2 = [0.81,0.77,0.72,0.69,0.65,0.61]$\n    - $Z^{(2)}_2 = [0.58,0.65,0.66,0.69,0.74,0.78]$\n  - 空间 $3$：\n    - $A_3 = [0.50,0.55,0.60,0.62,0.66,0.70]$\n    - $Z^{(0)}_3 = [0.49,0.54,0.59,0.61,0.65,0.69]$\n    - $Z^{(1)}_3 = [0.71,0.67,0.63,0.62,0.58,0.54]$\n    - $Z^{(2)}_3 = [0.51,0.57,0.58,0.60,0.64,0.68]$\n- 测试用例 $2$（两个空间，存在相同值和常量）：\n  - 空间 $1$：\n    - $A_1 = [0.70,0.72,0.74,0.76,0.78,0.80]$\n    - $Z^{(0)}_1 = [0.10,0.10,0.20,0.20,0.30,0.30]$\n    - $Z^{(1)}_1 = [0.30,0.30,0.20,0.20,0.10,0.10]$\n    - $Z^{(2)}_1 = [1.00,1.00,1.00,1.00,1.00,1.00]$\n  - 空间 $2$：\n    - $A_2 = [0.67,0.69,0.71,0.73,0.74,0.75]$\n    - $Z^{(0)}_2 = [10.00,10.00,20.00,20.00,30.00,30.00]$\n    - $Z^{(1)}_2 = [30.00,30.00,20.00,20.00,10.00,10.00]$\n    - $Z^{(2)}_2 = [5.00,5.00,5.00,5.00,5.00,5.00]$\n- 测试用例 $3$（三个空间，稳定性与强度）：\n  - 空间 $1$：\n    - $A_1 = [0.60,0.65,0.70,0.75,0.80,0.85]$\n    - $Z^{(0)}_1 = [1.00,2.00,3.00,4.00,5.00,6.00]$\n    - $Z^{(1)}_1 = [0.50,1.20,1.70,2.50,3.00,3.40]$\n    - $Z^{(2)}_1 = [6.00,5.00,4.00,3.00,2.00,1.00]$\n  - 空间 $2$：\n    - $A_2 = [0.50,0.55,0.58,0.62,0.66,0.72]$\n    - $Z^{(0)}_2 = [6.00,5.00,4.00,3.00,2.00,1.00]$\n    - $Z^{(1)}_2 = [0.30,0.60,0.90,1.10,1.50,1.80]$\n    - $Z^{(2)}_2 = [1.00,1.00,1.00,1.00,1.00,1.00]$\n  - 空间 $3$：\n    - $A_3 = [0.78,0.79,0.80,0.81,0.82,0.83]$\n    - $Z^{(0)}_3 = [2.00,3.00,1.00,5.00,6.00,7.00]$\n    - $Z^{(1)}_3 = [0.10,0.20,0.30,0.40,0.50,0.60]$\n    - $Z^{(2)}_3 = [10.00,9.00,8.00,7.00,6.00,5.00]$\n- 测试用例 $4$（两个空间，小样本量）：\n  - 空间 $1$：\n    - $A_1 = [0.70,0.75,0.80]$\n    - $Z^{(0)}_1 = [3.00,2.00,1.00]$\n    - $Z^{(1)}_1 = [1.00,2.00,3.00]$\n    - $Z^{(2)}_1 = [1.00,1.00,2.00]$\n  - 空间 $2$：\n    - $A_2 = [0.65,0.70,0.76]$\n    - $Z^{(0)}_2 = [3.00,2.00,1.00]$\n    - $Z^{(1)}_2 = [0.00,1.00,2.00]$\n    - $Z^{(2)}_2 = [2.00,2.00,1.00]$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。\n- 对于每个测试用例，输出一个形式为 $[p^{\\star},\\overline{\\rho},\\overline{\\tau}]$ 的嵌套列表，其中 $p^{\\star}$ 是所选代理索引， $\\overline{\\rho}$ 是所选代理在各空间上的聚合 Spearman 值，$ \\overline{\\tau}$ 是聚合 Kendall 值。将 $\\overline{\\rho}$ 和 $\\overline{\\tau}$ 表示为四舍五入到小数点后四位的小数值。\n- 所需单行格式的示例：$[[p_1,\\overline{\\rho}_1,\\overline{\\tau}_1],[p_2,\\overline{\\rho}_2,\\overline{\\tau}_2],\\dots]$。\n\n注意：\n- 此问题中没有物理单位。\n- 如果由于输入为常量而导致任何相关性未定义，则在聚合和选择代理时将其视为 $0$。", "solution": "该问题要求我们开发一个有原则的协议，用于在多个搜索空间中选择最可靠的神经架构性能预测代理。这是神经架构搜索（NAS）中的一个常见挑战，其中使用评估成本低廉的代理 $Z(\\alpha)$ 来估计架构 $\\alpha$ 的最终性能 $A(\\alpha)$，而后者测量成本高昂。一个可靠的代理应该与最终性能表现出强烈且一致的单调关系。\n\n该协议将分三个阶段制定：\n1.  使用基于排名的相关系数来量化每个搜索空间内的单调关系。\n2.  定义一个评分指标来聚合这些跨空间的相关性，奖励强度和稳定性。\n3.  建立一个选择规则，以根据该分数识别最佳代理。\n\n## 1. 基于排名的相关系数\n\n为了衡量代理分数 $Z(\\alpha)$ 和最终结果 $A(\\alpha)$ 之间的单调关系，我们使用两种标准的非参数秩相关系数：Spearman 的 $\\rho$ 和 Kendall 的 $\\tau$。\n\n### Spearman 等级相关系数 ($\\rho$)\n\nSpearman 的 $\\rho$ 是应用于秩变换后变量的 Pearson 相关系数。对于两个数据序列 $X = \\{x_1, \\dots, x_n\\}$ 和 $Y = \\{y_1, \\dots, y_n\\}$，设其对应的秩序列为 $r_X$ 和 $r_Y$。在出现相同值（tie）的情况下，每个相同值被赋予它们在没有相同时本应被分配的秩的平均值。系数 $\\rho$ 则为：\n$$ \\rho = \\frac{\\sum_{i=1}^{n} (r_{X,i} - \\bar{r}_X)(r_{Y,i} - \\bar{r}_Y)}{\\sqrt{\\sum_{i=1}^{n} (r_{X,i} - \\bar{r}_X)^2 \\sum_{i=1}^{n} (r_{Y,i} - \\bar{r}_Y)^2}} $$\n其中 $\\bar{r}_X$ 和 $\\bar{r}_Y$ 是平均秩。$\\rho$ 的取值范围从 $-1$（完全负单调关系）到 $+1$（完全正单调关系）。\n\n### Kendall 等级相关系数 ($\\tau$)\n\nKendall 的 $\\tau$ 评估当数据按每种量进行排序时，其排序的相似性。一对观测值 $(x_i, y_i)$ 和 $(x_j, y_j)$ 如果两个元素的秩一致，即如果 ($x_i > x_j$ 且 $y_i > y_j$) 或 ($x_i  x_j$ 且 $y_i  y_j$)，则为一致对。如果它们不一致，则为不一致对。我们使用 Kendall $\\tau_b$ 变体，该变体对相同值进行了调整：\n$$ \\tau_b = \\frac{N_c - N_d}{\\sqrt{(N_0 - N_1)(N_0 - N_2)}} $$\n其中 $N_c$ 是一致对的数量， $N_d$ 是不一致对的数量， $N_0 = n(n-1)/2$ 是总对数， $N_1 = \\sum_i t_i(t_i-1)/2$ 是仅在第一个变量中存在相同值的对数， $N_2 = \\sum_j u_j(u_j-1)/2$ 是仅在第二个变量中存在相同值的对数。$t_i$ 和 $u_j$ 分别是第一个和第二个变量中每个相同值组中相同值的数量。$\\tau_b$ 的取值范围也为 $-1$ 到 $+1$。\n\n根据问题规范，如果由于其中一个输入向量为常数（导致秩的方差为零和除以零）而导致相关性未定义，则其值将被视为 $0$。\n\n## 2. 用于代理选择的有原则的协议\n\n一个好的代理不仅是与真实结果具有高平均相关性的代理，而且是在不同搜索空间中稳定可靠的代理。一个在一个空间上表现完美但在另一个空间上不相关或负相关的代理，不如一个在所有空间上都具有中等良好相关性的代理有用。我们的协议必须捕捉到这种在强度（平均性能）和稳定性（一致性）之间的权衡。\n\n对于每个代理 $p$，我们计算一组跨越 $S$ 个搜索空间的相关性值：$\\{\\rho_{p,s}\\}_{s=1}^S$ 和 $\\{\\tau_{p,s}\\}_{s=1}^S$。\n\n### 聚合与评分\n\n为了平衡强度和稳定性，我们可以制定一个分数，奖励高平均相关性并惩罚高方差（或标准差）的相关性。一种科学上合理的方法是使用一个类似于性能置信下界的指标。\n\n对于代理 $p$，我们将其 Spearman $\\rho$ 的稳定性调整分数定义为：\n$$ \\mathcal{S}_{\\rho}(p) = \\mu_{\\rho,p} - \\sigma_{\\rho,p} $$\n其中 $\\mu_{\\rho,p}$ 是所有空间上 Spearman 相关性的平均值，而 $\\sigma_{\\rho,p}$ 是总体标准差。使用总体标准差是因为我们是在评估给定空间的完整集合上的性能，而不是一个样本。\n\n类似地，Kendall $\\tau$ 的分数为：\n$$ \\mathcal{S}_{\\tau}(p) = \\mu_{\\tau,p} - \\sigma_{\\tau,p} $$\n\n为了得到一个选择最佳代理的单一指标，我们结合这两个分数。由于 $\\rho$ 和 $\\tau$ 都测量相同的基本属性（单调关联），给予它们相同的权重是合理的。我们将代理 $p$ 的最终分数 $\\mathcal{S}(p)$ 定义为其稳定性调整分数的平均值：\n$$ \\mathcal{S}(p) = \\frac{1}{2} \\left( \\mathcal{S}_{\\rho}(p) + \\mathcal{S}_{\\tau}(p) \\right) = \\frac{1}{2} \\left[ (\\mu_{\\rho,p} - \\sigma_{\\rho,p}) + (\\mu_{\\tau,p} - \\sigma_{\\tau,p}) \\right] $$\n\n### 选择规则\n\n最佳代理，表示为 $p^\\star$，是使这个总分最大化的代理。这个代理被认为是最可靠的，因为它提供了最佳的保证性能水平，同时考虑了其在不同环境下的变异性。\n$$ p^\\star = \\arg\\max_{p} \\mathcal{S}(p) $$\n\n每个测试用例的最终输出将是所选代理的索引 $p^\\star$，以及其在各空间上的平均 Spearman ($\\overline{\\rho} = \\mu_{\\rho, p^\\star}$) 和平均 Kendall ($\\overline{\\tau} = \\mu_{\\tau, p^\\star}$) 相关性，四舍五入到小数点后四位。这些平均值代表了所选代理性能的“强度”部分。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr, kendalltau\n\ndef solve():\n    \"\"\"\n    Solves the Neural Architecture Search (NAS) proxy selection problem.\n    \"\"\"\n    test_cases = [\n        # Test Case 1: Three spaces, three proxies\n        (\n            [ # A_by_space\n                [0.70, 0.74, 0.76, 0.79, 0.82, 0.85],\n                [0.60, 0.63, 0.68, 0.71, 0.74, 0.80],\n                [0.50, 0.55, 0.60, 0.62, 0.66, 0.70],\n            ],\n            [ # Z_by_proxy_then_space\n                [[0.68, 0.73, 0.75, 0.80, 0.81, 0.86], [0.59, 0.64, 0.67, 0.70, 0.73, 0.79], [0.49, 0.54, 0.59, 0.61, 0.65, 0.69]], # Proxy 0\n                [[0.90, 0.86, 0.83, 0.80, 0.77, 0.74], [0.81, 0.77, 0.72, 0.69, 0.65, 0.61], [0.71, 0.67, 0.63, 0.62, 0.58, 0.54]], # Proxy 1\n                [[0.69, 0.75, 0.74, 0.78, 0.83, 0.84], [0.58, 0.65, 0.66, 0.69, 0.74, 0.78], [0.51, 0.57, 0.58, 0.60, 0.64, 0.68]], # Proxy 2\n            ]\n        ),\n        # Test Case 2: Two spaces, ties and constants\n        (\n            [ # A_by_space\n                [0.70, 0.72, 0.74, 0.76, 0.78, 0.80],\n                [0.67, 0.69, 0.71, 0.73, 0.74, 0.75],\n            ],\n            [ # Z_by_proxy_then_space\n                [[0.10, 0.10, 0.20, 0.20, 0.30, 0.30], [10.00, 10.00, 20.00, 20.00, 30.00, 30.00]], # Proxy 0\n                [[0.30, 0.30, 0.20, 0.20, 0.10, 0.10], [30.00, 30.00, 20.00, 20.00, 10.00, 10.00]], # Proxy 1\n                [[1.00, 1.00, 1.00, 1.00, 1.00, 1.00], [5.00, 5.00, 5.00, 5.00, 5.00, 5.00]],       # Proxy 2\n            ]\n        ),\n        # Test Case 3: Three spaces, stability versus strength\n        (\n            [ # A_by_space\n                [0.60, 0.65, 0.70, 0.75, 0.80, 0.85],\n                [0.50, 0.55, 0.58, 0.62, 0.66, 0.72],\n                [0.78, 0.79, 0.80, 0.81, 0.82, 0.83],\n            ],\n            [ # Z_by_proxy_then_space\n                [[1.00, 2.00, 3.00, 4.00, 5.00, 6.00], [6.00, 5.00, 4.00, 3.00, 2.00, 1.00], [2.00, 3.00, 1.00, 5.00, 6.00, 7.00]], # Proxy 0\n                [[0.50, 1.20, 1.70, 2.50, 3.00, 3.40], [0.30, 0.60, 0.90, 1.10, 1.50, 1.80], [0.10, 0.20, 0.30, 0.40, 0.50, 0.60]], # Proxy 1\n                [[6.00, 5.00, 4.00, 3.00, 2.00, 1.00], [1.00, 1.00, 1.00, 1.00, 1.00, 1.00], [10.00, 9.00, 8.00, 7.00, 6.00, 5.00]], # Proxy 2\n            ]\n        ),\n        # Test Case 4: Two spaces, small sample size\n        (\n            [ # A_by_space\n                [0.70, 0.75, 0.80],\n                [0.65, 0.70, 0.76],\n            ],\n            [ # Z_by_proxy_then_space\n                [[3.00, 2.00, 1.00], [3.00, 2.00, 1.00]], # Proxy 0\n                [[1.00, 2.00, 3.00], [0.00, 1.00, 2.00]], # Proxy 1\n                [[1.00, 1.00, 2.00], [2.00, 2.00, 1.00]], # Proxy 2\n            ]\n        ),\n    ]\n\n    all_results = []\n    \n    for A_by_space, Z_by_proxy_then_space in test_cases:\n        num_proxies = len(Z_by_proxy_then_space)\n        num_spaces = len(A_by_space)\n        \n        proxy_scores = []\n        proxy_rhos = []\n        proxy_taus = []\n\n        for p_idx in range(num_proxies):\n            rhos = []\n            taus = []\n            \n            for s_idx in range(num_spaces):\n                A_s = A_by_space[s_idx]\n                Z_ps = Z_by_proxy_then_space[p_idx][s_idx]\n\n                # Calculate Spearman's rho\n                rho, _ = spearmanr(A_s, Z_ps)\n                rho = np.nan_to_num(rho, nan=0.0)\n                rhos.append(rho)\n                \n                # Calculate Kendall's tau_b\n                tau, _ = kendalltau(A_s, Z_ps, variant='b')\n                tau = np.nan_to_num(tau, nan=0.0)\n                taus.append(tau)\n            \n            # Calculate mean and std dev for rho and tau across spaces\n            mu_rho = np.mean(rhos)\n            sigma_rho = np.std(rhos, ddof=0)\n            mu_tau = np.mean(taus)\n            sigma_tau = np.std(taus, ddof=0)\n            \n            # Calculate the stability-adjusted score\n            score = 0.5 * ((mu_rho - sigma_rho) + (mu_tau - sigma_tau))\n            \n            proxy_scores.append(score)\n            proxy_rhos.append(rhos)\n            proxy_taus.append(taus)\n\n        # Select the best proxy\n        p_star = np.argmax(proxy_scores)\n        \n        # Get the aggregated (mean) values for the selected proxy\n        mu_rho_star = np.mean(proxy_rhos[p_star])\n        mu_tau_star = np.mean(proxy_taus[p_star])\n\n        all_results.append([p_star, mu_rho_star, mu_tau_star])\n\n    # Format the final output string\n    formatted_results = []\n    for res in all_results:\n        p, rho, tau = res\n        rho_str = f\"{rho:.4f}\"\n        tau_str = f\"{tau:.4f}\"\n        formatted_results.append(f\"[{p},{rho_str},{tau_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3158046"}]}