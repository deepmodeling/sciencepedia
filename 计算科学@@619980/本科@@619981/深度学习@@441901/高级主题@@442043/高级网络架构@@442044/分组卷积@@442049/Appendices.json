{"hands_on_practices": [{"introduction": "理解群卷积最有效的方法之一是从头开始构建一个。这个练习将指导你实现一个针对$90^\\circ$旋转（即循环群 $C_4$）的简单群卷积层。通过亲手编写代码并进行数值验证，你将清晰地看到权重共享是如何保证网络对旋转操作的等变性（equivariance）的。[@problem_id:3180084]", "problem": "你的任务是实现并测试一个基于由 $90^\\circ$ 旋转生成的四阶平面旋转循环群的群卷积。此任务基于离散二维卷积（实现为不进行卷积核翻转的离散二维互相关，并采用有效窗口）和旋转在数组上的作用的核心定义。你必须仅从这些基本定义出发，通过权重共享构建一个旋转等变映射，并精确地验证其行为。\n\n使用的定义和设置：\n- 令 $X \\in \\mathbb{R}^{H \\times W}$ 为一个方格网格上的实值图像，令 $K \\in \\mathbb{R}^{k_h \\times k_w}$ 为一个实值卷积核。\n- 定义旋转算子 $R_m$（其中 $m \\in \\{0,1,2,3\\}$）作用于任意数组，将其在平面内连续旋转 $m$ 次 $90^\\circ$（以度为单位）。即，$R_0$ 是恒等变换，$R_1$ 旋转 $90^\\circ$，$R_2$ 旋转 $180^\\circ$，$R_3$ 旋转 $270^\\circ$。角度应理解为度数。\n- 将带有有效窗口的离散二维互相关定义为：对于卷积核完全覆盖图像的每个空间位置，进行逐元素相乘并求和的操作。这将输出形状减小至 $(H - k_h + 1) \\times (W - k_w + 1)$。\n- 定义四阶循环群（记作 $C_4$）上的群卷积，通过上述互相关操作生成由 $g \\in \\{0,1,2,3\\}$ 索引的四个方向通道：$Y_g = X \\star R_g K$，其中 $R_g K$ 是将卷积核旋转 $g$ 个 90 度得到的。这强制实现了跨方向的权重共享，因为所有的 $Y_g$ 都是使用单个基础卷积核 $K$ 的旋转副本计算得出的。\n- 需要验证的旋转等变性是这样一个陈述：对于 $m \\in \\{0,1,2,3\\}$，如果输入被旋转为 $X' = R_m X$，那么从 $X'$ 和相同的共享基础卷积核计算出的群卷积输出 $Y'_g$ 满足 $Y'_g = R_m \\, Y_{g - m}$，其中索引运算为模 4 算术。此处 $Y_{g - m}$ 表示索引为 $(g - m) \\bmod 4$ 的方向通道。\n\n你的程序必须：\n1. 使用在数组上连续进行 $m$ 次 $90^\\circ$ 旋转的方式实现 $R_m$。\n2. 实现上述的有效离散二维互相关。\n3. 通过 $R_g K$ 实现权重共享的 $C_4$ 群卷积 $X \\mapsto (Y_0,Y_1,Y_2,Y_3)$。\n4. 对于每个测试用例，计算一个布尔值，断言在 $10^{-9}$ 的绝对容差内，对于所有的 $g \\in \\{0,1,2,3\\}$，等变性陈述 $Y'_g = R_m \\, Y_{g - m}$ 成立。其中 $Y'_g$ 是通过将相同的 $C_4$ 群卷积应用于 $X' = R_m X$ 和相同的基础卷积核 $K$ 计算得出的。\n5. 包含一个对比测试，其中四个方向通道是使用四个不相关、不共享且没有旋转关系的卷积核 $(K_0,K_1,K_2,K_3)$ 计算的。在该测试中，断言相同的等变性条件并报告布尔结果（该结果通常为 false），从而证明跨旋转副本的权重共享是实现旋转等变性的强制机制。\n\n测试套件规范：\n- 测试用例 1 (一般情况): $X$ 大小为 $6 \\times 6$，从伪随机种子为 $1$ 的标准正态分布中抽取；$K$ 大小为 $3 \\times 3$，从伪随机种子为 $0$ 的标准正态分布中抽取；旋转索引 $m = 1$，对应于 $90^\\circ$。\n- 测试用例 2 (偶数尺寸卷积核的边界行为): $X$ 大小为 $5 \\times 5$，种子为 $3$；$K$ 大小为 $2 \\times 2$，种子为 $2$；旋转索引 $m = 1$，对应于 $90^\\circ$。\n- 测试用例 3 (多步旋转): $X$ 大小为 $7 \\times 7$，种子为 $5$；$K$ 大小为 $3 \\times 3$，种子为 $4$；旋转索引 $m = 2$，对应于 $180^\\circ$。\n- 测试用例 4 (恒等变换): $X$ 大小为 $6 \\times 6$，种子为 $7$；$K$ 大小为 $3 \\times 3$，种子为 $6$；旋转索引 $m = 0$，对应于 $0^\\circ$。\n- 测试用例 5 (结构化输入): $X$ 大小为 $8 \\times 8$，所有元素设置为 1；$K$ 大小为 $3 \\times 3$，种子为 $8$；旋转索引 $m = 3$，对应于 $270^\\circ$。\n- 测试用例 6 (无权重共享的对比测试): $X$ 大小为 $6 \\times 6$，种子为 $9$；四个不相关的卷积核 $(K_0,K_1,K_2,K_3)$，大小均为 $3 \\times 3$，分别使用种子 $10, 11, 12, 13$ 生成；旋转索引 $m = 1$，对应于 $90^\\circ$。在此情况下，使用 $Y_g = X \\star K_g$ 和 $Y'_g = (R_m X) \\star K_g$ 计算四个方向通道，并检查是否对于所有 $g$ 都成立 $Y'_g = R_m \\, Y_{g - m}$，报告布尔结果。\n\n数值容差规范：\n- 当比较数组是否相等时，使用 $10^{-9}$ 的绝对容差和 $0$ 的相对容差。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含按顺序排列的测试用例 1 到 6 的六个布尔结果，格式为方括号括起来的逗号分隔列表，例如 `[true,false,true,false,true,false]`。请使用 Python 的布尔字面量，因此实际输出应类似于 `[True,False,True,False,True,False]`。", "solution": "该问题要求实现并验证一种特定类型的卷积层（称为群卷积）的旋转等变性，该卷积层定义于 4 阶平面旋转循环群 $C_4 = \\{0^\\circ, 90^\\circ, 180^\\circ, 270^\\circ\\}$ 之上。任务的核心是，从基本原理出发，证明一个通过特定权重共享方案构建的卷积映射对其输入的旋转具有等变性。\n\n等变性是物理学和机器学习中的一个基本概念。如果一个函数或系统 $\\Phi$ 对于一组变换 $G$ 是等变的，那么对该系统的输入施加一个来自 $G$ 的变换，会产生一个可预测的变换后输出。更形式化地，对于群 $G$ 中的任意变换 $T_g$，在输出空间上都存在一个对应的变换 $T'_g$，使得 $\\Phi(T_g(X)) = T'_g(\\Phi(X))$。这一性质是理想的，因为它能确保系统对一个物体的分析与该物体的位置或朝向无关。\n\n我们的目标是验证 $C_4$ 群卷积的以下等变性恒等式：当输入图像 $X$ 被旋转 $m$ 个 90 度得到 $X' = R_m X$ 时，所得到的一组输出特征图 $(Y'_0, Y'_1, Y'_2, Y'_3)$ 是原始输出图 $(Y_0, Y_1, Y_2, Y_3)$ 的一个旋转和置换后的版本。具体来说，需要验证的恒等式为：对于每个方向通道 $g \\in \\{0, 1, 2, 3\\}$，$Y'_g = R_m Y_{(g - m) \\pmod 4}$。\n\n该解决方案基于所提供的定义，分步构建。\n\n**1. 旋转算子 $R_m$**\n\n变换群是循环群 $C_4$，其元素对应于角度为 $\\{0^\\circ, 90^\\circ, 180^\\circ, 270^\\circ\\}$ 的旋转。我们定义一个算子 $R_m$，它作用于一个二维数组，将其逆时针连续旋转 $m$ 次 $90^\\circ$，其中 $m \\in \\{0, 1, 2, 3\\}$。此操作可直接使用已有的数值库函数实现，这些函数能在矩阵上执行旋转。$R_m$ 作用于一个形状为 $H \\times W$ 的数组，如果 $m$ 为奇数，则结果数组的形状为 $W \\times H$；如果 $m$ 为偶数，则为 $H \\times W$。\n\n**2. 离散二维互相关**\n\n基本的构建块是离散二维互相关操作，用 $\\star$ 表示。给定一个输入图像 $X \\in \\mathbb{R}^{H \\times W}$ 和一个卷积核 $K \\in \\mathbb{R}^{k_h \\times k_w}$，计算输出特征图 $Y = X \\star K$。该操作指定使用“有效”窗口，意味着只在卷积核完全覆盖输入的位置计算输出。每个输出位置 $(i, j)$ 的值是卷积核与对应输入子区域逐元素相乘后的总和。\n$$Y[i,j] = \\sum_{u=0}^{k_h-1} \\sum_{v=0}^{k_w-1} X[i+u, j+v] K[u,v]$$\n此操作产生的输出图大小为 $(H - k_h + 1) \\times (W - k_w + 1)$。这与完整的卷积不同，后者会涉及翻转卷积核。\n\n**3. 通过权重共享实现的 $C_4$ 群卷积**\n\n标准的卷积层本身不具备旋转等变性。为了构建一个 $C_4$ 等变层，我们强制采用一种称为权重共享的特定结构。我们不为不同方向学习独立的卷积核，而是使用单个基础卷积核 $K$，并通过对其应用群的变换来生成所有其他必要的卷积核。\n\n$C_4$ 群卷积的输出包含四个方向通道 $(Y_0, Y_1, Y_2, Y_3)$，由群的元素 $g \\in \\{0, 1, 2, 3\\}$ 索引。每个通道 $Y_g$ 都是通过将输入图像 $X$ 与基础卷积核 $K$ 的旋转版本进行卷积计算得出的：\n$$Y_g = X \\star (R_g K)$$\n在这里，$R_g K$ 是将卷积核 $K$ 旋转 $g$ 个 90 度得到的。这种构造确保了该层检测到的特征在四个通道中是相同的，只是方向不同。\n\n**4. 等变性属性的验证**\n\n核心任务是数值上验证等变关系。该性质是旋转与互相关之间关系的直接推论。对于任意定义了该操作的二维数组 $A$ 和 $B$，以下恒等式成立：\n$$(R_m A) \\star B = R_m (A \\star R_{-m} B)$$\n其中 $R_{-m}$ 是 $R_m$ 的逆旋转。在群 $C_4$ 中，$R_m$ 的逆是 $R_{(-m) \\pmod 4}$。\n\n让我们将此恒等式应用于我们的问题。新的输出通道 $Y'_g$ 是通过一个旋转后的输入 $X' = R_m X$ 和同一组共享的、旋转后的卷积核计算的：\n$$Y'_g = X' \\star (R_g K) = (R_m X) \\star (R_g K)$$\n将该恒等式应用于 $A=X$ 和 $B=R_g K$，我们得到：\n$$Y'_g = R_m (X \\star R_{(-m) \\pmod 4} (R_g K))$$\n由于旋转算子构成一个群，它们的复合是另一个旋转：$R_{(-m) \\pmod 4} (R_g K) = R_{(-m+g) \\pmod 4} K = R_{(g-m) \\pmod 4} K$。将其代回得到：\n$$Y'_g = R_m (X \\star R_{(g-m) \\pmod 4} K)$$\n根据定义，括号中的项 $X \\star R_{(g-m) \\pmod 4} K$ 就是原始的输出通道 $Y_{(g-m) \\pmod 4}$。因此，我们得到了等变性属性：\n$$Y'_g = R_m Y_{(g-m) \\pmod 4}$$\n这个推导表明，该性质在数学上是由其构造保证的。我们的实现将对几个测试用例进行数值验证。对于每个用例，我们计算该方程对所有 $g \\in \\{0, 1, 2, 3\\}$ 的左侧和右侧，并在指定的数值容差内检查它们是否相等。\n\n**5. 对比测试：权重共享的必要性**\n\n最后一个测试用例作为一个关键的对比。在这种情况下，我们定义一个有四个输出通道的“卷积”层，但我们打破了权重共享的约束。这四个通道是使用四个独立、不相关的卷积核 $(K_0, K_1, K_2, K_3)$ 计算的。\n$$Y_g = X \\star K_g$$\n当输入被旋转为 $X' = R_m X$ 时，新的输出为 $Y'_g = (R_m X) \\star K_g$。等变性检查保持不变：我们测试是否 $Y'_g = R_m Y_{(g - m) \\pmod 4}$。由于卷积核 $K_g$ 之间没有通过群作用建立关联（即，通常情况下 $K_g \\neq R_g K_0$），步骤 4 的推导不再成立。等变性在结构上没有被强制实现，因此预计会验证失败，这证明了特定的权重共享方案是实现等变性的机制。\n\n最终的程序将系统地实现这些步骤并执行所述测试，为每个用例生成一个布尔结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef solve():\n    \"\"\"\n    Implements and verifies C4 rotation equivariance for a 2D group convolution.\n    \"\"\"\n\n    # Test suite specification\n    # (id, X_shape, X_seed, K_shape, K_seed(s), m)\n    test_cases = [\n        (1, (6, 6), 1, (3, 3), [0], 1),\n        (2, (5, 5), 3, (2, 2), [2], 1),\n        (3, (7, 7), 5, (3, 3), [4], 2),\n        (4, (6, 6), 7, (3, 3), [6], 0),\n        (5, (8, 8), 8, (3, 3), [8], 3),  # Structured input\n        (6, (6, 6), 9, (3, 3), [10, 11, 12, 13], 1)  # Contrast test\n    ]\n\n    results = []\n    \n    # Numerical tolerance for array comparisons\n    atol = 1e-9\n    rtol = 0.0\n\n    for case in test_cases:\n        case_id, x_shape, x_seed, k_shape, k_seeds, m = case\n\n        # --- Data Generation ---\n        rng_x = np.random.RandomState(x_seed)\n        if case_id == 5: # Structured input of all ones\n            X = np.ones(x_shape)\n        else:\n            X = rng_x.randn(*x_shape)\n        \n        kernels = []\n        for seed in k_seeds:\n            rng_k = np.random.RandomState(seed)\n            kernels.append(rng_k.randn(*k_shape))\n\n        is_equivariant = False # Placeholder\n        \n        # --- Helper Functions ---\n        def rotate_array(arr, k):\n            \"\"\"Rotates a 2D array by k*90 degrees counter-clockwise.\"\"\"\n            return np.rot90(arr, k=k)\n\n        def cross_correlate(image, kernel):\n            \"\"\"Computes 2D cross-correlation with 'valid' padding.\"\"\"\n            return correlate2d(image, kernel, mode='valid')\n\n        # --- Main Logic ---\n        if case_id != 6:\n            # Standard C4 Group Convolution with weight sharing\n            K_base = kernels[0]\n            \n            # 1. Compute original output channels Y_g = X * (R_g K)\n            Y_channels = []\n            for g in range(4):\n                rotated_K = rotate_array(K_base, g)\n                Y_g = cross_correlate(X, rotated_K)\n                Y_channels.append(Y_g)\n            \n            # 2. Rotate input: X' = R_m X\n            X_prime = rotate_array(X, m)\n            \n            # 3. Compute new output channels Y'_g = X' * (R_g K)\n            Y_prime_channels = []\n            for g in range(4):\n                rotated_K = rotate_array(K_base, g)\n                Y_prime_g = cross_correlate(X_prime, rotated_K)\n                Y_prime_channels.append(Y_prime_g)\n\n        else: # Contrast test without weight sharing\n            # 1. Compute original output channels Y_g = X * K_g\n            Y_channels = []\n            for g in range(4):\n                Y_g = cross_correlate(X, kernels[g])\n                Y_channels.append(Y_g)\n                \n            # 2. Rotate input: X' = R_m X\n            X_prime = rotate_array(X, m)\n\n            # 3. Compute new output channels Y'_g = X' * K_g\n            Y_prime_channels = []\n            for g in range(4):\n                Y_prime_g = cross_correlate(X_prime, kernels[g])\n                Y_prime_channels.append(Y_prime_g)\n\n        # 4. Verify equivariance: Y'_g == R_m(Y_{g-m mod 4}) for all g\n        all_g_hold = True\n        for g in range(4):\n            # LHS: Y'_g\n            lhs = Y_prime_channels[g]\n            \n            # RHS: R_m(Y_{g-m mod 4})\n            original_channel_index = (g - m) % 4\n            y_original_permuted = Y_channels[original_channel_index]\n            rhs = rotate_array(y_original_permuted, m)\n            \n            # Check for equality within tolerance\n            if not np.allclose(lhs, rhs, rtol=rtol, atol=atol):\n                all_g_hold = False\n                break\n        \n        is_equivariant = all_g_hold\n        results.append(is_equivariant)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3180084"}, {"introduction": "在掌握了特定群的实现后，下一步是将其推广到更普适的理论框架。本练习将挑战你从$C_4$群推广到更复杂的二面体群$D_8$，它包含了旋转和反射两种对称性。你将推导出群卷积的通用数学形式，并理解“提升层”（lifting layer）如何将输入数据从基础空间映射到群空间，从而为后续的等变处理奠定基础。[@problem_id:3133440]", "problem": "考虑一个小型图像识别任务，该任务旨在探究卷积神经网络（CNN）中的方向敏感性。每个输入是一个灰度图像块 $f : \\mathbb{Z}^{2} \\to \\mathbb{R}$，支撑在一个以原点为中心的 $9 \\times 9$ 网格上，其中包含一个宽度为 $1$ 像素的直条，该直条穿过中心，其方向角是 $45^{\\circ}$ 的倍数。除了以 $45^{\\circ}$ 的倍数进行旋转外，数据集还通过关于水平轴的反射进行了增强。为了实现对这些变换的等变性，我们考虑一个群等变卷积神经网络（G-CNN），其中的群是二面体群 $D_{8}$，即正八边形的对称群，它有 $|D_{8}| = 16$ 个元素，包括 $8$ 个旋转 $\\{r^{k} : k \\in \\{0,\\dots,7\\}\\}$ 和 $8$ 个反射 $\\{r^{k} s : k \\in \\{0,\\dots,7\\}\\}$，满足 $r^{8} = e$，$s^{2} = e$ 和 $s r s = r^{-1}$。\n\n你将推导保证在 $D_{8}$ 的左正则作用下具有等变性的显式群卷积，然后计算将基通道集提升到群上时所需的方向通道数。\n\n使用以下基本依据：\n- 群元素 $u \\in G$ 对函数 $F : G \\to \\mathbb{R}^{C}$ 的左正则作用 $L_{u}$ 的定义，由 $(L_{u} F)(g) = F(u^{-1} g)$ 给出。\n- 层映射的线性性和跨群的平稳性（权重共享）。\n- $D_{8}$ 通过对应于 $45^{\\circ}$ 倍数旋转和反射的正交变换作用于图像域 $\\mathbb{Z}^{2}$ 这一事实，记为 $g \\cdot x$，其中 $g \\in D_{8}$，$x \\in \\mathbb{Z}^{2}$。\n\n任务：\n1. 从列出的基本依据出发，不假设任何特定的卷积公式，推导从群 $G$ 上的函数到群 $G$ 上的函数的唯一线性、群平稳算子，该算子在 $D_{8}$ 的左正则作用下是等变的。写出其作为在 $G$ 上求和的显式形式，并简要解释其为何是等变的。\n2. 对于将 $f : \\mathbb{Z}^{2} \\to \\mathbb{R}^{C_{0}}$ 映射到 $D_{8}$ 上特征图的第一个（提升）层，推导对 $\\mathbb{Z}^{2}$ 上的 $D_{8}$ 作用等变的显式提升卷积。\n3. 网络设计者为提升层选择了基通道数 $C_{0} = 12$。预测提升后每个空间位置的总方向通道数，以单个整数表示。无需四舍五入；报告精确的整数。最终答案中不要包含单位。", "solution": "该问题要求在一个群等变卷积神经网络（G-CNN）中，推导群等变卷积算子，并计算由提升操作产生的特征通道数量。我们关注的群是阶数为 $16$ 的二面体群 $D_{8}$。\n\n首先，我们按顺序解决这三个任务。\n\n### 任务 1：群到群（$G-G$）卷积的推导\n\n我们被要求推导唯一的线性、群平稳算子 $\\Phi$，它将群 $G=D_8$ 上的函数映射到群 $G$ 上的函数，并且在左正则作用下是等变的。为简单起见，我们考虑函数 $F_{in}, F_{out}: G \\to \\mathbb{R}$。扩展到多通道函数 $F: G \\to \\mathbb{R}^C$ 是直接的。\n\n算子 $\\Phi$ 必须满足三个性质：\n1.  **线性性**：对于标量 $a, b \\in \\mathbb{R}$ 和函数 $F_1, F_2$，有 $\\Phi(a F_1 + b F_2) = a \\Phi(F_1) + b \\Phi(F_2)$。\n2.  **等变性**：$\\Phi$ 必须与群作用交换。在群 $G$ 上的函数上的作用是左正则作用，即 $(L_u F)(g) = F(u^{-1}g)$。因此，对于任意 $u \\in G$，必须有 $\\Phi(L_u F_{in}) = L_u (\\Phi F_{in})$。\n3.  **平稳性（权重共享）**：如推导将显示的，该性质是线性性和等变性的一个推论。\n\n让我们从这些原理推导 $\\Phi$ 的形式。\n任何函数 $F_{in}: G \\to \\mathbb{R}$ 都可以表示为规范基函数的线性组合。对于有限群，一个方便的基是狄拉克δ函数集 $\\{\\delta_h\\}_{h \\in G}$，其中当 $g=h$ 时 $\\delta_h(g) = 1$，否则为 $0$。\n我们可以将 $F_{in}$ 写为：\n$$F_{in}(g) = \\sum_{h \\in G} F_{in}(h) \\delta_h(g)$$\n根据线性性，$\\Phi$ 对 $F_{in}$ 的作用是：\n$$(\\Phi F_{in})(g) = \\Phi \\left( \\sum_{h \\in G} F_{in}(h) \\delta_h \\right)(g) = \\sum_{h \\in G} F_{in}(h) (\\Phi \\delta_h)(g)$$\n现在，我们使用等变性。任意元素 $h$ 处的狄拉克δ函数可以通过将左正则作用应用于单位元 $e$ 处的狄拉克δ函数得到：$\\delta_h = L_h \\delta_e$，因为 $(L_h \\delta_e)(g) = \\delta_e(h^{-1}g) = 1$ 当且仅当 $h^{-1}g=e$，即 $g=h$。\n\n利用 $\\Phi$ 的等变性，我们有：\n$$(\\Phi \\delta_h)(g) = (\\Phi (L_h \\delta_e))(g) = (L_h (\\Phi \\delta_e))(g)$$\n让我们定义一个函数 $\\kappa: G \\to \\mathbb{R}$ 作为算子对单位元处脉冲的响应，称为卷积核：$\\kappa \\equiv \\Phi \\delta_e$。\n那么，$(\\Phi \\delta_h)(g) = (L_h \\kappa)(g)$。根据左正则作用的定义，$(L_h \\kappa)(g) = \\kappa(h^{-1}g)$。\n将此代回 $(\\Phi F_{in})(g)$ 的表达式中：\n$$(\\Phi F_{in})(g) = \\sum_{h \\in G} F_{in}(h) \\kappa(h^{-1}g)$$\n这就是群卷积的显式形式，通常表示为 $(F_{in} * \\kappa)(g)$。该算子由核 $\\kappa$ 唯一确定。对于给定的群 $G=D_8$，求和是对 $D_8$ 的 $16$ 个元素进行的。\n\n通过其构造，这种形式保证是等变的。结构 $\\kappa(h^{-1}g)$ 确保如果输入函数 $F_{in}$ 在群上被元素 $u$ “平移”（即被 $L_u F_{in}$ 替换），输出函数也会被相同的元素 $u$ 平移。这满足了要求 $\\Phi(L_u F_{in}) = L_u(\\Phi F_{in})$。平稳性条件体现在核 $\\kappa$ 是一个应用于整个群域的单一函数，仅取决于输出位置 $g$ 和输入位置 $h$ 之间的相对群“距离”$h^{-1}g$。\n\n对于多通道函数 $F_{in}: G \\to \\mathbb{R}^{C_{in}}$ 和 $F_{out}: G \\to \\mathbb{R}^{C_{out}}$，核变成一个张量 $\\kappa: G \\to \\mathbb{R}^{C_{out} \\times C_{in}}$，卷积为：\n$$(F_{out})_j(g) = \\sum_{i=1}^{C_{in}} \\sum_{h \\in D_8} \\kappa_{ji}(h^{-1}g) (F_{in})_i(h)$$\n\n### 任务 2：提升卷积的推导\n\nG-CNN 的第一层是一个“提升”层，它将在基空间（这里是 $\\mathbb{Z}^2$）上的函数映射到每个空间位置处群 $D_8$ 上的函数。设该算子为 $\\mathcal{L}$。\n- 输入：$f: \\mathbb{Z}^2 \\to \\mathbb{R}^{C_0}$。在输入上的群作用为 $(T_u f)_i(x) = f_i(u^{-1} \\cdot x)$。\n- 输出：乘积空间 $\\mathbb{Z}^2 \\times D_8$ 上的一个特征图 $F$。我们用 $F(x,g)$ 表示在空间位置 $x \\in \\mathbb{Z}^2$ 和群方向 $g \\in D_8$ 处的特征。该映射为 $F: \\mathbb{Z}^2 \\times D_8 \\to \\mathbb{R}^{C_1}$。在输出空间上的群作用是诱导表示作用 $(\\Pi_u F)(x,g) = F(u^{-1} \\cdot x, u^{-1}g)$。\n\n提升层 $\\mathcal{L}$ 的等变性条件是，对于所有 $u \\in D_8$，都有 $\\mathcal{L}(T_u f) = \\Pi_u(\\mathcal{L}f)$。\n\n与标准 CNN 类似，我们假设该层是一个空间卷积（更准确地说，是互相关）。G-CNN 的关键洞见在于，对于不同方向 $g \\in D_8$ 的卷积滤波器不是独立的，而是通过群作用变换单个“母”核 $\\kappa$ 生成的。\n\n设 $\\kappa: \\mathbb{Z}^2 \\to \\mathbb{R}^{C_1 \\times C_0}$ 是可学习的核。对于每个输出基通道 $j \\in \\{1, \\dots, C_1\\}$，提升卷积算子 $\\mathcal{L}$ 定义如下：\n$$F_j(x, g) = (\\mathcal{L}f)_j(x,g) = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i(x+y) \\kappa_{ji}(g^{-1} \\cdot y)$$\n这里，$g^{-1} \\cdot y$ 是 $g^{-1} \\in D_8$ 对坐标向量 $y \\in \\mathbb{Z}^2$ 的作用。该公式使用了深度学习中常见的互相关约定。\n\n我们来验证该算子是等变的。\n让我们计算等变性方程的左侧（LHS），即 $(\\mathcal{L}(T_u f))_j(x,g)$：\n$$(\\mathcal{L}(T_u f))_j(x,g) = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} (T_u f_i)(x+y) \\kappa_{ji}(g^{-1} \\cdot y)$$\n$$= \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot (x+y)) \\kappa_{ji}(g^{-1} \\cdot y)$$\n由于群在 $\\mathbb{Z}^2$ 上的作用是线性的，所以 $u^{-1} \\cdot (x+y) = u^{-1} \\cdot x + u^{-1} \\cdot y$。\n$$LHS = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot x + u^{-1} \\cdot y) \\kappa_{ji}(g^{-1} \\cdot y)$$\n我们在求和中进行变量替换：$y' = u^{-1} \\cdot y$，这意味着 $y = u \\cdot y'$。求和域 $\\mathbb{Z}^2$ 在此变换下是不变的。\n$$LHS = \\sum_{i=1}^{C_0} \\sum_{y' \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot x + y') \\kappa_{ji}(g^{-1} \\cdot (u \\cdot y'))$$\n利用群性质 $(ab)\\cdot z = a \\cdot (b \\cdot z)$，我们得到 $g^{-1} \\cdot (u \\cdot y') = (g^{-1}u) \\cdot y'$。\n$$LHS = \\sum_{i=1}^{C_0} \\sum_{y' \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot x + y') \\kappa_{ji}((g^{-1}u) \\cdot y')$$\n\n现在我们计算右侧（RHS），即 $(\\Pi_u(\\mathcal{L}f))_j(x,g)$：\n$$(\\Pi_u(\\mathcal{L}f))_j(x,g) = (\\mathcal{L}f)_j(u^{-1} \\cdot x, u^{-1}g)$$\n使用我们为输入 $(u^{-1}\\cdot x, u^{-1}g)$ 定义的算子：\n$$RHS = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i((u^{-1} \\cdot x) + y) \\kappa_{ji}((u^{-1}g)^{-1} \\cdot y)$$\n$u^{-1}g$ 的逆是 $g^{-1}u$。\n$$RHS = \\sum_{i=1}^{C_0} \\sum_{y \\in \\mathbb{Z}^2} f_i(u^{-1} \\cdot x + y) \\kappa_{ji}((g^{-1}u) \\cdot y)$$\n比较 LHS（求和变量为 $y'$）和 RHS（求和变量为 $y$）的最终表达式，我们发现它们是相同的。因此，所定义的提升卷积确实相对于 $D_8$ 的作用是等变的。\n\n### 任务 3：方向通道的计算\n\n提升层接受一个输入特征图 $f: \\mathbb{Z}^2 \\to \\mathbb{R}^{C_0}$ 并产生一个输出特征图 $F: \\mathbb{Z}^2 \\times D_8 \\to \\mathbb{R}^{C_1}$。问题陈述该层的基通道数为 $C_0=12$。在神经网络层规范的背景下，这最合理地解释为输出基通道的数量，所以我们设置 $C_1=12$。\n\n输出特征图 $F$ 的值 $F(x,g)$ 对每个空间位置 $x \\in \\mathbb{Z}^2$ 和每个群元素 $g \\in D_8$ 都有定义。在一个固定的空间位置 $x$ 处，特征是一个从 $D_8$ 到 $\\mathbb{R}^{C_1}$ 的函数。这个函数可以看作是维数为 $C_1 \\times |D_8|$ 的空间中的单个向量。\n\n“方向通道”是由群 $D_8$ 的元素索引的通道。问题要求每个空间位置的*总*方向通道数。这是点 $x$ 处特征向量的全维度，不包括空间维度。\n\n群是 $D_8$，即阶数为 $16$ 的二面体群。所以 $|D_8| = 16$。\n输出基通道数被指定为 $C_1 = 12$。\n\n对于 $C_1$ 个基通道中的每一个，提升过程都会创建 $|D_8|$ 个特定方向的通道。因此，每个空间位置的总通道数是输出基通道数与群中元素个数的乘积。\n\n每个空间位置的总通道数 = $C_1 \\times |D_8|$。\n代入给定值：\n总通道数 = $12 \\times 16$。\n总通道数 = $192$。\n\n这意味着对于二维网格中的每个点，提升层输出一个包含 $192$ 个特征值的向量，其结构是在群 $D_8$ 上的 $12$ 个特征向量，每个向量的维度为 $16$。", "answer": "$$\\boxed{192}$$", "id": "3133440"}, {"introduction": "拥有了构建和推广群卷积的知识后，最后的关键一步是将其应用于实际的设计问题。本练习探讨了一个关于手性（chirality）分类的场景，其中旋转和反射对物体标签的影响是不同的。通过这个练习，你将学习如何精心设计网络的输出层，使其输出的表示（representation）能够精确匹配任务本身的对称性，从而确保模型的预测在输入变换时能够做出正确且一致的响应。[@problem_id:3133472]", "problem": "一位实践者设计了一个群卷积神经网络（G-CNN），用于处理具有手性（左手性与右手性）的图案的平面图像。其对称群是二面体群 $D_n$，由一个 $n$ 阶旋转 $r$ 和一个反射 $s$ 生成，满足关系 $r^n = e$，$s^2 = e$ 以及 $s r s = r^{-1}$。该网络由对 $D_n$ 等变的群卷积层构成，即对于任何在特征场 $x$ 上实现映射 $f$ 的层，以及任何 $g \\in D_n$，我们有 $f(T_g x) = \\rho(g) f(x)$，其中 $T_g$ 是 $g$ 对输入的动作，而 $\\rho(g)$ 是一个线性表示，它决定了该层输出的变换方式。\n\n任务是手性的二元分类。根据经验和对称性，旋转 $r^k$（$k \\in \\{0,1,\\dots,n-1\\}$）不改变手性，而反射 $s r^k$ 会反转手性。在测试时，输入可能相对于训练集被旋转或反射，分类器必须做出一致的响应：对于旋转，它应保持预测类别不变，而对于反射，它应交换左/右预测。\n\n从群等变性的定义 $f(T_g x) = \\rho(g) f(x)$ 和标签应根据与任务物理对称性（旋转保持手性；反射反转手性）一致的表示 $\\pi(g)$ 进行变换的观念出发，选择在分类头上能正确强制执行所需行为的设计。假设对类别概率使用标准的交叉熵训练，或在适用时对带符号目标使用均方误差，并且G-CNN的前期层是 $D_n$ 等变的。\n\n以下哪种输出头设计能正确地编码奇偶性行为，并在测试时产生所需的旋转不变性和反射引起的类别反转？\n\nA. 使用一个 $D_n$ 等变头，其在两个logit上的输出表示是平凡的，即对于所有 $g \\in D_n$，$\\rho(g) = I_2$，并对这两个logit应用softmax以获得类别概率。\n\nB. 使用一个 $D_n$ 等变头，其输出为二维，其中旋转的作用是平凡的，而反射会交换分量：$\\rho(r^k) = I_2$ 且 $\\rho(s r^k) = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$。对这两个分量应用softmax以获得左手性与右手性的类别概率。\n\nC. 舍弃反射，仅使用一个循环群 $C_n$ 等变头（仅考虑旋转）。使用数据增强进行训练，其中包括将反射样本重新标记为相反类别，依赖网络来学习这种翻转，而没有明确的表示层级约束。\n\nD. 使用一个 $D_n$ 等变头，其具有单个标量输出 $y \\in \\mathbb{R}$，训练用于回归一个带符号目标 $t \\in \\{-1,+1\\}$，且 $\\rho(r^k) = 1$ 和 $\\rho(s r^k) = -1$。在测试时，通过决策规则 $\\hat{c} = \\mathbf{1}[y > 0]$ 来预测类别。\n\n选择所有适用项。", "solution": "在给出解答之前，首先对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- **对称群**：由一个旋转 $r$ 和一个反射 $s$ 生成的二面体群 $D_n$。\n- **群关系**：$r^n = e$, $s^2 = e$, and $s r s = r^{-1}$。\n- **网络架构**：一个带有 $D_n$ 等变层的群卷积神经网络（G-CNN）。\n- **等变性定义**：对于作用于特征场 $x$ 的层映射 $f$，以及任何群元素 $g \\in D_n$，等变关系为 $f(T_g x) = \\rho(g) f(x)$。这里，$T_g$ 是 $g$ 在输入场上的作用，$\\rho(g)$ 是定义输出场变换方式的线性表示。\n- **任务**：手性的二元分类（左手性 vs 右手性）。\n- **手性的对称性**：\n    - 旋转（$g=r^k$，其中 $k \\in \\{0, 1, \\dots, n-1\\}$）保持手性。\n    - 反射（$g=s r^k$）反转手性。\n- **要求的分类器行为**：\n    - 在旋转下，预测的类别必须保持不变（不变性）。\n    - 在反射下，预测的类别必须反转（例如，左 $\\to$ 右）。\n- **假设**：G-CNN的前期层是 $D_n$ 等变的。训练使用标准方法，如交叉熵或均方误差。\n- **目标**：选择能够正确强制执行所需对称性行为的输出头设计。\n\n### 步骤2：使用提取的已知条件进行验证\n对问题陈述进行有效性评估。\n- **科学依据**：该问题基于成熟的群论数学框架及其通过群等变CNN在深度学习中的应用。二面体群 $D_n$ 的性质和手性的几何概念被正确陈述和使用。这是几何深度学习领域的一个标准问题。该问题在科学上是合理的。\n- **适定性**：问题定义清晰。它指明了群、任务、输出所需的对称性以及等变性的定义。问题要求选择满足这些约束的特定设计。可以使用群表示论的原理推导出解决方案。该问题是适定的。\n- **客观性**：问题使用了精确的技术语言（例如“$D_n$-等变”、“线性表示”），并且没有主观或模糊的术语。该问题是客观的。\n\n检查清单中未发现任何缺陷（例如，没有科学不合理性、不完整性或模糊性）。\n\n### 步骤3：结论与行动\n问题陈述是有效的。将推导完整的解决方案。\n\n### 基于原理的推导\n为特定任务设计等变网络的核心原则是，确保网络输出的表示 $\\rho(g)$ 与任务标签的表示（我们称之为 $\\pi(g)$）相匹配。网络头是一个映射 $f_{\\text{head}}$。如果其输入 $x_{\\text{penultimate}}$ 被 $T_g$ 变换，其输出 $y = f_{\\text{head}}(x_{\\text{penultimate}})$ 会变换为 $y \\mapsto \\rho(g) y$。那么最终的分类决策 $C(y)$ 必须展现出所需的对称性。\n\n让我们将标签所需的对称性 $\\pi(g)$ 形式化：\n$1$. **旋转**：旋转 $g = r^k$ 保持手性。这意味着分类决策必须是不变的。\n$2$. **反射**：反射 $g = sr^k$ 反转手性。这意味着分类决策必须被翻转。\n\n现在我们必须逐一检查每个提议的设计，看其输出表示 $\\rho(g)$ 和随后的决策规则是否能产生这种行为。\n\n### 逐项分析\n\n**A. 使用一个 $D_n$ 等变头，其在两个logit上的输出表示是平凡的，即对于所有 $g \\in D_n$，$\\rho(g) = I_2$，并对这两个logit应用softmax以获得类别概率。**\n\n- **分析**：该头的输出是一个二维logit向量，$y \\in \\mathbb{R}^2$。等变性属性意味着，对于一个输入图像 $x_{in}$，如果它被任何 $g \\in D_n$ 变换，输出将变为 $y(T_g x_{in}) = \\rho(g) y(x_{in})$。根据此选项，对于所有 $g$，$\\rho(g) = I_2$（$2 \\times 2$ 单位矩阵）。因此，$y(T_g x_{in}) = I_2 y(x_{in}) = y(x_{in})$。\n- 输出的logit在所有群作用（包括旋转和反射）下都是不变的。应用于不变向量的softmax函数将产生不变的概率。因此，最终的分类对于旋转和反射都将是不变的。虽然旋转不变性是期望的，但对反射的不变性与反射必须反转手性的要求相矛盾。该模型在架构上被约束为忽略反射，这对于此任务是不正确的。\n- **结论**：不正确。\n\n**B. 使用一个 $D_n$ 等变头，其输出为二维，其中旋转的作用是平凡的，而反射会交换分量：$\\rho(r^k) = I_2$ 且 $\\rho(s r^k) = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$。对这两个分量应用softmax以获得左手性与右手性的类别概率。**\n\n- **分析**：输出是一个二维logit向量 $y = [y_1, y_2]^T$。让我们检查其在两种变换类型下的行为。\n- **旋转 ($g=r^k$)**：输出变换为 $y(T_{r^k} x_{in}) = \\rho(r^k) y(x_{in}) = I_2 y(x_{in}) = y(x_{in})$。logit是不变的。随后的softmax概率和最终分类也是不变的。这符合对旋转的要求。\n- **反射 ($g=sr^k$)**：输出变换为 $y(T_{sr^k} x_{in}) = \\rho(sr^k) y(x_{in}) = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} y_2 \\\\ y_1 \\end{pmatrix}$。两个logit被交换。设类别概率为 $p_1 = e^{y_1} / (e^{y_1} + e^{y_2})$ 和 $p_2 = e^{y_2} / (e^{y_1} + e^{y_2})$。对于变换后的输入，新的logit是 $[y_2, y_1]^T$，导致新的概率 $p'_1 = e^{y_2} / (e^{y_2} + e^{y_1}) = p_2$ 和 $p'_2 = e^{y_1} / (e^{y_2} + e^{y_1}) = p_1$。两个类别的概率被交换，这正确地实现了手性的反转。\n- **表示的有效性**：所提出的映射 $\\rho$ 是 $D_n$ 的一个有效表示。它正确地将 $r$ 映射到单位矩阵，将 $s$ 映射到置换矩阵，满足群关系。\n- **结论**：正确。\n\n**C. 舍弃反射，仅使用一个循环群 $C_n$ 等变头（仅考虑旋转）。使用数据增强进行训练，其中包括将反射样本重新标记为相反类别，依赖网络来学习这种翻转，而没有明确的表示层级约束。**\n\n- **分析**：此选项建议在架构上强制执行旋转等变性，但将反射行为留给数据增强来学习。虽然数据增强是深度学习中一种强大且常用的技术，但它并不*强制*或*保证*精确的对称性。网络从有限的数据中学习反射规则的近似值，并且没有架构上的约束来确保 $f(T_s x)$ 以所需的方式与 $f(x)$ 完美相关。问题要求的是一种*正确强制执行所需行为*的设计。此选项用经验学习取代了架构强制，这是一种不同的设计哲学，并且不能提供与针对整个 $D_n$ 群的等变设计相同的保证。\n- **结论**：不正确。\n\n**D. 使用一个 $D_n$ 等变头，其具有单个标量输出 $y \\in \\mathbb{R}$，训练用于回归一个带符号目标 $t \\in \\{-1,+1\\}$，且 $\\rho(r^k) = 1$ 和 $\\rho(s r^k) = -1$。在测试时，通过决策规则 $\\hat{c} = \\mathbf{1}[y > 0]$ 来预测类别。**\n\n- **分析**：输出是一个单一标量 $y \\in \\mathbb{R}$。这对应于 $D_n$ 的一个一维表示。让我们检查其行为。\n- **旋转 ($g=r^k$)**：输出变换为 $y(T_{r^k} x_{in}) = \\rho(r^k) y(x_{in}) = (1) \\cdot y(x_{in}) = y(x_{in})$。标量输出是不变的。基于其符号的决策 $\\mathbf{1}[y > 0]$ 也是不变的。这符合对旋转的要求。\n- **反射 ($g=sr^k$)**：输出变换为 $y(T_{sr^k} x_{in}) = \\rho(sr^k) y(x_{in}) = (-1) \\cdot y(x_{in}) = -y(x_{in})$。标量输出被取反。如果原始预测是一个类别（例如，$y > 0$），新的输出 $-y$ 将是负的（$-y  0$），从而导致预测为另一个类别。如果原始预测是 $y  0$，新的输出 $-y$ 将是正的（$-y > 0$），同样翻转了类别。这正确地实现了手性的反转。\n- **表示的有效性**：映射 $\\rho(r^k)=1$ 和 $\\rho(sr^k)=-1$ 是 $D_n$ 众所周知的符号（或交错）表示。它是一个有效的一维不可约表示，正确地反映了群元素的奇偶性（对旋转是偶，对反射是奇）。\n- **结论**：正确。", "answer": "$$\\boxed{BD}$$", "id": "3133472"}]}