{"hands_on_practices": [{"introduction": "理论是基础，但真正的理解源于实践。本章提供了一系列动手实践，旨在将胶囊网络的核心概念转化为可执行的代码和可观察的结果。我们将从胶囊网络最根本的优势——等变性（equivariance）——开始。与传统卷积神经网络（CNN）追求的平移不变性（invariance）不同，等变性使得网络不仅能识别物体，还能理解其姿态信息。这个练习 [@problem_id:3104851] 将引导你通过一个简洁的数值实验，亲手实现并量化比较胶囊网络与CNN在面对旋转变换时的不同表现，从而深刻理解等变性的本质及其对于表征学习的价值。", "problem": "要求你实现一个独立的数值实验，在一个模拟姿态上仿射变换效果的合成设置中，对比 Capsule Networks (Capsule Networks (CapsNets)) 的等变性与 Convolutional Neural Networks (Convolutional Neural Networks (CNNs)) 的不变性。该实验必须完全在向量空间中进行，使用显式的矩阵和向量，不加载任何外部数据集。你将实现一个带有动态路由的单一高层胶囊和一个简单的类 CNN 基线。目标是在旋转变换下测量和比较姿态向量的对齐误差，并分析该误差如何依赖于动态路由的迭代次数。\n\n使用的基本基础和定义：\n- 二维旋转变换由矩阵表示\n$$\nT(\\theta) \\;=\\; \\begin{bmatrix}\n\\cos(\\theta) & -\\sin(\\theta)\\\\\n\\sin(\\theta) & \\cos(\\theta)\n\\end{bmatrix},\n$$\n它属于特殊正交群 $SO(2)$ 并且保持向量范数。\n- 胶囊间的动态路由过程使用通过对路由对数（routing logits）应用 softmax 计算出的耦合系数和“挤压”（squash）非线性函数。对于一组“预测”向量 $\\{ \\mathbf{u}_i \\in \\mathbb{R}^2 \\}_{i=1}^m$ 和路由对数 $\\{ b_i \\in \\mathbb{R} \\}_{i=1}^m$，耦合系数为\n$$\nc_i \\;=\\; \\frac{\\exp(b_i)}{\\sum_{k=1}^{m} \\exp(b_k)},\n$$\n预激活值为\n$$\n\\mathbf{s} \\;=\\; \\sum_{i=1}^{m} c_i \\, \\mathbf{u}_i.\n$$\n挤压非线性函数通过以下方式将 $\\mathbf{s}$ 映射到输出姿态向量 $\\mathbf{v} \\in \\mathbb{R}^2$\n$$\n\\mathbf{v} \\;=\\; \\frac{\\lVert \\mathbf{s} \\rVert^2}{1 + \\lVert \\mathbf{s} \\rVert^2} \\cdot \\frac{\\mathbf{s}}{\\lVert \\mathbf{s} \\rVert},\n$$\n约定当 $\\lVert \\mathbf{s} \\rVert = 0$ 时，$\\mathbf{v} = \\mathbf{0}$。\n- 路由对数通过每个预测与当前输出之间的点积（一致性）进行更新，更新次数为指定的迭代次数 $r \\in \\mathbb{N}_0$（非负整数）：\n$$\nb_i \\leftarrow b_i + \\mathbf{u}_i \\cdot \\mathbf{v}.\n$$\n- 表示 $\\mathbf{v}$ 相对于 $T(\\theta)$ 的等变性意味着，如果输入通过 $T(\\theta)$ 进行变换，那么输出姿态也通过同一个 $T(\\theta)$ 进行变换。用公式表示，如果 $\\mathbf{v}(\\theta)$ 是变换后输入的输出姿态，等变性意味着 $\\mathbf{v}(\\theta) \\approx T(\\theta)\\,\\mathbf{v}(0)$。\n- 我们测量两个非零向量 $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^2$ 之间的姿态向量对齐误差，定义为它们之间的夹角（以弧度为单位），\n$$\n\\operatorname{ang}(\\mathbf{a}, \\mathbf{b}) \\;=\\; \\arccos\\!\\left( \\frac{\\mathbf{a}^\\top \\mathbf{b}}{\\lVert \\mathbf{a} \\rVert \\,\\lVert \\mathbf{b} \\rVert} \\right).\n$$\n\n合成设置：\n- 设“真实”的规范姿态为 $\\mathbf{p}_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$。\n- 对于任意旋转角 $\\theta \\in \\mathbb{R}$（以弧度为单位），变换后的姿态为 $\\mathbf{p}(\\theta) = T(\\theta)\\,\\mathbf{p}_0$。\n- 有 $m = 5$ 个主胶囊。每个主胶囊产生一个二维的“投票”向量，它是对变换后姿态的一个经过缩放和轻微错位的观测：\n$$\n\\mathbf{u}_i(\\theta) \\;=\\; s_i \\, R(\\varphi_i)\\, \\mathbf{p}(\\theta),\n$$\n其中 $R(\\varphi)$ 是旋转矩阵 $T(\\varphi)$，常数为\n$$\n[s_1, s_2, s_3, s_4, s_5] \\;=\\; [1.2, 0.9, 1.1, 0.8, 1.0],\n$$\n$$\n[\\varphi_1, \\varphi_2, \\varphi_3, \\varphi_4, \\varphi_5] \\;=\\; [-0.2, 0.1, 0.0, 0.15, -0.1].\n$$\n- 高层胶囊使用单位变换（即，预测到输出的变换是 $\\mathbb{R}^2$ 中的单位矩阵），因此 $\\mathbf{u}_i(\\theta)$ 直接参与路由。\n- 类 Convolutional Neural Network (CNN) 基线通过仅聚合幅度来丢弃姿态方向。将其对角度 $\\theta$ 的表示定义为二维向量\n$$\n\\mathbf{v}_{\\mathrm{cnn}}(\\theta) \\;=\\; \\begin{bmatrix} g(\\theta) \\\\ 0 \\end{bmatrix}, \\quad \\text{其中} \\quad g(\\theta) \\;=\\; \\sum_{i=1}^{m} \\lVert \\mathbf{u}_i(\\theta) \\rVert.\n$$\n该基线在方向上有意对 $\\theta$ 保持不变，因为它沿固定的 x 轴对齐。\n\n你的任务：\n1. 为单个高层胶囊实现上述动态路由算法，该算法由路由迭代次数 $r \\in \\mathbb{N}_0$ 参数化。对所有 $i$ 使用初始路由对数 $b_i = 0$。在每次迭代中，通过对 $\\{b_i\\}$ 应用 softmax 来计算耦合系数 $c_i$，根据上述公式计算 $\\mathbf{s}$ 和 $\\mathbf{v}$，然后更新 $b_i \\leftarrow b_i + \\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}$。对于给定的 $\\theta$ 和 $r$，输出姿态 $\\mathbf{v}_{\\mathrm{caps}}(\\theta; r)$ 是在最后一次迭代中计算出的 $\\mathbf{v}$。对于边界情况 $r = 0$，将 $\\mathbf{v}_{\\mathrm{caps}}(\\theta; 0)$ 定义为均匀平均值的挤压结果，即使用 $c_i = 1/m$ 且不进行任何更新。\n2. 对于类 CNN 基线，按上述定义计算 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta)$。\n3. 对于一个输出为 $\\mathbf{v}(\\theta)$ 的给定模型，将角度 $\\theta$ 处的等变性对齐误差定义为\n$$\n\\varepsilon(\\theta) \\;=\\; \\operatorname{ang}\\!\\left( \\mathbf{v}(\\theta), \\; T(\\theta)\\,\\mathbf{v}(0) \\right),\n$$\n以弧度为单位测量。\n\n测试套件：\n在以下情况下评估误差对 $\\left[ \\varepsilon_{\\mathrm{caps}}(\\theta; r), \\; \\varepsilon_{\\mathrm{cnn}}(\\theta) \\right]$：\n- 情况 1：$\\theta = 0.0$, $r = 0$。\n- 情况 2：$\\theta = 0.5$, $r = 0$。\n- 情况 3：$\\theta = 0.5$, $r = 5$。\n- 情况 4：$\\theta = 1.0$, $r = 0$。\n- 情况 5：$\\theta = 1.0$, $r = 5$。\n- 情况 6：$\\theta = 1.0$, $r = 10$。\n\n角度单位：\n- 所有角度 $\\theta$、$\\varphi_i$ 和误差 $\\varepsilon$ 都必须以弧度为单位进行处理和报告。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含六种情况的结果，格式为一个由逗号分隔的包含六个内部列表的列表，每个内部列表包含两个浮点数，按上述顺序排列，并精确到小数点后六位。输出必须包含在一对单独的方括号中。例如，打印的结构必须类似于\n$$\n[ [a\\_1, b\\_1], [a\\_2, b\\_2], \\ldots, [a\\_6, b\\_6] ],\n$$\n但为了符合确切要求，打印时逗号后不带任何空格\n$$\n[[a\\_1,b\\_1],[a\\_2,b\\_2],\\ldots,[a\\_6,b\\_6]].\n$$", "solution": "该问题陈述经评估，具有科学依据、定义明确、客观且内部一致。它提供了一个清晰且可形式化的数值实验，用以对比 Capsule Networks (CapsNets) 和简化的 Convolutional Neural Network (CNN) 基线在仿射变换（特别是 $SO(2)$ 旋转）方面的属性。所有必要的常数、定义和流程都已明确提供。因此，我们可以着手提供一个完整的解决方案。\n\n这个问题的核心在于理解和量化等变性。如果对于一个变换群 $G$ 中的任意变换 $T \\in G$，将变换应用于输入然后再通过函数 $f$ 处理，与先将原始输入通过函数处理然后再将变换应用于输出所得到的结果相同，那么函数 $f$ 相对于该变换群 $G$ 是等变的。在此背景下，我们研究的是相对于旋转群 $SO(2)$ 的等变性，其条件为 $\\mathbf{v}(\\theta) = T(\\theta)\\mathbf{v}(0)$，其中 $\\mathbf{v}(\\theta)$ 是模型对于旋转了 $\\theta$ 角度的输入的输出。与此理想情况的偏差通过等变性对齐误差 $\\varepsilon(\\theta) = \\operatorname{ang}(\\mathbf{v}(\\theta), T(\\theta)\\mathbf{v}(0))$ 来衡量。\n\n我们首先将在问题的合成设置下分析 CapsNet 和 CNN 模型的理论行为。然后，我们将实现数值计算来验证这一分析。\n\n### 理论分析\n\n**1. 类 CNN 基线模型**\n\n类 CNN 基线模型计算一个表示 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [g(\\theta), 0]^\\top$，其中 $g(\\theta) = \\sum_{i=1}^{m} \\lVert \\mathbf{u}_i(\\theta) \\rVert$。预测向量定义为 $\\mathbf{u}_i(\\theta) = s_i R(\\varphi_i) \\mathbf{p}(\\theta)$，其中 $\\mathbf{p}(\\theta) = T(\\theta)\\mathbf{p}_0$。这里，$s_i$ 是标量常数，而 $R(\\varphi_i)$ 和 $T(\\theta)$ 是来自 $SO(2)$ 的旋转矩阵。由于旋转是正交变换，它们保持向量范数。\n预测向量的范数为：\n$$\n\\lVert \\mathbf{u}_i(\\theta) \\rVert = \\lVert s_i R(\\varphi_i) T(\\theta) \\mathbf{p}_0 \\rVert = |s_i| \\cdot \\lVert R(\\varphi_i) \\rVert \\cdot \\lVert T(\\theta) \\rVert \\cdot \\lVert \\mathbf{p}_0 \\rVert\n$$\n鉴于对于任何旋转矩阵 $A$，都有 $\\lVert A\\mathbf{x} \\rVert = \\lVert\\mathbf{x}\\rVert$，并且所有 $s_i > 0$，这可以简化为：\n$$\n\\lVert \\mathbf{u}_i(\\theta) \\rVert = s_i \\lVert \\mathbf{p}_0 \\rVert\n$$\n这个结果与旋转角 $\\theta$ 无关。因此，聚合的幅度 $g(\\theta)$ 也是一个与 $\\theta$ 无关的常数：\n$$\ng(\\theta) = \\sum_{i=1}^{m} s_i \\lVert \\mathbf{p}_0 \\rVert = \\left( \\sum_{i=1}^{m} s_i \\right) \\lVert \\mathbf{p}_0 \\rVert = G\n$$\n因此，对于任何 $\\theta$，CNN 的输出都是 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [G, 0]^\\top$。这个模型是完全不变的；其输出从不改变。\n\n为了计算等变性误差 $\\varepsilon_{\\mathrm{cnn}}(\\theta)$，我们需要 $\\mathbf{v}_{\\mathrm{cnn}}(0) = [G, 0]^\\top$。目标等变向量是 $T(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0)$：\n$$\nT(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0) = \\begin{bmatrix} \\cos(\\theta)  -\\sin(\\theta) \\\\ \\sin(\\theta)  \\cos(\\theta) \\end{bmatrix} \\begin{bmatrix} G \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} G\\cos(\\theta) \\\\ G\\sin(\\theta) \\end{bmatrix}\n$$\n误差是实际输出 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [G, 0]^\\top$ 与目标向量之间的夹角。\n$$\n\\varepsilon_{\\mathrm{cnn}}(\\theta) = \\operatorname{ang}\\left( \\begin{bmatrix} G \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} G\\cos(\\theta) \\\\ G\\sin(\\theta) \\end{bmatrix} \\right) = \\arccos\\left( \\frac{[G, 0] \\cdot [G\\cos(\\theta), G\\sin(\\theta)]}{G \\cdot G} \\right) = \\arccos(\\cos(\\theta))\n$$\n对于主值范围 $[0, \\pi]$ 内的 $\\theta$（所有测试用例都包含在此范围内），这可以简化为 $\\varepsilon_{\\mathrm{cnn}}(\\theta) = \\theta$。CNN 基线未能旋转导致了等于旋转角的误差，这表明它完全缺乏等变性。\n\n**2. Capsule Network 模型**\n\nCapsNet 模型的行为有根本性的不同。让我们分析预测向量 $\\mathbf{u}_i(\\theta)$：\n$$\n\\mathbf{u}_i(\\theta) = s_i R(\\varphi_i) \\mathbf{p}(\\theta) = s_i T(\\varphi_i) T(\\theta) \\mathbf{p}_0\n$$\n由于 $SO(2)$ 中的旋转是可交换的，所以 $T(\\varphi_i)T(\\theta) = T(\\theta)T(\\varphi_i)$。因此：\n$$\n\\mathbf{u}_i(\\theta) = s_i T(\\theta) T(\\varphi_i) \\mathbf{p}_0\n$$\n在 $\\theta=0$ 时的预测向量是 $\\mathbf{u}_i(0) = s_i T(\\varphi_i) \\mathbf{p}_0$。通过代入，我们发现一个直接的关系：\n$$\n\\mathbf{u}_i(\\theta) = T(\\theta) \\mathbf{u}_i(0)\n$$\n每个单独的预测向量都相对于全局旋转 $\\theta$ 是完美等变的。\n\n现在我们分析动态路由过程。我们将模型在角度 $\\theta$、经过 $k$ 次路由迭代后的状态（向量 $\\mathbf{s}, \\mathbf{v}$ 和标量 $b_i, c_i$）用上标 $(\\theta, k)$ 表示。\n\n**基本情况 ($r=0$)：**\n输出是使用均匀耦合系数 $c_i = 1/m$ 计算的。预激活向量 $\\mathbf{s}^{(\\theta, 0)}$ 是：\n$$\n\\mathbf{s}^{(\\theta, 0)} = \\sum_{i=1}^{m} \\frac{1}{m} \\mathbf{u}_i(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} T(\\theta) \\mathbf{u}_i(0) = T(\\theta) \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{u}_i(0) \\right) = T(\\theta) \\mathbf{s}^{(0, 0)}\n$$\n预激活向量 $\\mathbf{s}$ 是完美等变的。现在我们应用挤压函数。挤压函数的一个关键特性是它与旋转的相互作用。由于 $\\lVert T(\\theta)\\mathbf{s} \\rVert = \\lVert \\mathbf{s} \\rVert$，我们有：\n$$\n\\mathbf{v}^{(\\theta, 0)} = \\operatorname{squash}(\\mathbf{s}^{(\\theta, 0)}) = \\operatorname{squash}(T(\\theta)\\mathbf{s}^{(0, 0)}) = T(\\theta)\\operatorname{squash}(\\mathbf{s}^{(0, 0)}) = T(\\theta)\\mathbf{v}^{(0, 0)}\n$$\n这表明对于 $r=0$ 的情况，胶囊输出是完美等变的。误差 $\\varepsilon_{\\mathrm{caps}}(\\theta; 0)$ 必须为 $0$。\n\n**归纳步骤 ($r>0$)：**\n假设经过 $k-1$ 次迭代后，输出是完美等变的：$\\mathbf{v}^{(\\theta, k-1)} = T(\\theta)\\mathbf{v}^{(0, k-1)}$。路由对数按 $b_i \\leftarrow b_i + \\mathbf{u}_i \\cdot \\mathbf{v}$ 更新。我们来检查更新项：\n$$\n\\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}^{(\\theta, k-1)} = (T(\\theta)\\mathbf{u}_i(0)) \\cdot (T(\\theta)\\mathbf{v}^{(0, k-1)})\n$$\n点积在旋转下是不变的，即 $(Ra)\\cdot(Rb) = a \\cdot b$。因此：\n$$\n\\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}^{(\\theta, k-1)} = \\mathbf{u}_i(0) \\cdot \\mathbf{v}^{(0, k-1)}\n$$\n路由对数的更新量与 $\\theta$ 无关。通过归纳法，如果初始路由对数对于所有 $\\theta$ 都相同（确实如此，$b_i=0$），那么在任何迭代次数 $k$ 的路由对数 $b_i^{(\\theta, k)}$ 都与 $\\theta$ 无关：$b_i^{(\\theta, k)} = b_i^{(0, k)}$。\n这意味着耦合系数 $c_i^{(\\theta, k)}$ 也与 $\\theta$ 无关。\n下一次迭代的预激活值 $\\mathbf{s}^{(\\theta, k)}$ 是：\n$$\n\\mathbf{s}^{(\\theta, k)} = \\sum_{i=1}^{m} c_i^{(\\theta, k)} \\mathbf{u}_i(\\theta) = \\sum_{i=1}^{m} c_i^{(0, k)} T(\\theta)\\mathbf{u}_i(0) = T(\\theta) \\sum_{i=1}^{m} c_i^{(0, k)} \\mathbf{u}_i(0) = T(\\theta)\\mathbf{s}^{(0, k)}\n$$\n遵循与基本情况相同的逻辑，应用挤压函数会得到 $\\mathbf{v}^{(\\theta, k)} = T(\\theta)\\mathbf{v}^{(0, k)}$。\n归纳成立：在这个理想化的设置中，对于任意数量的路由迭代 $r$，胶囊网络的输出都是完美等变的。对于所有测试用例，误差 $\\varepsilon_{\\mathrm{caps}}(\\theta; r)$ 理论上都为 $0$。对其与 $r$ 依赖关系的分析表明，不存在依赖关系；该系统从一开始就是完美等变的。\n\n### 数值实现\n\n我们现在将按照描述实现算法，以确认我们的理论分析。实现将遵循为两种模型指定的流程，并为给定的测试套件计算误差。我们期望 CapsNet 的误差为 $0$（或一个在机器精度数量级的值），而 CNN 的误差为 $\\theta$。\n\n每个测试用例 $(\\theta, r)$ 的总体流程将是：\n1.  计算 $\\mathbf{v}_{\\mathrm{caps}}(\\theta; r)$ 和 $\\mathbf{v}_{\\mathrm{cnn}}(\\theta)$。\n2.  计算参考向量 $\\mathbf{v}_{\\mathrm{caps}}(0; r)$ 和 $\\mathbf{v}_{\\mathrm{cnn}}(0)$。\n3.  计算目标向量 $T(\\theta)\\mathbf{v}_{\\mathrm{caps}}(0; r)$ 和 $T(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0)$。\n4.  计算实际向量与目标向量之间的夹角，以求得 $\\varepsilon_{\\mathrm{caps}}$ 和 $\\varepsilon_{\\mathrm{cnn}}$。\n\n所有计算都将使用 Python 的 `numpy` 库，通过浮点算术来执行。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to contrast Capsule Network equivariance\n    with CNN invariance under 2D rotation.\n    \"\"\"\n\n    # --- Problem Constants ---\n    P0 = np.array([2.0, 1.0])\n    S_VALS = np.array([1.2, 0.9, 1.1, 0.8, 1.0])\n    PHI_VALS = np.array([-0.2, 0.1, 0.0, 0.15, -0.1])\n    M = 5\n\n    # --- Helper Functions ---\n    def rotation_matrix(theta: float) -> np.ndarray:\n        \"\"\"Computes the 2D rotation matrix T(theta).\"\"\"\n        c, s = np.cos(theta), np.sin(theta)\n        return np.array([[c, -s], [s, c]])\n\n    def squash(s: np.ndarray, epsilon: float = 1e-9) -> np.ndarray:\n        \"\"\"Applies the squashing non-linearity to a vector s.\"\"\"\n        s_norm_sq = np.sum(s**2)\n        s_norm = np.sqrt(s_norm_sq)\n        if s_norm < epsilon:\n            return np.zeros_like(s)\n        scale = s_norm_sq / (1.0 + s_norm_sq)\n        return scale * s / s_norm\n\n    def get_votes(theta: float) -> np.ndarray:\n        \"\"\"Generates the set of prediction vectors u_i(theta).\"\"\"\n        p_theta = rotation_matrix(theta) @ P0\n        votes = np.zeros((M, 2))\n        for i in range(M):\n            # u_i(theta) = s_i * R(phi_i) * p(theta)\n            votes[i, :] = S_VALS[i] * (rotation_matrix(PHI_VALS[i]) @ p_theta)\n        return votes\n\n    def angle_between(v1: np.ndarray, v2: np.ndarray, epsilon: float = 1e-9) -> float:\n        \"\"\"Calculates the angle in radians between two 2D vectors.\"\"\"\n        norm1 = np.linalg.norm(v1)\n        norm2 = np.linalg.norm(v2)\n        if norm1 < epsilon or norm2 < epsilon:\n            return 0.0\n        \n        dot_product = np.dot(v1, v2)\n        cos_angle = dot_product / (norm1 * norm2)\n        # Clip to handle potential floating point inaccuracies\n        cos_angle = np.clip(cos_angle, -1.0, 1.0)\n        return np.arccos(cos_angle)\n\n    # --- Model Implementations ---\n    def capsule_model(theta: float, r: int) -> np.ndarray:\n        \"\"\"\n        Computes the output pose vector v_caps(theta; r) using dynamic routing.\n        \"\"\"\n        votes = get_votes(theta)\n        \n        if r == 0:\n            # Uniform averaging, no routing iterations\n            s = np.mean(votes, axis=0)\n            return squash(s)\n\n        # Dynamic routing for r > 0\n        b = np.zeros(M)\n        v = np.zeros(2) # v is initialized implicitly in loop\n        for _ in range(r):\n            # Softmax to get coupling coefficients\n            c = np.exp(b) / np.sum(np.exp(b))\n            # Weighted sum of votes\n            s = np.sum(c[:, np.newaxis] * votes, axis=0)\n            # Squash to get output pose vector\n            v = squash(s)\n            # Update logits by agreement\n            agreement = np.dot(votes, v) # shape (M,)\n            b += agreement\n            \n        return v\n\n    def cnn_model(theta: float) -> np.ndarray:\n        \"\"\"Computes the output representation v_cnn(theta).\"\"\"\n        votes = get_votes(theta)\n        g_theta = np.sum(np.linalg.norm(votes, axis=1))\n        return np.array([g_theta, 0.0])\n\n    # --- Test Suite ---\n    test_cases = [\n        # (theta, r)\n        (0.0, 0),  # Case 1\n        (0.5, 0),  # Case 2\n        (0.5, 5),  # Case 3\n        (1.0, 0),  # Case 4\n        (1.0, 5),  # Case 5\n        (1.0, 10), # Case 6\n    ]\n\n    results = []\n    for theta, r in test_cases:\n        # 1. Calculate CapsNet error\n        v_caps_theta = capsule_model(theta, r)\n        v_caps_0 = capsule_model(0.0, r)\n        target_caps = rotation_matrix(theta) @ v_caps_0\n        eps_caps = angle_between(v_caps_theta, target_caps)\n\n        # 2. Calculate CNN error\n        v_cnn_theta = cnn_model(theta)\n        v_cnn_0 = cnn_model(0.0)\n        target_cnn = rotation_matrix(theta) @ v_cnn_0\n        eps_cnn = angle_between(v_cnn_theta, target_cnn)\n\n        results.append(f\"[{eps_caps:.6f},{eps_cnn:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3104851"}, {"introduction": "理解了胶囊网络“是什么”之后，我们需要深入探究其核心机制“如何工作”。动态路由是实现等变性的关键，但这个“协议”并非完美无缺。在某些情况下，例如当场景中存在高度对称性时，路由算法可能会陷入“困惑”，无法明确地将“部分”分配给正确的“整体”。这个练习 [@problem_id:3104796] 精心设计了一个对称性模糊（symmetric ambiguity）的场景，让你亲眼见证标准动态路由的失效，并通过实现两种不同的“先验知识”来打破僵局，从而加深对路由算法内在逻辑和潜在局限性的理解。", "problem": "考虑一个简化的基于向量的胶囊网络（CapsNet）路由场景，该场景有两层：一组由 $i \\in \\{0,1\\}$ 索引的初级胶囊（部件）和一组由 $j \\in \\{0,1\\}$ 索引的高级胶囊（整体）。每个初级胶囊 $i$ 为每个高级胶囊 $j$ 发出一个预测向量 $u_{\\hat{i}j} \\in \\mathbb{R}^d$。姿态维度为 $d = 2$。要实现的路由算法是由以下基本组件定义的一致性路由机制。\n\n- 设 $b_{ij}$ 为路由对数（logits）。根据您包含的任何先验，按需初始化 $b_{ij}$。在每次路由迭代中，通过对每个固定的 $i$ 在所有 $j$ 上应用 softmax 来计算耦合系数 $c_{ij}$，即 $c_{ij} = \\exp(b_{ij}) \\big/ \\sum_{j'} \\exp(b_{ij'})$。\n- 计算每个高级胶囊 $j$ 的预激活值 $s_j = \\sum_i c_{ij} u_{\\hat{i}j}$。\n- 应用标准的向量胶囊 squash 非线性函数，通过以下公式生成 $v_j \\in \\mathbb{R}^d$：\n$$\nv_j = \\frac{\\lVert s_j \\rVert^2}{1 + \\lVert s_j \\rVert^2} \\cdot \\frac{s_j}{\\lVert s_j \\rVert},\n$$\n需要理解的是，一个数值稳定的实现必须处理 $\\lVert s_j \\rVert = 0$ 的情况。\n- 通过一致性更新路由对数：$b_{ij} \\leftarrow b_{ij} + u_{\\hat{i}j} \\cdot v_j$，其中 $\\cdot$ 表示欧几里得内积。\n\n您将研究路由中的对称模糊性，其中由于对称性，部件可能被分配给错误的整体，然后添加对称性破除先验以改进路由。\n\n您的任务是实现上述路由算法，固定迭代次数为 $r = 3$，并评估导向基准整体的总耦合质量。对于给定的基准索引 $j^\\star \\in \\{0,1\\}$，定义评估指标\n$$\n\\mathcal{S} = \\sum_{i \\in \\{0,1\\}} c_{i j^\\star}.\n$$\n\n您必须实现两种对称性破除先验：\n\n- 父级对数偏置先验：一个父级偏置向量 $\\beta = (\\beta_0, \\beta_1)$，它被相同地加到两个部件 $i$ 的初始对数 $b_{ij}$ 上（即，在第一次 softmax 之前，$b_{ij} \\leftarrow b_{ij} + \\beta_j$）。\n- 投票扰动先验：一个小的向量 $\\Delta = (\\delta_x, \\delta_y)$，它扰动预测向量，使得朝向基准整体的预测被移动为 $u_{\\hat{i} j^\\star} \\leftarrow u_{\\hat{i} j^\\star} + \\Delta$，而朝向其他整体的预测则被相反地移动为 $u_{\\hat{i} j} \\leftarrow u_{\\hat{i} j} - \\Delta$（对于 $j \\neq j^\\star$）。\n\n为以下每个测试用例实现路由并评估指标 $\\mathcal{S}$。在所有情况下，使用 $n = 2$ 个部件，$m = 2$ 个整体，$d = 2$，以及 $r = 3$ 次迭代。基准整体为 $j^\\star = 0$。\n\n- 测试用例 A（完全对称，无先验）：\n  - 预测：$u_{\\hat{00}} = (1, 0)$，$u_{\\hat{10}} = (1, 0)$，$u_{\\hat{01}} = (0, 1)$，$u_{\\hat{11}} = (0, 1)$。\n  - 先验：$\\beta = (0, 0)$，$\\Delta = (0, 0)$。\n\n- 测试用例 B（完全对称，对数偏置先验）：\n  - 预测：同测试用例 A。\n  - 先验：$\\beta = (0.05, -0.05)$，$\\Delta = (0, 0)$。\n\n- 测试用例 C（冲突投票，无先验）：\n  - 预测：$u_{\\hat{00}} = (1, 0)$，$u_{\\hat{10}} = (0, 1)$，$u_{\\hat{01}} = (0, 1)$，$u_{\\hat{11}} = (1, 0)$。\n  - 先验：$\\beta = (0, 0)$，$\\Delta = (0, 0)$。\n\n- 测试用例 D（冲突投票，对数偏置先验）：\n  - 预测：同测试用例 C。\n  - 先验：$\\beta = (0.2, -0.2)$，$\\Delta = (0, 0)$。\n\n- 测试用例 E（完全对称，投票扰动先验）：\n  - 预测：同测试用例 A。\n  - 先验：$\\beta = (0, 0)$，$\\Delta = (0.05, 0.00)$。\n\n您的程序必须按 A、B、C、D、E 的顺序计算每个测试用例的指标 $\\mathcal{S}$，每个都是一个实数。您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个实数都四舍五入到恰好 $6$ 位小数（例如，$[1.000000,1.937500,1.000000,1.750000,1.875000]$）。不应产生其他输出。", "solution": "用户提供了一个问题，要求实现和评估一个简化的动态路由算法，这是胶囊网络（CapsNets）的核心组成部分。任务是在五个旨在测试算法在存在对称性时的行为以及对称性破除先验效果的不同场景下，计算一个特定的指标 $\\mathcal{S}$。\n\n首先，确认问题陈述的有效性。该问题在科学上基于深度学习的原理，特别是原始 CapsNet 文献中详细介绍的一致性路由机制。所有提供的参数、方程和条件都是自洽的、数学上一致且定义明确的，构成了一个适定问题。没有事实错误、歧义或主观因素。\n\n解决方案涉及按规定实现迭代路由算法。系统的状态存储在几个矩阵中。从初级胶囊 $i$ 到高级胶囊 $j$ 的预测向量由 $u_{\\hat{i}j}$ 给出。这些存储在一个大小为 $n \\times m \\times d$ 的张量中，其中 $n=2$ 是初级胶囊的数量，$m=2$ 是高级胶囊的数量，$d=2$ 是向量维度。动态调整以控制路由的路由对数 $b_{ij}$ 存储在一个 $n \\times m$ 矩阵中。\n\n算法流程如下：\n\n1.  **初始化**：\n    路由对数 $b_{ij}$ 初始化为 $0$。问题定义了两种修改此初始状态的先验。\n    - **投票扰动先验**：在路由过程开始之前，扰动预测向量 $u_{\\hat{i}j}$。对于基准胶囊 $j^\\star=0$，预测向量通过一个小的向量 $\\Delta$ 进行偏移，即 $u_{\\hat{i}0} \\leftarrow u_{\\hat{i}0} + \\Delta$。对于另一个胶囊（$j=1$），它们通过 $-\\Delta$ 进行偏移，即 $u_{\\hat{i}1} \\leftarrow u_{\\hat{i}1} - \\Delta$。此操作应用于所有初级胶囊 $i \\in \\{0,1\\}$。\n    - **父级对数偏置先验**：将一个偏置向量 $\\beta = (\\beta_0, \\beta_1)$ 添加到初始对数中。对于每个初级胶囊 $i$，其到高级胶囊 $j$ 的对数更新为 $b_{ij} \\leftarrow b_{ij} + \\beta_j$。此操作在路由循环的第一次迭代之前执行。\n\n2.  **迭代路由**：算法的核心是一个循环，运行固定的迭代次数 $r = 3$。每次迭代包括四个步骤：\n    a.  **耦合系数 ($c_{ij}$)**：使用 softmax 函数将路由对数 $b_{ij}$ 转换为耦合系数 $c_{ij}$。Softmax 函数应用于每个初级胶囊 $i$ 上的所有高级胶囊 $j$：\n        $$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{j' \\in \\{0,1\\}} \\exp(b_{ij'})}$$\n        值 $c_{ij}$ 表示初级胶囊 $i$ 应被路由到高级胶囊 $j$ 的概率。\n\n    b.  **预激活值 ($s_j$)**：每个高级胶囊 $j$ 的预激活向量 $s_j$ 计算为来自所有初级胶囊的预测向量的加权和。权重是耦合系数 $c_{ij}$：\n        $$s_j = \\sum_{i \\in \\{0,1\\}} c_{ij} u_{\\hat{i}j}$$\n\n    c.  **激活值 ($v_j$)**：预激活向量 $s_j$ 通过一个非线性的“squash”函数，以产生高级胶囊的最终输出向量 $v_j$。Squash 函数将其向量的模长缩放到 $0$ 和 $1$ 之间，同时保持其方向。\n        $$v_j = \\frac{\\lVert s_j \\rVert^2}{1 + \\lVert s_j \\rVert^2} \\frac{s_j}{\\lVert s_j \\rVert}$$\n        一个数值稳定的实现至关重要，特别是对于 $\\lVert s_j \\rVert = 0$ 的情况。在这种情况下，$v_j$ 必须是零向量。这可以通过在分母项 $\\lVert s_j \\rVert$ 中添加一个小的 epsilon 来处理，然后再进行除法。\n\n    d.  **对数更新 ($b_{ij}$)**：路由对数根据预测向量 $u_{\\hat{i}j}$ 与最终的高级胶囊激活值 $v_j$ 之间的一致性进行更新。一致性由欧几里得内积（点积）度量。\n        $$b_{ij} \\leftarrow b_{ij} + u_{\\hat{i}j} \\cdot v_j$$\n        此更新实现了“一致性路由”原则：如果一个初级胶囊的预测与一个高级胶囊的聚合输出很好地对齐，则连接它们的对数会增加，从而在下一次迭代中加强该路由路径。\n\n3.  **最终评估**：\n    经过 $r=3$ 次迭代后，获得最终的路由对数 $b_{ij}$。从这些对数中，使用与步骤 2a 中相同的 softmax 函数计算最后一组耦合系数 $c_{ij}$。评估指标 $\\mathcal{S}$ 是从所有初级胶囊导向基准胶囊 $j^\\star=0$ 的耦合系数之和：\n    $$\\mathcal{S} = \\sum_{i \\in \\{0,1\\}} c_{i, j^\\star} = c_{00} + c_{10}$$\n    该指标量化了分配给正确整体的总“路由质量”，最大可能值为 $2$（如果两个初级胶囊都完全路由到 $j^\\star=0$），最小值为 $0$。值为 $1$ 表示平均而言，路由是完全模糊的。\n\n该实现将通过设置适当的预测向量和先验参数来处理五个测试用例中的每一个，并执行所述算法以计算指标 $\\mathcal{S}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef squash(s, epsilon=1e-9):\n    \"\"\"\n    Applies the squash nonlinearity to a set of vectors.\n    s: A NumPy array of shape (m, d) representing pre-activation vectors.\n    Returns: A NumPy array of shape (m, d) representing activated vectors.\n    \"\"\"\n    # s_norm_sq has shape (m, 1) to enable broadcasting\n    s_norm_sq = np.sum(s**2, axis=-1, keepdims=True)\n    # scale factor also has shape (m, 1)\n    scale = s_norm_sq / (1.0 + s_norm_sq)\n    # s_norm has shape (m, 1)\n    s_norm = np.sqrt(s_norm_sq)\n    # unit_s is s normalized, with protection against division by zero\n    unit_s = s / (s_norm + epsilon)\n    # The result v is the scaled unit vector\n    v = scale * unit_s\n    return v\n\ndef run_routing(u_in, beta, delta, j_star, r, n, m, d):\n    \"\"\"\n    Executes the dynamic routing algorithm for a given configuration.\n    \n    Args:\n        u_in (np.ndarray): Prediction vectors, shape (n, m, d).\n        beta (np.ndarray): Parent logit bias prior, shape (m,).\n        delta (np.ndarray): Vote perturbation prior, shape (d,).\n        j_star (int): Ground-truth whole index.\n        r (int): Number of routing iterations.\n        n (int): Number of primary capsules.\n        m (int): Number of higher-level capsules.\n        d (int): Dimensionality of pose vectors.\n\n    Returns:\n        float: The evaluation metric S.\n    \"\"\"\n    # Make a copy to avoid modifying the global test case data\n    u = u_in.copy()\n    \n    # 1. Apply vote perturbation prior\n    if np.any(delta):\n        other_j = 1 - j_star\n        u[:, j_star, :] += delta\n        u[:, other_j, :] -= delta\n\n    # 2. Initialize routing logits b_ij\n    b = np.zeros((n, m))\n    \n    # 3. Apply parent logit bias prior\n    if np.any(beta):\n        b += beta # Broadcasting adds beta to each row of b\n\n    # 4. Routing loop for r iterations\n    for _ in range(r):\n        # a. Compute coupling coefficients c_ij by softmax over logits\n        c = softmax(b, axis=1)\n        \n        # b. Compute pre-activations s_j as a weighted sum of predictions\n        s = np.einsum('ij,ijd->jd', c, u)\n        \n        # c. Compute activations v_j using the squash function\n        v = squash(s)\n        \n        # d. Update logits b_ij based on agreement (dot product)\n        agreement = np.einsum('ijd,jd->ij', u, v)\n        b += agreement\n        \n    # 5. Compute final coupling coefficients after all iterations\n    final_c = softmax(b, axis=1)\n    \n    # 6. Compute the metric S: sum of coupling mass to the ground-truth whole\n    metric_s = np.sum(final_c[:, j_star])\n    \n    return metric_s\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the routing algorithm for each, printing the results.\n    \"\"\"\n    # Define constants for all test cases\n    n, m, d = 2, 2, 2\n    r = 3\n    j_star = 0\n    \n    # Define prediction vectors for the test cases\n    u_A = np.array([[[1.0, 0.0], [0.0, 1.0]], \n                    [[1.0, 0.0], [0.0, 1.0]]])\n    \n    u_C = np.array([[[1.0, 0.0], [0.0, 1.0]], \n                    [[0.0, 1.0], [1.0, 0.0]]])\n\n    # Define the 5 test cases from the problem statement\n    test_cases = [\n        # Case A: Perfect symmetry, no prior\n        {'u': u_A, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.0, 0.0])},\n        # Case B: Perfect symmetry, logit bias prior\n        {'u': u_A, 'beta': np.array([0.05, -0.05]), 'delta': np.array([0.0, 0.0])},\n        # Case C: Conflicting votes, no prior\n        {'u': u_C, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.0, 0.0])},\n        # Case D: Conflicting votes, logit bias prior\n        {'u': u_C, 'beta': np.array([0.2, -0.2]), 'delta': np.array([0.0, 0.0])},\n        # Case E: Perfect symmetry, vote perturbation prior\n        {'u': u_A, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.05, 0.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        s_metric = run_routing(\n            u_in=case['u'],\n            beta=case['beta'],\n            delta=case['delta'],\n            j_star=j_star,\n            r=r, n=n, m=m, d=d\n        )\n        results.append(s_metric)\n\n    # Format the output as a comma-separated list with 6 decimal places, enclosed in brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3104796"}, {"introduction": "动态路由作为一个迭代过程，其行为动态本身也值得探究和优化。在路由的早期阶段，我们希望网络能“探索”所有可能的父子胶囊配对；而在后期，则需要“收敛”到一个确定的、高置信度的路由决策。这种在探索（exploration）与利用（exploitation）之间的权衡，是许多优化算法面临的核心挑战。这个练习 [@problem_id:3104863] 引入了温度退火（temperature annealing）的概念，通过在softmax函数中加入一个温度参数 $\\tau_t$ 来控制路由的确定性。你将通过实验不同的温度衰减策略，学习如何引导路由过程，以避免过早陷入局部最优，同时确保最终能做出精确的判断。", "problem": "考虑具有动态路由的胶囊网络 (CapsNets)。在路由迭代 $t \\in \\{0,1,\\dots,T-1\\}$ 中，对于每个底层胶囊索引 $i \\in \\{0,\\dots,I-1\\}$ 和每个父胶囊索引 $j \\in \\{0,\\dots,J-1\\}$，定义路由对数（logits）$b_{ij}^{(t)} \\in \\mathbb{R}$。耦合系数 $c_{ij}^{(t)}$ 通过对父胶囊应用温度缩放的 softmax 函数来计算：\n$$\nc_{ij}^{(t)} \\triangleq \\frac{\\exp\\left(b_{ij}^{(t)}/\\tau_t\\right)}{\\sum_{k=0}^{J-1}\\exp\\left(b_{ik}^{(t)}/\\tau_t\\right)},\n$$\n其中 $\\tau_t > 0$ 是在迭代 $t$ 时的温度。父胶囊的输出使用 squash 非线性函数。设预测投票 $\\mathbf{\\hat{u}}_{j|i} \\in \\mathbb{R}^d$ 是固定的。在迭代 $t$ 时，父胶囊的预激活值为\n$$\n\\mathbf{s}_j^{(t)} \\triangleq \\sum_{i=0}^{I-1} c_{ij}^{(t)} \\,\\mathbf{\\hat{u}}_{j|i},\n$$\n父胶囊的输出是经过 squash 函数处理的向量\n$$\n\\mathbf{v}_j^{(t)} \\triangleq \\mathrm{squash}\\!\\left(\\mathbf{s}_j^{(t)}\\right) = \\frac{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2}{1 + \\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2} \\cdot \\frac{\\mathbf{s}_j^{(t)}}{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert},\n$$\n约定当 $\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert = 0$ 时，结果为零向量。一致性（agreement）更新路由对数如下\n$$\nb_{ij}^{(t+1)} \\triangleq b_{ij}^{(t)} + \\mathbf{\\hat{u}}_{j|i} \\cdot \\mathbf{v}_j^{(t)}.\n$$\n将每个底层胶囊 $i$ 在迭代 $t$ 时的耦合分布的香农熵定义为\n$$\nH_i^{(t)} \\triangleq -\\sum_{j=0}^{J-1} c_{ij}^{(t)} \\log c_{ij}^{(t)}.\n$$\n您必须研究温度退火方案 $\\tau_t$，该方案需能防止早期坍塌（过早地将所有耦合质量分配给单个父胶囊），同时确保最终的选择性（在最后一次迭代时进行置信分配）。\n\n从上述基本定义出发，设计一个程序，该程序：\n- 精确地实现所述的温度缩放 softmax 路由、squash 非线性函数和一致性更新。\n- 在一个固定的小型场景下模拟路由，其中有 $I=3$ 个底层胶囊，$J=2$ 个父胶囊，向量维度 $d=2$，路由迭代次数 $T=6$，并且所有 $i,j$ 的初始对数 $b_{ij}^{(0)} = 0$。\n- 使用以下固定的预测投票（每个条目是一个二维向量）：\n  - 对于底层胶囊 $i=0$：$\\mathbf{\\hat{u}}_{0|0} = [1.2, 0.0]$, $\\mathbf{\\hat{u}}_{1|0} = [0.0, 1.1]$。\n  - 对于底层胶囊 $i=1$：$\\mathbf{\\hat{u}}_{0|1} = [1.0, 0.1]$, $\\mathbf{\\hat{u}}_{1|1} = [0.1, 0.9]$。\n  - 对于底层胶囊 $i=2$：$\\mathbf{\\hat{u}}_{0|2} = [0.0, 1.0]$, $\\mathbf{\\hat{u}}_{1|2} = [1.0, 0.0]$。\n\n您的程序必须评估五个候选的温度调度方案，每个方案都由参数指定，但在本问题陈述中未规定任何明确的公式。对于每个方案，您的程序必须判断是否同时满足以下两个约束条件：\n- 早期防坍塌：在迭代 $t=0$ 和 $t=1$ 时，底层胶囊的平均熵至少为 $H_{\\mathrm{early,min}} = 0.55$。\n- 最终选择性：在迭代 $t=T-1$ 时，底层胶囊的平均熵至多为 $H_{\\mathrm{final,max}} = 0.20$。\n\n待评估的五个方案及其参数如下：\n- 情况 A（快速衰减、非增）：初始温度 $3.0$，衰减因子参数 $0.5$。\n- 情况 B（始终为高）：恒定温度 $3.0$。\n- 情况 C（始终为低）：恒定温度 $0.25$。\n- 情况 D（在 $T$ 次迭代中带下限的平滑衰减）：下界 $0.2$，上界 $3.0$。\n- 情况 E（熵触发的两阶段方案）：高阶段温度 $3.0$，低阶段温度 $0.2$，平均熵的切换阈值 $0.60$，从迭代 $3$ 开始强制使用低温度。\n\n您的程序必须实现这些方案，为每个方案运行路由模拟，并为每种情况生成一个布尔值，指示是否同时满足两个约束。最终输出格式必须是单行，包含情况 A 到 E 的五个布尔值，形式为用方括号括起来的逗号分隔列表，例如 $[\\mathrm{True},\\mathrm{False},\\dots]$。\n\n您的程序必须是自包含的，不需要任何输入，并使用指定的场景和参数。不涉及物理单位或角度。测试套件是上述五个方案，它们涵盖了一般情况、边界条件和边缘情况。答案是布尔值。", "solution": "问题陈述已经过严格审查，并被确定为有效。它在科学上基于胶囊网络的原理，问题定义良好，具有一套完整且一致的定义和参数，并提出了一个可验证的、非平凡的计算任务。对于某些温度方案定义中存在的轻微模糊性，通过选择与所提供描述一致的标准函数形式来解决，这是形式化算法问题中的常见做法。\n\n任务是针对一个特定的小规模配置，模拟胶囊网络中的动态路由过程，并根据两个性能约束评估五种不同的温度退火方案：防止过早收敛（早期防坍塌）和确保最终的决策性（最终选择性）。\n\n模拟的核心是一个在 $t \\in \\{0, 1, \\dots, T-1\\}$（其中 $T=6$）上的迭代过程。每次迭代中系统的状态由路由对数 $b_{ij}^{(t)}$ 定义。我们从对所有 $i \\in \\{0, \\dots, 2\\}$ 和 $j \\in \\{0, \\dots, 1\\}$ 设置 $b_{ij}^{(0)} = 0$ 开始。\n\n**步骤 1：耦合系数计算**\n在每次迭代 $t$ 中，耦合系数 $c_{ij}^{(t)}$ 是通过使用温度缩放的 softmax 函数从对数 $b_{ij}^{(t)}$ 计算得出的。对于每个底层胶囊 $i$，这些系数构成了父胶囊 $j$ 上的一个概率分布：\n$$c_{ij}^{(t)} = \\frac{\\exp\\left(b_{ij}^{(t)}/\\tau_t\\right)}{\\sum_{k=0}^{J-1}\\exp\\left(b_{ik}^{(t)}/\\tau_t\\right)}$$\n这里，$\\tau_t > 0$ 是迭代 $t$ 时的温度。高温会使分布扁平化，鼓励探索；而低温会使其尖锐化，鼓励利用。为了数值稳定性，一种常用技术是在进行指数运算前，从所有对数中减去最大对数值，这不会改变 softmax 的结果。\n\n**步骤 2：熵计算**\n计算每个胶囊 $i$ 的耦合分布的香农熵，以衡量路由的确定性程度：\n$$H_i^{(t)} = -\\sum_{j=0}^{J-1} c_{ij}^{(t)} \\log c_{ij}^{(t)}$$\n按照标准做法，使用自然对数。所有 $I$ 个底层胶囊的平均熵 $\\bar{H}^{(t)} = \\frac{1}{I}\\sum_{i=0}^{I-1}H_i^{(t)}$ 是我们评估约束的主要指标。\n\n**步骤 3：父胶囊激活**\n每个父胶囊 $j$ 的预激活向量 $\\mathbf{s}_j^{(t)}$ 是来自所有底层胶囊 $i$ 的预测向量 $\\mathbf{\\hat{u}}_{j|i}$ 的加权和：\n$$\\mathbf{s}_j^{(t)} = \\sum_{i=0}^{I-1} c_{ij}^{(t)} \\,\\mathbf{\\hat{u}}_{j|i}$$\n这个向量汇集了来自下一层的“投票”。每个投票的贡献由相应的耦合系数加权。\n\n**步骤 4：Squash 非线性函数**\n预激活向量 $\\mathbf{s}_j^{(t)}$ 通过一个非线性的“squash”函数，生成父胶囊的输出向量 $\\mathbf{v}_j^{(t)}$。该函数将其向量长度归一化到 $0$ 和 $1$ 之间，同时保持其方向：\n$$\\mathbf{v}_j^{(t)} = \\frac{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2}{1 + \\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2} \\cdot \\frac{\\mathbf{s}_j^{(t)}}{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert}$$\n如果 $\\mathbf{s}_j^{(t)}$ 是零向量，其范数为 $0$，输出 $\\mathbf{v}_j^{(t)}$ 也是零向量，必须处理这种情况以防止除以零。\n\n**步骤 5：路由对数更新**\n下一次迭代的对数 $b_{ij}^{(t+1)}$ 根据预测向量 $\\mathbf{\\hat{u}}_{j|i}$ 和父胶囊输出 $\\mathbf{v}_j^{(t)}$ 之间的一致性进行更新。一致性由标量积度量：\n$$b_{ij}^{(t+1)} = b_{ij}^{(t)} + \\mathbf{\\hat{u}}_{j|i} \\cdot \\mathbf{v}_j^{(t)}$$\n这种“通过一致性路由”的机制加强了导致强一致性的耦合。\n\n**温度方案**\n五个候选方案的实现如下：\n- **情况 A（快速衰减）：** 使用指数衰减方案：$\\tau_t = \\tau_{init} \\cdot \\gamma^t$，其中 $\\tau_{init} = 3.0$，$\\gamma = 0.5$。\n- **情况 B（始终为高）：** 恒定方案：$\\tau_t = 3.0$。\n- **情况 C（始终为低）：** 恒定方案：$\\tau_t = 0.25$。\n- **情况 D（平滑衰减）：** 从一个上界到一个下界的线性衰减方案：$\\tau_t = \\tau_{upper} - (\\tau_{upper} - \\tau_{lower}) \\frac{t}{T-1}$，其中 $\\tau_{upper} = 3.0$，$\\tau_{lower} = 0.2$。\n- **情况 E（熵触发）：** 一种动态方案。对于迭代 $t$：\n  _如果_ $t \\ge 3$，则 $\\tau_t=0.2$。\n  _否则如果_ $t=0$，则 $\\tau_t=3.0$。\n  _否则_ ($t \\in \\{1, 2\\}$)，如果 $\\bar{H}^{(t-1)} \\ge 0.60$，则 $\\tau_t=3.0$，否则 $\\tau_t=0.2$。\n\n**评估与程序设计**\n设计一个函数，该函数接收一个方案的参数，为 $T=6$ 次迭代运行完整的模拟，并记录每一步的平均熵 $\\bar{H}^{(t)}$。模拟结束后，它检查两个约束：\n1.  **早期防坍塌**：$\\bar{H}^{(0)} \\ge 0.55$ 并且 $\\bar{H}^{(1)} \\ge 0.55$。\n2.  **最终选择性**：$\\bar{H}^{(5)} \\le 0.20$。\n\n如果两个约束都满足，函数返回 `True`，否则返回 `False`。主程序对五个方案中的每一个执行此评估，并按指定格式化布尔结果。为提高效率，采用了基于 NumPy 的向量化实现，特别是对于一致性计算，其可以表示为爱因斯坦求和约定：`agreement = np.einsum('ijd,jd->ij', u_hat, v)`。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to evaluate five temperature schedules for dynamic routing\n    in a Capsule Network and print the results.\n    \"\"\"\n\n    def evaluate_schedule(schedule_type, params):\n        \"\"\"\n        Simulates dynamic routing for a given temperature schedule and\n        evaluates it against specified entropy constraints.\n\n        Args:\n            schedule_type (str): An identifier for the schedule ('A', 'B', 'C', 'D', 'E').\n            params (tuple): Parameters for the schedule.\n\n        Returns:\n            bool: True if both early anti-collapse and final selectivity\n                  constraints are met, False otherwise.\n        \"\"\"\n        # --- Fixed Simulation Parameters ---\n        I, J, d, T = 3, 2, 2, 6\n        H_early_min, H_final_max = 0.55, 0.20\n\n        # --- Fixed Predicted Votes (u_hat[i, j, :] is u_j|i) ---\n        u_hat = np.zeros((I, J, d))\n        u_hat[0, 0, :] = [1.2, 0.0]\n        u_hat[0, 1, :] = [0.0, 1.1]\n        u_hat[1, 0, :] = [1.0, 0.1]\n        u_hat[1, 1, :] = [0.1, 0.9]\n        u_hat[2, 0, :] = [0.0, 1.0]\n        u_hat[2, 1, :] = [1.0, 0.0]\n\n        # --- Initialization ---\n        b = np.zeros((I, J))  # Routing logits\n        mean_entropies = []\n\n        # --- Dynamic Routing Simulation Loop ---\n        for t in range(T):\n            # 1. Determine current temperature tau_t\n            current_tau = 0.0\n            if schedule_type == 'A':\n                initial_temp, decay_factor = params\n                current_tau = initial_temp * (decay_factor ** t)\n            elif schedule_type == 'B' or schedule_type == 'C':\n                const_temp, = params\n                current_tau = const_temp\n            elif schedule_type == 'D':\n                lower, upper = params\n                current_tau = upper - (upper - lower) * t / (T - 1)\n            elif schedule_type == 'E':\n                high_temp, low_temp, switch_thresh, forced_low_iter = params\n                if t >= forced_low_iter:\n                    current_tau = low_temp\n                elif t == 0:\n                    current_tau = high_temp\n                else:  # t in {1, 2}\n                    if mean_entropies[t - 1] < switch_thresh:\n                        current_tau = low_temp\n                    else:\n                        current_tau = high_temp\n\n            # 2. Calculate coupling coefficients c_ij\n            # Using a numerically stable softmax implementation\n            scaled_b = b / current_tau\n            max_b = np.max(scaled_b, axis=1, keepdims=True)\n            exp_b = np.exp(scaled_b - max_b)\n            c = exp_b / np.sum(exp_b, axis=1, keepdims=True)\n\n            # 3. Calculate and store mean entropy\n            # c_ij * log(c_ij) -> 0 as c_ij -> 0. Add a small epsilon for safety,\n            # though softmax output is strictly positive with float precision.\n            entropies_i = -np.sum(c * np.log(c + 1e-12), axis=1)\n            mean_entropies.append(np.mean(entropies_i))\n\n            # 4. Calculate parent capsule pre-activations s_j\n            s = np.zeros((J, d))\n            for j in range(J):\n                s[j, :] = np.sum(c[:, j:j + 1] * u_hat[:, j, :], axis=0)\n\n            # 5. Calculate parent capsule outputs v_j (squash nonlinearity)\n            v = np.zeros((J, d))\n            for j in range(J):\n                s_norm_sq = np.sum(np.square(s[j]))\n                if s_norm_sq > 0:\n                    s_norm = np.sqrt(s_norm_sq)\n                    scale = s_norm_sq / (1.0 + s_norm_sq)\n                    v[j, :] = scale * s[j] / s_norm\n            \n            # 6. Update routing logits b_ij\n            agreement = np.einsum('ijd,jd->ij', u_hat, v)\n            b = b + agreement\n\n        # --- Check Constraints ---\n        early_ok = (mean_entropies[0] >= H_early_min) and \\\n                   (mean_entropies[1] >= H_early_min)\n        final_ok = (mean_entropies[T - 1] <= H_final_max)\n\n        return early_ok and final_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('A', (3.0, 0.5)),      # fast-decaying\n        ('B', (3.0,)),          # always high\n        ('C', (0.25,)),         # always low\n        ('D', (0.2, 3.0)),      # smooth decay\n        ('E', (3.0, 0.2, 0.60, 3)), # entropy-triggered\n    ]\n\n    results = []\n    for case_type, case_params in test_cases:\n        result = evaluate_schedule(case_type, case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3104863"}]}