{"hands_on_practices": [{"introduction": "要真正掌握 Transformer 如何从海量文本中学习，我们必须首先理解其核心掩码过程的机制。这项练习 [@problem_id:3164748] 邀请你使用基础概率论来为动态掩码策略建模。通过推导单个词元（token）被用于梯度更新的期望次数，你将对预训练过程的效率和覆盖范围获得精确的、定量的理解。", "problem": "一个Transformer编码器使用掩码语言建模（MLM）进行预训练，在每次遍历数据期间，每个词元位置都以概率 $p$ 独立地成为一个预测目标，无论它是否被一个特殊的掩码符号、一个随机的词元替换，或保持不变。训练语料库包含一组固定的词元位置，每个周期（epoch）精确地遍历每个词元位置一次。掩码在每个周期动态地重新采样，因此在一个周期中对给定词元位置的选择决策与在任何其他周期中的选择是独立的。训练共进行 $E$ 个周期。虽然使用随机梯度下降（SGD），但对于此问题，仅关注每个词元的贡献：每次一个词元位置被选为预测目标时，它贡献一个单一的损失项，从而在该周期内为该词元位置贡献一个梯度贡献事件。\n\n对于一个固定的词元位置 $i$，定义随机变量 $S$ 为该位置被选为预测目标的总周期数。仅使用独立伯努利试验的概率公理和期望的线性性作为基本依据，以闭合形式推导以下关于 $E$ 和 $p$ 的量：\n- 每个词元位置的梯度贡献事件的期望数量，$\\mathbb{E}[S]$。\n- 在 $E$ 个周期内，该词元位置至少被选择一次的覆盖概率，$\\Pr(S \\geq 1)$。\n\n将你的最终答案表示为单个行向量 $\\begin{pmatrix} \\mathbb{E}[S] & \\Pr(S \\geq 1) \\end{pmatrix}$。无需四舍五入，也不涉及物理单位。", "solution": "在尝试解答之前，对问题陈述的有效性进行评估。\n\n### 第1步：提取已知条件\n- 一个Transformer编码器使用掩码语言建模（MLM）进行预训练。\n- 在每次遍历（周期）中，每个词元位置独立地以概率 $p$ 成为预测目标。\n- 给定词元位置在某一个周期的选择决策与在任何其他周期的选择无关。\n- 训练的总周期数为 $E$。\n- 每次一个词元位置被选为预测目标时，它贡献一个单独的梯度贡献事件。\n- 对于一个固定的词元位置 $i$，随机变量 $S$ 被定义为该位置被选为预测目标的总周期数。\n- 推导必须仅使用独立伯努利试验的概率公理和期望的线性性。\n- 需要推导的量是每个词元位置的梯度贡献事件的期望数量 $\\mathbb{E}[S]$，以及该词元位置至少被选择一次的覆盖概率 $\\Pr(S \\geq 1)$。\n\n### 第2步：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学依据：** 该问题描述了掩码语言建模（MLM）预训练目标的一个简化但基本正确的概率模型，MLM是像BERT这样的现代自然语言处理模型的基石。使用伯努利试验来建模独立的掩码事件是标准的，并且在数学上是合理的。\n- **适定性：** 问题定义清晰，包含了所有必要的参数（$E$, $p$），并明确定义了随机变量 $S$ 和目标量（$\\mathbb{E}[S]$, $\\Pr(S \\geq 1)$）。这些条件导向一个唯一且有意义的解。\n- **客观性：** 语言正式、精确，没有任何主观或模糊的术语。\n- **缺陷清单：**\n    1.  **科学或事实不健全：** 无。该设定是真实世界机器学习过程的有效抽象。\n    2.  **不可形式化或不相关：** 该问题完全可以在概率论框架内形式化，并且与深度学习领域直接相关。\n    3.  **不完整或矛盾的设定：** 问题是自洽且一致的。跨周期的事件独立性已明确说明。\n    4.  **不切实际或不可行：** 该场景是用于分析此类算法的标准理论模型。它并非不切实际。\n    5.  **不适定或结构不良：** 问题结构良好，并能导出一个唯一的解。\n    6.  **伪深刻、琐碎或同义反复：** 问题需要正确应用概率论的基本原理（期望的线性性，独立事件的性质），并且是一个非平凡的形式推导练习。\n    7.  **超出科学可验证性范围：** 推导过程在数学上是可验证的。\n\n### 第3步：结论与行动\n问题是**有效的**且适定的。将进行推导求解。\n\n### 推导过程\n\n让我们考虑一个单一的、固定的词元位置。训练过程共进行 $E$ 个周期。\n\n首先，我们为每个周期 $j \\in \\{1, 2, \\dots, E\\}$ 定义一组指示随机变量 $X_j$。\n如果该词元位置在周期 $j$ 中被选为预测目标，则令 $X_j = 1$；否则令 $X_j = 0$。\n\n根据问题陈述，任何给定周期的选择都是一个独立的事件，概率为 $p$。因此，每个 $X_j$ 都是一个参数为 $p$ 的独立伯努利随机变量。每个 $X_j$ 的概率质量函数为：\n$$ \\Pr(X_j = 1) = p $$\n$$ \\Pr(X_j = 0) = 1 - p $$\n\n随机变量 $S$ 表示该位置被选择的总周期数。这是所有周期上指示变量的总和：\n$$ S = \\sum_{j=1}^{E} X_j $$\n\n**1. 期望值 $\\mathbb{E}[S]$ 的推导**\n\n我们需要求出梯度贡献事件的总期望数，即 $\\mathbb{E}[S]$。问题要求使用期望的线性性原理。\n\n单个伯努利随机变量 $X_j$ 的期望是：\n$$ \\mathbb{E}[X_j] = 1 \\cdot \\Pr(X_j=1) + 0 \\cdot \\Pr(X_j=0) = 1 \\cdot p + 0 \\cdot (1-p) = p $$\n\n利用期望的线性性，即随机变量之和的期望等于它们各自期望之和，我们有：\n$$ \\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{j=1}^{E} X_j\\right] = \\sum_{j=1}^{E} \\mathbb{E}[X_j] $$\n\n由于对于每个周期 $j$，都有 $\\mathbb{E}[X_j] = p$，所以总和变为：\n$$ \\mathbb{E}[S] = \\sum_{j=1}^{E} p = Ep $$\n\n**2. 覆盖概率 $\\Pr(S \\geq 1)$ 的推导**\n\n我们需要求出在 $E$ 个周期内该词元位置至少被选择一次的概率。这就是概率 $\\Pr(S \\geq 1)$。\n\n使用补集法则计算这个概率更直接。事件“$S \\geq 1$”（词元至少被选择一次）是事件“$S = 0$”（词元从未被选择）的补集。\n$$ \\Pr(S \\geq 1) = 1 - \\Pr(S = 0) $$\n\n事件“$S=0$”发生当且仅当该词元在 $E$ 个周期中均未被选择。这意味着 $X_1=0$ 且 $X_2=0$ 且 ... 且 $X_E=0$。\n$$ \\Pr(S=0) = \\Pr(X_1=0, X_2=0, \\dots, X_E=0) $$\n\n问题陈述了跨周期的选择决策是独立的。因此，联合事件的概率是各个独立事件概率的乘积：\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} \\Pr(X_j=0) $$\n\n对于每个周期 $j$，词元未被选择的概率是 $\\Pr(X_j=0) = 1-p$。\n将此代入乘积中，我们得到：\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} (1-p) = (1-p)^E $$\n\n最后，将此结果代回补集法则的方程中，得到覆盖概率：\n$$ \\Pr(S \\geq 1) = 1 - (1-p)^E $$\n\n推导出的量是 $\\mathbb{E}[S] = Ep$ 和 $\\Pr(S \\geq 1) = 1 - (1-p)^E$。这些已按要求以 $E$ 和 $p$ 的闭合形式表示。", "answer": "$$\\boxed{\\begin{pmatrix} Ep & 1 - (1-p)^{E} \\end{pmatrix}}$$", "id": "3164748"}, {"introduction": "神经网络学习的魔力在于反向传播，预训练目标也不例外。这个问题 [@problem_id:3164800] 将带你深入掩码语言建模（MLM）更新规则的核心，要求你从第一性原理出发推导梯度。通过计算损失信号如何反向传播到掩码词元（mask token）的不同参数化方案，你将为这些模型如何从损坏的文本中实际学习建立坚实的直觉。", "problem": "考虑一个使用掩码语言建模 (MLM) 训练的基于 Transformer 的序列模型，其中掩码语言建模 (MLM) 定义为最小化在掩码位置上真实词元（token）的负对数似然。设词汇表大小为 $V$，嵌入维度为 $d$，输出分类层为一个线性映射 $U \\in \\mathbb{R}^{V \\times d}$，其后接一个 softmax 函数。对于一个给定的训练样本，假设有 $K$ 个不相交的掩码片段（span），由 $k \\in \\{1,\\dots,K\\}$ 索引，其中片段 $k$ 中的掩码位置集合表示为 $S^{(k)}$。\n\n对于每个掩码位置 $i \\in S^{(k)}$，Transformer 生成一个隐藏状态 $h_{i} \\in \\mathbb{R}^{d}$，该状态被送入输出头以产生 logits $z_{i} = U h_{i} \\in \\mathbb{R}^{V}$ 和概率 $p_{i} = \\mathrm{softmax}(z_{i}) \\in \\mathbb{R}^{V}$。设位置 $i$ 处的真实标签索引为 $y_{i} \\in \\{1,\\dots,V\\}$，并设 $e_{y_{i}} \\in \\mathbb{R}^{V}$ 为 $y_{i}$ 的独热向量。将每个样本的 MLM 损失定义为\n$$\n\\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k=1}^{K} \\sum_{i \\in S^{(k)}} \\left( - \\ln p_{i}[y_{i}] \\right).\n$$\n\n假设 Transformer 在掩码输入周围存在以下一阶局部线性化：\n- 在共享掩码词元设置中，一个单一的全局参数向量 $m \\in \\mathbb{R}^{d}$ 被用作每个片段中每个掩码词元的输入嵌入，并且隐藏状态满足\n$$\nh_{i} = A_{i} m + b_{i},\n$$\n对于每个掩码位置 $i$，其中 $A_{i} \\in \\mathbb{R}^{d \\times d}$ 和 $b_{i} \\in \\mathbb{R}^{d}$ 不依赖于 $m$。\n\n- 在学习的片段级掩码嵌入设置中，每个片段 $k$ 都有其自己的参数向量 $s^{(k)} \\in \\mathbb{R}^{d}$，用作该片段中每个掩码词元的输入嵌入，并且隐藏状态满足\n$$\nh_{i} = A_{i} s^{(k)} + b_{i},\n$$\n对于每个 $i \\in S^{(k)}$，其中的 $A_{i}$ 和 $b_{i}$ 与上述相同，且不依赖于 $s^{(k)}$。\n\n从 softmax 函数和交叉熵的核心定义出发，仅使用标准微积分（包括链式法则），推导 $\\mathcal{L}_{\\mathrm{MLM}}$ 相对于以下参数的梯度的闭式解析表达式：\n- 共享掩码词元 $m$；\n- 对于任意固定的片段索引 $k$，学习的片段级掩码嵌入 $s^{(k)}$。\n\n将您的最终答案表示为一个单行矩阵，其第一个条目是 $\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}}$，第二个条目是 $\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}}$。不需要数值近似或四舍五入，也不涉及物理单位。您的推导必须是自包含的，并且除了所述定义外，不得调用任何快捷公式。", "solution": "问题要求推导掩码语言建模 (MLM) 损失 $\\mathcal{L}_{\\mathrm{MLM}}$ 相对于两组参数的梯度：一个共享的掩码词元嵌入 $m$ 和一组片段级的掩码嵌入 $s^{(k)}$。推导将基于第一性原理，使用微积分的链式法则进行构建。\n\nMLM 损失定义为所有掩码位置上负对数似然的总和：\n$$\n\\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i\n$$\n其中 $\\mathcal{L}_i = - \\ln p_{i}[y_{i}]$ 是位置 $i$ 处单个词元的损失。索引 $k'$ 用作对片段求和的哑变量，以区别于问题第二部分中的固定索引 $k$。\n\n对于单个位置 $i$ 的计算序列如下：\n1. 输入嵌入（$m$ 或 $s^{(k)}$）用于计算隐藏状态 $h_i$。\n2. 隐藏状态 $h_i \\in \\mathbb{R}^d$ 用于通过 $z_i = U h_i$ 计算 logits $z_i \\in \\mathbb{R}^V$。\n3. Logits $z_i$ 通过 $p_i = \\mathrm{softmax}(z_i)$ 转换为概率 $p_i \\in \\mathbb{R}^V$。\n4. 损失 $\\mathcal{L}_i$ 根据 $p_i$ 和真实标签索引 $y_i$ 计算得出。\n\n我们将通过此计算图反向应用链式法则来计算梯度。\n\n**步骤 1：$\\mathcal{L}_i$ 相对于 logits $z_i$ 的梯度**\n\n位置 $i$ 的损失为 $\\mathcal{L}_i = -\\ln p_i[y_i]$。对于任何类别索引 $j \\in \\{1, \\dots, V\\}$，概率 $p_i[j]$ 由 softmax 函数给出：\n$$\np_i[j] = \\frac{\\exp(z_i[j])}{\\sum_{l=1}^{V} \\exp(z_i[l])}\n$$\n其中 $z_i[j]$ 是 logit 向量 $z_i$ 的第 $j$ 个分量。\n\n将其代入真实类别 $y_i$ 的损失函数中：\n$$\n\\mathcal{L}_i = - \\ln \\left( \\frac{\\exp(z_i[y_i])}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\right) = -z_i[y_i] + \\ln\\left(\\sum_{l=1}^{V} \\exp(z_i[l])\\right)\n$$\n我们现在计算 $\\mathcal{L}_i$ 相对于 logit 向量 $z_i$ 的任意分量 $z_i[j]$ 的偏导数。\n\n如果 $j = y_i$：\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i[y_i]} = -1 + \\frac{1}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\cdot \\frac{\\partial}{\\partial z_i[y_i]} \\left( \\sum_{l=1}^{V} \\exp(z_i[l]) \\right) = -1 + \\frac{\\exp(z_i[y_i])}{\\sum_{l=1}^{V} \\exp(z_i[l])} = p_i[y_i] - 1\n$$\n如果 $j \\neq y_i$：\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i[j]} = 0 + \\frac{1}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\cdot \\frac{\\partial}{\\partial z_i[j]} \\left( \\sum_{l=1}^{V} \\exp(z_i[l]) \\right) = \\frac{\\exp(z_i[j])}{\\sum_{l=1}^{V} \\exp(z_i[l])} = p_i[j]\n$$\n这两种情况可以合并为一个单一的向量表达式。设 $e_{y_i}$ 为独热向量，其中对应于索引 $y_i$ 的分量为 $1$，所有其他分量为 $0$。标量 $\\mathcal{L}_i$ 相对于向量 $z_i$ 的梯度是：\n$$\n\\nabla_{z_i} \\mathcal{L}_i = p_i - e_{y_i}\n$$\n这个向量 $\\nabla_{z_i} \\mathcal{L}_i \\in \\mathbb{R}^V$ 表示在 logit 层面的误差信号。\n\n**步骤 2：$\\mathcal{L}_i$ 相对于隐藏状态 $h_i$ 的梯度**\n\nLogits 是隐藏状态的线性函数：$z_i = U h_i$，其中 $U \\in \\mathbb{R}^{V \\times d}$ 且 $h_i \\in \\mathbb{R}^d$。我们使用向量值函数的链式法则。标量 $\\mathcal{L}_i$ 相对于向量 $h_i$ 的梯度是：\n$$\n\\nabla_{h_i} \\mathcal{L}_i = \\left(\\frac{\\partial z_i}{\\partial h_i}\\right)^T (\\nabla_{z_i} \\mathcal{L}_i)\n$$\n项 $\\frac{\\partial z_i}{\\partial h_i}$ 是函数 $z_i(h_i)$ 的雅可比矩阵。由于 $z_i = U h_i$，这个雅可比矩阵就是矩阵 $U$。因此：\n$$\n\\nabla_{h_i} \\mathcal{L}_i = U^T (\\nabla_{z_i} \\mathcal{L}_i) = U^T (p_i - e_{y_i})\n$$\n这个梯度，我们记为 $g_i = \\nabla_{h_i} \\mathcal{L}_i$，是一个在 $\\mathbb{R}^d$ 中的向量。\n\n**步骤 3：$\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}}$ 的推导**\n\n在共享掩码词元设置中，每个掩码位置 $i$ 的隐藏状态 $h_i$ 是单个共享参数向量 $m \\in \\mathbb{R}^d$ 的线性函数：\n$$\nh_i = A_i m + b_i\n$$\n总损失 $\\mathcal{L}_{\\mathrm{MLM}}$ 是 $m$ 的函数，通过其对每个 $h_i$ 的依赖性实现。根据梯度算子的线性和链式法则：\n$$\n\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}} = \\nabla_{m} \\left( \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i(h_i(m)) \\right) = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\nabla_{m} \\mathcal{L}_i\n$$\n对于每一项 $\\mathcal{L}_i$，我们再次应用链式法则：\n$$\n\\nabla_{m} \\mathcal{L}_i = \\left(\\frac{\\partial h_i}{\\partial m}\\right)^T (\\nabla_{h_i} \\mathcal{L}_i)\n$$\n函数 $h_i(m)$ 的雅可比矩阵是矩阵 $A_i \\in \\mathbb{R}^{d \\times d}$。代入这个以及 $\\nabla_{h_i} \\mathcal{L}_i = g_i$ 的表达式：\n$$\n\\nabla_{m} \\mathcal{L}_i = A_i^T g_i = A_i^T U^T (p_i - e_{y_i})\n$$\n对所有掩码位置求和，得到最终相对于 $m$ 的梯度：\n$$\n\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} A_i^T U^T (p_i - e_{y_i})\n$$\n\n**步骤 4：$\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}}$ 的推导**\n\n在片段级掩码嵌入设置中，对于一个固定的片段索引 $k$，参数向量 $s^{(k)} \\in \\mathbb{R}^d$ 仅影响 $i \\in S^{(k)}$ 的隐藏状态 $h_i$。对于这些位置，关系是：\n$$\nh_i = A_i s^{(k)} + b_i \\quad \\text{for } i \\in S^{(k)}\n$$\n对于在不同片段中的任何位置 $j$，$j \\in S^{(k')}$ 且 $k' \\neq k$，隐藏状态 $h_j$ 依赖于 $s^{(k')}$，而不依赖于 $s^{(k)}$。因此，对于 $j \\notin S^{(k)}$，$\\nabla_{s^{(k)}} \\mathcal{L}_j = 0$。\n\n总损失相对于 $s^{(k)}$ 的梯度是：\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}} = \\nabla_{s^{(k)}} \\left( \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i \\right) = \\sum_{i \\in S^{(k)}} \\nabla_{s^{(k)}} \\mathcal{L}_i\n$$\n求和仅针对片段 $k$ 中的位置。对于此和中的每一项，我们应用链式法则：\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_i = \\left(\\frac{\\partial h_i}{\\partial s^{(k)}}\\right)^T (\\nabla_{h_i} \\mathcal{L}_i) \\quad \\text{for } i \\in S^{(k)}\n$$\n雅可比矩阵 $\\frac{\\partial h_i}{\\partial s^{(k)}}$ 是矩阵 $A_i$。代入这个以及 $g_i = \\nabla_{h_i} \\mathcal{L}_i$ 的表达式：\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_i = A_i^T g_i = A_i^T U^T (p_i - e_{y_i})\n$$\n对片段 $k$ 内的所有位置求和，得到最终相对于 $s^{(k)}$ 的梯度：\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}} = \\sum_{i \\in S^{(k)}} A_i^T U^T (p_i - e_{y_i})\n$$\n结果即为所求梯度的闭式解析表达式。", "answer": "$$\n\\boxed{\\pmatrix{ \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} A_i^T U^T (p_i - e_{y_i}) & \\sum_{i \\in S^{(k)}} A_i^T U^T (p_i - e_{y_i}) }}\n$$", "id": "3164800"}, {"introduction": "除了简单地预测缺失的单词，预训练目标还可以被设计用来教模型关于语言的结构，例如词序。这项实践 [@problem_id:3164810] 探索了一种基于跨度级别（span-level）排列的替代目标，并挑战你分析其对句法恢复的影响。通过推导并实现一个模型来分析这种损坏如何影响单词邻接关系，你将深入了解不同的自监督任务是如何赋予 Transformer 模型语言结构知识的。", "problem": "考虑一个在自然语言处理（NLP）的Transformer预训练中使用的简化版span级词序损坏目标。一个长度为 $n$ 的词符序列被划分为连续的span。固定一个基本span大小 $s \\in \\mathbb{Z}_{\\ge 1}$。该序列被划分为 $\\lfloor n/s \\rfloor$ 个长度为 $s$ 的完整span，如果 $r \\neq 0$，则外加一个长度为 $r = n \\bmod s$ 的末尾span；如果 $r = 0$，则没有剩余的span。每个span以概率 $p \\in [0,1]$ 独立地进行损坏处理：如果一个span被损坏，其词符的顺序将被替换为其元素的均匀随机排列；否则，该span的顺序保持不变。span本身保持其原始的段顺序；只有span内部的词符顺序可能改变。\n\n假设基准真实（ground-truth）句法结构是建立在词符位置上的有向链：对于所有 $i \\in \\{1,2,\\dots,n-1\\}$，从位置 $i$ 到 $i+1$ 都存在一个有向依赖边。将句法恢复定义为这样一个事件：在损坏后的序列中，某个有向边作为相同两个词符之间、保持相同从左到右顺序的有向邻接关系而依然存在。假设有一个没有词汇知识的解析器，它只能依赖于损坏序列中的邻接关系；因此，一个边是可恢复的，当且仅当其端点在损坏序列中以正确的顺序相邻。\n\n仅从以下基本事实出发：\n- 均匀排列：对于一个固定的span长度 $k \\in \\mathbb{Z}_{\\ge 1}$，如果一个span被损坏，所有 $k!$ 种排列都是等可能的。\n- 期望的线性性、独立span损坏事件的独立性，以及全概率定律。\n\n从第一性原理出发，推导在所有 $n-1$ 个边中可恢复有向边的期望分数公式，该公式表示为 $n$、$s$ 和 $p$ 的函数，并需要考虑span内部的边和跨越span边界的边。\n\n您的程序必须实现推导出的公式，并计算以下每个测试用例的期望分数。每个答案必须是 $[0,1]$ 范围内的实数，四舍五入到六位小数。\n\n测试套件参数集 $(n,s,p)$：\n- 情况 A：$(12, 4, 0.3)$\n- 情况 B：$(10, 3, 0.6)$\n- 情况 C：$(15, 5, 1.0)$\n- 情况 D：$(8, 1, 0.9)$\n- 情况 E：$(20, 4, 0.0)$\n- 情况 F：$(5, 10, 0.5)$\n\n最终输出格式：\n- 您的程序应生成单行文本，其中包含按顺序排列的情况 A 到 F 的结果，形式为逗号分隔的列表并用方括号括起，每个值都四舍五入到六位小数，例如 $[x_1,x_2,x_3,x_4,x_5,x_6]$，其中每个 $x_i$ 是小数点后有六位数的小数，且无空格。", "solution": "该问题是有效的，因为它具有科学依据、定义明确且客观。它提供了一套自洽、一致的定义和约束，以便基于概率论和组合数学的既定原则推导公式并计算结果。\n\n目标是求出可恢复有向边的期望分数。设 $n$ 为序列长度。基准真实有向链中的总边数为 $N = n-1$。期望分数是恢复边的总期望数 $E_{\\text{total}}$ 除以 $N$。\n\n根据期望的线性性，恢复边的总期望数是 $n-1$ 个独立边各自恢复概率的总和。设 $X_i$ 为从位置 $i$ 到 $i+1$ 的边被恢复这一事件的指示随机变量。\n$$ E[\\text{fraction}] = \\frac{1}{n-1} E\\left[\\sum_{i=1}^{n-1} X_i\\right] = \\frac{1}{n-1} \\sum_{i=1}^{n-1} E[X_i] = \\frac{1}{n-1} \\sum_{i=1}^{n-1} P(\\text{edge } i \\to i+1 \\text{ recovered}) $$\n为了计算这个和，我们必须首先求出单个边的恢复概率。问题的结构将序列划分为span。一个边要么完全包含在单个span内（内部边），要么连接两个相邻的span（跨界边）。我们分别分析这两种情况。\n\n**1. 内部边的恢复概率**\n如果位置 $i$ 和 $i+1$ 都落在同一个span内，则从位置 $i$ 到 $i+1$ 的边是内部边。这当且仅当 $i$ 不是基本span大小 $s$ 的倍数时发生。\n设包含该边的span长度为 $k$。该边的恢复取决于这个span是否被损坏，该事件以概率 $p$ 发生。我们使用全概率定律：\n$$ P(\\text{recovery}) = P(\\text{recovery}|\\text{corrupted}) \\cdot P(\\text{corrupted}) + P(\\text{recovery}|\\text{not corrupted}) \\cdot P(\\text{not corrupted}) $$\n- 如果span未被损坏（概率为 $1-p$），原始词符顺序得以保留。该边必然被恢复。因此，$P(\\text{recovery}|\\text{not corrupted}) = 1$。\n- 如果span被损坏（概率为 $p$），其 $k$ 个词符会根据一个均匀随机排列重新排序。共有 $k!$ 种可能的排列。为了使该边被恢复，原始位置 $i$ 的词符必须紧随原始位置 $i+1$ 的词符。我们可以将这对词符视为一个单独的块。剩下的 $k-2$ 个词符加上这个块的排列数为 $(k-1)!$。因此，这种特定邻接关系发生的概率是 $\\frac{(k-1)!}{k!} = \\frac{1}{k}$。所以，$P(\\text{recovery}|\\text{corrupted}) = \\frac{1}{k}$。\n\n在一个长度为 $k$ 的span中，一个内部边的恢复概率，记为 $P_{\\text{intra}}(k)$，是：\n$$ P_{\\text{intra}}(k) = \\frac{1}{k} \\cdot p + 1 \\cdot (1-p) = 1 - p + \\frac{p}{k} $$\n\n**2. 跨界边的恢复概率**\n跨界边连接一个span的最后一个词符和下一个span的第一个词符。这发生在位置 $i, i+1$ 处，其中 $i$ 是 $s$ 的倍数。\n假设该边从一个长度为 $k_1$ 的span跨越到一个长度为 $k_2$ 的span。要使该边被恢复，必须同时满足两个条件：\n1. 原始位于第一个span末尾的词符，在其可能被损坏后，必须最终位于该span的最后一个位置。\n2. 原始位于第二个span开头的词符，在其可能被损坏后，必须最终位于该span的第一个位置。\n\n由于span损坏是独立事件，两个条件同时满足的概率是它们各自概率的乘积。\n让我们计算其中一个条件的概率，例如，在一个长度为 $k$ 的span中，一个特定的词符最终位于一个特定位置（第一个或最后一个）。\n- 如果span未被损坏（概率为 $1-p$），词符保持其原始位置。此条件以概率 $1$ 满足。\n- 如果span被损坏（概率为 $p$），该词符被等可能地放置在 $k$ 个位置中的任意一个。它落在所需特定位置的概率是 $\\frac{1}{k}$。\n单个词符被正确定位的概率 $P_{\\text{pos}}(k)$ 是：\n$$ P_{\\text{pos}}(k) = \\frac{1}{k} \\cdot p + 1 \\cdot (1-p) = 1 - p + \\frac{p}{k} $$\n在长度为 $k_1$ 和 $k_2$ 的span之间的跨界边的恢复概率，记为 $P_{\\text{cross}}(k_1, k_2)$，是：\n$$ P_{\\text{cross}}(k_1, k_2) = P_{\\text{pos}}(k_1) \\cdot P_{\\text{pos}}(k_2) = \\left(1 - p + \\frac{p}{k_1}\\right) \\left(1 - p + \\frac{p}{k_2}\\right) $$\n\n**3. 恢复边的总期望数**\n为了求出 $E_{\\text{total}}$，我们将恢复的内部边期望数 ($E_{\\text{intra}}$) 和跨界边期望数 ($E_{\\text{cross}}$) 相加。设 $q = \\lfloor n/s \\rfloor$ 为完整span的数量，$r = n \\bmod s$ 为剩余span的长度。\n\n- **内部边的期望数 ($E_{\\text{intra}}$):**\n  - 有 $q$ 个长度为 $s$ 的完整span。如果 $s>1$，每个span有 $s-1$ 个内部边。它们的贡献是 $q(s-1)P_{\\text{intra}}(s)$。\n  - 如果 $r>1$，有一个长度为 $r$ 的剩余span，它有 $r-1$ 个内部边。其贡献是 $(r-1)P_{\\text{intra}}(r)$。\n$$ E_{\\text{intra}} = q \\cdot \\max(0, s-1) \\left(1 - p + \\frac{p}{s}\\right) + \\mathbb{I}(r>1) \\cdot (r-1) \\left(1 - p + \\frac{p}{r}\\right) $$\n其中 $\\mathbb{I}(r>1)$ 是一个指示函数。\n\n- **跨界边的期望数 ($E_{\\text{cross}}$):**\n  - 跨界边的数量是 $\\lfloor (n-1)/s \\rfloor$。\n  - 如果 $r=0$（即 $n=qs$），则有 $q-1$ 个跨界边，全部位于长度为 $s$ 的span之间。这仅在 $q > 1$ 时适用。\n    $$ E_{\\text{cross}} = \\max(0, q-1) \\left(1 - p + \\frac{p}{s}\\right)^2 $$\n  - 如果 $r>0$（即 $n=qs+r$），则有 $q$ 个跨界边。如果 $q \\ge 1$，我们有 $q-1$ 个在长度为 $s$ 的span之间的边，以及一个在长度为 $s$ 的span和长度为 $r$ 的span之间的边。\n    $$ E_{\\text{cross}} = \\max(0, q-1) \\left(1 - p + \\frac{p}{s}\\right)^2 + \\mathbb{I}(q \\ge 1) \\left(1 - p + \\frac{p}{s}\\right) \\left(1 - p + \\frac{p}{r}\\right) $$\n\n将这些部分组合起来得到 $E_{\\text{total}} = E_{\\text{intra}} + E_{\\text{cross}}$。最终的期望分数是 $\\frac{E_{\\text{total}}}{n-1}$。对于 $n \\le 1$ 的边界情况，该分数未定义，但所有测试用例都满足 $n>1$。\n\n这个推导出的公式在以下程序中实现，用于计算给定测试用例的结果。", "answer": "```python\ndef solve():\n    \"\"\"\n    Computes the expected fraction of recoverable directed edges\n    for a span-level word-order corruption objective.\n    \"\"\"\n\n    test_cases = [\n        # (n, s, p)\n        (12, 4, 0.3),  # Case A\n        (10, 3, 0.6),  # Case B\n        (15, 5, 1.0),  # Case C\n        (8, 1, 0.9),   # Case D\n        (20, 4, 0.0),  # Case E\n        (5, 10, 0.5),  # Case F\n    ]\n\n    def calculate_expected_fraction(n, s, p):\n        \"\"\"\n        Derives the expected fraction of recoverable edges based on n, s, and p.\n\n        Args:\n            n: Total number of tokens in the sequence.\n            s: Base span size.\n            p: Probability of corrupting a span.\n\n        Returns:\n            The expected fraction of recoverable edges as a float.\n        \"\"\"\n        if n = 1:\n            return 0.0\n        \n        # If no corruption, all edges are recovered.\n        if p == 0.0:\n            return 1.0\n\n        # If span size is 1, permutation has no effect.\n        if s == 1:\n            return 1.0\n\n        q = n // s  # Number of full spans\n        r = n % s   # Length of the remainder span\n\n        E_intra = 0.0\n        # Contribution from q full spans of length s\n        if s > 1:\n            P_intra_s = 1.0 - p + p / s\n            E_intra += q * (s - 1) * P_intra_s\n\n        # Contribution from remainder span of length r\n        if r > 1:\n            P_intra_r = 1.0 - p + p / r\n            E_intra += (r - 1) * P_intra_r\n\n        E_cross = 0.0\n        # Check if any full spans exist\n        if q > 0:\n            P_pos_s = 1.0 - p + p / s\n            \n            if r == 0:\n                # n is a multiple of s. q spans of length s.\n                # q-1 boundaries between s-length spans.\n                if q > 1:\n                    E_cross = (q - 1) * (P_pos_s ** 2)\n            else:\n                # n = qs + r. q full spans and 1 remainder span.\n                # Total q boundaries.\n                # q-1 boundaries between s-length spans (if q>1).\n                if q > 1:\n                    E_cross += (q - 1) * (P_pos_s ** 2)\n                \n                # 1 boundary between an s-length span and an r-length span.\n                P_pos_r = 1.0 - p + p / r\n                E_cross += P_pos_s * P_pos_r\n        \n        # If q=0, n  s. The whole sequence is one span of length r=n.\n        # E_cross is 0, correctly handled by the q>0 check.\n\n        E_total = E_intra + E_cross\n        \n        return E_total / (n - 1)\n\n    results = []\n    for n_val, s_val, p_val in test_cases:\n        fraction = calculate_expected_fraction(n_val, s_val, p_val)\n        results.append(fraction)\n\n    # Format output to six decimal places.\n    # The default f-string rounding (\"round half to even\") is sufficient\n    # as no test case result has a 7th decimal digit of exactly 5.\n    formatted_results = [f'{res:.6f}' for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3164810"}]}