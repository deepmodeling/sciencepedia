{"hands_on_practices": [{"introduction": "视觉变换器（ViT）通常在固定的图像分辨率下进行预训练，但这与现实世界中图像尺寸多变的应用场景不符。一个核心挑战在于，模型学习到的绝对位置嵌入需要被调整以适应新的、不同分辨率的图像。本练习将引导你探索并量化不同位置嵌入插值策略（如最近邻与双线性插值）所引入的“位置漂移”误差，从而让你掌握在部署ViT时一个至关重要的实践细节。[@problem_id:3199247]", "problem": "给定一个基于视觉变换器（Vision Transformer, ViT）的模型族。视觉变换器将输入图像分块，形成一个由不重叠的图像块（patch）组成的网格，并为每个图像块的嵌入（embedding）增添一个绝对位置嵌入。设预训练图像分辨率产生的图像块中心网格为 $G_{0}^{(h)} \\times G_{0}^{(w)}$，其中 $G_{0}^{(h)} \\in \\mathbb{N}$ 和 $G_{0}^{(w)} \\in \\mathbb{N}$ 分别表示沿高度和宽度的图像块数量。在测试时，图像可能会被缩放，从而产生一个不同大小的图像块中心网格 $G_{1}^{(h)} \\times G_{1}^{(w)}$。绝对位置嵌入仅在预训练网格上有定义；因此，在测试时，必须将测试时网格的位置映射到预训练网格，以便为新的词元（token）构建位置嵌入。考虑以下两种策略：\n\n- 最近邻选择（无插值）：对于测试时网格索引为 $(i,j)$（其中 $i \\in \\{0,\\dots,G_{1}^{(h)}-1\\}$ 且 $j \\in \\{0,\\dots,G_{1}^{(w)}-1\\}$）的每个词元，通过对从测试网格到预训练网格的连续映射进行四舍五入，选择一个单一的预训练网格索引 $(u,v)$。这样就得到了一个单一的预训练中心，其绝对嵌入被直接重用。\n- 带边界钳位的双线性插值：对于每个测试时位于 $(i,j)$ 的词元，计算其在预训练网格中的连续坐标，然后使用四个最近的预训练中心进行双线性插值。当某个邻近点的索引超出了有效的预训练索引范围时，将其钳位（clamp）到最近的有效边界索引。\n\n定义网格大小为 $G^{(h)} \\times G^{(w)}$ 的网格中，索引 $(i,j)$ 的归一化中心坐标为 $c(i,j;G^{(h)},G^{(w)}) = \\left(\\frac{i+\\frac{1}{2}}{G^{(h)}}, \\frac{j+\\frac{1}{2}}{G^{(w)}}\\right) \\in [0,1]^{2}$。定义与测试时索引 $(i,j)$ 对应的连续预训练网格坐标为 $(\\tilde{u},\\tilde{v}) \\in \\mathbb{R}^{2}$，其满足中心对齐约束 $\\left(\\frac{\\tilde{u}+\\frac{1}{2}}{G_{0}^{(h)}}, \\frac{\\tilde{v}+\\frac{1}{2}}{G_{0}^{(w)}}\\right) = \\left(\\frac{i+\\frac{1}{2}}{G_{1}^{(h)}}, \\frac{j+\\frac{1}{2}}{G_{1}^{(w)}}\\right)$。此映射在不同网格间转换时，保持了在 $[0,1]^{2}$ 空间中的相对中心位置。\n\n对于给定的策略，将测试时索引 $(i,j)$ 的词元位置漂移定义为在 $[0,1]^{2}$ 空间中，测试时中心 $c(i,j;G_{1}^{(h)},G_{1}^{(w)})$ 与该策略所使用的预训练位置所隐含的中心之间的欧几里得距离。对于最近邻选择，隐含中心是所选的单一预训练中心。对于双线性插值，隐含中心是在边界钳位下，起作用的多个预训练中心的双线性重心。通过均方根漂移来聚合所有测试时词元的漂移，即每个词元距离平方的均值的平方根。\n\n从坐标归一化和网格重采样的第一性原理出发，推导出一个可计算的程序，用于计算两种策略下的均方根漂移。然后，将其实现为一个程序，该程序针对下面的每个测试用例，计算两个浮点数：最近邻选择的均方根漂移和带边界钳位的双线性插值的均方根漂移，并按此顺序排列。\n\n使用以下测试套件，涵盖标称、上采样、下采样、矩形变化和极端折叠情况。每个元组列出 $(G_{0}^{(h)}, G_{0}^{(w)}, G_{1}^{(h)}, G_{1}^{(w)})$：\n- 情况 1：$(14, 14, 14, 14)$。\n- 情况 2：$(14, 14, 16, 16)$。\n- 情况 3：$(14, 14, 10, 10)$。\n- 情况 4：$(14, 20, 16, 10)$。\n- 情况 5：$(14, 14, 1, 1)$。\n\n你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。列表中的数字按 $[\\text{nn}_{1}, \\text{bi}_{1}, \\text{nn}_{2}, \\text{bi}_{2}, \\dots, \\text{nn}_{5}, \\text{bi}_{5}]$ 的顺序排列，每个值都是一个浮点数。不涉及物理单位。此任务中没有角度或百分比。程序不得读取任何输入，且必须按原样运行。", "solution": "该问题定义明确，有科学依据，并包含足够的信息来推导出唯一且可验证的解。网格重采样、坐标归一化和插值的概念在计算机视觉和数值分析中是标准方法。该问题是这些原理在深度学习特定场景中的直接应用，即视觉变换器位置嵌入的自适应调整。我们将进行形式化推导。\n\n### 1. 坐标系与映射\n\n设网格大小为 $G^{(h)} \\times G^{(w)}$。位于整数索引 $(k, l)$（其中 $k \\in \\{0, \\dots, G^{(h)}-1\\}$ 且 $l \\in \\{0, \\dots, G^{(w)}-1\\}$）的词元的归一化中心坐标定义为：\n$$\nc(k,l; G^{(h)}, G^{(w)}) = \\left( \\frac{k+0.5}{G^{(h)}}, \\frac{l+0.5}{G^{(w)}} \\right)\n$$\n在 $G_{1}^{(h)} \\times G_{1}^{(w)}$ 网格中，索引为 $(i,j)$ 的测试时词元的归一化中心为：\n$$\nc_{test}(i,j) = c(i,j; G_{1}^{(h)}, G_{1}^{(w)}) = \\left( \\frac{i+0.5}{G_{1}^{(h)}}, \\frac{j+0.5}{G_{1}^{(w)}} \\right)\n$$\n问题通过保持此归一化中心不变，定义了一个到预训练网格坐标 $(\\tilde{u}, \\tilde{v}) \\in \\mathbb{R}^2$ 的连续映射。这给出了约束条件：\n$$\n\\left( \\frac{\\tilde{u}+0.5}{G_{0}^{(h)}}, \\frac{\\tilde{v}+0.5}{G_{0}^{(w)}} \\right) = \\left( \\frac{i+0.5}{G_{1}^{(h)}}, \\frac{j+0.5}{G_{1}^{(w)}} \\right)\n$$\n求解 $\\tilde{u}$ 和 $\\tilde{v}$ 可得到显式映射：\n$$\n\\tilde{u}(i) = G_{0}^{(h)} \\left( \\frac{i+0.5}{G_{1}^{(h)}} \\right) - 0.5\n$$\n$$\n\\tilde{v}(j) = G_{0}^{(w)} \\left( \\frac{j+0.5}{G_{1}^{(w)}} \\right) - 0.5\n$$\n这些连续坐标构成了两种重采样策略的基础。\n\n### 2. 策略 1：最近邻（NN）选择\n\n在此策略中，连续坐标 $(\\tilde{u}, \\tilde{v})$ 被四舍五入到最近的整数，以选择一个单一的预训练网格索引 $(u, v)$：\n$$\nu = \\text{round}(\\tilde{u})\n$$\n$$\nv = \\text{round}(\\tilde{v})\n$$\n隐含中心是这个单一预训练词元的归一化中心：\n$$\nc_{implied, NN}(i,j) = c(u,v; G_{0}^{(h)}, G_{0}^{(w)}) = \\left( \\frac{u+0.5}{G_{0}^{(h)}}, \\frac{v+0.5}{G_{0}^{(w)}} \\right)\n$$\n词元 $(i,j)$ 的位置漂移平方是测试中心与隐含中心之间的欧几里得距离平方：\n$$\nd_{NN}^2(i,j) = || c_{test}(i,j) - c_{implied, NN}(i,j) ||_2^2\n$$\n利用中心对齐约束，我们可以简化此表达式。第一个坐标的差值为：\n$$\nc_{test}^{(h)}(i,j) - c_{implied, NN}^{(h)}(i,j) = \\frac{\\tilde{u}+0.5}{G_{0}^{(h)}} - \\frac{u+0.5}{G_{0}^{(h)}} = \\frac{\\tilde{u}-u}{G_{0}^{(h)}}\n$$\n类似的关系也适用于第二个坐标。因此，漂移平方为：\n$$\nd_{NN}^2(i,j) = \\left(\\frac{\\tilde{u}-u}{G_{0}^{(h)}}\\right)^2 + \\left(\\frac{\\tilde{v}-v}{G_{0}^{(w)}}\\right)^2\n$$\n总 RMSD 是在所有 $G_{1}^{(h)} \\times G_{1}^{(w)}$ 个测试词元上，这些漂移平方的均值的平方根：\n$$\n\\text{RMSD}_{NN} = \\sqrt{ \\frac{1}{G_{1}^{(h)} G_{1}^{(w)}} \\sum_{i=0}^{G_{1}^{(h)}-1} \\sum_{j=0}^{G_{1}^{(w)}-1} d_{NN}^2(i,j) }\n$$\n\n### 3. 策略 2：带边界钳位的双线性插值（BI）\n\n该策略使用围绕连续坐标 $(\\tilde{u}, \\tilde{v})$ 的 4 个预训练网格中心。令 $u_0 = \\lfloor \\tilde{u} \\rfloor$，$u_1 = u_0+1$，$v_0 = \\lfloor \\tilde{v} \\rfloor$，$v_1 = v_0+1$。令小数部分为 $\\Delta u = \\tilde{u} - u_0$ 和 $\\Delta v = \\tilde{v} - v_0$。\n\n关键步骤是边界钳位。预训练索引必须位于 $[0, G_{0}^{(h)}-1]$ 和 $[0, G_{0}^{(w)}-1]$ 范围内。4 个角点索引按如下方式进行钳位：\n$$\nu'_0 = \\text{clamp}(u_0, 0, G_0^{(h)}-1) \\quad u'_1 = \\text{clamp}(u_1, 0, G_0^{(h)}-1)\n$$\n$$\nv'_0 = \\text{clamp}(v_0, 0, G_0^{(w)}-1) \\quad v'_1 = \\text{clamp}(v_1, 0, G_0^{(w)}-1)\n$$\n隐含中心是这 4 个被钳位的预训练中心的归一化坐标的双线性重心。权重由 $\\Delta u$ 和 $\\Delta v$ 决定。隐含中心的高度分量是：\n$$\nc_{implied, BI}^{(h)}(i,j) = \\frac{(1-\\Delta u)(u'_0+0.5) + \\Delta u(u'_1+0.5)}{G_0^{(h)}}\n$$\n宽度分量也类似：\n$$\nc_{implied, BI}^{(w)}(i,j) = \\frac{(1-\\Delta v)(v'_0+0.5) + \\Delta v(v'_1+0.5)}{G_0^{(w)}}\n$$\n如果钳位未激活（即 $u'_0=u_0$，$u'_1=u_1$ 等），则高度分量的分子简化为 $\\tilde{u}+0.5$。在这种情况下，$c_{implied, BI}^{(h)}(i,j) = \\frac{\\tilde{u}+0.5}{G_0^{(h)}} = c_{test}^{(h)}(i,j)$，导致漂移为零。因此，此策略下的位置漂移完全由边界钳位的非线性效应引起。这种情况发生在上采样（$G_1 > G_0$）时，因为 $(\\tilde{u}, \\tilde{v})$ 的范围超出了有效索引范围 $[0, G_0-1]$。\n\n词元 $(i,j)$ 的位置漂移平方为：\n$$\nd_{BI}^2(i,j) = || c_{test}(i,j) - c_{implied, BI}(i,j) ||_2^2\n$$\n$$\nd_{BI}^2(i,j) = \\left( c_{test}^{(h)}(i,j) - c_{implied, BI}^{(h)}(i,j) \\right)^2 + \\left( c_{test}^{(w)}(i,j) - c_{implied, BI}^{(w)}(i,j) \\right)^2\n$$\n总 RMSD 通过对所有测试词元取平均值来计算：\n$$\n\\text{RMSD}_{BI} = \\sqrt{ \\frac{1}{G_{1}^{(h)} G_{1}^{(w)}} \\sum_{i=0}^{G_{1}^{(h)}-1} \\sum_{j=0}^{G_{1}^{(w)}-1} d_{BI}^2(i,j) }\n$$\n\n### 4. 计算流程\n\n使用 NumPy 进行向量化计算是高效的。对于每个测试用例 $(G_{0}^{(h)}, G_{0}^{(w)}, G_{1}^{(h)}, G_{1}^{(w)})$：\n1.  创建形状为 $(G_{1}^{(h)}, G_{1}^{(w)})$ 的测试索引 $i$ 和 $j$ 的二维数组。\n2.  以向量化的方式计算所有测试词元的连续预训练坐标 $(\\tilde{u}, \\tilde{v})$。\n3.  对于 NN 策略，对 $(\\tilde{u}, \\tilde{v})$ 进行四舍五入得到 $(u, v)$，计算隐含中心，计算距离的平方，然后求其均值的平方根。\n4.  对于 BI 策略，计算 $\\lfloor \\tilde{u} \\rfloor$、$\\lfloor \\tilde{v} \\rfloor$ 和小数部分 $\\Delta u$、$\\Delta v$。对角点索引进行钳位，计算隐含的重心，计算距离的平方，然后求其均值的平方根。\n5.  存储得到的两个 RMSD 值。对所有测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Vision Transformer positional drift problem for a suite of test cases.\n    \"\"\"\n\n    # Test suite: each tuple is (G0h, G0w, G1h, G1w)\n    test_cases = [\n        (14, 14, 14, 14),\n        (14, 14, 16, 16),\n        (14, 14, 10, 10),\n        (14, 20, 16, 10),\n        (14, 14, 1, 1),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        G0h, G0w, G1h, G1w = case\n\n        if G1h == 0 or G1w == 0:\n            # Handle trivial case with no tokens to avoid division by zero.\n            results.extend([0.0, 0.0])\n            continue\n            \n        # 1. Create grids of test indices\n        # Generate 1D arrays for i and j coordinates\n        i_coords = np.arange(G1h)\n        j_coords = np.arange(G1w)\n        # Create 2D grids for vectorized computation\n        ii, jj = np.meshgrid(i_coords, j_coords, indexing='ij')\n\n        # 2. Coordinate Mapping\n        # Calculate continuous pretraining coordinates (u_tilde, v_tilde)\n        u_tilde = G0h * (ii + 0.5) / G1h - 0.5\n        v_tilde = G0w * (jj + 0.5) / G1w - 0.5\n\n        # Normalized coordinates of the test tokens\n        c_test_h = (ii + 0.5) / G1h\n        c_test_w = (jj + 0.5) / G1w\n\n        # -------- Strategy 1: Nearest-Neighbor (NN) Selection --------\n        \n        # Round to get nearest pretraining indices. np.round rounds .5 to nearest even.\n        u_nn = np.round(u_tilde)\n        v_nn = np.round(v_tilde)\n\n        # Implied centers from the selected pretraining indices\n        c_implied_nn_h = (u_nn + 0.5) / G0h\n        c_implied_nn_w = (v_nn + 0.5) / G0w\n\n        # Squared Euclidean distance for each token\n        dist_sq_nn = (c_test_h - c_implied_nn_h)**2 + (c_test_w - c_implied_nn_w)**2\n\n        # Root-mean-square drift\n        rmsd_nn = np.sqrt(np.mean(dist_sq_nn))\n        results.append(rmsd_nn)\n\n        # -------- Strategy 2: Bilinear Interpolation (BI) with Border Clamping --------\n\n        # Get floor indices for interpolation\n        u0 = np.floor(u_tilde)\n        v0 = np.floor(v_tilde)\n\n        # Get fractional parts for interpolation weights\n        delta_u = u_tilde - u0\n        delta_v = v_tilde - v0\n\n        # Clamp the four corner indices to the valid pretraining index range\n        u0_clamped = np.clip(u0, 0, G0h - 1)\n        u1_clamped = np.clip(u0 + 1, 0, G0h - 1)\n        v0_clamped = np.clip(v0, 0, G0w - 1)\n        v1_clamped = np.clip(v0 + 1, 0, G0w - 1)\n        \n        # Compute the implied center coordinates (barycenter of clamped indices)\n        # This is a linear interpolation between the centers of the clamped indices\n        implied_center_coord_h = (1 - delta_u) * (u0_clamped + 0.5) + delta_u * (u1_clamped + 0.5)\n        implied_center_coord_w = (1 - delta_v) * (v0_clamped + 0.5) + delta_v * (v1_clamped + 0.5)\n\n        # Normalize the implied center for BI\n        c_implied_bi_h = implied_center_coord_h / G0h\n        c_implied_bi_w = implied_center_coord_w / G0w\n\n        # Squared Euclidean distance for each token\n        dist_sq_bi = (c_test_h - c_implied_bi_h)**2 + (c_test_w - c_implied_bi_w)**2\n\n        # Root-mean-square drift\n        rmsd_bi = np.sqrt(np.mean(dist_sq_bi))\n        results.append(rmsd_bi)\n\n    # Format the final output string as a list of floats\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3199247"}, {"introduction": "从零开始训练像ViT这样的大型模型需要海量数据，而知识蒸馏是一种高效的替代方案，它能将一个预训练的“教师”模型（如CNN）的知识迁移给“学生”模型（ViT）。本练习将指导你实现一个标准的蒸馏损失函数，该函数结合了对教师模型预测的模仿（通过KL散度）和对真实标签的拟合（通过交叉熵损失）。通过这个过程，你不仅能学会如何高效地训练ViT，还能学习分析其内部的注意力模式，从而深入理解其工作机制。[@problem_id:3199218]", "problem": "要求您从第一性原理出发，对从卷积神经网络 (CNN) 教师模型蒸馏而来的数据高效图像 Transformer (DeiT) 进行推理，并实现一个程序来计算一个有原理依据的蒸馏损失并分析学习到的注意力模式。重点是视觉 Transformer (ViT)、Kullback–Leibler 散度 (KL)、交叉熵 (CE) 和多头自注意力。请从概率论和优化的基本定义和经过充分检验的事实出发，不要依赖这些定义之外的捷径公式。\n\n场景如下。一个视觉 Transformer (ViT) 学生模型产生类别 logit，一个卷积神经网络 (CNN) 教师模型产生类别 logit，并且有一个真实的 one-hot 标签。蒸馏结合了两个项：一项使学生模型的预测分布与教师模型的预测分布对齐，另一项强制要求对真实标签的保真度。注意力模式表示为每个头和每层的 token 上的行随机矩阵，您必须从中计算信息论和对齐度量，以表征注意力是如何分布的，以及它如何与教师模型提供的空间引导分布对齐。\n\n您的任务是：\n- 从第一性原理出发，推导由 Kullback–Leibler 散度和交叉熵这两个基于基本定义构建的目标的加权和所定义的复合蒸馏损失。证明这会产生一个形式为 $\\mathcal{L} = \\lambda \\, \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) + (1-\\lambda) \\, \\mathcal{L}_{\\text{CE}}$ 的标量目标，其中 $p_{\\text{ViT}}$ 和 $p_{\\text{teacher}}$ 是分别对各自的 logit 应用 softmax 函数得到的预测分布，$\\lambda \\in [0,1]$，且 $\\mathcal{L}_{\\text{CE}}$ 是与真实 one-hot 标签的交叉熵。\n- 实现以下计算：\n  1. 计算学生模型和教师模型 logit 的 softmax 分布 $p_i = \\exp(z_i) / \\sum_j \\exp(z_j)$。\n  2. 使用 $p = p_{\\text{ViT}}$ 和 $q = p_{\\text{teacher}}$ 计算 Kullback–Leibler 散度 $\\mathrm{KL}(p \\| q) = \\sum_i p_i \\big( \\log p_i - \\log q_i \\big)$。\n  3. 使用真实 one-hot 向量 $y$ 和 $p = p_{\\text{ViT}}$ 计算交叉熵 $\\mathcal{L}_{\\text{CE}} = - \\sum_i y_i \\log p_i$。\n  4. 按上述规定计算蒸馏损失 $\\mathcal{L}$。\n  5. 注意力分析：\n     - 对于每个层 $l$ 和头 $h$，给定多头自注意力矩阵 $A^{(l,h)} \\in \\mathbb{R}^{N \\times N}$，其中 $N$ 是 token 数量，每一行是目标 token 上的一个概率分布。提取类别 token 的注意力行（行索引为 $0$），并将其限制在图像块 token（列索引从 $1$ 到 $N-1$）。将此受限向量重新归一化，使其总和为 $1$。\n     - 计算这些受限的类别 token 注意力分布在所有层和头上的平均香农熵：$H = - \\sum_{i} a_i \\log a_i$，在所有头和层上取平均值。\n     - 计算平均类别 token 图像块注意力向量（在所有层和头上取平均，然后重新归一化使其总和为 $1$）与教师模型提供的图像块级引导向量 $g$（重新归一化使其总和为 $1$）之间的余弦相似度对齐：$\\cos(\\theta) = \\frac{\\langle a, g \\rangle}{\\|a\\|_2 \\, \\|g\\|_2}$。\n\n角度单位不适用。没有物理单位。所有输出必须是实数。\n\n测试套件：\n提供以下四个测试用例。在每个用例中，类别数为 $3$，token 数量为 $N=5$（一个类别 token 加四个图像块 token），注意力矩阵有 $L=2$ 层和 $H=2$ 个头。为简单起见，对于类别 token 行以外的行，您可以假设为均匀分布。\n\n- 测试用例 $1$（正常路径）：\n  - 学生模型 logit $z_{\\text{ViT}} = [2.0, 0.5, -1.0]$\n  - 教师模型 logit $z_{\\text{teacher}} = [3.0, -0.5, -2.0]$\n  - 真实 one-hot $y = [1, 0, 0]$\n  - 权重 $\\lambda = 0.5$\n  - 教师图像块引导 $g = [0.4, 0.4, 0.1, 0.1]$\n  - 注意力矩阵 $A^{(l,h)}$，$L=2$，$H=2$，$N=5$：\n    - 层 $1$，头 $1$，行：\n      - 行 $0$：$[0.1, 0.5, 0.3, 0.05, 0.05]$\n      - 行 $1$：$[0.2, 0.2, 0.2, 0.2, 0.2]$\n      - 行 $2$：$[0.2, 0.2, 0.2, 0.2, 0.2]$\n      - 行 $3$：$[0.2, 0.2, 0.2, 0.2, 0.2]$\n      - 行 $4$：$[0.2, 0.2, 0.2, 0.2, 0.2]$\n    - 层 $1$，头 $2$，行：\n      - 行 $0$：$[0.15, 0.45, 0.25, 0.10, 0.05]$\n      - 其他行为相同的均匀分布 $[0.2, 0.2, 0.2, 0.2, 0.2]$\n    - 层 $2$，头 $1$，行：\n      - 行 $0$：$[0.12, 0.55, 0.20, 0.08, 0.05]$\n      - 其他行：均匀分布 $[0.2, 0.2, 0.2, 0.2, 0.2]$\n    - 层 $2$，头 $2$，行：\n      - 行 $0$：$[0.10, 0.50, 0.25, 0.10, 0.05]$\n      - 其他行：均匀分布 $[0.2, 0.2, 0.2, 0.2, 0.2]$\n\n- 测试用例 $2$（边界条件 $\\lambda = 1$ 且预测相同）：\n  - 学生模型 logit $z_{\\text{ViT}} = [1.2, -0.1, 0.0]$\n  - 教师模型 logit $z_{\\text{teacher}} = [1.2, -0.1, 0.0]$\n  - 真实 one-hot $y = [0, 1, 0]$\n  - 权重 $\\lambda = 1.0$\n  - 教师图像块引导 $g = [0.7, 0.1, 0.1, 0.1]$\n  - 注意力矩阵 $A^{(l,h)}$ 具有扩散的类别 token 注意力（所有类别 token 行均为均匀分布）：\n    - 对于所有 $l \\in \\{1,2\\}$ 和 $h \\in \\{1,2\\}$，行 $0$：$[0.2, 0.2, 0.2, 0.2, 0.2]$\n    - 其他行：均匀分布 $[0.2, 0.2, 0.2, 0.2, 0.2]$\n\n- 测试用例 $3$（边界条件 $\\lambda = 0$ 且学生模型为均匀分布）：\n  - 学生模型 logit $z_{\\text{ViT}} = [0.0, 0.0, 0.0]$\n  - 教师模型 logit $z_{\\text{teacher}} = [0.5, -0.2, 0.1]$\n  - 真实 one-hot $y = [0, 0, 1]$\n  - 权重 $\\lambda = 0.0$\n  - 教师图像块引导 $g = [0.2, 0.3, 0.3, 0.2]$\n  - 注意力矩阵 $A^{(l,h)}$，类别 token 注意力集中在一个图像块上：\n    - 层 $1$，头 $1$，行 $0$：$[0.05, 0.10, 0.80, 0.03, 0.02]$\n    - 层 $1$，头 $2$，行 $0$：$[0.04, 0.12, 0.78, 0.04, 0.02]$\n    - 层 $2$，头 $1$，行 $0$：$[0.06, 0.08, 0.80, 0.04, 0.02]$\n    - 层 $2$，头 $2$，行 $0$：$[0.05, 0.10, 0.75, 0.06, 0.04]$\n    - 其他行：均匀分布 $[0.2, 0.2, 0.2, 0.2, 0.2]$\n\n- 测试用例 $4$（空间引导未对齐的边缘情况）：\n  - 学生模型 logit $z_{\\text{ViT}} = [0.3, 1.0, -0.7]$\n  - 教师模型 logit $z_{\\text{teacher}} = [-0.2, 2.5, -1.0]$\n  - 真实 one-hot $y = [0, 1, 0]$\n  - 权重 $\\lambda = 0.8$\n  - 教师图像块引导 $g = [0.50, 0.20, 0.20, 0.10]$\n  - 注意力矩阵 $A^{(l,h)}$，类别 token 注意力偏向于后面的图像块：\n    - 层 $1$，头 $1$，行 $0$：$[0.05, 0.10, 0.20, 0.30, 0.35]$\n    - 层 $1$，头 $2$，行 $0$：$[0.05, 0.15, 0.20, 0.25, 0.35]$\n    - 层 $2$，头 $1$，行 $0$：$[0.04, 0.12, 0.18, 0.30, 0.36]$\n    - 层 $2$，头 $2$, 行 $0$：$[0.06, 0.10, 0.22, 0.28, 0.34]$\n    - 其他行：均匀分布 $[0.2, 0.2, 0.2, 0.2, 0.2]$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个条目必须是 $[\\mathcal{L}, H, \\cos(\\theta)]$ 顺序的三元素列表，其中 $\\mathcal{L}$ 是蒸馏损失，$H$ 是类别 token 在图像块 token 上的平均注意力熵，$\\cos(\\theta)$ 是与 $g$ 的余弦相似度对齐。例如，输出格式必须类似于 $[[l_1,h_1,c_1],[l_2,h_2,c_2],[l_3,h_3,c_3],[l_4,h_4,c_4]]$, 其中包含按顺序 1 到 4 的每个测试用例的数值。", "solution": "用户提供的问题是有效的，因为它具有科学依据、适定且客观。它植根于深度学习的既定原则，特别是视觉 Transformer (ViT) 中的知识蒸馏，并使用标准的信息论度量。所有必要的数据和定义都已提供，以得出一个唯一且有意义的解。\n\n### 第 1 部分：蒸馏损失的第一性原理推导\n\n目标是从基本原理出发推导复合损失函数 $\\mathcal{L} = \\lambda \\, \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) + (1-\\lambda) \\, \\mathcal{L}_{\\text{CE}}$。该损失函数旨在通过结合两个目标来训练学生模型 (ViT)：匹配教师模型 (CNN) 的预测，以及基于真实标签正确分类输入。\n\n1.  **从 Logit 到概率分布**：\n    一个用于 $K$ 类分类问题的神经网络通常输出一个原始分数向量，或称为 logit，记为 $z \\in \\mathbb{R}^K$。为了将这些分数解释为类别上的概率分布，会应用 softmax 函数。对于一个 logit 向量 $z = [z_1, z_2, \\ldots, z_K]$，第 $i$ 类的概率由下式给出：\n    $$\n    p_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)}\n    $$\n    令 $p_{\\text{ViT}}$ 为学生 ViT 从其 logit $z_{\\text{ViT}}$ 生成的概率分布，令 $p_{\\text{teacher}}$ 为教师 CNN 的 logit $z_{\\text{teacher}}$ 生成的分布。\n\n2.  **目标 1：对真实值的保真度（交叉熵损失）**：\n    分类模型的主要目标是准确预测真实类别。对于给定的输入，真实标签表示为一个 one-hot 向量 $y \\in \\{0, 1\\}^K$，其中对于真实类别 $k$，$y_k=1$，而对于 $i \\neq k$，$y_i=0$。训练目标是最大化分配给真实类别的概率 $(p_{\\text{ViT}})_k$。这等同于最大化对数似然 $\\log((p_{\\text{ViT}})_k)$，或者更常见地，最小化负对数似然 $-\\log((p_{\\text{ViT}})_k)$。\n    对于 one-hot 向量 $y$ 进行泛化，损失为：\n    $$\n    \\mathcal{L}_{\\text{CE}} = - \\sum_{i=1}^K y_i \\log((p_{\\text{ViT}})_i)\n    $$\n    这就是真实分布 $y$ 和预测分布 $p_{\\text{ViT}}$ 之间交叉熵损失的定义。它衡量了模型预测与“硬”真实标签之间的不相似性。\n\n3.  **目标 2：知识蒸馏（Kullback-Leibler 散度）**：\n    知识蒸馏旨在将“知识”从一个更大的、预训练的教师模型转移到一个更小的学生模型。教师的知识封装在其输出分布 $p_{\\text{teacher}}$ 中，该分布通过指示错误类别的相对概率（例如，“猫比汽车更像狗”）提供了比 one-hot 标签更丰富的信息。学生模型被训练来模仿这种“软”目标分布。\n    衡量两个概率分布 $P$ 和 $Q$ 之间差异的一个有原理依据的度量是 Kullback-Leibler (KL) 散度。问题指定了要最小化从学生分布到教师分布的 KL 散度，其定义为：\n    $$\n    \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) = \\sum_{i=1}^K (p_{\\text{ViT}})_i \\log\\left(\\frac{(p_{\\text{ViT}})_i}{(p_{\\text{teacher}})_i}\\right) = \\sum_{i=1}^K (p_{\\text{ViT}})_i \\left( \\log((p_{\\text{ViT}})_i) - \\log((p_{\\text{teacher}})_i) \\right)\n    $$\n    最小化此项会促使 $p_{\\text{ViT}}$ 变得与 $p_{\\text{teacher}}$ 相似。当且仅当两个分布相同时，散度为零。\n\n4.  **复合蒸馏损失**：\n    最终的损失函数是这两个目标的凸组合，由超参数 $\\lambda \\in [0, 1]$ 进行平衡。权重 $\\lambda$ 控制着从教师的软目标学习与从硬真实标签学习之间的权衡。\n    复合损失 $\\mathcal{L}$ 的公式如下：\n    $$\n    \\mathcal{L} = \\lambda \\cdot (\\text{Distillation Objective}) + (1-\\lambda) \\cdot (\\text{Fidelity Objective})\n    $$\n    代入每个目标的推导表达式，得到最终形式：\n    $$\n    \\mathcal{L} = \\lambda \\, \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) + (1-\\lambda) \\, \\mathcal{L}_{\\text{CE}}\n    $$\n    至此，推导完成。\n\n### 第 2 部分：计算的实现\n\n实现将遵循推导出的原理和问题陈述中概述的步骤。\n\n1.  **Softmax**：对于给定的 logit $z$ 计算 $p_i = \\exp(z_i) / \\sum_j \\exp(z_j)$。使用数值稳定版本。\n2.  **Kullback-Leibler 散度**：计算 $\\mathrm{KL}(p \\| q) = \\sum_i p_i (\\log p_i - \\log q_i)$。\n3.  **交叉熵损失**：计算 $\\mathcal{L}_{\\text{CE}} = - \\sum_i y_i \\log p_i$。鉴于 $y$ 是 one-hot 向量，这可以简化为 $-\\log p_k$，其中 $y_k=1$。\n4.  **蒸馏损失**：将上述各项组合为 $\\mathcal{L} = \\lambda \\cdot \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) + (1-\\lambda) \\cdot \\mathcal{L}_{\\text{CE}}$。\n5.  **注意力分析**：\n    - **类别 Token 注意力**：对于每个注意力矩阵 $A^{(l,h)}$，提取第一行（索引 0）。该行表示从类别 token 到所有其他 token 的注意力。\n    - **图像块注意力**：将该向量限制为其对应于图像块 token（索引 1 到 N-1）的分量。\n    - **重新归一化**：将受限向量重新归一化，使其总和为 1，从而得到图像块 token 上的概率分布 $a^{(l,h)}$。\n    - **平均熵**：对 $L \\times H$ 个重新归一化的图像块注意力向量中的每一个计算香农熵 $H(a) = -\\sum_i a_i \\log a_i$（使用自然对数）。最终度量 $H$ 是这些单个熵的平均值。\n    - **对齐**：通过对所有 $a^{(l,h)}$ 向量进行逐元素平均，然后重新归一化，计算出平均图像块注意力向量 $\\bar{a}$。教师引导向量 $g$ 也被重新归一化为概率分布 $g'$。计算余弦相似度 $\\mathrm{cos}(\\theta) = \\frac{\\langle \\bar{a}, g' \\rangle}{\\|\\bar{a}\\|_2 \\, \\|g'\\|_2}$ 以衡量学生模型的平均空间注意力焦点与教师指导之间的对齐程度。\n\n将对提供的四个测试用例中的每一个执行这些计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DeiT distillation and attention analysis problem for the given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (happy path)\n        {\n            \"z_vit\": np.array([2.0, 0.5, -1.0]),\n            \"z_teacher\": np.array([3.0, -0.5, -2.0]),\n            \"y_true\": np.array([1, 0, 0]),\n            \"lambda_val\": 0.5,\n            \"g_guidance\": np.array([0.4, 0.4, 0.1, 0.1]),\n            \"attention_matrices\": [\n                # Layer 1\n                np.array([ # Head 1\n                    [0.1, 0.5, 0.3, 0.05, 0.05],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2]\n                ]),\n                np.array([ # Head 2\n                    [0.15, 0.45, 0.25, 0.10, 0.05],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2]\n                ]),\n                # Layer 2\n                np.array([ # Head 1\n                    [0.12, 0.55, 0.20, 0.08, 0.05],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2]\n                ]),\n                np.array([ # Head 2\n                    [0.10, 0.50, 0.25, 0.10, 0.05],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2]\n                ]),\n            ]\n        },\n        # Test Case 2 (boundary lambda=1 and identical predictions)\n        {\n            \"z_vit\": np.array([1.2, -0.1, 0.0]),\n            \"z_teacher\": np.array([1.2, -0.1, 0.0]),\n            \"y_true\": np.array([0, 1, 0]),\n            \"lambda_val\": 1.0,\n            \"g_guidance\": np.array([0.7, 0.1, 0.1, 0.1]),\n            \"attention_matrices\": [\n                # Uniform class-token attention for all L=2, H=2\n                np.array([[0.2, 0.2, 0.2, 0.2, 0.2]] * 5)\n            ] * 4\n        },\n        # Test Case 3 (boundary lambda=0 and a uniform student)\n        {\n            \"z_vit\": np.array([0.0, 0.0, 0.0]),\n            \"z_teacher\": np.array([0.5, -0.2, 0.1]),\n            \"y_true\": np.array([0, 0, 1]),\n            \"lambda_val\": 0.0,\n            \"g_guidance\": np.array([0.2, 0.3, 0.3, 0.2]),\n            \"attention_matrices\": [\n                # L1H1\n                 np.array([[0.05, 0.10, 0.80, 0.03, 0.02], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L1H2\n                 np.array([[0.04, 0.12, 0.78, 0.04, 0.02], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L2H1\n                 np.array([[0.06, 0.08, 0.80, 0.04, 0.02], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L2H2\n                 np.array([[0.05, 0.10, 0.75, 0.06, 0.04], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n            ]\n        },\n        # Test Case 4 (edge case with misaligned spatial guidance)\n        {\n            \"z_vit\": np.array([0.3, 1.0, -0.7]),\n            \"z_teacher\": np.array([-0.2, 2.5, -1.0]),\n            \"y_true\": np.array([0, 1, 0]),\n            \"lambda_val\": 0.8,\n            \"g_guidance\": np.array([0.50, 0.20, 0.20, 0.10]),\n            \"attention_matrices\": [\n                # L1H1\n                np.array([[0.05, 0.10, 0.20, 0.30, 0.35], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L1H2\n                np.array([[0.05, 0.15, 0.20, 0.25, 0.35], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L2H1\n                np.array([[0.04, 0.12, 0.18, 0.30, 0.36], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L2H2\n                np.array([[0.06, 0.10, 0.22, 0.28, 0.34], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n            ]\n        }\n    ]\n\n    def softmax(z):\n        \"\"\"Computes a numerically stable softmax distribution.\"\"\"\n        e_z = np.exp(z - np.max(z))\n        return e_z / e_z.sum()\n\n    def kl_divergence(p, q):\n        \"\"\"Computes KL-divergence KL(p || q).\"\"\"\n        return np.sum(p * (np.log(p) - np.log(q)))\n\n    def cross_entropy(y_true, p_pred):\n        \"\"\"Computes cross-entropy loss.\"\"\"\n        # Since y_true is one-hot, find the index of the true class\n        true_class_idx = np.argmax(y_true)\n        # Loss is the negative log probability of the true class\n        return -np.log(p_pred[true_class_idx])\n\n    def shannon_entropy(p):\n        \"\"\"Computes Shannon entropy H(p) using natural logarithm.\"\"\"\n        # Filter out zero probabilities to avoid log(0)\n        p_nz = p[p > 0]\n        return -np.sum(p_nz * np.log(p_nz))\n\n    def cosine_similarity(v1, v2):\n        \"\"\"Computes cosine similarity between two vectors.\"\"\"\n        dot_product = np.dot(v1, v2)\n        norm_v1 = np.linalg.norm(v1)\n        norm_v2 = np.linalg.norm(v2)\n        if norm_v1 == 0 or norm_v2 == 0:\n            return 0.0\n        return dot_product / (norm_v1 * norm_v2)\n\n    results = []\n    # Process each test case\n    for case in test_cases:\n        # --- Loss Calculation ---\n        p_vit = softmax(case[\"z_vit\"])\n        p_teacher = softmax(case[\"z_teacher\"])\n        \n        l_kl = kl_divergence(p_vit, p_teacher)\n        l_ce = cross_entropy(case[\"y_true\"], p_vit)\n        \n        distillation_loss = case[\"lambda_val\"] * l_kl + (1 - case[\"lambda_val\"]) * l_ce\n\n        # --- Attention Analysis ---\n        entropies = []\n        renormalized_attentions = []\n        \n        for A in case[\"attention_matrices\"]:\n            # Extract class token attention row (index 0)\n            class_token_attn = A[0, :]\n            # Restrict to patch tokens (indices 1 to N-1)\n            patch_attn_raw = class_token_attn[1:]\n            \n            # Renormalize to sum to 1\n            patch_attn_sum = patch_attn_raw.sum()\n            if patch_attn_sum > 0:\n                renormalized_patch_attn = patch_attn_raw / patch_attn_sum\n            else: # Handle case of zero sum to avoid division by zero\n                # If all patch attentions are 0, create a uniform distribution\n                n_patches = len(patch_attn_raw)\n                renormalized_patch_attn = np.ones(n_patches) / n_patches\n                \n            renormalized_attentions.append(renormalized_patch_attn)\n            entropies.append(shannon_entropy(renormalized_patch_attn))\n\n        # Calculate average entropy\n        avg_entropy = np.mean(entropies)\n        \n        # Calculate mean patch attention vector\n        # This is already a probability distribution as it's a convex combination\n        # of probability distributions. Renormalizing again for formal correctness.\n        mean_patch_attn_unnorm = np.mean(renormalized_attentions, axis=0)\n        mean_patch_attn = mean_patch_attn_unnorm / mean_patch_attn_unnorm.sum()\n        \n        # Renormalize teacher guidance vector\n        g_guidance = case[\"g_guidance\"]\n        g_norm = g_guidance / g_guidance.sum()\n        \n        # Calculate cosine similarity\n        alignment = cosine_similarity(mean_patch_attn, g_norm)\n        \n        results.append([distillation_loss, avg_entropy, alignment])\n        \n    # Final print statement in the exact required format.\n    # The str() representation of a list includes spaces after commas, which matches\n    # the implicit formatting in the problem description's LaTeX examples.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3199218"}, {"introduction": "预训练的ViT是强大的特征提取器，在将其应用于新任务时，我们面临一个关键抉择：是更新整个模型（完全微调），还是仅训练一个新的分类头（线性探测）？本练习提供了一个量化框架，用于评估完全微调带来的性能增益，并将其与预训练模型所学得的“可迁移性”联系起来。通过计算这两个指标之间的相关性，你将深刻理解在何种情况下，付出完全微调的巨大计算成本是合理的，这取决于预训练表征的质量。[@problem_id:3199207]", "problem": "视觉变换器 (Vision Transformer, ViT) 是一种神经网络架构，它通过图像块嵌入和 Transformer 模块将图像映射到一个表示。考虑在目标数据集上进行下游分类的两种训练策略：全量微调（更新模型的所有参数）和线性探测（在冻结的嵌入之上仅训练一个线性分类器）。假设使用相同的测试集来评估这两种策略。目标是计算准确率增益，并将其与冻结嵌入的可迁移性度量关联起来。\n\n使用以下基本定义作为基础：\n1. 准确率定义为 $ \\mathrm{acc} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\{ \\hat{y}_i = y_i \\} $，其中 $n$ 是测试样本的总数，$y_i$ 是真实标签，$ \\hat{y}_i $ 是分类器预测的标签。全量微调和线性探测之间的准确率增益定义为 $ \\Delta \\mathrm{acc} = \\mathrm{acc}_{\\mathrm{full}} - \\mathrm{acc}_{\\mathrm{probe}} $。\n2. 为了量化冻结嵌入的可迁移性，对于具有类均值嵌入 $ \\mu_k \\in \\mathbb{R}^d $ 和各向同性的类内协方差矩阵 $ S_k = s_k^2 I_d $ 的 $K$ 个类别，定义可迁移性分数为\n$$\nT = \\frac{2}{K(K-1)} \\sum_{1 \\le i  j \\le K} \\frac{\\| \\mu_i - \\mu_j \\|_2}{\\sqrt{\\operatorname{tr}(S_i)} + \\sqrt{\\operatorname{tr}(S_j)}} ,\n$$\n其中 $ \\operatorname{tr}(S_k) = d s_k^2 $。\n3. 为了将 $ \\Delta \\mathrm{acc} $ 与嵌入可迁移性关联起来，计算所有测试用例的 $ \\Delta \\mathrm{acc} $ 值列表与相同测试用例的逆可迁移性分数 $ 1/T $ 列表之间的斯皮尔曼等级相关系数 $ \\rho $。斯皮尔曼相关系数是等级变换后变量之间的皮尔逊相关系数，对相同值使用平均排名：\n$$\n\\rho = \\frac{\\sum_{i=1}^{m} (R_x(i) - \\bar{R_x})(R_y(i) - \\bar{R_y})}{\\sqrt{\\sum_{i=1}^{m} (R_x(i) - \\bar{R_x})^2} \\sqrt{\\sum_{i=1}^{m} (R_y(i) - \\bar{R_y})^2}},\n$$\n其中 $m$ 是测试用例的数量，$R_x(i)$ 是用例 $i$ 按 $ \\Delta \\mathrm{acc} $ 的排名，$R_y(i)$ 是用例 $i$ 按 $ 1/T $ 的排名，$ \\bar{R_x}, \\bar{R_y} $ 是平均排名。\n\n你的任务是实现一个程序，对每个测试用例，计算 $ \\Delta \\mathrm{acc} $ 和 $ T $，然后计算列表 $ [\\Delta \\mathrm{acc}_1, \\dots, \\Delta \\mathrm{acc}_m] $ 和列表 $ [1/T_1, \\dots, 1/T_m] $ 之间的斯皮尔曼等级相关系数 $ \\rho $。将所有准确率和相关系数值表示为十进制浮点数。不涉及物理单位或角度单位。\n\n测试套件：\n所有用例均使用 $ K = 3 $ 个类别和嵌入维度 $ d = 4 $。\n\n- 用例 $1$：\n    - $ n_{\\mathrm{test}} = 200 $, $ c_{\\mathrm{full}} = 182 $, $ c_{\\mathrm{probe}} = 176 $。\n    - $ \\mu_1 = [2.0, -1.0, 0.5, 1.5] $, $ \\mu_2 = [-1.5, 2.2, -0.3, 0.7] $, $ \\mu_3 = [0.0, 0.0, 2.5, -1.0] $。\n    - $ s_1 = 0.50 $, $ s_2 = 0.60 $, $ s_3 = 0.55 $。\n\n- 用例 $2$：\n    - $ n_{\\mathrm{test}} = 200 $, $ c_{\\mathrm{full}} = 158 $, $ c_{\\mathrm{probe}} = 130 $。\n    - $ \\mu_1 = [0.5, 0.6, 0.4, 0.5] $, $ \\mu_2 = [0.6, 0.5, 0.5, 0.6] $, $ \\mu_3 = [0.4, 0.7, 0.5, 0.4] $。\n    - $ s_1 = 1.20 $, $ s_2 = 1.00 $, $ s_3 = 1.10 $。\n\n- 用例 $3$：\n    - $ n_{\\mathrm{test}} = 100 $, $ c_{\\mathrm{full}} = 98 $, $ c_{\\mathrm{probe}} = 98 $。\n    - $ \\mu_1 = [3.0, 0.0, 0.0, 0.0] $, $ \\mu_2 = [0.0, 3.0, 0.0, 0.0] $, $ \\mu_3 = [0.0, 0.0, 3.0, 0.0] $。\n    - $ s_1 = 0.30 $, $ s_2 = 0.30 $, $ s_3 = 0.30 $。\n\n- 用例 $4$：\n    - $ n_{\\mathrm{test}} = 50 $, $ c_{\\mathrm{full}} = 41 $, $ c_{\\mathrm{probe}} = 35 $。\n    - $ \\mu_1 = [1.0, -0.5, 0.0, 0.5] $, $ \\mu_2 = [-0.5, 1.0, 0.5, 0.0] $, $ \\mu_3 = [0.5, 0.5, -0.5, 1.0] $。\n    - $ s_1 = 0.90 $, $ s_2 = 0.80 $, $ s_3 = 1.00 $。\n\n- 用例 $5$：\n    - $ n_{\\mathrm{test}} = 500 $, $ c_{\\mathrm{full}} = 450 $, $ c_{\\mathrm{probe}} = 420 $。\n    - $ \\mu_1 = [1.5, 1.5, -0.5, 0.0] $, $ \\mu_2 = [-1.0, -1.2, 0.5, 0.0] $, $ \\mu_3 = [0.0, 2.0, 1.0, -1.0] $。\n    - $ s_1 = 0.40 $, $ s_2 = 0.45 $, $ s_3 = 0.50 $。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须包含 $6$ 个浮点数：按顺序排列的用例 $1$ 到 $5$ 的五个 $ \\Delta \\mathrm{acc} $ 值，其后是 $ [\\Delta \\mathrm{acc}_1, \\dots, \\Delta \\mathrm{acc}_5] $ 和 $ [1/T_1, \\dots, 1/T_5] $ 之间的斯皮尔曼等级相关系数 $ \\rho $。每个浮点数必须格式化为恰好六位小数，例如 $ [0.030000,0.140000,0.000000,0.120000,0.060000,1.000000] $。", "solution": "目标是为涉及视觉变换器 (ViT) 的五个不同测试用例计算准确率增益 $ \\Delta \\mathrm{acc} $ 和可迁移性分数 $ T $。随后，我们必须确定准确率增益序列与逆可迁移性分数 $ 1/T $ 序列之间的斯皮尔曼等级相关系数 $ \\rho $。\n\n问题提供了一套全面的定义和数据。所有测试用例都共享一个通用设置，即 $ K=3 $ 个类别和嵌入维度 $ d=4 $。总体分析包括三个主要阶段：\n1.  对 $ m=5 $ 个测试用例中的每一个，计算准确率增益 $ \\Delta \\mathrm{acc} $。\n2.  对每个测试用例，计算可迁移性分数 $ T $。\n3.  计算 $ \\Delta \\mathrm{acc} $ 值列表和 $ 1/T $ 值列表之间的斯皮尔曼等级相关系数 $ \\rho $。\n\n让我们详细说明每个阶段的计算。\n\n**1. 准确率增益 ( $ \\Delta \\mathrm{acc} $ ) 计算**\n\n准确率由 $ \\mathrm{acc} = c/n $ 给出，其中 $ c $ 是正确分类的样本数量，$ n $ 是测试样本总数。准确率增益定义为：\n$$\n\\Delta \\mathrm{acc} = \\mathrm{acc}_{\\mathrm{full}} - \\mathrm{acc}_{\\mathrm{probe}} = \\frac{c_{\\mathrm{full}}}{n_{\\mathrm{test}}} - \\frac{c_{\\mathrm{probe}}}{n_{\\mathrm{test}}}\n$$\n\n**2. 可迁移性分数 ( $ T $ ) 计算**\n\n可迁移性分数 $ T $ 定义为：\n$$\nT = \\frac{2}{K(K-1)} \\sum_{1 \\le i  j \\le K} \\frac{\\| \\mu_i - \\mu_j \\|_2}{\\sqrt{\\operatorname{tr}(S_i)} + \\sqrt{\\operatorname{tr}(S_j)}}\n$$\n鉴于协方差矩阵是各向同性的，$ S_k = s_k^2 I_d $，其迹为 $ \\operatorname{tr}(S_k) = \\operatorname{tr}(s_k^2 I_d) = d s_k^2 $。分母项变为 $ \\sqrt{d s_i^2} + \\sqrt{d s_j^2} = (s_i + s_j)\\sqrt{d} $。\n对于给定的参数 $ K=3 $ 和 $ d=4 $，归一化常数为 $ \\frac{2}{K(K-1)\\sqrt{d}} = \\frac{2}{3(2)\\sqrt{4}} = \\frac{2}{12} = \\frac{1}{6} $。\n求和遍历类别对 $ (1, 2) $、$ (1, 3) $ 和 $ (2, 3) $。我们用例的具体公式是：\n$$\nT = \\frac{1}{6} \\left( \\frac{\\|\\mu_1 - \\mu_2\\|_2}{s_1 + s_2} + \\frac{\\|\\mu_1 - \\mu_3\\|_2}{s_1 + s_3} + \\frac{\\|\\mu_2 - \\mu_3\\|_2}{s_2 + s_3} \\right)\n$$\n其中 $ \\| \\cdot \\|_2 $ 表示欧几里得范数。\n\n**逐个用例计算**\n\n**用例 1：**\n-   $ n_{\\mathrm{test}} = 200 $, $ c_{\\mathrm{full}} = 182 $, $ c_{\\mathrm{probe}} = 176 $。\n-   $ \\Delta \\mathrm{acc}_1 = \\frac{182}{200} - \\frac{176}{200} = 0.91 - 0.88 = 0.03 $。\n-   $ s_1 = 0.50, s_2 = 0.60, s_3 = 0.55 $。\n-   $ \\mu_1 - \\mu_2 = [3.5, -3.2, 0.8, 0.8] $, $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{23.77} \\approx 4.8754 $。\n-   $ \\mu_1 - \\mu_3 = [2.0, -1.0, -2.0, 2.5] $, $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{15.25} \\approx 3.9051 $。\n-   $ \\mu_2 - \\mu_3 = [-1.5, 2.2, -2.8, 1.7] $, $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{17.82} \\approx 4.2214 $。\n-   $ T_1 = \\frac{1}{6} \\left(\\frac{4.8754}{0.5+0.6} + \\frac{3.9051}{0.5+0.55} + \\frac{4.2214}{0.6+0.55}\\right) = \\frac{1}{6} (4.4322 + 3.7192 + 3.6708) \\approx 1.9704 $。\n-   $ 1/T_1 \\approx 0.507522 $。\n\n**用例 2：**\n-   $ n_{\\mathrm{test}} = 200 $, $ c_{\\mathrm{full}} = 158 $, $ c_{\\mathrm{probe}} = 130 $。\n-   $ \\Delta \\mathrm{acc}_2 = \\frac{158 - 130}{200} = \\frac{28}{200} = 0.14 $。\n-   $ s_1 = 1.20, s_2 = 1.00, s_3 = 1.10 $。\n-   $ \\mu_1 - \\mu_2 = [-0.1, 0.1, -0.1, -0.1] $, $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{0.04} = 0.2 $。\n-   $ \\mu_1 - \\mu_3 = [0.1, -0.1, -0.1, 0.1] $, $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{0.04} = 0.2 $。\n-   $ \\mu_2 - \\mu_3 = [0.2, -0.2, 0.0, 0.2] $, $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{0.12} \\approx 0.3464 $。\n-   $ T_2 = \\frac{1}{6} \\left(\\frac{0.2}{1.2+1.0} + \\frac{0.2}{1.2+1.1} + \\frac{0.3464}{1.0+1.1}\\right) = \\frac{1}{6} (0.0909 + 0.0870 + 0.1650) \\approx 0.0571 $。\n-   $ 1/T_2 \\approx 17.502035 $。\n\n**用例 3：**\n-   $ n_{\\mathrm{test}} = 100 $, $ c_{\\mathrm{full}} = 98 $, $ c_{\\mathrm{probe}} = 98 $。\n-   $ \\Delta \\mathrm{acc}_3 = \\frac{98 - 98}{100} = 0.0 $。\n-   $ s_1 = 0.30, s_2 = 0.30, s_3 = 0.30 $。\n-   $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{3^2+(-3)^2} = \\sqrt{18} $, $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{18} $, $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{18} $。\n-   $ T_3 = \\frac{1}{6} \\left(\\frac{\\sqrt{18}}{0.3+0.3} + \\frac{\\sqrt{18}}{0.3+0.3} + \\frac{\\sqrt{18}}{0.3+0.3}\\right) = \\frac{1}{2} \\frac{\\sqrt{18}}{0.6} \\approx 3.5355 $。\n-   $ 1/T_3 \\approx 0.282843 $。\n\n**用例 4：**\n-   $ n_{\\mathrm{test}} = 50 $, $ c_{\\mathrm{full}} = 41 $, $ c_{\\mathrm{probe}} = 35 $。\n-   $ \\Delta \\mathrm{acc}_4 = \\frac{41 - 35}{50} = \\frac{6}{50} = 0.12 $。\n-   $ s_1 = 0.90, s_2 = 0.80, s_3 = 1.00 $。\n-   $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{1.5^2+(-1.5)^2+(-0.5)^2+0.5^2} = \\sqrt{5} \\approx 2.2361 $。\n-   $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{0.5^2+(-1)^2+0.5^2+(-0.5)^2} = \\sqrt{1.75} \\approx 1.3229 $。\n-   $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{(-1)^2+0.5^2+1^2+(-1)^2} = \\sqrt{3.25} \\approx 1.8028 $。\n-   $ T_4 = \\frac{1}{6} \\left(\\frac{\\sqrt{5}}{0.9+0.8} + \\frac{\\sqrt{1.75}}{0.9+1.0} + \\frac{\\sqrt{3.25}}{0.8+1.0}\\right) = \\frac{1}{6} (1.3153 + 0.6963 + 1.0015) \\approx 0.5022 $。\n-   $ 1/T_4 \\approx 1.991286 $。\n\n**用例 5：**\n-   $ n_{\\mathrm{test}} = 500 $, $ c_{\\mathrm{full}} = 450 $, $ c_{\\mathrm{probe}} = 420 $。\n-   $ \\Delta \\mathrm{acc}_5 = \\frac{450 - 420}{500} = \\frac{30}{500} = 0.06 $。\n-   $ s_1 = 0.40, s_2 = 0.45, s_3 = 0.50 $。\n-   $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{2.5^2+2.7^2+(-1)^2} = \\sqrt{14.54} \\approx 3.8131 $。\n-   $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{1.5^2+(-0.5)^2+(-1.5)^2+1^2} = \\sqrt{5.75} \\approx 2.3979 $。\n-   $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{(-1)^2+(-3.2)^2+(-0.5)^2+1^2} = \\sqrt{12.49} \\approx 3.5341 $。\n-   $ T_5 = \\frac{1}{6} \\left(\\frac{3.8131}{0.4+0.45} + \\frac{2.3979}{0.4+0.5} + \\frac{3.5341}{0.45+0.5}\\right) = \\frac{1}{6} (4.4860 + 2.6644 + 3.7201) \\approx 1.8118 $。\n-   $ 1/T_5 \\approx 0.551954 $。\n\n**3. 斯皮尔曼等级相关系数 ( $ \\rho $ ) 计算**\n\n我们计算准确率增益列表 $ X = [\\Delta \\mathrm{acc}_i] $ 与逆可迁移性分数列表 $ Y = [1/T_i] $ 之间的相关性。\n$ X = [0.03, 0.14, 0.00, 0.12, 0.06] $\n$ Y = [0.507522, 17.502035, 0.282843, 1.991286, 0.551954] $\n\n首先，我们必须对每个列表中的数据进行排名。\n-   对 $ X $ 排名：\n    -   $ 0.00 $ (用例 $3$) 排名为 $1$\n    -   $ 0.03 $ (用例 $1$) 排名为 $2$\n    -   $ 0.06 $ (用例 $5$) 排名为 $3$\n    -   $ 0.12 $ (用例 $4$) 排名为 $4$\n    -   $ 0.14 $ (用例 $2$) 排名为 $5$\n    按用例编号（$1$ 到 $5$）排序的 $ X $ 的排名向量为 $ R_x = [2, 5, 1, 4, 3] $。\n\n-   对 $ Y $ 排名：\n    -   $ 0.282843 $ (用例 $3$) 排名为 $1$\n    -   $ 0.507522 $ (用例 $1$) 排名为 $2$\n    -   $ 0.551954 $ (用例 $5$) 排名为 $3$\n    -   $ 1.991286 $ (用例 $4$) 排名为 $4$\n    -   $ 17.502035 $ (用例 $2$) 排名为 $5$\n    按用例编号（$1$ 到 $5$）排序的 $ Y $ 的排名向量为 $ R_y = [2, 5, 1, 4, 3] $。\n\n由于两个列表中都没有相同的值，因此排名是 $ \\{1, 2, 3, 4, 5\\} $ 的一个排列。\n两个列表的平均排名均为 $ \\bar{R} = (1+2+3+4+5)/5 = 3 $。\n排名向量是相同的：$ R_x = R_y $。这表明存在一个完美的单调关系。斯皮尔曼相关系数 $ \\rho $ 是这些排名的皮尔逊相关系数。\n$$\n\\rho = \\frac{\\sum_{i=1}^{m} (R_x(i) - \\bar{R_x})(R_y(i) - \\bar{R_y})}{\\sqrt{\\sum_{i=1}^{m} (R_x(i) - \\bar{R_x})^2} \\sqrt{\\sum_{i=1}^{m} (R_y(i) - \\bar{R_y})^2}}\n$$\n由于对于所有 $ i=1, \\dots, 5 $ 都有 $ R_x(i) = R_y(i) $，分子变为 $ \\sum_{i=1}^{5} (R_x(i) - \\bar{R_x})^2 $，这与分母中平方根内的项相同。\n-   排名的离均差：$ R - \\bar{R} = [-1, 2, -2, 1, 0] $。\n-   离均差平方和：$ \\sum (R(i) - \\bar{R})^2 = (-1)^2 + 2^2 + (-2)^2 + 1^2 + 0^2 = 1+4+4+1+0 = 10 $。\n因此，\n$$\n\\rho = \\frac{10}{\\sqrt{10}\\sqrt{10}} = \\frac{10}{10} = 1.0\n$$\n$ 1.0 $ 的相关系数表示 $ \\Delta \\mathrm{acc} $ 和 $ 1/T $ 之间存在完美的正单调关系。这支持了这样一个假设：全量微调相对于线性探测所获得的更大准确率增益与冻结嵌入较差的可迁移性相关联。\n\n最终结果，格式化为六位小数：\n-   $ \\Delta \\mathrm{acc}_1 = 0.030000 $\n-   $ \\Delta \\mathrm{acc}_2 = 0.140000 $\n-   $ \\Delta \\mathrm{acc}_3 = 0.000000 $\n-   $ \\Delta \\mathrm{acc}_4 = 0.120000 $\n-   $ \\Delta \\mathrm{acc}_5 = 0.060000 $\n-   $ \\rho = 1.000000 $\n\n最终输出是列表 $ [0.030000, 0.140000, 0.000000, 0.120000, 0.060000, 1.000000] $。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes accuracy gains and transferability scores for several Vision Transformer\n    training scenarios, then calculates the Spearman rank correlation between them.\n    \"\"\"\n    \n    # Common parameters for all test cases as specified in the problem statement\n    K_classes = 3\n    d_embedding = 4\n\n    test_cases = [\n        {\n            \"n_test\": 200, \"c_full\": 182, \"c_probe\": 176,\n            \"mu\": np.array([\n                [2.0, -1.0, 0.5, 1.5],\n                [-1.5, 2.2, -0.3, 0.7],\n                [0.0, 0.0, 2.5, -1.0]\n            ]),\n            \"s\": np.array([0.50, 0.60, 0.55]),\n        },\n        {\n            \"n_test\": 200, \"c_full\": 158, \"c_probe\": 130,\n            \"mu\": np.array([\n                [0.5, 0.6, 0.4, 0.5],\n                [0.6, 0.5, 0.5, 0.6],\n                [0.4, 0.7, 0.5, 0.4]\n            ]),\n            \"s\": np.array([1.20, 1.00, 1.10]),\n        },\n        {\n            \"n_test\": 100, \"c_full\": 98, \"c_probe\": 98,\n            \"mu\": np.array([\n                [3.0, 0.0, 0.0, 0.0],\n                [0.0, 3.0, 0.0, 0.0],\n                [0.0, 0.0, 3.0, 0.0]\n            ]),\n            \"s\": np.array([0.30, 0.30, 0.30]),\n        },\n        {\n            \"n_test\": 50, \"c_full\": 41, \"c_probe\": 35,\n            \"mu\": np.array([\n                [1.0, -0.5, 0.0, 0.5],\n                [-0.5, 1.0, 0.5, 0.0],\n                [0.5, 0.5, -0.5, 1.0]\n            ]),\n            \"s\": np.array([0.90, 0.80, 1.00]),\n        },\n        {\n            \"n_test\": 500, \"c_full\": 450, \"c_probe\": 420,\n            \"mu\": np.array([\n                [1.5, 1.5, -0.5, 0.0],\n                [-1.0, -1.2, 0.5, 0.0],\n                [0.0, 2.0, 1.0, -1.0]\n            ]),\n            \"s\": np.array([0.40, 0.45, 0.50]),\n        }\n    ]\n\n    delta_accs = []\n    inv_Ts = []\n\n    for case in test_cases:\n        # 1. Compute Accuracy Gain (delta_acc)\n        acc_full = case[\"c_full\"] / case[\"n_test\"]\n        acc_probe = case[\"c_probe\"] / case[\"n_test\"]\n        delta_acc = acc_full - acc_probe\n        delta_accs.append(delta_acc)\n\n        # 2. Compute Transferability Score (T)\n        mu = case[\"mu\"]\n        s = case[\"s\"]\n        \n        # The transferability score formula can be simplified for the given K and d.\n        # Constant factor: 2 / (K*(K-1)*sqrt(d)) = 2 / (3*2*sqrt(4)) = 1/6\n        constant_factor = 1.0 / (K_classes * (K_classes - 1) * np.sqrt(d_embedding) / 2.0)\n        \n        sum_term = 0\n        for i in range(K_classes):\n            for j in range(i + 1, K_classes):\n                norm_diff = np.linalg.norm(mu[i] - mu[j])\n                sum_s = s[i] + s[j]\n                sum_term += norm_diff / sum_s\n        \n        T = constant_factor * sum_term\n        # The problem requires correlation with 1/T\n        inv_Ts.append(1.0 / T)\n\n    # 3. Compute Spearman Rank Correlation (rho)\n    def rank_data(data):\n        \"\"\"\n        Assigns ranks to data, using average rank for ties.\n        \"\"\"\n        indexed_data = sorted([(val, i) for i, val in enumerate(data)])\n        ranks = [0] * len(data)\n        i = 0\n        while i  len(data):\n            j = i\n            # Find group of ties\n            while j  len(data) - 1 and indexed_data[j][0] == indexed_data[j+1][0]:\n                j += 1\n            # Ranks for this group are from i+1 to j+1\n            avg_rank = sum(range(i + 1, j + 2)) / (j - i + 1)\n            # Assign average rank to all tied elements\n            for k in range(i, j + 1):\n                original_index = indexed_data[k][1]\n                ranks[original_index] = avg_rank\n            i = j + 1\n        return np.array(ranks)\n\n    Rx = rank_data(delta_accs)\n    Ry = rank_data(inv_Ts)\n    \n    # Using the formula for Pearson correlation on the ranks\n    # which is the definition of Spearman correlation given in the problem\n    Rx_dev = Rx - np.mean(Rx)\n    Ry_dev = Ry - np.mean(Ry)\n    \n    numerator = np.sum(Rx_dev * Ry_dev)\n    denominator_x = np.sqrt(np.sum(Rx_dev**2))\n    denominator_y = np.sqrt(np.sum(Ry_dev**2))\n    \n    # Handle case where standard deviation is zero (all ranks are the same)\n    if denominator_x == 0 or denominator_y == 0:\n        rho = 0.0  # Or undefined, but 0 is a common convention\n    else:\n        rho = numerator / (denominator_x * denominator_y)\n\n    # Format the results for the final output\n    results_to_print = [f\"{val:.6f}\" for val in delta_accs]\n    results_to_print.append(f\"{rho:.6f}\")\n    \n    print(f\"[{','.join(results_to_print)}]\")\n\nsolve()\n\n```", "id": "3199207"}]}