{"hands_on_practices": [{"introduction": "这个练习将深入探讨注意力机制的理论基础。我们将分析注意力分数的计算方式如何影响模型在初始化时的统计特性。通过推导乘法注意力分数的方差，我们可以理解为什么像“缩放点积注意力”（scaled dot-product attention）这样的机制对于训练稳定性至关重要，尤其是在高维模型中。这个练习旨在为注意力机制的设计选择建立数学直觉。[@problem_id:3097429]", "problem": "考虑一个带注意力机制的序列到序列模型，其中时间步 $t$ 的解码器状态是一个向量 $s_t \\in \\mathbb{R}^{d_h}$，位置 $i$ 的编码器隐藏状态是一个向量 $h_i \\in \\mathbb{R}^{d_h}$。乘性 (Luong) 能量定义为 $e_i^{\\text{mult}} = s_t^{\\top} W h_i$，其中权重矩阵为 $W \\in \\mathbb{R}^{d_h \\times d_h}$。加性 (Bahdanau) 能量定义为 $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$，其中权重向量为 $v \\in \\mathbb{R}^{d_h}$，权重矩阵为 $W_s, W_h \\in \\mathbb{R}^{d_h \\times d_h}$。假设有以下随机初始化和分布假设：\n- $s_t$ 的分量是独立同分布 (i.i.d.) 的，对于所有 $a \\in \\{1, \\dots, d_h\\}$，$s_{t,a} \\sim \\mathcal{N}(0, \\sigma_s^2)$。\n- $h_i$ 的分量是独立同分布的，对于所有 $b \\in \\{1, \\dots, d_h\\}$，$h_{i,b} \\sim \\mathcal{N}(0, \\sigma_h^2)$。\n- $W$ 的分量是独立同分布的，$W_{ab} \\sim \\mathcal{N}(0, \\sigma_W^2)$，并且独立于 $s_t$ 和 $h_i$。\n- $v$ 的分量是独立同分布的，$v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$，并且独立于所有其他变量。\n- $W_s$ 和 $W_h$ 的分量是独立同分布的，$W_{s,ka} \\sim \\mathcal{N}(0, \\sigma_{W_s}^2)$ 和 $W_{h,kb} \\sim \\mathcal{N}(0, \\sigma_{W_h}^2)$，且独立于 $s_t$、$h_i$ 和 $v$。\n- 所有提到的随机变量在不同索引和不同参数组之间都是相互独立的，双曲正切函数 $\\tanh(\\cdot)$ 逐元素应用。\n从期望和方差的基本定义、独立性性质以及零均值高斯分布的对称性出发，推导乘性能量的期望 $\\mathbb{E}[e_i^{\\text{mult}}]$ 和方差 $\\operatorname{Var}(e_i^{\\text{mult}})$。然后，将你的方差表达式特化到 $W$ 采用 Xavier 正态初始化的情况，其中 $\\sigma_W^2 = 1/d_h$。接下来，在相同的独立性和对称性假设下，推导加性能量的期望 $\\mathbb{E}[e_i^{\\text{add}}]$。明确量化在 Xavier 正态初始化选择下，乘性能量的方差如何依赖于关键维度 $d_h$。你的最终答案必须是一个单一的解析表达式，按顺序整合 $\\mathbb{E}[e_i^{\\text{mult}}]$、在 $\\sigma_W^2 = 1/d_h$ 下的 $\\operatorname{Var}(e_i^{\\text{mult}})$ 和 $\\mathbb{E}[e_i^{\\text{add}}]$ 这三个量。不需要进行数值计算。", "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取已知条件\n- **解码器状态：** $s_t \\in \\mathbb{R}^{d_h}$\n- **编码器状态：** $h_i \\in \\mathbb{R}^{d_h}$\n- **乘性能量：** $e_i^{\\text{mult}} = s_t^{\\top} W h_i$，其中 $W \\in \\mathbb{R}^{d_h \\times d_h}$\n- **加性能量：** $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$，其中 $v \\in \\mathbb{R}^{d_h}$ 且 $W_s, W_h \\in \\mathbb{R}^{d_h \\times d_h}$\n- **分布假设：**\n  - $s_t$ 的分量：$s_{t,a} \\sim \\mathcal{N}(0, \\sigma_s^2)$ 是独立同分布 (i.i.d.) 的。\n  - $h_i$ 的分量：$h_{i,b} \\sim \\mathcal{N}(0, \\sigma_h^2)$ 是 i.i.d. 的。\n  - $W$ 的分量：$W_{ab} \\sim \\mathcal{N}(0, \\sigma_W^2)$ 是 i.i.d. 的。\n  - $v$ 的分量：$v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$ 是 i.i.d. 的。\n  - $W_s$ 的分量：$W_{s,ka} \\sim \\mathcal{N}(0, \\sigma_{W_s}^2)$ 是 i.i.d. 的。\n  - $W_h$ 的分量：$W_{h,kb} \\sim \\mathcal{N}(0, \\sigma_{W_h}^2)$ 是 i.i.d. 的。\n- **独立性：** 所有指定的随机变量都是相互独立的。\n- **特殊条件：** 在部分分析中，假设对 $W$ 进行 Xavier 正态初始化，即 $\\sigma_W^2 = 1/d_h$。\n- **目标：** 推导 $\\mathbb{E}[e_i^{\\text{mult}}]$、$\\operatorname{Var}(e_i^{\\text{mult}})$（及其在 Xavier 初始化下的特化形式）和 $\\mathbb{E}[e_i^{\\text{add}}]$。\n\n### 步骤 2：使用提取的已知条件进行验证\n评估问题的有效性：\n- **科学基础：** 该问题牢固地建立在神经网络的理论分析之上，特别是注意力机制。乘性 (Luong) 和加性 (Bahdanau) 注意力的定义是标准的。对权重和状态采用零均值高斯先验的假设在分析网络初始化行为时很常见。该问题是概率论和数理统计应用于一个成熟的机器学习概念的标准练习。\n- **良构性：** 该问题提供了一套完整的定义、分布假设和独立性标准，这些都是推导所要求的量（期望和方差）所必需的。目标清晰且在数学上无歧义，保证了唯一且有意义的解。\n- **客观性：** 问题使用形式化的数学语言陈述，没有任何主观或有偏见的措辞。\n\n该问题不存在任何列出的缺陷（例如，科学上不健全、不完整、模糊不清）。所有术语都定义良好，前提相互一致。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。现在将提供一个完整的、有理有据的解答。\n\n### 解题推导\n\n**第 1 部分：乘性能量的期望 $\\mathbb{E}[e_i^{\\text{mult}}]$**\n\n乘性能量定义为 $e_i^{\\text{mult}} = s_t^{\\top} W h_i$。这可以写成求和形式：\n$$e_i^{\\text{mult}} = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}$$\n为了求其期望，我们应用期望算子并利用其线性性质：\n$$\\mathbb{E}[e_i^{\\text{mult}}] = \\mathbb{E}\\left[\\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}\\right] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a} W_{ab} h_{i,b}]$$\n问题陈述所有随机变量 $s_{t,a}$、$W_{ab}$ 和 $h_{i,b}$ 是相互独立的。因此，它们乘积的期望等于它们期望的乘积：\n$$\\mathbb{E}[s_{t,a} W_{ab} h_{i,b}] = \\mathbb{E}[s_{t,a}] \\mathbb{E}[W_{ab}] \\mathbb{E}[h_{i,b}]$$\n根据给定的分布假设，所有这些变量都来自零均值高斯分布：\n- $\\mathbb{E}[s_{t,a}] = 0$\n- $\\mathbb{E}[W_{ab}] = 0$\n- $\\mathbb{E}[h_{i,b}] = 0$\n因此，对于每一组索引 $(a,b)$，我们有：\n$$\\mathbb{E}[s_{t,a} W_{ab} h_{i,b}] = 0 \\cdot 0 \\cdot 0 = 0$$\n将此代回求和中，我们得到总期望：\n$$\\mathbb{E}[e_i^{\\text{mult}}] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} 0 = 0$$\n\n**第 2 部分：乘性能量的方差 $\\operatorname{Var}(e_i^{\\text{mult}}]$**\n\n随机变量 $X$ 的方差定义为 $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。由于我们已经证明 $\\mathbb{E}[e_i^{\\text{mult}}] = 0$，方差简化为平方的期望：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\mathbb{E}\\left[ (e_i^{\\text{mult}})^2 \\right] = \\mathbb{E}\\left[ \\left(\\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}\\right)^2 \\right]$$\n我们展开求和的平方：\n$$\\left(\\sum_{a,b} s_{t,a} W_{ab} h_{i,b}\\right)^2 = \\sum_{a,b} \\sum_{c,d} (s_{t,a} W_{ab} h_{i,b}) (s_{t,c} W_{cd} h_{i,d})$$\n其中关于 $a, b, c, d$ 的求和都从 $1$ 到 $d_h$。取期望：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a,b,c,d} \\mathbb{E}[s_{t,a} s_{t,c} W_{ab} W_{cd} h_{i,b} h_{i,d}]$$\n由于独立性，乘积的期望可以分开。如果任何构成项的随机变量指数为 1，则该项的期望将为零，因为所有变量都是零均值的。为了使期望不为零，乘积中的每个随机变量必须与自身配对。这只在索引匹配时发生：\n- $s_{t,a}$ 必须与 $s_{t,c}$ 配对，这要求 $a=c$。\n- $W_{ab}$ 必须与 $W_{cd}$ 配对，这要求 $(a,b)=(c,d)$。\n- $h_{i,b}$ 必须与 $h_{i,d}$ 配对，这要求 $b=d$。\n条件 $(a,b)=(c,d)$ 包含了其他两个条件。因此，当 $(a,b) \\neq (c,d)$ 时，交叉项的期望为零。我们只需要考虑 $a=c$ 和 $b=d$ 的项：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[(s_{t,a} W_{ab} h_{i,b})^2] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a}^2 W_{ab}^2 h_{i,b}^2]$$\n根据独立性，这变为：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a}^2] \\mathbb{E}[W_{ab}^2] \\mathbb{E}[h_{i,b}^2]$$\n对于任何方差为 $\\sigma^2$ 的零均值随机变量 $Z$，我们有 $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) + (\\mathbb{E}[Z])^2 = \\sigma^2 + 0^2 = \\sigma^2$。应用此结论：\n- $\\mathbb{E}[s_{t,a}^2] = \\sigma_s^2$\n- $\\mathbb{E}[W_{ab}^2] = \\sigma_W^2$\n- $\\mathbb{E}[h_{i,b}^2] = \\sigma_h^2$\n将这些代入求和中：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} (\\sigma_s^2 \\sigma_W^2 \\sigma_h^2)$$\n求和内部的项相对于索引 $a$ 和 $b$ 是常数。该求和共有 $d_h \\times d_h = d_h^2$ 个相同的项。\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = d_h^2 \\sigma_s^2 \\sigma_W^2 \\sigma_h^2$$\n\n**第 3 部分：Xavier 初始化的特例以及对 $d_h$ 的依赖性**\n\n问题要求将此结果特化到矩阵 $W$ 采用 Xavier 正态初始化的情况，其中 $\\sigma_W^2 = 1/d_h$。将此代入我们的方差表达式中：\n$$\\operatorname{Var}(e_i^{\\text{mult}})_{\\text{Xavier}} = d_h^2 \\sigma_s^2 \\left(\\frac{1}{d_h}\\right) \\sigma_h^2 = d_h \\sigma_s^2 \\sigma_h^2$$\n这个结果明确量化了能量方差对隐藏维度 $d_h$ 的依赖性。在 Xavier 初始化下，乘性注意力能量的方差随 $d_h$ 线性增长。这种缩放行为是“缩放点积注意力”机制的一个主要动机，该机制将能量除以 $\\sqrt{d_k}$（其中 $d_k$ 是键维度，此处等同于 $d_h$）以抵消这种增长并在训练期间稳定梯度。\n\n**第 4 部分：加性能量的期望 $\\mathbb{E}[e_i^{\\text{add}}]$**\n\n加性能量定义为 $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$。以求和形式表示：\n$$e_i^{\\text{add}} = \\sum_{k=1}^{d_h} v_k \\tanh((W_s s_t + W_h h_i)_k)$$\n其中下标 $k$ 表示结果向量的第 $k$ 个元素。令 $z_k = \\tanh((W_s s_t + W_h h_i)_k)$。表达式变为 $e_i^{\\text{add}} = \\sum_{k=1}^{d_h} v_k z_k$。\n为了求期望，我们再次使用线性性：\n$$\\mathbb{E}[e_i^{\\text{add}}] = \\mathbb{E}\\left[\\sum_{k=1}^{d_h} v_k z_k\\right] = \\sum_{k=1}^{d_h} \\mathbb{E}[v_k z_k]$$\n项 $z_k$ 是 $s_t$、$h_i$、$W_s$ 和 $W_h$ 中随机变量的函数。问题陈述向量 $v$ 的分量独立于所有其他变量。因此，对于每个 $k$，$v_k$ 独立于 $z_k$。这使我们能够分离期望：\n$$\\mathbb{E}[v_k z_k] = \\mathbb{E}[v_k] \\mathbb{E}[z_k]$$\n我们已知 $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$，这意味着 $\\mathbb{E}[v_k] = 0$。\n因此，对于每个 $k$：\n$$\\mathbb{E}[v_k z_k] = 0 \\cdot \\mathbb{E}[z_k] = 0$$\n无论 $\\mathbb{E}[z_k]$ 的值是多少，这都成立。总期望是：\n$$\\mathbb{E}[e_i^{\\text{add}}] = \\sum_{k=1}^{d_h} 0 = 0$$\n值得注意的是，$\\mathbb{E}[z_k]$ 也为零。$\\tanh$ 函数的参数，我们称之为 $u_k = (W_s s_t + W_h h_i)_k$，是独立的零均值随机变量乘积之和。因此，其分布关于零对称。由于 $\\tanh(x)$ 是一个奇函数（$\\tanh(-x) = -\\tanh(x)$），在一个对称分布上 $\\mathbb{E}[\\tanh(u_k)]$ 的期望必然为零。然而，$v$ 的独立性及其零均值提供了得出结果的最直接路径。\n\n### 结果总结\n1.  **乘性能量的期望：** $\\mathbb{E}[e_i^{\\text{mult}}] = 0$\n2.  **乘性能量的方差（Xavier 初始化）：** $\\operatorname{Var}(e_i^{\\text{mult}}) = d_h \\sigma_s^2 \\sigma_h^2$\n3.  **加性能量的期望：** $\\mathbb{E}[e_i^{\\text{add}}] = 0$\n这三个量被收集到一个行矩阵中作为最终答案。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & d_h \\sigma_s^2 \\sigma_h^2 & 0\n\\end{pmatrix}\n}\n$$", "id": "3097429"}, {"introduction": "从理论转向具体实例，这个练习使用一个简单的一维场景来突显加法和乘法注意力之间一个根本的行为差异。我们将研究当查询向量的模长（代表其“重要性”）增加时，每种机制如何响应。这个动手编程任务将通过数值和可视化方式展示加法注意力的饱和效应，并将其与乘法注意力的缩放特性进行对比，揭示它们在表达能力上的关键权衡。[@problem_id:3097423]", "problem": "考虑一个带有编码器状态注意力机制的序列到序列模型，专注于单个标量维度，以隔离解码器状态中量级的作用。设编码器隐藏状态为三个标量 $h_{1}$、$h_{2}$ 和 $h_{3}$，每个都属于 $\\mathbb{R}$，并设时间步 $t$ 的解码器状态为一个标量 $s_{t} \\in \\mathbb{R}$。$s_{t}$ 的量级编码了重要性：更大的 $|s_{t}|$ 应导致对与 $s_{t}$ 符号一致的输入产生更具选择性的注意力。我们比较两种注意力分数的构造方法：加性注意力（Bahdanau）和乘性注意力（Luong）。\n\n使用以下基础定义和参数化设置：\n\n- 对于一个实值能量向量 $e \\in \\mathbb{R}^{n}$，带有温度 $\\tau$ 的 Softmax 函数定义为\n$$\n\\alpha_{i} = \\frac{\\exp\\left(\\frac{e_{i}}{\\tau}\\right)}{\\sum_{j=1}^{n} \\exp\\left(\\frac{e_{j}}{\\tau}\\right)}.\n$$\n在所有计算中，设置 $\\tau = 1$。\n\n- 加性注意力能量使用一个带有双曲正切非线性的单隐藏层前馈构造：\n$$\ne_{i}^{\\mathrm{add}} = v \\cdot \\tanh\\left(w_{h} h_{i} + w_{s} s_{t} + b\\right),\n$$\n其中标量参数固定为 $v = 1$、$w_{h} = 1$、$w_{s} = 10$ 和 $b = 0$。\n\n- 乘性注意力能量使用一种双线性（缩放点积）形式：\n$$\ne_{i}^{\\mathrm{mul}} = s_{t} \\cdot W \\cdot h_{i},\n$$\n其中标量参数固定为 $W = 1$。\n\n设编码器状态为\n$$\nh_{1} = 1, \\quad h_{2} = -1, \\quad h_{3} = 0.\n$$\n将 $h_{1}$ 解释为正值 $s_{t}$ 的对齐状态，因为它们的符号一致。\n\n对于每种能量构造，使用上述 Softmax 定义将能量 $\\{e_{i}\\}$ 转换为注意力权重 $\\{\\alpha_{i}\\}$。在给定的构造下，分配给 $h_{1}$ 的注意力权重是相应的 $\\alpha_{1}$。\n\n测试套件规范（每个测试用例指定一个 $s_{t}$ 的值）：\n- 用例 1：$s_{t} = 0$（没有量级的边界情况）。\n- 用例 2：$s_{t} = 0.1$（小量级）。\n- 用例 3：$s_{t} = 1$（中等量级）。\n- 用例 4：$s_{t} = 5$（大量级）。\n- 用例 5：$s_{t} = 50$（极大量级）。\n\n对每个用例，计算：\n- $h_{1}$ 上的乘性注意力权重，记为 $w^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}}$。\n- $h_{1}$ 上的加性注意力权重，记为 $w^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}}$。\n\n然后，在整个测试套件上评估以下布尔属性：\n- $B_{\\mathrm{mul}}$：在整个测试套件中，$w^{\\mathrm{mul}}(s_{t})$ 随着 $s_{t}$ 的增加而非递减。形式上，对于有序集 $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$，检查 $w^{\\mathrm{mul}}(0) \\le w^{\\mathrm{mul}}(0.1) \\le w^{\\mathrm{mul}}(1) \\le w^{\\mathrm{mul}}(5) \\le w^{\\mathrm{mul}}(50)$。\n- $B_{\\mathrm{add}}$：加性注意力在 $|s_{t}|$ 较大时饱和并丢失量级信息，这可以通过在大量级下权重的近似恒定来量化。检查大量级和极大量级情况之间的变化是否可以忽略不计：\n$$\n\\left| w^{\\mathrm{add}}(50) - w^{\\mathrm{add}}(5) \\right| \\le \\epsilon,\n$$\n其中 $\\epsilon = 10^{-12}$。\n\n你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含：\n- 对于 $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$ 的五个 $w^{\\mathrm{mul}}(s_{t})$ 值。\n- 对于 $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$ 的五个 $w^{\\mathrm{add}}(s_{t})$ 值。\n- 布尔值 $B_{\\mathrm{mul}}$。\n- 布尔值 $B_{\\mathrm{add}}$。\n\n例如，输出行的格式为\n$$\n[\\underbrace{w^{\\mathrm{mul}}(0), w^{\\mathrm{mul}}(0.1), w^{\\mathrm{mul}}(1), w^{\\mathrm{mul}}(5), w^{\\mathrm{mul}}(50)}_{\\text{五个乘性权重}}, \\underbrace{w^{\\mathrm{add}}(0), w^{\\mathrm{add}}(0.1), w^{\\mathrm{add}}(1), w^{\\mathrm{add}}(5), w^{\\mathrm{add}}(50)}_{\\text{五个加性权重}}, B_{\\mathrm{mul}}, B_{\\mathrm{add}} ].\n$$\n所有值都是无单位的，不涉及任何物理单位。权重的结果是浮点数，最后两个属性的结果是布尔值。", "solution": "用户提供的问题被评估为有效。它在科学上基于神经注意力机制的原理，定义和参数齐全，问题适定，且其表述是客观的。该问题要求对加性（Bahdanau 风格）和乘性（Luong 风格）注意力进行定量比较，重点关注每种机制如何响应解码器状态量级的变化。我们现在将给出一个完整且有理有据的解答。\n\n问题的核心在于计算注意力权重，这些权重是通过 Softmax 函数从一组能量分数中导出的。对于一组能量 $\\{e_{1}, e_{2}, \\dots, e_{n}\\}$，相应的注意力权重 $\\{\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{n}\\}$ 由下式给出：\n$$\n\\alpha_{i} = \\frac{\\exp\\left(\\frac{e_{i}}{\\tau}\\right)}{\\sum_{j=1}^{n} \\exp\\left(\\frac{e_{j}}{\\tau}\\right)}\n$$\n问题指定温度 $\\tau = 1$。编码器隐藏状态为标量 $h_{1} = 1$、$h_{2} = -1$ 和 $h_{3} = 0$。我们将为测试套件 $\\{0, 0.1, 1, 5, 50\\}$ 中解码器状态 $s_{t}$ 的每个指定值计算注意力权重 $\\alpha_{1}$。\n\n**1. 乘性注意力分析**\n\n乘性注意力能量由双线性形式 $e_{i}^{\\mathrm{mul}} = s_{t} \\cdot W \\cdot h_{i}$ 定义。当标量参数 $W = 1$ 时，该式简化为：\n$$\ne_{i}^{\\mathrm{mul}} = s_{t} h_{i}\n$$\n对于给定的编码器状态，能量为：\n- $e_{1}^{\\mathrm{mul}} = s_{t} \\cdot (1) = s_{t}$\n- $e_{2}^{\\mathrm{mul}} = s_{t} \\cdot (-1) = -s_{t}$\n- $e_{3}^{\\mathrm{mul}} = s_{t} \\cdot (0) = 0$\n\n第一个编码器状态上的注意力权重 $w^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}}$ 使用 Softmax 函数计算如下：\n$$\nw^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}} = \\frac{\\exp(e_{1}^{\\mathrm{mul}})}{\\exp(e_{1}^{\\mathrm{mul}}) + \\exp(e_{2}^{\\mathrm{mul}}) + \\exp(e_{3}^{\\mathrm{mul}})} = \\frac{\\exp(s_{t})}{\\exp(s_{t}) + \\exp(-s_{t}) + \\exp(0)} = \\frac{\\exp(s_{t})}{\\exp(s_{t}) + \\exp(-s_{t}) + 1}\n$$\n我们为测试套件中的每个 $s_{t}$ 计算此值：\n- 对于 $s_{t} = 0$：$w^{\\mathrm{mul}}(0) = \\frac{\\exp(0)}{\\exp(0) + \\exp(0) + 1} = \\frac{1}{1 + 1 + 1} = \\frac{1}{3} \\approx 0.3333333333333333$\n- 对于 $s_{t} = 0.1$：$w^{\\mathrm{mul}}(0.1) = \\frac{\\exp(0.1)}{\\exp(0.1) + \\exp(-0.1) + 1} \\approx 0.3670997184200147$\n- 对于 $s_{t} = 1$：$w^{\\mathrm{mul}}(1) = \\frac{\\exp(1)}{\\exp(1) + \\exp(-1) + 1} \\approx 0.6652409557224216$\n- 对于 $s_{t} = 5$：$w^{\\mathrm{mul}}(5) = \\frac{\\exp(5)}{\\exp(5) + \\exp(-5) + 1} \\approx 0.9932620531633535$\n- 对于 $s_{t} = 50$：$w^{\\mathrm{mul}}(50) = \\frac{\\exp(50)}{\\exp(50) + \\exp(-50) + 1} \\approx 0.9999999999999999$\n\n**2. 加性注意力分析**\n\n加性注意力能量由 $e_{i}^{\\mathrm{add}} = v \\cdot \\tanh(w_{h} h_{i} + w_{s} s_{t} + b)$ 定义。当参数为 $v = 1$、$w_{h} = 1$、$w_{s} = 10$ 和 $b = 0$ 时，该式变为：\n$$\ne_{i}^{\\mathrm{add}} = \\tanh(h_{i} + 10s_{t})\n$$\n对于给定的编码器状态，能量为：\n- $e_{1}^{\\mathrm{add}} = \\tanh(1 + 10s_{t})$\n- $e_{2}^{\\mathrm{add}} = \\tanh(-1 + 10s_{t})$\n- $e_{3}^{\\mathrm{add}} = \\tanh(0 + 10s_{t}) = \\tanh(10s_{t})$\n\n注意力权重 $w^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}}$ 为：\n$$\nw^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}} = \\frac{\\exp(\\tanh(1 + 10s_{t}))}{\\exp(\\tanh(1 + 10s_{t})) + \\exp(\\tanh(-1 + 10s_{t})) + \\exp(\\tanh(10s_{t}))}\n$$\n我们为每个 $s_{t}$ 计算此值：\n- 对于 $s_{t} = 0$：$e_{1} = \\tanh(1)$，$e_{2} = \\tanh(-1)$，$e_{3} = \\tanh(0) = 0$。\n  $w^{\\mathrm{add}}(0) = \\frac{\\exp(\\tanh(1))}{\\exp(\\tanh(1)) + \\exp(-\\tanh(1)) + 1} \\approx 0.5052857441183307$\n- 对于 $s_{t} = 0.1$：$e_{1} = \\tanh(2)$，$e_{2} = \\tanh(0) = 0$，$e_{3} = \\tanh(1)$。\n  $w^{\\mathrm{add}}(0.1) = \\frac{\\exp(\\tanh(2))}{\\exp(\\tanh(2)) + \\exp(0) + \\exp(\\tanh(1))} \\approx 0.44917951253013896$\n- 对于 $s_{t} = 1$：$e_{1} = \\tanh(11)$，$e_{2} = \\tanh(9)$，$e_{3} = \\tanh(10)$。\n  $w^{\\mathrm{add}}(1) = \\frac{\\exp(\\tanh(11))}{\\exp(\\tanh(11)) + \\exp(\\tanh(9)) + \\exp(\\tanh(10))} \\approx 0.3333333333333333$\n- 对于 $s_{t} = 5$：$e_{1} = \\tanh(51)$，$e_{2} = \\tanh(49)$，$e_{3} = \\tanh(50)$。\n  $w^{\\mathrm{add}}(5) \\approx 0.3333333333333333$\n- 对于 $s_{t} = 50$：$e_{1} = \\tanh(501)$，$e_{2} = \\tanh(499)$，$e_{3} = \\tanh(500)$。\n  $w^{\\mathrm{add}}(50) \\approx 0.3333333333333333$\n\n对于较大的 $s_{t}$ 的行为可以通过双曲正切函数的饱和特性来解释。对于一个大的正值参数 $x$，$\\tanh(x) \\approx 1$。当 $s_{t} = 5$ 和 $s_{t} = 50$ 时，$\\tanh$ 函数的参数是大的正数（分别为 $49, 50, 51$ 和 $499, 500, 501$）。因此，所有三个能量 $e_{1}^{\\mathrm{add}}, e_{2}^{\\mathrm{add}}, e_{3}^{\\mathrm{add}}$ 都极度接近于 $1$。当所有能量几乎相同时，Softmax 函数会几乎均匀地分配概率质量，导致每个权重都趋近于 $\\frac{1}{3}$。这表明在大量级下，加性注意力丧失了对输入量级的敏感度，这是饱和非线性带来的直接结果。\n\n**3. 布尔属性评估**\n\n- **$B_{\\mathrm{mul}}$**：此属性检查对于 $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$，$w^{\\mathrm{mul}}(s_{t})$ 是否非递减。函数 $f(x) = \\frac{\\exp(x)}{\\exp(x) + \\exp(-x) + 1}$ 的导数是 $f'(x) = \\frac{\\exp(x) + 2}{(\\exp(x) + \\exp(-x) + 1)^2}$，对于所有实数 $x$，该导数严格为正。因此，$w^{\\mathrm{mul}}(s_{t})$ 是 $s_{t}$ 的一个严格递增函数，这满足了非递减条件。计算出的值证实了这一点：$0.333... \\le 0.367... \\le 0.665... \\le 0.993... \\le 0.999...$。因此，$B_{\\mathrm{mul}}$ 为真。\n\n- **$B_{\\mathrm{add}}$**：此属性检查 $|w^{\\mathrm{add}}(50) - w^{\\mathrm{add}}(5)| \\le \\epsilon$ 是否成立，其中 $\\epsilon = 10^{-12}$。如前所述，对于 $s_t=5$ 和 $s_t=50$，$\\tanh$ 函数的所有参数都很大，导致能量饱和到极度接近 $1$ 的值。这些参数的微小差异（例如，$\\tanh(49)$ 和 $\\tanh(499)$ 之间的差异）导致能量上的差异是指数级小的。随后应用 $\\exp$ 函数和 Softmax 归一化，得到的 $s_{t}=5$ 和 $s_{t}=50$ 的注意力权重实际上是相同的。数值计算证实了它们的差值远低于阈值 $\\epsilon = 10^{-12}$。因此，$B_{\\mathrm{add}}$ 为真。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and verifies properties of additive and multiplicative attention\n    for a given set of encoder and decoder states.\n    \"\"\"\n    # Define givens from the problem statement\n    h = np.array([1, -1, 0])\n    s_t_values = [0, 0.1, 1, 5, 50]\n    tau = 1.0\n\n    # Parameters for additive attention\n    v_add = 1.0\n    w_h_add = 1.0\n    w_s_add = 10.0\n    b_add = 0.0\n\n    # Parameter for multiplicative attention\n    W_mul = 1.0\n    \n    epsilon = 1e-12\n\n    # Lists to store results\n    w_mul_results = []\n    w_add_results = []\n\n    def softmax(energies, temperature):\n        \"\"\"Computes softmax probabilities for a vector of energies.\"\"\"\n        # The temperature is given as 1, so division is nominal.\n        # Stabilize by subtracting the max energy.\n        e_stable = energies / temperature\n        e_stable = e_stable - np.max(e_stable)\n        exps = np.exp(e_stable)\n        return exps / np.sum(exps)\n\n    # Loop through each test case for s_t\n    for s_t in s_t_values:\n        # 1. Multiplicative Attention Calculation\n        energies_mul = s_t * W_mul * h\n        alphas_mul = softmax(energies_mul, tau)\n        w_mul = alphas_mul[0]\n        w_mul_results.append(w_mul)\n\n        # 2. Additive Attention Calculation\n        args_add = w_h_add * h + w_s_add * s_t + b_add\n        energies_add = v_add * np.tanh(args_add)\n        alphas_add = softmax(energies_add, tau)\n        w_add = alphas_add[0]\n        w_add_results.append(w_add)\n\n    # 3. Boolean Property Evaluation\n    # B_mul: Check if w_mul is nondecreasing\n    B_mul = all(w_mul_results[i] = w_mul_results[i+1] for i in range(len(w_mul_results) - 1))\n\n    # B_add: Check for saturation at large magnitudes\n    B_add = abs(w_add_results[4] - w_add_results[3]) = epsilon\n    \n    # Combine all results into a single list\n    final_results = w_mul_results + w_add_results + [B_mul, B_add]\n\n    # Format the output as specified\n    formatted_results = []\n    for r in final_results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        else:\n            # Format floats to a high precision to show stability\n            formatted_results.append(f\"{r:.16f}\")\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "3097423"}, {"introduction": "最后一个练习鼓励我们对注意力机制的组成部分进行批判性思考。我们将探讨一个假设情景：如果我们将加法注意力中标准的双曲正切（$\\tanh$）激活函数替换为修正线性单元（$\\text{ReLU}$），会发生什么？这个思想实验将挑战你对模型对称性、梯度流以及是否需要引入偏置项等其他架构调整的后果进行推理。这个练习旨在让你不仅理解注意力是如何工作的，更能理解其各个组成部分如何为整体功能做出贡献。[@problem_id:3097395]", "problem": "给定一个带有加性 (Bahdanau) 注意力机制的编码器-解码器序列模型。设源索引 $i$ 处的编码器隐藏状态为 $h_i \\in \\mathbb{R}^{d_h}$，目标时间 $t$ 处的解码器状态为 $s_{t-1} \\in \\mathbb{R}^{d_s}$。加性注意力分数定义为\n$$\ne_{t,i} \\;=\\; v^{\\top}\\,\\phi\\!\\left(W_h\\,h_i \\;+\\; W_s\\,s_{t-1} \\;+\\; b\\right),\n$$\n其中 $W_h \\in \\mathbb{R}^{d_a \\times d_h}$，$W_s \\in \\mathbb{R}^{d_a \\times d_s}$，$v \\in \\mathbb{R}^{d_a}$，$b \\in \\mathbb{R}^{d_a}$ 是可训练参数，$\\phi$ 是一个逐点非线性函数。注意力权重为 $\\alpha_{t,i} = \\exp(e_{t,i}) / \\sum_j \\exp(e_{t,j})$，上下文向量为 $c_t = \\sum_i \\alpha_{t,i} h_i$。在标准形式中，$\\phi(x) = \\tanh(x)$。考虑将 $\\phi$ 替换为修正线性单元 (Rectified Linear Unit) $\\phi(x) = \\mathrm{ReLU}(x) = \\max\\{0, x\\}$。\n\n假设在初始化时满足以下条件：\n- 输入投影产生的预激活值 $z_{t,i} = W_h h_i + W_s s_{t-1} + b$ 的各个坐标近似独立，均值为零，关于 $0$ 对称，且具有非零方差（例如，近似为均值为 $0$ 的高斯分布），这是由于 $W_h$、$W_s$ 的零均值随机初始化、$b = 0$ 以及零均值的 $h_i$、$s_{t-1}$。\n- 梯度通过链式法则传播，且 $\\tanh$ 和 $\\mathrm{ReLU}$ 的导数是标准导数。\n\n仅使用上述定义以及 $\\tanh$ 和 $\\mathrm{ReLU}$ 的基本性质，推断在加性注意力评分网络中将 $\\tanh$ 换成 $\\mathrm{ReLU}$ 会如何改变 (i) 中间表示的对称性及其对分数贡献的正负平衡的影响，(ii) 通过 $\\phi$ 反向传播的梯度的分布，以及 (iii) 是否可能需要在 $\\phi$ 之前设置一个偏置项以避免退化的学习动态。\n\n在所述假设下，以下哪些陈述是正确的？\n\nA. 对于零均值对称的预激活值 $z_{t,i}$，用 $\\mathrm{ReLU}$ 替换 $\\tanh$ 会使得 $\\phi(z_{t,i})$ 的每个坐标都具有严格为正的均值，并消除了奇对称性 $\\phi(-x) = -\\phi(x)$。相比之下，使用 $\\tanh$ 时，激活值近似为零均值且是奇对称的。这消除了对分数 $e_{t,i}$ 贡献的符号对称性，除非参数或偏置重新中心化预激活值。\n\nB. 与 $\\tanh$ 相比，$\\mathrm{ReLU}$ 到处都消除了梯度消失问题，因为它的导数要么是 $0$ 要么是 $1$，所以梯度通过所有单元时都能保持不变地反向传播。\n\nC. 在 $\\mathrm{ReLU}$ 前没有偏置项且初始化时预激活值为对称零均值的情况下，大约 $1/2$ 的单元将处于非激活状态，导数为零，这会减慢学习速度；引入一个正偏置可以改变预激活值的分布，使得更大部分的单元处于线性区域并接收到非零梯度。\n\nD. 因为 $\\mathrm{softmax}$ 对于在给定时间步将所有分数加上同一个标量是不变的，所以由 $\\mathrm{ReLU}$ 引入的非零中心化问题总能通过在计算 $e_{t,i}$ 后加上一个标量偏置来完美抵消；因此在评分网络内部不需要任何偏置。\n\nE. 在加性注意力评分网络中从 $\\tanh$ 切换到 $\\mathrm{ReLU}$，对于合适的参数选择，会使其等效于乘性 (Luong) 注意力机制，因此两种机制对所有输入计算出的分数都相同。", "solution": "对用户的问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **模型：** 带有加性 (Bahdanau) 注意力机制的编码器-解码器。\n- **编码器隐藏状态：** 在源索引 $i$ 处的 $h_i \\in \\mathbb{R}^{d_h}$。\n- **解码器状态：** 在目标时间 $t$ 处的 $s_{t-1} \\in \\mathbb{R}^{d_s}$。\n- **注意力分数函数：** $e_{t,i} = v^{\\top}\\,\\phi\\!\\left(W_h\\,h_i + W_s\\,s_{t-1} + b\\right)$。\n- **参数：** $W_h \\in \\mathbb{R}^{d_a \\times d_h}$，$W_s \\in \\mathbb{R}^{d_a \\times d_s}$，$v \\in \\mathbb{R}^{d_a}$，$b \\in \\mathbb{R}^{d_a}$。\n- **激活函数：** $\\phi$ 是一个逐点非线性函数。\n- **参考激活函数：** $\\phi(x) = \\tanh(x)$。\n- **提议的激活函数：** $\\phi(x) = \\mathrm{ReLU}(x) = \\max\\{0, x\\}$。\n- **注意力权重：** $\\alpha_{t,i} = \\exp(e_{t,i}) / \\sum_j \\exp(e_{t,j})$。\n- **上下文向量：** $c_t = \\sum_i \\alpha_{t,i} h_i$。\n- **初始化假设：**\n    - 预激活值 $z_{t,i} = W_h h_i + W_s s_{t-1} + b$ 的坐标近似独立，均值为零，关于 $0$ 对称，且具有非零方差。\n    - 这是由于 $W_h$、$W_s$ 的零均值随机初始化，$b=0$，以及零均值的 $h_i$、$s_{t-1}$ 导致的。\n    - 梯度通过链式法则计算，使用 $\\tanh$ 和 $\\mathrm{ReLU}$ 的标准导数。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题陈述在科学上是合理的、适定的和客观的。它清晰、形式化地描述了 Bahdanau 注意力机制，并就用 $\\mathrm{ReLU}$ 替代标准 $\\tanh$ 激活函数提出了具体问题。关于初始化时预激活值分布的假设在神经网络分析中是标准的。该问题没有违反任何基本原则，没有歧义或矛盾。这是深度学习领域一个有效的理论问题。\n\n### 步骤 3：结论与行动\n问题有效。将对各选项进行全面评估。\n\n### 解题推导\n\n令 $z = z_{t,i}$ 为预激活向量。根据假设，在初始化时 $b=0$，并且 $z$ 的每个分量（记为 $z_k$）都是从一个关于 $0$ 对称且均值为 $0$ 的分布中抽取的随机变量。我们用 $Z$ 表示这个泛型随机变量。其概率密度函数 $f_Z(x)$ 满足 $E[Z] = 0$ 和 $f_Z(x) = f_Z(-x)$。我们来分析激活函数 $\\phi(z)$ 的性质及其对网络的影响。\n\n**陈述 A 的分析：激活值的对称性与均值**\n\n- **使用 $\\phi(x) = \\tanh(x)$：** $\\tanh$ 函数是奇函数，即 $\\tanh(-x) = -\\tanh(x)$。对于一个对称的零均值输入随机变量 $Z$，输出 $Y = \\tanh(Z)$ 也是对称且零均值的。其期望为 $E[Y] = \\int_{-\\infty}^{\\infty} \\tanh(x) f_Z(x) dx$。由于 $\\tanh(x)$ 是奇函数，$f_Z(x)$ 是偶函数，它们的乘积是奇函数，在一个对称区间上的积分为 $0$。因此，激活向量 $\\phi(z)$ 的每个坐标的均值近似为 $0$。激活值可以是正数或负数，保持了符号对称性。\n\n- **使用 $\\phi(x) = \\mathrm{ReLU}(x)$：** $\\mathrm{ReLU}$ 函数是 $\\max\\{0, x\\}$。它不是奇函数。对于如上所述的输入 $Z$，输出 $Y = \\mathrm{ReLU}(Z)$ 总是非负的。其期望为 $E[Y] = \\int_{-\\infty}^{\\infty} \\max\\{0, x\\} f_Z(x) dx = \\int_{0}^{\\infty} x f_Z(x) dx$。由于 $Z$ 的方差非零，$f_Z(x)$ 不会集中在 $x=0$ 处，所以这个积分严格为正。因此，$\\phi(z)$ 的每个坐标都具有严格为正的均值。奇对称性丢失了。激活值都是非负的，这消除了使用 $\\tanh$ 时中间表示中存在的符号对称性。虽然如果权重 $v_k$ 的符号有正有负，对分数 $e_{t,i} = \\sum_k v_k \\phi(z_k)$ 的最终贡献可能会被平衡，但激活值 $\\phi(z_k)$ 本身已经失去了这个性质。如果没有偏置来移动预激活值，激活值中这种系统性的正向偏移会持续存在。\n\n陈述 A 准确地捕捉到了中间表示统计特性的这一根本变化。\n\n**陈述 B 的分析：梯度传播**\n\n- **使用 $\\phi(x) = \\tanh(x)$：** 其导数为 $\\phi'(x) = 1 - \\tanh^2(x)$。该导数始终在范围 $(0, 1]$ 内。当 $|x| \\gg 0$ 时，神经元饱和，$\\tanh^2(x) \\to 1$，梯度 $\\phi'(x) \\to 0$。这就是经典的梯度消失问题。\n\n- **使用 $\\phi(x) = \\mathrm{ReLU}(x)$：** 当 $x  0$ 时，导数为 $\\phi'(x) = 1$；当 $x \\le 0$ 时，导数为 $\\phi'(x) = 0$。对于激活的单元（$x0$），梯度会原封不动地通过，这在这些路径上缓解了梯度消失问题。然而，对于未激活的单元（$x \\le 0$），梯度被完全置零。这通常被称为“死亡 ReLU”问题。$\\mathrm{ReLU}$“到处都消除了梯度消失问题”的说法是错误的；它为未激活的单元引入了更严重的梯度阻断问题。“梯度通过所有单元时都能保持不变地反向传播”的说法也是错误的，因为对于任何预激活值为负的单元，梯度都会被阻断。\n\n陈述 B 对通过 $\\mathrm{ReLU}$ 网络的梯度传播做出了一个过于强烈且不正确的断言。\n\n**陈述 C 的分析：$\\mathrm{ReLU}$ 中偏置项的角色**\n\n- 根据初始化假设，预激活值 $z_k$ 从一个均值为 $0$ 的对称分布中抽取。这意味着 $P(z_k  0) = 1/2$。\n- 对于每个预激活值 $z_k$ 为负的单元，$\\mathrm{ReLU}$ 激活值为 $0$，其导数也为 $0$。因此，在初始化时，注意力机制隐藏层中大约 $1/2$ 的单元将是“非激活的”，并且相对于其输入的梯度为零。这意味着与这些单元相关的权重（$W_h, W_s$）对于给定的输入将不会被更新，这会减慢学习过程。\n- 如果为每个单元引入一个正偏置项 $b_k  0$（或者如果共享偏置向量 $b$ 的分量为正），预激活值变为 $z'_k = (W_h h_i + W_s s_{t-1})_k + b_k$。这个新预激活值的均值为 $E[z'_k] = b_k  0$。对于一个具有正均值的分布，抽到一个正数的概率大于 $1/2$（即 $P(z'_k  0)  1/2$）。这增加了激活单元的比例，允许更多梯度传播和更多权重更新，从而可能加速学习，尤其是在开始阶段。\n\n陈述 C 正确地诊断了初始化时神经元失活的问题，并准确地描述了使用正偏置项作为补救措施的效用。\n\n**陈述 D 的分析：用后置分数偏置抵消 ReLU 效应**\n\n- $\\mathrm{softmax}$ 函数确实对其输入的常量平移不敏感：$\\mathrm{softmax}(e_{t,1}+C, e_{t,2}+C, \\ldots) = \\mathrm{softmax}(e_{t,1}, e_{t,2}, \\ldots)$。\n- 在 A 和 C 中讨论的使用 $\\mathrm{ReLU}$ 的问题，不是关于 $\\alpha_{t,i}$ 的最终值，而是关于评分网络的内部动态，特别是激活值的统计特性（均值偏移、对称性丧失）和梯度流（神经元死亡）。\n- 在计算分数 $e_{t,i}$ *之后* 添加偏置，对计算 $e_{t,i}$ 关于网络参数 $W_h, W_s, b$ 的梯度没有任何影响。梯度路径在 $\\mathrm{ReLU}$ 单元未激活时被剪枝，这是在评分网络*内部*决定的。对最终分数进行事后调整无法修复这些断裂的梯度路径。\n- 因此，声称外部偏置可以消除网络*内部*（参数 $b$）偏置的需求是错误的。内部偏置 $b$对于控制 $\\mathrm{ReLU}$ 非线性函数的工作点和确保健康的梯度流至关重要，这与添加到最终分数的任何偏置的功能完全不同。\n\n陈述 D 将最终 softmax 层的属性与评分网络的内部学习动态混淆了。\n\n**陈述 E 的分析：与乘性注意力的等价性**\n\n- 带有 $\\mathrm{ReLU}$ 的加性注意力分数为 $e_{t,i} = v^{\\top}\\,\\mathrm{ReLU}(W_h h_i + W_s s_{t-1} + b)$。由于 $\\mathrm{ReLU}$ 的分段线性特性，这是 $h_i$ 和 $s_{t-1}$ 的一个非线性函数。具体来说，它表示一个仿射变换和一个非线性激活的复合，然后是另一个线性变换。这是一个小型的、单隐藏层的前馈网络的结构。\n- “通用”的乘性 (Luong) 注意力分数为 $e_{t,i} = s_{t-1}^{\\top} W_a h_i$。这是 $s_{t-1}$ 和 $h_i$ 的一个双线性函数。\n- 一般而言，一个双线性形式不能由带有 $\\mathrm{ReLU}$ 激活的加性结构表示。对于固定的 $s_{t-1}$，乘性分数是 $h_i$ 的线性函数。相反，对于固定的 $s_{t-1}$，带有 $\\mathrm{ReLU}$ 的加性分数是 $h_i$ 的一个分段线性、凸函数。这些是根本不同的函数形式。不存在任何参数 $v, W_h, W_s, b$ 的选择可以使加性形式对所有输入 $h_i$ 和 $s_{t-1}$ 都等效于乘性形式。这两种机制在设计上就是不同的。\n\n陈述 E 对两种不同注意力机制之间的函数等价性做出了不正确的断言。\n\n### 逐项分析\n\n- **A. 对于零均值对称的预激活值 $z_{t,i}$，用 $\\mathrm{ReLU}$ 替换 $\\tanh$ 会使得 $\\phi(z_{t,i})$ 的每个坐标都具有严格为正的均值，并消除了奇对称性 $\\phi(-x) = -\\phi(x)$。相比之下，使用 $\\tanh$ 时，激活值近似为零均值且是奇对称的。这消除了对分数 $e_{t,i}$ 贡献的符号对称性，除非参数或偏置重新中心化预激活值。**\n  - **结论：正确。** 分析表明，对于对称的零均值输入，$\\mathrm{ReLU}$ 会在激活值中引入正均值，并破坏 $\\tanh$ 固有的对称性。\n\n- **B. 与 $\\tanh$ 相比，$\\mathrm{ReLU}$ 到处都消除了梯度消失问题，因为它的导数要么是 $0$ 要么是 $1$，所以梯度通过所有单元时都能保持不变地反向传播。**\n  - **结论：不正确。** 对于负输入，导数为 $0$，这会完全阻断这些单元的梯度。这是一种梯度消失的形式，而不是消除。梯度只通过激活的单元传播。\n\n- **C. 在 $\\mathrm{ReLU}$ 前没有偏置项且初始化时预激活值为对称零均值的情况下，大约 $1/2$ 的单元将处于非激活状态，导数为零，这会减慢学习速度；引入一个正偏置可以改变预激活值的分布，使得更大部分的单元处于线性区域并接收到非零梯度。**\n  - **结论：正确。** 这准确地描述了在此背景下的“死亡 ReLU”问题，以及使用正偏置作为增加初始激活单元比例的标准解决方案。\n\n- **D. 因为 $\\mathrm{softmax}$ 对于在给定时间步将所有分数加上同一个标量是不变的，所以由 $\\mathrm{ReLU}$ 引入的非零中心化问题总能通过在计算 $e_{t,i}$ 后加上一个标量偏置来完美抵消；因此在评分网络内部不需要任何偏置。**\n  - **结论：不正确。** 这混淆了 $\\mathrm{softmax}$ 的输出属性与网络的内部学习动态。在分数计算后添加的偏置无法修复网络内部的梯度流问题。\n\n- **E. 在加性注意力评分网络中从 $\\tanh$ 切换到 $\\mathrm{ReLU}$，对于合适的参数选择，会使其等效于乘性 (Luong) 注意力机制，因此两种机制对所有输入计算出的分数都相同。**\n  - **结论：不正确。** 加性注意力（一个小型神经网络）和乘性注意力（一个双线性形式）的函数形式根本不同，并不等价。\n\n根据分析，陈述 A 和 C 是正确的。", "answer": "$$\\boxed{AC}$$", "id": "3097395"}]}