{"hands_on_practices": [{"introduction": "从根本上说，自注意力机制可以被看作一个动态的内容寻址系统，它根据输入序列的内部相似性来聚合信息。这个练习将通过一个简化的场景，推导注意力机制退化为“恒等”或“复制”操作的条件[@problem_id:3180941]。通过这个推导，你将对注意力如何根据输入数据的结构，在复制特定标记和平均多个标记之间进行权衡，建立起深刻的直觉。", "problem": "考虑一个使用缩放点积机制的单头自注意力层，该层没有位置编码和残差连接。设输入序列为 $X = (x_{1}, x_{2}, \\dots, x_{n})$，其中每个词元嵌入 $x_{i} \\in \\mathbb{R}^{d}$。查询、键和值定义为 $Q = K = X$ 且 $V = X$。注意力对数（logits）是通过一个非负逆温度参数 $s \\geq 0$ 缩放逐对点积来构建的，因此对于每对 $\\{i,j\\}$，其对数为 $L_{ij} = s \\, \\langle x_{i}, x_{j} \\rangle$，而逐行的注意力权重为 $A_{ij} = \\exp(L_{ij}) \\big/ \\sum_{k=1}^{n} \\exp(L_{ik})$。注意力层的输出为 $Y = A V$，其中 $A$ 是逐行归一化的权重矩阵。\n\n假设序列具有以下低复杂度相似性结构：存在实数 $m$ 和 $c$，使得对于所有 $i$，有 $\\langle x_{i}, x_{i} \\rangle = m$；对于所有 $i \\neq j$，有 $\\langle x_{i}, x_{j} \\rangle = c$，且 $m > c$。在此设定下，一种类单位矩阵行为的特征是每个查询将其几乎所有的权重都放在自己的键上，这可通过对角注意力权重满足 $A_{ii} \\geq 1 - \\tau$ 来量化，其中 $0  \\tau  1$ 是一个给定的容差。请根据上述定义，且不借助任何额外的架构捷径，推导出一个关于逆温度 $s$ 的显式闭式下界。当 $n \\geq 2$ 时，该下界能保证所有行的 $A_{ii} \\geq 1 - \\tau$。结果需用 $n$、$m$、$c$ 和 $\\tau$ 表示。\n\n然后，对于 $n = 6$，$m = 1$，$c = 0.1$ 和 $\\tau = 0.02$ 的具体情况，将你的界限实例化。将最终答案表示为单个闭式表达式，不要进行四舍五入。\n\n最后，请仅使用相同的基本定义，简要说明当数据在 $x_{1} = x_{2} = \\dots = x_{n}$ 的意义上达到最大程度低熵时，注意力权重会变成什么（此说明无需提供数值）。", "solution": "该问题要求推导逆温度缩放参数 $s$ 的一个下界，以保证特定水平的自注意力，并分析一个退化情况。对问题陈述的验证表明，该问题具有科学依据、是适定的且客观的。因此，我们可以进行形式化的求解。\n\n主要任务是找到 $s$ 的最小值，使得对角注意力权重 $A_{ii}$ 对于给定的容差 $\\tau \\in (0, 1)$ 至少为 $1 - \\tau$。注意力权重由 softmax 函数逐行应用于对数矩阵 $L_{ij}$ 定义。对于任意行 $i$，对角权重 $A_{ii}$ 由下式给出：\n$$\nA_{ii} = \\frac{\\exp(L_{ii})}{\\sum_{k=1}^{n} \\exp(L_{ik})}\n$$\n对数为 $L_{ij} = s \\langle x_{i}, x_{j} \\rangle$，其中 $s \\geq 0$。利用给定的相似性结构，我们有 $\\langle x_{i}, x_{i} \\rangle = m$ 和对于所有 $i \\neq j$ 有 $\\langle x_{i}, x_{j} \\rangle = c$。这使我们可以将对数表示为 $L_{ii} = sm$ 和对于 $i \\neq j$ 有 $L_{ij} = sc$。\n\n将这些代入 $A_{ii}$ 的表达式中，我们发现分母可以分解为 $k=i$ 的项和 $n-1$ 个 $k \\neq i$ 的项：\n$$\n\\sum_{k=1}^{n} \\exp(L_{ik}) = \\exp(L_{ii}) + \\sum_{k=1, k \\neq i}^{n} \\exp(L_{ik}) = \\exp(sm) + (n-1)\\exp(sc)\n$$\n因此，$A_{ii}$ 的表达式变得与索引 $i$ 无关：\n$$\nA_{ii} = \\frac{\\exp(sm)}{\\exp(sm) + (n-1)\\exp(sc)}\n$$\n需要满足的条件是 $A_{ii} \\geq 1 - \\tau$。我们可以直接建立不等式：\n$$\n\\frac{\\exp(sm)}{\\exp(sm) + (n-1)\\exp(sc)} \\geq 1 - \\tau\n$$\n由于分母是正数项之和，且 $1-\\tau > 0$，我们可以在不等式两边同乘以分母而不改变其方向：\n$$\n\\exp(sm) \\geq (1 - \\tau) \\left( \\exp(sm) + (n-1)\\exp(sc) \\right)\n$$\n分配 $(1-\\tau)$ 项：\n$$\n\\exp(sm) \\geq (1-\\tau)\\exp(sm) + (n-1)(1-\\tau)\\exp(sc)\n$$\n合并包含 $\\exp(sm)$ 的项：\n$$\n\\exp(sm) - (1-\\tau)\\exp(sm) \\geq (n-1)(1-\\tau)\\exp(sc)\n$$\n$$\n\\tau \\exp(sm) \\geq (n-1)(1-\\tau)\\exp(sc)\n$$\n为了分离出对 $s$ 的依赖关系，我们可以两边除以 $\\tau \\exp(sc)$。由于 $\\tau > 0$ 且 $\\exp(sc) > 0$，这是一个保持不等式成立的有效操作：\n$$\n\\frac{\\exp(sm)}{\\exp(sc)} \\geq \\frac{(n-1)(1-\\tau)}{\\tau}\n$$\n使用指数性质 $\\exp(a)/\\exp(b) = \\exp(a-b)$：\n$$\n\\exp(s(m-c)) \\geq \\frac{(n-1)(1-\\tau)}{\\tau}\n$$\n右边对数的参数是正的，因为 $n \\geq 2$ 意味着 $n-1 > 0$，而 $0  \\tau  1$ 意味着 $1-\\tau > 0$。我们可以对两边取自然对数。由于 $\\ln(x)$ 是一个单调递增函数，不等式得以保持：\n$$\n\\ln\\left(\\exp(s(m-c))\\right) \\geq \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\n$$\ns(m-c) \\geq \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\n问题陈述中给出 $m > c$，这意味着 $m - c > 0$。因此我们可以两边除以 $(m-c)$ 来解出 $s$：\n$$\ns \\geq \\frac{1}{m-c} \\ln\\left(\\frac{(n-1)(1-\\tau)}{\\tau}\\right)\n$$\n这个不等式提供了 $s$ 的显式闭式下界，它保证了所期望的类单位矩阵行为。\n\n接下来，我们将此界限实例化到 $n = 6$，$m = 1$，$c = 0.1$ 和 $\\tau = 0.02$ 的具体情况。\n表达式的各组成部分为：\n- $m-c = 1 - 0.1 = 0.9 = \\frac{9}{10}$\n- $n-1 = 6-1 = 5$\n- $1-\\tau = 1 - 0.02 = 0.98 = \\frac{98}{100}$\n- $\\tau = 0.02 = \\frac{2}{100}$\n\n将这些代入对数的参数中：\n$$\n\\frac{(n-1)(1-\\tau)}{\\tau} = \\frac{5 \\times 0.98}{0.02} = \\frac{5 \\times 98}{2} = 5 \\times 49 = 245\n$$\n因此，$s$ 的下界为：\n$$\ns \\geq \\frac{1}{0.9} \\ln(245) = \\frac{10}{9} \\ln(245)\n$$\n这就是实例化界限所要求的闭式表达式。\n\n最后，我们被要求说明当数据达到最大程度低熵时，即 $x_{1} = x_{2} = \\dots = x_{n}$，注意力权重会变成什么。\n在这种情况下，所有输入向量都是相同的。令对所有 $i \\in \\{1, \\dots, n\\}$ 有 $x_i = x_0$。逐对点积变得一致：\n$$\n\\langle x_{i}, x_{j} \\rangle = \\langle x_{0}, x_{0} \\rangle \\quad \\forall i, j\n$$\n设这个恒定的点积值为 $M = \\langle x_{0}, x_{0} \\rangle$。那么所有对数都相等：\n$$\nL_{ij} = s \\langle x_{i}, x_{j} \\rangle = sM \\quad \\forall i, j\n$$\n注意力权重 $A_{ij}$ 计算如下：\n$$\nA_{ij} = \\frac{\\exp(L_{ij})}{\\sum_{k=1}^{n} \\exp(L_{ik})} = \\frac{\\exp(sM)}{\\sum_{k=1}^{n} \\exp(sM)}\n$$\n分母是 $n$ 个相同项的和，所以 $\\sum_{k=1}^{n} \\exp(sM) = n \\exp(sM)$。\n将此代入 $A_{ij}$ 的表达式中，得出：\n$$\nA_{ij} = \\frac{\\exp(sM)}{n \\exp(sM)} = \\frac{1}{n}\n$$\n因此，当所有输入词元都相同时，注意力机制无法区分它们。得到的注意力矩阵 $A$ 的每个条目都等于 $1/n$，这表明每个查询对所有键的注意力是均匀分布的。这代表了一种最大不确定性的状态，与问题主要部分所寻求的类单位矩阵的集中性在逻辑上是截然相反的。", "answer": "$$\\boxed{\\frac{10}{9}\\ln(245)}$$", "id": "3180941"}, {"introduction": "注意力机制的灵活性很大程度上源于其核心的Softmax函数，而温度参数 $\\tau$ 是调节Softmax行为的关键。本练习将引导你探索当温度趋近于零时，注意力如何从一个“软”的加权平均平滑地过渡到一个“硬”的最大值选择器（argmax）[@problem_id:3100390]。你还将通过一个精心设计的例子，分析硬选择可能导致的模型不稳定性，并理解温度参数作为一种平滑策略的重要性。", "problem": "考虑一个在 Transformer 模型中使用的、基于缩放点积注意力（SDPA）的注意力机制。设有一个查询向量 $q \\in \\mathbb{R}^d$，一组键向量 $\\{k_i\\}_{i=1}^n \\subset \\mathbb{R}^d$，以及一组对应的值向量 $\\{v_i\\}_{i=1}^n \\subset \\mathbb{R}^m$。将键 $k_i$ 的得分定义为 $s_i = q^\\top k_i$。对于一个温度参数 $\\tau  0$，通过对得分的指数进行归一化来定义 softmax 权重 $w_i(\\tau)$，并将注意力输出 $a(\\tau)$ 定义为使用这些权重对值进行加权求和。通过选择使得分最大化的索引 $i^\\star$ 对应的值来定义 argmax 注意力输出 $a^\\star$。仅从指数函数、极限和按和归一化的基本定义出发，推导当 $\\tau \\to 0$ 时 $a(\\tau)$ 和 $a^\\star$ 之间的极限关系，并分析 argmax 选择在 $q$ 的微小扰动下的脆弱性。\n\n你的任务是编写一个完整的、可运行的程序，对指定的测试套件执行以下计算，并按下面描述的精确格式生成单行输出。不涉及任何物理单位、角度单位或百分比；所有输出必须是数字浮点数、整数、布尔值或它们的列表。\n\n使用的定义：\n- Softmax 权重：$w_i(\\tau) = \\dfrac{\\exp\\!\\left(s_i/\\tau\\right)}{\\sum_{j=1}^n \\exp\\!\\left(s_j/\\tau\\right)}$，其中 $s_i = q^\\top k_i$。\n- Softmax 注意力输出：$a(\\tau) = \\sum_{i=1}^n w_i(\\tau) v_i$。\n- Argmax 索引：$i^\\star = \\operatorname{argmax}_{1 \\leq i \\leq n} s_i$（假设存在一个任意但固定的平局打破规则）。\n- Argmax 注意力输出：$a^\\star = v_{i^\\star}$。\n\n测试套件规范：\n\n- 测试用例 1（唯一最大值，不同温度下的近似质量）：\n  - 维度 $d = 2$，值维度 $m = 2$。\n  - 查询 $q = (1.0, 0.2)$。\n  - 键 $k_1 = (1.0, 0.0)$, $k_2 = (0.5, 0.7)$, $k_3 = (-0.5, 0.3)$, $k_4 = (0.1, 0.2)$。\n  - 值 $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$, $v_3 = (1.0, 1.0)$, $v_4 = (-1.0, 1.0)$。\n  - 温度 $\\tau \\in \\{1.0, 0.5, 0.1, 0.01, 10^{-6}\\}$。\n  - 对于每个 $\\tau$，计算欧几里得范数 $\\|a(\\tau) - a^\\star\\|_2$，并按 $\\tau$ 的顺序列出这些浮点数。\n\n- 测试用例 2（微小查询扰动下的近似平局脆弱性及温度平滑效应）：\n  - 维度 $d = 2$，值维度 $m = 2$。\n  - 基础查询 $q_0 = (1.0, 0.0)$，扰动查询 $q_+ = (1.0, \\epsilon)$ 和 $q_- = (1.0, -\\epsilon)$，其中 $\\epsilon = 10^{-3}$。\n  - 键 $k_1 = (1.0, 0.0)$, $k_2 = (1.0, \\eta)$，其中 $\\eta = 1000.0$。\n  - 值 $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$。\n  - 用于比较的温度 $\\tau \\in \\{0.5, 0.1, 0.01\\}$。\n  - 计算：\n    - 扰动下 argmax 翻转的整数指示器，定义为如果 $\\operatorname{argmax}_i q_+^\\top k_i \\neq \\operatorname{argmax}_i q_-^\\top k_i$ 则 $I = 1$，否则 $I = 0$。\n    - 对于每个指定的 $\\tau$，计算 $q_+$ 和 $q_-$ 对应的两个 softmax 注意力输出之间的欧几里得距离 $\\|a_{+}(\\tau) - a_{-}(\\tau)\\|_2$。\n  - 返回一个列表，包含整数 $I$，后面跟着按所列 $\\tau$ 值排序的三个距离。\n\n- 测试用例 3（精确平局，softmax 权重的对称性）：\n  - 维度 $d = 2$，值维度 $m = 2$。\n  - 查询 $q = (1.0, 0.0)$。\n  - 键 $k_1 = (1.0, 0.0)$, $k_2 = (1.0, 0.0)$, $k_3 = (0.0, 1.0)$（因此 $k_1$ 和 $k_2$ 精确平局）。\n  - 值 $v_1 = (1.0, 0.0)$, $v_2 = (0.0, 1.0)$, $v_3 = (1.0, 1.0)$。\n  - 温度 $\\tau = 0.1$。\n  - 计算 softmax 权重 $w_1(\\tau)$ 和 $w_2(\\tau)$，并返回浮点数 $|w_1(\\tau) - w_2(\\tau)|$。\n\n最终输出格式：\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，其中每个测试用例的结果按描述顺序出现：\n  - 第一个元素是来自测试用例 1 的浮点数列表。\n  - 第二个元素是来自测试用例 2 的列表，包含一个整数和三个浮点数。\n  - 第三个元素是来自测试用例 3 的单个浮点数。\n- 例如，最后一行应类似于：$[[x_1,x_2,x_3,x_4,x_5],[i,d_1,d_2,d_3],y]$，其中每个 $x_j$, $i$, $d_j$ 和 $y$ 是为指定测试套件计算出的数值结果。", "solution": "该问题定义明确，具有科学依据，且内部一致。它为得出唯一且有意义的解提供了所有必要的定义和数据。我们继续进行推导和计算实现。\n\n分析分为两部分。首先，我们推导当温度参数 $\\tau$ 趋近于零时，softmax 注意力输出 $a(\\tau)$ 的极限行为。其次，我们分析注意力机制在微小扰动下的稳定性，对比 argmax 选择与带温度的 softmax。\n\n**1. 当 $\\tau \\to 0^+$ 时 softmax 注意力的极限行为**\n\nsoftmax 注意力输出被定义为值向量的加权和：\n$$ a(\\tau) = \\sum_{i=1}^n w_i(\\tau) v_i $$\n其中权重 $w_i(\\tau)$ 由应用于得分 $s_i = q^\\top k_i$ 的温度缩放 softmax 函数给出：\n$$ w_i(\\tau) = \\frac{\\exp(s_i/\\tau)}{\\sum_{j=1}^n \\exp(s_j/\\tau)} $$\n我们希望计算极限 $\\lim_{\\tau \\to 0^+} a(\\tau)$。分析的关键在于权重 $w_i(\\tau)$ 在此极限下的行为。\n\n令 $s_{\\max} = \\max_{1 \\leq j \\leq n} s_j$ 为最大得分。为了避免数值溢出并简化极限计算，我们可以从分子和分母中提出因子 $\\exp(s_{\\max}/\\tau)$：\n$$ w_i(\\tau) = \\frac{\\exp(s_{\\max}/\\tau) \\exp((s_i - s_{\\max})/\\tau)}{\\exp(s_{\\max}/\\tau) \\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau)} = \\frac{\\exp((s_i - s_{\\max})/\\tau)}{\\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau)} $$\n现在，我们考察当 $\\tau \\to 0^+$ 时指数项 $\\exp((s_j - s_{\\max})/\\tau)$ 的极限。其行为取决于 $s_j$ 是否等于 $s_{\\max}$。\n\n情况 1：$s_j = s_{\\max}$。在这种情况下，指数为 $(s_j - s_{\\max})/\\tau = 0/\\tau = 0$。因此，$\\exp((s_j - s_{\\max})/\\tau) = \\exp(0) = 1$。\n\n情况 2：$s_j  s_{\\max}$。此时，差值 $s_j - s_{\\max}$ 是一个严格为负的常数。当 $\\tau \\to 0^+$ 时，指数 $(s_j - s_{\\max})/\\tau \\to -\\infty$。因此，$\\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) = 0$。\n\n令 $I_{\\max} = \\{i \\mid s_i = s_{\\max}\\}$ 为对应最大得分的索引集合，令 $M = |I_{\\max}|$ 为此类索引的数量。我们现在可以计算 $w_i(\\tau)$ 分母的极限：\n$$ \\lim_{\\tau \\to 0^+} \\sum_{j=1}^n \\exp((s_j - s_{\\max})/\\tau) = \\sum_{j \\in I_{\\max}} \\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) + \\sum_{j \\notin I_{\\max}} \\lim_{\\tau \\to 0^+} \\exp((s_j - s_{\\max})/\\tau) $$\n$$ = \\sum_{j \\in I_{\\max}} 1 + \\sum_{j \\notin I_{\\max}} 0 = M $$\n分子 $\\exp((s_i - s_{\\max})/\\tau)$ 的极限在 $i \\in I_{\\max}$ 时为 $1$，在 $i \\notin I_{\\max}$ 时为 $0$。\n\n综合这些结果，权重 $w_i(\\tau)$ 的极限为：\n$$ \\lim_{\\tau \\to 0^+} w_i(\\tau) = \\begin{cases} 1/M  \\text{若 } i \\in I_{\\max} \\\\ 0  \\text{若 } i \\notin I_{\\max} \\end{cases} $$\n这表明当 $\\tau \\to 0^+$ 时，softmax 函数将其所有概率质量集中在达到最大得分的索引上，并在它们之间均匀分配。它实际上变成了一个“软 argmax”。\n\n现在，我们可以求出注意力输出 $a(\\tau)$ 的极限：\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\lim_{\\tau \\to 0^+} \\sum_{i=1}^n w_i(\\tau) v_i = \\sum_{i=1}^n \\left(\\lim_{\\tau \\to 0^+} w_i(\\tau)\\right) v_i $$\n代入极限权重：\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\sum_{i \\in I_{\\max}} \\frac{1}{M} v_i + \\sum_{i \\notin I_{\\max}} 0 \\cdot v_i = \\frac{1}{M} \\sum_{i \\in I_{\\max}} v_i $$\n极限注意力输出是其对应键达到最大得分的值向量的算术平均值。\n\n问题将 argmax 注意力输出定义为 $a^\\star = v_{i^\\star}$，其中 $i^\\star = \\operatorname{argmax}_{i} s_i$ 并有固定的平局打破规则。如果最大得分是唯一的，那么 $I_{\\max} = \\{i^\\star\\}$ 且 $M=1$。在这种常见情况下，极限简化为：\n$$ \\lim_{\\tau \\to 0^+} a(\\tau) = \\frac{1}{1} v_{i^\\star} = v_{i^\\star} = a^\\star $$\n因此，当最大得分唯一时，softmax 注意力输出收敛于 argmax 注意力输出。\n\n**2. Argmax 选择的脆弱性及温度的正则化作用**\n\nArgmax 选择本质上是不连续的。$i^\\star$ 的选择取决于得分 $s_i(q) = q^\\top k_i$ 的排序。对查询的微小扰动 $q' = q + \\delta q$ 会引起得分的变化：$s_i(q') = s_i(q) + (\\delta q)^\\top k_i$。如果两个得分，比如 $s_j(q)$ 和 $s_k(q)$ 非常接近，即使是无穷小的扰动 $\\delta q$ 也可能改变它们的相对顺序，导致 $i^\\star$ 从一个索引跳到另一个。这会导致输出 $a^\\star$ 发生从 $v_j$到 $v_k$ 的离散变化，这个变化可能很大。这种不稳定性是“脆弱性”的一种形式。\n\n相反，对于任何严格为正的温度 $\\tau > 0$，softmax 注意力输出 $a(\\tau)$ 是查询 $q$ 的一个连续可微函数。权重 $w_i(\\tau)$ 是得分 $s_i$ 的平滑函数，而得分 $s_i$ 是 $q$ 的线性函数。因此，查询的微小变化 $\\delta q$ 会导致输出 $a(\\tau)$ 相应地产生微小变化。该变化由梯度决定：$a(q+\\delta q, \\tau) \\approx a(q, \\tau) + (\\nabla_q a(q, \\tau)) \\cdot \\delta q$。\n\n较高的温度 $\\tau$ 具有平滑或正则化效应。它“平坦化”了 softmax 分布，这意味着权重 $w_i(\\tau)$ 对得分的微小差异不那么敏感。这使得输出 $a(\\tau)$对 $q$ 的微小扰动更具鲁棒性。反之，当 $\\tau \\to 0$ 时，softmax 函数变得更陡峭，$a(\\tau)$ 关于 $q$ 的梯度可能会变得非常大，特别是在影响接近最大值得分之间差异的方向上。在这种情况下，平滑的 softmax 输出 $a(\\tau)$ 紧密地近似于 argmax 输出 $a^\\star$ 的脆弱、不连续行为，表现出对可能翻转最高得分排序的微小扰动的高度敏感性，如测试用例 2 所示。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three test cases specified in the problem statement regarding\n    Scaled Dot-Product Attention.\n    \"\"\"\n\n    def softmax_attention_with_weights(q, K, V, tau):\n        \"\"\"\n        Computes the softmax attention output and corresponding weights.\n\n        Args:\n            q (np.ndarray): Query vector of shape (d,).\n            K (np.ndarray): Key matrix of shape (n, d).\n            V (np.ndarray): Value matrix of shape (n, m).\n            tau (float): Temperature parameter.\n\n        Returns:\n            tuple: A tuple containing:\n                - np.ndarray: Attention output vector of shape (m,).\n                - np.ndarray: Softmax weights vector of shape (n,).\n        \"\"\"\n        scores = K @ q\n        # Scale scores by temperature as per the definition.\n        scaled_scores = scores / tau\n        # Use the max-subtraction trick for numerical stability of exp.\n        stable_scores = scaled_scores - np.max(scaled_scores)\n        exps = np.exp(stable_scores)\n        weights = exps / np.sum(exps)\n        # Attention output is the weighted sum of value vectors.\n        # V.T is (m, n), weights is (n,). Result is (m,).\n        attention_output = V.T @ weights\n        return attention_output, weights\n\n    # --- Test Case 1: Unique maximum, approximation quality ---\n    q1 = np.array([1.0, 0.2])\n    K1 = np.array([[1.0, 0.0], [0.5, 0.7], [-0.5, 0.3], [0.1, 0.2]])\n    V1 = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [-1.0, 1.0]])\n    taus1 = [1.0, 0.5, 0.1, 0.01, 1e-6]\n    \n    scores1 = K1 @ q1\n    # np.argmax respects the \"fixed tie-breaking rule\" by taking the first occurrence.\n    i_star1 = np.argmax(scores1)\n    a_star1 = V1[i_star1]\n    \n    results1 = []\n    for tau in taus1:\n        a_tau, _ = softmax_attention_with_weights(q1, K1, V1, tau)\n        error = np.linalg.norm(a_tau - a_star1)\n        results1.append(error)\n\n    # --- Test Case 2: Near-tie brittleness ---\n    epsilon = 1e-3\n    q_plus = np.array([1.0, epsilon])\n    q_minus = np.array([1.0, -epsilon])\n    eta = 1000.0\n    K2 = np.array([[1.0, 0.0], [1.0, eta]])\n    V2 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    taus2 = [0.5, 0.1, 0.01]\n    \n    scores_plus = K2 @ q_plus\n    scores_minus = K2 @ q_minus\n    argmax_plus = np.argmax(scores_plus)\n    argmax_minus = np.argmax(scores_minus)\n    I = 1 if argmax_plus != argmax_minus else 0\n    \n    results2 = [I]\n    for tau in taus2:\n        a_plus_tau, _ = softmax_attention_with_weights(q_plus, K2, V2, tau)\n        a_minus_tau, _ = softmax_attention_with_weights(q_minus, K2, V2, tau)\n        dist = np.linalg.norm(a_plus_tau - a_minus_tau)\n        results2.append(dist)\n        \n    # --- Test Case 3: Exact tie, symmetry of weights ---\n    q3 = np.array([1.0, 0.0])\n    K3 = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n    # V3 is not strictly needed as we only compute weights.\n    # Pass a dummy V matrix of the correct shape.\n    V3 = np.zeros((3, 2))\n    tau3 = 0.1\n    \n    _, weights3 = softmax_attention_with_weights(q3, K3, V3, tau3)\n    w1_tau = weights3[0]\n    w2_tau = weights3[1]\n    result3 = abs(w1_tau - w2_tau)\n    \n    # --- Format and print the final output ---\n    # The output format is a single line string `[[...],[...],...]`.\n    str_res1 = f\"[{','.join(map(str, results1))}]\"\n    str_res2 = f\"[{','.join(map(str, results2))}]\"\n    str_res3 = str(result3)\n    \n    print(f\"[{str_res1},{str_res2},{str_res3}]\")\n\nsolve()\n```", "id": "3100390"}, {"introduction": "理解了注意力的前向计算过程后，掌握其训练方式——即梯度如何反向传播——是至关重要的一步。这个高级实践任务要求你从零开始，为因果自注意力机制实现反向模式自动微分（即向量-雅可比积，VJP）[@problem_id:3100434]。通过亲手实现并验证梯度流，你将深刻理解因果掩码如何确保模型在训练时，信息不会从未来“泄露”到现在，这是所有自回归语言模型（如GPT）的基础。", "problem": "为带有因果掩码的单头缩放点积注意力机制实现一个反向模式自动微分 (AD) 的向量-雅可比积 (VJP)。其目标是，从第一性原理出发，证明带有因果掩码的注意力机制的反向传播不会将来自时间索引 $t$ 的词元的梯度传播到与未来词元（索引 $j$ 满足 $j  t$）相关联的参数中。请使用以下形式化设定和要求。\n\n基本基础和定义：\n- 令 $L$ 表示序列长度，令 $d$ 表示查询和键的维度。令 $d_v$ 表示值的维度。\n- 令 $\\mathbf{Q} \\in \\mathbb{R}^{L \\times d}$、$\\mathbf{K} \\in \\mathbb{R}^{L \\times d}$ 和 $\\mathbf{V} \\in \\mathbb{R}^{L \\times d_v}$ 分别表示查询、键和值矩阵。\n- 定义缩放点积得分矩阵为\n$$\n\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\, \\mathbf{Q} \\, \\mathbf{K}^\\top \\in \\mathbb{R}^{L \\times L}.\n$$\n- 定义因果掩码 $\\mathbf{M} \\in \\{0,1\\}^{L \\times L}$，其中当且仅当 $j \\le i$ 时 $\\mathbf{M}_{i,j} = 1$，否则 $\\mathbf{M}_{i,j} = 0$。\n- 通过设置当 $j \\le i$ 时 $\\widetilde{\\mathbf{S}}_{i,j} = \\mathbf{S}_{i,j}$ 和当 $j  i$ 时 $\\widetilde{\\mathbf{S}}_{i,j} = -\\infty$ 来构建掩码后的得分 $\\widetilde{\\mathbf{S}}$。\n- 令 $\\operatorname{softmax}$ 表示按行应用的 softmax 函数。定义注意力权重为\n$$\n\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}}) \\in \\mathbb{R}^{L \\times L},\n$$\n以及注意力输出为\n$$\n\\mathbf{Y} = \\mathbf{A} \\, \\mathbf{V} \\in \\mathbb{R}^{L \\times d_v}.\n$$\n\n任务与约束：\n- 实现一个完整、可运行的程序，该程序：\n  - 按照定义，使用因果掩码计算前向注意力 $\\mathbf{Y}$。\n  - 实现反向模式 AD 的向量-雅可比积 (VJP)，它将上游余切（梯度）$\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}} \\in \\mathbb{R}^{L \\times d_v}$ 映射到下游余切（梯度）$(\\mathbf{G_Q}, \\mathbf{G_K}, \\mathbf{G_V}) = \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}}, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}}, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}} \\right)$。此实现仅使用链式法则、softmax 函数的定义和线性代数恒等式。不要使用任何外部自动微分框架；仅依靠基本的微分法则和线性代数来推导和实现 VJP 公式。\n  - 通过在进行指数运算前减去行最大值，使用数值稳定的 softmax 计算。\n- 测试目标：\n  - 对于每个测试，将上游余切（梯度）$\\mathbf{G_Y}$ 设置为除选定的时间索引 $t$ 和值通道 $c$ 处的一个非零条目外，其余所有条目均为零，即 $\\mathbf{G_Y}_{t, c} = 1$，所有其他条目等于 $0$。\n  - 在因果掩码下，验证 VJP 计算出的梯度 $\\mathbf{G_K}$ 和 $\\mathbf{G_V}$ 对于所有未来词元行 $j$（其中 $j  t$）均为零。使用 $\\varepsilon = 10^{-12}$ 的数值容差进行零值检查。\n- 角度单位不适用于此任务。\n- 此问题中不涉及物理单位。\n\n测试套件和参数值：\n- 使用以下确定性测试用例，每个用例由 $(L, d, d_v, t, c, s, \\text{causal})$ 指定，其中 $s$ 是用于构建输入的随机种子，$\\text{causal}$ 是一个布尔值，指示是否使用因果掩码。\n  - 测试 $1$：$(L, d, d_v, t, c, s, \\text{causal}) = (4, 3, 2, 1, 0, 0, \\text{True})$。\n  - 测试 $2$：$(L, 3, 2, 0, 1, 1, \\text{True})$。\n  - 测试 $3$：$(L, d, d_v, t, c, s, \\text{causal}) = (5, 4, 3, 4, 2, 2, \\text{True})$。\n  - 测试 $4$ (不带掩码的对照组)：$(L, d, d_v, t, c, s, \\text{causal}) = (4, 3, 2, 1, 1, 3, \\text{False})$。\n- 对于每个测试用例，通过使用给定的种子 $s$ 采样独立的标准正态分布条目来构建 $\\mathbf{Q}$、$\\mathbf{K}$ 和 $\\mathbf{V}$ 以确保可复现性，然后为了数值调节，将它们乘以因子 $0.5$。形式上，将伪随机数生成器的种子设置为 $s$，从 $\\mathcal{N}(0,1)$ 中采样具有独立条目的 $\\mathbf{Q}$、$\\mathbf{K}$ 和 $\\mathbf{V}$，然后将每个采样矩阵替换为其自身的 $0.5$ 倍。\n- 对于每个测试，将上游余切（梯度）$\\mathbf{G_Y}$ 定义为 $\\mathbb{R}^{L \\times d_v}$ 中的零矩阵，但 $\\mathbf{G_Y}_{t,c} = 1$ 除外。\n- 对于每个测试，程序必须通过计算以下布尔谓词来评估在因果掩码下，指向未来词元的梯度是否为零：\n$$ \\max_{j > t} \\left( \\max\\left( \\left| \\mathbf{G_K}[j,:] \\right| \\right), \\, \\max\\left( \\left| \\mathbf{G_V}[j,:] \\right| \\right) \\right) \\le \\varepsilon $$\n其中 $\\varepsilon = 10^{-12}$。如果集合 $\\{ j : j > t \\}$ 为空（例如，当 $t = L - 1$ 时），则将该谓词视为真。\n- 最终输出格式必须是单行文本，包含 4 个测试的布尔结果，形式为逗号分隔的列表并用方括号括起来，例如 $[\\text{True},\\text{False},\\text{True},\\text{True}]$。\n\n您的程序必须以指定格式精确地产生一行输出，并且不得读取任何输入。", "solution": "该问题要求为单头缩放点积注意力机制实现一个反向模式自动微分 (AD) 的向量-雅可比积 (VJP)，并特别关注展示因果掩码对梯度流的影响。我们必须验证来自时间点 $t$ 的输出词元的梯度不会传播到与未来词元 $j  t$ 相关联的输入参数。\n\n首先，我们将通过对前向传播中的操作序列应用链式法则，从第一性原理推导 VJP 方程。令 $\\mathcal{L}$ 为一个标量损失函数。对于一个函数 $f: \\mathbb{R}^n \\to \\mathbb{R}^m$，其 VJP 将关于其输出的余切（梯度）$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\in \\mathbb{R}^m$ 映射到关于其输入的余切（梯度）$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^n$。我们给定上游余切 $\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}$，并且必须计算 $\\mathbf{G_Q} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}}$、$\\mathbf{G_K} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}}$ 和 $\\mathbf{G_V} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}}$。\n\n前向传播由以下操作序列定义：\n1.  缩放点积得分：$\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\mathbf{Q} \\mathbf{K}^\\top$\n2.  掩码：$\\widetilde{\\mathbf{S}} = \\operatorname{mask}(\\mathbf{S}, \\mathbf{M})$ (其中被掩码的元素被设置为 $-\\infty$)\n3.  按行 softmax：$\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}})$\n4.  输出计算：$\\mathbf{Y} = \\mathbf{A} \\mathbf{V}$\n\n我们将通过反转此序列来推导 VJP。\n\n**步骤 1：输出计算 $\\mathbf{Y} = \\mathbf{A} \\mathbf{V}$ 的 VJP**\n\n此操作是标准矩阵乘法。我们给定 $\\mathbf{G_Y} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Y}}$。使用矩阵乘法的 VJP 法则 ($C = AB \\implies \\mathbf{G_A} = \\mathbf{G_C} B^\\top, \\mathbf{G_B} = A^\\top \\mathbf{G_C}$)，我们求得 $\\mathbf{A}$ 和 $\\mathbf{V}$ 的余切（梯度）：\n$$\n\\mathbf{G_A} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}} = \\mathbf{G_Y} \\mathbf{V}^\\top\n$$\n$$\n\\mathbf{G_V} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{V}} = \\mathbf{A}^\\top \\mathbf{G_Y}\n$$\n\n**步骤 2：按行 Softmax $\\mathbf{A} = \\operatorname{softmax}(\\widetilde{\\mathbf{S}})$ 的 VJP**\n\nSoftmax 函数应用于 $\\widetilde{\\mathbf{S}}$ 的每一行。对于单个行向量 $\\mathbf{a} = \\operatorname{softmax}(\\mathbf{s})$，从 $\\mathbf{g_a}$ 到 $\\mathbf{g_s}$ 的 VJP 映射由 softmax 函数的雅可比矩阵 $J_{jk} = \\frac{\\partial a_j}{\\partial s_k} = a_j(\\delta_{jk} - a_k)$ 推导得出。VJP 为：\n$$\n(\\mathbf{g_s})_k = \\sum_j (\\mathbf{g_a})_j J_{jk} = \\sum_j (\\mathbf{g_a})_j a_j(\\delta_{jk} - a_k) = (\\mathbf{g_a})_k a_k - a_k \\sum_j (\\mathbf{g_a})_j a_j\n$$\n用向量表示法，这可以写成 $\\mathbf{g_s} = \\mathbf{a} \\odot (\\mathbf{g_a} - (\\mathbf{g_a} \\cdot \\mathbf{a}))$，其中 $\\odot$ 是逐元素乘积，$\\cdot$ 是点积。将此应用于矩阵 $\\mathbf{A}$ 和 $\\mathbf{G_A}$ 的每一行 $i$：\n$$\n(\\mathbf{G}_{\\widetilde{\\mathbf{S}}})_{i,:} = \\mathbf{A}_{i,:} \\odot \\left( \\mathbf{G_A}_{i,:} - \\left( \\sum_{k=1}^L \\mathbf{A}_{i,k} \\mathbf{G_A}_{i,k} \\right) \\right)\n$$\n这样就从 $\\mathbf{G_A}$ 计算出 $\\mathbf{G}_{\\widetilde{\\mathbf{S}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\mathbf{S}}}$。\n\n**步骤 3：掩码操作 $\\widetilde{\\mathbf{S}} = \\operatorname{mask}(\\mathbf{S}, \\mathbf{M})$ 的 VJP**\n\n在前向传播中，如果 $\\mathbf{M}_{i,j} = 0$ (即 $j > i$)，则 $\\widetilde{\\mathbf{S}}_{i,j}$ 被设置为常量 $-\\infty$。如果 $\\mathbf{M}_{i,j} = 1$ (即 $j \\le i$)，则 $\\widetilde{\\mathbf{S}}_{i,j} = \\mathbf{S}_{i,j}$。梯度相应地向后传递：\n$$\n(\\mathbf{G_S})_{i,j} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{S}_{i,j}} = \\frac{\\partial \\mathcal{L}}{\\partial \\widetilde{\\mathbf{S}}_{i,j}} \\frac{\\partial \\widetilde{\\mathbf{S}}_{i,j}}{\\partial \\mathbf{S}_{i,j}}\n$$\n导数 $\\frac{\\partial \\widetilde{\\mathbf{S}}_{i,j}}{\\partial \\mathbf{S}_{i,j}}$ 在 $j \\le i$ 时为 1，在 $j  i$ 时为 0。因此，梯度只流经未被掩码的元素：\n$$\n(\\mathbf{G_S})_{i,j} = \\begin{cases} (\\mathbf{G}_{\\widetilde{\\mathbf{S}}})_{i,j}  \\text{if } j \\le i \\\\ 0  \\text{if } j  i \\end{cases}\n$$\n如果禁用了因果掩码，则 $\\mathbf{G_S} = \\mathbf{G}_{\\widetilde{\\mathbf{S}}}$。\n\n**步骤 4：缩放点积 $\\mathbf{S} = \\frac{1}{\\sqrt{d}} \\mathbf{Q} \\mathbf{K}^\\top$ 的 VJP**\n\n这一步同样是矩阵乘法，并按常数 $\\frac{1}{\\sqrt{d}}$ 进行了缩放。应用 VJP 法则：\n$$\n\\mathbf{G_Q} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}} = \\frac{1}{\\sqrt{d}} \\mathbf{G_S} (\\mathbf{K}^\\top)^\\top = \\frac{1}{\\sqrt{d}} \\mathbf{G_S} \\mathbf{K}\n$$\n对于 $\\mathbf{K}$，我们首先求出关于 $\\mathbf{K}^\\top$ 的梯度，然后将结果转置。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}^\\top} = \\frac{1}{\\sqrt{d}} \\mathbf{Q}^\\top \\mathbf{G_S}\n$$\n$$\n\\mathbf{G_K} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}} = \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}^\\top} \\right)^\\top = \\frac{1}{\\sqrt{d}} (\\mathbf{Q}^\\top \\mathbf{G_S})^\\top = \\frac{1}{\\sqrt{d}} \\mathbf{G_S}^\\top \\mathbf{Q}\n$$\n\n**因果梯度流分析**\n测试目标是验证对于一个仅在第 $t$ 行为非零的上游余切（梯度）$\\mathbf{G_Y}$，其下游余切（梯度）$\\mathbf{G_K}$ 和 $\\mathbf{G_V}$ 对于所有行 $j > t$ 均为零。\n\n1.  **到 $\\mathbf{V}$ 的梯度**：$\\mathbf{G_V} = \\mathbf{A}^\\top \\mathbf{G_Y}$。$\\mathbf{G_V}$ 的第 $j$ 行是 $(\\mathbf{G_V})_{j,:} = \\sum_{i=0}^{L-1} (\\mathbf{A}^\\top)_{j,i} (\\mathbf{G_Y})_{i,:} = \\sum_{i=0}^{L-1} \\mathbf{A}_{i,j} (\\mathbf{G_Y})_{i,:}$。由于 $\\mathbf{G_Y}$ 仅在第 $t$ 行非零，此和简化为 $(\\mathbf{G_V})_{j,:} = \\mathbf{A}_{t,j} (\\mathbf{G_Y})_{t,:}$。使用因果掩码时，注意力权重矩阵 $\\mathbf{A}$ 是一个下三角矩阵，意味着对于 $j > i$，有 $\\mathbf{A}_{i,j} = 0$。因此，在我们的情况中，当 $i=t$ 时，对于所有 $j > t$，我们有 $\\mathbf{A}_{t,j} = 0$。因此，对于所有 $j > t$，$(\\mathbf{G_V})_{j,:}$ 是一个零向量。\n\n2.  **到 $\\mathbf{K}$ 的梯度**：追踪到 $\\mathbf{K}$ 的梯度传播过程，可以揭示出类似的逻辑。\n    - $\\mathbf{G_A} = \\mathbf{G_Y} \\mathbf{V}^\\top$。由于 $\\mathbf{G_Y}$ 只有第 $t$ 行非零，$\\mathbf{G_A}$ 也将只有第 $t$ 行非零。\n    - $\\mathbf{G}_{\\widetilde{\\mathbf{S}}}$ 是从 $\\mathbf{G_A}$ 计算得出的，因此它也只有第 $t$ 行非零。\n    - $\\mathbf{G_S}$ 是通过对 $\\mathbf{G}_{\\widetilde{\\mathbf{S}}}$ 应用因果掩码计算得出的。这意味着对于所有 $i \\ne t$，有 $(\\mathbf{G_S})_{i,j} = 0$，并且对于所有 $j > t$，有 $(\\mathbf{G_S})_{t,j} = 0$。总而言之，$\\mathbf{G_S}$ 是一个仅在索引 $(t,j)$ 处（其中 $j \\le t$）可能存在非零项的矩阵。\n    - 最后，$\\mathbf{G_K} = \\frac{1}{\\sqrt{d}} \\mathbf{G_S}^\\top \\mathbf{Q}$。$\\mathbf{G_K}$ 的第 $j$ 行由 $(\\mathbf{G_K})_{j,:} = \\frac{1}{\\sqrt{d}} \\sum_{i=0}^{L-1} (\\mathbf{G_S}^\\top)_{j,i} \\mathbf{Q}_{i,:} = \\frac{1}{\\sqrt{d}} \\sum_{i=0}^{L-1} (\\mathbf{G_S})_{i,j} \\mathbf{Q}_{i,:}$ 给出。\n    - 由于 $(\\mathbf{G_S})_{i,j}$ 仅在 $i=t$ 时非零，这变为 $(\\mathbf{G_K})_{j,:} = \\frac{1}{\\sqrt{d}} (\\mathbf{G_S})_{t,j} \\mathbf{Q}_{t,:}$。\n    - 对于任何未来词元 $j > t$，我们从 $\\mathbf{G_S}$ 的结构中知道 $(\\mathbf{G_S})_{t,j} = 0$。因此，对于所有 $j > t$，$(\\mathbf{G_K})_{j,:}$ 必须是一个零向量。\n\n这从第一性原理上证实了因果掩码正确地阻止了信息从给定的时间步反向流动，从而影响未来时间步的参数。接下来的实现将把这些推导出的 VJP 方程编写成代码，并从数值上验证这一性质。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests the Vector-Jacobian Product (VJP) for scaled\n    dot-product attention with and without causal masking, verifying the\n    gradient flow properties of causal attention.\n    \"\"\"\n    \n    # Test suite format: (L, d, d_v, t, c, s, causal)\n    # L: sequence length, d: query/key dim, d_v: value dim,\n    # t: time index for gradient, c: channel index for gradient,\n    # s: random seed, causal: boolean for causal mask.\n    test_cases = [\n        (4, 3, 2, 1, 0, 0, True),\n        (1, 3, 2, 0, 1, 1, True),\n        (5, 4, 3, 4, 2, 2, True),\n        (4, 3, 2, 1, 1, 3, False),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        L, d, d_v, t, c, s, causal = case\n\n        # 1. Initialize inputs\n        rng = np.random.default_rng(s)\n        Q = rng.standard_normal((L, d), dtype=np.float64) * 0.5\n        K = rng.standard_normal((L, d), dtype=np.float64) * 0.5\n        V = rng.standard_normal((L, d_v), dtype=np.float64) * 0.5\n\n        # 2. Forward Pass\n        \n        # S = (Q @ K.T) / sqrt(d)\n        scores = (Q @ K.T) / np.sqrt(d)\n        \n        # Apply causal mask if specified\n        masked_scores = np.copy(scores)\n        if causal:\n            mask = np.triu(np.ones((L, L), dtype=bool), k=1)\n            masked_scores[mask] = -np.inf\n\n        # Numerically stable softmax\n        stable_scores = masked_scores - np.max(masked_scores, axis=1, keepdims=True)\n        A = np.exp(stable_scores)\n        A /= np.sum(A, axis=1, keepdims=True)\n        \n        # Y = A @ V\n        _Y = A @ V\n\n        # 3. Backward Pass (VJP)\n\n        # Initialize upstream gradient G_Y\n        G_Y = np.zeros((L, d_v), dtype=np.float64)\n        G_Y[t, c] = 1.0\n\n        # Backprop through Y = A @ V\n        # G_A = G_Y @ V.T\n        # G_V = A.T @ G_Y\n        G_A = G_Y @ V.T\n        G_V = A.T @ G_Y\n\n        # Backprop through A = softmax(S_tilde)\n        # G_S_tilde = A * (G_A - sum(G_A * A, axis=1))\n        row_wise_dot = np.sum(G_A * A, axis=1, keepdims=True)\n        G_S_tilde = A * (G_A - row_wise_dot)\n\n        # Backprop through masking\n        G_S = np.copy(G_S_tilde)\n        if causal:\n            # Gradient is zero for masked-out elements\n            mask = np.triu(np.ones((L, L), dtype=bool), k=1)\n            G_S[mask] = 0.0\n\n        # Backprop through S = (Q @ K.T) / sqrt(d)\n        # G_Q = (G_S @ K) / sqrt(d)\n        # G_K = (G_S.T @ Q) / sqrt(d)\n        sqrt_d = np.sqrt(d)\n        G_Q = (G_S @ K) / sqrt_d\n        G_K = (G_S.T @ Q) / sqrt_d\n\n        # 4. Verification\n        \n        # Check if gradients to future tokens are zero.\n        # This is vacuously true if t is the last token index.\n        if not causal:\n            # For the non-causal case, the predicate is not expected to hold, \n            # but we run the check for consistency. The expected result is False.\n             G_K_future = G_K[t + 1 :, :] if t  L -1 else np.array([[]])\n             G_V_future = G_V[t + 1 :, :] if t  L -1 else np.array([[]])\n             max_grad_k = np.max(np.abs(G_K_future)) if G_K_future.size > 0 else 0\n             max_grad_v = np.max(np.abs(G_V_future)) if G_V_future.size > 0 else 0\n             tolerance = 1e-12\n             predicate_holds = max(max_grad_k, max_grad_v) = tolerance\n        elif t >= L - 1:\n            predicate_holds = True\n        else:\n            # Extract gradients corresponding to future tokens (j > t)\n            G_K_future = G_K[t + 1 :, :]\n            G_V_future = G_V[t + 1 :, :]\n\n            # Compute max absolute value of these future gradients\n            max_grad_k = np.max(np.abs(G_K_future))\n            max_grad_v = np.max(np.abs(G_V_future))\n            \n            # Check if the maximum is below the tolerance\n            tolerance = 1e-12\n            predicate_holds = max(max_grad_k, max_grad_v) = tolerance\n            \n        results.append(predicate_holds)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3100434"}]}