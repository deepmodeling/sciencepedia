{"hands_on_practices": [{"introduction": "在我们深入探索 BERT 的强大功能之前，首先必须理解其内部构造。BERT 编码器块是其架构的基本重复单元，其规模由隐藏层大小和前馈网络维度等关键超参数决定。通过亲手计算一个编码器块中的参数数量，你将对模型的规模以及架构选择如何影响其复杂度建立一个具体而深刻的认识。[@problem_id:3102535]", "problem": "考虑一个来自“来自 Transformer 的双向编码器表示”（Bidirectional Encoder Representations from Transformers, BERT）中的单个编码器模块。该模块包含一个多头自注意力（MHSA）层，其后是一个两层的位置前馈网络，并带有残差连接和两个层归一化（LN）模块。假设采用以下与原始架构一致的典型设计选择：\n- 隐藏层大小为 $d$，注意力头的数量为 $h$，前馈网络的内部维度为 $d_{ff}$。\n- 查询、键和值的投影均实现为从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的可学习仿射映射，每个映射都有一个 $\\mathbb{R}^{d \\times d}$ 的权重矩阵和一个 $\\mathbb{R}^{d}$ 的偏置向量。同样，注意力输出投影也是一个具有 $\\mathbb{R}^{d \\times d}$ 的权重和 $\\mathbb{R}^{d}$ 的偏置的可学习仿射映射。\n- 前馈网络由两个可学习的仿射映射组成：第一个是从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d_{ff}}$，第二个是从 $\\mathbb{R}^{d_{ff}}$ 回到 $\\mathbb{R}^{d}$，每个映射都有一个权重矩阵和一个偏置向量。两者之间的非线性激活函数是高斯误差线性单元（GELU），且没有可学习的参数。\n- 每个模块应用两次层归一化，每个 LN 都有一个可学习的缩放因子 $\\gamma \\in \\mathbb{R}^{d}$ 和一个平移因子 $\\beta \\in \\mathbb{R}^{d}$。该模块中没有其他可学习的参数。\n- 词元嵌入和位置编码属于模块外部的嵌入层，因此不应在此计算。\n\n仅从核心定义出发——即从 $\\mathbb{R}^{m}$ 到 $\\mathbb{R}^{n}$ 的可学习仿射映射有 $n \\times m$ 个权重参数和 $n$ 个偏置参数，且每次应用层归一化会贡献 $2 \\times d$ 个参数（缩放和平移）——推导出一个关于 $d$、$d_{ff}$ 和 $h$ 的该编码器模块总可学习参数数量的闭式表达式。然后，提出一个变体，将头的数量从 $h$ 增加到 $\\alpha h$，同时对于给定的隐藏层大小 $d$ 和序列长度，保持 MHSA 投影和注意力分数乘法的总计算量不变（你可以假设每个头的维度是 $d/h$，且保持 $d$ 不变可以固定主要的 MHSA 计算量）。对于这个 $\\alpha = 2$ 的变体，给出最终的每个头的维度。\n\n最后，对于 BERT-Base 配置 $d = 768$，$d_{ff} = 3072$，$h = 12$ 和 $\\alpha = 2$，对你的表达式进行数值计算。以一个包含两个条目的行矩阵形式提供你的最终答案：\n- 编码器模块的总参数数量，以及\n- 将头的数量增加因子 $\\alpha$ 后的新每个头的维度。\n\n无需四舍五入；报告精确整数。以 $\\begin{pmatrix} \\text{param\\_count}  \\text{new\\_head\\_dim} \\end{pmatrix}$ 的行矩阵形式表达你的最终答案。", "solution": "该问题陈述被评估为有效。它在科学上基于已建立的 Transformer 模型架构，问题定义良好，提供了足够的信息以获得唯一解，并以客观、正式的语言表述。所提供的数据是一致且现实的，对应于众所周知的 BERT-Base 配置。该问题是分析深度学习模型参数化的一个标准的、非平凡的练习。\n\n解决方案分为三个部分：首先，推导 BERT 编码器模块中可学习参数数量的通用符号表达式；其次，分析所提出的架构变体；第三，对给定的配置，对推导出的表达式进行数值计算。\n\n**第一部分：总可学习参数数量的推导**\n\n可学习参数的总数 $P_{\\text{total}}$ 是其组成部分参数的总和：多头自注意力（MHSA）模块、位置前馈网络（FFN）和两个层归一化（LN）模块。我们将根据提供的定义计算每个组件的参数。\n\n一个从 $\\mathbb{R}^{m}$ 到 $\\mathbbR^{n}$ 的可学习仿射映射有一个大小为 $n \\times m$ 的权重矩阵和一个大小为 $n$ 的偏置向量，总共有 $n \\times m + n$ 个参数。\n\n1.  **多头自注意力（MHSA）参数 ($P_{\\text{MHSA}}$)**\n    MHSA 机制由问题陈述中定义的四个可学习的仿射投影组成。\n    *   **查询投影 (Q)：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_Q \\in \\mathbb{R}^{d \\times d}$：$d \\times d = d^2$ 个参数。\n        - 偏置向量 $b_Q \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - Q 的总参数：$d^2 + d$。\n    *   **键投影 (K)：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_K \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_K \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - K 的总参数：$d^2 + d$。\n    *   **值投影 (V)：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_V \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_V \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - V 的总参数：$d^2 + d$。\n    *   **输出投影 (O)：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_O \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_O \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - O 的总参数：$d^2 + d$。\n    MHSA 模块中的参数总数是这四个分量的总和。请注意，头的数量 $h$ 在此公式中没有明确出现，因为投影被定义为对维度为 $d$ 的整个隐藏状态进行的单个变换。\n    $$P_{\\text{MHSA}} = (d^2 + d) + (d^2 + d) + (d^2 + d) + (d^2 + d) = 4(d^2 + d) = 4d^2 + 4d$$\n\n2.  **位置前馈网络（FFN）参数 ($P_{\\text{FFN}}$)**\n    FFN 由两个可学习的仿射映射和一个 GELU 非线性激活函数组成，该激活函数没有参数。\n    *   **第一层：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d_{ff}}$ 的仿射映射。\n        - 权重矩阵 $W_1 \\in \\mathbb{R}^{d_{ff} \\times d}$：$d_{ff} \\times d$ 个参数。\n        - 偏置向量 $b_1 \\in \\mathbb{R}^{d_{ff}}$：$d_{ff}$ 个参数。\n        - 第一层的总参数：$d \\cdot d_{ff} + d_{ff}$。\n    *   **第二层：** 从 $\\mathbb{R}^{d_{ff}}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_2 \\in \\mathbb{R}^{d \\times d_{ff}}$：$d \\times d_{ff}$ 个参数。\n        - 偏置向量 $b_2 \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - 第二层的总参数：$d \\cdot d_{ff} + d$。\n    FFN 中的参数总数是这两层的总和。\n    $$P_{\\text{FFN}} = (d \\cdot d_{ff} + d_{ff}) + (d \\cdot d_{ff} + d) = 2d \\cdot d_{ff} + d_{ff} + d$$\n\n3.  **层归一化（LN）参数 ($P_{\\text{LN}}$)**\n    该模块包含两个层归一化模块。每个模块都有一个可学习的缩放向量 $\\gamma \\in \\mathbb{R}^{d}$ 和一个可学习的平移向量 $\\beta \\in \\mathbb{R}^{d}$。\n    - 每个 LN 模块的参数：$d + d = 2d$。\n    - 两个 LN 模块的总参数：\n    $$P_{\\text{LN}} = 2 \\times (2d) = 4d$$\n\n**总参数 ($P_{\\text{total}}$)**\n编码器模块中可学习参数的总数是所有组件参数的总和。\n$$P_{\\text{total}} = P_{\\text{MHSA}} + P_{\\text{FFN}} + P_{\\text{LN}}$$\n$$P_{\\text{total}} = (4d^2 + 4d) + (2d \\cdot d_{ff} + d_{ff} + d) + (4d)$$\n$$P_{\\text{total}} = 4d^2 + (4d + d + 4d) + 2d \\cdot d_{ff} + d_{ff}$$\n$$P_{\\text{total}} = 4d^2 + 9d + d_{ff}(2d + 1)$$\n这就是总参数数量的闭式表达式。\n\n**第二部分：架构变体与新的每个头的维度**\n\n该问题提出了一个变体，其中注意力头的数量从 $h$ 变为 $h' = \\alpha h$，而模型的隐藏层大小 $d$ 保持不变。在标准的 Transformer 架构中，所有头的总维度等于模型的隐藏层大小。也就是说，如果 $d_k$ 是每个头的查询、键和值向量的维度，那么 $h \\cdot d_k = d$。\n\n原始的每个头的维度是 $d_k = \\frac{d}{h}$。\n新的头的数量是 $h' = \\alpha h$。\n为了保持关系 $h' \\cdot d'_k = d$，其中 $d'_k$ 是新的每个头的维度，我们可以求解 $d'_k$：\n$$d'_k = \\frac{d}{h'} = \\frac{d}{\\alpha h} = \\frac{1}{\\alpha} \\left(\\frac{d}{h}\\right)$$\n新的每个头的维度是原始每个头的维度乘以 $1/\\alpha$。\n\n**第三部分：数值计算**\n\n我们给定的 BERT-Base 配置为：$d = 768$，$d_{ff} = 3072$，$h = 12$，以及修改因子 $\\alpha = 2$。\n\n*   **总参数数量：**\n    我们将给定值代入推导出的 $P_{\\text{total}}$ 公式中。\n    $$P_{\\text{total}} = 4(768)^2 + 9(768) + 3072(2 \\cdot 768 + 1)$$\n    $$P_{\\text{total}} = 4(589824) + 6912 + 3072(1536 + 1)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 3072(1537)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 4721664$$\n    $$P_{\\text{total}} = 7087872$$\n\n*   **新的每个头的维度：**\n    原始头的数量是 $h=12$。新的头的数量是 $h' = \\alpha h = 2 \\times 12 = 24$。\n    隐藏层大小是 $d = 768$。新的每个头的维度 $d'_k$ 是：\n    $$d'_k = \\frac{d}{h'} = \\frac{768}{24}$$\n    为了计算这个值，我们可以先除以 $12$ 再除以 $2$：\n    $$d'_k = \\frac{768/12}{2} = \\frac{64}{2} = 32$$\n    新的每个头的维度是 $32$。\n\n最终答案要求一个包含总参数数量和新的每个头的维度的行矩阵。", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 7087872  32 \\end{pmatrix}\n}\n$$", "id": "3102535"}, {"introduction": "BERT 的强大能力在于其理解词语之间复杂关系的能力，而自注意力机制是实现这一能力的核心。本练习将让你扮演一名“侦探”，通过分析简化的注意力分数来解决一个指代消解难题。通过这个过程，你不仅能揭示模型如何进行推理，还能识别出哪些注意力头专门负责追踪长距离的词语依赖关系。[@problem_id:3102501]", "problem": "给定一个简化的单层多头自注意力设置，其灵感来自BERT（Bidirectional Encoder Representations from Transformers）。目标是在三个共指消解示例中，计算从一个代词词元到其候选先行词的基于注意力的归因，并识别哪些注意力头专用于处理长程依赖关系。\n\n基本原理：\n- 在缩放点积自注意力中，会为每个词元计算查询（queries）、键（keys）和值（values）。从词元 $i$ 到词元 $j$ 的注意力权重为 $a_{ij} = \\mathrm{softmax}\\left(\\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\\right)$，由于softmax函数的特性，该权重是行随机的（row-stochastic），因此对于每一行 $i$，都有 $\\sum_{j=0}^{N-1} a_{ij} = 1$。\n- 在具有 $H$ 个头的多头注意力中，每个头 $h$ 产生一个行随机矩阵 $A^{(h)} \\in \\mathbb{R}^{N \\times N}$。对所有头进行简单的均匀聚合，得到 $A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)}$，该矩阵也是行随机的。\n- 在注意力归因层面，残差连接可以通过凸组合 $\\tilde{A} = \\lambda I + (1 - \\lambda) A$ 来近似，其中 $I$ 是单位矩阵，$\\lambda \\in [0,1]$ 控制词元原始表示的保留程度。\n- 从源索引 $s$到所有词元的注意力归因计算为行向量 $e_s^\\top \\tilde{A}$，其中 $e_s$ 是在位置 $s$ 处为1、其余位置为0的独热行向量。对于候选先行词索引 $k$ 的归因分数是 $e_s^\\top \\tilde{A}$ 的第 $k$ 个元素。\n\n长程专业化定义：\n- 固定一个距离阈值 $D \\in \\mathbb{N}$ 和一个专业化阈值 $T \\in [0,1]$。对于头 $h$，在源索引 $s$ 处的长程权重和（long-range mass）为 $r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k}$。如果 $r^{(h)}(s) \\ge T$，则认为头 $h$ 在 $s$ 处是“长程专业化”的。\n\n完整注意力矩阵的假设与构建：\n- 对于每个测试用例，代词所在行（即代词索引处的查询）被明确指定为每个头在所有词元索引上的概率分布。对于所有非代词行，假设一个简单的、科学上合理的局部上下文分布：设 $w_{\\text{self}} = 0.7$，并将其余的权重 $1 - w_{\\text{self}} = 0.3$ 平均分配给现有的直接相邻词元（左侧和右侧），并根据需要进行边界调整，以使每个非代词行的总和为1。\n\n索引规则：\n- 使用从零开始的索引。词元索引从 $0$到 $N-1$。\n- 每个测试用例都提供了候选先行词的索引。如果两个候选者获得相等的最大归因值（在机器精度范围内），则选择最小的索引来打破平局。\n\n全局参数：\n- 层数 $L = 1$。\n- 头数 $H = 3$。\n- 残差混合参数 $\\lambda = 0.2$。\n- 长程距离阈值 $D = 2$。\n- 专业化阈值 $T = 0.6$。\n\n测试套件：\n- 测试用例 1（短程 vs. 中程共指消解）：\n  - 序列长度 $N = 7$。\n  - 代词索引 $s = 5$。\n  - 候选先行词 $\\{1, 3\\}$。\n  - 每个头的代词行分布：\n    - 头 0：$[0.05, 0.30, 0.05, 0.45, 0.05, 0.00, 0.10]$。\n    - 头 1：$[0.10, 0.20, 0.10, 0.40, 0.10, 0.00, 0.10]$。\n    - 头 2：$[0.05, 0.20, 0.05, 0.55, 0.05, 0.00, 0.10]$。\n- 测试用例 2（更靠前的长程先行词）：\n  - 序列长度 $N = 12$。\n  - 代词索引 $s = 11$。\n  - 候选先行词 $\\{6, 9\\}$。\n  - 每个头的代词行分布：\n    - 头 0：$[0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.55, 0.05, 0.15, 0.05, 0.03, 0.02]$。\n    - 头 1：$[0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.30, 0.05, 0.10, 0.35, 0.03, 0.02]$。\n    - 头 2：$[0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.45, 0.05, 0.20, 0.10, 0.03, 0.02]$。\n- 测试用例 3（两个候选者之间出现平局）：\n  - 序列长度 $N = 6$。\n  - 代词索引 $s = 5$。\n  - 候选先行词 $\\{1, 4\\}$。\n  - 每个头的代词行分布：\n    - 头 0：$[0.05, 0.40, 0.05, 0.15, 0.30, 0.05]$。\n    - 头 1：$[0.05, 0.20, 0.05, 0.30, 0.40, 0.00]$。\n    - 头 2：$[0.05, 0.30, 0.05, 0.20, 0.20, 0.20]$。\n\n每个测试用例所需的计算：\n1. 构建每个头的完整注意力矩阵 $A^{(h)} \\in \\mathbb{R}^{N \\times N}$，方法是将给定的代词行放置在索引 $s$ 处，并使用上述局部上下文规则填充所有其他行。验证每行的总和为 $1$。\n2. 计算均匀头部聚合 $A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)}$，然后计算残差混合矩阵 $\\tilde{A} = \\lambda I + (1 - \\lambda) A$。\n3. 计算归因向量 $u = e_s^\\top \\tilde{A}$，并提取候选索引处的归因分数。选择具有最大归因值的先行词索引 $k^\\star$；通过选择最小索引来打破平局。\n4. 对每个头 $h$，计算长程权重和 $r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k}$。报告满足 $r^{(h)}(s) \\ge T$ 的头索引列表。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个列表，每个测试用例对应一个条目。每个条目本身必须是一个包含两个元素的列表：选定的先行词索引 $k^\\star$（一个整数）和长程专业化头的索引列表（一个整数列表）。整个输出必须打印为一个用方括号括起来的、无空格的、逗号分隔的列表，例如 $[\\,[k_1^\\star,[h_{1,1},\\dots]],[k_2^\\star,[\\dots]],\\dots\\,]$。", "solution": "用户的请求是一个有效且定义明确的问题，需要逐步应用从Transformer架构中抽象出来的自注意力机制原理。我将提供详细的解决方案。\n\n该问题要求在三个测试用例中分析一个简化的单层多头自注意力模型。对于每个用例，我们必须从一组候选中确定代词最可能的先行词，并识别哪些注意力头专用于处理长程依赖关系。\n\n分析基于以下全局参数：\n- 注意力头数：$H = 3$\n- 残差混合参数：$\\lambda = 0.2$\n- 长程距离阈值：$D = 2$\n- 长程专业化阈值：$T = 0.6$\n- 非代词行的自注意力权重：$w_{\\text{self}} = 0.7$\n\n每个测试用例的解决方案涉及四个连续的计算步骤。\n\n**步骤 1：构建每个头的注意力矩阵 ($A^{(h)}$)**\n对于每个头 $h \\in \\{0, 1, 2\\}$，我们需要构建一个完整的 $N \\times N$ 注意力矩阵 $A^{(h)}$。与索引为 $s$ 的代词相对应的行，表示为 $A^{(h)}_{s,:}$，被明确地以概率分布的形式提供。所有其他行 $i \\neq s$ 根据局部上下文启发式规则填充：自注意力权重为 $A^{(h)}_{i,i} = w_{\\text{self}} = 0.7$。剩余的概率 $1 - w_{\\text{self}} = 0.3$ 被平均分配给其直接相邻词元。这导致以下对 $i \\neq s$ 的赋值：\n- 对于内部词元 $i \\in \\{1, \\dots, N-2\\}$：$A^{(h)}_{i, i-1} = 0.15$ 和 $A^{(h)}_{i, i+1} = 0.15$。\n- 对于第一个词元 $i=0$：$A^{(h)}_{0, 1} = 0.3$。\n- 对于最后一个词元 $i=N-1$：$A^{(h)}_{N-1, N-2} = 0.3$。\n行 $i \\neq s$ 中的所有其他条目均为零。这种构造确保每个矩阵 $A^{(h)}$ 都是行随机的。\n\n**步骤 2：头聚合与残差混合**\n将各个头的矩阵进行平均，形成一个单一的聚合注意力矩阵 $A$：\n$$ A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)} $$\n接下来，通过取 $A$ 和单位矩阵 $I$ 的凸组合来建模残差连接：\n$$ \\tilde{A} = \\lambda I + (1 - \\lambda) A $$\n当 $\\lambda = 0.2$ 时，公式变为 $\\tilde{A} = 0.2 I + 0.8 A$。\n\n**步骤 3：通过注意力归因进行先行词识别**\n从源 $s$ 处的代词到所有其他词元的归因由 $\\tilde{A}$ 的第 $s$ 行给出，我们将其表示为向量 $u = e_s^\\top \\tilde{A}$。对于索引为 $k$ 的候选先行词，其分数为该向量的第 $k$ 个元素，即 $u_k = (\\tilde{A})_{s,k}$。\n通过找到具有最大归因分数的候选词，从给定的候选集合中选择先行词 $k^\\star$：\n$$ k^\\star = \\arg\\max_{k \\in \\text{Candidates}} u_k $$\n如果出现平局，则选择索引最小的候选词。需要注意的是，对于任何候选词 $k \\neq s$，其分数为 $u_k = (1-\\lambda)A_{s,k}$。由于 $(1-\\lambda)$ 是一个正常数，最大化 $u_k$ 等同于最大化 $A_{s,k}$。\n\n**步骤 4：识别长程专业化的头**\n如果一个头 $h$ 对距离大于或等于 $D$ 的词元的总注意力达到或超过阈值 $T$，则该头在源索引 $s$ 处被视为“长程专业化”。长程权重和计算如下：\n$$ r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k} $$\n如果 $r^{(h)}(s) \\ge T$，则该头是专业化的。\n\n一个关键的观察是，步骤3（先行词识别）和步骤4（专业化识别）的结果仅取决于代词所在行 $A^{(h)}_{s,:}$。因此，构建非代词行对于最终所需的输出是无关紧要的。然而，为了严格遵守问题陈述，我们仍按规定执行此过程。\n\n**各测试用例的计算与结果：**\n\n**测试用例 1：**\n- 参数：$N=7$，$s=5$，候选词 $=\\{1, 3\\}$。\n- **先行词识别**：我们比较候选词 $k=1$ 和 $k=3$ 的聚合注意力分数。\n  - $A_{5,1} = \\frac{1}{3} (A^{(0)}_{5,1} + A^{(1)}_{5,1} + A^{(2)}_{5,1}) = \\frac{1}{3} (0.30 + 0.20 + 0.20) = \\frac{0.70}{3} \\approx 0.2333$\n  - $A_{5,3} = \\frac{1}{3} (A^{(0)}_{5,3} + A^{(1)}_{5,3} + A^{(2)}_{5,3}) = \\frac{1}{3} (0.45 + 0.40 + 0.55) = \\frac{1.40}{3} \\approx 0.4667$\n  由于 $A_{5,3} > A_{5,1}$，选择的先行词是 $k^\\star = 3$。\n- **长程专业化识别**：对于 $s=5$ 和 $D=2$，长程索引为 $k \\in \\{0, 1, 2, 3\\}$。\n  - $r^{(0)}(5) = A^{(0)}_{5,0}+A^{(0)}_{5,1}+A^{(0)}_{5,2}+A^{(0)}_{5,3} = 0.05 + 0.30 + 0.05 + 0.45 = 0.85 \\ge 0.6$。头 0 是专业化的。\n  - $r^{(1)}(5) = A^{(1)}_{5,0}+A^{(1)}_{5,1}+A^{(1)}_{5,2}+A^{(1)}_{5,3} = 0.10 + 0.20 + 0.10 + 0.40 = 0.80 \\ge 0.6$。头 1 是专业化的。\n  - $r^{(2)}(5) = A^{(2)}_{5,0}+A^{(2)}_{5,1}+A^{(2)}_{5,2}+A^{(2)}_{5,3} = 0.05 + 0.20 + 0.05 + 0.55 = 0.85 \\ge 0.6$。头 2 是专业化的。\n- 结果：$k^\\star=3$；专业化的头：$[0, 1, 2]$。\n\n**测试用例 2：**\n- 参数：$N=12$，$s=11$，候选词 $=\\{6, 9\\}$。\n- **先行词识别**：\n  - $A_{11,6} = \\frac{1}{3}(0.55 + 0.30 + 0.45) = \\frac{1.30}{3} \\approx 0.4333$\n  - $A_{11,9} = \\frac{1}{3}(0.15 + 0.35 + 0.10) = \\frac{0.60}{3} = 0.2000$\n  由于 $A_{11,6} > A_{11,9}$，选择的先行词是 $k^\\star = 6$。\n- **长程专业化识别**：对于 $s=11$ 和 $D=2$，长程索引为 $k \\in \\{0, \\dots, 9\\}$。长程权重和可以计算为 $r^{(h)}(11) = 1 - (A^{(h)}_{11,10} + A^{(h)}_{11,11})$。\n  - 对于所有头 $h \\in \\{0,1,2\\}$，我们有 $A^{(h)}_{11,10}=0.03$ 和 $A^{(h)}_{11,11}=0.02$。\n  - 因此，对于每个头，$r^{(h)}(11) = 1 - (0.03 + 0.02) = 0.95 \\ge 0.6$。所有三个头都是专业化的。\n- 结果：$k^\\star=6$；专业化的头：$[0, 1, 2]$。\n\n**测试用例 3：**\n- 参数：$N=6$，$s=5$，候选词 $=\\{1, 4\\}$。\n- **先行词识别**：\n  - $A_{5,1} = \\frac{1}{3}(0.40 + 0.20 + 0.30) = \\frac{0.90}{3} = 0.3000$\n  - $A_{5,4} = \\frac{1}{3}(0.30 + 0.40 + 0.20) = \\frac{0.90}{3} = 0.3000$\n  分数相同。平局规则要求选择最小的索引，因此 $k^\\star = 1$。\n- **长程专业化识别**：对于 $s=5$ 和 $D=2$，长程索引为 $k \\in \\{0, 1, 2, 3\\}$。\n  - $r^{(0)}(5) = 0.05 + 0.40 + 0.05 + 0.15 = 0.65 \\ge 0.6$。头 0 是专业化的。\n  - $r^{(1)}(5) = 0.05 + 0.20 + 0.05 + 0.30 = 0.60 \\ge 0.6$。头 1 是专业化的。\n  - $r^{(2)}(5) = 0.05 + 0.30 + 0.05 + 0.20 = 0.60 \\ge 0.6$。头 2 是专业化的。\n- 结果：$k^\\star=1$；专业化的头：$[0, 1, 2]$。", "answer": "[[3,[0,1,2]],[6,[0,1,2]],[1,[0,1,2]]]", "id": "3102501"}, {"introduction": "尽管像 BERT 这样的大型语言模型功能强大，但它们并非无懈可击，有时会暴露出令人惊讶的“盲点”。理解并探测这些弱点是当前一个至关重要的研究领域。在本练习中，你将亲手实现一种名为 HotFlip 的梯度引导攻击算法，通过对输入进行微小改动来“欺骗”模型，从而让你对如何评估和挑战模型的稳健性有一个深入的实践理解。[@problem_id:3102527]", "problem": "给定一个经过微调的双向编码器表示来自变换器 (BERT) 情感分类器的一阶代理模型。该代理模型对作用于聚合词符表示的最终分类头进行建模。设词汇表大小为 $V$，嵌入维度为 $d$，一个长度为 $L$ 的序列由词符索引 $\\{t_1, t_2, \\dots, t_L\\}$ 表示，其中每个 $t_i \\in \\{0,1,\\dots,V-1\\}$。嵌入矩阵为 $E \\in \\mathbb{R}^{V \\times d}$，其行 $E_k \\in \\mathbb{R}^d$ 对应词符索引 $k$。序列表示是其词符嵌入的平均值，\n$$\nh = \\frac{1}{L} \\sum_{i=1}^L E_{t_i}.\n$$\n一个权重为 $W \\in \\mathbb{R}^d$、偏置为 $b \\in \\mathbb{R}$ 的逻辑回归头计算 logit 值\n$$\nz = W^\\top h + b,\n$$\n正类的概率\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}},\n$$\n预测标签 $\\hat{y}$ 由阈值规则定义：如果 $p \\ge 0.5$，则 $\\hat{y} = 1$，否则 $\\hat{y} = 0$。对于一个标签 $y \\in \\{0,1\\}$，其二元交叉熵损失为\n$$\n\\mathcal{L}(y,p) = -\\left(y\\log(p) + (1-y)\\log(1-p)\\right).\n$$\n\nHotFlip 攻击通过翻转词符来构建对抗性样本，其目标是在最小化输入改动的同时，最大化损失的增量。使用一阶泰勒近似，将位置 $j$ 上的单个词符从 $a$ 翻转为 $b$，所造成的损失变化近似为\n$$\n\\Delta \\mathcal{L}_{j,a \\rightarrow b} \\approx \\nabla_{E_{t_j}} \\mathcal{L}(y,p)^\\top \\left(E_b - E_a\\right).\n$$\n在上述代理模型下，根据链式法则可得\n$$\n\\nabla_{E_{t_j}} \\mathcal{L}(y,p) = \\frac{1}{L} \\left(p - y\\right) W,\n$$\n因此\n$$\n\\Delta \\mathcal{L}_{j,a \\rightarrow b} \\approx \\frac{p - y}{L} \\, W^\\top \\left(E_b - E_a\\right).\n$$\n\n您的任务是实现一个贪婪的 HotFlip 攻击，该攻击针对每个输入序列，迭代地应用能够最大化 $\\Delta \\mathcal{L}$ 的单次翻转 $(j,a \\rightarrow b)$，其中 $y$ 固定为当前预测的标签 $\\hat{y}$（也就是说，您总是最大化当前预测类别的损失）。每次翻转后，重新计算 $p$ 和 $\\hat{y}$，并持续此过程，直到预测的标签翻转到相反的类别。当多个翻转产生相同的 $\\Delta \\mathcal{L}$ 时，首先选择最小的新词符索引 $b$ 来打破平局，然后选择最小的位置索引 $j$。如果没有可行的翻转（在本测试套件中不会出现这种情况），则序列保持不变。目标是在最小化词符改动的同时改变情感，因此当预测翻转到相反类别时，贪婪过程必须立即停止。\n\n使用以下固定参数，它们定义了 BERT 分类头及其词汇表的代理模型：\n\n- 词汇表大小 $V = 6$，嵌入维度 $d = 2$。\n- 嵌入矩阵 $E \\in \\mathbb{R}^{6 \\times 2}$ 按行定义如下\n  - $E_0 = [0.0, 0.0]$,\n  - $E_1 = [2.0, 2.0]$,\n  - $E_2 = [-2.0, -2.0]$,\n  - $E_3 = [1.0, 0.5]$,\n  - $E_4 = [-1.0, -0.5]$,\n  - $E_5 = [-1.0, -0.5]$.\n- 分类器权重 $W = [1.0, 1.0]$ 和偏置 $b = 0.0$。\n\n设计并实现一个算法来执行上述的贪婪 HotFlip 攻击。您必须将其应用于以下序列测试套件（每个序列是一个词符索引列表）：\n\n- 测试用例 1（一般情况）：$[\\,3,\\,1,\\,0,\\,3\\,]$。\n- 测试用例 2（需要多次翻转）：$[\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4\\,]$。\n- 测试用例 3（候选词符间的平局打破）：$[\\,3,\\,3\\,]$。\n\n对于每个测试用例，当预测标签翻转到相反类别时，程序必须输出通过贪婪 HotFlip 过程获得的最终对抗性词符索引序列。每个测试用例的答案必须是一个整数列表。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，每个内部列表的格式不含空格。例如，格式必须是\n$$\n[\\,[a_1,a_2,\\dots],\\,[b_1,b_2,\\dots],\\,[c_1,c_2,\\dots]\\,].\n$$", "solution": "用户希望在一个简化的情感分类器代理模型上实现一个贪婪的 HotFlip 对抗性攻击。\n\n### 步骤 1：问题验证\n\n**1. 提取已知条件**\n\n- **模型定义：**\n  - 词汇表大小 $V = 6$，嵌入维度 $d = 2$。\n  - 长度为 $L$ 的序列：$\\{t_1, t_2, \\dots, t_L\\}$，其中 $t_i \\in \\{0, \\dots, 5\\}$。\n  - 嵌入矩阵 $E \\in \\mathbb{R}^{6 \\times 2}$ 由以下给出：\n    $E_0 = [\\,0.0,\\, 0.0\\,]$, $E_1 = [\\,2.0,\\, 2.0\\,]$, $E_2 = [\\,-2.0,\\, -2.0\\,]$, $E_3 = [\\,1.0,\\, 0.5\\,]$, $E_4 = [\\,-1.0,\\, -0.5\\,]$, $E_5 = [\\,-1.0,\\, -0.5\\,]$。\n  - 序列表示：$h = \\frac{1}{L} \\sum_{i=1}^L E_{t_i}$。\n  - 分类器权重 $W = [\\,1.0,\\, 1.0\\,]$ 和偏置 $b = 0.0$。\n  - Logit 值：$z = W^\\top h + b$。\n  - 正类概率：$p = \\sigma(z)$。\n  - 预测规则：如果 $p \\ge 0.5$（即 $z \\ge 0$），则 $\\hat{y} = 1$，否则 $\\hat{y} = 0$。\n\n- **攻击定义：**\n  - **贪婪迭代过程：** 在每一步，应用能够最大化近似损失变化 $\\Delta \\mathcal{L}$ 的单次词符翻转 $(j, a \\rightarrow b)$。\n  - **目标函数：** 最大化 $\\Delta \\mathcal{L}_{j,a \\rightarrow b} \\approx \\frac{p - \\hat{y}}{L} \\, W^\\top \\left(E_b - E_a\\right)$，其中 $\\hat{y}$ 是当前预测的标签。\n  - **终止条件：** 当预测标签 $\\hat{y}$ 翻转到与初始预测相反时，立即停止。\n  - **平局打破规则：** 如果多个翻转产生相同的最大 $\\Delta \\mathcal{L}$，选择新词符索引 $b$ 最小的那个。如果仍然平局，选择位置索引 $j$ 最小的那个。\n\n- **测试用例：**\n  1. $[\\,3,\\,1,\\,0,\\,3\\,]$\n  2. $[\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4\\,]$\n  3. $[\\,3,\\,3\\,]$\n\n- **输出格式：** 单行 `[[a1,a2,...],[b1,b2,...],[c1,c2,...]]`。\n\n**2. 使用提取的已知条件进行验证**\n\n- **科学依据：** 该问题在深度学习领域，特别是在神经网络的对抗性攻击方面，有充分的理论基础。该模型是一个作用于平均嵌入的标准逻辑回归分类器。攻击方法 HotFlip 是一种已知的技术，使用一阶泰勒近似来指导搜索是一种标准方法。所涉及的数学（向量微积分、线性代数）是正确的。\n- **良定性：** 该问题是良定的。模型、参数和输入都已完全指定。贪婪算法有明确的定义，包括清晰的目标函数、终止条件和全面的平局打破规则。这确保了每个测试用例都存在唯一的解。\n- **客观性：** 该问题以精确、客观和数学化的语言陈述，没有歧义或主观因素。\n\n**3. 结论与行动**\n\n该问题是有效的。它在科学上是合理的、良定的和客观的。我将继续提供解决方案。\n\n### 步骤 2：算法设计与原则\n\n任务的核心是实现指定的贪婪迭代攻击。对于每个测试用例，我们从初始序列及其预测开始。然后，我们进行循环，在每次迭代中，我们找到并应用单个最佳的词符翻转，直到预测结果被逆转。\n\n**1. 简化目标函数**\n目标是最大化 $\\Delta \\mathcal{L} \\approx \\frac{p - \\hat{y}}{L} W^\\top(E_b - E_a)$。我们来分析一下 $\\frac{p - \\hat{y}}{L}$ 这一项：\n- 如果当前预测为 $\\hat{y} = 1$，则 $p \\ge 0.5$，所以 $p-1 \\le 0$。要最大化 $\\Delta \\mathcal{L}$，就需要最小化 $S_{j,b} = W^\\top(E_b - E_a)$ 这一项。\n- 如果当前预测为 $\\hat{y} = 0$，则 $p  0.5$，所以 $p-0 > 0$。要最大化 $\\Delta \\mathcal{L}$，就需要最大化 $S_{j,b} = W^\\top(E_b - E_a)$ 这一项。\n\n对于每个词符 $k$，$W^\\top E_k$ 这一项在整个过程中是恒定的。我们可以预先计算这些值，以加速寻找最佳翻转的过程。我们定义一个向量 `wte_scores`，其中 `wte_scores[k]` $= W^\\top E_k$。\n\n**2. 贪婪搜索策略**\n在攻击的每一步，我们必须根据目标和平局打破规则找到最优的配对 $(j, b)$。一个稳健的实现方法是采用两阶段过程：\n1.  **寻找最佳分数：** 遍历所有可能的翻转（所有位置 $j$ 和所有潜在的新词符 $b \\neq t_j$），并找到最佳可能的分数（$S_{j,b}$ 的最小值或最大值）。\n2.  **收集与选择：** 收集所有达到此最佳分数的翻转 $(j, b)$。从此最优候选列表中，应用平局打破规则：首先按新词符索引 $b$ 排序，然后按位置索引 $j$ 排序，并选择第一个。\n\n**3. 高效更新**\n每次翻转后都从头计算序列表示 $h = \\frac{1}{L} \\sum E_{t_i}$ 效率很低。一种更高效的方法是更新嵌入的总和。如果我们将位置 $j$ 上的词符 $a$ 翻转为词符 $b$，新的嵌入总和为：\n$$ \\sum_{\\text{new}} = \\sum_{\\text{old}} - E_a + E_b $$\n我们可以维护这个总和，并且仅在需要计算预测时才通过除以 $L$ 来计算 $h$。\n\n**4. 算法**\n对于每个测试用例：\n1.  用输入序列初始化 `tokens`。\n2.  计算初始的嵌入总和 $\\sum E = \\sum_{i=1}^L E_{t_i}$。\n3.  从 $\\sum E$ 计算初始预测 $\\hat{y}_{\\text{initial}}$。\n4.  进入一个 `while` 循环，直到预测翻转。\n    a.  在循环内部，确定当前预测 $\\hat{y}_{\\text{current}}$。\n    b.  如果 $\\hat{y}_{\\text{current}} \\neq \\hat{y}_{\\text{initial}}$，则跳出循环。\n    c.  找到最佳候选翻转集合：\n        i. 初始化一个空列表 `candidate_flips` 和 `best_score` 为 $+\\infty$ 或 $-\\infty$。\n        ii. 遍历每个位置 $j \\in [0, L-1]$ 和每个潜在的新词符 $b \\in [0, V-1]$。\n        iii. 如果 $b$ 与位置 $j$ 上的当前词符相同，则跳过。\n        iv. 计算分数 $S_{j,b} = \\text{wte\\_scores}[b] - \\text{wte\\_scores}[t_j]$。\n        v. 将 `score` 与 `best_score` 比较。如果更优，则将 `candidate_flips` 重置为 $[(j, b)]$ 并更新 `best_score`。如果相等，则将 $(j, b)$ 附加到 `candidate_flips`。\n    d.  应用平局打破规则：按 $b$ 然后按 $j$ 对 `candidate_flips` 进行排序。取第一个元素作为 `(best_j, best_b)`。\n    e.  更新状态：\n        i. 令 $a = tokens[\\text{best\\_j}]$。\n        ii. 更新 `tokens[best_j] = best_b`。\n        iii. 更新 $\\sum E = \\sum E - E_a + E_b$。\n5.  将最终的 `tokens` 列表添加到结果中。\n6.  格式化并打印所有结果。\n\n此过程正确且高效地实现了指定的攻击。", "answer": "[[3,2,0,3],[2,2,2,2,2,2,2,2],[2,3]]", "id": "3102527"}]}