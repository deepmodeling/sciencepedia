{"hands_on_practices": [{"introduction": "在设计卷积神经网络时，精确控制特征图的空间维度至关重要。这个练习将挑战你回归第一性原理：仅仅根据卷积核的有效放置和步长定义，推导出为达到特定输出尺寸所需的精确填充量。通过这个过程，你将不再是简单地将数字代入现有公式，而是深刻理解这些公式背后的几何与算术约束。[@problem_id:3177669]", "problem": "在深度学习中，一个一维 (1D) 离散卷积层接收一个长度为 $n$ 的输入，应用一个宽度为 $k$ 的卷积核，使用步长 $s \\in \\mathbb{N}$，并在左侧和右侧各进行 $p \\in \\mathbb{Z}_{\\ge 0}$ 个元素的对称零填充。假设使用标准的、带步长的连续采样（无空洞卷积），并且只有当整个卷积核窗口位于填充后的输入内部时，才会产生一个输出。\n\n仅从步长下有效卷积核位置的索引定义出发，推导关于 $p$ 的约束条件，以保证输出长度恰好为预设的目标值 $m \\in \\mathbb{N}$。将这些约束明确地表示为包含 $n$, $k$, $s$, $p$, 和 $m$ 的不等式，然后解这些关于 $p$ 的不等式，以获得所有可行 $p$ 值的闭式表征（涉及整数边界）。讨论当 $s>1$ 时的可行性细节，包括 $2p$ 中隐含的奇偶性，并解释最后一个与步长对齐的卷积核位置恰好落在填充后输入末端的边界情况。\n\n然后，对于具体配置 $n=37$，$k=7$，$s=3$ 和目标 $m=12$，判断是否存在一个可行的非负整数 $p$，如果存在，计算出最小的那个 $p$ 值。如果不存在可行的 $p$，则说明无解。你最终报告的答案必须是单个最小整数 $p$（此整数值无需单位，也无需舍入说明）。", "solution": "题目要求推导在一维卷积中为达到特定输出长度 $m$ 而对对称零填充 $p$ 施加的约束，然后将此推导应用于一个具体的数值案例。\n\n首先，我们根据给定的条件来形式化卷积操作。\n一个长度为 $n$ 的输入在两侧各填充 $p$ 个零。得到的填充后输入的总长度为 $n_{padded} = n + 2p$。我们对这个填充后序列的元素使用从 0 开始的索引，范围从索引 $0$ 到 $n+2p-1$。\n\n一个宽度为 $k$ 的卷积核被应用于这个填充后的输入。通过将卷积核放置在不同位置来产生输出。步长 $s$ 决定了连续卷积核位置之间的步进大小。设输出元素由 $j$ 索引，其中 $j \\in \\{0, 1, 2, \\ldots, m-1\\}$，因为目标输出长度是 $m$。\n\n对于第 $j$ 个输出（其中 $j=0$ 对应第一个输出），卷积核的起始位置由 $j \\cdot s$ 给出。卷积核窗口随后覆盖填充后输入中从索引 $j \\cdot s$ 到 $j \\cdot s + k - 1$ 的范围。\n\n题目说明，只有当整个卷积核窗口位于填充后的输入内部时，才会产生输出。这对任何有效的起始位置 $i_{start}$ 施加了两个边界条件：\n1. 卷积核的起始位置 $i_{start}$ 必须在填充后输入的开头或之后：$i_{start} \\ge 0$。\n2. 卷积核的结束位置 $i_{start} + k - 1$ 必须在填充后输入的末尾或之前：$i_{start} + k - 1 \\le n+2p-1$。\n\n第二个条件可以重写为 $i_{start} \\le n+2p-k$。\n由于起始位置由 $j \\cdot s$ 给出，其中 $j \\ge 0$ 且 $s \\in \\mathbb{N}$，第一个条件 $j \\cdot s \\ge 0$ 总是满足的。因此，对第 $j$ 个输出的有效位置的唯一约束是：\n$$ j \\cdot s \\le n+2p-k $$\n\n为了使输出长度恰好为 $m$，必须同时满足两个条件：\n1. 必须为索引 $j=m-1$ 产生一个输出。这意味着第 $m$ 个输出的位置是有效的。\n   $$ (m-1)s \\le n+2p-k $$\n2. 必须*不*为索引 $j=m$ 产生输出。这意味着一个假设的第 $(m+1)$ 个输出的位置是无效的。\n   $$ m \\cdot s > n+2p-k $$\n\n将这两个不等式组合起来，得到一个复合不等式，它约束了可用于卷积的特征图的有效长度 $n+2p-k$：\n$$ (m-1)s \\le n+2p-k  ms $$\n\n为了找到对 $p$ 的约束，我们解这个关于 $p$ 的复合不等式。\n从左边的不等式：\n$(m-1)s \\le n+2p-k \\implies (m-1)s - n + k \\le 2p \\implies p \\ge \\frac{(m-1)s - n + k}{2}$。\n从右边的不等式：\n$n+2p-k  ms \\implies 2p  ms - n + k \\implies p  \\frac{ms - n + k}{2}$。\n\n因此，所有可行 $p$ 值的闭式表征由以下公式给出：\n$$ \\frac{(m-1)s - n + k}{2} \\le p  \\frac{ms - n + k}{2} $$\n由于 $p$ 必须是一个非负整数 ($p \\in \\mathbb{Z}_{\\ge 0}$)，只有当区间 $[\\frac{(m-1)s - n + k}{2}, \\frac{ms - n + k}{2})$ 包含至少一个非负整数时，才存在有效解。\n\n现在，我们讨论当 $s>1$ 时的可行性细节。\n让我们分析 $2p$ 的区间：\n$$ (m-1)s - n + k \\le 2p  ms - n + k $$\n令 $A = (m-1)s - n + k$。条件是 $A \\le 2p  A+s$。我们正在寻找区间 $[A, A+s)$ 内的一个非负偶数 $2p$。这个区间的长度是 $s$。\n如果 $s=1$，区间是 $[A, A+1)$，它只包含一个整数 $A$。要存在解，$A$ 必须是一个非负偶数。这是一个非常严格的条件。\n如果 $s>1$，区间 $[A, A+s)$ 的长度 $s \\ge 2$。对于任意整数 $A$，区间 $[A, A+s)$ 保证至少包含一个偶数。例如，如果 $A$ 是偶数，那么 $A$ 本身就是一个候选值。如果 $A$ 是奇数，那么 $A+1$ 是偶数，并且由于 $s \\ge 2$，$A+1  A+s$，所以 $A+1$ 在区间内。\n因此，对于 $s > 1$，总能保证存在一个满足边界条件的整数 $p$。唯一剩下的条件是 $p$ 必须是非负的。这意味着区间 $[A, A+s)$ 必须包含至少一个非负偶数。只要 $2p$ 区间的上界是正的，即 $A+s > 0$，也就是 $ms - n + k > 0$，这一点就能得到保证。\n\n$2p$ 为偶数的奇偶性已在公式中被隐式处理。项 $2p$ 必须是一个偶数。表达式 $(m-1)s - n + k$ 可以是偶数也可以是奇数。如果它是偶数，那么边界情况是可能发生的。\n\n边界情况，即最后一个与步长对齐的卷积核位置恰好落在填充后输入的末端，发生在不等式 $(m-1)s \\le n+2p-k$ 取等号时：\n$$ (m-1)s = n+2p-k $$\n解出 $p$，我们得到 $p = \\frac{(m-1)s - n + k}{2}$。为了使这种精确对齐成为可能，$p$ 的值必须是一个非负整数。这要求分子 $(m-1)s - n + k$ 是一个非负偶数。\n\n最后，我们将这些结果应用于具体配置：$n=37$，$k=7$，$s=3$ 和目标 $m=12$。\n我们需要找到满足所推导不等式的最小非负整数 $p$。\n将给定值代入 $p$ 的区间：\n下界：\n$$ p \\ge \\frac{(12-1) \\cdot 3 - 37 + 7}{2} = \\frac{11 \\cdot 3 - 37 + 7}{2} = \\frac{33 - 37 + 7}{2} = \\frac{-4 + 7}{2} = \\frac{3}{2} = 1.5 $$\n上界：\n$$ p  \\frac{12 \\cdot 3 - 37 + 7}{2} = \\frac{36 - 37 + 7}{2} = \\frac{-1 + 7}{2} = \\frac{6}{2} = 3 $$\n所以，对 $p$ 的条件是：\n$$ 1.5 \\le p  3 $$\n我们正在寻找满足此条件的最小整数 $p$。半开区间 $[1.5, 3)$ 中的整数是 $\\{2\\}$。唯一的整数解是 $p=2$。由于 $p=2$ 是一个非负整数，它是一个可行的填充值。由于它是有效范围内的唯一整数解，它也是最小的。\n\n因此，对于给定的配置，存在一个可行的非负整数 $p$，其最小值为 2。", "answer": "$$\\boxed{2}$$", "id": "3177669"}, {"introduction": "在深度学习框架中，看似等效的操作在实现上可能存在微妙但重要的差异。本练习将对比两种常见的下采样方法：直接使用步长为 $s \\gt 1$ 的卷积，与先进行步长为 $1$ 的卷积再进行子采样。通过动手实现和量化比较，你将发现“SAME”填充模式与这两种计算图的交互方式有何不同，以及为何它们可能导致不同的结果。[@problem_id:3177651]", "problem": "你需要比较两个计算图，它们在深度学习中实现了离散一维互相关，并量化由步幅和填充对齐引起的数值差异。比较的重点是“步幅为 $s$ 的卷积”和“步幅为 $1$ 的卷积后接步长为 $s$ 的下采样”之间的区别。\n\n使用的定义和规则：\n- 令 $x \\in \\mathbb{R}^n$ 为一维输入信号，$w \\in \\mathbb{R}^k$ 为一维核。在位置 $j$ 的离散互相关（深度学习框架中通常以“卷积”之名实现的操作）定义为\n  $$y[j] = \\sum_{t=0}^{k-1} x_{\\text{pad}}[j + t] \\, w[t],$$\n  其中 $x_{\\text{pad}}$ 表示 $x$ 在左侧和右侧进行零填充后的结果。\n- 填充模式：\n  1. \"valid\"：无零填充，即左、右填充长度均为 $0$。\n  2. \"same\"：选择两侧的零填充，使得以步幅 $s$ 提取的不同感受野的数量等于不小于输入长度与步幅之比的最小整数，并将总填充分为左填充和右填充，使得左填充为不大于总填充一半的最大整数。具体来说，当步幅等于 $1$ 时，“same”模式使用 $k-1$ 的总填充，并将其分为左填充和右填充，使得左填充等于不大于 $k-1$ 一半的最大整数。\n\n待比较的计算图：\n- 图 A（“步幅为 $s$ 的卷积”）：根据所选模式（如上所述）进行填充，然后通过在填充后的输入上以步长 $s$ 滑动窗口来计算互相关，取完全包含在填充后信号内的窗口。\n- 图 B（“先卷积后下采样”）：根据所选模式进行填充，但步幅等于 $1$，在所有完全包含的窗口上计算步幅为 $1$ 的互相关，然后从索引 $0$ 开始，每 $s$ 个元素取一个，对结果序列进行下采样。\n\n输入和核的合成（确定性）：\n- 对于给定的 $n$，定义 $x[i] = \\sin(0.2 \\, i) + 0.1 \\, i$，其中 $i = 0, 1, \\dots, n-1$。\n- 对于给定的 $k$，定义 $w[t] = \\frac{t+1}{k}$，其中 $t = 0, 1, \\dots, k-1$。\n\n度量：\n- 对于每个测试用例，从图 A 生成 $y_A$，从图 B 生成 $y_B$。\n- 令 $m = \\min\\{|y_A|, |y_B|\\}$。定义比较长度为 $m$。如果 $m = 0$，则按惯例将差值设为 $0$。\n- 计算最大绝对差\n  $$d_{\\max} = \\max_{0 \\le i  m} \\left| y_A[i] - y_B[i] \\right|.$$\n\n你的程序必须严格按照上述规则实现这两个图，并报告每个测试用例的 $d_{\\max}$。\n\n测试套件：\n- 用例 1（通用 \"valid\"）：$n = 16$, $k = 3$, $s = 2$, 模式 $\\text{valid}$。\n- 用例 2（通用 \"same\"，奇数核）：$n = 17$, $k = 5$, $s = 2$, 模式 $\\text{same}$。\n- 用例 3（边界情况，小输入大步幅）：$n = 3$, $k = 4$, $s = 3$, 模式 $\\text{same}$。\n- 用例 4（边缘情况，偶数核不对称）：$n = 10$, $k = 2$, $s = 3$, 模式 $\\text{same}$。\n- 用例 5（边缘情况，\"valid\" 模式下步幅大于核）：$n = 8$, $k = 3$, $s = 4$, 模式 $\\text{valid}$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，按上述用例的顺序排列，例如 $[d_1,d_2,d_3,d_4,d_5]$，其中每个 $d_i$ 是第 $i$ 个测试用例的 $d_{\\max}$ 的浮点数值。", "solution": "该问题要求对一维互相关的两种计算图实现进行定量比较，这两种实现在步幅和填充的处理上有所不同。问题的核心在于直接进行步幅卷积（图 A）与先进行单位步幅卷积再下采样（图 B）之间的区别。数值差异源于填充的计算方式，两种图的定义不同。\n\n基本原理是，当且仅当各自填充后的输入信号相同时，两个计算图（图 A 和图 B）才会产生相同的输出。这是因为核心操作——核与信号窗口的点积——在两个图中都应用于相同的起始位置（由步幅 $s$ 的倍数索引）。因此，任何差异都必定源于对原始信号 $x$ 应用的填充。\n\n设输入信号为 $x \\in \\mathbb{R}^n$，核为 $w \\in \\mathbb{R}^k$。步幅为 $s$。\n\n**图 A（“步幅为 $s$ 的卷积”）分析**\n\n图 A 首先确定达到目标输出维度所需的填充，然后执行步幅互相关。\n1.  **填充计算**：\n    - 对于 `mode='valid'`，两侧填充均为 $0$。总填充 $P_A = 0$。\n    - 对于 `mode='same'`，输出长度 $o_A$ 定义为 $\\lceil n/s \\rceil$。为达到此输出长度所需的总填充 $P_A$ 由标准公式 $P_A = \\max(0, (o_A-1)s + k - n)$ 给出。总填充被分为左填充 $p_{l,A} = \\lfloor P_A/2 \\rfloor$ 和右填充 $p_{r,A} = P_A - p_{l,A}$。\n2.  **填充后信号**：输入 $x$ 的左侧填充 $p_{l,A}$ 个零，右侧填充 $p_{r,A}$ 个零，形成 $x_{pad,A}$。\n3.  **互相关**：通过在 $x_{pad,A}$ 上以步长 $s$ 滑动核 $w$ 来计算输出 $y_A$。输出的第 $j$ 个元素（对于 $j=0, 1, \\dots, o_A-1$）是：\n    $$y_A[j] = \\sum_{t=0}^{k-1} x_{pad,A}[j \\cdot s + t] \\, w[t]$$\n\n**图 B（“先卷积后下采样”）分析**\n\n图 B 执行单位步幅卷积，然后对结果进行下采样。填充的计算方式如同步幅始终为 $1$。\n1.  **填充计算**：\n    - 对于 `mode='valid'`，两侧填充均为 $0$。总填充 $P_B = 0$。\n    - 对于 `mode='same'`，问题规定填充是为步幅 $s=1$ 计算的。这得出总填充为 $P_B = k-1$。这被分为左填充 $p_{l,B} = \\lfloor P_B/2 \\rfloor = \\lfloor (k-1)/2 \\rfloor$ 和右填充 $p_{r,B} = P_B - p_{l,B}$。\n2.  **填充后信号**：输入 $x$ 被填充以形成 $x_{pad,B}$。\n3.  **互相关与下采样**：首先，使用步幅 $1$ 计算一个中间信号 $y_{int,B}$：\n    $$y_{int,B}[j'] = \\sum_{t=0}^{k-1} x_{pad,B}[j' + t] \\, w[t]$$\n    最终输出 $y_B$ 是通过从索引 $0$ 开始，每 $s$ 个元素取一个，对 $y_{int,B}$ 进行下采样得到的：$y_B[j] = y_{int,B}[j \\cdot s]$。代入 $y_{int,B}$ 的表达式：\n    $$y_B[j] = \\sum_{t=0}^{k-1} x_{pad,B}[j \\cdot s + t] \\, w[t]$$\n\n**等价条件与差异来源**\n\n比较 $y_A[j]$ 和 $y_B[j]$ 的最终表达式，很明显，$y_A$ 和 $y_B$ 相等当且仅当它们底层的填充信号 $x_{pad,A}$ 和 $x_{pad,B}$ 相等（假设它们的长度兼容，事实也如此）。这等价于总填充相等（$P_A = P_B$）且以相同方式分割（基于左侧 $\\lfloor P/2 \\rfloor$ 的规则，它们确实是这样分割的）的条件。\n\n-   **对于 `mode='valid'`**：$P_A = 0$ 且 $P_B = 0$。两个图总是等价的，差异 $d_{\\max}$ 必须为 $0$。\n\n-   **对于 `mode='same'`**：我们必须比较 $P_A = \\max(0, (\\lceil n/s \\rceil-1)s+k-n)$ 与 $P_B = k-1$。两者相等，如果 $(\\lceil n/s \\rceil-1)s+k-n = k-1$（假设该项为非负）。这可以简化为：\n    $$(\\lceil n/s \\rceil-1)s = n-1$$\n    我们来分析这个条件。设 $n = q \\cdot s + r$，其中 $q$ 是商，$r$ 是余数（$0 \\le r  s$）。\n    - 如果 $r=0$（即 $n$ 是 $s$ 的倍数），那么 $\\lceil n/s \\rceil=q$。条件变为 $(q-1)s = n-1 \\implies qs-s=n-1 \\implies n-s=n-1 \\implies s=1$。因此，对于 $s > 1$，该条件不成立。\n    - 如果 $r>0$，那么 $\\lceil n/s \\rceil=q+1$。条件变为 $(q+1-1)s = n-1 \\implies qs=n-1 \\implies qs = (qs+r)-1 \\implies r-1=0 \\implies r=1$。\n    因此，对于 `mode='same'` 和 $s>1$，当且仅当 $n \\pmod s = 1$ 时，两个图是等价的。如果 $n \\pmod s \\neq 1$，填充后的信号将不同，导致 $y_A \\neq y_B$ 和一个潜在的非零差异 $d_{\\max}$。\n\n**应用于测试用例**\n\n-   **用例 1**：$n=16, k=3, s=2$, `mode=valid`。$P_A=P_B=0$。预期 $d_{\\max}=0$。\n-   **用例 2**：$n=17, k=5, s=2$, `mode=same`。这里，$17 \\pmod 2 = 1$。等价条件成立。预期 $d_{\\max}=0$。\n-   **用例 3**：$n=3, k=4, s=3$, `mode=same`。这里，$3 \\pmod 3 = 0$。由于 $s > 1$，等价条件不成立。填充将会不同：\n    - $P_A = \\max(0, (\\lceil 3/3 \\rceil - 1)3 + 4 - 3) = \\max(0, 0+1) = 1$。\n    - $P_B = k-1 = 3$。\n    预期 $d_{\\max}$ 非零。\n-   **用例 4**：$n=10, k=2, s=3$, `mode=same`。这里，$10 \\pmod 3 = 1$。等价条件成立。预期 $d_{\\max}=0$。\n-   **用例 5**：$n=8, k=3, s=4$, `mode=valid`。$P_A=P_B=0$。预期 $d_{\\max}=0$。\n\n实现将严格按照这些规则构建两个图，并计算指定的最大绝对差。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the numerical difference between two implementations of 1D cross-correlation\n    for a given set of test cases.\n\n    The two implementations are:\n    - Graph A: Direct strided convolution.\n    - Graph B: Unit-stride convolution followed by subsampling.\n\n    The difference arises from how padding is calculated in 'same' mode, which is\n    stride-dependent in Graph A but stride-agnostic (fixed to s=1) in Graph B.\n    \"\"\"\n\n    test_cases = [\n        # (n, k, s, mode)\n        (16, 3, 2, 'valid'),\n        (17, 5, 2, 'same'),\n        (3, 4, 3, 'same'),\n        (10, 2, 3, 'same'),\n        (8, 3, 4, 'valid'),\n    ]\n\n    results = []\n\n    def cross_correlate(x_padded: np.ndarray, w: np.ndarray, stride: int) - np.ndarray:\n        \"\"\"\n        Computes 1D cross-correlation with a given stride on a pre-padded signal.\n        \"\"\"\n        k = len(w)\n        n_padded = len(x_padded)\n        output_len = math.floor((n_padded - k) / stride) + 1\n        if output_len = 0:\n            return np.array([])\n        \n        y = np.zeros(output_len)\n        for j in range(output_len):\n            start_index = j * stride\n            window = x_padded[start_index : start_index + k]\n            y[j] = np.dot(window, w)\n        return y\n\n    for n, k, s, mode in test_cases:\n        # Step 1: Synthesize input signal x and kernel w\n        i_vals = np.arange(n)\n        x = np.sin(0.2 * i_vals) + 0.1 * i_vals\n\n        t_vals = np.arange(k)\n        w = (t_vals + 1) / k\n\n        # Step 2: Implement and compute y_A for Graph A\n        if mode == 'valid':\n            p_left_A = 0\n            p_right_A = 0\n        elif mode == 'same':\n            out_len_A = math.ceil(n / s)\n            total_pad_A = max(0, (out_len_A - 1) * s + k - n)\n            p_left_A = math.floor(total_pad_A / 2)\n            p_right_A = total_pad_A - p_left_A\n        else:\n            raise ValueError(\"Invalid mode\")\n\n        x_padded_A = np.pad(x, (p_left_A, p_right_A), 'constant', constant_values=0)\n        y_A = cross_correlate(x_padded_A, w, s)\n\n        # Step 3: Implement and compute y_B for Graph B\n        if mode == 'valid':\n            p_left_B = 0\n            p_right_B = 0\n        elif mode == 'same':\n            # Padding is calculated as if stride were 1\n            total_pad_B = k - 1\n            p_left_B = math.floor(total_pad_B / 2)\n            p_right_B = total_pad_B - p_left_B\n        else:\n            raise ValueError(\"Invalid mode\")\n\n        x_padded_B = np.pad(x, (p_left_B, p_right_B), 'constant', constant_values=0)\n        y_intermediate_B = cross_correlate(x_padded_B, w, 1)\n        y_B = y_intermediate_B[::s]\n\n        # Step 4: Compute the maximum absolute difference d_max\n        m = min(len(y_A), len(y_B))\n        if m == 0:\n            d_max = 0.0\n        else:\n            diff = np.abs(y_A[:m] - y_B[:m])\n            d_max = np.max(diff)\n\n        results.append(d_max)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3177651"}, {"introduction": "底层的操作细节如何影响高层的网络架构？本练习将步长和填充的概念与一个经典的架构问题联系起来：U-Net中的特征不对齐。通过建立一个严谨的坐标映射系统，你将可以追踪特征在网络中的“世界坐标”，并量化由卷积核尺寸和填充策略共同导致的像素级偏移。这对于理解并修复U-Net等需要精确特征融合的架构至关重要。[@problem_id:3180119]", "problem": "考虑一个二维（2D）U形卷积神经网络（U-Net），该网络执行一个下采样阶段和一个上采样阶段。该网络使用二维离散卷积的互相关形式，其中对于一个输入特征图 $X$ 和一个大小为 $k_{x} \\times k_{y}$ 的卷积核 $W$，输出特征图 $Y$（沿每个空间轴独立计算）由下式给出\n$$\nY[i,j] \\;=\\; \\sum_{u=0}^{k_{x}-1} \\sum_{v=0}^{k_{y}-1} W[u,v] \\, X[s_{x} i - p_{x} + u, \\, s_{y} j - p_{y} + v].\n$$\n假设使用以下约定和参数：\n- 标准卷积沿每个轴的步幅为 $s_{x} = s_{y} = 1$。\n- 填充（padding）实现常用的“相同（same）”规则，即 $p_{x} = \\left\\lfloor \\frac{k_{x}}{2} \\right\\rfloor$ 和 $p_{y} = \\left\\lfloor \\frac{k_{y}}{2} \\right\\rfloor$。\n- 卷积核离散中心沿每个轴的索引为 $c_{x} = \\left\\lfloor \\frac{k_{x}-1}{2} \\right\\rfloor$ 和 $c_{y} = \\left\\lfloor \\frac{k_{y}-1}{2} \\right\\rfloor$。\n- 二维最大池化使用大小为 $2 \\times 2$ 的窗口、步幅为 $2$、并且沿两个轴的填充为零。将池化输出的代表性位置视为由上述 $(c_{x}, c_{y})$ 确定的窗口中心。\n- 二维转置卷积（分数步幅卷积）的核大小为 $2 \\times 2$，沿每个轴的步幅为 $2$，其实现遵循标准的深度学习定义，等效于在输入之间插入零，然后应用给定核的互相关；其填充为 $p_{x} = p_{y} = 0$，中心索引为 $c_{x} = c_{y} = \\left\\lfloor \\frac{2-1}{2} \\right\\rfloor$。\n\n定义一个坐标映射，将任何中间特征图的索引 $(i,j)$ 映射回输入图像的坐标系，该映射由一对参数 $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$ 表示，其中 $S_{x}$ 和 $S_{y}$ 是累积步幅，$(\\Delta_{x}, \\Delta_{y})$ 是以输入像素为单位的累积偏移量。对于一个应用于具有映射 $(S_{x}^{\\mathrm{in}}, S_{y}^{\\mathrm{in}}, \\Delta_{x}^{\\mathrm{in}}, \\Delta_{y}^{\\mathrm{in}})$ 的输入的标准卷积层或池化层，其步幅为 $(s_{x}, s_{y})$，填充为 $(p_{x}, p_{y})$，中心索引为 $(c_{x}, c_{y})$，输出映射为\n$$\nS_{x}^{\\mathrm{out}} \\;=\\; S_{x}^{\\mathrm{in}} \\, s_{x}, \\quad S_{y}^{\\mathrm{out}} \\;=\\; S_{y}^{\\mathrm{in}} \\, s_{y}, \\quad\n\\Delta_{x}^{\\mathrm{out}} \\;=\\; \\Delta_{x}^{\\mathrm{in}} + S_{x}^{\\mathrm{in}} (c_{x} - p_{x}), \\quad\n\\Delta_{y}^{\\mathrm{out}} \\;=\\; \\Delta_{y}^{\\mathrm{in}} + S_{y}^{\\mathrm{in}} (c_{y} - p_{y}).\n$$\n对于一个应用于具有映射 $(S_{x}^{\\mathrm{in}}, S_{y}^{\\mathrm{in}}, \\Delta_{x}^{\\mathrm{in}}, \\Delta_{y}^{\\mathrm{in}})$ 的输入的转置卷积，其步幅为 $(s_{x}, s_{y})$，填充为 $(p_{x}, p_{y})$，中心索引为 $(c_{x}, c_{y})$，输出映射为\n$$\nS_{x}^{\\mathrm{out}} \\;=\\; \\frac{S_{x}^{\\mathrm{in}}}{s_{x}}, \\quad S_{y}^{\\mathrm{out}} \\;=\\; \\frac{S_{y}^{\\mathrm{in}}}{s_{y}}, \\quad\n\\Delta_{x}^{\\mathrm{out}} \\;=\\; \\Delta_{x}^{\\mathrm{in}} + \\frac{S_{x}^{\\mathrm{in}}}{s_{x}} (p_{x} - c_{x}), \\quad\n\\Delta_{y}^{\\mathrm{out}} \\;=\\; \\Delta_{y}^{\\mathrm{in}} + \\frac{S_{y}^{\\mathrm{in}}}{s_{y}} (p_{y} - c_{y}).\n$$\n\n考虑以下最小化的U-Net路径：\n1. 编码器阶段：一个 $3 \\times 3$ 的标准卷积，使用“相同（same）”填充，然后是一个 $2 \\times 2$ 的最大池化，步幅为 $2$，填充为零。\n2. 解码器阶段：一个 $2 \\times 2$ 的转置卷积，步幅为 $2$，填充为零，然后是一个标准卷积，其核大小为 $3 \\times 3$（奇数）或 $2 \\times 2$（偶数），使用“相同（same）”填充。\n\n一个跳跃连接（skip connection）将第一个 $3 \\times 3$ 卷积后的编码器特征图与最后一个标准卷积后的解码器特征图进行拼接。使用上述坐标映射规则，推导编码器特征和解码器特征的累积 $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$，并用它们计算当解码器的最后一个标准卷积使用 $2 \\times 2$ 卷积核和“相同（same）”填充时，这两个特征在跳跃连接处的空间未对齐向量。然后，将此未对齐向量的欧几里得幅度（以跳跃连接分辨率下的像素为单位）报告为单个实数值。如果不存在未对齐，则幅度为零。请仅提供该幅度作为您的最终答案。", "solution": "用户希望确定一个简化U-Net架构中跳跃连接处特征之间的空间未对齐。问题为追踪通过各种神经网络层的坐标系提供了一个正式定义。我将通过计算跳跃连接中涉及的每个特征的坐标映射，然后找出它们相应空间位置之间差异的幅度来解决这个问题。\n\n特征图的坐标映射由一个元组 $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$ 描述，其中 $(S_x, S_y)$ 是累积步幅，$(\\Delta_x, \\Delta_y)$ 是相对于输入图像坐标系的累积偏移量。特征图中索引为 $(i,j)$ 的点对应于输入图像坐标系中的点 $(i \\cdot S_x + \\Delta_x, j \\cdot S_y + \\Delta_y)$。\n\n输入图像本身具有单位映射，我们可以将其表示为第 $0$ 层。\n初始映射（第 $0$ 层）：$(S_x^{(0)}, S_y^{(0)}, \\Delta_x^{(0)}, \\Delta_y^{(0)}) = (1, 1, 0, 0)$。\n\n问题描述了一个跳跃连接，它将来自编码器的特征图与来自解码器的特征图进行拼接。我们必须找出这两个特征图各自的坐标映射。\n\n**1. 用于跳跃连接的编码器特征图（E1层）**\n\n该特征图是编码器中第一个操作的输出：一个 $3 \\times 3$ 的标准卷积，使用“相同（same）”填充。\n- 输入映射：$(S_x^{(0)}, S_y^{(0)}, \\Delta_x^{(0)}, \\Delta_y^{(0)}) = (1, 1, 0, 0)$。\n- 操作：标准卷积。\n- 参数：\n  - 核大小：$k_x = 3, k_y = 3$。\n  - 步幅：$s_x = 1, s_y = 1$。\n  - 填充（“same”）：$p_x = \\lfloor \\frac{3}{2} \\rfloor = 1$, $p_y = \\lfloor \\frac{3}{2} \\rfloor = 1$。\n  - 核中心：$c_x = \\lfloor \\frac{3-1}{2} \\rfloor = 1$, $c_y = \\lfloor \\frac{3-1}{2} \\rfloor = 1$。\n\n我们应用给定的标准卷积更新规则：\n$S_{x}^{\\mathrm{out}} = S_{x}^{\\mathrm{in}} \\, s_{x}$\n$\\Delta_{x}^{\\mathrm{out}} = \\Delta_{x}^{\\mathrm{in}} + S_{x}^{\\mathrm{in}} (c_{x} - p_{x})$\n（$y$ 维度的计算与此类似）。\n\n- E1的累积步幅：\n$S_x^{(E1)} = S_x^{(0)} \\cdot s_x = 1 \\cdot 1 = 1$\n$S_y^{(E1)} = S_y^{(0)} \\cdot s_y = 1 \\cdot 1 = 1$\n\n- E1的累积偏移量：\n$\\Delta_x^{(E1)} = \\Delta_x^{(0)} + S_x^{(0)} (c_x - p_x) = 0 + 1(1 - 1) = 0$\n$\\Delta_y^{(E1)} = \\Delta_y^{(0)} + S_y^{(0)} (c_y - p_y) = 0 + 1(1 - 1) = 0$\n\n在跳跃连接处，编码器特征的坐标映射为 $(S_x, S_y, \\Delta_x, \\Delta_y)_{E1} = (1, 1, 0, 0)$。\n\n**2. 用于跳跃连接的解码器特征图（D2层）**\n\n该特征图是数据通过整个指定的U-Net路径后的结果。我们逐步追踪坐标映射的变换。\n\n**步骤 2a：最大池化（E2层）**\n此操作在第一次卷积之后。其输入是特征图 E1。\n- 输入映射：$(S_x^{(E1)}, S_y^{(E1)}, \\Delta_x^{(E1)}, \\Delta_y^{(E1)}) = (1, 1, 0, 0)$。\n- 操作：最大池化，为进行坐标映射，将其视为标准卷积。\n- 参数：\n  - 窗口大小：$k_x = 2, k_y = 2$。\n  - 步幅：$s_x = 2, s_y = 2$。\n  - 填充：$p_x = 0, p_y = 0$。\n  - 窗口中心：$c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$。\n\n- E2的累积步幅：\n$S_x^{(E2)} = S_x^{(E1)} \\cdot s_x = 1 \\cdot 2 = 2$\n$S_y^{(E2)} = S_y^{(E1)} \\cdot s_y = 1 \\cdot 2 = 2$\n\n- E2的累积偏移量：\n$\\Delta_x^{(E2)} = \\Delta_x^{(E1)} + S_x^{(E1)} (c_x - p_x) = 0 + 1(0 - 0) = 0$\n$\\Delta_y^{(E2)} = \\Delta_y^{(E1)} + S_y^{(E1)} (c_y - p_y) = 0 + 1(0 - 0) = 0$\n\n瓶颈特征图（E2）的坐标映射为 $(2, 2, 0, 0)$。\n\n**步骤 2b：转置卷积（D1层）**\n这是解码器中的第一个操作（上采样路径）。其输入是瓶颈特征图 E2。\n- 输入映射：$(S_x^{(E2)}, S_y^{(E2)}, \\Delta_x^{(E2)}, \\Delta_y^{(E2)}) = (2, 2, 0, 0)$。\n- 操作：转置卷积。\n- 参数：\n  - 核大小：$k_x = 2, k_y = 2$。\n  - 步幅：$s_x = 2, s_y = 2$。\n  - 填充：$p_x = 0, p_y = 0$。\n  - 核中心：$c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$。\n\n我们应用给定的转置卷积更新规则：\n$S_{x}^{\\mathrm{out}} = S_{x}^{\\mathrm{in}} / s_{x}$\n$\\Delta_{x}^{\\mathrm{out}} = \\Delta_{x}^{\\mathrm{in}} + \\frac{S_{x}^{\\mathrm{in}}}{s_{x}} (p_{x} - c_{x})$\n\n- D1的累积步幅：\n$S_x^{(D1)} = S_x^{(E2)} / s_x = 2 / 2 = 1$\n$S_y^{(D1)} = S_y^{(E2)} / s_y = 2 / 2 = 1$\n\n- D1的累积偏移量：\n$\\Delta_x^{(D1)} = \\Delta_x^{(E2)} + \\frac{S_x^{(E2)}}{s_x} (p_x - c_x) = 0 + \\frac{2}{2}(0 - 0) = 0$\n$\\Delta_y^{(D1)} = \\Delta_y^{(E2)} + \\frac{S_y^{(E2)}}{s_y} (p_y - c_y) = 0 + \\frac{2}{2}(0 - 0) = 0$\n\n上采样后（D1）的坐标映射为 $(1, 1, 0, 0)$。\n\n**步骤 2c：最终标准卷积（D2层）**\n这是跳跃连接前的最后一个操作。其输入是上采样后的特征图 D1。问题指定使用 $2 \\times 2$ 的卷积核。\n- 输入映射：$(S_x^{(D1)}, S_y^{(D1)}, \\Delta_x^{(D1)}, \\Delta_y^{(D1)}) = (1, 1, 0, 0)$。\n- 操作：标准卷积。\n- 参数：\n  - 核大小：$k_x = 2, k_y = 2$。\n  - 步幅：$s_x = 1, s_y = 1$。\n  - 填充（“same”）：$p_x = \\lfloor \\frac{2}{2} \\rfloor = 1$, $p_y = \\lfloor \\frac{2}{2} \\rfloor = 1$。\n  - 核中心：$c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$。\n\n- D2的累积步幅：\n$S_x^{(D2)} = S_x^{(D1)} \\cdot s_x = 1 \\cdot 1 = 1$\n$S_y^{(D2)} = S_y^{(D1)} \\cdot s_y = 1 \\cdot 1 = 1$\n\n- D2的累积偏移量：\n$\\Delta_x^{(D2)} = \\Delta_x^{(D1)} + S_x^{(D1)} (c_x - p_x) = 0 + 1(0 - 1) = -1$\n$\\Delta_y^{(D2)} = \\Delta_y^{(D1)} + S_y^{(D1)} (c_y - p_y) = 0 + 1(0 - 1) = -1$\n\n在跳跃连接处，解码器特征（D2）的坐标映射为 $(S_x, S_y, \\Delta_x, \\Delta_y)_{D2} = (1, 1, -1, -1)$。\n\n**3. 计算未对齐**\n\n我们现在拥有在跳跃连接处被拼接的两个特征的坐标映射：\n- 编码器特征 E1：$(S_x, S_y, \\Delta_x, \\Delta_y)_{E1} = (1, 1, 0, 0)$。\n- 解码器特征 D2：$(S_x, S_y, \\Delta_x, \\Delta_y)_{D2} = (1, 1, -1, -1)$。\n\n让我们考虑跳跃连接处特征图空间网格中索引为 $(i,j)$ 的一个点。\n- 编码器特征在输入图像中的对应位置是：\n$(x_E, y_E) = (i \\cdot S_{x,E1} + \\Delta_{x,E1}, j \\cdot S_{y,E1} + \\Delta_{y,E1}) = (i \\cdot 1 + 0, j \\cdot 1 + 0) = (i, j)$。\n- 解码器特征在输入图像中的对应位置是：\n$(x_D, y_D) = (i \\cdot S_{x,D2} + \\Delta_{x,D2}, j \\cdot S_{y,D2} + \\Delta_{y,D2}) = (i \\cdot 1 + (-1), j \\cdot 1 + (-1)) = (i-1, j-1)$。\n\n在输入图像坐标系中的空间未对齐向量是这两个位置之间的差：\n$\\vec{v} = (x_E - x_D, y_E - y_D) = (i - (i-1), j - (j-1)) = (1, 1)$。\n\n问题要求计算该向量的幅度，“以跳跃连接分辨率下的像素为单位”进行测量。在跳跃连接处，两个特征的累积步幅均为 $S_x=1, S_y=1$。这意味着特征图中的一个像素步长对应于原始输入图像中的一个像素步长。因此，输入像素单位下的未对齐向量 $(1, 1)$ 也是特征图像素单位下的未对齐向量。\n\n该未对齐向量的欧几里得幅度为：\n$|\\vec{v}| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$。\n\n这种未对齐是由使用偶数尺寸（$2 \\times 2$）卷积核与“相同（same）”填充的不对称性引起的。选择核中心 $c=0$ 并结合填充 $p=1$ 会产生位置偏移。", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3180119"}]}