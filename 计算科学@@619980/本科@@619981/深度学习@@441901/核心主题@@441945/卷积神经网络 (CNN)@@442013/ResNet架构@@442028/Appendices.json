{"hands_on_practices": [{"introduction": "这个动手实践旨在通过编码帮助你建立关于残差学习的直观理解。我们将创建一个合成数据集，模拟一个残差模块能够发挥独特作用的场景，而非直接深入复杂的理论。通过这个练习 [@problem_id:3169949]，你将具体地看到，残差块之所以强大，是因为它让网络能够轻易地学习对一个“基本够用”的恒等映射进行微小修正，这比从零开始学习整个复杂变换要简单得多。", "problem": "考虑一个为模拟残差网络（ResNet）中单个残差块而构建的二元分类任务，其中残差块计算形式为 $H(\\mathbf{x}) = I(\\mathbf{x}) + F(\\mathbf{x})$ 的映射，恒等映射 $I(\\mathbf{x})$ 是一个已有的输入表示，而 $F(\\mathbf{x})$ 是一个学习到的修正。使用以下基本设定：分类决策是通过将标量 logit 在 $0$ 处进行阈值化来做出的，分类准确率是所有样本中正确预测的比例。真实决策边界由一个带加性噪声的线性函数的符号定义。您的任务是形式化描述输入接近决策边界、使得残差修正起决定性作用的情况，并量化在恒等基线 $I$ 的基础上添加残差 $F$ 所带来的准确率提升。\n\n数据集和决策定义：\n- 令 $\\mathbf{x} = (x_1, x_2) \\in \\mathbb{R}^2$ 为输入。对于一个大小为 $N$ 的数据集，独立地从 $x_1 \\sim \\mathrm{Uniform}(-\\delta, \\delta)$ 和 $x_2 \\sim \\mathrm{Uniform}(-1, 1)$ 中采样，其中 $\\delta > 0$ 很小，以使输入接近由 $x_1 = 0$ 定义的决策边界。\n- 令 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 为独立的加性噪声。\n- 真实标签定义为 $y = \\mathbb{I}\\{x_1 + \\alpha\\, x_2 + \\varepsilon \\ge 0\\}$，其中 $\\alpha \\in \\mathbb{R}$ 控制与 $x_2$ 相关的残差分量的强度，$\\mathbb{I}\\{\\cdot\\}$ 表示指示函数，当条件为真时返回 $1$，否则返回 $0$。\n- 恒等基线分类器使用 logit $I(\\mathbf{x}) = x_1$ 并预测 $\\hat{y}_I = \\mathbb{I}\\{I(\\mathbf{x}) \\ge 0\\}$。\n- 残差化分类器使用 logit $H(\\mathbf{x}) = I(\\mathbf{x}) + F(\\mathbf{x})$，其中 $F(\\mathbf{x}) = \\alpha\\, x_2$，并预测 $\\hat{y}_R = \\mathbb{I}\\{H(\\mathbf{x}) \\ge 0\\}$。\n\n对于每个数据集，计算恒等基线准确率\n$$\\mathrm{Acc}_I = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\{\\hat{y}_I^{(i)} = y^{(i)}\\},$$\n残差化准确率\n$$\\mathrm{Acc}_R = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\{\\hat{y}_R^{(i)} = y^{(i)}\\},$$\n以及准确率提升\n$$\\Delta \\mathrm{Acc} = \\mathrm{Acc}_R - \\mathrm{Acc}_I.$$\n\n您的程序必须使用带有指定种子的伪随机数生成器确定性地生成数据集，并为以下每个测试用例产生准确率提升 $\\Delta \\mathrm{Acc}$：\n- 测试用例 $1$：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 0.05, 0.5, 0.0, 42)$。\n- 测试用例 $2$：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 0.05, 0.0, 0.0, 123)$。\n- 测试用例 $3$：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 10^{-6}, 0.5, 0.0, 7)$。\n- 测试用例 $4$：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 0.5, 0.5, 0.0, 99)$。\n- 测试用例 $5$：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 0.05, 0.5, 0.3, 2023)$。\n\n覆盖性设计：\n- 第一个用例是一个典型的近边界场景，其中残差修正应该有所帮助。\n- 第二个用例设置 $\\alpha = 0$，因此残差不添加任何信息，作为一个基线边界情况。\n- 第三个用例通过设置 $\\delta = 10^{-6}$ 使输入极度接近决策边界。\n- 第四个用例通过增加 $\\delta$ 来放宽近边界条件，测试对残差修正依赖性的降低。\n- 第五个用例引入了 $\\sigma > 0$ 的噪声，测试当标签被破坏时的鲁棒性。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目是相应测试用例计算出的 $\\Delta \\mathrm{Acc}$，四舍五入到 $6$ 位小数（例如，$[0.123456,0.000000,0.250000,0.050000,0.010000]$）。不应打印任何其他文本。", "solution": "该问题要求我们在一个简化的二元分类设置中，量化由单个残差连接带来的性能提升。该场景旨在模拟一个 ResNet 块，其中恒等映射 $I(\\mathbf{x})$ 通过残差函数 $F(\\mathbf{x})$ 得到增强，形成新的映射 $H(\\mathbf{x}) = I(\\mathbf{x}) + F(\\mathbf{x})$。我们将模拟此过程，计算基线（仅恒等）模型和残差化模型的分类准确率，然后确定准确率提升 $\\Delta \\mathrm{Acc}$。整个过程将通过使用带种子的伪随机数生成器，对几个测试用例进行确定性执行。\n\n对于每个由参数 $(N, \\delta, \\alpha, \\sigma, \\text{seed})$ 定义的测试用例，解决方案按以下步骤进行。\n\n1.  **数据集生成**：首先，我们生成一个包含 $N$ 个样本的数据集。每个样本包含一个输入向量 $\\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)})$ 和一个对应的真实标签 $y^{(i)}$。\n    -   使用指定的 `seed` 初始化伪随机数生成器，以确保结果的确定性和可复现性。\n    -   输入分量从独立的均匀分布中采样：$x_1^{(i)} \\sim \\mathrm{Uniform}(-\\delta, \\delta)$ 和 $x_2^{(i)} \\sim \\mathrm{Uniform}(-1, 1)$，对于 $i=1, \\dots, N$。参数 $\\delta$ 控制输入与决策边界 $x_1 = 0$ 的接近程度。\n    -   从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中采样一个独立的加性噪声项 $\\varepsilon^{(i)}$。参数 $\\sigma$ 是噪声的标准差。\n    -   真实标签 $y^{(i)}$ 由输入的带噪线性组合的符号确定。它使用指示函数 $\\mathbb{I}\\{\\cdot\\}$ 定义为：\n        $$y^{(i)} = \\mathbb{I}\\{x_1^{(i)} + \\alpha\\, x_2^{(i)} + \\varepsilon^{(i)} \\ge 0\\}$$\n        如果条件为真，此函数返回 $1$，否则返回 $0$。\n\n2.  **模型预测**：我们定义两个分类器，并为数据集中的每个样本生成它们的预测。\n    -   **恒等基线分类器**仅使用输入的 $x_1$ 分量。其 logit 为 $I(\\mathbf{x}) = x_1$，其预测 $\\hat{y}_I$ 基于此 logit 的符号：\n        $$\\hat{y}_I^{(i)} = \\mathbb{I}\\{I(\\mathbf{x}^{(i)}) \\ge 0\\} = \\mathbb{I}\\{x_1^{(i)} \\ge 0\\}$$\n    -   **残差化分类器**将来自 $x_2$ 的贡献作为残差修正项。其 logit 为 $H(\\mathbf{x}) = I(\\mathbf{x}) + F(\\mathbf{x})$，其中恒等映射为 $I(\\mathbf{x}) = x_1$，残差函数为 $F(\\mathbf{x}) = \\alpha\\, x_2$。预测 $\\hat{y}_R$ 为：\n        $$\\hat{y}_R^{(i)} = \\mathbb{I}\\{H(\\mathbf{x}^{(i)}) \\ge 0\\} = \\mathbb{I}\\{x_1^{(i)} + \\alpha\\, x_2^{(i)} \\ge 0\\}$$\n        该模型有效地尝试学习真实的决策边界，但无法访问噪声项 $\\varepsilon$。\n\n3.  **性能评估**：我们计算每个分类器的准确率以及由此产生的提升。\n    -   分类器的准确率是正确分类样本的比例。恒等基线准确率 $\\mathrm{Acc}_I$ 计算如下：\n        $$\\mathrm{Acc}_I = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\{\\hat{y}_I^{(i)} = y^{(i)}\\}$$\n    -   类似地，残差化准确率 $\\mathrm{Acc}_R$ 为：\n        $$\\mathrm{Acc}_R = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\{\\hat{y}_R^{(i)} = y^{(i)}\\}$$\n    -   我们最终感兴趣的指标是准确率提升 $\\Delta \\mathrm{Acc}$，定义为两个准确率之差：\n        $$\\Delta \\mathrm{Acc} = \\mathrm{Acc}_R - \\mathrm{Acc}_I$$\n        $\\Delta \\mathrm{Acc}$ 的正值表示在给定条件下，残差连接为分类性能提供了切实的益处。\n\n在 $\\sigma = 0$ 的无噪声情况下，真实标签为 $y^{(i)} = \\mathbb{I}\\{x_1^{(i)} + \\alpha x_2^{(i)} \\ge 0\\}$。残差化分类器的预测 $\\hat{y}_R^{(i)}$ 与真实标签 $y^{(i)}$ 完全相同。因此，在这种情况下，$\\mathrm{Acc}_R$ 将精确为 $1$。模拟中与此的任何偏差都将归因于浮点精度，但预计可以忽略不计。准确率提升则变为 $\\Delta \\mathrm{Acc} = 1 - \\mathrm{Acc}_I$。\n\n提供的测试用例探讨了模型在不同机制下的行为：\n-   **用例 1 ($\\alpha=0.5$, $\\delta=0.05$, $\\sigma=0.0$):** 一个标准场景，其中残差项包含有用信息，输入接近恒等边界，并且没有标签噪声。我们预计会有一个显著的正 $\\Delta \\mathrm{Acc}$。\n-   **用例 2 ($\\alpha=0.0$, $\\delta=0.05$, $\\sigma=0.0$):** 在此，残差项 $F(\\mathbf{x}) = 0 \\cdot x_2 = 0$。两个分类器变得相同，所以我们必须有 $\\mathrm{Acc}_I = \\mathrm{Acc}_R$，导致 $\\Delta \\mathrm{Acc} = 0$。\n-   **用例 3 ($\\alpha=0.5$, $\\delta=10^{-6}$, $\\sigma=0.0$):** 由于 $\\delta$ 极小，输入非常紧密地聚集在 $x_1=0$ 边界周围。恒等分类器的性能预计将接近随机猜测（准确率 $0.5$），使得残差项至关重要。$\\Delta \\mathrm{Acc}$ 应该接近 $0.5$。\n-   **用例 4 ($\\alpha=0.5$, $\\delta=0.5$, $\\sigma=0.0$):** 在 $\\delta$ 较大的情况下，更大部分的输入远离 $x_1=0$ 边界。对于这些点，$x_1$ 的符号更能预测 $x_1 + \\alpha x_2$ 的符号，从而提高 $\\mathrm{Acc}_I$ 并减少*相对*提升 $\\Delta \\mathrm{Acc}$。\n-   **用例 5 ($\\alpha=0.5$, $\\delta=0.05$, $\\sigma=0.3$):** 引入显著的标签噪声 ($\\sigma > 0$) 使问题在根本上是随机的。真实标签的符号可能相对于无噪声边界发生翻转。两种准确率都预计会下降，但残差分类器由于与底层数据结构更好地对齐，应能保持优势，从而产生一个正的 $\\Delta \\mathrm{Acc}$。\n\n以下程序实现了计算每个测试用例的 $\\Delta \\mathrm{Acc}$ 的整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the accuracy improvement from a residual connection in a simplified\n    binary classification task for a set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, delta, alpha, sigma, seed)\n    test_cases = [\n        (5000, 0.05, 0.5, 0.0, 42),\n        (5000, 0.05, 0.0, 0.0, 123),\n        (5000, 1e-6, 0.5, 0.0, 7),\n        (5000, 0.5, 0.5, 0.0, 99),\n        (5000, 0.05, 0.5, 0.3, 2023),\n    ]\n\n    results = []\n    for N, delta, alpha, sigma, seed in test_cases:\n        # 1. Dataset Generation\n        # Initialize a pseudorandom number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Sample x1 and x2 from their respective uniform distributions.\n        x1 = rng.uniform(-delta, delta, size=N)\n        x2 = rng.uniform(-1, 1, size=N)\n\n        # Sample additive noise from a normal distribution. scale=sigma (std dev).\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=N)\n\n        # Determine ground-truth labels.\n        # y = I{x1 + alpha*x2 + epsilon >= 0}\n        ground_truth_logit = x1 + alpha * x2 + epsilon\n        y_true = (ground_truth_logit >= 0).astype(int)\n\n        # 2. Model Predictions\n        # Identity-baseline classifier prediction.\n        # y_hat_I = I{x1 >= 0}\n        y_pred_I = (x1 >= 0).astype(int)\n\n        # Residualized classifier prediction.\n        # y_hat_R = I{x1 + alpha*x2 >= 0}\n        y_pred_R = (x1 + alpha * x2 >= 0).astype(int)\n\n        # 3. Performance Evaluation\n        # Calculate accuracy for the identity-baseline classifier.\n        acc_I = np.mean(y_pred_I == y_true)\n\n        # Calculate accuracy for the residualized classifier.\n        acc_R = np.mean(y_pred_R == y_true)\n\n        # Calculate the accuracy improvement.\n        delta_acc = acc_R - acc_I\n        \n        results.append(delta_acc)\n\n    # Format results to 6 decimal places and print in the specified format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3169949"}, {"introduction": "在建立了残差块学习内容的直观理解后，我们现在转向一个更具理论性的问题：它们是如何支持极深网络训练的？本练习 [@problem_id:3169939] 使用一个简化的数学模型来分析梯度下降的动态过程。通过推导误差在网络中的传播方式，你将揭示恒等初始化在维持梯度稳定性方面的关键作用，从而理解 ResNet 如何有效避免传统深度网络中常见的梯度消失或爆炸问题。", "problem": "考虑一个一维残差网络 (ResNet)，其定义如下。一个包含 $L$ 个残差块的残差网络 (ResNet) 在第 $l$ 个块应用变换 $x \\mapsto x + W_{l} x$。因此，在一维情况下，每个块的参数都是一个标量 $w_{l} \\in \\mathbb{R}$，整个网络应用变换 $x \\mapsto \\left(\\prod_{l=1}^{L} (1 + w_{l})\\right) x$。假设我们绑定块参数，使得对所有 $l \\in \\{1, \\dots, L\\}$ 都有 $w_{l} = w$，从而得到网络输出 $y = (1 + w)^{L} x$。设目标映射为 $y^{\\star} = a x$，其中 $a > 0$ 是一个固定的标量，并假设输入 $x$ 是一个零均值随机变量，其方差为 $\\mathbb{E}[x^{2}] = 1$。考虑二次损失函数 $\\mathcal{L}(w) = \\frac{1}{2} \\mathbb{E}\\!\\left[\\left(y - y^{\\star}\\right)^{2}\\right]$。使用梯度下降法进行训练，步长为常数 $\\eta > 0$。\n\n从梯度下降和链式法则的基本定义出发，推导在当前参数 $w$ 下，单步梯度下降中标量误差 $e(w) = (1 + w)^{L} - a$ 的局部线性化单步收缩因子 $r(w)$ 的表达式。该因子由近似式 $e^{+} \\approx r(w)\\, e$ 定义，其中 $e^{+}$ 是一次更新后的误差，$e$ 是更新前的误差。然后，通过推导比率 $R = \\frac{r(\\delta)}{r(0)}$ 关于 $L$、$\\eta$ 和 $\\delta$ 的闭式表达式，比较恒等初始化 $w = 0$ 与小的随机初始化 $w = \\delta$（其中 $|\\delta| \\ll 1$）。\n\n你的最终答案必须是 $R$ 的这个单一解析表达式。不需要进行数值四舍五入。", "solution": "问题要求推导比率 $R = \\frac{r(\\delta)}{r(0)}$，其中 $r(w)$ 是一个用梯度下降法训练的简化一维残差网络中误差的局部线性化收缩因子。\n\n首先，我们形式化损失函数 $\\mathcal{L}(w)$。网络输出为 $y = (1 + w)^{L} x$，目标为 $y^{\\star} = a x$。损失由下式给出：\n$$\n\\mathcal{L}(w) = \\frac{1}{2} \\mathbb{E}\\!\\left[\\left(y - y^{\\star}\\right)^{2}\\right] = \\frac{1}{2} \\mathbb{E}\\!\\left[\\left((1 + w)^{L} x - a x\\right)^{2}\\right]\n$$\n我们可以将包含 $x$ 的项提取出来：\n$$\n\\mathcal{L}(w) = \\frac{1}{2} \\mathbb{E}\\!\\left[\\left((1 + w)^{L} - a\\right)^{2} x^{2}\\right]\n$$\n由于项 $\\left((1 + w)^{L} - a\\right)^{2}$ 对于关于随机变量 $x$ 的期望而言是常数，我们可以将其移到期望符号之外：\n$$\n\\mathcal{L}(w) = \\frac{1}{2} \\left((1 + w)^{L} - a\\right)^{2} \\mathbb{E}[x^{2}]\n$$\n问题陈述输入 $x$ 是一个零均值随机变量，其方差为 $\\mathbb{E}[x^{2}] = 1$。将此代入损失方程，得到：\n$$\n\\mathcal{L}(w) = \\frac{1}{2} \\left((1 + w)^{L} - a\\right)^{2}\n$$\n问题还将标量误差定义为 $e(w) = (1 + w)^{L} - a$。因此，损失函数可以紧凑地用误差表示：\n$$\n\\mathcal{L}(w) = \\frac{1}{2} e(w)^{2}\n$$\n接下来，我们使用梯度下降法进行训练。参数 $w$ 的更新规则是 $w^{+} = w - \\eta \\frac{d\\mathcal{L}}{dw}$，其中 $w^{+}$ 是一次更新步骤后的参数值，$\\eta > 0$ 是学习率。我们需要计算损失函数关于 $w$ 的梯度。使用链式法则：\n$$\n\\frac{d\\mathcal{L}}{dw} = \\frac{d\\mathcal{L}}{de} \\frac{de}{dw}\n$$\n导数是：\n$$\n\\frac{d\\mathcal{L}}{de} = \\frac{d}{de} \\left(\\frac{1}{2} e^{2}\\right) = e(w)\n$$\n$$\n\\frac{de}{dw} = \\frac{d}{dw} \\left((1 + w)^{L} - a\\right) = L(1 + w)^{L-1}\n$$\n结合这些结果，梯度为：\n$$\n\\frac{d\\mathcal{L}}{dw} = e(w) L (1 + w)^{L-1}\n$$\n现在我们可以写出 $w$ 的梯度下降更新规则：\n$$\nw^{+} = w - \\eta \\, e(w) \\, L (1 + w)^{L-1}\n$$\n问题要求我们找到误差的线性化单步动态 $e^{+} \\approx r(w) e(w)$，其中 $e^{+} = e(w^{+})$。我们可以通过围绕当前值 $w$ 对 $e(w^{+})$ 进行一阶泰勒展开来找到它：\n$$\ne^{+} = e(w^{+}) \\approx e(w) + (w^{+} - w) \\frac{de}{dw}(w)\n$$\n$w$ 的变化量是 $\\Delta w = w^{+} - w = -\\eta \\frac{d\\mathcal{L}}{dw}$。将此以及 $\\frac{de}{dw}$ 的表达式代入泰勒展开式，得到：\n$$\ne^{+} \\approx e(w) + \\left(-\\eta \\frac{d\\mathcal{L}}{dw}\\right) \\left(L(1 + w)^{L-1}\\right)\n$$\n现在，我们代入梯度的完整表达式 $\\frac{d\\mathcal{L}}{dw} = e(w) L (1+w)^{L-1}$：\n$$\ne^{+} \\approx e(w) - \\eta \\left(e(w) L (1+w)^{L-1}\\right) \\left(L(1 + w)^{L-1}\\right)\n$$\n简化此表达式：\n$$\ne^{+} \\approx e(w) - \\eta \\, e(w) \\, L^{2} (1+w)^{2(L-1)}\n$$\n提取出当前误差 $e(w)$：\n$$\ne^{+} \\approx e(w) \\left[ 1 - \\eta L^{2} (1+w)^{2(L-1)} \\right]\n$$\n通过将其与给定形式 $e^{+} \\approx r(w) e(w)$ 进行比较，我们可以确定局部收缩因子 $r(w)$：\n$$\nr(w) = 1 - \\eta L^{2} (1+w)^{2(L-1)}\n$$\n最后一步是计算比率 $R = \\frac{r(\\delta)}{r(0)}$。首先，我们在恒等初始化 $w=0$ 处计算 $r(w)$ 的值：\n$$\nr(0) = 1 - \\eta L^{2} (1+0)^{2(L-1)} = 1 - \\eta L^{2} (1) = 1 - \\eta L^{2}\n$$\n接下来，我们在小的随机初始化 $w=\\delta$ 处计算 $r(w)$ 的值：\n$$\nr(\\delta) = 1 - \\eta L^{2} (1+\\delta)^{2(L-1)}\n$$\n最后，我们构成比率 $R$：\n$$\nR = \\frac{r(\\delta)}{r(0)} = \\frac{1 - \\eta L^{2} (1+\\delta)^{2(L-1)}}{1 - \\eta L^{2}}\n$$\n这就是所要求的比率 $R$ 关于 $L$、$\\eta$ 和 $\\delta$ 的闭式表达式。", "answer": "$$\\boxed{\\frac{1 - \\eta L^{2} (1+\\delta)^{2(L-1)}}{1 - \\eta L^{2}}}$$", "id": "3169939"}, {"introduction": "残差学习的基本原理可以被有力地扩展和推广。最后的这个实践将带你探索 ResNeXt 架构，它通过聚合多个平行的转换路径来增强单个残差块的表示能力。本练习 [@problem_id:3169937] 将这一架构创新与经典的集成学习（ensemble learning）思想联系起来，要求你量化这些路径的“多样性”如何促成一个更精确的整体模型。你将推导并实现相关指标，从而理解为什么一个由多个多样化、简单变换构成的“更宽”模块，可能比单个复杂的变换更为有效。", "problem": "给定一个带有 ResNeXt 风格聚合变换的残差网络 (ResNet) 模块的形式化表示，其中模块输出被建模为并行分支的聚合。假设有 $C$ 个分支，每个分支实现一个标量变换 $F_i$，并将输入 $x$ 上的聚合变换定义为 $F(x) = \\sum_{i=1}^{C} F_i(x)$。考虑一个带有标量目标的有限数据集 $\\mathcal{D} = \\{(x_n, y_n)\\}_{n=1}^{N}$。为了进行分析，定义 $\\mathbb{R}^{N}$ 上的经验内积为 $\\langle u, v \\rangle_{\\mathcal{D}} = \\frac{1}{N} \\sum_{n=1}^{N} u_n v_n$，以及诱导范数为 $\\|u\\|_{\\mathcal{D}} = \\sqrt{\\langle u, u \\rangle_{\\mathcal{D}}}$。将整个数据集上的分支输出堆叠成一个矩阵 $Z \\in \\mathbb{R}^{N \\times C}$，其元素为 $Z_{n,i} = F_i(x_n)$，并定义目标向量 $y \\in \\mathbb{R}^{N}$，其元素为 $y_n$。令 $z_i \\in \\mathbb{R}^{N}$ 表示 $Z$ 的第 $i$ 列，经验残差为 $r_i = z_i - y$，其中 $i \\in \\{1,\\dots,C\\}$。\n\n您的任务是：\n- 仅使用上述定义和内积的线性性质，将分支输出之间的平均非对角余弦相似度定义为\n$$\nS = \\frac{2}{C(C-1)} \\sum_{1 \\le i  j \\le C} \\frac{\\langle z_i, z_j \\rangle_{\\mathcal{D}}}{\\|z_i\\|_{\\mathcal{D}} \\, \\|z_j\\|_{\\mathcal{D}}}\n$$\n并约定，对于任何满足 $\\|z_i\\|_{\\mathcal{D}}=0$ 或 $\\|z_j\\|_{\\mathcal{D}}=0$ 的配对 $(i,j)$，都将其从平均值计算中排除；如果排除后没有剩余的配对，则设 $S = 0$。定义多样性得分 $D = 1 - S$。\n- 定义平均集成 $\\bar{F}(x) = \\frac{1}{C} \\sum_{i=1}^{C} F_i(x)$ 及其在 $\\mathcal{D}$ 上的经验均方误差 (MSE) 为 $\\mathrm{MSE}_{\\mathrm{avg}} = \\left\\|\\frac{1}{C} \\sum_{i=1}^{C} r_i \\right\\|_{\\mathcal{D}}^{2}$。定义平均单分支经验 MSE 为 $\\mathrm{MSE}_{\\mathrm{single}} = \\frac{1}{C} \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2}$。仅使用内积的双线性和对称性，将精确改进率\n$$\n\\mathrm{IR}_{\\mathrm{exact}} = \\frac{\\mathrm{MSE}_{\\mathrm{avg}}}{\\mathrm{MSE}_{\\mathrm{single}}}\n$$\n用 $\\|r_i\\|_{\\mathcal{D}}^{2}$ 和 $\\langle r_i, r_j \\rangle_{\\mathcal{D}}$ （其中 $1 \\le i  j \\le C$）表示。\n- 定义平均残差余弦相似度\n$$\n\\hat{\\rho} = \\frac{2}{C(C-1)} \\sum_{1 \\le i  j \\le C} \\frac{\\langle r_i, r_j \\rangle_{\\mathcal{D}}}{\\|r_i\\|_{\\mathcal{D}} \\, \\|r_j\\|_{\\mathcal{D}}}\n$$\n同样地，排除分母中范数为零的任何配对；如果没有剩余配对，则设 $\\hat{\\rho} = 0$。在所有分支具有相等残差方差和共同成对相关性的简化近似下，定义近似改进率\n$$\n\\mathrm{IR}_{\\rho} = \\frac{1}{C} \\left( 1 + (C-1)\\hat{\\rho} \\right).\n$$\n\n程序要求：\n- 实现一个程序，给定一个小型合成案例 $(Z, y)$ 的测试套件，为每个案例计算以下五个量：$S$, $D$, $\\hat{\\rho}$, $\\mathrm{IR}_{\\mathrm{exact}}$, $\\mathrm{IR}_{\\rho}$。在计算余弦相似度的平均值时，按上述规定排除未定义的配对；如果所有配对都被排除，则平均值使用默认值 $0$。如果 $\\mathrm{MSE}_{\\mathrm{single}} = 0$，则定义 $\\mathrm{IR}_{\\mathrm{exact}} = 0$。\n- 程序必须输出单行内容，该行为一个包含各案例结果列表的列表。每个案例的结果列表必须按 $[S, D, \\hat{\\rho}, \\mathrm{IR}_{\\mathrm{exact}}, \\mathrm{IR}_{\\rho}]$ 的顺序排列，每个值都打印为四舍五入到六位小数的十进制浮点数。\n\n用于覆盖不同情况的测试套件：\n- 案例 1（理想情况，中等相关性）：$N=6$, $C=4$, $y = [1.0, -1.0, 0.5, -0.5, 0.0, 1.0]$，以及\n  $z_1 = y + [0.1, -0.1, 0.2, -0.2, 0.0, 0.1]$,\n  $z_2 = y + [-0.1, 0.1, 0.1, -0.1, 0.0, -0.05]$,\n  $z_3 = y + [0.05, 0.0, -0.1, 0.1, 0.0, -0.05]$,\n  $z_4 = y + [0.0, 0.05, 0.0, 0.0, 0.0, 0.1]$.\n- 案例 2（无多样性，分支相同）：$N=5$, $C=3$, $y = [0.5, -0.5, 1.0, -1.0, 0.0]$，以及 $z_1 = z_2 = z_3 = y + [0.2, -0.2, 0.1, -0.1, 0.0]$。\n- 案例 3（两个分支的最大负残差相关性）：$N=4$, $C=2$, $y = [1.0, 0.0, -1.0, 1.0]$, $z_1 = y + [0.5, -0.5, 0.5, -0.5]$, $z_2 = y - [0.5, -0.5, 0.5, -0.5]$。\n- 案例 4（近似正交的残差）：$N=4$, $C=2$, $y = [0.0, 0.0, 0.0, 0.0]$, $z_1 = [1.0, -1.0, 1.0, -1.0]$, $z_2 = [1.0, 1.0, -1.0, -1.0]$。\n- 案例 5（一个完美分支，其他有噪声；存在退化的零范数残差）：$N=2$, $C=3$, $y = [1.0, -1.0]$, $z_1 = [1.0, -1.0]$, $z_2 = [1.5, -1.5]$, $z_3 = [0.5, -0.5]$。\n\n最终输出格式：\n- 您的程序应生成单行内容，其中包含一个 Python 风格的列表的列表，每个内部列表对应一个测试案例，按指定顺序排列，每个浮点数四舍五入到六位小数，例如：\"[[s1,d1,rho1,ir_e1,ir_r1],[s2,d2,rho2,ir_e2,ir_r2],...]\"。", "solution": "问题陈述已经过验证，被认为是有效的。它在科学上基于集成学习理论应用于神经网络架构的原理，在数学上是适定的，提供了所有必要的定义和数据，并以客观、正式的语言表述。不存在矛盾、歧义或不合理的假设。因此，我们可以着手解决问题。\n\n主要任务是推导精确改进率 $\\mathrm{IR}_{\\mathrm{exact}}$ 的显式公式，并实现一个程序来为给定的测试套件计算五个指定的度量指标。这些度量指标是：分支输出的平均非对角余弦相似度 $S$；多样性得分 $D$；平均残差余弦相似度 $\\hat{\\rho}$；精确改进率 $\\mathrm{IR}_{\\mathrm{exact}}$；以及近似改进率 $\\mathrm{IR}_{\\rho}$。\n\n首先，我们来推导精确改进率 $\\mathrm{IR}_{\\mathrm{exact}}$。问题将此比率定义为：\n$$\n\\mathrm{IR}_{\\mathrm{exact}} = \\frac{\\mathrm{MSE}_{\\mathrm{avg}}}{\\mathrm{MSE}_{\\mathrm{single}}}\n$$\n其中，平均集成均方误差 (MSE) 为 $\\mathrm{MSE}_{\\mathrm{avg}} = \\left\\|\\frac{1}{C} \\sum_{i=1}^{C} r_i \\right\\|_{\\mathcal{D}}^{2}$，平均单分支 MSE 为 $\\mathrm{MSE}_{\\mathrm{single}} = \\frac{1}{C} \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2}$。向量 $r_i \\in \\mathbb{R}^N$ 代表 $C$ 个分支中每个分支的经验残差。\n\n使用诱导范数的定义 $\\|u\\|_{\\mathcal{D}}^2 = \\langle u, u \\rangle_{\\mathcal{D}}$，我们可以展开分子 $\\mathrm{MSE}_{\\mathrm{avg}}$：\n$$\n\\mathrm{MSE}_{\\mathrm{avg}} = \\left\\langle \\frac{1}{C} \\sum_{i=1}^{C} r_i, \\frac{1}{C} \\sum_{j=1}^{C} r_j \\right\\rangle_{\\mathcal{D}}\n$$\n根据内积的双线性，我们可以将常数因子提出并展开求和：\n$$\n\\mathrm{MSE}_{\\mathrm{avg}} = \\frac{1}{C^2} \\left\\langle \\sum_{i=1}^{C} r_i, \\sum_{j=1}^{C} r_j \\right\\rangle_{\\mathcal{D}} = \\frac{1}{C^2} \\sum_{i=1}^{C} \\sum_{j=1}^{C} \\langle r_i, r_j \\rangle_{\\mathcal{D}}\n$$\n这个双重求和可以分为 $i = j$ 的项和 $i \\neq j$ 的项。根据内积的对称性 $\\langle r_i, r_j \\rangle_{\\mathcal{D}} = \\langle r_j, r_i \\rangle_{\\mathcal{D}}$，对 $i \\neq j$ 的求和可以表示为对 $i  j$ 的配对求和的两倍：\n$$\n\\sum_{i=1}^{C} \\sum_{j=1}^{C} \\langle r_i, r_j \\rangle_{\\mathcal{D}} = \\sum_{i=1}^{C} \\langle r_i, r_i \\rangle_{\\mathcal{D}} + \\sum_{i \\neq j} \\langle r_i, r_j \\rangle_{\\mathcal{D}} = \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2} + 2 \\sum_{1 \\le i  j \\le C} \\langle r_i, r_j \\rangle_{\\mathcal{D}}\n$$\n将此代回 $\\mathrm{MSE}_{\\mathrm{avg}}$ 的表达式得到：\n$$\n\\mathrm{MSE}_{\\mathrm{avg}} = \\frac{1}{C^2} \\left( \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2} + 2 \\sum_{1 \\le i  j \\le C} \\langle r_i, r_j \\rangle_{\\mathcal{D}} \\right)\n$$\n现在，我们构造比率 $\\mathrm{IR}_{\\mathrm{exact}}$：\n$$\n\\mathrm{IR}_{\\mathrm{exact}} = \\frac{\\frac{1}{C^2} \\left( \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2} + 2 \\sum_{1 \\le i  j \\le C} \\langle r_i, r_j \\rangle_{\\mathcal{D}} \\right)}{\\frac{1}{C} \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2}}\n$$\n通过消去因子 $1/C$ 并重新整理，可以得到以 $\\|r_i\\|_{\\mathcal{D}}^{2}$ 和 $\\langle r_i, r_j \\rangle_{\\mathcal{D}}$ 表示的所需形式：\n$$\n\\mathrm{IR}_{\\mathrm{exact}} = \\frac{\\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2} + 2 \\sum_{1 \\le i  j \\le C} \\langle r_i, r_j \\rangle_{\\mathcal{D}}}{C \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2}}\n$$\n此推导完成了任务的分析部分。\n\n计算策略包括实现一个程序，为每个测试案例 $(Z, y)$ 计算五个指定的量。关键步骤如下：\n\n1.  **数据表示**：每个案例的输入数据，包括目标向量 $y$ 和分支输出矩阵 $Z$，将表示为 `numpy` 数组。这允许高效的向量化操作。从 $Z$ 和 $y$ 中，我们获得数据点数 $N$、分支数 $C$、分支输出向量 $z_i$ 以及残差向量 $r_i = z_i - y$。\n\n2.  **内积和范数**：将实现辅助函数来计算经验内积 $\\langle u, v \\rangle_{\\mathcal{D}} = \\frac{1}{N} \\sum_{n=1}^{N} u_n v_n$ 和诱导范数 $\\|u\\|_{\\mathcal{D}} = \\sqrt{\\langle u, u \\rangle_{\\mathcal{D}}}$。它们分别对应于 `numpy.dot(u, v) / N` 和 `numpy.sqrt(numpy.dot(u, u) / N)`。\n\n3.  **相似度平均值（$S$ 和 $\\hat{\\rho}$）**：平均余弦相似度 $S$（针对输出 $z_i$）和 $\\hat{\\rho}$（针对残差 $r_i$）是通过遍历所有唯一的向量对 $(v_i, v_j)$（其中 $i  j$）来计算的。对于每对向量，我们计算其范数 $\\|v_i\\|_{\\mathcal{D}}$ 和 $\\|v_j\\|_{\\mathcal{D}}$。如果任一范数为零，则该配对被排除，如问题陈述中所指定。对于所有有效配对，计算其余弦相似度 $\\frac{\\langle v_i, v_j \\rangle_{\\mathcal{D}}}{\\|v_i\\|_{\\mathcal{D}} \\|v_j\\|_{\\mathcal{D}}}$。最终值（$S$ 或 $\\hat{\\rho}$）是这些相似度的算术平均值。如果不存在有效配对，则该值设为 $0$。\n\n4.  **多样性得分 ($D$)**：直接由 $S$ 计算得出，$D = 1 - S$。\n\n5.  **改进率（$\\mathrm{IR}_{\\mathrm{exact}}$ 和 $\\mathrm{IR}_{\\rho}$）**：\n    *   $\\mathrm{IR}_{\\mathrm{exact}}$ 的计算方法是首先根据残差向量 $r_i$ 计算 $\\mathrm{MSE}_{\\mathrm{avg}}$ 和 $\\mathrm{MSE}_{\\mathrm{single}}$。如果 $\\mathrm{MSE}_{\\mathrm{single}}$ 为零，则根据问题约定将 $\\mathrm{IR}_{\\mathrm{exact}}$ 设为 $0$。否则，计算该比率。该方法在数值上等同于使用上面推导的公式。\n    *   $\\mathrm{IR}_{\\rho}$ 使用其定义 $\\mathrm{IR}_{\\rho} = \\frac{1}{C} ( 1 + (C-1)\\hat{\\rho} )$ 进行计算，其中使用先前计算出的 $\\hat{\\rho}$ 值。\n\n该计算计划系统地解决了所有要求，包括对边缘情况的指定处理，确保了实现方式的稳健性和正确性。每个测试案例的结果将被收集并格式化为所需的输出字符串。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ResNeXt analysis problem for the given test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (happy path, moderate correlation)\n        {\n            \"N\": 6, \"C\": 4, \n            \"y\": np.array([1.0, -1.0, 0.5, -0.5, 0.0, 1.0]),\n            \"Z\": np.array([\n                [1.1, -1.1, 0.7, -0.7, 0.0, 1.1],     # z1\n                [0.9, -0.9, 0.6, -0.6, 0.0, 0.95],    # z2\n                [1.05, -1.0, 0.4, -0.4, 0.0, 0.95],   # z3\n                [1.0, -0.95, 0.5, -0.5, 0.0, 1.1]     # z4\n            ]).T\n        },\n        # Case 2 (no diversity, identical branches)\n        {\n            \"N\": 5, \"C\": 3,\n            \"y\": np.array([0.5, -0.5, 1.0, -1.0, 0.0]),\n            \"Z\": np.array([\n                [0.7, -0.7, 1.1, -1.1, 0.0], # z1\n                [0.7, -0.7, 1.1, -1.1, 0.0], # z2\n                [0.7, -0.7, 1.1, -1.1, 0.0]  # z3\n            ]).T\n        },\n        # Case 3 (maximally negative residual correlation)\n        {\n            \"N\": 4, \"C\": 2,\n            \"y\": np.array([1.0, 0.0, -1.0, 1.0]),\n            \"Z\": np.array([\n                [1.5, -0.5, -0.5, 0.5], # z1\n                [0.5, 0.5, -1.5, 1.5]  # z2\n            ]).T\n        },\n        # Case 4 (approximately orthogonal residuals)\n        {\n            \"N\": 4, \"C\": 2,\n            \"y\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"Z\": np.array([\n                [1.0, -1.0, 1.0, -1.0], # z1\n                [1.0, 1.0, -1.0, -1.0]  # z2\n            ]).T\n        },\n        # Case 5 (one perfect branch, degenerate zero-norm residual)\n        {\n            \"N\": 2, \"C\": 3,\n            \"y\": np.array([1.0, -1.0]),\n            \"Z\": np.array([\n                [1.0, -1.0],   # z1 (perfect)\n                [1.5, -1.5],   # z2\n                [0.5, -0.5]    # z3\n            ]).T\n        }\n    ]\n\n    # Helper function for empirical inner product\n    def empirical_inner_product(u, v, N):\n        return np.dot(u, v) / N\n\n    # Helper function for empirical norm\n    def empirical_norm(u, N):\n        return np.sqrt(empirical_inner_product(u, u, N))\n\n    def compute_average_cosine_similarity(vectors, C, N):\n        if C  2:\n            return 0.0\n        \n        similarities = []\n        for i in range(C):\n            for j in range(i + 1, C):\n                u, v = vectors[i], vectors[j]\n                norm_u = empirical_norm(u, N)\n                norm_v = empirical_norm(v, N)\n                \n                if norm_u == 0 or norm_v == 0:\n                    continue\n                \n                inner_prod = empirical_inner_product(u, v, N)\n                cos_sim = inner_prod / (norm_u * norm_v)\n                similarities.append(cos_sim)\n                \n        if not similarities:\n            return 0.0\n        \n        return np.mean(similarities)\n\n    def calculate_metrics(Z, y, C, N):\n        # Extract z_i vectors\n        z_vectors = [Z[:, i] for i in range(C)]\n        \n        # Calculate S and D\n        S = compute_average_cosine_similarity(z_vectors, C, N)\n        D = 1 - S\n        \n        # Calculate residual vectors r_i\n        r_vectors = [z - y for z in z_vectors]\n        \n        # Calculate rho_hat\n        rho_hat = compute_average_cosine_similarity(r_vectors, C, N)\n        \n        # Calculate IR_exact\n        per_branch_mses = [empirical_inner_product(r, r, N) for r in r_vectors]\n        mse_single = np.mean(per_branch_mses)\n        \n        if mse_single == 0:\n            ir_exact = 0.0\n        else:\n            avg_residual = np.sum(r_vectors, axis=0) / C\n            mse_avg = empirical_inner_product(avg_residual, avg_residual, N)\n            ir_exact = mse_avg / mse_single\n            \n        # Calculate IR_rho\n        ir_rho = (1 + (C - 1) * rho_hat) / C if C > 0 else 0.0\n        \n        return [S, D, rho_hat, ir_exact, ir_rho]\n\n    all_results = []\n    for case in test_cases:\n        Z, y, C, N = case['Z'], case['y'], case['C'], case['N']\n        results = calculate_metrics(Z, y, C, N)\n        all_results.append(results)\n\n    # Format the final output string\n    formatted_cases = []\n    for res_list in all_results:\n        # Round each value and format to 6 decimal places\n        formatted_list_str = '[' + ','.join([f'{val:.6f}' for val in res_list]) + ']'\n        formatted_cases.append(formatted_list_str)\n\n    final_output = '[' + ','.join(formatted_cases) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3169937"}]}