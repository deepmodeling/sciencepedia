{"hands_on_practices": [{"introduction": "GoogLeNet Inception模块的核心思想是同时在多个尺度上处理信息。本练习将从信号处理的角度探讨这一选择背后的直觉。我们将使用一个简化的模型来研究不同的卷积核尺寸（在此作为平均滤波器）如何通过信噪比（SNR）这一量化指标来影响噪声的抑制效果，帮助您理解为何结合不同感受野的输出是有益的。[@problem_id:3130750]", "problem": "您将实现并分析一个简化的Inception类多分支模块，仅使用第一性原理推导，研究在白噪声模型下，空间核尺寸如何通过内在平均影响降噪效果。考虑一个单通道图像和一个Inception类模块，该模块具有多个并行分支，每个分支应用一次卷积，使用大小为 $k \\times k$、步幅为 $1$ 的方形核。每个分支使用一个归一化至和为1的均匀平均核，该核模拟了Inception分支中较大感受野的空间聚合行为。您只能使用以下基础原理：卷积的线性性、卷积作为加权和的定义、信噪比（SNR）定义为信号功率除以噪声功率，以及独立同分布的零均值高斯噪声的性质。\n\n任务。给定一个干净图像 $S$ 和加性噪声 $N$，其中 $N$ 是独立同分布的零均值高斯噪声，方差为 $\\sigma^2$。在指定的空间支持域 $\\Omega$ 上，将输入信噪比定义为\n$$\n\\mathrm{SNR}_{\\text{in}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right]}{\\mathbb{E}\\left[N(i,j)^2\\right]}。\n$$\n对于一个使用大小为 $k \\times k$ 的核 $h_k$ 的分支，将输出信号定义为 $S_{\\mathrm{out}} = h_k * S$，输出噪声定义为 $N_{\\mathrm{out}} = h_k * N$。输出信噪比为\n$$\n\\mathrm{SNR}_{\\text{out}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right]}{\\mathbb{E}\\left[N_{\\mathrm{out}}(i,j)^2\\right]}。\n$$\n此处 $*$ 表示卷积。为避免边缘效应并确保在每个输出位置应用相同的滤波器，您必须使用有效卷积（无填充），使得输出支持域 $\\Omega_k$ 是所有 $k \\times k$ 核完全包含在输入中的位置集合。对于每个分支，信噪比改善因子定义为比率\n$$\nG_k = \\frac{\\mathrm{SNR}_{\\text{out}}}{\\mathrm{SNR}_{\\text{in}}}。\n$$\n您必须在每个测试用例中，通过第一性原理，在独立同分布高斯假设下推导滤波后的噪声功率，来计算每个核尺寸的 $G_k$。不要使用对多个噪声实现的经验平均；相反，应使用独立变量线性组合的性质来获得滤波后的噪声功率。所有卷积必须以步幅为 $1$ 和有效模式实现。\n\n微光数据集模型。根据测试用例描述确定性地构建干净图像 $S$。通过选择小的正振幅 $A$ 和相对较大的噪声标准差 $\\sigma$ 来模拟微光。噪声分布仅用于按推导要求符号化地确定噪声功率；您无需对噪声进行采样。\n\n滤波器。对于给定的核尺寸 $k$，分支滤波器是一个均匀平均核，其系数在 $k \\times k$ 窗口内对所有 $u,v$ 均为 $h(u,v)=1/k^2$。\n\n要求的输出。对于每个测试用例，报告一个信噪比改善因子列表 $G_k$，针对指定的核尺寸 $k$，顺序与测试用例中列出的顺序相同。将每个报告的因子四舍五入到 $6$ 位小数。\n\n测试套件。精确实现以下测试用例：\n- 测试用例 $1$：图像尺寸 $H=W=32$，干净信号类型：常量，振幅 $A=0.1$，噪声标准差 $\\sigma=0.2$，核尺寸 $k \\in \\{1, 3, 5\\}$。\n- 测试用例 $2$：图像尺寸 $H=W=32$，干净信号类型：棋盘格，像素值在 $\\{0,A\\}$ 中交替，振幅 $A=0.1$，噪声标准差 $\\sigma=0.2$，核尺寸 $k \\in \\{1, 3, 5\\}$。\n- 测试用例 $3$：图像尺寸 $H=W=32$，干净信号类型：常量，振幅 $A=0.05$，噪声标准差 $\\sigma=0.3$，核尺寸 $k \\in \\{1, 5, 7\\}$。\n- 测试用例 $4$：图像尺寸 $H=W=7$，干净信号类型：常量，振幅 $A=0.1$，噪声标准差 $\\sigma=0.2$，核尺寸 $k \\in \\{1, 7\\}$。\n\n最终输出格式。您的程序应生成单行输出，其中包含所有测试用例的结果，形式为单一的列表之列表。每个内部列表必须包含该测试用例的信噪比改善因子 $G_k$，顺序与为该测试用例给定的核尺寸顺序一致。所有数字必须四舍五入到 $6$ 位小数。该行必须格式化为类JSON的列表之列表，不含空格，例如使用符号占位符：$[[g_{1,1},g_{1,2},\\dots],[g_{2,1},\\dots],\\dots]$。", "solution": "此用户问题被认为是有效的，因为它是科学上合理的、适定的、客观的，并包含了唯一解所需的所有必要信息。任务是为一个使用大小为 $k \\times k$ 均匀平均核的简化Inception类模块分支，推导并计算信噪比（SNR）改善因子 $G_k$。\n\n推导按规定从第一性原理出发。信噪比改善因子 $G_k$ 定义为输出信噪比与输入信噪比之比：\n$$\nG_k = \\frac{\\mathrm{SNR}_{\\text{out}}}{\\mathrm{SNR}_{\\text{in}}}\n$$\n输入和输出信噪比定义为信号功率与噪声功率之比。\n$$\n\\mathrm{SNR}_{\\text{in}} = \\frac{P_{\\mathrm{S, in}}}{P_{\\mathrm{N, in}}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right]}{\\mathbb{E}\\left[N(i,j)^2\\right]}\n$$\n$$\n\\mathrm{SNR}_{\\text{out}} = \\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{N, out}}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right]}{\\mathbb{E}\\left[N_{\\mathrm{out}}(i,j)^2\\right]}\n$$\n此处，$S$ 和 $N$ 是输入信号和噪声，而 $S_{\\mathrm{out}}$ 和 $N_{\\mathrm{out}}$ 是与核 $h_k$ 卷积后的信号和噪声。符号 $\\mathbb{E}_{(i,j)\\in\\Omega}[\\cdot]$ 表示在支持域 $\\Omega$ 上的空间平均，而 $\\mathbb{E}[\\cdot]$ 表示在噪声分布上的统计期望。\n\n将这些定义代入 $G_k$ 的表达式中，得到：\n$$\nG_k = \\frac{P_{\\mathrm{S, out}} / P_{\\mathrm{N, out}}}{P_{\\mathrm{S, in}} / P_{\\mathrm{N, in}}} = \\left(\\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{S, in}}}\\right) \\left(\\frac{P_{\\mathrm{N, in}}}{P_{\\mathrm{N, out}}}\\right)\n$$\n我们分别分析噪声功率比和信号功率比。\n\n首先，我们分析噪声功率比。输入噪声 $N(i,j)$ 是独立同分布（i.i.d.）的，其均值为零，方差为 $\\sigma^2$。因此，任何像素的输入噪声功率为：\n$$\nP_{\\mathrm{N, in}} = \\mathbb{E}[N(i,j)^2] = \\mathrm{Var}(N(i,j)) + (\\mathbb{E}[N(i,j)])^2 = \\sigma^2 + 0^2 = \\sigma^2\n$$\n输出噪声 $N_{\\mathrm{out}}$ 是输入噪声 $N$ 与均匀平均核 $h_k$（其中 $h_k(u,v) = 1/k^2$）卷积的结果。在输出位置 $(i,j)$，输出噪声是输入噪声样本的线性组合：\n$$\nN_{\\mathrm{out}}(i,j) = (h_k * N)(i,j) = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} N(i+u, j+v)\n$$\n由于期望的线性性和 $\\mathbb{E}[N]=0$，输出噪声的期望为零：\n$$\n\\mathbb{E}[N_{\\mathrm{out}}(i,j)] = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} \\mathbb{E}[N(i+u, j+v)] = 0\n$$\n输出噪声功率是其方差。由于输入噪声样本 $N(i,j)$ 是独立的，其加权和的方差等于各项方差之和：\n$$\nP_{\\mathrm{N, out}} = \\mathbb{E}[N_{\\mathrm{out}}(i,j)^2] = \\mathrm{Var}(N_{\\mathrm{out}}(i,j)) = \\mathrm{Var}\\left(\\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} N(i+u, j+v)\\right)\n$$\n$$\nP_{\\mathrm{N, out}} = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\mathrm{Var}\\left(\\frac{1}{k^2} N(i+u, j+v)\\right) = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\left(\\frac{1}{k^2}\\right)^2 \\mathrm{Var}(N(i+u, j+v))\n$$\n由于 $k^2$ 个噪声样本中的每一个方差都是 $\\sigma^2$：\n$$\nP_{\\mathrm{N, out}} = k^2 \\cdot \\frac{1}{k^4} \\sigma^2 = \\frac{\\sigma^2}{k^2}\n$$\n因此，噪声功率比为：\n$$\n\\frac{P_{\\mathrm{N, in}}}{P_{\\mathrm{N, out}}} = \\frac{\\sigma^2}{\\sigma^2 / k^2} = k^2\n$$\n这证明了通过平均来降低噪声的基本原理：对 $k^2$ 个独立同分布的噪声样本进行平均，会将噪声功率降低 $k^2$ 倍。\n\n现在，我们考虑信号功率比。这是通过在各自的域上进行空间平均来计算的。对于一个大小为 $H \\times W$ 的图像，输入信号功率为：\n$$\nP_{\\mathrm{S, in}} = \\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right] = \\frac{1}{HW} \\sum_{i=0}^{H-1} \\sum_{j=0}^{W-1} S(i,j)^2\n$$\n输出信号为 $S_{\\mathrm{out}} = h_k * S$。使用‘有效’卷积，输出图像的维度为 $(H-k+1) \\times (W-k+1)$。输出信号功率为：\n$$\nP_{\\mathrm{S, out}} = \\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right] = \\frac{1}{(H-k+1)(W-k+1)} \\sum_{i=0}^{H-k} \\sum_{j=0}^{W-k} S_{\\mathrm{out}}(i,j)^2\n$$\n结合噪声和信号功率比，信噪比改善因子 $G_k$ 的最终表达式为：\n$$\nG_k = k^2 \\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{S, in}}}\n$$\n此公式被实现用于解决每个测试用例的 $G_k$。\n对于恒定信号 $S(i,j)=A$，平均核产生恒定的输出 $S_{\\mathrm{out}}(i,j) = A$。因此，$P_{\\mathrm{S, out}} = A^2$ 且 $P_{\\mathrm{S, in}} = A^2$。信号功率比为 $1$，所以 $G_k=k^2$。\n对于像棋盘格这样的高频信号，平均操作相当于一个低通滤波器，会衰减信号。这导致 $P_{\\mathrm{S, out}}  P_{\\mathrm{S, in}}$，因此 $G_k  k^2$。实现中通过数值计算卷积和后续的空间平均，来确定每种情况下信号功率的比值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_g_factors(H, W, signal_type, A, k_list):\n    \"\"\"\n    Computes the SNR improvement factor G_k for a list of kernel sizes.\n\n    Args:\n        H (int): Image height.\n        W (int): Image width.\n        signal_type (str): Type of clean signal ('constant' or 'checkerboard').\n        A (float): Amplitude of the signal.\n        k_list (list): List of kernel sizes (k).\n\n    Returns:\n        list: A list of SNR improvement factors G_k, rounded to 6 decimal places.\n    \"\"\"\n    # 1. Construct the clean signal S\n    if signal_type == 'constant':\n        S = np.full((H, W), A, dtype=np.float64)\n    elif signal_type == 'checkerboard':\n        S = np.zeros((H, W), dtype=np.float64)\n        # Use broadcasting for efficient checkerboard generation\n        i_coords = np.arange(H).reshape(-1, 1)\n        j_coords = np.arange(W).reshape(1, -1)\n        mask = (i_coords + j_coords) % 2 == 0\n        S[mask] = A\n    else:\n        raise ValueError(\"Unknown signal type\")\n\n    # 2. Calculate input signal power P_s_in\n    # This is the mean of the squared signal values\n    P_s_in = np.mean(S**2)\n\n    g_factors = []\n    \n    # Check for zero-signal case to prevent division by zero, though not in test cases\n    if P_s_in == 0:\n        # If signal is zero, SNR is zero both in and out. The ratio is ill-defined.\n        # Based on the formula, P_s_out will also be 0, leading to 0/0.\n        # We can return a placeholder or handle as an error. For this problem, A  0.\n        # Arbitrarily returning k^2 as if signal power ratio is 1.\n        return [float(k**2) for k in k_list]\n\n    for k in k_list:\n        # 3. Calculate output signal S_out by 'valid' convolution\n        H_out = H - k + 1\n        W_out = W - k + 1\n        S_out = np.zeros((H_out, W_out), dtype=np.float64)\n        \n        # Explicitly implement convolution with a uniform averaging kernel\n        for i in range(H_out):\n            for j in range(W_out):\n                patch = S[i:i+k, j:j+k]\n                S_out[i, j] = np.mean(patch)\n\n        # 4. Calculate output signal power P_s_out\n        P_s_out = np.mean(S_out**2) if S_out.size  0 else 0.0\n\n        # 5. Calculate G_k using the derived formula\n        g_k = (k**2) * P_s_out / P_s_in\n        g_factors.append(g_k)\n\n    return [round(g, 6) for g in g_factors]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'H': 32, 'W': 32, 'signal_type': 'constant', 'A': 0.1, 'k_list': [1, 3, 5]},\n        {'H': 32, 'W': 32, 'signal_type': 'checkerboard', 'A': 0.1, 'k_list': [1, 3, 5]},\n        {'H': 32, 'W': 32, 'signal_type': 'constant', 'A': 0.05, 'k_list': [1, 5, 7]},\n        {'H': 7, 'W': 7, 'signal_type': 'constant', 'A': 0.1, 'k_list': [1, 7]}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = calculate_g_factors(case['H'], case['W'], case['signal_type'], case['A'], case['k_list'])\n        all_results.append(results)\n\n    # Format the final output as a single-line JSON-like list of lists\n    result_str = '[' + ','.join(\n        '[' + ','.join([f'{val:.6f}' for val in res]) + ']' for res in all_results\n    ) + ']'\n    print(result_str)\n\nsolve()\n\n```", "id": "3130750"}, {"introduction": "虽然较大的卷积核能捕捉更多的上下文信息并有助于降噪，但它们的计算成本也更高。本练习将探讨后续Inception版本中引入的一项关键优化：将一个大的方形卷积分解为一系列更小的非对称卷积。您将定量分析浮点运算次数（FLOPs）的减少，并理解这种分解如何在不带来过高计算成本的情况下，构建更深、更宽的网络。[@problem_id:3130734]", "problem": "您正在研究 GoogLeNet (Inception) 架构中的计算效率技术，特别是将一个方形卷积分解为两个可分离的矩形卷积的方法。使用深度学习中卷积成本的标准定义，从第一性原理推导出单个二维卷积和分解后的两个卷积序列的精确浮点运算次数 (FLOPs)。然后，量化用一个 $1 \\times K$ 卷积后跟一个 $K \\times 1$ 卷积替换单个 $K \\times K$ 卷积时所节省的计算量。最后，在一个提供了标签和预测类别的抽象 ImageNet-100 子集上计算 top-1 准确率，以评估分解是否保持了准确率。\n\n使用的基础和定义如下：\n- 一个二维卷积，其输入通道数为 $C_{\\text{in}}$，输出通道数为 $C_{\\text{out}}$，卷积核大小为 $K \\times K$，输出空间维度为 $H_{\\text{out}} \\times W_{\\text{out}}$。按照惯例，每次乘加运算（一次乘法和一次加法）计为 $2$ FLOPs。在此惯例下，单个 $K \\times K$ 卷积的 FLOPs 为\n$$\nF_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2.\n$$\n- 对于一对分解后的卷积，包括一个将 $C_{\\text{in}} \\to C_{\\text{mid}}$ 的 $1 \\times K$ 卷积，后跟一个将 $C_{\\text{mid}} \\to C_{\\text{out}}$ 的 $K \\times 1$ 卷积，其 FLOPs 为\n$$\nF_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right).\n$$\n- FLOPs 节省率定义为\n$$\nS \\;=\\; \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}}.\n$$\n- Top-1 准确率定义为预测类别索引与真实标签相等的样本所占的比例，即：\n$$\n\\text{acc} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{ \\hat{y}_i = y_i \\},\n$$\n其中 $N$ 是样本数量，$y_i \\in \\{0, \\dots, C-1\\}$ 是真实标签，$\\hat{y}_i$ 是预测的类别索引。\n\n任务：\n1) 代数证明，如果 $C_{\\text{mid}} = C_{\\text{out}} = C_{\\text{in}}$ 且 $K = 7$，那么分解后的卷积对使用的 FLOPs 恰好是原始 $7 \\times 7$ 卷积的 $2/7$，因此 FLOPs 降低为原始值的 $2/7$。报告精确的节省率 $S$。\n2) 针对以下三个测试用例，计算精确的 FLOPs 节省率 $S$，假设步长为 $1$ 且输出空间维度如给定。根据上述惯例计算 FLOPs，并将节省率表示为四舍五入到六位小数的小数。\n   - 测试 A (理想情况)：$H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$。\n   - 测试 B (边界情况)：$H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$。\n   - 测试 C (瓶颈分解)：$H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$。\n3) 使用一个包含 $N = 12$ 个样本和 $C = 100$ 个类别的抽象 ImageNet-100 子集，其标签和预测值如下所示，计算基线模型（单个 $K \\times K$ 卷积）和分解模型（一个 $1 \\times K$ 卷积后跟一个 $K \\times 1$ 卷积）的 top-1 准确率，并报告两种准确率及其差值（分解模型减去基线模型）。标签和预测的类别索引如下：\n   - 真实标签 $y$：$[5,12,23,23,11,65,99,0,3,42,77,13]$。\n   - 基线模型预测类别 $\\hat{y}^{\\text{base}}$：$[5,11,23,3,11,65,13,0,2,42,8,13]$。\n   - 分解模型预测类别 $\\hat{y}^{\\text{fact}}$：$[5,12,23,23,9,65,99,0,3,1,77,88]$。\n\n角度或物理单位不适用。所有要求的数值输出必须是四舍五入到六位小数的小数。\n\n程序要求：\n- 实现一个程序，计算：\n  - Test A、Test B 和 Test C 的 FLOPs 节省率 $S$。\n  - 在提供的 ImageNet-100 子集上，基线和分解模型预测的 top-1 准确率及其差值。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按以下顺序排列：\n  $[S_{\\text{A}}, S_{\\text{B}}, S_{\\text{C}}, \\text{acc}_{\\text{base}}, \\text{acc}_{\\text{fact}}, \\Delta \\text{acc}]$,\n  其中 $\\Delta \\text{acc} = \\text{acc}_{\\text{fact}} - \\text{acc}_{\\text{base}}$。\n- 所有六个数字都必须四舍五入到六位小数。", "solution": "该问题要求分析将方形卷积分解为两个可分离的矩形卷积（一种在 GoogLeNet (Inception) 架构中广泛使用的技术）所带来的计算节省和性能影响。我将首先验证问题陈述。\n\n### 问题验证\n\n**第 1 步：提取已知信息**\n- 原始卷积的 FLOPs：$F_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2$\n- 分解卷积的 FLOPs：$F_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right)$\n- FLOPs 节省率：$S \\;=\\; \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}}$\n- Top-1 准确率：$\\text{acc} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{ \\hat{y}_i = y_i \\}$\n- 任务 1 条件：$C_{\\text{mid}} = C_{\\text{out}} = C_{\\text{in}}$, $K = 7$\n- 测试 A 参数：$H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$\n- 测试 B 参数：$H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$\n- 测试 C 参数：$H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$\n- 任务 3 数据：$N = 12$, $C = 100$\n    - $y$: $[5,12,23,23,11,65,99,0,3,42,77,13]$\n    - $\\hat{y}^{\\text{base}}$: $[5,11,23,3,11,65,13,0,2,42,8,13]$\n    - $\\hat{y}^{\\text{fact}}$: $[5,12,23,23,9,65,99,0,3,1,77,88]$\n\n**第 2 步：使用提取的已知信息进行验证**\n该问题在科学上基于深度学习原理以及神经网络运算的计算复杂度分析。所有定义和公式均为该领域的标准。问题定义明确，提供了所有必要的数据和约束，可以为每个任务得出唯一的数值解。语言客观、正式。给定的参数对于卷积神经网络架构是切合实际的。该问题不违反任何无效性标准。\n\n**第 3 步：结论与行动**\n问题有效。将提供完整的解决方案。\n\n### 解题推导\n\n按顺序解决三个任务，过程如下。\n\n**任务 1：代数证明与节省率**\n\n题目要求我们证明，在 $C_{\\text{in}} = C_{\\text{mid}} = C_{\\text{out}} = C$ 且 $K=7$ 的特定情况下，分解卷积使用的 FLOPs 是原始卷积的 $2/7$，并求出节省率 $S$。\n\n设 $C_{\\text{in}} = C_{\\text{mid}} = C_{\\text{out}} = C$。FLOPs 表达式变为：\n$$\nF_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C \\cdot C \\cdot K^2 \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2 \\cdot K^2\n$$\n$$\nF_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C \\cdot C \\cdot K \\;+\\; C \\cdot C \\cdot K\\right) \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(2 \\cdot C^2 \\cdot K\\right)\n$$\n为了求出分解后的 FLOPs 与原始 FLOPs 的比率，我们计算 $\\frac{F_{\\text{fact}}}{F_{\\text{orig}}}$：\n$$\n\\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; \\frac{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot 2 \\cdot C^2 \\cdot K}{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2 \\cdot K^2} \\;=\\; \\frac{2K}{K^2} \\;=\\; \\frac{2}{K}\n$$\n公因式 $2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2$ 可以消掉，表明该比率与空间维度和通道数无关（只要它们相等）。\n对于卷积核大小 $K=7$，该比率为：\n$$\n\\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; \\frac{2}{7}\n$$\n这证明了在这些条件下，分解后的卷积对使用的 FLOPs 恰好是原始 $7 \\times 7$ 卷积的 $2/7$。\n\n节省率 $S$ 定义为 $S = \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}} = 1 - \\frac{F_{\\text{fact}}}{F_{\\text{orig}}}$。\n代入推导出的比率：\n$$\nS \\;=\\; 1 - \\frac{2}{K}\n$$\n对于 $K=7$，节省率为：\n$$\nS \\;=\\; 1 - \\frac{2}{7} \\;=\\; \\frac{5}{7}\n$$\n精确的节省率为 $5/7$。\n\n**任务 2：测试用例的 FLOPs 节省率**\n\n为了计算给定测试用例的节省率，我们首先推导 $S$ 的通用简化表达式。\n$$\nS \\;=\\; 1 - \\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; 1 - \\frac{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right)}{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2}\n$$\n项 $2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}}$ 可以消掉。我们还可以从分子中提取公因式 $C_{\\text{mid}} \\cdot K$：\n$$\nS \\;=\\; 1 - \\frac{C_{\\text{mid}} \\cdot K \\cdot (C_{\\text{in}} + C_{\\text{out}})}{C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2} \\;=\\; 1 - \\frac{C_{\\text{mid}} (C_{\\text{in}} + C_{\\text{out}})}{K \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}}\n$$\n使用这个简化公式，我们计算每个测试用例的 $S$。\n\n- **测试 A：** $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$。\n该情况符合任务 1 的条件。\n$$\nS_{\\text{A}} \\;=\\; 1 - \\frac{64 \\cdot (64 + 64)}{7 \\cdot 64 \\cdot 64} \\;=\\; 1 - \\frac{64 \\cdot 128}{7 \\cdot 4096} \\;=\\; 1 - \\frac{128}{7 \\cdot 64} \\;=\\; 1 - \\frac{2}{7} \\;=\\; \\frac{5}{7} \\approx 0.714286\n$$\n\n- **测试 B：** $H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$。\n这个边界情况涉及一个 $1 \\times 1$ 的卷积核，在这种情况下分解通常没有好处。\n$$\nS_{\\text{B}} \\;=\\; 1 - \\frac{32 \\cdot (32 + 32)}{1 \\cdot 32 \\cdot 32} \\;=\\; 1 - \\frac{32 \\cdot 64}{1024} \\;=\\; 1 - \\frac{2048}{1024} \\;=\\; 1 - 2 \\;=\\; -1\n$$\n节省率为 $-1$ 表示 FLOPs 翻了一倍，这符合预期，因为用两个连续的 $1 \\times 1$ 卷积替换一个 $1 \\times 1$ 卷积就是如此。\n\n- **测试 C：** $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$。\n这代表了一种瓶颈设计，其中 $C_{\\text{mid}}$ 小于 $C_{\\text{out}}$。\n$$\nS_{\\text{C}} \\;=\\; 1 - \\frac{64 \\cdot (64 + 128)}{7 \\cdot 64 \\cdot 128} \\;=\\; 1 - \\frac{64 \\cdot 192}{7 \\cdot 8192} \\;=\\; 1 - \\frac{192}{7 \\cdot 128} \\;=\\; 1 - \\frac{192}{896}\n$$\n通过将分子和分母除以它们的最大公约数 $64$ 来简化分数 $\\frac{192}{896}$：$\\frac{192 \\div 64}{896 \\div 64} = \\frac{3}{14}$。\n$$\nS_{\\text{C}} \\;=\\; 1 - \\frac{3}{14} \\;=\\; \\frac{11}{14} \\approx 0.785714\n$$\n\n**任务 3：Top-1 准确率计算**\n\n我们在包含 $N=12$ 个样本的抽象数据集上，计算基线模型和分解模型的 top-1 准确率。准确率是正确预测的数量除以样本总数。\n\n- **真实标签 $y$**：$[5,12,23,23,11,65,99,0,3,42,77,13]$\n\n- **基线模型预测类别 $\\hat{y}^{\\text{base}}$**：$[5,11,23,3,11,65,13,0,2,42,8,13]$\n我们逐元素比较 $y$ 和 $\\hat{y}^{\\text{base}}$ 以查找匹配项：\n- 索引 0：$5 = 5$ (匹配)\n- 索引 1：$12 \\neq 11$\n- 索引 2：$23 = 23$ (匹配)\n- 索引 3：$23 \\neq 3$\n- 索引 4：$11 = 11$ (匹配)\n- 索引 5：$65 = 65$ (匹配)\n- 索引 6：$99 \\neq 13$\n- 索引 7：$0 = 0$ (匹配)\n- 索引 8：$3 \\neq 2$\n- 索引 9：$42 = 42$ (匹配)\n- 索引 10：$77 \\neq 8$\n- 索引 11：$13 = 13$ (匹配)\n共有 $7$ 个正确预测。\n$$\n\\text{acc}_{\\text{base}} \\;=\\; \\frac{7}{12} \\approx 0.583333\n$$\n\n- **分解模型预测类别 $\\hat{y}^{\\text{fact}}$**：$[5,12,23,23,9,65,99,0,3,1,77,88]$\n我们逐元素比较 $y$ 和 $\\hat{y}^{\\text{fact}}$ 以查找匹配项：\n- 索引 0：$5 = 5$ (匹配)\n- 索引 1：$12 = 12$ (匹配)\n- 索引 2：$23 = 23$ (匹配)\n- 索引 3：$23 = 23$ (匹配)\n- 索引 4：$11 \\neq 9$\n- 索引 5：$65 = 65$ (匹配)\n- 索引 6：$99 = 99$ (匹配)\n- 索引 7：$0 = 0$ (匹配)\n- 索引 8：$3 = 3$ (匹配)\n- 索引 9：$42 \\neq 1$\n- 索引 10：$77 = 77$ (匹配)\n- 索引 11：$13 \\neq 88$\n共有 $9$ 个正确预测。\n$$\n\\text{acc}_{\\text{fact}} \\;=\\; \\frac{9}{12} \\;=\\; \\frac{3}{4} \\;=\\; 0.75\n$$\n\n- **准确率差异 $\\Delta \\text{acc}$**：\n$$\n\\Delta \\text{acc} \\;=\\; \\text{acc}_{\\text{fact}} - \\text{acc}_{\\text{base}} \\;=\\; \\frac{9}{12} - \\frac{7}{12} \\;=\\; \\frac{2}{12} \\;=\\; \\frac{1}{6} \\approx 0.166667\n$$\n结果将在程序中编译成所需的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three tasks defined in the problem:\n    1. Computes FLOP savings fractions S for three test cases.\n    2. Computes top-1 accuracies for baseline and factorized models.\n    3. Computes the difference in accuracies.\n    \"\"\"\n\n    # --- Task 2: Compute FLOP Savings Fraction S ---\n\n    def calculate_savings_fraction(C_in, C_out, C_mid, K):\n        \"\"\"\n        Computes the FLOP savings fraction S using the derived formula:\n        S = 1 - (C_mid * (C_in + C_out)) / (K * C_in * C_out)\n        \n        A kernel size K=0 would lead to division by zero, but is physically\n        meaningless and not present in the test cases.\n        \"\"\"\n        if K == 0 or C_in == 0 or C_out == 0:\n            # Handle edge cases to prevent division by zero, though not strictly\n            # needed for the given problem data. An invalid configuration would\n            # result in undefined savings.\n            return np.nan\n\n        # Simplified formula derived in the solution text\n        numerator = C_mid * (C_in + C_out)\n        denominator = K * C_in * C_out\n        \n        # When K=1, the original convolution cost formula for F_orig can be zero if K=0.\n        # However, the problem states KxK conv, so K=1. For K=1, F_orig has K^2=1.\n        # F_fact has K=1. The formula is arithmetically sound.\n        \n        ratio = numerator / denominator\n        S = 1 - ratio\n        return S\n\n    # Test Case A\n    params_A = {'C_in': 64, 'C_out': 64, 'C_mid': 64, 'K': 7}\n    S_A = calculate_savings_fraction(**params_A)\n\n    # Test Case B\n    params_B = {'C_in': 32, 'C_out': 32, 'C_mid': 32, 'K': 1}\n    S_B = calculate_savings_fraction(**params_B)\n\n    # Test Case C\n    params_C = {'C_in': 64, 'C_out': 128, 'C_mid': 64, 'K': 7}\n    S_C = calculate_savings_fraction(**params_C)\n\n    # --- Task 3: Compute Top-1 Accuracies ---\n\n    # Provided data for the abstract ImageNet-100 subset\n    y_true = np.array([5, 12, 23, 23, 11, 65, 99, 0, 3, 42, 77, 13])\n    y_pred_base = np.array([5, 11, 23, 3, 11, 65, 13, 0, 2, 42, 8, 13])\n    y_pred_fact = np.array([5, 12, 23, 23, 9, 65, 99, 0, 3, 1, 77, 88])\n\n    # Top-1 accuracy is the mean of correct predictions\n    acc_base = np.mean(y_true == y_pred_base)\n    acc_fact = np.mean(y_true == y_pred_fact)\n\n    # Difference in accuracies\n    delta_acc = acc_fact - acc_base\n\n    # --- Final Output Formatting ---\n\n    results = [S_A, S_B, S_C, acc_base, acc_fact, delta_acc]\n    \n    # Format each result to six decimal places and join into the required string format\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n\n    print(output_string)\n\n# This block ensures the solve function is called when the script is executed.\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3130734"}, {"introduction": "为了有效训练像GoogLeNet这样的深度网络，批量归一化（Batch Normalization, BN）对于稳定学习过程至关重要。在多分支结构中，一个关键的设计决策是BN层的位置。本练习将挑战您分析在拼接不同分支的输出之前与之后应用BN所产生的统计效应，从而帮助您理解其中的细微权衡以及Inception-v2设计背后的原理。[@problem_id:3130685]", "problem": "在一个 GoogLeNet (Inception) 模块中，多个分支生成特征图，这些特征图在送入下一个卷积层之前会沿着通道维度进行拼接。考虑一个 Inception v2 风格的块中的两个分支，分别标记为 $A$ 和 $B$。每个分支输出一组通道，其激活前统计量（在批次和空间位置上）可以被建模为每个通道上独立的、服从高斯分布的随机变量，每个通道的均值为 $\\mu$，方差为 $\\sigma^{2}$。拼接操作通过堆叠来自 A 分支和 B 分支的所有通道，形成一个单一的张量。\n\n批量归一化 (BN) 遵循标准定义：对于一个随机变量为 $X$ 的输入通道，BN 变换为 $Y = \\gamma \\frac{X - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma}^{2} + \\varepsilon}} + \\beta$，其中 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^{2}$ 是批次均值和批次方差的估计值，$\\gamma$ 和 $\\beta$ 是可学习的缩放和平移参数，$\\varepsilon$ 是一个用于保持数值稳定性的很小的常数。在初始化时，我们取 $\\gamma = 1, \\beta = 0$，并假设 $\\varepsilon$ 可以忽略不计，从而 BN 理想地将每个通道归一化为零均值和单位方差。\n\n将拼接边界处的分支级别错配度量定义为\n$$\nD = \\left(m_{A} - m_{B}\\right)^{2} + \\left(v_{A} - v_{B}\\right)^{2},\n$$\n其中 $m_{A}$ 和 $m_{B}$ 分别是 A 分支和 B 分支内各通道均值的平均值，而 $v_{A}$ 和 $v_{B}$ 分别是 A 分支和 B 分支内各通道方差的平均值。这个 $D$ 量化了在拼接操作之前，分支间的均值/方差错配程度。\n\n您将比较两种 BN 放置方式：\n- BN-before：在拼接前，对每个分支内的每个通道独立应用批量归一化。\n- BN-after：在分支中省略 BN，而在拼接后应用单个批量归一化层（根据上述 BN 定义，该层独立地归一化每个拼接后的通道）。\n\n假设 $A$ 分支有 3 个通道，其 $(\\mu,\\sigma^{2})$ 值为\n$$\n(\\mu_{A,1},\\sigma^{2}_{A,1}) = (0.5,\\,1.6),\\quad (\\mu_{A,2},\\sigma^{2}_{A,2}) = (-0.2,\\,0.9),\\quad (\\mu_{A,3},\\sigma^{2}_{A,3}) = (0.3,\\,1.1),\n$$\n假设 $B$ 分支有 2 个通道，其值为\n$$\n(\\mu_{B,1},\\sigma^{2}_{B,1}) = (-0.4,\\,0.8),\\quad (\\mu_{B,2},\\sigma^{2}_{B,2}) = (0.1,\\,1.4).\n$$\n\n从上面提供的拼接、期望、方差和批量归一化的定义出发（并且除了这些基本原理外不使用任何快捷公式），推导在拼接前应用 BN 时的错配度量 $D_{\\text{before}}$ 和在拼接后应用 BN 时的错配度量 $D_{\\text{after}}$。然后计算差值\n$$\n\\Delta D = D_{\\text{after}} - D_{\\text{before}}.\n$$\n将 $\\Delta D$ 作为单个实数报告。除了精确算术外，不需要进行任何舍入，也不涉及单位。确保您的推导能够解释每种 BN 放置方式是如何影响在 $D$ 中所使用的 $m_{A}$、$m_{B}$、$v_{A}$ 和 $v_{B}$ 的。", "solution": "用户提供的问题已经过验证，并被确定为深度学习领域中的一个有效的、适定的问题。它具有科学依据、自成体系且客观。我们现在可以开始解答。\n\n目标是计算两种批量归一化 (BN) 放置策略之间的分支级别错配度量之差，即 $\\Delta D = D_{\\text{after}} - D_{\\text{before}}$。错配度量定义为 $D = (m_{A} - m_{B})^{2} + (v_{A} - v_{B})^{2}$，其中 $m$ 和 $v$ 分别表示在拼接操作之前，给定分支内各通道均值和方差的平均值。\n\n首先，我们确定 A 分支和 B 分支的初始统计数据。\nA 分支有 $N_A = 3$ 个通道，其均值和方差 $(\\mu, \\sigma^2)$ 对如下：\n$(\\mu_{A,1}, \\sigma^{2}_{A,1}) = (0.5, 1.6)$\n$(\\mu_{A,2}, \\sigma^{2}_{A,2}) = (-0.2, 0.9)$\n$(\\mu_{A,3}, \\sigma^{2}_{A,3}) = (0.3, 1.1)$\n\nB 分支有 $N_B = 2$ 个通道，其统计数据如下：\n$(\\mu_{B,1}, \\sigma^{2}_{B,1}) = (-0.4, 0.8)$\n$(\\mu_{B,2}, \\sigma^{2}_{B,2}) = (0.1, 1.4)$\n\n问题的核心在于确定在 `BN-before` 和 `BN-after` 这两种情况下 $m_A, m_B, v_A, v_B$ 的值。度量 $D$ 是基于特征图在它们被拼接*之前*的统计数据计算的。\n\n**情况1：`BN-before` 放置**\n在这种情况下，批量归一化在拼接发生*之前*独立地应用于每个分支内的每个通道。问题指出，在给定的理想条件下（$\\gamma=1, \\beta=0, \\varepsilon \\approx 0$），BN “理想地将每个通道归一化为零均值和单位方差”。\n\n令 $\\mu'_{A,i}$ 和 $\\sigma'^2_{A,i}$ 为 $A$ 分支中通道 $i$ 归一化后的均值和方差。根据问题对理想 BN 的定义，对于 $A$ 分支中的每个通道 $i$：\n$$\n\\mu'_{A,i} = 0 \\\\\n\\sigma'^2_{A,i} = 1\n$$\n同样地，对于 $B$ 分支中的每个通道 $j$，其 BN 后的统计数据为：\n$$\n\\mu'_{B,j} = 0 \\\\\n\\sigma'^2_{B,j} = 1\n$$\n这些是通道在拼接点的统计数据。我们现在计算每个分支的平均统计数据以求得 $D_{\\text{before}}$。\n\n$A$ 分支的各通道均值的平均值，记为 $m_{A, \\text{before}}$，是：\n$$\nm_{A, \\text{before}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\mu'_{A,i} = \\frac{1}{3} (0 + 0 + 0) = 0\n$$\n$A$ 分支的各通道方差的平均值，记为 $v_{A, \\text{before}}$，是：\n$$\nv_{A, \\text{before}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\sigma'^2_{A,i} = \\frac{1}{3} (1 + 1 + 1) = 1\n$$\n同样，对于 $B$ 分支：\n各通道均值的平均值，$m_{B, \\text{before}}$：\n$$\nm_{B, \\text{before}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\mu'_{B,j} = \\frac{1}{2} (0 + 0) = 0\n$$\n各通道方差的平均值，$v_{B, \\text{before}}$：\n$$\nv_{B, \\text{before}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\sigma'^2_{B,j} = \\frac{1}{2} (1 + 1) = 1\n$$\n现在我们可以计算错配度量 $D_{\\text{before}}$：\n$$\nD_{\\text{before}} = (m_{A, \\text{before}} - m_{B, \\text{before}})^{2} + (v_{A, \\text{before}} - v_{B, \\text{before}})^{2} \\\\\nD_{\\text{before}} = (0 - 0)^{2} + (1 - 1)^{2} = 0^2 + 0^2 = 0\n$$\n\n**情况2：`BN-after` 放置**\n在这种情况下，分支内部省略了 BN。先进行拼接，然后将单个 BN 层应用于拼接后的张量。错配度量 $D$ 是根据拼接*前*的统计数据来评估的。因此，我们必须使用通道的原始、未归一化的统计数据。\n\n$A$ 分支的各通道均值的平均值，记为 $m_{A, \\text{after}}$，是：\n$$\nm_{A, \\text{after}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\mu_{A,i} = \\frac{1}{3} (0.5 + (-0.2) + 0.3) = \\frac{1}{3}(0.6) = 0.2\n$$\n$A$ 分支的各通道方差的平均值，记为 $v_{A, \\text{after}}$，是：\n$$\nv_{A, \\text{after}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\sigma^{2}_{A,i} = \\frac{1}{3} (1.6 + 0.9 + 1.1) = \\frac{1}{3}(3.6) = 1.2\n$$\n对于 $B$ 分支，相应的平均值为：\n各通道均值的平均值，$m_{B, \\text{after}}$：\n$$\nm_{B, \\text{after}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\mu_{B,j} = \\frac{1}{2} (-0.4 + 0.1) = \\frac{1}{2}(-0.3) = -0.15\n$$\n各通道方差的平均值，$v_{B, \\text{after}}$：\n$$\nv_{B, \\text{after}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\sigma^{2}_{B,j} = \\frac{1}{2} (0.8 + 1.4) = \\frac{1}{2}(2.2) = 1.1\n$$\n现在我们使用这些值计算错配度量 $D_{\\text{after}}$：\n$$\nD_{\\text{after}} = (m_{A, \\text{after}} - m_{B, \\text{after}})^{2} + (v_{A, \\text{after}} - v_{B, \\text{after}})^{2} \\\\\nD_{\\text{after}} = (0.2 - (-0.15))^{2} + (1.2 - 1.1)^{2} \\\\\nD_{\\text{after}} = (0.35)^{2} + (0.1)^{2} \\\\\nD_{\\text{after}} = 0.1225 + 0.01 = 0.1325\n$$\n\n**最终计算：差值 $\\Delta D$**\n最后，我们计算差值 $\\Delta D = D_{\\text{after}} - D_{\\text{before}}$。\n$$\n\\Delta D = 0.1325 - 0 = 0.1325\n$$\n这个结果量化了当从 `BN-before` 设计迁移到 `BN-after` 设计时，在拼接边界处统计错配的增加程度。`BN-before` 策略通过预归一化每个分支来完全消除错配，而 `BN-after` 策略则允许分支之间原始的统计差异持续存在直到拼接点。", "answer": "$$\\boxed{0.1325}$$", "id": "3130685"}]}