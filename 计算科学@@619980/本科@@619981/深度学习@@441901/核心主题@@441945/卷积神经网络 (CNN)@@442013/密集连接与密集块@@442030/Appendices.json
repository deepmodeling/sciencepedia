{"hands_on_practices": [{"introduction": "为了理解一个深度网络如何识别复杂的模式，我们必须首先理解其感受野——即输入图像中能够影响单个神经元输出的区域。本练习将通过理论推导，深入分析密集连接这一独特的结构如何影响感受野的增长。通过将其与另一种著名架构 ResNet 进行比较，你将对现代卷积神经网络中的特征传播建立更深刻的直觉。[@problem_id:3114064]", "problem": "考虑一个卷积神经网络 (CNN) 中的密集块（dense block），如密集连接卷积网络 (DenseNet) 中所使用的。该块由 $L$ 个顺序连接的二维卷积层组成，每个卷积层使用 $3 \\times 3$ 的卷积核，步幅为 $1$，四周各填充 $1$ 个零（因此空间分辨率得以保持），无池化，无膨胀，并带有不改变空间依赖性的逐点非线性操作。每个层都将原始块输入与块内所有先前层的输出在通道维度上进行级联，并将结果作为其输入。块的输出是所有层输出的通道级联（忽略块之后的任何压缩层或过渡层）。对于任何层的输出，我们将其感受野边长定义为：能影响该层单个输出像素的、沿着原始块输入的单个空间轴的输入像素数量。我们将块感受野边长定义为其所有输出通道中感受野边长的最大值。\n\n从离散卷积和感受野传播的定义出发，推导该密集块末端的块感受野边长关于 $L$ 的闭式表达式。然后，考虑一个“等效深度”的残差堆叠，它由 $L$ 个顺序连接的 $3 \\times 3$ 步幅为 $1$ 的卷积层组成，同样带有 $1$ 的零填充，无池化，并包含排列成标准残差路径且不改变空间维度的恒等跳跃连接。使用相同的第一性原理推断，推导该残差堆叠末端的块感受野边长，并将其与密集块的感受野边长进行比较。\n\n只需报告块输出（沿单一空间轴测量）的感受野边长关于 $L$ 的那个唯一的、共同的闭式表达式。您的最终答案必须是单一的解析表达式。无需四舍五入。", "solution": "问题要求为由 $L$ 层组成的密集块和残差堆叠，给出一个统一的感受野边长闭式表达式。我们将基于第一性原理，通过分析每种架构中的感受野传播来推导这个表达式。\n\n当对一个输入特征图应用卷积层时，感受野边长 $RF$ 的基本递推关系如下：\n$$RF_{out} = RF_{in} + (k - 1) \\times S_{in}$$\n其中，$RF_{in}$ 是输入特征图相对于原始网络输入的感受野边长，$k$ 是当前层的卷积核边长，$S_{in}$ 是从原始网络输入到当前层输入之间所有先前层的步幅之积。原始输入中单个像素的感受野定义为 $RF_{original} = 1$。\n\n在本问题中，对于每个卷积层，卷积核大小为 $k=3$，步幅为 $s=1$。由于所有步幅都为 $1$，因此步幅之积 $S_{in}$ 恒为 $1$。因此，单步卷积的传播公式简化为：\n$$RF_{out} = RF_{in} + (3 - 1) \\times 1 = RF_{in} + 2$$\n该公式表明，每个步幅为 $1$ 的 $3 \\times 3$ 卷积会使感受野边长增加 $2$。\n\n现在我们来分析每种架构。我们将块的原始输入表示为 $x_0$。\n\n**1. 密集块（DenseNet）分析**\n\n在密集块中，第 $l$ 层（其中 $l \\in \\{1, 2, \\dots, L\\}$）的输入是原始块输入 $x_0$ 与所有先前层输出 $y_1, y_2, \\dots, y_{l-1}$ 的通道级联。第 $l$ 层的输出为 $y_l$。\n令 $RF(Z)$ 表示张量 $Z$ 相对于原始输入 $x_0$ 的感受野边长。我们定义 $RF_l \\equiv RF(y_l)$。$y_l$ 中的单个输出像素受到其复合输入 $\\text{concat}(x_0, y_1, \\dots, y_{l-1})$ 的一个 $3 \\times 3$ 区域的影响。总感受野 $RF_l$ 是来自所有输入路径的感受野的并集。该并集的大小由产生最大感受野的路径决定。\n\n对于第 $l=1$ 层：\n输入为 $x_0$。输入像素的感受野为 $RF(x_0) = 1$。\n$$RF_1 = RF(x_0) + (3-1) = 1 + 2 = 3$$\n\n对于第 $l=2$ 层：\n输入为 $\\text{concat}(x_0, y_1)$。$y_2$ 中的一个像素受到该输入的一个 $3 \\times 3$ 区域的影响。\n- 从 $x_0$ 经过第 2 层卷积的路径产生的感受野大小为 $k=3$。\n- 从 $y_1$ 经过第 2 层卷积的路径产生的感受野大小为 $RF_1 + (3-1) = 3 + 2 = 5$。\n$y_2$ 的感受野是这些值中的最大值：\n$$RF_2 = \\max(3, 5) = 5$$\n\n对于第 $l=3$ 层：\n输入为 $\\text{concat}(x_0, y_1, y_2)$。感受野 $RF_3$ 是来自所有路径的感受野中的最大值：\n- 经由 $x_0$ 的路径：$RF=3$。\n- 经由 $y_1$ 的路径：$RF = RF_1 + 2 = 3 + 2 = 5$。\n- 经由 $y_2$ 的路径：$RF = RF_2 + 2 = 5 + 2 = 7$。\n$$RF_3 = \\max(3, 5, 7) = 7$$\n\n通过归纳法，对于任意第 $l$ 层，其感受野为：\n$$RF_l = \\max(RF(\\text{经由 } x_0 \\text{ 的路径}), RF(\\text{经由 } y_1 \\text{ 的路径}), \\dots, RF(\\text{经由 } y_{l-1} \\text{ 的路径}))$$\n$$RF_l = \\max(3, RF_1+2, RF_2+2, \\dots, RF_{l-1}+2)$$\n由于当 $j  l$ 时，$RF_j = 2j+1$，序列 $RF_j+2 = 2j+3$ 随 $j$ 严格递增。最大值由通过最近一层 $y_{l-1}$ 的路径决定。\n$$RF_l = RF_{l-1} + 2$$\n这是一个简单的等差数列。以 $RF_1 = 3$ 为基本情况，第 $l$ 层感受野的闭式解为：\n$$RF_l = RF_1 + (l-1) \\times 2 = 3 + 2l - 2 = 2l + 1$$\n\n块的输出是所有层输出的级联：$[y_1, y_2, \\dots, y_L]$。“块感受野边长”被定义为其所有输出通道中感受野边长的最大值，即 $\\max(RF_1, RF_2, \\dots, RF_L)$。由于 $RF_l = 2l+1$ 是关于 $l$ 的严格递增函数，最大值在 $l=L$ 时取得。\n$$RF_{\\text{密集块}} = RF_L = 2L + 1$$\n\n**2. 残差堆叠（ResNet）分析**\n\n在一个标准的残差堆叠中，第 $l$ 个块的输出（我们表示为 $z_l$）是该块的输入 $z_{l-1}$ 与输入经过卷积变换后的输出 $\\text{Conv}_l(z_{l-1})$ 之和。（问题陈述中提到，逐点非线性不改变空间依赖性，因此它们不影响感受野的计算）。\n$$z_l = \\text{Conv}_l(z_{l-1}) + z_{l-1}$$\n令 $RF_l'$ 为 $z_l$ 相对于初始输入 $z_0 = x_0$ 的感受野边长。初始输入的感受野为 $RF_0' = 1$。\n\n对于第 $l$ 层，$z_l$ 中的一个输出像素受到源自 $z_{l-1}$ 的两条路径的影响：\n- 卷积路径：$\\text{Conv}_l(z_{l-1})$。该路径贡献的感受野为 $RF_{l-1}' + (3-1) = RF_{l-1}' + 2$。\n- 恒等跳跃连接路径：$z_{l-1}$。该路径贡献的感受野即为 $RF_{l-1}'$。\n\n$z_l$ 的总感受野是这两条路径感受野的并集。并集的大小是各独立感受野大小的最大值。\n$$RF_l' = \\max(RF_{l-1}' + 2, RF_{l-1}') = RF_{l-1}' + 2$$\n这建立了一个与简单卷积层堆叠相同的递推关系。我们得到一个从 $RF_0' = 1$ 开始的等差数列。经过 $L$ 层后，感受野为：\n$$RF_L' = RF_0' + \\sum_{i=1}^{L} 2 = 1 + 2L$$\n\n残差堆叠的最终输出是 $z_L$，所以该块的感受野是 $RF_L'$。\n$$RF_{\\text{残差堆叠}} = 2L + 1$$\n\n**结论**\n\n在指定条件下（$3 \\times 3$ 卷积核，步幅为 $1$），密集块和残差堆叠的感受野大小都表现出相同的线性增长。在这两种架构中，感受野扩张的主导路径都是顺序的卷积链。由密集连接或残差连接创建的较短路径具有较小的感受野，这些感受野被包含在最长路径的感受野之内。\n\n对于这两种架构，块感受野边长的唯一的、共同的闭式表达式都是 $2L+1$。", "answer": "$$\\boxed{2L+1}$$", "id": "3114064"}, {"introduction": "理论分析之后，我们必须面对现实世界中的计算约束。由于每个新层都会拼接之前所有层的特征图，密集块的计算量可能变得非常庞大，因此优化效率至关重要。在这个练习中，你将扮演一名深度学习工程师，通过精确计算浮点运算次数（FLOPs），来量化在瓶颈层中使用深度可分离卷积替代标准卷积所带来的计算节省，这是一项设计高效模型的关键技能。[@problem_id:3113990]", "problem": "密集卷积网络（DenseNet）的密集块中的单个层使用一个包含两个阶段的瓶颈设计：一个 $1 \\times 1$ 卷积，后跟一个空间卷积。增长率为 $k$，意味着该层产生 $k$ 个新的特征图，瓶颈扩展因子为 $b$，意味着经过 $1 \\times 1$ 卷积后的中间通道数为 $b k$。考虑空间阶段的两种设计选择：(i) 一个标准的 $3 \\times 3$ 卷积，从 $b k$ 个输入通道到 $k$ 个输出通道；以及 (ii) 一个深度可分离卷积（DSC），它由一个逐通道应用于 $b k$ 个通道的 $3 \\times 3$ 深度卷积阶段，和一个将 $b k$ 个通道映射到 $k$ 个通道的 $1 \\times 1$ 逐点卷积阶段组成。假设步长为 $1$，且输出特征图的空间维度为 $H \\times W$。\n\n使用离散卷积作为乘积之和的基本定义，以及深度学习中普遍接受的惯例，即每次乘法和每次加法都算作一次浮点运算（FLOPs），并忽略偏置加法、激活函数和边界效应，推导出在该瓶颈层中用深度可分离卷积替换标准 $3 \\times 3$ 卷积所实现的总 FLOP 节省量 $S(k,b,H,W)$ 的精确解析表达式。将 $S(k,b,H,W)$ 仅用 $k$、$b$、$H$ 和 $W$ 表示，并尽可能简化您的最终表达式。最终答案必须是单一的封闭形式解析表达式。", "solution": "问题要求推导在一个 DenseNet 瓶颈层内，用一个 $3 \\times 3$ 深度可分离卷积（DSC）替换一个标准的 $3 \\times 3$ 空间卷积时，所节省的总浮点运算（FLOP）量 $S(k,b,H,W)$ 的解析表达式。节省量是标准卷积所需的 FLOPs 与 DSC 所需的 FLOPs 之间的差值。\n\n首先，我们建立计算 FLOPs 的公式。卷积操作本质上是一系列乘加（MAC）操作。根据问题所述的惯例，每次乘法和每次加法都算作一次 FLOP。因此，一个计算乘积之和的 MAC 操作包含 $2$ 个 FLOPs。对于一个标准的卷积层，总 FLOPs 数由 MACs 的数量乘以 $2$ 得到。\n使用 $C_{\\text{out}}$ 个大小为 $K_H \\times K_W$ 的卷积核，从一个具有 $C_{\\text{in}}$ 通道的输入生成一个大小为 $H_{\\text{out}} \\times W_{\\text{out}}$ 的输出特征图所需的 MACs 数量为 $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W$。\n因此，忽略偏置的情况下，FLOPs 的通用公式为：\n$$\n\\text{FLOPs} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W\n$$\n问题规定步长为 $1$，输出空间维度为 $H \\times W$，因此我们设 $H_{\\text{out}} = H$ 和 $W_{\\text{out}} = W$。瓶颈层空间阶段的输入有 $b k$ 个通道。\n\n情况 (i)：标准 $3 \\times 3$ 卷积\n对于这种情况，参数如下：\n- 输入通道数，$C_{\\text{in}} = b k$\n- 输出通道数，$C_{\\text{out}} = k$\n- 卷积核高度，$K_H = 3$\n- 卷积核宽度，$K_W = 3$\n- 输出空间维度：$H \\times W$\n\n使用通用公式，标准卷积的总 FLOPs，$\\text{FLOPs}_{\\text{standard}}$，为：\n$$\n\\text{FLOPs}_{\\text{standard}} = 2 \\times H \\times W \\times k \\times (b k) \\times 3 \\times 3\n$$\n$$\n\\text{FLOPs}_{\\text{standard}} = 18 b H W k^2\n$$\n\n情况 (ii)：深度可分离卷积（DSC）\nDSC 由两个连续的阶段组成：一个深度卷积，后跟一个逐点卷积。我们计算每个阶段的 FLOPs 并将它们相加。\n\n阶段 $1$：$3 \\times 3$ 深度卷积\n深度卷积为每个输入通道独立地应用一个空间滤波器。因此，输出通道的数量等于输入通道的数量。\n- 输入通道数，$C_{\\text{in}} = b k$\n- 卷积核高度，$K_H = 3$\n- 卷积核宽度，$K_W = 3$\n深度卷积的 FLOPs 计算公式为 $\\text{FLOPs} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W$。\n此阶段的 FLOPs，$\\text{FLOPs}_{\\text{DW}}$，为：\n$$\n\\text{FLOPs}_{\\text{DW}} = 2 \\times H \\times W \\times (b k) \\times 3 \\times 3\n$$\n$$\n\\text{FLOPs}_{\\text{DW}} = 18 b H W k\n$$\n此阶段的输出有 $b k$ 个通道，空间维度为 $H \\times W$。\n\n阶段 $2$：$1 \\times 1$ 逐点卷积\n这个阶段将深度卷积阶段的特征映射到最终的输出通道数。它是一个使用 $1 \\times 1$ 卷积核的标准卷积。\n- 输入通道数（来自阶段 $1$），$C_{\\text{in}} = b k$\n- 输出通道数，$C_{\\text{out}} = k$\n- 卷积核高度，$K_H = 1$\n- 卷积核宽度，$K_W = 1$\n此阶段的 FLOPs，$\\text{FLOPs}_{\\text{PW}}$，为：\n$$\n\\text{FLOPs}_{\\text{PW}} = 2 \\times H \\times W \\times k \\times (b k) \\times 1 \\times 1\n$$\n$$\n\\text{FLOPs}_{\\text{PW}} = 2 b H W k^2\n$$\n\nDSC 的总 FLOPs，$\\text{FLOPs}_{\\text{DSC}}$，是两个阶段 FLOPs 的总和：\n$$\n\\text{FLOPs}_{\\text{DSC}} = \\text{FLOPs}_{\\text{DW}} + \\text{FLOPs}_{\\text{PW}} = 18 b H W k + 2 b H W k^2\n$$\n\n最后，我们通过从标准卷积的 FLOPs 中减去 DSC 的 FLOPs 来计算 FLOP 节省量 $S(k,b,H,W)$：\n$$\nS(k,b,H,W) = \\text{FLOPs}_{\\text{standard}} - \\text{FLOPs}_{\\text{DSC}}\n$$\n$$\nS(k,b,H,W) = (18 b H W k^2) - (18 b H W k + 2 b H W k^2)\n$$\n$$\nS(k,b,H,W) = 18 b H W k^2 - 18 b H W k - 2 b H W k^2\n$$\n合并含有 $k^2$ 的项：\n$$\nS(k,b,H,W) = (18 - 2) b H W k^2 - 18 b H W k\n$$\n$$\nS(k,b,H,W) = 16 b H W k^2 - 18 b H W k\n$$\n为简化表达式，我们提取公因式 $2$、$b$、$H$、$W$ 和 $k$：\n$$\nS(k,b,H,W) = 2 b H W k (8k - 9)\n$$\n这就是 FLOP 节省量的最终简化解析表达式。", "answer": "$$\n\\boxed{2bHWk(8k - 9)}\n$$", "id": "3113990"}, {"introduction": "掌握了理论和效率分析之后，是时候将知识付诸实践了。本练习将指导你从零开始构建一个简化的密集连接网络，并在一项综合性的编程任务中比较不同激活函数（如 ReLU, GELU, SiLU）的效果。通过亲手实现并训练网络，你将亲眼观察到这些看似微小的组件选择如何实际影响模型的训练动态和最终性能，从而架起理论与实验结果之间的桥梁。[@problem_id:3113972]", "problem": "您将实现并比较仅在激活函数上有所不同的密集块神经网络，以研究在修正线性单元 (ReLU)、高斯误差线性单元 (GELU) 和 Sigmoid 线性单元 (SiLU) 之间的选择如何影响优化动态和最终准确率。您必须编写一个完整的程序，该程序构建一个综合的多类别分类任务，建立一个带有串联连接的简化密集块，使用全批量梯度下降进行训练，并为指定的测试套件报告量化指标。\n\n基本基础：\n- 密集块是一系列层的序列，其中每个新层接收所有先前特征图的串联作为输入。具体来说，对于输入向量 $x \\in \\mathbb{R}^{d_0}$、增长率 $k \\in \\mathbb{N}$ 和 $L \\in \\mathbb{N}$ 个层，第 $\\ell \\in \\{1,\\dots,L\\}$ 层计算 $z_\\ell = W_\\ell \\, \\mathrm{concat}(x, h_1, \\dots, h_{\\ell-1}) + b_\\ell$，然后 $h_\\ell = \\phi(z_\\ell)$，其中 $\\phi$ 是激活函数。最终表示为 $H = \\mathrm{concat}(x, h_1, \\dots, h_L) \\in \\mathbb{R}^{d_0 + kL}$，它被传递给一个线性分类器，为 $C$ 个类别产生 logits $o = H W_c + b_c \\in \\mathbb{R}^{C}$。\n- 对于多类别分类，使用标准的 softmax 和交叉熵：给定样本 $i$ 的 logits $o_i \\in \\mathbb{R}^{C}$ 和真实类别索引 $y_i \\in \\{0,\\dots,C-1\\}$，定义概率 $p_{i,c} = \\exp(o_{i,c}) / \\sum_{j=1}^{C} \\exp(o_{i,j})$ 和损失 $\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( - \\log p_{i, y_i} \\right)$（对于 $N$ 个样本）。\n- 通过全批量梯度下降进行训练，使用固定的学习率 $\\eta  0$ 和固定的周期数 $E \\in \\mathbb{N}$。损失关于参数的梯度通过微积分的链式法则计算。分类准确率是正确预测的比例 $\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}[\\arg\\max_c p_{i,c} = y_i]$。\n\n待比较的激活函数：\n- 修正线性单元 (ReLU): $\\phi(x) = \\max(0, x)$。\n- 高斯误差线性单元 (GELU)，使用广泛应用的 tanh 近似：$\\phi(x) \\approx \\frac{1}{2} x \\left( 1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} (x + 0.044715 x^3)\\right) \\right)$。\n- Sigmoid 线性单元 (SiLU)，也称为 Swish：$\\phi(x) = x \\cdot \\sigma(x)$ 其中 $\\sigma(x) = \\frac{1}{1 + e^{-x}}$。\n\n数据：\n- 通过从半径为 $r$ 的圆上均匀间隔角度的中心点处的各向同性高斯分布中为每个类别抽取 $N_c$ 个样本，在 $\\mathbb{R}^{2}$ 中合成一个 $C$ 类数据集。对于类别 $c \\in \\{0,\\dots,C-1\\}$，均值为 $\\mu_c = r [\\cos(2\\pi c / C), \\sin(2\\pi c / C)]$，每个样本为 $\\mu_c + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_2)$。使用 $C = 3$，$N_c = 100$，$r = 3$ 和 $\\sigma = 0.5$。总共有 $N = 300$ 个样本。\n\n训练：\n- 权重使用 He 风格初始化：对于一个扇入为 $f$ 的层，将其条目初始化为来自 $\\mathcal{N}(0, \\sqrt{2/f})$ 的独立样本。将偏置初始化为零。在所有可比较的情况下使用相同的随机种子，以隔离激活函数的影响。\n- 使用全批量梯度下降，不带动量或权重衰减。\n- 每次运行计算以下指标：\n  1. $E$ 个周期后的最终平均交叉熵损失，报告为四舍五入到四位小数的实数。\n  2. 最终分类准确率，报告为在 $[0,1]$ 范围内四舍五入到三位小数的实数。\n  3. 平均损失首次严格低于阈值 $\\tau$ 的周期索引（从1开始），如果这在 $E$ 个周期内从未发生，则为 $-1$。使用 $\\tau = 0.6$。\n\n测试套件：\n- 固定如上所述的数据集，并为所有测试重复使用。为保证可比性，在适用的情况下对权重初始化使用相同的随机种子。\n- 评估以下五种情况（每种情况是一个包含激活函数、层数 $L$、增长率 $k$、学习率 $\\eta$、周期数 $E$ 和初始化种子 $s$ 的元组）：\n  1. 情况 A（顺利路径）：激活函数 ReLU，$L = 3$，$k = 12$，$\\eta = 0.05$，$E = 200$，$s = 123$。\n  2. 情况 B（顺利路径）：激活函数 GELU，$L = 3$，$k = 12$，$\\eta = 0.05$，$E = 200$，$s = 123$。\n  3. 情况 C（顺利路径）：激活函数 SiLU，$L = 3$，$k = 12$，$\\eta = 0.05$，$E = 200$，$s = 123$。\n  4. 情况 D（连接深度边界条件）：激活函数 ReLU，$L = 1$，$k = 12$，$\\eta = 0.05$，$E = 200$，$s = 123$。\n  5. 情况 E（优化速度边缘案例）：激活函数 GELU，$L = 3$，$k = 12$，$\\eta = 0.005$，$E = 200$，$s = 123$。\n\n角度单位：\n- 三角函数中使用的所有角度都以弧度为单位。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每种情况，按 [final_loss, final_accuracy, first_epoch_index_or_-1] 的顺序输出一个包含三个值的列表。因此，总输出应为一个包含五个列表的列表，中间没有空格。例如：[[0.1234,0.987,42],[...],...]。", "solution": "用户提供的问题已经过严格验证，并被确定为一个有效且定义明确的科学问题。构建合成数据集、建立密集块神经网络并使用全批量梯度下降进行训练所需的所有必要参数和定义均已提供。该问题在科学上基于已建立的深度学习原理，是客观的，并具有唯一、可验证的解决方案。\n\n解决方案通过以基于原则的、分步的方式实现指定的组件来着手。\n\n### 1. 合成数据生成\n根据问题规范创建了一个合成数据集。它包含 $N=300$ 个样本，分布在 $d_0=2$ 维空间中，属于 $C=3$ 个类别，每个类别有 $N_c=100$ 个样本。每个类别 $c \\in \\{0, 1, 2\\}$ 的中心或均值 $\\mu_c$ 位于半径为 $r=3$ 的圆上。坐标由极坐标到笛卡尔坐标转换的原理给出：$\\mu_c = r [\\cos(2\\pi c / C), \\sin(2\\pi c / C)]$。每个数据点通过从标准差为 $\\sigma=0.5$ 的各向同性高斯分布 $\\mathcal{N}(\\mu_c, \\sigma^2 I_2)$ 中采样生成。数据生成使用单一、固定的随机种子，以确保所有模型都在完全相同的数据集上进行训练和评估，从而隔离架构和超参数变化的影响。\n\n### 2. 密集块网络架构\n模型的核心是一个密集块，实现为 $L$ 层的序列。密集块的定义原则是其连接模式：每个层都接收所有前面层的特征图与原始输入串联后的结果作为输入。\n\n- **初始化**：网络参数（权重和偏置）在训练前进行初始化。遵循 He 初始化原则，扇入为 $f$ 的层的权重从正态分布 $\\mathcal{N}(0, \\sigma_W^2)$ 中抽取，其中标准差为 $\\sigma_W = \\sqrt{2/f}$。这有助于缓解梯度消失/爆炸问题。所有偏置都初始化为零。在每个测试用例中都使用特定的随机种子 $s$ 进行初始化，以保证可复现性。\n- **前向传播**：前向传播计算网络对给定输入批次 $X$ 的输出 (logits)。对于每个层 $\\ell \\in \\{1, \\dots, L\\}$：\n    1. 该层的输入 $H_{\\ell-1}$ 是通过将原始输入 $X$ 与所有先前层的输出串联而形成的：$H_{\\ell-1} = \\mathrm{concat}(X, h_1, \\dots, h_{\\ell-1})$。该输入的维度是 $d_0 + (\\ell-1)k$，它作为权重初始化的扇入。\n    2. 应用线性变换：$z_\\ell = H_{\\ell-1} W_\\ell + b_\\ell$，其中 $W_\\ell \\in \\mathbb{R}^{(d_0+(\\ell-1)k) \\times k}$ 且 $b_\\ell \\in \\mathbb{R}^k$。\n    3. 逐元素应用非线性激活函数 $\\phi$ 以产生该层的输出：$h_\\ell = \\phi(z_\\ell) \\in \\mathbb{R}^k$。\n- 在最后一个块层之后，所有中间输出与原始输入串联，形成最终表示 $H = \\mathrm{concat}(X, h_1, \\dots, h_L)$。\n- 一个最终的线性分类器层将此表示映射到 $C$ 个类别的 logits：$o = H W_c + b_c$。\n- **激活函数**：按规定实现了三个激活函数：ReLU ($\\phi(x) = \\max(0, x)$)、GELU 的 tanh 近似 ($\\phi(x) \\approx \\frac{1}{2} x (1 + \\tanh(\\sqrt{2/\\pi} (x + 0.044715 x^3)))$) 和 SiLU ($\\phi(x) = x \\cdot (1 + e^{-x})^{-1}$)。\n\n### 3. 损失计算与优化\n该模型被训练以执行多类别分类。\n- **Softmax 和交叉熵损失**：输出 logits $o$ 使用 softmax 函数转换为概率 $p$，$p_c = \\exp(o_c) / \\sum_j \\exp(o_j)$。使用 log-sum-exp 技巧 ($o_c \\leftarrow o_c - \\max(o)$) 来确保数值稳定性。训练目标是最小化批次上的平均交叉熵损失，$\\mathcal{L} = -\\frac{1}{N}\\sum_i \\log p_{i, y_i}$，其中 $y_i$ 是样本 $i$ 的真实类别。\n- **反向传播**：损失相对于所有模型参数（$W_\\ell, b_\\ell, W_c, b_c$）的梯度通过链式法则计算，这个过程称为反向传播。交叉熵损失相对于 logits 的导数是 $\\frac{\\partial \\mathcal{L}}{\\partial o_i} = (p_i - y_{i, \\text{one-hot}}) / N$。然后这个梯度通过分类器和密集块向后传播。对于密集块，给定隐藏输出 $h_\\ell$ 的梯度是它所贡献的所有路径的梯度之和：它与最终分类器的直接连接，以及它作为所有后续层 $j > \\ell$ 的输入。我们的实现为每个层的参数仔细地累积了这些梯度。每个激活函数的导数也是必需的，并相应地实现。\n- **梯度下降**：使用全批量梯度下降规则更新参数：$\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}$，其中 $\\theta$ 代表任何参数，$\\eta$ 是学习率，$\\nabla_\\theta \\mathcal{L}$ 是计算出的梯度。\n\n### 4. 评估和报告\n模型被训练固定的周期数 $E$。对于每个测试用例，计算三个指标：\n1.  **最终损失**：在 $E$ 个周期后，整个数据集上的平均交叉熵损失。\n2.  **最终准确率**：在 $E$ 个周期后，正确分类样本的比例。\n3.  **收敛速度**：训练损失首次严格低于阈值 $\\tau = 0.6$ 的周期索引（从1开始）。如果损失从未低于此阈值，则报告 -1。\n\n该实现将此逻辑封装到一个类中，然后为五个指定的测试用例中的每一个实例化和训练。收集结果并格式化为所需的单行输出。", "answer": "```python\nimport numpy as np\n\n# This program implements a dense-block neural network and compares the performance\n# of different activation functions (ReLU, GELU, SiLU) on a synthetic classification task.\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef relu_grad(x):\n    \"\"\"Gradient of the ReLU function.\"\"\"\n    return (x > 0).astype(float)\n\ndef gelu_approx(x):\n    \"\"\"Gaussian Error Linear Unit (GELU) approximation.\"\"\"\n    a = np.sqrt(2 / np.pi)\n    b = 0.044715\n    return 0.5 * x * (1 + np.tanh(a * (x + b * x**3)))\n\ndef gelu_approx_grad(x):\n    \"\"\"Gradient of the GELU approximation.\"\"\"\n    a = np.sqrt(2 / np.pi)\n    b = 0.044715\n    x_cubed = x**3\n    inner_term = x + b * x_cubed\n    g_x = a * inner_term\n    tanh_g_x = np.tanh(g_x)\n    g_prime_x = a * (1 + 3 * b * x**2)\n    sech_g_x_sq = 1 - tanh_g_x**2\n    return 0.5 * (1 + tanh_g_x) + 0.5 * x * sech_g_x_sq * g_prime_x\n\ndef silu(x):\n    \"\"\"Sigmoid Linear Unit (SiLU) activation function.\"\"\"\n    # sigma(x) = 1 / (1 + exp(-x))\n    return x / (1 + np.exp(-x))\n\ndef silu_grad(x):\n    \"\"\"Gradient of the SiLU function.\"\"\"\n    sigma = 1 / (1 + np.exp(-x))\n    return sigma * (1 + x * (1 - sigma))\n\ndef generate_data(C, N_c, r, sigma, seed=42):\n    \"\"\"Generates a synthetic C-class dataset.\"\"\"\n    rng = np.random.default_rng(seed)\n    N = C * N_c\n    X = np.zeros((N, 2))\n    y = np.zeros(N, dtype=int)\n    for c in range(C):\n        angle = 2 * np.pi * c / C\n        mu = r * np.array([np.cos(angle), np.sin(angle)])\n        start_idx, end_idx = c * N_c, (c + 1) * N_c\n        X[start_idx:end_idx, :] = mu + rng.normal(0, sigma, size=(N_c, 2))\n        y[start_idx:end_idx] = c\n    return X, y\n\nclass DenseBlockNet:\n    \"\"\"A neural network with a simplified dense block architecture.\"\"\"\n    def __init__(self, input_dim, num_classes, L, k, activation, seed):\n        self.input_dim = input_dim\n        self.num_classes = num_classes\n        self.L = L\n        self.k = k\n        self.rng = np.random.default_rng(seed)\n        self.activation_fn, self.activation_grad = self._get_activation(activation)\n\n        self.weights = []\n        self.biases = []\n\n        # Initialize dense block layer parameters\n        current_dim = self.input_dim\n        for _ in range(L):\n            fan_in = current_dim\n            std_dev = np.sqrt(2.0 / fan_in)\n            self.weights.append(self.rng.normal(0, std_dev, (fan_in, k)))\n            self.biases.append(np.zeros(k))\n            current_dim += k\n        \n        # Initialize classifier layer parameters\n        fan_in = current_dim\n        std_dev = np.sqrt(2.0 / fan_in)\n        self.weights.append(self.rng.normal(0, std_dev, (fan_in, num_classes)))\n        self.biases.append(np.zeros(num_classes))\n\n    def _get_activation(self, name):\n        if name == 'ReLU': return relu, relu_grad\n        if name == 'GELU': return gelu_approx, gelu_approx_grad\n        if name == 'SiLU': return silu, silu_grad\n        raise ValueError(f\"Unknown activation function: {name}\")\n\n    def forward(self, X):\n        cache = {'h_outputs': [], 'z_outputs': [], 'layer_inputs': []}\n        layer_input = X\n        \n        for i in range(self.L):\n            cache['layer_inputs'].append(layer_input)\n            W, b = self.weights[i], self.biases[i]\n            z = layer_input @ W + b\n            h = self.activation_fn(z)\n            cache['z_outputs'].append(z)\n            cache['h_outputs'].append(h)\n            layer_input = np.concatenate([X] + cache['h_outputs'], axis=1)\n\n        final_H = layer_input\n        cache['final_H'] = final_H\n        \n        W_c, b_c = self.weights[-1], self.biases[-1]\n        logits = final_H @ W_c + b_c\n\n        stable_logits = logits - np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(stable_logits)\n        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        cache['probs'] = probs\n        return probs, cache\n\n    def backward(self, X, Y_one_hot, cache):\n        N = X.shape[0]\n        grads = {'weights': [None]*(self.L + 1), 'biases': [None]*(self.L + 1)}\n\n        d_logits = (cache['probs'] - Y_one_hot) / N\n        H_final = cache['final_H']\n        W_c = self.weights[-1]\n        \n        grads['weights'][-1] = H_final.T @ d_logits\n        grads['biases'][-1] = np.sum(d_logits, axis=0)\n        d_H_final = d_logits @ W_c.T\n\n        d_h_accum = {}\n        current_dim = self.input_dim\n        for i in range(self.L):\n            d_h_accum[i] = d_H_final[:, current_dim : current_dim + self.k]\n            current_dim += self.k\n\n        for i in range(self.L - 1, -1, -1):\n            d_h = d_h_accum[i]\n            z = cache['z_outputs'][i]\n            d_z = d_h * self.activation_grad(z)\n            \n            layer_input = cache['layer_inputs'][i]\n            W = self.weights[i]\n            grads['weights'][i] = layer_input.T @ d_z\n            grads['biases'][i] = np.sum(d_z, axis=0)\n            \n            d_layer_input = d_z @ W.T\n            current_dim = self.input_dim\n            for j in range(i):\n                grad_for_hj = d_layer_input[:, current_dim : current_dim + self.k]\n                d_h_accum[j] += grad_for_hj\n                current_dim += self.k\n        return grads\n\n    def train(self, X, y, epochs, learning_rate, loss_threshold):\n        N, C = X.shape[0], self.num_classes\n        y_one_hot = np.zeros((N, C)); y_one_hot[np.arange(N), y] = 1\n        first_epoch_below_threshold = -1\n\n        for epoch in range(1, epochs + 1):\n            probs, cache = self.forward(X)\n            loss = -np.sum(np.log(probs[np.arange(N), y] + 1e-9)) / N\n\n            if loss  loss_threshold and first_epoch_below_threshold == -1:\n                first_epoch_below_threshold = epoch\n            \n            grads = self.backward(X, y_one_hot, cache)\n            for i in range(self.L + 1):\n                self.weights[i] -= learning_rate * grads['weights'][i]\n                self.biases[i] -= learning_rate * grads['biases'][i]\n        \n        final_probs, _ = self.forward(X)\n        final_loss = -np.sum(np.log(final_probs[np.arange(N), y] + 1e-9)) / N\n        predictions = np.argmax(final_probs, axis=1)\n        final_accuracy = np.mean(predictions == y)\n        \n        return final_loss, final_accuracy, first_epoch_below_threshold\n\ndef solve():\n    test_cases = [\n        ('ReLU', 3, 12, 0.05, 200, 123),\n        ('GELU', 3, 12, 0.05, 200, 123),\n        ('SiLU', 3, 12, 0.05, 200, 123),\n        ('ReLU', 1, 12, 0.05, 200, 123),\n        ('GELU', 3, 12, 0.005, 200, 123),\n    ]\n\n    X_data, y_data = generate_data(C=3, N_c=100, r=3, sigma=0.5, seed=0)\n    \n    results = []\n    for activation, L, k, eta, E, s in test_cases:\n        model = DenseBlockNet(\n            input_dim=2, num_classes=3, L=L, k=k, activation=activation, seed=s)\n        \n        loss, acc, epoch_idx = model.train(\n            X_data, y_data, epochs=E, learning_rate=eta, loss_threshold=0.6)\n        \n        results.append([round(loss, 4), round(acc, 3), epoch_idx])\n\n    outer_parts = []\n    for res in results:\n        outer_parts.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    print(f\"[{','.join(outer_parts)}]\")\n\nsolve()\n```", "id": "3113972"}]}