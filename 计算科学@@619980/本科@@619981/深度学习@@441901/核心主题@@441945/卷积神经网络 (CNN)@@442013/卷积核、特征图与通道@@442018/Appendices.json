{"hands_on_practices": [{"introduction": "现代卷积神经网络（CNN）设计中一个基本原则是，倾向于堆叠小尺寸的卷积核，而不是使用单个大尺寸的卷积核。这个练习将让你定量地探究其原因：通过比较单个 $5 \\times 5$ 卷积层与两个堆叠的 $3 \\times 3$ 卷积层，你将推导出每种架构的感受野和参数数量。这个练习揭示了堆叠卷积如何在参数更少、非线性更强的情况下，实现相同的感受野，这是VGGNet等有影响力的架构背后的核心思想。[@problem_id:3126220]", "problem": "考虑一个卷积神经网络（CNN），它被定义为一系列层的序列，其中每一层都使用可学习的卷积核对多通道输入执行离散卷积，并可选地应用逐点非线性。设输入特征图为 $x \\in \\mathbb{R}^{H \\times W \\times C}$，其中有 $C$ 个通道。本问题中的所有卷积都使用步幅为 $1$、保持空间维度的零填充，并且不使用扩张卷积。单个卷积层通过聚合其卷积核覆盖的空间邻域内以及所有输入通道的信息，将 $\\mathbb{R}^{H \\times W \\times C}$ 映射到 $\\mathbb{R}^{H \\times W \\times C}$。逐点非线性 $\\phi$ 独立地应用于每个空间位置和每个通道。\n\n考虑两种架构：\n\n- 架构 A：一个带有 $5 \\times 5$ 卷积核的卷积层，后跟一次非线性 $\\phi$ 的应用。\n- 架构 B：两个连续的卷积层，每个层都带有 $3 \\times 3$ 卷积核，其中非线性 $\\phi$ 在两个层之后都应用一次。\n\n假设两种架构都保持通道数不变，因此每一层都将 $C$ 个输入通道映射到 $C$ 个输出通道。卷积层中的每个输出通道由一个连接到所有 $C$ 个输入通道的大小为 $k \\times k$ 的可学习卷积核和一个用于该输出通道的标量偏置生成。\n\n任务：\n1. 使用步幅为 $1$ 的卷积复合下感受野增长的定义，确定架构 B 第二层中单个输出空间位置受影响的不同输入空间位置的数量，即感受野的精确边长 $R$。\n2. 根据卷积层中每个输出通道都有一个连接到所有 $C$ 个输入通道的大小为 $k \\times k$ 的可学习卷积核和一个标量偏置的定义，推导出架构 A 和架构 B 中可学习参数总数的代数表达式（用 $C$ 表示）。\n3. 论证为什么当中间的非线性函数设置成恒等函数时，架构 B 可以模拟架构 A，并说明当 $\\phi$ 非平凡时，在架构 A 和架构 B 中，每个输出位置应用的逐点非线性次数。\n\n以单行向量 $\\left(R,\\; P_{\\text{A}},\\; P_{\\text{B}},\\; N_{\\text{A}},\\; N_{\\text{B}}\\right)$ 的形式报告您的最终答案，其中 $P_{\\text{A}}$ 和 $P_{\\text{B}}$ 分别是架构 A 和架构 B 的总参数数量，$N_{\\text{A}}$ 和 $N_{\\text{B}}$ 是每个输出位置应用的逐点非线性次数。所有量均需精确表示，无需四舍五入。", "solution": "该问题要求对两种卷积神经网络（CNN）架构（表示为架构 A 和架构 B）进行比较分析，比较的依据是感受野大小、参数数量和功能特性。\n\n### 步骤 1：问题陈述的验证\n首先，对问题陈述进行严格的验证。\n\n**提取已知条件：**\n- 输入特征图：$x \\in \\mathbb{R}^{H \\times W \\times C}$，有 $C$ 个通道。\n- 卷积属性：步幅为 $1$，零填充保持空间维度，无扩张卷积。\n- 层的映射关系：$\\mathbb{R}^{H \\times W \\times C} \\to \\mathbb{R}^{H \\times W \\times C}$（通道数 $C$ 保持不变）。\n- 逐点非线性：$\\phi$，独立应用于每个空间位置和通道。\n- 架构 A：一个带有 $5 \\times 5$ 卷积核的卷积层，后跟一次 $\\phi$ 的应用。\n- 架构 B：两个连续的卷积层，每个层都带有 $3 \\times 3$ 卷积核，$\\phi$ 在每个层后应用。\n- 参数定义：每个输出通道由一个大小为 $k \\times k$、连接到所有 $C$ 个输入通道的可学习卷积核，外加一个标量偏置生成。\n\n**验证：**\n1.  **科学依据：** 该问题牢固地植根于深度学习和 CNN 的基本原理。感受野、参数计算和架构比较（例如，单个大卷积核与堆叠的小卷积核）等概念是该领域的标准和核心主题。\n2.  **适定性：** 所有必要条件（步幅、填充、通道映射）都已明确定义，确保了待求量（$R$、$P_{\\text{A}}$、$P_{\\text{B}}$、$N_{\\text{A}}$、$N_{\\text{B}}$）具有唯一且有意义的解。\n3.  **客观性：** 问题使用精确、正式且无偏见的语言陈述，不含主观论断。\n\n问题陈述是自洽的、科学上合理的且适定的。它没有明显缺陷。\n\n**结论：** 该问题有效。\n\n### 步骤 2：求解推导\n\n**任务 1：架构 B 的感受野**\n\n神经元的感受野定义了输入空间中影响其激活的区域。对于一系列卷积层，感受野大小随每层增加而增长。第 $l$ 层之后的感受野边长 $R_l$ 可以递归计算。给定前一层的感受野为 $R_{l-1}$，卷积核大小为 $k_l \\times k_l$，步幅为 $S_l$，则新的感受野为：\n$$R_l = R_{l-1} + (k_l - 1) \\prod_{i=1}^{l-1} S_i$$\n设输入（第 $0$ 层）的感受野为 $R_0 = 1$。问题指定所有步幅均为 $S=1$。该公式简化为一个简单的和：\n$$R_l = R_{l-1} + (k_l - 1)$$\n架构 B 有两层，卷积核大小均为 $k_1 = k_2 = 3$。\n\n对于架构 B 的第一层：\n$$R_1 = R_0 + (k_1 - 1) = 1 + (3 - 1) = 3$$\n这意味着第一层输出中的一个神经元可以看到输入的 $3 \\times 3$ 区域。\n\n对于架构 B 的第二层，其输入是第一层的输出。第二层输出中一个神经元的感受野 $R$ 为：\n$$R = R_2 = R_1 + (k_2 - 1) = 3 + (3 - 1) = 5$$\n因此，架构 B 的感受野边长为 $5$。\n\n**任务 2：可学习参数的数量**\n\n卷积层中的参数数量是其权重和偏置的总和。对于一个具有 $C_{\\text{in}}$ 个输入通道、$C_{\\text{out}}$ 个输出通道和大小为 $k \\times k$ 的卷积核的层：\n- 权重数量 = $C_{\\text{out}} \\times (k \\times k \\times C_{\\text{in}})$。\n- 偏置数量 = $C_{\\text{out}}$。\n总参数 $P = (k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}) + C_{\\text{out}}$。\n在本问题中，所有层都保持通道维度不变，因此 $C_{\\text{in}} = C_{\\text{out}} = C$。单层参数的公式变为：\n$$P_{\\text{layer}} = k^2 C^2 + C$$\n\n- **架构 A：** 它有一个带有 $5 \\times 5$ 卷积核（$k=5$）的层。总参数数量 $P_{\\text{A}}$ 为：\n$$P_{\\text{A}} = 5^2 C^2 + C = 25C^2 + C$$\n\n- **架构 B：** 它有两个连续的层，每个都带有 $3 \\times 3$ 卷积核（$k=3$）。\n  - 第一层的参数（$P_{\\text{B1}}$）：$P_{\\text{B1}} = 3^2 C^2 + C = 9C^2 + C$。\n  - 第二层的参数（$P_{\\text{B2}}$）：该层的输入是第一层的输出，同样有 $C$ 个通道。因此，$P_{\\text{B2}} = 3^2 C^2 + C = 9C^2 + C$。\n总参数数量 $P_{\\text{B}}$ 是两层之和：\n$$P_{\\text{B}} = P_{\\text{B1}} + P_{\\text{B2}} = (9C^2 + C) + (9C^2 + C) = 18C^2 + 2C$$\n\n**任务 3：模拟与非线性次数**\n\n- **模拟的论证：**\n卷积是一种线性运算（具体来说，是由互相关和偏置加法组成的线性映射）。架构 B 若将中间的非线性 $\\phi$ 设置为恒等函数（$\\phi(z)=z$），则变成两个线性卷积层的复合。两个线性变换的复合本身也是一个线性变换。设这两个层为 $L_1(x) = W_1 * x + b_1$ 和 $L_2(y) = W_2 * y + b_2$。复合函数为 $L_B(x) = L_2(L_1(x)) = W_2 * (W_1 * x + b_1) + b_2$。根据卷积的结合律，这等价于 $(W_2 * W_1) * x + \\text{bias terms}$，可以写成 $W_{\\text{eff}} * x + b_{\\text{eff}}$。一个 $3 \\times 3$ 的卷积核（$W_1$）与另一个 $3 \\times 3$ 的卷积核（$W_2$）进行卷积，会产生一个大小为 $5 \\times 5$ 的有效卷积核 $W_{\\text{eff}}$。这与架构 A 中单层的卷积核大小以及（如任务 1 所示）感受野相匹配。因此，架构 B 在没有中间非线性的情况下，功能上是一个具有 $5 \\times 5$ 感受野的线性卷积层，这与架构 A 的结构相同。\n\n- **逐点非线性的次数：**\n此量，对于每个输出位置而言，被解释为信号在序列中经过的非线性变换的数量。非线性的一次“应用”指的是网络数据流中的一个阶段。\n  - **架构 A ($N_{\\text{A}}$)：** 该架构是 $\\text{conv}_{5 \\times 5} \\rightarrow \\phi$。在单个卷积层之后只应用了一层非线性。因此，$N_{\\text{A}} = 1$。\n  - **架构 B ($N_{\\text{B}}$)：** 该架构是 $\\text{conv}_{3 \\times 3} \\rightarrow \\phi \\rightarrow \\text{conv}_{3 \\times 3} \\rightarrow \\phi$。非线性在两个卷积层之后都各应用一次。从输入到输出的信号经过两个连续的非线性阶段。因此，$N_{\\text{B}} = 2$。\n\n**最终计算总结：**\n- $R = 5$\n- $P_{\\text{A}} = 25C^2 + C$\n- $P_{\\text{B}} = 18C^2 + 2C$\n- $N_{\\text{A}} = 1$\n- $N_{\\text{B}} = 2$\n最终答案是行向量 $(R, P_{\\text{A}}, P_{\\text{B}}, N_{\\text{A}}, N_{\\text{B}})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n5  25C^2 + C  18C^2 + 2C  1  2\n\\end{pmatrix}\n}\n$$", "id": "3126220"}, {"introduction": "卷积理论通常假设信号是无限的，但在实践中，我们处理的是有限大小的特征图。这就带来了一个关键问题：我们应该如何处理边界处的像素？这个动手编程练习将指导你实现并比较三种常见的填充（padding）策略——零填充、反射填充和循环填充——以量化它们对输出的影响。通过在一个简单的斜坡信号上观察边界伪影，你将具体理解填充选择如何影响图像边缘的特征提取。[@problem_id:3139428]", "problem": "您需要设计并实现一个完全指定的数值实验，以比较不同填充策略在一维离散卷积中对边界行为的影响。该卷积用于深度学习中对通道进行核操作的特征提取。实验侧重于单个输入通道和单个输出通道。输入为斜坡信号，核为一个实值、奇数长度、对称且和为一的权重向量。您的程序必须计算并报告三种填充模式（零填充、反射填充和循环填充）的边缘伪影的量化度量。\n\n从以下基本定义和事实开始：\n1. 离散卷积：给定一个长度为 $L$ 的一维输入信号 $x[n]$ 和一个长度为奇数 $m$、半径为 $r = \\frac{m-1}{2}$ 的核 $w[j]$，“same”模式下长度为 $L$ 的输出特征图 $y[n]$ 定义为\n$$\ny[n] = \\sum_{j=0}^{m-1} w[j] \\, \\tilde{x}\\big(n + j - r\\big),\n$$\n其中 $\\tilde{x}[k]$ 是对 $x[k]$ 的填充扩展，用于计算 $[0, L-1]$ 范围之外的索引值。\n2. 核归一化：对于满足 $\\sum_{j=0}^{m-1} w[j] = 1$ 且围绕其中心索引对称的对称核 $w[j]$，当对一个无限长的斜坡信号进行卷积时，其内部响应是一个具有相同斜率的斜坡。由于扩展 $\\tilde{x}$ 的存在，边界处理仅在边缘附近会改变这一结果。\n3. 待比较的填充模式：\n   - 零填充：当 $0 \\le k \\le L-1$ 时，$\\tilde{x}[k] = x[k]$，否则 $\\tilde{x}[k] = 0$。\n   - 反射填充：当 $k  0$ 时，$\\tilde{x}[k] = x[-k-1]$；当 $0 \\le k \\le L-1$ 时，$\\tilde{x}[k] = x[k]$；当 $k \\ge L$ 时，$\\tilde{x}[k] = x[2L - k - 1]$。\n   - 循环填充：对于所有整数 $k$，$\\tilde{x}[k] = x[(k \\bmod L)]$。\n4. 斜坡输入：输入信号为 $x[n] = b + s \\, n$，其中 $n \\in \\{0, 1, \\dots, L-1\\}$，$s$ 为斜率，$b$ 为偏移量。\n\n您的任务是：\n- 为每种填充模式实现上述的“same”离散卷积 $y[n]$。\n- 将无边界影响的理想斜坡输出定义为 $y_{\\text{ideal}}[n] = b + s \\, n$。\n- 在边界索引集 $E = \\{0, 1, \\dots, r-1\\} \\cup \\{L-r, L-r+1, \\dots, L-1\\}$ 上，使用以下指标量化每种填充模式的边缘伪影：\n   1. 边界最大绝对偏差：\n   $$\n   D_{\\max} = \\max_{n \\in E} \\left| y[n] - y_{\\text{ideal}}[n] \\right|.\n   $$\n   2. 边界均方偏差：\n   $$\n   D_{\\text{mse}} = \\begin{cases}\n   \\dfrac{1}{|E|} \\sum\\limits_{n \\in E} \\left( y[n] - y_{\\text{ideal}}[n] \\right)^2,  \\text{若 } r > 0, \\\\\n   0,  \\text{若 } r = 0.\n   \\end{cases}\n   $$\n- 对于每个测试用例，按固定顺序 $[D_{\\max}^{\\text{zero}}, D_{\\text{mse}}^{\\text{zero}}, D_{\\max}^{\\text{reflect}}, D_{\\text{mse}}^{\\text{reflect}}, D_{\\max}^{\\text{circular}}, D_{\\text{mse}}^{\\text{circular}}]$ 返回六个浮点数值。\n\n使用以下涵盖不同方面（正常情况、不同斜率、恒定信号和小长度边界压力）的参数值测试套件：\n- 案例 1：$L = 16$, $s = 1.0$, $b = 0.0$, $w = \\dfrac{1}{16}[1, 4, 6, 4, 1]$（因此 $m = 5$, $r = 2$）。\n- 案例 2：$L = 8$, $s = 0.5$, $b = 2.0$, $w = \\dfrac{1}{4}[1, 2, 1]$（因此 $m = 3$, $r = 1$）。\n- 案例 3：$L = 6$, $s = 2.0$, $b = -1.0$, $w = \\dfrac{1}{16}[1, 4, 6, 4, 1]$（因此 $m = 5$, $r = 2$）。\n- 案例 4：$L = 5$, $s = 0.0$, $b = 1.0$, $w = \\dfrac{1}{4}[1, 2, 1]$（因此 $m = 3$, $r = 1$）。\n- 案例 5：$L = 4$, $s = 1.0$, $b = 0.0$, $w = \\dfrac{1}{16}[1, 4, 6, 4, 1]$（因此 $m = 5$, $r = 2$）。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，该列表聚合了所有提供的测试用例，每个测试用例的结果本身也是一个用方括号括起来的逗号分隔列表。确切格式必须为\n$$\n\\big[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}, r_{1,5}, r_{1,6}], [r_{2,1}, \\dots, r_{2,6}], \\dots, [r_{5,1}, \\dots, r_{5,6}] \\big],\n$$\n输出中任何地方都不能有空格。", "solution": "用户提供了一个问题，要求进行数值实验以比较一维离散卷积中不同填充策略的边界效应。在着手解决之前，需要对问题陈述进行全面验证。\n\n### 步骤 1：提取给定信息\n\n- **离散卷积**：对于长度为 $L$ 的输入信号 $x[n]$ 和长度为奇数 $m$、半径为 $r = \\frac{m-1}{2}$ 的核 $w[j]$，“same”模式的输出为 $y[n] = \\sum_{j=0}^{m-1} w[j] \\, \\tilde{x}\\big(n + j - r\\big)$，其中 $\\tilde{x}$ 是填充后的信号。\n- **核属性**：核 $w[j]$ 是对称的，且归一化使得 $\\sum_{j=0}^{m-1} w[j] = 1$。\n- **输入信号**：一个由 $x[n] = b + s \\, n$ 定义的斜坡信号，其中 $n \\in \\{0, 1, \\dots, L-1\\}$。\n- **理想输出**：无边界伪影的理想输出为 $y_{\\text{ideal}}[n] = b + s \\, n$。\n- **填充模式**：\n  1.  **零填充**：当 $0 \\le k \\le L-1$ 时，$\\tilde{x}[k] = x[k]$，否则 $\\tilde{x}[k] = 0$。\n  2.  **反射填充**：当 $k  0$ 时，$\\tilde{x}[k] = x[-k-1]$；当 $0 \\le k \\le L-1$ 时，$\\tilde{x}[k] = x[k]$；当 $k \\ge L$ 时，$\\tilde{x}[k] = x[2L - k - 1]$。\n  3.  **循环填充**：对于所有整数 $k$，$\\tilde{x}[k] = x[(k \\bmod L)]$。\n- **伪影度量**：\n  -   边界索引集：$E = \\{0, 1, \\dots, r-1\\} \\cup \\{L-r, L-r+1, \\dots, L-1\\}$。\n  -   边界最大绝对偏差：$D_{\\max} = \\max_{n \\in E} \\left| y[n] - y_{\\text{ideal}}[n] \\right|$。\n  -   边界均方偏差：当 $r>0$ 时，$D_{\\text{mse}} = \\frac{1}{|E|} \\sum_{n \\in E} ( y[n] - y_{\\text{ideal}}[n] )^2$；当 $r=0$ 时，$D_{\\text{mse}} = 0$。\n- **测试用例**：\n  1.  $L = 16$, $s = 1.0$, $b = 0.0$, $w = \\frac{1}{16}[1, 4, 6, 4, 1]$ ($m=5, r=2$)。\n  2.  $L = 8$, $s = 0.5$, $b = 2.0$, $w = \\frac{1}{4}[1, 2, 1]$ ($m=3, r=1$)。\n  3.  $L = 6$, $s = 2.0$, $b = -1.0$, $w = \\frac{1}{16}[1, 4, 6, 4, 1]$ ($m=5, r=2$)。\n  4.  $L = 5$, $s = 0.0$, $b = 1.0$, $w = \\frac{1}{4}[1, 2, 1]$ ($m=3, r=1$)。\n  5.  $L = 4$, $s = 1.0$, $b = 0.0$, $w = \\frac{1}{16}[1, 4, 6, 4, 1]$ ($m=5, r=2$)。\n- **输出格式**：一个浮点数列表的列表，格式化为紧凑字符串：`[[r_1,1,...,r_1,6],[r_2,1,...,r_2,6],...]`。\n\n### 步骤 2：使用提取的给定信息进行验证\n\n- **科学依据充分**：该问题基于数字信号处理和深度学习中的基本和标准概念，即离散卷积和填充方法。所提供的定义在数学上是精确且被广泛使用的。对称、和为一的核在内部能够再现斜坡信号，这是线性时不变系统的一个已知属性。\n- **定义明确**：所有必要的参数（$L, s, b, w$）、公式和程序都已明确定义。对于每个测试用例，输入都是完整的，可以计算出一组唯一的、确定性的输出度量。问题没有歧义，并且是自包含的。\n- **客观性**：问题以客观的数学语言陈述，没有任何主观因素。\n\n该问题未违反任何无效标准。它科学合理、定义明确、客观、可形式化、完整且可验证。测试用例包括标准场景以及压力测试（例如，案例5中核半径使整个信号都成为边界），这对于全面的数值实验是合适的。\n\n### 步骤 3：结论与行动\n\n问题是**有效的**。将提供详细的解决方案。\n\n### 基于原则的解决方案设计\n\n解决方案将通过遵循所提供的数学定义来实现，以确保正确性和可验证性。\n\n1.  **模块化结构**：整个任务被分解为多个逻辑组件：一个用于组织实验的主函数，一个执行卷积的函数，一个处理填充信号访问的辅助函数，以及一个计算指定误差度量的函数。这有助于提高清晰度和正确性。\n\n2.  **卷积实现**：卷积是通过直接转换给定的求和公式 $y[n] = \\sum_{j=0}^{m-1} w[j] \\, \\tilde{x}(n + j - r)$ 来实现的。将使用嵌套循环结构遍历每个输出点 $n$ 和每个核元素 $j$。虽然这种方法对于大信号可能不是性能最高的，但它是对问题定义最直接和忠实的实现，优先保证了这个指定的小规模实验的正确性。\n\n3.  **填充信号访问**：为了计算卷积和，需要概念上无限的填充信号 $\\tilde{x}[k]$ 在可能位于原始信号域 $[0, L-1]$ 之外的索引 $k$ 处的值。设计一个辅助函数 `get_x_tilde(k, x, L, mode)` 来为三种填充模式（`zero`、`reflect`、`circular`）实现问题陈述中定义的精确逻辑。这种方法避免了使用预构建库函数可能带来的任何歧义，因为它们的命名约定（`reflect` vs. `symmetric`）可能有所不同。\n\n4.  **度量计算**：一旦计算出给定填充模式的输出信号 $y[n]$，就会计算其与理想信号 $y_{\\text{ideal}}[n]$ 的偏差。根据核半径 $r$ 构建边界索引集 $E$。然后提取与这些边界索引相对应的偏差。最后，直接根据定义计算 $D_{\\max}$ 和 $D_{\\text{mse}}$ 指标。对于 $r=0$（意味着边界集为空）的特殊情况，将按规定处理，尽管它在提供的测试套件中并未出现。\n\n5.  **测试执行与格式化**：一个主循环遍历五个测试用例中的每一个。对于每个案例，它会生成输入信号和理想信号，然后计算所需的六个度量（三种填充模式各两个）。结果被收集并格式化为最终输出所需的精确、无空格、逗号分隔的字符串格式。浮点运算是固有的，参数使用 `numpy` 数组设置，以便在适当的地方（例如，信号生成）进行向量化操作。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and implements a numerical experiment to compare padding strategies\n    in one-dimensional discrete convolution.\n    \"\"\"\n\n    def get_x_tilde(k, x, L, mode):\n        \"\"\"\n        Retrieves the value of the conceptually padded signal \\tilde{x} at index k.\n        \"\"\"\n        if 0 = k  L:\n            return x[k]\n        \n        if mode == 'zero':\n            return 0.0\n        elif mode == 'reflect':\n            if k  0:\n                # Per problem spec: \\tilde{x}[k] = x[-k-1] for k  0\n                return x[-k - 1]\n            else:  # k = L\n                # Per problem spec: \\tilde{x}[k] = x[2L - k - 1] for k = L\n                return x[2 * L - k - 1]\n        elif mode == 'circular':\n            # Per problem spec: \\tilde{x}[k] = x[k mod L]\n            return x[k % L]\n        else:\n            raise ValueError(f\"Unknown padding mode: {mode}\")\n\n    def compute_convolution(x, w, mode):\n        \"\"\"\n        Computes the 'same' 1D discrete convolution based on the problem's definition.\n        \"\"\"\n        L = len(x)\n        m = len(w)\n        r = (m - 1) // 2\n        y = np.zeros(L, dtype=float)\n\n        for n in range(L):\n            y_n = 0.0\n            for j in range(m):\n                k = n + j - r\n                x_val = get_x_tilde(k, x, L, mode)\n                y_n += w[j] * x_val\n            y[n] = y_n\n        return y\n\n    def calculate_metrics(y, y_ideal, L, r):\n        \"\"\"\n        Calculates the boundary deviation metrics D_max and D_mse.\n        \"\"\"\n        if r == 0:\n            return 0.0, 0.0\n\n        # Create boundary index set E\n        # Note: Slicing handles cases where L  2*r, e.g., dev[:r]\n        # would take all possible elements up to r.\n        \n        deviation = y - y_ideal\n        \n        # Boundary indices are {0..r-1} and {L-r..L-1}\n        # Special case: if 2*r = L, the boundary is the whole signal.\n        if 2 * r = L:\n            boundary_indices = np.arange(L)\n        else:\n            boundary_indices = np.concatenate((np.arange(r), np.arange(L - r, L)))\n\n        boundary_devs = deviation[boundary_indices]\n        \n        d_max = np.max(np.abs(boundary_devs))\n        d_mse = np.mean(np.square(boundary_devs)) # mean is sum/|E|\n        \n        return d_max, d_mse\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'L': 16, 's': 1.0, 'b': 0.0, 'w_raw': [1, 4, 6, 4, 1], 'w_norm': 16.0},\n        {'L': 8, 's': 0.5, 'b': 2.0, 'w_raw': [1, 2, 1], 'w_norm': 4.0},\n        {'L': 6, 's': 2.0, 'b': -1.0, 'w_raw': [1, 4, 6, 4, 1], 'w_norm': 16.0},\n        {'L': 5, 's': 0.0, 'b': 1.0, 'w_raw': [1, 2, 1], 'w_norm': 4.0},\n        {'L': 4, 's': 1.0, 'b': 0.0, 'w_raw': [1, 4, 6, 4, 1], 'w_norm': 16.0},\n    ]\n\n    all_results = []\n    for params in test_cases:\n        L, s, b = params['L'], params['s'], params['b']\n        w = np.array(params['w_raw']) / params['w_norm']\n        m = len(w)\n        r = (m - 1) // 2\n\n        # Generate input and ideal signals\n        n_indices = np.arange(L, dtype=float)\n        x = b + s * n_indices\n        y_ideal = x.copy()\n        \n        case_results = []\n        padding_modes = ['zero', 'reflect', 'circular']\n        \n        for mode in padding_modes:\n            # Compute convolution output\n            y = compute_convolution(x, w, mode)\n            \n            # Calculate metrics\n            d_max, d_mse = calculate_metrics(y, y_ideal, L, r)\n            case_results.extend([d_max, d_mse])\n            \n        all_results.append(case_results)\n\n    # Format output according to specification\n    inner_results_str = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3139428"}, {"introduction": "除了空间处理，构建强大特征表示的关键在于理解通道（channel）之间的关系。本练习通过让你实现一个“压缩-激励”（Squeeze-and-Excitation, SE）模块来介绍通道注意力的概念，这是一种学习自适应地重新加权特征通道的机制。你将从其基本组成部分——全局池化、一个小型多层感知机和通道尺度缩放——从零开始构建SE模块，从而理解网络如何能够动态地强调信息量大的特征并抑制作用较小的特征。[@problem_id:3139403]", "problem": "要求您实现一个逐通道的 Squeeze-and-Excitation (SE) 模块，并从第一性原理出发量化其参数开销。该 SE 模块作用于一个具有通道、高度和宽度轴的三维输入张量。您必须实现的操作定义如下，仅使用线性映射和逐点非线性的核心定义。\n\n给定一个输入张量 $x \\in \\mathbb{R}^{C \\times H \\times W}$，squeeze 操作定义为逐通道的全局平均值\n$$\ns_c \\;=\\; \\frac{1}{H\\,W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}, \\quad \\text{for } c \\in \\{0,\\dots,C-1\\}.\n$$\n将 $s_c$ 堆叠起来，得到描述子向量 $s \\in \\mathbb{R}^{C}$。将 excitation 定义为一个两层的多层感知器 (MLP)，其缩减率为 $r$，其中 $r$ 是一个能整除 $C$ 的正整数。令 $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$ 表示修正线性单元，$\\sigma(u)=\\frac{1}{1+e^{-u}}$ 表示 logistic sigmoid 函数。使用学习参数 $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$、$b_1 \\in \\mathbb{R}^{C/r}$、$W_2 \\in \\mathbb{R}^{C \\times (C/r)}$ 和 $b_2 \\in \\mathbb{R}^{C}$，excitation 的输出 $z \\in \\mathbb{R}^{C}$ 为\n$$\nt \\;=\\; \\mathrm{ReLU}(W_1 s + b_1), \\qquad z \\;=\\; \\sigma(W_2 t + b_2).\n$$\n最后，用 $z$ 逐通道地缩放输入张量，\n$$\ny_{cij} \\;=\\; x_{cij}\\, z_c, \\quad \\text{for all } c,i,j,\n$$\n以获得经 SE 增强的张量 $y \\in \\mathbb{R}^{C \\times H \\times W}$。\n\n参数开销定义为 excitation MLP 相对于没有 SE 模块的基线模型所引入的额外参数数量。对于两个非共享权重的线性层，权重数量为\n$$\n\\#\\text{weights}_{\\text{untied}} \\;=\\; C \\cdot \\frac{C}{r} \\;+\\; \\frac{C}{r} \\cdot C \\;=\\; \\frac{2C^2}{r},\n$$\n偏置数量为\n$$\n\\#\\text{biases} \\;=\\; \\frac{C}{r} \\;+\\; C.\n$$\n在权重共享（$W_2 = W_1^{\\top}$）的变体中，独立权重的数量减少为\n$$\n\\#\\text{weights}_{\\text{tied}} \\;=\\; C \\cdot \\frac{C}{r} \\;=\\; \\frac{C^2}{r}.\n$$\n\n请完全按照规定实现 SE 模块，并为每个测试用例计算以下输出：\n- squeeze 描述子的总和，$\\sum_{c=0}^{C-1} s_c$。\n- 缩放后输出的所有元素之和，$\\sum_{c,i,j} y_{cij}$，四舍五入到六位小数。\n- 非共享权重的开销 $\\#\\text{weights}_{\\text{untied}}$。\n- 共享权重的开销 $\\#\\text{weights}_{\\text{tied}}$。\n- 非共享权重加偏置的总开销 $\\#\\text{weights}_{\\text{untied}} + \\#\\text{biases}$。\n\n您的程序必须使用以下测试套件。在所有情况下，$C$ 都能被 $r$ 整除，且以下所有定义都必须被精确实现。\n\n测试用例 A (正常路径)：\n- $C = 4$, $H = 2$, $W = 2$, $r = 2$.\n- 输入定义为 $x_{cij} = c - i + j$ for $c \\in \\{0,1,2,3\\}$, $i \\in \\{0,1\\}$, $j \\in \\{0,1\\}$。\n- 参数：\n$$\nW_1 \\;=\\; \\begin{bmatrix}\n1  -1  0.5  0 \\\\\n0.25  0.5  -0.5  1\n\\end{bmatrix}, \\quad\nb_1 \\;=\\; \\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix},\n$$\n$$\nW_2 \\;=\\; \\begin{bmatrix}\n1  0.5 \\\\\n-0.5  0.25 \\\\\n0  1 \\\\\n0.75  -1\n\\end{bmatrix}, \\quad\nb_2 \\;=\\; \\begin{bmatrix} 0 \\\\ 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}.\n$$\n\n测试用例 B (空间维度的边界情况，$H W = 1$)：\n- $C = 8$, $H = 1$, $W = 1$, $r = 2$.\n- 输入定义为 $x_{c00} = c + 1$ for $c \\in \\{0,1,\\dots,7\\}$。\n- 参数由基于索引的公式逐元素定义。令 $a$ 为行索引，$b$ 为列索引，两者均从零开始：\n$$\nW_1[a,b] \\;=\\; \\frac{a\\cdot 8 + b - 12}{16}, \\quad \\text{for } a \\in \\{0,1,2,3\\}, \\; b \\in \\{0,1,\\dots,7\\},\n$$\n$$\nb_1 \\;=\\; \\begin{bmatrix} -0.25 \\\\ 0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}, \\quad\nW_2[a,b] \\;=\\; \\frac{a\\cdot 4 + b - 8}{8}, \\quad \\text{for } a \\in \\{0,1,\\dots,7\\}, \\; b \\in \\{0,1,2,3\\},\n$$\n$$\nb_2 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{8}.\n$$\n\n测试用例 C (边缘案例 $r = 1$，通道符号交替)：\n- $C = 6$, $H = 3$, $W = 1$, $r = 1$.\n- 输入定义为 $x_{ci0} = (-1)^c \\cdot (i+1)$ for $c \\in \\{0,1,\\dots,5\\}$ and $i \\in \\{0,1,2\\}$。\n- 参数：\n$$\nW_1 \\;=\\; 0.5\\, I_6 \\;-\\; 0.25\\,(\\mathbf{1}_6 \\mathbf{1}_6^{\\top} - I_6), \\quad b_1 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{6},\n$$\n$$\nW_2 \\;=\\; 0.3\\, I_6 \\;-\\; 0.05\\, \\mathbf{1}_6 \\mathbf{1}_6^{\\top}, \\quad b_2 \\;=\\; \\begin{bmatrix} -0.1 \\\\ -0.05 \\\\ 0 \\\\ 0.05 \\\\ 0.1 \\\\ 0.15 \\end{bmatrix},\n$$\n其中 $I_6$ 是 $6 \\times 6$ 的单位矩阵，$\\mathbf{1}_6$ 是长度为 6 的全一向量。\n\n角度单位和物理单位在此不适用。所有数值输出都应是标准十进制形式的实数。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含所有三个测试用例的结果，形式为列表的列表。每个内部列表必须按以下顺序排列\n$$\n\\left[ \\sum_{c} s_c,\\; \\mathrm{round}\\!\\left(\\sum_{c,i,j} y_{cij},\\,6\\right),\\; \\#\\text{weights}_{\\text{untied}},\\; \\#\\text{weights}_{\\text{tied}},\\; \\#\\text{weights}_{\\text{untied}} + \\#\\text{biases} \\right].\n$$\n例如，打印的结构必须如下所示\n$$\n\\big[\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,\\big].\n$$", "solution": "该问题要求基于其基本数学定义，实现并分析 Squeeze-and-Excitation (SE) 模块，这是一种深度学习中常见的架构组件。解决方案包括为三个不同的测试用例分步执行所定义的操作。其核心原理是通过数据驱动机制进行逐通道的特征重校准。\n\nSE 模块作用于一个输入张量 $x \\in \\mathbb{R}^{C \\times H \\times W}$，其中 $C$ 是通道数，$H$ 和 $W$ 分别是空间高度和宽度。该过程包括三个主要阶段：Squeeze、Excitation 和 Scale。\n\n**1. Squeeze：全局信息嵌入**\n第一步，“Squeeze”，在特征图的空间维度（$H \\times W$）上进行聚合，以生成一个通道描述子。这是通过全局平均池化实现的，该操作计算每个通道的平均值。得到的向量 $s \\in \\mathbb{R}^{C}$ 为每个通道嵌入了全局感受野的信息。$s$ 的第 $c$ 个元素的公式为：\n$$\ns_c = \\frac{1}{H \\cdot W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}\n$$\n在实现中，这对应于沿着输入张量 $x$ 的空间轴（轴1和轴2）取平均值。每个测试用例的第一个要求输出是这个 squeeze 后的向量的元素之和，即 $\\sum_{c=0}^{C-1} s_c$。\n\n**2. Excitation：自适应重校准**\n“Excitation”阶段从 squeeze 后的描述子 $s$ 生成一组每通道的调制权重，或称为“激活值”。这是通过一个小型神经网络完成的，具体来说是一个两层的多层感知器 (MLP)。该 MLP 的结构是先降低通道维度，然后再恢复它，从而形成一个计算瓶颈，有助于捕捉非线性的通道间相互依赖关系。\n\nMLP 包括：\n- 一个降维线性层，其权重为 $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$，偏置为 $b_1 \\in \\mathbb{R}^{C/r}$，其中 $r$ 是缩减率。\n- 一个修正线性单元（$\\mathrm{ReLU}$）激活函数，定义为 $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$。\n- 一个升维线性层，其权重为 $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$，偏置为 $b_2 \\in \\mathbb{R}^{C}$。\n- 一个 logistic sigmoid 激活函数，$\\sigma(u)=\\frac{1}{1+e^{-u}}$，它将输出归一化到 (0, 1) 范围内。\n\n计算过程如下：\n$$\nt = \\mathrm{ReLU}(W_1 s + b_1)\n$$\n$$\nz = \\sigma(W_2 t + b_2)\n$$\n得到的向量 $z \\in \\mathbb{R}^{C}$ 包含了逐通道的缩放因子。\n\n**3. Scale：特征重校准**\n最后一个阶段将 excitation 步骤中学到的缩放因子应用于原始输入张量 $x$。输入张量的每个通道都乘以向量 $z$ 中对应的缩放因子。这会对特征图进行重校准，放大信息丰富的通道并抑制作用较小的通道。输出张量 $y \\in \\mathbb{R}^{C \\times H \\times W}$ 由下式给出：\n$$\ny_{cij} = x_{cij} \\cdot z_c\n$$\n在实现中，这是通过将缩放向量 $z$（重塑为 $C \\times 1 \\times 1$ 的维度）广播到输入张量 $x$ 上来实现的。第二个要求输出是最终缩放张量中所有元素的总和，$\\sum_{c,i,j} y_{cij}$，其计算方式为 $\\sum_c (z_c \\cdot \\sum_{i,j} x_{cij}) = H \\cdot W \\cdot \\sum_c (z_c \\cdot s_c)$。\n\n**4. 参数开销计算**\n问题还要求量化 SE 模块的 MLP 所引入的额外可学习参数的数量。这些参数包括两个线性层的权重和偏置。\n- **非共享权重：** 当 $W_1$ 和 $W_2$ 相互独立时，总权重数是两个矩阵中元素数量之和：$(\\frac{C}{r} \\times C) + (C \\times \\frac{C}{r}) = \\frac{2C^2}{r}$。\n- **共享权重：** 在 $W_2 = W_1^{\\top}$ 的变体中，独立权重的数量减少为 $W_1$ 的大小，即 $\\frac{C^2}{r}$。\n- **偏置：** 偏置参数的数量是 $b_1$ 和 $b_2$ 中元素数量之和，即 $\\frac{C}{r} + C$。\n- **总非共享开销：** 非共享权重情况下的总参数数量是非共享权重和偏置之和：$\\frac{2C^2}{r} + \\frac{C}{r} + C$。\n\n该实现将使用指定的输入张量和 MLP 参数，为提供的三个测试用例中的每一个计算这五个量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format results for all test cases.\n    \"\"\"\n\n    def se_block_and_overhead(C, H, W, r, x, W1, b1, W2, b2):\n        \"\"\"\n        Implements the SE block operations and calculates parameter overhead.\n\n        Args:\n            C (int): Number of channels.\n            H (int): Height of the input tensor.\n            W (int): Width of the input tensor.\n            r (int): Reduction ratio.\n            x (np.ndarray): Input tensor of shape (C, H, W).\n            W1 (np.ndarray): Weights of the first linear layer.\n            b1 (np.ndarray): Biases of the first linear layer.\n            W2 (np.ndarray): Weights of the second linear layer.\n            b2 (np.ndarray): Biases of the second linear layer.\n\n        Returns:\n            list: A list containing the five required output values.\n        \"\"\"\n        # 1. Squeeze Operation: Global Average Pooling\n        # s_c = (1/(H*W)) * sum(x_cij for i,j)\n        s = np.mean(x, axis=(1, 2))\n        sum_s = np.sum(s)\n\n        # 2. Excitation Operation: MLP\n        # t = ReLU(W1*s + b1)\n        t = np.maximum(0, W1 @ s + b1)\n        \n        # z = sigmoid(W2*t + b2)\n        z_raw = W2 @ t + b2\n        z = 1 / (1 + np.exp(-z_raw))\n\n        # 3. Scale Operation\n        # y_cij = x_cij * z_c\n        # Reshape z to (C, 1, 1) for broadcasting\n        y = x * z.reshape((C, 1, 1))\n        sum_y = np.sum(y)\n\n        # 4. Parameter Overhead Calculation\n        weights_untied = (2 * C**2) // r\n        weights_tied = (C**2) // r\n        biases = (C // r) + C\n        total_untied_with_biases = weights_untied + biases\n        \n        return [\n            sum_s,\n            round(sum_y, 6),\n            weights_untied,\n            weights_tied,\n            total_untied_with_biases\n        ]\n\n    results = []\n\n    # --- Test Case A ---\n    C_A, H_A, W_A, r_A = 4, 2, 2, 2\n    x_A = np.fromfunction(lambda c, i, j: c - i + j, (C_A, H_A, W_A))\n    W1_A = np.array([\n        [1, -1, 0.5, 0],\n        [0.25, 0.5, -0.5, 1]\n    ])\n    b1_A = np.array([-0.5, 0])\n    W2_A = np.array([\n        [1, 0.5],\n        [-0.5, 0.25],\n        [0, 1],\n        [0.75, -1]\n    ])\n    b2_A = np.array([0, 0.1, -0.2, 0.3])\n    results.append(se_block_and_overhead(C_A, H_A, W_A, r_A, x_A, W1_A, b1_A, W2_A, b2_A))\n\n    # --- Test Case B ---\n    C_B, H_B, W_B, r_B = 8, 1, 1, 2\n    x_B = (np.arange(C_B) + 1).reshape(C_B, H_B, W_B)\n    W1_B = np.fromfunction(lambda a, b: (a * 8 + b - 12) / 16, (C_B // r_B, C_B))\n    b1_B = np.array([-0.25, 0, 0.1, -0.1])\n    W2_B = np.fromfunction(lambda a, b: (a * 4 + b - 8) / 8, (C_B, C_B // r_B))\n    b2_B = np.zeros(C_B)\n    results.append(se_block_and_overhead(C_B, H_B, W_B, r_B, x_B, W1_B, b1_B, W2_B, b2_B))\n\n    # --- Test Case C ---\n    C_C, H_C, W_C, r_C = 6, 3, 1, 1\n    c_vec_C = (-1)**np.arange(C_C)\n    i_vec_C = np.arange(1, H_C + 1)\n    x_C = (c_vec_C[:, np.newaxis, np.newaxis]\n           * i_vec_C[np.newaxis, :, np.newaxis])\n    \n    I6 = np.identity(C_C)\n    J6 = np.ones((C_C, C_C))\n    \n    W1_C = 0.5 * I6 - 0.25 * (J6 - I6)\n    b1_C = np.zeros(C_C)\n    W2_C = 0.3 * I6 - 0.05 * J6\n    b2_C = np.array([-0.1, -0.05, 0, 0.05, 0.1, 0.15])\n    results.append(se_block_and_overhead(C_C, H_C, W_C, r_C, x_C, W1_C, b1_C, W2_C, b2_C))\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string and join them with commas.\n    # The outer brackets are added by the f-string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3139403"}]}