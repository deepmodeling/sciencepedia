{"hands_on_practices": [{"introduction": "对于任何深度学习工程师而言，理解和量化模型的计算成本是一项基本技能。本练习将引导你从第一性原理出发，推导一个 DenseNet 密集块的浮点运算次数 (FLOPs)，并将其与一个结构等效的 ResNet 块进行直接的量化比较。通过这种方式，你将深刻理解 DenseNet 在计算效率方面的核心设计思想。[@problem_id:3114885]", "problem": "给定一个在空间分辨率 $H \\times W$ 上运行的密集连接卷积网络 (DenseNet) 密集块。该块有 $L$ 层，增长率为 $k \\in \\mathbb{N}$，$k \\ge 1$。该块的输入有 $C_0$ 个通道。每一层 $\\ell \\in \\{1,\\dots,L\\}$ 接收块中所有先前输出的拼接作为输入，因此它有 $C_0 + (\\ell - 1) k$ 个输入通道。每一层都使用瓶颈设计，该设计由一个输出 $4k$ 个通道的 $1 \\times 1$ 卷积，后跟一个输出 $k$ 个通道的 $3 \\times 3$ 卷积组成。来自每一层的 $k$ 个输出通道被拼接到该块的特征上，以供后续层使用。假设块内空间尺寸没有变化，并忽略批量归一化 (BN)、修正线性单元 (ReLU)、拼接、偏置以及任何非卷积操作的计算成本。\n\n为了进行比较，我们考虑一个残差网络 (ResNet) 瓶颈块，它处理相同的空间分辨率 $H \\times W$ 并接收 $C_0$ 个通道作为输入。它由一个将通道数减少到 $4k$ 的 $1 \\times 1$ 卷积、一个将 $4k$ 通道映射到 $4k$ 通道的 $3 \\times 3$ 卷积，以及一个将通道数扩展到 $C_0 + Lk$ 的 $1 \\times 1$ 卷积组成，以使最终通道数与 DenseNet 块的输出宽度相匹配。假设空间尺寸没有变化，并忽略 BN、ReLU、跳跃连接的加法、偏置以及任何非卷积操作的成本。\n\n使用以下卷积成本的基础定义：对于一个产生 $H \\times W$ 空间尺寸输出、具有 $C_{\\text{in}}$ 个输入通道、$C_{\\text{out}}$ 个输出通道和 $K \\times K$ 卷积核的卷积，其乘加运算次数为 $H W \\, C_{\\text{out}} \\, C_{\\text{in}} \\, K^2$。将一次乘法和一次加法计为两次浮点运算 (FLOPs)，则 FLOPs 为 $2 H W \\, C_{\\text{out}} \\, C_{\\text{in}} \\, K^2$。\n\n仅从该定义和上述架构描述出发，推导以下项的总浮点运算 (FLOPs) 的闭式表达式：\n- DenseNet 密集块，以及\n- 具有匹配输出宽度 $C_0 + Lk$ 和瓶颈宽度 $4k$ 的 ResNet 瓶颈块。\n\n然后，求出 DenseNet 块 FLOPs 与 ResNet 块 FLOPs 的比率 $R$。以 $H$、$W$、$C_0$、$k$ 和 $L$ 的形式，提供 $R$ 的单个简化解析表达式作为你的最终答案。无需进行数值计算。", "solution": "首先验证该问题，以确保其具有科学依据、适定且客观。\n\n### 步骤 1：提取给定条件\n- **DenseNet 块：**\n  - 空间分辨率：$H \\times W$\n  - 层数：$L$\n  - 增长率：$k \\in \\mathbb{N}$，$k \\ge 1$\n  - 块输入通道数：$C_0$\n  - 第 $\\ell$ 层的输入通道数：$C_0 + (\\ell - 1)k$，其中 $\\ell \\in \\{1,\\dots,L\\}$\n  - 第 $\\ell$ 层的架构：一个产生 $4k$ 个通道的 $1 \\times 1$ 卷积，后跟一个产生 $k$ 个通道的 $3 \\times 3$ 卷积。\n- **ResNet 块：**\n  - 空间分辨率：$H \\times W$\n  - 块输入通道数：$C_0$\n  - 架构：一个将通道数减少到 $4k$ 的 $1 \\times 1$ 卷积，一个从 $4k$ 通道到 $4k$ 通道的 $3 \\times 3$ 卷积，以及一个将通道数扩展到 $C_0 + Lk$ 的 $1 \\times 1$ 卷积。\n- **FLOPs 定义：**\n  - 对于一个输出尺寸为 $H \\times W$、有 $C_{\\text{in}}$ 个输入通道、$C_{\\text{out}}$ 个输出通道和 $K \\times K$ 卷积核的卷积，其浮点运算 (FLOPs) 次数为 $2 H W C_{\\text{out}} C_{\\text{in}} K^2$。\n- **假设：** 块内空间尺寸无变化。忽略批量归一化、ReLU、拼接、偏置和其他非卷积操作的成本。\n\n### 步骤 2：使用提取的给定条件进行验证\n该问题具有科学依据，描述了深度学习领域中使用的标准架构 (DenseNet, ResNet) 和计算成本度量 (FLOPs)。问题是适定的，提供了推导唯一解析解所需的所有参数和定义。语言客观且精确。问题是自洽且无矛盾的。其设置没有违反任何基本原则。\n\n### 步骤 3：结论与行动\n该问题被认为是有效的。将提供一个完整的、有理有据的解答。\n\n### DenseNet 块 FLOPs 的推导\n\n设 $\\text{FLOPs}_{\\text{DN}}$ 为 DenseNet 块的总 FLOPs。这是一共 $L$ 层中每一层 FLOPs 的总和。对于单个层 $\\ell \\in \\{1, \\dots, L\\}$，其输入有 $C_{\\text{in}, \\ell} = C_0 + (\\ell-1)k$ 个通道。该层由两个卷积操作组成。\n\n1.  **$1 \\times 1$ 瓶颈卷积：**\n    -   输入通道数：$C_{\\text{in}, \\ell} = C_0 + (\\ell - 1)k$\n    -   输出通道数：$C_{\\text{out},1} = 4k$\n    -   卷积核尺寸：$K_1 = 1$\n    -   此操作的 FLOPs ($\\text{FLOPs}_{\\text{DN}, \\ell, 1}$)：\n        $$ \\text{FLOPs}_{\\text{DN}, \\ell, 1} = 2 H W C_{\\text{out},1} C_{\\text{in}, \\ell} K_1^2 = 2 H W (4k) (C_0 + (\\ell-1)k) (1^2) = 8HWk(C_0 + (\\ell-1)k) $$\n\n2.  **$3 \\times 3$ 卷积：**\n    -   输入通道数 (前一层的输出)：$C_{\\text{in},2} = 4k$\n    -   输出通道数：$C_{\\text{out},2} = k$\n    -   卷积核尺寸：$K_2 = 3$\n    -   此操作的 FLOPs ($\\text{FLOPs}_{\\text{DN}, \\ell, 2}$)：\n        $$ \\text{FLOPs}_{\\text{DN}, \\ell, 2} = 2 H W C_{\\text{out},2} C_{\\text{in},2} K_2^2 = 2 H W (k) (4k) (3^2) = 72HWk^2 $$\n\n第 $\\ell$ 层的总 FLOPs 是这两项成本之和：\n$$ \\text{FLOPs}_{\\text{DN}, \\ell} = \\text{FLOPs}_{\\text{DN}, \\ell, 1} + \\text{FLOPs}_{\\text{DN}, \\ell, 2} = 8HWk(C_0 + (\\ell-1)k) + 72HWk^2 $$\n\n为了求出该块的总 FLOPs，我们对所有 $L$ 层进行求和：\n$$ \\text{FLOPs}_{\\text{DN}} = \\sum_{\\ell=1}^{L} \\text{FLOPs}_{\\text{DN}, \\ell} = \\sum_{\\ell=1}^{L} \\left( 8HWk(C_0 + (\\ell-1)k) + 72HWk^2 \\right) $$\n我们可以将求和式中的各项分开：\n$$ \\text{FLOPs}_{\\text{DN}} = \\sum_{\\ell=1}^{L} 8HWkC_0 + \\sum_{\\ell=1}^{L} 8HWk^2(\\ell-1) + \\sum_{\\ell=1}^{L} 72HWk^2 $$\n计算每个求和式：\n-   $\\sum_{\\ell=1}^{L} 8HWkC_0 = L \\cdot 8HWkC_0 = 8LHWkC_0$\n-   $\\sum_{\\ell=1}^{L} 8HWk^2(\\ell-1) = 8HWk^2 \\sum_{\\ell=1}^{L} (\\ell-1) = 8HWk^2 \\sum_{j=0}^{L-1} j$。使用等差数列求和公式 $\\sum_{j=0}^{n} j = \\frac{n(n+1)}{2}$，当 $n=L-1$ 时，我们得到 $8HWk^2 \\frac{(L-1)L}{2} = 4L(L-1)HWk^2$。\n-   $\\sum_{\\ell=1}^{L} 72HWk^2 = L \\cdot 72HWk^2 = 72LHWk^2$\n\n将这些结果相加：\n$$ \\text{FLOPs}_{\\text{DN}} = 8LHWkC_0 + 4L(L-1)HWk^2 + 72LHWk^2 $$\n提取公因式：\n$$ \\text{FLOPs}_{\\text{DN}} = HWL \\left[ 8kC_0 + 4(L-1)k^2 + 72k^2 \\right] $$\n$$ \\text{FLOPs}_{\\text{DN}} = HWL \\left[ 8kC_0 + (4L-4)k^2 + 72k^2 \\right] $$\n$$ \\text{FLOPs}_{\\text{DN}} = HWL \\left[ 8kC_0 + (4L+68)k^2 \\right] $$\n$$ \\text{FLOPs}_{\\text{DN}} = 4HWLk \\left[ 2C_0 + (L+17)k \\right] $$\n\n### ResNet 块 FLOPs 的推导\n\n设 $\\text{FLOPs}_{\\text{RN}}$ 为 ResNet 块的总 FLOPs。该块有 $C_0$ 个输入通道，由三个卷积操作组成。最终输出有 $C_0 + Lk$ 个通道，以匹配 DenseNet 块的输出特征图深度。\n\n1.  **$1 \\times 1$ 降维卷积：**\n    -   输入通道数: $C_{\\text{in},1} = C_0$\n    -   输出通道数: $C_{\\text{out},1} = 4k$\n    -   卷积核尺寸: $K_1 = 1$\n    -   FLOPs: $\\text{FLOPs}_{\\text{RN}, 1} = 2 H W (4k) (C_0) (1^2) = 8HWkC_0$\n\n2.  **$3 \\times 3$ 卷积：**\n    -   输入通道数: $C_{\\text{in},2} = 4k$\n    -   输出通道数: $C_{\\text{out},2} = 4k$\n    -   卷积核尺寸: $K_2 = 3$\n    -   FLOPs: $\\text{FLOPs}_{\\text{RN}, 2} = 2 H W (4k) (4k) (3^2) = 2 H W (16k^2)(9) = 288HWk^2$\n\n3.  **$1 \\times 1$ 升维卷积：**\n    -   输入通道数: $C_{\\text{in},3} = 4k$\n    -   输出通道数: $C_{\\text{out},3} = C_0 + Lk$\n    -   卷积核尺寸: $K_3 = 1$\n    -   FLOPs: $\\text{FLOPs}_{\\text{RN}, 3} = 2 H W (C_0 + Lk) (4k) (1^2) = 8HWk(C_0 + Lk)$\n\nResNet 块的总 FLOPs 是这三项成本之和：\n$$ \\text{FLOPs}_{\\text{RN}} = \\text{FLOPs}_{\\text{RN}, 1} + \\text{FLOPs}_{\\text{RN}, 2} + \\text{FLOPs}_{\\text{RN}, 3} $$\n$$ \\text{FLOPs}_{\\text{RN}} = 8HWkC_0 + 288HWk^2 + 8HWk(C_0 + Lk) $$\n$$ \\text{FLOPs}_{\\text{RN}} = 8HWkC_0 + 288HWk^2 + 8HWkC_0 + 8HWLk^2 $$\n$$ \\text{FLOPs}_{\\text{RN}} = 16HWkC_0 + 288HWk^2 + 8HWLk^2 $$\n提取公因式：\n$$ \\text{FLOPs}_{\\text{RN}} = 8HWk \\left[ 2C_0 + 36k + Lk \\right] $$\n$$ \\text{FLOPs}_{\\text{RN}} = 8HWk \\left[ 2C_0 + (L+36)k \\right] $$\n\n### DenseNet 与 ResNet 的 FLOPs 比率\n\n比率 $R$ 定义为 $R = \\frac{\\text{FLOPs}_{\\text{DN}}}{\\text{FLOPs}_{\\text{RN}}}$。代入推导出的表达式：\n$$ R = \\frac{4HWLk \\left[ 2C_0 + (L+17)k \\right]}{8HWk \\left[ 2C_0 + (L+36)k \\right]} $$\n分子和分母中的公因式 $4HWk$ 可以消去：\n$$ R = \\frac{L \\left[ 2C_0 + (L+17)k \\right]}{2 \\left[ 2C_0 + (L+36)k \\right]} $$\n该表达式无法进一步简化。这是比率 $R$ 的最终解析表达式。", "answer": "$$\n\\boxed{\\frac{L(2C_0 + (L+17)k)}{2(2C_0 + (L+36)k)}}\n$$", "id": "3114885"}, {"introduction": "在设计神经网络时，一个常见的误区是认为“更多的参数等于更好的性能”。本练习引入了一个实用的模型，旨在探索“收益递减”这一重要概念。你将通过计算来量化增加增长率 $k$ 如何影响模型精度的提升，并分析其与参数成本增加之间的关系，这是设计高效网络架构时必须权衡的关键因素。[@problem_id:3114924]", "problem": "考虑一个来自密集连接卷积网络（Densely Connected Convolutional Network, DenseNet）的单一稠密块，其深度固定为 $L$ 层，增长率为每层 $k$ 个通道。设该块的输入具有 $k_0$ 个通道。网络使用二维卷积。您将分析总参数量如何随 $k$ 变化，以及在一个容量饱和模型下，随着 $k$ 的增加，期望准确率的行为如何，并因冗余和有限的空间分辨率而表现出收益递减。\n\n使用的基本原理和定义：\n- 一个内核大小为 $s \\times s$，将 $c_{\\text{in}}$ 个输入通道映射到 $c_{\\text{out}}$ 个输出通道的卷积，具有 $s^2 \\, c_{\\text{in}} \\, c_{\\text{out}}$ 个可学习权重。在本分析中可以忽略偏置项。\n- 在一个增长率为 $k$ 的 DenseNet 块中，第 $l$ 层（索引 $l \\in \\{1,\\dots,L\\}$）接收所有先前输出和初始特征的拼接作为输入，因此第 $l$ 层的输入通道数为 $c_{\\text{in}}(l) = k_0 + k\\,(l-1)$。\n- 块中的每一层都由一个产生 $k$ 个输出通道的 $3 \\times 3$ 卷积组成。\n\n用于将参数与期望准确率关联的建模假设：\n- 令 $P(k)$ 表示稠密块中卷积参数总数，作为 $k$ 的函数。\n- 由于特征冗余随增长率增加而增加，定义有效参数量为 $P_{\\text{eff}}(k) = \\dfrac{P(k)}{1 + r\\,(k-1)}$，其中 $r$ 是一个非负冗余系数。\n- 由于有限的空间分辨率和有限的任务复杂度，将期望准确率建模为对有效容量的饱和响应：\n$$\nA\\big(P_{\\text{eff}}(k)\\big) \\;=\\; A_{\\min} \\;+\\; \\big(A_{\\max} - A_{\\min}\\big)\\,\\Big(1 - e^{-\\,P_{\\text{eff}}(k)\\,/\\,P_{\\text{sat}}}\\Big),\n$$\n其中 $A_{\\min}$ 和 $A_{\\max}$ 是准确率的下界和上界（以小数表示），$P_{\\text{sat}}$ 是一个控制 e 倍饱和点的正尺度参数。\n\n任务：\n1) 仅从上述基本定义出发，推导稠密块总参数量 $P(k)$ 作为 $L$、$k_0$ 和 $k$ 的函数的闭式表达式。\n2) 使用经冗余调整的有效参数量 $P_{\\text{eff}}(k)$ 和 $A\\big(\\cdot\\big)$ 的饱和模型，为每个指定的 $k$ 计算期望准确率 $A(k)$。\n3) 将两个增长率 $k_1$ 和 $k_2$（其中 $k_2  k_1$）之间的参数-准确率边际效率定义为\n$$\n\\mathcal{E}\\big(k_1 \\rightarrow k_2\\big) \\;=\\; \\dfrac{A(k_2) - A(k_1)}{P(k_2) - P(k_1)}.\n$$\n在提供的有序测试集中，对连续的 $k$ 计算此边际效率。\n\n使用以下固定常数和测试集：\n- 深度 $L = 12$。\n- 初始通道数 $k_0 = 24$。\n- 冗余系数 $r = 0.02$。\n- 饱和尺度 $P_{\\text{sat}} = 200000$。\n- 准确率边界 $A_{\\min} = 0.4$ 和 $A_{\\max} = 0.9$。\n- 增长率的有序测试集 $k \\in \\{4, 8, 16, 32, 64\\}$。\n\n计算要求：\n- 对于主要分析集 $k \\in \\{8, 16, 32\\}$，计算准确率 $A(8)$、$A(16)$ 和 $A(32)$，每个值都作为 $[0,1]$ 区间内的小数，并四舍五入到六位小数。\n- 为覆盖边界和边缘行为，也请计算有序测试集 $\\{4 \\rightarrow 8,\\, 8 \\rightarrow 16,\\, 16 \\rightarrow 32,\\, 32 \\rightarrow 64\\}$ 中连续对的边际效率，每个值都四舍五入到六位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个含有两个子列表的列表：\n  - 第一个子列表是 $[A(8), A(16), A(32)]$。\n  - 第二个子列表是 $[E(4 \\rightarrow 8), E(8 \\rightarrow 16), E(16 \\rightarrow 32), E(32 \\rightarrow 64)]$。\n- 最终打印的行必须严格遵循以下格式\n$[[A(8), A(16), A(32)],[E(4 \\rightarrow 8), E(8 \\rightarrow 16), E(16 \\rightarrow 32), E(32 \\rightarrow 64)]]$\n其中每个 $A(\\cdot)$ 和 $E(\\cdot)$ 是一个四舍五入到六位小数的实数，以小数形式表示（无百分号）。", "solution": "问题陈述已经过分析并被认为是有效的。这是一个在深度学习领域内，特别是关于 DenseNets 架构分析的适定、自洽且有科学依据的问题。所有定义、常数和目标都清晰明确，未发现矛盾、歧义或事实性错误。参数冗余和准确率饱和的建模，虽然基于简化假设，但代表了一种用于神经网络行为理论分析的有效且标准的方法。\n\n解决方案按要求分两部分进行。首先，我们推导总参数量的闭式表达式。其次，我们概述计算过程，以根据所提供的模型确定期望准确率和边际效率。\n\n**第一部分：总参数量 $P(k)$ 的推导**\n\n稠密块中的总参数数量 $P(k)$ 是其 $L$ 层中每一层参数的总和。我们首先定义单层 $l$ 的参数数量。\n\n根据问题陈述，单个卷积层中的可学习权重数量由 $s^2 \\, c_{\\text{in}} \\, c_{\\text{out}}$ 给出，其中 $s$ 是内核大小，$c_{\\text{in}}$ 是输入通道数，$c_{\\text{out}}$ 是输出通道数。偏置项被忽略。\n\n对于我们稠密块中的第 $l$ 层，其中 $l \\in \\{1, 2, \\dots, L\\}$：\n- 内核大小指定为 $3 \\times 3$，因此 $s = 3$。\n- 每层生成的输出通道数是增长率，因此 $c_{\\text{out}} = k$。\n- 输入通道数 $c_{\\text{in}}(l)$ 是初始块输入（有 $k_0$ 个通道）和所有前面 $l-1$ 层输出的拼接，每层贡献 $k$ 个通道。因此，$c_{\\text{in}}(l) = k_0 + k(l-1)$。\n\n第 $l$ 层的参数数量，表示为 $P_l(k)$，是：\n$$\nP_l(k) = s^2 \\cdot c_{\\text{in}}(l) \\cdot c_{\\text{out}} = 3^2 \\cdot \\big(k_0 + k(l-1)\\big) \\cdot k = 9k\\big(k_0 + k(l-1)\\big)\n$$\n\n整个块的总参数量 $P(k)$ 是所有 $L$ 层参数的总和：\n$$\nP(k) = \\sum_{l=1}^{L} P_l(k) = \\sum_{l=1}^{L} 9k\\big(k_0 + k(l-1)\\big)\n$$\n\n我们可以从求和中提出常数并分离各项：\n$$\nP(k) = 9k \\left( \\sum_{l=1}^{L} k_0 + \\sum_{l=1}^{L} k(l-1) \\right) = 9k \\left( k_0 \\sum_{l=1}^{L} 1 + k \\sum_{l=1}^{L} (l-1) \\right)\n$$\n\n这两个和的计算如下：\n- 第一个和是 $\\sum_{l=1}^{L} 1 = L$。\n- 第二个和是一个等差数列的和：$\\sum_{l=1}^{L} (l-1) = 0 + 1 + 2 + \\dots + (L-1)$。前 $n$ 个非负整数的和为 $\\frac{n(n+1)}{2}$。当 $n = L-1$ 时，和为 $\\frac{(L-1)L}{2}$。\n\n将这些结果代回 $P(k)$ 的表达式中：\n$$\nP(k) = 9k \\left( k_0 L + k \\frac{L(L-1)}{2} \\right)\n$$\n\n展开此式，得到总参数量作为 $L$、$k_0$ 和 $k$ 的函数的最终闭式表达式：\n$$\nP(k) = 9Lk_0k + \\frac{9}{2}L(L-1)k^2\n$$\n\n**第二部分：准确率和边际效率的计算**\n\n使用推导出的 $P(k)$ 公式和提供的模型，我们可以计算所需的指标。对于给定的增长率 $k$，其过程包括以下步骤：\n1.  使用常数 $L=12$ 和 $k_0=24$ 计算总参数量 $P(k)$：\n    $$ P(k) = 9(12)(24)k + \\frac{9}{2}(12)(11)k^2 = 2592k + 594k^2 $$\n2.  使用冗余系数 $r=0.02$ 计算有效参数量 $P_{\\text{eff}}(k)$：\n    $$ P_{\\text{eff}}(k) = \\frac{P(k)}{1 + r(k-1)} = \\frac{2592k + 594k^2}{1 + 0.02(k-1)} $$\n3.  使用饱和模型及参数 $A_{\\min}=0.4$、$A_{\\max}=0.9$ 和 $P_{\\text{sat}}=200000$ 计算期望准确率 $A(k)$：\n    $$ A(k) = 0.4 + (0.9 - 0.4) \\left( 1 - e^{-P_{\\text{eff}}(k) / P_{\\text{sat}}} \\right) = 0.4 + 0.5 \\left( 1 - e^{-P_{\\text{eff}}(k) / 200000} \\right) $$\n\n首先对有序测试集 $\\{4, 8, 16, 32, 64\\}$ 中的所有 $k$ 值执行这些步骤，以获得相应的 $P(k)$ 和 $A(k)$ 值。\n\n然后，组装所需的输出：\n- 提取准确率 $A(8)$、$A(16)$ 和 $A(32)$，并四舍五入到六位小数。\n- 参数-准确率边际效率 $\\mathcal{E}(k_1 \\rightarrow k_2)$ 使用以下公式对测试集中的连续对进行计算：\n    $$ \\mathcal{E}(k_1 \\rightarrow k_2) = \\frac{A(k_2) - A(k_1)}{P(k_2) - P(k_1)} $$\n    对 $4 \\rightarrow 8$、$8 \\rightarrow 16$、$16 \\rightarrow 32$ 和 $32 \\rightarrow 64$ 这些配对计算效率，每个结果都四舍五入到六位小数。\n\n以下 Python 代码实现了这个完整的计算过程，并按规定格式化输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DenseNet analysis problem by deriving parameter counts,\n    accuracies, and marginal efficiencies.\n    \"\"\"\n    \n    # Define fixed constants from the problem statement\n    L = 12\n    k0 = 24\n    r = 0.02\n    P_sat = 200000.0\n    A_min = 0.4\n    A_max = 0.9\n\n    # Ordered test suite for a growth rate k\n    k_suite = [4, 8, 16, 32, 64]\n\n    def calculate_metrics(k):\n        \"\"\"\n        Calculates P(k), P_eff(k), and A(k) for a given growth rate k.\n        \n        Args:\n            k (int): The growth rate (number of channels per layer).\n            \n        Returns:\n            tuple: A tuple containing (P(k), A(k)).\n        \"\"\"\n        # Calculate total parameters P(k) using the derived closed-form expression:\n        # P(k) = 9*L*k0*k + 4.5*L*(L-1)*k^2\n        P_k = 9 * L * k0 * k + 4.5 * L * (L - 1) * k**2\n        \n        # Calculate effective parameters P_eff(k)\n        # For k=1, denominator is 1. Prevent division by zero if k=1 is ever used.\n        if k == 1:\n            P_eff_k = P_k\n        else:\n            P_eff_k = P_k / (1 + r * (k - 1))\n        \n        # Calculate expected accuracy A(k) using the saturation model\n        exponent = -P_eff_k / P_sat\n        A_k = A_min + (A_max - A_min) * (1 - np.exp(exponent))\n        \n        return P_k, A_k\n\n    # Store results for P(k) and A(k) for each k in the test suite\n    results = {}\n    for k_val in k_suite:\n        p_val, a_val = calculate_metrics(k_val)\n        results[k_val] = {'p': p_val, 'a': a_val}\n\n    # Task 1: Compute the accuracies A(8), A(16), A(32)\n    accuracies_to_compute = [8, 16, 32]\n    accuracy_results = [round(results[k]['a'], 6) for k in accuracies_to_compute]\n    \n    # Task 2: Compute the marginal efficiencies for consecutive pairs\n    efficiency_results = []\n    for i in range(len(k_suite) - 1):\n        k1 = k_suite[i]\n        k2 = k_suite[i+1]\n        \n        A1 = results[k1]['a']\n        A2 = results[k2]['a']\n        P1 = results[k1]['p']\n        P2 = results[k2]['p']\n        \n        delta_A = A2 - A1\n        delta_P = P2 - P1\n        \n        if delta_P == 0:\n            efficiency = 0.0\n        else:\n            efficiency = delta_A / delta_P\n            \n        efficiency_results.append(round(efficiency, 8)) # Use higher precision for rounding small numbers\n\n    # Final print statement in the exact required format.\n    acc_str = ', '.join(f\"{val:.6f}\" for val in accuracy_results)\n    \n    # Format efficiencies to 6 decimal places as requested, which may result in 0.000000\n    eff_str = ', '.join(f\"{val:.6f}\" for val in [\n        (results[8]['a'] - results[4]['a']) / (results[8]['p'] - results[4]['p']),\n        (results[16]['a'] - results[8]['a']) / (results[16]['p'] - results[8]['p']),\n        (results[32]['a'] - results[16]['a']) / (results[32]['p'] - results[16]['p']),\n        (results[64]['a'] - results[32]['a']) / (results[64]['p'] - results[32]['p']),\n    ])\n\n    final_accuracy_list = [f\"{x:.6f}\" for x in accuracy_results]\n    \n    raw_efficiencies = [\n        (results[8]['a'] - results[4]['a']) / (results[8]['p'] - results[4]['p']),\n        (results[16]['a'] - results[8]['a']) / (results[16]['p'] - results[8]['p']),\n        (results[32]['a'] - results[16]['a']) / (results[32]['p'] - results[16]['p']),\n        (results[64]['a'] - results[32]['a']) / (results[64]['p'] - results[32]['p']),\n    ]\n    final_efficiency_list = [f\"{x:.6f}\" for x in raw_efficiencies]\n    \n    print(f\"[[{', '.join(final_accuracy_list)}], [{', '.join(final_efficiency_list)}]]\")\n\nsolve()\n\n```", "id": "3114924"}, {"introduction": "一个优秀的架构设计不仅在于宏观的模块连接，更在于其中微妙而关键的实现细节。本练习聚焦于密集块（Dense Block）中的一个核心问题：批量归一化（Batch Normalization）层应置于特征拼接之前还是之后。你将通过理论分析和编程实践，揭示这一决策如何影响特征的统计分布，并最终影响网络的训练稳定性。[@problem_id:3114019]", "problem": "考虑一个稠密块，它沿着通道维度拼接特征图，这是稠密卷积网络 (DenseNet) 风格架构的一个典型模式。设来自先前层的通道集合表示为 $A$，包含 $c_A$ 个通道；新生成的通道集合表示为 $B$，包含 $c_B$ 个通道。对于每个通道 $j$，将其归一化前的激活值建模为一个随机变量 $x_j$，其均值为 $\\mu_j$，方差为 $\\sigma_j^2$。假设通道之间相互独立，并且统计量在整个批次上是平稳的。我们研究在拼接操作前后放置批归一化 (BN) 的两种策略：\n- 策略 $S_1$ (拼接前BN)：仅对新集合 $B$ 应用BN，然后将 $A$ 和归一化后的 $B$ 拼接起来。\n- 策略 $S_2$ (拼接后BN)：首先拼接 $A$ 和 $B$，然后对所有拼接后的通道应用一个BN层。\n\n使用以下基本定义：\n- 对单个通道 $x$ 进行批归一化 (BN)，其参数为 $\\gamma$ 和 $\\beta$，以及一个很小的数值稳定器 $\\varepsilon0$，计算公式如下：\n$$\n\\mathrm{BN}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta,\n$$\n其中 $\\mu$ 和 $\\sigma^2$ 是该通道的批估计均值和方差。在本问题中，假设 $\\gamma=1$ 和 $\\beta=0$，因此输出纯粹由批统计量和 $\\varepsilon$ 进行中心化和缩放。\n- 沿通道的拼接是一种纯粹的结构性操作，它本身不会改变每个通道的统计数据；它只是将来自 $A$ 和 $B$ 的通道聚合成一个单一的张量。\n\n任务：\n1. 根据 BN 定义和独立性假设，推导出在策略 $S_1$ 和 $S_2$ 下，每个通道归一化后的均值和方差，用给定的归一化前均值 $\\mu_j$、方差 $\\sigma_j^2$ 和 $\\varepsilon$ 表示。你的推导必须仅使用上述基本定义以及关于拼接和逐通道BN如何相互作用的逻辑。\n2. 定义以下两个标量指标，用于概括归一化步骤后的跨通道异质性：\n   - 跨通道最大绝对均值\n   $$\n   M_{\\max} \\;=\\; \\max_{j} \\left| \\mathbb{E}[y_j] \\right|,\n   $$\n   其中 $y_j$ 是通道 $j$ 归一化后的激活值。\n   - 跨通道方差均匀性比率\n   $$\n   R_{\\mathrm{var}} \\;=\\; \\frac{\\max_{j} \\mathrm{Var}(y_j)}{\\min_{j} \\mathrm{Var}(y_j)}.\n   $$\n3. 实现一个程序，对于下面的每个测试用例，计算并输出包含四个浮点数的元组：\n$$\n\\left[ M_{\\max}^{(S_1)}, \\; R_{\\mathrm{var}}^{(S_1)}, \\; M_{\\max}^{(S_2)}, \\; R_{\\mathrm{var}}^{(S_2)} \\right],\n$$\n其中 $M_{\\max}^{(S_k)}$ 和 $R_{\\mathrm{var}}^{(S_k)}$ 表示策略 $S_k$ 下的指标。\n\n重要实现细节：\n- 将BN统计量视为与为每个通道提供的 $\\mu_j$ 和 $\\sigma_j^2$ 完全相等（即没有估计误差），并在给定的测试用例中对所有通道使用提供的 $\\varepsilon$。\n- 在策略 $S_1$ 下，仅对 $B$ 中的通道应用BN；$A$ 中的通道保持不变。在策略 $S_2$ 下，在拼接后对 $A$ 和 $B$ 中的所有通道应用BN。\n- 你的程序必须生成单行输出，其中包含所有测试用例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个测试用例的结果本身是如上所述的四个浮点数列表。例如，如果有 $T$ 个测试用例，第 $t$ 个结果是列表 $r_t$，则最终输出格式必须是\n$$\n[ r_1, r_2, \\dots, r_T ].\n$$\n\n测试套件：\n精确使用以下四个测试用例，每个用例由 $(\\mu_A, \\sigma_A^2, \\mu_B, \\sigma_B^2, \\varepsilon)$ 定义，其中 $\\mu_A$ 和 $\\sigma_A^2$ 是长度为 $c_A$ 的列表，$\\mu_B$ 和 $\\sigma_B^2$ 是长度为 $c_B$ 的列表。提供的每个数字都必须按原样使用。\n\n- 测试用例 1 (理想情况，先前已归一化的 $A$ 和未归一化的 $B$)：\n  - $\\mu_A = [\\, 0.0, \\; 0.0, \\; 0.0 \\,]$\n  - $\\sigma_A^2 = [\\, 1.0, \\; 1.0, \\; 1.0 \\,]$\n  - $\\mu_B = [\\, 3.0, \\; -2.0 \\,]$\n  - $\\sigma_B^2 = [\\, 4.0, \\; 0.5 \\,]$\n  - $\\varepsilon = 10^{-5}$\n\n- 测试用例 2 (异质的 $A$ 和 $B$)：\n  - $\\mu_A = [\\, 1.5, \\; -0.5 \\,]$\n  - $\\sigma_A^2 = [\\, 2.0, \\; 0.2 \\,]$\n  - $\\mu_B = [\\, 0.0, \\; 0.0, \\; 0.0 \\,]$\n  - $\\sigma_B^2 = [\\, 1.0, \\; 5.0, \\; 0.01 \\,]$\n  - $\\varepsilon = 10^{-3}$\n\n- 测试用例 3 (极小方差的边缘情况)：\n  - $\\mu_A = [\\, 0.0 \\,]$\n  - $\\sigma_A^2 = [\\, 10^{-6} \\,]$\n  - $\\mu_B = [\\, 0.0 \\,]$\n  - $\\sigma_B^2 = [\\, 10^{-6} \\,]$\n  - $\\varepsilon = 10^{-6}$\n\n- 测试用例 4 (大方差不匹配)：\n  - $\\mu_A = [\\, 5.0, \\; -3.0, \\; 1.0 \\,]$\n  - $\\sigma_A^2 = [\\, 100.0, \\; 0.1, \\; 10.0 \\,]$\n  - $\\mu_B = [\\, 2.0 \\,]$\n  - $\\sigma_B^2 = [\\, 50.0 \\,]$\n  - $\\varepsilon = 10^{-4}$\n\n你的程序必须为每个测试用例计算这四个指标，并以精确的格式打印单行输出\n$$\n[ [M_{\\max}^{(S_1)}, R_{\\mathrm{var}}^{(S_1)}, M_{\\max}^{(S_2)}, R_{\\mathrm{var}}^{(S_2)}], \\; \\dots ]\n$$\n每个测试用例一个带括号的列表，顺序与上面提供的一致。", "solution": "此问题被判定为有效。它在科学上基于深度学习和统计学原理，问题定义明确，每个测试用例都有唯一解，并且表述客观。解决问题所需的所有数据和定义均已提供且一致。我们可以开始求解。\n\n### 1. 归一化后统计量的推导\n\n分析依赖于期望和方差的基本性质。对于一个随机变量 $X$ 和常量 $a$ 、 $b$：\n- 期望的线性性质： $\\mathbb{E}[aX + b] = a\\mathbb{E}[X] + b$\n- 方差的性质： $\\mathrm{Var}(aX + b) = a^2 \\mathrm{Var}(X)$\n\n问题给出了单个通道激活值 $x$ 的批归一化 (BN) 变换如下：\n$$\n\\mathrm{BN}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta\n$$\n在约束条件 $\\gamma=1$ 和 $\\beta=0$下，上式简化为：\n$$\ny = \\mathrm{BN}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n$$\n其中 $y$ 是归一化后的激活值，$\\mu = \\mathbb{E}[x]$ 和 $\\sigma^2 = \\mathrm{Var}(x)$ 分别是该通道归一化前的均值和方差。\n\n我们基于此定义分析 $S_1$ 和 $S_2$ 这两种策略。\n\n#### 策略 $S_1$：拼接前BN\n\n在此策略中，只有集合 $B$ 中的新通道被归一化。集合 $A$ 中的通道未经修改直接拼接。\n\n- **对于集合 $A$ 中的通道 $j$**：\n激活值没有改变。设 $x_j$ 为归一化前的激活值，$y_j$ 为归一化后的激活值。\n$$\ny_j = x_j\n$$\n因此，归一化后的统计量与归一化前的统计量相同：\n$$\n\\mathbb{E}[y_j] = \\mathbb{E}[x_j] = \\mu_j\n$$\n$$\n\\mathrm{Var}(y_j) = \\mathrm{Var}(x_j) = \\sigma_j^2\n$$\n\n- **对于集合 $B$ 中的通道 $j$**：\n激活值通过BN进行变换。设通道 $j \\in B$ 的归一化前激活值为 $x_j$，其均值为 $\\mu_j$，方差为 $\\sigma_j^2$。归一化后的激活值为：\n$$\ny_j = \\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\n$$\n我们推导 $y_j$ 的均值和方差：\n均值为：\n$$\n\\mathbb{E}[y_j] = \\mathbb{E}\\left[\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} \\mathbb{E}[x_j - \\mu_j]\n$$\n根据期望的线性性质，$\\mathbb{E}[x_j - \\mu_j] = \\mathbb{E}[x_j] - \\mu_j = \\mu_j - \\mu_j = 0$。\n$$\n\\mathbb{E}[y_j] = 0\n$$\n方差为：\n$$\n\\mathrm{Var}(y_j) = \\mathrm{Var}\\left(\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right) = \\left(\\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right)^2 \\mathrm{Var}(x_j - \\mu_j)\n$$\n因为对于任意常数 $c$，有 $\\mathrm{Var}(X-c) = \\mathrm{Var}(X)$，所以我们得到 $\\mathrm{Var}(x_j - \\mu_j) = \\mathrm{Var}(x_j) = \\sigma_j^2$。\n$$\n\\mathrm{Var}(y_j) = \\frac{1}{\\sigma_j^2 + \\varepsilon} \\cdot \\sigma_j^2 = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\n$$\n\n#### 策略 $S_2$：拼接后BN\n\n在此策略中，首先拼接集合 $A$ 和 $B$ 的通道。然后，对所有通道应用一个BN层。问题指明BN是逐通道操作，意味着拼接后的集合 $A \\cup B$ 中的每个通道 $j$ 都使用其自身的统计量 $\\mu_j$ 和 $\\sigma_j^2$ 进行归一化。\n\n- **对于拼接后集合 $A \\cup B$ 中的任意通道 $j$**：\n归一化后的激活值 $y_j$ 由下式给出：\n$$\ny_j = \\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\n$$\n$y_j$ 的均值和方差的推导过程与策略 $S_1$ 下集合 $B$ 中通道的推导过程完全相同。\n均值为：\n$$\n\\mathbb{E}[y_j] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} \\mathbb{E}[x_j - \\mu_j] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} (\\mu_j - \\mu_j) = 0\n$$\n方差为：\n$$\n\\mathrm{Var}(y_j) = \\left(\\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right)^2 \\mathrm{Var}(x_j) = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\n$$\n这对所有通道都成立，无论它们是源自集合 $A$ 还是集合 $B$。\n\n### 2. 跨通道指标公式\n\n使用推导出的统计量，我们可以为指标 $M_{\\max}$ 和 $R_{\\mathrm{var}}$ 构建表达式。\n\n#### 策略 $S_1$ 的指标\n\n归一化后的均值集合是 $\\{\\mu_j\\}_{j \\in A} \\cup \\{0\\}_{j \\in B}$。\n最大绝对均值 $M_{\\max}^{(S_1)}$ 是：\n$$\nM_{\\max}^{(S_1)} = \\max_{j \\in A \\cup B} |\\mathbb{E}[y_j]| = \\max(\\max_{j \\in A} |\\mu_j|, \\max_{j \\in B} |0|) = \\max_{j \\in A} |\\mu_j|\n$$\n如果集合 $A$ 为空，则 $M_{\\max}^{(S_1)} = 0$。\n\n归一化后的方差集合是 $\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B}$。\n方差均匀性比率 $R_{\\mathrm{var}}^{(S_1)}$ 是：\n$$\nR_{\\mathrm{var}}^{(S_1)} = \\frac{\\max(\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B})}{\\min(\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B})}\n$$\n\n#### 策略 $S_2$ 的指标\n\n每个通道 $j \\in A \\cup B$ 归一化后的均值为 $0$。\n最大绝对均值 $M_{\\max}^{(S_2)}$ 是：\n$$\nM_{\\max}^{(S_2)} = \\max_{j \\in A \\cup B} |\\mathbb{E}[y_j]| = \\max_{j \\in A \\cup B} |0| = 0\n$$\n\n归一化后的方差集合是 $\\{\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\}_{j \\in A \\cup B}$。\n方差均匀性比率 $R_{\\mathrm{var}}^{(S_2)}$ 是：\n$$\nR_{\\mathrm{var}}^{(S_2)} = \\frac{\\max_{j \\in A \\cup B} \\left(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\right)}{\\min_{j \\in A \\cup B} \\left(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\right)}\n$$\n函数 $f(v) = \\frac{v}{v + \\varepsilon}$ 在 $v \\ge 0$ 时是单调递增的。因此，变换后方差的最大值和最小值对应于原始方差 $\\sigma_j^2$ 的最大值和最小值。\n$$\nR_{\\mathrm{var}}^{(S_2)} = \\frac{\\frac{\\max_{j \\in A \\cup B} \\sigma_j^2}{\\max_{j \\in A \\cup B} \\sigma_j^2 + \\varepsilon}}{\\frac{\\min_{j \\in A \\cup B} \\sigma_j^2}{\\min_{j \\in A \\cup B} \\sigma_j^2 + \\varepsilon}}\n$$\n这些公式现在可以用于实现了。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the metrics for cross-channel heterogeneity\n    under two different batch normalization strategies in a dense block.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            [0.0, 0.0, 0.0],  # mu_A\n            [1.0, 1.0, 1.0],  # sigma_A^2\n            [3.0, -2.0],      # mu_B\n            [4.0, 0.5],       # sigma_B^2\n            1e-5              # epsilon\n        ),\n        # Test case 2\n        (\n            [1.5, -0.5],      # mu_A\n            [2.0, 0.2],       # sigma_A^2\n            [0.0, 0.0, 0.0],  # mu_B\n            [1.0, 5.0, 0.01], # sigma_B^2\n            1e-3              # epsilon\n        ),\n        # Test case 3\n        (\n            [0.0],            # mu_A\n            [1e-6],           # sigma_A^2\n            [0.0],            # mu_B\n            [1e-6],           # sigma_B^2\n            1e-6              # epsilon\n        ),\n        # Test case 4\n        (\n            [5.0, -3.0, 1.0], # mu_A\n            [100.0, 0.1, 10.0],# sigma_A^2\n            [2.0],            # mu_B\n            [50.0],           # sigma_B^2\n            1e-4              # epsilon\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_A, sigma_A_sq, mu_B, sigma_B_sq, epsilon = case\n\n        # --- Strategy S1: BN-before-concatenation ---\n        \n        # Means: Channels from A are unchanged, channels from B are centered to 0.\n        # M_max is the max absolute mean from the un-normalized channels in A.\n        if mu_A:\n            M_max_s1 = np.max(np.abs(np.array(mu_A)))\n        else:\n            M_max_s1 = 0.0\n            \n        # Variances: Channels from A are unchanged, channels from B are scaled.\n        post_vars_A_s1 = np.array(sigma_A_sq)\n        post_vars_B_s1 = np.array([s2 / (s2 + epsilon) for s2 in sigma_B_sq])\n        \n        all_post_vars_s1 = np.concatenate((post_vars_A_s1, post_vars_B_s1))\n        \n        # R_var is the ratio of max to min variance across all channels.\n        if all_post_vars_s1.size > 0:\n            min_var_s1 = np.min(all_post_vars_s1)\n            max_var_s1 = np.max(all_post_vars_s1)\n            # Avoid division by zero, though problem constraints ensure min_var > 0.\n            R_var_s1 = max_var_s1 / min_var_s1 if min_var_s1 > 0 else float('inf')\n        else:\n            R_var_s1 = 1.0 # No variance heterogeneity if there are no channels.\n\n        # --- Strategy S2: BN-after-concatenation ---\n\n        # Means: All channels are normalized, so all means are 0.\n        M_max_s2 = 0.0\n        \n        # Variances: All channels are normalized.\n        all_pre_vars_s2 = np.array(sigma_A_sq + sigma_B_sq)\n        \n        all_post_vars_s2 = np.array([s2 / (s2 + epsilon) for s2 in all_pre_vars_s2])\n        \n        # R_var is the ratio of max to min variance across all channels.\n        if all_post_vars_s2.size > 0:\n            min_var_s2 = np.min(all_post_vars_s2)\n            max_var_s2 = np.max(all_post_vars_s2)\n            R_var_s2 = max_var_s2 / min_var_s2 if min_var_s2 > 0 else float('inf')\n        else:\n            R_var_s2 = 1.0\n            \n        result = [M_max_s1, R_var_s1, M_max_s2, R_var_s2]\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() for a list already includes spaces, e.g., '[1.0, 2.0]'.\n    # Joining these with a comma results in '...,[...', which when wrapped\n    # in brackets becomes '[[...],[...]]', matching the required format.\n    print(f\"[{', '.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3114019"}]}