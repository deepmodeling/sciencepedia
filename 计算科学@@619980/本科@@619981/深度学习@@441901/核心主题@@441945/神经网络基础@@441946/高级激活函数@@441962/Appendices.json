{"hands_on_practices": [{"introduction": "激活函数的选择并非随心所欲，其数学形式决定了信息（如信号的均值和方差）如何在深度网络中传播。这项练习将深入探讨修正线性单元（ReLU）和高斯误差线性单元（GELU）这两个基石激活函数的统计特性。通过为标准高斯输入 $X \\sim \\mathcal{N}(0,1)$ 推导其输出的均值 $E[\\phi(X)]$ 和方差 $\\operatorname{Var}[\\phi(X)]$，你将从第一性原理层面理解它们对信号统计特性的影响，而这是设计稳定深度神经网络的关键因素 [@problem_id:3097811]。", "problem": "设 $X$ 表示一个从标准高斯分布 $X \\sim \\mathcal{N}(0,1)$ 中独立同分布抽取的标量预激活值。考虑两个激活函数：由 $\\phi_{\\mathrm{ReLU}}(x) = \\max(0,x)$ 定义的修正线性单元 (ReLU) 和由 $\\phi_{\\mathrm{GELU}}(x) = x \\, \\Phi(x)$ 定义的高斯误差线性单元 (GELU)，其中 $\\Phi(x)$ 是标准正态分布的累积分布函数。仅使用概率论和积分的第一性原理，通过计算每种激活的均值 $E[\\phi(X)]$ 和方差 $\\operatorname{Var}[\\phi(X)]$，来推导这些激活下的输出分布。然后，对于批量归一化 (BN)，其定义为 $y = \\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta$，其中总体统计量为 $\\mu = E[\\phi(X)]$ 和 $\\sigma^{2} = \\operatorname{Var}[\\phi(X)]$，确定在无限批量大小的极限情况下，能使输出 $y$ 达到零均值和单位方差的仿射参数 $(\\gamma^{\\ast}, \\beta^{\\ast})$。您的推导必须从期望和方差的定义、标准正态概率密度函数和累积分布函数，以及联合高斯随机变量的基本性质出发。请将您的最终答案表示为闭式解析表达式。无需进行四舍五入。请按顺序 $\\big(E[\\phi_{\\mathrm{ReLU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)], E[\\phi_{\\mathrm{GELU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)], \\gamma^{\\ast}, \\beta^{\\ast}\\big)$ 提供最终的元组。", "solution": "该问题被评估为有效的，因为它具有科学依据、问题良定、客观，并包含得出唯一解所需的所有必要信息。我们开始进行推导。\n\n设 $X$ 是一个服从标准正态分布的随机变量，$X \\sim \\mathcal{N}(0,1)$。其概率密度函数 (PDF) 为 $p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$，其累积分布函数 (CDF) 为 $\\Phi(x) = \\int_{-\\infty}^{x} p(t) dt$。对于函数 $g(X)$，期望定义为 $E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) p(x) dx$，方差为 $\\operatorname{Var}[g(X)] = E[g(X)^2] - (E[g(X)])^2$。\n\n**第1部分：ReLU 激活的统计特性**\n\n修正线性单元 (ReLU) 定义为 $\\phi_{\\mathrm{ReLU}}(x) = \\max(0,x)$。\n\n**1.1. ReLU 输出的均值：**\n期望 $E[\\phi_{\\mathrm{ReLU}}(X)]$ 计算如下：\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\int_{-\\infty}^{\\infty} \\max(0,x) p(x) dx = \\int_{0}^{\\infty} x p(x) dx$$\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\int_{0}^{\\infty} x \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx$$\n我们进行换元，令 $u = \\frac{x^2}{2}$，这意味着 $du = x dx$。积分上下限仍为从 $0$ 到 $\\infty$。\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\exp(-u) du = \\frac{1}{\\sqrt{2\\pi}} [-\\exp(-u)]_{0}^{\\infty} = \\frac{1}{\\sqrt{2\\pi}} (-0 - (-1)) = \\frac{1}{\\sqrt{2\\pi}}$$\n\n**1.2. ReLU 输出的方差：**\n首先，我们计算二阶矩 $E[(\\phi_{\\mathrm{ReLU}}(X))^2]$。\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\int_{-\\infty}^{\\infty} (\\max(0,x))^2 p(x) dx = \\int_{0}^{\\infty} x^2 p(x) dx$$\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\int_{0}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx$$\n全积分 $\\int_{-\\infty}^{\\infty} x^2 p(x) dx$ 对应于 $X$ 的二阶矩，即 $E[X^2] = \\operatorname{Var}[X] + (E[X])^2 = 1 + 0^2 = 1$。被积函数 $x^2 p(x)$ 是一个偶函数，因此从 $0$ 到 $\\infty$ 的积分是从 $-\\infty$ 到 $\\infty$ 积分的一半。\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\frac{1}{2} \\int_{-\\infty}^{\\infty} x^2 p(x) dx = \\frac{1}{2} E[X^2] = \\frac{1}{2}$$\n那么方差为：\n$$\\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)] = E[(\\phi_{\\mathrm{ReLU}}(X))^2] - (E[\\phi_{\\mathrm{ReLU}}(X)])^2 = \\frac{1}{2} - \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2 = \\frac{1}{2} - \\frac{1}{2\\pi} = \\frac{\\pi - 1}{2\\pi}$$\n\n**第2部分：GELU 激活的统计特性**\n\n高斯误差线性单元 (GELU) 定义为 $\\phi_{\\mathrm{GELU}}(x) = x \\Phi(x)$。我们将使用斯坦因引理 (Stein's Lemma)，这是高斯变量的一个基本性质，其内容为：对于 $X \\sim \\mathcal{N}(0,1)$ 和一个可微函数 $g$，$E[X g(X)] = E[g'(X)]$。\n\n**2.1. GELU 输出的均值：**\n我们想计算 $E[\\phi_{\\mathrm{GELU}}(X)] = E[X \\Phi(X)]$。令 $g(x) = \\Phi(x)$。其导数为 $g'(x) = p(x)$。应用斯坦因引理：\n$$E[X \\Phi(X)] = E[p(X)] = \\int_{-\\infty}^{\\infty} p(x) p(x) dx = \\int_{-\\infty}^{\\infty} p(x)^2 dx$$\n$$E[\\phi_{\\mathrm{GELU}}(X)] = \\int_{-\\infty}^{\\infty} \\left(\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\\right)^2 dx = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-x^2) dx$$\n使用高斯积分结果 $\\int_{-\\infty}^{\\infty} \\exp(-ax^2) dx = \\sqrt{\\frac{\\pi}{a}}$，其中 $a=1$：\n$$E[\\phi_{\\mathrm{GELU}}(X)] = \\frac{1}{2\\pi} \\sqrt{\\pi} = \\frac{1}{2\\sqrt{\\pi}}$$\n\n**2.2. GELU 输出的方差：**\n我们首先计算二阶矩 $E[(\\phi_{\\mathrm{GELU}}(X))^2] = E[X^2 \\Phi(X)^2]$。我们应用斯坦因引理，令 $g(x) = x \\Phi(x)^2$。其导数为 $g'(x) = \\frac{d}{dx}(x \\Phi(x)^2) = \\Phi(x)^2 + x(2\\Phi(x)p(x)) = \\Phi(x)^2 + 2x\\Phi(x)p(x)$。\n$$E[X^2 \\Phi(X)^2] = E[g'(X)] = E[\\Phi(X)^2 + 2X\\Phi(X)p(X)] = E[\\Phi(X)^2] + 2E[X\\Phi(X)p(X)]$$\n我们必须计算两个期望：\n\n(a) $E[\\Phi(X)^2] = \\int_{-\\infty}^{\\infty} \\Phi(x)^2 p(x) dx$。这可以解释为 $P(\\max(Y,Z) \\le X)$，其中 $Y, Z, X$ 是独立同分布的 $\\mathcal{N}(0,1)$ 变量。我们使用分部积分法来求解。令 $u = \\Phi(x)^2$ 和 $dv = p(x) dx$。则 $du = 2\\Phi(x)p(x) dx$ 且 $v = \\Phi(x)$。\n$$J = \\int_{-\\infty}^{\\infty} \\Phi(x)^2 p(x) dx = [\\Phi(x)^3]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} \\Phi(x) (2\\Phi(x)p(x)) dx = (1^3 - 0^3) - 2J$$\n$$J = 1 - 2J \\implies 3J = 1 \\implies J = E[\\Phi(X)^2] = \\frac{1}{3}$$\n\n(b) $E[X\\Phi(X)p(X)] = \\int_{-\\infty}^{\\infty} x \\Phi(x) p(x)^2 dx$。\n$$p(x)^2 = \\frac{1}{2\\pi}\\exp(-x^2)$$\n所以，$E[X\\Phi(X)p(X)] = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} x \\Phi(x) \\exp(-x^2) dx$。\n分部积分法：令 $u=\\Phi(x)$ 和 $dv = x \\exp(-x^2) dx$。则 $du=p(x)dx$ 且 $v = -\\frac{1}{2}\\exp(-x^2)$。\n$$\\int x \\Phi(x) e^{-x^2} dx = \\left[-\\frac{1}{2} e^{-x^2} \\Phi(x)\\right]_{-\\infty}^\\infty - \\int_{-\\infty}^{\\infty} \\left(-\\frac{1}{2}e^{-x^2}\\right) p(x) dx$$\n边界项为 $0$。积分为 $\\frac{1}{2} \\int_{-\\infty}^{\\infty} e^{-x^2} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx = \\frac{1}{2\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-3x^2/2} dx$。\n使用 $\\int_{-\\infty}^{\\infty} e^{-ax^2} dx = \\sqrt{\\pi/a}$，其中 $a=3/2$：\n$$\\frac{1}{2\\sqrt{2\\pi}} \\sqrt{\\frac{2\\pi}{3}} = \\frac{1}{2\\sqrt{3}}$$\n因此，$E[X\\Phi(X)p(X)] = \\frac{1}{2\\pi} \\frac{1}{2\\sqrt{3}} = \\frac{1}{4\\pi\\sqrt{3}}$。\n\n结合这些结果，得到 GELU 的二阶矩：\n$$E[(\\phi_{\\mathrm{GELU}}(X))^2] = \\frac{1}{3} + 2 \\left( \\frac{1}{4\\pi\\sqrt{3}} \\right) = \\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}}$$\nGELU 的方差为：\n$$\\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)] = E[(\\phi_{\\mathrm{GELU}}(X))^2] - (E[\\phi_{\\mathrm{GELU}}(X)])^2 = \\left(\\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}}\\right) - \\left(\\frac{1}{2\\sqrt{\\pi}}\\right)^2$$\n$$\\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)] = \\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}} - \\frac{1}{4\\pi} = \\frac{1}{3} + \\frac{\\sqrt{3}}{6\\pi} - \\frac{1}{4\\pi} = \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi}$$\n\n**第3部分：批量归一化参数**\n\n批量归一化变换由 $y = \\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta$ 给出，其中 $\\mu = E[\\phi(X)]$ 和 $\\sigma^2 = \\operatorname{Var}[\\phi(X)]$。我们需要找到参数 $(\\gamma^{\\ast}, \\beta^{\\ast})$，使得输出 $y$ 的期望 $E[y]=0$ 且方差 $\\operatorname{Var}[y]=1$。\n\n**3.1. y 的均值：**\n利用期望算子的线性性质：\n$$E[y] = E\\left[\\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta\\right] = \\frac{\\gamma}{\\sigma} E[\\phi(X) - \\mu] + E[\\beta] = \\frac{\\gamma}{\\sigma} (E[\\phi(X)] - \\mu) + \\beta$$\n由于 $\\mu=E[\\phi(X)]$，项 $(E[\\phi(X)] - \\mu)$ 为 $0$。\n$$E[y] = \\beta$$\n为了使 $E[y]=0$，我们必须有 $\\beta^{\\ast} = 0$。\n\n**3.2. y 的方差：**\n利用方差的性质 $\\operatorname{Var}[aZ+b] = a^2 \\operatorname{Var}[Z]$：\n$$\\operatorname{Var}[y] = \\operatorname{Var}\\left[\\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta\\right] = \\operatorname{Var}\\left[\\frac{\\gamma}{\\sigma}\\phi(X)\\right] = \\left(\\frac{\\gamma}{\\sigma}\\right)^2 \\operatorname{Var}[\\phi(X)]$$\n由于 $\\sigma^2 = \\operatorname{Var}[\\phi(X)]$：\n$$\\operatorname{Var}[y] = \\frac{\\gamma^2}{\\sigma^2} \\sigma^2 = \\gamma^2$$\n为了使 $\\operatorname{Var}[y]=1$，我们必须有 $(\\gamma^{\\ast})^2=1$。这得出 $\\gamma^{\\ast} = 1$ 或 $\\gamma^{\\ast} = -1$。在深度学习的背景下，$\\gamma$ 是一个缩放因子，通常初始化为 $1$，并经常被约束为正值。我们采用标准选择 $\\gamma^{\\ast}=1$。\n\n仿射参数为 $(\\gamma^{\\ast}, \\beta^{\\ast}) = (1, 0)$。这些参数与具体的激活函数 $\\phi$ 无关。\n\n最终的元组是 $\\big(E[\\phi_{\\mathrm{ReLU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)], E[\\phi_{\\mathrm{GELU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)], \\gamma^{\\ast}, \\beta^{\\ast}\\big)$。\n代入推导出的表达式：\n$\\left( \\frac{1}{\\sqrt{2\\pi}}, \\frac{\\pi-1}{2\\pi}, \\frac{1}{2\\sqrt{\\pi}}, \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi}, 1, 0 \\right)$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\sqrt{2\\pi}}  \\frac{\\pi-1}{2\\pi}  \\frac{1}{2\\sqrt{\\pi}}  \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi}  1  0 \\end{pmatrix}}\n$$", "id": "3097811"}, {"introduction": "除了统计稳定性，激活函数还深刻地影响着优化曲面，并因此影响训练速度。本练习提供了一个实验框架，用于量化激活函数的“非线性度”与一个简单模型的收敛速度之间的关系。通过从零开始实现梯度下降训练循环并测试多种激活函数，你将对不同的函数形式如何加速或阻碍学习过程，建立起直观且定量的认识 [@problem_id:3097784]。", "problem": "要求你设计、实现并分析一个最小化的实验，以量化一个简单监督学习任务中训练速度与激活函数非线性度之间的关系。使用以下基础：均方误差（MSE）损失的定义、梯度下降（GD）更新规则以及初等微积分中的链式法则。该任务关注一个标量模型参数和几种激活函数，所有量都必须以纯数学术语处理。\n\n按如下方式定义数据、模型、损失和训练过程。\n\n1. 数据与目标：构建一个包含 $N$ 个点的一维数据集，其输入 $x_i$ 在区间 $[-B, B]$ 上均匀分布，目标输出为 $y_i = x_i$。使用 $N = 201$ 个点覆盖区间 $[-1, 1]$（因此 $B = 1$）。不涉及物理单位。\n\n2. 模型：考虑一个参数模型 $f_w(x) = \\phi(w x)$，其中 $w \\in \\mathbb{R}$ 是单一可训练参数，$\\phi$ 是一个激活函数。将 $w$ 初始化为 $w_0 = 0.5$。\n\n3. 损失：使用均方误差（MSE）损失\n$$\nL(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\phi(w x_i) - x_i \\right)^2.\n$$\n\n4. 训练：使用全批量梯度下降（GD），学习率为 $\\eta = 0.05$，最多进行 $T_{\\max} = 2000$ 个轮次（epoch）。一个轮次定义为使用完整梯度对 $w$ 进行一次完整更新。一旦损失首次满足 $L(w) \\le \\varepsilon$（其中 $\\varepsilon = 0.14$），训练就停止。如果未能在 $T_{\\max}$ 个轮次内达到此阈值，则记录数值 $T_{\\max}$。\n\n5. 激活函数：评估以下激活函数 $\\phi$ 及其导数 $\\phi'$：\n   - 恒等函数：$\\phi(x) = x$。\n   - 双曲正切：$\\phi(x) = \\tanh(x)$。\n   - 带泄露修正线性单元（Leaky ReLU），负斜率 $\\alpha = 0.1$：$\\phi(x) = \\max(x, \\alpha x)$。\n   - 指数线性单元（ELU），参数 $\\alpha = 1$：$\\phi(x) = \\begin{cases} x  \\text{if } x  0, \\\\ \\alpha (\\exp(x) - 1)  \\text{if } x \\le 0. \\end{cases}$\n   - Swish 函数，参数 $\\beta = 1$：$\\phi(x) = x \\cdot \\sigma(\\beta x)$，其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n\n6. 非线性度度量：对于每个激活函数 $\\phi$，计算其在区间 $[-B, B]$ 上与恒等映射的上确界范数偏差：\n$$\n\\left\\| \\phi - \\operatorname{id} \\right\\|_{\\infty, [-B,B]} = \\sup_{x \\in [-B,B]} \\left| \\phi(x) - x \\right|.\n$$\n通过在 $[-B,B]$ 上均匀分布的 $M$ 个点的密集网格上评估 $\\left| \\phi(x) - x \\right|$ 并取最大值来数值近似此量。使用 $M = 10001$。\n\n7. 训练速度度量：对于每个激活函数 $\\phi$，将训练速度度量定义为在上述 GD 过程中达到损失阈值 $L(w) \\le \\varepsilon$ 所需的整数轮次数。如果未能在 $T_{\\max}$ 个轮次内达到该阈值，则记录 $T_{\\max}$。\n\n8. 测试套件：使用以下激活函数集，每个作为独立的测试用例：\n   - 用例 1：恒等函数。\n   - 用例 2：$\\tanh$。\n   - 用例 3：带 $\\alpha = 0.1$ 的 Leaky ReLU。\n   - 用例 4：带 $\\alpha = 1$ 的 ELU。\n   - 用例 5：带 $\\beta = 1$ 的 Swish。\n   所有用例共享以上指定的相同的 $N$、$B$、$M$、$w_0$、$\\eta$、$\\varepsilon$ 和 $T_{\\max}$。\n\n你的程序必须：\n- 为每个用例实现 $\\phi$ 和 $\\phi'$。\n- 为每个用例计算 $\\left\\| \\phi - \\operatorname{id} \\right\\|_{\\infty, [-B,B]}$ 的数值近似。\n- 在数据集上使用全批量 GD 训练 $w$，直到满足停止标准，并记录达到 $\\varepsilon$ 所需的轮次数。\n- 以指定的输出格式汇总结果。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中，每个激活函数贡献两个值，按测试套件给出的顺序排列：上确界范数偏差，后跟达到 $\\varepsilon$ 所需的轮次数。因此，输出为\n$$\n[\\|\\phi_1 - \\operatorname{id}\\|_{\\infty}, t_1, \\|\\phi_2 - \\operatorname{id}\\|_{\\infty}, t_2, \\dots, \\|\\phi_5 - \\operatorname{id}\\|_{\\infty}, t_5],\n$$\n其中 $t_k$ 是激活函数 $\\phi_k$ 达到 $L(w) \\le \\varepsilon$ 所需的整数轮次数（如果未达到则为 $T_{\\max}$）。不涉及角度，因此不需要角度单位，所有输出都是无单位的实数或整数。", "solution": "问题陈述已经过分析，被认为是有效的。它是自洽的、在数学和科学上是合理的，并且是适定的。该任务涉及一个数值实验，在一个简化的机器学习背景下研究激活函数非线性度与训练收敛速度之间的关系。解决方案将基于微积分和数值优化的原理构建。\n\n问题的核心在于为一个简单的参数模型实现全批量梯度下降（GD）算法。模型定义为 $f_w(x) = \\phi(w x)$，其中 $w \\in \\mathbb{R}$ 是单一可训练的标量参数，$\\phi$ 是一个给定的激活函数。目标是训练 $w$ 以最小化均方误差（MSE）损失函数，该函数定义在一个包含 $N=201$ 个点 $(x_i, y_i)$ 的数据集上。输入 $x_i$ 在区间 $[-1, 1]$ 上均匀分布，目标输出为 $y_i = x_i$。\n\nMSE 损失由下式给出：\n$$\nL(w) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(f_w(x_i) - y_i\\right)^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\phi(w x_i) - x_i \\right)^2\n$$\n\n训练过程使用梯度下降更新规则，这需要损失函数关于参数 $w$ 的导数。我们使用微分的链式法则计算这个导数，记为 $\\frac{dL}{dw}$。\n\n损失函数的导数为：\n$$\n\\frac{dL}{dw} = \\frac{d}{dw} \\left( \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\phi(w x_i) - x_i \\right)^2 \\right)\n$$\n根据微分的线性性质，我们可以将导数移到求和符号内部：\n$$\n\\frac{dL}{dw} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{d}{dw} \\left(\\phi(w x_i) - x_i \\right)^2\n$$\n应用链式法则，其中外层函数为 $u^2$，内层函数为 $u(w) = \\phi(w x_i) - x_i$：\n$$\n\\frac{d}{dw} \\left( \\dots \\right)^2 = 2 \\left(\\phi(w x_i) - x_i \\right) \\cdot \\frac{d}{dw} \\left(\\phi(w x_i) - x_i \\right)\n$$\n内层函数的导数为：\n$$\n\\frac{d}{dw} \\left(\\phi(w x_i) - x_i \\right) = \\frac{d}{dw} \\left(\\phi(w x_i)\\right) - \\frac{d}{dw} \\left(x_i\\right)\n$$\n项 $x_i$ 相对于 $w$ 是常数，所以其导数为 $0$。对于第一项，我们再次应用链式法则，外层函数为 $\\phi(z)$，内层函数为 $z(w) = w x_i$：\n$$\n\\frac{d}{dw} \\left(\\phi(w x_i)\\right) = \\phi'(w x_i) \\cdot \\frac{d}{dw}(w x_i) = \\phi'(w x_i) \\cdot x_i\n$$\n其中 $\\phi'$ 是激活函数 $\\phi$ 的导数。\n\n综合这些结果，梯度的最终表达式为：\n$$\n\\frac{dL}{dw} = \\frac{2}{N} \\sum_{i=1}^{N} \\left(\\phi(w x_i) - x_i \\right) \\phi'(w x_i) x_i\n$$\n\n对于每个指定的激活函数 $\\phi$，数值实验按以下步骤进行：\n\n1.  **非线性度量化**：与恒等函数的偏差通过数值方法近似。我们通过在一个包含 $M=10001$ 个均匀分布于区间 $[-1, 1]$ 的点的精细网格上评估 $|\\phi(x) - x|$ 并找到最大值，来计算度量 $\\left\\| \\phi - \\operatorname{id} \\right\\|_{\\infty, [-1,1]} = \\sup_{x \\in [-1,1]} \\left| \\phi(x) - x \\right|$。\n\n2.  **梯度下降训练**：参数 $w$ 通过迭代训练。\n    - **初始化**：参数初始化为 $w_0 = 0.5$。\n    - **迭代**：对于从 $1$ 到最大值 $T_{\\max} = 2000$ 的每个轮次 $k$：\n        a. 使用上面推导的公式，在当前 $w$ 值下计算梯度 $\\frac{dL}{dw}$。这是一个“全批量”梯度，因为它对所有 $N$ 个数据点求和。\n        b. 根据规则 $w_{k} = w_{k-1} - \\eta \\frac{dL}{dw}$ 更新参数 $w$，其中学习率为 $\\eta = 0.05$。\n        c. 更新后，计算损失 $L(w_k)$。\n        d. **停止准则**：如果 $L(w_k) \\le \\varepsilon = 0.14$，训练过程终止，并记录所用的轮次数 $k$。\n    - **终止**：如果在 $T_{\\max}$ 个轮次后仍未达到损失阈值，过程停止，并记录轮次数为 $T_{\\max}$。会检查 $w_0$ 处的初始损失；如果它已经满足准则，则轮次数为 $0$。\n\n实现以下激活函数 $\\phi(z)$ 及其导数 $\\phi'(z)$：\n-   **恒等函数**：$\\phi(z) = z$，$\\phi'(z) = 1$。\n-   **双曲正切**：$\\phi(z) = \\tanh(z)$，$\\phi'(z) = 1 - \\tanh^2(z)$。\n-   **带泄露修正线性单元 (Leaky ReLU)**：对于 $\\alpha = 0.1$，$\\phi(z) = \\begin{cases} z  \\text{if } z  0 \\\\ \\alpha z  \\text{if } z \\le 0 \\end{cases}$，其导数为 $\\phi'(z) = \\begin{cases} 1  \\text{if } z  0 \\\\ \\alpha  \\text{if } z \\le 0 \\end{cases}$。\n-   **指数线性单元 (ELU)**：对于 $\\alpha = 1$，$\\phi(z) = \\begin{cases} z  \\text{if } x  0 \\\\ \\alpha (e^z - 1)  \\text{if } z \\le 0 \\end{cases}$，其导数为 $\\phi'(z) = \\begin{cases} 1  \\text{if } z  0 \\\\ \\alpha e^z  \\text{if } z \\le 0 \\end{cases}$。\n-   **Swish 函数**：对于 $\\beta = 1$，$\\phi(z) = z \\cdot \\sigma(z)$ 其中 $\\sigma(z) = (1+e^{-z})^{-1}$。其导数为 $\\phi'(z) = \\phi(z) + \\sigma(z)(1 - \\phi(z))$。\n\n实现将为所有五个激活函数系统地执行此过程，为每个函数收集非线性度度量和所需的轮次数。结果将按照规定编译成一个单一列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the numerical experiment to quantify the relationship\n    between activation function nonlinearity and training speed.\n    \"\"\"\n\n    # --- 1. Define Constants and Data ---\n    N = 201\n    B = 1.0\n    M = 10001\n    w_0 = 0.5\n    eta = 0.05\n    epsilon = 0.14\n    T_max = 2000\n    \n    # Dataset\n    x_data = np.linspace(-B, B, N)\n    y_data = x_data  # Target is the identity function\n\n    # Grid for nonlinearity calculation\n    x_dense = np.linspace(-B, B, M)\n\n    # --- 2. Define Activation Functions and Their Derivatives ---\n    \n    # Sigmoid for Swish\n    def sigmoid(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    activations = [\n        {\n            \"name\": \"Identity\",\n            \"phi\": lambda z: z,\n            \"phi_prime\": lambda z: np.ones_like(z),\n            \"params\": {}\n        },\n        {\n            \"name\": \"Tanh\",\n            \"phi\": lambda z: np.tanh(z),\n            \"phi_prime\": lambda z: 1.0 - np.tanh(z)**2,\n            \"params\": {}\n        },\n        {\n            \"name\": \"Leaky ReLU\",\n            \"phi\": lambda z, alpha: np.where(z > 0, z, alpha * z),\n            \"phi_prime\": lambda z, alpha: np.where(z > 0, 1.0, alpha),\n            \"params\": {\"alpha\": 0.1}\n        },\n        {\n            \"name\": \"ELU\",\n            \"phi\": lambda z, alpha: np.where(z > 0, z, alpha * (np.exp(z) - 1.0)),\n            \"phi_prime\": lambda z, alpha: np.where(z > 0, 1.0, alpha * np.exp(z)),\n            \"params\": {\"alpha\": 1.0}\n        },\n        {\n            \"name\": \"Swish\",\n            \"phi\": lambda z, beta: z * sigmoid(beta * z),\n            \"phi_prime\": lambda z, beta: (z * sigmoid(beta * z)) + sigmoid(beta * z) * (1.0 - (z * sigmoid(beta * z))),\n            \"params\": {\"beta\": 1.0}\n        }\n    ]\n\n    all_results = []\n\n    # --- 3. Main Loop for Each Activation Function ---\n    for act_config in activations:\n        phi = lambda z: act_config[\"phi\"](z, **act_config[\"params\"])\n        phi_prime = lambda z: act_config[\"phi_prime\"](z, **act_config[\"params\"])\n\n        # --- Calculate Nonlinearity Metric ---\n        nonlinearity = np.max(np.abs(phi(x_dense) - x_dense))\n        all_results.append(nonlinearity)\n\n        # --- Run Training ---\n        w = w_0\n        epochs_to_epsilon = 0\n\n        # Check loss at initialization\n        y_pred_initial = phi(w * x_data)\n        initial_loss = np.mean((y_pred_initial - y_data)**2)\n\n        if initial_loss = epsilon:\n            epochs_to_epsilon = 0\n        else:\n            # Run Gradient Descent\n            for epoch_count in range(1, T_max + 1):\n                # Calculate gradient\n                y_pred = phi(w * x_data)\n                grad_term = (y_pred - y_data) * phi_prime(w * x_data) * x_data\n                grad = (2.0 / N) * np.sum(grad_term)\n\n                # Update weight\n                w = w - eta * grad\n\n                # Calculate new loss\n                current_loss = np.mean((phi(w * x_data) - y_data)**2)\n\n                # Check stopping criterion\n                if current_loss = epsilon:\n                    epochs_to_epsilon = epoch_count\n                    break\n            else:  # This executes if the loop completes without a break\n                epochs_to_epsilon = T_max\n\n        all_results.append(epochs_to_epsilon)\n\n    # --- 4. Format and Print Final Output ---\n    output_str = \",\".join(map(str, all_results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3097784"}, {"introduction": "虽然许多激活函数是逐元素应用的，但像最大池化（max-pooling）或注意力机制这样的操作会聚合特征，通常会用到最大值函数，而这可能导致一个关键的训练问题。这个问题旨在解决“梯度饥饿”（gradient starvation）现象，即非最大值的特征会接收到零梯度从而停止学习。你将为“尖锐”的最大值函数及其“平滑”的 LogSumExp 近似推导梯度，然后通过编程实现来展示平滑化如何确保所有输入都能接收到学习信号，从而缓解梯度饥饿问题 [@problem_id:3097871]。", "problem": "考虑一个玩具单层回归模型，该模型具有 $d$ 个标量特征。设输入向量为 $x \\in \\mathbb{R}^d$，参数向量为 $w \\in \\mathbb{R}^d$，并为每个索引 $i \\in \\{1,2,\\dots,d\\}$ 定义预激活值 $u \\in \\mathbb{R}^d$，其计算方式为 $u_i = w_i x_i$。该模型通过一个激活聚合器 $\\phi$ 从 $u$ 生成一个标量输出 $s$，并使用平方损失 $L = \\tfrac{1}{2} (s - y)^2$ 针对一个标量目标 $y \\in \\mathbb{R}$进行训练。考虑两种激活聚合器：\n- 一种尖锐拐点聚合器 $\\phi_{\\text{sharp}}$，定义为 $\\phi_{\\text{sharp}}(u) = \\max_{i} u_i$，它在存在多个最大值时是不可微的。\n- 一种温度平滑聚合器 $\\phi_{\\tau}$，定义为 $\\phi_{\\tau}(u)$，对于一个正温度参数 $\\tau \\in \\mathbb{R}_{0}$，它平滑地近似最大值。$\\phi_{\\tau}$ 的具体函数形式将从使用指数函数对最大值进行平滑近似的第一性原理出发进行推导。\n\n你必须仅从损失 $L$ 的定义和导数的链式法则出发，并且不使用任何关于 $\\phi_{\\text{sharp}}$ 或 $\\phi_{\\tau}$ 的预先给定的导数公式，来完成以下任务：\n1. 对于 $\\phi_{\\text{sharp}}$，推导 $\\,\\dfrac{\\partial L}{\\partial w_i}\\,$，用 $x$、$w$ 和 $y$ 表示。在出现多个最大值时，需做出合理的次梯度选择。如果多个索引达到最大值，你必须在这些索引之间选择一个均匀次梯度（即将梯度均等地分配给所有达到最大值的索引 $i$），并确保你的选择与链式法则和 $L$ 的定义一致。\n2. 对于一个以指数函数和正温度参数 $\\tau$ 为基础构建的光滑最大值形式的温度平滑聚合器 $\\phi_{\\tau}$，推导 $\\,\\dfrac{\\partial L}{\\partial w_i}\\,$。你的推导必须清楚地表明平滑性如何为所有坐标产生非零梯度，以及梯度量的分布如何依赖于 $\\tau$。\n3. 对给定的 $(x,w,y,\\tau)$ 定义“梯度饥饿指数”如下。设 $A \\subseteq \\{1,\\dots,d\\}$ 是向量 $u$ 中达到最大值的索引集合（包括并列情况），$A^c$ 是其补集。设 $g^{\\text{sharp}} \\in \\mathbb{R}^d$ 和 $g^{\\tau} \\in \\mathbb{R}^d$ 分别为在 $\\phi_{\\text{sharp}}$ 和 $\\phi_{\\tau}$ 下的梯度 $\\,\\dfrac{\\partial L}{\\partial w}\\,$。非最大值坐标的梯度饥饿指数为\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} \\left| g^{\\text{sharp}}_i \\right|, \\quad\n\\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} \\left| g^{\\tau}_i \\right|.\n$$\n计算这两个指数和“缓解量”\n$$\n\\Delta_{\\tau} = \\operatorname{SI}_{\\tau} - \\operatorname{SI}_{\\text{sharp}}.\n$$\n\n实现一个完整的程序，对下面指定的每个测试用例，使用推导出的公式和针对 $\\phi_{\\text{sharp}}$ 的均匀次梯度选择，计算三个浮点数量 $\\,\\operatorname{SI}_{\\text{sharp}},\\,\\operatorname{SI}_{\\tau},\\,\\Delta_{\\tau}\\,$。为平滑聚合器使用数值稳定的实现，使其在 $\\tau$ 非常小时仍能表现良好。\n\n测试套件（每个案例为 $(d,x,w,y,\\tau)$）：\n- 案例 1：$d = 4$, $x = [1.0, 0.9, 0.8, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 2.0$, $\\tau = 0.5$。\n- 案例 2：$d = 4$, $x = [1.0, 0.9, 0.8, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 2.0$, $\\tau = 0.001$。\n- 案例 3：$d = 4$, $x = [1.0, 1.0, 0.1, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 3.0$, $\\tau = 0.5$。\n- 案例 4：$d = 3$, $x = [-0.5, 0.4, 0.39]$, $w = [-2.0, 1.0, 1.0]$, $y = 0.0$, $\\tau = 0.2$。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含结果，形式为一个逗号分隔的内部列表列表，每个内部列表为 $[\\operatorname{SI}_{\\text{sharp}}, \\operatorname{SI}_{\\tau}, \\Delta_{\\tau}]$，并用方括号括起来。例如：$[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$。不应打印任何其他文本。", "solution": "该问题要求在一个简单的回归模型中，针对两种不同的激活聚合方案，推导和计算梯度，并随后分析一个“梯度饥饿指数”。验证过程确认了该问题是适定的、有科学依据的，并且内部一致。我们将进行形式化推导。\n\n该模型由输入 $x \\in \\mathbb{R}^d$、参数 $w \\in \\mathbb{R}^d$ 和预激活值 $u \\in \\mathbb{R}^d$ 定义，其中 $u_i = w_i x_i$。模型输出为 $s = \\phi(u)$，相对于目标 $y \\in \\mathbb{R}$ 的损失为 $L = \\frac{1}{2}(s - y)^2$。我们的目标是求损失相对于每个权重的偏导数 $\\frac{\\partial L}{\\partial w_i}$。\n\n我们首先应用链式法则。$L$ 相对于 $w_i$ 的导数可以分解如下：\n$$\n\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial s} \\frac{\\partial s}{\\partial w_i}\n$$\n第一项，即损失相对于模型输出 $s$ 的导数，是直接的：\n$$\n\\frac{\\partial L}{\\partial s} = \\frac{\\partial}{\\partial s} \\left( \\frac{1}{2}(s - y)^2 \\right) = s - y\n$$\n第二项 $\\frac{\\partial s}{\\partial w_i}$ 需要进一步应用链式法则，因为 $s = \\phi(u)$ 且 $u$ 是 $w$ 的函数。输出 $s$ 依赖于 $u$ 的所有分量，而这些分量又依赖于 $w$。\n$$\n\\frac{\\partial s}{\\partial w_i} = \\sum_{j=1}^{d} \\frac{\\partial s}{\\partial u_j} \\frac{\\partial u_j}{\\partial w_i}\n$$\n预激活值 $u_j$ 定义为 $u_j = w_j x_j$。它相对于 $w_i$ 的偏导数仅在 $j=i$ 时非零：\n$$\n\\frac{\\partial u_j}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} (w_j x_j) = \\begin{cases} x_i  \\text{if } j=i \\\\ 0  \\text{if } j \\neq i \\end{cases}\n$$\n将此代入求和式，所有 $j \\neq i$ 的项都消失了，只剩下 $j=i$ 的项：\n$$\n\\frac{\\partial s}{\\partial w_i} = \\frac{\\partial s}{\\partial u_i} \\frac{\\partial u_i}{\\partial w_i} = \\frac{\\partial s}{\\partial u_i} x_i\n$$\n结合这些结果，我们得到梯度分量 $g_i = \\frac{\\partial L}{\\partial w_i}$ 的通用公式：\n$$\ng_i = \\frac{\\partial L}{\\partial w_i} = (s - y) \\frac{\\partial s}{\\partial u_i} x_i\n$$\n因此，梯度的具体形式取决于 $\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial \\phi}{\\partial u_i}$ 这一项，我们现在将为指定的两种聚合器推导它。\n\n**1. 尖锐拐点聚合器 $\\phi_{\\text{sharp}}$ 的梯度**\n\n尖锐拐点聚合器定义为最大值函数：$s = \\phi_{\\text{sharp}}(u) = \\max_{j} u_j$。\n最大值函数在除了 $u$ 的两个或多个分量同时等于最大值之外的所有点都是可微的。\n设 $A = \\{k \\in \\{1,\\dots,d\\} \\mid u_k = \\max_j u_j\\}$ 为对应于 $u$ 的最大值的索引集合。设 $|A|$ 为此集合的基数。\n\n情况 1：唯一最大值 ($|A|=1$)。设 $A = \\{k\\}$。那么在当前值的一个邻域内，$s = u_k$。偏导数为：\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial u_k}{\\partial u_i} = \\delta_{ik} = \\begin{cases} 1  \\text{if } i=k \\\\ 0  \\text{if } i \\neq k \\end{cases}\n$$\n情况 2：多个最大值 ($|A|1$)。函数在这一点上不可微。我们必须选择一个次梯度。最大值函数的次微分 $\\partial(\\max)(u)$ 是对应于集合 $A$ 中索引的标准基向量的凸包：$\\partial(\\max)(u) = \\text{conv}\\{e_k \\mid k \\in A\\}$。一个向量 $v \\in \\mathbb{R}^d$ 是一个次梯度，如果其分量 $v_i$ 满足 $v_i \\ge 0$，$\\sum_{i \\in A} v_i = 1$，且对于 $i \\notin A$ 有 $v_i = 0$。\n\n问题要求采用“均匀次梯度选择”，这对应于将梯度在所有最大分量中均等分配。这是一个特定的、合理的次梯度选择，我们将系数设置为均匀的：\n$$\n\\frac{\\partial s}{\\partial u_i} = \\begin{cases} 1/|A|  \\text{if } i \\in A \\\\ 0  \\text{if } i \\notin A \\end{cases}\n$$\n这个单一公式正确地处理了唯一最大值（此时 $|A|=1$）和并列最大值两种情况。\n\n因此，尖锐聚合器的梯度分量 $g^{\\text{sharp}}_i$ 为：\n$$\ng^{\\text{sharp}}_i = (\\max_j u_j - y) \\cdot x_i \\cdot \\begin{cases} 1/|A|  \\text{if } i \\in A \\\\ 0  \\text{if } i \\notin A \\end{cases}\n$$\n\n**2. 温度平滑聚合器 $\\phi_{\\tau}$ 的梯度**\n\n我们的任务是构建一个基于指数函数的光滑最大值函数近似。一个标准的构造是 LogSumExp 函数，由温度参数 $\\tau  0$ 缩放。我们定义 $\\phi_{\\tau}(u)$ 为：\n$$\ns = \\phi_{\\tau}(u) = \\tau \\ln \\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right)\n$$\n为了说明这能近似最大值，设 $u_k = \\max_j u_j$。我们可以将表达式重写为：\n$$\n\\phi_{\\tau}(u) = \\tau \\ln \\left( e^{u_k/\\tau} \\sum_{j=1}^{d} e^{(u_j - u_k)/\\tau} \\right) = u_k + \\tau \\ln \\left( 1 + \\sum_{j \\neq k} e^{(u_j - u_k)/\\tau} \\right)\n$$\n当 $\\tau \\to 0^+$ 时，对于任何 $j \\neq k$，项 $(u_j - u_k)/\\tau \\to -\\infty$，所以 $e^{(u_j - u_k)/\\tau} \\to 0$。求和项消失，对数接近 $\\ln(1)=0$，因此 $\\phi_{\\tau}(u) \\to u_k = \\max_j u_j$。\n\n偏导数 $\\frac{\\partial s}{\\partial u_i}$ 通过对 $\\phi_{\\tau}(u)$ 微分得到：\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial}{\\partial u_i} \\left[ \\tau \\ln \\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right) \\right] = \\tau \\cdot \\frac{1}{\\sum_{j=1}^{d} e^{u_j/\\tau}} \\cdot \\frac{\\partial}{\\partial u_i}\\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right)\n$$\n求和项相对于 $u_i$ 的导数就是第 $i$ 项的导数：\n$$\n\\frac{\\partial}{\\partial u_i}\\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right) = e^{u_i/\\tau} \\cdot \\frac{1}{\\tau}\n$$\n代回后，我们得到：\n$$\n\\frac{\\partial s}{\\partial u_i} = \\tau \\cdot \\frac{1}{\\sum_j e^{u_j/\\tau}} \\cdot \\frac{e^{u_i/\\tau}}{\\tau} = \\frac{e^{u_i/\\tau}}{\\sum_j e^{u_j/\\tau}}\n$$\n这是应用于向量 $u/\\tau$ 的 softmax 函数。对于所有 $i$，此表达式都是严格为正的，这表明平滑性如何为所有权重坐标产生非零的偏导数（从而产生非零梯度），这与尖锐聚合器不同，后者对非最大坐标产生零梯度。梯度的大小取决于 $u_i$ 的相对值和温度 $\\tau$。较小的 $\\tau$ 会导致一个更“尖锐”的 softmax 分布，将梯度量集中在具有最大 $u_i$ 的分量上，而较大的 $\\tau$ 则导致一个更“柔和”、更均匀的分布。\n\n为了数值稳定性，特别是当某些 $u_j/\\tau$ 很大时，我们可以提出最大值 $u_{\\max} = \\max_j u_j$：\n$$\n\\phi_{\\tau}(u) = u_{\\max} + \\tau \\ln \\left( \\sum_j e^{(u_j-u_{\\max})/\\tau} \\right)\n$$\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{e^{(u_i-u_{\\max})/\\tau}}{\\sum_j e^{(u_j-u_{\\max})/\\tau}}\n$$\n因此，平滑聚合器的梯度分量 $g^{\\tau}_i$ 为：\n$$\ng^{\\tau}_i = \\left(\\phi_{\\tau}(u) - y\\right) \\cdot \\frac{e^{u_i/\\tau}}{\\sum_j e^{u_j/\\tau}} \\cdot x_i\n$$\n\n**3. 梯度饥饿指数**\n\n最大化索引的集合是 $A = \\{i \\mid u_i = \\max_j u_j\\}$，其补集是 $A^c$。梯度饥饿指数定义为：\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} \\left| g^{\\text{sharp}}_i \\right|, \\quad \\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} \\left| g^{\\tau}_i \\right|\n$$\n根据我们对 $g^{\\text{sharp}}_i$ 的推导，如果 $i \\notin A$（即 $i \\in A^c$），则项 $\\frac{\\partial s}{\\partial u_i}$ 为零。因此，对于所有 $i \\in A^c$，$g^{\\text{sharp}}_i=0$。这得出的结论是，对于尖锐聚合器，非最大坐标的梯度饥饿是绝对的：\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} |0| = 0\n$$\n因此，“梯度饥饿”一词对于尖锐拐点情况是字面意义上的。\n\n对于平滑聚合器，通常对于所有 $i$，$g^{\\tau}_i$ 都是非零的，所以 $\\operatorname{SI}_{\\tau}$ 将是一个正值，表示“泄漏”到非最大分量的总梯度大小。\n缓解量定义为 $\\Delta_{\\tau} = \\operatorname{SI}_{\\tau} - \\operatorname{SI}_{\\text{sharp}}$。根据我们的发现，这简化为：\n$$\n\\Delta_{\\tau} = \\operatorname{SI}_{\\tau}\n$$\n每个测试用例的计算过程是：首先计算预激活值 $u$，确定集合 $A^c$，然后使用数值稳定的公式计算所有 $i \\in A^c$ 的 $g^{\\tau}_i$。所需的量就是 $\\operatorname{SI}_{\\text{sharp}}=0$，$\\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} |g^{\\tau}_i|$，以及 $\\Delta_{\\tau} = \\operatorname{SI}_{\\tau}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating gradient starvation indices for sharp and\n    smooth max aggregators in a simple regression model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: d=4, x=[1.0, 0.9, 0.8, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=2.0, tau=0.5\n        (4, [1.0, 0.9, 0.8, 0.1], [1.0, 1.0, 1.0, 1.0], 2.0, 0.5),\n        # Case 2: d=4, x=[1.0, 0.9, 0.8, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=2.0, tau=0.001\n        (4, [1.0, 0.9, 0.8, 0.1], [1.0, 1.0, 1.0, 1.0], 2.0, 0.001),\n        # Case 3: d=4, x=[1.0, 1.0, 0.1, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=3.0, tau=0.5\n        (4, [1.0, 1.0, 0.1, 0.1], [1.0, 1.0, 1.0, 1.0], 3.0, 0.5),\n        # Case 4: d=3, x=[-0.5, 0.4, 0.39], w=[-2.0, 1.0, 1.0], y=0.0, tau=0.2\n        (3, [-0.5, 0.4, 0.39], [-2.0, 1.0, 1.0], 0.0, 0.2),\n    ]\n\n    results = []\n    for d, x_list, w_list, y, tau in test_cases:\n        x = np.array(x_list, dtype=float)\n        w = np.array(w_list, dtype=float)\n        \n        # 1. Compute preactivations u\n        u = w * x\n        \n        # 2. Identify maximal and non-maximal indices\n        u_max = np.max(u)\n        # Using a small tolerance for float comparison, though not strictly necessary for given inputs\n        # but is good practice. In this case, direct comparison works.\n        is_max = (u == u_max)\n        is_not_max = ~is_max\n        \n        # 3. Calculate SI_sharp\n        # As derived, the gradient for non-maximal elements is always 0 for phi_sharp.\n        si_sharp = 0.0\n        \n        # 4. Calculate SI_tau\n        # Numerically stable calculation for s_tau and softmax probabilities\n        u_shifted = u - u_max\n        exp_terms = np.exp(u_shifted / tau)\n        sum_exp_terms = np.sum(exp_terms)\n        \n        # s_tau = u_max + tau * np.log(sum_exp_terms)\n        s_tau = u_max + tau * np.log(np.sum(np.exp((u - u_max) / tau)))\n        \n        # Common loss derivative term\n        dL_ds = s_tau - y\n        \n        # Softmax probabilities\n        p = exp_terms / sum_exp_terms\n        \n        # Gradient g_tau\n        g_tau = dL_ds * p * x\n        \n        # Sum of absolute gradients for non-maximal components\n        si_tau = np.sum(np.abs(g_tau[is_not_max]))\n        \n        # 5. Calculate Delta_tau\n        delta_tau = si_tau - si_sharp\n        \n        results.append([si_sharp, si_tau, delta_tau])\n\n    # Final print statement in the exact required format.\n    # The format [[a1,b1,c1],[a2,b2,c2]] is a string representation of a list of lists.\n    # We construct this string manually to avoid spaces and ensure exact format.\n    inner_lists_str = [','.join(map(str, r)) for r in results]\n    result_str = f\"[[{'],['.join(inner_lists_str)}]]\"\n    \n    print(result_str)\n\nsolve()\n```", "id": "3097871"}]}