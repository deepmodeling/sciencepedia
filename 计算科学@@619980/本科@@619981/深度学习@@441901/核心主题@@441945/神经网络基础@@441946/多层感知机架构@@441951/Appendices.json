{"hands_on_practices": [{"introduction": "多层感知机（MLP）的核心能力在于其能够逼近复杂的函数。我们从一个基础但极具启发性的练习开始：使用修正线性单元（ReLU）作为激活函数，精确地构建绝对值函数 $|x|$。这个练习将揭示 ReLU 网络如何通过组合简单的线性片段来构造非线性函数，并让我们思考实现这种表示所需的最小网络结构 [@problem_id:3151215]。", "problem": "考虑一个带修正线性单元 (ReLU) 激活函数的一维两层前馈网络，其中修正线性单元 (ReLU) 定义为 $\\sigma(u)=\\max\\{0,u\\}$。该网络有 $m$ 个隐藏神经元，其计算公式为\n$$f(x)=w_{2}^{\\top}\\sigma(W_{1}x+b_{1})+b_{2},$$\n其中输入为 $x\\in\\mathbb{R}$，权重矩阵为 $W_{1}\\in\\mathbb{R}^{m\\times 1}$，偏置向量为 $b_{1}\\in\\mathbb{R}^{m}$，输出权重为 $w_{2}\\in\\mathbb{R}^{m}$，标量输出偏置为 $b_{2}\\in\\mathbb{R}$。令目标函数为 $f^{\\star}(x)=|x|$。\n\n仅使用前馈架构和修正线性单元 (ReLU) 的核心定义，完成以下任务：\n- 为任意容差 $\\epsilon>0$，构建显式的权重和偏置 $(W_{1},b_{1},w_{2},b_{2})$，使得在整个 $x\\in\\mathbb{R}$ 上对 $f^{\\star}(x)$ 产生一个一致逼近，且其最坏情况误差至多为 $\\epsilon$。\n- 确定为达到此一致误差保证所需的最小隐藏神经元数量 $m$，并将其表示为 $\\epsilon$ 的函数。\n\n你的最终答案必须是关于 $m$ 作为 $\\epsilon$ 的函数的单个闭式表达式。无需四舍五入，也无需单位。", "solution": "首先验证问题，以确保其是适定的、有科学依据且无歧义的。\n\n### 步骤 1：提取已知条件\n- 网络架构：一个计算 $f(x)=w_{2}^{\\top}\\sigma(W_{1}x+b_{1})+b_{2}$ 的两层前馈网络。\n- 输入域：$x \\in \\mathbb{R}$。\n- 激活函数：修正线性单元 (ReLU)，$\\sigma(u)=\\max\\{0,u\\}$。\n- 网络参数：$W_{1}\\in\\mathbb{R}^{m\\times 1}$ (第一层权重)，$b_{1}\\in\\mathbb{R}^{m}$ (第一层偏置)，$w_{2}\\in\\mathbb{R}^{m}$ (第二层权重)，以及 $b_{2}\\in\\mathbb{R}$ (第二层偏置)。$m$ 是隐藏神经元的数量。\n- 目标函数：$f^{\\star}(x)=|x|$。\n- 逼近要求：在 $\\mathbb{R}$ 上的一致逼近，使得最坏情况误差以 $\\epsilon$ 为界。即，对于任意容差 $\\epsilon>0$，有 $\\sup_{x\\in\\mathbb{R}} |f(x) - f^{\\star}(x)| \\le \\epsilon$。\n- 目标 1：构建满足逼近要求的参数 $(W_{1},b_{1},w_{2},b_{2})$。\n- 目标 2：确定为满足此保证所需的最小隐藏神经元数量 $m$，并将其表示为 $\\epsilon$ 的函数。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题是良定义的且科学上是合理的。它属于使用神经网络的逼近理论的标准框架。目标函数 $f^{\\star}(x)=|x|$ 是连续的，全局逼近定理保证了两层网络可以逼近它。本问题要求一个特定的构造和对最小网络规模的分析。问题是自洽且客观的。尽管“作为 $\\epsilon$ 的函数”这一措辞可能暗示一个非恒定的关系，但常数函数是一个有效的答案。问题没有缺陷；其结构考察了对 ReLU 网络表示某些函数的精确能力的理解。该问题被认为是 **有效的**。\n\n### 步骤 3：解的推导\n求解过程分为两部分：首先，我们确定所需的最小神经元数量，其次，我们提供一个显式的构造。\n\n让我们分析所需的神经元数量 $m$。我们首先研究 $m=1$ 是否足够。\n当 $m=1$ 时，网络参数均为标量。令 $W_1 = a \\in \\mathbb{R}$，$b_1 = d \\in \\mathbb{R}$，$w_2 = c \\in \\mathbb{R}$，以及 $b_2 = e \\in \\mathbb{R}$。网络函数为：\n$$f(x) = c \\cdot \\sigma(ax+d) + e = c \\cdot \\max\\{0, ax+d\\} + e$$\n我们必须对所有 $x \\in \\mathbb{R}$ 满足 $|f(x) - |x|| \\le \\epsilon$。\n\n考虑 $f(x)$ 和 $|x|$ 的渐近行为。\n- 如果 $a=0$，$f(x) = c \\cdot \\max\\{0,d\\} + e$ 是一个常数。当 $|x| \\to \\infty$ 时，误差 $|f(x) - |x||$ 会变得无界。所以，我们必须有 $a \\ne 0$。\n- 如果 $a > 0$：\n  - 对于大的正数 $x$，$ax+d > 0$，因此 $f(x) = c(ax+d)+e = (ac)x + (cd+e)$。为确保当 $x \\to \\infty$ 时误差 $|f(x) - x|$ 有界，线性项必须抵消。这要求 $ac=1$。\n  - 对于大的负数 $x$ (即 $x \\to -\\infty$)，$ax+d  0$，因此 $f(x) = e$。误差为 $|f(x) - |x|| = |e - (-x)| = |e+x|$。当 $x \\to -\\infty$ 时，此误差无界。\n- 如果 $a  0$：\n  - 对于大的负数 $x$，$ax+d  0$，因此 $f(x) = c(ax+d)+e = (ac)x + (cd+e)$。为确保当 $x \\to -\\infty$ 时误差 $|f(x) - (-x)|$ 有界，我们必须有 $ac=-1$。\n  - 对于大的正数 $x$ (即 $x \\to \\infty$)，$ax+d  0$，因此 $f(x) = e$。误差为 $|f(x) - |x|| = |e - x|$。当 $x \\to \\infty$ 时，此误差无界。\n\n在 $m=1$ 的所有情况下，都无法在整个定义域 $\\mathbb{R}$ 上维持有界误差。因此，单个隐藏神经元是不够的，所以我们必须有 $m  1$。\n\n现在，让我们研究 $m=2$ 的情况。我们将尝试构造 $f(x)$ 来精确表示 $f^{\\star}(x)=|x|$。考虑恒等式：\n$$|x| = \\max\\{0, x\\} + \\max\\{0, -x\\}$$\n这将 $|x|$ 表示为两个函数的和，每个函数都具有 ReLU 激活函数的形式。我们可以用一个双神经元网络来实现这一点。\n网络输出为 $f(x) = w_{2,1}\\sigma(W_{1,1}x+b_{1,1}) + w_{2,2}\\sigma(W_{1,2}x+b_{1,2}) + b_2$。\n\n为了实现恒等式 $|x| = \\sigma(x) + \\sigma(-x)$，我们可以按如下方式选择参数：\n- 对于第一个神经元，我们想要计算 $\\sigma(x)$。我们将其对应的权重和偏置设置为 $W_{1,1}=1$ 和 $b_{1,1}=0$。\n- 对于第二个神经元，我们想要计算 $\\sigma(-x)$。我们将其权重和偏置设置为 $W_{1,2}=-1$ 和 $b_{1,2}=0$。\n- 输出层必须将这两个结果相加。我们将输出权重设置为 $w_{2,1}=1$ 和 $w_{2,2}=1$。\n- 没有整体偏移，所以我们将输出偏置 $b_2$ 设置为 0。\n\n以矩阵/向量形式，这些参数是：\n- $W_{1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\in \\mathbb{R}^{2\\times 1}$\n- $b_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n- $w_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n- $b_{2} = 0 \\in \\mathbb{R}$\n\n使用这些参数，网络函数为：\n$$f(x) = \\begin{pmatrix} 1  1 \\end{pmatrix} \\sigma\\left(\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}x + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\right) + 0$$\n$$f(x) = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} \\sigma(x) \\\\ \\sigma(-x) \\end{pmatrix} = \\sigma(x) + \\sigma(-x)$$\n$$f(x) = \\max\\{0, x\\} + \\max\\{0, -x\\}$$\n我们验证这等于 $|x|$：\n- 如果 $x \\ge 0$，$f(x) = x + 0 = x = |x|$。\n- 如果 $x  0$，$f(x) = 0 + (-x) = -x = |x|$。\n这个构造提供了 $f^{\\star}(x) = |x|$ 的一个精确表示。\n\n逼近误差为 $|f(x) - f^{\\star}(x)| = ||x| - |x|| = 0$。\n问题要求对于任何 $\\epsilon  0$，误差至多为 $\\epsilon$。由于我们的构造实现了 0 误差，并且对于任何 $\\epsilon  0$ 都有 $0 \\le \\epsilon$，所以这个构造对于任何容差都是一个有效的解。\n\n我们已经证明 $m=1$ 是不足的，而 $m=2$ 是足够的（实现了零误差）。因此，所需的最小隐藏神经元数量是 $m=2$。这个结果对于 $\\epsilon  0$ 的任何选择都成立。因此，最小神经元数量 $m$ 是 $\\epsilon$ 的一个常数函数：\n$$m(\\epsilon) = 2$$", "answer": "$$\\boxed{2}$$", "id": "3151215"}, {"introduction": "在掌握了基础函数构造后，我们来挑战一个更复杂的任务：构建一个能够逼近双变量乘法函数 $f(x,y)=x \\cdot y$ 的 MLP。这个练习不仅展示了如何通过巧妙的数学恒等式将复杂问题简化为对单变量函数的逼近，还引出了深度学习架构设计的核心权衡：深度与宽度 [@problem_id:3155543]。通过分析实现给定逼近精度所需的网络资源，你将深入理解深度在构建高效表征中的关键作用。", "problem": "您的任务是构建一个多层感知机 (MLP)，使用仅由修正线性单元 (ReLU) 激活函数构建的分段线性函数，在方形域 $[0,1]^2$ 上逼近乘法函数 $f(x,y) = x \\cdot y$。期望的逼近误差为一个非负容差 $\\epsilon$。您的目标是从第一性原理推导其架构，并实现一个程序来验证误差并报告所需的架构复杂度。该构建过程必须具有科学合理性，并从基本依据出发：MLP 的定义、将 $x \\cdot y$ 简化为单变量函数组合的代数恒等式，以及针对二阶可导函数进行线性插值的标准误差界。\n\n用作基础的定义：\n- 多层感知机 (MLP) 是一个由交替的仿射变换和非线性函数构成的复合函数。对于修正线性单元 (ReLU) 激活函数，一个层计算 $\\sigma(z)$，其中 $\\sigma(t) = \\max\\{0,t\\}$，$z$ 是其输入的仿射变换。\n- 一个带有断点 $\\{c_i\\}$ 的分段线性单变量函数可以由一个单隐藏层 ReLU 网络精确表示为 $g(t) = \\alpha_0 + \\alpha_1 t + \\sum_i \\beta_i \\max\\{0, t - c_i\\}$，其中系数由各段的斜率决定。\n- 目标函数为 $[0,1]^2$ 上的 $f(x,y) = x \\cdot y$。\n\n任务：\n1. 从第一性原理出发，推导如何将 $f(x,y) = x \\cdot y$ 简化为单变量函数的复合，以便一个能够精确计算单变量函数分段线性逼近的 MLP 可以用来逼近 $f(x,y)$。从一个通过单变量函数表示 $x \\cdot y$ 的有效恒等式开始，然后使用 ReLU 单元推导出最终的 MLP 结构。\n2. 使用数值分析中经过充分检验的结论，推导在区间 $[-R,R]$ 上用 $M$ 个等长分段对一个二阶可导函数 $s(t)$ 进行线性插值时的一致逼近误差界。将此结论应用于 $s(t) = t^2$，然后通过您为 $x \\cdot y$ 导出的简化公式传播误差，以说明如何选择 $M$（从而确定 MLP 隐藏层的宽度），使得对 $x \\cdot y$ 的最终逼近误差在 $[0,1]^2$ 上一致地满足 $\\le \\epsilon$。\n3. 分析深度与宽度的权衡。提供两种渐近构造：\n   - 一个浅宽网络，它使用单个隐藏层来精确计算 $[-R,R]$ 上具有 $M$ 个分段的 $t^2$ 的分段线性逼近（深度恒定，宽度随 $M$ 增长）。\n   - 一个深窄网络，它通过复合获得相同数量的有效线性段，其中线性段的数量随深度呈指数增长；证明在假设使用 ReLU 运算的构造性复合方案下，深度可以为 $O(\\log M)$，而宽度受一个与 $M$ 无关的微小常数限制。\n   使用基本定义和逻辑来证明这两种构造的合理性，不要使用未经证明的捷径。\n4. 实现要求：\n   - 具体实现浅宽构造。使用 $M$ 个等长分段在 $[-2,2]$ 上构建 $t^2$ 的单变量分段线性逼近 $S(t)$，将 $S$ 精确表示为一个在分段边界处有断点的单隐藏层 ReLU 网络，然后通过您的简化公式，仅使用 $S(x)$、$S(y)$ 和 $S(x+y)$ 的仿射组合来计算 $x \\cdot y$ 的逼近。您的程序必须计算 $[0,1]^2$ 网格上的一致逼近误差，并验证其 $\\le \\epsilon$。\n   - 计算并报告此浅宽网络作为 $\\epsilon$ 函数的理论宽度（使用最宽隐藏层中的隐藏单元数量作为宽度），并报告使用复合论证达到相同误差容差所需的深窄网络的理论深度（您可以假设深窄构造的宽度为一个固定的微小常数）。\n5. 测试套件规范：\n   - 使用误差容差 $\\epsilon \\in \\{\\, 10^{-1}, \\, 10^{-2}, \\, 2.5 \\times 10^{-3} \\,\\}$。\n   - 对每个 $\\epsilon$，根据您推导的规则选择 $M$，并在一个均匀笛卡尔网格上评估浅宽网络的最大绝对误差 $\\sup_{(x,y) \\in [0,1]^2} | \\hat{f}(x,y) - x y |$，其中 $x$ 和 $y$ 各自在 $[0,1]$ 中（包括端点）以 $N$ 个等距点进行采样，$N$ 是一个正整数，其选择应足够大以在实践中捕捉到上确界。显式地包括 $x=0$、$x=1$、$y=0$ 和 $y=1$。\n   - 对每个 $\\epsilon$，报告三个量：实测一致误差（一个浮点数）、浅宽网络的理论宽度（一个整数），以及达到相同误差容差的深窄网络的理论深度（一个整数）。\n6. 最终输出格式：\n   - 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为逗号分隔的列表的列表，每个内部列表的形式为 $[\\text{error}, \\text{width}, \\text{depth}]$。也就是说，打印一个类似 $[[e_1,w_1,d_1],[e_2,w_2,d_2],[e_3,w_3,d_3]]$ 的单个字符串，不含多余的空格或文本。\n\n此问题陈述中的所有数学实体必须以 LaTeX 表示法出现。不涉及物理单位、角度或百分比；以小数或整数报告数值。", "solution": "在尝试解答之前，该问题需经过验证过程。\n\n### 第 1 步：提取已知信息\n- **目标函数**：在域 $[0,1]^2$ 上的 $f(x,y) = x \\cdot y$。\n- **逼近容差**：一个非负误差容差 $\\epsilon$。\n- **激活函数**：修正线性单元 (ReLU)，$\\sigma(t) = \\max\\{0,t\\}$。\n- **MLP 定义**：一个由交替的仿射变换和非线性函数构成的复合函数。\n- **分段线性函数表示**：一个带有断点 $\\{c_i\\}$ 的单变量分段线性函数 $g(t)$ 可以表示为 $g(t) = \\alpha_0 + \\alpha_1 t + \\sum_i \\beta_i \\max\\{0, t - c_i\\}$。\n- **任务**：\n    1. 推导将 $f(x,y)$ 简化为适合 MLP 逼近的单变量函数。\n    2. 推导逼近的误差界以及选择线性段数 $M$ 作为 $\\epsilon$ 函数的规则。\n    3. 分析浅宽网络与深窄网络的架构权衡。\n    4. 实现浅宽构造。在 $[-2,2]$ 上用 $M$ 个分段逼近 $s(t) = t^2$。验证误差 $\\le \\epsilon$。\n    5. 为 $\\epsilon \\in \\{10^{-1}, 10^{-2}, 2.5 \\times 10^{-3}\\}$ 计算结果。报告每个 $\\epsilon$ 的实测误差、浅层网络宽度和深层网络深度。\n    6. 最终输出必须是格式为 `[[e1,w1,d1],[e2,w2,d2],[e3,w3,d3]]` 的单个字符串。\n\n### 第 2 步：使用提取的已知信息进行验证\n该问题基于预定义标准进行评估：\n\n-   **科学性**：该问题基于成熟的数学领域——逼近理论及其在神经网络中的应用，特别是关于 ReLU 网络的通用逼近定理和构造性证明。使用极化恒等式和标准数值分析误差界是科学合理的。\n-   **适定性**：该问题是适定的。它要求一个基于给定误差容差 $\\epsilon$ 的构造性过程和分析，这会导出一组确定的架构参数和一个可验证的误差度量。\n-   **客观性**：语言正式且无歧义。所有术语在数学和计算机科学中都是标准术语。\n-   **完整性与一致性**：该问题提供了进行唯一推理路线所需的所有必要定义和约束，没有矛盾之处。\n-   **现实性与可行性**：这些任务在数学上是可行的，构成了深度学习中的一个标准理论练习。\n-   **结构与其他标准**：该问题结构良好、具有非平凡性且可验证。\n\n### 第 3 步：结论与行动\n该问题有效。将提供完整解答。\n\n### 基于第一性原理的解答\n\n#### 1. 将乘法简化为单变量函数\n任务的核心是逼近二元函数 $f(x,y) = x \\cdot y$。通过使用极化恒等式，可以将其简化为对一个单变量函数的逼近。和的平方由 $(x+y)^2 = x^2 + 2xy + y^2$ 给出。重新整理此恒等式以解出 $x \\cdot y$ 可得：\n$$\nx \\cdot y = \\frac{1}{2} \\left( (x+y)^2 - x^2 - y^2 \\right)\n$$\n这个恒等式表明，乘法可以仅使用单变量平方函数 $s(t) = t^2$ 和线性组合来表示。因此，如果我们能构建一个逼近 $s(t) = t^2$ 的 MLP，我们就能构建一个逼近 $x \\cdot y$ 的 MLP。\n设 $S(t)$ 是由 ReLU 网络计算的 $s(t) = t^2$ 的一个分段线性逼近。相应的 $f(x,y)$ 的逼近，我们记为 $\\hat{f}(x,y)$，是：\n$$\n\\hat{f}(x,y) = \\frac{1}{2} \\left( S(x+y) - S(x) - S(y) \\right)\n$$\n$(x,y)$ 的问题域是 $[0,1]^2$。函数 $S(t)$ 的参数将是 $x \\in [0,1]$、$y \\in [0,1]$ 和 $x+y \\in [0,2]$。实现要求指定在区间 $[-2,2]$ 上逼近 $t^2$，这轻松地包含了所需的域 $[0,2]$。我们将 $s(t)$ 的逼近范围设定为 $[-R, R]$，其中 $R=2$。\n\n#### 2. 误差分析与架构规模确定\n我们必须确定分段逼近 $S(t)$ 所需的线性段数 $M$，以实现最终误差 $|\\hat{f}(x,y) - xy| \\le \\epsilon$。策略是首先限定 $S(t)$ 的误差，然后通过简化恒等式传播该误差。\n\n根据数值分析中的标准结果，在区间 $[a,b]$ 上对一个二阶可导函数 $g(t)$ 进行分段线性插值的一致误差界为 $|g(t) - L(t)| \\le \\frac{1}{8}(b-a)^2 \\sup_{t \\in [a,b]} |g''(t)|$。\n我们正在创建一个分段线性函数 $S(t)$，它在区间 $[-R,R] = [-2,2]$ 上用 $M$ 个等长分段插值 $s(t)=t^2$。每个分段的长度是 $h = \\frac{2R}{M} = \\frac{4}{M}$。$S(t)$ 在任何单个分段上，因此在 $[-2,2]$ 上一致的误差界由下式给出：\n$$\n\\epsilon_S = \\sup_{t \\in [-2,2]} |S(t) - s(t)| \\le \\frac{1}{8}h^2 \\sup_{t \\in [-2,2]} |s''(t)|\n$$\n对于 $s(t) = t^2$，其二阶导数是 $s''(t) = 2$，一个常数。该界变为：\n$$\n\\epsilon_S \\le \\frac{1}{8} \\left(\\frac{4}{M}\\right)^2 \\cdot 2 = \\frac{1}{8} \\frac{16}{M^2} \\cdot 2 = \\frac{4}{M^2}\n$$\n现在，我们将此误差传播到 $x \\cdot y$ 的逼近中。总误差为：\n$$\n|\\hat{f}(x,y) - f(x,y)| = \\left| \\frac{1}{2}(S(x+y) - S(x) - S(y)) - \\frac{1}{2}((x+y)^2 - x^2 - y^2) \\right|\n$$\n$$\n= \\frac{1}{2} |(S(x+y) - (x+y)^2) - (S(x) - x^2) - (S(y) - y^2)|\n$$\n根据三角不等式：\n$$\n\\le \\frac{1}{2} \\left( |S(x+y) - (x+y)^2| + |S(x) - x^2| + |S(y) - y^2| \\right)\n$$\n由于 $x,y,x+y$ 都在定义了 $S(t)$ 的域 $[-2,2]$ 内，每一项都受 $\\epsilon_S$ 限制：\n$$\n|\\hat{f}(x,y) - f(x,y)| \\le \\frac{1}{2}(\\epsilon_S + \\epsilon_S + \\epsilon_S) = \\frac{3}{2}\\epsilon_S\n$$\n代入 $\\epsilon_S$ 的界，我们得到总误差界：\n$$\n\\epsilon_{xy} \\le \\frac{3}{2} \\left(\\frac{4}{M^2}\\right) = \\frac{6}{M^2}\n$$\n为确保误差在容差 $\\epsilon$ 之内，我们要求 $\\frac{6}{M^2} \\le \\epsilon$，这意味着 $M^2 \\ge \\frac{6}{\\epsilon}$。由于 $M$ 必须是整数，我们必须选择：\n$$\nM = \\left\\lceil \\sqrt{\\frac{6}{\\epsilon}} \\right\\rceil\n$$\n\n#### 3. 架构权衡：浅宽网络与深窄网络\n\n**浅宽构造：**\n一个具有 $M$ 个分段（因此有 $M-1$ 个断点）的分段线性函数可以由一个单隐藏层 ReLU 网络精确表示。具有断点 $\\{c_i\\}_{i=1}^{M-1}$ 的函数 $S(t)$ 由下式给出：\n$$\nS(t) = \\alpha_0 + \\alpha_1 t + \\sum_{i=1}^{M-1} \\beta_i \\max\\{0, t - c_i\\}\n$$\n这在隐藏层中需要 $M-1$ 个 ReLU 神经元。为了实现 $\\hat{f}(x,y)$，我们需要计算 $S(x)$、$S(y)$ 和 $S(x+y)$。这可以在一个更宽的单一隐藏层内并行完成。所需的隐藏单元是 $\\max\\{0, x-c_i\\}$、$\\max\\{0, y-c_i\\}$ 和 $\\max\\{0, x+y-c_i\\}$，对于所有 $i=1, \\dots, M-1$。这导致总共有 $3(M-1)$ 个隐藏神经元。输出是这些神经元输出的线性组合。架构是：输入（2个神经元）$\\to$ 隐藏层（$3(M-1)$个神经元）$\\to$ 输出（1个神经元）。这个浅层网络的宽度，定义为最宽隐藏层的大小，是：\n$$\nW = 3(M-1) = 3\\left(\\left\\lceil \\sqrt{\\frac{6}{\\epsilon}} \\right\\rceil - 1\\right)\n$$\n这是一个常数深度（深度为 2，即一个隐藏层）的网络，其宽度以 $O(1/\\sqrt{\\epsilon})$ 的速度增长。\n\n**深窄构造：**\n或者，一个深而窄的网络可以实现相同的逼近。关键思想是，复合 ReLU 单元层可以使线性段的数量相对于深度呈指数增长。可以构建一个简单的、恒定宽度的网络模块，以有效地将其输入函数的分段数量加倍。对于定义在 $[0,1]$ 上的函数，一个模块可以将其映射到一个具有两倍“频率”的新函数。\n让一个恒定宽度的单隐藏层能够将一个有 $k$ 个线性段的函数转换为一个有 $2k$ 个分段的函数。通过复合 $d$ 个这样的层，我们可以生成一个有 $2^d k_0$ 个分段的函数，其中 $k_0$ 是开始时的分段数。为了达到所需的 $M$ 个分段，我们需要 $2^d \\approx M$，这意味着 $d \\approx \\log_2 M$。所需的最小整数层数，即深度，为：\n$$\nD = \\lceil \\log_2 M \\rceil = \\left\\lceil \\log_2 \\left\\lceil \\sqrt{\\frac{6}{\\epsilon}} \\right\\rceil \\right\\rceil\n$$\n这代表了一个深度为 $O(\\log(1/\\sqrt{\\epsilon}))$ 且宽度为微小常数的网络。\n\n#### 4. 实现细节与测试用例计算\n为了实现浅宽网络，我们需要 $S(t)$ 的具体系数。\n断点是 $c_k = -2 + k \\cdot h$，对于 $k=1, \\dots, M-1$，其中 $h=4/M$。\n$S(t)$ 的斜率在每个断点 $c_k$ 处发生变化。斜率的变化是 $\\beta_k$。分段 $[c_{k}, c_{k+1}]$ 的斜率是 $m_k = c_k+c_{k+1}$。变化是 $\\beta_k = m_{k}-m_{k-1} = (c_k+c_{k+1})-(c_{k-1}+c_k) = c_{k+1}-c_{k-1} = 2h = 8/M$。所以，对所有 $k$，$\\beta_k = 8/M$。\n第一段的斜率是 $\\alpha_1 = m_0 = c_0+c_1 = -2 + (-2+h) = h-4 = 4/M-4$。\n起始点的值是 $S(-2) = s(-2) = (-2)^2 = 4$。使用公式，$S(-2) = \\alpha_0 + \\alpha_1(-2) = 4$。\n这得到 $\\alpha_0 = 4 + 2\\alpha_1 = 4 + 2(4/M - 4) = 4 + 8/M - 8 = 8/M - 4$。\n$S(t)$ 的系数是：\n- $\\alpha_0 = \\frac{8}{M} - 4$\n- $\\alpha_1 = \\frac{4}{M} - 4$\n- $\\beta_k = \\frac{8}{M}$，对于 $k=1, \\dots, M-1$。\n\n对于给定的测试用例，架构参数为：\n1.  **$\\epsilon = 10^{-1}$**:\n    $M = \\lceil \\sqrt{6/0.1} \\rceil = \\lceil \\sqrt{60} \\rceil = 8$。\n    宽度 $W = 3(8-1) = 21$。\n    深度 $D = \\lceil \\log_2 8 \\rceil = 3$。\n2.  **$\\epsilon = 10^{-2}$**:\n    $M = \\lceil \\sqrt{6/0.01} \\rceil = \\lceil \\sqrt{600} \\rceil = 25$。\n    宽度 $W = 3(25-1) = 72$。\n    深度 $D = \\lceil \\log_2 25 \\rceil = 5$。\n3.  **$\\epsilon = 2.5 \\times 10^{-3}$**:\n    $M = \\lceil \\sqrt{6/0.0025} \\rceil = \\lceil \\sqrt{2400} \\rceil = 49$。\n    宽度 $W = 3(49-1) = 144$。\n    深度 $D = \\lceil \\log_2 49 \\rceil = 6$。\n\n最终答案中的程序将实现此逻辑并验证误差界。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Derives and verifies an MLP architecture for approximating f(x,y)=xy.\n\n    This function implements the shallow-wide network construction derived from\n    first principles. It calculates the required number of linear segments (M) for a\n    given error tolerance epsilon, constructs the piecewise linear approximation\n    S(t) to t^2, and evaluates the uniform approximation error for xy on a\n    fine grid over [0,1]^2. It also reports the theoretical width of this\n    network and the theoretical depth of an equivalent deep-narrow network.\n    \"\"\"\n\n    test_cases = [1e-1, 1e-2, 2.5e-3]\n    results = []\n    \n    # Use a sufficiently large grid for error verification.\n    N_GRID = 201\n\n    def S_approx(t, M):\n        \"\"\"\n        Computes the piecewise linear approximation of s(t) = t^2 on [-2, 2]\n        using M equal-length segments.\n        S(t) = alpha_0 + alpha_1*t + sum(beta_i * max(0, t - c_i))\n        \n        Args:\n            t (np.ndarray): Input values.\n            M (int): Number of linear segments.\n            \n        Returns:\n            np.ndarray: The approximated values s(t).\n        \"\"\"\n        if M == 0:\n            # Should not happen with the formula for M\n            return t**2\n        \n        is_scalar = not isinstance(t, np.ndarray)\n        if is_scalar:\n            t = np.array([t])\n            \n        # Interval [-R, R] with R=2\n        R = 2.0\n        h = 2.0 * R / float(M)\n\n        # Coefficients for the ReLU network representation\n        alpha_0 = (2.0 * h) - (2.0 * R) # Simplified from 8/M - 4\n        alpha_1 = h - (2.0 * R)       # Simplified from 4/M - 4\n        beta = 2.0 * h                # Simplified from 8/M\n\n        # Breakpoints\n        c = -R + np.arange(1, M) * h\n\n        # Reshape for broadcasting\n        t_reshaped = t.reshape(-1, 1)\n        c_reshaped = c.reshape(1, -1)\n        \n        # ReLU activations\n        relu_terms = np.sum(np.maximum(0, t_reshaped - c_reshaped), axis=1)\n        \n        # Final piecewise linear function\n        s_val = alpha_0 + alpha_1 * t + beta * relu_terms.reshape(t.shape)\n\n        if is_scalar:\n            return s_val.item()\n        return s_val\n\n    def f_hat(x, y, M):\n        \"\"\"\n        Approximates f(x,y) = xy using the identity and S_approx.\n        f_hat(x,y) = 0.5 * (S(x+y) - S(x) - S(y))\n        \"\"\"\n        return 0.5 * (S_approx(x + y, M) - S_approx(x, M) - S_approx(y, M))\n\n    for epsilon in test_cases:\n        # 1. Determine M based on the derived formula\n        M = math.ceil(math.sqrt(6.0 / epsilon))\n\n        # 2. Evaluate the uniform error on a grid\n        grid_points = np.linspace(0, 1, N_GRID)\n        X, Y = np.meshgrid(grid_points, grid_points)\n        \n        Z_true = X * Y\n        Z_hat = f_hat(X, Y, M)\n        \n        measured_error = np.max(np.abs(Z_hat - Z_true))\n\n        # 3. Calculate theoretical width of the shallow-wide network\n        # Width = 3 * (M - 1)\n        theoretical_width = 3 * (M - 1)\n\n        # 4. Calculate theoretical depth of the deep-narrow network\n        # Depth = ceil(log2(M))\n        theoretical_depth = math.ceil(math.log2(M)) if M > 0 else 0\n        \n        results.append([measured_error, theoretical_width, theoretical_depth])\n    \n    # Format the final output string exactly as required.\n    # str(list) produces \"[item1, item2, ...]\" so this will create the nested structure\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3155543"}, {"introduction": "理论上的函数构造为我们提供了架构能力的洞察，但在实践中，我们还需要通过训练来获得最终模型。本练习将带你从理论走向实践，要求你训练一个 MLP 来逼近一个不连续的阶跃函数 [@problem_id:3151131]。通过量化分析模型在不连续点附近的“吉布斯现象”（Gibbs-like phenomena），你将亲身体会到使用平滑激活函数的 MLP 在拟合非平滑目标时的固有局限性。", "problem": "要求您设计并分析一个小型多层感知机 (MLP)，用以近似一个不连续函数，并使用一种规范的度量方法量化跳跃间断点附近的吉布斯类现象。考虑一个一维输入 $x \\in \\mathbb{R}$ 和由下式定义的目标函数\n$$\nf(x) = \\begin{cases}\n-1,  x  0, \\\\\n+1,  x \\ge 0.\n\\end{cases}\n$$\n该函数在 $x_0 = 0$ 处有一个跳跃间断点。多层感知机 (MLP) 是一种前馈结构，具有一个宽度为 $m$ 的隐藏层、一个平滑激活函数和一个线性输出，其形式如下\n$$\ng(x; \\theta) = \\mathbf{v}^{\\top}\\,\\phi\\!\\left(\\mathbf{W}x + \\mathbf{b}\\right) + c,\n$$\n其中 $\\phi(\\cdot)$ 是双曲正切激活函数 $\\tanh(\\cdot)$，$\\mathbf{W} \\in \\mathbb{R}^{m \\times 1}$，$\\mathbf{b} \\in \\mathbb{R}^{m}$，$\\mathbf{v} \\in \\mathbb{R}^{m}$，以及 $c \\in \\mathbb{R}$；这些参数统称为 $\\theta = (\\mathbf{W}, \\mathbf{b}, \\mathbf{v}, c)$。\n\n您必须通过最小化以下目标函数，在一个训练集 $\\{(x_i, y_i)\\}_{i=1}^N$（其中 $x_i \\sim \\operatorname{Uniform}([-1,1])$ 且 $y_i = f(x_i)$）上实现带有 $\\ell_2$ 正则化的经验风险最小化：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left(g(x_i;\\theta) - y_i\\right)^2 + \\lambda\\left(\\|\\mathbf{W}\\|_2^2 + \\|\\mathbf{b}\\|_2^2 + \\|\\mathbf{v}\\|_2^2 + c^2\\right),\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度，$\\|\\cdot\\|_2$ 是欧几里得范数。使用批量梯度下降法，以学习率 $\\eta$ 训练指定的周期数，所有参数从具有固定尺度的零均值高斯分布中初始化。激活函数 $\\tanh$ 是可微的，因此损失函数关于 $\\theta$ 的所有分量也都是可微的。\n\n为量化跳跃点附近的吉布斯类过冲，定义一个以 $x_0 = 0$ 为中心、半径为 $r > 0$ 的对称分析窗口，并计算局部过冲幅度为\n$$\nA(r; \\theta) = \\max\\left\\{\n\\max_{x \\in [0,r]} \\max\\left(0,\\, g(x;\\theta) - 1\\right),\\quad\n\\max_{x \\in [-r,0)} \\max\\left(0,\\, -\\left(g(x;\\theta) + 1\\right)\\right)\n\\right\\}.\n$$\n这将 $g(x;\\theta)$ 与跳跃点两侧的目标平台值进行比较。报告归一化吉布斯类比率\n$$\n\\rho(r; \\theta) = \\frac{A(r; \\theta)}{2},\n$$\n即过冲幅度除以跳跃高度 $2$。所有量均为无量纲。\n\n您的程序必须：\n- 使用给定的随机种子在 $[-1,1]$ 上均匀采样 $x_i$ 来构建大小为 $N$ 的训练集，以确保可复现性，并令 $y_i = f(x_i)$。\n- 使用给定的种子，从均值为零、标准差为 $0.1$ 的高斯分布中初始化参数 $\\theta$。\n- 使用批量梯度下降法训练 MLP，采用指定的周期数、学习率 $\\eta$ 和正则化参数 $\\lambda$。\n- 训练后，在 $[-r,r]$ 上的一个精细网格上评估 $g(x;\\theta)$，以计算如上定义的 $A(r;\\theta)$ 和 $\\rho(r;\\theta)$。\n- 对于测试套件中的每个测试用例，输出 $\\rho(r;\\theta)$ 的值。\n\n使用以下带有参数 $(m, \\lambda, \\eta, \\text{epochs}, r, \\text{seed}, N)$ 的测试套件：\n1. $(10,\\, 0.001,\\, 0.1,\\, 2000,\\, 0.1,\\, 0,\\, 1024)$，一个通用的“理想路径”用例。\n2. $(2,\\, 0.0,\\, 0.1,\\, 2000,\\, 0.1,\\, 1,\\, 1024)$，一个低容量边界用例。\n3. $(10,\\, 0.1,\\, 0.1,\\, 2000,\\, 0.1,\\, 2,\\, 1024)$，一个强正则化边缘用例。\n4. $(10,\\, 0.001,\\, 0.05,\\, 4000,\\, 0.01,\\, 3,\\, 1024)$，一个窄窗口边缘用例。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_k$ 是为第 $k$ 个测试用例计算出的 $\\rho(r;\\theta)$。不应打印任何额外文本。", "solution": "用户提供的问题被评估为**有效**。这是一个统计学习领域中适定的、有科学依据的问题，具体涉及使用多层感知机 (MLP) 近似不连续函数，并对由此产生的吉布斯类现象进行定量分析。问题陈述是自洽的、客观的，并为唯一的、可计算的解提供了所有必要的参数和定义。\n\n任务是训练一个单隐藏层 MLP 来近似一个阶跃函数，然后量化间断点附近的过冲。解决方案涉及实现批量梯度下降以最小化一个正则化的均方误差损失函数，然后数值评估一个自定义的过冲度量。\n\n目标函数是符号函数，在 $x=0$ 处有一个跳跃间断点：\n$$\nf(x) = \\begin{cases}\n-1,  x  0, \\\\\n+1,  x \\ge 0.\n\\end{cases}\n$$\n我们使用一个具有宽度为 m 的单个隐藏层的 MLP 来近似该函数。模型对输入 $x \\in \\mathbb{R}$ 的输出由下式给出：\n$$\ng(x; \\theta) = \\mathbf{v}^{\\top}\\,\\phi\\!\\left(\\mathbf{W}x + \\mathbf{b}\\right) + c\n$$\n此处，参数为 $\\theta = (\\mathbf{W}, \\mathbf{b}, \\mathbf{v}, c)$，其中 $\\mathbf{W} \\in \\mathbb{R}^{m \\times 1}$，$\\mathbf{b} \\in \\mathbb{R}^{m}$，$\\mathbf{v} \\in \\mathbb{R}^{m}$，以及 $c \\in \\mathbb{R}$。激活函数 $\\phi(\\cdot)$ 是双曲正切函数 $\\tanh(\\cdot)$，这是一个平滑、可微的函数。因此，MLP $g(x; \\theta)$ 也是 $x$ 的一个平滑函数。\n\n训练过程基于经验风险最小化。给定一个训练集 $\\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i$ 从 $\\operatorname{Uniform}([-1,1])$ 分布中采样，且 $y_i=f(x_i)$，我们的目标是找到使目标函数 $\\mathcal{L}(\\theta)$ 最小化的参数 $\\theta$。该函数由一个均方误差 (MSE) 项和一个 $\\ell_2$ 正则化项组成：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left(g(x_i;\\theta) - y_i\\right)^2 + \\lambda\\left(\\|\\mathbf{W}\\|_2^2 + \\|\\mathbf{b}\\|_2^2 + \\|\\mathbf{v}\\|_2^2 + c^2\\right)\n$$\n由 $\\lambda \\ge 0$ 缩放的正则化项惩罚大的参数值，这有助于防止过拟合，并可能产生更平滑的解。\n\n优化是使用批量梯度下降法进行的。在每个周期中，我们计算整个损失函数 $\\mathcal{L}(\\theta)$ 关于每个参数的梯度，并在与梯度相反的方向上更新参数。对于一个通用参数 $p \\in \\theta$ 的更新规则是 $p \\leftarrow p - \\eta \\nabla_p \\mathcal{L}(\\theta)$，其中 $\\eta$ 是学习率。\n\n为了实现这一点，我们需要解析梯度。我们定义一个包含 $N$ 个训练样本的批次（表示为形状为 $(N, 1)$ 的矩阵 $\\mathbf{X}$）的前向传播。参数向量 $\\mathbf{b}$ 和 $\\mathbf{v}$ 被视为形状为 $(m, 1)$ 的列向量。\n1.  隐藏层预激活：$\\mathbf{Z} = \\mathbf{X}\\mathbf{W}^{\\top} + \\mathbf{b}^{\\top}$。此操作中 $\\mathbf{W}^{\\top}$ 是 $(1, m)$，$\\mathbf{b}^{\\top}$ 是 $(1, m)$，结果 $\\mathbf{Z}$ 是一个形状为 $(N,m)$ 的矩阵。\n2.  隐藏层激活：A = $\\phi(\\mathbf{Z}) = \\tanh(\\mathbf{Z})$，形状为 $(N,m)$。\n3.  模型输出：$\\mathbf{Y}_{\\text{pred}} = \\mathbf{A}\\mathbf{v} + c$，形状为 $(N,1)$。\n\n使用链式法则，我们可以推导出反向传播的梯度。设批次的误差为 $\\mathbf{E} = \\mathbf{Y}_{\\text{pred}} - \\mathbf{Y}_{\\text{train}}$。MSE项关于输出 $\\mathbf{Y}_{\\text{pred}}$ 的梯度是 $\\nabla_{\\mathbf{Y}_{\\text{pred}}} \\mathcal{L}_{\\text{MSE}} = \\frac{2}{N}\\mathbf{E}$。\n总损失 $\\mathcal{L}$ 的梯度为：\n-   关于 $c$：$\\nabla_c \\mathcal{L} = \\frac{2}{N}\\sum_i E_i + 2\\lambda c$。\n-   关于 $\\mathbf{v}$：$\\nabla_{\\mathbf{v}} \\mathcal{L} = \\mathbf{A}^{\\top}(\\frac{2}{N}\\mathbf{E}) + 2\\lambda \\mathbf{v}$。\n-   梯度被反向传播到隐藏层。损失关于预激活 $\\mathbf{Z}$ 的梯度是 $\\nabla_{\\mathbf{Z}} \\mathcal{L} = (\\frac{2}{N}\\mathbf{E} \\cdot \\mathbf{v}^{\\top}) \\odot \\phi'(\\mathbf{Z})$，其中 $\\odot$ 是逐元素乘积，$\\phi'(z) = 1 - \\tanh^2(z)$。\n-   关于 $\\mathbf{b}$：$\\nabla_{\\mathbf{b}} \\mathcal{L} = (\\sum_{i=1}^N (\\nabla_{\\mathbf{Z}} \\mathcal{L})_i)^{\\top} + 2\\lambda\\mathbf{b}$，其中求和是在批次维度上进行的。\n-   关于 $\\mathbf{W}$：$\\nabla_{\\mathbf{W}} \\mathcal{L} = (\\mathbf{X}^{\\top} (\\nabla_{\\mathbf{Z}} \\mathcal{L}))^{\\top} + 2\\lambda\\mathbf{W}$。\n\n在训练模型指定周期数后，我们分析吉布斯类现象。这通过在间断点 $x_0=0$ 周围的对称窗口 $[-r, r]$ 内的过冲幅度 $A(r; \\theta)$ 来量化：\n$$\nA(r; \\theta) = \\max\\left\\{\n\\max_{x \\in [0,r]} \\max\\left(0,\\, g(x;\\theta) - 1\\right),\\quad\n\\max_{x \\in [-r,0)} \\max\\left(0,\\, -\\left(g(x;\\theta) + 1\\right)\\right)\n\\right\\}\n$$\n该度量捕捉了学习到的函数 $g(x; \\theta)$ 与目标平台值 $+1$ 和 $-1$ 的最大偏差。在数值上，我们在 $[0,r]$ 和 $[-r,0)$ 内的精细点网格上评估 $g(x; \\theta)$ 以近似最大值。最终报告的值是归一化吉布斯类比率 $\\rho_r(\\theta) = A(r; \\theta) / 2$，其中 $2$ 是跳跃间断点的总高度。\n\n实现将遍历每个测试用例，在固定随机种子下执行数据生成和参数初始化以确保可复现性，运行批量梯度下降优化，最后计算并存储 $\\rho(r; \\theta)$ 的值。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the MLP training and analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # (m, lambda, eta, epochs, r, seed, N)\n        (10, 0.001, 0.1, 2000, 0.1, 0, 1024),\n        (2, 0.0, 0.1, 2000, 0.1, 1, 1024),\n        (10, 0.1, 0.1, 2000, 0.1, 2, 1024),\n        (10, 0.001, 0.05, 4000, 0.01, 3, 1024),\n    ]\n\n    results = []\n\n    for m, reg_lambda, eta, epochs, r, seed, N in test_cases:\n        \n        # 1. Setup: Seeding for reproducibility\n        rng = np.random.default_rng(seed)\n        std_dev = 0.1\n        num_eval_points_half = 1000\n\n        # 2. Data Generation and Parameter Initialization\n        # Training data\n        x_train = rng.uniform(-1, 1, (N, 1))\n        y_train = np.where(x_train  0, -1.0, 1.0)\n\n        # MLP parameters, shapes based on the problem statement\n        # W in R^{m x 1}, b in R^m, v in R^m, c in R\n        W = rng.normal(0, std_dev, (m, 1))\n        b = rng.normal(0, std_dev, (m, 1))\n        v = rng.normal(0, std_dev, (m, 1))\n        c = rng.normal(0, std_dev)\n\n        # 3. Training using Batch Gradient Descent\n        for _ in range(epochs):\n            # Forward pass\n            Z = x_train @ W.T + b.T\n            A = np.tanh(Z)\n            y_pred = A @ v + c\n\n            # Backward pass (Gradients)\n            d_loss_ypred = (2 / N) * (y_pred - y_train)\n\n            # Gradient for c (scalar bias)\n            grad_c = np.sum(d_loss_ypred) + 2 * reg_lambda * c\n            \n            # Gradient for v (output weights)\n            grad_v = A.T @ d_loss_ypred + 2 * reg_lambda * v\n\n            # Backpropagate to hidden layer\n            d_loss_Z = (d_loss_ypred @ v.T) * (1 - A**2)\n            \n            # Gradient for b (hidden biases)\n            grad_b = np.sum(d_loss_Z, axis=0, keepdims=True).T + 2 * reg_lambda * b\n            \n            # Gradient for W (input weights)\n            grad_W = (x_train.T @ d_loss_Z).T + 2 * reg_lambda * W\n\n            # Parameter update\n            W -= eta * grad_W\n            b -= eta * grad_b\n            v -= eta * grad_v\n            c -= eta * grad_c\n        \n        # 4. Gibbs-like Ratio Calculation\n        def g_forward(x_vals, W_p, b_p, v_p, c_p):\n            \"\"\"Evaluates the trained MLP on a set of input points.\"\"\"\n            x_vals_col = x_vals.reshape(-1, 1)\n            Z_eval = x_vals_col @ W_p.T + b_p.T\n            A_eval = np.tanh(Z_eval)\n            return A_eval @ v_p + c_p\n\n        # Create evaluation grids for positive and negative sides of the jump\n        x_pos_eval = np.linspace(0, r, num_eval_points_half)\n        x_neg_eval = np.linspace(-r, 0, num_eval_points_half, endpoint=False)\n\n        # Evaluate model and find overshoot on the positive side\n        g_pos = g_forward(x_pos_eval, W, b, v, c)\n        overshoot_pos = np.max(np.maximum(0, g_pos - 1))\n\n        # Evaluate model and find overshoot on the negative side\n        overshoot_neg = 0.0\n        if x_neg_eval.size > 0:\n            g_neg = g_forward(x_neg_eval, W, b, v, c)\n            overshoot_neg = np.max(np.maximum(0, -(g_neg + 1)))\n\n        # Compute the overshoot amplitude A(r; theta) and normalized ratio rho\n        A_r_theta = np.max([overshoot_pos, overshoot_neg])\n        rho_r_theta = A_r_theta / 2.0\n        \n        results.append(rho_r_theta)\n\n    # 5. Output in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3151131"}]}