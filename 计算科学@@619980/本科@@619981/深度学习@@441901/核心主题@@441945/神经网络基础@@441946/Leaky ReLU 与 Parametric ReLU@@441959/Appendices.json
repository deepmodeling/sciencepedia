{"hands_on_practices": [{"introduction": "要让PReLU的参数$\\alpha$真正“参数化”，就必须能通过梯度下降法来学习它。本练习将引导你完成一个核心演算：利用链式法则推导出$\\alpha$的梯度，这是理解和开发自定义神经网络组件的一项基本技能。通过这个具体的计算问题[@problem_id:3101068]，你还将看到如何对$\\alpha$参数本身施加正则化。", "problem": "考虑一个前馈网络中的单个神经元，其激活函数为参数化修正线性单元（PReLU），形式上，该参数化激活函数 $f_{\\alpha}(z)$ 定义为\n$$\nf_{\\alpha}(z) \\;=\\; \\max(0, z) \\;+\\; \\alpha\\,\\min(0, z).\n$$\n设该神经元的预激活值为 $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其输出为 $y = f_{\\alpha}(z)$。对于一个标量目标 $t$，将训练目标定义为数据拟合项与关于 $\\alpha$ 的 $\\ell_{2}$ 正则化项之和：\n$$\nL \\;=\\; \\frac{1}{2}\\,\\big(y - t\\big)^{2} \\;+\\; \\frac{\\lambda}{2}\\,\\alpha^{2}.\n$$\n从微积分的链式法则以及 $\\max(\\cdot,\\cdot)$ 和 $\\min(\\cdot,\\cdot)$ 的定义出发，推导出一个用 $z$、$y$、$t$、$\\alpha$ 和 $\\lambda$ 表示的 $\\frac{\\partial L}{\\partial \\alpha}$ 的显式表达式。然后，使用具体数值 $\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$、$\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$、$b = 0.1$、$\\alpha = 0.25$、$t = -0.3$ 和 $\\lambda = 0.1$，计算 $\\frac{\\partial L}{\\partial \\alpha}$ 的数值。\n\n最后，根据你推导出的表达式，用一两句话简要解释，在梯度下降过程中，对 $\\alpha$ 的正则化如何影响梯度的方向和大小。\n\n将 $\\frac{\\partial L}{\\partial \\alpha}$ 的最终数值答案四舍五入到 3 位有效数字。无需单位。", "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取已知条件\n提供了以下数据和定义：\n-   参数化修正线性单元（PReLU）激活函数：$f_{\\alpha}(z) = \\max(0, z) + \\alpha\\,\\min(0, z)$。\n-   神经元预激活值：$z = \\mathbf{w}^{\\top}\\mathbf{x} + b$。\n-   神经元输出：$y = f_{\\alpha}(z)$。\n-   损失函数（目标函数）：$L = \\frac{1}{2}\\,(y - t)^{2} + \\frac{\\lambda}{2}\\,\\alpha^{2}$。\n-   输入向量：$\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$。\n-   权重向量：$\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$。\n-   偏置：$b = 0.1$。\n-   PReLU 参数：$\\alpha = 0.25$。\n-   目标值：$t = -0.3$。\n-   正则化超参数：$\\lambda = 0.1$。\n-   任务：\n    1.  推导出一个用 $z, y, t, \\alpha, \\lambda$ 表示的 $\\frac{\\partial L}{\\partial \\alpha}$ 的显式表达式。\n    2.  根据给定值计算 $\\frac{\\partial L}{\\partial \\alpha}$ 的数值，并四舍五入到 3 位有效数字。\n    3.  解释正则化项对梯度的影响。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学基础**：该问题基于机器学习和计算神经科学中标准、成熟的概念，包括神经网络激活函数（PReLU）、带 $\\ell_2$ 正则化的损失函数以及基于梯度的优化（反向传播）。其数学框架是合理的。\n-   **适定性**：该问题是适定的。所涉及的函数几乎处处连续可微。提供的数值允许一个唯一且稳定的解。所有必要信息都已提供，不存在矛盾之处。函数 $f_{\\alpha}(z)$ 及其导数在将要遇到的情况下（$z \\neq 0$）是有明确定义的。\n-   **客观性**：该问题使用精确的数学语言陈述，没有主观或含糊的术语。\n\n### 步骤 3：结论与行动\n该问题是有效的，因为它具有科学基础、适定性、客观性，并包含一个完整且一致的设定。因此，我将继续进行完整解答。\n\n### 第 1 部分：梯度表达式的推导\n\n目标是求损失函数 $L$ 关于参数 $\\alpha$ 的偏导数，记为 $\\frac{\\partial L}{\\partial \\alpha}$。损失函数由下式给出：\n$$\nL(\\alpha, y) \\;=\\; \\frac{1}{2}\\,(y - t)^{2} \\;+\\; \\frac{\\lambda}{2}\\,\\alpha^{2}\n$$\n神经元的输出 $y$ 是 $\\alpha$ 和 $z$ 的函数，其中 $y = f_{\\alpha}(z)$。预激活值 $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$ 不依赖于 $\\alpha$。\n我们应用偏微分的链式法则。由于 $L$ 通过正则化项直接依赖于 $\\alpha$，并通过 $y$ 间接依赖于 $\\alpha$，我们有：\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; \\frac{\\partial L}{\\partial y}\\,\\frac{\\partial y}{\\partial \\alpha} \\;+\\; \\frac{\\partial}{\\partial \\alpha}\\left( \\frac{\\lambda}{2}\\,\\alpha^{2} \\right)\n$$\n我们分别计算每一项。\n首先，损失函数对输出 $y$ 的导数：\n$$\n\\frac{\\partial L}{\\partial y} \\;=\\; \\frac{\\partial}{\\partial y}\\left( \\frac{1}{2}\\,(y - t)^{2} \\right) \\;=\\; y - t\n$$\n其次，正则化项对 $\\alpha$ 的导数：\n$$\n\\frac{\\partial}{\\partial \\alpha}\\left( \\frac{\\lambda}{2}\\,\\alpha^{2} \\right) \\;=\\; \\lambda\\alpha\n$$\n第三，神经元输出 $y$ 对 $\\alpha$ 的导数。输出为 $y = f_{\\alpha}(z) = \\max(0, z) + \\alpha\\,\\min(0, z)$。由于 $z$ 不是 $\\alpha$ 的函数，在求此偏导数时我们将其视为常数：\n$$\n\\frac{\\partial y}{\\partial \\alpha} \\;=\\; \\frac{\\partial}{\\partial \\alpha}\\left( \\max(0, z) + \\alpha\\,\\min(0, z) \\right) \\;=\\; 0 + 1 \\cdot \\min(0, z) \\;=\\; \\min(0, z)\n$$\n我们可以通过检查 $f_\\alpha(z)$ 的分段定义来验证这一点。\n如果 $z > 0$，则 $y = z$，所以 $\\frac{\\partial y}{\\partial \\alpha} = 0$。在这种情况下，$\\min(0, z) = 0$。\n如果 $z \\le 0$，则 $y = \\alpha z$，所以 $\\frac{\\partial y}{\\partial \\alpha} = z$。在这种情况下，$\\min(0, z) = z$。\n因此，表达式 $\\frac{\\partial y}{\\partial \\alpha} = \\min(0, z)$ 是正确的。\n\n将这些分量代回链式法则表达式中：\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; (y - t)\\,\\min(0, z) \\;+\\; \\lambda\\alpha\n$$\n这就是用 $z, y, t, \\alpha$ 和 $\\lambda$ 表示的梯度的显式表达式。\n\n### 第 2 部分：数值计算\n\n现在我们使用提供的数据计算 $\\frac{\\partial L}{\\partial \\alpha}$ 的数值。\n-   $\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$，$\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$，$b = 0.1$。\n-   $\\alpha = 0.25$，$t = -0.3$，$\\lambda = 0.1$。\n\n首先，我们计算预激活值 $z$：\n$$\nz \\;=\\; \\mathbf{w}^{\\top}\\mathbf{x} + b \\;=\\; (-0.8)(1.2) + (0.4)(-0.5) + (0.6)(0.3) + 0.1\n$$\n$$\nz \\;=\\; -0.96 - 0.20 + 0.18 + 0.1 \\;=\\; -1.16 + 0.28 \\;=\\; -0.88\n$$\n由于 $z = -0.88  0$，神经元在“泄漏”区域工作。现在我们计算输出 $y$：\n$$\ny \\;=\\; f_{\\alpha}(z) \\;=\\; \\max(0, -0.88) + (0.25)\\,\\min(0, -0.88)\n$$\n$$\ny \\;=\\; 0 + (0.25)(-0.88) \\;=\\; -0.22\n$$\n现在我们有了计算 $\\frac{\\partial L}{\\partial \\alpha}$ 所需的所有分量：\n-   $y - t = -0.22 - (-0.3) = 0.08$\n-   $\\min(0, z) = \\min(0, -0.88) = -0.88$\n-   $\\lambda\\alpha = (0.1)(0.25) = 0.025$\n\n将这些值代入推导出的梯度表达式中：\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; (0.08)(-0.88) + 0.025\n$$\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; -0.0704 + 0.025 \\;=\\; -0.0454\n$$\n问题要求答案四舍五入到 3 位有效数字。计算出的值 $-0.0454$ 已经恰好有三位有效数字（数字 $4$、$5$ 和 $4$）。\n\n### 第 3 部分：正则化的影响\n\n推导出的梯度为 $\\frac{\\partial L}{\\partial \\alpha} = (y - t)\\,\\min(0, z) + \\lambda\\alpha$。其中 $\\lambda\\alpha$ 项是来自 $\\ell_2$ 正则化惩罚项的直接贡献。在梯度下降的更新步骤中，$\\alpha$ 将按 $\\alpha_{new} = \\alpha_{old} - \\eta \\frac{\\partial L}{\\partial \\alpha}$ 更新，这意味着无论数据拟合项如何，正则化项 $\\lambda\\alpha$ 总会为梯度贡献一个分量，该分量作用是推动 $\\alpha$ 的值趋向于零。这种效应被称为权重衰减，它惩罚较大的 $\\alpha$ 值，有助于控制模型复杂度并防止过拟合。", "answer": "$$\n\\boxed{-0.0454}\n$$", "id": "3101068"}, {"introduction": "在学会如何更新$\\alpha$之后，一个自然的问题是：是否应该对$\\alpha$的取值范围进行限制？本练习通过一个$\\alpha$为负值的假想情景，揭示了为何通常需要将$\\alpha$约束为非负。通过构建一个反例[@problem_id:3142552]，你将亲眼看到负斜率如何破坏激活函数重要的单调性，并可能引发不稳定的训练动态。", "problem": "考虑一个带单个神经元的标量回归模型。预激活值为 $z = w x + b$，其中 $x \\in \\mathbb{R}$ 是输入，$w \\in \\mathbb{R}$ 是权重，$b \\in \\mathbb{R}$ 是偏置。激活函数是带参数的修正线性单元 (Parametric Rectified Linear Unit, PReLU)，由分段函数 $f(z)$ 定义，其参数 $\\alpha \\in \\mathbb{R}$ 满足：当 $z \\geq 0$ 时，$f(z) = z$；当 $z  0$ 时，$f(z) = \\alpha z$。神经元的输出为 $y = f(z)$。训练目标是平方误差 $L(y) = (y - t)^{2}$，目标值为 $t \\in \\mathbb{R}$。使用链式法则和给定的定义作为唯一的出发点。\n\n通过考虑以下特定设置，研究负斜率参数 $\\alpha  0$ 对单调性和梯度行为的影响：\n- 选择 $b = 0$，$w = 1$，$\\alpha = -\\frac{1}{3}$。\n- 选择两个输入 $x_{1} = -2$ 和 $x_{2} = -1$ 以及一个目标值 $t = 1$。\n\n任务：\n1. 仅使用 $f(z)$ 的定义和 $\\alpha$ 的符号，构造一个具体的反例，证明当限制在负输入时，$y$作为$x$的函数不是单调不减的。显式地计算 $y$ 在 $x_{1}$ 和 $x_{2}$ 处的值并进行比较。\n2. 对于输入 $x = x_{1} = -2$，通过链式法则和 $f(z)$ 的分段定义计算梯度 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial \\alpha}$。\n3. 解释当 $\\alpha  0$ 时，这些梯度的符号如何与潜在的训练不稳定性相关联。\n\n将您的最终答案表示为包含两个梯度值 $\\left(\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial \\alpha}\\right)$ 的行矩阵。无需四舍五入。", "solution": "首先验证问题陈述，以确保其科学基础扎实、问题定义明确且客观。\n\n### 步骤 1：提取已知条件\n- **模型**：单神经元回归。\n- **预激活值**：$z = w x + b$，其中 $x, w, b \\in \\mathbb{R}$。\n- **激活函数 (PReLU)**：$y = f(z)$，其中如果 $z \\geq 0$ 则 $f(z) = z$，如果 $z  0$ 则 $f(z) = \\alpha z$。$\\alpha \\in \\mathbb{R}$。\n- **损失函数**：平方误差 $L(y) = (y - t)^{2}$，目标值 $t \\in \\mathbb{R}$。\n- **特定设置**：\n    - 权重 $w = 1$。\n    - 偏置 $b = 0$。\n    - PReLU 参数 $\\alpha = -\\frac{1}{3}$。\n    - 输入 $x_{1} = -2$ 和 $x_{2} = -1$。\n    - 目标值 $t = 1$。\n- **任务**：\n    1. 使用 $x_1$ 和 $x_2$ 提供一个反例，证明对于负输入，$y(x)$ 不是单调不减的。\n    2. 计算在 $x = x_1 = -2$ 时的梯度 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial \\alpha}$。\n    3. 解释当 $\\alpha  0$ 时，这些梯度的符号如何与潜在的训练不稳定性相关。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在深度学习领域具有科学依据，特别涉及激活函数和反向传播。模型、PReLU 激活函数和损失函数的定义都是标准的。问题定义明确，为所需的计算和分析提供了所有必要的常量和变量。语言精确且客观。该问题不违反任何无效性标准。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整解答。\n\n### 解答\n\n模型由以下方程定义：\n预激活值：$z = w x + b$\n输出：$y = f(z) = \\begin{cases} z  \\text{若 } z \\geq 0 \\\\ \\alpha z  \\text{若 } z  0 \\end{cases}$\n损失：$L = (y - t)^2$\n\n给定特定参数 $w=1$，$b=0$ 和 $\\alpha = -1/3$。模型简化为：\n$z = x$\n$y = f(x) = \\begin{cases} x  \\text{若 } x \\geq 0 \\\\ -\\frac{1}{3} x  \\text{若 } x  0 \\end{cases}$\n\n#### 1. 单调性的反例\n\n为了证明当 $y$ 作为 $x$ 的函数在负输入上不是单调不减的，我们必须找到 $x_a$ 和 $x_b$ 使得 $x_a  x_b  0$ 但 $y(x_a)  y(x_b)$。我们使用提供的输入 $x_1 = -2$ 和 $x_2 = -1$。\n我们有 $x_1  x_2$。\n\n- 计算 $y$ 在 $x_1 = -2$ 处的值：\n由于 $x_1 = -2  0$，我们使用 $y$ 的分段函数的第二种情况。\n$y_1 = f(x_1) = \\alpha x_1 = \\left(-\\frac{1}{3}\\right)(-2) = \\frac{2}{3}$。\n\n- 计算 $y$ 在 $x_2 = -1$ 处的值：\n由于 $x_2 = -1  0$，我们再次使用第二种情况。\n$y_2 = f(x_2) = \\alpha x_2 = \\left(-\\frac{1}{3}\\right)(-1) = \\frac{1}{3}$。\n\n- 比较输出：\n我们有 $x_1 = -2  x_2 = -1$，但我们发现 $y_1 = \\frac{2}{3} > y_2 = \\frac{1}{3}$。这违反了非递减函数的条件 $y(x_1) \\leq y(x_2)$。因此，我们用一个具体的反例证明了对于 $\\alpha  0$，PReLU 激活函数在负数域不是单调的；实际上，它是递减的。\n\n#### 2. 梯度计算\n\n我们需要计算在输入 $x = x_1 = -2$、参数 $w=1$、$b=0$、$\\alpha = -1/3$ 和目标值 $t=1$ 时的梯度 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial \\alpha}$。\n\n首先，我们计算该输入的中间值：\n- 预激活值：$z = w x + b = (1)(-2) + 0 = -2$。\n- 输出：由于 $z = -2  0$，$y = \\alpha z = \\left(-\\frac{1}{3}\\right)(-2) = \\frac{2}{3}$。\n\n接下来，我们使用链式法则求梯度。所需的偏导数是：\n- $\\frac{\\partial L}{\\partial y} = 2(y - t)$\n- $\\frac{\\partial y}{\\partial z} = f'(z) = \\begin{cases} 1  \\text{若 } z  0 \\\\ \\alpha  \\text{若 } z  0 \\end{cases}$\n- $\\frac{\\partial z}{\\partial w} = x$\n- $\\frac{\\partial y}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha}(\\alpha z) = z$ (因为 $z  0$)\n\n现在我们计算在给定点处的这些偏导数值：\n- $\\frac{\\partial L}{\\partial y} = 2\\left(\\frac{2}{3} - 1\\right) = 2\\left(-\\frac{1}{3}\\right) = -\\frac{2}{3}$。\n- 由于 $z = -2  0$，$\\frac{\\partial y}{\\partial z} = \\alpha = -\\frac{1}{3}$。\n- $\\frac{\\partial z}{\\partial w} = x = -2$。\n- 由于 $z = -2  0$，$\\frac{\\partial y}{\\partial \\alpha} = z = -2$。\n\n现在我们使用链式法则组合这些来求最终的梯度。\n\n- **关于 $w$ 的梯度**：\n$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial w} $$\n$$ \\frac{\\partial L}{\\partial w} = \\left(-\\frac{2}{3}\\right) \\left(-\\frac{1}{3}\\right) (-2) = \\left(\\frac{2}{9}\\right) (-2) = -\\frac{4}{9} $$\n\n- **关于 $\\alpha$ 的梯度**：\n$$ \\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial \\alpha} $$\n$$ \\frac{\\partial L}{\\partial \\alpha} = \\left(-\\frac{2}{3}\\right) (-2) = \\frac{4}{3} $$\n\n所求的梯度为 $\\frac{\\partial L}{\\partial w} = -\\frac{4}{9}$ 和 $\\frac{\\partial L}{\\partial \\alpha} = \\frac{4}{3}$。\n\n#### 3. 与训练不稳定性的关系\n\n梯度 $\\frac{\\partial L}{\\partial w}  0$ 和 $\\frac{\\partial L}{\\partial \\alpha} > 0$ 的符号揭示了当 $\\alpha  0$ 时训练中存在的潜在问题。\n\n1.  **梯度符号翻转**：损失关于预激活值的梯度是 $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} = \\frac{\\partial L}{\\partial y} \\alpha$。在我们的例子中，输出 $y = 2/3$ 小于目标值 $t=1$，因此输出误差梯度 $\\frac{\\partial L}{\\partial y} = -2/3$ 是负的。因为 $\\alpha = -1/3$ 也是负的，所以反向传播的梯度是 $\\frac{\\partial L}{\\partial z} = (-2/3)(-1/3) = 2/9$，为正值。这意味着对于负输入，误差信号的符号在反向通过激活函数时被翻转了。虽然这在数学上是正确的，但这种行为并不常规。一个标准的 Leaky ReLU (其中 $\\alpha > 0$) 会保留误差梯度的符号。预激活值的误差信号的这种符号翻转直接导致了我们权重梯度的符号：$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} x = (2/9)(-2) = -4/9$。一个正的 $\\alpha$ 会导致一个正的权重梯度。权重梯度的符号对 $\\alpha$ 符号的这种依赖性可能导致训练过程中出现复杂且可能不稳定的动态。\n\n2.  **非单调性引起的冲突更新**：$\\alpha  0$ 的核心问题是激活函数的非单调性，如任务1所示。考虑使用一批包含多个负输入的数据进行训练。因为函数在 $x  0$ 时是递减的，所以可能出现某些数据点需要增加 $\\alpha$ 来减少其损失，而其他数据点则需要减少 $\\alpha$。例如，如果我们有一个点 $(x_a, t_a)$，其输出 $y_a = \\alpha x_a$ 过高，而另一个点 $(x_b, t_b)$ 的输出 $y_b = \\alpha x_b$ 过低，那么梯度 $\\frac{\\partial L_a}{\\partial \\alpha}$ 和 $\\frac{\\partial L_b}{\\partial \\alpha}$ 可能会有相反的符号。这些针对共享参数 $\\alpha$ 的冲突更新信号可能导致训练过程振荡或停滞，这是一种训练不稳定性。对于我们的单个数据点，梯度 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial \\alpha}$ 的符号相反，正是这种由非单调激活函数所产生的不寻常、耦合的优化景观的一个症状。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{4}{9}  \\frac{4}{3}\n\\end{pmatrix}\n}\n$$", "id": "3142552"}, {"introduction": "基于上一练习中$\\alpha$应为非负的结论，本次实践聚焦于如何“实现”这一约束。我们将通过一个编码练习[@problem_id:3142517]，实现并对比无约束的$\\alpha$优化与使用“重参数化”技巧的约束优化。这个练习为前一个问题中发现的理论隐患提供了具体的解决方案，并强调了在实践中确保激活函数具有良好性质的重要性。", "problem": "考虑深度学习中使用的分段线性激活函数族：leaky rectified linear unit (leaky ReLU) 和 parametric rectified linear unit (PReLU)。应用于标量预激活值 $z$ 的单单元 PReLU 由函数 $f_{\\alpha}(z)$ 定义，其中 $f_{\\alpha}(z) = \\max(0,z) + \\alpha \\min(0,z)$，标量参数 $\\alpha \\in \\mathbb{R}$。当 $\\alpha$ 是一个固定常数时，该函数即为 leaky ReLU；而 PReLU 则是从数据中学习 $\\alpha$。\n\n根据分段线性函数的核心定义和导数的定义，如果一个函数 $g(z)$ 的导数 $g'(z)$ 对于所有 $z$ 都大于等于 $0$，则该函数对其参数 $z$ 是单调不减的。对于 $f_{\\alpha}(z)$，每个单元（作为其输入 $z$ 的函数）的单调性要求对 $\\alpha$ 进行约束。\n\n你的任务是：\n- 提出并实现一种参数化方法，通过在整个训练过程中确保 $\\alpha \\ge 0$ 来强制执行每个单元的单调性（例如，通过适当的重参数化）。\n- 展示当 $\\alpha$ 不受约束并可能变为负值时的违规情况，并量化其经验影响。\n\n使用以下基本基础：\n- 均方误差 (MSE) 损失：对于预测值 $\\hat{y}_i$ 和目标值 $y_i$，平均损失为 $L = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$。\n- 梯度下降更新：对于参数 $\\theta$，更新规则为 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L$，其中 $\\eta > 0$ 是学习率。\n- 链式求导法则：如果 $\\alpha = h(\\beta)$，则 $\\nabla_{\\beta} L = \\nabla_{\\alpha} L \\cdot h'(\\beta)$。\n\n需要实现的实验设置：\n- 构建一个包含 $N = 2001$ 个在区间 $[-2,2]$ 上均匀分布的标量 $z_i$ 的合成数据集。\n- 将目标定义为标准 rectified linear unit 的输出 $y_i = \\max(0,z_i)$。\n- 使用一个单单元模型，其输出为 $\\hat{y}_i = f_{\\alpha}(z_i)$，其中唯一的可训练参数是 $\\alpha$，线性预激活值就是 $z$（没有额外的权重或偏置）。\n- 使用普通梯度下降法训练指定的周期数，从第一性原理计算精确的解析梯度。\n\n单调性评估和违规计数：\n- 一个单元是单调不减的当且仅当 $\\alpha \\ge 0$，因为当 $z > 0$ 时 $f'_{\\alpha}(z) = 1$ 且当 $z  0$ 时 $f'_{\\alpha}(z) = \\alpha$。报告一个布尔值以指示单调性，以及在数据集网格上严格违规的整数计数，定义为局部斜率为负的 $z_i  0$ 点的数量（当 $\\alpha \\ge 0$ 时此计数为 $0$，当 $\\alpha  0$ 时等于负 $z_i$ 的数量）。\n\n约束设计：\n- 通过使用非负映射（例如 softplus 函数）对 $\\alpha$ 进行重参数化来实施约束变体，使得对于自由参数 $\\beta \\in \\mathbb{R}$ 有 $\\alpha = \\log(1 + e^{\\beta})$。这确保了对于所有 $\\beta$ 都有 $\\alpha \\ge 0$。链式法则所需的导数是 $d\\alpha/d\\beta = \\frac{1}{1 + e^{-\\beta}}$。\n\n测试套件：\n运行以下四个测试用例，每个用例指定为一个元组 $(\\text{mode}, \\text{init}, \\eta, E)$，其中 $\\text{mode}$ 是 \"unconstrained\"（直接优化 $\\alpha$）或 \"softplus\"（优化 $\\beta$ 并令 $\\alpha=\\log(1+e^{\\beta})$），$\\text{init}$ 是初始值（对于 \"unconstrained\" 是 $\\alpha_0$，对于 \"softplus\" 是 $\\beta_0$），$\\eta$ 是学习率，而 $E$ 是周期数：\n1. 负初始化的无约束情况：$(\\text{\"unconstrained\"}, \\alpha_0=-0.5, \\eta=0.05, E=10)$。\n2. 正初始化的无约束情况：$(\\text{\"unconstrained\"}, \\alpha_0=0.5, \\eta=0.05, E=10)$。\n3. 采用 softplus 约束且中度负初始化的情況：$(\\text{\"softplus\"}, \\beta_0=-1.0, \\eta=0.10, E=10)$。\n4. 采用 softplus 约束且靠近边界的初始化情況：$(\\text{\"softplus\"}, \\beta_0=-10.0, \\eta=0.10, E=10)$。\n\n对于每个测试用例，在训练后计算并返回：\n- $\\alpha$ 的最终值，为浮点数。\n- 单元的单调性布尔值，定义为 $(\\alpha \\ge 0)$。\n- 整个数据集上的最终均方误差 $L$，为浮点数。\n- 网格上的整数违规计数，如上定义。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试用例，并且本身是一个包含 $[\\alpha_{\\text{final}}, \\text{is\\_monotone}, L_{\\text{final}}, \\text{violation\\_count}]$ 的列表。例如，一个有效的输出格式是 $[[0.0,\\text{True},0.123,0],[\\dots]]$。", "solution": "该问题要求分析 PReLU 激活函数 $f_{\\alpha}(z) = \\max(0,z) + \\alpha \\min(0,z)$，特别是关于其单调性的强制执行。如果一个函数的导数处处非负，则该函数是单调不减的。函数 $f_{\\alpha}(z)$ 关于其输入 $z$ 的导数是分段常数：\n$$\nf'_{\\alpha}(z) = \\frac{d}{dz} f_{\\alpha}(z) =\n\\begin{cases}\n1  \\text{if } z > 0 \\\\\n\\alpha  \\text{if } z  0\n\\end{cases}\n$$\n为了使该函数单调不减，其导数必须在所有有定义的 $z$ 处都大于或等于 $0$。这就施加了约束 $\\alpha \\ge 0$。任务是实现并比较两种优化策略，用于一个旨在学习标准 ReLU 函数的单 PReLU 单元：一种是无约束的 $\\alpha$ 优化，另一种是保证 $\\alpha \\ge 0$ 的有约束优化。\n\n实验设置涉及一个包含 $N=2001$ 个在 $[-2, 2]$ 区间内均匀分布的点 $z_i$ 的合成数据集。目标输出是 $y_i = \\max(0, z_i)$，模型的预测是 $\\hat{y}_i = f_{\\alpha}(z_i)$。目标是找到最小化均方误差 (MSE) 损失函数的参数 $\\alpha$：\n$$\nL(\\alpha) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n$$\n代入 $y_i$ 和 $\\hat{y}_i$ 的表达式：\n$$\ny_i - \\hat{y}_i = \\max(0, z_i) - (\\max(0, z_i) + \\alpha \\min(0, z_i)) = -\\alpha \\min(0, z_i)\n$$\n损失函数简化为：\n$$\nL(\\alpha) = \\frac{1}{N} \\sum_{i=1}^{N} (-\\alpha \\min(0, z_i))^2 = \\frac{\\alpha^2}{N} \\sum_{i=1}^{N} (\\min(0, z_i))^2\n$$\n项 $(\\min(0, z_i))^2$ 仅在 $z_i  0$ 时非零，此时它等于 $z_i^2$。令 $z_i  0$ 的索引集合为 $\\mathcal{I}^- = \\{i \\mid z_i  0\\}$。损失可以写成：\n$$\nL(\\alpha) = \\frac{\\alpha^2}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2\n$$\n这个简化形式表明，损失是关于 $\\alpha$ 的一个简单二次函数，在 $\\alpha=0$ 处有全局最小值。这是符合预期的，因为 $\\alpha=0$ 可以恢复目标函数 $y=\\max(0,z)$。\n\n我们现在将推导两种优化模式的梯度。\n\n**无约束优化**\n在这种模式下，我们使用梯度下降直接优化 $\\alpha \\in \\mathbb{R}$。损失函数关于 $\\alpha$ 的梯度是：\n$$\n\\nabla_{\\alpha} L = \\frac{\\partial L}{\\partial \\alpha} = \\frac{d}{d\\alpha} \\left( \\frac{\\alpha^2}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right) = \\frac{2\\alpha}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2\n$$\n每个周期的 $\\alpha$ 的梯度下降更新规则是：\n$$\n\\alpha \\leftarrow \\alpha - \\eta \\, \\nabla_{\\alpha} L = \\alpha - \\eta \\left( \\frac{2\\alpha}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right)\n$$\n其中 $\\eta$ 是学习率。如果初始值 $\\alpha_0$ 为负，$\\alpha$ 将从负侧趋近于 $0$，从而在训练期间违反单调性条件 $\\alpha \\ge 0$。\n\n**通过 Softplus 重参数化进行约束优化**\n为强制执行约束 $\\alpha \\ge 0$，我们使用一个值域为非负的函数来重参数化 $\\alpha$。我们选择 softplus 函数，用一个新的无约束参数 $\\beta \\in \\mathbb{R}$ 来定义 $\\alpha$：\n$$\n\\alpha(\\beta) = \\log(1 + e^{\\beta})\n$$\n这确保了对于任何实值 $\\beta$，$\\alpha(\\beta) \\ge 0$。我们现在对 $\\beta$ 执行梯度下降。梯度 $\\nabla_{\\beta} L$ 可通过链式法则求得：\n$$\n\\nabla_{\\beta} L = \\frac{\\partial L}{\\partial \\beta} = \\frac{\\partial L}{\\partial \\alpha} \\frac{d\\alpha}{d\\beta}\n$$\n我们已经计算了 $\\frac{\\partial L}{\\partial \\alpha}$。$\\alpha$ 关于 $\\beta$ 的导数是：\n$$\n\\frac{d\\alpha}{d\\beta} = \\frac{d}{d\\beta} \\log(1 + e^{\\beta}) = \\frac{e^{\\beta}}{1 + e^{\\beta}} = \\frac{1}{1 + e^{-\\beta}}\n$$\n这就是逻辑 S 型函数（logistic sigmoid function），通常表示为 $\\sigma(\\beta)$。\n将这些部分组合起来，关于 $\\beta$ 的梯度是：\n$$\n\\nabla_{\\beta} L = \\left( \\frac{2\\alpha(\\beta)}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right) \\cdot \\sigma(\\beta)\n$$\n$\\beta$ 的梯度下降更新规则是：\n$$\n\\beta \\leftarrow \\beta - \\eta \\, \\nabla_{\\beta} L\n$$\n在这个方案中，无论 $\\beta$ 的值是多少，得到的参数 $\\alpha$ 将始终为非负，从而在整个训练过程中保持激活函数的单调性。优化过程将驱使 $\\beta$ 趋向于 $-\\infty$，这又会使 $\\alpha$ 趋向其最优值 $0$。\n\n**评估指标**\n对于每个测试用例，在指定的训练周期数之后，我们计算四个值：\n1.  **最终 $\\alpha$**：PReLU 参数的值。对于 \"softplus\" 模式，这是从最终的 $\\beta$ 计算得出的。\n2.  **单调性 `is_monotone`**：一个布尔值，如果 $\\alpha_{\\text{final}} \\ge 0$ 则为 `True`，否则为 `False`。\n3.  **最终 MSE 损失 $L_{\\text{final}}$**：用最终的 $\\alpha$ 计算出的损失。\n4.  **违规计数**：数据集中局部斜率 $f'_{\\alpha}(z_i)$ 为负的 $z_i  0$ 点的数量。如果 $\\alpha  0$，此计数等于负 $z_i$ 点的数量；否则为 $0$。对于给定的数据集，有 1000 个点的 $z_i  0$。\n\n通过运行指定的测试用例，我们可以凭经验观察到无约束方法的结果（可能导致非单调性）以及通过 softplus 重参数化保证单调性的强制执行。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates unconstrained and constrained optimization\n    for a single PReLU unit learning a ReLU function.\n    \"\"\"\n    # Define dataset parameters\n    N = 2001\n    z_min, z_max = -2.0, 2.0\n    z = np.linspace(z_min, z_max, N, dtype=np.float64)\n\n    # Define target output (standard ReLU)\n    y = np.maximum(0, z)\n\n    # Pre-calculate components that are constant throughout training\n    z_neg = z[z  0]\n    sum_z_neg_sq = np.sum(z_neg**2)\n    num_z_neg = len(z_neg)\n\n    # Test suite: (mode, initial_value, learning_rate, epochs)\n    test_cases = [\n        (\"unconstrained\", -0.5, 0.05, 10),\n        (\"unconstrained\", 0.5, 0.05, 10),\n        (\"softplus\", -1.0, 0.10, 10),\n        (\"softplus\", -10.0, 0.10, 10),\n    ]\n\n    results = []\n\n    for mode, init_val, eta, epochs in test_cases:\n        if mode == \"unconstrained\":\n            # Directly optimize alpha\n            alpha = float(init_val)\n            \n            for _ in range(epochs):\n                # Gradient of Loss w.r.t. alpha\n                # L(alpha) = (alpha^2 / N) * sum(z_i^2 for z_i  0)\n                # dL/d_alpha = (2 * alpha / N) * sum(z_i^2 for z_i  0)\n                grad_alpha = (2.0 * alpha / N) * sum_z_neg_sq\n                \n                # Gradient descent update\n                alpha -= eta * grad_alpha\n                \n            final_alpha = alpha\n\n        elif mode == \"softplus\":\n            # Optimize beta, where alpha = log(1 + exp(beta))\n            beta = float(init_val)\n\n            for _ in range(epochs):\n                # Calculate alpha from beta using a numerically stable method\n                # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x))\n                alpha = np.logaddexp(0, beta)\n                \n                # Derivative of alpha w.r.t. beta (sigmoid function)\n                # d_alpha/d_beta = exp(beta)/(1+exp(beta)) = 1/(1+exp(-beta))\n                # This is numerically stable.\n                d_alpha_d_beta = 1.0 / (1.0 + np.exp(-beta))\n\n                # Gradient of Loss w.r.t. alpha\n                grad_alpha = (2.0 * alpha / N) * sum_z_neg_sq\n                \n                # Gradient of Loss w.r.t. beta (using chain rule)\n                grad_beta = grad_alpha * d_alpha_d_beta\n                \n                # Gradient descent update\n                beta -= eta * grad_beta\n            \n            final_alpha = np.logaddexp(0, beta)\n\n        # Calculate final metrics after training\n        is_monotone = (final_alpha >= 0.0)\n        \n        # Final loss: L = (alpha^2 / N) * sum_z_neg_sq\n        final_loss = (final_alpha**2 / N) * sum_z_neg_sq\n        \n        # Violation count: number of negative slopes for z  0\n        violation_count = num_z_neg if final_alpha  0.0 else 0\n        \n        results.append([\n            float(final_alpha),\n            bool(is_monotone),\n            float(final_loss),\n            int(violation_count)\n        ])\n\n    # The final print statement must follow the specified format:\n    # A string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3142517"}]}