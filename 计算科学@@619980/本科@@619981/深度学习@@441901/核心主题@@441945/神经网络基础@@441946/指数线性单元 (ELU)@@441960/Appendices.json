{"hands_on_practices": [{"introduction": "在使用任何激活函数之前，首要任务是深入理解其基本特性。本练习将引导你像研究员一样，通过设计思想实验来探究指数线性单元（ELU）的关键行为。我们将重点关注它在正输入下的线性响应，以及在负输入下有助于推动激活均值趋近于零的饱和特性，同时分析其在“拐点”处的连续性。[@problem_id:3185350]", "problem": "使用单神经元层来研究指数线性单元（ELU）激活函数的前向传播。该神经元接收一个标量输入 $x$，通过标准前向规则 $z = w x + b$ 计算预激活值 $z$，并输出 $y = \\phi(z)$，其中 $\\phi$ 是参数为 $\\alpha  0$ 的 ELU 函数，定义如下\n$$\n\\phi(z) = \\begin{cases}\nz,  z \\ge 0 \\\\\n\\alpha\\left(\\exp(z) - 1\\right),  z  0\n\\end{cases}\n$$\n假设 $w = 1$ 且 $b = 0$，因此对于所有输入都有 $z = x$。当 $\\alpha$ 很大时（例如 $\\alpha = 100$），您想要设计一小组测试输入，以经验性地探究两种不同的前向行为：(i) 负区饱和，以及 (ii) 前向映射在 $x = 0$ 处的连续性。仅使用上述前向传播的定义和连续性的定义（极限等于在该点的函数值），选择一个既正确提出合适的输入集，又正确陈述关于在 $\\alpha$ 很大时 $x = 0$ 处连续性的分析事实的选项。\n\nA. 使用输入 $\\{ -10^{1}, -10^{-6}, 0, 10^{-6}, 10^{1} \\}$ 来探究极端的负值区域和 $0$ 点两侧的紧邻域；对于任何 $\\alpha  0$（包括大的 $\\alpha$），ELU 前向映射在 $x = 0$ 处都是连续的。\n\nB. 使用输入 $\\{ 10^{3}, 10^{4} \\}$，因为非常大的正输入足以观察到负饱和；当 $\\alpha$ 很大时，ELU 前向映射在 $x = 0$ 处不连续。\n\nC. 使用输入 $\\{ -10^{2}, -10^{1}, -1 \\}$ 来揭示负饱和，并使用 $\\{ -10^{-6}, 10^{-6} \\}$ 来检查 $0$ 附近的行为（无需包含 $0$ 本身）；只要 $\\alpha \\ne 1$，ELU 前向映射在 $x = 0$ 处就存在可去间断点。\n\nD. 使用输入 $\\{ -10^{6} \\}$ 来观察到当 $x \\to -\\infty$ 时 ELU 饱和到 $-\\infty$；ELU 前向映射仅在 $\\alpha$ 足够小时才在 $x = 0$ 处连续。", "solution": "用户希望对一个关于使用指数线性单元 (ELU) 激活函数的单个神经元的前向传播行为的问题进行批判性分析。\n\n首先，我将验证问题陈述。\n\n### 步骤 1：提取已知条件\n- 具有标量输入 $x$ 的单神经元层。\n- 预激活函数：$z = w x + b$。\n- 激活函数（输出）：$y = \\phi(z)$，其中 $\\phi$ 是 ELU 函数。\n- ELU 函数定义：\n$$\n\\phi(z) = \\begin{cases}\nz,  z \\ge 0 \\\\\n\\alpha\\left(\\exp(z) - 1\\right),  z  0\n\\end{cases}\n$$\n- 参数约束：$\\alpha  0$。\n- 特定神经元参数：权重 $w = 1$ 和偏置 $b = 0$。\n- 参数的含义：对于所有输入，$z = x$。输出为 $y = \\phi(x)$。\n- 需要考虑的特定条件：$\\alpha$ 很大（例如 $\\alpha = 100$）。\n- 任务是选择一个选项，该选项正确地提出了一个测试输入集，以经验性地探究两种行为：(i) 负区饱和，以及 (ii) 前向映射在 $x=0$ 处的连续性。该选项还必须正确陈述关于在 $x=0$ 处连续性的分析事实。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学或事实的合理性**：该问题在科学上是合理的。ELU 函数是深度学习和机器学习中的标准激活函数。其定义被正确提供。前向传播、饱和和数学连续性的概念是基础性的，并且被正确地引用。\n2.  **适定性**：该问题是适定的。函数 $\\phi(x)$ 被明确定义，并且要研究的性质（饱和和连续性）是标准的数学概念。问题要求一个合适的测试点集和一个正确的分析陈述，这使得选项中存在唯一的正确答案。\n3.  **客观性**：问题以客观的数学语言陈述。没有主观或模棱两可的术语。\n4.  **完整性和一致性**：问题陈述是自洽的。所有必要的定义和参数（$w$、$b$、$\\phi(z)$、$\\alpha0$）都已提供。简化 $z = x$ 被明确说明。没有矛盾之处。\n\n### 步骤 3：结论与行动\n问题陈述是**有效的**。这是一个定义明确的练习，旨在分析标准激活函数的性质。我现在将继续解决问题并评估各个选项。\n\n### 解题推导\n\n给定 $w=1$ 和 $b=0$，预激活值为 $z=x$，输出为 $y = \\phi(x)$。函数为：\n$$\ny = \\phi(x) = \\begin{cases}\nx,  x \\ge 0 \\\\\n\\alpha\\left(\\exp(x) - 1\\right),  x  0\n\\end{cases}\n$$\n我们需要分析两个性质：负区饱和和在 $x=0$ 处的连续性。\n\n**1. 负区饱和**\n负区的饱和指的是当其输入趋近于 $-\\infty$ 时函数的行为。我们需要计算当 $x \\to -\\infty$ 时 $\\phi(x)$ 的极限。\n对于 $x  0$，函数为 $\\phi(x) = \\alpha(\\exp(x) - 1)$。\n极限是：\n$$\n\\lim_{x \\to -\\infty} \\phi(x) = \\lim_{x \\to -\\infty} \\alpha\\left(\\exp(x) - 1\\right)\n$$\n因为 $\\lim_{x \\to -\\infty} \\exp(x) = 0$，我们有：\n$$\n\\lim_{x \\to -\\infty} \\phi(x) = \\alpha(0 - 1) = -\\alpha\n$$\n当 $x$ 在负方向上变得非常大时，函数 $\\phi(x)$ 饱和到常数值 $-\\alpha$。为了经验性地探究这一点，我们需要选择大的负值输入 $x$，例如 $x = -10$ 或 $x = -100$。对于这样的输入，$\\exp(x)$ 将非常接近于 $0$，而 $\\phi(x)$ 将非常接近于 $-\\alpha$。\n\n**2. 在 $x=0$ 处的连续性**\n一个函数在点 $c$ 处连续，必须满足三个条件：\n(i) $f(c)$ 有定义。\n(ii) $\\lim_{x \\to c} f(x)$ 存在。\n(iii) $\\lim_{x \\to c} f(x) = f(c)$。\n\n让我们检查 $\\phi(x)$ 在 $x=0$ 处是否满足这些条件。\n\n(i) 函数在 $x=0$ 处的值由分段函数的第一种情况定义：\n$$\n\\phi(0) = 0\n$$\n\n(ii) 要检查极限是否存在，我们必须评估左极限和右极限，看它们是否相等。\n右极限（当 $x$ 从正方向趋近于 $0$ 时，$x > 0$）：\n$$\n\\lim_{x \\to 0^+} \\phi(x) = \\lim_{x \\to 0^+} x = 0\n$$\n左极限（当 $x$ 从负方向趋近于 $0$ 时，$x  0$）：\n$$\n\\lim_{x \\to 0^-} \\phi(x) = \\lim_{x \\to 0^-} \\alpha\\left(\\exp(x) - 1\\right) = \\alpha\\left(\\exp(0) - 1\\right) = \\alpha(1 - 1) = 0\n$$\n因为左极限（$0$）等于右极限（$0$），所以极限存在且等于 $0$。\n$$\n\\lim_{x \\to 0} \\phi(x) = 0\n$$\n\n(iii) 我们将极限与函数值进行比较：\n$$\n\\lim_{x \\to 0} \\phi(x) = 0 \\quad \\text{and} \\quad \\phi(0) = 0\n$$\n因为 $\\lim_{x \\to 0} \\phi(x) = \\phi(0)$，所以函数 $\\phi(x)$ 在 $x=0$ 处是连续的。\n关键是，这个结果对**任何** $\\alpha  0$ 的值都成立。$\\alpha$ 的大小不影响函数在 $x=0$ 处的连续性，尽管它确实影响函数在该点的导数。\n\n为了经验性地探究在 $x=0$ 处的连续性，一个合适的输入集应包括从两侧非常接近 $0$ 的点，例如 $-10^{-6}$ 和 $10^{-6}$，以及点 $0$ 本身。\n\n**合适的测试集总结：**\n- 测试饱和：至少一个大的负数（例如 $-10^1$）。\n- 测试在 $0$ 处的连续性：围绕 $0$ 的一小组密集的点（例如 $-10^{-6}, 0, 10^{-6}$）。\n- 测试正线性区域：一个正数（例如 $10^1$）。\n一个全面的集合将是 $\\{ -10^1, -10^{-6}, 0, 10^{-6}, 10^1 \\}$。\n\n### 逐项分析\n\n**A. 使用输入 $\\{ -10^{1}, -10^{-6}, 0, 10^{-6}, 10^{1} \\}$ 来探究极端的负值区域和 $0$ 点两侧的紧邻域；对于任何 $\\alpha  0$（包括大的 $\\alpha$），ELU 前向映射在 $x = 0$ 处都是连续的。**\n- **输入集：** 提议的输入集非常好。输入 $-10^1$ 探究了负饱和区域。输入 $-10^{-6}$、$0$ 和 $10^{-6}$ 构成了 $0$ 周围的一个紧邻域，以经验性地检查连续性。输入 $10^1$ 测试了正输入的线性行为。这个集合为既定目标设计得很好。\n- **连续性陈述：** “对于任何 $\\alpha  0$（包括大的 $\\alpha$），ELU 前向映射在 $x = 0$ 处都是连续的”这一陈述在分析上是正确的，如上所述。\n- **结论：** 正确。\n\n**B. 使用输入 $\\{ 10^{3}, 10^{4} \\}$，因为非常大的正输入足以观察到负饱和；当 $\\alpha$ 很大时，ELU 前向映射在 $x = 0$ 处不连续。**\n- **输入集：** 提议的输入是大的正数。这些输入位于 $\\phi(x)=x$ 的区域。它们完全没有提供关于负区域（$x \\le 0$）或饱和行为的任何信息。其理由是荒谬的。\n- **连续性陈述：** 函数“当 $\\alpha$ 很大时在 $x = 0$ 处不连续”的说法是错误的。我们证明了对于所有 $\\alpha  0$ 函数都是连续的。\n- **结论：** 不正确。\n\n**C. 使用输入 $\\{ -10^{2}, -10^{1}, -1 \\}$ 来揭示负饱和，并使用 $\\{ -10^{-6}, 10^{-6} \\}$ 来检查 $0$ 附近的行为（无需包含 $0$ 本身）；只要 $\\alpha \\ne 1$，ELU 前向映射在 $x = 0$ 处就存在可去间断点。**\n- **输入集：** 建议的输入对于探究指定的行为是合理的。\n- **连续性陈述：** 函数“只要 $\\alpha \\ne 1$ 就在 $x = 0$ 处有可去间断点”的说法是错误的。如果极限存在但不等于函数值，则会发生可去间断点。在这里，极限存在且等于函数值（$0$），这意味着函数是连续的。条件 $\\alpha \\ne 1$ 与*导数* $\\phi'(x)$ 在 $x=0$ 处的连续性有关，但问题是关于函数 $\\phi(x)$ 本身的连续性。\n- **结论：** 不正确。\n\n**D. 使用输入 $\\{ -10^{6} \\}$ 来观察到当 $x \\to -\\infty$ 时 ELU 饱和到 $-\\infty$；ELU 前向映射仅在 $\\alpha$ 足够小时才在 $x = 0$ 处连续。**\n- **输入集：** 单个输入点不足以同时测试饱和和在 $0$ 处的连续性。\n- **饱和陈述：** “ELU 饱和到 $-\\infty$”的说法是错误的。根据推导，函数饱和到有限值 $-\\alpha$。\n- **连续性陈述：** “仅当 $\\alpha$ 足够小时，在 $x=0$ 处才连续”的说法是错误的。我们证明了对于所有 $\\alpha  0$ 都是连续的。\n- **结论：** 不正确。\n\n根据详细分析，选项 A 是唯一一个既提供了合适的实验设计（输入集）又给出了正确分析陈述的选项。", "answer": "$$\\boxed{A}$$", "id": "3185350"}, {"introduction": "理解了前向传播的行为后，我们自然要转向反向传播——这是训练神经网络的核心。本练习将聚焦于如何应用多元链式法则，在一个包含ELU激活函数的网络中计算梯度。你将特别处理其在原点处的不可导“拐点”，这是在实践中处理分段激活函数时常会遇到的挑战。[@problem_id:3190277]", "problem": "考虑一个标量输入 $x \\in \\mathbb{R}$，它被输入到一个指数线性单元（ELU）激活函数 $g(x)$ 中，该函数带有一个参数 $\\alpha \\in (0, \\infty)$，定义如下\n$$\ng(x) = \n\\begin{cases}\nx,  x \\ge 0 \\\\\n\\alpha\\left(\\exp(x) - 1\\right),  x  0\n\\end{cases}\n$$\n该激活函数分流到一个双分支网络中：\n- 分支 1 计算 $y_{1} = \\sigma\\!\\left(w_{1}\\,g(x) + b_{1}\\right)$，其中 $\\sigma(z)$ 是 logistic sigmoid 函数，$\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$，并且 $w_{1}, b_{1} \\in \\mathbb{R}$ 是固定参数。\n- 分支 2 计算 $y_{2} = \\left(g(x)\\right)^{2}$。\n\n标量损失是平方误差和的一半\n$$\nL = \\frac{1}{2}\\left(y_{1} - t_{1}\\right)^{2} + \\frac{1}{2}\\left(y_{2} - t_{2}\\right)^{2},\n$$\n其中 $t_{1}, t_{2} \\in \\mathbb{R}$ 是固定目标值。\n\n仅使用导数的基本定义和多元链式法则，通过在两个分支上传播导数，推导出当 $x  0$ 和 $x  0$ 时 $\\frac{\\partial L}{\\partial x}$ 的反向传播表达式。然后，使用次梯度在拐点 $x = 0$ 处计算 $\\frac{\\partial L}{\\partial x}$。对于不可微点 $x = 0$，采用反向传播选择 ELU 右导数的约定（即，取 $\\frac{d g}{d x}\\big|_{x=0}$ 等于 $x  0$ 时的导数）。请用 $w_{1}$、$b_{1}$、$t_{1}$ 和标准函数表示 $x = 0$ 处 $\\frac{\\partial L}{\\partial x}$ 的最终表达式。不需要进行数值舍入。", "solution": "该问题是有效的。这是一个适定的微积分问题，及其在神经网络中的应用，这是深度学习中的一个标准课题。该问题具有科学依据，自成体系且客观。所提供的处理 ELU 激活函数不可微点的约定使得问题清晰明确。\n\n目标是求标量损失 $L$ 对标量输入 $x$ 的导数，记为 $\\frac{\\partial L}{\\partial x}$。损失 $L$ 通过一系列涉及激活函数 $g(x)$ 的中间计算依赖于 $x$。总的依赖关系图如下：\n$x \\rightarrow g(x)$\n$g(x) \\rightarrow y_{1} \\rightarrow L$\n$g(x) \\rightarrow y_{2} \\rightarrow L$\n\n全导数 $\\frac{\\partial L}{\\partial x}$ 可以使用多元链式法则求得。由于输入 $x$ 通过单个中间变量 $g(x)$ 影响损失 $L$，然后该变量再分支出去，因此链式法则可以构建如下：\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial g(x)} \\frac{d g(x)}{d x}\n$$\n第一项 $\\frac{\\partial L}{\\partial g(x)}$ 解释了源于 $g(x)$ 的两个分支。它是沿每条路径的偏导数之和：\n$$\n\\frac{\\partial L}{\\partial g(x)} = \\frac{\\partial L}{\\partial y_{1}} \\frac{\\partial y_{1}}{\\partial g(x)} + \\frac{\\partial L}{\\partial y_{2}} \\frac{\\partial y_{2}}{\\partial g(x)}\n$$\n我们将计算该表达式的每个组成部分。\n\n首先，我们计算损失 $L$ 对其直接输入 $y_{1}$ 和 $y_{2}$ 的导数。\n损失为 $L = \\frac{1}{2}\\left(y_{1} - t_{1}\\right)^{2} + \\frac{1}{2}\\left(y_{2} - t_{2}\\right)^{2}$。\n偏导数是：\n$$\n\\frac{\\partial L}{\\partial y_{1}} = y_{1} - t_{1}\n$$\n$$\n\\frac{\\partial L}{\\partial y_{2}} = y_{2} - t_{2}\n$$\n\n接下来，我们计算 $y_{1}$ 和 $y_{2}$ 关于 $g(x)$ 的导数。\n对于分支 1，$y_{1} = \\sigma\\!\\left(w_{1}\\,g(x) + b_{1}\\right)$。令 $z_{1} = w_{1}\\,g(x) + b_{1}$。logistic sigmoid 函数 $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ 的导数是 $\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))$。\n对 $y_{1}$ 使用链式法则：\n$$\n\\frac{\\partial y_{1}}{\\partial g(x)} = \\frac{d\\sigma(z_{1})}{dz_{1}} \\frac{\\partial z_{1}}{\\partial g(x)} = \\sigma(z_{1})(1 - \\sigma(z_{1})) \\cdot w_{1} = y_{1}(1 - y_{1})w_{1}\n$$\n对于分支 2，$y_{2} = \\left(g(x)\\right)^{2}$。其导数是：\n$$\n\\frac{\\partial y_{2}}{\\partial g(x)} = 2 g(x)\n$$\n\n现在我们将这些部分组合起来求 $\\frac{\\partial L}{\\partial g(x)}$：\n$$\n\\frac{\\partial L}{\\partial g(x)} = (y_{1} - t_{1}) \\cdot [y_{1}(1 - y_{1})w_{1}] + (y_{2} - t_{2}) \\cdot [2 g(x)]\n$$\n$$\n\\frac{\\partial L}{\\partial g(x)} = w_{1}(y_{1} - t_{1})y_{1}(1 - y_{1}) + 2(y_{2} - t_{2})g(x)\n$$\n\n最后一步是确定 ELU 激活函数的导数 $\\frac{d g(x)}{d x}$。这取决于 $x$ 的值。\nELU 函数定义为：\n$$\ng(x) = \n\\begin{cases}\nx,  x \\ge 0 \\\\\n\\alpha\\left(\\exp(x) - 1\\right),  x  0\n\\end{cases}\n$$\n- 当 $x  0$ 时：$g(x) = x$，因此 $\\frac{d g}{d x} = 1$。\n- 当 $x  0$ 时：$g(x) = \\alpha(\\exp(x) - 1)$，因此 $\\frac{d g}{d x} = \\alpha \\exp(x)$。\n- 当 $x = 0$ 时：函数在该点（一个“拐点”）不可微，因为左导数是 $\\alpha \\exp(0) = \\alpha$，右导数是 $1$。问题指定了一个约定：“采用反向传播选择右导数的约定”。因此，我们必须使用：\n$$\n\\frac{d g}{d x}\\bigg|_{x=0} = 1\n$$\n\n题目要求我们求出在 $x = 0$ 处 $\\frac{\\partial L}{\\partial x}$ 的最终表达式。我们在 $x=0$ 处计算所有中间量：\n首先，在 $x=0$ 处，激活函数的值是：\n$$\ng(0) = 0\n$$\n接下来，我们在 $x=0$ 处计算 $y_{1}$ 和 $y_{2}$：\n$$\ny_{1}\\big|_{x=0} = \\sigma(w_{1} g(0) + b_{1}) = \\sigma(w_{1}(0) + b_{1}) = \\sigma(b_{1})\n$$\n$$\ny_{2}\\big|_{x=0} = (g(0))^{2} = 0^{2} = 0\n$$\n现在我们可以在 $x=0$ 处计算梯度项 $\\frac{\\partial L}{\\partial g(x)}$：\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(y_{1}\\big|_{x=0} - t_{1})y_{1}\\big|_{x=0}(1 - y_{1}\\big|_{x=0}) + 2(y_{2}\\big|_{x=0} - t_{2})g(0)\n$$\n代入我们刚刚求得的值：\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1})) + 2(0 - t_{2})(0)\n$$\n第二项消失了：\n$$\n\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1}))\n$$\n最后，我们使用主链式法则公式和为 $\\frac{d g}{d x}$ 指定的约定来计算在 $x=0$ 处的 $\\frac{\\partial L}{\\partial x}$：\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = \\left(\\frac{\\partial L}{\\partial g(x)}\\bigg|_{x=0}\\right) \\left(\\frac{d g}{d x}\\bigg|_{x=0}\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = \\left( w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1})) \\right) \\cdot 1\n$$\n$$\n\\frac{\\partial L}{\\partial x}\\bigg|_{x=0} = w_{1}(\\sigma(b_{1}) - t_{1})\\sigma(b_{1})(1 - \\sigma(b_{1}))\n$$\n该表达式以所要求的参数 $w_{1}$、$b_{1}$、$t_{1}$ 以及标准 sigmoid 函数 $\\sigma$ 表示，符合题目要求。请注意，$t_{2}$ 和 $\\alpha$ 并未出现在 $x=0$ 处导数的最终表达式中。", "answer": "$$\n\\boxed{w_{1}\\left(\\sigma(b_{1}) - t_{1}\\right)\\sigma(b_{1})\\left(1 - \\sigma(b_{1})\\right)}\n$$", "id": "3190277"}, {"introduction": "这项综合性练习将理论与实践相结合，将ELU的超参数 $\\alpha$ 变为一个可学习的参数。这不仅要求你为其推导梯度，还需要你通过编程实现并验证。更重要的是，本练习将引导你探索参数可辨识性（identifiability）这一高级概念，这是在设计和调试复杂模型时需要考虑的一个关键实际问题。[@problem_id:3123807]", "problem": "您需要实现并分析一个使用指数线性单元 (ELU) 的单隐藏层，其中 ELU 参数是每层一个的可学习标量。分析必须基于 ELU 函数的基本定义和导数的链式法则。\n\n考虑一个单隐藏层，其预激活值为 $z \\in \\mathbb{R}^n$，经过一个指数线性单元 (ELU) 激活函数 $f(x;\\alpha)$，该函数带有一个层特定的可学习参数 $\\alpha \\in \\mathbb{R}$。ELU 函数分段定义为：当 $x \\ge 0$ 时，$f(x;\\alpha) = x$；当 $x  0$ 时，$f(x;\\alpha) = \\alpha (\\exp(x) - 1)$。该层的输出随后被传递到一个一维线性输出 $y_{\\text{hat}} = w^\\top f(z;\\alpha) + b$，其中权重为 $w \\in \\mathbb{R}^n$，偏置为 $b \\in \\mathbb{R}$。损失函数为均方误差 (MSE) $L = \\frac{1}{2}(y_{\\text{hat}} - y)^2$，目标值为 $y \\in \\mathbb{R}$。\n\n任务：\n- 使用导数的链式法则，从第一性原理推导梯度 $\\partial L / \\partial \\alpha$ 的闭式表达式，该表达式依赖于 $z$、$w$、$y_{\\text{hat}}$、$y$ 和 $\\alpha$。推导过程必须从 $f(x;\\alpha)$、$y_{\\text{hat}}$ 和 $L$ 的定义出发，并应用链式法则，不使用任何快捷公式。\n- 实现一个完整的程序，该程序：\n  1. 对于给定的输入，计算前向传播得到的 $y_{\\text{hat}}$、损失 $L$ 以及解析梯度 $\\partial L / \\partial \\alpha$。\n  2. 将解析梯度与通过对 $\\alpha$ 进行中心有限差分计算出的数值梯度进行验证。\n  3. 检验 $\\alpha$ 在层缩放下的可辨识性：讨论并测试当将 $\\alpha$ 缩放因子 $s$ 并同时将后续线性层的权重 $w$ 缩放 $1/s$ 时，何时会产生相同的输出，何时不会。可辨识性指的是不同的参数设置是否对给定的输入产生不可区分的模型输出。\n  4. 涵盖在 $x=0$ 处的边界行为以及全为正的预激活值的情况。\n\n使用以下特定的测试套件：\n- 用于梯度检查的混合符号情况：\n  - $z_{\\text{mixed}} = [-1.2, 0.8, -0.3]$，$\\alpha = 1.3$，$w = [0.9, -1.1, 0.5]$，$b = 0.2$，$y = 0.7$，有限差分步长 $\\varepsilon = 10^{-6}$。\n  - 报告解析梯度和数值梯度之间的相对误差，以浮点数形式表示，计算公式为 $\\frac{|\\text{analytical} - \\text{numerical}|}{\\max(10^{-12}, |\\text{analytical}| + |\\text{numerical}|)}$。\n- 零点边界情况：\n  - $z_{0} = [0.0]$，$\\alpha = 1.0$，$w = [1.0]$，$b = 0.0$，$y = 0.0$。\n  - 报告解析梯度 $\\partial L / \\partial \\alpha$，以浮点数形式表示。\n- 全为负的预激活值的可辨识性：\n  - $z_{\\text{neg}} = [-1.0, -0.5]$，$\\alpha = 0.7$，$w = [1.2, -0.8]$，$b = 0.3$，缩放因子 $s = 2.5$。\n  - 报告一个布尔值，指示 $w^\\top f(z_{\\text{neg}};\\alpha) + b$ 是否在 $10^{-12}$ 的容差范围内等于 $(w/s)^\\top f(z_{\\text{neg}}; s\\alpha) + b$。\n- 混合符号情况下的可辨识性失效：\n  - $z_{\\text{mix2}} = [-1.0, 0.5]$，使用与上述相同的 $\\alpha$、$w$、$b$ 和 $s$。\n  - 报告一个布尔值，指示 $w^\\top f(z_{\\text{mix2}};\\alpha) + b$ 是否在 $10^{-12}$ 的容差范围内等于 $(w/s)^\\top f(z_{\\text{mix2}}; s\\alpha) + b$。\n- 全为正的预激活值：\n  - $z_{\\text{pos}} = [0.4, 0.2]$，$\\alpha = 1.7$，$w = [0.6, -0.3]$，$b = -0.1$，$y = 0.5$。\n  - 报告解析梯度 $\\partial L / \\partial \\alpha$，以浮点数形式表示。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表（例如，$[result_1,result_2,result_3,result_4,result_5]$），结果的顺序与上述测试套件指定的精确顺序一致：混合情况的相对误差（浮点数）、零点边界情况的梯度（浮点数）、全为负情况的可辨识性布尔值、混合符号情况的可辨识性布尔值、全为正情况的梯度（浮点数）。不应打印任何其他文本。", "solution": "该问题是有效的。它在科学上基于神经网络理论的原理，在数学上是适定的，并且陈述是客观的。为获得唯一解所需的所有数据和定义均已提供。\n\n### 1. 梯度 $\\partial L / \\partial \\alpha$ 的推导\n\n主要任务是推导损失函数 $L$ 相对于指数线性单元 (ELU) 参数 $\\alpha$ 的梯度的闭式表达式。此推导从所提供的定义出发，并应用微积分的链式法则。\n\n相关量如下：\n- ELU 激活函数，针对标量输入 $x$ 和参数 $\\alpha$ 定义如下：\n$$\nf(x; \\alpha) = \\begin{cases}\nx  \\text{if } x \\ge 0 \\\\\n\\alpha (\\exp(x) - 1)  \\text{if } x  0\n\\end{cases}\n$$\n- 模型的输出预测值 $y_{\\text{hat}}$，针对预激活向量 $z \\in \\mathbb{R}^n$、权重向量 $w \\in \\mathbb{R}^n$ 和标量偏置 $b$ 定义如下：\n$$\ny_{\\text{hat}} = w^\\top f(z; \\alpha) + b = \\sum_{i=1}^n w_i f(z_i; \\alpha) + b\n$$\n其中 $f(z; \\alpha)$ 逐元素应用于向量 $z$。\n- 均方误差 (MSE) 损失函数，针对目标值 $y$ 定义如下：\n$$\nL = \\frac{1}{2}(y_{\\text{hat}} - y)^2\n$$\n\n我们应用链式法则来计算 $\\frac{\\partial L}{\\partial \\alpha}$：\n$$\n\\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial y_{\\text{hat}}} \\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha}\n$$\n\n首先，我们计算损失 $L$ 对预测值 $y_{\\text{hat}}$ 的导数：\n$$\n\\frac{\\partial L}{\\partial y_{\\text{hat}}} = \\frac{\\partial}{\\partial y_{\\text{hat}}} \\left( \\frac{1}{2}(y_{\\text{hat}} - y)^2 \\right) = y_{\\text{hat}} - y\n$$\n\n接下来，我们计算预测值 $y_{\\text{hat}}$ 对参数 $\\alpha$ 的导数。\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\sum_{i=1}^n w_i f(z_i; \\alpha) + b \\right)\n$$\n由于微分是线性算子，且 $b$ 不是 $\\alpha$ 的函数：\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\sum_{i=1}^n w_i \\frac{\\partial f(z_i; \\alpha)}{\\partial \\alpha}\n$$\n现在，我们必须求出 ELU 函数 $f(x; \\alpha)$ 对 $\\alpha$ 的导数。我们考虑其定义中的两种情况：\n- 如果 $x \\ge 0$，则 $f(x; \\alpha) = x$。对 $\\alpha$ 的导数为：\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha}(x) = 0\n$$\n- 如果 $x  0$，则 $f(x; \\alpha) = \\alpha (\\exp(x) - 1)$。对 $\\alpha$ 的导数为：\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} (\\alpha (\\exp(x) - 1)) = \\exp(x) - 1\n$$\n这可以使用指示函数 $I(c)$ 紧凑地表示，如果条件 $c$ 为真，则 $I(c)$ 为 1，否则为 0：\n$$\n\\frac{\\partial f(x; \\alpha)}{\\partial \\alpha} = (\\exp(x) - 1) \\cdot I(x  0)\n$$\n注意，该导数在 $x=0$ 处是良定义且连续的，因为 $\\lim_{x\\to0^-}(\\exp(x)-1) = 0$，这与 $x \\ge 0$ 时的导数相匹配。\n\n将此代入 $\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha}$ 的表达式中：\n$$\n\\frac{\\partial y_{\\text{hat}}}{\\partial \\alpha} = \\sum_{i=1}^n w_i (\\exp(z_i) - 1) \\cdot I(z_i  0)\n$$\n这个求和只包含预激活值 $z_i$ 为负的项。\n\n最后，我们将各部分组合起来，得到 $\\frac{\\partial L}{\\partial \\alpha}$ 的完整表达式：\n$$\n\\frac{\\partial L}{\\partial \\alpha} = (y_{\\text{hat}} - y) \\left( \\sum_{i=1}^n w_i (\\exp(z_i) - 1) \\cdot I(z_i  0) \\right)\n$$\n这就是梯度的闭式表达式，它只依赖于给定的变量 $z、w、y_{\\text{hat}}、y$ 和 $\\alpha$。\n\n### 2. 特殊情况与可辨识性分析\n\n- **全为正或零的预激活值 ($z_i \\ge 0$ 对所有 $i$ 成立)**：在这种情况下，对所有 $i$ 都有 $I(z_i  0) = 0$。梯度表达式中的求和项变为零。因此，$\\frac{\\partial L}{\\partial \\alpha} = 0$。这是合乎逻辑的，因为对于非负输入，ELU 函数表现为恒等函数，$f(z_i; \\alpha) = z_i$，这与 $\\alpha$ 无关。因此，模型的输出 $y_{\\text{hat}}$ 和损失 $L$ 不依赖于 $\\alpha$，其梯度必须为零。这在边界情况 ($z=[0.0]$) 和全为正的情况下进行了测试。\n\n- **缩放下的可辨识性**：我们分析将 $\\alpha$ 缩放因子 $s$ 并将 $w$ 缩放 $1/s$ 是否会保持模型输出不变。设新参数为 $\\alpha' = s\\alpha$ 和 $w' = w/s$。新的输出为 $y'_{\\text{hat}} = (w')^\\top f(z; \\alpha') + b$。\n\n- **情况 A：全为负的预激活值 ($z_i  0$ 对所有 $i$ 成立)**：\n对于任意分量 $i$，$f(z_i; \\alpha) = \\alpha(\\exp(z_i)-1)$。原始输出为：\n$$y_{\\text{hat}} = \\sum_i w_i \\alpha (\\exp(z_i)-1) + b$$\n新的输出为：\n$$y'_{\\text{hat}} = \\sum_i w'_i f(z_i; \\alpha') + b = \\sum_i \\frac{w_i}{s} \\left( (s\\alpha)(\\exp(z_i)-1) \\right) + b = \\sum_i w_i \\alpha (\\exp(z_i)-1) + b$$\n这里，$y'_{\\text{hat}} = y_{\\text{hat}}$。模型是不可辨识的，因为对于任何具有全为负分量的输入 $z$，不同的参数集 $(\\alpha, w)$ 和 $(s\\alpha, w/s)$ 会产生相同的输出。\n\n- **情况 B：混合符号的预激活值**：\n假设一些 $z_j \\ge 0$ 而另一些 $z_k  0$。原始输出为：\n$$y_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} w_i z_i + \\sum_{i \\mid z_i  0} w_i \\alpha(\\exp(z_i)-1) + b$$\n新的输出为：\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} w'_i z_i + \\sum_{i \\mid z_i  0} w'_i \\alpha'(\\exp(z_i)-1) + b$$\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} \\frac{w_i}{s} z_i + \\sum_{i \\mid z_i  0} \\frac{w_i}{s} (s\\alpha)(\\exp(z_i)-1) + b$$\n$$y'_{\\text{hat}} = \\sum_{i \\mid z_i \\ge 0} \\frac{w_i}{s} z_i + \\sum_{i \\mid z_i  0} w_i \\alpha(\\exp(z_i)-1) + b$$\n比较 $y_{\\text{hat}}$ 和 $y'_{\\text{hat}}$，对应于负 $z_i$ 的项是相同的，但对应于非负 $z_i$ 的项被缩放了 $1/s$。除非 $s=1$ 或对于非负部分所有 $w_i z_i=0$，否则 $y'_{\\text{hat}} \\neq y_{\\text{hat}}$。在这种情况下，参数是可辨识的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes results for all test cases as specified in the problem statement.\n    \"\"\"\n    \n    def elu(z, alpha):\n        \"\"\"Computes the element-wise ELU activation.\"\"\"\n        # The problem defines ELU as x for x>=0 and alpha*(exp(x)-1) for x0.\n        return np.where(z = 0, z, alpha * (np.exp(z) - 1))\n\n    def compute_forward_loss(z, alpha, w, b, y):\n        \"\"\"Computes the forward pass and loss.\"\"\"\n        activations = elu(z, alpha)\n        y_hat = np.dot(w, activations) + b\n        loss = 0.5 * (y_hat - y)**2\n        return y_hat, loss\n\n    def analytical_gradient(z, alpha, w, y_hat, y):\n        \"\"\"Computes the analytical gradient dL/d(alpha).\"\"\"\n        # Derivative of ELU with respect to alpha\n        # d/d_alpha (x) = 0 for x >= 0\n        # d/d_alpha (alpha*(exp(x)-1)) = exp(x)-1 for x  0\n        d_elu_d_alpha = np.where(z  0, np.exp(z) - 1, 0.0)\n        \n        # Derivative of y_hat with respect to alpha\n        d_yhat_d_alpha = np.dot(w, d_elu_d_alpha)\n        \n        # Derivative of Loss with respect to y_hat\n        d_L_d_yhat = y_hat - y\n        \n        # Final gradient using the chain rule\n        d_L_d_alpha = d_L_d_yhat * d_yhat_d_alpha\n        return d_L_d_alpha\n\n    def numerical_gradient(z, w, b, y, alpha, epsilon):\n        \"\"\"Computes the numerical gradient using central finite differences.\"\"\"\n        _, loss_plus = compute_forward_loss(z, alpha + epsilon, w, b, y)\n        _, loss_minus = compute_forward_loss(z, alpha - epsilon, w, b, y)\n        return (loss_plus - loss_minus) / (2 * epsilon)\n\n    results = []\n\n    # Test Case 1: Mixed-sign case for gradient check\n    z_mixed = np.array([-1.2, 0.8, -0.3])\n    alpha_mixed = 1.3\n    w_mixed = np.array([0.9, -1.1, 0.5])\n    b_mixed = 0.2\n    y_mixed = 0.7\n    epsilon = 1e-6\n    \n    y_hat_mixed, _ = compute_forward_loss(z_mixed, alpha_mixed, w_mixed, b_mixed, y_mixed)\n    grad_analyt = analytical_gradient(z_mixed, alpha_mixed, w_mixed, y_hat_mixed, y_mixed)\n    grad_numer = numerical_gradient(z_mixed, w_mixed, b_mixed, y_mixed, alpha_mixed, epsilon)\n    \n    rel_error_num = np.abs(grad_analyt - grad_numer)\n    rel_error_den = np.maximum(1e-12, np.abs(grad_analyt) + np.abs(grad_numer))\n    relative_error = rel_error_num / rel_error_den\n    results.append(relative_error)\n    \n    # Test Case 2: Boundary case at zero\n    z_0 = np.array([0.0])\n    alpha_0 = 1.0\n    w_0 = np.array([1.0])\n    b_0 = 0.0\n    y_0 = 0.0\n    \n    y_hat_0, _ = compute_forward_loss(z_0, alpha_0, w_0, b_0, y_0)\n    grad_at_zero = analytical_gradient(z_0, alpha_0, w_0, y_hat_0, y_0)\n    results.append(grad_at_zero)\n    \n    # Test Case 3: Identifiability with all-negative pre-activations\n    z_neg = np.array([-1.0, -0.5])\n    alpha_neg = 0.7\n    w_neg = np.array([1.2, -0.8])\n    b_neg = 0.3\n    s_neg = 2.5\n    \n    y_hat_orig_neg, _ = compute_forward_loss(z_neg, alpha_neg, w_neg, b_neg, 0.0)\n    \n    alpha_scaled_neg = s_neg * alpha_neg\n    w_scaled_neg = w_neg / s_neg\n    y_hat_scaled_neg, _ = compute_forward_loss(z_neg, alpha_scaled_neg, w_scaled_neg, b_neg, 0.0)\n    \n    is_equal_neg = np.isclose(y_hat_orig_neg, y_hat_scaled_neg, atol=1e-12)\n    results.append(bool(is_equal_neg))\n    \n    # Test Case 4: Identifiability fails for mixed signs\n    z_mix2 = np.array([-1.0, 0.5])\n    alpha_mix2 = alpha_neg #0.7\n    w_mix2 = w_neg #[1.2, -0.8]\n    b_mix2 = b_neg #0.3\n    s_mix2 = s_neg # 2.5\n    \n    y_hat_orig_mix2, _ = compute_forward_loss(z_mix2, alpha_mix2, w_mix2, b_mix2, 0.0)\n    \n    alpha_scaled_mix2 = s_mix2 * alpha_mix2\n    w_scaled_mix2 = w_mix2 / s_mix2\n    y_hat_scaled_mix2, _ = compute_forward_loss(z_mix2, alpha_scaled_mix2, w_scaled_mix2, b_mix2, 0.0)\n    \n    is_equal_mix2 = np.isclose(y_hat_orig_mix2, y_hat_scaled_mix2, atol=1e-12)\n    results.append(bool(is_equal_mix2))\n\n    # Test Case 5: All-positive pre-activations\n    z_pos = np.array([0.4, 0.2])\n    alpha_pos = 1.7\n    w_pos = np.array([0.6, -0.3])\n    b_pos = -0.1\n    y_pos = 0.5\n\n    y_hat_pos, _ = compute_forward_loss(z_pos, alpha_pos, w_pos, b_pos, y_pos)\n    grad_all_pos = analytical_gradient(z_pos, alpha_pos, w_pos, y_hat_pos, y_pos)\n    results.append(grad_all_pos)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3123807"}]}