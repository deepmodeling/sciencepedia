{"hands_on_practices": [{"introduction": "要构建稳定且易于训练的深度网络，理解其最基本的组成部分——单个神经元的行为至关重要。这个练习将带你深入探究一个核心问题：我们如何通过巧妙地设置初始化参数，来控制神经元输出的统计特性。通过从第一性原理出发进行推导 [@problem_id:3098917]，你将学会如何通过调整偏置（bias）来中心化激活前的信号，并理解激活函数的对称性如何帮助在整个网络中维持信号的稳定传播。", "problem": "考虑深度前馈网络层中的单个隐藏单元。该单元接收一个标量随机输入 $X$，其均值为 $\\mu$，且其分布关于 $\\mu$ 对称，即 $X - \\mu$ 的分布与 $-(X - \\mu)$ 的分布相同。该单元通过线性变换后接非线性函数来计算预激活值 $z$ 和激活值 $a$：\n$$\nz = w X + b, \\quad a = \\phi(z),\n$$\n其中 $w \\in \\mathbb{R}$ 是权重，$b \\in \\mathbb{R}$ 是偏置，$\\phi:\\mathbb{R}\\to\\mathbb{R}$ 是一个固定的激活函数。偏置使用启发式方法初始化为 $b = -\\mu$。\n\n仅使用期望、期望的线性性以及对称分布的定义，且不调用任何快捷公式，完成以下任务：\n1. 在给定的偏置初始化和输入均值下，推导出作为 $w$ 和 $\\mu$ 函数的 $\\mathbb{E}[z]$。\n2. 从第一性原理出发，确定在 $b=-\\mu$ 的条件下，能确保 $\\mathbb{E}[z]=0$ 的 $w$ 值。\n3. 找出一个 $\\phi$ 的充分且明确的函数属性，该属性能保证当 $z$ 的分布关于 $0$ 对称时，$a$ 的分布也关于 $0$ 对称。\n4. 将激活的对称性得分 $S$ 定义为 $S := \\mathbb{E}[a]$。在你推导出的条件下，精确计算 $S$ 的值。\n\n将你的最终答案表示为 $S$ 的精确值。无需四舍五入。", "solution": "用户希望我解决一个关于神经网络中单个隐藏单元统计特性的问题。\n\n### 问题验证\n\n首先，我将根据指定的程序验证问题陈述。\n\n#### 步骤 1：提取已知条件\n\n- 单个隐藏单元接收一个标量随机输入 $X$。\n- 输入的均值为 $\\mathbb{E}[X] = \\mu$。\n- $X$ 的分布关于 $\\mu$ 对称，意味着 $X - \\mu$ 的分布与 $-(X - \\mu)$ 的分布相同。\n- 预激活值为 $z = w X + b$，其中 $w \\in \\mathbb{R}$ 是权重，$b \\in \\mathbb{R}$ 是偏置。\n- 激活值为 $a = \\phi(z)$，其中 $\\phi: \\mathbb{R} \\to \\mathbb{R}$ 是一个固定的激活函数。\n- 偏置使用启发式方法初始化为 $b = -\\mu$。\n- 任务包含四个部分：\n    1. 推导出作为 $w$ 和 $\\mu$ 函数的 $\\mathbb{E}[z]$。\n    2. 确定能确保 $\\mathbb{E}[z]=0$ 的 $w$ 值。\n    3. 找出一个 $\\phi$ 的充分函数属性，该属性能保证当 $z$ 的分布关于 $0$ 对称时，$a$ 的分布也关于 $0$ 对称。\n    4. 定义对称性得分 $S := \\mathbb{E}[a]$ 并在推导出的条件下计算其精确值。\n\n#### 步骤 2：使用提取的已知条件进行验证\n\n- **科学依据：** 该问题牢固地基于概率论的基本原理（期望、对称分布）及其在标准前馈网络神经元模型中的应用。所有概念在数学和机器学习领域都是公认的。\n- **适定性：** 该问题以一系列逻辑上关联的问题呈现。每个问题都陈述清晰，目标明确。所提供的信息足以推导出每个部分的唯一解，从而得出最终答案。\n- **客观性：** 问题陈述使用了精确、形式化的数学语言和定义。没有主观或模糊的术语。\n- **缺陷清单：**\n    1.  **科学或事实不健全：** 无。定义和关系均为该领域的标准。\n    2.  **不可形式化或不相关：** 无。该问题是一个形式化的数学练习，与深度学习架构的分析直接相关。\n    3.  **不完整或矛盾的设置：** 无。所有必要的变量、定义（$z$, $a$）和条件（$b=-\\mu$，$X$ 的对称性）均已提供。\n    4.  **不切实际或不可行：** 无。该设置是用于分析网络动态的常见理论简化。\n    5.  **不适定或结构不良：** 无。对称性的定义已明确给出。问题结构逻辑地引导了推导过程。\n    6.  **伪深刻、琐碎或同义反复：** 无。该问题需要对第一性原理有扎实的理解和应用，其结果并非显而易见。\n    7.  **无法进行科学验证：** 无。推导过程是数学性的，完全可以验证。\n\n#### 步骤 3：结论与行动\n\n问题陈述是 **有效的**。这是一个适定的、科学上合理的、客观的应用数学练习。我现在将开始解题。\n\n### 解答\n\n该问题要求分四部分进行推导，涉及隐藏单元激活值的期望值。我将使用第一性原理按顺序解决每个部分。\n\n**1. 推导作为 $w$ 和 $\\mu$ 函数的 $\\mathbb{E}[z]$**\n\n预激活值 $z$ 定义为 $z = w X + b$。为了求其期望 $\\mathbb{E}[z]$，我们将期望算子应用于此定义：\n$$\n\\mathbb{E}[z] = \\mathbb{E}[w X + b]\n$$\n根据期望的线性性，即对于常数 $c_1, c_2$ 和随机变量 $Y_1, Y_2$，有 $\\mathbb{E}[c_1 Y_1 + c_2 Y_2] = c_1 \\mathbb{E}[Y_1] + c_2 \\mathbb{E}[Y_2]$，我们可以分离这些项。这里，$w$ 和 $b$ 是常数（非随机参数）。\n$$\n\\mathbb{E}[z] = \\mathbb{E}[w X] + \\mathbb{E}[b] = w \\mathbb{E}[X] + b\n$$\n给定 $\\mathbb{E}[X] = \\mu$ 且偏置初始化为 $b = -\\mu$。将这些值代入 $\\mathbb{E}[z]$ 的表达式中：\n$$\n\\mathbb{E}[z] = w \\mu + (-\\mu) = (w - 1)\\mu\n$$\n\n**2. 确定能确保 $\\mathbb{E}[z]=0$ 的 $w$ 值**\n\n使用上一步的结果，我们令 $\\mathbb{E}[z]$ 等于 $0$：\n$$\n(w - 1)\\mu = 0\n$$\n为使此方程成立，要么 $w-1=0$，要么 $\\mu=0$。如果 $\\mu=0$，则对于任意 $w$ 值，$\\mathbb{E}[z]=0$。然而，问题要求的是“$w$ 的值”，这意味着一个唯一的解，该解应适用于一般的输入分布，而不仅仅是均值为零的分布。因此，我们假设 $\\mu \\neq 0$。在这个非平凡的假设下，满足该方程的唯一方法是：\n$$\nw - 1 = 0 \\implies w = 1\n$$\n\n**3. 找出一个 $\\phi$ 的充分属性以保证 $a$ 的对称性**\n\n给定 $z$ 的分布关于 $0$ 对称。这意味着随机变量 $z$ 和 $-z$ 具有相同的分布。我们需要找到函数 $\\phi$ 的一个属性，使得 $a = \\phi(z)$ 的分布也关于 $0$ 对称。\n\n要使 $a$ 的分布关于 $0$ 对称，随机变量 $a$ 和 $-a$ 必须具有相同的分布。\n让我们用随机变量 $z$ 来表示这一点。我们需要 $\\phi(z)$ 和 $-\\phi(z)$ 具有相同的分布。\n由于 $z$ 和 $-z$ 具有相同的分布，这意味着对于任何行为良好的测试函数 $g$，我们有 $\\mathbb{E}[g(z)] = \\mathbb{E}[g(-z)]$。\n\n我们希望 $\\phi(z)$ 的分布与 $-\\phi(z)$ 的分布相同。确保这一点的充分条件是，映射 $z \\to \\phi(z)$ 的函数与映射 $z \\to -\\phi(z)$ 的函数以特定方式关联。\n考虑 $\\phi(z)$ 和 $\\phi(-z)$ 之间的关系。\n如果我们施加条件 $\\phi(-z) = -\\phi(z)$ 对所有 $z \\in \\mathbb{R}$ 成立，这意味着 $\\phi$ 是一个 **奇函数**。\n让我们看看这个属性是否充分。我们感兴趣的是 $-\\phi(z)$ 的分布。如果 $\\phi$ 是奇函数，那么 $\\phi(-z)$ 的分布与 $-\\phi(z)$ 的分布相同。\n由于 $z$ 和 $-z$ 具有相同的分布，因此 $\\phi(z)$ 的分布与 $\\phi(-z)$ 的分布相同。\n综合这些事实：\n$\\phi(z)$ 的分布 = $\\phi(-z)$ 的分布 = $-\\phi(z)$ 的分布。\n这表明 $a = \\phi(z)$ 的分布与 $-a = -\\phi(z)$ 的分布相同，这正是关于 $0$ 对称的分布的定义。\n因此，一个充分属性是 $\\phi$ 是一个 **奇函数**。\n\n**4. 计算对称性得分 $S = \\mathbb{E}[a]$**\n\n我们必须在推导出的条件下计算 $S = \\mathbb{E}[a]$：$w=1$ 且 $\\phi$ 是一个奇函数。给定的偏置是 $b=-\\mu$。\n预激活值为 $z = wX+b = (1)X + (-\\mu) = X - \\mu$。\n激活值为 $a = \\phi(z) = \\phi(X - \\mu)$。\n对称性得分是 $S = \\mathbb{E}[a] = \\mathbb{E}[\\phi(X - \\mu)]$。\n\n让我们定义一个新的随机变量 $Z' = X - \\mu$。\n问题陈述指出，$X$ 的分布关于 $\\mu$ 对称，其形式化定义为 $X - \\mu$ 的分布与 $-(X - \\mu)$ 的分布相同。\n这意味着 $Z'$ 的分布与 $-Z'$ 的分布相同。换句话说，$Z'$ 的分布关于 $0$ 对称。\n\n我们需要计算 $S = \\mathbb{E}[\\phi(Z')]$。\n根据关于 $0$ 对称的分布的定义，对于任何可测函数 $g$，我们有 $\\mathbb{E}[g(Z')] = \\mathbb{E}[g(-Z')]$。\n让我们选择我们的函数 $g$ 为激活函数 $\\phi$。因此，\n$$\n\\mathbb{E}[\\phi(Z')] = \\mathbb{E}[\\phi(-Z')]\n$$\n从第 3 部分，我们得到条件 $\\phi$ 是一个奇函数，即对于任何输入 $y$，有 $\\phi(-y) = -\\phi(y)$。将此应用于随机变量 $Z'$：\n$$\n\\phi(-Z') = -\\phi(Z')\n$$\n现在将此代回到我们的期望等式中：\n$$\n\\mathbb{E}[\\phi(Z')] = \\mathbb{E}[-\\phi(Z')]\n$$\n使用期望的线性性，$\\mathbb{E}[-Y] = -\\mathbb{E}[Y]$，我们可以写出：\n$$\n\\mathbb{E}[\\phi(Z')] = -\\mathbb{E}[\\phi(Z')]\n$$\n我们要计算的量是 $S = \\mathbb{E}[\\phi(Z')]$。方程变为：\n$$\nS = -S\n$$\n两边同时加上 $S$ 得到 $2S = 0$，这意味着 $S=0$。\n\n对称性得分 $S$，即在指定条件下的期望激活值，恰好为 $0$。对于任何关于其均值 $\\mu$ 对称的输入分布和任何奇激活函数 $\\phi$，在给定的权重和偏置初始化 $w=1$ 和 $b=-\\mu$ 的情况下，这个结论都成立。", "answer": "$$\n\\boxed{0}\n$$", "id": "3098917"}, {"introduction": "在单个神经元的基础上，我们进而考虑如何设计高效的网络层。在实践中，模型的大小（参数数量）与性能之间常常需要权衡。这个练习 [@problem_id:3098849] 探讨了一种强大的参数压缩技术——低秩分解（low-rank factorization），它可以在大幅减少计算和内存开销的同时，维持模型的表达能力。通过亲手计算参数量并分析秩的限制，你将具体地理解非线性激活函数（如 $\\mathrm{ReLU}$）如何在“受限”的线性变换之上，通过分段线性的方式恢复甚至增强网络的整体表达能力。", "problem": "考虑一个全连接深度前馈网络，其输入维度为 $n_0 = 600$，有一个宽度为 $n_1 = 400$ 的隐藏层，输出维度为 $n_2 = 200$。每个层都使用一个偏置向量。对于第一层，权重矩阵表示为 $W_1 \\in \\mathbb{R}^{n_1 \\times n_0}$，偏置为 $b_1 \\in \\mathbb{R}^{n_1}$；对于第二层，权重矩阵为 $W_2 \\in \\mathbb{R}^{n_2 \\times n_1}$，偏置为 $b_2 \\in \\mathbb{R}^{n_2}$。层间的激活函数是修正线性单元（Rectified Linear Unit, ReLU），其按元素定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$。\n\n该网络在两种设计方案下进行训练：\n- 设计 $A$：未分解的权重 $W_1$ 和 $W_2$。\n- 设计 $B$：每个权重矩阵通过分解 $W_i = U_i V_i^\\top$ 被约束为低秩，其中 $U_1 \\in \\mathbb{R}^{n_1 \\times r_1}$，$V_1 \\in \\mathbb{R}^{n_0 \\times r_1}$，$U_2 \\in \\mathbb{R}^{n_2 \\times r_2}$，以及 $V_2 \\in \\mathbb{R}^{n_1 \\times r_2}$，且 $r_1 = 50$，$r_2 = 40$。\n\n仅使用仿射映射、矩阵秩、线性复合和逐点非线性的基本定义来推断参数数量和表示能力。选择所有正确的陈述：\n\nA. 设计 $A$ 中的可训练参数总数为 $320600$，而设计 $B$ 中的可训练参数总数为 $74600$。\n\nB. 在给定的维度和秩下，与设计 $A$ 相比，设计 $B$ 的参数数量减少了超过 $70\\%$。\n\nC. 如果移除激活函数（使得网络变为纯线性），则整体线性映射 $x \\mapsto W_2 W_1 x + W_2 b_1 + b_2$ 的矩阵部分 $W_2 W_1$ 的秩最多为 $\\min(r_1, r_2) = 40$。\n\nD. 在层间使用修正线性单元（ReLU）时，增加深度可以通过将输入空间划分为多个区域来弥补秩的限制，从而使网络在不同区域实现不同的低秩线性映射；尽管在任何单个输入处的雅可比矩阵的秩最多为 $\\min(r_1, r_2)$，但整体的分段线性函数可以逼近单个秩为 r 的线性层无法表示的映射。\n\nE. 在层间使用修正线性单元（ReLU）时，存在某些输入，使得网络的雅可比矩阵的秩严格大于 $\\min(r_1, r_2)$。", "solution": "我们从基本原理开始。一个全连接层计算一个仿射映射 $x \\mapsto W x + b$，其中 $W$ 是一个自由参数矩阵，$b$ 是一个自由参数向量。这样一个层中的参数数量等于 $W$ 中的自由元素数量加上 $b$ 中的元素数量。如果一个矩阵 $W \\in \\mathbb{R}^{m \\times n}$ 不受约束，它有 $mn$ 个自由元素。如果 $W$ 被约束分解为 $U V^\\top$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 且 $V \\in \\mathbb{R}^{n \\times r}$，那么自由元素的数量是 $mr + nr$，并且矩阵的秩最多为 $r$。对于线性映射的复合，乘积的秩满足 $\\mathrm{rank}(A B) \\le \\min(\\mathrm{rank}(A), \\mathrm{rank}(B))$。对于具有像修正线性单元（ReLU）这样的逐点非线性的网络，函数变为分段线性的；在任何输入 $x$ 处，网络的雅可比矩阵等于权重矩阵和对角激活掩码的乘积，其秩受该乘积中最小秩瓶颈的限制。然而，不同区域的不同激活掩码会产生不同的局部线性映射，从而能够实现比单一全局线性映射更丰富的函数类别。\n\n计算参数数量。\n\n- 设计 $A$（未分解）：\n\n第一层参数：$W_1$ 有 $n_1 n_0 = 400 \\cdot 600 = 240000$ 个元素；$b_1$ 有 $n_1 = 400$ 个元素。第二层参数：$W_2$ 有 $n_2 n_1 = 200 \\cdot 400 = 80000$ 个元素；$b_2$ 有 $n_2 = 200$ 个元素。设计 A 的总参数：\n$$240000 + 400 + 80000 + 200 = 320600.$$\n\n- 设计 $B$（分解）：\n\n第一层参数：$U_1$ 有 $n_1 r_1 = 400 \\cdot 50 = 20000$ 个元素；$V_1$ 有 $n_0 r_1 = 600 \\cdot 50 = 30000$ 个元素；$b_1$ 有 $n_1 = 400$ 个元素。总和：$$20000 + 30000 + 400 = 50400.$$\n第二层参数：$U_2$ 有 $n_2 r_2 = 200 \\cdot 40 = 8000$ 个元素；$V_2$ 有 $n_1 r_2 = 400 \\cdot 40 = 16000$ 个元素；$b_2$ 有 $n_2 = 200$ 个元素。总和：$$8000 + 16000 + 200 = 24200.$$\n设计 B 的总参数：\n$$50400 + 24200 = 74600.$$\n\n计算相对减少量：\n$$\\text{reduction fraction} = 1 - \\frac{74600}{320600} \\approx 1 - 0.2326 \\approx 0.7674,$$\n这是一个大约 $76.74\\%$ 的减少，即大于 $70\\%$。\n\n秩的性质。\n\n如果移除激活函数，整体映射是线性的：$x \\mapsto W_2 W_1 x + W_2 b_1 + b_2$。矩阵部分是 $W_2 W_1$。在分解约束下，$\\mathrm{rank}(W_1) \\le r_1$ 且 $\\mathrm{rank}(W_2) \\le r_2$。因此\n$$\\mathrm{rank}(W_2 W_1) \\le \\min(\\mathrm{rank}(W_2), \\mathrm{rank}(W_1)) \\le \\min(r_2, r_1) = 40.$$\n\n使用修正线性单元（ReLU），网络函数是分段线性的。对于任何特定的输入 $x$，激活模式会产生一个对角元素在 $\\{0,1\\}$ 中的对角矩阵 $D$，因此局部雅可比矩阵等于 $J(x) = W_2 D W_1$。因此，对于任何固定的 $x$，\n$$\\mathrm{rank}(J(x)) \\le \\min(\\mathrm{rank}(W_2), \\mathrm{rank}(D), \\mathrm{rank}(W_1)) \\le \\min(r_2, r_1) = 40.$$\n然而，随着 $x$ 的变化，$D$ 也会变化，在不同区域产生不同的局部线性映射 $W_2 D W_1$。这种划分允许网络实现大量不同的低秩线性片段的拼接，从而逼近单一全局低秩线性层无法表示的函数。这是带有非线性的深度弥补窄层或低秩层的核心机制：它增加了线性区域的数量并启用了复合结构，而不是增加在给定输入处雅可比矩阵的逐点秩。\n\n逐项分析：\n\nA. 上述计算表明，设计 A 有 $320600$ 个参数，设计 B 有 $74600$ 个参数。结论：正确。\n\nB. 减少量约为 $76.74\\%$，确实超过了 $70\\%$。结论：正确。\n\nC. 在没有非线性的情况下，根据秩的次可乘性， $W_2 W_1$ 的秩最多为 $\\min(r_1, r_2) = 40$。结论：正确。\n\nD. 使用修正线性单元（ReLU），网络变为分段线性的，在不同区域具有不同的激活掩码 $D$。每个区域的雅可比矩阵的秩受 $\\min(r_1, r_2)$ 的限制，但网络可以通过拼接许多低秩片段来逼近单个秩为 r 的线性层无法表示的映射。这准确地描述了深度如何弥补秩的限制。结论：正确。\n\nE. 在任何输入 $x$ 处的雅可比矩阵等于 $W_2 D W_1$，其秩的上界为 $\\min(r_1, r_2)$。因此，它不能超过 $\\min(r_1, r_2)$。结论：错误。", "answer": "$$\\boxed{ABCD}$$", "id": "3098849"}, {"introduction": "除了优化网络内部的权重和偏置，我们还可以通过设计更优质的输入特征来提升模型性能。这个动手编码练习 [@problem_id:3098829] 将向你展示一种在现代深度学习架构（如 Transformer 和神经辐射场）中广泛应用的技术：位置编码（positional encoding）。你将通过编写代码，亲眼见证一个简单的浅层网络如何借助固定的、高频的特征输入，成功学习和拟合复杂的目标函数。这个过程清晰地揭示了，精巧的特征工程有时能媲美增加网络深度所带来的表达能力提升。", "problem": "要求您形式化并量化一种称为位置编码的固定非线性特征映射，如何增强一个浅层前馈架构的有效表达能力（相对于一个直接在原始输入上操作的基线模型）。请考虑在域 $[0,1]$ 上的以下组件和定义。\n\n1. 定义阶为 $K$ 的位置编码 $\\gamma_K(x)$，它是角频率呈指数增长的正弦和余弦特征的拼接：\n$$\n\\gamma_K(x) = \\Big[ \\sin(2^0 \\pi x), \\cos(2^0 \\pi x), \\sin(2^1 \\pi x), \\cos(2^1 \\pi x), \\ldots, \\sin(2^K \\pi x), \\cos(2^K \\pi x) \\Big].\n$$\n\n2. 定义一个基于位置编码的浅层模型类，即 $\\gamma_K(x)$ 上的线性读出，\n$$\n\\mathcal{F}_{\\text{PE},K} = \\left\\{ f(x) = \\sum_{k=0}^{K} \\big(a_k \\sin(2^k \\pi x) + b_k \\cos(2^k \\pi x)\\big) + c \\,:\\, a_k, b_k, c \\in \\mathbb{R} \\right\\}.\n$$\n这是一个在固定非线性特征上的单层线性模型，在深度学习实践中，它是一种带有预设特征映射的标准浅层架构。\n\n3. 定义一个在原始输入上操作而不使用位置编码的基线浅层模型，\n$$\n\\mathcal{F}_{\\text{plain}} = \\left\\{ f(x) = w_1 x + w_0 \\,:\\, w_1, w_0 \\in \\mathbb{R} \\right\\}.\n$$\n\n4. 定义在 $[0,1]$ 上要逼近的目标函数：\n   - 一个单高频目标：$g_1(x) = \\sin(2^m \\pi x)$，其中 $m = 3$。\n   - 一个混合频率目标：$g_2(x) = 0.5 \\sin(2 \\pi x) + 0.25 \\cos(4 \\pi x) + 0.2 \\sin(8 \\pi x)$。\n   - 一个平滑多项式目标：$g_3(x) = x^2$。\n\n5. 设一个模型类对目标 $g(x)$ 在均匀网格 $x_i = \\frac{i}{N-1}$（其中 $i \\in \\{0,1,\\ldots,N-1\\}$）上的经验最小二乘拟合，定义为最小化以下表达式：\n$$\n\\frac{1}{N}\\sum_{i=0}^{N-1} \\big( f(x_i) - g(x_i) \\big)^2.\n$$\n在本问题中，使用大小为 $N = 4096$ 个点的网格，并通过对模型的特征进行线性最小二乘来获得最小化器。\n\n6. 定义拟合 $f$ 对目标 $g$ 的经验均方误差 (MSE) 为\n$$\nE(f, g) = \\frac{1}{N}\\sum_{i=0}^{N-1} \\big( f(x_i) - g(x_i) \\big)^2.\n$$\n\n7. 定义对于给定的目标 $g$ 和位置编码阶数 $K$ 的表达能力提升比为\n$$\nR(g, K) = \\frac{E\\big(f_{\\text{plain}}^\\star, g\\big)}{E\\big(f_{\\text{PE},K}^\\star, g\\big)},\n$$\n其中 $f_{\\text{plain}}^\\star \\in \\mathcal{F}_{\\text{plain}}$ 和 $f_{\\text{PE},K}^\\star \\in \\mathcal{F}_{\\text{PE},K}$ 表示它们各自类别中的最小二乘最小化器。为了在可完美表示的情况下避免数值上的除零错误，分母使用一个下限值 $10^{-12}$，即除以 $\\max\\big(E(f_{\\text{PE},K}^\\star, g), 10^{-12}\\big)$。\n\n8. 定义一个基于在 $[0,1]$ 上最大符号变化次数（零点穿越次数）的简单容量代理指标：\n   - 对于基线模型 $\\mathcal{F}_{\\text{plain}}$，最大符号变化次数为 $S_{\\text{plain}} = 1$。\n   - 对于位置编码模型 $\\mathcal{F}_{\\text{PE},K}$（一个最大角频率为 $2^K \\pi$ 的三角多项式），使用上界 $S_{\\text{PE}}(K) = 2 \\cdot 2^K$。\n\n您的任务：\n\nA) 对每个目标 $g_1, g_2, g_3$ 和 $K \\in \\{0,1,3\\}$，实现 $\\mathcal{F}_{\\text{plain}}$ 和 $\\mathcal{F}_{\\text{PE},K}$ 的最小二乘拟合，并计算 $R(g,K)$。\n\nB) 对 $K \\in \\{0,1,3\\}$，计算符号变化容量代理指标 $S_{\\text{plain}}$ 和 $S_{\\text{PE}}(K)$。\n\n测试套件和要求输出：\n\n- 使用上面指定的 $N = 4096$、$m = 3$、$K \\in \\{0,1,3\\}$ 以及三个目标 $g_1, g_2, g_3$。\n- 生成单行输出，包含以下 13 个值，并严格遵循此顺序：\n$$\n\\big[ S_{\\text{plain}}, S_{\\text{PE}}(0), S_{\\text{PE}}(1), S_{\\text{PE}}(3), R(g_1,0), R(g_1,1), R(g_1,3), R(g_2,0), R(g_2,1), R(g_2,3), R(g_3,0), R(g_3,1), R(g_3,3) \\big].\n$$\n所有输出均为不带单位的实数或整数。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，$\\big[ \\text{result}_1, \\text{result}_2, \\ldots \\big]$）。", "solution": "这个问题要求我们量化由位置编码特征所带来的表达能力提升（与简单线性模型相比）。问题的核心在于使用线性最小二乘进行函数逼近。我们将求解两个不同模型类 $\\mathcal{F}_{\\text{plain}}$ 和 $\\mathcal{F}_{\\text{PE},K}$ 的最优参数，以最佳拟合三个不同的目标函数，然后比较它们产生的逼近误差。\n\n基本原理是，两个模型类 $\\mathcal{F}_{\\text{plain}}$ 和 $\\mathcal{F}_{\\text{PE},K}$ 在其参数上都是线性的。如果一个函数 $f(x; \\mathbf{w})$ 可以写成基函数 $\\phi_j(x)$ 的线性组合，那么它在其参数 $\\mathbf{w} = (w_1, \\ldots, w_p)^T$ 上是线性的：\n$$\nf(x; \\mathbf{w}) = \\sum_{j=1}^{p} w_j \\phi_j(x) = \\mathbf{w}^T \\mathbf{\\phi}(x)\n$$\n线性最小二乘的目标是找到参数向量 $\\mathbf{w}^\\star$，它能最小化在一组 $N$ 个数据点 $(x_i, g_i)$ 上的均方误差 (MSE)：\n$$\nE(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j=1}^{p} w_j \\phi_j(x_i) - g(x_i) \\right)^2\n$$\n在矩阵形式下，令 $\\mathbf{g}$ 为目标值 $g(x_i)$ 的向量，$\\mathbf{\\Phi}$ 为 $N \\times p$ 的设计矩阵，其中 $\\mathbf{\\Phi}_{ij} = \\phi_j(x_i)$。MSE 为 $E(\\mathbf{w}) = \\frac{1}{N} \\| \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{g} \\|_2^2$。最小化此误差的最优 $\\mathbf{w}^\\star$ 可以通过求解称为正规方程的线性系统 $\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\mathbf{w} = \\mathbf{\\Phi}^T\\mathbf{g}$ 来找到，或者通过更稳健的方法如 QR 分解或奇异值分解 (SVD) 来找到，这些都是数值库中的标准方法。\n\n容量代理指标 $S_{\\text{plain}}$ 和 $S_{\\text{PE}}(K)$ 直接根据提供的公式计算。$S_{\\text{plain}} = 1$ 是固定的。对于 $K \\in \\{0, 1, 3\\}$，我们计算 $S_{\\text{PE}}(K) = 2 \\cdot 2^K$。\n\n主要的计算任务是为每个模型和目标找到最小化的误差 $E(f^\\star, g)$。\n\n1.  **定义网格和目标**：我们首先在区间 $[0,1]$ 上创建包含 $N=4096$ 个点的均匀网格 $x_i = \\frac{i}{N-1}$。然后，我们在此网格上评估目标函数 $g_1(x) = \\sin(8\\pi x)$、$g_2(x) = 0.5 \\sin(2 \\pi x) + 0.25 \\cos(4 \\pi x) + 0.2 \\sin(8 \\pi x)$ 和 $g_3(x) = x^2$，以获得目标向量 $\\mathbf{g}_1, \\mathbf{g}_2, \\mathbf{g}_3$。\n\n2.  **拟合基线模型 $\\mathcal{F}_{\\text{plain}}$**：模型为 $f(x) = w_1 x + w_0$。基函数为 $\\phi_1(x) = x$ 和 $\\phi_2(x) = 1$。设计矩阵 $\\mathbf{\\Phi}_{\\text{plain}}$ 是一个 $N \\times 2$ 的矩阵，其第一列是 $x_i$ 的值，第二列全是 1。我们为每个目标 $\\mathbf{g}_j$ 求解最小二乘问题，以找到最小化的 MSE $E(f_{\\text{plain}}^\\star, g_j)$。\n\n3.  **拟合位置编码模型 $\\mathcal{F}_{\\text{PE},K}$**：对于每个阶数 $K \\in \\{0, 1, 3\\}$，模型是一个三角多项式。基函数是 $\\{\\sin(2^k\\pi x), \\cos(2^k\\pi x)\\}_{k=0}^K$ 再加上一个常数基函数 $\\phi(x)=1$。参数（和基函数）的总数是 $p = 2(K+1) + 1$。对于每个 $K$，我们通过在网格上评估这些基函数来构建相应的 $N \\times p$ 设计矩阵 $\\mathbf{\\Phi}_{\\text{PE},K}$。对于每个目标 $\\mathbf{g}_j$，我们求解最小二乘问题以找到最小化的 MSE $E(f_{\\text{PE},K}^\\star, g_j)$。\n\n4.  **计算表达能力提升比**：在计算出两种模型类的最小 MSE 后，我们为每个目标 $g_j$ 和阶数 $K$ 计算比率 $R(g, K)$：\n    $$\n    R(g_j, K) = \\frac{E(f_{\\text{plain}}^\\star, g_j)}{\\max\\big(E(f_{\\text{PE},K}^\\star, g_j), 10^{-12}\\big)}\n    $$\n    这个比率衡量了位置编码模型的误差相对于基线线性模型小多少倍。大的比率表示显著的表达能力优势。分母被设定了一个下限值以防止除以零，当模型类 $\\mathcal{F}_{\\text{PE},K}$ 能够完美表示目标函数时（例如，对于 $g_1, g_2$ 当 $K=3$ 时），这种情况很可能发生。\n\n该算法的流程是：首先计算容量代理指标。然后，对于三个目标函数中的每一个，计算基线误差 $E(f_{\\text{plain}}^\\star, g)$。接着，对于每个 $K \\in \\{0, 1, 3\\}$，计算位置编码模型误差 $E(f_{\\text{PE},K}^\\star, g)$ 和相应的比率 $R(g, K)$。最后，所有计算出的值按照指定的顺序组合成一个单一列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes capacity proxies and expressivity boost ratios for shallow models\n    with and without positional encoding.\n    \"\"\"\n    \n    # --- Problem Parameters ---\n    N = 4096\n    m = 3\n    K_values = [0, 1, 3]\n\n    # --- Step 1: Define Grid and Target Functions ---\n    x_grid = np.linspace(0, 1, N, dtype=np.float64)\n    \n    # g1(x) = sin(2^3 * pi * x) = sin(8 * pi * x)\n    g1 = np.sin(2**m * np.pi * x_grid)\n    \n    # g2(x) = 0.5*sin(2*pi*x) + 0.25*cos(4*pi*x) + 0.2*sin(8*pi*x)\n    g2 = (0.5 * np.sin(2 * np.pi * x_grid) + \n          0.25 * np.cos(4 * np.pi * x_grid) + \n          0.2 * np.sin(8 * np.pi * x_grid))\n          \n    # g3(x) = x^2\n    g3 = x_grid**2\n    \n    targets = {\n        'g1': g1,\n        'g2': g2,\n        'g3': g3\n    }\n    target_names = ['g1', 'g2', 'g3']\n\n    # --- Task B: Compute Capacity Proxies ---\n    S_plain = 1\n    S_PE = {K: 2 * (2**K) for K in K_values}\n\n    def get_mse(X, y):\n        \"\"\"\n        Solves the linear least-squares problem and returns the mean squared error.\n        \n        A robust method for calculating MSE is used by finding the optimal weights `w`\n        and then explicitly computing the error `mean((X@w - y)**2)`. This handles\n        cases of perfect fits where the 'residuals' output of np.linalg.lstsq\n        can be an empty array.\n        \"\"\"\n        w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        y_pred = X @ w\n        mse = np.mean((y - y_pred)**2)\n        return mse\n\n    # --- Fit Baseline Model (F_plain) ---\n    X_plain = np.vstack([x_grid, np.ones(N)]).T\n    mse_plain_results = {}\n    for name in target_names:\n        mse_plain_results[name] = get_mse(X_plain, targets[name])\n        \n    # --- Fit Positional Encoding Model (F_PE,K) ---\n    mse_pe_results = {name: {} for name in target_names}\n    for K in K_values:\n        # Construct the design matrix X_PE for the given K\n        features = []\n        for k in range(K + 1):\n            freq = 2**k\n            features.append(np.sin(freq * np.pi * x_grid))\n            features.append(np.cos(freq * np.pi * x_grid))\n        features.append(np.ones(N)) # Bias term\n        X_pe = np.vstack(features).T\n        \n        # Calculate MSE for each target\n        for name in target_names:\n            mse_pe_results[name][K] = get_mse(X_pe, targets[name])\n            \n    # --- Task A: Compute Expressivity Boost Ratios (R) ---\n    R_results = {name: {} for name in target_names}\n    for name in target_names:\n        for K in K_values:\n            numerator = mse_plain_results[name]\n            denominator = max(mse_pe_results[name][K], 1e-12)\n            R_results[name][K] = numerator / denominator\n\n    # --- Assemble Final Output in the specified order ---\n    final_output = []\n    # Capacity Proxies\n    final_output.append(S_plain)\n    final_output.append(S_PE[0])\n    final_output.append(S_PE[1])\n    final_output.append(S_PE[3])\n    \n    # Ratios for g1, g2, g3\n    for name in target_names:\n        for K in K_values:\n            final_output.append(R_results[name][K])\n            \n    # Print the formatted output\n    print(f\"[{','.join(f'{v:.6f}' for v in final_output)}]\")\n\nsolve()\n```", "id": "3098829"}]}