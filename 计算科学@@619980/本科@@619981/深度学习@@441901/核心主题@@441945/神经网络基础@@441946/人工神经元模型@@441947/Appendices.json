{"hands_on_practices": [{"introduction": "本练习将带您回到神经网络的起源，探索作为其基本构建单元的感知器模型。我们将首先通过实现一个简单的“多数表决”逻辑功能，直观地理解单个神经元如何作为线性分类器工作。接着，我们将直面著名的异或（XOR）问题，从第一性原理出发，揭示单个神经元的根本局限性，并为理解为何需要更复杂的网络结构或特征工程奠定基础。[@problem_id:3190716]", "problem": "考虑一个二元分类任务，其感知机模型由决策函数 $\\hat{y} = \\operatorname{sign}(w^{\\top} x + b)$ 定义，其中 $x \\in \\{0,1\\}^{d}$，$w \\in \\mathbb{R}^{d}$，$b \\in \\mathbb{R}$。在以下所有部分中，类别标签为 $y \\in \\{-1,+1\\}$，其中正类标记为 $+1$。\n\nA部分（三输入多数表决）：令 $d = 3$，并将目标定义为多数表决逻辑：如果 $x_1 + x_2 + x_3 \\geq 2$，则 $y = +1$，否则 $y = -1$。假设感知机具有相等的正权重，$w_1 = w_2 = w_3 = w$ 且 $w  0$，以及偏置 $b \\in \\mathbb{R}$。在所有对 $x \\in \\{0,1\\}^{3}$ 都能正确分类的分离超平面中，确定比值 $b/w$ 的值，该值能最大化决策边界与任意训练点之间的最小有符号距离（沿感知机法线方向测量）。\n\nB部分（异或（XOR）问题的失败）：考虑 $d=2$ 的情况，其异或（XOR）目标定义为：如果 $x_1 + x_2 = 1$，则 $y = +1$；如果 $x_1 + x_2 \\in \\{0,2\\}$，则 $y = -1$。请从第一性原理（不使用已陈述的定理）解释为什么在原始输入空间中，没有任何 $(w,b)$ 的选择能产生一个分离超平面。\n\nC部分（通过成对交互特征恢复可分性）：通过添加成对交互项来增广 $d=2$ 的异或（XOR）输入，形成特征映射 $\\phi(x_1,x_2) = (x_1, x_2, x_1 x_2) \\in \\mathbb{R}^{3}$。解释在增广空间中，如何通过适当选择权重和偏置来正确分离异或数据。\n\n你的最终答案应该是A部分中 $b/w$ 的精确值，表示为一个最简分数。不包含任何单位。无需四舍五入。", "solution": "该问题对感知机模型进行了三部分分析。在验证问题完整性后，我将系统地解决每个部分。\n\n问题陈述是有效的。它科学地植根于线性分类器和感知机模型的既定理论。任务适定、客观，并使用精确的数学语言，提供了推导解决方案所需的所有必要信息。该问题是机器学习基础中的一个标准的、非平凡的练习。\n\n### A部分：三输入多数表决函数\n\n任务是找到比值 $b/w$，该比值能为一个用于分类三输入多数表决函数的感知机最大化最小几何间隔。\n\n输入空间为 $x \\in \\{0,1\\}^3$。共有 $2^3=8$ 个可能的输入向量。目标函数为：如果输入之和 $S = x_1+x_2+x_3$ 至少为 $2$，则 $y=+1$，否则 $y=-1$。数据点及其对应的标签 $y$ 和和值 $S$ 如下：\n- $S=0$：$x=(0,0,0)$，$y=-1$\n- $S=1$：$x \\in \\{(1,0,0), (0,1,0), (0,0,1)\\}$，$y=-1$\n- $S=2$：$x \\in \\{(1,1,0), (1,0,1), (0,1,1)\\}$，$y=+1$\n- $S=3$：$x=(1,1,1)$，$y=+1$\n\n感知机模型的决策边界由 $w^{\\top}x+b=0$ 定义。给定权重相等且为正，$w_1=w_2=w_3=w  0$。因此，激活值为：\n$$w^{\\top}x+b = w_1x_1+w_2x_2+w_3x_3+b = w(x_1+x_2+x_3)+b = wS+b$$\n为了使带有标签 $y$ 的点 $x$ 被正确分类，必须满足 $y(wS+b)0$。让我们为所有8个点的可分性建立条件。\n- 对于 $S=0, y=-1$：$(-1)(w \\cdot 0 + b)  0 \\implies -b  0 \\implies b  0$。\n- 对于 $S=1, y=-1$：$(-1)(w \\cdot 1 + b)  0 \\implies -w-b  0 \\implies w+b  0$。\n- 对于 $S=2, y=+1$：$(+1)(w \\cdot 2 + b)  0 \\implies 2w+b  0$。\n- 对于 $S=3, y=+1$：$(+1)(w \\cdot 3 + b)  0 \\implies 3w+b  0$。\n\n从 $w+b0$ 我们得到 $b  -w$。从 $2w+b0$ 我们得到 $b  -2w$。结合这些条件，我们发现任何这种形式的分离超平面都必须满足 $-2w  b  -w$。因为 $w0$，这意味着 $b0$，这与我们的第一个不等式一致。它还意味着 $3w+b  3w-2w = w  0$，满足第四个不等式。因此，可分性的条件是 $-2  b/w  -1$。\n\n一个点 $x_i$ 到决策边界的有符号几何距离（或间隔）由 $\\gamma_i = \\frac{y_i(w^{\\top}x_i+b)}{\\|w\\|}$ 给出。我们的目标是最大化最小的该距离，即 $\\gamma_{\\min} = \\min_{i} \\gamma_i$。\n权重向量的范数为 $\\|w\\| = \\sqrt{w_1^2+w_2^2+w_3^2} = \\sqrt{w^2+w^2+w^2} = \\sqrt{3w^2} = w\\sqrt{3}$（因为 $w0$）。\n\n对于由其和 $S$ 区分的每一组点，其间隔为：\n- $S=0, y=-1$：$\\gamma_0 = \\frac{(-1)(w \\cdot 0 + b)}{w\\sqrt{3}} = \\frac{-b}{w\\sqrt{3}}$\n- $S=1, y=-1$：$\\gamma_1 = \\frac{(-1)(w \\cdot 1 + b)}{w\\sqrt{3}} = \\frac{-w-b}{w\\sqrt{3}}$\n- $S=2, y=+1$：$\\gamma_2 = \\frac{(+1)(w \\cdot 2 + b)}{w\\sqrt{3}} = \\frac{2w+b}{w\\sqrt{3}}$\n- $S=3, y=+1$：$\\gamma_3 = \\frac{(+1)(w \\cdot 3 + b)}{w\\sqrt{3}} = \\frac{3w+b}{w\\sqrt{3}}$\n\n整体间隔为 $\\gamma_{\\min} = \\min(\\gamma_0, \\gamma_1, \\gamma_2, \\gamma_3)$。约束间隔的点（即支持向量）是那些离决策边界最近的点。在可分区域（$-2w  b  -w$）内，$y(wS+b)$ 的最小正值将来自 $S=1$ 和 $S=2$。让我们来验证这一点。令 $f(S) = y(wS+b)$。\n- $f(0)=-b$，$f(1)=-w-b$。因为 $w0$，所以 $-b  -w-b$。因此 $\\gamma_0  \\gamma_1$。\n- $f(2)=2w+b$，$f(3)=3w+b$。因为 $w0$，所以 $2w+b  3w+b$。因此 $\\gamma_2  \\gamma_3$。\n因此最小间隔为 $\\gamma_{\\min} = \\min(\\gamma_1, \\gamma_2)$。为了最大化这个最小值，我们必须使两个参数相等。这在最大间隔超平面上发生。\n$$\\gamma_1 = \\gamma_2$$\n$$\\frac{-w-b}{w\\sqrt{3}} = \\frac{2w+b}{w\\sqrt{3}}$$\n$$-w-b = 2w+b$$\n$$-3w = 2b$$\n$$\\frac{b}{w} = -\\frac{3}{2}$$\n这个值位于区间 $(-2, -1)$ 内，证实了它对应一个有效的分离超平面。最大化最小距离的比值 $b/w$ 是 $-3/2$。\n\n### B部分：异或问题的不可分性\n\n对于 $d=2$ 的异或（XOR）函数由以下输入-输出对定义：\n- $x^{(1)}=(0,0)$，$y^{(1)}=-1$\n- $x^{(2)}=(0,1)$，$y^{(2)}=+1$\n- $x^{(3)}=(1,0)$，$y^{(3)}=+1$\n- $x^{(4)}=(1,1)$，$y^{(4)}=-1$\n\n线性分离器是一个由 $w_1 x_1 + w_2 x_2 + b = 0$ 定义的超平面。为了使数据线性可分，必须存在参数 $(w_1, w_2, b)$ 使得对于所有 $i \\in \\{1,2,3,4\\}$ 都有 $y^{(i)}(w_1 x_1^{(i)} + w_2 x_2^{(i)} + b)  0$。这产生了一个由四个线性不等式组成的系统：\n1. 对于 $(0,0), y=-1$：$(-1)(w_1 \\cdot 0 + w_2 \\cdot 0 + b)  0 \\implies -b  0 \\implies b  0$。\n2. 对于 $(0,1), y=+1$：$(+1)(w_1 \\cdot 0 + w_2 \\cdot 1 + b)  0 \\implies w_2 + b  0$。\n3. 对于 $(1,0), y=+1$：$(+1)(w_1 \\cdot 1 + w_2 \\cdot 0 + b)  0 \\implies w_1 + b  0$。\n4. 对于 $(1,1), y=-1$：$(-1)(w_1 \\cdot 1 + w_2 \\cdot 1 + b)  0 \\implies w_1 + w_2 + b  0$。\n\n让我们证明这个系统无解。\n从不等式(2)，我们有 $w_2  -b$。\n从不等式(3)，我们有 $w_1  -b$。\n将这两个表达式相加得到：\n$$w_1 + w_2  -2b$$\n从不等式(1)，我们知道 $b$ 是负数。令 $c = -b$，其中 $c  0$。这些不等式变为：\n- $w_2  c$\n- $w_1  c$\n- $w_1 + w_2  2c$\n不等式(4)可以重写为 $w_1 + w_2  -b$，即 $w_1 + w_2  c$。\n我们推导出了两个相互矛盾的要求：$w_1 + w_2  2c$ 和 $w_1 + w_2  c$。由于 $c$ 是一个正值，一个数不能同时大于 $2c$ 又小于 $c$。这个矛盾证明了没有任何一个线性超平面能满足所有四个条件。因此，异或函数是线性不可分的。\n\n### C部分：用特征映射恢复可分性\n\n我们通过使用 $\\phi(x_1,x_2) = (x_1, x_2, x_1 x_2)$ 将输入 $x=(x_1, x_2)$ 映射到一个新的特征空间来增广输入空间。我们称新坐标为 $z=(z_1, z_2, z_3)$。数据点转换如下：\n- $x=(0,0) \\implies z^{(1)}=(0,0,0)$，$y^{(1)}=-1$\n- $x=(0,1) \\implies z^{(2)}=(0,1,0)$，$y^{(2)}=+1$\n- $x=(1,0) \\implies z^{(3)}=(1,0,0)$，$y^{(3)}=+1$\n- $x=(1,1) \\implies z^{(4)}=(1,1,1)$，$y^{(4)}=-1$\n\n我们现在在这个新的三维空间中寻找一个分离超平面，它由 $w'^{\\top}z + b' = w'_1 z_1 + w'_2 z_2 + w'_3 z_3 + b' = 0$ 定义。可分性条件是：\n1. 对于 $z=(0,0,0), y=-1$：$-b'  0 \\implies b'  0$。\n2. 对于 $z=(0,1,0), y=+1$：$w'_2 + b'  0$。\n3. 对于 $z=(1,0,0), y=+1$：$w'_1 + b'  0$。\n4. 对于 $z=(1,1,1), y=-1$：$-(w'_1 + w'_2 + w'_3 + b')  0 \\implies w'_1 + w'_2 + w'_3 + b'  0$。\n\n与原始问题不同，这个不等式系统有解。让我们构建一个解。\n选择 $w'_1 = 1$ 和 $w'_2 = 1$。\n根据条件(2)和(3)，我们需要 $1+b'  0$，所以 $b'  -1$。条件(1)要求 $b'  0$。因此我们必须有 $-1  b'  0$。我们选择 $b' = -1/2$。\n现在，将这些值代入条件(4)：\n$1 + 1 + w'_3 - \\frac{1}{2}  0 \\implies \\frac{3}{2} + w'_3  0 \\implies w'_3  -\\frac{3}{2}$。\n我们选择 $w'_3 = -2$。\n\n一个有效的分离器由参数 $w'=(1, 1, -2)$ 和 $b'=-1/2$ 给出。让我们验证一下：\n- $z^{(1)}=(0,0,0), y=-1$：$1(0)+1(0)-2(0) - 1/2 = -1/2$。$\\operatorname{sign}(-1/2)=-1$。正确。\n- $z^{(2)}=(0,1,0), y=+1$：$1(0)+1(1)-2(0) - 1/2 = 1/2$。$\\operatorname{sign}(1/2)=+1$。正确。\n- $z^{(3)}=(1,0,0), y=+1$：$1(1)+1(0)-2(0) - 1/2 = 1/2$。$\\operatorname{sign}(1/2)=+1$。正确。\n- $z^{(4)}=(1,1,1), y=-1$：$1(1)+1(1)-2(1) - 1/2 = -1/2$。$\\operatorname{sign}(-1/2)=-1$。正确。\n\n添加非线性特征 $x_1x_2$ 将数据映射到更高维的空间，使其变得线性可分。从几何上看，原始 $x_1, x_2$ 平面上的四个点无法被一条直线分开。该特征映射将其中一个点 $(1,1)$ 从 $z_3=0$ 平面上提升到 $(1,1,1)$。在 $\\mathbb{R}^3$ 中的四个点是 $(0,0,0)$、$(0,1,0)$、$(1,0,0)$ 和 $(1,1,1)$。现在可以放置一个平面（例如 $z_1+z_2-2z_3 - 1/2 = 0$）来将正类点 $\\{(0,1,0), (1,0,0)\\}$ 与负类点 $\\{(0,0,0), (1,1,1)\\}$ 分开。\n\n最终要求的答案是A部分的具体结果。如上所推导，这是比值 $b/w$。", "answer": "$$\\boxed{-\\frac{3}{2}}$$", "id": "3190716"}, {"introduction": "在了解了感知器的分类能力后，本练习将视角转换为将神经元视为一个回归工具。通过使用恒等激活函数，我们将探索神经元如何拟合连续的数值目标，而不仅仅是类别标签。您将从最小化平方误差的原则出发，推导出最优的权重和偏置，这个过程实际上是从神经网络的角度重新发现了线性回归的核心思想，从而加深您对神经网络与经典统计方法之间联系的理解。[@problem_id:3180363]", "problem": "考虑一个使用恒等激活函数的单个人工神经元，其对于输入向量 $\\mathbf{x} \\in \\mathbb{R}^{d}$ 的输出为 $f(\\mathbf{x};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其中 $\\mathbf{w} \\in \\mathbb{R}^{d}$ 是突触权重，$b \\in \\mathbb{R}$ 是偏置。对于一个带标签的数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$，其中 $y_{i} \\in \\{-1, +1\\}$，定义经验平方误差目标函数为 $J(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}$。仅从这些定义出发，根据第一性原理推导最小化 $J(\\mathbf{w}, b)$ 的闭式最优参数 $(\\mathbf{w}^{\\star}, b^{\\star})$。\n\n然后，将您的推导应用于 $\\mathbb{R}^{2}$ 中的一个特定的线性可分数据集：\n- 正类 ($y_{i} = +1$): $\\mathbf{x}_{1} = (2, 0)$, $\\mathbf{x}_{2} = (0, 2)$。\n- 负类 ($y_{i} = -1$): $\\mathbf{x}_{3} = (-2, 0)$, $\\mathbf{x}_{4} = (0, -2)$。\n\n计算该数据集的最优 $\\mathbf{w}^{\\star} = (w_{1}^{\\star}, w_{2}^{\\star})$ 和 $b^{\\star}$，并以行向量 $\\big[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}\\big]$ 的形式提供您的最终答案。将每个数值分量四舍五入到四位有效数字。\n\n最后，定性地（无需计算）讨论，在何种条件下，单个恒等激活神经元的平方误差最小化器所产生的决策边界与支持向量机（SVM）的大间隔分离超平面重合，例如类的对称性和协方差属性。您的讨论应基于以上定义以及任何关于几何间隔的公认事实进行推理。", "solution": "用户提供了一个包含多个部分的问题。在验证问题陈述后，我将按顺序解决每个部分。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- **模型：** 具有恒等激活函数的单个人工神经元。\n- **输出函数：** $f(\\mathbf{x};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其中 $\\mathbf{x} \\in \\mathbb{R}^{d}$，$\\mathbf{w} \\in \\mathbb{R}^{d}$，以及 $b \\in \\mathbb{R}$。\n- **数据集：** 一个带标签的数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$，其标签 $y_{i} \\in \\{-1, +1\\}$。\n- **目标函数：** 经验平方误差，$J(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}$。\n- **任务 1：** 推导最小化 $J(\\mathbf{w}, b)$ 的闭式最优参数 $(\\mathbf{w}^{\\star}, b^{\\star})$。\n- **任务 2：** 将推导应用于 $\\mathbb{R}^{2}$ 中的一个特定数据集（$d=2$, $n=4$）：\n  - 正类 ($y=+1$): $\\mathbf{x}_{1} = (2, 0)$, $\\mathbf{x}_{2} = (0, 2)$。\n  - 负类 ($y=-1$): $\\mathbf{x}_{3} = (-2, 0)$, $\\mathbf{x}_{4} = (0, -2)$。\n- **任务 3：** 计算 $(\\mathbf{w}^{\\star}, b^{\\star})$ 的数值，并以行向量 $[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}]$ 的形式提供最终答案，每个分量四舍五入到四位有效数字。\n- **任务 4：** 定性讨论在何种条件下，平方误差最小化器的决策边界与大间隔支持向量机（SVM）的决策边界重合。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题是线性回归的标准表述，是统计学和机器学习中的一个基本课题。模型是一个简单的线性神经元，目标函数是平方误差和。这在科学上和数学上都是合理的。\n- **适定性：** 目标函数 $J(\\mathbf{w}, b)$ 是关于参数 $\\mathbf{w}$ 和 $b$ 的二次函数。它是凸函数，并有唯一的全局最小值。因此，存在一个唯一的、稳定的且有意义的解。\n- **客观性：** 问题使用精确的数学符号和客观的语言进行陈述。\n- **其他缺陷：** 问题是自洽的，没有缺失或矛盾的信息。提供的数据是一致且简单的。问题并非微不足道，需要使用多元微积分进行标准推导。它不是比喻性的，并且是可验证的。\n\n**步骤 3：结论与行动**\n问题是**有效的**。我将继续进行解答。\n\n### 第 1 部分：最优参数的推导\n\n问题是要找到最小化目标函数的参数 $(\\mathbf{w}^{\\star}, b^{\\star})$：\n$$\nJ(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}\n$$\n这是一个无约束优化问题。一个可微凸函数的最小值出现在其关于参数的梯度为零向量的地方。解决这个问题的一个常用方法是使用正规方程，这可以通过增广输入向量来推导。\n\n我们定义一个增广权重向量 $\\hat{\\mathbf{w}} \\in \\mathbb{R}^{d+1}$ 和一个增广输入向量 $\\hat{\\mathbf{x}}_{i} \\in \\mathbb{R}^{d+1}$ 如下：\n$$\n\\hat{\\mathbf{w}} = \\begin{pmatrix} \\mathbf{w} \\\\ b \\end{pmatrix}, \\quad \\hat{\\mathbf{x}}_{i} = \\begin{pmatrix} \\mathbf{x}_{i} \\\\ 1 \\end{pmatrix}\n$$\n通过这些定义，神经元的输出可以写成单个内积：\n$$\nf(\\mathbf{x}_{i};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x}_{i} + b = \\hat{\\mathbf{w}}^{\\top}\\hat{\\mathbf{x}}_{i}\n$$\n令 $\\hat{X}$ 为 $n \\times (d+1)$ 的设计矩阵，其中第 $i$ 行是 $\\hat{\\mathbf{x}}_{i}^{\\top}$，并令 $\\mathbf{y}$ 为标签 $y_i$ 组成的 $n \\times 1$ 列向量。目标函数可以以矩阵形式写为残差向量的平方欧几里得范数：\n$$\nJ(\\hat{\\mathbf{w}}) = \\|\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y}\\|_{2}^{2}\n$$\n为了找到最小值，我们计算关于 $\\hat{\\mathbf{w}}$ 的梯度并将其设为零：\n$$\n\\nabla_{\\hat{\\mathbf{w}}}J(\\hat{\\mathbf{w}}) = \\nabla_{\\hat{\\mathbf{w}}} \\left( (\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y})^{\\top}(\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y}) \\right) = \\mathbf{0}\n$$\n$$\n\\nabla_{\\hat{\\mathbf{w}}} \\left( \\hat{\\mathbf{w}}^{\\top}\\hat{X}^{\\top}\\hat{X}\\hat{\\mathbf{w}} - 2\\mathbf{y}^{\\top}\\hat{X}\\hat{\\mathbf{w}} + \\mathbf{y}^{\\top}\\mathbf{y} \\right) = \\mathbf{0}\n$$\n$$\n2\\hat{X}^{\\top}\\hat{X}\\hat{\\mathbf{w}} - 2\\hat{X}^{\\top}\\mathbf{y} = \\mathbf{0}\n$$\n这就得出了正规方程：\n$$\n(\\hat{X}^{\\top}\\hat{X})\\hat{\\mathbf{w}} = \\hat{X}^{\\top}\\mathbf{y}\n$$\n假设矩阵 $\\hat{X}^{\\top}\\hat{X}$ 是可逆的，则唯一的最优参数向量 $\\hat{\\mathbf{w}}^{\\star}$ 由下式给出：\n$$\n\\hat{\\mathbf{w}}^{\\star} = (\\hat{X}^{\\top}\\hat{X})^{-1}\\hat{X}^{\\top}\\mathbf{y}\n$$\n项 $\\hat{X}^{\\top}\\hat{X}$ 和 $\\hat{X}^{\\top}\\mathbf{y}$ 可以用数据集上的和来表示：\n$$\n\\hat{X}^{\\top}\\hat{X} = \\sum_{i=1}^{n} \\hat{\\mathbf{x}}_{i}\\hat{\\mathbf{x}}_{i}^{\\top} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i} \\\\ 1 \\end{pmatrix} \\begin{pmatrix} \\mathbf{x}_{i}^{\\top}  1 \\end{pmatrix} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\mathbf{x}_{i} \\\\ \\mathbf{x}_{i}^{\\top}  1 \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\sum_{i=1}^{n} \\mathbf{x}_{i} \\\\ \\left(\\sum_{i=1}^{n} \\mathbf{x}_{i}\\right)^{\\top}  n \\end{pmatrix}\n$$\n$$\n\\hat{X}^{\\top}\\mathbf{y} = \\sum_{i=1}^{n} \\hat{\\mathbf{x}}_{i}y_{i} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i}y_{i} \\\\ y_{i} \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} y_{i}\\mathbf{x}_{i} \\\\ \\sum_{i=1}^{n} y_{i} \\end{pmatrix}\n$$\n最优参数 $(\\mathbf{w}^{\\star}, b^{\\star})$ 通过求解由这些聚合矩阵和向量定义的线性系统得到。\n\n### 第 2 部分：应用于特定数据集\n\n给定的数据集为：\n- $\\mathbf{x}_{1} = (2, 0)^{\\top}$, $y_{1} = +1$\n- $\\mathbf{x}_{2} = (0, 2)^{\\top}$, $y_{2} = +1$\n- $\\mathbf{x}_{3} = (-2, 0)^{\\top}$, $y_{3} = -1$\n- $\\mathbf{x}_{4} = (0, -2)^{\\top}$, $y_{4} = -1$\n\n这里，$n=4$ 且 $d=2$。我们计算必要的总和：\n$$\nn = 4\n$$\n$$\n\\sum_{i=1}^{4} \\mathbf{x}_{i} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\sum_{i=1}^{4} y_{i} = 1 + 1 - 1 - 1 = 0\n$$\n$$\n\\sum_{i=1}^{4} y_{i}\\mathbf{x}_{i} = (1)\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + (1)\\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + (-1)\\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} + (-1)\\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix}\n$$\n$$\n\\sum_{i=1}^{4} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top} = \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix} + \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix} = \\begin{pmatrix} 8  0 \\\\ 0  8 \\end{pmatrix}\n$$\n现在，我们构建系统 $(\\hat{X}^{\\top}\\hat{X})\\hat{\\mathbf{w}} = \\hat{X}^{\\top}\\mathbf{y}$：\n$$\n\\begin{pmatrix} \\sum \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\sum \\mathbf{x}_{i} \\\\ (\\sum \\mathbf{x}_{i})^{\\top}  n \\end{pmatrix} \\begin{pmatrix} \\mathbf{w} \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\sum y_{i}\\mathbf{x}_{i} \\\\ \\sum y_{i} \\end{pmatrix}\n$$\n代入计算出的值：\n$$\n\\begin{pmatrix} 8  0  0 \\\\ 0  8  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} w_{1} \\\\ w_{2} \\\\ b \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 0 \\end{pmatrix}\n$$\n这对应一个由三个线性方程组成的简单系统：\n1. $8w_{1} = 4 \\implies w_{1}^{\\star} = \\frac{4}{8} = 0.5$\n2. $8w_{2} = 4 \\implies w_{2}^{\\star} = \\frac{4}{8} = 0.5$\n3. $4b = 0 \\implies b^{\\star} = 0$\n\n最优参数为 $\\mathbf{w}^{\\star} = (0.5, 0.5)^{\\top}$ 和 $b^{\\star} = 0$。\n\n### 第 3 部分：数值答案与定性讨论\n\n问题要求以行向量 $[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}]$ 的形式提供最终答案，每个分量四舍五入到四位有效数字。\n- $w_{1}^{\\star} = 0.5 \\implies 0.5000$\n- $w_{2}^{\\star} = 0.5 \\implies 0.5000$\n- $b^{\\star} = 0 \\implies 0.0000$\n\n最终的数值答案向量是 $[0.5000\\;\\;0.5000\\;\\;0.0000]$。\n\n**定性讨论：**\n单个神经元的决策边界是输出为零的点集 $\\mathbf{x}$：$\\mathbf{w}^{\\star\\top}\\mathbf{x} + b^{\\star} = 0$。平方误差最小化器通过对类别标签 $y_i \\in \\{-1, +1\\}$ 进行线性回归来找到这个边界。相比之下，硬间隔支持向量机（SVM）找到的是最大化两类之间几何间隔的分离超平面。SVM 的解只依赖于最靠近边界的数据点（即支持向量），而最小二乘解则受到训练集中每个数据点的影响。\n\n只有在特定的、高度对称的条件下，这两个决策边界才会重合。这些条件确保了最小二乘回归的“质心”逻辑与 SVM 的“最大间隔”逻辑相一致。关键条件是：\n\n1.  **类别均衡：** 每个类别中的数据点数量必须相等（$n_+ = n_-$）。这确保了标签的均值为零（$\\bar{y}=0$），并且整体数据质心 $\\bar{\\mathbf{x}} = (\\mu_+ + \\mu_-)/2$ 位于类别质心 $\\mu_+$ 和 $\\mu_-$ 之间的几何中点。在这种条件下，最小二乘决策边界将通过这个中点，SVM 边界也是如此。\n\n2.  **对称的类分布：** 每个类内的数据点分布必须是对称的，以至于不会使最小二乘解偏离最大间隔解。理想情况是当类条件分布 $p(\\mathbf{x}|y=+1)$ 和 $p(\\mathbf{x}|y=-1)$ 具有相同的、球形的协方差矩阵（例如，与单位矩阵成比例，$cI$）。在这种情况下，每个类中的数据点围绕其各自的均值各向同性地分布。远离边界的点的“平均影响”会完美抵消，由最小二乘法确定的权重向量 $\\mathbf{w}^\\star$ 的方向（这与 Fisher 的线性判别分析有关）变得与连接类均值的向量 $\\mu_+ - \\mu_-$ 平行。在这样的对称性下，这与 SVM 的权重向量 $\\mathbf{w}_{\\text{SVM}}$ 的方向相同。\n\n如果这些条件不满足，解通常会发散。例如，如果一个类有离群点或者比另一个类分布得更广，这些点会把最小二乘回归线“拉”向它们，使其偏离 SVM 会找到的最优大间隔位置。所提供的数据集是这些对称性成立的一个例子，这就是为什么得到的边界 $0.5x_1 + 0.5x_2 = 0$（或 $x_1+x_2=0$）确实是最大间隔分离超平面。", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5000  0.5000  0.0000 \\end{pmatrix}}\n$$", "id": "3180363"}, {"introduction": "最后一个练习将我们的分析带入现代深度学习的背景，采用当今最常见的激活函数——修正线性单元（ReLU）。我们将不再局限于分析单个输入，而是从统计的视角考察神经元在面对整个输入分布时的行为。您将计算一个神经元被“激活”的概率，这一过程将让您初步体验支撑我们理解大规模神经网络的统计分析方法，并揭示偏置项 $b$ 对神经元激活率的敏感性。[@problem_id:3180428]", "problem": "考虑一个单一人工神经元，其预激活为 $a=w^{\\top}x+b$，激活函数为整流线性单元（ReLU）$f(a)=\\max\\{0,a\\}$。设输入 $x\\in\\mathbb{R}^{d}$ 来自零均值各向同性高斯分布 $x\\sim\\mathcal{N}(0,I_{d})$，权重向量 $w\\in\\mathbb{R}^{d}$ 固定且满足 $\\|w\\|_{2}0$。如果 $a0$，则称该神经元被激活。从多元正态分布的核心定义以及高斯随机向量线性泛函的性质出发，推导概率 $\\mathbb{P}(w^{\\top}x+b0)$ 关于 $b$ 和 $\\|w\\|_{2}$ 的闭式表达式。然后，为分析当偏置与权重大小成比例时的灵敏度，设 $b=\\alpha\\|w\\|_{2}$（其中 $\\alpha\\in\\mathbb{R}$ 为标量），将概率 $\\mathbb{P}(w^{\\top}x+b0)$ 表示为 $\\alpha$ 的函数，并计算该概率对 $\\alpha$ 在 $\\alpha=0$ 处的导数。你可以用 $\\Phi$ 表示标准正态分布的累积分布函数，用 $\\phi$ 表示其概率密度函数。将你的最终答案表示为在 $\\alpha=0$ 处导数的精确值。无需四舍五入。", "solution": "在尝试求解之前，对问题陈述进行严格验证。\n\n### 第1步：提取已知条件\n- 神经元预激活: $a = w^{\\top}x + b$\n- 激活函数: 整流线性单元 (ReLU), $f(a) = \\max\\{0, a\\}$\n- 输入向量: $x \\in \\mathbb{R}^{d}$，取自零均值各向同性高斯分布, $x \\sim \\mathcal{N}(0, I_d)$\n- 权重向量: $w \\in \\mathbb{R}^{d}$，固定的，其范数满足 $\\|w\\|_2  0$\n- 偏置项: $b \\in \\mathbb{R}$\n- 神经元激活条件: $a  0$，等价于 $w^{\\top}x + b  0$\n- 任务第1部分：推导概率 $\\mathbb{P}(w^{\\top}x + b  0)$ 关于 $b$ 和 $\\|w\\|_2$ 的闭式表达式。\n- 任务第2部分：对于标量 $\\alpha \\in \\mathbb{R}$，设 $b = \\alpha \\|w\\|_2$，并将概率表示为 $\\alpha$ 的函数。\n- 任务第3部分：计算该概率关于 $\\alpha$ 在 $\\alpha = 0$ 处的导数。\n- 符号说明：$\\Phi$ 表示标准正态分布的累积分布函数 (CDF)，$\\phi$ 表示其概率密度函数 (PDF)。\n\n### 第2步：使用提取的已知条件进行验证\n对问题进行有效性评估。\n- **具有科学基础：** 该问题牢固地植根于概率论及其在人工神经网络分析中的应用。高斯随机向量、随机变量的线性变换、ReLU 激活以及神经元激活的统计特性等概念，都是机器学习理论中的标准和基础内容。\n- **适定性：** 问题陈述清晰。它提供了所有必要的信息：输入的分布、神经元组件的定义以及明确的目标。条件 $\\|w\\|_2  0$ 至关重要且被正确地包含，以防止除以零，从而确保解是良定义的。可以从前提中推导出一个唯一、稳定且有意义的解。\n- **客观性：** 该问题以精确、客观的数学语言陈述，没有任何歧义或主观论断。\n\n### 第3步：结论与行动\n该问题是**有效**的。这是一个神经网络统计分析中的标准、适定的问题。现在可以开始求解过程。\n\n### 解题推导\n目标是计算神经元被激活的概率，即 $\\mathbb{P}(a  0)$。预激活为 $a = w^{\\top}x + b$，因此我们需要计算 $\\mathbb{P}(w^{\\top}x + b  0)$。\n\n定义随机变量 $Y$ 为输入向量 $x$ 在权重向量 $w$ 上的线性投影：\n$$Y = w^{\\top}x = \\sum_{i=1}^{d} w_i x_i$$\n输入向量 $x$ 取自多元正态分布 $x \\sim \\mathcal{N}(0, I_d)$，其中 $0$ 是零向量，$I_d$ 是 $d \\times d$ 单位矩阵。这意味着分量 $x_i$ 是独立同分布的标准正态随机变量，$x_i \\sim \\mathcal{N}(0, 1)$。\n\n高斯随机变量的线性组合本身也是一个高斯随机变量。为了确定 $Y$ 的分布，我们必须求出其均值 $\\mathbb{E}[Y]$ 和方差 $\\text{Var}(Y)$。\n\n$Y$ 的均值为：\n$$\\mathbb{E}[Y] = \\mathbb{E}[w^{\\top}x] = w^{\\top}\\mathbb{E}[x]$$\n由于 $\\mathbb{E}[x] = 0$，均值为：\n$$\\mathbb{E}[Y] = w^{\\top}0 = 0$$\n\n$Y$ 的方差由通用公式 $\\text{Var}(w^{\\top}x) = w^{\\top}\\text{Cov}(x)w$ 给出。$x$ 的协方差矩阵为 $\\text{Cov}(x) = I_d$。因此：\n$$\\text{Var}(Y) = w^{\\top}I_d w = w^{\\top}w = \\sum_{i=1}^{d} w_i^2 = \\|w\\|_2^2$$\n因此，随机变量 $Y$ 服从均值为 $0$、方差为 $\\|w\\|_2^2$ 的正态分布。我们将其记为 $Y \\sim \\mathcal{N}(0, \\|w\\|_2^2)$。\n\n现在，我们可以计算激活概率：\n$$\\mathbb{P}(a  0) = \\mathbb{P}(w^{\\top}x + b  0) = \\mathbb{P}(Y  -b)$$\n为了使用标准正态分布计算这个概率，我们对随机变量 $Y$ 进行标准化。设 $Z$ 为一个标准正态随机变量，$Z \\sim \\mathcal{N}(0, 1)$。$Y$ 的标准化为：\n$$Z = \\frac{Y - \\mathbb{E}[Y]}{\\sqrt{\\text{Var}(Y)}} = \\frac{w^{\\top}x - 0}{\\sqrt{\\|w\\|_2^2}} = \\frac{w^{\\top}x}{\\|w\\|_2}$$\n不等式 $Y  -b$ 可以改写为用 $Z$ 表示的形式：\n$$\\frac{Y}{\\|w\\|_2}  \\frac{-b}{\\|w\\|_2} \\implies Z  -\\frac{b}{\\|w\\|_2}$$\n那么概率就是 $\\mathbb{P}(Z  -\\frac{b}{\\|w\\|_2})$。根据标准正态累积分布函数 (CDF) $\\Phi(z) = \\mathbb{P}(Z \\le z)$ 的定义，我们有 $\\mathbb{P}(Z  z) = 1 - \\Phi(z)$。此外，由于标准正态分布的对称性，有 $\\Phi(-z) = 1 - \\Phi(z)$。这意味着 $\\mathbb{P}(Z  z) = \\Phi(-z)$。\n\n应用此性质，令 $z = -\\frac{b}{\\|w\\|_2}$：\n$$\\mathbb{P}(Z  -\\frac{b}{\\|w\\|_2}) = \\Phi\\left(-\\left(-\\frac{b}{\\|w\\|_2}\\right)\\right) = \\Phi\\left(\\frac{b}{\\|w\\|_2}\\right)$$\n这就得到了用 $b$ 和 $\\|w\\|_2$ 表示的概率的闭式表达式。\n\n接下来，我们分析该概率对偏置的灵敏度，条件是偏置与权重大小成比例。对于某个标量 $\\alpha \\in \\mathbb{R}$，我们令 $b = \\alpha \\|w\\|_2$。\n设 $P(\\alpha)$ 为激活概率作为 $\\alpha$ 的函数：\n$$P(\\alpha) = \\mathbb{P}(w^{\\top}x + \\alpha\\|w\\|_2  0) = \\Phi\\left(\\frac{\\alpha \\|w\\|_2}{\\|w\\|_2}\\right) = \\Phi(\\alpha)$$\n题目要求计算该概率关于 $\\alpha$ 在 $\\alpha = 0$ 处的导数。我们首先求出导数 $\\frac{d}{d\\alpha}P(\\alpha)$。\n根据微积分基本定理，连续随机变量的累积分布函数（CDF）的导数是其概率密度函数（PDF）。标准正态分布的概率密度函数用 $\\phi$ 表示。\n$$\\frac{d}{d\\alpha}P(\\alpha) = \\frac{d}{d\\alpha}\\Phi(\\alpha) = \\phi(\\alpha)$$\n标准正态分布的概率密度函数由以下公式给出：\n$$\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$$\n我们需要在 $\\alpha = 0$ 处计算这个导数：\n$$\\frac{d P}{d\\alpha}\\bigg|_{\\alpha=0} = \\phi(0)$$\n将 $\\alpha=0$ 代入概率密度函数公式：\n$$\\phi(0) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{0^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp(0) = \\frac{1}{\\sqrt{2\\pi}} \\cdot 1 = \\frac{1}{\\sqrt{2\\pi}}$$\n这就是导数的最终精确值。", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi}}}$$", "id": "3180428"}]}