{"hands_on_practices": [{"introduction": "修正线性单元（ReLU）网络之所以强大，在于它们能够通过组合简单的部分来构建复杂的函数。本练习将探讨其基本机制：每个 ReLU 单元如何像一个“门”一样定义一个线性区域，通过组合这些门，网络能够将整个输入空间划分为多个部分。此练习的目标不是训练网络，而是从第一性原理出发，通过解析法构建一个网络，以模拟 k-means 聚类的决策边界，从而为您提供对 ReLU 网络表示能力的清晰、直观的理解。[@problem_id:3167799]", "problem": "你需要实现并分析一个带有修正线性单元（ReLU）的小型前馈网络，其中修正线性单元（ReLU）定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$。目标是从第一性原理出发，展示一个单隐藏层 ReLU 网络如何将 $\\mathbb{R}^2$ 划分成多个区域，这些区域的线性分隔器对应于 $k$-均值聚类中使用的最近中心分配。你必须仅从平方欧几里得距离和修正线性单元（ReLU）的定义出发，推导仿射变换后通过 $\\mathrm{ReLU}$ 进行门控如何能够复现 $k$-均值聚类的最近中心法则的分段线性决策边界。然后，你的程序必须使用直接从提供的聚类中心构造的权重和偏置来实现这个网络，无需任何训练，并在一组测试用例上验证其行为。\n\n使用的基本原理：\n- 在 $\\mathbb{R}^2$ 中，一个点 $x \\in \\mathbb{R}^2$ 和一个中心 $c \\in \\mathbb{R}^2$ 之间的平方欧几里得距离为 $\\|x - c\\|_2^2$。\n- $k$-均值聚类中使用的最近中心分配将点 $x$ 映射到使 $\\|x - c_i\\|_2^2$ 最小化的中心 $c_i$ 的索引 $i$。\n- 修正线性单元（ReLU）定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$，它作为一个门控非线性函数，当 $z \\ge 0$ 时输出 $z$，否则输出 $0$。\n\n你必须实现一个单隐藏层 ReLU 网络，其隐藏单元计算输入 $x$ 的仿射函数，其输出层线性聚合隐藏层输出，为每个聚类生成一个分数。对于所有提供的测试点，该网络必须产生与最近中心法则相同的聚类索引，并且平局情况通过选择最小索引来确定性地解决（例如，如果两个中心等距，则选择较小的索引）。你的推导和实现不得使用任何预构建的机器学习库或进行训练；所有参数都必须根据聚类中心解析地构建。\n\n本问题不涉及角度单位。没有物理量，因此不需要物理单位。当你报告比例或准确率时，请以小数形式表示（例如，$0.75$），不要使用百分号。\n\n测试套件规范：\n对于每个测试用例，参数集是由一个聚类中心列表和一个 $\\mathbb{R}^2$ 中的点列表组成的对。下面给出的所有坐标都是精确的，必须逐字使用。\n\n- 测试用例 1（两个聚类；边界位于垂直平分线上；包含边界上的平局情况）：\n  - 中心点：$\\left[(0, 0), (2, 0)\\right]$。\n  - 点：$\\left[(-1, 0), (0, 0), (0.9, 0), (1.0, 0), (1.1, 0), (3, 0), (2, 1)\\right]$。\n\n- 测试用例 2（三个聚类形成一个三角形；一般位置）：\n  - 中心点：$\\left[(0, 0), (2, 0), (1, 2)\\right]$。\n  - 点：$\\left[(0.1, 0.2), (2.1, -0.1), (1.0, 1.8), (0.9, 0.9), (1.1, 1.1)\\right]$。\n\n- 测试用例 3（仅含边界点，在两个聚类之间存在平局；需要显式处理平局）：\n  - 中心点：$\\left[(0, 0), (2, 0)\\right]$。\n  - 点：$\\left[(1, 0), (1, 2), (1, -2)\\right]$。\n\n- 测试用例 4（退化情况，中心点重合；可能出现三方平局；需要显式处理平局）：\n  - 中心点：$\\left[(0, 0), (0, 0), (2, 0)\\right]$。\n  - 点：$\\left[(-1, 0), (0, 0), (1, 0), (2, 0)\\right]$。\n\n- 测试用例 5（单聚类情况；平凡划分）：\n  - 中心点：$\\left[(3, 3)\\right]$。\n  - 点：$\\left[(-10, -10), (0, 0), (3, 3), (5, 5)\\right]$。\n\n程序要求：\n- 对于每个测试用例，使用以下两种方法为每个点计算聚类分配：\n  - 最小化 $\\|x - c_i\\|_2^2$ 的最近中心法则，平局情况通过选择最小索引来确定性地解决。\n  - ReLU 网络，其参数从中心点解析推导得出，其每个聚类的输出分数通过区域门控实现，使得决策边界是与中心点之间垂直平分线相对应的线性分隔器。当多个聚类达到最大分数时，使用与上述相同的确定性平局规则。\n- 对于每个测试用例，输出一个小数，表示两种方法产生相同索引的点所占的比例。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表，按测试用例 1 到 5 的顺序排列。例如，格式必须与 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$ 完全一样，其中每个 $\\text{result}_i$ 是一个小数，如 $1.0$ 或 $0.0$。", "solution": "目标是构建一个单隐藏层 ReLU 网络，该网络能够复制 k-均值聚类的最近中心分配规则。该规则将一个点 $x \\in \\mathbb{R}^2$ 分配给索引为 $j$ 的聚类，前提是其中心 $c_j$ 在所有中心 $\\{c_0, c_1, \\dots, c_{K-1}\\}$ 中是最近的。在数学上，分配的索引是：\n$$\n\\hat{k}(x) = \\underset{k \\in \\{0, \\dots, K-1\\}}{\\mathrm{argmin}} \\|x - c_k\\|_2^2\n$$\n这是一个 `argmin` 操作。如果我们对目标函数取反，它就等价于一个 `argmax` 操作。我们为每个聚类 $k$ 定义一个分数 $S_k(x)$，使得最大化这个分数等价于最小化平方距离。\n$$\n\\hat{k}(x) = \\underset{k}{\\mathrm{argmax}} \\left( -\\|x - c_k\\|_2^2 \\right)\n$$\n我们展开平方欧几里得距离项：\n$$\n-\\|x - c_k\\|_2^2 = -( (x - c_k)^T (x - c_k) ) = -(x^T x - 2x^T c_k + c_k^T c_k) = 2x^T c_k - \\|c_k\\|_2^2 - \\|x\\|_2^2\n$$\n最大化问题现在是：\n$$\n\\hat{k}(x) = \\underset{k}{\\mathrm{argmax}} \\left( 2x^T c_k - \\|c_k\\|_2^2 - \\|x\\|_2^2 \\right)\n$$\n对于给定的点 $x$，项 $-\\|x\\|_2^2$ 在所有聚类 $k$ 中都是常数。因此，它不影响 `argmax` 操作的结果，可以舍弃。我们可以定义一个等价的目标分数函数 $S_k^*(x)$，网络必须计算这个函数或与其成比例的函数：\n$$\nS_k^*(x) = 2x^T c_k - \\|c_k\\|_2^2\n$$\n对于一个输入 $x = [x_1, x_2]^T$ 和一个中心 $c_k = [c_{k1}, c_{k2}]^T$，上式展开为：\n$$\nS_k^*(x) = 2(c_{k1}x_1 + c_{k2}x_2) - (c_{k1}^2 + c_{k2}^2)\n$$\n这是输入 $x$ 的一个仿射函数。一个没有隐藏层（即一个线性层）的网络可以直接计算这些分数。然而，问题明确要求使用一个单隐藏层 ReLU 网络。为了满足这个要求，我们必须使用 ReLU 单元来表示对 $x$ 的线性依赖关系。任何实数 $z$ 都可以表示为其正部和负部之差，这可以用 ReLU 函数实现：\n$$\nz = \\max(0, z) - \\max(0, -z) = \\mathrm{ReLU}(z) - \\mathrm{ReLU}(-z)\n$$\n将此分解应用于输入分量 $x_1$ 和 $x_2$：\n$$\nx_1 = \\mathrm{ReLU}(x_1) - \\mathrm{ReLU}(-x_1)\n$$\n$$\nx_2 = \\mathrm{ReLU}(x_2) - \\mathrm{ReLU}(-x_2)\n$$\n将这些代入 $S_k^*(x)$ 的表达式中：\n$$\nS_k^*(x) = 2c_{k1}(\\mathrm{ReLU}(x_1) - \\mathrm{ReLU}(-x_1)) + 2c_{k2}(\\mathrm{ReLU}(x_2) - \\mathrm{ReLU}(-x_2)) - \\|c_k\\|_2^2\n$$\n此表达式展示了如何将目标分数构建为输入的 ReLU 激活函数的线性组合。我们现在可以设计网络架构了。\n\n**网络架构**\n\n1.  **输入层**：输入是向量 $x = [x_1, x_2]^T$。\n\n2.  **隐藏层**：隐藏层必须计算输出层所需的项。根据推导出的 $S_k^*(x)$ 表达式，我们需要以下隐藏单元激活值：$\\mathrm{ReLU}(x_1)$、$\\mathrm{ReLU}(-x_1)$、$\\mathrm{ReLU}(x_2)$ 和 $\\mathrm{ReLU}(-x_2)$。此外，常数偏置项 $-\\|c_k\\|_2^2$ 可以通过一个恒定激活值为 $1$ 的隐藏单元来实现。这可以通过一个权重为零、偏置为 $1$ 的神经元实现：$\\mathrm{ReLU}(0 \\cdot x_1 + 0 \\cdot x_2 + 1) = 1$。\n    因此，隐藏层有 $5$ 个单元，其激活值 $h = [h_1, h_2, h_3, h_4, h_5]^T$ 定义如下：\n    - $h_1 = \\mathrm{ReLU}(1 \\cdot x_1 + 0 \\cdot x_2 + 0) = \\mathrm{ReLU}(x_1)$\n    - $h_2 = \\mathrm{ReLU}(-1 \\cdot x_1 + 0 \\cdot x_2 + 0) = \\mathrm{ReLU}(-x_1)$\n    - $h_3 = \\mathrm{ReLU}(0 \\cdot x_1 + 1 \\cdot x_2 + 0) = \\mathrm{ReLU}(x_2)$\n    - $h_4 = \\mathrm{ReLU}(0 \\cdot x_1 - 1 \\cdot x_2 + 0) = \\mathrm{ReLU}(-x_2)$\n    - $h_5 = \\mathrm{ReLU}(0 \\cdot x_1 + 0 \\cdot x_2 + 1) = 1$\n    因此，隐藏层的权重 $W_h$ 和偏置 $b_h$ 为：\n    $$\n    W_h = \\begin{pmatrix} 1  0 \\\\ -1  0 \\\\ 0  1 \\\\ 0  -1 \\\\ 0  0 \\end{pmatrix}, \\quad b_h = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n    $$\n\n3.  **输出层**：输出层计算 $K$ 个分数 $S_k(x)$，每个聚类一个。它是一个线性层，用于组合隐藏层的激活值：$S(x) = W_o h$。通过重新排列 $S_k^*(x)$ 的表达式，我们可以确定输出权重矩阵 $W_o$ 中的权重。\n    $$\n    S_k^*(x) = (2c_{k1})\\mathrm{ReLU}(x_1) + (-2c_{k1})\\mathrm{ReLU}(-x_1) + (2c_{k2})\\mathrm{ReLU}(x_2) + (-2c_{k2})\\mathrm{ReLU}(-x_2) + (-\\|c_k\\|_2^2) \\cdot 1\n    $$\n    这对应于一个权重向量和隐藏激活向量 $h$ 的点积。因此，输出权重矩阵 $W_o$ 的第 $k$ 行为：\n    $$\n    (W_o)_k = [2c_{k1}, -2c_{k1}, 2c_{k2}, -2c_{k2}, -\\|c_k\\|_2^2]\n    $$\n\n这个构造为计算分数 $S_k(x) = S_k^*(x)$ 的单隐藏层 ReLU 网络提供了精确的权重和偏置。最终的聚类分配是 $\\mathrm{argmax}_k S_k(x)$，这等价于最近中心法则。在实现中，平局打破条件（选择最小索引）可以由标准的 `numpy.argmin` 和 `numpy.argmax` 函数自然地处理。\n\n推导到此结束。实现将遵循此解析构造。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and verifies a one-hidden-layer ReLU network for k-means nearest-center assignment.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        (\n            [(0, 0), (2, 0)],\n            [(-1, 0), (0, 0), (0.9, 0), (1.0, 0), (1.1, 0), (3, 0), (2, 1)],\n        ),\n        # Test case 2\n        (\n            [(0, 0), (2, 0), (1, 2)],\n            [(0.1, 0.2), (2.1, -0.1), (1.0, 1.8), (0.9, 0.9), (1.1, 1.1)],\n        ),\n        # Test case 3\n        (\n            [(0, 0), (2, 0)],\n            [(1, 0), (1, 2), (1, -2)],\n        ),\n        # Test case 4\n        (\n            [(0, 0), (0, 0), (2, 0)],\n            [(-1, 0), (0, 0), (1, 0), (2, 0)],\n        ),\n        # Test case 5\n        (\n            [(3, 3)],\n            [(-10, -10), (0, 0), (3, 3), (5, 5)],\n        )\n    ]\n\n    results = []\n\n    for centers_list, points_list in test_cases:\n        centers = np.array(centers_list, dtype=np.float64)\n        points = np.array(points_list, dtype=np.float64)\n        \n        num_points = points.shape[0]\n        if num_points == 0:\n            results.append(1.0)\n            continue\n            \n        num_matches = 0\n\n        # Construct the output weight matrix W_o for the ReLU network\n        # W_o has shape (K, 5) where K is the number of clusters.\n        # The k-th row is [2*c_k1, -2*c_k1, 2*c_k2, -2*c_k2, -||c_k||^2]\n        num_clusters = centers.shape[0]\n        W_o = np.zeros((num_clusters, 5), dtype=np.float64)\n        for k in range(num_clusters):\n            c_k1, c_k2 = centers[k, 0], centers[k, 1]\n            W_o[k, 0] = 2 * c_k1\n            W_o[k, 1] = -2 * c_k1\n            W_o[k, 2] = 2 * c_k2\n            W_o[k, 3] = -2 * c_k2\n            W_o[k, 4] = -(c_k1**2 + c_k2**2)\n            \n        for point in points:\n            x1, x2 = point[0], point[1]\n\n            # 1. Nearest-center rule (k-means)\n            # Calculate squared Euclidean distances: ||x - c_k||^2\n            dist_sq = np.sum((point - centers)**2, axis=1)\n            # Find index of minimum distance. np.argmin breaks ties by choosing the smallest index.\n            kmeans_idx = np.argmin(dist_sq)\n\n            # 2. ReLU network assignment\n            # Hidden layer activations h = [ReLU(x1), ReLU(-x1), ReLU(x2), ReLU(-x2), 1]\n            h = np.array([\n                max(0, x1),\n                max(0, -x1),\n                max(0, x2),\n                max(0, -x2),\n                1.0\n            ])\n            \n            # Output layer scores S = W_o @ h\n            scores = W_o @ h\n            # Find index of maximum score. np.argmax breaks ties by choosing the smallest index.\n            relu_net_idx = np.argmax(scores)\n\n            if kmeans_idx == relu_net_idx:\n                num_matches += 1\n        \n        accuracy = float(num_matches) / float(num_points)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3167799"}, {"introduction": "在理解了 ReLU 网络能表示什么之后，我们将转向如何使用梯度下降法来训练它们。ReLU 函数在零点处有一个不可微的“拐点”，这给基于梯度的优化方法带来了挑战。本练习旨在研究在这一点上选择不同次梯度（$0$、$1$ 或随机选择）所带来的实际影响。通过模拟不同次梯度策略下的训练动态，您将深入了解自动微分（autograd）系统中的一个微妙但关键的方面，并理解为何一个看似微小的实现细节会影响模型的收敛，尤其是在特定的边界情况下。[@problem_id:3167839]", "problem": "要求你研究在梯度下降法下，修正线性单元（ReLU）激活函数在其不可微点处选择不同的次梯度将如何影响训练动态。修正线性单元（ReLU）是函数 $\\phi(z)$，其逐点定义为 $0$ 和 $z$ 的最大值。考虑一个带单个隐藏激活的标量线性模型，其中预激活为 $z = w x + b$，激活为 $\\hat{y} = \\phi(z)$，且在数据集上的损失为均方误差。使用初等微积分中可微函数复合的链式法则以及全批量梯度下降的定义，来推导参数更新所需的梯度表达式。对于在 $z = 0$ 处的次梯度，使用以下策略之一：\n- 策略 A：在 $z = 0$ 处始终使用次梯度值 $0$。\n- 策略 B：在 $z = 0$ 处始终使用次梯度值 $1$。\n- 策略 C：每当出现 $z = 0$ 时，以相等的概率 $1/2$ 独立地选择 $0$ 或 $1$。\n\n实现一个程序，对上述模型模拟全批量梯度下降。对每个测试用例，运行三个训练过程，每个策略（A、B、C）一个，每个过程都使用固定的步数、固定的学习率和固定的初始参数。为每个策略计算训练结束后的最终均方误差损失。必须通过使用为该测试用例提供的随机种子，来确保策略 C 中的随机选择是可复现的。\n\n你的推导和实现必须仅基于以下基础元素：\n- 修正线性单元（ReLU）$\\phi(z)$ 的定义。\n- 复合函数求导的链式法则。\n- 用于最小化均方误差的全批量梯度下降的定义。\n\n不要假设或使用超出这些定义范围的任何未经证明的捷径或专门公式。严格按照这些基础所隐含的方式实现梯度下降更新。\n\n测试套件和参数：\n- 测试用例 1 (边界情况，除非 $b$ 移动，否则 $z = 0$ 会持续存在): 数据集 $\\{(x, y)\\} = \\{(0.0, 1.0)\\}$，初始 $w = 0.0$，初始 $b = 0.0$，学习率 $\\alpha = 0.1$，步数 $T = 50$，随机种子 $s = 42$。\n- 测试用例 2 (混合输入，所有样本初始时 $z = 0$): 数据集 $\\{(x, y)\\} = \\{(-1.0, 0.0), (1.0, 1.0)\\}$，初始 $w = 0.0$，初始 $b = 0.0$，学习率 $\\alpha = 0.1$，步数 $T = 200$，随机种子 $s = 123$。\n- 测试用例 3 (一般情况， $z = 0$ 不太可能出现，因此各策略结果应一致): 数据集 $\\{(x, y)\\} = \\{(1.0, 2.0), (2.0, 4.0), (3.0, 6.0), (-1.0, 0.0)\\}$，初始 $w = 0.1$，初始 $b = 0.11$，学习率 $\\alpha = 0.01$，步数 $T = 1000$，随机种子 $s = 2024$。\n- 测试用例 4 (目标全为零的平凡精确拟合情况): 数据集 $\\{(x, y)\\} = \\{(0.0, 0.0)\\}$，初始 $w = 0.0$，初始 $b = 0.0$，学习率 $\\alpha = 0.1$，步数 $T = 50$，随机种子 $s = 7$。\n\n对于每个测试用例，按 [策略 A, 策略 B, 策略 C] 的顺序报告最终损失，每个值四舍五入到六位小数。你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身是代表一个测试用例的、用方括号括起来的逗号分隔三元组。例如，整体格式必须是：\n\"[[L_A1,L_B1,L_C1],[L_A2,L_B2,L_C2],[L_A3,L_B3,L_C3],[L_A4,L_B4,L_C4]]\"\n其中 $L\\_\\mathrm{A1}$ 表示策略 A 在测试用例 1 上的最终损失，以此类推。不应打印任何额外文本。", "solution": "问题的核心是为一个简单的标量模型推导并实现全批量梯度下降的更新规则。模型对输入 $x$ 的预测 $\\hat{y}$ 由 $\\hat{y} = \\phi(wx+b)$ 给出，其中 $\\phi$ 是 ReLU 函数。在包含 $N$ 个样本 $\\{(x_i, y_i)\\}_{i=1}^N$ 的数据集上，损失函数 $L$ 是均方误差：\n$$\nL(w, b) = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2 = \\frac{1}{N} \\sum_{i=1}^N (\\phi(wx_i+b) - y_i)^2\n$$\n\n参数 $w$ 和 $b$ 通过梯度下降进行更新：\n$$\nw_{t+1} = w_t - \\alpha \\frac{\\partial L}{\\partial w}\n$$\n$$\nb_{t+1} = b_t - \\alpha \\frac{\\partial L}{\\partial b}\n$$\n其中 $\\alpha$ 是学习率。\n\n为了求出偏导数 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial b}$，我们应用链式法则。设 $L_i = (\\hat{y}_i - y_i)^2$ 为单个样本的损失，$z_i = wx_i+b$ 为预激活。总损失梯度是各个样本梯度的平均值：\n$$\n\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial w}\n\\quad \\text{和} \\quad\n\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial L_i}{\\partial b}\n$$\n\n对于每个样本 $i$，链式法则给出：\n$$\n\\frac{\\partial L_i}{\\partial w} = \\frac{\\partial L_i}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w}\n$$\n$$\n\\frac{\\partial L_i}{\\partial b} = \\frac{\\partial L_i}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial b}\n$$\n\n我们来计算每个部分：\n1.  平方误差对预测值的导数是：\n    $$\n    \\frac{\\partial L_i}{\\partial \\hat{y}_i} = 2(\\hat{y}_i - y_i) = 2(\\phi(z_i) - y_i)\n    $$\n2.  线性预激活 $z_i = wx_i+b$ 的导数是：\n    $$\n    \\frac{\\partial z_i}{\\partial w} = x_i\n    $$\n    $$\n    \\frac{\\partial z_i}{\\partial b} = 1\n    $$\n3.  ReLU 激活函数 $\\hat{y}_i = \\phi(z_i) = \\max(0, z_i)$ 的导数是：\n    $$\n    \\phi'(z_i) = \\frac{d\\phi}{dz_i} = \\begin{cases} 1  &\\text{if } z_i > 0 \\\\ 0  &\\text{if } z_i < 0 \\end{cases}\n    $$\n    在 $z_i = 0$ 处，该函数不可微。其在此处的次微分是区间 $[0, 1]$。问题陈述提供了三种不同的策略来在此点选择一个次梯度 $g \\in [0, 1]$：\n    -   策略 A：$g = 0$\n    -   策略 B：$g = 1$\n    -   策略 C：从 $\\{0, 1\\}$ 中以 $1/2$ 的概率选择 $g$。\n    为方便表示，即使在 $z_i=0$ 处，我们也将所选的次梯度记为 $\\phi'(z_i)$。\n\n综合这些部分，单个样本 $i$ 的梯度是：\n$$\n\\frac{\\partial L_i}{\\partial w} = 2(\\phi(z_i) - y_i) \\cdot \\phi'(z_i) \\cdot x_i\n$$\n$$\n\\frac{\\partial L_i}{\\partial b} = 2(\\phi(z_i) - y_i) \\cdot \\phi'(z_i) \\cdot 1\n$$\n\n全批量梯度是所有 $N$ 个样本的平均值：\n$$\n\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N 2(\\phi(wx_i+b) - y_i) \\phi'(wx_i+b) x_i\n$$\n$$\n\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N 2(\\phi(wx_i+b) - y_i) \\phi'(wx_i+b)\n$$\n\n模拟过程首先初始化 $w$ 和 $b$，并在固定的步数内，反复计算这些梯度（根据为 $\\phi'(0)$ 指定的策略）并更新参数。对于策略 C，在每一步中，对每个出现 $z_i=0$ 的样本，都会使用为每个测试用例设好种子的可复现随机数生成器，为 $\\phi'(0)$ 进行一次新的随机选择。在最后一步之后，计算总损失 $L(w,b)$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'dataset': [(0.0, 1.0)], 'w': 0.0, 'b': 0.0, 'alpha': 0.1, 'steps': 50, 'seed': 42},\n        {'dataset': [(-1.0, 0.0), (1.0, 1.0)], 'w': 0.0, 'b': 0.0, 'alpha': 0.1, 'steps': 200, 'seed': 123},\n        {'dataset': [(1.0, 2.0), (2.0, 4.0), (3.0, 6.0), (-1.0, 0.0)], 'w': 0.1, 'b': 0.11, 'alpha': 0.01, 'steps': 1000, 'seed': 2024},\n        {'dataset': [(0.0, 0.0)], 'w': 0.0, 'b': 0.0, 'alpha': 0.1, 'steps': 50, 'seed': 7},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        case_results = []\n        for policy in ['A', 'B', 'C']:\n            loss = run_training(\n                policy=policy,\n                dataset=case['dataset'],\n                w_init=case['w'],\n                b_init=case['b'],\n                alpha=case['alpha'],\n                steps=case['steps'],\n                seed=case['seed']\n            )\n            case_results.append(loss)\n        all_results.append(case_results)\n\n    # Format the final output string exactly as required.\n    formatted_cases = []\n    for res in all_results:\n        formatted_cases.append(f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\ndef run_training(policy, dataset, w_init, b_init, alpha, steps, seed):\n    \"\"\"\n    Simulates full-batch gradient descent for a given policy and parameters.\n    \n    Args:\n        policy (str): The subgradient policy ('A', 'B', or 'C').\n        dataset (list): The list of (x, y) data points.\n        w_init (float): Initial weight.\n        b_init (float): Initial bias.\n        alpha (float): Learning rate.\n        steps (int): Number of training steps.\n        seed (int): Random seed for Policy C.\n        \n    Returns:\n        float: The final average squared error loss.\n    \"\"\"\n    x_data = np.array([p[0] for p in dataset], dtype=np.float64)\n    y_data = np.array([p[1] for p in dataset], dtype=np.float64)\n    n_samples = len(x_data)\n    \n    w = float(w_init)\n    b = float(b_init)\n    \n    # Use a RandomState object for reproducible randomness in Policy C.\n    rng = np.random.RandomState(seed) if policy == 'C' else None\n\n    for _ in range(steps):\n        # Forward pass\n        z = w * x_data + b\n        y_hat = np.maximum(0, z)\n        \n        # Calculate the derivative of ReLU, phi_prime(z), based on the policy.\n        # Initialize with the cases for z  0 and z > 0.\n        phi_prime = np.zeros_like(z, dtype=np.float64)\n        phi_prime[z > 0] = 1.0\n        \n        # Handle the non-differentiable point z = 0.\n        zero_indices = np.where(z == 0)[0]\n        if len(zero_indices) > 0:\n            if policy == 'A':\n                # For Policy A, phi_prime(0) is 0, which is the default.\n                pass\n            elif policy == 'B':\n                # For Policy B, phi_prime(0) is 1.\n                phi_prime[zero_indices] = 1.0\n            elif policy == 'C':\n                # For Policy C, choose 0 or 1 with equal probability.\n                for idx in zero_indices:\n                    phi_prime[idx] = rng.choice([0, 1])\n\n        # Calculate gradients using the Chain Rule.\n        # The common term in the derivative is 2 * (y_hat - y) * phi_prime.\n        delta = 2 * (y_hat - y_data) * phi_prime\n        \n        # Full-batch gradients are the average over the dataset.\n        grad_w = np.mean(delta * x_data)\n        grad_b = np.mean(delta)\n        \n        # Update parameters with gradient descent.\n        w -= alpha * grad_w\n        b -= alpha * grad_b\n        \n    # After training, calculate the final loss.\n    final_z = w * x_data + b\n    final_y_hat = np.maximum(0, final_z)\n    final_loss = np.mean((final_y_hat - y_data)**2)\n    \n    return final_loss\n\nsolve()\n```", "id": "3167839"}, {"introduction": "除了基本理论，激活函数的选择还具有重大的工程意义，尤其是在资源受限的硬件上部署模型时。本问题将在模型量化（即将模型权重和激活值转换为较低精度数值的过程）的背景下，比较标准 ReLU 和其流行变体 ReLU6。通过分析动态范围、量化误差和梯度饱和之间的权衡，您将学习像机器学习工程师一样思考，在模型精度和部署效率之间寻求平衡。这个练习突显了像 ReLU6 这样的变体是如何被设计出来以解决实际工程问题的。[@problem_id:3167884]", "problem": "考虑一个深度神经网络中的前馈层，其预激活（pre-activation）是一个实值变量 $x \\in \\mathbb{R}$。在不同的实验中使用了两种激活函数：修正线性单元（ReLU），定义为 $f(x) = \\max(0, x)$，以及修正线性单元 $6$（ReLU6），定义为 $f_6(x) = \\min(\\max(0, x), 6)$。假设激活值使用无符号 $8$ 位整型均匀量化，其仿射映射的选择方式是让一个小批量（minibatch）中的最大激活值来设定动态范围。具体来说，对于一个激活值 $y$，其量化编码为 $q = \\mathrm{round}\\left(\\dfrac{255}{R} \\, y\\right)$，其中 $R$ 是该小批量中 $y$ 的最大值，反量化后的重建值为 $\\hat{y} = \\dfrac{R}{255} \\, q$。假设在一个代表性的小批量中，使用普通 ReLU 时，由于偶尔出现较大的输入，激活值分布有 $99\\%$ 的值在 $[0, 6]$ 区间内，剩下的 $1\\%$ 的值在 $(6, 60]$ 区间内；而使用 ReLU6 时，根据定义，所有激活值都在 $[0, 6]$ 区间内。从第一性原理分析这对量化精度和反向传播梯度的影响。\n\n选择在此场景下所有正确的陈述：\n\nA. 在使用 ReLU 且 $R = 60$ 的情况下，量化步长为 $\\Delta = \\dfrac{60}{255} \\approx 0.235$；而在使用 ReLU6 且 $R = 6$ 的情况下，步长为 $\\Delta = \\dfrac{6}{255} \\approx 0.0235$，这将主体部分的均方量化误差大致减小了 $100$ 倍。\n\nB. 对于输入 $x  0$，ReLU6 会导致所有梯度都为零，从而在 $x$ 为正时完全阻断梯度流。\n\nC. ReLU6 关于 $x$ 的导数在 $0  x  6$ 时等于 $1$，在 $x \\le 0$ 或 $x \\ge 6$ 时等于 $0$；因此，与 ReLU 相比，ReLU6 在 $x$ 较大时引入了额外的饱和区。\n\nD. 对于固定的位深度，ReLU6 必然会增加整个小批量的平均量化误差，因为裁剪（clipping）总是比离群值引起的较大步长更糟糕。\n\nE. ReLU6 在 $6$ 处的裁剪会通过限制高于 $6$ 的值来引入偏差（bias），但它同时缩小了动态范围，使得对于集中在 $6$ 以下的大多数激活值，张量级（per-tensor）均匀量化更加精确。", "solution": "分析包括两个主要部分：对量化精度的影响和对反向传播梯度的影响。\n\n**第一部分：量化精度分析**\n\n量化方案是将实值范围 $[0, R]$ 均匀仿射映射到 $2^8 = 256$ 个整数级别（$0, 1, \\dots, 255$）。量化步长（或称缩放因子）是由一个整数级别所代表的区间大小。\n$$\n\\Delta = \\dfrac{\\text{动态范围}}{\\text{步数}} = \\dfrac{R}{255}\n$$\n反量化值为 $\\hat{y} = \\Delta \\cdot q$。对于一个未被裁剪的值 $y$，量化误差的界限为 $|y - \\hat{y}| \\le \\frac{\\Delta}{2}$。假设误差均匀分布，量化引起的均方误差（MSE）（也称为颗粒误差），与 $\\Delta^2$ 成正比。$\\text{MSE}_{\\text{quant}} \\approx \\dfrac{\\Delta^2}{12}$。\n\n-   **情况 1：ReLU 激活**\n    问题陈述指出，离群值导致最大激活值为 $R = 60$。\n    量化步长为：\n    $$\n    \\Delta_{\\text{ReLU}} = \\dfrac{60}{255} \\approx 0.2353\n    $$\n\n-   **情况 2：ReLU6 激活**\n    ReLU6 函数内在地将最大激活值裁剪到 $6$。因此，可能的最大值为 $R = 6$。\n    量化步长为：\n    $$\n    \\Delta_{\\text{ReLU6}} = \\dfrac{6}{255} \\approx 0.02353\n    $$\n\n**比较：** ReLU 情况下的量化步长恰好是 ReLU6 情况下的 $10$ 倍（$\\Delta_{\\text{ReLU}} = 10 \\cdot \\Delta_{\\text{ReLU6}}$）。对于位于 $[0, 6]$ 区间内的 $99\\%$ 的激活值，使用 ReLU6 会得到更精细的量化。这些值的 MSE 将减小 $(\\Delta_{\\text{ReLU}}/\\Delta_{\\text{ReLU6}})^2 = 10^2 = 100$ 倍。然而，对于那 $1\\%$ 的、本应产生大于 $6$ 的激活值的预激活，ReLU6 引入了裁剪误差（偏差），因为这些值都被映射到了 $6$。\n\n**第二部分：梯度分析**\n\n激活函数相对于其输入 $x$ 的梯度对于反向传播至关重要。让我们用 $L$ 表示损失函数。通过激活函数反向传播的梯度是 $\\dfrac{\\partial L}{\\partial x} = \\dfrac{\\partial L}{\\partial f} \\dfrac{\\partial f}{\\partial x}$。我们需要分析 $\\dfrac{\\partial f}{\\partial x}$。\n\n-   **ReLU 梯度：** $f(x) = \\max(0, x)$\n    导数为：\n    $$\n    \\dfrac{df}{dx} = \\begin{cases} 1  \\text{if } x  0 \\\\ 0  \\text{if } x  0 \\end{cases}\n    $$\n    （在 $x=0$ 处，函数不可导，但在实践中通常使用子梯度，典型值为 $0$）。对于任何正输入，梯度以因子 $1$ 回传。\n\n-   **ReLU6 梯度：** $f_6(x) = \\min(\\max(0, x), 6)$\n    这可以分段写成：\n    $$\n    f_6(x) = \\begin{cases} 0  \\text{if } x \\le 0 \\\\ x  \\text{if } 0  x  6 \\\\ 6  \\text{if } x \\ge 6 \\end{cases}\n    $$\n    导数为：\n    $$\n    \\dfrac{df_6}{dx} = \\begin{cases} 0  \\text{if } x \\le 0 \\text{ or } x \\ge 6 \\\\ 1  \\text{if } 0  x  6 \\end{cases}\n    $$\n    （同样，我们忽略不可导点 $x=0$ 和 $x=6$，在这些点梯度通常设为 $0$）。与 ReLU 相比，ReLU6 在 $x \\ge 6$ 时引入了一个零梯度区域。这是一种饱和形式。当预激活 $x$ 是大的正数时，ReLU 允许梯度流动，而 ReLU6 则完全阻止它。这可以防止激活值变得过大，但也会阻碍在该饱和区域工作的神经元的学习。\n\n### 逐项分析\n\n**A. 在使用 ReLU 且 $R = 60$ 的情况下，量化步长为 $\\Delta = \\dfrac{60}{255} \\approx 0.235$；而在使用 ReLU6 且 $R = 6$ 的情况下，步长为 $\\Delta = \\dfrac{6}{255} \\approx 0.0235$，这将主体部分的均方量化误差大致减小了 $100$ 倍。**\n如上所述，ReLU 的量化步长为 $\\Delta_{\\text{ReLU}} = \\frac{60}{255} \\approx 0.235$。对于 ReLU6，它是 $\\Delta_{\\text{ReLU6}} = \\frac{6}{255} \\approx 0.0235$。计算是正确的。“主体部分”（bulk）指的是在 $[0, 6]$ 内的 $99\\%$ 的激活值。对于这部分，均方量化误差（MSE）与步长的平方 $\\Delta^2$ 成正比。这部分数据的 MSE 之比为 $\\frac{\\text{MSE}_{\\text{ReLU}}}{\\text{MSE}_{\\text{ReLU6}}} \\approx \\frac{\\Delta_{\\text{ReLU}}^2}{\\Delta_{\\text{ReLU6}}^2} = \\left(\\frac{60/255}{6/255}\\right)^2 = 10^2 = 100$。因此，主体部分的 MSE 大约减少了 $100$ 倍。该陈述完全正确。\n**结论：正确**\n\n**B. 对于输入 $x  0$，ReLU6 会导致所有梯度都为零，从而在 $x$ 为正时完全阻断梯度流。**\n在梯度分析中推导得出，对于区间 $(0, 6)$ 内的输入 $x$，ReLU6 的导数 $\\dfrac{df_6}{dx} = 1$。梯度仅在 $x \\le 0$ 和 $x \\ge 6$ 时为零。陈述中说对于*所有* $x  0$ 梯度都为零是错误的。\n**结论：错误**\n\n**C. ReLU6 关于 $x$ 的导数在 $0  x  6$ 时等于 $1$，在 $x \\le 0$ 或 $x \\ge 6$ 时等于 $0$；因此，与 ReLU 相比，ReLU6 在 $x$ 较大时引入了额外的饱和区。**\n陈述的第一部分正确描述了 ReLU6 的导数，如上面的分析所示（忽略了不可导点）。第二部分将其与 ReLU 进行了比较，ReLU 的导数对于所有 $x  0$ 都为 $1$。对于 ReLU6，当 $x \\ge 6$ 时梯度变为 $0$。对于大输入值梯度为零是饱和的定义。因此，ReLU6 引入了一个标准 ReLU 中不存在的额外饱和区域。推理和结论都是正确的。\n**结论：正确**\n\n**D. 对于固定的位深度，ReLU6 必然会增加整个小批量的平均量化误差，因为裁剪（clipping）总是比离群值引起的较大步长更糟糕。**\n这个选项声称使用 ReLU6 的总误差*必然*更高。总误差是裁剪误差（针对 $1\\%$ 的离群值）和量化误差（针对所有值）的组合。虽然 ReLU6 对离群值引入了大的裁剪误差，但它极大地减少了 $99\\%$ 主体数据的量化误差。陈述中的绝对词“必然”（necessarily）和“总是”（always）过于强烈。在某些情况下，由裁剪引入的误差可能小于由大动态范围引起的颗粒误差的增加。它取决于离群值的分布和幅度。由于“必然”一词，该陈述是错误的。\n**结论：错误**\n\n**E. ReLU6 在 $6$ 处的裁剪会通过限制高于 $6$ 的值来引入偏差（bias），但它同时缩小了动态范围，使得对于集中在 $6$ 以下的大多数激活值，张量级（per-tensor）均匀量化更加精确。**\n这个陈述描述了在量化背景下使用 ReLU6 的基本权衡。\n-   “ReLU6 在 $6$ 处的裁剪会通过限制高于 $6$ 的值来引入偏差（bias）”：正确。对于一个输入 $x$ 使得 $f(x)6$，输出被强制为 $6$，引入了系统性误差（偏差）。\n-   “它同时缩小了动态范围”：正确。量化的动态范围从 $[0, 60]$ 变为 $[0, 6]$。\n-   “使得对于集中在 $6$ 以下的大多数激活值，张量级（per-tensor）均匀量化更加精确”：正确。将动态范围从 $R=60$ 缩小到 $R=6$ 使量化步长 $\\Delta$ 减小了十倍，从而提高了该范围内值的精度，而这些值构成了数据的大多数（$99\\%$）。\n该陈述准确地总结了情况。\n**结论：正确**", "answer": "$$\\boxed{ACE}$$", "id": "3167884"}]}