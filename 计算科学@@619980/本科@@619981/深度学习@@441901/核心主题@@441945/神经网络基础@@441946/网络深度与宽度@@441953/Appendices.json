{"hands_on_practices": [{"introduction": "神经网络的宽度如何影响其表示能力？一个网络若要能够拟合复杂的数据，其内部特征表示必须足够“丰富”。这个练习 [@problem_id:3157479] 提供了一种动手的方式，通过计算特征矩阵的秩来量化这种丰富性，让你亲身体验网络宽度与函数空间表达能力之间的直接联系。", "problem": "您的任务是为一个完全指定的深度网络设计并实现一个受控的宽度剪枝实验，以确定一个最小宽度阈值。当网络深度固定且仅最后一层线性读出层可训练时，低于此阈值，网络所能实现的函数空间将失去在固定的有限数据集上插值任意标签的能力。该实验必须遵循线性代数的第一性原理和精确的架构定义。所有数学符号、函数、运算符和数字都必须用 LaTeX 书写。\n\n考虑以下设定。假设有一个数据集，包含 $m$ 个输入样本，表示为矩阵 $X \\in \\mathbb{R}^{m \\times d}$。考虑一个深度为 $L$ 的前馈网络，它有 $L$ 个隐藏层和一个单一的线性输出神经元，其中只有最终线性读出层的权重是可训练的，所有隐藏层的参数都是固定的。经过 $L$ 层后的隐藏表示记为 $H \\in \\mathbb{R}^{m \\times w}$，其中 $w$ 是每个隐藏层共享的宽度（通道数）。当仅训练最后一层来拟合标签向量 $y \\in \\mathbb{R}^m$ 时，最终的网络输出对应于求解一个关于特征 $H$ 的线性系统。因此，要在 $m$ 个样本上插值任意标签，要求 $H$ 的列空间维度为 $m$；等价地，$\\mathrm{rank}(H) = m$。\n\n架构。隐藏层的定义如下：\n- 第一个隐藏层：$h^{(1)} = \\mathrm{ReLU}(X A + \\mathbf{1} b^{(1)\\top})$，其中 $A \\in \\mathbb{R}^{d \\times W_0}$ 是初始的第一层权重矩阵，$b^{(1)} \\in \\mathbb{R}^{W_0}$ 是偏置向量，$\\mathbf{1} \\in \\mathbb{R}^{m}$ 是全为 1 的向量。\n- 对于 $\\ell \\in \\{2,\\dots,L\\}$ 的后续隐藏层：$h^{(\\ell)} = \\mathrm{ReLU}\\!\\left(h^{(\\ell-1)} \\odot s^{(\\ell)} + \\mathbf{1} b^{(\\ell)\\top}\\right)$，其中 $s^{(\\ell)} \\in \\mathbb{R}^{W_0}$ 是一个按通道的缩放向量，$b^{(\\ell)} \\in \\mathbb{R}^{W_0}$ 是一个按通道的偏置向量，$\\odot$ 表示在 $m$ 个样本上广播的逐元素乘法。\n- 最终特征矩阵是 $H = h^{(L)} \\in \\mathbb{R}^{m \\times w}$，它是经过如下定义的结构化宽度剪枝到宽度 $w$ 之后得到的。\n\n宽度剪枝协议。您必须实现一个宽度剪枝过程，该过程使用根据固定的隐藏参数计算出的重要性度量，从初始宽度 $W_0$ 中选择一个大小为 $w$ 的通道子集。通道 $j \\in \\{1,\\dots,W_0\\}$ 的重要性定义为\n$$\nI_j \\;=\\; \\left\\|A_{:,j}\\right\\|_2 \\,\\times\\, \\prod_{\\ell=2}^{L}\\left|s^{(\\ell)}_j\\right| ,\n$$\n约定当 $L = 1$ 时空积等于 $1$。要剪枝到宽度 $w$，选择具有最大 $I_j$ 值的 $w$ 个通道并保留这些索引，同时保持它们的原始顺序。设选定的索引集为 $S_w \\subset \\{1,\\dots,W_0\\}$，其中 $|S_w| = w$。剪枝后的隐藏参数则为 $A_{:,S_w}$、$b^{(1)}_{S_w}$，以及对于 $\\ell \\in \\{2,\\dots,L\\}$ 的向量 $s^{(\\ell)}_{S_w}$ 和 $b^{(\\ell)}_{S_w}$。使用这些剪枝后的参数应用上述前向传播定义，以生成 $H_w \\in \\mathbb{R}^{m \\times w}$。只有最后的线性读出层（不属于此计算部分）会被训练；因此，通过仅改变最后一层在 $m$ 个数据点上可实现的函数空间是 $H_w$ 的列空间，其维度为 $\\mathrm{rank}(H_w)$。\n\n坍塌阈值。将坍塌阈值宽度 $w^\\star$ 定义为满足 $\\mathrm{rank}(H_w) = m$ 的最小宽度 $w \\in \\{1,\\dots,W_0\\}$。如果不存在这样的 $w$，则定义 $w^\\star = -1$。直观上，对于任何 $w  w^\\star$，函数空间会发生坍塌，即当仅训练最后一层时，它无法在 $m$ 个样本上插值任意标签。\n\n数据和参数生成。对于每个测试用例，使用指定的整数种子初始化的伪随机数生成器，并从均值为零、方差为一的标准正态分布中独立抽取所有条目：\n- $X \\in \\mathbb{R}^{m \\times d}$，\n- $A \\in \\mathbb{R}^{d \\times W_0}$，\n- $b^{(1)} \\in \\mathbb{R}^{W_0}$，\n- 对于每个 $\\ell \\in \\{2,\\dots,L\\}$：$s^{(\\ell)} \\in \\mathbb{R}^{W_0}$ 和 $b^{(\\ell)} \\in \\mathbb{R}^{W_0}$。\n\n秩的计算。给定 $H_w \\in \\mathbb{R}^{m \\times w}$，使用标准的奇异值分解定义和一个数值稳定的阈值来计算 $\\mathrm{rank}(H_w)$。您可以使用一个基于奇异值分解的、经过良好测试的用于计算矩阵秩的数值线性代数例程。\n\n任务。实现一个程序，对于下面的每个测试用例，该程序会构建数据和参数，对所有候选宽度 $w \\in \\{1,2,\\dots,W_0\\}$ 执行剪枝，计算每个 $w$ 对应的 $\\mathrm{rank}(H_w)$，并返回如上定义的阈值 $w^\\star$。\n\n测试套件。使用以下四个测试用例，每个用例以元组 $(L, d, m, W_0, \\text{seed})$ 的形式给出：\n- 用例 1：$(L, d, m, W_0, \\text{seed}) = (2, 8, 6, 16, 1)$。\n- 用例 2：$(L, d, m, W_0, \\text{seed}) = (4, 8, 6, 12, 2)$。\n- 用例 3（边缘情况，其中 $W_0  m$）：$(L, d, m, W_0, \\text{seed}) = (3, 8, 10, 7, 3)$。\n- 用例 4（边界情况，其中 $L=1$）：$(L, d, m, W_0, \\text{seed}) = (1, 6, 5, 5, 4)$。\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含用方括号括起来的结果，结果为逗号分隔的列表，顺序与上述测试套件相同。例如，如果阈值是 $a_1,a_2,a_3,a_4$，则需精确打印单行内容\n$[a_1,a_2,a_3,a_4]$。\n\n本问题中不涉及物理单位、角度单位和百分比。所有输出均为整数，其中值 $-1$ 保留用以指示对于给定的测试用例，没有宽度能够使得 $\\mathrm{rank}(H_w)=m$。", "solution": "问题陈述已经过分析，并被确定为是有效的。它在科学上基于神经网络容量理论，在数学上是适定的，并为要执行的计算实验提供了完整、客观且可形式化的描述。所有术语都有明确定义，并且为每个测试用例指定了所需的参数。\n\n该解决方案通过为每个测试用例实现指定的计算实验来展开。目标是找到坍塌阈值宽度 $w^\\star$，其定义为使得最终隐藏表示矩阵 $H_w$ 的秩等于数据样本数 $m$ 的最小宽度 $w \\in \\{1, \\dots, W_0\\}$。如果不存在这样的宽度，则 $w^\\star = -1$。\n\n对于单个测试用例 $(L, d, m, W_0, \\text{seed})$ 的整体流程如下：\n\n1.  **参数生成**：使用提供的 `seed` 初始化一个伪随机数生成器。数据矩阵 $X \\in \\mathbb{R}^{m \\times d}$ 和所有固定的网络参数——$A \\in \\mathbb{R}^{d \\times W_0}$，$b^{(1)} \\in \\mathbb{R}^{W_0}$，以及对于每个层 $\\ell \\in \\{2, \\dots, L\\}$ 的缩放向量和偏置向量 $s^{(\\ell)}, b^{(\\ell)} \\in \\mathbb{R}^{W_0}$——都从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取。\n\n2.  **重要性分数计算**：根据以下公式计算 $W_0$ 个初始通道中每一个（由 $j \\in \\{1, \\dots, W_0\\}$ 索引）的重要性 $I_j$：\n    $$\n    I_j = \\left\\|A_{:,j}\\right\\|_2 \\times \\prod_{\\ell=2}^{L}\\left|s^{(\\ell)}_j\\right|\n    $$\n    此处，$A_{:,j}$ 是第一层权重矩阵 $A$ 的第 $j$ 列。L2 范数 $\\| \\cdot \\|_2$ 衡量连接输入特征与第 $j$ 个通道的权重的大小。乘积项聚合了所有后续层中按通道的缩放因子的大小。对于 $L=1$ 的情况，该乘积为空，并按惯例定义为 $1$。计算并存储所有 $W_0$ 个通道的重要性分数。\n\n3.  **通道剪枝顺序**：根据重要性分数 $I_j$ 对通道进行降序排名。这决定了在剪枝网络中考虑包含哪些通道的顺序。具体来说，我们获得一个从最重要到最不重要的通道索引 $\\{1, \\dots, W_0\\}$ 的有序列表。\n\n4.  **搜索坍塌阈值 $w^\\star$**：任务的核心是找到满足秩条件的最小宽度 $w$。\n    -   矩阵秩的一个基本性质是 $\\mathrm{rank}(H_w) \\le \\min(m, w)$。要满足条件 $\\mathrm{rank}(H_w) = m$，必须有 $w \\ge m$。因此，如果最大可能宽度 $W_0$ 小于样本数 $m$，则不可能存在解。在这种情况下，根据定义 $w^\\star = -1$。\n    -   如果 $W_0 \\ge m$，我们搜索满足条件的最小 $w$。搜索过程通过按升序测试宽度 $w$ 来进行。由于任何宽度 $w  m$ 都不能满足条件，搜索可以高效地从 $w=m$ 开始，一直进行到 $W_0$。\n    -   对于每个候选宽度 $w \\in \\{m, m+1, \\dots, W_0\\}$：\n        a.  **选择通道**：选择具有最高重要性分数的 $w$ 个通道。设它们的原始索引为集合 $S_w$。为了保留原始的通道编号，这些索引在使用前会按升序排序，然后用于对参数张量进行切片。\n        b.  **构建剪枝网络**：通过从原始参数张量中选择与 $S_w$ 中索引对应的列和元素，来构成宽度为 $w$ 的剪枝网络的参数：$A_{:,S_w}$、$b^{(1)}_{S_w}$、$s^{(\\ell)}_{S_w}$ 和 $b^{(\\ell)}_{S_w}$。\n        c.  **前向传播**：将输入数据 $X$ 通过剪枝网络传播，以计算最终的隐藏表示矩阵 $H_w \\in \\mathbb{R}^{m \\times w}$。\n            -   第一个隐藏层的激活值为 $h^{(1)} = \\mathrm{ReLU}(X A_{:,S_w} + \\mathbf{1} (b^{(1)}_{S_w})^\\top)$。\n            -   对于层 $\\ell \\in \\{2, \\dots, L\\}$，激活值迭代更新：$h^{(\\ell)} = \\mathrm{ReLU}(h^{(\\ell-1)} \\odot s^{(\\ell)}_{S_w} + \\mathbf{1} (b^{(\\ell)}_{S_w})^\\top)$，其中 $\\odot$ 是带广播的逐元素乘法。\n            -   最终矩阵为 $H_w = h^{(L)}$。\n        d.  **秩计算**：使用基于奇异值分解 (SVD) 的数值稳定方法计算所得矩阵 $H_w$ 的秩，该方法由标准数值库提供。\n        e.  **条件检查**：如果 $\\mathrm{rank}(H_w) = m$，那么 $w$ 就是满足插值条件的最小宽度（从 $m$ 开始）。这个值被记录为 $w^\\star$，并且当前测试用例的搜索终止。\n\n5.  **最终结果**：如果搜索循环完成而没有找到合适的 $w$，这意味着直到 $W_0$ 的所有宽度都不能满足秩条件，因此 $w^\\star$ 被设置为 $-1$。收集每个测试用例计算出的 $w^\\star$ 值。最终输出是这些值的列表。", "answer": "```python\nimport numpy as np\n\ndef compute_w_star(L: int, d: int, m: int, W0: int, seed: int) - int:\n    \"\"\"\n    Computes the collapse threshold width w_star for a single test case.\n\n    Args:\n        L: Number of hidden layers.\n        d: Input dimension.\n        m: Number of samples.\n        W0: Initial width of hidden layers.\n        seed: Seed for the pseudorandom number generator.\n\n    Returns:\n        The collapse threshold width w_star, or -1 if no such width exists.\n    \"\"\"\n    # According to the problem, rank(H_w) must be m. The rank of a matrix\n    # cannot exceed its number of columns, w. Thus, if W0  m, it's impossible\n    # to achieve rank m.\n    if W0  m:\n        return -1\n\n    # 1. Parameter Generation\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal(size=(m, d))\n    A = rng.standard_normal(size=(d, W0))\n    b1 = rng.standard_normal(size=W0)\n    \n    s_params = []\n    b_params = []\n    if L  1:\n        for _ in range(2, L + 1):\n            s_params.append(rng.standard_normal(size=W0))\n            b_params.append(rng.standard_normal(size=W0))\n\n    # 2. Importance Score Calculation\n    # I_j = ||A_(:,j)||_2 * product(|s^(l)_j|) for l=2..L\n    A_col_norms = np.linalg.norm(A, axis=0)\n    \n    # The product is 1 if L=1 (empty product)\n    s_prods = np.ones(W0)\n    if L  1:\n        for s_l in s_params:\n            s_prods *= np.abs(s_l)\n            \n    importances = A_col_norms * s_prods\n\n    # 3. Channel Pruning Order\n    # Get indices of channels sorted by importance in descending order\n    sorted_channel_indices = np.argsort(importances)[::-1]\n\n    # 4. Search for Collapse Threshold w_star\n    # Search starts from w=m, as rank(H_w) = w  m is not possible.\n    # The first w = m that satisfies the condition is the minimal one.\n    for w in range(m, W0 + 1):\n        # a. Select Channels\n        top_w_indices = sorted_channel_indices[:w]\n        # Sort indices to preserve original order as per problem spec\n        current_indices = np.sort(top_w_indices)\n\n        # b. Construct Pruned Network\n        A_w = A[:, current_indices]\n        b1_w = b1[current_indices]\n\n        # c. Forward Pass\n        h = np.maximum(0, X @ A_w + b1_w)\n        \n        if L  1:\n            for l_idx in range(L - 1):\n                s_l_w = s_params[l_idx][current_indices]\n                b_l_w = b_params[l_idx][current_indices]\n                h = np.maximum(0, h * s_l_w + b_l_w)\n        \n        H_w = h\n        \n        # d. Rank Computation and e. Condition Check\n        if np.linalg.matrix_rank(H_w) == m:\n            return w\n\n    # If the loop finishes, no width w in [m, W0] satisfied the condition\n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (2, 8, 6, 16, 1),  # Case 1\n        (4, 8, 6, 12, 2),  # Case 2\n        (3, 8, 10, 7, 3),  # Case 3\n        (1, 6, 5, 5, 4),   # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        L, d, m, W0, seed = case\n        w_star = compute_w_star(L, d, m, W0, seed)\n        results.append(w_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3157479"}, {"introduction": "在现实世界中，计算资源（如模型参数量）是有限的。这个练习 [@problem_id:3157484] 探讨了一个核心的架构设计问题：在固定的参数预算下，我们应该构建更深的网络还是更宽的网络？通过比较标准网络与参数共享网络，你将学习如何在深度和宽度之间进行权衡，并分析这种权衡对模型理论容量的影响。", "problem": "考虑一个使用修正线性单元（ReLU）激活函数的全连接前馈网络，该网络将一个维度为 $d$ 的输入向量映射到一个标量输出。该网络有 $L$ 个隐藏层，每个隐藏层的宽度为 $n$，并在每个隐藏层中使用仿射变换后跟一个 ReLU 非线性激活。在固定的参数预算下，考虑两种架构变体：(i) 一个非绑定网络，其各层参数相互独立；(ii) 一个绑定网络，除了第一层外，其隐藏层之间进行跨层参数共享，即第 $2$ 层到第 $L$ 层共享相同的权重矩阵和偏置向量。\n\n从统计学习理论中参数计数的基石定义和经过充分检验的容量代理出发，您必须推导出表达式，以在保持可训练参数总数近似恒定的情况下，比较这两种变体的表示能力。使用分段线性网络的瓦普尼克-切尔沃宁基斯 (Vapnik–Chervonenkis, VC) 维度上界缩放作为容量代理，已知对于 $P$ 个可训练参数，其数量级为 $P \\log P$。为本问题之目的，将比例常数设为 $1$，并使用自然对数。\n\n定义以下精确的参数计数模型：\n\n1. 非绑定网络（每个隐藏层参数独立）：\n   - 第一个隐藏层：权重矩阵形状为 $d \\times n$，偏置向量长度为 $n$。\n   - 第 $2$ 到第 $L$ 个隐藏层：每个都有一个形状为 $n \\times n$ 的权重矩阵和一个长度为 $n$ 的偏置向量。\n   - 输出层：权重向量长度为 $n$，以及一个标量偏置。\n   总参数数量为\n   $$\n   P_{\\text{untied}}(d,L,n) = d n + (L - 1) n^2 + (L + 1) n + 1.\n   $$\n\n2. 绑定网络（第 $2$ 到第 $L$ 个隐藏层之间跨层参数共享）：\n   - 第一个隐藏层：权重矩阵形状为 $d \\times n$，偏置向量长度为 $n$。\n   - 第 $2$ 到第 $L$ 层的共享隐藏块：一个共享的权重矩阵，形状为 $n \\times n$，以及一个共享的偏置向量，长度为 $n$。\n   - 输出层：权重向量长度为 $n$，以及一个标量偏置。\n   总参数数量为\n   $$\n   P_{\\text{tied}}(d,L,n) = d n + n^2 + 3 n + 1.\n   $$\n\n给定一个宽度为 $n_0$ 的基准非绑定网络，通过求解等式 $P_{\\text{tied}}(d,L,n_s) \\approx P_{\\text{untied}}(d,L,n_0)$ 来定义绑定网络的宽度 $n_s$，选择 $n_s$ 为大于或等于相应二次方程正根的最小整数。形式上，求出\n$$\nn_s^2 + (d + 3) n_s + 1 - P_{\\text{untied}}(d,L,n_0) = 0,\n$$\n的正根，并将 $n_s$ 设为该根的上取整。\n\n对于每个测试用例，为非绑定网络和绑定网络在指定宽度下计算容量代理\n$$\nV(P) = P \\log P,\n$$\n并通过检查是否满足\n$$\n\\frac{V\\big(P_{\\text{tied}}(d,L,n_s)\\big)}{V\\big(P_{\\text{untied}}(d,L,n_0)\\big)} \\ge \\tau,\n$$\n来判断绑定网络相对于非绑定网络是否保持了容量，其中 $\\tau$ 是一个给定的小数阈值。\n\n您的程序必须为以下测试套件计算每个用例的布尔值保留结果，并将它们以逗号分隔的列表形式单行输出，并用方括号括起来：\n\n- 用例 1：$d = 8$, $L = 5$, $n_0 = 32$, $\\tau = 0.98$。\n- 用例 2：$d = 4$, $L = 2$, $n_0 = 16$, $\\tau = 1.02$。\n- 用例 3：$d = 3$, $L = 10$, $n_0 = 8$, $\\tau = 0.99$。\n\n您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$[result1,result2,result3]$）。不涉及物理单位或角度单位。所有阈值均以小数形式提供，而非百分比。", "solution": "该问题要求在可训练参数总数近似恒定的约束下，对两种神经网络架构——一个非绑定网络和一个带参数共享的绑定网络——的表示能力进行比较分析。该能力通过基于分段线性网络的瓦普尼克-切尔沃宁基斯（VC）维度上界的代理进行量化，该代理由 $V(P) = P \\log P$ 给出，其中 $P$ 是可训练参数的数量，$\\log$ 表示自然对数。分析过程为：首先计算出绑定网络为匹配基准非绑定网络的参数数量所需的宽度，然后比较它们各自的容量代理是否满足给定的阈值。\n\n对于每个由参数 $(d, L, n_0, \\tau)$ 定义的测试用例，解法按以下步骤推导，这些参数分别代表输入维度、隐藏层数量、基准非绑定网络宽度和容量保持阈值。\n\n1.  **计算非绑定网络的参数数量**\n\n    首先，对于给定的基准非绑定网络，其输入维度为 $d$，$L$ 个隐藏层，统一的隐藏层宽度为 $n_0$，我们使用提供的公式计算其可训练参数的总数 $P_{\\text{untied}}$：\n    $$\n    P_{\\text{untied}}(d, L, n_0) = d n_0 + (L - 1) n_0^2 + (L + 1) n_0 + 1\n    $$\n    该公式计入了第一个隐藏层（$d \\times n_0$ 个权重和 $n_0$ 个偏置）、随后的 $L-1$ 个隐藏层（每个有 $n_0 \\times n_0$ 个权重和 $n_0$ 个偏置）以及最后的输出层（$n_0$ 个权重和 $1$ 个偏置）中的参数。\n\n2.  **确定绑定网络的等效宽度**\n\n    接下来，我们确定一个绑定网络的宽度 $n_s$，使其参数数量与非绑定网络大致相同。该绑定网络在第 $2$ 到第 $L$ 个隐藏层之间共享单个权重矩阵和偏置向量。其参数数量由以下公式给出：\n    $$\n    P_{\\text{tied}}(d, L, n_s) = d n_s + n_s^2 + 3 n_s + 1\n    $$\n    我们设定 $P_{\\text{tied}}(d,L,n_s) \\approx P_{\\text{untied}}(d,L,n_0)$，这导出了关于 $n_s$ 的二次方程：\n    $$\n    n_s^2 + (d + 3) n_s + 1 - P_{\\text{untied}}(d,L,n_0) = 0\n    $$\n    这是一个标准形式为 $an^2 + bn + c = 0$ 的二次方程，其系数为 $a = 1$，$b = d + 3$，以及 $c = 1 - P_{\\text{untied}}$。由于 $d$ 是正整数，$b$ 是正数。由于对于典型的网络配置，$P_{\\text{untied}}$ 是一个大的正整数，$c$ 是负数。判别式 $\\Delta = b^2 - 4ac$ 将为正，保证有两个不同的实根。由于根的乘积 $c/a = c$ 是负数，一个根必定为正，另一个为负。我们关心的是唯一的正根，它代表一个有效的网络宽度：\n    $$\n    n_{s, \\text{root}} = \\frac{-(d+3) + \\sqrt{(d+3)^2 - 4(1)(1 - P_{\\text{untied}})}}{2}\n    $$\n    由于网络宽度必须是整数，$n_s$ 定义为大于或等于这个正根的最小整数，即 $n_s = \\lceil n_{s, \\text{root}} \\rceil$。\n\n3.  **计算绑定网络的实际参数数量**\n\n    使用上一步中找到的整数宽度 $n_s$，我们计算得到的绑定网络的精确参数数量 $P_{\\text{tied}}(d,L,n_s)$。由于上取整运算，$P_{\\text{tied}}(d,L,n_s)$ 将大于或等于 $P_{\\text{untied}}(d,L,n_0)$。\n    $$\n    P_{\\text{tied}} = n_s^2 + (d+3)n_s + 1\n    $$\n\n4.  **计算并比较容量代理**\n\n    在确定了两种架构的参数数量后，我们使用函数 $V(P) = P \\log P$ 计算它们各自的容量代理：\n    $$\n    V_{\\text{untied}} = P_{\\text{untied}}(d,L,n_0) \\log\\big(P_{\\text{untied}}(d,L,n_0)\\big)\n    $$\n    $$\n    V_{\\text{tied}} = P_{\\text{tied}}(d,L,n_s) \\log\\big(P_{\\text{tied}}(d,L,n_s)\\big)\n    $$\n    最后，我们通过检查绑定网络的容量与非绑定网络容量之比是否达到或超过阈值 $\\tau$ 来评估容量保持情况：\n    $$\n    \\frac{V_{\\text{tied}}}{V_{\\text{untied}}} \\ge \\tau\n    $$\n    此比较的结果是一个布尔值。\n\n我们现在将此过程应用于每个指定的测试用例。\n\n**用例 1:** $d = 8$, $L = 5$, $n_0 = 32$, $\\tau = 0.98$\n1.  $P_{\\text{untied}} = 8(32) + (5-1)(32)^2 + (5+1)(32) + 1 = 256 + 4(1024) + 6(32) + 1 = 4545$。\n2.  关于 $n_s$ 的方程：$n_s^2 + (8+3)n_s + 1 - 4545 = 0 \\implies n_s^2 + 11n_s - 4544 = 0$。\n    $n_{s, \\text{root}} = \\frac{-11 + \\sqrt{11^2 - 4(-4544)}}{2} = \\frac{-11 + \\sqrt{18297}}{2} \\approx 62.133$。\n    $n_s = \\lceil 62.133 \\rceil = 63$。\n3.  $P_{\\text{tied}} = 63^2 + (8+3)(63) + 1 = 3969 + 11(63) + 1 = 3969 + 693 + 1 = 4663$。\n4.  $V_{\\text{untied}} = 4545 \\log(4545) \\approx 38266.6$。\n    $V_{\\text{tied}} = 4663 \\log(4663) \\approx 39391.8$。\n    比率：$\\frac{39391.8}{38266.6} \\approx 1.0294$。\n    比较：$1.0294 \\ge 0.98$，结果为**真 (True)**。\n\n**用例 2:** $d = 4$, $L = 2$, $n_0 = 16$, $\\tau = 1.02$\n1.  $P_{\\text{untied}} = 4(16) + (2-1)(16)^2 + (2+1)(16) + 1 = 64 + 256 + 48 + 1 = 369$。\n2.  关于 $n_s$ 的方程：$n_s^2 + (4+3)n_s + 1 - 369 = 0 \\implies n_s^2 + 7n_s - 368 = 0$。\n    $n_{s, \\text{root}} = \\frac{-7 + \\sqrt{7^2 - 4(-368)}}{2} = \\frac{-7 + \\sqrt{1521}}{2} = \\frac{-7 + 39}{2} = 16$。\n    $n_s = \\lceil 16 \\rceil = 16$。\n3.  $P_{\\text{tied}} = 16^2 + (4+3)(16) + 1 = 256 + 7(16) + 1 = 256 + 112 + 1 = 369$。\n4.  因为 $P_{\\text{tied}} = P_{\\text{untied}}$，所以 $V_{\\text{tied}} = V_{\\text{untied}}$。\n    比率：$1$。\n    比较：$1 \\ge 1.02$，结果为**假 (False)**。\n\n**用例 3:** $d = 3$, $L = 10$, $n_0 = 8$, $\\tau = 0.99$\n1.  $P_{\\text{untied}} = 3(8) + (10-1)(8)^2 + (10+1)(8) + 1 = 24 + 9(64) + 11(8) + 1 = 689$。\n2.  关于 $n_s$ 的方程：$n_s^2 + (3+3)n_s + 1 - 689 = 0 \\implies n_s^2 + 6n_s - 688 = 0$。\n    $n_{s, \\text{root}} = \\frac{-6 + \\sqrt{6^2 - 4(-688)}}{2} = \\frac{-6 + \\sqrt{2788}}{2} \\approx 23.401$。\n    $n_s = \\lceil 23.401 \\rceil = 24$。\n3.  $P_{\\text{tied}} = 24^2 + (3+3)(24) + 1 = 576 + 6(24) + 1 = 576 + 144 + 1 = 721$。\n4.  $V_{\\text{untied}} = 689 \\log(689) \\approx 4502.75$。\n    $V_{\\text{tied}} = 721 \\log(721) \\approx 4744.61$。\n    比率：$\\frac{4744.61}{4502.75} \\approx 1.0537$。\n    比较：$1.0537 \\ge 0.99$，结果为**真 (True)**。\n\n最终结果是 [True, False, True]。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the network capacity comparison problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (d, L, n0, tau)\n    test_cases = [\n        (8, 5, 32, 0.98),  # Case 1\n        (4, 2, 16, 1.02),  # Case 2\n        (3, 10, 8, 0.99),  # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        d, L, n0, tau = case\n\n        # Step 1: Calculate the parameter count for the untied network.\n        # P_untied = d*n0 + (L-1)*n0^2 + (L+1)*n0 + 1\n        P_untied = d * n0 + (L - 1) * n0**2 + (L + 1) * n0 + 1\n        \n        # Step 2: Calculate the width of the tied network (n_s).\n        # This involves solving the quadratic equation:\n        # n_s^2 + (d+3)*n_s + (1 - P_untied) = 0\n        a = 1\n        b = d + 3\n        c = 1 - P_untied\n        \n        # Calculate the positive root of the quadratic equation.\n        # The discriminant is b^2 - 4*a*c.\n        discriminant = b**2 - 4 * a * c\n        ns_root = (-b + np.sqrt(discriminant)) / (2 * a)\n        \n        # n_s is the ceiling of the positive root.\n        ns = int(np.ceil(ns_root))\n\n        # Step 3: Calculate the actual parameter count for the tied network.\n        # P_tied = d*ns + ns^2 + 3*ns + 1\n        P_tied = d * ns + ns**2 + 3 * ns + 1\n\n        # Step 4: Compute the capacity proxies for both networks.\n        # The capacity proxy is V(P) = P * log(P).\n        V_untied = P_untied * np.log(P_untied)\n        V_tied = P_tied * np.log(P_tied)\n        \n        # Step 5: Compare capacities and determine retention.\n        # Check if the ratio of capacities meets the threshold tau.\n        # Handle the edge case where V_untied might be zero, although\n        # for this problem's constraints, P will be  1.\n        if V_untied == 0:\n            retention_check = False # Or handle as per problem spec if V_untied=0 is possible\n        else:\n            capacity_ratio = V_tied / V_untied\n            retention_check = capacity_ratio = tau\n        \n        results.append(retention_check)\n\n    # Final print statement in the exact required format.\n    # Example: [True,False,True]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3157484"}, {"introduction": "理论分析固然重要，但模型的最终表现还需通过实验来验证。这个练习 [@problem_id:3157535] 让你扮演一名研究科学家的角色，通过分析模拟的实验数据，来揭示测试损失与网络深度、宽度之间的经验缩放定律（scaling law）。你将运用统计建模的方法，从数据中提取出关于网络架构设计的宝贵见解。", "problem": "您的任务是使用一个基于原则的统计程序来估计测试损失如何随网络深度和宽度进行缩放。考虑一个由深度 $L$（正整数）和宽度 $w$（正整数）表征的神经网络族。对于一个固定的数据集大小范围（被视为一个分类组），观察到经验测试损失遵循一个具有组特定比例性的乘性幂律：\n$$\nt \\approx A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon,\n$$\n其中 $t$ 是观测到的测试损失（一个正实数），$A_g$ 是特定于组 $g$（数据集大小范围）的未知正常数，$\\alpha$ 和 $\\beta$ 是在各组间保持不变的实值指数，而 $\\varepsilon$ 是一个正噪声因子。假设噪声是乘性的且在对数域中为对数正态分布，均值为零，这意味着观测测试损失的自然对数具有加性的、独立同分布的高斯扰动。您的任务是使用一个与这些假设一致的、有统计学依据的估计器来推断指数 $\\alpha$ 和 $\\beta$，同时汇集所有组的数据，并允许每个组有其自己的截距。\n\n您必须依赖的基础原理：\n- 将乘性关系转换为加性关系的自然对数变换的定义。\n- 高斯分布的性质以及加性高斯噪声下的最大似然估计与最小二乘估计量之间的等价性。\n- 线性最小二乘法和正规方程组。\n\n您的程序必须实现一个基于这些原则的估计器，从提供的观测数据中推断出 $\\alpha$ 和 $\\beta$。将每个数据集大小视为一个独立的组，有其自身的常数 $A_g$，并通过在对数域中最小化带有组特定截距的残差平方和，来跨所有组联合估计 $\\alpha$ 和 $\\beta$。\n\n您将获得包含三个用例的以下测试套件。在每个用例中，您会得到按数据集大小分组的观测数据（组标签显示为 $N$）。每个观测数据是一个元组 $(L,w,t)$，其中 $L$ 是深度，$w$ 是宽度，$t$ 是观测到的测试损失。所有数字都是无量纲的。\n\n测试用例 1（理想情况，三个组，变化充足）：\n- 组 $N = 10^3$：\n  - $(L,w,t) = (\\,4,\\,128,\\,0.0883883476483184\\,)$\n  - $(L,w,t) = (\\,16,\\,128,\\,0.0441941738241592\\,)$\n- 组 $N = 10^4$：\n  - $(L,w,t) = (\\,8,\\,64,\\,0.0441941738241592\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.0220970869120796\\,)$\n- 组 $N = 10^5$：\n  - $(L,w,t) = (\\,16,\\,256,\\,0.0125\\,)$\n  - $(L,w,t) = (\\,4,\\,64,\\,0.05\\,)$\n\n测试用例 2（可识别性边界：恰好确定，两个组）：\n- 组 $N = 2 \\times 10^3$：\n  - $(L,w,t) = (\\,4,\\,64,\\,0.75\\,)$\n  - $(L,w,t) = (\\,8,\\,64,\\,0.375\\,)$\n- 组 $N = 2 \\times 10^4$：\n  - $(L,w,t) = (\\,8,\\,128,\\,0.15\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.15\\,)$\n\n测试用例 3（$(L,w)$ 模式中存在近似共线性的边界情况，两个组）：\n- 组 $N = 5 \\times 10^2$：\n  - $(L,w,t) = (\\,4,\\,64,\\,0.078125\\,)$\n  - $(L,w,t) = (\\,16,\\,64,\\,0.078125\\,)$\n- 组 $N = 5 \\times 10^3$：\n  - $(L,w,t) = (\\,8,\\,128,\\,0.01953125\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.009765625\\,)$\n\n要求：\n- 将观测测试损失的对数建模为一个加性线性模型，该模型具有组特定截距以及 $\\ln L$ 和 $\\ln w$ 的共同斜率。使用一个在对数域中与高斯噪声假设下的最大似然估计一致的最小二乘估计器来估计 $\\alpha$ 和 $\\beta$。\n- 实现一个程序，处理所有三个测试用例（固定在您的代码中），并按顺序输出每个用例的估计值 $\\alpha$ 和 $\\beta$，四舍五入到三位小数。如果一个值的绝对值小于 $5 \\times 10^{-4}$，在四舍五入后将其打印为 $0.0$。\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身必须是一个双元素列表 $[\\alpha,\\beta]$。例如，一个有效的最终输出形式为 $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$，其中 $a_i$ 和 $b_i$ 是不带额外空格打印的浮点数。\n\n不涉及物理单位。不涉及角度。不使用百分比。每个测试用例的答案必须是一个浮点数列表。最终打印的行必须严格符合指定格式。", "solution": "该问题的核心是将在对数尺度上呈线性的乘性幂律模型进行拟合。首先，我们对模型关系进行变换，以应用线性回归。给定的模型是：\n$$\nt \\approx A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon\n$$\n对两边取自然对数，得到一个加性模型：\n$$\n\\ln(t) \\approx \\ln(A_g) + \\alpha \\ln(L) + \\beta \\ln(w) + \\ln(\\varepsilon)\n$$\n这个方程可以被重新表述为一个线性回归模型。对于第 $i$ 个观测，我们定义：\n- 响应变量：$y_i = \\ln(t_i)$\n- 预测变量：$x_{i,1} = \\ln(L_i)$ 和 $x_{i,2} = \\ln(w_i)$\n- 参数：我们希望估计的共同斜率是 $\\alpha$ 和 $\\beta$。对于每个组 $g$，有一个特定的截距 $\\gamma_g = \\ln(A_g)$。\n- 误差项：$\\epsilon'_i = \\ln(\\varepsilon_i)$。问题陈述中乘性对数正态噪声的假设意味着 $\\epsilon'_i$ 是一个均值为零的加性高斯噪声项。\n\n因此，对于属于组 $g(i)$ 的第 $i$ 个观测，模型是：\n$$\ny_i = \\gamma_{g(i)} + \\alpha x_{i,1} + \\beta x_{i,2} + \\epsilon'_i\n$$\n这是一个协方差分析（ANCOVA）模型，其中 $\\ln(L)$ 和 $\\ln(w)$ 是协变量，而数据集大小是分类因子。\n\n为了联合估计所有组的共同斜率 $\\alpha$ 和 $\\beta$ 以及每个组的特定截距 $\\gamma_g$，我们将所有观测数据汇集起来，并使用最小二乘法。在加性高斯噪声的假设下，最小二乘估计等价于最大似然估计。\n\n我们可以将此问题表示为矩阵形式 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon'}$，其中：\n- $\\mathbf{y}$ 是所有观测测试损失的对数组成的向量。\n- $\\boldsymbol{\\theta}$ 是待估计的参数向量，它包含了所有组的截距以及共同的斜率 $\\alpha$ 和 $\\beta$。如果有个 $k$ 组，则 $\\boldsymbol{\\theta} = [\\gamma_1, \\dots, \\gamma_k, \\alpha, \\beta]^T$。\n- $\\mathbf{X}$ 是设计矩阵。对于第 $i$ 个观测（属于组 $j$），$\\mathbf{X}$ 的第 $i$ 行由 $k$ 个指示变量（其中第 $j$ 个为 1，其余为 0，用于选择正确的截距）和两个协变量值 $\\ln(L_i)$ 和 $\\ln(w_i)$ 组成。\n\n最小二乘解 $\\hat{\\boldsymbol{\\theta}}$ 通过求解正规方程 $\\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\theta}} = \\mathbf{X}^T \\mathbf{y}$ 得到。在数值实现中，这通常通过更稳定的方法（如 QR 分解）来完成，例如使用 `numpy.linalg.lstsq`。\n\n在求解得到参数向量 $\\hat{\\boldsymbol{\\theta}}$ 后，估计的缩放指数 $\\hat{\\alpha}$ 和 $\\hat{\\beta}$ 就是该向量的最后两个元素。此过程将应用于每个测试用例，以从提供的数据中提取这些指数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Processes all test cases to estimate scaling exponents alpha and beta.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1 (happy path, three groups, ample variation)\n        {\n            \"10^3\": [\n                (4, 128, 0.0883883476483184),\n                (16, 128, 0.0441941738241592)\n            ],\n            \"10^4\": [\n                (8, 64, 0.0441941738241592),\n                (8, 256, 0.0220970869120796)\n            ],\n            \"10^5\": [\n                (16, 256, 0.0125),\n                (4, 64, 0.05)\n            ]\n        },\n        # Test Case 2 (boundary of identifiability: exactly determined, two groups)\n        {\n            \"2x10^3\": [\n                (4, 64, 0.75),\n                (8, 64, 0.375)\n            ],\n            \"2x10^4\": [\n                (8, 128, 0.15),\n                (8, 256, 0.15)\n            ]\n        },\n        # Test Case 3 (edge case with near-collinearity in (L,w) patterns, two groups)\n        {\n            \"5x10^2\": [\n                (4, 64, 0.078125),\n                (16, 64, 0.078125)\n            ],\n            \"5x10^3\": [\n                (8, 128, 0.01953125),\n                (8, 256, 0.009765625)\n            ]\n        }\n    ]\n\n    def custom_round(value):\n        \"\"\"\n        Rounds a value to 3 decimal places. If the magnitude is less than\n        5e-4, it returns 0.0.\n        \"\"\"\n        if abs(value)  5e-4:\n            return 0.0\n        return round(value, 3)\n\n    results_str_list = []\n    \n    for case in test_cases:\n        observations = []\n        group_indices = []\n        \n        # Assign an integer index to each unique group label\n        group_labels = sorted(case.keys())\n        group_map = {label: i for i, label in enumerate(group_labels)}\n        num_groups = len(group_map)\n\n        # Collect all observations and their corresponding group indices\n        for group_label, data_points in case.items():\n            group_idx = group_map[group_label]\n            for point in data_points:\n                observations.append(point)\n                group_indices.append(group_idx)\n\n        num_obs = len(observations)\n        \n        # Extract L, w, t and transform them to the log domain\n        L_vals = np.array([obs[0] for obs in observations])\n        w_vals = np.array([obs[1] for obs in observations])\n        t_vals = np.array([obs[2] for obs in observations])\n\n        ln_L = np.log(L_vals)\n        ln_w = np.log(w_vals)\n        y = np.log(t_vals) # This is the response vector\n        \n        # Construct the design matrix X\n        # Columns: k group intercepts, 1 for alpha (ln L), 1 for beta (ln w)\n        num_params = num_groups + 2\n        X = np.zeros((num_obs, num_params))\n\n        # Populate the one-hot encoded group intercept columns\n        X[np.arange(num_obs), group_indices] = 1\n        \n        # Populate the slope columns for alpha and beta\n        X[:, num_groups] = ln_L\n        X[:, num_groups + 1] = ln_w\n        \n        # Perform linear least squares regression\n        # theta = (X^T X)^-1 X^T y\n        theta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        \n        # Extract estimated alpha and beta from the parameter vector theta\n        alpha_hat = theta[num_groups]\n        beta_hat = theta[num_groups + 1]\n        \n        # Apply custom rounding as per problem specification\n        rounded_alpha = custom_round(alpha_hat)\n        rounded_beta = custom_round(beta_hat)\n\n        # Format the result for this case\n        results_str_list.append(f\"[{rounded_alpha},{rounded_beta}]\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```", "id": "3157535"}]}