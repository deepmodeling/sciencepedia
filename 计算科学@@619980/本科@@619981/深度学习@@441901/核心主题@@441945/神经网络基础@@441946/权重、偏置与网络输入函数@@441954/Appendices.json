{"hands_on_practices": [{"introduction": "本练习是一个基础性的思想实验，旨在加深你对权重、偏置和净输入函数之间关系的理解。通过探究对输入和权重进行缩放如何影响单个神经元的决策，我们可以揭示加权和（$\\mathbf{w}^{\\top}\\mathbf{x}$）与偏置（$b$）各自扮演的独特角色。这个练习将阐明偏置如何作为一个可调节的决策阈值，其作用独立于输入和权重的缩放变换 [@problem_id:3199772]。", "problem": "考虑人工神经网络（ANN）中的单个线性单元（一个神经元），其净输入由核心线性模型和偏置定义，即 $z=\\mathbf{w}^{\\top}\\mathbf{x}+b$，其中 $\\mathbf{w}\\in\\mathbb{R}^{d}$ 是权重向量，$\\mathbf{x}\\in\\mathbb{R}^{d}$ 是输入向量，$b\\in\\mathbb{R}$ 是偏置。神经元的决策是净输入的符号，由函数 $\\operatorname{sign}(z)$ 给出，其中如果 $t0$，$\\operatorname{sign}(t)$ 返回 $+1$；如果 $t=0$，返回 $0$；如果 $t0$，返回 $-1$。假设输入和权重分别同时按实数因子 $\\alpha$ 和 $\\beta$ 进行缩放，而偏置保持不变，即 $\\mathbf{x}\\leftarrow\\alpha\\mathbf{x}$，$\\mathbf{w}\\leftarrow\\beta\\mathbf{w}$，以及 $b\\leftarrow b$。\n\n仅从净输入和决策函数的定义出发，推导在缩放后神经元决策 $\\operatorname{sign}(z)$ 保持不变时，$(\\alpha,\\beta)$ 必须满足的充要条件。根据内积 $s=\\mathbf{w}^{\\top}\\mathbf{x}$ 和偏置 $b$ 来充分描述各种情况，区分平凡和非平凡情景，并解释这些区别为何出现。\n\n最后，令 $\\gamma=\\alpha\\beta$ 表示作用于内积的有效净缩放。以封闭形式确定 $\\gamma$ 的临界值 $\\gamma^{\\star}$（作为关于 $s$ 和 $b$ 的符号表达式），在该临界值下，神经元的决策在缩放变换下会翻转其符号。以单个解析表达式的形式提供你的最终答案。无需四舍五入。", "solution": "神经元的净输入由带偏置的线性模型定义，$z=\\mathbf{w}^{\\top}\\mathbf{x}+b$。决策是 $\\operatorname{sign}(z)$，取值于 $\\{-1,0,+1\\}$。在同时缩放 $\\mathbf{x}\\leftarrow\\alpha\\mathbf{x}$ 和 $\\mathbf{w}\\leftarrow\\beta\\mathbf{w}$ 并保持 $b$ 固定的情况下，变换后的净输入变为\n$$\nz'=(\\beta\\mathbf{w})^{\\top}(\\alpha\\mathbf{x})+b=\\alpha\\beta\\,\\mathbf{w}^{\\top}\\mathbf{x}+b.\n$$\n引入简写 $s=\\mathbf{w}^{\\top}\\mathbf{x}$。那么原始和变换后的净输入为\n$$\nz=s+b,\\qquad z'=\\alpha\\beta\\,s+b.\n$$\n我们要求决策保持不变，即 $\\operatorname{sign}(z')=\\operatorname{sign}(z)$，这等价于要求 $z'$ 与 $z$ 保持在零的同一侧，或者当且仅当 $z$ 等于零时等于零。\n\n我们根据 $s$ 和 $b$ 的值分情况分析：\n\n1. 情况 $s=0$。此时 $z=b$ 且 $z'=\\alpha\\beta\\cdot 0+b=b$。因此，对于所有实数 $\\alpha$ 和 $\\beta$，都有 $\\operatorname{sign}(z')=\\operatorname{sign}(b)=\\operatorname{sign}(z)$。这是一个平凡不变性情况：内积对净输入没有贡献，所以缩放 $\\mathbf{x}$ 和 $\\mathbf{w}$ 没有影响。\n\n2. 情况 $s\\neq 0$。此时 $z=s+b$ 且 $z'=\\alpha\\beta\\,s+b$。定义 $\\gamma=\\alpha\\beta$。不变性条件 $\\operatorname{sign}(z')=\\operatorname{sign}(z)$ 等价于要求 $z'$ 和 $z$ 具有相同的符号（包括零）。符号可能改变的 $\\gamma$ 的边界发生在 $z'=0$ 时，即\n$$\n\\gamma\\,s+b=0.\n$$\n解出 $\\gamma$ 得到临界值\n$$\n\\gamma^{\\star}=-\\frac{b}{s}.\n$$\n这个值分开了产生正净输入与负净输入的 $\\gamma$ 的区域。具体来说：\n- 如果 $s0$，那么 $z'= \\gamma s + b$ 是 $\\gamma$ 的增函数。因此：\n  - 如果 $z=s+b0$，不变性要求 $\\gamma\\gamma^{\\star}$。\n  - 如果 $z=s+b0$，不变性要求 $\\gamma\\gamma^{\\star}$。\n  - 如果 $z=s+b=0$，在边界上的不变性要求 $\\gamma=\\gamma^{\\star}$（任何偏离都会改变符号）。\n- 如果 $s0$，那么 $z'=\\gamma s + b$ 是 $\\gamma$ 的减函数。因此：\n  - 如果 $z=s+b0$，不变性要求 $\\gamma\\gamma^{\\star}$。\n  - 如果 $z=s+b0$，不变性要求 $\\gamma\\gamma^{\\star}$。\n  - 如果 $z=s+b=0$，在边界上的不变性要求 $\\gamma=\\gamma^{\\star}$（任何偏离都会改变符号）。\n\n这些结果表明，对于 $s\\neq 0$ 和 $z\\neq 0$（非平凡情景），保持决策不变的 $(\\alpha,\\beta)$ 集合，就 $\\gamma=\\alpha\\beta$ 而言，是由 $s$ 的符号以及原始净输入位于边界 $\\gamma^{\\star}$ 的哪一侧所决定的半空间。决策翻转的精确阈值是迫使变换后的净输入为零的唯一缩放乘积。\n\n特殊子情况 $b=0$（其中 $s\\neq 0$）说明了一个经典的缩放不变性：$z=s$ 且 $z'=\\gamma s$。决策保持不变当且仅当 $\\gamma0$（$\\gamma=0$ 会迫使 $z'=0$）。在这里，$\\gamma^{\\star}=-b/s=0$，这与符号可能改变的边界一致。\n\n因此，决策发生翻转的有效缩放乘积的临界值完全由偏置与内积的比值给出，并带一个负号：\n$$\n\\gamma^{\\star}=-\\frac{b}{s}.\n$$\n这个表达式简洁地描述了非平凡情况 $s\\neq 0$；在平凡情况 $s=0$ 下，缩放对决策没有影响，并且在 $\\gamma$ 中不存在有意义的有限阈值，因为 $z'$ 与 $\\gamma$ 无关。", "answer": "$$\\boxed{-\\frac{b}{s}}$$", "id": "3199772"}, {"introduction": "在理解了净输入函数的基本构成后，我们现在转向一个动态的训练场景。正则化是防止模型过拟合的关键技术，但过度的正则化会导致模型欠拟合。本练习将通过理论推导和代码实践，展示一个强大的 $L_2$ 正则化惩罚项如何压缩权重向量，从而使偏置项在净输入中占据主导地位，最终导致模型做出几乎恒定的预测 [@problem_id:3199770]。", "problem": "给定一个深度学习中的单神经元模型，其净输入函数通过仿射变换将输入向量映射为标量。其学习目标遵循经验风险最小化原则，采用平方误差损失和仅对权重施加的 $L_2$（欧几里得范数）惩罚。您的目标是从第一性原理出发，严格证明强 $L_2$ 正则化如何收缩权重向量，从而使偏置项在净输入中占主导地位，导致预测结果几乎恒定并产生欠拟合。\n\n从经验风险最小化和 $L_2$ 正则化的基本定义开始。假设一个数据集包含 $N$ 个样本，每个输入向量的维度为 $d$。存在一个标量偏置参数。构建训练准则并推导刻画最小化器的条件。利用这些条件，推断当正则化强度很大时，权重向量的范数会变小，净输入值由偏置项主导。解释为什么这会导致以几乎恒定的输出为特征的欠拟合。\n\n实现一个程序，通过精确求解带仅权重 $L_2$ 惩罚的平方误差损失所对应的必要线性系统，来计算训练后的权重和偏置。然后，为下面的每个测试用例，计算优势比\n$$\nR \\equiv \\frac{\\max_{i \\in \\{1,\\dots,N\\}} \\left| \\mathbf{w}^\\top \\mathbf{x}_i \\right|}{|b| + 10^{-12}},\n$$\n该比率量化了在给定数据集上，线性（依赖于权重）贡献的振幅相对于偏置大小的比例。一个小的比率 $R$ 表示偏置在净输入函数中占主导地位，预测趋于一个常数，而一个大的比率则表示线性项占主导地位。\n\n您的程序必须：\n- 构建指定的数据集。\n- 通过最小化仅对权重施加 $L_2$ 惩罚的平方误差来训练模型（偏置项必须不被正则化）。\n- 为每个测试用例计算净输入贡献和比率 $R$。\n- 输出一行，包含五个 $R$ 值，四舍五入到六位小数，格式为用方括号括起来的逗号分隔列表。\n\n测试套件（所有向量均为行主序，所有数字均为精确整数或小数；不涉及物理单位）：\n- 用例 1（强正则化，偏置主导的理想情况）：$N=5$, $d=3$, \n  $$\n  X=\\begin{bmatrix}\n  -2  1  0\\\\\n  -1  0  1\\\\\n  0  1  -1\\\\\n  1  -1  2\\\\\n  2  2  -2\n  \\end{bmatrix},\\quad\n  \\mathbf{a}=\\begin{bmatrix}2\\\\-1\\\\0.5\\end{bmatrix},\\quad\n  c=3,\\quad\n  \\boldsymbol{\\epsilon}=\\begin{bmatrix}0\\\\-0.1\\\\0.1\\\\-0.05\\\\0.05\\end{bmatrix},\n  $$\n  目标值为 $y_i = \\mathbf{a}^\\top \\mathbf{x}_i + c + \\epsilon_i$。使用正则化强度 $\\lambda = 10{,}000$。\n- 用例 2（无正则化，对照情况）：与用例 1 相同的 $X$、$\\mathbf{a}$、 $c$、$\\boldsymbol{\\epsilon}$ 和目标值，但 $\\lambda = 0$。\n- 用例 3（边界情况：零特征）：$N=4$, $d=3$,\n  $$\n  X=\\begin{bmatrix}\n  0  0  0\\\\\n  0  0  0\\\\\n  0  0  0\\\\\n  0  0  0\n  \\end{bmatrix},\\quad\n  \\mathbf{y}=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\end{bmatrix},\n  $$\n  且 $\\lambda = 500$。\n- 用例 4（边缘情况：即使使用中等强度的正则化，极小的特征尺度也会导致微弱的线性贡献）：复用用例 1 中的 $X$ 但将其缩放 $10^{-6}$ 倍（即，用 $10^{-6} X$ 替换 $X$），并复用用例 1 中定义的 $\\mathbf{a}$、$c$、$\\boldsymbol{\\epsilon}$ 和基于缩放后 $X$ 的目标值。使用 $\\lambda = 1$。\n- 用例 5（特征幅度大且使用中等强度的正则化，强调线性项）：复用用例 1 中的 $X$ 但将其缩放 $10^2$ 倍（即，用 $100 X$ 替换 $X$），并复用用例 1 中定义的 $\\mathbf{a}$、$c$、$\\boldsymbol{\\epsilon}$ 和基于缩放后 $X$ 的目标值。使用 $\\lambda = 10$。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含五个 $R$ 值，每个值四舍五入到六位小数，并以逗号分隔列表的形式用方括号括起来（例如，“[r1,r2,r3,r4,r5]”）。", "solution": "该问题要求从第一性原理出发，证明强 $L_2$ 正则化如何作用于单神经元模型的权重，导致以几乎恒定的预测为特征的欠拟合。出现这种情况是因为权重向量 $\\mathbf{w}$ 被收缩到零，导致偏置项 $b$ 在净输入函数中占主导地位。我们将首先推导模型参数的解析解，然后用它来解释这一现象。\n\n该模型是一个带线性激活函数的单神经元，对于输入向量 $\\mathbf{x}_i \\in \\mathbb{R}^d$，其预测值 $\\hat{y}_i$ 由仿射变换（净输入函数）给出：\n$$\n\\hat{y}_i = \\mathbf{w}^\\top \\mathbf{x}_i + b\n$$\n这里，$\\mathbf{w} \\in \\mathbb{R}^d$ 是权重向量，$b \\in \\mathbb{R}$ 是标量偏置项。\n\n学习目标是找到参数 $(\\mathbf{w}, b)$ 以最小化经验风险，该风险定义为在一个包含 $N$ 个样本 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$ 的数据集上的平方误差总和，并附加对权重 $\\mathbf{w}$ 的 $L_2$ 正则化惩罚。目标函数 $J(\\mathbf{w}, b)$ 为：\n$$\nJ(\\mathbf{w}, b) = \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 + \\lambda \\|\\mathbf{w}\\|_2^2 = \\sum_{i=1}^{N} (y_i - (\\mathbf{w}^\\top \\mathbf{x}_i + b))^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度。根据题目要求，偏置项 $b$ 不被正则化。\n\n为了找到最小化 $J(\\mathbf{w}, b)$ 的最优参数，我们将其关于 $\\mathbf{w}$ 和 $b$ 的偏导数设为零。这是最小值的必要条件，并且由于当 $\\lambda  0$ 时 $J$ 是 $(\\mathbf{w}, b)$ 的严格凸函数，所以这也是唯一全局最小值的充分条件。\n\n首先，我们求关于偏置 $b$ 的偏导数：\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{N} \\frac{\\partial}{\\partial b} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b)^2 = \\sum_{i=1}^{N} 2(y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b)(-1) = -2 \\sum_{i=1}^{N} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b)\n$$\n将其设为零：\n$$\n\\sum_{i=1}^{N} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b) = 0 \\implies \\sum_{i=1}^{N} y_i - \\mathbf{w}^\\top \\sum_{i=1}^{N} \\mathbf{x}_i - N b = 0\n$$\n解出 $b$ 可得：\n$$\nb = \\frac{1}{N}\\sum_{i=1}^{N} y_i - \\mathbf{w}^\\top \\left(\\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{x}_i\\right)\n$$\n令 $\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$ 和 $\\bar{\\mathbf{x}} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{x}_i$ 分别为目标值和输入的均值。那么最优偏置为：\n$$\nb = \\bar{y} - \\mathbf{w}^\\top \\bar{\\mathbf{x}}\n$$\n这个结果表明，最优偏置确保了学习到的超平面穿过数据的质心 $(\\bar{\\mathbf{x}}, \\bar{y})$。\n\n现在，我们可以将 $b$ 的这个表达式代回到目标函数 $J$ 中。这将寻找 $\\mathbf{w}$ 的问题与 $b$ 解耦。\n$$\ny_i - (\\mathbf{w}^\\top \\mathbf{x}_i + b) = y_i - (\\mathbf{w}^\\top \\mathbf{x}_i + \\bar{y} - \\mathbf{w}^\\top \\bar{\\mathbf{x}}) = (y_i - \\bar{y}) - \\mathbf{w}^\\top(\\mathbf{x}_i - \\bar{\\mathbf{x}})\n$$\n令 $\\tilde{y}_i = y_i - \\bar{y}$ 和 $\\tilde{\\mathbf{x}}_i = \\mathbf{x}_i - \\bar{\\mathbf{x}}$ 分别为中心化的目标值和输入。目标函数变为仅关于 $\\mathbf{w}$ 的函数：\n$$\nJ(\\mathbf{w}) = \\sum_{i=1}^{N} (\\tilde{y}_i - \\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i)^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}\n$$\n用矩阵表示法，令 $\\tilde{X}$ 为一个 $N \\times d$ 矩阵，其行是 $\\tilde{\\mathbf{x}}_i^\\top$，$\\tilde{\\mathbf{y}}$ 是一个包含 $\\tilde{y}_i$ 的 $N \\times 1$ 向量。目标函数为：\n$$\nJ(\\mathbf{w}) = (\\tilde{\\mathbf{y}} - \\tilde{X}\\mathbf{w})^\\top(\\tilde{\\mathbf{y}} - \\tilde{X}\\mathbf{w}) + \\lambda \\mathbf{w}^\\top \\mathbf{w}\n$$\n对 $\\mathbf{w}$ 求梯度：\n$$\n\\nabla_{\\mathbf{w}} J = -2\\tilde{X}^\\top\\tilde{\\mathbf{y}} + 2\\tilde{X}^\\top\\tilde{X}\\mathbf{w} + 2\\lambda\\mathbf{w}\n$$\n将梯度设为零以求最优的 $\\mathbf{w}$：\n$$\n\\tilde{X}^\\top\\tilde{X}\\mathbf{w} + \\lambda\\mathbf{w} = \\tilde{X}^\\top\\tilde{\\mathbf{y}} \\implies (\\tilde{X}^\\top\\tilde{X} + \\lambda I_d)\\mathbf{w} = \\tilde{X}^\\top\\tilde{\\mathbf{y}}\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。权重向量的解为：\n$$\n\\mathbf{w} = (\\tilde{X}^\\top\\tilde{X} + \\lambda I_d)^{-1} \\tilde{X}^\\top\\tilde{\\mathbf{y}}\n$$\n这就是著名的岭回归解。\n\n现在我们分析大正则化强度的影响，即当 $\\lambda \\to \\infty$ 时。在待求逆的矩阵中，$\\lambda I_d$ 项将占主导地位。\n$$\n\\lim_{\\lambda \\to \\infty} (\\tilde{X}^\\top\\tilde{X} + \\lambda I_d) = \\lim_{\\lambda \\to \\infty} \\lambda \\left(\\frac{1}{\\lambda}\\tilde{X}^\\top\\tilde{X} + I_d\\right) \\approx \\lambda I_d\n$$\n那么逆矩阵可以近似为：\n$$\n(\\tilde{X}^\\top\\tilde{X} + \\lambda I_d)^{-1} \\approx (\\lambda I_d)^{-1} = \\frac{1}{\\lambda}I_d\n$$\n将其代回到 $\\mathbf{w}$ 的解中：\n$$\n\\mathbf{w} \\approx \\left(\\frac{1}{\\lambda}I_d\\right) \\tilde{X}^\\top\\tilde{\\mathbf{y}} = \\frac{1}{\\lambda} (\\tilde{X}^\\top\\tilde{\\mathbf{y}})\n$$\n当 $\\lambda \\to \\infty$ 时，项 $\\frac{1}{\\lambda} \\to 0$，因此，权重向量 $\\mathbf{w}$ 向零向量收缩：\n$$\n\\lim_{\\lambda \\to \\infty} \\mathbf{w} = \\mathbf{0}\n$$\n随着权重向量趋近于零，我们来考察其对偏置项和整体预测的影响。最优偏置为 $b = \\bar{y} - \\mathbf{w}^\\top \\bar{\\mathbf{x}}$。当 $\\mathbf{w} \\to \\mathbf{0}$ 时，项 $\\mathbf{w}^\\top \\bar{\\mathbf{x}} \\to 0$，偏置收敛于目标值的均值：\n$$\n\\lim_{\\lambda \\to \\infty} b = \\bar{y}\n$$\n对任意输入 $\\mathbf{x}_i$ 的预测为 $\\hat{y}_i = \\mathbf{w}^\\top \\mathbf{x}_i + b$。在大 $\\lambda$ 的极限情况下：\n$$\n\\lim_{\\lambda \\to \\infty} \\hat{y}_i = (\\lim_{\\lambda \\to \\infty} \\mathbf{w})^\\top \\mathbf{x}_i + (\\lim_{\\lambda \\to \\infty} b) = \\mathbf{0}^\\top \\mathbf{x}_i + \\bar{y} = \\bar{y}\n$$\n这证明了对于强 $L_2$ 正则化，模型对所有输入的预测都收敛到一个常数值，即目标变量的样本均值 $\\bar{y}$。模型实际上忽略了输入特征 $\\mathbf{x}_i$，这是一个明显的欠拟合案例。净输入 $\\mathbf{w}^\\top \\mathbf{x}_i + b$ 由偏置项 $b$ 主导，而依赖于特征的部分 $\\mathbf{w}^\\top \\mathbf{x}_i$ 则趋于消失。因此，比较线性项与偏置项大小的优势比 $R$ 将趋近于 0。\n\n实现上将通过为增广参数向量 $\\boldsymbol{\\theta} = [\\mathbf{w}^\\top, b]^\\top$ 构建并求解一个单一线性系统来求解参数。这种方法在数学上与上面推导的方法等价。令 $X_{aug} = [X, \\mathbf{1}]$ 为增广了全一列的设计矩阵。目标函数可以写成 $J(\\boldsymbol{\\theta}) = (\\mathbf{y} - X_{aug}\\boldsymbol{\\theta})^\\top(\\mathbf{y} - X_{aug}\\boldsymbol{\\theta}) + \\boldsymbol{\\theta}^\\top \\Lambda_R \\boldsymbol{\\theta}$，其中 $\\Lambda_R$ 是一个 $(d+1) \\times (d+1)$ 的矩阵，其左上角块为 $\\lambda I_d$，其余部分为零。将梯度设为零可得到线性系统 $(X_{aug}^\\top X_{aug} + \\Lambda_R)\\boldsymbol{\\theta} = X_{aug}^\\top \\mathbf{y}$，可以通过数值方法求解 $\\boldsymbol{\\theta}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the weights and bias of a linear model with L2 regularization\n    on the weights only, and computes the dominance ratio R for several test cases.\n    \"\"\"\n\n    def train_and_compute_ratio(X, y, lambda_reg):\n        \"\"\"\n        Trains the model and computes the dominance ratio R.\n\n        The model is y_hat = X @ w + b.\n        The objective is ||y - (X @ w + b)||^2_2 + lambda * ||w||^2_2.\n        \n        This is solved by setting up an augmented linear system:\n        Let theta = [w, b]^T and X_aug = [X, 1].\n        The objective is ||y - X_aug @ theta||^2_2 + theta^T @ Lambda_mat @ theta,\n        where Lambda_mat is a diagonal matrix with lambda on the first d entries\n        and 0 for the bias term.\n        \n        The normal equation is (X_aug^T @ X_aug + Lambda_mat) @ theta = X_aug^T @ y.\n        \"\"\"\n        N, d = X.shape\n\n        # Construct the augmented design matrix [X, 1]\n        X_aug = np.hstack([X, np.ones((N, 1))])\n\n        # Construct the regularization matrix Lambda_mat\n        # It has shape (d+1)x(d+1) with lambda on the first d diagonal elements\n        # and 0 on the last diagonal element (for the unregularized bias).\n        lambda_mat = np.zeros((d + 1, d + 1))\n        lambda_mat[:d, :d] = lambda_reg * np.identity(d)\n        \n        # Form the linear system A * theta = B\n        A = X_aug.T @ X_aug + lambda_mat\n        B = X_aug.T @ y\n\n        # Solve for theta = [w, b]^T\n        try:\n            theta = np.linalg.solve(A, B)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for singular matrices (e.g., lambda=0 and rank-deficient X)\n            A_pinv = np.linalg.pinv(A)\n            theta = A_pinv @ B\n\n        # Extract weights w and bias b\n        w = theta[:d]\n        b = theta[d]\n\n        # Compute the dominance ratio R\n        # R = max(|w^T @ x_i|) / (|b| + epsilon)\n        linear_contribution = np.abs(X @ w)\n        max_abs_linear_contribution = np.max(linear_contribution) if N  0 else 0.0\n        \n        dominance_ratio = max_abs_linear_contribution / (np.abs(b) + 1e-12)\n        \n        return dominance_ratio\n\n    # --- Test Case Definitions ---\n\n    # Case 1  2  4  5 data setup\n    X1 = np.array([\n        [-2.0, 1.0, 0.0],\n        [-1.0, 0.0, 1.0],\n        [ 0.0, 1.0, -1.0],\n        [ 1.0, -1.0, 2.0],\n        [ 2.0, 2.0, -2.0]\n    ])\n    a = np.array([2.0, -1.0, 0.5])\n    c = 3.0\n    epsilon = np.array([0.0, -0.1, 0.1, -0.05, 0.05])\n    \n    # --- Execute Test Cases ---\n    \n    results = []\n\n    # Case 1: Strong regularization\n    lambda1 = 10000.0\n    y1 = X1 @ a + c + epsilon\n    r1 = train_and_compute_ratio(X1, y1, lambda1)\n    results.append(r1)\n\n    # Case 2: No regularization\n    lambda2 = 0.0\n    y2 = y1 # Same data as case 1\n    r2 = train_and_compute_ratio(X1, y2, lambda2)\n    results.append(r2)\n\n    # Case 3: Zero features\n    X3 = np.zeros((4, 3))\n    y3 = np.array([1.0, 2.0, 3.0, 4.0])\n    lambda3 = 500.0\n    r3 = train_and_compute_ratio(X3, y3, lambda3)\n    results.append(r3)\n\n    # Case 4: Tiny feature scale\n    X4 = X1 * 1e-6\n    y4 = X4 @ a + c + epsilon\n    lambda4 = 1.0\n    r4 = train_and_compute_ratio(X4, y4, lambda4)\n    results.append(r4)\n\n    # Case 5: Large feature scale\n    X5 = X1 * 100.0\n    y5 = X5 @ a + c + epsilon\n    lambda5 = 10.0\n    r5 = train_and_compute_ratio(X5, y5, lambda5)\n    results.append(r5)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3199770"}, {"introduction": "现在，我们来探讨一个在模型训练完成后的实用技术：模型校准。模型校准旨在微调模型的输出概率，使其更可靠、更接近真实的置信度。本练习将向你展示一种高效的方法，即只调整输出层的偏置项。你将亲手推导softmax交叉熵损失函数相对于偏置的梯度，并实现一个简单的优化过程，以观察仅调整偏置如何能显著改善模型的性能 [@problem_id:3199738]。", "problem": "给定一个多类线性-logit分类器，其对于输入向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 的类别 $k$ 的净输入函数定义为 $z_k(\\mathbf{x}) = \\mathbf{w}_k^\\top \\mathbf{x} + b_k$，其中 $\\mathbf{w}_k \\in \\mathbb{R}^d$ 是固定的权重，$b_k \\in \\mathbb{R}$ 是特定于类的偏置。预测的类概率由 softmax 函数定义：$p_k(\\mathbf{x}) = \\exp(z_k(\\mathbf{x}))/\\sum_{j=1}^K \\exp(z_j(\\mathbf{x}))$。对于单个带标签样本 $(\\mathbf{x}, y)$，其中 $y \\in \\{0,1,\\dots,K-1\\}$，交叉熵损失为 $\\ell(\\mathbf{x}, y) = -\\log p_{y}(\\mathbf{x})$。\n\n任务 A (推导)：从上述净输入函数、softmax 函数和交叉熵损失的定义出发，推导单个带标签样本的损失函数相对于偏置 $b_k$ 的梯度。您的推导必须从这些定义开始，并遵循微积分的基本原理，包括乘法法则和链式法则。最终表达式必须适合用于偏置的算法优化。请勿使用简便公式或预先推导出的结果。\n\n任务 B (算法校准与测量)：考虑一种事后校准方法，该方法仅调整偏置 $b_k$，同时保持所有权重 $\\mathbf{w}_k$ 固定。对于一个校准数据集，其 logits 矩阵为 $L^{\\text{val}} \\in \\mathbb{R}^{N_{\\text{val}} \\times K}$，标签为 $\\mathbf{y}^{\\text{val}} \\in \\{0,1,\\dots,K-1\\}^{N_{\\text{val}}}$，将校准后的 logits 定义为 $L^{\\text{val}} + \\mathbf{1} \\mathbf{b}^\\top$，其中 $\\mathbf{b} \\in \\mathbb{R}^K$ 是待学习的偏置向量，$\\mathbf{1} \\in \\mathbb{R}^{N_{\\text{val}}}$ 是全一向量。校准目标是校准数据集上的平均交叉熵。学习到 $\\mathbf{b}$ 后，在一个独立的测试数据集（logits 为 $L^{\\text{test}} \\in \\mathbb{R}^{N_{\\text{test}} \\times K}$，标签为 $\\mathbf{y}^{\\text{test}} \\in \\{0,1,\\dots,K-1\\}^{N_{\\text{test}}}$）上评估校准效果。评估方法是计算测试数据集上平均负对数似然（交叉熵）的变化量，该变化量定义为校准前的平均交叉熵减去将学习到的偏置应用于 $L^{\\text{test}}$ 后的平均交叉熵。将此变化量表示为一个实数（浮点数）。\n\n您必须实现一个完整、可运行的程序，该程序：\n- 将任务 A 中推导出的梯度转化为用于优化任务 B 中 $\\mathbf{b}$ 的算法形式。\n- 对每个校准数据集，使用基于梯度的方法通过最小化平均交叉熵来优化 $\\mathbf{b}$。\n- 按照上述方法计算在测试数据集上的提升量，并四舍五入到 $6$ 位小数。\n\n使用以下数据集测试套件。每个案例由 $(L^{\\text{val}}, \\mathbf{y}^{\\text{val}}, L^{\\text{test}}, \\mathbf{y}^{\\text{test}})$ 定义，其中 $K = 3$：\n\n案例 1 (一般情况)：\n$$\nL^{\\text{val}(1)} = \\begin{bmatrix}\n2.0  1.0  0.0\\\\\n1.5  -0.5  0.0\\\\\n0.0  1.0  1.0\\\\\n1.0  0.5  -1.0\\\\\n-0.5  0.0  0.5\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(1)} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\\\ 0\\\\ 2 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(1)} = \\begin{bmatrix}\n1.8  1.2  0.0\\\\\n1.4  -0.3  0.1\\\\\n0.1  0.9  1.0\\\\\n0.9  0.4  -0.8\\\\\n-0.6  0.1  0.6\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(1)} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\\\ 0\\\\ 2 \\end{bmatrix}.\n$$\n\n案例 2 (近乎完美的预测)：\n$$\nL^{\\text{val}(2)} = \\begin{bmatrix}\n3.0  0.0  -1.0\\\\\n0.0  3.0  -1.0\\\\\n-1.0  0.0  3.0\\\\\n3.5  -0.5  0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 2\\\\ 0 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(2)} = \\begin{bmatrix}\n3.0  0.0  -1.0\\\\\n0.0  3.0  -1.0\\\\\n-1.0  0.0  3.0\\\\\n3.5  -0.5  0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 2\\\\ 0 \\end{bmatrix}.\n$$\n\n案例 3 (无信息量的 logits，不平衡的标签)：\n$$\nL^{\\text{val}(3)} = \\begin{bmatrix}\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(3)} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 1\\\\ 1\\\\ 2 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(3)} = \\begin{bmatrix}\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(3)} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 1\\\\ 1\\\\ 2 \\end{bmatrix}.\n$$\n\n最终输出格式：您的程序应生成单行输出，其中包含案例 1、案例 2 和案例 3 各自的提升量，四舍五入到 6 位小数，以逗号分隔的列表形式包含在方括号中，例如 $[0.012345,0.000000,0.086421]$。", "solution": "该问题要求推导交叉熵损失相对于模型偏置的梯度，并为多类线性-logit分类器实现一个偏置校准程序。该问题定义明确，科学上合理，并为获得唯一解提供了所有必要信息。\n\n### A部分：梯度的推导\n\n第一个任务是推导交叉熵损失 $\\ell$ 相对于特定类别偏置 $b_k$ 的梯度。推导将从所提供的基本原理出发。\n\n对于单个带标签样本 $(\\mathbf{x}, y)$，交叉熵损失定义为：\n$$\n\\ell(\\mathbf{x}, y) = -\\log p_{y}(\\mathbf{x})\n$$\n其中 $y$ 是真实类别索引，$p_{y}(\\mathbf{x})$ 是真实类别的预测概率。概率由 softmax 函数给出：\n$$\np_k(\\mathbf{x}) = \\frac{\\exp(z_k(\\mathbf{x}))}{\\sum_{j=1}^K \\exp(z_j(\\mathbf{x}))}\n$$\n类别 $k$ 的净输入函数（或称 logit）为：\n$$\nz_k(\\mathbf{x}) = \\mathbf{w}_k^\\top \\mathbf{x} + b_k\n$$\n将 softmax 的定义代入损失函数可得：\n$$\n\\ell = -\\log \\left( \\frac{\\exp(z_y)}{\\sum_{j=1}^K \\exp(z_j)} \\right)\n$$\n使用对数性质 $\\log(a/b) = \\log(a) - \\log(b)$，损失函数可以重写为：\n$$\n\\ell = - \\left( \\log(\\exp(z_y)) - \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right)\n$$\n由于 $\\log(\\exp(u)) = u$，上式可简化为 log-sum-exp 形式：\n$$\n\\ell = -z_y + \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right)\n$$\n我们要求解 $\\ell$ 相对于一个偏置项 $b_k$ 的偏导数，其中 $k$ 是任意类别 $k \\in \\{0, 1, \\dots, K-1\\}$。根据求导的加法法则：\n$$\n\\frac{\\partial \\ell}{\\partial b_k} = \\frac{\\partial}{\\partial b_k} \\left( -z_y + \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right) = -\\frac{\\partial z_y}{\\partial b_k} + \\frac{\\partial}{\\partial b_k}\\left( \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right)\n$$\n我们分别计算每一项。\n\n对于第一项，我们对 $z_y$ 求关于 $b_k$ 的导数。净输入 $z_j = \\mathbf{w}_j^\\top \\mathbf{x} + b_j$ 仅在 $j=k$ 时才依赖于 $b_k$。因此：\n$$\n\\frac{\\partial z_j}{\\partial b_k} = \\delta_{jk}\n$$\n其中 $\\delta_{jk}$ 是克罗内克δ函数，当 $j=k$ 时为 $1$，否则为 $0$。将其应用于 $z_y$：\n$$\n\\frac{\\partial z_y}{\\partial b_k} = \\delta_{yk}\n$$\n对于第二项，我们应用链式法则。令 $S = \\sum_{j=1}^K \\exp(z_j)$。该项为 $\\log(S)$。\n$$\n\\frac{\\partial}{\\partial b_k} \\log(S) = \\frac{1}{S} \\cdot \\frac{\\partial S}{\\partial b_k}\n$$\n现在，我们求 $S$ 相对于 $b_k$ 的导数：\n$$\n\\frac{\\partial S}{\\partial b_k} = \\frac{\\partial}{\\partial b_k} \\left( \\sum_{j=1}^K \\exp(z_j) \\right) = \\sum_{j=1}^K \\frac{\\partial}{\\partial b_k} \\exp(z_j)\n$$\n对和中的每一项应用链式法则：\n$$\n\\frac{\\partial}{\\partial b_k} \\exp(z_j) = \\exp(z_j) \\cdot \\frac{\\partial z_j}{\\partial b_k} = \\exp(z_j) \\cdot \\delta_{jk}\n$$\n求和式 $\\sum_{j=1}^K \\exp(z_j) \\cdot \\delta_{jk}$ 中仅包含一个非零项，即 $j=k$ 时。因此，该和式简化为 $\\exp(z_k)$。\n$$\n\\frac{\\partial S}{\\partial b_k} = \\exp(z_k)\n$$\n将此结果代回到对数项的导数表达式中：\n$$\n\\frac{\\partial}{\\partial b_k} \\log(S) = \\frac{1}{S} \\cdot \\exp(z_k) = \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)} = p_k(\\mathbf{x})\n$$\n最后，我们将两部分组合起来，得到损失 $\\ell$ 相对于偏置 $b_k$ 的完整梯度：\n$$\n\\frac{\\partial \\ell}{\\partial b_k} = p_k(\\mathbf{x}) - \\delta_{yk}\n$$\n这个表达式是单个样本的梯度。如果 $k$ 是正确类别（$k=y$），梯度为 $p_y - 1$。如果 $k$ 是任何其他类别（$k \\neq y$），梯度为 $p_k$。以向量形式表示，即为 $\\nabla_{\\mathbf{b}}\\ell = \\mathbf{p} - \\mathbf{e}_y$，其中 $\\mathbf{p}$ 是 softmax 概率向量，$\\mathbf{e}_y$ 是真实标签 $y$ 的 one-hot 编码向量。\n\n### B部分：算法实现\n\n第二个任务是执行事后偏置校准并测量由此带来的提升。这涉及到优化偏置 $\\mathbf{b}$ 以最小化校准数据集上的平均交叉熵损失。\n\n需要最小化的目标函数是 $N_{\\text{val}}$ 个样本的平均交叉熵：\n$$\n\\mathcal{L}(\\mathbf{b}) = \\frac{1}{N_{\\text{val}}} \\sum_{i=1}^{N_{\\text{val}}} \\ell_i(\\mathbf{b})\n$$\n其中 $\\ell_i(\\mathbf{b})$ 是第 $i$ 个样本的损失，其校准后的 logits 为 $z_{ik} = L^{\\text{val}}_{ik} + b_k$。该目标函数相对于 $b_k$ 的梯度是各个梯度的平均值：\n$$\n\\nabla_{b_k} \\mathcal{L}(\\mathbf{b}) = \\frac{1}{N_{\\text{val}}} \\sum_{i=1}^{N_{\\text{val}}} (p_{ik} - \\delta_{y_i k})\n$$\n其中 $p_{ik}$ 是使用校准后 logits 计算出的样本 $i$ 属于类别 $k$ 的 softmax 概率，而 $y_i$ 是样本 $i$ 的真实标签。\n\n我们将使用一个基于梯度的优化算法，具体是 `scipy.optimize.minimize` 提供的 L-BFGS-B 算法，来找到最小化 $\\mathcal{L}(\\mathbf{b})$ 的最优偏置向量 $\\mathbf{b}^*$。对于像这样的平滑、无约束（或箱式约束）的优化问题，这是一种鲁棒且高效的方法。偏置的初始猜测值将为 $\\mathbf{b} = \\mathbf{0}$。\n\n为了数值稳定性，交叉熵损失和 softmax 的计算将采用 log-sum-exp 技巧。单个样本的交叉熵损失计算为 $\\ell_i = \\log\\left(\\sum_j \\exp(z_{ij})\\right) - z_{iy_i}$。这可以避免处理较大或较小的 logit 值时出现浮点数上溢和下溢问题。\n\n在使用校准集 $(L^{\\text{val}}, \\mathbf{y}^{\\text{val}})$ 确定最优偏置向量 $\\mathbf{b}^*$ 后，我们在测试集 $(L^{\\text{test}}, \\mathbf{y}^{\\text{test}})$ 上评估其性能。评估指标是平均负对数似然 (NLL) 的变化量：\n$$\n\\Delta_{\\text{NLL}} = \\text{NLL}_{\\text{before}} - \\text{NLL}_{\\text{after}}\n$$\n其中：\n- $\\text{NLL}_{\\text{before}}$ 是使用原始 logits $L^{\\text{test}}$ 在测试集上的平均交叉熵。\n- $\\text{NLL}_{\\text{after}}$ 是使用校准后 logits $L^{\\text{test}} + \\mathbf{1}(\\mathbf{b}^*)^\\top$ 在测试集上的平均交叉熵。\n\n$\\Delta_{\\text{NLL}}$ 的正值表示模型校准有所改善，即调整偏置后，预测的概率更加准确。最终的程序将为所提供的三个测试案例中的每一个实现此过程。", "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax, logsumexp\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the bias calibration problem for the three given test cases.\n    \"\"\"\n    \n    # CASE 1\n    L_val_1 = np.array([\n        [2.0, 1.0, 0.0],\n        [1.5, -0.5, 0.0],\n        [0.0, 1.0, 1.0],\n        [1.0, 0.5, -1.0],\n        [-0.5, 0.0, 0.5]\n    ])\n    y_val_1 = np.array([0, 0, 2, 0, 2])\n    L_test_1 = np.array([\n        [1.8, 1.2, 0.0],\n        [1.4, -0.3, 0.1],\n        [0.1, 0.9, 1.0],\n        [0.9, 0.4, -0.8],\n        [-0.6, 0.1, 0.6]\n    ])\n    y_test_1 = np.array([0, 0, 2, 0, 2])\n\n    # CASE 2\n    L_val_2 = np.array([\n        [3.0, 0.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-1.0, 0.0, 3.0],\n        [3.5, -0.5, 0.0]\n    ])\n    y_val_2 = np.array([0, 1, 2, 0])\n    L_test_2 = np.array([\n        [3.0, 0.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-1.0, 0.0, 3.0],\n        [3.5, -0.5, 0.0]\n    ])\n    y_test_2 = np.array([0, 1, 2, 0])\n\n    # CASE 3\n    L_val_3 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y_val_3 = np.array([0, 0, 0, 1, 1, 2])\n    L_test_3 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y_test_3 = np.array([0, 0, 0, 1, 1, 2])\n\n    test_cases = [\n        (L_val_1, y_val_1, L_test_1, y_test_1),\n        (L_val_2, y_val_2, L_test_2, y_test_2),\n        (L_val_3, y_val_3, L_test_3, y_test_3)\n    ]\n\n    results = []\n\n    def avg_cross_entropy_loss(logits, labels):\n        \"\"\"\n        Computes the average cross-entropy loss using the log-sum-exp trick\n        for numerical stability.\n        \"\"\"\n        num_samples = logits.shape[0]\n        # Log-sum-exp over classes for each sample\n        lse = logsumexp(logits, axis=1)\n        # Get the logits for the true classes\n        true_class_logits = logits[np.arange(num_samples), labels]\n        # Negative log-likelihood for each sample\n        nll = lse - true_class_logits\n        return np.mean(nll)\n\n    def objective_function(b, L, y):\n        \"\"\"\n        The objective function to minimize: average cross-entropy on the\n        calibration set with the current bias vector b.\n        \"\"\"\n        calibrated_logits = L + b\n        return avg_cross_entropy_loss(calibrated_logits, y)\n\n    def gradient_function(b, L, y):\n        \"\"\"\n        The gradient of the objective function with respect to the bias vector b.\n        \"\"\"\n        num_samples, num_classes = L.shape\n        calibrated_logits = L + b\n        \n        # Calculate softmax probabilities\n        probs = softmax(calibrated_logits, axis=1)\n        \n        # Create one-hot encoded labels\n        y_one_hot = np.zeros((num_samples, num_classes))\n        y_one_hot[np.arange(num_samples), y] = 1.0\n        \n        # Gradient is the average of (probs - one_hot_labels) over samples\n        grad = np.mean(probs - y_one_hot, axis=0)\n        return grad\n\n    for L_val, y_val, L_test, y_test in test_cases:\n        # Number of classes\n        K = L_val.shape[1]\n        \n        # Initial guess for the bias vector\n        b_initial = np.zeros(K)\n        \n        # Optimize the biases using the validation set\n        opt_result = minimize(\n            fun=objective_function,\n            x0=b_initial,\n            args=(L_val, y_val),\n            method='L-BFGS-B',\n            jac=gradient_function\n        )\n        b_optimal = opt_result.x\n        \n        # Evaluate on the test set\n        # 1. NLL before calibration\n        nll_before = avg_cross_entropy_loss(L_test, y_test)\n        \n        # 2. NLL after calibration\n        calibrated_L_test = L_test + b_optimal\n        nll_after = avg_cross_entropy_loss(calibrated_L_test, y_test)\n        \n        # 3. Compute the improvement\n        improvement = nll_before - nll_after\n        \n        results.append(f\"{improvement:.6f}\")\n\n    # Print the final results in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3199738"}]}