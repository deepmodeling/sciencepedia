{"hands_on_practices": [{"introduction": "一个模型的能力边界和其局限性同样重要。线性分类器虽然强大且高效，但并非万能。这个练习将带你探讨一个经典案例——异或（XOR）问题，它揭示了单个线性单元的根本局限性。通过计算被称为“周氏参数” (Chow Parameters) 的一组描述符，你将能够从数学上严格证明，像奇偶校验这样的函数为何无法被一条直线（或一个超平面）分开，从而为理解为何需要更复杂的模型（如多层神经网络）奠定基础。 [@problem_id:1916478]", "problem": "在模式分类和阈值逻辑的背景下，一个具有二元输入 $x_i \\in \\{0, 1\\}$ 的布尔函数 $f(x_1, x_2, \\ldots, x_n)$ 被定义为**线性可分**的，如果存在一组实值权重 $(w_1, w_2, \\ldots, w_n)$ 和一个实值阈值 $T$，使得当且仅当其输入的加权和达到或超过该阈值时，该函数的值为 1。也就是说，$f(x_1, \\ldots, x_n) = 1 \\iff \\sum_{i=1}^{n} w_i x_i \\ge T$。\n\n对于任何布尔函数，都可以计算一组称为**Chow 参数**的值来分析其性质。对于函数 $f(x_1, \\ldots, x_n)$，这些参数定义如下：\n1.  $C_0(f)$: 使 $f=1$ 的输入向量（最小项）的总数。\n2.  $C_i(f)$，对于 $i \\in \\{1, \\ldots, n\\}$: 使 $f=1$ 且输入变量 $x_i=1$ 的输入向量的数量。\n\n考虑4变量奇校验函数，定义为 $f(x_1, x_2, x_3, x_4) = x_1 \\oplus x_2 \\oplus x_3 \\oplus x_4$，其中 $\\oplus$ 表示异或运算。\n\n您的任务是计算该函数的 Chow 参数向量，顺序为 $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f))$，并基于形式化分析，判断该函数是否线性可分。选择正确提供 Chow 参数向量和可分性结论的选项。\n\nA. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (8, 4, 4, 4, 4)$; 线性可分。\n\nB. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (8, 4, 4, 4, 4)$; 非线性可分。\n\nC. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (4, 1, 1, 1, 1)$; 非线性可分。\n\nD. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (16, 8, 8, 8, 8)$; 线性可分。\n\nE. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (8, 3, 3, 3, 3)$; 非线性可分。", "solution": "我们已知4变量奇校验函数 $f(x_{1},x_{2},x_{3},x_{4})=x_{1}\\oplus x_{2}\\oplus x_{3}\\oplus x_{4}$，当且仅当 $(x_{1},x_{2},x_{3},x_{4})$ 的汉明权重（1的个数）为奇数时，该函数等于 $1$。\n\n首先，计算 $C_{0}(f)$，即 $f=1$ 的输入数量。对于 $n=4$，具有奇数汉明权重的赋值数量为\n$$\n\\sum_{k\\ \\text{odd}} \\binom{4}{k}=\\binom{4}{1}+\\binom{4}{3}=4+4=8.\n$$\n因此 $C_{0}(f)=8$。\n\n接下来，对于每个 $i\\in\\{1,2,3,4\\}$，计算 $C_{i}(f)$，即 $f=1$ 且 $x_{i}=1$ 的输入数量。固定 $i$ 并以 $x_{i}=1$ 为条件。剩下三个变量有 $2^{3}=8$ 种赋值。将 $f$ 写为 $f=x_{i}\\oplus(x_{j}\\oplus x_{k}\\oplus x_{\\ell})$，当 $x_{i}=1$ 时，我们有 $f=1$ 当且仅当剩下三个变量的奇偶性为偶数。三个变量中偶校验赋值的数量为\n$$\n\\sum_{j\\ \\text{even}} \\binom{3}{j}=\\binom{3}{0}+\\binom{3}{2}=1+3=4.\n$$\n因此，对于每个 $i\\in\\{1,2,3,4\\}$，$C_{i}(f)=4$。所以 Chow 参数向量是 $(C_{0},C_{1},C_{2},C_{3},C_{4})=(8,4,4,4,4)$。\n\n现在判断线性可分性。用反证法，假设存在权重 $w\\in\\mathbb{R}^{4}$ 和阈值 $T\\in\\mathbb{R}$，使得 $f(x)=1$ 当且仅当 $w\\cdot x\\geq T$，因此 $f(x)=0$ 当且仅当 $w\\cdot x  T$。可以证明，对于奇偶校验函数，这样的权重和阈值不存在，因此它不是线性可分的。", "answer": "$$\\boxed{B}$$", "id": "1916478"}, {"introduction": "认识到线性模型的局限性后，下一步自然是探索如何突破这些限制。这个练习紧接着上一个异或（XOR）问题的挑战，为你展示了解决之道的核心思想：特征变换。你将发现，通过向原始数据中添加一个简单的非线性“交互”特征（即 $x_1 x_2$），原本线性不可分的数据集在一个更高维的特征空间中变得线性可分。这个过程生动地诠释了特征工程的力量，也是深度学习网络通过其隐藏层自动学习有效数据表示的基本原理。 [@problem_id:3144385]", "problem": "考虑一个在 $\\mathbb{R}^{2}$ 中的二元分类任务，其输入 $x = (x_1, x_2)$ 被约束在集合 $\\{(-1,-1), (-1,1), (1,-1), (1,1)\\}$ 中。标签由一个小的非线性交互作用决定：对于一个固定的 $\\delta$ 且 $0  \\delta  1$，定义\n$$\ny(x) = \\mathrm{sign}\\!\\big(x_1 x_2 + \\delta\\big),\n$$\n其中如果 $t > 0$，则 $\\mathrm{sign}(t) = 1$，如果 $t  0$，则 $\\mathrm{sign}(t) = -1$。换句话说，$x_1 x_2 = 1$ 的点 $x$ 具有标签 $+1$，而 $x_1 x_2 = -1$ 的点具有标签 $-1$。一个单层线性分类器的决策函数为 $f(x) = w_1 x_1 + w_2 x_2 + b$，其参数为 $(w_1, w_2, b) \\in \\mathbb{R}^{3}$。\n\n从线性可分性的定义（即存在 $w \\in \\mathbb{R}^{2}$ 和 $b \\in \\mathbb{R}$，使得对于所有带有标签 $y_i$ 的训练点 $x_i$，都有 $y_i\\,(w^\\top x_i + b) > 0$）出发，首先确定此数据集在原始输入空间中对于 $f(x)$ 是否是线性可分的。然后，考虑用 $m$ 个乘法交互单元来增强分类器，每个单元计算标量特征 $z_k(x) = x_1 x_2$（其中 $k = 1, \\dots, m$），并构成增强后的线性决策函数\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\sum_{k=1}^{m} \\tilde{v}_k\\, z_k(x) + \\tilde{b},\n$$\n其参数为 $(\\tilde{w}_1, \\tilde{w}_2, \\tilde{v}_1, \\dots, \\tilde{v}_m, \\tilde{b}) \\in \\mathbb{R}^{m+3}$。确定最小整数 $m^{\\star}$，使得存在参数能让该数据集在增强后的特征空间中变得线性可分，即对于所有四个输入 $x$，都有 $y(x)\\, f_{m^{\\star}}(x) > 0$。你的答案必须是 $m^{\\star}$ 的值，以单个整数形式给出。无需四舍五入。", "solution": "用户希望我按照以下步骤解决问题：\n1.  **验证问题：** 问题陈述是有效的。\n    -   **给定条件：**\n        -   输入空间：$x = (x_1, x_2) \\in \\mathbb{R}^{2}$，其点来自集合 $\\{(-1,-1), (-1,1), (1,-1), (1,1)\\}$。\n        -   标签函数：$y(x) = \\mathrm{sign}(x_1 x_2 + \\delta)$，对于固定的 $\\delta$ 且 $0  \\delta  1$。\n        -   线性分类器：$f(x) = w_1 x_1 + w_2 x_2 + b$。\n        -   线性可分性条件：对于所有点 $(x_i, y_i)$，均有 $y_i (w^\\top x_i + b) > 0$。\n        -   增强特征单元：$z_k(x) = x_1 x_2$，其中 $k = 1, \\dots, m$。\n        -   增强分类器：$f_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\sum_{k=1}^{m} \\tilde{v}_k z_k(x) + \\tilde{b}$。\n    -   **验证：** 该问题是机器学习中一个关于线性模型表示能力的标准的、定义明确的练习，具体来说是异或（XOR）问题。所有术语都有正式定义，设置是一致的，并且它在科学上植根于神经网络理论。该问题是有效的。\n\n2.  **解决问题：**\n首先，我们使用给定的标签函数 $y(x) = \\mathrm{sign}(x_1 x_2 + \\delta)$（其中 $0  \\delta  1$）来确定四个输入点的标签。\n\n-   对于 $x^{(1)} = (1,1)$，$x_1 x_2 = 1$。$y^{(1)} = \\mathrm{sign}(1+\\delta) = +1$。\n-   对于 $x^{(2)} = (-1,-1)$，$x_1 x_2 = 1$。$y^{(2)} = \\mathrm{sign}(1+\\delta) = +1$。\n-   对于 $x^{(3)} = (1,-1)$，$x_1 x_2 = -1$。$y^{(3)} = \\mathrm{sign}(-1+\\delta) = -1$。\n-   对于 $x^{(4)} = (-1,1)$，$x_1 x_2 = -1$。$y^{(4)} = \\mathrm{sign}(-1+\\delta) = -1$。\n\n该数据集包含两个标签为 $+1$ 的点 $\\{(1,1), (-1,-1)\\}$，以及两个标签为 $-1$ 的点 $\\{(1,-1), (-1,1)\\}$。这就是经典的异或（XOR）问题。\n\n**第1部分：原始输入空间中的线性可分性**\n\n如果存在一个权重向量 $w = (w_1, w_2)$ 和一个偏置 $b$，使得对于所有四个点都有 $y_i (w^\\top x_i + b) > 0$，那么该数据集就是线性可分的。我们来写出针对我们数据集的不等式：\n1.  对于 $(x^{(1)}, y^{(1)}) = ((1,1), +1)$：$1 \\cdot (w_1(1) + w_2(1) + b) > 0 \\implies w_1 + w_2 + b > 0$。\n2.  对于 $(x^{(2)}, y^{(2)}) = ((-1,-1), +1)$：$1 \\cdot (w_1(-1) + w_2(-1) + b) > 0 \\implies -w_1 - w_2 + b > 0$。\n3.  对于 $(x^{(3)}, y^{(3)}) = ((1,-1), -1)$：$-1 \\cdot (w_1(1) + w_2(-1) + b) > 0 \\implies w_1 - w_2 + b  0$。\n4.  对于 $(x^{(4)}, y^{(4)}) = ((-1,1), -1)$：$-1 \\cdot (w_1(-1) + w_2(1) + b) > 0 \\implies -w_1 + w_2 + b  0$。\n\n从不等式(1)中，我们得到 $b > -w_1 - w_2$。\n从不等式(2)中，我们得到 $b > w_1 + w_2$。\n我们将不等式(1)和(2)相加：\n$(w_1 + w_2 + b) + (-w_1 - w_2 + b) > 0 \\implies 2b > 0 \\implies b > 0$。\n\n现在我们来考虑不等式(3)和(4)：\n从不等式(3)中，我们得到 $b  -w_1 + w_2$。\n从不等式(4)中，我们得到 $b  w_1 - w_2$。\n我们将不等式(3)和(4)相加（注意，两个“小于”不等式相加会保持不等式方向）：\n$(w_1 - w_2 + b) + (-w_1 + w_2 + b)  0 \\implies 2b  0 \\implies b  0$。\n\n我们推导出了条件 $b > 0$ 和 $b  0$，这是一个逻辑矛盾。因此，不存在这样的参数 $(w_1, w_2, b)$。该数据集在原始输入空间 $\\mathbb{R}^2$ 中不是线性可分的。\n\n**第2部分：增强特征空间中的线性可分性与最小 $m^{\\star}$**\n\n问题引入了一个增强后的线性决策函数：\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\sum_{k=1}^{m} \\tilde{v}_k\\, z_k(x) + \\tilde{b}\n$$\n其中每个交互单元计算相同的特征 $z_k(x) = x_1 x_2$。通过提取公因子特征 $x_1 x_2$，该函数可以重写为：\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\left(\\sum_{k=1}^{m} \\tilde{v}_k\\right) (x_1 x_2) + \\tilde{b}\n$$\n我们为新特征 $z(x) = x_1 x_2$ 定义一个有效权重 $W_v = \\sum_{k=1}^{m} \\tilde{v}_k$。现在，决策函数是在一个坐标为 $(x_1, x_2, x_1 x_2)$ 的三维特征空间中的线性函数：\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + W_v (x_1 x_2) + \\tilde{b}\n$$\n如果我们能找到满足条件 $y(x) f_m(x) > 0$（对于所有四个点）的参数 $(\\tilde{w}_1, \\tilde{w}_2, W_v, \\tilde{b})$，那么问题就变得线性可分。\n\n问题是找到能实现这一点的最小整数 $m$。能否实现可分性取决于有效权重 $W_v$ 能否被设为非零值。\n-   如果 $m=0$，和 $\\sum_{k=1}^{0} \\tilde{v}_k$ 是一个空和，其值为 $0$。因此 $W_v=0$。决策函数简化为 $f_0(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\tilde{b}$。这就是原始的线性分类器，我们已经证明它无法分离数据。因此，$m^{\\star}$ 必须大于 $0$。\n-   如果 $m \\ge 1$，我们可以选择权重 $\\tilde{v}_k$ 使它们的和 $W_v$ 非零。例如，如果 $m=1$，我们有 $W_v = \\tilde{v}_1$。我们可以选择 $\\tilde{v}_1 = 1$。如果 $m=2$，我们有 $W_v = \\tilde{v}_1 + \\tilde{v}_2$。我们可以选择 $\\tilde{v}_1 = 1$ 和 $\\tilde{v}_2 = 0$ 来得到 $W_v=1$。通常，对于任何 $m \\ge 1$，我们可以设置 $\\tilde{v}_1 = 1$ 和对于 $k>1$ 的 $\\tilde{v}_k=0$ 来实现 $W_v=1$。\n\n所以，当且仅当 $m \\ge 1$ 时，线性可分是可能的。让我们来证明当 $m=1$ 时确实是可能的。\n设 $m=1$。那么 $W_v = \\tilde{v}_1$。我们尝试找到一组有效的参数。考虑一个简单的选择：$\\tilde{w}_1 = 0$，$\\tilde{w}_2 = 0$，以及 $\\tilde{b} = 0$。\n决策函数变为 $f_1(x) = \\tilde{v}_1 (x_1 x_2)$。\n线性可分性的条件是 $y(x) \\cdot (\\tilde{v}_1 x_1 x_2) > 0$。\n\n我们来为两类点检查这个条件：\n-   类别 $+1$：点满足 $x_1 x_2 = 1$ 且标签为 $y=+1$。\n    条件是 $(+1) \\cdot (\\tilde{v}_1 \\cdot 1) > 0 \\implies \\tilde{v}_1 > 0$。\n-   类别 $-1$：点满足 $x_1 x_2 = -1$ 且标签为 $y=-1$。\n    条件是 $(-1) \\cdot (\\tilde{v}_1 \\cdot (-1)) > 0 \\implies \\tilde{v}_1 > 0$。\n\n两个类别都要求 $\\tilde{v}_1 > 0$。我们可以自由选择参数，因此可以设置 $\\tilde{v}_1 = 1$。这满足了条件。\n当 $m=1$ 时，我们可以选择参数 $(\\tilde{w}_1, \\tilde{w}_2, \\tilde{v}_1, \\tilde{b}) = (0, 0, 1, 0)$ 来使数据集线性可分。\n决策函数为 $f_1(x) = x_1 x_2$。\n对于 $y=+1$ 的点，$x_1x_2=1$，因此 $y \\cdot f_1(x) = 1 \\cdot 1 = 1 > 0$。\n对于 $y=-1$ 的点，$x_1x_2=-1$，因此 $y \\cdot f_1(x) = (-1) \\cdot (-1) = 1 > 0$。\n该条件对所有点都满足。\n\n由于 $m=0$ 不足而 $m=1$ 足够，使得数据集变得线性可分的最小整数值 $m^{\\star}$ 是 $1$。", "answer": "$$\\boxed{1}$$", "id": "3144385"}, {"introduction": "理论上，一旦数据集是线性可分的（无论是天生如此还是通过特征变换实现），我们就需要一种算法来实际找到那条“分界线”。这个练习将指导你从零开始实现逻辑回归，这是一种用于寻找分类超平面的基石算法。通过动手推导并编写梯度下降的优化过程，你不仅能掌握如何将数学模型转化为可执行的代码，还能深入理解正则化和特征标准化等在现代机器学习实践中至关重要的技术。 [@problem_id:3278883]", "problem": "您需要从第一性原理实现二元逻辑回归，通过最速下降法（也称梯度下降）进行训练，以获得一个用于线性可分数据集的分离超平面。请从以下原理出发：独立的二元标签被建模为伯努利随机变量，其成功概率通过线性预测器遵循逻辑斯谛链接；参数的选择通过最小化由负对数似然构造的正则化经验风险来实现。不要使用预构建的机器学习库；从基本定义推导所需的梯度，并显式地实现优化过程。\n\n程序必须：\n- 在二维空间中构造两个固定的确定性数据集。\n- 使用最速下降法，通过固定的学习率和迭代次数，最小化正则化经验风险。\n- 提供一个标准化特征的选项，以减少病态条件。\n- 将每个提供的测试用例的训练分类准确率报告为介于 $0$ 和 $1$ 之间的小数。\n\n您必须使用的基本原理：\n- 二元标签 $y_i \\in \\{0,1\\}$ 的独立伯努利建模。\n- 逻辑斯谛链接 $p_i = \\sigma(z_i)$，其中 $\\sigma$ 是逻辑斯谛S型函数，$z_i$ 是线性预测器。\n- 使用平均负对数似然加上权重的 $\\ell_2$ 惩罚项进行经验风险最小化。\n\n需采用的定义：\n- 对于输入 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 和参数 $(\\mathbf{w}, b)$，定义线性预测器 $z_i = \\mathbf{w}^\\top \\mathbf{x}_i + b$ 和逻辑斯谛S型函数 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$。\n- 定义正则化经验风险\n$$\n\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\Big( - y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\Big) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2,\n$$\n其中 $n$ 是样本数量，$\\lambda \\ge 0$ 是正则化参数。\n- 最速下降法通过以步长 $\\alpha > 0$ 沿 $\\mathcal{L}$ 梯度的反方向移动来更新参数，并执行固定的迭代次数。\n\n数据集：\n- 数据集 $D_1$（线性可分，中等尺度）。正类（$y=1$）点：\n$$\n\\{(2.5, 2.0), (3.0, 1.0), (2.0, 2.5), (3.5, 2.2), (2.2, 3.0), (2.8, 2.7), (3.2, 1.8), (2.4, 3.1)\\}.\n$$\n负类（$y=0$）点：\n$$\n\\{(-2.5, -1.5), (-3.0, -2.0), (-2.0, -2.2), (-3.2, -1.8), (-1.8, -2.5), (-2.7, -2.9), (-3.1, -1.7), (-2.3, -3.2)\\}.\n$$\n令 $X^{(1)}$ 为 $D_1$ 中所有点的堆叠，$y^{(1)}$ 为相应的标签。\n- 数据集 $D_2$（几何形状相同但条件不佳）。通过将 $X^{(1)}$ 的第二个特征乘以因子 $1000$ 来构造 $X^{(2)}$，即对于 $X^{(1)}$ 中的每个点 $(x_1, x_2)$，将 $(x_1, 1000 x_2)$ 加入 $X^{(2)}$。$y^{(2)}$ 使用与 $y^{(1)}$ 相同的标签。\n\n标准化选项：\n- 如果启用标准化，通过 $\\tilde{X}_{ij} = \\dfrac{X_{ij} - \\mu_j}{\\sigma_j}$ 将 $X$ 转换为 $\\tilde{X}$，其中 $\\mu_j$ 和 $\\sigma_j$ 是训练样本中特征 $j$ 的均值和标准差，并约定如果 $\\sigma_j = 0$ 则使用 $\\sigma_j = 1$。\n\n训练与预测：\n- 将 $\\mathbf{w}$ 初始化为 $\\mathbb{R}^2$ 中的零向量，将 $b$ 初始化为 $0$。\n- 使用学习率 $\\alpha$ 执行固定次数的最速下降迭代，以最小化 $\\mathcal{L}(\\mathbf{w}, b)$。\n- 训练后，使用规则 $\\hat{y}_i = 1$（如果 $\\sigma(z_i) \\ge 0.5$）和 $\\hat{y}_i = 0$（其他情况）来预测类别标签 $\\hat{y}_i$。\n- 将训练准确率计算为小数 $\\dfrac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n测试套件：\n提供以下四个测试用例，每个用例由元组 $(\\text{dataset}, \\alpha, \\text{iterations}, \\lambda, \\text{standardize})$ 指定，其中 $\\text{dataset} \\in \\{D_1, D_2\\}$，$\\alpha > 0$，$\\text{iterations} \\in \\mathbb{N}$，$\\lambda \\ge 0$，且 $\\text{standardize} \\in \\{\\text{True}, \\text{False}\\}$。\n- 用例 $1$（理想情况）：$(D_1, 0.1, 3000, 0.01, \\text{True})$。\n- 用例 $2$（边界条件：极小步长）：$(D_1, 0.0001, 200, 0.01, \\text{True})$。\n- 用例 $3$（边缘情况：不进行标准化的病态条件）：$(D_2, 0.0001, 3000, 0.01, \\text{False})$。\n- 用例 $4$（边缘情况：可能导致欠拟合的强正则化）：$(D_1, 0.1, 3000, 10.0, \\text{True})$。\n\n要求的最终输出格式：\n- 您的程序应生成一行输出，其中包含四个用例的训练分类准确率，格式为一个用方括号括起来的逗号分隔列表，每个准确率四舍五入到六位小数，例如 $$[a_1,a_2,a_3,a_4]$$ 其中每个 $a_k$ 都是十进制形式的浮点数。不应打印任何其他文本。", "solution": "该问题要求使用最速下降法（也称为梯度下降）来训练并实现一个二元逻辑回归模型。整个过程必须从第一性原理推导。在给出计算解决方案之前，我们必须形式化其底层的数学模型，并推导优化所需的方程。\n\n### 1. 概率模型与似然\n\n逻辑回归的基础是对二元结果进行概率建模。给定一个包含 $n$ 个样本的数据集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是一个特征向量，$y_i \\in \\{0, 1\\}$ 是对应的二元标签。\n\n我们将每个标签 $y_i$ 建模为一个独立的伯努利随机变量，$Y_i \\sim \\text{Bernoulli}(p_i)$，其中 $p_i$ 是“成功”结果的概率，即 $P(Y_i=1|\\mathbf{x}_i) = p_i$。\n\n逻辑回归的核心是特征向量 $\\mathbf{x}_i$ 和概率 $p_i$ 之间的“链接”。这是通过一个线性预测器 $z_i$ 和逻辑斯谛S型函数 $\\sigma(z)$ 实现的。\n线性预测器是输入特征的线性函数，由一个权重向量 $\\mathbf{w} \\in \\mathbb{R}^d$ 和一个偏置项 $b \\in \\mathbb{R}$ 参数化：\n$$ z_i = \\mathbf{w}^\\top \\mathbf{x}_i + b $$\n逻辑斯谛S型函数将这个无界的线性预测器 $z_i \\in \\mathbb{R}$ 映射到 $(0, 1)$ 范围内的有效概率：\n$$ p_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}} $$\n因此，对于第 $i$ 个样本，每个结果的概率由下式给出：\n$$ P(Y_i=y_i|\\mathbf{x}_i; \\mathbf{w}, b) = p_i^{y_i} (1 - p_i)^{1 - y_i} $$\n假设样本是独立同分布 (i.i.d.) 的，那么在给定特征 $X = (\\mathbf{x}_1, ..., \\mathbf{x}_n)$ 和参数 $(\\mathbf{w}, b)$ 的情况下，观测到整个标签集 $\\mathbf{y} = (y_1, ..., y_n)$ 的总似然是各个概率的乘积：\n$$ L(\\mathbf{w}, b) = \\prod_{i=1}^n P(Y_i=y_i|\\mathbf{x}_i; \\mathbf{w}, b) = \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1 - y_i} $$\n\n### 2. 目标函数：正则化经验风险\n\n最大似然估计 (MLE) 原理指导我们选择使似然 $L(\\mathbf{w}, b)$ 最大化的参数 $(\\mathbf{w}, b)$。在数学上，处理对数似然 $\\ell(\\mathbf{w}, b) = \\log L(\\mathbf{w}, b)$ 更为方便，因为它将乘积转换为和，并且不会改变最大值的位置。\n$$ \\ell(\\mathbf{w}, b) = \\sum_{i=1}^n \\log \\left( p_i^{y_i} (1 - p_i)^{1 - y_i} \\right) = \\sum_{i=1}^n \\left( y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right) $$\n代入 $p_i = \\sigma(z_i)$：\n$$ \\ell(\\mathbf{w}, b) = \\sum_{i=1}^n \\left( y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \\right) $$\n在机器学习中，我们通常将问题框架化为最小化一个损失或风险函数。最大化对数似然等价于最小化负对数似然。经验风险是数据集上的平均负对数似然：\n$$ \\mathcal{R}_{\\text{emp}}(\\mathbf{w}, b) = -\\frac{1}{n} \\ell(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\left( -y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\right) $$\n此函数也称为二元交叉熵损失。\n\n为了防止过拟合并提高泛化能力，我们在经验风险上增加了一个正则化项。问题指定了对权重的 $\\ell_2$ 惩罚项（也称为Tikhonov正则化或权重衰减），它惩罚较大的权重值。我们必须最小化的正则化经验风险是：\n$$ \\mathcal{L}(\\mathbf{w}, b) = \\mathcal{R}_{\\text{emp}}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2 $$\n$$ \\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\Big( - y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\Big) + \\frac{\\lambda}{2} \\sum_{j=1}^d w_j^2 $$\n其中 $\\lambda \\ge 0$ 是正则化参数。请注意，偏置项 $b$ 通常不进行正则化。\n\n### 3. 通过最速下降法进行优化\n\n最速下降法是一种迭代优化算法，它通过沿目标函数梯度的反方向移动一步来更新参数。更新规则如下：\n$$ \\mathbf{w}^{(k+1)} \\leftarrow \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}^{(k)}, b^{(k)}) $$\n$$ b^{(k+1)} \\leftarrow b^{(k)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}(\\mathbf{w}^{(k)}, b^{(k)}) $$\n其中 $\\alpha > 0$ 是学习率。为了实现这一点，我们必须计算 $\\mathcal{L}(\\mathbf{w}, b)$ 关于 $\\mathbf{w}$ 和 $b$ 的偏导数。\n\n首先，我们建立一个关于S型函数导数的关键恒等式：\n$$ \\frac{d\\sigma}{dz} = \\frac{d}{dz} (1 + e^{-z})^{-1} = -(1+e^{-z})^{-2}(-e^{-z}) = \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}} \\cdot \\frac{e^{-z}}{1+e^{-z}} = \\sigma(z)(1 - \\sigma(z)) $$\n\n让我们先计算非正则化部分的梯度。令 $L_i = -y_i \\log(\\sigma_i) - (1-y_i)\\log(1-\\sigma_i)$，其中 $\\sigma_i = \\sigma(z_i)$。对于单个权重 $w_j$，使用链式法则：\n$$ \\frac{\\partial L_i}{\\partial w_j} = \\frac{\\partial L_i}{\\partial \\sigma_i} \\frac{\\partial \\sigma_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_j} $$\n各个部分是：\n- $\\frac{\\partial L_i}{\\partial \\sigma_i} = -\\frac{y_i}{\\sigma_i} - \\frac{1-y_i}{-(1-\\sigma_i)} = -\\frac{y_i}{\\sigma_i} + \\frac{1-y_i}{1-\\sigma_i} = \\frac{-y_i(1-\\sigma_i) + \\sigma_i(1-y_i)}{\\sigma_i(1-\\sigma_i)} = \\frac{\\sigma_i - y_i}{\\sigma_i(1-\\sigma_i)}$\n- $\\frac{\\partial \\sigma_i}{\\partial z_i} = \\sigma_i(1-\\sigma_i)$\n- $\\frac{\\partial z_i}{\\partial w_j} = \\frac{\\partial}{\\partial w_j}(\\sum_{k=1}^d w_k x_{ik} + b) = x_{ij}$ (其中 $x_{ij}$ 是样本 $i$ 的第 $j$ 个特征)\n\n将它们组合起来得到：\n$$ \\frac{\\partial L_i}{\\partial w_j} = \\left( \\frac{\\sigma_i - y_i}{\\sigma_i(1-\\sigma_i)} \\right) (\\sigma_i(1-\\sigma_i)) (x_{ij}) = (\\sigma_i - y_i) x_{ij} $$\n完整目标函数 $\\mathcal{L}$ 关于 $w_j$ 的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial L_i}{\\partial w_j} + \\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2} \\sum_{k=1}^d w_k^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n (\\sigma_i - y_i) x_{ij} + \\lambda w_j $$\n对于偏置项 $b$，链式法则类似，但 $\\frac{\\partial z_i}{\\partial b} = 1$：\n$$ \\frac{\\partial L_i}{\\partial b} = (\\sigma_i - y_i) \\cdot 1 = \\sigma_i - y_i $$\n正则化项不依赖于 $b$，所以其导数为零。因此：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma_i - y_i) $$\n最终梯度表达式 $(\\sigma_i - y_i)$ 的简洁性是使用逻辑斯谛损失与线性模型的一个显著且优雅的特性。\n\n### 4. 向量化实现\n\n为了高效计算，我们用向量形式表示梯度。设 $X$ 是 $n \\times d$ 的特征向量矩阵（设计矩阵），$\\mathbf{y}$ 是 $n \\times 1$ 的标签向量，$\\mathbf{w}$ 是 $d \\times 1$ 的权重向量，$\\boldsymbol{\\sigma}$ 是 $n \\times 1$ 的预测概率 $\\sigma_i$ 的向量。\n线性预测器向量为 $\\mathbf{z} = X\\mathbf{w} + b\\mathbf{1}$，其中 $\\mathbf{1}$ 是一个全为一的向量。\n梯度向量 $\\nabla_{\\mathbf{w}} \\mathcal{L}$ 和标量导数 $\\frac{\\partial \\mathcal{L}}{\\partial b}$ 为：\n$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{n} X^\\top(\\boldsymbol{\\sigma} - \\mathbf{y}) + \\lambda \\mathbf{w} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\text{mean}(\\boldsymbol{\\sigma} - \\mathbf{y}) $$\n一次迭代的最速下降更新规则是：\n1.  计算线性预测器：$\\mathbf{z} = X\\mathbf{w} + b$\n2.  计算概率：$\\boldsymbol{\\sigma} = \\sigma(\\mathbf{z})$\n3.  计算梯度：\n    - $\\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{n} X^\\top(\\boldsymbol{\\sigma} - \\mathbf{y}) + \\lambda \\mathbf{w}$\n    - $\\frac{\\partial \\mathcal{L}}{\\partial b} = \\text{mean}(\\boldsymbol{\\sigma} - \\mathbf{y})$\n4.  更新参数：\n    - $\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}$\n    - $b \\leftarrow b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n\n### 5. 标准化与正则化\n\n- **标准化**：此预处理步骤将特征转换为零均值和单位标准差。对于尺度差异巨大的特征（如数据集 $D_2$），梯度下降的性能可能会很差。梯度的幅度将由尺度较大的特征主导，导致在某些方向上收敛缓慢，而在其他方向上可能出现振荡。标准化将所有特征置于一个共同的尺度上，使损失函数的水平集更接近球形，从而减轻这种病态条件。\n\n- **正则化**：$\\ell_2$ 惩罚项 $\\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2$ 用于控制模型复杂度。较大的 $\\lambda$ 值会迫使权重 $\\mathbf{w}$ 变小，从而产生一个“更简单”的模型，其决策边界不那么复杂。这可以防止对训练数据的过拟合。然而，如果 $\\lambda$ 过大（如用例4中所探讨的），可能会导致欠拟合，即模型过于简单，无法捕捉数据的基本结构，即使在训练集上也会表现不佳。\n\n### 6. 预测与评估\n\n经过固定次数的迭代训练后，最终的参数 $(\\mathbf{w}, b)$ 定义了一个分离超平面 $\\mathbf{w}^\\top \\mathbf{x} + b = 0$。新样本 $\\mathbf{x}$ 的分类取决于它落在超平面的哪一侧。预测规则是：\n$$ \\hat{y}_i = \\begin{cases} 1  \\text{if } \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 0.5 \\\\ 0  \\text{if } \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)  0.5 \\end{cases} $$\n由于 $\\sigma(z)$ 是一个单调递增函数且 $\\sigma(0) = 0.5$，这等价于：\n$$ \\hat{y}_i = \\begin{cases} 1  \\text{if } \\mathbf{w}^\\top \\mathbf{x}_i + b \\ge 0 \\\\ 0  \\text{if } \\mathbf{w}^\\top \\mathbf{x}_i + b  0 \\end{cases} $$\n模型在训练数据上的性能通过分类准确率进行评估，定义为正确分类样本的比例：\n$$ \\text{Accuracy} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\} $$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。以下程序实现了这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests binary logistic regression from first principles\n    using the steepest descent method.\n    \"\"\"\n\n    # --- Dataset Construction ---\n    # Dataset D1 (linearly separable, moderate scale)\n    X1_pos = np.array([\n        [2.5, 2.0], [3.0, 1.0], [2.0, 2.5], [3.5, 2.2], \n        [2.2, 3.0], [2.8, 2.7], [3.2, 1.8], [2.4, 3.1]\n    ])\n    X1_neg = np.array([\n        [-2.5, -1.5], [-3.0, -2.0], [-2.0, -2.2], [-3.2, -1.8], \n        [-1.8, -2.5], [-2.7, -2.9], [-3.1, -1.7], [-2.3, -3.2]\n    ])\n    X1 = np.vstack((X1_pos, X1_neg))\n    y1 = np.array([1] * 8 + [0] * 8)\n\n    # Dataset D2 (ill-conditioned)\n    X2 = np.copy(X1)\n    X2[:, 1] *= 1000.0\n    y2 = np.copy(y1)\n    \n    datasets = {\n        'D1': (X1, y1),\n        'D2': (X2, y2)\n    }\n\n    # --- Test Suite ---\n    test_cases = [\n        # (dataset_name, alpha, iterations, lambda_reg, standardize)\n        ('D1', 0.1, 3000, 0.01, True),\n        ('D1', 0.0001, 200, 0.01, True),\n        ('D2', 0.0001, 3000, 0.01, False),\n        ('D1', 0.1, 3000, 10.0, True),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        dataset_name, alpha, iterations, lambda_reg, standardize = case\n        X_orig, y = datasets[dataset_name]\n        \n        X = np.copy(X_orig)\n        \n        # --- Standardization ---\n        if standardize:\n            mu = np.mean(X, axis=0)\n            sigma = np.std(X, axis=0)\n            # Per problem spec, if stddev is 0, use 1 to avoid division by zero.\n            sigma[sigma == 0] = 1.0\n            X = (X - mu) / sigma\n\n        n_samples, n_features = X.shape\n\n        # --- Initialization ---\n        w = np.zeros(n_features)\n        b = 0.0\n\n        # --- Training via Steepest Descent ---\n        for _ in range(iterations):\n            # Linear predictor: z = Xw + b\n            z = X @ w + b\n            \n            # Sigmoid activation: sigma(z)\n            # This is p_i, the predicted probability for class 1\n            predictions_prob = 1 / (1 + np.exp(-z))\n            \n            # Error term: (sigma(z) - y)\n            error = predictions_prob - y\n            \n            # Compute gradients\n            # Gradient of loss w.r.t. w: (1/n) * X^T * (sigma(z) - y) + lambda * w\n            grad_w = (1 / n_samples) * (X.T @ error) + lambda_reg * w\n            \n            # Gradient of loss w.r.t. b: (1/n) * sum(sigma(z) - y)\n            grad_b = (1 / n_samples) * np.sum(error)\n            \n            # Update parameters\n            w -= alpha * grad_w\n            b -= alpha * grad_b\n\n        # --- Prediction and Accuracy ---\n        # Note: We use the original (unstandardized) X for final evaluation if standardization\n        # was done, as standardization parameters (mu, sigma) are part of the learned model.\n        # But here, we are asked for TRAINING accuracy, so we predict on the same data we trained on (X).\n        final_z = X @ w + b\n        \n        # Predict labels: 1 if z >= 0, else 0\n        y_pred = (final_z >= 0).astype(int)\n        \n        # Calculate accuracy\n        accuracy = np.mean(y_pred == y)\n        results.append(f\"{accuracy:.6f}\")\n\n    # --- Final Output ---\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3278883"}]}