{"hands_on_practices": [{"introduction": "在将这些函数应用于复杂网络之前，理解其基本的微积分性质至关重要。本练习将探讨缩放参数 $k$ 在 $\\tanh(kx)$ 中如何影响函数的斜率，这是基于梯度的学习中的一个关键因素。通过推导并找到最大斜率，您将直观地理解为何过“陡”的激活函数可能导致训练不稳定，即与梯度爆炸和梯度消失相关的问题。[@problem_id:3174538]", "problem": "考虑一个深度学习模型中的单个神经元激活，它由缩放的双曲正切函数 $f_{k}(x) = \\tanh(k x)$ 定义，其中 $k > 0$ 是一个标量超参数。从双曲正切的指数定义出发，\n$$\n\\tanh(z) = \\frac{\\exp(z) - \\exp(-z)}{\\exp(z) + \\exp(-z)},\n$$\n并且只使用微分学的标准法则（乘法法则、商法则和链式法则），推导斜率 $f_{k}'(x)$ 的表达式，并确定其幅值在整个实数轴上达到最大值时的点 $x$。然后，求出作为 $k$ 的函数的精确最大斜率。最后，基于梯度优化（如随机梯度下降 (SGD)）的基本原理，简要讨论极度陡峭的激活（大的 $k$）对学习动态的影响，包括非饱和区域的宽度和稳定性方面的考量。您最终提交的答案必须是关于最大斜率的、用 $k$ 表示的单一精确闭式解析表达式。不需要四舍五入，也不涉及单位。", "solution": "问题陈述已经过验证，被认为是有效的。它在科学上基于微分学及其在神经网络激活函数中的应用，问题提法适定，目标明确，信息充分，并以客观、正式的语言表述。因此，我们可以进行完整解答。\n\n该问题要求推导函数 $f_{k}(x) = \\tanh(k x)$（其中标量超参数 $k > 0$）的斜率，找出其幅值最大的位置，求出这个最大斜率的值，并讨论在基于梯度的优化背景下，大的 $k$ 值所带来的影响。\n\n首先，我们推导斜率 $f_{k}'(x)$ 的表达式。函数为 $f_{k}(x) = \\tanh(k x)$。令 $u(x) = kx$。根据链式法则，其导数为 $f_{k}'(x) = \\frac{d}{dx}\\tanh(u(x)) = \\frac{d(\\tanh u)}{du} \\cdot \\frac{du}{dx}$。$u(x)$ 关于 $x$ 的导数是 $\\frac{du}{dx} = k$。\n\n现在我们必须从 $\\tanh(u)$ 的指数定义出发，求它关于 $u$ 的导数：\n$$\n\\tanh(u) = \\frac{\\exp(u) - \\exp(-u)}{\\exp(u) + \\exp(-u)}\n$$\n我们应用商法则，该法则表明，对于 $h(u) = \\frac{g(u)}{p(u)}$，其导数为 $h'(u) = \\frac{g'(u)p(u) - g(u)p'(u)}{[p(u)]^2}$。\n令 $g(u) = \\exp(u) - \\exp(-u)$ 且 $p(u) = \\exp(u) + \\exp(-u)$。\n它们的导数是：\n$g'(u) = \\frac{d}{du}(\\exp(u) - \\exp(-u)) = \\exp(u) - (-\\exp(-u)) = \\exp(u) + \\exp(-u)$。\n$p'(u) = \\frac{d}{du}(\\exp(u) + \\exp(-u)) = \\exp(u) - \\exp(-u)$。\n\n将这些代入商法则公式：\n$$\n\\frac{d}{du}\\tanh(u) = \\frac{(\\exp(u) + \\exp(-u))(\\exp(u) + \\exp(-u)) - (\\exp(u) - \\exp(-u))(\\exp(u) - \\exp(-u))}{(\\exp(u) + \\exp(-u))^2}\n$$\n$$\n\\frac{d}{du}\\tanh(u) = \\frac{(\\exp(u) + \\exp(-u))^2 - (\\exp(u) - \\exp(-u))^2}{(\\exp(u) + \\exp(-u))^2}\n$$\n该表达式化简为：\n$$\n\\frac{d}{du}\\tanh(u) = 1 - \\left(\\frac{\\exp(u) - \\exp(-u)}{\\exp(u) + \\exp(-u)}\\right)^2 = 1 - \\tanh^2(u)\n$$\n将此结果与链式法则结合，我们得到 $f_k(x)$ 的斜率：\n$$\nf_{k}'(x) = (1 - \\tanh^2(kx)) \\cdot k = k(1 - \\tanh^2(kx))\n$$\n\n接下来，我们确定斜率的幅值 $|f_{k}'(x)|$ 在哪个点 $x$ 处最大化。由于 $k > 0$ 且项 $1 - \\tanh^2(kx)$ 始终为非负（因为 $\\tanh(z) \\in (-1, 1)$，所以 $\\tanh^2(z) \\in [0, 1)$），斜率 $f_{k}'(x)$ 始终为非负。因此，最大化 $|f_{k}'(x)|$ 等价于最大化 $f_{k}'(x)$。\n当项 $1 - \\tanh^2(kx)$ 最大化时，斜率 $f_{k}'(x) = k(1 - \\tanh^2(kx))$ 也最大化。这在 $\\tanh^2(kx)$ 最小化时发生。$\\tanh^2(kx)$ 的最小值为 $0$。当 $\\tanh(kx) = 0$ 时达到此最小值。\n方程 $\\tanh(z) = 0$ 成立的充要条件是 $z = 0$。因此，我们必须有 $kx = 0$。鉴于 $k > 0$，这意味着 $x = 0$。\n斜率幅值最大的点是 $x=0$。\n\n为了严格地证实这个结果，我们可以考察斜率的二阶导数。令 $g(x) = f_k'(x)$。斜率的一阶导数是：\n$g'(x) = \\frac{d}{dx} [k(1 - \\tanh^2(kx))] = -k \\cdot \\frac{d}{dx}[\\tanh^2(kx)]$。\n再次使用链式法则：\n$g'(x) = -k \\cdot 2\\tanh(kx) \\cdot \\frac{d}{dx}[\\tanh(kx)] = -2k\\tanh(kx) \\cdot [k(1 - \\tanh^2(kx))] = -2k^2\\tanh(kx)(1 - \\tanh^2(kx))$。\n令 $g'(x) = 0$ 可以得到临界点。由于 $k \\neq 0$，这要求要么 $\\tanh(kx) = 0$（得出 $x=0$），要么 $1 - \\tanh^2(kx) = 0$（意味着 $\\tanh(kx) = \\pm 1$，这个条件只有在 $x \\to \\pm\\infty$ 的极限情况下才满足，此时斜率趋近于 $0$）。在 $\\mathbb{R}$ 中唯一的临界点是 $x=0$。斜率的二阶导数是 $g''(x) = -2k^3(1 - 4\\tanh^2(kx) + 3\\tanh^4(kx))$。在 $x=0$ 处，$\\tanh(0)=0$，所以 $g''(0) = -2k^3$。由于 $k > 0$，所以 $g''(0)  0$，这证实了 $x=0$ 是一个局部最大值。由于当 $|x| \\to \\infty$ 时斜率趋近于 $0$，所以这是全局最大值。\n\n现在，我们求出作为 $k$ 的函数的精确最大斜率。这个值可以通过计算 $f_{k}'(x)$ 在 $x=0$ 处的值得到：\n$$\nf_{k}'(0) = k(1 - \\tanh^2(k \\cdot 0)) = k(1 - \\tanh^2(0)) = k(1 - 0^2) = k\n$$\n最大斜率恰好是 $k$。\n\n最后，我们讨论大的 $k$ 对学习动态的影响。\n当 $|kx|$ 变大时，激活函数 $f_k(x) = \\tanh(kx)$ 会饱和（即其输出趋近于 $\\pm 1$，其导数趋近于 $0$）。函数未饱和的区域，通常称为激活区或伪线性区，以 $x=0$ 为中心。该区域的宽度与 $k$ 成反比。对于大的 $k$ 值，该区域变得极其狭窄。例如，如果我们认为 $|kx|  3$ 的区域为非饱和区，其宽度大约为 $6/k$。\n这对诸如随机梯度下降（SGD）的基于梯度的优化有两个主要影响：\n1.  **梯度爆炸**：在 $x=0$ 附近非常狭窄的非饱和区域内，激活函数的斜率很大，最大值为 $k$。在反向传播过程中，这些大梯度会跨多层相乘，导致损失函数相对于网络早期层权重的总梯度呈指数级增长。这种现象被称为梯度爆炸，它会导致巨大且不稳定的权重更新，并可能阻止优化算法收敛。\n2.  **梯度消失**：在这个狭窄的中心区域之外，激活函数是饱和的，其导数 $f_k'(x)$ 非常接近 $0$。如果一个神经元的输入持续落入这个饱和区，实际上将没有梯度信息能通过它向后传播。这就是梯度消失问题，它会使深层网络的学习停滞，因为早期层的权重不再更新。\n\n总之，一个极度陡峭的激活（大的 $k$）创造了一个不稳定的优化景观，其特点是在一个小的激活区内的梯度爆炸和在其他所有地方的梯度消失之间只有一线之隔。这使得训练过程高度不稳定，并且对权重初始化和学习率选择等因素非常敏感。\n\n问题要求一个单一的最终答案，即最大斜率的精确闭式解析表达式。\n最大斜率是 $k$。", "answer": "$$\n\\boxed{k}\n$$", "id": "3174538"}, {"introduction": "sigmoid 和 tanh 函数在实践中的一个关键区别在于它们的输出范围。本练习提供了一个具体的、基于代码的探索，旨在说明为何 $\\tanh$ 的零中心特性对于训练深度网络通常更为有利。您将量化有偏输入下的激活值的“中心性”，甚至设计一个简单的校正层，从而展示神经网络设计的一个实用原则。[@problem_id:3174564]", "problem": "考虑深度学习中的一个前馈计算，其中输入向量在通过逐点非线性变换之前，先经过一个仿射平移。设输入向量为固定列表 $x = [ -3, -1, -0.5, 0, 0.5, 1, 3 ]$。设一个偏置参数 $b \\in \\mathbb{R}$ 被逐元素地应用，以产生预激活向量 $z = x + b$，其中加法是逐元素的。我们感兴趣的两个激活函数是逻辑S型函数（logistic sigmoid）$\\sigma$ 和双曲正切函数（hyperbolic tangent）$\\tanh$。仅使用它们的标准数学定义和基本性质，实现这些逐点函数，并比较它们在有偏置输入下的行为。\n\n您的任务是：\n- 实现作用于 $\\mathbb{R}$ 上的激活函数 $\\sigma$ 和 $\\tanh$，并将其逐元素地应用于向量。\n- 将逐点函数 $f$ 在向量 $u$ 上的零中心化分数（zero-centeredness score）定义为 $$S(f,u) = \\left| \\frac{1}{n} \\sum_{i=1}^{n} f(u_i) \\right|,$$ 其中 $n$ 是 $u$ 的长度，$u_i$ 表示第 $i$ 个元素。\n- 对于测试集中的每个偏置 $b$，计算 $z = x + b$ 对应的分数 $S(\\sigma,z)$ 和 $S(\\tanh,z)$。\n- 提出、从第一性原理推导并实现一个偏置校正层，该层利用 $\\tanh$ 的奇对称性来重新中心化激活值。具体来说，使用预激活值的经验均值作为校正偏移量：设 $$\\hat{\\mu}(z) = \\frac{1}{n}\\sum_{i=1}^{n} z_i,$$ 并定义校正后的激活值为 $$c(z) = \\tanh\\!\\big(z - \\hat{\\mu}(z)\\big),$$ 其中减法是逐元素的。然后计算分数 $S(c,z)$。\n\n使用以下偏置测试集，其选择旨在探测正常和极端情况：$$B = [\\,0,\\,0.5,\\,-0.5,\\,2,\\,-2,\\,5,\\,-5\\,].$$ 该集合包括无平移情况、小的正负平移、中等平移以及会导致饱和趋势的大平移。\n\n您的程序必须：\n- 对于每个 $b \\in B$，计算得分三元组 $[\\,S(\\sigma,z),\\, S(\\tanh,z),\\, S(c,z)\\,]$，其中 $z = x + b$。\n- 将每个分数四舍五入到 $6$ 位小数。\n- 生成单行输出，其中包含一个以逗号分隔的列表的列表，并用方括号括起来。列表的顺序必须与 $B$ 中偏置的给定顺序相对应。例如，要求的输出格式为 $$[\\,[s_{1,1},s_{1,2},s_{1,3}],\\,[s_{2,1},s_{2,2},s_{2,3}],\\,\\dots,\\, [s_{m,1},s_{m,2},s_{m,3}]\\,],$$ 其中 $m$ 是测试用例的数量，每个 $s_{j,k}$ 是一个保留 $6$ 位小数的浮点数。", "solution": "问题陈述是有效的。它在科学上基于神经网络激活函数的原理，在数学上是适定的，并且是客观陈述的。唯一解所需的所有定义、变量和常数都已提供。因此，我们可以进行形式化的求解。\n\n目标是分析和比较两种常见的激活函数——逻辑S型函数 $\\sigma$ 和双曲正切函数 $\\tanh$——在应用于经过仿射平移的输入向量时的零中心化程度。此外，我们将推导并评估一个旨在恢复激活值零中心化的偏置校正机制。\n\n首先，我们确定所涉及函数的数学定义。逻辑S型函数的定义为：\n$$ \\sigma(t) = \\frac{1}{1 + e^{-t}} $$\n它将任何实数输入 $t \\in \\mathbb{R}$ 映射到开区间 $(0, 1)$。双曲正切函数的定义为：\n$$ \\tanh(t) = \\frac{e^t - e^{-t}}{e^t + e^{-t}} $$\n它将任何实数输入 $t \\in \\mathbb{R}$ 映射到开区间 $(-1, 1)$。$\\tanh$ 的一个关键性质是它是一个奇函数，即 $\\tanh(-t) = -\\tanh(t)$。相比之下，S型函数既不是奇函数也不是偶函数，但它关于点 $(0, 0.5)$ 对称，满足 $\\sigma(t) + \\sigma(-t) = 1$。\n\n问题定义了一个从固定输入向量 $x$ 开始的计算过程。\n$$ x = [ -3, -1, -0.5, 0, 0.5, 1, 3 ] $$\n该向量长度为 $n=7$ 且关于 $0$ 对称。这种对称性的一个直接结果是其经验均值为零：\n$$ \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{7} (-3 - 1 - 0.5 + 0 + 0.5 + 1 + 3) = 0 $$\n\n将一个逐元素的偏置 $b \\in \\mathbb{R}$ 加到 $x$ 上，以产生预激活向量 $z$：\n$$ z = x + b $$\n$z$ 的分量为 $z_i = x_i + b$。这些预激活值随后通过一个逐点的激活函数。\n\n所得激活向量的零中心化程度由分数 $S(f, u)$ 来量化：\n$$ S(f,u) = \\left| \\frac{1}{n} \\sum_{i=1}^{n} f(u_i) \\right| $$\n这个分数是函数输出均值的绝对值。分数为 $0$ 表示完美的零中心化。\n\n我们的任务是为测试集 $B = [\\,0,\\,0.5,\\,-0.5,\\,2,\\,-2,\\,5,\\,-5\\,]$ 中的每个偏置 $b$ 计算三个分数：\n1. $S(\\sigma, z)$：S型函数的分数。由于对所有 $t$ 都有 $\\sigma(t) > 0$，因此总和 $\\sum \\sigma(z_i)$ 将始终为正，从而 $S(\\sigma, z) > 0$。激活值本质上不是零中心的。\n2. $S(\\tanh, z)$：双曲正切函数的分数。如果 $\\tanh$ 的输入关于 $0$ 对称，则输出也将对称，其和为 $0$。向量 $x$ 是对称的，因此当 $b=0$ 时，$z=x$，我们预期 $S(\\tanh, x) = 0$。对于任何 $b \\neq 0$，向量 $z=x+b$ 不再关于 $0$ 对称，因此我们预期 $S(\\tanh, z) > 0$。\n3. $S(c, z)$：偏置校正后激活值的分数。此过程旨在恢复零中心化。首先，计算预激活值的经验均值 $\\hat{\\mu}(z)$：\n$$ \\hat{\\mu}(z) = \\frac{1}{n}\\sum_{i=1}^{n} z_i $$\n我们可以基于 $x$ 的性质推导出 $\\hat{\\mu}(z)$ 的简化表达式：\n$$ \\hat{\\mu}(z) = \\frac{1}{n}\\sum_{i=1}^{n} (x_i + b) = \\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i\\right) + \\left(\\frac{1}{n}\\sum_{i=1}^{n} b\\right) = 0 + \\frac{1}{n}(nb) = b $$\n因此，预激活值的经验均值恰好是所施加的偏置 $b$。\n\n然后，偏置校正层通过在应用 $\\tanh$ 函数之前减去这个经验均值来中心化预激活值：\n$$ c(z) = \\tanh(z - \\hat{\\mu}(z)) $$\n代入我们的发现，$\\tanh$ 的参数变为：\n$$ z - \\hat{\\mu}(z) = (x + b) - b = x $$\n因此，校正后的激活值就是 $c(z) = \\tanh(x)$，与偏置 $b$ 无关。$\\tanh$ 的输入是原始向量 $x$，它关于 $0$ 对称。由于 $\\tanh$ 是一个奇函数，对于向量 $x$ 中的每个分量 $x_i$，其加法逆元 $-x_i$ 也存在于向量中，并且 $\\tanh(-x_i) = -\\tanh(x_i)$。因此，激活值的总和保证为零：\n$$ \\sum_{i=1}^{n} \\tanh(x_i) = \\tanh(-3) + \\tanh(-1) + \\tanh(-0.5) + \\tanh(0) + \\tanh(0.5) + \\tanh(1) + \\tanh(3) $$\n$$ = (-\\tanh(3)) + (-\\tanh(1)) + (-\\tanh(0.5)) + 0 + \\tanh(0.5) + \\tanh(1) + \\tanh(3) = 0 $$\n因此，对于所有 $b$ 值，校正后激活值的分数必须为零：\n$$ S(c, z) = S(\\tanh, x) = \\left| \\frac{1}{n} \\sum_{i=1}^{n} \\tanh(x_i) \\right| = \\left| \\frac{0}{7} \\right| = 0 $$\n这个从第一性原理出发的推导表明，由于初始输入向量 $x$ 的零均值特性，所提出的校正方法能够对任何给定的偏置 $b$ 完美地重新中心化激活值。\n\n对于每个 $b \\in B$，计算过程如下：\n1. 构建预激活向量 $z = x + b$。\n2. 计算S型激活值 $\\sigma(z_i)$ 及其分数 $S(\\sigma, z)$。\n3. 计算双曲正切激活值 $\\tanh(z_i)$ 及其分数 $S(\\tanh, z)$。\n4. 计算校正后的激活值（简化为 $\\tanh(x_i)$）及其分数 $S(c, z)$（理论上为 $0$）。\n5. 收集三个分数 $[\\,S(\\sigma,z),\\, S(\\tanh,z),\\, S(c,z)\\,]$ 并将每个分数四舍五入到 $6$ 位小数。\n\n将对整个测试集 $B$ 实施此过程。", "answer": "[[0.500000,0.000000,0.000000],[0.562177,0.062085,0.000000],[0.437823,0.062085,0.000000],[0.750262,0.245248,0.000000],[0.249738,0.245248,0.000000],[0.952574,0.571169,0.000000],[0.047426,0.571169,0.000000]]", "id": "3174564"}, {"introduction": "像 $\\tanh$ 这样的函数的用途远不止于简单的神经元激活。这个高级练习将挑战您使用 $\\tanh$ 作为构建模块，来创建一个平滑、可微的“软阈值”算子版本，该算子在稀疏编码和信号处理等领域至关重要。本练习展示了我们如何为深度学习模型设计可学习的定制组件，从而连接不同科学领域的概念。[@problem_id:3174524]", "problem": "您的任务是在深度学习的稀疏编码和去噪背景下，开发和评估基于双曲正切函数的可微收缩算子。从以下基本概念出发：逻辑S型函数 $\\sigma(x)$ 定义为 $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$，双曲正切函数 $\\tanh(x)$ 定义为 $\\tanh(x) = \\dfrac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$，并遵循恒等式 $\\tanh(x) = 2\\sigma(2x) - 1$。标准软阈值（也称为 $\\ell_{1}$-范数的近端算子）在阈值 $\\lambda > 0$ 下定义为 $S_{\\lambda}(x) = \\mathrm{sign}(x)\\cdot \\max(|x| - \\lambda, 0)$，它是最小绝对收缩和选择算子 (LASSO) 的核心。\n\n您的目标是构建与 $\\tanh$ 相关的平滑收缩算子，分析它们的性质，并量化它们对 $S_{\\lambda}(x)$ 的近似程度。考虑两个由 $\\lambda > 0$ 参数化的候选可微算子：\n- $A_{\\lambda}(x) = \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right)$,\n- $B_{\\lambda}(x) = x - \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right)$.\n\n使用第一性原理、逻辑S型函数的定义和双曲正切恒等式，以及软阈值的定义，论证在稀疏编码和去噪的应用场景中，为什么每个算子可能是或可能不是 $S_{\\lambda}(x)$ 的合适可微近似。特别地，将 $A_{\\lambda}$ 和 $B_{\\lambda}$ 在 $x = 0$ 附近、在 $x = \\pm \\lambda$ 处以及对于大的 $|x|$ 的定性行为与 $S_{\\lambda}$ 相关联。\n\n然后，编写一个完整的程序，实现 $S_{\\lambda}$、$A_{\\lambda}$ 和 $B_{\\lambda}$，并对指定的测试套件定量评估以下指标。使用以下阈值参数测试套件：\n- $\\lambda \\in \\{0.1, 0.5, 1.0, 2.0\\}$。\n\n对于测试套件中的每个 $\\lambda$：\n1. 构建一个 $x$ 值的网格，包含从 $-4\\lambda$ 到 $4\\lambda$（含两端）的 $N = 801$ 个等距点。\n2. 在该网格上，分别计算 $A_{\\lambda}$ 与 $S_{\\lambda}$ 之间以及 $B_{\\lambda}$ 与 $S_{\\lambda}$ 之间的均方误差 (MSE)，其定义为 $\\mathrm{MSE}(f, g) = \\dfrac{1}{N} \\sum_{i=1}^{N} \\left(f(x_{i}) - g(x_{i})\\right)^{2}$。\n3. 在该网格上，分别计算 $A_{\\lambda}$ 相对于 $S_{\\lambda}$ 和 $B_{\\lambda}$ 相对于 $S_{\\lambda}$ 的最大绝对误差，其定义为 $\\max_{i} \\left| f(x_{i}) - g(x_{i}) \\right|$。\n4. 计算 $A_{\\lambda}$ 和 $B_{\\lambda}$ 在 $x = 0$ 处的导数。使用 $\\tanh$ 的解析导数，即 $\\dfrac{d}{dx}\\tanh(x) = \\mathrm{sech}^{2}(x)$，其中 $\\mathrm{sech}(x) = \\dfrac{1}{\\cosh(x)}$ 且 $\\cosh(x) = \\dfrac{e^{x} + e^{-x}}{2}$。\n5. 计算在阈值点 $x = \\lambda$ 和 $x = -\\lambda$ 处的点态误差，即 $A_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda)$、$B_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda)$、$A_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda)$ 和 $B_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda)$。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，按 $\\lambda$ 升序排列。对于每个 $\\lambda$，按以下顺序输出一个包含10个浮点数的序列，每个数四舍五入到六位小数：\n- $\\mathrm{MSE}(A_{\\lambda}, S_{\\lambda})$,\n- $\\mathrm{MSE}(B_{\\lambda}, S_{\\lambda})$,\n- $\\max |A_{\\lambda} - S_{\\lambda}|$,\n- $\\max |B_{\\lambda} - S_{\\lambda}|$,\n- $A'_{\\lambda}(0)$,\n- $B'_{\\lambda}(0)$,\n- $A_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda)$,\n- $B_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda)$,\n- $A_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda)$,\n- $B_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda)$.\n\n因此，对于四个 $\\lambda$ 值，最终输出将是一个包含40个数字的列表。不涉及物理单位；不使用角度。确保所有计算都纯粹是数值上的且数学上是合理的。最终输出格式必须严格为单行，如 $[r_{1},r_{2},\\dots,r_{40}]$，其中每个 $r_{k}$ 是一个四舍五入到六位小数的浮点数。", "solution": "该问题已经过验证，被认为是有效的。它在科学上基于与深度学习相关的既定数学函数，问题阐述清晰，定义和目标明确，并且没有任何列出的无效缺陷。\n\n任务是分析两个可微算子 $A_{\\lambda}(x)$ 和 $B_{\\lambda}(x)$，作为软阈值函数 $S_{\\lambda}(x)$ 的潜在平滑近似。$S_{\\lambda}(x)$ 是稀疏编码和 LASSO 的基础，但不可微。在这种情况下，一个近似的适用性取决于三个主要标准：\n1.  **对小值的收缩**：它应将幅度较小的输入（假定为噪声）映射到接近零的值。\n2.  **对大值的保留**：它应将幅度较大的输入（假定为信号）映射到接近输入原始值的值。\n3.  **可微性**：该函数必须处处可微，以便在深度学习中常见的标准梯度优化算法中使用。\n\n标准软阈值算子定义为：\n$$S_{\\lambda}(x) = \\mathrm{sign}(x) \\cdot \\max(|x| - \\lambda, 0), \\quad \\lambda > 0$$\n该函数完美地体现了前两个标准：它将所有满足 $|x| \\le \\lambda$ 的输入 $x$ 设置为 $0$，而对于 $|x| > \\lambda$，它返回 $x - \\lambda \\cdot \\mathrm{sign}(x)$，从而在保留输入的同时引入一个恒定的偏移。然而，它在 $x = \\pm \\lambda$ 处不可微，违反了第三个标准。\n\n我们现在将根据这些标准分析两个候选近似 $A_{\\lambda}(x)$ 和 $B_{\\lambda}(x)$。\n\n### $A_{\\lambda}(x) = \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right)$ 的分析\n\n算子 $A_{\\lambda}(x)$ 是基于双曲正切函数 $\\tanh(u)$ 构建的，该函数具有以下性质：$\\tanh(0)=0$，$\\lim_{u\\to\\pm\\infty} \\tanh(u) = \\pm 1$，并且对于小的 $u$，$\\tanh(u) \\approx u$。\n\n1.  **在 $x=0$ 附近的行为**：当 $x$ 接近 $0$ 时，其参数 $u = x/\\lambda$ 也很小。使用 $\\tanh$ 的线性近似，我们得到：\n    $$A_{\\lambda}(x) = \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right) \\approx \\lambda \\left(\\dfrac{x}{\\lambda}\\right) = x$$\n    这种行为与收缩原则相悖。$A_{\\lambda}(x)$ 对于小输入的作用类似于恒等函数，将其传递出去而不是置零。这未能满足第一个标准。\n\n2.  **对于大 $|x|$ 的行为**：当 $|x| \\to \\infty$ 时，参数 $x/\\lambda \\to \\pm\\infty$。因此：\n    $$A_{\\lambda}(x) \\to \\lambda \\cdot (\\pm 1) = \\pm \\lambda$$\n    该函数在 $\\pm\\lambda$ 处饱和或削波大值。这与 $S_{\\lambda}(x)$ 有根本的不同，后者对于大 $|x|$ 呈线性增长，即 $S_{\\lambda}(x) \\approx x$。削波大幅度信号会导致严重的信息丢失，未能满足第二个标准。\n\n3.  **可微性**：$A_{\\lambda}(x)$ 是可微函数的复合，因此处处可微。这满足了第三个标准。\n\n**关于 $A_{\\lambda}(x)$ 的结论**：尽管 $A_{\\lambda}(x)$ 可微，但它作为软阈值算子的近似效果很差。它既不收缩小值，也不以稀疏编码所需的方式保留大值。它的行为更像一个带饱和的线性函数，而不是一个收缩算子。\n\n### $B_{\\lambda}(x) = x - \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right)$ 的分析\n\n该算子可以看作 $B_{\\lambda}(x) = x - A_{\\lambda}(x)$。\n\n1.  **在 $x=0$ 附近的行为**：使用与之前相同的近似，对于接近 $0$ 的 $x$：\n    $$B_{\\lambda}(x) = x - A_{\\lambda}(x) \\approx x - x = 0$$\n    这种行为与 $S_{\\lambda}(x)$ 在区域 $|x| \\le \\lambda$ 内的行为相匹配。它有效地将小值收缩至零，满足了第一个标准。\n\n2.  **对于大 $|x|$ 的行为**：当 $|x| \\to \\infty$ 时：\n    $$B_{\\lambda}(x) = x - \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right) \\to x - \\lambda \\cdot \\mathrm{sign}(x)$$\n    对于 $|x| > \\lambda$，这种渐近行为与 $S_{\\lambda}(x)$ 的行为相同。它保留了大值，并带有软阈值的特征性偏移，满足了第二个标准。\n\n3.  **可微性**：与 $A_{\\lambda}(x)$ 一样，$B_{\\lambda}(x)$ 处处可微。这满足了第三个标准。\n\n**关于 $B_{\\lambda}(x)$ 的结论**：算子 $B_{\\lambda}(x)$ 是软阈值函数 $S_{\\lambda}(x)$ 的一个优秀的平滑可微近似。它正确地模仿了收缩小值和保留大值的关键特性，使其成为深度学习中如学习型迭代收缩阈值算法 (LISTA) 等情境下的一个可行且广泛使用的替代方案。\n\n### 定量评估指标\n\n该问题要求计算几个指标。必要的解析公式如下：\n\n*   **在 $x=0$ 处的导数**：我们使用链式法则和给定的导数 $\\dfrac{d}{du}\\tanh(u) = \\mathrm{sech}^{2}(u)$。\n    $$A'_{\\lambda}(x) = \\dfrac{d}{dx} \\left[ \\lambda \\tanh\\left(\\frac{x}{\\lambda}\\right) \\right] = \\lambda \\cdot \\mathrm{sech}^2\\left(\\frac{x}{\\lambda}\\right) \\cdot \\frac{1}{\\lambda} = \\mathrm{sech}^2\\left(\\frac{x}{\\lambda}\\right)$$\n    在 $x=0$ 处，$A'_{\\lambda}(0) = \\mathrm{sech}^2(0) = 1$，因为 $\\cosh(0)=1$。\n    $$B'_{\\lambda}(x) = \\dfrac{d}{dx} \\left[ x - A_{\\lambda}(x) \\right] = 1 - A'_{\\lambda}(x) = 1 - \\mathrm{sech}^2\\left(\\frac{x}{\\lambda}\\right) = \\tanh^2\\left(\\frac{x}{\\lambda}\\right)$$\n    在 $x=0$ 处，$B'_{\\lambda}(0) = \\tanh^2(0) = 0$。\n    对于任何 $\\lambda > 0$，这些导数都是常数。\n\n*   **在 $x=\\pm\\lambda$ 处的点态误差**：\n    在 $x=\\lambda$ 处，$S_{\\lambda}(\\lambda)=0$。误差为：\n    $A_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda) = \\lambda \\tanh(1) - 0 = \\lambda \\tanh(1)$。\n    $B_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda) = (\\lambda - \\lambda \\tanh(1)) - 0 = \\lambda(1 - \\tanh(1))$。\n    在 $x=-\\lambda$ 处，$S_{\\lambda}(-\\lambda)=0$。误差为：\n    $A_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda) = \\lambda \\tanh(-1) - 0 = -\\lambda \\tanh(1)$。\n    $B_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda) = (-\\lambda - \\lambda \\tanh(-1)) - 0 = -\\lambda + \\lambda\\tanh(1) = -\\lambda(1 - \\tanh(1))$。\n\n其余指标（MSE 和最大绝对误差）是在指定的网格上进行数值计算的。下面的程序实现了这些计算。", "answer": "[0.001693,0.000030,0.076159,0.023841,1.000000,0.000000,0.076159,0.023841,-0.076159,-0.023841,0.042323,0.000755,0.380797,0.119203,1.000000,0.000000,0.380797,0.119203,-0.380797,-0.119203,0.169293,0.003021,0.761594,0.238406,1.000000,0.000000,0.761594,0.238406,-0.761594,-0.238406,0.677172,0.012083,1.523188,0.476812,1.000000,0.000000,1.523188,0.476812,-1.523188,-0.476812]", "id": "3174524"}]}