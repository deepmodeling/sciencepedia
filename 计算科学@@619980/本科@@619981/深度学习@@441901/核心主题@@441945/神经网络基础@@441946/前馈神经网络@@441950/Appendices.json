{"hands_on_practices": [{"introduction": "在深度学习中，一个核心的观察是，许多不同的参数组合可以产生完全相同的网络函数，这种现象被称为参数的非唯一性或重参数化对称性。本练习将引导您从第一性原理出发，推导并验证一种常见的对称性——神经元尺度不变性。通过这个实践，您将深入理解网络参数与其所代表的函数之间的关系，并揭示不同激活函数（如 ReLU 与 Sigmoid）在此对称性下表现各异的根本原因。[@problem_id:3125231]", "problem": "考虑一个具有 $L$ 层的标准前馈神经网络。设输入为 $x \\in \\mathbb{R}^{n_0}$，对于每个层索引 $l \\in \\{1,\\dots,L\\}$，定义预激活和激活为 $z^{(l)} = W^{(l)} x^{(l-1)} + b^{(l)}$ 和 $x^{(l)} = \\phi^{(l)}(z^{(l)})$，其中 $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ 且 $b^{(l)} \\in \\mathbb{R}^{n_l}$。网络的输出为 $f(x) = x^{(L)}$。假设为全连接架构，且输出激活函数 $\\phi^{(L)}$ 为恒等函数。\n\n你的任务是：\n- 从第一性原理以及前馈计算和激活函数的定义出发，推导在何种激活函数和参数变换条件下，两组不同的参数集会产生相同的函数 $f$。重点关注隐藏层中逐神经元的重参数化对称性。你必须在不依赖未经证明的捷径的情况下推导这些条件，并且必须明确指出对于常见的激活函数，这种对称性何时成立，何时失效。\n- 实现一个程序，通过构建随机化网络并对隐藏层应用逐神经元缩放变换，然后在固定的输入集上数值检验函数等价性，从而测试参数的可识别性。\n\n你必须编写一个完整的、可运行的程序，该程序：\n- 为了可复现性，使用固定的随机种子 $42$。\n- 从区间 $[-0.5, 0.5]$ 上的均匀分布中独立同分布地抽取权重 $W^{(l)}$ 和偏置 $b^{(l)}$。\n- 通过在一批 $N = 200$ 个从 $[-1,1]^{n_0}$ 上的均匀分布中独立同分布采样的输入上，计算原始网络和变换后网络的输出 $f(x)$ 和 $\\tilde{f}(x)$ 之间的最大绝对差来评估函数等价性。如果最大绝对差小于容差 $\\varepsilon = 10^{-8}$，则声明等价。\n- 在指定的隐藏层应用以下逐神经元缩放变换：对于隐藏层索引 $l$，其神经元级别的正缩放因子为 $s^{(l)}_j$，组合成对角矩阵 $D^{(l)} = \\mathrm{diag}(s^{(l)}_1,\\dots,s^{(l)}_{n_l})$，将 $W^{(l)}$ 替换为 $\\tilde{W}^{(l)} = D^{(l)} W^{(l)}$，将 $b^{(l)}$ 替换为 $\\tilde{b}^{(l)} = D^{(l)} b^{(l)}$，并将 $W^{(l+1)}$ 替换为 $\\tilde{W}^{(l+1)} = W^{(l+1)} (D^{(l)})^{-1}$。输出层 $L$ 不应用变换，因为没有后续层来进行补偿。\n- 测试六种涵盖典型和边缘场景的案例。在所有案例中，输出激活函数均为恒等函数。测试套件如下：\n    1.  隐藏层激活函数为 $\\mathrm{ReLU}$ (整流线性单元)，架构为 $[3,4,2]$，在隐藏层 $l=1$ 进行缩放，缩放因子为 $s^{(1)} = [1.7, 0.5, 2.2, 0.9]$。预期函数等价性成立。\n    2.  隐藏层激活函数为 $\\mathrm{ReLU}$，架构为 $[3,4,2]$，在隐藏层 $l=1$ 进行缩放，缩放因子为 $s^{(1)} = [-1.3, 1.2, 0.8, 1.1]$ (注意其中的负值)。预期函数等价性失效。\n    3.  隐藏层激活函数为 $\\sigma$ (sigmoid)，架构为 $[2,3,1]$，在隐藏层 $l=1$ 进行缩放，缩放因子为 $s^{(1)} = [1.4, 0.8, 1.1]$。预期函数等价性失效。\n    4.  隐藏层激活函数为恒等函数 (线性)，架构为 $[3,3,2]$，在隐藏层 $l=1$ 进行缩放，缩放因子为 $s^{(1)} = [-0.8, -1.2, 0.5]$。预期函数等价性成立。\n    5.  隐藏层激活函数为 $\\mathrm{ReLU}$，架构为 $[4,3,2]$，在隐藏层 $l=1$ 进行缩放，使用非常小的正缩放因子 $s^{(1)} = [10^{-6}, 3 \\cdot 10^{-6}, 2 \\cdot 10^{-6}]$。预期函数等价性成立。\n    6.  两个隐藏层的激活函数均为 $\\mathrm{ReLU}$，架构为 $[3,4,3,2]$，在隐藏层 $l=1$ 使用缩放因子 $s^{(1)} = [1.3, 0.7, 2.0, 0.9]$ 进行缩放，并在隐藏层 $l=2$ 使用缩放因子 $s^{(2)} = [1.2, 0.6, 1.5]$ 进行缩放。预期函数等价性成立。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。每个条目都应是一个布尔值，表示对应测试案例的函数等价性是否成立，顺序与上面列出的一致（例如，`[`True`, `False`, ...]`）。不应打印任何其他文本。", "solution": "该问题要求推导在前馈神经网络中出现重参数化对称性的条件，特别是关于隐藏层中逐神经元缩放的对称性。它还要求对几个测试案例进行数值验证。\n\n### 第1部分：重参数化对称性条件的推导\n\n让我们考虑一个具有 $L$ 层的标准前馈神经网络。输入为 $x^{(0)} = x \\in \\mathbb{R}^{n_0}$。对于每个层 $l \\in \\{1, \\dots, L\\}$，计算定义如下：\n$$ z^{(l)} = W^{(l)} x^{(l-1)} + b^{(l)} $$\n$$ x^{(l)} = \\phi^{(l)}(z^{(l)}) $$\n其中 $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ 是权重矩阵，$b^{(l)} \\in \\mathbb{R}^{n_l}$ 是偏置向量，$z^{(l)}$ 是预激活向量，$x^{(l)}$ 是激活向量（第 $l$ 层的输出），而 $\\phi^{(l)}$ 是第 $l$ 层的逐元素激活函数。网络的最终输出是 $f(x) = x^{(L)}$。我们已知输出激活函数 $\\phi^{(L)}$ 是恒等函数，因此 $f(x) = z^{(L)}$。\n\n我们感兴趣的是应用于单个隐藏层 $l \\in \\{1,\\dots,L-1\\}$ 参数的变换。设原始网络参数表示为 $\\theta = \\{W^{(k)}, b^{(k)}\\}_{k=1}^L$。该变换涉及一组针对第 $l$ 层中每个神经元 $j \\in \\{1, \\dots, n_l\\}$ 的非零缩放因子 $s^{(l)}_j$。这些因子被组合成一个对角矩阵 $D^{(l)} = \\mathrm{diag}(s^{(l)}_1, \\dots, s^{(l)}_{n_l})$。\n\n变换后的参数集 $\\tilde{\\theta}$ 通过修改与第 $l$ 层相关的参数以及下一层 $l+1$ 的权重来定义：\n1.  $\\tilde{W}^{(l)} = D^{(l)} W^{(l)}$\n2.  $\\tilde{b}^{(l)} = D^{(l)} b^{(l)}$\n3.  $\\tilde{W}^{(l+1)} = W^{(l+1)} (D^{(l)})^{-1}$\n4.  所有其他参数保持不变：对于 $k \\notin \\{l, l+1\\}$，$\\tilde{W}^{(k)} = W^{(k)}$；对于 $k \\neq l$，$\\tilde{b}^{(k)} = b^{(k)}$。\n\n我们用 $\\tilde{f}(x)$ 表示由变换后的网络计算的函数。我们希望找到激活函数 $\\phi^{(l)}$ 需满足的条件，使得对于所有输入 $x$，都有 $\\tilde{f}(x) = f(x)$。\n\n在变换后的网络中（我们用波浪号表示），对于 $k  l$ 的层，其计算与原始网络相同，因为参数未改变。因此，第 $l$ 层的输入是相同的：$\\tilde{x}^{(l-1)} = x^{(l-1)}$。\n\n在第 $l$ 层，变换后网络的预激活是：\n$$ \\tilde{z}^{(l)} = \\tilde{W}^{(l)} x^{(l-1)} + \\tilde{b}^{(l)} = (D^{(l)} W^{(l)}) x^{(l-1)} + (D^{(l)} b^{(l)}) = D^{(l)} (W^{(l)} x^{(l-1)} + b^{(l)}) = D^{(l)} z^{(l)} $$\n第 $l$ 层的激活是：\n$$ \\tilde{x}^{(l)} = \\phi^{(l)}(\\tilde{z}^{(l)}) = \\phi^{(l)}(D^{(l)} z^{(l)}) $$\n\n现在，考虑第 $l+1$ 层。其预激活是：\n$$ \\tilde{z}^{(l+1)} = \\tilde{W}^{(l+1)} \\tilde{x}^{(l)} + \\tilde{b}^{(l+1)} $$\n由于只有权重 $\\tilde{W}^{(l+1)}$ 被变换，而偏置 $\\tilde{b}^{(l+1)} = b^{(l+1)}$ 未变，我们有：\n$$ \\tilde{z}^{(l+1)} = (W^{(l+1)} (D^{(l)})^{-1}) \\phi^{(l)}(D^{(l)} z^{(l)}) + b^{(l+1)} $$\n对于原始网络，第 $l+1$ 层的预激活是：\n$$ z^{(l+1)} = W^{(l+1)} x^{(l)} + b^{(l+1)} = W^{(l+1)} \\phi^{(l)}(z^{(l)}) + b^{(l+1)} $$\n\n为了使两个网络计算相同的函数，它们的输出必须相同。由于从第 $l+2$ 层开始的所有参数在两个网络中都相同，如果第 $l+2$ 层的输入相同，则可以保证函数等价性 $\\tilde{f}(x) = f(x)$。这意味着我们需要 $\\tilde{x}^{(l+1)} = x^{(l+1)}$。由于两个网络的激活函数 $\\phi^{(l+1)}$ 相同，这等价于要求它们的预激活相同：$\\tilde{z}^{(l+1)} = z^{(l+1)}$。\n\n令 $\\tilde{z}^{(l+1)}$ 和 $z^{(l+1)}$ 的表达式相等：\n$$ W^{(l+1)} (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) + b^{(l+1)} = W^{(l+1)} \\phi^{(l)}(z^{(l)}) + b^{(l+1)} $$\n从两边减去 $b^{(l+1)}$ 得：\n$$ W^{(l+1)} \\left( (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) - \\phi^{(l)}(z^{(l)}) \\right) = 0 $$\n这个等式必须对所有可能的输入 $x$（它会生成所有可达的预激活 $z^{(l)}$）和任何有效的权重矩阵 $W^{(l+1)}$ 选择都成立。为了使等式对任意 $W^{(l+1)}$ 都成立，括号中的项必须是零向量。\n$$ (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) - \\phi^{(l)}(z^{(l)}) = 0 $$\n$$ \\implies (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) = \\phi^{(l)}(z^{(l)}) $$\n两边乘以 $D^{(l)}$（由于所有 $s^{(l)}_j \\neq 0$，它是可逆的），我们得到核心条件：\n$$ \\phi^{(l)}(D^{(l)} z^{(l)}) = D^{(l)} \\phi^{(l)}(z^{(l)}) $$\n由于 $\\phi^{(l)}$ 是一个逐元素函数且 $D^{(l)}$ 是对角的，我们可以逐个神经元地分析这个条件。对于第 $l$ 层中的第 $j$ 个神经元，其缩放因子为 $s_j$，预激活为 $z_j$，条件是：\n$$ \\phi_j(s_j z_j) = s_j \\phi_j(z_j) $$\n这意味着激活函数必须是关于缩放因子 $s_j$ 的一次齐次函数。\n\n让我们针对问题中提到的常见激活函数分析这个条件：\n\n1.  **恒等（线性）激活函数：** $\\phi(z) = z$。\n    条件是 $s_j z_j = s_j z_j$。这对于任何非零标量 $s_j$（正或负）总是成立的。因此，对于任何非零缩放，重参数化对称性对线性层都成立。\n\n2.  **整流线性单元 (ReLU)：** $\\phi(z) = \\max(0, z)$。\n    条件是 $\\max(0, s_j z_j) = s_j \\max(0, z_j)$。\n    -   如果 $s_j > 0$：\n        -   对于 $z_j \\ge 0$，方程变为 $s_j z_j = s_j z_j$，成立。\n        -   对于 $z_j  0$，方程变为 $0 = s_j \\cdot 0$，成立。\n        对称性对于任何正缩放因子 $s_j > 0$ 都成立。\n    -   如果 $s_j  0$：\n        -   让我们用一个反例来测试。设 $z_j = 1$ 且 $s_j = -2$。\n        -   左边：$\\max(0, (-2) \\cdot 1) = \\max(0, -2) = 0$。\n        -   右边：$(-2) \\cdot \\max(0, 1) = -2 \\cdot 1 = -2$。\n        -   由于 $0 \\neq -2$，条件不成立。对称性对于负缩放因子不成立。\n\n3.  **Sigmoid 激活函数：** $\\phi(z) = \\frac{1}{1 + e^{-z}}$。\n    条件是 $\\frac{1}{1 + e^{-s_j z_j}} = s_j \\frac{1}{1 + e^{-z_j}}$。\n    这个等式通常在 $s_j \\neq 1$ 时不成立。例如，如果我们取 $z_j=0$，左边是 $\\phi(0) = 0.5$，而右边是 $s_j \\phi(0) = 0.5 s_j$。要使它们相等，$s_j$ 必须为 $1$。如果 $s_j \\neq 1$，对称性就被破坏了。\n\n**多层缩放：**\n对于同时缩放多层的情况（如测试案例6，缩放层 $l=1$ 和 $l=2$），逻辑可以推广。在第 $l$ 层的缩放会调整其输出参数 $W^{(l)}, b^{(l)}$ 和下一层 $W^{(l+1)}$ 的输入权重。如果我们也缩放第 $l+1$ 层，它会调整其参数 $W^{(l+1)}, b^{(l+1)}$ 和第 $l+2$ 层的输入权重，即 $W^{(l+2)}$。\n权重矩阵 $W^{(l+1)}$ 受到两种变换的影响：其行被 $D^{(l+1)}$ 缩放，其列被 $(D^{(l)})^{-1}$ 缩放。最终变换后的权重为 $\\tilde{W}^{(l+1)} = D^{(l+1)} W^{(l+1)} (D^{(l)})^{-1}$。\n像之前一样沿着前向传播路径，我们发现为了保持网络函数不变，齐次性条件 $\\phi_j^{(k)}(s_j^{(k)} z_j^{(k)}) = s_j^{(k)} \\phi_j^{(k)}(z_j^{(k)})$ 必须对每个被缩放的层 $k$ 和其中的每个神经元 $j$ 都成立。如果所有被缩放的层都满足这个条件（例如，对于使用正缩放因子的 ReLU），整个网络函数将保持不变。\n\n### 第2部分：数值实现与验证\n\n下面的 Python 程序实现了指定的测试。它构建具有随机参数的神经网络，应用定义的缩放变换，并根据推导出的条件数值检验函数等价性。测试结果直接验证了上述理论分析。\n- 测试 $1$：ReLU，正缩放。理论：成立。结果：`True`。\n- 测试 $2$：ReLU，负缩放。理论：失败。结果：`False`。\n- 测试 $3$：Sigmoid，非单位缩放。理论：失败。结果：`False`。\n- 测试 $4$：线性，负缩放。理论：成立。结果：`True`。\n- 测试 $5$：ReLU，小的正缩放。理论：成立。结果：`True`。\n- 测试 $6$：两个相邻的 ReLU 层，正缩放。理论：成立。结果：`True`。\n\n数值结果预计将与理论预测相符。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates neural network reparameterization symmetries by implementing and testing\n    per-neuron scaling transformations on feedforward networks.\n    \"\"\"\n    \n    # Use a fixed random seed for reproducibility, as specified.\n    rng = np.random.default_rng(42)\n\n    # Define activation functions.\n    def relu(z):\n        return np.maximum(0, z)\n\n    def sigmoid(z):\n        # Added a clamp for numerical stability with large z\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def identity(z):\n        return z\n\n    activations_map = {'relu': relu, 'sigmoid': sigmoid, 'identity': identity}\n\n    def create_network(architecture, rng_gen):\n        \"\"\"Initializes network parameters from U[-0.5, 0.5].\"\"\"\n        weights = []\n        biases = []\n        for i in range(len(architecture) - 1):\n            n_out, n_in = architecture[i+1], architecture[i]\n            # Weights W^(l) from U[-0.5, 0.5]\n            w = rng_gen.uniform(-0.5, 0.5, size=(n_out, n_in))\n            # Biases b^(l) from U[-0.5, 0.5]\n            b = rng_gen.uniform(-0.5, 0.5, size=(n_out, 1))\n            weights.append(w)\n            biases.append(b)\n        return weights, biases\n\n    def forward_pass(weights, biases, x_batch, activation_funcs):\n        \"\"\"Computes the network output for a batch of inputs.\"\"\"\n        x_l = x_batch\n        num_layers = len(weights)\n        for l_idx in range(num_layers):\n            w = weights[l_idx]\n            b = biases[l_idx]\n            phi = activation_funcs[l_idx]\n            \n            z_l = w @ x_l + b\n            x_l = phi(z_l)\n        return x_l\n\n    def run_test_case(case_params, rng_gen):\n        \"\"\"Executes a single test case for functional equality.\"\"\"\n        arch = case_params['arch']\n        hidden_act_name = case_params['hidden_act']\n        output_act_name = 'identity'\n        scales_to_apply = case_params['scales']\n        \n        # Create the original network\n        original_weights, original_biases = create_network(arch, rng_gen)\n        \n        # Prepare activation functions for all layers\n        num_layers = len(arch) - 1\n        activation_funcs = []\n        hidden_act_func = activations_map[hidden_act_name]\n        output_act_func = activations_map[output_act_name]\n        for l_idx in range(num_layers):\n          # All hidden layers have the same activation, output is identity\n          is_output_layer = (l_idx == num_layers - 1)\n          activation_funcs.append(output_act_func if is_output_layer else hidden_act_func)\n\n        # Create the transformed network by applying scaling\n        # Start with a copy of the original network\n        transformed_weights = [w.copy() for w in original_weights]\n        transformed_biases = [b.copy() for b in original_biases]\n\n        for l_1based, s_vec in scales_to_apply.items():\n            l_idx = l_1based - 1 # Convert to 0-based index\n            s_vec_arr = np.array(s_vec)\n            \n            # Check for non-invertible scales\n            if np.any(s_vec_arr == 0):\n                raise ValueError(f\"Scaling factors must be non-zero. Found 0 in {s_vec}.\")\n\n            D = np.diag(s_vec_arr)\n            D_inv = np.diag(1.0 / s_vec_arr)\n\n            # Apply transformation: tilde_W(l) = D(l) * W(l)\n            transformed_weights[l_idx] = D @ transformed_weights[l_idx]\n            # Apply transformation: tilde_b(l) = D(l) * b(l)\n            transformed_biases[l_idx] = D @ transformed_biases[l_idx]\n            \n            # The compensation is applied to the next layer W(l+1)\n            # tilde_W(l+1) = W(l+1) * (D(l))^-1\n            # Note: This modifies the weights that might be a target for a subsequent scaling (e.g. Case 6)\n            if l_idx + 1  len(transformed_weights):\n                transformed_weights[l_idx + 1] = transformed_weights[l_idx + 1] @ D_inv\n\n        # Generate test inputs\n        N = 200 # Number of test inputs\n        n_0 = arch[0] # Input dimension\n        x_batch = rng_gen.uniform(-1, 1, size=(n_0, N))\n\n        # Perform forward passes\n        y_original = forward_pass(original_weights, original_biases, x_batch, activation_funcs)\n        y_transformed = forward_pass(transformed_weights, transformed_biases, x_batch, activation_funcs)\n        \n        # Check for functional equality\n        max_abs_diff = np.max(np.abs(y_original - y_transformed))\n        tolerance = 1e-8\n        \n        return max_abs_diff  tolerance\n\n    # Define the 6 test cases as specified in the problem statement.\n    test_cases = [\n        # 1. ReLU, positive scales -> should hold\n        {'arch': [3, 4, 2], 'hidden_act': 'relu', 'scales': {1: [1.7, 0.5, 2.2, 0.9]}},\n        # 2. ReLU, negative scale -> should fail\n        {'arch': [3, 4, 2], 'hidden_act': 'relu', 'scales': {1: [-1.3, 1.2, 0.8, 1.1]}},\n        # 3. Sigmoid, positive scales -> should fail\n        {'arch': [2, 3, 1], 'hidden_act': 'sigmoid', 'scales': {1: [1.4, 0.8, 1.1]}},\n        # 4. Identity, negative scales -> should hold\n        {'arch': [3, 3, 2], 'hidden_act': 'identity', 'scales': {1: [-0.8, -1.2, 0.5]}},\n        # 5. ReLU, small positive scales -> should hold\n        {'arch': [4, 3, 2], 'hidden_act': 'relu', 'scales': {1: [1e-6, 3e-6, 2e-6]}},\n        # 6. Two ReLU layers, positive scales -> should hold\n        {'arch': [3, 4, 3, 2], 'hidden_act': 'relu', 'scales': {1: [1.3, 0.7, 2.0, 0.9], 2: [1.2, 0.6, 1.5]}},\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(case, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3125231"}, {"introduction": "神经网络的训练过程通常对其输入和权重的尺度非常敏感，这可能导致训练不稳定或收敛缓慢。批量归一化（Batch Normalization, BN）是解决这一问题的关键技术之一。本练习将挑战您通过数学推导来揭示批量归一化实现尺度不变性的内在机理，从而深刻理解它为何能有效稳定并加速网络训练。[@problem_id:3125166]", "problem": "考虑一个实值前馈神经网络，它有一个隐藏层，输入维度为 $d$，隐藏维度为 $m$，输出维度为 $1$。隐藏层的非线性激活函数是修正线性单元（ReLU），其逐元素定义为 $\\sigma(u) = \\max\\{0,u\\}$，并满足正齐次性 $\\sigma(cu) = c\\,\\sigma(u)$（对于所有 $c \\geq 0$）。不带归一化的网络计算\n$$\nf(x) = w_2^{\\top}\\,\\sigma\\!\\left(W_1 x\\right),\n$$\n其中 $x \\in \\mathbb{R}^{d}$，$W_1 \\in \\mathbb{R}^{m \\times d}$，$w_2 \\in \\mathbb{R}^{m}$。网络中没有偏置项。现在假设输入被一个严格为正的标量 $\\alpha  0$ 重新缩放，因此新的输入为 $x' = \\alpha x$。你只能通过将第一层权重矩阵 $W_1$ 乘以一个标量 $\\beta  0$ 来进行补偿，得到扰动后的第一层权重 $W_1' = \\beta W_1$，同时保持 $w_2$ 不变。请确定能够为每个输入 $x$ 保持原始输出不变的 $\\beta$ 值。\n\n接下来，考虑在第一个线性层之后和 ReLU 之前立即插入批量归一化（BN），它按每个隐藏单元进行应用，并且可训练的缩放和平移参数 $\\gamma_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ 和 $\\beta_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ 保持固定。对于一个给定的 mini-batch，BN 将预激活值 $a = W_1 x$ 转换为\n$$\n\\hat{a} = \\gamma_{\\mathrm{BN}} \\odot \\frac{a - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}},\n$$\n其中 $\\mu \\in \\mathbb{R}^{m}$ 和 $\\sigma \\in \\mathbb{R}^{m}$ 是在 mini-batch 上逐元素计算的批均值和标准差，$\\odot$ 表示逐元素乘法，并且为了本次分析，数值稳定器被设置为零（即 $\\epsilon = 0$）。带 BN 的网络计算\n$$\ng(x) = w_2^{\\top}\\,\\sigma\\!\\left(\\hat{a}\\right).\n$$\n再次假设，输入被一个严格为正的标量 $\\alpha  0$ 重新缩放，并且你只能通过将第一层权重矩阵乘以相同的标量 $\\beta  0$ 来进行补偿，同时保持 $w_2$、$\\gamma_{\\mathrm{BN}}$ 和 $\\beta_{\\mathrm{BN}}$ 不变。请确定能够为每个输入 $x$ 以及整个 mini-batch 保持原始输出不变的 $\\beta$ 值。\n\n你的任务是从第一性原理出发，推导在每种情况下参数缩放 $\\beta$ 如何补偿输入缩放 $\\alpha$。请将你的最终答案表示为一个 $1 \\times 2$ 的行向量，按顺序包含无 BN 和有 BN 情况下的 $\\beta$ 的闭式表达式。不需要进行数值四舍五入，也不涉及物理单位。", "solution": "在尝试解决问题之前，将根据指定标准对问题进行验证。\n\n### 第 1 步：提取已知条件\n- **情况 1：不带批量归一化（BN）的网络**\n    - 网络模型：$f(x) = w_2^{\\top}\\,\\sigma\\!\\left(W_1 x\\right)$。\n    - 参数：输入 $x \\in \\mathbb{R}^{d}$，第一层权重 $W_1 \\in \\mathbb{R}^{m \\times d}$，第二层权重 $w_2 \\in \\mathbb{R}^{m}$。输入维度为 $d$，隐藏维度为 $m$，输出维度为 $1$。\n    - 激活函数：修正线性单元（ReLU），$\\sigma(u) = \\max\\{0,u\\}$，它是正齐次的：对于任何标量 $c \\geq 0$，有 $\\sigma(cu) = c\\,\\sigma(u)$。\n    - 扰动：输入被重新缩放为 $x' = \\alpha x$，其中 $\\alpha  0$。第一层权重被重新缩放为 $W_1' = \\beta W_1$，其中 $\\beta  0$。权重向量 $w_2$ 保持不变。\n    - 目标：找到使 $f(x)$ 对所有 $x$ 保持不变的 $\\beta$ 值。\n\n- **情况 2：带批量归一化（BN）的网络**\n    - BN 变换：插入在第一个线性层和 ReLU 激活函数之间。对于预激活值 $a$，BN 层计算 $\\hat{a} = \\gamma_{\\mathrm{BN}} \\odot \\frac{a - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}}$。\n    - BN 参数：$\\mu$ 和 $\\sigma$ 是预激活值在一个 mini-batch 上的逐元素均值和标准差。$\\gamma_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ 和 $\\beta_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ 是固定的可训练参数。数值稳定器 $\\epsilon$ 为 $0$。\n    - 网络模型：$g(x) = w_2^{\\top}\\,\\sigma\\!\\left(\\hat{a}\\right)$，其中 $\\hat{a}$ 是 BN 层应用于预激活值 $W_1 x$ 的输出。\n    - 扰动：输入被重新缩放为 $x' = \\alpha x$，其中 $\\alpha  0$。第一层权重被重新缩放为 $W_1' = \\beta W_1$，其中 $\\beta  0$。参数 $w_2$、$\\gamma_{\\mathrm{BN}}$ 和 $\\beta_{\\mathrm{BN}}$ 保持不变。\n    - 目标：找到使 $g(x)$ 对一个 mini-batch 中的所有 $x$ 保持不变的 $\\beta$ 值。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题在科学上植根于深度学习理论，使用了前馈网络、ReLU 激活和批量归一化的标准定义。其表述是客观且数学上精确的。该问题是适定（well-posed）的；尽管第二部分表现出一种特殊的不变性，但在问题背景下解释“补偿”一词会导出一个唯一的、有意义的解。该问题是自洽的，没有矛盾或事实错误。\n\n### 第 3 步：结论与行动\n该问题被判定为**有效**。将提供一个完整的、有理有据的解答。\n\n### 推导\n\n我们分别分析这两种情况。\n\n**情况 1：不带批量归一化的网络**\n\n原始网络计算输出 $f(x)$ 如下：\n$$\nf(x) = w_2^{\\top}\\,\\sigma\\!\\left(W_1 x\\right)\n$$\n输入被一个因子 $\\alpha  0$ 重新缩放，因此新输入为 $x' = \\alpha x$。第一层权重矩阵被一个因子 $\\beta  0$ 缩放，得到 $W_1' = \\beta W_1$。我们记新的网络输出为 $f_{\\text{new}}(x')$，其表达式为：\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left(W_1' x'\\right)\n$$\n代入 $x'$ 和 $W_1'$ 的表达式可得：\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left((\\beta W_1) (\\alpha x)\\right)\n$$\n利用矩阵乘法的结合律和标量属性：\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left((\\alpha\\beta) (W_1 x)\\right)\n$$\n预激活值被因子 $\\alpha\\beta$ 缩放。因为 $\\alpha  0$ 且 $\\beta  0$，所以乘积 $\\alpha\\beta$ 也严格为正。我们可以使用 ReLU 函数的正齐次性，即对于 $c \\geq 0$ 有 $\\sigma(cu) = c\\,\\sigma(u)$：\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\left( (\\alpha\\beta) \\sigma(W_1 x) \\right)\n$$\n由于 $\\alpha\\beta$ 是一个标量，我们可以将其从点积中提出：\n$$\nf_{\\text{new}}(x') = (\\alpha\\beta) \\left( w_2^{\\top} \\sigma(W_1 x) \\right) = (\\alpha\\beta) f(x)\n$$\n为了对每个输入 $x$ 都保持原始输出，我们必须有 $f_{\\text{new}}(x') = f(x)$。这意味着：\n$$\n(\\alpha\\beta) f(x) = f(x)\n$$\n为了使该等式对任何任意的、非平凡的网络（即 $f(x)$不恒为零）都成立，标量系数必须等于 $1$：\n$$\n\\alpha\\beta = 1\n$$\n解出 $\\beta$，我们得到第一层权重所需的缩放因子：\n$$\n\\beta = \\frac{1}{\\alpha}\n$$\n\n**情况 2：带批量归一化的网络**\n\n在这种情况下，在 ReLU 非线性激活之前，一个批量归一化层被应用于预激活值 $a = W_1 x$。对于一个包含输入 $\\{x_i\\}_{i=1}^N$ 的 mini-batch，预激活值为 $\\{a_i = W_1 x_i\\}_{i=1}^N$。BN 层首先计算这些预激活值在整个 mini-batch 上的逐元素均值 $\\mu$ 和标准差 $\\sigma$：\n$$\n\\mu = \\frac{1}{N} \\sum_{i=1}^N a_i \\quad ; \\quad \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (a_i - \\mu)^2}\n$$\n然后，它计算归一化后的激活值 $\\hat{a}_i$：\n$$\n\\hat{a}_i = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}}\n$$\n对于输入 $x_i$，最终的网络输出是 $g(x_i) = w_2^{\\top}\\,\\sigma(\\hat{a}_i)$。\n\n现在，我们考虑带有重新缩放的输入 $x_i' = \\alpha x_i$ 和重新缩放的权重 $W_1' = \\beta W_1$ 的扰动网络。新的预激活值 $a_i'$ 是：\n$$\na_i' = W_1' x_i' = (\\beta W_1)(\\alpha x_i) = (\\alpha\\beta) (W_1 x_i) = (\\alpha\\beta) a_i\n$$\n该批次中的每个预激活值都被因子 $\\alpha\\beta$ 缩放。让我们为这组新的预激活值 $\\{a_i'\\}$ 计算新的批统计量 $\\mu'$ 和 $\\sigma'$：\n新的均值 $\\mu'$ 是：\n$$\n\\mu' = \\frac{1}{N} \\sum_{i=1}^N a_i' = \\frac{1}{N} \\sum_{i=1}^N (\\alpha\\beta) a_i = (\\alpha\\beta) \\left(\\frac{1}{N} \\sum_{i=1}^N a_i\\right) = (\\alpha\\beta) \\mu\n$$\n新的标准差 $\\sigma'$ 是：\n$$\n\\sigma' = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (a_i' - \\mu')^2} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N ((\\alpha\\beta)a_i - (\\alpha\\beta)\\mu)^2} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (\\alpha\\beta)^2(a_i - \\mu)^2}\n$$\n$$\n\\sigma' = \\sqrt{(\\alpha\\beta)^2 \\left(\\frac{1}{N} \\sum_{i=1}^N (a_i - \\mu)^2\\right)} = \\sqrt{(\\alpha\\beta)^2 \\sigma^2} = |\\alpha\\beta| \\sigma\n$$\n由于 $\\alpha  0$ 且 $\\beta  0$，我们有 $|\\alpha\\beta| = \\alpha\\beta$。因此，$\\sigma' = (\\alpha\\beta) \\sigma$。\n\n现在我们使用这些新的统计量来计算新的归一化激活值 $\\hat{a}_i'$：\n$$\n\\hat{a}_i' = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i' - \\mu'}{\\sigma'} + \\beta_{\\mathrm{BN}} = \\gamma_{\\mathrm{BN}} \\odot \\frac{(\\alpha\\beta)a_i - (\\alpha\\beta)\\mu}{(\\alpha\\beta)\\sigma} + \\beta_{\\mathrm{BN}}\n$$\n假设批方差非零（因此对每个分量 $j$ 都有 $\\sigma_j \\neq 0$），我们可以约去分子和分母中的标量因子 $\\alpha\\beta$：\n$$\n\\hat{a}_i' = \\gamma_{\\mathrm{BN}} \\odot \\frac{(\\alpha\\beta)(a_i - \\mu)}{(\\alpha\\beta)\\sigma} + \\beta_{\\mathrm{BN}} = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}} = \\hat{a}_i\n$$\n批量归一化层的输出与原始输出相同，即 $\\hat{a}_i' = \\hat{a}_i$。这对 mini-batch 中的每个样本 $i$ 都成立。因此，ReLU 函数的输入保持不变，最终的网络输出 $g(x_i)$ 也保持不变。\n\n这种不变性对缩放因子 $\\beta  0$ 的*任何*选择都成立。问题要求的是*补偿*输入缩放 $\\alpha$ 的 $\\beta$ 值。我们的分析表明，批量归一化层本身就完美地补偿了其输入的任何缩放。BN 层的输入被因子 $\\alpha\\beta$ 缩放。由于该层的输出对这个因子是不变的，因此不需要参数 $\\beta$ 进行额外的补偿来抵消 $\\alpha$ 的影响。一个不产生任何变化的缩放操作是乘以 $1$。因此，尽管任何 $\\beta  0$ 都可以，但代表所需（零）补偿的特定 $\\beta$ 值是 $1$。\n\n### 结果总结\n为保持网络输出不变所需的缩放因子 $\\beta$ 是：\n1. 无 BN：$\\beta = \\frac{1}{\\alpha}$\n2. 有 BN：$\\beta = 1$\n\n最终答案以一个 $1 \\times 2$ 行向量的形式呈现。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\alpha}  1 \\end{pmatrix}}\n$$", "id": "3125166"}, {"introduction": "神经网络与传统的机器学习模型（如核方法）之间存在着深刻的联系。本练习旨在通过一个具体的编码任务，揭示前馈网络与核岭回归（Kernel Ridge Regression）之间的等价关系。您将通过实践证明，一个具有随机固定第一层权重和可训练输出层的浅层神经网络，其功能等价于一个在这些随机特征所构成的核空间中进行操作的线性模型，从而加深对神经网络“特征映射”本质的理解。[@problem_id:3125270]", "problem": "要求您实现并分析一个单隐层前馈神经网络，其第一层特征是固定的随机特征，并将其训练好的输出层与在相同特征上构建的核岭回归（Kernel Ridge Regression, KRR）进行比较。该比较应基于线性模型、前馈神经网络和核方法的基本原理。\n\n基本原理：\n- 前馈神经网络通过仿射变换和非线性函数的复合，定义了一个从输入向量 $x \\in \\mathbb{R}^d$ 到标量输出的映射。在一个具有 $h$ 个隐藏单元、固定的第一层权重和偏置 $(W_1, b)$ 以及逐元素激活函数 $\\sigma(\\cdot)$ 的单隐层网络中，网络输出是通过隐藏层激活值的线性组合获得的。\n- 岭回归（也称为 $L_2$ 正则化最小二乘法）惩罚参数向量的平方 $L_2$ 范数，以稳定解并控制过拟合。\n- 核方法通过将预测表示为特征的内积来运作，从而允许解可以根据核矩阵来写出。\n\n您的程序必须：\n1. 生成合成回归数据。对于给定的输入维度 $d$、训练集大小 $n$ 和测试集大小 $m$，使用固定的随机种子从标准正态分布中抽取输入 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ 和 $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$。为 $x \\in \\mathbb{R}^d$ 定义一个目标函数\n   $$ f(x) = \\sum_{j=1}^{d} \\sin(x_j) + 0.1 \\, \\|x\\|_2^2, $$\n   并观察带有标准差为 $\\sigma_{\\text{noise}}$ 的加性高斯噪声的输出：\n   $$ y_{\\text{train}} = f(X_{\\text{train}}) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I_n). $$\n2. 使用固定的随机第一层构建随机特征。对于隐藏维度 $h$，从标准正态分布中采样 $W_1 \\in \\mathbb{R}^{d \\times h}$ 和 $b \\in \\mathbb{R}^{1 \\times h}$（使用与数据构建相同的种子以确保每个测试用例内的确定性）。定义特征映射\n   $$ \\phi(x) = \\sigma(x W_1 + b), $$\n   该映射逐元素应用，其中 $\\sigma$ 是以下激活函数之一：\n   - 修正线性单元 (ReLU)：$\\sigma(z) = \\max(0, z)$ 逐元素应用。\n   - 双曲正切 (tanh)：$\\sigma(z) = \\tanh(z)$ 逐元素应用。\n   - 恒等 (线性)：$\\sigma(z) = z$ 逐元素应用。\n   对于矩阵，通过对 $X_{\\text{train}}$ 和 $X_{\\text{test}}$ 分别逐行应用 $\\phi(\\cdot)$ 来定义 $Z_{\\text{train}} \\in \\mathbb{R}^{n \\times h}$ 和 $Z_{\\text{test}} \\in \\mathbb{R}^{m \\times h}$。\n3. 仅通过最小化一个带正则化强度 $\\lambda  0$ 的 $L_2$ 正则化最小二乘目标函数来训练网络输出层的权重 $W_2 \\in \\mathbb{R}^{h \\times 1}$：\n   $$ \\min_{W_2} \\; \\frac{1}{n} \\, \\|Z_{\\text{train}} W_2 - y_{\\text{train}}\\|_2^2 + \\lambda \\, \\|W_2\\|_2^2. $$\n   使用数值稳定的线性求解方法获得正规方程所蕴含的最小化器，然后计算得到的测试预测\n   $$ \\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2. $$\n4. 根据相同的固定随机特征定义输入上的核函数：\n   $$ k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle, $$\n   这会产生训练格拉姆矩阵 (Gram matrix) $K \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = k(x_i, x_j)$，即 $K = Z_{\\text{train}} Z_{\\text{train}}^\\top$，以及测试-训练核块 $K_{\\text{test}} = Z_{\\text{test}} Z_{\\text{train}}^\\top \\in \\mathbb{R}^{m \\times n}$。使用相同的正则化参数 $\\lambda$ 执行核岭回归，并计算测试预测\n   $$ \\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha, $$\n   其中 $\\alpha \\in \\mathbb{R}^n$ 是通过求解一个涉及 $K$ 和 $\\lambda$ 的线性系统获得的。\n5. 通过计算最大绝对差异，对每个测试用例的两组测试预测进行数值比较\n   $$ \\Delta = \\max_{1 \\le i \\le m} \\left| \\hat{y}_{\\text{test}, i}^{\\text{primal}} - \\hat{y}_{\\text{test}, i}^{\\text{kernel}} \\right|. $$\n\n测试套件：\n为以下五个测试用例实现上述流程，每个用例由一个元组 $(\\text{seed}, d, h, \\text{activation}, n, m, \\lambda, \\sigma_{\\text{noise}})$ 指定：\n- 用例 1：$(0, 3, 50, \\text{ReLU}, 64, 32, 10^{-2}, 0.05)$。\n- 用例 2：$(1, 5, 10, \\tanh, 80, 40, 10^{-4}, 0.10)$。\n- 用例 3：$(2, 2, 5, \\text{identity}, 40, 20, 1.0, 0.00)$。\n- 用例 4：$(3, 4, 20, \\text{ReLU}, 60, 30, 10^{6}, 0.20)$。\n- 用例 5：$(4, 3, 1, \\tanh, 50, 25, 10^{-2}, 0.05)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含五个测试用例的结果，形式为一个由方括号括起来的逗号分隔列表，按用例 1 到 5 的顺序排列，每个条目是该用例的单个浮点数 $\\Delta$。例如：\n$$ [\\Delta_1, \\Delta_2, \\Delta_3, \\Delta_4, \\Delta_5]. $$\n不应打印任何其他文本。所有值都是无单位的实数。", "solution": "该问题要求对一个正则化线性回归问题的两种等价表述进行比较。第一种是在由神经网络隐藏层定义的特征空间中进行直接的（或称为原始的）回归。第二种是使用从相同特征派生出的核函数进行间接的（或称为对偶的）回归。问题的核心在于这两种方法的数学等价性，这一原理被称为原始-对偶等价性，是核方法的基础。此处的比较是数值上的，旨在衡量因浮点运算中不同计算路径而产生的差异。\n\n首先，我们验证问题陈述。\n\n### 第1步：提取已知条件\n- **数据生成：**\n  - 输入：$X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$，$X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$ 来自标准正态分布 $\\mathcal{N}(0, I)$。\n  - 目标函数：$f(x) = \\sum_{j=1}^{d} \\sin(x_j) + 0.1 \\, \\|x\\|_2^2$。\n  - 输出：$y_{\\text{train}} = f(X_{\\text{train}}) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I_n)$。\n- **随机特征：**\n  - 隐藏层维度：$h$。\n  - 权重和偏置：$W_1 \\in \\mathbb{R}^{d \\times h}$，$b \\in \\mathbb{R}^{1 \\times h}$ 来自 $\\mathcal{N}(0, 1)$。\n  - 特征映射：$\\phi(x) = \\sigma(x W_1 + b)$，其中 $\\sigma \\in \\{\\text{ReLU}, \\tanh, \\text{identity}\\}$。\n  - 变换后的数据：$Z_{\\text{train}} = \\phi(X_{\\text{train}}) \\in \\mathbb{R}^{n \\times h}$，$Z_{\\text{test}} = \\phi(X_{\\text{test}}) \\in \\mathbb{R}^{m \\times h}$。\n- **原始问题（特征上的岭回归）：**\n  - 目标函数：$\\min_{W_2} \\; \\frac{1}{n} \\, \\|Z_{\\text{train}} W_2 - y_{\\text{train}}\\|_2^2 + \\lambda \\, \\|W_2\\|_2^2$。\n  - 预测：$\\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2$。\n- **对偶问题（核岭回归）：**\n  - 核函数：$k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle$。\n  - 格拉姆矩阵 (Gram matrix)：$K = Z_{\\text{train}} Z_{\\text{train}}^\\top \\in \\mathbb{R}^{n \\times n}$。\n  - 测试-训练核：$K_{\\text{test}} = Z_{\\text{test}} Z_{\\text{train}}^\\top \\in \\mathbb{R}^{m \\times n}$。\n  - 预测：$\\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha$，其中 $\\alpha$ 是涉及 $K$ 和 $\\lambda$ 的线性系统的解。\n- **比较度量：** $\\Delta = \\max_{i} \\left| \\hat{y}_{\\text{test}, i}^{\\text{primal}} - \\hat{y}_{\\text{test}, i}^{\\text{kernel}} \\right|$。\n- **参数：** 提供了五个测试用例，包含 $(\\text{seed}, d, h, \\text{activation}, n, m, \\lambda, \\sigma_{\\text{noise}})$ 的具体值。\n\n### 第2步：使用提取的已知条件进行验证\n该问题在科学上是合理的、适定的和客观的。它探讨了权重空间（原始）和样本空间（对偶）正则化最小二乘之间的对偶性，这是机器学习理论的基石。当 $\\lambda  0$ 时，目标函数是严格凸的，保证了唯一解的存在。核函数从特征映射中得到了正确的定义。数学设置是自洽和一致的。核心任务是展示并量化通过不同矩阵运算求解两个数学上等价的问题时所产生的数值效应。该问题不违反任何科学原理，形式上规范，并且可通过计算验证。\n\n### 第3步：结论与行动\n问题有效。我们继续提供完整的解决方案。\n\n### 基于原理的解决方案\n\n该问题要求实现并比较两种解决给定特征集上正则化最小二乘问题的方法。这是机器学习中原始-对偶等价性的一个经典例子。\n\n**1. 原始表述：特征空间中的岭回归**\n单层神经网络的输出由 $g(x) = \\phi(x)W_2$ 给出，其中 $\\phi(x) = \\sigma(x W_1 + b)$ 是固定的、随机生成的特征向量。第二层权重 $W_2 \\in \\mathbb{R}^{h \\times 1}$ 通过最小化一个 $L_2$ 正则化最小二乘目标函数（也称为岭回归）来训练：\n$$\n\\mathcal{L}(W_2) = \\frac{1}{n} \\|Z_{\\text{train}} W_2 - y_{\\text{train}}\\|_2^2 + \\lambda \\|W_2\\|_2^2\n$$\n这里，$Z_{\\text{train}} \\in \\mathbb{R}^{n \\times h}$ 是训练集的特征向量矩阵，$y_{\\text{train}} \\in \\mathbb{R}^{n \\times 1}$ 是目标值。该目标函数是关于 $W_2$ 的凸函数。为了找到最小化器，我们计算其关于 $W_2$ 的梯度并将其设为零向量：\n$$\n\\nabla_{W_2} \\mathcal{L}(W_2) = \\frac{2}{n} Z_{\\text{train}}^\\top (Z_{\\text{train}} W_2 - y_{\\text{train}}) + 2\\lambda W_2 = 0\n$$\n整理各项以求解 $W_2$，得到此问题的正规方程：\n$$\n\\left(\\frac{1}{n} Z_{\\text{train}}^\\top Z_{\\text{train}} + \\lambda I_h\\right) W_2 = \\frac{1}{n} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n两边乘以 $n$，我们得到一个更常见的形式：\n$$\n(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h) W_2 = Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n其中 $I_h$ 是 $h \\times h$ 的单位矩阵。当 $\\lambda  0$ 时，矩阵 $(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)$ 是正定的，因此是可逆的。$W_2$ 的解为：\n$$\nW_2 = (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n这涉及求解一个 $h \\times h$ 矩阵的线性系统。然后，对测试集 $Z_{\\text{test}}$ 的预测计算如下：\n$$\n\\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2\n$$\n\n**2. 对偶表述：核岭回归**\n“核技巧”允许我们在能够用内积表示算法的情况下，无需显式构造特征向量即可解决相同的问题。核函数定义为特征空间中的内积：\n$$\nk(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\phi(x_i) \\phi(x_j)^\\top\n$$\n在这个问题中，我们可以显式地访问特征映射 $\\phi$，因此可以直接计算训练格拉姆矩阵 (Gram matrix) $K \\in \\mathbb{R}^{n \\times n}$ 为 $K = Z_{\\text{train}} Z_{\\text{train}}^\\top$。\n\n表示定理 (representer theorem) 指出，最优权重向量 $W_2$ 可以表示为训练数据特征向量的线性组合，即 $W_2 = Z_{\\text{train}}^\\top \\alpha$，其中 $\\alpha \\in \\mathbb{R}^{n \\times 1}$ 是某个对偶系数向量。将此代入 $W_2$ 的原始解中，可以解出 $\\alpha$。在核岭回归中，与原始目标函数一致的 $\\alpha$ 的标准解是以下线性系统的解：\n$$\n(K + n\\lambda I_n) \\alpha = y_{\\text{train}}\n$$\n其中 $I_n$ 是 $n \\times n$ 的单位矩阵。求解此系统得到 $\\alpha$：\n$$\n\\alpha = (K + n\\lambda I_n)^{-1} y_{\\text{train}}\n$$\n这涉及求解一个 $n \\times n$ 矩阵的线性系统。对测试集的预测是使用测试点和训练点之间的核函数来完成的：\n$$\n\\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha = (Z_{\\text{test}} Z_{\\text{train}}^\\top) \\alpha\n$$\n\n**3. 原始-对偶等价性**\n理论上，两种方法的预测必须完全相同。我们可以通过将 $W_2$ 和 $\\alpha$ 的表达式代入预测方程来证明这一点。\n$$\n\\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2 = Z_{\\text{test}} (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n$$\n\\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha = (Z_{\\text{test}} Z_{\\text{train}}^\\top) (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1} y_{\\text{train}}\n$$\n这两个表达式的等价性依赖于一个矩阵恒等式（Woodbury 恒等式的一种形式，也称为穿透恒等式）：\n$$\n(A^\\top A + cI_h)^{-1} A^\\top = A^\\top (A A^\\top + cI_n)^{-1}\n$$\n令 $A = Z_{\\text{train}}$ 和 $c = n\\lambda$，我们有：\n$$\n(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top = Z_{\\text{train}}^\\top (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1}\n$$\n在等式两边左乘 $Z_{\\text{test}}$，表明作用于 $y_{\\text{train}}$ 的算子是相同的：\n$$\nZ_{\\text{test}} (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top = Z_{\\text{test}} Z_{\\text{train}}^\\top (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1}\n$$\n因此，在精确算术中，$\\hat{y}_{\\text{test}}^{\\text{primal}} = \\hat{y}_{\\text{test}}^{\\text{kernel}}$。\n\n原始和对偶公式之间的计算选择取决于训练样本数 $n$ 和特征维度 $h$ 的相对大小。如果 $n \\gg h$，原始公式更高效，因为它需要求解一个 $h \\times h$ 的系统。相反，如果 $h \\gg n$，则首选对偶公式，因为它需要求解一个 $n \\times n$ 的系统。所要求的差异 $\\Delta$ 将是一个小的非零值，反映了两种不同计算路径中浮点误差的累积差异。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares primal and dual solutions for ridge regression\n    on random features for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # (seed, d, h, activation, n, m, lambda, sigma_noise)\n        (0, 3, 50, 'ReLU', 64, 32, 1e-2, 0.05),\n        (1, 5, 10, 'tanh', 80, 40, 1e-4, 0.10),\n        (2, 2, 5, 'identity', 40, 20, 1.0, 0.00),\n        (3, 4, 20, 'ReLU', 60, 30, 1e6, 0.20),\n        (4, 3, 1, 'tanh', 50, 25, 1e-2, 0.05),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        seed, d, h, activation_name, n, m, lam, sigma_noise = case\n        \n        # Set the seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate synthetic regression data\n        X_train = np.random.randn(n, d)\n        X_test = np.random.randn(m, d)\n\n        def target_function(X):\n            sin_term = np.sum(np.sin(X), axis=1)\n            norm_term = 0.1 * np.sum(X**2, axis=1) # Faster than np.linalg.norm for this purpose\n            return (sin_term + norm_term).reshape(-1, 1)\n\n        y_true_train = target_function(X_train)\n        noise = np.random.randn(n, 1) * sigma_noise\n        y_train = y_true_train + noise\n\n        # 2. Construct random features\n        W1 = np.random.randn(d, h)\n        b = np.random.randn(1, h)\n\n        activations = {\n            'ReLU': lambda z: np.maximum(0, z),\n            'tanh': lambda z: np.tanh(z),\n            'identity': lambda z: z\n        }\n        activation_func = activations[activation_name]\n\n        Z_train = activation_func(X_train @ W1 + b)\n        Z_test = activation_func(X_test @ W1 + b)\n\n        # 3. Primal solution (Ridge Regression)\n        # Solve (Z_train.T @ Z_train + n * lambda * I_h) W2 = Z_train.T @ y_train\n        h_dim = Z_train.shape[1]\n        A_primal = Z_train.T @ Z_train + n * lam * np.identity(h_dim)\n        c_primal = Z_train.T @ y_train\n        \n        W2 = np.linalg.solve(A_primal, c_primal)\n        \n        y_hat_primal = Z_test @ W2\n\n        # 4. Dual solution (Kernel Ridge Regression)\n        # Solve (K + n * lambda * I_n) alpha = y_train\n        # where K = Z_train @ Z_train.T\n        K = Z_train @ Z_train.T\n        A_kernel = K + n * lam * np.identity(n)\n        \n        alpha = np.linalg.solve(A_kernel, y_train)\n        \n        # Predictions: y_hat_kernel = K_test @ alpha\n        # where K_test = Z_test @ Z_train.T\n        K_test = Z_test @ Z_train.T\n        y_hat_kernel = K_test @ alpha\n\n        # 5. Compare predictions\n        delta = np.max(np.abs(y_hat_primal - y_hat_kernel))\n        results.append(delta)\n\n    # Format and print the final results\n    print(f\"[{','.join(f'{r:.12e}' for r in results)}]\")\n\nsolve()\n```", "id": "3125270"}]}