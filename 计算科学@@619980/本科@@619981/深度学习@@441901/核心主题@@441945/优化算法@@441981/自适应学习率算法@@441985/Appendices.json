{"hands_on_practices": [{"introduction": "自适应学习率算法的核心在于其能够根据梯度的历史信息动态调整每个参数的步长。其中，二阶矩估计量 $v_t$（例如在 Adam 和 RMSProp 中）扮演着“感知”损失曲率的角色。超参数 $\\beta_2$ 控制着这个估计的“记忆”长度：$\\beta_2$ 值越高，记忆越长，估计越稳定，但对环境变化的反应也越慢。本练习 [@problem_id:3096921] 通过一个简化的确定性模型，让您亲手量化这种响应能力，从而深刻理解在稳定性和适应性之间进行权衡的本质。", "problem": "您需要设计并分析一个简化的训练场景，以研究自适应矩估计 (Adam) 中的二阶原始矩估计量如何响应损失曲率的突变。考虑一个一维参数，其损失由一个二次函数局部近似，该二次函数的曲率在指定的迭代索引处会突然改变。在此设定中，梯度平方的真实二阶矩被建模为一个经历突然状态转换（相变）的分段常数序列。\n\n基本原理和设置：\n- 设每次迭代的随机梯度为 $g_t$。自适应矩估计 (Adam) 使用的二阶原始矩估计量是一个指数移动平均 (EMA)，定义如下\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, g_t^2,\n$$\n其中 $v_t$ 是在迭代 $t$ 时的估计量，$\\beta_2 \\in [0,1)$ 是二阶矩的衰减参数。\n- 为了在没有混淆优化动态的情况下，单独研究对曲率变化的响应性，我们将随机输入 $g_t^2$ 替换为其依赖于状态的真实二阶矩 $m_t = \\mathbb{E}[g_t^2]$。我们假设 $m_t$ 是随时间分段常数的，并在指定的迭代索引处突然改变。在此替换下，估计量通过以下方式确定性地演化\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t,\n$$\n初始条件为 $v_0 = 0$。\n- 在 $m_t$ 每次突变后，我们将响应性度量定义为最小的非负整数延迟 $\\ell$（以迭代次数衡量），使得 $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta \\, m_{\\text{new}}$ 成立。其中 $t_c$ 是发生变化的迭代索引，$m_{\\text{new}}$ 是变化后 $m_t$ 的新常数值，$\\delta \\in (0,1)$ 是一个指定的容差。如果在给定的时间范围 $T$ 内未能满足容差，则将延迟记录为 $T - t_c$。一个测试用例的最终分数是该用例中所有变化的延迟的平均值，四舍五入到最接近的整数。\n\n您的任务：\n- 为多个测试场景实现确定性更新 $v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t$，时间范围固定为 $T$。每个场景由 $(\\beta_2, T, \\{t_c\\}, \\{m\\}, \\delta)$ 定义，其中 $\\{t_c\\}$ 是一个递增的变化索引列表，$\\{m\\}$ 是 $m_t$ 的状态值列表，其长度为 $|\\{t_c\\}| + 1$。\n- 对于每个场景，计算所有变化的平均稳定时间，并四舍五入到最接近的整数。\n\n测试套件：\n使用以下六个场景，它们探索了一系列行为，包括一般情况、大幅度的向上和向下相变、多次转换以及极慢的自适应过程。在下面的每一项中，所有给出的数学实体都必须被精确使用。\n\n1. 场景 1：$\\beta_2 = 0.0$，$T = 200$，变化点 $\\{50, 120\\}$，状态 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n2. 场景 2：$\\beta_2 = 0.9$，$T = 200$，变化点 $\\{50, 120\\}$，状态 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n3. 场景 3：$\\beta_2 = 0.99$，$T = 200$，变化点 $\\{50, 120\\}$，状态 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n4. 场景 4：$\\beta_2 = 0.999$，$T = 200$，变化点 $\\{50, 120\\}$，状态 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n5. 场景 5：$\\beta_2 = 0.99$，$T = 200$，变化点 $\\{20, 40, 80, 120\\}$，状态 $\\{1.0, 4.0, 0.5, 2.0, 1.5\\}$，$\\delta = 0.05$。\n6. 场景 6：$\\beta_2 = 0.9999$，$T = 1000$，变化点 $\\{50, 120\\}$，状态 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目对应一个场景的四舍五入后的平均稳定时间，顺序为场景1到6（例如，$[r_1,r_2,r_3,r_4,r_5,r_6]$）。所有条目必须是整数。此问题不涉及任何物理单位或角度；所有量都是无量纲的。", "solution": "用户提供了一个问题，要求分析在损失景观曲率发生突变时，来自自适应矩估计 (Adam) 算法的简化二阶矩估计量的响应。此分析将在一个确定性模型下进行，其中随机梯度的平方被其分段常数期望所取代。\n\n问题的核心是控制二阶矩估计量 $v_t$ 演化的确定性一阶线性递推关系：\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t\n$$\n初始条件为 $v_0 = 0$。其中，$v_t$ 是在迭代 $t$ 时的估计量，$\\beta_2 \\in [0, 1)$ 是指数衰减率，$m_t$ 是梯度的真实二阶原始矩，它被给定为一个关于 $t$ 的分段常数函数。\n\n为了理解其动态特性，我们考虑一个 $m_t$ 为常量的时期，例如 $m_t = m_{\\text{const}}$。该递推关系的不动点是 $v = m_{\\text{const}}$。我们将与此不动点的误差或偏差定义为 $\\epsilon_t = v_t - m_{\\text{const}}$。将此代入更新规则可得：\n$$\n\\epsilon_t + m_{\\text{const}} = \\beta_2 (\\epsilon_{t-1} + m_{\\text{const}}) + (1 - \\beta_2) m_{\\text{const}}\n$$\n$$\n\\epsilon_t + m_{\\text{const}} = \\beta_2 \\epsilon_{t-1} + \\beta_2 m_{\\text{const}} + m_{\\text{const}} - \\beta_2 m_{\\text{const}}\n$$\n$$\n\\epsilon_t = \\beta_2 \\epsilon_{t-1}\n$$\n这表明，与目标矩的偏差在每次迭代中都以因子 $\\beta_2$ 指数衰减。因此，在进入一个新的常数状态 $\\ell$ 次迭代后，初始偏差 $\\epsilon_0$ 将会减少一个因子 $\\beta_2^\\ell$。接近 $1$ 的 $\\beta_2$ 值意味着缓慢的衰减，从而对新矩的适应也较慢，因为估计量保留了对过去值的长期记忆。相反，$\\beta_2$ 为 $0$ 则意味着瞬时适应，因为 $v_t = m_t$。\n\n问题要求一个响应性度量：延迟 $\\ell$，即在迭代 $t_c$ 发生变化后， $v_t$ 稳定到新目标矩 $m_{\\text{new}}$ 的一个容差范围 $\\delta$ 内所需的迭代次数。具体来说，我们必须找到最小的非负整数 $\\ell$，使得 $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta m_{\\text{new}}$。\n\n虽然对于孤立的状态变化可以推导出 $\\ell$ 的解析表达式，但问题指定了包含多个连续变化的场景。在 $t_c$ 发生变化后 $v_t$ 的演化不仅受新矩 $m_{\\text{new}}$ 的影响，还受到在 $v_t$ 完全收敛之前 $m_t$ 发生的任何后续变化的影响。这种复杂性使得直接模拟该过程成为解决此问题最鲁棒和最忠实的方法。\n\n算法流程如下：\n1.  **构建矩序列**：对于每个场景，我们首先根据给定的变化点 $\\{t_c\\}$ 和状态值 $\\{m\\}$，构建从迭代 $t=1$ 到时间上限 $T$ 的完整真实矩序列 $m_t$。\n2.  **模拟估计量的演化**：然后我们在整个时间范围内模拟估计量 $v_t$ 的演化。从 $v_0 = 0$ 开始，我们使用更新规则 $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) m_t$ 和预先计算好的矩序列，迭代计算从 $t=1$到$T$ 的每个 $v_t$。\n3.  **计算稳定延迟**：对于一个场景中每个指定的变化点 $t_c$，我们分析得到的 $v_t$ 序列以找到稳定延迟。\n    - 新的目标矩是 $m_{\\text{new}}$，即 $m_t$ 序列中从 $t_c$ 开始的状态值。\n    - 收敛标准是 $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta m_{\\text{new}}$。\n    - 我们搜索最小的非负整数 $\\ell$（从 $\\ell=0$ 到 $T-t_c$），使其满足此条件。\n    - 如果在搜索窗口内（即直到时间 $t=T$）任何 $\\ell$ 都不能满足该条件，则根据问题规范，将延迟记录为可能的最大值 $T-t_c$。\n4.  **计算最终得分**：每个场景的最终得分是所有计算出的延迟的算术平均值，四舍五入到最接近的整数。\n\n此计算过程精确地模拟了指定的确定性系统，并允许在复杂的多阶段状态转换下精确计算响应性度量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_mean_lag(beta2, T, changes, regimes, delta):\n    \"\"\"\n    Calculates the rounded mean settling time for a single scenario.\n    \"\"\"\n    # Step 1: Construct the piecewise-constant moment sequence m_t\n    # m_seq is 1-indexed for problem clarity, so size is T+1\n    m_seq = np.zeros(T + 1)\n    \n    # Define the start and end of each regime\n    regime_starts = [1] + changes\n    regime_ends = changes + [T + 1]\n    \n    for i, m_val in enumerate(regimes):\n        start_idx = regime_starts[i]\n        end_idx = regime_ends[i]\n        if start_idx  end_idx: # Ensure the interval is valid\n            m_seq[start_idx:end_idx] = m_val\n\n    # Step 2: Simulate the evolution of the estimator v_t\n    # v_seq is 0-indexed for v_0, so size is T+1\n    v_seq = np.zeros(T + 1)  # v_seq[0] is v_0 = 0\n    \n    for t in range(1, T + 1):\n        v_seq[t] = beta2 * v_seq[t-1] + (1 - beta2) * m_seq[t]\n        \n    # Step 3: Calculate settling lags for each change\n    all_lags = []\n    for i, tc in enumerate(changes):\n        m_new = regimes[i + 1]\n        threshold = delta * m_new\n        lag_found = False\n        \n        # Search for the smallest non-negative lag l\n        max_l = T - tc\n        for l in range(max_l + 1):\n            t = tc + l\n            if abs(v_seq[t] - m_new) = threshold:\n                all_lags.append(l)\n                lag_found = True\n                break\n        \n        # If tolerance is not met, use the capped value\n        if not lag_found:\n            all_lags.append(max_l)\n            \n    # Step 4: Compute the final score (rounded mean lag)\n    if not all_lags:\n        return 0\n        \n    mean_lag = np.mean(all_lags)\n    return int(round(mean_lag))\n\ndef solve():\n    \"\"\"\n    Main function to run all test scenarios and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (beta2, T, changes, regimes, delta)\n        (0.0, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.9, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.99, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.999, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.99, 200, [20, 40, 80, 120], [1.0, 4.0, 0.5, 2.0, 1.5], 0.05),\n        (0.9999, 1000, [50, 120], [1.0, 10.0, 0.1], 0.05)\n    ]\n\n    results = []\n    for case in test_cases:\n        beta2, T, changes, regimes, delta = case\n        result = calculate_mean_lag(beta2, T, changes, regimes, delta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3096921"}, {"introduction": "在理解了动量和自适应缩放等独立组件后，探索它们的相互作用至关重要。在一个简化的、梯度呈周期性变化的环境中，优化器的长期行为可能并非显而易见，这种情况可以模拟小批量数据交替出现不同特征时的场景。本练习 [@problem_id:3097011] 将引导您推导并分析优化器在这种周期性信号下的稳态行为，揭示动量和自适应分母如何相互作用，共同决定最终的参数更新方向和大小。", "problem": "您需要分析一个结合了动量和自适应逐级缩放的一维优化器。考虑一个标量参数序列 $ \\theta_t $，它通过一个动量累积量 $ m_t $ 和一个二阶矩累积量 $ v_t $ 按如下方式更新。动量是梯度的指数移动平均（Exponential Moving Average, EMA），定义为 $ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $。二阶矩累积量是梯度平方的 EMA，定义为 $ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $。参数通过一个缩放步长进行更新 $ \\theta_{t+1} = \\theta_t - \\alpha \\, m_t / \\sqrt{v_t + \\varepsilon} $，其中 $ \\alpha  0 $ 是基础学习率，$ \\varepsilon  0 $ 是一个很小的常数。\n\n假设存在一个周期为 $ 2 $ 的确定性周期梯度信号：\n- 在偶数步 $ t = 0, 2, 4, \\dots $，梯度为 $ g_t = +a $。\n- 在奇数步 $ t = 1, 3, 5, \\dots $，梯度为 $ g_t = -b $。\n这里 $ a  0 $ 和 $ b  0 $ 是常数。这种情况模拟了在两个梯度幅值不等、符号相反的循环批次之间交替。\n\n从 $ m_t $、$ v_t $ 的定义以及上述更新规则出发，完成以下任务：\n1) 推导当周期性梯度长时间作用后，内部状态 $ (m_t, v_t) $ 所趋近的稳态双周期值 $ (m_{\\mathrm{even}}, v_{\\mathrm{even}}) $ 和 $ (m_{\\mathrm{odd}}, v_{\\mathrm{odd}}) $。将 $ m_{\\mathrm{even}} $ 和 $ m_{\\mathrm{odd}} $ 仅用 $ a $、$ b $ 和 $ \\beta_1 $ 表示，并将 $ v_{\\mathrm{even}} $ 和 $ v_{\\mathrm{odd}} $ 仅用 $ a $、$ b $ 和 $ \\beta_2 $ 表示。您必须直接从 EMA 定义出发进行推导，不得引入任何未经证明的简化。\n\n2) 使用稳态值，推导在一个完整周期内参数的净两步漂移，\n$$\n\\Delta_\\theta \\equiv \\theta_{t+2} - \\theta_t = - \\alpha \\left( \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}} + \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}} \\right),\n$$\n并将其尽可能地简化为关于 $ \\alpha, \\beta_1, \\beta_2, a, b, \\varepsilon $ 的函数。\n\n3) 实现一个程序，该程序：\n- 对以下测试套件中的每一组参数，使用第 $ 1 $ 部分得到的精确闭式稳态值计算 $ \\Delta_\\theta $。\n- 生成单行输出，其中包含结果。结果是一个以逗号分隔的十进制数列表，四舍五入到 $ 12 $ 位小数，并用方括号括起来，例如 $ [0.123000000000,-0.045600000000] $。\n\n使用以下参数集测试套件 $ (\\alpha, \\beta_1, \\beta_2, a, b, \\varepsilon) $：\n- 情况 $ 1 $ (理想情况): $ (0.001, 0.9, 0.999, 1.0, 0.5, 1\\mathrm{e}{-8}) $。\n- 情况 $ 2 $ (无动量): $ (0.001, 0.0, 0.9, 1.0, 0.5, 1\\mathrm{e}{-8}) $。\n- 情况 $ 3 $ (瞬时二阶矩): $ (0.001, 0.9, 0.0, 1.0, 0.5, 1\\mathrm{e}{-8}) $。\n- 情况 $ 4 $ (对称幅值): $ (0.001, 0.9, 0.999, 1.0, 1.0, 1\\mathrm{e}{-8}) $。\n- 情况 $ 5 $ (极端平滑与不平衡): $ (0.001, 0.99, 0.9999, 1.5, 0.2, 1\\mathrm{e}{-8}) $。\n\n所有量均为无量纲。您的程序必须按顺序计算五种情况下的五个 $ \\Delta_\\theta $ 值，并按照上述确切格式在单行上打印它们。不需要也不允许用户输入。最终输出必须只有一行，没有任何附加文本。", "solution": "该问题要求在确定性周期梯度信号下，分析一个带​​有动量和自适应缩放的一维优化器，该优化器类似于 Adam 优化器。我们必须首先验证问题陈述。该问题具有科学依据，定义明确且客观。它基于数值优化和指数移动平均的既定原则，提供了一套完整且一致的定义和参数。在简化的周期性输入下分析优化器的稳态行为，是理解其动态特性的一种标准且有价值的技术。因此，该问题被认为是有效的。我们继续进行求解。\n\n问题的核心是确定在双周期梯度下，动量累积量 $m_t$ 和二阶矩累积量 $v_t$ 的稳态行为。\n\n更新规则如下：\n$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$\n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n$\\theta_{t+1} = \\theta_t - \\alpha \\frac{m_t}{\\sqrt{v_t + \\varepsilon}}$\n\n梯度 $g_t$ 遵循一个双周期：\n$g_t = +a$ 对于偶数步 $t$ ($t=0, 2, \\dots$)\n$g_t = -b$ 对于奇数步 $t$ ($t=1, 3, \\dots$)\n其中 $a  0$ 且 $b  0$。\n\n**1. 稳态双周期值的推导**\n\n在稳态下，系统进入一个双周期循环，其中偶数步结束时累积量的值是恒定的，奇数步结束时的值也是恒定的。我们将这些稳态值表示为 $(m_{\\mathrm{even}}, v_{\\mathrm{even}})$ 和 $(m_{\\mathrm{odd}}, v_{\\mathrm{odd}})$。\n\n**动量累积量 ($m_t$)**\n\n考虑一个偶数时间步 $t$。动量为 $m_t$，在稳态下趋近于 $m_{\\mathrm{even}}$。前一个动量是 $m_{t-1}$，趋近于 $m_{\\mathrm{odd}}$。梯度为 $g_t = a$。递推关系变为：\n$$m_{\\mathrm{even}} = \\beta_1 m_{\\mathrm{odd}} + (1 - \\beta_1) a \\quad (1)$$\n\n现在考虑接下来的奇数时间步 $t+1$。动量为 $m_{t+1}$，趋近于 $m_{\\mathrm{odd}}$。前一个动量是 $m_t$，趋近于 $m_{\\mathrm{even}}$。梯度为 $g_{t+1} = -b$。递推关系变为：\n$$m_{\\mathrm{odd}} = \\beta_1 m_{\\mathrm{even}} + (1 - \\beta_1) (-b) \\quad (2)$$\n\n我们得到了一个关于 $m_{\\mathrm{even}}$ 和 $m_{\\mathrm{odd}}$ 的二元线性方程组。为了求解它，我们可以将方程 (2) 代入方程 (1)：\n$$m_{\\mathrm{even}} = \\beta_1 (\\beta_1 m_{\\mathrm{even}} - (1 - \\beta_1) b) + (1 - \\beta_1) a$$\n$$m_{\\mathrm{even}} = \\beta_1^2 m_{\\mathrm{even}} - \\beta_1 (1 - \\beta_1) b + (1 - \\beta_1) a$$\n$$m_{\\mathrm{even}} (1 - \\beta_1^2) = (1 - \\beta_1) a - \\beta_1 (1 - \\beta_1) b$$\n假设 $\\beta_1 \\neq 1$，我们可以除以 $(1 - \\beta_1)$：\n$$m_{\\mathrm{even}} (1 + \\beta_1) = a - \\beta_1 b$$\n$$m_{\\mathrm{even}} = \\frac{a - \\beta_1 b}{1 + \\beta_1}$$\n\n现在，将此结果代回方程 (2) 以求得 $m_{\\mathrm{odd}}$：\n$$m_{\\mathrm{odd}} = \\beta_1 \\left(\\frac{a - \\beta_1 b}{1 + \\beta_1}\\right) - (1 - \\beta_1) b$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b}{1 + \\beta_1} - \\frac{(1 - \\beta_1)(1 + \\beta_1)b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b - (1 - \\beta_1^2)b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b - b + \\beta_1^2 b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - b}{1 + \\beta_1}$$\n\n因此，稳态动量值为：\n$$m_{\\mathrm{even}} = \\frac{a - \\beta_1 b}{1 + \\beta_1} \\quad \\text{和} \\quad m_{\\mathrm{odd}} = \\frac{\\beta_1 a - b}{1 + \\beta_1}$$\n\n**二阶矩累积量 ($v_t$)**\n\n$v_t$ 的推导过程完全类似。递推关系为 $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$。梯度平方为：\n$g_t^2 = a^2$ 对于偶数步 $t$。\n$g_t^2 = (-b)^2 = b^2$ 对于奇数步 $t$。\n\n稳态值 $v_{\\mathrm{even}}$ 和 $v_{\\mathrm{odd}}$ 的方程组为：\n$$v_{\\mathrm{even}} = \\beta_2 v_{\\mathrm{odd}} + (1 - \\beta_2) a^2 \\quad (3)$$\n$$v_{\\mathrm{odd}} = \\beta_2 v_{\\mathrm{even}} + (1 - \\beta_2) b^2 \\quad (4)$$\n\n该方程组与动量的方程组具有相同的结构，只需进行替换：$\\beta_1 \\to \\beta_2$、$a \\to a^2$ 以及 $-b \\to b^2$。将这些替换应用于 $m_t$ 的解，可得：\n$$v_{\\mathrm{even}} = \\frac{a^2 - \\beta_2 (-b^2)}{1 + \\beta_2} = \\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2}$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + (-b^2)}{1 + \\beta_2} = \\frac{\\beta_2 a^2 - b^2}{1 + \\beta_2} \\quad \\text{（错误的类比应用）}$$\n\n为避免符号错误，直接重新推导更安全。将 (4) 代入 (3)：\n$$v_{\\mathrm{even}} = \\beta_2 (\\beta_2 v_{\\mathrm{even}} + (1 - \\beta_2) b^2) + (1 - \\beta_2) a^2$$\n$$v_{\\mathrm{even}} (1 - \\beta_2^2) = \\beta_2 (1 - \\beta_2) b^2 + (1 - \\beta_2) a^2$$\n$$v_{\\mathrm{even}} (1 + \\beta_2) = \\beta_2 b^2 + a^2$$\n$$v_{\\mathrm{even}} = \\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2}$$\n\n将此代入 (4)：\n$$v_{\\mathrm{odd}} = \\beta_2 \\left(\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2}\\right) + (1 - \\beta_2) b^2$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + \\beta_2^2 b^2 + (1 - \\beta_2^2)b^2}{1 + \\beta_2}$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + \\beta_2^2 b^2 + b^2 - \\beta_2^2 b^2}{1 + \\beta_2}$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2}$$\n\n正确的稳态二阶矩值为：\n$$v_{\\mathrm{even}} = \\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} \\quad \\text{和} \\quad v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2}$$\n\n**2. 净两步漂移（$\\Delta_\\theta$）的推导**\n\n一个完整周期（两步）内的净漂移 $\\Delta_\\theta$ 定义为 $\\theta_{t+2} - \\theta_t$。假设步 $t$ 是一个偶数步。参数更新如下：\n第 1 步（从 $t$ 到 $t+1$）：\n$$\\theta_{t+1} = \\theta_t - \\alpha \\frac{m_t}{\\sqrt{v_t + \\varepsilon}}$$\n在稳态下，这变为：\n$$\\theta_{t+1} - \\theta_t = - \\alpha \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}}$$\n\n第 2 步（从 $t+1$ 到 $t+2$）：\n$$\\theta_{t+2} = \\theta_{t+1} - \\alpha \\frac{m_{t+1}}{\\sqrt{v_{t+1} + \\varepsilon}}$$\n在稳态下，这变为：\n$$\\theta_{t+2} - \\theta_{t+1} = - \\alpha \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}}$$\n\n总漂移是这两次变化的总和：\n$$\\Delta_\\theta = (\\theta_{t+2} - \\theta_{t+1}) + (\\theta_{t+1} - \\theta_t) = - \\alpha \\left( \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}} + \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}} \\right)$$\n\n代入推导出的稳态表达式，得到净漂移的最终公式：\n$$ \\Delta_\\theta = - \\alpha \\left( \\frac{\\frac{a - \\beta_1 b}{1 + \\beta_1}}{\\sqrt{\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} + \\varepsilon}} + \\frac{\\frac{\\beta_1 a - b}{1 + \\beta_1}}{\\sqrt{\\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2} + \\varepsilon}} \\right) $$\n该表达式可以稍微进行因式分解：\n$$ \\Delta_\\theta = - \\frac{\\alpha}{1 + \\beta_1} \\left( \\frac{a - \\beta_1 b}{\\sqrt{\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} + \\varepsilon}} + \\frac{\\beta_1 a - b}{\\sqrt{\\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2} + \\varepsilon}} \\right) $$\n此表达式是两步漂移的闭式解，将用于实现并计算数值结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the net two-step drift of a parameter for a one-dimensional\n    optimizer with periodic gradients, based on a derived closed-form solution\n    for the steady-state behavior of its internal accumulators.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (alpha, beta1, beta2, a, b, epsilon)\n    test_cases = [\n        (0.001, 0.9, 0.999, 1.0, 0.5, 1e-8),      # Case 1 (happy path)\n        (0.001, 0.0, 0.9, 1.0, 0.5, 1e-8),       # Case 2 (no momentum)\n        (0.001, 0.9, 0.0, 1.0, 0.5, 1e-8),       # Case 3 (instantaneous second moment)\n        (0.001, 0.9, 0.999, 1.0, 1.0, 1e-8),      # Case 4 (symmetric magnitudes)\n        (0.001, 0.99, 0.9999, 1.5, 0.2, 1e-8),   # Case 5 (extreme smoothing and imbalance)\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta1, beta2, a, b, epsilon = case\n\n        # Calculate steady-state momentum values\n        # m_even = (a - beta1 * b) / (1 + beta1)\n        # m_odd = (beta1 * a - b) / (1 + beta1)\n        # Handle beta1 = -1 case to avoid division by zero, though not in test cases\n        if beta1 == -1.0:\n            # The recurrence does not converge to a 2-cycle in this case.\n            # However, problem constraints imply beta1 is not -1.\n            # For a numerically robust implementation, one might add handling.\n            # We proceed assuming beta1 != -1 as per standard optimizer design.\n            pass\n\n        m_even = (a - beta1 * b) / (1.0 + beta1)\n        m_odd = (beta1 * a - b) / (1.0 + beta1)\n\n        # Calculate steady-state second-moment values\n        # v_even = (a**2 + beta2 * b**2) / (1 + beta2)\n        # v_odd = (beta2 * a**2 + b**2) / (1 + beta2)\n        # Handle beta2 = -1 case.\n        if beta2 == -1.0:\n            pass # Similar logic as for beta1\n        \n        v_even = (a**2 + beta2 * b**2) / (1.0 + beta2)\n        v_odd = (beta2 * a**2 + b**2) / (1.0 + beta2)\n\n        # Calculate the two terms of the parameter update\n        term_even = m_even / np.sqrt(v_even + epsilon)\n        term_odd = m_odd / np.sqrt(v_odd + epsilon)\n\n        # Calculate the net two-step drift delta_theta\n        delta_theta = -alpha * (term_even + term_odd)\n        \n        results.append(delta_theta)\n\n    # Final print statement in the exact required format.\n    # The format specifier .12f ensures rounding to 12 decimal places.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3097011"}, {"introduction": "在实际训练中，梯度偶尔会出现异常大的“尖峰”或“毛刺”，这可能严重破坏训练的稳定性，导致参数更新过大。虽然简单的全局梯度裁剪是一种常用方法，但它忽略了不同参数具有不同尺度的事实。本练习 [@problem_id:3097005] 提出了一种更智能的解决方案：尺度感知裁剪。您将从第一性原理出发，设计并验证一个裁剪规则，该规则的边界与梯度自身的历史尺度成正比，从而在有效抑制“梯度爆炸”的同时，保留自适应学习率的优势。", "problem": "本题要求您从第一性原理出发，对自适应学习率算法中的尺度感知裁剪进行推理，并在一个完整的、可运行的程序中实现并测试您的结论。请从标准的均方根传播（RMSProp）更新方法开始，这是一种自适应方法，通过梯度平方的指数移动平均来对每个参数的梯度进行重新缩放。具体来说，设 $i$ 为参数索引，$t$ 为步骤索引。设 $g_{i,t}$ 表示在步骤 $t$ 时可微目标函数的随机梯度，并通过指数递归 $v_{i,t} \\leftarrow \\beta v_{i,t-1} + (1-\\beta) g_{i,t}^{2}$ 定义二阶矩累积量 $v_{i,t}$，其中 $v_{i,0} = 0$ 且 $\\beta \\in [0,1)$。在时间 $t$ 对参数 $i$ 的基线 RMSProp 步长为 $\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\, g_{i,t} / \\left(\\sqrt{v_{i,t-1}} + \\varepsilon\\right)$，其中学习率 $\\alpha  0$，稳定项 $\\varepsilon \\ge 0$，然后进行 $\\theta_{i,t} \\leftarrow \\theta_{i,t-1} + \\Delta \\theta_{i,t}$。请注意，预处理使用 $v_{i,t-1}$ 以避免在步骤 $t$ 内产生循环依赖。\n\n任务 A（推导）。提出并论证一个逐参数裁剪规则，该规则与二阶矩累积量的平方根成正比，并由一个常数 $\\lambda  0$ 和相同的稳定项 $\\varepsilon \\ge 0$ 参数化。您的规则应以关于裁剪后梯度 $\\tilde{g}_{i,t}$ 的显式不等式形式表述，该不等式涉及 $v_{i,t-1}$、$\\lambda$ 和 $\\varepsilon$。根据上述 RMSProp 定义和您的裁剪规则，推导出一个形式为 $\\lvert \\Delta \\theta_{i,t} \\rvert \\le \\text{(expression independent of $g_{i,t}$)}$ 的步长边界，该边界不依赖于 $g_{i,t}$ 的无界大小，并解释为什么这有望在减少爆炸性更新的同时，保留 RMSProp 的自适应重缩放益处。\n\n任务 B（实现与测试）。在一个合成的凸二次目标函数 $f(\\theta) = \\tfrac{1}{2} \\sum_{i=1}^{d} c_i \\, \\theta_i^2$ 上实现两个优化器，其中 $d$ 是维度，曲率向量 $c \\in \\mathbb{R}^{d}_{0}$ 在每次测试中提供。精确梯度为 $g_{i,t}^{\\text{base}} = c_i \\, \\theta_{i,t-1}$。为模拟突发性或重尾随机性，在步骤 $t$ 用于更新的观测梯度为 $g_{i,t} = g_{i,t}^{\\text{base}} + s_{i,t}$，其中尖峰安排 $s_{i,t}$ 是一个稀疏向量，仅在指定的 $(t,i)$ 对处有非零项。请实现：\n- 一个基线 RMSProp 变体（无裁剪）：在步长计算中使用 $g_{i,t}$，并使用标准递归更新 $v_{i,t}$。\n- 一个尺度感知裁剪变体：将您提出的逐参数裁剪规则应用于 $g_{i,t}$ 以获得 $\\tilde{g}_{i,t}$，然后在步长计算中使用 $\\tilde{g}_{i,t}$，同时仍使用未修改的 $g_{i,t}$ 通过标准递归更新 $v_{i,t}$。\n\n对于每次运行，跟踪最大逐参数绝对步长 $M = \\max_{t,i} \\lvert \\Delta \\theta_{i,t} \\rvert$ 和在 $T$ 步结束时的最终目标函数值 $f(\\theta_T)$。\n\n为每个测试用例 $j$ 定义两个评估谓词：\n- 爆炸性更新减少谓词 $E_j$：如果基线版本相对于提供的阈值 $\\tau$ 表现出过大步长，而裁剪版本的最大步长受您推导的边界所限制，则该谓词为真。形式上，如果 $\\left(M^{\\text{base}}  \\tau\\right)$ 且 $\\left(M^{\\text{clip}} \\le \\alpha \\lambda + \\text{tol}\\right)$，则 $E_j$ 为真，其中 $\\text{tol}  0$ 是一个小的数值容差。\n- 自适应益处保持谓词 $P_j$：如果裁剪版本的最终损失不比基线版本差超过一个提供的相对容差 $\\delta  0$，则该谓词为真，即 $f^{\\text{clip}}(\\theta_T) \\le (1+\\delta) \\, f^{\\text{base}}(\\theta_T)$。\n\n对于每个测试用例 $j$，如果 $E_j$ 和 $P_j$ 都为真，则输出 $r_j = 1$，否则输出 $r_j = 0$。\n\n测试套件。您的程序必须精确运行以下 $3$ 个测试用例，每个用例由一个包含 $(d, c, \\theta_0, T, \\alpha, \\beta, \\varepsilon, \\lambda, \\text{spikes}, \\tau, \\delta, \\text{tol})$ 的元组指定：\n- 用例 1（在低曲率坐标上出现大尖峰的理想路径）：\n  - $d = 3$\n  - $c = [1.0, 0.1, 10.0]$\n  - $\\theta_0 = [1.0, 1.0, 1.0]$\n  - $T = 60$\n  - $\\alpha = 0.05$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 2.0$\n  - spikes: 在步骤 $t=10$ 时，为坐标 $i=2$（从零开始的索引）增加 $+200.0$；在步骤 $t=20$ 时，为坐标 $i=2$ 增加 $-150.0$\n  - $\\tau = 50.0$\n  - $\\delta = 0.5$\n  - $\\text{tol} = 10^{-12}$\n- 用例 2（无尖峰，裁剪应处于非活动状态的边界条件）：\n  - $d = 3$\n  - $c = [1.0, 0.3, 3.0]$\n  - $\\theta_0 = [1.5, -0.5, 0.75]$\n  - $T = 100$\n  - $\\alpha = 0.05$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 5.0$\n  - spikes: 无\n  - $\\tau = 1.0$\n  - $\\delta = 0.05$\n  - $\\text{tol} = 10^{-12}$\n- 用例 3（在一个极低曲率坐标上出现多个尖峰，并在别处出现一个中等尖峰）：\n  - $d = 4$\n  - $c = [1.0, 0.2, 5.0, 0.05]$\n  - $\\theta_0 = [1.0, 1.0, 1.0, 1.0]$\n  - $T = 120$\n  - $\\alpha = 0.04$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 1.5$\n  - spikes: 在步骤 $t=5$ 时为坐标 $i=3$ 增加 $+300.0$；在步骤 $t=6$ 时为坐标 $i=3$ 增加 $-300.0$；在步骤 $t=30$ 时为坐标 $i=0$ 增加 $+150.0$\n  - $\\tau = 30.0$\n  - $\\delta = 0.5$\n  - $\\text{tol} = 10^{-12}$\n\n角度单位不适用。未使用物理单位。您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表，无空格，并按测试用例的顺序排列。例如，如果 $r_1 = 1$，$r_2 = 0$，$r_3 = 1$，则要求的输出为“[1,0,1]”。", "solution": "已对问题陈述进行了严格验证。问题被认定为具有科学依据、良定，并且为得出唯一且有意义的解所需的所有数据和条件均已提供。因此，该问题被视为**有效**。注意到测试用例1的描述性文本中存在一个微小的不一致，其描述为在“低曲率坐标”上存在尖峰，而提供的数据 `c = [1.0, 0.1, 10.0]` 和 `spikes` 在坐标 `i=2` 上，将尖峰放置在了曲率*最高*的坐标上（$c_2=10.0$）。这被认为是一个非关键性描述错误，因为形式化的问题规范是无歧义且自洽的。我们现在继续进行求解。\n\n### 任务A：尺度感知裁剪和步长边界的推导\n\n目标是为 RMSProp 优化器提出并论证一个逐参数裁剪规则，该规则与梯度的估计尺度成正比。RMSProp 算法为每个参数维护一个二阶矩累积量 $v_{i,t}$，该累积量跟踪梯度平方的指数移动平均：\n$$v_{i,t} \\leftarrow \\beta v_{i,t-1} + (1-\\beta) g_{i,t}^{2}$$\n这里，$g_{i,t}$ 是目标函数关于参数 $\\theta_i$ 在步骤 $t$ 的梯度，$\\beta \\in [0,1)$ 是一个衰减因子。参数 $i$ 近期梯度的大小由 $\\sqrt{v_{i,t-1}}$ 来估计。标准的 RMSProp 更新通过此尺度估计（加上一个小的稳定项 $\\varepsilon \\ge 0$）来归一化梯度：\n$$\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\frac{g_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n\n**提出的裁剪规则**\n\n为了使裁剪具有“尺度感知”能力，我们定义一个与梯度尺度估计成正比的裁剪阈值。问题指定该阈值应由一个常数 $\\lambda  0$ 参数化。对于参数 $i$ 在步骤 $t$ 的裁剪阈值，一个自然的选择是 $C_{i,t} = \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$。这直接将裁剪边界与 RMSProp 更新本身使用的自适应缩放因子联系起来。\n\n裁剪规则将原始梯度 $g_{i,t}$ 转换为一个裁剪后的梯度 $\\tilde{g}_{i,t}$。我们强制要求裁剪后梯度的大小不超过此阈值。这可以表述为关于裁剪后梯度 $\\tilde{g}_{i,t}$ 的以下不等式：\n$$|\\tilde{g}_{i,t}| \\le \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$$\n这个规则可以通过将 $g_{i,t}$ 的值裁剪到范围 $[-C_{i,t}, C_{i,t}]$ 内来实现。\n\n**步长边界的推导**\n\n在建立了尺度感知裁剪规则后，我们现在考虑使用裁剪后梯度 $\\tilde{g}_{i,t}$ 的修改后的参数更新：\n$$\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\frac{\\tilde{g}_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n为了推导此更新步长大小 $|\\Delta \\theta_{i,t}|$ 的边界，我们取该表达式的绝对值：\n$$|\\Delta \\theta_{i,t}| = \\left| -\\alpha \\frac{\\tilde{g}_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon} \\right| = \\alpha \\frac{|\\tilde{g}_{i,t}|}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n现在，我们代入我们提出的裁剪规则中的不等式 $|\\tilde{g}_{i,t}| \\le \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$:\n$$|\\Delta \\theta_{i,t}| \\le \\alpha \\frac{\\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n项 $(\\sqrt{v_{i,t-1}} + \\varepsilon)$ 被约去，得到最终的步长边界：\n$$|\\Delta \\theta_{i,t}| \\le \\alpha \\lambda$$\n这个边界是一个优雅的结果：参数更新的大小保证不大于学习率 $\\alpha$ 和裁剪参数 $\\lambda$ 的乘积。至关重要的是，这个边界完全独立于原始未裁剪梯度 $g_{i,t}$ 的大小。\n\n**理由与益处**\n\n这种尺度感知裁剪机制提供了两个主要优点：\n\n1.  **减少爆炸性更新**：推导出的边界 $|\\Delta \\theta_{i,t}| \\le \\alpha \\lambda$ 表明，大的、异常的梯度（尖峰）不会导致任意大的参数更新。通过在更新步骤中使用梯度之前对其进行裁剪，该算法防止此类事件破坏训练过程的稳定性，并将参数推向损失景观的不良区域。\n\n2.  **保留自适应重缩放的益处**：裁剪并非使用单一的全局阈值执行。相反，阈值 $C_{i,t}$ 是自适应的且特定于参数，因为它依赖于 $v_{i,t-1}$。对于天然具有较大梯度的参数，$v_{i,t-1}$ 会较大，导致一个更宽松的裁剪阈值。相反，对于梯度较小的参数，阈值会更严格。这保留了 RMSProp 的根本益处，即根据其历史梯度尺度，逐参数地调整有效学习率。此外，问题指定二阶矩累积量 $v_{i,t}$ 使用*未修改*的梯度 $g_{i,t}$ 进行更新。这是一个关键的设计选择。它确保了 $v_{i,t}$ 始终是梯度真实二阶矩（包括任何尖峰）的一个准确、无偏的估计量。如果 $v_{i,t}$ 使用裁剪后的梯度 $\\tilde{g}_{i,t}$ 进行更新，它将系统性地低估梯度方差，从而随着时间的推移损害自适应缩放本身的完整性。\n\n### 任务B：实现与测试\n\n我们现在进入实现和测试阶段。将在一个合成的凸二次目标上模拟两个优化器，一个基线 RMSProp 和我们的尺度感知裁剪变体。它们的性能将根据三个测试用例中关于爆炸性更新减少和自适应益处保持的指定谓词进行评估。最终程序将封装整个模拟和评估过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the complete simulation and print the final result.\n    It defines test cases and calls helper functions to perform the optimization\n    and evaluation logic.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Spike on the highest-curvature coordinate.\n        (3, [1.0, 0.1, 10.0], [1.0, 1.0, 1.0], 60, 0.05, 0.9, 1e-3, 2.0, {(10, 2): 200.0, (20, 2): -150.0}, 50.0, 0.5, 1e-12),\n        # Case 2: No spikes, testing behavior from initial state.\n        (3, [1.0, 0.3, 3.0], [1.5, -0.5, 0.75], 100, 0.05, 0.9, 1e-3, 5.0, {}, 1.0, 0.05, 1e-12),\n        # Case 3: Multiple spikes on different coordinates.\n        (4, [1.0, 0.2, 5.0, 0.05], [1.0, 1.0, 1.0, 1.0], 120, 0.04, 0.9, 1e-3, 1.5, {(5, 3): 300.0, (6, 3): -300.0, (30, 0): 150.0}, 30.0, 0.5, 1e-12),\n    ]\n\n    def _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, mode):\n        \"\"\"\n        Runs a single optimization trajectory for either baseline or clipped RMSProp.\n        \n        Returns:\n            - max_abs_step (float): The maximum absolute step magnitude over the trajectory.\n            - final_loss (float): The final value of the objective function.\n        \"\"\"\n        # Use np.float64 for higher numerical precision and stability.\n        theta = np.array(theta0, dtype=np.float64)\n        c_arr = np.array(c, dtype=np.float64)\n        v = np.zeros(d, dtype=np.float64)\n        max_abs_step = 0.0\n\n        for t_step in range(1, T + 1):\n            # Calculate the base gradient from the convex quadratic objective\n            grad_base = c_arr * theta\n            \n            # Construct the sparse spike vector for the current step\n            spikes = np.zeros(d, dtype=np.float64)\n            for i in range(d):\n                if (t_step, i) in spikes_dict:\n                    spikes[i] = spikes_dict.get((t_step, i), 0.0)\n            \n            # The observed gradient is the sum of the base gradient and the spike\n            grad_observed = grad_base + spikes\n\n            # Denominator for the RMSProp update\n            denominator = np.sqrt(v) + epsilon\n\n            if mode == 'baseline':\n                # Standard RMSProp update uses the observed gradient\n                step = -alpha * grad_observed / denominator\n            elif mode == 'clipped':\n                # Scale-aware clipping: threshold is proportional to the scale estimate\n                clip_threshold = lam * denominator\n                grad_clipped = np.clip(grad_observed, -clip_threshold, clip_threshold)\n                # The step uses the clipped gradient\n                step = -alpha * grad_clipped / denominator\n            else:\n                raise ValueError(\"Invalid optimizer mode specified.\")\n\n            # Track the maximum absolute per-parameter step magnitude\n            current_max_abs_step = np.max(np.abs(step))\n            if current_max_abs_step > max_abs_step:\n                max_abs_step = current_max_abs_step\n            \n            # Update parameters\n            theta += step\n            \n            # Update the second-moment accumulator using the unclipped gradient\n            v = beta * v + (1 - beta) * np.square(grad_observed)\n\n        # Calculate the final objective value\n        final_loss = 0.5 * np.sum(c_arr * np.square(theta))\n        \n        return max_abs_step, final_loss\n\n    def _run_simulation(case_params):\n        \"\"\"\n        Manages a single test case, running both optimizers and evaluating predicates.\n        \n        Returns:\n            - 1 if both predicates (E_j and P_j) are true, 0 otherwise.\n        \"\"\"\n        d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, tau, delta, tol = case_params\n\n        # Run both baseline and clipped optimizers\n        M_base, f_final_base = _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, 'baseline')\n        M_clip, f_final_clip = _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, 'clipped')\n\n        # Evaluate Predicate E_j: Exploding-update reduction\n        # Checks if the baseline step exploded while the clipped step remained bounded as derived.\n        e_j = (M_base > tau) and (M_clip = alpha * lam + tol)\n        \n        # Evaluate Predicate P_j: Adaptive-benefit preservation\n        # Checks if the clipped version's final loss is not substantially worse than the baseline.\n        p_j = f_final_clip = (1 + delta) * f_final_base\n\n        return 1 if e_j and p_j else 0\n\n    results = []\n    for case in test_cases:\n        result = _run_simulation(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3097005"}]}