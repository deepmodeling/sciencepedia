{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本节的第一个练习将引导你通过代码，在一个受控的一维二次损失函数环境中，亲手实现并对比两种核心的学习率衰减策略：阶梯衰减（step decay）和指数衰减（exponential decay）。通过这个练习，你将直观地理解这两种策略如何影响优化过程的动态，并为更复杂的场景打下坚实的基础。[@problem_id:3176459]", "problem": "要求您实现并分析在一维严格凸二次损失函数上使用离散时间梯度下降的学习率调度方案。重点是对比指数衰减（由半衰期参数化）和步进衰减（由一组下降时间参数化），并评估它们在多个数据集上性能的一致性。您的程序必须是一个完整的、可运行的单一程序，无需任何用户输入即可生成指定的输出。\n\n从以下基本原理开始：\n\n- 对于具有标量参数的可微函数，离散时间梯度下降的更新规则如下：对于形式为 $f(w) = \\tfrac{1}{2} a (w - b)^{2}$ 且 $a \\gt 0$ 的二次损失，其梯度为 $\\nabla f(w) = a (w - b)$。在第 $t$ 步使用随时间变化的学习率 $\\eta_{t}$ 时，更新公式为\n$$\nw_{t+1} = w_{t} - \\eta_{t} \\, a \\, (w_{t} - b).\n$$\n- 定义误差 $e_{t} = w_{t} - b$。则其动态满足\n$$\ne_{t+1} = \\left(1 - a \\, \\eta_{t}\\right) e_{t}.\n$$\n这些关系是唯一的出发点；您使用的任何其他关系都必须从中推导出来。\n\n需要实现的调度方案：\n\n- 由半衰期参数化的指数衰减调度方案。半衰期为 $h$ 步意味着在第 $h$ 步的学习率恰好是其初始值的一半。设 $\\eta_{0}$ 表示初始学习率， $T$ 表示总更新步数。您必须确定衰减率，使得对于给定的训练长度分数 $f$ ，在 $h = f \\, T$ 时半衰期条件成立。\n\n- 在预设的里程碑步骤中以 $1/2$ 的因子进行乘性下降的步进衰减调度方案。对于一组里程碑分数 $\\{f_{1}, f_{2}, \\ldots, f_{k}\\}$，下降步骤为整数里程碑 $\\{\\lfloor f_{1} T \\rfloor, \\lfloor f_{2} T \\rfloor, \\ldots, \\lfloor f_{k} T \\rfloor\\}$。在每个这样的步骤 $t$，学习率在应用该步骤的更新之前乘以 $1/2$。如果多个分数映射到同一个整数步骤，则通过每个里程碑最多使用一次来合并重复项。\n\n必须精确比较六个调度方案，索引如下：\n\n- 索引 $0$：指数衰减，半衰期分数为 $0.05$。\n- 索引 $1$：指数衰减，半衰期分数为 $0.10$。\n- 索引 $2$：指数衰减，半衰期分数为 $0.20$。\n- 索引 $3$：步进衰减，在分数 $\\{0.05\\}$ 处下降。\n- 索引 $4$：步进衰减，在分数 $\\{0.05, 0.10\\}$ 处下降。\n- 索引 $5$：步进衰减，在分数 $\\{0.05, 0.10, 0.20\\}$ 处下降。\n\n对于每个测试用例中的每个调度方案和每个数据集，计算 $T$ 步后的确切最终损失，\n$$\nL = \\tfrac{1}{2} a \\, e_{T}^{2},\n$$\n其中 $e_{T}$ 由更新动态和所选的调度方案确定。不要模拟带噪声的梯度；使用更新所隐含的精确二次动态。\n\n测试用例中跨数据集的评估：\n\n- 对于每个调度方案索引 $s \\in \\{0,1,2,3,4,5\\}$，计算该测试用例中所有数据集的最终损失的平均值；将此平均值表示为 $M_{s}$。\n- 令 $s^{\\star}$ 为实现 $\\min_{s} M_{s}$ 的最小索引。\n- 对于每个数据集，单独找出在该数据集上产生最小最终损失的最小调度方案索引；统计有多少个数据集选择了 $s^{\\star}$。将此计数表示为 $C$。\n- 报告该测试用例的三元组 $[s^{\\star}, C, M_{s^{\\star}}]$。\n\n约定和精确定义：\n\n- 总步数 $T$ 是一个正整数，步数由 $t \\in \\{0,1,\\ldots,T-1\\}$ 索引。\n- 对于指数衰减，第 $t$ 步的学习率必须在 $t=0$ 时满足 $\\eta_{t} = \\eta_{0}$，并且在指定分数 $f$ 的 $t = h$ 时满足半衰期条件。您必须从第一性原理推导出相应的衰减率。\n- 对于步进衰减，在里程碑步骤 $t_{\\mathrm{milestone}}$，用于该步骤的学习率在应用该步骤的更新之前通过乘性下降进行更新。\n- 对于每个数据集，在同一个测试用例中对所有调度方案使用相同的初始参数 $w_{0}$。\n- 对于任何对数推导，使用自然对数。\n- 此任务不涉及任何物理单位。\n\n测试套件：\n\n实现您的程序以运行以下三个测试用例。每个测试用例包含一个初始学习率 $\\eta_{0}$、一个总步数 $T$、一个共享的初始参数 $w_{0}$，以及一个数据集列表，其中每个数据集是一个定义 $f(w) = \\tfrac{1}{2} a (w-b)^{2}$ 的数对 $(a,b)$。\n\n- 测试用例 $1$：\n  - $\\eta_{0} = 0.15$, $T = 100$, $w_{0} = 5.0$。\n  - 数据集：$(a,b) \\in \\{(1.0, 2.0), (3.0, -1.0), (10.0, 0.5)\\}$。\n\n- 测试用例 $2$：\n  - $\\eta_{0} = 0.12$, $T = 120$, $w_{0} = -3.0$。\n  - 数据集：$(a,b) \\in \\{(0.8, 0.0), (2.5, 1.0), (6.0, -2.0)\\}$。\n\n- 测试用例 $3$：\n  - $\\eta_{0} = 0.18$, $T = 60$, $w_{0} = 2.0$。\n  - 数据集：$(a,b) \\in \\{(1.5, 1.0), (4.0, -2.0), (9.0, 3.0)\\}$。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序为每个测试用例包含一个元素，其中每个元素是上面定义的列表 $[s^{\\star}, C, M_{s^{\\star}}]$。例如，一个语法上有效的输出行看起来像\n$[ [0,3,0.00123], [2,2,0.00456], [5,1,0.00789] ]$\n但其中的实际数值由您的实现确定。", "solution": "该问题要求对一维二次损失函数的两种学习率衰减调度方案——指数衰减和步进衰减进行分析。分析的核心在于误差项 $e_t = w_t - b$ 的离散时间动态，该误差项表示参数 $w_t$ 与最优值 $b$ 之间的偏差。\n\n第 $t+1$ 步误差的控制方程由梯度下降更新规则推导得出：\n$$\nw_{t+1} = w_t - \\eta_t \\nabla f(w_t)\n$$\n对于指定的二次损失 $f(w) = \\frac{1}{2} a (w - b)^2$，其梯度为 $\\nabla f(w) = a(w-b)$。代入此梯度和误差 $e_t$ 的定义可得：\n$$\nw_{t+1} - b = (w_t - b) - \\eta_t a (w_t - b)\n$$\n$$\ne_{t+1} = e_t - \\eta_t a e_t = (1 - a \\eta_t) e_t\n$$\n这个递推关系是模拟优化过程的基础。经过 $T-1$ 步后的最终误差 $e_T$ 可以表示为初始误差 $e_0 = w_0 - b$ 与每一步的缩减因子的乘积：\n$$\ne_T = e_0 \\prod_{t=0}^{T-1} (1 - a \\eta_t)\n$$\n然后，最终损失计算为 $L = \\frac{1}{2} a e_T^2$。我们的分析涉及为不同的学习率调度方案 $\\eta_t$ 计算这个最终损失。\n\n两种类型的学习率调度方案定义如下：\n\n1.  **指数衰减调度方案**：第 $t$ 步的学习率由等比数列 $\\eta_t = \\eta_0 \\gamma^t$ 给出，其中 $\\eta_0$ 是初始学习率，$\\gamma \\in (0, 1)$ 是衰减率。问题规定衰减率 $\\gamma$ 必须由半衰期条件确定：在第 $h = fT$ 步的学习率是其初始值的一半，其中 $f$ 是给定的分数， $T$ 是总步数。我们从此条件推导出 $\\gamma$：\n    $$\n    \\eta_h = \\eta_0 \\gamma^h = \\frac{1}{2} \\eta_0\n    $$\n    $$\n    \\gamma^h = \\frac{1}{2}\n    $$\n    对两边取自然对数：\n    $$\n    h \\ln(\\gamma) = \\ln\\left(\\frac{1}{2}\\right) = -\\ln(2)\n    $$\n    解出 $\\gamma$：\n    $$\n    \\ln(\\gamma) = -\\frac{\\ln(2)}{h} \\implies \\gamma = \\exp\\left(-\\frac{\\ln(2)}{h}\\right)\n    $$\n    代入 $h=fT$，衰减率为 $\\gamma = \\exp\\left(-\\frac{\\ln(2)}{fT}\\right)$，第 $t$ 步的学习率为：\n    $$\n    \\eta_t = \\eta_0 \\exp\\left(-\\frac{t \\ln(2)}{fT}\\right)\n    $$\n\n2.  **步进衰减调度方案**：学习率保持不变，然后在预定义的里程碑步骤进行乘性缩减。给定一组里程碑分数 $\\{f_1, f_2, \\ldots, f_k\\}$，对应的整数里程碑步骤为 $T_{\\text{miles}} = \\{\\lfloor f_1 T \\rfloor, \\lfloor f_2 T \\rfloor, \\ldots, \\lfloor f_k T \\rfloor\\}$，并移除重复项。学习率从 $\\eta_0$ 开始。在每个时间步 $t \\in T_{\\text{miles}}$，学习率在用于该步的梯度下降更新*之前*乘以 $1/2$。设 $\\eta'_t$ 为在第 $t$ 步可能发生下降之前的学习率。那么 $\\eta'_0 = \\eta_0$，且对于 $t0$, $\\eta'_{t} = \\eta_{t-1}$。用于第 $t$ 步更新的学习率 $\\eta_t$ 为：\n    $$\n    \\eta_t = \\begin{cases} \\frac{1}{2} \\eta'_t  \\text{if } t \\in T_{\\text{miles}} \\\\ \\eta'_t  \\text{otherwise} \\end{cases}\n    $$\n\n模拟过程首先初始化误差 $e_0 = w_0 - b$，然后对于六个指定的调度方案中的每一个，使用相应的 $\\eta_t$ 为 $t \\in \\{0, 1, \\ldots, T-1\\}$ 迭代应用误差更新 $e_{t+1} = (1 - a \\eta_t) e_t$。\n\n在为每个调度方案和每个数据集计算出最终损失 $L$ 后，对每个测试用例进行评估。\n-   对于每个调度方案索引 $s \\in \\{0, \\dots, 5\\}$，计算该测试用例中所有数据集的平均最终损失 $M_s$。\n-   平均表现最佳的调度方案 $s^\\star$ 被确定为达到最小平均损失的最小索引：$s^\\star = \\arg\\min_s M_s$。\n-   为了衡量这个最佳调度方案的一致性，我们统计有多少个独立的数据集也认为 $s^\\star$ 是它们的最优调度方案。这个计数表示为 $C$。\n-   该测试用例的最终结果以三元组 $[s^\\star, C, M_{s^\\star}]$ 的形式报告。\n\n这个结构化的流程允许基于所提供的动态和参数，对学习率调度方案进行严谨且可复现的比较。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {\n            \"eta0\": 0.15, \"T\": 100, \"w0\": 5.0,\n            \"datasets\": [(1.0, 2.0), (3.0, -1.0), (10.0, 0.5)]\n        },\n        {\n            \"eta0\": 0.12, \"T\": 120, \"w0\": -3.0,\n            \"datasets\": [(0.8, 0.0), (2.5, 1.0), (6.0, -2.0)]\n        },\n        {\n            \"eta0\": 0.18, \"T\": 60, \"w0\": 2.0,\n            \"datasets\": [(1.5, 1.0), (4.0, -2.0), (9.0, 3.0)]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_simulation(case[\"eta0\"], case[\"T\"], case[\"w0\"], case[\"datasets\"])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format [ [r1], [r2], ... ] from the example is illustrative of structure.\n    # The code template provides the authoritative format generation rule.\n    # str([1, 2, 3]) -> '[1, 2, 3]' which includes spaces.\n    # ','.join(['[1,2]', '[3,4]']) -> '[1,2],[3,4]' which has no space between elements.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef run_simulation(eta0, T, w0, datasets):\n    \"\"\"\n    Runs the simulation and evaluation for a single test case.\n    \"\"\"\n    # Schedule definitions\n    exp_decay_fractions = [0.05, 0.10, 0.20]\n    step_decay_fractions_sets = [[0.05], [0.05, 0.10], [0.05, 0.10, 0.20]]\n    num_schedules = len(exp_decay_fractions) + len(step_decay_fractions_sets)\n    \n    all_schedules_losses = [[] for _ in range(num_schedules)]\n\n    # Loop through each of the 6 schedules\n    for s_idx in range(num_schedules):\n        # Loop through each dataset for the current schedule\n        for a, b in datasets:\n            e = w0 - b\n            \n            # Run gradient descent for T steps\n            if s_idx  3: # Exponential decay schedules (indices 0, 1, 2)\n                f = exp_decay_fractions[s_idx]\n                h = f * T\n                # Handle potential h=0, though not possible with given data\n                gamma = np.exp(-np.log(2.0) / h) if h > 0 else 1.0\n                \n                for t in range(T):\n                    eta_t = eta0 * (gamma ** t)\n                    e = e * (1.0 - a * eta_t)\n            \n            else: # Step decay schedules (indices 3, 4, 5)\n                fracs = step_decay_fractions_sets[s_idx - 3]\n                milestones = {int(f * T) for f in fracs}\n                current_eta = eta0\n                \n                for t in range(T):\n                    if t in milestones:\n                        current_eta /= 2.0\n                    eta_t = current_eta\n                    e = e * (1.0 - a * eta_t)\n            \n            # Calculate final loss\n            final_loss = 0.5 * a * e**2\n            all_schedules_losses[s_idx].append(final_loss)\n            \n    # Evaluation\n    mean_losses = [np.mean(losses) for losses in all_schedules_losses]\n    \n    s_star = int(np.argmin(mean_losses))\n    m_s_star = mean_losses[s_star]\n    \n    # Count how many datasets selected s_star as their best schedule\n    c = 0\n    losses_by_dataset = np.array(all_schedules_losses).T\n    for dataset_losses in losses_by_dataset:\n        best_s_for_dataset = int(np.argmin(dataset_losses))\n        if best_s_for_dataset == s_star:\n            c += 1\n            \n    return [s_star, c, m_s_star]\n\n# Execute the main function\nsolve()\n```", "id": "3176459"}, {"introduction": "在真实的深度学习训练中，随机性是不可避免的，它源于数据抽样、参数初始化和梯度噪声。这个练习将带你探究这种随机性如何与学习率策略相互作用，特别是比较阶梯衰减和指數衰减在面对随机梯度噪声时的“可复现性”。通过模拟多次训练并分析最终结果的方差，你将检验一个重要假设：离散的阶梯式下降是否比平滑的指数衰减更容易受到随机性的影响，从而导致训练结果的不稳定。[@problem_id:3176492]", "problem": "考虑深度学习中迭代参数更新的训练动态，其中单个标量参数 $w$ 被优化以最小化一个以目标 $w^\\star$ 为中心的凸目标函数。作为一个基本的基础，我们假设采用标准的梯度下降更新，学习率随时间变化，并且每次迭代时都有附加的、零均值、独立的梯度噪声。具体来说，迭代遵循以下规则：对于每个离散时间步 $t \\in \\{0,1,\\dots,T-1\\}$，应用形式为 $w_{t+1} = w_t - \\eta_t \\cdot g_t$ 的更新，其中 $g_t$ 是梯度加上噪声扰动，$\\eta_t$ 是时间 $t$ 的学习率。目标函数是以 $w^\\star$ 为中心的可微二次函数，噪声在不同步骤和不同种子之间是独立的。\n\n你的任务是实现两种在深度学习中被广泛研究的学习率策略：\n- 一种在固定的迭代阈值处以离散阶跃方式降低学习率的策略。\n- 一种在每次迭代时平滑地降低学习率的策略。\n\n你将比较这两种策略在不同随机种子间的可复现性。可复现性定义为最终训练结果对随机种子选择的敏感程度。对于本问题，敏感性将量化为最终目标函数值在不同随机种子间的经验方差。待检验的假设是，由于存在噪声时的阈值效应，具有离散阈值降低的策略会引起更大的跨种子敏感性。\n\n使用以下建模假设：\n- 参数是标量，其初始化 $w_0$ 从一个依赖于种子的分布中抽取。\n- 目标值固定为 $w^\\star = 1$。\n- 目标函数是二次函数 $f(w) = \\tfrac{1}{2} (w - w^\\star)^2$，因此在迭代 $t$ 时的无噪声梯度是 $w_t - w^\\star$。\n- 迭代 $t$ 时的随机梯度是 $g_t = (w_t - w^\\star) + \\epsilon_t$，其中 $\\epsilon_t$ 在不同 $t$ 和不同种子间独立，且 $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$。\n- 对于 $t \\in \\{0,1,\\dots,T-1\\}$，更新规则为 $w_{t+1} = w_t - \\eta_t \\cdot g_t$。\n\n对于下面测试套件中的每个测试用例，你必须：\n1. 使用相同的随机种子和噪声参数，分别在离散阈值策略和平滑策略下模拟训练过程。\n2. 对每种策略，计算最终目标函数值 $f(w_T)$ 在不同种子间的经验方差。\n3. 对于该测试用例，给出一个布尔结果，指明离散阈值策略是否表现出比平滑策略严格更大的方差。\n\n测试套件和参数定义：\n- 对于给定的 $S$，设随机种子集合为 $\\{0,1,\\dots,S-1\\}$。\n- 对于每个测试用例，给定 $(T,S,\\eta_0,\\gamma,s,\\sigma)$，其中：\n  - $T$ 是总迭代次数。\n  - $S$ 是种子数量。\n  - $\\eta_0$ 是初始学习率。\n  - $\\gamma$ 是一个介于 $0$ 和 $1$ 之间的衰减因子，控制学习率缩小的速度。\n  - $s$ 是阈值降低的步长间隔。\n  - $\\sigma$ 是梯度噪声的标准差。\n- 你必须构建离散阈值策略，使其在由 $s$ 和 $\\gamma$ 决定的迭代阈值处进行衰减；并构建平滑策略，使其在每次迭代时使用 $s$ 和 $\\gamma$ 连续地降低学习率，以使得每 $s$ 次迭代的总体衰减具有可比性。\n\n使用以下测试用例：\n- 用例 1：$(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,40,0.1)$。\n- 用例 2：$(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,40,0)$。\n- 用例 3：$(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.05,0.5,40,0.1)$。\n- 用例 4：$(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.2,40,0.2)$。\n- 用例 5：$(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,10,0.2)$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[result_1,result_2,\\dots]$），其中每个 $result_i$ 是对应于用例 $i$ 的布尔值，如果离散阈值策略产生的 $f(w_T)$ 的经验方差严格大于平滑策略，则其值为 $True$，否则为 $False$。不涉及物理单位，也无需报告角度或百分比。对于给定的测试套件，输出必须是确定性的。", "solution": "该问题要求在随机梯度下降的背景下，对两种学习率策略——离散阶梯式衰减和平滑指数衰减——的稳定性进行对比分析。比较的度量标准是最终目标函数值在多次模拟中的经验方差，每次模拟都以不同的随机种子启动。较高的方差表明对随机性（初始参数和梯度噪声）的具体实现更为敏感，意味着可复现性较低。\n\n首先，我们形式化这个动力系统。待优化的参数是一个标量 $w$，对于每个时间步 $t \\in \\{0, 1, \\dots, T-1\\}$ 的更新规则为：\n$$\nw_{t+1} = w_t - \\eta_t \\cdot g_t\n$$\n其中 $\\eta_t$ 是步骤 $t$ 的学习率，$g_t$ 是随机梯度。目标函数是一个简单的二次碗型函数，$f(w) = \\frac{1}{2}(w - w^\\star)^2$，目标参数固定为 $w^\\star = 1$。真实梯度为 $\\nabla f(w_t) = w_t - w^\\star$。随机梯度 $g_t$ 包含一个加性噪声项 $\\epsilon_t$：\n$$\ng_t = (w_t - w^\\star) + \\epsilon_t\n$$\n噪声项 $\\epsilon_t$ 被建模为一个独立同分布（i.i.d.）的随机变量，从零均值的正态分布中抽取，即 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n为了分析系统的动态，我们考虑误差项 $\\delta_t = w_t - w^\\star$。将其代入更新规则，得到一个关于误差的线性递推关系：\n$$\n\\delta_{t+1} = \\delta_t - \\eta_t (\\delta_t + \\epsilon_t) = (1 - \\eta_t)\\delta_t - \\eta_t \\epsilon_t\n$$\n该方程描述了参数偏离最优值的演化过程，由学习率策略 $\\eta_t$ 和噪声序列 $\\{\\epsilon_t\\}_{t=0}^{T-1}$ 驱动。\n\n模拟的随机性源于两个方面，两者都依赖于一个选定的种子 $k \\in \\{0, 1, \\dots, S-1\\}$：\n1.  初始参数 $w_0^{(k)}$，由此确定初始误差 $\\delta_0^{(k)} = w_0^{(k)} - w^\\star$。我们将从标准正态分布中抽取 $w_0^{(k)}$，即 $w_0 \\sim \\mathcal{N}(0, 1)$。\n2.  梯度噪声项序列 $\\{\\epsilon_t^{(k)}\\}_{t=0}^{T-1}$。\n\n对于每个种子，必须使用相同的初始条件 $w_0^{(k)}$ 和噪声序列 $\\{\\epsilon_t^{(k)}\\}$ 来模拟两种学习率策略的轨迹。这确保了观察到的任何结果差异都仅归因于策略本身。\n\n接下来，我们根据初始学习率 $\\eta_0$、衰减因子 $\\gamma \\in (0,1)$ 和步长间隔 $s$ 来定义两种学习率策略。\n\n1.  **离散阈值策略 ($\\eta_t^{\\text{disc}}$):** 学习率在 $s$ 个步骤内保持不变，然后乘以 $\\gamma$。这表示为：\n    $$\n    \\eta_t^{\\text{disc}} = \\eta_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}\n    $$\n    指数 $\\lfloor t/s \\rfloor$ 是一个阶跃函数，每 $s$ 次迭代增加一次，导致学习率在 $s$ 的倍数处突然下降。\n\n2.  **平滑策略 ($\\eta_t^{\\text{smth}}$):** 为了提供可比的衰减，该策略被构造成一个连续的指数衰减。每一步的衰减率 $\\gamma_{\\text{smth}}$ 的选择使得在 $s$ 个步骤内的总衰减等于 $\\gamma$。这意味着 $(\\gamma_{\\text{smth}})^s = \\gamma$，所以 $\\gamma_{\\text{smth}} = \\gamma^{1/s}$。步骤 $t$ 的学习率为：\n    $$\n    \\eta_t^{\\text{smth}} = \\eta_0 \\cdot (\\gamma^{1/s})^t = \\eta_0 \\cdot \\gamma^{t/s}\n    $$\n    该策略在每一次迭代中都将学习率减小一个很小的因子。\n\n问题的核心是检验假设 $\\text{Var}_{\\text{seeds}}(f(w_T^{\\text{disc}}))  \\text{Var}_{\\text{seeds}}(f(w_T^{\\text{smth}}))$，其中 $f(w_T)=\\frac{1}{2}\\delta_T^2$ 是最终的目标函数值。\n\n针对每个测试用例 $(T, S, \\eta_0, \\gamma, s, \\sigma)$ 的模拟算法如下：\n1.  初始化两个大小为 $S$ 的数组 $V_{\\text{disc}}$ 和 $V_{\\text{smth}}$，用于存储最终的目标函数值。\n2.  对于从 $0$ 到 $S-1$ 的每个种子 $k$：\n    a.  用种子 $k$ 实例化一个随机数生成器。\n    b.  生成一个公共的初始参数 $w_0$ 和一个长度为 $T$ 的公共噪声向量 $(\\epsilon_0, \\dots, \\epsilon_{T-1})$。\n    c.  模拟离散策略的轨迹：从 $w = w_0$ 开始，对 $t=0, \\dots, T-1$ 迭代更新规则 $w_{t+1} = w_t - \\eta_t^{\\text{disc}} g_t$。计算最终目标值 $f(w_T)$ 并将其存储在 $V_{\\text{disc}}[k]$ 中。\n    d.  模拟平滑策略的轨迹：从相同的 $w_0$ 开始，对 $t=0, \\dots, T-1$ 迭代更新规则 $w_{t+1} = w_t - \\eta_t^{\\text{smth}} g_t$。计算最终目标值 $f(w_T)$ 并将其存储在 $V_{\\text{smth}}[k]$ 中。\n3.  计算 $V_{\\text{disc}}$ 和 $V_{\\text{smth}}$ 中元素的经验方差。\n4.  如果 $V_{\\text{disc}}$ 的方差严格大于 $V_{\\text{smth}}$ 的方差，则结果为 `True`，否则为 `False`。对所有提供的测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(T, S, eta0, gamma, s, sigma):\n    \"\"\"\n    Simulates training for discrete and smooth LR schedules and compares their final variance.\n\n    Args:\n        T (int): Total number of iterations.\n        S (int): Number of random seeds to simulate.\n        eta0 (float): Initial learning rate.\n        gamma (float): Decay factor.\n        s (int): Step interval for discrete decay.\n        sigma (float): Standard deviation of gradient noise.\n\n    Returns:\n        bool: True if the variance of the discrete schedule's final objective\n              is strictly greater than the smooth schedule's, False otherwise.\n    \"\"\"\n    w_star = 1.0\n\n    final_objs_disc = np.zeros(S)\n    final_objs_smth = np.zeros(S)\n\n    for seed in range(S):\n        # Use a seed-specific random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate the same initial weight and noise sequence for both schedules.\n        w0 = rng.normal(loc=0.0, scale=1.0)\n        noise = rng.normal(loc=0.0, scale=sigma, size=T)\n\n        # --- Discrete-Threshold Schedule Simulation ---\n        w = w0\n        for t in range(T):\n            lr_disc = eta0 * (gamma ** (t // s))\n            gradient = (w - w_star) + noise[t]\n            w = w - lr_disc * gradient\n        final_objs_disc[seed] = 0.5 * (w - w_star)**2\n\n        # --- Smooth Schedule Simulation ---\n        w = w0  # Reset to the same initial weight\n        for t in range(T):\n            lr_smth = eta0 * (gamma ** (t / s))\n            gradient = (w - w_star) + noise[t]\n            w = w - lr_smth * gradient\n        final_objs_smth[seed] = 0.5 * (w - w_star)**2\n\n    # Compute the empirical variance across all seeds for each schedule.\n    # np.var computes the population variance by default (ddof=0).\n    var_disc = np.var(final_objs_disc)\n    var_smth = np.var(final_objs_smth)\n\n    return var_disc > var_smth\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (T, S, eta0, gamma, s, sigma)\n    test_cases = [\n        (240, 200, 0.4, 0.5, 40, 0.1),\n        (240, 200, 0.4, 0.5, 40, 0.0),\n        (240, 200, 0.05, 0.5, 40, 0.1),\n        (240, 200, 0.4, 0.2, 40, 0.2),\n        (240, 200, 0.4, 0.5, 10, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(*case)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3176492"}, {"introduction": "现代优化器（如 Adam）不仅仅依赖于当前梯度，还维护着关于过去梯度的“记忆”（即动量估计）。这个练习将深入探讨一个高级话题：学习率策略与 Adam 优化器内部状态的相互作用。你将模拟当学习率或梯度发生剧烈变化时，阶梯衰减策略如何可能导致优化器状态与当前的优化环境“失配”，并量化由此产生的“更新冲击”和“方向失准”问题，同时探索可能的解决方案。[@problem_id:3176478]", "problem": "在一个受控的、纯算法的环境中，要求您研究学习率调度与自适应矩估计（Adam）内部状态在突变期间的相互作用。您的任务是编写一个完整且可运行的程序，模拟在一维优化中使用Adam，并应用不同的学习率调度和梯度机制，量化在调度变化后立即出现的状态不匹配，并比较突变的阶梯衰减与平滑的指数衰减。\n\n请基于以下基本且广为接受的定义进行研究。对于整数时间 $t \\in \\{1,2,\\ldots,T\\}$，标量梯度为 $g_t \\in \\mathbb{R}$，Adam维护移动平均值 $m_t$ 和 $v_t$，超参数为 $\\beta_1 \\in (0,1)$，$\\beta_2 \\in (0,1)$，以及 $\\epsilon  0$：\n$$\nm_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t,\\quad\nv_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2,\n$$\n偏差校正后的估计值为\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^{p_t}},\\quad\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^{q_t}},\n$$\n其中 $p_t$ 和 $q_t$ 记录了自初始化或自偏差校正计数器重置以来的更新次数。Adam在时间 $t$ 的参数更新为\n$$\n\\Delta \\theta_t = - \\eta_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon},\n$$\n其中 $\\eta_t  0$ 是学习率。考虑两种学习率调度：\n1. 阶梯衰减，阶跃时间为 $t_s$：\n$$\n\\eta_t = \\begin{cases}\n\\eta_0,  t  t_s,\\\\\n\\gamma \\eta_0,  t \\ge t_s,\n\\end{cases}\n$$\n其中 $\\eta_0  0$ 且 $\\gamma \\in (0,1)$。\n2. 指数衰减，其选择是为了匹配阶梯衰减在 $t=t_s$ 时的值：\n$$\n\\eta_t = \\eta_0 \\exp(-k (t-1)),\\quad \\text{其中 } k = \\frac{\\ln(1/\\gamma)}{t_s-1}.\n$$\n为了模拟分布偏移，施加一个分段常数梯度机制：\n$$\ng_t = \\begin{cases}\ng_\\text{pre},  t  t_s,\\\\\ng_\\text{post},  t \\ge t_s.\n\\end{cases}\n$$\n\n在调度变化时间 $t=t_s$ 定义两个量化诊断指标：\n- 更新冲击幅度\n$$\nJ = \\left| \\Delta \\theta_{t_s} - \\Delta \\theta_{t_s-1} \\right|,\n$$\n它衡量更新变化的突兀程度；较大的 $J$ 表示更强的冲击。\n- 方向错配指示器\n$$\nB = \\begin{cases}\n\\text{True},  \\Delta \\theta_{t_s} \\cdot g_{t_s} > 0,\\\\\n\\text{False},  \\Delta \\theta_{t_s} \\cdot g_{t_s} \\le 0,\n\\end{cases}\n$$\n当更新方向与当前梯度的下降方向不一致时（即，在一维中更新局部增加了损失），该值为True，否则为False。在一维中，与下降方向对齐的更新应满足 $\\Delta \\theta_{t_s} \\cdot g_{t_s}  0$。\n\n为了减轻学习率阶跃变化后立即出现的不匹配，实施一种偏差校正调整，在 $t=t_s$ 时重置内部状态：\n- 动量重置：在计算 $m_{t_s}$ 和 $v_{t_s}$ 之前，设置 $m_{t_s-1} \\leftarrow 0$, $v_{t_s-1} \\leftarrow 0$。\n- 偏差校正计数器重置：重置偏差校正中使用的幂，使得 $p_{t_s}=1$ 和 $q_{t_s}=1$。\n\n此重置强制 $t_s$ 时的偏差校正动量能够无滞后地反映当前梯度。\n\n实现一个程序，对以下每个测试用例，模拟 $T$ 个步骤，计算如上定义的对 $(J,B)$，并将所有结果作为平坦列表单行打印。使用以下超参数和常量\n- $T = 60$，\n- $t_s = 30$，\n- $\\eta_0 = 10^{-3}$，\n- $\\gamma = 10^{-1}$，\n- $\\beta_1 = 9 \\times 10^{-1}$，\n- $\\beta_2 = 9.99 \\times 10^{-1}$，\n- $\\epsilon = 10^{-8}$。\n\n测试套件（每个用例返回在 $t=t_s$ 时的 $(J,B)$）：\n1. 阶梯衰减，无重置，在 $t_s$ 处梯度符号翻转：$g_\\text{pre} = 10^{-1}$，$g_\\text{post} = -1$。\n2. 阶梯衰减，在 $t_s$ 处重置，在 $t_s$ 处梯度符号翻转：$g_\\text{pre} = 10^{-1}$，$g_\\text{post} = -1$。\n3. 阶梯衰减，无重置，无梯度符号翻转：$g_\\text{pre} = 10^{-1}$，$g_\\text{post} = 10^{-1}$。\n4. 指数衰减，无重置，在 $t_s$ 处梯度符号翻转：$g_\\text{pre} = 10^{-1}$，$g_\\text{post} = -1$。\n\n您的程序应生成单行输出，包含一个逗号分隔的列表，用方括号括起来，按以下顺序排列\n$$\n[J_1, B_1, J_2, B_2, J_3, B_3, J_4, B_4].\n$$\n不涉及物理单位。所有角度（如果有）均应视为无量纲实数。最终输出必须是使用语言默认字符串格式计算出的精确值，其中布尔值是允许的，并显示为其语言原生字面量。", "solution": "问题陈述已经过仔细验证，并被确定为有效。它在科学上基于数值优化，特别是Adam算法的既定原则，并且问题设定良好，所有必要的常数、初始条件和函数形式都已指定。这些定义在数学上是精确和客观的，允许一个唯一且可验证的解。\n\n任务是在不同的学习率调度和梯度剖面配置下，模拟一维Adam优化器在时间范围 $T$ 内的行为。分析的核心集中在优化器在特定时间点 $t_s$ 的行为，此时学习率和梯度都可能发生突变。我们将使用两个定义的度量标准 $J$ 和 $B$ 来量化由此对优化过程产生的冲击。\n\nAdam算法维护过去梯度的指数衰减移动平均（一阶矩，$m_t$）和过去梯度平方的指数衰减移动平均（二阶矩，$v_t$）。在每个时间步 $t$，对于给定的梯度 $g_t$，这些矩按以下方式更新，初始值为 $m_0 = 0$ 和 $v_0 = 0$：\n$$\nm_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n$$\n$$\nv_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n$$\n这里，$\\beta_1$ 和 $\\beta_2$ 是区间 $(0, 1)$ 内的超参数，控制移动平均的衰减率。由于矩初始化为零，它们会偏向零，尤其是在初始步骤中。通过计算以下值来校正这种偏差：\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^{p_t}}\n$$\n$$\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^{q_t}}\n$$\n指数 $p_t$ 和 $q_t$ 表示对矩有贡献的更新次数，通常对应于当前时间步 $t$。然后，使用当前学习率 $\\eta_t$、经偏差校正的矩和一个小的稳定常数 $\\epsilon  0$ 来计算最终的参数更新 $\\Delta \\theta_t$：\n$$\n\\Delta \\theta_t = - \\eta_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n$$\n模拟将进行 $T=60$ 步，关键事件发生在第 $t_s=30$ 步。梯度遵循分段常数机制：\n$$\ng_t = \\begin{cases}\ng_\\text{pre},  t  t_s \\\\\ng_\\text{post},  t \\ge t_s\n\\end{cases}\n$$\n我们研究两种学习率调度 $\\eta_t$：\n1.  **阶梯衰减**：学习率在 $\\eta_0$ 处保持不变，在时间 $t_s$ 时下降到其初始值的 $\\gamma$ 倍。\n    $$\n    \\eta_t = \\begin{cases}\n    \\eta_0,  t  t_s \\\\\n    \\gamma \\eta_0,  t \\ge t_s\n    \\end{cases}\n    $$\n2.  **指数衰减**：学习率随时间平滑衰减。衰减率 $k$ 的选择使得第 $t_s$ 步的学习率与阶梯衰减调度的学习率相匹配。\n    $$\n    \\eta_t = \\eta_0 \\exp(-k (t-1)), \\quad \\text{其中 } k = \\frac{\\ln(1/\\gamma)}{t_s-1}\n    $$\n在关键步骤 $t_s$，我们测量两个诊断指标：\n-   **更新冲击幅度 ($J$)**：这衡量了从 $t_s-1$ 到 $t_s$ 更新步长的变化幅度。大的 $J$ 表示优化轨迹受到了显著的颠簸。\n    $$\n    J = \\left| \\Delta \\theta_{t_s} - \\Delta \\theta_{t_s-1} \\right|\n    $$\n-   **方向错配指示器 ($B$)**：如果 $t_s$ 处的更新指向损失*上升*而非下降的方向，则此布尔指示器变为 $\\text{True}$。标准的梯度下降步骤沿 $- \\alpha g_t$ 方向移动，因此更新与梯度的乘积应为负。$B$ 标记了违反此条件的情况。\n    $$\n    B = (\\Delta \\theta_{t_s} \\cdot g_{t_s}  0)\n    $$\n一个测试用例探讨了状态不匹配的缓解策略。在 $t=t_s$ 时，优化器的状态（$m_{t_s-1}$，$v_{t_s-1}$）反映了梯度 $g_\\text{pre}$ 的历史，这可能与新的梯度 $g_\\text{post}$ 不一致。**重置机制**通过在计算新矩 $m_{t_s}$ 和 $v_{t_s}$ 之前将 $m_{t_s-1}$ 和 $v_{t_s-1}$ 设置为 $0$ 来解决此问题。同时，偏差校正计数器被重置，因此 $p_{t_s}=1$ 和 $q_{t_s}=1$。这有效地重新初始化了优化器，使其能快速适应新的梯度机制。\n\n将使用提供的超参数：$\\eta_0 = 10^{-3}$，$\\gamma = 10^{-1}$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$ 和 $\\epsilon = 10^{-8}$，对四个指定的测试用例中的每一个进行模拟。对于每个用例，将在 $t = t_s = 30$ 时计算并报告对 $(J, B)$。\n\n实现将包括一个从 $t=1$ 到 $T=60$ 迭代的主循环。在循环内部，根据当前步骤 $t$ 和正在模拟的用例，确定适当的梯度 $g_t$ 和学习率 $\\eta_t$。如果指定，则在 $t=t_s$ 应用重置机制的逻辑，更改该步骤的动量输入和偏差校正幂。然后，顺序应用Adam更新方程来计算 $\\Delta\\theta_t$，并将其存储。最后，在模拟之后，使用存储在 $t=29$ 和 $t=30$ 的更新来计算诊断指标 $J$ 和 $B$。", "answer": "```python\nimport numpy as np\n\ndef simulate_adam(\n    lr_schedule_type,\n    reset,\n    g_pre,\n    g_post,\n    T,\n    t_s,\n    eta_0,\n    gamma,\n    beta1,\n    beta2,\n    epsilon,\n):\n    \"\"\"\n    Simulates one-dimensional Adam optimization for a given scenario.\n\n    Args:\n        lr_schedule_type (str): 'step' or 'exp'.\n        reset (bool): Whether to apply the reset mechanism at t_s.\n        g_pre (float): Gradient value for t  t_s.\n        g_post (float): Gradient value for t >= t_s.\n        T (int): Total number of simulation steps.\n        t_s (int): The step at which changes occur.\n        eta_0 (float): Initial learning rate.\n        gamma (float): Decay factor for step decay.\n        beta1 (float): Exponential decay rate for the 1st moment estimates.\n        beta2 (float): Exponential decay rate for the 2nd moment estimates.\n        epsilon (float): Term added to the denominator for numerical stability.\n\n    Returns:\n        tuple: A pair (J, B) containing the update shock amplitude and the\n               directional misalignment indicator.\n    \"\"\"\n    m = 0.0\n    v = 0.0\n    delta_thetas = []\n\n    # Calculate k for exponential decay if needed\n    k = 0.0\n    if lr_schedule_type == \"exp\":\n        k = np.log(1 / gamma) / (t_s - 1)\n\n    for t in range(1, T + 1):\n        # 1. Determine current gradient\n        g_t = g_pre if t  t_s else g_post\n\n        # 2. Determine current learning rate\n        if lr_schedule_type == \"step\":\n            eta_t = eta_0 if t  t_s else gamma * eta_0\n        else:  # 'exp'\n            eta_t = eta_0 * np.exp(-k * (t - 1))\n\n        # 3. Handle reset mechanism and bias-correction powers\n        m_prev_eff = m\n        v_prev_eff = v\n\n        if reset and t >= t_s:\n            p_t = t - t_s + 1\n            q_t = t - t_s + 1\n            if t == t_s:\n                m_prev_eff = 0.0\n                v_prev_eff = 0.0\n        else:\n            p_t = t\n            q_t = t\n\n        # 4. Adam update equations\n        # Update biased first moment estimate\n        m = beta1 * m_prev_eff + (1 - beta1) * g_t\n        # Update biased second raw moment estimate\n        v = beta2 * v_prev_eff + (1 - beta2) * (g_t**2)\n\n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1**p_t)\n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2**q_t)\n\n        # Compute parameter update\n        delta_theta_t = -eta_t * m_hat / (np.sqrt(v_hat) + epsilon)\n        delta_thetas.append(delta_theta_t)\n\n    # 5. Compute diagnostics at t=t_s\n    delta_theta_ts_minus_1 = delta_thetas[t_s - 2]  # t_s-1 corresponds to index t_s-2\n    delta_theta_ts = delta_thetas[t_s - 1]          # t_s corresponds to index t_s-1\n    g_ts = g_post\n\n    # Update shock amplitude\n    J = np.abs(delta_theta_ts - delta_theta_ts_minus_1)\n\n    # Directional misalignment indicator\n    B = (delta_theta_ts * g_ts) > 0\n\n    return J, B\n\n\ndef solve():\n    \"\"\"\n    Runs the simulation for all test cases and prints the results.\n    \"\"\"\n    # Hyperparameters and constants\n    T = 60\n    t_s = 30\n    eta_0 = 1e-3\n    gamma = 1e-1\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n\n    # Test suite\n    test_cases = [\n        # 1. Step decay, no reset, gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": -1.0},\n        # 2. Step decay, with reset, gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": True, \"g_pre\": 1e-1, \"g_post\": -1.0},\n        # 3. Step decay, no reset, no gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": 1e-1},\n        # 4. Exponential decay, no reset, gradient sign flip\n        {\"lr_schedule\": \"exp\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": -1.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        J, B = simulate_adam(\n            lr_schedule_type=case[\"lr_schedule\"],\n            reset=case[\"reset\"],\n            g_pre=case[\"g_pre\"],\n            g_post=case[\"g_post\"],\n            T=T,\n            t_s=t_s,\n            eta_0=eta_0,\n            gamma=gamma,\n            beta1=beta1,\n            beta2=beta2,\n            epsilon=epsilon,\n        )\n        results.extend([J, B])\n\n    # Format the final output string\n    # Booleans are correctly converted to 'True'/'False' by str()\n    # Floats use default Python formatting\n    formatted_results = [str(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3176478"}]}