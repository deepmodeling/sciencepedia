{"hands_on_practices": [{"introduction": "要真正掌握 Adam 优化器，第一步是亲手计算一次完整的更新过程。这个练习将引导你分解 Adam 的核心机制，从计算梯度开始，到更新一阶和二阶矩估计，再到进行偏差修正，最终完成参数的单步更新。通过这个基础计算 [@problem_id:2152250]，你将对 Adam 的内部工作流程建立一个清晰而具体的第一印象。", "problem": "在数值优化领域，Adam算法是一种广泛用于寻找函数最小值的优化方法。考虑一维成本函数 $f(x) = 5x^2$。我们希望从初始猜测值 $x_0 = 2$ 开始，找到使该函数最小化的 $x$ 值。\n\n使用Adam算法计算一次更新步骤后的参数值，记为 $x_1$。该算法配置了以下超参数：\n- 学习率, $\\alpha = 0.1$\n- 一阶矩估计的指数衰减率, $\\beta_1 = 0.9$\n- 二阶矩估计的指数衰减率, $\\beta_2 = 0.999$\n- 用于数值稳定性的一个很小的常数, $\\epsilon = 10^{-8}$\n\n初始的一阶矩和二阶矩估计值 $m_0$ 和 $v_0$ 均初始化为零。\n\n将您的最终答案四舍五入到五位有效数字。", "solution": "我们使用Adam算法从 $x_{0}=2$ 开始最小化 $f(x)=5x^{2}$。梯度为 $\\nabla f(x)=10x$。在第一步（$t=1$）中，在 $x_{0}$ 处计算的梯度为\n$$\ng_{1}=\\nabla f(x_{0})=10x_{0}=10\\cdot 2=20.\n$$\nAdam的矩更新为\n$$\nm_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2}.\n$$\n给定 $m_{0}=0$ 和 $v_{0}=0$，$\\beta_{1}=0.9$ 及 $\\beta_{2}=0.999$，\n$$\nm_{1}=0.9\\cdot 0+(1-0.9)\\cdot 20=2, \\quad v_{1}=0.999\\cdot 0+(1-0.999)\\cdot 20^{2}=0.001\\cdot 400=0.4.\n$$\n偏差校正后的估计值为\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{2}{1-0.9}=\\frac{2}{0.1}=20, \\quad \\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{0.4}{1-0.999}=\\frac{0.4}{0.001}=400.\n$$\nAdam更新步骤为\n$$\nx_{1}=x_{0}-\\alpha\\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}=2-0.1\\cdot \\frac{20}{\\sqrt{400}+10^{-8}}=2-0.1\\cdot \\frac{20}{20+10^{-8}}=2-\\frac{2}{20+10^{-8}}.\n$$\n计算该分数，\n$$\n\\frac{2}{20+10^{-8}} \\approx 0.1-5\\times 10^{-11},\n$$\n因此\n$$\nx_{1}\\approx 2-\\left(0.1-5\\times 10^{-11}\\right)=1.90000000005.\n$$\n四舍五入到五位有效数字，结果为 $1.9000$。", "answer": "$$\\boxed{1.9000}$$", "id": "2152250"}, {"introduction": "在掌握了单步更新之后，我们来看看 Adam 优化器如何连续工作。这个练习 [@problem_id:2152273] 要求你执行两个连续的更新步骤，让你能够观察到动量和自适应学习率如何随时间演变。通过处理一个梯度方向交替变化的场景，你将更深刻地理解一阶矩估计 $m_t$ 是如何平滑梯度，从而在优化路径中保持稳定的前进方向。", "problem": "考虑使用 Adam 优化算法对机器学习模型的单个标量参数 $\\theta$ 进行优化。在时间步 $t$，Adam 的更新规则如下：\n\n1.  更新有偏一阶矩估计：$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n2.  更新有偏二阶原始矩估计：$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n3.  计算偏差校正后的一阶矩估计：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n4.  计算偏差校正后的二阶原始矩估计：$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n5.  更新参数：$\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n\n这里，$g_t$ 是在时间步 $t$ 时损失函数关于 $\\theta$ 的梯度。一阶和二阶矩估计被初始化为零，即 $m_0 = 0$ 和 $v_0 = 0$。\n\n假设初始参数值为 $\\theta_0 = 0.5$。优化过程从 $t=1$ 开始。优化器的超参数设置如下：\n- 学习率：$\\alpha = 0.1$\n- 一阶矩衰减率：$\\beta_1 = 0.9$\n- 二阶矩衰减率：$\\beta_2 = 0.999$\n- 用于数值稳定性的 Epsilon：$\\epsilon = 10^{-8}$\n\n在优化的前两个步骤中计算出的梯度为 $g_1 = 10$ 和 $g_2 = -10$。\n\n计算第二次更新步骤后参数 $\\theta$ 的值，即 $\\theta_2$。将最终答案四舍五入到四位有效数字。", "solution": "我们的目标是计算两次更新步骤后参数 $\\theta$ 的值，即 $\\theta_2$。我们已知初始条件和超参数：\n- 初始参数：$\\theta_0 = 0.5$\n- 初始一阶矩：$m_0 = 0$\n- 初始二阶矩：$v_0 = 0$\n- 学习率：$\\alpha = 0.1$\n- 衰减率：$\\beta_1 = 0.9$, $\\beta_2 = 0.999$\n- Epsilon：$\\epsilon = 10^{-8}$\n- 梯度：$g_1 = 10$, $g_2 = -10$\n\n我们将对每个时间步 $t=1$ 和 $t=2$ 进行计算。\n\n**第 1 步：时间步 $t=1$**\n\n首先，我们计算有偏的一阶和二阶矩估计 $m_1$ 和 $v_1$。\n$m_1 = \\beta_1 m_0 + (1-\\beta_1) g_1 = (0.9)(0) + (1-0.9)(10) = (0.1)(10) = 1$\n$v_1 = \\beta_2 v_0 + (1-\\beta_2) g_1^2 = (0.999)(0) + (1-0.999)(10^2) = (0.001)(100) = 0.1$\n\n接下来，我们计算偏差校正后的估计值 $\\hat{m}_1$ 和 $\\hat{v}_1$。\n$\\hat{m}_1 = \\frac{m_1}{1 - \\beta_1^1} = \\frac{1}{1 - 0.9} = \\frac{1}{0.1} = 10$\n$\\hat{v}_1 = \\frac{v_1}{1 - \\beta_2^1} = \\frac{0.1}{1 - 0.999} = \\frac{0.1}{0.001} = 100$\n\n最后，我们更新参数 $\\theta$ 以得到 $\\theta_1$。\n$\\theta_1 = \\theta_0 - \\alpha \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\epsilon} = 0.5 - (0.1) \\frac{10}{\\sqrt{100} + 10^{-8}} = 0.5 - \\frac{1}{10 + 10^{-8}}$\n$\\theta_1 \\approx 0.5 - 0.099999999 = 0.400000001$\n\n我们为下一步保留此值的高精度。\n\n**第 2 步：时间步 $t=2$**\n\n现在我们使用 $t=1$ 的结果 ($m_1=1, v_1=0.1, \\theta_1 \\approx 0.400000001$) 来执行第二次更新。\n首先，计算有偏估计 $m_2$ 和 $v_2$。\n$m_2 = \\beta_1 m_1 + (1-\\beta_1) g_2 = (0.9)(1) + (1-0.9)(-10) = 0.9 - 1 = -0.1$\n$v_2 = \\beta_2 v_1 + (1-\\beta_2) g_2^2 = (0.999)(0.1) + (1-0.999)((-10)^2) = 0.0999 + (0.001)(100) = 0.0999 + 0.1 = 0.1999$\n\n接下来，计算偏差校正后的估计值 $\\hat{m}_2$ 和 $\\hat{v}_2$。偏差校正的指数为 $t=2$。\n$\\hat{m}_2 = \\frac{m_2}{1 - \\beta_1^2} = \\frac{-0.1}{1 - 0.9^2} = \\frac{-0.1}{1 - 0.81} = \\frac{-0.1}{0.19}$\n$\\hat{v}_2 = \\frac{v_2}{1 - \\beta_2^2} = \\frac{0.1999}{1 - 0.999^2} = \\frac{0.1999}{1 - 0.998001} = \\frac{0.1999}{0.001999}$\n\n最后，我们更新参数从 $\\theta_1$ 得到 $\\theta_2$。\n$\\theta_2 = \\theta_1 - \\alpha \\frac{\\hat{m}_2}{\\sqrt{\\hat{v}_2} + \\epsilon}$\n$\\theta_2 = 0.400000001 - (0.1) \\frac{-0.1 / 0.19}{\\sqrt{0.1999 / 0.001999} + 10^{-8}}$\n$\\theta_2 = 0.400000001 + \\frac{0.01 / 0.19}{\\sqrt{100.00000...} + 10^{-8}}$\n平方根下的项是 $\\frac{0.1999}{0.001999} \\approx 100.0$。\n$\\theta_2 \\approx 0.400000001 + \\frac{0.01 / 0.19}{10 + 10^{-8}}$\n$\\theta_2 \\approx 0.400000001 + \\frac{0.0526315789...}{10.00000001}$\n$\\theta_2 \\approx 0.400000001 + 0.0052631578...$\n$\\theta_2 \\approx 0.405263158$\n\n题目要求将答案四舍五入到四位有效数字。\n$\\theta_2 \\approx 0.4053$", "answer": "$$\\boxed{0.4053}$$", "id": "2152273"}, {"introduction": "计算是基础，但理解算法在特殊情况下的行为更能揭示其设计的精妙之处。本练习 [@problem_id:2152247] 提出了一个思想实验：当梯度持续高频振荡时，Adam 的一阶矩估计 $m_t$ 会发生什么？通过分析这种极端情况，你将洞察到动量项（即一阶矩）的内在稳定性，并理解它为何能帮助优化器在复杂的损失曲面上“看穿”局部噪声，而不是随之摇摆。", "problem": "在机器学习领域，Adam 优化器是训练深度神经网络的一种流行算法。Adam 的一个关键组成部分是一阶矩估计（通常称为动量），它是梯度的指数移动平均值。对于单个模型参数，在时间步 $t$ 的动量 $m_t$ 使用以下规则进行更新：\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$$\n此处，$g_t$ 是在时间步 $t$ 损失函数关于该参数的梯度，而 $\\beta_1$ 是一阶矩估计的指数衰减率，是一个满足 $0  \\beta_1  1$ 的常数。动量通常初始化为零，即 $m_0 = 0$。\n\n考虑一个为测试动量更新行为而设计的特殊病态场景。假设梯度序列不收敛，而是以恒定幅度振荡。对于所有时间步 $t \\ge 1$，梯度由序列 $g_t = g_0 (-1)^t$ 给出，其中 $g_0$ 是一个正常数。\n\n确定当 $t$ 通过偶数趋于无穷大时，一阶矩估计 $m_t$ 的极限值。换句话说，计算 $\\lim_{n \\to \\infty} m_{2n}$ 的值。将您的答案表示为关于 $g_0$ 和 $\\beta_1$ 的符号表达式。", "solution": "我们要求解在梯度序列 $g_t = g_0 (-1)^t$ 下，一阶矩估计 $m_t$ 在偶数时间步的极限。\n一阶矩的更新规则为：\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$$\n初始条件为 $m_0 = 0$。\n\n我们来推导偶数项 $m_{2n}$ 的递推关系。\n对于偶数时间步 $2n$ ($n \\ge 1$)：\n$$m_{2n} = \\beta_1 m_{2n-1} + (1 - \\beta_1)g_{2n}$$\n由于 $g_{2n} = g_0(-1)^{2n} = g_0$，我们有：\n$$m_{2n} = \\beta_1 m_{2n-1} + (1 - \\beta_1)g_0 \\quad (1)$$\n对于奇数时间步 $2n-1$：\n$$m_{2n-1} = \\beta_1 m_{2n-2} + (1 - \\beta_1)g_{2n-1}$$\n由于 $g_{2n-1} = g_0(-1)^{2n-1} = -g_0$，我们有：\n$$m_{2n-1} = \\beta_1 m_{2n-2} - (1 - \\beta_1)g_0 \\quad (2)$$\n将式 (2) 代入式 (1)：\n$$m_{2n} = \\beta_1 (\\beta_1 m_{2n-2} - (1 - \\beta_1)g_0) + (1 - \\beta_1)g_0$$\n$$m_{2n} = \\beta_1^2 m_{2n-2} - \\beta_1(1 - \\beta_1)g_0 + (1 - \\beta_1)g_0$$\n合并关于 $g_0$ 的项：\n$$m_{2n} = \\beta_1^2 m_{2n-2} + g_0(1 - \\beta_1)(1 - \\beta_1)$$\n$$m_{2n} = \\beta_1^2 m_{2n-2} + (1 - \\beta_1)^2 g_0$$\n这是一个关于偶数项序列 $\\{m_{2n}\\}$ 的线性递推关系。当 $n \\to \\infty$ 时，如果该序列收敛到一个极限值 $m^*$，那么这个极限值必须是该递推关系的一个不动点。\n令 $m^* = \\lim_{n \\to \\infty} m_{2n}$。那么 $\\lim_{n \\to \\infty} m_{2n-2}$ 也等于 $m^*$。将 $m^*$ 代入递推关系：\n$$m^* = \\beta_1^2 m^* + (1 - \\beta_1)^2 g_0$$\n$$m^*(1 - \\beta_1^2) = (1 - \\beta_1)^2 g_0$$\n解出 $m^*$：\n$$m^* = \\frac{(1 - \\beta_1)^2}{1 - \\beta_1^2} g_0$$\n利用平方差公式 $1 - \\beta_1^2 = (1 - \\beta_1)(1 + \\beta_1)$，我们得到：\n$$m^* = \\frac{(1 - \\beta_1)^2}{(1 - \\beta_1)(1 + \\beta_1)} g_0 = \\frac{1 - \\beta_1}{1 + \\beta_1} g_0$$\n由于 $0  \\beta_1  1$，极限存在且收敛于此值。\n因此，当 $t$ 通过偶数趋于无穷大时，一阶矩估计的极限值为：\n$$ \\lim_{n \\to \\infty} m_{2n} = \\frac{1 - \\beta_1}{1 + \\beta_1} g_0 $$", "answer": "$$\\boxed{\\frac{1 - \\beta_{1}}{1 + \\beta_{1}}\\,g_{0}}$$", "id": "2152247"}]}