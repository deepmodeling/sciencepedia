{"hands_on_practices": [{"introduction": "Adagrad 的自适应学习率调整机制并非凭空捏造的启发式方法。这个练习将通过一个理论推导，揭示 Adagrad 与一种被称为特征白化（feature whitening）的强大预处理技术之间的深刻联系。通过比较 Adagrad 的更新方向与一个在理想化（白化）特征空间中进行梯度下降的更新方向，你将从根本上理解为什么根据损失函数的几何特性调整学习率是有效的。[@problem_id:3095404]", "problem": "考虑由经验风险定义的岭回归问题：$$L(\\mathbf{w}) = \\frac{1}{2n}\\|X\\mathbf{w} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2},$$ 其中 $X \\in \\mathbb{R}^{n \\times d}$ 是特征矩阵，$\\mathbf{y} \\in \\mathbb{R}^{n}$ 是目标向量，$\\mathbf{w} \\in \\mathbb{R}^{d}$ 是参数向量，$\\lambda \\geq 0$ 是岭系数。梯度 $\\nabla L(\\mathbf{w})$ 由基本的多变量微积分明确定义。设初始参数为 $\\mathbf{w}_{0} = \\mathbf{0} \\in \\mathbb{R}^{2}$，并假设在 $\\mathbf{w}_{0}$ 处已知以下经验量：\n- 特征协方差矩阵为 $$S = \\frac{1}{n}X^{\\top}X = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix},$$ 该矩阵是正定的。\n- 数据梯度项为 $$\\frac{1}{n}X^{\\top}\\mathbf{y} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}.$$\n\n定义自适应梯度（Adagrad）方法，其各坐标的累加器初始化为零，且不使用数值稳定器，即 $s_{0} = \\mathbf{0}$ 且 $\\epsilon = 0$。整个过程使用固定的学习率 $\\eta > 0$。同时，考虑在完全白化的特征上执行标准梯度下降，这意味着对特征进行线性变换后得到单位协方差；在白化坐标中执行更新，然后将其映射回原始参数空间。\n\n仅使用基本定义和第一性原理：\n1. 从 $\\mathbf{w}_{0}$ 开始，推导在原始参数空间中 Adagrad 更新的第一步方向。\n2. 从对应的原点开始，在白化特征空间中执行一步梯度下降，推导由此产生的在原始参数空间中的第一步更新方向。\n3. 计算这两个更新方向之间夹角的余弦值。\n\n以余弦值的精确值形式给出最终答案。不要四舍五入。不需要单位。你的推导过程必须从给定的基本定义（损失、梯度、协方差、白化）开始，并进行明确的推导；不要调用任何快捷公式。", "solution": "问题要求计算岭回归问题中两种不同优化算法——Adagrad 和在白化特征空间中的梯度下降——其第一步更新方向之间夹角的余弦值。整个过程从验证问题陈述开始。\n\n### 问题验证\n**第 1 步：提取已知条件**\n- 损失函数：$L(\\mathbf{w}) = \\frac{1}{2n}\\|X\\mathbf{w} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$\n- 初始参数：$\\mathbf{w}_{0} = \\mathbf{0} \\in \\mathbb{R}^{2}$\n- 特征协方差矩阵：$S = \\frac{1}{n}X^{\\top}X = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}$\n- 数据梯度项：$\\frac{1}{n}X^{\\top}\\mathbf{y} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}$\n- Adagrad 参数：$s_{0} = \\mathbf{0}$，$\\epsilon = 0$，学习率 $\\eta > 0$。\n- 白化空间：梯度下降在一个特征被变换为具有单位协方差矩阵的空间中执行。\n\n**第 2 步：使用提取的已知条件进行验证**\n该问题具有科学依据，使用了优化和机器学习中的标准定义。问题是适定的，为获得唯一解提供了所有必要量。语言客观精确。数据一致。因此，该问题被认定为有效。\n\n### 解题推导\n首先，我们求损失函数 $L(\\mathbf{w})$ 的梯度。\n$L(\\mathbf{w}) = \\frac{1}{2n}(X\\mathbf{w} - \\mathbf{y})^{\\top}(X\\mathbf{w} - \\mathbf{y}) + \\frac{\\lambda}{2}\\mathbf{w}^{\\top}\\mathbf{w}$\n$L(\\mathbf{w}) = \\frac{1}{2n}(\\mathbf{w}^{\\top}X^{\\top}X\\mathbf{w} - 2\\mathbf{y}^{\\top}X\\mathbf{w} + \\mathbf{y}^{\\top}\\mathbf{y}) + \\frac{\\lambda}{2}\\mathbf{w}^{\\top}\\mathbf{w}$\n关于 $\\mathbf{w}$ 的梯度是：\n$\\nabla L(\\mathbf{w}) = \\frac{1}{n}X^{\\top}X\\mathbf{w} - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda\\mathbf{w}$\n使用所给的定义，我们可以将其写为：\n$\\nabla L(\\mathbf{w}) = S\\mathbf{w} - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda\\mathbf{w}$\n\n我们需要在初始参数 $\\mathbf{w}_{0} = \\mathbf{0}$ 处计算梯度：\n$\\mathbf{g}_{0} = \\nabla L(\\mathbf{w}_{0}) = S(\\mathbf{0}) - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda(\\mathbf{0}) = - \\frac{1}{n}X^{\\top}\\mathbf{y}$\n代入给定值：\n$\\mathbf{g}_{0} = - \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n注意，在原点处的梯度 $\\mathbf{g}_{0}$ 与正则化参数 $\\lambda$ 无关。\n\n**1. Adagrad 第一步更新方向**\nAdagrad 的更新规则由下式给出：\n$\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_{t+1}} + \\epsilon} \\odot \\mathbf{g}_{t}$\n其中 $\\mathbf{s}_{t+1} = \\mathbf{s}_{t} + \\mathbf{g}_{t} \\odot \\mathbf{g}_{t}$（逐元素乘积）。\n对于第一步（从 $t=0$ 到 $t=1$），我们有 $\\mathbf{w}_{0} = \\mathbf{0}$，$\\mathbf{s}_{0} = \\mathbf{0}$，且 $\\epsilon = 0$。梯度为 $\\mathbf{g}_{0}$。\n\n首先，我们更新累加器 $\\mathbf{s}$：\n$\\mathbf{s}_{1} = \\mathbf{s}_{0} + \\mathbf{g}_{0} \\odot \\mathbf{g}_{0} = \\mathbf{0} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2^2 \\\\ 1^2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}$\n接下来，我们计算参数的更新量。更新向量为 $\\Delta\\mathbf{w}_{\\text{Adagrad}} = \\mathbf{w}_{1} - \\mathbf{w}_{0}$。\n$\\mathbf{w}_{1} = \\mathbf{w}_{0} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_{1}} + \\epsilon} \\odot \\mathbf{g}_{0} = \\mathbf{0} - \\frac{\\eta}{\\sqrt{\\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}} + 0} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n$\\Delta\\mathbf{w}_{\\text{Adagrad}} = -\\eta \\begin{pmatrix} 1/\\sqrt{4} \\\\ 1/\\sqrt{1} \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} 1/2 \\\\ 1 \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} (1/2) \\cdot 2 \\\\ 1 \\cdot 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n更新方向是与 $\\Delta\\mathbf{w}_{\\text{Adagrad}}$ 成比例的任意向量。我们可以选择方向向量 $\\mathbf{d}_{\\text{Adagrad}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$。\n\n**2. 白化梯度下降的第一步更新方向**\n特征白化意味着找到一个变换矩阵 $W$，使得新的特征 $\\tilde{X} = XW$ 具有单位协方差矩阵。新的协方差是 $\\tilde{S} = \\frac{1}{n}\\tilde{X}^{\\top}\\tilde{X} = W^{\\top}(\\frac{1}{n}X^{\\top}X)W = W^{\\top}SW$。我们需要 $\\tilde{S} = I$。白化矩阵的一个标准选择是 $W = S^{-1/2}$，其中 $S^{-1/2}$ 是 $S$ 的逆矩阵的主平方根。由于 $S$ 是对称正定的，$S^{-1/2}$ 也是对称正定的。使用这个选择，$W^{\\top}SW = (S^{-1/2})^{\\top} S S^{-1/2} = S^{-1/2} S S^{-1/2} = I$。\n\n为保持预测不变，参数必须进行相应变换：$X\\mathbf{w} = (XW)\\tilde{\\mathbf{w}} = \\tilde{X}\\tilde{\\mathbf{w}}$，这意味着 $\\mathbf{w} = W\\tilde{\\mathbf{w}}$。初始参数 $\\mathbf{w}_0 = \\mathbf{0}$ 映射到 $\\tilde{\\mathbf{w}}_0 = W^{-1}\\mathbf{0} = \\mathbf{0}$。\n\n我们在以 $\\tilde{\\mathbf{w}}$ 表示的损失函数上执行梯度下降：\n$\\tilde{L}(\\tilde{\\mathbf{w}}) = L(W\\tilde{\\mathbf{w}}) = \\frac{1}{2n}\\|\\tilde{X}\\tilde{\\mathbf{w}} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|W\\tilde{\\mathbf{w}}\\|^{2}$\n梯度为 $\\nabla_{\\tilde{\\mathbf{w}}}\\tilde{L}(\\tilde{\\mathbf{w}}) = \\frac{1}{n}\\tilde{X}^{\\top}(\\tilde{X}\\tilde{\\mathbf{w}}-\\mathbf{y}) + \\lambda W^{\\top}W\\tilde{\\mathbf{w}}$。\n代入 $\\frac{1}{n}\\tilde{X}^{\\top}\\tilde{X} = I$ 和 $W^{\\top}W = (S^{-1/2})^{\\top}S^{-1/2} = S^{-1}$：\n$\\nabla_{\\tilde{\\mathbf{w}}}\\tilde{L}(\\tilde{\\mathbf{w}}) = \\tilde{\\mathbf{w}} - \\frac{1}{n}\\tilde{X}^{\\top}\\mathbf{y} + \\lambda S^{-1}\\tilde{\\mathbf{w}} = (I + \\lambda S^{-1})\\tilde{\\mathbf{w}} - W^{\\top}(\\frac{1}{n}X^{\\top}\\mathbf{y})$。\n在 $\\tilde{\\mathbf{w}}_0 = \\mathbf{0}$ 处，梯度为：\n$\\tilde{\\mathbf{g}}_0 = -\\frac{1}{n}\\tilde{X}^{\\top}\\mathbf{y} = -W^{\\top}(\\frac{1}{n}X^{\\top}\\mathbf{y}) = -S^{-1/2}(-\\mathbf{g}_0) = S^{-1/2}\\mathbf{g}_0$。\n在白化空间中的梯度下降更新为 $\\tilde{\\mathbf{w}}_1 = \\tilde{\\mathbf{w}}_0 - \\eta' \\tilde{\\mathbf{g}}_0 = -\\eta' \\tilde{\\mathbf{g}}_0$。\n\n我们将此更新映射回原始参数空间：$\\mathbf{w}_1 = W\\tilde{\\mathbf{w}}_1$。\n更新向量为 $\\Delta\\mathbf{w}_{\\text{white}} = \\mathbf{w}_1 - \\mathbf{w}_0 = W\\tilde{\\mathbf{w}}_1 - \\mathbf{0} = S^{-1/2}(-\\eta' \\tilde{\\mathbf{g}}_0) = -\\eta' S^{-1/2} (S^{-1/2}\\mathbf{g}_0) = -\\eta' S^{-1}\\mathbf{g}_0$。\n更新方向与 $-S^{-1}\\mathbf{g}_0$ 成比例。我们来计算这个向量。\n我们有 $S = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}$ 和 $\\mathbf{g}_{0} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n首先，我们求 $S$ 的逆矩阵：\n$\\det(S) = 3 \\times 3 - 2 \\times 2 = 9 - 4 = 5$。\n$S^{-1} = \\frac{1}{\\det(S)}\\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix}$。\n现在我们计算方向向量：\n$-S^{-1}\\mathbf{g}_0 = -\\frac{1}{5}\\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix}\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 3(2) - 2(1) \\\\ -2(2) + 3(1) \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 6-2 \\\\ -4+3 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$。\n更新方向由任何与此成比例的向量指定。我们可以选择 $\\mathbf{d}_{\\text{white}} = \\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$。\n\n**3. 两个更新方向之间夹角的余弦值**\n我们需要求 $\\mathbf{d}_{\\text{Adagrad}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$ 和 $\\mathbf{d}_{\\text{white}} = \\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$ 之间夹角 $\\theta$ 的余弦值。\n余弦值由公式 $\\cos(\\theta) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}$ 给出。\n点积为：\n$\\mathbf{d}_{\\text{Adagrad}} \\cdot \\mathbf{d}_{\\text{white}} = (-1)(-4) + (-1)(1) = 4 - 1 = 3$。\n向量的模（范数）为：\n$\\|\\mathbf{d}_{\\text{Adagrad}}\\| = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2}$。\n$\\|\\mathbf{d}_{\\text{white}}\\| = \\sqrt{(-4)^2 + 1^2} = \\sqrt{16+1} = \\sqrt{17}$。\n夹角的余弦值为：\n$\\cos(\\theta) = \\frac{3}{\\sqrt{2} \\sqrt{17}} = \\frac{3}{\\sqrt{34}}$。\n为了使分母有理化，可以将其写为 $\\frac{3\\sqrt{34}}{34}$，但题目不作此要求。我们将其保留为更简洁的精确形式。", "answer": "$$\\boxed{\\frac{3}{\\sqrt{34}}}$$", "id": "3095404"}, {"introduction": "现在，让我们从理论走向实践。这个编程练习将带你直面优化中的一个经典挑战：病态（ill-conditioned）问题，即不同参数方向上的曲率差异巨大，因而需要截然不同的学习率。通过在一个精心设计的二次函数上亲手实现并比较 Adagrad 与标准梯度下降（SGD），你将直观地观察到 Adagrad 在各个维度上实现均衡进展的卓越能力。[@problem_id:3095498]", "problem": "要求您在严格凸、可分离的二次目标函数上实现并比较随机梯度下降（SGD）和自适应梯度方法（Adagrad），并量化当曲率在不同坐标轴上相差几个数量级时它们的行为。您的实现必须是一个完整的、可运行的程序，仅使用标准库和指定的科学库，并以精确的格式打印所需的输出。\n\n考虑由可分离凸二次函数定义的目标函数\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} a_i\\, \\theta_i^2,\n$$\n其中 $d$ 是维度，$\\boldsymbol{\\theta}\\in\\mathbb{R}^d$，且 $a_i \\gt 0$ 是曲率系数。该函数是可微的，其梯度由偏导数向量给出，其海森矩阵是主对角线元素为 $a_i$ 的对角矩阵，从而确保了严格凸性。\n\n您必须使用的基本事实：\n- 对于任何可微函数 $f$，梯度是偏导数向量，当步长足够小时，沿梯度相反方向移动会局部减小函数值（一阶最优性原理）。\n- 对于上述可分离二次函数，关于 $\\theta_i$ 的偏导数等于曲率系数与该坐标的乘积，这是求导的幂次法则和线性性质的结果。\n- 随机梯度下降（SGD）是一种通过减去一个由学习率缩放的梯度来迭代更新参数的方法。\n- Adagrad（自适应梯度方法）是一种逐坐标自适应步长的方法，它通过该坐标过去所有梯度平方累积和的平方根的倒数，再加上一个小的正常数以确保数值稳定性，来缩放每个坐标的步长。\n\n您的任务：\n1) 从相同的初始点 $\\boldsymbol{\\theta}_0$ 开始，实现两种优化过程：\n   - 随机梯度下降（SGD）：应用典型的迭代更新规则，在每一步减去一个由常数步长缩放的梯度。\n   - 自适应梯度方法（Adagrad）：应用典型的逐坐标自适应步长方案，该方案累积梯度平方，并通过该累积值加上一个固定的正常数的平方根的倒数来逐坐标缩放步长。\n\n2) 对于每个优化器和每个测试用例，精确模拟 $T$ 步并计算：\n   - 每个优化器各自的最终参数向量 $\\boldsymbol{\\theta}_T$。\n   - 每个优化器的目标函数值 $f(\\boldsymbol{\\theta}_T)$。\n   - 每个优化器的逐轴进展分数\n     $$\n     \\rho_i \\;=\\; \\frac{\\lvert \\theta_{0,i}\\rvert - \\lvert \\theta_{T,i}\\rvert}{\\lvert \\theta_{0,i}\\rvert},\n     $$\n     为处理潜在的数值异常，结果被裁剪到区间 $\\left[0,1\\right]$ 内。\n   - 每个优化器的轴向进展各向异性指数，定义为\n     $$\n     \\mathrm{anisotropy} \\;=\\; \\frac{\\max_i \\rho_i}{\\min_i \\rho_i},\n     $$\n     约定如果 $\\min_i \\rho_i = 0$，则各向异性指数视为 $+\\infty$。\n   - 每个优化器的经验条件数敏感度，定义为对数对 $\\left(\\log a_i,\\, \\log \\rho_i\\right)$（其中 $i\\in\\{1,\\dots,d\\}$）进行最小二乘线性拟合的斜率。这里的对数是自然对数，并且如果需要，通过裁剪到一个小的正下界来确保 $\\rho_i$ 严格为正。具体来说，设 $x_i = \\log a_i$ 和 $y_i = \\log\\left(\\max(\\rho_i, \\delta)\\right)$（其中 $\\delta \\gt 0$ 是一个极小值），计算\n     $$\n     s \\;=\\; \\frac{\\sum_{i=1}^{d} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{d} (x_i - \\bar{x})^2},\n     $$\n     其中 $\\bar{x}$ 和 $\\bar{y}$ 分别是 $\\{x_i\\}$ 和 $\\{y_i\\}$ 的样本均值。较大的 $\\lvert s\\rvert$ 表示进展对曲率的依赖性更强；接近 $0$ 的值表示对条件数的敏感度降低。\n\n3) 对于每个测试用例，还需计算条件数\n   $$\n   \\kappa \\;=\\; \\frac{\\max_i a_i}{\\min_i a_i}。\n   $$\n\n测试套件：\n精确实现以下三个测试用例，每个用例都包含维度 $d$、曲率向量 $\\boldsymbol{a}$、初始点 $\\boldsymbol{\\theta}_0$、步数 $T$ 和学习率超参数。下面提供的所有数值必须完全按原样使用。\n\n- 测试用例 1（理想情况，多数量级曲率，中等步数）：\n  - $d = 3$\n  - $\\boldsymbol{a} = \\left[10^{-2},\\, 1,\\, 10^{2}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[10,\\, -10,\\, 10\\right]$\n  - $T = 200$\n  - SGD 学习率 $\\eta_{\\mathrm{sgd}} = 0.015$\n  - Adagrad 基础学习率 $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad 稳定性常数 $\\epsilon = 10^{-8}$\n\n- 测试用例 2（病态条件严重，混合小曲率和大曲率）：\n  - $d = 4$\n  - $\\boldsymbol{a} = \\left[10^{-4},\\, 10^{-2},\\, 1,\\, 10^{3}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[1,\\, 1,\\, 1,\\, 1\\right]$\n  - $T = 400$\n  - SGD 学习率 $\\eta_{\\mathrm{sgd}} = 0.001$\n  - Adagrad 基础学习率 $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad 稳定性常数 $\\epsilon = 10^{-8}$\n\n- 测试用例 3（极端曲率和大步数的边界条件）：\n  - $d = 2$\n  - $\\boldsymbol{a} = \\left[1,\\, 10^{6}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[10^{3},\\, 10^{3}\\right]$\n  - $T = 5000$\n  - SGD 学习率 $\\eta_{\\mathrm{sgd}} = 10^{-6}$\n  - Adagrad 基础学习率 $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad 稳定性常数 $\\epsilon = 10^{-8}$\n\n输出规范：\n- 对于每个测试用例，按以下顺序生成一个包含 7 个实数的列表：\n  $$\n  \\left[\\kappa,\\ \\mathrm{anisotropy}_{\\mathrm{sgd}},\\ \\mathrm{anisotropy}_{\\mathrm{ada}},\\ f\\!\\left(\\boldsymbol{\\theta}^{\\mathrm{sgd}}_{T}\\right),\\ f\\!\\left(\\boldsymbol{\\theta}^{\\mathrm{ada}}_{T}\\right),\\ s_{\\mathrm{sgd}},\\ s_{\\mathrm{ada}}\\right]。\n  $$\n- 将三个测试用例的结果列表聚合到一个外部列表中，并将该外部列表打印在单一行上，不含空格，使用方括号和逗号分隔符，例如：\n  $$\n  \\left[[x_{1,1},x_{1,2},\\dots,x_{1,7}],[x_{2,1},\\dots,x_{2,7}],[x_{3,1},\\dots,x_{3,7}]\\right]。\n  $$\n- 所有输出都是不带单位的纯数字。不涉及角度。如果任何中间值可能导致出现 $\\log 0$，您的实现必须在取对数之前将参数裁剪为一个极小的正数（例如 $\\delta = 10^{-300}$），以确保结果是有限的。\n- 您的程序不得读取任何输入，并且必须精确实现上述值和顺序。最终的打印输出必须仅包含指定格式的单行输出。", "solution": "该问题已经过验证，被认为是可靠、适定且可形式化的。所提供的参数和定义足以确保唯一解，且相互一致。\n\n这个问题的核心是在一种特定类型的目标函数上，比较两种基本的基于梯度的优化算法——随机梯度下降（SGD）和自适应梯度方法（Adagrad）——的性能。该函数是一个可分离的凸二次函数，它是一个理想的测试平台，因为其逐坐标的曲率可以被精确控制，从而能够清晰地分析每种算法如何处理病态条件（ill-conditioning）。\n\n目标函数定义为：\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} a_i\\, \\theta_i^2\n$$\n其中 $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_d) \\in\\mathbb{R}^d$ 是参数向量，而 $\\boldsymbol{a} = (a_1, \\dots, a_d)$（其中 $a_i > 0$）是曲率系数向量。该函数的全局最小值唯一地位于 $\\boldsymbol{\\theta} = \\mathbf{0}$。\n\n梯度向量 $\\nabla f(\\boldsymbol{\\theta})$ 的分量由偏导数给出：\n$$\n\\frac{\\partial f}{\\partial \\theta_i} \\;=\\; a_i \\theta_i\n$$\n海森矩阵是一个对角矩阵，其对角线元素为 $a_i$，这证实了 $a_i$ 值确实是函数等值面（即超椭球体）沿各个主轴的曲率。\n\n**随机梯度下降（SGD）**\n\nSGD 是一种迭代优化算法，它通过将参数沿梯度相反方向移动来更新参数。在第 $t$ 步，整个参数向量 $\\boldsymbol{\\theta}$ 的更新规则是：\n$$\n\\boldsymbol{\\theta}_{t+1} \\;=\\; \\boldsymbol{\\theta}_t - \\eta_{\\mathrm{sgd}} \\nabla f(\\boldsymbol{\\theta}_t)\n$$\n其中 $\\eta_{\\mathrm{sgd}}$ 是学习率，一个正的标量常数。由于函数是可分离的，我们可以独立分析每个分量 $\\theta_i$ 的更新：\n$$\n\\theta_{t+1, i} \\;=\\; \\theta_{t, i} - \\eta_{\\mathrm{sgd}} (a_i \\theta_{t, i}) \\;=\\; (1 - \\eta_{\\mathrm{sgd}} a_i) \\theta_{t, i}\n$$\n这揭示了每个坐标的值都以几何级数的方式衰减至 $0$。衰减速率由因子 $(1 - \\eta_{\\mathrm{sgd}} a_i)$ 决定。为使算法收敛，必须满足 $|1 - \\eta_{\\mathrm{sgd}} a_i| < 1$，这意味着 $0 < \\eta_{\\mathrm{sgd}} a_i < 2$。为确保所有坐标同时收敛，学习率的选择必须满足最大曲率下的这个条件，即 $\\eta_{\\mathrm{sgd}} < 2 / \\max_i a_i$。然而，这个选择对曲率较小（即 $a_i$ 较小）的坐标施加了非常慢的收敛速率，因为它们的衰减因子 $(1 - \\eta_{\\mathrm{sgd}} a_i)$ 会非常接近 $1$。这是 SGD 在病态条件问题中的根本弱点：单一的学习率无法对所有坐标都是最优的。\n\n**自适应梯度方法（Adagrad）**\n\nAdagrad 通过使用逐参数的自适应学习率来解决 SGD 的缺点。它将每个参数的学习率与该参数过去所有梯度平方和的平方根成反比进行缩放。在第 $t$ 步，分量 $\\theta_i$ 的更新规则是：\n$$\n\\theta_{t+1, i} \\;=\\; \\theta_{t, i} - \\frac{\\eta_{\\mathrm{ada}}}{\\sqrt{S_{t,i} + \\epsilon}} g_{t,i}\n$$\n其中 $\\eta_{\\mathrm{ada}}$ 是基础学习率， $g_{t,i} = a_i \\theta_{t,i}$ 是梯度分量，$\\epsilon$ 是一个很小的稳定常数，以防止除以零。项 $S_{t,i}$ 是梯度平方的累加器：\n$$\nS_{t,i} \\;=\\; \\sum_{k=1}^{t} g_{k,i}^2\n$$\n在第 $t$ 步，参数 $\\theta_i$ 的有效学习率是 $\\frac{\\eta_{\\mathrm{ada}}}{\\sqrt{S_{t,i} + \\epsilon}}$。对于具有高曲率（大 $a_i$）的坐标，梯度 $g_{t,i}$ 的量级会很大，导致 $S_{t,i}$ 快速增长。这会迅速降低有效学习率，防止发散和过冲。相反，对于具有低曲率（小 $a_i$）的坐标，梯度很小，$S_{t,i}$ 增长缓慢，有效学习率保持较高水平，从而实现显著的进展。这种机制自动平衡了学习率，即使在条件恶劣的问题中，也能在所有维度上实现更均匀的进展。\n\n**分析指标**\n\n问题要求计算几个指标来量化每个优化器的性能和行为：\n1.  **条件数 ($\\kappa$)**：$\\kappa = \\frac{\\max_i a_i}{\\min_i a_i}$。这个指标衡量问题的病态程度。大的 $\\kappa$ 表示曲率之间存在巨大差异。\n2.  **逐轴进展分数 ($\\rho_i$)**：$\\rho_i = \\frac{\\lvert \\theta_{0,i}\\rvert - \\lvert \\theta_{T,i}\\rvert}{\\lvert \\theta_{0,i}\\rvert}$，裁剪到 $[0,1]$ 区间。它衡量沿每个轴朝向最小值所取得的进展比例，其中 $1$ 表示完全进展，$0$ 表示没有进展。\n3.  **进展各向异性**：$\\mathrm{anisotropy} = \\frac{\\max_i \\rho_i}{\\min_i \\rho_i}$。这个比率量化了跨轴进展的不均匀性。值为 $1$ 表示完全各向同性（均匀）的进展。我们预期 Adagrad 会产生比 SGD 更低的各向异性。\n4.  **条件数敏感度 ($s$)**：这是 $\\log \\rho_i$ 对 $\\log a_i$ 进行线性回归的斜率。它衡量一个轴上的进展对其曲率的依赖强度。对于 SGD，我们预期存在强烈的负相关，即更高的曲率导致更少的进展（负斜率 $s$）。对于旨在抵消这种效应的 Adagrad，我们预期进展 $\\rho_i$ 在很大程度上独立于 $a_i$，从而导致敏感度 $s$ 接近 $0$。\n\n下面的实现将针对指定的测试用例模拟 SGD 和 Adagrad，并计算这些指标，以展示它们截然不同的行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares SGD and Adagrad optimizers on a separable quadratic\n    objective function for a suite of test cases, and prints the results in the\n    specified format.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"d\": 3,\n            \"a\": np.array([1e-2, 1.0, 1e2]),\n            \"theta_0\": np.array([10.0, -10.0, 10.0]),\n            \"T\": 200,\n            \"eta_sgd\": 0.015,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"d\": 4,\n            \"a\": np.array([1e-4, 1e-2, 1.0, 1e3]),\n            \"theta_0\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"T\": 400,\n            \"eta_sgd\": 0.001,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"d\": 2,\n            \"a\": np.array([1.0, 1e6]),\n            \"theta_0\": np.array([1e3, 1e3]),\n            \"T\": 5000,\n            \"eta_sgd\": 1e-6,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case[\"a\"]\n        theta_0 = case[\"theta_0\"]\n        T = case[\"T\"]\n        eta_sgd = case[\"eta_sgd\"]\n        eta_ada = case[\"eta_ada\"]\n        epsilon = case[\"epsilon\"]\n        \n        # --- SGD Simulation ---\n        theta_sgd = theta_0.copy()\n        for _ in range(T):\n            grad = a * theta_sgd\n            theta_sgd -= eta_sgd * grad\n        \n        # --- Adagrad Simulation ---\n        theta_ada = theta_0.copy()\n        sum_sq_grad = np.zeros_like(theta_ada)\n        for _ in range(T):\n            grad = a * theta_ada\n            sum_sq_grad += grad**2\n            theta_ada -= eta_ada * grad / (np.sqrt(sum_sq_grad) + epsilon)\n\n        # --- Metrics Calculation Function ---\n        def calculate_optimizer_metrics(theta_T, theta_0, a):\n            # Final objective value\n            f_val = 0.5 * np.sum(a * theta_T**2)\n            \n            # Per-axis progress fraction (rho)\n            abs_theta_0 = np.abs(theta_0)\n            # Avoid division by zero if an initial coordinate is 0\n            safe_abs_theta_0 = np.where(abs_theta_0 == 0, 1.0, abs_theta_0)\n            rho = (abs_theta_0 - np.abs(theta_T)) / safe_abs_theta_0\n            rho = np.clip(rho, 0, 1)\n\n            # Anisotropy\n            min_rho = np.min(rho)\n            if min_rho == 0.0:\n                anisotropy = np.inf\n            else:\n                anisotropy = np.max(rho) / min_rho\n            \n            # Empirical condition-number sensitivity (s)\n            # Use a tiny delta to avoid log(0) as per problem statement\n            delta = 1e-300\n            log_a = np.log(a)\n            log_rho = np.log(np.maximum(rho, delta))\n            \n            x = log_a\n            y = log_rho\n            x_mean = np.mean(x)\n            y_mean = np.mean(y)\n            \n            # The denominator is zero only if all a_i are identical.\n            numerator = np.sum((x - x_mean) * (y - y_mean))\n            denominator = np.sum((x - x_mean)**2)\n            s = numerator / denominator if denominator != 0 else 0.0\n            \n            return anisotropy, f_val, s\n\n        # --- Aggregate Results for the Case ---\n        kappa = np.max(a) / np.min(a)\n        \n        anisotropy_sgd, f_sgd, s_sgd = calculate_optimizer_metrics(theta_sgd, theta_0, a)\n        anisotropy_ada, f_ada, s_ada = calculate_optimizer_metrics(theta_ada, theta_0, a)\n\n        case_results = [\n            kappa,\n            anisotropy_sgd,\n            anisotropy_ada,\n            f_sgd,\n            f_ada,\n            s_sgd,\n            s_ada\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified\n    sublist_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(sublist_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3095498"}, {"introduction": "尽管 Adagrad 功能强大，但它有一个关键的局限性：其学习率只会单调递减，永不增加。这个练习将探讨 Adagrad 的这一“阿喀琉斯之踵”在一个数据分布随时间变化的非平稳（nonstationary）场景中的表现。你将研究 Adagrad 的设计如何阻碍其适应新数据的能力，并将其与一个能够“遗忘”旧梯度信息的改进版本进行比较，为理解更高级的优化器（如 RMSprop 和 Adam）铺平道路。[@problem_id:3095442]", "problem": "你将构建并分析一个流式优化场景，其中一个特征在某个变化点之后变得相关，并比较两种自适应步长策略。从以下基本设定开始：使用随机梯度下降对每步平方损失 $\\ell_t(\\boldsymbol{w}) = \\tfrac{1}{2}\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)^2$ 训练线性预测器 $f_{\\boldsymbol{w}}(\\boldsymbol{x}) = \\boldsymbol{w}^{\\top}\\boldsymbol{x}$，其梯度为 $\\nabla \\ell_t(\\boldsymbol{w}) = -\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)\\boldsymbol{x}_t$。任务是实现两种优化器，它们根据过去的梯度来修改每个坐标的学习率：一种累加所有历史梯度平方，另一种对累加器进行指数衰减，从而更重地加权近期的梯度。\n\n构建一个包含 $2$ 个特征的非平稳数据集，其中第二个特征仅在变化时间 $t^*$ 之后才变得相关。使用 $T$ 步的流式处理范围，恒定特征 $\\boldsymbol{x}_t = [1, 1]$，以及真实权重 $\\boldsymbol{w}^*_t = [1, w^*_2(t)]$，其中当 $t < t^*$ 时 $w^*_2(t) = 0$，当 $t \\ge t^*$ 时 $w^*_2(t) = 1$。不存在观测噪声，即 $y_t = \\boldsymbol{w}^{*}_t{}^{\\top}\\boldsymbol{x}_t$。将模型参数初始化为 $\\boldsymbol{w}_0 = [0,0]$。\n\n实现两种更新方案，分别应用于相同的流数据：\n1. 一个基准累加器，对每个坐标的梯度平方进行求和。在每个时间点 $t$，设累加器为 $\\boldsymbol{G}_t$，梯度为 $\\boldsymbol{g}_t = \\nabla \\ell_t(\\boldsymbol{w}_t)$。通过加上 $\\boldsymbol{g}_t$ 的逐元素平方来更新 $\\boldsymbol{G}_t$，并通过减去梯度与一个按坐标的步长的逐元素乘积来更新 $\\boldsymbol{w}_t$。该步长与累加器平方根成反比，并由一个小常数 $\\,\\epsilon\\,$ 进行稳定。\n2. 一个衰减累加器，形成梯度平方的指数移动平均。在每个时间点 $t$，设累加器为 $\\boldsymbol{H}_t$，并将其更新为 $\\boldsymbol{g}_t^2$ 的指数移动平均（衰减参数为 $\\alpha \\in (0,1)$），然后使用与 $\\sqrt{\\boldsymbol{H}_t + \\epsilon}$ 成反比的按坐标步长来更新 $\\boldsymbol{w}_t$。\n\n对于一个优化器，将第二个坐标的适应延迟定义为变化时间 $t^*$ 之后满足 $\\left|w_{t^*+s,\\,2} - 1\\right| \\le \\tau$ 的最小非负整数 $s$，每次更新后立即进行评估。如果在可用步数内从未达到该阈值，则返回 $T - t^*$ 作为延迟。对两种优化器使用相同的基础学习率 $\\eta$ 和稳定化常数 $\\epsilon$。\n\n你的程序必须实现此模拟，并为每个测试用例计算一对整数 $[L_{\\text{sum}}, L_{\\text{decay}}]$，分别表示基准求和累加器和衰减累加器的适应延迟。\n\n测试套件：\n- 用例 $1$：$T=200$, $t^*=50$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$。\n- 用例 $2$：$T=200$, $t^*=0$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$。\n- 用例 $3$：$T=200$, $t^*=199$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$。\n- 用例 $4$：$T=200$, $t^*=50$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.99$, $\\tau=0.1$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表。列表中的每个元素是对应一个测试用例的结果对。例如，确切的格式是 $\\left[ [L_{\\text{sum},1},L_{\\text{decay},1}], [L_{\\text{sum},2},L_{\\text{decay},2}], [L_{\\text{sum},3},L_{\\text{decay},3}], [L_{\\text{sum},4},L_{\\text{decay},4}] \\right]$。不涉及物理单位，所有角度（此处没有）均假定为不相关。所有输出必须是整数。", "solution": "该问题要求在一个模拟的非平稳在线学习环境中，对两种自适应学习率优化算法进行比较分析。核心任务是实现并评估一个 Adagrad 风格的优化器（累加所有历史梯度平方）与一个 RMSprop 风格的优化器（使用梯度平方的指数移动平均）的性能。比较的依据是“适应延迟”，该指标衡量在底层数据生成过程发生突变后，每个优化器将模型参数调整到其新的真实值的速度。\n\n首先，我们按照规定将模拟的各个组成部分形式化。\n\n**1. 模型与学习任务**\n预测模型是输入特征 $\\boldsymbol{x}_t \\in \\mathbb{R}^2$ 的线性函数：\n$$f_{\\boldsymbol{w}}(\\boldsymbol{x}_t) = \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t$$\n其中 $\\boldsymbol{w} \\in \\mathbb{R}^2$ 是模型参数的向量。学习过程通过最小化每个时间步 $t$ 的平方误差损失来驱动：\n$$\\ell_t(\\boldsymbol{w}) = \\frac{1}{2}\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)^2$$\n损失函数关于参数 $\\boldsymbol{w}$ 的梯度（记为 $\\boldsymbol{g}_t$）对优化更新至关重要：\n$$\\boldsymbol{g}_t = \\nabla_{\\boldsymbol{w}} \\ell_t(\\boldsymbol{w}) = -\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)\\boldsymbol{x}_t$$\n\n**2. 非平稳数据生成**\n模拟总共运行 $T$ 个时间步。该环境的特点是在时间 $t^*$ 有一个“变化点”。\n- 特征向量是恒定的：对于所有 $t=0, \\dots, T-1$，$\\boldsymbol{x}_t = [1, 1]^{\\top}$。\n- 真实参数向量 $\\boldsymbol{w}^*_t$ 是非平稳的：\n  $$ \\boldsymbol{w}^*_t = [1, w^*_2(t)]^{\\top} \\quad \\text{其中} \\quad w^*_2(t) = \\begin{cases} 0  &\\text{若 } t < t^* \\\\ 1  &\\text{若 } t \\ge t^* \\end{cases} $$\n- 目标值 $y_t$ 由真实模型无噪声地生成：$y_t = (\\boldsymbol{w}^*_t)^{\\top}\\boldsymbol{x}_t$。这导致：\n  $$ y_t = \\begin{cases} [1, 0]^{\\top}[1, 1] = 1  &\\text{若 } t < t^* \\\\ [1, 1]^{\\top}[1, 1] = 2  &\\text{若 } t \\ge t^* \\end{cases} $$\n- 初始模型参数设置为零：$\\boldsymbol{w}_0 = [0, 0]^{\\top}$。\n\n**3. 优化器实现**\n实现了两个独立的优化器，每个都从相同的初始状态 $\\boldsymbol{w}_0$ 开始，并接收相同的数据流 $(\\boldsymbol{x}_t, y_t)$。对于 $t = 0, \\dots, T-1$：\n\n**优化器1：梯度平方和（Adagrad风格）**\n该优化器维护一个累加器 $\\boldsymbol{G}_t$，该累加器对截至时间 $t$ 所见过的所有梯度的逐元素平方进行求和。\n- **初始化**：$\\boldsymbol{G}_{-1} = \\boldsymbol{0}$。\n- **累加器更新**：$\\boldsymbol{G}_t = \\boldsymbol{G}_{t-1} + \\boldsymbol{g}_t^2$，其中平方是逐元素的。\n- **参数更新**：$\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t - \\frac{\\eta}{\\sqrt{\\boldsymbol{G}_t + \\epsilon}} \\odot \\boldsymbol{g}_t$，其中 $\\eta$ 是基础学习率，$\\epsilon$ 是一个小的稳定化常数，$\\odot$ 表示逐元素乘法，除法和平方根也是逐元素的。\n\n该方法的原理是为那些收到较大或频繁梯度更新的参数降低学习率。每个参数的学习率 $\\eta_j = \\frac{\\eta}{\\sqrt{G_{t,j} + \\epsilon}}$ 是单调递减的。虽然这对于在凸问题上收敛是有益的，但在非平稳环境中却构成了挑战。变化点之前在 $G_{t,j}$ 中累积的大值会严重减慢对新目标值的适应速度。\n\n**优化器2：衰减梯度平方（RMSprop风格）**\n该优化器对累加器使用指数移动平均（EMA），这使其能够“忘记”旧的梯度并适应近期的梯度统计数据。\n- **初始化**：$\\boldsymbol{H}_{-1} = \\boldsymbol{0}$。\n- **累加器更新**：$\\boldsymbol{H}_t = \\alpha \\boldsymbol{H}_{t-1} + (1-\\alpha) \\boldsymbol{g}_t^2$，其中 $\\alpha \\in (0,1)$ 是衰减参数。此更新是 EMA 的标准公式。\n- **参数更新**：$\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t - \\frac{\\eta}{\\sqrt{\\boldsymbol{H}_t + \\epsilon}} \\odot \\boldsymbol{g}_t$。\n\n基于 EMA 的累加器确保了如果近期梯度较小，每个参数的学习率可以增加；如果近期梯度较大，学习率则可以减小。据推测，在给定的非平稳设置中，这种自适应性更为优越，因为来自变化点前时代（$t < t^*$）的梯度的影响会随时间减弱。\n\n**4. 适应延迟计算**\n用于比较的指标是适应延迟 $L$。它量化了在变化点 $t^*$ 之后，第二个参数 $w_2$ 收敛到其新目标值 $1$ 所需的步数。\n虽然问题陈述使用了符号 $\\left|w_{t^*+s,\\,2} - 1\\right| \\le \\tau$，但字面解释存在问题，因为 $\\boldsymbol{w}_{t^*}$ 是在变化发生之前的权重。我们采用一种更直接、与问题框架一致的程序化解释。我们寻求表示在变化点 $t^*$ 之后收敛所需的更新步数的最小非负整数 $s$。\n- 模拟循环运行 $t = 0, \\dots, T-1$。\n- 收敛性检查在 $t = t^*, t^*+1, \\dots, T-1$ 这些步骤的更新时进行。\n- 对于每个这样的步骤 $t$，我们计算更新后的权重向量 $\\boldsymbol{w}_{t+1}$。\n- 然后我们检查是否满足 $|w_{t+1, 2} - 1| \\le \\tau$。\n- 如果在步骤 $t$ 首次满足此条件，则适应延迟记录为 $s = t - t^*$。\n- 如果在最终更新（$t=T-1$ 时）仍未满足该条件，则将延迟赋予默认值 $T - t^*$，表示未能在可用的时间窗口内收敛。\n\n该模拟将针对测试套件中的每组参数运行，并为每个用例确定所得的延迟对 $[L_{\\text{sum}}, L_{\\text{decay}}]$。预期在有流中变化点的用例中（例如，用例1），会出现 $L_{\\text{decay}} < L_{\\text{sum}}$ 的情况，这表明衰减累加器具有更优越的自适应性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes two adaptive optimizers on a nonstationary\n    linear regression problem.\n    \"\"\"\n\n    test_cases = [\n        {'T': 200, 't_star': 50, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 0, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 199, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 50, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.99, 'tau': 0.1},\n    ]\n\n    results = []\n\n    for params in test_cases:\n        T = params['T']\n        t_star = params['t_star']\n        eta = params['eta']\n        epsilon = params['epsilon']\n        alpha = params['alpha']\n        tau = params['tau']\n\n        # --- Initialize parameters for both optimizers ---\n        # Weight vectors\n        w_sum = np.zeros(2)\n        w_decay = np.zeros(2)\n\n        # Accumulators\n        G = np.zeros(2)  # Summed accumulator\n        H = np.zeros(2)  # Decayed accumulator\n\n        # Lag tracking\n        lag_sum = -1\n        lag_decay = -1\n\n        # Constant feature vector\n        x_t = np.ones(2)\n\n        # --- Main simulation loop ---\n        for t in range(T):\n            # Determine true weight and target value y_t\n            if t  t_star:\n                w_star_2 = 0.0\n            else:\n                w_star_2 = 1.0\n            w_star_t = np.array([1.0, w_star_2])\n            y_t = np.dot(w_star_t, x_t)\n\n            # --- Optimizer 1: Summed Accumulator (Adagrad-style) ---\n            pred_sum = np.dot(w_sum, x_t)\n            error_sum = y_t - pred_sum\n            g_t_sum = -error_sum * x_t\n\n            G += g_t_sum**2\n            step_size_sum = eta / (np.sqrt(G) + epsilon)\n            w_sum -= step_size_sum * g_t_sum\n\n            # --- Optimizer 2: Decayed Accumulator (RMSprop-style) ---\n            pred_decay = np.dot(w_decay, x_t)\n            error_decay = y_t - pred_decay\n            g_t_decay = -error_decay * x_t\n\n            H = alpha * H + (1 - alpha) * g_t_decay**2\n            step_size_decay = eta / (np.sqrt(H) + epsilon)\n            w_decay -= step_size_decay * g_t_decay\n\n            # --- Check for adaptation lag after change point ---\n            if t >= t_star:\n                # Check for summed accumulator\n                if lag_sum == -1 and abs(w_sum[1] - 1.0) = tau:\n                    lag_sum = t - t_star\n\n                # Check for decayed accumulator\n                if lag_decay == -1 and abs(w_decay[1] - 1.0) = tau:\n                    lag_decay = t - t_star\n\n        # If threshold was never met, set lag to the default value\n        if lag_sum == -1:\n            lag_sum = T - t_star\n        if lag_decay == -1:\n            lag_decay = T - t_star\n            \n        results.append([lag_sum, lag_decay])\n\n    # Format the final output string\n    result_str = '[' + ','.join(f'[{res[0]},{res[1]}]' for res in results) + ']'\n    print(result_str)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3095442"}]}