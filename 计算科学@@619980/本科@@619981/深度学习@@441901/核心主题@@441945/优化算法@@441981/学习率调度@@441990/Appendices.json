{"hands_on_practices": [{"introduction": "在带有权重衰减的随机梯度下降（SGD）中，我们通常认为学习率 $\\eta_t$ 直接控制了更新步长。然而，权重衰减的存在使得 $\\eta_t$ 与衰减系数 $\\lambda$ 的效果耦合在一起，影响了真实的“有效步长”。本练习 [@problem_id:3142924] 将引导你从近端优化（proximal optimization）的视角出发，推导出有效步长与学习率之间的精确关系，并设计一个学习率调度方案来直接控制这个更根本的量，从而实现更可预测和稳定的训练初期行为。", "problem": "要求您为带权重衰减正则化的随机梯度下降（SGD）设计并实现一个学习率调度函数。从离散时间优化的一个基于原理的模型开始：将权重衰减视为对平方范数惩罚项的一个隐式近端步骤。基本目标函数为 $J(\\mathbf{w}) = L(\\mathbf{w}) + \\frac{\\lambda}{2}\\lVert \\mathbf{w} \\rVert_2^2$，其中 $L(\\mathbf{w})$ 是数据拟合项，$\\mathbf{w} \\in \\mathbb{R}^d$ 是参数，$\\lambda \\ge 0$ 是权重衰减系数。考虑在第 $t$ 次迭代时，使用学习率 $\\eta_t$ 的隐式近端更新：\n$$\n\\mathbf{w}_{t+1} \\in \\arg\\min_{\\mathbf{w}} \\left\\{ \\frac{1}{2}\\left\\lVert \\mathbf{w} - \\left(\\mathbf{w}_t - \\eta_t \\nabla L(\\mathbf{w}_t)\\right)\\right\\rVert_2^2 + \\frac{\\eta_t \\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2 \\right\\}.\n$$\n将在得到的第 $t$ 次迭代的更新规则中，乘以负梯度方向的标量定义为权重空间中的有效步长。您的设计目标是在一个早期的预热窗口迭代 $t \\in \\{0,1,\\dots,T_{\\mathrm{warm}}-1\\}$ 期间，保持此有效步长等于一个目标常数 $s_{\\mathrm{target}}$，然后在训练结束时（即 $t = T_{\\mathrm{total}}-1$ 时）通过单调指数衰减将其平滑地减小到一个更小的值 $s_{\\min}$，并确保在最后一次迭代时精确达到 $s_{\\min}$。您的调度方案必须遵守正定性和稳定性约束：学习率必须满足 $0  \\eta_t \\le \\eta_{\\max}$，其中 $\\eta_{\\max} = \\frac{2}{L_g}$，$L_g > 0$ 是为 $L(\\mathbf{w})$ 的梯度提供的一个 Lipschitz 常数界。如果为了保持有效步长等于 $s_{\\mathrm{target}}$（或其指数衰减的延续）所需的无约束值违反了 $0  \\eta_t \\le \\eta_{\\max}$，则用 $\\eta_{\\max}$ 替代它。\n\n任务：\n1. 仅从隐式近端更新的定义以及有效步长是所得更新规则中 $-\\nabla L(\\mathbf{w}_t)$ 的系数这一概念出发，推导出第 $t$ 次迭代时有效步长的表达式，并将其反转以得到 $\\eta_t$ 作为第 $t$ 次迭代时期望的有效步长和 $\\lambda$ 的函数。\n2. 指定一个分段有效步长调度 $s(t)$，使其在 $t \\in \\{0,1,\\dots,T_{\\mathrm{warm}}-1\\}$ 时等于 $s_{\\mathrm{target}}$，并在 $t = T_{\\mathrm{total}}-1$ 时指数衰减至 $s_{\\min}$。确定指数衰减常数，以精确满足端点条件。\n3. 结合以上内容，生成一个在所有 $t$ 下都满足约束 $0  \\eta_t \\le \\eta_{\\max}$ 的学习率调度 $\\eta_t$。\n4. 实现一个独立的程序，为以下四个测试用例中的每一个计算并返回在指定评估迭代次数下的学习率值。如果 $T_{\\mathrm{total}} - T_{\\mathrm{warm}} - 1 \\le 0$，在定义指数调度时，将衰减区间长度视为 $1$。\n\n测试套件（每个用例指定 $(T_{\\mathrm{total}}, T_{\\mathrm{warm}}, s_{\\mathrm{target}}, s_{\\min}, \\lambda, L_g, \\text{eval times})$）：\n- 用例 A：$(100, 40, 0.01, 0.002, 0.1, 50, [0, 20, 40, 60, 99])$。\n- 用例 B：$(60, 20, 0.02, 0.005, 0, 100, [0, 19, 20, 59])$。\n- 用例 C：$(80, 30, 0.01, 0.003, 80, 50, [0, 29, 30, 79])$。\n- 用例 D：$(50, 0, 0.05, 0.01, 0.5, 200, [0, 25, 49])$。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含结果，格式为一个由方括号括起来的列表的列表（例如，$[[\\text{list for Case A}],[\\text{list for Case B}],[\\text{list for Case C}],[\\text{list for Case D}]]$）。\n- 每个内部列表必须包含该用例在指定评估时间点的学习率值 $\\eta_t$，顺序与提供的一致。\n- 所有返回的值必须是实数（浮点数）。不使用物理单位；不使用角度；任何地方都不要使用百分比。", "solution": "用户提供的问题陈述是有效的。它在科学上基于优化理论，特别是近端梯度方法，并与深度学习中的学习率调度直接相关。该问题是适定的，所有必要的参数、定义和约束都已提供，足以推导出唯一解。语言客观而精确。\n\n### 第1步：更新规则和有效步长的推导\n\n问题始于第 $t$ 次迭代时权重向量 $\\mathbf{w}$ 的隐式近端更新规则：\n$$\n\\mathbf{w}_{t+1} \\in \\arg\\min_{\\mathbf{w}} \\left\\{ F(\\mathbf{w}) = \\frac{1}{2}\\left\\lVert \\mathbf{w} - \\left(\\mathbf{w}_t - \\eta_t \\nabla L(\\mathbf{w}_t)\\right)\\right\\rVert_2^2 + \\frac{\\eta_t \\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2 \\right\\}\n$$\n其中 $L(\\mathbf{w})$ 是数据拟合损失，$\\eta_t$ 是学习率，$\\lambda$ 是权重衰减系数。目标函数 $F(\\mathbf{w})$ 是关于 $\\mathbf{w}$ 的严格凸二次函数，因此可以通过将其关于 $\\mathbf{w}$ 的梯度设为零来找到其最小值。\n\n设 $\\mathbf{c}_t = \\mathbf{w}_t - \\eta_t \\nabla L(\\mathbf{w}_t)$。目标函数为：\n$$\nF(\\mathbf{w}) = \\frac{1}{2} (\\mathbf{w} - \\mathbf{c}_t)^T (\\mathbf{w} - \\mathbf{c}_t) + \\frac{\\eta_t \\lambda}{2} \\mathbf{w}^T \\mathbf{w}\n$$\n梯度 $\\nabla_{\\mathbf{w}} F(\\mathbf{w})$ 是：\n$$\n\\nabla_{\\mathbf{w}} F(\\mathbf{w}) = (\\mathbf{w} - \\mathbf{c}_t) + \\eta_t \\lambda \\mathbf{w}\n$$\n在 $\\mathbf{w} = \\mathbf{w}_{t+1}$ 处将梯度设为零：\n$$\n(\\mathbf{w}_{t+1} - \\mathbf{c}_t) + \\eta_t \\lambda \\mathbf{w}_{t+1} = \\mathbf{0}\n$$\n$$\n\\mathbf{w}_{t+1}(1 + \\eta_t \\lambda) = \\mathbf{c}_t\n$$\n$$\n\\mathbf{w}_{t+1} = \\frac{1}{1 + \\eta_t \\lambda} \\mathbf{c}_t\n$$\n将 $\\mathbf{c}_t$ 的定义代回：\n$$\n\\mathbf{w}_{t+1} = \\frac{1}{1 + \\eta_t \\lambda} \\left(\\mathbf{w}_t - \\eta_t \\nabla L(\\mathbf{w}_t)\\right)\n$$\n$$\n\\mathbf{w}_{t+1} = \\frac{1}{1 + \\eta_t \\lambda} \\mathbf{w}_t - \\frac{\\eta_t}{1 + \\eta_t \\lambda} \\nabla L(\\mathbf{w}_t)\n$$\n此方程表示带权重衰减的标准随机梯度下降（SGD）更新，其中权重衰减项 $\\frac{1}{1 + \\eta_t \\lambda}$ 和学习率是耦合的。\n\n问题将“有效步长” $s_t$ 定义为乘以负梯度方向 $-\\nabla L(\\mathbf{w}_t)$ 的标量。从推导出的更新规则中，我们可以确定 $s_t$：\n$$\ns_t = \\frac{\\eta_t}{1 + \\eta_t \\lambda}\n$$\n为了控制调度，我们需要将学习率 $\\eta_t$ 表示为期望有效步长 $s_t$ 的函数。我们反转上述关系：\n$$\ns_t (1 + \\eta_t \\lambda) = \\eta_t\n$$\n$$\ns_t + s_t \\eta_t \\lambda = \\eta_t\n$$\n$$\ns_t = \\eta_t (1 - s_t \\lambda)\n$$\n$$\n\\eta_t = \\frac{s_t}{1 - s_t \\lambda}\n$$\n为使 $\\eta_t$ 为正且有明确定义，我们必须有 $s_t > 0$ 和 $1 - s_t \\lambda > 0$，这意味着 $s_t \\lambda  1$。\n\n### 第2步：有效步长调度 $s(t)$ 的设计\n\n有效步长调度 $s(t)$ 是分段的。\n1.  **预热阶段：** 对于迭代 $t \\in \\{0, 1, \\dots, T_{\\mathrm{warn}}-1\\}$，有效步长是恒定的。\n    $$\n    s(t) = s_{\\mathrm{target}} \\quad \\text{对于 } t  T_{\\mathrm{warm}}\n    $$\n2.  **衰减阶段：** 对于迭代 $t \\in \\{T_{\\mathrm{warm}}, T_{\\mathrm{warm}}+1, \\dots, T_{\\mathrm{total}}-1\\}$，有效步长经历一个“单调指数衰减”。这被建模为一个连接 $s(T_{\\mathrm{warm}}) = s_{\\mathrm{target}}$ 到 $s(T_{\\mathrm{total}}-1) = s_{\\min}$ 的几何级数。\n\n令衰减阶段定义在区间 $[T_{\\mathrm{warm}}, T_{\\mathrm{total}}-1]$ 上。我们需要一个函数 $s(t)$ 使得 $s(T_{\\mathrm{warm}}) = s_{\\mathrm{target}}$ 且 $s(T_{\\mathrm{total}}-1) = s_{\\min}$。一个满足这些边界条件的指数函数（几何级数）是：\n$$\ns(t) = s_{\\mathrm{target}} \\cdot \\left(\\frac{s_{\\min}}{s_{\\mathrm{target}}}\\right)^{\\frac{t - T_{\\mathrm{warm}}}{(T_{\\mathrm{total}}-1) - T_{\\mathrm{warm}}}}\n$$\n这在分母 $D = (T_{\\mathrm{total}}-1) - T_{\\mathrm{warm}} = T_{\\mathrm{total}} - T_{\\mathrm{warm}} - 1$ 为正时有效。\n\n我们必须处理边界情况：\n-   如果 $T_{\\mathrm{total}} - T_{\\mathrm{warm}} - 1 > 0$：上述公式适用。\n-   如果 $T_{\\mathrm{total}} - T_{\\mathrm{warm}} - 1 = 0$，即 $T_{\\mathrm{total}} = T_{\\mathrm{warm}} + 1$：衰减阶段仅包含一个点 $t=T_{\\mathrm{warm}}$。由于 $t=T_{\\mathrm{warm}}$ 也是最后一次迭代 $T_{\\mathrm{total}}-1$，其值必须是 $s_{\\min}$。这在 $t=T_{\\mathrm{warm}}$ 处产生一个不连续点。所以，$s(T_{\\mathrm{warm}}) = s_{\\min}$。\n-   如果 $T_{\\mathrm{total}} - T_{\\mathrm{warm}} - 1  0$，即 $T_{\\mathrm{total}} \\le T_{\\mathrm{warm}}$：衰减阶段为空。所有迭代 $t \\in \\{0, \\dots, T_{\\mathrm{total}}-1\\}$ 都落在预热阶段内，因为 $t  T_{\\mathrm{total}} \\le T_{\\mathrm{warm}}$。因此，对于所有相关的 $t$，$s(t) = s_{\\mathrm{target}}$。\n\n### 第3步：完整的学习率调度 $\\eta_t$\n\n最终的学习率调度 $\\eta_t$ 是通过结合上述元素并应用上界约束来构建的。对于任何给定的迭代 $t \\in \\{0, 1, \\dots, T_{\\mathrm{total}}-1\\}$：\n1.  使用第2步中定义的分段调度确定有效步长 $s(t)$。\n2.  计算无约束学习率 $\\eta_t^{\\text{unconstrained}} = \\frac{s(t)}{1 - s(t) \\lambda}$。\n3.  确定最大允许学习率 $\\eta_{\\max} = \\frac{2}{L_g}$。\n4.  最终学习率是无约束值在 $\\eta_{\\max}$ 处被裁剪后的值：\n    $$\n    \\eta_t = \\min(\\eta_t^{\\text{unconstrained}}, \\eta_{\\max})\n    $$\n\n### 第4步：实现\n\n下面的独立程序实现了推导出的学习率调度，并为提供的测试用例计算所需的值。其逻辑被封装在一个函数中，该函数为给定的 $t$ 和一组参数计算 $\\eta_t$，然后在测试套件中为每个指定的评估时间点调用该函数。最终输出格式化为列表的列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It computes the learning rate at specified iterations for each case\n    and prints the results in the required format.\n    \"\"\"\n    test_cases = [\n        # (T_total, T_warm, s_target, s_min, lambda, L_g, eval_times)\n        (100, 40, 0.01, 0.002, 0.1, 50, [0, 20, 40, 60, 99]),\n        (60, 20, 0.02, 0.005, 0, 100, [0, 19, 20, 59]),\n        (80, 30, 0.01, 0.003, 80, 50, [0, 29, 30, 79]),\n        (50, 0, 0.05, 0.01, 0.5, 200, [0, 25, 49]),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        T_total, T_warm, s_target, s_min, lambd, L_g, eval_times = case\n        \n        eta_max = 2.0 / L_g\n        \n        # Pre-calculate constants for the decay phase\n        decay_duration_denominator = float(T_total - 1 - T_warm)\n        if decay_duration_denominator > 0:\n            ratio = s_min / s_target\n\n        case_results = []\n        for t in eval_times:\n            # 1. Determine the effective step s(t)\n            s_t = 0.0\n            if t  T_warm:\n                s_t = s_target\n            else: # t >= T_warm\n                if decay_duration_denominator == 0:\n                    # This case corresponds to T_total == T_warm + 1.\n                    # As per the derived logic, if the decay phase has one point,\n                    # its value must be s_min. This occurs at t = T_total - 1.\n                    # For t > T_warm but t  T_total - 1 in this scenario (which is impossible),\n                    # we would continue s_target, but since the only valid t is T_total - 1,\n                    # the value is simply s_min.\n                    s_t = s_min\n                elif decay_duration_denominator  0: # Case where T_total = T_warm\n                    s_t = s_target\n                else:\n                    # Standard exponential decay\n                    exponent = (t - T_warm) / decay_duration_denominator\n                    s_t = s_target * (ratio ** exponent)\n\n            # 2. Calculate the unconstrained learning rate\n            # Denominator is 1 - s_t * lambda. This should be > 0.\n            # It is guaranteed by problem constraints and schedule design (s(t) = s_target).\n            eta_unconstrained = s_t / (1.0 - s_t * lambd)\n\n            # 3. Apply the upper-bound constraint\n            eta_t = min(eta_unconstrained, eta_max)\n            case_results.append(eta_t)\n            \n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # We manually format the lists to avoid spaces.\n    inner_lists_str = []\n    for res_list in all_results:\n        # Use a high-precision format for floating point numbers\n        inner_lists_str.append(f\"[{','.join(f'{val:.17g}' for val in res_list)}]\")\n    \n    print(f\"[{','.join(inner_lists_str)}]\")\n\nsolve()\n```", "id": "3142924"}, {"introduction": "现代深度学习模型大量使用像ReLU这样的分段线性激活函数，这导致损失景观在许多地方是“非光滑”的，即存在不可导的“扭结”（kink）。不同的学习率调度策略在这些区域的行为迥异，直接影响模型的收敛性和最终性能。本实践 [@problem_id:3142877] 构建了一个包含绝对值项的合成损失函数来模拟这种非光滑性，并要求你通过实验比较几种经典的学习率调度方案（如固定学习率、步进衰减和余弦退火）在穿越这些“扭结”时的动态表现，从而深化对次梯度噪声和稳定性控制的理解。", "problem": "要求您在一个具有非光滑尖点的分段光滑合成经验风险上，设计并分析不同学习率方案下的随机梯度下降 (SGD) 动态。您的实现必须是一个完整、可运行的程序。目标是量化当迭代值进入非光滑区域时，不同学习率序列 $\\{\\eta_t\\}_{t=0}^{T-1}$ 的行为，并估计一个数据驱动的、单位学习率下的次梯度噪声放大度量。\n\n基本原理与设置：\n- 包含分段光滑绝对值项的经验风险：对于标量参数 $w \\in \\mathbb{R}$，定义\n$$\nL(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left| a_i w - b_i \\right| + \\frac{\\lambda}{2} w^2,\n$$\n其中 $n$ 是数据点的数量，$(a_i,b_i)$ 是固定标量，$\\lambda > 0$ 是一个 $\\ell_2$ 正则化权重。每当 $a_i w - b_i = 0$ 时，绝对值项会产生非光滑的尖点。\n- 使用次梯度的随机梯度下降 (SGD) 更新规则：初始化 $w_0 \\in \\mathbb{R}$，并在每个步骤 $t \\in \\{0,1,\\dots,T-1\\}$，从 $\\{1,\\dots,n\\}$ 中均匀采样一个索引 $I_t$ 并更新\n$$\nw_{t+1} = w_t - \\eta_t \\, g_t,\n$$\n其中随机次梯度为\n$$\ng_t = a_{I_t} \\cdot \\operatorname{sign}(a_{I_t} w_t - b_{I_t}) + \\lambda w_t,\n$$\n约定 $\\operatorname{sign}(0)=0$。步骤 $t$ 的学习率为 $\\eta_t > 0$。\n- 非光滑区域检测：定义残差向量 $r(w) \\in \\mathbb{R}^n$，其分量为 $r_i(w) = a_i w - b_i$。对于一个容差 $\\varepsilon > 0$，如果 $\\min_{1 \\le i \\le n} |r_i(w_t)| \\le \\varepsilon$，我们称迭代值 $w_t$ 处于一个非光滑区域中。\n\n合成数据集与固定常量：\n- 使用 $n = 8$ 个数据点，其值为\n$$\na = [0.5,\\, 1.0,\\, 1.5,\\, 0.8,\\, 1.2,\\, 0.3,\\, 2.0,\\, 0.7], \\quad\nb = [0.6,\\, 1.2,\\, 1.8,\\, 0.9,\\, 1.5,\\, 0.2,\\, 2.3,\\, 0.85].\n$$\n- 使用 $\\lambda = 0.01$。\n- 使用总步数 $T = 4000$。\n- 使用非光滑容差 $\\varepsilon = 0.01$。\n- 初始化 $w_0 = 0$。\n- 使用固定的伪随机数生成器种子 $20231105$ 进行索引采样。在每一步中，从 $\\{1,\\dots,n\\}$ 中均匀采样一个索引。\n\n待测试的方案（测试套件）：\n对每个方案，从 $t=0$ 到 $t=T-1$ 运行上述 SGD 过程，并在每一步应用该方案的 $\\eta_t$。\n\n- 方案 A（恒定小学习率）：对所有 $t$，$\\eta_t = 0.05$。\n- 方案 B（阶梯式衰减）：\n  - 对 $0 \\le t  1200$，$\\eta_t = 0.08$，\n  - 对 $1200 \\le t  2500$，$\\eta_t = 0.02$，\n  - 对 $2500 \\le t  4000$，$\\eta_t = 0.005$。\n- 方案 C（无重启的余弦退火）：设 $\\eta_{\\max} = 0.08$，$\\eta_{\\min} = 0.002$ 和 $T = 4000$，定义\n$$\n\\eta_t = \\eta_{\\min} + \\frac{1}{2}\\left(\\eta_{\\max}-\\eta_{\\min}\\right)\\left(1 + \\cos\\left(\\pi \\frac{t}{T-1}\\right)\\right),\n$$\n对 $t \\in \\{0,\\dots,T-1\\}$。\n- 方案 D（逆时衰减）：设 $\\eta_0 = 0.12$ 和 $\\gamma = 0.0015$，\n$$\n\\eta_t = \\frac{\\eta_0}{1 + \\gamma t}.\n$$\n- 方案 E（恒定大学习率）：对所有 $t$，$\\eta_t = 0.2$。\n\n噪声放大度量：\n- 令 $\\Delta w_t = w_{t+1} - w_t$。在步数集合 $K = \\{ t \\in \\{0,\\dots,T-1\\} : \\min_i |a_i w_t - b_i| \\le \\varepsilon \\}$ 内，通过零截距的最小二乘法估计一个学习率归一化的放大斜率：\n$$\n\\hat{s} = \\underset{s \\in \\mathbb{R}}{\\arg\\min} \\sum_{t \\in K} \\left( |\\Delta w_t| - s \\, \\eta_t \\right)^2.\n$$\n证明其最小化解为\n$$\n\\hat{s} = \\frac{\\sum_{t \\in K} \\eta_t \\, |\\Delta w_t|}{\\sum_{t \\in K} \\eta_t^2},\n$$\n并为每个方案计算此值。如果 $K$ 为空，则定义 $\\hat{s} = 0$。\n\n每个方案的必需输出：\n- 运行 $T$ 步后，计算：\n  1. 最终的经验风险 $L(w_T)$。\n  2. 如上定义的放大斜率 $\\hat{s}$。\n  3. 处于非光滑区域内的步数的整数计数 $|K|$。\n\n最终输出格式：\n- 您的程序必须生成单行输出，其中包含方案 A 到 E 的结果串联，每个方案贡献一个三元组 $[L(w_T), \\hat{s}, |K|]$，并按 A、B、C、D、E 的顺序展平成一个列表。\n- 浮点数表示为四舍五入到六位小数，整数不带小数点。\n- 最终输出必须是单行，形式为用方括号括起来的逗号分隔列表。例如，包含两个方案的输出将类似于 $[0.123456,0.654321,42,0.234567,0.345678,7]$。\n\n角度单位和物理单位：\n- 此问题不涉及物理单位或角度。\n\n您的实现必须是最终答案部分指定的一个完整、可运行的程序。不需要外部输入。所有随机性必须遵循指定的种子。输出必须精确反映此处提供的定义，包括浮点值的舍入规则。", "solution": "用户在机器学习的数值优化领域提供了一个定义明确的计算问题。任务是在一个分段光滑的损失函数上模拟随机梯度下降 (SGD)，并分析在五种不同学习率方案下迭代值的行为。分析的重点是量化当迭代值接近目标函数中的非光滑“尖点”时学习率的影响。\n\n该问题是有效的，因为它在科学上基于非光滑优化理论，是适定的（所有必要的参数和条件都已指定），并且是客观和可形式化的。我们将继续提供完整的解决方案。\n\n解决方案包括三个主要部分：\n1.  噪声放大度量 $\\hat{s}$ 的数学推导。\n2.  为每个学习率方案模拟 SGD 过程的算法流程描述。\n3.  所需输出指标的计算：最终损失 $L(w_T)$、放大斜率 $\\hat{s}$ 以及非光滑区域中的步数计数 $|K|$。\n\n### 放大斜率 $\\hat{s}$ 的推导\n\n问题将放大斜率 $\\hat{s}$ 定义为一个零截距最小二乘问题的解。目标是找到 $s \\in \\mathbb{R}$ 的值，该值能够最小化权重更新的幅度 $|\\Delta w_t|$ 与学习率的线性模型 $s \\eta_t$ 之间的平方误差之和，覆盖所有处于非光滑区域集合 $K$ 中的步数 $t$。要最小化的目标函数是：\n$$\nJ(s) = \\sum_{t \\in K} \\left( |\\Delta w_t| - s \\eta_t \\right)^2\n$$\n为了找到最小化解 $\\hat{s}$，我们计算 $J(s)$ 关于 $s$ 的导数并将其设为零。\n$$\n\\frac{dJ}{ds} = \\frac{d}{ds} \\sum_{t \\in K} \\left( |\\Delta w_t|^2 - 2s\\eta_t|\\Delta w_t| + s^2\\eta_t^2 \\right)\n$$\n由于求和与微分算子是线性的，我们可以交换它们的顺序。项 $|\\Delta w_t|$ 和 $\\eta_t$ 相对于 $s$ 是常数。\n$$\n\\frac{dJ}{ds} = \\sum_{t \\in K} \\frac{d}{ds} \\left( |\\Delta w_t|^2 - 2s\\eta_t|\\Delta w_t| + s^2\\eta_t^2 \\right) = \\sum_{t \\in K} \\left( -2\\eta_t|\\Delta w_t| + 2s\\eta_t^2 \\right)\n$$\n将导数设为零可得到最优值 $\\hat{s}$：\n$$\n\\sum_{t \\in K} \\left( -2\\eta_t|\\Delta w_t| + 2\\hat{s}\\eta_t^2 \\right) = 0\n$$\n$$\n2\\hat{s} \\sum_{t \\in K} \\eta_t^2 = 2 \\sum_{t \\in K} \\eta_t|\\Delta w_t|\n$$\n假设集合 $K$ 非空（否则，求和为零，$\\hat{s}$ 定义为 $0$），我们知道 $\\sum_{t \\in K} \\eta_t^2 > 0$，因为 $\\eta_t > 0$。因此，我们可以解出 $\\hat{s}$：\n$$\n\\hat{s} = \\frac{\\sum_{t \\in K} \\eta_t |\\Delta w_t|}{\\sum_{t \\in K} \\eta_t^2}\n$$\n这证实了问题陈述中给出的公式。二阶导数 $\\frac{d^2J}{ds^2} = \\sum_{t \\in K} 2\\eta_t^2$ 是正的，确认了 $\\hat{s}$ 确实是一个最小化解。\n\n### 算法流程\n\n对于五个学习率方案（A、B、C、D、E）中的每一个，我们执行以下模拟：\n\n1.  **初始化**：\n    *   设置模型参数 $w = w_0 = 0$。\n    *   使用固定的种子 $20231105$ 初始化伪随机数生成器。这对于确保五个方案模拟中的随机采样索引序列 $\\{I_t\\}_{t=0}^{T-1}$ 完全相同至关重要，从而实现公平比较。\n    *   初始化一个空列表 `k_data`，用于存储落入非光滑区域的步数 $t$ 对应的 $(\\eta_t, |\\Delta w_t|)$ 对。\n\n2.  **SGD 迭代**：模拟运行 $T=4000$ 步，从 $t=0$ 到 $t=3999$。在每一步 $t$ 中：\n    *   **学习率**：根据当前方案的公式计算学习率 $\\eta_t$。\n    *   **非光滑区域检查**：计算残差向量 $r(w_t)$，其分量为 $r_i(w_t) = a_i w_t - b_i$。如果 $\\min_{1 \\le i \\le n} |r_i(w_t)| \\le \\varepsilon = 0.01$，则迭代值 $w_t$ 处于非光滑区域。\n    *   **随机次梯度**：从 $\\{1, \\dots, 8\\}$ 中均匀采样一个索引 $I_t$。然后按如下方式计算随机次梯度 $g_t$：\n        $$\n        g_t = a_{I_t} \\cdot \\operatorname{sign}(a_{I_t} w_t - b_{I_t}) + \\lambda w_t\n        $$\n        其中 $\\lambda=0.01$，我们使用约定 $\\operatorname{sign}(0)=0$。\n    *   **参数更新**：根据 SGD 规则更新权重：$w_{t+1} = w_t - \\eta_t g_t$。我们将变化量定义为 $\\Delta w_t = w_{t+1} - w_t = -\\eta_t g_t$。\n    *   **数据收集**：如果 $w_t$ 满足非光滑区域条件，则将对 $(\\eta_t, |\\Delta w_t|)$ 附加到 `k_data` 列表中。\n\n3.  **模拟后分析**：完成所有 $T$ 步后，我们得到最终权重 $w_T$。然后我们计算所需的指标：\n    *   **最终经验风险 $L(w_T)$**：使用以下公式计算：\n        $$\n        L(w_T) = \\frac{1}{n} \\sum_{i=1}^{n} |a_i w_T - b_i| + \\frac{\\lambda}{2} w_T^2\n        $$\n    *   **放大斜率 $\\hat{s}$**：使用收集到的 `k_data` 列表。如果列表为空（即集合 $K$ 为空），则 $\\hat{s}=0$。否则，使用推导出的公式计算 $\\hat{s}$。\n    *   **非光滑区域计数 $|K|$**：这仅仅是 `k_data` 列表中的条目数。\n\n对五个方案中的每一个都重复此整个过程。将得到的三元组 $[L(w_T), \\hat{s}, |K|]$ 收集起来，并格式化为单个展平列表作为最终输出。浮点值四舍五入到六位小数，整数按原样呈现。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the SGD simulations and produce the final output.\n    \"\"\"\n    \n    # --- Synthetic dataset and fixed constants ---\n    A = np.array([0.5, 1.0, 1.5, 0.8, 1.2, 0.3, 2.0, 0.7])\n    B = np.array([0.6, 1.2, 1.8, 0.9, 1.5, 0.2, 2.3, 0.85])\n    LAMBDA = 0.01\n    T = 4000\n    EPSILON = 0.01\n    W0 = 0.0\n    SEED = 20231105\n    N = len(A)\n\n    def get_lr(schedule_name, t, T_total):\n        \"\"\"Calculates the learning rate for a given step and schedule.\"\"\"\n        if schedule_name == 'A':\n            return 0.05\n        elif schedule_name == 'B':\n            if t  1200:\n                return 0.08\n            elif t  2500:\n                return 0.02\n            else:\n                return 0.005\n        elif schedule_name == 'C':\n            eta_max = 0.08\n            eta_min = 0.002\n            return eta_min + 0.5 * (eta_max - eta_min) * (1 + np.cos(np.pi * t / (T_total - 1)))\n        elif schedule_name == 'D':\n            eta0 = 0.12\n            gamma = 0.0015\n            return eta0 / (1 + gamma * t)\n        elif schedule_name == 'E':\n            return 0.2\n        else:\n            raise ValueError(f\"Unknown schedule: {schedule_name}\")\n\n    def run_sgd_simulation(schedule_name):\n        \"\"\"Runs one full SGD simulation for a given learning rate schedule.\"\"\"\n        \n        # Initialize RNG with fixed seed for reproducibility across schedules\n        rng = np.random.default_rng(SEED)\n\n        w = W0\n        k_data = []  # Stores (eta_t, delta_w_t) for t in K\n\n        for t in range(T):\n            # 1. Get learning rate\n            eta_t = get_lr(schedule_name, t, T)\n\n            # 2. Check for nonsmooth region\n            residuals = A * w - B\n            min_abs_residual = np.min(np.abs(residuals))\n            is_in_k = min_abs_residual = EPSILON\n\n            # 3. Sample index and compute stochastic subgradient\n            i_t = rng.integers(0, N)\n            residual_t = A[i_t] * w - B[i_t]\n            # np.sign(0.0) returns 0.0, matching the sign(0)=0 convention.\n            sign_term = np.sign(residual_t)\n            g_t = A[i_t] * sign_term + LAMBDA * w\n            \n            # 4. Compute weight update\n            delta_w_t = -eta_t * g_t\n            w_next = w + delta_w_t\n\n            # 5. Store data for hat_s if in nonsmooth region\n            if is_in_k:\n                k_data.append((eta_t, delta_w_t))\n\n            # 6. Update w for the next iteration\n            w = w_next\n        \n        w_T = w\n        \n        # --- Post-simulation analysis ---\n        \n        # 1. Final empirical risk L(w_T)\n        final_loss = np.mean(np.abs(A * w_T - B)) + (LAMBDA / 2.0) * w_T**2\n        \n        # 2. Amplification slope hat_s\n        if not k_data:\n            hat_s = 0.0\n        else:\n            k_etas = np.array([item[0] for item in k_data])\n            k_delta_ws_abs = np.abs(np.array([item[1] for item in k_data]))\n            \n            numerator = np.sum(k_etas * k_delta_ws_abs)\n            denominator = np.sum(k_etas**2)\n            \n            hat_s = numerator / denominator if denominator > 0 else 0.0\n\n        # 3. Count |K|\n        k_count = len(k_data)\n        \n        return final_loss, hat_s, k_count\n\n    schedules_to_test = ['A', 'B', 'C', 'D', 'E']\n    all_results = []\n    \n    for schedule in schedules_to_test:\n        l_wT, s_hat, k_size = run_sgd_simulation(schedule)\n        all_results.append(f\"{l_wT:.6f}\")\n        all_results.append(f\"{s_hat:.6f}\")\n        all_results.append(str(k_size))\n        \n    # --- Final Output Formatting ---\n    print(f\"[{','.join(all_results)}]\")\n\n# Execute the main function\nsolve()\n```", "id": "3142877"}, {"introduction": "预先设定的学习率衰减曲线虽然简单，但往往无法完美匹配训练过程中复杂的动态变化。更先进的策略是让学习率“响应”模型的实际表现，即根据验证集上的性能自动调整。在本练习 [@problem_id:3142901] 中，你将亲手构建一个复杂的自适应调度器，它通过监测验证损失、利用移动平均线来平滑噪声并识别趋势，在检测到训练停滞时自动降低学习率。这个过程不仅能让你掌握“Plateau”检测的核心技术，还能让你学会量化和分析调度决策中可能出现的“误报”（false positives）。", "problem": "您的任务是设计并测试一种学习率 (LR) 调度策略，该策略由早停 (ES) 式检测器触发，并由验证集移动平均 (MA) 交叉进行验证，同时量化假阳性率。目标是从基本原理出发实现检测器逻辑，并将其应用于合成但科学上合理的验证损失序列。该调度策略应在有证据表明训练停滞或恶化时，在冷却阶段降低学习率。您的程序必须为一小组测试用例输出一个紧凑的摘要。\n\n需要使用并在此基础上构建的基本原理：\n- 基于梯度的学习使用形式为 $x_{t+1} = x_t - \\eta_t g_t$ 的更新，其中 $x_t$ 是在轮次 $t$ 的参数向量，$g_t$ 是梯度估计，$\\eta_t > 0$ 是在轮次 $t$ 的学习率 (LR)。调度策略控制 $\\eta_t$。\n- 早停 (ES) 监控一个验证标准，以检测在“耐心窗口”内是否没有改进。检测器是一个由可观察的验证统计数据驱动的逻辑谓词。\n- 移动平均 (MA) 作为降噪统计量。验证序列 $(y_t)$ 在时间 $t$ 时窗口大小为 $w$ 的简单移动平均 (SMA) 是最后 $w$ 个可用值的均值（如果存在的数值少于 $w$ 个，则使用较短的窗口），即\n$$\n\\operatorname{SMA}_w(t) = \\frac{1}{m}\\sum_{i=t-m+1}^{t} y_i,\\quad m=\\min(w, t+1).\n$$\n\n需要实现的调度定义：\n- 设初始学习率为 $\\eta_0 > 0$。每次调度器触发时，将当前 LR 乘以一个衰减因子 $\\gamma$（其中 $0  \\gamma  1$），并进入一个持续 $c$ 个轮次的冷却阶段。在冷却期间，不允许新的触发。冷却结束后，LR 保持其最近衰减后的值。\n- 维护两个简单移动平均：一个窗口为 $w_s$ 的短期 SMA 和一个窗口为 $w_\\ell$ 的长期 SMA，其中 $w_s  w_\\ell$。将它们表示为 $S_t = \\operatorname{SMA}_{w_s}(t)$ 和 $L_t = \\operatorname{SMA}_{w_\\ell}(t)$。\n- 维护迄今为止观察到的最佳平滑值，$B_t = \\min_{0 \\le i \\le t} S_i$。如果在时间 $t$ 发生改进，即 $S_t  B_{t-1} - \\varepsilon$（其中 $\\varepsilon \\ge 0$ 是一个容忍度），则更新 $B_t = S_t$ 并将轮次 $t$ 记录为最近的改进时间。\n- 定义在时间 $t$ 的早停 (ES) 条件为：在过去的 $p$ 个轮次内没有发生改进，即 $t - \\text{last\\_improve} \\ge p$。\n- 定义在时间 $t$ 的交叉确认条件为：$(S_i - L_i) \\ge \\delta$ 至少在最近的 $k$ 个连续轮次（包括 $t$）为真，其中 $\\delta \\ge 0$ 且整数 $k \\ge 1$。\n- 触发规则：当不处于冷却阶段时，如果在时间 $t$ 同时满足 ES 条件和交叉确认条件，则在轮次 $t$ 开始触发一个冷却，设置 $\\eta \\leftarrow \\gamma \\eta$，并阻止后续触发，直到冷却结束。\n\n假阳性定义：\n- 在轮次 $t_0$ 的一次触发是假阳性，如果在其后的 $q$ 个轮次内 $(t_0+1,\\dots,\\min(T-1, t_0+q))$，存在一个新的最佳原始验证损失，该损失比截至 $t_0$ 观察到的最佳原始验证损失至少严格小 $\\varepsilon$。形式上，设 $y_t$ 为原始验证损失，$b_{\\text{raw}}(t_0) = \\min_{0 \\le i \\le t_0} y_i$。如果满足以下条件，则该触发为假阳性：\n$$\n\\min_{t_0+1 \\le j \\le \\min(T-1, t_0+q)} y_j \\le b_{\\text{raw}}(t_0) - \\varepsilon.\n$$\n\n验证损失生成器：\n- 对于序列长度 $T$，为轮次 $t \\in \\{0,1,\\dots,T-1\\}$ 定义原始验证损失\n$$\ny_t = a + b \\exp\\!\\left(-\\frac{t}{\\tau}\\right) + u \\cdot \\frac{\\max(0,\\, t - t_u)}{T} + \\epsilon_t,\n$$\n其中 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$。为了可复现性，使用一个带有指定整数种子的伪随机数生成器 (PRNG)。带有 $u$ 的项在 $t_u$ 之后引入一个缓和的上升趋势，以模拟过拟合。\n\n任务：\n- 按照规定实现由合成验证损失驱动的调度器和检测器。使用从 $t=0$ 开始的轮次索引。如上所述，在开始阶段使用可用样本计算简单移动平均。当在轮次 $t$ 发生触发时，将该轮次作为长度为 $c$ 的冷却期的第一个轮次。\n- 对下面的每个测试用例，计算：\n    1. 冷却触发的总次数（一个整数）。\n    2. 假阳性的数量（一个整数）。\n    3. 最终的学习率（一个浮点数），从 $\\eta_0$ 开始，每次触发时乘以 $\\gamma$。\n\n测试套件：\n- 所有用例的初始学习率均为 $\\eta_0 = 0.1$。\n\n- 用例 A（理想路径，带有轻微的过拟合尾部）：\n    - 序列参数：$T = 60$, $a = 0.2$, $b = 0.9$, $\\tau = 16$, $\\sigma = 0.008$, $u = 0.12$, $t_u = 45$, seed $= 0$。\n    - 调度参数：$\\gamma = 0.5$, $c = 5$, $p = 5$, $w_s = 3$, $w_\\ell = 9$, $k = 2$, $\\delta = 0.002$, $\\varepsilon = 0.0005$, $q = 4$。\n\n- 用例 B（持续改进，无漂移）：\n    - 序列参数：$T = 40$, $a = 0.1$, $b = 0.8$, $\\tau = 30$, $\\sigma = 0.005$, $u = 0.0$, $t_u = 100$, seed $= 1$。\n    - 调度参数：$\\gamma = 0.5$, $c = 6$, $p = 7$, $w_s = 2$, $w_\\ell = 8$, $k = 3$, $\\delta = 0.001$, $\\varepsilon = 0.0005$, $q = 5$。\n\n- 用例 C（用于探测假阳性的嘈杂平坦区域）：\n    - 序列参数：$T = 50$, $a = 0.5$, $b = 0.0$, $\\tau = 1$, $\\sigma = 0.02$, $u = 0.0$, $t_u = 0$, seed $= 2$。\n    - 调度参数：$\\gamma = 0.5$, $c = 4$, $p = 3$, $w_s = 1$, $w_\\ell = 6$, $k = 1$, $\\delta = 0.0$, $\\varepsilon = 0.0025$, $q = 5$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。将所有用例的结果聚合成一个列表的列表，每个内部列表对应一个用例，顺序为 A、B、C。每个内部列表必须是 $[\\text{num\\_cooldowns},\\text{num\\_false\\_positives},\\text{final\\_lr}]$ 的形式。最终字符串不得包含空格。例如，一个有效的输出形式是\n\"[[1,0,0.05],[0,0,0.1],[3,2,0.0125]]\"。", "solution": "用户要求设计并实现一种基于早停 (ES) 原则和移动平均 (MA) 交叉的学习率 (LR) 调度策略。任务包括在合成的验证损失数据上模拟调度器的行为，并量化其性能，特别是学习率降低的次数和假阳性率。\n\n该解决方案是根据基本原理，通过对一系列轮次 $t \\in \\{0, 1, \\dots, T-1\\}$ 进行离散时间模拟而开发的。\n\n**1. 合成验证损失生成**\n\n对于每个测试用例，会为总共 $T$ 个轮次生成一个原始验证损失序列，表示为 $y_t$。用于生成此数据的函数是：\n$$\ny_t = a + b \\exp\\left(-\\frac{t}{\\tau}\\right) + u \\cdot \\frac{\\max(0, t - t_u)}{T} + \\epsilon_t\n$$\n该模型在科学上是合理的，可以表示模型训练期间的验证损失曲线：\n- 常数项 $a$ 表示不可约误差或渐近损失值。\n- 项 $b \\exp(-t/\\tau)$ 模拟了学习的初始阶段，其中损失以时间常数 $\\tau$ 呈指数下降。\n- 项 $u \\cdot \\frac{\\max(0, t - t_u)}{T}$ 模拟了在轮次 $t_u$ 之后过拟合的开始，为损失引入了一个由因子 $u$ 缩放的线性上升漂移。\n- 项 $\\epsilon_t$ 是一个随机噪声分量，从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中采样，表示小批量梯度下降和验证中固有的随机性。\n\n通过为每个用例的伪随机数生成器 (PRNG) 使用特定的整数种子来确保可复现性。在主调度器模拟开始之前，会预先生成整个序列 $\\{y_t\\}_{t=0}^{T-1}$。\n\n**2. 调度器设计与状态演化**\n\n调度器的逻辑在每个轮次 $t$ 执行。其行为由一组内部状态变量和预定义规则确定。\n\n- **状态变量**:\n    - $\\eta_t$: 在轮次 $t$ 的学习率，初始化为 $\\eta_0$。\n    - 冷却期：一个状态，指示调度器是否暂时不活动。我们跟踪 `cooldown_until`，即冷却期后调度器再次变为活动的第一个轮次。\n    - $B_t$: 迄今观察到的最佳（最低）平滑验证损失。它被初始化为 $B_{-1} = \\infty$。\n    - `last_improve_epoch`: 平滑损失发生最后一次显著改进的轮次 $t$。\n    - 交叉连续计数器：跟踪交叉条件已满足的连续轮次数。\n\n- **移动平均 (SMA)**: 在每个轮次 $t$，计算原始损失 $y_t$ 的两个简单移动平均以平滑噪声：\n    - 短期 SMA: $S_t = \\operatorname{SMA}_{w_s}(t) = \\frac{1}{\\min(w_s, t+1)}\\sum_{i=t-\\min(w_s, t+1)+1}^{t} y_i$\n    - 长期 SMA: $L_t = \\operatorname{SMA}_{w_\\ell}(t) = \\frac{1}{\\min(w_\\ell, t+1)}\\sum_{i=t-\\min(w_\\ell, t+1)+1}^{t} y_i$\n    其中 $w_s  w_\\ell$ 是各自的窗口大小。短期 SMA $S_t$ 对近期变化更敏感，而长期 SMA $L_t$ 反映了更广泛的趋势。\n\n- **模拟循环 (轮次 $t = 0, \\dots, T-1$)**:\n    1. **计算 SMA**: 从历史原始损失数据 $\\{y_i\\}_{i=0}^{t}$ 计算 $S_t$ 和 $L_t$。\n    2. **检查改进**: 如果当前的短期 SMA $S_t$ 显著优于迄今为止的最佳值 $B_{t-1}$，则记录一次改进。条件是 $S_t  B_{t-1} - \\varepsilon$，其中 $\\varepsilon \\ge 0$ 是最小改进阈值。如果满足此条件，则更新状态：$B_t = S_t$ 且 `last_improve_epoch` 设置为 $t$。否则，$B_t = B_{t-1}$。在 $t=0$ 时，假定有改进，因此 $B_0=S_0$ 且 `last_improve_epoch` 设置为 $0$。\n    3. **检查触发**: 只有当调度器不处于冷却期（即 $t \\ge \\text{cooldown\\_until}$）时，才能触发学习率降低。触发需要两个条件同时满足：\n        - **早停 (ES) 条件**: 在指定的轮次数内没有观察到显著改进。如果 $t - \\text{last\\_improve\\_epoch} \\ge p$ 则为真，其中 $p$ 是耐心参数。\n        - **交叉确认条件**: 短期 SMA 持续高于长期 SMA，表明学习趋势可能发生逆转（即损失开始增加）。如果条件 $(S_i - L_i) \\ge \\delta$ 在最后的 $k$ 个连续轮次（从 $t-k+1$ 到 $t$）都成立，则为真，其中 $\\delta \\ge 0$ 是一个容忍度，$k \\ge 1$ 是确认长度。\n    4. **触发动作**: 如果两个条件都满足，则在轮次 $t$ 发生触发事件。学习率按乘法因子 $\\gamma$ 降低（即 $\\eta \\leftarrow \\gamma \\cdot \\eta$），并启动一个为期 $c$ 个轮次的新冷却期。这可以防止在轮次 $t+c$ 之前发生进一步的触发。触发的轮次被记录下来以供后续分析。\n\n**3. 假阳性分析**\n\n在所有 $T$ 个轮次的模拟完成后，将执行事后分析以识别记录的触发中的假阳性。在轮次 $t_0$ 的触发被定义为假阳性，如果尽管调度器决定降低学习率，模型本可以在一个短的 $q$ 个轮次的前瞻窗口内实现一个新的最佳原始验证损失。\n\n形式上，对于每个触发轮次 $t_0$，我们计算：\n- 截至触发时的最佳原始损失：$b_{\\text{raw}}(t_0) = \\min_{0 \\le i \\le t_0} y_i$。\n- 在随后的 $q$ 个轮次中的最小原始损失：$\\min_{t_0+1 \\le j \\le \\min(T-1, t_0+q)} y_j$。\n\n如果满足以下条件，则在 $t_0$ 的触发被计为假阳性：\n$$\n\\min_{t_0+1 \\le j \\le \\min(T-1, t_0+q)} y_j \\le b_{\\text{raw}}(t_0) - \\varepsilon\n$$\n这个条件意味着，在学习率降低后不久，就找到了一个至少比之前好 $\\varepsilon$ 的新原始损失，这表明降低学习率可能为时过早。\n\n最终的实现将这整个逻辑封装在一个函数中，对每个提供的测试用例执行该函数，并汇总结果——总触发次数、假阳性数量和最终学习率。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the simulation.\n    \"\"\"\n\n    def run_case(T, a, b, tau, sigma, u, t_u, seed,\n                 eta_0, gamma, c, p, w_s, w_ell, k, delta, epsilon, q):\n        \"\"\"\n        Runs a single simulation case for the LR scheduler.\n\n        Returns:\n            A list containing [num_triggers, num_false_positives, final_lr].\n        \"\"\"\n        # 1. Generate the full raw validation loss sequence\n        rng = np.random.default_rng(seed)\n        epochs = np.arange(T)\n        noise = rng.normal(0, sigma, T)\n        drift = u * np.maximum(0, epochs - t_u) / T\n        y_raw = a + b * np.exp(-epochs / tau) + drift + noise\n\n        # 2. Initialize scheduler state variables\n        lr = eta_0\n        cooldown_until = 0\n        trigger_epochs = []\n        \n        best_smooth_loss = np.inf\n        last_improve_epoch = 0\n        consecutive_crossover_count = 0\n        \n        # 3. Main simulation loop over epochs\n        for t in range(T):\n            # Compute Short-term and Long-term Simple Moving Averages (SMAs)\n            m_s = min(w_s, t + 1)\n            S_t = np.mean(y_raw[t - m_s + 1 : t + 1])\n\n            m_ell = min(w_ell, t + 1)\n            L_t = np.mean(y_raw[t - m_ell + 1 : t + 1])\n\n            # Check for improvement in smoothed loss\n            if t == 0:\n                best_smooth_loss = S_t\n                last_improve_epoch = 0\n            else:\n                if S_t  best_smooth_loss - epsilon:\n                    best_smooth_loss = S_t\n                    last_improve_epoch = t\n\n            # Check for trigger conditions if not in cooldown\n            if t >= cooldown_until:\n                # Early Stopping condition: patience exceeded\n                es_holds = (t - last_improve_epoch) >= p\n\n                # Crossover confirmation condition\n                if (S_t - L_t) >= delta:\n                    consecutive_crossover_count += 1\n                else:\n                    consecutive_crossover_count = 0\n                \n                crossover_holds = consecutive_crossover_count >= k\n\n                # If both conditions hold, trigger LR reduction\n                if es_holds and crossover_holds:\n                    lr *= gamma\n                    cooldown_until = t + c\n                    trigger_epochs.append(t)\n        \n        num_triggers = len(trigger_epochs)\n\n        # 4. Post-hoc analysis for false positives\n        num_false_positives = 0\n        for t0 in trigger_epochs:\n            best_raw_loss_at_trigger = np.min(y_raw[:t0 + 1])\n            \n            j_start = t0 + 1\n            j_end_inclusive = min(T - 1, t0 + q)\n            \n            # Check if lookahead window is valid\n            if j_start > j_end_inclusive:\n                continue\n                \n            future_losses = y_raw[j_start : j_end_inclusive + 1]\n            min_future_loss = np.min(future_losses)\n\n            if min_future_loss = best_raw_loss_at_trigger - epsilon:\n                num_false_positives += 1\n\n        return [num_triggers, num_false_positives, lr]\n\n    # Define the test suite as specified in the problem\n    test_cases = [\n        # Case A: Happy path with mild overfitting tail\n        dict(\n            T=60, a=0.2, b=0.9, tau=16, sigma=0.008, u=0.12, t_u=45, seed=0,\n            eta_0=0.1, gamma=0.5, c=5, p=5, w_s=3, w_ell=9, k=2, delta=0.002,\n            epsilon=0.0005, q=4\n        ),\n        # Case B: Continued improvement, no drift\n        dict(\n            T=40, a=0.1, b=0.8, tau=30, sigma=0.005, u=0.0, t_u=100, seed=1,\n            eta_0=0.1, gamma=0.5, c=6, p=7, w_s=2, w_ell=8, k=3, delta=0.001,\n            epsilon=0.0005, q=5\n        ),\n        # Case C: Noisy flat region to probe false positives\n        dict(\n            T=50, a=0.5, b=0.0, tau=1, sigma=0.02, u=0.0, t_u=0, seed=2,\n            eta_0=0.1, gamma=0.5, c=4, p=3, w_s=1, w_ell=6, k=1, delta=0.0,\n            epsilon=0.0025, q=5\n        ),\n    ]\n\n    # Run all test cases and collect results\n    results = [run_case(**case) for case in test_cases]\n\n    # Format the final output string as required\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "3142901"}]}