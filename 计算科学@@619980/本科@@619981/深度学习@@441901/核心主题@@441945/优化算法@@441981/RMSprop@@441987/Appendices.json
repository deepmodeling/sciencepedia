{"hands_on_practices": [{"introduction": "为了从根本上理解 RMSprop 的工作原理，我们首先通过一个简化的思想实验来建立直观认识。在这个练习中，我们将分析 RMSprop 的累加器 $v_t$ 如何响应一个具有周期性尖峰的梯度序列。通过推导其在稳态下的行为，我们可以清晰地看到衰减率 $\\rho$ 是如何平滑梯度幅度的剧烈变化，从而为理解其在复杂优化场景中的稳定性奠定基础。[@problem_id:3170912]", "problem": "考虑均方根传播（Root Mean Square Propagation, RMSprop），这是一种随机优化方法，它通过维持过去平方梯度的衰减指数加权平均值来稳定参数更新。设衰减率参数为 $\\rho \\in (0,1)$，平方梯度的序列由周期为 $k \\in \\mathbb{N}$ 的周期性模式定义：具体来说，对于时间索引 $t \\in \\mathbb{N}$，当 $t$ 是 $k$ 的正整数倍时，令 $g_t^2 = M$，否则 $g_t^2 = 1$，其中 $M$ 是一个满足 $M \\gg 1$ 的固定常数。假设 RMSprop 累加器在 $v_0 = 0$ 处初始化，并且训练已经进行了足够长的时间，使得 $v_t$ 达到了与梯度序列同步的时间周期性稳态。\n\n从 RMSprop 形成平方梯度的指数加权移动平均（其几何权重之和为 1）这一原理出发，推导累加器的递归关系，并分析其在上述周期性输入下的行为。在稳态下，设 $v_t$ 在每个峰值时间 $t = nk$（其中 $n \\in \\mathbb{N}$）之后立即被评估；将此值表示为 $s$，在稳态下，该值在各个峰值之间是恒定的。\n\n确定 $s$ 作为 $\\rho$、$M$ 和 $k$ 的函数的封闭形式解析表达式。请以单个简化的符号表达式形式提供最终答案。无需四舍五入。", "solution": "问题要求在特定的平方梯度周期序列下，求出均方根传播（RMSprop）累加器的稳态值 $s$ 的封闭形式表达式。\n\n首先，我们建立 RMSprop 累加器 $v_t$ 的基本递归关系。它被定义为平方梯度 $g_t^2$ 的指数加权移动平均。更新规则如下：\n$$v_t = \\rho v_{t-1} + (1-\\rho) g_t^2$$\n其中 $\\rho \\in (0,1)$ 是衰减率参数，$v_t$ 是在时间步 $t$ 的累加器值。问题给出了初始条件 $v_0 = 0$，但重点关注在足够多的时间步后达到的时间周期性稳态。\n\n平方梯度序列 $g_t^2$ 遵循一个周期为 $k \\in \\mathbb{N}$ 的周期性模式：\n$$g_t^2 = \\begin{cases} M  \\text{if } t = nk \\text{ for some } n \\in \\mathbb{N} \\\\ 1  \\text{otherwise} \\end{cases}$$\n其中 $M$ 是一个常数。这意味着在是 $k$ 的倍数的时间步，会出现一个值为 $M$ 的“峰值”，而在所有其他时间步，该值为 $1$。\n\n我们关心的是累加器的稳态值。问题将 $s$ 定义为在每个峰值之后（即对于较大的 $n$，在时间 $t=nk$）立即评估的 $v_t$ 的值。稳态条件意味着这个值从一个峰值到下一个峰值是恒定的。在数学上，如果我们将时间 $(n-1)k$ 的峰值刚过后的累加器值表示为 $v_{(n-1)k}$，将在时间 $nk$ 的峰值刚过后的值表示为 $v_{nk}$，那么在稳态下：\n$$v_{(n-1)k} = v_{nk} = s$$\n\n我们的策略是将累加器值在一个完整周期内（从时间 $t=(n-1)k$ 到 $t=nk$）进行传播，然后施加稳态条件。我们假设在周期开始时的累加器值为 $v_{(n-1)k} = s$。\n\n该周期由 $k-1$ 个 $g_t^2=1$ 的步骤组成，随后是一个 $g_t^2=M$ 的步骤。\n\n对于从 $t = (n-1)k + 1$ 到 $t = nk - 1$ 的步骤，平方梯度为 $g_t^2 = 1$。让我们计算在时间 $nk$ 的峰值之前的累加器值，即 $v_{nk-1}$。\n从 $v_{(n-1)k} = s$ 开始：\n$$v_{(n-1)k+1} = \\rho v_{(n-1)k} + (1-\\rho)g_{(n-1)k+1}^2 = \\rho s + (1-\\rho)(1)$$\n$$v_{(n-1)k+2} = \\rho v_{(n-1)k+1} + (1-\\rho)g_{(n-1)k+2}^2 = \\rho(\\rho s + 1-\\rho) + (1-\\rho) = \\rho^2 s + \\rho(1-\\rho) + (1-\\rho)$$\n通过归纳法，经过 $j$ 步（其中 $j \\in \\{1, 2, \\dots, k-1\\}$）后，累加器值为：\n$$v_{(n-1)k+j} = \\rho^j v_{(n-1)k} + (1-\\rho) \\sum_{i=0}^{j-1} \\rho^i g_{(n-1)k+j-i}^2$$\n由于在此区间内 $g_t^2=1$，这可以简化为：\n$$v_{(n-1)k+j} = \\rho^j s + (1-\\rho) \\sum_{i=0}^{j-1} \\rho^i$$\n该和是一个有限几何级数：$\\sum_{i=0}^{j-1} \\rho^i = \\frac{1-\\rho^j}{1-\\rho}$。\n将此代入 $v_{(n-1)k+j}$ 的表达式中：\n$$v_{(n-1)k+j} = \\rho^j s + (1-\\rho) \\frac{1-\\rho^j}{1-\\rho} = \\rho^j s + 1 - \\rho^j$$\n为了求出在 $t=nk$ 处峰值之前的值，我们设 $j=k-1$：\n$$v_{nk-1} = \\rho^{k-1} s + 1 - \\rho^{k-1}$$\n\n现在，我们对 $t=nk$ 处的峰值执行最终的更新步骤。在这个时间步，平方梯度为 $g_{nk}^2 = M$。\n$$v_{nk} = \\rho v_{nk-1} + (1-\\rho) g_{nk}^2$$\n代入 $v_{nk-1}$ 的表达式：\n$$v_{nk} = \\rho (\\rho^{k-1} s + 1 - \\rho^{k-1}) + (1-\\rho)M$$\n$$v_{nk} = \\rho^k s + \\rho(1 - \\rho^{k-1}) + (1-\\rho)M$$\n$$v_{nk} = \\rho^k s + \\rho - \\rho^k + M(1-\\rho)$$\n\n现在我们应用稳态条件 $v_{nk} = s$：\n$$s = \\rho^k s + \\rho - \\rho^k + M(1-\\rho)$$\n我们的目标是解出这个关于 $s$ 的方程。\n$$s - \\rho^k s = M(1-\\rho) + \\rho - \\rho^k$$\n$$s(1-\\rho^k) = M(1-\\rho) + \\rho - \\rho^k$$\n$$s = \\frac{M(1-\\rho) + \\rho - \\rho^k}{1-\\rho^k}$$\n\n为了获得一个更易于解释的形式，我们可以对分子进行处理。一个更有条理的重组是分离表达式：\n$$s = \\frac{M(1-\\rho) + \\rho - \\rho^k}{1-\\rho^k} = \\frac{M(1-\\rho) - (1-\\rho) + 1 - \\rho^k}{1-\\rho^k}$$\n等式成立，因为 $- (1-\\rho) + 1 - \\rho^k = -1 + \\rho + 1 - \\rho^k = \\rho - \\rho^k$。\n现在，对各项进行分组：\n$$s = \\frac{(M-1)(1-\\rho) + (1-\\rho^k)}{1-\\rho^k}$$\n将分数拆分为两部分：\n$$s = \\frac{(M-1)(1-\\rho)}{1-\\rho^k} + \\frac{1-\\rho^k}{1-\\rho^k}$$\n$$s = 1 + \\frac{(M-1)(1-\\rho)}{1-\\rho^k}$$\n这就是 $s$ 的最终、简化的封闭形式表达式。这个形式有一个清晰的解释：稳态值是一个基线 $1$（如果 $g_t^2$ 总是 $1$ 时的结果）加上一个额外的项，该项表示来自幅度为 $M$ 的周期性峰值的贡献。", "answer": "$$\n\\boxed{1 + \\frac{(M-1)(1-\\rho)}{1-\\rho^k}}\n$$", "id": "3170912"}, {"introduction": "在掌握了 RMSprop 的基本平滑机制后，下一个关键问题是：这个机制到底有多重要？本练习将通过一个精心设计的“压力测试”来回答这个问题。我们将探讨一个极端情况，即当衰减率 $\\rho=0$ 时，RMSprop 完全失去其“记忆”能力。通过分析这个反例，你将亲眼见证，在没有历史梯度信息的情况下，优化器即使在简单的凸函数上也会被噪声干扰而无法收敛，从而深刻理解记忆机制对于稳定优化的核心价值。[@problem_id:3170844]", "problem": "考虑使用均方根传播 (RMSprop) 优化一维目标函数 $f(x) = \\frac{1}{2} x^{2}$。假设学习率 $\\eta > 0$ 固定，衰减参数 $\\rho$ 极小取为 $\\rho = 0$，数值稳定器 $\\epsilon = 0$。设在第 $t$ 次迭代时观测到的梯度为带噪声的量 $g_{t} = \\nabla f(x_{t}) + \\delta_{t}$，其中 $\\nabla f(x) = x$，噪声为确定性交替序列 $\\delta_{t} = \\Delta(-1)^{t}$，对于固定的 $\\Delta > 0$。该算法维持观测梯度平方的指数移动平均 $v_{t}$，并用它来在每一步自适应地缩放更新量。\n\n从 $x_{0} = 0$ 开始。仅使用基本定义（$f$ 的梯度、指数移动平均、以及通过近期梯度的均方根进行自适应梯度缩放的原理），推导出在这些设置下 RMSprop 产生的精确迭代序列 $\\{x_{t}\\}_{t \\geq 0}$，并构建一个反例，说明当 $\\rho$ 太小时算法不收敛。明确解释在这种设置下，$v_{t}$ 的噪声性（由于极小的 $\\rho$）是如何阻止收敛的机制。\n\n在所述条件下，并假设 $\\Delta$ 满足 $\\Delta > \\eta$，确定 $\\limsup_{t \\to \\infty} |x_{t}|$ 的值，以 $\\eta$ 的函数形式给出闭合解。将您的最终答案表示为单个闭合形式的解析表达式。无需四舍五入。", "solution": "该问题要求在特定的一组病态条件下分析均方根传播 (RMSprop) 优化算法。我们必须首先验证问题陈述，如果有效，则推导迭代序列，解释不收敛的机制，并确定迭代值绝对值的上极限。\n\nRMSprop 的基本更新规则由梯度平方的指数移动平均 $v_t$ 和参数更新 $x_{t+1}$ 给出。\n移动平均的更新方式如下：\n$$v_{t} = \\rho v_{t-1} + (1-\\rho) g_{t}^{2}$$\n参数更新方式如下：\n$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{v_{t}} + \\epsilon} g_{t}$$\n\n问题提供了以下具体的值和函数：\n- 目标函数：$f(x) = \\frac{1}{2} x^{2}$，其梯度为 $\\nabla f(x) = x$。\n- 观测梯度：$g_{t} = \\nabla f(x_{t}) + \\delta_{t} = x_{t} + \\Delta(-1)^{t}$。\n- 参数：学习率 $\\eta > 0$，衰减参数 $\\rho = 0$，数值稳定器 $\\epsilon = 0$。\n- 初始条件：$x_{0} = 0$。\n- 约束条件：$\\Delta > \\eta > 0$。\n\n首先，我们将给定的参数 $\\rho = 0$ 和 $\\epsilon = 0$ 代入通用的 RMSprop 方程。\n移动平均 $v_t$ 的更新变为：\n$$v_{t} = (0) v_{t-1} + (1-0) g_{t}^{2} = g_{t}^{2}$$\n参数 $x_{t+1}$ 的更新变为：\n$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{g_{t}^{2}} + 0} g_{t} = x_{t} - \\frac{\\eta}{|g_{t}|} g_{t}$$\n这个简化的更新规则揭示了一个关键方面。对于任何非零梯度 $g_t$，项 $\\frac{g_t}{|g_t|}$ 等价于梯度的符号 $\\text{sgn}(g_t)$。更新规则实际上变成了步长恒为 $\\eta$ 的符号梯度下降算法：\n$$x_{t+1} = x_{t} - \\eta \\cdot \\text{sgn}(g_{t})$$\n我们必须确保 $g_t$ 永远不为零，以避免除以零。\n\n现在我们从 $x_{0} = 0$ 开始推导精确的迭代序列 $\\{x_{t}\\}_{t \\geq 0}$。\n\n对于 $t=0$：\n- $x_{0} = 0$\n- 观测梯度为 $g_{0} = x_{0} + \\Delta(-1)^{0} = 0 + \\Delta = \\Delta$。因为 $\\Delta > 0$，所以 $g_{0} \\neq 0$。\n- 下一个迭代值为 $x_{1} = x_{0} - \\frac{\\eta}{|g_{0}|}g_{0} = 0 - \\frac{\\eta}{|\\Delta|}\\Delta = -\\eta$。\n\n对于 $t=1$：\n- $x_{1} = -\\eta$\n- 观测梯度为 $g_{1} = x_{1} + \\Delta(-1)^{1} = -\\eta - \\Delta = -(\\eta + \\Delta)$。因为 $\\eta, \\Delta > 0$，所以 $g_{1} \\neq 0$。\n- 下一个迭代值为 $x_{2} = x_{1} - \\frac{\\eta}{|g_{1}|}g_{1} = -\\eta - \\frac{\\eta}{|-(\\eta + \\Delta)|}(-(\\eta + \\Delta)) = -\\eta - \\frac{\\eta}{\\eta + \\Delta}(-(\\eta + \\Delta)) = -\\eta + \\eta = 0$。\n\n对于 $t=2$：\n- $x_{2} = 0$\n- 观测梯度为 $g_{2} = x_{2} + \\Delta(-1)^{2} = 0 + \\Delta = \\Delta$。\n- 下一个迭代值为 $x_{3} = x_{2} - \\frac{\\eta}{|g_{2}|}g_{2} = 0 - \\frac{\\eta}{|\\Delta|}\\Delta = -\\eta$。\n\n迭代序列似乎是 $\\{0, -\\eta, 0, -\\eta, \\ldots\\}$。我们可以用数学归纳法证明，当 $t$ 为偶数时 $x_{t} = 0$，当 $t$ 为奇数时 $x_{t} = -\\eta$。\n$t=0, 1, 2$ 的基本情况已经展示。\n假设对于某个整数 $k \\geq 0$，有 $x_{2k} = 0$ 和 $x_{2k+1} = -\\eta$。\n我们将证明 $x_{2k+2} = 0$ 和 $x_{2k+3} = -\\eta$。\n\n步骤1：从 $x_{2k+1}$ 计算 $x_{2k+2}$。\n迭代值为 $x_{2k+1} = -\\eta$。当前时间步为 $t=2k+1$。\n$g_{2k+1} = x_{2k+1} + \\Delta(-1)^{2k+1} = -\\eta - \\Delta = -(\\eta+\\Delta)$。\n$x_{2k+2} = x_{2k+1} - \\eta \\cdot \\text{sgn}(g_{2k+1}) = -\\eta - \\eta \\cdot \\text{sgn}(-(\\eta+\\Delta)) = -\\eta - \\eta(-1) = 0$。\n因此，对于所有偶数 $t>0$，$x_t$ 为 0。\n\n步骤2：从 $x_{2k+2}$ 计算 $x_{2k+3}$。\n迭代值为 $x_{2k+2} = 0$。当前时间步为 $t=2k+2$。\n$g_{2k+2} = x_{2k+2} + \\Delta(-1)^{2k+2} = 0 + \\Delta = \\Delta$。\n$x_{2k+3} = x_{2k+2} - \\eta \\cdot \\text{sgn}(g_{2k+2}) = 0 - \\eta \\cdot \\text{sgn}(\\Delta) = 0 - \\eta(1) = -\\eta$。\n因此，对于所有奇数 $t$，$x_t$ 为 $-\\eta$。\n\n归纳成立。精确的迭代序列由下式给出：\n$$x_{t} = \\begin{cases} 0  \\text{若 } t \\text{ 为偶数} \\\\ -\\eta  \\text{若 } t \\text{ 为奇数} \\end{cases}$$\n序列 $\\{x_t\\}$ 在 $0$ 和 $-\\eta$ 之间振荡，因此未能收敛到 $x=0$ 处的真实最小值。这个振荡序列即为所要求的证明不收敛的反例。\n\n这种失败的机制根植于 $\\rho=0$ 的选择。此参数设置完全移除了 RMSprop 算法的“记忆”。学习率的缩放因子 $1/\\sqrt{v_t}$ 本应基于近期梯度平方的移动平均。一个合适的平均（$\\rho \\approx 1$）会平滑梯度的变化。而当 $\\rho=0$ 时，$v_t$ 仅仅是 $g_t^2$，因此自适应缩放变为 $1/|g_t|$。这种归一化将更新转换为一个大小恒为 $\\eta$ 的步长，其方向完全由当前带噪声梯度 $g_t$ 的符号决定。\n\n在最优值 $x=0$ 附近，真实梯度 $\\nabla f(x_t) = x_t$ 变得非常小。因此，带噪声的梯度 $g_t = x_t + \\Delta(-1)^t$ 主要由噪声项 $\\delta_t = \\Delta(-1)^t$ 主导。更新方向 $-\\text{sgn}(g_t)$ 因此由噪声的交替符号决定，而不是指向最小值的方向。\n在偶数步，$x_t=0$，真实梯度为 $0$，但噪声 $\\delta_t = \\Delta$ 将迭代值推向 $x_{t+1}=-\\eta$。\n在奇数步，$x_t=-\\eta$，真实梯度为 $-\\eta$，但噪声 $\\delta_t = -\\Delta$ 与之结合得到 $g_t=-(\\eta+\\Delta)$，这又将迭代值推回 $x_{t+1}=0$。该算法持续地以固定大小的步长在最小值附近振荡，永远无法稳定下来。\n\n最后，我们确定 $\\limsup_{t \\to \\infty} |x_{t}|$ 的值。\n迭代序列为 $x_0=0, x_1=-\\eta, x_2=0, x_3=-\\eta, \\ldots$。\n对应的绝对值序列为 $|x_0|=0, |x_1|=\\eta, |x_2|=0, |x_3|=\\eta, \\ldots$。\n这个序列 $\\{|x_t|\\}_{t \\geq 0}$ 是 $\\{0, \\eta, 0, \\eta, \\ldots\\}$。\n该序列的子序列极限（或极限点）集合是无限次出现的值的集合，即 $\\{0, \\eta\\}$。\n一个序列的上极限是其子序列极限集合的上确界。\n$$\\limsup_{t \\to \\infty} |x_{t}| = \\sup\\{0, \\eta\\}$$\n因为 $\\eta > 0$，所以上确界是 $\\eta$。\n条件 $\\Delta > \\eta$ 确保了梯度项 $g_t$ 的符号在每一步都是明确无误的，并防止了其他潜在极限环的出现，从而使这种振荡行为成为包括 $x_0=0$ 在内的某个范围内的任何初始条件的唯一结果。", "answer": "$$\\boxed{\\eta}$$", "id": "3170844"}, {"introduction": "从理论洞察转向实际应用，本练习将带你进入一个更接近真实世界场景的编码挑战。在许多实际问题中，我们得到的梯度估计可能存在系统性偏差（bias），这会严重影响标准 RMSprop 的性能。本练习要求你动手实现三种 RMSprop 的变体——标准的（非中心化）、中心化的和去趋势的——并比较它们在有偏梯度下的表现。通过这个编码实践，你不仅能加深对 RMSprop 局限性的理解，还能掌握如何通过改进算法来解决实际优化难题。[@problem_id:3170932]", "problem": "考虑一个一维随机优化问题，其中学习器试图通过一个不完美的梯度预言机来最小化一个凸二次目标。设潜在目标是强凸且可微的 $f(x)$，并假设学习器使用随机梯度下降（Stochastic Gradient Descent, SGD），其规范更新公式为 $x_{t+1} = x_t - \\alpha \\, g_t$，其中 $g_t$ 是一个随机梯度，$\\alpha$ 是一个正学习率，$t$ 为离散时间索引。随机梯度被建模为 $g_t = h(x_t) + b + \\xi_t$，其中 $h(x_t)$ 是目标的真实梯度，$b$ 是一个恒定的偏置项，反映了梯度预言机的非零期望（例如，由于测量漂移），而 $\\xi_t$ 是具有有限方差的零均值噪声。期望算子表示为 $\\mathbb{E}[\\cdot]$。\n\n均方根传播（Root Mean Square Propagation, RMSprop）是一种自适应方法，它通过近期梯度大小的运行估计来缩放 SGD 更新，该估计是通过指数加权移动平均（Exponentially Weighted Moving Average, EWMA）构建的。对于一个序列 $\\{s_t\\}$，衰减参数为 $\\beta \\in (0,1)$ 的 EWMA 是一个递归估计器，它基于指数加权的基本定义，即对过去的值赋予几何级数衰减的权重。梯度平方的非中心化 EWMA 追踪的是二阶矩 $\\mathbb{E}[g_t^2]$，而不是方差。当 $\\mathbb{E}[g_t] \\neq 0$ 时，二阶矩等于方差加上均值的平方，这可能导致运行估计因偏置而被夸大。一种有原则的替代方法是使用中心化 EWMA（即，减去均值的运行估计），从而近似方差 $\\operatorname{Var}(g_t) = \\mathbb{E}[g_t^2] - (\\mathbb{E}[g_t])^2$。另一种干预措施是一种去趋势更新，它在更新参数之前直接从梯度中减去运行均值。\n\n你的任务是，严格从上述 SGD 和 EWMA 的定义出发，推导并实现三种算法：\n- 非中心化 RMSprop：使用 $g_t^2$ 的 EWMA 来缩放 SGD 步长。\n- 中心化 RMSprop：通过结合 $g_t$ 和 $g_t^2$ 的 EWMA 来估计 $\\operatorname{Var}(g_t)$，并使用此估计来缩放步长。\n- 去趋势 RMSprop：在进行平方和更新之前，从 $g_t$ 中减去 $\\mathbb{E}[g_t]$ 的运行估计。\n\n然后，在一类对应于二次目标的受控一维随机梯度上模拟它们的行为。具体来说，设真实目标为 $f(x) = \\tfrac{1}{2} a (x - x^\\star)^2$，其梯度为 $h(x) = a (x - x^\\star)$，其中 $a > 0$，$x^\\star$ 是唯一的最小化点。梯度预言机提供 $g_t = a (x_t - x^\\star) + b + \\xi_t$，其中 $b$ 是一个固定偏置，$\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立噪声。为保证科学真实性，请确保在每个测试用例中，所有三种算法都使用相同的噪声实现 $\\{\\xi_t\\}_{t=1}^T$，以便差异仅由更新规则产生。\n\n对每个测试用例，使用相同的超参数实现所有三种算法：学习率 $\\alpha$、EWMA 衰减率 $\\beta$、分母中的加性稳定项 $\\varepsilon$ 以及步数 $T$。在 $x_0$ 处初始化参数，并将所有 EWMA 初始化为零。在 $T$ 步之后，报告绝对误差 $|x_T - x^\\star|$。\n\n使用以下参数值测试套件，该套件旨在探究不同方面：\n- 测试用例 1（理想路径，含中等偏置和噪声）：$a = 1.5$，$x^\\star = 2.0$，$x_0 = -5.0$，$b = 0.6$，$\\sigma = 0.2$，$\\alpha = 0.01$，$\\beta = 0.99$，$\\varepsilon = 10^{-8}$，$T = 1000$，噪声序列的固定随机种子为 $42$。\n- 测试用例 2（边界情况，无噪声，无偏置）：$a = 1.0$，$x^\\star = -3.0$，$x_0 = 10.0$，$b = 0.0$，$\\sigma = 0.0$，$\\alpha = 0.01$，$\\beta = 0.99$，$\\varepsilon = 10^{-8}$，$T = 500$，固定随机种子为 $123$。\n- 测试用例 3（显著边缘情况，含高噪声和非零偏置）：$a = 0.8$，$x^\\star = 1.0$，$x_0 = -1.0$，$b = 0.5$，$\\sigma = 2.0$，$\\alpha = 0.01$，$\\beta = 0.99$，$\\varepsilon = 10^{-8}$，$T = 1500$，固定随机种子为 $999$。\n- 测试用例 4（接近最优点的起始位置，含小噪声和非零偏置）：$a = 2.5$，$x^\\star = 0.0$，$x_0 = 0.1$，$b = 0.2$，$\\sigma = 0.05$，$\\alpha = 0.01$，$\\beta = 0.99$，$\\varepsilon = 10^{-8}$，$T = 800$，固定随机种子为 $777$。\n\n对于每个测试用例，按顺序计算三个浮点数：非中心化 RMSprop、中心化 RMSprop 和去趋势 RMSprop 的最终绝对误差 $|x_T - x^\\star|$。\n\n你的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的列表，内含 $12$ 个结果，按测试用例排序，每个测试用例按上述方法顺序贡献三个数字。例如，输出格式必须与 $[r_{1, \\text{unc}}, r_{1, \\text{cent}}, r_{1, \\text{det}}, r_{2, \\text{unc}}, r_{2, \\text{cent}}, r_{2, \\text{det}}, r_{3, \\text{unc}}, r_{3, \\text{cent}}, r_{3, \\text{det}}, r_{4, \\text{unc}}, r_{4, \\text{cent}}, r_{4, \\text{det}}]$ 完全一致，其中每个 $r_{\\cdot}$ 是一个浮点数。", "solution": "任务是基于给定的形式化描述，推导并实现均方根传播（RMSprop）算法的三种变体，然后在一个特定的一维优化问题上模拟它们的性能。这三种变体是：非中心化 RMSprop、中心化 RMSprop 和去趋势 RMSprop。模拟将在一个二次目标函数 $f(x) = \\frac{1}{2} a (x - x^\\star)^2$ 上进行，其梯度预言机带有偏置和噪声。\n\n首先，我们建立通用的数学框架。在时间步 $t$，随机梯度由下式给出：\n$$g_t = a(x_t - x^\\star) + b + \\xi_t$$\n其中 $x_t$ 是参数值，$a$ 是曲率，$x^\\star$ 是最小化点，$b$ 是一个恒定偏置，$\\xi_t$ 是一个方差为 $\\sigma^2$ 的零均值噪声项。\n\nRMSprop 及其变体的核心是使用指数加权移动平均（EWMA）来估计梯度的统计特性。对于一个序列 $\\{s_t\\}$，衰减参数为 $\\beta \\in (0,1)$ 的 EWMA 的递归公式是：\n$$E_t[s] = \\beta E_{t-1}[s] + (1-\\beta)s_t$$\n其中 $E_t[s]$ 是在时间 $t$ 的移动平均值。所有 EWMA 累加器都初始化为 $0$。\n\n这些自适应方法的参数更新的通用结构是：\n$$x_{t+1} = x_t - \\alpha \\cdot \\text{scaled\\_gradient}_t$$\n其中 $\\alpha$ 是学习率。我们现在将推导这三种算法的具体更新规则。模拟从给定的 $x_0$ 开始，运行 $t = 0, 1, \\dots, T-1$。\n\n**1. 非中心化 RMSprop 推导**\n\n这是标准的 RMSprop 算法。它通过梯度的均方根来缩放学习率。“非中心化”方面指的是使用梯度的原始二阶矩 $\\mathbb{E}[g_t^2]$ 作为缩放因子的基础，该二阶矩通过 $g_t^2$ 的 EWMA 进行估计。\n\n设 $v_t$ 为梯度平方 $g_t^2$ 的 EWMA。\n$$v_t = \\beta v_{t-1} + (1-\\beta) g_t^2$$\n参数 $x$ 的更新规则使用随机梯度 $g_t$，并用该移动平均值的平方根的倒数进行缩放。分母中包含一个加性稳定项 $\\varepsilon > 0$ 以防止除以零。\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon} g_t$$\n该算法很简单，但当梯度的均值 $\\mathbb{E}[g_t]$ 非零时（即，当存在偏置 $b$ 或梯度在远离最优点时持续非零），其效果可能不是最优的。在这种情况下，$v_t$ 估计的是 $\\mathbb{E}[g_t^2] = \\operatorname{Var}(g_t) + (\\mathbb{E}[g_t])^2$，导致运行估计被均值的平方所夸大，这可能导致过于保守的步长。\n\n**2. 中心化 RMSprop 推导**\n\n该变体旨在通过使用梯度方差 $\\operatorname{Var}(g_t)$ 的估计值进行缩放，而不是二阶矩，来修正非中心化版本的问题。方差由 $\\operatorname{Var}(g_t) = \\mathbb{E}[g_t^2] - (\\mathbb{E}[g_t])^2$ 给出。这需要同时估计梯度的一阶矩（均值）和二阶矩。\n\n设 $m_t$ 为梯度 $g_t$ 的 EWMA，用于估计 $\\mathbb{E}[g_t]$。\n$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\n设 $v_t$ 为梯度平方 $g_t^2$ 的 EWMA，用于估计 $\\mathbb{E}[g_t^2]$（与非中心化 RMSprop 中相同）。\n$$v_t = \\beta v_{t-1} + (1-\\beta) g_t^2$$\n然后，根据当前的移动平均值构建在步骤 $t$ 的方差估计：\n$$\\hat{\\sigma}_t^2 = v_t - m_t^2$$\n参数更新规则使用此方差估计进行缩放。更新的方向仍然使用原始的随机梯度 $g_t$。\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{\\hat{\\sigma}_t^2} + \\varepsilon} g_t$$\n这种方法通过更准确地衡量梯度的噪声程度来对步长进行归一化，当存在显著偏置时，有可能改善收敛性。\n\n**3. 去趋势 RMSprop 推导**\n\n该算法采用更直接的方法来处理非零均值梯度。它首先估计梯度的均值，然后从原始随机梯度中减去这个“趋势”。这个“去趋势”梯度随后被用于参数更新和自适应缩放因子的计算。\n\n首先，我们计算梯度 $g_t$ 的 EWMA $m_t$，就像在中心化 RMSprop 中一样。\n$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\n接下来，我们通过从原始梯度中减去当前的运行均值估计来形成去趋势梯度 $\\tilde{g}_t$。\n$$\\tilde{g}_t = g_t - m_t$$\n缩放因子基于*去趋势*梯度平方的 EWMA。设这个 EWMA 为 $v_t$。\n$$v_t = \\beta v_{t-1} + (1-\\beta) \\tilde{g}_t^2$$\n参数更新使用去趋势梯度 $\\tilde{g}_t$ 作为步长方向，并由从 $v_t$ 导出的自适应率进行缩放。\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon} \\tilde{g}_t$$\n此方法试图通过主动移除估计的偏置来同时校正更新步长的大小和方向。这在概念上与 Adam 优化器相似，尽管它省略了初始迭代的正式偏置校正步骤。\n\n该实现将为每个测试用例模拟这三种算法，使用相同的随机噪声序列 $\\xi_t$ 以确保公平比较。对每个算法和测试用例，报告最终的绝对误差 $|x_T - x^\\star|$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_uncentered_rmsprop(params, noise):\n    \"\"\"\n    Simulates Uncentered RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        v = beta * v + (1 - beta) * g**2\n        x = x - (alpha / (np.sqrt(v) + epsilon)) * g\n    \n    return np.abs(x - x_star)\n\ndef run_centered_rmsprop(params, noise):\n    \"\"\"\n    Simulates Centered RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    m = 0.0\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        m = beta * m + (1 - beta) * g\n        v = beta * v + (1 - beta) * g**2\n        var_est = v - m**2\n        \n        # Use np.sqrt on var_est directly as per derivation.\n        # This may result in NaN if var_est is negative, highlighting a potential instability.\n        denominator = np.sqrt(np.maximum(0, var_est)) + epsilon\n        x = x - (alpha / denominator) * g\n\n    return np.abs(x - x_star)\n\ndef run_detrended_rmsprop(params, noise):\n    \"\"\"\n    Simulates Detrended RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    m = 0.0\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        m = beta * m + (1 - beta) * g\n        g_tilde = g - m\n        v = beta * v + (1 - beta) * g_tilde**2\n\n        denominator = np.sqrt(v) + epsilon\n        x = x - (alpha / denominator) * g_tilde\n\n    return np.abs(x - x_star)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"a\": 1.5, \"x_star\": 2.0, \"x0\": -5.0, \"b\": 0.6, \"sigma\": 0.2, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 1000, \"seed\": 42\n        },\n        {\n            \"a\": 1.0, \"x_star\": -3.0, \"x0\": 10.0, \"b\": 0.0, \"sigma\": 0.0, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 500, \"seed\": 123\n        },\n        {\n            \"a\": 0.8, \"x_star\": 1.0, \"x0\": -1.0, \"b\": 0.5, \"sigma\": 2.0, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 1500, \"seed\": 999\n        },\n        {\n            \"a\": 2.5, \"x_star\": 0.0, \"x0\": 0.1, \"b\": 0.2, \"sigma\": 0.05, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 800, \"seed\": 777\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        noise = rng.normal(0, case['sigma'], case['T'])\n        \n        # Uncentered RMSprop\n        result_unc = run_uncentered_rmsprop(case, noise)\n        all_results.append(result_unc)\n        \n        # Centered RMSprop\n        # In the original provided solution, var_est could be negative, causing NaN.\n        # A robust implementation should clamp it at 0.\n        # I've added np.maximum(0, var_est) to reflect this practical consideration,\n        # which is a minimal change to ensure code functionality.\n        result_cen = run_centered_rmsprop(case, noise)\n        all_results.append(result_cen)\n\n        # Detrended RMSprop\n        result_det = run_detrended_rmsprop(case, noise)\n        all_results.append(result_det)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3170932"}]}