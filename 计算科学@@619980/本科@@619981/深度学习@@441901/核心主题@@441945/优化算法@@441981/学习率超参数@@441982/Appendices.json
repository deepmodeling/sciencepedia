{"hands_on_practices": [{"introduction": "学习率是决定梯度下降法能否稳定收敛的关键。对于任何给定的损失函数，学习率都存在一个理论上限，超过该上限会导致优化过程发散。本练习将通过一个二次损失函数的简化模型，引导你从第一性原理出发，推导稳定学习率的上限与损失曲率（由Hessian矩阵的最大特征值决定）之间的关系，并通过编程仿真来验证你的理论发现，从而建立起对学习率稳定边界的直观理解。[@problem_id:3187300]", "problem": "考虑在深度学习中最小化一个二次可微的二次目标函数，其中参数向量表示为 $\\theta \\in \\mathbb{R}^d$，损失函数为 $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$，其中 $A \\in \\mathbb{R}^{d \\times d}$ 是一个对称正定矩阵。梯度下降（GD）的更新由迭代映射 $\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)$ 定义，学习率为 $\\eta0$。矩阵的谱半径（SR）定义为 $\\rho(M)=\\max_i |\\lambda_i(M)|$，其中 $\\lambda_i(M)$ 是 $M$ 的特征值。从这些基本定义出发，推导学习率 $\\eta$ 使 GD 迭代渐近稳定（即，对于任何初始 $\\theta_0$，都有 $\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$）的充要条件，并用 $A$ 的谱半径表示最大的渐近稳定学习率 $\\eta$。然后，通过沿 $A$ 的特征向量方向模拟 GD 来数值验证此稳定性条件。\n\n您的程序必须：\n- 对于每个测试用例矩阵 $A$，计算谱半径 $\\rho(A)$ 和预测的最大渐近稳定学习率，记为 $\\eta_{\\text{max}}$。\n- 使用 $A$ 的特征向量，模拟 GD 更新 $\\theta_{t+1}=\\theta_t-\\eta A\\theta_t$ 共 $T$ 步，初始条件选择为单个特征向量方向，具体为对于 $A$ 的每个特征向量 $v_i$，设置 $\\theta_0=v_i$。\n- 对于每个测试用例的三个候选学习率：一个严格低于预测阈值的值（$\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$），一个位于阈值处的值（$\\eta_{\\text{at}}=\\eta_{\\text{max}}$），以及一个严格高于阈值的值（$\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$），报告 GD 迭代是否沿所有特征向量方向收敛到原点，即当模拟 $T$ 步时，对于每个特征向量方向，是否都满足 $\\|\\theta_T\\|\\le \\varepsilon$。\n- 角度单位说明：测试定义中出现的所有角度均以弧度为单位。\n\n使用以下测试套件：\n1. 矩阵 $A_1=\\operatorname{diag}(1.0,4.0)$，因此 $d=2$。\n2. 矩阵 $A_2=Q_2 \\operatorname{diag}(0.5,1.5,3.0) Q_2^\\top$，旋转矩阵为 $Q_2=\\begin{bmatrix}\\cos(\\pi/4)-\\sin(\\pi/4)0\\\\ \\sin(\\pi/4)\\cos(\\pi/4)0\\\\ 001\\end{bmatrix}$，因此 $d=3$。\n3. 矩阵 $A_3=Q_3 \\operatorname{diag}(0.1,2.0,10.0,20.0) Q_3^\\top$，其中 $Q_3$ 是 $\\mathbb{R}^4$ 中两个分块旋转的乘积：在 $(0,1)$-平面中旋转 $\\phi=\\pi/6$，在 $(2,3)$-平面中旋转 $\\psi=\\pi/3$（两个角度均以弧度为单位），因此 $d=4$。\n\n模拟参数：\n- GD 步数 $T=200$。\n- 收敛容差 $\\varepsilon=10^{-6}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每个测试用例 $i\\in\\{1,2,3\\}$，按顺序附加以下四个值：\n- 预测的 $\\eta_{\\text{max}}^{(i)}$，作为一个浮点数，\n- 在 $\\eta_{\\text{below}}^{(i)}$ 处的稳定性的布尔值，\n- 在 $\\eta_{\\text{at}}^{(i)}$ 处的稳定性的布尔值，\n- 在 $\\eta_{\\text{above}}^{(i)}$ 处的稳定性的布尔值。\n\n因此，最终输出是一个包含 12 个条目的扁平列表，形式为 $[\\eta_{\\text{max}}^{(1)}, s_{\\text{below}}^{(1)}, s_{\\text{at}}^{(1)}, s_{\\text{above}}^{(1)}, \\eta_{\\text{max}}^{(2)}, s_{\\text{below}}^{(2)}, s_{\\text{at}}^{(2)}, s_{\\text{above}}^{(2)}, \\eta_{\\text{max}}^{(3)}, s_{\\text{below}}^{(3)}, s_{\\text{at}}^{(3)}, s_{\\text{above}}^{(3)}]$，其中每个 $s$ 是一个布尔值，表示对于该 $\\eta$ 的所有特征向量方向模拟是否都满足 $\\|\\theta_T\\|\\le \\varepsilon$。", "solution": "用户希望找到在二次目标函数的梯度下降（GD）优化中学习率 $\\eta$ 的稳定性条件，然后数值验证这个条件。\n\n### 问题验证\n根据指定标准分析问题陈述。\n\n**步骤 1：提取已知条件**\n- 目标函数：$f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$，其中 $\\theta \\in \\mathbb{R}^d$。\n- 矩阵 $A$：一个对称正定矩阵，$A \\in \\mathbb{R}^{d \\times d}$。\n- 梯度下降（GD）更新规则：$\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)$，学习率为 $\\eta0$。\n- 谱半径：$\\rho(M)=\\max_i |\\lambda_i(M)|$，其中 $\\lambda_i(M)$ 是矩阵 $M$ 的特征值。\n- 渐近稳定性条件：对于任何初始向量 $\\theta_0$，$\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$。\n- 测试矩阵：\n  1. $A_1=\\operatorname{diag}(1.0,4.0)$，$d=2$。\n  2. $A_2=Q_2 \\operatorname{diag}(0.5,1.5,3.0) Q_2^\\top$，其中 $Q_2$ 是 $\\mathbb{R}^3$ 的 $(0,1)$-平面中旋转 $\\pi/4$ 的矩阵。\n  3. $A_3=Q_3 \\operatorname{diag}(0.1,2.0,10.0,20.0) Q_3^\\top$，其中 $Q_3$ 是 $\\mathbb{R}^4$ 的 $(0,1)$-平面中旋转 $\\phi=\\pi/6$ 和 $(2,3)$-平面中旋转 $\\psi=\\pi/3$ 的矩阵的乘积。\n- 模拟参数：\n  - 步数：$T=200$。\n  - 收敛容差：$\\varepsilon=10^{-6}$。\n- 数值验证：\n  - 初始条件：对于 $A$ 的每个特征向量 $v_i$，$\\theta_0=v_i$。\n  - 待测试学习率：$\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$，$\\eta_{\\text{at}}=\\eta_{\\text{max}}$，$\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$。\n  - 稳定性检查：从每个特征向量开始的模拟都必须满足 $\\|\\theta_T\\|\\le \\varepsilon$。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题是优化理论和数值线性代数中的一个标准练习。\n- **科学依据**：该问题基于微积分（二次型的梯度）和线性代数（线性动力系统的特征值分析）的基本原理。它是优化器在局部最小值附近行为的简化，这是深度学习中的一个核心概念。\n- **适定性**：问题定义清晰。它要求推导一个特定量（$\\eta_{\\text{max}}$），并使用所有指定的参数和矩阵进行数值验证。这种结构导向一个唯一且有意义的解。\n- **客观性**：问题以精确的数学术语陈述，没有任何主观性或歧义。\n\n该问题没有任何无效性缺陷。它是完整的、一致的、科学上合理的和适定的。\n\n**步骤 3：结论和行动**\n问题有效。将提供一个完整的解决方案。\n\n### 稳定性条件的推导\n\n目标函数是一个二次型 $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$。由于 $A$ 是对称的，该函数关于 $\\theta$ 的梯度是 $\\nabla f(\\theta) = A\\theta$。\n\n梯度下降（GD）更新规则如下：\n$$\n\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)\n$$\n代入梯度，我们得到一个线性迭代映射：\n$$\n\\theta_{t+1} = \\theta_t - \\eta A\\theta_t = (I - \\eta A)\\theta_t\n$$\n其中 $I$ 是单位矩阵。设迭代矩阵为 $M = I - \\eta A$。那么更新就是 $\\theta_{t+1} = M\\theta_t$。经过 $t$ 步后，参数向量由 $\\theta_t = M^t \\theta_0$ 给出。\n\n为了使迭代收敛到零向量，即对于任意初始向量 $\\theta_0$，$\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$，其充要条件是迭代矩阵 $M$ 的谱半径严格小于 1。谱半径定义为其特征值的最大绝对值。\n$$\n\\rho(M)  1\n$$\n设 $\\lambda_i(A)$ 是矩阵 $A$ 的特征值，对应的特征向量为 $v_i$。由于 $A$ 是一个实对称矩阵，其特征值是实数。迭代矩阵 $M = I - \\eta A$ 的特征值可以通过将 $M$ 应用于 $A$ 的一个特征向量 $v_i$ 来找到：\n$$\nMv_i = (I - \\eta A)v_i = Iv_i - \\eta Av_i = v_i - \\eta\\lambda_i(A)v_i = (1 - \\eta\\lambda_i(A))v_i\n$$\n因此，$M$ 的特征值，记为 $\\lambda_i(M)$，是 $\\lambda_i(M) = 1 - \\eta\\lambda_i(A)$。\n\n稳定性条件 $\\rho(M)  1$ 可转化为：\n$$\n\\max_i |\\lambda_i(M)| = \\max_i |1 - \\eta\\lambda_i(A)|  1\n$$\n这必须对 $A$ 的所有特征值 $\\lambda_i(A)$ 成立。这个单一的不等式等价于以下一对不等式：\n$$\n-1  1 - \\eta\\lambda_i(A)  1 \\quad \\text{for all } i\n$$\n我们分析不等式的每个部分：\n1.  $1 - \\eta\\lambda_i(A)  1 \\implies -\\eta\\lambda_i(A)  0$。根据定义，学习率 $\\eta  0$，因此这可以简化为 $\\lambda_i(A)  0$。这个条件对于所有 $i$ 都保证成立，因为问题陈述中说明了矩阵 $A$ 是正定的。\n\n2.  $-1  1 - \\eta\\lambda_i(A) \\implies \\eta\\lambda_i(A)  2 \\implies \\eta  \\frac{2}{\\lambda_i(A)}$。这个不等式必须对所有特征值 $\\lambda_i(A)$ 成立。为确保这一点，$\\eta$ 必须小于所有 $\\frac{2}{\\lambda_i(A)}$ 值中的最小值。这等价于选择最大的特征值 $\\lambda_{\\max}(A)$，因为它对 $\\eta$ 提供了最严格的约束：\n    $$\n    \\eta  \\frac{2}{\\lambda_{\\max}(A)}\n    $$\n\n问题将矩阵 $M$ 的谱半径定义为 $\\rho(M)=\\max_i |\\lambda_i(M)|$。对于对称正定矩阵 $A$，所有特征值 $\\lambda_i(A)$ 都是正的，因此其谱半径就是其最大特征值：$\\rho(A) = \\lambda_{\\max}(A)$。\n\n结合这些发现，GD 迭代渐近稳定的充要条件是：\n$$\n0  \\eta  \\frac{2}{\\rho(A)}\n$$\n最大的渐近稳定学习率 $\\eta_{\\text{max}}$ 是这个区间的上确界：\n$$\n\\eta_{\\text{max}} = \\frac{2}{\\rho(A)}\n$$\n\n### 数值验证\n程序将实现这个推导。对于每个给定的矩阵 $A$，它将首先计算其特征值以找到 $\\rho(A) = \\lambda_{\\max}(A)$ 和相应的 $\\eta_{\\text{max}}$。然后，它将从 $A$ 的每个特征向量开始模拟 GD 过程 $T=200$ 步。这对三个学习率进行：$\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$（预期收敛）、$\\eta_{\\text{at}}=\\eta_{\\text{max}}$（预期对于对应于 $\\lambda_{\\max}(A)$ 的模式不稳定）和 $\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$（预期发散）。通过验证最终迭代的范数 $\\|\\theta_T\\|$ 是否小于或等于一个小的容差 $\\varepsilon=10^{-6}$ 来数值检查稳定性。\n- 对于 $\\eta_{\\text{below}}$，我们预期对所有 $i$ 都有 $|1 - \\eta\\lambda_i(A)|  1$，从而导致收敛。\n- 对于 $\\eta_{\\text{at}}$，对应于 $\\lambda_{\\max}(A)$ 的模式将有一个特征值 $1 - \\frac{2}{\\lambda_{\\max}(A)}\\lambda_{\\max}(A) = -1$。该模式的迭代幅值不会衰减，导致收敛测试失败。\n- 对于 $\\eta_{\\text{above}}$，对应于 $\\lambda_{\\max}(A)$ 的模式将有一个幅值大于 1 的特征值，导致发散。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies the stability condition for Gradient Descent\n    on a quadratic objective.\n    \"\"\"\n    # Simulation parameters from the problem statement\n    T = 200\n    eps = 1e-6\n    \n    # === Define Test Cases ===\n\n    # Test Case 1: A1 = diag(1.0, 4.0)\n    A1 = np.diag([1.0, 4.0])\n\n    # Test Case 2: A2 = Q2 * diag(0.5, 1.5, 3.0) * Q2^T\n    angle2 = np.pi / 4\n    c2, s2 = np.cos(angle2), np.sin(angle2)\n    Q2 = np.array([\n        [c2, -s2, 0],\n        [s2, c2, 0],\n        [0, 0, 1]\n    ])\n    D2 = np.diag([0.5, 1.5, 3.0])\n    A2 = Q2 @ D2 @ Q2.T\n\n    # Test Case 3: A3 = Q3 * diag(0.1, 2.0, 10.0, 20.0) * Q3^T\n    phi = np.pi / 6\n    psi = np.pi / 3\n    c_phi, s_phi = np.cos(phi), np.sin(phi)\n    c_psi, s_psi = np.cos(psi), np.sin(psi)\n    \n    # Rotation in (0,1)-plane\n    R01 = np.array([\n        [c_phi, -s_phi, 0, 0],\n        [s_phi, c_phi, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ])\n    \n    # Rotation in (2,3)-plane\n    R23 = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, c_psi, -s_psi],\n        [0, 0, s_psi, c_psi]\n    ])\n    \n    Q3 = R01 @ R23\n    D3 = np.diag([0.1, 2.0, 10.0, 20.0])\n    A3 = Q3 @ D3 @ Q3.T\n    \n    test_cases = [A1, A2, A3]\n    all_results = []\n\n    for A in test_cases:\n        # Since A is symmetric, eigh is preferred for numerical stability\n        # and guarantees orthogonal eigenvectors and real eigenvalues.\n        eigenvalues, eigenvectors = np.linalg.eigh(A)\n        \n        # The spectral radius of a symmetric positive definite matrix is its largest eigenvalue.\n        rho_A = np.max(eigenvalues)\n        \n        # The predicted maximum stable learning rate from the derivation.\n        eta_max = 2.0 / rho_A\n        all_results.append(eta_max)\n        \n        etas_to_test = {\n            'below': 0.9 * eta_max,\n            'at': eta_max,\n            'above': 1.1 * eta_max\n        }\n        \n        # Test stability for each candidate learning rate\n        for key in ['below', 'at', 'above']:\n            eta = etas_to_test[key]\n            is_stable_for_this_eta = True\n            \n            # Dimension of the parameter space\n            d = A.shape[0]\n            \n            # Iterate through each eigen-direction as an initial condition\n            for i in range(d):\n                theta_0 = eigenvectors[:, i]\n                theta = theta_0.copy()\n                \n                # Perform T steps of Gradient Descent\n                for _ in range(T):\n                    theta = theta - eta * (A @ theta)\n                \n                # Check if the final state has converged to the origin\n                final_norm = np.linalg.norm(theta)\n                if final_norm > eps:\n                    is_stable_for_this_eta = False\n                    # If one direction is unstable, no need to check others for this eta\n                    break\n            \n            all_results.append(is_stable_for_this_eta)\n            \n    # Final print statement in the exact required format.\n    # The default str() for booleans ('True'/'False') is used.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3187300"}, {"introduction": "在梯度下降中引入动量是加速模型训练的常用技巧。然而，动量的加入改变了优化的动态过程，这种改变可以通过“有效学习率”的概念来理解，它揭示了基础学习率 $\\eta$ 和动量系数 $\\beta$ 之间的相互作用。本练习将指导你推导这种关系，并用数值实验证明，为了在调整动量的同时保持相似的更新幅度，必须相应地调整学习率，这是优化器调参中的一个核心实践。[@problem_id:3187263]", "problem": "考虑动量梯度下降法 (Gradient Descent with Momentum, GDM)，也称为重球法 (heavy-ball method)。设在第 $t$ 次迭代时，参数向量表示为 $w_t \\in \\mathbb{R}^d$，梯度表示为 $g_t \\in \\mathbb{R}^d$，动量系数为 $\\beta \\in [0,1)$，学习率为 $\\eta  0$。更新方程为以下基本定义：速度 $v_t \\in \\mathbb{R}^d$ 遵循 $v_{t+1} = \\beta v_t + g_t$，参数更新方式为 $w_{t+1} = w_t - \\eta v_{t+1}$。假设 $v_0 = 0$ 且 $w_0$ 为给定值。\n\n任务 1 (推导)：仅从这些更新方程出发，且不假设任何快捷公式，在恒定梯度条件下（其中 $g_t = g$ 是一个固定的非零向量，且 $\\beta \\in [0,1)$），推导稳态下的单次迭代参数更新。用 $g$、$ \\eta$ 和 $\\beta$ 表示你的结果，并给出推导有效的数学条件。\n\n任务 2 (计算测试)：在一维情况下 ($d = 1$)，实现一个程序来模拟在恒定标量梯度 $g \\in \\mathbb{R}$ 条件下的 GDM。对于给定的 $(\\beta,\\eta)$ 对，从 $v_0 = 0$ 和 $w_0 = 0$ 开始运行 $T$ 次迭代，记录每次迭代的更新幅度 $\\Delta_t = |w_{t+1} - w_t|$，并计算最后 $K$ 次迭代的平均值。定义在不同 $\\beta$ 值之间“均衡的训练后期动力学”概念如下：对于一组 $\\beta$ 值，如果计算出的平均值（在最后 $K$ 次迭代中）的最大值与最小值之差小于或等于一个容差 $\\varepsilon$，则认为动力学是均衡的。\n\n测试套件：你的程序必须评估以下四个测试用例，并为每个用例输出一个布尔值，以指示根据上述标准，训练后期动力学是否均衡。下面测试套件中的所有标量数值都是无量纲的。\n\n- 测试 1 (一般情况)：恒定梯度 $g = 1.2$，动量系数 $\\beta \\in \\{0.0, 0.5, 0.9\\}$，目标稳态缩放参数 $c = 0.05$，为每个 $\\beta$ 选择满足 $\\eta = c \\cdot (1 - \\beta)$ 的学习率，迭代次数 $T = 200$，最后窗口长度 $K = 50$，容差 $\\varepsilon = 10^{-6}$。\n\n- 测试 2 (接近 $\\beta \\to 1$ 的边界情况)：恒定梯度 $g = 0.7$，动量系数 $\\beta \\in \\{0.0, 0.99\\}$，选择 $\\eta = c \\cdot (1 - \\beta)$ 且 $c = 0.1$，迭代次数 $T = 3000$，最后窗口长度 $K = 500$，容差 $\\varepsilon = 10^{-6}$。\n\n- 测试 3 (不匹配的学习率)：恒定梯度 $g = 1.0$，动量系数 $\\beta \\in \\{0.8, 0.9\\}$，对两个 $\\beta$ 值使用相同的学习率 $\\eta = 0.01$ (不通过 $(1 - \\beta)$ 进行调整)，迭代次数 $T = 200$，最后窗口长度 $K = 50$，容差 $\\varepsilon = 10^{-6}$。\n\n- 测试 4 (零梯度边缘情况)：恒定梯度 $g = 0.0$，动量系数 $\\beta \\in \\{0.0, 0.95, 0.99\\}$，选择 $\\eta = c \\cdot (1 - \\beta)$ 且 $c = 0.2$，迭代次数 $T = 100$，最后窗口长度 $K = 50$，容差 $\\varepsilon = 10^{-9}$。\n\n最终输出格式：你的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是一个布尔值，对应于上面列出的第 $i$ 个测试。", "solution": "问题陈述已经过验证，并被认为是科学上合理、定义明确且完整的。它描述了一种标准的优化算法，即动量梯度下降法 (GDM)，并提出了两个任务：在恒定梯度下其稳态行为的数学推导，以及一个用于在特定超参数设置下验证此行为的计算测试。所有参数和条件都已明确定义。\n\n### 任务 1：稳态更新的推导\n\nGDM 的更新方程如下：\n$$v_{t+1} = \\beta v_t + g_t$$\n$$w_{t+1} = w_t - \\eta v_{t+1}$$\n初始条件为 $v_0 = 0$ 和给定的 $w_0$。参数为动量系数 $\\beta \\in [0,1)$ 和学习率 $\\eta  0$。我们被要求在恒定梯度条件下分析该系统，其中对于所有迭代 $t$，$g_t = g$，$g$ 是一个固定的非零向量。\n\n目标是找到稳态下的单次迭代参数更新，我们将其表示为 $\\Delta w_{ss} = \\lim_{t \\to \\infty} (w_{t+1} - w_t)$。在任意迭代 $t$ 时的参数更新为 $\\Delta w_t = w_{t+1} - w_t = -\\eta v_{t+1}$。因此，推导稳态更新需要找到稳态速度 $v_{ss} = \\lim_{t \\to \\infty} v_t$。\n\n让我们分析速度向量 $v_t$ 的递推关系：\n$$v_{t+1} = \\beta v_t + g$$\n我们可以从 $v_0 = 0$ 开始展开这个递推关系：\n在 $t=0$ 时：$v_1 = \\beta v_0 + g = \\beta(0) + g = g$\n在 $t=1$ 时：$v_2 = \\beta v_1 + g = \\beta g + g = (1 + \\beta)g$\n在 $t=2$ 时：$v_3 = \\beta v_2 + g = \\beta(1+\\beta)g + g = (1 + \\beta + \\beta^2)g$\n\n通过归纳法，我们可以看出在任意迭代 $t  0$ 时 $v_t$ 的通用形式：\n$$v_t = \\left(\\sum_{i=0}^{t-1} \\beta^i\\right) g$$\n这个求和是一个有限几何级数。对于 $\\beta \\neq 1$，其和由下式给出：\n$$\\sum_{i=0}^{t-1} \\beta^i = \\frac{1 - \\beta^t}{1 - \\beta}$$\n因此，在迭代 $t$ 时的速度为：\n$$v_t = \\frac{1 - \\beta^t}{1 - \\beta} g$$\n为了找到稳态速度 $v_{ss}$，我们取 $t \\to \\infty$ 的极限：\n$$v_{ss} = \\lim_{t \\to \\infty} v_t = \\lim_{t \\to \\infty} \\left(\\frac{1 - \\beta^t}{1 - \\beta}\\right) g$$\n该极限收敛当且仅当项 $\\beta^t$ 收敛。问题陈述中说明 $\\beta \\in [0, 1)$，这是一个比 $|\\beta|  1$ 更严格但已足够的条件。在此条件下，$\\lim_{t \\to \\infty} \\beta^t = 0$。\n\n因此，稳态速度为：\n$$v_{ss} = \\frac{1 - 0}{1 - \\beta} g = \\frac{g}{1 - \\beta}$$\n速度以指数方式接近这个终值。项 $1/(1-\\beta)$ 作为一个标量，放大了梯度 $g$。\n\n现在，我们可以找到稳态下的单次迭代参数更新 $\\Delta w_{ss}$。在稳态下，$v_{t+1} \\approx v_{ss}$，所以：\n$$\\Delta w_{ss} = w_{t+1} - w_t = -\\eta v_{ss}$$\n代入 $v_{ss}$ 的表达式，我们得到：\n$$\\Delta w_{ss} = -\\eta \\left(\\frac{g}{1 - \\beta}\\right)$$\n这就是推导出的稳态单次迭代参数更新。该推导在速度的几何级数收敛的条件下是有效的，而这一条件由给定的约束 $\\beta \\in [0, 1)$ 保证。\n\n### 任务 2：计算测试与分析\n\n计算任务要求在一维 ($d=1$) 情况下模拟 GDM，并评估一个关于“均衡的训练后期动力学”的标准。该标准检查在不同的 $\\beta$ 值下，平均更新幅度 $\\bar{\\Delta} = \\text{avg}_{t \\in \\{T-K, \\dots, T-1\\}} |w_{t+1} - w_t|$ 是否相似。\n\n根据我们的推导，稳态更新的幅度为：\n$$|\\Delta w_{ss}| = \\left|-\\eta \\frac{g}{1 - \\beta}\\right| = \\frac{\\eta |g|}{1 - \\beta}$$\n这个公式是理解这些测试用例的关键。\n\n**测试用例 1、2 和 4 的分析：**\n在这些测试中，学习率 $\\eta$ 通过规则 $\\eta = c \\cdot (1 - \\beta)$（其中 $c$ 为某个常数）与动量系数 $\\beta$ 耦合。让我们将此代入我们的稳态更新幅度公式：\n$$|\\Delta w_{ss}| = \\frac{(c \\cdot (1 - \\beta)) |g|}{1 - \\beta} = c|g|$$\n这个结果意义深远：当 $\\eta$ 按 $(1 - \\beta)$ 缩放时，稳态更新幅度变得与 $\\beta$ 无关。它仅取决于常数标量 $c$ 和梯度 $g$ 的幅度。\n\n- **测试 1 ($g=1.2, c=0.05$) 和 测试 2 ($g=0.7, c=0.1$)：** 对于这些测试，由于 $g \\neq 0$，我们预计对于每个 $\\beta$，系统将收敛到一个非零的稳态更新幅度 $c|g|$。对于测试 1，$|\\Delta w_{ss}| = 0.05 \\times 1.2 = 0.06$。对于测试 2，$|\\Delta w_{ss}| = 0.1 \\times 0.7 = 0.07$。由于这个理论值在给定测试中的所有 $\\beta$ 值下都是相同的，我们预计模拟的平均更新幅度的最大值和最小值之差会非常小（在容差 $\\varepsilon$ 内）。选择的迭代次数 $T$ 远大于特征收敛时间尺度（约为 $1/(1-\\beta)$ 的数量级），确保系统在最后的 $K$ 次迭代期间处于稳态。因此，我们预测动力学将是均衡的，两个测试的结果都应该是 `True`。\n\n- **测试 4 ($g=0.0, c=0.2$)：** 此处梯度为零。速度更新为 $v_{t+1} = \\beta v_t$。由于 $v_0 = 0$，因此对于所有 $t \\geq 1$，$v_t = 0$。因此，参数更新 $w_{t+1} - w_t = -\\eta v_{t+1}$ 始终为零。最后 $K$ 次迭代的平均更新幅度对于所有 $\\beta$ 值都将是 $0$。最大平均值 ($0$) 和最小平均值 ($0$) 之间的差为 $0$，小于或等于容差 $\\varepsilon = 10^{-9}$。我们预测结果为 `True`。\n\n**测试用例 3 的分析：**\n在此测试中，学习率 $\\eta = 0.01$ 对两个不同的 $\\beta$ 值（$\\beta_1 = 0.8$ 和 $\\beta_2 = 0.9$）保持不变。规则 $\\eta = c(1-\\beta)$ 未被使用。我们必须使用稳态更新幅度的通用公式：$|\\Delta w_{ss}| = \\frac{\\eta |g|}{1 - \\beta}$。\n\n- 对于 $\\beta_1 = 0.8$：$|\\Delta w_{ss}| = \\frac{0.01 \\times |1.0|}{1 - 0.8} = \\frac{0.01}{0.2} = 0.05$。\n- 对于 $\\beta_2 = 0.9$：$|\\Delta w_{ss}| = \\frac{0.01 \\times |1.0|}{1 - 0.9} = \\frac{0.01}{0.1} = 0.10$。\n\n理论上的稳态更新幅度显著不同（$0.05$ 对 $0.10$）。差值为 $0.05$，远大于容差 $\\varepsilon = 10^{-6}$。因此，我们预测动力学将不均衡，此测试的结果应为 `False`。\n\n**预测摘要：**\n- 测试 1：`True`\n- 测试 2：`True`\n- 测试 3：`False`\n- 测试 4：`True`\n\n以下程序实现了模拟以验证这些预测。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gdm_simulation(g: float, beta: float, eta: float, T: int, K: int) - float:\n    \"\"\"\n    Simulates Gradient Descent with Momentum for a constant scalar gradient.\n\n    Args:\n        g: The constant scalar gradient.\n        beta: The momentum coefficient.\n        eta: The learning rate.\n        T: The total number of iterations.\n        K: The number of last iterations to average over.\n\n    Returns:\n        The average update magnitude over the last K iterations.\n    \"\"\"\n    v = 0.0\n    update_magnitudes = []\n\n    for _ in range(T):\n        # Velocity update: v_{t+1} = beta * v_t + g_t\n        # In our case, v_t is the current v, and g_t is the constant g.\n        v = beta * v + g\n\n        # Parameter update magnitude: |w_{t+1} - w_t| = |-eta * v_{t+1}|\n        # The newly computed v is effectively v_{t+1}.\n        update_mag = eta * abs(v)\n        update_magnitudes.append(update_mag)\n        # Note: We do not need to track the parameter w itself, as only the\n        # magnitude of its change is required for the analysis.\n\n    # Calculate the average over the last K iterations.\n    # The slice update_magnitudes[T-K:] extracts the last K elements.\n    avg_update = np.mean(update_magnitudes[T - K:])\n    return avg_update\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for GDM dynamics equalization.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1 (general case)\n        {\n            'g': 1.2, \n            'betas': [0.0, 0.5, 0.9], \n            'eta_rule': 'c(1-beta)', \n            'c': 0.05,\n            'T': 200, \n            'K': 50, \n            'eps': 1e-6\n        },\n        # Test 2 (boundary near beta - 1)\n        {\n            'g': 0.7, \n            'betas': [0.0, 0.99], \n            'eta_rule': 'c(1-beta)',\n            'c': 0.1,\n            'T': 3000, \n            'K': 500, \n            'eps': 1e-6\n        },\n        # Test 3 (mismatched learning rates)\n        {\n            'g': 1.0, \n            'betas': [0.8, 0.9], \n            'eta_rule': 'fixed',\n            'eta': 0.01,\n            'T': 200, \n            'K': 50, \n            'eps': 1e-6\n        },\n        # Test 4 (zero gradient edge case)\n        {\n            'g': 0.0, \n            'betas': [0.0, 0.95, 0.99], \n            'eta_rule': 'c(1-beta)',\n            'c': 0.2,\n            'T': 100, \n            'K': 50, \n            'eps': 1e-9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        averages = []\n        for beta in case['betas']:\n            # Determine the learning rate based on the rule for the test case\n            if case['eta_rule'] == 'c(1-beta)':\n                eta = case['c'] * (1.0 - beta)\n            else:  # 'fixed' rule\n                eta = case['eta']\n            \n            # Run the simulation and get the average update magnitude\n            avg_mag = run_gdm_simulation(\n                g=case['g'], \n                beta=beta, \n                eta=eta, \n                T=case['T'], \n                K=case['K']\n            )\n            averages.append(avg_mag)\n        \n        # Check if the dynamics are equalized\n        if len(averages) > 0:\n            diff = max(averages) - min(averages)\n            is_equalized = diff = case['eps']\n        else:\n            is_equalized = True # Vacuously true for empty set of betas\n            \n        results.append(is_equalized)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187263"}, {"introduction": "权重衰减是深度学习中广泛使用的正则化方法，但它与学习率之间存在着微妙的相互作用，尤其在现代优化器中。本练习将探讨“解耦权重衰减”（decoupled weight decay）这一先进技术，它被用于AdamW等优化器中，旨在打破学习率与有效正则化强度之间的不良耦合。通过解决这个问题，你将理解为何调整学习率会影响传统权重衰减的效果，并掌握现代优化器的一个关键设计原则，即关注比率 $\\lambda / \\eta$ 的重要性。[@problem_id:3187375]", "problem": "要求您在使用基于梯度的优化中的解耦权重衰减时，形式化、推导并验证有效二次参数惩罚的缩放规律。考虑一个一维二次损失和一个解耦权重衰减更新，该更新对参数应用一个乘法收缩，然后进行一个梯度步长。从经验风险最小化的基本基础和梯度下降动力学的第一性原理开始，然后推导迭代更新的不动点。使用推导出的表达式来确定有效二次惩罚如何依赖于学习率和解耦衰减参数，并设计一个数值程序来在一组测试集上验证该预测。\n\n所有数学实体必须用 LaTeX 书写。设损失为一个严格凸的二次函数，由下式给出\n$$\n\\mathcal{L}(w) = \\frac{1}{2} a \\left(w - w^\\star\\right)^2,\n$$\n其中曲率 $a  0$，最小化点为 $w^\\star$。将解耦权重衰减更新定义为一个复合操作：首先对参数 $w$ 应用一个因子为 $(1 - \\lambda)$ 的乘法收缩，然后对原始损失 $\\mathcal{L}(w)$（没有向 $\\mathcal{L}$ 添加惩罚项）进行学习率为 $\\eta  0$ 的梯度下降步骤。得到的离散时间更新为\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta \\, \\nabla \\mathcal{L}(w_t).\n$$\n\n任务：\n1) 从可微函数上的梯度下降定义和所述的解耦权重衰减方案出发，推导当迭代收敛时其不动点 $w_\\infty$ 的一个闭式表达式，用 $a$、$w^\\star$、$\\eta$ 和 $\\lambda$ 表示。明确地表达您的答案，并根据线性系数的幅值来证明收敛条件。\n\n2) 使用您的不动点表达式，证明解耦收缩加梯度步长在其不动点上等价于最小化一个形如下式的二次目标函数\n$$\n\\mathcal{L}(w) + \\frac{\\alpha}{2}\\,w^2,\n$$\n对于某个依赖于 $\\lambda$ 和 $\\eta$ 的有效惩罚强度 $\\alpha$。求解 $\\alpha$ 并尽可能简化您的表达式。解释为什么在给定 $a$ 和 $w^\\star$ 的情况下，当改变 $\\eta$ 时保持比率 $\\lambda / \\eta$ 不变会使不动点保持不变，从而将学习率对暂态动力学的纯粹影响与正则化偏差分离开来。\n\n3) 实现一个程序，该程序对所述更新进行有限步数的数值模拟，并在以下测试集上验证推导出的预测。在所有情况下，使用 $a = 3$，$w^\\star = 2$，$w_0 = 10$，并运行 $T = 500$ 次迭代：\n- 情况 A（理想路径，中等设置）：$\\eta = 0.1$，$\\lambda = 0.02$。\n- 情况 B（与情况 A 具有相同的 $\\lambda / \\eta$，但 $\\eta$ 不同）：$\\eta = 0.05$，$\\lambda = 0.01$。\n- 情况 C（与情况 A 具有不同的 $\\lambda / \\eta$，但与情况 A 具有相同的 $\\eta$）：$\\eta = 0.1$，$\\lambda = 0.01$。\n- 情况 D（无衰减的边界情况）：$\\eta = 0.1$，$\\lambda = 0$。\n\n对于每种情况，定义迭代\n$$\nw_{t+1} = (1 - \\lambda)\\,w_t - \\eta\\,a\\,(w_t - w^\\star),\n$$\n并计算：\n- $r_1$：$T$ 步后情况 A 和情况 B 最终参数之间的绝对差，即 $|w_T^{\\mathrm{A}} - w_T^{\\mathrm{B}}|$。这测试了当 $\\lambda / \\eta$ 固定但 $\\eta$ 改变时的不变性。\n- $r_2$：在情况 A、B 和 C 中，模拟的最终参数与预测的不动点值 $w_\\infty = \\dfrac{a}{a + \\lambda / \\eta} w^\\star$ 之间的最大绝对差异。形式上，\n$$\nr_2 = \\max\\Big(\\big|w_T^{\\mathrm{A}} - \\tfrac{a}{a + \\lambda_{\\mathrm{A}} / \\eta_{\\mathrm{A}}} w^\\star\\big|,\\ \\big|w_T^{\\mathrm{B}} - \\tfrac{a}{a + \\lambda_{\\mathrm{B}} / \\eta_{\\mathrm{B}}} w^\\star\\big|,\\ \\big|w_T^{\\mathrm{C}} - \\tfrac{a}{a + \\lambda_{\\mathrm{C}} / \\eta_{\\mathrm{C}}} w^\\star\\big|\\Big).\n$$\n- $r_3$：$T$ 步后情况 A 和情况 C 最终参数之间的绝对差，即 $|w_T^{\\mathrm{A}} - w_T^{\\mathrm{C}}|$。这测试了改变 $\\lambda / \\eta$ 会改变不动点。\n- $r_4$：情况 D 相对于未正则化最小化点 $|w_T^{\\mathrm{D}} - w^\\star|$ 的绝对误差。\n\n所有数值答案必须是无单位的实数。最终输出格式必须是单行，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序为 $[r_1, r_2, r_3, r_4]$。\n\n您的程序必须是一个完整、可运行的程序，它能准确地产生这个单行输出，不带任何附加文本。", "solution": "该问题是有效的，因为它科学地基于梯度优化的原理，问题设定良好，有明确的目标和足够的数据，并以精确、客观的语言表达。我们将继续进行推导和实现。\n\n该问题要求对一个简单的一维二次损失函数分析解耦权重衰减。根据要求，解决方案分为三个部分：不动点和收敛条件的推导、有效二次惩罚的推导以及数值验证。\n\n### 任务 1：不动点推导与收敛性\n\n我们给定的损失函数为 $\\mathcal{L}(w) = \\frac{1}{2} a (w - w^\\star)^2$，其中 $a  0$。\n该损失关于参数 $w$ 的梯度是：\n$$\n\\nabla \\mathcal{L}(w) = \\frac{d\\mathcal{L}}{dw} = a (w - w^\\star)\n$$\n在每个离散时间步 $t$ 时参数 $w$ 的更新规则被定义为一个乘法收缩和梯度下降步骤的复合：\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta \\, \\nabla \\mathcal{L}(w_t)\n$$\n其中 $\\lambda$ 是解耦权重衰减参数，$\\eta  0$ 是学习率。\n\n将梯度的表达式代入更新规则得到：\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta a (w_t - w^\\star)\n$$\n为了找到这次迭代的不动点，记为 $w_\\infty$，我们寻找一个 $w$ 的值，使得 $w_{t+1} = w_t = w_\\infty$。在不动点处，更新规则变成一个代数方程：\n$$\nw_\\infty = (1 - \\lambda) w_\\infty - \\eta a (w_\\infty - w^\\star)\n$$\n我们现在可以解出 $w_\\infty$：\n$$\nw_\\infty = w_\\infty - \\lambda w_\\infty - \\eta a w_\\infty + \\eta a w^\\star\n$$\n从两边减去 $w_\\infty$ 得到：\n$$\n0 = - \\lambda w_\\infty - \\eta a w_\\infty + \\eta a w^\\star\n$$\n将包含 $w_\\infty$ 的项分组：\n$$\n(\\lambda + \\eta a) w_\\infty = \\eta a w^\\star\n$$\n假设 $\\lambda + \\eta a \\neq 0$，这是成立的，因为 $a  0$，$\\eta  0$，并且我们考虑 $\\lambda \\ge 0$，我们可以分离出 $w_\\infty$：\n$$\nw_\\infty = \\frac{\\eta a}{\\lambda + \\eta a} w^\\star\n$$\n为了方便下一部分的分析，我们可以将分子和分母同时除以 $\\eta$：\n$$\nw_\\infty = \\frac{a}{(\\lambda / \\eta) + a} w^\\star\n$$\n这就是迭代不动点的闭式表达式。\n\n为了使迭代收敛到这个不动点，我们将更新规则分析为一个线性递推关系。让我们重新排列更新规则中的项以分离出 $w_t$：\n$$\nw_{t+1} = (1 - \\lambda - \\eta a) w_t + \\eta a w^\\star\n$$\n这是一个形式为 $w_{t+1} = C w_t + D$ 的一阶线性递推关系，其中系数 $C = 1 - \\lambda - \\eta a$，常数项 $D = \\eta a w^\\star$。这样的迭代当且仅当线性系数 $C$ 的幅值严格小于 $1$ 时，即 $|C|  1$，收敛到一个唯一的不动点。\n因此，收敛的条件是：\n$$\n|1 - \\lambda - \\eta a|  1\n$$\n这个不等式可以展开为两个独立的不等式：\n1) $1 - \\lambda - \\eta a  1 \\implies -\\lambda - \\eta a  0 \\implies \\lambda + \\eta a  0$。由于 $a  0$，$\\eta  0$，且 $\\lambda \\ge 0$，这个条件总是满足的。\n2) $1 - \\lambda - \\eta a  -1 \\implies 2  \\lambda + \\eta a$。\n\n结合这些，收敛的充要条件是 $0  \\lambda + \\eta a  2$。问题陈述中提供的所有数值案例都满足这个条件。\n\n### 任务 2：有效二次惩罚\n\n我们现在证明不动点 $w_\\infty$ 等价于一个加上了二次惩罚项的目标函数的最小化点。考虑正则化的目标函数：\n$$\n\\mathcal{L}_{eff}(w) = \\mathcal{L}(w) + \\frac{\\alpha}{2} w^2 = \\frac{1}{2} a (w - w^\\star)^2 + \\frac{\\alpha}{2} w^2\n$$\n为了找到 $\\mathcal{L}_{eff}(w)$ 的最小化点，我们将其关于 $w$ 的一阶导数设为零：\n$$\n\\frac{d\\mathcal{L}_{eff}}{dw} = \\frac{d}{dw} \\left( \\frac{1}{2} a (w - w^\\star)^2 + \\frac{\\alpha}{2} w^2 \\right) = 0\n$$\n$$\na (w - w^\\star) + \\alpha w = 0\n$$\n求解 $w$：\n$$\na w - a w^\\star + \\alpha w = 0\n$$\n$$\n(a + \\alpha) w = a w^\\star\n$$\n正则化目标的最小化点是：\n$$\nw = \\frac{a}{a + \\alpha} w^\\star\n$$\n为了使这个最小化点与解耦权重衰减更新的不动点 $w_\\infty$ 相同，它们的表达式必须相等：\n$$\n\\frac{a}{a + \\alpha} w^\\star = \\frac{a}{(\\lambda / \\eta) + a} w^\\star\n$$\n假设 $w^\\star \\neq 0$ 且 $a \\neq 0$，我们可以令系数的分母相等：\n$$\na + \\alpha = a + \\frac{\\lambda}{\\eta}\n$$\n这给出了有效二次惩罚强度 $\\alpha$ 为：\n$$\n\\alpha = \\frac{\\lambda}{\\eta}\n$$\n这个结果表明，参数为 $\\lambda$ 和学习率为 $\\eta$ 的解耦权重衰减，在其不动点处，等价于惩罚强度为 $\\lambda / \\eta$ 的标准 $L_2$ 正则化（或权重衰减）。\n\n不动点的表达式 $w_\\infty = \\frac{a}{a + \\lambda / \\eta} w^\\star$ 仅通过 $\\lambda$ 和 $\\eta$ 的比率 $\\lambda / \\eta$ 依赖于它们。因此，如果我们改变 $\\eta$ 和 $\\lambda$ 使得它们的比率 $\\lambda / \\eta$ 保持不变，那么对于给定的曲率 $a$ 和未正则化的最小化点 $w^\\star$，不动点 $w_\\infty$ 保持不变。学习率 $\\eta$ 仍然影响优化过程的暂态动力学——具体来说，是收敛速度，它由 $|1 - \\lambda - \\eta a|$ 控制。通过保持 $\\lambda / \\eta$ 不变，可以调整学习率 $\\eta$ 来调节收敛速度，而不改变最终的正则化解（正则化偏差）。这种解耦是该优化方案的一个关键优势。\n\n### 任务 3：数值验证\n\n第三个任务要求一个数值程序来模拟几个测试用例的更新规则，并验证上面推导出的理论预测。该程序将为指定的参数实现迭代 $w_{t+1} = (1 - \\lambda)\\,w_t - \\eta\\,a\\,(w_t - w^\\star)$，并计算所需的度量 $r_1$、$r_2$、$r_3$ 和 $r_4$。实现将在最终答案块中提供。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs numerical simulation of decoupled weight decay to verify theoretical predictions.\n    \"\"\"\n    # Define common parameters and test cases from the problem statement.\n    a = 3.0\n    w_star = 2.0\n    w0 = 10.0\n    T = 500\n\n    test_cases_params = {\n        'A': {'eta': 0.1, 'lambda': 0.02},\n        'B': {'eta': 0.05, 'lambda': 0.01},\n        'C': {'eta': 0.1, 'lambda': 0.01},\n        'D': {'eta': 0.1, 'lambda': 0.0}\n    }\n\n    # --- Simulation Phase ---\n    # Store the final parameter value for each case after T iterations.\n    final_ws = {}\n    for case_id, params in test_cases_params.items():\n        eta = params['eta']\n        lam = params['lambda']\n        \n        w = w0\n        for _ in range(T):\n            # The decoupled weight decay update rule for a quadratic loss.\n            w = (1.0 - lam) * w - eta * a * (w - w_star)\n        \n        final_ws[case_id] = w\n\n    # --- Verification Phase ---\n    # Calculate the required metrics r1, r2, r3, r4.\n\n    # r1: Absolute difference between Case A and Case B final parameters.\n    # Theoretical expectation: r1 should be very small as they share the same fixed point.\n    r1 = np.abs(final_ws['A'] - final_ws['B'])\n\n    # r2: Maximum absolute discrepancy between simulated and theoretical fixed points for A, B, C.\n    # The theoretical fixed point is w_inf = (a / (a + lambda/eta)) * w_star\n    w_inf_A = a / (a + test_cases_params['A']['lambda'] / test_cases_params['A']['eta']) * w_star\n    w_inf_B = a / (a + test_cases_params['B']['lambda'] / test_cases_params['B']['eta']) * w_star\n    w_inf_C = a / (a + test_cases_params['C']['lambda'] / test_cases_params['C']['eta']) * w_star\n    \n    err_A = np.abs(final_ws['A'] - w_inf_A)\n    err_B = np.abs(final_ws['B'] - w_inf_B)\n    err_C = np.abs(final_ws['C'] - w_inf_C)\n    \n    r2 = max(err_A, err_B, err_C)\n\n    # r3: Absolute difference between Case A and Case C final parameters.\n    # Theoretical expectation: r3 should be non-trivial as their fixed points differ.\n    r3 = np.abs(final_ws['A'] - final_ws['C'])\n    \n    # r4: Absolute error in Case D relative to the unregularized minimizer w_star.\n    # Case D is standard gradient descent, which should converge to w_star.\n    r4 = np.abs(final_ws['D'] - w_star)\n    \n    results = [r1, r2, r3, r4]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187375"}]}