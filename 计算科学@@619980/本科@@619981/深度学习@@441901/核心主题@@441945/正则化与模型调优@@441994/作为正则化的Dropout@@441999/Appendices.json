{"hands_on_practices": [{"introduction": "“反向随机失活”（Inverted Dropout）是现代神经网络中的标准实践，但“反向”这一步——即在训练时按保留概率的倒数进行缩放——为何如此关键？本练习通过一个假设情景，即错误地省略了这一缩放步骤，来揭示其背后的原理。通过定量分析由此产生的训练与测试阶段之间的差异，你将从根本上理解为何保持激活值的期望值恒定对于保证模型性能至关重要。[@problem_id:3118056]", "problem": "一个单隐藏单元线性网络在一个无噪声回归任务上进行训练，使用 dropout 作为正则化方法，但其反向 dropout 的实现是错误的（没有在训练时进行缩放）。输入特征 $x$ 是一个随机变量，满足 $\\mathbb{E}[x]=0$ 和 $\\operatorname{Var}(x)=\\sigma_x^2$，真实目标是 $y=\\beta x$。网络输出为 $\\hat{y}=v\\,(m\\,u\\,x)$，其中 $u$ 和 $v$ 是权重，$m\\sim\\text{Bernoulli}(p)$ 是一个独立的 dropout 掩码，以保留概率 $p$ 应用于隐藏层的激活值。通常情况下，反向 dropout 会在训练时对激活值进行缩放，以保持其在训练和测试之间的期望值不变，但此处在训练期间没有使用任何缩放。假设在这种错误的训练程序下，对于每个固定的 $x$，优化器已经达到了 $\\mathbb{E}_m[\\hat{y}\\,|\\,x]=y$ 的状态。在测试时，dropout 被移除且不应用任何缩放（即，确定性地令 $m=1$）。\n\n给定 $p=0.5$，$\\beta=2$ 和 $\\sigma_x^2=4$，哪个选项正确给出了测试时的均方误差（Mean Squared Error (MSE)）并解释了观察到的训练-测试不匹配的来源？\n\nA. 测试时 MSE 为 $16$，由上偏引起：测试输出相对于训练时期望被放大了 $1/p$ 倍，因此 $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]=(\\beta/p)\\,x>\\beta x$。\n\nB. 测试时 MSE 为 $4$，纯粹由掩码带来的方差引起；不存在偏差，因为测试时期望值是匹配的。\n\nC. 测试时 MSE 为 $0$，因为将训练期望与目标匹配可以保证在测试时做出完美的预测。\n\nD. 测试时 MSE 为 $4$，由下偏引起：测试输出相对于训练时期望被衰减了。", "solution": "用户希望我验证问题陈述，如果有效，则推导一个使用非标准 dropout 实现训练的单单元线性网络的测试时均方误差（MSE）。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   网络模型：一个单隐藏单元线性网络。\n-   任务：无噪声回归。\n-   输入：一个随机变量 $x$，满足 $\\mathbb{E}[x]=0$ 和 $\\operatorname{Var}(x)=\\sigma_x^2$。\n-   真实目标：$y=\\beta x$。\n-   网络输出：$\\hat{y}=v\\,(m\\,u\\,x)$，其中 $u$ 和 $v$ 是标量权重。\n-   Dropout 掩码：$m\\sim\\text{Bernoulli}(p)$，一个独立的随机变量，保留概率为 $p$。\n-   训练过程：错误的反向 dropout；训练期间不使用缩放。\n-   训练结果：对于每个固定的 $x$，优化器已达到 $\\mathbb{E}_m[\\hat{y}\\,|\\,x]=y$。\n-   测试过程：移除 Dropout（确定性地令 $m=1$），且不应用缩放。\n-   常数：$p=0.5$，$\\beta=2$，$\\sigma_x^2=4$。\n-   问题：确定测试时的 MSE 以及训练-测试不匹配的来源。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学基础**：该问题牢固地基于神经网络和正则化技术（特别是 dropout）的原理。它描述了一个合理但错误的实现场景，用以测试概念性理解。所有概念（MSE、期望、方差、伯努利分布、偏差）都是统计学和机器学习中的标准概念。\n-   **适定性**：该问题是适定的。训练目标被明确陈述，从而可以确定学习到的参数的有效值。测试时条件也是明确的。可以从给定信息中推导出一个唯一、稳定且有意义的解（即测试 MSE）。\n-   **客观性**：该问题以精确、客观的数学语言陈述。\n-   **完整性和一致性**：该问题是自洽的。它提供了求解所需量所必需的所有参数（$p$, $\\beta$, $\\sigma_x^2$）和条件。没有内部矛盾。\n\n**步骤 3：结论与行动**\n问题陈述是**有效的**。我将继续进行推导和求解。\n\n### 推导\n\n**1. 分析训练阶段**\n\n训练期间的网络输出由 $\\hat{y}_{\\text{train}} = v(m u x)$ 给出。优化器调整权重 $u$ 和 $v$ 以满足对于任意给定的 $x$，条件 $\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = y$ 成立。我们来计算这个期望。期望是针对随机 dropout 掩码 $m$ 计算的。\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = \\mathbb{E}_m[v m u x \\,|\\, x]\n$$\n\n由于 $v$、$u$ 和 $x$ 相对于 $m$ 的期望被视为常数，我们有：\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = v u x \\, \\mathbb{E}[m]\n$$\n\nDropout 掩码 $m$ 服从伯努利分布，$m \\sim \\text{Bernoulli}(p)$。伯努利随机变量的期望是其成功概率，因此 $\\mathbb{E}[m] = p$。\n\n将此代回，我们得到：\n\n$$\n\\mathbb{E}_m[\\hat{y}_{\\text{train}} \\,|\\, x] = p v u x\n$$\n\n训练条件规定，这个期望输出必须等于真实目标 $y = \\beta x$。\n\n$$\np v u x = \\beta x\n$$\n\n为了使这个等式对所有 $x$ 的值都成立，等式两边 $x$ 的系数必须相等。令 $W = vu$ 为该线性网络的有效权重。\n\n$$\np W = \\beta \\implies W = \\frac{\\beta}{p}\n$$\n\n这就是网络在这种特定（且错误）的训练程序下学到的有效权重。\n\n**2. 分析测试阶段**\n\n在测试时，dropout 被禁用，这意味着掩码被确定性地设置为 $m=1$。不应用任何缩放。测试时输出 $\\hat{y}_{\\text{test}}$ 为：\n\n$$\n\\hat{y}_{\\text{test}} = v (1 \\cdot u x) = v u x = W x\n$$\n\n代入学习到的有效权重 $W = \\beta/p$：\n\n$$\n\\hat{y}_{\\text{test}} = \\left(\\frac{\\beta}{p}\\right) x\n$$\n\n**3. 计算测试时均方误差 (MSE)**\n\nMSE 是测试时预测与真实目标之间差的平方的期望值，其中期望是针对输入数据 $x$ 的分布计算的。\n\n$$\n\\text{MSE} = \\mathbb{E}_x[(\\hat{y}_{\\text{test}} - y)^2]\n$$\n\n代入 $\\hat{y}_{\\text{test}}$ 和 $y$ 的表达式：\n\n$$\n\\text{MSE} = \\mathbb{E}_x\\left[ \\left( \\left(\\frac{\\beta}{p}\\right)x - \\beta x \\right)^2 \\right]\n$$\n\n提出因子 $\\beta$ 和 $x$：\n\n$$\n\\text{MSE} = \\mathbb{E}_x\\left[ \\left( \\beta \\left(\\frac{1}{p} - 1\\right) x \\right)^2 \\right] = \\mathbb{E}_x\\left[ \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 x^2 \\right]\n$$\n\n由于 $\\beta$ 和 $p$ 是常数，我们可以将它们从期望中提出：\n\n$$\n\\text{MSE} = \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 \\mathbb{E}_x[x^2]\n$$\n\n给定 $\\operatorname{Var}(x) = \\sigma_x^2$ 和 $\\mathbb{E}[x]=0$。方差定义为 $\\operatorname{Var}(x) = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2$。因此，$\\sigma_x^2 = \\mathbb{E}[x^2] - 0^2$，这意味着 $\\mathbb{E}[x^2] = \\sigma_x^2$。\n\n将此代入 MSE 公式：\n\n$$\n\\text{MSE} = \\beta^2 \\left(\\frac{1}{p} - 1\\right)^2 \\sigma_x^2\n$$\n\n**4. 代入数值**\n\n给定 $p=0.5$，$\\beta=2$ 和 $\\sigma_x^2=4$。\n\n$$\n\\text{MSE} = (2)^2 \\left(\\frac{1}{0.5} - 1\\right)^2 (4)\n$$\n\n$$\n\\text{MSE} = 4 \\left(2 - 1\\right)^2 (4)\n$$\n\n$$\n\\text{MSE} = 4 (1)^2 (4)\n$$\n\n$$\n\\text{MSE} = 16\n$$\n\n**5. 分析误差来源（训练-测试不匹配）**\n\n在训练期间，网络的期望输出与目标匹配：$\\mathbb{E}_m[\\hat{y}_{\\text{train}}\\,|\\,x] = y = \\beta x$。在测试时，确定性输出为 $\\hat{y}_{\\text{test}} = (\\beta/p)x$。由于 $p=0.5$，我们有 $1/p = 2$。所以，$\\hat{y}_{\\text{test}} = (\\beta/0.5)x = 2\\beta x$。真实目标仍然是 $y = \\beta x$。\n\n测试时输出始终比真实目标大 $1/p = 2$ 倍。这种系统性的过高预测是一种条件偏差，其中 $\\mathbb{E}[\\hat{y}_{\\text{test}} - y \\,|\\, x] = (\\beta/p)x - \\beta x \\neq 0$。发生这种情况是因为标准的反向 dropout 做法是在*训练*期间将激活值乘以 $1/p$ 进行缩放，以确保期望的激活值大小与没有 dropout 时保持相同。由于没有这样做，网络权重 $W=vu$ 被“膨胀”了 $1/p$ 倍以进行补偿。在测试时，当 dropout 掩码被移除时，这些膨胀的权重导致输出被放大。这是一种“上偏”或输出信号的放大。\n\n### 逐项分析选项\n\n**A. 测试时 MSE 为 $16$，由上偏引起：测试输出相对于训练时期望被放大了 $1/p$ 倍，因此 $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]=(\\beta/p)\\,x>\\beta x$。**\n-   计算出的测试时 MSE 确实是 $16$。\n-   原因被正确地识别为上偏（放大）。\n-   放大因子被正确地识别为 $1/p$。测试输出 $\\hat{y}_{\\text{test}} = (\\beta/p)x$ 与训练目标 $y = \\beta x$（即训练时期望）进行比较。\n-   所示关系为 $\\hat{y}_{\\text{test}} = (\\beta/p)x > \\beta x = y$（对于 $x>0$），正确描述了放大效应。使用 $\\mathbb{E}[\\hat{y}_{\\text{test}}\\,|\\,x]$ 等同于 $\\hat{y}_{\\text{test}}$，因为对于给定的 $x$，测试输出是确定性的。\n-   **结论：正确。**\n\n**B. 测试时 MSE 为 $4$，纯粹由掩码带来的方差引起；不存在偏差，因为测试时期望值是匹配的。**\n-   MSE 是 $16$，不是 $4$。\n-   误差来源是系统性偏差，而不是方差。测试时不使用 dropout 掩码，因此它不能成为测试时方差的来源。\n-   条件期望在测试时不匹配；$\\hat{y}_{\\text{test}} = (\\beta/p)x \\neq \\beta x = y$。\n-   **结论：错误。**\n\n**C. 测试时 MSE 为 $0$，因为将训练期望与目标匹配可以保证在测试时做出完美的预测。**\n-   MSE 是 $16$，不是 $0$。\n-   推理存在缺陷。如果模型的行为在训练和测试之间发生变化（例如，由于移除了随机掩码而没有适当的缩放补偿），那么在训练期间匹配期望输出并不能保证在测试时得到正确的预测。\n-   **结论：错误。**\n\n**D. 测试时 MSE 为 $4$，由下偏引起：测试输出相对于训练时期望被衰减了。**\n-   MSE 是 $16$，不是 $4$。\n-   偏差是上偏（放大），而不是下偏（衰减），因为 $p=0.5 < 1$，这使得 $1/p = 2 > 1$。\n-   **结论：错误。**", "answer": "$$\\boxed{A}$$", "id": "3118056"}, {"introduction": "在复杂的神经网络架构中，我们很少孤立地使用某种技术，而是将多个模块组合在一起。随机失活（Dropout）和批归一化（Batch Normalization）是两种最常用的层，但它们的相对顺序是一个关键的设计决策。这个实践引导你通过一个思想实验，分析将随机失活置于批归一化之前或之后所产生的不同统计后果，从而诊断训练和推理模式之间的分布偏移。理解这种相互作用对于构建稳定且表现优异的深度模型至关重要。[@problem_id:3118023]", "problem": "给定两个深度神经网络块，它们在其他方面完全相同，唯一的区别在于 dropout 层相对于批量归一化 (BN) 层的位置。批量归一化 (BN) 指的是一种标准变换，在训练期间，它使用批次均值和批次方差对每个通道进行归一化；在推理期间，它使用训练期间累积的运行（指数平均）均值和方差进行归一化，之后再进行一次学习到的仿射变换。Dropout 在训练期间使用常见的“反向”实现：每个单元以概率 $p$ 被保留并按 $1/p$ 进行缩放，而在推理期间 dropout 则被禁用。考虑一个通用的预激活值 $x$ 进入每个块中的 BN 层，并假设 dropout 掩码与 $x$ 无关。您将针对每个块和每个相关张量位置，在训练模式和推理模式下，对留出数据测量经验性的逐层统计量 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$，以诊断分布偏移。这两个块是：\n- 块 A（“dropout 在 BN 之前”）：前一层的激活值 $\\to$ 保留概率为 $p$ 且带反向缩放的 dropout $\\to$ BN $\\to$ 剩余层。\n- 块 B（“dropout 在 BN 之后”）：前一层的激活值 $\\to$ BN $\\to$ 保留概率为 $p$ 且带反向缩放的 dropout $\\to$ 剩余层。\n假设这些块的上游输入具有有限的一阶矩和二阶矩，并且训练使用足够大的批次，以使 BN 的运行统计量收敛到相应的训练时批次统计量。在这些假设下，当比较块 A 与块 B 在训练与推理模式下，于 BN 输入和 BN 输出处测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 时，哪个陈述最能预测您将观察到的模式？\n\nA. 在块 A 中，BN 的运行方差被训练期间遇到的掩码噪声所放大，因此在推理时，BN 输入的方差小于运行方差，而 BN 输出的方差收缩至约 $p$（小于 $1$），同时均值保持在约 $0$。在块 B 中，BN 的统计量在训练和推理之间，在 BN 输入和 BN 输出处都非常匹配，因此它们的均值和方差是一致的；任何训练-推理的方差差异仅出现在 dropout 层之后，其中训练方差大约大 $1/p$ 倍。\n\nB. 在块 A 中，BN 消除了任何训练-推理不匹配，因此 BN 输入和 BN 输出在训练和推理之间的均值和方差完全匹配。在块 B 中，将 dropout 放在 BN 之后会在推理时于 BN 输出处引起一个与 $(1-p)$ 成比例的均值偏移。\n\nC. 在块 A 中，BN 前的掩码操作使 BN 的运行均值向 $0$ 产生偏差，因此在推理时，BN 输出存在非零的均值不匹配，而其方差匹配。在块 B 中，BN 后的掩码操作使 BN 的运行方差向上产生偏差，因此在推理时，BN 输出的方差会放大 $1/p$ 倍。\n\nD. 对于反向 dropout，位置无关紧要：对于任何 $p$，每一层的期望和方差在训练和推理之间都是匹配的，因此在两个块的 BN 输入和输出处，所有测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 都重合。", "solution": "用户希望我分析涉及批量归一化 (BN) 和 Dropout 的两个神经网络结构块的统计特性，特别关注训练和推理模式之间的分布偏移。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   **块 A（“dropout 在 BN 之前”）：** 前一层的激活值 $\\to$ dropout $\\to$ BN $\\to$ 剩余层。\n-   **块 B（“dropout 在 BN 之后”）：** 前一层的激活值 $\\to$ BN $\\to$ dropout $\\to$ 剩余层。\n-   **批量归一化 (BN)：**\n    -   训练：使用批次均值/方差进行归一化。之后进行学习到的仿射变换 $z = \\gamma \\hat{x} + \\beta$。\n    -   推理：使用训练期间累积的运行均值/方差进行归一化。之后进行相同的仿射变换。\n-   **Dropout：**\n    -   “反向”实现。\n    -   训练：每个单元以概率 $p$ 被保留，并按 $1/p$ 缩放。否则置零。\n    -   推理：Dropout 是一个恒等函数（被禁用）。\n-   **输入：** 一个通用的预激活值，我们称之为 $y$，进入该块。dropout 掩码与 $y$ 无关。\n-   **任务：** 比较两个块在训练与推理模式下，BN 输入和 BN 输出处的经验性逐层统计量 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$。\n-   **假设：**\n    1.  上游输入 $y$ 具有有限的一阶矩和二阶矩。设 $\\operatorname{E}[y] = \\mu_y$ 且 $\\operatorname{Var}[y] = \\sigma_y^2$。\n    2.  训练使用足够大的批次，以使 BN 的运行统计量收敛到相应的训练时批次统计量。这意味着 BN 存储的运行均值 $\\mu_{\\text{run}}$ 和运行方差 $\\sigma^2_{\\text{run}}$ 等于训练期间进入 BN 层的张量的总体均值和方差。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学基础：** 该问题在深度学习和统计学原理上有坚实的基础。对批量归一化和反向 Dropout 的描述是标准的。这些层之间的相互作用在设计神经网络结构时是一个已知且重要的实践和理论考量。\n-   **问题定义良好：** 问题定义清晰。两个块、各种操作以及分析目标都得到了明确的规定。关于大批次和收敛统计量的假设使得问题可解析处理，并导向唯一的结论。\n-   **目标：** 该问题要求对统计矩进行严格的、定量的比较，这是一项客观的任务。\n\n**步骤 3：结论与行动**\n问题陈述是有效的。这是一个在深度学习理论领域中定义良好的问题。我将进行形式化推导。\n\n### 推导\n\n设 $y$ 是块的输入，其 $\\operatorname{E}[y] = \\mu_y$ 且 $\\operatorname{Var}[y] = \\sigma_y^2$。\n设 $m$ 是用于 dropout 的伯努利随机变量，$P(m=1) = p$，且与 $y$ 无关。\nBN 层有可学习的参数 $\\gamma$ 和 $\\beta$。为简单起见且不失一般性，我们通常会分析仿射变换之前的归一化输出（其均值为 $0$，方差为 $1$），然后考虑 $\\gamma$ 和 $\\beta$ 的影响。因此，训练期间 BN 之后的目标均值和方差分别为 $\\beta$ 和 $\\gamma^2$。\n\n#### 对块 A（Dropout $\\to$ BN）的分析\n\n设该块为 $y \\xrightarrow{\\text{Dropout}} x \\xrightarrow{\\text{BN}} z$。张量 $x$ 是 BN 层的输入。\n\n**1. 训练模式：**\n-   BN 层的输入是 $x_{\\text{train}} = y \\cdot \\frac{m}{p}$。\n-   我们计算其统计量，根据假设，这些统计量将成为 BN 的运行统计量。\n-   **均值：** $\\operatorname{E}[x_{\\text{train}}] = \\operatorname{E}[y \\cdot \\frac{m}{p}] = \\operatorname{E}[y]\\operatorname{E}[\\frac{m}{p}] = \\mu_y \\cdot \\frac{\\operatorname{E}[m]}{p} = \\mu_y \\cdot \\frac{p}{p} = \\mu_y$。\n    运行均值为 $\\mu_{\\text{run}} = \\mu_y$。\n-   **方差：** 为了计算 $\\operatorname{Var}[x_{\\text{train}}]$，我们首先求 $\\operatorname{E}[x_{\\text{train}}^2]$。\n    $\\operatorname{E}[x_{\\text{train}}^2] = \\operatorname{E}[(y \\frac{m}{p})^2] = \\operatorname{E}[y^2 \\frac{m^2}{p^2}] = \\operatorname{E}[y^2]\\operatorname{E}[\\frac{m^2}{p^2}]$。\n    因为 $m \\in \\{0, 1\\}$，所以 $m^2=m$，因此 $\\operatorname{E}[m^2]=\\operatorname{E}[m]=p$。\n    $\\operatorname{E}[x_{\\text{train}}^2] = \\operatorname{E}[y^2] \\frac{p}{p^2} = \\frac{1}{p}\\operatorname{E}[y^2] = \\frac{1}{p}(\\sigma_y^2 + \\mu_y^2)$。\n    $\\operatorname{Var}[x_{\\text{train}}] = \\operatorname{E}[x_{\\text{train}}^2] - (\\operatorname{E}[x_{\\text{train}}])^2 = \\frac{1}{p}(\\sigma_y^2 + \\mu_y^2) - \\mu_y^2 = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1}{p} - 1) = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$。\n    运行方差为 $\\sigma_{\\text{run}}^2 = \\frac{\\sigma_y^2}{p} + \\mu_y^2(\\frac{1-p}{p})$。这个方差被 dropout 噪声“放大”了，因为 $1/p > 1$。\n-   **BN 输出：** 训练期间的 BN 输出 $z_{\\text{train}}$ 将有 $\\operatorname{E}[z_{\\text{train}}] \\approx \\beta$ 和 $\\operatorname{Var}[z_{\\text{train}}] \\approx \\gamma^2$。\n\n**2. 推理模式：**\n-   Dropout 被禁用。BN 层的输入是 $x_{\\text{inf}} = y$。\n-   **BN 输入处的统计量：** $\\operatorname{E}[x_{\\text{inf}}] = \\mu_y$ 且 $\\operatorname{Var}[x_{\\text{inf}}] = \\sigma_y^2$。\n-   **BN 输入处训练与推理的对比：**\n    -   均值匹配：$\\operatorname{E}[x_{\\text{inf}}] = \\operatorname{E}[x_{\\text{train}}] = \\mu_y$。\n    -   方差不匹配：$\\operatorname{Var}[x_{\\text{inf}}] = \\sigma_y^2 < \\sigma_{\\text{run}}^2 = \\operatorname{Var}[x_{\\text{train}}]$。BN 存储的运行方差是对真实推理时输入方差的高估。\n-   **BN 输出：** BN 层使用存储的运行统计量对 $x_{\\text{inf}}$ 进行归一化：\n    $z_{\\text{inf}} = \\gamma \\frac{x_{\\text{inf}} - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta$。\n-   **BN 输出的统计量：**\n    -   均值：$\\operatorname{E}[z_{\\text{inf}}] = \\frac{\\gamma}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\\operatorname{E}[y - \\mu_y] + \\beta = \\beta$。输出的均值与训练时一致。\n    -   方差：$\\operatorname{Var}[z_{\\text{inf}}] = \\operatorname{Var}[\\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}] = \\frac{\\gamma^2}{\\sigma_{\\text{run}}^2 + \\epsilon}\\operatorname{Var}[y] = \\frac{\\gamma^2 \\sigma_y^2}{\\sigma_{\\text{run}}^2 + \\epsilon}$。\n    -   由于 $\\sigma_{\\text{run}}^2 > \\sigma_y^2$，我们有 $\\operatorname{Var}[z_{\\text{inf}}] < \\gamma^2 = \\operatorname{Var}[z_{\\text{train}}]$。推理时的输出方差小于训练时。\n    -   为简单起见，如果我们假设输入是中心化的（$\\mu_y=0$），那么 $\\sigma_{\\text{run}}^2 = \\sigma_y^2/p$。输出方差变为 $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\frac{\\gamma^2 \\sigma_y^2}{\\sigma_y^2/p} = \\gamma^2 p$。方差按因子 $p$ 收缩。\n\n**块 A 总结：** 发生了训练-推理分布偏移。BN 层使用了来自训练的被放大的方差估计，导致它错误地归一化了推理数据，从而使其输出的方差收缩。\n\n#### 对块 B（BN $\\to$ Dropout）的分析\n\n设该块为 $y \\xrightarrow{\\text{BN}} x \\xrightarrow{\\text{Dropout}} z$。张量 $x$ 现在是 BN 层的输出。\n\n**1. 训练模式：**\n-   BN 层的输入是 $y$。\n-   它计算并存储来自 $y$ 的统计量。\n-   运行均值：$\\mu_{\\text{run}} = \\operatorname{E}[y] = \\mu_y$。\n-   运行方差：$\\sigma_{\\text{run}}^2 = \\operatorname{Var}[y] = \\sigma_y^2$。\n-   **BN 输出 / Dropout 输入：** $x_{\\text{train}}$ 将有 $\\operatorname{E}[x_{\\text{train}}] \\approx \\beta$ 和 $\\operatorname{Var}[x_{\\text{train}}] \\approx \\gamma^2$。\n-   **Dropout 输出：** 最终输出为 $z_{\\text{train}} = x_{\\text{train}} \\cdot \\frac{m}{p}$。其方差将被 dropout 噪声放大，$\\operatorname{Var}[z_{\\text{train}}] \\approx \\frac{\\gamma^2}{p} + \\beta^2(\\frac{1-p}{p})$。\n\n**2. 推理模式：**\n-   Dropout 被禁用。\n-   **BN 输入：** 输入仍然是 $y$。其统计量与训练时相比没有变化。因此，在 BN 输入处，没有训练-推理分布偏移。\n-   **BN 输出 / Dropout 输入：** BN 层使用 $\\mu_{\\text{run}} = \\mu_y$ 和 $\\sigma_{\\text{run}}^2 = \\sigma_y^2$ 对 $y$ 进行归一化。\n    $x_{\\text{inf}} = \\gamma \\frac{y - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}} + \\beta = \\gamma \\frac{y - \\mu_y}{\\sqrt{\\sigma_y^2 + \\epsilon}} + \\beta$。\n-   **BN 输出的统计量：**\n    -   均值：$\\operatorname{E}[x_{\\text{inf}}] \\approx \\beta$。\n    -   方差：$\\operatorname{Var}[x_{\\text{inf}}] \\approx \\gamma^2$。\n-   **BN 输出处训练与推理的对比：** $x$（BN 输出）的统计量在训练和推理之间匹配：$\\operatorname{E}[x_{\\text{train}}] \\approx \\operatorname{E}[x_{\\text{inf}}]$ 且 $\\operatorname{Var}[x_{\\text{train}}] \\approx \\operatorname{Var}[x_{\\text{inf}}]$。\n-   **Dropout 输出：** Dropout 是一个恒等映射，所以 $z_{\\text{inf}} = x_{\\text{inf}}$。最终输出有 $\\operatorname{E}[z_{\\text{inf}}] \\approx \\beta$ 和 $\\operatorname{Var}[z_{\\text{inf}}] \\approx \\gamma^2$。\n-   训练-推理的差异仅出现在 dropout 层*之后*，其中 $\\operatorname{Var}[z_{\\text{train}}] > \\operatorname{Var}[z_{\\text{inf}}]$。\n\n**块 B 总结：** 在 BN 层的输入或输出处没有分布偏移。BN 层在训练和测试中看到相同的数据分布，其存储的统计量是合适的。训练-推理的差异被隔离在 Dropout 层本身的输出处。\n\n### 逐项分析选项\n\n**A. 在块 A 中，BN 的运行方差被训练期间遇到的掩码噪声所放大，因此在推理时，BN 输入的方差小于运行方差，而 BN 输出的方差收缩至约 $p$（小于 $1$），同时均值保持在约 $0$。在块 B 中，BN 的统计量在训练和推理之间，在 BN 输入和 BN 输出处都非常匹配，因此它们的均值和方差是一致的；任何训练-推理的方差差异仅出现在 dropout 层之后，其中训练方差大约大 $1/p$ 倍。**\n-   对块 A 的描述与我们的分析完全一致。运行方差被噪声放大。这导致 BN 输出方差在推理时收缩约 $p$ 倍。均值保持稳定。\n-   对块 B 的描述也完全一致。BN 层的输入和输出统计量在训练和推理之间是稳定的。方差不匹配被隔离在 dropout 层的输出处，其中训练方差被放大了约 $1/p$ 倍。\n-   **结论：正确。**\n\n**B. 在块 A 中，BN 消除了任何训练-推理不匹配，因此 BN 输入和 BN 输出在训练和推理之间的均值和方差完全匹配。在块 B 中，将 dropout 放在 BN 之后会在推理时于 BN 输出处引起一个与 $(1-p)$ 成比例的均值偏移。**\n-   关于块 A 的陈述不正确。BN 没有消除不匹配；由于使用了不正确的运行统计量，它反而是其输出处不匹配的来源。在 BN 输出处存在方差不匹配。\n-   关于块 B 的陈述不正确。反向 dropout 保留了均值。没有均值偏移。\n-   **结论：不正确。**\n\n**C. 在块 A 中，BN 前的掩码操作使 BN 的运行均值向 $0$ 产生偏差，因此在推理时，BN 输出存在非零的均值不匹配，而其方差匹配。在块 B 中，BN 后的掩码操作使 BN 的运行方差向上产生偏差，因此在推理时，BN 输出的方差会放大 $1/p$ 倍。**\n-   关于块 A 的陈述不正确。反向 dropout 确保运行均值是无偏估计（$\\operatorname{E}[x_{\\text{train}}] = \\mu_y$）。它还错误地声称方差匹配，而实际上方差是收缩的。\n-   关于块 B 的陈述不正确。掩码操作发生在 BN *之后*，因此它不能使 BN 的运行方差产生偏差。BN 输出的方差在推理时与训练时匹配，并不会放大。\n-   **结论：不正确。**\n\n**D. 对于反向 dropout，位置无关紧要：对于任何 $p$，每一层的期望和方差在训练和推理之间都是匹配的，因此在两个块的 BN 输入和输出处，所有测量的 $\\operatorname{E}[x]$ 和 $\\operatorname{Var}[x]$ 都重合。**\n-   这个陈述显然是错误的。我们的分析表明，位置非常重要。块 A 在 BN 输出处存在方差不匹配，而块 B 则没有。反向 dropout 保留了一阶矩（均值）但没有保留二阶矩（方差），这是观察到的现象的根本原因。\n-   **结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "3118023"}, {"introduction": "随机失活的本质是在网络中引入随机性，但这种随机性如何与赋予神经网络强大能力的非线性激活函数相互作用？这个更深入的练习探讨了标准确定性推理规则（即在测试时关闭随机失活）与“真实”随机预测均值之间的偏差。通过运用泰勒展开，你将把这种偏差与激活函数的曲率联系起来，从而更深刻地理解随机失活作为一种近似模型集成的作用。[@problem_id:3118065]", "problem": "考虑一个神经网络中的单个标量隐藏单元，其激活前的值为 $a \\in \\mathbb{R}$，非线性函数为 $\\phi:\\mathbb{R}\\to\\mathbb{R}$。在推理时，假设通过采样一个保留概率为 $p \\in (0,1]$ 的伯努利掩码 $z \\sim \\text{Bernoulli}(p)$ 来显式地对 dropout 进行建模，并使用反向 dropout 缩放，使得随机输出为 $y(z) = \\phi\\!\\left(\\frac{z}{p}\\,a\\right)$。两种常见的推理规则是：\n- 确定性缩放（无采样）：使用 $y_{\\text{det}} = \\phi(a)$。\n- 蒙特卡洛 (MC) dropout：通过对 $N \\in \\mathbb{N}$ 个独立采样的掩码进行平均，来近似随机预测均值 $y_{\\star} = \\mathbb{E}_{z}\\big[\\phi\\!\\left(\\frac{z}{p}\\,a\\right)\\big]$，即 $y_{\\text{MC}} = \\frac{1}{N}\\sum_{i=1}^{N} \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$。\n\n除非另有说明，假设 $\\phi$ 在 $a$ 的一个邻域内是二阶连续可微的。仅使用期望和方差的定义、Jensen 不等式以及 $\\phi$ 在 $a$ 点的二阶泰勒近似来对偏差和曲率进行推理。选择所有正确的陈述。\n\nA. 对于一个 $\\phi''(a) \\neq 0$ 的二阶可微函数 $\\phi$，确定性规则 $y_{\\text{det}}$ 相对于 $y_{\\star}$ 是有偏的，其主阶偏差为\n$$\n\\mathbb{E}\\big[\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big] - \\phi(a) \\approx \\tfrac{1}{2}\\,\\mathrm{Var}\\!\\left(\\tfrac{z}{p}\\,a\\right)\\,\\phi''(a) = \\tfrac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a).\n$$\n\nB. 除非 $N \\to \\infty$，否则 MC 估计量 $y_{\\text{MC}}$ 是有偏的；对于凸函数 $\\phi$，其期望值超过 $y_{\\star}$。\n\nC. 如果 $\\phi$ 是线性的，那么对于任何 $p \\in (0,1]$ 和任何 $a \\in \\mathbb{R}$，都有 $y_{\\text{det}} = y_{\\star}$ 精确成立。\n\nD. 保持 $p$ 和 $a$ 不变，确定性偏差的大小 $\\big|y_{\\star} - y_{\\text{det}}\\big|$ 随着曲率 $\\big|\\phi''(a)\\big|$ 的增加而减小，因为平均平滑了非线性。\n\nE. 对于修正线性单元 $\\phi(u)=\\max(0,u)$，对于所有的 $a \\in \\mathbb{R}$ 和 $p \\in (0,1]$，都有 $y_{\\text{det}} = y_{\\star}$。\n\nF. 对于任何 $N \\in \\mathbb{N}$，MC 估计量 $y_{\\text{MC}}$ 是 $y_{\\star}$ 的一个无偏估计量，其方差按 $\\mathrm{Var}(y_{\\text{MC}}) = \\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)/N$ 缩放，因此增加 $N$ 会减少由采样引起的误差，而不会引入偏差；更高的曲率通常会通过非线性对输入变异性的放大作用来增加 $\\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)$。", "solution": "用户希望对一个关于神经网络中 dropout 推理的问题陈述进行批判性分析，并在问题有效的情况下给出解答。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 一个单个标量隐藏单元，其激活前的值为 $a \\in \\mathbb{R}$，非线性函数为 $\\phi:\\mathbb{R}\\to\\mathbb{R}$。\n- 在推理时，采样一个保留概率为 $p \\in (0,1]$ 的伯努利掩码 $z \\sim \\text{Bernoulli}(p)$。\n- 使用反向 dropout 缩放，得到随机输出 $y(z) = \\phi\\!\\left(\\frac{z}{p}\\,a\\right)$。\n- 定义了两种推理规则：\n    1. 确定性缩放：$y_{\\text{det}} = \\phi(a)$。\n    2. 蒙特卡洛 (MC) dropout：$y_{\\text{MC}} = \\frac{1}{N}\\sum_{i=1}^{N} \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$，其中 $z_i$ 为 $N \\in \\mathbb{N}$ 个独立采样的掩码。\n- 目标量是随机预测均值：$y_{\\star} = \\mathbb{E}_{z}\\big[\\phi\\!\\left(\\frac{z}{p}\\,a\\right)\\big]$。\n- 假设：除非另有说明，$\\phi$ 在 $a$ 的一个邻域内是二阶连续可微的。\n- 允许使用的工具：期望和方差的定义、Jensen 不等式以及 $\\phi$ 在 $a$ 点的二阶泰勒近似。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学上成立：** 该问题描述了对 dropout（一种在深度学习中广泛使用的正则化技术）的标准理论分析。反向 dropout、MC dropout、确定性近似以及通过泰勒级数对它们进行的分析等概念，对于理解 dropout 的行为至关重要。该问题在科学上是合理的。\n- **提法明确：** 所有术语（$y(z)$, $y_{\\text{det}}$, $y_{\\text{MC}}$, $y_{\\star}$, $p$, $a$, $z$）都有清晰无歧义的定义。问题要求基于这些定义评估几个陈述的正确性，这是一个定义明确的任务。\n- **客观性：** 问题以精确的数学语言陈述，没有主观性或歧义。\n\n**缺陷检查清单：**\n1.  **科学或事实不成立：**无。该设定是用于分析的标准且有效的简化。\n2.  **无法形式化或不相关：**无。该问题是一个与其所述主题直接相关的形式化数学练习。\n3.  **不完整或矛盾的设定：**无。所有必要信息都已提供。\n4.  **不切实际或不可行：**无。这是一个计算过程的理论模型。\n5.  **提法不当或结构不良：**无。\n6.  **故作高深、无聊或同义反复：**无。该问题需要概率论和微积分的非平凡应用。\n7.  **超出科学可验证性范围：**无。所有陈述在数学上都是可验证的。\n\n**步骤3：结论与行动**\n问题陈述是**有效的**。开始进行解答。\n\n### 解答推导\n\n首先，我们确定所涉及的随机变量的性质。非线性函数的输入是随机变量 $X = \\frac{z}{p}a$，其中 $z \\sim \\text{Bernoulli}(p)$。随机变量 $z$ 以概率 $p$ 取值 $1$，以概率 $1-p$ 取值 $0$。\n因此，$X$ 以概率 $p$ 取值 $\\frac{1}{p}a$，以概率 $1-p$ 取值 $\\frac{0}{p}a = 0$。\n\n$X$ 的期望是：\n$$ \\mathbb{E}[X] = \\mathbb{E}\\left[\\frac{z}{p}a\\right] = \\frac{a}{p}\\mathbb{E}[z] = \\frac{a}{p} \\cdot p = a $$\n这证实了反向 dropout 缩放在训练时保持了激活前的值的均值。\n\n$X$ 的方差是：\n$$ \\mathrm{Var}(X) = \\mathrm{Var}\\left(\\frac{z}{p}a\\right) = \\left(\\frac{a}{p}\\right)^2 \\mathrm{Var}(z) = \\frac{a^2}{p^2} \\left(p(1-p)\\right) = a^2 \\frac{1-p}{p} $$\n\n目标量是 $y_{\\star} = \\mathbb{E}[\\phi(X)]$。使用无意识统计学家法则（或离散随机变量期望的定义）：\n$$ y_{\\star} = \\phi\\left(\\frac{a}{p}\\right) \\cdot p + \\phi(0) \\cdot (1-p) $$\n\n确定性规则给出 $y_{\\text{det}} = \\phi(a) = \\phi(\\mathbb{E}[X])$。\n\n确定性规则的偏差是 $y_{\\star} - y_{\\text{det}} = \\mathbb{E}[\\phi(X)] - \\phi(\\mathbb{E}[X])$。对于一个非线性函数 $\\phi$，Jensen 不等式表明这个偏差是非零的。\n\n现在我们评估每个选项。\n\n**A. 对于一个 $\\phi''(a) \\neq 0$ 的二阶可微函数 $\\phi$，确定性规则 $y_{\\text{det}}$ 相对于 $y_{\\star}$ 是有偏的，其主阶偏差为 $\\mathbb{E}\\big[\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big] - \\phi(a) \\approx \\tfrac{1}{2}\\,\\mathrm{Var}\\!\\left(\\tfrac{z}{p}\\,a\\right)\\,\\phi''(a) = \\tfrac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a)$。**\n\n为了近似这个偏差，我们使用 $\\phi(X)$ 在其均值 $\\mathbb{E}[X] = a$ 附近的二阶泰勒展开：\n$$ \\phi(X) \\approx \\phi(a) + (X-a)\\phi'(a) + \\frac{1}{2}(X-a)^2\\phi''(a) $$\n对两边取期望：\n$$ \\mathbb{E}[\\phi(X)] \\approx \\mathbb{E}\\left[\\phi(a) + (X-a)\\phi'(a) + \\frac{1}{2}(X-a)^2\\phi''(a)\\right] $$\n利用期望的线性性质：\n$$ y_{\\star} \\approx \\phi(a) + \\mathbb{E}[X-a]\\phi'(a) + \\frac{1}{2}\\mathbb{E}[(X-a)^2]\\phi''(a) $$\n我们知道 $\\mathbb{E}[X-a] = \\mathbb{E}[X]-a = a-a = 0$，并且根据定义，$\\mathbb{E}[(X-a)^2] = \\mathrm{Var}(X)$。\n将这些代入近似式：\n$$ y_{\\star} \\approx \\phi(a) + 0 \\cdot \\phi'(a) + \\frac{1}{2}\\mathrm{Var}(X)\\phi''(a) $$\n因此，偏差为：\n$$ \\mathbb{E}\\big[\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big] - \\phi(a) = y_{\\star} - y_{\\text{det}} \\approx \\frac{1}{2}\\mathrm{Var}(X)\\phi''(a) $$\n代入 $\\mathrm{Var}(X) = \\mathrm{Var}\\!\\left(\\tfrac{z}{p}\\,a\\right) = a^2 \\frac{1-p}{p}$ 的表达式：\n$$ \\mathbb{E}\\big[\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big] - \\phi(a) \\approx \\frac{1}{2} \\left(a^2 \\frac{1-p}{p}\\right) \\phi''(a) = \\frac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a) $$\n该陈述与此推导完全匹配。\n**结论：正确。**\n\n**B. 除非 $N \\to \\infty$，否则 MC 估计量 $y_{\\text{MC}}$ 是有偏的；对于凸函数 $\\phi$ 其期望值超过 $y_{\\star}$。**\n\nMC 估计量定义为 $y_{\\text{MC}} = \\frac{1}{N}\\sum_{i=1}^{N} \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$，其中 $z_i$ 是来自 Bernoulli($p$) 的独立同分布 (i.i.d.) 样本。令 $Y_i = \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$。根据定义，每个 $Y_i$ 的期望都是 $y_{\\star}$。\n让我们计算估计量 $y_{\\text{MC}}$ 的期望：\n$$ \\mathbb{E}[y_{\\text{MC}}] = \\mathbb{E}\\left[ \\frac{1}{N}\\sum_{i=1}^{N} Y_i \\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[Y_i] $$\n由于每个 $Y_i$ 都有相同的分布，对于所有的 $i = 1, \\dots, N$，都有 $\\mathbb{E}[Y_i] = y_{\\star}$。\n$$ \\mathbb{E}[y_{\\text{MC}}] = \\frac{1}{N} \\sum_{i=1}^{N} y_{\\star} = \\frac{1}{N} (N \\cdot y_{\\star}) = y_{\\star} $$\n对于任何有限的 $N \\ge 1$，MC 估计量的期望都精确地是 $y_{\\star}$。因此，$y_{\\text{MC}}$ 是 $y_{\\star}$ 的一个无偏估计量。声称除非 $N \\to \\infty$ 否则它是有偏的，这个说法是错误的。该陈述的后半部分也是错误的，因为它的期望值等于 $y_{\\star}$，而不是大于它。\n**结论：不正确。**\n\n**C. 如果 $\\phi$ 是线性的，那么对于任何 $p \\in (0,1]$ 和任何 $a \\in \\mathbb{R}$，$y_{\\text{det}} = y_{\\star}$ 精确成立。**\n\n设 $\\phi$ 是一个线性函数，$\\phi(u) = c u + d$，其中 $c, d \\in \\mathbb{R}$ 是常数。\n目标值 $y_{\\star}$ 是：\n$$ y_{\\star} = \\mathbb{E}[\\phi(X)] = \\mathbb{E}[c X + d] $$\n根据期望的线性性质：\n$$ y_{\\star} = c \\mathbb{E}[X] + d $$\n我们已经证明了 $\\mathbb{E}[X] = a$。\n$$ y_{\\star} = c a + d $$\n确定性估计是：\n$$ y_{\\text{det}} = \\phi(a) = c a + d $$\n因此，$y_{\\star} = y_{\\text{det}}$ 精确成立。这个结果也可以从选项 A 的偏差近似中得到。对于一个线性函数，对所有 $u$ 都有 $\\phi''(u) = 0$。泰勒展开式只有一阶项且是精确的，所以偏差恰好为 $0$。\n**结论：正确。**\n\n**D. 保持 $p$ 和 $a$ 不变，确定性偏差的大小 $\\big|y_{\\star} - y_{\\text{det}}\\big|$ 随着曲率 $\\big|\\phi''(a)\\big|$ 的增加而减小，因为平均平滑了非线性。**\n\n从 A 中的分析可知，主阶确定性偏差为：\n$$ \\text{Bias} = y_{\\star} - y_{\\text{det}} \\approx \\frac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a) $$\n这个偏差的大小是：\n$$ |\\text{Bias}| \\approx \\left| \\frac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\,\\phi''(a) \\right| = \\left(\\frac{1}{2}\\,\\frac{1-p}{p}\\,a^{2}\\right) |\\phi''(a)| $$\n当 $p$ 和 $a$ 固定时（$a \\neq 0, p \\in (0,1)$），括号中的项是一个正常数。偏差的大小与曲率的大小 $|\\phi''(a)|$ 成正比。因此，随着 $|\\phi''(a)|$ 的增加，偏差的大小会*增加*，而不是减少。直观上讲，一个更弯曲的函数会更多地偏离其线性近似，$\\mathbb{E}[\\phi(X)]$ 和 $\\phi(\\mathbb{E}[X])$ 之间的差异会变得更大。\n**结论：不正确。**\n\n**E. 对于修正线性单元 $\\phi(u)=\\max(0,u)$，对于所有的 $a \\in \\mathbb{R}$ 和 $p \\in (0,1]$，都有 $y_{\\text{det}} = y_{\\star}$。**\n\nReLU 函数在 $u=0$ 处不是二阶可微的，所以我们必须使用 $y_{\\star}$ 的精确定义。\n随机变量 $X = \\frac{z}{p}a$ 以概率 $p$ 取值 $\\frac{a}{p}$，以概率 $1-p$ 取值 $0$。\n$$ y_{\\star} = \\mathbb{E}\\left[\\phi\\left(\\frac{z}{p}a\\right)\\right] = p \\cdot \\phi\\left(\\frac{a}{p}\\right) + (1-p) \\cdot \\phi(0) $$\n由于 $\\phi(0) = \\max(0,0)=0$，这可以简化为：\n$$ y_{\\star} = p \\cdot \\phi\\left(\\frac{a}{p}\\right) = p \\cdot \\max\\left(0, \\frac{a}{p}\\right) $$\n确定性估计是 $y_{\\text{det}} = \\phi(a) = \\max(0, a)$。\n我们必须检查是否对所有 $a$ 都有 $y_{\\star} = y_{\\text{det}}$。\n情况1：$a > 0$。由于 $p \\in (0,1]$，我们有 $a/p > 0$。\n- $y_{\\star} = p \\cdot \\max(0, a/p) = p \\cdot (a/p) = a$。\n- $y_{\\text{det}} = \\max(0, a) = a$。\n在这种情况下，$y_{\\star} = y_{\\text{det}}$。\n情况2：$a \\le 0$。由于 $p > 0$，我们有 $a/p \\le 0$。\n- $y_{\\star} = p \\cdot \\max(0, a/p) = p \\cdot 0 = 0$。\n- $y_{\\text{det}} = \\max(0, a) = 0$。\n在这种情况下，$y_{\\star} = y_{\\text{det}}$。\n由于等式对 $a > 0$ 和 $a \\le 0$ 都成立，所以该陈述对所有 $a \\in \\mathbb{R}$ 都为真。这个特殊的性质之所以出现，是因为随机输入的一个状态是 $0$，而这正是 ReLU 函数的拐点，并且对于正输入，该函数是线性的，我们已经知道在线性情况下等式成立。\n**结论：正确。**\n\n**F. 对于任何 $N \\in \\mathbb{N}$，MC 估计量 $y_{\\text{MC}}$ 是 $y_{\\star}$ 的一个无偏估计量，其方差按 $\\mathrm{Var}(y_{\\text{MC}}) = \\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)/N$ 缩放，因此增加 $N$ 会减少由采样引起的误差，而不会引入偏差；更高的曲率通常会通过非线性对输入变异性的放大作用来增加 $\\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)$。**\n\n这个陈述有多个部分。\n1. “$y_{\\text{MC}}$ 是 $y_{\\star}$ 的一个无偏估计量”：如选项 B 的分析所示，$\\mathbb{E}[y_{\\text{MC}}] = y_{\\star}$。这是正确的。\n2. “其方差按 $\\mathrm{Var}(y_{\\text{MC}}) = \\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)/N$ 缩放”：$y_{\\text{MC}}$ 是 $N$ 个独立同分布随机变量 $Y_i = \\phi\\!\\left(\\frac{z_i}{p}\\,a\\right)$ 的样本均值。样本均值的方差是总体方差除以样本大小。因此，$\\mathrm{Var}(y_{\\text{MC}}) = \\frac{\\mathrm{Var}(Y_1)}{N} = \\frac{\\mathrm{Var}(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right))}{N}$。这是正确的。\n3. “增加 $N$ 会减少由采样引起的误差，而不会引入偏差”：由于无论 $N$ 为何值，偏差都是 $0$，并且方差以 $1/N$ 的速度减小，所以这是一个正确的结论。估计量的均方误差是 $\\mathrm{MSE}(y_{\\text{MC}}) = \\mathrm{Var}(y_{\\text{MC}}) + (\\text{偏差})^2 = \\frac{\\mathrm{Var}(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right))}{N} + 0^2$，它随着 $N$ 的增加而减小。\n4. “更高的曲率通常会增加 $\\mathrm{Var}\\!\\big(\\phi\\!\\left(\\tfrac{z}{p}\\,a\\right)\\big)$...”：输出的方差 $\\mathrm{Var}(\\phi(X))$ 取决于输出值 $\\phi(0)$ 和 $\\phi(a/p)$ 的离散程度。与曲率较小的函数相比，曲率较高的函数通常会将输入值 $0$ 和 $a/p$ 映射到相距更远的输出值，从而增加了输出的方差。这是一个合理的定性陈述。非线性“放大”了其输入的变异性，而放大的程度与函数的非线性程度（即弯曲程度）有关。\n该陈述的所有部分在事实上和概念上都是正确的。\n**结论：正确。**\n\n最终复核正确选项：A, C, E, F。", "answer": "$$\\boxed{ACEF}$$", "id": "3118065"}]}