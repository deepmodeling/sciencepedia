{"hands_on_practices": [{"introduction": "要真正掌握批量归一化（Batch Normalization），动手实现其核心机制至关重要。本练习将指导你从零开始，区分并实现批量归一化的训练模式和评估模式，并通过数值验证来确认“冻结”统计数据在评估时的等效性。完成这个练习将夯实你对批量归一化双重角色的理解，这是在实践中正确使用它的第一步 [@problem_id:3101661]。", "problem": "你的任务是实现一个最小化的前馈模型，该模型包含一个仿射层，后跟一个批量归一化（Batch Normalization, BN）层，并演示从训练模式到评估模式的转换。该模型必须纯粹使用数值编程环境中的数组和操作来实现，不得依赖外部的深度学习框架。演示必须在指定的容差范围内，通过冻结统计量来验证数值等效性。\n\n从以下基本基础开始，仅使用广泛接受的定义：\n\n- 给定一个小批量的特征向量 $x \\in \\mathbb{R}^{B \\times d}$，沿批次维度定义经验性的小批量均值和方差为\n$$\n\\mu_{\\text{batch}} = \\frac{1}{B} \\sum_{i=1}^{B} x_i, \\quad\n\\sigma_{\\text{batch}}^2 = \\frac{1}{B} \\sum_{i=1}^{B} (x_i - \\mu_{\\text{batch}})^2,\n$$\n两者都是对每个特征维度逐元素计算的。添加一个小的正常数 $\\epsilon > 0$ 以稳定方差的倒数计算。\n- 使用动量 $\\rho \\in [0,1]$ 通过指数移动平均来维持移动统计量：\n$$\n\\mu_{\\text{run}} \\leftarrow (1 - \\rho)\\, \\mu_{\\text{run}} + \\rho\\, \\mu_{\\text{batch}}, \\quad\n\\sigma_{\\text{run}}^2 \\leftarrow (1 - \\rho)\\, \\sigma_{\\text{run}}^2 + \\rho\\, \\sigma_{\\text{batch}}^2.\n$$\n将 $\\mu_{\\text{run}}$ 初始化为零向量，将 $\\sigma_{\\text{run}}^2$ 初始化为适当维度的一向量。\n- 设仿射变换由权重 $W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ 和偏置 $b \\in \\mathbb{R}^{d_{\\text{out}}}$ 定义为\n$$\ny = x W + b,\n$$\n批量归一化逐特征地应用于 $y$，带有可学习的缩放和平移参数 $\\gamma, \\beta \\in \\mathbb{R}^{d_{\\text{out}}}$：\n$$\n\\hat{y} = \\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad\n\\text{BN}(y; \\gamma, \\beta, \\mu, \\sigma^2, \\epsilon) = \\gamma \\odot \\hat{y} + \\beta,\n$$\n其中 $\\odot$ 表示逐元素乘法。\n\n目标是：\n\n1. 实现训练模式下的BN，它计算当前小批量的 $\\mu_{\\text{batch}}$ 和 $\\sigma_{\\text{batch}}^2$，使用动量 $\\rho$ 更新移动统计量 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$，并使用当前小批量的统计量输出归一化后的激活值。\n2. 实现评估模式下的BN，它使用冻结的移动统计量 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ 而不更新它们，并使用这些冻结的统计量输出归一化后的激活值。\n3. 实现一个“冻结统计量的训练前向传播”，它不更新 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$，并使用 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ 进行归一化，即使全局模型模式是训练模式。这是“冻结统计量”的操作性定义。\n4. 在通过处理一系列小批量来更新 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ 以模拟训练之后，将模型转换为评估模式，并通过检查评估模式BN和冻结统计量的训练前向传播在相同输入下的输出之间的最大绝对差是否小于或等于给定的容差界限 $\\tau$，来验证它们之间的数值等效性。\n\n你的程序必须实现上述内容并评估以下测试套件。对于每种情况，完全按照指定构造模型参数和数据。所有随机数必须使用指定的种子确定性地生成。对方差使用无偏性自由度 $0$（即除数为 $B$）。本说明中的每个数值都是无量纲的。\n\n对于每个测试用例，定义：\n- $d_{\\text{in}}$：输入特征维度。\n- $d_{\\text{out}}$：输出特征维度。\n- $B$：小批量大小。\n- $T$：要处理的训练小批量数量。\n- $\\rho$：移动统计量的动量。\n- $\\epsilon$：添加到方差中的稳定项。\n- 种子：$s_{\\text{model}}$ 用于初始化 $W$, $b$, $\\gamma$, $\\beta$；$s_{\\text{train}}$ 用于生成训练小批量；$s_{\\text{test}}$ 用于生成测试小批量。\n- 训练数据生成规则和测试数据生成规则：\n  - 如果指定了一个常数值 $c$，则训练小批量中的所有条目都等于 $c$。\n  - 否则，对于正态分布，从具有指定均值 $\\mu$ 和标准差 $\\sigma$ 的 $\\mathcal{N}(\\mu, \\sigma^2)$ 中独立抽取条目。\n\n每种情况的验证标准：\n- 在模拟训练 $T$ 个小批量之后，对一个测试小批量计算模型输出两次：一次在评估模式下，一次在统计量冻结的训练模式下。设 $\\Delta$ 为这两个输出之间的最大绝对差。如果 $\\Delta \\le \\tau$，则结果为 $ \\text{True} $，否则为 $ \\text{False} $。\n\n使用以下测试套件，每个套件的容差界限为 $\\tau = 10^{-12}$：\n\n- 情况 $1$（一般的“理想路径”）：\n  - $d_{\\text{in}} = 8$, $d_{\\text{out}} = 8$, $B = 32$, $T = 50$, $\\rho = 0.1$, $\\epsilon = 10^{-5}$，\n  - 种子：$s_{\\text{model}} = 0$, $s_{\\text{train}} = 1$, $s_{\\text{test}} = 2$，\n  - 训练数据：$\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$，\n  - 测试数据：$\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$。\n\n- 情况 $2$（批量大小为 $1$ 和零方差训练批次的边界情况）：\n  - $d_{\\text{in}} = 4$, $d_{\\text{out}} = 4$, $B = 1$, $T = 3$, $\\rho = 0.5$, $\\epsilon = 10^{-3}$，\n  - 种子：$s_{\\text{model}} = 10$, $s_{\\text{train}} = 11$, $s_{\\text{test}} = 12$，\n  - 训练数据：常数 $c = 3.3$，\n  - 测试数据：$\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$。\n\n- 情况 $3$（大 $\\epsilon$ 和可能为负的缩放参数）：\n  - $d_{\\text{in}} = 16$, $d_{\\text{out}} = 16$, $B = 64$, $T = 10$, $\\rho = 0.9$, $\\epsilon = 10^{-1}$，\n  - 种子：$s_{\\text{model}} = 21$, $s_{\\text{train}} = 22$, $s_{\\text{test}} = 23$，\n  - 训练数据：$\\mathcal{N}(\\mu = 1.5, \\sigma = 2.0)$，\n  - 测试数据：$\\mathcal{N}(\\mu = -0.5, \\sigma = 1.0)$。\n\n- 情况 $4$（动量 $\\rho = 0$，不更新移动统计量）：\n  - $d_{\\text{in}} = 5$, $d_{\\text{out}} = 5$, $B = 20$, $T = 25$, $\\rho = 0.0$, $\\epsilon = 10^{-6}$，\n  - 种子：$s_{\\text{model}} = 31$, $s_{\\text{train}} = 32$, $s_{\\text{test}} = 33$，\n  - 训练数据：$\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$，\n  - 测试数据：$\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$。\n\n- 情况 $5$（动量 $\\rho = 1$，移动统计量等于最后一个批次的统计量）：\n  - $d_{\\text{in}} = 7$, $d_{\\text{out}} = 7$, $B = 10$, $T = 7$, $\\rho = 1.0$, $\\epsilon = 10^{-6}$，\n  - 种子：$s_{\\text{model}} = 41$, $s_{\\text{train}} = 42$, $s_{\\text{test}} = 43$，\n  - 训练数据：$\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$，\n  - 测试数据：$\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$。\n\n模型参数的构造：\n- 对于每种情况，使用种子 $s_{\\text{model}}$ 通过抽取独立的标准正态分布项来初始化仿射参数 $W$ 和 $b$，以及BN参数 $\\gamma$ 和 $\\beta$。\n\n你的程序应该生成一行输出，其中包含五个测试用例的结果，格式为一个包含在方括号中的逗号分隔列表（例如，\"[$\\text{True}$,$\\text{False}$,$\\text{True}$,$\\text{True}$,$\\text{True}$]\"）。这些条目必须是根据上述验证标准计算出的布尔值，并严格按照用例的顺序排列。本问题不涉及任何物理单位或角度单位；仅报告布尔值。", "solution": "问题陈述已分析完毕，并被认为是有效的。它构成了一个适定、自洽且在计算上可验证的任务，其科学基础是深度学习的原理。关于仿射层、批量归一化（BN）以及移动统计量更新规则的定义都是标准且数学上合理的。目标是实现这些组件，并验证BN层两种操作模式之间的数值等效性，这是对实现正确性的一种测试。\n\n解决方案是遵循问题中阐述的架构和程序规范构建的。\n\n**模型架构**\n\n该模型是一个包含两个层的前馈序列：一个仿射层和一个批量归一化层。\n\n$1$. **仿射层**：该层执行一个线性变换。对于一个输入的小批量 $x \\in \\mathbb{R}^{B \\times d_{\\text{in}}}$，其中 $B$ 是批量大小，$d_{\\text{in}}$ 是输入特征维度，该层计算一个输出 $y \\in \\mathbb{R}^{B \\times d_{\\text{out}}}$。这由一个权重矩阵 $W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ 和一个偏置向量 $b \\in \\mathbb{R}^{d_{\\text{out}}}$ 控制。变换是：\n$$y = x W + b$$\n偏置 $b$ 被广播，使其加到矩阵乘积 $xW$ 的每一行。\n\n$2$. **批量归一化（BN）层**：该层逐个特征地对其输入 $y$ 进行归一化。它以两种不同的模式运行：训练和评估。\n\n**训练模式下的BN**\n\n在训练阶段，BN层使用从当前小批量计算出的统计量来归一化其输入。它同时更新一组近似于全局数据集统计量的移动统计量。\n\n- **步骤 $1$：小批量统计量计算**。对于输入 $y \\in \\mathbb{R}^{B \\times d_{\\text{out}}}$，逐元素的均值 $\\mu_{\\text{batch}} \\in \\mathbb{R}^{d_{\\text{out}}}$ 和方差 $\\sigma_{\\text{batch}}^2 \\in \\mathbb{R}^{d_{\\text{out}}}$ 是沿着批次维度（大小为 $B$）计算的：\n$$\n\\mu_{\\text{batch}} = \\frac{1}{B} \\sum_{i=1}^{B} y_i\n$$\n$$\n\\sigma_{\\text{batch}}^2 = \\frac{1}{B} \\sum_{i=1}^{B} (y_i - \\mu_{\\text{batch}})^2\n$$\n问题明确指出使用无偏性自由度为 $0$，这对应于除以 $B$。\n\n- **步骤 $2$：归一化**。输入 $y$ 使用这些特定于批次的统计量进行归一化。一个小的正常数 $\\epsilon > 0$ 被加入到分母中以确保数值稳定性：\n$$\n\\hat{y} = \\frac{y - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}}\n$$\n$\\mu_{\\text{batch}}$ 和分母项都被广播到整个批次维度以进行此逐元素操作。\n\n- **步骤 $3$：缩放和平移变换**。归一化后的激活值 $\\hat{y}$ 随后经过一个学习到的仿射变换，该变换由一个缩放参数 $\\gamma \\in \\mathbb{R}^{d_{\\text{out}}}$ 和一个平移参数 $\\beta \\in \\mathbb{R}^{d_{\\text{out}}}$ 定义：\n$$\n\\text{output} = \\gamma \\odot \\hat{y} + \\beta\n$$\n其中 $\\odot$ 表示逐元素乘法。\n\n- **步骤 $4$：移动统计量更新**。该层维护均值（$\\mu_{\\text{run}}$）和方差（$\\sigma_{\\text{run}}^2$）的长期移动估计。这些估计通过指数移动平均进行更新，由动量参数 $\\rho \\in [0, 1]$ 控制：\n$$\n\\mu_{\\text{run}} \\leftarrow (1 - \\rho)\\mu_{\\text{run}} + \\rho\\mu_{\\text{batch}}\n$$\n$$\n\\sigma_{\\text{run}}^2 \\leftarrow (1 - \\rho)\\sigma_{\\text{run}}^2 + \\rho\\sigma_{\\text{batch}}^2\n$$\n按照规定，$\\mu_{\\text{run}}$ 初始化为零向量，$\\sigma_{\\text{run}}^2$ 初始化为一向量。\n\n**评估模式下的BN**\n\n在评估或推理期间，使用单个小批量的统计数据可能是不合需要的或不可能的（例如，如果 $B=1$）。取而代之的是，使用在训练过程中累积的“冻结的”移动统计量 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$。\n\n- **步骤 $1$：使用移动统计量进行归一化**。输入 $y$ 使用固定的 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$ 进行归一化：\n$$\n\\hat{y} = \\frac{y - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\n$$\n\n- **步骤 $2$：缩放和平移变换**。此步骤与训练模式中的相同：\n$$\n\\text{output} = \\gamma \\odot \\hat{y} + \\beta\n$$\n\n关键是，在评估模式下，移动统计量 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$ 不会被修改。\n\n**验证程序**\n\n核心任务是确认一个正确的BN层实现在经过模拟训练阶段后，对于两个概念上等效的场景，能够产生数值上相同的输出。\n\n$1$. **训练模拟**。对于五个测试用例中的每一个，过程首先通过从标准正态分布中抽样来初始化模型参数 $W, b, \\gamma, \\beta$，使用指定的种子 $s_{\\text{model}}$。移动统计量 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$ 分别初始化为 $\\vec{0}$ 和 $\\vec{1}$。然后将模型设置为其训练配置。一个模拟循环执行 $T$ 步。在每一步中，使用以 $s_{\\text{train}}$ 为种子的随机数生成器生成一个训练小批量。执行一次前向传播，它使用小批量统计量进行归一化并更新移动统计量。\n\n$2$. **等效性测试**。训练模拟完成后，累积的移动统计量被视为最终的或“冻结的”。使用种子 $s_{\\text{test}}$ 生成一个新的测试小批量。然后通过两种规定的方式计算模型对此测试批量的输出：\n    - **场景 A（评估模式）**：将模型切换到其评估配置（例如，通过标志 `training=False`）。对测试数据执行一次前向传播。在此模式下，该层必须使用冻结的 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$ 进行归一化。\n    - **场景 B（冻结统计量的训练前向传播）**：模型保持其名义上的训练配置（例如，`training=True`），但通过一个特殊标志（例如 `freeze_stats=True`）指示执行一次前向传播，该传播使用冻结的移动统计量进行归一化，从而绕过批次统计量的计算和移动统计量的更新。\n\n根据它们的定义，这两种场景描述了相同的计算。该测试旨在验证实现是否正确地反映了这一等同性。\n\n$3$. **数值验证**。计算场景A和场景B输出矩阵之间逐元素最大绝对差 $\\Delta$。如果这个差值小于或等于指定的容差 $\\Delta \\le \\tau$（其中 $\\tau = 10^{-12}$），则该测试用例的结果为 `True`。这个布尔结果被记录下来用于五个测试用例中的每一个，从而产生最终的输出列表。", "answer": "```python\nimport numpy as np\n\nclass Affine:\n    \"\"\"Implements a single affine (fully connected) layer.\"\"\"\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n\n    def forward(self, x):\n        \"\"\"Performs the forward pass: y = xW + b.\"\"\"\n        return x @ self.W + self.b\n\nclass BatchNorm1d:\n    \"\"\"Implements a Batch Normalization layer for 1D features.\"\"\"\n    def __init__(self, d_out, gamma, beta, eps, momentum):\n        self.gamma = gamma  # scale\n        self.beta = beta    # shift\n        self.eps = eps\n        self.momentum = momentum\n        \n        self.running_mean = np.zeros(d_out)\n        self.running_var = np.ones(d_out)\n        \n        self.training = True\n\n    def forward(self, y, freeze_stats=False):\n        \"\"\"Performs the forward pass for Batch Normalization.\"\"\"\n        if not self.training or freeze_stats:\n            # Evaluation mode or frozen-stats training mode: use running stats\n            mean_to_use = self.running_mean\n            var_to_use = self.running_var\n        else:\n            # Standard training mode: use batch stats and update running stats\n            if y.shape[0] > 1:\n                batch_mean = np.mean(y, axis=0)\n                batch_var = np.var(y, axis=0, ddof=0)\n            else: # B=1 case\n                batch_mean = y[0]\n                batch_var = np.zeros_like(y[0])\n\n            mean_to_use = batch_mean\n            var_to_use = batch_var\n\n            # Update running statistics\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n            \n        # Normalize\n        y_hat = (y - mean_to_use) / np.sqrt(var_to_use + self.eps)\n        \n        # Scale and shift\n        out = self.gamma * y_hat + self.beta\n        return out\n\n    def train(self):\n        self.training = True\n\n    def eval(self):\n        self.training = False\n\nclass Model:\n    \"\"\"Combines an Affine layer and a BatchNorm1d layer.\"\"\"\n    def __init__(self, affine_layer, bn_layer):\n        self.affine = affine_layer\n        self.bn = bn_layer\n\n    def forward(self, x, freeze_stats=False):\n        y = self.affine.forward(x)\n        return self.bn.forward(y, freeze_stats=freeze_stats)\n\n    def train(self):\n        self.bn.train()\n\n    def eval(self):\n        self.bn.eval()\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (general \"happy path\")\n        {\n            \"d_in\": 8, \"d_out\": 8, \"B\": 32, \"T\": 50, \"rho\": 0.1, \"epsilon\": 1e-5,\n            \"s_model\": 0, \"s_train\": 1, \"s_test\": 2,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 2 (boundary with batch size 1 and zero-variance training batches)\n        {\n            \"d_in\": 4, \"d_out\": 4, \"B\": 1, \"T\": 3, \"rho\": 0.5, \"epsilon\": 1e-3,\n            \"s_model\": 10, \"s_train\": 11, \"s_test\": 12,\n            \"train_gen\": lambda rng, B, d: np.full((B, d), 3.3),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 3 (large epsilon and potentially negative scale parameters)\n        {\n            \"d_in\": 16, \"d_out\": 16, \"B\": 64, \"T\": 10, \"rho\": 0.9, \"epsilon\": 1e-1,\n            \"s_model\": 21, \"s_train\": 22, \"s_test\": 23,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=1.5, scale=2.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=-0.5, scale=1.0, size=(B, d)),\n        },\n        # Case 4 (momentum rho = 0, no running-statistics update)\n        {\n            \"d_in\": 5, \"d_out\": 5, \"B\": 20, \"T\": 25, \"rho\": 0.0, \"epsilon\": 1e-6,\n            \"s_model\": 31, \"s_train\": 32, \"s_test\": 33,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 5 (momentum rho = 1, running statistics equal last batch statistics)\n        {\n            \"d_in\": 7, \"d_out\": 7, \"B\": 10, \"T\": 7, \"rho\": 1.0, \"epsilon\": 1e-6,\n            \"s_model\": 41, \"s_train\": 42, \"s_test\": 43,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n    ]\n\n    tau = 1e-12\n    results = []\n\n    for case in test_cases:\n        d_in, d_out, B, T = case[\"d_in\"], case[\"d_out\"], case[\"B\"], case[\"T\"]\n        \n        # 1. Initialize Model\n        rng_model = np.random.default_rng(case[\"s_model\"])\n        W = rng_model.standard_normal(size=(d_in, d_out))\n        b = rng_model.standard_normal(size=(d_out,))\n        gamma = rng_model.standard_normal(size=(d_out,))\n        beta = rng_model.standard_normal(size=(d_out,))\n        \n        affine_layer = Affine(W, b)\n        bn_layer = BatchNorm1d(d_out, gamma, beta, case[\"epsilon\"], case[\"rho\"])\n        model = Model(affine_layer, bn_layer)\n        \n        # 2. Simulate Training\n        model.train()\n        rng_train = np.random.default_rng(case[\"s_train\"])\n        for _ in range(T):\n            train_batch = case[\"train_gen\"](rng_train, B, d_in)\n            model.forward(train_batch) # Forward pass updates running stats\n\n        # 3. Verification Step\n        rng_test = np.random.default_rng(case[\"s_test\"])\n        test_batch = case[\"test_gen\"](rng_test, B, d_in)\n        \n        # Scenario A: Evaluation mode\n        model.eval()\n        output_eval = model.forward(test_batch)\n\n        # Scenario B: Frozen-stats training forward\n        model.train() # Set global mode to training\n        output_frozen = model.forward(test_batch, freeze_stats=True)\n\n        # 4. Compare and record result\n        max_abs_diff = np.max(np.abs(output_eval - output_frozen))\n        results.append(max_abs_diff <= tau)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3101661"}, {"introduction": "在理解了训练和评估模式的区别后，下一个关键问题是：为什么这个区别如此重要？此练习通过一个精心设计的反例，生动地展示了在推理时误用批次统计数据的危害。你将看到，一个样本的预测结果会因“邻居”样本的不同而改变，这揭示了为何模型的预测必须是确定性的，而不能依赖于一同推理的其他数据 [@problem_id:3101625]。", "problem": "构建一个程序，通过生成批次依赖的预测，来证明在推理时使用批归一化（BN）的批次统计数据是危险的。在一维空间中操作，因此每个输入都是一个标量。使用以下基本框架。\n\n对于一个批次 $\\mathcal{B} = \\{x_i\\}_{i=1}^m$，其批次均值为 $\\mu_{\\mathcal{B}} = \\frac{1}{m}\\sum_{i=1}^m x_i$，批次方差为 $\\sigma^2_{\\mathcal{B}} = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{\\mathcal{B}})^2$，批归一化（BN）将样本 $x$ 转换为\n$$\n\\operatorname{BN}_{\\mathcal{B}}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}} \\;+\\; \\beta,\n$$\n其中 $\\gamma$ 和 $\\beta$ 是学习到的仿射参数，$\\epsilon$ 是一个很小的正常数，以确保数值稳定性。正确的推理方法是将在训练期间累积的运行估计值 $\\mu_r$ 和 $\\sigma_r^2$ 替换 $\\mu_{\\mathcal{B}}$ 和 $\\sigma^2_{\\mathcal{B}}$：\n$$\n\\operatorname{BN}_{\\mathrm{run}}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu_r}{\\sqrt{\\sigma_r^2 + \\epsilon}} \\;+\\; \\beta.\n$$\n一个简单的分类器使用 BN 输出 $z$ 并通过阈值规则预测二元标签 $\\hat{y}$\n$$\n\\hat{y} \\;=\\; \n\\begin{cases}\n1, & z \\ge 0,\\\\\n0, & z < 0.\n\\end{cases}\n$$\n您必须针对一个固定的目标输入 $x^\\star$ 实现这两种推理模式，并证明如果错误地在推理时使用每个批次的统计数据，那么 $x^\\star$ 的预测标签会根据批次中的其他样本而翻转。\n\n在所有计算中使用以下固定参数和定义：\n- 目标输入：$x^\\star = 0.2$。\n- BN 参数：$\\gamma = 1.0$，$\\beta = -0.1$，$\\epsilon = 10^{-5}$。\n- 运行统计数据（正确的推理统计数据）：$\\mu_r = 0.0$，$\\sigma_r^2 = 1.0$。\n- 分类规则如上所述。\n\n对于下面的每个批次 $\\mathcal{B}$，计算：\n1. 使用 $z_{\\mathrm{run}} = \\operatorname{BN}_{\\mathrm{run}}(x^\\star)$ 计算 $x^\\star$ 的正确推理预测。\n2. 使用 $z_{\\mathcal{B}} = \\operatorname{BN}_{\\mathcal{B}}(x^\\star)$ 以及对包括 $x^\\star$ 在内的批次所有元素计算出的 $\\mu_{\\mathcal{B}}$ 和 $\\sigma^2_{\\mathcal{B}}$，计算 $x^\\star$ 的不正确的、依赖于批次的预测。\n3. 一个布尔值，指示不正确的、依赖于批次的预测是否与正确的推理预测不同。\n\n测试套件（每个批次都是一个多重集，始终包含 $x^\\star$）：\n- 具有大致标准化同伴的正常情况：$\\mathcal{B}_1 = [\\,x^\\star,\\,-1.0,\\,0.0,\\,1.0\\,]$。\n- 具有大正数同伴的病态均值偏移：$\\mathcal{B}_2 = [\\,x^\\star,\\,10.0,\\,10.0,\\,10.0\\,]$。\n- 均值低于 $x^\\star$ 且方差较小：$\\mathcal{B}_3 = [\\,x^\\star,\\,0.0,\\,0.3,\\,-0.3\\,]$。\n- 相同均值但方差较大：$\\mathcal{B}_4 = [\\,x^\\star,\\,0.0,\\,3.0,\\,-3.0\\,]$。\n- 批次大小为一的边界情况：$\\mathcal{B}_5 = [\\,x^\\star\\,]$。\n\n您的程序必须按 $\\mathcal{B}_1$ 到 $\\mathcal{B}_5$ 的顺序计算五个布尔值，当且仅当对于 $x^\\star$ 的不正确的、依赖于批次的预测与使用运行统计数据的正确推理预测不同时，每个布尔值为真。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[b_1,b_2,b_3,b_4,b_5]$，其中每个 $b_i$ 为 True 或 False。", "solution": "所述问题是有效的。它在科学上基于深度学习中批归一化的原理，问题陈述清晰，提供了所有必要的参数和定义，并且其表述是客观的。它要求对一个众所周知的概念进行计算演示，这是一个合适且可验证的任务。\n\n需要证明的核心原理是，在模型训练和推理期间，必须以不同的方式处理批归一化（BN）。在训练期间，BN 使用当前小批次数据的均值和方差对层输入进行归一化。这使得训练过程更加稳定。然而，在推理时，处理的是单个输入，而不是一个批次。如果在推理时使用大小为一的“批次”或任何可用的临时批次的统计数据，将导致模型对给定输入的预测依赖于同时处理的其他不相关输入。这违背了训练好的模型应为确定性函数的理想要求。正确的推理过程是使用固定的、总体级别的统计数据——这些数据是通过聚合训练期间看到的所有小批次的统计数据（例如，通过指数移动平均）来近似的。\n\n我们将首先使用提供的运行统计数据，计算目标输入 $x^\\star$ 的正确的、确定性的预测。然后，对于每个测试批次，我们将计算不正确的、依赖于批次的预测，并对两者进行比较。\n\n固定参数为：\n- 目标输入：$x^\\star = 0.2$\n- BN 仿射参数：$\\gamma = 1.0$，$\\beta = -0.1$\n- 数值稳定性常数：$\\epsilon = 10^{-5}$\n- 用于正确推理的运行统计数据：$\\mu_r = 0.0$，$\\sigma_r^2 = 1.0$\n\n分类规则为：\n$$\n\\hat{y} = \\begin{cases} 1, & z \\ge 0 \\\\ 0, & z < 0 \\end{cases}\n$$\n其中 $z$ 是 BN 层的输出。\n\n### 步骤 1：正确的推理预测\n\n正确的推理使用运行统计数据 $\\mu_r$ 和 $\\sigma_r^2$。$x^\\star$ 的 BN 输出由下式给出：\n$$\nz_{\\mathrm{run}} = \\operatorname{BN}_{\\mathrm{run}}(x^\\star) = \\gamma \\cdot \\frac{x^\\star - \\mu_r}{\\sqrt{\\sigma_r^2 + \\epsilon}} + \\beta\n$$\n代入给定值：\n$$\nz_{\\mathrm{run}} = 1.0 \\cdot \\frac{0.2 - 0.0}{\\sqrt{1.0 + 10^{-5}}} - 0.1 \\approx \\frac{0.2}{1.000005} - 0.1 \\approx 0.199999 - 0.1 = 0.099999\n$$\n由于 $z_{\\mathrm{run}} = 0.099999 \\ge 0$，正确的预测是 $\\hat{y}_{\\mathrm{run}} = 1$。这是唯一的、恒定的基准，所有其他对 $x^\\star$ 的预测都将与之进行比较。\n\n### 步骤 2：不正确的、依赖于批次的推理\n\n我们现在为每个测试批次 $\\mathcal{B}_i$ 计算 $x^\\star$ 的预测，并错误地使用该批次的统计数据。BN 输出为：\n$$\nz_{\\mathcal{B}_i} = \\operatorname{BN}_{\\mathcal{B}_i}(x^\\star) = \\gamma \\cdot \\frac{x^\\star - \\mu_{\\mathcal{B}_i}}{\\sqrt{\\sigma^2_{\\mathcal{B}_i} + \\epsilon}} + \\beta\n$$\n其中 $\\mu_{\\mathcal{B}_i}$ 和 $\\sigma^2_{\\mathcal{B}_i}$ 是根据每个批次 $\\mathcal{B}_i$ 的元素计算得出的。\n\n**情况 1：批次 $\\mathcal{B}_1 = [0.2, -1.0, 0.0, 1.0]$**\n- 批次均值：$\\mu_{\\mathcal{B}_1} = \\frac{1}{4}(0.2 - 1.0 + 0.0 + 1.0) = \\frac{0.2}{4} = 0.05$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_1} = \\frac{1}{4}((0.2-0.05)^2 + (-1.0-0.05)^2 + (0.0-0.05)^2 + (1.0-0.05)^2) = \\frac{1}{4}(0.0225+1.1025+0.0025+0.9025) = \\frac{2.03}{4} = 0.5075$\n- BN 输出：$z_{\\mathcal{B}_1} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{0.5075 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{0.7124} - 0.1 \\approx 0.2105 - 0.1 = 0.1105$\n- 预测：$\\hat{y}_{\\mathcal{B}_1} = 1$ 因为 $z_{\\mathcal{B}_1} \\ge 0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_1} = \\hat{y}_{\\mathrm{run}}$。布尔值为 **False**。\n\n**情况 2：批次 $\\mathcal{B}_2 = [0.2, 10.0, 10.0, 10.0]$**\n- 批次均值：$\\mu_{\\mathcal{B}_2} = \\frac{1}{4}(0.2 + 10.0 + 10.0 + 10.0) = \\frac{30.2}{4} = 7.55$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_2} = \\frac{1}{4}((0.2-7.55)^2 + 3 \\cdot (10.0-7.55)^2) = \\frac{1}{4}(54.0225 + 3 \\cdot 6.0025) = \\frac{72.03}{4} = 18.0075$\n- BN 输出：$z_{\\mathcal{B}_2} = 1.0 \\cdot \\frac{0.2 - 7.55}{\\sqrt{18.0075 + 10^{-5}}} - 0.1 \\approx \\frac{-7.35}{4.2435} - 0.1 \\approx -1.732 - 0.1 = -1.832$\n- 预测：$\\hat{y}_{\\mathcal{B}_2} = 0$ 因为 $z_{\\mathcal{B}_2} < 0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_2} \\neq \\hat{y}_{\\mathrm{run}}$。布尔值为 **True**。这演示了批次中的巨大均值偏移如何极大地改变 $x^\\star$ 的归一化值。\n\n**情况 3：批次 $\\mathcal{B}_3 = [0.2, 0.0, 0.3, -0.3]$**\n- 批次均值：$\\mu_{\\mathcal{B}_3} = \\frac{1}{4}(0.2 + 0.0 + 0.3 - 0.3) = \\frac{0.2}{4} = 0.05$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_3} = \\frac{1}{4}((0.2-0.05)^2 + (0.0-0.05)^2 + (0.3-0.05)^2 + (-0.3-0.05)^2) = \\frac{1}{4}(0.0225+0.0025+0.0625+0.1225) = \\frac{0.21}{4} = 0.0525$\n- BN 输出：$z_{\\mathcal{B}_3} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{0.0525 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{0.22915} - 0.1 \\approx 0.6546 - 0.1 = 0.5546$\n- 预测：$\\hat{y}_{\\mathcal{B}_3} = 1$ 因为 $z_{\\mathcal{B}_3} \\ge 0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_3} = \\hat{y}_{\\mathrm{run}}$。布尔值为 **False**。\n\n**情况 4：批次 $\\mathcal{B}_4 = [0.2, 0.0, 3.0, -3.0]$**\n- 批次均值：$\\mu_{\\mathcal{B}_4} = \\frac{1}{4}(0.2 + 0.0 + 3.0 - 3.0) = \\frac{0.2}{4} = 0.05$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_4} = \\frac{1}{4}((0.2-0.05)^2 + (0.0-0.05)^2 + (3.0-0.05)^2 + (-3.0-0.05)^2) = \\frac{1}{4}(0.0225+0.0025+8.7025+9.3025) = \\frac{18.03}{4} = 4.5075$\n- BN 输出：$z_{\\mathcal{B}_4} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{4.5075 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{2.1231} - 0.1 \\approx 0.0706 - 0.1 = -0.0294$\n- 预测：$\\hat{y}_{\\mathcal{B}_4} = 0$ 因为 $z_{\\mathcal{B}_4} < 0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_4} \\neq \\hat{y}_{\\mathrm{run}}$。布尔值为 **True**。这里，批次均值与情况 1 和情况 3 相同，但较大的方差减小了归一化值的幅度，在经过 $\\beta$ 偏移后将其推到分类阈值以下。\n\n**情况 5：批次 $\\mathcal{B}_5 = [0.2]$**\n- 批次均值：$\\mu_{\\mathcal{B}_5} = \\frac{1}{1}(0.2) = 0.2$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_5} = \\frac{1}{1}((0.2 - 0.2)^2) = 0.0$\n- BN 输出：$z_{\\mathcal{B}_5} = 1.0 \\cdot \\frac{0.2 - 0.2}{\\sqrt{0.0 + 10^{-5}}} - 0.1 = \\frac{0.0}{\\sqrt{10^{-5}}} - 0.1 = 0.0 - 0.1 = -0.1$\n- 预测：$\\hat{y}_{\\mathcal{B}_5} = 0$ 因为 $z_{\\mathcal{B}_5} < 0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_5} \\neq \\hat{y}_{\\mathrm{run}}$。布尔值为 **True**。这是一个极端情况，其中分子 $x^\\star - \\mu_{\\mathcal{B}}$ 变为零。输出就是 $\\beta$，这突显了 $\\epsilon$ 在防止除以零方面的关键作用，但仍然导致了预测的翻转。\n\n这些计算清楚地表明，在推理时使用批次统计数据会使得对固定输入 $x^\\star$ 的预测变得不稳定，并依赖于批次中其他输入的上下文，从而导致错误的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Demonstrates the danger of using batch statistics for Batch Normalization\n    at inference time by showing that predictions can become batch-dependent.\n    \"\"\"\n    # Define fixed parameters and the target input.\n    x_star = 0.2\n    gamma = 1.0\n    beta = -0.1\n    epsilon = 1e-5\n\n    # Define running statistics for correct inference.\n    mu_r = 0.0\n    sigma_r_sq = 1.0\n\n    # Define the test cases (batches).\n    # Each batch is a list of floats and always contains x_star.\n    test_cases = [\n        # Happy path with roughly standardized peers\n        [x_star, -1.0, 0.0, 1.0],\n        # Pathological mean shift with large positive peers\n        [x_star, 10.0, 10.0, 10.0],\n        # Small variance around a mean below x_star\n        [x_star, 0.0, 0.3, -0.3],\n        # Large variance around the same mean\n        [x_star, 0.0, 3.0, -3.0],\n        # Boundary case of batch size one\n        [x_star],\n    ]\n\n    # --- Step 1: Compute the correct-inference prediction ---\n    # This prediction is deterministic and serves as the ground truth.\n    z_run = gamma * (x_star - mu_r) / np.sqrt(sigma_r_sq + epsilon) + beta\n    y_hat_run = 1 if z_run >= 0 else 0\n\n    results = []\n    # --- Step 2: Compute incorrect predictions for each batch ---\n    for batch_data in test_cases:\n        batch = np.array(batch_data)\n        \n        # Calculate batch-specific statistics.\n        # The variance is calculated using the population formula (1/m),\n        # as is standard in BN implementations.\n        mu_b = np.mean(batch)\n        sigma_b_sq = np.var(batch)\n\n        # Calculate the batch-dependent BN output for x_star.\n        z_b = gamma * (x_star - mu_b) / np.sqrt(sigma_b_sq + epsilon) + beta\n\n        # Determine the batch-dependent prediction.\n        y_hat_b = 1 if z_b >= 0 else 0\n\n        # Compare the incorrect prediction to the correct one.\n        # The result is True if the predictions differ.\n        prediction_differs = (y_hat_b != y_hat_run)\n        results.append(prediction_differs)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3101625"}, {"introduction": "任何可训练层的核心在于其反向传播算法的正确性。作为本章的综合性练习，你将为批量归一化层推导并实现解析梯度，然后通过数值方法（有限差分）进行严格的梯度检查。完成此练习意味着你已深刻理解了批量归一化层中复杂的梯度流动，尤其是由共享批次统计数据引起的依赖关系，这是从根本上理解其工作原理的标志 [@problem_id:3101647]。", "problem": "您需要实现一个完整的程序，对批量归一化（Batch Normalization, BN）进行鲁棒的梯度检查。批量归一化（BN）应在训练模式下为单个全连接层实现，并在批次上对每个特征进行归一化。目标是通过有限差分验证关于输入、缩放和偏移的反向梯度与链式法则及批次统计量的核心定义是否一致。程序必须构建一个标量损失函数，并使用中心有限差分来近似梯度，同时注意在扰动任何单个元素时，正确地重新计算共享的、依赖于批次的量。\n\n从以下与上下文相符的基本原理开始：\n- 用于复合函数的微分学链式法则。\n- 每个特征的批次均值和方差的定义：对于大小为 $N$、特征维度为 $D$ 的批次，输入矩阵 $X \\in \\mathbb{R}^{N \\times D}$ 的逐特征均值为 $ \\mu_j = \\frac{1}{N} \\sum_{i=1}^{N} x_{ij} $，逐特征方差为 $ \\sigma_j^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_{ij} - \\mu_j)^2 $。\n- 对于一个标量值函数 $f(\\theta)$ 在标量参数分量 $\\theta_k$ 处的中心有限差分近似：对于一个小的步长 $h$，有 $ \\frac{\\partial f}{\\partial \\theta_k} \\approx \\frac{f(\\theta_k + h) - f(\\theta_k - h)}{2 h} $。\n\n用纯数学和逻辑实现以下要求：\n1. 在训练模式下实现BN的前向传播，输入为 $X \\in \\mathbb{R}^{N \\times D}$，缩放向量为 $\\gamma \\in \\mathbb{R}^{D}$，偏移向量为 $\\beta \\in \\mathbb{R}^{D}$，以及一个添加在平方根内部的小稳定化参数 $\\epsilon > 0$。前向传播必须在批次上对每个特征进行归一化，然后应用缩放和偏移。\n2. 定义一个标量损失 $ L(X, \\gamma, \\beta) = \\sum_{i=1}^{N} \\sum_{j=1}^{D} y_{ij} \\, g_{ij} $，其中 $Y$ 是 BN 的输出，$G \\in \\mathbb{R}^{N \\times D}$ 是一个固定矩阵，您的程序将使用一个指定种子的随机数生成器来确定性地构建它。\n3. 基于第一性原理和链式法则，仅使用均值、方差和归一化的定义，推导并实现反向传播，以计算关于 $X$、$\\gamma$ 和 $\\beta$ 的梯度。不要在问题陈述中假设任何快捷公式；推导和公式应属于解决方案部分。\n4. 为每个参数实现一个中心有限差分梯度检查器：\n   - 对于 $X$，将单个条目 $x_{ij}$ 扰动 $+h$ 和 $-h$，每次都重新计算整个前向传播，以使共享的、依赖于批次的统计量 $ \\mu_j $ 和 $ \\sigma_j^2 $ 反映出被扰动后的批次。\n   - 对于 $\\gamma_j$ 和 $\\beta_j$，同样地将每个分量扰动 $+h$ 和 $-h$。\n   使用中心有限差分公式来近似 $ \\frac{\\partial L}{\\partial x_{ij}} $、$ \\frac{\\partial L}{\\partial \\gamma_j} $ 和 $ \\frac{\\partial L}{\\partial \\beta_j} $。\n5. 对于每个测试用例，计算三个参数组中每一组的解析梯度和数值梯度之间的最大相对误差。对于形状相同的数组 $A$ 和 $B$，定义逐元素的相对差异并取最大值：\n   $$ \\mathrm{RelErr}(A, B) = \\max_{k} \\frac{|A_k - B_k|}{\\max(1, |A_k| + |B_k|)}. $$\n6. 如果所有三个参数组的最大相对误差都严格小于指定的容差 $ \\tau $，则该测试用例通过。\n\n使用以下测试套件。对于所有随机值，使用由指定种子初始化的伪随机数生成器生成的标准正态分布。对于矩阵 $X$、矩阵 $G$ 以及向量 $\\gamma$ 和 $\\beta$，请精确遵循初始化规则。步长 $h$ 用于有限差分。\n\n- 测试用例 1 (正常路径):\n  - $N = 4$, $D = 3$, 种子 $0$, $\\epsilon = 10^{-5}$, $h = 10^{-4}$, 容差 $ \\tau = 10^{-5}$。\n  - 通过从具有给定种子的标准正态分布中抽样来初始化 $X$、$G$、$\\gamma$ 和 $\\beta$。\n- 测试用例 2 (小批量和单位缩放/零偏移):\n  - $N = 2$, $D = 5$, 种子 $1$, $\\epsilon = 10^{-5}$, $h = 10^{-4}$, 容差 $ \\tau = 2 \\cdot 10^{-5}$。\n  - 从标准正态分布初始化 $X$ 和 $G$；将 $\\gamma$ 设置为全一向量，$\\beta$ 设置为全零向量。\n- 测试用例 3 (较大的稳定化参数):\n  - $N = 8$, $D = 4$, 种子 $2$, $\\epsilon = 10^{-1}$, $h = 10^{-4}$, 容差 $ \\tau = 10^{-5}$。\n  - 从标准正态分布初始化 $X$、$G$、$\\gamma$ 和 $\\beta$。\n- 测试用例 4 (退化方差边缘情况):\n  - $N = 4$, $D = 3$, 种子 $3$, $\\epsilon = 10^{-5}$, $h = 10^{-4}$, 容差 $ \\tau = 10^{-5}$。\n  - 从标准正态分布初始化 $\\gamma$、$\\beta$ 和 $G$。构造 $X$ 使其每个特征列在整个批次中是恒定的：对于所有 $ i \\in \\{1, \\dots, N\\} $，设置 $ X_{i,:} = [0.5, -1.0, 2.0] $。\n\n您的程序应生成单行输出，其中包含四个测试用例的通过/失败结果，形式为方括号括起来的逗号分隔列表，例如 $ [\\mathrm{True}, \\mathrm{False}, \\mathrm{True}, \\mathrm{True}] $。每个条目必须是一个布尔值，指示该测试用例的所有三个梯度组是否都通过。不应打印其他任何文本。", "solution": "我们从微分学的链式法则和批次统计量的核心定义开始。令 $ X \\in \\mathbb{R}^{N \\times D} $ 表示输入批次，其条目为 $ x_{ij} $。对于每个特征索引 $ j $，定义批次均值和方差\n$$\n\\mu_j = \\frac{1}{N} \\sum_{i=1}^{N} x_{ij}, \\quad \\sigma_j^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_{ij} - \\mu_j)^2.\n$$\n定义 $ \\epsilon > 0 $ 和一个类似标准差的项 $ s_j = \\sqrt{\\sigma_j^2 + \\epsilon} $。每个特征的归一化激活值和仿射变换输出为\n$$\n\\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}, \\quad y_{ij} = \\gamma_j \\hat{x}_{ij} + \\beta_j,\n$$\n其中 $ \\gamma \\in \\mathbb{R}^{D} $ 和 $ \\beta \\in \\mathbb{R}^{D} $ 是可训练的参数。\n\n我们通过一个固定矩阵 $ G \\in \\mathbb{R}^{N \\times D} $ 构建一个标量损失：\n$$\nL(X, \\gamma, \\beta) = \\sum_{i=1}^{N} \\sum_{j=1}^{D} y_{ij} \\, g_{ij}.\n$$\n根据构造，$ \\frac{\\partial L}{\\partial y_{ij}} = g_{ij} $，这与 $X$、$ \\gamma $ 和 $ \\beta $ 无关。\n\n为获得解析梯度，我们直接对 $ y_{ij} $ 关于 $ \\beta_j $ 和 $ \\gamma_j $ 求导：\n- 由于 $ y_{ij} $ 线性依赖于 $ \\beta_j $，因此 $ \\frac{\\partial y_{ij}}{\\partial \\beta_j} = 1 $，从而\n$$\n\\frac{\\partial L}{\\partial \\beta_j} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial \\beta_j} = \\sum_{i=1}^{N} g_{ij}.\n$$\n- 由于 $ y_{ij} $ 通过 $ \\hat{x}_{ij} $ 线性依赖于 $ \\gamma_j $，因此 $ \\frac{\\partial y_{ij}}{\\partial \\gamma_j} = \\hat{x}_{ij} $，从而\n$$\n\\frac{\\partial L}{\\partial \\gamma_j} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial \\gamma_j} = \\sum_{i=1}^{N} g_{ij} \\hat{x}_{ij}.\n$$\n\n更微妙的部分是 $ \\frac{\\partial L}{\\partial x_{ij}} $，因为 $ \\hat{x}_{ij} $ 通过 $ \\mu_j $ 和 $ s_j $ 依赖于整个批次。应用链式法则，并认识到对于每个特征 $ j $ 我们有一个计算图：\n$$\nx_{ij} \\xrightarrow{\\text{减去 } \\mu_j} (x_{ij} - \\mu_j) \\xrightarrow{\\text{除以 } s_j} \\hat{x}_{ij} \\xrightarrow{\\text{缩放 } \\gamma_j} \\gamma_j \\hat{x}_{ij} \\xrightarrow{\\text{偏移 } \\beta_j} y_{ij} \\xrightarrow{\\text{与 } g_{ij} \\text{ 求和}} L.\n$$\n关于 $ x_{ij} $ 的全导数传播了来自通过 $ (x_{ij} - \\mu_j) $ 的直接路径的贡献，以及通过 $ \\mu_j $ 和 $ s_j $ 的间接依赖的贡献。\n\n首先，对于固定的 $ j $，定义 $ u_i = x_{ij} - \\mu_j $ 和 $ s = s_j $。归一化为 $ \\hat{x}_{ij} = u_i / s $。考虑给定 $ d\\hat{x}_{ij} $ 时的 $dL$：\n$$\n\\frac{\\partial L}{\\partial \\hat{x}_{ij}} = \\frac{\\partial L}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial \\hat{x}_{ij}} = g_{ij} \\gamma_j.\n$$\n我们需要对所有 $ k $ 求 $ \\frac{\\partial \\hat{x}_{ij}}{\\partial x_{kj}} $。注意\n$$\nu_i = x_{ij} - \\mu_j, \\quad \\mu_j = \\frac{1}{N} \\sum_{m=1}^{N} x_{mj}, \\quad s = \\sqrt{\\sigma_j^2 + \\epsilon}, \\quad \\sigma_j^2 = \\frac{1}{N} \\sum_{m=1}^{N} (x_{mj} - \\mu_j)^2.\n$$\n对 $ u_i $ 关于 $ x_{kj} $ 求导：\n$$\n\\frac{\\partial u_i}{\\partial x_{kj}} = \\delta_{ik} - \\frac{1}{N},\n$$\n其中 $ \\delta_{ik} $ 是克罗内克（Kronecker）$\\delta$。对于 $ s $，首先对 $ \\sigma_j^2 $ 求导：\n$$\n\\frac{\\partial \\sigma_j^2}{\\partial x_{kj}} = \\frac{2}{N} (x_{kj} - \\mu_j) - \\frac{2}{N} \\sum_{m=1}^{N} (x_{mj} - \\mu_j) \\cdot \\frac{\\partial \\mu_j}{\\partial x_{kj}}.\n$$\n由于 $ \\frac{\\partial \\mu_j}{\\partial x_{kj}} = \\frac{1}{N} $ 且 $ \\sum_{m=1}^{N} (x_{mj} - \\mu_j) = 0 $，第二项消失，所以\n$$\n\\frac{\\partial \\sigma_j^2}{\\partial x_{kj}} = \\frac{2}{N} (x_{kj} - \\mu_j).\n$$\n然后\n$$\n\\frac{\\partial s}{\\partial x_{kj}} = \\frac{1}{2} (\\sigma_j^2 + \\epsilon)^{-1/2} \\cdot \\frac{\\partial \\sigma_j^2}{\\partial x_{kj}} = \\frac{1}{N} \\frac{x_{kj} - \\mu_j}{s}.\n$$\n现在对 $ \\hat{x}_{ij} = u_i / s $ 应用商法则：\n$$\n\\frac{\\partial \\hat{x}_{ij}}{\\partial x_{kj}} = \\frac{1}{s} \\frac{\\partial u_i}{\\partial x_{kj}} - \\frac{u_i}{s^2} \\frac{\\partial s}{\\partial x_{kj}}\n= \\frac{1}{s} \\left( \\delta_{ik} - \\frac{1}{N} \\right) - \\frac{u_i}{s^2} \\cdot \\frac{1}{N} \\frac{x_{kj} - \\mu_j}{s}\n= \\frac{1}{s} \\left( \\delta_{ik} - \\frac{1}{N} \\right) - \\frac{1}{N} \\frac{u_i (x_{kj} - \\mu_j)}{s^3}.\n$$\n用归一化变量 $ \\hat{x}_{ij} = u_i / s $ 和 $ \\hat{x}_{kj} = (x_{kj} - \\mu_j) / s $ 表示所有内容。那么\n$$\n\\frac{\\partial \\hat{x}_{ij}}{\\partial x_{kj}} = \\frac{1}{s} \\left( \\delta_{ik} - \\frac{1}{N} \\right) - \\frac{1}{N} \\hat{x}_{ij} \\hat{x}_{kj} \\cdot \\frac{1}{s}.\n$$\n结合 $ \\frac{\\partial L}{\\partial \\hat{x}_{ij}} = g_{ij} \\gamma_j $ 并对 $ i $ 求和以得到 $ \\frac{\\partial L}{\\partial x_{kj}} $：\n$$\n\\frac{\\partial L}{\\partial x_{kj}} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial \\hat{x}_{ij}} \\frac{\\partial \\hat{x}_{ij}}{\\partial x_{kj}}\n= \\sum_{i=1}^{N} g_{ij} \\gamma_j \\left[ \\frac{1}{s} \\left( \\delta_{ik} - \\frac{1}{N} \\right) - \\frac{1}{N} \\hat{x}_{ij} \\hat{x}_{kj} \\cdot \\frac{1}{s} \\right].\n$$\n分配求和：\n$$\n\\frac{\\partial L}{\\partial x_{kj}} = \\frac{\\gamma_j}{s} \\left[ g_{kj} - \\frac{1}{N} \\sum_{i=1}^{N} g_{ij} - \\frac{1}{N} \\hat{x}_{kj} \\sum_{i=1}^{N} g_{ij} \\hat{x}_{ij} \\right].\n$$\n因此，对于每个特征 $ j $，在批次上的向量化形式为\n$$\n\\frac{\\partial L}{\\partial X_{:,j}} = \\frac{\\gamma_j}{s_j} \\left[ G_{:,j} - \\frac{1}{N} \\mathbf{1} \\left( \\sum_{i=1}^{N} g_{ij} \\right) - \\frac{1}{N} \\hat{X}_{:,j} \\left( \\sum_{i=1}^{N} g_{ij} \\hat{x}_{ij} \\right) \\right],\n$$\n其中 $ \\mathbf{1} $ 是一个全为1的 $ N $ 维向量。等价地，合并因子后得到：\n$$\n\\frac{\\partial L}{\\partial X_{:,j}} = \\frac{1}{N} \\frac{\\gamma_j}{s_j} \\left[ N G_{:,j} - \\mathbf{1} \\left( \\sum_{i=1}^{N} g_{ij} \\right) - \\hat{X}_{:,j} \\left( \\sum_{i=1}^{N} g_{ij} \\hat{x}_{ij} \\right) \\right].\n$$\n\n有限差分梯度检查使用中心差分近似。对于 $ X $、$ \\gamma $ 或 $ \\beta $ 的条目中的每个标量参数分量 $ \\theta_k $，我们计算\n$$\n\\frac{\\partial L}{\\partial \\theta_k} \\approx \\frac{L(\\ldots, \\theta_k + h, \\ldots) - L(\\ldots, \\theta_k - h, \\ldots)}{2 h},\n$$\n其中 $ h $ 很小。关键在于，由于 BN 依赖于共享的批次统计量 $ \\mu_j $ 和 $ \\sigma_j^2 $，当扰动单个 $ x_{ij} $ 时，我们必须从受扰动的批次中重新计算 $ \\mu_j $ 和 $ \\sigma_j^2 $；我们不将它们视为常数。这确保了数值梯度能考虑到完整的依赖结构。\n\n为评估一致性，我们计算最大相对误差\n$$\n\\mathrm{RelErr}(A, B) = \\max_{k} \\frac{|A_k - B_k|}{\\max(1, |A_k| + |B_k|)}.\n$$\n当 $ \\frac{\\partial L}{\\partial X} $、$ \\frac{\\partial L}{\\partial \\gamma} $ 和 $ \\frac{\\partial L}{\\partial \\beta} $ 的最大相对误差都严格小于给定的容差 $ \\tau $ 时，测试通过。\n\n算法设计：\n- 实现 BN 前向传播以生成 $ Y $，并缓存 $ \\hat{X} $ 和 $ s $ 以用于反向传播。\n- 使用推导出的表达式实现 BN 反向传播：对每个特征聚合 $ \\sum_{i} g_{ij} $ 和 $ \\sum_{i} g_{ij} \\hat{x}_{ij} $，并应用 $ \\frac{\\partial L}{\\partial X} $ 的闭式表达式，而 $ \\frac{\\partial L}{\\partial \\gamma} $ 和 $ \\frac{\\partial L}{\\partial \\beta} $ 通过直接求和获得。\n- 通过中心差分为 $ X $、$ \\gamma $ 和 $ \\beta $ 的所有条目实现数值梯度，每次重新计算前向传播并通过与 $ G $ 进行缩并计算 $ L $。\n- 对于每个测试用例，计算解析梯度和数值梯度，测量最大相对误差，并与 $ \\tau $ 比较。\n- 输出一行布尔值，表示每个测试用例的通过或失败情况。\n\n该设计集成了链式法则、均值和方差的定义以及中心有限差分，以在多种情况下（包括小批量大小和由 $ \\epsilon $ 控制的退化方差场景）验证 BN 的反向传播公式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef bn_forward(x: np.ndarray, gamma: np.ndarray, beta: np.ndarray, eps: float):\n    \"\"\"\n    Batch Normalization forward (training mode), per-feature across the batch.\n    x: (N, D)\n    gamma: (D,)\n    beta: (D,)\n    eps: scalar\n    Returns:\n      y: (N, D)\n      cache: dict with intermediates needed for backward\n    \"\"\"\n    mu = np.mean(x, axis=0)                    # (D,)\n    var = np.mean((x - mu) ** 2, axis=0)       # (D,)\n    std = np.sqrt(var + eps)                   # (D,)\n    x_hat = (x - mu) / std                     # (N, D)\n    y = gamma * x_hat + beta                   # (N, D)\n    cache = {\"x_hat\": x_hat, \"std\": std, \"gamma\": gamma}\n    return y, cache\n\ndef bn_backward(dy: np.ndarray, cache: dict):\n    \"\"\"\n    Batch Normalization backward given upstream gradient dy = dL/dy.\n    Uses derived closed-form expressions from first principles.\n    dy: (N, D)\n    cache: dict containing x_hat, std, gamma\n    Returns:\n      dx: (N, D)\n      dgamma: (D,)\n      dbeta: (D,)\n    \"\"\"\n    x_hat = cache[\"x_hat\"]        # (N, D)\n    std = cache[\"std\"]            # (D,)\n    gamma = cache[\"gamma\"]        # (D,)\n    N = dy.shape[0]\n\n    # Gradients w.r.t. beta and gamma\n    dbeta = np.sum(dy, axis=0)                    # (D,)\n    dgamma = np.sum(dy * x_hat, axis=0)           # (D,)\n\n    # Gradient w.r.t. x using compact expression\n    sum_dy = np.sum(dy, axis=0)                   # (D,)\n    sum_dy_xhat = np.sum(dy * x_hat, axis=0)      # (D,)\n\n    # Broadcasting handles shapes: (N,D) operations with (D,) vectors\n    dx = (1.0 / N) * (gamma / std) * (\n        N * dy - sum_dy - x_hat * sum_dy_xhat\n    )\n    return dx, dgamma, dbeta\n\ndef numerical_grad_x(x, gamma, beta, eps, G, h):\n    \"\"\"\n    Numerical gradient of L with respect to x via central differences.\n    L = sum(y * G), y = BN(x)\n    Returns gradient with shape (N, D).\n    \"\"\"\n    N, D = x.shape\n    grad = np.zeros_like(x)\n    for i in range(N):\n        for j in range(D):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[i, j] += h\n            x_minus[i, j] -= h\n            y_plus, _ = bn_forward(x_plus, gamma, beta, eps)\n            y_minus, _ = bn_forward(x_minus, gamma, beta, eps)\n            L_plus = np.sum(y_plus * G)\n            L_minus = np.sum(y_minus * G)\n            grad[i, j] = (L_plus - L_minus) / (2.0 * h)\n    return grad\n\ndef numerical_grad_gamma(x, gamma, beta, eps, G, h):\n    \"\"\"\n    Numerical gradient of L with respect to gamma via central differences.\n    Returns gradient with shape (D,).\n    \"\"\"\n    D = gamma.shape[0]\n    grad = np.zeros_like(gamma)\n    for j in range(D):\n        gamma_plus = gamma.copy()\n        gamma_minus = gamma.copy()\n        gamma_plus[j] += h\n        gamma_minus[j] -= h\n        y_plus, _ = bn_forward(x, gamma_plus, beta, eps)\n        y_minus, _ = bn_forward(x, gamma_minus, beta, eps)\n        L_plus = np.sum(y_plus * G)\n        L_minus = np.sum(y_minus * G)\n        grad[j] = (L_plus - L_minus) / (2.0 * h)\n    return grad\n\ndef numerical_grad_beta(x, gamma, beta, eps, G, h):\n    \"\"\"\n    Numerical gradient of L with respect to beta via central differences.\n    Returns gradient with shape (D,).\n    \"\"\"\n    D = beta.shape[0]\n    grad = np.zeros_like(beta)\n    for j in range(D):\n        beta_plus = beta.copy()\n        beta_minus = beta.copy()\n        beta_plus[j] += h\n        beta_minus[j] -= h\n        y_plus, _ = bn_forward(x, gamma, beta_plus, eps)\n        y_minus, _ = bn_forward(x, gamma, beta_minus, eps)\n        L_plus = np.sum(y_plus * G)\n        L_minus = np.sum(y_minus * G)\n        grad[j] = (L_plus - L_minus) / (2.0 * h)\n    return grad\n\ndef max_relative_error(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"\n    Compute maximum relative error across all elements using:\n      rel = |a - b| / max(1, |a| + |b|)\n    \"\"\"\n    denom = np.maximum(1.0, np.abs(a) + np.abs(b))\n    rel = np.abs(a - b) / denom\n    return float(np.max(rel))\n\ndef make_case(case_id):\n    \"\"\"\n    Construct test cases per specification.\n    Returns tuple: (x, gamma, beta, eps, G, h, tau)\n    \"\"\"\n    if case_id == 1:\n        N, D = 4, 3\n        seed = 0\n        eps = 1e-5\n        h = 1e-4\n        tau = 1e-5\n        rng = np.random.default_rng(seed)\n        x = rng.normal(size=(N, D))\n        G = rng.normal(size=(N, D))\n        gamma = rng.normal(size=(D,))\n        beta = rng.normal(size=(D,))\n        return x, gamma, beta, eps, G, h, tau\n    elif case_id == 2:\n        N, D = 2, 5\n        seed = 1\n        eps = 1e-5\n        h = 1e-4\n        tau = 2e-5\n        rng = np.random.default_rng(seed)\n        x = rng.normal(size=(N, D))\n        G = rng.normal(size=(N, D))\n        gamma = np.ones(D)\n        beta = np.zeros(D)\n        return x, gamma, beta, eps, G, h, tau\n    elif case_id == 3:\n        N, D = 8, 4\n        seed = 2\n        eps = 1e-1\n        h = 1e-4\n        tau = 1e-5\n        rng = np.random.default_rng(seed)\n        x = rng.normal(size=(N, D))\n        G = rng.normal(size=(N, D))\n        gamma = rng.normal(size=(D,))\n        beta = rng.normal(size=(D,))\n        return x, gamma, beta, eps, G, h, tau\n    elif case_id == 4:\n        N, D = 4, 3\n        seed = 3\n        eps = 1e-5\n        h = 1e-4\n        tau = 1e-5\n        rng = np.random.default_rng(seed)\n        # Constant columns across the batch\n        const_row = np.array([0.5, -1.0, 2.0])\n        x = np.tile(const_row, (N, 1))\n        G = rng.normal(size=(N, D))\n        gamma = rng.normal(size=(D,))\n        beta = rng.normal(size=(D,))\n        return x, gamma, beta, eps, G, h, tau\n    else:\n        raise ValueError(\"Invalid case_id\")\n\ndef run_case(x, gamma, beta, eps, G, h, tau):\n    \"\"\"\n    Run analytical and numerical gradient computations and compare.\n    Returns boolean pass/fail for the case.\n    \"\"\"\n    # Forward and analytical backward\n    y, cache = bn_forward(x, gamma, beta, eps)\n    # Set upstream gradient dy = G because dL/dy = G for L = sum(y * G)\n    dx_analytic, dgamma_analytic, dbeta_analytic = bn_backward(G, cache)\n\n    # Numerical gradients\n    dx_numeric = numerical_grad_x(x, gamma, beta, eps, G, h)\n    dgamma_numeric = numerical_grad_gamma(x, gamma, beta, eps, G, h)\n    dbeta_numeric = numerical_grad_beta(x, gamma, beta, eps, G, h)\n\n    # Relative errors\n    rel_dx = max_relative_error(dx_analytic, dx_numeric)\n    rel_dgamma = max_relative_error(dgamma_analytic, dgamma_numeric)\n    rel_dbeta = max_relative_error(dbeta_analytic, dbeta_numeric)\n\n    # Check pass criteria\n    return (rel_dx < tau) and (rel_dgamma < tau) and (rel_dbeta < tau)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1, 2, 3, 4\n    ]\n\n    results = []\n    for case_id in test_cases:\n        x, gamma, beta, eps, G, h, tau = make_case(case_id)\n        passed = run_case(x, gamma, beta, eps, G, h, tau)\n        results.append(passed)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3101647"}]}