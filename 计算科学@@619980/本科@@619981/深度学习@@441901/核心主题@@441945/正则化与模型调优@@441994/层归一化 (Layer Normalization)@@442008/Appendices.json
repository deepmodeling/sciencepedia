{"hands_on_practices": [{"introduction": "我们从层归一化 (Layer Normalization) 中一个至关重要的实现细节开始：添加到方差上的小常数 $\\epsilon$。本练习 [@problem_id:3142048] 将使用一个精心构造的“病态”输入，其特征方差会趋近于零。通过推导这种极限情况下的梯度，你将亲眼见证 $\\epsilon$ 如何扮演保护者的角色，防止数值溢出，并确保模型在这些退化情况下依然能够稳定训练。", "problem": "考虑对一个特征维度为 $d=3$ 的层进行单次层归一化（Layer Normalization, LN）的应用。对于一个输入向量 $x \\in \\mathbb{R}^{3}$，LN 计算特征上的均值 $\\mu(x)$ 和方差 $\\sigma^{2}(x)$，并返回归一化输出 $y(x)$，其共享的缩放参数 $\\gamma$ 和共享的平移参数 $\\beta$ 定义如下\n$$\n\\mu(x) = \\frac{1}{3}\\sum_{i=1}^{3} x_{i},\\qquad\n\\sigma^{2}(x) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_{i}-\\mu(x)\\big)^{2},\\qquad\ny_{i}(x) = \\gamma \\,\\frac{x_{i}-\\mu(x)}{\\sqrt{\\sigma^{2}(x)+\\epsilon}} + \\beta,\n$$\n其中 $\\epsilon>0$ 是一个小的稳定化常数。\n\n通过令\n$$\nx(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v,\\quad \\text{with}\\quad \\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix},\\quad v=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix},\\quad c\\in\\mathbb{R},\\quad \\alpha>0,\n$$\n构造一个接近退化方向（近零方差）的病态输入序列，并选择 $\\beta=0$ 和一个共享的正缩放参数 $\\gamma=g>0$。将标量损失定义为第一个 LN 输出坐标\n$$\nL(x) = y_{1}(x).\n$$\n\n任务：\n1. 仅从 $\\mu(x)$ 和 $\\sigma^{2}(x)$ 的定义出发，证明沿着 $x(\\alpha)$，有 $\\mu\\big(x(\\alpha)\\big)=c$ 和 $\\sigma^{2}\\big(x(\\alpha)\\big)=\\alpha^{2}\\,s_{v}^{2}$，其中 $s_{v}^{2}=\\frac{1}{3}\\sum_{i=1}^{3} v_{i}^{2}$。\n2. 使用链式法则和上述核心定义，推导出在 $\\alpha>0$ 和 $\\epsilon>0$ 的通用情况下的梯度 $\\nabla_{x}L\\big(x(\\alpha)\\big)$。\n3. 计算当 $\\alpha \\to 0^{+}$（等价于 $\\sigma^{2}\\to 0$）时该梯度的极限，并将关于 $x$ 的极限梯度向量以行矩阵的形式报告。此行矩阵将是您最终报告的答案。\n4. 在您的推导中，通过比较在假设 $\\epsilon=0$ 时您所得到的缩放情况，解释 $\\epsilon>0$ 的存在如何防止梯度在接近退化方向时发生爆炸。\n\n不需要进行数值取整。您的最终答案必须是单一的闭式极限梯度向量，用 $g$ 和 $\\epsilon$ 的符号表示，并以行矩阵的形式报告。", "solution": "该问题要求推导和分析层归一化（LN）输出相对于其输入的梯度，特别是在近零方差的极限情况下。\n\n首先，我将验证问题陈述。\n\n### 步骤 1：提取已知条件\n-   **定义域**: 输入向量 $x \\in \\mathbb{R}^{3}$ (特征维度 $d=3$)。\n-   **层归一化定义**:\n    -   均值: $\\mu(x) = \\frac{1}{3}\\sum_{i=1}^{3} x_{i}$\n    -   方差: $\\sigma^{2}(x) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_{i}-\\mu(x)\\big)^{2}$\n    -   输出: $y_{i}(x) = \\gamma \\,\\frac{x_{i}-\\mu(x)}{\\sqrt{\\sigma^{2}(x)+\\epsilon}} + \\beta$\n-   **稳定化常数**: $\\epsilon>0$。\n-   **病态输入序列**: $x(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v$，其中 $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$，$v=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$，$c\\in\\mathbb{R}$ 且 $\\alpha>0$。\n-   **参数**: 共享平移 $\\beta=0$，共享正缩放 $\\gamma=g>0$。\n-   **损失函数**: $L(x) = y_{1}(x)$。\n-   **辅助定义**: $s_{v}^{2}=\\frac{1}{3}\\sum_{i=1}^{3} v_{i}^{2}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题是科学基础扎实、定义明确且客观的。\n-   **科学基础扎实**: 该问题根植于深度学习领域，这是计算机科学和应用数学的一个子领域。层归一化的定义是标准的。在退化情况下的梯度分析是理解神经网络优化算法数值稳定性的一个关键方面。\n-   **定义明确**: 该问题提供了执行所要求的推导和分析所需的所有必要定义、约束和变量。它要求一个特定的极限梯度，正如将要展示的，在给定条件下该梯度存在且唯一。\n-   **客观性**: 该问题以精确的数学语言陈述，没有主观或含糊的术语。\n\n### 步骤 3：结论和行动\n该问题被认为是**有效的**。我将继续进行解答。\n\n解答根据问题中概述的四个任务进行组织。维度固定为 $d=3$。\n\n### 任务 1：$x(\\alpha)$ 的均值和方差\n输入序列定义为 $x(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v$。以分量形式表示，$x_i(\\alpha) = c + \\alpha v_i$，其中 $i \\in \\{1, 2, 3\\}$。\n\n首先，我们计算均值 $\\mu\\big(x(\\alpha)\\big)$：\n$$\n\\mu\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} x_i(\\alpha) = \\frac{1}{3}\\sum_{i=1}^{3} (c + \\alpha v_i) = \\frac{1}{3} \\left( \\sum_{i=1}^{3} c + \\alpha \\sum_{i=1}^{3} v_i \\right)\n$$\n向量 $v = \\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$ 的分量之和为 $\\sum_{i=1}^{3} v_i = 1+0+(-1) = 0$。和 $\\sum_{i=1}^{3} c = 3c$。\n因此，均值为：\n$$\n\\mu\\big(x(\\alpha)\\big) = \\frac{1}{3} (3c + \\alpha \\cdot 0) = c\n$$\n这表明均值在所定义的路径上是恒定的，等于 $c$。\n\n接下来，我们计算方差 $\\sigma^2\\big(x(\\alpha)\\big)$：\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_i(\\alpha) - \\mu(x(\\alpha))\\big)^2\n$$\n使用上述结果，平方内的项为 $x_i(\\alpha) - \\mu(x(\\alpha)) = (c + \\alpha v_i) - c = \\alpha v_i$。\n将此代入方差定义中：\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} (\\alpha v_i)^2 = \\frac{1}{3}\\sum_{i=1}^{3} \\alpha^2 v_i^2 = \\alpha^2 \\left( \\frac{1}{3}\\sum_{i=1}^{3} v_i^2 \\right)\n$$\n根据问题的定义，$s_v^2 = \\frac{1}{3}\\sum_{i=1}^{3} v_i^2$。这给出了最终结果：\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\alpha^2 s_v^2\n$$\n\n### 任务 2：梯度推导\n损失为 $L(x) = y_1(x)$。参数为 $\\gamma=g$ 和 $\\beta=0$。我们有：\n$$\nL(x) = y_1(x) = g \\frac{x_1 - \\mu(x)}{\\sqrt{\\sigma^2(x) + \\epsilon}}\n$$\n我们需要求梯度 $\\nabla_x L(x) = \\begin{pmatrix} \\frac{\\partial L}{\\partial x_1}  \\frac{\\partial L}{\\partial x_2}  \\frac{\\partial L}{\\partial x_3} \\end{pmatrix}$。让我们对任意分量 $k \\in \\{1, 2, 3\\}$ 求偏导数 $\\frac{\\partial L}{\\partial x_k}$。\n\n为了清晰起见，我们引入中间变量：$n_i(x) = x_i - \\mu(x)$ 和 $s(x) = \\sqrt{\\sigma^2(x) + \\epsilon}$。损失为 $L(x) = g \\, \\frac{n_1(x)}{s(x)}$。\n使用商法则，梯度为：\n$$\n\\nabla_x L(x) = g \\left( \\frac{\\nabla_x n_1(x)}{s(x)} - \\frac{n_1(x)}{s(x)^2} \\nabla_x s(x) \\right)\n$$\n我们需要 $n_1(x)$ 和 $s(x)$ 的梯度。维度为 $d=3$。\n均值 $\\mu(x) = \\frac{1}{3}\\sum_j x_j$ 的梯度是 $\\nabla_x \\mu(x) = \\frac{1}{3}\\mathbf{1}^T = \\begin{pmatrix} \\frac{1}{3}  \\frac{1}{3}  \\frac{1}{3} \\end{pmatrix}$。\n$n_1(x) = x_1 - \\mu(x)$ 的梯度是 $\\nabla_x n_1(x) = \\nabla_x x_1 - \\nabla_x \\mu(x) = e_1^T - \\frac{1}{3}\\mathbf{1}^T$，其中 $e_1^T = \\begin{pmatrix} 1  0  0 \\end{pmatrix}$。所以，$\\nabla_x n_1(x) = \\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix}$。\n\n接下来，求 $\\nabla_x s(x)$：\n$$\n\\nabla_x s(x) = \\nabla_x \\left( \\sigma^2(x) + \\epsilon \\right)^{1/2} = \\frac{1}{2s(x)} \\nabla_x \\sigma^2(x)\n$$\n方差 $\\sigma^2(x) = \\frac{1}{3}\\sum_j (x_j-\\mu)^2$ 的梯度是：\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_k} = \\frac{1}{3}\\sum_j 2(x_j-\\mu)\\left(\\frac{\\partial x_j}{\\partial x_k} - \\frac{\\partial \\mu}{\\partial x_k}\\right) = \\frac{2}{3}\\sum_j(x_j-\\mu)(\\delta_{jk} - \\frac{1}{3}) = \\frac{2}{3}\\left((x_k-\\mu) - \\frac{1}{3}\\sum_j(x_j-\\mu)\\right)\n$$\n由于 $\\sum_j(x_j-\\mu) = 0$，我们有 $\\frac{\\partial \\sigma^2}{\\partial x_k} = \\frac{2}{3}(x_k-\\mu) = \\frac{2}{3}n_k(x)$。\n以向量形式表示，$\\nabla_x \\sigma^2(x) = \\frac{2}{3}n(x)$，其中 $n(x)$ 是分量为 $n_k(x)$ 的向量。\n所以，$\\nabla_x s(x) = \\frac{1}{2s(x)} \\frac{2}{3}n(x) = \\frac{n(x)}{3s(x)}$。\n\n将这些梯度代回到 $\\nabla_x L(x)$ 的表达式中：\n$$\n\\nabla_x L(x) = g \\left( \\frac{e_1^T - \\frac{1}{3}\\mathbf{1}^T}{s(x)} - \\frac{n_1(x)}{s(x)^2} \\frac{n(x)}{3s(x)} \\right) = \\frac{g}{s(x)}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g n_1(x)}{3s(x)^3}n(x)\n$$\n这是一般的梯度表达式。现在我们在 $x=x(\\alpha)$ 处对其进行求值：\n-   $n(x(\\alpha)) = x(\\alpha) - \\mu(x(\\alpha))\\mathbf{1} = (c\\mathbf{1} + \\alpha v) - c\\mathbf{1} = \\alpha v$。\n-   $n_1(x(\\alpha)) = \\alpha v_1$。\n-   $s(x(\\alpha)) = \\sqrt{\\sigma^2(x(\\alpha)) + \\epsilon} = \\sqrt{\\alpha^2 s_v^2 + \\epsilon}$。\n\n将这些代入梯度公式中：\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g (\\alpha v_1)}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}(\\alpha v)\n$$\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v\n$$\n\n### 任务 3：当 $\\alpha \\to 0^+$ 时的极限梯度\n我们现在求推导出的梯度在 $\\alpha \\to 0^+$ 时的极限：\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\lim_{\\alpha \\to 0^+} \\left( \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v \\right)\n$$\n我们分别计算每一项的极限。\n对于第一项，当 $\\alpha \\to 0^+$ 时，分母变为 $\\sqrt{0 + \\epsilon} = \\sqrt{\\epsilon}$。\n$$\n\\lim_{\\alpha \\to 0^+} \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) = \\frac{g}{\\sqrt{\\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right)\n$$\n对于第二项，分子包含一个因子 $\\alpha^2$，它趋向于 $0$。分母趋向于 $3\\epsilon^{3/2}$，由于 $\\epsilon > 0$，这是一个非零常数。\n$$\n\\lim_{\\alpha \\to 0^+} \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v = \\frac{g \\cdot 0 \\cdot v_1}{3\\epsilon^{3/2}}v = \\mathbf{0}\n$$\n合并极限，我们得到：\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right)\n$$\n代入向量 $e_1^T = \\begin{pmatrix} 1  0  0 \\end{pmatrix}$ 和 $\\mathbf{1}^T = \\begin{pmatrix} 1  1  1 \\end{pmatrix}$：\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\left(\\begin{pmatrix} 1  0  0 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} 1  1  1 \\end{pmatrix}\\right) = \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} 1-\\frac{1}{3}  0-\\frac{1}{3}  0-\\frac{1}{3} \\end{pmatrix}\n$$\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{2g}{3\\sqrt{\\epsilon}}  -\\frac{g}{3\\sqrt{\\epsilon}}  -\\frac{g}{3\\sqrt{\\epsilon}} \\end{pmatrix}\n$$\n这就是极限梯度向量。\n\n### 任务 4：$\\epsilon > 0$ 的作用\n为了理解 $\\epsilon$ 的作用，我们比较在有和没有 $\\epsilon$ 的情况下，当 $\\alpha \\to 0^+$ 时梯度大小的行为。\n\n**情况 1：$\\epsilon > 0$（如上推导）**\n梯度 $\\nabla_x L\\big(x(\\alpha)\\big)$ 在 $\\alpha \\to 0^+$ 时趋近于一个有限的常数向量。极限梯度的模为：\n$$\n\\left\\| \\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) \\right\\| = \\left\\| \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} \\frac{2}{3}  -\\frac{1}{3}  -\\frac{1}{3} \\end{pmatrix} \\right\\| = \\frac{g}{\\sqrt{\\epsilon}} \\sqrt{(\\frac{2}{3})^2 + (-\\frac{1}{3})^2 + (-\\frac{1}{3})^2} = \\frac{g}{\\sqrt{\\epsilon}}\\sqrt{\\frac{4+1+1}{9}} = \\frac{g\\sqrt{6}}{3\\sqrt{\\epsilon}}\n$$\n这是一个有限的非零值。$\\epsilon>0$ 的存在确保了 LN 公式中的分母 $\\sqrt{\\sigma^2(x)+\\epsilon}$ 有一个下界 $\\sqrt{\\epsilon}$，从而防止了梯度的爆炸。\n\n**情况 2：假设 $\\epsilon = 0$**\n如果我们设置 $\\epsilon=0$，梯度表达式变为：\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} = \\frac{g}{\\sqrt{\\alpha^2 s_v^2}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2)^{3/2}}v\n$$\n因为 $\\alpha > 0$，所以 $\\sqrt{\\alpha^2 s_v^2} = \\alpha s_v$。\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} = \\frac{g}{\\alpha s_v}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3\\alpha^3 s_v^3}v = \\frac{1}{\\alpha}\\left[ \\frac{g}{s_v}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g v_1}{3 s_v^3}v \\right]\n$$\n方括号中的项是一个通常非零的常数向量。因此，梯度向量的模与 $1/\\alpha$ 成比例。当 $\\alpha \\to 0^+$ 时，方差 $\\sigma^2 = \\alpha^2 s_v^2 \\to 0$，梯度的模发散：\n$$\n\\left\\| \\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} \\right\\| \\to \\infty\n$$\n这种梯度的“爆炸”对应于优化算法中的数值不稳定性。稳定化常数 $\\epsilon > 0$至关重要，因为它确保了即使对于具有近零方差的输入，归一化分母仍然有界且不为零，从而产生行为良好、有限的梯度。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2g}{3\\sqrt{\\epsilon}}  -\\frac{g}{3\\sqrt{\\epsilon}}  -\\frac{g}{3\\sqrt{\\epsilon}} \\end{pmatrix}}\n$$", "id": "3142048"}, {"introduction": "在许多实际应用中，例如自然语言处理，我们的输入数据包含应被忽略的填充（padding）或缺失数据。本练习 [@problem_id:3142018] 要求你调整层归一化算法以处理带掩码的输入，确保仅对有效数据点计算统计量（均值和方差）。这项练习不仅将考验你对 LN 核心公式的理解，还会引导你思考当有效特征数量变得非常少时，算法的数值稳定性问题。", "problem": "在深度神经网络中，单个输入样本具有特征向量 $x \\in \\mathbb{R}^{d}$ 和一个二元有效性掩码 $m \\in \\{0,1\\}^{d}$，其中 $m_{i} = 1$ 表示特征 $x_{i}$ 是有效的，而 $m_{i} = 0$ 表示其无效或缺失。层归一化（Layer Normalization, LN）是逐样本跨特征应用的。从有限实数集上的算术平均值和方差的基本定义出发，并采用将总和除以被平均元素数量的总体方差约定。\n\n任务1：推导仅使用由 $m$ 指示的有效特征的掩码均值 $\\mu_{m}$ 和掩码方差 $\\sigma_{m}^{2}$ 的表达式。你的推导必须从算术平均值和方差的定义开始，并且必须与对子集 $\\{i : m_{i} = 1\\}$ 进行平均化保持一致。\n\n任务2：考虑一个掩码层归一化，它使用任务1中的掩码统计量，并通过对每个有效索引 $i$ 执行以下序列来仅对有效特征应用归一化：减去掩码均值，除以掩码方差加上一个严格为正的平滑参数 $\\epsilon$ 的平方根，然后应用一个带有参数 $\\gamma$ 和 $\\beta$ 的标量仿射变换。无效特征保持不变。对于特定实例 $d = 4$，其中 $x = (2, -1, 3, 0)$，$m = (1, 0, 1, 0)$，$\\gamma = 1$，$\\beta = 0$ 和 $\\epsilon = 1 \\times 10^{-5}$，计算索引 $i = 1$ 处的归一化输出值。报告你的数值答案，四舍五入到四位有效数字。\n\n任务3：使用任务1中的表达式，分析当 $\\sum_{i=1}^{d} m_{i}$ 很小时，掩码层归一化的数值稳定性。你的分析应讨论方差的估计量如何表现、$\\epsilon$ 的作用以及归一化因子的潜在量级。任务3无需数值答案。", "solution": "我们从基本定义开始。对于一个有限实数多重集 $\\{z_{1}, z_{2}, \\dots, z_{k}\\}$，算术平均值由下式给出\n$$\n\\bar{z} \\;=\\; \\frac{1}{k} \\sum_{j=1}^{k} z_{j},\n$$\n而总体方差（即二阶中心矩，除以 $k$）是\n$$\n\\operatorname{Var}(z) \\;=\\; \\frac{1}{k} \\sum_{j=1}^{k} (z_{j} - \\bar{z})^{2}.\n$$\n在我们的掩码设置中，令 $K = \\sum_{i=1}^{d} m_{i}$ 表示有效特征的数量，并令有效索引集为 $S = \\{i \\in \\{1,\\dots,d\\} : m_{i} = 1\\}$。掩码均值仅聚合有效条目。对集合 $\\{x_{i} : i \\in S\\}$ 使用均值定义，我们得到\n$$\n\\mu_{m} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} x_{i}.\n$$\n等价地，由于 $m_{i} \\in \\{0,1\\}$ 用于选择有效特征，\n$$\n\\mu_{m} \\;=\\; \\frac{\\sum_{i=1}^{d} m_{i} x_{i}}{\\sum_{i=1}^{d} m_{i}}.\n$$\n接下来，掩码方差直接由应用于同一有效集的总体方差定义得出：\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} (x_{i} - \\mu_{m})^{2} \\;=\\; \\frac{\\sum_{i=1}^{d} m_{i} (x_{i} - \\mu_{m})^{2}}{\\sum_{i=1}^{d} m_{i}}.\n$$\n这完成了任务1。\n\n对于任务2，掩码层归一化通过执行带有平滑和仿射参数的标准化变换，仅在索引 $i$ 处（其中 $m_i = 1$）应用归一化。具体来说，对于有效的 $i$，\n$$\ny_{i} \\;=\\; \\gamma \\,\\frac{x_{i} - \\mu_{m}}{\\sqrt{\\sigma_{m}^{2} + \\epsilon}} \\;+\\; \\beta,\n$$\n而对于无效的 $i$（其中 $m_i=0$），我们设置 $y_i=x_i$。对于 $x = (2, -1, 3, 0)$，$m = (1, 0, 1, 0)$，我们有 $K = \\sum_{i=1}^{4} m_{i} = 2$。掩码均值为\n$$\n\\mu_{m} \\;=\\; \\frac{m_{1} x_{1} + m_{2} x_{2} + m_{3} x_{3} + m_{4} x_{4}}{m_{1} + m_{2} + m_{3} + m_{4}} \\;=\\; \\frac{1 \\cdot 2 + 0 \\cdot (-1) + 1 \\cdot 3 + 0 \\cdot 0}{2} \\;=\\; \\frac{5}{2} \\;=\\; 2.5.\n$$\n掩码方差为\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{m_{1} (x_{1} - \\mu_{m})^{2} + m_{2} (x_{2} - \\mu_{m})^{2} + m_{3} (x_{3} - \\mu_{m})^{2} + m_{4} (x_{4} - \\mu_{m})^{2}}{K}\n\\;=\\; \\frac{(2 - 2.5)^{2} + (3 - 2.5)^{2}}{2}\n\\;=\\; \\frac{0.25 + 0.25}{2}\n\\;=\\; 0.25.\n$$\n当 $\\gamma = 1$，$\\beta = 0$ 和 $\\epsilon = 1 \\times 10^{-5}$ 时，分母为\n$$\n\\sqrt{\\sigma_{m}^{2} + \\epsilon} \\;=\\; \\sqrt{0.25 + 1 \\times 10^{-5}} \\;=\\; \\sqrt{0.25001}.\n$$\n因此，\n$$\ny_{1} \\;=\\; 1 \\cdot \\frac{2 - 2.5}{\\sqrt{0.25001}} + 0 \\;=\\; \\frac{-0.5}{\\sqrt{0.25001}}.\n$$\n为了获得一个数值，请注意 $\\sqrt{0.25001}$ 略大于 $0.5$。使用恒等式 $\\sqrt{0.25 + a} = 0.5 \\sqrt{1 + 4a}$，对于 $a = 10^{-5}$，我们有 $\\sqrt{0.25001} = 0.5 \\sqrt{1 + 4 \\times 10^{-5}}$。对于小的 $\\delta$，使用一阶展开 $\\sqrt{1 + \\delta} \\approx 1 + \\frac{\\delta}{2}$，其中 $\\delta = 4 \\times 10^{-5}$，可得\n$$\n\\sqrt{0.25001} \\;\\approx\\; 0.5 \\left(1 + 2 \\times 10^{-5}\\right) \\;=\\; 0.50001.\n$$\n因此，\n$$\ny_{1} \\;\\approx\\; \\frac{-0.5}{0.50001} \\;=\\; -\\frac{0.5}{0.50001} \\;=\\; -\\frac{1}{1 + 2 \\times 10^{-5}} \\;\\approx\\; -(1 - 2 \\times 10^{-5}) \\;=\\; -0.99998.\n$$\n四舍五入到四位有效数字，结果是 $-1.000$。\n\n对于任务3，我们分析当 $K = \\sum_{i=1}^{d} m_{i}$ 很小时的稳定性。掩码方差\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} (x_{i} - \\mu_{m})^{2}\n$$\n是一个在 $K$ 个样本上计算的估计量。当 $K$ 很小时，该估计量表现出高方差（在统计学意义上），如果有效值几乎相等，这会使得分母 $\\sqrt{\\sigma_{m}^{2} + \\epsilon}$ 可能非常小，或者在样本间变化很大。如果没有平滑项 $\\epsilon > 0$，分母可能会趋近于零，产生极大的归一化值和不稳定的梯度。平滑项 $\\epsilon$ 为分母提供了一个下界 $\\sqrt{\\epsilon}$，从而得出量级界限\n$$\n\\left|\\frac{x_{i} - \\mu_{m}}{\\sqrt{\\sigma_{m}^{2} + \\epsilon}}\\right| \\;\\leq\\; \\frac{|x_{i} - \\mu_{m}|}{\\sqrt{\\epsilon}},\n$$\n因此，适当地选择 $\\epsilon$ 可以减轻数值爆炸。然而，小的 $K$ 也使得 $\\mu_{m}$ 本身对单个特征敏感，这会放大 $x_{i} - \\mu_{m}$ 的波动。在 $K=1$ 的极端情况下，对于唯一的有效索引 $i^{\\star}$，我们有 $\\mu_{m} = x_{i^{\\star}}$，这导致 $\\sigma_{m}^{2} = 0$ 和归一化偏差 $x_{i^{\\star}} - \\mu_{m} = 0$，因此 $y_{i^{\\star}} = \\beta$ 是良性的；当 $K$ 很小但至少为2，且有效值几乎相同或方差估计有噪声时，不稳定性更容易出现。因此，实际的稳定性取决于 $K$、$\\{x_{i} : i \\in S\\}$ 的离散程度以及 $\\epsilon$ 和 $\\gamma$ 的选择之间的相互作用；当 $K$ 很小时，较大的 $\\epsilon$ 和适度的 $\\gamma$ 会降低敏感性。", "answer": "$$\\boxed{-1.000}$$", "id": "3142018"}, {"introduction": "在探究了层归一化的内部机制后，我们现在来分析它为何能如此有效地稳定训练过程。这最后一个练习 [@problem_id:3142044] 提供了一个理论分析，比较了一个简单的神经网络在有和没有中间 LN 层两种情况下的训练动态。通过分析归一化后特征的协方差以及最大稳定学习率，你将得出一个严谨的结论，揭示层归一化如何改善优化问题的条件数，从而为其广泛应用提供更深层次的理论依据。", "problem": "考虑一个标量输出的两层线性网络，层间使用层归一化（LN）。设隐藏层宽度为 $H \\geq 2$。设输入 $x \\in \\mathbb{R}^{H}$ 从零均值、单位协方差的高斯分布中抽取，即 $x \\sim \\mathcal{N}(0, I_{H})$。目标由一个线性教师模型生成，$y = t^{\\top} x$，其中 $t \\in \\mathbb{R}^{H}$ 是一个固定但任意的向量。我们比较两个模型，它们唯一的区别在于两个线性层之间是否存在 LN，并且我们只通过在总体最小二乘风险上进行全批量梯度下降（GD）来训练第二层的权重。\n\n不带 LN 的模型：网络输出为 $f_{\\mathrm{noLN}}(x; u) = u^{\\top} x$，其中 $u \\in \\mathbb{R}^{H}$ 是可训练的顶层权重。\n\n带 LN 的模型：网络输出为 $f_{\\mathrm{LN}}(x; u) = u^{\\top} h$，其中 $h = \\mathrm{LN}(x)$ 是层归一化的输入向量，它是在特征维度上对每个样本计算的，不带仿射参数：\n$$\nm(x) = \\frac{1}{H} \\mathbf{1}^{\\top} x, \n\\quad \ns(x) = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H} \\bigl(x_{i} - m(x)\\bigr)^{2}},\n\\quad \nh = \\frac{x - m(x)\\mathbf{1}}{s(x)} \\in \\mathbb{R}^{H}.\n$$\n这里 $\\mathbf{1} \\in \\mathbb{R}^{H}$ 表示全为 1 的向量。\n\n在两种情况下，我们都最小化总体风险\n$$\n\\mathcal{L}(u) = \\frac{1}{2}\\,\\mathbb{E}\\bigl[(f(x; u) - y)^{2}\\bigr]\n$$\n使用固定步长 $\\eta > 0$ 的全批量梯度下降法：\n$$\nu_{k+1} = u_{k} - \\eta \\nabla \\mathcal{L}(u_{k}).\n$$\n\n任务：\n1) 从最小二乘总体风险的定义和梯度下降更新规则出发，推导两种模型的闭式训练动态 $u_{k}$。你的推导应将 $u_{k}$ 表示为 $u_{0}$、$\\eta$ 和相关特征二阶统计量的函数，并应明确特征协方差在动态过程中的作用。\n\n2) 对于带 LN 的模型，确定由上述高斯输入和逐样本 LN 定义所隐含的特征协方差矩阵 $\\Sigma_{\\mathrm{LN}} = \\mathbb{E}[h h^{\\top}]$ 的谱。使用对称性和 LN 施加的约束来证明你的结果。\n\n3) 使用你的结果，确定在每个模型中保证损失单调收敛（等价于线性 GD 动态的稳定性）的最大步长 $\\eta_{\\max}$。\n\n4) 最后，计算这些最大稳定学习率的比率 $r(H)$，\n$$\nr(H) = \\frac{\\eta_{\\max}^{\\mathrm{LN}}}{\\eta_{\\max}^{\\mathrm{noLN}}},\n$$\n以 $H$ 的闭式表达式形式给出。请将你的最终答案表示为单个解析表达式。不需要四舍五入。", "solution": "用户希望分析两种线性模型（一种带层归一化 LN，一种不带）的训练动态，以确定每种模型的最大稳定学习率及其比率。\n\n### 第 1 步：提取已知条件\n- 输入：$x \\in \\mathbb{R}^{H}$，$x \\sim \\mathcal{N}(0, I_{H})$，其中 $H \\geq 2$ 是隐藏层宽度。\n- 目标：$y = t^{\\top} x$，其中 $t \\in \\mathbb{R}^{H}$ 是一个固定向量。\n- 不带 LN 的模型：$f_{\\mathrm{noLN}}(x; u) = u^{\\top} x$，其中 $u \\in \\mathbb{R}^{H}$ 是可训练的权重。\n- 带 LN 的模型：$f_{\\mathrm{LN}}(x; u) = u^{\\top} h$，其中 $h = \\mathrm{LN}(x)$ 是归一化的输入。\n- LN 定义（无仿射参数）：\n  - 均值：$m(x) = \\frac{1}{H} \\mathbf{1}^{\\top} x$\n  - 标准差：$s(x) = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H} \\bigl(x_{i} - m(x)\\bigr)^{2}}$\n  - 归一化向量：$h = \\frac{x - m(x)\\mathbf{1}}{s(x)}$\n  - $\\mathbf{1} \\in \\mathbb{R}^{H}$ 是全为 1 的向量。\n- 风险函数：$\\mathcal{L}(u) = \\frac{1}{2}\\,\\mathbb{E}\\bigl[(f(x; u) - y)^{2}\\bigr]$。\n- 训练算法：固定步长 $\\eta > 0$ 的全批量梯度下降（GD）。\n- 更新规则：$u_{k+1} = u_{k} - \\eta \\nabla \\mathcal{L}(u_{k})$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学严谨性：** 该问题是深度学习领域一个明确定义的理论练习，基于标准的优化和统计数学原理。它在科学上是合理的。\n- **良构性：** 问题提供了所有必要的定义，并要求推导特定的、可导出的量。这些任务导向一个唯一且有意义的解。\n- **客观性：** 问题使用精确的数学语言陈述，没有歧义或主观论断。\n\n问题陈述是完整的、一致的且科学有效的。\n\n### 第 3 步：判断与行动\n问题有效。我将继续进行解答。\n\n### 任务 1：推导训练动态\n\n风险是权重 $u$ 的二次函数。梯度的一般形式为：\n$$\n\\nabla_u \\mathcal{L}(u) = \\nabla_u \\frac{1}{2}\\,\\mathbb{E}\\bigl[(u^{\\top} z - y)^{2}\\bigr] = \\mathbb{E}\\bigl[(u^{\\top} z - y)z\\bigr] = \\mathbb{E}[zz^{\\top}]u - \\mathbb{E}[zy^{\\top}]\n$$\n其中 $z$ 表示给予最后一层的特征。GD 更新为 $u_{k+1} = u_k - \\eta(\\mathbb{E}[zz^{\\top}]u_k - \\mathbb{E}[zy^{\\top}])$。这是一个线性动力系统。令 $u^*$ 为不动点（最优权重），它满足 $\\nabla_u \\mathcal{L}(u^*) = 0$，即 $\\mathbb{E}[zz^{\\top}]u^* = \\mathbb{E}[zy^{\\top}]$。误差向量 $e_k = u_k - u^*$ 的演化如下：\n$$\ne_{k+1} = u_{k+1} - u^* = (I - \\eta\\mathbb{E}[zz^{\\top}])u_k + \\eta\\mathbb{E}[zy^{\\top}] - u^* = (I - \\eta\\mathbb{E}[zz^{\\top}])(e_k+u^*) + \\eta\\mathbb{E}[zy^{\\top}] - u^*\n$$\n$$\ne_{k+1} = (I - \\eta\\mathbb{E}[zz^{\\top}])e_k + u^* - \\eta\\mathbb{E}[zz^{\\top}]u^* + \\eta\\mathbb{E}[zy^{\\top}] - u^* = (I - \\eta\\mathbb{E}[zz^{\\top}])e_k\n$$\n解为 $e_k = (I - \\eta\\mathbb{E}[zz^{\\top}])^k e_0$，这给出了 $u_k$ 的闭式动态：\n$$\nu_k = u^* + (I - \\eta\\mathbb{E}[zz^{\\top}])^k (u_0 - u^*)\n$$\n\n**不带 LN 的模型：**\n对于这个模型，特征向量是 $z=x$。目标是 $y=t^\\top x$。\n所需的二阶统计量为：\n- 特征协方差：$\\Sigma_{\\mathrm{noLN}} = \\mathbb{E}[xx^{\\top}]$。因为 $x \\sim \\mathcal{N}(0, I_H)$，所以 $\\mathbb{E}[x]=0$ 且 $\\mathrm{Cov}(x) = I_H$。因此，$\\Sigma_{\\mathrm{noLN}} = \\mathrm{Cov}(x) + \\mathbb{E}[x]\\mathbb{E}[x]^\\top = I_H$。\n- 互协方差：$\\mathbb{E}[xy^{\\top}] = \\mathbb{E}[x(t^\\top x)^\\top] = \\mathbb{E}[xx^{\\top}]t = \\Sigma_{\\mathrm{noLN}} t = I_H t = t$。\n最优权重 $u^*$ 满足 $\\Sigma_{\\mathrm{noLN}} u^* = \\mathbb{E}[xy^{\\top}]$，即 $I_H u^* = t$，所以 $u^*=t$。\n\n不带 LN 模型的训练动态为：\n$$\nu_k = t + (I_H - \\eta I_H)^k (u_0 - t) = t + (1-\\eta)^k(u_0 - t)\n$$\n动态由特征协方差矩阵 $\\Sigma_{\\mathrm{noLN}} = I_H$ 决定。由于它是单位矩阵，误差向量 $u_k - t$ 的所有分量都以相同的速率 $(1-\\eta)$ 衰减。\n\n**带 LN 的模型：**\n对于这个模型，特征向量是 $z=h=\\mathrm{LN}(x)$。目标仍然是 $y=t^\\top x$。\n所需的二阶统计量为：\n- 特征协方差：$\\Sigma_{\\mathrm{LN}} = \\mathbb{E}[hh^{\\top}]$。\n- 交叉项：$C = \\mathbb{E}[hy^{\\top}] = \\mathbb{E}[h(t^\\top x)^\\top] = \\mathbb{E}[hx^{\\top}]t$。\n梯度为 $\\nabla \\mathcal{L}(u) = \\Sigma_{\\mathrm{LN}} u - C$。GD 更新为 $u_{k+1} = u_k - \\eta(\\Sigma_{\\mathrm{LN}} u_k - C)$。\n\n最优权重 $u^*$ 是 $\\Sigma_{\\mathrm{LN}} u^* = C$ 的解。我们将在任务 2 中证明，$\\Sigma_{\\mathrm{LN}}$ 是奇异的，所以不动点不是唯一的。然而，误差 $u_k - u^*$（对于任何有效的不动点 $u^*$）的动态仍然由相同的齐次方程决定。闭式动态为：\n$$\nu_k = u^* + (I_H - \\eta \\Sigma_{\\mathrm{LN}})^k (u_0 - u^*)\n$$\n其中 $u^*$ 是 $\\Sigma_{\\mathrm{LN}} u^* = \\mathbb{E}[hx^{\\top}]t$ 的任意解。特征协方差 $\\Sigma_{\\mathrm{LN}}$ 决定了权重可学习部分的演化，而交叉项 $\\mathbb{E}[hx^{\\top}]t$ 决定了目标解流形。\n\n### 任务 2：特征协方差矩阵 $\\Sigma_{\\mathrm{LN}}$ 的谱\n\n根据构造，对于任何 $x$，归一化向量 $h$ 都具有两个关键属性：\n1.  其分量之和为零：$\\mathbf{1}^{\\top}h = \\mathbf{1}^{\\top} \\frac{x - m(x)\\mathbf{1}}{s(x)} = \\frac{\\mathbf{1}^{\\top}x - m(x)\\mathbf{1}^{\\top}\\mathbf{1}}{s(x)} = \\frac{H m(x) - m(x)H}{s(x)} = 0$。\n2.  其欧几里得范数的平方为 $H$：$h^{\\top}h = \\frac{(x-m(x)\\mathbf{1})^{\\top}(x-m(x)\\mathbf{1})}{s(x)^2} = \\frac{\\sum_{i=1}^H (x_i-m(x))^2}{\\frac{1}{H}\\sum_{i=1}^H (x_i-m(x))^2} = H$。\n\n现在我们计算 $\\Sigma_{\\mathrm{LN}} = \\mathbb{E}[hh^{\\top}]$。\n从属性 1，取期望：$\\mathbb{E}[\\mathbf{1}^{\\top}h] = \\mathbf{1}^{\\top}\\mathbb{E}[h]=0$。此外，$\\mathbf{1}^{\\top}\\Sigma_{\\mathrm{LN}} = \\mathbf{1}^{\\top}\\mathbb{E}[hh^{\\top}] = \\mathbb{E}[(\\mathbf{1}^{\\top}h)h^{\\top}] = \\mathbb{E}[0 \\cdot h^{\\top}] = \\mathbf{0}^{\\top}$。这意味着 $\\Sigma_{\\mathrm{LN}}\\mathbf{1} = (\\mathbf{1}^{\\top}\\Sigma_{\\mathrm{LN}})^{\\top} = \\mathbf{0}$。因此，$\\mathbf{1}$ 是 $\\Sigma_{\\mathrm{LN}}$ 的一个特征向量，其特征值为 $0$。\n\n从属性 2，取迹的期望：$\\mathrm{Tr}(\\Sigma_{\\mathrm{LN}}) = \\mathrm{Tr}(\\mathbb{E}[hh^{\\top}]) = \\mathbb{E}[\\mathrm{Tr}(hh^{\\top})] = \\mathbb{E}[h^{\\top}h] = \\mathbb{E}[H] = H$。\n\n输入分布 $\\mathcal{N}(0,I_H)$ 在坐标置换下是不变的。LN 是一种置换等变操作。因此，矩阵 $\\Sigma_{\\mathrm{LN}}$ 必须在其索引置换下保持不变。这意味着它具有以下结构：\n$$\n(\\Sigma_{\\mathrm{LN}})_{ij} = \\begin{cases} c_1  i=j \\\\ c_2  i \\neq j \\end{cases}\n$$\n其中 $c_1, c_2$ 是某个常数。我们可以将 $\\Sigma_{\\mathrm{LN}}$ 写成 $\\Sigma_{\\mathrm{LN}} = (c_1-c_2)I_H + c_2\\mathbf{1}\\mathbf{1}^{\\top}$。\n由 $\\mathrm{Tr}(\\Sigma_{\\mathrm{LN}}) = H$，我们有 $H c_1 = H$，这意味着 $c_1=1$。\n由 $\\Sigma_{\\mathrm{LN}}\\mathbf{1} = \\mathbf{0}$，任何一列的和必须为零：$c_1 + (H-1)c_2 = 0$。\n代入 $c_1=1$，我们得到 $1 + (H-1)c_2 = 0$，所以 $c_2 = -\\frac{1}{H-1}$。\n\n该矩阵为 $\\Sigma_{\\mathrm{LN}} = (1 - (-\\frac{1}{H-1}))I_H + (-\\frac{1}{H-1})\\mathbf{1}\\mathbf{1}^{\\top} = \\frac{H}{H-1}I_H - \\frac{1}{H-1}\\mathbf{1}\\mathbf{1}^{\\top}$。\n我们来求它的谱。我们已经知道一个特征值为 $0$（特征向量为 $\\mathbf{1}$）。\n设 $v$ 是任何与 $\\mathbf{1}$ 正交的向量（即 $\\mathbf{1}^{\\top}v=0$）。存在一个 $(H-1)$ 维的此类向量子空间。\n$$\n\\Sigma_{\\mathrm{LN}} v = \\left(\\frac{H}{H-1}I_H - \\frac{1}{H-1}\\mathbf{1}\\mathbf{1}^{\\top}\\right)v = \\frac{H}{H-1}v - \\frac{1}{H-1}\\mathbf{1}(\\mathbf{1}^{\\top}v) = \\frac{H}{H-1}v - 0 = \\frac{H}{H-1}v\n$$\n所以，在与 $\\mathbf{1}$ 正交的子空间中的任何向量都是特征向量，其特征值为 $\\frac{H}{H-1}$。\n$\\Sigma_{\\mathrm{LN}}$ 的谱由以下部分组成：\n- 一个等于 $0$ 的特征值（重数为 $1$）。\n- 一个等于 $\\frac{H}{H-1}$ 的特征值（重数为 $H-1$）。\n\n### 任务 3：确定最大稳定步长 $\\eta_{\\max}$\n\n对于线性 GD 更新 $e_{k+1} = (I - \\eta \\Sigma) e_k$，如果更新矩阵 $(I - \\eta \\Sigma)$ 的谱半径不大于 $1$，则保证损失的单调收敛（稳定性）。$(I - \\eta \\Sigma)$ 的特征值是 $1-\\eta\\lambda_i$，其中 $\\lambda_i$ 是 $\\Sigma$ 的特征值。条件是对于所有 $i$，$|1 - \\eta\\lambda_i| \\le 1$。\n由于 $\\Sigma$ 是一个协方差矩阵，其特征值是非负的（$\\lambda_i \\ge 0$）。该条件变为：\n$$\n-1 \\le 1 - \\eta\\lambda_i \\le 1 \\implies 0 \\le \\eta\\lambda_i \\le 2\n$$\n对于 $\\lambda_i > 0$，这要求 $\\eta \\le \\frac{2}{\\lambda_i}$。为了对所有特征值都满足这个条件，我们必须有 $\\eta \\le \\frac{2}{\\lambda_{\\max}(\\Sigma)}$，其中 $\\lambda_{\\max}(\\Sigma)$ 是 $\\Sigma$ 的最大特征值。\n因此，最大稳定步长为 $\\eta_{\\max} = \\frac{2}{\\lambda_{\\max}(\\Sigma)}$。\n\n**不带 LN 的模型：**\n特征协方差为 $\\Sigma_{\\mathrm{noLN}}=I_H$。所有特征值都为 $1$。\n$\\lambda_{\\max}(\\Sigma_{\\mathrm{noLN}}) = 1$。\n最大稳定步长为 $\\eta_{\\max}^{\\mathrm{noLN}} = \\frac{2}{1} = 2$。\n\n**带 LN 的模型：**\n特征协方差为 $\\Sigma_{\\mathrm{LN}}$。其非零特征值均为 $\\frac{H}{H-1}$。\n由于 $H \\geq 2$，我们有 $\\frac{H}{H-1} = 1 + \\frac{1}{H-1} > 1$。最大特征值为 $\\lambda_{\\max}(\\Sigma_{\\mathrm{LN}}) = \\frac{H}{H-1}$。\n最大稳定步长为 $\\eta_{\\max}^{\\mathrm{LN}} = \\frac{2}{\\lambda_{\\max}(\\Sigma_{\\mathrm{LN}})} = \\frac{2}{H/(H-1)} = \\frac{2(H-1)}{H}$。\n\n### 任务 4：计算比率 $r(H)$\n\n最大稳定学习率的比率为：\n$$\nr(H) = \\frac{\\eta_{\\max}^{\\mathrm{LN}}}{\\eta_{\\max}^{\\mathrm{noLN}}} = \\frac{2(H-1)/H}{2} = \\frac{H-1}{H}\n$$\n这也可以写成 $1 - \\frac{1}{H}$。", "answer": "$$\n\\boxed{\\frac{H-1}{H}}\n$$", "id": "3142044"}]}