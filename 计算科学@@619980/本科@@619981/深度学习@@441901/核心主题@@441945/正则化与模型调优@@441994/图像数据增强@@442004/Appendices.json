{"hands_on_practices": [{"introduction": "数据增强并非“一刀切”的解决方案；其有效性高度依赖于数据本身的特性以及特定任务的需求。此练习旨在通过构建一个合成数据集来阐明这一点，其中图像的类别完全由其全局空间排列模式（而非局部纹理）决定。这个巧妙设计的任务为我们提供了一个理想的实验平台，用于检验那些遮挡或混合图像区域的增强方法（如 Cutout 和 CutMix）将如何影响模型学习这些关键全局线索的能力 ([@problem_id:3151909])。通过这个实践，你将亲身体会到为特定问题选择合适增强策略的重要性。", "problem": "你必须编写一个完整、可运行的程序，该程序构建一个合成图像数据集，其中类别身份由全局空间排列决定；在不同的增强配置下训练一个多项逻辑回归模型；并评估这些增强是否会破坏模型学习全局上下文的能力。你的方法应基于softmax模型下的交叉熵经验风险最小化，以及对数据增强的正式定义。数据集和学习问题必须是纯粹数学和逻辑上定义的，不依赖任何外部文件。你的程序必须使用线性代数运算从头开始实现所有步骤。\n\n数据集设计：创建大小为 $H \\times W$ 的灰度图像，其强度值在 $[0,1]$ 范围内。数据集恰好有两个类别，使用 $\\{[1,0],[0,1]\\}$ 中的独热向量进行编码。每张图像被划分为四个相等的象限。设 $c_{\\mathrm{lo}} \\in (0,1)$ 和 $c_{\\mathrm{hi}} \\in (0,1)$，且 $c_{\\mathrm{lo}}  c_{\\mathrm{hi}}$。定义类别条件生成规则如下：\n- 对于类别 $0$：左上和右下象限填充为 $c_{\\mathrm{hi}}$，右上和左下象限填充为 $c_{\\mathrm{lo}}$。\n- 对于类别 $1$：左上和右下象限填充为 $c_{\\mathrm{lo}}$，右上和左下象限填充为 $c_{\\mathrm{hi}}$。\n在此确定性构造之后，向每个像素添加独立的高斯噪声 $\\mathcal{N}(0,\\sigma^{2})$，并将值裁剪到 $[0,1]$ 范围内。这种构造确保了类别仅通过象限的全局排列来区分，而非局部纹理。\n\n模型和学习目标：使用具有两个类别的多项逻辑回归（softmax回归）。对于一个输入向量 $x \\in \\mathbb{R}^{D}$，其中 $D = H \\cdot W$，将类别概率建模为\n$$\np_{\\theta}(y=k \\mid x) = \\frac{\\exp(w_{k}^{\\top} x + b_{k})}{\\sum_{j=0}^{1} \\exp(w_{j}^{\\top} x + b_{j})}, \\quad k \\in \\{0,1\\},\n$$\n参数为 $\\theta = \\{W,b\\}$，其中 $W \\in \\mathbb{R}^{D \\times 2}$，$b \\in \\mathbb{R}^{2}$。通过最小化交叉熵下的经验风险进行训练，可能使用软目标：\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{k=0}^{1} y_{i,k} \\log p_{\\theta}(y=k \\mid x_{i}),\n$$\n其中 $y_{i} \\in [0,1]^{2}$ 对于未增强的样本是独热编码，对于如下所述的CutMix可以是凸组合。使用小批量梯度下降优化 $\\mathcal{L}$。\n\n要实现的增强方法：\n- Cutout：给定一张图像 $x \\in [0,1]^{H \\times W}$ 和一个边长为 $s = \\lfloor \\sqrt{f} \\cdot H \\rfloor$（其中比例 $f \\in (0,1)$）的方形掩码，选择一个均匀随机的左上角位置以确保方块在边界内，并用一个常数值 $m \\in [0,1]$（使用数据集的平均强度）填充被掩码的区域，标签保持不变。\n- CutMix：给定两张带有标签 $y_{a}, y_{b}$ 的图像 $x_{a}, x_{b}$，对于 $\\alpha>0$，从 $\\operatorname{Beta}(\\alpha,\\alpha)$ 分布中采样 $\\lambda$。通过将其边长设置为与 $\\sqrt{1-\\lambda}$ 成比例，并将其放置在均匀随机的位置，来计算一个面积比例为 $(1-\\lambda)$ 的矩形区域。用 $x_{b}$ 中对应的图像块替换 $x_{a}$ 中的该区域以获得 $\\tilde{x}$，并使用混合标签 $\\tilde{y} = \\lambda y_{a} + (1-\\lambda) y_{b}$。作为一个边界情况，允许 $\\lambda = 0$ 的完全替换，这对应于使用整个 $x_{b}$ 图像及其标签 $y_{b}$。\n\n训练和评估协议：将图像展平为 $\\mathbb{R}^{D}$ 中的向量，使用固定的学习率和批量大小，训练模型固定的轮次数，然后在留出的测试集上报告测试准确率，该准确率定义为正确预测的类别索引的比例。使用固定的随机种子以确保确定性行为。\n\n测试套件：你的程序必须运行以下四种配置，并按所列顺序返回每种配置的测试准确率。\n- 情况1（理想路径）：无增强。\n- 情况2（覆盖变体）：使用Cutout，比例 $f = 0.50$，填充值等于数据集的平均强度 $m$。\n- 情况3（研究中的增强）：使用CutMix，Beta分布 $\\operatorname{Beta}(\\alpha,\\alpha)$ 中的 $\\alpha = 1.0$。\n- 情况4（边界条件）：使用完全替换的CutMix，即确定性地设置 $\\lambda = 0$，使得粘贴的矩形是整个图像。\n\n所有情况中使用的固定超参数和数据规格：\n- 图像高度 $H = 16$，宽度 $W = 16$。\n- 低强度 $c_{\\mathrm{lo}} = 0.20$，高强度 $c_{\\mathrm{hi}} = 0.80$。\n- 噪声标准差 $\\sigma = 0.05$。\n- 训练集大小 $N_{\\mathrm{train}} = 400$，测试集大小 $N_{\\mathrm{test}} = 200$。\n- 批量大小 $B = 64$，轮次数 $E = 60$，学习率 $\\eta = 0.1$。\n- 随机种子 $s_{0} = 42$。\n- 对两个标签使用独热编码，仅在应用CutMix时使用软凸组合。\n\n必需的最终输出格式：你的程序应生成单行输出，其中包含结果，格式为逗号分隔的浮点准确率列表，范围在 $[0,1]$ 内，顺序为 $[a_{1},a_{2},a_{3},a_{4}]$，对应于情况1到4，用方括号括起来，不含多余的空格或文本（例如，$[0.9750,0.9600,0.9100,0.5200]$）。此问题不涉及单位，任何分数量必须在程序输出中表示为小数。", "solution": "目标是研究空间数据增强——特别是Cutout和CutMix——对多项逻辑回归模型性能的影响。该学习任务的设计使得类别身份完全由特征的全局空间排列决定，而不是由局部内容决定。我们将从第一性原理出发，实现整个实验流程，包括数据集生成、通过梯度下降进行模型训练以及增强算法，以评估四种不同的训练配置。\n\n首先，我们正式定义合成数据集。图像大小为 $H \\times W$，其中 $H=16$ 且 $W=16$。有两个类别，$k \\in \\{0, 1\\}$。图像画布被划分为四个相等的 $8 \\times 8$ 象限。对于类别 $k=0$ 的图像，左上和右下象限填充高强度值 $c_{\\mathrm{hi}}=0.80$，而右上和左下象限填充低强度值 $c_{\\mathrm{lo}}=0.20$。对于类别 $k=1$，此分配相反。在此确定性构造之后，我们向每个像素添加从 $\\mathcal{N}(0, \\sigma^2)$（其中 $\\sigma=0.05$）中抽取的独立同分布高斯噪声。最终的像素强度被裁剪到 $[0, 1]$ 范围内。此过程生成一个数据集，其核心区分特征是全局性的“棋盘”强度模式。生成大小为 $N_{\\mathrm{train}}=400$ 和 $N_{\\mathrm{test}}=200$ 的训练集和测试集，类别均衡。所有随机过程的随机种子固定为 $s_0=42$ 以确保可复现性。\n\n使用的模型是多项逻辑回归，也称为softmax回归。输入图像首先被展平为一个向量 $x \\in \\mathbb{R}^{D}$，其中维度 $D = H \\cdot W = 256$。模型参数是一个权重矩阵 $W \\in \\mathbb{R}^{D \\times 2}$ 和一个偏置向量 $b \\in \\mathbb{R}^{2}$。对于给定的输入 $x$，模型为每个类别 $k$ 计算得分 $z_k = w_k^\\top x + b_k$。这些得分使用softmax函数转换为概率：\n$$\np_{\\theta}(y=k \\mid x) = \\frac{\\exp(z_k)}{\\sum_{j=0}^{1} \\exp(z_j)}\n$$\n其中 $\\theta = \\{W, b\\}$。\n\n模型参数通过最小化经验风险来学习，具体是指在训练数据集上的平均交叉熵损失。对于一个包含 $N$ 个样本 $\\{(x_i, y_i)\\}_{i=1}^N$ 的训练集，其中 $y_i$ 是标签向量，损失函数为：\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=0}^{1} y_{i,k} \\log p_{\\theta}(y=k \\mid x_{i})\n$$\n标签向量 $y_i$ 是标准分类的独热编码（例如，类别0为$[1, 0]$）。对于由CutMix创建的样本，$y_i$ 成为一个“软”标签，表示原始独热标签的凸组合。\n\n优化过程使用小批量梯度下降。对于大小为 $B$ 的小批量，损失函数关于参数的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{1}{B} X_{\\text{batch}}^\\top (P - Y_{\\text{batch}})\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{B} \\sum_{i=1}^{B} (p_i - y_i)\n$$\n其中 $X_{\\text{batch}} \\in \\mathbb{R}^{B \\times D}$ 是输入向量矩阵，$Y_{\\text{batch}} \\in \\mathbb{R}^{B \\times 2}$ 是标签向量矩阵，$P \\in \\mathbb{R}^{B \\times 2}$ 是预测概率矩阵。使用学习率 $\\eta=0.1$ 对参数进行 $E=60$ 轮的迭代更新：\n$$\nW \\leftarrow W - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W}\n$$\n$$\nb \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b}\n$$\n\n我们实现并评估两种数据增强技术：\n\n1.  **Cutout**：对于批次中的每张图像，选择一个方形区域并将其像素替换为一个常数值。方形的边长为 $s = \\lfloor \\sqrt{f} \\cdot H \\rfloor$，其中比例 $f=0.50$，得出 $s = \\lfloor \\sqrt{0.50} \\cdot 16 \\rfloor = 11$。方形的左上角被均匀随机地选择，以使其保持在图像边界内。填充值是训练数据集的平均强度。图像的类别标签保持不变。这种增强会遮挡图像的很大一部分，可能破坏全局模式。\n\n2.  **CutMix**：此技术结合了成对的训练样本。对于一对带有标签 $(y_a, y_b)$ 的图像 $(x_a, x_b)$，从Beta分布 $\\lambda \\sim \\operatorname{Beta}(\\alpha, \\alpha)$ 中采样一个混合系数 $\\lambda$，其中 $\\alpha=1.0$（这等价于一个均匀分布 $\\mathcal{U}[0,1]$）。从 $x_b$ 中剪切一个矩形块并粘贴到 $x_a$ 上。该块的面积是总图像面积的 $(1-\\lambda)$ 倍，其边长与 $\\sqrt{1-\\lambda}$ 成正比。块的位置是均匀随机选择的。得到的合成图像 $\\tilde{x}$ 被赋予一个软标签 $\\tilde{y} = \\lambda y_a + (1-\\lambda) y_b$。这迫使模型从碎片化的模式中学习，并将它们与按比例混合的标签关联起来。\n\n我们将执行四个测试案例来评估这些增强的影响：\n- **情况1**：无增强，作为基线。\n- **情况2**：使用Cutout进行训练（$f=0.50$）。\n- **情况3**：使用CutMix进行训练（$\\alpha=1.0$）。\n- **情况4**：CutMix的一个边界条件，其中 $\\lambda$ 被确定性地设置为 $0$。这导致用同一批次中随机选择的另一个样本 $(x_b, y_b)$ 替换训练样本 $(x_a, y_a)$。\n\n对于每种情况，我们在相同的训练数据上从头开始训练一个模型，并在相同的留出测试集上报告其最终准确率。这种受控比较将阐明这些增强如何与一个严重依赖全局上下文的学习问题相互作用。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and print the results.\n    \"\"\"\n    \n    # --- Fixed Hyperparameters and Data Specifications ---\n    H, W = 16, 16\n    c_lo, c_hi = 0.20, 0.80\n    sigma = 0.05\n    N_train, N_test = 400, 200\n    B = 64\n    E = 60\n    eta = 0.1\n    s0 = 42\n    \n    # --- Case-specific parameters ---\n    case_params = [\n        {'aug': 'none'},\n        {'aug': 'cutout', 'f': 0.50},\n        {'aug': 'cutmix', 'alpha': 1.0},\n        {'aug': 'cutmix_lambda_0'},\n    ]\n\n    results = []\n    for params in case_params:\n        # Each case must be fully deterministic and reproducible\n        accuracy = train_and_evaluate(\n            H=H, W=W, c_lo=c_lo, c_hi=c_hi, sigma=sigma,\n            N_train=N_train, N_test=N_test, B=B, E=E, eta=eta,\n            seed=s0, aug_params=params\n        )\n        results.append(f\"{accuracy:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef train_and_evaluate(H, W, c_lo, c_hi, sigma, N_train, N_test, B, E, eta, seed, aug_params):\n    \"\"\"\n    Generates data, trains a model under a specific augmentation, and evaluates it.\n    \"\"\"\n    \n    # --- Seeding for reproducibility ---\n    rng = np.random.default_rng(seed)\n\n    # --- Helper Functions ---\n    def softmax(z):\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def cross_entropy_loss(y_true, y_pred):\n        # Clip y_pred to avoid log(0)\n        y_pred = np.clip(y_pred, 1e-12, 1. - 1e-12)\n        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n        \n    # --- Dataset Generation ---\n    def generate_dataset(N, H, W, c_lo, c_hi, sigma, rng_gen):\n        n_per_class = N // 2\n        X = np.zeros((N, H, W), dtype=np.float32)\n        Y = np.zeros((N, 2), dtype=np.float32)\n        \n        h_half, w_half = H // 2, W // 2\n        \n        # Class 0\n        for i in range(n_per_class):\n            img = np.full((H, W), c_lo, dtype=np.float32)\n            img[:h_half, :w_half] = c_hi\n            img[h_half:, w_half:] = c_hi\n            X[i] = img\n            Y[i] = [1, 0]\n            \n        # Class 1\n        for i in range(n_per_class, N):\n            img = np.full((H, W), c_hi, dtype=np.float32)\n            img[:h_half, :w_half] = c_lo\n            img[h_half:, w_half:] = c_lo\n            X[i] = img\n            Y[i] = [0, 1]\n\n        # Add noise and clip\n        X += rng_gen.normal(0, sigma, X.shape)\n        X = np.clip(X, 0.0, 1.0)\n        \n        # Shuffle dataset\n        indices = np.arange(N)\n        rng_gen.shuffle(indices)\n        X, Y = X[indices], Y[indices]\n        \n        return X, Y\n\n    X_train, Y_train = generate_dataset(N_train, H, W, c_lo, c_hi, sigma, rng)\n    X_test, Y_test = generate_dataset(N_test, H, W, c_lo, c_hi, sigma, rng)\n\n    # --- Model Initialization ---\n    D = H * W\n    K = 2 \n    # Use the same RNG for reproducible weight initialization\n    w_rng = np.random.default_rng(seed)\n    W_mat = w_rng.normal(0, 0.01, (D, K))\n    b_vec = np.zeros((1, K))\n\n    # --- Augmentation setup ---\n    aug = aug_params['aug']\n    \n    # Calculate dataset mean for Cutout\n    mean_intensity = 0.0\n    if aug == 'cutout':\n        mean_intensity = np.mean(X_train)\n\n    # --- Training Loop ---\n    for epoch in range(E):\n        indices = np.arange(N_train)\n        rng.shuffle(indices)\n        X_train_shuffled, Y_train_shuffled = X_train[indices], Y_train[indices]\n\n        for i in range(0, N_train, B):\n            X_batch_orig = X_train_shuffled[i:i+B]\n            Y_batch_orig = Y_train_shuffled[i:i+B]\n            \n            actual_B = X_batch_orig.shape[0]\n            if actual_B == 0: continue\n\n            # Apply augmentations\n            X_batch_aug, Y_batch_aug = X_batch_orig.copy(), Y_batch_orig.copy()\n\n            if aug == 'cutout':\n                f = aug_params['f']\n                s = int(np.floor(np.sqrt(f) * H))\n                for j in range(actual_B):\n                    y1 = rng.integers(0, H - s + 1)\n                    x1 = rng.integers(0, W - s + 1)\n                    X_batch_aug[j, y1:y1+s, x1:x1+s] = mean_intensity\n            \n            elif aug == 'cutmix':\n                alpha = aug_params['alpha']\n                for j in range(actual_B):\n                    lam = rng.beta(alpha, alpha)\n                    rand_index = rng.integers(actual_B)\n                    \n                    xa, ya = X_batch_aug[j], Y_batch_aug[j]\n                    xb, yb = X_batch_aug[rand_index], Y_batch_aug[rand_index]\n                    \n                    ratio = np.sqrt(1. - lam)\n                    patch_h = int(H * ratio)\n                    patch_w = int(W * ratio)\n\n                    if patch_h > 0 and patch_w > 0:\n                        cy = rng.integers(H - patch_h + 1)\n                        cx = rng.integers(W - patch_w + 1)\n                        xa[cy:cy+patch_h, cx:cx+patch_w] = xb[cy:cy+patch_h, cx:cx+patch_w]\n\n                    X_batch_aug[j] = xa\n                    Y_batch_aug[j] = lam * ya + (1. - lam) * yb\n\n            elif aug == 'cutmix_lambda_0':\n                # Deterministic lambda = 0\n                for j in range(actual_B):\n                    rand_index = rng.integers(actual_B)\n                    # Complete replacement of image and label\n                    X_batch_aug[j] = X_batch_orig[rand_index]\n                    Y_batch_aug[j] = Y_batch_orig[rand_index]\n\n            # Flatten images\n            X_batch_flat = X_batch_aug.reshape(actual_B, D)\n\n            # Forward pass\n            scores = X_batch_flat @ W_mat + b_vec\n            probs = softmax(scores)\n\n            # Backward pass (gradient calculation)\n            grad_scores = (probs - Y_batch_aug) / actual_B\n            grad_W = X_batch_flat.T @ grad_scores\n            grad_b = np.sum(grad_scores, axis=0, keepdims=True)\n\n            # Update parameters\n            W_mat -= eta * grad_W\n            b_vec -= eta * grad_b\n\n    # --- Evaluation ---\n    X_test_flat = X_test.reshape(N_test, D)\n    test_scores = X_test_flat @ W_mat + b_vec\n    test_probs = softmax(test_scores)\n    \n    predictions = np.argmax(test_probs, axis=1)\n    ground_truth = np.argmax(Y_test, axis=1)\n    \n    accuracy = np.mean(predictions == ground_truth)\n    return accuracy\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3151909"}, {"introduction": "Mixup 是一种强大的增强技术，它通过对图像及其标签进行线性插值来创造“虚拟”训练样本。尽管这种方法在许多任务中表现出色，但此练习将引导你探索其一个潜在的陷阱：这种线性插值在何时会变得有害？通过在一个具有高度弯曲决策边界的合成数据集上进行实验，这个实践将让你能够量化 Mixup 如何在无意中鼓励模型学习一个过于简化的线性分类器，从而无法捕捉数据的真实复杂性 ([@problem_id:3111279])。这有助于你建立关于 Mixup 可能失败的场景和根本原因的深刻直觉。", "problem": "给定一个关于两个细粒度图像类别的纯数学合成模型，其贝叶斯最优决策边界是高度弯曲的。目标是研究带有参数 $\\alpha$ 的 Mixup 增强在何种情况下会产生有害的标签插值，从而鼓励形成过于线性的决策边界。您必须编写一个完整、可运行的程序，通过蒙特卡洛模拟，根据一个源于第一性原理和经过充分检验的事实得出的原则性准则，计算出现这种有害效应的 $\\alpha$ 值范围。\n\n设置如下。考虑位于 $\\mathbb{R}^2$ 中的两个类别，由两个同心圆上的点定义。类别 $\\mathcal{C}_0$ 是半径为 $r_0$ 的内圆，类别 $\\mathcal{C}_1$ 是半径为 $r_1$ 的外圆。这两个类别的贝叶斯最优分类器是径向阈值 $R$ 的指示函数，即，如果一个点 $x \\in \\mathbb{R}^2$ 满足 $\\|x\\|  R$，则被分类为 $\\mathcal{C}_0$；如果满足 $\\|x\\| \\ge R$，则被分类为 $\\mathcal{C}_1$。该决策边界是一个圆，因此是高度弯曲的。Mixup 通过凸组合生成增强样本：给定点 $x_i \\in \\mathcal{C}_0$、$x_j \\in \\mathcal{C}_1$、混合系数 $\\lambda \\sim \\operatorname{Beta}(\\alpha,\\alpha)$ 以及独热（one-hot）标签 $y_i = [1,0]$、$y_j = [0,1]$，增强后的点为 $x_{\\text{mix}} = \\lambda x_i + (1 - \\lambda) x_j$，增强后的软标签为 $y_{\\text{mix}} = \\lambda [1,0] + (1 - \\lambda) [0,1]$。对于具有弯曲边界的细粒度类别，Mixup 可能会将接近 0.5 的模糊标签应用于空间上远离贝叶斯决策边界的位置，从而迫使学习到的决策面向弦的方向趋于线性，而不是尊重原有的曲率。\n\n您必须设计一个量化准则，用于判断 Mixup 何时是“有害线性化的”。请使用以下定义、参数和单位：\n\n- 内圆和外圆的半径分别为 $r_0 = 1.0$ 和 $r_1 = 2.0$。\n- 贝叶斯决策阈值半径为 $R = 1.5$。\n- 角度在 $[0, 2\\pi)$ 范围内以弧度为单位进行均匀采样。\n- 如果一个 Mixup 样本满足 $|\\|x_{\\text{mix}}\\| - R| \\ge m$（其中边距 $m = 0.2$），则认为该样本相对于贝叶斯边界是“深的”。一个“深的”样本在空间上明确属于某一类别区域，而不是靠近边界。\n- 如果一个 Mixup 标签满足 $|\\lambda - 0.5| \\le \\delta$（其中 $\\delta = 0.1$），则认为该标签是“模糊的”，意味着软标签接近于对半插值。\n- 将有害性得分 $H(\\alpha)$ 定义为既是“深的”又具有“模糊”标签的增强样本所占的比例。直观上，较大的 $H(\\alpha)$ 表示 Mixup 频繁地将接近 0.5 的标签分配给明确属于某一类别区域的点，这会鼓励在弯曲区域上形成过于线性的决策边界。\n- 如果在给定的 $\\alpha$ 下，$H(\\alpha) \\ge \\tau$（其中阈值 $\\tau = 0.15$），则声明 Mixup 是“过度线性的”。\n\n您的程序必须执行以下操作：\n\n1. 固定随机种子以确保可复现性。生成 $N = 10000$ 个随机点对 $(x_i, x_j)$，其中 $x_i$ 位于半径为 $r_0$ 的内圆上，$x_j$ 位于半径为 $r_1$ 的外圆上，每个点都由从 $[0, 2\\pi)$ 均匀采样（以弧度为单位）的独立角度通过 $x = r[\\cos(\\theta), \\sin(\\theta)]$ 构建。\n\n2. 对于一个由从 $\\alpha_{\\text{min}} = 0.1$到 $\\alpha_{\\text{max}} = 64.0$ 线性间隔的 $M_{\\alpha} = 41$ 个点组成的 $\\alpha$ 值网格 $\\mathcal{A}$，通过以下方式估计 $H(\\alpha)$：\n   - 为 $N$ 个点对中的每一个独立采样 $\\lambda_k \\sim \\operatorname{Beta}(\\alpha, \\alpha)$。\n   - 构建 $x_{\\text{mix},k} = \\lambda_k x_{i,k} + (1 - \\lambda_k) x_{j,k}$。\n   - 计算同时满足 $|\\|x_{\\text{mix},k}\\| - R| \\ge m$ 和 $|\\lambda_k - 0.5| \\le \\delta$ 的索引 $k$ 所占的比例。\n\n3. 在网格 $\\mathcal{A}$ 中，识别出满足 $H(\\alpha) \\ge \\tau$ 的连续区间 $[\\alpha_{\\mathrm{low}}, \\alpha_{\\mathrm{high}}]$。如果没有网格点满足 $H(\\alpha) \\ge \\tau$，则定义 $\\alpha_{\\mathrm{low}} = 0.0$ 和 $\\alpha_{\\mathrm{high}} = 0.0$。\n\n4. 对以下 $\\alpha$ 值测试套件评估条件 $H(\\alpha) \\ge \\tau$：$\\alpha \\in \\{0.2, 0.5, 1.0, 2.0, 8.0, 64.0\\}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。前两个元素必须是区间端点 $\\alpha_{\\mathrm{low}}$ 和 $\\alpha_{\\mathrm{high}}$（四舍五入到3位小数），后面是按给定顺序排列的测试套件中每个 $\\alpha$ 值的布尔结果。例如，输出格式必须严格为 $[\\alpha_{\\mathrm{low}},\\alpha_{\\mathrm{high}},b_1,b_2,b_3,b_4,b_5,b_6]$ 的形式，其中每个 $b_i$ 为 $\\text{True}$ 或 $\\text{False}$。\n\n所有角度必须以弧度为单位。不涉及物理单位。阈值 $\\tau$ 必须作为小数（而非百分比）处理。\n\n该问题被设计为可测试的，并包含以下用于覆盖的案例：\n- 一般情况：$\\alpha = 2.0$。\n- 类似边界的情况：$\\alpha = 1.0$ 和 $\\alpha = 0.5$。\n- 边缘情况：非常小的 $\\alpha = 0.2$ 和非常大的 $\\alpha = 64.0$。\n- 较高但非极端的情况：$\\alpha = 8.0$。\n\n您的程序必须是自包含的，不需要用户输入，并严格遵守指定的输出格式。", "solution": "该问题要求通过蒙特卡洛模拟来确定 Mixup 参数 $\\alpha$ 的一个范围，在此范围内，增强方法对一个具有弯曲决策边界的合成数据集产生“有害线性化”效应。解决方案包括将问题准则形式化、实现模拟并分析结果。\n\n### 1. 几何模型与贝叶斯最优分类器\n\n该问题定义了 $\\mathbb{R}^2$ 中的一个二分类任务。类别 $\\mathcal{C}_0$ 由半径为 $r_0 = 1.0$ 的圆上的点组成，类别 $\\mathcal{C}_1$ 由半径为 $r_1 = 2.0$ 的同心圆上的点组成。贝叶斯最优决策边界能够最小化分类错误，将这两个类别分开。对于这个同心圆模型，最优边界是一个半径为 $R$ 的圆，其半径介于 $r_0$ 和 $r_1$ 之间。问题将此贝叶斯决策阈值半径指定为 $R = 1.5$。如果一个点 $x \\in \\mathbb{R}^2$ 的欧几里得范数 $\\|x\\|  R$，则被分类为 $\\mathcal{C}_0$；如果 $\\|x\\| \\ge R$，则被分类为 $\\mathcal{C}_1$。这个边界是“高度弯曲的”，理想的分类器应该能学习到这种圆形分隔。\n\n模拟所用的点是通过从 $[0, 2\\pi)$ 均匀采样角度 $\\theta$ 并使用极坐标到笛卡尔坐标的转换生成的：\n-   对于类别 $\\mathcal{C}_0$：$x_i = [r_0 \\cos(\\theta_i), r_0 \\sin(\\theta_i)]$\n-   对于类别 $\\mathcal{C}_1$：$x_j = [r_1 \\cos(\\theta_j), r_1 \\sin(\\theta_j)]$\n\n### 2. Mixup 增强\n\nMixup 通过对现有样本对进行凸组合来生成新的训练样本。给定一个来自类别 $\\mathcal{C}_0$ 的点 $x_i$ 和一个来自类别 $\\mathcal{C}_1$ 的点 $x_j$，以及它们对应的独热（one-hot）标签 $y_i = [1, 0]$ 和 $y_j = [0, 1]$，新样本 $(x_{\\text{mix}}, y_{\\text{mix}})$ 的创建方式如下：\n$$\n\\lambda \\sim \\operatorname{Beta}(\\alpha, \\alpha)\n$$\n$$\nx_{\\text{mix}} = \\lambda x_i + (1 - \\lambda) x_j\n$$\n$$\ny_{\\text{mix}} = \\lambda y_i + (1 - \\lambda) y_j = [\\lambda, 1-\\lambda]\n$$\n参数 $\\alpha$ 控制混合系数 $\\lambda$ 的分布。当 $\\alpha \\to \\infty$ 时，$\\lambda$ 急剧集中在 0.5 附近。当 $\\alpha \\to 0$ 时，$\\lambda$ 集中在端点 0 和 1。当 $\\alpha=1$ 时，$\\lambda$ 在 $[0,1]$上均匀分布。\n\n其几何解释是 $x_{\\text{mix}}$ 位于连接 $x_i$ 和 $x_j$ 的线段（弦）上。问题在于，这种空间上的线性插值与标签的线性插值相匹配。对于弯曲的边界，一个空间上远离边界的点可能会被分配一个高度模糊的标签（例如 $[0.5, 0.5]$），这会鼓励模型沿着弦学习一个线性的决策边界，而不是尊重真实的曲率。\n\n### 3. “有害线性化”的量化准则\n\n该问题提供了一种精确的、量化的方法来识别这种有害效应。它定义了两个条件：\n\n1.  **“模糊”标签**：如果插值标签 $y_{\\text{mix}} = [\\lambda, 1-\\lambda]$ 的分量接近 0.5，则认为它是模糊的。这被形式化为 $|\\lambda - 0.5| \\le \\delta$，其中 $\\delta = 0.1$。这对应于 $\\lambda \\in [0.4, 0.6]$。\n\n2.  **“深的”样本**：如果生成的点 $x_{\\text{mix}}$ 在空间上远离贝叶斯边界，则认为它是“深的”。这被形式化为 $|\\|x_{\\text{mix}}\\| - R| \\ge m$，其中边距为 $m = 0.2$。这意味着该点明确位于某一类别区域内部，而不是在半径为 $R=1.5$ 的边界圆附近的模糊区域内。\n\n**有害性得分 $H(\\alpha)$** 被定义为这两个事件同时发生的联合概率：即空间上“深的”且标签“模糊”的 Mixup 样本所占的比例。较高的 $H(\\alpha)$ 表示 Mixup 正在频繁地为分类器生成混淆信号。\n\n如果该得分超过一个阈值：$H(\\alpha) \\ge \\tau$（其中 $\\tau = 0.15$），则 Mixup 被视为**“过度线性”**。\n\n### 4. 蒙特卡洛模拟设计\n\n我们使用蒙特卡洛模拟来估计 $H(\\alpha)$。其步骤如下：\n\n1.  **固定随机性**：设置全局随机种子以确保可复现性。\n2.  **生成基础几何结构**：为 $k=1, \\dots, N$ 生成一个固定的包含 $N = 10000$ 个点对 $(x_{i,k}, x_{j,k})$ 的集合。角度 $\\theta_{i,k}$ 和 $\\theta_{j,k}$ 从 $[0, 2\\pi)$ 中独立均匀采样。这组几何配置在整个模拟过程中保持不变，以减少方差。\n3.  **遍历 $\\alpha$**：对于指定网格和测试套件中的每个 $\\alpha$ 值：\n    a.  **采样混合系数**：抽取 $N$ 个独立样本 $\\lambda_k \\sim \\operatorname{Beta}(\\alpha, \\alpha)$。\n    b.  **创建 Mixup 样本**：对每个 $k$ 计算 $x_{\\text{mix},k} = \\lambda_k x_{i,k} + (1 - \\lambda_k) x_{j,k}$。\n    c.  **评估条件**：对于每个生成的样本 $k$，检查是否满足“模糊”和“深”的条件：\n        -   $C_{\\text{ambiguous}, k}: |\\lambda_k - 0.5| \\le \\delta$\n        -   $C_{\\text{deep}, k}: |\\|x_{\\text{mix},k}\\| - R| \\ge m$\n    d.  **估计 $H(\\alpha)$**：得分是两个条件都为真的样本的经验分数：\n        $$\n        H(\\alpha) \\approx \\frac{1}{N} \\sum_{k=1}^{N} \\mathbb{I}(C_{\\text{ambiguous}, k} \\land C_{\\text{deep}, k})\n        $$\n        其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n### 5. 算法实现与分析\n\n该模拟使用 Python 的 `numpy` 库实现，以进行高效的向量化操作。\n\n-   一个辅助函数 `calculate_H` 封装了为给定 $\\alpha$ 计算 $H(\\alpha)$ 的逻辑，它使用预先生成的几何点对。\n-   该函数首先对从 $\\alpha_{\\text{min}}=0.1$ 到 $\\alpha_{\\text{max}}=64.0$ 的线性间隔网格上的每个 $\\alpha$ 进行调用。\n-   将得到的 $H(\\alpha)$ 值数组与阈值 $\\tau=0.15$ 进行比较。网格中满足 $H(\\alpha) \\ge \\tau$ 的最小和最大 $\\alpha$ 值定义了区间 $[\\alpha_{\\mathrm{low}}, \\alpha_{\\mathrm{high}}]$。如果没有值满足该条件，则区间默认为 $[0.0, 0.0]$。\n-   然后对特定测试套件 $\\{0.2, 0.5, 1.0, 2.0, 8.0, 64.0\\}$ 中的每个 $\\alpha$ 调用 `calculate_H` 函数，以确定每种情况下 $H(\\alpha) \\ge \\tau$ 的布尔结果。\n-   最后，将结果格式化为所需的字符串输出，浮点数四舍五入到三位小数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the range of Mixup alpha values that cause \"harmful linearization\"\n    and evaluates this condition for a specific test suite.\n    \"\"\"\n    # 1. Define constants and set random seed for reproducibility\n    SEED = 42\n    np.random.seed(SEED)\n\n    r0 = 1.0\n    r1 = 2.0\n    R = 1.5\n    m = 0.2\n    delta = 0.1\n    tau = 0.15\n\n    N = 10000\n    M_alpha = 41\n    alpha_min = 0.1\n    alpha_max = 64.0\n\n    # 2. Generate N random pairs of points (xi, xj)\n    # This geometric data is fixed for all subsequent alpha evaluations\n    theta_i = np.random.uniform(0.0, 2.0 * np.pi, N)\n    theta_j = np.random.uniform(0.0, 2.0 * np.pi, N)\n    \n    xi = r0 * np.column_stack([np.cos(theta_i), np.sin(theta_i)])\n    xj = r1 * np.column_stack([np.cos(theta_j), np.sin(theta_j)])\n\n    def calculate_H(alpha, n_samples, xi_data, xj_data):\n        \"\"\"\n        Calculates the harmfulness score H(alpha) for a given alpha.\n        \"\"\"\n        if alpha == 0:\n            return 0.0\n            \n        # Sample N mixing coefficients from the Beta(alpha, alpha) distribution\n        lambdas = np.random.beta(alpha, alpha, n_samples)\n        \n        # Form the mixed points\n        # Reshape lambdas for broadcasting: (n_samples,) -> (n_samples, 1)\n        x_mix = lambdas[:, np.newaxis] * xi_data + (1 - lambdas)[:, np.newaxis] * xj_data\n        \n        # Calculate the Euclidean norms of the mixed points\n        x_mix_norm = np.linalg.norm(x_mix, axis=1)\n        \n        # Check the \"deep\" and \"ambiguous\" conditions\n        is_deep = np.abs(x_mix_norm - R) >= m\n        is_ambiguous = np.abs(lambdas - 0.5) = delta\n        \n        # The harmfulness score is the fraction of samples meeting both criteria\n        H = np.mean(is_deep  is_ambiguous)\n        return H\n\n    # 3. Compute H(alpha) over the specified grid of alpha values\n    alpha_grid = np.linspace(alpha_min, alpha_max, M_alpha)\n    H_values = np.array([calculate_H(alpha, N, xi, xj) for alpha in alpha_grid])\n\n    # 4. Identify the interval [alpha_low, alpha_high]\n    is_overly_linear = H_values >= tau\n    harmful_alphas = alpha_grid[is_overly_linear]\n    \n    if harmful_alphas.size > 0:\n        alpha_low = np.min(harmful_alphas)\n        alpha_high = np.max(harmful_alphas)\n    else:\n        alpha_low = 0.0\n        alpha_high = 0.0\n\n    # 5. Evaluate the condition for the test suite of alpha values\n    test_suite = [0.2, 0.5, 1.0, 2.0, 8.0, 64.0]\n    test_results = []\n    for alpha_test in test_suite:\n        H_test = calculate_H(alpha_test, N, xi, xj)\n        test_results.append(H_test >= tau)\n\n    # 6. Format the final output string\n    # Format: [alpha_low, alpha_high, b1, b2, b3, b4, b5, b6]\n    output_list = [f\"{alpha_low:.3f}\", f\"{alpha_high:.3f}\"] + [str(b) for b in test_results]\n    print(f\"[{','.join(output_list)}]\")\n\nsolve()\n```", "id": "3111279"}, {"introduction": "传统的增强策略通常在整个训练过程中使用固定的变换强度，但更高级的方法是让增强策略动态化。如果增强的强度可以根据模型的训练状态自适应调整，效果会如何呢？这个练习将指导你实现一个先进的自适应增强方案，其中变换的强度会随着模型学习的停滞（以梯度范数 $g_t = \\\\lVert \\\\nabla_{\\\\theta} \\\\ell(\\\\theta_t) \\\\rVert_2$ 减小为标志）而增强 ([@problem_id:3111262])。这种策略模拟了一种“推动”模型跳出潜在过拟合区域的机制，帮助其更稳健地探索数据空间，从而引出从静态策略到动态自适应策略的思维转变。", "problem": "您将实现一个完整的、确定性的程序，为一张玩具图像构建一个自适应图像数据增强流程。增强强度必须根据梯度范数序列的演变进行自适应调整，当梯度范数较小时，应采用更强的变换以减轻过拟合。设计必须从经验风险最小化和基于梯度的优化的基本原则出发，并且只能使用基本的统计操作和确定性变换。所有三角函数运算必须使用弧度制。\n\n从监督学习中的以下基础开始。对于一个通过最小化经验风险来训练的、参数为 $\\theta$ 的模型，瞬时小批量损失为 $\\ell(\\theta)$，梯度为 $\\nabla_{\\theta} \\ell(\\theta)$。您的增强控制器可用的标量信号是在离散步骤 $t = 1, 2, \\dots$ 观察到的梯度范数 $g_t = \\lVert \\nabla_{\\theta} \\ell(\\theta_t) \\rVert_2$。过拟合与持续存在的极小 $g_t$ 相关，而泛化能力并未随之改善，这促使当 $g_t$ 相对于其近期典型尺度较小时，应采用更强的数据增强。\n\n您的任务如下。\n\n1) 在每个步骤 $t$，仅使用在线统计数据，从观测序列 $\\{g_1, \\dots, g_t\\}$ 中定义一个自适应增强强度 $s_t \\in (0, 1)$。使用指数移动平均 (EMA) 来跟踪 $g_t$ 的一阶矩和二阶矩，平滑参数为 $\\beta$，并用第一次观测值进行初始化以避免冷启动偏差。使用以下更新方程估算矩：\n$$\nm_t = (1 - \\beta) m_{t-1} + \\beta g_t, \\quad v_t = (1 - \\beta) v_{t-1} + \\beta g_t^2,\n$$\n其中 $m_1 = g_1$ 且 $v_1 = g_1^2$。为避免在计算 $t \\ge 2$ 的标准化偏差时出现循环，使用上一步的统计数据构成\n$$\nz_t = \\frac{g_t - m_{t-1}}{\\sqrt{\\max(v_{t-1} - m_{t-1}^2, \\varepsilon)}},\n$$\n其中 $\\varepsilon$ 是一个小的正常数。通过一个 logistic 函数将此标准化偏差映射到一个强度分量\n$$\ns^{(1)}_t = \\frac{1}{1 + \\exp(\\gamma z_t)},\n$$\n其中 $\\gamma  0$ 控制陡峭度。为防止在方差崩溃时出现全局小梯度，定义一个绝对尺度分量\n$$\ns^{(2)}_t = \\frac{1}{1 + \\left(\\frac{g_t}{\\tau_0}\\right)^k},\n$$\n阈值为 $\\tau_0  0$ 且指数为 $k \\ge 1$。将这两个分量组合成最终强度\n$$\ns_t = \\frac{1}{2} s^{(1)}_t + \\frac{1}{2} s^{(2)}_t.\n$$\n使用 $\\beta = 0.2$、$\\gamma = 1.5$、$\\varepsilon = 10^{-8}$、$\\tau_0 = 0.05$ 和 $k = 2$。您为每个测试报告的 $s_t$ 值应在所提供序列的最终时间索引 $t$ 处计算。\n\n2) 定义一个确定性的玩具灰度图像和由 $s_t$ 控制的确定性增强变换。设图像为一个网格 $I \\in [0,1]^{8 \\times 8}$，其条目为\n$$\nI_{ij} = \\frac{i + j}{14}, \\quad i,j \\in \\{0,1,\\dots,7\\},\n$$\n使得 $I$ 是一个平滑斜坡。定义一个确定性的、零均值的空间模式\n$$\nN_{ij} = \\sin\\!\\left(\\frac{2\\pi (i + j)}{8}\\right),\n$$\n其中正弦函数的参数以弧度为单位。给定 $s_t$，计算以下变换：\n- 亮度缩放因子 $B = 1 + 0.3 s_t$。\n- 应用于 $N$ 的附加模式振幅 $\\sigma = 0.1 s_t$。\n- 合成中间图像\n$$\nJ = \\operatorname{clip}(B \\cdot I + \\sigma \\cdot N, 0, 1),\n$$\n其中 $\\operatorname{clip}$ 将值饱和到区间 $[0,1]$ 内。\n- 应用一个中心裁剪，其像素边长为 $L = \\left\\lfloor 0.5 s_t \\cdot 8 \\right\\rfloor$。如果 $L = 0$，则不执行任何操作。否则，将 $J$ 中边长为 $L$ 的中心正方形替换为 $J$ 的全局均值，以获得增强后的图像 $A$。\n\n通过均方失真来量化增强效果\n$$\nD = \\frac{1}{64} \\sum_{i=0}^{7} \\sum_{j=0}^{7} \\left(A_{ij} - I_{ij}\\right)^2.\n$$\n\n3) 使用指定的运行时将上述内容实现为一个独立的、自包含的程序。程序必须为以下五个梯度范数序列中的每一个计算增强强度 $s_t$（在最后一步 $t$）和失真 $D$，这些序列构成了测试套件：\n- 案例1（递减）：$[1.0, 0.8, 0.6, 0.5, 0.4]$。\n- 案例2（递增）：$[0.2, 0.3, 0.4, 0.6, 0.9]$。\n- 案例3（恒定小值）：$[0.05, 0.05, 0.05, 0.05, 0.05]$。\n- 案例4（交替）：$[0.9, 0.1, 0.9, 0.1, 0.9]$。\n- 案例5（接近零）：$[0.0, 0.0, 0.0, 0.0]$。\n\n对于每个案例，输出两个浮点数：首先是最终的增强强度 $s_t$，然后是失真 $D$。您的程序应生成单行输出，其中包含所有结果，形式为方括号内以逗号分隔的列表，按上述案例的顺序排列，每个浮点数四舍五入到六位小数，即 $[s_1, D_1, s_2, D_2, s_3, D_3, s_4, D_4, s_5, D_5]$。", "solution": "解决方案通过遵循问题陈述中描述的三个主要任务来实现。该过程被分解为增强强度的计算、对玩具图像应用变换以及失真的最终计算。\n\n### 1. 自适应增强强度计算 ($s_t$)\n\n自适应机制的核心是增强强度 $s_t \\in (0, 1)$，它在每个步骤 $t$ 根据观测到的梯度范数历史 $\\{g_1, g_2, \\dots, g_t\\}$ 动态计算得出。对于给定的梯度范数序列，其过程如下：\n\n- **初始化**：在步骤 $t=1$ 时，指数移动平均 (EMA) 矩直接从第一次观测值初始化：$m_1 = g_1$ 和 $v_1 = g_1^2$。\n- **递归更新**：对于后续步骤 $t \\ge 2$，梯度范数序列的一阶矩 $m_t$（均值）和二阶矩 $v_t$（平方的均值）使用 EMA 公式进行更新，平滑参数为 $\\beta = 0.2$：\n$$\nm_t = (1 - \\beta) m_{t-1} + \\beta g_t\n$$\n$$\nv_t = (1 - \\beta) v_{t-1} + \\beta g_t^2\n$$\n- **标准化偏差 ($z_t$)**：当前梯度范数 $g_t$ 相对于上一步的统计数据 $m_{t-1}$ 和 $v_{t-1}$ 进行标准化。这产生了一个衡量当前梯度范数异常程度的指标。对于 $t=1$，我们设置 $z_1=0$。对于 $t \\ge 2$，公式为：\n$$\nz_t = \\frac{g_t - m_{t-1}}{\\sqrt{\\max(v_{t-1} - m_{t-1}^2, \\varepsilon)}}\n$$\n其中 $\\varepsilon = 10^{-8}$ 是一个小的常数，确保分母（近似于标准差）不为零。\n- **强度分量**：强度 $s_t$ 是两个分量的组合。\n    1.  $s^{(1)}_t$ 基于标准化偏差 $z_t$。它将大的负 $z_t$（即 $g_t$ 远小于其近期平均值）映射到高强度。使用陡峭度 $\\gamma = 1.5$ 的 logistic 函数：\n        $$\n        s^{(1)}_t = \\frac{1}{1 + \\exp(\\gamma z_t)}\n        $$\n    2.  $s^{(2)}_t$ 基于 $g_t$ 的绝对大小。它确保在梯度范数绝对值较小时，无论其近期历史如何，都具有高增强强度。这可以防止所有梯度都小且方差崩溃的情况。它使用阈值 $\\tau_0 = 0.05$ 和指数 $k=2$ 定义：\n        $$\n        s^{(2)}_t = \\frac{1}{1 + \\left(\\frac{g_t}{\\tau_0}\\right)^k}\n        $$\n- **最终强度**：将两个分量平均以产生步骤 $t$ 的最终强度：\n$$\ns_t = \\frac{1}{2} s^{(1)}_t + \\frac{1}{2} s^{(2)}_t\n$$\n对于每个测试案例，此计算针对整个梯度范数序列进行，并使用最后一步的 $s_t$ 值进行后续的图像增强。\n\n### 2. 图像增强流程\n\n定义了一个确定性的玩具图像和一组变换，以展示计算出的强度 $s_t$ 的效果。\n\n- **基础图像 ($I$)**：生成一个尺寸为 $8 \\times 8$ 的灰度图像 $I$，其像素值形成一个线性斜坡。对于 $i,j \\in \\{0, 1, \\dots, 7\\}$，位置 $(i,j)$ 的值为：\n$$\nI_{ij} = \\frac{i + j}{14}\n$$\n这确保了像素值在 $[0,1]$ 范围内。\n- **噪声模式 ($N$)**：使用正弦函数创建一个尺寸为 $8 \\times 8$ 的确定性、零均值空间模式 $N$，其参数以弧度为单位：\n$$\nN_{ij} = \\sin\\!\\left(\\frac{2\\pi (i + j)}{8}\\right)\n$$\n- **变换**：最终的增强强度 $s_t$ 控制应用于基础图像 $I$ 的一系列变换：\n    1.  **亮度和对比度**：通过应用亮度缩放和添加噪声模式形成中间图像 $J$。亮度因子为 $B = 1 + 0.3 s_t$，噪声振幅为 $\\sigma = 0.1 s_t$。生成的图像被裁剪到 $[0,1]$ 范围内：\n        $$\n        J = \\operatorname{clip}(B \\cdot I + \\sigma \\cdot N, 0, 1)\n        $$\n    2.  **中心裁剪**：图像 $J$ 中心的正方形区域被 $J$ 的全局均值替换。该正方形的边长为 $L = \\lfloor 0.5 s_t \\cdot 8 \\rfloor = \\lfloor 4 s_t \\rfloor$ 像素。如果 $L=0$，则不执行裁剪。对于 $8 \\times 8$ 的网格，边长为 $L$ 的中心正方形起始索引为 $(\\lfloor \\frac{8-L}{2} \\rfloor, \\lfloor \\frac{8-L}{2} \\rfloor)$。此操作的结果是最终的增强图像 $A$。\n\n### 3. 失真度量 ($D$)\n\n为了量化所应用增强的幅度，计算了增强图像 $A$ 和原始图像 $I$ 之间的均方失真 $D$。这被定义为整个 $8 \\times 8 = 64$ 像素网格上像素值平方差的平均值：\n$$\nD = \\frac{1}{64} \\sum_{i=0}^{7} \\sum_{j=0}^{7} \\left(A_{ij} - I_{ij}\\right)^2\n$$\n\n所提供的程序实现了这个完整的流程，为五个指定的梯度范数序列中的每一个计算最终强度 $s_t$ 和相应的失真 $D$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the adaptive augmentation simulation for all test cases.\n    \"\"\"\n    \n    # Define the problem parameters.\n    BETA = 0.2\n    GAMMA = 1.5\n    EPSILON = 1e-8\n    TAU0 = 0.05\n    K_EXP = 2\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        [1.0, 0.8, 0.6, 0.5, 0.4],       # Case 1 (decreasing)\n        [0.2, 0.3, 0.4, 0.6, 0.9],       # Case 2 (increasing)\n        [0.05, 0.05, 0.05, 0.05, 0.05], # Case 3 (constant small)\n        [0.9, 0.1, 0.9, 0.1, 0.9],       # Case 4 (alternating)\n        [0.0, 0.0, 0.0, 0.0],            # Case 5 (near zero)\n    ]\n\n    # --- Helper function to process one case ---\n    def process_case(g_sequence):\n        # Part 1: Compute final augmentation strength s_t\n        m_prev, v_prev = 0.0, 0.0\n        s_t = 0.0\n        \n        for t, g_t in enumerate(g_sequence):\n            # t is 0-indexed, so simulation step is t+1\n            if t == 0:  # Step t=1\n                m_t = g_t\n                v_t = g_t**2\n                # As per standard convention for the first step where no\n                # prior statistics exist, z_1 is taken as 0.\n                z_t = 0.0\n            else:  # Step t>=2\n                # Standardized deviation z_t uses statistics from step t-1\n                var_prev = v_prev - m_prev**2\n                std_dev_prev = np.sqrt(max(var_prev, EPSILON))\n                z_t = (g_t - m_prev) / std_dev_prev\n                \n                # Update EMA moments for current step t\n                m_t = (1.0 - BETA) * m_prev + BETA * g_t\n                v_t = (1.0 - BETA) * v_prev + BETA * g_t**2\n            \n            # Calculate strength components for step t\n            s1_t = 1.0 / (1.0 + np.exp(GAMMA * z_t))\n            s2_t = 1.0 / (1.0 + (g_t / TAU0)**K_EXP)\n            \n            # Final combined strength for step t\n            s_t = 0.5 * s1_t + 0.5 * s2_t\n            \n            # Store current moments for the next iteration\n            m_prev, v_prev = m_t, v_t\n        \n        final_s = s_t\n    \n        # Part 2: Image Augmentation\n        img_size = 8\n        \n        # Create original image I\n        i_coords, j_coords = np.indices((img_size, img_size))\n        # Denominator is 2 * (img_size - 1) = 14\n        I = (i_coords + j_coords) / (2.0 * (img_size - 1))\n\n        # Create noise pattern N\n        N_pattern = np.sin(2.0 * np.pi * (i_coords + j_coords) / img_size)\n\n        # Augmentation parameters from final strength\n        B = 1.0 + 0.3 * final_s\n        sigma = 0.1 * final_s\n\n        # Intermediate image J\n        J = np.clip(B * I + sigma * N_pattern, 0.0, 1.0)\n\n        # Cutout logic\n        # L = floor(0.5 * s_t * 8) = floor(4 * s_t)\n        L = int(np.floor(4.0 * final_s))\n        A = J.copy()  # Start with J, then apply cutout\n        \n        if L > 0:\n            mean_J = np.mean(J)\n            start_idx = (img_size - L) // 2\n            A[start_idx:start_idx + L, start_idx:start_idx + L] = mean_J\n\n        # Part 3: Distortion\n        D = np.mean((A - I)**2)\n        \n        return final_s, D\n\n    # --- Main loop to gather results ---\n    results = []\n    for case_sequence in test_cases:\n        s, d = process_case(case_sequence)\n        results.extend([s, d])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3111262"}]}